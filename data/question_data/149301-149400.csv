,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Linear span of $\{e^{-n x}\}_{n\in\mathbb N}$ is dense in $L^2((0,\infty))$. Explicit expansion?",Linear span of  is dense in . Explicit expansion?,"\{e^{-n x}\}_{n\in\mathbb N} L^2((0,\infty))","I proved that the linear span of $\{e^{-n x}\}_{n\in\mathbb N}$ is dense in $L^2((0,\infty))$ (see below). Question : Given $f\in L^2((0,\infty))$ , can I find ""expansion coefficients"" $f_n$ such that $$f(x) = \sum_{n\in\mathbb N} f_ne^{-n x} ?$$ Proof of density : Using the Stone-Weierstrass theorem for locally compact Hausdorff space, the linear span of $\{e^{-n x}\}_{n\in\mathbb N}$ is dense in $C_0((0,\infty))$ , the space of continuous functions vanishing at infinity. Since $C_0((0,\infty))$ is dense in $L^2((0,\infty))$ , we prove the result. Note : After very nice comments, the question is simplified.","I proved that the linear span of is dense in (see below). Question : Given , can I find ""expansion coefficients"" such that Proof of density : Using the Stone-Weierstrass theorem for locally compact Hausdorff space, the linear span of is dense in , the space of continuous functions vanishing at infinity. Since is dense in , we prove the result. Note : After very nice comments, the question is simplified.","\{e^{-n x}\}_{n\in\mathbb N} L^2((0,\infty)) f\in L^2((0,\infty)) f_n f(x) = \sum_{n\in\mathbb N} f_ne^{-n x} ? \{e^{-n x}\}_{n\in\mathbb N} C_0((0,\infty)) C_0((0,\infty)) L^2((0,\infty))","['real-analysis', 'analysis', 'dense-subspaces']"
1,Let $f \in C^2(\mathbb{R}^n)$ harmonic and $g(x):=tan(f(x))+e^{-|x|^2}$ bounded,Let  harmonic and  bounded,f \in C^2(\mathbb{R}^n) g(x):=tan(f(x))+e^{-|x|^2},"I am working on the following exercise: Let $f \in C^2(\mathbb{R}^n)$ be a harmonic function. Let further $g(x):=tan(f(x))+e^{-|x|^2}$ be bounded. Show that f is constant. My approach: To show the above, I want to use the liouville theorem for harmonic functions. Since I know that $f$ is harmonic, I need to show that it is also bounded: Since $g(x):=tan(f(x))+e^{-|x|^2}$ I can rewrite to $arctan(g(x)-e^{-|x|^2})=f(x)$ . Because $arctan$ is bounded and equal to $f$ it follows that $f$ is bounded. Applying the liouville theorem shows that $f$ is constant. Question: Is my approach correct?","I am working on the following exercise: Let be a harmonic function. Let further be bounded. Show that f is constant. My approach: To show the above, I want to use the liouville theorem for harmonic functions. Since I know that is harmonic, I need to show that it is also bounded: Since I can rewrite to . Because is bounded and equal to it follows that is bounded. Applying the liouville theorem shows that is constant. Question: Is my approach correct?",f \in C^2(\mathbb{R}^n) g(x):=tan(f(x))+e^{-|x|^2} f g(x):=tan(f(x))+e^{-|x|^2} arctan(g(x)-e^{-|x|^2})=f(x) arctan f f f,"['real-analysis', 'analysis', 'harmonic-functions']"
2,Limit of $u_n = \sum_{k=1}^{n}\frac{1}{\sqrt{n^2+2k}}$,Limit of,u_n = \sum_{k=1}^{n}\frac{1}{\sqrt{n^2+2k}},"I want to find the limit of this sequence if it exists : $$ u_n = \sum_{k=1}^{n}\frac{1}{\sqrt{n^2+2k}} $$ My attempt is to first remark that for $k\in\{1,...,n\}$ : $$ \frac{1}{\sqrt{n^2+2n}}\leq\frac{1}{\sqrt{n^2+2k}}\leq\frac{1}{\sqrt{n^2+2}}\implies\frac{n}{\sqrt{n^2+2n}}\leq\sum_{k=1}^{n}\frac{1}{\sqrt{n^2+2k}}\leq\frac{n}{\sqrt{n^2+2}} $$ Which leads to : $$ \frac{1}{\sqrt{1+\frac{2}{n}}}\leq\sum_{k=1}^{n}\frac{1}{\sqrt{n^2+2k}}\leq\frac{1}{\sqrt{1+\frac{2}{n^2}}}\implies\lim\limits_{n\to\infty}u_n=\lim\limits_{n\to\infty}\sum_{k=1}^{n}\frac{1}{\sqrt{n^2+2k}} = 1 $$ The last implication follows from the fact that the square root function is continuous at $x = 1$ and the squeeze theorem. I would like to know first if my attempt is correct and if you have other ideas to show this that can be enlightening ! Thank you a lot",I want to find the limit of this sequence if it exists : My attempt is to first remark that for : Which leads to : The last implication follows from the fact that the square root function is continuous at and the squeeze theorem. I would like to know first if my attempt is correct and if you have other ideas to show this that can be enlightening ! Thank you a lot,"
u_n = \sum_{k=1}^{n}\frac{1}{\sqrt{n^2+2k}}
 k\in\{1,...,n\} 
\frac{1}{\sqrt{n^2+2n}}\leq\frac{1}{\sqrt{n^2+2k}}\leq\frac{1}{\sqrt{n^2+2}}\implies\frac{n}{\sqrt{n^2+2n}}\leq\sum_{k=1}^{n}\frac{1}{\sqrt{n^2+2k}}\leq\frac{n}{\sqrt{n^2+2}}
 
\frac{1}{\sqrt{1+\frac{2}{n}}}\leq\sum_{k=1}^{n}\frac{1}{\sqrt{n^2+2k}}\leq\frac{1}{\sqrt{1+\frac{2}{n^2}}}\implies\lim\limits_{n\to\infty}u_n=\lim\limits_{n\to\infty}\sum_{k=1}^{n}\frac{1}{\sqrt{n^2+2k}} = 1
 x = 1","['calculus', 'sequences-and-series', 'analysis', 'solution-verification']"
3,Maximum of derivatives of a differentiable function,Maximum of derivatives of a differentiable function,,"I am preparing for the ""concours aux grandes écoles"" and I was looking for some problems on analysis. These problem are hard but I generally am able to solve them. However, I am stuck on a problem since this morning and have absolutely no idea about how to solve it. I am new to this forum so do not blame me if I express myself incorrectly or in the wrong feed. Here is the problem. Let $n\geq 2$ and $f:\mathbb{R}\rightarrow \mathbb{R}$ be a $\mathcal{C}^n$ function. For any $0\leq k\leq n$ define $$M_k=\sup_{t\in \mathbb{R}}\vert f^{(k)}(t)\vert.$$ Suppose that $M_0$ and $M_n$ are finite. Show that $$M_k < +\infty$$ for every $k$ . Show furthermore that $$M_1 \leq \sqrt{2M_0 M_2}.$$ For the second one, Taylor at the left and at the right of some point $x$ , up to $2$ nd derivative. This gives me $$f'(x)\leq \frac{2M_0}{2h}+\frac{h}{4}2M_2$$ Thank to Ted Shifrin, I noticed that I can get the second inequality by just minimizing the bound over $h$ , silly me ! Doing this, I get the desired bound. However, I have still no idea how to do the first question.","I am preparing for the ""concours aux grandes écoles"" and I was looking for some problems on analysis. These problem are hard but I generally am able to solve them. However, I am stuck on a problem since this morning and have absolutely no idea about how to solve it. I am new to this forum so do not blame me if I express myself incorrectly or in the wrong feed. Here is the problem. Let and be a function. For any define Suppose that and are finite. Show that for every . Show furthermore that For the second one, Taylor at the left and at the right of some point , up to nd derivative. This gives me Thank to Ted Shifrin, I noticed that I can get the second inequality by just minimizing the bound over , silly me ! Doing this, I get the desired bound. However, I have still no idea how to do the first question.",n\geq 2 f:\mathbb{R}\rightarrow \mathbb{R} \mathcal{C}^n 0\leq k\leq n M_k=\sup_{t\in \mathbb{R}}\vert f^{(k)}(t)\vert. M_0 M_n M_k < +\infty k M_1 \leq \sqrt{2M_0 M_2}. x 2 f'(x)\leq \frac{2M_0}{2h}+\frac{h}{4}2M_2 h,"['analysis', 'taylor-expansion']"
4,"If $f$ is continuous and $f'(x)\ge 0$, outside of a countable set, then $f$ is increasing","If  is continuous and , outside of a countable set, then  is increasing",f f'(x)\ge 0 f,"PROBLEM. Let $f:[a,b]\to\mathbb R$ be a continuous function, such that $f'(x)\ge 0$ , for all $x\in [a,b]\setminus A$ , where $A\subset [a,b]$ is a countable set. Show that $f$ is increasing. Attention. In this problem, we DO NOT assume that $f$ is differentiable in the whole $[a,b]$ . Notes. (1) If we assume that $f$ is differentiable in the whole interval, then we can easily show that $f'(x)\ge 0$ , everywhere. For otherwise, if $f'(x_0)=c<0$ , for some $x_0\in [a,b]$ , then by virtue of Darboux's Theorem , $(c,0)\subset f'([a,b])$ , and hence, $f'(x)<0$ , for uncountably many $x$ 's. (2) The conclusion of the problem does not hold if we replace the assumption $A$ is countable with $A$ is a set of measure zero . Take for example the Devil's staircase , with a negative sign in front. (3) If the hypothesis $f'(x)\ge 0$ , is replaced by $f'(x)=0$ , then the conclusion becomes f is constant .","PROBLEM. Let be a continuous function, such that , for all , where is a countable set. Show that is increasing. Attention. In this problem, we DO NOT assume that is differentiable in the whole . Notes. (1) If we assume that is differentiable in the whole interval, then we can easily show that , everywhere. For otherwise, if , for some , then by virtue of Darboux's Theorem , , and hence, , for uncountably many 's. (2) The conclusion of the problem does not hold if we replace the assumption is countable with is a set of measure zero . Take for example the Devil's staircase , with a negative sign in front. (3) If the hypothesis , is replaced by , then the conclusion becomes f is constant .","f:[a,b]\to\mathbb R f'(x)\ge 0 x\in [a,b]\setminus A A\subset [a,b] f f [a,b] f f'(x)\ge 0 f'(x_0)=c<0 x_0\in [a,b] (c,0)\subset f'([a,b]) f'(x)<0 x A A f'(x)\ge 0 f'(x)=0","['real-analysis', 'calculus', 'derivatives', 'continuity', 'monotone-functions']"
5,Upper estimates for $\int_0^M |f(x)|^2 dx$ and $ \int_0^M f(x) dx$ when $\|f\|_{\infty}\le M$.,Upper estimates for  and  when .,\int_0^M |f(x)|^2 dx  \int_0^M f(x) dx \|f\|_{\infty}\le M,"Let $M>0$ be fixed and let $f$ be a continuous real valued function such that $$\|f\|_{\infty}\le M,$$ where $\|\cdot\|_{\infty}$ denotes the sup-norm. Under these assumption, I need an upper estimate for $$\int_0^M |f(x)|^2 dx\quad\text{ and }\quad \int_0^M f(x) dx.$$ About me the correct ones are the following: $$\int_0^M |f(x)|^2 dx\le M^3$$ and $$\int_0^M f(x) dx\le M^2.$$ Could someone please tell me if am I right or not? Thank you in advance.","Let be fixed and let be a continuous real valued function such that where denotes the sup-norm. Under these assumption, I need an upper estimate for About me the correct ones are the following: and Could someone please tell me if am I right or not? Thank you in advance.","M>0 f \|f\|_{\infty}\le M, \|\cdot\|_{\infty} \int_0^M |f(x)|^2 dx\quad\text{ and }\quad \int_0^M f(x) dx. \int_0^M |f(x)|^2 dx\le M^3 \int_0^M f(x) dx\le M^2.","['real-analysis', 'calculus', 'integration', 'analysis']"
6,"Proof that $f_n(y)=\frac{n^2 y}{1+n y+n^4 y^2}$ does not converge uniformly, on $[0, \infty)$, to the identically-0 function","Proof that  does not converge uniformly, on , to the identically-0 function","f_n(y)=\frac{n^2 y}{1+n y+n^4 y^2} [0, \infty)","For $n \in \mathbb{N}$ , the function $f_n:[0, \infty) \rightarrow \mathbb{R}$ is defined by $$ f_n(y)=\frac{n^2 y}{1+n y+n^4 y^2} $$ I am trying to prove that the sequence $\{f_{n}\}_{n\in \mathbb{N}}$ does not converge uniformly to the identically-zero function (the function $f$ given by $f(y)=0$ , $\forall$ y). Is the following correct? Set $\epsilon=\frac{1}{3}$ . Consider arbitrary $N\in \mathbb{N}$ . Choose an arbitrary $n>N$ . Set $y=\frac{1}{n^2}$ . Thus $$\left|\frac{n^2 y}{1+n y+n^4 y^2}\right|\geq\left|\frac{n^2 y}{1+n^2 y+n^4 y^2}\right|= \left|\frac{1}{1+1+1 }\right|=\epsilon$$","For , the function is defined by I am trying to prove that the sequence does not converge uniformly to the identically-zero function (the function given by , y). Is the following correct? Set . Consider arbitrary . Choose an arbitrary . Set . Thus","n \in \mathbb{N} f_n:[0, \infty) \rightarrow \mathbb{R} 
f_n(y)=\frac{n^2 y}{1+n y+n^4 y^2}
 \{f_{n}\}_{n\in \mathbb{N}} f f(y)=0 \forall \epsilon=\frac{1}{3} N\in \mathbb{N} n>N y=\frac{1}{n^2} \left|\frac{n^2 y}{1+n y+n^4 y^2}\right|\geq\left|\frac{n^2 y}{1+n^2 y+n^4 y^2}\right|= \left|\frac{1}{1+1+1 }\right|=\epsilon","['real-analysis', 'analysis', 'solution-verification', 'uniform-convergence']"
7,How is this integral form for the remainder for Taylor formula proved?,How is this integral form for the remainder for Taylor formula proved?,,"Let $\varphi \colon \mathbb R^n \to \mathbb R, \varphi \in \mathcal C^2(\mathbb R^n)$ . How is this formula proved? $$ \varphi(x)-\varphi(x_0)=\langle D \varphi\left(x_0\right), x-x_0\rangle + \int_0^1\langle D^2 \varphi\left(x_0+(1-t)\left(x-x_0\right)\right)\left(x-x_0\right), x-x_0\rangle d t $$ Note that in the first order case by the fundamental theorem we have: $$ \varphi(x)-\varphi(x_0)=\int_0^1 \frac{ d\varphi(x_0+t(x-x_0))}{dt} d t= \int_0^1\langle D \varphi\left(x_0+t\left(x-x_0\right)\right), x-x_0\rangle d t $$ How is the other case proved?",Let . How is this formula proved? Note that in the first order case by the fundamental theorem we have: How is the other case proved?,"\varphi \colon \mathbb R^n \to \mathbb R, \varphi \in \mathcal C^2(\mathbb R^n) 
\varphi(x)-\varphi(x_0)=\langle D \varphi\left(x_0\right), x-x_0\rangle + \int_0^1\langle D^2 \varphi\left(x_0+(1-t)\left(x-x_0\right)\right)\left(x-x_0\right), x-x_0\rangle d t
 
\varphi(x)-\varphi(x_0)=\int_0^1 \frac{ d\varphi(x_0+t(x-x_0))}{dt} d t= \int_0^1\langle D \varphi\left(x_0+t\left(x-x_0\right)\right), x-x_0\rangle d t
","['calculus', 'integration', 'analysis', 'derivatives', 'definite-integrals']"
8,Canonical inverse for Laplacian operator in $C^\infty(\mathbb R^n)$,Canonical inverse for Laplacian operator in,C^\infty(\mathbb R^n),"Let $f \in C^\infty_c(\mathbb R^n)$ , we know $$(Af)(x) := (f * \Gamma)(x) - (f * \Gamma)(0)$$ solves the Poisson's equation on $\mathbb R^n$ $$\Delta u = f.$$ where $\Gamma(x)$ is the fundamental solution of the Laplcace equation. This means the operator $A$ defined above is a right inverse to the Laplacian operator. Of course there are many other right inverses, but I think the operator $A$ is somewhat a ""canonical"" right inverse. For general $f\in C^\infty(\mathbb R^n)$ , the expression $f * \Gamma$ does not make sense. However, I believe (at least intuitively) there are still many right inverses to Laplacian operator in this case. And I guess $$(Bf)(x) := \lim_{R\to \infty} \Big((f\chi_{B_R} * \Gamma)(x) -  (f\chi_{B_R} * \Gamma)(0)\Big) $$ is well-defined and $B$ is again a ""canonical"" right inverse to Laplacian. Can I actually prove it? This may be a silly question. So I'm very sorry if it wasted your time.","Let , we know solves the Poisson's equation on where is the fundamental solution of the Laplcace equation. This means the operator defined above is a right inverse to the Laplacian operator. Of course there are many other right inverses, but I think the operator is somewhat a ""canonical"" right inverse. For general , the expression does not make sense. However, I believe (at least intuitively) there are still many right inverses to Laplacian operator in this case. And I guess is well-defined and is again a ""canonical"" right inverse to Laplacian. Can I actually prove it? This may be a silly question. So I'm very sorry if it wasted your time.",f \in C^\infty_c(\mathbb R^n) (Af)(x) := (f * \Gamma)(x) - (f * \Gamma)(0) \mathbb R^n \Delta u = f. \Gamma(x) A A f\in C^\infty(\mathbb R^n) f * \Gamma (Bf)(x) := \lim_{R\to \infty} \Big((f\chi_{B_R} * \Gamma)(x) -  (f\chi_{B_R} * \Gamma)(0)\Big)  B,"['integration', 'analysis', 'partial-differential-equations', 'harmonic-functions', 'laplacian']"
9,Prove or disprove that $u=0$ a.e. on $\Bbb R^d$,Prove or disprove that  a.e. on,u=0 \Bbb R^d,"Let $\Omega\subset\Bbb R^d$ be an open set. Let $k:\Bbb R^d\to [0,\infty)$ be measurable such that $0\in \operatorname{supp}k$ . This implies that $\Omega\subset \Omega_k=\Omega+\operatorname{supp}k$ . Recall that ( see here ) \begin{split} \operatorname{supp} k &= \Bbb R^d\setminus\bigcup\big\{O\,:\, \text{$O$ is open and $k=0$ a.e. on $O$}\big\}\\ &= \bigcap\big\{ \Bbb R^d\setminus O\,:\, \text{$O$ is open and $k=0$ a.e. on $O$}\big\}.  \end{split} Let $u:\Bbb R^d\to \Bbb R$ be measurable. Assume that $k$ is symmetric, i.e. $k(x)=k(-x)$ . $k(x)>0$ for all $x\in \operatorname{supp}k$ . $u=0$ a.e. $\Bbb R^d\setminus\Omega_k$ and $u=0$ a.e. $\Bbb \Omega$ . $$I=\iint\limits_{\Omega \Omega_k}|u(y)|^2k(x-y)d y dx=0.$$ Question: Prove or disprove that $u=0$ a.e. on $\Bbb R^d$ that is, we do have $u=0$ a.e. on $\Omega_k\setminus \Omega$ ? Note that when $\mathrm{supp}\ k=\Bbb R^d$ we easily have $u=0$ a.e. on $\Bbb R^d$ .  Indeed, in this case we get $\Omega_k= \Omega+\Bbb R^d= \Bbb R^d$ and hence $$I=\iint\limits_{\Omega \Bbb R^d}|u(y)|^2k(x-y)d y dx= \iint\limits_{\Omega \Bbb R^d}|u(x+h)|^2k(h)d h dx=0.$$ Then we pick $a\in \Omega$ such that $$\int_{\Bbb R^d}|u(a+h)|^2k(h)d h=0.$$ Since $k>0$ on $\Bbb R^d$ we $u(a+h)=0$ for almost every $h\in \Bbb R^d$ and thus $u=0$ a.e. on $\Bbb R^d$ . General observations Note that $x-y\not\in \mathrm{supp}\ k$ for $x\in \Omega$ and $y\not\in \Omega_k$ . Thus $k(x-y)=0$ for $x\in \Omega$ and $y\not\in \Omega_k$ . This implies $$I=\iint\limits_{\Omega \Omega_k}|u(y)|^2k(x-y)d y dx=\iint\limits_{\Omega \Bbb R^d}|u(y)|^2k(x-y)d y dx\\= \iint\limits_{\Omega \Bbb R^d}|u(x+h)|^2k(h)d h dx= \iint\limits_{\Omega \operatorname{supp} k}|u(x+h)|^2k(h)d h dx$$ PS: A good answer to this question may be awarded a bounty","Let be an open set. Let be measurable such that . This implies that . Recall that ( see here ) Let be measurable. Assume that is symmetric, i.e. . for all . a.e. and a.e. . Question: Prove or disprove that a.e. on that is, we do have a.e. on ? Note that when we easily have a.e. on .  Indeed, in this case we get and hence Then we pick such that Since on we for almost every and thus a.e. on . General observations Note that for and . Thus for and . This implies PS: A good answer to this question may be awarded a bounty","\Omega\subset\Bbb R^d k:\Bbb R^d\to [0,\infty) 0\in \operatorname{supp}k \Omega\subset \Omega_k=\Omega+\operatorname{supp}k \begin{split}
\operatorname{supp} k
&= \Bbb R^d\setminus\bigcup\big\{O\,:\, \text{O is open and k=0 a.e. on O}\big\}\\
&= \bigcap\big\{ \Bbb R^d\setminus O\,:\, \text{O is open and k=0 a.e. on O}\big\}. 
\end{split} u:\Bbb R^d\to \Bbb R k k(x)=k(-x) k(x)>0 x\in \operatorname{supp}k u=0 \Bbb R^d\setminus\Omega_k u=0 \Bbb \Omega I=\iint\limits_{\Omega \Omega_k}|u(y)|^2k(x-y)d y dx=0. u=0 \Bbb R^d u=0 \Omega_k\setminus \Omega \mathrm{supp}\ k=\Bbb R^d u=0 \Bbb R^d \Omega_k= \Omega+\Bbb R^d= \Bbb R^d I=\iint\limits_{\Omega \Bbb R^d}|u(y)|^2k(x-y)d y dx= \iint\limits_{\Omega \Bbb R^d}|u(x+h)|^2k(h)d h dx=0. a\in \Omega \int_{\Bbb R^d}|u(a+h)|^2k(h)d h=0. k>0 \Bbb R^d u(a+h)=0 h\in \Bbb R^d u=0 \Bbb R^d x-y\not\in \mathrm{supp}\ k x\in \Omega y\not\in \Omega_k k(x-y)=0 x\in \Omega y\not\in \Omega_k I=\iint\limits_{\Omega \Omega_k}|u(y)|^2k(x-y)d y dx=\iint\limits_{\Omega \Bbb R^d}|u(y)|^2k(x-y)d y dx\\= \iint\limits_{\Omega \Bbb R^d}|u(x+h)|^2k(h)d h dx= \iint\limits_{\Omega \operatorname{supp} k}|u(x+h)|^2k(h)d h dx","['analysis', 'measure-theory', 'measurable-functions', 'geometric-measure-theory']"
10,Equivalent Cauchy sequences.,Equivalent Cauchy sequences.,,"Hi everyone I'm having a bad time with two questions in the Analysis book of Terry Tao. I finally finished one of the exercises and I'm wondering if the next reasoning is correct or maybe needs some changes: Definitions : Two sequence are equivalence $\iff$ $(\forall \varepsilon \in \mathbb{Q}^+\,) ( \, \exists N\in \mathbb{N}\,) \text{ s.t. }(\, \forall n \ge N\, (|a_n-b_n|\le \varepsilon)\,)  $ (where $\mathbb{Q}^+$ is a positive rational number). Exercise : Show that if $\langle a_n \rangle_{n=1}^{\infty}$ and $\langle b_n \rangle_{n=1}^{\infty}$ are equivalences secuences of rationals. Then  $\langle a_n \rangle_{n=1}^{\infty}$ is a Cauchy sequence if and only if $\langle b_n \rangle_{n=1}^{\infty}$  is a Cauchy sequence. Proof:  We suppose that $\langle a_n \rangle_{n=1}^{\infty}$ is a Cauchy sequence, and  also we may assume that $\langle a_n \rangle_{n=1}^{\infty}$ and $\langle b_n \rangle_{n=1}^{\infty}$ are equivalent sequences; we wish to show that $\langle b_n \rangle_{n=1}^{\infty}$ is a Cauchy sequence. Let $\, \varepsilon $ be  an arbitrary positive integer, we shall show that there is some $N \in \mathbb{N}$ such that $|b_j-b_k| \le \varepsilon$ for all $\,j,k \ge N$. Let $\gamma$ be a positive rational number such that $\gamma < \varepsilon$. Since $\langle a_n \rangle _{n = 1}^{\infty}$ is a Cauchy sequence it follows that there is some $N'\in \mathbb{N}$ such that $|a_{j'}-a_{k'} |\le \varepsilon-\gamma$ for all $j',k' \ge N'$. Now we set $|a_n-b_n|\le \frac{\gamma}{2}\,$  for all $n \ge M$. So either $M>N'$ or $\,M\le N'$ by the ordering of natural numbers. If $M>N'$ we choose $j',k'\ge M$ and then, we have that: $|b_{j'} -b_{k'}|-|a_{j'} -a_{k'}|\le|(a_{j'} -a_{k'})- (b_{j'} -b_{k'})| \le |a_{j'} -b_{j'}|+|a_{k'} -b_{k'}|\le \gamma$ And so we have that $|b_{j'} -b_{k'}|\le \gamma +|a_{j'} -a_{k'}| \le \varepsilon$, it follows that $|b_{j'} -b_{k'}|\le \varepsilon$ for all $j',k'\ge M$ as desired. On the other hand if $\,M \le N'$ we choose a $n\ge N'$ and a similar argument give us that $|b_{j'} -b_{k'}|\le \varepsilon$ for all $j',k'\ge N'$ as desired. So in either case, $|b_{j} -b_{k}|\le \varepsilon\,$ for all $\,j,k \ge N$ and hence the sequence $\langle b_n \rangle_{n=1}^{\infty}$ is a Cauchy sequence. To conclude note that the converse may be disposed with no additional work, by applying the same argument with the roles of $\langle a_n \rangle_{n=1}^{\infty}$ and $\langle b_n \rangle_{n=1}^{\infty}$ interchanged, i.e., where $\langle b_n \rangle_{n=1}^{\infty}$ is a Cauchy sequence. Thanks in advance as usual.","Hi everyone I'm having a bad time with two questions in the Analysis book of Terry Tao. I finally finished one of the exercises and I'm wondering if the next reasoning is correct or maybe needs some changes: Definitions : Two sequence are equivalence $\iff$ $(\forall \varepsilon \in \mathbb{Q}^+\,) ( \, \exists N\in \mathbb{N}\,) \text{ s.t. }(\, \forall n \ge N\, (|a_n-b_n|\le \varepsilon)\,)  $ (where $\mathbb{Q}^+$ is a positive rational number). Exercise : Show that if $\langle a_n \rangle_{n=1}^{\infty}$ and $\langle b_n \rangle_{n=1}^{\infty}$ are equivalences secuences of rationals. Then  $\langle a_n \rangle_{n=1}^{\infty}$ is a Cauchy sequence if and only if $\langle b_n \rangle_{n=1}^{\infty}$  is a Cauchy sequence. Proof:  We suppose that $\langle a_n \rangle_{n=1}^{\infty}$ is a Cauchy sequence, and  also we may assume that $\langle a_n \rangle_{n=1}^{\infty}$ and $\langle b_n \rangle_{n=1}^{\infty}$ are equivalent sequences; we wish to show that $\langle b_n \rangle_{n=1}^{\infty}$ is a Cauchy sequence. Let $\, \varepsilon $ be  an arbitrary positive integer, we shall show that there is some $N \in \mathbb{N}$ such that $|b_j-b_k| \le \varepsilon$ for all $\,j,k \ge N$. Let $\gamma$ be a positive rational number such that $\gamma < \varepsilon$. Since $\langle a_n \rangle _{n = 1}^{\infty}$ is a Cauchy sequence it follows that there is some $N'\in \mathbb{N}$ such that $|a_{j'}-a_{k'} |\le \varepsilon-\gamma$ for all $j',k' \ge N'$. Now we set $|a_n-b_n|\le \frac{\gamma}{2}\,$  for all $n \ge M$. So either $M>N'$ or $\,M\le N'$ by the ordering of natural numbers. If $M>N'$ we choose $j',k'\ge M$ and then, we have that: $|b_{j'} -b_{k'}|-|a_{j'} -a_{k'}|\le|(a_{j'} -a_{k'})- (b_{j'} -b_{k'})| \le |a_{j'} -b_{j'}|+|a_{k'} -b_{k'}|\le \gamma$ And so we have that $|b_{j'} -b_{k'}|\le \gamma +|a_{j'} -a_{k'}| \le \varepsilon$, it follows that $|b_{j'} -b_{k'}|\le \varepsilon$ for all $j',k'\ge M$ as desired. On the other hand if $\,M \le N'$ we choose a $n\ge N'$ and a similar argument give us that $|b_{j'} -b_{k'}|\le \varepsilon$ for all $j',k'\ge N'$ as desired. So in either case, $|b_{j} -b_{k}|\le \varepsilon\,$ for all $\,j,k \ge N$ and hence the sequence $\langle b_n \rangle_{n=1}^{\infty}$ is a Cauchy sequence. To conclude note that the converse may be disposed with no additional work, by applying the same argument with the roles of $\langle a_n \rangle_{n=1}^{\infty}$ and $\langle b_n \rangle_{n=1}^{\infty}$ interchanged, i.e., where $\langle b_n \rangle_{n=1}^{\infty}$ is a Cauchy sequence. Thanks in advance as usual.",,"['calculus', 'real-analysis', 'self-learning', 'proof-verification', 'cauchy-sequences']"
11,Generalization of Taylor series,Generalization of Taylor series,,"I noticed the similarity between Taylor series and Newton series: $$\sum\limits_{n \geq 0} \frac{D^nf(0)}{n!} x^n $$ $$\sum\limits_{n \geq 0} \frac{\Delta^nf(0)}{n!} x^{\underline{n}} $$ where $Df = df/dx$ , $\Delta f(x) = f(x+1)-f(x)$ , $x^\underline{n} = x(x-1)\ldots(x-n+1)$ . The natural question that comes to mind is that we can generalize that for a more general linear operator $A$ and a family of functions $\{ p_n \}_{n\geq 0}$ which satisfies properties $$  p_0 (x) = 1 $$ $$ p_n(0)= 0, \ n \geq 1$$ $$ A p_n = n p_{n-1} $$ $$ A1 = 0$$ So we will have the series $$\sum\limits_{n \geq 0} \frac{A^nf(0)}{n!} p_n (x) $$ Has it been studied already? Which conditions should satisfy $A$ , $\{ p_n \}_{n\geq 0}$ so the series converges to $f$ ? What are the other examples of such series?","I noticed the similarity between Taylor series and Newton series: where , , . The natural question that comes to mind is that we can generalize that for a more general linear operator and a family of functions which satisfies properties So we will have the series Has it been studied already? Which conditions should satisfy , so the series converges to ? What are the other examples of such series?","\sum\limits_{n \geq 0} \frac{D^nf(0)}{n!} x^n  \sum\limits_{n \geq 0} \frac{\Delta^nf(0)}{n!} x^{\underline{n}}  Df = df/dx \Delta f(x) = f(x+1)-f(x) x^\underline{n} = x(x-1)\ldots(x-n+1) A \{ p_n \}_{n\geq 0}   p_0 (x) = 1   p_n(0)= 0, \ n \geq 1  A p_n = n p_{n-1}   A1 = 0 \sum\limits_{n \geq 0} \frac{A^nf(0)}{n!} p_n (x)  A \{ p_n \}_{n\geq 0} f","['calculus', 'sequences-and-series', 'analysis', 'taylor-expansion', 'newton-series']"
12,A function on $\mathbb R^4$ descends to a function on $RP^3$,A function on  descends to a function on,\mathbb R^4 RP^3,"Consider the smooth function $f:\mathbb R^4\to \mathbb R; (x,y,z,t)\mapsto 4x^2+3y^2+3z^2+t^2.$ Show that $f$ descends to a smooth function $g\colon RP^3\to \mathbb R$ such that $g\circ \pi = f$ . Here $\pi:\mathbb R^4-\{0\}\to RP^3$ is the canonical projection. Find the critical values of $g$ . I am having difficulty understanding this question. If $g$ is well-defined on $RP^3$ then for $p:=(x,y,z,t)\in \mathbb R^4-\{0\}$ and $\alpha\in \mathbb R-\{0\}$ , we should have $$f(p)=g\circ \pi(p)=g\circ\pi(\alpha p)=f(\alpha p)=\alpha^2 f(p).$$ But this cannot hold for any non-zero $\alpha$ .","Consider the smooth function Show that descends to a smooth function such that . Here is the canonical projection. Find the critical values of . I am having difficulty understanding this question. If is well-defined on then for and , we should have But this cannot hold for any non-zero .","f:\mathbb R^4\to \mathbb R; (x,y,z,t)\mapsto 4x^2+3y^2+3z^2+t^2. f g\colon RP^3\to \mathbb R g\circ \pi = f \pi:\mathbb R^4-\{0\}\to RP^3 g g RP^3 p:=(x,y,z,t)\in \mathbb R^4-\{0\} \alpha\in \mathbb R-\{0\} f(p)=g\circ \pi(p)=g\circ\pi(\alpha p)=f(\alpha p)=\alpha^2 f(p). \alpha","['analysis', 'differential-geometry', 'smooth-manifolds']"
13,Understanding compatibility of PDE,Understanding compatibility of PDE,,"THEOREM $1$ : The equations $$f(x, y, z, p, q)=0\qquad (1)$$ and $$g(x, y, z, p, q)=0\qquad(2)$$ are compatible on a domain $D$ if (i) $J=\frac{\partial(f, g)}{\partial(p, q)}=\left|\begin{array}{ll}f_{p} & f_{q} \\ g_{p} & g_{q}\end{array}\right| \neq 0$ on $D .$ (ii) $p$ and $q$ can be explicitly solved from $(1)$ and $(2)$ as $p=\phi(x, y, z)$ and $q=\psi(x, y, z)$ . Further, the equation $$ d z=\phi(x, y, z) d x+\psi(x, y, z) d y $$ is integrable. THEOREM $2$ : A necessary and sufficient condition for the integrability of the equation $d z=\phi(x, y, z) d x+\psi(x, y, z) d y$ is $$ [f, g] \equiv \frac{\partial(f, g)}{\partial(x, p)}+\frac{\partial(f, g)}{\partial(y, q)}+p \frac{\partial(f, g)}{\partial(z, p)}+q \frac{\partial(f, g)}{\partial(z, q)}=0\qquad(3) $$ In other words, the equations (1) and (2) are compatible iff (3) holds. In my lecture note, these two theorems are mentioned without any proof. But I am wondering how the Jacobian tell me the system are compatible? And how the expression $[f, g]$ guarantee the integrability of the equation $d z=\phi(x, y, z) d x+\psi(x, y, z) d y$ ? The definition of compatibility they followed are, A system of two first-order PDEs are said to be compatible if they have a common solution I am more interested to get the answer about their intuition (or connection) rather a complete proof (which may be beyond my scope of the syllabus). Thanks in advance.","THEOREM : The equations and are compatible on a domain if (i) on (ii) and can be explicitly solved from and as and . Further, the equation is integrable. THEOREM : A necessary and sufficient condition for the integrability of the equation is In other words, the equations (1) and (2) are compatible iff (3) holds. In my lecture note, these two theorems are mentioned without any proof. But I am wondering how the Jacobian tell me the system are compatible? And how the expression guarantee the integrability of the equation ? The definition of compatibility they followed are, A system of two first-order PDEs are said to be compatible if they have a common solution I am more interested to get the answer about their intuition (or connection) rather a complete proof (which may be beyond my scope of the syllabus). Thanks in advance.","1 f(x, y, z, p, q)=0\qquad (1) g(x, y, z, p, q)=0\qquad(2) D J=\frac{\partial(f, g)}{\partial(p, q)}=\left|\begin{array}{ll}f_{p} & f_{q} \\ g_{p} & g_{q}\end{array}\right| \neq 0 D . p q (1) (2) p=\phi(x, y, z) q=\psi(x, y, z) 
d z=\phi(x, y, z) d x+\psi(x, y, z) d y
 2 d z=\phi(x, y, z) d x+\psi(x, y, z) d y 
[f, g] \equiv \frac{\partial(f, g)}{\partial(x, p)}+\frac{\partial(f, g)}{\partial(y, q)}+p \frac{\partial(f, g)}{\partial(z, p)}+q \frac{\partial(f, g)}{\partial(z, q)}=0\qquad(3)
 [f, g] d z=\phi(x, y, z) d x+\psi(x, y, z) d y","['analysis', 'partial-differential-equations', 'soft-question', 'intuition']"
14,"$\sqrt{f}$ has bounded tangential derivatives if $f\in C^{1,1}$?",has bounded tangential derivatives if ?,"\sqrt{f} f\in C^{1,1}","This question comes from the paper written by Guan, Trudinger, and Wang , which can be stated precisely as follows. Let $n\geq2$ be an integer and $\Omega\subset\mathbf R^n$ be a bounded domain with smooth boundary. Assume $\Omega$ is uniformly convex if necessary. Given $f\in C^{1,1}(\bar\Omega)$ such that $$f>0\quad\hbox{in $\Omega$.}$$ Does there exist a constant $C>0,$ which is at most dependent on $n,~\Omega,$ and $\|f\|_{C^{1,1}(\bar\Omega)},$ such that \begin{equation}|\nabla  f(x)\cdot\tau(x)|^2\leq Cf(x)\quad\hbox{for all $x\in\partial\Omega$},\label{1}\tag{1}\end{equation} where $\tau(x)$ is a unit tangent vector of $\partial\Omega$ at $x.$ In other words, does the tangential derivative $\partial_{\tau}\sqrt f$ have a uniform upper bound? Actually, authors used in above reference that an incorrect inequality as \begin{equation*}|\nabla f|^2\leq Cf\quad\hbox{in $\Omega.$}\end{equation*} Near the boundary, it is clear that the distance function $d(x)=\mathrm{dist}(x,\partial\Omega)$ is a counterexample. It seems that all consequences still hold if \eqref{1} is true. If $f(x_0)=0$ for some boundary point $x_0,$ then obviously we can take above $C=1$ at $x_0.$ The difficulty is to control the tangential derivative of $\sqrt f$ if $f(x)>0$ but it is very small.","This question comes from the paper written by Guan, Trudinger, and Wang , which can be stated precisely as follows. Let be an integer and be a bounded domain with smooth boundary. Assume is uniformly convex if necessary. Given such that Does there exist a constant which is at most dependent on and such that where is a unit tangent vector of at In other words, does the tangential derivative have a uniform upper bound? Actually, authors used in above reference that an incorrect inequality as Near the boundary, it is clear that the distance function is a counterexample. It seems that all consequences still hold if \eqref{1} is true. If for some boundary point then obviously we can take above at The difficulty is to control the tangential derivative of if but it is very small.","n\geq2 \Omega\subset\mathbf R^n \Omega f\in C^{1,1}(\bar\Omega) f>0\quad\hbox{in \Omega.} C>0, n,~\Omega, \|f\|_{C^{1,1}(\bar\Omega)}, \begin{equation}|\nabla
 f(x)\cdot\tau(x)|^2\leq Cf(x)\quad\hbox{for all x\in\partial\Omega},\label{1}\tag{1}\end{equation} \tau(x) \partial\Omega x. \partial_{\tau}\sqrt f \begin{equation*}|\nabla f|^2\leq Cf\quad\hbox{in \Omega.}\end{equation*} d(x)=\mathrm{dist}(x,\partial\Omega) f(x_0)=0 x_0, C=1 x_0. \sqrt f f(x)>0","['real-analysis', 'analysis', 'partial-differential-equations']"
15,"if $f,\phi$ are continuous such that $\lim_{x\to\infty}\phi(x)-x=\infty, \ \phi(x)=x$ finitely many times, and $f\circ \phi=f$, show $f$ is constant","if  are continuous such that  finitely many times, and , show  is constant","f,\phi \lim_{x\to\infty}\phi(x)-x=\infty, \ \phi(x)=x f\circ \phi=f f","Question: Let $\phi:\mathbb{R}\to\mathbb{R}$ be continuous, satisfying: $\lim\limits_{x\to+\infty}(\phi(x)-x)=+\infty$ ; $\{x\in\mathbb{R}|\phi(x)=x\}$ is a non-empty,finite set. Prove that if $f:\mathbb{R}\to\mathbb{R}$ is continuous and $f\circ\phi=f$ ,then $f$ is constant. Attempt: Set $x_0=\sup\{x\in\mathbb{R}|\phi(x)=x\}$ ,then $\phi(x_0)=x_0,\forall x>x_0,\phi(x)>x$ . Let $f(x_0)=c.$ Suppose there exists $x_1>x_0$ , such that $f(x_1)=d\neq c$ . Since $f\circ\phi=f$ , we have by induction $f\circ\phi^n=f$ . Set $x_{n}=\phi^{n}(x_1)$ , then $$x_{n+1}=\phi(x_n)>x_n,$$ $$x_{n+1}-x_n=\phi(x_n)-x_n\implies x_n\to+\infty,x_{n+1}-x_n\to+\infty,$$ so $$\{f(x_n)=f(x_1)=d\}_{n=1}^{+\infty}$$ is a subsequence of $\{f(x)\}$ goes to $d$ when $x\to+\infty$ . Since $f$ is continuous,this is true for any value between $c,d$ (domain in $(x_0,x_1)$ ). I wonder if this could lead to a contradiction.","Question: Let be continuous, satisfying: ; is a non-empty,finite set. Prove that if is continuous and ,then is constant. Attempt: Set ,then . Let Suppose there exists , such that . Since , we have by induction . Set , then so is a subsequence of goes to when . Since is continuous,this is true for any value between (domain in ). I wonder if this could lead to a contradiction.","\phi:\mathbb{R}\to\mathbb{R} \lim\limits_{x\to+\infty}(\phi(x)-x)=+\infty \{x\in\mathbb{R}|\phi(x)=x\} f:\mathbb{R}\to\mathbb{R} f\circ\phi=f f x_0=\sup\{x\in\mathbb{R}|\phi(x)=x\} \phi(x_0)=x_0,\forall x>x_0,\phi(x)>x f(x_0)=c. x_1>x_0 f(x_1)=d\neq c f\circ\phi=f f\circ\phi^n=f x_{n}=\phi^{n}(x_1) x_{n+1}=\phi(x_n)>x_n, x_{n+1}-x_n=\phi(x_n)-x_n\implies x_n\to+\infty,x_{n+1}-x_n\to+\infty, \{f(x_n)=f(x_1)=d\}_{n=1}^{+\infty} \{f(x)\} d x\to+\infty f c,d (x_0,x_1)","['real-analysis', 'analysis', 'functions']"
16,Why is the Lebesgue integral linear when only one summand is integrable?,Why is the Lebesgue integral linear when only one summand is integrable?,,"Let $(X, A, \mu)$ be a measure space, $(f_n)_n$ measurable for all $n$ , and $f$ measurable, non-negative and integrable such that $f_n \le f$ for all $n$ . Then $\int _X \limsup_{n\to \infty} f_n d\mu \ge \limsup _{n\to \infty} \int _X f_n d\mu$ . In the proof I saw for this statement, we define $h_n = f - f_n$ . We have that $$\int _X \liminf _{n\to \infty} h_n d\mu= \int _X \liminf _{n\to \infty} (f - f_n) d\mu = \int _X (f + \liminf _{n\to \infty} (-f_n)) d\mu = \int _X f d\mu + \int _X \liminf _{n\to \infty} (-f_n) d\mu$$ The explanation for the last equality is that $f$ is integrable. However, to use linearity we need all summands to be integrable. Why can we use it here? Thanks.","Let be a measure space, measurable for all , and measurable, non-negative and integrable such that for all . Then . In the proof I saw for this statement, we define . We have that The explanation for the last equality is that is integrable. However, to use linearity we need all summands to be integrable. Why can we use it here? Thanks.","(X, A, \mu) (f_n)_n n f f_n \le f n \int _X \limsup_{n\to \infty} f_n d\mu \ge \limsup _{n\to \infty} \int _X f_n d\mu h_n = f - f_n \int _X \liminf _{n\to \infty} h_n d\mu= \int _X \liminf _{n\to \infty} (f - f_n) d\mu = \int _X (f + \liminf _{n\to \infty} (-f_n)) d\mu = \int _X f d\mu + \int _X \liminf _{n\to \infty} (-f_n) d\mu f","['integration', 'analysis', 'measure-theory']"
17,"Is $f(x) = 1/x$ over $[0.1 , 1]$ uniformly continuous but $1/x$ on $(0,1)$ is not?",Is  over  uniformly continuous but  on  is not?,"f(x) = 1/x [0.1 , 1] 1/x (0,1)","I know that there is a theorem that says that ""Every continuous function on a bounded closed interval $[a,b]$ is uniformly continuous therein."" But I know that the function $f(x) = 1/x$ defined on (0,1) is not uniformly continuous (I know how to prove this). Does the previous theorem is saying that $f(x) = 1/x$ defined on [0.1,1] is uniformly continuous? why? what should I change in my proof of being not uniform continuous. Here is my proof of not being uniformly continuous: Let $\delta > 0,$ take $\epsilon = 1$ and $x = \min \{1, \delta\}$ and $a = \frac{x}{2}.$ then $|x - a | = |x - x/2| = \frac{x}{2} < \delta$ but $|\frac{1}{a} - \frac{1}{x}| = |\frac{2}{x} - \frac{1}{x}| = \frac{1}{x} > 1.$ And so $f$ is not uniformly continuous. Also, my justification that it is continuous because it is the division of 2 polynomials (and in this justification I do not see any use of open or closed intervals) Could anyone help me in answering my questions, please? Thanks in advance!","I know that there is a theorem that says that ""Every continuous function on a bounded closed interval is uniformly continuous therein."" But I know that the function defined on (0,1) is not uniformly continuous (I know how to prove this). Does the previous theorem is saying that defined on [0.1,1] is uniformly continuous? why? what should I change in my proof of being not uniform continuous. Here is my proof of not being uniformly continuous: Let take and and then but And so is not uniformly continuous. Also, my justification that it is continuous because it is the division of 2 polynomials (and in this justification I do not see any use of open or closed intervals) Could anyone help me in answering my questions, please? Thanks in advance!","[a,b] f(x) = 1/x f(x) = 1/x \delta > 0, \epsilon = 1 x = \min \{1, \delta\} a = \frac{x}{2}. |x - a | = |x - x/2| = \frac{x}{2} < \delta |\frac{1}{a} - \frac{1}{x}| = |\frac{2}{x} - \frac{1}{x}| = \frac{1}{x} > 1. f","['real-analysis', 'analysis']"
18,Understanding Analysis by Abott (Prerequisites) [closed],Understanding Analysis by Abott (Prerequisites) [closed],,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 6 months ago . Improve this question I just bought „Understanding Analysis“ by Stephan Abott. I am a CS Student who already passed his math lectures in Germany but I want a better understanding since I memorized a lot. I want to build a deep understanding and intuition. My goal is to self study math. „Understanding Analysis“ is a introduction to rigerous analysis, so basically proof based calculus right? So would a calculus book before working through „Understanding Analysis“ beneficial to get an intuition? Or does „Understanding Analysis“ cover the same stuff as an Calculus book?","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 6 months ago . Improve this question I just bought „Understanding Analysis“ by Stephan Abott. I am a CS Student who already passed his math lectures in Germany but I want a better understanding since I memorized a lot. I want to build a deep understanding and intuition. My goal is to self study math. „Understanding Analysis“ is a introduction to rigerous analysis, so basically proof based calculus right? So would a calculus book before working through „Understanding Analysis“ beneficial to get an intuition? Or does „Understanding Analysis“ cover the same stuff as an Calculus book?",,"['real-analysis', 'calculus', 'analysis', 'intuition']"
19,Asymptotic behaviour of a shifted sinus function,Asymptotic behaviour of a shifted sinus function,,"Consider the function $f(x)=\cos(x)-\cos((1+t)x)$ for $t>0$ on $x\in\mathbb{R}$ . I would like to show the following claim which is obvious from looking at the graph of the function. Claim. If $0<t<1$ and $\varepsilon>0$ then there is $x\in\mathbb{R}$ with $|f(x)|>2-\varepsilon$ . What I tried: First I tried to characterise the local maxima, by looking at $f'(x)=(1+t)\sin((1+t)x)-\sin(x)$ . However the equation we obtain by setting $f'(x)=0$ is probably not solvable. Or does somebody know a trick to solve $$ \frac{\sin(x)}{\sin((1+t)x)}=1+t? $$ Second I tried to find an $x\in\mathbb{R}$ that is close to $2k\pi$ and for which $(1+t)x$ is close to $(2l+1)\pi$ for $k,l\in\mathbb{N}$ . However when I do this I run into an issue I cannot solve. For example: Set $x=\frac{(2l+1)\pi}{1+t}$ for $l$ to be determined. Since the rationals are dense in the continuum we can certainly find $m,n\in\mathbb{R}$ so that $\frac{m}{n}$ is close to $\frac{1}{1+t}$ . Perhaps we can also choose $m=2k$ to be even and $n=2l+1$ to be odd. Hence for all $\varepsilon>0$ we have $$ x=\frac{n\pi}{1+t}=(\frac{1}{1+t}-\frac{m}{n})n\pi+m\pi\leq\varepsilon n\pi+m\pi. $$ It seems that no matter what I do, the $\varepsilon$ will be in a product with $n$ , however $n$ depends on $\varepsilon$ and thus my proof does not show that $x$ is close to $m\pi$ . How to get rid of this problem? Context. The goal is to show that the semigroup $T_tu(x)=u((1+t)x)$ for $x\in\mathbb{R},t\geq0,u\in C_b(\mathbb{R})$ is not strongly continuous. Obviously any periodic function will do the job, so if you suggest I should better look at a simpler version of $cos$ like the sharktooth-function, then let me know. Edit: So something to note is that these operators do not form a semigroup. In order to solve it, take $T_tu(x)=u(e^t x)$ instead. By continuity of the exponential the calculation will still work.","Consider the function for on . I would like to show the following claim which is obvious from looking at the graph of the function. Claim. If and then there is with . What I tried: First I tried to characterise the local maxima, by looking at . However the equation we obtain by setting is probably not solvable. Or does somebody know a trick to solve Second I tried to find an that is close to and for which is close to for . However when I do this I run into an issue I cannot solve. For example: Set for to be determined. Since the rationals are dense in the continuum we can certainly find so that is close to . Perhaps we can also choose to be even and to be odd. Hence for all we have It seems that no matter what I do, the will be in a product with , however depends on and thus my proof does not show that is close to . How to get rid of this problem? Context. The goal is to show that the semigroup for is not strongly continuous. Obviously any periodic function will do the job, so if you suggest I should better look at a simpler version of like the sharktooth-function, then let me know. Edit: So something to note is that these operators do not form a semigroup. In order to solve it, take instead. By continuity of the exponential the calculation will still work.","f(x)=\cos(x)-\cos((1+t)x) t>0 x\in\mathbb{R} 0<t<1 \varepsilon>0 x\in\mathbb{R} |f(x)|>2-\varepsilon f'(x)=(1+t)\sin((1+t)x)-\sin(x) f'(x)=0 
\frac{\sin(x)}{\sin((1+t)x)}=1+t?
 x\in\mathbb{R} 2k\pi (1+t)x (2l+1)\pi k,l\in\mathbb{N} x=\frac{(2l+1)\pi}{1+t} l m,n\in\mathbb{R} \frac{m}{n} \frac{1}{1+t} m=2k n=2l+1 \varepsilon>0 
x=\frac{n\pi}{1+t}=(\frac{1}{1+t}-\frac{m}{n})n\pi+m\pi\leq\varepsilon n\pi+m\pi.
 \varepsilon n n \varepsilon x m\pi T_tu(x)=u((1+t)x) x\in\mathbb{R},t\geq0,u\in C_b(\mathbb{R}) cos T_tu(x)=u(e^t x)","['real-analysis', 'analysis', 'semigroup-of-operators']"
20,"Problem $u_t - u_{xx} = f(x)$ in $\mathbb{R} \times (0,+\infty)$ with $f$ given",Problem  in  with  given,"u_t - u_{xx} = f(x) \mathbb{R} \times (0,+\infty) f","Consider the following problem: $$\left\{\begin{array}{rcl} u_t - u_{xx} & = & f(x), \ \ \mathbb{R} \times (0,+\infty)\\ u(x,0) & = & e^{-|x|}, \ \ \mathbb{R} \end{array}\right.$$ with $$f(x) = \left\{\begin{array}{cc} e^{-x}, & x > 0\\  0, & x \leq 0.  \end{array}\right.$$ Note that $$\mathcal{F}(u_t) = \mathcal{F}(u_{xx} + f(x)) = -\xi^2\mathcal{F}(u) + \mathcal{F}(f).$$ In addition, we also have $$\mathcal{F}(f) = \dfrac{1-i\xi }{1 + \xi^2}.$$ So, we will analyze the problem: $$\left\{\begin{array}{rcl} \widehat{u_t}(\xi,t) & = & -\xi^2\widehat{u}(\xi,t) + \dfrac{1-i\xi }{1 + \xi^2}\\ \widehat{u}(\xi,0) & = & \dfrac{\sqrt{2/\pi}}{\xi^2 + 1}, \end{array}\right.$$ because $$\mathcal{F}(e^{-|x|}) = \dfrac{\sqrt{2/\pi}}{\xi^2 + 1}.$$ Now, for every $\xi \in \mathbb{R}$ , we can explicitly solve the ODE, getting: $$\widehat{u}(\xi,t) = Ce^{-\xi^2t} + \dfrac{1-i\xi}{\xi^2(1 + \xi^2)},$$ where $C$ is a constant. From the initial condition, it follows that $$C = \dfrac{\sqrt{2/\pi}}{\xi^2 + 1} - \dfrac{1-i\xi}{\xi^2(1 + \xi^2)}.$$ So it follows that \begin{eqnarray} \widehat{u}(\xi,t) & = & \dfrac{\sqrt{2/\pi}}{\xi^2 + 1}e^{-\xi^2t} - \dfrac{1-i\xi}{\xi^2(1 + \xi^2)}e^{-\xi^2t} + \dfrac{1-i\xi}{\xi^2(1 + \xi^2)}\\  & = & \dfrac{\sqrt{2/\pi}}{\xi^2 + 1}e^{-\xi^2t} + \dfrac{1-i\xi}{\xi^2(1 + \xi^2)}[1-e^{-\xi^2t}]. \end{eqnarray} Doubt: How do I return the $u$ function? I tried to calculate the inverse Fourier transform but I couldn't.","Consider the following problem: with Note that In addition, we also have So, we will analyze the problem: because Now, for every , we can explicitly solve the ODE, getting: where is a constant. From the initial condition, it follows that So it follows that Doubt: How do I return the function? I tried to calculate the inverse Fourier transform but I couldn't.","\left\{\begin{array}{rcl}
u_t - u_{xx} & = & f(x), \ \ \mathbb{R} \times (0,+\infty)\\
u(x,0) & = & e^{-|x|}, \ \ \mathbb{R}
\end{array}\right. f(x) = \left\{\begin{array}{cc}
e^{-x}, & x > 0\\ 
0, & x \leq 0. 
\end{array}\right. \mathcal{F}(u_t) = \mathcal{F}(u_{xx} + f(x)) = -\xi^2\mathcal{F}(u) + \mathcal{F}(f). \mathcal{F}(f) = \dfrac{1-i\xi }{1 + \xi^2}. \left\{\begin{array}{rcl}
\widehat{u_t}(\xi,t) & = & -\xi^2\widehat{u}(\xi,t) + \dfrac{1-i\xi }{1 + \xi^2}\\
\widehat{u}(\xi,0) & = & \dfrac{\sqrt{2/\pi}}{\xi^2 + 1},
\end{array}\right. \mathcal{F}(e^{-|x|}) = \dfrac{\sqrt{2/\pi}}{\xi^2 + 1}. \xi \in \mathbb{R} \widehat{u}(\xi,t) = Ce^{-\xi^2t} + \dfrac{1-i\xi}{\xi^2(1 + \xi^2)}, C C = \dfrac{\sqrt{2/\pi}}{\xi^2 + 1} - \dfrac{1-i\xi}{\xi^2(1 + \xi^2)}. \begin{eqnarray}
\widehat{u}(\xi,t) & = & \dfrac{\sqrt{2/\pi}}{\xi^2 + 1}e^{-\xi^2t} - \dfrac{1-i\xi}{\xi^2(1 + \xi^2)}e^{-\xi^2t} + \dfrac{1-i\xi}{\xi^2(1 + \xi^2)}\\ 
& = & \dfrac{\sqrt{2/\pi}}{\xi^2 + 1}e^{-\xi^2t} + \dfrac{1-i\xi}{\xi^2(1 + \xi^2)}[1-e^{-\xi^2t}].
\end{eqnarray} u","['analysis', 'partial-differential-equations', 'fourier-transform', 'heat-equation']"
21,Doubt in one step of the proof.,Doubt in one step of the proof.,,"I'm dubious in step one proof. The reference I'm following claims to use the Implicit Function Theorem, but I haven't seen where it was used. Lagrange Multipliers Theorem: Let $f:U\to\mathbb{R}$ a differential function in a open $U \subset \mathbb{R}^{m+n}$ and $M = \varphi^{-1}(c)$ a orietable surface in $U$ , inverse image of regular value $c \in \mathbb{R}^n$ by a aplication $\varphi:U \to \mathbb{R}^n$ , of class $C^1$ . A point $p \in M$ is critic of restriction $f|M$ iff there exists $\lambda_1, \ldots, \lambda_n$ such that $$\nabla f(p) = \lambda_1 \cdot \nabla \varphi_1 (p) + \ldots + \lambda_n \cdot \nabla \varphi_n(p),$$ where $\varphi_1, \ldots, \varphi_n: U \to \mathbb{R}$ are the coordinates of $\varphi$ . Proof: Assume the notation of the statement. Since $c$ is a regular value of $\varphi$ , then for each $x \in M$ , the vectors $\nabla \varphi_1 (x) ,\ldots, \nabla \varphi_n(x)$ are linearly independent and so this form a basis of vector space normal in $M$ in the point $x$ , denoted by $(T_xM)^{\bot}$ . In this case, if $f:U \to \mathbb{R}$ a differentiable function, then $p \in M$ is a critic point of the $f|M$ iff the vector $\nabla f(p)$ is a linear combination of vectors $\varphi_i (p)$ , ie,  there exists $\lambda_1, \ldots, \lambda_n$ such that $$\nabla f(p) = \lambda_1 \cdot \nabla \varphi_1 (p) + \ldots + \lambda_n \cdot \nabla \varphi_n(p).$$ I can't figure out where he used the implicit function theorem, I believe it's to show that those vectors form the basis, but I'm not convinced of that. I looked at the case in one variable, it seems to be in that step. Thank you in advance for the tips. Next I did a proof trying to explain where the theorem was used. Proof : If $x \in M$ , then by $M = \varphi^{-1}(c)$ , it follows that $\varphi(x) = c$ and so, since $c$ is a regular value of $\varphi=(\varphi_1, \ldots, \varphi_n):U \to \mathbb{R}^n$ , then $\nabla \varphi_i(x) \neq 0$ , for all $i = 1 ,\ldots, n$ . Thus, by the implicit function theorem, there exists a open $Z \subset U$ with $x \in Z$ such that $\varphi^{-1}(c) \cap Z$ is the graphic of function $\xi:V \to \mathbb{R}^n$ , o class $C^k$ in a open $V \subset \mathbb{R}^n$ . Thus, considering the open $V$ , we have that the set $\{\nabla \varphi_1 (x), \ldots, \nabla \varphi_n(x)\}$ is  linearly independent and so this form a basis of vector space normal in $M$ in the point $x$ , denoted by $(T_xM)^{\bot}$ . In this case, if $f:U \to \mathbb{R}$ a differentiable function, then $p \in M$ is a critic point of the $f|M$ iff the vector $\nabla f(p)$ is a linear combination of vectors $\varphi_i (p)$ , ie,  there exists $\lambda_1, \ldots, \lambda_n$ such that $$\nabla f(p) = \lambda_1 \cdot \nabla \varphi_1 (p) + \ldots + \lambda_n \cdot \nabla \varphi_n(p).$$ Is this second version correct? I'm not convinced it's right. In short, my question is: at what point is the Implicit Function Theorem being used. Taking the question, I would like to ask what would be another interesting application of this theorem in the context of analysis in $\mathbb{R}^n$ .","I'm dubious in step one proof. The reference I'm following claims to use the Implicit Function Theorem, but I haven't seen where it was used. Lagrange Multipliers Theorem: Let a differential function in a open and a orietable surface in , inverse image of regular value by a aplication , of class . A point is critic of restriction iff there exists such that where are the coordinates of . Proof: Assume the notation of the statement. Since is a regular value of , then for each , the vectors are linearly independent and so this form a basis of vector space normal in in the point , denoted by . In this case, if a differentiable function, then is a critic point of the iff the vector is a linear combination of vectors , ie,  there exists such that I can't figure out where he used the implicit function theorem, I believe it's to show that those vectors form the basis, but I'm not convinced of that. I looked at the case in one variable, it seems to be in that step. Thank you in advance for the tips. Next I did a proof trying to explain where the theorem was used. Proof : If , then by , it follows that and so, since is a regular value of , then , for all . Thus, by the implicit function theorem, there exists a open with such that is the graphic of function , o class in a open . Thus, considering the open , we have that the set is  linearly independent and so this form a basis of vector space normal in in the point , denoted by . In this case, if a differentiable function, then is a critic point of the iff the vector is a linear combination of vectors , ie,  there exists such that Is this second version correct? I'm not convinced it's right. In short, my question is: at what point is the Implicit Function Theorem being used. Taking the question, I would like to ask what would be another interesting application of this theorem in the context of analysis in .","f:U\to\mathbb{R} U \subset \mathbb{R}^{m+n} M = \varphi^{-1}(c) U c \in \mathbb{R}^n \varphi:U \to \mathbb{R}^n C^1 p \in M f|M \lambda_1, \ldots, \lambda_n \nabla f(p) = \lambda_1 \cdot \nabla \varphi_1 (p) + \ldots + \lambda_n \cdot \nabla \varphi_n(p), \varphi_1, \ldots, \varphi_n: U \to \mathbb{R} \varphi c \varphi x \in M \nabla \varphi_1 (x) ,\ldots, \nabla \varphi_n(x) M x (T_xM)^{\bot} f:U \to \mathbb{R} p \in M f|M \nabla f(p) \varphi_i (p) \lambda_1, \ldots, \lambda_n \nabla f(p) = \lambda_1 \cdot \nabla \varphi_1 (p) + \ldots + \lambda_n \cdot \nabla \varphi_n(p). x \in M M = \varphi^{-1}(c) \varphi(x) = c c \varphi=(\varphi_1, \ldots, \varphi_n):U \to \mathbb{R}^n \nabla \varphi_i(x) \neq 0 i = 1 ,\ldots, n Z \subset U x \in Z \varphi^{-1}(c) \cap Z \xi:V \to \mathbb{R}^n C^k V \subset \mathbb{R}^n V \{\nabla \varphi_1 (x), \ldots, \nabla \varphi_n(x)\} M x (T_xM)^{\bot} f:U \to \mathbb{R} p \in M f|M \nabla f(p) \varphi_i (p) \lambda_1, \ldots, \lambda_n \nabla f(p) = \lambda_1 \cdot \nabla \varphi_1 (p) + \ldots + \lambda_n \cdot \nabla \varphi_n(p). \mathbb{R}^n","['calculus', 'analysis', 'solution-verification', 'lagrange-multiplier', 'implicit-function-theorem']"
22,Interchange integration and summation for function $f_n(x)=\frac{e^{-nx}}{n}\ln{x}$,Interchange integration and summation for function,f_n(x)=\frac{e^{-nx}}{n}\ln{x},"My question is, for $$ f_n(x)=\frac{e^{-nx}}{n}\ln{x}, $$ how to prove that $$ \int_0^\infty\left[\sum_{n=1}^\infty f_n(x)\right]\mathrm{d}x =\sum_{n=1}^\infty \left[\int_0^\infty f_n(x)\,\mathrm{d}x\right] $$ With the theorems descried here , for $x\in[1,\infty)$ , I am able to find a dominant function $$ g_n(x)=\frac{1}{n^2}\frac{\ln{x}}{1+x^2} $$ so that $0\leq|f_n(x)|\leq g_n(x)$ for all for $x\in[1,\infty)$ . And we have $$ \left|\sum_{n=1}^N f_n(x)\right|\leq\sum_{n=1}^N\left|f_n(x)\right|\leq\sum_{n=1}^N g_n(x)\leq\sum_{n=1}^\infty g_n(x)=\frac{\pi^2}{6}\frac{\ln{x}}{1+x^2} $$ $$ \sum_{n=1}^\infty\left[\int_1^\infty g_n(x)\,\mathrm{d}x\right]=\sum_{n=1}^\infty\frac{G}{n^2}=\frac{\pi^2}{6}G<\infty $$ where $G$ is the Catalan constant . Therefore, I can show that the integration and summation can be interchanged for $x\in[1,\infty)$ . My difficulty is that I am not able to find a similar dominant function for $x\in[0,1]$ . Any help would be greatly appreciated.","My question is, for how to prove that With the theorems descried here , for , I am able to find a dominant function so that for all for . And we have where is the Catalan constant . Therefore, I can show that the integration and summation can be interchanged for . My difficulty is that I am not able to find a similar dominant function for . Any help would be greatly appreciated.","
f_n(x)=\frac{e^{-nx}}{n}\ln{x},
 
\int_0^\infty\left[\sum_{n=1}^\infty f_n(x)\right]\mathrm{d}x =\sum_{n=1}^\infty \left[\int_0^\infty f_n(x)\,\mathrm{d}x\right]
 x\in[1,\infty) 
g_n(x)=\frac{1}{n^2}\frac{\ln{x}}{1+x^2}
 0\leq|f_n(x)|\leq g_n(x) x\in[1,\infty) 
\left|\sum_{n=1}^N f_n(x)\right|\leq\sum_{n=1}^N\left|f_n(x)\right|\leq\sum_{n=1}^N g_n(x)\leq\sum_{n=1}^\infty g_n(x)=\frac{\pi^2}{6}\frac{\ln{x}}{1+x^2}
 
\sum_{n=1}^\infty\left[\int_1^\infty g_n(x)\,\mathrm{d}x\right]=\sum_{n=1}^\infty\frac{G}{n^2}=\frac{\pi^2}{6}G<\infty
 G x\in[1,\infty) x\in[0,1]","['real-analysis', 'integration', 'analysis']"
23,proof of uniqueness of fourier series in Stein's book,proof of uniqueness of fourier series in Stein's book,,"On page 40, the book tries to prove uniqueness of fourier series the following: Suppose that $f$ is an integrable function on the circle with $\hat{f}(n)=0$ for all $n \in \mathbb{Z}$ . Then $f(\theta_{0})=0$ whenever f is continuous at the point $\theta_{0}$ . To prove this, assuming that $f$ satisfies the hypotheses, $\theta_{0}=0$ , and $f(0)>0$ . The idea is now to construct a family of trigonometric polynomials $\{p_{k}\}$ that peak at $0$ , resulting in $$\int p_{k}(\theta)f(\theta)\; d\theta \rightarrow \infty \;\; \text{as}\;\; k \rightarrow \infty.$$ And this will contradict the requirement that the coefficient needs to be 0. To construct the p, since $f$ is continuous at $0$ , we can choose $0 < \delta \leq \pi/2$ , so that $f(\theta)>f(0)/2$ whenever $|\theta|< \delta$ . Let $p(\theta) = \epsilon + \cos\theta$ where $\epsilon >0$ is chosen so small that $|p(\theta)|< 1-\epsilon/2$ , whenever $\delta \leq |\theta| \leq \pi$ . Then choose a positive $\eta$ with $\eta < \delta$ , so that $p(\theta) \geq 1 + \epsilon/2$ , for $|\theta| < \eta$ . Finally, we define $p_{k}(\theta) = [p(\theta)]^k$ . Then the key point will be when $|\theta| < \eta$ , $$\int p_{k}(\theta)f(\theta)\; d\theta \geq 2\eta \frac{f(0)}{2}(1+\frac{\epsilon}{2})^k \rightarrow \infty \; , k \rightarrow \infty$$ This contracts that the coefficient needs to be 0. So the proof finishes here. What I'm not understanding is when computing the fourier coefficient using normal formula, $$\hat{f}(n)=\int f(\theta)e^{-in\theta}\; d\theta.$$ If I sum over multiple coefficients, the formula will be $\int f(\theta)(e^{-i\theta}+e^{-i2\theta}+e^{-i3\theta}+...)\; d\theta.$ it will not be in the form of $p_k(\theta) = (\cos\theta + \epsilon)^k $ . Unless $p_k(\theta) = (\cos\theta + \epsilon)^k = e^{-i\theta}+e^{-i2\theta}+e^{-i3\theta}+... $ . under some conditions?","On page 40, the book tries to prove uniqueness of fourier series the following: Suppose that is an integrable function on the circle with for all . Then whenever f is continuous at the point . To prove this, assuming that satisfies the hypotheses, , and . The idea is now to construct a family of trigonometric polynomials that peak at , resulting in And this will contradict the requirement that the coefficient needs to be 0. To construct the p, since is continuous at , we can choose , so that whenever . Let where is chosen so small that , whenever . Then choose a positive with , so that , for . Finally, we define . Then the key point will be when , This contracts that the coefficient needs to be 0. So the proof finishes here. What I'm not understanding is when computing the fourier coefficient using normal formula, If I sum over multiple coefficients, the formula will be it will not be in the form of . Unless . under some conditions?","f \hat{f}(n)=0 n \in \mathbb{Z} f(\theta_{0})=0 \theta_{0} f \theta_{0}=0 f(0)>0 \{p_{k}\} 0 \int p_{k}(\theta)f(\theta)\; d\theta \rightarrow \infty \;\; \text{as}\;\; k \rightarrow \infty. f 0 0 < \delta \leq \pi/2 f(\theta)>f(0)/2 |\theta|< \delta p(\theta) = \epsilon + \cos\theta \epsilon >0 |p(\theta)|< 1-\epsilon/2 \delta \leq |\theta| \leq \pi \eta \eta < \delta p(\theta) \geq 1 + \epsilon/2 |\theta| < \eta p_{k}(\theta) = [p(\theta)]^k |\theta| < \eta \int p_{k}(\theta)f(\theta)\; d\theta \geq 2\eta \frac{f(0)}{2}(1+\frac{\epsilon}{2})^k \rightarrow \infty \; , k \rightarrow \infty \hat{f}(n)=\int f(\theta)e^{-in\theta}\; d\theta. \int f(\theta)(e^{-i\theta}+e^{-i2\theta}+e^{-i3\theta}+...)\; d\theta. p_k(\theta) = (\cos\theta + \epsilon)^k  p_k(\theta) = (\cos\theta + \epsilon)^k = e^{-i\theta}+e^{-i2\theta}+e^{-i3\theta}+... ","['analysis', 'fourier-analysis', 'fourier-series']"
24,The translation in french of «Layer Cake Representation»,The translation in french of «Layer Cake Representation»,,"I would like to know if there exists a generic way of referring to this identity $$\int_X |f(x)| \mathrm{d} \mu(x) = \int_0^\infty \mu\{x \in X : |f(x)| > t \} \mathrm{d} t$$ in french. In english, it is the layer cake representation. The same question arose on this site for the German translation : translate layer cake representation in German and the question was closed ; I apologize if it is not on-topic. I didn't managed to find to wikipedia page in french : see [EN] https://en.wikipedia.org/wiki/Layer_cake_representation .","I would like to know if there exists a generic way of referring to this identity in french. In english, it is the layer cake representation. The same question arose on this site for the German translation : translate layer cake representation in German and the question was closed ; I apologize if it is not on-topic. I didn't managed to find to wikipedia page in french : see [EN] https://en.wikipedia.org/wiki/Layer_cake_representation .",\int_X |f(x)| \mathrm{d} \mu(x) = \int_0^\infty \mu\{x \in X : |f(x)| > t \} \mathrm{d} t,"['analysis', 'measure-theory', 'translation-request', 'mathematical-french']"
25,Spivak Chapter 7 Question 13: Clarification,Spivak Chapter 7 Question 13: Clarification,,"In question 13 b of chapter 7 of Spivak, I'm being asked to prove that if a function satisfies the conclusion of the intermediate value theorem, and if that function takes on each value only once, it is continuous. I don't see why this is true, however. Let $f(x) = x$ in the interval $[1, 2)$ , and 2 when x = 3. Then f(x) satisfies the conclusion of the intermediate value theorem on [1, 3], but isn't continuous. Could someone explain to me why this isn't a counterexample for the given statement, or if I've misinterpreted the question? Could you also avoid giving hints on how to solve this problem? I would still like to attempt solving this question. Thanks so much in advance!","In question 13 b of chapter 7 of Spivak, I'm being asked to prove that if a function satisfies the conclusion of the intermediate value theorem, and if that function takes on each value only once, it is continuous. I don't see why this is true, however. Let in the interval , and 2 when x = 3. Then f(x) satisfies the conclusion of the intermediate value theorem on [1, 3], but isn't continuous. Could someone explain to me why this isn't a counterexample for the given statement, or if I've misinterpreted the question? Could you also avoid giving hints on how to solve this problem? I would still like to attempt solving this question. Thanks so much in advance!","f(x) = x [1, 2)","['calculus', 'analysis']"
26,Proving $\delta \notin C_c$,Proving,\delta \notin C_c,"Prove there is no $\delta \in C_c(\mathbb{R})$ such that for all $f \in C_c(\mathbb{R}) \ f= \delta \ast f$ . I think I have a solution to this problem, but am a bit unsure about it, as the author of this problem offers a more complicated solution. So I'd be grateful if anyone could check my work. Consider $f_n(t)= 1-nt$ when $0 \leq t \leq \frac{1}{n}$ , $f_n(t)= 1+nt$ when $-\frac{1}{n} \leq t \leq 0$ and $f_n(t)=0$ otherwise. Clearly this defines a sequence of compactly supported continuous functions. This sequence is uniformly bounded by $1$ and for each $n \in \mathbb{N}$ $supp(f_n)= [-\frac{1}{n}, \frac{1}{n}]$ . Hence $1= f_n(0)= \int_{-\frac{1}{n}}^{\frac{1}{n}} \delta(-y) f_n(y) dy \leq \frac{2}{n} ||\delta||_{\infty}$ . So any such $\delta$ has to be unbounded, so can't belong to $C_c$ . ( In case it is of interest: The author considers the same sequence but with spike $(0,n)$ rather than spike $(0,1)$ .)","Prove there is no such that for all . I think I have a solution to this problem, but am a bit unsure about it, as the author of this problem offers a more complicated solution. So I'd be grateful if anyone could check my work. Consider when , when and otherwise. Clearly this defines a sequence of compactly supported continuous functions. This sequence is uniformly bounded by and for each . Hence . So any such has to be unbounded, so can't belong to . ( In case it is of interest: The author considers the same sequence but with spike rather than spike .)","\delta \in C_c(\mathbb{R}) f \in C_c(\mathbb{R}) \ f= \delta \ast f f_n(t)= 1-nt 0 \leq t \leq \frac{1}{n} f_n(t)= 1+nt -\frac{1}{n} \leq t \leq 0 f_n(t)=0 1 n \in \mathbb{N} supp(f_n)= [-\frac{1}{n}, \frac{1}{n}] 1= f_n(0)= \int_{-\frac{1}{n}}^{\frac{1}{n}} \delta(-y) f_n(y) dy \leq \frac{2}{n} ||\delta||_{\infty} \delta C_c (0,n) (0,1)","['integration', 'analysis']"
27,A textbook with a proof of the Rank Theorem using only Banach's Fixed Point Theorem.,A textbook with a proof of the Rank Theorem using only Banach's Fixed Point Theorem.,,"It is well known that the Rank Theorem for $C^1$ maps can be obtained as a consequence of the Implicit Mapping Theorem and the Inverse Mapping Theorem for $C^1$ maps. See for example Zorich vol 1 . It is also well known that the Implicit and Inverse Mapping theorems are a consequence of the Banach's Fixed Point Theorem. See Pugh for a proof of Implicit Mapping Theorem directly from Banach's Fixed Point Theorem. See Rudin for a proof of the Inverse Mapping Theorem directly from Banach's Fixed Point Theorem. Obviously, for what was exposed in the previous paragraph, a proof (or several proofs) of Rank Theorem for $C^1$ mapping by means of the Banach Fixed Point Theorem exists. In other words, the affirmations of the Banach fixed point theorem and Rank Theorem for $C^1$ mapping  are not independent. In view of these considerations, my question is as follows. Is there any textbook with a proof of the rank theorem using Banach's Fixed Point Theorem? In the absence of a textbook, I would be very grateful for articles and lecture notes.","It is well known that the Rank Theorem for maps can be obtained as a consequence of the Implicit Mapping Theorem and the Inverse Mapping Theorem for maps. See for example Zorich vol 1 . It is also well known that the Implicit and Inverse Mapping theorems are a consequence of the Banach's Fixed Point Theorem. See Pugh for a proof of Implicit Mapping Theorem directly from Banach's Fixed Point Theorem. See Rudin for a proof of the Inverse Mapping Theorem directly from Banach's Fixed Point Theorem. Obviously, for what was exposed in the previous paragraph, a proof (or several proofs) of Rank Theorem for mapping by means of the Banach Fixed Point Theorem exists. In other words, the affirmations of the Banach fixed point theorem and Rank Theorem for mapping  are not independent. In view of these considerations, my question is as follows. Is there any textbook with a proof of the rank theorem using Banach's Fixed Point Theorem? In the absence of a textbook, I would be very grateful for articles and lecture notes.",C^1 C^1 C^1 C^1,"['real-analysis', 'analysis', 'reference-request', 'alternative-proof', 'fixed-point-theorems']"
28,Resources to learn hard analysis,Resources to learn hard analysis,,"I am looking for book (or any other resource) recommendations to practice ""epsilonics"", and technical analysis of the asymptotics of integer sequences (or real sequences), hard analytical estimates, etc. To give an example of what I have in mind, the following is from a paper of Erdos on the number of partitions $p(n)$ of a positive integer $n$ . Let $d=\liminf \frac{np(n)}{e^{cn^{1/2}}}$ , $D=\limsup \frac{np(n)}{e^{cn^{1/2}}}$ , where $c=\pi\sqrt{2/3}$ . Since $p(n)$ is an increasing function of $n$ there exists a $c_2$ such that for every $m$ in the range $n \le m \le n + c_2n^{1/2}$ , we have $$ \frac{mp(m)}{e^{cm^{1/2}}} > \frac{D + d}{2} $$ Now we claim that for every $r_1$ there exists a $\delta_{r_1} = \delta(r_1)$ such that for $n \le m \le n + c_2n^{1/2}$ , $$ \frac{mp(m)}{e^{cm^{1/2}}} > d + \delta_{r_1} $$ Then he goes on to prove the lemma. When I see statements such as ""for every $r_1$ there exists a $\delta_{r_1}...$ , or ""there exist constants $c$ and $x_0$ such that some function $f(n) < cn^{1/2}$ "", I'm in complete awe. I can read, but can't easily understand such hard technical statements, though the prerequisites seem to be nothing  beyond a first course in real analysis, which I have met (I'm an undergrad student). I want to learn how to create such technical arguments.Where does one go to get good at such things (None of my classes offer these sort of things)? What are some good resources for practicing?","I am looking for book (or any other resource) recommendations to practice ""epsilonics"", and technical analysis of the asymptotics of integer sequences (or real sequences), hard analytical estimates, etc. To give an example of what I have in mind, the following is from a paper of Erdos on the number of partitions of a positive integer . Let , , where . Since is an increasing function of there exists a such that for every in the range , we have Now we claim that for every there exists a such that for , Then he goes on to prove the lemma. When I see statements such as ""for every there exists a , or ""there exist constants and such that some function "", I'm in complete awe. I can read, but can't easily understand such hard technical statements, though the prerequisites seem to be nothing  beyond a first course in real analysis, which I have met (I'm an undergrad student). I want to learn how to create such technical arguments.Where does one go to get good at such things (None of my classes offer these sort of things)? What are some good resources for practicing?","p(n) n d=\liminf \frac{np(n)}{e^{cn^{1/2}}} D=\limsup \frac{np(n)}{e^{cn^{1/2}}} c=\pi\sqrt{2/3} p(n) n c_2 m n \le m \le n + c_2n^{1/2} 
\frac{mp(m)}{e^{cm^{1/2}}} > \frac{D + d}{2}
 r_1 \delta_{r_1} = \delta(r_1) n \le m \le n + c_2n^{1/2} 
\frac{mp(m)}{e^{cm^{1/2}}} > d + \delta_{r_1}
 r_1 \delta_{r_1}... c x_0 f(n) < cn^{1/2}","['number-theory', 'analysis', 'soft-question', 'book-recommendation']"
29,A $C^1$ function whose critical values contains the ternary Cantor set,A  function whose critical values contains the ternary Cantor set,C^1,"Recall that Sard's theorem says that if $U$ is an open subset of $\mathbb{R}^m$ and $f:U\rightarrow \mathbb{R}^n$ is a $C^k$ function, then the set of critical values has emasure zero if $k>m/n-1$ . My question is the following - Is there a continuously differentiable function $f:I=[0,1]\rightarrow \mathbb{R}$ (that is, a function in $C^1[0,1])$ such that the set of critical values is exactly the standard ternary Cantor set (obtained by the usual deletion of middle thirds construction)? By Sard's theorem above a $C^2$ function would be impossible to construct, since then $h:I^2\rightarrow \mathbb{R}^2$ given by $h(x,y) = f(x) + f(y)$ would contain the entire interval $[0,2]$ as the set of critical values violating Sard's theorem.","Recall that Sard's theorem says that if is an open subset of and is a function, then the set of critical values has emasure zero if . My question is the following - Is there a continuously differentiable function (that is, a function in such that the set of critical values is exactly the standard ternary Cantor set (obtained by the usual deletion of middle thirds construction)? By Sard's theorem above a function would be impossible to construct, since then given by would contain the entire interval as the set of critical values violating Sard's theorem.","U \mathbb{R}^m f:U\rightarrow \mathbb{R}^n C^k k>m/n-1 f:I=[0,1]\rightarrow \mathbb{R} C^1[0,1]) C^2 h:I^2\rightarrow \mathbb{R}^2 h(x,y) = f(x) + f(y) [0,2]","['analysis', 'differential-topology', 'smooth-manifolds']"
30,"Prob. 21, Chap. 2, in Royden's REAL ANALYSIS: The union of two measurable sets is measurable","Prob. 21, Chap. 2, in Royden's REAL ANALYSIS: The union of two measurable sets is measurable",,"Here is Prob. 21, Chap. 2, in the book Real Analysis by H.L. Royden and P.M. Fitzpatrick, 4th edition: Use property (ii) of Theorem 11 as the primitive definition of a measurable set and prove that the union of two measurable sets is measurable. Then do the same for property (iv). Here is Theorem 11: Let $E$ be any set of real numbers. Then each of the following four assertions is equivalent to the measurability of $E$ . (i) For each $\epsilon > 0$ , there is an open set $O$ containing $E$ for which $m^*(O\setminus E) < \epsilon$ . (ii) There is a $G_\delta$ -set $G$ containing $E$ for which $m^*(G \setminus E) = 0$ . (iii) For each $\epsilon > 0$ , there is a closed set $F$ contained in $E$ for which $m^*(E \setminus F) < \epsilon$ . (iv) There is an $F_\sigma$ -set $F$ contained in $E$ for which $m^*(E \setminus F) = 0$ . My Attempt: Using property (ii) as the primitive definition of a measurable set, let us suppose that $E_1$ and $E_2$ are measurable sets, where $E_1 \subset \mathbb{R}$ and $E_2 \subset \mathbb{R}$ of course. Then there are $G_\delta$ -sets $G_1$ and $G_2$ containing $E_1$ and $E_2$ , respectively, for which $$ m^* \left( G_1 \setminus E_1 \right) = 0 = m^* \left( G_2 \setminus E_2 \right).  $$ As $E_1 \subset G_1$ and $E_2 \subset G_2$ , so we also have $E_1 \cup E_2 \subset G_1 \cup G_2$ . We show that $G_1 \cup G_2$ is also a $G_\delta$ -set. As $G_1$ and $G_2$ are $G_\delta$ -sets, so there are countable collections $\left\{ U_{1j} \right\}_{j=1}^\infty$ and $\left\{ U_{2k} \right\}_{k=1}^\infty$ of open sets (of $\mathbb{R}$ ) such that $$ G_1 = \bigcap_{j=1}^\infty U_{1j} \qquad \mbox{ and } \qquad G_2 = \bigcap_{k=1}^\infty U_{2k}.  $$ Then we obtain $$ \begin{align} G_1 \cup G_2 &= \left( \bigcap_{j=1}^\infty U_{1j}  \right) \bigcup \left(  \bigcap_{k=1}^\infty U_{2k} \right) \\ &= \bigcap_{j=1}^\infty  \left[ U_{1j} \bigcup  \left(  \bigcap_{k=1}^\infty U_{2k} \right) \right]  \\ &= \bigcap_{j=1}^\infty  \bigcap_{k=1}^\infty \left(  U_{1j} \bigcup U_{2k} \right), \end{align} $$ whcih being the intersection of the  countable collection $\left\{ U_{1j} \cup U_{2k} \right\}_{j, k =1}^\infty$ of open sets is a $G_\delta$ -set. Furthermore, we also find that $$ \begin{align} \left( G_1 \cup G_2 \right) \setminus \left( E_1 \cup E_2 \right) &= \left( G_1 \cup G_2 \right) \cap  \left( E_1 \cup E_2 \right)^C \\ &= \left( G_1 \cup G_2 \right) \cap  \left( E_1^C \cap E_2^C \right) \\ &= \left[ G_1 \cap \left( E_1^C \cap E_2^C \right) \right] \cup \left[ G_2 \cap \left( E_1^C \cap E_2^C \right) \right] \\ &= \left[ \left(  G_1 \cap E_1^C \right)  \cap E_2^C \right] \cup \left[ G_2 \cap \left( E_2^C \cap E_1^C \right) \right] \\ &= \left[ \left(  G_1 \cap E_1^C \right)  \cap E_2^C \right] \cup \left[ \left(  G_2 \cap E_2^C \right) \cap E_1^C \right] \\ &\subset \left( G_1 \cap E_1^C \right) \cup \left( G_2 \cap E_2^C \right) \\ &= \left( G_1 \setminus E_1 \right) \cup \left( G_2 \setminus E_2 \right). \end{align}  $$ Thus we have the inclusion $$ \left( G_1 \cup G_2 \right) \setminus \left( E_1 \cup E_2 \right) \ \subset \ \left( G_1 \setminus E_1 \right) \cup \left( G_2 \setminus E_2 \right). $$ Therefore using the monotonocity and sub-additivity of the outer measure, we have $$ \begin{align} 0 &\leq m^* \left( \left( G_1 \cup G_2 \right) \setminus \left( E_1 \cup E_2 \right)  \right) \\  &\leq m^* \left( \left( G_1 \setminus E_1 \right) \cup \left( G_2 \setminus E_2 \right) \right) \\ &\leq m^* \left( G_1 \setminus E_1 \right) + m^* \left( G_2 \setminus E_2 \right) \\ &= 0 + 0 \\ &= 0, \end{align} $$ which implies that $$ m^* \left( \left( G_1 \cup G_2 \right) \setminus \left( E_1 \cup E_2 \right)  \right) = 0. $$ Thus we have shown that there exists a $G_\delta$ -set $G := G_1 \cup G_2$ for which $$ m^* \left( G \setminus \left( E_1 \cup E_2 \right) \right) = 0.  $$ Hence $E_1 \cup E_2$ is measurable. Is this proof correct? Now using the property (iv), let us suppose that $E_1$ and $E_2$ are measurable sets. Then there are $F_\sigma$ -sets $F_1$ and $F_2$ contained in $E_1$ and $E_2$ , respectively, for which $$ m^* \left( E_1 \setminus F_1 \right) = 0 = m^* \left( E_2 \setminus F_2 \right).  $$ As $F_1 \subset E_1$ and $F_2 \subset E_2$ , so $F_1 \cup F_2 \subset E_1 \cup E_2$ . As $F_1$ and $F_2$ are $F_\sigma$ -sets, so there are countable collections $\left\{ V_{1j} \right\}_{j=1}^\infty$ and $\left\{ V_{2k} \right\}_{k=1}^\infty$ of closed sets of $\mathbb{R}$ for which $$ F_1 = \bigcup_{j=1}^\infty V_{1j} \qquad \mbox{ and } \qquad F_2 = \bigcup_{k=1}^\infty V_{2k}.  $$ Then we obtain $$ F_1 \cup F_2 = \bigcup_{j=1}^\infty \bigcup_{k=1}^\infty \left( V_{1j} \cup V_{2k} \right),  $$ and thus $F_1 \cup F_2$ , being the union of the countable collection $\left\{ V_{1j} \cup V_{2k} \right\}_{j, k = 1}^\infty$ of closed sets, is an $F_\sigma$ -set. Furthermore, as before, since $$ \left( E_1 \cup E_2 \right) \setminus \left( F_1 \cup F_2 \right)\ \subset \  \left( E_1 \setminus F_1 \right) \cup \left( E_2 \setminus F_2 \right),  $$ therefore we have $$ \begin{align}  0 &\leq m^* \left( \left( E_1 \cup E_2 \right) \setminus \left( F_1 \cup F_2 \right)  \right) \\  &\leq m^* \left( \left( E_1 \setminus F_1 \right) \cup \left( E_2 \setminus F_2 \right) \right) \\  &\leq m^*  \left( E_1 \setminus F_1  \right) + m^* \left( E_2 \setminus F_2 \right) \\ &= 0 + 0 \\ &= 0, \end{align} $$ and so $$ m^* \left( \left( E_1 \cup E_2 \right) \setminus \left( F_1 \cup F_2 \right)  \right) = 0, $$ Thus we have an $F_\sigma$ set $F := F_1 \cup F_2$ contined in $E_1 \cup E_2$ for which $m^* \left( \left( E_1 \cup E_2 \right) \setminus F \right) = 0$ . Therefore it follows that set $E_1 \cup E_2$ is measurable. Is this proof correct? Are both of my above proofs correct and clear enough in each and every detail? Or, are there any issues?","Here is Prob. 21, Chap. 2, in the book Real Analysis by H.L. Royden and P.M. Fitzpatrick, 4th edition: Use property (ii) of Theorem 11 as the primitive definition of a measurable set and prove that the union of two measurable sets is measurable. Then do the same for property (iv). Here is Theorem 11: Let be any set of real numbers. Then each of the following four assertions is equivalent to the measurability of . (i) For each , there is an open set containing for which . (ii) There is a -set containing for which . (iii) For each , there is a closed set contained in for which . (iv) There is an -set contained in for which . My Attempt: Using property (ii) as the primitive definition of a measurable set, let us suppose that and are measurable sets, where and of course. Then there are -sets and containing and , respectively, for which As and , so we also have . We show that is also a -set. As and are -sets, so there are countable collections and of open sets (of ) such that Then we obtain whcih being the intersection of the  countable collection of open sets is a -set. Furthermore, we also find that Thus we have the inclusion Therefore using the monotonocity and sub-additivity of the outer measure, we have which implies that Thus we have shown that there exists a -set for which Hence is measurable. Is this proof correct? Now using the property (iv), let us suppose that and are measurable sets. Then there are -sets and contained in and , respectively, for which As and , so . As and are -sets, so there are countable collections and of closed sets of for which Then we obtain and thus , being the union of the countable collection of closed sets, is an -set. Furthermore, as before, since therefore we have and so Thus we have an set contined in for which . Therefore it follows that set is measurable. Is this proof correct? Are both of my above proofs correct and clear enough in each and every detail? Or, are there any issues?","E E \epsilon > 0 O E m^*(O\setminus E) < \epsilon G_\delta G E m^*(G \setminus E) = 0 \epsilon > 0 F E m^*(E \setminus F) < \epsilon F_\sigma F E m^*(E \setminus F) = 0 E_1 E_2 E_1 \subset \mathbb{R} E_2 \subset \mathbb{R} G_\delta G_1 G_2 E_1 E_2 
m^* \left( G_1 \setminus E_1 \right) = 0 = m^* \left( G_2 \setminus E_2 \right). 
 E_1 \subset G_1 E_2 \subset G_2 E_1 \cup E_2 \subset G_1 \cup G_2 G_1 \cup G_2 G_\delta G_1 G_2 G_\delta \left\{ U_{1j} \right\}_{j=1}^\infty \left\{ U_{2k} \right\}_{k=1}^\infty \mathbb{R} 
G_1 = \bigcap_{j=1}^\infty U_{1j} \qquad \mbox{ and } \qquad G_2 = \bigcap_{k=1}^\infty U_{2k}. 
 
\begin{align}
G_1 \cup G_2 &= \left( \bigcap_{j=1}^\infty U_{1j}  \right) \bigcup \left(  \bigcap_{k=1}^\infty U_{2k} \right) \\
&= \bigcap_{j=1}^\infty  \left[ U_{1j} \bigcup  \left(  \bigcap_{k=1}^\infty U_{2k} \right) \right]  \\
&= \bigcap_{j=1}^\infty  \bigcap_{k=1}^\infty \left(  U_{1j} \bigcup U_{2k} \right),
\end{align}
 \left\{ U_{1j} \cup U_{2k} \right\}_{j, k =1}^\infty G_\delta 
\begin{align}
\left( G_1 \cup G_2 \right) \setminus \left( E_1 \cup E_2 \right) &= \left( G_1 \cup G_2 \right) \cap  \left( E_1 \cup E_2 \right)^C \\
&= \left( G_1 \cup G_2 \right) \cap  \left( E_1^C \cap E_2^C \right) \\
&= \left[ G_1 \cap \left( E_1^C \cap E_2^C \right) \right] \cup \left[ G_2 \cap \left( E_1^C \cap E_2^C \right) \right] \\
&= \left[ \left(  G_1 \cap E_1^C \right)  \cap E_2^C \right] \cup \left[ G_2 \cap \left( E_2^C \cap E_1^C \right) \right] \\
&= \left[ \left(  G_1 \cap E_1^C \right)  \cap E_2^C \right] \cup \left[ \left(  G_2 \cap E_2^C \right) \cap E_1^C \right] \\
&\subset \left( G_1 \cap E_1^C \right) \cup \left( G_2 \cap E_2^C \right) \\
&= \left( G_1 \setminus E_1 \right) \cup \left( G_2 \setminus E_2 \right).
\end{align} 
 
\left( G_1 \cup G_2 \right) \setminus \left( E_1 \cup E_2 \right) \ \subset \ \left( G_1 \setminus E_1 \right) \cup \left( G_2 \setminus E_2 \right).
 
\begin{align}
0 &\leq m^* \left( \left( G_1 \cup G_2 \right) \setminus \left( E_1 \cup E_2 \right)  \right) \\ 
&\leq m^* \left( \left( G_1 \setminus E_1 \right) \cup \left( G_2 \setminus E_2 \right) \right) \\
&\leq m^* \left( G_1 \setminus E_1 \right) + m^* \left( G_2 \setminus E_2 \right) \\
&= 0 + 0 \\
&= 0,
\end{align}
 
m^* \left( \left( G_1 \cup G_2 \right) \setminus \left( E_1 \cup E_2 \right)  \right) = 0.
 G_\delta G := G_1 \cup G_2 
m^* \left( G \setminus \left( E_1 \cup E_2 \right) \right) = 0. 
 E_1 \cup E_2 E_1 E_2 F_\sigma F_1 F_2 E_1 E_2 
m^* \left( E_1 \setminus F_1 \right) = 0 = m^* \left( E_2 \setminus F_2 \right). 
 F_1 \subset E_1 F_2 \subset E_2 F_1 \cup F_2 \subset E_1 \cup E_2 F_1 F_2 F_\sigma \left\{ V_{1j} \right\}_{j=1}^\infty \left\{ V_{2k} \right\}_{k=1}^\infty \mathbb{R} 
F_1 = \bigcup_{j=1}^\infty V_{1j} \qquad \mbox{ and } \qquad F_2 = \bigcup_{k=1}^\infty V_{2k}. 
 
F_1 \cup F_2 = \bigcup_{j=1}^\infty \bigcup_{k=1}^\infty \left( V_{1j} \cup V_{2k} \right), 
 F_1 \cup F_2 \left\{ V_{1j} \cup V_{2k} \right\}_{j, k = 1}^\infty F_\sigma 
\left( E_1 \cup E_2 \right) \setminus \left( F_1 \cup F_2 \right)\ \subset \  \left( E_1 \setminus F_1 \right) \cup \left( E_2 \setminus F_2 \right), 
 
\begin{align} 
0 &\leq m^* \left( \left( E_1 \cup E_2 \right) \setminus \left( F_1 \cup F_2 \right)  \right) \\ 
&\leq m^* \left( \left( E_1 \setminus F_1 \right) \cup \left( E_2 \setminus F_2 \right) \right) \\ 
&\leq m^*  \left( E_1 \setminus F_1  \right) + m^* \left( E_2 \setminus F_2 \right) \\
&= 0 + 0 \\
&= 0,
\end{align}
 
m^* \left( \left( E_1 \cup E_2 \right) \setminus \left( F_1 \cup F_2 \right)  \right) = 0,
 F_\sigma F := F_1 \cup F_2 E_1 \cup E_2 m^* \left( \left( E_1 \cup E_2 \right) \setminus F \right) = 0 E_1 \cup E_2","['real-analysis', 'analysis', 'measure-theory', 'solution-verification']"
31,Can the points of a convergent sequence be connected by a smooth curve?,Can the points of a convergent sequence be connected by a smooth curve?,,"Suppose $U\subset \mathbb{R}^n$ is the unit ball and $\left\{x_i\right\}_{i\in \mathbb{N}}$ is a sequence in $U$ that converges to $0$ . Does there exist a smooth curve $\gamma \colon \left[0,1 \right] \to U$ such that the image of $\gamma$ contains a subsequence of $\left\{x_i\right\}_{i\in \mathbb{N}}$ and such that $\gamma(0)=x_1$ and $\gamma(1)=0$ ? I do not know how to approach this. The case for finitely many points already troubles me. I know about smoothing piece-wise linear curves but that is not really helpful here. Edit: So, I did have an idea to get a $C^0$ -curve that is analytic on $(0,1]$ , so everywhere except one boundary point. Sadly my approach will probably never explicitly produce a $C^{\infty}$ -curve. Consider $\mathbb{R}^n\subseteq \mathbb{C}^n$ and the given sequence as a sequence in $\mathbb{C}^n$ . That is one gets $n$ sequences in $\mathbb{C}$ by considering the components $\left\{ x_i^j \right\}_{i\in \mathbb{N}}$ . Also one may consider the discrete sequence $\left\{1,2,3,... \right\}$ in $\mathbb{C}$ . As $\mathbb{C}$ is a Stein manifold it follows that there exists holomorphic functions $f^j\colon \mathbb{C} \to \mathbb{C}$ such that $f^j(i)=x^j_i.$ Then $g'^j:=\mathrm{Re}(f^j)\mid_{\mathbb{R}}\colon \mathbb{R} \to \mathbb{R}$ also satisfies the above equation and note that $\lim_{x\to \infty }g'^j(x)=0$ . One may moreover consider the analytic  map $g^j\colon (0,1] \to \mathbb{R},\; x \mapsto g'^j(1/x)$ it satisfies $\lim_{x\to 0} g^j(x)=0$ and thus extends continuously to $[0,1]$ . Moreover, its image contains the entire sequence one started with by construction.","Suppose is the unit ball and is a sequence in that converges to . Does there exist a smooth curve such that the image of contains a subsequence of and such that and ? I do not know how to approach this. The case for finitely many points already troubles me. I know about smoothing piece-wise linear curves but that is not really helpful here. Edit: So, I did have an idea to get a -curve that is analytic on , so everywhere except one boundary point. Sadly my approach will probably never explicitly produce a -curve. Consider and the given sequence as a sequence in . That is one gets sequences in by considering the components . Also one may consider the discrete sequence in . As is a Stein manifold it follows that there exists holomorphic functions such that Then also satisfies the above equation and note that . One may moreover consider the analytic  map it satisfies and thus extends continuously to . Moreover, its image contains the entire sequence one started with by construction.","U\subset \mathbb{R}^n \left\{x_i\right\}_{i\in \mathbb{N}} U 0 \gamma \colon \left[0,1 \right] \to U \gamma \left\{x_i\right\}_{i\in \mathbb{N}} \gamma(0)=x_1 \gamma(1)=0 C^0 (0,1] C^{\infty} \mathbb{R}^n\subseteq \mathbb{C}^n \mathbb{C}^n n \mathbb{C} \left\{ x_i^j \right\}_{i\in \mathbb{N}} \left\{1,2,3,... \right\} \mathbb{C} \mathbb{C} f^j\colon \mathbb{C} \to \mathbb{C} f^j(i)=x^j_i. g'^j:=\mathrm{Re}(f^j)\mid_{\mathbb{R}}\colon \mathbb{R} \to \mathbb{R} \lim_{x\to \infty }g'^j(x)=0 g^j\colon (0,1] \to \mathbb{R},\; x \mapsto g'^j(1/x) \lim_{x\to 0} g^j(x)=0 [0,1]","['real-analysis', 'analysis', 'differential-geometry', 'curves']"
32,Proof feedback: integral of a function with f(x)=0 is 0,Proof feedback: integral of a function with f(x)=0 is 0,,"I'm attempting to prove the following statement, and having some difficulty: Let $f:[a,b] \rightarrow \mathbb{R}$ be continuous, $f(x)\ge 0$ for all $x$ and $\alpha(x)=x$ . Prove: If $\int_a^b f \, d\alpha=0$ then $f(x)=0$ for all $x\in[a,b]$ . My attempt, seeking to find a contradiction, goes as follows: Assume $\int_a^b f \,d\alpha=0$ and $f(c)=q>0$ for some $c \in [a,b].$ $Lemma \ 1:$ Let $f$ be continuous at $c$ and suppose $f(c)\neq 0$ . Then there exists $\delta>0$ and an interval $(c-\delta,c+\delta)$ in which $f$ has the same sign as $f(c)$ . Since $f(c)=q>0$ and $f$ is continuous on $[a,b]$ , we invoke $Lemma \ 1$ and get that there exists $x$ such that $f(x)>\frac{f(c)}{2}=\frac{q}{2}$ when $x\in (c-\delta,c+\delta)$ . Now, note that for any partition $P$ , we have $U(P,f,\alpha)>\delta\frac{q}{2}>0$ , since we would always at least have $f(x)$ in some arbitrarily small $\delta$ -interval. We then get that $$     0<\frac{q}{2} \delta \leq \overline{\int_a^b} f \, d\alpha = \int_a^b f \, d\alpha=0 $$ which is a contradiction. So $f(x)=0$ $\forall x\in[a,b]$ . In general, I feel ok about this proof, but there are certainly some gaps that I can't seem to entirely justify. Any help is appreciated! Thanks.","I'm attempting to prove the following statement, and having some difficulty: Let be continuous, for all and . Prove: If then for all . My attempt, seeking to find a contradiction, goes as follows: Assume and for some Let be continuous at and suppose . Then there exists and an interval in which has the same sign as . Since and is continuous on , we invoke and get that there exists such that when . Now, note that for any partition , we have , since we would always at least have in some arbitrarily small -interval. We then get that which is a contradiction. So . In general, I feel ok about this proof, but there are certainly some gaps that I can't seem to entirely justify. Any help is appreciated! Thanks.","f:[a,b] \rightarrow \mathbb{R} f(x)\ge 0 x \alpha(x)=x \int_a^b f \, d\alpha=0 f(x)=0 x\in[a,b] \int_a^b f \,d\alpha=0 f(c)=q>0 c \in [a,b]. Lemma \ 1: f c f(c)\neq 0 \delta>0 (c-\delta,c+\delta) f f(c) f(c)=q>0 f [a,b] Lemma \ 1 x f(x)>\frac{f(c)}{2}=\frac{q}{2} x\in (c-\delta,c+\delta) P U(P,f,\alpha)>\delta\frac{q}{2}>0 f(x) \delta 
    0<\frac{q}{2} \delta \leq \overline{\int_a^b} f \, d\alpha = \int_a^b f \, d\alpha=0
 f(x)=0 \forall x\in[a,b]","['integration', 'analysis']"
33,Continuity of $e^x-1$ with $\epsilon-\delta$,Continuity of  with,e^x-1 \epsilon-\delta,"Let $f \colon \mathbb{R}^- \to \mathbb{R} \quad x \mapsto e^x-1 ,\; \epsilon >0,\; x<0$ . Now, in order to prove continuity at $x_0=0$ having $|x|<\delta$ , we can follow: $|f(x)-f(0)|= |e^x-1| \overset{|x|<1}{\leq} \frac{|x|}{1-|x|}\overset{\delta \neq 1}{<} \frac{\delta}{1-\delta} \overset{!}{=}\epsilon$ . So in total we need $|x|<1, \delta \neq 1  \land \delta = \frac{\epsilon}{1+\epsilon}$ . As latter is $<1$ for $\delta= \frac{\epsilon}{1+\epsilon}$ we have shown continuity. Problem: the master solution shows the following $\delta$ , which I don't get: $\delta= \begin{cases} \frac{\epsilon}{1+\epsilon} & \epsilon <1\\ \frac{1}{2} & \epsilon \geq 1\end{cases}$ . I noticed that at $\epsilon=1$ the terms are equal, but why the restriction for big $\epsilon$ as the above term is anyway always smaller 1?","Let . Now, in order to prove continuity at having , we can follow: . So in total we need . As latter is for we have shown continuity. Problem: the master solution shows the following , which I don't get: . I noticed that at the terms are equal, but why the restriction for big as the above term is anyway always smaller 1?","f \colon \mathbb{R}^- \to \mathbb{R} \quad x \mapsto e^x-1 ,\; \epsilon >0,\; x<0 x_0=0 |x|<\delta |f(x)-f(0)|= |e^x-1| \overset{|x|<1}{\leq} \frac{|x|}{1-|x|}\overset{\delta \neq 1}{<} \frac{\delta}{1-\delta} \overset{!}{=}\epsilon |x|<1, \delta \neq 1  \land \delta = \frac{\epsilon}{1+\epsilon} <1 \delta= \frac{\epsilon}{1+\epsilon} \delta \delta= \begin{cases} \frac{\epsilon}{1+\epsilon} & \epsilon <1\\
\frac{1}{2} & \epsilon \geq 1\end{cases} \epsilon=1 \epsilon","['real-analysis', 'calculus', 'analysis']"
34,Is there a useful asymptotic expansion of $(1 + z^{\sqrt{2} - 1} + z^{\sqrt{2}})^{-1}$ at $z = 0$?,Is there a useful asymptotic expansion of  at ?,(1 + z^{\sqrt{2} - 1} + z^{\sqrt{2}})^{-1} z = 0,I believe that an asymptotic expansion in terms of powers of $z$ can not exist because we could use the geometric series to find something of the form $$ \frac 1{1 + z^{\sqrt 2 - 1} +z^{\sqrt 2 }} = \sum_{n \ge 0} (-1)^n\sum_{i = 0}^n \binom n i  z^{\sqrt 2  n -  i} $$ so Dirichlet's Approx Thm would furnish the existence of a sequence of powers converging to $0$ . Using powers of logs does not work either since they would have to be indexed from infinity to 0. What functions should one use to asymptotically analyze series whose naive expansions in terms of powers yield powers converging to $0$ (e.g. $(1+z^a+z^b)^{-1}$ where $a/b\notin \Bbb Q$ )?,I believe that an asymptotic expansion in terms of powers of can not exist because we could use the geometric series to find something of the form so Dirichlet's Approx Thm would furnish the existence of a sequence of powers converging to . Using powers of logs does not work either since they would have to be indexed from infinity to 0. What functions should one use to asymptotically analyze series whose naive expansions in terms of powers yield powers converging to (e.g. where )?,"z 
\frac 1{1 + z^{\sqrt 2 - 1} +z^{\sqrt 2 }} = \sum_{n \ge 0} (-1)^n\sum_{i = 0}^n \binom n i  z^{\sqrt 2  n -  i}
 0 0 (1+z^a+z^b)^{-1} a/b\notin \Bbb Q","['analysis', 'asymptotics', 'taylor-expansion']"
35,Proof Verification: Baby Rudin Ch. 6. Ex. 7a,Proof Verification: Baby Rudin Ch. 6. Ex. 7a,,"I want to follow up on my previous question . Based on the comments and responses I got for my previous question, I developed a new proof for Baby Rudin Ch. 6. Ex. 7a. The exercise is: Suppose $f$ is a real function on $(0, 1]$ and $f \in \mathscr{R}$ on $[c, 1]$ for every $c>0$ . Define \begin{equation}\tag{7.0}     \int_0^1 f(x) \, dx = \lim_{c \to 0} \int_{c}^1 f(x) \, dx \end{equation} if this limit exists (and is finite). If $f \in \mathscr{R}$ on $[0, 1]$ , show that this definition of the integral agrees with the old one. My latest attempt: Firstly, we state a modified version of Theorem 6.12 (c) in the text; call it Theorem 6.12 (c $^*$ ). Theorem 6.12 (c) states that: If $f \in \mathscr{R}(\alpha)$ on $[a, b]$ and if $a<c<b$ , then $f \in \mathscr{R}(\alpha)$ on $[a, c]$ and on $[c, b]$ and \begin{equation*}             \int_a^c f \, d\alpha + \int_c^b f \, d\alpha = \int_a^b f \, d\alpha \end{equation*} Now, if we allow for the possibility of $c=b$ in Theorem 6.12 (c), we obtain  Theorem 6.12 (c $^*$ ). To wit, take $c=b$ in Theorem 6.12 (c). Then, If $f \in \mathscr{R}(\alpha)$ on $[a, b]$ and if $a<c\le b$ , then $f \in \mathscr{R}(\alpha)$ on $[a, c]$ and on $[c, b]$ and \begin{equation*}         \int_a^c f \, d\alpha + \int_c^b f \, d\alpha = \int_a^b f \, d\alpha + \int_b^b f \, d\alpha = \int_a^b f \, d\alpha     \end{equation*} In other words, we can safely replace $a<c<b$ with $a<c\le b$ in Th. 6.12. Now, we prove the original question. Suppose $f$ is a real function and is Riemann integrable on $[0, 1]$ in the sense of the definition of Riemann integrals given on Pg. 121 in Rudin. Let $0<c\le1$ and that the limit on the right-hand side of (7.0) exists. By Theorem 6.12 (c $^*$ ), $f$ is Riemann integrable on $[c, 1]$ . We want to show that \begin{equation} \lim_{c \to 0} \int_{c}^1 f(x) \, dx = \int_0^1 f(x) \, dx = \lim_{c \to 0} \int_{0}^1 f(x) \, dx  \end{equation} Let $\epsilon> 0$ be given. It suffices to that there exists a $\delta>0$ such that \begin{equation}\tag{7.1}     0< |c-0| < \delta \implies \left|\int_{c}^1 f(x) \, dx- \int_{0}^1 f(x) \, dx \right| < \epsilon \end{equation} By Theorem 6.12 (c $^*$ ), we know that $f$ is Riemann integrable on $[0, c]$ and \begin{equation}\tag{7.2}     \int_0^c f(x) \, dx =  \int_0^1 f(x) \, dx - \int_c^1 f(x) \, dx \end{equation} Since $f$ is Riemann integrable on $[0, c]$ , it must also be bounded on $[0, c]$ , for if $f$ is not bounded on $[0, c]$ , then it cannot be Riemann integrable on $[0, c]$ . Put $M = \sup\limits_{0 \, \le x \, \le c} |f(x)|$ . Clearly, $M \ge 0$ . If $M =0$ , then $f(x) = 0 \, \forall x$ , which implies \begin{equation}      \int_0^1 f(x) \, dx =0= \int_{c}^1 f(x) \, dx   \end{equation} which means that the implication in (7.1) will hold for any $\delta$ . So, assume WLOG that $0 < \epsilon\le M$ . By Theorem 6.12 (d),we have \begin{equation*}     \left| \int_0^c f(x) \, dx\right| \le M(c-0) = Mc \stackrel{(7.2)}{\implies} \left| \int_0^1 f(x) \, dx - \int_c^1 f(x) \, dx\right| \le Mc \end{equation*} Put $\delta = \dfrac{\epsilon}{M}$ . Notice that $c\in(0, 1]$ and $\dfrac{\epsilon}{M}\in(0, 1]$ . If $c=\dfrac{\epsilon}{M}, Mc=\epsilon$ and \begin{equation*}     \left| \int_0^1 f(x) \, dx - \int_c^1 f(x) \, dx\right| \le Mc = \epsilon \end{equation*} If $c<\dfrac{\epsilon}{M}$ , we have \begin{equation*}     \left| \int_0^1 f(x) \, dx - \int_c^1 f(x) \, dx\right| \le Mc < M \cdot \frac{\epsilon}{M} = \epsilon \end{equation*} If $c>\dfrac{\epsilon}{M}, \dots$ My question : Is Theorem 6.12(c $^*$ ) actually correct? If my proof correct, or is there something in the proof that can be improved? Can someone please suggest how the proof can be completed? Thanks!","I want to follow up on my previous question . Based on the comments and responses I got for my previous question, I developed a new proof for Baby Rudin Ch. 6. Ex. 7a. The exercise is: Suppose is a real function on and on for every . Define if this limit exists (and is finite). If on , show that this definition of the integral agrees with the old one. My latest attempt: Firstly, we state a modified version of Theorem 6.12 (c) in the text; call it Theorem 6.12 (c ). Theorem 6.12 (c) states that: If on and if , then on and on and Now, if we allow for the possibility of in Theorem 6.12 (c), we obtain  Theorem 6.12 (c ). To wit, take in Theorem 6.12 (c). Then, If on and if , then on and on and In other words, we can safely replace with in Th. 6.12. Now, we prove the original question. Suppose is a real function and is Riemann integrable on in the sense of the definition of Riemann integrals given on Pg. 121 in Rudin. Let and that the limit on the right-hand side of (7.0) exists. By Theorem 6.12 (c ), is Riemann integrable on . We want to show that Let be given. It suffices to that there exists a such that By Theorem 6.12 (c ), we know that is Riemann integrable on and Since is Riemann integrable on , it must also be bounded on , for if is not bounded on , then it cannot be Riemann integrable on . Put . Clearly, . If , then , which implies which means that the implication in (7.1) will hold for any . So, assume WLOG that . By Theorem 6.12 (d),we have Put . Notice that and . If and If , we have If My question : Is Theorem 6.12(c ) actually correct? If my proof correct, or is there something in the proof that can be improved? Can someone please suggest how the proof can be completed? Thanks!","f (0, 1] f \in \mathscr{R} [c, 1] c>0 \begin{equation}\tag{7.0}
    \int_0^1 f(x) \, dx = \lim_{c \to 0} \int_{c}^1 f(x) \, dx
\end{equation} f \in \mathscr{R} [0, 1] ^* f \in \mathscr{R}(\alpha) [a, b] a<c<b f \in \mathscr{R}(\alpha) [a, c] [c, b] \begin{equation*}
            \int_a^c f \, d\alpha + \int_c^b f \, d\alpha = \int_a^b f \, d\alpha
\end{equation*} c=b ^* c=b f \in \mathscr{R}(\alpha) [a, b] a<c\le b f \in \mathscr{R}(\alpha) [a, c] [c, b] \begin{equation*}
        \int_a^c f \, d\alpha + \int_c^b f \, d\alpha = \int_a^b f \, d\alpha + \int_b^b f \, d\alpha = \int_a^b f \, d\alpha
    \end{equation*} a<c<b a<c\le b f [0, 1] 0<c\le1 ^* f [c, 1] \begin{equation}
\lim_{c \to 0} \int_{c}^1 f(x) \, dx = \int_0^1 f(x) \, dx = \lim_{c \to 0} \int_{0}^1 f(x) \, dx 
\end{equation} \epsilon> 0 \delta>0 \begin{equation}\tag{7.1}
    0< |c-0| < \delta \implies \left|\int_{c}^1 f(x) \, dx- \int_{0}^1 f(x) \, dx \right| < \epsilon
\end{equation} ^* f [0, c] \begin{equation}\tag{7.2}
    \int_0^c f(x) \, dx =  \int_0^1 f(x) \, dx - \int_c^1 f(x) \, dx
\end{equation} f [0, c] [0, c] f [0, c] [0, c] M = \sup\limits_{0 \, \le x \, \le c} |f(x)| M \ge 0 M =0 f(x) = 0 \, \forall x \begin{equation}
     \int_0^1 f(x) \, dx =0= \int_{c}^1 f(x) \, dx 
 \end{equation} \delta 0 < \epsilon\le M \begin{equation*}
    \left| \int_0^c f(x) \, dx\right| \le M(c-0) = Mc \stackrel{(7.2)}{\implies} \left| \int_0^1 f(x) \, dx - \int_c^1 f(x) \, dx\right| \le Mc
\end{equation*} \delta = \dfrac{\epsilon}{M} c\in(0, 1] \dfrac{\epsilon}{M}\in(0, 1] c=\dfrac{\epsilon}{M}, Mc=\epsilon \begin{equation*}
    \left| \int_0^1 f(x) \, dx - \int_c^1 f(x) \, dx\right| \le Mc = \epsilon
\end{equation*} c<\dfrac{\epsilon}{M} \begin{equation*}
    \left| \int_0^1 f(x) \, dx - \int_c^1 f(x) \, dx\right| \le Mc < M \cdot \frac{\epsilon}{M} = \epsilon
\end{equation*} c>\dfrac{\epsilon}{M}, \dots ^*","['real-analysis', 'integration', 'analysis', 'proof-writing', 'solution-verification']"
36,"Weak formulation, Variational formulation, Solution of a PDE.","Weak formulation, Variational formulation, Solution of a PDE.",,"Ok, I'm struggling with some basic stuff. My question is: given a PDE are the concepts of a variational formulation and weak formulation the same? Take a PDE (letting $\phi : \mathbb{R}\to \mathbb{R}$ say convex) $$  \partial_t u(t,x)=-\nabla_x \phi(u(t,x)),\quad u(0,x)=u_{(0)}(x)\label{1}\tag{1} $$ A strong solution $u:([0,T] \times \mathbb{R}^d) \to \mathbb{R}$ satisfies the above equation for all $t$ and $x$ . Upon multiplication by a test function $\psi$ and integration of \eqref{1} and moving the derivatives onto $\psi$ via integration by parts one can obtain the weak formulation. $\textbf{Question :}$ My question is sometimes instead of writing down the weak formulation of a PDE an author will claim it has an associated variational formulation, for instance see this book chapter , where Eq. $(1)$ is the PDE, and eq. $(5)$ is its variational formulation. What does a variational formulation mean? And where does it come from?","Ok, I'm struggling with some basic stuff. My question is: given a PDE are the concepts of a variational formulation and weak formulation the same? Take a PDE (letting say convex) A strong solution satisfies the above equation for all and . Upon multiplication by a test function and integration of \eqref{1} and moving the derivatives onto via integration by parts one can obtain the weak formulation. My question is sometimes instead of writing down the weak formulation of a PDE an author will claim it has an associated variational formulation, for instance see this book chapter , where Eq. is the PDE, and eq. is its variational formulation. What does a variational formulation mean? And where does it come from?","\phi : \mathbb{R}\to \mathbb{R}  
\partial_t u(t,x)=-\nabla_x \phi(u(t,x)),\quad u(0,x)=u_{(0)}(x)\label{1}\tag{1}
 u:([0,T] \times \mathbb{R}^d) \to \mathbb{R} t x \psi \psi \textbf{Question :} (1) (5)","['real-analysis', 'analysis', 'partial-differential-equations', 'convex-analysis', 'calculus-of-variations']"
37,Scale-invariant functions and the case of the logarithm,Scale-invariant functions and the case of the logarithm,,"I just read the paper in Wikipedia [1] on scale invariance of some functions or curves $f(x)$ . I have a few questions on the mathematics related to this. In ref. [1], $f(x)$ is said to be scale invariant when $f(\lambda x) = \lambda^{\Delta} f(x),  \ \ \ \ \ \ \ \ \ \ \ \ Eq. [1]$ where $\lambda$ is a scale factor, and $\Delta$ an arbitrary exponent (they didn't provide the definition domains of such constants). 1) Is it the most formal definition of a scale-invariant function ? What are the definition domains of $\lambda$ and $\Delta$ ? I understand why the the monomials $f(x)=x^n$ is scale invariant but what about 2) the logarithmic function $f(x) = \mathrm{ln}(x)$ ? 3) And the spiral logarithmic function defined as $\theta = \frac{1}{b} \mathrm{ln}(r/a)$ ? I have some difficulties to prove that the above functions are scale-invariant according to Eq. [1], typically because of Eq. [1], I have a sum of logs and I can not factorize. I am probably missing a property on logarithmic functions that could help... [1] https://en.wikipedia.org/wiki/Scale_invariance","I just read the paper in Wikipedia [1] on scale invariance of some functions or curves . I have a few questions on the mathematics related to this. In ref. [1], is said to be scale invariant when where is a scale factor, and an arbitrary exponent (they didn't provide the definition domains of such constants). 1) Is it the most formal definition of a scale-invariant function ? What are the definition domains of and ? I understand why the the monomials is scale invariant but what about 2) the logarithmic function ? 3) And the spiral logarithmic function defined as ? I have some difficulties to prove that the above functions are scale-invariant according to Eq. [1], typically because of Eq. [1], I have a sum of logs and I can not factorize. I am probably missing a property on logarithmic functions that could help... [1] https://en.wikipedia.org/wiki/Scale_invariance","f(x) f(x) f(\lambda x) = \lambda^{\Delta} f(x),  \ \ \ \ \ \ \ \ \ \ \ \ Eq. [1] \lambda \Delta \lambda \Delta f(x)=x^n f(x) = \mathrm{ln}(x) \theta = \frac{1}{b} \mathrm{ln}(r/a)","['real-analysis', 'analysis', 'functions', 'definition']"
38,"Prove that $ f(x) $ has at least two real roots in $ (0,\pi) $",Prove that  has at least two real roots in," f(x)   (0,\pi) ","Let $ f $ be a continuous function defined on $ [0,\pi] $. Suppose that $$ \int_{0}^{\pi}f(x)\sin {x} dx=0,   \int_{0}^{\pi}f(x)\cos {x} dx=0 $$ Prove that $ f(x) $ has at least two real roots in $ (0,\pi) $","Let $ f $ be a continuous function defined on $ [0,\pi] $. Suppose that $$ \int_{0}^{\pi}f(x)\sin {x} dx=0,   \int_{0}^{\pi}f(x)\cos {x} dx=0 $$ Prove that $ f(x) $ has at least two real roots in $ (0,\pi) $",,['calculus']
39,What is the geometric significance of differentiable vs continuously differentiable?,What is the geometric significance of differentiable vs continuously differentiable?,,"What is the geometric significance of differentiable vs continuously differentiable for functions (based on $\mathbb{R}$ )?  By 'geometric' i mean the appearance of the plot of such functions. Perhaps this question is best splilt into three categories: $\mathbb{R}$ to $\mathbb{R}^n$ , $\mathbb{R}^n$ to $\mathbb{R}$ , and $\mathbb{R}^n$ to $\mathbb{R}^m$ .","What is the geometric significance of differentiable vs continuously differentiable for functions (based on )?  By 'geometric' i mean the appearance of the plot of such functions. Perhaps this question is best splilt into three categories: to , to , and to .",\mathbb{R} \mathbb{R} \mathbb{R}^n \mathbb{R}^n \mathbb{R} \mathbb{R}^n \mathbb{R}^m,"['real-analysis', 'analysis']"
40,A function with big eigenvalues acting on the unit ball.,A function with big eigenvalues acting on the unit ball.,,"Let $B(0,1)=\{(x,y,z):x^2+y^2+z^2< 1\}$ denote the unit ball in $\mathbb{R}^3$ and $C$ denote a cylinder around the $z$ -axis with radius $2$ . Suppose that on each point $w$ in $B(0,1)$ we attach a pair of linearly independent vectors $ u_w,v_w$ . Suppose now that we can find a function $f:\mathbb{R}^3\to\mathbb{R}^3$ which is continuously differentiable everywhere and whose total derivative, $D_wf$ at a point $w\in B(0,1)$ has as its two biggest eigenvalues $\lambda_u$ and $\lambda_v$ for which the corresponding eigenvectors are $u_w$ and $v_w$ . Assume that the eigenvalues $\lambda_u$ and $\lambda_v$ corresponding to  those two vectors both satisfy $|\lambda_u|,|\lambda_v|>M$ , where $M$ is a big number. Question : Is it true that the image of the ball $f(B(0,1))$ will necessarily intersect the cylinder $C$ ,  assuming the number $M$ is big enough? Intuitively it seems to me that with all those assumptions our function $f$ stretches the unit ball  at two different directions locally by a lot so the answer to the question should be yes. Edit : We can also assume that $f$ is a homeomorphism. Added later: If we also knew that the smallest eigenvalue $\lambda_0$ is big,  meaning $|\lambda_0|>M$ , then the answer would be yes. That is because in this case  if $\gamma$ is the shortest curve joining $f(0)$ with $\partial f(B(0,1))$ then $$ \operatorname{length}(\gamma)\geq M.$$ This can be proved as follows : Let $\gamma_1=f^{-1}(\gamma)$ be the inverse image of $\gamma$ which is a curve joining $0$ with $\partial B(0,1)$ . Then \begin{align*} 	\operatorname{length}(\gamma)&=\int_{\gamma}|dx|=\int_{f(\gamma_1)}|dx|=\int_{0}^{1}\left|\left(f\left(\gamma_1(t)\right)\right)'\right|dt\\&=\int_{0}^{1}\left|D_{\gamma_1(t)}f\left(\frac{\gamma_1'(t)}{|\gamma_1'(t)|}\right)\right||\gamma_1(t)'|dt\\&\geq\int_{0}^{1}\inf_{|x|=1}\left|D_{\gamma_1(t)}f\left(x\right)\right||\gamma_1(t)'|dt\geq M\int_{0}^{1}|\gamma_1(t)'|dt\geq M \end{align*}","Let denote the unit ball in and denote a cylinder around the -axis with radius . Suppose that on each point in we attach a pair of linearly independent vectors . Suppose now that we can find a function which is continuously differentiable everywhere and whose total derivative, at a point has as its two biggest eigenvalues and for which the corresponding eigenvectors are and . Assume that the eigenvalues and corresponding to  those two vectors both satisfy , where is a big number. Question : Is it true that the image of the ball will necessarily intersect the cylinder ,  assuming the number is big enough? Intuitively it seems to me that with all those assumptions our function stretches the unit ball  at two different directions locally by a lot so the answer to the question should be yes. Edit : We can also assume that is a homeomorphism. Added later: If we also knew that the smallest eigenvalue is big,  meaning , then the answer would be yes. That is because in this case  if is the shortest curve joining with then This can be proved as follows : Let be the inverse image of which is a curve joining with . Then","B(0,1)=\{(x,y,z):x^2+y^2+z^2< 1\} \mathbb{R}^3 C z 2 w B(0,1)  u_w,v_w f:\mathbb{R}^3\to\mathbb{R}^3 D_wf w\in B(0,1) \lambda_u \lambda_v u_w v_w \lambda_u \lambda_v |\lambda_u|,|\lambda_v|>M M f(B(0,1)) C M f f \lambda_0 |\lambda_0|>M \gamma f(0) \partial f(B(0,1)) 
\operatorname{length}(\gamma)\geq M. \gamma_1=f^{-1}(\gamma) \gamma 0 \partial B(0,1) \begin{align*}
	\operatorname{length}(\gamma)&=\int_{\gamma}|dx|=\int_{f(\gamma_1)}|dx|=\int_{0}^{1}\left|\left(f\left(\gamma_1(t)\right)\right)'\right|dt\\&=\int_{0}^{1}\left|D_{\gamma_1(t)}f\left(\frac{\gamma_1'(t)}{|\gamma_1'(t)|}\right)\right||\gamma_1(t)'|dt\\&\geq\int_{0}^{1}\inf_{|x|=1}\left|D_{\gamma_1(t)}f\left(x\right)\right||\gamma_1(t)'|dt\geq M\int_{0}^{1}|\gamma_1(t)'|dt\geq M
\end{align*}","['analysis', 'multivariable-calculus', 'derivatives', 'eigenvalues-eigenvectors']"
41,homotopic $C^{2}$ paths are $C^{2}$ homotopic?,homotopic  paths are  homotopic?,C^{2} C^{2},"The context is the following : Proposition : Let $E$ be a real Banach space, $\Omega \subseteq E$ open, $\omega \in C^{1}(\Omega,E^{*})$ a closed 1-form. Let be $\gamma_{0},\gamma_{1}$ $C^{2}$ two $\gamma$ -homotopic paths with fixed endpoints. It holds that $\int_{\gamma_{0}} \omega = \int_{\gamma_{1}} \omega$ . I do understand the proof I have of the above statement, which goes on defining $\phi(s) = \int_{\gamma(s,\bullet)} \omega$ and prooving that it's constant; What I don't get is the assumption (without loss of generality) that $\gamma$ , the homotopy between $\gamma_{0},\gamma_{1}$ , a priori $C^{0}$ , can actually be considered $C^{2}$ . Are there any known approximation Thorems for this? I thought about using Stone-Weierstrass or trying to approximate the homotopy with piecewise $C^{2}$ polygonals taken in appropriate dense set, failing. I found some references of what I'm looking for only between manifolds and smooth maps in the following links, but since manifold are above my knowledge i didn't find find any solution. Any help or comment would be appreciated,thanks. Smooth homotopy , Homotopic and Smoothly Homotopic Manifolds","The context is the following : Proposition : Let be a real Banach space, open, a closed 1-form. Let be two -homotopic paths with fixed endpoints. It holds that . I do understand the proof I have of the above statement, which goes on defining and prooving that it's constant; What I don't get is the assumption (without loss of generality) that , the homotopy between , a priori , can actually be considered . Are there any known approximation Thorems for this? I thought about using Stone-Weierstrass or trying to approximate the homotopy with piecewise polygonals taken in appropriate dense set, failing. I found some references of what I'm looking for only between manifolds and smooth maps in the following links, but since manifold are above my knowledge i didn't find find any solution. Any help or comment would be appreciated,thanks. Smooth homotopy , Homotopic and Smoothly Homotopic Manifolds","E \Omega \subseteq E \omega \in C^{1}(\Omega,E^{*}) \gamma_{0},\gamma_{1} C^{2} \gamma \int_{\gamma_{0}} \omega = \int_{\gamma_{1}} \omega \phi(s) = \int_{\gamma(s,\bullet)} \omega \gamma \gamma_{0},\gamma_{1} C^{0} C^{2} C^{2}","['analysis', 'banach-spaces', 'homotopy-theory', 'closed-form', 'approximation-theory']"
42,$1-1+1-1+1-1+\cdots$ and $1-2+3-4+5-6+7-\cdots$ and Taylor's theorem,and  and Taylor's theorem,1-1+1-1+1-1+\cdots 1-2+3-4+5-6+7-\cdots,"Some background. I was exploring the series expansion for $\ln(1+\cos x)$ in an attempt to expand it (at least, initially!) up to the third non-zero term, and along the way I unexpectedly stumbled upon the infinite series $1-1+1-1+...$ and $1-2+3-4+5-6+7-...$ . Then I thought to expand it instead via a different method, as I had divergent series as coefficients which obviously wouldn't have helped in determining the terms exactly. The coefficients via this method (as expected) came out as reals, and I was then, out of curiosity, tempted to equate these respectively, arriving at a very unexpected (in this context) result: $1-1+1-1+1-1+1-...=\frac{1}{2}$ and $1-2+3-4+5-6+...=\frac{1}{4}$ . What caught me as particularly surprising is that these are well-known results for the sums via other methodologies. The ""proof"" is below. By Taylor's theorem, substituting the series for $\ln(1+x)$ and $\cos x$ : \begin{align} \ln(1+\cos x) &=\sum_{n=1}^\infty \left(\frac{(-1)^{n-1}}{n}\left({\sum_{k=0}^\infty \frac{(-1)^{k}x^{2k}}{(2k)!}}\right)^{n}\right)\\ &=\sum_{n=1}^\infty \left(\frac{(-1)^{n-1}}{n}\left(1-\frac{x^2}2+\frac{x^4}{24}-\cdots\right)^n\right)\\ \end{align} We are only concerned with the first 3 terms overall, and terms whose order is 6 or above cannot contribute towards the coefficients of the lower order terms (0,2,4 in this case). So consider $\left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^n$ , ignoring order 6 or higher terms, for $n\in\mathbb{Z_{>0}}$ : \begin{align} \left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^2 &=\ 1-\frac{2x^2}{2}+\frac{8x^4}{24}-\cdots\\ \left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^3 &=\ 1-\frac{3x^2}{2}+\frac{21x^4}{24}-\cdots\\ \left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^4 &=\ 1-\frac{4x^2}{2}+\frac{40x^4}{24}-\cdots\\ \end{align} and so on. Conjecture that (from the patterns in the coefficients) $$\left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^m=1-\frac{mx^2}{2}+\frac{am^2+bm+c}{24}x^4-\cdots$$ for some $a,b,c\in\mathbb{R}$ . We know some $x^4$ coefficients from the manual calculations, so we can solve for a, b, c (using m = 1, 2, 3): $$a+b+c=1$$ $$4a+2b+c=8$$ $$9a+3b+c=21$$ $$\implies a=3,b=-2,c=0$$ $$\therefore\left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^m=1-\frac{mx^2}{2}+\frac{3m^2-2m}{24}x^4-\cdots$$ for m = 1, 2, 3. Assuming this as an inductive hypothesis for some $m\in\mathbb{Z_{>0}}$ , we have: \begin{align} \left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^{m+1} &=\left(1-\frac{x^2}{2}+\frac{x^4}{24})^m(1-\frac{x^2}{2}+\frac{x^4}{24}\right)\\ &=\left(1-\frac{mx^2}{2}+\frac{3m^2-2m}{24}x^4-\cdots\right)\left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)\\ &=1-\frac{m+1}{2}x^2+\frac{3m^2-2m+6m+1}{24}x^4-\cdots\\ &=1-\frac{m+1}{2}x^2+\frac{3(m+1)^2-2(m+1)}{24}x^4-\cdots\\ \end{align} $$\therefore\left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^m=1-\frac{mx^2}{2}+\frac{3m^2-2m}{24}x^4-\cdots\forall{m}\in\mathbb{Z_{>0}}$$ by mathematical induction. Substituting this expansion back into the initial series expression, ignoring any terms in $x^6$ or higher (again because these terms will not contribute towards lower order coefficients): \begin{align} \sum_{n=1}^\infty \left(\frac{(-1)^{n-1}}{n}(1-\frac{x^2}{2}+\frac{x^4}{24})^n\right) &=\sum_{n=1}^\infty \left(\frac{(-1)^{n-1}}{n}\left(1-\frac{nx^2}{2}+\frac{3n^2-2n}{24}x^4\right)\right)\\ &=\sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n}-\frac{x^2}{2}\sum_{n=1}^{\infty}(-1)^{n-1}+\frac{x^4}{24}\sum_{n=1}^{\infty}(3n-2)(-1)^{n-1}\\ &=\sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n}-\frac{x^2}{2}\sum_{n=1}^{\infty}(-1)^{n-1}+\frac{x^4}{24}\left(3\sum_{n=1}^{\infty}n(-1)^{n-1}-2\sum_{n=1}^{\infty}(-1)^{n-1}\right)\\ \end{align} Now we've expressed the first 3 terms with the coefficients as series, let's expand it in a different way. \begin{align} \ln(1+\cos x) &=\ln(2+(\cos x-1))\\ &=\ln\left(2\left(1+\frac{1}{2}\left(\cos x-1\right)\right)\right)\\ &=\ln2+\ln\left(1+\frac{1}{2}(\cos x-1)\right)\\ &=\ln2+\sum_{n=1}^\infty \left(\frac{(-1)^{n-1}}{n}\left(\frac{1}{2}{\sum_{k=1}^\infty \frac{(-1)^{k}x^{2k}}{(2k)!}}\right)^n\right)\\ &=\ln2+\frac{1}{2}\sum_{n=1}^\infty \left(\frac{\left(\frac{-1}{2}\right)^{n-1}}{n}\left(-\frac{x^2}{2}+\frac{x^4}{24}-\cdots\right)^n\right)\\ &=\ln2+\frac{1}{2}\left(\left(-\frac{x^2}{2}+\frac{x^4}{24}-\frac{x^6}{720}+\cdots\right)-\frac{1}{4}\left(-\frac{x^2}{2}+\frac{x^4}{24}-\frac{x^6}{720}+\cdots\right)^2+\cdots\right)\\ \end{align} Here, we can leave from consideration any of the order 6 or higher terms in the first ""bracket"", any of the order 4 or higher terms in the second ""bracket"" and all terms in any subsequent ""bracket"" for the purpose of our expansion (only looking for the first 3 terms). None of these will contribute towards the coefficients of the first few terms, as the power of the ""bracket"" is incrementing by 1 at each step. We get: \begin{align} \ln(1+\cos x) &=\ln2+\frac{1}{2}\left(\left(-\frac{x^2}{2}+\frac{x^4}{24}\right)-\frac{1}{4}\left(-\frac{x^2}{2}\right)^2+\cdots\right)\\ &=\ln2-\frac{x^2}{4}-\frac{x^4}{96}+\cdots\\ \end{align} Now that we have 2 different expressions for $\ln(1+\cos x)$ , we can compare their coefficients: $$\sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n}-\frac{x^2}{2}\sum_{n=1}^{\infty}(-1)^{n-1}+\frac{x^4}{24}(3\sum_{n=1}^{\infty}n(-1)^{n-1}-2\sum_{n=1}^{\infty}(-1)^{n-1})=\ln2-\frac{x^2}{4}-\frac{x^4}{96}$$ From this: $$\sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n}=\ln2;$$ $$-\frac{1}{2}\sum_{n=1}^{\infty}(-1)^{n-1}=-\frac{1}{4}\implies\sum_{n=1}^{\infty}(-1)^{n-1}=\frac{1}{2};$$ $$\frac{1}{24}\left(3\sum_{n=1}^{\infty}n(-1)^{n-1}-2\sum_{n=1}^{\infty}(-1)^{n-1}\right)=-\frac{1}{96}$$ $$\implies3\sum_{n=1}^{\infty}n(-1)^{n-1}-2\left(\frac{1}{2}\right)=-\frac{1}{4}\implies\sum_{n=1}^{\infty}n(-1)^{n-1}=\frac{1}{4}$$ So the reason for the post was, why is it that this arrives at said conclusions, and where do the flaws in the method lie? What's the catch? These are obviously quite unintuitive (but interesting, nonetheless) values for such series, arrived at in a completely unforeseen (to me, at least) way. Would highly appreciate any clarifications/explanations. P.S - I'm an A-level/high-school student, so if there are obvious issues then apologies; I haven't done any analysis yet.","Some background. I was exploring the series expansion for in an attempt to expand it (at least, initially!) up to the third non-zero term, and along the way I unexpectedly stumbled upon the infinite series and . Then I thought to expand it instead via a different method, as I had divergent series as coefficients which obviously wouldn't have helped in determining the terms exactly. The coefficients via this method (as expected) came out as reals, and I was then, out of curiosity, tempted to equate these respectively, arriving at a very unexpected (in this context) result: and . What caught me as particularly surprising is that these are well-known results for the sums via other methodologies. The ""proof"" is below. By Taylor's theorem, substituting the series for and : We are only concerned with the first 3 terms overall, and terms whose order is 6 or above cannot contribute towards the coefficients of the lower order terms (0,2,4 in this case). So consider , ignoring order 6 or higher terms, for : and so on. Conjecture that (from the patterns in the coefficients) for some . We know some coefficients from the manual calculations, so we can solve for a, b, c (using m = 1, 2, 3): for m = 1, 2, 3. Assuming this as an inductive hypothesis for some , we have: by mathematical induction. Substituting this expansion back into the initial series expression, ignoring any terms in or higher (again because these terms will not contribute towards lower order coefficients): Now we've expressed the first 3 terms with the coefficients as series, let's expand it in a different way. Here, we can leave from consideration any of the order 6 or higher terms in the first ""bracket"", any of the order 4 or higher terms in the second ""bracket"" and all terms in any subsequent ""bracket"" for the purpose of our expansion (only looking for the first 3 terms). None of these will contribute towards the coefficients of the first few terms, as the power of the ""bracket"" is incrementing by 1 at each step. We get: Now that we have 2 different expressions for , we can compare their coefficients: From this: So the reason for the post was, why is it that this arrives at said conclusions, and where do the flaws in the method lie? What's the catch? These are obviously quite unintuitive (but interesting, nonetheless) values for such series, arrived at in a completely unforeseen (to me, at least) way. Would highly appreciate any clarifications/explanations. P.S - I'm an A-level/high-school student, so if there are obvious issues then apologies; I haven't done any analysis yet.","\ln(1+\cos x) 1-1+1-1+... 1-2+3-4+5-6+7-... 1-1+1-1+1-1+1-...=\frac{1}{2} 1-2+3-4+5-6+...=\frac{1}{4} \ln(1+x) \cos x \begin{align}
\ln(1+\cos x)
&=\sum_{n=1}^\infty \left(\frac{(-1)^{n-1}}{n}\left({\sum_{k=0}^\infty \frac{(-1)^{k}x^{2k}}{(2k)!}}\right)^{n}\right)\\
&=\sum_{n=1}^\infty \left(\frac{(-1)^{n-1}}{n}\left(1-\frac{x^2}2+\frac{x^4}{24}-\cdots\right)^n\right)\\
\end{align} \left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^n n\in\mathbb{Z_{>0}} \begin{align}
\left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^2
&=\ 1-\frac{2x^2}{2}+\frac{8x^4}{24}-\cdots\\
\left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^3
&=\ 1-\frac{3x^2}{2}+\frac{21x^4}{24}-\cdots\\
\left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^4
&=\ 1-\frac{4x^2}{2}+\frac{40x^4}{24}-\cdots\\
\end{align} \left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^m=1-\frac{mx^2}{2}+\frac{am^2+bm+c}{24}x^4-\cdots a,b,c\in\mathbb{R} x^4 a+b+c=1 4a+2b+c=8 9a+3b+c=21 \implies a=3,b=-2,c=0 \therefore\left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^m=1-\frac{mx^2}{2}+\frac{3m^2-2m}{24}x^4-\cdots m\in\mathbb{Z_{>0}} \begin{align}
\left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^{m+1}
&=\left(1-\frac{x^2}{2}+\frac{x^4}{24})^m(1-\frac{x^2}{2}+\frac{x^4}{24}\right)\\
&=\left(1-\frac{mx^2}{2}+\frac{3m^2-2m}{24}x^4-\cdots\right)\left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)\\
&=1-\frac{m+1}{2}x^2+\frac{3m^2-2m+6m+1}{24}x^4-\cdots\\
&=1-\frac{m+1}{2}x^2+\frac{3(m+1)^2-2(m+1)}{24}x^4-\cdots\\
\end{align} \therefore\left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^m=1-\frac{mx^2}{2}+\frac{3m^2-2m}{24}x^4-\cdots\forall{m}\in\mathbb{Z_{>0}} x^6 \begin{align}
\sum_{n=1}^\infty \left(\frac{(-1)^{n-1}}{n}(1-\frac{x^2}{2}+\frac{x^4}{24})^n\right)
&=\sum_{n=1}^\infty \left(\frac{(-1)^{n-1}}{n}\left(1-\frac{nx^2}{2}+\frac{3n^2-2n}{24}x^4\right)\right)\\
&=\sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n}-\frac{x^2}{2}\sum_{n=1}^{\infty}(-1)^{n-1}+\frac{x^4}{24}\sum_{n=1}^{\infty}(3n-2)(-1)^{n-1}\\
&=\sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n}-\frac{x^2}{2}\sum_{n=1}^{\infty}(-1)^{n-1}+\frac{x^4}{24}\left(3\sum_{n=1}^{\infty}n(-1)^{n-1}-2\sum_{n=1}^{\infty}(-1)^{n-1}\right)\\
\end{align} \begin{align}
\ln(1+\cos x)
&=\ln(2+(\cos x-1))\\
&=\ln\left(2\left(1+\frac{1}{2}\left(\cos x-1\right)\right)\right)\\
&=\ln2+\ln\left(1+\frac{1}{2}(\cos x-1)\right)\\
&=\ln2+\sum_{n=1}^\infty \left(\frac{(-1)^{n-1}}{n}\left(\frac{1}{2}{\sum_{k=1}^\infty \frac{(-1)^{k}x^{2k}}{(2k)!}}\right)^n\right)\\
&=\ln2+\frac{1}{2}\sum_{n=1}^\infty \left(\frac{\left(\frac{-1}{2}\right)^{n-1}}{n}\left(-\frac{x^2}{2}+\frac{x^4}{24}-\cdots\right)^n\right)\\
&=\ln2+\frac{1}{2}\left(\left(-\frac{x^2}{2}+\frac{x^4}{24}-\frac{x^6}{720}+\cdots\right)-\frac{1}{4}\left(-\frac{x^2}{2}+\frac{x^4}{24}-\frac{x^6}{720}+\cdots\right)^2+\cdots\right)\\
\end{align} \begin{align}
\ln(1+\cos x)
&=\ln2+\frac{1}{2}\left(\left(-\frac{x^2}{2}+\frac{x^4}{24}\right)-\frac{1}{4}\left(-\frac{x^2}{2}\right)^2+\cdots\right)\\
&=\ln2-\frac{x^2}{4}-\frac{x^4}{96}+\cdots\\
\end{align} \ln(1+\cos x) \sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n}-\frac{x^2}{2}\sum_{n=1}^{\infty}(-1)^{n-1}+\frac{x^4}{24}(3\sum_{n=1}^{\infty}n(-1)^{n-1}-2\sum_{n=1}^{\infty}(-1)^{n-1})=\ln2-\frac{x^2}{4}-\frac{x^4}{96} \sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n}=\ln2; -\frac{1}{2}\sum_{n=1}^{\infty}(-1)^{n-1}=-\frac{1}{4}\implies\sum_{n=1}^{\infty}(-1)^{n-1}=\frac{1}{2}; \frac{1}{24}\left(3\sum_{n=1}^{\infty}n(-1)^{n-1}-2\sum_{n=1}^{\infty}(-1)^{n-1}\right)=-\frac{1}{96} \implies3\sum_{n=1}^{\infty}n(-1)^{n-1}-2\left(\frac{1}{2}\right)=-\frac{1}{4}\implies\sum_{n=1}^{\infty}n(-1)^{n-1}=\frac{1}{4}","['real-analysis', 'sequences-and-series']"
43,Solving Nonlinear Wave Equation $u_{tt}=a(e^{\lambda u}u_x)_x$,Solving Nonlinear Wave Equation,u_{tt}=a(e^{\lambda u}u_x)_x,"Salutations, I have been trying to solve this nonlinear wave equation that is a partial differential equation of the form (just for academical curiosity): $$u_{tt}=a(e^{\lambda u}u_x)_x$$ As I read in this document ( link ) (page 3, equation No. 7), it can be solved using this form of solution: $$ \begin{align} u(x,t) & =\phi(x)+\psi(t)\\ & \Updownarrow \\ u_{tt} & =\psi_{tt} \\  u_x & =\phi_x \end{align}$$ Then, replacing in non-linear PDE, I got this procedure: $$ \begin{split} \psi_{tt} & = a[e^{\lambda\left(\phi+\psi\right)}\phi_x]_x \\ & =a\left(e^\left(\lambda\phi\right)e^\left(\lambda\psi\right)\phi_x\right)_x \\ &=ae^\left(\lambda\psi\right)\left(e^\left(\lambda\phi\right)\phi_x\right)_x \\ & \Updownarrow \\   \frac{1}{e^\left(\lambda\psi\right)}\psi_{tt} &=a\left(e^\left(\lambda\phi\right)\phi_x\right)_x  =m \end{split}$$ The previous procedure led to the following equations: $$ \begin{align} a\left(e^\left(\lambda\phi\right)\phi_x\right)_x &=m \label{1}\tag{1}\\ \psi_{tt} &=me^\left(\lambda\psi\right)\label{2}\tag{2} \end{align} $$ Approaching equation \eqref{1}, I got this procedure: $$ \begin{split} a\left(e^\left(\lambda\phi\right)\phi_x\right)_x=m \implies  \int a\left(e^\left(\lambda\phi\right)\phi_x\right)_x \,dx & = \int \frac{m}{a} \,dx\\  e^\left(\lambda\phi\right)\phi_x & =\frac{mx}{a}+c_1\\  \int e^\left(\lambda\phi\right) \,d\phi &= \int \left(\frac{mx}{a}+c_1\right) \,dx \\  \frac{1}{\lambda}e^\left(\lambda\phi\right)&=\frac{m}{2a}x^2+c_1x+c_2 \end{split} $$ Finally, $$\phi(x)=\frac{1}{\lambda}\ln\left(\frac{\lambda m}{2a}x^2+\lambda c_1x+\lambda c_2\right)$$ Next, when I approached equation \eqref{2}, I got this procedure: $$ \begin{split} \frac{\partial^2 \psi}{\partial t^2} &= me^\left(\lambda\psi\right) \\  \int \frac{\partial^2 \psi}{\partial t^2} \frac{d\psi}{dt} \,dt &=m\int e^\left(\lambda\psi\right) \frac{d\psi}{dt}\,dt\\  \frac{1}{2}\left(\frac{d\psi}{dt}\right)^2 &= me^\left(\lambda\psi\right)+c  \end{split} $$ Then, $$ \int \frac{1}{\sqrt{2me^\left(\lambda\psi\right)+c_1}}\,d\psi = t+k  $$ I´ve taken trigonometric substitutions and even I´ve used the hyperbolic inverse function and I´ve got these results for integral in left side: $$ \begin{align} \frac{-2}{\lambda\sqrt{c_1}}\ln\left(\frac{\sqrt{c_1} + \sqrt{2me^\left(\lambda\psi\right)+c_1}}{\sqrt{2c_1-2me^\left(\lambda*\psi\right)}}\right)\label{I}\tag{I}&\\ \frac{-2}{\lambda\sqrt{2me^\left(\lambda\psi\right)+c_1}} & \label{II}\tag{II} \\ \frac{-1}{\lambda\sqrt c_1}\ln\left(\frac{\sqrt{c_1} +w}{\sqrt{c_1} -w}\right)&\text{ where }w=\sqrt{2me^\left(\lambda\psi\right)+c_1}\label{3}\tag{II} \end{align} $$ I require help for finding the solution to equation $(2)$ because none of possible solutions guide to exact solution shown in this paper in spanish ( link ) (page 3, section 2). I would be very thankful with any guidance or starting steps and/or explanations to find the correct procedure and exact solution. Thanks for your time and your attention.","Salutations, I have been trying to solve this nonlinear wave equation that is a partial differential equation of the form (just for academical curiosity): As I read in this document ( link ) (page 3, equation No. 7), it can be solved using this form of solution: Then, replacing in non-linear PDE, I got this procedure: The previous procedure led to the following equations: Approaching equation \eqref{1}, I got this procedure: Finally, Next, when I approached equation \eqref{2}, I got this procedure: Then, I´ve taken trigonometric substitutions and even I´ve used the hyperbolic inverse function and I´ve got these results for integral in left side: I require help for finding the solution to equation because none of possible solutions guide to exact solution shown in this paper in spanish ( link ) (page 3, section 2). I would be very thankful with any guidance or starting steps and/or explanations to find the correct procedure and exact solution. Thanks for your time and your attention.","u_{tt}=a(e^{\lambda u}u_x)_x 
\begin{align}
u(x,t) & =\phi(x)+\psi(t)\\ & \Updownarrow \\
u_{tt} & =\psi_{tt} \\ 
u_x & =\phi_x
\end{align} 
\begin{split}
\psi_{tt} & = a[e^{\lambda\left(\phi+\psi\right)}\phi_x]_x \\
& =a\left(e^\left(\lambda\phi\right)e^\left(\lambda\psi\right)\phi_x\right)_x \\
&=ae^\left(\lambda\psi\right)\left(e^\left(\lambda\phi\right)\phi_x\right)_x \\
& \Updownarrow \\  
\frac{1}{e^\left(\lambda\psi\right)}\psi_{tt} &=a\left(e^\left(\lambda\phi\right)\phi_x\right)_x  =m
\end{split} 
\begin{align}
a\left(e^\left(\lambda\phi\right)\phi_x\right)_x &=m \label{1}\tag{1}\\ \psi_{tt} &=me^\left(\lambda\psi\right)\label{2}\tag{2}
\end{align}
 
\begin{split}
a\left(e^\left(\lambda\phi\right)\phi_x\right)_x=m \implies 
\int a\left(e^\left(\lambda\phi\right)\phi_x\right)_x \,dx & = \int \frac{m}{a} \,dx\\ 
e^\left(\lambda\phi\right)\phi_x & =\frac{mx}{a}+c_1\\ 
\int e^\left(\lambda\phi\right) \,d\phi &= \int \left(\frac{mx}{a}+c_1\right) \,dx \\ 
\frac{1}{\lambda}e^\left(\lambda\phi\right)&=\frac{m}{2a}x^2+c_1x+c_2
\end{split}
 \phi(x)=\frac{1}{\lambda}\ln\left(\frac{\lambda m}{2a}x^2+\lambda c_1x+\lambda c_2\right) 
\begin{split}
\frac{\partial^2 \psi}{\partial t^2} &= me^\left(\lambda\psi\right) \\ 
\int \frac{\partial^2 \psi}{\partial t^2} \frac{d\psi}{dt} \,dt &=m\int e^\left(\lambda\psi\right) \frac{d\psi}{dt}\,dt\\ 
\frac{1}{2}\left(\frac{d\psi}{dt}\right)^2 &= me^\left(\lambda\psi\right)+c 
\end{split}
 
\int \frac{1}{\sqrt{2me^\left(\lambda\psi\right)+c_1}}\,d\psi = t+k 
 
\begin{align}
\frac{-2}{\lambda\sqrt{c_1}}\ln\left(\frac{\sqrt{c_1} + \sqrt{2me^\left(\lambda\psi\right)+c_1}}{\sqrt{2c_1-2me^\left(\lambda*\psi\right)}}\right)\label{I}\tag{I}&\\
\frac{-2}{\lambda\sqrt{2me^\left(\lambda\psi\right)+c_1}} & \label{II}\tag{II} \\ \frac{-1}{\lambda\sqrt c_1}\ln\left(\frac{\sqrt{c_1} +w}{\sqrt{c_1} -w}\right)&\text{ where }w=\sqrt{2me^\left(\lambda\psi\right)+c_1}\label{3}\tag{II}
\end{align}
 (2)","['analysis', 'partial-differential-equations', 'nonlinear-system', 'wave-equation']"
44,Is there an intuitive way to view the definition of the McShane integral?,Is there an intuitive way to view the definition of the McShane integral?,,"To my understanding the definition of the McShane integral is identical to the definition of the Henstock–Kurzweil integral with the exception that each tag does not have to be contained in the subinterval for which it is assigned to. However, the gauge function in general will force the corresponding subinterval to be relatively close to the tag. Still, the tag can be picked a bit outside of the subinterval. If I am not mistakes this implies that a function is McShane integrable if and only if it is absolutely integrable. This is not the case for the Henstock–Kurzweil integral. I do not understand how such a subtle modification of the definition of the Henstock–Kurzweil integral makes every McShane integrable function absolutely McShane integrable. By functions I am simply refering to functions on $\mathbb{R}$ for the sake of simplicity. Is there an intuitive way to view the differences between the aforementioned integrals and in particular why McShane integrable functions are absolutely integrable?","To my understanding the definition of the McShane integral is identical to the definition of the Henstock–Kurzweil integral with the exception that each tag does not have to be contained in the subinterval for which it is assigned to. However, the gauge function in general will force the corresponding subinterval to be relatively close to the tag. Still, the tag can be picked a bit outside of the subinterval. If I am not mistakes this implies that a function is McShane integrable if and only if it is absolutely integrable. This is not the case for the Henstock–Kurzweil integral. I do not understand how such a subtle modification of the definition of the Henstock–Kurzweil integral makes every McShane integrable function absolutely McShane integrable. By functions I am simply refering to functions on for the sake of simplicity. Is there an intuitive way to view the differences between the aforementioned integrals and in particular why McShane integrable functions are absolutely integrable?",\mathbb{R},"['real-analysis', 'integration', 'analysis', 'lebesgue-integral', 'gauge-integral']"
45,Prove Fubini's Theorem for non-negative functions,Prove Fubini's Theorem for non-negative functions,,"The objective is to prove the complete Fubini's Theorem $$\int_\mathbb{R}\left(\int_\mathbb{R}f(x,y)dy\right)dx=\int_\mathbb{R^2}f$$ where $f : \mathbb{R^2} \rightarrow \mathbb{R}$ is a Lebesgue absolutely integrable function. Since the Lebesgue integration is defined by positive and negative parts (i.e. $\int_\Omega f=\int_\Omega f^+ - \int_\Omega f^-$ where $f^+=max(f,0)$ and $f^-=-min(f,0)$ ). It suffices to prove the theorem for non-negative functions, and use the linearity of non-negative integration to prove the general case. Next, it suffices to prove the theorem for non-negatively supported functions $f$ on $[-N,N]\times[-N,N]$ for some positive integer N then take suprema $$f=\sup_{N>0}f1_{[-N,N]\times[-N,N]}$$ and apply the monotone convergence theorem ( $\sup\int f_n=\int\sup f_n$ where $f_n(x)$ is monotone increasing on $n$ for any fixed $x$ ). Similarly, the integration of non-negative functions is defined as $$\int_\Omega f=\sup\left\{\int_\Omega s:\text{s is a simple function and minorizes f}\right\}$$ I think I could use the monotone convergence theorem again. But it applies to sequences of functions, not sets of functions. If I were to construct a sequence, the subscript would depend on $x$ and I don't know how to proceed.","The objective is to prove the complete Fubini's Theorem where is a Lebesgue absolutely integrable function. Since the Lebesgue integration is defined by positive and negative parts (i.e. where and ). It suffices to prove the theorem for non-negative functions, and use the linearity of non-negative integration to prove the general case. Next, it suffices to prove the theorem for non-negatively supported functions on for some positive integer N then take suprema and apply the monotone convergence theorem ( where is monotone increasing on for any fixed ). Similarly, the integration of non-negative functions is defined as I think I could use the monotone convergence theorem again. But it applies to sequences of functions, not sets of functions. If I were to construct a sequence, the subscript would depend on and I don't know how to proceed.","\int_\mathbb{R}\left(\int_\mathbb{R}f(x,y)dy\right)dx=\int_\mathbb{R^2}f f : \mathbb{R^2} \rightarrow \mathbb{R} \int_\Omega f=\int_\Omega f^+ - \int_\Omega f^- f^+=max(f,0) f^-=-min(f,0) f [-N,N]\times[-N,N] f=\sup_{N>0}f1_{[-N,N]\times[-N,N]} \sup\int f_n=\int\sup f_n f_n(x) n x \int_\Omega f=\sup\left\{\int_\Omega s:\text{s is a simple function and minorizes f}\right\} x","['real-analysis', 'analysis']"
46,Weird mistake that I cannot spot in a proof,Weird mistake that I cannot spot in a proof,,"I was doing this exercise: Suppose that $(X,\mathcal{S},\mu )$ is a measure space, $1<p<\infty $ and $f,g\in \mathcal{L}^p(\mu )$ . Prove that Minkowski's s inequality is an equality if and only if there exists non-negative numebrs $a$ and $b$ , not both zero, such that $af(x)=bg(x)$ almost everywhere. My wrong proof below: We want to show that if $\left(\int |f+g|^p\right)^{1/p}=\left(\int |f|^p\right)^{1/p}+\left(\int |g|^p\right)^{1/p}$ then there are some $a,b\geqslant 0$ , not both zero, such that $af(x)=bg(x)$ a.e. WLOG we can assume that $\|f+g\|_p=1$ and $f,g\neq 0$ a.e., therefore $\|f\|_p,\|g\|_p\in(0,1)$ and so there is some constant $b>0$ such that $b\|f\|_p=\|g\|_p$ , but this means that $$ \int b^p|f|^p \,\mathrm d  \mu=\int |g|^p \,\mathrm d \mu \implies |bf|=|g|\text{ a.e. }\tag1 $$ Then there is some measurable function $h:X\to \Bbb F$ such that $|h(x)|=1$ a.e. and $b fh=g$ , thus we want to show that $h=1$ a.e. Now note that $$ \|f+g\|_p=\|f(1+hb)\|_p\quad \,\land\,\quad  \|f\|_p+\|g\|_p=(1+b)\|f\|_p\\ \therefore\, \|f(1+bh)\|_p=\|f(1+b)\|_p\implies |1+h(x)b|=1+b \,\text{ a.e. }\tag2 $$ Because $b>0$ and $|h(x)|=1$ a.e. then is easy to conclude that $h=1$ a.e., finishing one direction of the proof. The other direction is easy to see due to the homogeneity of $\|{\cdot}\|_p$ , so we are done. $\Box$ However above I didnt used the fact that $p\in (1,\infty )$ , indeed the fake proof above seems to hold for any chosen $p>0$ . However it doesn't hold for $p=1$ because if $f$ and $g$ are non-negative then is easy to check that $\|f+g\|_1=\|f\|_1+\|g\|_1$ , but in general $f$ and $g$ doesn't need to be proportional a.e. And probably the statement doesn't hold either when $p\in (0,1)$ . Where is my mistake in the above proof? EDIT: I see my mistake... It is the assertion that $|bf|=|g|$ a.e. I forget that $\int h\,\mathrm d \mu =0\Rightarrow h=0$ a.e. just holds when $h=|h|$ .","I was doing this exercise: Suppose that is a measure space, and . Prove that Minkowski's s inequality is an equality if and only if there exists non-negative numebrs and , not both zero, such that almost everywhere. My wrong proof below: We want to show that if then there are some , not both zero, such that a.e. WLOG we can assume that and a.e., therefore and so there is some constant such that , but this means that Then there is some measurable function such that a.e. and , thus we want to show that a.e. Now note that Because and a.e. then is easy to conclude that a.e., finishing one direction of the proof. The other direction is easy to see due to the homogeneity of , so we are done. However above I didnt used the fact that , indeed the fake proof above seems to hold for any chosen . However it doesn't hold for because if and are non-negative then is easy to check that , but in general and doesn't need to be proportional a.e. And probably the statement doesn't hold either when . Where is my mistake in the above proof? EDIT: I see my mistake... It is the assertion that a.e. I forget that a.e. just holds when .","(X,\mathcal{S},\mu ) 1<p<\infty  f,g\in \mathcal{L}^p(\mu ) a b af(x)=bg(x) \left(\int |f+g|^p\right)^{1/p}=\left(\int |f|^p\right)^{1/p}+\left(\int |g|^p\right)^{1/p} a,b\geqslant 0 af(x)=bg(x) \|f+g\|_p=1 f,g\neq 0 \|f\|_p,\|g\|_p\in(0,1) b>0 b\|f\|_p=\|g\|_p 
\int b^p|f|^p \,\mathrm d  \mu=\int |g|^p \,\mathrm d \mu \implies |bf|=|g|\text{ a.e. }\tag1
 h:X\to \Bbb F |h(x)|=1 b fh=g h=1 
\|f+g\|_p=\|f(1+hb)\|_p\quad \,\land\,\quad  \|f\|_p+\|g\|_p=(1+b)\|f\|_p\\
\therefore\, \|f(1+bh)\|_p=\|f(1+b)\|_p\implies |1+h(x)b|=1+b \,\text{ a.e. }\tag2
 b>0 |h(x)|=1 h=1 \|{\cdot}\|_p \Box p\in (1,\infty ) p>0 p=1 f g \|f+g\|_1=\|f\|_1+\|g\|_1 f g p\in (0,1) |bf|=|g| \int h\,\mathrm d \mu =0\Rightarrow h=0 h=|h|","['analysis', 'lebesgue-integral', 'lp-spaces', 'fake-proofs']"
47,How to calculate Arithmetic–logarithmic mean?,How to calculate Arithmetic–logarithmic mean?,,"As is known to us, the limit of the sequence $\{a_n\},\{b_n\}\ $ where $\ a_0=a,b_0=b,a_{n+1}=\dfrac{a_n+b_n}{2},b_{n+1}=\sqrt{a_n b_n}\ $ is related to the Arithmetic–geometric mean of $a,b$ , which can be expressed as below $AG(a,b)=\dfrac{\pi}{2I(a,b)}$ , where $\displaystyle I(a,b)=\int_{0}^{\frac{\pi}{2}}\frac{d\theta}{\sqrt{a^2\cos^2\theta+b^2\sin^2\theta}}$ Now I wonder whether we can calculate the limit of the following sequence $\ a_0=a,b_0=b,a_{n+1}=\dfrac{a_n+b_n}{2},b_{n+1}=\dfrac{a_n-b_n}{\ln a_n-\ln b_n}$ (without the loss of generality we can assume $a>b>0$ ) and I'd like to call this limit the Arithmetic–logarithmic mean Anyone can solve this?","As is known to us, the limit of the sequence where is related to the Arithmetic–geometric mean of , which can be expressed as below , where Now I wonder whether we can calculate the limit of the following sequence (without the loss of generality we can assume ) and I'd like to call this limit the Arithmetic–logarithmic mean Anyone can solve this?","\{a_n\},\{b_n\}\  \ a_0=a,b_0=b,a_{n+1}=\dfrac{a_n+b_n}{2},b_{n+1}=\sqrt{a_n b_n}\  a,b AG(a,b)=\dfrac{\pi}{2I(a,b)} \displaystyle I(a,b)=\int_{0}^{\frac{\pi}{2}}\frac{d\theta}{\sqrt{a^2\cos^2\theta+b^2\sin^2\theta}} \ a_0=a,b_0=b,a_{n+1}=\dfrac{a_n+b_n}{2},b_{n+1}=\dfrac{a_n-b_n}{\ln a_n-\ln b_n} a>b>0","['real-analysis', 'analysis', 'means']"
48,Example of a quasi-open set and how the capacity of the set fits in it?,Example of a quasi-open set and how the capacity of the set fits in it?,,"While going through some research papers, I came across a result of D. Bucur where the existence of a minimizer for the general $k^{th}$ eigenvalue of the Dirichlet Laplacian among a class of quasi-open sets with fixed measure was shown. I have looked into the definition of quasi-open sets from A. Henrot, Extremum problems for eigenvalues of elliptic operator . But I was unable to find any insight as to what kind of sets one might expect to be quasi-open. First, in Definition 2.4.3, they defined the capacity of a set as Let $D$ be a bounded open set in $\mathbb{R}^n$ . For any compact subset $K$ in $D$ , define $$\text{cap}_D(K)= \inf \left\{\int_D|\nabla v|^2: v\in C_0^{\infty}, v\geq 1 \text{ surrounding } K \right\}$$ The definition can be extended for an open subset $\omega$ of $D$ as $$\text{cap}_D(\omega):= \sup \{\text{cap}_D(K): K \text{ compact, } K\subset \omega\}$$ Quasi-open sets as in Definition 2.4.4 is defined as follows, A subset $\Omega$ of $D$ (a bounded open set in $\mathbb{R}^n$ ) is quasi-open if there exist a decreasing sequence of open sets $\omega_n$ such that $$\displaystyle \lim_{n\to +\infty} \text{cap}_D(\omega_n)=0$$ where $\Omega \cup \omega_n$ is open for all $n$ and $\text{cap}_D(\omega_n)$ is the capacity of $\omega_n$ relative to $D$ . I would love to get an insight on how the capacity of a set fits in the definition of quasi-open sets. Also, it would be helpful if I can get a detailed example of a quasi-open set or a reference to find one. I would also appreciate if someone can provide me with some intuition as to how to look at the idea of capacity of a set and which aspect of a set does it actually measure.","While going through some research papers, I came across a result of D. Bucur where the existence of a minimizer for the general eigenvalue of the Dirichlet Laplacian among a class of quasi-open sets with fixed measure was shown. I have looked into the definition of quasi-open sets from A. Henrot, Extremum problems for eigenvalues of elliptic operator . But I was unable to find any insight as to what kind of sets one might expect to be quasi-open. First, in Definition 2.4.3, they defined the capacity of a set as Let be a bounded open set in . For any compact subset in , define The definition can be extended for an open subset of as Quasi-open sets as in Definition 2.4.4 is defined as follows, A subset of (a bounded open set in ) is quasi-open if there exist a decreasing sequence of open sets such that where is open for all and is the capacity of relative to . I would love to get an insight on how the capacity of a set fits in the definition of quasi-open sets. Also, it would be helpful if I can get a detailed example of a quasi-open set or a reference to find one. I would also appreciate if someone can provide me with some intuition as to how to look at the idea of capacity of a set and which aspect of a set does it actually measure.","k^{th} D \mathbb{R}^n K D \text{cap}_D(K)= \inf \left\{\int_D|\nabla v|^2: v\in C_0^{\infty}, v\geq 1 \text{ surrounding } K \right\} \omega D \text{cap}_D(\omega):= \sup \{\text{cap}_D(K): K \text{ compact, } K\subset \omega\} \Omega D \mathbb{R}^n \omega_n \displaystyle \lim_{n\to +\infty} \text{cap}_D(\omega_n)=0 \Omega \cup \omega_n n \text{cap}_D(\omega_n) \omega_n D","['general-topology', 'analysis', 'reference-request', 'partial-differential-equations', 'metric-geometry']"
49,A counterexample in measure theory on $\sigma$-infinite spaces,A counterexample in measure theory on -infinite spaces,\sigma,"Usually measure theory books include the following theorem (citing Proposition 5.1.3 in Cohn's measure theory book) Let $(X, \mathcal A , \mu )$ and $(Y, \mathcal B, \nu )$ be $\sigma$ -finite measure spaces. If $E$ belongs to the $\sigma$ -algebra $\mathcal{A}\times\mathcal{B}$ , then the function $x\mapsto\nu(E_x)$ is $\mathcal{A}$ -measurable   and the function $y\mapsto\mu(E^y)$ is $\mathcal{B}$ -measurable. where $E_x=\{y\in Y\mid (x,y)\in E\}$ and $E^y=\{x\in X\mid (x,y)\in E\}$ are the slices of $E$ . I'm looking for an example of two spaces (with at least one of them necessarily not $\sigma$ -finite) for which this theorem fails. I believe one of the two needs to be not only $\sigma$ -infinite but also s-infinite. If this is not the case I would be very interested in an example involving an s-finite but not $\sigma$ -finite space.","Usually measure theory books include the following theorem (citing Proposition 5.1.3 in Cohn's measure theory book) Let and be -finite measure spaces. If belongs to the -algebra , then the function is -measurable   and the function is -measurable. where and are the slices of . I'm looking for an example of two spaces (with at least one of them necessarily not -finite) for which this theorem fails. I believe one of the two needs to be not only -infinite but also s-infinite. If this is not the case I would be very interested in an example involving an s-finite but not -finite space.","(X, \mathcal A , \mu ) (Y, \mathcal B, \nu ) \sigma E \sigma \mathcal{A}\times\mathcal{B} x\mapsto\nu(E_x) \mathcal{A} y\mapsto\mu(E^y) \mathcal{B} E_x=\{y\in Y\mid (x,y)\in E\} E^y=\{x\in X\mid (x,y)\in E\} E \sigma \sigma \sigma","['analysis', 'measure-theory', 'examples-counterexamples']"
50,$2 \cdot \int_0^1 \log\big(\Gamma(x)\big) \cdot \sin(2 \pi n x) dx = \frac{\gamma + \log(2 \pi) + \log(n)}{n \pi}$,,2 \cdot \int_0^1 \log\big(\Gamma(x)\big) \cdot \sin(2 \pi n x) dx = \frac{\gamma + \log(2 \pi) + \log(n)}{n \pi},"I want to proof the series of kummer. Therefore I want to show $$2 \cdot \int_0^1 \log\big(\Gamma(x)\big) \cdot \sin(2 \pi n x) dx = \frac{\gamma + \log(2 \pi) + \log(n)}{n \pi},$$ where $\gamma$ is the Euler-Mascheroni-constant. Any help would be appreciated. Thanks in advance.",I want to proof the series of kummer. Therefore I want to show where is the Euler-Mascheroni-constant. Any help would be appreciated. Thanks in advance.,"2 \cdot \int_0^1 \log\big(\Gamma(x)\big) \cdot \sin(2 \pi n x) dx = \frac{\gamma + \log(2 \pi) + \log(n)}{n \pi}, \gamma","['analysis', 'logarithms', 'fourier-series', 'gamma-function']"
51,How many fixed points can a function have?,How many fixed points can a function have?,,"For dimension one, it is easy to think in samples of continuous functions $f:[a,b]\rightarrow [a,b]$ with one, two, three,... fixed points. Or even, infinitely fixed points (take the idendity map).  But I am intereseted in the following result that one old teacher told me: Theorem: For any given non-empty and closed subset $C\subseteq \overline{B}_{\mathbb{R}^n}(0,r)$ , there exists a continuous function $f:\overline{B}_{\mathbb{R}^n}(0,r)\rightarrow\overline{B}_{\mathbb{R}^n}(0,r)$ , where $\overline{B}_{\mathbb{R}^n}(0,r)$ is the closed ball of radius $r>0$ centered at $0$ , such that the set of fixed points of $f$ is exactly the set $C$ . Can anyone give me some idea or hint to prove it? My attempts have not worked sucessfully. Thanks in advance.","For dimension one, it is easy to think in samples of continuous functions with one, two, three,... fixed points. Or even, infinitely fixed points (take the idendity map).  But I am intereseted in the following result that one old teacher told me: Theorem: For any given non-empty and closed subset , there exists a continuous function , where is the closed ball of radius centered at , such that the set of fixed points of is exactly the set . Can anyone give me some idea or hint to prove it? My attempts have not worked sucessfully. Thanks in advance.","f:[a,b]\rightarrow [a,b] C\subseteq \overline{B}_{\mathbb{R}^n}(0,r) f:\overline{B}_{\mathbb{R}^n}(0,r)\rightarrow\overline{B}_{\mathbb{R}^n}(0,r) \overline{B}_{\mathbb{R}^n}(0,r) r>0 0 f C","['general-topology', 'analysis', 'fixed-point-theorems']"
52,Proof of second Fundamental theorem of calculus,Proof of second Fundamental theorem of calculus,,"Is there any other proof of this? Second fundamental Theorem of Calculus: If $f$ is differentiable on $[a,b]$ and $f'$ is integrable on $[a,b]$ , then $$\int^{b}_a f'(t)dt=f(b)-f(a)$$ My proof Since $f$ is differentiable on [a,b], then $f'(t)$ exists for all $t\in[a,b]$ . For each $n\in \Bbb{N}$ , let $P_n$ be an arbitrary partition such that $$a=x_0<x_1<\cdots<x_n=b.$$ Since $f$ is differentiable on $[a,b]$ , then $f$ is continuous on $(a,b).$ So, by Mean Value Theorem, there exists $t_i\in [x_{i-1},x_i]$ , for $1\leq i\leq n$ such that $$ f(x_i)-f(x_{i-1})=f'(t_i)(x_{i-1}-x_i).$$ Summing these up, we have $$ f(b)-f(a)=\sum^{n}_{i=1}\left[f(x_i)-f(x_{i-1})\right]=\sum^{n}_{i=1}f'(t_i)(x_{i-1}-x_i).$$ Integrability of $f'$ on $[a,b]$ implies that $$ \sum^{n}_{i=1}m^{f'}_i(x_{i-1}-x_i)\leq \sum^{n}_{i=1}f'(t_i)(x_{i-1}-x_i)\leq \sum^{n}_{i=1}M^{f'}_i(x_{i-1}-x_i),$$ which implies that $$ L(f',P_n)=\sum^{n}_{i=1}m^{f'}_i(x_{i-1}-x_i)\leq f(b)-f(a)\leq \sum^{n}_{i=1}M^{f'}_i(x_{i-1}-x_i)=U(f',P_n),$$ As $n\to\infty,$ $$\lim\limits_{n\to\infty} L(f',P_n)=\lim\limits_{n\to\infty} U(f',P_n)=\int^{b}_a f'(t)dt.$$ Therefore, $$\int^{b}_a f'(t)dt=f(b)-f(a)$$","Is there any other proof of this? Second fundamental Theorem of Calculus: If is differentiable on and is integrable on , then My proof Since is differentiable on [a,b], then exists for all . For each , let be an arbitrary partition such that Since is differentiable on , then is continuous on So, by Mean Value Theorem, there exists , for such that Summing these up, we have Integrability of on implies that which implies that As Therefore,","f [a,b] f' [a,b] \int^{b}_a f'(t)dt=f(b)-f(a) f f'(t) t\in[a,b] n\in \Bbb{N} P_n a=x_0<x_1<\cdots<x_n=b. f [a,b] f (a,b). t_i\in [x_{i-1},x_i] 1\leq i\leq n  f(x_i)-f(x_{i-1})=f'(t_i)(x_{i-1}-x_i).  f(b)-f(a)=\sum^{n}_{i=1}\left[f(x_i)-f(x_{i-1})\right]=\sum^{n}_{i=1}f'(t_i)(x_{i-1}-x_i). f' [a,b]  \sum^{n}_{i=1}m^{f'}_i(x_{i-1}-x_i)\leq \sum^{n}_{i=1}f'(t_i)(x_{i-1}-x_i)\leq \sum^{n}_{i=1}M^{f'}_i(x_{i-1}-x_i),  L(f',P_n)=\sum^{n}_{i=1}m^{f'}_i(x_{i-1}-x_i)\leq f(b)-f(a)\leq \sum^{n}_{i=1}M^{f'}_i(x_{i-1}-x_i)=U(f',P_n), n\to\infty, \lim\limits_{n\to\infty} L(f',P_n)=\lim\limits_{n\to\infty} U(f',P_n)=\int^{b}_a f'(t)dt. \int^{b}_a f'(t)dt=f(b)-f(a)","['real-analysis', 'calculus', 'analysis', 'proof-verification', 'riemann-integration']"
53,Find $(1-p)\log\left [\int^{\infty}_{0} [f(x)]^pdx\right]$,Find,(1-p)\log\left [\int^{\infty}_{0} [f(x)]^pdx\right],"I want to find \begin{align}(1-p)\log\left [\int^{\infty}_{0} [f(x)]^pdx\right],\end{align} given that $\alpha,\beta, \theta$ and $p$ are constants and \begin{align} f(x)=\dfrac{ \frac{ \theta\alpha \beta}{x^2}\left( 1+\frac{ \beta}{x}  \right) ^{-(1-\alpha)} }{\Big\{ 1-(1-\theta)\left[1-\left( 1+\frac{ \beta}{x}  \right) ^{-\alpha}   \right] \Big\}^2},\;x\in\Bbb{R}\end{align} MY TRIAL By substitution, let $u=1+\frac{ \beta}{x}, $ then $du=-[(u-1)^2/\beta]dx$ \begin{align} f(u)&=\dfrac{ \frac{ \theta\alpha }{\beta}(u-1)^2u ^{-(1-\alpha)} }{\Big\{ 1-(1-\theta)\left[1-u ^{-\alpha}   \right] \Big\}^2}\\&=\dfrac{ \frac{ \theta\alpha }{\beta}(u-1)^2u ^{-(1-\alpha)} }{ 1-2(1-\theta)\left[1-u ^{-\alpha}   \right]+(1-\theta)^2\left[1-u ^{-\alpha}   \right] ^2}\end{align} I'm stuck here, please, how do I continue?","I want to find given that and are constants and MY TRIAL By substitution, let then I'm stuck here, please, how do I continue?","\begin{align}(1-p)\log\left [\int^{\infty}_{0} [f(x)]^pdx\right],\end{align} \alpha,\beta, \theta p \begin{align} f(x)=\dfrac{ \frac{ \theta\alpha \beta}{x^2}\left( 1+\frac{ \beta}{x}  \right) ^{-(1-\alpha)} }{\Big\{ 1-(1-\theta)\left[1-\left( 1+\frac{ \beta}{x}  \right) ^{-\alpha}   \right] \Big\}^2},\;x\in\Bbb{R}\end{align} u=1+\frac{ \beta}{x},  du=-[(u-1)^2/\beta]dx \begin{align} f(u)&=\dfrac{ \frac{ \theta\alpha }{\beta}(u-1)^2u ^{-(1-\alpha)} }{\Big\{ 1-(1-\theta)\left[1-u ^{-\alpha}   \right] \Big\}^2}\\&=\dfrac{ \frac{ \theta\alpha }{\beta}(u-1)^2u ^{-(1-\alpha)} }{ 1-2(1-\theta)\left[1-u ^{-\alpha}   \right]+(1-\theta)^2\left[1-u ^{-\alpha}   \right] ^2}\end{align}","['integration', 'analysis', 'probability-distributions']"
54,Erdos-Kac implies the prime number theorem?,Erdos-Kac implies the prime number theorem?,,"I am curious as to whether the Erdos-Kac theorem can imply a lower bound on the density of primes, or perhaps imply the (weaker) infinitude of prime numbers all in all. It seems as if the Erdos-Kac theorem should imply that the probability of having only $1$ prime factor should be roughly $1/\log n$ , however, extrapolating this information from the assumption that the distribution is roughly Gaussian, and then calculating the probability of that event over the Gaussian random variable could lead one to think that the probability of having 0, or a negative number of prime divisors is also roughly $1/\log n$ , which doesn't even make sense. Most likely, this implies that the convergence to Gaussian is slower in the distribution's tail, and that in particular there should be some error term roughly of that size. Can anyone give me some direction as to how to think about the relation between the two theorems? And what kind of actual information can be extrapolated from the normal limit distribution, implied by the Erdos-Kac theorem, regarding the density of primes? Ciao","I am curious as to whether the Erdos-Kac theorem can imply a lower bound on the density of primes, or perhaps imply the (weaker) infinitude of prime numbers all in all. It seems as if the Erdos-Kac theorem should imply that the probability of having only prime factor should be roughly , however, extrapolating this information from the assumption that the distribution is roughly Gaussian, and then calculating the probability of that event over the Gaussian random variable could lead one to think that the probability of having 0, or a negative number of prime divisors is also roughly , which doesn't even make sense. Most likely, this implies that the convergence to Gaussian is slower in the distribution's tail, and that in particular there should be some error term roughly of that size. Can anyone give me some direction as to how to think about the relation between the two theorems? And what kind of actual information can be extrapolated from the normal limit distribution, implied by the Erdos-Kac theorem, regarding the density of primes? Ciao",1 1/\log n 1/\log n,"['number-theory', 'analysis', 'prime-numbers', 'analytic-number-theory']"
55,"For non-differentiable functions, does fundamental theorem of calculus give the subgradient?","For non-differentiable functions, does fundamental theorem of calculus give the subgradient?",,"As we know, if $f:R→R$ is continuous, and $F(x)=\int_0^xf(y)dy$, then $F$ is differentiable and $F'(x)=f(x)$. (Fundamental Theorem of Calculus) Consider the case that $f$ is not continuous, let $F(x)=\int_0^xf(y)dy$. Assume $f$ is increasing thus $F$ is convex. Now $F$ doesn't have to be differentiable but it has subgradients (subderivatives). Is there any relationship between $f$ and subgradient of $F$? Is it true? Can we prove this: $c$ is a subgradient of $F(x)$, if for any $y<x,f(y)<c$ and for any $y>x, f(y)>c$. Here is an example to justify this proposition. Suppose $f(x)=1$ for $x>=0$, $f(x)=-1$ for $x<0$. $F(x)=\int_0^xf(y)dy=|x|$. The proposition holds in this example.","As we know, if $f:R→R$ is continuous, and $F(x)=\int_0^xf(y)dy$, then $F$ is differentiable and $F'(x)=f(x)$. (Fundamental Theorem of Calculus) Consider the case that $f$ is not continuous, let $F(x)=\int_0^xf(y)dy$. Assume $f$ is increasing thus $F$ is convex. Now $F$ doesn't have to be differentiable but it has subgradients (subderivatives). Is there any relationship between $f$ and subgradient of $F$? Is it true? Can we prove this: $c$ is a subgradient of $F(x)$, if for any $y<x,f(y)<c$ and for any $y>x, f(y)>c$. Here is an example to justify this proposition. Suppose $f(x)=1$ for $x>=0$, $f(x)=-1$ for $x<0$. $F(x)=\int_0^xf(y)dy=|x|$. The proposition holds in this example.",,"['calculus', 'analysis', 'subgradient']"
56,What is a relation between energy space and $L^p_s-$Sobolev spaces?,What is a relation between energy space and Sobolev spaces?,L^p_s-,"We define energy space  $$E= \left\{ f\in \mathcal{S}'(\mathbb R^d):\|\nabla f\|_{L^2} + \|xf\|_{L^2} < \infty \right\}.$$ and Sobolev spaces  $$L^p_s(\mathbb R^d)=\{f\in \mathcal{S}'(\mathbb R^d): \mathcal{F}^{-1} [\langle \cdot \rangle^s \mathcal{F}(f)] \in L^p(\mathbb R^d) \}$$ where $\langle \cdot \rangle = (1+ |\cdot|^2)^{1/2}, s\in \mathbb R,$ and $\mathcal{F}$ and $\mathcal{F}^{-1}$ are Fourier transform and the inverse Fourier transform. My Question is: Is there any relation between $E$ and $L^p_s (p\neq 2)$? Specifically, I'm interested to know the relation between $E$ and $L^p_s$ for $s>d(\frac{2}{p}-1), 1\leq p <2$? Side Thoughts : I think there is no any relations between $L^p-$spaces on $\mathbb R^d,$ I'm guessing there might no relations between $E$ and $L^{p}_s (p\neq 2).$ But I do not know how to construct counter examples? Motivation: This spaces appear in PDE very often. Rough Ideas in constructing counter example : Define $$f_p(x)= \sum_{k\neq 0} |k|^{-\frac{d}{p}-\epsilon} e^{ik\cdot x} e^{-|x|^2}$$ in $\mathcal{S}'(\mathbb R^d) \ (1< p<2).$ Can we say that $f_p\in E$? For simplicity  we consider one dimension case, that is, $d=1.$ We note that $$\nabla f(x) =f'(x)= \sum_{k\neq 0} |k|^{-\frac{d}{p}-\epsilon} e^{ik\cdot x} e^{-|x|^2}=\sum_{k\neq 0} |k|^{-\frac{d}{p}-\epsilon} (-2|x|e^{ik\cdot x} e^{-|x|^2} + ik e^{ik\cdot x} e^{-|x|^2} ) $$ Now $\|\nabla f\|_{L^2}^2 =  \int_{\mathbb R} \left|\sum_{k\in \mathbb Z \setminus \{0\} } |k|^{-\frac{d}{p}-\epsilon} (-2|x|e^{ik\cdot x} e^{-|x|^2} + ik e^{ik\cdot x} e^{-|x|^2} ) \right|^2 dx$ I do not know how to proceed but my guess is there might exists $1<p<2$ such that $f_p \notin E.$","We define energy space  $$E= \left\{ f\in \mathcal{S}'(\mathbb R^d):\|\nabla f\|_{L^2} + \|xf\|_{L^2} < \infty \right\}.$$ and Sobolev spaces  $$L^p_s(\mathbb R^d)=\{f\in \mathcal{S}'(\mathbb R^d): \mathcal{F}^{-1} [\langle \cdot \rangle^s \mathcal{F}(f)] \in L^p(\mathbb R^d) \}$$ where $\langle \cdot \rangle = (1+ |\cdot|^2)^{1/2}, s\in \mathbb R,$ and $\mathcal{F}$ and $\mathcal{F}^{-1}$ are Fourier transform and the inverse Fourier transform. My Question is: Is there any relation between $E$ and $L^p_s (p\neq 2)$? Specifically, I'm interested to know the relation between $E$ and $L^p_s$ for $s>d(\frac{2}{p}-1), 1\leq p <2$? Side Thoughts : I think there is no any relations between $L^p-$spaces on $\mathbb R^d,$ I'm guessing there might no relations between $E$ and $L^{p}_s (p\neq 2).$ But I do not know how to construct counter examples? Motivation: This spaces appear in PDE very often. Rough Ideas in constructing counter example : Define $$f_p(x)= \sum_{k\neq 0} |k|^{-\frac{d}{p}-\epsilon} e^{ik\cdot x} e^{-|x|^2}$$ in $\mathcal{S}'(\mathbb R^d) \ (1< p<2).$ Can we say that $f_p\in E$? For simplicity  we consider one dimension case, that is, $d=1.$ We note that $$\nabla f(x) =f'(x)= \sum_{k\neq 0} |k|^{-\frac{d}{p}-\epsilon} e^{ik\cdot x} e^{-|x|^2}=\sum_{k\neq 0} |k|^{-\frac{d}{p}-\epsilon} (-2|x|e^{ik\cdot x} e^{-|x|^2} + ik e^{ik\cdot x} e^{-|x|^2} ) $$ Now $\|\nabla f\|_{L^2}^2 =  \int_{\mathbb R} \left|\sum_{k\in \mathbb Z \setminus \{0\} } |k|^{-\frac{d}{p}-\epsilon} (-2|x|e^{ik\cdot x} e^{-|x|^2} + ik e^{ik\cdot x} e^{-|x|^2} ) \right|^2 dx$ I do not know how to proceed but my guess is there might exists $1<p<2$ such that $f_p \notin E.$",,"['analysis', 'partial-differential-equations', 'fourier-analysis', 'examples-counterexamples']"
57,Derivative of a monotone function discontinuous only at rational numbers,Derivative of a monotone function discontinuous only at rational numbers,,"Let $$f\left(x\right)=\sum_{n=1}^\infty \frac{\left[nx\right]}{2^n}.$$ where $\left[•\right]$ is the floor function. I proved that it is convergent for every real number $x$, and $f$ is strictly increasing, continuous at irrational points and discontinuous at rational points. Since monotonity, $f$ must be differentiable almost everywhere. I guess it is differentiable at irrational points, and the derivative is $0$. My question is: Does $f’\left(s\right)$ exists for every irrational number $s$? If it exists, is it equal to $0$?","Let $$f\left(x\right)=\sum_{n=1}^\infty \frac{\left[nx\right]}{2^n}.$$ where $\left[•\right]$ is the floor function. I proved that it is convergent for every real number $x$, and $f$ is strictly increasing, continuous at irrational points and discontinuous at rational points. Since monotonity, $f$ must be differentiable almost everywhere. I guess it is differentiable at irrational points, and the derivative is $0$. My question is: Does $f’\left(s\right)$ exists for every irrational number $s$? If it exists, is it equal to $0$?",,"['real-analysis', 'sequences-and-series', 'analysis']"
58,Why are all analytic functions equivalent to their Taylor series?,Why are all analytic functions equivalent to their Taylor series?,,"""A function is analytic if and only if its Taylor series about $x_0$ converges to the function in some neighborhood for every $x_0$ in its domain."" Clearly if its Taylor series converges to $f$ then the function is analytic, but why is the converse true? I would really appreciate any help/thoughts.","""A function is analytic if and only if its Taylor series about $x_0$ converges to the function in some neighborhood for every $x_0$ in its domain."" Clearly if its Taylor series converges to $f$ then the function is analytic, but why is the converse true? I would really appreciate any help/thoughts.",,"['real-analysis', 'analysis', 'power-series', 'taylor-expansion', 'analyticity']"
59,"Theorem 5.16 in Apostol's MATHEMATICAL ANALYSIS, 2nd ed: Intermediate-Value Theorem for Derivatives","Theorem 5.16 in Apostol's MATHEMATICAL ANALYSIS, 2nd ed: Intermediate-Value Theorem for Derivatives",,"Here is Theorem 5.16 (intermediate-value theorem for derivatives) in the book Mathematical Analysis by Tom M. Apostol, 2nd edition: Assume that $f$ is defined on a compact interval $[a, b]$ and that $f$ has a derivative (finite or infinite) at each interior point. Assume also that $f$ has finite one-sided derivatives $f^\prime_+(a)$ and $f^\prime_-(b)$ at the endpoints, with $f^\prime_+(a) \neq f^\prime_-(b)$. Then, if $c$ is a real number between $f^\prime_+(a)$ and $f^\prime_-(b)$, there exists at least one interior point $x$ such that $f^\prime(x) = c$. And, here is Apostol's proof: Define a new function $g$ as follows:    $$ g(x) = \frac{ f(x) - f(a) }{ x-a } \ \mbox{ if } x \neq a, \qquad  g(a) = f^\prime_+(a). $$   Then $g$ is continuous on the closed interval $[a, b]$. By the intermediate-value theorem for continuous functions, $g$ takes on every value between $f^\prime_+(a)$ and $[ f(b) - f(a) ]/(b-a)$ in the interior $(a, b)$. By the Mean-Value Theorem, we have $g(x) = f^\prime (k)$ for some $k$ in $(a, x)$ whenever $x \in (a, b)$. Therefore $f^\prime$ takes on every value between $f^\prime_+(a)$ and $[ f(b) - f(a) ]/( b-a )$ in the interior $(a, b)$. A similar argument applies to the function $h$, defined by   $$ h(x) = \frac{ f(x) - f(b) }{ x-b } \ \mbox{ if } x \neq b, \qquad h(b) = f^\prime_-(b), $$   shows that $f^\prime$ takes on every value between $[ f(b) - f(a) ]/(b-a)$ and $f^\prime_-(b)$ in the interior $(a, b)$. Combining these results, we see that $f^\prime$ takes on every value between $f^\prime_+(a)$ and $f^\prime_-(b)$ in the interior $(a, b)$, and this proves the theorem. What Apostol has not mentioned is the fact that $f^\prime$ does actually take on the value $[ f(b) - f(a) ]/(b-a)$ somewhere in $(a, b)$, by virtue of the Mean-Value Theorem. Am I right? Immediately following the above proof is this Note: Theorem 5.16 is still valid if one or both of the one-sided derivatives $f^\prime_+(a)$, $f^\prime_-(b)$, is infinite. The proof in this case can be given by considering the auxiliary function $g$ defined by the equation $ g(x) = f(x) - cx$, if $x \in [a, b]$. Details are left to the reader. Although I think I fully understand Apostol's proof of Theorem 5.16 as given above, I'm unable to figure out how to prove the result if either one or both of the one-sided derivatives $f^\prime_+(a)$ and $f^\prime_-(b)$ is infinite; I have no idea of how the new auxiliary function $g$ is going to be helpful in this regard.","Here is Theorem 5.16 (intermediate-value theorem for derivatives) in the book Mathematical Analysis by Tom M. Apostol, 2nd edition: Assume that $f$ is defined on a compact interval $[a, b]$ and that $f$ has a derivative (finite or infinite) at each interior point. Assume also that $f$ has finite one-sided derivatives $f^\prime_+(a)$ and $f^\prime_-(b)$ at the endpoints, with $f^\prime_+(a) \neq f^\prime_-(b)$. Then, if $c$ is a real number between $f^\prime_+(a)$ and $f^\prime_-(b)$, there exists at least one interior point $x$ such that $f^\prime(x) = c$. And, here is Apostol's proof: Define a new function $g$ as follows:    $$ g(x) = \frac{ f(x) - f(a) }{ x-a } \ \mbox{ if } x \neq a, \qquad  g(a) = f^\prime_+(a). $$   Then $g$ is continuous on the closed interval $[a, b]$. By the intermediate-value theorem for continuous functions, $g$ takes on every value between $f^\prime_+(a)$ and $[ f(b) - f(a) ]/(b-a)$ in the interior $(a, b)$. By the Mean-Value Theorem, we have $g(x) = f^\prime (k)$ for some $k$ in $(a, x)$ whenever $x \in (a, b)$. Therefore $f^\prime$ takes on every value between $f^\prime_+(a)$ and $[ f(b) - f(a) ]/( b-a )$ in the interior $(a, b)$. A similar argument applies to the function $h$, defined by   $$ h(x) = \frac{ f(x) - f(b) }{ x-b } \ \mbox{ if } x \neq b, \qquad h(b) = f^\prime_-(b), $$   shows that $f^\prime$ takes on every value between $[ f(b) - f(a) ]/(b-a)$ and $f^\prime_-(b)$ in the interior $(a, b)$. Combining these results, we see that $f^\prime$ takes on every value between $f^\prime_+(a)$ and $f^\prime_-(b)$ in the interior $(a, b)$, and this proves the theorem. What Apostol has not mentioned is the fact that $f^\prime$ does actually take on the value $[ f(b) - f(a) ]/(b-a)$ somewhere in $(a, b)$, by virtue of the Mean-Value Theorem. Am I right? Immediately following the above proof is this Note: Theorem 5.16 is still valid if one or both of the one-sided derivatives $f^\prime_+(a)$, $f^\prime_-(b)$, is infinite. The proof in this case can be given by considering the auxiliary function $g$ defined by the equation $ g(x) = f(x) - cx$, if $x \in [a, b]$. Details are left to the reader. Although I think I fully understand Apostol's proof of Theorem 5.16 as given above, I'm unable to figure out how to prove the result if either one or both of the one-sided derivatives $f^\prime_+(a)$ and $f^\prime_-(b)$ is infinite; I have no idea of how the new auxiliary function $g$ is going to be helpful in this regard.",,"['calculus', 'real-analysis', 'analysis', 'derivatives', 'proof-explanation']"
60,How to find where a Fourier series is discontinuous,How to find where a Fourier series is discontinuous,,"I have the Fourier series $$\sum_{n=1}^{\infty}\frac{\sin nx\sin^2n\alpha}{n}$$ which represents some function $f$ to be found, and I aim to determine the points at which the Fourier series is discontinuous. The answer is that it is some constant for $0<x<2\alpha$ and zero for $2\alpha<x<\pi.$ To do this I used the method of Stokes. I divided the interval $(0,\pi)$ at $n$ points $k_1,k_2,\dots,k_n,$ and if we break up the integrals for $a_n$ (the cosine coefficients) and $b_n$ (the sine coefficients) then we get $$na_n=A_n-b'_n,~nb_n=B_n+a'_n$$ where $a'_n$ and $b'_n$ represent the Fourier coefficients of the derivative, and $A_n,$ $B_n$ given by $$\pi A_n=\sum_{r=1}^{n}\{f(k_r^-)-f(k_r^+)\}\sin nk_r,~\pi B_n=-\sum_{r=1}^{n}\{f(k_r^-)-f(k_r^+)\}\cos nk_r$$ will vanish when $f$ is continuous at all points. I thought that since $f$ has a sine series it should be an odd function of $x$ and therefore its derivative should also be an odd function of $x.$ So that would mean $a'_n=0$ for all $n$ and therefore $nb_n=B_n=\sin^2n\alpha=\tfrac12(1-\cos2n\alpha),$ which seems to suggest that the points of discontinuity are only $k_1=2\alpha.$ Then if I take $$\pi A_n=\{f(2\alpha^-)-f(2\alpha^+)\}\sin2n\alpha$$ then if $0<2\alpha<\pi,$ the sine term cannot vanish, and I have just claimed that $f(2\alpha^-)\neq f(2\alpha^+),$ so this means $A_n\neq0.$ But then that demands $b'_n\neq0,$ and therefore a nonconstant $f.$ On the other hand, I can't see how to show that $b'_n=0$ for this Fourier series. Would someone be able to let me know how to see when a Fourier series is discontinuous when we know only the series, but not the function? I would appreciate either correcting any mistakes I have made (I am not a professional or a maths student and I'm aware this method might be outdated or cumbersome) or showing me a better way to tackle these sorts of problems.","I have the Fourier series $$\sum_{n=1}^{\infty}\frac{\sin nx\sin^2n\alpha}{n}$$ which represents some function $f$ to be found, and I aim to determine the points at which the Fourier series is discontinuous. The answer is that it is some constant for $0<x<2\alpha$ and zero for $2\alpha<x<\pi.$ To do this I used the method of Stokes. I divided the interval $(0,\pi)$ at $n$ points $k_1,k_2,\dots,k_n,$ and if we break up the integrals for $a_n$ (the cosine coefficients) and $b_n$ (the sine coefficients) then we get $$na_n=A_n-b'_n,~nb_n=B_n+a'_n$$ where $a'_n$ and $b'_n$ represent the Fourier coefficients of the derivative, and $A_n,$ $B_n$ given by $$\pi A_n=\sum_{r=1}^{n}\{f(k_r^-)-f(k_r^+)\}\sin nk_r,~\pi B_n=-\sum_{r=1}^{n}\{f(k_r^-)-f(k_r^+)\}\cos nk_r$$ will vanish when $f$ is continuous at all points. I thought that since $f$ has a sine series it should be an odd function of $x$ and therefore its derivative should also be an odd function of $x.$ So that would mean $a'_n=0$ for all $n$ and therefore $nb_n=B_n=\sin^2n\alpha=\tfrac12(1-\cos2n\alpha),$ which seems to suggest that the points of discontinuity are only $k_1=2\alpha.$ Then if I take $$\pi A_n=\{f(2\alpha^-)-f(2\alpha^+)\}\sin2n\alpha$$ then if $0<2\alpha<\pi,$ the sine term cannot vanish, and I have just claimed that $f(2\alpha^-)\neq f(2\alpha^+),$ so this means $A_n\neq0.$ But then that demands $b'_n\neq0,$ and therefore a nonconstant $f.$ On the other hand, I can't see how to show that $b'_n=0$ for this Fourier series. Would someone be able to let me know how to see when a Fourier series is discontinuous when we know only the series, but not the function? I would appreciate either correcting any mistakes I have made (I am not a professional or a maths student and I'm aware this method might be outdated or cumbersome) or showing me a better way to tackle these sorts of problems.",,"['calculus', 'analysis', 'fourier-analysis', 'fourier-series']"
61,Uniform convergence of integrals over a parameter,Uniform convergence of integrals over a parameter,,"Let $\mu$ be a regular $\sigma$-finite Borel measure on $X$. Suppose we have a sequence of continuous functions $$f_n: X \times [0,T] \rightarrow \mathbb{R}: (x,t) \mapsto f(x,t), $$ which converges pointwise to a continuous function $f: X \times [0,T] \rightarrow \mathbb{R}$.  Suppose the following: There exists a $g: X \times [0,T] \rightarrow \mathbb{R}$ such that for each $t \in [0,T]$ the integral of $g(\cdot, t)$ exists and for all $n$: $f_n \leq g$, For each $t \in [0,T]$ we have that $$ \lim_{n \rightarrow \infty} \int_X f_n(\cdot,t) d\mu = \int_X f(\cdot,t), $$ For each $x \in X$ the convergence $f_n(x,\cdot) \rightarrow f(x,\cdot)$ is uniform. Is this enough to conclude that the convergence  $$ \lim_{n \rightarrow \infty} \int_X f_n(\cdot,t) d\mu = \int_X f(\cdot,t) $$ is uniform in $t$? Maybe a useful theorem to prove it would be Egonov's Theorem. I found the article Emanual Parzen, Some conditions for uniform convergence of integrals where in the beginning of the article the author claims it is true and refers to a book. I looked into the book, but the notations and assumptions are too vague to me and the Theorem isn't stated in its proper form. I ask this question since I'm not sure if I interpreted the previous references correctly.","Let $\mu$ be a regular $\sigma$-finite Borel measure on $X$. Suppose we have a sequence of continuous functions $$f_n: X \times [0,T] \rightarrow \mathbb{R}: (x,t) \mapsto f(x,t), $$ which converges pointwise to a continuous function $f: X \times [0,T] \rightarrow \mathbb{R}$.  Suppose the following: There exists a $g: X \times [0,T] \rightarrow \mathbb{R}$ such that for each $t \in [0,T]$ the integral of $g(\cdot, t)$ exists and for all $n$: $f_n \leq g$, For each $t \in [0,T]$ we have that $$ \lim_{n \rightarrow \infty} \int_X f_n(\cdot,t) d\mu = \int_X f(\cdot,t), $$ For each $x \in X$ the convergence $f_n(x,\cdot) \rightarrow f(x,\cdot)$ is uniform. Is this enough to conclude that the convergence  $$ \lim_{n \rightarrow \infty} \int_X f_n(\cdot,t) d\mu = \int_X f(\cdot,t) $$ is uniform in $t$? Maybe a useful theorem to prove it would be Egonov's Theorem. I found the article Emanual Parzen, Some conditions for uniform convergence of integrals where in the beginning of the article the author claims it is true and refers to a book. I looked into the book, but the notations and assumptions are too vague to me and the Theorem isn't stated in its proper form. I ask this question since I'm not sure if I interpreted the previous references correctly.",,"['real-analysis', 'sequences-and-series', 'analysis', 'measure-theory', 'convergence-divergence']"
62,Uniform Convergence on open subset,Uniform Convergence on open subset,,"I have a sequence of continuous functions $f_{n}:[0,1]\rightarrow\mathbb{R}$ that converge pointwise to $0$ on $[0,1]$. I have to prove that given $\epsilon>0$ I can find a non-empty interval $(a,b)\subset [0,1]$ and N such that: $|f_{n}(t)|<\epsilon$ for all $t\in(a,b)$ and $n\geq N$. My attempt: I look at $E_{N}=\{x\in[0,1]:|f_{n}(t)|<\epsilon, \forall n\geq N\}$. Then $E_{N} \subset E_{N+1}$. Forthermore, as $f_{n}$ converges pointwise to $0$, we have that for any $x$ in $[0,1]$ there exists some $n$ such that $x \in E_{n}$.  So we must have that $\cup_{N=0}^{\infty} E_{N}=[0,1]$. I don't know how to conclude that there exists such an $(a,b)$.","I have a sequence of continuous functions $f_{n}:[0,1]\rightarrow\mathbb{R}$ that converge pointwise to $0$ on $[0,1]$. I have to prove that given $\epsilon>0$ I can find a non-empty interval $(a,b)\subset [0,1]$ and N such that: $|f_{n}(t)|<\epsilon$ for all $t\in(a,b)$ and $n\geq N$. My attempt: I look at $E_{N}=\{x\in[0,1]:|f_{n}(t)|<\epsilon, \forall n\geq N\}$. Then $E_{N} \subset E_{N+1}$. Forthermore, as $f_{n}$ converges pointwise to $0$, we have that for any $x$ in $[0,1]$ there exists some $n$ such that $x \in E_{n}$.  So we must have that $\cup_{N=0}^{\infty} E_{N}=[0,1]$. I don't know how to conclude that there exists such an $(a,b)$.",,"['analysis', 'uniform-convergence']"
63,Proving the error estimate of Simpson's rule using the Euler–Maclaurin sum formula,Proving the error estimate of Simpson's rule using the Euler–Maclaurin sum formula,,"I am trying to prove the following lemma for the Simpson's rule. Suppose $g\in C^4([-1,1],\mathbb{R})$. Then $$\bigg|\int_{-1}^1g(x)dx-\frac{g(-1)+4g(0)+g(1)}{3}\bigg|\leq\frac{1}{90}\|g^{(4)}\|_\infty.$$ Here $\|\!\cdot\!\|_\infty$ is the supremum norm defined by $\|f\|_\infty:=\sup_{x\in[-1,1]}|f(x)|$. My attempt : Applying the Euler–Maclaurin sum formula, we get $$\int_0^1g(x)dx-\frac{2g(0)+g(1)}{3}=\frac{g(1)-g(0)}{6}-\frac{g'(1)-g'(0)}{12}-\frac{1}{6}\int_0^1B_3(x)g^{(3)}(x)dx,$$ where $B_3(x):=x^3-3x^2/2+x/2$ is the third Bernoulli polynomial. Now integrate by parts to get $$\int_0^1B_3(x)g^{(3)}(x)dx=\frac{x^2(x-1)^2}{4}g^{(3)}(x)\bigg|_0^1-\int_0^1\frac{x^2(x-1)^2}{4}g^{(4)}(x)dx.$$ The first term equals $0$, while for the second we have, by the mean value theorem for integrals, $$\int_0^1\frac{x^2(x-1)^2}{4}g^{(4)}(x)dx=\bigg(\int_0^1\frac{x^2(x-1)^2}{4}dx\bigg)g^{(4)}(\xi)=\frac{1}{120}g^{(4)}(\xi)$$ for some $\xi\in[0,1]$. The problem is how to estimate the other two terms. If this approach were to work, then we must show that $$\bigg|\frac{g(1)-g(0)}{6}-\frac{g'(1)-g'(0)}{12}\bigg|\leq\frac{1}{240}\|g^{(4)}\|_\infty.$$ But I don't know how to relate these to the fourth derivative. I tried Lagrange's mean value theorem but got stuck. How should I proceed? This appears as an exercise VI.6.7. from Analysis II by Amann and Escher, where the whole section is devoted to applications of the Euler–Maclaurin sum formula. So if you have other proofs, please make use of this formula. Thanks in advance!","I am trying to prove the following lemma for the Simpson's rule. Suppose $g\in C^4([-1,1],\mathbb{R})$. Then $$\bigg|\int_{-1}^1g(x)dx-\frac{g(-1)+4g(0)+g(1)}{3}\bigg|\leq\frac{1}{90}\|g^{(4)}\|_\infty.$$ Here $\|\!\cdot\!\|_\infty$ is the supremum norm defined by $\|f\|_\infty:=\sup_{x\in[-1,1]}|f(x)|$. My attempt : Applying the Euler–Maclaurin sum formula, we get $$\int_0^1g(x)dx-\frac{2g(0)+g(1)}{3}=\frac{g(1)-g(0)}{6}-\frac{g'(1)-g'(0)}{12}-\frac{1}{6}\int_0^1B_3(x)g^{(3)}(x)dx,$$ where $B_3(x):=x^3-3x^2/2+x/2$ is the third Bernoulli polynomial. Now integrate by parts to get $$\int_0^1B_3(x)g^{(3)}(x)dx=\frac{x^2(x-1)^2}{4}g^{(3)}(x)\bigg|_0^1-\int_0^1\frac{x^2(x-1)^2}{4}g^{(4)}(x)dx.$$ The first term equals $0$, while for the second we have, by the mean value theorem for integrals, $$\int_0^1\frac{x^2(x-1)^2}{4}g^{(4)}(x)dx=\bigg(\int_0^1\frac{x^2(x-1)^2}{4}dx\bigg)g^{(4)}(\xi)=\frac{1}{120}g^{(4)}(\xi)$$ for some $\xi\in[0,1]$. The problem is how to estimate the other two terms. If this approach were to work, then we must show that $$\bigg|\frac{g(1)-g(0)}{6}-\frac{g'(1)-g'(0)}{12}\bigg|\leq\frac{1}{240}\|g^{(4)}\|_\infty.$$ But I don't know how to relate these to the fourth derivative. I tried Lagrange's mean value theorem but got stuck. How should I proceed? This appears as an exercise VI.6.7. from Analysis II by Amann and Escher, where the whole section is devoted to applications of the Euler–Maclaurin sum formula. So if you have other proofs, please make use of this formula. Thanks in advance!",,"['real-analysis', 'analysis', 'numerical-methods']"
64,Understanding Rudin's PMA Theorem 9.17,Understanding Rudin's PMA Theorem 9.17,,"$9.17$ Theorem Suppose f maps an open set $E\subset R^n$ into $R^m,$ and f is differentiable at a point $\mathbf{x}\in E.$ Then the partial derivatives $D_jf_i(\mathbf{x})$ exist, and  \begin{align} \mathbf{f'(x)e_j}=\sum\limits_{i=1}^m(D_jf_i)\mathbf{(x)u_i} &&(1\le j\le n)\tag{27} \end{align} $\mathbf{e_i}'$s  and $\mathbf{u_j}'$s are standard bases of $R^n$ and $R^m$ respectively. Proof Fix $j.$ Since f is differentiable at $\mathbf{x},$ $$\mathbf{f(x+}t\mathbf{e_j)}-\mathbf{f(x)}=\mathbf{f'(x)(}t\mathbf{e_j)}+\mathbf{r(}t\mathbf{e_j)}\tag{28}$$ where $\left|\mathbf{r(}t\mathbf{e_j)}\right|/t\to 0$ as $t\to 0.$ The linearity of $\mathbf{f'(x)}$ shows therefore that $$\lim_{t\to 0}\sum\limits_{i=1}^m\frac{f_i(\mathbf{x}+t\mathbf{e_j})-f_i(\mathbf{x})}{t}\mathbf{u_i}=\mathbf{f'(x)e_j.}\tag{29}$$ It follows that each quotient in this sum has a limit, as $t\to 0$(see Theorem $4.10$), so that each $(D_jf_i)(\mathbf{x})$ exists, and then $(27)$ follows from $(29).$ link to theorem $4.10$ Question Suppose I  denote $g_i(t) =\frac{f_i(\mathbf{x}+t\mathbf{e_j})-f_i(\mathbf{x})}{t}\mathbf{u_i}.$ Then $(29)$ is just saying that $\lim_{t\to 0}\sum g_i(t) =L.$ Then why can't I just conclude $\lim_{t\to 0}g_i(t)=l$ for some $l\in R^m.$ Why do I need Theorem $4.10$ here?","$9.17$ Theorem Suppose f maps an open set $E\subset R^n$ into $R^m,$ and f is differentiable at a point $\mathbf{x}\in E.$ Then the partial derivatives $D_jf_i(\mathbf{x})$ exist, and  \begin{align} \mathbf{f'(x)e_j}=\sum\limits_{i=1}^m(D_jf_i)\mathbf{(x)u_i} &&(1\le j\le n)\tag{27} \end{align} $\mathbf{e_i}'$s  and $\mathbf{u_j}'$s are standard bases of $R^n$ and $R^m$ respectively. Proof Fix $j.$ Since f is differentiable at $\mathbf{x},$ $$\mathbf{f(x+}t\mathbf{e_j)}-\mathbf{f(x)}=\mathbf{f'(x)(}t\mathbf{e_j)}+\mathbf{r(}t\mathbf{e_j)}\tag{28}$$ where $\left|\mathbf{r(}t\mathbf{e_j)}\right|/t\to 0$ as $t\to 0.$ The linearity of $\mathbf{f'(x)}$ shows therefore that $$\lim_{t\to 0}\sum\limits_{i=1}^m\frac{f_i(\mathbf{x}+t\mathbf{e_j})-f_i(\mathbf{x})}{t}\mathbf{u_i}=\mathbf{f'(x)e_j.}\tag{29}$$ It follows that each quotient in this sum has a limit, as $t\to 0$(see Theorem $4.10$), so that each $(D_jf_i)(\mathbf{x})$ exists, and then $(27)$ follows from $(29).$ link to theorem $4.10$ Question Suppose I  denote $g_i(t) =\frac{f_i(\mathbf{x}+t\mathbf{e_j})-f_i(\mathbf{x})}{t}\mathbf{u_i}.$ Then $(29)$ is just saying that $\lim_{t\to 0}\sum g_i(t) =L.$ Then why can't I just conclude $\lim_{t\to 0}g_i(t)=l$ for some $l\in R^m.$ Why do I need Theorem $4.10$ here?",,"['real-analysis', 'analysis']"
65,Differentiation of the Newtonian potential of a function (Sandro Salsa's PDE book),Differentiation of the Newtonian potential of a function (Sandro Salsa's PDE book),,"I am studying the PDE book Partial Differential Equations in Action, From Modelling to Theory , by Sandro Salsa. I would like to ask two questions about the differentiation of the Newtonian potential of a function $f$, $$ u(x)=\int_{\mathbb{R}^3} \Phi(x-y) f(y)\,\mathrm{d}y=(\Phi\ast f)(x), $$ where $$\Phi(x)=\frac{1}{4\pi}\frac{1}{|x|}$$ is the fundamental solution of the Poisson equation $-\Delta u=\delta_0$. In a footnote in page 130, it is stated the following: if $g\in C^1(\mathbb{R}^3)$ and $|g(x)|\leq M/|x|^{2+\epsilon}$, then  $$ \frac{\partial}{\partial x_j}\int_{\mathbb{R}^3}\frac{1}{|x-y|}g(y)\,\mathrm{d}y=\int_{\mathbb{R}^3}\frac{1}{|x-y|}\frac{\partial g}{\partial y_j}(y)\,\mathrm{d}y. $$ Usually, results on differentiation under the integral sign requiere bounds on the derivative, so that the Dominated Convergence Theorem can be applied. I do not know how to proceed in this case. In Theorem 3.9, it is said that if $f\in C_c^2(\mathbb{R}^3)$ (i.e., $C^2$ with compact support), then its Newtonian potential $u=\Phi\ast f$ is $C^2(\mathbb{R}^3)$ and $-\Delta u=f$. The fact that $f$ is $0$ outside a ball and $1/|x|$ is locally integrable allows differentiating under the integral sign. But, in Remark 3.6, it is said that the theorem holds when $f\in C^1(\mathbb{R}^3)$ and $|f(x)|\leq M/|x|^{2+\epsilon}$. Any idea or reference on why this statement is true?","I am studying the PDE book Partial Differential Equations in Action, From Modelling to Theory , by Sandro Salsa. I would like to ask two questions about the differentiation of the Newtonian potential of a function $f$, $$ u(x)=\int_{\mathbb{R}^3} \Phi(x-y) f(y)\,\mathrm{d}y=(\Phi\ast f)(x), $$ where $$\Phi(x)=\frac{1}{4\pi}\frac{1}{|x|}$$ is the fundamental solution of the Poisson equation $-\Delta u=\delta_0$. In a footnote in page 130, it is stated the following: if $g\in C^1(\mathbb{R}^3)$ and $|g(x)|\leq M/|x|^{2+\epsilon}$, then  $$ \frac{\partial}{\partial x_j}\int_{\mathbb{R}^3}\frac{1}{|x-y|}g(y)\,\mathrm{d}y=\int_{\mathbb{R}^3}\frac{1}{|x-y|}\frac{\partial g}{\partial y_j}(y)\,\mathrm{d}y. $$ Usually, results on differentiation under the integral sign requiere bounds on the derivative, so that the Dominated Convergence Theorem can be applied. I do not know how to proceed in this case. In Theorem 3.9, it is said that if $f\in C_c^2(\mathbb{R}^3)$ (i.e., $C^2$ with compact support), then its Newtonian potential $u=\Phi\ast f$ is $C^2(\mathbb{R}^3)$ and $-\Delta u=f$. The fact that $f$ is $0$ outside a ball and $1/|x|$ is locally integrable allows differentiating under the integral sign. But, in Remark 3.6, it is said that the theorem holds when $f\in C^1(\mathbb{R}^3)$ and $|f(x)|\leq M/|x|^{2+\epsilon}$. Any idea or reference on why this statement is true?",,"['analysis', 'partial-differential-equations', 'lebesgue-integral', 'convolution', 'poissons-equation']"
66,How to prove $\sum_{m=1}^\infty \sum_{n=1}^\infty \frac{1}{m^2+n^2}=+\infty$,How to prove,\sum_{m=1}^\infty \sum_{n=1}^\infty \frac{1}{m^2+n^2}=+\infty,How to prove $$\sum_{m=1}^\infty \sum_{n=1}^\infty \frac{1}{m^2+n^2}=+\infty.$$ I try to do like $$\sum_{m=1}^\infty \sum_{n=1}^\infty \frac{1}{m^2+n^2}=\sum_{N=1}^\infty \sum_{n+m=N}^\infty  \frac{1}{m^2+n^2}=\sum_{N=1}^\infty \sum_{m=1}^{N-1}  \frac{1}{m^2+(N-m)^2}$$  $$\frac{1}{m^2+(N-m)^2}\leq \frac{2}{N^2}$$ but it doesn't work.,How to prove $$\sum_{m=1}^\infty \sum_{n=1}^\infty \frac{1}{m^2+n^2}=+\infty.$$ I try to do like $$\sum_{m=1}^\infty \sum_{n=1}^\infty \frac{1}{m^2+n^2}=\sum_{N=1}^\infty \sum_{n+m=N}^\infty  \frac{1}{m^2+n^2}=\sum_{N=1}^\infty \sum_{m=1}^{N-1}  \frac{1}{m^2+(N-m)^2}$$  $$\frac{1}{m^2+(N-m)^2}\leq \frac{2}{N^2}$$ but it doesn't work.,,"['real-analysis', 'sequences-and-series']"
67,Density of Square Rationals,Density of Square Rationals,,"This is a homework question I have in my analysis class. Prove that for all $x, z \in \Bbb Q$ with $0 < x < z$, there exists $y \in\Bbb Q$ such that $x < y < z$ and $y$ has a rational square root (i.e., $y = w^2$ for some $w \in Q$.)  Here, $\Bbb R$ is the set of all real numbers, and $\Bbb Q$ is the set of all rational numbers. I tried the following: Let $x,z \in \Bbb Q$ with $0 < x < z$. Since $x < y < z$, we can say $\sqrt x < \sqrt z$.  Since $\Bbb Q$ is dense in $\Bbb R$, there exists $w \in\Bbb Q$ such that $\sqrt x < w < \sqrt z$.  If we square the expression, we get $x < y^2 < z$, so $w^2 = y$. QED I was told that this isn't a proper way to prove this statement and haven't been able to figure out how to start it correctly. Any help is appreciated!","This is a homework question I have in my analysis class. Prove that for all $x, z \in \Bbb Q$ with $0 < x < z$, there exists $y \in\Bbb Q$ such that $x < y < z$ and $y$ has a rational square root (i.e., $y = w^2$ for some $w \in Q$.)  Here, $\Bbb R$ is the set of all real numbers, and $\Bbb Q$ is the set of all rational numbers. I tried the following: Let $x,z \in \Bbb Q$ with $0 < x < z$. Since $x < y < z$, we can say $\sqrt x < \sqrt z$.  Since $\Bbb Q$ is dense in $\Bbb R$, there exists $w \in\Bbb Q$ such that $\sqrt x < w < \sqrt z$.  If we square the expression, we get $x < y^2 < z$, so $w^2 = y$. QED I was told that this isn't a proper way to prove this statement and haven't been able to figure out how to start it correctly. Any help is appreciated!",,"['analysis', 'proof-verification']"
68,1st Order Nonlinear PDE: Understanding Envelopes and Monge Cones,1st Order Nonlinear PDE: Understanding Envelopes and Monge Cones,,"I have a question about envelopes of surfaces. In a book I am reading the following: Suppose $S_a$ is a one parameter family of surfaces in $R^3$ given by $z=w(x,y;a)$ where $w$ depends smoothly on $x,y$ and the real parameter $a$. Consider also the equation $\partial_a w(x,y;a)=0$. For a fixed values of $a$, these two equations determine a curve $\gamma_a$. The envelope $E$ of the family of surfaces $S_a$ is just the union of these curves $\gamma_a$. The equation for $E$ is found simply by solving $\partial_a w(x,y,a)=0$ for $a$ as a function of $x$ and $y$, $a=f(x,y)$, and then substituting into $z=w(x,y,f(x,y))$. Moreover, along $\gamma_a$, $a$ is constant and we have $$dz = w_xdx + w_ydy \\0 = w_{ax}dx + w_{ay}dy$$   For instance, if $S_a$ is a one-parameter family of 2-spheres: $(x-a)^2+y^2+z^2 = 1$, then the envelope is a cylinder of radius 1. Can anyone provide an ""intuitive"" geometric reason for why taking the derivative with respect to the parameter, setting it equal to zero, and plugging it back into $F$ gives the envelope? I see that it works in the example of the sphere, I obtain a cylinder $y^2 + z^2 = 1$. In the procedure descirbed above they use the notation $$dz = w_xdx + w_ydy \\0 = w_{ax}dx + w_{ay}dy$$ Is this a formal expression? When I read it as $$\frac{dz}{dt} = w_x\frac{dx}{dt} + w_y\frac{dy}{dt} \\0 = w_{ax}\frac{dx}{dt} + w_{ay}\frac{dy}{dt}$$ It makes sense to me. I looked this up and saw some junk on cotangent spaces, but couldn't understand how it was related to the discussion above. They introduce a notion of Monge Cone in the following way: Consider the 1st order PDE: $F(x,y,z,p,q)=0$. At any point $(x_0,y_0,z_0)$, $F$ establishes a functional relation between $p$ and $q$. Assuming $F_q(x_0,y_0,z_0,p,q)\neq 0$, implicit function theorem gives us: $F(x_0,y_0,z_0,p,q(p))=0$ for all $p$. The possible tangent planes to the graph $z=u(x,y)$ are given by: $$(z-z_0) = p(x-x_0)+q(p)(y-y_0)$$ which, as $p$ varies, describe a one-parameter family of planes through the point $(x_0,y_0,z_0)$. Using the equations in #2, they solve for the envelope of planes at $(x_0,y_0,z_0)$ (parameter is $p$) and find: $$dz = pdx + qdy \\ 0 = dx + \frac{dq}{dp}dy$$ How do I see this is a cone at $(x_0,y_0,z_0)$?","I have a question about envelopes of surfaces. In a book I am reading the following: Suppose $S_a$ is a one parameter family of surfaces in $R^3$ given by $z=w(x,y;a)$ where $w$ depends smoothly on $x,y$ and the real parameter $a$. Consider also the equation $\partial_a w(x,y;a)=0$. For a fixed values of $a$, these two equations determine a curve $\gamma_a$. The envelope $E$ of the family of surfaces $S_a$ is just the union of these curves $\gamma_a$. The equation for $E$ is found simply by solving $\partial_a w(x,y,a)=0$ for $a$ as a function of $x$ and $y$, $a=f(x,y)$, and then substituting into $z=w(x,y,f(x,y))$. Moreover, along $\gamma_a$, $a$ is constant and we have $$dz = w_xdx + w_ydy \\0 = w_{ax}dx + w_{ay}dy$$   For instance, if $S_a$ is a one-parameter family of 2-spheres: $(x-a)^2+y^2+z^2 = 1$, then the envelope is a cylinder of radius 1. Can anyone provide an ""intuitive"" geometric reason for why taking the derivative with respect to the parameter, setting it equal to zero, and plugging it back into $F$ gives the envelope? I see that it works in the example of the sphere, I obtain a cylinder $y^2 + z^2 = 1$. In the procedure descirbed above they use the notation $$dz = w_xdx + w_ydy \\0 = w_{ax}dx + w_{ay}dy$$ Is this a formal expression? When I read it as $$\frac{dz}{dt} = w_x\frac{dx}{dt} + w_y\frac{dy}{dt} \\0 = w_{ax}\frac{dx}{dt} + w_{ay}\frac{dy}{dt}$$ It makes sense to me. I looked this up and saw some junk on cotangent spaces, but couldn't understand how it was related to the discussion above. They introduce a notion of Monge Cone in the following way: Consider the 1st order PDE: $F(x,y,z,p,q)=0$. At any point $(x_0,y_0,z_0)$, $F$ establishes a functional relation between $p$ and $q$. Assuming $F_q(x_0,y_0,z_0,p,q)\neq 0$, implicit function theorem gives us: $F(x_0,y_0,z_0,p,q(p))=0$ for all $p$. The possible tangent planes to the graph $z=u(x,y)$ are given by: $$(z-z_0) = p(x-x_0)+q(p)(y-y_0)$$ which, as $p$ varies, describe a one-parameter family of planes through the point $(x_0,y_0,z_0)$. Using the equations in #2, they solve for the envelope of planes at $(x_0,y_0,z_0)$ (parameter is $p$) and find: $$dz = pdx + qdy \\ 0 = dx + \frac{dq}{dp}dy$$ How do I see this is a cone at $(x_0,y_0,z_0)$?",,"['analysis', 'partial-differential-equations', 'characteristics']"
69,Equivalence between Brouwer fixed-point theorem and Borsuk-Ulam theorem. Is there a simple proof of equivalence between them?,Equivalence between Brouwer fixed-point theorem and Borsuk-Ulam theorem. Is there a simple proof of equivalence between them?,,"I wonder if Brouwer's fixed-point theorem and Borsuk-Ulam's theorem are equivalent. Brouwer's fixed-point theorem (simple form). Let $B_{\mathbb{R}^{n}}[0,1]=\{x\in \mathbb{R}^n: \|x-0\|\leq 1\}$ Then, any continuous $f:B_{\mathbb{R}^{n}}[0,1]\to B_{\mathbb{R}^{n}}[0,1] $ has a fixed point, i.e. there is a $x\in B_{\mathbb{R}^{n}}[0,1]$ such that $f(x)=x$.   \ \ Borsuk-Ulam's theorem. Let a continuous function $f:\mathbb{S}^n\to \mathbb{R}^n$ (where $\mathbb{S}^n=\partial B_{\mathbb{R}^{n+1}}[0,1]$ is the $n$-sphere). Then there is a point $x\in \mathbb{S}^n$ for which $f(x)=f(−x)$. In other words, I would like to know if there are any proofs of the implications below: Brouwer's fixed-point theorem implies Borsuk-Ulam's theorem . Borsuk-Ulam's theorem implies Brouwer's fixed-point theorem . I would be happy if there were a simple (or as self-contained as possible) proof that could be reproduced here. But as the proofs of these theorems (at least in the bibliographic research I have done) are intricate depending on various definitions and lemas I think such simple proofs do not exist. In that case I would be satisfied with a set of references (books or articles) that had proof of confirmation or proof of the negative of both of the implications above. My bibliographic research is summarized as follows: Book: Using the Borsuk-Ulam Theorem: Lectures on Topological Methods in Combinatorics and Geometry (Universitext) MathOverflow question: Generalization of Borsuk-Ulam. Math.stackexchange question: Is there a simple proof of Borsuk-Ulam, given Brouwer? This question (my question is not a duplicate of this question) can provide answers related to the first implication above. But it does not touch upon the question of the equivalence of these two theorems. Thesis: Extensions of the Borsuk-Ulam Theorem. The Wikipedia entry on Borsuk-Ulam's theorem states that such theorems are equivalent ( see here ) but provide no direct proof. And if I understood correctly, it is not possible to conclude by any chain of implications on other theorems set forth therein. Also the references of Wikipedia do not bring any proof of this affirmation.","I wonder if Brouwer's fixed-point theorem and Borsuk-Ulam's theorem are equivalent. Brouwer's fixed-point theorem (simple form). Let $B_{\mathbb{R}^{n}}[0,1]=\{x\in \mathbb{R}^n: \|x-0\|\leq 1\}$ Then, any continuous $f:B_{\mathbb{R}^{n}}[0,1]\to B_{\mathbb{R}^{n}}[0,1] $ has a fixed point, i.e. there is a $x\in B_{\mathbb{R}^{n}}[0,1]$ such that $f(x)=x$.   \ \ Borsuk-Ulam's theorem. Let a continuous function $f:\mathbb{S}^n\to \mathbb{R}^n$ (where $\mathbb{S}^n=\partial B_{\mathbb{R}^{n+1}}[0,1]$ is the $n$-sphere). Then there is a point $x\in \mathbb{S}^n$ for which $f(x)=f(−x)$. In other words, I would like to know if there are any proofs of the implications below: Brouwer's fixed-point theorem implies Borsuk-Ulam's theorem . Borsuk-Ulam's theorem implies Brouwer's fixed-point theorem . I would be happy if there were a simple (or as self-contained as possible) proof that could be reproduced here. But as the proofs of these theorems (at least in the bibliographic research I have done) are intricate depending on various definitions and lemas I think such simple proofs do not exist. In that case I would be satisfied with a set of references (books or articles) that had proof of confirmation or proof of the negative of both of the implications above. My bibliographic research is summarized as follows: Book: Using the Borsuk-Ulam Theorem: Lectures on Topological Methods in Combinatorics and Geometry (Universitext) MathOverflow question: Generalization of Borsuk-Ulam. Math.stackexchange question: Is there a simple proof of Borsuk-Ulam, given Brouwer? This question (my question is not a duplicate of this question) can provide answers related to the first implication above. But it does not touch upon the question of the equivalence of these two theorems. Thesis: Extensions of the Borsuk-Ulam Theorem. The Wikipedia entry on Borsuk-Ulam's theorem states that such theorems are equivalent ( see here ) but provide no direct proof. And if I understood correctly, it is not possible to conclude by any chain of implications on other theorems set forth therein. Also the references of Wikipedia do not bring any proof of this affirmation.",,"['analysis', 'reference-request', 'algebraic-topology', 'fixed-point-theorems', 'reference-works']"
70,A problem in integral,A problem in integral,,"Let $f: [0,1]\to \mathbb R$,  $f \in C^2[0,1]$ with $f(0) = f(1) = 0$. For all $x\in (0,1)$, $f(x) > 0$. Indicate with $M = \max \limits_{x\in [0,1]} f(x)$. Prove that $\forall a,b \in [0,1]$,  $a<b$, $$ \int_a^b \left| \frac{f''(x)}{f(x)} \right| \mathrm{d}x \geq \frac{1}{M} \vert f'(b) - f'(a)\vert. $$ Prove that $$ \int_0^1 \left| \frac{f''(x)}{f(x)} \right| \mathrm{d}x \geq 4. $$ The first half of the problem is easy to solve with Newton-Leibniz formula and basic properties of Riemann integral. I wonder the proof of 2. Thanks in advance!","Let $f: [0,1]\to \mathbb R$,  $f \in C^2[0,1]$ with $f(0) = f(1) = 0$. For all $x\in (0,1)$, $f(x) > 0$. Indicate with $M = \max \limits_{x\in [0,1]} f(x)$. Prove that $\forall a,b \in [0,1]$,  $a<b$, $$ \int_a^b \left| \frac{f''(x)}{f(x)} \right| \mathrm{d}x \geq \frac{1}{M} \vert f'(b) - f'(a)\vert. $$ Prove that $$ \int_0^1 \left| \frac{f''(x)}{f(x)} \right| \mathrm{d}x \geq 4. $$ The first half of the problem is easy to solve with Newton-Leibniz formula and basic properties of Riemann integral. I wonder the proof of 2. Thanks in advance!",,"['calculus', 'integration', 'analysis']"
71,"If $f:\mathbb{R}\to\mathbb{R}$ is continuous, then is the pseudo-inverse $f^+$ measurable?","If  is continuous, then is the pseudo-inverse  measurable?",f:\mathbb{R}\to\mathbb{R} f^+,"Assume that $\sigma$-algebra contains Borel sets and let $f:\mathbb{R}\to \mathbb{R}$ be a continuous function. Define $g=f^+:\mathbb{R}\to \mathbb{R}$ as $g(x) = 0$ if $f(x) = 0$ and $g(x) = 1/f(x)$ if $f(x)\neq0$. I am trying to prove that $g$ is measurable but not confident in my trial. Would you check the proof below? Since $f$ is continuous and $f(x) = 0 \Leftrightarrow g(x) = 0$, $f^{-1}(0) = g^{-1}(0)$ is closed, so measurable. Let $A = \{x\in \mathbb{R}\mid f(x) = 0\}$ and $B = \mathbb{R}\setminus A$. The restriction of $g$ into $B$, $g|_B$, is continuous in $B$. Let $V\in \mathbb{R}$ be an arbitrary open set. Then, $W = V\setminus\{0\}$ is open and $g^{-1}(W) = g|_B^{-1}(W) \subset B$ is open, so measurable. Since $g^{-1}(0)$ and $g^{-1}(W)$ are measurable, $g^{-1}(V)$ is measurable. Therefore, $g$ is measurable.","Assume that $\sigma$-algebra contains Borel sets and let $f:\mathbb{R}\to \mathbb{R}$ be a continuous function. Define $g=f^+:\mathbb{R}\to \mathbb{R}$ as $g(x) = 0$ if $f(x) = 0$ and $g(x) = 1/f(x)$ if $f(x)\neq0$. I am trying to prove that $g$ is measurable but not confident in my trial. Would you check the proof below? Since $f$ is continuous and $f(x) = 0 \Leftrightarrow g(x) = 0$, $f^{-1}(0) = g^{-1}(0)$ is closed, so measurable. Let $A = \{x\in \mathbb{R}\mid f(x) = 0\}$ and $B = \mathbb{R}\setminus A$. The restriction of $g$ into $B$, $g|_B$, is continuous in $B$. Let $V\in \mathbb{R}$ be an arbitrary open set. Then, $W = V\setminus\{0\}$ is open and $g^{-1}(W) = g|_B^{-1}(W) \subset B$ is open, so measurable. Since $g^{-1}(0)$ and $g^{-1}(W)$ are measurable, $g^{-1}(V)$ is measurable. Therefore, $g$ is measurable.",,"['real-analysis', 'analysis']"
72,When can one pass a linear operator under the integral?,When can one pass a linear operator under the integral?,,"Specifically, one the webpage: http://mathworld.wolfram.com/GreensFunction.html It is written that  $$ \int\mathcal{L}G(x,s)f(s)ds = \mathcal{L}\left(\int G(x,s)f(s)ds\right) $$ where $G$ is a Green function. Why can the linear operator be pulled out of the integral? Thanks!","Specifically, one the webpage: http://mathworld.wolfram.com/GreensFunction.html It is written that  $$ \int\mathcal{L}G(x,s)f(s)ds = \mathcal{L}\left(\int G(x,s)f(s)ds\right) $$ where $G$ is a Green function. Why can the linear operator be pulled out of the integral? Thanks!",,['analysis']
73,Pitchfork Bifurcation vs. Period-Doubling Bifurcation,Pitchfork Bifurcation vs. Period-Doubling Bifurcation,,"I'm trying to understand how symmetry transformations give us indication of what kind of bifurcation occurs in a particular system, and I'm currently following Ghrist et. al.'s monograph on Knots & Links in 3-D flows, which is a little sketchy at places. In particular, for pitchfork bifurcation he writes: Consider $f:\mathbb{R}^1\rightarrow \mathbb{R}^1$ which is generic in the class of maps which is invariant under the symmetry transformation $x\mapsto -x$ e.g. $x\mapsto x + \mu x - x^3$. Then, by symmetry, the origin must be a fixed point for all $\mu$. In this case there is a pitchfork bifurcation at $\mu=0$. For the period-doubling bifurcation, he writes: Let $f_{\mu}:\mathbb{R}^1\rightarrow \mathbb{R}^1$ be a generic map whose derivative satisfies $f'_0(0)=-1$, e.g. $x\mapsto - x - \mu x + x^3$. Then, the bifurcation at $\mu=0$ is called a period-doubling bifurcation. My questions: 1) I learnt about pitchfork bifurcation in the context of ODEs, where there were a bunch of other conditions necessary for a pitchfork bifurcation to take place, yet they aren't mentioned here. It's unclear to me why a generic real-valued map that is invariant under the symmetry transformation results in a pitchfork bifurcation when we vary the parameters. For instance, $g(x)=x$ is invariant under the symmetry transformation since $-x \mapsto -x = g(-x)$ 2) I don't really know what he means by symmetry transformation in this case. Does he mean that the function is odd? But in this case, isn't the period-doubling bifurcation also invariant under the symmetry transformation. For if,  $f(x) = - x - \mu x + x^3$, then $f(-x)= - (x -\mu x + x^3) = -f(x)$, no? Wouldn't that make it a pitchfork bifurcation under Ghrist's definition above? Yet that can't be right... I'm basically confused about how period-doubling transformations are symmetry breaking... I know these are probably really basic questions, but any help would be much appreciated!","I'm trying to understand how symmetry transformations give us indication of what kind of bifurcation occurs in a particular system, and I'm currently following Ghrist et. al.'s monograph on Knots & Links in 3-D flows, which is a little sketchy at places. In particular, for pitchfork bifurcation he writes: Consider $f:\mathbb{R}^1\rightarrow \mathbb{R}^1$ which is generic in the class of maps which is invariant under the symmetry transformation $x\mapsto -x$ e.g. $x\mapsto x + \mu x - x^3$. Then, by symmetry, the origin must be a fixed point for all $\mu$. In this case there is a pitchfork bifurcation at $\mu=0$. For the period-doubling bifurcation, he writes: Let $f_{\mu}:\mathbb{R}^1\rightarrow \mathbb{R}^1$ be a generic map whose derivative satisfies $f'_0(0)=-1$, e.g. $x\mapsto - x - \mu x + x^3$. Then, the bifurcation at $\mu=0$ is called a period-doubling bifurcation. My questions: 1) I learnt about pitchfork bifurcation in the context of ODEs, where there were a bunch of other conditions necessary for a pitchfork bifurcation to take place, yet they aren't mentioned here. It's unclear to me why a generic real-valued map that is invariant under the symmetry transformation results in a pitchfork bifurcation when we vary the parameters. For instance, $g(x)=x$ is invariant under the symmetry transformation since $-x \mapsto -x = g(-x)$ 2) I don't really know what he means by symmetry transformation in this case. Does he mean that the function is odd? But in this case, isn't the period-doubling bifurcation also invariant under the symmetry transformation. For if,  $f(x) = - x - \mu x + x^3$, then $f(-x)= - (x -\mu x + x^3) = -f(x)$, no? Wouldn't that make it a pitchfork bifurcation under Ghrist's definition above? Yet that can't be right... I'm basically confused about how period-doubling transformations are symmetry breaking... I know these are probably really basic questions, but any help would be much appreciated!",,"['real-analysis', 'analysis', 'dynamical-systems', 'symmetry', 'bifurcation']"
74,Evaluate the limit inf of $\cos(n^4+n+1)$,Evaluate the limit inf of,\cos(n^4+n+1),"I want to calculate $\liminf\cos(n^4+n+1).$ In fact, I don't know how to proceed. I know that $\liminf\cos(n)$ is $-1$, but I have no idea if this will help me. So, I am asking if someone has an idea for the solution","I want to calculate $\liminf\cos(n^4+n+1).$ In fact, I don't know how to proceed. I know that $\liminf\cos(n)$ is $-1$, but I have no idea if this will help me. So, I am asking if someone has an idea for the solution",,"['real-analysis', 'analysis', 'limsup-and-liminf']"
75,"Prob. 17, Chap. 6, in Baby Rudin: $\int_a^b \alpha(x) g(x) \ \mathrm{d} x = G(b) \alpha(b) - G(a) \alpha(a) - \int_a^b G \ \mathrm{d} \alpha$","Prob. 17, Chap. 6, in Baby Rudin:",\int_a^b \alpha(x) g(x) \ \mathrm{d} x = G(b) \alpha(b) - G(a) \alpha(a) - \int_a^b G \ \mathrm{d} \alpha,"Here is Prob. 17, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $\alpha$ increases monotonically on $[a, b]$, $g$ is continuous, and $g(x) = G^\prime (x)$ for $a \leq x \leq b$. Prove that    $$ \int_a^b \alpha(x) g(x) \ \mathrm{d} x = G(b) \alpha(b) - G(a) \alpha(a) - \int_a^b G \ \mathrm{d} \alpha. $$ Hint: Take $g$ real, without loss of generality. Given $P = \left\{ \ x_0, x_1, \ldots, x_n \ \right\}$, choose $t_i \in \left( x_{i-1}, x_i \right)$ so that $g \left( t_i \right) \Delta x_i = G \left( x_i \right) - G \left( x_{i-1} \right)$. Show that    $$ \sum_{i=1}^n \alpha \left( x_i \right) g \left( t_i \right) \Delta x_i = G(b) \alpha (b) - G(a) \alpha (a) - \sum_{i=1}^n G \left( x_{i-1} \right) \Delta \alpha_i. $$ My Attempt: Here is the link to one of my Math SE posts where I've included Definitions 6.1 and 6.2 in Rudin: Theorem 6.15 in Baby Rudin: If $a<s<b$, $f$ is bounded on $[a,b]$, $f$ is continuous at $s$, and $\alpha(x)=I(x-s)$, then . . . All the notation to be used below is contained in these definitions. We first assume that $g$ is real; then so is $G$. As $g$ is continuous on $[a, b]$, so $g$ is Riemann-integrable, by Theorem 6.8 in Rudin; as $\alpha$ is monotonic on $[a, b]$, so $\alpha$ is Riemann-integrable, by Theorem 6.9 in Rudin. Now as both $\alpha$ and $g$ are Riemann-integrable on $[a, b]$, so is $\alpha g$, by Theorem 6.13 (a) in Rudin. Thus $ \int_a^b \alpha(x) g(x) \ \mathrm{d} x$ exists in $\mathbb{R}$. As $G$ is differentiable on $[a, b]$, so $G$ is also continuous on $[a, b]$, by Theorem 5.2 in Rudin, and as $G$ is continuous on $[a, b]$ and $\alpha$ is monotonically increasing on $[a, b]$, so $G$ is Riemann-Stieltjes integrable with respect to $\alpha$ on $[a, b]$, again by Theorem 6.8 in Rudin. Thus $\int_a^b G \ \mathrm{d} \alpha$ exists in $\mathbb{R}$. Let $P = \left\{ \ x_0, x_1, \ldots, x_n \ \right\}$ be a partition of $[a, b]$. For each $i = 1, \ldots, n$, as $G$ is continuous on $\left[ x_{i-1}, x_i \right]$ and differentiable on $\left( x_{i-1}, x_i \right)$, so by the mean-value theorem we can find a point $t_i \in \left( x_{i-1}, x_i \right)$ such that    $$ G \left( x_i \right) - G \left( x_{i-1} \right) = G^\prime \left( t_i \right) \cdot \left( x_i - x_{i-1} \right) = g \left( t_i \right) \Delta x_i. \tag{0} $$    So    $$ \begin{align} \sum_{i=1}^n \alpha \left( x_i \right) g \left( t_i \right) \Delta x_i &= \sum_{i=1}^n \alpha \left( x_i \right) \left[ G \left( x_i \right) - G \left( x_{i-1} \right) \right] \qquad \mbox{ [ using (0) ] } \\ &= \sum_{i=1}^n \alpha \left( x_i \right) G \left( x_i \right) - \sum_{i=1}^n  \alpha \left( x_i \right) G \left( x_{i-1} \right)  \\ &= \sum_{i=1}^n \alpha \left( x_i \right) G \left( x_i \right) - \sum_{i=0}^{n-1}  \alpha \left( x_{i+1} \right) G \left( x_i \right)  \\ &= \alpha \left(x_n \right) G \left( x_n \right) + \sum_{i=1}^{n-1} \alpha \left( x_i \right) G \left( x_i \right) - \sum_{i=0}^{n-1}  \alpha \left( x_{i+1} \right) G \left( x_i \right) \\ &= \alpha(b) G(b) - \alpha (a) G(a) + \sum_{i=0}^{n-1} \alpha \left( x_i \right) G \left( x_i \right) - \sum_{i=0}^{n-1}  \alpha \left( x_{i+1} \right) G \left( x_i \right) \\ &= \alpha(b) G(b) - \alpha (a) G(a) + \sum_{i=0}^{n-1} \left[ \alpha \left( x_i \right) -  \alpha \left( x_{i+1} \right) \right] G \left( x_i \right) \\  &= \alpha(b) G(b) - \alpha(a) G(a) -  \sum_{i=0}^{n-1} \left[ \alpha \left( x_{i+1} \right) -   \alpha \left( x_i  \right) \right] G \left( x_i \right) \\ &= \alpha(b) G(b) - \alpha(a) G(a) -  \sum_{i=1}^{n} \left[ \alpha \left( x_{i} \right) -   \alpha \left( x_{i-1}  \right) \right] G \left( x_{i-1} \right) \\ &= \alpha(b) G(b) - \alpha(a) G(a) -  \sum_{i=1}^{n} G \left( x_{i-1} \right) \Delta \alpha_i.  \end{align} $$ What next? How to proceed from here? Now we assume that $g$ is a complex function on $[a, b]$, that $g$ is continuous, and $g(x) = G^\prime (x)$. Then $g = \Re g + \iota \Im g$, and $\Re g$ and $\Im g$ are both real continuous functions on $[a, b]$. Moreover, the function $G$ too is complex, and $G = \Re G + \iota \Im G$, where $\Re G$ and $\Im G$ are real functions on $[a, b]$ such that $ \left( \Re G \right)^\prime = \Re g$ and $ \left( \Im G \right)^\prime = \Im g$. The function $\alpha$ is still real of course. So    $$  \begin{align} \int_a^b \alpha(x) g(x) \ \mathrm{d} x &= \int_a^b \alpha(x) \Re g(x) \ \mathrm{d} x + \iota \int_a^b \alpha(x) \Im g(x) \ \mathrm{d} x \\ &= \left[ \alpha (b) \Re G(b) - \alpha(a) \Re G(a) - \int_a^b \Re G \ \mathrm{d} \alpha \right] \\ & \qquad  + \iota \left[ \alpha (b) \Im G(b) - \alpha(a) \Im G(a) - \int_a^b \Im G \ \mathrm{d} \alpha  \right] \\ &= \alpha (b) G(b) - \alpha(a) G(a) - \int_a^b G \ \mathrm{d} \alpha,  \end{align} $$   as required. Is this proof for complex functions correct? If so, then is it rigorous enough too? If not, then where have I erred?","Here is Prob. 17, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $\alpha$ increases monotonically on $[a, b]$, $g$ is continuous, and $g(x) = G^\prime (x)$ for $a \leq x \leq b$. Prove that    $$ \int_a^b \alpha(x) g(x) \ \mathrm{d} x = G(b) \alpha(b) - G(a) \alpha(a) - \int_a^b G \ \mathrm{d} \alpha. $$ Hint: Take $g$ real, without loss of generality. Given $P = \left\{ \ x_0, x_1, \ldots, x_n \ \right\}$, choose $t_i \in \left( x_{i-1}, x_i \right)$ so that $g \left( t_i \right) \Delta x_i = G \left( x_i \right) - G \left( x_{i-1} \right)$. Show that    $$ \sum_{i=1}^n \alpha \left( x_i \right) g \left( t_i \right) \Delta x_i = G(b) \alpha (b) - G(a) \alpha (a) - \sum_{i=1}^n G \left( x_{i-1} \right) \Delta \alpha_i. $$ My Attempt: Here is the link to one of my Math SE posts where I've included Definitions 6.1 and 6.2 in Rudin: Theorem 6.15 in Baby Rudin: If $a<s<b$, $f$ is bounded on $[a,b]$, $f$ is continuous at $s$, and $\alpha(x)=I(x-s)$, then . . . All the notation to be used below is contained in these definitions. We first assume that $g$ is real; then so is $G$. As $g$ is continuous on $[a, b]$, so $g$ is Riemann-integrable, by Theorem 6.8 in Rudin; as $\alpha$ is monotonic on $[a, b]$, so $\alpha$ is Riemann-integrable, by Theorem 6.9 in Rudin. Now as both $\alpha$ and $g$ are Riemann-integrable on $[a, b]$, so is $\alpha g$, by Theorem 6.13 (a) in Rudin. Thus $ \int_a^b \alpha(x) g(x) \ \mathrm{d} x$ exists in $\mathbb{R}$. As $G$ is differentiable on $[a, b]$, so $G$ is also continuous on $[a, b]$, by Theorem 5.2 in Rudin, and as $G$ is continuous on $[a, b]$ and $\alpha$ is monotonically increasing on $[a, b]$, so $G$ is Riemann-Stieltjes integrable with respect to $\alpha$ on $[a, b]$, again by Theorem 6.8 in Rudin. Thus $\int_a^b G \ \mathrm{d} \alpha$ exists in $\mathbb{R}$. Let $P = \left\{ \ x_0, x_1, \ldots, x_n \ \right\}$ be a partition of $[a, b]$. For each $i = 1, \ldots, n$, as $G$ is continuous on $\left[ x_{i-1}, x_i \right]$ and differentiable on $\left( x_{i-1}, x_i \right)$, so by the mean-value theorem we can find a point $t_i \in \left( x_{i-1}, x_i \right)$ such that    $$ G \left( x_i \right) - G \left( x_{i-1} \right) = G^\prime \left( t_i \right) \cdot \left( x_i - x_{i-1} \right) = g \left( t_i \right) \Delta x_i. \tag{0} $$    So    $$ \begin{align} \sum_{i=1}^n \alpha \left( x_i \right) g \left( t_i \right) \Delta x_i &= \sum_{i=1}^n \alpha \left( x_i \right) \left[ G \left( x_i \right) - G \left( x_{i-1} \right) \right] \qquad \mbox{ [ using (0) ] } \\ &= \sum_{i=1}^n \alpha \left( x_i \right) G \left( x_i \right) - \sum_{i=1}^n  \alpha \left( x_i \right) G \left( x_{i-1} \right)  \\ &= \sum_{i=1}^n \alpha \left( x_i \right) G \left( x_i \right) - \sum_{i=0}^{n-1}  \alpha \left( x_{i+1} \right) G \left( x_i \right)  \\ &= \alpha \left(x_n \right) G \left( x_n \right) + \sum_{i=1}^{n-1} \alpha \left( x_i \right) G \left( x_i \right) - \sum_{i=0}^{n-1}  \alpha \left( x_{i+1} \right) G \left( x_i \right) \\ &= \alpha(b) G(b) - \alpha (a) G(a) + \sum_{i=0}^{n-1} \alpha \left( x_i \right) G \left( x_i \right) - \sum_{i=0}^{n-1}  \alpha \left( x_{i+1} \right) G \left( x_i \right) \\ &= \alpha(b) G(b) - \alpha (a) G(a) + \sum_{i=0}^{n-1} \left[ \alpha \left( x_i \right) -  \alpha \left( x_{i+1} \right) \right] G \left( x_i \right) \\  &= \alpha(b) G(b) - \alpha(a) G(a) -  \sum_{i=0}^{n-1} \left[ \alpha \left( x_{i+1} \right) -   \alpha \left( x_i  \right) \right] G \left( x_i \right) \\ &= \alpha(b) G(b) - \alpha(a) G(a) -  \sum_{i=1}^{n} \left[ \alpha \left( x_{i} \right) -   \alpha \left( x_{i-1}  \right) \right] G \left( x_{i-1} \right) \\ &= \alpha(b) G(b) - \alpha(a) G(a) -  \sum_{i=1}^{n} G \left( x_{i-1} \right) \Delta \alpha_i.  \end{align} $$ What next? How to proceed from here? Now we assume that $g$ is a complex function on $[a, b]$, that $g$ is continuous, and $g(x) = G^\prime (x)$. Then $g = \Re g + \iota \Im g$, and $\Re g$ and $\Im g$ are both real continuous functions on $[a, b]$. Moreover, the function $G$ too is complex, and $G = \Re G + \iota \Im G$, where $\Re G$ and $\Im G$ are real functions on $[a, b]$ such that $ \left( \Re G \right)^\prime = \Re g$ and $ \left( \Im G \right)^\prime = \Im g$. The function $\alpha$ is still real of course. So    $$  \begin{align} \int_a^b \alpha(x) g(x) \ \mathrm{d} x &= \int_a^b \alpha(x) \Re g(x) \ \mathrm{d} x + \iota \int_a^b \alpha(x) \Im g(x) \ \mathrm{d} x \\ &= \left[ \alpha (b) \Re G(b) - \alpha(a) \Re G(a) - \int_a^b \Re G \ \mathrm{d} \alpha \right] \\ & \qquad  + \iota \left[ \alpha (b) \Im G(b) - \alpha(a) \Im G(a) - \int_a^b \Im G \ \mathrm{d} \alpha  \right] \\ &= \alpha (b) G(b) - \alpha(a) G(a) - \int_a^b G \ \mathrm{d} \alpha,  \end{align} $$   as required. Is this proof for complex functions correct? If so, then is it rigorous enough too? If not, then where have I erred?",,"['calculus', 'real-analysis', 'integration', 'analysis', 'definite-integrals']"
76,"Prob. 12, Chap. 6, in Baby Rudin: Denseness of $\mathrm{C}[a,b]$ in $L^2[a,b]$","Prob. 12, Chap. 6, in Baby Rudin: Denseness of  in","\mathrm{C}[a,b] L^2[a,b]","Here is Prob. 12, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: With the notations of Exercise 11, suppose $f \in \mathscr{R}(\alpha)$ and $\varepsilon > 0$ . Prove that there exists a continuous function $g$ on $[a, b]$ such that $\lVert f-g \rVert_2 < \varepsilon$ . Hint: Let $P = \left\{ \ x_0, \ldots, x_n \ \right\}$ be a suitable partition of $[a, b]$ , define $$ g(t) = \frac{ x_i - t }{ \Delta x_i } f \left( x_{i-1} \right) + \frac{ t- x_{i-1} }{ \Delta x_i } f \left( x_i \right) $$ if $x_{i-1} \leq t \leq x_i$ . My Attempt: Here is the link to one of my Math SE posts in which I have included Definitions 6.1 and 6.2 from Rudin, where all the notation to be used below is  given: Theorem 6.15 in Baby Rudin: If $a<s<b$, $f$ is bounded on $[a,b]$, $f$ is continuous at $s$, and $\alpha(x)=I(x-s)$, then . . . Let us first assume that $f$ is a real function defined on $[a, b]$ . As $f$ is  Riemann-Stieltjes integrable on $[a, b]$ , so it is  bounded on $[a, b]$ . Let us put $$ M \colon= \sup \left\{ \ \lvert f(x) \rvert \ \colon \ a \leq x \leq b \ \right\}. \tag{A} $$ Again as $f$ is  Riemann-Stieltjes integrable on $[a, b]$ , so by Theorem 6.4 in Rudin we can find a partition $P = \left\{ \ x_0, \ldots, x_n \ \right\}$ of $[a, b]$ for which $$ U(P, f, \alpha) - L(P, f, \alpha) < \frac{\varepsilon^2 }{4 M \left[ \alpha(b) - \alpha(a) \right] + 1  }. \tag{B}$$ Then for each $i = 1, \ldots, n$ , we have $$ \left( M_i - m_i \right) \Delta \alpha_i  < \frac{\varepsilon^2 }{4 M \left[ \alpha(b) - \alpha(a) \right] + 1  }. \tag{C}$$ For each $i = 1, \ldots, n$ , and for every $t_i$ such that $x_{i-1} \leq t_i \leq x_i$ , we see that $$ \begin{align}  \left\lvert f\left(t_i \right) - g \left(t_i \right) \right\rvert &= \left\lvert \frac{x_i - t_i + t_i - x_{i-1} }{\Delta x_i } f\left(t_i \right) - \left[ \frac{ x_i - t_i }{ \Delta x_i } f \left( x_{i-1} \right) + \frac{ t_i- x_{i-1} }{ \Delta x_i } f \left( x_i \right)  \right] \right\rvert  \\ &= \left\lvert \frac{x_i - t_i }{\Delta x_i} \left[ f \left(t_i \right) - f \left( x_{i-1} \right) \right] + \frac{ t_i - x_{i-1} }{\Delta x_i} \left[ f \left(t_i \right) - f \left( x_i  \right) \right] \right\rvert \\ &\leq \frac{x_i - t_i }{\Delta x_i} \left\lvert f \left(t_i \right) - f \left( x_{i-1} \right)  \right\rvert + \frac{ t_i - x_{i-1} }{\Delta x_i} \left\lvert f \left(t_i \right) - f \left( x_i  \right)  \right\rvert \\ &\leq \frac{x_i - t_i }{\Delta x_i} \left( M_i - m_i \right)  + \frac{t_i - x_{i-1} }{\Delta x_i} \left( M_i - m_i \right) \\ &  \mbox{ [ where $m_i \colon= \inf \left\{ \ f(t) \ \colon \ x_{i-1} \leq t \leq x_i \ \right\}$, and $M_i \colon= \sup \left\{ \ f(t) \ \colon \ x_{i-1} \leq t \leq x_i \ \right\}$ ] } \\ &= M_i - m_i,  \end{align} $$ which implies that $$ \left\lvert f\left(t_i \right) - g \left(t_i \right) \right\rvert^2 \leq \left( M_i - m_i \right)^2, $$ and so $$ \left\lvert f \left(t_i \right) - g \left(t_i \right) \right\rvert^2 \Delta \alpha_i \leq \left( M_i - m_i \right)^2 \Delta \alpha_i. $$ Therefore, for each $t_i \in \left[ x_{i-1}, x_i \right]$ for each $i = 1, \ldots, n$ , we have $$ \begin{align} 0 &\leq  \sum_{i=1}^n \left\lvert f \left(t_i \right) - g \left(t_i \right) \right\rvert^2 \Delta \alpha_i  \\ &\leq \sum_{i=1}^n \left( M_i - m_i \right)^2 \Delta \alpha_i \\ &\leq \sum_{i-1}^n 2M \left( M_i - m_i \right) \Delta \alpha_i \qquad \mbox{ [ using (A) above ] } \\ &= \sum_{i=1}^n 2 M \frac{ \varepsilon^2 }{ 4 M \left[ \alpha(b) - \alpha(a) \right] + 1} \Delta \alpha_i \qquad \mbox{ [ using (C) above ] } \\ &= 2M \frac{ \varepsilon^2 }{ 4M \left[ \alpha(b) - \alpha(a) \right] + 1} \left[ \alpha(b) - \alpha(a) \right]\\ &< \frac{\varepsilon^2 }{2},  \end{align} $$ and so $$ - \frac{\varepsilon^2 }{2} \leq \sum_{i=1}^n \left\lvert f \left(t_i \right) - g \left(t_i \right) \right\rvert^2 \Delta \alpha_i \leq  \frac{\varepsilon^2 }{2}. \tag{0}  $$ But $$ L \left(P, \lvert f-g \rvert^2 , \alpha \right) = \inf \left\{ \ \sum_{i=1}^n \left\lvert f \left(t_i \right) - g \left(t_i \right) \right\rvert^2 \Delta \alpha_i  \ \colon \ t_i \in \left[ x_{i-1}, x_i \right] \ i = 1, \ldots, n \ \right\}, \tag{1} $$ and $$ U \left( P, \lvert f-g \rvert^2 , \alpha \right) = \sup \left\{ \ \sum_{i=1}^n \left\lvert f \left(t_i \right) - g \left(t_i \right) \right\rvert^2 \Delta \alpha_i  \ \colon \ t_i \in \left[ x_{i-1}, x_i \right] \ i = 1, \ldots, n \ \right\}. \tag{2} $$ Here is the link to a relevant post of mine here on Math SE: https://math.stackexchange.com/questions/2358808/riemann-stieltjes-upper-and-lower-sums-as-suprema-and-infima Now from (0), (1), and (2)  we can conclude that $$ - \frac{\varepsilon^2 }{2} \leq L \left( P, \lvert f-g \rvert^2, \alpha \right) \leq U \left( P , \lvert f-g \rvert^2 , \alpha \right) \leq \frac{\varepsilon^2 }{2}. \tag{3}   $$ As $g$ is continuous on $[a, b]$ , so $g \in \mathscr{R}(\alpha)$ on $[a, b]$ , by Theorem 6.8 in Rudin. Now as $f, g \in \mathscr{R}(\alpha)$ on $[a, b]$ , so $f - g \in \mathscr{R}(\alpha)$ , by Theorem 6.12 (a) in Rudin. Here are the links to my Math SE post on Theorem 6.12 (a) in Rudin: Theorem 6.12 (a) in Baby Rudin: $\int_a^b \left( f_1 + f_2 \right) d \alpha=\int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha$ https://math.stackexchange.com/questions/2327134/theorem-6-12-a-in-baby-rudin-if-f-in-mathscrr-alpha-on-a-b-then-c Furthermore, as $ f-g  \in \mathscr{R} (\alpha)$ on $[a, b]$ , so $ \lvert f-g  \rvert \in \mathscr{R} (\alpha) $ on $[a, b]$ as well, by virtue of Theorem 6.13 (b) in Rudin. Finally, as $ \lvert f-g  \rvert  \in \mathscr{R} (\alpha) $ on $[a, b]$ and as the map $y \mapsto y^2$ is continuous on all of $[0, +\infty)$ , so we can conclude that $ \lvert f-g  \rvert^2  \in \mathscr{R} (\alpha) $ on $[a, b]$ also, by virtue of Theorem 6.11 in Rudin. Thus $\int_a^b \lvert f-g \rvert^2 \ \mathrm{d} \alpha $ exists.  Moreover, for any partition $Q$ of $[a, b]$ , we have the inequalities $$ L \left( Q, \lvert f-g \rvert^2, \alpha \right) \leq \int_a^b \lvert f-g \rvert^2 \ \mathrm{d} \alpha \leq  U \left( Q, \lvert f-g \rvert^2 , \alpha \right). \tag{4} $$ From (3) and (4) we can conclude that $$ - \frac{\varepsilon^2}{2} \leq \int_a^b \lvert f-g \rvert^2 \ \mathrm{d} \alpha \leq \frac{\varepsilon^2}{2}. \tag{5} $$ But as $\lvert f - g \rvert^2 \geq 0$ on $[a, b]$ , so by Theorem 6.12 (b) in Rudin $$ \int_a^b \lvert f - g \rvert^2 \ \mathrm{d} \alpha \geq 0. \tag{6} $$ Here is the link to my Math SE post on Theorem 6.12 (b) in Rudin: Theorem 6.12 (b) in Baby Rudin: If $f_1 \leq f_2$ on $[a, b]$, then $\int_a^b f_1 d\alpha \leq \int_a^b f_2 d\alpha$ From (5) and (6) we obtain $$ 0 \leq \int_a^b \lvert f - g \rvert^2 \ \mathrm{d} \alpha \leq \frac{\varepsilon^2}{2} < \varepsilon^2. $$ Therefore, $$ \left( \int_a^b \lvert f - g \rvert^2 \ \mathrm{d} \alpha \right)^{1/2} < \varepsilon, $$ which is the same as $$ \lVert f-g \rVert_2 < \varepsilon. $$ And the function $g$ is continuous on $[a, b]$ . Is my proof correct? Is it rigorous enough? If not, then where does it lack? How to rigorously show that this  function $g$ is continuous? Now we assume that $f$ is a complex function defined on $[a, b]$ . Then $f = \Re f + \iota \Im f$ , where $\Re f$ and $\Im f$ are real functions defined on $[a, b]$ . As $f$ is Riemann-Stieltjes integrable with respect to $\alpha$ on $[a, b]$ , so are $\Re f$ and $\Im f$ . Then we have $$ \int_a^b f \ \mathrm{d} \alpha =  \int_a^b \Re f \ \mathrm{d} \alpha +  \iota \int_a^b \Im f \ \mathrm{d} \alpha.  $$ Applying the result just proved for real functions to $\Re f$ and $\Im f$ , we can conclude that there are continuous functions $g_1$ and $g_2$ on $[a, b]$ such that $$ \left\lVert \Re f \ - \ g_1 \right\rVert < \frac{ \varepsilon}{4}, $$ and $$ \left\lVert \Im f \ - \ g_2  \right\rVert < \frac{ \varepsilon}{4}; $$ therefore if we put $ g \colon= g_1 + \iota g_2$ on $[a, b]$ , we  have $$  \begin{align} \lVert f - g \rVert_2 &= \left\lVert \left( \Re f + \iota \Im f \right) \ -  \ \left( g_1 + \iota g_2 \right) \right\rVert_2 \\ &= \left\lVert \left( \Re f - g_1 \right) \ + \ \iota    \left( \Im f -  g_2 \right) \right\rVert_2 \\ &\leq \left\lVert  \Re f - g_1 \right\rVert_2 \ + \ \left\lVert \iota    \left( \Im f -  g_2 \right) \right\rVert_2 \\ &= \left\lVert  \Re f - g_1 \right\rVert_2 \ + \ \lvert \iota \rvert \cdot  \left\lVert \Im f -  g_2  \right\rVert_2 \\  &= \left\lVert  \Re f - g_1 \right\rVert_2 \ + \  \left\lVert \Im f -  g_2  \right\rVert_2 \\ &< \frac{\varepsilon}{4} + \frac{\varepsilon}{4} \\ &< \varepsilon. \end{align} $$ Finally, as $g_1 = \Re g$ and $g_2 = \Im g$ are both continuous on $[a, b]$ , so is $g$ . Is this proof for complex functions correct too? If so, is it rigorous enough too? If not, then what is it that I am missing out on?","Here is Prob. 12, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: With the notations of Exercise 11, suppose and . Prove that there exists a continuous function on such that . Hint: Let be a suitable partition of , define if . My Attempt: Here is the link to one of my Math SE posts in which I have included Definitions 6.1 and 6.2 from Rudin, where all the notation to be used below is  given: Theorem 6.15 in Baby Rudin: If $a<s<b$, $f$ is bounded on $[a,b]$, $f$ is continuous at $s$, and $\alpha(x)=I(x-s)$, then . . . Let us first assume that is a real function defined on . As is  Riemann-Stieltjes integrable on , so it is  bounded on . Let us put Again as is  Riemann-Stieltjes integrable on , so by Theorem 6.4 in Rudin we can find a partition of for which Then for each , we have For each , and for every such that , we see that which implies that and so Therefore, for each for each , we have and so But and Here is the link to a relevant post of mine here on Math SE: https://math.stackexchange.com/questions/2358808/riemann-stieltjes-upper-and-lower-sums-as-suprema-and-infima Now from (0), (1), and (2)  we can conclude that As is continuous on , so on , by Theorem 6.8 in Rudin. Now as on , so , by Theorem 6.12 (a) in Rudin. Here are the links to my Math SE post on Theorem 6.12 (a) in Rudin: Theorem 6.12 (a) in Baby Rudin: $\int_a^b \left( f_1 + f_2 \right) d \alpha=\int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha$ https://math.stackexchange.com/questions/2327134/theorem-6-12-a-in-baby-rudin-if-f-in-mathscrr-alpha-on-a-b-then-c Furthermore, as on , so on as well, by virtue of Theorem 6.13 (b) in Rudin. Finally, as on and as the map is continuous on all of , so we can conclude that on also, by virtue of Theorem 6.11 in Rudin. Thus exists.  Moreover, for any partition of , we have the inequalities From (3) and (4) we can conclude that But as on , so by Theorem 6.12 (b) in Rudin Here is the link to my Math SE post on Theorem 6.12 (b) in Rudin: Theorem 6.12 (b) in Baby Rudin: If $f_1 \leq f_2$ on $[a, b]$, then $\int_a^b f_1 d\alpha \leq \int_a^b f_2 d\alpha$ From (5) and (6) we obtain Therefore, which is the same as And the function is continuous on . Is my proof correct? Is it rigorous enough? If not, then where does it lack? How to rigorously show that this  function is continuous? Now we assume that is a complex function defined on . Then , where and are real functions defined on . As is Riemann-Stieltjes integrable with respect to on , so are and . Then we have Applying the result just proved for real functions to and , we can conclude that there are continuous functions and on such that and therefore if we put on , we  have Finally, as and are both continuous on , so is . Is this proof for complex functions correct too? If so, is it rigorous enough too? If not, then what is it that I am missing out on?","f \in \mathscr{R}(\alpha) \varepsilon > 0 g [a, b] \lVert f-g \rVert_2 < \varepsilon P = \left\{ \ x_0, \ldots, x_n \ \right\} [a, b]  g(t) = \frac{ x_i - t }{ \Delta x_i } f \left( x_{i-1} \right) + \frac{ t- x_{i-1} }{ \Delta x_i } f \left( x_i \right)  x_{i-1} \leq t \leq x_i f [a, b] f [a, b] [a, b]  M \colon= \sup \left\{ \ \lvert f(x) \rvert \ \colon \ a \leq x \leq b \ \right\}. \tag{A}  f [a, b] P = \left\{ \ x_0, \ldots, x_n \ \right\} [a, b]  U(P, f, \alpha) - L(P, f, \alpha) < \frac{\varepsilon^2 }{4 M \left[ \alpha(b) - \alpha(a) \right] + 1  }. \tag{B} i = 1, \ldots, n  \left( M_i - m_i \right) \Delta \alpha_i  < \frac{\varepsilon^2 }{4 M \left[ \alpha(b) - \alpha(a) \right] + 1  }. \tag{C} i = 1, \ldots, n t_i x_{i-1} \leq t_i \leq x_i 
\begin{align}
 \left\lvert f\left(t_i \right) - g \left(t_i \right) \right\rvert &= \left\lvert \frac{x_i - t_i + t_i - x_{i-1} }{\Delta x_i } f\left(t_i \right) - \left[ \frac{ x_i - t_i }{ \Delta x_i } f \left( x_{i-1} \right) + \frac{ t_i- x_{i-1} }{ \Delta x_i } f \left( x_i \right)  \right] \right\rvert  \\
&= \left\lvert \frac{x_i - t_i }{\Delta x_i} \left[ f \left(t_i \right) - f \left( x_{i-1} \right) \right] + \frac{ t_i - x_{i-1} }{\Delta x_i} \left[ f \left(t_i \right) - f \left( x_i  \right) \right] \right\rvert \\
&\leq \frac{x_i - t_i }{\Delta x_i} \left\lvert f \left(t_i \right) - f \left( x_{i-1} \right)  \right\rvert + \frac{ t_i - x_{i-1} }{\Delta x_i} \left\lvert f \left(t_i \right) - f \left( x_i  \right)  \right\rvert \\
&\leq \frac{x_i - t_i }{\Delta x_i} \left( M_i - m_i \right)  + \frac{t_i - x_{i-1} }{\Delta x_i} \left( M_i - m_i \right) \\
&  \mbox{ [ where m_i \colon= \inf \left\{ \ f(t) \ \colon \ x_{i-1} \leq t \leq x_i \ \right\}, and M_i \colon= \sup \left\{ \ f(t) \ \colon \ x_{i-1} \leq t \leq x_i \ \right\} ] } \\
&= M_i - m_i, 
\end{align}
  \left\lvert f\left(t_i \right) - g \left(t_i \right) \right\rvert^2 \leq \left( M_i - m_i \right)^2,   \left\lvert f \left(t_i \right) - g \left(t_i \right) \right\rvert^2 \Delta \alpha_i \leq \left( M_i - m_i \right)^2 \Delta \alpha_i.  t_i \in \left[ x_{i-1}, x_i \right] i = 1, \ldots, n 
\begin{align}
0 &\leq  \sum_{i=1}^n \left\lvert f \left(t_i \right) - g \left(t_i \right) \right\rvert^2 \Delta \alpha_i  \\
&\leq \sum_{i=1}^n \left( M_i - m_i \right)^2 \Delta \alpha_i \\
&\leq \sum_{i-1}^n 2M \left( M_i - m_i \right) \Delta \alpha_i \qquad \mbox{ [ using (A) above ] } \\
&= \sum_{i=1}^n 2 M \frac{ \varepsilon^2 }{ 4 M \left[ \alpha(b) - \alpha(a) \right] + 1} \Delta \alpha_i \qquad \mbox{ [ using (C) above ] } \\
&= 2M \frac{ \varepsilon^2 }{ 4M \left[ \alpha(b) - \alpha(a) \right] + 1} \left[ \alpha(b) - \alpha(a) \right]\\
&< \frac{\varepsilon^2 }{2}, 
\end{align}
  - \frac{\varepsilon^2 }{2} \leq \sum_{i=1}^n \left\lvert f \left(t_i \right) - g \left(t_i \right) \right\rvert^2 \Delta \alpha_i \leq  \frac{\varepsilon^2 }{2}. \tag{0}    L \left(P, \lvert f-g \rvert^2 , \alpha \right) = \inf \left\{ \ \sum_{i=1}^n \left\lvert f \left(t_i \right) - g \left(t_i \right) \right\rvert^2 \Delta \alpha_i  \ \colon \ t_i \in \left[ x_{i-1}, x_i \right] \ i = 1, \ldots, n \ \right\}, \tag{1}   U \left( P, \lvert f-g \rvert^2 , \alpha \right) = \sup \left\{ \ \sum_{i=1}^n \left\lvert f \left(t_i \right) - g \left(t_i \right) \right\rvert^2 \Delta \alpha_i  \ \colon \ t_i \in \left[ x_{i-1}, x_i \right] \ i = 1, \ldots, n \ \right\}. \tag{2}   - \frac{\varepsilon^2 }{2} \leq L \left( P, \lvert f-g \rvert^2, \alpha \right) \leq U \left( P , \lvert f-g \rvert^2 , \alpha \right) \leq \frac{\varepsilon^2 }{2}. \tag{3}    g [a, b] g \in \mathscr{R}(\alpha) [a, b] f, g \in \mathscr{R}(\alpha) [a, b] f - g \in \mathscr{R}(\alpha)  f-g  \in \mathscr{R} (\alpha) [a, b]  \lvert f-g  \rvert \in \mathscr{R} (\alpha)  [a, b]  \lvert f-g  \rvert  \in \mathscr{R} (\alpha)  [a, b] y \mapsto y^2 [0, +\infty)  \lvert f-g  \rvert^2  \in \mathscr{R} (\alpha)  [a, b] \int_a^b \lvert f-g \rvert^2 \ \mathrm{d} \alpha  Q [a, b]  L \left( Q, \lvert f-g \rvert^2, \alpha \right) \leq \int_a^b \lvert f-g \rvert^2 \ \mathrm{d} \alpha \leq  U \left( Q, \lvert f-g \rvert^2 , \alpha \right). \tag{4}   - \frac{\varepsilon^2}{2} \leq \int_a^b \lvert f-g \rvert^2 \ \mathrm{d} \alpha \leq \frac{\varepsilon^2}{2}. \tag{5}  \lvert f - g \rvert^2 \geq 0 [a, b]  \int_a^b \lvert f - g \rvert^2 \ \mathrm{d} \alpha \geq 0. \tag{6}   0 \leq \int_a^b \lvert f - g \rvert^2 \ \mathrm{d} \alpha \leq \frac{\varepsilon^2}{2} < \varepsilon^2.   \left( \int_a^b \lvert f - g \rvert^2 \ \mathrm{d} \alpha \right)^{1/2} < \varepsilon,   \lVert f-g \rVert_2 < \varepsilon.  g [a, b] g f [a, b] f = \Re f + \iota \Im f \Re f \Im f [a, b] f \alpha [a, b] \Re f \Im f  \int_a^b f \ \mathrm{d} \alpha =  \int_a^b \Re f \ \mathrm{d} \alpha +  \iota \int_a^b \Im f \ \mathrm{d} \alpha.   \Re f \Im f g_1 g_2 [a, b]  \left\lVert \Re f \ - \ g_1 \right\rVert < \frac{ \varepsilon}{4},   \left\lVert \Im f \ - \ g_2  \right\rVert < \frac{ \varepsilon}{4};   g \colon= g_1 + \iota g_2 [a, b]  
\begin{align}
\lVert f - g \rVert_2 &= \left\lVert \left( \Re f + \iota \Im f \right) \ - 
\ \left( g_1 + \iota g_2 \right) \right\rVert_2 \\
&= \left\lVert \left( \Re f - g_1 \right) \ + \ \iota  
 \left( \Im f -  g_2 \right) \right\rVert_2 \\
&\leq \left\lVert  \Re f - g_1 \right\rVert_2 \ + \ \left\lVert \iota  
 \left( \Im f -  g_2 \right) \right\rVert_2 \\
&= \left\lVert  \Re f - g_1 \right\rVert_2 \ + \ \lvert \iota \rvert \cdot 
\left\lVert \Im f -  g_2  \right\rVert_2 \\ 
&= \left\lVert  \Re f - g_1 \right\rVert_2 \ + \ 
\left\lVert \Im f -  g_2  \right\rVert_2 \\
&< \frac{\varepsilon}{4} + \frac{\varepsilon}{4} \\
&< \varepsilon.
\end{align}
 g_1 = \Re g g_2 = \Im g [a, b] g","['real-analysis', 'integration', 'analysis', 'definite-integrals', 'lp-spaces']"
77,"Prob. 8, Chap. 6, in Baby Rudin: The Integral Test for Convergence of Series","Prob. 8, Chap. 6, in Baby Rudin: The Integral Test for Convergence of Series",,"Here is Prob. 8, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f \in \mathscr{R}$ on $[a, b]$ for every $b > a$ where $a$ is fixed. Define    $$ \int_a^\infty f(x) \ \mathrm{d} x = \lim_{b \to \infty} \int_a^b f(x) \ \mathrm{d} x $$   if this limit exists (and is finite). In that case, we say that the integral on the left converges. If it also converges after $f$ has been replaced by $\lvert f \rvert$, it is said to converge absolutely. Assume that $f(x) \geq 0$ and that $f$ decreases monotonically on $[1, \infty)$. Prove that    $$   \int_a^\infty f(x) \ \mathrm{d} x $$   converges if and only if    $$ \sum_{n=1}^\infty f(n) $$   converges. (This is the so-called ""integral test"" for convergence of series. ) My Attempt: As $f(x) \geq 0$ on $[1, \infty)$, so for each $n\in \mathbb{N}$ we see that    $$ \sum_{k=1}^{n+1} f(k) = \sum_{k=1}^n f(k) \ + \  f(n+1) \geq \sum_{k=1}^n f(k). \tag{0} $$    Thus the sequence $\left( \sum_{k=1}^n f(k) \right)_{n \in \mathbb{N} }$ is a monotonically increasing sequence of (non-negative) real numbers. Suppose $b$ and $c$ are any two real numbers such that $1 < b < c$. Then we have    $$ \begin{align} \int_1^c f(x) \ \mathrm{d} x &=  \int_1^b f(x) \ \mathrm{d} x +  \int_b^c f(x) \ \mathrm{d} x \qquad \mbox{ [ by Theorem 6.12 (c) in Baby Rudin ] } \\ &\geq \int_1^b f(x) \ \mathrm{d} x + 0 \\ & \ \ \ \qquad \mbox{ [ by Theorem 6.12 (b) in Baby Rudin since $f(x) \geq 0$ on $[b, c]$ ] } \\ &= \int_1^b f(x) \ \mathrm{d} x. \tag{1} \end{align} $$    which shows that the function $g$ defined on $(1, \infty)$ by    $$ g(b) = \int_1^b f(x) \ \mathrm{d} x \tag{A} $$   is a monotonically increasing function. Now suppose that $b$ is a real number such that $b > 2$. Let $n$ be the natural number such that $n \leq b < n+1$. Then we see that    $$  \begin{align} \int_1^b f(x) \ \mathrm{d} x &\leq \int_1^{n+1} f(x) \ \mathrm{d} x \qquad \mbox{ [ using (1) above; note that $n \leq b < n+1 $ ] } \\ &= \sum_{k=1}^n \int_k^{k+1} f(x) \ \mathrm{d} x \qquad \mbox{ [ by an extension of Theorem 6.12 (c) in Rudin ] } \\ &\leq \sum_{k=1}^n \int_k^{k+1} f(k) \ \mathrm{d} x \\ &\ \ \ \qquad \mbox{ [ using Theorem 6.12 in Baby Rudin and the monotonicity of $f$ ] } \\ &= \sum_{k=1}^n f(k). \tag{2}  \end{align} $$   And, also    $$  \begin{align} \int_1^b f(x) \ \mathrm{d} x &\geq \int_1^n  f(x) \ \mathrm{d} x \qquad \mbox{ [ using (1) above; note that $n \leq b < n+1 $ ] } \\ &= \sum_{k=1}^{n-1} \int_k^{k+1} f(x) \ \mathrm{d} x \qquad \mbox{ [ by an extension of Theorem 6.12 (c) in Rudin ] } \\ &\geq \sum_{k=1}^{n-1} \int_k^{k+1} f(k+1) \ \mathrm{d} x \\ &\ \ \ \qquad \mbox{ [ using Theorem 6.12 in Baby Rudin and the monotonicity of $f$ ] } \\ &= \sum_{k=1}^{n-1} f(k+1) \\ &= \sum_{k=2}^n f(k). \tag{3}  \end{align} $$ Thus from (2) and (3) we can conclude that for every real number $b > 2$, we have    $$ \sum_{k=2}^n f(k) \leq \int_1^b f(x) \ \mathrm{d} x \leq  \sum_{k=1}^n f(k), \tag{4} $$   where $n$ is the natural number such that $n \leq b < n+1$. Now suppose that $\int_1^\infty f(x) \ \mathrm{d} x$ converges. This means that $\lim_{b \to \infty} \int_1^b f(x) \ \mathrm{d} x $ exists in $\mathbb{R}$,  and in the light of (1) we can also write $$\int_1^\infty f(x) \ \mathrm{d} x =  \lim_{b \to \infty} \int_1^b f(x) \ \mathrm{d} x = \sup \left\{ \ \int_1^b f(x) \ \mathrm{d} x \ \colon \ b \in \mathbb{R}, \ b > 1 \ \right\}. \tag{5} $$ Then for any natural number $n \geq 2$, if we take $b \in (n, n+1)$, then from (4) and (5) we can conclude that    $$ \sum_{k=2}^n f(k) \leq \int_1^\infty f(x) \ \mathrm{d} x, $$   which implies that    $$ \sum_{k=1}^n f(k) \leq f(1) \ + \ \int_1^\infty f(x) \ \mathrm{d} x. $$   Thus the sequence  $\left( \sum_{k=1}^n f(k) \right)_{n \in \mathbb{N}}$ of (non-negative) real numbers is bounded above, and (0) shows that this sequence is also monotonically increasing;  therefore this sequence is convergent (in $\mathbb{R}$). That is, the series $\sum f(n) $ is convergent. Conversely, suppose that the series $\sum f(n)$ is convergent. Then the sequence  $\left( \sum_{k=1}^n f(k) \right)_{n \in \mathbb{N}}$ converges in $\mathbb{R}$; but by (0) above this is a monotonically increasing sequence of real numbers; so it is a bounded above sequence and    $$ \sum_{n=1}^\infty f(n) = \lim_{n \to \infty} \sum_{k=1}^n f(k) = \sup \left\{ \  \sum_{k=1}^n f(k) \ \colon \ n \in \mathbb{N} \ \right\}. \tag{6} $$ Now let $b$ be any real number such that $b > 2$, and let $n$ be the natural number such that $n \leq b < n+1$. Then from (4) and (6) we can conclude that    $$ \int_1^b f(x) \ \mathrm{d} x \leq \sum_{k=1}^n f(k) \leq \sum_{n=1}^\infty f(n). $$ Thus the function $g$ given by (A) above is a monotonically increasing function on $(2, \infty)$ which is also bounded above. So $\lim_{b \to \infty} g(b)$ exists in $\mathbb{R}$; that is $ \lim_{b \to \infty} \int_1^b f(x) \ \mathrm{d} x$ exists in $\mathbb{R}$, which is the same as saying that    $\int_1^\infty f(x) \ \mathrm{d} x$ converges. Is my proof correct? If so, then have I managed to present it in enough detail and rigorous for it to be understood by someone who is not very sharp (like myself) and is also taking their very first course in analysis? Or, have I committed some blunders?","Here is Prob. 8, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f \in \mathscr{R}$ on $[a, b]$ for every $b > a$ where $a$ is fixed. Define    $$ \int_a^\infty f(x) \ \mathrm{d} x = \lim_{b \to \infty} \int_a^b f(x) \ \mathrm{d} x $$   if this limit exists (and is finite). In that case, we say that the integral on the left converges. If it also converges after $f$ has been replaced by $\lvert f \rvert$, it is said to converge absolutely. Assume that $f(x) \geq 0$ and that $f$ decreases monotonically on $[1, \infty)$. Prove that    $$   \int_a^\infty f(x) \ \mathrm{d} x $$   converges if and only if    $$ \sum_{n=1}^\infty f(n) $$   converges. (This is the so-called ""integral test"" for convergence of series. ) My Attempt: As $f(x) \geq 0$ on $[1, \infty)$, so for each $n\in \mathbb{N}$ we see that    $$ \sum_{k=1}^{n+1} f(k) = \sum_{k=1}^n f(k) \ + \  f(n+1) \geq \sum_{k=1}^n f(k). \tag{0} $$    Thus the sequence $\left( \sum_{k=1}^n f(k) \right)_{n \in \mathbb{N} }$ is a monotonically increasing sequence of (non-negative) real numbers. Suppose $b$ and $c$ are any two real numbers such that $1 < b < c$. Then we have    $$ \begin{align} \int_1^c f(x) \ \mathrm{d} x &=  \int_1^b f(x) \ \mathrm{d} x +  \int_b^c f(x) \ \mathrm{d} x \qquad \mbox{ [ by Theorem 6.12 (c) in Baby Rudin ] } \\ &\geq \int_1^b f(x) \ \mathrm{d} x + 0 \\ & \ \ \ \qquad \mbox{ [ by Theorem 6.12 (b) in Baby Rudin since $f(x) \geq 0$ on $[b, c]$ ] } \\ &= \int_1^b f(x) \ \mathrm{d} x. \tag{1} \end{align} $$    which shows that the function $g$ defined on $(1, \infty)$ by    $$ g(b) = \int_1^b f(x) \ \mathrm{d} x \tag{A} $$   is a monotonically increasing function. Now suppose that $b$ is a real number such that $b > 2$. Let $n$ be the natural number such that $n \leq b < n+1$. Then we see that    $$  \begin{align} \int_1^b f(x) \ \mathrm{d} x &\leq \int_1^{n+1} f(x) \ \mathrm{d} x \qquad \mbox{ [ using (1) above; note that $n \leq b < n+1 $ ] } \\ &= \sum_{k=1}^n \int_k^{k+1} f(x) \ \mathrm{d} x \qquad \mbox{ [ by an extension of Theorem 6.12 (c) in Rudin ] } \\ &\leq \sum_{k=1}^n \int_k^{k+1} f(k) \ \mathrm{d} x \\ &\ \ \ \qquad \mbox{ [ using Theorem 6.12 in Baby Rudin and the monotonicity of $f$ ] } \\ &= \sum_{k=1}^n f(k). \tag{2}  \end{align} $$   And, also    $$  \begin{align} \int_1^b f(x) \ \mathrm{d} x &\geq \int_1^n  f(x) \ \mathrm{d} x \qquad \mbox{ [ using (1) above; note that $n \leq b < n+1 $ ] } \\ &= \sum_{k=1}^{n-1} \int_k^{k+1} f(x) \ \mathrm{d} x \qquad \mbox{ [ by an extension of Theorem 6.12 (c) in Rudin ] } \\ &\geq \sum_{k=1}^{n-1} \int_k^{k+1} f(k+1) \ \mathrm{d} x \\ &\ \ \ \qquad \mbox{ [ using Theorem 6.12 in Baby Rudin and the monotonicity of $f$ ] } \\ &= \sum_{k=1}^{n-1} f(k+1) \\ &= \sum_{k=2}^n f(k). \tag{3}  \end{align} $$ Thus from (2) and (3) we can conclude that for every real number $b > 2$, we have    $$ \sum_{k=2}^n f(k) \leq \int_1^b f(x) \ \mathrm{d} x \leq  \sum_{k=1}^n f(k), \tag{4} $$   where $n$ is the natural number such that $n \leq b < n+1$. Now suppose that $\int_1^\infty f(x) \ \mathrm{d} x$ converges. This means that $\lim_{b \to \infty} \int_1^b f(x) \ \mathrm{d} x $ exists in $\mathbb{R}$,  and in the light of (1) we can also write $$\int_1^\infty f(x) \ \mathrm{d} x =  \lim_{b \to \infty} \int_1^b f(x) \ \mathrm{d} x = \sup \left\{ \ \int_1^b f(x) \ \mathrm{d} x \ \colon \ b \in \mathbb{R}, \ b > 1 \ \right\}. \tag{5} $$ Then for any natural number $n \geq 2$, if we take $b \in (n, n+1)$, then from (4) and (5) we can conclude that    $$ \sum_{k=2}^n f(k) \leq \int_1^\infty f(x) \ \mathrm{d} x, $$   which implies that    $$ \sum_{k=1}^n f(k) \leq f(1) \ + \ \int_1^\infty f(x) \ \mathrm{d} x. $$   Thus the sequence  $\left( \sum_{k=1}^n f(k) \right)_{n \in \mathbb{N}}$ of (non-negative) real numbers is bounded above, and (0) shows that this sequence is also monotonically increasing;  therefore this sequence is convergent (in $\mathbb{R}$). That is, the series $\sum f(n) $ is convergent. Conversely, suppose that the series $\sum f(n)$ is convergent. Then the sequence  $\left( \sum_{k=1}^n f(k) \right)_{n \in \mathbb{N}}$ converges in $\mathbb{R}$; but by (0) above this is a monotonically increasing sequence of real numbers; so it is a bounded above sequence and    $$ \sum_{n=1}^\infty f(n) = \lim_{n \to \infty} \sum_{k=1}^n f(k) = \sup \left\{ \  \sum_{k=1}^n f(k) \ \colon \ n \in \mathbb{N} \ \right\}. \tag{6} $$ Now let $b$ be any real number such that $b > 2$, and let $n$ be the natural number such that $n \leq b < n+1$. Then from (4) and (6) we can conclude that    $$ \int_1^b f(x) \ \mathrm{d} x \leq \sum_{k=1}^n f(k) \leq \sum_{n=1}^\infty f(n). $$ Thus the function $g$ given by (A) above is a monotonically increasing function on $(2, \infty)$ which is also bounded above. So $\lim_{b \to \infty} g(b)$ exists in $\mathbb{R}$; that is $ \lim_{b \to \infty} \int_1^b f(x) \ \mathrm{d} x$ exists in $\mathbb{R}$, which is the same as saying that    $\int_1^\infty f(x) \ \mathrm{d} x$ converges. Is my proof correct? If so, then have I managed to present it in enough detail and rigorous for it to be understood by someone who is not very sharp (like myself) and is also taking their very first course in analysis? Or, have I committed some blunders?",,"['real-analysis', 'integration', 'sequences-and-series', 'analysis', 'definite-integrals']"
78,Strong convexity and the Legendre transform,Strong convexity and the Legendre transform,,"Suppose that I have a strongly convex function $f(\mathbf{x}): \mathbb{R}^m \rightarrow \mathbb{R}$. Is the Legendre transform of this function also strongly convex? As far as I can tell, strict convexity of $f$ implies strict convexity of the Legendre transform of $f$ (as shown in https://proofwiki.org/wiki/Convexity_of_Function_implies_Convexity_of_its_Legendre_Transform ). However, I am unsure as to whether the same argument holds for strongly convex $f$. EDIT: The following link appears to support this claim for the case of $m =1$. https://books.google.ca/books?id=HjznBwAAQBAJ&pg=PA86&lpg=PA86&dq=linear+programming+vanderbei+strongly+convex+legendre+transform&source=bl&ots=mBLOVV7_a0&sig=xMI0kBz7bHdN4kkEnvQQ87LGkhk&hl=en&sa=X&ved=0ahUKEwiayNCfupXVAhVFeSYKHQI_Dl8Q6AEIJjAA#v=onepage&q=linear%20programming%20vanderbei%20strongly%20convex%20legendre%20transform&f=false","Suppose that I have a strongly convex function $f(\mathbf{x}): \mathbb{R}^m \rightarrow \mathbb{R}$. Is the Legendre transform of this function also strongly convex? As far as I can tell, strict convexity of $f$ implies strict convexity of the Legendre transform of $f$ (as shown in https://proofwiki.org/wiki/Convexity_of_Function_implies_Convexity_of_its_Legendre_Transform ). However, I am unsure as to whether the same argument holds for strongly convex $f$. EDIT: The following link appears to support this claim for the case of $m =1$. https://books.google.ca/books?id=HjznBwAAQBAJ&pg=PA86&lpg=PA86&dq=linear+programming+vanderbei+strongly+convex+legendre+transform&source=bl&ots=mBLOVV7_a0&sig=xMI0kBz7bHdN4kkEnvQQ87LGkhk&hl=en&sa=X&ved=0ahUKEwiayNCfupXVAhVFeSYKHQI_Dl8Q6AEIJjAA#v=onepage&q=linear%20programming%20vanderbei%20strongly%20convex%20legendre%20transform&f=false",,"['analysis', 'convex-analysis', 'inner-products', 'transformation']"
79,Do bounded derivatives imply equi-continuity of function sequence?,Do bounded derivatives imply equi-continuity of function sequence?,,"Let $f_n: [0, 1] \rightarrow \mathbb{R}$ $\forall n \in \mathbb{N}$ If $f_n \in C^1([0, 1])$ and $\vert\vert f_n'\vert\vert_\infty \le 3 \Longrightarrow \{f_n\}$ are equicontinuous I know that bounded derivative for $f \in C^1([a, b])$ implies $f$ is Lipschitz, which implies uniform continuity. I'm sure that it isn't true for the $\{fn\}$ sequence, because the previous one is an answer between other three, and it can't be the correct one. But I can't find a counter-example. If $\vert\vert f_n'\vert\vert_\infty \le 3$, $\{fn\}$ should be equi-Lipschitz (Lipschitz $\forall$ $n$), because $\exists$ $L > 0: \vert\vert f_n(x_1) - f_n(x_2) \vert \vert \le L \vert \vert x_1 - x_2 \vert \vert$ because of Lagrange theorem (we can use the ""biggest"" $L$ that is good $\forall$ $n$). But in that case, $\{f_n\}$ are equicontinuous, so I can't understand where I am wrong. Maybe I can't take the ""biggest"" $L$ because I could have infinite $L$s ($n \in \mathbb{N})$? Any help is appreciated, thanks! EDIT: The answer I checked as correct is: $f_n \in C^0([0, 1])$ $\Longrightarrow $ they are equibounded because of Weierstrass theorem. Indeed, $\exists \max f_n, \exists \min f_n \Longrightarrow f_n$ are bounded $\forall$ $n$ EDIT: So, Weierstrass lost, and bounded derivatives imply equicontinuity in my case!","Let $f_n: [0, 1] \rightarrow \mathbb{R}$ $\forall n \in \mathbb{N}$ If $f_n \in C^1([0, 1])$ and $\vert\vert f_n'\vert\vert_\infty \le 3 \Longrightarrow \{f_n\}$ are equicontinuous I know that bounded derivative for $f \in C^1([a, b])$ implies $f$ is Lipschitz, which implies uniform continuity. I'm sure that it isn't true for the $\{fn\}$ sequence, because the previous one is an answer between other three, and it can't be the correct one. But I can't find a counter-example. If $\vert\vert f_n'\vert\vert_\infty \le 3$, $\{fn\}$ should be equi-Lipschitz (Lipschitz $\forall$ $n$), because $\exists$ $L > 0: \vert\vert f_n(x_1) - f_n(x_2) \vert \vert \le L \vert \vert x_1 - x_2 \vert \vert$ because of Lagrange theorem (we can use the ""biggest"" $L$ that is good $\forall$ $n$). But in that case, $\{f_n\}$ are equicontinuous, so I can't understand where I am wrong. Maybe I can't take the ""biggest"" $L$ because I could have infinite $L$s ($n \in \mathbb{N})$? Any help is appreciated, thanks! EDIT: The answer I checked as correct is: $f_n \in C^0([0, 1])$ $\Longrightarrow $ they are equibounded because of Weierstrass theorem. Indeed, $\exists \max f_n, \exists \min f_n \Longrightarrow f_n$ are bounded $\forall$ $n$ EDIT: So, Weierstrass lost, and bounded derivatives imply equicontinuity in my case!",,"['real-analysis', 'analysis']"
80,How can I evaluate $ \int{\sqrt{1-k \cos{x}} dx} $?,How can I evaluate ?, \int{\sqrt{1-k \cos{x}} dx} ,"When $ k = 1 $ the solution is simple to find. But how can I do when $ k \neq 1 $? With the aid of WolframAlpha I realized that this primitive involves the E elliptic integral (i.e. second kind), but how can I reduce my problem to this? Thanks.","When $ k = 1 $ the solution is simple to find. But how can I do when $ k \neq 1 $? With the aid of WolframAlpha I realized that this primitive involves the E elliptic integral (i.e. second kind), but how can I reduce my problem to this? Thanks.",,"['calculus', 'analysis', 'indefinite-integrals']"
81,Pick out true statements about the limit of $f_n(x)=\frac{1}{1+n^2x^2}$,Pick out true statements about the limit of,f_n(x)=\frac{1}{1+n^2x^2},"For the sequence of functions $f_n(x)=\frac{1}{1+n^2x^2}$ for $n \in \mathbb{N}, x \in \mathbb{R}$ which of the following are true? (A) $f_n$ converges point-wise to a continuous function on $[0,1]$ (B) $f_n$ converges uniformly on $[0,1]$ (C) $f_n$ converges uniformly on $[\frac{1}{2},1]$ (D) $\lim\limits_{n \to \infty} \int_0^1 f_n(x)dx=\int_0^1\lim\limits_{n \to \infty} f_n(x) dx$ So here is my take on this and I want to know if I am correct. Me and a friend are having quite a debate on this question and I can't see how he can disagree with the following logic. Obviously A is false because the limit of $f_n$ on $[0,1]$ is $$ F(x) =\begin{cases} 1 & if \ x=0\\ 0 & otherwise \end{cases} $$ Statement B is also false because if $f_n$ were to converge uniformly on $[0,1]$ then its limit would have been a continuous function which is obviously not the case. C is true. This is because the uniform norm $||f_n - 0||$ converges to zero. D is false because $$\lim\limits_{n \to \infty} \int_0^1 f_n(x)dx=\lim\limits_{n \to \infty} \int_0^1 \frac{1}{1+n^2x^2}dx=\frac{\pi}{2}$$ But $$ \int_0^1\lim\limits_{n \to \infty} f_n(x) dx = 0$$","For the sequence of functions $f_n(x)=\frac{1}{1+n^2x^2}$ for $n \in \mathbb{N}, x \in \mathbb{R}$ which of the following are true? (A) $f_n$ converges point-wise to a continuous function on $[0,1]$ (B) $f_n$ converges uniformly on $[0,1]$ (C) $f_n$ converges uniformly on $[\frac{1}{2},1]$ (D) $\lim\limits_{n \to \infty} \int_0^1 f_n(x)dx=\int_0^1\lim\limits_{n \to \infty} f_n(x) dx$ So here is my take on this and I want to know if I am correct. Me and a friend are having quite a debate on this question and I can't see how he can disagree with the following logic. Obviously A is false because the limit of $f_n$ on $[0,1]$ is $$ F(x) =\begin{cases} 1 & if \ x=0\\ 0 & otherwise \end{cases} $$ Statement B is also false because if $f_n$ were to converge uniformly on $[0,1]$ then its limit would have been a continuous function which is obviously not the case. C is true. This is because the uniform norm $||f_n - 0||$ converges to zero. D is false because $$\lim\limits_{n \to \infty} \int_0^1 f_n(x)dx=\lim\limits_{n \to \infty} \int_0^1 \frac{1}{1+n^2x^2}dx=\frac{\pi}{2}$$ But $$ \int_0^1\lim\limits_{n \to \infty} f_n(x) dx = 0$$",,"['real-analysis', 'integration', 'analysis', 'real-numbers']"
82,"Is $e^{-\|x\|^2}$ Lipschitz? If so, what is the Lipschitz norm?","Is  Lipschitz? If so, what is the Lipschitz norm?",e^{-\|x\|^2},"Suppose $x\in \mathbb{R}^n$ and $\|.\|_2$ is the Euclidean norm. Is $e^{-\|x\|^2}$ Lipschitz? If so, what is the Lipschitz norm? Here is my attempt: $\big(e^{-\|x\|^2}-e^{-\|y\|^2}\big) \leq |\|x\|^2-\|y\|^2| = |\|x\|-\|y\||(\|x\|+\|y\|)\leq \|x-y\|(\|x\|+\|y\|)$ Do we need to define it on a compact set to make it Lipschitz?","Suppose $x\in \mathbb{R}^n$ and $\|.\|_2$ is the Euclidean norm. Is $e^{-\|x\|^2}$ Lipschitz? If so, what is the Lipschitz norm? Here is my attempt: $\big(e^{-\|x\|^2}-e^{-\|y\|^2}\big) \leq |\|x\|^2-\|y\|^2| = |\|x\|-\|y\||(\|x\|+\|y\|)\leq \|x-y\|(\|x\|+\|y\|)$ Do we need to define it on a compact set to make it Lipschitz?",,"['calculus', 'analysis']"
83,"Prob. 18, Chap. 5, in Baby Rudin: Another Form of Taylor's Theorem","Prob. 18, Chap. 5, in Baby Rudin: Another Form of Taylor's Theorem",,"Here is Prob. 18, Chap. 5, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is a real function on $[a, b]$ , $n$ is a positive integer, and $f^{(n-1)}$ exists for every $t \in [a, b]$ . Let $\alpha$ , $\beta$ , and $P$ be as in Taylor's theorem (5.15). Define $$ Q(t) = \frac{ f(t) - f(\beta) }{ t- \beta} $$ for $t \in [a, b]$ , $t \neq \beta$ , differentiate $$ f(t) - f(\beta) = (t-\beta) Q(t) $$ $n-1$ times at $t = \alpha$ , and derive the following version of Taylor's theorem: $$ f(\beta) = P(\beta) + \frac{Q^{(n-1)}(\alpha)}{(n-1)!} (\beta - \alpha)^n. $$ And, here is Theorem 5.15 in Baby Rudin, 3rd edition: Suppose $f$ is a real function on $[a, b]$ , $n$ is a positive integer, $f^{(n-1)}$ is continuous on $[a, b]$ , and $f^{(n)}(t)$ exists for every $t \in (a, b)$ . Let $\alpha$ , $\beta$ be distinct points of $[a, b]$ , and define $$ P(t) = \sum_{k=0}^{n-1} \frac{f^{(k)}(\alpha)}{k!} \left( t-\alpha \right)^k.$$ Then there exists a point $x$ between $\alpha$ and $\beta$ such that $$ f(\beta) = P(\beta) + \frac{f^{(n)}(x)}{n!} (\beta - \alpha )^n.$$ An Attempt: For all $t \in [a, b]$ , we have $$ \begin{align} f(t) - f(\beta) &= ( t-\beta) Q(t),  \tag{1} \\ f^\prime(t) &= Q(t) + (t-\beta) Q^\prime(t), \tag{2} \\ f^{\prime\prime}(t) &= 2Q^\prime(t) + (t-\beta) Q^{\prime\prime}(t),  \tag{3} \\ f^{(3)}(t) &= 3 Q^{\prime\prime}(t) + (t-\beta)Q^{(3)}(t), \tag{4} \\ f^{(4)}(t) &= 4 Q^{(3)}(t) + (t-\beta) Q^{(4)}(t), \tag{5} \\  \cdots &= \cdots \\ f^{(n-1)}(t) &= (n-1) Q^{(n-2)}(t) + (t-\beta) Q^{(n-1)}(t). \tag{*}  \end{align} $$ So, for $t = \alpha$ , the above chain of equations yields $$  \begin{align} & \qquad f(\beta) \\ &= f(\alpha) + Q(\alpha)  (\beta - \alpha )  \qquad \mbox{ [ using (1) ] } \\ &= f(\alpha) +  \left[ f^\prime(\alpha) + (\beta - \alpha) Q^\prime(\alpha) \right] (\beta - \alpha ) \\  & \qquad \qquad \mbox{ [ using (2) ] } \\ &= f(\alpha) + f^\prime(\alpha) (\beta - \alpha) +  Q^\prime(\alpha) (\beta - \alpha)^2  \\ &= f(\alpha) +  f^\prime(\alpha) (\beta - \alpha) \\  & \qquad +  \left[  \frac{1}{2} \left( f^{\prime\prime}(\alpha) + (\beta - \alpha) Q^{\prime\prime}(\alpha) \right) \right] (\beta - \alpha)^2 \\  & \qquad \qquad \mbox{ [ using (3) ] } \\ &= f(\alpha) +  \frac{f^\prime(\alpha)}{1!} (\beta - \alpha) + \frac{f^{\prime\prime}(\alpha)}{2!} (\beta - \alpha)^2 + \frac{Q^{\prime\prime}(\alpha)}{2!} (\beta - \alpha)^3 \\  &=  f(\alpha) +  f^\prime(\alpha) (\beta - \alpha) + \frac{f^{\prime\prime}(\alpha)}{2} (\beta - \alpha)^2 + \frac{\frac{1}{3} \left( f^{(3)}(\alpha) + (\beta - \alpha) Q^{(3)}(\alpha) \right) }{2} (\beta - \alpha)^3 \qquad \mbox{ [ using (4) ] } \\  &=  f(\alpha) +  \frac{f^\prime(\alpha)}{1!} (\beta - \alpha) + \frac{f^{\prime\prime}(\alpha)}{2!} (\beta - \alpha)^2 + \frac{f^{(3)}(\alpha)}{3!}(\beta-\alpha)^3 + \frac{Q^{(3)}(\alpha)}{3!} (\beta-\alpha)^4 \\  &=  f(\alpha) +  \frac{f^\prime(\alpha)}{1!} (\beta - \alpha) + \frac{f^{\prime\prime}(\alpha)}{2!} (\beta - \alpha)^2 + \frac{f^{(3)}(\alpha)}{3!}(\beta-\alpha)^3 + \frac{ \frac{1}{4} \left( f^{(4)}(\alpha) + (\beta - \alpha) Q^{(4)}(\alpha) \right)  }{3!}  (\beta-\alpha)^4 \\  & \qquad \qquad \mbox{ [ using (5) ] } \\ &= f(\alpha) +  \frac{f^\prime(\alpha)}{1!} (\beta - \alpha) + \frac{f^{\prime\prime}(\alpha)}{2!} (\beta - \alpha)^2 + \frac{f^{(3)}(\alpha)}{3!}(\beta-\alpha)^3 + \frac{f^{(4)}(\alpha)}{4!} (\beta - \alpha)^4 + \frac{Q^{(4)}(\alpha)}{4!} (\beta-\alpha)^5 \\ &= \cdots \\ &= f(\alpha) +  \frac{f^\prime(\alpha)}{1!} (\beta - \alpha) + \frac{f^{\prime\prime}(\alpha)}{2!} (\beta - \alpha)^2 \\  & \qquad + \frac{f^{(3)}(\alpha)}{3!}(\beta-\alpha)^3 + \frac{f^{(4)}(\alpha)}{4!} (\beta - \alpha)^4 + \cdots + \frac{f^{(n-1)}(\alpha)}{(n-1)!} (\beta-\alpha)^{n-1} + \frac{Q^{(n-1)}(\alpha)}{(n-1)!} (\beta - \alpha)^n \\ &= P(\beta) +  \frac{Q^{(n-1)}(\alpha)}{(n-1)!} (\beta - \alpha)^n,  \end{align} $$ as required. Is this proof correct? If so, then is it rigorous enough for Rudin as well?","Here is Prob. 18, Chap. 5, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose is a real function on , is a positive integer, and exists for every . Let , , and be as in Taylor's theorem (5.15). Define for , , differentiate times at , and derive the following version of Taylor's theorem: And, here is Theorem 5.15 in Baby Rudin, 3rd edition: Suppose is a real function on , is a positive integer, is continuous on , and exists for every . Let , be distinct points of , and define Then there exists a point between and such that An Attempt: For all , we have So, for , the above chain of equations yields as required. Is this proof correct? If so, then is it rigorous enough for Rudin as well?","f [a, b] n f^{(n-1)} t \in [a, b] \alpha \beta P  Q(t) = \frac{ f(t) - f(\beta) }{ t- \beta}  t \in [a, b] t \neq \beta  f(t) - f(\beta) = (t-\beta) Q(t)  n-1 t = \alpha  f(\beta) = P(\beta) + \frac{Q^{(n-1)}(\alpha)}{(n-1)!} (\beta - \alpha)^n.  f [a, b] n f^{(n-1)} [a, b] f^{(n)}(t) t \in (a, b) \alpha \beta [a, b]  P(t) = \sum_{k=0}^{n-1} \frac{f^{(k)}(\alpha)}{k!} \left( t-\alpha \right)^k. x \alpha \beta  f(\beta) = P(\beta) + \frac{f^{(n)}(x)}{n!} (\beta - \alpha )^n. t \in [a, b] 
\begin{align}
f(t) - f(\beta) &= ( t-\beta) Q(t),  \tag{1} \\
f^\prime(t) &= Q(t) + (t-\beta) Q^\prime(t), \tag{2} \\
f^{\prime\prime}(t) &= 2Q^\prime(t) + (t-\beta) Q^{\prime\prime}(t),  \tag{3} \\
f^{(3)}(t) &= 3 Q^{\prime\prime}(t) + (t-\beta)Q^{(3)}(t), \tag{4} \\
f^{(4)}(t) &= 4 Q^{(3)}(t) + (t-\beta) Q^{(4)}(t), \tag{5} \\ 
\cdots &= \cdots \\
f^{(n-1)}(t) &= (n-1) Q^{(n-2)}(t) + (t-\beta) Q^{(n-1)}(t). \tag{*} 
\end{align}
 t = \alpha  
\begin{align}
& \qquad f(\beta) \\
&= f(\alpha) + Q(\alpha)  (\beta - \alpha )  \qquad \mbox{ [ using (1) ] } \\
&= f(\alpha) +  \left[ f^\prime(\alpha) + (\beta - \alpha) Q^\prime(\alpha) \right] (\beta - \alpha ) \\ 
& \qquad \qquad \mbox{ [ using (2) ] } \\
&= f(\alpha) + f^\prime(\alpha) (\beta - \alpha) +  Q^\prime(\alpha) (\beta - \alpha)^2  \\
&= f(\alpha) +  f^\prime(\alpha) (\beta - \alpha) \\ 
& \qquad +  \left[  \frac{1}{2} \left( f^{\prime\prime}(\alpha) + (\beta - \alpha) Q^{\prime\prime}(\alpha) \right) \right] (\beta - \alpha)^2 \\ 
& \qquad \qquad \mbox{ [ using (3) ] } \\
&= f(\alpha) +  \frac{f^\prime(\alpha)}{1!} (\beta - \alpha) + \frac{f^{\prime\prime}(\alpha)}{2!} (\beta - \alpha)^2 + \frac{Q^{\prime\prime}(\alpha)}{2!} (\beta - \alpha)^3 \\ 
&=  f(\alpha) +  f^\prime(\alpha) (\beta - \alpha) + \frac{f^{\prime\prime}(\alpha)}{2} (\beta - \alpha)^2 + \frac{\frac{1}{3} \left( f^{(3)}(\alpha) + (\beta - \alpha) Q^{(3)}(\alpha) \right) }{2} (\beta - \alpha)^3 \qquad \mbox{ [ using (4) ] } \\ 
&=  f(\alpha) +  \frac{f^\prime(\alpha)}{1!} (\beta - \alpha) + \frac{f^{\prime\prime}(\alpha)}{2!} (\beta - \alpha)^2 + \frac{f^{(3)}(\alpha)}{3!}(\beta-\alpha)^3 + \frac{Q^{(3)}(\alpha)}{3!} (\beta-\alpha)^4 \\ 
&=  f(\alpha) +  \frac{f^\prime(\alpha)}{1!} (\beta - \alpha) + \frac{f^{\prime\prime}(\alpha)}{2!} (\beta - \alpha)^2 + \frac{f^{(3)}(\alpha)}{3!}(\beta-\alpha)^3 + \frac{ \frac{1}{4} \left( f^{(4)}(\alpha) + (\beta - \alpha) Q^{(4)}(\alpha) \right)  }{3!}  (\beta-\alpha)^4 \\ 
& \qquad \qquad \mbox{ [ using (5) ] } \\
&= f(\alpha) +  \frac{f^\prime(\alpha)}{1!} (\beta - \alpha) + \frac{f^{\prime\prime}(\alpha)}{2!} (\beta - \alpha)^2 + \frac{f^{(3)}(\alpha)}{3!}(\beta-\alpha)^3 + \frac{f^{(4)}(\alpha)}{4!} (\beta - \alpha)^4 + \frac{Q^{(4)}(\alpha)}{4!} (\beta-\alpha)^5 \\
&= \cdots \\
&= f(\alpha) +  \frac{f^\prime(\alpha)}{1!} (\beta - \alpha) + \frac{f^{\prime\prime}(\alpha)}{2!} (\beta - \alpha)^2 \\ 
& \qquad + \frac{f^{(3)}(\alpha)}{3!}(\beta-\alpha)^3 + \frac{f^{(4)}(\alpha)}{4!} (\beta - \alpha)^4 + \cdots + \frac{f^{(n-1)}(\alpha)}{(n-1)!} (\beta-\alpha)^{n-1} + \frac{Q^{(n-1)}(\alpha)}{(n-1)!} (\beta - \alpha)^n \\
&= P(\beta) +  \frac{Q^{(n-1)}(\alpha)}{(n-1)!} (\beta - \alpha)^n, 
\end{align}
","['calculus', 'real-analysis', 'analysis', 'derivatives', 'taylor-expansion']"
84,An ordinary generating function involving the gamma function,An ordinary generating function involving the gamma function,,"Is there anything known about ordinary generating functions of the form $$ S(z;a) = \sum_{n=1}^\infty \Gamma(ani)z^n, $$ for $z \in \mathbb{C}$ and $a \in \mathbb{R}$, $a \neq 0$. Here $i$ is the imaginary unit. Is there a nice expression for it, or good bounds, in terms of $z,a$? Thanks. Edit 1: An application of the root test and the fact that $|\Gamma(\sigma + it)| \sim \sqrt{2\pi}|t|^{\sigma - 1/2}e^{-\pi |t|/2}$ as $|t| \to \infty$, for $\sigma$ fixed (see for instance Corollary 16 here ) shows that $S(z;a)$ has radius of convergence $e^{\pi|a|/2}$. Edit 2: I have also posted this question here","Is there anything known about ordinary generating functions of the form $$ S(z;a) = \sum_{n=1}^\infty \Gamma(ani)z^n, $$ for $z \in \mathbb{C}$ and $a \in \mathbb{R}$, $a \neq 0$. Here $i$ is the imaginary unit. Is there a nice expression for it, or good bounds, in terms of $z,a$? Thanks. Edit 1: An application of the root test and the fact that $|\Gamma(\sigma + it)| \sim \sqrt{2\pi}|t|^{\sigma - 1/2}e^{-\pi |t|/2}$ as $|t| \to \infty$, for $\sigma$ fixed (see for instance Corollary 16 here ) shows that $S(z;a)$ has radius of convergence $e^{\pi|a|/2}$. Edit 2: I have also posted this question here",,"['analysis', 'generating-functions', 'gamma-function']"
85,Elementary Doubt about Differential Forms and wedge product,Elementary Doubt about Differential Forms and wedge product,,"I'm working through Browder's Mathematical Analysis ( http://www.springer.com/in/book/9780387946146 ) and am having a bit of trouble being convinced by one of his definitions. On Page 288, for Definition 13.5, he says:  Let $\omega_0 = dx^1 \wedge \dots \wedge dx^n$; for each $j, 1\leq j \leq n$, we put  $$\eta^j = (-1)^j dx^1 \wedge dx^{j-1} \wedge dx^{j+1} \dots \wedge dx^n$$ The $(-1)^j$ is justified so that $dx^j \wedge \eta^j = \omega_0$. My doubt is that: Shouldn't it be  $(-1)^{j-1}$ ? I test Browder's definition with the $j =1, 2$ cases to get:  $$dx^1 \wedge (-1)^1 dx^2\wedge \dots \wedge dx^n = -\omega_0 $$ and  $$dx^2 \wedge (-1)^2 dx^1\wedge dx^3 \wedge \dots \wedge dx^n = -\omega_0$$ Each time, I swap positions of $dx^i$s, I multiply by $(-1)$. In the first case, no swapping was required and in the second case, one swap was required. Essentially, what I'm asserting is that in the general case, $j-1$ swaps are required so why are we multiplying the $\eta^j$ expression by $(-1)^j$  instead of $(-1)^{j-1}$?","I'm working through Browder's Mathematical Analysis ( http://www.springer.com/in/book/9780387946146 ) and am having a bit of trouble being convinced by one of his definitions. On Page 288, for Definition 13.5, he says:  Let $\omega_0 = dx^1 \wedge \dots \wedge dx^n$; for each $j, 1\leq j \leq n$, we put  $$\eta^j = (-1)^j dx^1 \wedge dx^{j-1} \wedge dx^{j+1} \dots \wedge dx^n$$ The $(-1)^j$ is justified so that $dx^j \wedge \eta^j = \omega_0$. My doubt is that: Shouldn't it be  $(-1)^{j-1}$ ? I test Browder's definition with the $j =1, 2$ cases to get:  $$dx^1 \wedge (-1)^1 dx^2\wedge \dots \wedge dx^n = -\omega_0 $$ and  $$dx^2 \wedge (-1)^2 dx^1\wedge dx^3 \wedge \dots \wedge dx^n = -\omega_0$$ Each time, I swap positions of $dx^i$s, I multiply by $(-1)$. In the first case, no swapping was required and in the second case, one swap was required. Essentially, what I'm asserting is that in the general case, $j-1$ swaps are required so why are we multiplying the $\eta^j$ expression by $(-1)^j$  instead of $(-1)^{j-1}$?",,"['analysis', 'multivariable-calculus', 'differential-geometry', 'differential-forms']"
86,Theorem 5.13 in Baby Rudin: Is this statement of the L'Hospital's rule the most optimal one?,Theorem 5.13 in Baby Rudin: Is this statement of the L'Hospital's rule the most optimal one?,,"Here is Theorem 5.13 (L'Hospital's Rule) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ and $g$ are real and differentiable in $(a, b)$, and $g^\prime(x) \neq 0$ for all $x \in (a, b)$, where $-\infty \leq a < b \leq +\infty$. Suppose    $$ \frac{f^\prime(x)}{g^\prime(x)} \to A \ \mbox{ as } \ x \to a. \tag{13} $$   If    $$ f(x) \to 0 \ \mbox{ and } \ g(x) \to 0 \ \mbox{ as } \ x \to a, \tag{14} $$   or if    $$ g(x) \to +\infty \ \mbox{ as } \ x \to a, \tag{15} $$   then    $$ \frac{f(x)}{g(x)} \to A \ \mbox{ as } \ x \to a. \tag{16}$$   The analogous statement is of course also true if $x \to b$, or if $g(x) \to -\infty$ in (15). Let us note that we now use the limit concept in the extended sense of Definition 4.33. Here is Definition 4.33: Let $f$ be a real function defined on $E \subset \mathbb{R}$. We say that    $$ f(t) \to A \ \mbox{ as } \ t \to x, $$   where $A$ and $x$ are in the extended real number system, if for every neighborhood $U$ of $A$ there is a neighborhood $V$ of $x$ such that $V \cap E$ is not empty, and such that $f(t) \in U$ for all $t \in V \cap E$, $t \neq x$. And, here is Rudin's proof: We first consider the case in which $-\infty \leq A < +\infty$. Choose a real number $q$ such that $A < q$, and then choose $r$ such that $A < r < q$. By (13) there is a point $c \in (a, b)$ such that $a < x < c$ implies    $$ \frac{ f^\prime(x) }{ g^\prime(x) } < r. \tag{17} $$   If $a < x < y < c$, then Theorem 5.9 shows that there is a point $t \in (x, y)$ such that    $$ \frac{ f(x)-f(y) }{ g(x)-g(y) } = \frac{f^\prime(t)}{g^\prime(t)} < r. \tag{18} $$   Suppose (14) holds. Letting $x \to a$ in (18), we see that    $$ \frac{f(y)}{g(y)} \leq r < q \qquad \qquad \qquad  (a < y < c) \tag{19} $$ Next, suppose (15) holds. Keeping $y$ fixed in (18), we can choos a point $c_1 \in (a, y)$ such that $g(x) > g(y)$ and $g(x) > 0$ if $a < x < c_1$. Multiplying (18) by $\left[ g(x)- g(y) \right]/g(x)$, we obtain   $$ \frac{ f(x) }{ g(x) } < r - r \frac{ g(y) }{g(x)} + \frac{f(y)}{g(x)} \qquad \qquad \qquad (a < x < c_1). \tag{20}$$   If we let $x \to a$ in (20), (15) shows that there is a point $c_2 \in \left( a, c_1 \right)$ such that    $$ \frac{ f(x) }{ g(x) } < q \qquad \qquad \qquad (a < x < c_2 ). \tag{21} $$ Summing up, (19) and (21) show that for any $q$, subject only to the condition $A < q$, there is a point $c_2$ such that $f(x)/g(x) < q$ if $a < x < c_2$. In the same manner, if $-\infty < A \leq +\infty$, and $p$ is chosen so that $p < A$, we can find a point $c_3$ such that    $$ p < \frac{ f(x) }{ g(x) } \qquad \qquad \qquad ( a< x < c_3), \tag{22} $$    and (16) follows from these two statements. Now I have the following queries: In (20), why has the term $f(x)/g(x)$ not been affected as we let $x \to a$, although two of the three terms on the right side have gone to $0$? Don't we need any assumption about $f$ in (15)?","Here is Theorem 5.13 (L'Hospital's Rule) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ and $g$ are real and differentiable in $(a, b)$, and $g^\prime(x) \neq 0$ for all $x \in (a, b)$, where $-\infty \leq a < b \leq +\infty$. Suppose    $$ \frac{f^\prime(x)}{g^\prime(x)} \to A \ \mbox{ as } \ x \to a. \tag{13} $$   If    $$ f(x) \to 0 \ \mbox{ and } \ g(x) \to 0 \ \mbox{ as } \ x \to a, \tag{14} $$   or if    $$ g(x) \to +\infty \ \mbox{ as } \ x \to a, \tag{15} $$   then    $$ \frac{f(x)}{g(x)} \to A \ \mbox{ as } \ x \to a. \tag{16}$$   The analogous statement is of course also true if $x \to b$, or if $g(x) \to -\infty$ in (15). Let us note that we now use the limit concept in the extended sense of Definition 4.33. Here is Definition 4.33: Let $f$ be a real function defined on $E \subset \mathbb{R}$. We say that    $$ f(t) \to A \ \mbox{ as } \ t \to x, $$   where $A$ and $x$ are in the extended real number system, if for every neighborhood $U$ of $A$ there is a neighborhood $V$ of $x$ such that $V \cap E$ is not empty, and such that $f(t) \in U$ for all $t \in V \cap E$, $t \neq x$. And, here is Rudin's proof: We first consider the case in which $-\infty \leq A < +\infty$. Choose a real number $q$ such that $A < q$, and then choose $r$ such that $A < r < q$. By (13) there is a point $c \in (a, b)$ such that $a < x < c$ implies    $$ \frac{ f^\prime(x) }{ g^\prime(x) } < r. \tag{17} $$   If $a < x < y < c$, then Theorem 5.9 shows that there is a point $t \in (x, y)$ such that    $$ \frac{ f(x)-f(y) }{ g(x)-g(y) } = \frac{f^\prime(t)}{g^\prime(t)} < r. \tag{18} $$   Suppose (14) holds. Letting $x \to a$ in (18), we see that    $$ \frac{f(y)}{g(y)} \leq r < q \qquad \qquad \qquad  (a < y < c) \tag{19} $$ Next, suppose (15) holds. Keeping $y$ fixed in (18), we can choos a point $c_1 \in (a, y)$ such that $g(x) > g(y)$ and $g(x) > 0$ if $a < x < c_1$. Multiplying (18) by $\left[ g(x)- g(y) \right]/g(x)$, we obtain   $$ \frac{ f(x) }{ g(x) } < r - r \frac{ g(y) }{g(x)} + \frac{f(y)}{g(x)} \qquad \qquad \qquad (a < x < c_1). \tag{20}$$   If we let $x \to a$ in (20), (15) shows that there is a point $c_2 \in \left( a, c_1 \right)$ such that    $$ \frac{ f(x) }{ g(x) } < q \qquad \qquad \qquad (a < x < c_2 ). \tag{21} $$ Summing up, (19) and (21) show that for any $q$, subject only to the condition $A < q$, there is a point $c_2$ such that $f(x)/g(x) < q$ if $a < x < c_2$. In the same manner, if $-\infty < A \leq +\infty$, and $p$ is chosen so that $p < A$, we can find a point $c_3$ such that    $$ p < \frac{ f(x) }{ g(x) } \qquad \qquad \qquad ( a< x < c_3), \tag{22} $$    and (16) follows from these two statements. Now I have the following queries: In (20), why has the term $f(x)/g(x)$ not been affected as we let $x \to a$, although two of the three terms on the right side have gone to $0$? Don't we need any assumption about $f$ in (15)?",,"['calculus', 'real-analysis', 'analysis', 'derivatives']"
87,"Prove that $\int_{0}^{1}\sin{(\pi x)}x^x(1-x)^{1-x}\,dx =\frac{\pi e}{24} $",Prove that,"\int_{0}^{1}\sin{(\pi x)}x^x(1-x)^{1-x}\,dx =\frac{\pi e}{24} ","I've found here the following integral. $$I = \int_{0}^{1}\sin{(\pi (1-x))}x^x(1-x)^{1-x}\,dx=\int_{0}^{1}\sin{(\pi x)}x^x(1-x)^{1-x}\,dx=\frac{\pi e}{24}$$ I've never seen it before and I also didn't find the evaluation on math.se. How could we verify it? If it is a well-known integral, then could you give a reference?","I've found here the following integral. $$I = \int_{0}^{1}\sin{(\pi (1-x))}x^x(1-x)^{1-x}\,dx=\int_{0}^{1}\sin{(\pi x)}x^x(1-x)^{1-x}\,dx=\frac{\pi e}{24}$$ I've never seen it before and I also didn't find the evaluation on math.se. How could we verify it? If it is a well-known integral, then could you give a reference?",,"['calculus', 'integration', 'definite-integrals', 'closed-form']"
88,"Show that if $L^p$ is not a subset of $L^q$ for $q \gt p$ then $L^p$ contains indicators of sets of arbitrarily small, positive measure","Show that if  is not a subset of  for  then  contains indicators of sets of arbitrarily small, positive measure",L^p L^q q \gt p L^p,"Show that if $L^p$ is not a subset of  $L^q$ for $q \gt p$ then $L^p$ contains indicator functions of sets of arbitrarily small, yet positive measure. Since $L^p$ is not a subset of $L^q$, there exists $f \in L^p$ such that $f \not \in L^q$. For each $n_o \in \mathbb{N}$, Let $E_{n_0}=\{x \in X: |f(x)| \gt n_0\}$. Then we have $||f||_p^p=\int |f|^p d\mu \ge n_0^p \mu(E_{n_o})$ which implies that $\mu(E_{n_0}) \le \dfrac{||f||_p^p}{n_o^p}$. Now I claim that $\mu(E_{n_o}) \gt 0$ which will establish the claim. Suppose that $\mu(E_{n_o})=0$.  Now Let $F_1=\{x \in X : 0 \lt |f(x)| \le 1\}, F_2=\{x \in X : 1 \lt |f(x)| \le n_0\}$. Then $E_{n_0}^c=F_1 \cup F_2$. Now $\int_{F_1} |f| ^q \le \int_{F_1}|f|^p \lt \infty$ as $p \lt q$. Thus $\int |f|^q =\int_{E_{n_0}^c}|f|^q=\int_{F_1}|f|^q  +\int_{F_2}|f|^q= +\infty$ which implies that $\int_{F_2}|f|^q=\infty$ and therefore $\mu(F_2) =\infty$. On the other hand $\int |f|^p \ge \int_{F_2}|f|^p \ge \mu(F_2)=\infty $ which is a contradiction. Hence, the claim holds and we are done. Is this alright?","Show that if $L^p$ is not a subset of  $L^q$ for $q \gt p$ then $L^p$ contains indicator functions of sets of arbitrarily small, yet positive measure. Since $L^p$ is not a subset of $L^q$, there exists $f \in L^p$ such that $f \not \in L^q$. For each $n_o \in \mathbb{N}$, Let $E_{n_0}=\{x \in X: |f(x)| \gt n_0\}$. Then we have $||f||_p^p=\int |f|^p d\mu \ge n_0^p \mu(E_{n_o})$ which implies that $\mu(E_{n_0}) \le \dfrac{||f||_p^p}{n_o^p}$. Now I claim that $\mu(E_{n_o}) \gt 0$ which will establish the claim. Suppose that $\mu(E_{n_o})=0$.  Now Let $F_1=\{x \in X : 0 \lt |f(x)| \le 1\}, F_2=\{x \in X : 1 \lt |f(x)| \le n_0\}$. Then $E_{n_0}^c=F_1 \cup F_2$. Now $\int_{F_1} |f| ^q \le \int_{F_1}|f|^p \lt \infty$ as $p \lt q$. Thus $\int |f|^q =\int_{E_{n_0}^c}|f|^q=\int_{F_1}|f|^q  +\int_{F_2}|f|^q= +\infty$ which implies that $\int_{F_2}|f|^q=\infty$ and therefore $\mu(F_2) =\infty$. On the other hand $\int |f|^p \ge \int_{F_2}|f|^p \ge \mu(F_2)=\infty $ which is a contradiction. Hence, the claim holds and we are done. Is this alright?",,"['real-analysis', 'analysis', 'measure-theory', 'lp-spaces']"
89,Proving convergence of the Dirichlet Eta function,Proving convergence of the Dirichlet Eta function,,"I've been struggling to prove this for a project as I'm not an expert in the field, so i decided to go back to basics. The Dirichlet Eta Function is defined by: $$ \eta (s) = \sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{n^{s}},  \text{  where }  s= \sigma +it $$ The idea i have had is to use the fact that $n^{s} = n^{\sigma}n^{it} = n^{\sigma}(\cos(\ln(n)t)+i\sin(\ln(n)t))$ So we can re-write the sum as: $$\sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{n^{s}}  = \sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{n^{\sigma}(\cos(\ln(n)t)+i\sin(\ln(n)t))} = \sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n^{\sigma}} (\cos(\ln(n)t)-i\sin(\ln(n)t)) $$ So what we end up turning our original problem into two different real series: $$ \eta (s) = \sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{n^{s}} = \sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n^{\sigma}}\cos(\ln(n)t) - i\sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n^{\sigma}}\sin(\ln(n)t)) $$ So now my idea is to prove convergence for these two individual real series, which in turn will give me convergence for the complex series. Is there anything wrong with this approach?","I've been struggling to prove this for a project as I'm not an expert in the field, so i decided to go back to basics. The Dirichlet Eta Function is defined by: $$ \eta (s) = \sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{n^{s}},  \text{  where }  s= \sigma +it $$ The idea i have had is to use the fact that $n^{s} = n^{\sigma}n^{it} = n^{\sigma}(\cos(\ln(n)t)+i\sin(\ln(n)t))$ So we can re-write the sum as: $$\sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{n^{s}}  = \sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{n^{\sigma}(\cos(\ln(n)t)+i\sin(\ln(n)t))} = \sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n^{\sigma}} (\cos(\ln(n)t)-i\sin(\ln(n)t)) $$ So what we end up turning our original problem into two different real series: $$ \eta (s) = \sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{n^{s}} = \sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n^{\sigma}}\cos(\ln(n)t) - i\sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n^{\sigma}}\sin(\ln(n)t)) $$ So now my idea is to prove convergence for these two individual real series, which in turn will give me convergence for the complex series. Is there anything wrong with this approach?",,"['analysis', 'number-theory', 'convergence-divergence', 'riemann-zeta', 'dirichlet-series']"
90,Reference to $C^{1/\epsilon}$-topology in a paper,Reference to -topology in a paper,C^{1/\epsilon},"I am reading a paper in which a curve is referred to as being '$\epsilon$-close to a horizontal line in the $C^{1/\epsilon}$-topology'. There is no explanation of what this means, and as an undergraduate student I have not come across this topology (or even know if it's standard notation). The concept of a Whitney topology has similar notation but seems only defined for integers and I'm not sure whether this notation is just a coincidence. Has anyone come across this and could please explain it? A google search has returned nothing. Thanks! Edit: didn't include the paper as the above is mentioned as a comment rather than part of the main text. But here it is for those interested, remark 4.2 on page 9.","I am reading a paper in which a curve is referred to as being '$\epsilon$-close to a horizontal line in the $C^{1/\epsilon}$-topology'. There is no explanation of what this means, and as an undergraduate student I have not come across this topology (or even know if it's standard notation). The concept of a Whitney topology has similar notation but seems only defined for integers and I'm not sure whether this notation is just a coincidence. Has anyone come across this and could please explain it? A google search has returned nothing. Thanks! Edit: didn't include the paper as the above is mentioned as a comment rather than part of the main text. But here it is for those interested, remark 4.2 on page 9.",,"['real-analysis', 'general-topology', 'analysis', 'differential-topology']"
91,Is $\mathbb R^n$ the product of anything not homeomorphic to $\mathbb R^p$?,Is  the product of anything not homeomorphic to ?,\mathbb R^n \mathbb R^p,"One has $\mathbb R^n=\mathbb R^p\times\mathbb R^q$ whenever $p+q=n$. My question is whether or not one can choose different factors in the product. Do there exist topological spaces $A,B$ so that $A\times B\cong \mathbb R^n$ for some $n$ and $A\not\cong \mathbb R^p$ for any $p$? Intuitively: no way. But I have no idea how I would tackle such a problem, I can't even tell if it is simple or incredibly difficult.","One has $\mathbb R^n=\mathbb R^p\times\mathbb R^q$ whenever $p+q=n$. My question is whether or not one can choose different factors in the product. Do there exist topological spaces $A,B$ so that $A\times B\cong \mathbb R^n$ for some $n$ and $A\not\cong \mathbb R^p$ for any $p$? Intuitively: no way. But I have no idea how I would tackle such a problem, I can't even tell if it is simple or incredibly difficult.",,"['general-topology', 'analysis']"
92,Prove the following statement about a function that is continuous,Prove the following statement about a function that is continuous,,"I was given this problem the following problem in my exam: A function continuous on $[a,b]$ attains a minimum value on $[a,b]$ . Note: proof should not involve compact sets or sequences . Help me with this proof please. Thank you.",I was given this problem the following problem in my exam: A function continuous on attains a minimum value on . Note: proof should not involve compact sets or sequences . Help me with this proof please. Thank you.,"[a,b] [a,b]","['analysis', 'continuity']"
93,Recursive sequence,Recursive sequence,,"Let $\{c_k\}_{k=0}^{\infty}$ a sequence of positive numbers such that there exists $K>0$ with $c_m=0$ for all $m\geq K$ and $$c_k \leq c_{k-1}+2a^{k}c_{2k} \mbox{ for all } k\geq 1$$ with $\frac{1}{2}<a<1$ a constant. Then $c_k \leq Ac_0$ for all $k\geq 0$ for some fixed constant $A>0$. I tried the substitution $d_k=\frac{c_k}{k+1}$, to have $d_k < d_{k-1}\frac{k}{k+1} + d_{2k}\frac{1}{k+1}$ if $4a^{k} < \frac{1}{k+1}$, but this happens for large $k$. I don't know what to do for small $k$. I would appreciate any help.","Let $\{c_k\}_{k=0}^{\infty}$ a sequence of positive numbers such that there exists $K>0$ with $c_m=0$ for all $m\geq K$ and $$c_k \leq c_{k-1}+2a^{k}c_{2k} \mbox{ for all } k\geq 1$$ with $\frac{1}{2}<a<1$ a constant. Then $c_k \leq Ac_0$ for all $k\geq 0$ for some fixed constant $A>0$. I tried the substitution $d_k=\frac{c_k}{k+1}$, to have $d_k < d_{k-1}\frac{k}{k+1} + d_{2k}\frac{1}{k+1}$ if $4a^{k} < \frac{1}{k+1}$, but this happens for large $k$. I don't know what to do for small $k$. I would appreciate any help.",,"['real-analysis', 'sequences-and-series', 'analysis']"
94,Double integral of Periodic function,Double integral of Periodic function,,"Let $f$ be periodic with period 1: $f(t+1)=f(t)$. Is it correct to claim that: $$\iint\limits_{[-1,1]\times[0,1]} |f(x+t)-f(-x+t)|\,dt\,dx=2\iint\limits_{[0,1]^2} |f(x+t)-f(-x+t)|\,dt\,dx$$? My reasoning is that $\iint\limits_{[-1,1]\times[0,1]} |f(x+t)-f(-x+t)|\,dt\,dx\\=\iint\limits_{[-1,0]\times[0,1]} |f(x+t)-f(-x+t)|\,dt\,dx+\iint\limits_{[0,1]\times[0,1]} |f(x+t)-f(-x+t)|\,dt\,dx$ Thanks for any help. (this is an intermediate step in something I wish to prove)","Let $f$ be periodic with period 1: $f(t+1)=f(t)$. Is it correct to claim that: $$\iint\limits_{[-1,1]\times[0,1]} |f(x+t)-f(-x+t)|\,dt\,dx=2\iint\limits_{[0,1]^2} |f(x+t)-f(-x+t)|\,dt\,dx$$? My reasoning is that $\iint\limits_{[-1,1]\times[0,1]} |f(x+t)-f(-x+t)|\,dt\,dx\\=\iint\limits_{[-1,0]\times[0,1]} |f(x+t)-f(-x+t)|\,dt\,dx+\iint\limits_{[0,1]\times[0,1]} |f(x+t)-f(-x+t)|\,dt\,dx$ Thanks for any help. (this is an intermediate step in something I wish to prove)",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus']"
95,Equivalent definitions of absolutely continuous functions,Equivalent definitions of absolutely continuous functions,,"A function $f:[a,b]\rightarrow \mathbb{C}$ is said to be absolutely continuous if for each $\epsilon>0$, there exists $\delta>0$ such that for every mutually disjoint finite sequence of closed sub-intervals $\{[a_1,b_1],...,[a_n,b_n]\}$ of $[a,b]$ satisfying $\sum_{i=1}^n |b_i-a_i|<\delta$, $\sum_{i=1}^n |f(b_i)-f(a_i)|<\epsilon$ holds. Let $f:[a,b]\rightarrow \mathbb{C}$ be an absolutely continuous function and $\epsilon>0$. Then, how do I prove that there exists $\delta>0$ such that for every mutually disjoint finite sequence of open sub-intervals $\{(a_1,b_1),...,(a_n,b_n)\}$ of $[a,b]$ satisfying $\sum_{i=1}^n |b_i-a_i|<\delta$, $\sum_{i=1}^n |f(b_i)-f(a_i)|<\epsilon$ holds ?","A function $f:[a,b]\rightarrow \mathbb{C}$ is said to be absolutely continuous if for each $\epsilon>0$, there exists $\delta>0$ such that for every mutually disjoint finite sequence of closed sub-intervals $\{[a_1,b_1],...,[a_n,b_n]\}$ of $[a,b]$ satisfying $\sum_{i=1}^n |b_i-a_i|<\delta$, $\sum_{i=1}^n |f(b_i)-f(a_i)|<\epsilon$ holds. Let $f:[a,b]\rightarrow \mathbb{C}$ be an absolutely continuous function and $\epsilon>0$. Then, how do I prove that there exists $\delta>0$ such that for every mutually disjoint finite sequence of open sub-intervals $\{(a_1,b_1),...,(a_n,b_n)\}$ of $[a,b]$ satisfying $\sum_{i=1}^n |b_i-a_i|<\delta$, $\sum_{i=1}^n |f(b_i)-f(a_i)|<\epsilon$ holds ?",,"['real-analysis', 'analysis', 'continuity']"
96,how prove $\sum_{n=1}^\infty\frac{a_n}{b_n+a_n} $is convergent?,how prove is convergent?,\sum_{n=1}^\infty\frac{a_n}{b_n+a_n} ,"Let$a_n,b_n\in\mathbb R$ and $(a_n+b_n)b_n\neq 0\quad \forall n\in \mathbb{N}$. The series $\sum_{n=1}^\infty\frac{a_n}{b_n} $ and $\sum_{n=1}^\infty(\frac{a_n}{b_n})^2 $ are convergent. How to prove that$$\sum_{n=1}^\infty\frac{a_n}{b_n+a_n} $$ is convergent. Thanks in advance","Let$a_n,b_n\in\mathbb R$ and $(a_n+b_n)b_n\neq 0\quad \forall n\in \mathbb{N}$. The series $\sum_{n=1}^\infty\frac{a_n}{b_n} $ and $\sum_{n=1}^\infty(\frac{a_n}{b_n})^2 $ are convergent. How to prove that$$\sum_{n=1}^\infty\frac{a_n}{b_n+a_n} $$ is convergent. Thanks in advance",,"['sequences-and-series', 'analysis', 'contest-math']"
97,Prove that if $g$ is integrable $f$ is integrable,Prove that if  is integrable  is integrable,g f,"Let $\|x\|= \biggl( \sum_{k=1}^n |x_k|^p \biggr)^{\!1/p\;}$ for a given $p>1$ and $f:\mathbb{R}^n\to [0,\infty)$ of the form $f(x)=g(\|x\|)$ for a given $g:[0,\infty)\to[0,\infty)$ Prove that if $g$ is integrable then $f$ is integrable and $\int_{\mathbb{R}^n} f =nV_1\int_0^\infty g(r)r^{n-1} \, dr$ where $V_a$ is the volume of $B_a=\{x;\|x\|\leq a\}$ . My work: So I started by taking $g$ to be $\mathbb{1}_{[0,a]}$ for some $a>0$ . In this case $f=\mathbb{1}_{B_a}$ and if $g$ is integrable then $f$ is too, then I took $g$ to be a step function which is a linear combination of indicator functions so again if $g$ is integrable so is $f$ . Now because every integrable function is sandwiched between two step functions with integrals that differ by $\epsilon$ we conclude that if $g$ is integrable $f$ is as well. (Not too sure about this part) Now to prove the integral equality above, I checked it on $n=2$ and $p=2$ and it was true but I am pretty lost on how to do an induction here..Any ideas?","Let for a given and of the form for a given Prove that if is integrable then is integrable and where is the volume of . My work: So I started by taking to be for some . In this case and if is integrable then is too, then I took to be a step function which is a linear combination of indicator functions so again if is integrable so is . Now because every integrable function is sandwiched between two step functions with integrals that differ by we conclude that if is integrable is as well. (Not too sure about this part) Now to prove the integral equality above, I checked it on and and it was true but I am pretty lost on how to do an induction here..Any ideas?","\|x\|= \biggl( \sum_{k=1}^n |x_k|^p \biggr)^{\!1/p\;} p>1 f:\mathbb{R}^n\to [0,\infty) f(x)=g(\|x\|) g:[0,\infty)\to[0,\infty) g f \int_{\mathbb{R}^n} f =nV_1\int_0^\infty g(r)r^{n-1} \, dr V_a B_a=\{x;\|x\|\leq a\} g \mathbb{1}_{[0,a]} a>0 f=\mathbb{1}_{B_a} g f g g f \epsilon g f n=2 p=2","['integration', 'analysis', 'multivariable-calculus', 'improper-integrals']"
98,Equality case in the Prékopa-Leindler inequality.,Equality case in the Prékopa-Leindler inequality.,,"In the paper 'Remarks on the conjectured log-Brunn-Minkowski inequality' by C. Saraoglou, the author uses the result (Lemma A. 3.) about the equality case in the Prékopa-Leindler inequality. For the proof the reader is sent to a paper written by S. Dubuc. The paper itself is in French, which I don't know, and the case is more general. QUESTION: Does anybody know a proof of the equality case of Prékopa-Leindler inequality? Or can say where can I find something (preferably in English). Thank you.","In the paper 'Remarks on the conjectured log-Brunn-Minkowski inequality' by C. Saraoglou, the author uses the result (Lemma A. 3.) about the equality case in the Prékopa-Leindler inequality. For the proof the reader is sent to a paper written by S. Dubuc. The paper itself is in French, which I don't know, and the case is more general. QUESTION: Does anybody know a proof of the equality case of Prékopa-Leindler inequality? Or can say where can I find something (preferably in English). Thank you.",,"['analysis', 'convex-analysis']"
99,Proving there is a unique binary operation we call multiplication,Proving there is a unique binary operation we call multiplication,,"The following is a theorem from the book The Real Numbers and Real Analysis by Bloch which I am currently self-studying. I am pretty sure my proof for uniqueness is correct, but I am wondering is there any other way to prove uniqueness for this theorem. The following is the axiom used: Axiom : There exists a set $\mathbb{N}$ with an element $1 \in\mathbb{N}$ and a function $s: \mathbb{N}\rightarrow\mathbb{N}$ that satisfy the following three properties. There is no n $\in\mathbb{N}$ such that $s(n)=1$. The function $s$ is injective. Let $G\subseteq\mathbb{N}$ be a set. Suppose $1 \in G$, and that if g $\in G$ then $s(g)\in G$. Then $G=\mathbb{N}$. The following is the theorem I want to prove uniqueness for. Theorem : There is a unique binary operation $\circ:\mathbb{N}\times\mathbb{N}\rightarrow\mathbb{N}$ that satisfies the following two properties for all $n,m\in\mathbb{N}$. $n\circ 1=n$ $n\circ s(m)=n\circ m +n$ Proof : Suppose there are two binary operations, $\circ$ and $\bullet$, that follow the two properties. Let $G=\{x\in\mathbb{N}:\forall n\in\mathbb{N} (n\circ x=n\bullet x)\}$. It is easy to see $G\subseteq\mathbb{N}$. When $x=1$, $n\circ 1=n=n\bullet 1$ by part 1 of the theorem, so $1\in G$. Suppose $p\in G$, then $n\circ s(p)=n\circ p +n=n\bullet p +n=n\bullet s(p)$ using the 2nd part of the theorem and the induction hypothesis that $n\circ p=n\bullet p$. Since $n\circ s(p)=n\bullet s(p)$, $s(p)\in\mathbb{N}$ Thus, by part 3 of the axiom $G=\mathbb{N}$ which means the two binary operations are the same. I saw here that uniqueness for a different theorem was proven using the same process (assuming the object is not unique) but also with a  different, less redundant process (using inverses). I am wondering if there is a known second way to prove uniqueness for the theorem in this question.","The following is a theorem from the book The Real Numbers and Real Analysis by Bloch which I am currently self-studying. I am pretty sure my proof for uniqueness is correct, but I am wondering is there any other way to prove uniqueness for this theorem. The following is the axiom used: Axiom : There exists a set $\mathbb{N}$ with an element $1 \in\mathbb{N}$ and a function $s: \mathbb{N}\rightarrow\mathbb{N}$ that satisfy the following three properties. There is no n $\in\mathbb{N}$ such that $s(n)=1$. The function $s$ is injective. Let $G\subseteq\mathbb{N}$ be a set. Suppose $1 \in G$, and that if g $\in G$ then $s(g)\in G$. Then $G=\mathbb{N}$. The following is the theorem I want to prove uniqueness for. Theorem : There is a unique binary operation $\circ:\mathbb{N}\times\mathbb{N}\rightarrow\mathbb{N}$ that satisfies the following two properties for all $n,m\in\mathbb{N}$. $n\circ 1=n$ $n\circ s(m)=n\circ m +n$ Proof : Suppose there are two binary operations, $\circ$ and $\bullet$, that follow the two properties. Let $G=\{x\in\mathbb{N}:\forall n\in\mathbb{N} (n\circ x=n\bullet x)\}$. It is easy to see $G\subseteq\mathbb{N}$. When $x=1$, $n\circ 1=n=n\bullet 1$ by part 1 of the theorem, so $1\in G$. Suppose $p\in G$, then $n\circ s(p)=n\circ p +n=n\bullet p +n=n\bullet s(p)$ using the 2nd part of the theorem and the induction hypothesis that $n\circ p=n\bullet p$. Since $n\circ s(p)=n\bullet s(p)$, $s(p)\in\mathbb{N}$ Thus, by part 3 of the axiom $G=\mathbb{N}$ which means the two binary operations are the same. I saw here that uniqueness for a different theorem was proven using the same process (assuming the object is not unique) but also with a  different, less redundant process (using inverses). I am wondering if there is a known second way to prove uniqueness for the theorem in this question.",,"['analysis', 'elementary-number-theory', 'peano-axioms']"
