,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Matrix of Ones with Diagonal of Integers,Matrix of Ones with Diagonal of Integers,,"My teacher posed a question to the class today asking us to find the determinant of the following matrix... \begin{bmatrix}     2 & 1 & 1 & 1 & 1 \\     1 & 3 & 1 & 1 & 1  \\     1 & 1 & 4 & 1 & 1 \\     1& 1 & 1 & 5 & 1 \\     1&1&1&1&6 \end{bmatrix} using a simple trick that doesn't involve transforming it into reduced row echelon form.  For the life of me I've been unable to figure it out.  Does anyone know the trick, or even which steps I should take to make my teachers supposed method more apparent?","My teacher posed a question to the class today asking us to find the determinant of the following matrix... \begin{bmatrix}     2 & 1 & 1 & 1 & 1 \\     1 & 3 & 1 & 1 & 1  \\     1 & 1 & 4 & 1 & 1 \\     1& 1 & 1 & 5 & 1 \\     1&1&1&1&6 \end{bmatrix} using a simple trick that doesn't involve transforming it into reduced row echelon form.  For the life of me I've been unable to figure it out.  Does anyone know the trick, or even which steps I should take to make my teachers supposed method more apparent?",,"['linear-algebra', 'matrices', 'determinant']"
1,Understanding the proof of Jordan-Hölder Theorem.,Understanding the proof of Jordan-Hölder Theorem.,,"I need some help to understand the proof of this theorem which can be found in the book Introduction to Representation Theory by Pavel Etingof, Oleg Golberg, Sebastian Hensel, Tiankai Liu, Alex Schwendner, Dmitry Vaintrob, and Elena Yudovina. Let $V$ be a finite dimensional representation of $A$, and $0=V_0\subset V_1 \subset  ...\subset V_n=V$, $0=V_0'\subset V_1' \subset  ...\subset V_m'=V$ be filtrations of $V$, such that the representations $W_i:=V_i/V_{i-1}$ and $W_i':=V_i'/V_{i-1}'$ are irreducible for all i. Then $n=m $, and there exists a permutation $\sigma$ of 1, ..., n such that $W_{\sigma (i)}$ is isomorphic to $W_i'$. First proof: (for k of characteristic zero). The character of V obviously equals the sum of characters of $W_i$, and also the sum of characters of $W_i'$ But by Theorem 2.17, the characters of irreducible representations are linearly independent, so the multiplicity of every irreducible representation $W$ of $A$ among $W_i$ and among $W_i'$ are the same. This implies the theorem. Second proof: (general) The proof is by induction on $\text{dim}V$ . The base of induction is clear, so let us prove the induction step. If $W_1=W_1'$ (as subspaces), we are done, since by the induction assumption the theorem holds for $V/W_1$. So asume $W_1\neq W_1'$. In this case $W_1 \cap W_1' = 0$ (as $W_1, W_1'$ are irreducible), so we have an embedding $f: W_1 \oplus W_1' \rightarrow V$. Let $U=V/(W_1 \oplus W_1')$ and $0=U_0 \subset U_1 \subset ... \subset U_p=U$ be a filtration of $U$ with simple quotients $Z_i=U_1/U_{i-1}$ (it exists by Lemma 2.8 in the book). Then we see that: 1) $W/W_1$ has a filtration with successive quotients $W_1', Z_1, ...,Z_p$ and another filtration with successive quotients $W_2. ..., W_n$. 2) $W/W_1'$ has a filtration with successive quotients $W_1, Z_1, ...,Z_p$ and another filtration with successive quotients $W_2'. ..., W_n'$. By the induction assumption, this means that the collection of irreducible representations with multiplicities $W_1. W_1', Z_1, ..., Z_p$ coincides on one hand with $W_1, ..., W_n$, and on the other hand, with $W_1', ..., W_n'$. We are done. I'm trying to complete some details and understand the proof. With respect to the proof 1. In this proof they claim that the character of $V$ is equals to the sum of the characters of $W_i$, and also the characters of $W_i'$. This means that each $W_i$ is not isomorphic to $W_j$ with $i \neq j $ as representation of $V$. The same happen with the $W_i'$ representations. Now, in the book we have the following definition: Let $A$ be an algebra and $V$ a finite-dimensional representation of $A$ with action $\rho$. Then the character of $V$ is the linear function $\chi_{V}: A \rightarrow k$ given by $$\chi_{V}(a)=tr|_{V}(\rho(a))$$ On this point I understand that the characters of irreducible representations are linearly independent, but I have a problem with the conclusion. Why this is possible? There is a note that says the following This proof does not work in characteristic $p$ because it only implies that the multiplicities of $W_i$ and $W_i'$ are the same modulo $p$, which is not sufficient. In fact, the character of the representation $pV$, where $V$ is any representation, is zero. With respect to the second proof. The base induction. If $\text{dim}V=1$, then we have $V$ has the filtration $0=V_0 \subset V_1=V$ (the reason is $V=\langle v\rangle$ where $v \in V$) then obviously any representation has the same length because any element $v \in V$ generates $V$ and $V/0$ is the same $V$. Right? Finally, I want to understand how works the embedding $f$ in the case when $W_1\neq W_1'$ Thanks.","I need some help to understand the proof of this theorem which can be found in the book Introduction to Representation Theory by Pavel Etingof, Oleg Golberg, Sebastian Hensel, Tiankai Liu, Alex Schwendner, Dmitry Vaintrob, and Elena Yudovina. Let $V$ be a finite dimensional representation of $A$, and $0=V_0\subset V_1 \subset  ...\subset V_n=V$, $0=V_0'\subset V_1' \subset  ...\subset V_m'=V$ be filtrations of $V$, such that the representations $W_i:=V_i/V_{i-1}$ and $W_i':=V_i'/V_{i-1}'$ are irreducible for all i. Then $n=m $, and there exists a permutation $\sigma$ of 1, ..., n such that $W_{\sigma (i)}$ is isomorphic to $W_i'$. First proof: (for k of characteristic zero). The character of V obviously equals the sum of characters of $W_i$, and also the sum of characters of $W_i'$ But by Theorem 2.17, the characters of irreducible representations are linearly independent, so the multiplicity of every irreducible representation $W$ of $A$ among $W_i$ and among $W_i'$ are the same. This implies the theorem. Second proof: (general) The proof is by induction on $\text{dim}V$ . The base of induction is clear, so let us prove the induction step. If $W_1=W_1'$ (as subspaces), we are done, since by the induction assumption the theorem holds for $V/W_1$. So asume $W_1\neq W_1'$. In this case $W_1 \cap W_1' = 0$ (as $W_1, W_1'$ are irreducible), so we have an embedding $f: W_1 \oplus W_1' \rightarrow V$. Let $U=V/(W_1 \oplus W_1')$ and $0=U_0 \subset U_1 \subset ... \subset U_p=U$ be a filtration of $U$ with simple quotients $Z_i=U_1/U_{i-1}$ (it exists by Lemma 2.8 in the book). Then we see that: 1) $W/W_1$ has a filtration with successive quotients $W_1', Z_1, ...,Z_p$ and another filtration with successive quotients $W_2. ..., W_n$. 2) $W/W_1'$ has a filtration with successive quotients $W_1, Z_1, ...,Z_p$ and another filtration with successive quotients $W_2'. ..., W_n'$. By the induction assumption, this means that the collection of irreducible representations with multiplicities $W_1. W_1', Z_1, ..., Z_p$ coincides on one hand with $W_1, ..., W_n$, and on the other hand, with $W_1', ..., W_n'$. We are done. I'm trying to complete some details and understand the proof. With respect to the proof 1. In this proof they claim that the character of $V$ is equals to the sum of the characters of $W_i$, and also the characters of $W_i'$. This means that each $W_i$ is not isomorphic to $W_j$ with $i \neq j $ as representation of $V$. The same happen with the $W_i'$ representations. Now, in the book we have the following definition: Let $A$ be an algebra and $V$ a finite-dimensional representation of $A$ with action $\rho$. Then the character of $V$ is the linear function $\chi_{V}: A \rightarrow k$ given by $$\chi_{V}(a)=tr|_{V}(\rho(a))$$ On this point I understand that the characters of irreducible representations are linearly independent, but I have a problem with the conclusion. Why this is possible? There is a note that says the following This proof does not work in characteristic $p$ because it only implies that the multiplicities of $W_i$ and $W_i'$ are the same modulo $p$, which is not sufficient. In fact, the character of the representation $pV$, where $V$ is any representation, is zero. With respect to the second proof. The base induction. If $\text{dim}V=1$, then we have $V$ has the filtration $0=V_0 \subset V_1=V$ (the reason is $V=\langle v\rangle$ where $v \in V$) then obviously any representation has the same length because any element $v \in V$ generates $V$ and $V/0$ is the same $V$. Right? Finally, I want to understand how works the embedding $f$ in the case when $W_1\neq W_1'$ Thanks.",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'ring-theory', 'representation-theory']"
2,show that $\det{(A^2+B^2+C^2-BA-CB-AC)}=0$,show that,\det{(A^2+B^2+C^2-BA-CB-AC)}=0,"Let $A,B,C\in M_{2}(C)$ such that $$A^2+B^2+C^2=AB+BC+CA$$ show that $$\det{(A^2+B^2+C^2-BA-CB-AC)}=0$$ from: matrix indentity","Let $A,B,C\in M_{2}(C)$ such that $$A^2+B^2+C^2=AB+BC+CA$$ show that $$\det{(A^2+B^2+C^2-BA-CB-AC)}=0$$ from: matrix indentity",,['linear-algebra']
3,Stochastic matrices with orthogonal eigenvectors,Stochastic matrices with orthogonal eigenvectors,,"I stumbled across an odd fact while thinking about Markov chains, and I have an odd proof for it. Claim: Let $P$ be the transition matrix of a finite irreducible aperiodic Markov chain, and assume that $P$'s left eigenvectors are orthogonal. Then $P$ is doubly stochastic. Is this a special case of some well known result? In any case, here's my proof. Proof: Let $n$ denote the number of states. Since $P$ is the transition matrix of a finite irreducible aperiodic Markov chain, there is a unique stationary distribution $\pi$ and for any initial distribution $\mu$ there holds $\mu P^t \rightarrow \pi$ when $t \rightarrow \infty$. Let $\mu$ be an arbitrary distribution, let $t \geq 0$, and consider the inner product $\langle \mu P^t,\pi \rangle$. Let $\{v_i\}$ be an orthonormal basis of left eigenvectors for $P$ whose corresponding eigenvalues are $\{\lambda_i\}$ with $v_1 = \pi/\|\pi\|_2$ and $\lambda_1 = 1$, and write $\mu = \sum_{i=1}^n{\alpha_i v_i}$. Then  $$ \langle \mu P^t,\pi \rangle = \langle \sum_{i=1}^n{\alpha_i v_i P^t},\pi \rangle = \sum_{i=1}^n{\alpha_i \lambda_i^t \langle v_i,\pi\rangle} = \alpha_1 \langle v_1,\pi \rangle = \alpha_1 \|\pi\|_2. $$ So $(\langle \mu P^t,\pi \rangle)_{t=0}^{\infty}$ is a contant series. On the other hand, $\mu P^t \rightarrow \pi$ so $\langle \mu P^t,\pi \rangle \rightarrow \langle \pi,\pi \rangle$. Now, if a constant series converges to a value, then all of the elements of the series are equal to the value. Therefore $\langle \mu,\pi \rangle=\langle \pi,\pi \rangle$. This holds for any distribution $\mu$. In particular consider indicator vectors $e_i$: We have that for any $i,j \in [n]$,  $$ \langle e_i,\pi\rangle = \langle \pi,\pi\rangle=\langle e_j,\pi\rangle \Rightarrow \pi_i = \pi_j. $$ So $\pi$ is uniform. Thus, $\bar{1}\cdot P = \bar{1}$, so $P$'s columns all sum to 1 and $P$ is doubly stochastic. $\square$ This is a very strange proof - I've never seen a proof that shows that two values are equal because the first belongs to a constant series that converges to the second value. Can anyone think of a direct, simple proof for this claim? Is this claim a special case of some well known result in linear algebra?","I stumbled across an odd fact while thinking about Markov chains, and I have an odd proof for it. Claim: Let $P$ be the transition matrix of a finite irreducible aperiodic Markov chain, and assume that $P$'s left eigenvectors are orthogonal. Then $P$ is doubly stochastic. Is this a special case of some well known result? In any case, here's my proof. Proof: Let $n$ denote the number of states. Since $P$ is the transition matrix of a finite irreducible aperiodic Markov chain, there is a unique stationary distribution $\pi$ and for any initial distribution $\mu$ there holds $\mu P^t \rightarrow \pi$ when $t \rightarrow \infty$. Let $\mu$ be an arbitrary distribution, let $t \geq 0$, and consider the inner product $\langle \mu P^t,\pi \rangle$. Let $\{v_i\}$ be an orthonormal basis of left eigenvectors for $P$ whose corresponding eigenvalues are $\{\lambda_i\}$ with $v_1 = \pi/\|\pi\|_2$ and $\lambda_1 = 1$, and write $\mu = \sum_{i=1}^n{\alpha_i v_i}$. Then  $$ \langle \mu P^t,\pi \rangle = \langle \sum_{i=1}^n{\alpha_i v_i P^t},\pi \rangle = \sum_{i=1}^n{\alpha_i \lambda_i^t \langle v_i,\pi\rangle} = \alpha_1 \langle v_1,\pi \rangle = \alpha_1 \|\pi\|_2. $$ So $(\langle \mu P^t,\pi \rangle)_{t=0}^{\infty}$ is a contant series. On the other hand, $\mu P^t \rightarrow \pi$ so $\langle \mu P^t,\pi \rangle \rightarrow \langle \pi,\pi \rangle$. Now, if a constant series converges to a value, then all of the elements of the series are equal to the value. Therefore $\langle \mu,\pi \rangle=\langle \pi,\pi \rangle$. This holds for any distribution $\mu$. In particular consider indicator vectors $e_i$: We have that for any $i,j \in [n]$,  $$ \langle e_i,\pi\rangle = \langle \pi,\pi\rangle=\langle e_j,\pi\rangle \Rightarrow \pi_i = \pi_j. $$ So $\pi$ is uniform. Thus, $\bar{1}\cdot P = \bar{1}$, so $P$'s columns all sum to 1 and $P$ is doubly stochastic. $\square$ This is a very strange proof - I've never seen a proof that shows that two values are equal because the first belongs to a constant series that converges to the second value. Can anyone think of a direct, simple proof for this claim? Is this claim a special case of some well known result in linear algebra?",,"['linear-algebra', 'markov-chains']"
4,Proof a corollary on Hahn-Banach theorem,Proof a corollary on Hahn-Banach theorem,,"A corollary says Let $F\subset E,$ is a closed linear subspace of $E$, where $E$ is a normed vector space. Then it exists a linear functional $f\in E^*,f\neq 0$ such that $$ \langle x,f \rangle = 0, \forall x\in F .$$ That is, every closed linear subspace $F$ is contained by the kernel of some non-trivial continuous linear functional $f$. I tried to prove this as follows. Hahn-Bananch theorem implies for each $x_0\in E \setminus F $, $$\exists f\in E^* \Longrightarrow  \langle x,f \rangle \le \alpha \leq \langle x_0, f \rangle, \forall x\in F .$$ But how to preceed ? Any one helps ? Thank you in advance","A corollary says Let $F\subset E,$ is a closed linear subspace of $E$, where $E$ is a normed vector space. Then it exists a linear functional $f\in E^*,f\neq 0$ such that $$ \langle x,f \rangle = 0, \forall x\in F .$$ That is, every closed linear subspace $F$ is contained by the kernel of some non-trivial continuous linear functional $f$. I tried to prove this as follows. Hahn-Bananch theorem implies for each $x_0\in E \setminus F $, $$\exists f\in E^* \Longrightarrow  \langle x,f \rangle \le \alpha \leq \langle x_0, f \rangle, \forall x\in F .$$ But how to preceed ? Any one helps ? Thank you in advance",,"['linear-algebra', 'analysis', 'functional-analysis']"
5,Probability of building an Invertible Matrix,Probability of building an Invertible Matrix,,"If we build a $10\times10$ matrix, randomly filling with $1$ 's and $0$ 's, is it more likely to be invertible or singular? First of all until we have 10 $1$ 's it's not going to be invertible. With 10 $1$ 's on the diagonal, we can make many lower and upper triangular matrices, each of which will be invertible. But I'm not finding a way to proceed.","If we build a matrix, randomly filling with 's and 's, is it more likely to be invertible or singular? First of all until we have 10 's it's not going to be invertible. With 10 's on the diagonal, we can make many lower and upper triangular matrices, each of which will be invertible. But I'm not finding a way to proceed.",10\times10 1 0 1 1,"['linear-algebra', 'matrices']"
6,Linear Transformation on $\mathbb{R}^6$,Linear Transformation on,\mathbb{R}^6,"Let $W$ be a vector space over $\mathbb R$ and let $T:\mathbb R^6 \to W$ be a linear transformation such that $S = \{Te_2, Te_4, Te_6\}$ spans $W$. Wich one of the following must be true? (A) $S$ is a basis of $W$ (B) $T(\mathbb R^6) \ne W$ (C) $\{Te_1, Te_3, Te_5\}$ spans $W$ (D) $\ker T$ contains more than one element I'm having trouble starting this problem. Here are my findings so far: I tried Dimension theorem and found that $\ker(T)$ contains more than three elements so (D) is incorrect. And taking $\dim(T(\mathbb R^6))=\dim(W)$ we get $T(\mathbb R^6)=W$ so (B) is incorrect. I think (C) option is correct but still not able to prove it.","Let $W$ be a vector space over $\mathbb R$ and let $T:\mathbb R^6 \to W$ be a linear transformation such that $S = \{Te_2, Te_4, Te_6\}$ spans $W$. Wich one of the following must be true? (A) $S$ is a basis of $W$ (B) $T(\mathbb R^6) \ne W$ (C) $\{Te_1, Te_3, Te_5\}$ spans $W$ (D) $\ker T$ contains more than one element I'm having trouble starting this problem. Here are my findings so far: I tried Dimension theorem and found that $\ker(T)$ contains more than three elements so (D) is incorrect. And taking $\dim(T(\mathbb R^6))=\dim(W)$ we get $T(\mathbb R^6)=W$ so (B) is incorrect. I think (C) option is correct but still not able to prove it.",,"['linear-algebra', 'vector-spaces']"
7,Reversed Cayley transformation for any unitary matrix,Reversed Cayley transformation for any unitary matrix,,"It is well known that if $Q$ is a complex unitary matrix such that $I+Q$ is invertible (where $I$ is the identity matrix), that is, $-1$ is not an eigenvalue of $Q$, then $$ A:=(I-Q)(I+Q)^{-1} $$ is skew-Hermitian ($A=-A^*=-\bar{A}^T$). It is easy to prove this by factoring out the inverse from $$ \begin{split} A+A^*&=(I-Q)(I+Q)^{-1}+(I+Q)^{-*}(I-Q)^*\\ &=(I+Q)^{-*}[\color{blue}{(I+Q)^*(I-Q)+(I-Q)^*(I+Q)}](I+Q)^{-1}. \end{split}\tag{1} $$ and showing that the blue term is equal to zero. One can also use the spectral decomposition of $Q$. There is a unitary $U$ and a diagonal $D$ such that $Q=UDU^*$ with $D\bar{D}=I$. Then $$ A=U(I-D)(I+D)^{-1}U^*. $$ For each diagonal entry $\delta:=\alpha+i\beta$ of $D$, where $\alpha,\beta\in\mathbb{R}$ and $\delta\bar\delta=\alpha^2+\beta^2=1$, the corresponding entry of $(I-D)(I+D)^{-1}$ is imaginary $(1-\delta)(1+\delta)^{-1}=-2i\alpha\beta$ and we can use the fact that a matrix is skew-Hermitian if and only if it is unitarily similar to a diagonal matrix with imaginary diagonal entries. However, the approach in (1) is more elementary (and, consequently, elegant) since it does not require to know anything about spectral decompositions of unitary and skew-Hermitian matrices. Now the problem comes when one would like to drop the assumption that $I+Q$ is invertible while replacing the inverse with the Moore-Penrose pseudoinverse. Then $$\tag{2}A:=(I-Q)(I+Q)^{\dagger}$$ (where $\dagger$ denotes the pseudoinverse) is still skew-Hermitian as can be shown using the spectral decomposition. Indeed, $$ A=U(I-D)(I+D)^{\dagger}U^*, $$ and we can again show that the diagonal entries $\delta\neq -1$ of $D$ give imaginary entries while the diagonal entries $\delta=-1$ give $0$ in $(I-D)(I+D)^{\dagger}$. However, to show this, it is not possible to use directly the same approach as in (1) since $(I+Q)^{\dagger}(I+Q)\neq I$ if $-1$ is the eigenvalue of $Q$. I was wondering whether there exists a more elementary way to show that (2) is skew-Hermitian for any unitary $Q$ (without spectral decompositions and, ideally, using only the four Penrose conditions of the pseudoinverse or some of their simple consequences). EDIT: It might be maybe useful to consider the identities $X^\dagger=(X^*X)^\dagger X^*=X^*(XX^*)^\dagger$, which give $(I+Q)^\dagger=B(I+Q)^*=(I+Q)^*B$, where $B:=(2I+Q+Q^*)^\dagger$ is Hermitian (the operations $\dagger$ and $*$ ""commute""). I don't know yet how to get the final result (e.g., the first identity gives that $A=-A^*$ iff $B=QBQ^*$).","It is well known that if $Q$ is a complex unitary matrix such that $I+Q$ is invertible (where $I$ is the identity matrix), that is, $-1$ is not an eigenvalue of $Q$, then $$ A:=(I-Q)(I+Q)^{-1} $$ is skew-Hermitian ($A=-A^*=-\bar{A}^T$). It is easy to prove this by factoring out the inverse from $$ \begin{split} A+A^*&=(I-Q)(I+Q)^{-1}+(I+Q)^{-*}(I-Q)^*\\ &=(I+Q)^{-*}[\color{blue}{(I+Q)^*(I-Q)+(I-Q)^*(I+Q)}](I+Q)^{-1}. \end{split}\tag{1} $$ and showing that the blue term is equal to zero. One can also use the spectral decomposition of $Q$. There is a unitary $U$ and a diagonal $D$ such that $Q=UDU^*$ with $D\bar{D}=I$. Then $$ A=U(I-D)(I+D)^{-1}U^*. $$ For each diagonal entry $\delta:=\alpha+i\beta$ of $D$, where $\alpha,\beta\in\mathbb{R}$ and $\delta\bar\delta=\alpha^2+\beta^2=1$, the corresponding entry of $(I-D)(I+D)^{-1}$ is imaginary $(1-\delta)(1+\delta)^{-1}=-2i\alpha\beta$ and we can use the fact that a matrix is skew-Hermitian if and only if it is unitarily similar to a diagonal matrix with imaginary diagonal entries. However, the approach in (1) is more elementary (and, consequently, elegant) since it does not require to know anything about spectral decompositions of unitary and skew-Hermitian matrices. Now the problem comes when one would like to drop the assumption that $I+Q$ is invertible while replacing the inverse with the Moore-Penrose pseudoinverse. Then $$\tag{2}A:=(I-Q)(I+Q)^{\dagger}$$ (where $\dagger$ denotes the pseudoinverse) is still skew-Hermitian as can be shown using the spectral decomposition. Indeed, $$ A=U(I-D)(I+D)^{\dagger}U^*, $$ and we can again show that the diagonal entries $\delta\neq -1$ of $D$ give imaginary entries while the diagonal entries $\delta=-1$ give $0$ in $(I-D)(I+D)^{\dagger}$. However, to show this, it is not possible to use directly the same approach as in (1) since $(I+Q)^{\dagger}(I+Q)\neq I$ if $-1$ is the eigenvalue of $Q$. I was wondering whether there exists a more elementary way to show that (2) is skew-Hermitian for any unitary $Q$ (without spectral decompositions and, ideally, using only the four Penrose conditions of the pseudoinverse or some of their simple consequences). EDIT: It might be maybe useful to consider the identities $X^\dagger=(X^*X)^\dagger X^*=X^*(XX^*)^\dagger$, which give $(I+Q)^\dagger=B(I+Q)^*=(I+Q)^*B$, where $B:=(2I+Q+Q^*)^\dagger$ is Hermitian (the operations $\dagger$ and $*$ ""commute""). I don't know yet how to get the final result (e.g., the first identity gives that $A=-A^*$ iff $B=QBQ^*$).",,"['linear-algebra', 'matrices', 'pseudoinverse']"
8,How do I show that $\inf\limits_{\det(X)\neq0}\|X^{-1}AX\|^{2}_{F}=\sum\limits_{\lambda\in{\Lambda}}|\lambda|^{2}$?,How do I show that ?,\inf\limits_{\det(X)\neq0}\|X^{-1}AX\|^{2}_{F}=\sum\limits_{\lambda\in{\Lambda}}|\lambda|^{2},"Show that $$\inf\limits_{\det(X)\neq 0}\|X^{-1}AX\|^2_F=\sum_{\lambda\in\Lambda}|\lambda|^{2}$$ holds, where $\Lambda(A)$ is the set containing all eigenvalues of A, and $\|\cdot\|_{F}$ is the Frobenius norm. Also, I want to show that it is indeed the minimum if and only if $A$ is diagonalizable.","Show that $$\inf\limits_{\det(X)\neq 0}\|X^{-1}AX\|^2_F=\sum_{\lambda\in\Lambda}|\lambda|^{2}$$ holds, where $\Lambda(A)$ is the set containing all eigenvalues of A, and $\|\cdot\|_{F}$ is the Frobenius norm. Also, I want to show that it is indeed the minimum if and only if $A$ is diagonalizable.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
9,Inverse of a tridiagonal Toeplitz matrix,Inverse of a tridiagonal Toeplitz matrix,,"Let  $${A_{n \times n}} = \left[ {\begin{array}{*{20}{c}} {-2}&{1}&{}&{}&{}\\ {1}&{-2}&{1}&{}&{}\\ {}&{1}&{\ddots}&{\ddots}&{}\\ {}&{}&{\ddots}&{\ddots}&{1}\\ {}&{}&{}&{1}&{-2} \end{array}} \right]$$ be a tridiagonal matrix. How we can prove that its inverse is the matrix $B=(b_{ij})$ where $$b_{ij}=-\frac{i(n+1-j)}{n+1} \; ,\quad i\leq j.$$","Let  $${A_{n \times n}} = \left[ {\begin{array}{*{20}{c}} {-2}&{1}&{}&{}&{}\\ {1}&{-2}&{1}&{}&{}\\ {}&{1}&{\ddots}&{\ddots}&{}\\ {}&{}&{\ddots}&{\ddots}&{1}\\ {}&{}&{}&{1}&{-2} \end{array}} \right]$$ be a tridiagonal matrix. How we can prove that its inverse is the matrix $B=(b_{ij})$ where $$b_{ij}=-\frac{i(n+1-j)}{n+1} \; ,\quad i\leq j.$$",,"['linear-algebra', 'matrices', 'inverse', 'tridiagonal-matrices', 'toeplitz-matrices']"
10,is this kind of symmetric matrix invertible?,is this kind of symmetric matrix invertible?,,"Give a matrix $A=\begin{bmatrix}M&B\\ B^T&0\end{bmatrix}$, where $M\in\mathbb{R}^{n\times n}$, $B\in \mathbb{R}^{n\times m}, (m<n)$. If we know that $rank(B)=m$ and  for any $v\neq 0$ and $v\in Null(B^T)$, i.e $B^T v=0$, we have $v^T Mv >0$. Then is the matrix $A$ invertible?","Give a matrix $A=\begin{bmatrix}M&B\\ B^T&0\end{bmatrix}$, where $M\in\mathbb{R}^{n\times n}$, $B\in \mathbb{R}^{n\times m}, (m<n)$. If we know that $rank(B)=m$ and  for any $v\neq 0$ and $v\in Null(B^T)$, i.e $B^T v=0$, we have $v^T Mv >0$. Then is the matrix $A$ invertible?",,"['linear-algebra', 'matrices']"
11,Equivalent norms and inner product,Equivalent norms and inner product,,"It is not hard to give examples of normed spaces which are not inner product spaces. Now let $(V, \|\cdot\|)$ be a normed space. Is it always possible to construct an inner product on $V$ which gives the same topology on $V$ as before? (As pointed out in comments, in finite dimensional case it it always the case).","It is not hard to give examples of normed spaces which are not inner product spaces. Now let $(V, \|\cdot\|)$ be a normed space. Is it always possible to construct an inner product on $V$ which gives the same topology on $V$ as before? (As pointed out in comments, in finite dimensional case it it always the case).",,"['linear-algebra', 'functional-analysis']"
12,"Consider $AX + XA = B$, how many equations and how many unknowns?","Consider , how many equations and how many unknowns?",AX + XA = B,"Here, $A$, $X$, and $B$ are real $n\times n$ matrices.  A and B are given, but X is unknown. Also, A is symmetric and positive definite, so $A^T = A$ and $z^TAz >0$ for all nonzero $z$ in $\mathbb R^n$. My question is:  I can easily show that $AX + XA$ is linear in $x$. but...how many equations and unknowns does it involve? I wrote out some scratch-work and agree with the solution that it involves $n^2$ unknowns.  However, the solution also says it involves $n^2$ equations.  I'm pretty sure that's wrong, and that there are only $n$ equations involved. Thanks, Edit: How could I show that F(x) = AX + XA is a bijection? I feel like I should be using what's given in the question: that A is real symmetric, positive definite, so then A has real, positive eigenvalues.  Also, A is diagonalizable - orthogonally diagonalizable, in fact - so it has n eigenvectors that form a basis for R^n.  Not sure how to proceed to prove the bijection...","Here, $A$, $X$, and $B$ are real $n\times n$ matrices.  A and B are given, but X is unknown. Also, A is symmetric and positive definite, so $A^T = A$ and $z^TAz >0$ for all nonzero $z$ in $\mathbb R^n$. My question is:  I can easily show that $AX + XA$ is linear in $x$. but...how many equations and unknowns does it involve? I wrote out some scratch-work and agree with the solution that it involves $n^2$ unknowns.  However, the solution also says it involves $n^2$ equations.  I'm pretty sure that's wrong, and that there are only $n$ equations involved. Thanks, Edit: How could I show that F(x) = AX + XA is a bijection? I feel like I should be using what's given in the question: that A is real symmetric, positive definite, so then A has real, positive eigenvalues.  Also, A is diagonalizable - orthogonally diagonalizable, in fact - so it has n eigenvectors that form a basis for R^n.  Not sure how to proceed to prove the bijection...",,"['linear-algebra', 'matrices', 'matrix-equations']"
13,Commuting matrices implies upper triangular simultaneously,Commuting matrices implies upper triangular simultaneously,,"Let $A_\alpha$ be a family of commuting matrices, that is, $A_\alpha A_\beta=A_\beta A_\alpha$. Show that there exists an unitary matrix $U$ such that $U^*A_\alpha U$ is upper triangular for each $\alpha$. I know that there should be something related to invariant subspaces. However, I could not proceed...","Let $A_\alpha$ be a family of commuting matrices, that is, $A_\alpha A_\beta=A_\beta A_\alpha$. Show that there exists an unitary matrix $U$ such that $U^*A_\alpha U$ is upper triangular for each $\alpha$. I know that there should be something related to invariant subspaces. However, I could not proceed...",,['linear-algebra']
14,Eigenvalue of the Power of a Matrix [duplicate],Eigenvalue of the Power of a Matrix [duplicate],,"This question already has answers here : Eigenvalues and power of a matrix (3 answers) Closed 3 years ago . Let $A$ be an n×n matrix with eigenvalues $\lambda_1,\dots,\lambda_n$. Show that $\lambda_1^k,\dots,\lambda_n^k$ are the eigenvalues of $A^k$. I don't know where to start.","This question already has answers here : Eigenvalues and power of a matrix (3 answers) Closed 3 years ago . Let $A$ be an n×n matrix with eigenvalues $\lambda_1,\dots,\lambda_n$. Show that $\lambda_1^k,\dots,\lambda_n^k$ are the eigenvalues of $A^k$. I don't know where to start.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
15,Non trivial solutions for homogeneous equations,Non trivial solutions for homogeneous equations,,"Consider the following homogeneous equation where $A$ and $X$ are matrices. $$AX = 0$$ I want to know whether there are non trivial solutions for this equations. Now, if $A^{-1}$ exists, then I can multiply throughout by it and get $X = 0$, so if $A$ is invertible, only the trivial solution exists. However, I do not understand why $A$ being non-invertible would imply that non-trivial solutions exists, shouldn't it just imply that no solutions exist?","Consider the following homogeneous equation where $A$ and $X$ are matrices. $$AX = 0$$ I want to know whether there are non trivial solutions for this equations. Now, if $A^{-1}$ exists, then I can multiply throughout by it and get $X = 0$, so if $A$ is invertible, only the trivial solution exists. However, I do not understand why $A$ being non-invertible would imply that non-trivial solutions exists, shouldn't it just imply that no solutions exist?",,['linear-algebra']
16,Explicit formula for inverse matrix elements,Explicit formula for inverse matrix elements,,"Let $A$ be an $n \times n$ invertible matrix with  \begin{align} \left(\begin{array}{ccc} a_{11} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{nn}  \end{array}\right)^{-1}= \left(\begin{array}{ccc} b_{11} & \cdots & b_{1n} \\ \vdots & \ddots & \vdots \\ b_{n1} & \cdots & b_{nn}  \end{array}\right) \end{align} Is there an explicit formula for $b_{ij}$ in terms of the elements of $A$ and the determinant of $A$? Edit: Here is a link to the different possible methods to invert a matrix http://en.m.wikipedia.org/wiki/Invertible_matrix#Methods_of_matrix_inversion . Originally I had in mind using the blockwise inversion method but I think all of the methods require using some submatrix of $A$ and their determinants and / or inverses. I don't think it's possible to write something explicit in terms of $\{a_{ij}\}_{i=1,j=1}^{n,n}$","Let $A$ be an $n \times n$ invertible matrix with  \begin{align} \left(\begin{array}{ccc} a_{11} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{nn}  \end{array}\right)^{-1}= \left(\begin{array}{ccc} b_{11} & \cdots & b_{1n} \\ \vdots & \ddots & \vdots \\ b_{n1} & \cdots & b_{nn}  \end{array}\right) \end{align} Is there an explicit formula for $b_{ij}$ in terms of the elements of $A$ and the determinant of $A$? Edit: Here is a link to the different possible methods to invert a matrix http://en.m.wikipedia.org/wiki/Invertible_matrix#Methods_of_matrix_inversion . Originally I had in mind using the blockwise inversion method but I think all of the methods require using some submatrix of $A$ and their determinants and / or inverses. I don't think it's possible to write something explicit in terms of $\{a_{ij}\}_{i=1,j=1}^{n,n}$",,"['linear-algebra', 'matrices', 'inverse']"
17,Statement about $(I-A)^{-1}$ matrices,Statement about  matrices,(I-A)^{-1},"Let $A \in \mathbb{R}^{n \times n}$ and let denote $I$ the $n \times n$ identitiy matrix . Theorem. If $(I-A)$ is invertible and $(I-A)^{-1}$ is a nonnegative matrix and there is such a diagonal element in $(I-A)^{-1}$ which is less then $1$, then at least one of the elements in $A$ is negative. Question. How could we prove this statement? I've tried to use Perron–Frobenius theorem for $(I-A)^{-1}$, but after I've taken the eigenequation I don't know how to use the condition for the diagonal element. I've also tried binomial inverse theorem and Woodbury matrix identity but they also didn't help me. Although I think using these tools lead us far away.","Let $A \in \mathbb{R}^{n \times n}$ and let denote $I$ the $n \times n$ identitiy matrix . Theorem. If $(I-A)$ is invertible and $(I-A)^{-1}$ is a nonnegative matrix and there is such a diagonal element in $(I-A)^{-1}$ which is less then $1$, then at least one of the elements in $A$ is negative. Question. How could we prove this statement? I've tried to use Perron–Frobenius theorem for $(I-A)^{-1}$, but after I've taken the eigenequation I don't know how to use the condition for the diagonal element. I've also tried binomial inverse theorem and Woodbury matrix identity but they also didn't help me. Although I think using these tools lead us far away.",,"['linear-algebra', 'matrices', 'inverse']"
18,Is Linear Algebra the foundation of Applied Mathematics?,Is Linear Algebra the foundation of Applied Mathematics?,,"I've lately taken an interest in foundations of my field. While there are many important areas that contribute to Applied Mathematics (differential equations, probability & statistics, numerical methods, combinatorics), it seems that Linear Algebra is an essential ingredient in most Applied Mathematics projects. If one had to take only one course after the general calculus sequence, it seems like advanced Linear Algebra would be the choice, hands-down. As examples: In statistical applications, you often rely on solutions falling (at least approximately) in the space of Normal distributions, which is a linear space defined over the reals. The differential operator is a linear operator, so you can represent systems of differential equations using linear algebra. Numerical methods typically linearize a problem and then iterate to converge to a solution. Is this just an artifact of my experiences, or is Linear Algebra really the foundational framework/theory of applied mathematics?","I've lately taken an interest in foundations of my field. While there are many important areas that contribute to Applied Mathematics (differential equations, probability & statistics, numerical methods, combinatorics), it seems that Linear Algebra is an essential ingredient in most Applied Mathematics projects. If one had to take only one course after the general calculus sequence, it seems like advanced Linear Algebra would be the choice, hands-down. As examples: In statistical applications, you often rely on solutions falling (at least approximately) in the space of Normal distributions, which is a linear space defined over the reals. The differential operator is a linear operator, so you can represent systems of differential equations using linear algebra. Numerical methods typically linearize a problem and then iterate to converge to a solution. Is this just an artifact of my experiences, or is Linear Algebra really the foundational framework/theory of applied mathematics?",,['linear-algebra']
19,Real vs. complex conjugate [duplicate],Real vs. complex conjugate [duplicate],,"This question already has answers here : Similar matrices and field extensions (6 answers) Closed 7 years ago . Suppose I have matrices $A,B\in\mathrm{Mat}_n(\mathbf{R})$ which are conjugate in $\mathrm{Mat}_n(\mathbf{C})$ in the sense that there is $S\in\mathrm{GL}_n(\mathbf{C})$ with $A=SBS^{-1}$. Then is it possible to choose $S$ with this property and $S\in\mathrm{GL}_n(\mathbf{R})$? this fact is claimed in the answers to An element of $SL(2,\mathbb{R})$","This question already has answers here : Similar matrices and field extensions (6 answers) Closed 7 years ago . Suppose I have matrices $A,B\in\mathrm{Mat}_n(\mathbf{R})$ which are conjugate in $\mathrm{Mat}_n(\mathbf{C})$ in the sense that there is $S\in\mathrm{GL}_n(\mathbf{C})$ with $A=SBS^{-1}$. Then is it possible to choose $S$ with this property and $S\in\mathrm{GL}_n(\mathbf{R})$? this fact is claimed in the answers to An element of $SL(2,\mathbb{R})$",,"['linear-algebra', 'matrices']"
20,Whats wrong with this proof of Cayley-Hamilton,Whats wrong with this proof of Cayley-Hamilton,,"There are a handful of longish proofs for Cayley-Hamilton, but I haven't seen one that goes along the following lines.  What's wrong in my thinking. Take a real square matrix $A$, with Jordan decomposition $D=Q^{-1}AQ$.  Call the eigenvalues of $A$ (and $D$) $\lambda_1,\lambda_2,\dotsc,\lambda_k$, with respective algebraic multiplicities $n_1,n_2,\dotsc,n_k$.  Then $A$ has the characteristic polynomial: $$\Delta(\lambda):=\det(\lambda I-A)=\prod_i(\lambda-\lambda_i)^{n_i}.$$ So, $$\Delta(A)= \prod_i(A-\lambda_i)^{n_i}=Q\left(\prod_i(D-\lambda_i)^{n_i}\right)Q^{-1}.$$ The block(s) associated with $\lambda_i$ for the matrix $(D-\lambda_i)^{n_i}$ are nilpotent (for at least $n_i$ if not smaller) so $\prod_i(D-\lambda_i)^{n_i}=\mathbf{0}$ so $\Delta(A)=\mathbf{0}$.","There are a handful of longish proofs for Cayley-Hamilton, but I haven't seen one that goes along the following lines.  What's wrong in my thinking. Take a real square matrix $A$, with Jordan decomposition $D=Q^{-1}AQ$.  Call the eigenvalues of $A$ (and $D$) $\lambda_1,\lambda_2,\dotsc,\lambda_k$, with respective algebraic multiplicities $n_1,n_2,\dotsc,n_k$.  Then $A$ has the characteristic polynomial: $$\Delta(\lambda):=\det(\lambda I-A)=\prod_i(\lambda-\lambda_i)^{n_i}.$$ So, $$\Delta(A)= \prod_i(A-\lambda_i)^{n_i}=Q\left(\prod_i(D-\lambda_i)^{n_i}\right)Q^{-1}.$$ The block(s) associated with $\lambda_i$ for the matrix $(D-\lambda_i)^{n_i}$ are nilpotent (for at least $n_i$ if not smaller) so $\prod_i(D-\lambda_i)^{n_i}=\mathbf{0}$ so $\Delta(A)=\mathbf{0}$.",,['linear-algebra']
21,General Cholesky-like decomposition,General Cholesky-like decomposition,,"For a real positive definite matrix $A$, we can find the Cholesky decomposition $LL^T = A$. If we relax the constraints that L has to be lower triangular, we should be able to find a whole host of decompositions $FF^T=A$. Is there anything known about how to find the general solution to this?","For a real positive definite matrix $A$, we can find the Cholesky decomposition $LL^T = A$. If we relax the constraints that L has to be lower triangular, we should be able to find a whole host of decompositions $FF^T=A$. Is there anything known about how to find the general solution to this?",,"['linear-algebra', 'matrices', 'matrix-decomposition']"
22,compute the bisecting normal hyperplane between two $n$-dimensional points.,compute the bisecting normal hyperplane between two -dimensional points.,n,"I have two points $\mathbf{x_1}$ and $\mathbf{x_2}$, where $\mathbf{x_i}=\{x^i_1, x^i_2, \ldots, x^i_n\}$. I need to find a normal hyperplane $P$ that goes through the midpoint of $\mathbf{x_1}$ and $\mathbf{x_2}$, and I need to do it numerically. My take -- Find the direction vector from $\mathbf{x_1}$ to $\mathbf{x_2}$: $$\mathbf{V} = \mathbf{x_2} - \mathbf{x_1}$$ Find the unit vector: $$\vec{v} = \frac{\mathbf{V}}{||\mathbf{v}||}$$ Find the mid-point: $$\mathbf{x_{mid}} = \left[\frac{x^1_1 + x^1_2}{2},\frac{x^2_1 + x^2_2}{2}, \ldots, \frac{x^n_1 + x^n_2}{2}\right]$$ Now suppose the plane $P$ goes through some arbitrary $\mathbf x$, therefore -- $${\vec v} \cdot (\mathbf{x} - \mathbf{x_{mid}}) = 0 \quad\Rightarrow\quad \vec{v} \cdot \mathbf{x} = \vec{v} \cdot \mathbf{x_{mid}}$$ or -- \begin{align} \Rightarrow [v_1, v_2, \ldots, v_n][x_1, x_2, \ldots, x_n]^T = [v_1, v_2, \ldots, v_3]\left[\frac{x^1_1 + x^1_2}{2},\frac{x^2_1 + x^2_2}{2}, \ldots, \frac{x^n_1 + x^n_2}{2}\right]^T\\ \Rightarrow [x_1, x_2, \ldots, x_n]^T = [v_1, v_2, \ldots, v_3]\left[\frac{x^1_1 + x^1_2}{2},\frac{x^2_1 + x^2_2}{2}, \ldots, \frac{x^n_1 + x^n_2}{2}\right]^T [v_1, v_2, \ldots, v_n]^{-1}\\ \Rightarrow[x_1, x_2, \ldots, x_n]^T = \left[\frac{x^1_1 + x^1_2}{2},\frac{x^2_1 + x^2_2}{2}, \ldots, \frac{x^n_1 + x^n_2}{2}\right]^T \Leftarrow ?? \end{align} I lost all my clues with this computation, what I am missing here?","I have two points $\mathbf{x_1}$ and $\mathbf{x_2}$, where $\mathbf{x_i}=\{x^i_1, x^i_2, \ldots, x^i_n\}$. I need to find a normal hyperplane $P$ that goes through the midpoint of $\mathbf{x_1}$ and $\mathbf{x_2}$, and I need to do it numerically. My take -- Find the direction vector from $\mathbf{x_1}$ to $\mathbf{x_2}$: $$\mathbf{V} = \mathbf{x_2} - \mathbf{x_1}$$ Find the unit vector: $$\vec{v} = \frac{\mathbf{V}}{||\mathbf{v}||}$$ Find the mid-point: $$\mathbf{x_{mid}} = \left[\frac{x^1_1 + x^1_2}{2},\frac{x^2_1 + x^2_2}{2}, \ldots, \frac{x^n_1 + x^n_2}{2}\right]$$ Now suppose the plane $P$ goes through some arbitrary $\mathbf x$, therefore -- $${\vec v} \cdot (\mathbf{x} - \mathbf{x_{mid}}) = 0 \quad\Rightarrow\quad \vec{v} \cdot \mathbf{x} = \vec{v} \cdot \mathbf{x_{mid}}$$ or -- \begin{align} \Rightarrow [v_1, v_2, \ldots, v_n][x_1, x_2, \ldots, x_n]^T = [v_1, v_2, \ldots, v_3]\left[\frac{x^1_1 + x^1_2}{2},\frac{x^2_1 + x^2_2}{2}, \ldots, \frac{x^n_1 + x^n_2}{2}\right]^T\\ \Rightarrow [x_1, x_2, \ldots, x_n]^T = [v_1, v_2, \ldots, v_3]\left[\frac{x^1_1 + x^1_2}{2},\frac{x^2_1 + x^2_2}{2}, \ldots, \frac{x^n_1 + x^n_2}{2}\right]^T [v_1, v_2, \ldots, v_n]^{-1}\\ \Rightarrow[x_1, x_2, \ldots, x_n]^T = \left[\frac{x^1_1 + x^1_2}{2},\frac{x^2_1 + x^2_2}{2}, \ldots, \frac{x^n_1 + x^n_2}{2}\right]^T \Leftarrow ?? \end{align} I lost all my clues with this computation, what I am missing here?",,"['linear-algebra', 'geometry', 'numerical-linear-algebra', 'vectors']"
23,When is there a vector $D$ with positive coordinates such that $e^{Ct}D$ has a negative coordinate?,When is there a vector  with positive coordinates such that  has a negative coordinate?,D e^{Ct}D,"Let $C$ be a $2 \times 2$ asymmetric matrix with real entries. Assume that $C$ has strictly negative, real eigenvalues. Fix $D\in\mathbb{R}^2$, where $D > 0$ (i.e., both coordinates are strictly positive). Let $t \geq 0$ be a scalar and define the vector valued function using the matrix exponential, $$ H(t) = e^{C t} D\in\mathbb{R}^2 $$ Denote $\min(H(t))<0$ if at least one of the elements is less than $0$. Fixing $D > 0$, under what conditions on $C$ and $D$ does a $t^{*}> 0$ exist such that $\min(H(t^*)) < 0$?  Or conversely, given a numerical $C$ and $D$, how can I test whether the $H(t) > 0$ for all $t$? Ideal conditions are checking signs of eigenvectors or diagonal elements of $C$, etc.  If they can be done without a particular $D$ matrix, so much the better. A few notes: If possible, you can use the assumption that $-C^{-1} D \cdot \begin{bmatrix} 1 &1\end{bmatrix} = 1$.  i.e. the sum of $-C^{-1}D = 1$  This comes out of necessary equilibrium condition to ensure that $H(t)$ is a valid PDF, and may pin down requirements to make the answer only requirements on $C$. $C$ negative definite should be enough to ensure that $\lim\limits_{t\to\infty}H(t) = 0$ If you wish, assume that C is diagonalizable and denote $C \equiv Q S Q^{-1}$, where $S$ the matrix of eigenvalues (both $< 0$).  In that case,  $$ H(t)= Q e^{S t} Q^{-1} D $$ Solution Approach?: Clearly, having negative eigenvalues of $C$ ensures convergence of $H(t) \to 0$, but what do the eigenvectors of $C$ tell us? Can they tell us from which direction it approaches $0$ (given the sign of $D$)? I notice that in examples which break, one of the eigenvectors has the same signs for both coordinates, while the other has different signs?  Could $Q \leq 0$ be the answer? Example where it goes below 0 with negative definite C: See the following matlab code: C = [- 2.2959 -1.5; -.1 -1.6918]; min(eig(-C)) %Can check the eigenvalues to ensure negative definite D = [2.0; 0.6918]; F_p = @(z) expm(C * z) * D; %Evaluate at a few z's F_p(1) %Both > 0 F_p(3) %One of them < 0!!! F_p(100) %They both converge to 0 %Could diagonalize C [Q,B] = eig(C) % Then define F''(z) F_pp = @(z) Q * diag(exp(diag(B) * z))* inv(Q) * C * D Added from Idea Given in Solution: The following may be a sketch of a proof that $D$ as the eigenvector of $C$ fulfills the requirement, Preliminaries: $A$ and $A^{n}$ have the same eigenvectors and if $\lambda $ is a root of $A,\ \lambda ^{n\text{ }}\ $is a root of $A^{n}.\ $And $-A$ has roots $ -\lambda $ $If$ $A$ is non-negative (with some positive elements) it has a non-negative eigenvector $v\geq 0$ associated to a dominant root $\hat{ \lambda}>0,$  which is real, simple and larger in modulus than its other roots. $A$ and $-A\ $\ have the same eigenvectors. Now consider $e^{C t}D=\left[ I+C t+\frac{1}{2!}\left( C t\right) ^{2}+\frac{1}{ 3!}\left( C t\right) ^{3}+\frac{1}{4!}\left( C t\right) ^{4}+\frac{1}{5!} \left( C t\right) ^{5}+\frac{1}{6!}\left( C t\right) ^{6}+..\right] D$ where $ C\leq 0.$ Take $v$ to be the dominant eigenvector of $-C.$ So if $\lambda $ is the dominant root of $-C,$ then the minimum eigenvalue of $-C$ is $ -\lambda .$ Take $D=v.$ Then \begin{eqnarray*} C t v &=&\left( -\lambda t\right) v \\ \left( C t\right) ^{2}v &=&\left( -\lambda t\right) ^{2}v \\ \left( C t\right) ^{3}v &=&\left( -\lambda t\right) ^{3}v \end{eqnarray*} \begin{eqnarray*} e^{C t}v &=&\left[ I+C t+\frac{1}{2!}\left( C t\right) ^{2}+\frac{1}{3!}\left( C t\right) ^{3}+\frac{1}{4!}\left( C t\right) ^{4}+\frac{1}{5!}\left( C t\right) ^{5}+\frac{1}{6!}\left( C t\right) ^{6}+..\right] v \\ &=&\left[ 1+\left( -\lambda t\right) +\frac{1}{2!}\left( -\lambda t\right) ^{2}+\frac{1}{3!}\left( -\lambda t\right) ^{3}+\frac{1}{4!}\left( -\lambda t\right) ^{4}+\frac{1}{5!}\left( -\lambda t\right) ^{5}+\frac{1}{6!}\left( -\lambda t\right) ^{6}+..\right] v \end{eqnarray*} But by definition $$ e^{-\lambda t}=\left[ 1+\left( -\lambda t\right) +\frac{1}{2!}\left( -\lambda t\right) ^{2}+\frac{1}{3!}\left( -\lambda t\right) ^{3}+\frac{1}{4!} \left( -\lambda t\right) ^{4}+\frac{1}{5!}\left( -\lambda t\right) ^{5}+ \frac{1}{6!}\left( -\lambda t\right) ^{6}+..\right]  $$ So $$ e^{C t}v=\left[ I+C t+\frac{1}{2!}\left( C t\right) ^{2}+\frac{1}{3!}\left( C t\right) ^{3}+\frac{1}{4!}\left( C t\right) ^{4}+\frac{1}{5!}\left( C t\right) ^{5}+\frac{1}{6!}\left( C t\right) ^{6}+..\right] v=e^{-\lambda t}v\geq 0 $$ since $v$ is the non-negative eigenvector. Choose $D=H\left( 0\right) =v$ and $H\left( t\right) =e^{Ct}D\geq 0.$","Let $C$ be a $2 \times 2$ asymmetric matrix with real entries. Assume that $C$ has strictly negative, real eigenvalues. Fix $D\in\mathbb{R}^2$, where $D > 0$ (i.e., both coordinates are strictly positive). Let $t \geq 0$ be a scalar and define the vector valued function using the matrix exponential, $$ H(t) = e^{C t} D\in\mathbb{R}^2 $$ Denote $\min(H(t))<0$ if at least one of the elements is less than $0$. Fixing $D > 0$, under what conditions on $C$ and $D$ does a $t^{*}> 0$ exist such that $\min(H(t^*)) < 0$?  Or conversely, given a numerical $C$ and $D$, how can I test whether the $H(t) > 0$ for all $t$? Ideal conditions are checking signs of eigenvectors or diagonal elements of $C$, etc.  If they can be done without a particular $D$ matrix, so much the better. A few notes: If possible, you can use the assumption that $-C^{-1} D \cdot \begin{bmatrix} 1 &1\end{bmatrix} = 1$.  i.e. the sum of $-C^{-1}D = 1$  This comes out of necessary equilibrium condition to ensure that $H(t)$ is a valid PDF, and may pin down requirements to make the answer only requirements on $C$. $C$ negative definite should be enough to ensure that $\lim\limits_{t\to\infty}H(t) = 0$ If you wish, assume that C is diagonalizable and denote $C \equiv Q S Q^{-1}$, where $S$ the matrix of eigenvalues (both $< 0$).  In that case,  $$ H(t)= Q e^{S t} Q^{-1} D $$ Solution Approach?: Clearly, having negative eigenvalues of $C$ ensures convergence of $H(t) \to 0$, but what do the eigenvectors of $C$ tell us? Can they tell us from which direction it approaches $0$ (given the sign of $D$)? I notice that in examples which break, one of the eigenvectors has the same signs for both coordinates, while the other has different signs?  Could $Q \leq 0$ be the answer? Example where it goes below 0 with negative definite C: See the following matlab code: C = [- 2.2959 -1.5; -.1 -1.6918]; min(eig(-C)) %Can check the eigenvalues to ensure negative definite D = [2.0; 0.6918]; F_p = @(z) expm(C * z) * D; %Evaluate at a few z's F_p(1) %Both > 0 F_p(3) %One of them < 0!!! F_p(100) %They both converge to 0 %Could diagonalize C [Q,B] = eig(C) % Then define F''(z) F_pp = @(z) Q * diag(exp(diag(B) * z))* inv(Q) * C * D Added from Idea Given in Solution: The following may be a sketch of a proof that $D$ as the eigenvector of $C$ fulfills the requirement, Preliminaries: $A$ and $A^{n}$ have the same eigenvectors and if $\lambda $ is a root of $A,\ \lambda ^{n\text{ }}\ $is a root of $A^{n}.\ $And $-A$ has roots $ -\lambda $ $If$ $A$ is non-negative (with some positive elements) it has a non-negative eigenvector $v\geq 0$ associated to a dominant root $\hat{ \lambda}>0,$  which is real, simple and larger in modulus than its other roots. $A$ and $-A\ $\ have the same eigenvectors. Now consider $e^{C t}D=\left[ I+C t+\frac{1}{2!}\left( C t\right) ^{2}+\frac{1}{ 3!}\left( C t\right) ^{3}+\frac{1}{4!}\left( C t\right) ^{4}+\frac{1}{5!} \left( C t\right) ^{5}+\frac{1}{6!}\left( C t\right) ^{6}+..\right] D$ where $ C\leq 0.$ Take $v$ to be the dominant eigenvector of $-C.$ So if $\lambda $ is the dominant root of $-C,$ then the minimum eigenvalue of $-C$ is $ -\lambda .$ Take $D=v.$ Then \begin{eqnarray*} C t v &=&\left( -\lambda t\right) v \\ \left( C t\right) ^{2}v &=&\left( -\lambda t\right) ^{2}v \\ \left( C t\right) ^{3}v &=&\left( -\lambda t\right) ^{3}v \end{eqnarray*} \begin{eqnarray*} e^{C t}v &=&\left[ I+C t+\frac{1}{2!}\left( C t\right) ^{2}+\frac{1}{3!}\left( C t\right) ^{3}+\frac{1}{4!}\left( C t\right) ^{4}+\frac{1}{5!}\left( C t\right) ^{5}+\frac{1}{6!}\left( C t\right) ^{6}+..\right] v \\ &=&\left[ 1+\left( -\lambda t\right) +\frac{1}{2!}\left( -\lambda t\right) ^{2}+\frac{1}{3!}\left( -\lambda t\right) ^{3}+\frac{1}{4!}\left( -\lambda t\right) ^{4}+\frac{1}{5!}\left( -\lambda t\right) ^{5}+\frac{1}{6!}\left( -\lambda t\right) ^{6}+..\right] v \end{eqnarray*} But by definition $$ e^{-\lambda t}=\left[ 1+\left( -\lambda t\right) +\frac{1}{2!}\left( -\lambda t\right) ^{2}+\frac{1}{3!}\left( -\lambda t\right) ^{3}+\frac{1}{4!} \left( -\lambda t\right) ^{4}+\frac{1}{5!}\left( -\lambda t\right) ^{5}+ \frac{1}{6!}\left( -\lambda t\right) ^{6}+..\right]  $$ So $$ e^{C t}v=\left[ I+C t+\frac{1}{2!}\left( C t\right) ^{2}+\frac{1}{3!}\left( C t\right) ^{3}+\frac{1}{4!}\left( C t\right) ^{4}+\frac{1}{5!}\left( C t\right) ^{5}+\frac{1}{6!}\left( C t\right) ^{6}+..\right] v=e^{-\lambda t}v\geq 0 $$ since $v$ is the non-negative eigenvector. Choose $D=H\left( 0\right) =v$ and $H\left( t\right) =e^{Ct}D\geq 0.$",,"['linear-algebra', 'matrices', 'exponential-function']"
24,Can I turn $Ax=b$ into $Ax=0$?,Can I turn  into ?,Ax=b Ax=0,"For a system of equations $$ \begin{bmatrix}d_1 & d_2 & \dots & d_n \end{bmatrix} \begin{bmatrix}u_1\\u_2\\ \vdots \\ u_n \end{bmatrix} = d_{n+1} $$ where each $d$ is a column of (possibly noisy) data and each $u$ is a scalar unknown, is the correct approach to call this $Ax=b$ and solve as $x = A\setminus b$, or to rearrange it into an $Ax=0$ form as below, $$ \begin{bmatrix}d_1 & d_2 & \cdots & d_n & d_{n+1}\end{bmatrix} \begin{bmatrix}u_1\\u_2\\ \vdots \\ u_n \\ -1\end{bmatrix} = 0 $$ using SVD to find the vector of unknowns, then normalizing by its final element? In matlab these give different answers, but I don't know if the difference is in noise or implementation or if there's some theory that I'm missing.","For a system of equations $$ \begin{bmatrix}d_1 & d_2 & \dots & d_n \end{bmatrix} \begin{bmatrix}u_1\\u_2\\ \vdots \\ u_n \end{bmatrix} = d_{n+1} $$ where each $d$ is a column of (possibly noisy) data and each $u$ is a scalar unknown, is the correct approach to call this $Ax=b$ and solve as $x = A\setminus b$, or to rearrange it into an $Ax=0$ form as below, $$ \begin{bmatrix}d_1 & d_2 & \cdots & d_n & d_{n+1}\end{bmatrix} \begin{bmatrix}u_1\\u_2\\ \vdots \\ u_n \\ -1\end{bmatrix} = 0 $$ using SVD to find the vector of unknowns, then normalizing by its final element? In matlab these give different answers, but I don't know if the difference is in noise or implementation or if there's some theory that I'm missing.",,"['linear-algebra', 'numerical-methods', 'svd', 'homogeneous-equation']"
25,Changing local coordinates on a manifold by a diffeomorphism,Changing local coordinates on a manifold by a diffeomorphism,,"This is the set up of my problem: Let $M$ be a manifold with local coordinates $x^1,\dots, x^n$. Let $x^1,\dots,x^n,\xi_1,\dots,\xi_n$ denote the induced coordinates on $T^\ast M$. Let $f:M\to M$ be a diffeomorphism and consider the induced map $$F:T^\ast M \to T^\ast M \ , \ (p,\xi)\mapsto (f(p),(f^\ast)^{-1}(\xi)).$$ This is the inverse of the pullback map. We get a new local coordinate system on $T^\ast M$ given by $\widetilde x^i=f(x^i)$ and $\widetilde\xi_i=(f^\ast)^{-1}_x(\xi_i)$ I know the chain rule gives $$\frac{\partial}{\partial x_i}=\frac{\partial \widetilde x^j}{\partial x^i}\frac{\partial}{\partial\widetilde x^j}.$$For simplicity let $x=(x^1,\dots,x^n)$ and $\xi=(\xi_1,\dots,\xi_n)$. I know that if the $\xi$'s were coordinates on the tangent bundle we would have $$\xi_i=\frac{\partial\widetilde x^j}{\partial x^i}\widetilde\xi_j$$ but I am not sure if this is true as coordinates on the cotangent bundle. Anyway, I was told that since $(f^\ast)^{-1}(x,\xi)=\widetilde\xi$ it follows that $$\widetilde\xi_i=\frac{\partial x^j}{\partial \widetilde x^i}\xi_j$$I am hoping that someone can explain it to me. I know it has something to do with the inverse transpose of the change of basis matrix. That is, working with induced coordinates $x^1,\dots,x^n,v^1,\dots,v^n$ on the tangent bundle we get the formula $$v^j=\frac{\partial\widetilde x^i}{\partial x^j}\widetilde v^i$$so that $$\frac{\partial \widetilde v^j}{\partial v^i}=\frac{\partial \widetilde x^j}{\partial x^i}$$and we are flipping the roles of $i,j$ and the tildes.","This is the set up of my problem: Let $M$ be a manifold with local coordinates $x^1,\dots, x^n$. Let $x^1,\dots,x^n,\xi_1,\dots,\xi_n$ denote the induced coordinates on $T^\ast M$. Let $f:M\to M$ be a diffeomorphism and consider the induced map $$F:T^\ast M \to T^\ast M \ , \ (p,\xi)\mapsto (f(p),(f^\ast)^{-1}(\xi)).$$ This is the inverse of the pullback map. We get a new local coordinate system on $T^\ast M$ given by $\widetilde x^i=f(x^i)$ and $\widetilde\xi_i=(f^\ast)^{-1}_x(\xi_i)$ I know the chain rule gives $$\frac{\partial}{\partial x_i}=\frac{\partial \widetilde x^j}{\partial x^i}\frac{\partial}{\partial\widetilde x^j}.$$For simplicity let $x=(x^1,\dots,x^n)$ and $\xi=(\xi_1,\dots,\xi_n)$. I know that if the $\xi$'s were coordinates on the tangent bundle we would have $$\xi_i=\frac{\partial\widetilde x^j}{\partial x^i}\widetilde\xi_j$$ but I am not sure if this is true as coordinates on the cotangent bundle. Anyway, I was told that since $(f^\ast)^{-1}(x,\xi)=\widetilde\xi$ it follows that $$\widetilde\xi_i=\frac{\partial x^j}{\partial \widetilde x^i}\xi_j$$I am hoping that someone can explain it to me. I know it has something to do with the inverse transpose of the change of basis matrix. That is, working with induced coordinates $x^1,\dots,x^n,v^1,\dots,v^n$ on the tangent bundle we get the formula $$v^j=\frac{\partial\widetilde x^i}{\partial x^j}\widetilde v^i$$so that $$\frac{\partial \widetilde v^j}{\partial v^i}=\frac{\partial \widetilde x^j}{\partial x^i}$$and we are flipping the roles of $i,j$ and the tildes.",,"['linear-algebra', 'differential-geometry', 'manifolds', 'coordinate-systems']"
26,Confusion regarding definition of $F^\infty$ in Sheldon Axler's *Linear Algebra Done Right*,Confusion regarding definition of  in Sheldon Axler's *Linear Algebra Done Right*,F^\infty,"In his linear algebra book, Sheldon Axler defines the set of all sequences of elements of $F$ as: $$F^\infty = \{(x_1, x_2, \ldots): x_j \in F\text{ for } j = 1, 2, \ldots\}.$$ He also says: Sometimes we will use the word list without specifying its length. Remember, however, that by definition each list has a finite length that is a nonnegative integer, so that an object that looks like $(x_1,x_2, \ldots)$,   which might be said to have infinite length, is not a list. I feel like there's a contradiction here. If we went by what's said in the quote we could(?) conclude that the elements of $F^\infty$ are finite in length. Please, elaborate on this.","In his linear algebra book, Sheldon Axler defines the set of all sequences of elements of $F$ as: $$F^\infty = \{(x_1, x_2, \ldots): x_j \in F\text{ for } j = 1, 2, \ldots\}.$$ He also says: Sometimes we will use the word list without specifying its length. Remember, however, that by definition each list has a finite length that is a nonnegative integer, so that an object that looks like $(x_1,x_2, \ldots)$,   which might be said to have infinite length, is not a list. I feel like there's a contradiction here. If we went by what's said in the quote we could(?) conclude that the elements of $F^\infty$ are finite in length. Please, elaborate on this.",,['linear-algebra']
27,Area preserving transformation in a higher dimensional space is unitary.,Area preserving transformation in a higher dimensional space is unitary.,,"In $\mathbb{R}^3$, a linear operator $Q:\mathbb{R}^3 \to \mathbb{R}^3$ preserves the area of parallelograms: that is, given $x,y\in \mathbb{R}^3$, the area of a parallelogram formed by $x$ and $y$ is the same as the area of a parallelogram formed by $Qx$ and $Qy$.  Show that $Q$ must be unitary (that is, $Q^T Q=I$, where $I$ is the identity.) This is from Shilov's Linear Algebra, Chapter $8$, exercise $32$.  I am self-studying and trying to do some exercises but I'm really stumped on this one.  I don't even know how to show the hint given: to show that $Q$ must preserve right angles: if $(x,y)=0$, then $(Qx,Qy)=0$ also (where $(\dot{},\dot{})$ means dot product).","In $\mathbb{R}^3$, a linear operator $Q:\mathbb{R}^3 \to \mathbb{R}^3$ preserves the area of parallelograms: that is, given $x,y\in \mathbb{R}^3$, the area of a parallelogram formed by $x$ and $y$ is the same as the area of a parallelogram formed by $Qx$ and $Qy$.  Show that $Q$ must be unitary (that is, $Q^T Q=I$, where $I$ is the identity.) This is from Shilov's Linear Algebra, Chapter $8$, exercise $32$.  I am self-studying and trying to do some exercises but I'm really stumped on this one.  I don't even know how to show the hint given: to show that $Q$ must preserve right angles: if $(x,y)=0$, then $(Qx,Qy)=0$ also (where $(\dot{},\dot{})$ means dot product).",,"['linear-algebra', 'self-learning', 'inner-products']"
28,Isomorphism implies direct sum of Kernel and Image,Isomorphism implies direct sum of Kernel and Image,,"If  $f: U \rightarrow V$ and $g: V \rightarrow W$ are linear transformations between  vector spaces over a field $K$ such that $ g \circ f$ is an isomorphism, then $V = \operatorname{Im}f \oplus \operatorname{Ker} g$. I tried this way: since $g\circ f\colon U\to W$ is an isomorphism, I can set $h\colon W\to U$ to be its inverse; then I have  $V$ back to $V$ with $\varphi=f\circ h\circ g$. If $v\in V$, so $v=\varphi(v)+(v-\varphi(v))$,  then $V = \operatorname{Im} f \oplus \operatorname{Ker} g$. Is it correct? Does there exist a direct or more simple proof of this? Some help please, thanks for your time.","If  $f: U \rightarrow V$ and $g: V \rightarrow W$ are linear transformations between  vector spaces over a field $K$ such that $ g \circ f$ is an isomorphism, then $V = \operatorname{Im}f \oplus \operatorname{Ker} g$. I tried this way: since $g\circ f\colon U\to W$ is an isomorphism, I can set $h\colon W\to U$ to be its inverse; then I have  $V$ back to $V$ with $\varphi=f\circ h\circ g$. If $v\in V$, so $v=\varphi(v)+(v-\varphi(v))$,  then $V = \operatorname{Im} f \oplus \operatorname{Ker} g$. Is it correct? Does there exist a direct or more simple proof of this? Some help please, thanks for your time.",,"['linear-algebra', 'algebra-precalculus', 'vector-spaces', 'transformation']"
29,Rank of the differential,Rank of the differential,,"Let $f:\mathbb R^n \to \mathbb R^n$ such that $f$ maps roots of a polynomial to its coefficients. Meaning: if $(x-x_1)(x-x_2)...(x-x_n)=x^n+a_1x^{n-1}+a_2x^{n-2}+...+a_n$ then $f\begin{pmatrix} x_1 \\x_2\\x_3\\ \vdots \\x_n \end{pmatrix} =\begin{pmatrix} a_1\\a_2\\a_3 \\ \vdots \\a_n \end{pmatrix}$ Show that the rank of the differential of $f$ is equal to the number of different roots. $rank(Df)=cardinal\{x_1,...x_n\}$","Let $f:\mathbb R^n \to \mathbb R^n$ such that $f$ maps roots of a polynomial to its coefficients. Meaning: if $(x-x_1)(x-x_2)...(x-x_n)=x^n+a_1x^{n-1}+a_2x^{n-2}+...+a_n$ then $f\begin{pmatrix} x_1 \\x_2\\x_3\\ \vdots \\x_n \end{pmatrix} =\begin{pmatrix} a_1\\a_2\\a_3 \\ \vdots \\a_n \end{pmatrix}$ Show that the rank of the differential of $f$ is equal to the number of different roots. $rank(Df)=cardinal\{x_1,...x_n\}$",,"['calculus', 'linear-algebra', 'multivariable-calculus', 'derivatives', 'matrix-rank']"
30,Trace of the exterior power as a determinant,Trace of the exterior power as a determinant,,"Let $A$ be a matrix. According to Wikipedia, $$tr(\wedge^k A) = \frac{1}{k!} \det \begin{pmatrix} tr (A) & k-1 & 0 & \cdots \\ tr (A^2) & tr (A) & k-2 & \cdots \\ \cdots & \cdots & \cdots & \cdots \\ tr(A^{k-1}) & tr(A^{k-2}) & \cdots & 1 \\ tr(A^k) & tr(A^{k-1}) & \cdots & tr A \end{pmatrix}$$ Where can I find a proof of this fact?","Let $A$ be a matrix. According to Wikipedia, $$tr(\wedge^k A) = \frac{1}{k!} \det \begin{pmatrix} tr (A) & k-1 & 0 & \cdots \\ tr (A^2) & tr (A) & k-2 & \cdots \\ \cdots & \cdots & \cdots & \cdots \\ tr(A^{k-1}) & tr(A^{k-2}) & \cdots & 1 \\ tr(A^k) & tr(A^{k-1}) & \cdots & tr A \end{pmatrix}$$ Where can I find a proof of this fact?",,"['linear-algebra', 'reference-request', 'determinant', 'multilinear-algebra', 'trace']"
31,Prove positive definiteness,Prove positive definiteness,,"I want to prove that the matrix $$\begin{pmatrix} 1   &\cfrac{1}{2} &\cfrac{1}{3} &\cdots &\cfrac{1}{n} \\ \cfrac{1}{2} &\cfrac{1}{3} &\cfrac{1}{4} &\cdots &\cfrac{1}{n+1} \\ &\vdots &&\ddots &\vdots \\ \cfrac{1}{n} &\cfrac{1}{n+1} &\cfrac{1}{n+2} &\cdots &\cfrac{1}{2n-1} \end{pmatrix}$$ is positive definite. Using mathematical induction, I only need to show that its determinant is positive. But I can't find the way out.","I want to prove that the matrix $$\begin{pmatrix} 1   &\cfrac{1}{2} &\cfrac{1}{3} &\cdots &\cfrac{1}{n} \\ \cfrac{1}{2} &\cfrac{1}{3} &\cfrac{1}{4} &\cdots &\cfrac{1}{n+1} \\ &\vdots &&\ddots &\vdots \\ \cfrac{1}{n} &\cfrac{1}{n+1} &\cfrac{1}{n+2} &\cdots &\cfrac{1}{2n-1} \end{pmatrix}$$ is positive definite. Using mathematical induction, I only need to show that its determinant is positive. But I can't find the way out.",,"['linear-algebra', 'matrices', 'determinant']"
32,Geometric intuition behind subspaces in $\mathbb C^n$,Geometric intuition behind subspaces in,\mathbb C^n,"While learning elementary linear algebra one develops a great deal of geometric intuition in $\mathbb R^n$. It helps to see the forest for the trees and leads through proofs. After meeting complexification , I've realized that my old intuition doesn't seem to work in $\mathbb C^n$. For example, if $U$ is a subspace of $\mathbb C^n$, what is $\bar U$? In what relation is it to $U$? Etc. Is there a handy way you think about $\mathbb C^n$?","While learning elementary linear algebra one develops a great deal of geometric intuition in $\mathbb R^n$. It helps to see the forest for the trees and leads through proofs. After meeting complexification , I've realized that my old intuition doesn't seem to work in $\mathbb C^n$. For example, if $U$ is a subspace of $\mathbb C^n$, what is $\bar U$? In what relation is it to $U$? Etc. Is there a handy way you think about $\mathbb C^n$?",,"['linear-algebra', 'vector-spaces', 'complex-numbers', 'intuition', 'visualization']"
33,Span of union of subspaces,Span of union of subspaces,,"I was reading a proof in my linear algebra notes, and it says: Let $U, W$ be subspaces of $\mathbb{R}^3$ , $U = \text{span}\{(1,0,1), (0,1,1)\}$ , $W = \text{span}\{(1,1,0), (0,1,1)\}$ Then it goes on to say $\text{span}(U \cup W) = \text{span} \{ (1,0,1), (0,1,1), (1,1,0) \}$ Why does the span of the union of $U$ and $W$ equal the span of the union of the sets that span $U$ and $W$ ?","I was reading a proof in my linear algebra notes, and it says: Let be subspaces of , , Then it goes on to say Why does the span of the union of and equal the span of the union of the sets that span and ?","U, W \mathbb{R}^3 U = \text{span}\{(1,0,1), (0,1,1)\} W = \text{span}\{(1,1,0), (0,1,1)\} \text{span}(U \cup W) = \text{span} \{ (1,0,1), (0,1,1), (1,1,0) \} U W U W","['linear-algebra', 'vector-spaces']"
34,Spectral decomposition of normal operator,Spectral decomposition of normal operator,,"Define $T$ from $L_{2}(R)$ into itself by $T(f)(t)=f(t+1)$. Show that $T$ is normal and finds its spectral decomposition. I've shown that $f$ is normal (in fact it's unitary) but how do I find its spectral decomposition? I know the decomposition is of the form $\int_{{\sigma}(T)} {\lambda}\,dE=T$ so I have presumably have to find the spectrum and the spectral measure. Thanks","Define $T$ from $L_{2}(R)$ into itself by $T(f)(t)=f(t+1)$. Show that $T$ is normal and finds its spectral decomposition. I've shown that $f$ is normal (in fact it's unitary) but how do I find its spectral decomposition? I know the decomposition is of the form $\int_{{\sigma}(T)} {\lambda}\,dE=T$ so I have presumably have to find the spectrum and the spectral measure. Thanks",,"['linear-algebra', 'functional-analysis']"
35,Construct a rational matrix $A$ s.t. $A^m = I$,Construct a rational matrix  s.t.,A A^m = I,"Let $K$ be a field of either $\mathbb{C}$, $\mathbb{R}$ or $\mathbb{Q}$, Let $V$ be a $n$ dimensional vector space over $K$. I want to construct a matrix $A \in GL(V)$ s.t. $A^m = I$ for some $m$ and $A^k \neq I$ for $ 0 < k < m$ (In other words, I want to construct an element $g$ of a finite group that has order $m$ and represent it in $GL(V)$) If $K = \mathbb{C}$, I can construct $A$ as a diagonal matrix with values from the unity of $\mathbb{C}$, i.e. $\{ z \in \mathbb{C}: z^m = 1 \}$ then, I will get $A^m = I~~~\forall m \in \mathbb{N}$ If $K = \mathbb{R}$, I can construct $A$ as a matrix with block like: $ \begin{bmatrix} R_m & 0 \\ 0 & I_{n-2}  \end{bmatrix} $ where $R_m = \begin{bmatrix} \cos ( \frac{2 \pi}{m} ) & -\sin( \frac{2 \pi}{m} ) \\ \sin ( \frac{2 \pi}{m} ) & \cos( \frac{2 \pi}{m} ) \\ \end{bmatrix} $ Then, I can also get $A^m = I~~~\forall m \in \mathbb{N}$ However, if $K = \mathbb{Q}$, the problem is different, I don't think I can construct $A$ for every $m$. If I think the problem as a cyclic subgroup $C_m$ of symmetric group $S_n$, and then I can represent the element of $C_n$ by permuting the rows of a identity matrix, e.g. For $n=3$, let $A = \begin{bmatrix} 0 & 0 & 1\\ 1 & 0 & 0\\ 0 & 1 & 0  \end{bmatrix}$, then $A^3 = I$, but $A^k \neq I$ for $0 < k < 3$ I can do the same thing for all $m \leq n$ by fixing $n-m$ rows of the identity matrix. How about if $m > n$? By adding odd numbers of minus sign to the rows of the identity matrix, like: For $n=3, m=6$, let $A = \begin{bmatrix} 0 & 0 & -1\\ 1 & 0 & 0\\ 0 & 1 & 0  \end{bmatrix}$, then $A^6 = I$, but $A^k \neq I$ for $0 < k < 6$ I think I can construct up to $m \leq 2n$ by using the above method. (But I think some $m$ is missed, like $m=5$ for $n=3$) Is it possible to construct $A$ for $m > 2n$? (In other words, is there any faithful linear representation of the cyclic group $C_m$, $m > 2n$ on $GL(n,\mathbb{Q})$?) I know that there is a bound of the order of an element $h$ from $S_n$ by Landau's function $g(n)$. By permuting the rows of an identity matrix and adding sign, the maximum order I can represent should be bounded by $2g(n)$, right? For $m$ between $n < m \leq 2g(n)$, are there any positive integer $m$ that I cannot construct $A$ s.t. $A^m = I$ and $A^k \neq I$ for $0 < k < m$? What are they?","Let $K$ be a field of either $\mathbb{C}$, $\mathbb{R}$ or $\mathbb{Q}$, Let $V$ be a $n$ dimensional vector space over $K$. I want to construct a matrix $A \in GL(V)$ s.t. $A^m = I$ for some $m$ and $A^k \neq I$ for $ 0 < k < m$ (In other words, I want to construct an element $g$ of a finite group that has order $m$ and represent it in $GL(V)$) If $K = \mathbb{C}$, I can construct $A$ as a diagonal matrix with values from the unity of $\mathbb{C}$, i.e. $\{ z \in \mathbb{C}: z^m = 1 \}$ then, I will get $A^m = I~~~\forall m \in \mathbb{N}$ If $K = \mathbb{R}$, I can construct $A$ as a matrix with block like: $ \begin{bmatrix} R_m & 0 \\ 0 & I_{n-2}  \end{bmatrix} $ where $R_m = \begin{bmatrix} \cos ( \frac{2 \pi}{m} ) & -\sin( \frac{2 \pi}{m} ) \\ \sin ( \frac{2 \pi}{m} ) & \cos( \frac{2 \pi}{m} ) \\ \end{bmatrix} $ Then, I can also get $A^m = I~~~\forall m \in \mathbb{N}$ However, if $K = \mathbb{Q}$, the problem is different, I don't think I can construct $A$ for every $m$. If I think the problem as a cyclic subgroup $C_m$ of symmetric group $S_n$, and then I can represent the element of $C_n$ by permuting the rows of a identity matrix, e.g. For $n=3$, let $A = \begin{bmatrix} 0 & 0 & 1\\ 1 & 0 & 0\\ 0 & 1 & 0  \end{bmatrix}$, then $A^3 = I$, but $A^k \neq I$ for $0 < k < 3$ I can do the same thing for all $m \leq n$ by fixing $n-m$ rows of the identity matrix. How about if $m > n$? By adding odd numbers of minus sign to the rows of the identity matrix, like: For $n=3, m=6$, let $A = \begin{bmatrix} 0 & 0 & -1\\ 1 & 0 & 0\\ 0 & 1 & 0  \end{bmatrix}$, then $A^6 = I$, but $A^k \neq I$ for $0 < k < 6$ I think I can construct up to $m \leq 2n$ by using the above method. (But I think some $m$ is missed, like $m=5$ for $n=3$) Is it possible to construct $A$ for $m > 2n$? (In other words, is there any faithful linear representation of the cyclic group $C_m$, $m > 2n$ on $GL(n,\mathbb{Q})$?) I know that there is a bound of the order of an element $h$ from $S_n$ by Landau's function $g(n)$. By permuting the rows of an identity matrix and adding sign, the maximum order I can represent should be bounded by $2g(n)$, right? For $m$ between $n < m \leq 2g(n)$, are there any positive integer $m$ that I cannot construct $A$ s.t. $A^m = I$ and $A^k \neq I$ for $0 < k < m$? What are they?",,"['linear-algebra', 'representation-theory']"
36,Lattice in a vector space of dim 2 over a valuated field.,Lattice in a vector space of dim 2 over a valuated field.,,"I'm reading ""Arbres, amalgames et SL2"" of J.P. Serre, and something is not clear to me, but is to him :) Let $k$ be a field, with a discrete valuation $v$, ie a group epimorphism $v:k^\ast \to \mathbb{Z}$ such that $\forall x,y \in k, v(x+y) \leq min(v(x),v(y))$ and $v(0)=\infty.$ Let $A$ be the valuation ring $\lbrace x \in k, v(x) \geq0 \rbrace$, pick $\pi \in k$ such that $v(\pi)=1$. Let $V$ a two dimensional vector space over $k$. A lattice in $V$ is a sub-$A$-module finitely generated who spans $V$ as a $k$ vector space, he's then free of rank 1. Serre says that if $L_1, L_2$ are two lattices, there exists a $A$-base $\lbrace e_1,e_2\rbrace$ of $L$, and integers $a,b$ such that $\lbrace\pi^a e_1,\pi^be_2\rbrace$ is a $A$-base of $L_2$ . Comes from the theory of elementary divisors, he says. Can someone explain me something clear about that? Seems very intuitive, but I need somethinga bit formal. Thanks a lot!","I'm reading ""Arbres, amalgames et SL2"" of J.P. Serre, and something is not clear to me, but is to him :) Let $k$ be a field, with a discrete valuation $v$, ie a group epimorphism $v:k^\ast \to \mathbb{Z}$ such that $\forall x,y \in k, v(x+y) \leq min(v(x),v(y))$ and $v(0)=\infty.$ Let $A$ be the valuation ring $\lbrace x \in k, v(x) \geq0 \rbrace$, pick $\pi \in k$ such that $v(\pi)=1$. Let $V$ a two dimensional vector space over $k$. A lattice in $V$ is a sub-$A$-module finitely generated who spans $V$ as a $k$ vector space, he's then free of rank 1. Serre says that if $L_1, L_2$ are two lattices, there exists a $A$-base $\lbrace e_1,e_2\rbrace$ of $L$, and integers $a,b$ such that $\lbrace\pi^a e_1,\pi^be_2\rbrace$ is a $A$-base of $L_2$ . Comes from the theory of elementary divisors, he says. Can someone explain me something clear about that? Seems very intuitive, but I need somethinga bit formal. Thanks a lot!",,"['linear-algebra', 'number-theory', 'field-theory', 'integer-lattices', 'valuation-theory']"
37,Converting sum of infinity norm and L1 norm to linear programming,Converting sum of infinity norm and L1 norm to linear programming,,"So I'm trying to convert this minimization problem, min $\parallel Ax-y \parallel_{\infty}$ + $\parallel x \parallel_{1}$ where $A$ is $m$ by $n$ , $y$ is $m$ by $1$ and $x$ is $n$ by $1$ . into a linear program. My attempted solution is to rewrite it as follows, $$\min\,\, t + \textbf{1}^T z : t \in R, z \in R^n $$ subject to $$z \geq x$$ $$-z \leq x$$ $$\textbf{1}t \geq Ax - y$$ $$-\textbf{1}t \leq Ax - y$$ However, I'm having doubts about this solution since it sort of ignores the relationship between $z$ and $t$ and my hunch is to introduce some sort of constraint to capture that relationship. It'd be great if someone could confirm my answer or if I'm wrong, at least point me in the right direction?","So I'm trying to convert this minimization problem, min + where is by , is by and is by . into a linear program. My attempted solution is to rewrite it as follows, subject to However, I'm having doubts about this solution since it sort of ignores the relationship between and and my hunch is to introduce some sort of constraint to capture that relationship. It'd be great if someone could confirm my answer or if I'm wrong, at least point me in the right direction?","\parallel Ax-y \parallel_{\infty} \parallel x \parallel_{1} A m n y m 1 x n 1 \min\,\, t + \textbf{1}^T z : t \in R, z \in R^n  z \geq x -z \leq x \textbf{1}t \geq Ax - y -\textbf{1}t \leq Ax - y z t","['linear-algebra', 'optimization', 'normed-spaces', 'linear-programming']"
38,Show a matrix M is invertible if and only if the eigenvalues of A are all distinct,Show a matrix M is invertible if and only if the eigenvalues of A are all distinct,,"Given a matrix $A \in M_n(\mathbb{C})$, define a matrix $$M=\begin{pmatrix}n&tr({A})&tr({A^2})&\cdots&tr(A^{n-1})\\tr({A}) &tr({A^2})&tr({A^3})&\cdots&tr({A}^n)\\ \vdots&\vdots&\vdots&\cdots&\vdots\\ tr({A}^{n-2}) &tr({A^{n-1}})&tr({A^{n}})&\cdots&tr({A}^{2n-1})\\ tr({A}^{n-1}) &tr({A^n})&tr({A^{n+1}})&\cdots&tr({A}^{2n-2})\end{pmatrix}$$ Can we show that $M$ is invertible if and only if the eigenvalues of $A$ are all distinct? I have a feeling that this relates to the Vandermonde matrix. I also have some work using the linearity of trace and the Cayley-Hamilton theorem. Do we have any other thoughts as to how to approach this problem? Note that the matrix is complex, so we do not know if it is diagonalizable.","Given a matrix $A \in M_n(\mathbb{C})$, define a matrix $$M=\begin{pmatrix}n&tr({A})&tr({A^2})&\cdots&tr(A^{n-1})\\tr({A}) &tr({A^2})&tr({A^3})&\cdots&tr({A}^n)\\ \vdots&\vdots&\vdots&\cdots&\vdots\\ tr({A}^{n-2}) &tr({A^{n-1}})&tr({A^{n}})&\cdots&tr({A}^{2n-1})\\ tr({A}^{n-1}) &tr({A^n})&tr({A^{n+1}})&\cdots&tr({A}^{2n-2})\end{pmatrix}$$ Can we show that $M$ is invertible if and only if the eigenvalues of $A$ are all distinct? I have a feeling that this relates to the Vandermonde matrix. I also have some work using the linearity of trace and the Cayley-Hamilton theorem. Do we have any other thoughts as to how to approach this problem? Note that the matrix is complex, so we do not know if it is diagonalizable.",,"['linear-algebra', 'matrices']"
39,Finding an integer orthogonal basis,Finding an integer orthogonal basis,,Say I have some non-orthogonal basis of some vector space that only have integer elements. Is it possible to find an orthogonal basis consisting of basis vectors with integer elements?,Say I have some non-orthogonal basis of some vector space that only have integer elements. Is it possible to find an orthogonal basis consisting of basis vectors with integer elements?,,['linear-algebra']
40,How to convert a permutation to permutation polynomial?,How to convert a permutation to permutation polynomial?,,"Let $\mathbf{F}_q$ be the finite field with $q$ elements, where $q$ is a prime power. A permutation on $\mathbf{F}_q$ is a bijection from $\mathbf{F}_q$ to itself. Let $\mathbf{F}_q$ be the ring of polynomials in a single indeterminate $x$ over $\mathbf{F}_q$ . A polynomial $f \in \mathbf{F}_q[x]$ is called a permutation polynomial of $\mathbf{F}_q$ if it induces a one-to-one map from $\mathbf{F}_q$ to itself. It is well known that every permutation on $\mathbf{F}_q$ can be expressed as a permutation polynomial over $\mathbf{F}_q$ . Problem 1: How to convert a permutation to a permutation polynomial? Please note that Lagrange polynomial as polynomial interpolation is possible. Any other method? Problem 2: as problem (1) but with $\mathbf{F}_q$ and $q$ composite (not a prime).","Let be the finite field with elements, where is a prime power. A permutation on is a bijection from to itself. Let be the ring of polynomials in a single indeterminate over . A polynomial is called a permutation polynomial of if it induces a one-to-one map from to itself. It is well known that every permutation on can be expressed as a permutation polynomial over . Problem 1: How to convert a permutation to a permutation polynomial? Please note that Lagrange polynomial as polynomial interpolation is possible. Any other method? Problem 2: as problem (1) but with and composite (not a prime).",\mathbf{F}_q q q \mathbf{F}_q \mathbf{F}_q \mathbf{F}_q x \mathbf{F}_q f \in \mathbf{F}_q[x] \mathbf{F}_q \mathbf{F}_q \mathbf{F}_q \mathbf{F}_q \mathbf{F}_q q,"['linear-algebra', 'polynomials', 'permutations']"
41,Sesquilinear Forms: Reals,Sesquilinear Forms: Reals,,"Given a real Hilbert space $\mathcal{H}$. Consider symmetric forms: $$s:\mathcal{H}\times\mathcal{H}\to\mathbb{R}:\quad s(\psi,\varphi)=s(\varphi,\psi)$$ By polarization one obtains: $$s(\varphi,\psi)=\frac{1}{4}\{s(\varphi+\psi,\varphi+\psi)-s(\varphi-\psi,\varphi-\psi)\}$$ Consider arbitrary forms: $$s:\mathcal{H}\times\mathcal{H}\to\mathbb{R}:\quad s(\psi,\varphi)\neq s(\varphi,\psi)$$ Can one reconstruct them?","Given a real Hilbert space $\mathcal{H}$. Consider symmetric forms: $$s:\mathcal{H}\times\mathcal{H}\to\mathbb{R}:\quad s(\psi,\varphi)=s(\varphi,\psi)$$ By polarization one obtains: $$s(\varphi,\psi)=\frac{1}{4}\{s(\varphi+\psi,\varphi+\psi)-s(\varphi-\psi,\varphi-\psi)\}$$ Consider arbitrary forms: $$s:\mathcal{H}\times\mathcal{H}\to\mathbb{R}:\quad s(\psi,\varphi)\neq s(\varphi,\psi)$$ Can one reconstruct them?",,"['linear-algebra', 'functional-analysis', 'vector-spaces', 'hilbert-spaces', 'sesquilinear-forms']"
42,Looping over $k$-element subsets by switching elements,Looping over -element subsets by switching elements,k,"I would like to iterate over the $k$-element subsets of $\{1,2, \dots, n\}$ in a natural way by switching elements. Two subsets $v,w$ are adjacent if $|v \cap w| = k-1$ or equivalently if their symmetric difference is $|v \triangle w| = 2$. Is it possible to loop over all $k$-element subsets so that each one is adjacent to the last?  This would be a Hamiltonian circuit in the Johnson Graph (see Chapter 7 in this pdf notes)","I would like to iterate over the $k$-element subsets of $\{1,2, \dots, n\}$ in a natural way by switching elements. Two subsets $v,w$ are adjacent if $|v \cap w| = k-1$ or equivalently if their symmetric difference is $|v \triangle w| = 2$. Is it possible to loop over all $k$-element subsets so that each one is adjacent to the last?  This would be a Hamiltonian circuit in the Johnson Graph (see Chapter 7 in this pdf notes)",,"['linear-algebra', 'combinatorics']"
43,Finding a basis to a vector space,Finding a basis to a vector space,,"Let $ W = \left \{\mathbf{x} = \begin{pmatrix} x_1 \\x_2 \\x_3 \end{pmatrix} : x_1 + x_2 + x_3 = 0  \right\}$ and find a basis for $W$ I don't really know how to do it by guess work so I tried this method: Solve $x_1 + x_2 + x_3 = 0$ to row echelon form (which it already is in) and so we get the solution $ \begin{pmatrix} x_1 \\x_2 \\x_3 \end{pmatrix} = \begin{pmatrix} -x_2 -x_3 \\x_2 \\x_3 \end{pmatrix}$ then use a simple method to find the matrix, let $x_2 = 1$ and $x_3 = 0$ which gives us $\begin{pmatrix} -1 \\1 \\0 \end{pmatrix}$ and let $x_3 = 1$ and $x_2 = 0$ giving $\begin{pmatrix} -1 \\ 0 \\1 \end{pmatrix}$ so the basis is $\left \{\begin{pmatrix} -1 \\1 \\0 \end{pmatrix}, \begin{pmatrix} -1 \\ 0 \\1 \end{pmatrix} \right\}$ Is this a valid method as I really don't like guess work (which my teacher said for us to do). I have tested and it is a basis for the vector space $W$ thanks","Let $ W = \left \{\mathbf{x} = \begin{pmatrix} x_1 \\x_2 \\x_3 \end{pmatrix} : x_1 + x_2 + x_3 = 0  \right\}$ and find a basis for $W$ I don't really know how to do it by guess work so I tried this method: Solve $x_1 + x_2 + x_3 = 0$ to row echelon form (which it already is in) and so we get the solution $ \begin{pmatrix} x_1 \\x_2 \\x_3 \end{pmatrix} = \begin{pmatrix} -x_2 -x_3 \\x_2 \\x_3 \end{pmatrix}$ then use a simple method to find the matrix, let $x_2 = 1$ and $x_3 = 0$ which gives us $\begin{pmatrix} -1 \\1 \\0 \end{pmatrix}$ and let $x_3 = 1$ and $x_2 = 0$ giving $\begin{pmatrix} -1 \\ 0 \\1 \end{pmatrix}$ so the basis is $\left \{\begin{pmatrix} -1 \\1 \\0 \end{pmatrix}, \begin{pmatrix} -1 \\ 0 \\1 \end{pmatrix} \right\}$ Is this a valid method as I really don't like guess work (which my teacher said for us to do). I have tested and it is a basis for the vector space $W$ thanks",,['linear-algebra']
44,Is $U = \left\{A \in \mathbb{M}^ {n \times n}(\mathbb{R} ): \ker{A} \cap \text{Im}A = \{\vec{0}\} \right\}$ a vector space?,Is  a vector space?,U = \left\{A \in \mathbb{M}^ {n \times n}(\mathbb{R} ): \ker{A} \cap \text{Im}A = \{\vec{0}\} \right\},"I recently tried to prove or reject the conjecture that the set of matrices $$U = \left\{A \in \mathbb{M}^ {n \times n}(\mathbb{R} ): \ker{A} \cap \text{Im}A = \{\vec{0}\} \right\}$$ is a linear subspace of $\mathbb{M}^{n \times n}(\mathbb{R})$. I didn't find it hard to prove that the set is multiplicative: Assume that $A \in U$. We know that $\ker{0}\cap \text{Im}0 = \vec{0}$. So in the case that $\lambda=0$ we are done. Suppose that $\lambda \neq 0$. It follows easily that $\ker{A}=\ker{\lambda A}$ and that $\text{Im}A = \text{Im}{\lambda A}$. I thought additivity was the hardest part. I don't have a good intition when it comes to "" adding"" linear maps. Can you help me with this?","I recently tried to prove or reject the conjecture that the set of matrices $$U = \left\{A \in \mathbb{M}^ {n \times n}(\mathbb{R} ): \ker{A} \cap \text{Im}A = \{\vec{0}\} \right\}$$ is a linear subspace of $\mathbb{M}^{n \times n}(\mathbb{R})$. I didn't find it hard to prove that the set is multiplicative: Assume that $A \in U$. We know that $\ker{0}\cap \text{Im}0 = \vec{0}$. So in the case that $\lambda=0$ we are done. Suppose that $\lambda \neq 0$. It follows easily that $\ker{A}=\ker{\lambda A}$ and that $\text{Im}A = \text{Im}{\lambda A}$. I thought additivity was the hardest part. I don't have a good intition when it comes to "" adding"" linear maps. Can you help me with this?",,"['linear-algebra', 'matrices']"
45,Is this function surjective; infinite linear combination.,Is this function surjective; infinite linear combination.,,"Let $A(t):[0,T] \rightarrow \mathbb{R^{n\times m}}$ be continuous function. Let $$U = \text{span}\left( \bigcup_{t\in [0,T]} \text{span}(A(t)) \right)$$   Define function $f(u): L^\infty ([0,T],\mathbb{R}^m) \rightarrow U$ by:   $$f(u) = \int_0^T A(t)u(t) dt$$   Is this function surjective? first idea: If $u$ would be allowed to be delta function, than for given $v\in U$ I would take finite number of times $t_1,\dots,t_n$ and define $u$ as $u(t) = \sum_i \delta_{t_i}(t) u_i$, that $v = \sum_i A(t_i) u_i$. Since I'm not allowed to take delta function, I took only approximations of them. Than I can do only $f(u) =  v \pm \epsilon$. Next I find $u_1$ that $f(u_1) = v-f(u) \pm \epsilon_1$. And I can continue like this. So I get $f(u)+f(u_1)+\dots + f(u_n) = v \pm \epsilon_n$. The problem is that I cant show that $u + \sum_n u_n$ converge. second idea: Approximate $A(t)$ by sequence of simple functions $A_n(t)$ that still $U =\text{span}\left( \bigcup_{t\in [0,T]} \text{span}(A_n(t)) \right)$. Than one can find $u_n(t)$ that $v=\int_0^T A_n(t)u_n(t) dt$. But Now I would like to take convergent subsequence of $u_n$ but I have no idea how. I guess one would have to choose $u_n$ in ""right"" way so such a convergent subsequence would even exist.","Let $A(t):[0,T] \rightarrow \mathbb{R^{n\times m}}$ be continuous function. Let $$U = \text{span}\left( \bigcup_{t\in [0,T]} \text{span}(A(t)) \right)$$   Define function $f(u): L^\infty ([0,T],\mathbb{R}^m) \rightarrow U$ by:   $$f(u) = \int_0^T A(t)u(t) dt$$   Is this function surjective? first idea: If $u$ would be allowed to be delta function, than for given $v\in U$ I would take finite number of times $t_1,\dots,t_n$ and define $u$ as $u(t) = \sum_i \delta_{t_i}(t) u_i$, that $v = \sum_i A(t_i) u_i$. Since I'm not allowed to take delta function, I took only approximations of them. Than I can do only $f(u) =  v \pm \epsilon$. Next I find $u_1$ that $f(u_1) = v-f(u) \pm \epsilon_1$. And I can continue like this. So I get $f(u)+f(u_1)+\dots + f(u_n) = v \pm \epsilon_n$. The problem is that I cant show that $u + \sum_n u_n$ converge. second idea: Approximate $A(t)$ by sequence of simple functions $A_n(t)$ that still $U =\text{span}\left( \bigcup_{t\in [0,T]} \text{span}(A_n(t)) \right)$. Than one can find $u_n(t)$ that $v=\int_0^T A_n(t)u_n(t) dt$. But Now I would like to take convergent subsequence of $u_n$ but I have no idea how. I guess one would have to choose $u_n$ in ""right"" way so such a convergent subsequence would even exist.",,['linear-algebra']
46,Prove that every hyperplane is a null space of a linear functional [duplicate],Prove that every hyperplane is a null space of a linear functional [duplicate],,"This question already has an answer here : How do I prove that a subspace of a vector space $X$ is the null space of some linear functional on $X$? (1 answer) Closed 8 years ago . How can we prove that every hyperplane (a subspace of dimension $n-1$) is a null space of a linear functional? I don't know how to prove this.  I tried a lot, but something is missing.","This question already has an answer here : How do I prove that a subspace of a vector space $X$ is the null space of some linear functional on $X$? (1 answer) Closed 8 years ago . How can we prove that every hyperplane (a subspace of dimension $n-1$) is a null space of a linear functional? I don't know how to prove this.  I tried a lot, but something is missing.",,['linear-algebra']
47,Relation between Algebraic multiplicities and rank of a matrix,Relation between Algebraic multiplicities and rank of a matrix,,"A is  a 6x6 matrix ,   $rank(A-3I) = 4$, the minimal polynomial of A is $(x-1)^2(x-3)^2$ I need to write the Jordan matrix options for A. How can I use the given information about the rank, what does it tell me about the eigenvalue 3 and his algebraic multiplicity in the characteristic polynomial? Can someone give me an elaborated explanation how it is connected? thank you","A is  a 6x6 matrix ,   $rank(A-3I) = 4$, the minimal polynomial of A is $(x-1)^2(x-3)^2$ I need to write the Jordan matrix options for A. How can I use the given information about the rank, what does it tell me about the eigenvalue 3 and his algebraic multiplicity in the characteristic polynomial? Can someone give me an elaborated explanation how it is connected? thank you",,"['linear-algebra', 'matrices', 'polynomials', 'eigenvalues-eigenvectors']"
48,How do I find upper triangular form of a given 3 by 3 matrix??,How do I find upper triangular form of a given 3 by 3 matrix??,,"We are asked to find an invertible matrix $P$ and an upper triangular matrix $U$ such that: $P^{-1}\begin{pmatrix} 3 & -1 & 1 \\ 2 & 0 & 0 \\ -1 & 1 & 3 \end{pmatrix}P=U$ I'm a bit stuck. I found the characteristic polynomial of the matrix, it is $(-x+2)^3$, so the only eigenvalue is 2. The problem comes with the eigenvectors. I could only find one, $\begin{pmatrix} 1 \\ 1\\0 \end{pmatrix}$ ...How do I go on from here? the zero vectors are not eigenvectors, and even if they were, the matrix $\begin{pmatrix} 1 & 0 & 0\\ 1 & 0 & 0\\0 & 0 & 0 \end{pmatrix}$ is singular so it can't be the $P$ i'm looking for...","We are asked to find an invertible matrix $P$ and an upper triangular matrix $U$ such that: $P^{-1}\begin{pmatrix} 3 & -1 & 1 \\ 2 & 0 & 0 \\ -1 & 1 & 3 \end{pmatrix}P=U$ I'm a bit stuck. I found the characteristic polynomial of the matrix, it is $(-x+2)^3$, so the only eigenvalue is 2. The problem comes with the eigenvectors. I could only find one, $\begin{pmatrix} 1 \\ 1\\0 \end{pmatrix}$ ...How do I go on from here? the zero vectors are not eigenvectors, and even if they were, the matrix $\begin{pmatrix} 1 & 0 & 0\\ 1 & 0 & 0\\0 & 0 & 0 \end{pmatrix}$ is singular so it can't be the $P$ i'm looking for...",,"['linear-algebra', 'matrices', 'matrix-decomposition']"
49,Is a Jordan block not further block diagonalizable?,Is a Jordan block not further block diagonalizable?,,"We can always find a Jordan canonical form of a matrix A. It is a block diagonal matrix. Is it true that each block cannot be reduced to a matrix with more blocks in diagonal? In other words, for a given linear operator T, is it true that ""each T-invariant subspace corresponding to a Jordan block cannot be decomposed into a direct sum of T-invariant proper subspaces of it"" ?","We can always find a Jordan canonical form of a matrix A. It is a block diagonal matrix. Is it true that each block cannot be reduced to a matrix with more blocks in diagonal? In other words, for a given linear operator T, is it true that ""each T-invariant subspace corresponding to a Jordan block cannot be decomposed into a direct sum of T-invariant proper subspaces of it"" ?",,"['linear-algebra', 'matrices', 'diagonalization', 'jordan-normal-form']"
50,No idea how to prove this property about symmetric matrices,No idea how to prove this property about symmetric matrices,,"This is from homework, so please hints only. Suppose $A$ is symmetric such that all of its eigenvalues are 1 or -1. Prove that $A$ is orthogonal. The converse is really easy, but I really have no idea how to do this. Any hints?","This is from homework, so please hints only. Suppose $A$ is symmetric such that all of its eigenvalues are 1 or -1. Prove that $A$ is orthogonal. The converse is really easy, but I really have no idea how to do this. Any hints?",,"['linear-algebra', 'orthonormal']"
51,Do I influence myself more than my neighbors?,Do I influence myself more than my neighbors?,,"Consider relations between people is defined by a weighted symmetric undirected graph $W$, and $w_{ij}$ shows amount of weight $i$ has for $j$. Assume all weights are non-negative and less than $1$ i.e. $$0\leq w_{ij}<1, \forall{i,j}$$ and symmetric $w_{ij}=w_{ji}$. We say $i$ and $j$ are friends if $w_{ij}>0$. Define the influence matrix $\Psi=[\text{I}+W]^{-1}$ (Assume it is well-defined). Is it always the case that my influence on myself is greater than my influence on my friends? $$\Psi_{i,i}>\sum_{j\in N(i)}|\Psi_{i,j}|$$ where $N(i)$ is a set of friends of $i$.","Consider relations between people is defined by a weighted symmetric undirected graph $W$, and $w_{ij}$ shows amount of weight $i$ has for $j$. Assume all weights are non-negative and less than $1$ i.e. $$0\leq w_{ij}<1, \forall{i,j}$$ and symmetric $w_{ij}=w_{ji}$. We say $i$ and $j$ are friends if $w_{ij}>0$. Define the influence matrix $\Psi=[\text{I}+W]^{-1}$ (Assume it is well-defined). Is it always the case that my influence on myself is greater than my influence on my friends? $$\Psi_{i,i}>\sum_{j\in N(i)}|\Psi_{i,j}|$$ where $N(i)$ is a set of friends of $i$.",,"['linear-algebra', 'matrices', 'graph-theory', 'intuition']"
52,Linear algebra and matrix.,Linear algebra and matrix.,,"prove or disprove : If A and B are 2 by 2 orthogonal matrices over R then A+B cannot be orthogonal. OR If S,T:R^2--->R^2 are orthogonal transformation then S+T is not an orthogonal transformation.","prove or disprove : If A and B are 2 by 2 orthogonal matrices over R then A+B cannot be orthogonal. OR If S,T:R^2--->R^2 are orthogonal transformation then S+T is not an orthogonal transformation.",,"['linear-algebra', 'matrices', 'transformation']"
53,Orthogonal projection and two subspaces,Orthogonal projection and two subspaces,,"Let $\mathcal{S}$ and $\mathcal{T}$ be two subspaces of $\mathbb{R}^n$, let $P$ be the orthogonal projection of $\mathbb{R}^n$ on $\mathcal{S}$ and let $Q$ be the orthogonal projection of $\mathbb{R}^n$ onto $\mathcal{T}$. Show that if $P$ and $Q$ commute, then $PQ$ is a projection and $PQ$ is the projection onto $\mathcal{S}\cap \mathcal{T}$. Is the converse assertion true? Suppose $PQ$ is the orthogonal projection of $\mathbb{R}^n$ onto the intersection $\mathcal{S}\cap \mathcal{T}$. Must $P$ commute with $Q$. Anybody has advice on how i should start proving this assertion?","Let $\mathcal{S}$ and $\mathcal{T}$ be two subspaces of $\mathbb{R}^n$, let $P$ be the orthogonal projection of $\mathbb{R}^n$ on $\mathcal{S}$ and let $Q$ be the orthogonal projection of $\mathbb{R}^n$ onto $\mathcal{T}$. Show that if $P$ and $Q$ commute, then $PQ$ is a projection and $PQ$ is the projection onto $\mathcal{S}\cap \mathcal{T}$. Is the converse assertion true? Suppose $PQ$ is the orthogonal projection of $\mathbb{R}^n$ onto the intersection $\mathcal{S}\cap \mathcal{T}$. Must $P$ commute with $Q$. Anybody has advice on how i should start proving this assertion?",,['linear-algebra']
54,Geometric interpretation of the addition of linear equations in general form,Geometric interpretation of the addition of linear equations in general form,,I have a very simple question: suppose I have two 2D linear equations in general form $$ a_1x + b_1y + c_1 = 0$$ $$ a_2x + b_2y + c_2 = 0$$ I'd like to know what's the (intuitive) geometric interpretation of their addition and subtraction $$ (a_1 + a_2)x + (b_1 + b_2)y + (c_1 + c_2) = 0$$ $$ (a_1 - a_2)x + (b_1 - b_2)y + (c_1 - c_2) = 0$$,I have a very simple question: suppose I have two 2D linear equations in general form $$ a_1x + b_1y + c_1 = 0$$ $$ a_2x + b_2y + c_2 = 0$$ I'd like to know what's the (intuitive) geometric interpretation of their addition and subtraction $$ (a_1 + a_2)x + (b_1 + b_2)y + (c_1 + c_2) = 0$$ $$ (a_1 - a_2)x + (b_1 - b_2)y + (c_1 - c_2) = 0$$,,"['linear-algebra', 'geometry', 'intuition']"
55,How do we find non-self-adjoint A and unitary U such that exp(iA) = U?,How do we find non-self-adjoint A and unitary U such that exp(iA) = U?,,"The following is a theorem: If $A$ is a self-adjoint matrix (i.e. $A^\dagger = A$), then $U = e^{iA}$ is a unitary matrix. This is easy to prove: $(e^{iA})^\dagger = e^{-iA^\dagger} = e^{-iA}$, with the last step a consequence of self-adjointness of $A$. Since powers of matrices commute with themselves, $e^{iA}$ commutes with $e^{-iA}$, and so we can write $e^{iA}e^{-iA}=e^{iA-iA}=e^{0}=I$, where $I$ is the identity matrix. This shows that $(e^{iA})^{-1}=(e^{iA})^\dagger$, and hence that $e^{iA}$ is unitary. This is all well and good. However, the following (the converse) is not a theorem: If $U$ is a unitary matrix, then the (a?) matrix $A$ defined by $e^{iA} = U$ is self-adjoint. I have been led to believe that the best way to disprove this is by counterexample, but I have no idea how to construct a non-self-adjoint matrix $A$ for which $e^{iA}$ is still unitary. An explicit counterexample would be great, but an approach to figuring out how to find a counterexample would be even beter.","The following is a theorem: If $A$ is a self-adjoint matrix (i.e. $A^\dagger = A$), then $U = e^{iA}$ is a unitary matrix. This is easy to prove: $(e^{iA})^\dagger = e^{-iA^\dagger} = e^{-iA}$, with the last step a consequence of self-adjointness of $A$. Since powers of matrices commute with themselves, $e^{iA}$ commutes with $e^{-iA}$, and so we can write $e^{iA}e^{-iA}=e^{iA-iA}=e^{0}=I$, where $I$ is the identity matrix. This shows that $(e^{iA})^{-1}=(e^{iA})^\dagger$, and hence that $e^{iA}$ is unitary. This is all well and good. However, the following (the converse) is not a theorem: If $U$ is a unitary matrix, then the (a?) matrix $A$ defined by $e^{iA} = U$ is self-adjoint. I have been led to believe that the best way to disprove this is by counterexample, but I have no idea how to construct a non-self-adjoint matrix $A$ for which $e^{iA}$ is still unitary. An explicit counterexample would be great, but an approach to figuring out how to find a counterexample would be even beter.",,['linear-algebra']
56,"If $A,B,C$ are nonsingular, so is $A\sin(t)+B\cos(t)+C$ for some real $t$","If  are nonsingular, so is  for some real","A,B,C A\sin(t)+B\cos(t)+C t","While trying to answer another question on this site, I found that I needed the following assertion: If $A,B,C$ are nonsingular complex matrices of the same sizes, then $A\sin(t)+B\cos(t)+C$ is nonsingular for some real number $t$. Clearly, we may assume that $A$ is the identity matrix and $B+C$ is in Jordan normal form. Then $A\sin(t)+B\cos(t)+C=tI+(B+C)+O(t^2)$. The assertion is clear if $B+C=0$. By using Schur complement, we can then prove the assertion for the case where $B+C$ is a single nilpotent Jordan block, and then for the case where $B+C$ is a nilpotent matrix, and finally for the general case. Such a proof, however, looks too tedious. Is there any concise proof? Since I intended to use this assertion as part of an elementary proof for another problem statement, the proof for this assertion is preferably elementary as well.","While trying to answer another question on this site, I found that I needed the following assertion: If $A,B,C$ are nonsingular complex matrices of the same sizes, then $A\sin(t)+B\cos(t)+C$ is nonsingular for some real number $t$. Clearly, we may assume that $A$ is the identity matrix and $B+C$ is in Jordan normal form. Then $A\sin(t)+B\cos(t)+C=tI+(B+C)+O(t^2)$. The assertion is clear if $B+C=0$. By using Schur complement, we can then prove the assertion for the case where $B+C$ is a single nilpotent Jordan block, and then for the case where $B+C$ is a nilpotent matrix, and finally for the general case. Such a proof, however, looks too tedious. Is there any concise proof? Since I intended to use this assertion as part of an elementary proof for another problem statement, the proof for this assertion is preferably elementary as well.",,"['linear-algebra', 'matrices']"
57,Properties of special rectangle (measure),Properties of special rectangle (measure),,"Let $I$ be a special rectangle in $\mathbb{R}^n$, and denote $\lambda(A)$ the measure of $A$. Prove that the following conditions are equivalent: a) $\lambda(I)=0$ b) $I^{\circ}=\emptyset$ (i.e., the interior of $I$ is empty) c) $I$ is contained in an affine subspace of $\mathbb{R}^n$ having dimension smaller than $n$. (An affine subspace is any set of the form $\{x_0+x|x\in E\}$, where $x_0\in\mathbb{R}^n$ is fixed and $E$ is a subspace of the vector space $\mathbb{R}^n$. Its dimension is equal to the dimension of $E$.) The implication $a\implies b$ isn't too bad (using the definition if $I=[a_1,b_1]\times...\times[a_n,b_n]$ then $\lambda(I)=(b_1-a_2)...(b_n-a_n)$. So we conclude $a_i=b_i$ for some $i$. And since $I^{\circ}$ is open, there can't be anything contained in it). I also see how c) makes sense (at least intuitively), but not sure how to show it formally.  Thank you.","Let $I$ be a special rectangle in $\mathbb{R}^n$, and denote $\lambda(A)$ the measure of $A$. Prove that the following conditions are equivalent: a) $\lambda(I)=0$ b) $I^{\circ}=\emptyset$ (i.e., the interior of $I$ is empty) c) $I$ is contained in an affine subspace of $\mathbb{R}^n$ having dimension smaller than $n$. (An affine subspace is any set of the form $\{x_0+x|x\in E\}$, where $x_0\in\mathbb{R}^n$ is fixed and $E$ is a subspace of the vector space $\mathbb{R}^n$. Its dimension is equal to the dimension of $E$.) The implication $a\implies b$ isn't too bad (using the definition if $I=[a_1,b_1]\times...\times[a_n,b_n]$ then $\lambda(I)=(b_1-a_2)...(b_n-a_n)$. So we conclude $a_i=b_i$ for some $i$. And since $I^{\circ}$ is open, there can't be anything contained in it). I also see how c) makes sense (at least intuitively), but not sure how to show it formally.  Thank you.",,"['real-analysis', 'linear-algebra', 'analysis', 'measure-theory', 'lebesgue-integral']"
58,Block Decomposition of a linear map on $\Lambda^2TM$,Block Decomposition of a linear map on,\Lambda^2TM,"I'm trying a exercise from Peter Petersen's book, and I did the following: Let $*$ be the Hodge star operator, I know that $\Lambda^2TM$ decompose into $+1$ and $-1$ eigenspaces $\Lambda^+TM$ and $\Lambda^-TM$ for $*$. I know that, if $e_1,e_2,e_3,e_4$ is an oriented orthonormal basis, then   $$e_1\wedge e_2\pm e_3\wedge e_4\in\Lambda^{\pm}TM$$   $$e_1\wedge e_3\pm e_4\wedge e_2\in\Lambda^{\pm}TM$$   $$e_1\wedge e_4\pm e_2\wedge e_3\in\Lambda^{\pm}TM$$ What i can't prove is: Thus, any linear map $L:\Lambda^2TM\to\Lambda^2TM$ has a block decomposition   $$\begin{bmatrix}  A&B \\   C&D  \end{bmatrix}$$ $$A:\Lambda^+TM\to\Lambda^+TM$$ $$B:\Lambda^-TM\to\Lambda^+TM$$ $$C:\Lambda^+TM\to\Lambda^-TM$$ $$D:\Lambda^-TM\to\Lambda^-TM$$","I'm trying a exercise from Peter Petersen's book, and I did the following: Let $*$ be the Hodge star operator, I know that $\Lambda^2TM$ decompose into $+1$ and $-1$ eigenspaces $\Lambda^+TM$ and $\Lambda^-TM$ for $*$. I know that, if $e_1,e_2,e_3,e_4$ is an oriented orthonormal basis, then   $$e_1\wedge e_2\pm e_3\wedge e_4\in\Lambda^{\pm}TM$$   $$e_1\wedge e_3\pm e_4\wedge e_2\in\Lambda^{\pm}TM$$   $$e_1\wedge e_4\pm e_2\wedge e_3\in\Lambda^{\pm}TM$$ What i can't prove is: Thus, any linear map $L:\Lambda^2TM\to\Lambda^2TM$ has a block decomposition   $$\begin{bmatrix}  A&B \\   C&D  \end{bmatrix}$$ $$A:\Lambda^+TM\to\Lambda^+TM$$ $$B:\Lambda^-TM\to\Lambda^+TM$$ $$C:\Lambda^+TM\to\Lambda^-TM$$ $$D:\Lambda^-TM\to\Lambda^-TM$$",,"['linear-algebra', 'riemannian-geometry']"
59,Determining the Jordan Canonical Form $18\times 18$ matrix,Determining the Jordan Canonical Form  matrix,18\times 18,"Let A be an $18\times 18$ matrix over $\mathbb{C}$ with characteristic polynomial equal to   $$ (x-1)^6(x-2)^6(x-3)^6 $$   and a minimal polynomial equal to   $$ (x-1)^4(x-2)^4(x-3)^3. $$   Assume $(A-I)$ has nullity $2$, $(A-2I)$ has nullity 3, and $(A-3I)^2$ has nullity 4. Find the Jordan canonical form of A. For $\lambda=1$, I figure the largest block is a $4\times 4$ (since the multiplicity of the root is $4$ in the minimal polynomial), and since the nullity is $2$, that there are $2$ blocks in total, thus forcing the other block to be $2\times2$. For $\lambda=2$, I figure the largest block is a $4\times4$, there are 3 blocks, thus making the others $1\times 1$ matrices. For $\lambda=3$, The largest block is a $3\times 3$ matrix. However, I am not sure how the nullity of $(A-3I)^2$ determines the nullity of $(A-3I)$. Any insight into this problem would be extremely valuable as I am trying to teach myself this process.","Let A be an $18\times 18$ matrix over $\mathbb{C}$ with characteristic polynomial equal to   $$ (x-1)^6(x-2)^6(x-3)^6 $$   and a minimal polynomial equal to   $$ (x-1)^4(x-2)^4(x-3)^3. $$   Assume $(A-I)$ has nullity $2$, $(A-2I)$ has nullity 3, and $(A-3I)^2$ has nullity 4. Find the Jordan canonical form of A. For $\lambda=1$, I figure the largest block is a $4\times 4$ (since the multiplicity of the root is $4$ in the minimal polynomial), and since the nullity is $2$, that there are $2$ blocks in total, thus forcing the other block to be $2\times2$. For $\lambda=2$, I figure the largest block is a $4\times4$, there are 3 blocks, thus making the others $1\times 1$ matrices. For $\lambda=3$, The largest block is a $3\times 3$ matrix. However, I am not sure how the nullity of $(A-3I)^2$ determines the nullity of $(A-3I)$. Any insight into this problem would be extremely valuable as I am trying to teach myself this process.",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
60,Solving commutator equation: $AX-XA=M$,Solving commutator equation:,AX-XA=M,"Consider the field square matrices $M_n(\mathbb{C})$ or $M_n(\mathbb{R})$. I wish to solve the equation $AX-XA=M$ for a given $A,M$. Obviously this is just $n^2$ linear equations and thus is trivial to actually calculate as $M_{ij}=\sum_k(A_{ik}X_{kj}-A_{kj}X_{ik})$, but I wonder if there is some closed-form equation for $X_{ij}$. Thanks!","Consider the field square matrices $M_n(\mathbb{C})$ or $M_n(\mathbb{R})$. I wish to solve the equation $AX-XA=M$ for a given $A,M$. Obviously this is just $n^2$ linear equations and thus is trivial to actually calculate as $M_{ij}=\sum_k(A_{ik}X_{kj}-A_{kj}X_{ik})$, but I wonder if there is some closed-form equation for $X_{ij}$. Thanks!",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
61,Positive semidefinite matrix problem,Positive semidefinite matrix problem,,"This is a simple question, at least, looks like. Let $x\in\mathbb{R}^n$ and consider the matrix $C$ such that $C_{ij}=|x_i|+|x_j|-|x_i-x_j|$, show that $C$ is positive semidefinitive. I could prove it for some special cases, but the general case is not coming, any help is welcome. Thanks in advance.","This is a simple question, at least, looks like. Let $x\in\mathbb{R}^n$ and consider the matrix $C$ such that $C_{ij}=|x_i|+|x_j|-|x_i-x_j|$, show that $C$ is positive semidefinitive. I could prove it for some special cases, but the general case is not coming, any help is welcome. Thanks in advance.",,"['linear-algebra', 'matrices']"
62,"If $A$ is a $3\times 3$ complex matrix such that $A^3=-I$, then $A$ has distinct eigenvalues?","If  is a  complex matrix such that , then  has distinct eigenvalues?",A 3\times 3 A^3=-I A,Let $A$ be a $3\times 3$ complex matrix such that $A^3=-I$ How to show that $A$ has distinct eigenvalues? What if i consider $A=-I?$ Isn't then -1 becoming the only eigen value?,Let $A$ be a $3\times 3$ complex matrix such that $A^3=-I$ How to show that $A$ has distinct eigenvalues? What if i consider $A=-I?$ Isn't then -1 becoming the only eigen value?,,['linear-algebra']
63,Every subspace of $\mathbb{R}^n$ is a solution space of a homogeneous system of linear equation.,Every subspace of  is a solution space of a homogeneous system of linear equation.,\mathbb{R}^n,All solution of $AX = 0$ where $A$ is a $n \times n$ matrix and $X$ is a column vector form a subspace of $\mathbb{R}^n$. All the subspaces of $\mathbb{R}^n$ are of this type. How to prove this result? Linear Algebra: solution of homogeneous system of equation Thank you.,All solution of $AX = 0$ where $A$ is a $n \times n$ matrix and $X$ is a column vector form a subspace of $\mathbb{R}^n$. All the subspaces of $\mathbb{R}^n$ are of this type. How to prove this result? Linear Algebra: solution of homogeneous system of equation Thank you.,,['linear-algebra']
64,Probability that random subspaces intersect,Probability that random subspaces intersect,,"Given an ambient space $\mathbb{R}^d$ and two randomly oriented subspaces $A,B$ with dimensions $a,b$ respectively, how can I express the probability that $A$ and $B$ intersect non-trivially?","Given an ambient space $\mathbb{R}^d$ and two randomly oriented subspaces $A,B$ with dimensions $a,b$ respectively, how can I express the probability that $A$ and $B$ intersect non-trivially?",,"['linear-algebra', 'probability']"
65,How to understand rank-nullity / dimension theorem proof?,How to understand rank-nullity / dimension theorem proof?,,"OK, I am working on proofs of the rank-nullity (otherwise in my class known as the dimension theorem). Here's a proof that my professor gave in the class. I want to be sure I understand the reasoning. So I will lay out what he had here with a less-precise layman's wording, as I want to be sure I know what I am doing. It makes the proof easier to memorize for me. So: Let V and V be vector spaces. T:V→W is linear and V is finite-dimensional and function $f \in Hom_K (V,W)$ Let dim(V) = n for some n$\in \mathbb N$ and dim(ker($f$) = $r$ dim(V) = nullity(T) + rank(T) = dim(ker($ f$ ) + dim(Im($ f$ )) in some notations (like the one in our text) this wold look like dim(V) = nullity(T) + rank(T) = dim(N(T)) + dim(R(T)) on to the proof: $ker(f) \subseteq V$. And it is a subspace. Why a subspace? Because, since the kernel of any function is the set of vectors that goes to zero, adding to those vectors another vector in V will still be in V, a will multiplying them (since they go to zero). since we let dim(V)=n all the bases (basis-es?) of V will have n elements. therefore  $  \exists$  a basis {$x_1 , x_2 , ... , x_r$} of $ ker(f)$  where r≤n. (The reason is that any basis will have an equal or lesser number of dimensions than the space it describes. ker(f) is a subspace). by the exchange lemma, which says that given any linearly independent subset  $ \exists  {y_1 , y_2 , ... y_s } \in V $ such that {$y_1 , y_2 , ... y_n $}$ \cap $ {$x_1 , x_2 ,... ,x_r$}$ = \varnothing $ the next step says that {$y_1 , y_2 , ... y_n$}$ \cup ${$x_1 , x_2 ,... ,x_r$} is a basis of V. Now, my question is if that is because the intersection of the two sets is the empty set and they are linearly independent? After that, we get to saying that {$ f(y_1), f(y_2),... f(y_n)$ } is a basis of Im($ f$ ). But I am not sure why that is. He then says we can claim the following:  $  λ_1 f(y_1) + λ_2 f(y_2)+... +λ_s f(y_s)= 0 $ for some $λ_1, λ_2, ..., λ_s $  \in $  K so taking  $$f \Big( \sum_{i=1}^s x_i y_i \Big) = 0 $$ we can make that into  $$ \Big[ \Big( \sum_{i=1}^s \lambda_i f(y_i) \Big) \Big] =   \sum_{i=1}^s \lambda_i y_i  \in ker(f)$$ That step I am a bit fuzzy on the reasoning. IIUC, it's just saying that taking the sum of f using the union of x an y sets equals zero (its just f(x,y) ) and the summation of the product of λ and all the f(y) terms is the same as the sum of all the λy terms and they are all in the kernel of f. But I wanted to be sure. He then said that the above implies that there exists some set of scalars, $α_1, α_2, ... α_s \in$  K s.t. $$ \sum_{i=1}^s \lambda_i y_i = \sum_{j=1}^r y_i x_j$$  and that further implies $$\sum_{j=1}^r \alpha_j x_j - \sum_{i=1}^s \alpha_i x_i  = 0 $$ which implies $α_j, λi = 0$ for all 1≤j≤r and 1≤i≤s. and that further implies that the set {$f(y_1), f(y_2), ... ,f(y_s )$} is linearly independent. Then he says: for all z in the Im(f) there exists x$ \in V$ s.t. $z=f(x)$ (this seems obvious at one level but I felt like it was just sleight of hand). then $z = f \Big(\sum_{j=1}^r \alpha_j x_j - \sum_{i=1}^s \lambda_i y_i \Big) = \sum_{j=1}^r \alpha_j f(x_i) + \sum_{i=1}^s x_i f(y_i)= 0 + \sum_{i=1}^s x_i f(y_i)$ and then he says dim(V) = r + s = dim(ker(f)) + dim(Im(f)) its the last few steps I can't seem to justify in my head. Any help would be appreciated (and seeing if I copied this wrong from the board).","OK, I am working on proofs of the rank-nullity (otherwise in my class known as the dimension theorem). Here's a proof that my professor gave in the class. I want to be sure I understand the reasoning. So I will lay out what he had here with a less-precise layman's wording, as I want to be sure I know what I am doing. It makes the proof easier to memorize for me. So: Let V and V be vector spaces. T:V→W is linear and V is finite-dimensional and function $f \in Hom_K (V,W)$ Let dim(V) = n for some n$\in \mathbb N$ and dim(ker($f$) = $r$ dim(V) = nullity(T) + rank(T) = dim(ker($ f$ ) + dim(Im($ f$ )) in some notations (like the one in our text) this wold look like dim(V) = nullity(T) + rank(T) = dim(N(T)) + dim(R(T)) on to the proof: $ker(f) \subseteq V$. And it is a subspace. Why a subspace? Because, since the kernel of any function is the set of vectors that goes to zero, adding to those vectors another vector in V will still be in V, a will multiplying them (since they go to zero). since we let dim(V)=n all the bases (basis-es?) of V will have n elements. therefore  $  \exists$  a basis {$x_1 , x_2 , ... , x_r$} of $ ker(f)$  where r≤n. (The reason is that any basis will have an equal or lesser number of dimensions than the space it describes. ker(f) is a subspace). by the exchange lemma, which says that given any linearly independent subset  $ \exists  {y_1 , y_2 , ... y_s } \in V $ such that {$y_1 , y_2 , ... y_n $}$ \cap $ {$x_1 , x_2 ,... ,x_r$}$ = \varnothing $ the next step says that {$y_1 , y_2 , ... y_n$}$ \cup ${$x_1 , x_2 ,... ,x_r$} is a basis of V. Now, my question is if that is because the intersection of the two sets is the empty set and they are linearly independent? After that, we get to saying that {$ f(y_1), f(y_2),... f(y_n)$ } is a basis of Im($ f$ ). But I am not sure why that is. He then says we can claim the following:  $  λ_1 f(y_1) + λ_2 f(y_2)+... +λ_s f(y_s)= 0 $ for some $λ_1, λ_2, ..., λ_s $  \in $  K so taking  $$f \Big( \sum_{i=1}^s x_i y_i \Big) = 0 $$ we can make that into  $$ \Big[ \Big( \sum_{i=1}^s \lambda_i f(y_i) \Big) \Big] =   \sum_{i=1}^s \lambda_i y_i  \in ker(f)$$ That step I am a bit fuzzy on the reasoning. IIUC, it's just saying that taking the sum of f using the union of x an y sets equals zero (its just f(x,y) ) and the summation of the product of λ and all the f(y) terms is the same as the sum of all the λy terms and they are all in the kernel of f. But I wanted to be sure. He then said that the above implies that there exists some set of scalars, $α_1, α_2, ... α_s \in$  K s.t. $$ \sum_{i=1}^s \lambda_i y_i = \sum_{j=1}^r y_i x_j$$  and that further implies $$\sum_{j=1}^r \alpha_j x_j - \sum_{i=1}^s \alpha_i x_i  = 0 $$ which implies $α_j, λi = 0$ for all 1≤j≤r and 1≤i≤s. and that further implies that the set {$f(y_1), f(y_2), ... ,f(y_s )$} is linearly independent. Then he says: for all z in the Im(f) there exists x$ \in V$ s.t. $z=f(x)$ (this seems obvious at one level but I felt like it was just sleight of hand). then $z = f \Big(\sum_{j=1}^r \alpha_j x_j - \sum_{i=1}^s \lambda_i y_i \Big) = \sum_{j=1}^r \alpha_j f(x_i) + \sum_{i=1}^s x_i f(y_i)= 0 + \sum_{i=1}^s x_i f(y_i)$ and then he says dim(V) = r + s = dim(ker(f)) + dim(Im(f)) its the last few steps I can't seem to justify in my head. Any help would be appreciated (and seeing if I copied this wrong from the board).",,"['linear-algebra', 'vector-spaces']"
66,compute pca with this useful trick,compute pca with this useful trick,,"A is matrix (m rows, n cols), each row is an object, and each cols is a feature (a dimension). Typically, I compute the pca based on the covariance matrix, that is A'A , A' is the transposed matrix of A . Today I read a book which presents a useful trick to compute pca , that is if n >> m , then we can compute the eigenvectors of the matrix AA' , which might save a lot of memory, here is the code from the book: def pca(X):     """"""     Principal Component Analysis     input: X, matrix with training data stored as flattened arrays in rows     return: projection matrix (with important dimensions first), variance     and mean.     """"""     # get dimensions     num_data,dim = X.shape     # center data     mean_X = X.mean(axis=0)     X = X - mean_X      # PCA - compact trick used     M = dot(X,X.T)        # covariance matrix, AA', not the A'A like usual     e,EV = linalg.eigh(M) # compute eigenvalues and eigenvectors     tmp = dot(X.T,EV).T   # this is the compact trick     V = tmp[::-1]         # reverse since last eigenvectors are the ones we want     S = sqrt(e)[::-1]     # reverse since eigenvalues are in increasing order     for i in range(V.shape[1]):         V[:,i] /= S       # What for?      # return the projection matrix, the variance and the mean     return V,S,mean_X Now I understand the algebra behind this useful trick, but there is something confuses me, that is the for-loop , why divide V by S ? Normolize the V to unit-length?","A is matrix (m rows, n cols), each row is an object, and each cols is a feature (a dimension). Typically, I compute the pca based on the covariance matrix, that is A'A , A' is the transposed matrix of A . Today I read a book which presents a useful trick to compute pca , that is if n >> m , then we can compute the eigenvectors of the matrix AA' , which might save a lot of memory, here is the code from the book: def pca(X):     """"""     Principal Component Analysis     input: X, matrix with training data stored as flattened arrays in rows     return: projection matrix (with important dimensions first), variance     and mean.     """"""     # get dimensions     num_data,dim = X.shape     # center data     mean_X = X.mean(axis=0)     X = X - mean_X      # PCA - compact trick used     M = dot(X,X.T)        # covariance matrix, AA', not the A'A like usual     e,EV = linalg.eigh(M) # compute eigenvalues and eigenvectors     tmp = dot(X.T,EV).T   # this is the compact trick     V = tmp[::-1]         # reverse since last eigenvectors are the ones we want     S = sqrt(e)[::-1]     # reverse since eigenvalues are in increasing order     for i in range(V.shape[1]):         V[:,i] /= S       # What for?      # return the projection matrix, the variance and the mean     return V,S,mean_X Now I understand the algebra behind this useful trick, but there is something confuses me, that is the for-loop , why divide V by S ? Normolize the V to unit-length?",,"['linear-algebra', 'statistics']"
67,Where to start when learning math (again)? [closed],Where to start when learning math (again)? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 7 years ago . Improve this question I have a few questions I hope you can help me answer. First, I'll introduce myself. I'm a finance undergraduate student in Australia, but I'm originally from Norway. Throughout school I always loved math, but I ended up studying finance. The last year or so I have started to realise that I should have done Computer Science or Engineering in stead, as I would like to see myself in a quant role when I finish studying. Even so, I've decided to finish this finance degree. The last month or so I've started programming in C++, and refreshing up on my maths knowledge (Khan Academy, algebra, precalculus). My problem is I don't know where to go from here, and what order I should be learning the different branches of math. I've picked up a book on linear algebra today; ""Linear algebra and its applications"" by Gilbert Strang. Do you think this is an alright place to start? Or do I start with basic calculus? And where can I go next? Differential equations? I hope some of you can help me in the right direction. Thanks.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 7 years ago . Improve this question I have a few questions I hope you can help me answer. First, I'll introduce myself. I'm a finance undergraduate student in Australia, but I'm originally from Norway. Throughout school I always loved math, but I ended up studying finance. The last year or so I have started to realise that I should have done Computer Science or Engineering in stead, as I would like to see myself in a quant role when I finish studying. Even so, I've decided to finish this finance degree. The last month or so I've started programming in C++, and refreshing up on my maths knowledge (Khan Academy, algebra, precalculus). My problem is I don't know where to go from here, and what order I should be learning the different branches of math. I've picked up a book on linear algebra today; ""Linear algebra and its applications"" by Gilbert Strang. Do you think this is an alright place to start? Or do I start with basic calculus? And where can I go next? Differential equations? I hope some of you can help me in the right direction. Thanks.",,"['calculus', 'linear-algebra', 'ordinary-differential-equations']"
68,Having trouble using eigenvectors to solve differential equations,Having trouble using eigenvectors to solve differential equations,,"The question asked to solve $$\frac{dx}{dy} = \begin{pmatrix}         5 & 4 \\         -1 & 1\\         \end{pmatrix}x$$ ,where $$ x = \begin{pmatrix} x_1 \\  x_2 \\ \end{pmatrix}$$ I went ahead an found the determinant of matrix $$ |A - I\lambda| = \lambda^2 - 4\lambda + 9$$ And found $\lambda = 3$ Then the $\alpha$ matrices was found to be $$ \begin{pmatrix} 5 & 4\\ -1 & 1\\ \end{pmatrix} \alpha = 3\alpha$$ where $$\alpha = \begin{pmatrix}  \alpha_1 \\ \alpha_2 \\ \end{pmatrix}$$ Ultimately $-\alpha_1 = 2\alpha_2$ so I write $$\alpha = \begin{pmatrix}  -1 \\ 2 \\ \end{pmatrix}$$ Then because $\lambda$ is a repeated root I know the solution is supposed to look something like this: $$x = c_1\alpha e^{\lambda t} + c_2( \alpha t + \beta) e^{\lambda t}t$$ And then this is where it gets tricky for me. I know we find the $\beta$ matrix by figuring this out: $$(A - I\lambda)\beta = \alpha$$ Now when I multiply all that out I get $$2\beta_1 + 4\beta_2 = -1$$ $$\beta_1 + 2\beta_2 = -2$$ This is the system of equations I can't seem to solve to get a suitable $\beta$. One option I have is to make $$\beta = \begin{pmatrix}  0\\ -1\\ \end{pmatrix}$$ But this doesn't work for the second system of equations. Help please.","The question asked to solve $$\frac{dx}{dy} = \begin{pmatrix}         5 & 4 \\         -1 & 1\\         \end{pmatrix}x$$ ,where $$ x = \begin{pmatrix} x_1 \\  x_2 \\ \end{pmatrix}$$ I went ahead an found the determinant of matrix $$ |A - I\lambda| = \lambda^2 - 4\lambda + 9$$ And found $\lambda = 3$ Then the $\alpha$ matrices was found to be $$ \begin{pmatrix} 5 & 4\\ -1 & 1\\ \end{pmatrix} \alpha = 3\alpha$$ where $$\alpha = \begin{pmatrix}  \alpha_1 \\ \alpha_2 \\ \end{pmatrix}$$ Ultimately $-\alpha_1 = 2\alpha_2$ so I write $$\alpha = \begin{pmatrix}  -1 \\ 2 \\ \end{pmatrix}$$ Then because $\lambda$ is a repeated root I know the solution is supposed to look something like this: $$x = c_1\alpha e^{\lambda t} + c_2( \alpha t + \beta) e^{\lambda t}t$$ And then this is where it gets tricky for me. I know we find the $\beta$ matrix by figuring this out: $$(A - I\lambda)\beta = \alpha$$ Now when I multiply all that out I get $$2\beta_1 + 4\beta_2 = -1$$ $$\beta_1 + 2\beta_2 = -2$$ This is the system of equations I can't seem to solve to get a suitable $\beta$. One option I have is to make $$\beta = \begin{pmatrix}  0\\ -1\\ \end{pmatrix}$$ But this doesn't work for the second system of equations. Help please.",,"['calculus', 'linear-algebra', 'matrices', 'ordinary-differential-equations']"
69,Addition of homogeneous vectors with different w component,Addition of homogeneous vectors with different w component,,"If I have two homogeneous vectors say $v_1 = (x_1, y_1, z_1, w_1)$ and $v_2 = (x_2, y_2, z_2, w_2)$, then is their addition defined if $w_1 \ne w_2$? If $w_1 = w_2 = w$, then I can simply do $v_1 + v_2 = (x_1+x_2, y_1+y_2, z_1+z_2, w)$, but I am having trouble with interpreting result when $w_1 \ne w_2$. * Edit: *If v1{5, 1, 2, 1} and v2{3, 3, 6, 1} are two homogeneous vectors then correct addition is add{8, 4, 8, 1} and not {8, 4, 8, 2}, which will be equivalent to {4, 2, 4, 1} in 3D space. It makes sense to define such operations on homogeneous 3D vectors when w1=w2 but I guess when w1 != w2 such operations are undefined but I am not sure so I am wondering if anyone can shed some light on it and confirm if my assumption is correct","If I have two homogeneous vectors say $v_1 = (x_1, y_1, z_1, w_1)$ and $v_2 = (x_2, y_2, z_2, w_2)$, then is their addition defined if $w_1 \ne w_2$? If $w_1 = w_2 = w$, then I can simply do $v_1 + v_2 = (x_1+x_2, y_1+y_2, z_1+z_2, w)$, but I am having trouble with interpreting result when $w_1 \ne w_2$. * Edit: *If v1{5, 1, 2, 1} and v2{3, 3, 6, 1} are two homogeneous vectors then correct addition is add{8, 4, 8, 1} and not {8, 4, 8, 2}, which will be equivalent to {4, 2, 4, 1} in 3D space. It makes sense to define such operations on homogeneous 3D vectors when w1=w2 but I guess when w1 != w2 such operations are undefined but I am not sure so I am wondering if anyone can shed some light on it and confirm if my assumption is correct",,"['linear-algebra', 'projective-space']"
70,Eigenvalues of a Hermition Matrix do not cross,Eigenvalues of a Hermition Matrix do not cross,,"Wikipedia's article on avoided crossing asserts that ""The eigenvalues of a Hermitian matrix depending on N continuous real parameters cannot cross except at a manifold of N-2 dimensions."" If it's true, does anyone have an elegant proof of this statement?  Does anyone have a good interpretation or a good intuition for why this would be true?","Wikipedia's article on avoided crossing asserts that ""The eigenvalues of a Hermitian matrix depending on N continuous real parameters cannot cross except at a manifold of N-2 dimensions."" If it's true, does anyone have an elegant proof of this statement?  Does anyone have a good interpretation or a good intuition for why this would be true?",,['linear-algebra']
71,Linear Algebra matrix $Ax=b$ true or false nullspace,Linear Algebra matrix  true or false nullspace,Ax=b,"$Ax=b$ $m$ number of Rows $n$ number of columns true or false A) If $n > m$, given any $b$ you can always solve $Ax=b$. The answer: False. Counterexample: A is the zero matrix. We have unknowns more than equations, so we can always solve $Ax=b$. Why the answer is false?   And even if $A$ is the zero matrix we have the matrices x = zeros then $b$ zeros so there is always answer!!   Can anyone explain why the answer is false? How can he proved false by $A$ is the zero matrix? B) If $n < m$, the only solution of $Ax=0$ is $x=0.$ The answer: False. Counterexample: let $A$be the zero matrix. I understand why it is false but I'm wondering, is there any special case makes this statement true? Edit: I made a matrix $A= 2$ rows $\times 1$ column contains $(1,0)$ and assume Ax=0 as the statement, then the only solution is when x=0 because we can't use row echelon form any more. is what I did correct to make the statement true?","$Ax=b$ $m$ number of Rows $n$ number of columns true or false A) If $n > m$, given any $b$ you can always solve $Ax=b$. The answer: False. Counterexample: A is the zero matrix. We have unknowns more than equations, so we can always solve $Ax=b$. Why the answer is false?   And even if $A$ is the zero matrix we have the matrices x = zeros then $b$ zeros so there is always answer!!   Can anyone explain why the answer is false? How can he proved false by $A$ is the zero matrix? B) If $n < m$, the only solution of $Ax=0$ is $x=0.$ The answer: False. Counterexample: let $A$be the zero matrix. I understand why it is false but I'm wondering, is there any special case makes this statement true? Edit: I made a matrix $A= 2$ rows $\times 1$ column contains $(1,0)$ and assume Ax=0 as the statement, then the only solution is when x=0 because we can't use row echelon form any more. is what I did correct to make the statement true?",,"['linear-algebra', 'matrices']"
72,Proving $\chi$ fixes two points in the unit sphere.,Proving  fixes two points in the unit sphere.,\chi,"Let $\chi:\mathbb{R}^{3}\to\mathbb{R}^{3}$ be an orthogonal transformation such that $\det(\chi)=1$ and $\chi$ is not the identity linear transformation.  Let $S \subset \mathbb{R}^{3}$, be the unit sphere.  Then how do we prove that $\chi$ fixes only two points of $S$? Any ideas of proceeding for the solution?","Let $\chi:\mathbb{R}^{3}\to\mathbb{R}^{3}$ be an orthogonal transformation such that $\det(\chi)=1$ and $\chi$ is not the identity linear transformation.  Let $S \subset \mathbb{R}^{3}$, be the unit sphere.  Then how do we prove that $\chi$ fixes only two points of $S$? Any ideas of proceeding for the solution?",,['linear-algebra']
73,Determine a basis for the solution set of the homogeneous system,Determine a basis for the solution set of the homogeneous system,,"Determine a basis for the solution set of the homogeneous system: $$\begin{align*} x_1 +x_2 +x_3 &=0\\ 3x_1+3x_2+x_3 &=0\\ 4x_1+4x_2+2x_3&=0 \end{align*}$$ Then the augmented matrix is: $$         \left[\begin{array}{ccc|c}         1 & 1 & 1 &0\\         3 & 3 & 1 &0\\         4 & 4 & 2 &0\\         \end{array}\right] $$ Reduced Row Echelon Form $\to$ $$         \left[\begin{array}{ccc|c}         1 & 1 & 0 &0\\         0 & 0 & 1 &0\\         0 & 0 & 0 &0\\         \end{array}\right] $$ I already looked at this example but it didn't help much. I am wondering can someone help to find basis (choosing some parameter for variables $x_1,x_2,x_3$) from RREF.","Determine a basis for the solution set of the homogeneous system: $$\begin{align*} x_1 +x_2 +x_3 &=0\\ 3x_1+3x_2+x_3 &=0\\ 4x_1+4x_2+2x_3&=0 \end{align*}$$ Then the augmented matrix is: $$         \left[\begin{array}{ccc|c}         1 & 1 & 1 &0\\         3 & 3 & 1 &0\\         4 & 4 & 2 &0\\         \end{array}\right] $$ Reduced Row Echelon Form $\to$ $$         \left[\begin{array}{ccc|c}         1 & 1 & 0 &0\\         0 & 0 & 1 &0\\         0 & 0 & 0 &0\\         \end{array}\right] $$ I already looked at this example but it didn't help much. I am wondering can someone help to find basis (choosing some parameter for variables $x_1,x_2,x_3$) from RREF.",,"['linear-algebra', 'matrices', 'systems-of-equations']"
74,Relation between linear transformations and traces,Relation between linear transformations and traces,,"a) Let $A \in M_n (K)$. We denote $f_A$ the linear form defined, for every $X \in M_n (K)$, by $f_A(X)=Tr(AX)$. Show that the function $f$ which maps $A \in M_n (K)$ to $f_A$ is an isomorphism between $M_n (K)$ and its dual. b) Let $f: M_n (K) \rightarrow K$ be a linear form such that, for every $(X,Y)$ in $M_n (K)^2$, $f(XY)=f(YX)$. Show that there exists $\lambda \in K$ such that for every $X \in M_n (K)$, $f(X)=\lambda Tr(X)$ That is what I have: a) Let $(E_{ij})_{1\leq i,j \leq n}$ be the standard basis for $M_n (K)$. Let us start by showing that for every $1 \leq i,j,k,l \leq n$, we have $E_{ij} E_{kl}= \delta_{jk} E_{il}$ We have $E_{ij}=(\delta_{pi} \delta_{qj})_{1 \leq p,q \leq n}$ and $E_{kl}=(\delta_{pk} \delta_{ql})_{1 \leq p,q \leq n}$ $A= E_{ij} E_{kl}= (a_{p,q})$ such that: $a_{p,q}= \sum \limits_{r=1}^n (\delta_{pi} \delta_{rj})(\delta_{rk} \delta_{ql})=( \sum \limits_{r=1}^n \delta_{rj} \delta_{rk})\delta_{pi} \delta_{ql}= \delta_{jk} \delta_{pi} \delta_{ql}$ Hence: $E_{ij} E_{kl}= \delta_{jk} E_{il}$ $$\begin{array}{c}\\\\\end{array}$$ It is obvious that $f$ is linear. Therefore, for dimensional reasons, we must only show that $f$ is injective. Let $A= (a_{ij})_{1\leq i,j \leq n}$ such that $f_A=0$. We therefore have, for $1 \leq i_0, j_0 \leq n$ $0= Tr(AE_{i_0 j_0})= Tr(\sum \limits_{1 \leq i,j \leq n} a_{ij}E_{ij}E{i_0 j_0})= Tr(\sum \limits_{i=1}^n a_{i i_{0}}E_{i i_0} E_{i_0 j_0})= \sum \limits_{i=1}^n a_{i i_0} Tr(E_{i j_0})=a_{j_{0} i_{0}}$ Hence $A$ equals zero. Therefore, $f_A$ is an isomorphism. $$\begin{array}{c}\\\\\end{array}$$ Can someone help me with b?","a) Let $A \in M_n (K)$. We denote $f_A$ the linear form defined, for every $X \in M_n (K)$, by $f_A(X)=Tr(AX)$. Show that the function $f$ which maps $A \in M_n (K)$ to $f_A$ is an isomorphism between $M_n (K)$ and its dual. b) Let $f: M_n (K) \rightarrow K$ be a linear form such that, for every $(X,Y)$ in $M_n (K)^2$, $f(XY)=f(YX)$. Show that there exists $\lambda \in K$ such that for every $X \in M_n (K)$, $f(X)=\lambda Tr(X)$ That is what I have: a) Let $(E_{ij})_{1\leq i,j \leq n}$ be the standard basis for $M_n (K)$. Let us start by showing that for every $1 \leq i,j,k,l \leq n$, we have $E_{ij} E_{kl}= \delta_{jk} E_{il}$ We have $E_{ij}=(\delta_{pi} \delta_{qj})_{1 \leq p,q \leq n}$ and $E_{kl}=(\delta_{pk} \delta_{ql})_{1 \leq p,q \leq n}$ $A= E_{ij} E_{kl}= (a_{p,q})$ such that: $a_{p,q}= \sum \limits_{r=1}^n (\delta_{pi} \delta_{rj})(\delta_{rk} \delta_{ql})=( \sum \limits_{r=1}^n \delta_{rj} \delta_{rk})\delta_{pi} \delta_{ql}= \delta_{jk} \delta_{pi} \delta_{ql}$ Hence: $E_{ij} E_{kl}= \delta_{jk} E_{il}$ $$\begin{array}{c}\\\\\end{array}$$ It is obvious that $f$ is linear. Therefore, for dimensional reasons, we must only show that $f$ is injective. Let $A= (a_{ij})_{1\leq i,j \leq n}$ such that $f_A=0$. We therefore have, for $1 \leq i_0, j_0 \leq n$ $0= Tr(AE_{i_0 j_0})= Tr(\sum \limits_{1 \leq i,j \leq n} a_{ij}E_{ij}E{i_0 j_0})= Tr(\sum \limits_{i=1}^n a_{i i_{0}}E_{i i_0} E_{i_0 j_0})= \sum \limits_{i=1}^n a_{i i_0} Tr(E_{i j_0})=a_{j_{0} i_{0}}$ Hence $A$ equals zero. Therefore, $f_A$ is an isomorphism. $$\begin{array}{c}\\\\\end{array}$$ Can someone help me with b?",,['linear-algebra']
75,Topology of the space of hermitian positive definite matrices,Topology of the space of hermitian positive definite matrices,,"Let $\mathcal{H}_n \mathbb{C}$ be the set of hermitian $n \times n$ complex matrices. This set carries the structure of a vector space over $\mathbb{R}$ under  usual addition. It also inherits the standard euclidean topology from $\mathbb{C}^{n\times n}$. Let $\mathcal{P}$ denote the subset of $\mathcal{H}_n \mathbb{C}$ of positive definite matrices and give it the subspace topology. My question is: is $\mathcal{P}$ locally compact? What I understand: $\mathcal{P}$ is closed under linear combination with positive coefficients, in particular it is closed under addition, multiplication by a positive scalar and is convex. In dimension $1$ it is just $]0,+\infty[$, so the answer is yes. My feeling is that the answer should be yes in higher dimensions as well, since $\mathcal{P}$ is a kind of ""cone"" in a real vector space, but I can't provide a rigorous proof of this.","Let $\mathcal{H}_n \mathbb{C}$ be the set of hermitian $n \times n$ complex matrices. This set carries the structure of a vector space over $\mathbb{R}$ under  usual addition. It also inherits the standard euclidean topology from $\mathbb{C}^{n\times n}$. Let $\mathcal{P}$ denote the subset of $\mathcal{H}_n \mathbb{C}$ of positive definite matrices and give it the subspace topology. My question is: is $\mathcal{P}$ locally compact? What I understand: $\mathcal{P}$ is closed under linear combination with positive coefficients, in particular it is closed under addition, multiplication by a positive scalar and is convex. In dimension $1$ it is just $]0,+\infty[$, so the answer is yes. My feeling is that the answer should be yes in higher dimensions as well, since $\mathcal{P}$ is a kind of ""cone"" in a real vector space, but I can't provide a rigorous proof of this.",,"['linear-algebra', 'general-topology', 'matrices', 'topological-vector-spaces']"
76,How to (dis-)prove that this symmetric matrix is P.S.D.?,How to (dis-)prove that this symmetric matrix is P.S.D.?,,"for two random $n$-vectors $(X_1,X_2)$ define $$f(X_1,X_2)=\text{med}((X_1+X_2)^2)-\text{med}((X_1-X_2)^2)$$ Now define the matrix $U$ with entries $$U_{ij}=\frac{n}{4}f(X_i,X_j)$$ Based on a large number of computer simulations using many nearly collinear matrices $X=(X_1,\ldots,X_p)\in\mathbb{R}^{n\times p}$ with $n>p=100$, I've come to suspect that if the members of $X$ are in general linear position, then $U$ is P.S.D.. To put the matter to rest, I'm trying to prove this statement (or find a counter example). My question is how would you go about attacking this problem? I've been trying to prove that $U$ is diagonally dominant (it is easy to show that $U_{ii}>U_{ij}\forall i\neq j$, but I have not been able to make any statement about the sum of the absolute values of the off-diagonal elements).","for two random $n$-vectors $(X_1,X_2)$ define $$f(X_1,X_2)=\text{med}((X_1+X_2)^2)-\text{med}((X_1-X_2)^2)$$ Now define the matrix $U$ with entries $$U_{ij}=\frac{n}{4}f(X_i,X_j)$$ Based on a large number of computer simulations using many nearly collinear matrices $X=(X_1,\ldots,X_p)\in\mathbb{R}^{n\times p}$ with $n>p=100$, I've come to suspect that if the members of $X$ are in general linear position, then $U$ is P.S.D.. To put the matter to rest, I'm trying to prove this statement (or find a counter example). My question is how would you go about attacking this problem? I've been trying to prove that $U$ is diagonally dominant (it is easy to show that $U_{ii}>U_{ij}\forall i\neq j$, but I have not been able to make any statement about the sum of the absolute values of the off-diagonal elements).",,['linear-algebra']
77,An eigenvector is a non-zero vector such that...,An eigenvector is a non-zero vector such that...,,"Various sources define eigenvalues and eigenvectors in slightly different ways (context independent).  For example, both of the following definitions seem not to exclude the zero-vector as an eigenvector.  In Lang : Let $V$ be a vector space over a field $K$, and let $A:V\to V$ be an operator on $V$.  An  element $\mathbf{v}\in V$ is called an eigenvector of $A$ if there exists $\lambda\in K$ such that $A\mathbf{v}=\lambda\mathbf{v}$.  If $\mathbf{v}\ne \mathbf{0}$, then... and, in Hoffman/Kunze Let $V$ be a vector space over a field $F$ and let $T$ be a linear operator on $V$.  A characteristic value of $T$ is a scalar $c$ in $F$ such that there is a non-zero vector $\alpha$ in $V$ with $T\alpha =c\alpha$.  If $c$ is a characteristic value of $T$, then (a) any $\alpha$ such that $T\alpha = c\alpha$ is called a characteristic vector of $T$ associated with the characteristic value $c$, and (b) the collection of all $\alpha$ such that $T\alpha = c\alpha$ is called the characteristic space associated with $c$. More typically (?), you would see explicit exclusion of the zero-vector as, An eigenvector is a non-zero vector $x$ such that $A\mathbf{x}=\lambda\mathbf{x}$ for some scalar $\lambda$.  The scalar $\lambda$ is called an eigenvalue and $x$ an associated eigenvector.  The eigenspace corresponding to the eigenvalue $\lambda$ is the set of all associated eigenvectors along with the zero-vector. I realize that the zero-vector must be explicitly excluded when defining an eigenvalue, but once that definition is made, explicitly excluding the zero-vector as being an eigenvector and then explicitly including it again to form the eigenspace seems rather artificial.  Is this simply a matter of attempting to make the definitions more natural depending on their order? That is, eigenvalue (excluding the zero-vector), then eigenvector -> zero vector ok eigenvector, then eigenvalue -> zero vector not ok or, is there anything incorrect about the definition given, for example, in Hoffman/Kunze above?  If so, what sort of inconsistency would result from the definition.  I'm not an algebraist, so something rather elementary (if possible) would be preferred.","Various sources define eigenvalues and eigenvectors in slightly different ways (context independent).  For example, both of the following definitions seem not to exclude the zero-vector as an eigenvector.  In Lang : Let $V$ be a vector space over a field $K$, and let $A:V\to V$ be an operator on $V$.  An  element $\mathbf{v}\in V$ is called an eigenvector of $A$ if there exists $\lambda\in K$ such that $A\mathbf{v}=\lambda\mathbf{v}$.  If $\mathbf{v}\ne \mathbf{0}$, then... and, in Hoffman/Kunze Let $V$ be a vector space over a field $F$ and let $T$ be a linear operator on $V$.  A characteristic value of $T$ is a scalar $c$ in $F$ such that there is a non-zero vector $\alpha$ in $V$ with $T\alpha =c\alpha$.  If $c$ is a characteristic value of $T$, then (a) any $\alpha$ such that $T\alpha = c\alpha$ is called a characteristic vector of $T$ associated with the characteristic value $c$, and (b) the collection of all $\alpha$ such that $T\alpha = c\alpha$ is called the characteristic space associated with $c$. More typically (?), you would see explicit exclusion of the zero-vector as, An eigenvector is a non-zero vector $x$ such that $A\mathbf{x}=\lambda\mathbf{x}$ for some scalar $\lambda$.  The scalar $\lambda$ is called an eigenvalue and $x$ an associated eigenvector.  The eigenspace corresponding to the eigenvalue $\lambda$ is the set of all associated eigenvectors along with the zero-vector. I realize that the zero-vector must be explicitly excluded when defining an eigenvalue, but once that definition is made, explicitly excluding the zero-vector as being an eigenvector and then explicitly including it again to form the eigenspace seems rather artificial.  Is this simply a matter of attempting to make the definitions more natural depending on their order? That is, eigenvalue (excluding the zero-vector), then eigenvector -> zero vector ok eigenvector, then eigenvalue -> zero vector not ok or, is there anything incorrect about the definition given, for example, in Hoffman/Kunze above?  If so, what sort of inconsistency would result from the definition.  I'm not an algebraist, so something rather elementary (if possible) would be preferred.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
78,Hint for proving ${\rm SL}_n(\mathbb{R})$ is generated by matrices that are off-diagonal entries added to identity. [duplicate],Hint for proving  is generated by matrices that are off-diagonal entries added to identity. [duplicate],{\rm SL}_n(\mathbb{R}),"This question already has answers here : What are the generators for $SL_n(\mathbb{R})$ (Michael Artin's Algebra book) (2 answers) Closed 11 years ago . Let $S = \{ I_n + a\cdot e_{i,j} \mid a\in\mathbb{R},\ i,j= 1,\ldots,n,\ i\neq j\}$, where $e_{i,j}$ is the matrix with $1$ at entry $(i,j)$ and zero elsewhere.  I need a hint to help prove that $\langle S\rangle ={\rm SL}_n(\mathbb{R})$.  That is, such matrices generate the multiplicative group of matrices with determinant one.","This question already has answers here : What are the generators for $SL_n(\mathbb{R})$ (Michael Artin's Algebra book) (2 answers) Closed 11 years ago . Let $S = \{ I_n + a\cdot e_{i,j} \mid a\in\mathbb{R},\ i,j= 1,\ldots,n,\ i\neq j\}$, where $e_{i,j}$ is the matrix with $1$ at entry $(i,j)$ and zero elsewhere.  I need a hint to help prove that $\langle S\rangle ={\rm SL}_n(\mathbb{R})$.  That is, such matrices generate the multiplicative group of matrices with determinant one.",,"['linear-algebra', 'matrices']"
79,Eigenvalues of the sum of a stochastic matrix and a diagonal matrix,Eigenvalues of the sum of a stochastic matrix and a diagonal matrix,,"Let $D$ be a real diagonal matrix $D=\operatorname{diag}(a_1,a_2,\ldots,a_n)$ with $a_1\le a_2\le\ldots\le a_n$. Assume that at least one of the $a_i$ is positive. Let $P$ be an irreducible, real, row-stochastic matrix (all entries between 0 and 1, the sum of the elements of every row is 1), with all its eigenvalues real. Let $k>0$ be a real number. I wish to prove that $D+kP$ has a positive eigenvalue. Do you think this is true?","Let $D$ be a real diagonal matrix $D=\operatorname{diag}(a_1,a_2,\ldots,a_n)$ with $a_1\le a_2\le\ldots\le a_n$. Assume that at least one of the $a_i$ is positive. Let $P$ be an irreducible, real, row-stochastic matrix (all entries between 0 and 1, the sum of the elements of every row is 1), with all its eigenvalues real. Let $k>0$ be a real number. I wish to prove that $D+kP$ has a positive eigenvalue. Do you think this is true?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
80,Find out whether a matrix is a distance matrix or not,Find out whether a matrix is a distance matrix or not,,I have a $5 \times 5$ symmetric matrix $A$ with zeroes on the diagonal and I am supposed to find whether there exist 5 points in $\mathbb R^4$ such that $A$ is the distance matrix. How can I solve this formally? I am guessing I have to use a Gram matrix but I don't know what to do after that... Here is the matrix:  $$A= \begin{pmatrix} 0 &1 &2 &2 &2\sqrt 2\\ 1& 0 &\sqrt5 &\sqrt5 &3 \\ 2 &\sqrt5 &0 &2\sqrt2 &2\\ 2 &\sqrt5 & 2\sqrt2 &0 &2\sqrt3\\ 2\sqrt2 & 3 &2 &2\sqrt3 &0 \end{pmatrix}.$$ Edit. I got stuck when I tried to use the technique proposed by user1551 on this matrix $$ \begin{pmatrix} 0& \sqrt5 &  \sqrt5 &  \sqrt5 & \sqrt5 \\  \sqrt5 & 0&  2\sqrt5 &  2\sqrt2 & 2\\ \sqrt5  &2\sqrt5 & 0 & 2 & 2\sqrt2 \\  \sqrt5 &  2\sqrt2 & 2 & 0 &2 \sqrt5 &\\  \sqrt5 & 2 & 2\sqrt2 & 2 \sqrt5 &0\\ \end{pmatrix}.$$ The problem arises when I try to compute $x_3$. I obtained the following system: $$-2\begin{pmatrix}  \sqrt5 &  0\\ -\sqrt5 &0 \\ \end{pmatrix} \begin{pmatrix} x_{31}\\ x_{32}\\ \end{pmatrix} =\begin{pmatrix} 10\\ -1\\ \end{pmatrix}.$$,I have a $5 \times 5$ symmetric matrix $A$ with zeroes on the diagonal and I am supposed to find whether there exist 5 points in $\mathbb R^4$ such that $A$ is the distance matrix. How can I solve this formally? I am guessing I have to use a Gram matrix but I don't know what to do after that... Here is the matrix:  $$A= \begin{pmatrix} 0 &1 &2 &2 &2\sqrt 2\\ 1& 0 &\sqrt5 &\sqrt5 &3 \\ 2 &\sqrt5 &0 &2\sqrt2 &2\\ 2 &\sqrt5 & 2\sqrt2 &0 &2\sqrt3\\ 2\sqrt2 & 3 &2 &2\sqrt3 &0 \end{pmatrix}.$$ Edit. I got stuck when I tried to use the technique proposed by user1551 on this matrix $$ \begin{pmatrix} 0& \sqrt5 &  \sqrt5 &  \sqrt5 & \sqrt5 \\  \sqrt5 & 0&  2\sqrt5 &  2\sqrt2 & 2\\ \sqrt5  &2\sqrt5 & 0 & 2 & 2\sqrt2 \\  \sqrt5 &  2\sqrt2 & 2 & 0 &2 \sqrt5 &\\  \sqrt5 & 2 & 2\sqrt2 & 2 \sqrt5 &0\\ \end{pmatrix}.$$ The problem arises when I try to compute $x_3$. I obtained the following system: $$-2\begin{pmatrix}  \sqrt5 &  0\\ -\sqrt5 &0 \\ \end{pmatrix} \begin{pmatrix} x_{31}\\ x_{32}\\ \end{pmatrix} =\begin{pmatrix} 10\\ -1\\ \end{pmatrix}.$$,,"['linear-algebra', 'abstract-algebra']"
81,Question on finite dimensional vector space,Question on finite dimensional vector space,,"Let $V$ be  a finite dimensional vector space over $F$, a finite field of two elements. Is it possible to find the sum $$\sum_{v\in V}v$$?","Let $V$ be  a finite dimensional vector space over $F$, a finite field of two elements. Is it possible to find the sum $$\sum_{v\in V}v$$?",,['linear-algebra']
82,Linear Algebra Text Problem,Linear Algebra Text Problem,,"We have specified number of light bulbs. In addition to the array there are buttons.  Pressing the button changing state of light bulbs which are connected to the switch. It is known that for each set of lamps exist button that is connected with the odd-number of bulbs from this set. Prove that properly pressing the buttons we can turn off all bulbs. Very nice task, but hard. I think that I can make a $Z^n$ space of bulbs, but how to prove that we have a basis of it? Or that's not a correct way? I'm looking for hints.","We have specified number of light bulbs. In addition to the array there are buttons.  Pressing the button changing state of light bulbs which are connected to the switch. It is known that for each set of lamps exist button that is connected with the odd-number of bulbs from this set. Prove that properly pressing the buttons we can turn off all bulbs. Very nice task, but hard. I think that I can make a $Z^n$ space of bulbs, but how to prove that we have a basis of it? Or that's not a correct way? I'm looking for hints.",,['linear-algebra']
83,Is it possible to reverse this sequence of permutations?,Is it possible to reverse this sequence of permutations?,,"Let  $ S = (a_1, a_2, ..., a_N) $ be a finite (arbitrarily long) sequence of elements, and let $p_1, p_2, ..., p_n $ be the first $n$ prime numbers, with $n \ge 3$. We apply a sequence of permutations to $S$ as follows. First, we take every element in $S$ whose index is congruent with 1 modulo 2, and we rotate them within themselves $A^2_1$ positions (where $A^2_1$ is an integer in $[0, N/2)$). For instance, if $S = (a,b,c,d,e,f)$, and $A^2_1=2$, the result would be $(\mathbf c,b,\mathbf e,d,\mathbf a,f)$. In the second place, we take every element in the sequence obtained whose index is congruent with $0$ (that is, the rest), and rotate them within themselves $A_0^2$ positions (where $A^2_0$ is an integer in $[0,N/2)$); that will be $S_1$. Following the previous example, rotating with $A^2_0 = 1$ would bear $S_1 = (c,\mathbf f,e,\mathbf b,a,\mathbf d)$. Now, we take every element in $S_1$ with index congruent with 1 modulo 3, and we rotate them within themselves $A^3_1$ positions; afterwards, every element with index congruent with 2 modulo 3, $A^3_2$ positions, and finally, every element with index congruent with 0 modulo 3, $A^3_0$ positions. $A_i^3$ is an integer in $[0,N/3)$. Continuing the example, assuming $A^3_1 = 1, A^3_2 = 0, A^3_0 = 1$, we would obtain $(b,f,d,c,a,e)$. This process is repeated for every prime up to $p_n$, and let $S_n$ be the result. (Edited: see below) My question is: assuming $n$ and $S_n$ are known, how much additional information would be necessary to calculate the coefficients ? (Edited: see below) By additional information I mean, for example, knowing the initial positions (in $S$) of some elements in $S_n$. I have been working for days on this, but my current option, which is straightforward using systems of equations, does not seem t otake me anywhere since I don't know which coefficients are being used in each case. Is there any other approach that I should consider? Apologies for my English, and sorry if this is not the correct stackexchange site for this question. EDIT: As Alexander pointed, coefficients would not be unique (see his example below), so to state it more accurately: would it be possible to obtain the original $S$ given $n, S_n$, and some additional information? This problem is related to a cryptography project, where I intend to cypher a message by rearranging it, using coefficients as a key. This means that, even if the original permutation (or any complete set of coefficients) were impossible to find, any way to determine information on them, or bounding them, would greatly hurt the scheme. I know this is not the place for crypto problems, but maybe explaining the objective would be useful to anyone trying to answer.","Let  $ S = (a_1, a_2, ..., a_N) $ be a finite (arbitrarily long) sequence of elements, and let $p_1, p_2, ..., p_n $ be the first $n$ prime numbers, with $n \ge 3$. We apply a sequence of permutations to $S$ as follows. First, we take every element in $S$ whose index is congruent with 1 modulo 2, and we rotate them within themselves $A^2_1$ positions (where $A^2_1$ is an integer in $[0, N/2)$). For instance, if $S = (a,b,c,d,e,f)$, and $A^2_1=2$, the result would be $(\mathbf c,b,\mathbf e,d,\mathbf a,f)$. In the second place, we take every element in the sequence obtained whose index is congruent with $0$ (that is, the rest), and rotate them within themselves $A_0^2$ positions (where $A^2_0$ is an integer in $[0,N/2)$); that will be $S_1$. Following the previous example, rotating with $A^2_0 = 1$ would bear $S_1 = (c,\mathbf f,e,\mathbf b,a,\mathbf d)$. Now, we take every element in $S_1$ with index congruent with 1 modulo 3, and we rotate them within themselves $A^3_1$ positions; afterwards, every element with index congruent with 2 modulo 3, $A^3_2$ positions, and finally, every element with index congruent with 0 modulo 3, $A^3_0$ positions. $A_i^3$ is an integer in $[0,N/3)$. Continuing the example, assuming $A^3_1 = 1, A^3_2 = 0, A^3_0 = 1$, we would obtain $(b,f,d,c,a,e)$. This process is repeated for every prime up to $p_n$, and let $S_n$ be the result. (Edited: see below) My question is: assuming $n$ and $S_n$ are known, how much additional information would be necessary to calculate the coefficients ? (Edited: see below) By additional information I mean, for example, knowing the initial positions (in $S$) of some elements in $S_n$. I have been working for days on this, but my current option, which is straightforward using systems of equations, does not seem t otake me anywhere since I don't know which coefficients are being used in each case. Is there any other approach that I should consider? Apologies for my English, and sorry if this is not the correct stackexchange site for this question. EDIT: As Alexander pointed, coefficients would not be unique (see his example below), so to state it more accurately: would it be possible to obtain the original $S$ given $n, S_n$, and some additional information? This problem is related to a cryptography project, where I intend to cypher a message by rearranging it, using coefficients as a key. This means that, even if the original permutation (or any complete set of coefficients) were impossible to find, any way to determine information on them, or bounding them, would greatly hurt the scheme. I know this is not the place for crypto problems, but maybe explaining the objective would be useful to anyone trying to answer.",,"['linear-algebra', 'finite-groups', 'permutations']"
84,Find the kernel of a linear transformation of $P_2$ to $P_1$,Find the kernel of a linear transformation of  to,P_2 P_1,"For some reason, this particular problem is throwing me off: Find the kernel of the linear transformation: $T: P_2 \rightarrow P_1$ $T(a_0+a_1x+a_2x^2)=a_1+2a_2x$ Since the kernel is the set of all vectors in $V$ that satisfy $T(\vec{v})=\vec{0}$, it's obvious that $a_0$ can be any real number. What matters, if I understand correctly, is that $a_1$ and $a_2$ should equal 0 in order to satisfy the zero vector (i.e. $0+2(0)x$). Granted that what I stated is correct, why would my book say that the $\ker(T)=\{a_0: a_0 \; \text{is real}\}$? Yes, $a_0$ can be any real number, but what must $a_1$ or $a_2$ equal? I don't see it specified within the set. Perhaps it's implied - I'm not sure. Let me add some more detail: Again, if I understand correctly, I could make a system of equations as such: $a_1 = 0$ $2a_2 = 0$ From that I can translate it into a matrix and find that $a_1$ and $a_2$ really does equal zero.","For some reason, this particular problem is throwing me off: Find the kernel of the linear transformation: $T: P_2 \rightarrow P_1$ $T(a_0+a_1x+a_2x^2)=a_1+2a_2x$ Since the kernel is the set of all vectors in $V$ that satisfy $T(\vec{v})=\vec{0}$, it's obvious that $a_0$ can be any real number. What matters, if I understand correctly, is that $a_1$ and $a_2$ should equal 0 in order to satisfy the zero vector (i.e. $0+2(0)x$). Granted that what I stated is correct, why would my book say that the $\ker(T)=\{a_0: a_0 \; \text{is real}\}$? Yes, $a_0$ can be any real number, but what must $a_1$ or $a_2$ equal? I don't see it specified within the set. Perhaps it's implied - I'm not sure. Let me add some more detail: Again, if I understand correctly, I could make a system of equations as such: $a_1 = 0$ $2a_2 = 0$ From that I can translate it into a matrix and find that $a_1$ and $a_2$ really does equal zero.",,['linear-algebra']
85,Textbook determinant convention,Textbook determinant convention,,"My text book is called ""Linear Algebra and its applications"" by David C. Lay. I am just wondering why the textbook uses the absolute value symbol when it wants us to compute determinants. For example, for some matrix A, the determinant is represented as |A| (Chapter 3.1 Determinants). And yet 2 sections later (3.3 Volume), we actually use absolute values around determinants, which is represented like this: | det(A) |. So if we wanted to compute the determinant of Matrix A as well as have the absolute value of it, it would look like ||A||? I just don't see why the text book would choose to denote determinants using the same symbols as absolute values. Why would they not choose a different representation of determinants?","My text book is called ""Linear Algebra and its applications"" by David C. Lay. I am just wondering why the textbook uses the absolute value symbol when it wants us to compute determinants. For example, for some matrix A, the determinant is represented as |A| (Chapter 3.1 Determinants). And yet 2 sections later (3.3 Volume), we actually use absolute values around determinants, which is represented like this: | det(A) |. So if we wanted to compute the determinant of Matrix A as well as have the absolute value of it, it would look like ||A||? I just don't see why the text book would choose to denote determinants using the same symbols as absolute values. Why would they not choose a different representation of determinants?",,"['linear-algebra', 'reference-request', 'notation', 'determinant']"
86,Similarity of diagonalizable matrices,Similarity of diagonalizable matrices,,"I am looking for an elegant proof of the following: Let $A,B \in \mathbb{K}^{n \times n}$ be diagonalizable matrices, and $P_A, P_B$ their characteristic polynomials. Then: $A, B$ similar $\Leftrightarrow$ $P_A = P_B$ I am somehow stuck at this.","I am looking for an elegant proof of the following: Let $A,B \in \mathbb{K}^{n \times n}$ be diagonalizable matrices, and $P_A, P_B$ their characteristic polynomials. Then: $A, B$ similar $\Leftrightarrow$ $P_A = P_B$ I am somehow stuck at this.",,"['linear-algebra', 'matrices']"
87,Proof of a matrix identity involving the Moore-Penrose pseudo-inverse.,Proof of a matrix identity involving the Moore-Penrose pseudo-inverse.,,"I'm having trouble proving the following. $A^\top B A (A^\top B A)^{\dagger} A^\top B = A^\top B$ In the above, $A$ ($n \times m$) and $B$ ($n \times n$) are any matrices (i.e. possibly rank-deficient). Edit : OK, I see it doesn't hold for general $B$. But what about the case where $A$ is still arbitrary, but $B$ is invertible? I have made two attempts. First, there is the well-known identity $A^\top B A (A^\top B A)^{\dagger} A^\top B A = A^\top B A$. But this doesn't imply the result because $A$ may have linearly dependent rows. Also, it is easy to show the result for $B=I$ by taking the SVD of $A$ and cancelling terms on the left-hand side. Sadly, this doesn't seem to generalize. I apologize in advance if the question is trivial - I am a computer scientist and don't have much experience doing this sort of thing.","I'm having trouble proving the following. $A^\top B A (A^\top B A)^{\dagger} A^\top B = A^\top B$ In the above, $A$ ($n \times m$) and $B$ ($n \times n$) are any matrices (i.e. possibly rank-deficient). Edit : OK, I see it doesn't hold for general $B$. But what about the case where $A$ is still arbitrary, but $B$ is invertible? I have made two attempts. First, there is the well-known identity $A^\top B A (A^\top B A)^{\dagger} A^\top B A = A^\top B A$. But this doesn't imply the result because $A$ may have linearly dependent rows. Also, it is easy to show the result for $B=I$ by taking the SVD of $A$ and cancelling terms on the left-hand side. Sadly, this doesn't seem to generalize. I apologize in advance if the question is trivial - I am a computer scientist and don't have much experience doing this sort of thing.",,"['linear-algebra', 'matrices']"
88,Where can I find a comprehensive guides to linear algebra and calculus?,Where can I find a comprehensive guides to linear algebra and calculus?,,"I'm a software engineer with a keen interest in all sorts of artificial intelligence and machine learning applications, and also quantum computing. Both areas require quite a bit of linear algebra and calculus, and I'm afraid mine is quite rusty. I'm looking for a good source on either or both. I'm not exactly a stranger to them, I'm familiar with the basics (functions, basic matrix operations, and a bit of single variable calculus I can remember from high school), but I'm more of a logic & set theory guy. Does anybody know of a good source that covers more than the basics, but isn't too technical? (I'm not a dedicated mathematician) Much appreciated :)","I'm a software engineer with a keen interest in all sorts of artificial intelligence and machine learning applications, and also quantum computing. Both areas require quite a bit of linear algebra and calculus, and I'm afraid mine is quite rusty. I'm looking for a good source on either or both. I'm not exactly a stranger to them, I'm familiar with the basics (functions, basic matrix operations, and a bit of single variable calculus I can remember from high school), but I'm more of a logic & set theory guy. Does anybody know of a good source that covers more than the basics, but isn't too technical? (I'm not a dedicated mathematician) Much appreciated :)",,"['calculus', 'linear-algebra', 'reference-request']"
89,Properties of generalized eigenvectors,Properties of generalized eigenvectors,,"Let $A\in\mathbb{R}^{n\times n}$ denote some symmetric, and $B\in\mathbb{R}^{n\times n}$ some positive-definite matrix. The generalized eigenvalue problem, $[A, B]$, corresponds to a scalar-vector pair, $(\lambda, u)$, satisfying $$Au=\lambda Bu.$$ What is the property of generalized eigenvectors $u$, e.g.,  are they mutually orthogonal? Are they somehow related to eigenvectors of $A$ (or $B$ )?","Let $A\in\mathbb{R}^{n\times n}$ denote some symmetric, and $B\in\mathbb{R}^{n\times n}$ some positive-definite matrix. The generalized eigenvalue problem, $[A, B]$, corresponds to a scalar-vector pair, $(\lambda, u)$, satisfying $$Au=\lambda Bu.$$ What is the property of generalized eigenvectors $u$, e.g.,  are they mutually orthogonal? Are they somehow related to eigenvectors of $A$ (or $B$ )?",,"['linear-algebra', 'matrices', 'vector-spaces']"
90,Linear Independence & Group Theory,Linear Independence & Group Theory,,"Given an elementary abelian p-group $G$ .  Can someone please explain me why it can be seen as a vector space over $\mathbb{Z}_p $ ?  It should be elementary but I can't figure it out. As a consequence, can  someone please give me some advice for possible papers / theorem that prove some kind of linear independence in the group context when regarding a group as a vector space? (something like proving that some elements in a group $G$ are linearly independent mod $\phi(G)$ when $ \phi(G) $ is the Frattini -group of $G$ ) . Can someone help me? Thanks in advance !","Given an elementary abelian p-group $G$ .  Can someone please explain me why it can be seen as a vector space over $\mathbb{Z}_p $ ?  It should be elementary but I can't figure it out. As a consequence, can  someone please give me some advice for possible papers / theorem that prove some kind of linear independence in the group context when regarding a group as a vector space? (something like proving that some elements in a group $G$ are linearly independent mod $\phi(G)$ when $ \phi(G) $ is the Frattini -group of $G$ ) . Can someone help me? Thanks in advance !",,"['linear-algebra', 'group-theory']"
91,$2$ question on matrix with some condition,question on matrix with some condition,2,"we need to tell whether $a,b$ true or false, I know that there does not exist $n\times n$ $A,B$ such that $(AB-BA)=I$,as if we take trace of both side they are not equal, so $a$ is false? $b$ I have no idea, could any one give me hint?","we need to tell whether $a,b$ true or false, I know that there does not exist $n\times n$ $A,B$ such that $(AB-BA)=I$,as if we take trace of both side they are not equal, so $a$ is false? $b$ I have no idea, could any one give me hint?",,"['linear-algebra', 'matrices']"
92,Quantum Information: Deutsch-Jozsa Algorithm,Quantum Information: Deutsch-Jozsa Algorithm,,"There is a step in the construction of this algorithm which I'm not understanding: $\displaystyle \left[\sum_x  \frac{| x \rangle}{\sqrt{2^n}}\right]\left[\frac{ | 0 \rangle -| 1 \rangle }{\sqrt{2}} \oplus | f(x) \rangle\right]=\sum_x \frac{(-1)^{f(x)} | x \rangle }{\sqrt{2^n}}\left(\frac{| 0 \rangle - |1 \rangle}{\sqrt{2}}\right)$ where $f:\{0,1\}^n \to \{0,1\}$ . I don't see how they are equal, and I believe part of my confusion is on how $\oplus$ work for $2$ -qubits. $\oplus$ denotes addition $\!\!\!\!\mod 2$ ... so does it work in each element separately?","There is a step in the construction of this algorithm which I'm not understanding: where . I don't see how they are equal, and I believe part of my confusion is on how work for -qubits. denotes addition ... so does it work in each element separately?","\displaystyle \left[\sum_x  \frac{| x \rangle}{\sqrt{2^n}}\right]\left[\frac{ | 0 \rangle -| 1 \rangle }{\sqrt{2}} \oplus | f(x) \rangle\right]=\sum_x \frac{(-1)^{f(x)} | x \rangle }{\sqrt{2^n}}\left(\frac{| 0 \rangle - |1 \rangle}{\sqrt{2}}\right) f:\{0,1\}^n \to \{0,1\} \oplus 2 \oplus \!\!\!\!\mod 2","['linear-algebra', 'algorithms', 'quantum-mechanics', 'quantum-computation']"
93,Is every nilpotent linear group triangularizable?,Is every nilpotent linear group triangularizable?,,"Is it true that every finitely generated nilpotent group of matrices over $\mathbb C$ is conjugated to a subgroup of the upper triangular group? If yes, what is a reference for that?","Is it true that every finitely generated nilpotent group of matrices over $\mathbb C$ is conjugated to a subgroup of the upper triangular group? If yes, what is a reference for that?",,"['linear-algebra', 'abstract-algebra', 'group-theory']"
94,How to show $A$ cannot have more than one Jordan block for any eigenvalue?,How to show  cannot have more than one Jordan block for any eigenvalue?,A,"I got stuck in this problem from Spring 99, Berkeley Problems in Mathematics: Let $A$ be a $n\times n$ matrix such that $a_{ij}\not=0$ if $i=j+1$ but $a_{ij}=0$ if $i\ge j+2$. Prove that $A$ cannot have more than one Jordan block for any eigenvalue. I thought the matrix would satisfy some obvious relationship like $A^{2}=0$, but I realized the entries not listed are not even specified; thus such a gross simplification cannot hold. Working on toy examples does not tell me much, so I decided to ask in here.","I got stuck in this problem from Spring 99, Berkeley Problems in Mathematics: Let $A$ be a $n\times n$ matrix such that $a_{ij}\not=0$ if $i=j+1$ but $a_{ij}=0$ if $i\ge j+2$. Prove that $A$ cannot have more than one Jordan block for any eigenvalue. I thought the matrix would satisfy some obvious relationship like $A^{2}=0$, but I realized the entries not listed are not even specified; thus such a gross simplification cannot hold. Working on toy examples does not tell me much, so I decided to ask in here.",,['linear-algebra']
95,"Please recommend books on calculus, linear algebra, statistics for someone trying to learn Probability Theory and Machine Learning?","Please recommend books on calculus, linear algebra, statistics for someone trying to learn Probability Theory and Machine Learning?",,"I am tackling some topics in Probability Theory and Machine Learning and while I have plenty of resources dedicated to those disciplines I am lacking in a good basic math foundation. Does anyone know any good, concise math books that can help introduce the foundations (calculus, linear algebra, statistics) of these disciplines to someone whose exposure to math is very limited? Of particular interest would be a book that could relate these concepts to someone familiar with programming to leverage that mode of thinking to relate the essential ideas.","I am tackling some topics in Probability Theory and Machine Learning and while I have plenty of resources dedicated to those disciplines I am lacking in a good basic math foundation. Does anyone know any good, concise math books that can help introduce the foundations (calculus, linear algebra, statistics) of these disciplines to someone whose exposure to math is very limited? Of particular interest would be a book that could relate these concepts to someone familiar with programming to leverage that mode of thinking to relate the essential ideas.",,"['calculus', 'linear-algebra', 'reference-request', 'probability-theory', 'machine-learning']"
96,Complex matrices with null trace [duplicate],Complex matrices with null trace [duplicate],,This question already has an answer here : If $\mathrm{Tr}(A)=0$ then $T=R^{-1}AR$ has all entries on its main diagonal equal to $0$ (1 answer) Closed 9 years ago . I'm trying to prove the following: Let $A\in \mathbb{C}^{n\times n}$ be a matrix with null trace; then $A$ is similar to a matrix $B$ such that $B_{jj}=0$ (i.e. it has zeroes on its diagonal). Any ideas? Induction on $n$ sounded feasible but I wasn't able to put together anything.,This question already has an answer here : If $\mathrm{Tr}(A)=0$ then $T=R^{-1}AR$ has all entries on its main diagonal equal to $0$ (1 answer) Closed 9 years ago . I'm trying to prove the following: Let $A\in \mathbb{C}^{n\times n}$ be a matrix with null trace; then $A$ is similar to a matrix $B$ such that $B_{jj}=0$ (i.e. it has zeroes on its diagonal). Any ideas? Induction on $n$ sounded feasible but I wasn't able to put together anything.,,['linear-algebra']
97,Is this function convex when the input vector is positive?,Is this function convex when the input vector is positive?,,"I am wondering if $f(\mathbf{x})$ is convex on the input of a vector of $n$ positive reals $\mathbf{x}$: $$f(\mathbf{x})=\operatorname{Tr}[(\mathbf{A}+\operatorname{diag}(\mathbf{x}))^{-1}]$$ where $\mathbf{A}$ is a positive-definite $n\times n$ matrix of reals, $\operatorname{diag}(\mathbf{x})$ returns a diagonal matrix with values from $\mathbf{x}$ placed on the diagonal, and $\operatorname{Tr}[\mathbf{M}]$ is the trace of matrix $\mathbf{M}$. I understand that I can prove convexity via taking the second derivative, but I am not sure how to it in the matrix case.  I also know that for positive-definite $\mathbf{A}$, $g(\mathbf{y})=\frac{1}{2}\mathbf{y}^T\mathbf{A}\mathbf{y}+\mathbf{y}^T\mathbf{b}$ is strictly convex, but I am not sure how that applies here (if it applies at all).","I am wondering if $f(\mathbf{x})$ is convex on the input of a vector of $n$ positive reals $\mathbf{x}$: $$f(\mathbf{x})=\operatorname{Tr}[(\mathbf{A}+\operatorname{diag}(\mathbf{x}))^{-1}]$$ where $\mathbf{A}$ is a positive-definite $n\times n$ matrix of reals, $\operatorname{diag}(\mathbf{x})$ returns a diagonal matrix with values from $\mathbf{x}$ placed on the diagonal, and $\operatorname{Tr}[\mathbf{M}]$ is the trace of matrix $\mathbf{M}$. I understand that I can prove convexity via taking the second derivative, but I am not sure how to it in the matrix case.  I also know that for positive-definite $\mathbf{A}$, $g(\mathbf{y})=\frac{1}{2}\mathbf{y}^T\mathbf{A}\mathbf{y}+\mathbf{y}^T\mathbf{b}$ is strictly convex, but I am not sure how that applies here (if it applies at all).",,"['linear-algebra', 'matrices', 'derivatives', 'convex-analysis']"
98,Maximizing a quadratic function subject to $\| x \|_2 \le 1$,Maximizing a quadratic function subject to,\| x \|_2 \le 1,"Consider the $n$-dimensional quadratically constrained quadratic optimization problem $$\begin{array}{ll} \text{maximize} & \frac12 x^T A x + b^T x\\ \text{subject to} & \| x \|_2 \le 1\end{array}$$ where $A$ is a symmetric $n\times n$ matrix that may be indefinite. Given the symmetry of the constraint, is there a nice closed-form solution, perhaps in terms of the eigendecomposition of $A$?","Consider the $n$-dimensional quadratically constrained quadratic optimization problem $$\begin{array}{ll} \text{maximize} & \frac12 x^T A x + b^T x\\ \text{subject to} & \| x \|_2 \le 1\end{array}$$ where $A$ is a symmetric $n\times n$ matrix that may be indefinite. Given the symmetry of the constraint, is there a nice closed-form solution, perhaps in terms of the eigendecomposition of $A$?",,"['linear-algebra', 'optimization']"
99,Division of a matrix polynomial,Division of a matrix polynomial,,"I'm trying to show: Let $C(t)=C_rt^r+C_{r-1}t^{r-1}+\cdots+C_1t^1+C_0\in \mathcal{M}_n(\mathbb{F}[t])$ a polynomial with coefficients $C_{i}$ in $\mathcal{M}_n(\mathbb{F})$. Show that, exists a matrix polynomial $Q(t)$ such that: $$C(t)=Q(t)(tI_n-A)+C(A)$$ That means, the remainder of the division 'to the right' of $C(t)$ by $(tI_n-A)$ is the matrix: $$C(A)=C_rA^r+C_{r-1}A^{r-1}+\cdots+C_1A^1+C_0$$ Actually, I'm a little confused with this exercise. It says nothing about $(tI_n-A)$. Thanks for your help.","I'm trying to show: Let $C(t)=C_rt^r+C_{r-1}t^{r-1}+\cdots+C_1t^1+C_0\in \mathcal{M}_n(\mathbb{F}[t])$ a polynomial with coefficients $C_{i}$ in $\mathcal{M}_n(\mathbb{F})$. Show that, exists a matrix polynomial $Q(t)$ such that: $$C(t)=Q(t)(tI_n-A)+C(A)$$ That means, the remainder of the division 'to the right' of $C(t)$ by $(tI_n-A)$ is the matrix: $$C(A)=C_rA^r+C_{r-1}A^{r-1}+\cdots+C_1A^1+C_0$$ Actually, I'm a little confused with this exercise. It says nothing about $(tI_n-A)$. Thanks for your help.",,['linear-algebra']
