,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What is the probability that you never lose this hypothetical dice game? [closed],What is the probability that you never lose this hypothetical dice game? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question You have a four sided die numbered 1-4 and are playing a game. On the first round, you roll the die once. If you get a 1 you lose the die. If you get a  2 you keep the die. If you get a 3 or 4 you get another, identical die. On the second round you roll each die that you have, and the same thing happens with each die. Once you finish rolling all your dice you move on to the third round and so on. If you have zero dice you lose the game. What is the probability that you never lose the game over infinite rounds?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question You have a four sided die numbered 1-4 and are playing a game. On the first round, you roll the die once. If you get a 1 you lose the die. If you get a  2 you keep the die. If you get a 3 or 4 you get another, identical die. On the second round you roll each die that you have, and the same thing happens with each die. Once you finish rolling all your dice you move on to the third round and so on. If you have zero dice you lose the game. What is the probability that you never lose the game over infinite rounds?",,[]
1,Probability of taking balls without replacement from a bag question,Probability of taking balls without replacement from a bag question,,"A bag contains $N$ balls, $2$ of which are red. Balls are removed, one by one, (without replacement), stopping when both red balls have emerged. Find the probability that exactly $n$ balls are removed. I'm honestly not sure where to even start. I know it's an intersection of ( $1$ Red ball in $n-1$ attempts) and (red on $n$ -th attempt). I think the $Pr(\text{Red on}\;n\text{-th attempt}\; | \;1 \; \text{Red already})$ is $$ \frac{1}{N-(n-1)} $$ but I'm not sure for the other probability, or whether this one is correct for that matter. Any help would be appreciated. :)","A bag contains balls, of which are red. Balls are removed, one by one, (without replacement), stopping when both red balls have emerged. Find the probability that exactly balls are removed. I'm honestly not sure where to even start. I know it's an intersection of ( Red ball in attempts) and (red on -th attempt). I think the is but I'm not sure for the other probability, or whether this one is correct for that matter. Any help would be appreciated. :)",N 2 n 1 n-1 n Pr(\text{Red on}\;n\text{-th attempt}\; | \;1 \; \text{Red already})  \frac{1}{N-(n-1)} ,"['probability', 'statistics']"
2,Conditional probability on zero probability events (Definition),Conditional probability on zero probability events (Definition),,"Let $(\Omega ,{\mathcal {F}},P)$ be a probability space, $X$ is a $(M,{\mathcal {M}})$-value random variable, $Y$ is a $(N,{\mathcal {N}})$-value random variable, and $f$ is a measurable function from $M \mapsto N$. We don't know about the existence of the joint probability density of $(X, f(X))$. Is $P(X \in E \vert f(X)=y)$ well-defined? In otherwords, does this limit exist or not: $\lim_{r \mapsto 0} P(X \in E \vert f(X) \in (y-r, y+r))$? This question is about the definition of conditional probability on zero probability events. Even if, it is well-defined, the application of the definition is not clear to me. I asked a question about the application of conditional probability on zero probability events here .","Let $(\Omega ,{\mathcal {F}},P)$ be a probability space, $X$ is a $(M,{\mathcal {M}})$-value random variable, $Y$ is a $(N,{\mathcal {N}})$-value random variable, and $f$ is a measurable function from $M \mapsto N$. We don't know about the existence of the joint probability density of $(X, f(X))$. Is $P(X \in E \vert f(X)=y)$ well-defined? In otherwords, does this limit exist or not: $\lim_{r \mapsto 0} P(X \in E \vert f(X) \in (y-r, y+r))$? This question is about the definition of conditional probability on zero probability events. Even if, it is well-defined, the application of the definition is not clear to me. I asked a question about the application of conditional probability on zero probability events here .",,"['probability', 'probability-theory', 'measure-theory', 'stochastic-processes']"
3,Choosing code using digits and letters,Choosing code using digits and letters,,"Want to put together a secret code. The code consists of 2 different digits and 3 different English letters (26 options).   How many different codes can be put together? I tried to think of it this way: $$10*9*26*25*24=1,404,000$$ Because I first choose two digits and then three letters. The answer by the book is: $$\binom{10}{2}\binom{26}{3}*5!=14,040,000 $$ I understand why they did it, but I do not know what I'm missing to get answer similar to their own. Thank you very much.","Want to put together a secret code. The code consists of 2 different digits and 3 different English letters (26 options).   How many different codes can be put together? I tried to think of it this way: $$10*9*26*25*24=1,404,000$$ Because I first choose two digits and then three letters. The answer by the book is: $$\binom{10}{2}\binom{26}{3}*5!=14,040,000 $$ I understand why they did it, but I do not know what I'm missing to get answer similar to their own. Thank you very much.",,"['probability', 'combinatorics']"
4,Sum of weighted chi square distributions,Sum of weighted chi square distributions,,Let $X_1 \sim \chi_{k}^2$ and $X_2 \sim \chi_{k}^2$ be i.i.d and both $a_1$ and $a_2$ positive real values. How can be expressed the PDF of $Y = a_1X_1 + a_2X_2$? Is it also a chi-square distribution? thanks,Let $X_1 \sim \chi_{k}^2$ and $X_2 \sim \chi_{k}^2$ be i.i.d and both $a_1$ and $a_2$ positive real values. How can be expressed the PDF of $Y = a_1X_1 + a_2X_2$? Is it also a chi-square distribution? thanks,,"['probability', 'probability-theory', 'probability-distributions']"
5,Odds of anyone in a group getting picked twice in a row,Odds of anyone in a group getting picked twice in a row,,"I'm going to make this question a little generic, but it's for a specific situation: You have a list of 10,000 people. Every week, you randomly select 2% (200) of those people. What are the odds of one or more of those 2% getting picked the following week? I initially thought the solution was simply, but as I run it through my head, I'm starting to think it's not so simple. I've been trying to brush up on my probability math, but as it's been about 100 years since I took probability and statistics, I just can't seem to find the way to solve it and would appreciate, not just the answer, but the method of solving the problem.","I'm going to make this question a little generic, but it's for a specific situation: You have a list of 10,000 people. Every week, you randomly select 2% (200) of those people. What are the odds of one or more of those 2% getting picked the following week? I initially thought the solution was simply, but as I run it through my head, I'm starting to think it's not so simple. I've been trying to brush up on my probability math, but as it's been about 100 years since I took probability and statistics, I just can't seem to find the way to solve it and would appreciate, not just the answer, but the method of solving the problem.",,['probability']
6,Expected distance between two vectors that belong to two different Gaussian distributions,Expected distance between two vectors that belong to two different Gaussian distributions,,"Let $X$, $Y$ be two random variables that follow the Gaussian distributions with mean vectors $\mu_x$, $\mu_y$, and covariance matrices $\Sigma_x$, $\Sigma_y$, respectively. The probability density functions of $X$, $Y$ are given, respectively, by $$ f_{X}(\mathbf{x})=\frac{1}{(2\pi)^{\frac{n}{2}}\lvert \Sigma_x \rvert^{\frac{1}{2}}} \exp\Big\{-\frac{1}{2}(\mathbf{x}-\mu_x)^\top\Sigma_x^{-1}(\mathbf{x}-\mu_x)\Big\}, $$ and $$ f_{Y}(\mathbf{y})=\frac{1}{(2\pi)^{\frac{n}{2}}\lvert \Sigma_y \rvert^{\frac{1}{2}}} \exp\Big\{-\frac{1}{2}(\mathbf{y}-\mu_y)^\top\Sigma_x^{-1}(\mathbf{y}-\mu_y)\Big\}, $$ where $\mathbf{x},\mathbf{y}\in\Bbb{R}^n$. We will be thinking of $\mathbf{x}$, $\mathbf{y}$ as ""members"" of the distributions $X$, $Y$, respectively. If we have two fixed vectors, say $\mathbf{x}$, $\mathbf{y}$, then the squared Euclidean distance between them would be equal to $$ \big\lVert \mathbf{x} - \mathbf{y} \big\rVert^2. $$ If we think about $\mathbf{x}$, $\mathbf{y}$ as above, i.e., as members of $X$, $Y$, respectively, then what would be the expected value of this distance ? Thank you very much for your help!","Let $X$, $Y$ be two random variables that follow the Gaussian distributions with mean vectors $\mu_x$, $\mu_y$, and covariance matrices $\Sigma_x$, $\Sigma_y$, respectively. The probability density functions of $X$, $Y$ are given, respectively, by $$ f_{X}(\mathbf{x})=\frac{1}{(2\pi)^{\frac{n}{2}}\lvert \Sigma_x \rvert^{\frac{1}{2}}} \exp\Big\{-\frac{1}{2}(\mathbf{x}-\mu_x)^\top\Sigma_x^{-1}(\mathbf{x}-\mu_x)\Big\}, $$ and $$ f_{Y}(\mathbf{y})=\frac{1}{(2\pi)^{\frac{n}{2}}\lvert \Sigma_y \rvert^{\frac{1}{2}}} \exp\Big\{-\frac{1}{2}(\mathbf{y}-\mu_y)^\top\Sigma_x^{-1}(\mathbf{y}-\mu_y)\Big\}, $$ where $\mathbf{x},\mathbf{y}\in\Bbb{R}^n$. We will be thinking of $\mathbf{x}$, $\mathbf{y}$ as ""members"" of the distributions $X$, $Y$, respectively. If we have two fixed vectors, say $\mathbf{x}$, $\mathbf{y}$, then the squared Euclidean distance between them would be equal to $$ \big\lVert \mathbf{x} - \mathbf{y} \big\rVert^2. $$ If we think about $\mathbf{x}$, $\mathbf{y}$ as above, i.e., as members of $X$, $Y$, respectively, then what would be the expected value of this distance ? Thank you very much for your help!",,"['probability', 'random-variables', 'normal-distribution']"
7,How many ways can $10$ digits be written down so that no even digit is in its original position,How many ways can  digits be written down so that no even digit is in its original position,10,"If I have the numbers $0,1,2,3,4,5,6,7,8,9$ written down in that order, how many ways can the $10$ digits be written down so that no even digit is in its original position? It would seem that I can move rewrite it starting from $0$ in $9!$ ways, and same with $2,4,6,8$, hence the answer is $5*9!=1814400$ is this correct?","If I have the numbers $0,1,2,3,4,5,6,7,8,9$ written down in that order, how many ways can the $10$ digits be written down so that no even digit is in its original position? It would seem that I can move rewrite it starting from $0$ in $9!$ ways, and same with $2,4,6,8$, hence the answer is $5*9!=1814400$ is this correct?",,"['probability', 'combinatorics', 'statistics', 'solution-verification']"
8,A seeming paradox in a coin-flipping game,A seeming paradox in a coin-flipping game,,"This is related to my other question on a similar topic. Suppose we play the following game: we flip a coin repeatedly and record the outcomes.  For example we might get HHTTTHTTHHTTT... .  Now Alice and Bob each choose distinct patterns of the same length, called $A$ and $B$, respectively.  Which ever player's pattern appears first wins the game. Now suppose Alice chooses $A=$ HHHH and Bob chooses $B=$ HHHT .  First, let's notice that the expected number of flips to obtain $A$ is 30, but for $B$ it's only 16.  This would seem to imply that Bob is very likely to win this game most of the time. However, thinking about the game in another way, neither Alice nor Bob can win until HHH occurs.  And after this, the game ends on the next flip with each player winning equiprobably. This seems counter-intuitive to me: we have two events, one expected to occur much sooner than the other, but relative to each other the ordering is 50-50.  What am I missing?","This is related to my other question on a similar topic. Suppose we play the following game: we flip a coin repeatedly and record the outcomes.  For example we might get HHTTTHTTHHTTT... .  Now Alice and Bob each choose distinct patterns of the same length, called $A$ and $B$, respectively.  Which ever player's pattern appears first wins the game. Now suppose Alice chooses $A=$ HHHH and Bob chooses $B=$ HHHT .  First, let's notice that the expected number of flips to obtain $A$ is 30, but for $B$ it's only 16.  This would seem to imply that Bob is very likely to win this game most of the time. However, thinking about the game in another way, neither Alice nor Bob can win until HHH occurs.  And after this, the game ends on the next flip with each player winning equiprobably. This seems counter-intuitive to me: we have two events, one expected to occur much sooner than the other, but relative to each other the ordering is 50-50.  What am I missing?",,['probability']
9,Run of $N$ successes before run of $k$ failures [closed],Run of  successes before run of  failures [closed],N k,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question What is the probability that a run of $N$ consecutive successes will occur before a run of $k$ consecutive failures when each trial has a probability $p$ of success and $q=1-p$ of failure?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question What is the probability that a run of $N$ consecutive successes will occur before a run of $k$ consecutive failures when each trial has a probability $p$ of success and $q=1-p$ of failure?",,['probability']
10,What is the frequentist's Bayesian prior for a coin with unknown bias,What is the frequentist's Bayesian prior for a coin with unknown bias,,"A ""coin"" has a fixed unknown bias $0\le p\le1$ for heads, and out of $n\ge0$ tosses it yielded $0\le h\le n$ heads. Note that this occurs with probability $P(h\;|\;p,n)=\binom{n}{h}p^h(1-p)^{n-h}$ . We would like a ""best guess"" for $p$ . The frequentist view is that $p$ should be the maximum-likelihood-estimate $\frac hn$ . Indeed $\frac{d}{d\rho}\binom{n}{h}\rho^h(1-\rho)^{n-h}=0$ occurs at $\rho=\frac hn$ . The uniform Bayesian view is that $p$ should be $\frac{h+1}{n+2}$ . Indeed it has prior distribution $f(p)=1$ and the posterior distribution conditional on $(n,h)$ is then a Beta distribution $f(p\;|\;n,h)=\frac{P(h\;|\;p,n)f(p)}{\int_0^1P(h\;|\;\rho,n)f(\rho)d\rho}=\frac{(n+1)!}{h!(n-h)!}p^h(1-p)^{n-h}$ hence $\mathbb E[p\;|\;n,h]=\frac{(n+1)!}{h!(n-h)!}\int_0^1\rho^{h+1}(1-\rho)^{n-h}d\rho=\frac{(n+1)!}{h!(n-h)!}\frac{(h+1)!(n-h)!}{(n+2)!}=\frac{h+1}{n+2}$ . I don't yet have intuition for why these two viewpoints are the same if and only if $n=2h$ , let me know! But my main question is: What is the frequentist's prior , i.e. what distribution $f$ satisfies $\mathbb E_f[p\;|\;n,h]=\frac hn$ for all pairs $\lbrace(n,h)\in\mathbb Z^2\;|\; 0\le h\le n\rbrace$ ? Rephrased, $n\int_0^1\rho^{h+1}(1-\rho)^{n-h}f(\rho)d\rho=h\int_0^1\rho^h(1-\rho)^{n-h}f(\rho)d\rho$ . Taking $(n,h)=(1,0)$ forces $f$ to obey $\int_0^1\rho(1-\rho)f(\rho)d\rho=0$ and so under some natural assumptions on the non-negative $f$ this should mean $f$ is almost-everywhere zero and not a normalized sum of Dirac-deltas. I believe this is Qiaochu's answer below. This would be poetic and intuitive: a frequentist by construction would have no a priori guess, consistent with the fact that the vacuous 0 heads out of 0 tosses has undefined quotient $\frac00$ (whereas the uniform Bayesian invokes symmetry to guess $\frac12=\frac{0+1}{0+2}$ ).","A ""coin"" has a fixed unknown bias for heads, and out of tosses it yielded heads. Note that this occurs with probability . We would like a ""best guess"" for . The frequentist view is that should be the maximum-likelihood-estimate . Indeed occurs at . The uniform Bayesian view is that should be . Indeed it has prior distribution and the posterior distribution conditional on is then a Beta distribution hence . I don't yet have intuition for why these two viewpoints are the same if and only if , let me know! But my main question is: What is the frequentist's prior , i.e. what distribution satisfies for all pairs ? Rephrased, . Taking forces to obey and so under some natural assumptions on the non-negative this should mean is almost-everywhere zero and not a normalized sum of Dirac-deltas. I believe this is Qiaochu's answer below. This would be poetic and intuitive: a frequentist by construction would have no a priori guess, consistent with the fact that the vacuous 0 heads out of 0 tosses has undefined quotient (whereas the uniform Bayesian invokes symmetry to guess ).","0\le p\le1 n\ge0 0\le h\le n P(h\;|\;p,n)=\binom{n}{h}p^h(1-p)^{n-h} p p \frac hn \frac{d}{d\rho}\binom{n}{h}\rho^h(1-\rho)^{n-h}=0 \rho=\frac hn p \frac{h+1}{n+2} f(p)=1 (n,h) f(p\;|\;n,h)=\frac{P(h\;|\;p,n)f(p)}{\int_0^1P(h\;|\;\rho,n)f(\rho)d\rho}=\frac{(n+1)!}{h!(n-h)!}p^h(1-p)^{n-h} \mathbb E[p\;|\;n,h]=\frac{(n+1)!}{h!(n-h)!}\int_0^1\rho^{h+1}(1-\rho)^{n-h}d\rho=\frac{(n+1)!}{h!(n-h)!}\frac{(h+1)!(n-h)!}{(n+2)!}=\frac{h+1}{n+2} n=2h f \mathbb E_f[p\;|\;n,h]=\frac hn \lbrace(n,h)\in\mathbb Z^2\;|\; 0\le h\le n\rbrace n\int_0^1\rho^{h+1}(1-\rho)^{n-h}f(\rho)d\rho=h\int_0^1\rho^h(1-\rho)^{n-h}f(\rho)d\rho (n,h)=(1,0) f \int_0^1\rho(1-\rho)f(\rho)d\rho=0 f f \frac00 \frac12=\frac{0+1}{0+2}","['probability', 'probability-distributions', 'statistical-inference']"
11,"Calculating $\mathbb{E}[N]$ for $N = \displaystyle \min_{n\in \mathbb{N}}\Big\{\sum_{i=1}^{n}{X_i\geq5000\Big\}}$, using Wald's lemma","Calculating  for , using Wald's lemma",\mathbb{E}[N] N = \displaystyle \min_{n\in \mathbb{N}}\Big\{\sum_{i=1}^{n}{X_i\geq5000\Big\}},"Suppose I have a sequence of $i.i.d$ random variables: $X_1,X_2,... \sim Geom(p)$ . That means that each of the $X_i$ 's is holding an unknown random number of trials until a 'success'. Since $0<p<1$ , we know that $X_i$ has a finite expectation $\mathbb{E}[X_i] = \frac{1}{p}$ , which also means that there exists an integer $N \in \mathbb{N}$ such that: $$N = \displaystyle \min_{n\in \mathbb{N}}\Big\{X_1+X_2+...+X_n= \sum_{i=1}^{n}{X_i\geq5000}\Big\}$$ We would like to calculate the expectation of this finite integer 'stopping time' $N$ . From Wald's lemma: If $X_i$ are i.i.d. with finite $\mathbb{E}[X_i] = \mu$ , and N is a finite stopping time then: $\mathbb{E}\Big[\sum_{i=1}^{N}{X_i}\Big] = \mu\mathbb{E}[N]$ . My problem is how to deal with the 'greater-equal' ( $\geq$ ) sign. Since we define $N = \displaystyle \min_{n\in \mathbb{N}}\Big\{\sum_{i=1}^{n}{X_i\geq5000\Big\}}$ , this means that $X_N$ contibutes a number of trials with which the sum exceeds $5000$ , but we dont know the exact sum. My intuition is something like: if we take the the sum as the bare minimum, then $$\mathbb{E}\Big[\sum_{i=1}^{N}{X_i}\Big] = 5000= \mathbb{E}[N]\times \frac{1}{p}  \to  \mathbb{E}[N] = 5000p$$ But even if that's the case I'm having trouble justifying taking the sum as exactly 5000. Another possible approach is to condition on $\sum_{i=1}^{N}{X_i}=k$ and take the expectation, but $k = 5000, 5001,...$ and I'm not sure how to formulate this, since $k$ is potentially unbounded ( $k\in [5000,\infty)$ ), if that's even a valid approach. I'd love some guidance please.","Suppose I have a sequence of random variables: . That means that each of the 's is holding an unknown random number of trials until a 'success'. Since , we know that has a finite expectation , which also means that there exists an integer such that: We would like to calculate the expectation of this finite integer 'stopping time' . From Wald's lemma: If are i.i.d. with finite , and N is a finite stopping time then: . My problem is how to deal with the 'greater-equal' ( ) sign. Since we define , this means that contibutes a number of trials with which the sum exceeds , but we dont know the exact sum. My intuition is something like: if we take the the sum as the bare minimum, then But even if that's the case I'm having trouble justifying taking the sum as exactly 5000. Another possible approach is to condition on and take the expectation, but and I'm not sure how to formulate this, since is potentially unbounded ( ), if that's even a valid approach. I'd love some guidance please.","i.i.d X_1,X_2,... \sim Geom(p) X_i 0<p<1 X_i \mathbb{E}[X_i] = \frac{1}{p} N \in \mathbb{N} N = \displaystyle \min_{n\in \mathbb{N}}\Big\{X_1+X_2+...+X_n= \sum_{i=1}^{n}{X_i\geq5000}\Big\} N X_i \mathbb{E}[X_i] = \mu \mathbb{E}\Big[\sum_{i=1}^{N}{X_i}\Big] = \mu\mathbb{E}[N] \geq N = \displaystyle \min_{n\in \mathbb{N}}\Big\{\sum_{i=1}^{n}{X_i\geq5000\Big\}} X_N 5000 \mathbb{E}\Big[\sum_{i=1}^{N}{X_i}\Big] = 5000= \mathbb{E}[N]\times \frac{1}{p}  \to  \mathbb{E}[N] = 5000p \sum_{i=1}^{N}{X_i}=k k = 5000, 5001,... k k\in [5000,\infty)","['probability', 'expected-value', 'stopping-times']"
12,"Harvard Stat 110 Strategic Practice 2, Fall 2011 - Inclusion Exclusion - Problem 1.1","Harvard Stat 110 Strategic Practice 2, Fall 2011 - Inclusion Exclusion - Problem 1.1",,"Harvard Stat 110 Strategic Practice 2, Fall 2011 - Inclusion Exclusion - Problem 1.1 For a group of $7$ people, find the probability that all $4$ seasons (winter, spring, summer, fall) occur at least once each among their birthdays, assuming that all seasons are equally likely. I tried to solve it using another method (which came into my mind at that moment) but I must be doing a mistake. Maybe someone could help me finding the error and also explaining WHY I make the error, so I can avoid it in the future. I tried to apply the naive definition. So, I have $7$ people and $4$ seasons to choose. 1st person can have $4$ picks, the 2nd $4$ picks, etc etc. All are independent, so I have a total of $4^7$ possibilities. The number of favourable outcomes are when from the $7$ people, $4$ have each Spring, Summer, Fall, Winter and the other $3$ might get any choice. From $7$ people, I chose $4$ to fill Spring, Summer, Fall, Winter and the other $3$ can have whatever choice. So, result should be $$\binom{7}{4} \cdot \frac{4^3}{4^7}$$ which yields $0.546$ which is clearly different from the practice answer of $0.513$ . Could somebody, please, point out what I am doing wrong? Thank you!","Harvard Stat 110 Strategic Practice 2, Fall 2011 - Inclusion Exclusion - Problem 1.1 For a group of people, find the probability that all seasons (winter, spring, summer, fall) occur at least once each among their birthdays, assuming that all seasons are equally likely. I tried to solve it using another method (which came into my mind at that moment) but I must be doing a mistake. Maybe someone could help me finding the error and also explaining WHY I make the error, so I can avoid it in the future. I tried to apply the naive definition. So, I have people and seasons to choose. 1st person can have picks, the 2nd picks, etc etc. All are independent, so I have a total of possibilities. The number of favourable outcomes are when from the people, have each Spring, Summer, Fall, Winter and the other might get any choice. From people, I chose to fill Spring, Summer, Fall, Winter and the other can have whatever choice. So, result should be which yields which is clearly different from the practice answer of . Could somebody, please, point out what I am doing wrong? Thank you!",7 4 7 4 4 4 4^7 7 4 3 7 4 3 \binom{7}{4} \cdot \frac{4^3}{4^7} 0.546 0.513,"['probability', 'combinatorics', 'inclusion-exclusion']"
13,Probability of rolling a 6 immediately after a 1 is rolled,Probability of rolling a 6 immediately after a 1 is rolled,,"Question Ann and Bob take turns to roll a fair six-sided die. The winner is the first person to roll a six immediately after the other person has rolled a one. Ann will go first. Find the probability that Ann will win. Answer $\mathbb{P} (\mathrm {Ann\ wins}) = \frac {36} {73}$ I have thought long and hard about this question but I am unable to even start. I have tried considering cases, but got stuck along the way. For example, it is trivial to calculate the probability that Ann wins if there are only three rolls (which is the minimum number of rolls needed for Ann to win). However, the problem easily becomes very complicated when we consider five rolls and more. The suggested solution by my professor uses first-step decomposition, but it is a new concept to me and I am struggling to understand it. If anyone can provide a detailed and intuitive explanation as to how this problem should be solved, that will be greatly appreciated!","Question Ann and Bob take turns to roll a fair six-sided die. The winner is the first person to roll a six immediately after the other person has rolled a one. Ann will go first. Find the probability that Ann will win. Answer I have thought long and hard about this question but I am unable to even start. I have tried considering cases, but got stuck along the way. For example, it is trivial to calculate the probability that Ann wins if there are only three rolls (which is the minimum number of rolls needed for Ann to win). However, the problem easily becomes very complicated when we consider five rolls and more. The suggested solution by my professor uses first-step decomposition, but it is a new concept to me and I am struggling to understand it. If anyone can provide a detailed and intuitive explanation as to how this problem should be solved, that will be greatly appreciated!",\mathbb{P} (\mathrm {Ann\ wins}) = \frac {36} {73},"['probability', 'statistics', 'conditional-probability', 'dice']"
14,Expected length of orbit for random permutation,Expected length of orbit for random permutation,,"Pick a permutation $\sigma$ of $\{1,2,\dots,n\}$ uniformly at random. Let $m$ be the smallest positive integer such that $\sigma^m(1)=1$ . What is $\mathbb E[m]$ ? I know that the expected number of cycles of length $k$ for a random permutation is $\frac{1}{k}$ . This seems relevant (we want the expected length of the cycle containing $1$ ), but I'm not sure how. I manually computed the answer for some small values: $n=2$ the answer is $\frac{3}{2}$ . $n=3$ the answer is $2$ . $n=4$ the answer is $\frac{5}{2}$ . So from a (way too small) sample size, it looks like $\frac{n+1}{2}$ ?","Pick a permutation of uniformly at random. Let be the smallest positive integer such that . What is ? I know that the expected number of cycles of length for a random permutation is . This seems relevant (we want the expected length of the cycle containing ), but I'm not sure how. I manually computed the answer for some small values: the answer is . the answer is . the answer is . So from a (way too small) sample size, it looks like ?","\sigma \{1,2,\dots,n\} m \sigma^m(1)=1 \mathbb E[m] k \frac{1}{k} 1 n=2 \frac{3}{2} n=3 2 n=4 \frac{5}{2} \frac{n+1}{2}","['probability', 'permutations', 'expected-value']"
15,Combinatorics problems that can be solved more easily using probability,Combinatorics problems that can be solved more easily using probability,,"I'm looking for examples of combinatorics problems which would be very difficult to solve by direct enumeration, but can be easily solved using ideas from probability, like independence, commuting sums and expectations, etc.  I know I have seen such problems before, especially in the HMMT combinatorics subject tests, but I can't now recall any good examples. I am NOT looking for probabilistic existence proofs (the so-called "" probabilistic method "" introduced by Erdos).  The sort of problems I'm interested in are enumerative.","I'm looking for examples of combinatorics problems which would be very difficult to solve by direct enumeration, but can be easily solved using ideas from probability, like independence, commuting sums and expectations, etc.  I know I have seen such problems before, especially in the HMMT combinatorics subject tests, but I can't now recall any good examples. I am NOT looking for probabilistic existence proofs (the so-called "" probabilistic method "" introduced by Erdos).  The sort of problems I'm interested in are enumerative.",,"['probability', 'combinatorics', 'discrete-mathematics', 'big-list', 'probabilistic-method']"
16,Expected number of draws until first ace,Expected number of draws until first ace,,"The following question has appeared elsewhere on the site : What is the expected number of cards that need to be turned over in a regular $52$ -card deck in order to see the first ace? The correct answer is $10.6$ . However, I got something different from the following approach of conditional expectation: Let $N$ denote the random variable for the number of cards to be turned over to see the first ace. Also, let $R$ denote the random variable for the « value » of the card in the first round, i.e. the four aces have values $1$ to $4$ respectively and the other $48$ cards admit values $5$ to $52$ respectively. Therefore, by the tower property of conditional expectation, \begin{eqnarray} \mathbb{E}[N] & = & \sum_{i=1}^4 \mathbb{E} [N| R=i] \mathbb{P}(R=i) \\ &  & + \sum_{i=5}^{52} \mathbb{E} [N| R=i] \mathbb{P}(R=i) \\ & = & \sum_{i=1}^4 1 \big( \frac{1}{52} \big) + \sum_{i=5}^{52} \Big( 1 + \mathbb{E}[N] \Big) \Big( \frac{1}{52} \Big) \\ & = & \frac{4}{52} + \frac{48}{52} \Big( 1+ \mathbb{E}[N] \Big). \end{eqnarray} This gives $$ \mathbb{E} [N] = 13. $$ I fail to see any problems with this approach of conditional expectations, yet this does not give the correct answer. Any ideas?","The following question has appeared elsewhere on the site : What is the expected number of cards that need to be turned over in a regular -card deck in order to see the first ace? The correct answer is . However, I got something different from the following approach of conditional expectation: Let denote the random variable for the number of cards to be turned over to see the first ace. Also, let denote the random variable for the « value » of the card in the first round, i.e. the four aces have values to respectively and the other cards admit values to respectively. Therefore, by the tower property of conditional expectation, This gives I fail to see any problems with this approach of conditional expectations, yet this does not give the correct answer. Any ideas?","52 10.6 N R 1 4 48 5 52 \begin{eqnarray}
\mathbb{E}[N] & = & \sum_{i=1}^4 \mathbb{E} [N| R=i] \mathbb{P}(R=i) \\
&  & + \sum_{i=5}^{52} \mathbb{E} [N| R=i] \mathbb{P}(R=i) \\
& = & \sum_{i=1}^4 1 \big( \frac{1}{52} \big) + \sum_{i=5}^{52} \Big( 1 + \mathbb{E}[N] \Big) \Big( \frac{1}{52} \Big) \\
& = & \frac{4}{52} + \frac{48}{52} \Big( 1+ \mathbb{E}[N] \Big).
\end{eqnarray}  \mathbb{E} [N] = 13. ","['probability', 'expected-value']"
17,"From an urn containing a white and b black balls, balls are successively drawn without replacement until only those of the same colour are left.","From an urn containing a white and b black balls, balls are successively drawn without replacement until only those of the same colour are left.",,"From an urn containing a white and b black balls, balls are successively drawn without replacement until only those of the same colour are left. What is the probability that balls left are white? This is what I have done. ( https://i.sstatic.net/CZH2x.jpg ) But the answer which is given is : a/(a+b) I can't possibly make out how it's happening.","From an urn containing a white and b black balls, balls are successively drawn without replacement until only those of the same colour are left. What is the probability that balls left are white? This is what I have done. ( https://i.sstatic.net/CZH2x.jpg ) But the answer which is given is : a/(a+b) I can't possibly make out how it's happening.",,['probability']
18,Probability of Winning Election if Outcomes not Equally Likely,Probability of Winning Election if Outcomes not Equally Likely,,"I just started learning probability, so my level is not very high. I am doing a homework problem, and my answer is different than the book's. I can't understand why. I see how the answer in the book makes sense, but I also see how my procedure makes sense. Could someone please point at what I am doing wrong? Problem: Four candidates, A, B, C, and D, are running in an election. A is twice as likely to be elected as B. B and C are equally likely. C is twice as likely as D. What is the probability that C will win? My answer: The sample space is $\{A,B,C,D\}$. Since all the events are mutually exclusive, $S = A\cup B\cup C\cup D$. So, $P(S) = P(A)+P(B)+P(C)+P(D)$. Since $P(S) = 1$, $P(A)+P(B)+P(C)+P(D) = 1$. Since A is twice as likely as B, $P(A) =2\times P(B)$. Since B and C are equally likely, $P(B)=P(C)$. Thus, $P(A) = 2\times P(C)$. Since C is twice as likely as D, $P(C) = 2\times P(D)$. Therefore, the probability of the sample space in terms of C is: $1 = 6P(C)$. So, $P(C) = \frac{1}{6}$. The book's answer: Let $p = P(D)$. Since C is twice as likely as D, $P(C) = 2p$. Since B and C are equally likely, $P(B) = 2x$. Since A is twice as likely as B, $P(A)= 4p$. Since $P(S) = 1$, $p + 2p + 2p + 4p = 1$. So, $p = \frac{1}{9}$ and $P(C) = \frac{2}{9}$. Thanks!","I just started learning probability, so my level is not very high. I am doing a homework problem, and my answer is different than the book's. I can't understand why. I see how the answer in the book makes sense, but I also see how my procedure makes sense. Could someone please point at what I am doing wrong? Problem: Four candidates, A, B, C, and D, are running in an election. A is twice as likely to be elected as B. B and C are equally likely. C is twice as likely as D. What is the probability that C will win? My answer: The sample space is $\{A,B,C,D\}$. Since all the events are mutually exclusive, $S = A\cup B\cup C\cup D$. So, $P(S) = P(A)+P(B)+P(C)+P(D)$. Since $P(S) = 1$, $P(A)+P(B)+P(C)+P(D) = 1$. Since A is twice as likely as B, $P(A) =2\times P(B)$. Since B and C are equally likely, $P(B)=P(C)$. Thus, $P(A) = 2\times P(C)$. Since C is twice as likely as D, $P(C) = 2\times P(D)$. Therefore, the probability of the sample space in terms of C is: $1 = 6P(C)$. So, $P(C) = \frac{1}{6}$. The book's answer: Let $p = P(D)$. Since C is twice as likely as D, $P(C) = 2p$. Since B and C are equally likely, $P(B) = 2x$. Since A is twice as likely as B, $P(A)= 4p$. Since $P(S) = 1$, $p + 2p + 2p + 4p = 1$. So, $p = \frac{1}{9}$ and $P(C) = \frac{2}{9}$. Thanks!",,['probability']
19,Find expected value of last roll,Find expected value of last roll,,"Suppose that you play the following game: You roll a fair die at most $N$ times and get an amount of dollars equal to the last number rolled. You can decide to stop the game at any time. What is the (approximate) value of this game for $N=60$? Here is my approach: If you can only roll the die once then the expected value is $3.5\$$. Therefore, if you can only roll the coin twice it makes sense to roll again only if your first roll is $1,2$ or $3$. Thus, the expected value is $\frac{1}{2}(3.5)+\frac{4+5+6}{6}=4.25$. If you have two rolls left, then the expected value of those is $4.25$. Hence, you roll again only if you get 1,2,3 or 4. The expected value is thus $\frac{2}{3}(4.25)+\frac{5+6}{6} \approx 4.67$. Similarly, if you have three rolls left, the expected value is $\frac{2}{3}(4.67)+\frac{5+6}{6} \approx 4.94$. With four rolls left, the expected value is $\frac{2}{3}(4.94)+\frac{5+6}{6} \approx 5.13$. Thus for the first $45$ rolls you stop only if you roll a $6$. It follows that the expected value should be $6(1-(5/6)^{45})+(5/6)^{45}\cdot 5.13 =5.999762..$ Is this correct? Somehow this seems too high. Thanks!","Suppose that you play the following game: You roll a fair die at most $N$ times and get an amount of dollars equal to the last number rolled. You can decide to stop the game at any time. What is the (approximate) value of this game for $N=60$? Here is my approach: If you can only roll the die once then the expected value is $3.5\$$. Therefore, if you can only roll the coin twice it makes sense to roll again only if your first roll is $1,2$ or $3$. Thus, the expected value is $\frac{1}{2}(3.5)+\frac{4+5+6}{6}=4.25$. If you have two rolls left, then the expected value of those is $4.25$. Hence, you roll again only if you get 1,2,3 or 4. The expected value is thus $\frac{2}{3}(4.25)+\frac{5+6}{6} \approx 4.67$. Similarly, if you have three rolls left, the expected value is $\frac{2}{3}(4.67)+\frac{5+6}{6} \approx 4.94$. With four rolls left, the expected value is $\frac{2}{3}(4.94)+\frac{5+6}{6} \approx 5.13$. Thus for the first $45$ rolls you stop only if you roll a $6$. It follows that the expected value should be $6(1-(5/6)^{45})+(5/6)^{45}\cdot 5.13 =5.999762..$ Is this correct? Somehow this seems too high. Thanks!",,['probability']
20,Probability of getting at least one of each balls from an urn,Probability of getting at least one of each balls from an urn,,"Consider an urn containing $4$ red balls, $4$ blue balls, $4$ yellow balls, and $4$ green balls. If 8 balls are randomly drawn from these 16 balls, what is the probability that it will contain at least one ball of each of the four colors? Attempted Solution: I think I did this right, but I just wanted to confirm. P(at least one of each ball) = $1$ - P(not one of each) = $1$ - $\frac{12\choose8}{16\choose8}$ = $.9615$ Edit: Actually, I think I have to choose one group of 4 to not get any selected, i.e. $4\choose1$ and then take $1$ - $4$$\frac{12\choose8}{16\choose8}$ = $.846$","Consider an urn containing $4$ red balls, $4$ blue balls, $4$ yellow balls, and $4$ green balls. If 8 balls are randomly drawn from these 16 balls, what is the probability that it will contain at least one ball of each of the four colors? Attempted Solution: I think I did this right, but I just wanted to confirm. P(at least one of each ball) = $1$ - P(not one of each) = $1$ - $\frac{12\choose8}{16\choose8}$ = $.9615$ Edit: Actually, I think I have to choose one group of 4 to not get any selected, i.e. $4\choose1$ and then take $1$ - $4$$\frac{12\choose8}{16\choose8}$ = $.846$",,"['probability', 'combinatorics']"
21,Interactive problem to demonstrate non-intuitive probability results to a large crowd?,Interactive problem to demonstrate non-intuitive probability results to a large crowd?,,"I will be presenting to a large crowd of about 2,000 people on how our intuition does not align with reality in many situations, as I've always been impressed with how statistical analysis can show us how the world really works, to get us to accept that our intuition on probability can be way off. What I'm looking for is Suggestions on specific questions/riddles or situations that I can give to this crowd that would demonstrate this in the most impressive way by them actually answering the questions & the result being counted right there Some suggested Statistical Mathematics applied to that problem, to demonstrate a likely expected (with, let's say 80% confidence, or whatever would be more appropriate) end-result given a sample size of N , so I can know ahead of time the number I should expect the result to be close to. The tone of this event doesn't allow me to give any Mathematical explanations, so a demonstration would be more powerful to illustrate the point. As such, questions I'd ask the crowd would need to be straightforward for most people to follow and to allow for a show of hands to be counted, or for them to have discussions with those around them (in groups of x , that could be defined in outlining the problem). I'll have about two dozen helpers to potentially count hands or to ask groups what the result is, and we'll be doing it quickly, so precision can be sacrificed for the sake of simply getting an impressive final result. The classic example that demonstrates this well for explanatory purposes (without relying on an actual live implementation) would be the birthday problem , but that only needs 23 people for the odds to be over 50/50 of the same birthday, and even if you tested it on a group of 23 people, doing it once will be a bad way to demonstrate your point because of how probability actually works meaning that maybe there won't be that shared birthday this time. But with a large group of people, we can be more confident to get close to a particular number. At first, I thought that I could scale up the birthday problem and simply ask a small number of questions to get to the most common birthday for the entire crowd, but in posing another question here to see what the expected number may be , I'm reminded why I'm asking here in the first place in how rusty my statistics knowledge is, given that doing so would face the law of big numbers and actually give the intuitively (much less impressive) expected result. I could, of course, give a simple twist to a classical probability situation, such as the Monty Hall problem , by getting a show of hands for who would do what, or who would expect what odds. That result won't be as impressive though since the large number isn't really a result, so much as psychological proof that most people's intuition is off. If nothing else works, I'd do this and simply explain the issue, but that would work for a small crowd just as well, and I'd like to take advantage of having such a big audience, and having the law of big numbers work for me to be confident on getting a particular result or very close. Here are a couple of my quick ideas on doing this, but I'd like to see if someone can give me a demonstrable proof of one of these or another demonstration you may have in mind, which would give a large final result, where intuition (for the layperson unfamiliar with statistics) may not expect it: Attempt to split the crowd into groups of approximately 23 (or slightly higher if seating arrangements make that easier), and to give them 2 minutes to figure out if there are any shared birthdays. When they are finished, count the number of successes, which should be approximately half (about 43 of ~87 groups that would form). [Would this work as expected, or am I forgetting something?] The top answer on this question has multiple interesting suggestions that I'll be looking into to see which could be most entertaining, but can we find an expected answer given my criteria, and from that, know which one is going to be most impressive?","I will be presenting to a large crowd of about 2,000 people on how our intuition does not align with reality in many situations, as I've always been impressed with how statistical analysis can show us how the world really works, to get us to accept that our intuition on probability can be way off. What I'm looking for is Suggestions on specific questions/riddles or situations that I can give to this crowd that would demonstrate this in the most impressive way by them actually answering the questions & the result being counted right there Some suggested Statistical Mathematics applied to that problem, to demonstrate a likely expected (with, let's say 80% confidence, or whatever would be more appropriate) end-result given a sample size of N , so I can know ahead of time the number I should expect the result to be close to. The tone of this event doesn't allow me to give any Mathematical explanations, so a demonstration would be more powerful to illustrate the point. As such, questions I'd ask the crowd would need to be straightforward for most people to follow and to allow for a show of hands to be counted, or for them to have discussions with those around them (in groups of x , that could be defined in outlining the problem). I'll have about two dozen helpers to potentially count hands or to ask groups what the result is, and we'll be doing it quickly, so precision can be sacrificed for the sake of simply getting an impressive final result. The classic example that demonstrates this well for explanatory purposes (without relying on an actual live implementation) would be the birthday problem , but that only needs 23 people for the odds to be over 50/50 of the same birthday, and even if you tested it on a group of 23 people, doing it once will be a bad way to demonstrate your point because of how probability actually works meaning that maybe there won't be that shared birthday this time. But with a large group of people, we can be more confident to get close to a particular number. At first, I thought that I could scale up the birthday problem and simply ask a small number of questions to get to the most common birthday for the entire crowd, but in posing another question here to see what the expected number may be , I'm reminded why I'm asking here in the first place in how rusty my statistics knowledge is, given that doing so would face the law of big numbers and actually give the intuitively (much less impressive) expected result. I could, of course, give a simple twist to a classical probability situation, such as the Monty Hall problem , by getting a show of hands for who would do what, or who would expect what odds. That result won't be as impressive though since the large number isn't really a result, so much as psychological proof that most people's intuition is off. If nothing else works, I'd do this and simply explain the issue, but that would work for a small crowd just as well, and I'd like to take advantage of having such a big audience, and having the law of big numbers work for me to be confident on getting a particular result or very close. Here are a couple of my quick ideas on doing this, but I'd like to see if someone can give me a demonstrable proof of one of these or another demonstration you may have in mind, which would give a large final result, where intuition (for the layperson unfamiliar with statistics) may not expect it: Attempt to split the crowd into groups of approximately 23 (or slightly higher if seating arrangements make that easier), and to give them 2 minutes to figure out if there are any shared birthdays. When they are finished, count the number of successes, which should be approximately half (about 43 of ~87 groups that would form). [Would this work as expected, or am I forgetting something?] The top answer on this question has multiple interesting suggestions that I'll be looking into to see which could be most entertaining, but can we find an expected answer given my criteria, and from that, know which one is going to be most impressive?",,"['probability', 'soft-question']"
22,How many ways are there to distribute 30 green balls to 4 persons?,How many ways are there to distribute 30 green balls to 4 persons?,,How many ways are there to distribute 30 green balls to 4 persons if Alice and Eve together get no more than 20 and Lucky gets at least 7? The answer is: 2464 but I'm not sure how to get it?,How many ways are there to distribute 30 green balls to 4 persons if Alice and Eve together get no more than 20 and Lucky gets at least 7? The answer is: 2464 but I'm not sure how to get it?,,"['probability', 'combinatorics', 'number-theory', 'discrete-mathematics']"
23,Trying to understand $\mathbb E[X\mid \mathcal F]$ and Martingale concept,Trying to understand  and Martingale concept,\mathbb E[X\mid \mathcal F],"Q1) Let $X$ a r.v. on a probability space $(\Omega ,\mathcal F,P)$ and $\mathcal G\subset \mathcal F$ a sub $\sigma -$algebra. What does $$\mathbb E[X\mid \mathcal G]\ \ ?$$ I understand what is $\mathbb E[X\mid Y]$ when $Y$ is an r.v., but not when it's a $\sigma -$algebra. Q2) Let $(M_t)$ a stochastic process adapted to the filtration $\{\mathcal F_t\}_t$. I know that $(M_t)$ is a martingale if $\mathbb E[|M_t|]<\infty $ and $$\mathbb E[M_t\mid \mathcal F_s]=M_s,\quad s\leq t.$$ Could you explain me what does mean the last condition ? I would interpret is that when $t>s$, it's enough to know $M_s$ to now $M_t$ given $\mathcal F_s$, but I have no intuition neither any idea of what it can mean.","Q1) Let $X$ a r.v. on a probability space $(\Omega ,\mathcal F,P)$ and $\mathcal G\subset \mathcal F$ a sub $\sigma -$algebra. What does $$\mathbb E[X\mid \mathcal G]\ \ ?$$ I understand what is $\mathbb E[X\mid Y]$ when $Y$ is an r.v., but not when it's a $\sigma -$algebra. Q2) Let $(M_t)$ a stochastic process adapted to the filtration $\{\mathcal F_t\}_t$. I know that $(M_t)$ is a martingale if $\mathbb E[|M_t|]<\infty $ and $$\mathbb E[M_t\mid \mathcal F_s]=M_s,\quad s\leq t.$$ Could you explain me what does mean the last condition ? I would interpret is that when $t>s$, it's enough to know $M_s$ to now $M_t$ given $\mathcal F_s$, but I have no intuition neither any idea of what it can mean.",,"['probability', 'martingales', 'conditional-expectation']"
24,Integral of conditional probability density function,Integral of conditional probability density function,,"As far as I understand, when we fix the condition for the conditional density, we get probability distribution and the integral over all the space is $1$ $P(X|Y=y_0)$: $$\int_{\mathbb{R}}f_{X \mid Y}(x \mid y=y_0)dx=P(X|Y=y_0)<1 $$ However, suppose we want to take integral: $$\int_{\mathbb{R}}\bigg(\int_{\mathbb{R}}f_{X \mid Y}(x \mid y)dx\bigg)dy $$ I thought it is equal to $1$, but approximate numerical computation through summation for continuous conditional density $$\sum_{i=1}^N \sum_{j=1}^N f_{X\mid Y}(a+\frac{b-a}{N}i \ \ \big| \ \ a+\frac{b-a}{N}j)\cdot(\frac{b-a}{N})^2 $$  gives very big values, e.g. $3000$ or even $1e+25$.","As far as I understand, when we fix the condition for the conditional density, we get probability distribution and the integral over all the space is $1$ $P(X|Y=y_0)$: $$\int_{\mathbb{R}}f_{X \mid Y}(x \mid y=y_0)dx=P(X|Y=y_0)<1 $$ However, suppose we want to take integral: $$\int_{\mathbb{R}}\bigg(\int_{\mathbb{R}}f_{X \mid Y}(x \mid y)dx\bigg)dy $$ I thought it is equal to $1$, but approximate numerical computation through summation for continuous conditional density $$\sum_{i=1}^N \sum_{j=1}^N f_{X\mid Y}(a+\frac{b-a}{N}i \ \ \big| \ \ a+\frac{b-a}{N}j)\cdot(\frac{b-a}{N})^2 $$  gives very big values, e.g. $3000$ or even $1e+25$.",,"['probability', 'probability-theory', 'estimation']"
25,almost everywhere Vs. almost sure,almost everywhere Vs. almost sure,,"I'm reading a book about measure theory and probability (first chapter of Durret's  Probability book), and it's starting to switch between the terms ""a.e."" and ""a.s."" in different contexts. I'm becoming confused about their meanings. What's the difference between almost everywhere and almost sure?","I'm reading a book about measure theory and probability (first chapter of Durret's  Probability book), and it's starting to switch between the terms ""a.e."" and ""a.s."" in different contexts. I'm becoming confused about their meanings. What's the difference between almost everywhere and almost sure?",,"['probability', 'measure-theory', 'almost-everywhere']"
26,Using Central Limit Theorem to approximate.,Using Central Limit Theorem to approximate.,,"$X_1,X_2 \dots X_n$ are independent R.V(random vars) that are uniform $\in$ [0, 1] and let $S_n = X_1 + \dots + X_n$. Now, I am trying to use the Central Limit Theorem to give an approximation of P$(S_{100} \in [40, 60]).$ This seems like a fairly straightforward question. But I'm not sure how to approach it. This is my attempt so far: Since $X_i$ are uniformly distributed, they are uniform RV. then E[X] = $\frac{40 + 60}{2}$ = 50 and my Variance = $\frac{(60-40)^2}{12} = 33.333$ I think I'm lost. Would appreciate any help and guidance! thanks","$X_1,X_2 \dots X_n$ are independent R.V(random vars) that are uniform $\in$ [0, 1] and let $S_n = X_1 + \dots + X_n$. Now, I am trying to use the Central Limit Theorem to give an approximation of P$(S_{100} \in [40, 60]).$ This seems like a fairly straightforward question. But I'm not sure how to approach it. This is my attempt so far: Since $X_i$ are uniformly distributed, they are uniform RV. then E[X] = $\frac{40 + 60}{2}$ = 50 and my Variance = $\frac{(60-40)^2}{12} = 33.333$ I think I'm lost. Would appreciate any help and guidance! thanks",,"['probability', 'statistics', 'probability-distributions', 'probability-limit-theorems']"
27,Expected number of flips until kth head,Expected number of flips until kth head,,"We flip a biased coin (P (head) = p) continously until we observe k heads, and then we stop. Let X be the number of flips. Find E[X]. My intuition tells me this should be a very simple problem, but I'm somehow struggling with it. I've tried to derive the distribution for X, then summing that over k to infinity, but the sum is not easy to evaluate. Thanks for any help.","We flip a biased coin (P (head) = p) continously until we observe k heads, and then we stop. Let X be the number of flips. Find E[X]. My intuition tells me this should be a very simple problem, but I'm somehow struggling with it. I've tried to derive the distribution for X, then summing that over k to infinity, but the sum is not easy to evaluate. Thanks for any help.",,"['probability', 'random-variables']"
28,Probability coupon collection question - nth coupon is a new type?,Probability coupon collection question - nth coupon is a new type?,,"I'm just solving some probability problems in preparation for my exam, and I stumbled upon this one which I cannot tackle: Suppose that you continually collect coupons and that there are $m$ different types. Suppose also that each time a new coupon is obtained, it is a type $i$ coupon with probability $p_i, i = 1, \ldots ,m$. Suppose that you have just collected your $n$-th coupon. What is the probability that it is a new type? Hint: Condition on the type of this coupon. Any help would be appreciated, thank you.","I'm just solving some probability problems in preparation for my exam, and I stumbled upon this one which I cannot tackle: Suppose that you continually collect coupons and that there are $m$ different types. Suppose also that each time a new coupon is obtained, it is a type $i$ coupon with probability $p_i, i = 1, \ldots ,m$. Suppose that you have just collected your $n$-th coupon. What is the probability that it is a new type? Hint: Condition on the type of this coupon. Any help would be appreciated, thank you.",,"['probability', 'conditional-probability']"
29,Throw a dice 4 times. What is the probability `6` be up at-least one time?,Throw a dice 4 times. What is the probability `6` be up at-least one time?,,"First time I approach a probability question (: Throw a dice 4 times. What is the probability 6 be up at-least one time? Intuitively, I would say: $\frac{1}{6}\times4$. I would explain as: If you throw one time, probability is $\frac{1}{6}$. If you do it 4 times, then multiply by 4. According to the answers I'm wrong. Can you explain please? thanks in advance.","First time I approach a probability question (: Throw a dice 4 times. What is the probability 6 be up at-least one time? Intuitively, I would say: $\frac{1}{6}\times4$. I would explain as: If you throw one time, probability is $\frac{1}{6}$. If you do it 4 times, then multiply by 4. According to the answers I'm wrong. Can you explain please? thanks in advance.",,['probability']
30,What happens if I toss a coin with decreasing probability to get a head?,What happens if I toss a coin with decreasing probability to get a head?,,"Yesterday night, while I was trying to sleep, I found myself stuck with a simple statistics problem. Let's imagine we have a ""magical coin"", which is completely identical to a normal coin but for a thing: every time you toss the coin, the probability to get a head halves. So at t = 1 we have 50:50, then 25:75, then 12.5:87.5 and so on. At t = ∞ we are going to have 0:100. My question is: if I toss this magical coin infinite times, can I say I am sure I am going to get at least one head? On one hand, I thought, the law of large numbers states that if an event is repeated infinite times, every state that is possible is bound to happen. On the other side, however, at t = ∞ the probability to get a head is zero. Surely the solution of the problem is fairly easy, so what am I doing wrong?","Yesterday night, while I was trying to sleep, I found myself stuck with a simple statistics problem. Let's imagine we have a ""magical coin"", which is completely identical to a normal coin but for a thing: every time you toss the coin, the probability to get a head halves. So at t = 1 we have 50:50, then 25:75, then 12.5:87.5 and so on. At t = ∞ we are going to have 0:100. My question is: if I toss this magical coin infinite times, can I say I am sure I am going to get at least one head? On one hand, I thought, the law of large numbers states that if an event is repeated infinite times, every state that is possible is bound to happen. On the other side, however, at t = ∞ the probability to get a head is zero. Surely the solution of the problem is fairly easy, so what am I doing wrong?",,"['probability', 'statistics', 'infinity', 'law-of-large-numbers']"
31,The largest of $N$ random numbers over a uniform distribution?,The largest of  random numbers over a uniform distribution?,N,"So I read somewhere than if you have $N$ numbers picked independently from a uniform distribution, say $[0,1]$, the greatest number has an expected value of $\frac{N}{N+1}$. So if you have 2 numbers the greatest has expected value $2/3$. The smallest has expected value of $1/3$. The expected values are uniformly distributed. This makes sense, but is there a clear/intuitive proof of this? Thanks :)","So I read somewhere than if you have $N$ numbers picked independently from a uniform distribution, say $[0,1]$, the greatest number has an expected value of $\frac{N}{N+1}$. So if you have 2 numbers the greatest has expected value $2/3$. The smallest has expected value of $1/3$. The expected values are uniformly distributed. This makes sense, but is there a clear/intuitive proof of this? Thanks :)",,"['probability', 'statistics']"
32,CDF related to sampling with replacement,CDF related to sampling with replacement,,"Consider a random process where integers are sampled uniformly with replacement from $\{1...n\}$.  Let $X$ be a random variable that represents the number of samples until either a duplicate is found or both the values $1$ and $2$ have been found.  So if the samples where $1,6,3,5,1$ then $X=5$ and if it was $1,6,3,2$ then $X=4$. How does one find the cumulative distribution function. That is how does one find $P(X \geq x)$?","Consider a random process where integers are sampled uniformly with replacement from $\{1...n\}$.  Let $X$ be a random variable that represents the number of samples until either a duplicate is found or both the values $1$ and $2$ have been found.  So if the samples where $1,6,3,5,1$ then $X=5$ and if it was $1,6,3,2$ then $X=4$. How does one find the cumulative distribution function. That is how does one find $P(X \geq x)$?",,[]
33,Tricky Probability question,Tricky Probability question,,"Each morning a student takes one of the three books he owns from his shelf. The probability that he chooses book $i$ is $a_i$, where $0 < a_i < 1$ for $i=1,2,3$ and the choices on successive days are independent. In the evening he replaces the book at the left-hand end of the shelf. If $P_n$ denotes the probability that on day $n$ the student finds the books in the order $1,2,3$, from left to right, show that, irrespective of the initial arrangement of the books, $P_n$ converges as $n\to\infty$ and find the limit.","Each morning a student takes one of the three books he owns from his shelf. The probability that he chooses book $i$ is $a_i$, where $0 < a_i < 1$ for $i=1,2,3$ and the choices on successive days are independent. In the evening he replaces the book at the left-hand end of the shelf. If $P_n$ denotes the probability that on day $n$ the student finds the books in the order $1,2,3$, from left to right, show that, irrespective of the initial arrangement of the books, $P_n$ converges as $n\to\infty$ and find the limit.",,"['probability', 'markov-chains']"
34,Use Chebychev's Inequality to find a lower bound.,Use Chebychev's Inequality to find a lower bound.,,"Let $X$ be the number of heads one would obtain in $140$ flips of a fair coin. Use Chebychev's Inequality to find a lower bound on the probability $P(60 < X < 80)$. Okay so Chebychev's Inequality is $P(|X - E(X)| > kσ) \le 1/k^2$ for $ k > 0$, where $σ^2$ is the variance of $X$. I'm not sure how to fill this in or anything. My probabilty test is tomorrow so help is much appreciated! Descriptive answers would be awesome.","Let $X$ be the number of heads one would obtain in $140$ flips of a fair coin. Use Chebychev's Inequality to find a lower bound on the probability $P(60 < X < 80)$. Okay so Chebychev's Inequality is $P(|X - E(X)| > kσ) \le 1/k^2$ for $ k > 0$, where $σ^2$ is the variance of $X$. I'm not sure how to fill this in or anything. My probabilty test is tomorrow so help is much appreciated! Descriptive answers would be awesome.",,"['probability', 'inequality']"
35,With probability $o(1)$,With probability,o(1),"I am not sure how to read little/big O expressions in probability theory: What does a statement like ""with probability $1-o(1)$"" mean? Does it mean with high probability?","I am not sure how to read little/big O expressions in probability theory: What does a statement like ""with probability $1-o(1)$"" mean? Does it mean with high probability?",,"['probability', 'asymptotics']"
36,A game of guessing a chosen number,A game of guessing a chosen number,,"Your (honest) opponent choose a random number from 1 to 13 inclusive. You have to guess the number, and you win if the guess is correct. If not, your opponent either reduces the number chosen by one or increases it by 1, and you guess again. The question is, what is the minimum # of attempts necessary to guarantee a win for you. I am not able to get a handle on the problem. Also, (a new variant just thought of), how many guesses should be allowed for a fair or ""nearest to fair"" game ?","Your (honest) opponent choose a random number from 1 to 13 inclusive. You have to guess the number, and you win if the guess is correct. If not, your opponent either reduces the number chosen by one or increases it by 1, and you guess again. The question is, what is the minimum # of attempts necessary to guarantee a win for you. I am not able to get a handle on the problem. Also, (a new variant just thought of), how many guesses should be allowed for a fair or ""nearest to fair"" game ?",,"['probability', 'combinatorics']"
37,Optimal number of answers for a test with wrong-answer penalty,Optimal number of answers for a test with wrong-answer penalty,,"Suppose you have to take a test with ten questions, each with four different options (no multiple answers), and a wrong-answer penalty of half a correct answer. Blank questions do not score neither positively nor negatively. Supposing you have not studied specially hard this time, what's the optimal number of questions to try to answer so the probabilities to pass the exam (having at least five points).","Suppose you have to take a test with ten questions, each with four different options (no multiple answers), and a wrong-answer penalty of half a correct answer. Blank questions do not score neither positively nor negatively. Supposing you have not studied specially hard this time, what's the optimal number of questions to try to answer so the probabilities to pass the exam (having at least five points).",,"['probability', 'statistics']"
38,Figuring out probabilities with Hidden Markov Models,Figuring out probabilities with Hidden Markov Models,,"I'm really new to Math so sorry in advance if this question does not make sense. Also I cross posted this on stats.stackexchange.com also. Background: I'm trying to learn about hidden Markov models and they seem interesting but I was wondering about the probabilities they use to generate their predictions. Most of the information I have read has probabilities already about changing states and staying in the current state. I was on Github and looked up code(mainly python code) to understand a bit better but either they have the probabilities already computed or are calculating it based on occurrences in other documents(in the case of a spell checker, it studied a book to understand the words occurrences as well as the probability of them being beside each other). Problem: But I'm trying to understand the theory of figuring out probabilities for the HMM when I'm not using speech or text recognition.  I'm trying to make a game which is trying to guess numbers based on a users behaviour(i.e. user is given a list of fruit prices and picks a basket of various fruits. We know the individual fruit prices and the users only reports the total cost of their basket, we then try to find the quantities of fruit they choose. They are not adversarial and cannot change their total quantities by more than 5%). My approach was to first find all the possibilities for all days, then using a graph to create edges between nodes that were within 5% and remove all improbable nodes.  Now I have a range and with every turn new paths are being created and impossible ones are being removed.  So within that range I am trying have a program 'zero' in on the correct solution. I think the HMM model would be helpful for this but I'm a bit confused on the application of  Baum–Welch or forward-backward algorithm to create probabilities(that I can feed into my implementation of hmm). If it helps, I originally posted this at stackexchange (alot of details about the game itself as well code I have) . I got a very good solution, to find the nodes with the highest amount of edges as solutions.  Its okay but it doesn't select a solution and if it makes a mistake it simply continues the path. I was also advised from mathexchange that this can be solved as a hidden markov model.  I am basically struggling with applying it to my problem(right now figuring out the probability part. I have been told to create all paths as equally probable but I don't understand how that helps a HMM pick the most probable solution). If anyone has any suggestions or suggested reading or sample code(I'm learning python, but I can try to understand any other code thats provided). If possible can it be explained to a layman(I think I can apply the code if I can clearly understand the theory, which apparently I do not). Thanks in advance and sorry again if this question does not apply or is not explained correctly.","I'm really new to Math so sorry in advance if this question does not make sense. Also I cross posted this on stats.stackexchange.com also. Background: I'm trying to learn about hidden Markov models and they seem interesting but I was wondering about the probabilities they use to generate their predictions. Most of the information I have read has probabilities already about changing states and staying in the current state. I was on Github and looked up code(mainly python code) to understand a bit better but either they have the probabilities already computed or are calculating it based on occurrences in other documents(in the case of a spell checker, it studied a book to understand the words occurrences as well as the probability of them being beside each other). Problem: But I'm trying to understand the theory of figuring out probabilities for the HMM when I'm not using speech or text recognition.  I'm trying to make a game which is trying to guess numbers based on a users behaviour(i.e. user is given a list of fruit prices and picks a basket of various fruits. We know the individual fruit prices and the users only reports the total cost of their basket, we then try to find the quantities of fruit they choose. They are not adversarial and cannot change their total quantities by more than 5%). My approach was to first find all the possibilities for all days, then using a graph to create edges between nodes that were within 5% and remove all improbable nodes.  Now I have a range and with every turn new paths are being created and impossible ones are being removed.  So within that range I am trying have a program 'zero' in on the correct solution. I think the HMM model would be helpful for this but I'm a bit confused on the application of  Baum–Welch or forward-backward algorithm to create probabilities(that I can feed into my implementation of hmm). If it helps, I originally posted this at stackexchange (alot of details about the game itself as well code I have) . I got a very good solution, to find the nodes with the highest amount of edges as solutions.  Its okay but it doesn't select a solution and if it makes a mistake it simply continues the path. I was also advised from mathexchange that this can be solved as a hidden markov model.  I am basically struggling with applying it to my problem(right now figuring out the probability part. I have been told to create all paths as equally probable but I don't understand how that helps a HMM pick the most probable solution). If anyone has any suggestions or suggested reading or sample code(I'm learning python, but I can try to understand any other code thats provided). If possible can it be explained to a layman(I think I can apply the code if I can clearly understand the theory, which apparently I do not). Thanks in advance and sorry again if this question does not apply or is not explained correctly.",,"['probability', 'stochastic-processes', 'algorithms', 'markov-chains', 'bayesian-network']"
39,Choosing the correct combinatorial method,Choosing the correct combinatorial method,,"A box contains 24 lightbulbs 2 of which are defective. If a person selects 10 lightbulbs at random without replacement, what is the probability that both defective bulbs will be selected? I'm searching for the right way to select the sample space. The denominator should be all the possible ways in which 10 balls can be selected from 24, i.e. ${24\choose 10}$ and the numerator should be all the possible ways in which the defective balls can be selected, but I can't decide whether that is: $2^{10}$ or just simply ${10\choose 2}$ can someone help clarify this? EDIT: second question: Suppose that 35 people are divided in a random manner into two teams in such a way that one team contains 10 people and the other team contains 25 people. What is the probability that two particular people A and B will be on the same team? i'm still having trouble finding the event space. The sample space should be all the possible ways in which 10 and 25 can be selected from 35, i.e. (35 choose 10)*(35 choose 10). But I have no idea how to find the event space... it seems like there are only 2 possibilities but I guess I'm wrong..","A box contains 24 lightbulbs 2 of which are defective. If a person selects 10 lightbulbs at random without replacement, what is the probability that both defective bulbs will be selected? I'm searching for the right way to select the sample space. The denominator should be all the possible ways in which 10 balls can be selected from 24, i.e. ${24\choose 10}$ and the numerator should be all the possible ways in which the defective balls can be selected, but I can't decide whether that is: $2^{10}$ or just simply ${10\choose 2}$ can someone help clarify this? EDIT: second question: Suppose that 35 people are divided in a random manner into two teams in such a way that one team contains 10 people and the other team contains 25 people. What is the probability that two particular people A and B will be on the same team? i'm still having trouble finding the event space. The sample space should be all the possible ways in which 10 and 25 can be selected from 35, i.e. (35 choose 10)*(35 choose 10). But I have no idea how to find the event space... it seems like there are only 2 possibilities but I guess I'm wrong..",,['probability']
40,Method for Constructing Poisson Processes,Method for Constructing Poisson Processes,,"I'm writing a bachelor thesis about Poisson Processes, and I need a method for the construction of these processes. I know that I can construct them defining inter-arrival times with the exponential distribution, but this method don't work with inhomogeneous processes. Someone can help me?","I'm writing a bachelor thesis about Poisson Processes, and I need a method for the construction of these processes. I know that I can construct them defining inter-arrival times with the exponential distribution, but this method don't work with inhomogeneous processes. Someone can help me?",,"['probability', 'probability-theory', 'stochastic-processes']"
41,Poisson CDF as lower bound to binomial CDF,Poisson CDF as lower bound to binomial CDF,,"Consider two random variables $X \sim \operatorname{Binom}[(n,p)]$ and $Y\sim\operatorname{Poisson}[(\lambda)]$, where $\lambda = n p $. Let their respective CDFs be $F_X(k)$ and $F_Y(k)$, then I would like to show that $$ F_Y(k) \leq F_X(k) $$ but I can't quite figure out how to. I'm not 100 % sure it's true but intuitively, it seems very much like that fatter tail of the Poisson distribution should make it true. I thought about trying to show that $X' \sim \operatorname{Binom}[(n+1,p')]$, where $(n+1)p'=\lambda$, satisfies $F_{X'}(k) \leq F_X(k)$, but didn't get something out. Any hints/answers appreciated! Thanks","Consider two random variables $X \sim \operatorname{Binom}[(n,p)]$ and $Y\sim\operatorname{Poisson}[(\lambda)]$, where $\lambda = n p $. Let their respective CDFs be $F_X(k)$ and $F_Y(k)$, then I would like to show that $$ F_Y(k) \leq F_X(k) $$ but I can't quite figure out how to. I'm not 100 % sure it's true but intuitively, it seems very much like that fatter tail of the Poisson distribution should make it true. I thought about trying to show that $X' \sim \operatorname{Binom}[(n+1,p')]$, where $(n+1)p'=\lambda$, satisfies $F_{X'}(k) \leq F_X(k)$, but didn't get something out. Any hints/answers appreciated! Thanks",,['probability']
42,Coin Betting Expectation,Coin Betting Expectation,,"Suppose I have a biased coin with probability of heads p, and tails q=(1-p). It is then used in a game which lasts at most N tosses, and start with a stake of £1.  Each time the coin is tails my money is doubled.  The first time it comes down heads my money is reduced to £1, and the second time it comes down heads, I lose all my money.  The game ends after N tosses, or after the second head.  What is the expectation of my money at the end of the game?","Suppose I have a biased coin with probability of heads p, and tails q=(1-p). It is then used in a game which lasts at most N tosses, and start with a stake of £1.  Each time the coin is tails my money is doubled.  The first time it comes down heads my money is reduced to £1, and the second time it comes down heads, I lose all my money.  The game ends after N tosses, or after the second head.  What is the expectation of my money at the end of the game?",,['probability']
43,Connection to Normal distribution,Connection to Normal distribution,,"I've been working on finding the probability for the event, that the sum of $n$ independent random variables are less than $s$, when they are evenly distributed on $[0,1)$. I've used the law of total probability to derive the formula: $P(S<s) = \frac{1}{n!}\sum_{k=0}^{\lfloor s\rfloor}(-1)^k\binom{n}{k}(s-k)^n$ $f_S(s) = \frac{1}{(n-1)!}\sum_{k=0}^{\lfloor s\rfloor}(-1)^k\binom{n}{k}(s-k)^{n-1}$ Now I'm not very strong in probability, but I believe you could express the same thing using the normal distribution. Or at least approximate it. Is that so? In that case how would you express it, and how would you normally determine the approximation error?","I've been working on finding the probability for the event, that the sum of $n$ independent random variables are less than $s$, when they are evenly distributed on $[0,1)$. I've used the law of total probability to derive the formula: $P(S<s) = \frac{1}{n!}\sum_{k=0}^{\lfloor s\rfloor}(-1)^k\binom{n}{k}(s-k)^n$ $f_S(s) = \frac{1}{(n-1)!}\sum_{k=0}^{\lfloor s\rfloor}(-1)^k\binom{n}{k}(s-k)^{n-1}$ Now I'm not very strong in probability, but I believe you could express the same thing using the normal distribution. Or at least approximate it. Is that so? In that case how would you express it, and how would you normally determine the approximation error?",,"['probability', 'probability-theory', 'random', 'normal-distribution']"
44,Distribution of days in a week on Christmas,Distribution of days in a week on Christmas,,"I am wondering if there is a strict argument about the probabilities of Christmas (Dec. 25) on Monday, Tuesday, ..., Sunday. My experiments give: Sunday 0.145 Monday  0.14 Tuesday 0.145 Wednesday 0.1425 Thursday  0.1425 Saturday 0.14 Friday 0.145 It looks that they are not equal. :) My question is: 1) How to obtain the probabilities without restricting the counts over certain range of years? 2) why Sunday is more probable than Wednesday, which is more probable than Monday, if it is true?","I am wondering if there is a strict argument about the probabilities of Christmas (Dec. 25) on Monday, Tuesday, ..., Sunday. My experiments give: Sunday 0.145 Monday  0.14 Tuesday 0.145 Wednesday 0.1425 Thursday  0.1425 Saturday 0.14 Friday 0.145 It looks that they are not equal. :) My question is: 1) How to obtain the probabilities without restricting the counts over certain range of years? 2) why Sunday is more probable than Wednesday, which is more probable than Monday, if it is true?",,"['probability', 'calendar-computations']"
45,Probability on Number Theory,Probability on Number Theory,,"Problem: Suppose that $a,b,c \in \{1,2,3,\cdots,1000\}$ are randomly selected with replacement. Find the probability that $abc+ab+2a$ is divisible by $5$ . Answer given from the worksheet: $33/125$ My answer: $\frac{641}{3125}$ Attempt : Since $abc+ab+2a = a(bc+b+2)$ , either $a \equiv 0 \pmod{5}$ or $bc+b+2 \equiv 0 \pmod{5}$ . The first case, which is $a \equiv 0 \pmod{5}$ , has a probability of $1/5$ . Now on the other case, $$bc+b+2 \equiv 0 \pmod{5} \Rightarrow b(c+1) \equiv 3 \pmod{5}$$ happens when $b\equiv 1$ and $c \equiv 2$ , $b \equiv 2$ and $c \equiv 3$ , $b \equiv 3$ and $c \equiv 0$ , or $b \equiv 4$ and $c \equiv 1$ , total of $4$ solutions. So, this case has a probability of $4 \cdot\frac{1}{5^4} = \frac{4}{625}$ . By Inclusion-exclusion principle, the final probability should be $\frac{1}{5} + \frac{4}{625} - \frac{1}{5} \cdot \frac{4}{625} = \frac{641}{3125}.$ This is apparently not the same from the given answer in the worksheet. Where did I go wrong?","Problem: Suppose that are randomly selected with replacement. Find the probability that is divisible by . Answer given from the worksheet: My answer: Attempt : Since , either or . The first case, which is , has a probability of . Now on the other case, happens when and , and , and , or and , total of solutions. So, this case has a probability of . By Inclusion-exclusion principle, the final probability should be This is apparently not the same from the given answer in the worksheet. Where did I go wrong?","a,b,c \in \{1,2,3,\cdots,1000\} abc+ab+2a 5 33/125 \frac{641}{3125} abc+ab+2a = a(bc+b+2) a \equiv 0 \pmod{5} bc+b+2 \equiv 0 \pmod{5} a \equiv 0 \pmod{5} 1/5 bc+b+2 \equiv 0 \pmod{5} \Rightarrow b(c+1) \equiv 3 \pmod{5} b\equiv 1 c \equiv 2 b \equiv 2 c \equiv 3 b \equiv 3 c \equiv 0 b \equiv 4 c \equiv 1 4 4 \cdot\frac{1}{5^4} = \frac{4}{625} \frac{1}{5} + \frac{4}{625} - \frac{1}{5} \cdot \frac{4}{625} = \frac{641}{3125}.","['probability', 'elementary-number-theory']"
46,How to solve $\sum\limits_{k=0}^{n-a-b}\binom{n-a-b}{k}(a+k-1)!(n-a-k)!$,How to solve,\sum\limits_{k=0}^{n-a-b}\binom{n-a-b}{k}(a+k-1)!(n-a-k)!,"I'm solving a probability problem, and I've ended up with this sum: $$\sum\limits_{k=0}^{n-a-b}\binom{n-a-b}{k}(a+k-1)!(n-a-k)!$$ WolframAlpha says I should get the answer $\frac{n!}{a\binom{a+b}{a}}$ , but I don't see how to get there. I tried to get to something containing $\binom{a+k-1}{k}$ so that I could use the Hockey Stick Theorem, but I wasn't successful. So any hints would be very welcome, thanks for any help","I'm solving a probability problem, and I've ended up with this sum: WolframAlpha says I should get the answer , but I don't see how to get there. I tried to get to something containing so that I could use the Hockey Stick Theorem, but I wasn't successful. So any hints would be very welcome, thanks for any help",\sum\limits_{k=0}^{n-a-b}\binom{n-a-b}{k}(a+k-1)!(n-a-k)! \frac{n!}{a\binom{a+b}{a}} \binom{a+k-1}{k},"['probability', 'combinatorics', 'summation', 'binomial-coefficients']"
47,Independent sub σ-algebra measured by the same random variable,Independent sub σ-algebra measured by the same random variable,,"Let $(\Omega,\mathcal{A},Pr)$ be a probability space and let $\mathcal{F}$ and $\mathcal{G}$ be two sub- $\sigma$ -algebras of $\mathcal{A}$ . Suppose that they are independent, and suppose that 𝑋 is real valued random variable on $\Omega$ which is measurable with respect to both $\mathcal{F}$ and $\mathcal{G}$ . Show that 𝑋 is almost surely constant; that is there is real number $c \in \mathbb{R}$ , so that $Pr(X=c)=1$ . Firstly, I am not sure what independent $\sigma$ -algebra means here and how I can connect the independence of $\sigma$ -algebra with random variable. Secondly, since $\mathcal{F}$ and $\mathcal{G}$ are different $\sigma$ -algebra, is it allowed to be measured by the same random variable? What does this imply? Maybe my question is a little bit vague, but I always think that different $\sigma$ -algebra should be measured by different random variable somehow. And I don't even know why I take it as granted. There is a solution somewhere. But I feel it makes no sense.","Let be a probability space and let and be two sub- -algebras of . Suppose that they are independent, and suppose that 𝑋 is real valued random variable on which is measurable with respect to both and . Show that 𝑋 is almost surely constant; that is there is real number , so that . Firstly, I am not sure what independent -algebra means here and how I can connect the independence of -algebra with random variable. Secondly, since and are different -algebra, is it allowed to be measured by the same random variable? What does this imply? Maybe my question is a little bit vague, but I always think that different -algebra should be measured by different random variable somehow. And I don't even know why I take it as granted. There is a solution somewhere. But I feel it makes no sense.","(\Omega,\mathcal{A},Pr) \mathcal{F} \mathcal{G} \sigma \mathcal{A} \Omega \mathcal{F} \mathcal{G} c \in \mathbb{R} Pr(X=c)=1 \sigma \sigma \mathcal{F} \mathcal{G} \sigma \sigma","['probability', 'measure-theory', 'probability-distributions', 'random-variables', 'independence']"
48,What does entropy capture that variance does not?,What does entropy capture that variance does not?,,"Consider a discrete distribution like this one: $[0.6,0.15,0.1,0.08,0.05,0.02]$ Its entropy is $-\sum p_i\log p_i = 1.805$ , and its variance is $\frac{\sum_i(p_i - \bar{p})^2}{n} = 0.039188$ They both measure the spread of this distribution. For distributions like this that are far from uniform, what information does one capture that the other does not?","Consider a discrete distribution like this one: Its entropy is , and its variance is They both measure the spread of this distribution. For distributions like this that are far from uniform, what information does one capture that the other does not?","[0.6,0.15,0.1,0.08,0.05,0.02] -\sum p_i\log p_i = 1.805 \frac{\sum_i(p_i - \bar{p})^2}{n} = 0.039188","['probability', 'variance', 'entropy']"
49,Tossing the coin problem: We toss symmetrical coin $10$ times. Calculate probability of tails appearing at least five times in a row.,Tossing the coin problem: We toss symmetrical coin  times. Calculate probability of tails appearing at least five times in a row.,10,"We toss symmetrical coin $10$ times. Calculate probability of tails appearing at least five times in a row. I tried by dividing by cases (the row TTTTT I observe as one object); first case exactly $5$ tails, second exacty $6$ tails etc. For the first case I decided to separate cases when the tails appear from the first toss to the fifth toss and when they appear from $i$ th toss to $(i+5)$ th toss but each time I get stuck on that case.","We toss symmetrical coin times. Calculate probability of tails appearing at least five times in a row. I tried by dividing by cases (the row TTTTT I observe as one object); first case exactly tails, second exacty tails etc. For the first case I decided to separate cases when the tails appear from the first toss to the fifth toss and when they appear from th toss to th toss but each time I get stuck on that case.",10 5 6 i (i+5),"['probability', 'combinatorics']"
50,Under what conditions does $E[f(X)] \approx f(E[X])$?,Under what conditions does ?,E[f(X)] \approx f(E[X]),"Intuitively, when a random variable $X$ has a low variance, a sufficiently smooth function of that random variable will have an expected value which is close to that same function applied to $E[X]$ . That is, $$ f(E[X]) \approx E[f(X)] $$ This is obviously true when $X$ takes on only one value, and appears in a few special cases I have worked out. Is this a well-known theorem? If not, how could this intuition be formalized?","Intuitively, when a random variable has a low variance, a sufficiently smooth function of that random variable will have an expected value which is close to that same function applied to . That is, This is obviously true when takes on only one value, and appears in a few special cases I have worked out. Is this a well-known theorem? If not, how could this intuition be formalized?","X E[X] 
f(E[X]) \approx E[f(X)]
 X",['probability']
51,Probability of getting 3 balls in 1st box if 12 balls are distributed randomly among 3 boxes,Probability of getting 3 balls in 1st box if 12 balls are distributed randomly among 3 boxes,,"$12$ balls are distributed at random among $3$ boxes.The probability that the 1st box will contain $3$ balls is_______? My Approach: $\quad \quad \quad \quad \quad \quad \text{Method}1$ [Considering all balls different] : Tatal no. of ways= $3^{12}$ No. of ways in which 1st box will contain $3$ balls: $$\binom{12}{3} * 2^9$$ Therefore,required probability= $$\frac{\binom{12}{3} * 2^9}{3^{12}}$$ $$=0.2119$$ $\quad \quad \quad \quad \quad \quad \text{Method}2$ [Considering all balls identical] : Tatal no. of ways: $$x_1 + x_2 + x_3=12$$ ,where $\,\, 0 \leq x_1 \leq 12,\,\, 0 \leq x_2 \leq 12,\,\, 0 \leq x_3 \leq 12$ $$= \binom{3+12-1}{12}$$ $$=\binom{14}{12}$$ No. of ways in which $1st$ box will contain $3$ balls $=$ No. of ways in which $2nd$ and $3rd$ boxes will contain $9$ balls (as the remaining $3$ balls will be given to $1st$ box) so, $$ x_2 + x_3=9$$ ,where $\,\, \,\, 0 \leq x_2 \leq 9,\,\, 0 \leq x_3 \leq 9$ $$=\binom{2+9-1}{9} =\binom{10}{9}$$ Therefore,required probability= $$\frac{\binom{10}{9} }{\binom{14}{12}}$$ $$=0.11$$ so, which method is the correct approach for the given problem and why the other is wrong,please explain...","balls are distributed at random among boxes.The probability that the 1st box will contain balls is_______? My Approach: [Considering all balls different] : Tatal no. of ways= No. of ways in which 1st box will contain balls: Therefore,required probability= [Considering all balls identical] : Tatal no. of ways: ,where No. of ways in which box will contain balls No. of ways in which and boxes will contain balls (as the remaining balls will be given to box) so, ,where Therefore,required probability= so, which method is the correct approach for the given problem and why the other is wrong,please explain...","12 3 3 \quad \quad \quad \quad \quad \quad \text{Method}1 3^{12} 3 \binom{12}{3} * 2^9 \frac{\binom{12}{3} * 2^9}{3^{12}} =0.2119 \quad \quad \quad \quad \quad \quad \text{Method}2 x_1 + x_2 + x_3=12 \,\, 0 \leq x_1 \leq 12,\,\, 0 \leq x_2 \leq 12,\,\, 0 \leq x_3 \leq 12 = \binom{3+12-1}{12} =\binom{14}{12} 1st 3 = 2nd 3rd 9 3 1st  x_2 + x_3=9 \,\, \,\, 0 \leq x_2 \leq 9,\,\, 0 \leq x_3 \leq 9 =\binom{2+9-1}{9} =\binom{10}{9} \frac{\binom{10}{9} }{\binom{14}{12}} =0.11","['probability', 'combinatorics']"
52,Chance of three same consecutive answers in A/B/C/D test,Chance of three same consecutive answers in A/B/C/D test,,"I am facing the following problem: What is the probability that there will be at least three same consecutive right answers in a $n$-question A/B/C/D test? I have started with $10$ questions while trying to solve this, so there are $4^{10}$ possible sequences. Now our three same results can begin at the first through the eighth place ($n-2$) because at the ninth place it would be $9-10-11$. For each of these possibilities there can be $4^{10-3} = 4^7$ combinations of $7$ remaining positions. So that's $8 \cdot 4^7$, and now we have to multiply by $4$ because there can be three or more As, Bs, Cs or Ds. That gives us $$\frac{8 \cdot 4^7 \cdot 4}{4^{10}}$$ which simplifies to $8/4^2 = 1/2 = 50\%$. I have checked the result practically with Python code with $100,000$ attempts, and I got about $38514/100000 = 0.38514 = 38.514\%$, which is not even remotely close to my result. Do you have any idea where I went wrong?","I am facing the following problem: What is the probability that there will be at least three same consecutive right answers in a $n$-question A/B/C/D test? I have started with $10$ questions while trying to solve this, so there are $4^{10}$ possible sequences. Now our three same results can begin at the first through the eighth place ($n-2$) because at the ninth place it would be $9-10-11$. For each of these possibilities there can be $4^{10-3} = 4^7$ combinations of $7$ remaining positions. So that's $8 \cdot 4^7$, and now we have to multiply by $4$ because there can be three or more As, Bs, Cs or Ds. That gives us $$\frac{8 \cdot 4^7 \cdot 4}{4^{10}}$$ which simplifies to $8/4^2 = 1/2 = 50\%$. I have checked the result practically with Python code with $100,000$ attempts, and I got about $38514/100000 = 0.38514 = 38.514\%$, which is not even remotely close to my result. Do you have any idea where I went wrong?",,"['probability', 'combinatorics']"
53,An example of three probability events with certain property,An example of three probability events with certain property,,"I am trying to find three events $A,B,C$ with the following properties \begin{gather} P(A|B)>P(A),\\ P(A|C)>P(A),\\ P(A|B\cup C)<P(A). \end{gather} I have not been able to come up with events satisfying all three, and would appreciate some help.","I am trying to find three events $A,B,C$ with the following properties \begin{gather} P(A|B)>P(A),\\ P(A|C)>P(A),\\ P(A|B\cup C)<P(A). \end{gather} I have not been able to come up with events satisfying all three, and would appreciate some help.",,['probability']
54,Probability of all 3 cards drawn being the same number,Probability of all 3 cards drawn being the same number,,"Consider a scenario where Ace is considered as 1 (a number card) and the face cards (King, Queen, Jack) are not number cards. We have to find the probability of 3 cards drawn consecutively (without replacement) being the same number. My friend solved it using the following: $$P = \frac{^4C_1\cdot ^{10}C_1\cdot ^3C_1\cdot ^2C_1 }{^{52}C_3} = \frac{12}{1105}$$ where we choose any one out of the 4 types, and then we choose a number card, and then we choose the same number card in the other types twice. I did the same problem like the following: $$P = \frac{10}{13}\cdot \frac{3}{51}\cdot \frac{2}{50} = \frac{2}{1105}$$ where $\frac{10}{13}$ is the probability of choosing a number card first, $\frac{3}{51}$ is the probability of choosing the same number card out of the rest of 51 cards and $\frac{2}{50}$ is the probability of choosing the same number card out of the rest of 50 cards. Could you please explain as to why we are getting different answers and which one is correct (I guess it has to do something with $6$ multiplication error in either one of the answers arising from problems in permutations (as $6$ = $3!$) of the types of cards)?","Consider a scenario where Ace is considered as 1 (a number card) and the face cards (King, Queen, Jack) are not number cards. We have to find the probability of 3 cards drawn consecutively (without replacement) being the same number. My friend solved it using the following: $$P = \frac{^4C_1\cdot ^{10}C_1\cdot ^3C_1\cdot ^2C_1 }{^{52}C_3} = \frac{12}{1105}$$ where we choose any one out of the 4 types, and then we choose a number card, and then we choose the same number card in the other types twice. I did the same problem like the following: $$P = \frac{10}{13}\cdot \frac{3}{51}\cdot \frac{2}{50} = \frac{2}{1105}$$ where $\frac{10}{13}$ is the probability of choosing a number card first, $\frac{3}{51}$ is the probability of choosing the same number card out of the rest of 51 cards and $\frac{2}{50}$ is the probability of choosing the same number card out of the rest of 50 cards. Could you please explain as to why we are getting different answers and which one is correct (I guess it has to do something with $6$ multiplication error in either one of the answers arising from problems in permutations (as $6$ = $3!$) of the types of cards)?",,['probability']
55,Exponential bulbs?,Exponential bulbs?,,"Can the expiration time of a new light bulb be an exponential random variable? (The second problem here namely features such a bulb.) I'm asking because of the curios memorylessness of the exponential distribution, which I understand like this. With $T$ an exponential variate with parameter $\lambda$ representing time till event $A$ occurs (like time until a bulb burns out), probability $\mathbb{P}\{T > t\} = e^{-\lambda \, t}$ is probability that $A$ doesn't occur till $t$. But if we know that $A$ hasn't occurred till $t$ then probability that $A$ doesn't occur till the additional time $X$ is the same: $\mathbb{P}\{X > t \, | \, T > t\} = e^{-\lambda \, t}$. Obviously if I leave a new bulb on, check it after a year and, given it still works, I can't really say that the chance of persisting for the same duration stays the same. Can I? A bulb is dying with use, specifically with working time and on-off switching, as stated on the halogen bulb package of mine (2000 operating hours, 50 thousand switchings). What kind of events and waiting times do follow exponential distribution? I remember my professor mentioning waiting time of supernovas. Why is that exponential?","Can the expiration time of a new light bulb be an exponential random variable? (The second problem here namely features such a bulb.) I'm asking because of the curios memorylessness of the exponential distribution, which I understand like this. With $T$ an exponential variate with parameter $\lambda$ representing time till event $A$ occurs (like time until a bulb burns out), probability $\mathbb{P}\{T > t\} = e^{-\lambda \, t}$ is probability that $A$ doesn't occur till $t$. But if we know that $A$ hasn't occurred till $t$ then probability that $A$ doesn't occur till the additional time $X$ is the same: $\mathbb{P}\{X > t \, | \, T > t\} = e^{-\lambda \, t}$. Obviously if I leave a new bulb on, check it after a year and, given it still works, I can't really say that the chance of persisting for the same duration stays the same. Can I? A bulb is dying with use, specifically with working time and on-off switching, as stated on the halogen bulb package of mine (2000 operating hours, 50 thousand switchings). What kind of events and waiting times do follow exponential distribution? I remember my professor mentioning waiting time of supernovas. Why is that exponential?",,"['probability', 'probability-distributions']"
56,"Probability question about ""good"" tickets.","Probability question about ""good"" tickets.",,"The problem: Out of 20 exam tickets, 16 tickets are ""good"". Tickets are carefully mixed, and students take turns pulling one ticket. Who has the better chance to draw a ""good"" ticket the first or the second student in the queue? My attempt: Obviously the probability of the first student getting a ""good"" ticket is $16/20 = 4/5 = 0.8$ Now here's where I'm confused. I considered two cases. If the first student got a ""good"" ticket then the chances of the second student are $15/19 = 0.789$, so lower than the first student. However if the first student didn't get a ""good"" ticket, the chances of the second student are $16/19 =0.842$, so slightly better chances than the first one. So is the answer ""depends on whether the first student gets a ""good"" ticket""?","The problem: Out of 20 exam tickets, 16 tickets are ""good"". Tickets are carefully mixed, and students take turns pulling one ticket. Who has the better chance to draw a ""good"" ticket the first or the second student in the queue? My attempt: Obviously the probability of the first student getting a ""good"" ticket is $16/20 = 4/5 = 0.8$ Now here's where I'm confused. I considered two cases. If the first student got a ""good"" ticket then the chances of the second student are $15/19 = 0.789$, so lower than the first student. However if the first student didn't get a ""good"" ticket, the chances of the second student are $16/19 =0.842$, so slightly better chances than the first one. So is the answer ""depends on whether the first student gets a ""good"" ticket""?",,['probability']
57,Roll two fair six-sided dice and adds the results,Roll two fair six-sided dice and adds the results,,Grace rolls two fair six-sided dice and adds the results. She than draws a square that has her result as the length of the diagonal. What is the probability that the numerical value of the area of her square will be less than the numerical value of the perimeter? The answer is 5/18. I drew a table and added up the results of dice rolls. But I am not quite sure of the length of the diagonal? Please help. Thank you in advance.,Grace rolls two fair six-sided dice and adds the results. She than draws a square that has her result as the length of the diagonal. What is the probability that the numerical value of the area of her square will be less than the numerical value of the perimeter? The answer is 5/18. I drew a table and added up the results of dice rolls. But I am not quite sure of the length of the diagonal? Please help. Thank you in advance.,,['probability']
58,Runners and their chance of winning [closed],Runners and their chance of winning [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Six runners are entered in a track meet, and have equal ability. What is the probability that a) they will finish in ascending order of their ages? b) Shanaze will finish first or Tanya will finish second? c) Shanaze and Tanya will not finish back-to-back?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Six runners are entered in a track meet, and have equal ability. What is the probability that a) they will finish in ascending order of their ages? b) Shanaze will finish first or Tanya will finish second? c) Shanaze and Tanya will not finish back-to-back?",,"['probability', 'combinatorics']"
59,Lottery Ticket Question,Lottery Ticket Question,,"A lottery ticket contains six different numbers, chosen from 1 to 39. The winning ticket will match all six numbers in the correct order, plus a bonus number, which may match the other six numbers. The second prize matches the six winning numbers in the correct order, but not the bonus number. What is the probability of winning first or second prize?","A lottery ticket contains six different numbers, chosen from 1 to 39. The winning ticket will match all six numbers in the correct order, plus a bonus number, which may match the other six numbers. The second prize matches the six winning numbers in the correct order, but not the bonus number. What is the probability of winning first or second prize?",,"['probability', 'permutations', 'data-analysis']"
60,Payoff of a dice game,Payoff of a dice game,,"I came across this question today: ""A fair die is tossed. If 2,3 or 5 occurs, the player wins that number of rupees, but if 1, 4 or 6 occurs, the player loses that number if rupees. Then find the possible payoffs for the player"". My textbook has then proceeded to solve it like this: What is the logic behind directly adding all the values? Secondly, how is it that the game is unfavorable to the player? There is an equal chance that the player will get 2, 3 or 5 and 1,4 or 6! Please help. (Also note that this doubt is not specific to this problem alone, since this concept is crucial to understanding this part of the chapter ""Probability"".) Much thanks in advance :) Regards. Edit : Thanks ever so much for these answers :) I read up on ""Expected values"" but encountered another important doubt. I have posted it here and hope that you will clear my doubt :)","I came across this question today: ""A fair die is tossed. If 2,3 or 5 occurs, the player wins that number of rupees, but if 1, 4 or 6 occurs, the player loses that number if rupees. Then find the possible payoffs for the player"". My textbook has then proceeded to solve it like this: What is the logic behind directly adding all the values? Secondly, how is it that the game is unfavorable to the player? There is an equal chance that the player will get 2, 3 or 5 and 1,4 or 6! Please help. (Also note that this doubt is not specific to this problem alone, since this concept is crucial to understanding this part of the chapter ""Probability"".) Much thanks in advance :) Regards. Edit : Thanks ever so much for these answers :) I read up on ""Expected values"" but encountered another important doubt. I have posted it here and hope that you will clear my doubt :)",,"['probability', 'probability-theory']"
61,Is there a way to find expected value of equation?,Is there a way to find expected value of equation?,,"If the random variable $X$ is binomially distributed with parameters $n=6$ and $p=0.3$, what is $$E(4+3X^2)$$ I know $E(X) = np = 1.8$. I solved this problem by finding $P(X)$ of all $X$ using $(^6_x)(0.3)^x(0.7)^{6-x}$ and then apply them with $4+3X^2$ to solve for $E(4+3X^2)$. But this takes too long and I think there should be a formula or another way to solve this. Can anyone tell help me out with this? Thank you in advance!","If the random variable $X$ is binomially distributed with parameters $n=6$ and $p=0.3$, what is $$E(4+3X^2)$$ I know $E(X) = np = 1.8$. I solved this problem by finding $P(X)$ of all $X$ using $(^6_x)(0.3)^x(0.7)^{6-x}$ and then apply them with $4+3X^2$ to solve for $E(4+3X^2)$. But this takes too long and I think there should be a formula or another way to solve this. Can anyone tell help me out with this? Thank you in advance!",,"['probability', 'discrete-mathematics', 'binomial-distribution']"
62,Expectation of expectation of indicator function,Expectation of expectation of indicator function,,"Is the following correct $$E[E[\mathbb{I}(X)]] = E[\mathbb{I}(X)]$$ I assume that $E[E[X]] = E[X]$, as $E[X]$ is a number and expected value of a constant is a constant, and that $\mathbb{I}(X)$ has a binomial distribution. $\mathbb{I}(X)$ represents an indicator function of some random variable $X$. Exact definition of $\mathbb{I}()$ is not important.","Is the following correct $$E[E[\mathbb{I}(X)]] = E[\mathbb{I}(X)]$$ I assume that $E[E[X]] = E[X]$, as $E[X]$ is a number and expected value of a constant is a constant, and that $\mathbb{I}(X)$ has a binomial distribution. $\mathbb{I}(X)$ represents an indicator function of some random variable $X$. Exact definition of $\mathbb{I}()$ is not important.",,"['probability', 'means']"
63,Characteristic functions of Poisson and normal distribution,Characteristic functions of Poisson and normal distribution,,"Basically the question is in two parts: $1.)$ Finding the characteristic of $P{(\lambda)}$ , and it is given, I just do not know how to get the sum that they got in the very last step in this expression(last equality is unclear): $$f(t)=\sum_{j=0}^{\infty}e^{itj}\frac{\lambda^j}{j!}e^\lambda=e^{\lambda(e^{it}-1)}$$ $2.)$ Finding the characteristic function of $N(0,1)$ and the last equality in the expression is unclear. $$ f_X(t) = \int_{\mathbb R}\frac{e^{itx}}{\sqrt{2 \pi}}e^{-x^2/2}dx        = \frac{1}{2\pi}\int_{\mathbb R}\cos(tx)e^{-x^2/2}dx $$ (I thought the reason for this is that the product of a even and odd function in an integral is $0$ but that's just probably my wishful thinking doing ) and the very last part that is unclear is that is says after differentiating this last line we get: $$ f_{X}'(t) = \frac{1}{2\pi}             \int_{-\infty}^{\infty} (-x)\sin (tx)e^{-x^2/2}dx $$","Basically the question is in two parts: Finding the characteristic of , and it is given, I just do not know how to get the sum that they got in the very last step in this expression(last equality is unclear): Finding the characteristic function of and the last equality in the expression is unclear. (I thought the reason for this is that the product of a even and odd function in an integral is but that's just probably my wishful thinking doing ) and the very last part that is unclear is that is says after differentiating this last line we get:","1.) P{(\lambda)} f(t)=\sum_{j=0}^{\infty}e^{itj}\frac{\lambda^j}{j!}e^\lambda=e^{\lambda(e^{it}-1)} 2.) N(0,1) 
f_X(t) = \int_{\mathbb R}\frac{e^{itx}}{\sqrt{2 \pi}}e^{-x^2/2}dx
       = \frac{1}{2\pi}\int_{\mathbb R}\cos(tx)e^{-x^2/2}dx
 0 
f_{X}'(t) = \frac{1}{2\pi}
            \int_{-\infty}^{\infty} (-x)\sin (tx)e^{-x^2/2}dx
","['probability', 'probability-distributions', 'normal-distribution', 'poisson-distribution', 'characteristic-functions']"
64,"If $\{X_n\}$ converges in probability to $1$, where does $\{1/X_n\}$ converge to?","If  converges in probability to , where does  converge to?",\{X_n\} 1 \{1/X_n\},"Without using the continuous mapping theorem, I want to show that, given $\{X_n\}$ is a sequence of random variables converging in probability to $1$, $\{1/X_n\}$ converges in probability to $1$. The place where I am stuck is: how do I know that probabilistically , $1/X_n$ converges? Because after I know that if $1/X_n\overset{p}\to a$ where $a$ is a real number, then I can use the fact that if $X_n$ and $Y_n$ converge in probability to $x$ and $y$ respectively, then $X_nY_n\overset{p}\to xy$. Then taking $Y_n=1/X_n$ the result will be immediate. But how do I know that the new sequence $1/X_n$ converges at all? I would love to use results in real analysis but I am sure they cannot be applied here.","Without using the continuous mapping theorem, I want to show that, given $\{X_n\}$ is a sequence of random variables converging in probability to $1$, $\{1/X_n\}$ converges in probability to $1$. The place where I am stuck is: how do I know that probabilistically , $1/X_n$ converges? Because after I know that if $1/X_n\overset{p}\to a$ where $a$ is a real number, then I can use the fact that if $X_n$ and $Y_n$ converge in probability to $x$ and $y$ respectively, then $X_nY_n\overset{p}\to xy$. Then taking $Y_n=1/X_n$ the result will be immediate. But how do I know that the new sequence $1/X_n$ converges at all? I would love to use results in real analysis but I am sure they cannot be applied here.",,"['probability', 'probability-theory', 'convergence-divergence']"
65,Sum of slowly varying functions,Sum of slowly varying functions,,"We call a function $L:(0,\infty)\rightarrow(0,\infty)$ slowly varying if for each $c>0$ one has $\displaystyle \lim_{x\to\infty}\frac{L(cx)}{L(x)}=1$. Can somebody give me a hint why the sum of such functions is also slowly varying?","We call a function $L:(0,\infty)\rightarrow(0,\infty)$ slowly varying if for each $c>0$ one has $\displaystyle \lim_{x\to\infty}\frac{L(cx)}{L(x)}=1$. Can somebody give me a hint why the sum of such functions is also slowly varying?",,"['calculus', 'probability', 'distribution-tails']"
66,"Throwing dice twice, with unlike probability of occourence?","Throwing dice twice, with unlike probability of occourence?",,"A loaded dice has the property that when the dice is thrown the probability of showing a given number is proportional to the number. For example $2$ is twice as likely to show up compared to $1$ and $3$ is thrice as likely to show up compared to $1$, And so on. What is the probability that when the dice is thrown twice the sum is $4$ or less. $$     P =  \frac 36 \cdot \frac 16 +\frac 16 \cdot \frac 36 + \frac 26 \cdot \frac 26 + \frac 16 \cdot \frac 26 + \frac 26 \cdot \frac 16 + \frac 16 \cdot \frac 16  =     \frac{15}{36}  $$ Where I am getting it wrong?","A loaded dice has the property that when the dice is thrown the probability of showing a given number is proportional to the number. For example $2$ is twice as likely to show up compared to $1$ and $3$ is thrice as likely to show up compared to $1$, And so on. What is the probability that when the dice is thrown twice the sum is $4$ or less. $$     P =  \frac 36 \cdot \frac 16 +\frac 16 \cdot \frac 36 + \frac 26 \cdot \frac 26 + \frac 16 \cdot \frac 26 + \frac 26 \cdot \frac 16 + \frac 16 \cdot \frac 16  =     \frac{15}{36}  $$ Where I am getting it wrong?",,"['probability', 'dice']"
67,What is the difference between $E[X\mid Y]$ vs $E[X\mid Y=y]$ and some of the properties of $E[X \mid Y]$?,What is the difference between  vs  and some of the properties of ?,E[X\mid Y] E[X\mid Y=y] E[X \mid Y],"I was trying to understand both intuitively and rigorously what the difference between $E[X\mid Y]$ vs $E[X\mid Y=y]$. Let me tell you first the things that do make sense to me. $E[X\mid Y=y]$ makes sense to me (I think). For me it means the value that we expect $X$ to have on average given that the event $Y=y$ was observed and it has deterministic value: $$E[X\mid Y=y] = \sum_x{xp_{X\mid Y}(x\mid y)} = \mu_{X\mid Y=y}$$ that can be computed like any other expectation (where the notation $\mu_{X\mid Y=y}$ denotes the actual real number that $E[X\mid Y=y]$ takes). $E[X\mid Y=y]$ is a function of y. Given $Y=y$, the conditional expectation will always be $\mu_{X\mid Y=y}$, which is governed by what specific y was observed. However, $E[X\mid Y]$ makes less sense to me. I have read something similar to the following: Notice now that Y is a random variable this time. Therefore, $E[X\mid Y]$ is a random variable. That sort of makes sense to me because, if $Y$ is random then the expected value of $X$ has to be random too. In other words, $Y$ being random, then consequently $E[X\mid Y]$ is random too. The statement $E[X|Y]$ is a r.v. and is the exact statement I would like to understand more precisely. I feel I understand less and I would like to address it. I feel if I really understood this concept of what $E[X\mid Y]$ really means, I should be able to answer the following questions: 1) If $E[X\mid Y]$ is random, then, what are the possible values it can take? Can it only take the values $\mu_{X\mid Y=y}$ for $y \in Y$? Say if $Y=\{1,2,3\}$ and $U_{X,Y} = \{\mu_{X\mid Y=1}= 11, \mu_{X\mid Y=2} = 22, \mu_{X\mid Y=3} = 33 \}$. Then is there any chance that $E[X\mid Y] = 123$? 2) Is the distribution of $E[X|Y]$ over $U_{X,Y}$ or over $Y$? 3) What is the probability distribution of $E[X\mid Y]$ if we have all the information we need about the distributions of $p_X(x)$ and $p_Y(y)$? Is it just the same as $p_Y(y)$? Is there a closed/specific formula for it? 4) When one is asked to find the distribution of $E[X\mid Y]$ are we asked to find $Pr[E[X\mid Y=y]]$ or $Pr[E[X\mid Y] = \mu]$? Is there a difference between the two? Is one nonsense while the other one is a valid probability distribution? 5) If we were to sketch the probability density function for $E[X \mid Y]$, would the horizontal axis be $y$ or $\mu$ ? i.e. would the probability density be a function of $y$ or of $E[X\mid Y] = \mu$? i.e. would $p_{E[X\mid Y]}(k)$ be a function of $y=Y$ or $E[X\mid Y]$? 6) Related to the above two question, it seems to me that writing an explicit formula for $E[X\mid Y=y]$ is easy, while for $E[X\mid Y]$ it is not (or at least for me). Is the formula $E[X\mid Y] = \sum_x{xp_{X\mid Y}(x\mid Y)}$? I would guess it is but for me its a very strange equation because we are conditioning on a random variable, or we are saying given $Y$, but $Y$ is random so its really not given. Therefore, I can't seem to find an expression for it that makes sense to me. Basically, it is not clear to me what $E[X\mid Y]$ means, because I don't know what its valid outcomes are, what its distribution is (in relation to $p_X(x), p_Y(y)$ or $p_{X,Y}(x,y)$ or anything) nor can I write an explicit formula for it that makes sense to me. I can't even decide if $p_{E[X \mid Y]}(k)$ is a function of y or $\mu=E[X|Y]$.","I was trying to understand both intuitively and rigorously what the difference between $E[X\mid Y]$ vs $E[X\mid Y=y]$. Let me tell you first the things that do make sense to me. $E[X\mid Y=y]$ makes sense to me (I think). For me it means the value that we expect $X$ to have on average given that the event $Y=y$ was observed and it has deterministic value: $$E[X\mid Y=y] = \sum_x{xp_{X\mid Y}(x\mid y)} = \mu_{X\mid Y=y}$$ that can be computed like any other expectation (where the notation $\mu_{X\mid Y=y}$ denotes the actual real number that $E[X\mid Y=y]$ takes). $E[X\mid Y=y]$ is a function of y. Given $Y=y$, the conditional expectation will always be $\mu_{X\mid Y=y}$, which is governed by what specific y was observed. However, $E[X\mid Y]$ makes less sense to me. I have read something similar to the following: Notice now that Y is a random variable this time. Therefore, $E[X\mid Y]$ is a random variable. That sort of makes sense to me because, if $Y$ is random then the expected value of $X$ has to be random too. In other words, $Y$ being random, then consequently $E[X\mid Y]$ is random too. The statement $E[X|Y]$ is a r.v. and is the exact statement I would like to understand more precisely. I feel I understand less and I would like to address it. I feel if I really understood this concept of what $E[X\mid Y]$ really means, I should be able to answer the following questions: 1) If $E[X\mid Y]$ is random, then, what are the possible values it can take? Can it only take the values $\mu_{X\mid Y=y}$ for $y \in Y$? Say if $Y=\{1,2,3\}$ and $U_{X,Y} = \{\mu_{X\mid Y=1}= 11, \mu_{X\mid Y=2} = 22, \mu_{X\mid Y=3} = 33 \}$. Then is there any chance that $E[X\mid Y] = 123$? 2) Is the distribution of $E[X|Y]$ over $U_{X,Y}$ or over $Y$? 3) What is the probability distribution of $E[X\mid Y]$ if we have all the information we need about the distributions of $p_X(x)$ and $p_Y(y)$? Is it just the same as $p_Y(y)$? Is there a closed/specific formula for it? 4) When one is asked to find the distribution of $E[X\mid Y]$ are we asked to find $Pr[E[X\mid Y=y]]$ or $Pr[E[X\mid Y] = \mu]$? Is there a difference between the two? Is one nonsense while the other one is a valid probability distribution? 5) If we were to sketch the probability density function for $E[X \mid Y]$, would the horizontal axis be $y$ or $\mu$ ? i.e. would the probability density be a function of $y$ or of $E[X\mid Y] = \mu$? i.e. would $p_{E[X\mid Y]}(k)$ be a function of $y=Y$ or $E[X\mid Y]$? 6) Related to the above two question, it seems to me that writing an explicit formula for $E[X\mid Y=y]$ is easy, while for $E[X\mid Y]$ it is not (or at least for me). Is the formula $E[X\mid Y] = \sum_x{xp_{X\mid Y}(x\mid Y)}$? I would guess it is but for me its a very strange equation because we are conditioning on a random variable, or we are saying given $Y$, but $Y$ is random so its really not given. Therefore, I can't seem to find an expression for it that makes sense to me. Basically, it is not clear to me what $E[X\mid Y]$ means, because I don't know what its valid outcomes are, what its distribution is (in relation to $p_X(x), p_Y(y)$ or $p_{X,Y}(x,y)$ or anything) nor can I write an explicit formula for it that makes sense to me. I can't even decide if $p_{E[X \mid Y]}(k)$ is a function of y or $\mu=E[X|Y]$.",,"['probability', 'probability-theory', 'probability-distributions', 'conditional-expectation']"
68,how is the probability of event will be affected by its successive events ??,how is the probability of event will be affected by its successive events ??,,There is a deck of 52 card. 1 card fall down. Then we take 2 cards from the rest. These 2 cards are spade. What is the probablity of that fallen card is spade?? I think answer should be $\frac 14$ ... ??,There is a deck of 52 card. 1 card fall down. Then we take 2 cards from the rest. These 2 cards are spade. What is the probablity of that fallen card is spade?? I think answer should be $\frac 14$ ... ??,,['probability']
69,Meaning of Fisher's information,Meaning of Fisher's information,,"If I am correct, Fisher's information at parameter $\theta$ is defined to be the variance of the score function at $\theta$. The score function is defined as the derivative of the log-likelhood function wrt $\theta$, and therefore measures the sensitivity of the log-likelihood function wrt $\theta$. I was wondering how to understand the meaning of Fisher's information? Especially, why does Wikipedia say: The Fisher information is a way of measuring the amount of information that an observable random variable $X$ carries about an unknown parameter $θ$ upon which the probability of $X$ depends. What kind of information is in ""the amount of information""? Shannon information, no? Why is the ""information"" carried by $X$ about $\theta$? Thanks and regards!","If I am correct, Fisher's information at parameter $\theta$ is defined to be the variance of the score function at $\theta$. The score function is defined as the derivative of the log-likelhood function wrt $\theta$, and therefore measures the sensitivity of the log-likelihood function wrt $\theta$. I was wondering how to understand the meaning of Fisher's information? Especially, why does Wikipedia say: The Fisher information is a way of measuring the amount of information that an observable random variable $X$ carries about an unknown parameter $θ$ upon which the probability of $X$ depends. What kind of information is in ""the amount of information""? Shannon information, no? Why is the ""information"" carried by $X$ about $\theta$? Thanks and regards!",,"['probability', 'statistics']"
70,Prove an inequality given only the moment generating function,Prove an inequality given only the moment generating function,,"If given $X$ as a non-negative random variable that has mgf $M_X(s) = E[e^{sX}]$, how can I prove $\Pr(X \ge a) \le e^{-as} \centerdot M_X(s)$ where $a > 0$ and $s \ge 0$ ? I figure the Markov inequality is a good start: $\Pr(X \ge a) \le \dfrac{E[X]}{a}$ But this is where I'm stuck. Of course you can derive $M_x(s)$ at $s=0$ to get $E[X]$, but that doesn't really get you anywhere here. Any suggestions?","If given $X$ as a non-negative random variable that has mgf $M_X(s) = E[e^{sX}]$, how can I prove $\Pr(X \ge a) \le e^{-as} \centerdot M_X(s)$ where $a > 0$ and $s \ge 0$ ? I figure the Markov inequality is a good start: $\Pr(X \ge a) \le \dfrac{E[X]}{a}$ But this is where I'm stuck. Of course you can derive $M_x(s)$ at $s=0$ to get $E[X]$, but that doesn't really get you anywhere here. Any suggestions?",,"['probability', 'statistics']"
71,Choosing between $n$ things using dice?,Choosing between  things using dice?,n,"For which $n$ is there a finite algorithm to choose between $n$ things with the same probability using a die? For example, we can choose between 2 things, 3 things, 4 things, 6 things, and 8 things, but it seems we cannot choose between 7 things with a finite algorithm.","For which $n$ is there a finite algorithm to choose between $n$ things with the same probability using a die? For example, we can choose between 2 things, 3 things, 4 things, 6 things, and 8 things, but it seems we cannot choose between 7 things with a finite algorithm.",,['probability']
72,Explicit solution of a linear SDE,Explicit solution of a linear SDE,,"I'd like an explicit formula as a function of $W_t$ (standard Brownian motion) and $\lambda >0$ for the solution of the following SDE: $$\mathrm dX_t = \mathrm dW_t - \lambda X_t \,\mathrm dt$$ Someone could help me please?","I'd like an explicit formula as a function of $W_t$ (standard Brownian motion) and $\lambda >0$ for the solution of the following SDE: $$\mathrm dX_t = \mathrm dW_t - \lambda X_t \,\mathrm dt$$ Someone could help me please?",,"['probability', 'stochastic-processes', 'stochastic-calculus', 'stochastic-integrals', 'stochastic-differential-equations']"
73,Almost sure convergence for a sequence of random variables,Almost sure convergence for a sequence of random variables,,I have read in some book that in case of almost sure convergence of a sequence of random variable it possible that $|X_{n}(\omega) - X(\omega)|$ can be extremely large for $\omega$ in a small probability set. How is that possible ? Here the sequence {$X_{n}$} of random variables converges to $X$ in almost sure sense. By almost sure convergence we mean that $P(\omega : X_{n}(\omega)=X(\omega)$ as $n$->infinity)=1 i.e.$P(\omega$ : for any $\epsilon > 0 $there exist an $N$ such that for all $n>=N$  $|X_{n}(\omega)-X(\omega)| < \epsilon)$ = 1 . Then how can $|X_{n}(\omega) - X(\omega)|$ be extremely large for $\omega$ in a small probability set where $|X_{n}(\omega) - X(\omega)| < \epsilon$ with probability 1  ? confused.,I have read in some book that in case of almost sure convergence of a sequence of random variable it possible that $|X_{n}(\omega) - X(\omega)|$ can be extremely large for $\omega$ in a small probability set. How is that possible ? Here the sequence {$X_{n}$} of random variables converges to $X$ in almost sure sense. By almost sure convergence we mean that $P(\omega : X_{n}(\omega)=X(\omega)$ as $n$->infinity)=1 i.e.$P(\omega$ : for any $\epsilon > 0 $there exist an $N$ such that for all $n>=N$  $|X_{n}(\omega)-X(\omega)| < \epsilon)$ = 1 . Then how can $|X_{n}(\omega) - X(\omega)|$ be extremely large for $\omega$ in a small probability set where $|X_{n}(\omega) - X(\omega)| < \epsilon$ with probability 1  ? confused.,,[]
74,How do you sum PDF's of random variables?,How do you sum PDF's of random variables?,,"I have a question asking me to determine the PDF of $L=X+Y+W$, where $X$, $Y$ and $W$ are all independent. $X$ is a Bernoulli random variable with parameter $p$, $Y \sim \mathrm{Binomial}(10, 0.6)$ and $W$ is a Gaussian random variable with zero mean and unit variance (meaning is is a standard normal random variable). I know the PDF's of $X$, $Y$ and $W$ (sort of hard to type out, but I know them). Could I get some sort of hint as to how these are added together?","I have a question asking me to determine the PDF of $L=X+Y+W$, where $X$, $Y$ and $W$ are all independent. $X$ is a Bernoulli random variable with parameter $p$, $Y \sim \mathrm{Binomial}(10, 0.6)$ and $W$ is a Gaussian random variable with zero mean and unit variance (meaning is is a standard normal random variable). I know the PDF's of $X$, $Y$ and $W$ (sort of hard to type out, but I know them). Could I get some sort of hint as to how these are added together?",,['probability']
75,Problem about $n$ six-sided dice and the sum of the values,Problem about  six-sided dice and the sum of the values,n,"(AHSME 1994) When $n$ standard six-sided dice are rolled, the probability of obtaining a sum of 1994 is greater than zero and is the same as the probability of obtaining a sum of $S$. What is the smallest possible value of $S$? (I've been trying to use generating function, but without success. I took this one from The Art and Craft of Problem Solving - Paul Zeitz, second ed, pag. 8, chapter 1 exercise 1.3.6.)","(AHSME 1994) When $n$ standard six-sided dice are rolled, the probability of obtaining a sum of 1994 is greater than zero and is the same as the probability of obtaining a sum of $S$. What is the smallest possible value of $S$? (I've been trying to use generating function, but without success. I took this one from The Art and Craft of Problem Solving - Paul Zeitz, second ed, pag. 8, chapter 1 exercise 1.3.6.)",,"['probability', 'elementary-number-theory', 'discrete-mathematics', 'dice']"
76,Statistical Significance Dice Probability,Statistical Significance Dice Probability,,"I think I just need to be pushed to the right formula or algorithm... Imagine you've got a ""real"" dice which is not an optimal one. And you want to know the confidence intervals for each result. So you rolled the dice a couple of times and get the following absolute probabilities as result: #eyes #occurrences ------------------ 1     10 2     11 3     24 4     13 5     14 6     11 You actually want to know weather e.g. this 24 times 3 eyes is just a random result or weather it's really more probable. If so, how much more probable is it (for sure)? So I would like to calculate a 99%-confidence interval for the probabilities. How to calculate this? I probably know this from statistics in university, but just forgot it... so you don't need to go to much into detail. Just need the right formula/algorithm to look for... Thanks for your help. --- edit --- Just to make clear, why I do not just lookup ""Confidence Interval"" at wikipedia. I would know how to calculate everything if there would be only two cases (e.g. like a coin... 0 and 1). Then I would be able to apply the formula, but I just didn't use such statistics for some years now and just don't see the solution how to reduce the problem. I just think about taking the result in question (e.g. 3 eyes) as ""p"" and all other results as ""\not p""; does that work?","I think I just need to be pushed to the right formula or algorithm... Imagine you've got a ""real"" dice which is not an optimal one. And you want to know the confidence intervals for each result. So you rolled the dice a couple of times and get the following absolute probabilities as result: #eyes #occurrences ------------------ 1     10 2     11 3     24 4     13 5     14 6     11 You actually want to know weather e.g. this 24 times 3 eyes is just a random result or weather it's really more probable. If so, how much more probable is it (for sure)? So I would like to calculate a 99%-confidence interval for the probabilities. How to calculate this? I probably know this from statistics in university, but just forgot it... so you don't need to go to much into detail. Just need the right formula/algorithm to look for... Thanks for your help. --- edit --- Just to make clear, why I do not just lookup ""Confidence Interval"" at wikipedia. I would know how to calculate everything if there would be only two cases (e.g. like a coin... 0 and 1). Then I would be able to apply the formula, but I just didn't use such statistics for some years now and just don't see the solution how to reduce the problem. I just think about taking the result in question (e.g. 3 eyes) as ""p"" and all other results as ""\not p""; does that work?",,"['probability', 'statistics', 'dice']"
77,Central Limit Theorem,Central Limit Theorem,,"If N is a poisson random variable, why is the following true? It is from ""Probability and Stochastic Processes"" by Yates, page 301, equation 8.2 to 8.3 $$ P\left(\left|\frac{N-E(N)}{\sigma_N}\right| \geq \frac{c}{\sigma_N}\right) = P(|Z| \geq \frac{c}{\sigma_N})$$ Z is the standard normal Gaussian random variable. The explanation in the text is : ""Since E[N] is large"", the CLT can be used. But I am familiar with the CLT being used with sums of random variables. Thanks.","If N is a poisson random variable, why is the following true? It is from ""Probability and Stochastic Processes"" by Yates, page 301, equation 8.2 to 8.3 $$ P\left(\left|\frac{N-E(N)}{\sigma_N}\right| \geq \frac{c}{\sigma_N}\right) = P(|Z| \geq \frac{c}{\sigma_N})$$ Z is the standard normal Gaussian random variable. The explanation in the text is : ""Since E[N] is large"", the CLT can be used. But I am familiar with the CLT being used with sums of random variables. Thanks.",,['probability']
78,"How to calculate $E[(\int_0^t{W_sds})^n], n \geq 2$",How to calculate,"E[(\int_0^t{W_sds})^n], n \geq 2","Let $W_t$ be a standard one dimension Brownian Motion with $W_0=0$ and $X_t=\int_0^t{W_sds}$. With the help of ito formula, we could get $$E[(X_t)^2]=\frac{1}{3}t^3$$  $$E[(X_t)^3]=0$$ When I try to employ the same method to calculate the general case $E[(X_t)^n]$, I got stuck. I guess $X_t$ should be normal distribution since it could be the limit of the following $$\lim_{n\rightarrow \infty}{\sum_{i=0}^{n-1}{W_{t_i}(t_{i+1}-t_i)}},$$ where $ W_{t_i}\sim norm(0,\sqrt{\frac{t_i}{n}}).$ If it is true, the problem would be trivial. Update: Thanks for all the suggestions. Now I believe $X_t$ is a Gaussian process. How about for this integral $$Y_t=\int_0^t{f(W_s)ds}$$ if we assume that $f$ is some good function, say polynomial or exponential, i.e  $$Y_t=\int_0^t{e^{W_s}ds}$$ $$Y_t=\int_0^t{[a_n(W_s)^n+a_{n-1}(W_s)^{n-1}+...+a_0]ds}$$","Let $W_t$ be a standard one dimension Brownian Motion with $W_0=0$ and $X_t=\int_0^t{W_sds}$. With the help of ito formula, we could get $$E[(X_t)^2]=\frac{1}{3}t^3$$  $$E[(X_t)^3]=0$$ When I try to employ the same method to calculate the general case $E[(X_t)^n]$, I got stuck. I guess $X_t$ should be normal distribution since it could be the limit of the following $$\lim_{n\rightarrow \infty}{\sum_{i=0}^{n-1}{W_{t_i}(t_{i+1}-t_i)}},$$ where $ W_{t_i}\sim norm(0,\sqrt{\frac{t_i}{n}}).$ If it is true, the problem would be trivial. Update: Thanks for all the suggestions. Now I believe $X_t$ is a Gaussian process. How about for this integral $$Y_t=\int_0^t{f(W_s)ds}$$ if we assume that $f$ is some good function, say polynomial or exponential, i.e  $$Y_t=\int_0^t{e^{W_s}ds}$$ $$Y_t=\int_0^t{[a_n(W_s)^n+a_{n-1}(W_s)^{n-1}+...+a_0]ds}$$",,"['probability', 'stochastic-processes', 'probability-distributions']"
79,expected number of shuffles to sort the cards,expected number of shuffles to sort the cards,,"Initially the deck is randomly ordered. The aim is to sort the deck in order. Now in each turn the deck is shuffled randomly. If any of the initial or last cards are in sorted order then they are kept aside, and the process is followed with rest of the cards. The question now is what is the expected number of shuffles for the entire deck to get sorted in this manner. e.g. let's say initially the deck is $(3,5,6,2,7,1,4)$. After the first shuffle it becomes $(1,2,6,4,5,3,7)$, then the next shuffle will be done with $(6,4,5,3)$ as $(1,2)$ and $(7)$ are already in proper order. Check that even if $5$ in also at right place, but not in continuous order hence considered for shuffle. What I know is that the expected number of shuffles to sort the deck in one attempt is $n!$. (Something known as bogo-sort). But can't think any further. Any help will be appreciated.","Initially the deck is randomly ordered. The aim is to sort the deck in order. Now in each turn the deck is shuffled randomly. If any of the initial or last cards are in sorted order then they are kept aside, and the process is followed with rest of the cards. The question now is what is the expected number of shuffles for the entire deck to get sorted in this manner. e.g. let's say initially the deck is $(3,5,6,2,7,1,4)$. After the first shuffle it becomes $(1,2,6,4,5,3,7)$, then the next shuffle will be done with $(6,4,5,3)$ as $(1,2)$ and $(7)$ are already in proper order. Check that even if $5$ in also at right place, but not in continuous order hence considered for shuffle. What I know is that the expected number of shuffles to sort the deck in one attempt is $n!$. (Something known as bogo-sort). But can't think any further. Any help will be appreciated.",,"['permutations', 'probability', 'sorting']"
80,What is wrong with my solution? Probability that every bridge player gets an ace.,What is wrong with my solution? Probability that every bridge player gets an ace.,,"Answers here and here do not answer my question. We have $4$ players playing bridge. What is the probability that each player gets an ace? The solution provided with the problem first finds the total no. of ways of dealing the cards using the multinomial coefficient : $$\frac{52!}{13! \ 13! \ 13! \ 13!}$$ For favourable outcomes, there are $4!$ ways of dealing out the aces ( $1$ to each player) and the remaining cards can be dealt in (again, by multinomial coefficient) $$\frac{48!}{12! \ 12! \ 12! \ 12!}$$ ways. Thus, the probability becomes: $$4!\frac{48!(13!)^4}{52!(12!)^4}$$ Simplifying: $$24\frac{13^4}{49\cdot 50\cdot 51\cdot 52}$$ $$\frac{13^3}{49\cdot 25\cdot 17} \approx 0.1055$$ The last calculation is still hard without a calculator, so I thought of an (apparently incorrect) approach. My solution (what is the mistake here?): Let us forget about the other cards, and concentrate on the aces. There are $4!$ ways that each player will get an ace. There are $4^4$ total ways of distributing the aces. So, the probability is: $$\frac{4!}{4^4} = \frac{3!}{4^3} = \frac{3}{32} = \color{red}{0.09375}$$ Which is significantly off. So obviously there is a mistake in my argument. My guess is that the other $12$ (or $48$ cards) somehow influence the answer. Could you please point out the mistake in the answer? I would also be grateful if anyone could explain the mistake with a smaller example, which shows how what I've done and the original answer differ.","Answers here and here do not answer my question. We have players playing bridge. What is the probability that each player gets an ace? The solution provided with the problem first finds the total no. of ways of dealing the cards using the multinomial coefficient : For favourable outcomes, there are ways of dealing out the aces ( to each player) and the remaining cards can be dealt in (again, by multinomial coefficient) ways. Thus, the probability becomes: Simplifying: The last calculation is still hard without a calculator, so I thought of an (apparently incorrect) approach. My solution (what is the mistake here?): Let us forget about the other cards, and concentrate on the aces. There are ways that each player will get an ace. There are total ways of distributing the aces. So, the probability is: Which is significantly off. So obviously there is a mistake in my argument. My guess is that the other (or cards) somehow influence the answer. Could you please point out the mistake in the answer? I would also be grateful if anyone could explain the mistake with a smaller example, which shows how what I've done and the original answer differ.",4 \frac{52!}{13! \ 13! \ 13! \ 13!} 4! 1 \frac{48!}{12! \ 12! \ 12! \ 12!} 4!\frac{48!(13!)^4}{52!(12!)^4} 24\frac{13^4}{49\cdot 50\cdot 51\cdot 52} \frac{13^3}{49\cdot 25\cdot 17} \approx 0.1055 4! 4^4 \frac{4!}{4^4} = \frac{3!}{4^3} = \frac{3}{32} = \color{red}{0.09375} 12 48,"['probability', 'combinatorics']"
81,Boxes and dice game,Boxes and dice game,,"You have $9$ boxes numbered $1$ through $9$ and then you have two $6$ -sided dice. Each turn you roll the two dice and deduct the sum of the dice from the boxes by removing the number itself or any combination of boxes that sum up to the sum of the dice. For example, let's say you first roll a $4$ and $5$ . You then have the option to remove some combination of boxes that sum up to $9$ (yes, you can remove $9$ itself too). After these boxes are removed you roll both dice again and continue until you roll a sum that's impossible to remove with the remaining boxes. For example, if you're left with boxes $1$ , $3$ , $4$ and you roll a $10$ . The game is over and your final score is $\sum_{i = 1}^9 i = 45$ minus the sum of the remaining boxes. So if you were left with $1$ , $3$ , $4$ you end up with $37$ . Question. What is the optimal strategy?","You have boxes numbered through and then you have two -sided dice. Each turn you roll the two dice and deduct the sum of the dice from the boxes by removing the number itself or any combination of boxes that sum up to the sum of the dice. For example, let's say you first roll a and . You then have the option to remove some combination of boxes that sum up to (yes, you can remove itself too). After these boxes are removed you roll both dice again and continue until you roll a sum that's impossible to remove with the remaining boxes. For example, if you're left with boxes , , and you roll a . The game is over and your final score is minus the sum of the remaining boxes. So if you were left with , , you end up with . Question. What is the optimal strategy?",9 1 9 6 4 5 9 9 1 3 4 10 \sum_{i = 1}^9 i = 45 1 3 4 37,"['probability', 'combinatorics', 'algebra-precalculus', 'dice', 'algorithmic-game-theory']"
82,Probability that $6$ of $9$ coin flips yield heads given that the first flip yields tails [closed],Probability that  of  coin flips yield heads given that the first flip yields tails [closed],6 9,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 2 years ago . Improve this question ANSWER ONLY part (c) Question : When coin 1 is flipped, it lands on heads with probability $\dfrac{4}{5}$ ; when coin $2$ is flipped it lands on heads with probability $\dfrac{7}{10}$ . (a)    If coin $1$ is flipped $11$ times, find the probability that it lands on heads at least $9$ times. (b)   If one of the coins is randomly selected and flipped $9$ times, what is the probability that it lands on heads exactly $6$ times? (c) In part (b), given that the first of these $9$ flips lands on tails, find the conditional probability that exactly $6$ of the $9$ flips land on heads. I got the answer to part (c) as $0.19688$ but it is wrong. My solution: $$\left(\left.\frac45\middle/\Big(\frac45+\frac7{10}\Big)\right.\right)\times\left({^8C_5}(\frac45)^5(\frac15)^3\right)+\left(\left.\frac7{10}\middle/\Big(\frac7{10}+\frac45\Big)\right.\right)\times\left({^8C_5}(\frac7{10})^5(\frac3{10})^3\right) =019688$$","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 2 years ago . Improve this question ANSWER ONLY part (c) Question : When coin 1 is flipped, it lands on heads with probability ; when coin is flipped it lands on heads with probability . (a)    If coin is flipped times, find the probability that it lands on heads at least times. (b)   If one of the coins is randomly selected and flipped times, what is the probability that it lands on heads exactly times? (c) In part (b), given that the first of these flips lands on tails, find the conditional probability that exactly of the flips land on heads. I got the answer to part (c) as but it is wrong. My solution:","\dfrac{4}{5} 2 \dfrac{7}{10} 1 11 9 9 6 9 6 9 0.19688 \left(\left.\frac45\middle/\Big(\frac45+\frac7{10}\Big)\right.\right)\times\left({^8C_5}(\frac45)^5(\frac15)^3\right)+\left(\left.\frac7{10}\middle/\Big(\frac7{10}+\frac45\Big)\right.\right)\times\left({^8C_5}(\frac7{10})^5(\frac3{10})^3\right)
=019688","['probability', 'combinatorics', 'conditional-probability']"
83,How can a markov transition matrix have eigenvalues other than 1?,How can a markov transition matrix have eigenvalues other than 1?,,"A Markov transition matrix has all nonnegative entries and so by the Perron-Frobenius theorem has real, positive eigenvalues. In particular the largest eigenvalue is 1 by property 11 here . Furthermore in these notes (sec 10.3) it says that the eigenvalues of $P$ are $1 = \lambda_1 > \lambda_2 \geq \dots \geq \lambda_N \geq -1$ . But how can a transition matrix $P$ have eigenvalues less than 1? Since the matrix is acting on probability distributions $v$ , which have to have $\sum_i v_i = 1$ , we cannot have $Pv = cv$ with $c\neq 1$ since $\sum_i cv_i = c$ .","A Markov transition matrix has all nonnegative entries and so by the Perron-Frobenius theorem has real, positive eigenvalues. In particular the largest eigenvalue is 1 by property 11 here . Furthermore in these notes (sec 10.3) it says that the eigenvalues of are . But how can a transition matrix have eigenvalues less than 1? Since the matrix is acting on probability distributions , which have to have , we cannot have with since .",P 1 = \lambda_1 > \lambda_2 \geq \dots \geq \lambda_N \geq -1 P v \sum_i v_i = 1 Pv = cv c\neq 1 \sum_i cv_i = c,"['probability', 'markov-chains', 'transition-matrix']"
84,Probability of two fair dice rolls having a total of $7$ or $11$?,Probability of two fair dice rolls having a total of  or ?,7 11,"What's the probability of getting a total of $7$ or $11$ when a pair of fair dice is tossed? I already looked it up on the internet and my answer matched the same answer on a site. However, though I am confident that my solution is right, I am curious if there's a method in which I could compute this faster since the photo below shows how time consuming that kind of approach would be. Thanks in advance.","What's the probability of getting a total of or when a pair of fair dice is tossed? I already looked it up on the internet and my answer matched the same answer on a site. However, though I am confident that my solution is right, I am curious if there's a method in which I could compute this faster since the photo below shows how time consuming that kind of approach would be. Thanks in advance.",7 11,"['probability', 'dice']"
85,Optimal Betting Strategy question,Optimal Betting Strategy question,,"I am preparing for an exam in probability theory and I bumped against a question I can't solve. Given are an integer starting capital $k$ , an end goal capital $m$ and a period of $n$ days. Each day I can bet some integer amount $X$ of my choosing $(X \leq k)$ on an unfair coin landing on heads. The probability the coin lands on heads is different each day, with $p_i$ denoting the probability of it landing on heads on day $i$ with $i \in (1,...,n)$ . If the bet is successful, I increase my capital by $X$ , if not I lose $X$ amount. (All probabilities $p_1, p_2,..., p_n$ are known before the betting process starts). The question is : With an optimal betting strategy, what is the probability of achieving capital at least equal to $m$ after $n$ days? An example input : $n = 5, k = 2, m = 20, p_1 = 0.3, p_2 = 0.5, p_3 = 0.2, p_4 = 0.7, p_5 = 1.0$ Though not from a homework, if this question falls under the category of questions one should solve by themselves or look for help from a tutor or elsewhere, please tell me, I will take it down. Any advice as to how to approach the problem would be awesome though.","I am preparing for an exam in probability theory and I bumped against a question I can't solve. Given are an integer starting capital , an end goal capital and a period of days. Each day I can bet some integer amount of my choosing on an unfair coin landing on heads. The probability the coin lands on heads is different each day, with denoting the probability of it landing on heads on day with . If the bet is successful, I increase my capital by , if not I lose amount. (All probabilities are known before the betting process starts). The question is : With an optimal betting strategy, what is the probability of achieving capital at least equal to after days? An example input : Though not from a homework, if this question falls under the category of questions one should solve by themselves or look for help from a tutor or elsewhere, please tell me, I will take it down. Any advice as to how to approach the problem would be awesome though.","k m n X (X \leq k) p_i i i \in (1,...,n) X X p_1, p_2,..., p_n m n n = 5, k = 2, m = 20, p_1 = 0.3, p_2 = 0.5, p_3 = 0.2, p_4 = 0.7, p_5 = 1.0","['probability', 'probability-theory']"
86,Mark a six-sided die with the results of six rolls of a previous die. How many iterations until all the faces match?,Mark a six-sided die with the results of six rolls of a previous die. How many iterations until all the faces match?,,"This is FiveThirtyEight's ""Riddler Classic"" puzzle for 27 March, 2020: From Chris Nho comes a question of rolling (and re-rolling) a die: You start with a fair 6-sided die and roll it six times, recording the results of each roll. You then write these numbers on the six faces of another, unlabeled fair die. For example, if your six rolls were 3, 5, 3, 6, 1 and 2, then your second die wouldn’t have a 4 on it; instead, it would have two 3s. Next, you roll this second die six times. You take those six numbers and write them on the faces  of yet another fair die, and you continue this process of generating a new die from the previous one. Eventually, you’ll have a die with the same number on all six faces. What is the average number of rolls it will take to reach this state? Through numerical simulations, I know that the average number of rows to reach the final state is approximately 9.66 , and the PDF of the number of rows to reach this state looks like My question is: how do we calculate the average number of rows analytically? Is is possible to analytically calculate its PDF as well?","This is FiveThirtyEight's ""Riddler Classic"" puzzle for 27 March, 2020: From Chris Nho comes a question of rolling (and re-rolling) a die: You start with a fair 6-sided die and roll it six times, recording the results of each roll. You then write these numbers on the six faces of another, unlabeled fair die. For example, if your six rolls were 3, 5, 3, 6, 1 and 2, then your second die wouldn’t have a 4 on it; instead, it would have two 3s. Next, you roll this second die six times. You take those six numbers and write them on the faces  of yet another fair die, and you continue this process of generating a new die from the previous one. Eventually, you’ll have a die with the same number on all six faces. What is the average number of rolls it will take to reach this state? Through numerical simulations, I know that the average number of rows to reach the final state is approximately 9.66 , and the PDF of the number of rows to reach this state looks like My question is: how do we calculate the average number of rows analytically? Is is possible to analytically calculate its PDF as well?",,"['probability', 'combinatorics', 'puzzle', 'dice']"
87,Is the Gaussian density Lipschitz continuous?,Is the Gaussian density Lipschitz continuous?,,"More precisely, define $\phi(x) = \frac1{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$ . Does there exists a constant $L$ such that $$|\phi(x)-\phi(y)|\le L|x-y|,$$ for all $x,y\in \mathbb{R}$ . If yes, what is the minimal $L$ ?","More precisely, define . Does there exists a constant such that for all . If yes, what is the minimal ?","\phi(x) = \frac1{\sqrt{2\pi}}e^{-\frac{x^2}{2}} L |\phi(x)-\phi(y)|\le L|x-y|, x,y\in \mathbb{R} L","['real-analysis', 'calculus', 'probability', 'lipschitz-functions']"
88,Toss two coins until two Heads and two Tails come up,Toss two coins until two Heads and two Tails come up,,"You play a game where you toss two fair coins in the air. You always win $1. However, if you have tossed 2 heads at least once, and 2 tails at least once, you surrender all winnings, and cannot play again. You may stop playing at anytime. What’s your strategy? My thoughts were that this seems similar to the coupon collector. We have two bad events (2H and 2T). So after the occurence of the first bad event the second one will occur in an expected number of 4 turns. So my strategy is to stop after 3 tosses from the moment the first bad event occured. However, I can't prove it.","You play a game where you toss two fair coins in the air. You always win $1. However, if you have tossed 2 heads at least once, and 2 tails at least once, you surrender all winnings, and cannot play again. You may stop playing at anytime. What’s your strategy? My thoughts were that this seems similar to the coupon collector. We have two bad events (2H and 2T). So after the occurence of the first bad event the second one will occur in an expected number of 4 turns. So my strategy is to stop after 3 tosses from the moment the first bad event occured. However, I can't prove it.",,"['probability', 'combinatorics']"
89,Married Couple Probability Question [closed],Married Couple Probability Question [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question There are $n$ husband and wife couples at a party. If the $n$ men and $n$ women are randomly paired with one another, what is the expected number of pairings that are actual husband-wife couples? Had this on a test earlier and cannot figure it out. Anyone have the answer? FYI, the answer choices were 1, n/5, n/2, none of the above","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question There are husband and wife couples at a party. If the men and women are randomly paired with one another, what is the expected number of pairings that are actual husband-wife couples? Had this on a test earlier and cannot figure it out. Anyone have the answer? FYI, the answer choices were 1, n/5, n/2, none of the above",n n n,"['probability', 'combinatorics']"
90,Best game strategy in draws from a bin,Best game strategy in draws from a bin,,"A bin has 2 white balls and 3 black balls. You play a game as follows: you draw balls one at a time without replacement. Every time you draw a white ball , you win a dollar, but every time you draw a black ball , you loose a dollar . You can stop the game at any time.Devise a strategy for playing this game which results in an expected profit. According to my reasoning the best strategy is not to play the game at all : the expected value at every extraction remains the same and it's always negative $$E[X]=\left(\frac{2}{5}\right)*1 +\left(\frac{3}{5}\right)*(-1) =-\frac{1}{5}$$ So since I can treat it as a sum of expectations of random variables ,the best strategy is not to play…so the best expected profit is zero dollars right?","A bin has 2 white balls and 3 black balls. You play a game as follows: you draw balls one at a time without replacement. Every time you draw a white ball , you win a dollar, but every time you draw a black ball , you loose a dollar . You can stop the game at any time.Devise a strategy for playing this game which results in an expected profit. According to my reasoning the best strategy is not to play the game at all : the expected value at every extraction remains the same and it's always negative So since I can treat it as a sum of expectations of random variables ,the best strategy is not to play…so the best expected profit is zero dollars right?",E[X]=\left(\frac{2}{5}\right)*1 +\left(\frac{3}{5}\right)*(-1) =-\frac{1}{5},"['probability', 'statistics', 'game-theory']"
91,Optimize the Game - Make Fair Game with Biased Coins,Optimize the Game - Make Fair Game with Biased Coins,,"The traditional question states that given an unfair coin, with 0.7 probability of heads, how would you make a fair game? The answer is only consider two outcomes. If two trials yields $HT$ then the player wins, else if $TH$ the player loses. Now the interesting part is the follow-up question. If now my head probability is 0.99, how should one go about optimizing making a fair game out of this coin? ""Optimizing"" means to come up with a fair game with minimal amount of coin tosses. The original approach is inefficient, because most of the outcome would be $HH$ , with probability $0.99^2$ ). The probability of $HT$ or $TH$ would be relatively small.","The traditional question states that given an unfair coin, with 0.7 probability of heads, how would you make a fair game? The answer is only consider two outcomes. If two trials yields then the player wins, else if the player loses. Now the interesting part is the follow-up question. If now my head probability is 0.99, how should one go about optimizing making a fair game out of this coin? ""Optimizing"" means to come up with a fair game with minimal amount of coin tosses. The original approach is inefficient, because most of the outcome would be , with probability ). The probability of or would be relatively small.",HT TH HH 0.99^2 HT TH,"['probability', 'statistics']"
92,"Showing $\int_{\mathbb R} \mid F(x)-G(x)\mid dx = \int_0^1 \mid F^{-1}(u)-G^{-1}(u)\mid du$ with $F$, $G$ CDF functions","Showing  with ,  CDF functions",\int_{\mathbb R} \mid F(x)-G(x)\mid dx = \int_0^1 \mid F^{-1}(u)-G^{-1}(u)\mid du F G,"Let's $X$ and $Y$ have CDF functions admitting moment of order $1$ . Let's be $F$ cdf of $X$ and $G$ cdf of $Y$ . I want to show that $$\int_{\mathbb R} \mid F(x)-G(x)\mid dx = \int_{0}^{1} \mid F^{-1}(u)-G^{-1}(u)\mid du\,.$$",Let's and have CDF functions admitting moment of order . Let's be cdf of and cdf of . I want to show that,"X Y 1 F X G Y \int_{\mathbb R} \mid F(x)-G(x)\mid dx = \int_{0}^{1} \mid F^{-1}(u)-G^{-1}(u)\mid du\,.","['probability', 'probability-theory', 'probability-distributions', 'definite-integrals', 'inverse-function']"
93,Concentration inequality applied for robust estimation of the mean,Concentration inequality applied for robust estimation of the mean,,"Problem: (Page 19 in ""Vershynin, Roman (2018). High-Dimensional Probability. Cambridge University Press. ISBN 9781108415194."") Suppose we want to estimate the mean µ of a random variable $X$ from a   sample $X_1 , \dots , X_N$ drawn independently from the distribution   of $X$ . We want an $\varepsilon$ -accurate estimate, i.e. one that   falls in the interval $(\mu − \varepsilon, \mu + \varepsilon)$ . Show that a sample of size $N = O( \log (\delta^{−1} )\, \sigma^2 / \varepsilon^2 )$ is sufficient to compute an $\varepsilon$ -accurate   estimate with probability at least $1 −\delta$ . Hint: Use the median of $O(log(\delta^{−1}))$ weak estimates. It is easy to use Chebyshev's inequality to find a weak estimate of $N = O(  \sigma^2 / (\delta \varepsilon^2) )$ . However, I do not how to find inequality about their median. The wikipedia of median ( https://en.wikipedia.org/wiki/Median#The_sample_median ) says sample median asymptotically normal but this does not give a bound for specific $N$ . Any suggestion is welcome.","Problem: (Page 19 in ""Vershynin, Roman (2018). High-Dimensional Probability. Cambridge University Press. ISBN 9781108415194."") Suppose we want to estimate the mean µ of a random variable from a   sample drawn independently from the distribution   of . We want an -accurate estimate, i.e. one that   falls in the interval . Show that a sample of size is sufficient to compute an -accurate   estimate with probability at least . Hint: Use the median of weak estimates. It is easy to use Chebyshev's inequality to find a weak estimate of . However, I do not how to find inequality about their median. The wikipedia of median ( https://en.wikipedia.org/wiki/Median#The_sample_median ) says sample median asymptotically normal but this does not give a bound for specific . Any suggestion is welcome.","X X_1 , \dots , X_N X \varepsilon (\mu − \varepsilon, \mu + \varepsilon) N = O( \log (\delta^{−1} )\, \sigma^2 / \varepsilon^2 ) \varepsilon 1 −\delta O(log(\delta^{−1})) N = O(  \sigma^2 / (\delta \varepsilon^2) ) N","['probability', 'statistics', 'median']"
94,Probability that in one rolling of 5 dice we obtain two '6' and one '5'?,Probability that in one rolling of 5 dice we obtain two '6' and one '5'?,,First I made a mistake of not taking into account that this event is dependent. So to get two ' $6$ ' the probability would be: $$ P_5(2) = \frac{5!}{2!(5-2)!} \cdot \left(\frac16\right)^2\cdot\left(\frac56\right)^3  $$ Where $p = \dfrac16$ $q = \dfrac56$ $N= 5$ (the number of elements of the system) Then there are $3$ dice left on the table and we want to know the probability that one of them is a ' $5$ '. We rule out the two dice with the ' $6$ ' face on. So the number of element in the system is now $3$ . So the probability would be: $$ P_3(1) = \frac{3!}{1!(3-1)!} \cdot \left(\frac16\right)^1\cdot\left(\frac56\right)^2  $$ But then the professor told me that $p=\dfrac15$ and not $p=\dfrac16$ That's what I don't get. Of course at the end we multiply those two probabilities to get the final probability that we want.,First I made a mistake of not taking into account that this event is dependent. So to get two ' ' the probability would be: Where (the number of elements of the system) Then there are dice left on the table and we want to know the probability that one of them is a ' '. We rule out the two dice with the ' ' face on. So the number of element in the system is now . So the probability would be: But then the professor told me that and not That's what I don't get. Of course at the end we multiply those two probabilities to get the final probability that we want.,6  P_5(2) = \frac{5!}{2!(5-2)!} \cdot \left(\frac16\right)^2\cdot\left(\frac56\right)^3   p = \dfrac16 q = \dfrac56 N= 5 3 5 6 3  P_3(1) = \frac{3!}{1!(3-1)!} \cdot \left(\frac16\right)^1\cdot\left(\frac56\right)^2   p=\dfrac15 p=\dfrac16,"['probability', 'dice']"
95,"Is there any counterexample to show that $X,Y$ are two random variables and $E(X\mid Y)=E(X)$, but $X$ and $Y$ are not independent.","Is there any counterexample to show that  are two random variables and , but  and  are not independent.","X,Y E(X\mid Y)=E(X) X Y","Is there any counterexample to show that $X,Y$ are two random variables and $E(X\mid Y)=E(X)$, but $X$ and $Y$ are not independent. I already know that if $X,Y$ are independent, then $E(X\mid Y)=E(X)$.","Is there any counterexample to show that $X,Y$ are two random variables and $E(X\mid Y)=E(X)$, but $X$ and $Y$ are not independent. I already know that if $X,Y$ are independent, then $E(X\mid Y)=E(X)$.",,"['probability', 'probability-theory', 'expectation', 'conditional-expectation']"
96,Expected maximum pairwise distance for $n$ points on a circle?,Expected maximum pairwise distance for  points on a circle?,n,"Place $n$ points uniformly at random on a circle of circumference $1$. What is the expected maximum distance between any pair $x_i$, $x_j$ of those points? I'm defining distance as distance on the circle, i.e., the length of the smallest path from point $X$ to point $Y$ which does not leave the circle.","Place $n$ points uniformly at random on a circle of circumference $1$. What is the expected maximum distance between any pair $x_i$, $x_j$ of those points? I'm defining distance as distance on the circle, i.e., the length of the smallest path from point $X$ to point $Y$ which does not leave the circle.",,"['probability', 'expected-value', 'order-statistics', 'geometric-probability']"
97,Limit of Probability and Probability of Limit,Limit of Probability and Probability of Limit,,"Let $\{x_k\}$ and $x^*$ be a sequence and a point in $\mathbb{R}^n$, respectively. Can we conclude that $$\lim_{k\to\infty} \mathrm{Prob}(x_k=x^*)=1$$ and $$\mathrm{Prob}(\lim_{k\to\infty} x_k=x^*)=1$$ are equivalent or that one implies the other? I think the first one implies the second, but not vice versa since $x^*$ might not be part of the sequence.","Let $\{x_k\}$ and $x^*$ be a sequence and a point in $\mathbb{R}^n$, respectively. Can we conclude that $$\lim_{k\to\infty} \mathrm{Prob}(x_k=x^*)=1$$ and $$\mathrm{Prob}(\lim_{k\to\infty} x_k=x^*)=1$$ are equivalent or that one implies the other? I think the first one implies the second, but not vice versa since $x^*$ might not be part of the sequence.",,"['real-analysis', 'probability', 'limits']"
98,Probability two withdrawn balls are the same color,Probability two withdrawn balls are the same color,,"Suppose we have $n$ white and $m$ black balls in a urn. First, randomly withdraw two balls, what is the probability (Call it $P_1$ ) that they are the same color? Now, suppose a ball is randomly withdrawn and then replaced before second one is drawn, what is the probability (Call it $P_2$ ) that withdrawn balls are same color? Finally prove that $P_2 > P_1$ . try For the first situation sample space size is ${m + n \choose 2 }$ . Now, in how many ways can we withdraw balls the same color? If both are white, then can do this in ${n \choose 2}$ ways and if both are black can do in ${m \choose 2}$ . Thus $$ P_1 = \frac{ {m \choose 2 } + {n \choose 2} }{ {m+n \choose 2 } } $$ Now, for second situation, two cases. If the first ball drawn is white, then the probability this happens is ${n \choose 1 } / {m+n \choose 1 } = \frac{n}{m+n} $ . For the seecond ball we want it to be white so this can be done in ${n-1 \choose 1 } / {m+n-1 \choose 1 } = \frac{n-1}{m+n-1} $ so for this case we have $\frac{n(n-1) }{(m+n)(m+n-1)}$ . Similarly if the first ball drawn is black we obtain probability $ \frac{m(m-1) }{(m+n)(m+n-1)}$ .Thus, $$ P_2 = \frac{ m(m-1)  + n(n-1) }{(m+n)(m+n-1) } $$ But, Im stuck in trying to prove $P_2 > P_1$ . Is my approach correct?","Suppose we have white and black balls in a urn. First, randomly withdraw two balls, what is the probability (Call it ) that they are the same color? Now, suppose a ball is randomly withdrawn and then replaced before second one is drawn, what is the probability (Call it ) that withdrawn balls are same color? Finally prove that . try For the first situation sample space size is . Now, in how many ways can we withdraw balls the same color? If both are white, then can do this in ways and if both are black can do in . Thus Now, for second situation, two cases. If the first ball drawn is white, then the probability this happens is . For the seecond ball we want it to be white so this can be done in so for this case we have . Similarly if the first ball drawn is black we obtain probability .Thus, But, Im stuck in trying to prove . Is my approach correct?",n m P_1 P_2 P_2 > P_1 {m + n \choose 2 } {n \choose 2} {m \choose 2}  P_1 = \frac{ {m \choose 2 } + {n \choose 2} }{ {m+n \choose 2 } }  {n \choose 1 } / {m+n \choose 1 } = \frac{n}{m+n}  {n-1 \choose 1 } / {m+n-1 \choose 1 } = \frac{n-1}{m+n-1}  \frac{n(n-1) }{(m+n)(m+n-1)}  \frac{m(m-1) }{(m+n)(m+n-1)}  P_2 = \frac{ m(m-1)  + n(n-1) }{(m+n)(m+n-1) }  P_2 > P_1,['probability']
99,Expected amount of time until Fred buys a new computer,Expected amount of time until Fred buys a new computer,,"Fred’s beloved computer will last an $Expo(λ)$ amount of time until it has a malfunction. When that happens, Fred will try to get it fixed. With probability $p$, he will be able to get it fixed. If he is able to get it fixed, the computer is good as new again and will last an additional, independent $Expo(λ)$ amount of time until the next malfunction (when again he is able to get it fixed with probability p, and so on). If after any malfunction Fred is unable to get it fixed, he will buy a new computer. Find the expected amount of time until Fred buys a new computer. (Assume that the time spent on computer   diagnosis, repair, and shopping is negligible.) $T$~$Expo(λ)$; Let $X$ be the time untill he buys a new computer: $E[X]=E[X|I_p=1]p+E[X|I_p=0]q$, where the first term in the right by meaning says that with prob. $p$ computer on average lasted $E[T]$ time untill it get broken +$E[X]$ after being repaired till the moment of being replaced. But this logic is wrong. Can you give me a hint?","Fred’s beloved computer will last an $Expo(λ)$ amount of time until it has a malfunction. When that happens, Fred will try to get it fixed. With probability $p$, he will be able to get it fixed. If he is able to get it fixed, the computer is good as new again and will last an additional, independent $Expo(λ)$ amount of time until the next malfunction (when again he is able to get it fixed with probability p, and so on). If after any malfunction Fred is unable to get it fixed, he will buy a new computer. Find the expected amount of time until Fred buys a new computer. (Assume that the time spent on computer   diagnosis, repair, and shopping is negligible.) $T$~$Expo(λ)$; Let $X$ be the time untill he buys a new computer: $E[X]=E[X|I_p=1]p+E[X|I_p=0]q$, where the first term in the right by meaning says that with prob. $p$ computer on average lasted $E[T]$ time untill it get broken +$E[X]$ after being repaired till the moment of being replaced. But this logic is wrong. Can you give me a hint?",,"['probability', 'conditional-expectation']"
