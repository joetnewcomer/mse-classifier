,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Motivated by Poisson distributions, prove that: If $n p_n \to \lambda$ then $ \left( 1 - p_n \right)^n \to e^{-\lambda}$","Motivated by Poisson distributions, prove that: If  then",n p_n \to \lambda  \left( 1 - p_n \right)^n \to e^{-\lambda},"I have this ugly proof that some sequence converges to something and I really don't like it because it seems too hard for no reason... can anyone help me make this more simple? Here it is : We are given a sequence $p_n$ with $n p_n \to \lambda > 0$. We show that  $$ \left( 1 - p_n \right)^n \to e^{-\lambda}. $$ So my proof begins by noticing that $$ (1-p_n)^n = \left( 1 + \frac{-np_n}n \right)^n  $$ and that the family of functions $s_n(x) = \left(1+\frac xn \right)^n$ is equicontinuous at $x$ (I think I can pull some argument stating that the derivative of $s_n(x)$ is always bounded (because it also converges to $e^x$) when considering an interval of arbitrary but fixed length centered at $x$, so that in this interval, all functions $s_n$ can have the same Lipschitz constant, therefore giving equicontinuity, correct me if I'm wrong) so that if I note by $x_n$ the sequence $-n p_n$, $x = -\lambda$ and $s(x) = e^x$, I wish to show that  $$ \forall \varepsilon > 0, \quad \exists N \quad s.t. \quad \forall n > N, \quad |s_n(x_n) - s(x)| < \varepsilon. $$ so now, $$ |s_n(x_n) - s(x)| \le |s_n(x_n) - s_n(x_m)| + |s_n(x_m) - s_n(x)| + |s_n(x) - s(x)|. $$ Since $x_n$ is convergent, it is also Cauchy, so that for all $\delta > 0$, there exists an $N_1$ such that for all $n,m > N_1$, $|x_m - x_n| < \delta$ and $|x_m - x| < \delta$. Since the family $\{s_n\}$ is equicontinuous, for all $\varepsilon > 0$, there exists a $\delta > 0$ such that $|x - y| < \delta \Rightarrow |s_n(x) - s_n(y)| < \frac{\varepsilon}3$, so that the first two terms are taken care of by $N_1$ by taking $(x,y) = (x_n, x_m)$ and $(x,y) = (x_m, x)$ respectively. The last term is also bounded by $\varepsilon/3$ when $n > N_2$ because $s_n \to s$ pointwisely. Taking $N = \max \{ N_1, N_2 \}$ we're done. I feel like my proof is a little too harsh on this easy-looking problem... anyone has a better proof? What I mean by ""better"" is using less powerful or advanced tools, something simple. I can't seem to find anything better right now.","I have this ugly proof that some sequence converges to something and I really don't like it because it seems too hard for no reason... can anyone help me make this more simple? Here it is : We are given a sequence $p_n$ with $n p_n \to \lambda > 0$. We show that  $$ \left( 1 - p_n \right)^n \to e^{-\lambda}. $$ So my proof begins by noticing that $$ (1-p_n)^n = \left( 1 + \frac{-np_n}n \right)^n  $$ and that the family of functions $s_n(x) = \left(1+\frac xn \right)^n$ is equicontinuous at $x$ (I think I can pull some argument stating that the derivative of $s_n(x)$ is always bounded (because it also converges to $e^x$) when considering an interval of arbitrary but fixed length centered at $x$, so that in this interval, all functions $s_n$ can have the same Lipschitz constant, therefore giving equicontinuity, correct me if I'm wrong) so that if I note by $x_n$ the sequence $-n p_n$, $x = -\lambda$ and $s(x) = e^x$, I wish to show that  $$ \forall \varepsilon > 0, \quad \exists N \quad s.t. \quad \forall n > N, \quad |s_n(x_n) - s(x)| < \varepsilon. $$ so now, $$ |s_n(x_n) - s(x)| \le |s_n(x_n) - s_n(x_m)| + |s_n(x_m) - s_n(x)| + |s_n(x) - s(x)|. $$ Since $x_n$ is convergent, it is also Cauchy, so that for all $\delta > 0$, there exists an $N_1$ such that for all $n,m > N_1$, $|x_m - x_n| < \delta$ and $|x_m - x| < \delta$. Since the family $\{s_n\}$ is equicontinuous, for all $\varepsilon > 0$, there exists a $\delta > 0$ such that $|x - y| < \delta \Rightarrow |s_n(x) - s_n(y)| < \frac{\varepsilon}3$, so that the first two terms are taken care of by $N_1$ by taking $(x,y) = (x_n, x_m)$ and $(x,y) = (x_m, x)$ respectively. The last term is also bounded by $\varepsilon/3$ when $n > N_2$ because $s_n \to s$ pointwisely. Taking $N = \max \{ N_1, N_2 \}$ we're done. I feel like my proof is a little too harsh on this easy-looking problem... anyone has a better proof? What I mean by ""better"" is using less powerful or advanced tools, something simple. I can't seem to find anything better right now.",,['real-analysis']
1,Intuition behind the ILATE rule,Intuition behind the ILATE rule,,"Often I have wondered about this question, but today I had a chance to recollect it and hence I am posting it here. During high-school days one generally learns Integration and I still loving doing problems on Integration. But I have never understood this idea of integration by parts . Why do we adopt the ILATE or LIATE rule to do problems and what was the reason behind thinking of such a rule? Reference: http://en.wikipedia.org/wiki/Integration_by_parts","Often I have wondered about this question, but today I had a chance to recollect it and hence I am posting it here. During high-school days one generally learns Integration and I still loving doing problems on Integration. But I have never understood this idea of integration by parts . Why do we adopt the ILATE or LIATE rule to do problems and what was the reason behind thinking of such a rule? Reference: http://en.wikipedia.org/wiki/Integration_by_parts",,['calculus']
2,How to calculate the value of $\sum\limits_{k=0}^{\infty}\frac{1}{(3k+1)\cdot(3k+2)\cdot(3k+3)}$?,How to calculate the value of ?,\sum\limits_{k=0}^{\infty}\frac{1}{(3k+1)\cdot(3k+2)\cdot(3k+3)},How do I calculate the value of the series $$\sum_{k=0}^{\infty}\frac{1}{(3k+1)\cdot(3k+2)\cdot(3k+3)}= \frac{1}{1\cdot2\cdot3}+\frac{1}{4\cdot5\cdot6}+\frac{1}{7\cdot8\cdot9}+\cdots?$$,How do I calculate the value of the series $$\sum_{k=0}^{\infty}\frac{1}{(3k+1)\cdot(3k+2)\cdot(3k+3)}= \frac{1}{1\cdot2\cdot3}+\frac{1}{4\cdot5\cdot6}+\frac{1}{7\cdot8\cdot9}+\cdots?$$,,"['real-analysis', 'sequences-and-series', 'definite-integrals', 'summation', 'fractions']"
3,Is $\sigma$-finiteness unnecessary for Radon Nikodym theorem?,Is -finiteness unnecessary for Radon Nikodym theorem?,\sigma,"Let $(X,\mathfrak{M},\mu)$ be a $\sigma$-finite measure space and $\lambda:\mathfrak{M}\rightarrow [0,\infty]$ be a measure. If $\lambda\ll \mu$, then there exists a measurable $f:(X,\mathfrak{M})\rightarrow [0,\infty)$ such that $d\lambda= fd\mu$. The above is the Radon Nikodym theorem stated in wikipedia. However, both texts Rudin and Folland prove the Radon Nikodym theorem under assumption that $\lambda$ is $\sigma$-finite. How do I prove the Wikipedia vesion of Radon Nikodym therem? Is there any reference?","Let $(X,\mathfrak{M},\mu)$ be a $\sigma$-finite measure space and $\lambda:\mathfrak{M}\rightarrow [0,\infty]$ be a measure. If $\lambda\ll \mu$, then there exists a measurable $f:(X,\mathfrak{M})\rightarrow [0,\infty)$ such that $d\lambda= fd\mu$. The above is the Radon Nikodym theorem stated in wikipedia. However, both texts Rudin and Folland prove the Radon Nikodym theorem under assumption that $\lambda$ is $\sigma$-finite. How do I prove the Wikipedia vesion of Radon Nikodym therem? Is there any reference?",,"['real-analysis', 'measure-theory']"
4,"Prove $d(x,y)=\arctan|x-y|$ is a metric",Prove  is a metric,"d(x,y)=\arctan|x-y|","I have to show that $d$ is a metric on the real numbers, and the first three axioms are straight forward, the triangle inequality poses a problem.  I know we need to get  $$ \begin{align*} d(x,y) &= \arctan|x-y| \\ &\le \arctan|x-z|+\arctan|z-y| \\ &= d(x,z)+d(z,y), \end{align*} $$ so what I've tried is  $$\arctan|x-y| = \arctan|x-z+z-y| \le \arctan(|x-z|+|z-y|),$$ but I'm not even sure if this accomplishes anything because I don't know how to split it up.","I have to show that $d$ is a metric on the real numbers, and the first three axioms are straight forward, the triangle inequality poses a problem.  I know we need to get  $$ \begin{align*} d(x,y) &= \arctan|x-y| \\ &\le \arctan|x-z|+\arctan|z-y| \\ &= d(x,z)+d(z,y), \end{align*} $$ so what I've tried is  $$\arctan|x-y| = \arctan|x-z+z-y| \le \arctan(|x-z|+|z-y|),$$ but I'm not even sure if this accomplishes anything because I don't know how to split it up.",,['real-analysis']
5,"On the evaluation of the integral $\int_{0}^{1}\sqrt{x^2+1}\,dx$",On the evaluation of the integral,"\int_{0}^{1}\sqrt{x^2+1}\,dx","The integral $\displaystyle \int_{0}^{1}\sqrt{x^2+1}\,dx$ can be evaluated with the standard technic of sub $u=\tan \theta$. However, in the book says evaluate the integral without trigonometric substitution. I can't find a way. I applied the sub $u=1-x$ and it got a little messie. I also tried to approach it geometrically, but again I had a problem because the graph is not a common shape thus saying that the integral is equal to the area of that shape. For example if I had the integral $\displaystyle \int_0^1 \sqrt{1-x^2}\,dx$ then this equal to aread of the quadrateral etc. In the case of the other integral I don't see something like this. If someone could give me a hint,that would be nice!!","The integral $\displaystyle \int_{0}^{1}\sqrt{x^2+1}\,dx$ can be evaluated with the standard technic of sub $u=\tan \theta$. However, in the book says evaluate the integral without trigonometric substitution. I can't find a way. I applied the sub $u=1-x$ and it got a little messie. I also tried to approach it geometrically, but again I had a problem because the graph is not a common shape thus saying that the integral is equal to the area of that shape. For example if I had the integral $\displaystyle \int_0^1 \sqrt{1-x^2}\,dx$ then this equal to aread of the quadrateral etc. In the case of the other integral I don't see something like this. If someone could give me a hint,that would be nice!!",,"['real-analysis', 'integration', 'definite-integrals']"
6,"limit laws:$\lim_{n\to\infty}\max(a_n,b_n)=\max(\lim_{n\to\infty}a_n,\lim_{n\to\infty}b_n)$",limit laws:,"\lim_{n\to\infty}\max(a_n,b_n)=\max(\lim_{n\to\infty}a_n,\lim_{n\to\infty}b_n)","Let $(a_n)^{\infty}_{n=m}$ and $(b_n)^{\infty}_{n=m}$ be convergent sequences of real numbers. Let $x$ and $y$ be the real numbers $x:=\lim\limits_{n\to\infty}a_n$ and $y:=\lim\limits_{n\to\infty}b_n$ . Show that the sequence $(\max(a_n,b_n))$ converges to $\max(x,y)$ ; in other words: $$\lim_{n\to\infty}\max(a_n,b_n)=\max\bigl(\lim_{n\to\infty}a_n,\lim_{n\to\infty}b_n\bigr)$$ I was not able to prove it and would appreciate your help.",Let and be convergent sequences of real numbers. Let and be the real numbers and . Show that the sequence converges to ; in other words: I was not able to prove it and would appreciate your help.,"(a_n)^{\infty}_{n=m} (b_n)^{\infty}_{n=m} x y x:=\lim\limits_{n\to\infty}a_n y:=\lim\limits_{n\to\infty}b_n (\max(a_n,b_n)) \max(x,y) \lim_{n\to\infty}\max(a_n,b_n)=\max\bigl(\lim_{n\to\infty}a_n,\lim_{n\to\infty}b_n\bigr)","['real-analysis', 'sequences-and-series', 'analysis', 'limits']"
7,"Showing that $f(x)=e^{-x}$ is uniformly continuous on $[0,\infty)$",Showing that  is uniformly continuous on,"f(x)=e^{-x} [0,\infty)","I am trying to show that $f(x)=e^{-x}$ is uniformly continuous on $[0,\infty)$ and not having much success.  I'm attempting to use a modified version of the following result (which I found on Math Stack Exchange here: https://math.stackexchange.com/questions/172988/please-prove-uniform-continuity ), that if $f$ is continuous on $[a,\infty)$ and the limit of $f(x)=L$ as $x$ approaches infinity, then $f$ is uniformly continuous on $[a,\infty)$. This is the work I've done so far.  I really don't know if it's valid reasoning... The first problem I'm encountering is that the book has not yet defined functional limits, so I can't really use the proof directly.  We have, however defined sequential limits and the notion of a decreasing function.  If I define a sequence $x_n=n-1$ for $n$$\in$$\mathbb{N}$, then as $n$ goes to infinity, $f(x_n)\rightarrow0$.  For $\epsilon/2$, $\exists$N$\in$$\mathbb{N}$ s.th for all n $\geq$ N,  |$f(x_n)|$ <$\epsilon/2$. Then, since $f(x)$ is a decreasing function, this inequality holds for all $x$$\geq$$x_N$.  Since $\mathcal{S}$=$[0,N+1]$ is closed and bounded, and hence a compact set in $\mathbb{R}$, and $f(x)=e^{-x}$ is continuous on $\mathcal{S}$, then by the Uniform Continuity Theorem, $f$ is uniformly continuous on $\mathcal{S}$. So, if $x,y$ $\in$$\mathcal{S}$,$\forall$$\epsilon>0,$ $\exists$ $\delta>0$ s.th |$x-y$|<$\delta$ then |$f(x)-f(y)$|<$\epsilon$.  Then (really not sure about this part), if $x, y$ $\in$$(N+1,\infty)$, for any $x$ and $y$ in this set, we have $|f(x)-f(y)|=|f(x)+(-f(y))|$$\leq|f(x)|+|f(y)|<\epsilon/2+\epsilon/2=\epsilon$.  So $f$ is uniformly continuous on $(N+1,\infty)$.  Provided this part is correct, I am having trouble with the case where $x$$\in[0,N+1]$ and $y$ $\in(N+1,\infty)$.  Provided it's not correct, I'm having trouble on that part as well.  Provided none of this is correct, well... I don't know. Any help would be appreciated.  Is this an appropriate way to approach the problem?  Are there other ways?","I am trying to show that $f(x)=e^{-x}$ is uniformly continuous on $[0,\infty)$ and not having much success.  I'm attempting to use a modified version of the following result (which I found on Math Stack Exchange here: https://math.stackexchange.com/questions/172988/please-prove-uniform-continuity ), that if $f$ is continuous on $[a,\infty)$ and the limit of $f(x)=L$ as $x$ approaches infinity, then $f$ is uniformly continuous on $[a,\infty)$. This is the work I've done so far.  I really don't know if it's valid reasoning... The first problem I'm encountering is that the book has not yet defined functional limits, so I can't really use the proof directly.  We have, however defined sequential limits and the notion of a decreasing function.  If I define a sequence $x_n=n-1$ for $n$$\in$$\mathbb{N}$, then as $n$ goes to infinity, $f(x_n)\rightarrow0$.  For $\epsilon/2$, $\exists$N$\in$$\mathbb{N}$ s.th for all n $\geq$ N,  |$f(x_n)|$ <$\epsilon/2$. Then, since $f(x)$ is a decreasing function, this inequality holds for all $x$$\geq$$x_N$.  Since $\mathcal{S}$=$[0,N+1]$ is closed and bounded, and hence a compact set in $\mathbb{R}$, and $f(x)=e^{-x}$ is continuous on $\mathcal{S}$, then by the Uniform Continuity Theorem, $f$ is uniformly continuous on $\mathcal{S}$. So, if $x,y$ $\in$$\mathcal{S}$,$\forall$$\epsilon>0,$ $\exists$ $\delta>0$ s.th |$x-y$|<$\delta$ then |$f(x)-f(y)$|<$\epsilon$.  Then (really not sure about this part), if $x, y$ $\in$$(N+1,\infty)$, for any $x$ and $y$ in this set, we have $|f(x)-f(y)|=|f(x)+(-f(y))|$$\leq|f(x)|+|f(y)|<\epsilon/2+\epsilon/2=\epsilon$.  So $f$ is uniformly continuous on $(N+1,\infty)$.  Provided this part is correct, I am having trouble with the case where $x$$\in[0,N+1]$ and $y$ $\in(N+1,\infty)$.  Provided it's not correct, I'm having trouble on that part as well.  Provided none of this is correct, well... I don't know. Any help would be appreciated.  Is this an appropriate way to approach the problem?  Are there other ways?",,['real-analysis']
8,Showing that $\int_{0}^{\infty} \frac{dx}{1 + x^2} = 2 \int_0^1 \frac{dx}{1 + x^2}$,Showing that,\int_{0}^{\infty} \frac{dx}{1 + x^2} = 2 \int_0^1 \frac{dx}{1 + x^2},"I was reading an article in which it was stated that, with a change of variable, one could show that: $$\int_{0}^{\infty} \frac{dx}{1 + x^2} = 2 \int_0^1 \frac{dx}{1 + x^2}$$ I tried with $t = 1 + \frac{1}{x}$ but that doesn't work out, especially because the lower bound doesn't become $0$.","I was reading an article in which it was stated that, with a change of variable, one could show that: $$\int_{0}^{\infty} \frac{dx}{1 + x^2} = 2 \int_0^1 \frac{dx}{1 + x^2}$$ I tried with $t = 1 + \frac{1}{x}$ but that doesn't work out, especially because the lower bound doesn't become $0$.",,"['real-analysis', 'integration', 'definite-integrals']"
9,"Is any differentiable function $f : (0,1)\rightarrow [0,1]$ is uniformly continuous",Is any differentiable function  is uniformly continuous,"f : (0,1)\rightarrow [0,1]","Question is to check if  : any differentiable function $f : (0,1)\rightarrow [0,1]$ is uniformly continuous. I know that any continuous function on compact subset of $\mathbb{R}$ is uniformly continuous. As $(0,1)$ is not compact, we can not say anything at this time. Now, as it is given that $f$ is differentiable, if its derivative $f'$is bounded then  $f$ is uniformly continuous. So, I am trying to look for differentiable functions $f$ on $(0,1)$ such that $f'$ is unbounded. i am not very familiar with large number of differentiable functions with unbounded derivatives. I know $f(x)=\sqrt{x}$ has unbounded derivative, but $\sqrt{x}$ is uniformly continuous.... So, I would like someone to help me out with some hint. P.S : I have just now saw one example $$f(x)=x^2\sin{\frac{1}{x^2}}$$ which  is differentiable but is not bounded. I see that $\sin(\frac{1}{x^2})$ is bounded by $1$ and if $x\in (0,1)$ then so is $x^2$ and so is $x^2\sin{\frac{1}{x^2}}$ So, $f(x)=x^2\sin{\frac{1}{x^2}}$ is from $(0,1)$ to $[0,1]$ whose derivative is unbounded. Now, the problem reduces to show that $f(x)$ is not uniformly continuous... :(","Question is to check if  : any differentiable function $f : (0,1)\rightarrow [0,1]$ is uniformly continuous. I know that any continuous function on compact subset of $\mathbb{R}$ is uniformly continuous. As $(0,1)$ is not compact, we can not say anything at this time. Now, as it is given that $f$ is differentiable, if its derivative $f'$is bounded then  $f$ is uniformly continuous. So, I am trying to look for differentiable functions $f$ on $(0,1)$ such that $f'$ is unbounded. i am not very familiar with large number of differentiable functions with unbounded derivatives. I know $f(x)=\sqrt{x}$ has unbounded derivative, but $\sqrt{x}$ is uniformly continuous.... So, I would like someone to help me out with some hint. P.S : I have just now saw one example $$f(x)=x^2\sin{\frac{1}{x^2}}$$ which  is differentiable but is not bounded. I see that $\sin(\frac{1}{x^2})$ is bounded by $1$ and if $x\in (0,1)$ then so is $x^2$ and so is $x^2\sin{\frac{1}{x^2}}$ So, $f(x)=x^2\sin{\frac{1}{x^2}}$ is from $(0,1)$ to $[0,1]$ whose derivative is unbounded. Now, the problem reduces to show that $f(x)$ is not uniformly continuous... :(",,['real-analysis']
10,"A sequence of functions $\{f_n(x)\}_{n=1}^{\infty} \subseteq C[0,1]$ that is pointwise bounded but not uniformly bounded.",A sequence of functions  that is pointwise bounded but not uniformly bounded.,"\{f_n(x)\}_{n=1}^{\infty} \subseteq C[0,1]","We were talking about pointwise bounded vs. uniformly bounded in my analysis class, and this question came up.  The problem is that we are working on a compact set, it would be much easier if the interval was $(0, 1]$.  My idea was to create a sequence of functions such that $f_n(\frac{1}{n}) = n$ and $f_n(0) = 0$, $f_n(1) = 0$ and then connect the ""spike"" with line segments to the endpoints.  Visually, the $f_n's$ would look like mountains.  After working out the slopes, I came up with these formulae: $$f_n(x) = \left\{         \begin{array}{ll}             n^2x & \quad 0 \leq x \leq \frac{1}{n} \\             \frac{-n^2}{n-1}(x-1) & \quad \frac{1}{n} < x \leq 1         \end{array}     \right.$$ This gives the picture I was visualizing in my head (unless my arithmetic is incorrect), but unfortunately, it does not work, since it is not pointwise bounded.  My idea was that at $x = \frac{1}{m}$ $f_n(x) \leq m$ for all $n$.  But this is not the case, for example, $f_{10}(\frac{1}{2}) = \frac{200}{9} \geq 5$. So the question is: can you give me a sequence of functions $\{f_n(x)\}_{n=1}^{\infty} \subseteq C[0,1]$ that is pointwise bounded but not uniformly bounded?  And if so, is there anyway to save my construction?  There must be a ""canonical"" example, because otherwise the uniform boundedness in the conclusion of the Arzela-Ascoli Theorem would not really be relevant. I did search for answers to this question, and did not find any.  I found these: Equicontinuity implies (pointwise bounded iff uniformly bounded) Why doesn't pointwise bounded imply uniform bounded? Does this problem make sense? ""Give an example of a set $F\subset C([0,1])$ which is pointwise bounded but not bounded"" The last one is obviously the same question I am asking, but it has no answer, and I could not think of anything based off the hint. Thanks!","We were talking about pointwise bounded vs. uniformly bounded in my analysis class, and this question came up.  The problem is that we are working on a compact set, it would be much easier if the interval was $(0, 1]$.  My idea was to create a sequence of functions such that $f_n(\frac{1}{n}) = n$ and $f_n(0) = 0$, $f_n(1) = 0$ and then connect the ""spike"" with line segments to the endpoints.  Visually, the $f_n's$ would look like mountains.  After working out the slopes, I came up with these formulae: $$f_n(x) = \left\{         \begin{array}{ll}             n^2x & \quad 0 \leq x \leq \frac{1}{n} \\             \frac{-n^2}{n-1}(x-1) & \quad \frac{1}{n} < x \leq 1         \end{array}     \right.$$ This gives the picture I was visualizing in my head (unless my arithmetic is incorrect), but unfortunately, it does not work, since it is not pointwise bounded.  My idea was that at $x = \frac{1}{m}$ $f_n(x) \leq m$ for all $n$.  But this is not the case, for example, $f_{10}(\frac{1}{2}) = \frac{200}{9} \geq 5$. So the question is: can you give me a sequence of functions $\{f_n(x)\}_{n=1}^{\infty} \subseteq C[0,1]$ that is pointwise bounded but not uniformly bounded?  And if so, is there anyway to save my construction?  There must be a ""canonical"" example, because otherwise the uniform boundedness in the conclusion of the Arzela-Ascoli Theorem would not really be relevant. I did search for answers to this question, and did not find any.  I found these: Equicontinuity implies (pointwise bounded iff uniformly bounded) Why doesn't pointwise bounded imply uniform bounded? Does this problem make sense? ""Give an example of a set $F\subset C([0,1])$ which is pointwise bounded but not bounded"" The last one is obviously the same question I am asking, but it has no answer, and I could not think of anything based off the hint. Thanks!",,"['real-analysis', 'sequences-and-series', 'analysis']"
11,"Does there exist a function $f:[0,1] \to[0,1]$ such its graph is dense in $[0,1]\times[0,1]$?",Does there exist a function  such its graph is dense in ?,"f:[0,1] \to[0,1] [0,1]\times[0,1]","Does there exist a function $f:[0,1]\to [0,1]$ such that the graph of $f$ is dense in $[0,1]\times[0,1]$? Not necessarily continuous.","Does there exist a function $f:[0,1]\to [0,1]$ such that the graph of $f$ is dense in $[0,1]\times[0,1]$? Not necessarily continuous.",,"['real-analysis', 'functions']"
12,Monotonic function only has jump discontinuities,Monotonic function only has jump discontinuities,,"I'm trying to show that a monotone function on a closed interval can only contain jump discontinuities. Could someone give me a hint as to how I should begin? I am not sure how to start this problem. Edit : Let $f$ be a increasing function. Then if $x \leq y, f(x) \leq f(y)$.","I'm trying to show that a monotone function on a closed interval can only contain jump discontinuities. Could someone give me a hint as to how I should begin? I am not sure how to start this problem. Edit : Let $f$ be a increasing function. Then if $x \leq y, f(x) \leq f(y)$.",,['real-analysis']
13,Uncountable dense subset whose complement is also uncountable and dense [duplicate],Uncountable dense subset whose complement is also uncountable and dense [duplicate],,This question already has answers here : Closed 12 years ago . Possible Duplicate: Locally non-enumerable dense subsets of R This may be a standard result... I don't know... anyway... Does there exists an uncountable dense subset of $\mathbb{R}$ whose complement is also uncountable and dense?,This question already has answers here : Closed 12 years ago . Possible Duplicate: Locally non-enumerable dense subsets of R This may be a standard result... I don't know... anyway... Does there exists an uncountable dense subset of $\mathbb{R}$ whose complement is also uncountable and dense?,,"['real-analysis', 'general-topology']"
14,Find $\lim\limits_{x \to 0} \frac{ \sqrt{1+x} - 1} { \sqrt[3]{1+x} - 1}$.,Find .,\lim\limits_{x \to 0} \frac{ \sqrt{1+x} - 1} { \sqrt[3]{1+x} - 1},"I'm having trouble finding $\lim\limits_{x \to 0} \frac{ \sqrt{1+x} - 1} { \sqrt[3]{1+x} - 1}$ . Here's my attempt: $$ \lim_{x \to 0} \frac{\sqrt{1+x} - 1}{\sqrt[3]{1+x} - 1}  = \lim_{x \to 0} \frac{\sqrt{1+x} - 1}{\sqrt[3]{1+x} - 1}  \cdot \frac{\sqrt{1+x} + 1}{\sqrt{1+x} + 1}  = \lim_{x \to 0} \frac{x}{(\sqrt[3]{1+x} - 1)  \cdot (\sqrt{1+x} + 1)} $$ I'm having trouble getting rid of the given expression in the denominator. My professor mentioned multiplying with $$ \frac{\sqrt[3]{(1+x)^2} + \sqrt[3]{1+x} + 1} {\sqrt[3]{(1+x)^2} + \sqrt[3]{1+x} + 1} $$ because of the factorization of $a^n-b^n$ (???) but even when I do that I don't get the correct result, which is $\frac{3}{2}$ . Any alternative ways to solve this would be appreciated, if someone could explain the professors logic it would also be of great help. Thanks.","I'm having trouble finding . Here's my attempt: I'm having trouble getting rid of the given expression in the denominator. My professor mentioned multiplying with because of the factorization of (???) but even when I do that I don't get the correct result, which is . Any alternative ways to solve this would be appreciated, if someone could explain the professors logic it would also be of great help. Thanks.","\lim\limits_{x \to 0} \frac{ \sqrt{1+x} - 1} { \sqrt[3]{1+x} - 1} 
\lim_{x \to 0} \frac{\sqrt{1+x} - 1}{\sqrt[3]{1+x} - 1} 
= \lim_{x \to 0} \frac{\sqrt{1+x} - 1}{\sqrt[3]{1+x} - 1} 
\cdot \frac{\sqrt{1+x} + 1}{\sqrt{1+x} + 1} 
= \lim_{x \to 0} \frac{x}{(\sqrt[3]{1+x} - 1) 
\cdot (\sqrt{1+x} + 1)}
 
\frac{\sqrt[3]{(1+x)^2} + \sqrt[3]{1+x} + 1}
{\sqrt[3]{(1+x)^2} + \sqrt[3]{1+x} + 1}
 a^n-b^n \frac{3}{2}","['real-analysis', 'calculus', 'limits', 'radicals', 'limits-without-lhopital']"
15,Show that $\int_0^\pi \frac{x\cos x \sinh x}{\sin^2x+\sinh^2x}dx=\frac{\pi}{2}\ln\left(\frac{e^\pi+1}{e^\pi-1}\right)$,Show that,\int_0^\pi \frac{x\cos x \sinh x}{\sin^2x+\sinh^2x}dx=\frac{\pi}{2}\ln\left(\frac{e^\pi+1}{e^\pi-1}\right),"I am trying to show that $$I=\int_0^\pi \frac{x\cos x \sinh x}{\sin^2x+\sinh^2x}dx=\frac{\pi}{2}\ln\left(\frac{e^\pi+1}{e^\pi-1}\right)$$ I know there is an antiderivative in terms of dilogarithms and logarithms, but this nice closed form makes me think there is a clever way to get this result for these specific bounds of integration. So please avoid posting proofs with the antiderivative. What I managed to do for now is to notice that $$I=-\Im\int_0^\pi\frac{x}{\sin(x(1+i))}dx$$ which you get by simple computation. From here how to proceed? I'm not sure if the substitution $t=x(1+i)$ is allowed here, since I think this would become a problem of contour integration, which I usually avoid. Here is my question: Is there a way to obtain this result without using the antiderivative and contour integration?","I am trying to show that I know there is an antiderivative in terms of dilogarithms and logarithms, but this nice closed form makes me think there is a clever way to get this result for these specific bounds of integration. So please avoid posting proofs with the antiderivative. What I managed to do for now is to notice that which you get by simple computation. From here how to proceed? I'm not sure if the substitution is allowed here, since I think this would become a problem of contour integration, which I usually avoid. Here is my question: Is there a way to obtain this result without using the antiderivative and contour integration?",I=\int_0^\pi \frac{x\cos x \sinh x}{\sin^2x+\sinh^2x}dx=\frac{\pi}{2}\ln\left(\frac{e^\pi+1}{e^\pi-1}\right) I=-\Im\int_0^\pi\frac{x}{\sin(x(1+i))}dx t=x(1+i),"['real-analysis', 'integration', 'complex-analysis', 'definite-integrals', 'contour-integration']"
16,"How do I evaluate $\lim_{n\to\infty} \,\sum_{k=1}^n\left(\frac{k}{n^2}\right)^{\frac{k}{n^2}+1}$? [closed]",How do I evaluate ? [closed],"\lim_{n\to\infty} \,\sum_{k=1}^n\left(\frac{k}{n^2}\right)^{\frac{k}{n^2}+1}","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I came across the following problem recently in a problem sheet aimed at high school students: Evaluate $$\lim_{n\to\infty} \,\sum_{k=1}^n\left(\frac{k}{n^2}\right)^{\frac{k}{n^2}+1}.$$ I tried to rewrite the inner sum as a Riemann sum hoping that the limit would become a definite integral, but no gain because of the extra $1/n$ 's.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I came across the following problem recently in a problem sheet aimed at high school students: Evaluate I tried to rewrite the inner sum as a Riemann sum hoping that the limit would become a definite integral, but no gain because of the extra 's.","\lim_{n\to\infty} \,\sum_{k=1}^n\left(\frac{k}{n^2}\right)^{\frac{k}{n^2}+1}. 1/n","['real-analysis', 'calculus', 'limits']"
17,"Pointwise convergence of uniformly continuous functions to zero, but not uniformly","Pointwise convergence of uniformly continuous functions to zero, but not uniformly",,"What would be an example of a sequence of uniformly continuous functions on a compact domain which converges pointwise to $0$ , but not uniformly?","What would be an example of a sequence of uniformly continuous functions on a compact domain which converges pointwise to , but not uniformly?",0,"['real-analysis', 'examples-counterexamples', 'uniform-convergence', 'uniform-continuity', 'pointwise-convergence']"
18,Why is $x^{-1} = \frac{1}{x}$?,Why is ?,x^{-1} = \frac{1}{x},"I've always taken for granted that $x^{-1} = \frac{1}{x}$ because it works and everything, but how can I see that this must be true? I am not looking for answers like ""If you have $x^n / x^m$ then this is $x^{n-m}$ and so $x^{0}/x^1 = 1/x = x^{0-1} = x^{-1}$ because then I am stuck wondering whether it probably makes sense that $x^n$ can be defined when $n$ is negative (I hesitate to just assume that because we can plug numbers into something that it will be valid).","I've always taken for granted that because it works and everything, but how can I see that this must be true? I am not looking for answers like ""If you have then this is and so because then I am stuck wondering whether it probably makes sense that can be defined when is negative (I hesitate to just assume that because we can plug numbers into something that it will be valid).",x^{-1} = \frac{1}{x} x^n / x^m x^{n-m} x^{0}/x^1 = 1/x = x^{0-1} = x^{-1} x^n n,"['real-analysis', 'algebra-precalculus', 'number-theory', 'inverse']"
19,"Ratio test for sequences, the other direction","Ratio test for sequences, the other direction",,"Suppose I have a real sequence $x_n\to 0$. Is it true that: $$ \left|\frac{x_{n+1}}{x_n}\right|\to r<1 $$ for some $r\in\mathbb{R}$? If not, is it true that: $$\exists N\in\mathbb{N}:\left|\frac{x_{n+1}}{x_n}\right|<1,\forall n>N$$ Thank you for your help and understanding! :D","Suppose I have a real sequence $x_n\to 0$. Is it true that: $$ \left|\frac{x_{n+1}}{x_n}\right|\to r<1 $$ for some $r\in\mathbb{R}$? If not, is it true that: $$\exists N\in\mathbb{N}:\left|\frac{x_{n+1}}{x_n}\right|<1,\forall n>N$$ Thank you for your help and understanding! :D",,"['real-analysis', 'sequences-and-series', 'examples-counterexamples']"
20,Is it possible to find an uncountable number of disjoint open intervals in $R$?,Is it possible to find an uncountable number of disjoint open intervals in ?,R,"Is it possible to find an uncountable number of disjoint open intervals in $R$? Several times I saw the sentence every open set in $\mathbb{R}$ can be expressed as a countable number of open intervals (Because $\mathbb{R}$ is second countable) Suppose we are able to find an uncountable number of disjoint open intervals in $\mathbb{R}$, then union of these intervals is an open set (say $G$) in $\mathbb{R}$. But $G$ cannot be expressed as a countable number of open intervals. Thus my answer is there is no such a collection exist. Is my think is correct? Give more hints and clarify it..!! Thanks in advance.","Is it possible to find an uncountable number of disjoint open intervals in $R$? Several times I saw the sentence every open set in $\mathbb{R}$ can be expressed as a countable number of open intervals (Because $\mathbb{R}$ is second countable) Suppose we are able to find an uncountable number of disjoint open intervals in $\mathbb{R}$, then union of these intervals is an open set (say $G$) in $\mathbb{R}$. But $G$ cannot be expressed as a countable number of open intervals. Thus my answer is there is no such a collection exist. Is my think is correct? Give more hints and clarify it..!! Thanks in advance.",,['real-analysis']
21,A serie about $\sum_{n = 1}^\infty {\arctan \frac{{10n}}{{\left( {3{n^2} + 2} \right)\left( {9{n^2} - 1} \right)}}}$,A serie about,\sum_{n = 1}^\infty {\arctan \frac{{10n}}{{\left( {3{n^2} + 2} \right)\left( {9{n^2} - 1} \right)}}},How to prove $$\sum\limits_{n = 1}^\infty  {\arctan \frac{{10n}}{{\left( {3{n^2} + 2} \right)\left( {9{n^2} - 1} \right)}}}  = \ln 3 - \frac{\pi }{4}.$$ Add: Maybe we can follow this !,How to prove $$\sum\limits_{n = 1}^\infty  {\arctan \frac{{10n}}{{\left( {3{n^2} + 2} \right)\left( {9{n^2} - 1} \right)}}}  = \ln 3 - \frac{\pi }{4}.$$ Add: Maybe we can follow this !,,"['calculus', 'real-analysis', 'sequences-and-series']"
22,Soft question: Union of infinitely many closed sets,Soft question: Union of infinitely many closed sets,,"this is a question that is not addressed in my book directly but I was curious. We just proved that the union of a finite collection of closed sets is also closed, but I was curious about if the union of infinitely many closed sets can be open. This question may not be at the level of the book so perhaps that's why it wasn't addressed. Just to make things easier, lets imagine sets that are disks in the x-y plane. I can imagine that if there are nested disks inside each other, that in this case the union would clearly be closed. But what if you could construct an infinite set of disks that together cover the entire real plane. Then in this case, it seems that every point in their union would have an open ball centered around the point that is also contained in the real plane, so that this union of an infinite collection of disks would create an open set. Is this a correct way of thinking? Or at least on the right track? I get the feeling that as long as you have no largest individual set that contains all the others, then you won't get a closed set. But I have a feeling there is more subtlety to it. Thanks everyone","this is a question that is not addressed in my book directly but I was curious. We just proved that the union of a finite collection of closed sets is also closed, but I was curious about if the union of infinitely many closed sets can be open. This question may not be at the level of the book so perhaps that's why it wasn't addressed. Just to make things easier, lets imagine sets that are disks in the x-y plane. I can imagine that if there are nested disks inside each other, that in this case the union would clearly be closed. But what if you could construct an infinite set of disks that together cover the entire real plane. Then in this case, it seems that every point in their union would have an open ball centered around the point that is also contained in the real plane, so that this union of an infinite collection of disks would create an open set. Is this a correct way of thinking? Or at least on the right track? I get the feeling that as long as you have no largest individual set that contains all the others, then you won't get a closed set. But I have a feeling there is more subtlety to it. Thanks everyone",,['real-analysis']
23,If $f$ is continuous & $\lim_{|x|\to {\infty}}f(x)=0$ then $f$ is uniformly continuous or NOT?,If  is continuous &  then  is uniformly continuous or NOT?,f \lim_{|x|\to {\infty}}f(x)=0 f,"Let, $f:\mathbb R\to \mathbb R$ be a continuous function such that $\lim_{|x|\to {\infty}}f(x)=0.$ Then prove or disprove that $f$ is uniformly continuous. I tried through the formal definition of uniform continuity but I could not proceed further. Is it directly follow from definition or any other property about continuity to prove this? Please help....","Let, $f:\mathbb R\to \mathbb R$ be a continuous function such that $\lim_{|x|\to {\infty}}f(x)=0.$ Then prove or disprove that $f$ is uniformly continuous. I tried through the formal definition of uniform continuity but I could not proceed further. Is it directly follow from definition or any other property about continuity to prove this? Please help....",,"['real-analysis', 'limits', 'metric-spaces', 'continuity', 'uniform-continuity']"
24,Can a sequence which decays more slowly still yield a converging series?,Can a sequence which decays more slowly still yield a converging series?,,"In Bergman's companion notes to Rudin, he says that ""If a sequence of positive terms has convergent sum, so does every sequence of positive terms which decays more rapidly."" So given a sequence $\{a_n\}$ of positive terms such that $\sum_n a_n$ converges, if $\{b_n\}$ is such that $$ \lim_{n\to\infty} \frac{a_n}{b_n} = +\infty, $$ then $\sum_nb_n$ converges. I can prove this given $\{a_n\}$ and $\{b_n\}$. However, can we find $b_n$ which decays more slowly, i.e. $$ \lim_{n\to\infty} \frac{b_n}{a_n} = +\infty $$ such that $\sum_n b_n$ converges? Similarly, we have the claim ""if a sequence of positive terms has divergent sum, then so does every sequence of positive terms which decays more slowly.""","In Bergman's companion notes to Rudin, he says that ""If a sequence of positive terms has convergent sum, so does every sequence of positive terms which decays more rapidly."" So given a sequence $\{a_n\}$ of positive terms such that $\sum_n a_n$ converges, if $\{b_n\}$ is such that $$ \lim_{n\to\infty} \frac{a_n}{b_n} = +\infty, $$ then $\sum_nb_n$ converges. I can prove this given $\{a_n\}$ and $\{b_n\}$. However, can we find $b_n$ which decays more slowly, i.e. $$ \lim_{n\to\infty} \frac{b_n}{a_n} = +\infty $$ such that $\sum_n b_n$ converges? Similarly, we have the claim ""if a sequence of positive terms has divergent sum, then so does every sequence of positive terms which decays more slowly.""",,['real-analysis']
25,The product of two divergent series is divergent?,The product of two divergent series is divergent?,,"This is an TRUE/FALSE queston: The product of two divergent series is divergent. The correct answer is FALSE. I know that the product of two convergent series may not be convergent (i.e. $\frac{(-1)^n}{\sqrt{n}}$) according to Cauchy Product. My question is why ""The product of two divergent series may not be divergent""?? Is there any counter example? Thanks!","This is an TRUE/FALSE queston: The product of two divergent series is divergent. The correct answer is FALSE. I know that the product of two convergent series may not be convergent (i.e. $\frac{(-1)^n}{\sqrt{n}}$) according to Cauchy Product. My question is why ""The product of two divergent series may not be divergent""?? Is there any counter example? Thanks!",,"['calculus', 'real-analysis', 'sequences-and-series']"
26,Nested sequence of closed sets,Nested sequence of closed sets,,Is it true that every nested sequence of non-empty closed sets $(I_n)$ (one such that $I_{n+1} \subset I_n$) has a non-empty intersection?,Is it true that every nested sequence of non-empty closed sets $(I_n)$ (one such that $I_{n+1} \subset I_n$) has a non-empty intersection?,,"['real-analysis', 'general-topology']"
27,How can I show that a sequence of regular polygons with $n$ sides becomes more and more like a circle as $n \to \infty$?,How can I show that a sequence of regular polygons with  sides becomes more and more like a circle as ?,n n \to \infty,"If we construct regular polygons with larger and larger numbers of sides, they will look more and more like circles. That is intuitively true. I hope you will help me to express and prove it mathematically.","If we construct regular polygons with larger and larger numbers of sides, they will look more and more like circles. That is intuitively true. I hope you will help me to express and prove it mathematically.",,"['real-analysis', 'geometry']"
28,Prove $\sum_{m=1}^{\infty}\sum_{n=1}^{\infty} \frac{1}{n(m^2+n^2)}=\sum_{m=1}^{\infty}\sum_{n=1}^{\infty} \frac{1}{n^2(m^2+n^2)}=\frac{\pi^4}{72}$,Prove,\sum_{m=1}^{\infty}\sum_{n=1}^{\infty} \frac{1}{n(m^2+n^2)}=\sum_{m=1}^{\infty}\sum_{n=1}^{\infty} \frac{1}{n^2(m^2+n^2)}=\frac{\pi^4}{72},"How may I prove that $$\sum_{m=1}^{\infty}\sum_{n=1}^{\infty} \frac{1}{n(m^2+n^2)}=\sum_{m=1}^{\infty}\sum_{n=1}^{\infty} \frac{1}{n^2(m^2+n^2)}=\frac{\pi^4}{72}?$$ I also discussed the problem in the chat, but no solution so far.  Some hints? Thanks!","How may I prove that $$\sum_{m=1}^{\infty}\sum_{n=1}^{\infty} \frac{1}{n(m^2+n^2)}=\sum_{m=1}^{\infty}\sum_{n=1}^{\infty} \frac{1}{n^2(m^2+n^2)}=\frac{\pi^4}{72}?$$ I also discussed the problem in the chat, but no solution so far.  Some hints? Thanks!",,"['calculus', 'real-analysis', 'sequences-and-series']"
29,Give an example of a function $f: \mathbb{R} \to \mathbb{R}$ which is continuous only at $0$.,Give an example of a function  which is continuous only at .,f: \mathbb{R} \to \mathbb{R} 0,I do not know an example. Will ask question if in doubt of the proofs provided thank you!!,I do not know an example. Will ask question if in doubt of the proofs provided thank you!!,,"['real-analysis', 'functions']"
30,"Show that $A=\{ \frac{m}{2^n}:m\in \mathbb {Z},n\in \mathbb {N} \} $ is dense in $\mathbb {R}$ [duplicate]",Show that  is dense in  [duplicate],"A=\{ \frac{m}{2^n}:m\in \mathbb {Z},n\in \mathbb {N} \}  \mathbb {R}","This question already has answers here : Closed 11 years ago . Possible Duplicate: Density of a Set on $\mathbb{R}$? I have to show that show that $A=\{ \frac{m}{2^n}:m\in \mathbb {Z},n\in \mathbb {N}\} $ is dense in $\mathbb {R}$. A set A is dense in $\mathbb {R}$ if $\overline A=\mathbb {R}$. But also $Y$ is a subset of $X$, we say that $Y$ is dense in $X$, if for every $x\in X$ , there is $y \in Y$  that is arbitary close to $x$. So ,I have to prove that for every $x \in \mathbb {R}$ ,there is a number $\frac{m}{2^n}$ arbitrarily close to $x$.So $\forall \epsilon,x ,\exists y$ such that $|y-x|<\epsilon$. I got a little stuck at this point...Could anyone give me a hint?Thanks a lot!","This question already has answers here : Closed 11 years ago . Possible Duplicate: Density of a Set on $\mathbb{R}$? I have to show that show that $A=\{ \frac{m}{2^n}:m\in \mathbb {Z},n\in \mathbb {N}\} $ is dense in $\mathbb {R}$. A set A is dense in $\mathbb {R}$ if $\overline A=\mathbb {R}$. But also $Y$ is a subset of $X$, we say that $Y$ is dense in $X$, if for every $x\in X$ , there is $y \in Y$  that is arbitary close to $x$. So ,I have to prove that for every $x \in \mathbb {R}$ ,there is a number $\frac{m}{2^n}$ arbitrarily close to $x$.So $\forall \epsilon,x ,\exists y$ such that $|y-x|<\epsilon$. I got a little stuck at this point...Could anyone give me a hint?Thanks a lot!",,['real-analysis']
31,Two questions about the Cantor set construction,Two questions about the Cantor set construction,,"Let $C_0$ be the segment $[0,1]$ $C_2$ will be $[0,1]$, with the middle third, an open set removed, so $[0,1/3]\cup[2/3,1]$ First, if we removed closed sets would the cantor set, the limit of what remains from this process, be the same? I was able to show that the limit of the sequence of partial sums of the pieces converges to $1$. And this happens if the removed bits are open or closed, it doesn't matter. Don't know if that helps or not. Second, I've been trying for two hours and I still can't find a sensible way to write $C_n$, in some compact notation.","Let $C_0$ be the segment $[0,1]$ $C_2$ will be $[0,1]$, with the middle third, an open set removed, so $[0,1/3]\cup[2/3,1]$ First, if we removed closed sets would the cantor set, the limit of what remains from this process, be the same? I was able to show that the limit of the sequence of partial sums of the pieces converges to $1$. And this happens if the removed bits are open or closed, it doesn't matter. Don't know if that helps or not. Second, I've been trying for two hours and I still can't find a sensible way to write $C_n$, in some compact notation.",,"['real-analysis', 'general-topology', 'descriptive-set-theory']"
32,"If $\int_a^bg(x)dx=0$, show that $\int_a^bf(x)g(x)dx=0.$","If , show that",\int_a^bg(x)dx=0 \int_a^bf(x)g(x)dx=0.,"Let $g(x)\ge0$ . If $\int_a^bg(x)dx=0$ , show that $\int_a^bf(x)g(x)dx=0,$ where $f$ is any integrable function. If simeone is allowed to use the Mean Value thorem for integrals, the proof is at hand. But for that $f$ must be continuous! Any suggestion?","Let . If , show that where is any integrable function. If simeone is allowed to use the Mean Value thorem for integrals, the proof is at hand. But for that must be continuous! Any suggestion?","g(x)\ge0 \int_a^bg(x)dx=0 \int_a^bf(x)g(x)dx=0, f f","['real-analysis', 'calculus', 'integration', 'riemann-integration']"
33,Does $\frac{n}{\sum\limits_{k=1}^{n}\Big(\frac{k}{k+1}\Big)^k}$ converge?,Does  converge?,\frac{n}{\sum\limits_{k=1}^{n}\Big(\frac{k}{k+1}\Big)^k},"Does the sequence $$\displaystyle \frac{n}{\sum\limits_{k=1}^{n}\Big(\frac{k}{k+1}\Big)^k}$$   converge? Attempt. Since $\Big(\frac{k}{k+1}\Big)^k \rightarrow 1/e\neq 0$ and the terms are positive, the series $\sum\limits_{k=1}^{\infty}\Big(\frac{k}{k+1}\Big)^k$ diverges to $+\infty$. I find hard to determine if $n$ or the sum goes faster to $+\infty.$ Thanks in advance.","Does the sequence $$\displaystyle \frac{n}{\sum\limits_{k=1}^{n}\Big(\frac{k}{k+1}\Big)^k}$$   converge? Attempt. Since $\Big(\frac{k}{k+1}\Big)^k \rightarrow 1/e\neq 0$ and the terms are positive, the series $\sum\limits_{k=1}^{\infty}\Big(\frac{k}{k+1}\Big)^k$ diverges to $+\infty$. I find hard to determine if $n$ or the sum goes faster to $+\infty.$ Thanks in advance.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'summation']"
34,"Is it true that if $(a_{n+1}-a_n)\to 0$ and $(a_n)$ is bounded, then $(a_n)$ is convergent","Is it true that if  and  is bounded, then  is convergent",(a_{n+1}-a_n)\to 0 (a_n) (a_n),"I want to prove or disprove the following Let $(a_n)$ be a bounded sequence such that   $$\lim_{n\to\infty}(a_{n+1}-a_n)=0$$   Then $(a_n)$ is convergent. My idea to prove that the proposition above is true is as follows: since $(a_n)$ is bounded, it has limit points. Then, by assuming that the sequence is not convergent, there are two different limit points $\alpha$ and $\beta$ and subsequences of $(a_n)$: $(a_{k_n})$ and $(a_{j_n})$ such that $a_{k_n}\to \alpha$ and $a_{j_n}\to\beta$. Now, I tried to build a subsequence $(a_{p_n})$ of $(a_n)$ with the property $p_{2n+1}=p_{2n}+1$ such that $(a_{p_{2n+1}})$ is a subsequence of $(a_{k_n})$ and $(a_{p_{2n}})$ is a subsequence of $(a_{j_n})$ thus there will exist a subsequence of the sequence $(a_{n+1}-a_{n})$ convergent to $\alpha-\beta$, which would be a contradiction. My problem was that I could not prove that you can always build the subsequece $(a_{p_n})$.","I want to prove or disprove the following Let $(a_n)$ be a bounded sequence such that   $$\lim_{n\to\infty}(a_{n+1}-a_n)=0$$   Then $(a_n)$ is convergent. My idea to prove that the proposition above is true is as follows: since $(a_n)$ is bounded, it has limit points. Then, by assuming that the sequence is not convergent, there are two different limit points $\alpha$ and $\beta$ and subsequences of $(a_n)$: $(a_{k_n})$ and $(a_{j_n})$ such that $a_{k_n}\to \alpha$ and $a_{j_n}\to\beta$. Now, I tried to build a subsequence $(a_{p_n})$ of $(a_n)$ with the property $p_{2n+1}=p_{2n}+1$ such that $(a_{p_{2n+1}})$ is a subsequence of $(a_{k_n})$ and $(a_{p_{2n}})$ is a subsequence of $(a_{j_n})$ thus there will exist a subsequence of the sequence $(a_{n+1}-a_{n})$ convergent to $\alpha-\beta$, which would be a contradiction. My problem was that I could not prove that you can always build the subsequece $(a_{p_n})$.",,"['real-analysis', 'sequences-and-series']"
35,$f'(x) = [f(x)]^{2}$. Prove $f(x) = 0 $,. Prove,f'(x) = [f(x)]^{2} f(x) = 0 ,"Let $f:\mathbb{R} \rightarrow \mathbb{R}$ be a differentiable function such that $f(0) = 0$ and $f'(x) = [f(x)]^{2}$, $\forall x \in \mathbb{R}$. Show that $f(x) = 0$,  $\forall x \in \mathbb{R} $. I first (unsuccessfully) tried using the Mean Value Theorem, but this is in the Integrals chapter so the solution probably involves them. Can't really see where integrals come in here though. What I've got so far: (i) Since $f$ is differentiable, thus it is continuous and, hence,  integrable. Therefore $f^2$ is also integrable and as $f'=f^2$, $f'$ is too. (ii) $f' \geq 0$, $\forall x \in \mathbb{R}$","Let $f:\mathbb{R} \rightarrow \mathbb{R}$ be a differentiable function such that $f(0) = 0$ and $f'(x) = [f(x)]^{2}$, $\forall x \in \mathbb{R}$. Show that $f(x) = 0$,  $\forall x \in \mathbb{R} $. I first (unsuccessfully) tried using the Mean Value Theorem, but this is in the Integrals chapter so the solution probably involves them. Can't really see where integrals come in here though. What I've got so far: (i) Since $f$ is differentiable, thus it is continuous and, hence,  integrable. Therefore $f^2$ is also integrable and as $f'=f^2$, $f'$ is too. (ii) $f' \geq 0$, $\forall x \in \mathbb{R}$",,"['real-analysis', 'ordinary-differential-equations']"
36,Measurability of a set in the definition of almost sure convergence,Measurability of a set in the definition of almost sure convergence,,"Many books define almost sure convergence as follows: The sequence of random variables ${(X_n)}_{n \in \mathbb{N}}$ defined on the probability space $(\Omega, \mathcal{F}, P)$ converges almost surely to a random variable $X$ defined on the same probability space, if    $$ P(\{ \omega \in \Omega: \lim_{n \rightarrow \infty} X_n(\omega) = X(\omega)\}) = 1. $$ In connection to this question , I wonder if the set $A := \{ \omega \in \Omega: \lim_{n \rightarrow \infty} X_n(\omega) = X(\omega)\}$ is implicitly assumed to be measurable or whether it is actually a priori measurable. If the latter is true, how can one show this?","Many books define almost sure convergence as follows: The sequence of random variables ${(X_n)}_{n \in \mathbb{N}}$ defined on the probability space $(\Omega, \mathcal{F}, P)$ converges almost surely to a random variable $X$ defined on the same probability space, if    $$ P(\{ \omega \in \Omega: \lim_{n \rightarrow \infty} X_n(\omega) = X(\omega)\}) = 1. $$ In connection to this question , I wonder if the set $A := \{ \omega \in \Omega: \lim_{n \rightarrow \infty} X_n(\omega) = X(\omega)\}$ is implicitly assumed to be measurable or whether it is actually a priori measurable. If the latter is true, how can one show this?",,"['real-analysis', 'probability-theory', 'measure-theory', 'convergence-divergence']"
37,Dominated Convergence Theorem,Dominated Convergence Theorem,,"Give an example of a sequence $\{f_n\}_{n=1}^\infty$ of integrable functions on $\mathbb{R}$ such that $f_n \to f$ but $\int f_n \not\to \int f$ . Explain why your example does not conflict with the Dominated Convergence Theorem. I do notice that the inequality $|f_n(x)| \le g(x)$ , where $g$ is an integrable function over $\mathbb{R}$ , is not listed here in this problem. So the function need not be dominated by an integrable function. But this is required as one hypothesis of the Dominated Convergence Theorem; hence the example will not conflict. If this is sound reasoning, how may I come up with functions that are not dominated by another function? Initially I was thinking $f_n(x)=x \sin (nx)$ because its $\lim \sup$ is $\infty$ , but even then, we still have $|f_n(x)| \le |x| =: g(x)$ .","Give an example of a sequence of integrable functions on such that but . Explain why your example does not conflict with the Dominated Convergence Theorem. I do notice that the inequality , where is an integrable function over , is not listed here in this problem. So the function need not be dominated by an integrable function. But this is required as one hypothesis of the Dominated Convergence Theorem; hence the example will not conflict. If this is sound reasoning, how may I come up with functions that are not dominated by another function? Initially I was thinking because its is , but even then, we still have .",\{f_n\}_{n=1}^\infty \mathbb{R} f_n \to f \int f_n \not\to \int f |f_n(x)| \le g(x) g \mathbb{R} f_n(x)=x \sin (nx) \lim \sup \infty |f_n(x)| \le |x| =: g(x),"['real-analysis', 'lebesgue-integral']"
38,How to prove $\sum_{m=1}^\infty \sum_{n=1}^\infty \frac{1}{m^2+n^2}=+\infty$,How to prove,\sum_{m=1}^\infty \sum_{n=1}^\infty \frac{1}{m^2+n^2}=+\infty,How to prove $$\sum_{m=1}^\infty \sum_{n=1}^\infty \frac{1}{m^2+n^2}=+\infty.$$ I try to do like $$\sum_{m=1}^\infty \sum_{n=1}^\infty \frac{1}{m^2+n^2}=\sum_{N=1}^\infty \sum_{n+m=N}^\infty  \frac{1}{m^2+n^2}=\sum_{N=1}^\infty \sum_{m=1}^{N-1}  \frac{1}{m^2+(N-m)^2}$$  $$\frac{1}{m^2+(N-m)^2}\leq \frac{2}{N^2}$$ but it doesn't work.,How to prove $$\sum_{m=1}^\infty \sum_{n=1}^\infty \frac{1}{m^2+n^2}=+\infty.$$ I try to do like $$\sum_{m=1}^\infty \sum_{n=1}^\infty \frac{1}{m^2+n^2}=\sum_{N=1}^\infty \sum_{n+m=N}^\infty  \frac{1}{m^2+n^2}=\sum_{N=1}^\infty \sum_{m=1}^{N-1}  \frac{1}{m^2+(N-m)^2}$$  $$\frac{1}{m^2+(N-m)^2}\leq \frac{2}{N^2}$$ but it doesn't work.,,"['real-analysis', 'sequences-and-series']"
39,"Prove that every function that verifies $|f(x)-f(y)|\leq(x-y)^2$ for all $x,y$ is constant.",Prove that every function that verifies  for all  is constant.,"|f(x)-f(y)|\leq(x-y)^2 x,y","Let $f\colon \mathbb{R} \rightarrow \mathbb{R}$ be a function. In order to prove that every function that verifies $\lvert f(x)-f(y)\rvert \leq(x-y)^2$ for all $x,y$, is constant, I have considered on applying the Lagrange's theorem. There exists some $c\in[x,y]$ for which: $$\frac{\lvert f(x)-f(y)\rvert}{x-y}=f'(c)\leq (x-y)$$ Which would certainly lead to success if I'm able to prove that that is $0$, the problem is I don't know how to proceed with the proof. Any help appreciated.","Let $f\colon \mathbb{R} \rightarrow \mathbb{R}$ be a function. In order to prove that every function that verifies $\lvert f(x)-f(y)\rvert \leq(x-y)^2$ for all $x,y$, is constant, I have considered on applying the Lagrange's theorem. There exists some $c\in[x,y]$ for which: $$\frac{\lvert f(x)-f(y)\rvert}{x-y}=f'(c)\leq (x-y)$$ Which would certainly lead to success if I'm able to prove that that is $0$, the problem is I don't know how to proceed with the proof. Any help appreciated.",,"['real-analysis', 'inequality']"
40,Evaluating $\lim\limits_{x\rightarrow0}\frac{e^{-1/x^2}}{x}$,Evaluating,\lim\limits_{x\rightarrow0}\frac{e^{-1/x^2}}{x},"Today in my analysis class, we were preparing for the final and this question came up: Evaluate $$\lim_{x\to0}\frac{e^{-1/x^2}}{x}$$ We tried taking the $\log$, using L'Hopitals and some other tricks but couldn't figure it out. I thought maybe viewing this limits as a sequence in the following way might help; $$\lim_{x\rightarrow0}\frac{e^{-1/x^2}}{x} = \lim_{n\rightarrow \infty}\frac{e^{-n^2}}{1/n} = \lim_{n\rightarrow \infty}\frac{n}{e^{n^2}}$$ But from them I'm not sure. Thank you.","Today in my analysis class, we were preparing for the final and this question came up: Evaluate $$\lim_{x\to0}\frac{e^{-1/x^2}}{x}$$ We tried taking the $\log$, using L'Hopitals and some other tricks but couldn't figure it out. I thought maybe viewing this limits as a sequence in the following way might help; $$\lim_{x\rightarrow0}\frac{e^{-1/x^2}}{x} = \lim_{n\rightarrow \infty}\frac{e^{-n^2}}{1/n} = \lim_{n\rightarrow \infty}\frac{n}{e^{n^2}}$$ But from them I'm not sure. Thank you.",,"['real-analysis', 'limits']"
41,"The set of limit points of the sequence $1,\frac12,\frac14,\frac34,\frac18,\frac38,\frac58,\frac78,\frac1{16},\frac3{16},\ldots$",The set of limit points of the sequence,"1,\frac12,\frac14,\frac34,\frac18,\frac38,\frac58,\frac78,\frac1{16},\frac3{16},\ldots","I came across the following problem that says: The set of limit points of the sequence $1,\dfrac12,\dfrac14,\dfrac34,\dfrac18,\dfrac38,\dfrac58,\dfrac78,\dfrac1{16},\dfrac3{16},\dfrac5{16},\dfrac7{16},\dfrac9{16},\ldots$ is which of the following? (a) $[0,1]$ (b) $(0,1]$ (c) the set of all rational numbers in $[0,1]$ (d) the set of all rational numbers in $[0,1]$ and of the form $m/2^n$ where $m$ and $n$ are integers. Please help. Thanks in advance for your time.","I came across the following problem that says: The set of limit points of the sequence $1,\dfrac12,\dfrac14,\dfrac34,\dfrac18,\dfrac38,\dfrac58,\dfrac78,\dfrac1{16},\dfrac3{16},\dfrac5{16},\dfrac7{16},\dfrac9{16},\ldots$ is which of the following? (a) $[0,1]$ (b) $(0,1]$ (c) the set of all rational numbers in $[0,1]$ (d) the set of all rational numbers in $[0,1]$ and of the form $m/2^n$ where $m$ and $n$ are integers. Please help. Thanks in advance for your time.",,"['real-analysis', 'sequences-and-series']"
42,Show that $f$ has at most one fixed point,Show that  has at most one fixed point,f,"Let $f\colon\mathbb{R}\to\mathbb{R}$ be a differentiable function. $x\in\mathbb{R}$ is a fixed point of $f$ if $f(x)=x$. Show that if $f'(t)\neq 1\;\forall\;t\in\mathbb{R}$, then $f$ has at most one fixed point. My biggest problem  with this is that it doesn't seem to be true. For example, consider $f(x)=x^2$. Then certainly $f(0)=0$ and $f(1)=1 \Rightarrow 0$ and $1$ are fixed points. But $f'(x)=2x\neq 1 \;\forall\;x\in\mathbb{R}$. Is there some sort of formulation that makes this statement correct? Am I missing something obvious? This is a problem from an old exam, so I'm assuming that maybe there's some sort of typo or missing condition.","Let $f\colon\mathbb{R}\to\mathbb{R}$ be a differentiable function. $x\in\mathbb{R}$ is a fixed point of $f$ if $f(x)=x$. Show that if $f'(t)\neq 1\;\forall\;t\in\mathbb{R}$, then $f$ has at most one fixed point. My biggest problem  with this is that it doesn't seem to be true. For example, consider $f(x)=x^2$. Then certainly $f(0)=0$ and $f(1)=1 \Rightarrow 0$ and $1$ are fixed points. But $f'(x)=2x\neq 1 \;\forall\;x\in\mathbb{R}$. Is there some sort of formulation that makes this statement correct? Am I missing something obvious? This is a problem from an old exam, so I'm assuming that maybe there's some sort of typo or missing condition.",,['real-analysis']
43,Compute $\sum \frac{1}{k^2}$ using Euler-Maclaurin formula,Compute  using Euler-Maclaurin formula,\sum \frac{1}{k^2},"I read that Euler used the summation formula to calculate the value of the series $\sum_{k =1}^{\infty} \frac{1}{k^2}$ to high precision without too much hassle. The article Dances between continuous and discrete: Euler’s summation formula goes into the calculation, however without too much justification of why it works (especially since the series used to calculate the limit does not converge and one has to truncate it at a certain point). I would be glad if someone could elaborate from a more modern viewpoint on how and why it works.","I read that Euler used the summation formula to calculate the value of the series $\sum_{k =1}^{\infty} \frac{1}{k^2}$ to high precision without too much hassle. The article Dances between continuous and discrete: Euler’s summation formula goes into the calculation, however without too much justification of why it works (especially since the series used to calculate the limit does not converge and one has to truncate it at a certain point). I would be glad if someone could elaborate from a more modern viewpoint on how and why it works.",,"['real-analysis', 'sequences-and-series', 'euler-maclaurin']"
44,Prove that $\exists a<b$ s.t. $f(a)=f(b)=0$ when $\int_0^1f(x)dx=\int_0^1xf(x)dx=0$,Prove that  s.t.  when,\exists a<b f(a)=f(b)=0 \int_0^1f(x)dx=\int_0^1xf(x)dx=0,"The exact question is: $f:[0,1]\rightarrow\mathbb{R}$ is a continuous function such that   $\int_0^1f(x)=\int_0^1xf(x)=0$. Prove that $\exists a,b \in [0,1], a<b$    such that $f(a)=f(b)=0$. By using mean value theorem of integration, we get some $a,b$ s.t $f(a)=f(b)=0$, but how can we show that $a\neq b$.","The exact question is: $f:[0,1]\rightarrow\mathbb{R}$ is a continuous function such that   $\int_0^1f(x)=\int_0^1xf(x)=0$. Prove that $\exists a,b \in [0,1], a<b$    such that $f(a)=f(b)=0$. By using mean value theorem of integration, we get some $a,b$ s.t $f(a)=f(b)=0$, but how can we show that $a\neq b$.",,"['real-analysis', 'integration']"
45,Is every continuum-sized dense subset of the irrationals order isomorphic to the irrationals?,Is every continuum-sized dense subset of the irrationals order isomorphic to the irrationals?,,"This is a strengthening of a question another user asked, here: Are irrational numbers order-isomorphic to real transcendental numbers? . In the answer to that question, it was stated that the irrationals are order-isomorphic to the transcendental reals. My question is this. Suppose $S$ is a continuum-sized subset of the set of irrational numbers, which has the property that it is everywhere dense, meaning, between any two distinct reals, there exists a real number belonging to $S$ . Must $S$ be order-isomorphic to the irrationals?","This is a strengthening of a question another user asked, here: Are irrational numbers order-isomorphic to real transcendental numbers? . In the answer to that question, it was stated that the irrationals are order-isomorphic to the transcendental reals. My question is this. Suppose is a continuum-sized subset of the set of irrational numbers, which has the property that it is everywhere dense, meaning, between any two distinct reals, there exists a real number belonging to . Must be order-isomorphic to the irrationals?",S S S,"['real-analysis', 'order-theory']"
46,Prove $\sqrt{x_1^2+4x_1x_2+5x_2^2+2x_1+6x_2+5}$ is convex,Prove  is convex,\sqrt{x_1^2+4x_1x_2+5x_2^2+2x_1+6x_2+5},I came across a question which contained this $\sqrt{x_1^2+4x_1x_2+5x_2^2+2x_1+6x_2+5}$ I needed to prove $x_1^2+4x_1x_2+5x_2^2+2x_1+6x_2+5\geq0$ which I've done by just showing that the minimum is 3 and it's convex function. The second part asked to show that the whole problem is convex but I didn't know how to prove $\sqrt{x_1^2+4x_1x_2+5x_2^2+2x_1+6x_2+5}$ is convex which is same as asking $\sqrt{x^TAx+b^Tx+c}$ is convex where $x^TAx+b^Tx+c\geq0$ . any help?,I came across a question which contained this I needed to prove which I've done by just showing that the minimum is 3 and it's convex function. The second part asked to show that the whole problem is convex but I didn't know how to prove is convex which is same as asking is convex where . any help?,\sqrt{x_1^2+4x_1x_2+5x_2^2+2x_1+6x_2+5} x_1^2+4x_1x_2+5x_2^2+2x_1+6x_2+5\geq0 \sqrt{x_1^2+4x_1x_2+5x_2^2+2x_1+6x_2+5} \sqrt{x^TAx+b^Tx+c} x^TAx+b^Tx+c\geq0,"['real-analysis', 'convex-analysis', 'convex-optimization']"
47,Are some irrational numbers closer to rational numbers than others?,Are some irrational numbers closer to rational numbers than others?,,"I'm not even sure this question is meaningful because any irrational number can be approximated by a rational as close as you want.  But given that there are infinitely many irrationals between any two rationals, surely some must be closer to a rational number than others? Is there a metric by which irrationals can be ranked, or can we compare two irrationals, based on how far they are from a rational? I'm having trouble wrapping my head around this, can anyone assist?","I'm not even sure this question is meaningful because any irrational number can be approximated by a rational as close as you want.  But given that there are infinitely many irrationals between any two rationals, surely some must be closer to a rational number than others? Is there a metric by which irrationals can be ranked, or can we compare two irrationals, based on how far they are from a rational? I'm having trouble wrapping my head around this, can anyone assist?",,"['real-analysis', 'irrational-numbers']"
48,How can we identify a set is infinite when proving Bolzano–Weierstrass theorem?,How can we identify a set is infinite when proving Bolzano–Weierstrass theorem?,,"I'm proving the Bolzano–Weierstrass theorem . It says $$ \text{If } A \text{ : bounded and infinite set}, \text{ then } A \text{ has at least one limit point.} $$ So the proof goes like the following. Since $A \subset \mathbb{R}^N$ is bounded, it can be a subset of a box $B_1  = I_1 \times \cdots \times I_N$, where $I_i$ is an interval in $\mathbb{R}$. Let's divide the $B_1$ into $2^N$ sub-boxes. Then, at least one sub-box has the infinite elements of $A$. Let's say that sub-box as $B_2$ . Let's say we can repeat this procedure for $B_3, B_4, \ldots$ Well, so proof ends as we prove some $x$ exists in $\cap_{n=1}^{\infty}B_n$ and it is a limit point of $A$. But what I'm wondering is, how can we identify the $B_2$? (and $B_3$, and so on) Yes, it's clear to me that such set exists, but the existence itself does not guarantee us to identify the set (that is, to select and label it as $B_2$), since we don't have any tools to discern if $B_2$ is infinite or not. It sounds a little bit philosophical, but is there anyone to help?","I'm proving the Bolzano–Weierstrass theorem . It says $$ \text{If } A \text{ : bounded and infinite set}, \text{ then } A \text{ has at least one limit point.} $$ So the proof goes like the following. Since $A \subset \mathbb{R}^N$ is bounded, it can be a subset of a box $B_1  = I_1 \times \cdots \times I_N$, where $I_i$ is an interval in $\mathbb{R}$. Let's divide the $B_1$ into $2^N$ sub-boxes. Then, at least one sub-box has the infinite elements of $A$. Let's say that sub-box as $B_2$ . Let's say we can repeat this procedure for $B_3, B_4, \ldots$ Well, so proof ends as we prove some $x$ exists in $\cap_{n=1}^{\infty}B_n$ and it is a limit point of $A$. But what I'm wondering is, how can we identify the $B_2$? (and $B_3$, and so on) Yes, it's clear to me that such set exists, but the existence itself does not guarantee us to identify the set (that is, to select and label it as $B_2$), since we don't have any tools to discern if $B_2$ is infinite or not. It sounds a little bit philosophical, but is there anyone to help?",,"['real-analysis', 'infinity']"
49,Experimental identities with Fibonacci series,Experimental identities with Fibonacci series,,I obtained the following Fibonacci identities experimentally using SageMath code. Can these be proved theoretically? Identity 1: $$ \sum_{n = 0}^{\infty} \frac{F_{n}x^{n}}{n!} + e^x\sum_{n = 0}^{\infty} \frac{F_{n}(-x)^{n}}{n!} = 0 $$ Identity 2: $$ 52\sum_{n = 0}^{\infty} \frac{F_{4n}x^{4n}}{4n!} = 156 \sum_{n = 0}^{\infty} \frac{F_{4n+1}x^{4n+1}}{(4n+1)!} = 585 \sum_{n = 0}^{\infty} \frac{F_{4n+2}x^{4n+2}}{(4n+2)!} = 630 \sum_{n = 0}^{\infty} \frac{F_{4n+3}x^{4n+3}}{(4n+3)!} $$,I obtained the following Fibonacci identities experimentally using SageMath code. Can these be proved theoretically? Identity 1: $$ \sum_{n = 0}^{\infty} \frac{F_{n}x^{n}}{n!} + e^x\sum_{n = 0}^{\infty} \frac{F_{n}(-x)^{n}}{n!} = 0 $$ Identity 2: $$ 52\sum_{n = 0}^{\infty} \frac{F_{4n}x^{4n}}{4n!} = 156 \sum_{n = 0}^{\infty} \frac{F_{4n+1}x^{4n+1}}{(4n+1)!} = 585 \sum_{n = 0}^{\infty} \frac{F_{4n+2}x^{4n+2}}{(4n+2)!} = 630 \sum_{n = 0}^{\infty} \frac{F_{4n+3}x^{4n+3}}{(4n+3)!} $$,,"['real-analysis', 'sequences-and-series', 'number-theory', 'summation', 'fibonacci-numbers']"
50,Find all continuous functions such that $f(x)f(2x)\dots f(nx) \le an^k$,Find all continuous functions such that,f(x)f(2x)\dots f(nx) \le an^k,"Find all continuous functions $f : \mathbb{R} \rightarrow [1,\infty)$ for which there exist $a \in \mathbb{R}$ and $k$ a positive integer such that  $$f(x)f(2x)\dots f(nx) ≤ an^k,$$ for every real number $x$ and positive integer $n$. EDIT: A trivial solution I could find was the constant function equal to one.","Find all continuous functions $f : \mathbb{R} \rightarrow [1,\infty)$ for which there exist $a \in \mathbb{R}$ and $k$ a positive integer such that  $$f(x)f(2x)\dots f(nx) ≤ an^k,$$ for every real number $x$ and positive integer $n$. EDIT: A trivial solution I could find was the constant function equal to one.",,['real-analysis']
51,The dual of $L^p$ is $L^q$,The dual of  is,L^p L^q,"Let $1 \leq p < \infty$ and $1/p + 1/q = 1$. Then if $l$ is a bounded linear functional on $L^p(E, d\mu)$ where $\mu$ is a $\sigma$-finite measure, $l(f) = \int_Efgd\mu$ for some $g \in L^q(E,d\mu)$ with $\|g\|_q = \|l\|_{op}$. The statement follows from the Radon-Nikodym Theorem, which requires $\sigma$-finiteness. Is it true that if we let $p>1$ we can get rid of $\sigma$-finiteness in the proof? Is there a reference for this? What is the idea of the proof?","Let $1 \leq p < \infty$ and $1/p + 1/q = 1$. Then if $l$ is a bounded linear functional on $L^p(E, d\mu)$ where $\mu$ is a $\sigma$-finite measure, $l(f) = \int_Efgd\mu$ for some $g \in L^q(E,d\mu)$ with $\|g\|_q = \|l\|_{op}$. The statement follows from the Radon-Nikodym Theorem, which requires $\sigma$-finiteness. Is it true that if we let $p>1$ we can get rid of $\sigma$-finiteness in the proof? Is there a reference for this? What is the idea of the proof?",,"['real-analysis', 'functional-analysis', 'measure-theory', 'lp-spaces', 'dual-spaces']"
52,Find the derivative of $\sqrt[n]{x}$ using the formal definition of a derivative,Find the derivative of  using the formal definition of a derivative,\sqrt[n]{x},"Given $\sqrt[n]{x}$, prove using the formal definition of a derivative that : $$\frac{d}{dx} (\sqrt[n]{x}) = \frac{x^{\frac{1-n}{n}}}{n}$$ Now this would be ridiculously easy to show using the Power Rule, but alas, that is not the goal of this question. Using the formal definition of a limit we get : \begin{equation}  \begin{split} f'(x) & = \lim_{h \ \to \  0} \frac{f(x+h)-f(x)}{h}  \\  & = \lim_{h \ \to \  0} \frac{\sqrt[n]{x+h}-\sqrt[n]{x}}{h} \\  & = \lim_{h \ \to \  0} \frac{(x+h)^{\frac{1}{n}}-(x)^{\frac{1}{n}}}{h} \end{split} \end{equation} But it is unclear to me how to proceed next, essentially all we need to do to get this limit into a determinate form (it currently is in an indeterminate form) is to factor out a $h$ in the numerator, but there doesn't seem to be an obvious way to do so. What algebraic technique , would you use to factor out a $h$ in the numerator in this case? For $n=2$, you could easily multiply the fraction by the conjugate to get the limit into a determinate form, and for $n=3$, you could do the same with the help of a few identities, but how would you go about this for the general case, as stated in the example I've given above. This question is the general $n^{th}$ case of finding the derivative using the formal definition, for functions such as $f(x) = \sqrt{x}$, $f(x) = \sqrt[3]{x}$ and so forth, and is aimed at finding the best algebraic technique to manipulate the limit to get it into a determinate form.","Given $\sqrt[n]{x}$, prove using the formal definition of a derivative that : $$\frac{d}{dx} (\sqrt[n]{x}) = \frac{x^{\frac{1-n}{n}}}{n}$$ Now this would be ridiculously easy to show using the Power Rule, but alas, that is not the goal of this question. Using the formal definition of a limit we get : \begin{equation}  \begin{split} f'(x) & = \lim_{h \ \to \  0} \frac{f(x+h)-f(x)}{h}  \\  & = \lim_{h \ \to \  0} \frac{\sqrt[n]{x+h}-\sqrt[n]{x}}{h} \\  & = \lim_{h \ \to \  0} \frac{(x+h)^{\frac{1}{n}}-(x)^{\frac{1}{n}}}{h} \end{split} \end{equation} But it is unclear to me how to proceed next, essentially all we need to do to get this limit into a determinate form (it currently is in an indeterminate form) is to factor out a $h$ in the numerator, but there doesn't seem to be an obvious way to do so. What algebraic technique , would you use to factor out a $h$ in the numerator in this case? For $n=2$, you could easily multiply the fraction by the conjugate to get the limit into a determinate form, and for $n=3$, you could do the same with the help of a few identities, but how would you go about this for the general case, as stated in the example I've given above. This question is the general $n^{th}$ case of finding the derivative using the formal definition, for functions such as $f(x) = \sqrt{x}$, $f(x) = \sqrt[3]{x}$ and so forth, and is aimed at finding the best algebraic technique to manipulate the limit to get it into a determinate form.",,"['calculus', 'real-analysis', 'limits', 'derivatives']"
53,Does $ \sum_{n\geq 2} \frac{\ln(1+n)}{\ln(n)}-1$ converge/diverge?,Does  converge/diverge?, \sum_{n\geq 2} \frac{\ln(1+n)}{\ln(n)}-1,How would you prove convergence/divergence of the following series? $$ \sum_{n\geq 2}\left( \dfrac{\ln(1+n)}{\ln(n)}-1\right)$$ I'm interested in more ways of proving convergence/divergence for this series. My thoughts $$\dfrac{\ln(1+n)}{\ln(n)}=\frac{\ln(n(1+\dfrac{1}{n})}{\ln(n)} =\frac{\ln(n)+\ln(1+\frac{1}{n})}{\ln(n)} =1+\frac{\ln(1+\frac{1}{n})}{\ln(n)}$$ then $$\dfrac{\ln(1+n)}{\ln(n)}-1=\frac{\ln(1+\frac{1}{n})}{\ln(n)}$$ note that  $\ln(1+\frac 1n)=\frac 1n+o(\frac 1n)$ then $\ln(1+\frac 1n)\sim \frac 1n$ thus $u_n-1\sim \frac 1{n\ln(n)}$ or the serie $\dfrac{1}{n\ln(n)}$ divergent by Bertrand's test the sum up $ \sum_{n\geq 2} \dfrac{\ln(1+n)}{\ln(n)}-1$ divergent Is my proof correct,How would you prove convergence/divergence of the following series? $$ \sum_{n\geq 2}\left( \dfrac{\ln(1+n)}{\ln(n)}-1\right)$$ I'm interested in more ways of proving convergence/divergence for this series. My thoughts $$\dfrac{\ln(1+n)}{\ln(n)}=\frac{\ln(n(1+\dfrac{1}{n})}{\ln(n)} =\frac{\ln(n)+\ln(1+\frac{1}{n})}{\ln(n)} =1+\frac{\ln(1+\frac{1}{n})}{\ln(n)}$$ then $$\dfrac{\ln(1+n)}{\ln(n)}-1=\frac{\ln(1+\frac{1}{n})}{\ln(n)}$$ note that  $\ln(1+\frac 1n)=\frac 1n+o(\frac 1n)$ then $\ln(1+\frac 1n)\sim \frac 1n$ thus $u_n-1\sim \frac 1{n\ln(n)}$ or the serie $\dfrac{1}{n\ln(n)}$ divergent by Bertrand's test the sum up $ \sum_{n\geq 2} \dfrac{\ln(1+n)}{\ln(n)}-1$ divergent Is my proof correct,,"['real-analysis', 'calculus', 'sequences-and-series', 'convergence-divergence', 'contest-math']"
54,"If $f(x)=8x^3+3x$ , $x\in\mathbb{R}$, how do I find $\lim_{x \to \infty}\frac {f^{-1}(8x)-f^{-1}(x)}{x^{1/3}}$?","If  , , how do I find ?",f(x)=8x^3+3x x\in\mathbb{R} \lim_{x \to \infty}\frac {f^{-1}(8x)-f^{-1}(x)}{x^{1/3}},"Let $f: \mathbb R \to \mathbb R$ be defined as $f(x)=8x^3+3x$. Then $f$ is continuous , strictly increasing, and $\lim _{x\to \infty}f(x)=\infty , \lim_{x \to -\infty}f(x)=-\infty$ , so $f$ is injective and surjective . How do I compute $$\lim_{x \to \infty}\dfrac {f^{-1}(8x)-f^{-1}(x)}{x^{1/3}}?$$","Let $f: \mathbb R \to \mathbb R$ be defined as $f(x)=8x^3+3x$. Then $f$ is continuous , strictly increasing, and $\lim _{x\to \infty}f(x)=\infty , \lim_{x \to -\infty}f(x)=-\infty$ , so $f$ is injective and surjective . How do I compute $$\lim_{x \to \infty}\dfrac {f^{-1}(8x)-f^{-1}(x)}{x^{1/3}}?$$",,['real-analysis']
55,"continuity of $\max \{f_1(x),f_2(x),\cdots,f_m(x) \}$ at $a$",continuity of  at,"\max \{f_1(x),f_2(x),\cdots,f_m(x) \} a","let $f_1,\cdots, f_m$ be a real valued functions defined on a set $S$ in $\mathbb R^n$. Assume that each $f_k$ is continuous at the point $a$ of $S$. For each $x \in S : f(x) = \max \{f_1(x),f_2(x),\cdots,f_m(x) \}$. Discuss continuity of $f$ at $a$. Attempt: I am trying to prove this without using induction. Each $f_k$ is continuous $\implies $ $\forall \epsilon>0, \exists \delta_1 >0$ s.t $|f_1(x)-f_1(a)|< \epsilon$ whenever $|x-a|<\delta_1$ $\forall \epsilon>0, \exists \delta_2 >0$ s.t $|f_2(x)-f_1(a)|< \epsilon$ whenever $|x-a|<\delta_2$ ... $\forall \epsilon>0, \exists \delta_m >0$ s.t $|f_m(x)-f_m(a)|< \epsilon$ whenever $|x-a|<\delta_m$ $\forall \epsilon>0, \exists \delta_k >0$ s.t $\max |f_k(x)-f_k(a)|< \epsilon$ whenever $|x-a|<\delta_k$ $\implies \max |f_k(x)-f_k(a)|< \epsilon $ whenever $|x -a|<\delta =  \min\{\delta_1,\cdots, \delta_m\}$ and $\max |f_k(x)-f_k(a)| > |\max f_k(x)-\max f_k(a)| $ $\implies |\max f_k(x)-\max f_k(a)| < \epsilon$ whenever $|x-a|<\delta = \min\{\delta_1,\cdots, \delta_m\}$ Am I correct? Thank you for your help","let $f_1,\cdots, f_m$ be a real valued functions defined on a set $S$ in $\mathbb R^n$. Assume that each $f_k$ is continuous at the point $a$ of $S$. For each $x \in S : f(x) = \max \{f_1(x),f_2(x),\cdots,f_m(x) \}$. Discuss continuity of $f$ at $a$. Attempt: I am trying to prove this without using induction. Each $f_k$ is continuous $\implies $ $\forall \epsilon>0, \exists \delta_1 >0$ s.t $|f_1(x)-f_1(a)|< \epsilon$ whenever $|x-a|<\delta_1$ $\forall \epsilon>0, \exists \delta_2 >0$ s.t $|f_2(x)-f_1(a)|< \epsilon$ whenever $|x-a|<\delta_2$ ... $\forall \epsilon>0, \exists \delta_m >0$ s.t $|f_m(x)-f_m(a)|< \epsilon$ whenever $|x-a|<\delta_m$ $\forall \epsilon>0, \exists \delta_k >0$ s.t $\max |f_k(x)-f_k(a)|< \epsilon$ whenever $|x-a|<\delta_k$ $\implies \max |f_k(x)-f_k(a)|< \epsilon $ whenever $|x -a|<\delta =  \min\{\delta_1,\cdots, \delta_m\}$ and $\max |f_k(x)-f_k(a)| > |\max f_k(x)-\max f_k(a)| $ $\implies |\max f_k(x)-\max f_k(a)| < \epsilon$ whenever $|x-a|<\delta = \min\{\delta_1,\cdots, \delta_m\}$ Am I correct? Thank you for your help",,"['real-analysis', 'proof-verification', 'continuity']"
56,Showing $\ln(\sin(x))$ is in $L_1$,Showing  is in,\ln(\sin(x)) L_1,"Prove $\ln[\sin(x)] \in L_1 [0,1].$ Since the problem does not require actually solving for the value, my strategy is to bound the integral somehow. I thought I was out of this one free since for $\epsilon > 0$ small enough, $$\lim_{\epsilon \to 0}\int_\epsilon^1 e^{\left|\ln(\sin(x))\right|}dx=\cos(\epsilon)-\cos(1) \to 1-\cos(1)<\infty$$ and so by Jensen's Inequality, $$e^{\int_0^1 \left| \ln(\sin(x))\right|\,dx}\le \int_0^1e^{\left|\ln(\sin(x))\right|}\,dx\le1-\cos(1)<\infty$$ so that $\int_0^1 \left|\ln(\sin(x))\right|\,dx<\infty$. The problem, of course, is that the argument begs the question, since Jensen's assumes the function in question is integrable to begin with, and that's what I'm trying to show. Any way to save my proof, or do I have to use a different method? I attempted integration by parts to no avail, so I am assuming there is some ""trick"" calculation I do not know that I should use here.","Prove $\ln[\sin(x)] \in L_1 [0,1].$ Since the problem does not require actually solving for the value, my strategy is to bound the integral somehow. I thought I was out of this one free since for $\epsilon > 0$ small enough, $$\lim_{\epsilon \to 0}\int_\epsilon^1 e^{\left|\ln(\sin(x))\right|}dx=\cos(\epsilon)-\cos(1) \to 1-\cos(1)<\infty$$ and so by Jensen's Inequality, $$e^{\int_0^1 \left| \ln(\sin(x))\right|\,dx}\le \int_0^1e^{\left|\ln(\sin(x))\right|}\,dx\le1-\cos(1)<\infty$$ so that $\int_0^1 \left|\ln(\sin(x))\right|\,dx<\infty$. The problem, of course, is that the argument begs the question, since Jensen's assumes the function in question is integrable to begin with, and that's what I'm trying to show. Any way to save my proof, or do I have to use a different method? I attempted integration by parts to no avail, so I am assuming there is some ""trick"" calculation I do not know that I should use here.",,['real-analysis']
57,"Does the sum $\sum\limits^{\infty}_{k=1} \frac{\sin(kx)}{k^{\alpha}}$ converge for $\alpha > \frac{1}{2}$ and $x \in [0,2 \pi]$?",Does the sum  converge for  and ?,"\sum\limits^{\infty}_{k=1} \frac{\sin(kx)}{k^{\alpha}} \alpha > \frac{1}{2} x \in [0,2 \pi]","Does the series $$ \sum^{\infty}_{k=1} \frac{\sin(kx)}{k^{\alpha}}, $$  converge for all $\alpha > \frac{1}{2}$  and for all $x \in [0,2 \pi]$? It is obvious that it does when $\alpha > 1$, but I have no idea how to deal with the case $$ \frac{1}{2} < \alpha \le 1.  $$ I already appreciate your hints/ideas.","Does the series $$ \sum^{\infty}_{k=1} \frac{\sin(kx)}{k^{\alpha}}, $$  converge for all $\alpha > \frac{1}{2}$  and for all $x \in [0,2 \pi]$? It is obvious that it does when $\alpha > 1$, but I have no idea how to deal with the case $$ \frac{1}{2} < \alpha \le 1.  $$ I already appreciate your hints/ideas.",,"['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence', 'trigonometric-series']"
58,How is the extended real number line modeled?,How is the extended real number line modeled?,,"In set theory, numbers are often constructed, e.g. from nestings of sets which eventually contain the empty set. The operations are defined in term of taking unions etc.etc. The extended real number line hat plus and minus infinity $\infty,-\infty$ in addition to the reals, and they fulfill certain axioms . How are the two objects ""$\infty$"" and ""$-\infty$"" of the extended real number line $\mathbb R\cup\{-\infty,\infty\}$ modeled in rigorous set theory? I figure it might be related to ordinal numbers. But these are two things and they behave somewhat similarly. So I wonder what they are made of, set theoretically.","In set theory, numbers are often constructed, e.g. from nestings of sets which eventually contain the empty set. The operations are defined in term of taking unions etc.etc. The extended real number line hat plus and minus infinity $\infty,-\infty$ in addition to the reals, and they fulfill certain axioms . How are the two objects ""$\infty$"" and ""$-\infty$"" of the extended real number line $\mathbb R\cup\{-\infty,\infty\}$ modeled in rigorous set theory? I figure it might be related to ordinal numbers. But these are two things and they behave somewhat similarly. So I wonder what they are made of, set theoretically.",,"['real-analysis', 'set-theory']"
59,"Can open sets be an open cover, for itself?","Can open sets be an open cover, for itself?",,"I have Baby Rudin's book with me and it clearly defines a cover to be open . In a followup, it defines a set $K$ to be compact if every open cover of $K$ contains a finite subcover. And the rest I quote More explicitly, the requirement is that if $\{ G_{\alpha}\}$ is an open cover of $K$, then there are finitely many indices $\alpha_1, \dots, \alpha_n$ such that $$K \subset G_{\alpha_1} \cup \dots\cup G_{\alpha_n}$$ From the notation, it would seem to suggest that we can't have $K$ to be an improper subset of the covers. Now in another book (which I referenced another user whom referenced the book) by Richardson (I don't know the full name sorry), it's definition of a cover allows a set to cover itself. So what subtlety could I have possibly overlooked? Sorry if this mysterious reference is vague, but I couldn't get enough information on the book.","I have Baby Rudin's book with me and it clearly defines a cover to be open . In a followup, it defines a set $K$ to be compact if every open cover of $K$ contains a finite subcover. And the rest I quote More explicitly, the requirement is that if $\{ G_{\alpha}\}$ is an open cover of $K$, then there are finitely many indices $\alpha_1, \dots, \alpha_n$ such that $$K \subset G_{\alpha_1} \cup \dots\cup G_{\alpha_n}$$ From the notation, it would seem to suggest that we can't have $K$ to be an improper subset of the covers. Now in another book (which I referenced another user whom referenced the book) by Richardson (I don't know the full name sorry), it's definition of a cover allows a set to cover itself. So what subtlety could I have possibly overlooked? Sorry if this mysterious reference is vague, but I couldn't get enough information on the book.",,"['real-analysis', 'general-topology', 'definition']"
60,Prove that $\lim_{x \rightarrow 0} \frac{1}{x}\int_0^x f(t) dt = f(0)$.,Prove that .,\lim_{x \rightarrow 0} \frac{1}{x}\int_0^x f(t) dt = f(0),"Assume $f: \mathbb{R} \rightarrow \mathbb{R}$ is continuous.  Prove that $\lim_{x \rightarrow 0} \frac{1}{x}\int_0^x f(t) dt = f(0)$. I'm having a little confusion about proving this.  So far, it is clear that $f$ is continuous at 0 and $f$ is Riemann integrable.  So with that knowledge, I am trying to use the definition of continuity.  So $|\frac{1}{x}\int_0^x f(t) dt - f(0)|=|\frac{1}{x}(f(x)-f(0))-f(0)|$.  From here, I'm not sure where to go.  Any help is appreciated.  Thanks in advance.","Assume $f: \mathbb{R} \rightarrow \mathbb{R}$ is continuous.  Prove that $\lim_{x \rightarrow 0} \frac{1}{x}\int_0^x f(t) dt = f(0)$. I'm having a little confusion about proving this.  So far, it is clear that $f$ is continuous at 0 and $f$ is Riemann integrable.  So with that knowledge, I am trying to use the definition of continuity.  So $|\frac{1}{x}\int_0^x f(t) dt - f(0)|=|\frac{1}{x}(f(x)-f(0))-f(0)|$.  From here, I'm not sure where to go.  Any help is appreciated.  Thanks in advance.",,['real-analysis']
61,Why does the tail $a_N+a_{N+1}+a_{N+2}\ldots$ of convergent series $\sum a_n$ tend to $0$ as $N\to\infty$?,Why does the tail  of convergent series  tend to  as ?,a_N+a_{N+1}+a_{N+2}\ldots \sum a_n 0 N\to\infty,"Let $a(n)>0$ for all $n \in \mathbb{N}$ be such that $\sum a(n)$ converges. Define $r(n):=\sum\limits_{k=n+1}^\infty a(k)$. The claim is $$\lim_{n \to \infty} r(n) = 0,$$ but I cannot see how to prove it. Please help.","Let $a(n)>0$ for all $n \in \mathbb{N}$ be such that $\sum a(n)$ converges. Define $r(n):=\sum\limits_{k=n+1}^\infty a(k)$. The claim is $$\lim_{n \to \infty} r(n) = 0,$$ but I cannot see how to prove it. Please help.",,"['real-analysis', 'sequences-and-series']"
62,Open Sets of $\mathbb{R}^1$ and axiom of choice,Open Sets of  and axiom of choice,\mathbb{R}^1,"In the proof of 'Every open set in $\mathbb{R}^1$ is a countable union of disjoint open intervals', we need to pick one rational representative from each of the intervals hence establish the countability. This seems to depend on the axiom of choice. Thus I wonder does this property of $\mathbb{R}$ is equivalent to the axiom of choice, that is, is there any construction such that this property is not true once we abandon the axiom of choice? Thanks!","In the proof of 'Every open set in $\mathbb{R}^1$ is a countable union of disjoint open intervals', we need to pick one rational representative from each of the intervals hence establish the countability. This seems to depend on the axiom of choice. Thus I wonder does this property of $\mathbb{R}$ is equivalent to the axiom of choice, that is, is there any construction such that this property is not true once we abandon the axiom of choice? Thanks!",,"['real-analysis', 'logic', 'axiom-of-choice']"
63,I need help interpreting Definition 1.10 in Rudin’s PMA.,I need help interpreting Definition 1.10 in Rudin’s PMA.,,"While I have a basic understanding of what this definition states, I've been running into trouble interpreting the results when I test it with certain sets other than the traditional examples used to show that the Rationals do not have this property. $\textbf{Definition 1.10:}$ ""An ordered set 𝑆 is said to have the least-upper-bound property if the following is true: $$\text{If }𝐸\subset𝑆\text{, }E \text{ is not empty, and } 𝐸 \text{ is bounded above, then } \sup(𝐸) \text{ exists in } 𝑆.""$$ $\\$ I am interpreting this as a series of nested conditional statements: $$\Big[ \big( \left( E\subset S \right) \land \left( E \neq \varnothing \right) \land \left( E \text{  bounded above}\right) \big) \implies (\ \sup(E) \text{ exists in }S)\ \Big] \implies S \text{ has LUB property}$$ From here, we usually let $S=\mathbb{Q}$ and $E=\{q\in\mathbb{Q}:q^2<2 \}$ to show that $\mathbb{Q}$ does not have the LUB property. This makes sense to me because while the antecedent within brackets is true, the consequent is false. Thus the bracketed conditional is false, forcing the entire definition to be true. But what about if (for example) $S=\mathbb{Q}$ and $E=\{q\in\mathbb{Q}:q^2<-1\}$ ? In this case, $E=\varnothing$ , thus the antecedent within the brackets is false. This renders the entire conditional in brackets true. Then the whole definition can only be true if $\mathbb{Q}$ has the LUB property - which is clearly wrong. Can someone help me track down my misunderstanding?","While I have a basic understanding of what this definition states, I've been running into trouble interpreting the results when I test it with certain sets other than the traditional examples used to show that the Rationals do not have this property. ""An ordered set 𝑆 is said to have the least-upper-bound property if the following is true: I am interpreting this as a series of nested conditional statements: From here, we usually let and to show that does not have the LUB property. This makes sense to me because while the antecedent within brackets is true, the consequent is false. Thus the bracketed conditional is false, forcing the entire definition to be true. But what about if (for example) and ? In this case, , thus the antecedent within the brackets is false. This renders the entire conditional in brackets true. Then the whole definition can only be true if has the LUB property - which is clearly wrong. Can someone help me track down my misunderstanding?","\textbf{Definition 1.10:} \text{If }𝐸\subset𝑆\text{, }E \text{ is not empty, and } 𝐸 \text{ is bounded above, then } \sup(𝐸) \text{ exists in } 𝑆."" \\ \Big[ \big( \left( E\subset S \right) \land \left( E \neq \varnothing \right) \land \left( E \text{  bounded above}\right) \big) \implies (\ \sup(E) \text{ exists in }S)\ \Big] \implies S \text{ has LUB property} S=\mathbb{Q} E=\{q\in\mathbb{Q}:q^2<2 \} \mathbb{Q} S=\mathbb{Q} E=\{q\in\mathbb{Q}:q^2<-1\} E=\varnothing \mathbb{Q}",['real-analysis']
64,Integrate $\int_0^1 \frac{x^2-1}{\left(x^4+x^3+x^2+x+1\right)\ln{x}} \mathop{dx}$,Integrate,\int_0^1 \frac{x^2-1}{\left(x^4+x^3+x^2+x+1\right)\ln{x}} \mathop{dx},Insane integral $$\int_0^1 \frac{x^2-1}{\left(x^4+x^3+x^2+x+1\right)\ln x} \mathop{dx}$$ I know $x^4+x^3+x^2+x+1=\frac{x^5-1}{x-1}$ but does it help?  I think $u=\ln{x}$ might be necessary some point.,Insane integral I know but does it help?  I think might be necessary some point.,\int_0^1 \frac{x^2-1}{\left(x^4+x^3+x^2+x+1\right)\ln x} \mathop{dx} x^4+x^3+x^2+x+1=\frac{x^5-1}{x-1} u=\ln{x},['real-analysis']
65,Does Young's inequality hold only for conjugate exponents?,Does Young's inequality hold only for conjugate exponents?,,"Suppose that $ab \leq \frac{1}{p}a^p+\frac{1}{q}b^q$ holds for every real numbers $a,b\ge 0$ . (where $p,q>0$ are some fixed numbers). Is it true that $ \frac{1}{p}+\frac{1}{q}=1$ ? I guess so, and I would like to find an easy proof of that fact. Plugging in $a=b=1$ , we get $ \frac{1}{p}+\frac{1}{q}\ge1$ . Is there an easy way to see that the converse inequality must hold? To be clear, I am not looking for proofs of Young's inequality; only for a way to see why the relation between the conjugate exponents is the only possible option. Edit: Here is a comment to help future readers (including future me) to see how can one come with Nicolás's nice idea to apply the inequality for $a=\lambda^{\frac{1}{p}}, b=\lambda^{\frac{1}{q}}$ . The idea is that it is inconvenient to compare a sum with another number; By using the specific choice of $a,b$ above, the sum is simplified, since the scales of the auxiliary parameter $\lambda$ in both summands are now identical. Comment: I know that the relation $ \frac{1}{p}+\frac{1}{q}=1$ is necessary for Holder's inequality in general; this can be seen by scaling the measure. However, I don't think this approach is applicable here.","Suppose that holds for every real numbers . (where are some fixed numbers). Is it true that ? I guess so, and I would like to find an easy proof of that fact. Plugging in , we get . Is there an easy way to see that the converse inequality must hold? To be clear, I am not looking for proofs of Young's inequality; only for a way to see why the relation between the conjugate exponents is the only possible option. Edit: Here is a comment to help future readers (including future me) to see how can one come with Nicolás's nice idea to apply the inequality for . The idea is that it is inconvenient to compare a sum with another number; By using the specific choice of above, the sum is simplified, since the scales of the auxiliary parameter in both summands are now identical. Comment: I know that the relation is necessary for Holder's inequality in general; this can be seen by scaling the measure. However, I don't think this approach is applicable here.","ab \leq \frac{1}{p}a^p+\frac{1}{q}b^q a,b\ge 0 p,q>0  \frac{1}{p}+\frac{1}{q}=1 a=b=1  \frac{1}{p}+\frac{1}{q}\ge1 a=\lambda^{\frac{1}{p}}, b=\lambda^{\frac{1}{q}} a,b \lambda  \frac{1}{p}+\frac{1}{q}=1","['real-analysis', 'inequality', 'symmetry', 'young-inequality']"
66,Evaluate $\lim_{n\to \infty}\left(\frac{1}{\sqrt{n(n+1)}}+\frac{1}{\sqrt{(n+1)(n+2)}}+\cdots+\frac{1}{\sqrt{(2n-1)2n}}\right)$,Evaluate,\lim_{n\to \infty}\left(\frac{1}{\sqrt{n(n+1)}}+\frac{1}{\sqrt{(n+1)(n+2)}}+\cdots+\frac{1}{\sqrt{(2n-1)2n}}\right),I want to evaluate $$\lim_{n\to \infty}\left(\frac{1}{\sqrt{n(n+1)}}+\frac{1}{\sqrt{(n+1)(n+2)}}+\cdots+\frac{1}{\sqrt{(2n-1)2n}}\right).$$ I have computed \begin{align*} a_{n+1}-a_n & = \frac{1}{\sqrt{(2n+1)(2n+2)}}-\frac{1}{\sqrt{n(n+1)}}\\             & = \frac{1}{\sqrt{n+1}}\left( \frac{1}{\sqrt{2(2n+1)}}-\frac{1}{\sqrt{n}} \right). \end{align*} Now I'm stuck.,I want to evaluate I have computed Now I'm stuck.,"\lim_{n\to \infty}\left(\frac{1}{\sqrt{n(n+1)}}+\frac{1}{\sqrt{(n+1)(n+2)}}+\cdots+\frac{1}{\sqrt{(2n-1)2n}}\right). \begin{align*}
a_{n+1}-a_n & = \frac{1}{\sqrt{(2n+1)(2n+2)}}-\frac{1}{\sqrt{n(n+1)}}\\
            & = \frac{1}{\sqrt{n+1}}\left( \frac{1}{\sqrt{2(2n+1)}}-\frac{1}{\sqrt{n}} \right).
\end{align*}","['real-analysis', 'convergence-divergence']"
67,Evaluating $\lim_{x\rightarrow 0}\frac{\cos(ax) - \cos(bx)}{\sin^2(x)}$,Evaluating,\lim_{x\rightarrow 0}\frac{\cos(ax) - \cos(bx)}{\sin^2(x)},"The problem is, as stated: $$\lim_{x\rightarrow 0}\frac{\cos(ax) - \cos(bx)}{\sin^2(x)}$$   Of course, a and b are real numbers. I tried implementing the trig identity: $$\cos(ax)-\cos(bx) = -2\sin\frac{(ax-bx)}{2}\sin\frac{(ax+bx)}{2}$$ But that didn't really take me anywhere.  In my book, similar problems were often solved this way but here that doesn't seem to work. One obviously has to try and use the known limit to reduce this to a more easy limit: $$\lim_{x \rightarrow 0}\frac{\sin(x)}{x} = 1$$ Any help would be much appreciated.","The problem is, as stated: $$\lim_{x\rightarrow 0}\frac{\cos(ax) - \cos(bx)}{\sin^2(x)}$$   Of course, a and b are real numbers. I tried implementing the trig identity: $$\cos(ax)-\cos(bx) = -2\sin\frac{(ax-bx)}{2}\sin\frac{(ax+bx)}{2}$$ But that didn't really take me anywhere.  In my book, similar problems were often solved this way but here that doesn't seem to work. One obviously has to try and use the known limit to reduce this to a more easy limit: $$\lim_{x \rightarrow 0}\frac{\sin(x)}{x} = 1$$ Any help would be much appreciated.",,"['calculus', 'real-analysis', 'limits', 'trigonometry', 'limits-without-lhopital']"
68,Two monotone functions which equal on rational numbers,Two monotone functions which equal on rational numbers,,"Let $f,g:\mathbb R\to \mathbb R$ be increasing and $f(r)=g(r)$ for every $r\in\mathbb Q$. Must we have $f(x)=g(x)$ for every $x\in\mathbb R$? Thanks in advance!","Let $f,g:\mathbb R\to \mathbb R$ be increasing and $f(r)=g(r)$ for every $r\in\mathbb Q$. Must we have $f(x)=g(x)$ for every $x\in\mathbb R$? Thanks in advance!",,"['calculus', 'real-analysis', 'functions']"
69,"How to prove from scratch that there exists $q^2\in(n,n+1)$?",How to prove from scratch that there exists ?,"q^2\in(n,n+1)","Given $n$ a positive integer, how would you prove from scratch that there exists a rational number $q$ such that $n<q^2<n+1$? By ""from scratch"" I mean by not using any ""advanced"" tools like the density of the rational numbers in the real numbers. Just using the definition of rational numbers, how to prove that? I faced this problem while trying to verify that the Dedekind cut $(A,B)$ cannot be determined by a rational number, where: $B=\{x \in Q^+: x^2>2\}$ $A=Q\setminus B$ where $Q^+$ denotes the positive rationals. So, for the purposes of the problem, I still don't even know what the real numbers are.","Given $n$ a positive integer, how would you prove from scratch that there exists a rational number $q$ such that $n<q^2<n+1$? By ""from scratch"" I mean by not using any ""advanced"" tools like the density of the rational numbers in the real numbers. Just using the definition of rational numbers, how to prove that? I faced this problem while trying to verify that the Dedekind cut $(A,B)$ cannot be determined by a rational number, where: $B=\{x \in Q^+: x^2>2\}$ $A=Q\setminus B$ where $Q^+$ denotes the positive rationals. So, for the purposes of the problem, I still don't even know what the real numbers are.",,"['real-analysis', 'elementary-number-theory', 'rational-numbers']"
70,Function that is both midpoint convex and concave: $f\left(\frac{x+y}{2}\right) = \frac{f(x)+f(y)}{2}$,Function that is both midpoint convex and concave:,f\left(\frac{x+y}{2}\right) = \frac{f(x)+f(y)}{2},"Which functions $f:\mathbb{R} \to \mathbb{R}$ do satisfy Jensen's functional equation $$f\left(\frac{x+y}{2}\right) = \frac{f(x)+f(y)}{2}$$ for all $x,y \in \mathbb{R}$ ? I think the only ones are of type $f(x) = c$ for some constant $c\in \mathbb{R}$ and the solutions of the Cauchy functional equation $f(x+y) = f(x)+f(y)$ and the sums and constant multiples of these functions. Are there other functions which are both midpoint convex and concave?",Which functions do satisfy Jensen's functional equation for all ? I think the only ones are of type for some constant and the solutions of the Cauchy functional equation and the sums and constant multiples of these functions. Are there other functions which are both midpoint convex and concave?,"f:\mathbb{R} \to \mathbb{R} f\left(\frac{x+y}{2}\right) = \frac{f(x)+f(y)}{2} x,y \in \mathbb{R} f(x) = c c\in \mathbb{R} f(x+y) = f(x)+f(y)","['real-analysis', 'functions', 'functional-equations']"
71,Convergence of $\sum_{m\text{ is composite}}\frac{1}{m}$,Convergence of,\sum_{m\text{ is composite}}\frac{1}{m},It can be easily show that the harmonic series $$\sum_{n=1}^{\infty}\dfrac{1}{n}$$ is divergent. Also it has shown that the infinite series of reciprocals of primes $$\sum_{p\text{ is prime}}\dfrac{1}{p}$$ is divergent. I believe that the series $$\sum_{m\text{ is composite}}\dfrac{1}{m}$$ is also divergent. But I have no idea to attempt for a proof. Any help will be appreciate. Thank you.,It can be easily show that the harmonic series $$\sum_{n=1}^{\infty}\dfrac{1}{n}$$ is divergent. Also it has shown that the infinite series of reciprocals of primes $$\sum_{p\text{ is prime}}\dfrac{1}{p}$$ is divergent. I believe that the series $$\sum_{m\text{ is composite}}\dfrac{1}{m}$$ is also divergent. But I have no idea to attempt for a proof. Any help will be appreciate. Thank you.,,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'prime-numbers']"
72,Remark 4.31 in Baby Rudin: How to verify these points?,Remark 4.31 in Baby Rudin: How to verify these points?,,"Let $a$ and $b$ be two real numbers such that $a < b$, let $E$ be any countable subset of the open interval $(a,b)$, and let the elements of $E$ be arranged in a sequence  $$x_1, x_2, x_3, \ldots.$$ Now let $\{c_n\}$ be any sequence of positive real numbers such that the series $\sum c_n$ converges. Now define the function $f \colon (a,b) \to \mathbb{R}$ as follows:  $$f(x) \colon= \sum_{x_n < x} c_n \ \ \ \ \text{ for all } x \in (a,b).$$ Then Rudin states that (a) the function $f$ is monotonically increasing on $(a,b)$; if $a < x < y < b$, then $$f(y) = \sum_{x_n < y} c_n  \geq \sum_{x_n < x} c_n = f(x)$$ because if any $c_n < x$, then that particular $c_n$ is obviously less than $y$ also.  (b) $f$ is discontinuous at every point of $E$; in fact,  $$ f(x_n + ) - f(x_n - ) = c_n.$$ How does this hold? How to show this rigorously using the $\epsilon$-$\delta$ approach? (c) $f$ is continuous at every other point of $(a,b)$. How to show this using the rigorous approach? Moreover, $f(x-) = f(x) = f(x+)$ at all points of $(a,b)$.","Let $a$ and $b$ be two real numbers such that $a < b$, let $E$ be any countable subset of the open interval $(a,b)$, and let the elements of $E$ be arranged in a sequence  $$x_1, x_2, x_3, \ldots.$$ Now let $\{c_n\}$ be any sequence of positive real numbers such that the series $\sum c_n$ converges. Now define the function $f \colon (a,b) \to \mathbb{R}$ as follows:  $$f(x) \colon= \sum_{x_n < x} c_n \ \ \ \ \text{ for all } x \in (a,b).$$ Then Rudin states that (a) the function $f$ is monotonically increasing on $(a,b)$; if $a < x < y < b$, then $$f(y) = \sum_{x_n < y} c_n  \geq \sum_{x_n < x} c_n = f(x)$$ because if any $c_n < x$, then that particular $c_n$ is obviously less than $y$ also.  (b) $f$ is discontinuous at every point of $E$; in fact,  $$ f(x_n + ) - f(x_n - ) = c_n.$$ How does this hold? How to show this rigorously using the $\epsilon$-$\delta$ approach? (c) $f$ is continuous at every other point of $(a,b)$. How to show this using the rigorous approach? Moreover, $f(x-) = f(x) = f(x+)$ at all points of $(a,b)$.",,"['calculus', 'real-analysis', 'analysis', 'continuity']"
73,What is an advatage of defining $\mathbb{C}$ as a set containing $\mathbb{R}$?,What is an advatage of defining  as a set containing ?,\mathbb{C} \mathbb{R},"It is a theorem that every field with least upper bound property and Archimedean property is isomorphic to each other. So it seems not necessary to define $\mathbb{R}$ exactly and we simply denote any field with least upper bound property and Archimedean property as $\mathbb{R}$, in general. Until now, i have defined $\mathbb{C}$ as a product of $\mathbb{R}$, that is, $\mathbb{R}\times\mathbb{R}$. (Very precisely in this case $\mathbb{R}$ is not a subset of $\mathbb{C}$) However, i found this definition a bit uncomfortable when i'm doing arguments which should be done successively from real to complex. (i.e Abstract integral theory) So i defined it newly, so that $\mathbb{R}\subset\mathbb{C}$. I found this definition really natural than the first one since under this definition, $\mathbb{R}$ is a (topological) subspace of $\mathbb{C}$. Well, i think this is advantageous, but i'm not sure. Is there an advantage of using this definition? And is it Okay to use this definition?","It is a theorem that every field with least upper bound property and Archimedean property is isomorphic to each other. So it seems not necessary to define $\mathbb{R}$ exactly and we simply denote any field with least upper bound property and Archimedean property as $\mathbb{R}$, in general. Until now, i have defined $\mathbb{C}$ as a product of $\mathbb{R}$, that is, $\mathbb{R}\times\mathbb{R}$. (Very precisely in this case $\mathbb{R}$ is not a subset of $\mathbb{C}$) However, i found this definition a bit uncomfortable when i'm doing arguments which should be done successively from real to complex. (i.e Abstract integral theory) So i defined it newly, so that $\mathbb{R}\subset\mathbb{C}$. I found this definition really natural than the first one since under this definition, $\mathbb{R}$ is a (topological) subspace of $\mathbb{C}$. Well, i think this is advantageous, but i'm not sure. Is there an advantage of using this definition? And is it Okay to use this definition?",,"['real-analysis', 'abstract-algebra']"
74,Why a function with compact support vanishes on the boundary of its domain,Why a function with compact support vanishes on the boundary of its domain,,"A function has compact support if its support is a compact set, while the support of a function $u:G\rightarrow\mathbb{R}$ is defined to be $$\mathrm{supp}(u)=\overline{\{x\in G\mid u(x)\neq0\}}.$$ Lately, a statement said that If a function has compact support, it vanishes on the boundary of its domain. So, how does this implication come up? Is there anyone who can prove it? Thanks.","A function has compact support if its support is a compact set, while the support of a function is defined to be Lately, a statement said that If a function has compact support, it vanishes on the boundary of its domain. So, how does this implication come up? Is there anyone who can prove it? Thanks.",u:G\rightarrow\mathbb{R} \mathrm{supp}(u)=\overline{\{x\in G\mid u(x)\neq0\}}.,"['real-analysis', 'general-topology', 'compactness']"
75,Find a limit in an efficent way,Find a limit in an efficent way,,"I'm trying to calculate the following limit: $$\mathop {\lim }\limits_{x \to {0^ + }} {\left( {\frac{{\sin x}}{x}} \right)^{\frac{1}{x}}}$$ What I did is writing it as: $${e^{\frac{1}{x}\ln \left( {\frac{{\sin x}}{x}} \right)}}$$ Therefore, we need to calculate: $$\mathop {\lim }\limits_{x \to {0^ + }} \frac{{\ln \left( {\frac{{\sin x}}{x}} \right)}}{x}$$ Now, we can apply L'Hopital rule, Which I did: $$\Rightarrow cot(x) - {1 \over x}$$ But in order to reach the final limit two more application of LHR are needed. Is there a better way?","I'm trying to calculate the following limit: $$\mathop {\lim }\limits_{x \to {0^ + }} {\left( {\frac{{\sin x}}{x}} \right)^{\frac{1}{x}}}$$ What I did is writing it as: $${e^{\frac{1}{x}\ln \left( {\frac{{\sin x}}{x}} \right)}}$$ Therefore, we need to calculate: $$\mathop {\lim }\limits_{x \to {0^ + }} \frac{{\ln \left( {\frac{{\sin x}}{x}} \right)}}{x}$$ Now, we can apply L'Hopital rule, Which I did: $$\Rightarrow cot(x) - {1 \over x}$$ But in order to reach the final limit two more application of LHR are needed. Is there a better way?",,"['calculus', 'real-analysis', 'limits']"
76,The n-th root of a prime number is irrational [duplicate],The n-th root of a prime number is irrational [duplicate],,"This question already has answers here : How to prove: if $a,b \in \mathbb N$, then $a^{1/b}$ is an integer or an irrational number? (14 answers) Closed 6 years ago . If $p$ is a prime number, how can I prove by contradiction that this equation  $x^{n}=p$ doesn't admit solutions in $\mathbb {Q}$ where $n\ge2$","This question already has answers here : How to prove: if $a,b \in \mathbb N$, then $a^{1/b}$ is an integer or an irrational number? (14 answers) Closed 6 years ago . If $p$ is a prime number, how can I prove by contradiction that this equation  $x^{n}=p$ doesn't admit solutions in $\mathbb {Q}$ where $n\ge2$",,"['real-analysis', 'prime-numbers', 'irrational-numbers']"
77,"If $\lim\limits_{x \to \pm\infty}f(x)=0$, does it imply that $\lim\limits_{x \to \pm\infty}f'(x)=0$?","If , does it imply that ?",\lim\limits_{x \to \pm\infty}f(x)=0 \lim\limits_{x \to \pm\infty}f'(x)=0,"Suppose $f:\mathbb{R} \rightarrow \mathbb{R}$ is everywhere differentiable and $\lim_{x \to \infty}f(x)=\lim_{x \to -\infty}f(x)=0$ , there exists $c \in \mathbb{R}$ such that $f(c) \gt 0$ . Can we say anything about $\lim_{x \to \infty}f'(x)$ and $\lim_{x \to -\infty}f'(x)$ ? I am tempted to say that $\lim_{x \to \infty}f'(x)$ = $\lim_{x \to -\infty}f'(x)=0$ . I started with the following, but I'm not sure this is the correct approach, $$\lim_{x \to \infty}f'(x)= \lim_{x \to \infty}\lim_{h \to 0}\frac{f(x+h)-f(x)}{h}.$$","Suppose is everywhere differentiable and , there exists such that . Can we say anything about and ? I am tempted to say that = . I started with the following, but I'm not sure this is the correct approach,",f:\mathbb{R} \rightarrow \mathbb{R} \lim_{x \to \infty}f(x)=\lim_{x \to -\infty}f(x)=0 c \in \mathbb{R} f(c) \gt 0 \lim_{x \to \infty}f'(x) \lim_{x \to -\infty}f'(x) \lim_{x \to \infty}f'(x) \lim_{x \to -\infty}f'(x)=0 \lim_{x \to \infty}f'(x)= \lim_{x \to \infty}\lim_{h \to 0}\frac{f(x+h)-f(x)}{h}.,['real-analysis']
78,Why is it meaningless for a closed set to be polygonal path connected?,Why is it meaningless for a closed set to be polygonal path connected?,,"My textbook (Complex Analysis by Saff & Snider) defines connectedness for open sets; the given definition of a connected open set is: a set in which every pair of points can be joined by a polygonal path that lies entirely in the set. Using the given definition of connected set, I don't understand why it isn't similarly defined for closed sets too?","My textbook (Complex Analysis by Saff & Snider) defines connectedness for open sets; the given definition of a connected open set is: a set in which every pair of points can be joined by a polygonal path that lies entirely in the set. Using the given definition of connected set, I don't understand why it isn't similarly defined for closed sets too?",,"['real-analysis', 'complex-analysis']"
79,Is the Euclidean Norm Differentiable at $ 0 $?,Is the Euclidean Norm Differentiable at ?, 0 ,"As the title, is the euclidean norm differentiable at $0$? I tried that prove by contradiction and apply the definition but can't really get a contradiction. Any hints?","As the title, is the euclidean norm differentiable at $0$? I tried that prove by contradiction and apply the definition but can't really get a contradiction. Any hints?",,['real-analysis']
80,How do I prove that $\lim_{x\to0^+} x\cdot\ln x=0$,How do I prove that,\lim_{x\to0^+} x\cdot\ln x=0,"How do I prove (without L'Hôpital's rule) that $$\lim_{x\to0^+} x\cdot\ln x = 0$$ . I'm trying to get some intuitive sense for this, but it's quite hard. It's like trying to prove that $x$ goes faster to $0$ then $\ln x$ goes to $-\infty$ , right ? I tried this, for $x\in(0,1)$ $$x \ln x=x⋅-\int_x^1 \frac{1}{t}dt>x\cdot\frac{(1-x)}{-x}=-1+x$$ So when $x\to0^+$ , I conclude that $x\cdot\ln x\ge-1$ .","How do I prove (without L'Hôpital's rule) that . I'm trying to get some intuitive sense for this, but it's quite hard. It's like trying to prove that goes faster to then goes to , right ? I tried this, for So when , I conclude that .","\lim_{x\to0^+} x\cdot\ln x = 0 x 0 \ln x -\infty x\in(0,1) x \ln x=x⋅-\int_x^1 \frac{1}{t}dt>x\cdot\frac{(1-x)}{-x}=-1+x x\to0^+ x\cdot\ln x\ge-1","['real-analysis', 'integration']"
81,Closed under countable union,Closed under countable union,,"I am reading a tutorial on measure theory and it states: ""Given an interval $E = [a, b]$ and a set $S$ of subsets of $E$ which is closed under countable unions, we deﬁne the following..."" I was wondering what 'closed under countable unions' meant in this case. Does it mean that $S$ contains every element of $E$? So if $E = [0,5]$, $S$ could be something like $\{[0,2], [1,3], [2,3], [2,5]\}$ since the union of those would be $[0,5]$... but it would impossible for $S$ to be something like $\{[0,1], [1,4], [2,3]\}$ since the union of those would be $[0,4]$. Am I correct? Thanks!","I am reading a tutorial on measure theory and it states: ""Given an interval $E = [a, b]$ and a set $S$ of subsets of $E$ which is closed under countable unions, we deﬁne the following..."" I was wondering what 'closed under countable unions' meant in this case. Does it mean that $S$ contains every element of $E$? So if $E = [0,5]$, $S$ could be something like $\{[0,2], [1,3], [2,3], [2,5]\}$ since the union of those would be $[0,5]$... but it would impossible for $S$ to be something like $\{[0,1], [1,4], [2,3]\}$ since the union of those would be $[0,4]$. Am I correct? Thanks!",,"['real-analysis', 'measure-theory', 'elementary-set-theory']"
82,Prove that $\zeta (4)\le 1.1$,Prove that,\zeta (4)\le 1.1,"Prove the following inequality $$\zeta (4)\le 1.1$$ I saw on the site some proofs for $\zeta(4)$ that use Fourier or Euler's way for computing its precise value, and that's fine and I can use it. Still, I wonder if there is a simpler  way around for proving this inequality.  Thanks!","Prove the following inequality $$\zeta (4)\le 1.1$$ I saw on the site some proofs for $\zeta(4)$ that use Fourier or Euler's way for computing its precise value, and that's fine and I can use it. Still, I wonder if there is a simpler  way around for proving this inequality.  Thanks!",,"['calculus', 'real-analysis', 'inequality']"
83,General Proof for the triangle inequality,General Proof for the triangle inequality,,"I am trying to prove: $P(n): |x_1| + \cdots + |x_n| \leq |x_1 + \cdots +x_n|$ for all natural numbers $n$. The $x_i$ are real numbers. Base: Let $n =1$:    we have $|x_1| \leq |x_1|$ which is clearly true Step: Let $k$ exist in the integers such that $k \geq 1$ and assume $P(k)$ is true. This is where I am lost. I do not see how to leverage the induction hypothesis. Here is my latest approach: Can you do the following in the induction step: Let $Y$ = |$x_1$ +...+$x_n$| and Let $Z$ = |$x_1$| +...+ |$x_n$| Then we have: |$Y$ + $x_n+1$| $\leq$ $Z$ + |$x_n+1$|. $Y$ $\leq$ $Z$ from the induction step, and then from the base case this is just another triangle inequality. End of proof.","I am trying to prove: $P(n): |x_1| + \cdots + |x_n| \leq |x_1 + \cdots +x_n|$ for all natural numbers $n$. The $x_i$ are real numbers. Base: Let $n =1$:    we have $|x_1| \leq |x_1|$ which is clearly true Step: Let $k$ exist in the integers such that $k \geq 1$ and assume $P(k)$ is true. This is where I am lost. I do not see how to leverage the induction hypothesis. Here is my latest approach: Can you do the following in the induction step: Let $Y$ = |$x_1$ +...+$x_n$| and Let $Z$ = |$x_1$| +...+ |$x_n$| Then we have: |$Y$ + $x_n+1$| $\leq$ $Z$ + |$x_n+1$|. $Y$ $\leq$ $Z$ from the induction step, and then from the base case this is just another triangle inequality. End of proof.",,"['real-analysis', 'inequality', 'absolute-value', 'triangle-inequality']"
84,Compute the trigonometric integrals,Compute the trigonometric integrals,,"How would you compute the following integrals? $$ I_{n} = \int_0^\pi \frac{1-\cos nx}{1- \cos x} dx  $$ $$ J_{n,m} = \int_0^\pi \frac{x^m(1-\cos nx)}{1- \cos x} dx  $$ For instance, i noticed that the first integral is convergent for any value of $n$ since $\lim_{x\to0} \frac{1- \cos nx}{1 - \cos x}= n^2$. This fact may also be extended to the second integral, as well.","How would you compute the following integrals? $$ I_{n} = \int_0^\pi \frac{1-\cos nx}{1- \cos x} dx  $$ $$ J_{n,m} = \int_0^\pi \frac{x^m(1-\cos nx)}{1- \cos x} dx  $$ For instance, i noticed that the first integral is convergent for any value of $n$ since $\lim_{x\to0} \frac{1- \cos nx}{1 - \cos x}= n^2$. This fact may also be extended to the second integral, as well.",,"['calculus', 'real-analysis', 'definite-integrals']"
85,Convergence of $a_{n}=\frac{1}{\sqrt{n}}\sum\limits_{k=1}^{n}\frac{1}{\sqrt{k}}$?,Convergence of ?,a_{n}=\frac{1}{\sqrt{n}}\sum\limits_{k=1}^{n}\frac{1}{\sqrt{k}},"For $n$ in $\mathbb{N}$, consider the sequence $\left \{ a_{n} \right \}$ defined by: $$a_{n}=\frac{1}{\sqrt{n}}\sum_{k=1}^{n}\frac{1}{\sqrt{k}}$$ I would like to prove whether this sequence is convergent, and if so what its limit is. I can prove by induction that $\sum\limits_{k=1}^{n}\frac{1}{\sqrt{k}}\geqslant \sqrt{n}$ For any $n$ in $\mathbb{N}$. Hence, $a_{n}\geqslant 1$. I wanted to prove that the sequence is decreasing and then use the monotone convergence theorem to prove it is convergent. However, I couldn't come up with a proof for this part. Anyone know how to prove convergence and find the limit? I had another proof based on using Riemann sums, but I am looking for another proof using onne of the tricks used to prove convergence of sequences. Here is my proof:  $$ a_{n}=\frac{1}{\sqrt{n}}\sum_{k=1}^{n}\frac{1}{\sqrt{k}}=\sum_{k=1}^{n}\frac{1}{n}\frac{1}{\sqrt{\frac{k}{n}}}. $$ Hence, $$\lim_{n \to \infty }a_{n}=\int_{0}^{1}\frac{dx}{\sqrt{x}}=2$$","For $n$ in $\mathbb{N}$, consider the sequence $\left \{ a_{n} \right \}$ defined by: $$a_{n}=\frac{1}{\sqrt{n}}\sum_{k=1}^{n}\frac{1}{\sqrt{k}}$$ I would like to prove whether this sequence is convergent, and if so what its limit is. I can prove by induction that $\sum\limits_{k=1}^{n}\frac{1}{\sqrt{k}}\geqslant \sqrt{n}$ For any $n$ in $\mathbb{N}$. Hence, $a_{n}\geqslant 1$. I wanted to prove that the sequence is decreasing and then use the monotone convergence theorem to prove it is convergent. However, I couldn't come up with a proof for this part. Anyone know how to prove convergence and find the limit? I had another proof based on using Riemann sums, but I am looking for another proof using onne of the tricks used to prove convergence of sequences. Here is my proof:  $$ a_{n}=\frac{1}{\sqrt{n}}\sum_{k=1}^{n}\frac{1}{\sqrt{k}}=\sum_{k=1}^{n}\frac{1}{n}\frac{1}{\sqrt{\frac{k}{n}}}. $$ Hence, $$\lim_{n \to \infty }a_{n}=\int_{0}^{1}\frac{dx}{\sqrt{x}}=2$$",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
86,integration of a continuous function $f(x) $ and $xf(x)$ is zero [duplicate],integration of a continuous function  and  is zero [duplicate],f(x)  xf(x),"This question already has an answer here : Closed 11 years ago . Possible Duplicate: Prove that $\exists a<b$ s.t. $f(a)=f(b)=0$ when $\int_0^1f(x)dx=\int_0^1xf(x)dx=0$ Suppose that $f:[0,1]\to \mathbb{R}$ is continuous, and that $$\int_{0}^{1} f(x)=\int_{0}^{1} xf(x)=0.$$ How does one prove that $f$ has at least two distinct zeroes in $[0,1]$? Well, if not say $\forall a,b\in [0,1] \ni a<b$, but $f(a)\neq f(b), f(a)>0$ then there will be a neighborhood of $a$ say $(a-\epsilon,a+\epsilon)$ where $f(x)>0$ and hence the integral will not be equal to $0$, but I don't know where I am using the other integral condition. am I wrong anywhere in my proof? please help.","This question already has an answer here : Closed 11 years ago . Possible Duplicate: Prove that $\exists a<b$ s.t. $f(a)=f(b)=0$ when $\int_0^1f(x)dx=\int_0^1xf(x)dx=0$ Suppose that $f:[0,1]\to \mathbb{R}$ is continuous, and that $$\int_{0}^{1} f(x)=\int_{0}^{1} xf(x)=0.$$ How does one prove that $f$ has at least two distinct zeroes in $[0,1]$? Well, if not say $\forall a,b\in [0,1] \ni a<b$, but $f(a)\neq f(b), f(a)>0$ then there will be a neighborhood of $a$ say $(a-\epsilon,a+\epsilon)$ where $f(x)>0$ and hence the integral will not be equal to $0$, but I don't know where I am using the other integral condition. am I wrong anywhere in my proof? please help.",,['real-analysis']
87,How do I evaluate this limit: $\lim_{n\to+\infty}\sum_{k=1}^{n} \frac{1}{k(k+1)\cdots(k+m)}$?,How do I evaluate this limit: ?,\lim_{n\to+\infty}\sum_{k=1}^{n} \frac{1}{k(k+1)\cdots(k+m)},"How do I evaluate this limit? $$\lim_{n\to+\infty}\sum_{k=1}^{n} \frac{1}{k(k+1)\cdots(k+m)} \qquad (m=1,2,3,\cdots)$$ Thanks in advance.","How do I evaluate this limit? $$\lim_{n\to+\infty}\sum_{k=1}^{n} \frac{1}{k(k+1)\cdots(k+m)} \qquad (m=1,2,3,\cdots)$$ Thanks in advance.",,"['calculus', 'real-analysis', 'sequences-and-series']"
88,Non-empty intersection always exists,Non-empty intersection always exists,,"Consider a sequence of closed sets $A_n\subset \mathbb{R}$ such that $A_{n+1}\subset A_n$. Define  $$ A = \bigcap\limits_n A_n. $$ If $A_1$ is bounded then $A_n$ is compact for all $n$. Hence if $A_n$ are not empty for all $n$ then $A$ is not empty. But given a metric space $X$, can we make it always compact with say $$ \rho'(x,y) = \tan^{-1}\rho(x,y) $$ where $\rho$ is an original metric and $\rho'$ is a new one. Also this procedure will preserve closeness of sets. This mean that regardless of compactness of $A_1$, if $A_n$ are not empty for all $n$ then $A$ is not empty? Edited: I would like to formulate the question more direct: if $A_1$ is no bounded but all $A_n$ are closed and non empty, does it mean that $A$ is not empty?","Consider a sequence of closed sets $A_n\subset \mathbb{R}$ such that $A_{n+1}\subset A_n$. Define  $$ A = \bigcap\limits_n A_n. $$ If $A_1$ is bounded then $A_n$ is compact for all $n$. Hence if $A_n$ are not empty for all $n$ then $A$ is not empty. But given a metric space $X$, can we make it always compact with say $$ \rho'(x,y) = \tan^{-1}\rho(x,y) $$ where $\rho$ is an original metric and $\rho'$ is a new one. Also this procedure will preserve closeness of sets. This mean that regardless of compactness of $A_1$, if $A_n$ are not empty for all $n$ then $A$ is not empty? Edited: I would like to formulate the question more direct: if $A_1$ is no bounded but all $A_n$ are closed and non empty, does it mean that $A$ is not empty?",,"['real-analysis', 'general-topology', 'metric-spaces']"
89,Constructive proof of boundedness of continuous functions,Constructive proof of boundedness of continuous functions,,"Consider the theorem for the continuous function: Let $a<b$ be real numbers, and let $f:[a,b]\to{\bf R}$ be a function continuous on $[a,b]$ . Then $f$ is a bounded function. The proof in the classical textbook on real analysis uses the Heine-Borel theorem. It dose not say how to find the bound for $f$ , but it show that having $f$ unbounded leads to a contradiction. Here are my questions : Is there a direct [EDITED: constructive] proof for this theorem? More generally, can a theorem in mathematics always have a constructive proof? Or what kind of statements do not have any constructive proof, say, one has to use techniques such as ""proof by contradiction"" in order to prove it?","Consider the theorem for the continuous function: Let be real numbers, and let be a function continuous on . Then is a bounded function. The proof in the classical textbook on real analysis uses the Heine-Borel theorem. It dose not say how to find the bound for , but it show that having unbounded leads to a contradiction. Here are my questions : Is there a direct [EDITED: constructive] proof for this theorem? More generally, can a theorem in mathematics always have a constructive proof? Or what kind of statements do not have any constructive proof, say, one has to use techniques such as ""proof by contradiction"" in order to prove it?","a<b f:[a,b]\to{\bf R} [a,b] f f f",[]
90,How to prove this asymptotic formula:$J_n=\int_{0}^{1}(1-x^2+x^3)^ndx\sim \sqrt{\frac{\pi}{4n}}$,How to prove this asymptotic formula:,J_n=\int_{0}^{1}(1-x^2+x^3)^ndx\sim \sqrt{\frac{\pi}{4n}},How to prove this asymptotic formula: $$J_n=\int_{0}^{1}(1-x^2+x^3)^ndx\sim \sqrt{\frac{\pi}{4n}}$$ My idea is to solve this $J_n=\int_{0}^{1}(1-x^2+x^3)^ndx$ .Is it feasible to calculate its recursion formula or general term formula？Or maybe there is a relationship between $J_n=\int_{0}^{1}(1-x^2+x^3)^ndx$ and $\sqrt{\frac{\pi}{4n}}$ which I need to find out? Any idea is ok. This problem is important to me. I really need your help.Thank you very much.,How to prove this asymptotic formula: My idea is to solve this .Is it feasible to calculate its recursion formula or general term formula？Or maybe there is a relationship between and which I need to find out? Any idea is ok. This problem is important to me. I really need your help.Thank you very much.,J_n=\int_{0}^{1}(1-x^2+x^3)^ndx\sim \sqrt{\frac{\pi}{4n}} J_n=\int_{0}^{1}(1-x^2+x^3)^ndx J_n=\int_{0}^{1}(1-x^2+x^3)^ndx \sqrt{\frac{\pi}{4n}},"['real-analysis', 'calculus', 'integration']"
91,Why do we need Lebesgue Integration and what are the limits to Riemann Integrals?,Why do we need Lebesgue Integration and what are the limits to Riemann Integrals?,,"I am studying Lebesgue integrals, or to be more general, Measure Theory. I am really having difficulties with understanding why we would go over to the Lebesgue measure. The books I'm studying are maybe to technical. Could you please elaborate a bit on the need for measure theory. Why would we want functions to be measurable? Why do we need Lebesgue Integrals? What are the limits of Riemann Integrals, and why wouldnt you maybe introduce Lebesgue Integrals first, if they are better? Thanks in advance.","I am studying Lebesgue integrals, or to be more general, Measure Theory. I am really having difficulties with understanding why we would go over to the Lebesgue measure. The books I'm studying are maybe to technical. Could you please elaborate a bit on the need for measure theory. Why would we want functions to be measurable? Why do we need Lebesgue Integrals? What are the limits of Riemann Integrals, and why wouldnt you maybe introduce Lebesgue Integrals first, if they are better? Thanks in advance.",,"['real-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure', 'riemann-integration']"
92,"if $f(x)\geq0$ for all $x$, then $f+f'+f''+f'''+ \cdots + f^n \geq0$ [duplicate]","if  for all , then  [duplicate]",f(x)\geq0 x f+f'+f''+f'''+ \cdots + f^n \geq0,"This question already has answers here : Sum of derivatives of a polynomial (3 answers) Closed 1 year ago . Suppose $f$ is a polynomial of degree $n$ , and $f\geq0$ for all $x$ , prove that $f+f'+f''+f'''+\cdots +f^n\geq0$ . (so just functions of the form $ax^n+bx^{n-1}+.....c$ . We are not going to include irrational functions, exponential, logs, etc.) So for this problem, I looked at a few functions and graphed them to see why this statement would be true. The first thing I found out is that the polynomial's highest degree must be even, otherwise there would be some $x$ where $f(x)$ is negative. Now this implies the odd numbered derivatives will have an odd number for the term with the highest degree. Having figured that out, I was wondering whether the sum of every successive would change the function so that $f+f'$ has negative values, and $f+f'+f''$ would have positive values and so on, and this does turn out to be true for the ones I've tried. If this is true in general, that would mean that $f+f'$ has some negative values, $f+f'+f''$ has not negative values, and $f+f'+f''+f'''+....f^{n-1}$ has negative values until we get to $f+f'+f''+f'''+....f^{n-1}+f^n$ which has no negative values. Ok but having figured this out, I'm not sure as to how to proceed with the proofs because the above assumptions I've made are not lemmas or proofs that I was given.","This question already has answers here : Sum of derivatives of a polynomial (3 answers) Closed 1 year ago . Suppose is a polynomial of degree , and for all , prove that . (so just functions of the form . We are not going to include irrational functions, exponential, logs, etc.) So for this problem, I looked at a few functions and graphed them to see why this statement would be true. The first thing I found out is that the polynomial's highest degree must be even, otherwise there would be some where is negative. Now this implies the odd numbered derivatives will have an odd number for the term with the highest degree. Having figured that out, I was wondering whether the sum of every successive would change the function so that has negative values, and would have positive values and so on, and this does turn out to be true for the ones I've tried. If this is true in general, that would mean that has some negative values, has not negative values, and has negative values until we get to which has no negative values. Ok but having figured this out, I'm not sure as to how to proceed with the proofs because the above assumptions I've made are not lemmas or proofs that I was given.",f n f\geq0 x f+f'+f''+f'''+\cdots +f^n\geq0 ax^n+bx^{n-1}+.....c x f(x) f+f' f+f'+f'' f+f' f+f'+f'' f+f'+f''+f'''+....f^{n-1} f+f'+f''+f'''+....f^{n-1}+f^n,"['real-analysis', 'calculus']"
93,Computing $\sum_{k=1}^{\infty}\frac{1}{k}\int_{\pi k}^{\infty}\frac{\sin(x)}{x}dx$,Computing,\sum_{k=1}^{\infty}\frac{1}{k}\int_{\pi k}^{\infty}\frac{\sin(x)}{x}dx,"I was asked to evaluate the following sum: $$\sum_{k=1}^{\infty}\frac{1}{k}\int_{\pi k}^{\infty}\frac{\sin(x)}{x}dx$$ I'm trying to use $$\int_{0}^{\infty} \frac{\sin(x)}{x}dx=\frac{\pi}{2}$$ However, it doesn't seem to work. Any help is greatly appreciated.","I was asked to evaluate the following sum: I'm trying to use However, it doesn't seem to work. Any help is greatly appreciated.",\sum_{k=1}^{\infty}\frac{1}{k}\int_{\pi k}^{\infty}\frac{\sin(x)}{x}dx \int_{0}^{\infty} \frac{\sin(x)}{x}dx=\frac{\pi}{2},"['real-analysis', 'sequences-and-series']"
94,Asymptotic expansion of $u_{n + 1} = \frac12 \arctan(u_n)$,Asymptotic expansion of,u_{n + 1} = \frac12 \arctan(u_n),"(I'm aware of Asymptotic expansion of $v_n = 2^nu_n$ where $u_{n+1} = \dfrac{1}{2}\arctan(u_n)$ but it has no answers…) Let be $u_0 \in \mathbb{R}$ and the sequence $(u_n)_n$ defined by: $u_{n + 1} = \frac12 \arctan(u_n)$ . I define also: $v_n = 2^n u_n$ , so I can show that: $\lim (u_n)_n = 0$ (by studying $x \mapsto \frac12 \arctan(x)$ ), thus, I can show that $(v_n)_n$ is monotone and converges because it is bound. Now, I conclude: $u_n \sim \dfrac{l}{2^n}$ , I'd like to determine $l$ more precisely. Here is what I tried, I suspect $l$ to be something like $f(\pi)$ for some $f$ : push the asymptotic expansion of $\arctan$ to the 2nd order and reinject it ; use $\arctan(u_n) + \arctan(1/u_n) = \dfrac{\pi}{2}$ ; use series techniques to look for $\sum v_{n + 1} - v_n$ , maybe conclude using Cesaro summation","(I'm aware of Asymptotic expansion of $v_n = 2^nu_n$ where $u_{n+1} = \dfrac{1}{2}\arctan(u_n)$ but it has no answers…) Let be and the sequence defined by: . I define also: , so I can show that: (by studying ), thus, I can show that is monotone and converges because it is bound. Now, I conclude: , I'd like to determine more precisely. Here is what I tried, I suspect to be something like for some : push the asymptotic expansion of to the 2nd order and reinject it ; use ; use series techniques to look for , maybe conclude using Cesaro summation",u_0 \in \mathbb{R} (u_n)_n u_{n + 1} = \frac12 \arctan(u_n) v_n = 2^n u_n \lim (u_n)_n = 0 x \mapsto \frac12 \arctan(x) (v_n)_n u_n \sim \dfrac{l}{2^n} l l f(\pi) f \arctan \arctan(u_n) + \arctan(1/u_n) = \dfrac{\pi}{2} \sum v_{n + 1} - v_n,"['real-analysis', 'sequences-and-series', 'limits', 'trigonometry', 'asymptotics']"
95,What's wrong with this proof that $0 = 1$?,What's wrong with this proof that ?,0 = 1,"Let $$f_n(x)=\frac{1}{\sqrt{\pi n}}e^{-x^2/n}.$$ Note that $f_n(x)\to 0$ uniformly as $n\to\infty$ . [Proof: $0\leq f_n(x)\leq\frac{1}{\sqrt{\pi n}}$ ; given any $\epsilon > 0$ , let $M=\left\lceil\frac{1}{\pi\epsilon^2}\right\rceil$ . This guarantees that $\forall n>M:\forall x:|f_n(x)-0|<\epsilon$ .] Uniform convergence justifies taking the limit $n\to\infty$ under the integral sign: $$\lim_{n\to\infty}\int_{-\infty}^{+\infty} f_n(x)\,dx =  \int_{-\infty}^{+\infty} \lim_{n\to\infty} f_n(x)\,dx$$ The left-hand side is $1$ , because $$\forall n>0:\int_{-\infty}^{+\infty} f_n(x)\,dx = 1,$$ whereas the right-hand side is $0$ , because $$\forall x\in\mathbb{R}:\lim_{n\to\infty} f_n(x) = 0.$$ Therefore, $$1=0.$$","Let Note that uniformly as . [Proof: ; given any , let . This guarantees that .] Uniform convergence justifies taking the limit under the integral sign: The left-hand side is , because whereas the right-hand side is , because Therefore,","f_n(x)=\frac{1}{\sqrt{\pi n}}e^{-x^2/n}. f_n(x)\to 0 n\to\infty 0\leq f_n(x)\leq\frac{1}{\sqrt{\pi n}} \epsilon > 0 M=\left\lceil\frac{1}{\pi\epsilon^2}\right\rceil \forall n>M:\forall x:|f_n(x)-0|<\epsilon n\to\infty \lim_{n\to\infty}\int_{-\infty}^{+\infty} f_n(x)\,dx = 
\int_{-\infty}^{+\infty} \lim_{n\to\infty} f_n(x)\,dx 1 \forall n>0:\int_{-\infty}^{+\infty} f_n(x)\,dx = 1, 0 \forall x\in\mathbb{R}:\lim_{n\to\infty} f_n(x) = 0. 1=0.","['real-analysis', 'improper-integrals', 'uniform-convergence', 'fake-proofs']"
96,Completion of measure spaces - uniqueness,Completion of measure spaces - uniqueness,,"If $(X,\mathcal{A},\mu)$ is a measure space,   $(X,\mathcal{B},\overline{\mu})$ is complete,   $\mathcal{B}\supset\mathcal{A}$, and $\overline{\mu}(A)=\mu(A)$ for   every $A\in\mathcal{A}$, is $(X,\mathcal{B},\overline{\mu})$   necessarily the completion of $(X,\mathcal{A},\mu)$? My definition of completion is: The completion of $\mathcal{A}$ is the smallest $\sigma$-algebra $\mathcal{B}$ containing $\mathcal{A}$ such that $(X,\mathcal{B},\mu)$ is complete. It seems like the answer to my question is no, because it isn't clear that $\mathcal{B}$ is necessarily the smallest $\sigma$-algebra satisfying the required properties. But I have been looking at the Completion Theorem where they seem to assume that the answer to my question is yes. How can I see that the smallest $\sigma$-algebra satisfying the required properties is the only $\sigma$-algebra satisfying the required properties?","If $(X,\mathcal{A},\mu)$ is a measure space,   $(X,\mathcal{B},\overline{\mu})$ is complete,   $\mathcal{B}\supset\mathcal{A}$, and $\overline{\mu}(A)=\mu(A)$ for   every $A\in\mathcal{A}$, is $(X,\mathcal{B},\overline{\mu})$   necessarily the completion of $(X,\mathcal{A},\mu)$? My definition of completion is: The completion of $\mathcal{A}$ is the smallest $\sigma$-algebra $\mathcal{B}$ containing $\mathcal{A}$ such that $(X,\mathcal{B},\mu)$ is complete. It seems like the answer to my question is no, because it isn't clear that $\mathcal{B}$ is necessarily the smallest $\sigma$-algebra satisfying the required properties. But I have been looking at the Completion Theorem where they seem to assume that the answer to my question is yes. How can I see that the smallest $\sigma$-algebra satisfying the required properties is the only $\sigma$-algebra satisfying the required properties?",,"['real-analysis', 'measure-theory']"
97,Is this proof that every strictly increasing $f\colon\mathbb{R\to R}$ is injective flawed?,Is this proof that every strictly increasing  is injective flawed?,f\colon\mathbb{R\to R},"The problem is as follows: A function $f:\mathbb{R}\to\mathbb{R}$ is called strictly increasing if $$\forall x,y\in\mathbf{R},\ x<y\implies f(x)<f(y).$$ Show that any strictly increasing function is injective. The provided solution is as follows: $\ \ $ $\text{S}\scriptstyle{\text{OLUTION}}:$ Suppose that $x_1,x_2\in\mathbb{R}$ are such that $f(x_1)=f(x_2)$. Then it is not true that $x_1<x_2$ (for then $f(x_1)<f(x_2)$) and so $x_1\geq x_2$. Similarly $x_2\geq x_1$, and so $x_1=x_2$. Thus $f$ is injective. I've been pondering over this proof, and it seems to me that it might be logically incorrect. In the definition of a strictly increasing function, we have that $x < y \implies f(x) < f(y)$. So we have that $A \implies B$, where $A = x < y$ and $B = f(x) < f(y)$. So it is a one-way implication . Then, in the solution proof, it begins by assuming that $f(x_1) = f(x_2)$. But it then claims that this implies that it is not true that $x_1 < x_2$, since we would otherwise have had $f(x_1) < f(x_2)$. However , as I just stated, the definition for a strictly increasing function is provided as $x < y \implies f(x) < f(y)$ -- a one-way implication from $x < y$ to $f(x) < f(y)$. But isn't the solution proof attempting to use $f(x) < f(y) \implies x < y$ instead, since it is using the fact that we have $f(x_1) = f(x_2)$ to claim that it is then not true that $x_1 < x_2$? So, in order for this solution proof to be correct, wouldn't the definition provided for a strictly increasing function have to also include the reverse one-way implication, so that we would have $x < y \iff f(x) < f(y)$? I would greatly appreciate it if people could please take the time to clarify this. If I'm misunderstanding anything, I would appreciate an explanation so that I may understand where my misunderstanding lies.","The problem is as follows: A function $f:\mathbb{R}\to\mathbb{R}$ is called strictly increasing if $$\forall x,y\in\mathbf{R},\ x<y\implies f(x)<f(y).$$ Show that any strictly increasing function is injective. The provided solution is as follows: $\ \ $ $\text{S}\scriptstyle{\text{OLUTION}}:$ Suppose that $x_1,x_2\in\mathbb{R}$ are such that $f(x_1)=f(x_2)$. Then it is not true that $x_1<x_2$ (for then $f(x_1)<f(x_2)$) and so $x_1\geq x_2$. Similarly $x_2\geq x_1$, and so $x_1=x_2$. Thus $f$ is injective. I've been pondering over this proof, and it seems to me that it might be logically incorrect. In the definition of a strictly increasing function, we have that $x < y \implies f(x) < f(y)$. So we have that $A \implies B$, where $A = x < y$ and $B = f(x) < f(y)$. So it is a one-way implication . Then, in the solution proof, it begins by assuming that $f(x_1) = f(x_2)$. But it then claims that this implies that it is not true that $x_1 < x_2$, since we would otherwise have had $f(x_1) < f(x_2)$. However , as I just stated, the definition for a strictly increasing function is provided as $x < y \implies f(x) < f(y)$ -- a one-way implication from $x < y$ to $f(x) < f(y)$. But isn't the solution proof attempting to use $f(x) < f(y) \implies x < y$ instead, since it is using the fact that we have $f(x_1) = f(x_2)$ to claim that it is then not true that $x_1 < x_2$? So, in order for this solution proof to be correct, wouldn't the definition provided for a strictly increasing function have to also include the reverse one-way implication, so that we would have $x < y \iff f(x) < f(y)$? I would greatly appreciate it if people could please take the time to clarify this. If I'm misunderstanding anything, I would appreciate an explanation so that I may understand where my misunderstanding lies.",,"['real-analysis', 'functions', 'proof-verification']"
98,Finding the limit of a sequence of integrals,Finding the limit of a sequence of integrals,,"Let us define a sequence of function as   $$f_n(x)=\frac{2nx^{n-1}}{x+1}\;\;\text{for each $x\in [0,1]$ and for all $n\in\mathbb{N}$}$$   What is $\displaystyle \lim_{n\to \infty} \int_0^1 f_ n(x) dx$ ? How to find the limit? If I can interchange the limit with integral the ans is surely $0$. But can we interchange the limit with integral here. All that I know is Lebesgue's monotone convergence theorem and dominated convergence theorem that allow us to interchange limit and integrals. But this seems not to be useful here. Then how to proceed? Any help would be appreciated. Thanks in advance.","Let us define a sequence of function as   $$f_n(x)=\frac{2nx^{n-1}}{x+1}\;\;\text{for each $x\in [0,1]$ and for all $n\in\mathbb{N}$}$$   What is $\displaystyle \lim_{n\to \infty} \int_0^1 f_ n(x) dx$ ? How to find the limit? If I can interchange the limit with integral the ans is surely $0$. But can we interchange the limit with integral here. All that I know is Lebesgue's monotone convergence theorem and dominated convergence theorem that allow us to interchange limit and integrals. But this seems not to be useful here. Then how to proceed? Any help would be appreciated. Thanks in advance.",,"['real-analysis', 'limits', 'sequence-of-function']"
99,Every infinite subset of $E$ in $\mathbb R^k$ having a limit point in $E$ implies $E$ is closed,Every infinite subset of  in  having a limit point in  implies  is closed,E \mathbb R^k E E,"This is exactly what is written in Walter Rudin chapter 2, Theorem 2.41: If $E$ is not closed, then there is a point $\mathbf{x}_o \in \mathbb{R}^k$ which is a limit point of $E$ but not a point of $E$. For $n = 1,2,3, \dots $ there are points $\mathbf{x}_n \in E$ such that $|\mathbf{x}_n-\mathbf{x}_o| < \frac{1}{n}$. Let $S$ be the set of these points $\mathbf{x}_n$. Then $S$ is infinite (otherwise $|\mathbf{x}_n-\mathbf{x}_o|$ would have a constant positive value, for infinitely many $n$), $S$ has  $\mathbf{x}_o$ as a limit point, and $S$ has no other limit point in $\mathbb{R}^k$. For if $\mathbf{y} \in \mathbb{R}^k, \mathbf{y} \neq \mathbf{x}_o$, then \begin{align} |\mathbf{x}_n-\mathbf{y}| \geq{} &|\mathbf{x}_o-\mathbf{y}| - |\mathbf{x}_n-\mathbf{x}_o|\\ \geq {} & |\mathbf{x}_o-\mathbf{y}| - \dfrac{1}{n} \geq \dfrac{1}{2} |\mathbf{x}_o-\mathbf{y}|  \end{align} for all but finitely many $n$. This shows that $\mathbf{y}$ is not a limit point of $S$. The question: I'm stuck in understanding the reason behind why $S$ is infinite.  Also I need clarification why the last inequality holds. May someone help, please?","This is exactly what is written in Walter Rudin chapter 2, Theorem 2.41: If $E$ is not closed, then there is a point $\mathbf{x}_o \in \mathbb{R}^k$ which is a limit point of $E$ but not a point of $E$. For $n = 1,2,3, \dots $ there are points $\mathbf{x}_n \in E$ such that $|\mathbf{x}_n-\mathbf{x}_o| < \frac{1}{n}$. Let $S$ be the set of these points $\mathbf{x}_n$. Then $S$ is infinite (otherwise $|\mathbf{x}_n-\mathbf{x}_o|$ would have a constant positive value, for infinitely many $n$), $S$ has  $\mathbf{x}_o$ as a limit point, and $S$ has no other limit point in $\mathbb{R}^k$. For if $\mathbf{y} \in \mathbb{R}^k, \mathbf{y} \neq \mathbf{x}_o$, then \begin{align} |\mathbf{x}_n-\mathbf{y}| \geq{} &|\mathbf{x}_o-\mathbf{y}| - |\mathbf{x}_n-\mathbf{x}_o|\\ \geq {} & |\mathbf{x}_o-\mathbf{y}| - \dfrac{1}{n} \geq \dfrac{1}{2} |\mathbf{x}_o-\mathbf{y}|  \end{align} for all but finitely many $n$. This shows that $\mathbf{y}$ is not a limit point of $S$. The question: I'm stuck in understanding the reason behind why $S$ is infinite.  Also I need clarification why the last inequality holds. May someone help, please?",,"['real-analysis', 'general-topology', 'compactness']"
