,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Optimizing Independent Variables to Maximize Dependent Variable,Optimizing Independent Variables to Maximize Dependent Variable,,I looked around online and couldn't find anything that was answering my question so I thought I would take to the stack! I'm interested in knowing if there is a statistical or mathematical way of optimizing ones dependent variable in regression analysis by shifting the values of your independent variables. For instance in market mix modeling generally the regression equation looks something like: $$\text{Sales = TV Spend + Print Spend + Online Spend + Radio Spend}$$ So once you have your model based on the data how do you optimize the spend levels to have the max sales without just randomly shifting spend from one category to another?,I looked around online and couldn't find anything that was answering my question so I thought I would take to the stack! I'm interested in knowing if there is a statistical or mathematical way of optimizing ones dependent variable in regression analysis by shifting the values of your independent variables. For instance in market mix modeling generally the regression equation looks something like: $$\text{Sales = TV Spend + Print Spend + Online Spend + Radio Spend}$$ So once you have your model based on the data how do you optimize the spend levels to have the max sales without just randomly shifting spend from one category to another?,,"['statistics', 'regression', 'regression-analysis']"
1,"Square root of Chi-square distribution tends to $N(0,1)$",Square root of Chi-square distribution tends to,"N(0,1)","The question requires to show that $\sqrt{2\chi^2_n}-\sqrt{2n}$ converges in distribution to $N(0,1)$ as $n \rightarrow \infty$, which I dont know how to proceed. The question also has a first part asking to show $\frac{\chi^2_n-n}{\sqrt{2n}}$ converges in distribution to $N(0,1)$, and it's straightforward by CLT. I am thinking if there is any connection between the two parts, however still have no idea. Can anyone help me ? Thanks in advance.","The question requires to show that $\sqrt{2\chi^2_n}-\sqrt{2n}$ converges in distribution to $N(0,1)$ as $n \rightarrow \infty$, which I dont know how to proceed. The question also has a first part asking to show $\frac{\chi^2_n-n}{\sqrt{2n}}$ converges in distribution to $N(0,1)$, and it's straightforward by CLT. I am thinking if there is any connection between the two parts, however still have no idea. Can anyone help me ? Thanks in advance.",,"['statistics', 'probability-theory', 'probability-distributions']"
2,"Whether or not $X_1$,...,$X_n$ are independent and exchangeable","Whether or not ,..., are independent and exchangeable",X_1 X_n,"For some n = 1,2,..., let $Y_1$,...,$Y_{n+1}$ denote iid real-valued random variables. Define $X_j$ = $Y_j$$Y_{j+1}$, $\hspace{10mm}$j=1,...,n a) Are $X_1$,$X_2$,...,$X_n$ independent? b) Are $X_1$,$X_2$,...,$X_n$ exchangeable? Attempts: a) No. Counterexample: Let Y be a Bernoulli (0,1) distribution, with p = 1/2, where $Y_1$ = 0 and $Y_2$ = 1. Then $X_1$ = $Y_1$$Y_{2}$ Now $E(Y_1*Y_2)%$ = $E(0*1)$ = $E(0)$ = 1/2 However, $E(Y_1)%$$E(Y_2)%$ = 1/2*1/2 = 1/4 Since $E(Y_1*Y_2)%$ $\neq$ $E(Y_1)%$$E(Y_2)%$, the $X_i$'s are not independent. b) Not sure.","For some n = 1,2,..., let $Y_1$,...,$Y_{n+1}$ denote iid real-valued random variables. Define $X_j$ = $Y_j$$Y_{j+1}$, $\hspace{10mm}$j=1,...,n a) Are $X_1$,$X_2$,...,$X_n$ independent? b) Are $X_1$,$X_2$,...,$X_n$ exchangeable? Attempts: a) No. Counterexample: Let Y be a Bernoulli (0,1) distribution, with p = 1/2, where $Y_1$ = 0 and $Y_2$ = 1. Then $X_1$ = $Y_1$$Y_{2}$ Now $E(Y_1*Y_2)%$ = $E(0*1)$ = $E(0)$ = 1/2 However, $E(Y_1)%$$E(Y_2)%$ = 1/2*1/2 = 1/4 Since $E(Y_1*Y_2)%$ $\neq$ $E(Y_1)%$$E(Y_2)%$, the $X_i$'s are not independent. b) Not sure.",,"['statistics', 'statistical-inference']"
3,Maximum likelihood estimator of a product of non-negative functions,Maximum likelihood estimator of a product of non-negative functions,,"Suppose that $a(\cdot)$ and $b(\cdot)$ are two non-negative functions such that  $$f(x;\theta)=a(\theta)b(x)$$ is a probability density function for each $\theta > 0$. Find the maximum likelihood estimator of $\theta$. My try: Our likelihood function is given by $$L(\theta) = \prod_{i=1}^n a(\theta)b(x_i) = a(\theta)^nb(x)^n$$ The log likelihood function is given by $$\ln L(\theta) = n\ln a(\theta) + n \ln b(x)$$ Equating it to zero we get $$\ln a (\theta) = - \ln b(x)$$ which obviously leads to nowhere. Moreover, the question itself seems weird to me. I am used to the form of ""Given a random sample $X_1,...,X_n$ of size $n$ (...)"", since this is missing now, does this imply that I cannot use the usual method I demonstrated above? Lastly, if you want to, could you check the exercise below for errors? Let $X_1,...,X_n$ denote a random sample from $$f(x;\theta) = f_\theta (x) = \theta f_1(x) + (1-\theta)f_0 (x)$$ where $0 \leq \theta \leq 1$ and $f_0(\cdot)$ and $f_1(\cdot)$ are known densities, estimate $\theta$ by the method of moments. Answer: First, we need to write $E[X]$ in a better form: \begin{align*} E[x] &= \int_{-\infty}^\infty x\cdot f(x;\theta)dx = \int_{-\infty}^\infty x(\theta f_1(x) + (1-\theta)f_0 (x))dx \\ &= \theta \int_{-\infty}^\infty x\cdot f_1 (x)dx + (1-\theta)\int_{-\infty}^\infty x\cdot f_0 (x)dx \\ &= \theta \int x(f_1-f_0)dx + \int x f_0 dx \\ &=\theta \left(E_1\left[x\right] - E_0\left[x\right]\right) + E_0\left[x\right] \end{align*} Equating this to the first sample moment ($m_1'$) we get: \begin{align*} m_1'= \theta \left(E_1\left[x\right] - E_0\left[x\right]\right) + E_0\left[x\right] \end{align*} which is equivalent to  \begin{align*} \theta = \dfrac{m_1' - E_0[x]}{E_1[x] - E_0[x]} \end{align*} Hence, our method of moments estimator for $\theta$ is given by: $$\hat{\theta} = \dfrac{m_1' - E_0[x]}{E_1[x] - E_0[x]}$$ Both are questions from ""Introduction to the theory of statistics"" by Mood, Graybill and Boes. Thanks in advance!","Suppose that $a(\cdot)$ and $b(\cdot)$ are two non-negative functions such that  $$f(x;\theta)=a(\theta)b(x)$$ is a probability density function for each $\theta > 0$. Find the maximum likelihood estimator of $\theta$. My try: Our likelihood function is given by $$L(\theta) = \prod_{i=1}^n a(\theta)b(x_i) = a(\theta)^nb(x)^n$$ The log likelihood function is given by $$\ln L(\theta) = n\ln a(\theta) + n \ln b(x)$$ Equating it to zero we get $$\ln a (\theta) = - \ln b(x)$$ which obviously leads to nowhere. Moreover, the question itself seems weird to me. I am used to the form of ""Given a random sample $X_1,...,X_n$ of size $n$ (...)"", since this is missing now, does this imply that I cannot use the usual method I demonstrated above? Lastly, if you want to, could you check the exercise below for errors? Let $X_1,...,X_n$ denote a random sample from $$f(x;\theta) = f_\theta (x) = \theta f_1(x) + (1-\theta)f_0 (x)$$ where $0 \leq \theta \leq 1$ and $f_0(\cdot)$ and $f_1(\cdot)$ are known densities, estimate $\theta$ by the method of moments. Answer: First, we need to write $E[X]$ in a better form: \begin{align*} E[x] &= \int_{-\infty}^\infty x\cdot f(x;\theta)dx = \int_{-\infty}^\infty x(\theta f_1(x) + (1-\theta)f_0 (x))dx \\ &= \theta \int_{-\infty}^\infty x\cdot f_1 (x)dx + (1-\theta)\int_{-\infty}^\infty x\cdot f_0 (x)dx \\ &= \theta \int x(f_1-f_0)dx + \int x f_0 dx \\ &=\theta \left(E_1\left[x\right] - E_0\left[x\right]\right) + E_0\left[x\right] \end{align*} Equating this to the first sample moment ($m_1'$) we get: \begin{align*} m_1'= \theta \left(E_1\left[x\right] - E_0\left[x\right]\right) + E_0\left[x\right] \end{align*} which is equivalent to  \begin{align*} \theta = \dfrac{m_1' - E_0[x]}{E_1[x] - E_0[x]} \end{align*} Hence, our method of moments estimator for $\theta$ is given by: $$\hat{\theta} = \dfrac{m_1' - E_0[x]}{E_1[x] - E_0[x]}$$ Both are questions from ""Introduction to the theory of statistics"" by Mood, Graybill and Boes. Thanks in advance!",,"['statistics', 'probability-distributions']"
4,Mean of random sum of random variable,Mean of random sum of random variable,,"Suppose that we have $X_1, X_2, \ldots$ is a sequence of i.i.d random variables with $E(X_i)<+\infty$ and $N$ is a random variable taking values in $\{1,2,\ldots\}$, $N$ is independent with $X_1, X_2, \ldots$. I know that $$E\left(\sum_{i=1}^N X_i \right) = E(N)E(X_1)$$ If I divide the sum by $N$,  $$M = \frac 1N \sum_{i=1}^N X_i  $$ I have sample mean of $X_1, X_2,\ldots, X_N$ but $N$ is random. In this case, what is the distribution of $M$ and how to calculate $E(M)$ and $Var(M)$. Thanks.","Suppose that we have $X_1, X_2, \ldots$ is a sequence of i.i.d random variables with $E(X_i)<+\infty$ and $N$ is a random variable taking values in $\{1,2,\ldots\}$, $N$ is independent with $X_1, X_2, \ldots$. I know that $$E\left(\sum_{i=1}^N X_i \right) = E(N)E(X_1)$$ If I divide the sum by $N$,  $$M = \frac 1N \sum_{i=1}^N X_i  $$ I have sample mean of $X_1, X_2,\ldots, X_N$ but $N$ is random. In this case, what is the distribution of $M$ and how to calculate $E(M)$ and $Var(M)$. Thanks.",,"['probability', 'statistics', 'probability-theory', 'probability-distributions']"
5,Simplify function with polynomial via least-squares,Simplify function with polynomial via least-squares,,"I want to ""adjust"" (simplify) $f(x)$, a function, by $g(x)$, a polynomial, via least-squares. I want to write code for that. Apperently my code is issuing wrong results, so I was wondering if my mistake lies on the math I had to do in order to simplify the problem into something ""codeable"" or on the code itself. My ""simplification"" was as follows: $r^2 = \int_{x_0}^{x_n}(f(x)-g(x))^2dx$ must be as low as possible, i.e. $$\nabla \int_{x_0}^{x_n}(f(x)-g(x))^2dx = 0$$ $g(x)$ being a polynomial of degree $n$ then there's a linear system of $n$ partials to be solved, such as $$\begin{cases}\int_{x_0}^{x_n}\frac{\partial r^2}{\partial c_i}(f(x)-\sum\limits_{j=0}^{n}c_jx^j)^2dx=0\end{cases}$$ $c_j$ the $j$th coefficient of the polynomial we need to determine, with the partial derivative in respect of $c_i$, $i$ being the system's $i$th equation (the $i$th partial). Solving the partial of $r^2$ (chain rule) gives $\int_{x_0}^{x_n}2r\frac{\partial r}{\partial c_i}rdx=0$. Taking the $2$ out of the integral, solving the partial of $r$ and replacing $r$ gives out $$\int_{x_0}^{x_n}(f(x)-\sum\limits_{j=0}^{n}c_j x^j)(-x^i)dx=0$$ Since $x^i$ is the only term of $r$ with $c_i$ (and it's $-g(x)$). Taking the $-1$ out of the integral, expanding and adjusting the equation such that $c_j$ gets out of the integral $$\int_{x_0}^{x_n}x^if(x)dx-\sum\limits_{j=0}^{n}c_j\int_{x_0}^{x_n}x^jx^idx=0$$ Alternatively (so it's easier to write code) $$\begin{cases}\int_{x_0}^{x_n}x^if(x)dx=\sum\limits_{j=i}^{n+i}c_{j-i}\int_{x_0}^{x_n}x^jdx\end{cases}$$ I tested it with the following case: $$f(x) = sin(2 \pi x) + cos(3 \pi x) \\ [x_0, x_n] = [-1, 1] \\ n (degree) = 5$$ Using the discrete least-squares approach, with $h=10^{-6}$, that is, 2000000 points, I got $$g(x) = 14.52221837x^5 - 3.90881979x^4 - 17.86276751x^3 + 3.09711194x^2 +   4.01638587x - 0.25060696$$ Which looks good. Using the continuous approach described above I got $$g(x) = -0.57270982x^5 - 3.90880894x^4 + 2.17959533x^3 + 3.0971047x^2 - 0.74400025x - 0.25060645$$ Which seems off. So... are there any issues with the math?","I want to ""adjust"" (simplify) $f(x)$, a function, by $g(x)$, a polynomial, via least-squares. I want to write code for that. Apperently my code is issuing wrong results, so I was wondering if my mistake lies on the math I had to do in order to simplify the problem into something ""codeable"" or on the code itself. My ""simplification"" was as follows: $r^2 = \int_{x_0}^{x_n}(f(x)-g(x))^2dx$ must be as low as possible, i.e. $$\nabla \int_{x_0}^{x_n}(f(x)-g(x))^2dx = 0$$ $g(x)$ being a polynomial of degree $n$ then there's a linear system of $n$ partials to be solved, such as $$\begin{cases}\int_{x_0}^{x_n}\frac{\partial r^2}{\partial c_i}(f(x)-\sum\limits_{j=0}^{n}c_jx^j)^2dx=0\end{cases}$$ $c_j$ the $j$th coefficient of the polynomial we need to determine, with the partial derivative in respect of $c_i$, $i$ being the system's $i$th equation (the $i$th partial). Solving the partial of $r^2$ (chain rule) gives $\int_{x_0}^{x_n}2r\frac{\partial r}{\partial c_i}rdx=0$. Taking the $2$ out of the integral, solving the partial of $r$ and replacing $r$ gives out $$\int_{x_0}^{x_n}(f(x)-\sum\limits_{j=0}^{n}c_j x^j)(-x^i)dx=0$$ Since $x^i$ is the only term of $r$ with $c_i$ (and it's $-g(x)$). Taking the $-1$ out of the integral, expanding and adjusting the equation such that $c_j$ gets out of the integral $$\int_{x_0}^{x_n}x^if(x)dx-\sum\limits_{j=0}^{n}c_j\int_{x_0}^{x_n}x^jx^idx=0$$ Alternatively (so it's easier to write code) $$\begin{cases}\int_{x_0}^{x_n}x^if(x)dx=\sum\limits_{j=i}^{n+i}c_{j-i}\int_{x_0}^{x_n}x^jdx\end{cases}$$ I tested it with the following case: $$f(x) = sin(2 \pi x) + cos(3 \pi x) \\ [x_0, x_n] = [-1, 1] \\ n (degree) = 5$$ Using the discrete least-squares approach, with $h=10^{-6}$, that is, 2000000 points, I got $$g(x) = 14.52221837x^5 - 3.90881979x^4 - 17.86276751x^3 + 3.09711194x^2 +   4.01638587x - 0.25060696$$ Which looks good. Using the continuous approach described above I got $$g(x) = -0.57270982x^5 - 3.90880894x^4 + 2.17959533x^3 + 3.0971047x^2 - 0.74400025x - 0.25060645$$ Which seems off. So... are there any issues with the math?",,"['calculus', 'statistics', 'numerical-methods', 'approximation', 'least-squares']"
6,How to tell if there is equal variance in a box plot?,How to tell if there is equal variance in a box plot?,,"I'm trying to decide if the variance in these groups in this boxplot are equal, so how can I tell how much variation each group has just looking at the box plot? And how can I tell if they all have equal variance? Here is the boxplot:","I'm trying to decide if the variance in these groups in this boxplot are equal, so how can I tell how much variation each group has just looking at the box plot? And how can I tell if they all have equal variance? Here is the boxplot:",,['statistics']
7,Probability of selecting distinct numbers in ascending order,Probability of selecting distinct numbers in ascending order,,"A relatively simple problem is stumping me on this :( Given a set of 4 distinct numbers that can be chosen from [0-9], what is the probability that after selecting, the first two numbers would be in ascending order? Here's my thinking so far: If we select 0 as our first, we have a 9/10 chance of choosing an appropriate second number If we select 1 as our first, we have an 8/10 chance of choosing an appropriate second number ... If we select 8 as our first, we only have a 1/10 chance of choosing an appropriate second number However if we select 9 as our first, we can't select anything greater So the probability of selecting the first two numbers such that they are ascending should be: P(1st selection) * P(2nd selection > 1st selection) * P(3rd selection that we don't care about) * P(4th selection that we don't care about) = 1/10 * 1/9 * 1/8 * 1/7 => 1/5040 I don't think I'm going about this problem the right way and my answer is completely off - I don't know how to handle what the probability of selecting the second number would be, since we could choose a great number like 0 for the first, or a bad one like 8 or 9 that would give us less choices.  Could someone explain this in a better way?","A relatively simple problem is stumping me on this :( Given a set of 4 distinct numbers that can be chosen from [0-9], what is the probability that after selecting, the first two numbers would be in ascending order? Here's my thinking so far: If we select 0 as our first, we have a 9/10 chance of choosing an appropriate second number If we select 1 as our first, we have an 8/10 chance of choosing an appropriate second number ... If we select 8 as our first, we only have a 1/10 chance of choosing an appropriate second number However if we select 9 as our first, we can't select anything greater So the probability of selecting the first two numbers such that they are ascending should be: P(1st selection) * P(2nd selection > 1st selection) * P(3rd selection that we don't care about) * P(4th selection that we don't care about) = 1/10 * 1/9 * 1/8 * 1/7 => 1/5040 I don't think I'm going about this problem the right way and my answer is completely off - I don't know how to handle what the probability of selecting the second number would be, since we could choose a great number like 0 for the first, or a bad one like 8 or 9 that would give us less choices.  Could someone explain this in a better way?",,"['probability', 'statistics']"
8,mean and variance normalization of vectors,mean and variance normalization of vectors,,"I have vectors $x \in \mathbb{R}^n$ and I expect some multivariate normal distribution . I want to normalize the vectors in such a way that $y = M (x - b)$ has mean zero ($\operatorname{E}[Y] = 0$) and variance 1 ($\operatorname{var}(Y) = 1_n$). For $n=1$, that is basically the Standard score . With $M = 1_n$ and $b = \operatorname{E}[X]$, I get it mean-normalized but not variance-normalized. With $M$ being a non-negative symmetric square root of $\operatorname{var}(X)^{-1}$, we get $\operatorname{var}(Y) = 1_n$. We can choose $b$ as in case 1. I want to estimate $M$ and $b$ based on some running/moving algorithm. For the first case, this is easy: For every new $x$, I can use the update rule $$b \leftarrow x \alpha + b (1 - \alpha)$$ for some $\alpha$. I could set $\alpha = 1/N$ and set $N \leftarrow N + 1$ in every iteration, starting with $N = 1$. How would I do something similar for the second case, esp. for $M$? I could use a similar update rule to calculate the covariance matrix, e.g. $$\Sigma \leftarrow (x - b) (x - b)^T \alpha + \Sigma (1- \alpha) $$ but I'm not sure how well that works in practice (also, I haven't really seen that formula in such form) or if that even converges (given that it uses also the moving $b$). Also, if I use this, I only have an estimation of $\operatorname{var}(X)$ but I still need the $M$. I probably could use another estimation to get the $M$ based on the $\Sigma$ (which one?), but I wonder if I could just get an estimation of $M$ more directly. (I asked the same question also on MetaOptimize .)","I have vectors $x \in \mathbb{R}^n$ and I expect some multivariate normal distribution . I want to normalize the vectors in such a way that $y = M (x - b)$ has mean zero ($\operatorname{E}[Y] = 0$) and variance 1 ($\operatorname{var}(Y) = 1_n$). For $n=1$, that is basically the Standard score . With $M = 1_n$ and $b = \operatorname{E}[X]$, I get it mean-normalized but not variance-normalized. With $M$ being a non-negative symmetric square root of $\operatorname{var}(X)^{-1}$, we get $\operatorname{var}(Y) = 1_n$. We can choose $b$ as in case 1. I want to estimate $M$ and $b$ based on some running/moving algorithm. For the first case, this is easy: For every new $x$, I can use the update rule $$b \leftarrow x \alpha + b (1 - \alpha)$$ for some $\alpha$. I could set $\alpha = 1/N$ and set $N \leftarrow N + 1$ in every iteration, starting with $N = 1$. How would I do something similar for the second case, esp. for $M$? I could use a similar update rule to calculate the covariance matrix, e.g. $$\Sigma \leftarrow (x - b) (x - b)^T \alpha + \Sigma (1- \alpha) $$ but I'm not sure how well that works in practice (also, I haven't really seen that formula in such form) or if that even converges (given that it uses also the moving $b$). Also, if I use this, I only have an estimation of $\operatorname{var}(X)$ but I still need the $M$. I probably could use another estimation to get the $M$ based on the $\Sigma$ (which one?), but I wonder if I could just get an estimation of $M$ more directly. (I asked the same question also on MetaOptimize .)",,"['probability', 'statistics', 'normal-distribution', 'covariance']"
9,Making a biased coin flip fair,Making a biased coin flip fair,,"I have a puzzle: Two groups want to break a tied vote using a    simple coin flip, however the only coin they have    available is a biased coin (i.e., one side will come    up more often than the other). To make matters    worse nobody in the room knows (or is willing to    admit) how the coin is biased. Assuming that the coin has two distinct sides,    design a method for using only this coin to    determine a fair outcome between the two parties. Initially, I thought, since neither party knows how the coin is biased, then either party could simply randomly guess which side the coin will land on and then they will receive a 50% chance they are correct (and 50% chance they are incorrect). This is assuming they are not influenced by any knowledge that they know for how the coin is biased when randomly deciding (or rather they do not know how it is biased). I even performed a simulation on this for a large amount of trials ( live demo in python ). However, I stumbled upon this wikipedia article , which mentions an alternative method. If a cheat has altered a coin to prefer one side over another (a biased coin), the coin can still be used for fair results by changing the game slightly. John von Neumann gave the following procedure: Toss the coin twice. If the results match, start over, forgetting both results. If the results differ, use the first result, forgetting the second. I'm just curious: is my method of solving the puzzle a viable solution? It seems so, but I'm not 100% certain.","I have a puzzle: Two groups want to break a tied vote using a    simple coin flip, however the only coin they have    available is a biased coin (i.e., one side will come    up more often than the other). To make matters    worse nobody in the room knows (or is willing to    admit) how the coin is biased. Assuming that the coin has two distinct sides,    design a method for using only this coin to    determine a fair outcome between the two parties. Initially, I thought, since neither party knows how the coin is biased, then either party could simply randomly guess which side the coin will land on and then they will receive a 50% chance they are correct (and 50% chance they are incorrect). This is assuming they are not influenced by any knowledge that they know for how the coin is biased when randomly deciding (or rather they do not know how it is biased). I even performed a simulation on this for a large amount of trials ( live demo in python ). However, I stumbled upon this wikipedia article , which mentions an alternative method. If a cheat has altered a coin to prefer one side over another (a biased coin), the coin can still be used for fair results by changing the game slightly. John von Neumann gave the following procedure: Toss the coin twice. If the results match, start over, forgetting both results. If the results differ, use the first result, forgetting the second. I'm just curious: is my method of solving the puzzle a viable solution? It seems so, but I'm not 100% certain.",,"['probability', 'statistics', 'puzzle']"
10,average number of rolls of a die between appearance of a side,average number of rolls of a die between appearance of a side,,"I saw this might have been duplicated in places here -- I think this might be a variation on the coupon collector problem -- but I wanted to be sure and understand how to do the calculation. I have an n-sided die. I want to know what the average number of rolls between the appearance of a number on the die, k is. I thought that the binomial distribution would be appropriate here. The way I originally approached it was to say that we have a 1/n chance of getting a number. The chance of getting any other number is (n-1)/n. I know that if I wanted to know the odds of getting the same number several times in a row is $\left(\frac{1}{n}\right)^m$ with m being the number of rolls.  But beyond that I was a bit stumped. I know that there's a binomial distribution or a Harmonic number involved somehow, and I read the coupon collector's problem but honestly that explanation seemed to make things less clear rather than more. Anyhow, if someone could point me to either a duplicate question or a better explanation that would be much appreciated.","I saw this might have been duplicated in places here -- I think this might be a variation on the coupon collector problem -- but I wanted to be sure and understand how to do the calculation. I have an n-sided die. I want to know what the average number of rolls between the appearance of a number on the die, k is. I thought that the binomial distribution would be appropriate here. The way I originally approached it was to say that we have a 1/n chance of getting a number. The chance of getting any other number is (n-1)/n. I know that if I wanted to know the odds of getting the same number several times in a row is $\left(\frac{1}{n}\right)^m$ with m being the number of rolls.  But beyond that I was a bit stumped. I know that there's a binomial distribution or a Harmonic number involved somehow, and I read the coupon collector's problem but honestly that explanation seemed to make things less clear rather than more. Anyhow, if someone could point me to either a duplicate question or a better explanation that would be much appreciated.",,"['probability', 'statistics']"
11,Average number of distinct values,Average number of distinct values,,Let $q$ such that $q < n$. I pick at random $n$ values in $\mathbb{F}_q$. What is the average number of distinct values ? Thank you,Let $q$ such that $q < n$. I pick at random $n$ values in $\mathbb{F}_q$. What is the average number of distinct values ? Thank you,,"['probability', 'statistics', 'finite-groups']"
12,Probability and Statistics random independent variables,Probability and Statistics random independent variables,,"I can't figure out how to determine if these variables are independent. Any help would be greatly appreciated. Random variables x and y are described by the PDF: $$f(x,y) = \begin{cases}   k,& \text{if } x + y ≤ 1, x > 0, \text{ and } y > 0,\\    0, & \text{otherwise.} \end{cases} $$ Are $x$ and $y$ random independent variables?","I can't figure out how to determine if these variables are independent. Any help would be greatly appreciated. Random variables x and y are described by the PDF: $$f(x,y) = \begin{cases}   k,& \text{if } x + y ≤ 1, x > 0, \text{ and } y > 0,\\    0, & \text{otherwise.} \end{cases} $$ Are $x$ and $y$ random independent variables?",,"['probability', 'statistics', 'random-variables']"
13,Distribution of ratio of uniform and exponential random variables,Distribution of ratio of uniform and exponential random variables,,"This is a homework question, I feel like I'm doing it right, but I can't seem to get the answer to match up.  I have a uniform RV from 2 to 4, and an exponential with mean 4, so $X \sim \text{UNI}(2,4)$, $Y \sim \text{EXP}(4)$. I'm looking for the density of $U=\dfrac{Y}{X}$.  So $f(x)=\dfrac{1}{2}$, $f(y)=\dfrac{e^{-y/4}}{4}$, and $f(x,y)=\dfrac{e^{-y/4}}{8}$. Now if I substitute $Y=Ux$, and multiply in a Jacobian ($\dfrac{\text{d}}{\text{d}u}ux=x$, this might be a mistake, I'm not completely clear on this), I can get $f(x,u)=\dfrac{xe^{-xu/4}}{8}$.  The marginal density $f(u)$ is found then by integrating out the $x$, which I can do by turning that joint density into a gamma function, where I need to get a $\dfrac{16}{u^2}$ on the bottom, so I end up with $\dfrac{2}{u^2}$ outside of the gamma integral, and after the gamma integrates to 1, f(u)=$\dfrac{2}{u^2}$.  Anyone see a mistake here?","This is a homework question, I feel like I'm doing it right, but I can't seem to get the answer to match up.  I have a uniform RV from 2 to 4, and an exponential with mean 4, so $X \sim \text{UNI}(2,4)$, $Y \sim \text{EXP}(4)$. I'm looking for the density of $U=\dfrac{Y}{X}$.  So $f(x)=\dfrac{1}{2}$, $f(y)=\dfrac{e^{-y/4}}{4}$, and $f(x,y)=\dfrac{e^{-y/4}}{8}$. Now if I substitute $Y=Ux$, and multiply in a Jacobian ($\dfrac{\text{d}}{\text{d}u}ux=x$, this might be a mistake, I'm not completely clear on this), I can get $f(x,u)=\dfrac{xe^{-xu/4}}{8}$.  The marginal density $f(u)$ is found then by integrating out the $x$, which I can do by turning that joint density into a gamma function, where I need to get a $\dfrac{16}{u^2}$ on the bottom, so I end up with $\dfrac{2}{u^2}$ outside of the gamma integral, and after the gamma integrates to 1, f(u)=$\dfrac{2}{u^2}$.  Anyone see a mistake here?",,"['statistics', 'uniform-distribution', 'ratio']"
14,"Proving negative natural log of Beta($\alpha$, 1) distribution is an exponential distribution","Proving negative natural log of Beta(, 1) distribution is an exponential distribution",\alpha,"I'm looking to prove that taking the negative natural logarithm of a Beta distribution with parameters $\alpha$ and $\beta=1$ is an exponential functions.  I've found two different proofs, both of which use a transformation formula.  However, that is not something I've learned yet.  Is there another way to show this?","I'm looking to prove that taking the negative natural logarithm of a Beta distribution with parameters $\alpha$ and $\beta=1$ is an exponential functions.  I've found two different proofs, both of which use a transformation formula.  However, that is not something I've learned yet.  Is there another way to show this?",,"['probability', 'statistics']"
15,How many times to roll a die before getting $n$ consecutive sixes given $m$ occurences?,How many times to roll a die before getting  consecutive sixes given  occurences?,n m,"Given a unbiased dice how to find the average number of rolls required to get $n$ consecutive sixes given already $m$ number of sixes occurred where $m\leq n$. I know how to solve using n consecutive sixes with out any occurrences from this link How many times to roll a die before getting two consecutive sixes? , can anyone help me finding out no of rolls if any occurrences occured before?","Given a unbiased dice how to find the average number of rolls required to get $n$ consecutive sixes given already $m$ number of sixes occurred where $m\leq n$. I know how to solve using n consecutive sixes with out any occurrences from this link How many times to roll a die before getting two consecutive sixes? , can anyone help me finding out no of rolls if any occurrences occured before?",,"['probability', 'statistics', 'dice']"
16,Expectation value with condition,Expectation value with condition,,how can i show that: $E[XY \vert X ] = XE[Y \vert X]$  for two random variables $X$ and $Y$ sorry this must be wrong what i meant was $E[ E[XY \vert X ] ]= E [XE[Y \vert X]]$,how can i show that: $E[XY \vert X ] = XE[Y \vert X]$  for two random variables $X$ and $Y$ sorry this must be wrong what i meant was $E[ E[XY \vert X ] ]= E [XE[Y \vert X]]$,,"['probability', 'statistics']"
17,How can the Wilson Confidence Interval be adapted for more than 2 outcomes?,How can the Wilson Confidence Interval be adapted for more than 2 outcomes?,,"In this link, the Wilson Score Interval is used to calculate the interval of a discrete distribution in which possible outcomes are 1 star , 2 stars , 3 stars , 4 stars and 5 stars (this is used to calculate a score for rating items/comments on a webpage). However, I was under the impression that the Wilson Score interval can only be used for a binomial distribution, so assuming that the link is computing the bounds correctly, how does it use WSI to get the values if the distribution isn't a binomial one? It indicates near the bottom of the page that the final score is 1 + 4 * (the result of the formula) ... can someone please explain why this will give the answer?","In this link, the Wilson Score Interval is used to calculate the interval of a discrete distribution in which possible outcomes are 1 star , 2 stars , 3 stars , 4 stars and 5 stars (this is used to calculate a score for rating items/comments on a webpage). However, I was under the impression that the Wilson Score interval can only be used for a binomial distribution, so assuming that the link is computing the bounds correctly, how does it use WSI to get the values if the distribution isn't a binomial one? It indicates near the bottom of the page that the final score is 1 + 4 * (the result of the formula) ... can someone please explain why this will give the answer?",,['statistics']
18,Player statistics as estimate of surreal number of game,Player statistics as estimate of surreal number of game,,"This is a rather complex question; it may require nontrivial assumptions about human cognition. But, I'm interested in getting mathematicians' perspective. With some finagling, you can associate many popular games, plus starting positions, with surreal numbers -- or more generally with games in Conway's sense. The number (or game) tells you which player has a winning strategy. Some of these games have online clients with tens of thousands of users, accumulating huge amounts of raw data. In particular, we can look at the win percentage for player one versus player two. That should give us some information about the surreal number or game (Conway) of the game. In the former case, perhaps an estimate or a bound. In the latter case, perhaps whether or not the game is comparable with zero, and if so where it falls. My question is: what kind of estimate can we get? How good would we expect it to be? And what other statistics might we consider looking at? I realize that in the case of e.g. standard chess, with a symmetric starting position, we can probably get a confidence estimate for whether the game is zero or incomparable in some obvious way. But I'm interested in the general case -- weird starting positions and so on. With advanced players this will lead to 100% win rates in all but the smallest (or incomparable) games, but we might still get some information about the larger games from beginner players, or more generally from statistics about win rate versus player rank.","This is a rather complex question; it may require nontrivial assumptions about human cognition. But, I'm interested in getting mathematicians' perspective. With some finagling, you can associate many popular games, plus starting positions, with surreal numbers -- or more generally with games in Conway's sense. The number (or game) tells you which player has a winning strategy. Some of these games have online clients with tens of thousands of users, accumulating huge amounts of raw data. In particular, we can look at the win percentage for player one versus player two. That should give us some information about the surreal number or game (Conway) of the game. In the former case, perhaps an estimate or a bound. In the latter case, perhaps whether or not the game is comparable with zero, and if so where it falls. My question is: what kind of estimate can we get? How good would we expect it to be? And what other statistics might we consider looking at? I realize that in the case of e.g. standard chess, with a symmetric starting position, we can probably get a confidence estimate for whether the game is zero or incomparable in some obvious way. But I'm interested in the general case -- weird starting positions and so on. With advanced players this will lead to 100% win rates in all but the smallest (or incomparable) games, but we might still get some information about the larger games from beginner players, or more generally from statistics about win rate versus player rank.",,"['statistics', 'combinatorial-game-theory']"
19,Differentiation for least squares method?,Differentiation for least squares method?,,Is there any reason that we use mathematical differentiation of least squares method for regression analysis? The theory say we use differentiation supposing the sum of errors is 0. I I don't really understand how differentiation can help in least squares method. Can somebody explain this?,Is there any reason that we use mathematical differentiation of least squares method for regression analysis? The theory say we use differentiation supposing the sum of errors is 0. I I don't really understand how differentiation can help in least squares method. Can somebody explain this?,,"['statistics', 'discrete-mathematics', 'statistical-inference']"
20,Proof using binomial coeff,Proof using binomial coeff,,I don't understand the step between left side and right side of my ? I,I don't understand the step between left side and right side of my ? I,,"['algebra-precalculus', 'statistics']"
21,Questions about Bayesian inference,Questions about Bayesian inference,,"From Wikipedia The prior distribution is the distribution of the parameter(s) before any data is observed, i.e. $p(\theta \mid \alpha )$. ... The sampling distribution is the distribution of the observed data conditional on its parameters, i.e. $p(\mathbf {X}\mid \theta )$ .   This is also termed the likelihood,... The marginal likelihood (sometimes also termed the evidence) is the distribution of the observed data marginalized over the   parameter(s), i.e. $$p(\mathbf {X}\mid \alpha )=\int_{\theta     }p(\mathbf {X}\mid \theta )p(\theta \mid \alpha )\operatorname     {d}\!\theta .$$ The posterior distribution is the distribution of the parameter(s) after taking into account the observed data. This is   determined by Bayes' rule, which forms the heart of Bayesian   inference: $$         p(\theta \mid \mathbf {X},\alpha )={\frac {p(\mathbf {X}\mid\theta )p(\theta \mid \alpha )}{p(\mathbf {X} \mid \alpha )}}\propto p(\mathbf     {X} \mid \theta )p(\theta \mid \alpha ) $$ In the calculation of the marginal likelihod and posterior distribution, I wonder what is the reason that $p(\mathbf     {X }\mid \theta )$ is not $p(\mathbf {X} \mid \theta, \alpha )$ instead? The posterior predictive distribution is the distribution of a new data point, marginalized over the posterior: $$         p(\tilde {x} \mid \mathbf {X},\alpha )=\int_{\theta}p(\tilde {x} \mid \theta )p(\theta \mid \mathbf {X},\alpha )\operatorname     {d}\!\theta $$ Why is $p(\tilde{x} \mid \theta )$ not $p(\tilde {x} \mid \theta, X,     \alpha )$ instead? Thanks!","From Wikipedia The prior distribution is the distribution of the parameter(s) before any data is observed, i.e. $p(\theta \mid \alpha )$. ... The sampling distribution is the distribution of the observed data conditional on its parameters, i.e. $p(\mathbf {X}\mid \theta )$ .   This is also termed the likelihood,... The marginal likelihood (sometimes also termed the evidence) is the distribution of the observed data marginalized over the   parameter(s), i.e. $$p(\mathbf {X}\mid \alpha )=\int_{\theta     }p(\mathbf {X}\mid \theta )p(\theta \mid \alpha )\operatorname     {d}\!\theta .$$ The posterior distribution is the distribution of the parameter(s) after taking into account the observed data. This is   determined by Bayes' rule, which forms the heart of Bayesian   inference: $$         p(\theta \mid \mathbf {X},\alpha )={\frac {p(\mathbf {X}\mid\theta )p(\theta \mid \alpha )}{p(\mathbf {X} \mid \alpha )}}\propto p(\mathbf     {X} \mid \theta )p(\theta \mid \alpha ) $$ In the calculation of the marginal likelihod and posterior distribution, I wonder what is the reason that $p(\mathbf     {X }\mid \theta )$ is not $p(\mathbf {X} \mid \theta, \alpha )$ instead? The posterior predictive distribution is the distribution of a new data point, marginalized over the posterior: $$         p(\tilde {x} \mid \mathbf {X},\alpha )=\int_{\theta}p(\tilde {x} \mid \theta )p(\theta \mid \mathbf {X},\alpha )\operatorname     {d}\!\theta $$ Why is $p(\tilde{x} \mid \theta )$ not $p(\tilde {x} \mid \theta, X,     \alpha )$ instead? Thanks!",,"['probability', 'statistics', 'statistical-inference', 'bayesian']"
22,Sampling distribution of the length of insects,Sampling distribution of the length of insects,,"The sampling distribution of length of insect is distributed with a mean of 12 cm , and a standard deviation of 3 cm . A. What is the probability of an insect being less than 11 cm long? B. What is the probability of an insect being between 10 cm and 12 cm long? C. What is the probability of an insect that is between  10 cm and 12 cm long being less * than 11 cm * long? I'm not sure if I am doing it the right way, but that's how I solved it: A. Pr(Y<11) Z= Y-mean / STD Z= 11-12 / 3 Z= -0.333... Area= 0.3707 (from the Z table) B. Pr(10>Y<12) Z= 100-100 / 15 Z= 0 Area= 0.500 (from the Z table) C. I'm not sure how to do part C of this problem. Is this just a tricky question and I    should find Pr(10>Y<11)?","The sampling distribution of length of insect is distributed with a mean of 12 cm , and a standard deviation of 3 cm . A. What is the probability of an insect being less than 11 cm long? B. What is the probability of an insect being between 10 cm and 12 cm long? C. What is the probability of an insect that is between  10 cm and 12 cm long being less * than 11 cm * long? I'm not sure if I am doing it the right way, but that's how I solved it: A. Pr(Y<11) Z= Y-mean / STD Z= 11-12 / 3 Z= -0.333... Area= 0.3707 (from the Z table) B. Pr(10>Y<12) Z= 100-100 / 15 Z= 0 Area= 0.500 (from the Z table) C. I'm not sure how to do part C of this problem. Is this just a tricky question and I    should find Pr(10>Y<11)?",,"['probability', 'statistics']"
23,Show that the co-variance is zero between $\bar{Y}$ and $Y_i - \bar{Y}$,Show that the co-variance is zero between  and,\bar{Y} Y_i - \bar{Y},"I have a quick question about how to mathematically show this result. Here is the question and my thoughts. Let $Y_1, \ldots, Y_n$ be i.i.d. R.Vs with mean $\mu$ and variance $\sigma^2$ and let $\bar{Y}$ be the mean of those R.Vs.  Show that the co variance is zero between $\bar{Y}$ and $Y_i - \bar{Y}$. Since we are told that $Y_1, \ldots, Y_n$ are i.i.d. R.V.s then we also know that $\bar{Y}$ also has the same distribution but is i.i.d correct? Would that not imply that the co-variance is zero between $\bar{Y}$ and $Y_i - \bar{Y}$ since this is simply a linear combination of R.V.s that are already independent? I am confused about how to mathematically show this however and help would be greatly appreciated! Thanks :) EDIT: While doing the problem, another question arose. It appears that $E(\bar{Y})$ = $\bar{Y}$ but how can that be if $\bar{Y}$ = $ (1\div n)$ $\sum_{i=1}^n Y_i$ ? If we take the expectation of this second value don't we get $\mu$? But how could $\mu$ = $\bar{Y}$ ? EDIT #2: So I have been solving the problem with various updates and here is where I am stuck now: We start here: $$\text{cov} (\bar{Y}, Y_i - \bar{Y})$ = $E[(\bar{Y}-E(\bar{Y})) * (Y_i-\bar{Y}-E(Y_i-\bar{Y}))]$$ so simplifying this expression according to the fact that $E(\bar{Y})$ = $\mu$ gives the following: $$\bar{Y}\mu - \bar{Y^2} - \mu^2 + \mu\bar{Y}$$  I am confused whether or not this equals zero. I would also greatly appreciate an explanation of how $\mu$ and $\bar{Y}$ relate in practical purposes. Thanks so much!","I have a quick question about how to mathematically show this result. Here is the question and my thoughts. Let $Y_1, \ldots, Y_n$ be i.i.d. R.Vs with mean $\mu$ and variance $\sigma^2$ and let $\bar{Y}$ be the mean of those R.Vs.  Show that the co variance is zero between $\bar{Y}$ and $Y_i - \bar{Y}$. Since we are told that $Y_1, \ldots, Y_n$ are i.i.d. R.V.s then we also know that $\bar{Y}$ also has the same distribution but is i.i.d correct? Would that not imply that the co-variance is zero between $\bar{Y}$ and $Y_i - \bar{Y}$ since this is simply a linear combination of R.V.s that are already independent? I am confused about how to mathematically show this however and help would be greatly appreciated! Thanks :) EDIT: While doing the problem, another question arose. It appears that $E(\bar{Y})$ = $\bar{Y}$ but how can that be if $\bar{Y}$ = $ (1\div n)$ $\sum_{i=1}^n Y_i$ ? If we take the expectation of this second value don't we get $\mu$? But how could $\mu$ = $\bar{Y}$ ? EDIT #2: So I have been solving the problem with various updates and here is where I am stuck now: We start here: $$\text{cov} (\bar{Y}, Y_i - \bar{Y})$ = $E[(\bar{Y}-E(\bar{Y})) * (Y_i-\bar{Y}-E(Y_i-\bar{Y}))]$$ so simplifying this expression according to the fact that $E(\bar{Y})$ = $\mu$ gives the following: $$\bar{Y}\mu - \bar{Y^2} - \mu^2 + \mu\bar{Y}$$  I am confused whether or not this equals zero. I would also greatly appreciate an explanation of how $\mu$ and $\bar{Y}$ relate in practical purposes. Thanks so much!",,"['statistics', 'statistical-inference']"
24,How to calculate projected value?,How to calculate projected value?,,"I am trying to figure out how to calculate a projected value for our goals. Each of our sales persons is assigned an annual goal for 2014, say 60 units. Let's say, as of today, one sales person has sold 5 units. How would I calculate, assuming everything trends the way it currently is, their projected actual value for year end? I'd like to show them how they are trending.","I am trying to figure out how to calculate a projected value for our goals. Each of our sales persons is assigned an annual goal for 2014, say 60 units. Let's say, as of today, one sales person has sold 5 units. How would I calculate, assuming everything trends the way it currently is, their projected actual value for year end? I'd like to show them how they are trending.",,['statistics']
25,I am going to learn these Mathematics Topics. I need advice and suggestions please .,I am going to learn these Mathematics Topics. I need advice and suggestions please .,,"I am really horrible when it comes to maths since I never had any maths background in my High school. I am fairly good at programming ( C++ and Java) but without mathematics I cant advance in any programming language. Since I love programming and computers I took a computer course in college and in my next semester i have these topics to learn , they are : - Unit 1  1. Set theory 2. Relation and Functions. Unit 2 1. Determinants 2. Matrices Unit 3 1.Differentiation 2.Integration Unit 4 1.Complex Numbers 2.Statistics Now the problem is , let me be honest here , I have no Idea of anything i wrote above. I just copied my Maths syllabus here but my professor says that I can do good if I start learning from now. Please suggest me what are the basic things I should know to start learning all these and is it possible for me to crack the examination ( which is after 6 months) if I learn all these. Most importantly , is it possible to learn all these for someone who never had maths in his high school. I learned maths till 10th Std. I attached a photo showing the details. If you have time , please read it as well. Thanks a lot for your time.","I am really horrible when it comes to maths since I never had any maths background in my High school. I am fairly good at programming ( C++ and Java) but without mathematics I cant advance in any programming language. Since I love programming and computers I took a computer course in college and in my next semester i have these topics to learn , they are : - Unit 1  1. Set theory 2. Relation and Functions. Unit 2 1. Determinants 2. Matrices Unit 3 1.Differentiation 2.Integration Unit 4 1.Complex Numbers 2.Statistics Now the problem is , let me be honest here , I have no Idea of anything i wrote above. I just copied my Maths syllabus here but my professor says that I can do good if I start learning from now. Please suggest me what are the basic things I should know to start learning all these and is it possible for me to crack the examination ( which is after 6 months) if I learn all these. Most importantly , is it possible to learn all these for someone who never had maths in his high school. I learned maths till 10th Std. I attached a photo showing the details. If you have time , please read it as well. Thanks a lot for your time.",,"['integration', 'matrices', 'statistics', 'soft-question']"
26,a probability question related to computing the variance of a specific pattern,a probability question related to computing the variance of a specific pattern,,"With respect to a given sequence of points $\{X_1, ... X_t, ...X_n\}$. I can understand why $E[S]= \frac{n-1}{2}$. But how to get that $Var[S]$.","With respect to a given sequence of points $\{X_1, ... X_t, ...X_n\}$. I can understand why $E[S]= \frac{n-1}{2}$. But how to get that $Var[S]$.",,"['probability', 'combinatorics', 'statistics', 'stochastic-processes']"
27,How to derive thresholds from a pooled sample of values,How to derive thresholds from a pooled sample of values,,"Question: The context of this question is actually finance, however the question itself is a statistical issue. Suppose I have the following expression: $$\rho = \frac{2\bar{x}}{(s^*_x)^2}+1  \ \ \ \ \ ... (1)$$ where $\bar{x} = \frac{1}{T} \sum_{t=1}^{T} x_t$ $(s^*_x)^2 = \frac{1}{T} \sum_{t=1}^T (x_t - \bar{x})^2$ Assume $T$ is some fixed constant. Note: $T$ is just the total number of observations of $x_t$. I have data on $x_t$ for each 'entity' (here an entity just simply refers to a firm/company). In total, I have 2228 entities and for each entity I have $T$ observations of $x_t$. Note there are no distributional assumptions on $x_t$, so one can make any reasonable assumptions in order to solve the problem that I mention further below. For each entity, I substitute the $T$ observations of $x_t$ into Eqn. $(1)$ and obtain a value for $\rho$. Thus in total, I have 2228 values of $\rho$. Now, a large value of $\rho$ means the entity is ""bad"" and a small value of $\rho$ means the entity is ""good"". However, the problem is how large does a value of $\rho$ have to be in order to classify an entity as ""bad""? That is, what is the threshold such that if $\rho$ exceeds the threshold value, then we can classify the entity as ""bad""? For example, let's say the threshold is $400$, if entity A had a $\rho = 300$ while entity B had a $\rho = 1000$, then entity A is ""good"" while entity B is ""bad"". My attempts so far: My first attempt was to try get data on an entity that is known to be ""bad"" and then calculate its $\rho$ and use this value as the threshold. The problem is that I cannot obtain data on ""bad"" entities. For my next attempt, I obtained the empirical distribution by applying a kernel density estimator on the 2228 values of $\rho$. Then I calculate the 99th percentile (for robustness, I also calculated the 97.5th and 95th percentile) of this pooled distribution and use this value as the threshold. However, the main critique is that this is too arbitrary and there is not enough rationale for using this method. Queries: So I am wondering if anyone has any ideas on how what statistical/mathematical techniques/methods I can apply to derive appropriate thresholds for $\rho$. Currently, I really have no idea on what tools are available for this problem. EDIT 1 This edit is in response to some of the comments below. I am ready (and quite confidently) to assume $\rho$ measures the ""goodness"" of a company. Actually I am dealing within the hedge funds industry and it is a very opaque industry. It can be shown through an arbitrage argument that $\rho$ outperforms many other standard measures of performance such as the Sharpe ratio, Sortino ratio, Jensen's alpha etc. In this sense, $\rho$ doesn't necessarily play a causal role, i.e., it is not actually concerned with deciding which hedge funds are ""bad"" or ""good"", it is in fact a measure of performance that is shown to be highly resistant to hedge fund's manipulation of their returns. It can be shown that if $\rho$ is high, then it most likely means the fund is manipulating their returns, hence the label ""bad"", whereas if $\rho$ is low, then it most likely means the fund is not manipulating their returns, hence the label ""good"". As a consequence, I do not actually know which hedge funds are actually ""good"" or ""bad"" in my sample. I simply know the $\rho$ values of each fund, hence the need for some cut-off point that determines whether the $\rho$ values are too big 'relative' to the rest of the sample. I am not sure if the following information will help, but it can be shown (through the use of Central Limit Theorem/Slutsky's Theorem) that if we assume $x_t$ to be a weakly stationary process with mean $\mu_x$ and variance $\sigma_x^2$, then the asymptotic distribution of $\rho$ for a single fund is given by: \begin{align*} \rho \stackrel{asymp}{\sim} N\left(1+ \frac {2\mu_x}{\sigma^2_x}, \frac 4{\sigma^2_xT}\right) \end{align*} However, this is only the $\rho$ value for one particular fund, I am not sure how it is of any use to determining some sort of threshold for the entire sample. Is there some sort of statistical framework/technique which we can apply here to deduce some sort of threshold? Additional assumptions are not a problem, I am just curious if we can get some sort of threshold first. Thank you all for the inputs so far, I highly appreciate them. EDIT 2: In response to Eupraxis1981 I ran MLE on the original log-likelihood (before the edit) which was $$\max\limits_{\mu,\sigma^2}\mathcal{L}(\hat{\dot\rho},s^{*2};\mu,\sigma^2) = -\frac{N\ln(2\pi)}{2}-\sum\limits_{i=1}^N \{\frac{1}{2}\ln(\sigma^2+\frac{4}{s^{*2}_iT})+\frac{1}{2(\sigma^2+\frac{4}{s^{*2}_iT})} (\hat{\dot\rho}_i-\mu)^2\}$$ and got some very sensible results for the estimates of $\mu$ and $\sigma^2$. However, I am not quite sure how you arrived at the modified log likelihood currently shown, could you outline how you derived it? More specifically, how did $\dot\rho_i$ get introduced into the log-likehood? Also, what values do I use for $\dot\rho_i$? For $\hat{\dot\rho} = \{\hat{\dot\rho}_1,\hat{\dot\rho}_2....\hat{\dot\rho}_N\}$, I use the values derived from equation (1), that is, for each investment, I calculate $\displaystyle \frac{2\bar{x}}{(s^*_x)^2}+1$ using the returns of that particular investment. But what would I use for $\dot\rho_i$?","Question: The context of this question is actually finance, however the question itself is a statistical issue. Suppose I have the following expression: $$\rho = \frac{2\bar{x}}{(s^*_x)^2}+1  \ \ \ \ \ ... (1)$$ where $\bar{x} = \frac{1}{T} \sum_{t=1}^{T} x_t$ $(s^*_x)^2 = \frac{1}{T} \sum_{t=1}^T (x_t - \bar{x})^2$ Assume $T$ is some fixed constant. Note: $T$ is just the total number of observations of $x_t$. I have data on $x_t$ for each 'entity' (here an entity just simply refers to a firm/company). In total, I have 2228 entities and for each entity I have $T$ observations of $x_t$. Note there are no distributional assumptions on $x_t$, so one can make any reasonable assumptions in order to solve the problem that I mention further below. For each entity, I substitute the $T$ observations of $x_t$ into Eqn. $(1)$ and obtain a value for $\rho$. Thus in total, I have 2228 values of $\rho$. Now, a large value of $\rho$ means the entity is ""bad"" and a small value of $\rho$ means the entity is ""good"". However, the problem is how large does a value of $\rho$ have to be in order to classify an entity as ""bad""? That is, what is the threshold such that if $\rho$ exceeds the threshold value, then we can classify the entity as ""bad""? For example, let's say the threshold is $400$, if entity A had a $\rho = 300$ while entity B had a $\rho = 1000$, then entity A is ""good"" while entity B is ""bad"". My attempts so far: My first attempt was to try get data on an entity that is known to be ""bad"" and then calculate its $\rho$ and use this value as the threshold. The problem is that I cannot obtain data on ""bad"" entities. For my next attempt, I obtained the empirical distribution by applying a kernel density estimator on the 2228 values of $\rho$. Then I calculate the 99th percentile (for robustness, I also calculated the 97.5th and 95th percentile) of this pooled distribution and use this value as the threshold. However, the main critique is that this is too arbitrary and there is not enough rationale for using this method. Queries: So I am wondering if anyone has any ideas on how what statistical/mathematical techniques/methods I can apply to derive appropriate thresholds for $\rho$. Currently, I really have no idea on what tools are available for this problem. EDIT 1 This edit is in response to some of the comments below. I am ready (and quite confidently) to assume $\rho$ measures the ""goodness"" of a company. Actually I am dealing within the hedge funds industry and it is a very opaque industry. It can be shown through an arbitrage argument that $\rho$ outperforms many other standard measures of performance such as the Sharpe ratio, Sortino ratio, Jensen's alpha etc. In this sense, $\rho$ doesn't necessarily play a causal role, i.e., it is not actually concerned with deciding which hedge funds are ""bad"" or ""good"", it is in fact a measure of performance that is shown to be highly resistant to hedge fund's manipulation of their returns. It can be shown that if $\rho$ is high, then it most likely means the fund is manipulating their returns, hence the label ""bad"", whereas if $\rho$ is low, then it most likely means the fund is not manipulating their returns, hence the label ""good"". As a consequence, I do not actually know which hedge funds are actually ""good"" or ""bad"" in my sample. I simply know the $\rho$ values of each fund, hence the need for some cut-off point that determines whether the $\rho$ values are too big 'relative' to the rest of the sample. I am not sure if the following information will help, but it can be shown (through the use of Central Limit Theorem/Slutsky's Theorem) that if we assume $x_t$ to be a weakly stationary process with mean $\mu_x$ and variance $\sigma_x^2$, then the asymptotic distribution of $\rho$ for a single fund is given by: \begin{align*} \rho \stackrel{asymp}{\sim} N\left(1+ \frac {2\mu_x}{\sigma^2_x}, \frac 4{\sigma^2_xT}\right) \end{align*} However, this is only the $\rho$ value for one particular fund, I am not sure how it is of any use to determining some sort of threshold for the entire sample. Is there some sort of statistical framework/technique which we can apply here to deduce some sort of threshold? Additional assumptions are not a problem, I am just curious if we can get some sort of threshold first. Thank you all for the inputs so far, I highly appreciate them. EDIT 2: In response to Eupraxis1981 I ran MLE on the original log-likelihood (before the edit) which was $$\max\limits_{\mu,\sigma^2}\mathcal{L}(\hat{\dot\rho},s^{*2};\mu,\sigma^2) = -\frac{N\ln(2\pi)}{2}-\sum\limits_{i=1}^N \{\frac{1}{2}\ln(\sigma^2+\frac{4}{s^{*2}_iT})+\frac{1}{2(\sigma^2+\frac{4}{s^{*2}_iT})} (\hat{\dot\rho}_i-\mu)^2\}$$ and got some very sensible results for the estimates of $\mu$ and $\sigma^2$. However, I am not quite sure how you arrived at the modified log likelihood currently shown, could you outline how you derived it? More specifically, how did $\dot\rho_i$ get introduced into the log-likehood? Also, what values do I use for $\dot\rho_i$? For $\hat{\dot\rho} = \{\hat{\dot\rho}_1,\hat{\dot\rho}_2....\hat{\dot\rho}_N\}$, I use the values derived from equation (1), that is, for each investment, I calculate $\displaystyle \frac{2\bar{x}}{(s^*_x)^2}+1$ using the returns of that particular investment. But what would I use for $\dot\rho_i$?",,"['statistics', 'mathematical-modeling']"
28,finding $E[X]$ of a exponential distribution with a deductable,finding  of a exponential distribution with a deductable,E[X],"Losses follow an exponential distribution with parameter $\theta$. For a deductible of 100, the expected payment per loss is 2000. What is the expected payment per loss for a deductible of 500? They get  $2,000e^{\frac{-400}{\theta}}$ but I'm not sure how they got this. I know I need to solve for $\lambda$ so what I did was I transformed the parameter $\theta$ to fit the deductible by subtracting 100. Therefore $Y=\theta-100$. Now to find $E[Y]= \displaystyle\int_{100}^{\infty} \lambda e^{-\lambda \theta +100\lambda}=2000$ but the solution doesn't exist. Where did I go wrong? Following with what Gaffney said, I integrated $\displaystyle\int_{100}^{\infty}(\theta - 100)\lambda e^{-\theta \lambda}$ which I get $\dfrac{-e^{-100\lambda}}{\lambda}=2000$ (since the $-\theta e^{-\theta \lambda} + 100e^{-\theta \lambda}$ will cancel out with 100) substituting 500 gives us $-400e^{-500\lambda} - \dfrac{e^{-500\lambda}}{\lambda}= ?$ which we are trying to figure out but I do not know how to make the connection of the 1st result with the 2nd.","Losses follow an exponential distribution with parameter $\theta$. For a deductible of 100, the expected payment per loss is 2000. What is the expected payment per loss for a deductible of 500? They get  $2,000e^{\frac{-400}{\theta}}$ but I'm not sure how they got this. I know I need to solve for $\lambda$ so what I did was I transformed the parameter $\theta$ to fit the deductible by subtracting 100. Therefore $Y=\theta-100$. Now to find $E[Y]= \displaystyle\int_{100}^{\infty} \lambda e^{-\lambda \theta +100\lambda}=2000$ but the solution doesn't exist. Where did I go wrong? Following with what Gaffney said, I integrated $\displaystyle\int_{100}^{\infty}(\theta - 100)\lambda e^{-\theta \lambda}$ which I get $\dfrac{-e^{-100\lambda}}{\lambda}=2000$ (since the $-\theta e^{-\theta \lambda} + 100e^{-\theta \lambda}$ will cancel out with 100) substituting 500 gives us $-400e^{-500\lambda} - \dfrac{e^{-500\lambda}}{\lambda}= ?$ which we are trying to figure out but I do not know how to make the connection of the 1st result with the 2nd.",,"['probability', 'statistics']"
29,Expectation of function of two random variables,Expectation of function of two random variables,,"I have the random variables X and Y, with joint density function $f(x,y)$ over the plane $-\infty < x < \infty$ and $-\infty < y < \infty$. I am trying to find the expectation of $(X-Y)^2$. Here is my strategy: 1) Let $Z=X-Y$ 2) Find the cumulative distribution function of Z, $F_Z(z)$, by integrating over the joint density function of X and Y, using the parameters $-\infty<x<z+y$, and $-\infty<y<\infty$. 3) Differentiate $F_z(z)$ to get the probability density function of z, $f_z(z)$ 4) Find the expected value of $Z^2$ I think that this method should work, but I was wondering if there is a simpler approach to solve it that doesn't use so many steps.","I have the random variables X and Y, with joint density function $f(x,y)$ over the plane $-\infty < x < \infty$ and $-\infty < y < \infty$. I am trying to find the expectation of $(X-Y)^2$. Here is my strategy: 1) Let $Z=X-Y$ 2) Find the cumulative distribution function of Z, $F_Z(z)$, by integrating over the joint density function of X and Y, using the parameters $-\infty<x<z+y$, and $-\infty<y<\infty$. 3) Differentiate $F_z(z)$ to get the probability density function of z, $f_z(z)$ 4) Find the expected value of $Z^2$ I think that this method should work, but I was wondering if there is a simpler approach to solve it that doesn't use so many steps.",,"['probability', 'statistics', 'functions']"
30,Find E{1/x} if we are given a density function with continuos random variable,Find E{1/x} if we are given a density function with continuos random variable,,Let X be a continuous random variable with density function $$f(x) = \begin{cases}\frac{x}{30}(1+3x) & 1 < x < 3 \\0 & \text{otherwise}\end{cases}$$ Find $E\left(\frac1x\right)$,Let X be a continuous random variable with density function $$f(x) = \begin{cases}\frac{x}{30}(1+3x) & 1 < x < 3 \\0 & \text{otherwise}\end{cases}$$ Find $E\left(\frac1x\right)$,,['probability']
31,Derive asymptotic behavior of inverse of the normal cdf with respect to 2^n,Derive asymptotic behavior of inverse of the normal cdf with respect to 2^n,,I have a normal distribution $\mu = 0$ and $\sigma = 0.58n$ where $n > 0 $ and I am trying to derive the asymptotic behavior of the following equation: $$\Phi\left(\frac{x}{0.58n}\right)\;=\;2^{1-n}$$ Follows: $$ \DeclareMathOperator\inverfc{inverfc} x = -0.58 \sqrt{2}n \inverfc{({2^{2 - n}})} $$ So I want to find $O( n\inverfc{({2^{2 - n}})}) $. More specifically I want to confirm my suspicion that it is in $O(n \log{n})$. However since Inverf is a special function I can't wrap my mind around to analyse it. I gave complete context since another derivation might be more helpful here.,I have a normal distribution $\mu = 0$ and $\sigma = 0.58n$ where $n > 0 $ and I am trying to derive the asymptotic behavior of the following equation: $$\Phi\left(\frac{x}{0.58n}\right)\;=\;2^{1-n}$$ Follows: $$ \DeclareMathOperator\inverfc{inverfc} x = -0.58 \sqrt{2}n \inverfc{({2^{2 - n}})} $$ So I want to find $O( n\inverfc{({2^{2 - n}})}) $. More specifically I want to confirm my suspicion that it is in $O(n \log{n})$. However since Inverf is a special function I can't wrap my mind around to analyse it. I gave complete context since another derivation might be more helpful here.,,"['real-analysis', 'analysis', 'statistics', 'asymptotics']"
32,Variance for mixed distribution (continuous + discrete),Variance for mixed distribution (continuous + discrete),,"A random variable X has the cumulative distribution function: $F(x)= \left\{  \begin{array}{l} 0 \text{ for x < 1}\\  \cfrac{x^2-2x+2}{2} \text{ for } 1 \le x <2\\  1 \text{ for } x \ge 2 \end{array} \right.  $ Calculate the variance of x. I know the definition of variance is:  $Var[X]=E[X^2]-E[X]^2$ so we proceed by calculating each term. $E[X]=\int\limits_{-\infty}^{\infty}xf(x)dx\tag{1}$ $E[X^2]=\int\limits_{-\infty}^{\infty}x^2f(x)dx\tag{2}$ So to evaluate equations (1) and (2), we need to calculate the PDF.  This is where I am confused.  I know the PDF is the derivative of the CDF.  Also, we note that $F(2)-F(1) = 1/2$ and by definition $F(\infty)=1$. This forces the probability for $x<1$ to be 1/2.  Is this correct?  How would we write the system of equations for the PDF below?  I don't really see how this is a combination of discrete and continuous if the limits are $(x<1), 1 \le x < 2, x \ge 2$.  Can someone please explain this and breakdown $E[X],E[X^2]$? $f(x)= \left\{  \begin{array}{l} 0 \text{ for x < 1}\\  (x-1) \text{ for } 1 \le x <2\\  0 \text{ for } x \ge 2 \end{array} \right.  $ Any help is appreciated.  Thank you!","A random variable X has the cumulative distribution function: $F(x)= \left\{  \begin{array}{l} 0 \text{ for x < 1}\\  \cfrac{x^2-2x+2}{2} \text{ for } 1 \le x <2\\  1 \text{ for } x \ge 2 \end{array} \right.  $ Calculate the variance of x. I know the definition of variance is:  $Var[X]=E[X^2]-E[X]^2$ so we proceed by calculating each term. $E[X]=\int\limits_{-\infty}^{\infty}xf(x)dx\tag{1}$ $E[X^2]=\int\limits_{-\infty}^{\infty}x^2f(x)dx\tag{2}$ So to evaluate equations (1) and (2), we need to calculate the PDF.  This is where I am confused.  I know the PDF is the derivative of the CDF.  Also, we note that $F(2)-F(1) = 1/2$ and by definition $F(\infty)=1$. This forces the probability for $x<1$ to be 1/2.  Is this correct?  How would we write the system of equations for the PDF below?  I don't really see how this is a combination of discrete and continuous if the limits are $(x<1), 1 \le x < 2, x \ge 2$.  Can someone please explain this and breakdown $E[X],E[X^2]$? $f(x)= \left\{  \begin{array}{l} 0 \text{ for x < 1}\\  (x-1) \text{ for } 1 \le x <2\\  0 \text{ for } x \ge 2 \end{array} \right.  $ Any help is appreciated.  Thank you!",,"['probability', 'statistics']"
33,"Autocorrelation function, cumulants and probability distribution","Autocorrelation function, cumulants and probability distribution",,"I have a doubt. Is it possible to get the cumulants of a probability distribution from the autocorrelation function? or the probability distribution?. For example, the variance (the second cumulant) corresponds to the autocorrelation function at $\tau =0$. Thank you so much","I have a doubt. Is it possible to get the cumulants of a probability distribution from the autocorrelation function? or the probability distribution?. For example, the variance (the second cumulant) corresponds to the autocorrelation function at $\tau =0$. Thank you so much",,"['probability', 'statistics', 'probability-distributions']"
34,Balls and boxes probability problem,Balls and boxes probability problem,,"Here is another question from the book of V. Rohatgi and A. Saleh. I would like to ask help again. Here it goes: Let A, B, and C be three boxes with three, four, and five cells, respectively. There are three yellow balls numbered 1 to 3, four green balls numbered 1 to 4, and five red balls numbered 1 to 5. The yellow balls are placed at random in box A, the green in B, and the red in C, with no cell receiving more than one ball. Find the probability that only one of the boxes will show no matches. My question is more on how to interpret the problem. I actually cannot understand what is being asked and how was the experiment performed. Can anyone help me please? Also, if you have an answer, please explain the solution as well. Thanks","Here is another question from the book of V. Rohatgi and A. Saleh. I would like to ask help again. Here it goes: Let A, B, and C be three boxes with three, four, and five cells, respectively. There are three yellow balls numbered 1 to 3, four green balls numbered 1 to 4, and five red balls numbered 1 to 5. The yellow balls are placed at random in box A, the green in B, and the red in C, with no cell receiving more than one ball. Find the probability that only one of the boxes will show no matches. My question is more on how to interpret the problem. I actually cannot understand what is being asked and how was the experiment performed. Can anyone help me please? Also, if you have an answer, please explain the solution as well. Thanks",,"['probability', 'statistics']"
35,a distribution of a sqrt of a Normal distribution,a distribution of a sqrt of a Normal distribution,,"i have a Normal(0,1)=X.  and (X_{1},....X_{20}). I have to calculate the distribution of $T=\sqrt{|Z|}$ with Z= $\dfrac{1}{20} \sum_{1}^{20}X_{i}$ and his average.  I have done this, but Im not very sure:\ Z= $\dfrac{1}{n} \sum_{1}^{20}X_{i} =N(0,1)$\ $P(T=x)=P(\sqrt{|Z|}=x)=P(Z=x^2)+P(z=-x^2)$ =$2f(x^2)=\dfrac{2}{\sqrt{2\pi}}exp({\dfrac{-x^{4}}{2}})$ $x\geq 0$\ it looks a bit strange no?? thanks!","i have a Normal(0,1)=X.  and (X_{1},....X_{20}). I have to calculate the distribution of $T=\sqrt{|Z|}$ with Z= $\dfrac{1}{20} \sum_{1}^{20}X_{i}$ and his average.  I have done this, but Im not very sure:\ Z= $\dfrac{1}{n} \sum_{1}^{20}X_{i} =N(0,1)$\ $P(T=x)=P(\sqrt{|Z|}=x)=P(Z=x^2)+P(z=-x^2)$ =$2f(x^2)=\dfrac{2}{\sqrt{2\pi}}exp({\dfrac{-x^{4}}{2}})$ $x\geq 0$\ it looks a bit strange no?? thanks!",,"['probability', 'statistics', 'probability-distributions', 'statistical-inference']"
36,How can I show that sample mean has the smallest variance?,How can I show that sample mean has the smallest variance?,,"Let the population distribution is $N(\mu,1)$. Sample mean: $\bar{X_n}=\frac{\sum_{i=1}^{n} X_i}{n}$ Then $E(\bar{X_n})=\mu$ and $V(\bar{X_n})=\frac{1}{n}$ It is an unbiased estimator, and as $n \rightarrow \infty$ it converges to $\mu$. But I want to show that it is the minimun variance estimator, for example, take another unbiased estimator $Y_n$ and show $V(\bar{X_n}) \le V(Y_n)$.","Let the population distribution is $N(\mu,1)$. Sample mean: $\bar{X_n}=\frac{\sum_{i=1}^{n} X_i}{n}$ Then $E(\bar{X_n})=\mu$ and $V(\bar{X_n})=\frac{1}{n}$ It is an unbiased estimator, and as $n \rightarrow \infty$ it converges to $\mu$. But I want to show that it is the minimun variance estimator, for example, take another unbiased estimator $Y_n$ and show $V(\bar{X_n}) \le V(Y_n)$.",,"['statistics', 'estimation']"
37,Unordered outcomes (counting),Unordered outcomes (counting),,"Suppose a social network has 20 users. (a) How many pairs of users exist? (b) In how many states can be the social network be assuming every pair of users can be in two states: friendship or no friendship. For a, it's just 20 choose 2, which 190. But I'm at a loss for how to approach the second question and I don't think it's as simple as $190 \times 2$. 380 would be the number of possible pairs of the 20 users assuming each pair can be labeled as either being friends or not friends. Am I totally overthinking this problem? The system is exactly 10 pairs which each pair either being friends or not friends. So it boils down to how many possible ways to choose 10 pairs of  the 190, without repeating any users...","Suppose a social network has 20 users. (a) How many pairs of users exist? (b) In how many states can be the social network be assuming every pair of users can be in two states: friendship or no friendship. For a, it's just 20 choose 2, which 190. But I'm at a loss for how to approach the second question and I don't think it's as simple as $190 \times 2$. 380 would be the number of possible pairs of the 20 users assuming each pair can be labeled as either being friends or not friends. Am I totally overthinking this problem? The system is exactly 10 pairs which each pair either being friends or not friends. So it boils down to how many possible ways to choose 10 pairs of  the 190, without repeating any users...",,"['combinatorics', 'statistics']"
38,How to prove MLE of variance is biased,How to prove MLE of variance is biased,,"How to show the $$ E(\Sigma_{MLE})=E\left(\dfrac{1}{n}\sum_{i=1}^{n}({x}_{i}-{\mu}_{MLE})({x}_{i}-\mathbf{{\mathbf{\mu}}}_{MLE})'\right)=\dfrac{n-1}{n}\Sigma $$ Where ${\mu}_{MLE}=\dfrac{1}{n}\sum_{i=1}^{n}{x}_{i}$   and ${x}_{i}\sim N(\mu,\Sigma)$   and $i=1,\ldots,n$ Here is what I've tried: $$ E(\Sigma_{MLE})=E\left(\dfrac{1}{n}\sum_{i=1}^{n}({x}_{i}-{\mu}_{MLE})({x}_{i}-\mathbf{{\mathbf{\mu}}}_{MLE})'\right)=\dfrac{1}{n}E\left\{\sum_{i=1}^{n}(x_{i}x_{i}'-x_{i}\mu_{MLE}'-\mu_{MLE}x_{i}'-\mu_{MLE}\mu_{MLE}')\right\} $$ $$ =\dfrac{1}{n}\sum_{i=1}^{n}E(x_{i}x_{i}'-2x_{i}'\mu_{MLE}-\mu_{MLE}\mu_{MLE}') $$ $$ =\dfrac{1}{n}\sum_{i=1}^n \{E(x_{i}x_{i}')-2E(x_{i}'\mu_{MLE})-E(\mu_{MLE}\mu_{MLE}')\}   $$ I think $E(x_{i}x_{i}')=\mu\mu'$   but don't know what is the $E(x_{i}'\mu_{MLE})$   and $E(\mu_{MLE}\mu_{MLE}')$","How to show the $$ E(\Sigma_{MLE})=E\left(\dfrac{1}{n}\sum_{i=1}^{n}({x}_{i}-{\mu}_{MLE})({x}_{i}-\mathbf{{\mathbf{\mu}}}_{MLE})'\right)=\dfrac{n-1}{n}\Sigma $$ Where ${\mu}_{MLE}=\dfrac{1}{n}\sum_{i=1}^{n}{x}_{i}$   and ${x}_{i}\sim N(\mu,\Sigma)$   and $i=1,\ldots,n$ Here is what I've tried: $$ E(\Sigma_{MLE})=E\left(\dfrac{1}{n}\sum_{i=1}^{n}({x}_{i}-{\mu}_{MLE})({x}_{i}-\mathbf{{\mathbf{\mu}}}_{MLE})'\right)=\dfrac{1}{n}E\left\{\sum_{i=1}^{n}(x_{i}x_{i}'-x_{i}\mu_{MLE}'-\mu_{MLE}x_{i}'-\mu_{MLE}\mu_{MLE}')\right\} $$ $$ =\dfrac{1}{n}\sum_{i=1}^{n}E(x_{i}x_{i}'-2x_{i}'\mu_{MLE}-\mu_{MLE}\mu_{MLE}') $$ $$ =\dfrac{1}{n}\sum_{i=1}^n \{E(x_{i}x_{i}')-2E(x_{i}'\mu_{MLE})-E(\mu_{MLE}\mu_{MLE}')\}   $$ I think $E(x_{i}x_{i}')=\mu\mu'$   but don't know what is the $E(x_{i}'\mu_{MLE})$   and $E(\mu_{MLE}\mu_{MLE}')$",,['statistics']
39,Beta distribution questions,Beta distribution questions,,"Just a simple beta distribution question just to be sure that I understand it. Say we do experiments, and we expect a proportion  $\theta$ of people having a specific property (which means $\theta \in [0,1]$) Assume we have a prior beta ""belief"" that $\theta = 0.3$ and we are very certain. The question now is, what should $a$ and $b$ be, if we define $$f(\theta, a, b) = \frac{\theta ^{a-1}(1-\theta)^{b-1}}{B(a,b)}$$ I would say, using the fact that the mean is $m =  \frac{a}{a+b}$, solving for $a$ and $b$ :  $$ a=mn; b=(1-m)n$$ If I am very sure that $\theta$ is $0.3$, then $a= mn = 0.3 \cdot25 =7.5$ and $b$ would be $0.7\cdot 25 = 17.5$ Is this good reasoning? I took the number $n$ quite randomly, but the idea is I guess that if you are very sure, $n$ should be big whereas if you are not that sure, $n$ should be quite low.","Just a simple beta distribution question just to be sure that I understand it. Say we do experiments, and we expect a proportion  $\theta$ of people having a specific property (which means $\theta \in [0,1]$) Assume we have a prior beta ""belief"" that $\theta = 0.3$ and we are very certain. The question now is, what should $a$ and $b$ be, if we define $$f(\theta, a, b) = \frac{\theta ^{a-1}(1-\theta)^{b-1}}{B(a,b)}$$ I would say, using the fact that the mean is $m =  \frac{a}{a+b}$, solving for $a$ and $b$ :  $$ a=mn; b=(1-m)n$$ If I am very sure that $\theta$ is $0.3$, then $a= mn = 0.3 \cdot25 =7.5$ and $b$ would be $0.7\cdot 25 = 17.5$ Is this good reasoning? I took the number $n$ quite randomly, but the idea is I guess that if you are very sure, $n$ should be big whereas if you are not that sure, $n$ should be quite low.",,"['probability', 'statistics', 'bayesian']"
40,Is a probability in the $z$-table less than or less than and equal to the $z$- score?,Is a probability in the -table less than or less than and equal to the - score?,z z,"When you look up the probability in a $z$-table is that the value less than or less than or equal? For example is it $p(z<2.35)=.0094$ or is it $p(z\leq2.35)=.0094$? And if they aren't equal then how would you calculate the other one? I would think given any single point has $0$ width that it shouldn't matter, but I am not sure.","When you look up the probability in a $z$-table is that the value less than or less than or equal? For example is it $p(z<2.35)=.0094$ or is it $p(z\leq2.35)=.0094$? And if they aren't equal then how would you calculate the other one? I would think given any single point has $0$ width that it shouldn't matter, but I am not sure.",,['statistics']
41,Mean response in linear regression,Mean response in linear regression,,"What does mean response in linear regession mean? I don't understand the definition given in wikipedia. This is the definition: Mean response is an estimate of the mean of the $y$ population associated with $x_d$, that is $E(y \mid x_d)=\hat{y}_d$. Also, Why is the variance predicted response different from mean response? Thanks!","What does mean response in linear regession mean? I don't understand the definition given in wikipedia. This is the definition: Mean response is an estimate of the mean of the $y$ population associated with $x_d$, that is $E(y \mid x_d)=\hat{y}_d$. Also, Why is the variance predicted response different from mean response? Thanks!",,"['statistics', 'regression']"
42,Solve the problem using linearity of expectation.,Solve the problem using linearity of expectation.,,"A building has $n$ floors numbered $1, 2, \cdots , n,$ plus a ground floor $G$. At the ground floor, $m$ people get on the elevator together, and each gets off at a uniformly random one of the $n$ floors (independently of everybody else). What is the expected number of floors the elevator stops at (not counting the ground floor)?","A building has $n$ floors numbered $1, 2, \cdots , n,$ plus a ground floor $G$. At the ground floor, $m$ people get on the elevator together, and each gets off at a uniformly random one of the $n$ floors (independently of everybody else). What is the expected number of floors the elevator stops at (not counting the ground floor)?",,"['probability', 'statistics', 'discrete-mathematics']"
43,Finding the value of Pr(X<Y),Finding the value of Pr(X<Y),,"Consider the joint density $f_{X,Y}(x,y)=1/40$ inside the rectangle 0< x <5 and 0< y< 8. How do I calculate $Pr[X<Y]$ ?","Consider the joint density $f_{X,Y}(x,y)=1/40$ inside the rectangle 0< x <5 and 0< y< 8. How do I calculate $Pr[X<Y]$ ?",,"['probability', 'statistics']"
44,For All Unique Combinations of 60 A's and 20 B's Number of Combinations that have BB,For All Unique Combinations of 60 A's and 20 B's Number of Combinations that have BB,,Here is my question. I have 60 A's and 20 B's and need to find out the number of unique combinations of those where B shows up consecutively at least once. For example (6 A's and 2 B's): AAAAAABB = 1 AAAAABAB = 0 AAAABBAA = 1,Here is my question. I have 60 A's and 20 B's and need to find out the number of unique combinations of those where B shows up consecutively at least once. For example (6 A's and 2 B's): AAAAAABB = 1 AAAAABAB = 0 AAAABBAA = 1,,"['combinatorics', 'statistics']"
45,How to find the Autocorrelation function and Covaraince,How to find the Autocorrelation function and Covaraince,,"I had to find the autocorrelation function in the following time series model $X_t = a+ bt+ Z_t + 0.6Z_{t-1} $ where $ a $ and $ b $ are constants. I used that $ \gamma (k) $ = $ Cov(X_t, X_{t+k})$. Then $Cov(a+ b t+ Z_t + 0.6 Z_{t-1}, a+ b(t+k)+ Z_{t+k} + 0.6 Z_{t+k-1}) $ but I am not sure how to evaluate that please help me on that. Any help would be appreciated.","I had to find the autocorrelation function in the following time series model $X_t = a+ bt+ Z_t + 0.6Z_{t-1} $ where $ a $ and $ b $ are constants. I used that $ \gamma (k) $ = $ Cov(X_t, X_{t+k})$. Then $Cov(a+ b t+ Z_t + 0.6 Z_{t-1}, a+ b(t+k)+ Z_{t+k} + 0.6 Z_{t+k-1}) $ but I am not sure how to evaluate that please help me on that. Any help would be appreciated.",,"['statistics', 'time-series']"
46,Maximum Likelihood Estimator,Maximum Likelihood Estimator,,"Let the random variable $X$ have a uniform density given by $$ f(x;\mu,\sigma)=\frac{1}{2\sqrt 3\sigma}I_{[\mu-\sqrt 3\sigma,\mu+\sqrt 3\sigma]}(x) $$ where $-\infty\lt\mu\lt\infty$ and  $\sigma\gt 0$ Find the maximum-likelihood-estimator [MLE] of $\mu$ and $\sigma$. the previous question Likelihood Function for the Uniform Density. had only one parameter $\theta$ . So it was easy to change the range with respect to $\theta$ and find the MLE of $\theta$.Also, there i have not asked to find the MLE of $\theta$ rather the questions were different in category. But in the present question, there are two parameters $\mu$ and $\sigma$. So it's not easy to me to change the range. I started to solve it $L(\mu,\sigma)=\prod_{i=1}^n f(x_i;\mu,\sigma)=\prod_{i=1}^n\frac{1}{2\sqrt 3\sigma}I_{[\mu-\sqrt 3\sigma,\mu+\sqrt 3\sigma]}(x_i)=[\frac{1}{2\sqrt 3\sigma}]^n \prod_{i=1}^n I_{[\mu-\sqrt 3\sigma,\mu+\sqrt 3\sigma]}(x_i)$ Then i don't know how to proceed.","Let the random variable $X$ have a uniform density given by $$ f(x;\mu,\sigma)=\frac{1}{2\sqrt 3\sigma}I_{[\mu-\sqrt 3\sigma,\mu+\sqrt 3\sigma]}(x) $$ where $-\infty\lt\mu\lt\infty$ and  $\sigma\gt 0$ Find the maximum-likelihood-estimator [MLE] of $\mu$ and $\sigma$. the previous question Likelihood Function for the Uniform Density. had only one parameter $\theta$ . So it was easy to change the range with respect to $\theta$ and find the MLE of $\theta$.Also, there i have not asked to find the MLE of $\theta$ rather the questions were different in category. But in the present question, there are two parameters $\mu$ and $\sigma$. So it's not easy to me to change the range. I started to solve it $L(\mu,\sigma)=\prod_{i=1}^n f(x_i;\mu,\sigma)=\prod_{i=1}^n\frac{1}{2\sqrt 3\sigma}I_{[\mu-\sqrt 3\sigma,\mu+\sqrt 3\sigma]}(x_i)=[\frac{1}{2\sqrt 3\sigma}]^n \prod_{i=1}^n I_{[\mu-\sqrt 3\sigma,\mu+\sqrt 3\sigma]}(x_i)$ Then i don't know how to proceed.",,"['statistics', 'statistical-inference']"
47,Ratio of PDF to complementary CDF,Ratio of PDF to complementary CDF,,"Let $f(x)$ be a probability density function, and $F(x)$ be the cumulative distribution function of $f(x)$. $$F(x) = \int_{-\infty}^{x}f(u)du$$ Then intuitively, what does the following ratio mean?  I'm trying to understand the ""common sense"" logic behind this ratio, if there is any.. $$ r(x) = \frac{f(x)}{1-F(x)}$$ Thanks!","Let $f(x)$ be a probability density function, and $F(x)$ be the cumulative distribution function of $f(x)$. $$F(x) = \int_{-\infty}^{x}f(u)du$$ Then intuitively, what does the following ratio mean?  I'm trying to understand the ""common sense"" logic behind this ratio, if there is any.. $$ r(x) = \frac{f(x)}{1-F(x)}$$ Thanks!",,"['probability', 'statistics', 'probability-theory']"
48,Showing uniform convergence in probability,Showing uniform convergence in probability,,"Suppose you want to show $sup_{x\in D}|f_n(x)|\to_p 0$, for $n\to \infty$, where $D\subset \mathbb R$ is a compact interval, $f$ is continuous depending on one or more random variables, and $\to_p$ means convergence in probability. For example, $f_n(x)=\sum_{i=1}^n(X_i-x)$ (this, however, is not the problem). Because showing statements as the one above directly is rather difficult I was wondering if it is sufficient to show $sup_{x\in D}| Ef_n(x)|\to 0$ and $sup_{x\in D}| Var(f_n(x))|\to 0$. Where $E$ and $Var$ are the expectation and variance operator, respectively. If some of you know a good read on this I appreciate your suggestions. Thanks in advance. Cheers.","Suppose you want to show $sup_{x\in D}|f_n(x)|\to_p 0$, for $n\to \infty$, where $D\subset \mathbb R$ is a compact interval, $f$ is continuous depending on one or more random variables, and $\to_p$ means convergence in probability. For example, $f_n(x)=\sum_{i=1}^n(X_i-x)$ (this, however, is not the problem). Because showing statements as the one above directly is rather difficult I was wondering if it is sufficient to show $sup_{x\in D}| Ef_n(x)|\to 0$ and $sup_{x\in D}| Var(f_n(x))|\to 0$. Where $E$ and $Var$ are the expectation and variance operator, respectively. If some of you know a good read on this I appreciate your suggestions. Thanks in advance. Cheers.",,"['probability', 'statistics', 'asymptotics']"
49,Linear Regression: Expectation Proof,Linear Regression: Expectation Proof,,"I found the following proof in my notes: $E(Y_i) = E[\beta_0 + \beta X_i + \varepsilon_i] =\cdots= \beta_0 + \beta X_i$.  This does not seem right to me, however.  Why would $E(\beta_1 X_i) = \beta_1 X_i$?  I wonder if i might have written it down incorrectly, with the actual proof meaning to be for the estimated value Yi hat (I don't know how to code this unfortunately).  Does anyone recall this property of linear regression?","I found the following proof in my notes: $E(Y_i) = E[\beta_0 + \beta X_i + \varepsilon_i] =\cdots= \beta_0 + \beta X_i$.  This does not seem right to me, however.  Why would $E(\beta_1 X_i) = \beta_1 X_i$?  I wonder if i might have written it down incorrectly, with the actual proof meaning to be for the estimated value Yi hat (I don't know how to code this unfortunately).  Does anyone recall this property of linear regression?",,"['statistics', 'regression']"
50,Mathematical Symbol,Mathematical Symbol,,"In the following paper , what does the symbol $\Phi$ in equation $3.1$ (page $3$) represent? Does it represent the normal distribution?","In the following paper , what does the symbol $\Phi$ in equation $3.1$ (page $3$) represent? Does it represent the normal distribution?",,"['probability', 'statistics', 'notation']"
51,"Does $\operatorname{MSE}(\hat{\theta}) = \operatorname{Var}(\theta)+ \left(\operatorname{Bias}(\hat{\theta},\theta)\right)^2$?",Does ?,"\operatorname{MSE}(\hat{\theta}) = \operatorname{Var}(\theta)+ \left(\operatorname{Bias}(\hat{\theta},\theta)\right)^2","We know that $\operatorname{MSE}(\hat{\theta})=\operatorname{E}\left[(\hat{\theta}-\theta)^2\right]$ and $\operatorname{MSE}(\hat{\theta})=\operatorname{Var}(\hat{\theta})+ \left(\operatorname{Bias}(\hat{\theta},\theta)\right)^2$ Is it true that : $\operatorname{MSE}(\hat{\theta})=\operatorname{Var}(\theta)+ \left(\operatorname{Bias}(\hat{\theta},\theta)\right)^2$ ?","We know that $\operatorname{MSE}(\hat{\theta})=\operatorname{E}\left[(\hat{\theta}-\theta)^2\right]$ and $\operatorname{MSE}(\hat{\theta})=\operatorname{Var}(\hat{\theta})+ \left(\operatorname{Bias}(\hat{\theta},\theta)\right)^2$ Is it true that : $\operatorname{MSE}(\hat{\theta})=\operatorname{Var}(\theta)+ \left(\operatorname{Bias}(\hat{\theta},\theta)\right)^2$ ?",,"['statistics', 'estimation', 'proof-without-words']"
52,Trouble understanding sum and product of probability distributions,Trouble understanding sum and product of probability distributions,,"Having trouble understanding where can we use the sum and product of probability distributions. Could someone please provide me with a real-life scenario? I think this is what I need to understand the formula. Also without finding any real-life scenarios on the Internet just looking at my notes I currently doesn't find it very logical that both for sum and product the probabilities are multiplied. Is there some mistake in my notes? Given that X and Y are both discrete probability distributions The sum of $(X,  P(X))$ and $(Y,  P(Y)$) $X+Y$ is $(Z, P(Z)) = {(z_k, p(z_k))}$, where  $z_k = x_i + y_j$ $(i=1,2,...,n;j=1,2,...,m)$ and$ p(z_k)= p(x_i)*p(y_j)$. The product of $(X,  P(X))$ and $(Y,  P(Y))$ $X*Y$ is $(Z, P(Z)) = {(z_k, p(z_k))}$, where $z_k=x_i*y_j(i=1,2,...,n;j=1,2,...,m)$ and $p(z_k)= p(x_i)*p(y_j)$. To make my question clearer: In what cases can we use the sum and product formula for discrete probability distribution?","Having trouble understanding where can we use the sum and product of probability distributions. Could someone please provide me with a real-life scenario? I think this is what I need to understand the formula. Also without finding any real-life scenarios on the Internet just looking at my notes I currently doesn't find it very logical that both for sum and product the probabilities are multiplied. Is there some mistake in my notes? Given that X and Y are both discrete probability distributions The sum of $(X,  P(X))$ and $(Y,  P(Y)$) $X+Y$ is $(Z, P(Z)) = {(z_k, p(z_k))}$, where  $z_k = x_i + y_j$ $(i=1,2,...,n;j=1,2,...,m)$ and$ p(z_k)= p(x_i)*p(y_j)$. The product of $(X,  P(X))$ and $(Y,  P(Y))$ $X*Y$ is $(Z, P(Z)) = {(z_k, p(z_k))}$, where $z_k=x_i*y_j(i=1,2,...,n;j=1,2,...,m)$ and $p(z_k)= p(x_i)*p(y_j)$. To make my question clearer: In what cases can we use the sum and product formula for discrete probability distribution?",,"['probability', 'statistics', 'probability-theory', 'probability-distributions']"
53,Probability to complete a sequence with two attempts,Probability to complete a sequence with two attempts,,"Imagine a slot machine with $N$ reels. I want to calculate the probability $P$ that a player hits a certain sequence $A$, if the player has given the possibility to spin again (and only once again), keeping the symbols belonging to $A$ (if any has come out). No problem with only one spin: $$P(A) = \prod_{n=1}^NP(a_n)$$ where $P(a_n)$ is the probability that a symbol belonging to $A$ comes out on the reel $n$ (with $a_n$ potentially different from $a_{n+1}$). But how to introduce the second spin into the calculation?","Imagine a slot machine with $N$ reels. I want to calculate the probability $P$ that a player hits a certain sequence $A$, if the player has given the possibility to spin again (and only once again), keeping the symbols belonging to $A$ (if any has come out). No problem with only one spin: $$P(A) = \prod_{n=1}^NP(a_n)$$ where $P(a_n)$ is the probability that a symbol belonging to $A$ comes out on the reel $n$ (with $a_n$ potentially different from $a_{n+1}$). But how to introduce the second spin into the calculation?",,"['probability', 'combinatorics', 'statistics']"
54,Chernoff bound proof using Markov,Chernoff bound proof using Markov,,"Does anyone familiar with the following format of Chernoff bound: $$ Pr\left(\frac{1}{n}\sum\limits_{i=1}^n X_i \gt T\right ) \le \inf_{\gamma \gt 0}{\left (  \frac{E[e^{\gamma X_i}]}{e^{\gamma T}} \right )}^n, $$ where $X_i$ are i.i.d. How can this format be evaluated from Markov bound? Thanks","Does anyone familiar with the following format of Chernoff bound: $$ Pr\left(\frac{1}{n}\sum\limits_{i=1}^n X_i \gt T\right ) \le \inf_{\gamma \gt 0}{\left (  \frac{E[e^{\gamma X_i}]}{e^{\gamma T}} \right )}^n, $$ where $X_i$ are i.i.d. How can this format be evaluated from Markov bound? Thanks",,"['probability', 'statistics', 'inequality']"
55,Finding variance given expected value,Finding variance given expected value,,"How would one find the variance of a random variable, $X$ given that it is composed of say two dependent random variables $Y_1$ and $Y_2$ (so $X = Y_1 + Y_2$), each with expected value of .5 and variance of .25. I'm basically stuck on how to account for the dependent aspect of the random variables. If I know the possible values for Y1 and Y2 are {0,1}, each with probability .5, how would I use cov(Y1,Y2) = E(Y1*Y2)-E(Y1)*E(Y2)? I'm not sure how to get E(Y1*Y2). Is it simply .5 given the above information?","How would one find the variance of a random variable, $X$ given that it is composed of say two dependent random variables $Y_1$ and $Y_2$ (so $X = Y_1 + Y_2$), each with expected value of .5 and variance of .25. I'm basically stuck on how to account for the dependent aspect of the random variables. If I know the possible values for Y1 and Y2 are {0,1}, each with probability .5, how would I use cov(Y1,Y2) = E(Y1*Y2)-E(Y1)*E(Y2)? I'm not sure how to get E(Y1*Y2). Is it simply .5 given the above information?",,"['probability', 'statistics', 'random-variables']"
56,Probability of two people meeting during a certain time.,Probability of two people meeting during a certain time.,,"I recently read a math problem and, having not yet taken anything beyond calculus 1, was curious about how to solve it correctly. Problem:  Calculate the probability of two people meeting at the same location between 1 and 2 p.m. Assume both people show and person 1 will wait 15 minutes for person 2. Doesn't this probability increase as the time frame shrinks?  For example, Isn't there a much smaller chance the pair will meet if person 1 arrives at 1:00 vs arrives at 1:45 (the probability of meeting becomes 1). Is this somewhat easily answered using basic statistics?  Just curious if my thought process is accurate or way wrong.","I recently read a math problem and, having not yet taken anything beyond calculus 1, was curious about how to solve it correctly. Problem:  Calculate the probability of two people meeting at the same location between 1 and 2 p.m. Assume both people show and person 1 will wait 15 minutes for person 2. Doesn't this probability increase as the time frame shrinks?  For example, Isn't there a much smaller chance the pair will meet if person 1 arrives at 1:00 vs arrives at 1:45 (the probability of meeting becomes 1). Is this somewhat easily answered using basic statistics?  Just curious if my thought process is accurate or way wrong.",,"['calculus', 'probability', 'statistics']"
57,A coin and die probability question?,A coin and die probability question?,,"A fair coin is tossed. If a head turns up, a fair die is tossed; if a tail turns up, two fair dice are tossed. What is the probability that the face (or the sum of the faces) showing on the die (or the dice) is equal to 6?","A fair coin is tossed. If a head turns up, a fair die is tossed; if a tail turns up, two fair dice are tossed. What is the probability that the face (or the sum of the faces) showing on the die (or the dice) is equal to 6?",,"['probability', 'statistics']"
58,Playing chess until one party wins,Playing chess until one party wins,,"Players $A$ and $B$ decide to play chess until one of them wins. Assume games are independent with $P(A\text{ wins})=0.3$, $P(B\text{ wins})=0.25$, $P(\text{draw})=0.45$ on each game. If the game ends in a draw another game will be played. Find the probability $A$ wins before $B$. Since the games are independent, I can simply calculate $P(A \text{ wins} \mid \text{somebody wins})$ right? The textbook does not have a solution.","Players $A$ and $B$ decide to play chess until one of them wins. Assume games are independent with $P(A\text{ wins})=0.3$, $P(B\text{ wins})=0.25$, $P(\text{draw})=0.45$ on each game. If the game ends in a draw another game will be played. Find the probability $A$ wins before $B$. Since the games are independent, I can simply calculate $P(A \text{ wins} \mid \text{somebody wins})$ right? The textbook does not have a solution.",,"['probability', 'statistics']"
59,Probability: Permutations,Probability: Permutations,,"Consider the experiment of picking a random permutation $\pi$ on $\{1,2,...,n\}$, and define the associated random variable $f(\pi)$ as the number of fixed points of $\pi$, i.e, the number of $i$ such that $f(i)=i$. I know that a permutation of $X=\{1,2,\ldots ,n\}$ is a one-to-one function $\pi: X \rightarrow X$. Any two functions, $\pi_1, \pi_2$ can be composed and the resulting function is also a permutation. How can I find $E(F)$ and what do we know about $E(f(\pi^2))$ and $ E(f( \pi^k))$?","Consider the experiment of picking a random permutation $\pi$ on $\{1,2,...,n\}$, and define the associated random variable $f(\pi)$ as the number of fixed points of $\pi$, i.e, the number of $i$ such that $f(i)=i$. I know that a permutation of $X=\{1,2,\ldots ,n\}$ is a one-to-one function $\pi: X \rightarrow X$. Any two functions, $\pi_1, \pi_2$ can be composed and the resulting function is also a permutation. How can I find $E(F)$ and what do we know about $E(f(\pi^2))$ and $ E(f( \pi^k))$?",,"['probability', 'statistics', 'probability-theory', 'probability-distributions']"
60,Proving consistency for an estimator.,Proving consistency for an estimator.,,"Let $Y_1,Y_2,...,Y_n$ be a random sample of size $n$ from a normal pdf having $\mu=0$. Show that $S_{n}^{2} = \frac{1}{n} \sum_{i=1}^n {Y_{i}^{2}}$ is a consistent estimator for $\sigma^2= Var(Y)$.","Let $Y_1,Y_2,...,Y_n$ be a random sample of size $n$ from a normal pdf having $\mu=0$. Show that $S_{n}^{2} = \frac{1}{n} \sum_{i=1}^n {Y_{i}^{2}}$ is a consistent estimator for $\sigma^2= Var(Y)$.",,['statistics']
61,"Sufficient Statistics, MLE and Unbiased Estimators of Uniform Type Distribution","Sufficient Statistics, MLE and Unbiased Estimators of Uniform Type Distribution",,"Let $X_1, \dots, X_n$ denote a random sample of size $n$ from the probability distribution with pdf: $$ f_X(x\mid\theta_1, \theta_2) = \frac{1}{\theta_2 - \theta_1} \ I(x)_{[\theta_1,\theta_2]} \ I(\theta_1)_{(-\infty,\theta_2)} \ I(\theta_2)_{(\theta_1,\infty)}\;.$$ (1) Find a pair of sufficient statistics for $(\theta_1, \theta_2)$ . ${\bf\text{My thoughts:}}$ This wasn't too bad.  I got $(X_{(1)}, X_{(n)})$ for this part (2) Find the maximum likelihood estimator $(\hat{\theta}_1, \hat{\theta}_2)$ for $(\theta_1, \theta_2)$ . ${\bf\text{My thoughts:}}$ Thinking I need to use monotone functions since it has 2 parameters and the variables are part of the interval.  I believe that $\frac{X_{(1)} + X_{(n)}}{2}$ will become one of my estimators. (3) Show that $\frac{X_{(1)} + X_{(n)}}{2}$ is an unbiased estimator for $\frac{\theta_1 + \theta_2}{2}$ . $\bf{My \ thoughts:}$ I think I will need to use Cramer-Rao Lower Bound in some form but not quite sure if that is right . (4) Construct an unbiased estimator for $\theta_2 - \theta_1$ . $\bf{My \ thoughts:}$ Very stuck on this part, but I think I can use some information from previous parts to help me. Any help is greatly appreciated.","Let denote a random sample of size from the probability distribution with pdf: (1) Find a pair of sufficient statistics for . This wasn't too bad.  I got for this part (2) Find the maximum likelihood estimator for . Thinking I need to use monotone functions since it has 2 parameters and the variables are part of the interval.  I believe that will become one of my estimators. (3) Show that is an unbiased estimator for . I think I will need to use Cramer-Rao Lower Bound in some form but not quite sure if that is right . (4) Construct an unbiased estimator for . Very stuck on this part, but I think I can use some information from previous parts to help me. Any help is greatly appreciated.","X_1, \dots, X_n n  f_X(x\mid\theta_1, \theta_2) = \frac{1}{\theta_2 - \theta_1} \ I(x)_{[\theta_1,\theta_2]} \ I(\theta_1)_{(-\infty,\theta_2)} \ I(\theta_2)_{(\theta_1,\infty)}\;. (\theta_1, \theta_2) {\bf\text{My thoughts:}} (X_{(1)}, X_{(n)}) (\hat{\theta}_1, \hat{\theta}_2) (\theta_1, \theta_2) {\bf\text{My thoughts:}} \frac{X_{(1)} + X_{(n)}}{2} \frac{X_{(1)} + X_{(n)}}{2} \frac{\theta_1 + \theta_2}{2} \bf{My \ thoughts:} \theta_2 - \theta_1 \bf{My \ thoughts:}",['statistics']
62,Bayesian Problem... I think,Bayesian Problem... I think,,"Let X be the number of coin tosses until heads is obtained. Without knowing that the coin is fair, I assume that the probability of heads is uniformly distributed. How would I find the distribution of X. From my knowledge I would need to first know the conditional distribution : $X\mid Y=y$ , correct? Should I be able to get this from the given information? I also need to show that the expected value of X,  $E[X]$ does not exist.","Let X be the number of coin tosses until heads is obtained. Without knowing that the coin is fair, I assume that the probability of heads is uniformly distributed. How would I find the distribution of X. From my knowledge I would need to first know the conditional distribution : $X\mid Y=y$ , correct? Should I be able to get this from the given information? I also need to show that the expected value of X,  $E[X]$ does not exist.",,"['statistics', 'bayesian', 'conditional-probability']"
63,Necessary condition for pairwise sufficient statistic [duplicate],Necessary condition for pairwise sufficient statistic [duplicate],,"This question already has answers here : Pairwise measurable derivatives imply measurability of combined derivative (2 answers) Closed 11 years ago . I'm struggling to prove the following. If $T:\left(X,\mathbf{A}\right)\rightarrow\left(Y,\mathbf{B}\right)$ is a pairwise sufficient statistic for a set $\left\{\mu_0,\mu_1,\mu_2\right\}$ of three measures on $\left(X,\mathbf{A}\right)$ , then $\frac{d\mu_0}{d\left(\mu_0+\mu_1+\mu_2\right)}$ (the Radon-Nikodym derivative) is $T^{-1}\left(\mathbf{B}\right)$ -measurable modulo $\mu_0+\mu_1+\mu_2$ . It is supposedly proved in the otherwise accessible and irreproachable article ""Application of the Radon-Nikodym Theorem to the Theory of Sufficient Statistics"" by Halmos and Savage ( Lemma 9 , page 238), but i'm dissatisfied with the proof, since in my opinion it justifies the claim modulo $\mu_0$ only. I'd appreciate help in either understanding Halmos & Savage's proof or proving it from scratch.","This question already has answers here : Pairwise measurable derivatives imply measurability of combined derivative (2 answers) Closed 11 years ago . I'm struggling to prove the following. If is a pairwise sufficient statistic for a set of three measures on , then (the Radon-Nikodym derivative) is -measurable modulo . It is supposedly proved in the otherwise accessible and irreproachable article ""Application of the Radon-Nikodym Theorem to the Theory of Sufficient Statistics"" by Halmos and Savage ( Lemma 9 , page 238), but i'm dissatisfied with the proof, since in my opinion it justifies the claim modulo only. I'd appreciate help in either understanding Halmos & Savage's proof or proving it from scratch.","T:\left(X,\mathbf{A}\right)\rightarrow\left(Y,\mathbf{B}\right) \left\{\mu_0,\mu_1,\mu_2\right\} \left(X,\mathbf{A}\right) \frac{d\mu_0}{d\left(\mu_0+\mu_1+\mu_2\right)} T^{-1}\left(\mathbf{B}\right) \mu_0+\mu_1+\mu_2 \mu_0","['measure-theory', 'statistics']"
64,"Poisson probability differs from combinatorial probability, why?","Poisson probability differs from combinatorial probability, why?",,"I am trying to figure out why the Poisson probability differs from the combinatorial probability. For example, assume that 10% of people are left handed. What is the probability that a classroom of 30 students contains only right-handed students? Using combinatorial probability the answer is 0.9^30 = 4.239 % Using Poisson probability the answer is e^(-0.1*30) = 4.979% My guess is that the difference is due to the 10% being an observed rate rather than an absolute truth in the Poisson case. What is the explanation? Edit: Note that if I change the problem to be what is the probability of there being more than 2 left handers in the class then the numbers are dramatically different. The combinatorial probability being 36.1% and the Poisson probability being 57.7%, quite a big difference. Update: I made a mistake in my calculation. The combinatorial probability for having exactly two left handers is 58.9 %, so it is fairly close to the Poisson estimate.","I am trying to figure out why the Poisson probability differs from the combinatorial probability. For example, assume that 10% of people are left handed. What is the probability that a classroom of 30 students contains only right-handed students? Using combinatorial probability the answer is 0.9^30 = 4.239 % Using Poisson probability the answer is e^(-0.1*30) = 4.979% My guess is that the difference is due to the 10% being an observed rate rather than an absolute truth in the Poisson case. What is the explanation? Edit: Note that if I change the problem to be what is the probability of there being more than 2 left handers in the class then the numbers are dramatically different. The combinatorial probability being 36.1% and the Poisson probability being 57.7%, quite a big difference. Update: I made a mistake in my calculation. The combinatorial probability for having exactly two left handers is 58.9 %, so it is fairly close to the Poisson estimate.",,"['probability', 'statistics']"
65,Bootstrap-related issue,Bootstrap-related issue,,Say I re-sample $N$ items with replacement from a numbered item sample of size $N$. What is the average number of data items that are not selected in each such sampling?,Say I re-sample $N$ items with replacement from a numbered item sample of size $N$. What is the average number of data items that are not selected in each such sampling?,,['statistics']
66,Bayes' Rules: The probability of at least one event occurring?,Bayes' Rules: The probability of at least one event occurring?,,"There is a 60 percent chance that the event $A$ will occur. If $A$ does not occur, then   there is a 10 percent chance that $B$ will occur. (a) What is the probability that at least one of the events $A$ or $B$ occurs? I am given $P(A)=0.6$, $P(A')=0.4$, $P(B\mid A')=0.1$. If the probability that at least one of the events occurring is the probability of the union of $A$ and $B$: $$P(A ∪ B)=P(A)+P(B)-P(A ∩ B)$$ then I must find $P(B): P(B)=P(B\mid A)P(A)+P(B\mid A')P(A')$. To do that I have to find $P(B|A)$. I can't assume that $P(B\mid A)=1-P(B\mid A')$ right? So $P(B\mid A)=\frac{P(AB)}{P(A)}=\frac{P(A\mid B)P(B)}{P(A)}$, which means I have to find $P(B)$ which I had to find in the first place. It feels like I am caught in a loop. Where was my error, and how can I solve this problem?","There is a 60 percent chance that the event $A$ will occur. If $A$ does not occur, then   there is a 10 percent chance that $B$ will occur. (a) What is the probability that at least one of the events $A$ or $B$ occurs? I am given $P(A)=0.6$, $P(A')=0.4$, $P(B\mid A')=0.1$. If the probability that at least one of the events occurring is the probability of the union of $A$ and $B$: $$P(A ∪ B)=P(A)+P(B)-P(A ∩ B)$$ then I must find $P(B): P(B)=P(B\mid A)P(A)+P(B\mid A')P(A')$. To do that I have to find $P(B|A)$. I can't assume that $P(B\mid A)=1-P(B\mid A')$ right? So $P(B\mid A)=\frac{P(AB)}{P(A)}=\frac{P(A\mid B)P(B)}{P(A)}$, which means I have to find $P(B)$ which I had to find in the first place. It feels like I am caught in a loop. Where was my error, and how can I solve this problem?",,"['probability', 'statistics', 'bayesian']"
67,What are the theorems on the inevitability of some kind of order in large sets?,What are the theorems on the inevitability of some kind of order in large sets?,,"I've read Paulos' A Mathematician Plays the Stock Market : The problem is that if you look hard enough, you will always find     some seemingly effective rule that resulted in large gains over a     certain time span or within a certain sector. (In fact, inspired by     the british economist Frank Ramsey, mathematicians over the last half     century have proved a variety of theorems on the inevitability of some     kind of order in large     sets) What are these theorems?","I've read Paulos' A Mathematician Plays the Stock Market : The problem is that if you look hard enough, you will always find     some seemingly effective rule that resulted in large gains over a     certain time span or within a certain sector. (In fact, inspired by     the british economist Frank Ramsey, mathematicians over the last half     century have proved a variety of theorems on the inevitability of some     kind of order in large     sets) What are these theorems?",,"['probability', 'statistics', 'probability-theory']"
68,How would I go about solving this statistics question on permutations?,How would I go about solving this statistics question on permutations?,,"Columba has two dozen each of n different coloured beads. If she can   select 20 beads (with repetition of colors allowed) in 230,230 ways,   what is the value of n? I'm trying to figure it out, but I'm having some trouble. I know that the question is asking how many colors are there, but I'm not sure how to go about getting there. Is there an applicable formula? And if so, what's it called?","Columba has two dozen each of n different coloured beads. If she can   select 20 beads (with repetition of colors allowed) in 230,230 ways,   what is the value of n? I'm trying to figure it out, but I'm having some trouble. I know that the question is asking how many colors are there, but I'm not sure how to go about getting there. Is there an applicable formula? And if so, what's it called?",,"['probability', 'statistics', 'permutations']"
69,What is the difference between a polynomial regression and a generalized linear model?,What is the difference between a polynomial regression and a generalized linear model?,,"I have seen that a polynomial linear regression can have this form: $y = c_0 + c_1 x_1 + c_2 x_2 + \dots  + c_k x_k $ but I have read that the general lineal model which is a form of the multiple linear regression, can have this form: $Y_i=b_0+b_1X_{i1}+\dots+b_pX_{ip}$ I see that almost both of them are the same. When I program in R the second case, I can use the function abline, that draws a line of ""best fit"" as it is called. But when I want to use it in the first case (the polynomial one) it is not possible. Why is that? If both returns a set of scattered points over the plane Examples of both cases: First case it could be the index of houses bought by clients over years and bimonthly. Second case, a classical one, it would be the weight prediction of a person considering the weight of their hips, arms, etc. Why I can use abline in the second case, but not in the first one? Thanks","I have seen that a polynomial linear regression can have this form: $y = c_0 + c_1 x_1 + c_2 x_2 + \dots  + c_k x_k $ but I have read that the general lineal model which is a form of the multiple linear regression, can have this form: $Y_i=b_0+b_1X_{i1}+\dots+b_pX_{ip}$ I see that almost both of them are the same. When I program in R the second case, I can use the function abline, that draws a line of ""best fit"" as it is called. But when I want to use it in the first case (the polynomial one) it is not possible. Why is that? If both returns a set of scattered points over the plane Examples of both cases: First case it could be the index of houses bought by clients over years and bimonthly. Second case, a classical one, it would be the weight prediction of a person considering the weight of their hips, arms, etc. Why I can use abline in the second case, but not in the first one? Thanks",,"['statistics', 'regression']"
70,Poisson Process with two Lambda,Poisson Process with two Lambda,,"I am quite stuck here, but well I am trying my best. Question: The Web page of a teacher receives hits from his students according to a poisson process with rate  λ=10 visits/day . Also, his colleagues , are vititng his page with rate 2 visits per/day . 1) What is the probability the visits to exceed 10 in a day? 2) What is the probability no student visits his page into 10 hours interval ? 3) Suppose that every student who visits the site , press the link with teachers publications with chance 2/5. What is the probability into a day that only one student pressed that link? My attempted Solution: 1) To begin with I turn the days into hours , for my convenience. So I assume that λ1= 10/24 and λ2=2/24=1/12 . Since the question is not very clear to me... I think that we just need P((s,s+24))= P((s,s+24))-N(s) >10) =P(N(24)> 10) . here λ=λ1+λ2 = 1/2 .  But, I am not sure how to continue from this point. Should I take some integral from 10 to inf of e^(-λs) * (λs)^x/x!  dx ? Couldn't also I say that its the probability 1-(p(0)+p(1)+...+p(10) )? Although this might take loads of time . 2) So for this question, λ=10/24 . And probably, we need something very very simple ,although for some reason I believe that I am missing something here, but anyway. This: P(0 student visits on (0,10)) so it should be e^100/24 =~ e^-(4.1)","I am quite stuck here, but well I am trying my best. Question: The Web page of a teacher receives hits from his students according to a poisson process with rate  λ=10 visits/day . Also, his colleagues , are vititng his page with rate 2 visits per/day . 1) What is the probability the visits to exceed 10 in a day? 2) What is the probability no student visits his page into 10 hours interval ? 3) Suppose that every student who visits the site , press the link with teachers publications with chance 2/5. What is the probability into a day that only one student pressed that link? My attempted Solution: 1) To begin with I turn the days into hours , for my convenience. So I assume that λ1= 10/24 and λ2=2/24=1/12 . Since the question is not very clear to me... I think that we just need P((s,s+24))= P((s,s+24))-N(s) >10) =P(N(24)> 10) . here λ=λ1+λ2 = 1/2 .  But, I am not sure how to continue from this point. Should I take some integral from 10 to inf of e^(-λs) * (λs)^x/x!  dx ? Couldn't also I say that its the probability 1-(p(0)+p(1)+...+p(10) )? Although this might take loads of time . 2) So for this question, λ=10/24 . And probably, we need something very very simple ,although for some reason I believe that I am missing something here, but anyway. This: P(0 student visits on (0,10)) so it should be e^100/24 =~ e^-(4.1)",,"['probability', 'statistics', 'probability-theory', 'stochastic-processes']"
71,Finding maximum likelihood to $\mu$,Finding maximum likelihood to,\mu,"I'd really love your help with this one. In a survey the probability that a person wouldn't like to answer on it is $p$. The survey company calls people until it meets a person who doesn't like to answer it. First I was asked to compute the geometric likelihood function $L(p;x_1=0,x_2=0,..x_5=1)$ and draw a graph of it as a function of $p$ , and I did it. Now I need to find the relation between the expected value of the number of people that would answer the survey until the first one who won't answer ($\mu$), and to use it to find Maximum likelihood to $\mu$. How do I find the requested relation? I can't remember from probability classes how to find the above expected value. can you please remind me/  give me a hint? Thanks a lot!","I'd really love your help with this one. In a survey the probability that a person wouldn't like to answer on it is $p$. The survey company calls people until it meets a person who doesn't like to answer it. First I was asked to compute the geometric likelihood function $L(p;x_1=0,x_2=0,..x_5=1)$ and draw a graph of it as a function of $p$ , and I did it. Now I need to find the relation between the expected value of the number of people that would answer the survey until the first one who won't answer ($\mu$), and to use it to find Maximum likelihood to $\mu$. How do I find the requested relation? I can't remember from probability classes how to find the above expected value. can you please remind me/  give me a hint? Thanks a lot!",,"['probability', 'statistics']"
72,Calculate odds of getting all hight point tiles in single game of scrabble,Calculate odds of getting all hight point tiles in single game of scrabble,,"I am wondering how to best calculate the probability of getting the all of the following ""high point"" tiles in a single game of scrabble: Z, Q, J, X, and K In a game of scrabble, there are 100 tiles for distribution. Of the Z, Q, J, X, and K tiles, only 1 each of these are in the bag of tiles. Let's assume that in a single game, each player gets 50 tiles each. I think of this as a bag of 100 marbles with 95 white marbles and 5 red ones. So, if you were to distribute them evenly to each of the two players, what are the odds that one person will receive all 5 red marbles/high point tiles. Now, what are the odds of this happening twice in a row? 3 times in a row?","I am wondering how to best calculate the probability of getting the all of the following ""high point"" tiles in a single game of scrabble: Z, Q, J, X, and K In a game of scrabble, there are 100 tiles for distribution. Of the Z, Q, J, X, and K tiles, only 1 each of these are in the bag of tiles. Let's assume that in a single game, each player gets 50 tiles each. I think of this as a bag of 100 marbles with 95 white marbles and 5 red ones. So, if you were to distribute them evenly to each of the two players, what are the odds that one person will receive all 5 red marbles/high point tiles. Now, what are the odds of this happening twice in a row? 3 times in a row?",,"['probability', 'statistics', 'probability-theory']"
73,Bounding the standard deviation of a bounded random variable,Bounding the standard deviation of a bounded random variable,,"Suppose $X$ is a random variable (discrete or continuous) whose values lie in the segment $[0,1]$. Is it safe to say that the standard deviation of $X$ is between $0$ and $\frac{1}{2}$?","Suppose $X$ is a random variable (discrete or continuous) whose values lie in the segment $[0,1]$. Is it safe to say that the standard deviation of $X$ is between $0$ and $\frac{1}{2}$?",,"['probability', 'statistics', 'standard-deviation']"
74,combinatorics: probability of picking elements,combinatorics: probability of picking elements,,"I have $6$ elements $$\{a, b, c, d, e, f \}$$ I close my eyes and randomly pick $5$ elements. What is the chance of getting $a$ and $b$ in those $5$ elements?","I have $6$ elements $$\{a, b, c, d, e, f \}$$ I close my eyes and randomly pick $5$ elements. What is the chance of getting $a$ and $b$ in those $5$ elements?",,"['probability', 'combinatorics', 'statistics']"
75,Hat Matrix Identities in Regression,Hat Matrix Identities in Regression,,"I need to show that $\bar h= \sum{h_{ii}/n} = \operatorname{Tr}[H]/n = (p+1)/n$ Using the fact that $\operatorname{Tr}[AB]=\operatorname{Tr}[BA]$ and $H=X(X^TX)^{-1}X^T$. But I have no idea how to calculate $\bar h$, I'm betting the first equality works out because $H$ is a symmetric idempotent matrix. I also have no clue what $\operatorname{Tr}[H]$ means, I have never seen this notation before an cannot find it in my notes.","I need to show that $\bar h= \sum{h_{ii}/n} = \operatorname{Tr}[H]/n = (p+1)/n$ Using the fact that $\operatorname{Tr}[AB]=\operatorname{Tr}[BA]$ and $H=X(X^TX)^{-1}X^T$. But I have no idea how to calculate $\bar h$, I'm betting the first equality works out because $H$ is a symmetric idempotent matrix. I also have no clue what $\operatorname{Tr}[H]$ means, I have never seen this notation before an cannot find it in my notes.",,"['matrices', 'statistics', 'regression']"
76,A die is rolled. A coin is flipped four times. Find the probability that the number on the die is twice the number of heads from the coin flips.,A die is rolled. A coin is flipped four times. Find the probability that the number on the die is twice the number of heads from the coin flips.,,I absolutely have no idea how to begin this problem. I am not looking for an exact answer as much as I am a proper solution so I can learn how to handle similar problems.,I absolutely have no idea how to begin this problem. I am not looking for an exact answer as much as I am a proper solution so I can learn how to handle similar problems.,,"['probability', 'statistics']"
77,Joint probability of two random variables both following poisson distribution,Joint probability of two random variables both following poisson distribution,,"I have this joint probability question I'm having trouble with. If X, Y follow a poisson distribution with Poisson($\lambda$) and X and Y are independent. Show that: $$1 \le \frac{VAR(max{X,Y})}{\lambda} \le 2$$ I know that variance of a random variable following a poisson distribution is $\lambda$. Any help would be greatly appreciated!","I have this joint probability question I'm having trouble with. If X, Y follow a poisson distribution with Poisson($\lambda$) and X and Y are independent. Show that: $$1 \le \frac{VAR(max{X,Y})}{\lambda} \le 2$$ I know that variance of a random variable following a poisson distribution is $\lambda$. Any help would be greatly appreciated!",,"['probability', 'statistics']"
78,Maximum likelihood estimators of three independent normal random variables with a common variance,Maximum likelihood estimators of three independent normal random variables with a common variance,,"Suppose that $X_1,\ldots,X_n$ are normal with mean $\mu_1$; $Y_1,\ldots,Y_n$ are normal with mean $\mu_2$; and $W_1,\ldots,W_n$ are normal with mean $\mu_1+\mu_2$. Assuming that all $3n$ random variables are independent, with a common variance, find the maximum likelihood estimators of $\mu_1$ and $\mu_2$. Solving for $\mu_1$ using $X_1,\ldots,X_n$ I got $\mu_1$ = the sample mean of $X$. Similarly, solving for $\mu_2$ using $Y_1,\ldots,Y_2$ I got $\mu_2$ = the sample mean of $Y$. However, I'm not sure if that's what this question meant, especially since I don't understand what the purpose of giving the mean of $W_1,\ldots,W_n$ is.","Suppose that $X_1,\ldots,X_n$ are normal with mean $\mu_1$; $Y_1,\ldots,Y_n$ are normal with mean $\mu_2$; and $W_1,\ldots,W_n$ are normal with mean $\mu_1+\mu_2$. Assuming that all $3n$ random variables are independent, with a common variance, find the maximum likelihood estimators of $\mu_1$ and $\mu_2$. Solving for $\mu_1$ using $X_1,\ldots,X_n$ I got $\mu_1$ = the sample mean of $X$. Similarly, solving for $\mu_2$ using $Y_1,\ldots,Y_2$ I got $\mu_2$ = the sample mean of $Y$. However, I'm not sure if that's what this question meant, especially since I don't understand what the purpose of giving the mean of $W_1,\ldots,W_n$ is.",,['statistics']
79,Probability a virus damages 800-850 files of 2400 files with an independent probability of 35%?,Probability a virus damages 800-850 files of 2400 files with an independent probability of 35%?,,"A certain computer virus can damage any file with probability 35%, independently of other files. Suppose this virus enters a folder containing 2400 files. Compute the probability that between 800 and 850 files get damaged. I'm not interested in so much the exact answer as much as I am into how to solve this type of problem. We are working with Poisson distribution and standard deviation which I haven't even figured out how I can even apply to this problem, or if it's even the correct path towards the solution.","A certain computer virus can damage any file with probability 35%, independently of other files. Suppose this virus enters a folder containing 2400 files. Compute the probability that between 800 and 850 files get damaged. I'm not interested in so much the exact answer as much as I am into how to solve this type of problem. We are working with Poisson distribution and standard deviation which I haven't even figured out how I can even apply to this problem, or if it's even the correct path towards the solution.",,"['probability', 'statistics']"
80,Do multiple polls with identical margins of error decrease the total margin of error?,Do multiple polls with identical margins of error decrease the total margin of error?,,"Is it possible to take multiple polls and combine their margin of error to produce a single, overall margin of error? For an example, let's say we have two populations with the following characteristics, obtained from similar polls: Population A prefers X over Y by 2% with a margin of error of 2%. The population size is 1000 people. Population B also prefers X over Y by 2% with a margin of error of 2%. The population size is also 1000 people. Can we conclude that the 2000 people surveyed prefer X over Y by a margin of error less than 2%, or is the margin of error still just 2%? If possible, provide a mathematical proof of either conclusion. My intuition collides with my friend's intuition, so I'm looking for an objective, conclusive answer. I'm searching through some of my old statistics material and general information on margins of error , but I'm not finding an answer to this specific question. (If it's not obvious, I'm interested in political polls, but I'm trying to keep the question objective.)","Is it possible to take multiple polls and combine their margin of error to produce a single, overall margin of error? For an example, let's say we have two populations with the following characteristics, obtained from similar polls: Population A prefers X over Y by 2% with a margin of error of 2%. The population size is 1000 people. Population B also prefers X over Y by 2% with a margin of error of 2%. The population size is also 1000 people. Can we conclude that the 2000 people surveyed prefer X over Y by a margin of error less than 2%, or is the margin of error still just 2%? If possible, provide a mathematical proof of either conclusion. My intuition collides with my friend's intuition, so I'm looking for an objective, conclusive answer. I'm searching through some of my old statistics material and general information on margins of error , but I'm not finding an answer to this specific question. (If it's not obvious, I'm interested in political polls, but I'm trying to keep the question objective.)",,['statistics']
81,Finding the probability that the temperature is between x and y,Finding the probability that the temperature is between x and y,,Hello I have a problem that I can't seem to come to the proper conclusion with. It is as follows: What is the probability that a randomly selected day in August will have a temperature greater than 85 but less than 100. Mean is 80 with a std dev of 8 assuming temperature is distributed normally. I missed this on a test and I'd like to know where I went wrong so if someone could work it out step by step I would appreciate it very much. Thanks,Hello I have a problem that I can't seem to come to the proper conclusion with. It is as follows: What is the probability that a randomly selected day in August will have a temperature greater than 85 but less than 100. Mean is 80 with a std dev of 8 assuming temperature is distributed normally. I missed this on a test and I'd like to know where I went wrong so if someone could work it out step by step I would appreciate it very much. Thanks,,"['probability', 'statistics']"
82,Formulae for combining statistical moments,Formulae for combining statistical moments,,"I am writing code to calculate statistical moments (mean, variance, skewness, kurtosis) for large samples of data and have the requirement of needing to be able to calculate moments for subsections of the sample (in parallel), then combine/merge them to give the moment for the sample as a whole. For example: $S = \lbrace 1.0, 1.2, 2.0, 1.7, 3.4, 0.9 \rbrace $ $A = \lbrace 1.0, 1.2, 2.0 \rbrace$ and $B = \lbrace  1.7, 3.4, 0.9 \rbrace$ So $A \cup B = S$ I need to calculate the statistics/moments for $A$ and $B$, then combine them to give the statistics/moments for $S$ Count is simple: $n_S = n_A + n_B$ Mean is not much worse: $\mu_S = (n_A\mu_A + n_B\mu_B) / n_S$ Variance is a little less pretty: $\sigma_S = [n_A\sigma_A + n_B\sigma_B + (\frac{n_An_A}{n_A+n_B})(\mu_A - \mu_B)^2] / n_S$ But now I'm struggling for skewness and, in particular, kurtosis. I have all 'lesser' moments for each of the subsections of the data available and have some idea of the direction I'm heading, but am really struggling to derive the formulae needed. Has anybody derived these formulae before? Could anyone point me in the right direction? These may be simple/obvious things to any with anyone with a decent amount of statistical knowledge, unfortunately that's something I completely lack...","I am writing code to calculate statistical moments (mean, variance, skewness, kurtosis) for large samples of data and have the requirement of needing to be able to calculate moments for subsections of the sample (in parallel), then combine/merge them to give the moment for the sample as a whole. For example: $S = \lbrace 1.0, 1.2, 2.0, 1.7, 3.4, 0.9 \rbrace $ $A = \lbrace 1.0, 1.2, 2.0 \rbrace$ and $B = \lbrace  1.7, 3.4, 0.9 \rbrace$ So $A \cup B = S$ I need to calculate the statistics/moments for $A$ and $B$, then combine them to give the statistics/moments for $S$ Count is simple: $n_S = n_A + n_B$ Mean is not much worse: $\mu_S = (n_A\mu_A + n_B\mu_B) / n_S$ Variance is a little less pretty: $\sigma_S = [n_A\sigma_A + n_B\sigma_B + (\frac{n_An_A}{n_A+n_B})(\mu_A - \mu_B)^2] / n_S$ But now I'm struggling for skewness and, in particular, kurtosis. I have all 'lesser' moments for each of the subsections of the data available and have some idea of the direction I'm heading, but am really struggling to derive the formulae needed. Has anybody derived these formulae before? Could anyone point me in the right direction? These may be simple/obvious things to any with anyone with a decent amount of statistical knowledge, unfortunately that's something I completely lack...",,['statistics']
83,bounding the the expected value of the maximum of two random variables,bounding the the expected value of the maximum of two random variables,,"Consider two standardized random variables $x$ and $y$, and define a function $g(x,y)=E[max(x,y)]$ where $E$ is the expected value operator. My question is finding the upper and lower bounds of $g(x,y)$?","Consider two standardized random variables $x$ and $y$, and define a function $g(x,y)=E[max(x,y)]$ where $E$ is the expected value operator. My question is finding the upper and lower bounds of $g(x,y)$?",,"['probability', 'statistics', 'economics']"
84,Finding Probability function when Expectation and Variance are given,Finding Probability function when Expectation and Variance are given,,"How do you find the probability of a specific value when you only have the expected value and the function's variance? For example, I'm asked to find a) $P\{X = 4\}$ and b) $P\{X>12\}$ If $E[X] = 7$ and $Var(X) = 2.1$","How do you find the probability of a specific value when you only have the expected value and the function's variance? For example, I'm asked to find a) and b) If and",P\{X = 4\} P\{X>12\} E[X] = 7 Var(X) = 2.1,"['probability', 'statistics']"
85,Inequality of Pearson correlation coefficient,Inequality of Pearson correlation coefficient,,"Let $x_1,\ldots x_n,y_1,\ldots y_n$ be reals and $\bar{x},\bar{y}$ the aritmetic mean of numbers $x_1,\ldots x_n$ and $y_1,\ldots y_n$ respectively. How can I show that $$-1\leq \dfrac{\sum_{i=1}^n (x_i-\bar{x})(y_1-\bar{y})}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2\sum_{i=1}^n(y_i-\bar{y})^2}}$$ Looks a bit like Cauchy (it gives me that the expression is at most one) but I was unable to find the proof.","Let $x_1,\ldots x_n,y_1,\ldots y_n$ be reals and $\bar{x},\bar{y}$ the aritmetic mean of numbers $x_1,\ldots x_n$ and $y_1,\ldots y_n$ respectively. How can I show that $$-1\leq \dfrac{\sum_{i=1}^n (x_i-\bar{x})(y_1-\bar{y})}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2\sum_{i=1}^n(y_i-\bar{y})^2}}$$ Looks a bit like Cauchy (it gives me that the expression is at most one) but I was unable to find the proof.",,"['statistics', 'inequality']"
86,Understanding importance sampling,Understanding importance sampling,,"From Wikipedia Let $X:\Omega\to \mathbb{R}$ be a random variable in some probability space $(\Omega,\mathcal{F},P)$.    The basic idea of importance sampling is to change the probability $P$   so that the estimation of $E[X;P]$ is easier. Choose a random variable   $L\geq 0$ such that $E[L;P]=1$ and that $P$-almost everywhere   $L(\omega)\neq 0$. The variate $L$ defines another probability   $P^{(L)}=L\, P$ that satisfies $$      \mathbf{E}[X;P] = \mathbf{E}\left[\frac{X}{L};P^{(L)}\right].  $$ I was wondering if $P^{(L)}$ is a probability measure on $\mathbb{R}$ induced by $L$ from $\Omega$? How shall $\mathbf{E}\left[\frac{X}{L};P^{(L)}\right]$ be understood as an integral? Why is it true that $      \mathbf{E}[X;P] = \mathbf{E}\left[\frac{X}{L};P^{(L)}\right]$? Thanks!","From Wikipedia Let $X:\Omega\to \mathbb{R}$ be a random variable in some probability space $(\Omega,\mathcal{F},P)$.    The basic idea of importance sampling is to change the probability $P$   so that the estimation of $E[X;P]$ is easier. Choose a random variable   $L\geq 0$ such that $E[L;P]=1$ and that $P$-almost everywhere   $L(\omega)\neq 0$. The variate $L$ defines another probability   $P^{(L)}=L\, P$ that satisfies $$      \mathbf{E}[X;P] = \mathbf{E}\left[\frac{X}{L};P^{(L)}\right].  $$ I was wondering if $P^{(L)}$ is a probability measure on $\mathbb{R}$ induced by $L$ from $\Omega$? How shall $\mathbf{E}\left[\frac{X}{L};P^{(L)}\right]$ be understood as an integral? Why is it true that $      \mathbf{E}[X;P] = \mathbf{E}\left[\frac{X}{L};P^{(L)}\right]$? Thanks!",,"['statistics', 'probability-theory']"
87,Calculating percentage,Calculating percentage,,"I'm wondering if my answer for C would be correct and if I was understanding A correctly? I heard drawing venn diagram could help but I'm not sure how to convert the numbers to a diagram. A quality-control program at a plastic bottle production line involves inspecting finished bottles for flaws such as microscopic holes. The proportion of bottles that actually have such a flaw is only 0.0002. If a bottle has a flaw, the probability is 0.995 that it will fail the inspection. If a bottle does not have a flaw, the probability is 0.99 that it will pass the inspection. $P(F)=$ fails inspection $P(P)=$ Passes inspection $P(f)=$ has flaw. a. If a bottle fails inspection, what is the probability that it has a flaw? $P(F\cap f) = P(F) \times P(f|F)$ c. If a bottle passes inspection, what is the probability that it does not have a flaw? $P(P \cap f^c) = \frac{(0.99)}{(0.995)} = 0.9949$","I'm wondering if my answer for C would be correct and if I was understanding A correctly? I heard drawing venn diagram could help but I'm not sure how to convert the numbers to a diagram. A quality-control program at a plastic bottle production line involves inspecting finished bottles for flaws such as microscopic holes. The proportion of bottles that actually have such a flaw is only 0.0002. If a bottle has a flaw, the probability is 0.995 that it will fail the inspection. If a bottle does not have a flaw, the probability is 0.99 that it will pass the inspection. $P(F)=$ fails inspection $P(P)=$ Passes inspection $P(f)=$ has flaw. a. If a bottle fails inspection, what is the probability that it has a flaw? $P(F\cap f) = P(F) \times P(f|F)$ c. If a bottle passes inspection, what is the probability that it does not have a flaw? $P(P \cap f^c) = \frac{(0.99)}{(0.995)} = 0.9949$",,"['statistics', 'problem-solving']"
88,"Example of a joint density function $f(x,y)$ such that $E(Y|x)=\ln(x)$:",Example of a joint density function  such that :,"f(x,y) E(Y|x)=\ln(x)","Let $X$ and $Y$ be jointly continuous random variables with $X$ taking values in some $D$. Give an example of a joint density function $f(x,y)$ such that $E(Y|x)=\ln(x)$: I have tried so many functions but none seem to fit! Is there a trick/method/theorem I am supposed to use? I know that: $$\int_c^d\int_a^{b(x)}\, f(x,y)\, dy\, dx=1$$ and $$E[Y|x]=\int_a^{b(x)}\,y\cdot  f_{Y|x}(y)\, dy=\ln x$$ (so I guess that  $$f_{Y|x}(y)=\frac{1}{y^2}$$ and $$b(x)=x\, , a=0$$ but then I can't find suitable $f(x,y)$ and $f_X(x)$.) Also, $$\int_a^{b(x)}f(x,y)\, dy=f_{X}(x)$$ and  $$f_{Y|x}(y)=\dfrac{f(x,y)}{f_X(x)}.$$ Please help me!","Let $X$ and $Y$ be jointly continuous random variables with $X$ taking values in some $D$. Give an example of a joint density function $f(x,y)$ such that $E(Y|x)=\ln(x)$: I have tried so many functions but none seem to fit! Is there a trick/method/theorem I am supposed to use? I know that: $$\int_c^d\int_a^{b(x)}\, f(x,y)\, dy\, dx=1$$ and $$E[Y|x]=\int_a^{b(x)}\,y\cdot  f_{Y|x}(y)\, dy=\ln x$$ (so I guess that  $$f_{Y|x}(y)=\frac{1}{y^2}$$ and $$b(x)=x\, , a=0$$ but then I can't find suitable $f(x,y)$ and $f_X(x)$.) Also, $$\int_a^{b(x)}f(x,y)\, dy=f_{X}(x)$$ and  $$f_{Y|x}(y)=\dfrac{f(x,y)}{f_X(x)}.$$ Please help me!",,"['probability', 'statistics', 'probability-theory', 'integration']"
89,Finding $E\Bigl(\overline{Y^2}\Bigm|\overline{Y\vphantom{Y^2}}\Bigr)$ by Basu's theorem?,Finding  by Basu's theorem?,E\Bigl(\overline{Y^2}\Bigm|\overline{Y\vphantom{Y^2}}\Bigr),"Suppose $Y_1,\ldots,Y_n$ are a random sample of normal distribution $\mathcal{N}(\mu,1)$. If $\overline{Y^2}=\displaystyle\frac{1}{n}\sum_{i=1}^n Y_i^2$, how can I find $E\Bigl(\overline{Y^2}\Bigm|\overline{Y\vphantom{Y^2}}\Bigr)$ by Basu's theorem ?","Suppose $Y_1,\ldots,Y_n$ are a random sample of normal distribution $\mathcal{N}(\mu,1)$. If $\overline{Y^2}=\displaystyle\frac{1}{n}\sum_{i=1}^n Y_i^2$, how can I find $E\Bigl(\overline{Y^2}\Bigm|\overline{Y\vphantom{Y^2}}\Bigr)$ by Basu's theorem ?",,"['probability', 'statistics']"
90,Understand simple problem on inflation,Understand simple problem on inflation,,"If the average food basket costs $100$ euros at 2nd quarter prices, how much will it cost for 3rd quarter prices? Below is the graph. I know that the answer is $97$ and can be achieved using follows: $$100 \left( 1 - \left(\frac{97.5-95}{95}\right)\right)$$ But can someone explain me the logic behind this?","If the average food basket costs $100$ euros at 2nd quarter prices, how much will it cost for 3rd quarter prices? Below is the graph. I know that the answer is $97$ and can be achieved using follows: $$100 \left( 1 - \left(\frac{97.5-95}{95}\right)\right)$$ But can someone explain me the logic behind this?",,['statistics']
91,What is the reason to define the variance? [duplicate],What is the reason to define the variance? [duplicate],,"This question already has answers here : Closed 11 years ago . Possible Duplicate: Usefulness of Variance i know Standard deviation is equal to a constant times the shortest distance from the point $(x_1,x,_2,\ldots,x_n)$ to a line $(r_1,r_2,r_3,\ldots,r_n)$ where the r is equal provide that these objects is in n-dimensional euclidean metric space, it is technically called the geometrical interpretation of the standard deviaiton. My question is, i know the standared deviation have its remarkable significance in both probability and statistics, so what is the reason to define another term called the variance","This question already has answers here : Closed 11 years ago . Possible Duplicate: Usefulness of Variance i know Standard deviation is equal to a constant times the shortest distance from the point $(x_1,x,_2,\ldots,x_n)$ to a line $(r_1,r_2,r_3,\ldots,r_n)$ where the r is equal provide that these objects is in n-dimensional euclidean metric space, it is technically called the geometrical interpretation of the standard deviaiton. My question is, i know the standared deviation have its remarkable significance in both probability and statistics, so what is the reason to define another term called the variance",,"['probability', 'statistics']"
92,Looking for the Name of this property: $\mathsf{P}\left(X \leqslant x\right) = \mathsf{P}\left(h(X) \leqslant h(x)\right)$,Looking for the Name of this property:,\mathsf{P}\left(X \leqslant x\right) = \mathsf{P}\left(h(X) \leqslant h(x)\right),$h(\cdot)$ denotes a strict monotonic increasing transformation such as $\log$. Another inequality I do not quite get is that $$\mathsf{P}\left(h(X) \le h(x)\right) \ge \mathsf{P}\left(X \le h(x)\right)$$ Some help would be very much appreciated!,$h(\cdot)$ denotes a strict monotonic increasing transformation such as $\log$. Another inequality I do not quite get is that $$\mathsf{P}\left(h(X) \le h(x)\right) \ge \mathsf{P}\left(X \le h(x)\right)$$ Some help would be very much appreciated!,,"['probability', 'statistics', 'probability-theory', 'terminology']"
93,example of the vector (bounding expectation of a form in Rademacher functions),example of the vector (bounding expectation of a form in Rademacher functions),,"Let $x\in R^{2m}$ such that $x_1+\ldots+x_{2m}=0$ and let $r_i, i=1, \ldots, 2m$ be Rademacher functions, i.e. $P(r_i=1)=P(r_i=-1)=1/2$. I would like to find an example of the vector $x$ such that $E(\sum_{i=1}^{2m}x_ir_i)^{2q}\geq C\sqrt{2q}\|x\|_2$.","Let $x\in R^{2m}$ such that $x_1+\ldots+x_{2m}=0$ and let $r_i, i=1, \ldots, 2m$ be Rademacher functions, i.e. $P(r_i=1)=P(r_i=-1)=1/2$. I would like to find an example of the vector $x$ such that $E(\sum_{i=1}^{2m}x_ir_i)^{2q}\geq C\sqrt{2q}\|x\|_2$.",,"['probability', 'combinatorics', 'statistics']"
94,Generating a random monotonically increasing polynomial?,Generating a random monotonically increasing polynomial?,,"Given a polynomial $y : \mathbb{R} \mapsto  \mathbb{R}$ of degree $p$: $$ y(x) = \sum_{k=0}^p c_k\, x^k,$$ can a random set of coefficients $\{c_0, \cdots ,c_p\}$ be generated such that $y$ is monotonically increasing for all $x \in [0,1]$? Alternatively, can a probabilistic bound on the monotonicity of $y$ be shown?  For example, if  $n$ tests of the nonnegativity of the slope of $y$ are made at different locations $\{x_1,\cdots, x_n\}$, can an upper bound be placed on the probability that $y$ is not monotonically increasing? Finally, if $\epsilon$ is this bound, then can $\epsilon$ be written as a function of $n$ and $p$?","Given a polynomial $y : \mathbb{R} \mapsto  \mathbb{R}$ of degree $p$: $$ y(x) = \sum_{k=0}^p c_k\, x^k,$$ can a random set of coefficients $\{c_0, \cdots ,c_p\}$ be generated such that $y$ is monotonically increasing for all $x \in [0,1]$? Alternatively, can a probabilistic bound on the monotonicity of $y$ be shown?  For example, if  $n$ tests of the nonnegativity of the slope of $y$ are made at different locations $\{x_1,\cdots, x_n\}$, can an upper bound be placed on the probability that $y$ is not monotonically increasing? Finally, if $\epsilon$ is this bound, then can $\epsilon$ be written as a function of $n$ and $p$?",,"['analysis', 'statistics', 'polynomials', 'nonlinear-optimization']"
95,On the semantics of the gamma distribution,On the semantics of the gamma distribution,,"Good Morning. I have recently been faced with modeling a quantity that is best modeled via a Gamma distribution. I have noticed that, in the characterization of the distribution via the parameters $k$ and $θ$, the mean of the distribution is $kθ$, whereas the ""top"" of the distribution, representing the point that is sampled with the highest probability, is found at point $θ( k - 1)$. This contradicts my intuition. If the mean represents the expected value of the distribution, then why is it not placed at the ""top"" of the distribution, much like in the normal distribution? Thanks, Jason","Good Morning. I have recently been faced with modeling a quantity that is best modeled via a Gamma distribution. I have noticed that, in the characterization of the distribution via the parameters $k$ and $θ$, the mean of the distribution is $kθ$, whereas the ""top"" of the distribution, representing the point that is sampled with the highest probability, is found at point $θ( k - 1)$. This contradicts my intuition. If the mean represents the expected value of the distribution, then why is it not placed at the ""top"" of the distribution, much like in the normal distribution? Thanks, Jason",,"['probability', 'statistics', 'probability-theory']"
96,Scale invariance of power law distribution.,Scale invariance of power law distribution.,,"On wikipedia i have find this statement: ...it is scale invariant, and the only continuous distribution that fits this (scale invariance) is one whose logarithm is uniformly distributed. how can be proven?","On wikipedia i have find this statement: ...it is scale invariant, and the only continuous distribution that fits this (scale invariance) is one whose logarithm is uniformly distributed. how can be proven?",,"['probability', 'statistics']"
97,Evaluating 'combinatorial' sum,Evaluating 'combinatorial' sum,,"Help me please to calculate the following sum. I have seen such kind of formulas in the papers related to combinatorics, specifically 'trees'. I am curious how to calculate or approximate this sum: Let $n \in N$, $q\geq 2$ $$ \sum_{m=-n}^n m^q {n \choose (m+n)/2}=\Gamma(n+1)\sum_{m=-n}^nm^q\frac{1}{\Gamma(n/2-m/2+1)\Gamma(n/2+m/2+1)} $$","Help me please to calculate the following sum. I have seen such kind of formulas in the papers related to combinatorics, specifically 'trees'. I am curious how to calculate or approximate this sum: Let $n \in N$, $q\geq 2$ $$ \sum_{m=-n}^n m^q {n \choose (m+n)/2}=\Gamma(n+1)\sum_{m=-n}^nm^q\frac{1}{\Gamma(n/2-m/2+1)\Gamma(n/2+m/2+1)} $$",,"['combinatorics', 'sequences-and-series', 'statistics', 'binomial-coefficients', 'trees']"
98,Expectation of quadratic form,Expectation of quadratic form,,I have a random sample of size 3 denoted by X below and it comes from a normal distribution with mean 7 and variance 14. I have the matrix A shown below. I am looking for E[Q]. I know that E[Q] = 1/sigma^2 * E[Q]. The formula from the textbook for E[Q] is shown below where sigma is the variance-covariance matrix. I am having trouble with the following two items: What is sigma? I am unsure how to find the variance-covariance matrix. Is it simply just a 3x3 matrix with 14 on the diagonals? What is $\mu$? Is it [7 7 7]? Thanks for the help.,I have a random sample of size 3 denoted by X below and it comes from a normal distribution with mean 7 and variance 14. I have the matrix A shown below. I am looking for E[Q]. I know that E[Q] = 1/sigma^2 * E[Q]. The formula from the textbook for E[Q] is shown below where sigma is the variance-covariance matrix. I am having trouble with the following two items: What is sigma? I am unsure how to find the variance-covariance matrix. Is it simply just a 3x3 matrix with 14 on the diagonals? What is $\mu$? Is it [7 7 7]? Thanks for the help.,,"['probability', 'statistics']"
99,Point estimation vs hypothesis testing and interval estimation,Point estimation vs hypothesis testing and interval estimation,,Can someone explain what are point estimators good for? Hypothesis tests and interval estimations give a fuzzy answer in terms of something sort of like a probability of the value being in some interval. In applied statistics why would you look at a single number or point estimator rather than these intervals?,Can someone explain what are point estimators good for? Hypothesis tests and interval estimations give a fuzzy answer in terms of something sort of like a probability of the value being in some interval. In applied statistics why would you look at a single number or point estimator rather than these intervals?,,['statistics']
