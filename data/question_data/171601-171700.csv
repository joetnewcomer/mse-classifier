,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Confidence interval in statistics,Confidence interval in statistics,,"I really need help on this question. I am taking an introductory statistics class, and here is the question: A taxi company wants to determine if customers will accept self driving cars. The company authorized a survey and subjects were to be selected at random from their current customer database (you can assume independence and a large population). Using the entire customer database, the company determined that the amount paid per trip was normally distributed with a mean of $\$50$ and a standar deviation of $\$5$ and $98\%$ of customers owned a cellphone and $59\%$ of the customers used paypal. The company then selected a simple random sample of size $27$ and conducted in person interviews. Analysis of the sample revealed that the $27$ customers paid an average of $\$53$ with a standard deviation of $\$12$, $47\%$ slept during their ride, and $55\%$ used paypal and 64% are males. Please construct a $95\%$ confidence interval for the population proportion of customers who slept during their ride. My question is: why is the question asking for population proportion when the scenario is giving information that indicates I would have to use a t-test for sample means? I am confused whether to use $2.056$ or $1.96$ as the multiplier to construct my confidence interval. I am guessing that I would have to use $1.96$ as my multiplier since the question is asking for population proportion instead of sample means. (and the fact that I would only use the degree of freedom to find my multiplier when they're asking for confidence levels not intervals)? If any of you guys could approve or disprove my line of thinking, I would greatly appreciate it.  Thanks","I really need help on this question. I am taking an introductory statistics class, and here is the question: A taxi company wants to determine if customers will accept self driving cars. The company authorized a survey and subjects were to be selected at random from their current customer database (you can assume independence and a large population). Using the entire customer database, the company determined that the amount paid per trip was normally distributed with a mean of $\$50$ and a standar deviation of $\$5$ and $98\%$ of customers owned a cellphone and $59\%$ of the customers used paypal. The company then selected a simple random sample of size $27$ and conducted in person interviews. Analysis of the sample revealed that the $27$ customers paid an average of $\$53$ with a standard deviation of $\$12$, $47\%$ slept during their ride, and $55\%$ used paypal and 64% are males. Please construct a $95\%$ confidence interval for the population proportion of customers who slept during their ride. My question is: why is the question asking for population proportion when the scenario is giving information that indicates I would have to use a t-test for sample means? I am confused whether to use $2.056$ or $1.96$ as the multiplier to construct my confidence interval. I am guessing that I would have to use $1.96$ as my multiplier since the question is asking for population proportion instead of sample means. (and the fact that I would only use the degree of freedom to find my multiplier when they're asking for confidence levels not intervals)? If any of you guys could approve or disprove my line of thinking, I would greatly appreciate it.  Thanks",,['statistics']
1,Find conditional expectation $E(\xi|\xi^2)$,Find conditional expectation,E(\xi|\xi^2),"The problem is to find the conditional expectation $E(\xi | \xi^2)$, where $\xi$ is a uniform random value on $[-1, 1]$. The approach I tried to implement is to prove somehow that $E(\xi|\xi^2)=E(-\xi|\xi^2)$, since $\xi$ is from $[-1,1]$, that is, symmetrical, but I don't know how to continue the solution.","The problem is to find the conditional expectation $E(\xi | \xi^2)$, where $\xi$ is a uniform random value on $[-1, 1]$. The approach I tried to implement is to prove somehow that $E(\xi|\xi^2)=E(-\xi|\xi^2)$, since $\xi$ is from $[-1,1]$, that is, symmetrical, but I don't know how to continue the solution.",,"['probability', 'statistics', 'conditional-expectation', 'uniform-distribution']"
2,What statistical test to use to compare effectiveness of drugs used to fight a disease,What statistical test to use to compare effectiveness of drugs used to fight a disease,,So let's say that I want to collect some data on a sample of people who has the same type of disease. One group will take a drug A Another one will take drug B Third type will take both of them. I want to perform a statistical test to see whether taking both of the drugs simultaneously results in getting back to health quicker. Can anyone tell me what test would I use and what hypothesis would I state? Assumptions that I would make? I need to plan a research project (Without actually doing it) and I thought about doing something like this but I am not sure how to start.,So let's say that I want to collect some data on a sample of people who has the same type of disease. One group will take a drug A Another one will take drug B Third type will take both of them. I want to perform a statistical test to see whether taking both of the drugs simultaneously results in getting back to health quicker. Can anyone tell me what test would I use and what hypothesis would I state? Assumptions that I would make? I need to plan a research project (Without actually doing it) and I thought about doing something like this but I am not sure how to start.,,"['statistics', 'mathematical-modeling']"
3,Find a and b such that $ P(\sigma_*^2 < a\sigma ^2) =0.025$ and $ P(\sigma_*^2 > b\sigma ^2) =0.025$,Find a and b such that  and, P(\sigma_*^2 < a\sigma ^2) =0.025  P(\sigma_*^2 > b\sigma ^2) =0.025,"Let $X_1, X_2 ... X_n \text{ ~ } N(\mu, \sigma^{2})$ be identically distributed and consider the estimate  $$ \sigma_* ^{2} = \frac{1}{n} \sum\limits_{i=1}^n (X_i - \mu)^2 $$ for the variance. Find number $a$ and $b \in \mathbb{R} $  such that $ P(\sigma_*^2 < a\sigma ^2) =0.025$ and  $ P(\sigma_*^2 > b\sigma ^2) =0.025$ This is all that was given in the question, and I'm not sure how to approach it, I was thinking of using the equation for $\sigma ^2$ but its the same (or I think it is) as $ \sigma_* ^{2}$. This question is in relation to hypothesis testing and so far I've looked at z and t tests, but I'm not sure how i can standardise these two values. Any help is much appreciated.","Let $X_1, X_2 ... X_n \text{ ~ } N(\mu, \sigma^{2})$ be identically distributed and consider the estimate  $$ \sigma_* ^{2} = \frac{1}{n} \sum\limits_{i=1}^n (X_i - \mu)^2 $$ for the variance. Find number $a$ and $b \in \mathbb{R} $  such that $ P(\sigma_*^2 < a\sigma ^2) =0.025$ and  $ P(\sigma_*^2 > b\sigma ^2) =0.025$ This is all that was given in the question, and I'm not sure how to approach it, I was thinking of using the equation for $\sigma ^2$ but its the same (or I think it is) as $ \sigma_* ^{2}$. This question is in relation to hypothesis testing and so far I've looked at z and t tests, but I'm not sure how i can standardise these two values. Any help is much appreciated.",,"['probability', 'statistics', 'normal-distribution', 'hypothesis-testing', 'variance']"
4,Estimating joint probability distributions from marginal distributions,Estimating joint probability distributions from marginal distributions,,"I'm currently studying a problem where I have well determined marginal distributions for N random variables that conform to the beta distribution and very few samples of the joint distribution.  I want to leverage the marginal distributions to estimate the joint distribution is this feasible? For a concrete case, there may be 10k to 100k samples of the marginal distributions and fewer than 100 samples of any given joint distribution.","I'm currently studying a problem where I have well determined marginal distributions for N random variables that conform to the beta distribution and very few samples of the joint distribution.  I want to leverage the marginal distributions to estimate the joint distribution is this feasible? For a concrete case, there may be 10k to 100k samples of the marginal distributions and fewer than 100 samples of any given joint distribution.",,"['statistics', 'statistical-inference']"
5,Calculate Confidence Interval With 20 Samples,Calculate Confidence Interval With 20 Samples,,"I have 20 samples of network latencies. Each sample consists of 5000 numbers (in milliseconds). I would like to find a 95% confidence interval for average (Mean), 90th percentile, 95th percentile and 99th percentile for network latencies. I have measured the above as follows: For a 95% confidence interval for average, I need to find average for each sample. So, I will get 20 values, each represents an average for one sample. Then, I calculate Mean (M) and Standard Deviation (SD) for those 20 values and apply the following formula to obtain a 95% confidence interval for average network latencies: a 95% confidence interval = M ± 1.96 (SD/√20) For a 95% confidence interval for 90th percentile. First I need to find ascending order for each sample. Then, the 90th value for each sample is picked. So, we will have 20 values, each represents 90th percentile for one sample. Then, I calculate Mean (M) and Standard Deviation (SD) for those 20 values and apply the following formula to obtain a 95% confidence interval for 90th percentile network latencies: a 95% confidence interval = M ± 1.96 (SD/√20) Is it true?","I have 20 samples of network latencies. Each sample consists of 5000 numbers (in milliseconds). I would like to find a 95% confidence interval for average (Mean), 90th percentile, 95th percentile and 99th percentile for network latencies. I have measured the above as follows: For a 95% confidence interval for average, I need to find average for each sample. So, I will get 20 values, each represents an average for one sample. Then, I calculate Mean (M) and Standard Deviation (SD) for those 20 values and apply the following formula to obtain a 95% confidence interval for average network latencies: a 95% confidence interval = M ± 1.96 (SD/√20) For a 95% confidence interval for 90th percentile. First I need to find ascending order for each sample. Then, the 90th value for each sample is picked. So, we will have 20 values, each represents 90th percentile for one sample. Then, I calculate Mean (M) and Standard Deviation (SD) for those 20 values and apply the following formula to obtain a 95% confidence interval for 90th percentile network latencies: a 95% confidence interval = M ± 1.96 (SD/√20) Is it true?",,['statistics']
6,statistics range rule of thumb confidence interval,statistics range rule of thumb confidence interval,,Suppose that the minimum and maximum ages for typical textbooks currently used in college courses are 0 and 8 years. Use the range rule of thumb to estimate the standard deviation. Standard deviation = I have gotten max - min / 4 = 8 - 0 / 4                  = 2 Find the size of the sample required to estimage the mean age of textbooks currently used in college courses. Assume that you want 98% confidence that the sample mean is within 0.4 year of the population mean. Required sample size = I have no clue how to get the required sample size What are the correct answers?,Suppose that the minimum and maximum ages for typical textbooks currently used in college courses are 0 and 8 years. Use the range rule of thumb to estimate the standard deviation. Standard deviation = I have gotten max - min / 4 = 8 - 0 / 4                  = 2 Find the size of the sample required to estimage the mean age of textbooks currently used in college courses. Assume that you want 98% confidence that the sample mean is within 0.4 year of the population mean. Required sample size = I have no clue how to get the required sample size What are the correct answers?,,"['statistics', 'means', 'confidence-interval']"
7,The difference between $\frac{\bar X-\mu}{S´/\sqrt{n}}$ and $\frac{\bar X-\mu}{s´/\sqrt{n}}$,The difference between  and,\frac{\bar X-\mu}{S´/\sqrt{n}} \frac{\bar X-\mu}{s´/\sqrt{n}},"Imagine we're dealing with a normal population, with known mean $\mu$, but unknown variance $\sigma^2$. We know that $\frac{\bar X-\mu}{S'/\sqrt{n}}\sim t(n-1)$. (T-Student distribution) Now, suppose someone asks us to calculate $P(\bar X >c)$ and give us $s'=3$. I've seen people write the following: $P(\bar X >c)=P(\frac{\bar X-\mu}{s'/\sqrt{n}}<\frac{c-\mu}{s'/\sqrt{n}})=P(T>\frac{c-\mu}{s'/\sqrt{n}})$, where $T\sim t(n-1)$. This sequence of equalities doesn't seem to be correct, since a draw of $S'$, i.e. $s'$, is not the same as the r.v.  $S'$. If it's correct, why is that? Any help would be appreciated.","Imagine we're dealing with a normal population, with known mean $\mu$, but unknown variance $\sigma^2$. We know that $\frac{\bar X-\mu}{S'/\sqrt{n}}\sim t(n-1)$. (T-Student distribution) Now, suppose someone asks us to calculate $P(\bar X >c)$ and give us $s'=3$. I've seen people write the following: $P(\bar X >c)=P(\frac{\bar X-\mu}{s'/\sqrt{n}}<\frac{c-\mu}{s'/\sqrt{n}})=P(T>\frac{c-\mu}{s'/\sqrt{n}})$, where $T\sim t(n-1)$. This sequence of equalities doesn't seem to be correct, since a draw of $S'$, i.e. $s'$, is not the same as the r.v.  $S'$. If it's correct, why is that? Any help would be appreciated.",,"['probability', 'statistics', 'statistical-inference']"
8,Re-sample a sparse matrix - keeping row- and column-sums constant,Re-sample a sparse matrix - keeping row- and column-sums constant,,"Lets say I have a square matrix $A_{n\times n}$. It is a sparse matrix and let's assume that it only holds 0's and 1's. I wish to generate another matrix $B_{n\times n}$ with the following properties: All the row-sums and column-sums (or equivalently, the norms) are kept constant, i.e equal between $A$ & $B$. None of the 1's are in the same location in $B$ as they were in $A$. The diagonal elements of $B$ are all zero (this is also true of $A$). So basically, I want to re-sample a matrix (or a set of matrices $B_{i}$) that keeps that above resemblance and difference with $A$. Of course there are no guarantees that this actually have a solution, but given a high level of sparsity and favorable distribution of the 1's, It should be possible. The matrix correspond to a social network with unidirectional connections between the $n$ users ($a_{ij}$ 'likes' $a_{kl}$), and I need to sample a new network over the $n$ users that keeps the distribution of 'likes' and 'is liked by' constant over the users. Anyone with an idea of how to systematically generate this? I guess I could brute force it, moving the connections around and test for validity. But there might be some theoretical framework to formalize the process a bit.","Lets say I have a square matrix $A_{n\times n}$. It is a sparse matrix and let's assume that it only holds 0's and 1's. I wish to generate another matrix $B_{n\times n}$ with the following properties: All the row-sums and column-sums (or equivalently, the norms) are kept constant, i.e equal between $A$ & $B$. None of the 1's are in the same location in $B$ as they were in $A$. The diagonal elements of $B$ are all zero (this is also true of $A$). So basically, I want to re-sample a matrix (or a set of matrices $B_{i}$) that keeps that above resemblance and difference with $A$. Of course there are no guarantees that this actually have a solution, but given a high level of sparsity and favorable distribution of the 1's, It should be possible. The matrix correspond to a social network with unidirectional connections between the $n$ users ($a_{ij}$ 'likes' $a_{kl}$), and I need to sample a new network over the $n$ users that keeps the distribution of 'likes' and 'is liked by' constant over the users. Anyone with an idea of how to systematically generate this? I guess I could brute force it, moving the connections around and test for validity. But there might be some theoretical framework to formalize the process a bit.",,"['linear-algebra', 'combinatorics', 'statistics', 'graph-theory']"
9,Klinefelter Syndrome Statistics Probability,Klinefelter Syndrome Statistics Probability,,Description of the question is in the picture provided b) What is the probability that at least one and at most three of the male births have Klinefelter Syndrome? Give your answer to 4 decimal places. What is the correct solution and answer?,Description of the question is in the picture provided b) What is the probability that at least one and at most three of the male births have Klinefelter Syndrome? Give your answer to 4 decimal places. What is the correct solution and answer?,,"['probability', 'statistics', 'binomial-distribution']"
10,Can a 2 paired sample sets of size 15 (30 readings in total) be assumed to have normal distribution when finding confidence intervals?,Can a 2 paired sample sets of size 15 (30 readings in total) be assumed to have normal distribution when finding confidence intervals?,,"I've seen that for large sample sizes (about 30 or more) the distribution tends to a normal distribution, but what if I have 2 sets of 15 which are paired? Would that count as a normal distribution, or would I need to use a t-distribution to approximate confidence intervals for the difference between their means? Thanks in advance.","I've seen that for large sample sizes (about 30 or more) the distribution tends to a normal distribution, but what if I have 2 sets of 15 which are paired? Would that count as a normal distribution, or would I need to use a t-distribution to approximate confidence intervals for the difference between their means? Thanks in advance.",,"['statistics', 'probability-distributions', 'normal-distribution', 'confidence-interval']"
11,finding minimum order statistic of uniform distirbution,finding minimum order statistic of uniform distirbution,,I am trying to find the minimum of a uniform distribution of $Y_1 .. Y_n$ idd from 0 to theta I understand how to derive it $n(1-F_{y_1}(y))^{n-1}f_{y_1}$ But the solution says that it is the following $\frac{n(1-\theta)^{n-1}}{\theta^n}$ And I don't understand where the $\theta^n$ in the denominator came from,I am trying to find the minimum of a uniform distribution of $Y_1 .. Y_n$ idd from 0 to theta I understand how to derive it $n(1-F_{y_1}(y))^{n-1}f_{y_1}$ But the solution says that it is the following $\frac{n(1-\theta)^{n-1}}{\theta^n}$ And I don't understand where the $\theta^n$ in the denominator came from,,"['statistics', 'uniform-distribution', 'order-statistics']"
12,Easy street poisson distribution accident question,Easy street poisson distribution accident question,,"The intersection of Easy Street and 69th Avenue averages 3 accidents every two days. Assuming the number of accidents follows a Poisson distribution: What is the probability that there are exactly 8 accidents at that intersection in a week (7 days)? Using Poisson's formula: e^-9 * 9^8 / 8! I got 9 since 3 accidents happens in two days so 3 * 3 equals a lambda of 9 in 6 days. Got it wrong, what is the correct answer and solution? Thinking of 10.5 since you add 1.5 from so that's the lambda for 7 days","The intersection of Easy Street and 69th Avenue averages 3 accidents every two days. Assuming the number of accidents follows a Poisson distribution: What is the probability that there are exactly 8 accidents at that intersection in a week (7 days)? Using Poisson's formula: e^-9 * 9^8 / 8! I got 9 since 3 accidents happens in two days so 3 * 3 equals a lambda of 9 in 6 days. Got it wrong, what is the correct answer and solution? Thinking of 10.5 since you add 1.5 from so that's the lambda for 7 days",,"['probability', 'statistics', 'poisson-distribution']"
13,How do I show that this is a T distribution?,How do I show that this is a T distribution?,,"$Y_1,...,Y_n$ is a random sample from a $N(\theta,\theta)$ How do I show that $$\frac{\sqrt{n}(\bar{Y}-\theta)}{s}$$ is a t-distribution with n-1 degrees of freedom? By the way, $$s^2=\frac{\sum_{i=1}^n(Y_i-\bar{Y})^2}{n-1}$$","$Y_1,...,Y_n$ is a random sample from a $N(\theta,\theta)$ How do I show that $$\frac{\sqrt{n}(\bar{Y}-\theta)}{s}$$ is a t-distribution with n-1 degrees of freedom? By the way, $$s^2=\frac{\sum_{i=1}^n(Y_i-\bar{Y})^2}{n-1}$$",,['statistics']
14,Hypothesis Testing for uniform distribution,Hypothesis Testing for uniform distribution,,"Let $X_i; i=1,2,3,\dots,n$ be iid samples from the uniform $U(0, θ)$ distribution, with $θ$ being the unknown parameter of interest. Consider the testing problem $H_0 : θ = θ_0$ versus $H_1 : θ > θ_0.$ One possible test is to reject $H_0$ when $X_{max} > C$ for some $C > 0.$ $1.$ Find $C$ in terms of the proposed  $θ_0$ such that the probability of Type I error is 0.05. $2.$  Use the duality between hypothesis testing and conﬁdence interval to write down a level 0.95 lower conﬁdence bound for $θ.$ That is, ﬁnd $L(X_1,\dots , X_n)$ such that $P_θ(θ ≥ L(X_1, \dots , X_n)) = 0.95$ for all $θ > 0.$ $3.$ Choose $C$ as in (a). What is the power of the test in the case  of $θ_0 = 1,\, θ = 2,\, n = 10?$ I have done $1$ using $P(X_{max}>C|θ = θ_0) = 1 - (C/θ_0)^n = 0.05;$ can anyone tell me if this is right? Also, how to do questions $2$ and $3$? Thanks so much!!!!","Let $X_i; i=1,2,3,\dots,n$ be iid samples from the uniform $U(0, θ)$ distribution, with $θ$ being the unknown parameter of interest. Consider the testing problem $H_0 : θ = θ_0$ versus $H_1 : θ > θ_0.$ One possible test is to reject $H_0$ when $X_{max} > C$ for some $C > 0.$ $1.$ Find $C$ in terms of the proposed  $θ_0$ such that the probability of Type I error is 0.05. $2.$  Use the duality between hypothesis testing and conﬁdence interval to write down a level 0.95 lower conﬁdence bound for $θ.$ That is, ﬁnd $L(X_1,\dots , X_n)$ such that $P_θ(θ ≥ L(X_1, \dots , X_n)) = 0.95$ for all $θ > 0.$ $3.$ Choose $C$ as in (a). What is the power of the test in the case  of $θ_0 = 1,\, θ = 2,\, n = 10?$ I have done $1$ using $P(X_{max}>C|θ = θ_0) = 1 - (C/θ_0)^n = 0.05;$ can anyone tell me if this is right? Also, how to do questions $2$ and $3$? Thanks so much!!!!",,['statistics']
15,"Find the joint distribution of $U_{1},\ldots,U_{n}$ where $U_{i}=\frac{F(X_{(i)})}{F(X_{(i+1)})}$ and $X_{(i)}$ order static.",Find the joint distribution of  where  and  order static.,"U_{1},\ldots,U_{n} U_{i}=\frac{F(X_{(i)})}{F(X_{(i+1)})} X_{(i)}","Let $X_{(1)}\leq X_{(2)}\leq \cdots X_{(n)}$ be the order statics for a random sample from a continuous distribution with c.d.f. $F(x)$ and density $f(x)$. Define $U_{i}$, $i=1,2\ldots,n,$ by  $$U_{i}=\frac{F(X_{(i)})}{F(X_{(i+1)})}, \quad i=1,\ldots,n-1, \: \mbox{and }\: U_{n}=F(X_{(n)}).$$ Find the joint distribution of $U_{1},\ldots,U_{n}$. Remark: I need a suggestion or someone tell me in which book I can find this exercise or related theory that allows me to solve it.","Let $X_{(1)}\leq X_{(2)}\leq \cdots X_{(n)}$ be the order statics for a random sample from a continuous distribution with c.d.f. $F(x)$ and density $f(x)$. Define $U_{i}$, $i=1,2\ldots,n,$ by  $$U_{i}=\frac{F(X_{(i)})}{F(X_{(i+1)})}, \quad i=1,\ldots,n-1, \: \mbox{and }\: U_{n}=F(X_{(n)}).$$ Find the joint distribution of $U_{1},\ldots,U_{n}$. Remark: I need a suggestion or someone tell me in which book I can find this exercise or related theory that allows me to solve it.",,"['probability-theory', 'statistics', 'probability-distributions', 'order-statistics']"
16,Conduct a Wald test and construct a 95% Wald confidence. Are these sensible?,Conduct a Wald test and construct a 95% Wald confidence. Are these sensible?,,"Question: In a crossover trial comparing a new drug to a standard, $\pi$ denotes the probability that the new one is judged better. it is desired to estimate $\pi$ and test $H_0:\pi=0.50$ against $H_a: \pi \neq 0.50$. In $20$ independent observations, the new drug is better each time. Give the ML estimate of $\pi$. Conduct a Wald test and construct a 95% Wald confidence interval of $\pi$. Are these sensible? I have this so far: Ml: $\hat{\pi} = \frac{y}{n} \rightarrow \frac{20}{20}=1.$ The Wald test is $Z_w = \frac{\hat{\pi}-\pi_o}{\sqrt{\frac{(1-\hat{\pi})\hat{\pi}}{n}}}$ Inputting our values we just get $\rightarrow Z_w = \frac{1-0.50}{\sqrt{\frac{(1-1)0.50}{20}}}=0$ For some reason the solution tells me that it goes to $\infty$, if someone could explain that. The 95% Wald confide interval of $\pi$ is$\rightarrow \hat{\pi} \pm Z_\frac{\alpha}{2} \sqrt{\frac{\hat{\pi}(1-\hat{\pi})}{n}}$. Which is just $ 1 \pm 1.96(0) \rightarrow (1,0) \& (1,0)$ Is this sensible?","Question: In a crossover trial comparing a new drug to a standard, $\pi$ denotes the probability that the new one is judged better. it is desired to estimate $\pi$ and test $H_0:\pi=0.50$ against $H_a: \pi \neq 0.50$. In $20$ independent observations, the new drug is better each time. Give the ML estimate of $\pi$. Conduct a Wald test and construct a 95% Wald confidence interval of $\pi$. Are these sensible? I have this so far: Ml: $\hat{\pi} = \frac{y}{n} \rightarrow \frac{20}{20}=1.$ The Wald test is $Z_w = \frac{\hat{\pi}-\pi_o}{\sqrt{\frac{(1-\hat{\pi})\hat{\pi}}{n}}}$ Inputting our values we just get $\rightarrow Z_w = \frac{1-0.50}{\sqrt{\frac{(1-1)0.50}{20}}}=0$ For some reason the solution tells me that it goes to $\infty$, if someone could explain that. The 95% Wald confide interval of $\pi$ is$\rightarrow \hat{\pi} \pm Z_\frac{\alpha}{2} \sqrt{\frac{\hat{\pi}(1-\hat{\pi})}{n}}$. Which is just $ 1 \pm 1.96(0) \rightarrow (1,0) \& (1,0)$ Is this sensible?",,"['statistics', 'maximum-likelihood']"
17,birthday problem near misses,birthday problem near misses,,"I am working with a case of the Birthday problem with near misses.  (i.e. The probability that there are 2 or more collisions after selecting $n$ elements uniformly from a set of $m$ total elements where a collision is defined as falling within $k$ elements of any previously selected element) I have the eq. from Wikipedia for the near miss formulation: $$ p(n,k,m) = 1 - \frac{(m-nk-1)!}{m^{n-1}(m-n(k+1))!} $$ My problem lies in computing the factorials.  I am working with numbers on the order of: $$ m = 1000 * (2^{32}-1), n = 1000, k = 10000 $$ Even Wolfram Alpha gives up with those values.  Are there any formulations for this problem that don't involve the factorial or any approximation methods that could make this a reasonably commutable problem?  As an extension, is it then also possible to compute/approximate the expected number of collisions after selecting $n$ elements?","I am working with a case of the Birthday problem with near misses.  (i.e. The probability that there are 2 or more collisions after selecting $n$ elements uniformly from a set of $m$ total elements where a collision is defined as falling within $k$ elements of any previously selected element) I have the eq. from Wikipedia for the near miss formulation: $$ p(n,k,m) = 1 - \frac{(m-nk-1)!}{m^{n-1}(m-n(k+1))!} $$ My problem lies in computing the factorials.  I am working with numbers on the order of: $$ m = 1000 * (2^{32}-1), n = 1000, k = 10000 $$ Even Wolfram Alpha gives up with those values.  Are there any formulations for this problem that don't involve the factorial or any approximation methods that could make this a reasonably commutable problem?  As an extension, is it then also possible to compute/approximate the expected number of collisions after selecting $n$ elements?",,"['probability', 'combinatorics', 'statistics', 'computability']"
18,Some simple probability,Some simple probability,,"Brandon has 3 chances to roll a single die. He rolls 2 '6's, and a ‘5’ on his turn. Assuming the dice are fair, what is the probability of (i) ‘6’s on the first two rolls, and a non-six in the third roll, and (ii) getting exactly two ‘6’s? On first glance, I calculated my answer to be i) 1/6 ^3 and ii) 1/6 ^2 . Is this right? I think that each event (roll) is independent. In a game of Monopoly, 2 (fair) six-sided dice are rolled. Every face of each die is labelled 1 to 6. A double occurs if both dice land up on the same number. Instead of rolling both dice together, Brandon rolls one die at a time. The first die lands on a ‘6’. What is the probability that he rolls a double? If the first die lands on either ‘1’, ‘2’, ‘3’, ‘4’, ‘5’ or ‘6’, what is the probability that he rolls a double? My answers to both the questions are 1/6 , because I think that the first event where he rolls a specific number, would make up a P of 1. Thus we only need to calculate the second roll. Am I thinking in the right direction?","Brandon has 3 chances to roll a single die. He rolls 2 '6's, and a ‘5’ on his turn. Assuming the dice are fair, what is the probability of (i) ‘6’s on the first two rolls, and a non-six in the third roll, and (ii) getting exactly two ‘6’s? On first glance, I calculated my answer to be i) 1/6 ^3 and ii) 1/6 ^2 . Is this right? I think that each event (roll) is independent. In a game of Monopoly, 2 (fair) six-sided dice are rolled. Every face of each die is labelled 1 to 6. A double occurs if both dice land up on the same number. Instead of rolling both dice together, Brandon rolls one die at a time. The first die lands on a ‘6’. What is the probability that he rolls a double? If the first die lands on either ‘1’, ‘2’, ‘3’, ‘4’, ‘5’ or ‘6’, what is the probability that he rolls a double? My answers to both the questions are 1/6 , because I think that the first event where he rolls a specific number, would make up a P of 1. Thus we only need to calculate the second roll. Am I thinking in the right direction?",,"['probability', 'statistics', 'dice']"
19,"Calculate $P(A' \cap B)$ and $P(A' \cap B')$ given $P(A) = 1/2$, $P(B) = 1/2$ and $P(A \cup B)= 2/3$","Calculate  and  given ,  and",P(A' \cap B) P(A' \cap B') P(A) = 1/2 P(B) = 1/2 P(A \cup B)= 2/3,We are given that $P(A) = P(B) = 1/2$ and $P(A \cup B)= 2/3$ . I found that events $A$ and $B$ are not mutually exclusive by showing that $P(A \cup B) \neq P(A) + P(B)$ (i.e. $2/3 \neq 1$ ) I also found that the two events are not independent by showing that $P(A \cap B) \neq P(A)P(B)$ (i.e. $1/3 \neq 1/4$ ) Because the events are not mutually exclusive or independent I am having trouble solving for $P(A'\cap B)$ and $P(A' \cap B')$ . I wanted to use the fact that $P(A'\cap B) = P(B) - P(A\cap B)$ but the events must be mutually exclusive to use this. Any suggestions?,We are given that and . I found that events and are not mutually exclusive by showing that (i.e. ) I also found that the two events are not independent by showing that (i.e. ) Because the events are not mutually exclusive or independent I am having trouble solving for and . I wanted to use the fact that but the events must be mutually exclusive to use this. Any suggestions?,P(A) = P(B) = 1/2 P(A \cup B)= 2/3 A B P(A \cup B) \neq P(A) + P(B) 2/3 \neq 1 P(A \cap B) \neq P(A)P(B) 1/3 \neq 1/4 P(A'\cap B) P(A' \cap B') P(A'\cap B) = P(B) - P(A\cap B),"['probability', 'statistics']"
20,The Covariance Matrix after a Functional Transformation,The Covariance Matrix after a Functional Transformation,,"Consider the random p-vector $X$ passed through some non-linear function $g: \mathbb{R}^p \rightarrow \mathbb{R}^q$ so that we have $g(X)$. I would like to compute the $q \times q$ covariance matrix $\text{Cov}(g(X))$. Here is the caveat though: we only have access to $\text{Cov}(X)$ and $g$. I am not sure if this is possible, but is there a way of recovering $\text{Cov}(g(X))$ from just $\text{Cov}(X)$ and $g$ (e.g., by using derivative info for $g$)? Obviously if $g$ is linear, then we have $\text{Cov}(A X) = A \text{Cov}(X) A^T$ for some real matrix $A$, but what if $g$ is non-linear?","Consider the random p-vector $X$ passed through some non-linear function $g: \mathbb{R}^p \rightarrow \mathbb{R}^q$ so that we have $g(X)$. I would like to compute the $q \times q$ covariance matrix $\text{Cov}(g(X))$. Here is the caveat though: we only have access to $\text{Cov}(X)$ and $g$. I am not sure if this is possible, but is there a way of recovering $\text{Cov}(g(X))$ from just $\text{Cov}(X)$ and $g$ (e.g., by using derivative info for $g$)? Obviously if $g$ is linear, then we have $\text{Cov}(A X) = A \text{Cov}(X) A^T$ for some real matrix $A$, but what if $g$ is non-linear?",,"['statistics', 'probability-distributions', 'covariance']"
21,"Consider events $A, B, C$ such that $P(A\mid B) > P(A)$ and $P(B\mid C) > P(B)$.",Consider events  such that  and .,"A, B, C P(A\mid B) > P(A) P(B\mid C) > P(B)","Hey guys I have a problem that I'm having trouble solving. Here is the question: Consider events $A, B, C$ such that $P(A\mid B) > P(A)$ and $P(B\mid C) > P(B)$. Does it follow that $P(A\mid C) > P(A)$? Either prove it to be so or provide a counterexample. And here is what I have so far: Partial solution I would greatly appreciate it if you guys can give a hit or a suggestion to complete the problem.","Hey guys I have a problem that I'm having trouble solving. Here is the question: Consider events $A, B, C$ such that $P(A\mid B) > P(A)$ and $P(B\mid C) > P(B)$. Does it follow that $P(A\mid C) > P(A)$? Either prove it to be so or provide a counterexample. And here is what I have so far: Partial solution I would greatly appreciate it if you guys can give a hit or a suggestion to complete the problem.",,"['probability', 'statistics', 'combinatorial-proofs']"
22,"Let $X \sim \operatorname{Exp}(\theta)$. Show that $Y = \mathrm{e}^{-X/\theta} \sim \operatorname{Uniform}(0,1]$.",Let . Show that .,"X \sim \operatorname{Exp}(\theta) Y = \mathrm{e}^{-X/\theta} \sim \operatorname{Uniform}(0,1]","Let $X \sim \operatorname{Exp}(\theta)$. Show that $Y = \mathrm{e}^{-X/\theta} \sim \operatorname{Uniform}(0,1]$. Hi, I tried to prove this in my tutorial but I got stuck trying to prove it using MGF. Can someone help me with the first method? I was planning to get the $My(t)$ expression to see if I can see any similarity with that of a uniform distribution. Is my train of thoughts correct? I also tried to prove using the pdf of $Y$. I have written my questions in red, and I was wondering how to prove the part where the uniform distribution is valid from $0$ (exclusive) to $1$ inclusive. Attached are my workings, I hope someone can enlighten me on this! thank you! EDIT: **I have taken down my detailed workings as I have verified that the workings are correct.* To answer my above questions, The  $My(t)$ expression can be gotten via a direct integration from E($\mathrm{e}^{tY}$) , and then a comparison of that with the  $Mx(t)$ of a uniform distribution. Also, the Uniform distribution is valid (0,1] because the exp distribution is valid from x more than or equals to 0.","Let $X \sim \operatorname{Exp}(\theta)$. Show that $Y = \mathrm{e}^{-X/\theta} \sim \operatorname{Uniform}(0,1]$. Hi, I tried to prove this in my tutorial but I got stuck trying to prove it using MGF. Can someone help me with the first method? I was planning to get the $My(t)$ expression to see if I can see any similarity with that of a uniform distribution. Is my train of thoughts correct? I also tried to prove using the pdf of $Y$. I have written my questions in red, and I was wondering how to prove the part where the uniform distribution is valid from $0$ (exclusive) to $1$ inclusive. Attached are my workings, I hope someone can enlighten me on this! thank you! EDIT: **I have taken down my detailed workings as I have verified that the workings are correct.* To answer my above questions, The  $My(t)$ expression can be gotten via a direct integration from E($\mathrm{e}^{tY}$) , and then a comparison of that with the  $Mx(t)$ of a uniform distribution. Also, the Uniform distribution is valid (0,1] because the exp distribution is valid from x more than or equals to 0.",,"['probability', 'statistics', 'probability-distributions', 'moment-generating-functions']"
23,What statistical methods would you recommend?,What statistical methods would you recommend?,,"I need to understand how a number of parameters contribute to decomposition in wood. The parameters in question are (1) temperature, (2) humidity and (3) exposure to sun light. We already know that the all contribute, but we don't onow exactly how they affect the specific decomposition process. My need to asses to what extent a 1% increase in either of these parameters will increase the decomposition process. At my disposal, I have a dataset covering some 30 datapoints where the degree of decomposition is recorded together with the three other parameters. What method would you recommend for this type of problem?","I need to understand how a number of parameters contribute to decomposition in wood. The parameters in question are (1) temperature, (2) humidity and (3) exposure to sun light. We already know that the all contribute, but we don't onow exactly how they affect the specific decomposition process. My need to asses to what extent a 1% increase in either of these parameters will increase the decomposition process. At my disposal, I have a dataset covering some 30 datapoints where the degree of decomposition is recorded together with the three other parameters. What method would you recommend for this type of problem?",,"['complex-analysis', 'statistics']"
24,Statistical significance of linear least squares,Statistical significance of linear least squares,,"Consider two data series, $X = (x_1, x_2, \dots, x_n)$ and $Y = (y_1, y_2, \dots, y_n),$ both with mean zero. We use linear regression (ordinary least squares) to regress $Y$ against $X$ (without fitting any intercept), as in $Y = aX + \epsilon$ where $\epsilon$ denotes a series of error terms. It can be shown that $a=\frac{\rho_{XY} \sigma_Y}{\sigma_X}$. Suppose that $\rho_{XY} = 0.01$. Is the resulting value of $a$ statistically significantly different from $0$ at the $95\%$ level if: i. $n=10^2$ ii. $n = 10^3$ iii. $n = 10^4$ I know that I need to find a $p$-value for each of these, and I assume that it will be in turns of $\rho$ and $n$. However, everything I attempt leaves a lingering standard deviation somewhere from the definition of $a$. How do you test significance of a least squares in this case?","Consider two data series, $X = (x_1, x_2, \dots, x_n)$ and $Y = (y_1, y_2, \dots, y_n),$ both with mean zero. We use linear regression (ordinary least squares) to regress $Y$ against $X$ (without fitting any intercept), as in $Y = aX + \epsilon$ where $\epsilon$ denotes a series of error terms. It can be shown that $a=\frac{\rho_{XY} \sigma_Y}{\sigma_X}$. Suppose that $\rho_{XY} = 0.01$. Is the resulting value of $a$ statistically significantly different from $0$ at the $95\%$ level if: i. $n=10^2$ ii. $n = 10^3$ iii. $n = 10^4$ I know that I need to find a $p$-value for each of these, and I assume that it will be in turns of $\rho$ and $n$. However, everything I attempt leaves a lingering standard deviation somewhere from the definition of $a$. How do you test significance of a least squares in this case?",,"['statistics', 'least-squares']"
25,How do I weight Win/Loss Percentage Based on Games Played?,How do I weight Win/Loss Percentage Based on Games Played?,,"I have a spreadsheet I use for tracking my statistics in a game across a number of different factors such as Map, Group Size and Class. In each case, I use the win percentage to determine a rating out of 5 (rounded to the nearest $.01$). For example, when playing with a friend, I have a win percentage of $44\%$, this translates to an overall rating of $2.20$ in my current spreadsheet. Likewise, when playing alone, I have a win percentage of $49.64\%$ which translates to a rating of $2.48$. I have $50$ games played with 2 people and $137$ games played alone. I would like to adjust the rating to reflect the number of games played, such that a win percentage of $50\%$ over $100$ games will be rated higher than $50\%$ over $50$ games. I'm not quite sure how to approach this. Has anyone any suggestions or places I can turn to to help me solve this problem?","I have a spreadsheet I use for tracking my statistics in a game across a number of different factors such as Map, Group Size and Class. In each case, I use the win percentage to determine a rating out of 5 (rounded to the nearest $.01$). For example, when playing with a friend, I have a win percentage of $44\%$, this translates to an overall rating of $2.20$ in my current spreadsheet. Likewise, when playing alone, I have a win percentage of $49.64\%$ which translates to a rating of $2.48$. I have $50$ games played with 2 people and $137$ games played alone. I would like to adjust the rating to reflect the number of games played, such that a win percentage of $50\%$ over $100$ games will be rated higher than $50\%$ over $50$ games. I'm not quite sure how to approach this. Has anyone any suggestions or places I can turn to to help me solve this problem?",,['statistics']
26,Finding a matrix that satisfies a trace constraint,Finding a matrix that satisfies a trace constraint,,"If $U$ is a $p\times q$ matrix such that $U^TU = I_{q},$ $S$ is a $p \times p$ symmetric matrix (either positive definite or nonnegative definite), and $D$ is a $q \times q$ diagonal matrix (can assume all diagonal elements are greater than zero), can we find a symmetric $p \times p$ matrix $A$ such that $$\operatorname{tr}(U^T S U D) = \operatorname{tr}(U^TAU) \text{?}$$ If this were true, then it would allow me to write a conditional distribution arising in a statistical model in a simpler form, corresponding to a known distribution. I think the answer is no, but I'm having a hard time seeing why. If not, I wonder if we have an additional symmetric matrix $\Omega,$ can we find a symmetric $B$ and $A$ as before such that $$\operatorname{tr}(U^T S U D + U^T\Omega U) = \operatorname{tr}(BU^TAU) \text{?}$$","If $U$ is a $p\times q$ matrix such that $U^TU = I_{q},$ $S$ is a $p \times p$ symmetric matrix (either positive definite or nonnegative definite), and $D$ is a $q \times q$ diagonal matrix (can assume all diagonal elements are greater than zero), can we find a symmetric $p \times p$ matrix $A$ such that $$\operatorname{tr}(U^T S U D) = \operatorname{tr}(U^TAU) \text{?}$$ If this were true, then it would allow me to write a conditional distribution arising in a statistical model in a simpler form, corresponding to a known distribution. I think the answer is no, but I'm having a hard time seeing why. If not, I wonder if we have an additional symmetric matrix $\Omega,$ can we find a symmetric $B$ and $A$ as before such that $$\operatorname{tr}(U^T S U D + U^T\Omega U) = \operatorname{tr}(BU^TAU) \text{?}$$",,"['linear-algebra', 'matrices', 'statistics', 'trace']"
27,change in the data value to give a lower/equal mean than the median,change in the data value to give a lower/equal mean than the median,,"Does anyone knows, how to change (or which ONE VALUE of data do I have to change), so that the mean of this data are: 1. lower than the median, and 2. as equal as the median. For example if I have the data: 1, 1.2, 1.4, 1.6, 1.6, 1.8, 2.0, 2.6, 2.8, 2.8, 3. I've read some on the internet which says that: median + (new value - old value)/number of value (or we called n)= mean then I've tried to make a new mean value which is lower than the old mean. But it's always give a negative result for the ONE NEW VALUE (of the data). Is it true? So I can't have any ONE NEW POSITIVE VALUE (to make the mean lower/equal than the median)?","Does anyone knows, how to change (or which ONE VALUE of data do I have to change), so that the mean of this data are: 1. lower than the median, and 2. as equal as the median. For example if I have the data: 1, 1.2, 1.4, 1.6, 1.6, 1.8, 2.0, 2.6, 2.8, 2.8, 3. I've read some on the internet which says that: median + (new value - old value)/number of value (or we called n)= mean then I've tried to make a new mean value which is lower than the old mean. But it's always give a negative result for the ONE NEW VALUE (of the data). Is it true? So I can't have any ONE NEW POSITIVE VALUE (to make the mean lower/equal than the median)?",,"['statistics', 'data-analysis', 'means', 'median']"
28,Transforming a Moment Generating Function,Transforming a Moment Generating Function,,Question : If we have the moment generating function of S as $g_s(t) = (\frac {1+e^t}2)^n$ Define $Y = \frac {S- \frac n2}{\sqrt n}$. Show that the moment generating function of Y is $\lim\limits_{n \to \infty} g_Y(t) → e^{t^2/8}$ Does anyone mind explaining how I could transform the moment generating function of S into Y?,Question : If we have the moment generating function of S as $g_s(t) = (\frac {1+e^t}2)^n$ Define $Y = \frac {S- \frac n2}{\sqrt n}$. Show that the moment generating function of Y is $\lim\limits_{n \to \infty} g_Y(t) → e^{t^2/8}$ Does anyone mind explaining how I could transform the moment generating function of S into Y?,,"['probability', 'statistics']"
29,Binominal distribution to standard normal distribution,Binominal distribution to standard normal distribution,,"I test my skills in statistics and probabilities and I decided to work with distributions. So, I tried to solve the below problem Problem Suppose that a hospital serves in average $80$ citizens daily from a city with $11000$ citizens.  In a random day, what is the probability that the hospital serves at most $8$ citizens? My solution I defined a random variable $X$ {number of citizens who will be served in one day }. $X \sim b(x;n=11000,p)$, where  \begin{align} p &= \frac{E(X)}{n} = \frac{80}{11000} = 0.07 \end{align} Provided that $npq = 76.4 > 10$: $b(x;n=11000,p) \sim N(pq,npq)$ According to the central limit theorem, \begin{align} Z = \frac{X - np}{\sqrt{npq}} = \frac{8-80}{8.74} = -8.23  \end{align} So $P(Z\le -8.23) = 0$. Where is my fault? I think my reasoning is not correct.","I test my skills in statistics and probabilities and I decided to work with distributions. So, I tried to solve the below problem Problem Suppose that a hospital serves in average $80$ citizens daily from a city with $11000$ citizens.  In a random day, what is the probability that the hospital serves at most $8$ citizens? My solution I defined a random variable $X$ {number of citizens who will be served in one day }. $X \sim b(x;n=11000,p)$, where  \begin{align} p &= \frac{E(X)}{n} = \frac{80}{11000} = 0.07 \end{align} Provided that $npq = 76.4 > 10$: $b(x;n=11000,p) \sim N(pq,npq)$ According to the central limit theorem, \begin{align} Z = \frac{X - np}{\sqrt{npq}} = \frac{8-80}{8.74} = -8.23  \end{align} So $P(Z\le -8.23) = 0$. Where is my fault? I think my reasoning is not correct.",,"['probability-theory', 'statistics', 'central-limit-theorem']"
30,"Copula of $C_{s(X),t(Y)}$",Copula of,"C_{s(X),t(Y)}","A question on copula: If $s: \Bbb R\to \Bbb R$ is an increasing function, and $t: \Bbb R\to \Bbb R$ is a decreasing function, find the copula $C_{s(X),t(Y)}$ of $(s(X), t(Y))$ in terms of $C_{X,Y}$. (Assume $X$ and $Y$ to be continuous random variables)","A question on copula: If $s: \Bbb R\to \Bbb R$ is an increasing function, and $t: \Bbb R\to \Bbb R$ is a decreasing function, find the copula $C_{s(X),t(Y)}$ of $(s(X), t(Y))$ in terms of $C_{X,Y}$. (Assume $X$ and $Y$ to be continuous random variables)",,"['probability', 'statistics']"
31,Why is my Cauchy Distribution going way larger than 1?,Why is my Cauchy Distribution going way larger than 1?,,"I believe my answer to be simple but I will explain as I cannot figure it out. I am writing a simulation for an optical experiment.  I have a very small square plate target of dimentions $W \times W$.  I want to set up a laser linewidth that has a center wavelength of $0.1W$ and a half-width-half-maximum of $0.002W$.  I am assuming that the bandwidth of the laser follows a Cauchy distribution, which has the following form: \begin{equation} P(\lambda) = \frac{1}{\pi}\frac{\gamma}{(\lambda-\lambda_0)^2+\gamma^2} \end{equation} where $\lambda_0$ is the center wavelength, which is equal to $0.1W$, and $\gamma$ is the half-width-half-maximum, equal to $0.002W$. So my goal is to try to leave actual dimensions out of this and keep everything as ratios in relation to $W$.  If I plug in my parameters I get: \begin{align} P(\lambda) &= \frac{1}{\pi}\frac{0.002W}{(\lambda-0.1W)^2+(0.002)^2W^2} \\ & \frac{1}{\pi W}\frac{0.002}{(\frac{\lambda}{W}-0.1)^2+(0.002)^2} \end{align} So as you can see, by virtue of the $1/(\pi W)$ term, I am left with an equation that forces me to use a value for $W$, instead of a ratio of $W$.  I don't think this is correct because the value for $W$ will be on the order of micrometers, which will make $P(\lambda)$ blow up.  Not to mention that the actual function part is never less than 1 because the input parameters are less than 1 and get smaller when squared. Does anyone have any thoughts or suggestions? EDIT: I didn't make it clear that I want to use the linewidth of the laser to make a pdf of how probable a certain wavelength is to illuminate the target.  That is the motivation for this construction.","I believe my answer to be simple but I will explain as I cannot figure it out. I am writing a simulation for an optical experiment.  I have a very small square plate target of dimentions $W \times W$.  I want to set up a laser linewidth that has a center wavelength of $0.1W$ and a half-width-half-maximum of $0.002W$.  I am assuming that the bandwidth of the laser follows a Cauchy distribution, which has the following form: \begin{equation} P(\lambda) = \frac{1}{\pi}\frac{\gamma}{(\lambda-\lambda_0)^2+\gamma^2} \end{equation} where $\lambda_0$ is the center wavelength, which is equal to $0.1W$, and $\gamma$ is the half-width-half-maximum, equal to $0.002W$. So my goal is to try to leave actual dimensions out of this and keep everything as ratios in relation to $W$.  If I plug in my parameters I get: \begin{align} P(\lambda) &= \frac{1}{\pi}\frac{0.002W}{(\lambda-0.1W)^2+(0.002)^2W^2} \\ & \frac{1}{\pi W}\frac{0.002}{(\frac{\lambda}{W}-0.1)^2+(0.002)^2} \end{align} So as you can see, by virtue of the $1/(\pi W)$ term, I am left with an equation that forces me to use a value for $W$, instead of a ratio of $W$.  I don't think this is correct because the value for $W$ will be on the order of micrometers, which will make $P(\lambda)$ blow up.  Not to mention that the actual function part is never less than 1 because the input parameters are less than 1 and get smaller when squared. Does anyone have any thoughts or suggestions? EDIT: I didn't make it clear that I want to use the linewidth of the laser to make a pdf of how probable a certain wavelength is to illuminate the target.  That is the motivation for this construction.",,['statistics']
32,A question regarding further studies of continuous probability distributions,A question regarding further studies of continuous probability distributions,,"I recently finished studying a book in elementary probability theory. I am particularly interested in continuous probability distributions, but since the book was just an introduction to probability theory it didn't cover those to a satisfactory extent. I never really got to put anything I learned to use other than solving a couple of basic assignments. If I want to learn more about continuous distributions, what book should I pick? I haven't had statistics yet, would that perhaps the be correct choice? Is there perhaps some book in differential equations that is centered around probability theory?","I recently finished studying a book in elementary probability theory. I am particularly interested in continuous probability distributions, but since the book was just an introduction to probability theory it didn't cover those to a satisfactory extent. I never really got to put anything I learned to use other than solving a couple of basic assignments. If I want to learn more about continuous distributions, what book should I pick? I haven't had statistics yet, would that perhaps the be correct choice? Is there perhaps some book in differential equations that is centered around probability theory?",,"['probability-theory', 'statistics', 'probability-distributions', 'reference-request', 'book-recommendation']"
33,Real-valued Expected Value Probability Question,Real-valued Expected Value Probability Question,,"I'm stuck on answering a problem in the famous Berteskas & Tsitsiklis probability book: The premise Imagine a TV game show where each contestant i spins an infinitely   calibrated wheel of fortune, which assigns him/her with some real number with a   value between 1 and 100. All values are equally likely and the value obtained by each   contestant is independent of the value obtained by any other contestant. Let N be the integer-valued random variable whose value is the index of the first   contestant who is assigned a smaller number than contestant 1. As an illustration,   if contestant 1 obtains a smaller value than contestants 2 and 3 but contestant 4   has a smaller value than contestant 1 (X4 < X1), then N = 4. Find P(N > n)   as a function of n. What I did for the first part, finding $P(N > n)$, I saw that it was a geometric distribution, with the chance that a number is larger than a given $n$, is $(100-n)/99$, since the random variable of the wheel # is uniformly distributed. Thus, for the given geometric distribution, if we want $N$-th roll to be the roll where the number is first lesser than the 1st roll , we must give the following probability: $P(N \gt n) = ((100-n)/99)^{N-1} ((n-1)/99)$ Which is effectively saying we want the first $N-1$ rolls to be greater than the first roll, and the last one to be the complement, or:$1-(100-n)/99 = (n-1)/99$. So now that we have this, assuming it's correct, I'm stuck on the following: Find E[N], assuming an infinite number of contestants. I know that the expected value formula is the following: $ sum_{x=2}^{\infty} xp_x(x)$ where x is the index of the contestant. However, the contestant's probability also relies on the value of the initial value of the first spin. Is the next logical step to decompose into a double sum and evaluate that? Or is it something completely different?","I'm stuck on answering a problem in the famous Berteskas & Tsitsiklis probability book: The premise Imagine a TV game show where each contestant i spins an infinitely   calibrated wheel of fortune, which assigns him/her with some real number with a   value between 1 and 100. All values are equally likely and the value obtained by each   contestant is independent of the value obtained by any other contestant. Let N be the integer-valued random variable whose value is the index of the first   contestant who is assigned a smaller number than contestant 1. As an illustration,   if contestant 1 obtains a smaller value than contestants 2 and 3 but contestant 4   has a smaller value than contestant 1 (X4 < X1), then N = 4. Find P(N > n)   as a function of n. What I did for the first part, finding $P(N > n)$, I saw that it was a geometric distribution, with the chance that a number is larger than a given $n$, is $(100-n)/99$, since the random variable of the wheel # is uniformly distributed. Thus, for the given geometric distribution, if we want $N$-th roll to be the roll where the number is first lesser than the 1st roll , we must give the following probability: $P(N \gt n) = ((100-n)/99)^{N-1} ((n-1)/99)$ Which is effectively saying we want the first $N-1$ rolls to be greater than the first roll, and the last one to be the complement, or:$1-(100-n)/99 = (n-1)/99$. So now that we have this, assuming it's correct, I'm stuck on the following: Find E[N], assuming an infinite number of contestants. I know that the expected value formula is the following: $ sum_{x=2}^{\infty} xp_x(x)$ where x is the index of the contestant. However, the contestant's probability also relies on the value of the initial value of the first spin. Is the next logical step to decompose into a double sum and evaluate that? Or is it something completely different?",,"['probability', 'probability-theory', 'statistics']"
34,Mathematical meaning of Residual Sum of Squares,Mathematical meaning of Residual Sum of Squares,,"I know what is Residual Sum of Squares (RSS) . The problem is I don't understand why it works. For example, you have a set of descreet values, and you want to describe them with linear, quadratic or any other function. To choose the best function you calculate the RSS (sum of variances) instead sum of deviances. Why to choose the most precise function you have to sum variances (instead of deviances)? How is that prooved?","I know what is Residual Sum of Squares (RSS) . The problem is I don't understand why it works. For example, you have a set of descreet values, and you want to describe them with linear, quadratic or any other function. To choose the best function you calculate the RSS (sum of variances) instead sum of deviances. Why to choose the most precise function you have to sum variances (instead of deviances)? How is that prooved?",,"['statistics', 'standard-deviation', 'variance']"
35,Probability of Royal Flush dealt in 25-card hand,Probability of Royal Flush dealt in 25-card hand,,"This question arose from the game ""Poker Shuffle"", where one can rearrange 25 cards in a 5x5 grid to maximize the net score of the resulting 10 poker hands. As the royal flush is the highest-scoring hand, I am wondering what the probability of being dealt a royal flush is. I don't even know where to begin in attacking such a problem—my only insight is that it will likely have to take the form of an inclusion-exclusion formula due to the possibility of multiple royal flushes in one ""hand"". Thanks in advance for any thought any of you put into this!","This question arose from the game ""Poker Shuffle"", where one can rearrange 25 cards in a 5x5 grid to maximize the net score of the resulting 10 poker hands. As the royal flush is the highest-scoring hand, I am wondering what the probability of being dealt a royal flush is. I don't even know where to begin in attacking such a problem—my only insight is that it will likely have to take the form of an inclusion-exclusion formula due to the possibility of multiple royal flushes in one ""hand"". Thanks in advance for any thought any of you put into this!",,"['probability', 'statistics', 'poker']"
36,What are the values of the ​average length? [closed],What are the values of the ​average length? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question A segment of length $1$ is broken into three pieces. We consider that the breakpoints are two random variables $X, Y$ independent and uniformly distributed $U([0, 1])$. We want to approximate average lengths of the three segments obtained. For this, simulate $N = 5000$ independent achievements of the couple $(X, Y)$. What are the values of ​​average length? I'm wondering if I can use SLLN. But as I see there many cases, for example if $X<Y$. Is there another solution?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question A segment of length $1$ is broken into three pieces. We consider that the breakpoints are two random variables $X, Y$ independent and uniformly distributed $U([0, 1])$. We want to approximate average lengths of the three segments obtained. For this, simulate $N = 5000$ independent achievements of the couple $(X, Y)$. What are the values of ​​average length? I'm wondering if I can use SLLN. But as I see there many cases, for example if $X<Y$. Is there another solution?",,"['probability-theory', 'statistics']"
37,How to normalize by volume the density of a set of points distributed within a sphere.,How to normalize by volume the density of a set of points distributed within a sphere.,,"Suppose we are in $R^3$ and we have a set of points distributed within a sphere of radius $\rho$. I want to have a measure of ""density"" of points in spherical shells of the sphere. To visualize this, I draw a histogram where the $n'th$ point on the x-axis represents the $n'th$ spherical shell, covering some radius $0\leq (r_n \pm\epsilon) \leq \rho$, and the y-axis represents the number of points belonging to the corresponding shell. Of course, this would give me a biased measure of density because shells further away from the center of the sphere have greater volume, hence more points in them, but are not necessarily denser. To remedy that, I divide the number of points in each shell by its volume $\frac{4}{3} \pi (r_2^3-r_1^3)$ where $0\leq r_1 \leq r_2 \leq \rho$ are the radial distances defining the shell. My statistical knowledge is extremely limited, is this a proper/usual method of normalizing in this context? To test this I distributed points uniformly in a sphere and proceeded with this method to normalize the density, and here's some graphs: Measuring the actual number of points per spherical shell: x-axis spherical shell. y-axis number of points in it Before normalization Now here's the same plot after normalizing as I describe above To somebody inexperienced in dealing with data, like myself, this would mean the method is fine it just needs a lot more shells and a lot more points, and it will converge to a good measure of normalized density (i.e. it will flatten to a line). However, this is for 50,000 points and 500 shells. I ran this for 500,000 points and 1000 shells, it didn't get much better. Perhaps it's a matter of slow convergence, but I do think there seems to be a problem near the origin, as I have observed in the specific problem I'm trying to solve. Any ideas out there?","Suppose we are in $R^3$ and we have a set of points distributed within a sphere of radius $\rho$. I want to have a measure of ""density"" of points in spherical shells of the sphere. To visualize this, I draw a histogram where the $n'th$ point on the x-axis represents the $n'th$ spherical shell, covering some radius $0\leq (r_n \pm\epsilon) \leq \rho$, and the y-axis represents the number of points belonging to the corresponding shell. Of course, this would give me a biased measure of density because shells further away from the center of the sphere have greater volume, hence more points in them, but are not necessarily denser. To remedy that, I divide the number of points in each shell by its volume $\frac{4}{3} \pi (r_2^3-r_1^3)$ where $0\leq r_1 \leq r_2 \leq \rho$ are the radial distances defining the shell. My statistical knowledge is extremely limited, is this a proper/usual method of normalizing in this context? To test this I distributed points uniformly in a sphere and proceeded with this method to normalize the density, and here's some graphs: Measuring the actual number of points per spherical shell: x-axis spherical shell. y-axis number of points in it Before normalization Now here's the same plot after normalizing as I describe above To somebody inexperienced in dealing with data, like myself, this would mean the method is fine it just needs a lot more shells and a lot more points, and it will converge to a good measure of normalized density (i.e. it will flatten to a line). However, this is for 50,000 points and 500 shells. I ran this for 500,000 points and 1000 shells, it didn't get much better. Perhaps it's a matter of slow convergence, but I do think there seems to be a problem near the origin, as I have observed in the specific problem I'm trying to solve. Any ideas out there?",,"['statistics', 'data-analysis', 'spheres']"
38,Calculating probability density function,Calculating probability density function,,"I am having difficulty solving this problem. Given this distribution   $$ \begin{align*} 	f_X(x) &= \begin{cases}e^{-x} &,& x\geq 0 \\ 	 0 &,& x<0\end{cases} \\[1ex] 	f_Y(y) &= \begin{cases}2e^{-2y} &,& y\geq 0 \\ 	 0 &,& y<0\end{cases} \end{align*} $$   Calculate $\mathsf P(X<3Y)$ I have attempted the following, however I am unsure if my methodology is correct. $$  \mathsf P(X<3Y)= \int_0^\infty \left[\int_0^{3y} f_X(x)f_Y(y) dx\right] dy $$","I am having difficulty solving this problem. Given this distribution   $$ \begin{align*} 	f_X(x) &= \begin{cases}e^{-x} &,& x\geq 0 \\ 	 0 &,& x<0\end{cases} \\[1ex] 	f_Y(y) &= \begin{cases}2e^{-2y} &,& y\geq 0 \\ 	 0 &,& y<0\end{cases} \end{align*} $$   Calculate $\mathsf P(X<3Y)$ I have attempted the following, however I am unsure if my methodology is correct. $$  \mathsf P(X<3Y)= \int_0^\infty \left[\int_0^{3y} f_X(x)f_Y(y) dx\right] dy $$",,"['probability', 'statistics']"
39,Calculating a Cumulative Distribution Function (CDF),Calculating a Cumulative Distribution Function (CDF),,"Let $Z$ be a continuous random variable with probability density function: $$ f_Z(z) =  \begin{cases} \gamma(1 + z^2) & \mbox{ if } -2 < z < 1, \\ 0 & \mbox{ otherwise} \end{cases} $$ a) For what $γ$ is this possible? b) Find the cumulative distribution function of $Z$.","Let $Z$ be a continuous random variable with probability density function: $$ f_Z(z) =  \begin{cases} \gamma(1 + z^2) & \mbox{ if } -2 < z < 1, \\ 0 & \mbox{ otherwise} \end{cases} $$ a) For what $γ$ is this possible? b) Find the cumulative distribution function of $Z$.",,"['probability', 'statistics']"
40,Probability Distribution Doubt,Probability Distribution Doubt,,"1) A drawer full of AAA batteries contains 40 batteries, but 6 of them are dead. Suppose 4 batteries are randomly selected from the drawer to use. a) What is the probability that all four are good?  (I used the Hypergeometric Distribution equation $ P(x) ={ \binom{k}{x} \binom{N - k}{n - x } \over \binom{N}{n}}$  assuming that the variables are N=40, k=34, n=4, x=4 ) $ P(x=4) ={ \binom{34}{4} \binom{40 - 34}{4 - 4 } \over \binom{40}{4}}$ $ P(x=4) = 0.507 $ would this be the correct way to answer this question? b) What is the probability that exactly one dead battery is selected amount the 4?  (I assume the hypergeometric applies to this as well. Using $N=40$, $k=6$, $n=4$,$x=1$) $ P(x=1) ={ \binom{6}{1} \binom{40 - 4}{4 - 1 } \over \binom{40}{4}}$ I don't think this is right. I am confused how to set this equation right. I want to know what the variables for this equation stand for. I am confused on what each mean. So far from what i understand is N = total # of things, n = randomly select without replacement, x = # of successes we need)","1) A drawer full of AAA batteries contains 40 batteries, but 6 of them are dead. Suppose 4 batteries are randomly selected from the drawer to use. a) What is the probability that all four are good?  (I used the Hypergeometric Distribution equation $ P(x) ={ \binom{k}{x} \binom{N - k}{n - x } \over \binom{N}{n}}$  assuming that the variables are N=40, k=34, n=4, x=4 ) $ P(x=4) ={ \binom{34}{4} \binom{40 - 34}{4 - 4 } \over \binom{40}{4}}$ $ P(x=4) = 0.507 $ would this be the correct way to answer this question? b) What is the probability that exactly one dead battery is selected amount the 4?  (I assume the hypergeometric applies to this as well. Using $N=40$, $k=6$, $n=4$,$x=1$) $ P(x=1) ={ \binom{6}{1} \binom{40 - 4}{4 - 1 } \over \binom{40}{4}}$ I don't think this is right. I am confused how to set this equation right. I want to know what the variables for this equation stand for. I am confused on what each mean. So far from what i understand is N = total # of things, n = randomly select without replacement, x = # of successes we need)",,"['probability', 'statistics', 'probability-distributions']"
41,Calculating probability of Type II Error for a one-sample t test [closed],Calculating probability of Type II Error for a one-sample t test [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question So I have the following problem: A transportation company is suspicious of the claim that the average useful life of certain tires is at least 28,000 miles. To verify that, 40 tires are placed in trucks and an average useful life of 27463 is obtained with a standard deviation of 1348 miles. a) Test this hypothesis with a level of significance of α = 0.01 b) If $\mu_1 = 27230,$ calculate the probability of Type II Error Any hint on how to do b)?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question So I have the following problem: A transportation company is suspicious of the claim that the average useful life of certain tires is at least 28,000 miles. To verify that, 40 tires are placed in trucks and an average useful life of 27463 is obtained with a standard deviation of 1348 miles. a) Test this hypothesis with a level of significance of α = 0.01 b) If $\mu_1 = 27230,$ calculate the probability of Type II Error Any hint on how to do b)?",,"['statistics', 'hypothesis-testing']"
42,The average rate of job submissions in a computer center is 2 per minute.,The average rate of job submissions in a computer center is 2 per minute.,,"Full Question: The average rate of job submissions in a computer   center is 2 per minute. If it can be assumed that the number of   submissions per minute has a Poisson distribution, calculate the   probability that (a) more than two jobs will arrive in a minute $P(X>2) = 1- P(X≤ 2) = 1- [P(X=0)+P(X=1)+P(X=2)]$ and I got 0.3233235 as my answer. Is that correct? Example: For $P(X=1),$ I did $((2^1)(e^-2))/1!$ (b) at least 30 sec will elapse between any two jobs This is where I'm confused. A similar problem was answered using Exponential distribution in the book even though the question said this was a Poisson distribution problem. I tried with Exponential and this is what I got: Since 30 seconds is $1/2$ of 1 minute, $(1/2) * λ$ or $(1/2)*2 = 1$ $P(X ≥ 1/2) = 1 - P(x < 1/2) = 1 - (1- e^-1) = 0.3678794$ Please tell me if I used the correct distribution functions for both a and b, and if I answered them correctly. Thank you.","Full Question: The average rate of job submissions in a computer   center is 2 per minute. If it can be assumed that the number of   submissions per minute has a Poisson distribution, calculate the   probability that (a) more than two jobs will arrive in a minute $P(X>2) = 1- P(X≤ 2) = 1- [P(X=0)+P(X=1)+P(X=2)]$ and I got 0.3233235 as my answer. Is that correct? Example: For $P(X=1),$ I did $((2^1)(e^-2))/1!$ (b) at least 30 sec will elapse between any two jobs This is where I'm confused. A similar problem was answered using Exponential distribution in the book even though the question said this was a Poisson distribution problem. I tried with Exponential and this is what I got: Since 30 seconds is $1/2$ of 1 minute, $(1/2) * λ$ or $(1/2)*2 = 1$ $P(X ≥ 1/2) = 1 - P(x < 1/2) = 1 - (1- e^-1) = 0.3678794$ Please tell me if I used the correct distribution functions for both a and b, and if I answered them correctly. Thank you.",,"['statistics', 'probability-distributions', 'exponential-function', 'poisson-distribution', 'exponential-distribution']"
43,Finding the indicated probability using tables of the Standard Normal Distribution [closed],Finding the indicated probability using tables of the Standard Normal Distribution [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question I have 3 questions that I'm trying to figure out. Lesson was on a day I was absent so I have no notes to go based off of. See below Find the indicated probability using the standard normal distribution. $P(Z < 3.21)$ $P(Z > 2.35)$ $P(1.52 < Z < 3.31)$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question I have 3 questions that I'm trying to figure out. Lesson was on a day I was absent so I have no notes to go based off of. See below Find the indicated probability using the standard normal distribution. $P(Z < 3.21)$ $P(Z > 2.35)$ $P(1.52 < Z < 3.31)$",,"['statistics', 'normal-distribution']"
44,Compute $E\left(\left(\sum\limits_{i=1}^dX_i^2\right)^{-1}\right)$ for i.i.d. standard normal random variables $(X_i)$,Compute  for i.i.d. standard normal random variables,E\left(\left(\sum\limits_{i=1}^dX_i^2\right)^{-1}\right) (X_i),"Does anyone know how to solve the following exercise? Let $X_i$, $i=1,\ldots,d$ be i.i.d $\mathcal N(0,1)$ random variables. Show that for $d>2$, $$E\left[ \frac1 {\sum_{i=1}^d X_i^2} \right] = \frac 1 {d-2}$$ I have tried to use that the sum of $X_i^2$ is a chi square...","Does anyone know how to solve the following exercise? Let $X_i$, $i=1,\ldots,d$ be i.i.d $\mathcal N(0,1)$ random variables. Show that for $d>2$, $$E\left[ \frac1 {\sum_{i=1}^d X_i^2} \right] = \frac 1 {d-2}$$ I have tried to use that the sum of $X_i^2$ is a chi square...",,"['probability', 'statistics', 'probability-distributions']"
45,"Distributing n elements randomnly in r bins, and counting how many bins end up wih 1 element","Distributing n elements randomnly in r bins, and counting how many bins end up wih 1 element",,"TL,DR I will distribute n elements (indistinguishable) in r bins at random (equal chance of each bin) and try to calculate the expected value of X1=""number of bins with exactly 1 element in it"" as a function of n (r is known). Getting the probability distribution for X1 is interesting but not vital. Context I'm trying to find the best strategy for a board game I play with my friends. In it, you can draw n cards (you decide how many beforehand) and pay an amount for each card you want to draw. There are r different types of card, all with equal chances of appearing. After this, getting only one card of a type is penalized, while getting a group of 2 or more of the same kind is rewarded. Obviously, drawing dozens of cards assures you will not get many ""singletons"", but the cost becomes prohibitive. Knowing how many singletons vs groups i can expect if i draw n cards can help me optimize my plays (yes, I'm THAT guy that can't stop optimizing irrelevant things). The model Each card type is a ""bin"", and each card drawn is an element. There are r bins. N elements are distributed in the bins. Each element has to go into one and only one bin (once you show the card, you know what type it is). Each bin has the same chances of being selected. There is no limit on how many elements a bin can hold. Elements are indistinguishable, because the only thing that matter is how many of each type you get, regardless of the order. What I want to do I'm trying to plot (expected number of bins with 1 element), (expected number of bins with 2 or more elements) vs (number of elements distributed), with a 10% and 90% percentile lines if possible. The graph tools i know all require a explicit formula (a function of n) for the plot lines and allow no code. I know how to make a simulation of the card draw, but I prefer to use it as a validation for the analytical formula. I'm very familiar with excel/google sheets and the rest of my related analisys are on that format. I can learn another tool if necesary, but the distribution has to be expressed as a function of n and r. What I've tried Reading the wikipedia article about probablility distributions, i find that this one https://en.wikipedia.org/wiki/Multinomial_distribution describes exactly the experiment I'm doing, but the information it provides is not the one I look for. This is already beyond the statistics I've learned in college and I'm stuck. I also tried modelling the problem as a system of differential equations of n variables X0, X1...Xn where Xi=""number of bins with exactly i elements in it"". When distributing an element, the probability that it falls in a bin with i elements is proportional to Xi, and the effect is that Xi decreases in 1 and X(i+1) increases in 1. It looks similar to this: dX0/dn=-k·X0 dX1/dn= + k·X0 - k · X1 dX2/dn= + k·X1 - k · X2 ... dXn/dn= + k · X(n-1) What failed in my approaches About the multinomial, I have to provide the specific combinations of number of elements in each bin to get the chances of the combination, but I want to ""group"" all the combinations that have say, 3 bins with 1 element, and combine the chances of all those combinations. This becomes unwieldly when n increases. About the differenctial equations,  I'm turning a discrete variable (number of elements) into continuous. It has undesirable effects, for example distributing only 1 element, and finding that there are 0,000001 bins with 20 elements already. Also, I only know the values at n=0 (all the bins are empty, so X0=r, and 0 for the rest) so I struggle giving a value to k. My questions • Is any probability distribution that models my problem and provides the information I look for better than the multinomial? • If not, is it possible to combine explicitly all the combinations of / into ? • What other data point (apart from n=0) can be used to estimate k in the case of the differential equations. Observations • n is at least 1 but has no upper bound • The cost of drawing a card is constant so it doesn't matter • The amount of physical cards is the game is high enough that drawing a card without replacing it doesn't change the probabilities of getting each card type in any meaningful way, so chances of getting each type remain constant. • First post here!  I really wanted to make a structured question, but maybe I was too verbose, sorry about that. Will try to correct it in later post once I get the hang of this.","TL,DR I will distribute n elements (indistinguishable) in r bins at random (equal chance of each bin) and try to calculate the expected value of X1=""number of bins with exactly 1 element in it"" as a function of n (r is known). Getting the probability distribution for X1 is interesting but not vital. Context I'm trying to find the best strategy for a board game I play with my friends. In it, you can draw n cards (you decide how many beforehand) and pay an amount for each card you want to draw. There are r different types of card, all with equal chances of appearing. After this, getting only one card of a type is penalized, while getting a group of 2 or more of the same kind is rewarded. Obviously, drawing dozens of cards assures you will not get many ""singletons"", but the cost becomes prohibitive. Knowing how many singletons vs groups i can expect if i draw n cards can help me optimize my plays (yes, I'm THAT guy that can't stop optimizing irrelevant things). The model Each card type is a ""bin"", and each card drawn is an element. There are r bins. N elements are distributed in the bins. Each element has to go into one and only one bin (once you show the card, you know what type it is). Each bin has the same chances of being selected. There is no limit on how many elements a bin can hold. Elements are indistinguishable, because the only thing that matter is how many of each type you get, regardless of the order. What I want to do I'm trying to plot (expected number of bins with 1 element), (expected number of bins with 2 or more elements) vs (number of elements distributed), with a 10% and 90% percentile lines if possible. The graph tools i know all require a explicit formula (a function of n) for the plot lines and allow no code. I know how to make a simulation of the card draw, but I prefer to use it as a validation for the analytical formula. I'm very familiar with excel/google sheets and the rest of my related analisys are on that format. I can learn another tool if necesary, but the distribution has to be expressed as a function of n and r. What I've tried Reading the wikipedia article about probablility distributions, i find that this one https://en.wikipedia.org/wiki/Multinomial_distribution describes exactly the experiment I'm doing, but the information it provides is not the one I look for. This is already beyond the statistics I've learned in college and I'm stuck. I also tried modelling the problem as a system of differential equations of n variables X0, X1...Xn where Xi=""number of bins with exactly i elements in it"". When distributing an element, the probability that it falls in a bin with i elements is proportional to Xi, and the effect is that Xi decreases in 1 and X(i+1) increases in 1. It looks similar to this: dX0/dn=-k·X0 dX1/dn= + k·X0 - k · X1 dX2/dn= + k·X1 - k · X2 ... dXn/dn= + k · X(n-1) What failed in my approaches About the multinomial, I have to provide the specific combinations of number of elements in each bin to get the chances of the combination, but I want to ""group"" all the combinations that have say, 3 bins with 1 element, and combine the chances of all those combinations. This becomes unwieldly when n increases. About the differenctial equations,  I'm turning a discrete variable (number of elements) into continuous. It has undesirable effects, for example distributing only 1 element, and finding that there are 0,000001 bins with 20 elements already. Also, I only know the values at n=0 (all the bins are empty, so X0=r, and 0 for the rest) so I struggle giving a value to k. My questions • Is any probability distribution that models my problem and provides the information I look for better than the multinomial? • If not, is it possible to combine explicitly all the combinations of / into ? • What other data point (apart from n=0) can be used to estimate k in the case of the differential equations. Observations • n is at least 1 but has no upper bound • The cost of drawing a card is constant so it doesn't matter • The amount of physical cards is the game is high enough that drawing a card without replacing it doesn't change the probabilities of getting each card type in any meaningful way, so chances of getting each type remain constant. • First post here!  I really wanted to make a structured question, but maybe I was too verbose, sorry about that. Will try to correct it in later post once I get the hang of this.",,"['combinatorics', 'statistics', 'discrete-mathematics', 'probability-distributions']"
46,Gamma Distribution Sum,Gamma Distribution Sum,,"I'm trying to show that the sum of three independent gamma distributions $X_1\sim \Gamma(2,5)$, $X_2\sim \Gamma(3,5)$, and $X_3\sim\Gamma(1.5,2.5)$ are a gamma distribution. $Y=X_1+X_2+2X_3$. I've multiplied the moment generating functions of $X_1$ and $X_2$ and I've squared the moment generating function of $X_3$ and am left with gamma distributions that I cannot multiply together to give me another Gamma distribution. How would I proceed? $\frac{1}{2.5^3\left(\frac{1}{2.5}-t\right)^3}\left(\frac{1}{5^5\left(\frac{1}{5}-t\right)^5}\right)$","I'm trying to show that the sum of three independent gamma distributions $X_1\sim \Gamma(2,5)$, $X_2\sim \Gamma(3,5)$, and $X_3\sim\Gamma(1.5,2.5)$ are a gamma distribution. $Y=X_1+X_2+2X_3$. I've multiplied the moment generating functions of $X_1$ and $X_2$ and I've squared the moment generating function of $X_3$ and am left with gamma distributions that I cannot multiply together to give me another Gamma distribution. How would I proceed? $\frac{1}{2.5^3\left(\frac{1}{2.5}-t\right)^3}\left(\frac{1}{5^5\left(\frac{1}{5}-t\right)^5}\right)$",,"['statistics', 'gamma-distribution']"
47,Rough trend calculation and approximation error,Rough trend calculation and approximation error,,"In many cases calculating trend by using least-squares method is best decision. But sometimes it is overabundant. What I need is just to know, whether trend is positive or not. Can I use such approach (difference in weight coefficient) and how to determine it's approximation error? ($x_1*1$ + $x_2*2$ + ... + $x_n * n$) - ($x_1*n$ + $x_2*(n-1)$ + ... + $x_n * 1$) Examples: Negative 19  17  11  1  4  4  6  3  2  2    => -283 Positive 0  0  1  3  1  1  7  5  6  12      => 182 Update Calculation using this approach and least-squares approach","In many cases calculating trend by using least-squares method is best decision. But sometimes it is overabundant. What I need is just to know, whether trend is positive or not. Can I use such approach (difference in weight coefficient) and how to determine it's approximation error? ($x_1*1$ + $x_2*2$ + ... + $x_n * n$) - ($x_1*n$ + $x_2*(n-1)$ + ... + $x_n * 1$) Examples: Negative 19  17  11  1  4  4  6  3  2  2    => -283 Positive 0  0  1  3  1  1  7  5  6  12      => 182 Update Calculation using this approach and least-squares approach",,['statistics']
48,How to find the P-Value for this test of hypothesis?,How to find the P-Value for this test of hypothesis?,,"I so we started a new section where I need to identify the (Null hypothesis, alternative hypothesis, test statistic, and p-value) I found all the information expect the p-value I couldn't understand my professor when he explained it. So here's the problem. Ex:In a recent poll of 755 randomly selected adults, 587 said that it is morally wrong to not report all income on tax returns. Use a 0.05 significance level to test the claim that 75​% of adults say that it is morally wrong to not report all income on tax returns. Identify the null​ hypothesis, alternative​ hypothesis, test​ statistic, P-value, conclusion about the null​ hypothesis, and final conclusion that addresses the original claim. Use the​ P-value method. Use the normal distribution as an approximation of the binomial distribution. I found most of the information this is what I have: $p = .75\;$ $\hat p = x/n,$ which gives you .777; $q = .25,\;$ $n = 755,\;$ and $x = 587.$ The significance level is 0.05, and I found the test statistic: its 1.71. I used the following formula $$z = ( \hat p - p)/\sqrt{.75(.25)/755} = 1.71.$$ The one I missing is the p-value which I don't how to obtain. I know that this is a two tail test however this is the part I'm stuck on. Could anyone share their expertise with me so I may learn to do it on my own?","I so we started a new section where I need to identify the (Null hypothesis, alternative hypothesis, test statistic, and p-value) I found all the information expect the p-value I couldn't understand my professor when he explained it. So here's the problem. Ex:In a recent poll of 755 randomly selected adults, 587 said that it is morally wrong to not report all income on tax returns. Use a 0.05 significance level to test the claim that 75​% of adults say that it is morally wrong to not report all income on tax returns. Identify the null​ hypothesis, alternative​ hypothesis, test​ statistic, P-value, conclusion about the null​ hypothesis, and final conclusion that addresses the original claim. Use the​ P-value method. Use the normal distribution as an approximation of the binomial distribution. I found most of the information this is what I have: $p = .75\;$ $\hat p = x/n,$ which gives you .777; $q = .25,\;$ $n = 755,\;$ and $x = 587.$ The significance level is 0.05, and I found the test statistic: its 1.71. I used the following formula $$z = ( \hat p - p)/\sqrt{.75(.25)/755} = 1.71.$$ The one I missing is the p-value which I don't how to obtain. I know that this is a two tail test however this is the part I'm stuck on. Could anyone share their expertise with me so I may learn to do it on my own?",,"['statistics', 'hypothesis-testing']"
49,Reparametrization of multivariable fisher information,Reparametrization of multivariable fisher information,,"The task is to prove that reparametrization of multivariable Fisher information  with diffeomorphism $\eta: \Theta \rightarrow H$ is  $$ I(\eta)=\bigg(\frac{\partial \theta^T}{\partial \eta}\bigg)I(\theta)  \bigg(\frac{\partial \theta}{\partial \eta^T}\bigg), $$ for given $I(\theta)$. My solution: We know that multivariable Fisher information is  $$ I(\theta)=\mathbb{E}\bigg\{\frac{\partial}{\partial\theta}\log{(f_\theta(x))}\bigg[\frac{\partial}{\partial \theta}\log{(f_\theta(x))}\bigg]^T\bigg\}. $$ We can apply chain rule  $$ \frac{\partial}{\partial \eta}= \frac{\partial \theta}{\partial \eta}\frac{\partial}{\partial \theta} \Rightarrow  \frac{\partial}{\partial \theta} = \bigg(\frac{\partial \theta}{\partial \eta}\bigg)^{-1}\frac{\partial}{\partial \eta} %\theta'(\eta)\frac{\partial}{\partial\theta} $$ Applying it to the first equation and changing $\theta \mapsto \eta$ we obtain $$ I(\eta)=\mathbb{E}\bigg\{\frac{\partial\theta}{\partial\eta}\frac{\partial}{\partial\theta}\log{(f_\eta(x))}\bigg[\frac{\partial\theta}{\partial\eta}\frac{\partial}{\partial \theta}\log{(f_\eta(x))}\bigg]^T\bigg\}. $$ Now it's somewhere close to solution, what I would like to do is to take this $\big(\frac{\partial\theta}{\partial\eta}\big)$s out of the expected value and tell that it is done. But I don't think it is correct. Can anyone help?","The task is to prove that reparametrization of multivariable Fisher information  with diffeomorphism $\eta: \Theta \rightarrow H$ is  $$ I(\eta)=\bigg(\frac{\partial \theta^T}{\partial \eta}\bigg)I(\theta)  \bigg(\frac{\partial \theta}{\partial \eta^T}\bigg), $$ for given $I(\theta)$. My solution: We know that multivariable Fisher information is  $$ I(\theta)=\mathbb{E}\bigg\{\frac{\partial}{\partial\theta}\log{(f_\theta(x))}\bigg[\frac{\partial}{\partial \theta}\log{(f_\theta(x))}\bigg]^T\bigg\}. $$ We can apply chain rule  $$ \frac{\partial}{\partial \eta}= \frac{\partial \theta}{\partial \eta}\frac{\partial}{\partial \theta} \Rightarrow  \frac{\partial}{\partial \theta} = \bigg(\frac{\partial \theta}{\partial \eta}\bigg)^{-1}\frac{\partial}{\partial \eta} %\theta'(\eta)\frac{\partial}{\partial\theta} $$ Applying it to the first equation and changing $\theta \mapsto \eta$ we obtain $$ I(\eta)=\mathbb{E}\bigg\{\frac{\partial\theta}{\partial\eta}\frac{\partial}{\partial\theta}\log{(f_\eta(x))}\bigg[\frac{\partial\theta}{\partial\eta}\frac{\partial}{\partial \theta}\log{(f_\eta(x))}\bigg]^T\bigg\}. $$ Now it's somewhere close to solution, what I would like to do is to take this $\big(\frac{\partial\theta}{\partial\eta}\big)$s out of the expected value and tell that it is done. But I don't think it is correct. Can anyone help?",,"['statistics', 'fisher-information']"
50,"Finding the probability mass function for a pair of random variables $(X,Y)$",Finding the probability mass function for a pair of random variables,"(X,Y)","A pair of random variables $(X,Y)$ which take only integer values have the following pmf: $$ \begin{equation*}  p_{x,y}=P\{X=m,Y=n\}=\begin{cases}  K, \forall  \;\; m = 0, 1, 2\;\;; \mbox{ and } n=0,...,m;\\ 0, \mbox{ otherwise. }  \end{cases} \end{equation*} $$ I need to find K . I need to determine the mean and variance of $Y$. I've created a table to determine the probability of each value of $X$ (0,1,2) and $Y$ (0,1,2). Using this, I think $K$ is $1/9$, but I'm not entirely sure, and it's just by using logic that I came to this. However, I don't think that this is correct, because in the next step, I'm getting  $E[Y] = 2$ and $Var[Y] = - 7/3$ based on the previous values. Since variance can't be negative, I know this isn't correct. Can someone help point me in the right direction? I'm sure it's a pretty simple solution, but I'm looking for the exact steps to find this. Thanks!","A pair of random variables $(X,Y)$ which take only integer values have the following pmf: $$ \begin{equation*}  p_{x,y}=P\{X=m,Y=n\}=\begin{cases}  K, \forall  \;\; m = 0, 1, 2\;\;; \mbox{ and } n=0,...,m;\\ 0, \mbox{ otherwise. }  \end{cases} \end{equation*} $$ I need to find K . I need to determine the mean and variance of $Y$. I've created a table to determine the probability of each value of $X$ (0,1,2) and $Y$ (0,1,2). Using this, I think $K$ is $1/9$, but I'm not entirely sure, and it's just by using logic that I came to this. However, I don't think that this is correct, because in the next step, I'm getting  $E[Y] = 2$ and $Var[Y] = - 7/3$ based on the previous values. Since variance can't be negative, I know this isn't correct. Can someone help point me in the right direction? I'm sure it's a pretty simple solution, but I'm looking for the exact steps to find this. Thanks!",,"['probability', 'statistics', 'random-variables', 'bivariate-distributions']"
51,How can I determine that $Y_n$ can be represented as a funtion of $X_n$,How can I determine that  can be represented as a funtion of,Y_n X_n,"On enter I'm having a sequence of pairs $(X_n, Y_n)$. Consider the following two graphics of two possible sequences. in first case $Y_n$ can be modeled as $f(X_n)$ while in second case it obviously has no sense. The question is how can I determine if trying to represent $Y_n$ as $f(X_n)$ makes sense? Probable there is some criterium or something like that? What can be done in multivariate situation (i.e. when we're trying to represent $Y_n$ as $f(X_1^{(n)}, X_2^{(n)}, ..., X_k^{(n)})$)? Please note that trying to fit points with something graphicaly (e.g. like on first graph) and seeing if it fits the data is not OK. I'm looking for numerical criterium. UPD. the variant that I'm thinking of is trying to look at variation of sequence. I.e. we order $(X_n, Y_n)$ in such a way that $X_{k-1} < X_k < X_{k+1}$ and then look at $\sum_{i=1}^N \frac{|Y_i - Y_{i-1}|}{X_i - X_{i-1}}$. And there can even be an analog for multivariate case.","On enter I'm having a sequence of pairs $(X_n, Y_n)$. Consider the following two graphics of two possible sequences. in first case $Y_n$ can be modeled as $f(X_n)$ while in second case it obviously has no sense. The question is how can I determine if trying to represent $Y_n$ as $f(X_n)$ makes sense? Probable there is some criterium or something like that? What can be done in multivariate situation (i.e. when we're trying to represent $Y_n$ as $f(X_1^{(n)}, X_2^{(n)}, ..., X_k^{(n)})$)? Please note that trying to fit points with something graphicaly (e.g. like on first graph) and seeing if it fits the data is not OK. I'm looking for numerical criterium. UPD. the variant that I'm thinking of is trying to look at variation of sequence. I.e. we order $(X_n, Y_n)$ in such a way that $X_{k-1} < X_k < X_{k+1}$ and then look at $\sum_{i=1}^N \frac{|Y_i - Y_{i-1}|}{X_i - X_{i-1}}$. And there can even be an analog for multivariate case.",,"['calculus', 'statistics', 'regression']"
52,Filling out a Joint Distribution Tables Given the marginal distributions.,Filling out a Joint Distribution Tables Given the marginal distributions.,,Hello I am trying to figure out how to fill out a joint distribution table when I am only given the marginal distributions. $$ \begin{array}{c|ccc|l} _{\large X}\backslash^{\large Y} & 0 & 1 & 2 \\ \hline 0 & \text{?} & \text{?} & \text{?} & 0.5 \\ 1 & \text{?} & \text{?} & \text{?} & 0.25  \\ 2 & \text{?} & \text{?} & \text{?} & 0.25  \\ \hline & 0.5 & 0.25 & 0.25 \end{array} $$ Basically I don't need answers as much as the marginal distributions relate to finding the unknown numbers.,Hello I am trying to figure out how to fill out a joint distribution table when I am only given the marginal distributions. $$ \begin{array}{c|ccc|l} _{\large X}\backslash^{\large Y} & 0 & 1 & 2 \\ \hline 0 & \text{?} & \text{?} & \text{?} & 0.5 \\ 1 & \text{?} & \text{?} & \text{?} & 0.25  \\ 2 & \text{?} & \text{?} & \text{?} & 0.25  \\ \hline & 0.5 & 0.25 & 0.25 \end{array} $$ Basically I don't need answers as much as the marginal distributions relate to finding the unknown numbers.,,['statistics']
53,normal distribution help,normal distribution help,,"The tensile strength (ksi) of a certain type of steel at room temperature is normally distributed with a mean of 100 ksi and a standard deviation of 8.2 ksi. If X is the tensile strength of a randomly selected sample of this steel find P(X>115); well, since this is normal distribution, I found this to be P(Z/sqrt(8.2) > 5.115) by using x-mew / sigma > 115 - mew / sigma but i can't find the value since the z table only goes up to 3.49. any help would be appreciated Agggh! I somehow misread the question. many thanks to pointing it out!","The tensile strength (ksi) of a certain type of steel at room temperature is normally distributed with a mean of 100 ksi and a standard deviation of 8.2 ksi. If X is the tensile strength of a randomly selected sample of this steel find P(X>115); well, since this is normal distribution, I found this to be P(Z/sqrt(8.2) > 5.115) by using x-mew / sigma > 115 - mew / sigma but i can't find the value since the z table only goes up to 3.49. any help would be appreciated Agggh! I somehow misread the question. many thanks to pointing it out!",,"['statistics', 'normal-distribution']"
54,How to do Statistical hypothesis testing,How to do Statistical hypothesis testing,,"Let $\Omega$ be the finite sample space and random variable $X$ is defined on $\Omega$ sending $x$ to $x$. The probability distribution function of $X$ is denoted by $f(x;p_1,..p_r)$ where $p_i$ are parameters. Given a sequences $S=\{a_{i}\}_{i=1}^{n}$ where $a_{i} \in \Omega$, I want to know whether this sequence is sampled according to that probability distribution $(\Omega,f(x))$ or not. For example $\Omega = \{0,1\}$, \begin{eqnarray}f(x;p)= \begin{cases} p, &x=0\cr 1-p, &x=1 \cr 0, &otherwise\end{cases}, \end{eqnarray} $p=0.1$,$n=10$,$S=0100100011$, Then how to do Statistical hypothesis testing ?","Let $\Omega$ be the finite sample space and random variable $X$ is defined on $\Omega$ sending $x$ to $x$. The probability distribution function of $X$ is denoted by $f(x;p_1,..p_r)$ where $p_i$ are parameters. Given a sequences $S=\{a_{i}\}_{i=1}^{n}$ where $a_{i} \in \Omega$, I want to know whether this sequence is sampled according to that probability distribution $(\Omega,f(x))$ or not. For example $\Omega = \{0,1\}$, \begin{eqnarray}f(x;p)= \begin{cases} p, &x=0\cr 1-p, &x=1 \cr 0, &otherwise\end{cases}, \end{eqnarray} $p=0.1$,$n=10$,$S=0100100011$, Then how to do Statistical hypothesis testing ?",,"['probability-theory', 'statistics', 'statistical-inference', 'sampling', 'hypothesis-testing']"
55,Finding domain of variables in joint density for marginal density,Finding domain of variables in joint density for marginal density,,"Let $(X,Y)$ have joint density $f(x,y) = \frac{1}{2}(1+x+y)$ for $0<x<1$ and $0<y<1$. So the joint density of $X$ and $U=X+Y$ is $f_{X,U}(x,u)=\frac{1}{2}(1+u)$. Now it is simple to get the domain of $X$ since it is provided in the problem description, but to get the domain of $U$ seems so be trickier since you need to extract the case of $U<1$ and $U\geq 1$. I need these values in order to get the marginal densities, but I am not quite sure what I need to do after I get $0<U-X<1$.","Let $(X,Y)$ have joint density $f(x,y) = \frac{1}{2}(1+x+y)$ for $0<x<1$ and $0<y<1$. So the joint density of $X$ and $U=X+Y$ is $f_{X,U}(x,u)=\frac{1}{2}(1+u)$. Now it is simple to get the domain of $X$ since it is provided in the problem description, but to get the domain of $U$ seems so be trickier since you need to extract the case of $U<1$ and $U\geq 1$. I need these values in order to get the marginal densities, but I am not quite sure what I need to do after I get $0<U-X<1$.",,"['statistics', 'density-function']"
56,What is the best strategy for roulette?,What is the best strategy for roulette?,,"You start with $\$10$. You have a $\dfrac {18}{38}$ chance of winning, and if you win you get back double the money you spent. The minimum bet is $\$1$. How should you split your bets so that you make $\$20$ the fastest? This question was given to me by a friend, who in turn got the question from another student. So unfortunately I don't know the context or the exact wording. The only way I thought of interpreting the question is to see which strategy has the best expected value. If you bet $\$10$ directly, your expected value is $E_1 = 10 \cdot \dfrac {18}{38} - 10 \cdot \dfrac {20}{38}$. If you bet $\$5$ twice, your expected value is $E_2 = 2\left( 5 \cdot \dfrac {18}{38} - 5 \cdot \dfrac {20}{38} \right) = E_1.$ I don't see how splitting the bets in different ways would ever make a difference.","You start with $\$10$. You have a $\dfrac {18}{38}$ chance of winning, and if you win you get back double the money you spent. The minimum bet is $\$1$. How should you split your bets so that you make $\$20$ the fastest? This question was given to me by a friend, who in turn got the question from another student. So unfortunately I don't know the context or the exact wording. The only way I thought of interpreting the question is to see which strategy has the best expected value. If you bet $\$10$ directly, your expected value is $E_1 = 10 \cdot \dfrac {18}{38} - 10 \cdot \dfrac {20}{38}$. If you bet $\$5$ twice, your expected value is $E_2 = 2\left( 5 \cdot \dfrac {18}{38} - 5 \cdot \dfrac {20}{38} \right) = E_1.$ I don't see how splitting the bets in different ways would ever make a difference.",,"['probability', 'statistics']"
57,Finding Density From Expected Value,Finding Density From Expected Value,,"Problem: Given that $X \sim N(0,1)$, let $Y = e^{X}$. Find a formula for the density, $f_{Y}(y)$. Progress: Using the Law of the Unconscious Statistician, I computed $$E[Y] = E[e^{X}] = \sqrt{e}.$$ I figured that a potential solution had to satisfy both: $$\int_{\mathrm{R}} f_{Y}(y) \, dy = 1$$ $$\int_{\mathrm{R}} yf_{Y}(y) \, dy = \sqrt{e}$$ I tried playing around with integration by parts on the second one but realized that wasn't going to get me anywhere because I can't find $f'$ or $\int f \, dy$. Now that I'm typing it out, maybe I can make integration by parts work using $$F(y) = \int_{-\infty}^{t} f_{Y}(t) \, dt$$ If anyone has some insight that would be really helpful. I'm probably on the wrong track but I'm gonna go play around with this some more.","Problem: Given that $X \sim N(0,1)$, let $Y = e^{X}$. Find a formula for the density, $f_{Y}(y)$. Progress: Using the Law of the Unconscious Statistician, I computed $$E[Y] = E[e^{X}] = \sqrt{e}.$$ I figured that a potential solution had to satisfy both: $$\int_{\mathrm{R}} f_{Y}(y) \, dy = 1$$ $$\int_{\mathrm{R}} yf_{Y}(y) \, dy = \sqrt{e}$$ I tried playing around with integration by parts on the second one but realized that wasn't going to get me anywhere because I can't find $f'$ or $\int f \, dy$. Now that I'm typing it out, maybe I can make integration by parts work using $$F(y) = \int_{-\infty}^{t} f_{Y}(t) \, dt$$ If anyone has some insight that would be really helpful. I'm probably on the wrong track but I'm gonna go play around with this some more.",,"['calculus', 'probability']"
58,Calculus book: for statistics and computer science,Calculus book: for statistics and computer science,,"I hope this question hasn't been asked 1000 times before: Can someone recommend a good book on calculus such that: 1) It covers linear algebra (or maybe how to connect lin.algebra with calculus - the only connection in my head is Jacobian - but i can only use it as a monkey - by applying) 2) It covers the topics of calculus used in statistics (esp. bayesian analysis) 3) Covers stuff like Laplace transform (need to understand wiki references to MGFs) and maybe Fourier - used in machine learning for vision, hearing, etc. 4) Can be gradually increasing difficulty - i dont write many proofs (never did sigma-proofs) and generally prefer intuitive approach, but am looking for formal explanations too, so both approaches in the text are valued. 5) Has some instruction set on which exercises to do after each chapter (i have limited time and would like to do the exercises that reinforce the topic, and are not puzzles for advanced minds) 6) My particular intention is understanding advanced proofs in probability and statistics, and as a bonus it is always good to understand calculus per se (but not too much - i dont really need such advanced things as topology, analysis or sth like that - even though im not sure what those subjects are - except they are continuation of calculus, though maths rigor is good and welcomed), because it is literally the cornerstone of all mathematics and improves brain function =) My background: 1) Calculus 1-2 at the uni. Had multivariable calculus (gradients, double-triple integrals, stokes, green - last topics) with Adams textbook (that from my points of view jumps too much without leaving 1 picture in my mind - but it might be only me). I think i understand well differentiation but my integration skills are not that good and i dont really know well what all transforms are, i'm bad with sequences and series, and i don't need vector integration - i intend to use my skills to understand stats and machine learning so everything until vector calculus will do fine. I know nothing (except basics) about complex numbers (like i^2 = -1). 2) self taught Gilbert Strang and Philip N. Klein linear algebra up to SVD both of which i liked very much. 3) Reading on intro to probability by Blitzstein that I like very much myself 4) Also watched MIT lectures on multivariable calculus myself and liked them too. I have browsed questions in here and 3 books that appear everywhere are - Apostol, Spivak, Courant. But Everyone says they are too difficult. Im not sure im ready for those, though I took a glimpse at Apostol - i like him starting from integration because that's my weak spot. Though these books are huge and they require a lot of time - and id like to be able to do 1 book per 6 months possibly in not a very fast tempo. Also, id like to know which exercises to do in those to enforce knowledge. The particular reason im asking this quest is that if I start with the wrong book i might have to start over with another one so id like to hear more recommendations. So, in the light of above facts - can someone suggest maybe some book? Or just give some recommendation. I guess this question might pop up for many stats students or cs students that would like to improve their knowledge of calculus, but are not aimed at becoming mathematicians.","I hope this question hasn't been asked 1000 times before: Can someone recommend a good book on calculus such that: 1) It covers linear algebra (or maybe how to connect lin.algebra with calculus - the only connection in my head is Jacobian - but i can only use it as a monkey - by applying) 2) It covers the topics of calculus used in statistics (esp. bayesian analysis) 3) Covers stuff like Laplace transform (need to understand wiki references to MGFs) and maybe Fourier - used in machine learning for vision, hearing, etc. 4) Can be gradually increasing difficulty - i dont write many proofs (never did sigma-proofs) and generally prefer intuitive approach, but am looking for formal explanations too, so both approaches in the text are valued. 5) Has some instruction set on which exercises to do after each chapter (i have limited time and would like to do the exercises that reinforce the topic, and are not puzzles for advanced minds) 6) My particular intention is understanding advanced proofs in probability and statistics, and as a bonus it is always good to understand calculus per se (but not too much - i dont really need such advanced things as topology, analysis or sth like that - even though im not sure what those subjects are - except they are continuation of calculus, though maths rigor is good and welcomed), because it is literally the cornerstone of all mathematics and improves brain function =) My background: 1) Calculus 1-2 at the uni. Had multivariable calculus (gradients, double-triple integrals, stokes, green - last topics) with Adams textbook (that from my points of view jumps too much without leaving 1 picture in my mind - but it might be only me). I think i understand well differentiation but my integration skills are not that good and i dont really know well what all transforms are, i'm bad with sequences and series, and i don't need vector integration - i intend to use my skills to understand stats and machine learning so everything until vector calculus will do fine. I know nothing (except basics) about complex numbers (like i^2 = -1). 2) self taught Gilbert Strang and Philip N. Klein linear algebra up to SVD both of which i liked very much. 3) Reading on intro to probability by Blitzstein that I like very much myself 4) Also watched MIT lectures on multivariable calculus myself and liked them too. I have browsed questions in here and 3 books that appear everywhere are - Apostol, Spivak, Courant. But Everyone says they are too difficult. Im not sure im ready for those, though I took a glimpse at Apostol - i like him starting from integration because that's my weak spot. Though these books are huge and they require a lot of time - and id like to be able to do 1 book per 6 months possibly in not a very fast tempo. Also, id like to know which exercises to do in those to enforce knowledge. The particular reason im asking this quest is that if I start with the wrong book i might have to start over with another one so id like to hear more recommendations. So, in the light of above facts - can someone suggest maybe some book? Or just give some recommendation. I guess this question might pop up for many stats students or cs students that would like to improve their knowledge of calculus, but are not aimed at becoming mathematicians.",,"['calculus', 'probability', 'statistics', 'soft-question', 'book-recommendation']"
59,How many bottle of wine must be purchased to acquire all 19 corks,How many bottle of wine must be purchased to acquire all 19 corks,,"19 Crimes is a red wine that has 19 different corks (each being imprinted with a particular crime).  Assume that there is an infinite number of bottles and assume that there are an equal distribution of each of the corks.  The question is:  How many bottles of wine must be opened (and consumed) before you were at least 95% confident that the next bottle would contain the last different cork, making a collection of at least 19 corks, with 19 different corks?  Please explain your approach. The same problem can be approximated by assuming that there was a large bowl of skittles, and that there were 19 different flavors of skittles.  Assume that the large bowl had an infinite number of skittles and further assume that there was an equal distribution of different flavor skittles in the bowl.  How many skittles would need to be drawn before you were at least 95% confident that the next skittle would contain the last different flavored skittle.","19 Crimes is a red wine that has 19 different corks (each being imprinted with a particular crime).  Assume that there is an infinite number of bottles and assume that there are an equal distribution of each of the corks.  The question is:  How many bottles of wine must be opened (and consumed) before you were at least 95% confident that the next bottle would contain the last different cork, making a collection of at least 19 corks, with 19 different corks?  Please explain your approach. The same problem can be approximated by assuming that there was a large bowl of skittles, and that there were 19 different flavors of skittles.  Assume that the large bowl had an infinite number of skittles and further assume that there was an equal distribution of different flavor skittles in the bowl.  How many skittles would need to be drawn before you were at least 95% confident that the next skittle would contain the last different flavored skittle.",,"['probability', 'combinatorics', 'statistics']"
60,Variance and Expectation,Variance and Expectation,,"Two standard normal variables $Z_1$ and $Z_2$ have covariance 0.3. Find the mean and variance of the random variable $X = 3Z_1- Z_2$. I found out the variance by using the formulae $$\textrm{Var}[X+Y]=\textrm{Var}[X]+ 2\textrm{cov}(X,Y) +\textrm{Var}[Y]$$ however I am not able to find out the mean (which i think is the expected value). In the second question i also have to find the PDF when $Z_1$ and $Z_2$ are independent. Seems like i am missing a trigger point for the mean. Can someone route me to the right direction?","Two standard normal variables $Z_1$ and $Z_2$ have covariance 0.3. Find the mean and variance of the random variable $X = 3Z_1- Z_2$. I found out the variance by using the formulae $$\textrm{Var}[X+Y]=\textrm{Var}[X]+ 2\textrm{cov}(X,Y) +\textrm{Var}[Y]$$ however I am not able to find out the mean (which i think is the expected value). In the second question i also have to find the PDF when $Z_1$ and $Z_2$ are independent. Seems like i am missing a trigger point for the mean. Can someone route me to the right direction?",,"['statistics', 'normal-distribution']"
61,Marginal Probability of joint distribution,Marginal Probability of joint distribution,,"Consider random variables X,Y,Z with joint distribution x=1,....y-1, y=2,3,....z-1; z=3,4.... and 0< q <1 p+q=1 $$ Pr(X=x,Y=y,Z=z)=p^3  q^{z-3} $$ What is the marginal probability of random variable X? I found $$Pr(X=x)=p  q^{x-1}$$ Does it correct ? Because ı can not show the following equality  $$ \sum_{x}  Pr(X=x)=1 $$  Can someone show me how to prove the above equality with my answer or your answers ?","Consider random variables X,Y,Z with joint distribution x=1,....y-1, y=2,3,....z-1; z=3,4.... and 0< q <1 p+q=1 $$ Pr(X=x,Y=y,Z=z)=p^3  q^{z-3} $$ What is the marginal probability of random variable X? I found $$Pr(X=x)=p  q^{x-1}$$ Does it correct ? Because ı can not show the following equality  $$ \sum_{x}  Pr(X=x)=1 $$  Can someone show me how to prove the above equality with my answer or your answers ?",,"['probability', 'probability-theory', 'statistics', 'probability-distributions']"
62,Using moment generating function to calculate expectation of a random variable,Using moment generating function to calculate expectation of a random variable,,"I'm having difficulties with a statistics problem on the book Rice - Mathematical statistics regarding moment generating function. I was given a random variable $X$ and i derived that the first order derivative of mgf of $X$ is  $$M'(t) =  2(e^t/t - 2e^t/t^2 + 2e^t/t^3 -2/t^3)$$ To calculate the $E(X)$, I have to plug in $t=0$ into $M'(t)$. However since the terms have t as denominator, I cannot simply plug in $t=0$. I have seen solution online that takes the limit of $t$ approaching $0$ of the function $M'(t)$ instead to calculate $M'(0)$. But I don't understand why so. Can someone please help me?","I'm having difficulties with a statistics problem on the book Rice - Mathematical statistics regarding moment generating function. I was given a random variable $X$ and i derived that the first order derivative of mgf of $X$ is  $$M'(t) =  2(e^t/t - 2e^t/t^2 + 2e^t/t^3 -2/t^3)$$ To calculate the $E(X)$, I have to plug in $t=0$ into $M'(t)$. However since the terms have t as denominator, I cannot simply plug in $t=0$. I have seen solution online that takes the limit of $t$ approaching $0$ of the function $M'(t)$ instead to calculate $M'(0)$. But I don't understand why so. Can someone please help me?",,"['calculus', 'statistics']"
63,Expected area of rectangle,Expected area of rectangle,,"This question is taken from Rice - Mathematical statistics. A random rectangle is formed in the following way: The base, X, is chosen to be a uniform [0, 1] random variable and after having generated the base, the height is chosen to be uniform on [0, X]. Use the law of total expectation, Theorem A of Section 4.4.1, to find the expected circumference and area of the rectangle. I have trouble understanding the answer for finding the expected area of rectangle. Let H be the height of the rectangle. The area is XH, so $E(XH) = E(E(XH|X)) = E(XE(H|X)) = 1/2 E(X^2)$ Can someone kindly explain how do you derive the 3rd part from the 2nd part of the equation ?","This question is taken from Rice - Mathematical statistics. A random rectangle is formed in the following way: The base, X, is chosen to be a uniform [0, 1] random variable and after having generated the base, the height is chosen to be uniform on [0, X]. Use the law of total expectation, Theorem A of Section 4.4.1, to find the expected circumference and area of the rectangle. I have trouble understanding the answer for finding the expected area of rectangle. Let H be the height of the rectangle. The area is XH, so $E(XH) = E(E(XH|X)) = E(XE(H|X)) = 1/2 E(X^2)$ Can someone kindly explain how do you derive the 3rd part from the 2nd part of the equation ?",,['statistics']
64,Why is the expected value of the score function 0?,Why is the expected value of the score function 0?,,"I am currently learning about the Cramér-Rao bound. According to the lecture, $\mathbb{E}_\vartheta [U_\vartheta(X_1)] = 0$, where $\vartheta$ is a parameter, $X_1$ is a random variable with density $f(x, \vartheta)$ and $U_\vartheta(X_1)$ is the score function $$U_\vartheta(X_1) := \frac{\partial \log f (X_1, \vartheta)}{\partial \vartheta}$$ We have 4 regularization constraints, but for this question I think we only need (R4): (R4) $\int f(x, \vartheta) dx$ may be differentiated under the integral sign twice I don't unterstand the first part of the proof: From $\int_{-\infty}^{+ \infty} f(x, \vartheta) = 1$ follows with (R4): $$\int_{-\infty}^{+ \infty} \frac{\partial f(x, \vartheta)}{\partial \vartheta} dx = 0$$ Why is that equal to 0?","I am currently learning about the Cramér-Rao bound. According to the lecture, $\mathbb{E}_\vartheta [U_\vartheta(X_1)] = 0$, where $\vartheta$ is a parameter, $X_1$ is a random variable with density $f(x, \vartheta)$ and $U_\vartheta(X_1)$ is the score function $$U_\vartheta(X_1) := \frac{\partial \log f (X_1, \vartheta)}{\partial \vartheta}$$ We have 4 regularization constraints, but for this question I think we only need (R4): (R4) $\int f(x, \vartheta) dx$ may be differentiated under the integral sign twice I don't unterstand the first part of the proof: From $\int_{-\infty}^{+ \infty} f(x, \vartheta) = 1$ follows with (R4): $$\int_{-\infty}^{+ \infty} \frac{\partial f(x, \vartheta)}{\partial \vartheta} dx = 0$$ Why is that equal to 0?",,"['integration', 'statistics', 'parameter-estimation']"
65,Is there a convergence for the series $ \sum_{i=0}^{\infty} \frac{(x-y*i)^i}{i!} $?,Is there a convergence for the series ?, \sum_{i=0}^{\infty} \frac{(x-y*i)^i}{i!} ,The following series converges to exponential. $\sum_{i=0}^{\infty} \frac{x^i}{i!} = e^x$ Is the convergence of the following series known? $\sum_{i=0}^{\infty} \frac{(x - y*i)^i}{i!}$,The following series converges to exponential. $\sum_{i=0}^{\infty} \frac{x^i}{i!} = e^x$ Is the convergence of the following series known? $\sum_{i=0}^{\infty} \frac{(x - y*i)^i}{i!}$,,['calculus']
66,Decision Tree Probability - With Back Step,Decision Tree Probability - With Back Step,,"For the below decision tree, I can see how the probabilities of each end state are calculated... simply multiply the previous decisions: But for this one below, I'm totally stumped. It seems in my head the chance at resetting back to the first decision is completely negated because essentially the whole decision restarts like the first decision was never made. But based on the end probabilities this gives s3 a larger chance at being chosen. What does the math behind this look like? How are those final probabilities calculated given the reset in the decision tree?","For the below decision tree, I can see how the probabilities of each end state are calculated... simply multiply the previous decisions: But for this one below, I'm totally stumped. It seems in my head the chance at resetting back to the first decision is completely negated because essentially the whole decision restarts like the first decision was never made. But based on the end probabilities this gives s3 a larger chance at being chosen. What does the math behind this look like? How are those final probabilities calculated given the reset in the decision tree?",,"['probability', 'statistics']"
67,Getting confused over a T-test,Getting confused over a T-test,,"its been a while since I've done this and I am getting rather confused. Let's say I have two data sets of size $n_1$ and $n_2$ $X={X_1, X_2,.. X_{n_1}}$ $Y={Y_1, Y_2,.. Y_{n_2}}$ and want to construct a t-test. I have seen this formula in lots of books, what is the intuition and is this correct for finding a t-test? $T=\dfrac{\bar{X}-\bar{Y}}{\sqrt{\dfrac{s_1^2}{n_1}+\dfrac{s_2^2}{n_2}}}$ I am getting confused a little, I see some books telling me to be careful is my variances are equal. Can i not just put them into this formula either way? Also are we always referring to sample population, mean and standard deviation. How does the formula change if we have a the population data? How does this formula change if the data is paired? Thank you all very much :)","its been a while since I've done this and I am getting rather confused. Let's say I have two data sets of size $n_1$ and $n_2$ $X={X_1, X_2,.. X_{n_1}}$ $Y={Y_1, Y_2,.. Y_{n_2}}$ and want to construct a t-test. I have seen this formula in lots of books, what is the intuition and is this correct for finding a t-test? $T=\dfrac{\bar{X}-\bar{Y}}{\sqrt{\dfrac{s_1^2}{n_1}+\dfrac{s_2^2}{n_2}}}$ I am getting confused a little, I see some books telling me to be careful is my variances are equal. Can i not just put them into this formula either way? Also are we always referring to sample population, mean and standard deviation. How does the formula change if we have a the population data? How does this formula change if the data is paired? Thank you all very much :)",,"['statistics', 'statistical-inference', 'standard-deviation', 'means', 'hypothesis-testing']"
68,Expected area of a rectangle made by breaking a stick into four pieces,Expected area of a rectangle made by breaking a stick into four pieces,,You are given a straight stick of length 21.97 cm. You break the stick at a position chosen uniformly at random along its length. Each of the two stick portions you break in half and make a rectangle with the four bits of the stick. What is the expected area of the rectangle? If I call the first point the stick is broken at $X$ then I have obtained an expression for the area of the rectangle: $\frac{21.97X-X^2}4$. But I am not able to understand how to work out the value of $X$. I have no idea how to go about finding the area of the rectangle made by the four sticks. Any help would be much appreciated.,You are given a straight stick of length 21.97 cm. You break the stick at a position chosen uniformly at random along its length. Each of the two stick portions you break in half and make a rectangle with the four bits of the stick. What is the expected area of the rectangle? If I call the first point the stick is broken at $X$ then I have obtained an expression for the area of the rectangle: $\frac{21.97X-X^2}4$. But I am not able to understand how to work out the value of $X$. I have no idea how to go about finding the area of the rectangle made by the four sticks. Any help would be much appreciated.,,"['probability', 'statistics', 'random-variables', 'uniform-distribution']"
69,"What is the Probability of the Outcomes of a ""Repeat Until Success"" Experiment?","What is the Probability of the Outcomes of a ""Repeat Until Success"" Experiment?",,"I have an procedure that is known to be a Bernoulli trial with two possible outcomes. Let the probability of the outcome labeled success be $p$. If the experiment is to repeat the procedure until a successful outcome is acquired, what is the probability of the different outcomes (all of which are described by $n$ fails followed by $1$ success)?","I have an procedure that is known to be a Bernoulli trial with two possible outcomes. Let the probability of the outcome labeled success be $p$. If the experiment is to repeat the procedure until a successful outcome is acquired, what is the probability of the different outcomes (all of which are described by $n$ fails followed by $1$ success)?",,"['probability', 'statistics', 'probability-distributions']"
70,What does glassware accuracy and precision (tolerance) mean statistically?,What does glassware accuracy and precision (tolerance) mean statistically?,,"According to Daniel C. Harris (Table 2-5, 9th edition), a 25 mL volumetric pipet has an accuracy of +- 0.06 mL and a precision of +- 0.02 mL. I'm trying to understand what these values mean from a statistical definition: 1.) a.) Is the ""accuracy"" value of 0.06 mL indicate a tolerance interval? If so, what is the confidence level and tolerance level(is it assumed 99% for both?) for that tolerance interval? In other words, if 100 experiments of 100 trials using that pipet were done: 99 of those 100 experiments would have at least 99 trials with results between 24.94 and 25.06? 1.) b.) Or is ""accuracy"" simply the TOLERANCE (permissible range of variation  from its nominal value) of the pipet? If so, how is tolerance for glassware determined? Is it arbitrary or is it based on the error inherent in the production process (like the mold used to hold the smelted glass)? 2.) Is the ""precision"" value of 0.02 mL indicate the population standard deviation? In other words, is it saying that 65% (one standard deviation for a normally distributed population) of all measurements would fall within 24.98 and 25.02? 3.) Does the precision of the glassware assume perfect sampling technique? If so, can the precision uncertainty be directly compounded with the sampling technique uncertainty? For example: If the pipet was used 10 times to reveal a sample standard deviation of +- 0.03 mL. Does it mean that 0.02 of that interval is due to the glassware and 0.01 of that interval is a consequence of sampling technique variance? NOTE: I asked this same question on Physics.exchange but realized it may be more appropriate here.","According to Daniel C. Harris (Table 2-5, 9th edition), a 25 mL volumetric pipet has an accuracy of +- 0.06 mL and a precision of +- 0.02 mL. I'm trying to understand what these values mean from a statistical definition: 1.) a.) Is the ""accuracy"" value of 0.06 mL indicate a tolerance interval? If so, what is the confidence level and tolerance level(is it assumed 99% for both?) for that tolerance interval? In other words, if 100 experiments of 100 trials using that pipet were done: 99 of those 100 experiments would have at least 99 trials with results between 24.94 and 25.06? 1.) b.) Or is ""accuracy"" simply the TOLERANCE (permissible range of variation  from its nominal value) of the pipet? If so, how is tolerance for glassware determined? Is it arbitrary or is it based on the error inherent in the production process (like the mold used to hold the smelted glass)? 2.) Is the ""precision"" value of 0.02 mL indicate the population standard deviation? In other words, is it saying that 65% (one standard deviation for a normally distributed population) of all measurements would fall within 24.98 and 25.02? 3.) Does the precision of the glassware assume perfect sampling technique? If so, can the precision uncertainty be directly compounded with the sampling technique uncertainty? For example: If the pipet was used 10 times to reveal a sample standard deviation of +- 0.03 mL. Does it mean that 0.02 of that interval is due to the glassware and 0.01 of that interval is a consequence of sampling technique variance? NOTE: I asked this same question on Physics.exchange but realized it may be more appropriate here.",,"['probability', 'statistics', 'almost-everywhere']"
71,show that $X/(X+Y) $ has a cauchy distribution if $X $ and $X+Y $ are standard normal,show that  has a cauchy distribution if  and  are standard normal,X/(X+Y)  X  X+Y ,"A random variable $X$ has a cauchy distribution with parameters $a$ and $b$ is the density of $X$ is $f(x\mid a,b)=\dfrac{1}{\pi b}\dfrac{1}{1+(\frac{x-a}{b})^2}$ where $-\infty <x< \infty $, $-\infty <a <\infty$, $b>0$ Suppose $X$ and $Y$ are independent standard normal random variables then show that $X/(X+Y)$ has a cauchy distribution. Since $X$ and $Y$ are independent standard normal random variables then the joint pdf for $(X,Y)$ is $$f_{X,Y}(x,y)=\frac{1}{2\pi}e^{-x^2/2}e^{-y^2/2}$$ I used the Jacobian method to try and find $f_{U,V}$ so I let $U=X+Y$ and $V=X/(X+Y)$ so $x=uv$ and $y=u-uv$ and then $J=u$. So $f_{U,V}(u,v)=\dfrac{u}{2\pi}e^{-(uv)^2/2}e^{-(u-uv)^2/2}=\dfrac{u}{2\pi}e^{-(uv)^2/2}e^{-(u^2-2u^2v + u^2v^2)/2}=\dfrac{u}{2\pi}e^{-(u^2v^2)/2}e^{(-u^2/2)+(u^2v) - (u^2v^2/2)}= \dfrac{u}{2\pi}e^{-(u^2v^2)-(u^2/2)+(u^2v)} = \dfrac{u}{2\pi}e^{-u^2(v^2-v+(1/2))} $ Therefore $$f_{U,V}(u,v)= \dfrac{u}{2\pi}e^{-u^2(v^2-v+(1/2))} ; -\infty<u,v<\infty$$ I now want to find the $V$ marginal density and that should have a cauchy distribution. $f_V(v)=2\int_0^{\infty} \dfrac{u}{2\pi}e^{-u^2(v^2-v+(1/2))}du=\int_0^{\infty} \dfrac{u}{\pi}e^{-u^2(v^2-v+(1/2))}du$. If I let $s=u^2$ then $\dfrac{ds}{2u}=du$ and then integral then becomes $ \int_0^{\infty} \dfrac{1}{2\pi}e^{-s(v^2-v+(1/2))}ds = \dfrac{1}{2\pi}\bigg( \dfrac{1}{v^2-v+(1/2)} \bigg)$. I think I might have made a mistake somewhere because I cant see how I can rearrange this to show that $f_V$ is a cauchy distribution.","A random variable $X$ has a cauchy distribution with parameters $a$ and $b$ is the density of $X$ is $f(x\mid a,b)=\dfrac{1}{\pi b}\dfrac{1}{1+(\frac{x-a}{b})^2}$ where $-\infty <x< \infty $, $-\infty <a <\infty$, $b>0$ Suppose $X$ and $Y$ are independent standard normal random variables then show that $X/(X+Y)$ has a cauchy distribution. Since $X$ and $Y$ are independent standard normal random variables then the joint pdf for $(X,Y)$ is $$f_{X,Y}(x,y)=\frac{1}{2\pi}e^{-x^2/2}e^{-y^2/2}$$ I used the Jacobian method to try and find $f_{U,V}$ so I let $U=X+Y$ and $V=X/(X+Y)$ so $x=uv$ and $y=u-uv$ and then $J=u$. So $f_{U,V}(u,v)=\dfrac{u}{2\pi}e^{-(uv)^2/2}e^{-(u-uv)^2/2}=\dfrac{u}{2\pi}e^{-(uv)^2/2}e^{-(u^2-2u^2v + u^2v^2)/2}=\dfrac{u}{2\pi}e^{-(u^2v^2)/2}e^{(-u^2/2)+(u^2v) - (u^2v^2/2)}= \dfrac{u}{2\pi}e^{-(u^2v^2)-(u^2/2)+(u^2v)} = \dfrac{u}{2\pi}e^{-u^2(v^2-v+(1/2))} $ Therefore $$f_{U,V}(u,v)= \dfrac{u}{2\pi}e^{-u^2(v^2-v+(1/2))} ; -\infty<u,v<\infty$$ I now want to find the $V$ marginal density and that should have a cauchy distribution. $f_V(v)=2\int_0^{\infty} \dfrac{u}{2\pi}e^{-u^2(v^2-v+(1/2))}du=\int_0^{\infty} \dfrac{u}{\pi}e^{-u^2(v^2-v+(1/2))}du$. If I let $s=u^2$ then $\dfrac{ds}{2u}=du$ and then integral then becomes $ \int_0^{\infty} \dfrac{1}{2\pi}e^{-s(v^2-v+(1/2))}ds = \dfrac{1}{2\pi}\bigg( \dfrac{1}{v^2-v+(1/2)} \bigg)$. I think I might have made a mistake somewhere because I cant see how I can rearrange this to show that $f_V$ is a cauchy distribution.",,"['statistics', 'normal-distribution', 'transformation']"
72,"Convergence in probabiliy, just not getting it","Convergence in probabiliy, just not getting it",,"Let $\{X_n,n>1\}$ be a sequence of i.i.d random variables with the probabilty density function $$f(x) =e^{-(x-a)} \text{ for } x\geq a.$$ Set $Y_n = \min\{X_1,X_2,\ldots,X_n\}$. Show that $Y_n$ converges in probability to $a$ as $n \to \infty$ So there's the question! The only info we have is the definition of the convergence in probability.","Let $\{X_n,n>1\}$ be a sequence of i.i.d random variables with the probabilty density function $$f(x) =e^{-(x-a)} \text{ for } x\geq a.$$ Set $Y_n = \min\{X_1,X_2,\ldots,X_n\}$. Show that $Y_n$ converges in probability to $a$ as $n \to \infty$ So there's the question! The only info we have is the definition of the convergence in probability.",,"['probability', 'statistics', 'convergence-divergence']"
73,"Expected value of Poisson pdf $p(a\mid X)$, where $X$ is also Poisson distributed","Expected value of Poisson pdf , where  is also Poisson distributed",p(a\mid X) X,"I had a hard time figuring out how to phrase this question, so please forgive the confusing title. To be concrete: the expected value of a function $g(X)$ of a random variable $X$, where $X$ has the pdf $f(x)$, can be computed as: $$ E[g(X)] = \int_R \! g(x) f(x) \, \mathrm{d}x $$ Now in my case, I have $$ f(x) = \mathrm{Poisson}(x\mid\lambda) = \frac{\lambda^x}{x!}{e^{-\lambda}} $$ and $g(x)$ is also Poisson, except its mean is $X$, so we have $$ g(x) = \mathrm{Poisson}(a\mid x) = \frac{x^a}{a!}{e^{-x}} $$ and what I want is the integral $$ E[\mathrm{Poisson}(a\mid X)] = \int_R \!  \mathrm{Poisson}(a\mid x) \mathrm{Poisson} (x\mid\lambda) \, \mathrm{d}x \\  = \int_0^\infty \! \frac{x^a}{a!}{e^{-x}} \frac{\lambda^x}{x!}{e^{-\lambda}} \, \mathrm{d}x $$ or the discrete version is fine as well: $$ E[\mathrm{Poisson}(a\mid X)] = \sum_x \frac{x^a}{a!}{e^{-x}} \frac{\lambda^x}{x!}{e^{-\lambda}} $$ Do either of these have an analytic solution? What if $$ \mathrm{Poisson}(a\mid x) $$ was approximated by a normal distribution? Physically this situation arises when e.g. a Poisson process produces some random number of particles, which then go on to trigger another cascade of particles, and we are interested in the pdf of the number of particles in the second cascade. So we sum over all the possible numbers of particles coming from the first process. This is straightforward to do numerically, but I just wonder if there isn't a way to do it analytically... I tried to compute it via characteristic functions but that just made it worse I think: $$ E[\mathrm{Poisson}(a\mid X)] = \frac{1}{2\pi}\frac{1}{a!}\Gamma(a+1) \int_{-\infty}^{+\infty} (1-i \omega)^{-a-1}\cdot e^{\lambda\left(e^{i \omega} - 1 \right)} \, \mathrm{d} \omega$$ (assuming I didn't screw up getting that far)","I had a hard time figuring out how to phrase this question, so please forgive the confusing title. To be concrete: the expected value of a function $g(X)$ of a random variable $X$, where $X$ has the pdf $f(x)$, can be computed as: $$ E[g(X)] = \int_R \! g(x) f(x) \, \mathrm{d}x $$ Now in my case, I have $$ f(x) = \mathrm{Poisson}(x\mid\lambda) = \frac{\lambda^x}{x!}{e^{-\lambda}} $$ and $g(x)$ is also Poisson, except its mean is $X$, so we have $$ g(x) = \mathrm{Poisson}(a\mid x) = \frac{x^a}{a!}{e^{-x}} $$ and what I want is the integral $$ E[\mathrm{Poisson}(a\mid X)] = \int_R \!  \mathrm{Poisson}(a\mid x) \mathrm{Poisson} (x\mid\lambda) \, \mathrm{d}x \\  = \int_0^\infty \! \frac{x^a}{a!}{e^{-x}} \frac{\lambda^x}{x!}{e^{-\lambda}} \, \mathrm{d}x $$ or the discrete version is fine as well: $$ E[\mathrm{Poisson}(a\mid X)] = \sum_x \frac{x^a}{a!}{e^{-x}} \frac{\lambda^x}{x!}{e^{-\lambda}} $$ Do either of these have an analytic solution? What if $$ \mathrm{Poisson}(a\mid x) $$ was approximated by a normal distribution? Physically this situation arises when e.g. a Poisson process produces some random number of particles, which then go on to trigger another cascade of particles, and we are interested in the pdf of the number of particles in the second cascade. So we sum over all the possible numbers of particles coming from the first process. This is straightforward to do numerically, but I just wonder if there isn't a way to do it analytically... I tried to compute it via characteristic functions but that just made it worse I think: $$ E[\mathrm{Poisson}(a\mid X)] = \frac{1}{2\pi}\frac{1}{a!}\Gamma(a+1) \int_{-\infty}^{+\infty} (1-i \omega)^{-a-1}\cdot e^{\lambda\left(e^{i \omega} - 1 \right)} \, \mathrm{d} \omega$$ (assuming I didn't screw up getting that far)",,"['probability', 'statistics', 'probability-distributions']"
74,Proof of Kurtosis for a sum of independent random Variables,Proof of Kurtosis for a sum of independent random Variables,,"I am trying to understand a proof for the Kurtosis of a sum of independent random variables, however, there is one part where I am quite stuck: Theorem: for $X_1, X_2, ..., X_n$ independent random variables with means $ \mu_1, ... , \mu_n$ and variance $\sigma_1^2, ... , \sigma_n^2 $  $E(X_i^4) < \infty$ Define $S_n = X_1 + ... + X_n$ and $S_n$ will be appropriately normal $ kurt(S_n) - 3 = (\sum_{i=1}^n\sigma_i^n)^{-2}\sum_{i=1}^n\sigma_i^4(kurt(X_i)-3)$ Proof (first part): assume WLOG that $E[X_i] = 0$ for all $i$ $ kurt(Sn) = \frac{E[(X_1 + ... + X_n )^4]}{(\sigma_1^2 + ... +\sigma_n^2)^2 } $ $ E[(X_1 + ... + X_n )^4] = \sum_{i=1}^n\sum_{j=1}^n\sum_{k=1}^n\sum_{l=1}^n E(X_iX_jX_kX_l) = \sum_{i=1}^n E(X_i^4) +6  \sum_{i<j}^n\sigma_i^2\sigma_j^2$ NOTE: $E(X_iX_jX_kX_l) = 0 $ unless $ i=j=k=l $ or if combinations of two pairs. Question: why is the NOTE: true? From my understanding the expected value of INDEPENDENT random variables is equal to the product of the expected values of the random variables. However I intuitively understand this case does not apply, or else, we would not get very far! So what is going on? Assuming this hickup true, i understand the rest of the proof, which I will not write. (as i am improvising the syntax)","I am trying to understand a proof for the Kurtosis of a sum of independent random variables, however, there is one part where I am quite stuck: Theorem: for $X_1, X_2, ..., X_n$ independent random variables with means $ \mu_1, ... , \mu_n$ and variance $\sigma_1^2, ... , \sigma_n^2 $  $E(X_i^4) < \infty$ Define $S_n = X_1 + ... + X_n$ and $S_n$ will be appropriately normal $ kurt(S_n) - 3 = (\sum_{i=1}^n\sigma_i^n)^{-2}\sum_{i=1}^n\sigma_i^4(kurt(X_i)-3)$ Proof (first part): assume WLOG that $E[X_i] = 0$ for all $i$ $ kurt(Sn) = \frac{E[(X_1 + ... + X_n )^4]}{(\sigma_1^2 + ... +\sigma_n^2)^2 } $ $ E[(X_1 + ... + X_n )^4] = \sum_{i=1}^n\sum_{j=1}^n\sum_{k=1}^n\sum_{l=1}^n E(X_iX_jX_kX_l) = \sum_{i=1}^n E(X_i^4) +6  \sum_{i<j}^n\sigma_i^2\sigma_j^2$ NOTE: $E(X_iX_jX_kX_l) = 0 $ unless $ i=j=k=l $ or if combinations of two pairs. Question: why is the NOTE: true? From my understanding the expected value of INDEPENDENT random variables is equal to the product of the expected values of the random variables. However I intuitively understand this case does not apply, or else, we would not get very far! So what is going on? Assuming this hickup true, i understand the rest of the proof, which I will not write. (as i am improvising the syntax)",,"['statistics', 'random-variables', 'expectation', 'proof-explanation', 'independence']"
75,how to find area under normal distribution curve,how to find area under normal distribution curve,,"Find out the area in percentage under standard normal distribution curve of random variable $Z$ within limits from $-3$ to $3$. my try: probability density function of standard normal distribution is $f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$ now the area under standard normal distribution curve ($-3\le x\le3$), $$=\frac{1}{\sqrt{2\pi}}\int_{-3}^3e^{-\frac{x^2}{2}}\ dx$$ $$=2\frac{1}{\sqrt{2\pi}}\int_{0}^3e^{-\frac{x^2}{2}}\ dx$$  $$=-\frac{2}{\sqrt{2\pi}}\int_{0}^3e^{-\frac{x^2}{2}}\ dx$$ $$=-\frac{2}{\sqrt{2\pi}}\left(\int_{0}^{\infty}e^{-\frac{x^2}{2}}\ dx-\int_{3}^{\infty}e^{-\frac{x^2}{2}}\ dx\right)$$ $$=-\frac{2}{\sqrt{2\pi}}\left(\frac 12-\int_{3}^{\infty}e^{-\frac{x^2}{2}}\ dx\right)$$ i got stuck here, i don't have any clue to solve above integral. please help me to solve it or give some other method to find the area under the curve.","Find out the area in percentage under standard normal distribution curve of random variable $Z$ within limits from $-3$ to $3$. my try: probability density function of standard normal distribution is $f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$ now the area under standard normal distribution curve ($-3\le x\le3$), $$=\frac{1}{\sqrt{2\pi}}\int_{-3}^3e^{-\frac{x^2}{2}}\ dx$$ $$=2\frac{1}{\sqrt{2\pi}}\int_{0}^3e^{-\frac{x^2}{2}}\ dx$$  $$=-\frac{2}{\sqrt{2\pi}}\int_{0}^3e^{-\frac{x^2}{2}}\ dx$$ $$=-\frac{2}{\sqrt{2\pi}}\left(\int_{0}^{\infty}e^{-\frac{x^2}{2}}\ dx-\int_{3}^{\infty}e^{-\frac{x^2}{2}}\ dx\right)$$ $$=-\frac{2}{\sqrt{2\pi}}\left(\frac 12-\int_{3}^{\infty}e^{-\frac{x^2}{2}}\ dx\right)$$ i got stuck here, i don't have any clue to solve above integral. please help me to solve it or give some other method to find the area under the curve.",,"['statistics', 'normal-distribution', 'area']"
76,Find the conditional variance of multivariate normal distribution variables,Find the conditional variance of multivariate normal distribution variables,,"Random variables $x,y,z$ have a multivariate normal distribution with mean $\mu_x$ $\mu_y$ and $\mu_z$; and variance and covariance: $\sigma_x^2$,$\sigma_y^2$,$\sigma_z^2$,$\sigma_{xy}$,$\sigma_{xz}$,$\sigma_{yz}$. Please find the conditional variance $\operatorname{Var}[y\mid x,z]$. Also, it is known that $\operatorname{E}[y\mid x]=\beta x+\alpha$ and $\operatorname{E}[y\mid x,z]=\beta_x x+\beta_z z+\alpha_2$ So far, I have used the variance decomposition rule and have gotten: $\operatorname{Var}[y\mid x]=\operatorname{Var}_z[\operatorname{E}[y\mid x,z]] + \operatorname{E}_z[\operatorname{Var}[y\mid x,z]]$, and $\operatorname{Var}[x\mid y] = \operatorname{Var}[y]-\beta^2\operatorname{Var}[x]$, but I am confused with the conditional mean and variance, and don't know how to proceed. Any hint or help will be greatly appreciated, thanks!!!","Random variables $x,y,z$ have a multivariate normal distribution with mean $\mu_x$ $\mu_y$ and $\mu_z$; and variance and covariance: $\sigma_x^2$,$\sigma_y^2$,$\sigma_z^2$,$\sigma_{xy}$,$\sigma_{xz}$,$\sigma_{yz}$. Please find the conditional variance $\operatorname{Var}[y\mid x,z]$. Also, it is known that $\operatorname{E}[y\mid x]=\beta x+\alpha$ and $\operatorname{E}[y\mid x,z]=\beta_x x+\beta_z z+\alpha_2$ So far, I have used the variance decomposition rule and have gotten: $\operatorname{Var}[y\mid x]=\operatorname{Var}_z[\operatorname{E}[y\mid x,z]] + \operatorname{E}_z[\operatorname{Var}[y\mid x,z]]$, and $\operatorname{Var}[x\mid y] = \operatorname{Var}[y]-\beta^2\operatorname{Var}[x]$, but I am confused with the conditional mean and variance, and don't know how to proceed. Any hint or help will be greatly appreciated, thanks!!!",,"['probability', 'statistics']"
77,Regression: Combining errors with idempotent symmetric matrix gives chi-squared distribution,Regression: Combining errors with idempotent symmetric matrix gives chi-squared distribution,,"Proposition: Consider a linear regression model $Y_n=X_\pi \beta + \varepsilon_n$ . $X_\pi$ is $n\times p_n$ matrix, and the errors $$\varepsilon _n=\left ( \varepsilon _{n1},...,\varepsilon _{nn} \right )'$$ consists of $n$ independent and identically distributed variables, $\varepsilon _{ni} \sim N(0,1)$ , for $i=1,...,n$ . Consider $M_\pi =X_\pi\left ( X_\pi'X_\pi \right )^{-1}X_\pi'$ . Then, $$\varepsilon _n' M_\pi \varepsilon _n \sim \chi ^2\left ( p_n \right ),$$ where $M_\pi$ is idempotent and symmetric matrix. How can I prove $\varepsilon _n' M_\pi \varepsilon _n $ follows chi-squared distribution??","Proposition: Consider a linear regression model . is matrix, and the errors consists of independent and identically distributed variables, , for . Consider . Then, where is idempotent and symmetric matrix. How can I prove follows chi-squared distribution??","Y_n=X_\pi \beta + \varepsilon_n X_\pi n\times p_n \varepsilon _n=\left ( \varepsilon _{n1},...,\varepsilon _{nn} \right )' n \varepsilon _{ni} \sim N(0,1) i=1,...,n M_\pi =X_\pi\left ( X_\pi'X_\pi \right )^{-1}X_\pi' \varepsilon _n' M_\pi \varepsilon _n \sim \chi ^2\left ( p_n \right ), M_\pi \varepsilon _n' M_\pi \varepsilon _n ","['statistics', 'statistical-inference', 'regression']"
78,"Interpretation of ""average""","Interpretation of ""average""",,"First of all, let me remark that I am not sure whether or not this question is on-topic here; if it is not, any hint as to where it should be posted instead is welcome! (One option would be to ask in an ""English Language"" community; however, I guess a similar ambiguity exists in many other languages as well, so it is more a math issue than a language one.) I am wondering whether there is a ""standard"" interpretation of the term ""average"" in a sentence such as ""On average, customers were refunded $x$ percent of their expenses"". I can think of at least two different interpretations which I'll illustrate with the following example: Assume that customer A spends 500 and is refunded 50, so his refund is 10%. Customer B spends 2000 and is refunded 100, which is 5%. By the first interpretation, the average refund would be $(10\% + 5\%)/2 = 7.5\%$. For the second interpretation, we compute the total expenses ($500 + 2000 = 2500$) and the total refunds ($50 + 100 = 150$), which is $6\%$ of the total cost, so this value could also be interpreted as the average refund. Would you say in this scenario that on average, customers were refunded $7.5$ percent or $6$ percent of their costs?","First of all, let me remark that I am not sure whether or not this question is on-topic here; if it is not, any hint as to where it should be posted instead is welcome! (One option would be to ask in an ""English Language"" community; however, I guess a similar ambiguity exists in many other languages as well, so it is more a math issue than a language one.) I am wondering whether there is a ""standard"" interpretation of the term ""average"" in a sentence such as ""On average, customers were refunded $x$ percent of their expenses"". I can think of at least two different interpretations which I'll illustrate with the following example: Assume that customer A spends 500 and is refunded 50, so his refund is 10%. Customer B spends 2000 and is refunded 100, which is 5%. By the first interpretation, the average refund would be $(10\% + 5\%)/2 = 7.5\%$. For the second interpretation, we compute the total expenses ($500 + 2000 = 2500$) and the total refunds ($50 + 100 = 150$), which is $6\%$ of the total cost, so this value could also be interpreted as the average refund. Would you say in this scenario that on average, customers were refunded $7.5$ percent or $6$ percent of their costs?",,"['statistics', 'soft-question']"
79,Vector Space and Column Space of linear models?,Vector Space and Column Space of linear models?,,"I've started reading a book on linear models, and it turns out there's a lot of linear algebra (which I'm a bit weak in). There were two exercises that I think might be simple, but I don't know if I'm doing right. (1) You are given that $X$ is an $(n \times p)$ matrix and $b ∈ R^p$. Prove that the linear model $y =Xb ∈ R^n$ satisfies the conditions of a vector space. (2) For a linear model $y = Xb$, is the origin 0 in the column space $C(X)$? Why or why not? OK, so for (1) I think what I need to do is show that for any two vectors that are in $y$, then the addition of those two vectors are also in $y$ (addition rule). The other condition I think, is that for any scalar $q$ in $R^n$, then $qy$ must also be in $y$. If I have this right, can someone show me how I go about proving this? My current answer seemed a little simple, which is why I doubt it. For (2), it actually confuses me. I thought the column space referred to all possible linear combinations of the columns of $X$ where the scalars which multiply each column is nonzero. So by definition, I thought you can't have the zero vector be in the column space of $X$? Seems a little weird to me, unless I'm missing something obvious. I'd really appreciate any help.","I've started reading a book on linear models, and it turns out there's a lot of linear algebra (which I'm a bit weak in). There were two exercises that I think might be simple, but I don't know if I'm doing right. (1) You are given that $X$ is an $(n \times p)$ matrix and $b ∈ R^p$. Prove that the linear model $y =Xb ∈ R^n$ satisfies the conditions of a vector space. (2) For a linear model $y = Xb$, is the origin 0 in the column space $C(X)$? Why or why not? OK, so for (1) I think what I need to do is show that for any two vectors that are in $y$, then the addition of those two vectors are also in $y$ (addition rule). The other condition I think, is that for any scalar $q$ in $R^n$, then $qy$ must also be in $y$. If I have this right, can someone show me how I go about proving this? My current answer seemed a little simple, which is why I doubt it. For (2), it actually confuses me. I thought the column space referred to all possible linear combinations of the columns of $X$ where the scalars which multiply each column is nonzero. So by definition, I thought you can't have the zero vector be in the column space of $X$? Seems a little weird to me, unless I'm missing something obvious. I'd really appreciate any help.",,"['linear-algebra', 'statistics', 'vector-spaces', 'regression', 'linear-regression']"
80,Relationship between X and its Projection Matrix,Relationship between X and its Projection Matrix,,"Suppose $Q_{1}$ is an $n\times p $ matrix (derived from the $QR$ Decomposition of $X$) whose columns provide an orthonormal basis for the subspace ${\chi}$ of $\mathbb R^{n}$ spanned by the columns of an $n\times p$ matrix $X = (x_1,...,x_p)$. The hat matrix $H = Q_{1}Q_{1}^{T}$ projects vectors orthogonally onto $X$. Suppose the first two rows of $X$ are the same. Explain why the first two rows of $H$ are the same.","Suppose $Q_{1}$ is an $n\times p $ matrix (derived from the $QR$ Decomposition of $X$) whose columns provide an orthonormal basis for the subspace ${\chi}$ of $\mathbb R^{n}$ spanned by the columns of an $n\times p$ matrix $X = (x_1,...,x_p)$. The hat matrix $H = Q_{1}Q_{1}^{T}$ projects vectors orthogonally onto $X$. Suppose the first two rows of $X$ are the same. Explain why the first two rows of $H$ are the same.",,"['linear-algebra', 'matrices', 'statistics', 'least-squares']"
81,"Basic Statistics, confused. Events","Basic Statistics, confused. Events",,"There are two questions that I am having problems with. The first one is  this one: Event A: an even number turns up               Event B: a 5 or 6 turns up A die is rolled once: The probability of getting a 1 is 1/21,  a 2 is 2/21, a 3 is 3/21, a 4 is 4/21,a 5 is 5/21, and a 6 is 6/21. They ask you to find P(A), P(B), P(AUB), P(AnB), P(AlB), P(BlA), and if the events are independent. I have an idea of how to do it and my results were: P(A): 4/7 ---- P(b): 11/21 ----P(AnB): 6/21----- P(AUB) 17/21 -----P(AlB): 6/11 ------ P(BlA): 1/2 and the events to not be independent because (AnB) =/= P(A) x P(B) I am not exactly sure if I'm right. The second one is this one: An unfair coin (Probability of 0.6 for heads) is flipped twice. Given that on at least one flip a head occurred, what is the probability that a head occurred on both flips. So in my mind, this question is really simple, but the wording makes me believe that I am overlooking something and that it is a bit more complicated. am I supposed to do like an x chooses n / binomial formula thing. Thank you in advance to anyone that can help me.","There are two questions that I am having problems with. The first one is  this one: Event A: an even number turns up               Event B: a 5 or 6 turns up A die is rolled once: The probability of getting a 1 is 1/21,  a 2 is 2/21, a 3 is 3/21, a 4 is 4/21,a 5 is 5/21, and a 6 is 6/21. They ask you to find P(A), P(B), P(AUB), P(AnB), P(AlB), P(BlA), and if the events are independent. I have an idea of how to do it and my results were: P(A): 4/7 ---- P(b): 11/21 ----P(AnB): 6/21----- P(AUB) 17/21 -----P(AlB): 6/11 ------ P(BlA): 1/2 and the events to not be independent because (AnB) =/= P(A) x P(B) I am not exactly sure if I'm right. The second one is this one: An unfair coin (Probability of 0.6 for heads) is flipped twice. Given that on at least one flip a head occurred, what is the probability that a head occurred on both flips. So in my mind, this question is really simple, but the wording makes me believe that I am overlooking something and that it is a bit more complicated. am I supposed to do like an x chooses n / binomial formula thing. Thank you in advance to anyone that can help me.",,"['probability', 'statistics']"
82,Proving Sauer-Shelah's theorum,Proving Sauer-Shelah's theorum,,"I've been working on some problems in introductory combinatorics, and I would like some help on going about proving Sauer-Shelah's theorem. A family $\mathcal{F}$ shatters a set $A$ if for every $B \subset A$, there is an $F \in \mathcal{F}$ such that $F \cap A = B.$ I want to show that if $\mathcal{F} \subset 2^{[n]}$ (i.e. the power set of $[n]$) and $|\mathcal{F}| > \sum_{i=0}^k \binom{n}{i}$, then there is a set $A \subset [n]$ of size $k+1$ such that $\mathcal{F}$ shatters $A$. I started this by going about an ""assume for the sake of contradiction"" proof in which I stated that for all sets $A \subset [n]$ of size $k+1$, $\mathcal{F}$ does not shatter $A$. Thus, for every $A$ that satisfies the above, there exists a $B \subset A$ such that there is no $F \in \mathcal{F}$ such that $F \cap A = B.$ The problem is, I am having some difficulty figuring out how the construction of $\mathcal{F}$ makes meaningful implications about the contents of $B$ in this situation. Is it possible that I should go about this proof directly? Any recommendations would be appreciated.","I've been working on some problems in introductory combinatorics, and I would like some help on going about proving Sauer-Shelah's theorem. A family $\mathcal{F}$ shatters a set $A$ if for every $B \subset A$, there is an $F \in \mathcal{F}$ such that $F \cap A = B.$ I want to show that if $\mathcal{F} \subset 2^{[n]}$ (i.e. the power set of $[n]$) and $|\mathcal{F}| > \sum_{i=0}^k \binom{n}{i}$, then there is a set $A \subset [n]$ of size $k+1$ such that $\mathcal{F}$ shatters $A$. I started this by going about an ""assume for the sake of contradiction"" proof in which I stated that for all sets $A \subset [n]$ of size $k+1$, $\mathcal{F}$ does not shatter $A$. Thus, for every $A$ that satisfies the above, there exists a $B \subset A$ such that there is no $F \in \mathcal{F}$ such that $F \cap A = B.$ The problem is, I am having some difficulty figuring out how the construction of $\mathcal{F}$ makes meaningful implications about the contents of $B$ in this situation. Is it possible that I should go about this proof directly? Any recommendations would be appreciated.",,"['statistics', 'discrete-mathematics']"
83,Why don't likelihood ratio tests all go to one?,Why don't likelihood ratio tests all go to one?,,"Suppose we have the following general setup where we wish to test the hypothesis, $$ H_0 : \theta\in\Theta_0 ~~~~~~~~~~~~~~~~~~~~~~~~~H_1 : \theta\not\in\Theta_0. $$ It seems to be pretty well established that under some regularity conditions that a likelihood ratio test, $$ \Lambda\left(\mathbf{X}\right) = \frac{\sup_{\theta\in\Theta_0} \mathcal{L}\left(\theta;\mathbf{X}\right)}{\sup_{\theta\in\Theta} \mathcal{L}\left(\theta;\mathbf{X}\right)} $$ has the property that $-2\log\Lambda\left(\mathbf{X}\right)\sim\chi^2_k$, where $k = \dim\left(\Theta\right) - \dim\left(\Theta_0\right)$ asymptotically. I don't understand why $\Lambda\left(\mathbf{X}\right) \to 1$ in the limit of infinite data when the null hypothesis is true. Allow me to explain. We know that maximum likelihood is asymptotically consistent such that $$ \lim_{n\to\infty} \Pr\left[\theta = \hat{\theta}\right] = 1. $$ Therefore, if the null is true (i.e. $\theta\in\Theta_0$) it should also be the case that with probability one $\hat{\theta} \in\Theta_0$. By definition, $\hat{\theta}$ maximizes the denominator of the likelihood ratio, but because $\hat{\theta}\in\Theta_0$, it is clear that $\hat{\theta}$ also maximizes the numerator. Additionally, because they are maximized by the same point, the numerator and denominator are equal so that, $$ \Lambda\left(\mathbf{X}\right) = \frac{\mathcal{L}\left(\hat{\theta};\mathbf{X}\right)}{\mathcal{L}\left(\hat{\theta};\mathbf{X}\right)} = 1. $$ Moreover, $-2\log\Lambda\left(\mathbf{X}\right)=0$. I know there is a problem with this reasoning, but I don't know what it is. It seems that the correction must have to do with the additional $k$ free parameters that were assumed to be present in the unrestricted parameter space $\Theta$.","Suppose we have the following general setup where we wish to test the hypothesis, $$ H_0 : \theta\in\Theta_0 ~~~~~~~~~~~~~~~~~~~~~~~~~H_1 : \theta\not\in\Theta_0. $$ It seems to be pretty well established that under some regularity conditions that a likelihood ratio test, $$ \Lambda\left(\mathbf{X}\right) = \frac{\sup_{\theta\in\Theta_0} \mathcal{L}\left(\theta;\mathbf{X}\right)}{\sup_{\theta\in\Theta} \mathcal{L}\left(\theta;\mathbf{X}\right)} $$ has the property that $-2\log\Lambda\left(\mathbf{X}\right)\sim\chi^2_k$, where $k = \dim\left(\Theta\right) - \dim\left(\Theta_0\right)$ asymptotically. I don't understand why $\Lambda\left(\mathbf{X}\right) \to 1$ in the limit of infinite data when the null hypothesis is true. Allow me to explain. We know that maximum likelihood is asymptotically consistent such that $$ \lim_{n\to\infty} \Pr\left[\theta = \hat{\theta}\right] = 1. $$ Therefore, if the null is true (i.e. $\theta\in\Theta_0$) it should also be the case that with probability one $\hat{\theta} \in\Theta_0$. By definition, $\hat{\theta}$ maximizes the denominator of the likelihood ratio, but because $\hat{\theta}\in\Theta_0$, it is clear that $\hat{\theta}$ also maximizes the numerator. Additionally, because they are maximized by the same point, the numerator and denominator are equal so that, $$ \Lambda\left(\mathbf{X}\right) = \frac{\mathcal{L}\left(\hat{\theta};\mathbf{X}\right)}{\mathcal{L}\left(\hat{\theta};\mathbf{X}\right)} = 1. $$ Moreover, $-2\log\Lambda\left(\mathbf{X}\right)=0$. I know there is a problem with this reasoning, but I don't know what it is. It seems that the correction must have to do with the additional $k$ free parameters that were assumed to be present in the unrestricted parameter space $\Theta$.",,"['statistics', 'maximum-likelihood', 'log-likelihood']"
84,Basic probability space problem,Basic probability space problem,,"I'm taking a signal analysis course with a strong emphasis on probability. Long story short, the professor is assigning homework from the text which he said was optional, and expects us to learn the material straight out of that text. Many of these problems have to do with probability spaces, something which I feel completely lost with, not to mention set theory in general. I have here an example problem but would love if someone could help explain a solution in very general terms applying to probability spaces as a whole. Please bear in mind I'm basically floundering in the water until my textbook comes in the mail, and would appreciate a very simple explanation. The Problem: A receiver listens to the data stream of zeros until the first one appears. Then $S = (0,01, 001, ...)$. Choose $A = 2^S$ and show that $(S, A, P)$ is a probability space. Use your own probability measure $P$. My approach: I'm dimly aware of the criteria for a probability space, but I'm probably wrong somewhere: $S$ must be a set: Check. $A$ must be a sigma-algebra of $S$: i) $S \in A$ : What does it mean if $S$ is in $2^S$? How is this expressed in terms of sets? ii)  $ x \in A \Rightarrow \bar{x} \in A $: What would be the complement of some element in $A$ here? My best guess would be the next element in the sequence; i.e. the complement of 001 would be 0001 because the sequence continues if a 0 is chosen in the third place instead of a one. However, that also works for the infinite elements following 001, and surely they can't all be complements? iii)  $ x, y \in A \Rightarrow x \cup y \in A$: I'm not sure how to express a union here. Isn't there only one set, the ""union"" of which would be itself? $P$ must be a (countably additive) probability measure of $S$: The biggest issue here is not with the criteria but the actual construction of this probability measure. It seems to me that each term in the series has half the probability of the one preceding it; i.e. the $nth$ term has a $1/(2^n)$ chance of defining the actual event. This is a geometric series adding to 1, and links to the definition of the sigma-algebra from before, but how do I express this as a probability measure? i) $P(S) = 1$ : Check. ii) $P($any  element  of $A) \geq 0 $: Check. iii) $P(A \cup B) = P(A) + P(B), A \cap B = \varnothing$: What is the significance of A and B here? An example would be very helpful. Thanks for any help you can offer, and sorry if the formatting is bad. I made an account and learned a bit of LaTex just to post this conundrum!","I'm taking a signal analysis course with a strong emphasis on probability. Long story short, the professor is assigning homework from the text which he said was optional, and expects us to learn the material straight out of that text. Many of these problems have to do with probability spaces, something which I feel completely lost with, not to mention set theory in general. I have here an example problem but would love if someone could help explain a solution in very general terms applying to probability spaces as a whole. Please bear in mind I'm basically floundering in the water until my textbook comes in the mail, and would appreciate a very simple explanation. The Problem: A receiver listens to the data stream of zeros until the first one appears. Then $S = (0,01, 001, ...)$. Choose $A = 2^S$ and show that $(S, A, P)$ is a probability space. Use your own probability measure $P$. My approach: I'm dimly aware of the criteria for a probability space, but I'm probably wrong somewhere: $S$ must be a set: Check. $A$ must be a sigma-algebra of $S$: i) $S \in A$ : What does it mean if $S$ is in $2^S$? How is this expressed in terms of sets? ii)  $ x \in A \Rightarrow \bar{x} \in A $: What would be the complement of some element in $A$ here? My best guess would be the next element in the sequence; i.e. the complement of 001 would be 0001 because the sequence continues if a 0 is chosen in the third place instead of a one. However, that also works for the infinite elements following 001, and surely they can't all be complements? iii)  $ x, y \in A \Rightarrow x \cup y \in A$: I'm not sure how to express a union here. Isn't there only one set, the ""union"" of which would be itself? $P$ must be a (countably additive) probability measure of $S$: The biggest issue here is not with the criteria but the actual construction of this probability measure. It seems to me that each term in the series has half the probability of the one preceding it; i.e. the $nth$ term has a $1/(2^n)$ chance of defining the actual event. This is a geometric series adding to 1, and links to the definition of the sigma-algebra from before, but how do I express this as a probability measure? i) $P(S) = 1$ : Check. ii) $P($any  element  of $A) \geq 0 $: Check. iii) $P(A \cup B) = P(A) + P(B), A \cap B = \varnothing$: What is the significance of A and B here? An example would be very helpful. Thanks for any help you can offer, and sorry if the formatting is bad. I made an account and learned a bit of LaTex just to post this conundrum!",,"['probability', 'statistics']"
85,What does the p-value really mean?,What does the p-value really mean?,,"Suppose I fit a linear model m1 to the data produced below: #true parameters     x = rnorm(100,5,1)     b = 0.5     e = rnorm(100,0,3)     beta_0= 2.5     beta_1= 0.5     y = beta_0 + beta_1*x + e     plot(x,y)    #linear fit     m1 = lm(y~x)     abline(m1)     summary(m1) The p-value I get is 0.66. When I sample x again (rerun the code), and fit the regression the p-value is 0.05. Why do I get different p-values for different samples, and how do you reconcile these differences?","Suppose I fit a linear model m1 to the data produced below: #true parameters     x = rnorm(100,5,1)     b = 0.5     e = rnorm(100,0,3)     beta_0= 2.5     beta_1= 0.5     y = beta_0 + beta_1*x + e     plot(x,y)    #linear fit     m1 = lm(y~x)     abline(m1)     summary(m1) The p-value I get is 0.66. When I sample x again (rerun the code), and fit the regression the p-value is 0.05. Why do I get different p-values for different samples, and how do you reconcile these differences?",,"['probability', 'statistics']"
86,block design and group testing,block design and group testing,,This is a question from course notes: The answer I'm given is something like this: So my questions are: 1.How do we know there should be 9 tests? why not 8 tests or 10 tests? 2.How many items are we testing? is it 9 items or 18 items? It doesn't make sense that we are doing 9 tests on 9 items.,This is a question from course notes: The answer I'm given is something like this: So my questions are: 1.How do we know there should be 9 tests? why not 8 tests or 10 tests? 2.How many items are we testing? is it 9 items or 18 items? It doesn't make sense that we are doing 9 tests on 9 items.,,"['statistics', 'discrete-mathematics', 'combinatorial-designs']"
87,average of averages vs sum of averages,average of averages vs sum of averages,,"Note, to potential ""duplicate"" claimants, there is a similar question posted here . However, 1. that post is actually asking a different question and 2. that question has removed the code in the OP and thus is difficult to follow.  Either way, it does not answer my question. Main Question I have calculated some monthly averages. I want to find out which season has the strongest value for animal presence. Do I sum the means of each month for each season summer = jun average + jul average + aug average Or do I find the average of those averages? summer = jun average + jul average + aug average / 3 Which method is correct for finding out the season with the highest and lowest values? Context to aid answering the question is provided below Details Say we have a 4x4 square with 16 cells. We lay this square on the on a beach and measure the presence of animals in each cell. Weekly Statistics We fill in each cell with the following rule If a animal is present in a cell, cell value = 1 if the cell is empty (animal does NOT appear), cell value = 0 This results in a cell like so, Monthly Totals We repeat this each week of each month. We add up the weekly quadrats to create  month summary, where each cell has a number between 0 and 4. 0 = no animal present in any of the weeks 1 = animal present in 1/4 of the weeks 2 = animal present in 2/4 of the weeks 3 = animal present in 3/4 of the weeks 4 = animal was present in every week value count  1          0      2 2          1      3 3          2      3 4          3      4 5          4      3 Monthly Statistics sum = sum of count (i.e. 1+4+2+1+0+1: sum of cells...) mean = sum / # of rows (i.e. # of weeks + 1) sum    mean  jan          10   2 feb          23   4.6  mar          45   9 apr          15   3 Summary Question Do I sum the averages of each month for each season? summer = jun average + jul average + aug average Or do I find the average of those averages? summer = jun average + jul average + aug average / 3 Which method is correct for finding out the season with the highest and lowest values? Desired output...a value showing which season generally has more animals. Should I sum the monthly averages or average the monthly averages? season value 1 autumn 85 2 spring 40 3 summer 62 4 winter 70","Note, to potential ""duplicate"" claimants, there is a similar question posted here . However, 1. that post is actually asking a different question and 2. that question has removed the code in the OP and thus is difficult to follow.  Either way, it does not answer my question. Main Question I have calculated some monthly averages. I want to find out which season has the strongest value for animal presence. Do I sum the means of each month for each season summer = jun average + jul average + aug average Or do I find the average of those averages? summer = jun average + jul average + aug average / 3 Which method is correct for finding out the season with the highest and lowest values? Context to aid answering the question is provided below Details Say we have a 4x4 square with 16 cells. We lay this square on the on a beach and measure the presence of animals in each cell. Weekly Statistics We fill in each cell with the following rule If a animal is present in a cell, cell value = 1 if the cell is empty (animal does NOT appear), cell value = 0 This results in a cell like so, Monthly Totals We repeat this each week of each month. We add up the weekly quadrats to create  month summary, where each cell has a number between 0 and 4. 0 = no animal present in any of the weeks 1 = animal present in 1/4 of the weeks 2 = animal present in 2/4 of the weeks 3 = animal present in 3/4 of the weeks 4 = animal was present in every week value count  1          0      2 2          1      3 3          2      3 4          3      4 5          4      3 Monthly Statistics sum = sum of count (i.e. 1+4+2+1+0+1: sum of cells...) mean = sum / # of rows (i.e. # of weeks + 1) sum    mean  jan          10   2 feb          23   4.6  mar          45   9 apr          15   3 Summary Question Do I sum the averages of each month for each season? summer = jun average + jul average + aug average Or do I find the average of those averages? summer = jun average + jul average + aug average / 3 Which method is correct for finding out the season with the highest and lowest values? Desired output...a value showing which season generally has more animals. Should I sum the monthly averages or average the monthly averages? season value 1 autumn 85 2 spring 40 3 summer 62 4 winter 70",,"['statistics', 'average']"
88,How many boys and girls should be included in the sample.,How many boys and girls should be included in the sample.,,"This is a statistics related question which follows an introduction to stratified sampling. A survey to estimate the number of vegetarians in a mixed college with 660 boys and 540 girls is carried out. A sample of 40 students is required. How many boys and girls should be included? If you assign weights to boys and girl then you should chose 22 boys and 18 girls, which is the answer in the book. But is this the correct approach to use here? Why should we assign weights based on gender here? I feel a random selection of 40 would be representative in this case. Something else to consider: Why stratify based on gender? Why not stratify based on other factors that might affect whether someone is vegetarian or not? For example whether the student was raised on a farm or in a town. Or whether the student is an athlete or not? I just feel that unless your experiment requires data on boys and girls then this stratification is unnecessary here.","This is a statistics related question which follows an introduction to stratified sampling. A survey to estimate the number of vegetarians in a mixed college with 660 boys and 540 girls is carried out. A sample of 40 students is required. How many boys and girls should be included? If you assign weights to boys and girl then you should chose 22 boys and 18 girls, which is the answer in the book. But is this the correct approach to use here? Why should we assign weights based on gender here? I feel a random selection of 40 would be representative in this case. Something else to consider: Why stratify based on gender? Why not stratify based on other factors that might affect whether someone is vegetarian or not? For example whether the student was raised on a farm or in a town. Or whether the student is an athlete or not? I just feel that unless your experiment requires data on boys and girls then this stratification is unnecessary here.",,['statistics']
89,Statistics - Expectation of OLS residual squared,Statistics - Expectation of OLS residual squared,,Suppose that $y = \beta_0+\beta_1x + \epsilon$ where $\operatorname{E}[\epsilon\mid X]=0$ and $\operatorname{Var}[\epsilon\mid X] = \sigma^2$. In what special case is $\hat{\epsilon_i}^2$ an unbiased estimator for $\sigma^2$? Is $\hat{\epsilon_i}^2$ a consistent estimator for $\sigma^2$? I have proved that $$E[\hat{\epsilon_i}^2] = \sigma^2\left(1-\frac{1}{n}-  \frac{(x_i -\bar{x})^2}{\sum_{j=1}^n (x_j -\bar{x})^2} \right)$$ But I don't know how it can be unbiased or maybe I have proved wrong? My proof:  \begin{equation} \begin{split} \nonumber  \hat{\epsilon_i} & = y_i - \hat{y_i} \\  & = \beta_0 + \beta_1 x_i + \epsilon_i - \hat{\beta_0} - \hat{\beta_1}x_i \\  & = \epsilon_i + (\beta_0 - \hat{\beta_0}) + (\beta_1 - \hat{\beta_1 })x_i \\  \hat{\beta_0} &= \bar{y} - \hat{\beta_1} \bar{x} \\  & = \frac{1}{n} \sum_{i=1}^{n}(\beta_0 + \beta_1 x_i + \epsilon_i) - \hat{\beta_1} \bar{x} \\  & = \beta_0 + (\beta_1 - \hat{\beta_1})\bar{x} + \frac{1}{n} \sum_{i=1}^{n} \epsilon_i \\ \hat{\beta_1} &= \frac{\sum (x_i - \bar{x}) y_i}{\sum (x_i - \bar{x})^2} \\  & = \frac{\sum (x_i - \bar{x})(\beta_0 + \beta_1 x_i + \epsilon_i)}{\sum (x_i - \bar{x})^2} \\  & = \beta_1 + \frac{\sum (x_i - \bar{x})\epsilon_i}{\sum (x_i - \bar{x})^2} \\  \therefore \hat{\epsilon_i} &= \epsilon_i - \frac{1}{n}\sum_{j=1}^{n} \epsilon_j + (\beta_1 - \hat{\beta_1})(x_i - \bar{x}) \\  & =  \epsilon_i - \bar{\epsilon} - \frac{\sum_{j=1}^{n} (x_j - \bar{x})\epsilon_j}{\sum_{j=1}^{n} (x_j - \bar{x})^2}  (x_i - \bar{x}) \\  \therefore \hat{\epsilon_i}^2 & = \epsilon_i^2 + \bar{\epsilon}^2 +\frac{(x_i -\bar{x})^2}{[\sum_{j=1}^{n} (x_j -\bar{x})^2]^2} [\sum_{j=1}^{n} (x_j -\bar{x}) \epsilon_j]^2 \\ & -2 \epsilon_i \bar{\epsilon} - 2\epsilon_i \frac{x_i - \bar{x}}{\sum_{j=1}^{n}(x_j - \bar{x})^2} \sum_{j=1}^{n} (x_j - \bar{x}) \epsilon_j + 2 \bar{\epsilon} \frac{x_i - \bar{x}}{\sum_{j=1}^{n}(x_j - \bar{x})^2} \sum_{j=1}^{n} (x_j - \bar{x}) \epsilon_j \\                                                      \end{split} \end{equation} Now take expectation of the six parts of $\hat{\epsilon_i}^2$ given X (conditional symbol is omitted):  \begin{equation} \begin{split} \nonumber  (1) \ E[\epsilon_i^2] &= \sigma^2 \\  (2) \ E[\bar{\epsilon}^2] &= \frac{\sigma^2}{n} \\  (3) \ E[\cdot] & =  \frac{(x_i -\bar{x})^2}{[\sum_{j=1}^{n} (x_j -\bar{x})^2]^2} E\left[[\sum_{j=1}^{n} (x_j -\bar{x}) \epsilon_j]^2 \right] \\   &= \frac{(x_i -\bar{x})^2}{[\sum_{j=1}^{n} (x_j -\bar{x})^2]^2} E\left[ \sum_{j=1}^{n}(x_j-\bar{x})^2 \epsilon_j^2 + 2 \sum_{j=1}^{n-1} \sum_{k=j+1}^{n} (x_j-\bar{x})\epsilon_j (x_k-\bar{x})\epsilon_k \right] \\   & =  \frac{(x_i -\bar{x})^2 \sigma^2}{\sum_{j=1}^{n} (x_j -\bar{x})^2} \\  (4) \ E[-2\epsilon_i \bar{\epsilon}] & = -\frac{2}{n} E[\epsilon_i \sum_{j=1}^{n} \epsilon_j] \\  & =  -\frac{2\sigma^2}{n} \\  (5) \ E[\cdot] &= -2 \frac{x_i - \bar{x}}{\sum_{j=1}^{n}(x_j - \bar{x})^2}  E\left[ \epsilon_i \sum_{j=1}^{n} (x_j - \bar{x}) \epsilon_j\right] \\  & =  -2 \frac{(x_i - \bar{x})^2 \sigma^2}{\sum_{j=1}^{n}(x_j - \bar{x})^2} \\  (6) \ E[\cdot] &= 2 \frac{x_i - \bar{x}}{\sum_{j=1}^{n}(x_j - \bar{x})^2} E\left[ \bar{\epsilon} \sum_{j=1}^{n} (x_j - \bar{x}) \epsilon_j\right] \\  & = 2 \frac{x_i - \bar{x}}{\sum_{j=1}^{n}(x_j - \bar{x})^2} \frac{1}{n} E[\sum_{j=1}^{n} (x_j -\bar{x}) \epsilon_j^2] \\  & = 2 \frac{x_i - \bar{x}}{\sum_{j=1}^{n}(x_j - \bar{x})^2} \frac{1}{n} \sigma^2 \sum_{j=1}^{n} (x_j -\bar{x}) \\  & = 0 \\  \therefore E[\hat{\epsilon_i}^2] & = \sigma^2 - \frac{\sigma^2}{n} -  \frac{(x_i -\bar{x})^2 \sigma^2}{\sum_{j=1}^{n} (x_j -\bar{x})^2} \\   & = \sigma^2\left(1-\frac{1}{n}-  \frac{(x_i -\bar{x})^2}{\sum_{j=1}^{n} (x_j -\bar{x})^2} \right) \end{split} \end{equation},Suppose that $y = \beta_0+\beta_1x + \epsilon$ where $\operatorname{E}[\epsilon\mid X]=0$ and $\operatorname{Var}[\epsilon\mid X] = \sigma^2$. In what special case is $\hat{\epsilon_i}^2$ an unbiased estimator for $\sigma^2$? Is $\hat{\epsilon_i}^2$ a consistent estimator for $\sigma^2$? I have proved that $$E[\hat{\epsilon_i}^2] = \sigma^2\left(1-\frac{1}{n}-  \frac{(x_i -\bar{x})^2}{\sum_{j=1}^n (x_j -\bar{x})^2} \right)$$ But I don't know how it can be unbiased or maybe I have proved wrong? My proof:  \begin{equation} \begin{split} \nonumber  \hat{\epsilon_i} & = y_i - \hat{y_i} \\  & = \beta_0 + \beta_1 x_i + \epsilon_i - \hat{\beta_0} - \hat{\beta_1}x_i \\  & = \epsilon_i + (\beta_0 - \hat{\beta_0}) + (\beta_1 - \hat{\beta_1 })x_i \\  \hat{\beta_0} &= \bar{y} - \hat{\beta_1} \bar{x} \\  & = \frac{1}{n} \sum_{i=1}^{n}(\beta_0 + \beta_1 x_i + \epsilon_i) - \hat{\beta_1} \bar{x} \\  & = \beta_0 + (\beta_1 - \hat{\beta_1})\bar{x} + \frac{1}{n} \sum_{i=1}^{n} \epsilon_i \\ \hat{\beta_1} &= \frac{\sum (x_i - \bar{x}) y_i}{\sum (x_i - \bar{x})^2} \\  & = \frac{\sum (x_i - \bar{x})(\beta_0 + \beta_1 x_i + \epsilon_i)}{\sum (x_i - \bar{x})^2} \\  & = \beta_1 + \frac{\sum (x_i - \bar{x})\epsilon_i}{\sum (x_i - \bar{x})^2} \\  \therefore \hat{\epsilon_i} &= \epsilon_i - \frac{1}{n}\sum_{j=1}^{n} \epsilon_j + (\beta_1 - \hat{\beta_1})(x_i - \bar{x}) \\  & =  \epsilon_i - \bar{\epsilon} - \frac{\sum_{j=1}^{n} (x_j - \bar{x})\epsilon_j}{\sum_{j=1}^{n} (x_j - \bar{x})^2}  (x_i - \bar{x}) \\  \therefore \hat{\epsilon_i}^2 & = \epsilon_i^2 + \bar{\epsilon}^2 +\frac{(x_i -\bar{x})^2}{[\sum_{j=1}^{n} (x_j -\bar{x})^2]^2} [\sum_{j=1}^{n} (x_j -\bar{x}) \epsilon_j]^2 \\ & -2 \epsilon_i \bar{\epsilon} - 2\epsilon_i \frac{x_i - \bar{x}}{\sum_{j=1}^{n}(x_j - \bar{x})^2} \sum_{j=1}^{n} (x_j - \bar{x}) \epsilon_j + 2 \bar{\epsilon} \frac{x_i - \bar{x}}{\sum_{j=1}^{n}(x_j - \bar{x})^2} \sum_{j=1}^{n} (x_j - \bar{x}) \epsilon_j \\                                                      \end{split} \end{equation} Now take expectation of the six parts of $\hat{\epsilon_i}^2$ given X (conditional symbol is omitted):  \begin{equation} \begin{split} \nonumber  (1) \ E[\epsilon_i^2] &= \sigma^2 \\  (2) \ E[\bar{\epsilon}^2] &= \frac{\sigma^2}{n} \\  (3) \ E[\cdot] & =  \frac{(x_i -\bar{x})^2}{[\sum_{j=1}^{n} (x_j -\bar{x})^2]^2} E\left[[\sum_{j=1}^{n} (x_j -\bar{x}) \epsilon_j]^2 \right] \\   &= \frac{(x_i -\bar{x})^2}{[\sum_{j=1}^{n} (x_j -\bar{x})^2]^2} E\left[ \sum_{j=1}^{n}(x_j-\bar{x})^2 \epsilon_j^2 + 2 \sum_{j=1}^{n-1} \sum_{k=j+1}^{n} (x_j-\bar{x})\epsilon_j (x_k-\bar{x})\epsilon_k \right] \\   & =  \frac{(x_i -\bar{x})^2 \sigma^2}{\sum_{j=1}^{n} (x_j -\bar{x})^2} \\  (4) \ E[-2\epsilon_i \bar{\epsilon}] & = -\frac{2}{n} E[\epsilon_i \sum_{j=1}^{n} \epsilon_j] \\  & =  -\frac{2\sigma^2}{n} \\  (5) \ E[\cdot] &= -2 \frac{x_i - \bar{x}}{\sum_{j=1}^{n}(x_j - \bar{x})^2}  E\left[ \epsilon_i \sum_{j=1}^{n} (x_j - \bar{x}) \epsilon_j\right] \\  & =  -2 \frac{(x_i - \bar{x})^2 \sigma^2}{\sum_{j=1}^{n}(x_j - \bar{x})^2} \\  (6) \ E[\cdot] &= 2 \frac{x_i - \bar{x}}{\sum_{j=1}^{n}(x_j - \bar{x})^2} E\left[ \bar{\epsilon} \sum_{j=1}^{n} (x_j - \bar{x}) \epsilon_j\right] \\  & = 2 \frac{x_i - \bar{x}}{\sum_{j=1}^{n}(x_j - \bar{x})^2} \frac{1}{n} E[\sum_{j=1}^{n} (x_j -\bar{x}) \epsilon_j^2] \\  & = 2 \frac{x_i - \bar{x}}{\sum_{j=1}^{n}(x_j - \bar{x})^2} \frac{1}{n} \sigma^2 \sum_{j=1}^{n} (x_j -\bar{x}) \\  & = 0 \\  \therefore E[\hat{\epsilon_i}^2] & = \sigma^2 - \frac{\sigma^2}{n} -  \frac{(x_i -\bar{x})^2 \sigma^2}{\sum_{j=1}^{n} (x_j -\bar{x})^2} \\   & = \sigma^2\left(1-\frac{1}{n}-  \frac{(x_i -\bar{x})^2}{\sum_{j=1}^{n} (x_j -\bar{x})^2} \right) \end{split} \end{equation},,['statistics']
90,Show that $\lambda(t)=\frac{f(t)}{S(t)}=-\frac{d}{dt} \log(S(t))$,Show that,\lambda(t)=\frac{f(t)}{S(t)}=-\frac{d}{dt} \log(S(t)),"Let $T$ a continuous non-negative random variable that represents a failure time.   Taking $S(t)=P(T\geq t)$ and $f(t)$ the probability density function   of $T$. Show that   $$\lambda(t)=\frac{f(t)}{S(t)}=-\frac{d}{dt} \log S(t)$$ where   $\lambda(t)$ represents a failure rate. I had some additional information $$\lambda(t)=\frac{P(T\geq t)-P(T\geq t+\Delta t)}{\Delta t \, S(t)}$$ and $$\lambda(t)=\lim_{\Delta t\rightarrow 0}\frac{P(t\leq T<t+\Delta t\mid T\leq t)}{\Delta t}$$ I tried to do some manipulation with that, but I can't get nowhere.","Let $T$ a continuous non-negative random variable that represents a failure time.   Taking $S(t)=P(T\geq t)$ and $f(t)$ the probability density function   of $T$. Show that   $$\lambda(t)=\frac{f(t)}{S(t)}=-\frac{d}{dt} \log S(t)$$ where   $\lambda(t)$ represents a failure rate. I had some additional information $$\lambda(t)=\frac{P(T\geq t)-P(T\geq t+\Delta t)}{\Delta t \, S(t)}$$ and $$\lambda(t)=\lim_{\Delta t\rightarrow 0}\frac{P(t\leq T<t+\Delta t\mid T\leq t)}{\Delta t}$$ I tried to do some manipulation with that, but I can't get nowhere.",,"['probability', 'statistics', 'proof-writing', 'self-learning']"
91,Finding the Total number of permutation using a selective formula,Finding the Total number of permutation using a selective formula,,"I am a python programmer,trying to upgrade a python program I created. I am aware the total number of permutations can be calculated using n^r where n is the total number of things to choose from and r is how many we choose.. for example consider the string ""abc"" where need to generate three letter ""words""..the result  below aaa aab aac aba abb abc aca acb acc baa bab bac bba bbb bbc bca bcb bcc caa cab cac cba cbb cbc cca ccb ccc Question What I am trying to accomplish is to derive a formula for the total number of permutations where each permutation has maximum nth characters of the same identity..consider this example for example the total number of permutations that has only two letters of the same type repeating.... This is my results for generating all permutations from my program with two letters of the same type reaming aab - only two 'a' aac - only two 'a' aba - onyl two 'a' abb - only two 'b' abc - maximum is two letters repeating..also good aca - maximum is two letters repeating..also good acb - maximum is two letters repeating..also good acc - maximum is two letters repeating..also good baa- .maximum is two letters repeating..also good bab .................. bac.................... bba bbc bca bcb bcc caa cab cac cba cbb cbc cca ccb TOTAL NUMBER OF PERMUTATION IS 24 I can't seem to derive a way to do this BEFORE outputing the results.. In a nutshell I want to find A FORMULA for the number of permutation that only has nth letters in each permutation repeating not more than nth of the same type.. I want to precalculate the total number of permutations where each permutation has nth letter of the same type remaining and output this to the user of my program My program is bulky but here is the permutation codes I am using (note I am using itertools from python built in libraries) for perm in itertools.product(string,repeat=self.word_length):     if args:         args[0]()                         #print(""after"",self.counter)     self.counter=self.counter+1     result1=[perm.count(z) for z in perm]     result1.sort(reverse=True)     if result1[0]<=self.actual_repeat:         yield perm I am using itertools to generate the permutations , count each letter and find their fequency and filter for two letter words...the variable self.actual repeat is 2","I am a python programmer,trying to upgrade a python program I created. I am aware the total number of permutations can be calculated using n^r where n is the total number of things to choose from and r is how many we choose.. for example consider the string ""abc"" where need to generate three letter ""words""..the result  below aaa aab aac aba abb abc aca acb acc baa bab bac bba bbb bbc bca bcb bcc caa cab cac cba cbb cbc cca ccb ccc Question What I am trying to accomplish is to derive a formula for the total number of permutations where each permutation has maximum nth characters of the same identity..consider this example for example the total number of permutations that has only two letters of the same type repeating.... This is my results for generating all permutations from my program with two letters of the same type reaming aab - only two 'a' aac - only two 'a' aba - onyl two 'a' abb - only two 'b' abc - maximum is two letters repeating..also good aca - maximum is two letters repeating..also good acb - maximum is two letters repeating..also good acc - maximum is two letters repeating..also good baa- .maximum is two letters repeating..also good bab .................. bac.................... bba bbc bca bcb bcc caa cab cac cba cbb cbc cca ccb TOTAL NUMBER OF PERMUTATION IS 24 I can't seem to derive a way to do this BEFORE outputing the results.. In a nutshell I want to find A FORMULA for the number of permutation that only has nth letters in each permutation repeating not more than nth of the same type.. I want to precalculate the total number of permutations where each permutation has nth letter of the same type remaining and output this to the user of my program My program is bulky but here is the permutation codes I am using (note I am using itertools from python built in libraries) for perm in itertools.product(string,repeat=self.word_length):     if args:         args[0]()                         #print(""after"",self.counter)     self.counter=self.counter+1     result1=[perm.count(z) for z in perm]     result1.sort(reverse=True)     if result1[0]<=self.actual_repeat:         yield perm I am using itertools to generate the permutations , count each letter and find their fequency and filter for two letter words...the variable self.actual repeat is 2",,"['probability', 'statistics', 'permutations', 'factorial']"
92,The exact meaning of 10!/(3!7!) [duplicate],The exact meaning of 10!/(3!7!) [duplicate],,"This question already has an answer here : Counting the exact number of coin tosses (1 answer) Closed 7 years ago . Assume we toss a coin 10 times, independent of each other.  Each time we can get Heads (H) or Tails (T) , regardless of whether it is fair or not. So for example this is one possible outcome: HHHHHHHHHH ie 10 heads. Let's call this a 10-toss sequence. The total number of possible 10-toss sequences is 2^10 because each toss has 2 possible outcomes: Heads or Tails. --The first question is What is the meaning of 10!/(3!7!) ? The answer is that It is the total number of ways, by which we can place the three heads inside the 10-toss sequence. The order by which we place the three heads does not matter. --The second question is : What is the total number of possible 3-head sequences? Is it 10!/(3!7!)? Some say YES. Others, say NO. A 10-toss sequence with three heads, can be this one: HHH THTHTHT. So three positions are fixed to 'heads'. The remaining seven positions can be H or T. So we have 2^7 possible 7-toss sequences. And the three heads can be anywhere in this 10-toss sequence. So an answer can be that the total number of 10-toss sequences with 3 heads is 10!/(3!7!) multiplied by (2^7) . That is in the 10-toss sequence the number of ways by which we can place 3 heads is 10!/(3!7!). Then, we have to say something about the remaining 7-toss sequence. Each position can take Heads or Tails . So 2^7 is the possible number of this 7-toss sequence. However, I know that the correct answer is 10!/(3!7!) and that we don't need to multiply by 2^7. But I cannot understand why.","This question already has an answer here : Counting the exact number of coin tosses (1 answer) Closed 7 years ago . Assume we toss a coin 10 times, independent of each other.  Each time we can get Heads (H) or Tails (T) , regardless of whether it is fair or not. So for example this is one possible outcome: HHHHHHHHHH ie 10 heads. Let's call this a 10-toss sequence. The total number of possible 10-toss sequences is 2^10 because each toss has 2 possible outcomes: Heads or Tails. --The first question is What is the meaning of 10!/(3!7!) ? The answer is that It is the total number of ways, by which we can place the three heads inside the 10-toss sequence. The order by which we place the three heads does not matter. --The second question is : What is the total number of possible 3-head sequences? Is it 10!/(3!7!)? Some say YES. Others, say NO. A 10-toss sequence with three heads, can be this one: HHH THTHTHT. So three positions are fixed to 'heads'. The remaining seven positions can be H or T. So we have 2^7 possible 7-toss sequences. And the three heads can be anywhere in this 10-toss sequence. So an answer can be that the total number of 10-toss sequences with 3 heads is 10!/(3!7!) multiplied by (2^7) . That is in the 10-toss sequence the number of ways by which we can place 3 heads is 10!/(3!7!). Then, we have to say something about the remaining 7-toss sequence. Each position can take Heads or Tails . So 2^7 is the possible number of this 7-toss sequence. However, I know that the correct answer is 10!/(3!7!) and that we don't need to multiply by 2^7. But I cannot understand why.",,"['probability', 'probability-theory', 'statistics', 'probability-distributions']"
93,Moments of uncorrelated random variables,Moments of uncorrelated random variables,,"Let $X$, $Y$ and $Z$ be three pairwise uncorrelated random variables with $E\left(X\right)=E\left(Y\right)=E\left(Z\right)=0$, $Var\left(X\right)=Var\left(Y\right)=Var\left(Z\right)=1.$ Is it true that $E\left[XY^{2}\right]=0$ and $E\left[XZY^{2}\right]=0?$ I don't know where to start.What kind of results do I need to show this? What are the minimal assumptions we need in order to make the statements true? ? Thank you","Let $X$, $Y$ and $Z$ be three pairwise uncorrelated random variables with $E\left(X\right)=E\left(Y\right)=E\left(Z\right)=0$, $Var\left(X\right)=Var\left(Y\right)=Var\left(Z\right)=1.$ Is it true that $E\left[XY^{2}\right]=0$ and $E\left[XZY^{2}\right]=0?$ I don't know where to start.What kind of results do I need to show this? What are the minimal assumptions we need in order to make the statements true? ? Thank you",,"['probability', 'probability-theory', 'statistics']"
94,Random multivariate in hyperannulus,Random multivariate in hyperannulus,,"Given the hyperannulus, an annulus generalized to arbitrary dimensions with outer radius $r_1$ and inner radius $r_0$. This object would be a ring in $\mathbb{R}^2$ and a spherical shell in $\mathbb{R}^3$. What is the solution of picking uniformly distributed random multivariate points in this hyperannulus without rejection sampling?","Given the hyperannulus, an annulus generalized to arbitrary dimensions with outer radius $r_1$ and inner radius $r_0$. This object would be a ring in $\mathbb{R}^2$ and a spherical shell in $\mathbb{R}^3$. What is the solution of picking uniformly distributed random multivariate points in this hyperannulus without rejection sampling?",,"['geometry', 'probability-theory', 'statistics', 'probability-distributions', 'random-variables']"
95,Life after linear algebra and multivariate calculus,Life after linear algebra and multivariate calculus,,"I have been following Strang's Linear Algebra course, and found it quite challenging. My goal, however, is to learn application of linear algebra and calculus in applied statistics (regression, linear mixed models, structural equation modeling, et cetera). I want to understand the math behind these techniques. Some people suggest learning abstract algebra after linear algebra. However, after trying one to two lectures by Benedict Gross (on youtube), I find that it is totally not for me - I was lost very soon about what he talked about. In addition, I am unsure if abstract algebra is very useful to applied statistics. Therefore, I am unsure what I should learn after linear algebra and calculus, and I preferably want to learn something with online videos (as self-learning stats can be quite difficult).","I have been following Strang's Linear Algebra course, and found it quite challenging. My goal, however, is to learn application of linear algebra and calculus in applied statistics (regression, linear mixed models, structural equation modeling, et cetera). I want to understand the math behind these techniques. Some people suggest learning abstract algebra after linear algebra. However, after trying one to two lectures by Benedict Gross (on youtube), I find that it is totally not for me - I was lost very soon about what he talked about. In addition, I am unsure if abstract algebra is very useful to applied statistics. Therefore, I am unsure what I should learn after linear algebra and calculus, and I preferably want to learn something with online videos (as self-learning stats can be quite difficult).",,"['calculus', 'linear-algebra', 'statistics']"
96,Paired t-test: More distance results in lower p-value?,Paired t-test: More distance results in lower p-value?,,"In my test I alter one parameter in three stages (p1 After plotting it, it is quite obvious that the parameter positively effects the outcome, because all 3 lines are above each other (no intersection). However, when I do the 3 paired t-test (p1 vs p2, p1 vs. p3, p2 vs. p3), the p-value of the test between the lower and the middle line is smaller than between the lower and the upper line. Logically it should not be the case, since the gap is larger for the extreme cases, right? I was wondering whether my t-test is wrong or whether the fact that the curves are not parallel and the upper line is fluctuating a bit more (but still never intersects even the middle line!) overrules the larger gap and therefore explains the worse p-value. (all p-values are very low, the bigger gap between the extremes is 1,70*E-23, the smaller gap between low and middle is 2,93*E-25) Looking forward to hearing your expertise on the subject. :)","In my test I alter one parameter in three stages (p1 After plotting it, it is quite obvious that the parameter positively effects the outcome, because all 3 lines are above each other (no intersection). However, when I do the 3 paired t-test (p1 vs p2, p1 vs. p3, p2 vs. p3), the p-value of the test between the lower and the middle line is smaller than between the lower and the upper line. Logically it should not be the case, since the gap is larger for the extreme cases, right? I was wondering whether my t-test is wrong or whether the fact that the curves are not parallel and the upper line is fluctuating a bit more (but still never intersects even the middle line!) overrules the larger gap and therefore explains the worse p-value. (all p-values are very low, the bigger gap between the extremes is 1,70*E-23, the smaller gap between low and middle is 2,93*E-25) Looking forward to hearing your expertise on the subject. :)",,['statistics']
97,Good free online PDFs for probability and statistics (with practice problems and answers)?,Good free online PDFs for probability and statistics (with practice problems and answers)?,,I'd like to rebuild from the ground up and really make sure I have a strong grasp on probability and statistics. I am able to compute answers in a pinch but I'd like a more formal understanding so I can really ensure I am being systematic about it. For example I have no idea what a probability mass/density function is. Are there any good resources that go over all the basics and fundamentals while also providing answers to check my work against?,I'd like to rebuild from the ground up and really make sure I have a strong grasp on probability and statistics. I am able to compute answers in a pinch but I'd like a more formal understanding so I can really ensure I am being systematic about it. For example I have no idea what a probability mass/density function is. Are there any good resources that go over all the basics and fundamentals while also providing answers to check my work against?,,"['probability', 'statistics', 'online-resources']"
98,Probability distribution question with drawing letters from a box,Probability distribution question with drawing letters from a box,,"Draw letters with replacement from the following box: [S  T  A  T I  S  T  I  C  S] Let $Y$ be the distribution of the number of times the letter 'A' is drawn until you get the letter 'S' 5 times. Find $P(Y = y)$ , $E[Y]$ and $Var[Y]$ . I'm having trouble finding $P(Y = y)$ . My initial thought was that I could use the Negative Binomial distribution, but I'm not looking for a number of trials until the rth success.","Draw letters with replacement from the following box: [S  T  A  T I  S  T  I  C  S] Let be the distribution of the number of times the letter 'A' is drawn until you get the letter 'S' 5 times. Find , and . I'm having trouble finding . My initial thought was that I could use the Negative Binomial distribution, but I'm not looking for a number of trials until the rth success.",Y P(Y = y) E[Y] Var[Y] P(Y = y),"['probability', 'statistics', 'probability-distributions']"
99,small populations representativeness,small populations representativeness,,"A question relating to home purchase loan denial rates, which I am examining as part of a work project. It's also a question that has come up in other aspects of my work. It is essentially about how much weight to put into figures drawn from small populations (not samples.) I know that when you are dealing with samples of a population you can talk about measures of variance (standard deviation, etc.) and margins of error. I have also seen, in researching this question, the coefficient of variance used in ratio data. But those only apply, as I understand it, when you are using a sample to estimate a population parameter. What I want to talk about is something like this: Suppose that I were the second person to apply to purchase a home in my town, and that the previous applicant had been successful. I could look at his success and say, ""Well, so far there has been a 100 percent success rate, so my chances are good!"", but that would probably be a little naïve. Is there a specific quantitative measure of how much weight you should put into small population figures for ""probabilistic"" purposes like this?","A question relating to home purchase loan denial rates, which I am examining as part of a work project. It's also a question that has come up in other aspects of my work. It is essentially about how much weight to put into figures drawn from small populations (not samples.) I know that when you are dealing with samples of a population you can talk about measures of variance (standard deviation, etc.) and margins of error. I have also seen, in researching this question, the coefficient of variance used in ratio data. But those only apply, as I understand it, when you are using a sample to estimate a population parameter. What I want to talk about is something like this: Suppose that I were the second person to apply to purchase a home in my town, and that the previous applicant had been successful. I could look at his success and say, ""Well, so far there has been a 100 percent success rate, so my chances are good!"", but that would probably be a little naïve. Is there a specific quantitative measure of how much weight you should put into small population figures for ""probabilistic"" purposes like this?",,"['probability', 'statistics']"
