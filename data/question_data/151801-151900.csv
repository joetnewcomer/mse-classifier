,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"A ""développement limité"" for function of several variables at the order $2$","A ""développement limité"" for function of several variables at the order",2,"Starting from the Taylor theorem of order 2 for functions of several variables (a proof is tried here A proof of multivariable Taylor theorem of order 2 ) I would like to find what we call in french ""un developpement limité d'ordre 2"", I state this now : Consider a function $f:A\subset\mathbb{R}^n\to\mathbb{R}$ that is $C^{2}$ on $N(X_0, r)\in A$ . Then we have $f(X_0 + Y) = f(X_0) + Df(X_0)Y + \frac{1}{2}Y^{t}D^{2}f(X_0)Y + \epsilon(X_0, Y)\lVert Y\rVert^{2}$ with $\lim_{Y\to 0_{n}}\epsilon(X_0, Y) = 0$ . Thus we can put rewrite this as $f(X_0 + Y) = f(X_0) + Df(X_0)Y + \frac{1}{2}Y^{t}D^{2}f(X_0)Y + o\left(\lVert Y\rVert^{2}\right)$ for $Y$ in a neighborhood of $0_n$ . The idea of the proof is the following : If $Y = 0_{n}$ we consider $\epsilon_{0}(X_0,Y) = 0$ which proves the result above. Otherwise (i.e. $\lVert Y\rVert\leq r$ ), consider $\epsilon(X_0,Y) = \frac{1}{\lVert Y\rVert^{2}}\left(f(X_0+Y) - f(X_0) - Df(X_0)Y - \frac{1}{2}Y^{t}D^{2}f(X_0)Y\right)$ The proof ends if we show that $\lim_{Y\to 0_{n}}\epsilon(X_0, Y) = 0$ . Since $f$ is $C^{2}$ in the neighborhood of $X_0$ we can use Taylor theorem of order $2$ on $f(X_0 + Y)$ to get $\epsilon(X_0,Y) = \frac{1}{\lVert Y\rVert^{2}}\left(f(X_0) + Df(X_0)Y + \frac{1}{2}Y^{t}D^{2}f(X_0+\theta Y)Y - f(X_0) - Df(X_0)Y - \frac{1}{2}Y^{t}D^{2}f(X_0)Y\right)$ $=\frac{1}{2\lVert Y\rVert^{2}}\left(Y^{t}D^{2}f(X_0+\theta Y)Y - Y^{t}D^{2}f(X_0)Y\right) = \frac{1}{2\lVert Y\rVert^{2}}Y^{t}\left(D^{2}f(X_0+\theta Y) - D^{2}f(X_0)\right)Y $ $ =\sum_{i=1}^{n}\sum_{j=1}^{n}\frac{y_i y_j}{2\lVert Y\rVert^{2}}\left(f_{x_i x_j}^{''}(X_0 + \theta Y) - f_{x_i x_j}^{''}(X_0)\right)$ It follows that (by the triangle inequality) $\lvert\epsilon(X_0, Y)\rvert\leq\sum_{i=1}^{n}\sum_{j=1}^{n}\left\rvert f_{x_i x_j}^{''}(X_0 + \theta Y) - f_{x_i x_j}^{''}(X_0)\right\rvert$ which shows that $\lim_{Y\to 0_{n}}\epsilon(X_0, Y) = 0$ using the continuity of the second order partial derivatives at $X_0$ This seems correct to you ? And do you an elegant way to extend this at the order $p$ ? I thought of an induction, thank you a lot !","Starting from the Taylor theorem of order 2 for functions of several variables (a proof is tried here A proof of multivariable Taylor theorem of order 2 ) I would like to find what we call in french ""un developpement limité d'ordre 2"", I state this now : Consider a function that is on . Then we have with . Thus we can put rewrite this as for in a neighborhood of . The idea of the proof is the following : If we consider which proves the result above. Otherwise (i.e. ), consider The proof ends if we show that . Since is in the neighborhood of we can use Taylor theorem of order on to get It follows that (by the triangle inequality) which shows that using the continuity of the second order partial derivatives at This seems correct to you ? And do you an elegant way to extend this at the order ? I thought of an induction, thank you a lot !","f:A\subset\mathbb{R}^n\to\mathbb{R} C^{2} N(X_0, r)\in A f(X_0 + Y) = f(X_0) + Df(X_0)Y + \frac{1}{2}Y^{t}D^{2}f(X_0)Y + \epsilon(X_0, Y)\lVert Y\rVert^{2} \lim_{Y\to 0_{n}}\epsilon(X_0, Y) = 0 f(X_0 + Y) = f(X_0) + Df(X_0)Y + \frac{1}{2}Y^{t}D^{2}f(X_0)Y + o\left(\lVert Y\rVert^{2}\right) Y 0_n Y = 0_{n} \epsilon_{0}(X_0,Y) = 0 \lVert Y\rVert\leq r \epsilon(X_0,Y) = \frac{1}{\lVert Y\rVert^{2}}\left(f(X_0+Y) - f(X_0) - Df(X_0)Y - \frac{1}{2}Y^{t}D^{2}f(X_0)Y\right) \lim_{Y\to 0_{n}}\epsilon(X_0, Y) = 0 f C^{2} X_0 2 f(X_0 + Y) \epsilon(X_0,Y) = \frac{1}{\lVert Y\rVert^{2}}\left(f(X_0) + Df(X_0)Y + \frac{1}{2}Y^{t}D^{2}f(X_0+\theta Y)Y - f(X_0) - Df(X_0)Y - \frac{1}{2}Y^{t}D^{2}f(X_0)Y\right) =\frac{1}{2\lVert Y\rVert^{2}}\left(Y^{t}D^{2}f(X_0+\theta Y)Y - Y^{t}D^{2}f(X_0)Y\right) = \frac{1}{2\lVert Y\rVert^{2}}Y^{t}\left(D^{2}f(X_0+\theta Y) - D^{2}f(X_0)\right)Y   =\sum_{i=1}^{n}\sum_{j=1}^{n}\frac{y_i y_j}{2\lVert Y\rVert^{2}}\left(f_{x_i x_j}^{''}(X_0 + \theta Y) - f_{x_i x_j}^{''}(X_0)\right) \lvert\epsilon(X_0, Y)\rvert\leq\sum_{i=1}^{n}\sum_{j=1}^{n}\left\rvert f_{x_i x_j}^{''}(X_0 + \theta Y) - f_{x_i x_j}^{''}(X_0)\right\rvert \lim_{Y\to 0_{n}}\epsilon(X_0, Y) = 0 X_0 p","['analysis', 'multivariable-calculus', 'taylor-expansion', 'approximation']"
1,Issue with proof of theorem 2.1.11 in Audin-Damian,Issue with proof of theorem 2.1.11 in Audin-Damian,,"The theorem in question is as follows: Let $V$ be a closed smooth manifold and $f: V\to \mathbb{R}$ a Morse function. Let $a$ be a critical point of $f$ with index $k$ and $\alpha=f(a)$ . Suppose that for some $\epsilon>0$ , $f^{-1}([\alpha-\epsilon, \alpha+\epsilon])$ is compact and does not contain any critical points distinct from $a$ . For any sufficiently small $\epsilon>0$ , the homotopy type of $V^{\alpha+\epsilon}=f^{-1}([\alpha+\epsilon,-\infty))$ is that of $V^{\alpha-\epsilon}$ with a cell of dimension $k$ attached. In the beginning of the proof, we wish to construct a function $F$ which coincides with $f$ outside of a neighborhood of $a$ where $F<f$ , such that $F^{-1}((-\infty, \alpha-\epsilon))$ is the union of $V^{\alpha-\epsilon}$ and a neighborhood of $a$ . This is done by taking a Morse chart $(U,h)$ of a neighborhood of $a$ and an $\epsilon>0$ such that $f^{-1}([\alpha-\epsilon,\alpha+\epsilon])$ is compact and $U$ contains a ball of radius $\sqrt{2\epsilon}$ with center $0$ and using a smooth function $\mu:\mathbb{R}^{\geq 0}\to \mathbb{R}^{\geq 0}$ such that \begin{align} \mu(0)&>\epsilon\\ \mu(s)&=0 \text{ for } s\geq 2\epsilon\\ -1<\mu'(s)&\leq 0 \text{ for every } s \end{align} In the Morse chart $h: U\to V$ where $U\subset \mathbb{R}^{n}=\mathbb{R}^{k}\times \mathbb{R}^{n-k}$ open, $f$ takes the form $f(h(x_-, x_+))=\alpha-||x_-||^2+||x_+||^2$ Using this $\mu$ , we can construct a function $$F(x)=\begin{cases} &f(x) &\text{ if } x\notin h(U)\\ &\alpha-||x_-||^2+||x_+||^2\mu(||x_-||^2+2||x_+||^2) &\text{ if } x=h(x_-,x_+) \end{cases}$$ The authors claim that outside of the set $||x_-||^2+2||x_+||^2\leq 2\epsilon$ one has $F=f$ , but if $\mu(s)=0$ for $s\geq 2\epsilon$ , then for $||x_-||^2+2||x_+||^2>2\epsilon$ we have $\mu(||x_-||^2+2||x_+||^2)=0$ and $$F(x)=\alpha-||x_-||^2\neq \alpha-||x_-||^2+||x_+||^2=f(x)$$ Is there a detail that I'm missing, or is there a typo?","The theorem in question is as follows: Let be a closed smooth manifold and a Morse function. Let be a critical point of with index and . Suppose that for some , is compact and does not contain any critical points distinct from . For any sufficiently small , the homotopy type of is that of with a cell of dimension attached. In the beginning of the proof, we wish to construct a function which coincides with outside of a neighborhood of where , such that is the union of and a neighborhood of . This is done by taking a Morse chart of a neighborhood of and an such that is compact and contains a ball of radius with center and using a smooth function such that In the Morse chart where open, takes the form Using this , we can construct a function The authors claim that outside of the set one has , but if for , then for we have and Is there a detail that I'm missing, or is there a typo?","V f: V\to \mathbb{R} a f k \alpha=f(a) \epsilon>0 f^{-1}([\alpha-\epsilon, \alpha+\epsilon]) a \epsilon>0 V^{\alpha+\epsilon}=f^{-1}([\alpha+\epsilon,-\infty)) V^{\alpha-\epsilon} k F f a F<f F^{-1}((-\infty, \alpha-\epsilon)) V^{\alpha-\epsilon} a (U,h) a \epsilon>0 f^{-1}([\alpha-\epsilon,\alpha+\epsilon]) U \sqrt{2\epsilon} 0 \mu:\mathbb{R}^{\geq 0}\to \mathbb{R}^{\geq 0} \begin{align}
\mu(0)&>\epsilon\\
\mu(s)&=0 \text{ for } s\geq 2\epsilon\\
-1<\mu'(s)&\leq 0 \text{ for every } s
\end{align} h: U\to V U\subset \mathbb{R}^{n}=\mathbb{R}^{k}\times \mathbb{R}^{n-k} f f(h(x_-, x_+))=\alpha-||x_-||^2+||x_+||^2 \mu F(x)=\begin{cases} &f(x) &\text{ if } x\notin h(U)\\
&\alpha-||x_-||^2+||x_+||^2\mu(||x_-||^2+2||x_+||^2) &\text{ if } x=h(x_-,x_+)
\end{cases} ||x_-||^2+2||x_+||^2\leq 2\epsilon F=f \mu(s)=0 s\geq 2\epsilon ||x_-||^2+2||x_+||^2>2\epsilon \mu(||x_-||^2+2||x_+||^2)=0 F(x)=\alpha-||x_-||^2\neq \alpha-||x_-||^2+||x_+||^2=f(x)","['analysis', 'differential-geometry', 'smooth-manifolds', 'smooth-functions', 'morse-theory']"
2,Show generalized spherical coordinate are surjective.,Show generalized spherical coordinate are surjective.,,"Define $G: \mathbb{R}^n \rightarrow \mathbb{R}^n$ by $G\left(r, \phi_1, \ldots, \phi_{n-2}, \theta\right)=\left(x_1, \ldots, x_n\right)$ where $$ \begin{aligned} & x_1=r \cos \phi_1, \quad x_2=r \sin \phi_1 \cos \phi_2, \quad x_3=r \sin \phi_1 \sin \phi_2 \cos \phi_3, \ldots \\ & x_{n-1}=r \sin \phi_1 \cdots \sin \phi_{n-2} \cos \phi_1, \quad x_n=r \sin \phi_1 \cdots \sin \phi_{n-2} \sin \theta \end{aligned} $$ Show $G$ maps $\mathbb{R}^n$ onto $\mathbb{R}^n$ . Here is my attempt. We will show by induction, that the map is surjective. Base Case when n=2, fix $(x,y)\in \mathbb{R}^2$ . Then choose $r=\sqrt{x^2+y^2}$ . Now, we must find a $\theta$ such that $(x,y)=(r\cos(\theta),r\sin(\theta))$ . Thus, $$\frac{y}{x}=\tan(\theta)$$ . Using the $\operatorname{atan2}$ function, we can define the inverse over any point on the plane excluding the origin. We have $$\operatorname{atan}2(y, x)= \begin{cases}\arctan \left(\frac{y}{x}\right) & \text { if } x>0, \\ \arctan \left(\frac{y}{x}\right)+\pi & \text { if } x<0 \text { and } y \geq 0, \\ \arctan \left(\frac{y}{x}\right)-\pi & \text { if } x<0 \text { and } y<0, \\ +\frac{\pi}{2} & \text { if } x=0 \text { and } y>0, \\ -\frac{\pi}{2} & \text { if } x=0 \text { and } y<0, \\ \text { undefined } & \text { if } x=0 \text { and } y=0 .\end{cases}$$ Thus, choosing $\theta=\operatorname{atan}2(y, x)$ , we are done. Induction Step Fix $(x_1,\dots,x_n,x_{n+1})\in \mathbb{R}^n$ . We would like to appeal to the induction step. Choose $r,\varphi_1,\dots,\varphi_{n-2},\theta$ by the induction hypothesis. How can I then select the proper final angle to complete the induction step? How can I choose this theta to complete the induction step? Finally, on wikipeida, the following function was obtained for the inverse but I am not sure how they came up with this. We may define a coordinate system in an $n$ -dimensional Euclidean space which is analogous to the spherical coordinate system defined for 3 -dimensional Euclidean space, in which the coordinates consist of a radial coordinate $r$ , and $n-1$ angular coordinates $\varphi_1, \varphi_2, \ldots \varphi_{n-1}$ , where the angles $\varphi_1, \varphi_2, \ldots \varphi_{n-2}$ range over $[0, \pi]$ radians (or over $[0,180]$ degrees) and $\varphi_{n-1}$ ranges over $\left[0,2 \pi\right.$ ) radians (or over $[0,360)$ degrees). If $x_i$ are the Cartesian coordinates, then we may compute $x_1, \ldots x_n$ from $r, \varphi_1, \ldots \varphi_{n-1}$ with: $[4]$ $$ \begin{aligned} x_1 & =r \cos \left(\varphi_1\right) \\ x_2 & =r \sin \left(\varphi_1\right) \cos \left(\varphi_2\right) \\ x_3 & =r \sin \left(\varphi_1\right) \sin \left(\varphi_2\right) \cos \left(\varphi_3\right) \\ & \vdots \\ x_{n-1} & =r \sin \left(\varphi_1\right) \cdots \sin \left(\varphi_{n-2}\right) \cos \left(\varphi_{n-1}\right) \\ x_n & =r \sin \left(\varphi_1\right) \cdots \sin \left(\varphi_{n-2}\right) \sin \left(\varphi_{n-1}\right) . \end{aligned} $$ Except in the special cases described below, the inverse transformation is unique: $$ \begin{aligned} & r=\sqrt{{x_n}^2+x_{n-1}{ }^2+\cdots+x_2{ }^2+x_1{ }^2} \\ & \varphi_1=\operatorname{arccot} \frac{x_1}{\sqrt{x_n{ }^2+x_{n-1}{ }^2+\cdots+x_2^2}} \quad=\arccos \frac{x_1}{\sqrt{x_n{ }^2+x_{n-1}{ }^2+\cdots+x_1^2}} \\ & \varphi_2=\operatorname{arccot} \frac{x_2}{\sqrt{x_n{ }^2+x_{n-1}{ }^2+\cdots+x_3^2}} \quad=\arccos \frac{x_2}{\sqrt{x_n{ }^2+x_{n-1}{ }^2+\cdots+x_2{ }^2}} \\ & \varphi_{n-2}=\operatorname{arccot} \frac{x_{n-2}}{\sqrt{x_n{ }^2+x_{n-1}{ }^2}} \quad \quad=\arccos \frac{x_{n-2}}{\sqrt{x_n{ }^2+x_{n-1}{ }^2+x_{n-2}{ }^2}} \\ & \varphi_{n-1}=2 \operatorname{arccot} \frac{x_{n-1}+\sqrt{x_n^2+x_{n-1}^2}}{x_n} \quad= \begin{cases}\arccos \frac{x_{n-1}}{\sqrt{x_n^2+x_{n-1}{ }^2}} & x_n \geq 0, \\ 2 \pi-\arccos \frac{x_{n-1}}{\sqrt{x_n^2+x_{n-1}{ }^2}} & x_n<0 .\end{cases} \\ & \end{aligned} $$ where if $x_k \neq 0$ for some $k$ but all of $x_{k+1}, \ldots x_n$ are zero then $\varphi_k=0$ when $x_k>0$ , and $\varphi_k=\pi$ (180 degrees) when $x_k<0$ . Was this contstruction also obtained inductively? and why do they not use tan but rather cot?","Define by where Show maps onto . Here is my attempt. We will show by induction, that the map is surjective. Base Case when n=2, fix . Then choose . Now, we must find a such that . Thus, . Using the function, we can define the inverse over any point on the plane excluding the origin. We have Thus, choosing , we are done. Induction Step Fix . We would like to appeal to the induction step. Choose by the induction hypothesis. How can I then select the proper final angle to complete the induction step? How can I choose this theta to complete the induction step? Finally, on wikipeida, the following function was obtained for the inverse but I am not sure how they came up with this. We may define a coordinate system in an -dimensional Euclidean space which is analogous to the spherical coordinate system defined for 3 -dimensional Euclidean space, in which the coordinates consist of a radial coordinate , and angular coordinates , where the angles range over radians (or over degrees) and ranges over ) radians (or over degrees). If are the Cartesian coordinates, then we may compute from with: Except in the special cases described below, the inverse transformation is unique: where if for some but all of are zero then when , and (180 degrees) when . Was this contstruction also obtained inductively? and why do they not use tan but rather cot?","G: \mathbb{R}^n \rightarrow \mathbb{R}^n G\left(r, \phi_1, \ldots, \phi_{n-2}, \theta\right)=\left(x_1, \ldots, x_n\right) 
\begin{aligned}
& x_1=r \cos \phi_1, \quad x_2=r \sin \phi_1 \cos \phi_2, \quad x_3=r \sin \phi_1 \sin \phi_2 \cos \phi_3, \ldots \\
& x_{n-1}=r \sin \phi_1 \cdots \sin \phi_{n-2} \cos \phi_1, \quad x_n=r \sin \phi_1 \cdots \sin \phi_{n-2} \sin \theta
\end{aligned}
 G \mathbb{R}^n \mathbb{R}^n (x,y)\in \mathbb{R}^2 r=\sqrt{x^2+y^2} \theta (x,y)=(r\cos(\theta),r\sin(\theta)) \frac{y}{x}=\tan(\theta) \operatorname{atan2} \operatorname{atan}2(y, x)= \begin{cases}\arctan \left(\frac{y}{x}\right) & \text { if } x>0, \\ \arctan \left(\frac{y}{x}\right)+\pi & \text { if } x<0 \text { and } y \geq 0, \\ \arctan \left(\frac{y}{x}\right)-\pi & \text { if } x<0 \text { and } y<0, \\ +\frac{\pi}{2} & \text { if } x=0 \text { and } y>0, \\ -\frac{\pi}{2} & \text { if } x=0 \text { and } y<0, \\ \text { undefined } & \text { if } x=0 \text { and } y=0 .\end{cases} \theta=\operatorname{atan}2(y, x) (x_1,\dots,x_n,x_{n+1})\in \mathbb{R}^n r,\varphi_1,\dots,\varphi_{n-2},\theta n r n-1 \varphi_1, \varphi_2, \ldots \varphi_{n-1} \varphi_1, \varphi_2, \ldots \varphi_{n-2} [0, \pi] [0,180] \varphi_{n-1} \left[0,2 \pi\right. [0,360) x_i x_1, \ldots x_n r, \varphi_1, \ldots \varphi_{n-1} [4] 
\begin{aligned}
x_1 & =r \cos \left(\varphi_1\right) \\
x_2 & =r \sin \left(\varphi_1\right) \cos \left(\varphi_2\right) \\
x_3 & =r \sin \left(\varphi_1\right) \sin \left(\varphi_2\right) \cos \left(\varphi_3\right) \\
& \vdots \\
x_{n-1} & =r \sin \left(\varphi_1\right) \cdots \sin \left(\varphi_{n-2}\right) \cos \left(\varphi_{n-1}\right) \\
x_n & =r \sin \left(\varphi_1\right) \cdots \sin \left(\varphi_{n-2}\right) \sin \left(\varphi_{n-1}\right) .
\end{aligned}
 
\begin{aligned}
& r=\sqrt{{x_n}^2+x_{n-1}{ }^2+\cdots+x_2{ }^2+x_1{ }^2} \\
& \varphi_1=\operatorname{arccot} \frac{x_1}{\sqrt{x_n{ }^2+x_{n-1}{ }^2+\cdots+x_2^2}} \quad=\arccos \frac{x_1}{\sqrt{x_n{ }^2+x_{n-1}{ }^2+\cdots+x_1^2}} \\
& \varphi_2=\operatorname{arccot} \frac{x_2}{\sqrt{x_n{ }^2+x_{n-1}{ }^2+\cdots+x_3^2}} \quad=\arccos \frac{x_2}{\sqrt{x_n{ }^2+x_{n-1}{ }^2+\cdots+x_2{ }^2}} \\
& \varphi_{n-2}=\operatorname{arccot} \frac{x_{n-2}}{\sqrt{x_n{ }^2+x_{n-1}{ }^2}} \quad \quad=\arccos \frac{x_{n-2}}{\sqrt{x_n{ }^2+x_{n-1}{ }^2+x_{n-2}{ }^2}} \\
& \varphi_{n-1}=2 \operatorname{arccot} \frac{x_{n-1}+\sqrt{x_n^2+x_{n-1}^2}}{x_n} \quad= \begin{cases}\arccos \frac{x_{n-1}}{\sqrt{x_n^2+x_{n-1}{ }^2}} & x_n \geq 0, \\
2 \pi-\arccos \frac{x_{n-1}}{\sqrt{x_n^2+x_{n-1}{ }^2}} & x_n<0 .\end{cases} \\
&
\end{aligned}
 x_k \neq 0 k x_{k+1}, \ldots x_n \varphi_k=0 x_k>0 \varphi_k=\pi x_k<0","['calculus', 'analysis', 'measure-theory', 'physics']"
3,Continuity of Convolution on $C_c( G)$,Continuity of Convolution on,C_c( G),"I’m reading through an introductory operator algebra paper currently, and I’m a little lost on how one proves continuity of the convolution on $C_c(G)$ for any locally compact topological group $G$ . The paper says “For $f,g \in C_c(G)$ and $t \in G$ , define: $$ (f \ast g)(t) = \int_{G} f(s)g(s^{-1}t) d\mu(s).$$ Then $f \ast g$ is continuous by the Dominated convergence theorem.” I follow this reasoning for $G$ first countable, since then for $(t_k)_{k=1}^{\infty}$ converging to $t$ , we can define $h_k(s) = f(s)g(s^{-1}t_k)$ and apply the DCT (maybe I’m not doing this correctly?).  But for $G$ not first countable, I don’t see how you could apply DCT, since convergent nets of measurable functions need not even be measurable. I’d be very appreciative if someone could point out what I’m missing (I feel like this may need a uniform convergence argument).  Thank you.","I’m reading through an introductory operator algebra paper currently, and I’m a little lost on how one proves continuity of the convolution on for any locally compact topological group . The paper says “For and , define: Then is continuous by the Dominated convergence theorem.” I follow this reasoning for first countable, since then for converging to , we can define and apply the DCT (maybe I’m not doing this correctly?).  But for not first countable, I don’t see how you could apply DCT, since convergent nets of measurable functions need not even be measurable. I’d be very appreciative if someone could point out what I’m missing (I feel like this may need a uniform convergence argument).  Thank you.","C_c(G) G f,g \in C_c(G) t \in G  (f \ast g)(t) = \int_{G} f(s)g(s^{-1}t) d\mu(s). f \ast g G (t_k)_{k=1}^{\infty} t h_k(s) = f(s)g(s^{-1}t_k) G","['functional-analysis', 'analysis', 'operator-theory', 'operator-algebras', 'convolution']"
4,Who is the father of modern PDEs,Who is the father of modern PDEs,,"Classical PDE theory fails especially when the PDE is nonlinear, i.e. smooth initial data does not imply differentiable solutions. As a result, we often use the notion of weak solution, which has its roots in the theory of distributions in functional analysis. Who connected these two subjects which seem entirely different at least in the first glance. In other words, who introduced the so called weak formulation/ notion of distributional solutions to PDEs. Which is the first paper in this direction?","Classical PDE theory fails especially when the PDE is nonlinear, i.e. smooth initial data does not imply differentiable solutions. As a result, we often use the notion of weak solution, which has its roots in the theory of distributions in functional analysis. Who connected these two subjects which seem entirely different at least in the first glance. In other words, who introduced the so called weak formulation/ notion of distributional solutions to PDEs. Which is the first paper in this direction?",,"['functional-analysis', 'analysis', 'partial-differential-equations']"
5,Weak formulation for heat and wave equation,Weak formulation for heat and wave equation,,"Consider the IVP for heat equation given by $$ \begin{cases} u_t-\Delta u =0 & (x,t) \in \mathbb{R}^d \times (0,\infty)\\ u(x,0)=u_0(x) &  x\in \mathbb{R}^d \end{cases} $$ We can define the weak formulations in two ways: The standard weak formulation of the above IVP (see for example the book Partial Differential Equations by L.C. Evans) is to look for $u\in L^2(\mathbb{R}^+;H^1(\mathbb{R}^d))$ with $u'\in L^2(\mathbb{R}^+,H^{-1}(\mathbb{R}^d))$ satisfying \begin{eqnarray} \langle u',v\rangle - \int\limits_{\mathbb{R}^d}  \sum\limits_{i=1}^du_{x_i}v_{x_i} dx = 0. \qquad \text{ for all }v\in H^1_0(\mathbb{R}^d) \label{1}\tag{1}  \end{eqnarray} On the other hand, one can also define the weak formulation as $u\in C([0,\infty);L^2(\mathbb{R}^d))$ satisfying \begin{eqnarray} \int\limits_{\mathbb{R}^d\times \mathbb{R}^+}(u \phi_t +u \Delta \phi)dx dt=0 \qquad \text{for all } \phi \in C_c^{\infty}(\mathbb{R}^d \times \mathbb{R}^+) \label{2}\tag{2} \end{eqnarray} Why the formulation \eqref{1} is preferred over \eqref{2}? The same question is applicable for the IVPs for wave equations as well. P.S.: I understand that \eqref{2} cannot be used for initial boundary value problems (IBVP) as the trace of $L^2$ functions do not exist in general. However, for IVPs we do not ned the existence of such traces traces so \eqref{2} seems more natural. Moreover, since the PDE is Parabolic, anyways solutions can be shown to be sufficiently smooth.","Consider the IVP for heat equation given by We can define the weak formulations in two ways: The standard weak formulation of the above IVP (see for example the book Partial Differential Equations by L.C. Evans) is to look for with satisfying On the other hand, one can also define the weak formulation as satisfying Why the formulation \eqref{1} is preferred over \eqref{2}? The same question is applicable for the IVPs for wave equations as well. P.S.: I understand that \eqref{2} cannot be used for initial boundary value problems (IBVP) as the trace of functions do not exist in general. However, for IVPs we do not ned the existence of such traces traces so \eqref{2} seems more natural. Moreover, since the PDE is Parabolic, anyways solutions can be shown to be sufficiently smooth.","
\begin{cases}
u_t-\Delta u =0 & (x,t) \in \mathbb{R}^d \times (0,\infty)\\
u(x,0)=u_0(x) &  x\in \mathbb{R}^d
\end{cases}
 u\in L^2(\mathbb{R}^+;H^1(\mathbb{R}^d)) u'\in L^2(\mathbb{R}^+,H^{-1}(\mathbb{R}^d)) \begin{eqnarray}
\langle u',v\rangle - \int\limits_{\mathbb{R}^d}  \sum\limits_{i=1}^du_{x_i}v_{x_i} dx = 0. \qquad \text{ for all }v\in H^1_0(\mathbb{R}^d) \label{1}\tag{1} 
\end{eqnarray} u\in C([0,\infty);L^2(\mathbb{R}^d)) \begin{eqnarray}
\int\limits_{\mathbb{R}^d\times \mathbb{R}^+}(u \phi_t +u \Delta \phi)dx dt=0 \qquad \text{for all } \phi \in C_c^{\infty}(\mathbb{R}^d \times \mathbb{R}^+) \label{2}\tag{2}
\end{eqnarray} L^2","['functional-analysis', 'analysis', 'partial-differential-equations']"
6,"Theorem on BFS $X$ and space C of continuous functions in $[0,1]$ and cartesian square of $[0,1]$($[0,1]^2$).",Theorem on BFS  and space C of continuous functions in  and cartesian square of ().,"X [0,1] [0,1] [0,1]^2","This is the definition which we need for the proof of the theorem: There is the theorem Let $X$ be a BFS on $I$ . The space $C(I)$ of continuous functions on $I$ is a closed linear subspace of $X$ if and only if there exists a positive constant $c$ satisfying $||\chi_{(a,b)}||_X$ $\geq$ $c$ whenever $0$ $\leq$ $a$ $\lt$ $b$ $\leq$ $1$ . (Mark this condition by ( $\star$ )). There is the proof: For sufficiency part it is enough to show that there is a positive constant $C$ such that for every $f$ $\in$ $C(I)$ , $C$$||f||_{C(I)}$ $\leq$ $||f||_X$ $\leq$ $||f||_{C(I)}$ . (Mark this by ( $\oplus$ )). The second of these inequalities is clear. For the first, let $f$ $\in$ $C(I)$ . There exists $x_0$ $\in$ $I$ such that $||f||_{C(I)}$ $=$ $|f(x_0)|$ ; there exists $\epsilon$ $\gt$ $0$ such that $|f(x_0)|$ $\leq$ $2|f(x)| $ if $x$ $\in$ $(x_0 - \epsilon,x_0 + \epsilon)$ $\cap$ $I$ : $=$ $E$ . Thus from $(\star)$ we see that $||f||_{C(I)}$ $=$ $|f(x_0)|$ $\leq$ $\frac {1}{c}$$|f(x_0)|$$||\chi_E||_X$ $\leq$ $\frac {2}{c}$$||f\chi_E||_X$ $\leq$ $\frac {2}{c}$ $||f||_X$ . Necessity. If $C(I)$ is a closed subset of $X$ , then by the closed graph theorem(there it is https://en.m.wikipedia.org/wiki/Closed_graph_theorem_(functional_analysis) , we have the estimate $(\oplus)$ . Let given any interval $(a,b)$ $\subset$ $I$ be given, if we take a continuous function $g$ on $I$ such that $g$ $\leq$ $\chi_{(a,b)}$ and $||g||_L^{\infty}$ $=$ $1$ we get $(\star)$ . There is my question: This theorem is on $X$ BFS on $[0,1]$ and on $C([0,1])$ space of continuous functions. How could we proof that if this theorem is fair or false for $X$ BFS on $[0,1]^2$ ( Cartesian product) and on $C([0,1]^2)$ space of continuous functions. Any help would be appreciated.","This is the definition which we need for the proof of the theorem: There is the theorem Let be a BFS on . The space of continuous functions on is a closed linear subspace of if and only if there exists a positive constant satisfying whenever . (Mark this condition by ( )). There is the proof: For sufficiency part it is enough to show that there is a positive constant such that for every , . (Mark this by ( )). The second of these inequalities is clear. For the first, let . There exists such that ; there exists such that if : . Thus from we see that . Necessity. If is a closed subset of , then by the closed graph theorem(there it is https://en.m.wikipedia.org/wiki/Closed_graph_theorem_(functional_analysis) , we have the estimate . Let given any interval be given, if we take a continuous function on such that and we get . There is my question: This theorem is on BFS on and on space of continuous functions. How could we proof that if this theorem is fair or false for BFS on ( Cartesian product) and on space of continuous functions. Any help would be appreciated.","X I C(I) I X c ||\chi_{(a,b)}||_X \geq c 0 \leq a \lt b \leq 1 \star C f \in C(I) C||f||_{C(I)} \leq ||f||_X \leq ||f||_{C(I)} \oplus f \in C(I) x_0 \in I ||f||_{C(I)} = |f(x_0)| \epsilon \gt 0 |f(x_0)| \leq 2|f(x)|  x \in (x_0 - \epsilon,x_0 + \epsilon) \cap I = E (\star) ||f||_{C(I)} = |f(x_0)| \leq \frac {1}{c}|f(x_0)|||\chi_E||_X \leq \frac {2}{c}||f\chi_E||_X \leq \frac {2}{c} ||f||_X C(I) X (\oplus) (a,b) \subset I g I g \leq \chi_{(a,b)} ||g||_L^{\infty} = 1 (\star) X [0,1] C([0,1]) X [0,1]^2 C([0,1]^2)","['real-analysis', 'functional-analysis', 'analysis', 'measure-theory', 'normed-spaces']"
7,Functions whose distributional second derivative is finite,Functions whose distributional second derivative is finite,,"Let $f\in L^1(\mathbb{R})$ (not necessarily continuous) such that $$\operatorname{sup} \left\{\int\limits f(x)\phi'(x)dx: \phi\in C_c^1(\mathbb{R}), \|\phi\|_{L^{\infty}} \leq 1 \right\} < \infty.$$ In otherwords, distributional derivative of $f$ is a finite Radon measure. Then such functions satisfy the following $$\int\limits_{\mathbb{R}}|f(x+h)-f(x)|\leq Ch  \quad \text{for all } h\geq 0.\label{1}\tag{1}$$ Similarly, can we characterise functions in some $L^p(\mathbb{R})$ which satisfy \begin{eqnarray} \int\limits_{\mathbb{R}}|f(x+h)-2f(x)+f(x-h)|dx \leq Ch^2 \label{2}\tag{2} \end{eqnarray} for all $h\geq 0?$ For example, suppose we assume $$\operatorname{sup} \left\{\int\limits f(x)\phi''(x)dx: \phi\in C_c^2(\mathbb{R}), \|\phi\|_{L^{\infty}} \leq 1 \right\}< \infty,$$ then do we get \eqref{2}? P.S.: Functions in $C_c^1(\mathbb{R})\cap W^{1,\infty}(\mathbb{R})$ and $C_c^2(\mathbb{R})\cap W^{2,\infty}(\mathbb{R})$ satisfy \eqref{1} and \eqref{2} respectively. But I am looking for a more general class of functions(not necessarily continuous).","Let (not necessarily continuous) such that In otherwords, distributional derivative of is a finite Radon measure. Then such functions satisfy the following Similarly, can we characterise functions in some which satisfy for all For example, suppose we assume then do we get \eqref{2}? P.S.: Functions in and satisfy \eqref{1} and \eqref{2} respectively. But I am looking for a more general class of functions(not necessarily continuous).","f\in L^1(\mathbb{R}) \operatorname{sup} \left\{\int\limits f(x)\phi'(x)dx: \phi\in C_c^1(\mathbb{R}), \|\phi\|_{L^{\infty}} \leq 1 \right\} < \infty. f \int\limits_{\mathbb{R}}|f(x+h)-f(x)|\leq Ch  \quad \text{for all } h\geq 0.\label{1}\tag{1} L^p(\mathbb{R}) \begin{eqnarray}
\int\limits_{\mathbb{R}}|f(x+h)-2f(x)+f(x-h)|dx \leq Ch^2 \label{2}\tag{2}
\end{eqnarray} h\geq 0? \operatorname{sup} \left\{\int\limits f(x)\phi''(x)dx: \phi\in C_c^2(\mathbb{R}), \|\phi\|_{L^{\infty}} \leq 1 \right\}< \infty, C_c^1(\mathbb{R})\cap W^{1,\infty}(\mathbb{R}) C_c^2(\mathbb{R})\cap W^{2,\infty}(\mathbb{R})","['real-analysis', 'functional-analysis', 'analysis', 'measure-theory']"
8,Why is there only one differentiation but several integration?,Why is there only one differentiation but several integration?,,"Differentiation and integration are inverse operations of each other. However, there are Riemann, Lebesgue, Henstock–Kurzweil, etc… integrations, while there is only one differentiation $\lim\limits_{h\rightarrow 0}\frac{f(x+h)-f(x)}{h}$ . Quote from a book A Garden of Integrals : The derivative and the integral are the fundamental notions of calculus. Though there is essentially only one derivative, there is a variety of integrals, developed over the years for a variety of purposes, and this book describes them.","Differentiation and integration are inverse operations of each other. However, there are Riemann, Lebesgue, Henstock–Kurzweil, etc… integrations, while there is only one differentiation . Quote from a book A Garden of Integrals : The derivative and the integral are the fundamental notions of calculus. Though there is essentially only one derivative, there is a variety of integrals, developed over the years for a variety of purposes, and this book describes them.",\lim\limits_{h\rightarrow 0}\frac{f(x+h)-f(x)}{h},"['calculus', 'integration']"
9,Discontinuous function of two variables,Discontinuous function of two variables,,"Let \begin{equation}      f: \mathbb{R}^{2} \rightarrow \mathbb{R} \end{equation} be a function of two real variables given by \begin{equation}  f(x,y) = \begin{cases}        \frac{x}{y} & \text{for}\quad y\neq 0  \\        0 & \text{for}\quad y=0 \ \\        \end{cases} \end{equation} Does $f(0,0)=0$ ? My understating is that $f(0,0) =0$ since $f(x,0)=0$ for all $x \in \mathbb{R}$ . Is this correct?",Let be a function of two real variables given by Does ? My understating is that since for all . Is this correct?,"\begin{equation}
     f: \mathbb{R}^{2} \rightarrow \mathbb{R}
\end{equation} \begin{equation}
 f(x,y) = \begin{cases}
       \frac{x}{y} & \text{for}\quad y\neq 0  \\
       0 & \text{for}\quad y=0 \ \\
       \end{cases}
\end{equation} f(0,0)=0 f(0,0) =0 f(x,0)=0 x \in \mathbb{R}","['limits', 'analysis', 'multivariable-calculus', 'functions', 'continuity']"
10,Local behavior around critical points in high dimensions?,Local behavior around critical points in high dimensions?,,"Let $f:\mathbb{R}^n\rightarrow \mathbb{R}$ be a function that is (at least) $C^2$ . In the standard calculus classes one learns: If the gradient of $f$ at $x_0$ , then $f$ has a local extremum (minimum or maximum) at $x_0$ . Hence the gradient test is a necessary but not sufficient condition (as, e.g., $x\mapsto x^3$ shows). Using the (semi)positive/negative definiteness of the Hessian, one can disambiguate and identify among the previously found extremum candidates some that are local minima or maxima. But so far this doesn't give complete classification of the extrema, as it is possible one the one-dimensional case for a test involving $n$ derivatives, and it seems that this problem is rather difficult for higher dimensions, involving potentially Morse theory, as I found it mentioned online. Can someone clarify whether a nice, complete description (i.e. a nice characterization) of which points are minima and maxima exists in all dimension or point me to a definitive reference (that ideally also outlines the state of the art)? There are various online references, that provide partial answers: What is the higher-order derivative test in multivariable calculus? (but this seems to give a test involving all derivatives, but following the discussion there it seems this test is rather hard to check in practice and one has to rely on numerical computations; in the comments it is actually stated ""there is not an analogue test in multivariable calculus"" , and this comment is backed up in the comments by a user I trust, KCd, which seems to indicate that the problem I outlined is open. Morse theory is mentioned, but not particular information is given. I would be also interested in a statement along the lines of whether for every $n$ one can construct a function such that for any ""feasible"" test using higher order derivatives, any ""natural"" test using higher order derivatives that allows ""easy checking"" fails.) http://www.u.arizona.edu/~mwalker/MathCamp2021/UnconstrainedOptimization.pdf (this gives actually necessary and sufficient conditions, but I'm not happy with them, because they are different conditions, so the don't characterize extrema completely) https://www.ripublication.com/adsa20/v15n2p11.pdf (it's from 2020, but I'm not 100% convinced this is really a legit or novel article; nonetheless it's interested to read) EDIT: I have also asked this question on Mathoverflow now.","Let be a function that is (at least) . In the standard calculus classes one learns: If the gradient of at , then has a local extremum (minimum or maximum) at . Hence the gradient test is a necessary but not sufficient condition (as, e.g., shows). Using the (semi)positive/negative definiteness of the Hessian, one can disambiguate and identify among the previously found extremum candidates some that are local minima or maxima. But so far this doesn't give complete classification of the extrema, as it is possible one the one-dimensional case for a test involving derivatives, and it seems that this problem is rather difficult for higher dimensions, involving potentially Morse theory, as I found it mentioned online. Can someone clarify whether a nice, complete description (i.e. a nice characterization) of which points are minima and maxima exists in all dimension or point me to a definitive reference (that ideally also outlines the state of the art)? There are various online references, that provide partial answers: What is the higher-order derivative test in multivariable calculus? (but this seems to give a test involving all derivatives, but following the discussion there it seems this test is rather hard to check in practice and one has to rely on numerical computations; in the comments it is actually stated ""there is not an analogue test in multivariable calculus"" , and this comment is backed up in the comments by a user I trust, KCd, which seems to indicate that the problem I outlined is open. Morse theory is mentioned, but not particular information is given. I would be also interested in a statement along the lines of whether for every one can construct a function such that for any ""feasible"" test using higher order derivatives, any ""natural"" test using higher order derivatives that allows ""easy checking"" fails.) http://www.u.arizona.edu/~mwalker/MathCamp2021/UnconstrainedOptimization.pdf (this gives actually necessary and sufficient conditions, but I'm not happy with them, because they are different conditions, so the don't characterize extrema completely) https://www.ripublication.com/adsa20/v15n2p11.pdf (it's from 2020, but I'm not 100% convinced this is really a legit or novel article; nonetheless it's interested to read) EDIT: I have also asked this question on Mathoverflow now.",f:\mathbb{R}^n\rightarrow \mathbb{R} C^2 f x_0 f x_0 x\mapsto x^3 n n,"['analysis', 'non-convex-optimization']"
11,"Convolution derivative, $(f*g)'=f*g'$","Convolution derivative,",(f*g)'=f*g',"Let $f,g \in L^1$ and $g$ is differentiable and $g' \in L^1$ , then $(f*g)'=f*g'$ PROOF : For this proof I am asked to use the dominated convergence theorem It can be quickly arrived at that $$\begin{align} (f*g)'(x) &= \lim\limits_{h \to 0}  \int_\mathbb{R} f(t)\frac{g(x+h-t)-g(x-t)}{h}dt\\ \end{align}$$ For the following, it is necessary to interchange the limit and the integral and it is here where the theorem mentioned above is used. To do so, it is sufficient to find a $k\in L^1$ such that $|f(t)\frac{g(x+h-t)-g(x-t)}{h}|\leq k(t)$ However, I am having trouble finding such a function $k$","Let and is differentiable and , then PROOF : For this proof I am asked to use the dominated convergence theorem It can be quickly arrived at that For the following, it is necessary to interchange the limit and the integral and it is here where the theorem mentioned above is used. To do so, it is sufficient to find a such that However, I am having trouble finding such a function","f,g \in L^1 g g' \in L^1 (f*g)'=f*g' \begin{align}
(f*g)'(x) &= \lim\limits_{h \to 0}  \int_\mathbb{R} f(t)\frac{g(x+h-t)-g(x-t)}{h}dt\\
\end{align} k\in L^1 |f(t)\frac{g(x+h-t)-g(x-t)}{h}|\leq k(t) k","['analysis', 'measure-theory', 'lebesgue-integral', 'convolution']"
12,"$f:[0,1]\times Y\to \mathbb{R}$ be a continuous function, prove $I_f(y)=\int_0^1f(x,y)dx$ is continuous. $Y$ is metric space","be a continuous function, prove  is continuous.  is metric space","f:[0,1]\times Y\to \mathbb{R} I_f(y)=\int_0^1f(x,y)dx Y","Let $f:[0,1]\times Y\to \mathbb{R}$ be a continuous function where $Y$ is a metric space. Now let $I_f(y):Y\to \mathbb{R}$ be the function $$I_f(y)=\int_0^1f(x,y)dx$$ then prove that $I_f(y)$ is continuous on $Y$ . I tried this problem first fixing $y$ .  If we fix $y$ then $g(x)=f(x,y)$ is continuous on $[0,1]$ , hence it is uniformly continuous. Therefore $\forall\ \epsilon >0\ \exists \ \delta>0$ such that $$|x-x'|<\delta\implies |f(x,y)-f(x',y)|<\epsilon$$ Suppose we want to prove continuity at $y$ . Now since $f$ is continuous for every $\epsilon>0$ there exists $\delta>0$ such that $|h|<\delta\implies |f(x,y+h)-f(x,y)|<\epsilon$ . I want to show $|f(x,y+h)-f(x,y)|<\epsilon$ for all $x\in [0,1]$ and for $|h|<\delta$ where $\delta>0$ so that $$\int_0^1|f(x,y+h)-f(x,y)|dx< \int_0^1\epsilon\, dx=\epsilon$$ But i cant not do anything further with this. How to do this","Let be a continuous function where is a metric space. Now let be the function then prove that is continuous on . I tried this problem first fixing .  If we fix then is continuous on , hence it is uniformly continuous. Therefore such that Suppose we want to prove continuity at . Now since is continuous for every there exists such that . I want to show for all and for where so that But i cant not do anything further with this. How to do this","f:[0,1]\times Y\to \mathbb{R} Y I_f(y):Y\to \mathbb{R} I_f(y)=\int_0^1f(x,y)dx I_f(y) Y y y g(x)=f(x,y) [0,1] \forall\ \epsilon >0\ \exists \ \delta>0 |x-x'|<\delta\implies |f(x,y)-f(x',y)|<\epsilon y f \epsilon>0 \delta>0 |h|<\delta\implies |f(x,y+h)-f(x,y)|<\epsilon |f(x,y+h)-f(x,y)|<\epsilon x\in [0,1] |h|<\delta \delta>0 \int_0^1|f(x,y+h)-f(x,y)|dx< \int_0^1\epsilon\, dx=\epsilon","['real-analysis', 'calculus', 'integration', 'analysis']"
13,"Is $A\setminus \{x_1,\dots,x_n\}$ an open set if $A$ is open?",Is  an open set if  is open?,"A\setminus \{x_1,\dots,x_n\} A","let $A$ be an open set of the metric space $(X,d)$ . Now I wanted to prove that for any $x_1,\dots,x_n\in X$ also the set $$A\setminus \{x_1,\dots,x_n\}$$ is open. My idea was the following. Let $a\in A\setminus \{x_1,\dots,x_n\}$ . Since $A$ is open there exists $r>0$ such that $B(a,r)\subset A$ . Now let me define $$r' = \min \left \{ r, \min_{k=1, \dots,n} d(a,x_k)\right\}$$ then clearly $r'\leq r$ and hence $B(a,r')\subset U$ . But, by construction, $x_k\notin B(a,r')$ for all $k$ , hence $B(a,r') \subset A \setminus \{x_1,\dots,x_n\}$ . Does this work or did I miss something? If it is wrong could you maybe show me why?","let be an open set of the metric space . Now I wanted to prove that for any also the set is open. My idea was the following. Let . Since is open there exists such that . Now let me define then clearly and hence . But, by construction, for all , hence . Does this work or did I miss something? If it is wrong could you maybe show me why?","A (X,d) x_1,\dots,x_n\in X A\setminus \{x_1,\dots,x_n\} a\in A\setminus \{x_1,\dots,x_n\} A r>0 B(a,r)\subset A r' = \min \left \{ r, \min_{k=1, \dots,n} d(a,x_k)\right\} r'\leq r B(a,r')\subset U x_k\notin B(a,r') k B(a,r') \subset A \setminus \{x_1,\dots,x_n\}","['real-analysis', 'functional-analysis', 'analysis', 'solution-verification', 'metric-spaces']"
14,Constrained Maximization problem involving integer variables,Constrained Maximization problem involving integer variables,,The maximization problem may be seen as a problem of integer programming problem and may be solved by Gomory cut method or branch and bound technique. Is there any other way to see this problem?,The maximization problem may be seen as a problem of integer programming problem and may be solved by Gomory cut method or branch and bound technique. Is there any other way to see this problem?,,"['analysis', 'inequality', 'optimization']"
15,Where is symmetry condition used in the proof of Banach fixed point theorem?,Where is symmetry condition used in the proof of Banach fixed point theorem?,,"I was learning the proof of Banach fixed point theorem from here . I am wondering where is the symmetry condition $d(x,y)=d(y,x)$ used in the proof?",I was learning the proof of Banach fixed point theorem from here . I am wondering where is the symmetry condition used in the proof?,"d(x,y)=d(y,x)","['real-analysis', 'general-topology', 'analysis', 'metric-spaces', 'proof-explanation']"
16,Papa Rudin abstract integration exercise $1$,Papa Rudin abstract integration exercise,1,"There is the exercise: Does there exist an infinite $\sigma$ -algebra which has only countably many members? Intuitively I think that the answer is no but I've really trouble to proof it. This is one proof But I have few questions about it. I don't understand this part in this proof: "" If not, then every other measurable set of $X$ must intersect both $E$ and $E^c$ nontrivially."" I also don't understand why is $F_n$ a set of size $n$ measurable sets? for example $F_2$ has three members . by this logic $F_n$ should have $n+1$ members. why do we need to make $n$ sufficiently large? Any help would be appreciated.","There is the exercise: Does there exist an infinite -algebra which has only countably many members? Intuitively I think that the answer is no but I've really trouble to proof it. This is one proof But I have few questions about it. I don't understand this part in this proof: "" If not, then every other measurable set of must intersect both and nontrivially."" I also don't understand why is a set of size measurable sets? for example has three members . by this logic should have members. why do we need to make sufficiently large? Any help would be appreciated.",\sigma X E E^c F_n n F_2 F_n n+1 n,"['real-analysis', 'analysis', 'measure-theory', 'measurable-sets']"
17,Papa Rudin theorem $1.39$,Papa Rudin theorem,1.39,"There is the theorem: Suppose $f:X\to [0,\infty ]$ is measurable, $E\in\mathfrak{M}$ , and $\displaystyle \int \limits _Ef\,d\mu =0$ . Then $f=0$ almost everywhere on $E$ . There is the proof: If $A_n=\left \{x\in E:f(x)>\dfrac{1}{n}\right \}$ , $n=1,2,3,\ldots$ , then $$\frac{1}{n}\mu (A_n)\leq \int \limits _{A_n}f\,d\mu \leq \int \limits _Ef\,d\mu =0,$$ so that $\mu (A_n)=0$ . Since $\displaystyle \{x\in E:f(x)>0\}=\bigcup \limits _{n=1}^\infty A_n$ , the theorem follows. I don't understand how  we get this inequality $\displaystyle \frac{1}{n}\mu(A_n)\leq \int \limits _{A_n}f\,d\mu$ . Any help would be appreciated.","There is the theorem: Suppose is measurable, , and . Then almost everywhere on . There is the proof: If , , then so that . Since , the theorem follows. I don't understand how  we get this inequality . Any help would be appreciated.","f:X\to [0,\infty ] E\in\mathfrak{M} \displaystyle \int \limits _Ef\,d\mu =0 f=0 E A_n=\left \{x\in E:f(x)>\dfrac{1}{n}\right \} n=1,2,3,\ldots \frac{1}{n}\mu (A_n)\leq \int \limits _{A_n}f\,d\mu \leq \int \limits _Ef\,d\mu =0, \mu (A_n)=0 \displaystyle \{x\in E:f(x)>0\}=\bigcup \limits _{n=1}^\infty A_n \displaystyle \frac{1}{n}\mu(A_n)\leq \int \limits _{A_n}f\,d\mu","['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral']"
18,Whats the idea behind using Bregman divergence (in particular Bregman proximal method) to minimise a functional?,Whats the idea behind using Bregman divergence (in particular Bregman proximal method) to minimise a functional?,,"Given some functional $E$ on a convex set $\Omega$ , the Bregman divergence $D_E$ (of some convex function $E$ ) is defined at a point $p$ as the difference between its value at that point and its first order Taylor expansion around some other point $q$ : $$D_E(p,q):=E(p)-E(q)-\langle \nabla E(q) , p-q\rangle. $$ For some strictly convex function $\phi$ the proximal map according to this Bregman divergence is $$ \text{Prox}^{D_E}_\phi(p)=\text{argmin}_{q\in \mathcal{D}(\phi)}D_E(q,p)+\phi(q).$$ How can iterative Bregman projections be used for finding the infimum of $\phi$ ?","Given some functional on a convex set , the Bregman divergence (of some convex function ) is defined at a point as the difference between its value at that point and its first order Taylor expansion around some other point : For some strictly convex function the proximal map according to this Bregman divergence is How can iterative Bregman projections be used for finding the infimum of ?","E \Omega D_E E p q D_E(p,q):=E(p)-E(q)-\langle \nabla E(q) , p-q\rangle.  \phi  \text{Prox}^{D_E}_\phi(p)=\text{argmin}_{q\in \mathcal{D}(\phi)}D_E(q,p)+\phi(q). \phi","['analysis', 'optimization', 'convex-optimization', 'numerical-optimization', 'proximal-operators']"
19,Calculation of limit with sandwich theorem,Calculation of limit with sandwich theorem,,I want to calculate the limit $$\lim_{n\rightarrow \infty}\frac{n+\sin(n)}{n^2+n+1}$$ I have done the following : $$\left |\frac{n+\sin(n)}{n^2+n+1}\right |=\frac{|n+\sin(n)|}{|n^2+n+1|}=\frac{|n+\sin(n)|}{n^2+n+1}\leq \frac{|n|+|\sin(n)|}{n^2+n+1}=\frac{n+|\sin(n)|}{n^2+n+1}\leq \frac{n+n}{n^2+n+1}=\frac{2n}{n^2+n+1}\leq \frac{2n}{n^2}=\frac{2}{n}$$ So since $\left |\frac{n+\sin(n)}{n^2+n+1}\right |\leq \frac{2}{n}$ we get $$-\frac{2}{n}\leq \frac{n+\sin(n)}{n^2+n+1}\leq \frac{2}{n}$$ and since $\lim_{n\rightarrow \infty}\left (-\frac{2}{n}\right )=\lim_{n\rightarrow \infty}\left (\frac{2}{n}\right )=0$ and so we get $\lim_{n\rightarrow \infty}\frac{n+\sin(n)}{n^2+n+1}=0$ . Is this correct? Or could we get an other easier inequality to use the sandwich theorem?,I want to calculate the limit I have done the following : So since we get and since and so we get . Is this correct? Or could we get an other easier inequality to use the sandwich theorem?,\lim_{n\rightarrow \infty}\frac{n+\sin(n)}{n^2+n+1} \left |\frac{n+\sin(n)}{n^2+n+1}\right |=\frac{|n+\sin(n)|}{|n^2+n+1|}=\frac{|n+\sin(n)|}{n^2+n+1}\leq \frac{|n|+|\sin(n)|}{n^2+n+1}=\frac{n+|\sin(n)|}{n^2+n+1}\leq \frac{n+n}{n^2+n+1}=\frac{2n}{n^2+n+1}\leq \frac{2n}{n^2}=\frac{2}{n} \left |\frac{n+\sin(n)}{n^2+n+1}\right |\leq \frac{2}{n} -\frac{2}{n}\leq \frac{n+\sin(n)}{n^2+n+1}\leq \frac{2}{n} \lim_{n\rightarrow \infty}\left (-\frac{2}{n}\right )=\lim_{n\rightarrow \infty}\left (\frac{2}{n}\right )=0 \lim_{n\rightarrow \infty}\frac{n+\sin(n)}{n^2+n+1}=0,"['calculus', 'limits', 'analysis']"
20,Hecke regularization of weight-2 Eisenstein series and differentiating under the infinite sum,Hecke regularization of weight-2 Eisenstein series and differentiating under the infinite sum,,"I have a ""proof"" of the false fact that the Hecke regularized Eisenstein series $$ E_2(\tau) = \lim_{s\to 0} \sum_{(m,n)\ne (0,0)} \frac{1}{(m+\tau n)^2 |m+\tau n|^s}$$ is holomorphic; could someone help me figure out where I'm being too naive? If we write $$ E_2(\tau, s) = \sum_{(m,n)\ne (0,0)} \frac{1}{(m+\tau n)^2 |m+\tau n|^s},$$ it converges absolutely and uniformly on compact subsets of the upper half-plane for $\text{Re }s>0$ . Thus, for $\text{Re }s>0$ , $$ \frac{\partial}{\partial \overline{\tau}}E_2(\tau, s) = \sum_{(m,n)\ne (0,0)} \frac{\partial}{\partial \overline{\tau}}\frac{1}{(m+\tau n)^2 |m+\tau n|^s} $$ and the RHS is still an absolutely convergent sum which approaches $0$ as $s\to 0$ since we pick up a factor of $s$ from the antiholomorphic derivative. Thus, by continuity, $ \frac{\partial}{\partial \overline{\tau}}E_2(\tau, 0)=0$ , i.e. it's holomorphic.","I have a ""proof"" of the false fact that the Hecke regularized Eisenstein series is holomorphic; could someone help me figure out where I'm being too naive? If we write it converges absolutely and uniformly on compact subsets of the upper half-plane for . Thus, for , and the RHS is still an absolutely convergent sum which approaches as since we pick up a factor of from the antiholomorphic derivative. Thus, by continuity, , i.e. it's holomorphic."," E_2(\tau) = \lim_{s\to 0} \sum_{(m,n)\ne (0,0)} \frac{1}{(m+\tau n)^2 |m+\tau n|^s}  E_2(\tau, s) = \sum_{(m,n)\ne (0,0)} \frac{1}{(m+\tau n)^2 |m+\tau n|^s}, \text{Re }s>0 \text{Re }s>0  \frac{\partial}{\partial \overline{\tau}}E_2(\tau, s) = \sum_{(m,n)\ne (0,0)} \frac{\partial}{\partial \overline{\tau}}\frac{1}{(m+\tau n)^2 |m+\tau n|^s}  0 s\to 0 s  \frac{\partial}{\partial \overline{\tau}}E_2(\tau, 0)=0","['sequences-and-series', 'complex-analysis', 'analysis', 'number-theory', 'modular-forms']"
21,Coincidence of Subspace and Metric Topologies,Coincidence of Subspace and Metric Topologies,,"How to show that a subspace of a metrizable space is also metrizable? Let $X$ be a topological space, and let $Y$ be a (non-empty) subset of $X$ ; let $d$ be a metric on $X$ that induces the topology of $X$ . Then the restriction $d | Y \times Y$ of $d$ to the subset $Y \times Y$ of $X \times X$ , which is defined by $$ \big( d | Y \times Y \big) ( p, q) := d(p, q) \ \mbox{ for all } p, q \in Y,  $$ is also a metric on $Y$ . Now let $\mathscr{T}$ be the subspace topology that $Y$ inherits as a subspace of $X$ , and let $\mathscr{T}^\prime$ be the topology induced by the metric $d | Y \times Y$ on $Y$ . How to show that these two topologies on $Y$ are the same? My Attempt: Let $V$ be a $\mathscr{T}$ -open set of $Y$ , and let $y \in Y$ . Then there exists an open set $U$ of $X$ such that $V = Y \cap U$ , and our $y \in U$ of course, which implies the existence a real number $\epsilon_y > 0$ such that $B_d \left( y, \epsilon_y \right) \subset U$ , that is, for any point $x \in X$ such that $d(x, y) < \epsilon_y$ , we also have $x \in U$ ; therefore for any point $x \in Y \subset X$ in particular such that $\big( d | Y \times Y \big) (x, y) < \epsilon_y$ , we  have $x \in U$ and hence $x \in Y \cap U = V$ , which implies that $$ B_{d | Y \times Y} \left( y, \epsilon_y \right) \subset V, $$ and so $$ V = \bigcup_{y \in V} B_{d | Y \times Y} \left( y, \epsilon_y \right), $$ from which it follows that $V$ is also a $\mathscr{T}^\prime$ -open set and hence that $$ \mathscr{T} \subset \mathscr{T}^\prime.  $$ Am I right? Conversely, let $V^\prime$ be a $\mathscr{T}^\prime$ -open set of $Y$ , and let $y^\prime \in V^\prime$ . Then there exists a real number $\epsilon_{y^\prime} > 0$ such that $$ B_{d | Y \times Y } \left( y^\prime, \epsilon_{y^\prime} \right) \subset V^\prime, $$ that is, for any point $y \in Y$ such that $\big( d | Y \times Y\big) \left( y, y^\prime \right) < \epsilon_{y^\prime}$ , we also have $y \in V^\prime$ . Let us put $$ U^\prime := \bigcup_{ y^\prime \in V^\prime} B_d \left( y^\prime, \epsilon_{y^\prime} \right).  $$ This set $U^\prime$ is an open set of $X$ of course and $V^\prime = Y \cap U^\prime$ , thus showing that $V^\prime$ is $\mathscr{T}$ -open, which implies that $$ \mathscr{T}^\prime \subset \mathscr{T}. $$ Am I right? Is the above proof correct, clear, and complete enough in each and every detail? Or, are there any gaps therein?","How to show that a subspace of a metrizable space is also metrizable? Let be a topological space, and let be a (non-empty) subset of ; let be a metric on that induces the topology of . Then the restriction of to the subset of , which is defined by is also a metric on . Now let be the subspace topology that inherits as a subspace of , and let be the topology induced by the metric on . How to show that these two topologies on are the same? My Attempt: Let be a -open set of , and let . Then there exists an open set of such that , and our of course, which implies the existence a real number such that , that is, for any point such that , we also have ; therefore for any point in particular such that , we  have and hence , which implies that and so from which it follows that is also a -open set and hence that Am I right? Conversely, let be a -open set of , and let . Then there exists a real number such that that is, for any point such that , we also have . Let us put This set is an open set of of course and , thus showing that is -open, which implies that Am I right? Is the above proof correct, clear, and complete enough in each and every detail? Or, are there any gaps therein?","X Y X d X X d | Y \times Y d Y \times Y X \times X 
\big( d | Y \times Y \big) ( p, q) := d(p, q) \ \mbox{ for all } p, q \in Y, 
 Y \mathscr{T} Y X \mathscr{T}^\prime d | Y \times Y Y Y V \mathscr{T} Y y \in Y U X V = Y \cap U y \in U \epsilon_y > 0 B_d \left( y, \epsilon_y \right) \subset U x \in X d(x, y) < \epsilon_y x \in U x \in Y \subset X \big( d | Y \times Y \big) (x, y) < \epsilon_y x \in U x \in Y \cap U = V 
B_{d | Y \times Y} \left( y, \epsilon_y \right) \subset V,
 
V = \bigcup_{y \in V} B_{d | Y \times Y} \left( y, \epsilon_y \right),
 V \mathscr{T}^\prime 
\mathscr{T} \subset \mathscr{T}^\prime. 
 V^\prime \mathscr{T}^\prime Y y^\prime \in V^\prime \epsilon_{y^\prime} > 0 
B_{d | Y \times Y } \left( y^\prime, \epsilon_{y^\prime} \right) \subset V^\prime,
 y \in Y \big( d | Y \times Y\big) \left( y, y^\prime \right) < \epsilon_{y^\prime} y \in V^\prime 
U^\prime := \bigcup_{ y^\prime \in V^\prime} B_d \left( y^\prime, \epsilon_{y^\prime} \right). 
 U^\prime X V^\prime = Y \cap U^\prime V^\prime \mathscr{T} 
\mathscr{T}^\prime \subset \mathscr{T}.
","['general-topology', 'analysis', 'solution-verification', 'metric-spaces', 'metrizability']"
22,Infinite product of Banach algebra is a Banach algebra or not,Infinite product of Banach algebra is a Banach algebra or not,,"First I am trying to show infinte product of banach spaces is banch or not. It is in the book Functional Analysis by John B. Conway . It is proposition 4.4 in chapter 3. It says that if $\{ \mathcal{H}_i. : i\in I\}$ be a collection of Banach spaces and let $\mathcal{H}=\oplus _p \mathcal{H}_i, 1 \leq p < \infty $ . Then $\mathcal{H}$ is also Banach space. Where $\mathcal{H}=\oplus _p \mathcal{H}_i= \{ x\in \prod_i \mathcal{H_i}: \|x\|={[\sum_i\|x(i)\|^p]}^{\frac{1}{p}} < \infty \}$ . My problem: Let $\{x_n\} \subset \mathcal{H} $ be a Cauchy sequence. Then each $\{ x_n(i) \}$ will be also a Cauchy sequence in $\mathcal{H}_i$ .lets say $x_n(i)$ converges to $x(i)$ so I will get $x=\prod_i x(i)$ . Now I am struggling to show $x\in \mathcal{H}$ and even if I show that I am unable to show $x_n$ converges to $x$ in this norm. If I take $p=\infty$ then I was able to show that thing but here it is problematic. Can anyone help me to arrange $\epsilon$ and $\delta$ . Now If I have all $\mathcal{H}_i$ are banach algebras how to define multiplication of $\mathcal{H}$",First I am trying to show infinte product of banach spaces is banch or not. It is in the book Functional Analysis by John B. Conway . It is proposition 4.4 in chapter 3. It says that if be a collection of Banach spaces and let . Then is also Banach space. Where . My problem: Let be a Cauchy sequence. Then each will be also a Cauchy sequence in .lets say converges to so I will get . Now I am struggling to show and even if I show that I am unable to show converges to in this norm. If I take then I was able to show that thing but here it is problematic. Can anyone help me to arrange and . Now If I have all are banach algebras how to define multiplication of,"\{ \mathcal{H}_i. : i\in I\} \mathcal{H}=\oplus _p \mathcal{H}_i, 1 \leq p < \infty  \mathcal{H} \mathcal{H}=\oplus _p \mathcal{H}_i= \{ x\in \prod_i \mathcal{H_i}: \|x\|={[\sum_i\|x(i)\|^p]}^{\frac{1}{p}} < \infty \} \{x_n\} \subset \mathcal{H}  \{ x_n(i) \} \mathcal{H}_i x_n(i) x(i) x=\prod_i x(i) x\in \mathcal{H} x_n x p=\infty \epsilon \delta \mathcal{H}_i \mathcal{H}","['functional-analysis', 'analysis', 'operator-theory', 'banach-spaces']"
23,Functions with a best linear approximation,Functions with a best linear approximation,,"Let $f : \mathbb{R}^2 \to \mathbb{R}$ . Fix a point $x_0$ throughout. Assume that all directional derivatives of $f$ exist and they are linear with respect to the direction, meaning $v \to D_{v}$ is  a linear map where $D_v$ is the derivative IN the direction $v$ . Geometrically we get a plane that is the 'best' linear approximation of $f$ at $x_0$ . Even then $f$ may not be differentiable at $x_0$ even if it is continuous. example $f(x, y) = \frac{x^3y}{x^4 + y^2}$ and defined $0$ at $0$ and take $x_0 = 0$ . So, such functions which have a 'best linear approximation' are a larger class than differentiable functions. One can check that they are closed under addition and multiplication. Is this class interesting? Is this notion of weaker derivative studied in the literature, because the examples for such functions that I could find (as above are not pathological also). Also the best linear appoximation can be made rigorous as if there is a linear $D_2$ such that $|f(x_0+h) - f(x_0) - D(v)h| < |f(x_0+h) - f(x_0) - D_2(v)h|$ for all $h$ sufficiently small then $D_2 = D$ (the linear function returning the directional derivatives). I understand this quesition is a bit vague but any comments or insights are also most welcome.","Let . Fix a point throughout. Assume that all directional derivatives of exist and they are linear with respect to the direction, meaning is  a linear map where is the derivative IN the direction . Geometrically we get a plane that is the 'best' linear approximation of at . Even then may not be differentiable at even if it is continuous. example and defined at and take . So, such functions which have a 'best linear approximation' are a larger class than differentiable functions. One can check that they are closed under addition and multiplication. Is this class interesting? Is this notion of weaker derivative studied in the literature, because the examples for such functions that I could find (as above are not pathological also). Also the best linear appoximation can be made rigorous as if there is a linear such that for all sufficiently small then (the linear function returning the directional derivatives). I understand this quesition is a bit vague but any comments or insights are also most welcome.","f : \mathbb{R}^2 \to \mathbb{R} x_0 f v \to D_{v} D_v v f x_0 f x_0 f(x, y) = \frac{x^3y}{x^4 + y^2} 0 0 x_0 = 0 D_2 |f(x_0+h) - f(x_0) - D(v)h| < |f(x_0+h) - f(x_0) - D_2(v)h| h D_2 = D","['calculus', 'analysis', 'multivariable-calculus', 'reference-request']"
24,Why zeta function is related to this logarithmic function's integral?,Why zeta function is related to this logarithmic function's integral?,,"I've been going through ""103 (almost) impossible integrals, sums and series"" which is a book containing lots of interesting integrals. I saw a very intriguing definite integral that the book stated has a relation with Riemann's zeta function in the following manner: $$ \int_{0}^{1} \frac{\ln^2(x+1)}{x} dx = \frac{1}{4}\zeta(3) $$ I thought this connection's meaning would become clear when I prove it but now that I've proved the relation I still feel uncomfortable with it. It's just so out of place. So my question is, what is the intuition behind this equation? Why would zeta function have anything to do with this particular function's integral?","I've been going through ""103 (almost) impossible integrals, sums and series"" which is a book containing lots of interesting integrals. I saw a very intriguing definite integral that the book stated has a relation with Riemann's zeta function in the following manner: I thought this connection's meaning would become clear when I prove it but now that I've proved the relation I still feel uncomfortable with it. It's just so out of place. So my question is, what is the intuition behind this equation? Why would zeta function have anything to do with this particular function's integral?","
\int_{0}^{1} \frac{\ln^2(x+1)}{x} dx = \frac{1}{4}\zeta(3)
","['integration', 'analysis', 'intuition', 'riemann-zeta']"
25,Generalizing a property of cones to cusps,Generalizing a property of cones to cusps,,"An $\alpha$ -cusp with slope $a$ and vertex $z$ is a function $ C^\alpha: \mathbb{R}^{n} \longrightarrow \mathbb{R} $ defined by $ C^\alpha := a| x - z |^{\alpha} + b  $ where $  a,b \in \mathbb{R} $ . For a given function $f:D \subset \mathbf{R}^n \longrightarrow \mathbb{R}$ and a real number $\alpha \in [0,1]$ we set for all $A \subset D$ \begin{equation} \label{Holder seminorm}     [f]_{\alpha,A} : = \inf \left \{ C \in \mathbb{R} : \frac{|u(x) - u(y)|}{|x-y|^\alpha } \le C \quad \mbox{for all} \quad x \neq y \in A  \right \}. \end{equation} I'd like to prove the following result Let $ W \subset \subset \mathbb{R}^{n} $ and $ C^{\alpha}_u$ be an $\alpha$ -cusp with vertex $ z \notin W $ and slop $ a \in \mathbb{R} $ . If $ u \in C( \overline{W}) $ satisfies $ u = C^{\alpha}_u $ on $ \partial W $ and $ [u]_{\alpha,W} = | a | $ , then $ u \equiv C^{\alpha}_u $ in $ W $ . Suppose that there exists $ y \in W $ so that $ u(y) \neq C^\alpha_u (y) $ , for instance $ u(y) > C^{\alpha}_u (y) $ . Let $L$ be the line determined by the vertex $z$ of the cusp $C^{\alpha}_u$ and $y$ . Let $y^{*  }$ and $y^{**}$ respectively  the closest and further point in $L \cap \partial W$ in relation to $y    $ . If $ a \geq 0 $ we obtain, $$ u (y) - u(y^{*}) \ > C^{\alpha}_u(y) - C^{\alpha}_u(y^{*})   = a| y - y^{*} |^{\alpha} . $$ The proof for $\alpha =1$ is the case when $C^\alpha_u$ is a cone. You can see detais em (1.4) in here https://www.ams.org/journals/bull/2004-41-04/S0273-0979-04-01035-3/S0273-0979-04-01035-3.pdf . But we do the proof below. Notice that we have used the equality in the triangle inequality at the end. For $\alpha \in (0,1)$ So I ask Is there a triangle inequalty for $d(x,y) = |x-y|^\alpha $ that reaches an equality for some $y^*,y^{**}$ similar to the aboves? Maybe instead of a Line passing through $y$ and $z$ we have to find some ""cusp"" instead a line. Given corrects $y^*,y^{**}$ in this case. If you can find a counterexemplo I appreciate too.","An -cusp with slope and vertex is a function defined by where . For a given function and a real number we set for all I'd like to prove the following result Let and be an -cusp with vertex and slop . If satisfies on and , then in . Suppose that there exists so that , for instance . Let be the line determined by the vertex of the cusp and . Let and respectively  the closest and further point in in relation to . If we obtain, The proof for is the case when is a cone. You can see detais em (1.4) in here https://www.ams.org/journals/bull/2004-41-04/S0273-0979-04-01035-3/S0273-0979-04-01035-3.pdf . But we do the proof below. Notice that we have used the equality in the triangle inequality at the end. For So I ask Is there a triangle inequalty for that reaches an equality for some similar to the aboves? Maybe instead of a Line passing through and we have to find some ""cusp"" instead a line. Given corrects in this case. If you can find a counterexemplo I appreciate too.","\alpha a z  C^\alpha: \mathbb{R}^{n} \longrightarrow \mathbb{R}   C^\alpha := a| x - z |^{\alpha} + b     a,b \in \mathbb{R}  f:D \subset \mathbf{R}^n \longrightarrow \mathbb{R} \alpha \in [0,1] A \subset D \begin{equation} \label{Holder seminorm}
    [f]_{\alpha,A} : = \inf \left \{ C \in \mathbb{R} : \frac{|u(x) - u(y)|}{|x-y|^\alpha } \le C \quad \mbox{for all} \quad x \neq y \in A  \right \}.
\end{equation}  W \subset \subset \mathbb{R}^{n}   C^{\alpha}_u \alpha  z \notin W   a \in \mathbb{R}   u \in C( \overline{W})   u = C^{\alpha}_u   \partial W   [u]_{\alpha,W} = | a |   u \equiv C^{\alpha}_u   W   y \in W   u(y) \neq C^\alpha_u (y)   u(y) > C^{\alpha}_u (y)  L z C^{\alpha}_u y y^{*  } y^{**} L \cap \partial W y      a \geq 0   u (y) - u(y^{*}) \ > C^{\alpha}_u(y) - C^{\alpha}_u(y^{*})   = a| y - y^{*} |^{\alpha} .  \alpha =1 C^\alpha_u \alpha \in (0,1) d(x,y) = |x-y|^\alpha  y^*,y^{**} y z y^*,y^{**}","['analysis', 'solution-verification']"
26,Reconciling two versions of Taylor's Theorem,Reconciling two versions of Taylor's Theorem,,"Lemma $1.4$ (Taylor's theorem with remainder). Let $f$ be a $C^{\infty}$ function on an open subset $U$ of $\mathbb{R}^{n}$ star-shaped with respect to a point $p=\left(p^{1}, \ldots, p^{n}\right)$ in $U$ . Then there are functions $g_{1}(x), \ldots, g_{n}(x) \in C^{\infty}(U)$ such that $$ f(x)=f(p)+\sum_{i=1}^{n}\left(x^{i}-p^{i}\right) g_{i}(x), \quad g_{i}(p)=\frac{\partial f}{\partial x^{i}}(p) . $$ I am not used to this form of Taylor's theorem. Previously, I have seen in the single variable case that there exists a point between the approximation and the point we know for which is equal to the difference between the Taylor polynomial and the function. 5.15 Theorem Suppose $f$ is a real function on $[a, b], n$ is a positive integer, $f^{(n-1)}$ is continuous on $[a, b], f^{(n)}(t)$ exists for every $t \in(a, b)$ . Let $\alpha, \beta$ be distinct points of $[a, b]$ , and define $$ P(t)=\sum_{k=0}^{n-1} \frac{f^{(k)}(\alpha)}{k !}(t-\alpha)^{k} . $$ Then there exists a point $x$ between $\alpha$ and $\beta$ such that $$ f(\beta)=P(\beta)+\frac{f^{(n)}(x)}{n !}(\beta-\alpha)^{n} . $$ How can I reconcile these two viewpoints? is the $\frac{f^{(n)}(x)}{n!}$ what we take to be our $g$ in the single variable case? In the second, I haven't thoguht to view the remainder as a smooth function I've taken for granted that there is some constant which gives you equality. Is there something about the remainder in the second theorem which is $C^\infty$ ?","Lemma (Taylor's theorem with remainder). Let be a function on an open subset of star-shaped with respect to a point in . Then there are functions such that I am not used to this form of Taylor's theorem. Previously, I have seen in the single variable case that there exists a point between the approximation and the point we know for which is equal to the difference between the Taylor polynomial and the function. 5.15 Theorem Suppose is a real function on is a positive integer, is continuous on exists for every . Let be distinct points of , and define Then there exists a point between and such that How can I reconcile these two viewpoints? is the what we take to be our in the single variable case? In the second, I haven't thoguht to view the remainder as a smooth function I've taken for granted that there is some constant which gives you equality. Is there something about the remainder in the second theorem which is ?","1.4 f C^{\infty} U \mathbb{R}^{n} p=\left(p^{1}, \ldots, p^{n}\right) U g_{1}(x), \ldots, g_{n}(x) \in C^{\infty}(U) 
f(x)=f(p)+\sum_{i=1}^{n}\left(x^{i}-p^{i}\right) g_{i}(x), \quad g_{i}(p)=\frac{\partial f}{\partial x^{i}}(p) .
 f [a, b], n f^{(n-1)} [a, b], f^{(n)}(t) t \in(a, b) \alpha, \beta [a, b] 
P(t)=\sum_{k=0}^{n-1} \frac{f^{(k)}(\alpha)}{k !}(t-\alpha)^{k} .
 x \alpha \beta 
f(\beta)=P(\beta)+\frac{f^{(n)}(x)}{n !}(\beta-\alpha)^{n} .
 \frac{f^{(n)}(x)}{n!} g C^\infty","['real-analysis', 'calculus', 'analysis', 'manifolds']"
27,A kind of pertubation of laplacian is an Fredholm operator,A kind of pertubation of laplacian is an Fredholm operator,,"Let $U, V$ be Banach spaces and $L(U,V)$ be the space of bounded linear operators from $U$ to $V$ . An operator $T \in L(U,V)$ is said to be a Fredholm operator if dim $N(T) < +\infty$ and $\mathrm{codim} R(T) < +\infty$ . In this case, we define the index of a Fredholm operator $T$ as the integer $$ \mathrm{ind}(T) = \mathrm{dim} N(T) - \mathrm{codim} R(T). $$ Denote by $\mathrm{Fred}_0 (U,V)$ the set of the Fredholm operators with index zero from $U$ to $V$ . Given $\lambda \in \mathbb{R}$ , consider the operator $$T := -\Delta - \lambda I : C^{2}(\Omega) \rightarrow C(\overline{\Omega})$$ I'm trying to prove that $T$ is a Fredholm operator with $\mathrm{ind}(T) = 0$ , for all $\lambda$ . I already know that the operator $-\Delta : C^{2}(\Omega) \rightarrow C(\overline{\Omega})$ is in $\mathrm{Fred}_0(C^{2}, C)$ , since its kernel is trivial by maximum principle and is surjective by Lax Milgram theorem and regularity theory. In addition, I know that $\mathrm{Fred}_0(U,V)$ is an open subset of $L(U,V)$ . So, as $-\Delta \in \mathrm{Fred}_0(C^{2}, C)$ , for small values of $\lambda$ , $-\Delta - \lambda I$ is in a ball centered in $-\Delta$ which is contained in $\mathrm{Fred}_0(C^{2}, C)$ . I don't know if there's another justification to conclude this with arbitrary values of $\lambda$ . I would appreciate any help.","Let be Banach spaces and be the space of bounded linear operators from to . An operator is said to be a Fredholm operator if dim and . In this case, we define the index of a Fredholm operator as the integer Denote by the set of the Fredholm operators with index zero from to . Given , consider the operator I'm trying to prove that is a Fredholm operator with , for all . I already know that the operator is in , since its kernel is trivial by maximum principle and is surjective by Lax Milgram theorem and regularity theory. In addition, I know that is an open subset of . So, as , for small values of , is in a ball centered in which is contained in . I don't know if there's another justification to conclude this with arbitrary values of . I would appreciate any help.","U, V L(U,V) U V T \in L(U,V) N(T) < +\infty \mathrm{codim} R(T) < +\infty T 
\mathrm{ind}(T) = \mathrm{dim} N(T) - \mathrm{codim} R(T).
 \mathrm{Fred}_0 (U,V) U V \lambda \in \mathbb{R} T := -\Delta - \lambda I : C^{2}(\Omega) \rightarrow C(\overline{\Omega}) T \mathrm{ind}(T) = 0 \lambda -\Delta : C^{2}(\Omega) \rightarrow C(\overline{\Omega}) \mathrm{Fred}_0(C^{2}, C) \mathrm{Fred}_0(U,V) L(U,V) -\Delta \in \mathrm{Fred}_0(C^{2}, C) \lambda -\Delta - \lambda I -\Delta \mathrm{Fred}_0(C^{2}, C) \lambda","['functional-analysis', 'analysis', 'partial-differential-equations', 'bifurcation', 'variational-analysis']"
28,Intersection points of two exponential decay functions,Intersection points of two exponential decay functions,,"I have not found much online regarding this question. It's by no means my area of research, but is a questioning I came across when writing a program. If I have two exponential decay functions defined on the same interval $[0,1]$ , how many intersection points can they have on this interval? The two functions can be defined in the form (with $a,b,c,a',b',c' > 0$ ): \begin{align} f(x) = a + b\exp(-c x), && g(x) = a' + b'\exp(-c' x). \end{align} Assuming $f(x)$ and $g(x)$ are not equal, I seem to either find $0$ , $1$ or $2$ intersection points. Would you have suggestions on how to approach this problem? Thank you!","I have not found much online regarding this question. It's by no means my area of research, but is a questioning I came across when writing a program. If I have two exponential decay functions defined on the same interval , how many intersection points can they have on this interval? The two functions can be defined in the form (with ): Assuming and are not equal, I seem to either find , or intersection points. Would you have suggestions on how to approach this problem? Thank you!","[0,1] a,b,c,a',b',c' > 0 \begin{align}
f(x) = a + b\exp(-c x), && g(x) = a' + b'\exp(-c' x).
\end{align} f(x) g(x) 0 1 2","['analysis', 'functions', 'exponential-function']"
29,Can differentiability classes be extended to negative values?,Can differentiability classes be extended to negative values?,,"A function $f$ is said to be of differentiability class $C^k$ if its first $k$ derivatives are continuous. It has the property that $m>n \implies C^m \subset C^n$ and that all $C^k$ s are algebras. My question is this: Seeing as many continuous functions are the integrals of discontinuous functions, can we define $C^k; k<0$ to be the set of functions whose $k^{th}$ integral is continuous? As the integral of a continuous function is continuous, $C^0 \subset C^k;k<0$ and it’s pretty easy to check that this is a vector space, but is it an algebra? Another question is if there exists a $C^{-\infty}$ as the intersection of all $C^k;k<0$ analogously to how $C^{\infty}$ is defined. If this set exists, is it still an algebra and what would it look like? Seeing as $C^k$ gets broader as $k$ gets smaller, $C^{-\infty}$ is actually just the limit as $k$ approaches $-\infty$ . Another thing to note would be what functions aren’t in $C^{-\infty}$ and the other $C^k$ s.","A function is said to be of differentiability class if its first derivatives are continuous. It has the property that and that all s are algebras. My question is this: Seeing as many continuous functions are the integrals of discontinuous functions, can we define to be the set of functions whose integral is continuous? As the integral of a continuous function is continuous, and it’s pretty easy to check that this is a vector space, but is it an algebra? Another question is if there exists a as the intersection of all analogously to how is defined. If this set exists, is it still an algebra and what would it look like? Seeing as gets broader as gets smaller, is actually just the limit as approaches . Another thing to note would be what functions aren’t in and the other s.",f C^k k m>n \implies C^m \subset C^n C^k C^k; k<0 k^{th} C^0 \subset C^k;k<0 C^{-\infty} C^k;k<0 C^{\infty} C^k k C^{-\infty} k -\infty C^{-\infty} C^k,"['integration', 'analysis', 'functions', 'indefinite-integrals', 'distribution-theory']"
30,"$\lim_{r\to\infty} \int \cos^2(rz + t)\, d\pi_{\#}\mu(z) = \frac12$ for all $t\in \Bbb R$",for all,"\lim_{r\to\infty} \int \cos^2(rz + t)\, d\pi_{\#}\mu(z) = \frac12 t\in \Bbb R","This question stems from the identity listed as Equation $(6.6)$ in this paper on Pg. $11$ . We want to show $$\color{blue}{\lim_{r\to\infty} \int \cos^2(rz + t)\, d\pi_{\#}\mu(z) = \frac12} \tag{6.6}$$ for all $t\in \Bbb R$ . Here, $\mu$ is a Borel probability measure on the cone $$\mathcal C^d = \{(x_1, \ldots, x_{d+1}): |(x_1, \ldots, x_d)| = |x_{d+1}|\} \subset \Bbb R^{d+1}$$ satisfying $$|\hat\mu(\xi)| \le |\xi|^{-\alpha/2} \quad\quad  \forall \xi\in \Bbb R^{d+1} \tag{6.1}$$ for some $\alpha > d-1$ . Further, we assume $\operatorname{supp} \mu \subset \{(\xi, |\xi|) \in \Bbb R^d\times \Bbb R: \epsilon \le |\xi| \le 1/\epsilon\}$ for some $\epsilon > 0$ . Let $\pi:\Bbb R^{d+1} \to \Bbb R$ be the projection map onto the last coordinate; i.e. $\pi: (x_1, \ldots, x_{d+1}) \mapsto x_{d+1}$ . The paper suggests: Since $d\ge 2$ and $\alpha/2 > (d-1)/2 \ge 1/2$ , $(6.1)$ gives $\pi_{\#}\mu\in L^2(\Bbb R)$ . Hence, $\pi_{\#}\mu \in L^1(\Bbb R)$ with $\|\pi_{\#}\mu\|_1 = 1$ , and $(6.6)$ then follows by approximating $\pi_{\#}\mu$ in $L^1$ with a finite linear combination of characteristic functions of disjoint intervals. I tried to follow the line of thought in the paper to first show that $\pi_{\#}\mu \in L^2(\Bbb R)$ . If we are to use Theorem $3.3$ of Mattila's Fourier Analysis and Hausdorff Dimension , we shall have to show that $\pi_{\#}\mu\in \mathcal M(\Bbb R)$ and $\widehat{\pi_{\#}\mu}\in L^2(\Bbb R)$ . I have \begin{align*} \widehat{\pi_{\#}\mu}(\xi) &= \int_{\Bbb R} e^{-2\pi i\xi z} \, d\pi_{\#} \mu(z) \\ &= \int_{\Bbb R^d\times\Bbb R} e^{-2\pi i\xi z}\, d\mu(x,z)\\ &= \widehat{\mu}(0, \ldots, 0, \xi) \end{align*} So, $$\int_{\Bbb R} \left| \widehat{\pi_{\#}\mu}(\xi) \right|^2 \, d\xi = \int_{\Bbb R} |\widehat{\mu}(0, \ldots, 0, \xi)|^2\, d\xi \lesssim \int_{\Bbb R} |\xi|^{-\alpha}\, d\xi = 2\int_0^\infty \xi^{-\alpha}\, d\xi$$ Is $\int_0^\infty |\xi|^{-\alpha}\, d\xi < \infty$ ? Note that $\alpha \ge 1$ . As suggested by @J.G. in the comments, it's enough to show $$\lim_{r\to\infty} \int e^{2irz}\, d\pi_{\#}\mu(z) = \lim_{r\to\infty}\widehat{\pi_{\#}\mu}\left(-\frac{r}{\pi}\right) = 0$$ Notation. $\mathcal M(\Bbb R^n)$ is the set of all Borel measures $\nu$ satisfying $0 < \nu(\Bbb R^n) < \infty$ with compact support $\operatorname{supp} \nu \subset \Bbb R^n$ .","This question stems from the identity listed as Equation in this paper on Pg. . We want to show for all . Here, is a Borel probability measure on the cone satisfying for some . Further, we assume for some . Let be the projection map onto the last coordinate; i.e. . The paper suggests: Since and , gives . Hence, with , and then follows by approximating in with a finite linear combination of characteristic functions of disjoint intervals. I tried to follow the line of thought in the paper to first show that . If we are to use Theorem of Mattila's Fourier Analysis and Hausdorff Dimension , we shall have to show that and . I have So, Is ? Note that . As suggested by @J.G. in the comments, it's enough to show Notation. is the set of all Borel measures satisfying with compact support .","(6.6) 11 \color{blue}{\lim_{r\to\infty} \int \cos^2(rz + t)\, d\pi_{\#}\mu(z) = \frac12} \tag{6.6} t\in \Bbb R \mu \mathcal C^d = \{(x_1, \ldots, x_{d+1}): |(x_1, \ldots, x_d)| = |x_{d+1}|\} \subset \Bbb R^{d+1} |\hat\mu(\xi)| \le |\xi|^{-\alpha/2} \quad\quad  \forall \xi\in \Bbb R^{d+1} \tag{6.1} \alpha > d-1 \operatorname{supp} \mu \subset \{(\xi, |\xi|) \in \Bbb R^d\times \Bbb R: \epsilon \le |\xi| \le 1/\epsilon\} \epsilon > 0 \pi:\Bbb R^{d+1} \to \Bbb R \pi: (x_1, \ldots, x_{d+1}) \mapsto x_{d+1} d\ge 2 \alpha/2 > (d-1)/2 \ge 1/2 (6.1) \pi_{\#}\mu\in L^2(\Bbb R) \pi_{\#}\mu \in L^1(\Bbb R) \|\pi_{\#}\mu\|_1 = 1 (6.6) \pi_{\#}\mu L^1 \pi_{\#}\mu \in L^2(\Bbb R) 3.3 \pi_{\#}\mu\in \mathcal M(\Bbb R) \widehat{\pi_{\#}\mu}\in L^2(\Bbb R) \begin{align*}
\widehat{\pi_{\#}\mu}(\xi) &= \int_{\Bbb R} e^{-2\pi i\xi z} \, d\pi_{\#} \mu(z) \\ &= \int_{\Bbb R^d\times\Bbb R} e^{-2\pi i\xi z}\, d\mu(x,z)\\
&= \widehat{\mu}(0, \ldots, 0, \xi)
\end{align*} \int_{\Bbb R} \left| \widehat{\pi_{\#}\mu}(\xi) \right|^2 \, d\xi = \int_{\Bbb R} |\widehat{\mu}(0, \ldots, 0, \xi)|^2\, d\xi \lesssim \int_{\Bbb R} |\xi|^{-\alpha}\, d\xi = 2\int_0^\infty \xi^{-\alpha}\, d\xi \int_0^\infty |\xi|^{-\alpha}\, d\xi < \infty \alpha \ge 1 \lim_{r\to\infty} \int e^{2irz}\, d\pi_{\#}\mu(z) = \lim_{r\to\infty}\widehat{\pi_{\#}\mu}\left(-\frac{r}{\pi}\right) = 0 \mathcal M(\Bbb R^n) \nu 0 < \nu(\Bbb R^n) < \infty \operatorname{supp} \nu \subset \Bbb R^n","['limits', 'analysis', 'measure-theory', 'proof-explanation', 'fourier-analysis']"
31,Product of distributions under wavefront set condition is zero,Product of distributions under wavefront set condition is zero,,"Assume $u, v \in \mathcal{D}'(\mathbb{R}^n)$ are distributions with compact support. Denote by $\operatorname{WF}(\bullet) \subset T^*\mathbb{R}^n \setminus 0$ the wavefront set of a distribution $\bullet$ . If $\operatorname{WF}(u) \cap \operatorname{WF}(v) = \emptyset$ , then their product $uv$ is well defined. If $uv = 0$ , does this imply that at an open and dense set of points one of the two distributions vanish, that is, $\operatorname{supp}(u)^c \cup \operatorname{supp}(v)^c \subset \mathbb{R}^n$ is open and dense? For the background on the wavefront set and distribution theory, see Chapter 8 of Hörmander's book The Analysis of Linear Partial Differential Operators I . For products under the wavefront set condition, see Theorem 8.2.10 of the same book. I've also asked this question on Mathematics Overflow . Remarks: As noted below by Vinicius in the comments, if $u, v \in C^\infty_c(\mathbb{R})$ with $\operatorname{supp}(u) = [-1, 0]$ and $\operatorname{supp}(v) = [0, 1]$ , then $uv = 0$ but $\operatorname{supp}(u)^c \cup \operatorname{supp}(v)^c = \mathbb{R}\setminus 0$ , so open and dense is the most we can hope for. The same question makes sense also if $u$ and $v$ don't have compact support. In that case, if for simplicity we set $n = 2$ and there are transversal smooth vector fields $X$ and $Y$ such that $X u = 0$ and $Y v = 0$ , then the wavefront set condition is automatically satisfied. In fact, in a suitable local coordinate system adapted to $X$ and $Y$ , one can show that $uv$ is locally a tensor product and so at each point either $u = 0$ or $v = 0$ . One should keep in mind the trivial case when $X = \partial_{x_1}$ and $Y = \partial_{x_2}$ .","Assume are distributions with compact support. Denote by the wavefront set of a distribution . If , then their product is well defined. If , does this imply that at an open and dense set of points one of the two distributions vanish, that is, is open and dense? For the background on the wavefront set and distribution theory, see Chapter 8 of Hörmander's book The Analysis of Linear Partial Differential Operators I . For products under the wavefront set condition, see Theorem 8.2.10 of the same book. I've also asked this question on Mathematics Overflow . Remarks: As noted below by Vinicius in the comments, if with and , then but , so open and dense is the most we can hope for. The same question makes sense also if and don't have compact support. In that case, if for simplicity we set and there are transversal smooth vector fields and such that and , then the wavefront set condition is automatically satisfied. In fact, in a suitable local coordinate system adapted to and , one can show that is locally a tensor product and so at each point either or . One should keep in mind the trivial case when and .","u, v \in \mathcal{D}'(\mathbb{R}^n) \operatorname{WF}(\bullet) \subset T^*\mathbb{R}^n \setminus 0 \bullet \operatorname{WF}(u) \cap \operatorname{WF}(v) = \emptyset uv uv = 0 \operatorname{supp}(u)^c \cup \operatorname{supp}(v)^c \subset \mathbb{R}^n u, v \in C^\infty_c(\mathbb{R}) \operatorname{supp}(u) = [-1, 0] \operatorname{supp}(v) = [0, 1] uv = 0 \operatorname{supp}(u)^c \cup \operatorname{supp}(v)^c = \mathbb{R}\setminus 0 u v n = 2 X Y X u = 0 Y v = 0 X Y uv u = 0 v = 0 X = \partial_{x_1} Y = \partial_{x_2}","['analysis', 'distribution-theory', 'microlocal-analysis']"
32,Why the definition of smooth boundary defined like this and how does it imply the interior ball property?,Why the definition of smooth boundary defined like this and how does it imply the interior ball property?,,"I am trying to understand what the definition of smooth boundary is. From the following lectures notes on analysis 3 : So intuitively, a smooth boundary ( in this case , it is a $C^1$ boundary) should have no edges and corners. So I think condition $b$ takes care of that. But why do we have condition $a$ ? Also why does having a smooth boundary imply $\Omega$ has the interior ball property? The interior ball property is : for each $x\in \partial \Omega$ , there exists a ball $B \subset \Omega$ such that $x\in \partial B$ . Intuitively, this seems like it should be true, but I don't see how we get from the definition of smooth bondary to this.","I am trying to understand what the definition of smooth boundary is. From the following lectures notes on analysis 3 : So intuitively, a smooth boundary ( in this case , it is a boundary) should have no edges and corners. So I think condition takes care of that. But why do we have condition ? Also why does having a smooth boundary imply has the interior ball property? The interior ball property is : for each , there exists a ball such that . Intuitively, this seems like it should be true, but I don't see how we get from the definition of smooth bondary to this.",C^1 b a \Omega x\in \partial \Omega B \subset \Omega x\in \partial B,"['real-analysis', 'analysis', 'submanifold']"
33,Find a sequence of functions that converge weakly to 1/2,Find a sequence of functions that converge weakly to 1/2,,"I'm trying solve the following problem: Find an example of a sequence of functions $f_n:[0,1]\to[0,1]$ such that $$(f_n)_\#\mathscr{L} = \mathscr L$$ but $f_n\rightharpoonup 1/2$ (weak convergence). Note that $(f_n)_\#(\mathscr L)$ denotes the push-forward image measure, i.e. $(f_n)_\#\mathscr L = \mathscr L(f_n^{-1})$ and $\mathscr L$ is the Lebesgue measure (restricted to $[0,1]$ ). Unfortunately, the problem doesn't have any more details other than this. My professor helped me figure out to consider the sequence $$ f_n(x) = \sum_{j=0}^{2^{n-1}-1}2^{n-1}\left(x - \frac{j}{2^{n-1}}\right)\chi_{[j/2^{n-1},(j+1)/2^{n-1})} \qquad \text{and}\qquad f_n(1) = 1. $$ and I've proved the equivalence of the pushforward of the Lebesgue measure and the Lebesgue measure, but I'm having a lot of trouble proving the weak convergence to 1/2. My professor just said to prove that if $f\in C([0,1])$ , then I need to show that $$\left< f_n,f\right> \to \left<1/2,f\right>$$ where $\left<\cdot,\cdot\right>$ is the $L^2$ inner product. Some immediate concerns I had were why would we assume that this is the weak convergence we want to use and also why do we assume $f$ is continuous. It seems the problem statement is missing details about the setting we are in. My other issue is that I'm just stuck on how to prove this limit. My work so far consists of $$\begin{align*}         \left|\left< \frac{1}{2}-f_n,f\right> \right| &= \left|\int_0^1 \left(\frac{1}{2}-f_n(x)\right) f(x)dx\right|\\         &= \left|\int_0^1f(x) \left(\sum_{j=0}^{2^{n-1}-1}\left(\frac{1}{2}-2^{n-1}\left( x- \frac{j}{2^{n-1}}\right)\right)\right)\chi_{\left[\frac{j}{2^{n-1}},\frac{j+1}{2^{n-1}}\right)}dx\right|\\        &=   \left|\int_0^1f(x) \sum_{j=0}^{2^{n-1}-1}\left(\frac{1}{2}- 2^{n-1}x + j\right)\chi_{\left[\frac{j}{2^{n-1}},\frac{j+1}{2^{n-1}}\right)} dx\right|\\        &= \left| \sum_{j=0}^{2^{n-1}-1} \int_{\frac{j}{2^{n-1}}}^{\frac{j+1}{2^{n-1}}}f(x) \left(\frac{1}{2}- 2^{n-1}x + j\right) dx\right|\\        &= \left|\sum_{j=0}^{2^{n-1}-1} \int_0^{\frac{1}{2^{n-1}}} f\left(\frac{j+1}{2^{n-1}}-x\right) \left(2^{n-1}x-\frac{1}{2}\right) dx\right| \tag{by substitution}     \end{align*}$$ I've tried bringing the absolute values in and pulling out the sup of $f$ , but it feels like all the things I've tried don't actually make the total quantity small, which makes me doubt that my original sequence was correct or perhaps this norm isn't correct, but if that's the case, I'm not sure what to do. If anyone has any suggestions, I'll greatly appreciate it.","I'm trying solve the following problem: Find an example of a sequence of functions such that but (weak convergence). Note that denotes the push-forward image measure, i.e. and is the Lebesgue measure (restricted to ). Unfortunately, the problem doesn't have any more details other than this. My professor helped me figure out to consider the sequence and I've proved the equivalence of the pushforward of the Lebesgue measure and the Lebesgue measure, but I'm having a lot of trouble proving the weak convergence to 1/2. My professor just said to prove that if , then I need to show that where is the inner product. Some immediate concerns I had were why would we assume that this is the weak convergence we want to use and also why do we assume is continuous. It seems the problem statement is missing details about the setting we are in. My other issue is that I'm just stuck on how to prove this limit. My work so far consists of I've tried bringing the absolute values in and pulling out the sup of , but it feels like all the things I've tried don't actually make the total quantity small, which makes me doubt that my original sequence was correct or perhaps this norm isn't correct, but if that's the case, I'm not sure what to do. If anyone has any suggestions, I'll greatly appreciate it.","f_n:[0,1]\to[0,1] (f_n)_\#\mathscr{L} = \mathscr L f_n\rightharpoonup 1/2 (f_n)_\#(\mathscr L) (f_n)_\#\mathscr L = \mathscr L(f_n^{-1}) \mathscr L [0,1] 
f_n(x) = \sum_{j=0}^{2^{n-1}-1}2^{n-1}\left(x - \frac{j}{2^{n-1}}\right)\chi_{[j/2^{n-1},(j+1)/2^{n-1})} \qquad \text{and}\qquad f_n(1) = 1.
 f\in C([0,1]) \left< f_n,f\right> \to \left<1/2,f\right> \left<\cdot,\cdot\right> L^2 f \begin{align*}
        \left|\left< \frac{1}{2}-f_n,f\right> \right| &= \left|\int_0^1 \left(\frac{1}{2}-f_n(x)\right) f(x)dx\right|\\
        &= \left|\int_0^1f(x) \left(\sum_{j=0}^{2^{n-1}-1}\left(\frac{1}{2}-2^{n-1}\left( x- \frac{j}{2^{n-1}}\right)\right)\right)\chi_{\left[\frac{j}{2^{n-1}},\frac{j+1}{2^{n-1}}\right)}dx\right|\\
       &=   \left|\int_0^1f(x) \sum_{j=0}^{2^{n-1}-1}\left(\frac{1}{2}- 2^{n-1}x + j\right)\chi_{\left[\frac{j}{2^{n-1}},\frac{j+1}{2^{n-1}}\right)} dx\right|\\
       &= \left| \sum_{j=0}^{2^{n-1}-1} \int_{\frac{j}{2^{n-1}}}^{\frac{j+1}{2^{n-1}}}f(x) \left(\frac{1}{2}- 2^{n-1}x + j\right) dx\right|\\
       &= \left|\sum_{j=0}^{2^{n-1}-1} \int_0^{\frac{1}{2^{n-1}}} f\left(\frac{j+1}{2^{n-1}}-x\right) \left(2^{n-1}x-\frac{1}{2}\right) dx\right| \tag{by substitution}
    \end{align*} f","['functional-analysis', 'analysis', 'weak-convergence']"
34,"Is there a nonlinear smooth function $f:\mathbb R\to\mathbb R$ such that $f(X) + X \sim N(0,1)$ when $X\sim N(0,1)$?",Is there a nonlinear smooth function  such that  when ?,"f:\mathbb R\to\mathbb R f(X) + X \sim N(0,1) X\sim N(0,1)","Suppose $X\sim N(0,1)$ . Is there a nonlinear smooth function $f:\mathbb R\to\mathbb R$ such that $f(X) + X \sim N(0,1)$ ? Here is my attempt. I consider the moment generating function of $X+f(X)$ : \begin{equation} \begin{split} \mathbb{E} e^{t(X+f(X))} = \int_{\mathbb{R}} \frac{1}{\sqrt{2\pi}} e^{t x + tf(x)} e^{-x^2/2} dx = e^{t^2/2}, \end{split} \end{equation} which can be further simplified as $$ \int_{\mathbb R} \frac{1}{\sqrt{2\pi}} e^{tf(x)} e^{-(x-t)^2/2} dx = 1. $$ This must hold for any $t\in\mathbb R$ . But I don't know how to deal with this integral equation. Is there any suggestion on this equation or an alternative approach? Thank you! Update : @Cretin2 Thanks for the suggestion. The density of $Y=f(X) + X$ is $$p_Y(y) = \sum_{x:f(x)+x=y} \frac{p_X(x)}{|f'(x) + 1|},$$ which is reduced to $$ \sum_{x:f(x)+x=y} \frac{e^{f(x)(y-f(x)/2)}}{|f'(x) + 1|} = 1,$$ for any $y\in\mathbb R$ . If $f(x)+x$ is not monotone, then there is some point $x_0 < x_1$ such that $f'(x_0) + 1= 0$ and $h:(x_0,x_1)\to\mathbb R, x\mapsto f(x)+x$ is injective. As $y\to h(x_0)$ , $$\frac{e^{f(h^{-1}(y)) (y - f( h^{-1}(y) )/2) }}{|f'(h^{-1}(y)) + 1|} \to \infty,$$ a contradiction. Then $f(x)+x$ must be monotone and $f$ must be linear. There is no such nonlinear transformation. I find https://en.wikibooks.org/wiki/Probability/Transformation_of_Probability_Densities useful (for density after non-injective transformation).","Suppose . Is there a nonlinear smooth function such that ? Here is my attempt. I consider the moment generating function of : which can be further simplified as This must hold for any . But I don't know how to deal with this integral equation. Is there any suggestion on this equation or an alternative approach? Thank you! Update : @Cretin2 Thanks for the suggestion. The density of is which is reduced to for any . If is not monotone, then there is some point such that and is injective. As , a contradiction. Then must be monotone and must be linear. There is no such nonlinear transformation. I find https://en.wikibooks.org/wiki/Probability/Transformation_of_Probability_Densities useful (for density after non-injective transformation).","X\sim N(0,1) f:\mathbb R\to\mathbb R f(X) + X \sim N(0,1) X+f(X) \begin{equation}
\begin{split}
\mathbb{E} e^{t(X+f(X))} = \int_{\mathbb{R}} \frac{1}{\sqrt{2\pi}} e^{t x + tf(x)} e^{-x^2/2} dx = e^{t^2/2},
\end{split}
\end{equation} 
\int_{\mathbb R} \frac{1}{\sqrt{2\pi}} e^{tf(x)} e^{-(x-t)^2/2} dx = 1.
 t\in\mathbb R Y=f(X) + X p_Y(y) = \sum_{x:f(x)+x=y} \frac{p_X(x)}{|f'(x) + 1|},  \sum_{x:f(x)+x=y} \frac{e^{f(x)(y-f(x)/2)}}{|f'(x) + 1|} = 1, y\in\mathbb R f(x)+x x_0 < x_1 f'(x_0) + 1= 0 h:(x_0,x_1)\to\mathbb R, x\mapsto f(x)+x y\to h(x_0) \frac{e^{f(h^{-1}(y)) (y - f( h^{-1}(y) )/2) }}{|f'(h^{-1}(y)) + 1|} \to \infty, f(x)+x f","['probability', 'analysis', 'integral-equations']"
35,On the continuity of the funciton $w \mapsto \mu(\{x \in X \mid w^\top v(x) > t\})$,On the continuity of the funciton,w \mapsto \mu(\{x \in X \mid w^\top v(x) > t\}),"Let $X$ be a measurable space and let $\mu$ be a probability measure on $X$ (assumed to be non-atomic , in case that helps). Let $v:X \to \mathbb R^k$ be a $\mu$ -integrable function and for every $t \in [0,b]$ (with $b>0$ fixed), consider the function $G_t:\mathbb R^k \to [0,1]$ defined by $$ G_t(w):= \mu(\{x \in X \mid w^\top v(x) > t\}). $$ Question. Under what minimal conditions on $\mu$ and $v$ are the functions $G_t$ continuous for (almost) all $t \in [0,b]$ ?","Let be a measurable space and let be a probability measure on (assumed to be non-atomic , in case that helps). Let be a -integrable function and for every (with fixed), consider the function defined by Question. Under what minimal conditions on and are the functions continuous for (almost) all ?","X \mu X v:X \to \mathbb R^k \mu t \in [0,b] b>0 G_t:\mathbb R^k \to [0,1] 
G_t(w):= \mu(\{x \in X \mid w^\top v(x) > t\}).
 \mu v G_t t \in [0,b]","['functional-analysis', 'analysis', 'measure-theory', 'set-valued-analysis']"
36,Transitivity of pointwise convergence?,Transitivity of pointwise convergence?,,"I want to know if the following statement is true or not: Here we consider functions from $\mathbb{R}$ to $\mathbb{R}$ . We are given a function $f$ , families of functions $\mathcal{F}$ and $\mathcal{G}$ with the following properties: (1) There is a sequence of functions $\{ \varphi_k \}$ in $\mathcal{F}$ pointwise convergent to $f$ . (2) For each function $\varphi \in \mathcal{F}$ , there is a sequence of functions $\{ \psi_l \}$ in $\mathcal{G}$ that converges pointwise to $\varphi$ almost everywhere. Then we can find a sequence of functions in $\mathcal{G}$ that converges pointwise to $f$ almost everywhere. In particular, if $f$ is any measurable function, $\mathcal{F}$ is the set of all simple functions and $\mathcal{G}$ is the set of all step functions then this is true. I believe it is generally false but I couldn't come up with a counter example. Initially, my question was the same as Measurable Functions as Limits (a.e) of Step Functions , which deals with the special case I described. I read both the proof in the book by Stein-Shakarchi and the answer of the question, but they rely on something stronger: (3) For each function $\varphi \in \mathcal{F}$ and $\epsilon > 0$ , we can find $\psi \in \mathcal{G}$ which agrees with $\varphi$ except for a set $E$ with $m(E) < \epsilon$ . Edit: I'm asking this question because the book starts the proof by saying: it suffices to show that every simple functions can be approximated by step functions and concludes: we've found a sequence of step functions that converges pointwise to given simple function almost everywhere. This seems to me, is using the general statement I wrote above even though we had a stronger condition (3) in the middle of the proof.","I want to know if the following statement is true or not: Here we consider functions from to . We are given a function , families of functions and with the following properties: (1) There is a sequence of functions in pointwise convergent to . (2) For each function , there is a sequence of functions in that converges pointwise to almost everywhere. Then we can find a sequence of functions in that converges pointwise to almost everywhere. In particular, if is any measurable function, is the set of all simple functions and is the set of all step functions then this is true. I believe it is generally false but I couldn't come up with a counter example. Initially, my question was the same as Measurable Functions as Limits (a.e) of Step Functions , which deals with the special case I described. I read both the proof in the book by Stein-Shakarchi and the answer of the question, but they rely on something stronger: (3) For each function and , we can find which agrees with except for a set with . Edit: I'm asking this question because the book starts the proof by saying: it suffices to show that every simple functions can be approximated by step functions and concludes: we've found a sequence of step functions that converges pointwise to given simple function almost everywhere. This seems to me, is using the general statement I wrote above even though we had a stronger condition (3) in the middle of the proof.",\mathbb{R} \mathbb{R} f \mathcal{F} \mathcal{G} \{ \varphi_k \} \mathcal{F} f \varphi \in \mathcal{F} \{ \psi_l \} \mathcal{G} \varphi \mathcal{G} f f \mathcal{F} \mathcal{G} \varphi \in \mathcal{F} \epsilon > 0 \psi \in \mathcal{G} \varphi E m(E) < \epsilon,"['real-analysis', 'analysis']"
37,Reference needed: Symbol sequence for pseudodifferential operators,Reference needed: Symbol sequence for pseudodifferential operators,,"In Higsons's book Analytic K-Homology there is a section (subsection (b) in 2.8 ""Geometric Examples of Extensions"", starting from page 46) which discusses the following exact sequence called pseudodifferential operator extension (sometimes referred to as symbol sequence ): $0 \longrightarrow \mathcal{K}(L^2(M)) \longrightarrow \Psi^{0}_{phg}(M) \xrightarrow{\sigma_0} C_0(S^*M) \longrightarrow 0$ where $M$ is a smooth compact manifold without boundary, $S^*M$ the unit cosphere bundle, $\mathcal{K}(L^2(M))$ denotes the compact operators on $L^2(M)$ , $\Psi^{0}_{phg}(M)$ are the order $0$ classical pseudodifferential operators and $\sigma_0$ is the principal symbol map. I've seen this statement quite often but always without proof. Could someone give me a reference for this? Also, it is known that (by Rellich) every negative order pseudodifferential operator extends to a compact operator $L^2(M) \to L^2(M)$ . In particular, $\Psi^{-1}_{phg}(M) \subseteq \mathcal{K}(L^2(M))$ . How do we prove that $\overline{\Psi^{-1}_{phg}(M)} = \mathcal{K}(L^2(M))$ where on the left side we have the norm closure?","In Higsons's book Analytic K-Homology there is a section (subsection (b) in 2.8 ""Geometric Examples of Extensions"", starting from page 46) which discusses the following exact sequence called pseudodifferential operator extension (sometimes referred to as symbol sequence ): where is a smooth compact manifold without boundary, the unit cosphere bundle, denotes the compact operators on , are the order classical pseudodifferential operators and is the principal symbol map. I've seen this statement quite often but always without proof. Could someone give me a reference for this? Also, it is known that (by Rellich) every negative order pseudodifferential operator extends to a compact operator . In particular, . How do we prove that where on the left side we have the norm closure?",0 \longrightarrow \mathcal{K}(L^2(M)) \longrightarrow \Psi^{0}_{phg}(M) \xrightarrow{\sigma_0} C_0(S^*M) \longrightarrow 0 M S^*M \mathcal{K}(L^2(M)) L^2(M) \Psi^{0}_{phg}(M) 0 \sigma_0 L^2(M) \to L^2(M) \Psi^{-1}_{phg}(M) \subseteq \mathcal{K}(L^2(M)) \overline{\Psi^{-1}_{phg}(M)} = \mathcal{K}(L^2(M)),"['functional-analysis', 'analysis', 'smooth-manifolds', 'singular-integrals', 'pseudo-differential-operators']"
38,Is it worth learning measure theory and Lebesgue integration using the Daniell scheme and Shilov's book for a first timer self-studying?,Is it worth learning measure theory and Lebesgue integration using the Daniell scheme and Shilov's book for a first timer self-studying?,,"I'm an undergraduate with somewhat little analysis background (one undergraduate course a year ago and a graduate complex analysis course this semester) who wishes to self-study advanced real analysis (measure theory, Lebesgue integration, etc.). This is most immediately so that I can take a Fourier analysis course next semester (textbook: Fourier Analysis by Duoandikoetxea, if anyone is familar), but I also just because I hope to get a deep and meaningful understanding of the topic for its own purposes. I've been reading Integral, Measure, and Derivative: a Unified Approach by Shilov and was wondering if anyone could comment on its ultimate utility as a first introduction to the topic. After a few days of reading, I've finished chapter 3 ( n -space Lebesgue integration) and feel I understand everything presented in the book well enough, but any time I look at an outside source I become completely lost. The Daniell scheme definitions rarely match with what nearly everyone else uses, and while I'm sure Shilov will eventually prove them equivalent, I can't help but feel like I'm making everything harder for myself by using this book. For example, when I think of a measurable function on a certain set, I think of a function which is the almost-everywhere limit of a sequence of some of the elementary functions which define integration on that set, but when I Google it, I'm hit with some measure theory-based definition that is completely new to me. Is the generality of the Daniell scheme worth being this (temporarily) isolated or should I look to switch to another text and only later come back to Shilov? I own but have not read Kolmogorov's Elements of the Theory of Functions and Functional Analysis , if anyone has an opinion to share on that. I was also thinking of potentially buying Stein's Princeton Lectures in Analysis edition. Any other suggestions/reviews would be appreciated, too.","I'm an undergraduate with somewhat little analysis background (one undergraduate course a year ago and a graduate complex analysis course this semester) who wishes to self-study advanced real analysis (measure theory, Lebesgue integration, etc.). This is most immediately so that I can take a Fourier analysis course next semester (textbook: Fourier Analysis by Duoandikoetxea, if anyone is familar), but I also just because I hope to get a deep and meaningful understanding of the topic for its own purposes. I've been reading Integral, Measure, and Derivative: a Unified Approach by Shilov and was wondering if anyone could comment on its ultimate utility as a first introduction to the topic. After a few days of reading, I've finished chapter 3 ( n -space Lebesgue integration) and feel I understand everything presented in the book well enough, but any time I look at an outside source I become completely lost. The Daniell scheme definitions rarely match with what nearly everyone else uses, and while I'm sure Shilov will eventually prove them equivalent, I can't help but feel like I'm making everything harder for myself by using this book. For example, when I think of a measurable function on a certain set, I think of a function which is the almost-everywhere limit of a sequence of some of the elementary functions which define integration on that set, but when I Google it, I'm hit with some measure theory-based definition that is completely new to me. Is the generality of the Daniell scheme worth being this (temporarily) isolated or should I look to switch to another text and only later come back to Shilov? I own but have not read Kolmogorov's Elements of the Theory of Functions and Functional Analysis , if anyone has an opinion to share on that. I was also thinking of potentially buying Stein's Princeton Lectures in Analysis edition. Any other suggestions/reviews would be appreciated, too.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'self-learning']"
39,$L_q$ is always dense in $L_p$?,is always dense in ?,L_q L_p,"From Theorem 3.13 on page 69 of Rudin's real and complex analysis, I learned that the set $S$ of all simple functions with finite support is dense in $L_p$ for $1 \leq p<\infty$ . Obviously $S\subset L_q$ for all $q>0$ . Then it seems obvious to conclude that $L_q \cap L_p$ is dense in $L_p$ simply because S is dense in $L_p$ and $S \subset L_q \cap L_p$ . I think the argument is quite simple here, but I couldn't find any resource to confirm this claim. Am I missing something in my argument?","From Theorem 3.13 on page 69 of Rudin's real and complex analysis, I learned that the set of all simple functions with finite support is dense in for . Obviously for all . Then it seems obvious to conclude that is dense in simply because S is dense in and . I think the argument is quite simple here, but I couldn't find any resource to confirm this claim. Am I missing something in my argument?",S L_p 1 \leq p<\infty S\subset L_q q>0 L_q \cap L_p L_p L_p S \subset L_q \cap L_p,"['functional-analysis', 'analysis', 'lp-spaces']"
40,Inequality for Fourier transform and Schwartz function,Inequality for Fourier transform and Schwartz function,,"I have only recently started to study the Fourier transform in Schwartz space: and I've met a problem which I really do not have any idea on how to solve it. The problem is precisely the following: consider $p\in[1,\infty]$ abd $k=2[1+\frac{d}{2}]$ where $d$ is the dimension of the ambient Euclidean space $\mathbb{R}^d$ , and a function $u\in L^p$ . Then I want to find a constant $C>0$ such that $$|\langle\hat{u},\psi\rangle|\leq C\lVert \psi\rVert_{k,\mathcal{S}}\quad\psi\in\mathcal{S}$$ where $\mathcal{S}$ is the Schwartz space and $\hat{u}$ represents the Fourier transform of $u$ . Can somebody help me?","I have only recently started to study the Fourier transform in Schwartz space: and I've met a problem which I really do not have any idea on how to solve it. The problem is precisely the following: consider abd where is the dimension of the ambient Euclidean space , and a function . Then I want to find a constant such that where is the Schwartz space and represents the Fourier transform of . Can somebody help me?","p\in[1,\infty] k=2[1+\frac{d}{2}] d \mathbb{R}^d u\in L^p C>0 |\langle\hat{u},\psi\rangle|\leq C\lVert \psi\rVert_{k,\mathcal{S}}\quad\psi\in\mathcal{S} \mathcal{S} \hat{u} u","['functional-analysis', 'analysis']"
41,"Best-fitting function subspaces in $L^2[-1,1]$",Best-fitting function subspaces in,"L^2[-1,1]","I recently came cross a question related to best-fitting function subspaces as follow. Let $L^2[-1,1]$ be the Hilbert space of real valued square integrable functions on $[-1,1]$ equipped with the norm $\|f\|$ = $\sqrt{\int_{{-1}}^1 {|f(x)|}^2 dx}$ . Obviously, $1,x,x^2,\cdots,x^{n-1}$ are all in $L^2[-1,1]$ for any positive integer $n$ . My question is that for any fixed positive integer $k$ ( $k<n$ ), are there $k$ functions $h_1,h_2,\cdots,h_k$ in $L^2[-1,1]$ that minimize \begin{equation} {\rm dist}\left({\rm span}(h_1,\cdots,h_k);1,x,\cdots,x^{n-1} \right) := \inf_{\alpha_{ij} \in \mathbb{R} :0\leq i \leq n-1, 1\leq j \leq k} \sum_{i=0}^{n-1} \left\| x^i - \sum_{j=1}^k \alpha_{ij} h_j \right\|^2. \end{equation} If there are $k$ such functions, what are their expressions? Indeed, the question is equivalent to find a $k$ -dimension best-fitting function subspaces of $1,x,x^2,\cdots,x^{n-1}$ in $L^2[-1,1]$ . I feel like this question might be related to principal component analysis(PCA), but I don't know how to generalize PCA to $L^2[-1,1]$ . Does anyone have a reference or a solution which answers this question? Thanks in advance.","I recently came cross a question related to best-fitting function subspaces as follow. Let be the Hilbert space of real valued square integrable functions on equipped with the norm = . Obviously, are all in for any positive integer . My question is that for any fixed positive integer ( ), are there functions in that minimize If there are such functions, what are their expressions? Indeed, the question is equivalent to find a -dimension best-fitting function subspaces of in . I feel like this question might be related to principal component analysis(PCA), but I don't know how to generalize PCA to . Does anyone have a reference or a solution which answers this question? Thanks in advance.","L^2[-1,1] [-1,1] \|f\| \sqrt{\int_{{-1}}^1 {|f(x)|}^2 dx} 1,x,x^2,\cdots,x^{n-1} L^2[-1,1] n k k<n k h_1,h_2,\cdots,h_k L^2[-1,1] \begin{equation}
{\rm dist}\left({\rm span}(h_1,\cdots,h_k);1,x,\cdots,x^{n-1} \right) :=
\inf_{\alpha_{ij} \in \mathbb{R} :0\leq i \leq n-1, 1\leq j \leq k} \sum_{i=0}^{n-1} \left\| x^i - \sum_{j=1}^k \alpha_{ij} h_j \right\|^2.
\end{equation} k k 1,x,x^2,\cdots,x^{n-1} L^2[-1,1] L^2[-1,1]","['real-analysis', 'functional-analysis', 'analysis', 'hilbert-spaces', 'approximation-theory']"
42,"The divergence and convergence about integration $ \int^\infty_0 \frac{x^x}{e^{x^\lambda}}\, dx$ when $\lambda>1$",The divergence and convergence about integration  when," \int^\infty_0 \frac{x^x}{e^{x^\lambda}}\, dx \lambda>1","I wonder whether this integration is convergent or divergent when $\lambda>1$ \begin{equation} \displaystyle \int^\infty_0 \dfrac{x^x}{e^{x^\lambda}}\,dx =\displaystyle \int^\infty_0 {x^x}{e^{-x^\lambda}}\,dx  \end{equation} I can prove that when $\lambda\leq1$ the integration diverges, but when $\lambda>1$ it's difficult for me to decide whether the integration diverges or not. I tried to split the integration at $x= 1,e,\lambda$ these places, but it doesn't help. Also, I tried to use limit comparison at $x=\infty$ , but I can't find a suitable function to do that. Then I tried to find whether there exists a function greater or less than the integrand, but it also doesn't seem to be useful. I noticed that $x^x$ and $x^\lambda$ are kind of suspicious, but I also don't know if this is a key observation for obtaining the answer. Thus, any suggestions or help on this? Thanks!","I wonder whether this integration is convergent or divergent when I can prove that when the integration diverges, but when it's difficult for me to decide whether the integration diverges or not. I tried to split the integration at these places, but it doesn't help. Also, I tried to use limit comparison at , but I can't find a suitable function to do that. Then I tried to find whether there exists a function greater or less than the integrand, but it also doesn't seem to be useful. I noticed that and are kind of suspicious, but I also don't know if this is a key observation for obtaining the answer. Thus, any suggestions or help on this? Thanks!","\lambda>1 \begin{equation}
\displaystyle \int^\infty_0 \dfrac{x^x}{e^{x^\lambda}}\,dx =\displaystyle \int^\infty_0 {x^x}{e^{-x^\lambda}}\,dx 
\end{equation} \lambda\leq1 \lambda>1 x= 1,e,\lambda x=\infty x^x x^\lambda","['integration', 'analysis', 'convergence-divergence']"
43,How to derive the triple product rule with Nonstandard Analysis?,How to derive the triple product rule with Nonstandard Analysis?,,"$\frac{\partial x}{\partial y} \cdot \frac{\partial y}{\partial z} \cdot \frac{\partial z}{\partial x} = -1$ according to the triple product rule However, it would be 1, if derivatives behaved like fractions. And in Nonstandard Calculus derivatives do behave like fractions. (it treats derivatives as fractions of actual infinitesimal quantities, rather than d/dx applied to a function) So, how does Nonstandard Calculus get the correct answer here? (which it must since it is equivalent to standard calculus)","according to the triple product rule However, it would be 1, if derivatives behaved like fractions. And in Nonstandard Calculus derivatives do behave like fractions. (it treats derivatives as fractions of actual infinitesimal quantities, rather than d/dx applied to a function) So, how does Nonstandard Calculus get the correct answer here? (which it must since it is equivalent to standard calculus)",\frac{\partial x}{\partial y} \cdot \frac{\partial y}{\partial z} \cdot \frac{\partial z}{\partial x} = -1,"['analysis', 'multivariable-calculus', 'nonstandard-analysis']"
44,Why does the definition of limits of a function have strict inequality?,Why does the definition of limits of a function have strict inequality?,,"Definition (As written in Michael Spivak's Calculus) The function $f$ approaches a limit $l$ near $a$ means: for every $\epsilon >0$ there is some $\delta > 0$ such that, for all $x$, if $0<|x-a|<\delta$, then $|f(x)-l|<\epsilon$. my question is: why can't it be: $$0<|x-a|\leq \delta,|f(x)-l|\leq \epsilon$$ After looking at limits of functions for a long time just to grasp it's meaning and using the definition quite a lot solving homework I realized I keep writing the same inequality without really understanding why. The only explanation given in Spivak's book for this part of the definition goes over it without explaining the inequality. I tried looking for an explanation myself but wasn't really able to find anything wrong with it. Is it also possible to write the definition like that or is there a problem with that? (first non-homework related question :p)","Definition (As written in Michael Spivak's Calculus) The function $f$ approaches a limit $l$ near $a$ means: for every $\epsilon >0$ there is some $\delta > 0$ such that, for all $x$, if $0<|x-a|<\delta$, then $|f(x)-l|<\epsilon$. my question is: why can't it be: $$0<|x-a|\leq \delta,|f(x)-l|\leq \epsilon$$ After looking at limits of functions for a long time just to grasp it's meaning and using the definition quite a lot solving homework I realized I keep writing the same inequality without really understanding why. The only explanation given in Spivak's book for this part of the definition goes over it without explaining the inequality. I tried looking for an explanation myself but wasn't really able to find anything wrong with it. Is it also possible to write the definition like that or is there a problem with that? (first non-homework related question :p)",,"['calculus', 'definition']"
45,Why does $f$ continuous and its Fourier coefficient converges absolutely implies the Fourier series converges uniformly?,Why does  continuous and its Fourier coefficient converges absolutely implies the Fourier series converges uniformly?,f,"I've read that if we have a continuous function defined on a circle and its Fourier coefficient converges absolutely, then that implies the Fourier series converges uniformly, but I'm not quite sure why because absolute convergence does not imply uniform convergence for infinite series. I think the reason is due to the Weierstrass $M$ test correct? If the Fourier series converges absolutely, then so do the Fourier coefficients which we use for the $M$ test. Is that correct?","I've read that if we have a continuous function defined on a circle and its Fourier coefficient converges absolutely, then that implies the Fourier series converges uniformly, but I'm not quite sure why because absolute convergence does not imply uniform convergence for infinite series. I think the reason is due to the Weierstrass test correct? If the Fourier series converges absolutely, then so do the Fourier coefficients which we use for the test. Is that correct?",M M,"['real-analysis', 'analysis', 'fourier-analysis', 'fourier-series', 'harmonic-analysis']"
46,Examples of Cauchy complete ordered fields that are not $\mathbb{R}$?,Examples of Cauchy complete ordered fields that are not ?,\mathbb{R},"According to this post , it is not true that Cauchy completeness (every Cauchy sequence has a limit) and Dedekind completeness (every nonempty set that is bounded above has a supremum) are equivalent for ordered fields. The link in the post is sadly not working (I wish people kept their notes up). The user says, Since the Archimedean hypothesis often goes almost without saying in calculus / analysis courses, many otherwise learned people are unaware that there are non-Archimedean sequentially complete fields. In fact there are rather a lot of them, and they can differ quite a lot in their behavior: e.g. some of them are first countable in the induced (order) topology, and some of them are not. This is actually surprising to me, because I had a contrary false misconception. I think my misconception comes from the mantra that "" $\mathbb{R}$ is the unique complete ordered field,"" which is misleadingly false unless we either refer to Dedekind completeness (which is usually termed as the least upperbound property) or we are presupposing the Archimedean property (which to be honest should be stated explicitly). So I'm very curious, what are the examples of Cauchy complete ordered fields that are not isomorphic to $\mathbb{R}$ ? Can we make a list of these fields?","According to this post , it is not true that Cauchy completeness (every Cauchy sequence has a limit) and Dedekind completeness (every nonempty set that is bounded above has a supremum) are equivalent for ordered fields. The link in the post is sadly not working (I wish people kept their notes up). The user says, Since the Archimedean hypothesis often goes almost without saying in calculus / analysis courses, many otherwise learned people are unaware that there are non-Archimedean sequentially complete fields. In fact there are rather a lot of them, and they can differ quite a lot in their behavior: e.g. some of them are first countable in the induced (order) topology, and some of them are not. This is actually surprising to me, because I had a contrary false misconception. I think my misconception comes from the mantra that "" is the unique complete ordered field,"" which is misleadingly false unless we either refer to Dedekind completeness (which is usually termed as the least upperbound property) or we are presupposing the Archimedean property (which to be honest should be stated explicitly). So I'm very curious, what are the examples of Cauchy complete ordered fields that are not isomorphic to ? Can we make a list of these fields?",\mathbb{R} \mathbb{R},"['analysis', 'examples-counterexamples', 'big-list', 'complete-spaces', 'ordered-fields']"
47,Prove the integration formula for arc length,Prove the integration formula for arc length,,"Suppose $f:J\to\mathbb R$ is differentiable and $f'$ is continuous on the domain. Then if $a,b\in J$ and $a<b$ , the length of the curve of the function on interval $[a,b]$ is \begin{equation}   l(f)=\displaystyle \int ^a_b \sqrt{1+f'(x)^2} dx \end{equation} This is my proof: Since the arc length is defined as $l(f,P)=\displaystyle \sum_{i=1}^n \sqrt{1+\dfrac{f(t_i)-f(t_{i-1})}{t_i-t_{i-1}}}\Delta t_i$ $f'(x)$ is the result from using MVT one each interval $[t_{i-1},t_i]$ created by the partition P then the definition of arc length can be turned to be \begin{equation} l(f,P)=\displaystyle \sum_{i=1}^n \sqrt{1+f'(x_i)^2} \Delta t_i \end{equation} Then since $f'$ is continuous on $[a,b]$ ,so $\sqrt{1+f'(x)^2}$ is continuous on $[a,b]$ . Hence it is also bounded on $[a,b]$ . Thus $\sqrt{1+f'(x)^2}$ is integrable on $[a,b]$ For any partition P= $\{t_0,...,t_n\}$ , and on any interval $[t_{i-1},t_i]$ , suppose $M_i$ is the supremum of $\sqrt{1+f'(x_i)^2}$ and $m_i$ is the infimum of $\sqrt{1+f'(x_i)^2}$ Then it's clear that $\displaystyle \sum m_i\Delta t_i\leq \sum \sqrt{1+f'(x_i)^2}\Delta t_i\leq\displaystyle \sum M_i\Delta t_i$ Then take $\displaystyle \lim_{|P|\to 0}$ to this inequality Since $\sqrt{1+f'(x)^2}$ is integrable on $[a,b]$ $\displaystyle \lim_{|P|\to 0} \displaystyle \sum m_i\Delta t_i=\displaystyle \lim_{|P|\to 0} \displaystyle \sum M_i\Delta t_i=\int^a_b \sqrt{1+f'(x)^2}dx$ Thus $\displaystyle \lim_{|P|\to 0} \sum_{i=1}^n \sqrt{1+f'(x_i)^2} \Delta t_i=\int^a_b \sqrt{1+f'(x)^2}dx=l(f)$","Suppose is differentiable and is continuous on the domain. Then if and , the length of the curve of the function on interval is This is my proof: Since the arc length is defined as is the result from using MVT one each interval created by the partition P then the definition of arc length can be turned to be Then since is continuous on ,so is continuous on . Hence it is also bounded on . Thus is integrable on For any partition P= , and on any interval , suppose is the supremum of and is the infimum of Then it's clear that Then take to this inequality Since is integrable on Thus","f:J\to\mathbb R f' a,b\in J a<b [a,b] \begin{equation}
  l(f)=\displaystyle \int ^a_b \sqrt{1+f'(x)^2} dx
\end{equation} l(f,P)=\displaystyle \sum_{i=1}^n \sqrt{1+\dfrac{f(t_i)-f(t_{i-1})}{t_i-t_{i-1}}}\Delta t_i f'(x) [t_{i-1},t_i] \begin{equation}
l(f,P)=\displaystyle \sum_{i=1}^n \sqrt{1+f'(x_i)^2} \Delta t_i
\end{equation} f' [a,b] \sqrt{1+f'(x)^2} [a,b] [a,b] \sqrt{1+f'(x)^2} [a,b] \{t_0,...,t_n\} [t_{i-1},t_i] M_i \sqrt{1+f'(x_i)^2} m_i \sqrt{1+f'(x_i)^2} \displaystyle \sum m_i\Delta t_i\leq \sum \sqrt{1+f'(x_i)^2}\Delta t_i\leq\displaystyle \sum M_i\Delta t_i \displaystyle \lim_{|P|\to 0} \sqrt{1+f'(x)^2} [a,b] \displaystyle \lim_{|P|\to 0} \displaystyle \sum m_i\Delta t_i=\displaystyle \lim_{|P|\to 0} \displaystyle \sum M_i\Delta t_i=\int^a_b \sqrt{1+f'(x)^2}dx \displaystyle \lim_{|P|\to 0} \sum_{i=1}^n \sqrt{1+f'(x_i)^2} \Delta t_i=\int^a_b \sqrt{1+f'(x)^2}dx=l(f)","['integration', 'analysis', 'solution-verification', 'arc-length']"
48,Showing that $\{x\in\mathbb{Q}:x≤0 \ or \ x^2≤2\}$ does not have a least upper bound in $\mathbb{Q}$,Showing that  does not have a least upper bound in,\{x\in\mathbb{Q}:x≤0 \ or \ x^2≤2\} \mathbb{Q},"I want to prove that $C=\{x\in\mathbb{Q}:x≤0  \ or  \ x^2≤2\}$ do not have a least upper bound in $\mathbb{Q}$ The definition I'm working with is that a least upper bound, $b$ has to be an upper bound, so $b\in\mathbb{Q}$ where $b>x \ \forall x\in C$ and that its the smallest upper bound, so if $c$ is some upper bound, then $b≤c$ I think the proof is to show that while I can find upper bounds, $b$ , I will always be able to find a smaller one. I can prove that upper bound exists by having $b$ be positive and $b^2>2$ but I would like to know how you would always be able to find a smaller one. I think I want to subtract $1/n$ to the bound for a small enough $n$ to create a smaller upper bound. And I would like to do this by avoiding $\sqrt{2}$ if possible","I want to prove that do not have a least upper bound in The definition I'm working with is that a least upper bound, has to be an upper bound, so where and that its the smallest upper bound, so if is some upper bound, then I think the proof is to show that while I can find upper bounds, , I will always be able to find a smaller one. I can prove that upper bound exists by having be positive and but I would like to know how you would always be able to find a smaller one. I think I want to subtract to the bound for a small enough to create a smaller upper bound. And I would like to do this by avoiding if possible",C=\{x\in\mathbb{Q}:x≤0  \ or  \ x^2≤2\} \mathbb{Q} b b\in\mathbb{Q} b>x \ \forall x\in C c b≤c b b b^2>2 1/n n \sqrt{2},"['real-analysis', 'analysis', 'irrational-numbers', 'rational-numbers', 'upper-lower-bounds']"
49,Evans Representation formula for solution of Poissons equation,Evans Representation formula for solution of Poissons equation,,"In Evans PDE, Chapter 2 Theorem 9 we have: Let $f \in C^2_c(\mathbb{R}^n)$ , $n \geq 3$ . Then any bounded solution of $$-\Delta u = f$$ in $\mathbb{R}^n$ has the form $$u(x)  = \int_{\mathbb{R}^n} \Phi(x-y)f(y)dy +C.$$ Then there's a remark: If $n=2$ , $\Phi(x) = - \frac{1}{2 \pi} \log|x|$ is unbounded as $|x| \rightarrow \infty$ , and so may be $\int_{\mathbb{R}^n} \Phi(x-y)f(y)dy. $ My question is: If $f$ has compact support then let the compact set $\overline{V}$ be the support of $f$ . Then the integral becomes $\int_V \Phi(x-y)f(y)dy. $ Now since $\Phi f$ is continuous away from zero it achieves its $\max$ on $V$ , so the integral is finite? Meaning we shouldn't look at large values for $x$ , because then we will be outside of $V$ in which case the integral will be zero. What am I missing? EDIT: see the comment by cmk which answers this.","In Evans PDE, Chapter 2 Theorem 9 we have: Let , . Then any bounded solution of in has the form Then there's a remark: If , is unbounded as , and so may be My question is: If has compact support then let the compact set be the support of . Then the integral becomes Now since is continuous away from zero it achieves its on , so the integral is finite? Meaning we shouldn't look at large values for , because then we will be outside of in which case the integral will be zero. What am I missing? EDIT: see the comment by cmk which answers this.",f \in C^2_c(\mathbb{R}^n) n \geq 3 -\Delta u = f \mathbb{R}^n u(x)  = \int_{\mathbb{R}^n} \Phi(x-y)f(y)dy +C. n=2 \Phi(x) = - \frac{1}{2 \pi} \log|x| |x| \rightarrow \infty \int_{\mathbb{R}^n} \Phi(x-y)f(y)dy.  f \overline{V} f \int_V \Phi(x-y)f(y)dy.  \Phi f \max V x V,"['analysis', 'partial-differential-equations']"
50,Exterior derivative of wedge product [duplicate],Exterior derivative of wedge product [duplicate],,"This question already has an answer here : Computing the exterior derivative of a wedge product (1 answer) Closed 7 years ago . How can I show that $$ \mathrm d(a\wedge b)=(\mathrm d\,a)\wedge b + (-1)^{q}a\wedge(\mathrm d\,b) $$ for a $q$-form $a$ and an $r$-form $b$?","This question already has an answer here : Computing the exterior derivative of a wedge product (1 answer) Closed 7 years ago . How can I show that $$ \mathrm d(a\wedge b)=(\mathrm d\,a)\wedge b + (-1)^{q}a\wedge(\mathrm d\,b) $$ for a $q$-form $a$ and an $r$-form $b$?",,"['differential-geometry', 'differential-forms']"
51,Double integral with absolute value solution verification,Double integral with absolute value solution verification,,"I would like to know if my way to solve the following integral is correct: Let $I=\frac{1}{4}\int\int_Te^{x^2/4+y^2}(4-x^2-4y^2)dxdy$ , with $T=\{(x,y)\in \mathbb{R^2}:0\leq x\leq 2y,\ \ x^2+4y^2 \leq 4 \}$ . I made the following substitution, $\begin{cases}x=2rcos(\theta)\\\ y=rsin(\theta)\end{cases}$ , so that $dxdy=2rdrd\theta$ , $T=\{(r,\theta): 0\leq cos(\theta) \leq sin(\theta), \ \ r^2\leq 1 \}=\{(r,\theta): \theta \in [\pi/4,\pi/2], \ \ r\in[0,1] \}$ , thus we can say $I=\frac{1}{4}\int_{\pi/4}^{\pi/2}\int_0^1e^{r^2}(4-4r^2)2rdrd\theta=\frac{\pi}{2}\int_0^1e^{r^2}(r-r^3)dr=\frac{\pi}{2}(\frac{e-2}{2})$ . Is my reasoning correct?","I would like to know if my way to solve the following integral is correct: Let , with . I made the following substitution, , so that , , thus we can say . Is my reasoning correct?","I=\frac{1}{4}\int\int_Te^{x^2/4+y^2}(4-x^2-4y^2)dxdy T=\{(x,y)\in \mathbb{R^2}:0\leq x\leq 2y,\ \ x^2+4y^2 \leq 4 \} \begin{cases}x=2rcos(\theta)\\\ y=rsin(\theta)\end{cases} dxdy=2rdrd\theta T=\{(r,\theta): 0\leq cos(\theta) \leq sin(\theta), \ \ r^2\leq 1 \}=\{(r,\theta): \theta \in [\pi/4,\pi/2], \ \ r\in[0,1] \} I=\frac{1}{4}\int_{\pi/4}^{\pi/2}\int_0^1e^{r^2}(4-4r^2)2rdrd\theta=\frac{\pi}{2}\int_0^1e^{r^2}(r-r^3)dr=\frac{\pi}{2}(\frac{e-2}{2})","['calculus', 'integration', 'analysis', 'solution-verification']"
52,Bounds on Gaussian process posterior variance.,Bounds on Gaussian process posterior variance.,,"This is in the context of Gaussian process models where, the posterior variance (given observations at $X = \{x_1, \ldots, x_n\}$ ) has the following form $\sigma_n^2(x)=k(x,x) - \mathbf{k}(X, x)^T \mathbf{K}_n^{-1} \mathbf{k}(X, x)$ , where $k(\cdot, \cdot)$ is a positive definite covariance kernel, $\mathbf{k}(X, x)$ is the vector $[k(x_1, x), \ldots, k(x_n, x)]$ , and $\mathbf{K}$ is an $n\times n$ postive definite matrix defined as $\mathbf{K}_{ij} = k(x_i, x_j)$ . Are there any existing works that discuss the rate of convergence of $\sigma_n^2(x)$ as $n \rightarrow \infty$ or some sort of an upper bound, e.g., (for a finite $n$ ) $\sigma_n^2(x) \leq \mathcal{O} ( \frac{1}{log(n)} )$ ?","This is in the context of Gaussian process models where, the posterior variance (given observations at ) has the following form , where is a positive definite covariance kernel, is the vector , and is an postive definite matrix defined as . Are there any existing works that discuss the rate of convergence of as or some sort of an upper bound, e.g., (for a finite ) ?","X = \{x_1, \ldots, x_n\} \sigma_n^2(x)=k(x,x) - \mathbf{k}(X, x)^T \mathbf{K}_n^{-1} \mathbf{k}(X, x) k(\cdot, \cdot) \mathbf{k}(X, x) [k(x_1, x), \ldots, k(x_n, x)] \mathbf{K} n\times n \mathbf{K}_{ij} = k(x_i, x_j) \sigma_n^2(x) n \rightarrow \infty n \sigma_n^2(x) \leq \mathcal{O} ( \frac{1}{log(n)} )","['linear-algebra', 'analysis', 'asymptotics']"
53,Is my demonstration correct?,Is my demonstration correct?,,"Is my demonstration correct? Let $f, g$ infinitely differentiable increasing functions. Suposse that: $$\star \ \ \ \ \  f(0) = g(0) = 0 $$ $$\star \ \ \ \ \  f'(0) = g'(0) = 1 $$ $$\star \ \ \ \ \ \lim\limits_{x \rightarrow 0} \dfrac{f(x)}{x} = 1 $$ $$\star \ \ \ \ \ f(-x) = -f(x), g(-x) = -g(x) $$ $$\star \ \ \ \ \ f(x) \neq g(x) \ \ \ \forall x \neq 0 $$ So $$\lim\limits_{x \rightarrow 0}\dfrac{f(x) - g(x)}{g^{-1}(x) - f^{-1}(x)} = 1 $$ My possible demonstration: We can approximate f(x) next to $x_0$ by $$(1) \ \ \ f(x) \approx f'(x_0)x + (f(x_0) - x_0f'(x_0)) = f'(x_0)x + b(x_0) $$ where $b(x_0) = (f(x_0) - x_0f'(x))$ . Replacing x by $f^{-1}(x)$ in (1) we got: $$(2) \ \ \ f^{-1}(x) = \dfrac{x - b(x_0)}{f'(x_0)} $$ (same about $g$ , $g(x) = g'(x_0)x + (g(x_0) - x_0g'(x_0)) = g'(x_0)x + c(x_0))$ , so $$\dfrac{f(x) - g(x)}{g^{-1}(x) - f^{-1}(x)} \approx \dfrac{f'(x_0)x + b(x_0) - (g'(x_0)x + c(x_0))}{\dfrac{x - c(x_0)}{g'(x_0)}-\dfrac{x - b(x_0)}{f'(x_0)}} $$ But $$\lim\limits_{x_0 \rightarrow 0} f'(x_0) = \lim\limits_{x_0 \rightarrow 0} g'(x_0) =1$$ So we got $$\dfrac{f'(x_0)x + b(x_0) - (g'(x_0)x + c(x_0))}{\dfrac{x - c(x_0)}{g'(x_0)}-\dfrac{x - b(x_0)}{f'(x_0)}} \approx \dfrac{b(x_0) - c(x_0)}{-c(x_0) + b(x_0)} = 1$$ It's wrong? How can I be more accurate? I want to prove for $f(x) = \sin(\tan(x)), g(x) = \tan(\sin(x))$ , however, I believe it is possible to generalize as above","Is my demonstration correct? Let infinitely differentiable increasing functions. Suposse that: So My possible demonstration: We can approximate f(x) next to by where . Replacing x by in (1) we got: (same about , , so But So we got It's wrong? How can I be more accurate? I want to prove for , however, I believe it is possible to generalize as above","f, g \star \ \ \ \ \  f(0) = g(0) = 0  \star \ \ \ \ \  f'(0) = g'(0) = 1  \star \ \ \ \ \ \lim\limits_{x \rightarrow 0} \dfrac{f(x)}{x} = 1  \star \ \ \ \ \ f(-x) = -f(x), g(-x) = -g(x)  \star \ \ \ \ \ f(x) \neq g(x) \ \ \ \forall x \neq 0  \lim\limits_{x \rightarrow 0}\dfrac{f(x) - g(x)}{g^{-1}(x) - f^{-1}(x)} = 1  x_0 (1) \ \ \ f(x) \approx f'(x_0)x + (f(x_0) - x_0f'(x_0)) = f'(x_0)x + b(x_0)  b(x_0) = (f(x_0) - x_0f'(x)) f^{-1}(x) (2) \ \ \ f^{-1}(x) = \dfrac{x - b(x_0)}{f'(x_0)}  g g(x) = g'(x_0)x + (g(x_0) - x_0g'(x_0)) = g'(x_0)x + c(x_0)) \dfrac{f(x) - g(x)}{g^{-1}(x) - f^{-1}(x)} \approx \dfrac{f'(x_0)x + b(x_0) - (g'(x_0)x + c(x_0))}{\dfrac{x - c(x_0)}{g'(x_0)}-\dfrac{x - b(x_0)}{f'(x_0)}}  \lim\limits_{x_0 \rightarrow 0} f'(x_0) = \lim\limits_{x_0 \rightarrow 0} g'(x_0) =1 \dfrac{f'(x_0)x + b(x_0) - (g'(x_0)x + c(x_0))}{\dfrac{x - c(x_0)}{g'(x_0)}-\dfrac{x - b(x_0)}{f'(x_0)}} \approx \dfrac{b(x_0) - c(x_0)}{-c(x_0) + b(x_0)} = 1 f(x) = \sin(\tan(x)), g(x) = \tan(\sin(x))",['calculus']
54,Nondecreasing and piecewise-continuous => piecewise-continuously-differentiable?,Nondecreasing and piecewise-continuous => piecewise-continuously-differentiable?,,"I'm interested to know if a piecewise-continuous monotone function from $\mathbb{R}$ to $\mathbb{R}$ is also piecewise-continuously differentiable. I mean piecewise in the sense that there is a finite or countably infinite and isolated set $S\subset\mathbb{R}$ , and intervals covering $\mathbb{R}\setminus S$ such that on any of these intervals, continuity and continuous-differentiability are verified. I know that a monotone function is almost everywhere differentiable ( this topic ) there are monotone and everywhere differentiable functions that are not continuously differentiable ( this other topic ) almost everywhere differentiable may not imply almost everywhere continuously differentiable, the derivative can actually be nowhere continuous ( this one ) It seems to me that this third reference is not exactly a counter-example as from what I understand, it features a sort of devil-staircase function and I guess this one is not piecewise-continuous? (possible accumulating sequence of discontinuities)","I'm interested to know if a piecewise-continuous monotone function from to is also piecewise-continuously differentiable. I mean piecewise in the sense that there is a finite or countably infinite and isolated set , and intervals covering such that on any of these intervals, continuity and continuous-differentiability are verified. I know that a monotone function is almost everywhere differentiable ( this topic ) there are monotone and everywhere differentiable functions that are not continuously differentiable ( this other topic ) almost everywhere differentiable may not imply almost everywhere continuously differentiable, the derivative can actually be nowhere continuous ( this one ) It seems to me that this third reference is not exactly a counter-example as from what I understand, it features a sort of devil-staircase function and I guess this one is not piecewise-continuous? (possible accumulating sequence of discontinuities)",\mathbb{R} \mathbb{R} S\subset\mathbb{R} \mathbb{R}\setminus S,"['analysis', 'piecewise-continuity']"
55,Relating Dirac mass with surface integral,Relating Dirac mass with surface integral,,"I know that for a smooth function $g:\mathbb{R}^n\to\mathbb{R}$ with $\nabla g(x)\neq 0$ , there holds the formula $$\int_{\mathbb{R}^n}\delta(g(x))f(x)\,dx=\int_{g=0}\frac{1}{|\nabla g(x)|}f(x)\,d\sigma(x)$$ where $\,d\sigma$ is the surface measure induced by $[g=0]$ and $\delta$ the one dimensional Dirac mass. I wonder, what is the equivalent expression for $g:\mathbb{R}^n\to\mathbb{R}^m$ for $m<n$ , when $Dg(x)\neq 0$ ? In other words what is the scaling factor in the surface integral to compensate for the $m$ -dimensional Dirac mass? Is it something like $|Dg(x)|^m$ ?","I know that for a smooth function with , there holds the formula where is the surface measure induced by and the one dimensional Dirac mass. I wonder, what is the equivalent expression for for , when ? In other words what is the scaling factor in the surface integral to compensate for the -dimensional Dirac mass? Is it something like ?","g:\mathbb{R}^n\to\mathbb{R} \nabla g(x)\neq 0 \int_{\mathbb{R}^n}\delta(g(x))f(x)\,dx=\int_{g=0}\frac{1}{|\nabla g(x)|}f(x)\,d\sigma(x) \,d\sigma [g=0] \delta g:\mathbb{R}^n\to\mathbb{R}^m m<n Dg(x)\neq 0 m |Dg(x)|^m","['real-analysis', 'integration', 'analysis', 'differential-geometry', 'dirac-delta']"
56,Can we use bounded convergence theorem to calculate $\lim_{n \rightarrow \infty} \int_{0}^{1} \frac{n \sin(x)}{1 + n^2 \sqrt{x}}$?,Can we use bounded convergence theorem to calculate ?,\lim_{n \rightarrow \infty} \int_{0}^{1} \frac{n \sin(x)}{1 + n^2 \sqrt{x}},"I know how to use dominated convergence theorem to calculate $\lim_{n \rightarrow \infty} \int_{0}^{1} \frac{n \sin(x)}{1 + n^2 \sqrt{x}}$ ? Can we use bounded convergence theorem to calculate $\lim_{n \rightarrow \infty} \int_{0}^{1} \frac{n \sin(x)}{1 + n^2 \sqrt{x}}$ ? The bounded convergence theorem statement is as follows: Let $\{f_n\}$ be a sequence of measurable functions on a set of finite measure $E.$ Suppose $\{f_n\}$ is uniformly pointwise bounded on $E,$ that is, there is a number $M \geq 0$ for which $$|f_n| \leq M $$ on $E$ for all $n.$ If $\{f_n\} \rightarrow f$ pointwise on $E,$ then $\lim_{n \rightarrow \infty} \int_E f_n = \int_E f.$ Could anyone help me in this please?","I know how to use dominated convergence theorem to calculate ? Can we use bounded convergence theorem to calculate ? The bounded convergence theorem statement is as follows: Let be a sequence of measurable functions on a set of finite measure Suppose is uniformly pointwise bounded on that is, there is a number for which on for all If pointwise on then Could anyone help me in this please?","\lim_{n \rightarrow \infty} \int_{0}^{1} \frac{n \sin(x)}{1 + n^2 \sqrt{x}} \lim_{n \rightarrow \infty} \int_{0}^{1} \frac{n \sin(x)}{1 + n^2 \sqrt{x}} \{f_n\} E. \{f_n\} E, M \geq 0 |f_n| \leq M  E n. \{f_n\} \rightarrow f E, \lim_{n \rightarrow \infty} \int_E f_n = \int_E f.","['real-analysis', 'analysis']"
57,Proving the inverse relationship between exponential function and natural logarithm,Proving the inverse relationship between exponential function and natural logarithm,,"I am working to prove that $E(x) = \displaystyle\sum_{n=0}^\infty \frac{x^n}{n!}$ and $L(x) = \displaystyle\int_1^x\frac{1}{t}dt $ are inverse functions to one another. I have already found that $(L \circ E)(x) = x$ for all $x \in \mathbb{R}$ , however I am struggling to prove that $(E \circ L)(x)=x$ for all $x > 0$ . My work for $L \circ E$ involved finding $(L \circ E)'(x) = 1$ to imply my result. Any tips how to proceed for this $(E \circ L)$ result? Thanks!","I am working to prove that and are inverse functions to one another. I have already found that for all , however I am struggling to prove that for all . My work for involved finding to imply my result. Any tips how to proceed for this result? Thanks!",E(x) = \displaystyle\sum_{n=0}^\infty \frac{x^n}{n!} L(x) = \displaystyle\int_1^x\frac{1}{t}dt  (L \circ E)(x) = x x \in \mathbb{R} (E \circ L)(x)=x x > 0 L \circ E (L \circ E)'(x) = 1 (E \circ L),"['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'analysis']"
58,Notation of Derivative and Differential,Notation of Derivative and Differential,,"Let $\gamma:[a;b] \to U, \gamma(t)=\big(x(t),y(t)\big)$ be a path. The integral of $\omega$ along $\gamma$ is defined as $$ \int_\gamma \omega := \int_a^b \omega_x\big(x(t),y(t)\big)\dot{x}(t)+\omega_y\big(x(t),y(t)\big)\dot{y}(t)~ dt$$ where $\dot{x}=\frac{dx}{dt}$ , $\dot{y}=\frac{dy}{dt}$ . My Question: What does $\dot{x}=\frac{dx}{dt}$ exactly mean? And could I replace it with $\dot{x}=\frac{\partial x}{\partial t}$ ?","Let be a path. The integral of along is defined as where , . My Question: What does exactly mean? And could I replace it with ?","\gamma:[a;b] \to U, \gamma(t)=\big(x(t),y(t)\big) \omega \gamma  \int_\gamma \omega := \int_a^b \omega_x\big(x(t),y(t)\big)\dot{x}(t)+\omega_y\big(x(t),y(t)\big)\dot{y}(t)~ dt \dot{x}=\frac{dx}{dt} \dot{y}=\frac{dy}{dt} \dot{x}=\frac{dx}{dt} \dot{x}=\frac{\partial x}{\partial t}","['integration', 'analysis', 'derivatives', 'notation', 'differential']"
59,Solve $(e^{2 \pi ix}-1)u'=0$ in $D'(\mathbb{R})$,Solve  in,(e^{2 \pi ix}-1)u'=0 D'(\mathbb{R}),"I'm trying to solve the following problems in the sense of distributions Q8c of example sheet 1 of the Distribution Theory course of part III of the mathematical tripos. $(e^{2 \pi ix}-1)u'=0$ . In order to solve this problem, I tried working backwards. Take a $\psi \in D(\mathbb{R})$ . I want to construct a $\phi \in D(\mathbb{R})$ such that $$ \langle u,\psi \rangle = \langle u,-\frac{\phi}{e^{2 \pi ix}-1} \rangle = \langle (e^{2 \pi ix}-1)u',\phi \rangle =0. $$ My issue is that such a $\phi$ satisfies $\phi(x)=\frac{{ \int_{0}^{x} \psi(y)dy}}{1-e^{2 \pi ix}}$ for $x \neq 0$ . This would cause $\phi$ to be undefined on $\mathbb{Z}$ . I'm slightly stuck here. Any help would be greatly appreciated. Thank you.","I'm trying to solve the following problems in the sense of distributions Q8c of example sheet 1 of the Distribution Theory course of part III of the mathematical tripos. . In order to solve this problem, I tried working backwards. Take a . I want to construct a such that My issue is that such a satisfies for . This would cause to be undefined on . I'm slightly stuck here. Any help would be greatly appreciated. Thank you.","(e^{2 \pi ix}-1)u'=0 \psi \in D(\mathbb{R}) \phi \in D(\mathbb{R}) 
\langle u,\psi \rangle = \langle u,-\frac{\phi}{e^{2 \pi ix}-1} \rangle
= \langle (e^{2 \pi ix}-1)u',\phi \rangle
=0.
 \phi \phi(x)=\frac{{ \int_{0}^{x} \psi(y)dy}}{1-e^{2 \pi ix}} x \neq 0 \phi \mathbb{Z}","['real-analysis', 'functional-analysis', 'analysis', 'distribution-theory']"
60,"Show that the function $u \in L^n(B^0(0,1))$ for $n > 1$.",Show that the function  for .,"u \in L^n(B^0(0,1)) n > 1","I am working on Exercise 5.10.14 in Evans's Partial Differential Equations. I am trying to show that the function $$ u(x) := \log\Bigg(\log\Bigg(1 + \frac{1}{|x|}\Bigg)\Bigg) $$ belongs to $L^n(B^0(0,1))$ with $n >1$ ,  where $B^0(0,1)$ is the open unit ball. By applying polar coordinates, we have. \begin{align*} \int_{B^0(0,1))}\log\Bigg(\log\Bigg(1 + \frac{1}{|x|}\Bigg)\Bigg)^n\text{d}x &= \int_0^1\int_{\partial B^0(0,r))}\log\Bigg(\log\Bigg(1 + \frac{1}{|x|}\Bigg)\Bigg)^n\text{d}S(x)dr\\ &= \int_0^1 \log\Bigg(\log\Bigg(1 + \frac{1}{r}\Bigg)\Bigg)^n\Big(n\alpha(n)r^{n-1}\Big)\text{d}r\\ &=n\alpha(n) \int_0^1 \log\Bigg(\log\Bigg(1 + \frac{1}{r}\Bigg)\Bigg)^nr^{n-1}\text{d}r\\ \end{align*} From there, I thought about applying the change of variables $w = \log\Bigg(1 + \frac{1}{r}\Bigg)$ which after a bit of work, I think yields $$ -n\alpha(n)\int_{w(0)}^{w(1)} (r\log(w))^n\text{d}w $$ I think this is infinite though. Any thoughts, hints, ideas?","I am working on Exercise 5.10.14 in Evans's Partial Differential Equations. I am trying to show that the function belongs to with ,  where is the open unit ball. By applying polar coordinates, we have. From there, I thought about applying the change of variables which after a bit of work, I think yields I think this is infinite though. Any thoughts, hints, ideas?","
u(x) := \log\Bigg(\log\Bigg(1 + \frac{1}{|x|}\Bigg)\Bigg)
 L^n(B^0(0,1)) n >1 B^0(0,1) \begin{align*}
\int_{B^0(0,1))}\log\Bigg(\log\Bigg(1 + \frac{1}{|x|}\Bigg)\Bigg)^n\text{d}x &= \int_0^1\int_{\partial B^0(0,r))}\log\Bigg(\log\Bigg(1 + \frac{1}{|x|}\Bigg)\Bigg)^n\text{d}S(x)dr\\
&= \int_0^1 \log\Bigg(\log\Bigg(1 + \frac{1}{r}\Bigg)\Bigg)^n\Big(n\alpha(n)r^{n-1}\Big)\text{d}r\\
&=n\alpha(n) \int_0^1 \log\Bigg(\log\Bigg(1 + \frac{1}{r}\Bigg)\Bigg)^nr^{n-1}\text{d}r\\
\end{align*} w = \log\Bigg(1 + \frac{1}{r}\Bigg) 
-n\alpha(n)\int_{w(0)}^{w(1)} (r\log(w))^n\text{d}w
","['integration', 'functional-analysis', 'analysis', 'partial-differential-equations']"
61,Is this Proof clear? Proof that $a^{1/n}$ converges to $1$ wherer $n$ is a natural number and $a>0$ is a real number,Is this Proof clear? Proof that  converges to  wherer  is a natural number and  is a real number,a^{1/n} 1 n a>0,"Proof that $a^{1/n}$ converges to $1$ wherer $n$ is a natural number and $a>0$ is a real number Is this proof well written? Probably not. If not can anyone help me how to improve it? Step 0 Let $n$ denote a natural number. We are going to use the lemma that $a \leq b$ $\iff $ $a^n \leq b^n$ where $a,b$ are two real numbers $>0$ . We are also going to use the squeeze theorem and the binomial theorem. We will first construct a lower bound then an upper bound for $a \geq 1$ and then we are going to generalize it for $0<a<1$ . Step 1 Let $a \geq 1$ . We will show that $1 \leq a^{1/n}$ . This is equivalent to $1 \leq a$ . But this is true by assumption and thus we have a lower bound. Step 2 Lets consider $a^{1/n} \leq 1 +\frac{a}{n}$ . This is equivalent to $a \leq (1+\frac{a}{n})^{n}$ . We can use the binomial theorem here and we find that it is equal to $1+n \frac{a}{n}1^{n}+...=1+a+...$ . Then we combine this and we have that $a \leq 1+a+...$ but this is obviously true. Step 3 We now have achieved the following for a real number $a \geq 1$ we have that $1 \leq a^{1/n} \leq 1+a/n$ . And by the squeeze theorem this converges to 1. Step 4 Last and least we need to show the same thing for $0 <a <1$ . We can rewrite this into $a^{1/n}=\frac{1}{(\frac{1}{a})^{1/n}}$ . But because if $a <1$ then $1/a > 1$ but because this is also a real number we can use the theorem we just proved and apply the quotient rule for limits. Thus we are done.",Proof that converges to wherer is a natural number and is a real number Is this proof well written? Probably not. If not can anyone help me how to improve it? Step 0 Let denote a natural number. We are going to use the lemma that where are two real numbers . We are also going to use the squeeze theorem and the binomial theorem. We will first construct a lower bound then an upper bound for and then we are going to generalize it for . Step 1 Let . We will show that . This is equivalent to . But this is true by assumption and thus we have a lower bound. Step 2 Lets consider . This is equivalent to . We can use the binomial theorem here and we find that it is equal to . Then we combine this and we have that but this is obviously true. Step 3 We now have achieved the following for a real number we have that . And by the squeeze theorem this converges to 1. Step 4 Last and least we need to show the same thing for . We can rewrite this into . But because if then but because this is also a real number we can use the theorem we just proved and apply the quotient rule for limits. Thus we are done.,"a^{1/n} 1 n a>0 n a \leq b \iff  a^n \leq b^n a,b >0 a \geq 1 0<a<1 a \geq 1 1 \leq a^{1/n} 1 \leq a a^{1/n} \leq 1 +\frac{a}{n} a \leq (1+\frac{a}{n})^{n} 1+n \frac{a}{n}1^{n}+...=1+a+... a \leq 1+a+... a \geq 1 1 \leq a^{1/n} \leq 1+a/n 0 <a <1 a^{1/n}=\frac{1}{(\frac{1}{a})^{1/n}} a <1 1/a > 1","['limits', 'analysis', 'proof-writing']"
62,Integrating over curve of constant modulus,Integrating over curve of constant modulus,,"In a paper of Raphael M. Robinson, which can be found here (given Jstor privileges, but I have included screenshots relevant to my question) the author defines a rational function $h(z) \in \mathbb{C}(z)$ such that $|h| < 1$ on some compact set $E \subset \mathbb{C}$ . He then chooses some $\lambda < 1$ for which $|h| < \lambda$ on $E$ , and integrates some meromorphic function $T$ over the closed curve $C = |h|^{-1}(\lambda)$ . He also claims this curve 'encloses' $E$ . I am wondering why $|h|^{-1}(\lambda)$ is one of the usual closed curves we can integrate over in complex analysis (so basically, piecewise $C^1$ ), and I am also unsure exactly what the meaning of 'enclose' here is. I suppose it means $\eta(C,e) = 1$ for all $e \in E$ . Here are the relevant screenshots: $h$ "" /> Also important is that the $q_k,p_k$ are finite (in fact they are integers, but this part does not seem necessary for my question), and $A_r \neq 0$ (in fact $|A_r| \geq 1$ but again I do not think this part is necessary for my question). Edit (partial solution): Here is a possible hacky way to circumvent the issue I brought up earlier, $h=\frac{P}{Q}$ is a rational function, i.e. some holomorphic map $\hat{\mathbb{C}} \rightarrow \hat{\mathbb{C}}$ , and in particular it is a branched cover in the sense that if we treat it as a map $\hat{\mathbb{C}} \setminus \{q_1,..., q_k \} \rightarrow \hat{\mathbb{C}} \setminus \{\alpha_1,...,\alpha_j \}$ (where the set we removed from the domain are the branch points and the set we remove from the range are the critical values), then it is a bonafide covering map. Now we can just select $\lambda < 1$ so that the circle of radius $\lambda$ does not contain any of the critical values, and lift the circle parameterized positively via the rational function. The rational function is a degree $\max(\deg(P),\deg(Q))$ cover, so there will be exactly these many disjoint lifts, all of these lifts will be smooth closed curves. Now I am guessing because of how close $h$ is to a covering map, there will be a way to modify this approach for arbitrary $\lambda$ . The risk with this hacky way is that it is possible $\lambda<1$ being arbitrary so that $|h|<\lambda$ on $E$ is essential for the rest of the proof, luckily it turns out this is not the case. So the disjoint union (after removing lifts that are reparameterizations of other lifts) of these lifts will be the entire preimage of $|h|^{-1}(\lambda)$ , which is a cycle (not exactly a closed curve, but we can still apply the usual residue theorems for cycles so this isn't an issue). Finally because $h(\infty) = \infty$ each connected components of $|h|^{-1}[0,\lambda]$ will coincide with the closure of the regions bounded by the lifts, and because these curves have winding number $+1$ with respect to points in their interior, we get the result for all but finitely many $\lambda$ . I suppose for the bad $\lambda$ there may be some continuity argument to show this still holds.","In a paper of Raphael M. Robinson, which can be found here (given Jstor privileges, but I have included screenshots relevant to my question) the author defines a rational function such that on some compact set . He then chooses some for which on , and integrates some meromorphic function over the closed curve . He also claims this curve 'encloses' . I am wondering why is one of the usual closed curves we can integrate over in complex analysis (so basically, piecewise ), and I am also unsure exactly what the meaning of 'enclose' here is. I suppose it means for all . Here are the relevant screenshots: $h$ "" /> Also important is that the are finite (in fact they are integers, but this part does not seem necessary for my question), and (in fact but again I do not think this part is necessary for my question). Edit (partial solution): Here is a possible hacky way to circumvent the issue I brought up earlier, is a rational function, i.e. some holomorphic map , and in particular it is a branched cover in the sense that if we treat it as a map (where the set we removed from the domain are the branch points and the set we remove from the range are the critical values), then it is a bonafide covering map. Now we can just select so that the circle of radius does not contain any of the critical values, and lift the circle parameterized positively via the rational function. The rational function is a degree cover, so there will be exactly these many disjoint lifts, all of these lifts will be smooth closed curves. Now I am guessing because of how close is to a covering map, there will be a way to modify this approach for arbitrary . The risk with this hacky way is that it is possible being arbitrary so that on is essential for the rest of the proof, luckily it turns out this is not the case. So the disjoint union (after removing lifts that are reparameterizations of other lifts) of these lifts will be the entire preimage of , which is a cycle (not exactly a closed curve, but we can still apply the usual residue theorems for cycles so this isn't an issue). Finally because each connected components of will coincide with the closure of the regions bounded by the lifts, and because these curves have winding number with respect to points in their interior, we get the result for all but finitely many . I suppose for the bad there may be some continuity argument to show this still holds.","h(z) \in \mathbb{C}(z) |h| < 1 E \subset \mathbb{C} \lambda < 1 |h| < \lambda E T C = |h|^{-1}(\lambda) E |h|^{-1}(\lambda) C^1 \eta(C,e) = 1 e \in E q_k,p_k A_r \neq 0 |A_r| \geq 1 h=\frac{P}{Q} \hat{\mathbb{C}} \rightarrow \hat{\mathbb{C}} \hat{\mathbb{C}} \setminus \{q_1,..., q_k \} \rightarrow \hat{\mathbb{C}} \setminus \{\alpha_1,...,\alpha_j \} \lambda < 1 \lambda \max(\deg(P),\deg(Q)) h \lambda \lambda<1 |h|<\lambda E |h|^{-1}(\lambda) h(\infty) = \infty |h|^{-1}[0,\lambda] +1 \lambda \lambda","['complex-analysis', 'analysis', 'complex-geometry', 'complex-integration']"
63,Does the integral of an operator be compact implies that the semigroup is compact?,Does the integral of an operator be compact implies that the semigroup is compact?,,"Let $M$ be a compact metric space and $$\mathcal C^0(M) :=\{f:M\to \mathbb R;\ f\ \text{is continuous}\}.$$ Consider the semigroup of  contractive bounded linear positive operators $$\{P^t : (\mathcal C^0(M),\|\cdot \|_{\infty}) \to \left(\mathcal C^0(M),\|\cdot\|_{\infty}\right)\}_{t\geq 0}, $$ i.e. $$\|P^t\|\leq 1,\ P^0 = \mathrm{Id}\ \ \text{and} \ P^{s+t} = P^t \circ P^s,\ \forall \ s,t\geq 0,$$ and $$ f\in \mathcal C^0(M),\ f\geq 0 \Rightarrow \ P^t f \geq 0,\ \forall\ t\geq 0.$$ Assume that there exist $K,\alpha >0$ , such that $$ \|P^t\|\leq Ke^{-\alpha t}.$$ and \begin{align} G: (\mathcal C^0(M,\|\cdot\|_{\infty}) &\to \left(\mathcal C^0(M),\|\cdot\|_{\infty}\right)\\ f&\mapsto \left(x\mapsto \int_0^\infty \left(P^t f\right)(x)\ \mathrm{d}t\right) \end{align} is a compact operator. Question: Do $G$ being a compact and $P^t$ presenting exponential decay imply that there exist a $t_0>0$ such that $P^{t_0}$ is a compact operator? In my head, it is harder to $G$ be compact than to $P^t$ be compact, but I was not able to prove it. Can anyone help me?","Let be a compact metric space and Consider the semigroup of  contractive bounded linear positive operators i.e. and Assume that there exist , such that and is a compact operator. Question: Do being a compact and presenting exponential decay imply that there exist a such that is a compact operator? In my head, it is harder to be compact than to be compact, but I was not able to prove it. Can anyone help me?","M \mathcal C^0(M) :=\{f:M\to \mathbb R;\ f\ \text{is continuous}\}. \{P^t : (\mathcal C^0(M),\|\cdot \|_{\infty}) \to \left(\mathcal C^0(M),\|\cdot\|_{\infty}\right)\}_{t\geq 0},  \|P^t\|\leq 1,\ P^0 = \mathrm{Id}\ \ \text{and} \ P^{s+t} = P^t \circ P^s,\ \forall \ s,t\geq 0,  f\in \mathcal C^0(M),\ f\geq 0 \Rightarrow \ P^t f \geq 0,\ \forall\ t\geq 0. K,\alpha >0  \|P^t\|\leq Ke^{-\alpha t}. \begin{align}
G: (\mathcal C^0(M,\|\cdot\|_{\infty}) &\to \left(\mathcal C^0(M),\|\cdot\|_{\infty}\right)\\
f&\mapsto \left(x\mapsto \int_0^\infty \left(P^t f\right)(x)\ \mathrm{d}t\right)
\end{align} G P^t t_0>0 P^{t_0} G P^t","['linear-algebra', 'functional-analysis', 'analysis', 'compact-operators']"
64,Must the radius of an open ball be real? [closed],Must the radius of an open ball be real? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Let $\mathbb{Q}$ be the metric space with $d(p,q) = |p-q|$ . Is the radius of an open ball/a neighborhood of a point $p$ real, or in this case is it rational only? Thank you!","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Let be the metric space with . Is the radius of an open ball/a neighborhood of a point real, or in this case is it rational only? Thank you!","\mathbb{Q} d(p,q) = |p-q| p","['real-analysis', 'analysis']"
65,If $f$ is atomic then int $(f (U)) \neq \emptyset$ (interior). True or False?,If  is atomic then int  (interior). True or False?,f (f (U)) \neq \emptyset,"Let $f:X \to Y$ be a continuous function between continua. If $f$ is atomic then int $(f (U)) \neq  \emptyset$ (interior). I don't know if this conjecture is true. Before presenting my attempt, I will recall some definitions. A map $f : X \to Y$ between continua is said to be atomic if it is surjective and for every subcontinuum $K$ of $X$ such that $f (K)$ is nondegenerate, we have that $K = f ^{−1}(f (K))$ . A map $f : X \to Y$ between continua is atomic if and only if $f ^{−1}(y)$ is a terminal subcontinuum in $X$ for each $y \in Y$ . My attempt: I first demonstrated the following. Lemma 1: Let $f:X \to Y$ be a continuous function between continua. If $U$ is an open of $X$ and $f^{-1} (y) \subseteq U$ for some $y \in Y$ , then $y \in $ int $f(U)$ . Now if i will start my attempt Let $U$ be an open of $ X $ . Let $ L $ be a nondegenerate continuum of $ X $ such that $L \subseteq U$ . If $|f(L)|>1$ . Let $y \in f(L)$ , then $f^{-1}(y) \subseteq f^{-1}(f(L))$ . Since $f$ is atomic then $L =f^{-1}(f(L))$ , thus $f^{-1}(y) \subseteq f^{-1}(f(L))=L \subseteq U$ .  Therefore, by Lemma 1, $y \in $ int $f(U)$ . If $|f(L)|=1$ . This case I have not been able to prove it. Any ideas or a counterexample? Thank you so much.","Let be a continuous function between continua. If is atomic then int (interior). I don't know if this conjecture is true. Before presenting my attempt, I will recall some definitions. A map between continua is said to be atomic if it is surjective and for every subcontinuum of such that is nondegenerate, we have that . A map between continua is atomic if and only if is a terminal subcontinuum in for each . My attempt: I first demonstrated the following. Lemma 1: Let be a continuous function between continua. If is an open of and for some , then int . Now if i will start my attempt Let be an open of . Let be a nondegenerate continuum of such that . If . Let , then . Since is atomic then , thus .  Therefore, by Lemma 1, int . If . This case I have not been able to prove it. Any ideas or a counterexample? Thank you so much.",f:X \to Y f (f (U)) \neq  \emptyset f : X \to Y K X f (K) K = f ^{−1}(f (K)) f : X \to Y f ^{−1}(y) X y \in Y f:X \to Y U X f^{-1} (y) \subseteq U y \in Y y \in  f(U) U  X   L   X  L \subseteq U |f(L)|>1 y \in f(L) f^{-1}(y) \subseteq f^{-1}(f(L)) f L =f^{-1}(f(L)) f^{-1}(y) \subseteq f^{-1}(f(L))=L \subseteq U y \in  f(U) |f(L)|=1,"['general-topology', 'analysis', 'functions', 'separation-axioms', 'continuum-theory']"
66,Prove that $\frac{\partial f}{\partial z}$ is a holomorphic function,Prove that  is a holomorphic function,\frac{\partial f}{\partial z},Let $\Omega \subset \mathbb{C}$ be an open connected subset of $\mathbb{C}$ .Assume that $f: \Omega \rightarrow \mathbb{C}$ is a $C^{2}$ map such that at any point $z \in \Omega$ either $\frac{\partial f}{\partial z}(z)=0$ or $\frac{\partial f}{\partial \bar{z}}(z)=0$ holds. Prove that $\frac{\partial f}{\partial z}: \Omega \rightarrow \mathbb{C}$ is a holomorphic function.(for definition of $\frac{\partial f}{\partial \bar{z}}(z)$ you can see definition of a Holomorphic function ) I try to solve this problem but I'm not sure that my way is right. I first show that if we define $$U=\{ z| z \in \Omega :\frac{\partial f}{\partial z}(z)=0 \} \ \ \ \ \ V=\{ z| z \in \Omega \frac{\partial f}{\partial \bar{z}}(z)=0\}$$ then U and V are open sets from that f is $C^{2}$ .(Is it true?) for points in U we have $\frac{\partial f}{\partial z}(z)=0$ is constant and so holomorphic.for points In $V$ we have $\frac{\partial f}{\partial \bar{z}}(z)=0$ and we have directly from $\frac{\partial f}{\partial \bar{z}}$ that f is holomorphic. then from that f is holomorphic and $C^{2}$ it is easy to see that we have Cauchy–Riemann equations for $\frac{\partial f}{\partial z}$ and so it is holomorphic in $V$ and so it's holomorphic in whole of $\Omega$ . But I think maybe my way is not true because I don't use the connectedness of $\Omega$ and I think maybe we have some problem in $U\cap V$ .,Let be an open connected subset of .Assume that is a map such that at any point either or holds. Prove that is a holomorphic function.(for definition of you can see definition of a Holomorphic function ) I try to solve this problem but I'm not sure that my way is right. I first show that if we define then U and V are open sets from that f is .(Is it true?) for points in U we have is constant and so holomorphic.for points In we have and we have directly from that f is holomorphic. then from that f is holomorphic and it is easy to see that we have Cauchy–Riemann equations for and so it is holomorphic in and so it's holomorphic in whole of . But I think maybe my way is not true because I don't use the connectedness of and I think maybe we have some problem in .,\Omega \subset \mathbb{C} \mathbb{C} f: \Omega \rightarrow \mathbb{C} C^{2} z \in \Omega \frac{\partial f}{\partial z}(z)=0 \frac{\partial f}{\partial \bar{z}}(z)=0 \frac{\partial f}{\partial z}: \Omega \rightarrow \mathbb{C} \frac{\partial f}{\partial \bar{z}}(z) U=\{ z| z \in \Omega :\frac{\partial f}{\partial z}(z)=0 \} \ \ \ \ \ V=\{ z| z \in \Omega \frac{\partial f}{\partial \bar{z}}(z)=0\} C^{2} \frac{\partial f}{\partial z}(z)=0 V \frac{\partial f}{\partial \bar{z}}(z)=0 \frac{\partial f}{\partial \bar{z}} C^{2} \frac{\partial f}{\partial z} V \Omega \Omega U\cap V,"['complex-analysis', 'analysis']"
67,An ordering of $\mathbb{Q}(X)$ which makes it an Archimedean field.,An ordering of  which makes it an Archimedean field.,\mathbb{Q}(X),"The following question is from one of my homework: Show that the field $\mathbb{Q}(X)$ , the field of rational functions over $\mathbb{Q}$ in one indeterminate $X$ , admits the structure of an ordered field having the Archimedean property. What can you say about $\mathbb{Q}(X, Y) ?$ My interpretation of the question is that it asks to define an valid ordering on $\mathbb{Q}(X)$ which has the Archimedean property. I am unable to find such an ordering. I could find some orderings on $\mathbb{Q}(X)$ (for example, to define $f>g$ if $f-g$ has both the numerator and denominator having positive leading coefficients) but all of those failed to satisfy the Archimedean property. Any help is appreciated. Thanks in advance.","The following question is from one of my homework: Show that the field , the field of rational functions over in one indeterminate , admits the structure of an ordered field having the Archimedean property. What can you say about My interpretation of the question is that it asks to define an valid ordering on which has the Archimedean property. I am unable to find such an ordering. I could find some orderings on (for example, to define if has both the numerator and denominator having positive leading coefficients) but all of those failed to satisfy the Archimedean property. Any help is appreciated. Thanks in advance.","\mathbb{Q}(X) \mathbb{Q} X \mathbb{Q}(X, Y) ? \mathbb{Q}(X) \mathbb{Q}(X) f>g f-g","['real-analysis', 'analysis', 'polynomials', 'field-theory', 'rational-functions']"
68,Estimating $\int_0^{2\pi}\left(\sum_{n=1}^{N} \cos (n^2x) \right)^4\ dx$ for large $N$,Estimating  for large,\int_0^{2\pi}\left(\sum_{n=1}^{N} \cos (n^2x) \right)^4\ dx N,"In a paper I am reading, it is claimed without proof that for every $\varepsilon > 0$ and $N > N_0(\varepsilon)$ $$ \int_0^{2\pi}\left(\sum_{n=1}^{N} \cos (n^2x) \right)^4\ dx < N^{2 + \varepsilon} $$ I do not quite see how this follows. The Cauchy-Schwarz inequality shows that $$ \left(\sum_{n=1}^{N} \cos (n^2x) \right)^4 \le N^2\left(\sum_{n=1}^{N} \cos^2(n^2x) \right)^2 \le N^3\left(\sum_{n=1}^{N} \cos^4(n^2x) \right) $$ and hence $$ \int_0^{2\pi}\left(\sum_{n=1}^{N} \cos (n^2x) \right)^4\ dx \le N^3\int_0^{2\pi}\sum_{n=1}^{N} \cos^4(n^2x) = \frac{3}{4}\pi N^4, $$ but this is evidently not good enough. Any help proving the claim is appreciated.","In a paper I am reading, it is claimed without proof that for every and I do not quite see how this follows. The Cauchy-Schwarz inequality shows that and hence but this is evidently not good enough. Any help proving the claim is appreciated.","\varepsilon > 0 N > N_0(\varepsilon) 
\int_0^{2\pi}\left(\sum_{n=1}^{N} \cos (n^2x) \right)^4\ dx < N^{2 + \varepsilon}
 
\left(\sum_{n=1}^{N} \cos (n^2x) \right)^4 \le N^2\left(\sum_{n=1}^{N} \cos^2(n^2x) \right)^2 \le N^3\left(\sum_{n=1}^{N} \cos^4(n^2x) \right)
 
\int_0^{2\pi}\left(\sum_{n=1}^{N} \cos (n^2x) \right)^4\ dx \le N^3\int_0^{2\pi}\sum_{n=1}^{N} \cos^4(n^2x) = \frac{3}{4}\pi N^4,
","['calculus', 'analysis', 'integral-inequality']"
69,how to use union and intersection symbol to denote the limit [closed],how to use union and intersection symbol to denote the limit [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question For any $x \in X$ , $f_n(x) \to \infty$ as $n \to \infty$ . Is this equivalent to saying $\cap_{K=1}^{\infty} \cup_{N=1}^{\infty} \cap_{n=N}^{\infty}   \{x: f_n(x)\geq K\}$ ? Why? My understanding: for any a positive integer K, we can find N, such that when n>N, then $f_n(x)>K$ ?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question For any , as . Is this equivalent to saying ? Why? My understanding: for any a positive integer K, we can find N, such that when n>N, then ?","x \in X f_n(x) \to \infty n \to \infty \cap_{K=1}^{\infty} \cup_{N=1}^{\infty} \cap_{n=N}^{\infty} 
 \{x: f_n(x)\geq K\} f_n(x)>K",['analysis']
70,Is the answer to this truth-statement evaluation wrong? [closed],Is the answer to this truth-statement evaluation wrong? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 2 years ago . Improve this question I have been working through an analysis course and the solutions to some of the exercies presented appear to make no sense. The exercise reads as follows. Determine which of the following are true: $(\forall x \in \mathbb{Z} )(\exists y \in \mathbb{Z})(\frac{x}{y}>0) $ I view the evaluation of such truth statemets a bit like a game where a challenger(usually the statement that appears after the $\forall$ quantifier) picks numbers $x \in \mathbb{Z}$ and I am trying to find(usually whatever appears after the $\exists$ quantifier) a suitable number $y \in \mathbb{Z}$ that satisfies the condition for any possible $x$ the challenger could pick. My answer is as follows. The statement is false. This is because for $x = 0$ there exists no possible $y$ which satisfies $\frac{x}{y}>0$ . At best, we could have $\frac{x}{y} = 0$ which is NOT strictly larger than $0$ . Now, onto main issue. The source I am using appears to disagree entirely with my argument. Their answers are as follows. Please note that I am quoting the solutions verbatim. The first statement is true since one can take y = x. I disagree with this answer, because one can easily pick $ x = 0 $ and run into all sorts of trouble. Have I missed something? Thank you in advance for any help you may provide.","Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 2 years ago . Improve this question I have been working through an analysis course and the solutions to some of the exercies presented appear to make no sense. The exercise reads as follows. Determine which of the following are true: I view the evaluation of such truth statemets a bit like a game where a challenger(usually the statement that appears after the quantifier) picks numbers and I am trying to find(usually whatever appears after the quantifier) a suitable number that satisfies the condition for any possible the challenger could pick. My answer is as follows. The statement is false. This is because for there exists no possible which satisfies . At best, we could have which is NOT strictly larger than . Now, onto main issue. The source I am using appears to disagree entirely with my argument. Their answers are as follows. Please note that I am quoting the solutions verbatim. The first statement is true since one can take y = x. I disagree with this answer, because one can easily pick and run into all sorts of trouble. Have I missed something? Thank you in advance for any help you may provide.",(\forall x \in \mathbb{Z} )(\exists y \in \mathbb{Z})(\frac{x}{y}>0)  \forall x \in \mathbb{Z} \exists y \in \mathbb{Z} x x = 0 y \frac{x}{y}>0 \frac{x}{y} = 0 0  x = 0 ,"['real-analysis', 'analysis', 'logic']"
71,Finding functions $g$ such that the Fourier transform of $g(x+c)g(x)$ does not vanish,Finding functions  such that the Fourier transform of  does not vanish,g g(x+c)g(x),"Suppose that $f$ is a function of the form $$ f(x) = g(x+c)g(x), $$ i.e. $f$ is a product of $g$ with a shifted $g$ . If $g$ is a Gaussian, so is $f$ for every $c \in \mathbb R$ . In particular, the Fourier transform of $f$ does not vanish. Does there exist other functions with this property or even a class of functions $g$ such that the Fourier transform of $x \mapsto g(x+c)g(x)$ does not vanish for every $c$ ?","Suppose that is a function of the form i.e. is a product of with a shifted . If is a Gaussian, so is for every . In particular, the Fourier transform of does not vanish. Does there exist other functions with this property or even a class of functions such that the Fourier transform of does not vanish for every ?","f 
f(x) = g(x+c)g(x),
 f g g g f c \in \mathbb R f g x \mapsto g(x+c)g(x) c","['functional-analysis', 'analysis', 'fourier-analysis', 'fourier-transform']"
72,A CDF related inequality,A CDF related inequality,,"Suppose $F$ and $G$ are two cdf over $[0, 1]$ . Let $$z \equiv \frac{\int_{0}^1 x (1 - G(x)) \mathrm{d}F(x)}{\int_{0}^1 (1 - G(x)) \mathrm{d}F(x) }.$$ Does the following inequality hold: $$ \int_z^1 (y - z) \mathrm{d}G(y) \geq \int_0^1 \int_x^1 (y - x) \mathrm{d} G(y) \mathrm{d} F(x) \,\, ?$$ If we define function $r(x) \equiv \int_x^1 (y - x) \mathrm{d} G(y)$ , the desired inequality can be stated as $r(z) \geq \int_0^1 r(x) \mathrm{d} F(x)$ . Although I know that $r$ is decreasing and convex, but yet have no idea how to proceed. Thank you very much!","Suppose and are two cdf over . Let Does the following inequality hold: If we define function , the desired inequality can be stated as . Although I know that is decreasing and convex, but yet have no idea how to proceed. Thank you very much!","F G [0, 1] z \equiv \frac{\int_{0}^1 x (1 - G(x)) \mathrm{d}F(x)}{\int_{0}^1 (1 - G(x)) \mathrm{d}F(x) }.  \int_z^1 (y - z) \mathrm{d}G(y) \geq \int_0^1 \int_x^1 (y - x) \mathrm{d} G(y) \mathrm{d} F(x) \,\, ? r(x) \equiv \int_x^1 (y - x) \mathrm{d} G(y) r(z) \geq \int_0^1 r(x) \mathrm{d} F(x) r","['probability', 'analysis', 'inequality']"
73,Is there any situation where it is interesting to rewrite the terms of a sequence as the partial sums of a series?,Is there any situation where it is interesting to rewrite the terms of a sequence as the partial sums of a series?,,"Any sequence $(a_n)\in\mathbb C^\mathbb N$ can always be re-written as a sequence of partial sums $\left(\sum_{k=0}^n u_k\right)$ for some sequence $(u_k)\in\mathbb C^\mathbb N$ (for instance, take $u_0=a_0$ and $u_n=a_n-a_{n-1}$ when $n\geq 1$ ). My question is, are there any situations where this transformation would make it easier to study the sequence $(a_n)$ ? For example, are there any cases where this method would make it easier to determine if $(a_n)$ is convergent or divergent, or to compute its limit?","Any sequence can always be re-written as a sequence of partial sums for some sequence (for instance, take and when ). My question is, are there any situations where this transformation would make it easier to study the sequence ? For example, are there any cases where this method would make it easier to determine if is convergent or divergent, or to compute its limit?",(a_n)\in\mathbb C^\mathbb N \left(\sum_{k=0}^n u_k\right) (u_k)\in\mathbb C^\mathbb N u_0=a_0 u_n=a_n-a_{n-1} n\geq 1 (a_n) (a_n),"['sequences-and-series', 'limits', 'analysis']"
74,If the derivative of $f:R^m\to R^m$ is isometric then $f$ is isometric,If the derivative of  is isometric then  is isometric,f:R^m\to R^m f,"I can't finish this problem Let $f:R^m\to R^m$ be a $C^1$ function such that for all $x\in R^m$, $f'(x):R^m\to R^m$ is an isometry i.e. $|f'(x)v|=|v|$ for all $v\in R^m$. Prove that $f$ is an isometry, i.e. that $|f(x)-f(y)|=|x-y|$ for all $x,y\in R^m$. Conclude that there is a linear isometry $T:R^m\to R^m$ and $a\in R^m$ such that $f(x)=Tx+a$ for all $x\in R^m$. This is what I've done: Since $|f'(x)v|=|v|$ for all $x,v$ we have that $|f'(x)|=1$. The mean value inequality then says that $|f(x)-f(y)|\le 1\cdot |x-y|=|x-y|$. Ideally one would like to apply the previous inequality with $f^{-1}$ in place of $f$ but we don't know that $f$ is bijective. I tried to use the inverse function theorem but it only assures the inverse of $f$ when $f$ is restricted to some open sets. So how would one prove that $f$ is bijective?. After that I think i can finish the problem.","I can't finish this problem Let $f:R^m\to R^m$ be a $C^1$ function such that for all $x\in R^m$, $f'(x):R^m\to R^m$ is an isometry i.e. $|f'(x)v|=|v|$ for all $v\in R^m$. Prove that $f$ is an isometry, i.e. that $|f(x)-f(y)|=|x-y|$ for all $x,y\in R^m$. Conclude that there is a linear isometry $T:R^m\to R^m$ and $a\in R^m$ such that $f(x)=Tx+a$ for all $x\in R^m$. This is what I've done: Since $|f'(x)v|=|v|$ for all $x,v$ we have that $|f'(x)|=1$. The mean value inequality then says that $|f(x)-f(y)|\le 1\cdot |x-y|=|x-y|$. Ideally one would like to apply the previous inequality with $f^{-1}$ in place of $f$ but we don't know that $f$ is bijective. I tried to use the inverse function theorem but it only assures the inverse of $f$ when $f$ is restricted to some open sets. So how would one prove that $f$ is bijective?. After that I think i can finish the problem.",,"['analysis', 'derivatives']"
75,Is finite the limit of $(x_n)_n$?,Is finite the limit of ?,(x_n)_n,"Let $ (x_n)_n$ and $ (y_n)_n$ two sequences such that $x_1 =1$ , $ y_1 =4$ and $ x_{n+1}= \frac{ 4+ 2 x_n + x_n y_n}{y_n}$ , $ y_{n+1}= \frac{ 4+ 2 y_n + x_n y_n}{x_n}$ . Find the limits of $ (x_n)_n$ and $ (y_n)_n$ . I seen that the sequences are increasing and that the limit of $(y_n)_n$ is infinite. In the same time $ y_n \geq x_n $ . I can't to decide if the limit of $(x_n)_n$ is finite or infinite.","Let and two sequences such that , and , . Find the limits of and . I seen that the sequences are increasing and that the limit of is infinite. In the same time . I can't to decide if the limit of is finite or infinite.", (x_n)_n  (y_n)_n x_1 =1  y_1 =4  x_{n+1}= \frac{ 4+ 2 x_n + x_n y_n}{y_n}  y_{n+1}= \frac{ 4+ 2 y_n + x_n y_n}{x_n}  (x_n)_n  (y_n)_n (y_n)_n  y_n \geq x_n  (x_n)_n,"['real-analysis', 'analysis']"
76,Lagrange Multiplier in higher codimension,Lagrange Multiplier in higher codimension,,"Consider the Lagrange Theorem: Let $U \subset \mathbb{R}^{n+k}$ be an open set, $f:U \rightarrow \mathbb{R}$ of class $C^1$ , $c \in \mathbb{R}^k$ be a regular value of $g$ and $M = g^{-1}(c)$ . Then $$x \ \text{is a critical point of } \ f|_M \Longleftrightarrow \exists\ \lambda \in \mathbb{R}^k  \ \text{with}\ \nabla f(x) = \lambda^T \nabla g'(x_0).$$ First: Does anyone has books on the subject? I search for it on Spivak's book and Rudin, but did't find anything. i would appreciate any help! Second: Consider the function $$\mathcal{L}(x,\lambda) = f(x) -\lambda^T (g(x)-c)$$ i must show that $(x_0,\lambda_0)$ is a critical point of $\mathcal{L}(x,y)$ if, and only if, $x_0$ is a critical point for $f|_{g(x)=c}$ .  I'm having some troubles to show this. Any hint on this exercise will be very helpful!","Consider the Lagrange Theorem: Let be an open set, of class , be a regular value of and . Then First: Does anyone has books on the subject? I search for it on Spivak's book and Rudin, but did't find anything. i would appreciate any help! Second: Consider the function i must show that is a critical point of if, and only if, is a critical point for .  I'm having some troubles to show this. Any hint on this exercise will be very helpful!","U \subset \mathbb{R}^{n+k} f:U \rightarrow \mathbb{R} C^1 c \in \mathbb{R}^k g M = g^{-1}(c) x \ \text{is a critical point of } \ f|_M \Longleftrightarrow \exists\ \lambda \in \mathbb{R}^k  \ \text{with}\ \nabla f(x) = \lambda^T \nabla g'(x_0). \mathcal{L}(x,\lambda) = f(x) -\lambda^T (g(x)-c) (x_0,\lambda_0) \mathcal{L}(x,y) x_0 f|_{g(x)=c}","['real-analysis', 'analysis', 'lagrange-multiplier']"
77,Continuity of minimum of a family of continuous functions on a compact space,Continuity of minimum of a family of continuous functions on a compact space,,"I have come up with the following problem, which I am sure has a simple solution, but I have not been able to find any until now, nor did I found any reference in literature: Let $f:\mathbb{R}\times K\rightarrow \mathbb{R}$ be a continuous function, with $K$ a compact space. For every $t\in\mathbb{R}$ , define $f_t:K\rightarrow\mathbb{R}$ as $f_t(x)=f(t,x)$ . Is it true that $\mathrm{min}_{x\in K}f_t(x)$ is continuous as a function $\mathbb{R}\rightarrow\mathbb{R}$ of the $t$ variable? Is the hypothesis on $K$ sufficient, or are more restrictive ones necessary, e.g. requiring that $K$ be a compact metric space? I was thinking about using the uniform continuity of the $f_t$ , or using some finite cover of $K$ by sufficiently small open balls (in which case it is necessary to assume that K is a metric space), or maybe even a proof by contradiction, but there simply does not seem to be any reasonable way to control the ""closeness"" of the minimum points of $f_t$ for different values of $t$ (especially because they might vary in cardinality). Any help would be much appreciated, thank you!","I have come up with the following problem, which I am sure has a simple solution, but I have not been able to find any until now, nor did I found any reference in literature: Let be a continuous function, with a compact space. For every , define as . Is it true that is continuous as a function of the variable? Is the hypothesis on sufficient, or are more restrictive ones necessary, e.g. requiring that be a compact metric space? I was thinking about using the uniform continuity of the , or using some finite cover of by sufficiently small open balls (in which case it is necessary to assume that K is a metric space), or maybe even a proof by contradiction, but there simply does not seem to be any reasonable way to control the ""closeness"" of the minimum points of for different values of (especially because they might vary in cardinality). Any help would be much appreciated, thank you!","f:\mathbb{R}\times K\rightarrow \mathbb{R} K t\in\mathbb{R} f_t:K\rightarrow\mathbb{R} f_t(x)=f(t,x) \mathrm{min}_{x\in K}f_t(x) \mathbb{R}\rightarrow\mathbb{R} t K K f_t K f_t t",['analysis']
78,"Relation between an ""algebra"" in Analysis and in Algebra","Relation between an ""algebra"" in Analysis and in Algebra",,"While studying some measure theory, I came across the following two definitions, given a set $\mathbb{X}$ : A ring is a non-empty subset of $\mathcal{P}(\mathbb{X})$ such that it is closed for unions and set-differences An algebra is a non-empty subset of $\mathcal{P}(\mathbb{X})$ such that it is a ring and contains $\mathbb{X}$ ; equivalently, it is closed for unions and complements This immediately got me thinking about the algebraic implications. Given a former question of mine on this site, I saw that the first definition actually said such a set was a subring of $\mathcal{P}(\mathbb{X})$ , with the operations $A \Delta B = (A\setminus B) \cup (B \setminus A)$ and $A\cup B$ . But this left me with the second one: algebras, as far as I've seen, are defined over fields. So what field is that algebra talking about? If there isn't any, why is the term ""algebra"" used? As a side note, is there an Algebraic equivalent of "" $\sigma$ -algebras"" and "" $\sigma$ -rings"", without talking about the particular context of subrings of $\mathcal{P}(\mathbb{X})$ ? Thanks in advance!","While studying some measure theory, I came across the following two definitions, given a set : A ring is a non-empty subset of such that it is closed for unions and set-differences An algebra is a non-empty subset of such that it is a ring and contains ; equivalently, it is closed for unions and complements This immediately got me thinking about the algebraic implications. Given a former question of mine on this site, I saw that the first definition actually said such a set was a subring of , with the operations and . But this left me with the second one: algebras, as far as I've seen, are defined over fields. So what field is that algebra talking about? If there isn't any, why is the term ""algebra"" used? As a side note, is there an Algebraic equivalent of "" -algebras"" and "" -rings"", without talking about the particular context of subrings of ? Thanks in advance!",\mathbb{X} \mathcal{P}(\mathbb{X}) \mathcal{P}(\mathbb{X}) \mathbb{X} \mathcal{P}(\mathbb{X}) A \Delta B = (A\setminus B) \cup (B \setminus A) A\cup B \sigma \sigma \mathcal{P}(\mathbb{X}),"['analysis', 'measure-theory', 'ring-theory', 'algebras']"
79,Traffic Dynamics. How to fix initial condition deficiency?,Traffic Dynamics. How to fix initial condition deficiency?,,"Traffic in a tunnel . A rather realistic model for the car speed in a very long tunnel is the following: $$ v(\rho)=\left\{\begin{matrix} v_m & 0 \leq \rho \leq \rho_c\\   \lambda log(\frac{\rho_m}{\rho}) &  \rho_c \leq \rho \leq \rho_m \end{matrix}\right.$$ where $ \lambda =\frac{v_m}{log(\rho_m / \rho_c)}.$ Observe that $v$ is continuous also at $$\rho_c=\rho_m e^{-v_m /\lambda},$$ which represents a critical density: if $\rho \leq \rho_c$ the drivers are free to reach the speed limit. Typical values are: $\rho_c=7car/Km$ , $v_m=90 Km/h$ , $\rho_m=110 car/Km$ , $v_m/\lambda=2.75.$ Assume the entrance is placed at $x=0$ that the cars are waiting (with maximum density) the tunnel to open to the traffic at time $t=0$ . Thus, the initial density is $$\rho=\begin{cases}  \rho _m& \text{ if } x<0 \\   0 & \text{ if } x>0  \end{cases}$$ a. Determine density and car speed; draw their graphs. b. Determine and draw in the $x$ , $t$ plane the trajectory of a car initially at $x = x_0 < 0$ ,and compute the time it takes to enter the tunnel. I really don't know where to start, my procedure to find density is using the characteristics method, first reformulating the initial condition as $$g_\epsilon(x_0)= \begin{cases}  \rho _m&  x_0<0 \\   \rho_m(1-\frac{x_0}{\epsilon}) & 0<x_0<\epsilon\\ 0 & x_0>0 \end{cases}$$ then, using the conservation of matter equation $ \rho_t + q (\rho) _x = 0$ with $q (\rho) = v (\rho) \rho $ replace the value of $ v (\rho) $ that gives the exercise in $ q (\rho) $ , however I don't know very well how the function is defined in pieces. Then my idea is to evaluate $ g_\epsilon (x_0) $ in $ q (\rho) $ but since I have not defined the function well by parts I do not know how to replace. thanks!! This exercise is in the book named Partial Differential Equation in Action of Sandro Salsa, third edition, Page. 253 - exercise 4.6","Traffic in a tunnel . A rather realistic model for the car speed in a very long tunnel is the following: where Observe that is continuous also at which represents a critical density: if the drivers are free to reach the speed limit. Typical values are: , , , Assume the entrance is placed at that the cars are waiting (with maximum density) the tunnel to open to the traffic at time . Thus, the initial density is a. Determine density and car speed; draw their graphs. b. Determine and draw in the , plane the trajectory of a car initially at ,and compute the time it takes to enter the tunnel. I really don't know where to start, my procedure to find density is using the characteristics method, first reformulating the initial condition as then, using the conservation of matter equation with replace the value of that gives the exercise in , however I don't know very well how the function is defined in pieces. Then my idea is to evaluate in but since I have not defined the function well by parts I do not know how to replace. thanks!! This exercise is in the book named Partial Differential Equation in Action of Sandro Salsa, third edition, Page. 253 - exercise 4.6"," v(\rho)=\left\{\begin{matrix}
v_m & 0 \leq \rho \leq \rho_c\\ 
 \lambda log(\frac{\rho_m}{\rho}) &  \rho_c \leq \rho \leq \rho_m
\end{matrix}\right.  \lambda =\frac{v_m}{log(\rho_m / \rho_c)}. v \rho_c=\rho_m e^{-v_m /\lambda}, \rho \leq \rho_c \rho_c=7car/Km v_m=90 Km/h \rho_m=110 car/Km v_m/\lambda=2.75. x=0 t=0 \rho=\begin{cases}
 \rho _m& \text{ if } x<0 \\ 
 0 & \text{ if } x>0 
\end{cases} x t x = x_0 < 0 g_\epsilon(x_0)=
\begin{cases}
 \rho _m&  x_0<0 \\ 
 \rho_m(1-\frac{x_0}{\epsilon}) & 0<x_0<\epsilon\\
0 & x_0>0
\end{cases}  \rho_t + q (\rho) _x = 0 q (\rho) = v (\rho) \rho   v (\rho)   q (\rho)   g_\epsilon (x_0)   q (\rho) ","['analysis', 'mathematical-modeling', 'transport-equation']"
80,Why can't this be a simple solution to Baby Rudin 9.12.d?,Why can't this be a simple solution to Baby Rudin 9.12.d?,,"Fix two real numbers $a$ and $b$ , $0<a<b$ . Define $f: \mathbb{R}^2 \rightarrow \mathbb{R}^3$ by $$f_1(s,t) = (b+a \cos s) \cos t$$ $$f_2 (s,t) = (b+a \cos s ) \sin t$$ $$f_3(s,t) = a \sin s.$$ $f$ maps $\mathbb{R}$ onto the torus, $K = f(\mathbb{R})$ . Let $\lambda$ be an irrational. Define $g:\mathbb{R} \rightarrow K$ by $g(t) = f(t, \lambda t )$ . Prove that the range of $g$ is a dense subset of $K$ . I know that this can proven using the Kronecker approximation theorem, as shown here A mapping from $\mathbb{R}^1$ to a dense subset of the surface of torus in $\mathbb{R}^3$ . My question is can't we simply use continuous functions to prove this. Define $h: \mathbb{R} \rightarrow \mathbb{R}^2$ by $h(t) = (t, \lambda t)$ . $h$ is continuous. We also know that $f$ is continuous, so any restriction of $f$ is continuous. Then $g(t) = f(h(t))$ is continuous. Then, by problem 4.4 in Rudin, if $g$ is a continuous mapping of a metric space $X$ into a metric space $Y$ and $E$ is a dense subset of $X$ , then $g(E)$ is dense in $g(X)$ . Since $\mathbb{R}$ is dense in $\mathbb{R}$ , and $g$ is continuous, then $g(\mathbb{R})$ is dense in $K$ . Where is my mistake? Or is this valid? EDIT: The mistake is in the last sentence as pointed out by Salcio in the comments.","Fix two real numbers and , . Define by maps onto the torus, . Let be an irrational. Define by . Prove that the range of is a dense subset of . I know that this can proven using the Kronecker approximation theorem, as shown here A mapping from $\mathbb{R}^1$ to a dense subset of the surface of torus in $\mathbb{R}^3$ . My question is can't we simply use continuous functions to prove this. Define by . is continuous. We also know that is continuous, so any restriction of is continuous. Then is continuous. Then, by problem 4.4 in Rudin, if is a continuous mapping of a metric space into a metric space and is a dense subset of , then is dense in . Since is dense in , and is continuous, then is dense in . Where is my mistake? Or is this valid? EDIT: The mistake is in the last sentence as pointed out by Salcio in the comments.","a b 0<a<b f: \mathbb{R}^2 \rightarrow \mathbb{R}^3 f_1(s,t) = (b+a \cos s) \cos t f_2 (s,t) = (b+a \cos s ) \sin t f_3(s,t) = a \sin s. f \mathbb{R} K = f(\mathbb{R}) \lambda g:\mathbb{R} \rightarrow K g(t) = f(t, \lambda t ) g K h: \mathbb{R} \rightarrow \mathbb{R}^2 h(t) = (t, \lambda t) h f f g(t) = f(h(t)) g X Y E X g(E) g(X) \mathbb{R} \mathbb{R} g g(\mathbb{R}) K","['analysis', 'continuity']"
81,"$f \in L^1$, but $f \not\in L^p$ for all $p > 1$",", but  for all",f \in L^1 f \not\in L^p p > 1,"""Find an $f \in [0,1]$ such that $f \in L^1$ but $f \not\in L^p$ for any $p > 1$."" I've thought about doing something like $$f(x) = \frac{1}{x}$$ where $|f|^p = \frac{1}{x^p}$ doesn't converge when $p > 1$.  But this function isn't itself in $L^1$.  Could someone please give me a hint for how to solve this problem?  I wish there were a situation where you had convergence on the closed half disc $[0,1]$ and divergence on $(1, \infty)$, rather than my current predicament where I have convergence on the open half-disc $[0, 1)$ and divergence on $[1, \infty)$.","""Find an $f \in [0,1]$ such that $f \in L^1$ but $f \not\in L^p$ for any $p > 1$."" I've thought about doing something like $$f(x) = \frac{1}{x}$$ where $|f|^p = \frac{1}{x^p}$ doesn't converge when $p > 1$.  But this function isn't itself in $L^1$.  Could someone please give me a hint for how to solve this problem?  I wish there were a situation where you had convergence on the closed half disc $[0,1]$ and divergence on $(1, \infty)$, rather than my current predicament where I have convergence on the open half-disc $[0, 1)$ and divergence on $[1, \infty)$.",,"['real-analysis', 'lebesgue-integral', 'lp-spaces']"
82,limit of quotient of two series,limit of quotient of two series,,"Let $Y_{n} > 0$ for all $ n\in \mathbb{N} $ , with $\sum{Y_{n}}= +\infty$ . If $\displaystyle\lim\limits_{n\rightarrow \infty}\frac{X_{n}}{Y_{n}}= a$ then $\displaystyle\lim\limits_{n\rightarrow\infty}\frac{X_{1}+X_{2}+X_{3}+\dots+X_{n}}{Y_{1}+Y_{2}+Y_{3}+\dots+Y_{n}}= a$ I need a idea for solution. Can I supose that $\lim\limits_{n\rightarrow\infty}Y_{n}$ exist?","Let for all , with . If then I need a idea for solution. Can I supose that exist?",Y_{n} > 0  n\in \mathbb{N}  \sum{Y_{n}}= +\infty \displaystyle\lim\limits_{n\rightarrow \infty}\frac{X_{n}}{Y_{n}}= a \displaystyle\lim\limits_{n\rightarrow\infty}\frac{X_{1}+X_{2}+X_{3}+\dots+X_{n}}{Y_{1}+Y_{2}+Y_{3}+\dots+Y_{n}}= a \lim\limits_{n\rightarrow\infty}Y_{n},"['sequences-and-series', 'limits']"
83,Proving an upper bound on trigonometric polynomial,Proving an upper bound on trigonometric polynomial,,"Let $k$ be a natural number. Apparently it is possible to show that there is a choice of $0<n_1\leq n_2\leq \cdots \leq n_k$ with $n_i$ integers such that as $k\rightarrow \infty,$ $$ \max_{\theta \in (0,2\pi]} \left|\sum_{i=1}^k \sin n_i \theta\right|\leq ck^{2/3}$$ for some constant $c>0,$ but I cannot find a reference. Edit: Regarding the comment, I am happy with even a sequence $(n_i)$ which changes with $k$ , to start with. For background see this MO question here","Let be a natural number. Apparently it is possible to show that there is a choice of with integers such that as for some constant but I cannot find a reference. Edit: Regarding the comment, I am happy with even a sequence which changes with , to start with. For background see this MO question here","k 0<n_1\leq n_2\leq \cdots \leq n_k n_i k\rightarrow \infty, 
\max_{\theta \in (0,2\pi]} \left|\sum_{i=1}^k \sin n_i \theta\right|\leq ck^{2/3} c>0, (n_i) k","['analysis', 'inequality', 'trigonometric-series']"
84,Fourier coefficients and series for $x\sin(x)$,Fourier coefficients and series for,x\sin(x),"Let $$g(x)=x \cdot \sin x,$$ $x\in [-\pi,\pi)$ ( $2\pi$ -period). Find the Fourier coefficients $c_n(g)$ for all $n\in\mathbb{N}$ and write out the Fourier series for $g$ . I normally understood this as $c_n$ would be defined as $$c_n=\left\{\begin{matrix} {a_0}/{2} & n=0 \\  (a_n-ib_n)/2 & n=1,2,...\\  (a_{-n}+ib_{-n})/2 & n=-1,-2,...  \end{matrix}\right.$$ for the Fourier series $$f(t)=\frac{a_0}{2}+\sum_{n=1}^{\infty}\left [ a_n\cos nt +b_n\sin nt \right ]=\sum_{n=-\infty}^{\infty}c_ne^{int}$$ Now I am stuck. I am not sure on how to find these coefficients for all $n\in\mathbb{N}$ and afterwards writing up the series when from the definition of $c_n$ it appears that $c_n$ will vary.",Let ( -period). Find the Fourier coefficients for all and write out the Fourier series for . I normally understood this as would be defined as for the Fourier series Now I am stuck. I am not sure on how to find these coefficients for all and afterwards writing up the series when from the definition of it appears that will vary.,"g(x)=x \cdot \sin x, x\in [-\pi,\pi) 2\pi c_n(g) n\in\mathbb{N} g c_n c_n=\left\{\begin{matrix}
{a_0}/{2} & n=0 \\ 
(a_n-ib_n)/2 & n=1,2,...\\ 
(a_{-n}+ib_{-n})/2 & n=-1,-2,... 
\end{matrix}\right. f(t)=\frac{a_0}{2}+\sum_{n=1}^{\infty}\left [ a_n\cos nt +b_n\sin nt \right ]=\sum_{n=-\infty}^{\infty}c_ne^{int} n\in\mathbb{N} c_n c_n","['real-analysis', 'sequences-and-series']"
85,"Prob. 25, Chap. 2, in Royden's REAL ANALYSIS: The assumption that $m\left(B_1\right)<\infty$ is necessary in ...","Prob. 25, Chap. 2, in Royden's REAL ANALYSIS: The assumption that  is necessary in ...",m\left(B_1\right)<\infty,"Here is Prob. 25, Chap. 2, in the book Real Analysis by H. L. Royden and P. M. Fitzpatrick, 4th edition: Show that the assumption that $m\left( B_1 \right) < \infty$ is necessary in part (ii) of the theorem regarding continuity of measure. Here is the theorem regarding continuity of measure (i.e. Theorem 15) in Royden: Lebesgue measure possesses the following continuity properties: (i) If $\left\{ A_k \right\}_{k=1}^\infty$ is an ascending collection of measurable sets, then $$ m \left( \bigcup_{k=1}^\infty A_k \right) = \lim_{k \to \infty} m \left( A_k \right). $$ (ii) If $\left\{ B_k \right\}_{k=1}^\infty$ is a descending collection of measurable sets and $m \left( B_1 \right) < \infty$ , then $$ m \left( \bigcap_{k=1}^\infty B_k \right) = \lim_{k \to \infty} m \left( B_k \right). $$ My Attempt: For each positive integer $k$ , let us put $$  B_k := \big( k, \infty \big) = \big\{ x \in \mathbb{R} \mid k < x < \infty \big\}.  $$ Then, for each $k$ , we have $$ B_k = \big( k, \infty \big) \supset \big( k+1, \infty \big) = B_{k+1}. $$ And, by Proposition 8 every set $B_k$ , being an interval, is measurable, and by Proposition 1 we have $$ m \left( B_k \right) = m^* \left( B_k \right) = l \left( B_k \right) = \infty. $$ Therefore $$ \lim_{k \to \infty} m \left( B_k \right) = \infty. \tag{1} $$ On the other hand, we note that $$ \bigcap_{k=1}^\infty B_k = \emptyset,  $$ because for every real number $a$ we can find a positive integer $k > a$ , and thus $a \not\in B_k$ for any such $k$ ; therefore we obtain $$ m \left( \bigcap_{k=1}^\infty B_k \right) = 0. \tag{2} $$ From (1) and (2) we conclude that part (ii) of Theorem 15 does not hold for this particular countable collection of measurable sets for which $m \left( B_1 \right) \not< \infty$ . Is my proof correct and clear enough? If not, then where are the issues?","Here is Prob. 25, Chap. 2, in the book Real Analysis by H. L. Royden and P. M. Fitzpatrick, 4th edition: Show that the assumption that is necessary in part (ii) of the theorem regarding continuity of measure. Here is the theorem regarding continuity of measure (i.e. Theorem 15) in Royden: Lebesgue measure possesses the following continuity properties: (i) If is an ascending collection of measurable sets, then (ii) If is a descending collection of measurable sets and , then My Attempt: For each positive integer , let us put Then, for each , we have And, by Proposition 8 every set , being an interval, is measurable, and by Proposition 1 we have Therefore On the other hand, we note that because for every real number we can find a positive integer , and thus for any such ; therefore we obtain From (1) and (2) we conclude that part (ii) of Theorem 15 does not hold for this particular countable collection of measurable sets for which . Is my proof correct and clear enough? If not, then where are the issues?","m\left( B_1 \right) < \infty \left\{ A_k \right\}_{k=1}^\infty 
m \left( \bigcup_{k=1}^\infty A_k \right) = \lim_{k \to \infty} m \left( A_k \right).
 \left\{ B_k \right\}_{k=1}^\infty m \left( B_1 \right) < \infty 
m \left( \bigcap_{k=1}^\infty B_k \right) = \lim_{k \to \infty} m \left( B_k \right).
 k  
B_k := \big( k, \infty \big) = \big\{ x \in \mathbb{R} \mid k < x < \infty \big\}. 
 k 
B_k = \big( k, \infty \big) \supset \big( k+1, \infty \big) = B_{k+1}.
 B_k 
m \left( B_k \right) = m^* \left( B_k \right) = l \left( B_k \right) = \infty.
 
\lim_{k \to \infty} m \left( B_k \right) = \infty. \tag{1}
 
\bigcap_{k=1}^\infty B_k = \emptyset, 
 a k > a a \not\in B_k k 
m \left( \bigcap_{k=1}^\infty B_k \right) = 0. \tag{2}
 m \left( B_1 \right) \not< \infty","['real-analysis', 'analysis', 'measure-theory', 'solution-verification', 'measurable-sets']"
86,$K= \overline{conv(E)}$ iff for every continuous linear functional $f:X\rightarrow \mathbb{R} $ we have $ \sup_{x \in K}f(x)= \sup_{x \in E}f(x)$,iff for every continuous linear functional  we have,K= \overline{conv(E)} f:X\rightarrow \mathbb{R}   \sup_{x \in K}f(x)= \sup_{x \in E}f(x),"We consider $X$ a normed space, $K$ a compact and convex subset of $X$ and $E \subseteq K$ . We need to show that the following are equivalent: $K= \overline{conv(E)}$ for every continuous linear functional $f:X\rightarrow \mathbb{R} $ we have $ \sup_{x \in K}f(x)= \sup_{x \in E}f(x)$ Also, by $conv(E)$ , we denote the convex hull of $E$ , meaning the set of all convex combinations of points in $E$ . I know that $K = conv(Ext(K))$ , by the Krein-Milman theorem, where $Ext(K)$ is the set of the extreme points of $K$ . I don't know where to go from there. Any help would be greatly appreciated.","We consider a normed space, a compact and convex subset of and . We need to show that the following are equivalent: for every continuous linear functional we have Also, by , we denote the convex hull of , meaning the set of all convex combinations of points in . I know that , by the Krein-Milman theorem, where is the set of the extreme points of . I don't know where to go from there. Any help would be greatly appreciated.",X K X E \subseteq K K= \overline{conv(E)} f:X\rightarrow \mathbb{R}   \sup_{x \in K}f(x)= \sup_{x \in E}f(x) conv(E) E E K = conv(Ext(K)) Ext(K) K,['functional-analysis']
87,Does $L^1(\mathbb R)$ implies $\limsup_{n\to\infty} f(nx)<\infty$ for almost every $x\in\mathbb R$?,Does  implies  for almost every ?,L^1(\mathbb R) \limsup_{n\to\infty} f(nx)<\infty x\in\mathbb R,"Question: Let $f\in L^1(\mathbb R)$ be non-negative. Is the proposition $$ \limsup_{n\to\infty} f(nx)<\infty $$ true for almost every $x$ in $\mathbb R$ ? What I have been trying to prove is $$ \limsup_{n\to\infty} a_n f(nx)<\infty,\quad \text{a.e. }x\in\mathbb R $$ for each real-values sequence $\{a_n\}$ that decreases to $0$ . I think Fubini's theorem might help, but I have no way of doing that.","Question: Let be non-negative. Is the proposition true for almost every in ? What I have been trying to prove is for each real-values sequence that decreases to . I think Fubini's theorem might help, but I have no way of doing that.","f\in L^1(\mathbb R) 
\limsup_{n\to\infty} f(nx)<\infty
 x \mathbb R 
\limsup_{n\to\infty} a_n f(nx)<\infty,\quad \text{a.e. }x\in\mathbb R
 \{a_n\} 0","['real-analysis', 'analysis', 'lebesgue-integral', 'lebesgue-measure']"
88,Applications of the Generalized Stokes Theorem?,Applications of the Generalized Stokes Theorem?,,"It states that for a $k$ -dimensional manifold (with boundary) $M$ and $k-1$ -differential form $f$ we have $$\int_{\partial M}f = \int_M df.$$ Most of textbooks spend significant effort to prove the general version and then derive the classical low-dimensional version, which are known for their applications in classical physics. However, I was curious whether the generalized version has some interesting applications, including in mathematics, except proving Brouwer-fixed point theorem.","It states that for a -dimensional manifold (with boundary) and -differential form we have Most of textbooks spend significant effort to prove the general version and then derive the classical low-dimensional version, which are known for their applications in classical physics. However, I was curious whether the generalized version has some interesting applications, including in mathematics, except proving Brouwer-fixed point theorem.",k M k-1 f \int_{\partial M}f = \int_M df.,"['analysis', 'stokes-theorem', 'manifolds-with-boundary']"
89,Does this Optimization paper contain a mistake?,Does this Optimization paper contain a mistake?,,"I am reading the popular paper Stochastic ADMM by Hua Ouyang et. al. I have redone one of their calculations many times, and I keep coming up with a different answer. It is much more likely that I am incorrect rather than the paper, but could somebody please take a look? Thank you very much. The problem is \begin{align*} &\text{minimize } \ \  \theta_1(x) + \theta_2(y)\\ & \text{subject to } \ \  Ax + By = b, x \in \mathcal{X} \end{align*} The augmented Lagrangian, as given on page 3 (bottom left) is $$L_{\beta}(x, y, \lambda) = \theta_1(x) + \theta_2(y) + \lambda^T(Ax + By - b) + \frac \beta 2 ||Ax + By - b||^2.$$ Shortly after this, it claimed that the x-subproblem $$\arg \min_{x \in \mathcal{X}} L_{\beta}(x, y^k, \lambda^k)$$ (where $y^k, \lambda^k$ are the $k^{th}$ iterates) can be written as $$\arg \min_{x \in \mathcal{X}} \theta_1(x) + \frac \beta 2||Ax + By^k - b - \frac 1 \beta \lambda^k ||^2.$$ In my calculations, I keep getting $+ \frac 1 \beta \lambda^k$ instead of $- \frac 1 \beta \lambda^k$ . My Calculation Here, I took away the superscript $k$ and the subscript $x \in \mathcal{X}$ for convenience. I also add and neglect constant terms without explicitly saying so. \begin{align*} \arg \min_x L_{\beta}(x, y, \lambda) &= \arg \min _x \theta_1(x) + \theta_2(y) + \lambda^T (Ax + By - b) + \frac \beta 2||Ax+By-b||^2\\ &= \arg \min_x \theta_1(x) + \lambda^TAx + \frac \beta 2(x^TA^TAx + 2y^TB^TAx - 2b^TAx )\\ &= \arg \min_x \theta_1(x) + \frac \beta 2(x^TA^TAx + 2(By - b + \frac 1 \beta \lambda)^TAx)\\ &= \arg \min_x \theta_1(x) + \frac \beta 2(x^TA^TAx + 2v^TAx)\\ &= \arg \min_x \theta_1(x) + \frac \beta 2(x^TA^TAx + 2v^TAx + v^Tv)\\ &= \arg \min_x \theta_1(x) + \frac \beta 2||Ax + v||^2\\ &= \arg \min_x \theta_1(x) + \frac \beta 2||Ax + By - b + \frac 1 \beta \lambda ||^2 \end{align*}","I am reading the popular paper Stochastic ADMM by Hua Ouyang et. al. I have redone one of their calculations many times, and I keep coming up with a different answer. It is much more likely that I am incorrect rather than the paper, but could somebody please take a look? Thank you very much. The problem is The augmented Lagrangian, as given on page 3 (bottom left) is Shortly after this, it claimed that the x-subproblem (where are the iterates) can be written as In my calculations, I keep getting instead of . My Calculation Here, I took away the superscript and the subscript for convenience. I also add and neglect constant terms without explicitly saying so.","\begin{align*} &\text{minimize } \ \  \theta_1(x) + \theta_2(y)\\
& \text{subject to } \ \  Ax + By = b, x \in \mathcal{X}
\end{align*} L_{\beta}(x, y, \lambda) = \theta_1(x) + \theta_2(y) + \lambda^T(Ax + By - b) + \frac \beta 2 ||Ax + By - b||^2. \arg \min_{x \in \mathcal{X}} L_{\beta}(x, y^k, \lambda^k) y^k, \lambda^k k^{th} \arg \min_{x \in \mathcal{X}} \theta_1(x) + \frac \beta 2||Ax + By^k - b - \frac 1 \beta \lambda^k ||^2. + \frac 1 \beta \lambda^k - \frac 1 \beta \lambda^k k x \in \mathcal{X} \begin{align*}
\arg \min_x L_{\beta}(x, y, \lambda) &= \arg \min _x \theta_1(x) + \theta_2(y) + \lambda^T (Ax + By - b) + \frac \beta 2||Ax+By-b||^2\\
&= \arg \min_x \theta_1(x) + \lambda^TAx + \frac \beta 2(x^TA^TAx + 2y^TB^TAx - 2b^TAx )\\
&= \arg \min_x \theta_1(x) + \frac \beta 2(x^TA^TAx + 2(By - b + \frac 1 \beta \lambda)^TAx)\\
&= \arg \min_x \theta_1(x) + \frac \beta 2(x^TA^TAx + 2v^TAx)\\
&= \arg \min_x \theta_1(x) + \frac \beta 2(x^TA^TAx + 2v^TAx + v^Tv)\\
&= \arg \min_x \theta_1(x) + \frac \beta 2||Ax + v||^2\\
&= \arg \min_x \theta_1(x) + \frac \beta 2||Ax + By - b + \frac 1 \beta \lambda ||^2
\end{align*}","['analysis', 'optimization', 'numerical-methods']"
90,Continuity of $f$ and monotone decreasing of $g(x) = \cos(f(x))$ implies uniform continuity of $f$,Continuity of  and monotone decreasing of  implies uniform continuity of,f g(x) = \cos(f(x)) f,"Let $f,g : \mathbb{R} \rightarrow \mathbb{R}$ . I want to show uniformly continuity of $f$ under $f$ is continuous and $f(0) = \frac{\pi}{2}$ , $g(x) = \cos(f(x))$ is monotonically decreasing. I know if $f$ is differentiable and $|f'|$ is bounded then $f$ is uniformly continuous. And from $g(x)$ is monotonically decreasing if $g$ is differentiable, then naively I can guess $g'(x) <0$ (decreasing) and deduce $g'(x) = - f'(x) \sin(f(x))<0$ ,  ... But the information given here is not enough to show $g$ and $f$ are differentiable.","Let . I want to show uniformly continuity of under is continuous and , is monotonically decreasing. I know if is differentiable and is bounded then is uniformly continuous. And from is monotonically decreasing if is differentiable, then naively I can guess (decreasing) and deduce ,  ... But the information given here is not enough to show and are differentiable.","f,g : \mathbb{R} \rightarrow \mathbb{R} f f f(0) = \frac{\pi}{2} g(x) = \cos(f(x)) f |f'| f g(x) g g'(x) <0 g'(x) = - f'(x) \sin(f(x))<0 g f","['real-analysis', 'analysis', 'uniform-continuity']"
91,Finding vectors in orthogonal complements to create a unique sum,Finding vectors in orthogonal complements to create a unique sum,,"Take U,W to be subspaces of $\mathbb{R}^{3}$ $U = \operatorname{Lin}\left\{\left(\begin{array}{c} -1 \\ 0 \\ 1 \end{array}\right),\left(\begin{array}{l} 1 \\ 1 \\ 1 \end{array}\right)\right\}$ and $V=\operatorname{Lin}\left\{\left(\begin{array}{l} 1 \\ -2 \\ 1 \end{array}\right)\right\}$ . Also : $U \oplus V  = \mathbb{R}^{3}$ . I need to find vectors $u \in U$ and $x \in U^{\perp}$ , such that $\left(\begin{array}{l} 1 \\ 1 \\ 0 \end{array}\right)$ = $u +x$ That is, find a choice of $u$ and $x$ that is unique and that gives me this vector. Now, my strategy is to find a orthogonal projection matrix that sends $\mathbb{R}^{3}$ to $U$ . This should give me a vector in $U$ such that when added to a vector $x \in V$ , say $\left(\begin{array}{l} 1 \\ -2 \\ 1 \end{array}\right)$ , I can obtain the required vector (in a unique way). But I if a build a projection matrix P = $U(U^{T}U)^{-1}U^{T}$ , and then take $u' = P  \left(\begin{array}{l} 1 \\ 1 \\ 0 \end{array}\right)$ , where $U = \left[\begin{array}{cc} -1 & 1 \\ 0 & 1 \\ 1 & 1 \end{array}\right]$ ... it doesn't quite give me the vector that I need. This is is my inital strategy. What am I missing ? What is the right way to approach this ?","Take U,W to be subspaces of and . Also : . I need to find vectors and , such that = That is, find a choice of and that is unique and that gives me this vector. Now, my strategy is to find a orthogonal projection matrix that sends to . This should give me a vector in such that when added to a vector , say , I can obtain the required vector (in a unique way). But I if a build a projection matrix P = , and then take , where ... it doesn't quite give me the vector that I need. This is is my inital strategy. What am I missing ? What is the right way to approach this ?","\mathbb{R}^{3} U = \operatorname{Lin}\left\{\left(\begin{array}{c}
-1 \\
0 \\
1
\end{array}\right),\left(\begin{array}{l}
1 \\
1 \\
1
\end{array}\right)\right\} V=\operatorname{Lin}\left\{\left(\begin{array}{l}
1 \\
-2 \\
1
\end{array}\right)\right\} U \oplus V  = \mathbb{R}^{3} u \in U x \in U^{\perp} \left(\begin{array}{l}
1 \\
1 \\
0
\end{array}\right) u +x u x \mathbb{R}^{3} U U x \in V \left(\begin{array}{l}
1 \\
-2 \\
1
\end{array}\right) U(U^{T}U)^{-1}U^{T} u' = P  \left(\begin{array}{l}
1 \\
1 \\
0
\end{array}\right) U = \left[\begin{array}{cc}
-1 & 1 \\
0 & 1 \\
1 & 1
\end{array}\right]","['linear-algebra', 'analysis', 'linear-transformations', 'orthogonal-matrices', 'projection-matrices']"
92,Understanding Quasi-self-similarity of the Julia Set (Falconer),Understanding Quasi-self-similarity of the Julia Set (Falconer),,"Here's an excerpt from the book: The highly intricate structure of the Julia set illustrated in Figure 0.6 stems from the single quadratic function $f(z) = z^2 + c$ for a suitable constant $c$ . Although the set is not strictly self-similar in the sense that the Cantor set and von Koch curve are, it is quasi-self-similar in that arbitrarily small portions of the set can be magnified and then distorted smoothly to coincide with a large part of the set. I don't know if the author rigorously defines quasi-self-similarity later in the text, but the ""definition"" above seems vague. In the particular example of the Julia set, what is meant by ""arbitrarily small portions of the set can be magnified and then distorted smoothly to coincide with a large part of the set"" ? What exactly are these distortions, and could someone help me understand with pictures how the magnified part on distortion coincides with a part of the set? How much do I need to magnify, and what part? Most of these questions are easy to answer in the case of Cantor sets, von Koch curve, and the Sierpinski triangle - since these are self-similar fractals. Quasi-self-similarity isn't that easy a nut to crack, it seems. I also came across this animation for $f(z) = z^2 - 1$ , but I'm not sure what it's trying to convey. References: Falconer, Fractal Geometry.","Here's an excerpt from the book: The highly intricate structure of the Julia set illustrated in Figure 0.6 stems from the single quadratic function for a suitable constant . Although the set is not strictly self-similar in the sense that the Cantor set and von Koch curve are, it is quasi-self-similar in that arbitrarily small portions of the set can be magnified and then distorted smoothly to coincide with a large part of the set. I don't know if the author rigorously defines quasi-self-similarity later in the text, but the ""definition"" above seems vague. In the particular example of the Julia set, what is meant by ""arbitrarily small portions of the set can be magnified and then distorted smoothly to coincide with a large part of the set"" ? What exactly are these distortions, and could someone help me understand with pictures how the magnified part on distortion coincides with a part of the set? How much do I need to magnify, and what part? Most of these questions are easy to answer in the case of Cantor sets, von Koch curve, and the Sierpinski triangle - since these are self-similar fractals. Quasi-self-similarity isn't that easy a nut to crack, it seems. I also came across this animation for , but I'm not sure what it's trying to convey. References: Falconer, Fractal Geometry.",f(z) = z^2 + c c f(z) = z^2 - 1,"['analysis', 'fractals']"
93,Find partial differential using chain rule,Find partial differential using chain rule,,Show that $\frac{\partial }{\partial x}=\frac{\partial }{\partial z}+\frac{\partial }{\partial \bar{z}}$ and $\frac{\partial }{\partial y}=i\left (\frac{\partial }{\partial z}-\frac{\partial }{\partial \bar{z}}  \right )$ Let $z=x+iy$ then $\bar{z}=x-iy$ from here $\qquad\begin{align}\frac{\partial }{\partial x}&=\frac{\partial z }{\partial x}\cdot\frac{\partial }{\partial z}+\frac{\partial \bar{z}}{\partial x}\cdot\frac{\partial }{\partial \bar{z}}\\&=1\cdot\frac{\partial }{\partial z}+1\cdot\frac{\partial }{\partial \bar{z}}\\&=\frac{\partial }{\partial z}+\frac{\partial }{\partial \bar{z}}\end{align}$ and $\qquad\begin{align}\frac{\partial }{\partial y}&=\frac{\partial z }{\partial y}\cdot\frac{\partial }{\partial z}+\frac{\partial \bar{z}}{\partial y}\cdot\frac{\partial }{\partial \bar{z}}\\&=i\cdot\frac{\partial }{\partial z}-i\cdot\frac{\partial }{\partial \bar{z}}\\&=i\cdot\left ( \frac{\partial }{\partial z} -\frac{\partial }{\partial \bar{z}}\right)\end{align}$ Is that okay or should I use the equalities $\frac{\partial }{\partial z}=\frac{1}{2}\left ( \frac{\partial }{\partial x}-i\frac{\partial }{\partial y} \right )$ and $\frac{\partial }{\partial \bar{z}}=\frac{1}{2}\left ( \frac{\partial }{\partial x} +i\frac{\partial }{\partial y}\right )$,Show that and Let then from here and Is that okay or should I use the equalities and,\frac{\partial }{\partial x}=\frac{\partial }{\partial z}+\frac{\partial }{\partial \bar{z}} \frac{\partial }{\partial y}=i\left (\frac{\partial }{\partial z}-\frac{\partial }{\partial \bar{z}}  \right ) z=x+iy \bar{z}=x-iy \qquad\begin{align}\frac{\partial }{\partial x}&=\frac{\partial z }{\partial x}\cdot\frac{\partial }{\partial z}+\frac{\partial \bar{z}}{\partial x}\cdot\frac{\partial }{\partial \bar{z}}\\&=1\cdot\frac{\partial }{\partial z}+1\cdot\frac{\partial }{\partial \bar{z}}\\&=\frac{\partial }{\partial z}+\frac{\partial }{\partial \bar{z}}\end{align} \qquad\begin{align}\frac{\partial }{\partial y}&=\frac{\partial z }{\partial y}\cdot\frac{\partial }{\partial z}+\frac{\partial \bar{z}}{\partial y}\cdot\frac{\partial }{\partial \bar{z}}\\&=i\cdot\frac{\partial }{\partial z}-i\cdot\frac{\partial }{\partial \bar{z}}\\&=i\cdot\left ( \frac{\partial }{\partial z} -\frac{\partial }{\partial \bar{z}}\right)\end{align} \frac{\partial }{\partial z}=\frac{1}{2}\left ( \frac{\partial }{\partial x}-i\frac{\partial }{\partial y} \right ) \frac{\partial }{\partial \bar{z}}=\frac{1}{2}\left ( \frac{\partial }{\partial x} +i\frac{\partial }{\partial y}\right ),"['complex-analysis', 'analysis', 'complex-numbers', 'partial-derivative', 'partial-fractions']"
94,"Problem of Analysis about Matrix Exponential, Infimum and Limit","Problem of Analysis about Matrix Exponential, Infimum and Limit",,"Let $f: \mathbb{R}^{n^2} \times \mathbb{Z}^{n} \longrightarrow \mathbb{R}$ defined by $$ f(X,z) = \prod_{i=1}^{n} |x_i z|, $$ where $x_i$ is the $i$ -th row of $X$ and $x_iz$ is a dot product of $x_i$ and $z$ . My question is: Is it true that $$ \inf_{z \neq 0} \lim_{X \rightarrow 0} f(e^{X+C}, z) = \lim_{X \rightarrow 0} \inf_{z \neq 0} f(e^{X+C}, z) ? $$ Some observations that I deduce about this: $e^{X} = \displaystyle\sum_{k=0}^{\infty} \dfrac{X^{k}}{k!}$ is infinitely differentiable and therefore $e^{X+C}$ is continuous. I think that you don´t need to know about the constant $C$ , but $C = \log b$ and the definition is that $e^{C}=b$ . $C$ is a matrix, i.e., $C \in \mathbb{R}^{n^2}$ . Actually, still I believe that we don´t need to know about the function $f$ the same way that I wrote above. I think that the problem can be formulated as follows: Given $f: \mathbb{R}^{n^2} \times \mathbb{Z}^{n} \longrightarrow \mathbb{R}$ continuous, is it true that $\inf_{z \neq 0} \lim_{X \rightarrow 0} f(X, z) = \lim_{X \rightarrow 0} \inf_{z \neq 0} f(X, z)$ ? So I can´t do how prove it. I tested with some examples in Matlab and Wolfram Mathematica and the equality has always been valid. One of the ways which I thought was to ""open"" the expressions of both sides of equality and try to reach the same result. To reach the infimum, I thought that $z$ should have all coordinates equal to zero, except 1 coordinate. In this case, I have $z \neq 0$ and maybe this is a vector that works to prove it. But I didn´t get anywhere.","Let defined by where is the -th row of and is a dot product of and . My question is: Is it true that Some observations that I deduce about this: is infinitely differentiable and therefore is continuous. I think that you don´t need to know about the constant , but and the definition is that . is a matrix, i.e., . Actually, still I believe that we don´t need to know about the function the same way that I wrote above. I think that the problem can be formulated as follows: Given continuous, is it true that ? So I can´t do how prove it. I tested with some examples in Matlab and Wolfram Mathematica and the equality has always been valid. One of the ways which I thought was to ""open"" the expressions of both sides of equality and try to reach the same result. To reach the infimum, I thought that should have all coordinates equal to zero, except 1 coordinate. In this case, I have and maybe this is a vector that works to prove it. But I didn´t get anywhere.","f: \mathbb{R}^{n^2} \times \mathbb{Z}^{n} \longrightarrow \mathbb{R} 
f(X,z) = \prod_{i=1}^{n} |x_i z|,
 x_i i X x_iz x_i z 
\inf_{z \neq 0} \lim_{X \rightarrow 0} f(e^{X+C}, z) = \lim_{X \rightarrow 0} \inf_{z \neq 0} f(e^{X+C}, z) ?
 e^{X} = \displaystyle\sum_{k=0}^{\infty} \dfrac{X^{k}}{k!} e^{X+C} C C = \log b e^{C}=b C C \in \mathbb{R}^{n^2} f f: \mathbb{R}^{n^2} \times \mathbb{Z}^{n} \longrightarrow \mathbb{R} \inf_{z \neq 0} \lim_{X \rightarrow 0} f(X, z) = \lim_{X \rightarrow 0} \inf_{z \neq 0} f(X, z) z z \neq 0","['analysis', 'multivariable-calculus']"
95,"Continuous $f$ on $[0,1]$ is not one-to-one. [closed]",Continuous  on  is not one-to-one. [closed],"f [0,1]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question We have continuous function $f$ on $[0,1]$ with $f([0,1])=[0,1]\times[0,1]$ . Prove $f$ is never one-to-one? I know I should show what I tried but I unable to think how to start on this one. I would appreciate some hints so I can update what I tried.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question We have continuous function on with . Prove is never one-to-one? I know I should show what I tried but I unable to think how to start on this one. I would appreciate some hints so I can update what I tried.","f [0,1] f([0,1])=[0,1]\times[0,1] f","['real-analysis', 'analysis']"
96,"Breaking a real valued increasing function into absolutely continuous, continuous and jump function","Breaking a real valued increasing function into absolutely continuous, continuous and jump function",,"Suppose $F$ is an increasing function on $[a, b]$ . (a) Prove that we can write $$ F=F_{A}+F_{C}+F_{J} $$ where each of the functions $F_{A}, F_{C}$ , and $F_{J}$ is increasing and: (i) $F_{A}$ is absolutely continuous. (ii) $F_{C}$ is continuous, but $F_{C}^{\prime}(x)=0$ for a.e. $x$ . (iii) $F_{j}$ is a jump function. If we let $$ j_{n}(x)=\left\{\begin{array}{cl} 0 & \text { if } x<x_{n} \\ \theta_{n} & \text { if } x=x_{n} \\ 1 & \text { if } x>x_{n} \end{array}\right. $$ then we define the jump function associated to $F$ by $$ J_{F}(x)=\sum_{n=1}^{\infty} \alpha_{n} j_{n}(x) . $$ I am kind of sure that I have to use the property that an increasing function can have only countably many jump discontinuity. Given an increasing order of discontinuities $\{x_n\}$ of $F$ i.e., $x_n\le x_{n+1}$ and $f$ has discontinuities at $x_n$ then we can represent the function $$ F(x)=f_n(x); x_n\le x\le x_{n+1} $$ and $f_n$ is continuous on $(x_n,x_{n+1})$ . Now we can define a function $$G(x)=\left\{\begin{array}{cl} g_1(x):=f_1(x) & \text { if } a=x_1\leq x<x_{2} \\ g_{n}(x) & \text { if } x_n\le x\le x_{n+1} \end{array}\right. $$ where $g_n(x)$ is a function defined on $(x_n,x_{n+1})$ having the same slope of $f_n(x)$ on $(x_n,x_{n+1})$ so that $G(x)$ is continuous basically I am removing the jump discontinuities of $F$ . So we can write $F(x)=G(x)+J(x)$ where $$J(x)=f_n(\frac{x_n+ x_{n+1}}{2})-g_n(\frac{x_n+ x_{n+1}}{2}); x_n\le x\le x_{n+1} $$ then $J(x)$ is a step function. I am wondering if I can at all transform this thing into the solution I want. Let me know if there is any other way. Please help!","Suppose is an increasing function on . (a) Prove that we can write where each of the functions , and is increasing and: (i) is absolutely continuous. (ii) is continuous, but for a.e. . (iii) is a jump function. If we let then we define the jump function associated to by I am kind of sure that I have to use the property that an increasing function can have only countably many jump discontinuity. Given an increasing order of discontinuities of i.e., and has discontinuities at then we can represent the function and is continuous on . Now we can define a function where is a function defined on having the same slope of on so that is continuous basically I am removing the jump discontinuities of . So we can write where then is a step function. I am wondering if I can at all transform this thing into the solution I want. Let me know if there is any other way. Please help!","F [a, b] 
F=F_{A}+F_{C}+F_{J}
 F_{A}, F_{C} F_{J} F_{A} F_{C} F_{C}^{\prime}(x)=0 x F_{j} 
j_{n}(x)=\left\{\begin{array}{cl}
0 & \text { if } x<x_{n} \\
\theta_{n} & \text { if } x=x_{n} \\
1 & \text { if } x>x_{n}
\end{array}\right.
 F 
J_{F}(x)=\sum_{n=1}^{\infty} \alpha_{n} j_{n}(x) .
 \{x_n\} F x_n\le x_{n+1} f x_n 
F(x)=f_n(x); x_n\le x\le x_{n+1}
 f_n (x_n,x_{n+1}) G(x)=\left\{\begin{array}{cl}
g_1(x):=f_1(x) & \text { if } a=x_1\leq x<x_{2} \\
g_{n}(x) & \text { if } x_n\le x\le x_{n+1}
\end{array}\right.
 g_n(x) (x_n,x_{n+1}) f_n(x) (x_n,x_{n+1}) G(x) F F(x)=G(x)+J(x) J(x)=f_n(\frac{x_n+ x_{n+1}}{2})-g_n(\frac{x_n+ x_{n+1}}{2}); x_n\le x\le x_{n+1}
 J(x)","['real-analysis', 'analysis', 'continuity', 'contest-math', 'absolute-continuity']"
97,"Munkres, Analysis on Manifolds, Problem 38-5","Munkres, Analysis on Manifolds, Problem 38-5",,"I am having trouble doing this problem. I know that we need to use Stokes' Theorem. $D_3$ is easy. The problem is I can't seem to picture a shape whose boundary is $D_1$ and $C_1$ and/or $D_1$ and $C_2$ (the way the lines are drawn seems to make a connected smooth surface impossible). Similarly, creating a smooth surface that has $D_2$ and $C_1$ and/or $D_2$ and $C_2$ as boundaries also seems impossible. Question: Let S be the subset of $\mathbb{R^3}$ consisting of the union of: i) the z-axis ii) the unit circle $x^2+y^2=1, z=0$ iii) the points $(0, y, 0)$ with $y \geq 1$ Let $A$ be the open set $\mathbb{R}^3-S$ of $\mathbb{R}^3$ . Let $C_1, C_2, D_1, D_2, D_3$ be the oriented 1-manifolds in $A$ that are pictured in Figure 38.3. Suppose that $F$ is a vector field in $A$ , with $curl F = 0$ in $A$ that $\int_{C_1} \langle F,T \rangle ds = 3$ and $\int_{C_2} \langle F,T \rangle ds = 7$ . What can you say about the integral $\int_{D_i} \langle F,T \rangle ds$ for $i=1,2,3$ ? Justify your answers.","I am having trouble doing this problem. I know that we need to use Stokes' Theorem. is easy. The problem is I can't seem to picture a shape whose boundary is and and/or and (the way the lines are drawn seems to make a connected smooth surface impossible). Similarly, creating a smooth surface that has and and/or and as boundaries also seems impossible. Question: Let S be the subset of consisting of the union of: i) the z-axis ii) the unit circle iii) the points with Let be the open set of . Let be the oriented 1-manifolds in that are pictured in Figure 38.3. Suppose that is a vector field in , with in that and . What can you say about the integral for ? Justify your answers.","D_3 D_1 C_1 D_1 C_2 D_2 C_1 D_2 C_2 \mathbb{R^3} x^2+y^2=1, z=0 (0, y, 0) y \geq 1 A \mathbb{R}^3-S \mathbb{R}^3 C_1, C_2, D_1, D_2, D_3 A F A curl F = 0 A \int_{C_1} \langle F,T \rangle ds = 3 \int_{C_2} \langle F,T \rangle ds = 7 \int_{D_i} \langle F,T \rangle ds i=1,2,3","['analysis', 'vector-analysis']"
98,Singular Value Decomposition Theorem: Corollary,Singular Value Decomposition Theorem: Corollary,,"In a linear algebra and analysis course [it's a hybrid course between the two], we recently had the SVD (singular value decomposition) theorem, and the prof. told us (due to lack of time without proof): Corollary 2.39 : Let $A = U\Sigma V^{T}$ be the singular value decomposition of $A\in \mathbb R^{m\times n}$ , where the singular values $\sigma_1 \geq \dots \geq \sigma_p \geq 0$ , where $p = \min\left\{ m, n\right\}$ . Further, for $k<p$ , define $$A_{k} = U\Sigma_{k}V^{T},$$ where $$\Sigma_{k} = \begin{pmatrix} \sigma_1 \qquad\qquad\qquad 0 \\ \qquad \ddots \qquad \\ \quad\quad\quad\sigma_{k}  \\ 0 \qquad\qquad\qquad 0  \end{pmatrix} \in \mathbb R^{m\times n},$$ with $k < p$ . It holds that: $$A_k = \arg\min_{\text{rank}\left( B \right) = k}\left|\left| A - B\right|\right|_{2} = \arg\min_{\text{rank}\left( B\right) = k}\left|\left| A - B\right|\right|_{F}.$$ Upon question on a hint of the proof the lecturer said that one might want to use the following relations: $$\text{Tr}\left( AB^{T} \right) \leq \sum_{j=1}^{p} \sigma_{j}\gamma_{j},$$ where $\gamma_{1} \geq \dots \geq \gamma_{p} \geq 0$ denote the singular values of $B\in\mathbb R^{m\times n}$ . I also proved the following two relations: $$\left|\left| A - A_{k} \right|\right|_{2} = \sigma_{k+1}, \qquad \left|\left| A - A_{k} \right|\right|_{F} = \left( \sum_{j = k+1}^{p} \sigma_{j}^{2} \right)^{1/2} \qquad $$ IDEA : I tried several things, one of them being: $$\left|\left| A - B\right|\right|_{2} \leq \left|\left| A - A_{k} \right|\right|_{2} + \left|\left| B - A_{k} \right|\right|_{2} = \sigma_{k+1} + \left|\left| B - A_{k} \right|\right|_{2}$$ But then, I am not sure how to continue. We already know from the Corollary that $A_k = U\Sigma_k V^{T}$ [by definition], and $B = \tilde{U}\tilde{\Sigma}\tilde{V^{T}}$ [by the SVD Theorem], i.e. $\left|\left| B - A_{k} \right|\right|_{2} = \left|\left| \tilde{U}\tilde{\Sigma}\tilde{V^{T}} -  U\Sigma_k V^{T}\right|\right|_{2}$ . Could anybody help out, please, on how to continue?","In a linear algebra and analysis course [it's a hybrid course between the two], we recently had the SVD (singular value decomposition) theorem, and the prof. told us (due to lack of time without proof): Corollary 2.39 : Let be the singular value decomposition of , where the singular values , where . Further, for , define where with . It holds that: Upon question on a hint of the proof the lecturer said that one might want to use the following relations: where denote the singular values of . I also proved the following two relations: IDEA : I tried several things, one of them being: But then, I am not sure how to continue. We already know from the Corollary that [by definition], and [by the SVD Theorem], i.e. . Could anybody help out, please, on how to continue?","A = U\Sigma V^{T} A\in \mathbb R^{m\times n} \sigma_1 \geq \dots \geq \sigma_p \geq 0 p = \min\left\{ m, n\right\} k<p A_{k} = U\Sigma_{k}V^{T}, \Sigma_{k} = \begin{pmatrix} \sigma_1 \qquad\qquad\qquad 0 \\ \qquad \ddots \qquad \\ \quad\quad\quad\sigma_{k}  \\ 0 \qquad\qquad\qquad 0  \end{pmatrix} \in \mathbb R^{m\times n}, k < p A_k = \arg\min_{\text{rank}\left( B \right) = k}\left|\left| A - B\right|\right|_{2} = \arg\min_{\text{rank}\left( B\right) = k}\left|\left| A - B\right|\right|_{F}. \text{Tr}\left( AB^{T} \right) \leq \sum_{j=1}^{p} \sigma_{j}\gamma_{j}, \gamma_{1} \geq \dots \geq \gamma_{p} \geq 0 B\in\mathbb R^{m\times n} \left|\left| A - A_{k} \right|\right|_{2} = \sigma_{k+1}, \qquad \left|\left| A - A_{k} \right|\right|_{F} = \left( \sum_{j = k+1}^{p} \sigma_{j}^{2} \right)^{1/2} \qquad  \left|\left| A - B\right|\right|_{2} \leq \left|\left| A - A_{k} \right|\right|_{2} + \left|\left| B - A_{k} \right|\right|_{2} = \sigma_{k+1} + \left|\left| B - A_{k} \right|\right|_{2} A_k = U\Sigma_k V^{T} B = \tilde{U}\tilde{\Sigma}\tilde{V^{T}} \left|\left| B - A_{k} \right|\right|_{2} = \left|\left| \tilde{U}\tilde{\Sigma}\tilde{V^{T}} -  U\Sigma_k V^{T}\right|\right|_{2}","['linear-algebra', 'analysis', 'svd']"
99,Maclaurin series for the function $f(x) = \ln\left(\frac{|x^2 - 1|}{x+1}\right)$,Maclaurin series for the function,f(x) = \ln\left(\frac{|x^2 - 1|}{x+1}\right),"I want to find the Maclaurin series (Taylor at $0$ ) for the following function: $f(x) = \ln\left(\frac{|x^2 - 1|}{x+1}\right)$ So far I have worked on this solution but I am not sure if it's correct: $$ f(x) = \ln\left(\frac{|x^2 - 1|}{x+1}\right) = \ln(|x^2-1|) - \ln|x+1| = \ln|x-1|+\ln|x+1| - \ln|x+1| = \ln|x-1| = \ln|1-x|  $$ Since we are around zero I think that we can can drop the absolute value and evaluate $$ \ln|1-x| = \ln(1-x) = -\sum_{i=0}^\infty \frac{x^n}{n} $$ If the solution is not correct, could you suggest an alternative solution, please?","I want to find the Maclaurin series (Taylor at ) for the following function: So far I have worked on this solution but I am not sure if it's correct: Since we are around zero I think that we can can drop the absolute value and evaluate If the solution is not correct, could you suggest an alternative solution, please?","0 f(x) = \ln\left(\frac{|x^2 - 1|}{x+1}\right) 
f(x) = \ln\left(\frac{|x^2 - 1|}{x+1}\right) = \ln(|x^2-1|) - \ln|x+1| = \ln|x-1|+\ln|x+1| - \ln|x+1|
= \ln|x-1| = \ln|1-x| 
 
\ln|1-x| = \ln(1-x) = -\sum_{i=0}^\infty \frac{x^n}{n}
","['real-analysis', 'sequences-and-series', 'analysis']"
