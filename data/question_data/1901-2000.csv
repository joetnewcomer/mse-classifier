,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Uniform Convergence of Difference Quotients to Partial Derivative,Uniform Convergence of Difference Quotients to Partial Derivative,,I'm currently reading Evans' PDE book. In it he claims that for $f \in C^2_c(\mathbb{R}^n)$ $$\frac{f(x + he_i) - f(x)}{h} \to \frac{\partial}{\partial x_i}f(x)$$ and $$\frac{\frac{\partial}{\partial x_i}f(x + he_j) - \frac{\partial}{\partial x_i}f(x)}{h} \to \frac{\partial^2}{\partial x_jx_i}f(x)$$ uniformly as $h \to 0$. My question is why must the convergence be uniform? Thanks in advance.,I'm currently reading Evans' PDE book. In it he claims that for $f \in C^2_c(\mathbb{R}^n)$ $$\frac{f(x + he_i) - f(x)}{h} \to \frac{\partial}{\partial x_i}f(x)$$ and $$\frac{\frac{\partial}{\partial x_i}f(x + he_j) - \frac{\partial}{\partial x_i}f(x)}{h} \to \frac{\partial^2}{\partial x_jx_i}f(x)$$ uniformly as $h \to 0$. My question is why must the convergence be uniform? Thanks in advance.,,['real-analysis']
1,When are the limit operations commutative?,When are the limit operations commutative?,,"I'll come up with a question that has bothered me for a long period of time. The question seems relatively simple, but I personally didn't manage to find an answer to it. In many cases I met problems where I had to deal with multiple limits and I've never known what is the rule that allows the limits to be interchanged. I'm referring at the situation when the limit operations are commutative. For some problems this would be of enormous help since the problem you have to deal with becomes much more easier when you change the limit operations. Could you help here?  Thanks.","I'll come up with a question that has bothered me for a long period of time. The question seems relatively simple, but I personally didn't manage to find an answer to it. In many cases I met problems where I had to deal with multiple limits and I've never known what is the rule that allows the limits to be interchanged. I'm referring at the situation when the limit operations are commutative. For some problems this would be of enormous help since the problem you have to deal with becomes much more easier when you change the limit operations. Could you help here?  Thanks.",,"['calculus', 'real-analysis', 'limits', 'faq']"
2,Decomposition of $\Bbb R^n$ as union of countable disjoint closed balls and a null set,Decomposition of  as union of countable disjoint closed balls and a null set,\Bbb R^n,"This is a problem in Frank Jones's Lebesgue integration on Euclidean space (p.57), $$\mathbb{R}^n = N \cup \bigcup_{k=1}^\infty \overline{B}_k$$ where $\lambda(N)=0$, and the closed balls are disjoint. could any one give some hints?","This is a problem in Frank Jones's Lebesgue integration on Euclidean space (p.57), $$\mathbb{R}^n = N \cup \bigcup_{k=1}^\infty \overline{B}_k$$ where $\lambda(N)=0$, and the closed balls are disjoint. could any one give some hints?",,"['real-analysis', 'general-topology', 'measure-theory']"
3,Evaluating $\int_{-\infty}^\infty \frac{\ln{(x^4+x^2+1)}}{x^4+1}dx$,Evaluating,\int_{-\infty}^\infty \frac{\ln{(x^4+x^2+1)}}{x^4+1}dx,"I recently attempted to evaluate the following integral $$\int_{-\infty}^\infty\frac{\ln{(x^4+x^2+1)}}{x^4+1}dx$$ I started by inserting a parameter, $t$ $$F(t)=\int_{-\infty}^\infty\frac{\ln{(tx^4+x^2+t)}}{x^4+1}dx$$ Where F(0) is the following $$F(0)=\int_{-\infty}^\infty\frac{\ln{(x^2)}}{x^4+1}dx=2\int_0^\infty\frac{\ln{(x^2)}}{x^4+1}dx=4\int_0^\infty\frac{\ln x}{x^4+1}dx$$ We can evaluate this using a common integral from complex analysis and taking the derivative using Leibniz’s rule. $$\int_0^\infty\frac{x^m}{x^n+1}dx=\frac{1}{m+1}\int_0^\infty\frac{(m+1)x^m}{(x^{m+1})^\frac{n}{m+1}+1}dx=\frac{1}{m+1}\int_0^\infty\frac{du}{x^\frac{n}{m+1}+1}$$ $$=\frac{1}{m+1}\frac{\pi}{\frac{n}{m+1}\sin{\frac{\pi}{\frac{n}{m+1}}}}=\frac{\pi}{n\sin{\frac{\pi(m+1)}{n}}}=\frac{\pi}{n}\csc{\frac{\pi(m+1)}{n}}$$ $$\int_0^\infty\frac{\ln{x}}{x^n+1}dx=\frac{d}{dm}\int_0^\infty\frac{x^m}{x^n+1}dx\Big|_{m=0}$$ $$=\frac{\pi}{n}\frac{d}{dm}\csc{\frac{\pi(m+1)}{n}}\Big|_{m=0}=-\frac{\pi^2}{n^2}\csc{\frac{\pi(m+1)}{n}}\cot{\frac{\pi(m+1)}{n}}\big|_{m=0}=-\frac{\pi^2}{n^2}\csc{\frac{\pi}{n}}\cot{\frac{\pi}{n}}$$ Therefore $$F(0)=4\int_0^\infty\frac{\ln{x}}{x^4+1}dx=-\frac{\pi^2\sqrt 2}{4}=-\frac{\pi^2}{2\sqrt 2}$$ Now that we found F(0), we can start applying Feynman’s trick. $$F’(t)=\int_{-\infty}^{\infty}\frac{dx}{tx^4+x^2+t}$$ Using a formula I derived we can continue $$\int_{-\infty}^\infty\frac{dx}{ax^4+bx^2+c}=\frac{\pi}{\sqrt{c}\sqrt{b+2\sqrt{ac}}}$$ $$F’(t)=\frac{\pi}{\sqrt{t}\sqrt{1+2t}}$$ Integrating both sides $$F(t)=\pi\sqrt2\ln{(\sqrt{2t}+\sqrt{2t+1})}+C$$ Set $t=0$ $$C=F(0)=-\frac{\pi^2}{2\sqrt2}$$ Therefore $$F(t)=\pi\sqrt2\ln{(\sqrt{2t}+\sqrt{2t+1})}-\frac{\pi^2}{2\sqrt2}$$ $$I=\pi\sqrt2\ln{(\sqrt2+\sqrt3)}-\frac{\pi^2}{2\sqrt2}$$ WolframAlpha confirms it numerically I am not satisfied with this solution.  I am curious as to what other solutions there might be. How else can we solve this integral?","I recently attempted to evaluate the following integral I started by inserting a parameter, Where F(0) is the following We can evaluate this using a common integral from complex analysis and taking the derivative using Leibniz’s rule. Therefore Now that we found F(0), we can start applying Feynman’s trick. Using a formula I derived we can continue Integrating both sides Set Therefore WolframAlpha confirms it numerically I am not satisfied with this solution.  I am curious as to what other solutions there might be. How else can we solve this integral?",\int_{-\infty}^\infty\frac{\ln{(x^4+x^2+1)}}{x^4+1}dx t F(t)=\int_{-\infty}^\infty\frac{\ln{(tx^4+x^2+t)}}{x^4+1}dx F(0)=\int_{-\infty}^\infty\frac{\ln{(x^2)}}{x^4+1}dx=2\int_0^\infty\frac{\ln{(x^2)}}{x^4+1}dx=4\int_0^\infty\frac{\ln x}{x^4+1}dx \int_0^\infty\frac{x^m}{x^n+1}dx=\frac{1}{m+1}\int_0^\infty\frac{(m+1)x^m}{(x^{m+1})^\frac{n}{m+1}+1}dx=\frac{1}{m+1}\int_0^\infty\frac{du}{x^\frac{n}{m+1}+1} =\frac{1}{m+1}\frac{\pi}{\frac{n}{m+1}\sin{\frac{\pi}{\frac{n}{m+1}}}}=\frac{\pi}{n\sin{\frac{\pi(m+1)}{n}}}=\frac{\pi}{n}\csc{\frac{\pi(m+1)}{n}} \int_0^\infty\frac{\ln{x}}{x^n+1}dx=\frac{d}{dm}\int_0^\infty\frac{x^m}{x^n+1}dx\Big|_{m=0} =\frac{\pi}{n}\frac{d}{dm}\csc{\frac{\pi(m+1)}{n}}\Big|_{m=0}=-\frac{\pi^2}{n^2}\csc{\frac{\pi(m+1)}{n}}\cot{\frac{\pi(m+1)}{n}}\big|_{m=0}=-\frac{\pi^2}{n^2}\csc{\frac{\pi}{n}}\cot{\frac{\pi}{n}} F(0)=4\int_0^\infty\frac{\ln{x}}{x^4+1}dx=-\frac{\pi^2\sqrt 2}{4}=-\frac{\pi^2}{2\sqrt 2} F’(t)=\int_{-\infty}^{\infty}\frac{dx}{tx^4+x^2+t} \int_{-\infty}^\infty\frac{dx}{ax^4+bx^2+c}=\frac{\pi}{\sqrt{c}\sqrt{b+2\sqrt{ac}}} F’(t)=\frac{\pi}{\sqrt{t}\sqrt{1+2t}} F(t)=\pi\sqrt2\ln{(\sqrt{2t}+\sqrt{2t+1})}+C t=0 C=F(0)=-\frac{\pi^2}{2\sqrt2} F(t)=\pi\sqrt2\ln{(\sqrt{2t}+\sqrt{2t+1})}-\frac{\pi^2}{2\sqrt2} I=\pi\sqrt2\ln{(\sqrt2+\sqrt3)}-\frac{\pi^2}{2\sqrt2},"['real-analysis', 'calculus', 'integration', 'definite-integrals']"
4,Measure of the irrational numbers?,Measure of the irrational numbers?,,"I have read that the measure of the irrational numbers on an interval $[a,b] = b-a$. This both makes sense and doesn't make sense to me. If you consider that the union of the irrationals with the rationals are the reals, then if the rationals have measure 0, then the irrationals must have the same measure as the reals. (right?) But if you consider that the irrationals are totally disconnected, then it is a collection of disconnected points. If it's a collection of disconnected points, like the rationals, how can it have a non-zero measure? Another fact I'm not sold on, which may be the source of the problem, is that there isn't a bijection between the rationals and the irrationals. If both of these sets are totally disconnected by each other and both dense, that is, ""Between any $ \forall a,b \in \mathbb{R} $, such that $a < b$, there is exists an irrational $i$ and a rational $q$ satisfying $a < i < b$, and $a < q < b$."" I know of the diagonalization argument but I've just never been completely sold on this fact. For the irrationals to be uncountable and the rationals to be countable, in my head it would make more sense if there exists an $\epsilon > 0$ such that around any irrational number there exists only other irrational numbers. I know that if it exists you could just sum enough of those epsilons together to get the length of it greater than some $\frac{p}{q}$ thus finding a contradiction... but I'm still not sold on it. Can someone help me finally understand these concepts?","I have read that the measure of the irrational numbers on an interval $[a,b] = b-a$. This both makes sense and doesn't make sense to me. If you consider that the union of the irrationals with the rationals are the reals, then if the rationals have measure 0, then the irrationals must have the same measure as the reals. (right?) But if you consider that the irrationals are totally disconnected, then it is a collection of disconnected points. If it's a collection of disconnected points, like the rationals, how can it have a non-zero measure? Another fact I'm not sold on, which may be the source of the problem, is that there isn't a bijection between the rationals and the irrationals. If both of these sets are totally disconnected by each other and both dense, that is, ""Between any $ \forall a,b \in \mathbb{R} $, such that $a < b$, there is exists an irrational $i$ and a rational $q$ satisfying $a < i < b$, and $a < q < b$."" I know of the diagonalization argument but I've just never been completely sold on this fact. For the irrationals to be uncountable and the rationals to be countable, in my head it would make more sense if there exists an $\epsilon > 0$ such that around any irrational number there exists only other irrational numbers. I know that if it exists you could just sum enough of those epsilons together to get the length of it greater than some $\frac{p}{q}$ thus finding a contradiction... but I'm still not sold on it. Can someone help me finally understand these concepts?",,"['real-analysis', 'general-topology', 'measure-theory']"
5,Distribution of higher powers than 2 of a gaussian distribution,Distribution of higher powers than 2 of a gaussian distribution,,"If $X \sim \mathcal{N}(0,1)$, then $X^2 \sim \chi^2(1)$. What about higher powers of $X$? I know that the Gamma Distribution is a generalization of the $\chi^2$ distribution, but I don't know how the Gamma Distribution parameters relate to the square part of $\chi^2$. In particular I'm trying to calculate $X^4$, where $X \sim \mathcal{N} \left(0,\frac{1}{N} \right)$. How do you even take on such a problem? Thanks for any tips!","If $X \sim \mathcal{N}(0,1)$, then $X^2 \sim \chi^2(1)$. What about higher powers of $X$? I know that the Gamma Distribution is a generalization of the $\chi^2$ distribution, but I don't know how the Gamma Distribution parameters relate to the square part of $\chi^2$. In particular I'm trying to calculate $X^4$, where $X \sim \mathcal{N} \left(0,\frac{1}{N} \right)$. How do you even take on such a problem? Thanks for any tips!",,"['real-analysis', 'probability', 'normal-distribution']"
6,What is the most elementary proof that $\lim_{n \to \infty} (1+1/n)^n$ exists?,What is the most elementary proof that  exists?,\lim_{n \to \infty} (1+1/n)^n,"Here is my candidate for  the most elementary proof that $\lim_{n \to \infty}(1+1/n)^n $ exists. I would be interested in seeing others. $***$ Added after some comments: I prove here by very elementary means that the limit exists. Calling the limit ""$e$"" names it. $***$ It only needs Bernoulli's inequality (BI) in the form $(1+x)^n \ge 1+nx$ for $x > -1$ and $n$ a positive integer, with equality only if $x = 0$ or $n = 1$. This is easily proved by induction: It is true for $n=1$, and $(1+x)^{n+1} = (1+x)(1+x)^n \ge (1+x)(1+nx) = 1+(n+1)x+nx^2 \ge 1+(n+1)x $. (If $-1 < x < 0$, if $1+nx \ge 0$, the above proof goes through, and if $1+nx < 0$, $1+mx < 0$ for all $m \ge n$ so certainly $(1+x)^m > 1+mx$.) This proof originally appeared in N.S Mendelsohn, An application of a famous inequality, Amer. Math. Monthly 58 (1951), 563 and uses the arithmetic-geometric mean inequality (AGMI) in the form $\big(\sum_{i=1}^n v_i/n\big)^n \ge \prod_{i=1}^n v_i$  (all $v_i$ positive) with equality if and only if all the $v_i$ are equal. Let $a_n = (1+1/n)^n$ and $b_n = (1+1/n)^{n+1}$.  We will prove that $a_n$ is an increasing sequence  and $b_n$ is an decreasing sequence.  Since $a_n < b_n$, this implies,  for any positive integers $n$ and $m$ with $m < n$  that $a_m < a_n < b_n < b_m$. For $a_n$, consider n values of $1+1/n$ and $1$ value of $1$.  Since their sum is $n+2$ and their product is $(1+1/n)^n$, by the AGMI,  $((n+2)/(n+1))^{n+1} > (1+1/n)^n$,  or $(1+1/(n+1))^{n+1} > (1+1/n)^n$,  or $a_{n+1} > a_n$.  For $b_n$, consider $n$ values of $1-1/n$ and $1$ value of $1$.  Since their sum is $n$ and their product is $(1-1/n)^n$, by the AGMI,  $(n/(n+1))^{n+1} > (1-1/n)^n$  or $(1+1/n)^{n+1} < (1+1/(n-1))^n$,  or $b_n > b_{n+1}$. Since $b_n-a_n  = (1+1/n)^{n+1} - (1+1/n)^n = (1+1/n)^n(1/n) =a_n/n $ and every $a_n$ is less than any $b_n$ and $b_3 = (1+1/3)^4 = 256/81 < 4$, $b_n-a_n < 4/n$, so $b_n$ and $a_n$ converge to a common limit. These proofs do not seem to be really elementary,  since they use the AGMI.  However, they use a special form of the AGMI,  where all but one of the values are the same,  and this will now be shown to be implied by BI,  and thus be truly elementary. Suppose we have $n-1$ values of $u$ and $1$ value of $v$ with $u$ and $v$ positive.  The AGMI for these values is  $(((n-1)u+v)/n)^n \ge u^{n-1}v$  with equality if and only if $u = v$.  We will now show that this is implied by BI: $(((n-1)u+v)/n)^n \ge u^{n-1}v$  is the same as  $(u+(v-u)/n)^n \ge u^n(v/u)$.  Dividing by $u^n$,  this is equivalent to  $(1+(v/u-1)/n)^n \ge v/u$.  By BI,  since $(v/u-1)/n > -1/n > -1$, $(1+(v/u-1)/n)^n \ge 1+n((v/u-1)/n) = v/u$  with equality only if $n=1$ or $v/u-1 = 0$. Thus BI implies this version of the AGMI.","Here is my candidate for  the most elementary proof that $\lim_{n \to \infty}(1+1/n)^n $ exists. I would be interested in seeing others. $***$ Added after some comments: I prove here by very elementary means that the limit exists. Calling the limit ""$e$"" names it. $***$ It only needs Bernoulli's inequality (BI) in the form $(1+x)^n \ge 1+nx$ for $x > -1$ and $n$ a positive integer, with equality only if $x = 0$ or $n = 1$. This is easily proved by induction: It is true for $n=1$, and $(1+x)^{n+1} = (1+x)(1+x)^n \ge (1+x)(1+nx) = 1+(n+1)x+nx^2 \ge 1+(n+1)x $. (If $-1 < x < 0$, if $1+nx \ge 0$, the above proof goes through, and if $1+nx < 0$, $1+mx < 0$ for all $m \ge n$ so certainly $(1+x)^m > 1+mx$.) This proof originally appeared in N.S Mendelsohn, An application of a famous inequality, Amer. Math. Monthly 58 (1951), 563 and uses the arithmetic-geometric mean inequality (AGMI) in the form $\big(\sum_{i=1}^n v_i/n\big)^n \ge \prod_{i=1}^n v_i$  (all $v_i$ positive) with equality if and only if all the $v_i$ are equal. Let $a_n = (1+1/n)^n$ and $b_n = (1+1/n)^{n+1}$.  We will prove that $a_n$ is an increasing sequence  and $b_n$ is an decreasing sequence.  Since $a_n < b_n$, this implies,  for any positive integers $n$ and $m$ with $m < n$  that $a_m < a_n < b_n < b_m$. For $a_n$, consider n values of $1+1/n$ and $1$ value of $1$.  Since their sum is $n+2$ and their product is $(1+1/n)^n$, by the AGMI,  $((n+2)/(n+1))^{n+1} > (1+1/n)^n$,  or $(1+1/(n+1))^{n+1} > (1+1/n)^n$,  or $a_{n+1} > a_n$.  For $b_n$, consider $n$ values of $1-1/n$ and $1$ value of $1$.  Since their sum is $n$ and their product is $(1-1/n)^n$, by the AGMI,  $(n/(n+1))^{n+1} > (1-1/n)^n$  or $(1+1/n)^{n+1} < (1+1/(n-1))^n$,  or $b_n > b_{n+1}$. Since $b_n-a_n  = (1+1/n)^{n+1} - (1+1/n)^n = (1+1/n)^n(1/n) =a_n/n $ and every $a_n$ is less than any $b_n$ and $b_3 = (1+1/3)^4 = 256/81 < 4$, $b_n-a_n < 4/n$, so $b_n$ and $a_n$ converge to a common limit. These proofs do not seem to be really elementary,  since they use the AGMI.  However, they use a special form of the AGMI,  where all but one of the values are the same,  and this will now be shown to be implied by BI,  and thus be truly elementary. Suppose we have $n-1$ values of $u$ and $1$ value of $v$ with $u$ and $v$ positive.  The AGMI for these values is  $(((n-1)u+v)/n)^n \ge u^{n-1}v$  with equality if and only if $u = v$.  We will now show that this is implied by BI: $(((n-1)u+v)/n)^n \ge u^{n-1}v$  is the same as  $(u+(v-u)/n)^n \ge u^n(v/u)$.  Dividing by $u^n$,  this is equivalent to  $(1+(v/u-1)/n)^n \ge v/u$.  By BI,  since $(v/u-1)/n > -1/n > -1$, $(1+(v/u-1)/n)^n \ge 1+n((v/u-1)/n) = v/u$  with equality only if $n=1$ or $v/u-1 = 0$. Thus BI implies this version of the AGMI.",,"['real-analysis', 'limits']"
7,$T$ surjective iff $T^*$ injective in infinite-dimensional Hilbert space?,surjective iff  injective in infinite-dimensional Hilbert space?,T T^*,"Let $T:H_1\rightarrow H_2$ be a bounded linear operator where $H_1$ and $H_2$ are Hilbert spaces. The Hilbert-adjoint is defined to the the operator $T^*:H_2\rightarrow H_1$ such that $\langle Tx,y\rangle_{H_2}=\langle x,T^*y\rangle_{H_1}$ for all $x\in H_1$ and $y\in H_2$. It can be shown that $\ker T^*=\left(\operatorname{im}\, T\right)^\perp$ and $\left(\ker T^*\right)^\perp=\overline{\operatorname{im}\, T}$ (see, e.g., here ). It follows that $$T \text{ surjective} \Rightarrow \operatorname{im}\, T = H_2 \Rightarrow \ker T^* = H_2^\perp = \{0\}\Rightarrow T^* \text{ injective}$$ In the finite-dimensional case, we have that $$T \text{ surjective} \iff T^* \text{ injective}$$ In the general infinite-dimensional case, does the same statement hold, or is there a counterexample where $T^*$ is injective but $T$ is not surjective?","Let $T:H_1\rightarrow H_2$ be a bounded linear operator where $H_1$ and $H_2$ are Hilbert spaces. The Hilbert-adjoint is defined to the the operator $T^*:H_2\rightarrow H_1$ such that $\langle Tx,y\rangle_{H_2}=\langle x,T^*y\rangle_{H_1}$ for all $x\in H_1$ and $y\in H_2$. It can be shown that $\ker T^*=\left(\operatorname{im}\, T\right)^\perp$ and $\left(\ker T^*\right)^\perp=\overline{\operatorname{im}\, T}$ (see, e.g., here ). It follows that $$T \text{ surjective} \Rightarrow \operatorname{im}\, T = H_2 \Rightarrow \ker T^* = H_2^\perp = \{0\}\Rightarrow T^* \text{ injective}$$ In the finite-dimensional case, we have that $$T \text{ surjective} \iff T^* \text{ injective}$$ In the general infinite-dimensional case, does the same statement hold, or is there a counterexample where $T^*$ is injective but $T$ is not surjective?",,"['real-analysis', 'functional-analysis', 'operator-theory']"
8,Convergence of $\sum\limits_{n=2}^\infty \frac{1}{n^\alpha \ln^\beta (n)} $ for nonnegative $\alpha$ and $\beta$,Convergence of  for nonnegative  and,\sum\limits_{n=2}^\infty \frac{1}{n^\alpha \ln^\beta (n)}  \alpha \beta,"Study the convergence of the following series: $$\sum_{n=2}^\infty \frac{1}{n^\alpha \cdot\ln^\beta(n)} \text{ where }\alpha,\beta \geq 0 $$ Applying d'Alembert criterion I have that $$ \lim_{n\to\infty} \frac{n^\alpha  \ln^\beta(n)}{(n+1)^\alpha  \ln^\beta (n+1)} = \lim_{n\to\infty} \left(\frac{n}{n+1}\right)^\alpha\left(\frac{\ln(n)}{\ln (n+1)}\right)^\beta = 1$$ so the nature of the series is inconclusive. If $\alpha = \beta = 0$, then the series diverges, since $\sum_{n=2}^\infty 1 = \infty$. Should I study the rest of the cases (i.e. if $\alpha = 0, \beta > 0$ the root test and the ratio test are also inconclusive). What is the best form to study the series?. Thanks in advance","Study the convergence of the following series: $$\sum_{n=2}^\infty \frac{1}{n^\alpha \cdot\ln^\beta(n)} \text{ where }\alpha,\beta \geq 0 $$ Applying d'Alembert criterion I have that $$ \lim_{n\to\infty} \frac{n^\alpha  \ln^\beta(n)}{(n+1)^\alpha  \ln^\beta (n+1)} = \lim_{n\to\infty} \left(\frac{n}{n+1}\right)^\alpha\left(\frac{\ln(n)}{\ln (n+1)}\right)^\beta = 1$$ so the nature of the series is inconclusive. If $\alpha = \beta = 0$, then the series diverges, since $\sum_{n=2}^\infty 1 = \infty$. Should I study the rest of the cases (i.e. if $\alpha = 0, \beta > 0$ the root test and the ratio test are also inconclusive). What is the best form to study the series?. Thanks in advance",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
9,Prove $\lim_{n \to \infty}\int_0^1 \dots \int_0^1 f(\sqrt[n]{x_1\dots x_n})dx_1\dots dx_n = f(\frac{1}{e}).$ $f$ is continuous on $[0;1].$,Prove   is continuous on,\lim_{n \to \infty}\int_0^1 \dots \int_0^1 f(\sqrt[n]{x_1\dots x_n})dx_1\dots dx_n = f(\frac{1}{e}). f [0;1].,"$f$ is continuous on $[0;1].$ Prove $$\lim_{n \to \infty} \underbrace{\int_0^1 \cdots \int_0^1}_{n} f(\sqrt[n]{x_1\cdots x_n})\mathrm \, dx_1\cdots \mathrm dx_n = f(\frac{1}{e}).$$ At first, I thought that we should get inside the function with the limit, but it's probably restricted due to $dx_1\dots dx_n$ . I feel like I'm missing an important Theorem here. And yet it seems like the problem should be easy. Can somebody smart help me out here (at least with a hint) ? Perhaps, it's an induction method problem.","is continuous on Prove At first, I thought that we should get inside the function with the limit, but it's probably restricted due to . I feel like I'm missing an important Theorem here. And yet it seems like the problem should be easy. Can somebody smart help me out here (at least with a hint) ? Perhaps, it's an induction method problem.","f [0;1]. \lim_{n \to \infty} \underbrace{\int_0^1 \cdots \int_0^1}_{n} f(\sqrt[n]{x_1\cdots x_n})\mathrm \, dx_1\cdots \mathrm dx_n = f(\frac{1}{e}). dx_1\dots dx_n","['real-analysis', 'calculus', 'integration', 'limits']"
10,Question about 'inner measure equals outer measure $\iff$ measurable set',Question about 'inner measure equals outer measure  measurable set',\iff,"I'm trying to solve the Folland Real analysis p.32 problem 19 and it was easy to show that the inner measure of a measurable set equals the outer measure. However I'm stuck at the converse. Could anyone help me how to prove that if the inner measure equals the outer meausure, then the set is measurable? This is the definition of measurability in Folland. And the problem 19 is the one that I'm stuck at.","I'm trying to solve the Folland Real analysis p.32 problem 19 and it was easy to show that the inner measure of a measurable set equals the outer measure. However I'm stuck at the converse. Could anyone help me how to prove that if the inner measure equals the outer meausure, then the set is measurable? This is the definition of measurability in Folland. And the problem 19 is the one that I'm stuck at.",,"['real-analysis', 'lebesgue-measure']"
11,Continuous decreasing function has a fixed point,Continuous decreasing function has a fixed point,,Let $f$ be continuous and decreasing everywhere on $\mathbb{R}$. Show that: 1) $f$ has a unique fixed point 2) $f\circ f$ has either an infinite number of fixed points or an odd number of fixed points. The first part is easy and I am sure it is available on this website. The basic idea is to use the fact that a decreasing function satisfies $\lim_{x \to -\infty}f(x) = A\text{ or }\infty$ and $\lim_{x \to \infty}f(x) = B\text{ or }-\infty$ and in each case apply the intermediate value theorem on $g(x) = f(x) - x$. The uniqueness of the fixed point is also easy to understand as $f(a) - a = 0 = f(b) - b$ would imply $b - a = f(b) - f(a)$. If $b \neq a$ then this goes against the decreasing nature of $f$. It is the second part of the problem which is bit troublesome. Let $h(x) = f(f(x))$ let $c$ be the unique fixed point of $f$ so that $f(c) = c$. This means that $f(f(c)) = f(c) = c$ so that $c$ is also a fixed point of $h = f \circ f$. But counting the number of fixed points of $h$ seems tricky. Any hints are welcome!,Let $f$ be continuous and decreasing everywhere on $\mathbb{R}$. Show that: 1) $f$ has a unique fixed point 2) $f\circ f$ has either an infinite number of fixed points or an odd number of fixed points. The first part is easy and I am sure it is available on this website. The basic idea is to use the fact that a decreasing function satisfies $\lim_{x \to -\infty}f(x) = A\text{ or }\infty$ and $\lim_{x \to \infty}f(x) = B\text{ or }-\infty$ and in each case apply the intermediate value theorem on $g(x) = f(x) - x$. The uniqueness of the fixed point is also easy to understand as $f(a) - a = 0 = f(b) - b$ would imply $b - a = f(b) - f(a)$. If $b \neq a$ then this goes against the decreasing nature of $f$. It is the second part of the problem which is bit troublesome. Let $h(x) = f(f(x))$ let $c$ be the unique fixed point of $f$ so that $f(c) = c$. This means that $f(f(c)) = f(c) = c$ so that $c$ is also a fixed point of $h = f \circ f$. But counting the number of fixed points of $h$ seems tricky. Any hints are welcome!,,"['calculus', 'real-analysis']"
12,$L^2$ function on finite interval implies $L^1$?,function on finite interval implies ?,L^2 L^1,"Let $a,b\in\mathbb{R}$. Suppse $f:\mathbb{R}\rightarrow\mathbb{C}$ is an $L^2$ function on the finite interval $(a,b)$. That is, $$\int_{a}^b|f(x)|^2dx<\infty$$ Is it always true that $f$ is an $L^1$ function on the same interval, that is, $$\int_{a}^b|f(x)|dx<\infty \text{ }?$$","Let $a,b\in\mathbb{R}$. Suppse $f:\mathbb{R}\rightarrow\mathbb{C}$ is an $L^2$ function on the finite interval $(a,b)$. That is, $$\int_{a}^b|f(x)|^2dx<\infty$$ Is it always true that $f$ is an $L^1$ function on the same interval, that is, $$\int_{a}^b|f(x)|dx<\infty \text{ }?$$",,"['real-analysis', 'lebesgue-integral']"
13,Which real functions have their higher derivatives tending pointwise to zero?,Which real functions have their higher derivatives tending pointwise to zero?,,"Let $\mathrm C^\infty\!(\Bbb R)$ be the space of infinitely differentiable functions $f:\Bbb R\rightarrow\Bbb R$, and define the subspace$$A:=\{f\in\mathrm C^\infty\!(\Bbb R):(\forall x\in \Bbb R)\lim_{n\rightarrow\infty} f^{(n)}(x)=0\},$$where $f^{(n)}$ is the $n$th derivative of $f\;(n=0,1,\dots).$ Clearly all polynomial functions are in $A$. Are any others? Edit: Alfonso has answered this question well, but is there any characterization of $A$ in terms of familiar types of function?","Let $\mathrm C^\infty\!(\Bbb R)$ be the space of infinitely differentiable functions $f:\Bbb R\rightarrow\Bbb R$, and define the subspace$$A:=\{f\in\mathrm C^\infty\!(\Bbb R):(\forall x\in \Bbb R)\lim_{n\rightarrow\infty} f^{(n)}(x)=0\},$$where $f^{(n)}$ is the $n$th derivative of $f\;(n=0,1,\dots).$ Clearly all polynomial functions are in $A$. Are any others? Edit: Alfonso has answered this question well, but is there any characterization of $A$ in terms of familiar types of function?",,['real-analysis']
14,Understanding rate of convergence and order of convergence,Understanding rate of convergence and order of convergence,,"I am trying to understand what is the difference between 'rate of convergence' and 'order of convergence'. Does anyone know an intuitive explanation of the difference between them? For example, say I have the sequence defined by $(1 + 1/n^2)$, $n>=1$ So it looks like $2, 1\frac{1}{4}, 1 \frac{1}{9}, 1 \frac{1}{16},...$ And has a limit of $1$ as n approaches infinity. So what is the rate of convergence and order of convergence for this example? And how does that sequence tie in with the convergence equation ( Wikipedia - Convergence speed for iterative methods ) - $$\lim_{k\to\infty} \frac{|x_{k+1} - L|}{|x_k - L|^q} = μ | μ > 0$$","I am trying to understand what is the difference between 'rate of convergence' and 'order of convergence'. Does anyone know an intuitive explanation of the difference between them? For example, say I have the sequence defined by $(1 + 1/n^2)$, $n>=1$ So it looks like $2, 1\frac{1}{4}, 1 \frac{1}{9}, 1 \frac{1}{16},...$ And has a limit of $1$ as n approaches infinity. So what is the rate of convergence and order of convergence for this example? And how does that sequence tie in with the convergence equation ( Wikipedia - Convergence speed for iterative methods ) - $$\lim_{k\to\infty} \frac{|x_{k+1} - L|}{|x_k - L|^q} = μ | μ > 0$$",,"['real-analysis', 'numerical-methods']"
15,A criterion for series convergence?,A criterion for series convergence?,,"I have a conjecture regarding series convergence that feels like it would be a useful tool to me if I could prove it, but I have been unable to prove or disprove it. Let $\sum a_n$ be a series of nonnegative terms. Define $\lambda(N)$ to be the number of terms of the series that are greater than $1/N$. Conjecture: If $\lambda(N)=O(N^\alpha)$, with $0<\alpha<1$, then $\sum a_n$ converges. The conjecture is based on the fact that this is true for series of the form $\sum 1/n^k$, $k$ constant (let $\alpha = 1/k$), and my intuition that the hypothesis of the conjecture is enough to make $\sum a_n$ ""sufficiently similar to or bounded by"" such a series. Can you offer a counterexample or point me toward an idea for a proof? (I tried to bound $\Delta\lambda(N) = \lambda(N+1)-\lambda(N)$ from the assumption $\lambda(N)=O(N^\alpha)$, since possibly excluding a finite number of terms, the series sum is bounded above by $\sum \Delta\lambda(N)/N$; but I couldn't see how to do this.)","I have a conjecture regarding series convergence that feels like it would be a useful tool to me if I could prove it, but I have been unable to prove or disprove it. Let $\sum a_n$ be a series of nonnegative terms. Define $\lambda(N)$ to be the number of terms of the series that are greater than $1/N$. Conjecture: If $\lambda(N)=O(N^\alpha)$, with $0<\alpha<1$, then $\sum a_n$ converges. The conjecture is based on the fact that this is true for series of the form $\sum 1/n^k$, $k$ constant (let $\alpha = 1/k$), and my intuition that the hypothesis of the conjecture is enough to make $\sum a_n$ ""sufficiently similar to or bounded by"" such a series. Can you offer a counterexample or point me toward an idea for a proof? (I tried to bound $\Delta\lambda(N) = \lambda(N+1)-\lambda(N)$ from the assumption $\lambda(N)=O(N^\alpha)$, since possibly excluding a finite number of terms, the series sum is bounded above by $\sum \Delta\lambda(N)/N$; but I couldn't see how to do this.)",,"['real-analysis', 'sequences-and-series']"
16,"Proving that the terms of the sequence $(nx-\lfloor nx \rfloor)$ is dense in $[0,1]$.",Proving that the terms of the sequence  is dense in .,"(nx-\lfloor nx \rfloor) [0,1]","I have been doing a basic math course on Real analysis...I encountered with a problem which follows as "" Prove that $na \pmod1$ is dense in $(0,1)$..where $a$ is an Irrational number , $n\ge1$... I tried to prove it using only basic principles...first of all I proved that above defined sequence is infinite..and also it is bounded...so by Bolzano-Weierstrass theorem it has a limit point in $(0,1)$..but to prove denseness I need to prove that for any given $(a,b)$ a subset of $(0,1)$ there is at least one element of the sequence...I am not getting how to figure out and link that limit point to that interval $(a,b)$..can any one help me in this..?...It would be of great help...","I have been doing a basic math course on Real analysis...I encountered with a problem which follows as "" Prove that $na \pmod1$ is dense in $(0,1)$..where $a$ is an Irrational number , $n\ge1$... I tried to prove it using only basic principles...first of all I proved that above defined sequence is infinite..and also it is bounded...so by Bolzano-Weierstrass theorem it has a limit point in $(0,1)$..but to prove denseness I need to prove that for any given $(a,b)$ a subset of $(0,1)$ there is at least one element of the sequence...I am not getting how to figure out and link that limit point to that interval $(a,b)$..can any one help me in this..?...It would be of great help...",,['real-analysis']
17,Convex sets as intersection of half spaces,Convex sets as intersection of half spaces,,"I want to prove that any closed convex sets can be written as an intersection of half spaces using only the separation theorem as a pre-requisite. I'm getting a feel that I need to show two sets are subsets of each other, but not being able to understand how exactly to go about it.","I want to prove that any closed convex sets can be written as an intersection of half spaces using only the separation theorem as a pre-requisite. I'm getting a feel that I need to show two sets are subsets of each other, but not being able to understand how exactly to go about it.",,"['real-analysis', 'convex-analysis']"
18,How to make a smart guess for this ODE,How to make a smart guess for this ODE,,"I am dealing with a strange problem currently, we have a differential equation  $$y(x)^2 = \pm \sqrt{-A \cos(x) - B \cos^2(x)+y'(x)-C},$$ where $C, A$ and $B $ are parameters. (The case that either $A$ or $B$ is zero is not interesting to me as these solutions are well-studied in the literature). So the idea of $C$ is to be some kind of adjustment variable, which means that I don't care about its value and it's only purpose is to make it easier to find solutions. Now my question is: Is there a smart way to do this so that I get a wide range of solutions and is there even a solution for all different kinds of $A $ and $B$, because I am particularly interested in having a free choice in these two paramters? But even if you are able to suggest a better guess to me that gives me more solutions, this would be tremendously helpful! Somehow I don't have much experience with differential equation and making this guess the way I did it, is probably not very sophisticated so I would be highly interested in hearing about better methods to do this. (Probably, if anybody here would tell me that he is able to solve this would remedy all my troubles, but I doubt that it is possible ;-))","I am dealing with a strange problem currently, we have a differential equation  $$y(x)^2 = \pm \sqrt{-A \cos(x) - B \cos^2(x)+y'(x)-C},$$ where $C, A$ and $B $ are parameters. (The case that either $A$ or $B$ is zero is not interesting to me as these solutions are well-studied in the literature). So the idea of $C$ is to be some kind of adjustment variable, which means that I don't care about its value and it's only purpose is to make it easier to find solutions. Now my question is: Is there a smart way to do this so that I get a wide range of solutions and is there even a solution for all different kinds of $A $ and $B$, because I am particularly interested in having a free choice in these two paramters? But even if you are able to suggest a better guess to me that gives me more solutions, this would be tremendously helpful! Somehow I don't have much experience with differential equation and making this guess the way I did it, is probably not very sophisticated so I would be highly interested in hearing about better methods to do this. (Probably, if anybody here would tell me that he is able to solve this would remedy all my troubles, but I doubt that it is possible ;-))",,"['calculus', 'real-analysis']"
19,Does there exist a real Hilbert space with countably infinite dimension as a vector space over $\mathbb{R}$?,Does there exist a real Hilbert space with countably infinite dimension as a vector space over ?,\mathbb{R},"Essentially what the title says - where to me a Hilbert space is a complete (Hermitian) inner product space, am I safe to assume every such real Hilbert space is of uncountable dimension over $\mathbb{R}$, or is there a countable-dimension example? Thanks a lot :)","Essentially what the title says - where to me a Hilbert space is a complete (Hermitian) inner product space, am I safe to assume every such real Hilbert space is of uncountable dimension over $\mathbb{R}$, or is there a countable-dimension example? Thanks a lot :)",,"['real-analysis', 'analysis', 'hilbert-spaces']"
20,"Does $\int_0^\infty \sin^2 (x^2)\, dx$ converge or diverge?",Does  converge or diverge?,"\int_0^\infty \sin^2 (x^2)\, dx","I'm trying to show determine if $\int_0^\infty \sin^2(x^2)\,dx$ converges. By continuity, we have that $\sin^2(x^2)$ is continuous on $[0,1]$, and therefore (by a theorem) it is Riemann integrable on $[0,1]$. And so, we will have that if $\int_1^\infty \sin^2(x^2) \,dx$ converges then so will $\int_0^\infty \sin^2(x^2) \, dx$. And so I'm left with $\int_0^\infty \sin^2(x^2) \,dx$ and I have no idea how to integrate this. Hints or help would be very much welcomed!","I'm trying to show determine if $\int_0^\infty \sin^2(x^2)\,dx$ converges. By continuity, we have that $\sin^2(x^2)$ is continuous on $[0,1]$, and therefore (by a theorem) it is Riemann integrable on $[0,1]$. And so, we will have that if $\int_1^\infty \sin^2(x^2) \,dx$ converges then so will $\int_0^\infty \sin^2(x^2) \, dx$. And so I'm left with $\int_0^\infty \sin^2(x^2) \,dx$ and I have no idea how to integrate this. Hints or help would be very much welcomed!",,"['real-analysis', 'improper-integrals']"
21,Prove that $f(x)$ is a constant function. [duplicate],Prove that  is a constant function. [duplicate],f(x),"This question already has answers here : Why is every such function constant: $f(x)=f(x^2)$ for $x \in [0,1]$? (4 answers) Closed 5 years ago . Here is the question:  Let f be a real valued continuous function on $[0, ∞)$. Suppose $f (x) = f (x^2)$ for all x ≥ 0, prove that f (x) is a constant function. My attempt: Since f(x) is continuous, and $f(x^2)$ is continuous, then $f(x)-f(x^2)$ is continuous. I will try to prove the contrapositive. If f(x) is not a constant function, then $f(x)$ does not equal $f(x^2)$  for all x. Let $f(x)$ be of the form $a_n*x^n + .... +a_1*x + a_0$  Let $f(x^2)$ be of the form $a_n*x^(2n) + .... +a_1*x^2 + a_0$  In both cases, $a_j$ does not equal 0 for all j in ${1,...,n}$ So $g(x)=f(x)-f(x^2)$ doesnt equal 0 for some x. Also, $g(x)$ is continuous.  Now I'm stuck, and your help would be appreciated. Also, we haven't yet covered derivatives in our class.","This question already has answers here : Why is every such function constant: $f(x)=f(x^2)$ for $x \in [0,1]$? (4 answers) Closed 5 years ago . Here is the question:  Let f be a real valued continuous function on $[0, ∞)$. Suppose $f (x) = f (x^2)$ for all x ≥ 0, prove that f (x) is a constant function. My attempt: Since f(x) is continuous, and $f(x^2)$ is continuous, then $f(x)-f(x^2)$ is continuous. I will try to prove the contrapositive. If f(x) is not a constant function, then $f(x)$ does not equal $f(x^2)$  for all x. Let $f(x)$ be of the form $a_n*x^n + .... +a_1*x + a_0$  Let $f(x^2)$ be of the form $a_n*x^(2n) + .... +a_1*x^2 + a_0$  In both cases, $a_j$ does not equal 0 for all j in ${1,...,n}$ So $g(x)=f(x)-f(x^2)$ doesnt equal 0 for some x. Also, $g(x)$ is continuous.  Now I'm stuck, and your help would be appreciated. Also, we haven't yet covered derivatives in our class.",,"['real-analysis', 'continuity']"
22,Prove that subsequence converges to limsup,Prove that subsequence converges to limsup,,"Given a sequence of real numbers, $\{ x_n \}_{n=1}^{\infty}$, let $\alpha =$ limsup$x_n$ and $\beta = $ liminf$x_n$. Prove that there exists a subsequence $\{ x_{n_k}\}$ that converges to $\alpha$ as $k \rightarrow \infty$. Not sure how to start this without since I'm not given that the subsequence is bounded..","Given a sequence of real numbers, $\{ x_n \}_{n=1}^{\infty}$, let $\alpha =$ limsup$x_n$ and $\beta = $ liminf$x_n$. Prove that there exists a subsequence $\{ x_{n_k}\}$ that converges to $\alpha$ as $k \rightarrow \infty$. Not sure how to start this without since I'm not given that the subsequence is bounded..",,"['real-analysis', 'limits', 'limsup-and-liminf']"
23,$f$ is a real function and it is $\alpha$-Holder continuous with $\alpha>1$. Is $f$ constant?,is a real function and it is -Holder continuous with . Is  constant?,f \alpha \alpha>1 f,"Maybe this is a well know result, however, I could not find it. Before stating it, let me write here a well know result (at least for me) Assume that $\Omega\subset\mathbb{R}^N$ is a open domain and $f:\Omega\to\mathbb{R}$. If there is constants $L>0$ and $\alpha>1$ such that $$|f(x)-f(y)|\leq L |x-y|^\alpha,\ \forall\ x,y\in\Omega$$   then, $f$ is constant in each connected componente of $\Omega$. The above result can be proved, for example, by showing that $\nabla f=0$ and then we join points in the same connected component by a continuous curve. Now my question is: Assume that $\Omega\subset\mathbb{R}^N$ and $f:\Omega\to\mathbb{R}$. Suppose that there is constants $L>0$ and $\alpha>1$ such that $$|f(x)-f(y)|\leq L |x-y|^\alpha,\ \forall\ x,y\in\Omega$$   Can we conclude that $f$ is constant in each connected componente of $\Omega$? Maybe it is necessary to add the hypothesis that each connected component of $\Omega$ is pathwise connected? Remark: Note that in the question, $\Omega$ does not need to be a open set. It is now any set.","Maybe this is a well know result, however, I could not find it. Before stating it, let me write here a well know result (at least for me) Assume that $\Omega\subset\mathbb{R}^N$ is a open domain and $f:\Omega\to\mathbb{R}$. If there is constants $L>0$ and $\alpha>1$ such that $$|f(x)-f(y)|\leq L |x-y|^\alpha,\ \forall\ x,y\in\Omega$$   then, $f$ is constant in each connected componente of $\Omega$. The above result can be proved, for example, by showing that $\nabla f=0$ and then we join points in the same connected component by a continuous curve. Now my question is: Assume that $\Omega\subset\mathbb{R}^N$ and $f:\Omega\to\mathbb{R}$. Suppose that there is constants $L>0$ and $\alpha>1$ such that $$|f(x)-f(y)|\leq L |x-y|^\alpha,\ \forall\ x,y\in\Omega$$   Can we conclude that $f$ is constant in each connected componente of $\Omega$? Maybe it is necessary to add the hypothesis that each connected component of $\Omega$ is pathwise connected? Remark: Note that in the question, $\Omega$ does not need to be a open set. It is now any set.",,"['real-analysis', 'holder-spaces']"
24,"A integral from ""irresistible integrals""","A integral from ""irresistible integrals""",,Evaluate : $$\int_0^1\frac{x}{(1-x+x^2)^2}\ln \ln \frac{1}{x}\text{d}x$$ The answer on the book is  $$-\frac{\gamma}{3}-\frac13\ln\frac{6\sqrt{3}}{\pi}+\frac{\pi\sqrt{3}}{27}\left(5\ln 2\pi-6\ln \Gamma\left(\frac16\right)\right)$$ Could anyone show a proof? Refers to chapter 12 of the book pg237.,Evaluate : $$\int_0^1\frac{x}{(1-x+x^2)^2}\ln \ln \frac{1}{x}\text{d}x$$ The answer on the book is  $$-\frac{\gamma}{3}-\frac13\ln\frac{6\sqrt{3}}{\pi}+\frac{\pi\sqrt{3}}{27}\left(5\ln 2\pi-6\ln \Gamma\left(\frac16\right)\right)$$ Could anyone show a proof? Refers to chapter 12 of the book pg237.,,"['calculus', 'real-analysis', 'sequences-and-series', 'integration']"
25,Chain rule for Hessian matrix,Chain rule for Hessian matrix,,Given $f\colon \mathbb{R}^n\rightarrow \mathbb{R}$ smooth and $\phi \in GL(n)$. What is the Hessian matrix $H_{f\circ \phi} = \left(\frac{\partial ^2 (f\circ \phi)}{\partial x_i\partial x_j}\right)_{ij}$?,Given $f\colon \mathbb{R}^n\rightarrow \mathbb{R}$ smooth and $\phi \in GL(n)$. What is the Hessian matrix $H_{f\circ \phi} = \left(\frac{\partial ^2 (f\circ \phi)}{\partial x_i\partial x_j}\right)_{ij}$?,,"['real-analysis', 'analysis', 'multivariable-calculus', 'vector-analysis']"
26,Prove the normed space of bounded variation functions is complete,Prove the normed space of bounded variation functions is complete,,"Let $\Vert f \Vert = |f(0)| + \mathrm{Var}f$ for all $f \in BV([0,1])$; we are given that it is a norm. Show that $BV([0,1])$ is a complete normed space with this norm. I have shown that any Cauchy sequence in $BV([0,1])$ must converge to some function pointwise, but I am stuck at proving that the function must have bounded variation. Could someone help me?","Let $\Vert f \Vert = |f(0)| + \mathrm{Var}f$ for all $f \in BV([0,1])$; we are given that it is a norm. Show that $BV([0,1])$ is a complete normed space with this norm. I have shown that any Cauchy sequence in $BV([0,1])$ must converge to some function pointwise, but I am stuck at proving that the function must have bounded variation. Could someone help me?",,"['real-analysis', 'banach-spaces', 'bounded-variation']"
27,What is the relationship between different definitions of Fourier transform?,What is the relationship between different definitions of Fourier transform?,,"I always see various definitions of Fourier transform. A standard form is: $$\hat{f}(\xi)=\int_{\mathbb{R}^d}f(x)e^{-2\pi ix\cdot\xi}dx$$ and its attached inversion is $$f(x)=\int_{\mathbb{R}^d}\hat{f}(\xi)e^{2\pi ix\cdot\xi}d\xi$$ Another form is like this: $$\hat{f}(\xi)=\int_{\mathbb{R}^d}f(x)e^{-ix\cdot\xi}dx$$ and the inversion formula is $$f(x)=\frac{1}{(2\pi)^d}\int_{\mathbb{R}^d}\hat{f}(\xi)e^{ix\cdot\xi}d\xi$$ I believe they are actually the same and I try to find their relationship. An article on ProofWiki says There exist several slightly different definitions of the Fourier transform which are commonly used; they differ in the choice of the constant 2π inside the exponential and/or a multiplicative constant before the integral.    Their properties are essentially the same, and by a simple change of variable one can always translate statements using one of the definitions into statements using another one. So I tried change of variable: $$ \int_{\mathbb{R}^d}f(x)e^{-ix\cdot\xi}dx=(2\pi)^d\int_{\mathbb{R}^d}f(2\pi x)e^{-2\pi ix\cdot\xi}dx$$ But then since the variable in $f$ is $2\pi ix$ rather than $x$, I don't know how to deal with it. Can you please help? Thank you. EDIT: According to James Edward Lewis, the change of variable should be $2\pi x$. I revised this.","I always see various definitions of Fourier transform. A standard form is: $$\hat{f}(\xi)=\int_{\mathbb{R}^d}f(x)e^{-2\pi ix\cdot\xi}dx$$ and its attached inversion is $$f(x)=\int_{\mathbb{R}^d}\hat{f}(\xi)e^{2\pi ix\cdot\xi}d\xi$$ Another form is like this: $$\hat{f}(\xi)=\int_{\mathbb{R}^d}f(x)e^{-ix\cdot\xi}dx$$ and the inversion formula is $$f(x)=\frac{1}{(2\pi)^d}\int_{\mathbb{R}^d}\hat{f}(\xi)e^{ix\cdot\xi}d\xi$$ I believe they are actually the same and I try to find their relationship. An article on ProofWiki says There exist several slightly different definitions of the Fourier transform which are commonly used; they differ in the choice of the constant 2π inside the exponential and/or a multiplicative constant before the integral.    Their properties are essentially the same, and by a simple change of variable one can always translate statements using one of the definitions into statements using another one. So I tried change of variable: $$ \int_{\mathbb{R}^d}f(x)e^{-ix\cdot\xi}dx=(2\pi)^d\int_{\mathbb{R}^d}f(2\pi x)e^{-2\pi ix\cdot\xi}dx$$ But then since the variable in $f$ is $2\pi ix$ rather than $x$, I don't know how to deal with it. Can you please help? Thank you. EDIT: According to James Edward Lewis, the change of variable should be $2\pi x$. I revised this.",,"['real-analysis', 'fourier-analysis']"
28,"Proof of bound on $\int t\,f(t)\ dt$ given well-behaved $f$",Proof of bound on  given well-behaved,"\int t\,f(t)\ dt f","I got the following question by mail from someone I don't know from Adam. (Quoted in part.) if $f(t)$ continuously diff. on $[0,1]$ and a) $\int_0^1f(t)\ dt=0$ b) $m\le f\,'\le M$ on $[0,1]$ Prove $\frac m{12}\le\int_0^1t\cdot f(t)\ dt\le\frac M{12}$ I suspect it might be an error I assumed immediately that it's an error, but my first two thoughts as counterexamples were $f(t)=\frac12-t$ and $f(t)=\sin(2\pi t)$, both of which satisfy the result. Anyone with a proof or counterexample?","I got the following question by mail from someone I don't know from Adam. (Quoted in part.) if $f(t)$ continuously diff. on $[0,1]$ and a) $\int_0^1f(t)\ dt=0$ b) $m\le f\,'\le M$ on $[0,1]$ Prove $\frac m{12}\le\int_0^1t\cdot f(t)\ dt\le\frac M{12}$ I suspect it might be an error I assumed immediately that it's an error, but my first two thoughts as counterexamples were $f(t)=\frac12-t$ and $f(t)=\sin(2\pi t)$, both of which satisfy the result. Anyone with a proof or counterexample?",,"['real-analysis', 'integration', 'inequality', 'integral-inequality']"
29,What is actually the standard definition for Radon measure?,What is actually the standard definition for Radon measure?,,"I see that there are various definitions for Radon measure and they are NOT equivalent, but they are equivalent on locally compact Hausdorff spaces. I think this is the reason why Radon measure has several different definitions, but I'm curious what is the standard one. On locally compact Hausdorff spaces, representation theorem let Radon measure play an important role. However, I think there would be some areas in mathematics that Radon measure still plays an important role when the given space is NOT locally compact hausdorff space. In that situation, what is the definition of Radon measure? (For example, under the definition given in Folland's Real Analysis, I found that Vitali-Carathéodory theorem is true for Radon measure on a Hausdorff space. (Not necessarily locally compact))","I see that there are various definitions for Radon measure and they are NOT equivalent, but they are equivalent on locally compact Hausdorff spaces. I think this is the reason why Radon measure has several different definitions, but I'm curious what is the standard one. On locally compact Hausdorff spaces, representation theorem let Radon measure play an important role. However, I think there would be some areas in mathematics that Radon measure still plays an important role when the given space is NOT locally compact hausdorff space. In that situation, what is the definition of Radon measure? (For example, under the definition given in Folland's Real Analysis, I found that Vitali-Carathéodory theorem is true for Radon measure on a Hausdorff space. (Not necessarily locally compact))",,"['real-analysis', 'definition']"
30,Behavior of derivative near the zero of a function.,Behavior of derivative near the zero of a function.,,"Suppose the function $f:[0,\delta) \to \mathbb{R}$ is continuous, differentiable in $(0,\delta)$ and $f(0)=0$. If the limit $\displaystyle \lim_{x \to 0+}\frac{f(x)}{f'(x)}= L$ exists, then is it always the case that $L = 0$. This seems to be true both for functions with well-behaved derivatives such as $f(x) = x,$ $$\lim_{x \to 0+} \frac{f(x)}{f'(x)}=\lim_{x \to 0+} \frac{x}{1}=0,$$ as well as functions with ""bad"" derivatives such as $$f(x) = \begin{cases}x \ln x &\mbox{if }x>0 ,\\0  &\mbox{if } x=0,  \end{cases},$$ where $$\lim_{x \to 0+} \frac{f(x)}{f'(x)}=\lim_{x \to 0+} \frac{x \ln x }{1+ \ln x}=0.$$ Is there a simple proof or counterexample?","Suppose the function $f:[0,\delta) \to \mathbb{R}$ is continuous, differentiable in $(0,\delta)$ and $f(0)=0$. If the limit $\displaystyle \lim_{x \to 0+}\frac{f(x)}{f'(x)}= L$ exists, then is it always the case that $L = 0$. This seems to be true both for functions with well-behaved derivatives such as $f(x) = x,$ $$\lim_{x \to 0+} \frac{f(x)}{f'(x)}=\lim_{x \to 0+} \frac{x}{1}=0,$$ as well as functions with ""bad"" derivatives such as $$f(x) = \begin{cases}x \ln x &\mbox{if }x>0 ,\\0  &\mbox{if } x=0,  \end{cases},$$ where $$\lim_{x \to 0+} \frac{f(x)}{f'(x)}=\lim_{x \to 0+} \frac{x \ln x }{1+ \ln x}=0.$$ Is there a simple proof or counterexample?",,"['calculus', 'real-analysis', 'limits']"
31,Ternary representation of Cantor set,Ternary representation of Cantor set,,"Associate to each sequence $a=\{\alpha_n\},$ in which $\alpha_n$ is $0$ or $2$, the real number $$x(a)=\sum \limits_{n=1}^{\infty}\frac{\alpha_n}{3^n}.$$ Prove that the set of all $x(a)$ is precisely the Cantor set. We know that $\frac{1}{3}\in C$ but also $\frac{1}{3}=\sum \limits_{n=2}^{\infty}\frac{2}{3^n}$ and $\frac{1}{3}=\frac{1}{3}+\sum \limits_{n=2}^{\infty}\frac{0}{3^n}$. And we have some ambiguity. Can we also add condition that $\alpha_j=1$ for some $j$? Can anyone explain this moment to me?","Associate to each sequence $a=\{\alpha_n\},$ in which $\alpha_n$ is $0$ or $2$, the real number $$x(a)=\sum \limits_{n=1}^{\infty}\frac{\alpha_n}{3^n}.$$ Prove that the set of all $x(a)$ is precisely the Cantor set. We know that $\frac{1}{3}\in C$ but also $\frac{1}{3}=\sum \limits_{n=2}^{\infty}\frac{2}{3^n}$ and $\frac{1}{3}=\frac{1}{3}+\sum \limits_{n=2}^{\infty}\frac{0}{3^n}$. And we have some ambiguity. Can we also add condition that $\alpha_j=1$ for some $j$? Can anyone explain this moment to me?",,"['real-analysis', 'cantor-set']"
32,Infinite Sum of Sines With Increasing Period,Infinite Sum of Sines With Increasing Period,,"A while ago, I was thinking about the Weierstrass function , which is a sum of sines with increasing frequencies in such a way that the curve is a fractal. However, I wondered what would happen if one took the sum where the frequencies decreased; in particular, noting that $|\sin(x)|\leq x$, it is clear that the function $$f(x)=\sum_{n=1}^{\infty}\sin\left(\frac{x}{s_n}\right)$$ converges pointwise for any sequence $s_n$ such that the sum of $\frac{1}{s_n}$ converges absolutely - and, in fact, yields an $f$ which is analytic. Of particular interest to me is the sequence of square numbers - that is, the function $$f(x)=\sum_{n=1}^{\infty}\sin\left(\frac{x}{n^2}\right).$$ I created the following plot of the function from the first 10,000 terms in the series: What I find interesting here is that, for some reason I can't determine, it looks like $f(x)$ might be asymptotic to $\sqrt{x}$. I've checked numerically for higher arguments and this seems to continue to be the case. This strikes me as odd, since I had expected it to appear more or less periodic, with long-term variation in amplitude and frequency. So, I am interested in a pair of questions about this series, neither of which I can answer: Is $f(0)=0$ the only (real) zero of $f$? Does $f$ grow without bound? What is it asymptotic to?","A while ago, I was thinking about the Weierstrass function , which is a sum of sines with increasing frequencies in such a way that the curve is a fractal. However, I wondered what would happen if one took the sum where the frequencies decreased; in particular, noting that $|\sin(x)|\leq x$, it is clear that the function $$f(x)=\sum_{n=1}^{\infty}\sin\left(\frac{x}{s_n}\right)$$ converges pointwise for any sequence $s_n$ such that the sum of $\frac{1}{s_n}$ converges absolutely - and, in fact, yields an $f$ which is analytic. Of particular interest to me is the sequence of square numbers - that is, the function $$f(x)=\sum_{n=1}^{\infty}\sin\left(\frac{x}{n^2}\right).$$ I created the following plot of the function from the first 10,000 terms in the series: What I find interesting here is that, for some reason I can't determine, it looks like $f(x)$ might be asymptotic to $\sqrt{x}$. I've checked numerically for higher arguments and this seems to continue to be the case. This strikes me as odd, since I had expected it to appear more or less periodic, with long-term variation in amplitude and frequency. So, I am interested in a pair of questions about this series, neither of which I can answer: Is $f(0)=0$ the only (real) zero of $f$? Does $f$ grow without bound? What is it asymptotic to?",,"['real-analysis', 'sequences-and-series', 'trigonometry', 'asymptotics']"
33,Dilogarithm inversion formula: $ \text{Li}_2(z) + \text{Li}_2(1/z) = -\zeta(2) - \log^2(-z)/2$,Dilogarithm inversion formula:, \text{Li}_2(z) + \text{Li}_2(1/z) = -\zeta(2) - \log^2(-z)/2,"I have been chugging through some proofs regarding the dilogarithm, also known as [Spencer's function][1]. \begin{alignat}{2}  		&  \operatorname{Li}_2(z) + \operatorname{Li}_2(-z) = \frac{1}{2} \operatorname{Li}_2(z^2)   		&& \text{(Double Identity)} \tag{1} \\ 		&  \operatorname{Li}_2(z) + \operatorname{Li}_2(1-z) = \frac{\pi^2}{6} - \log z \log (1 - z) \ 		&& \text{(Eulers reflection formula)} \tag{2} \\ 		&  \operatorname{Li}_2(-z) + \operatorname{Li}_2\left( \frac{z}{1+z} \right) = -\frac{1}{2} \log^2(z+1)   		&& \text{(Landen's Identity)} \tag{3} \\  		&  \operatorname{Li}_2(z) + \operatorname{Li}_2\left( \frac{1}{z} \right) = - \frac{\pi^2}{6} - \frac{1}{2}\log^2(-z) \ \  		&& \text{(Inversions formula)} \tag{4} \end{alignat} With the basis of $\text{Abel's Identity}$ (see Proving Abel's identity for the Dilogarithm. ). I have been able to prove all the the identities except the last. (My proof for $(3)$ , was somewhat convoluted, so hints there would be appreciated..). I have yet to prove $(4)$ , although I have given it two attempts below Attempt 1 In the Abel Identity let $x=y=1-z$ and divide by $2$ to obtain $$ \frac{1}{2}\log(-z)^2 = \operatorname{Li}_2\left( -\frac{1+z}{z}\right) - \frac{1}{2} \operatorname{Li}_2\left( \left[ \frac{1+z}{z} \right]^2 \right) -   \operatorname{Li}_2(1+z) $$ By now using $(1)$ with $z = [1+1/z]^2$ and inserting it into the equation above I obtain $$ \frac{1}{2}\log(-z)^2 = - \left[ \operatorname{Li}_2\left( \frac{1}{z} + 1\right)- \operatorname{Li}_2(1 + z)\right] $$ and from here I am stuck. It is close to what I want but I can not find any way to transform the right hand side. Attempt 2 From chat the suggestion was to instead look at the integral definition, this gives \begin{align*} \operatorname{Li}_2(z) + \operatorname{Li}_2\left( \frac{1}{z}\right)     & = - \int_0^z \frac{\log(1-t)}{t}\,\mathrm{d}t         - \int_0^{1/z} \frac{\log(1-t)}{t}\,\mathrm{d}t \\    &  =  - \int\limits_0^1 {\frac{{\log \left( {z - t} \right) + \log \left( {1 - zt} \right) - \log z}}{t}dt} \\       & = -\zeta(2) + \int_0^1 \frac{\log z - \log(1-zt)}{t}\,\mathrm{d}t \end{align*} The last step used that $$ \int_0^1 \frac{\log(1-t)}{t}\,\mathrm{d}t  = \int_0^1 -\frac{1}{t} \sum_{n=1}^\infty \frac{t^n}{n} \,\mathrm{d}t = \sum_{n=1}^\infty \frac{1}{n^2}  = \zeta(2) $$ And this is where I stopped. I think this argument can be finished by series expansion, but I got lost in the algebra. If possible I would very much like to prove this identity from Abel's Identity and the three equations stated above. Any hint or solutions is much appreciated as always =) [1]: http://en.wikipedia.org/wiki/Spence%27s_function","I have been chugging through some proofs regarding the dilogarithm, also known as [Spencer's function][1]. With the basis of (see Proving Abel's identity for the Dilogarithm. ). I have been able to prove all the the identities except the last. (My proof for , was somewhat convoluted, so hints there would be appreciated..). I have yet to prove , although I have given it two attempts below Attempt 1 In the Abel Identity let and divide by to obtain By now using with and inserting it into the equation above I obtain and from here I am stuck. It is close to what I want but I can not find any way to transform the right hand side. Attempt 2 From chat the suggestion was to instead look at the integral definition, this gives The last step used that And this is where I stopped. I think this argument can be finished by series expansion, but I got lost in the algebra. If possible I would very much like to prove this identity from Abel's Identity and the three equations stated above. Any hint or solutions is much appreciated as always =) [1]: http://en.wikipedia.org/wiki/Spence%27s_function","\begin{alignat}{2} 
		&  \operatorname{Li}_2(z) + \operatorname{Li}_2(-z) = \frac{1}{2} \operatorname{Li}_2(z^2)  
		&& \text{(Double Identity)} \tag{1} \\
		&  \operatorname{Li}_2(z) + \operatorname{Li}_2(1-z) = \frac{\pi^2}{6} - \log z \log (1 - z) \
		&& \text{(Eulers reflection formula)} \tag{2} \\
		&  \operatorname{Li}_2(-z) + \operatorname{Li}_2\left( \frac{z}{1+z} \right) = -\frac{1}{2} \log^2(z+1)  
		&& \text{(Landen's Identity)} \tag{3} \\ 
		&  \operatorname{Li}_2(z) + \operatorname{Li}_2\left( \frac{1}{z} \right) = - \frac{\pi^2}{6} - \frac{1}{2}\log^2(-z) \ \ 
		&& \text{(Inversions formula)} \tag{4}
\end{alignat} \text{Abel's Identity} (3) (4) x=y=1-z 2 
\frac{1}{2}\log(-z)^2 = \operatorname{Li}_2\left( -\frac{1+z}{z}\right) - \frac{1}{2} \operatorname{Li}_2\left( \left[ \frac{1+z}{z} \right]^2 \right) -   \operatorname{Li}_2(1+z)
 (1) z = [1+1/z]^2 
\frac{1}{2}\log(-z)^2 = - \left[ \operatorname{Li}_2\left( \frac{1}{z} + 1\right)- \operatorname{Li}_2(1 + z)\right]
 \begin{align*}
\operatorname{Li}_2(z) + \operatorname{Li}_2\left( \frac{1}{z}\right) 
   & = - \int_0^z \frac{\log(1-t)}{t}\,\mathrm{d}t 
       - \int_0^{1/z} \frac{\log(1-t)}{t}\,\mathrm{d}t \\
   &  =  - \int\limits_0^1 {\frac{{\log \left( {z - t} \right) + \log \left( {1 - zt} \right) - \log z}}{t}dt} \\ 
     & = -\zeta(2) + \int_0^1 \frac{\log z - \log(1-zt)}{t}\,\mathrm{d}t
\end{align*} 
\int_0^1 \frac{\log(1-t)}{t}\,\mathrm{d}t 
= \int_0^1 -\frac{1}{t} \sum_{n=1}^\infty \frac{t^n}{n} \,\mathrm{d}t
= \sum_{n=1}^\infty \frac{1}{n^2} 
= \zeta(2)
","['real-analysis', 'integration', 'special-functions']"
34,Period of derivative is the period of the original function,Period of derivative is the period of the original function,,"Let $f:I\to\mathbb R$  be a differentiable and periodic function with prime /minimum period $T$ (it is $T$-periodic) that is, $f(x+T) = f(x)$ for all $x\in I$. It is clear that $$ f'(x) = \lim_{h\to 0}\frac{f(x+h)-f(x)}{h} = \lim_{h\to 0} \frac{f(x+T+h) - f(x+T)}{h} = f'(x+T), $$ but how to prove that $f'$ has the same prime/minimum period $T$? I suppose that there exist $\tilde T < T$ such that $f'(x+\tilde T) = f'(x)$ for all $x\in I$ but can't find the way to get a contradiction.","Let $f:I\to\mathbb R$  be a differentiable and periodic function with prime /minimum period $T$ (it is $T$-periodic) that is, $f(x+T) = f(x)$ for all $x\in I$. It is clear that $$ f'(x) = \lim_{h\to 0}\frac{f(x+h)-f(x)}{h} = \lim_{h\to 0} \frac{f(x+T+h) - f(x+T)}{h} = f'(x+T), $$ but how to prove that $f'$ has the same prime/minimum period $T$? I suppose that there exist $\tilde T < T$ such that $f'(x+\tilde T) = f'(x)$ for all $x\in I$ but can't find the way to get a contradiction.",,"['real-analysis', 'derivatives', 'periodic-functions']"
35,"Showing a subset of $C([0,1])$ is compact.",Showing a subset of  is compact.,"C([0,1])","Let $${\cal F}=\left\{ f:\left[0,1\right]\to\mathbb{R} : \left|f\left(x\right)-f\left(y\right)\right|\le\left|x-y\right|\mbox{ and }{\displaystyle \int_{0}^{1}f\left(x\right)dx=1}\right\}.$$  Show that ${\cal F}$  is a compact subset of $C\left(\left[0,1\right]\right)$. When I am trying to show a set is compact, I usually resort to the every open cover has a finite subcover definition.  But if this case, we are dealing with functions.  So I am having a difficulty ""visualizing"" what's going on. Any help or solutions would be appreciated. Edit: I should mention that we are working with respect to the sup norm.","Let $${\cal F}=\left\{ f:\left[0,1\right]\to\mathbb{R} : \left|f\left(x\right)-f\left(y\right)\right|\le\left|x-y\right|\mbox{ and }{\displaystyle \int_{0}^{1}f\left(x\right)dx=1}\right\}.$$  Show that ${\cal F}$  is a compact subset of $C\left(\left[0,1\right]\right)$. When I am trying to show a set is compact, I usually resort to the every open cover has a finite subcover definition.  But if this case, we are dealing with functions.  So I am having a difficulty ""visualizing"" what's going on. Any help or solutions would be appreciated. Edit: I should mention that we are working with respect to the sup norm.",,"['real-analysis', 'functional-analysis']"
36,"Weird $\sin(\ln (x))$, $\cos(\ln (x))$ pattern","Weird ,  pattern",\sin(\ln (x)) \cos(\ln (x)),"I just came across these four integrals, evaluated them, and noticed they show this weird alternate pattern, I think its cool but I have no idea why they do. Anyone knows an intuitive reason for this? $$\hspace{1cm} \int_0^\infty \frac{\cos(\ln(x))}{1+x}\mathrm{d}x=0 \hspace{2cm} \int_0^\infty \frac{\sin(\ln(x))}{1+x}\mathrm{d}x= \frac{\pi}{\sinh(\pi)}$$ $$\int_0^\infty \frac{\cos(\ln(x))}{(1+x)^2}\mathrm{d}x= \frac{\pi}{\sinh(\pi)} \hspace{1cm} \int_0^\infty \frac{\sin(\ln(x))}{(1+x)^2}\mathrm{d}x=0 $$","I just came across these four integrals, evaluated them, and noticed they show this weird alternate pattern, I think its cool but I have no idea why they do. Anyone knows an intuitive reason for this?",\hspace{1cm} \int_0^\infty \frac{\cos(\ln(x))}{1+x}\mathrm{d}x=0 \hspace{2cm} \int_0^\infty \frac{\sin(\ln(x))}{1+x}\mathrm{d}x= \frac{\pi}{\sinh(\pi)} \int_0^\infty \frac{\cos(\ln(x))}{(1+x)^2}\mathrm{d}x= \frac{\pi}{\sinh(\pi)} \hspace{1cm} \int_0^\infty \frac{\sin(\ln(x))}{(1+x)^2}\mathrm{d}x=0 ,"['real-analysis', 'calculus', 'integration', 'definite-integrals', 'trigonometric-integrals']"
37,The 'Square root' Function,The 'Square root' Function,,"$G := \{f : f:[0,1] \rightarrow [0,1]$ such that it is bijective function  and strictly increasing } Now the question is For any $ h \in G,$does there exist $g \in G$ such that $h=g \circ g $? Is such a $g$, if it exist , unique? My observation : $G$ is a group under function composition.(Is it helpful?) Every function in $G$ is continuous. Conjecture: if $h \in G$ has $n \in \mathbb{N}$ fixed points in (0,1) then it has $n+1$ 'square root' functions. Please help me to solve the question!","$G := \{f : f:[0,1] \rightarrow [0,1]$ such that it is bijective function  and strictly increasing } Now the question is For any $ h \in G,$does there exist $g \in G$ such that $h=g \circ g $? Is such a $g$, if it exist , unique? My observation : $G$ is a group under function composition.(Is it helpful?) Every function in $G$ is continuous. Conjecture: if $h \in G$ has $n \in \mathbb{N}$ fixed points in (0,1) then it has $n+1$ 'square root' functions. Please help me to solve the question!",,"['real-analysis', 'functions']"
38,Infinite Series $\sum\limits_{n=1}^{\infty}\frac{(-1)^n}{n}\left\lfloor\frac{\log(n)}{\log(2)}\right\rfloor$,Infinite Series,\sum\limits_{n=1}^{\infty}\frac{(-1)^n}{n}\left\lfloor\frac{\log(n)}{\log(2)}\right\rfloor,How to prove that $$\sum_{n=1}^{\infty}\frac{(-1)^n}{n}\left\lfloor\frac{\log(n)}{\log(2)}\right\rfloor=\gamma$$ Can we find a known value for $\sum_{n=1}^{\infty}\frac{(-1)^n}{n}\left\lfloor\frac{\log(n)}{\log(k)}\right\rfloor$ for any $k\in\mathbb{N}?$,How to prove that $$\sum_{n=1}^{\infty}\frac{(-1)^n}{n}\left\lfloor\frac{\log(n)}{\log(2)}\right\rfloor=\gamma$$ Can we find a known value for $\sum_{n=1}^{\infty}\frac{(-1)^n}{n}\left\lfloor\frac{\log(n)}{\log(k)}\right\rfloor$ for any $k\in\mathbb{N}?$,,"['real-analysis', 'sequences-and-series', 'summation', 'closed-form', 'euler-mascheroni-constant']"
39,"If $x_{n}$ is decreasing and $\sum x_{n}$ converges, prove that $\lim nx_{n} = 0$ [duplicate]","If  is decreasing and  converges, prove that  [duplicate]",x_{n} \sum x_{n} \lim nx_{n} = 0,"This question already has answers here : If $(a_n)\subset[0,\infty)$ is non-increasing and $\sum_{n=1}^\infty a_n<\infty$, then $\lim\limits_{n\to\infty}{n a_n} = 0$ (16 answers) Closed 4 years ago . I need a little hint in a proof on convergence of a sequence. The problem I have is: Suppose $(x_n)$ is a monotone decreasing sequence of real numbers such that $\sum x_n$ converges, prove that: $$\lim_{n \to \infty} nx_n=0$$ My idea is: proving this fact aims to prove that given $\epsilon >0$ there's some natural $n_0 \in \mathbb{N}$ such that if $n >n_0$ we have $|nx_n|<\epsilon$. Now, I know that $\sum x_n$ converges, so that given $\epsilon'>0$ there's some $k_0 \in \mathbb{N}$ such that if $k > k_0$ we have: $$\left|\sum_{i=1}^{k}x_i - S\right|<\epsilon'$$ Where $S = \sum x_n$. Now, since $(x_n)$ is monotone decreasing we know that we must have $x_1 > \cdots > x_k$ so that the sum of all $x_i$ should be less or equal to $k x_k$. So I know that I have: $$\left|\sum_{i=1}^{k}x_i - S\right|\leq\left|\sum_{i=1}^{k}x_i\right|+|S|\leq|kx_k|+|S|$$ I feel that the proof will come from this, however I'm stuck at this point. Can someone give just a little hint on how to proceed from here? Thanks very much in advance for your help.","This question already has answers here : If $(a_n)\subset[0,\infty)$ is non-increasing and $\sum_{n=1}^\infty a_n<\infty$, then $\lim\limits_{n\to\infty}{n a_n} = 0$ (16 answers) Closed 4 years ago . I need a little hint in a proof on convergence of a sequence. The problem I have is: Suppose $(x_n)$ is a monotone decreasing sequence of real numbers such that $\sum x_n$ converges, prove that: $$\lim_{n \to \infty} nx_n=0$$ My idea is: proving this fact aims to prove that given $\epsilon >0$ there's some natural $n_0 \in \mathbb{N}$ such that if $n >n_0$ we have $|nx_n|<\epsilon$. Now, I know that $\sum x_n$ converges, so that given $\epsilon'>0$ there's some $k_0 \in \mathbb{N}$ such that if $k > k_0$ we have: $$\left|\sum_{i=1}^{k}x_i - S\right|<\epsilon'$$ Where $S = \sum x_n$. Now, since $(x_n)$ is monotone decreasing we know that we must have $x_1 > \cdots > x_k$ so that the sum of all $x_i$ should be less or equal to $k x_k$. So I know that I have: $$\left|\sum_{i=1}^{k}x_i - S\right|\leq\left|\sum_{i=1}^{k}x_i\right|+|S|\leq|kx_k|+|S|$$ I feel that the proof will come from this, however I'm stuck at this point. Can someone give just a little hint on how to proceed from here? Thanks very much in advance for your help.",,"['real-analysis', 'sequences-and-series']"
40,Sum of strictly convex and convex functions,Sum of strictly convex and convex functions,,"My prof mentioned that the sum of strictly convex and convex functions is strictly convex, Im having trouble swallowing that, is it accurate?","My prof mentioned that the sum of strictly convex and convex functions is strictly convex, Im having trouble swallowing that, is it accurate?",,"['real-analysis', 'convex-analysis']"
41,Prove $\frac{\text{Area}_1}{c_1^2}+\frac{\text{Area}_2}{c_2^2}\neq \frac{\text{Area}_3}{c_3^2}$ for all primitive Pythagorean triples,Prove  for all primitive Pythagorean triples,\frac{\text{Area}_1}{c_1^2}+\frac{\text{Area}_2}{c_2^2}\neq \frac{\text{Area}_3}{c_3^2},"Important update Yam Mir has found a more general form and Mathlove has found a necessary condition but as of now the problem is still open. Earlier I posted this pretty gross equality that I was trying to prove, $a,b,c,d,e,f \in \mathbb{N}-0, \gcd(a,b)=1 \  \wedge \ \gcd(c,d) = 1 \ \wedge \ \gcd(e,f) = 1, (a,b) \neq (c,d) \neq (e,f)$ $$\Rightarrow \frac{4a^3b-4ab^3}{a^4+2a^2b^2+b^4} + \frac{4c^3d-4cd^3}{c^4+2c^2d^2+d^4} \neq \frac{4e^3f-4ef^3}{e^4+2e^2f^2+f^4}$$ Which I completely missed some beautiful underlying math for, I've found that the terms can be rewritten as such, $$\frac{4m^3n-4mn^3}{m^4+2m^2n^2+n^4}=\frac{4mn(m-n)(m+n)}{(m^2+n^2)(m^2+n^2)} = \frac{4mn(m^2-n^2)}{(m^2+n^2)^2}$$ Since it is parameterized as $\gcd(a,b) = 1 \ \wedge \ a>b>0$ . It can be parametrized as a primitive Pythagorean triple! So now let, $$a=2mn, b=m^2-n^2,c=m^2+n^2$$ we get, $$\frac{2a_1b_1}{c_1^2}+\frac{2a_2b_2}{c_2^2} \neq \frac{2a_3b_3}{c^2_3}$$ Where $a_n,b_n,c_n$ form a primitive Pythagorean triple dividing by four yields, $$\frac{ab}{2c^2} = \text{Area}\cdot\frac{1}{c^2}$$ For terminology sake let's call this the characteristic ratio of a primitive Pythagorean triple. My conjecture is that for all primitive Pythagoreon triples, $$\frac{a_1b_1}{2c_1^2}+\frac{a_2b_2}{2c_2^2}\neq \frac{a_3b_3}{2c_3^2}$$ Interestingly I've found, $$\frac{1}{c_n^2} \approx \frac{1}{4n^2\pi^2}$$ plotting ratios from the original equation gives this curve indicating some kind of cyclical phenomenon, Another thing I've observed, $$\max{\frac{2a_nb_n}{c_n^2}} = 1$$ Additionally the numerator of the original inequality appears to be all congruent numbers apart of this sequence ! So to sum things up I'm trying to show that, $$\frac{\text{Area}_1}{c_1^2} + \frac{\text{Area}_2}{c_2^2} \neq \frac{\text{Area}_3}{c_3^2}$$ For all primitive Pythagorean triples or find a counter example. I'd also like to know why this may be true and if there is any regularity to the cyclical phenomenon showed? Must these ratios be unique given that primitive triples are rooted in prime factorization? What geometric meaning can be drawn from $\frac{\text{Area}}{c^2}$ , why the hypotenuse squared? (note these ratio's might also flirt with the Dirichlet L-function and or elliptic curves.) Edit @mathlove found a counter example but I unfortunately wrote the wrong parameterization failing to list $a>b>0$ so I am still looking for a different counter example. The problem is still open Edit for bounty: To be very specific about what I'm asking for, I'd like to prove $\frac{\text{Area}_1}{c_1^2}+\frac{\text{Area}_2}{c_2^2} \neq \frac{\text{Area}_3}{c_3^2}$ for all primitive Pythagorean triples or find a counter example. The other questions would be nice but is in no way a requirement to receive the bounty. This bounty will cost me almost $1/3$ of my reputation so even just commenting and sharing thoughts/ideas would go a long way.","Important update Yam Mir has found a more general form and Mathlove has found a necessary condition but as of now the problem is still open. Earlier I posted this pretty gross equality that I was trying to prove, Which I completely missed some beautiful underlying math for, I've found that the terms can be rewritten as such, Since it is parameterized as . It can be parametrized as a primitive Pythagorean triple! So now let, we get, Where form a primitive Pythagorean triple dividing by four yields, For terminology sake let's call this the characteristic ratio of a primitive Pythagorean triple. My conjecture is that for all primitive Pythagoreon triples, Interestingly I've found, plotting ratios from the original equation gives this curve indicating some kind of cyclical phenomenon, Another thing I've observed, Additionally the numerator of the original inequality appears to be all congruent numbers apart of this sequence ! So to sum things up I'm trying to show that, For all primitive Pythagorean triples or find a counter example. I'd also like to know why this may be true and if there is any regularity to the cyclical phenomenon showed? Must these ratios be unique given that primitive triples are rooted in prime factorization? What geometric meaning can be drawn from , why the hypotenuse squared? (note these ratio's might also flirt with the Dirichlet L-function and or elliptic curves.) Edit @mathlove found a counter example but I unfortunately wrote the wrong parameterization failing to list so I am still looking for a different counter example. The problem is still open Edit for bounty: To be very specific about what I'm asking for, I'd like to prove for all primitive Pythagorean triples or find a counter example. The other questions would be nice but is in no way a requirement to receive the bounty. This bounty will cost me almost of my reputation so even just commenting and sharing thoughts/ideas would go a long way.","a,b,c,d,e,f \in \mathbb{N}-0, \gcd(a,b)=1 \  \wedge \ \gcd(c,d) = 1 \ \wedge \ \gcd(e,f) = 1, (a,b) \neq (c,d) \neq (e,f) \Rightarrow \frac{4a^3b-4ab^3}{a^4+2a^2b^2+b^4} + \frac{4c^3d-4cd^3}{c^4+2c^2d^2+d^4} \neq \frac{4e^3f-4ef^3}{e^4+2e^2f^2+f^4} \frac{4m^3n-4mn^3}{m^4+2m^2n^2+n^4}=\frac{4mn(m-n)(m+n)}{(m^2+n^2)(m^2+n^2)} = \frac{4mn(m^2-n^2)}{(m^2+n^2)^2} \gcd(a,b) = 1 \ \wedge \ a>b>0 a=2mn, b=m^2-n^2,c=m^2+n^2 \frac{2a_1b_1}{c_1^2}+\frac{2a_2b_2}{c_2^2} \neq \frac{2a_3b_3}{c^2_3} a_n,b_n,c_n \frac{ab}{2c^2} = \text{Area}\cdot\frac{1}{c^2} \frac{a_1b_1}{2c_1^2}+\frac{a_2b_2}{2c_2^2}\neq \frac{a_3b_3}{2c_3^2} \frac{1}{c_n^2} \approx \frac{1}{4n^2\pi^2} \max{\frac{2a_nb_n}{c_n^2}} = 1 \frac{\text{Area}_1}{c_1^2} + \frac{\text{Area}_2}{c_2^2} \neq \frac{\text{Area}_3}{c_3^2} \frac{\text{Area}}{c^2} a>b>0 \frac{\text{Area}_1}{c_1^2}+\frac{\text{Area}_2}{c_2^2} \neq \frac{\text{Area}_3}{c_3^2} 1/3","['real-analysis', 'geometry', 'number-theory', 'diophantine-equations']"
42,Riemann sum on infinite interval,Riemann sum on infinite interval,,"It is well known that in the case of a finite interval $[0,1]$ with a partition of equal size $1/n$, we have: $$\lim_{n\rightarrow \infty} \frac{1}{n}\sum_{k=0}^{n-1} f\left(\frac{k}{n}\right)=\int_0^1 f(x)dx$$ I was wondering under which conditions on $f$ this could be extended to the case of the positive real line, i.e which conditions on $f$ would enable us to be able to rigorously write: $$\lim_{n\rightarrow \infty} \frac{1}{n}\sum_{k\geq0} f\left(\frac{k}{n}\right)=\int_0^{\infty} f(x)dx$$ Any ideas or references to literature would be greatly appreciated.","It is well known that in the case of a finite interval $[0,1]$ with a partition of equal size $1/n$, we have: $$\lim_{n\rightarrow \infty} \frac{1}{n}\sum_{k=0}^{n-1} f\left(\frac{k}{n}\right)=\int_0^1 f(x)dx$$ I was wondering under which conditions on $f$ this could be extended to the case of the positive real line, i.e which conditions on $f$ would enable us to be able to rigorously write: $$\lim_{n\rightarrow \infty} \frac{1}{n}\sum_{k\geq0} f\left(\frac{k}{n}\right)=\int_0^{\infty} f(x)dx$$ Any ideas or references to literature would be greatly appreciated.",,"['real-analysis', 'integration', 'analysis', 'limits', 'riemann-sum']"
43,Show that every interval is a Borel set,Show that every interval is a Borel set,,"Show that every interval is a Borel set. My textbook states: The intersection of all the $\sigma$-algebras of subsets of $\mathbb{R}$ that contain the open sets is a $\sigma$-algebra called the Borel $\sigma$-algebra; members of this collection are called Borel sets. My answer: Since $(-\infty, a)$, $(a,b)$, $(a,\infty)$ are open, they must be Borel. For any interval of the following forms, we also see that they can be represented by open intervals. $$ \begin{align} [a,b]&=\bigcap^{\infty}_{n=1} (a-1/n, b+1/n)\\ (a,b]&=\bigcap^{\infty}_{n=1} (a, b+1/n)\\ [a,b)&=\bigcap^{\infty}_{n=1} (a-1/n, b) \end{align} $$ Do you think my answer is correct?","Show that every interval is a Borel set. My textbook states: The intersection of all the $\sigma$-algebras of subsets of $\mathbb{R}$ that contain the open sets is a $\sigma$-algebra called the Borel $\sigma$-algebra; members of this collection are called Borel sets. My answer: Since $(-\infty, a)$, $(a,b)$, $(a,\infty)$ are open, they must be Borel. For any interval of the following forms, we also see that they can be represented by open intervals. $$ \begin{align} [a,b]&=\bigcap^{\infty}_{n=1} (a-1/n, b+1/n)\\ (a,b]&=\bigcap^{\infty}_{n=1} (a, b+1/n)\\ [a,b)&=\bigcap^{\infty}_{n=1} (a-1/n, b) \end{align} $$ Do you think my answer is correct?",,[]
44,References about Sierpinski's Theorem regarding Darboux functions,References about Sierpinski's Theorem regarding Darboux functions,,"I am writing something about the following two theorems: Every function $f: \Bbb{R} \to \Bbb{R}$ can be written $f=f_1+f_2$ where $f_1,f_2:\Bbb{R} \to \Bbb{R}$ both have the Darboux property. Denote (C) the Cauchy functional equation: $$ f: \Bbb{R} \to \Bbb{R}, \ f(x+y)=f(x)+f(y)$$ Prove that every solution of (C) can be written $f=f_1+f_2$ where $f_1,f_2$ are (discontinuous) solutions of (C) which have the Darboux property. How can I find the papers where the original proofs of these theorem first appeared. I tried googling, but Wikipedia doesn't work today. If the original articles aren't available, then maybe there are some books which contain the proof of the first theorem; those are good also. I only know a Romanian reference where the proof appears, but I would like to know a known English book which contains the proof. Thank you. I found the second theorem in a problem book, and maybe it is more recent than the first one.","I am writing something about the following two theorems: Every function can be written where both have the Darboux property. Denote (C) the Cauchy functional equation: Prove that every solution of (C) can be written where are (discontinuous) solutions of (C) which have the Darboux property. How can I find the papers where the original proofs of these theorem first appeared. I tried googling, but Wikipedia doesn't work today. If the original articles aren't available, then maybe there are some books which contain the proof of the first theorem; those are good also. I only know a Romanian reference where the proof appears, but I would like to know a known English book which contains the proof. Thank you. I found the second theorem in a problem book, and maybe it is more recent than the first one.","f: \Bbb{R} \to \Bbb{R} f=f_1+f_2 f_1,f_2:\Bbb{R} \to \Bbb{R}  f: \Bbb{R} \to \Bbb{R}, \ f(x+y)=f(x)+f(y) f=f_1+f_2 f_1,f_2","['real-analysis', 'reference-request', 'functional-equations']"
45,Find all functions $f:\mathbb{R}^+\to \mathbb{R}$ such that $xf(xf(x)-4)-1=4x$,Find all functions  such that,f:\mathbb{R}^+\to \mathbb{R} xf(xf(x)-4)-1=4x,"Find all functions $f:\mathbb{R}^+\to \mathbb{R}$ such that for all $x\in\mathbb{R}^+$ the following is valid: $$xf\big(xf(x)-4\big)-1=4x$$ All I could do is: $f(x)> {4\over x}$ for all $x$ so $f(x)>0$ for all $x$ . $(4,\infty )\subseteq {\rm Range}(f)$ , since $$f(xf(x)-4)={4x+1\over x} >4$$ Function $g(x)=xf(x)-4$ is injective: \begin{align}g(x_1)=g(x_2) &\implies  f(g(x_1))=f(g(x_2))\\&\implies {4x_1+1\over x_1}={4x_2+1\over x_2} \\&\implies x_1=x_2\end{align} Function $g$ satisfies $$\boxed{xg(g(x)) -(4x+1)g(x)+4x=0}$$","Find all functions such that for all the following is valid: All I could do is: for all so for all . , since Function is injective: Function satisfies","f:\mathbb{R}^+\to \mathbb{R} x\in\mathbb{R}^+ xf\big(xf(x)-4\big)-1=4x f(x)> {4\over x} x f(x)>0 x (4,\infty )\subseteq {\rm Range}(f) f(xf(x)-4)={4x+1\over x} >4 g(x)=xf(x)-4 \begin{align}g(x_1)=g(x_2) &\implies  f(g(x_1))=f(g(x_2))\\&\implies {4x_1+1\over x_1}={4x_2+1\over x_2} \\&\implies x_1=x_2\end{align} g \boxed{xg(g(x)) -(4x+1)g(x)+4x=0}","['real-analysis', 'contest-math', 'functional-equations']"
46,Show uniform convergence of bounded functions implies uniform boundness.,Show uniform convergence of bounded functions implies uniform boundness.,,"Suppose that $f_{n} : E\rightarrow \mathbb{R}$ is a sequence of bounded functions that converge uniformly. Prove that there exists $M > 0$ such that for all $n \in \mathbb{N}$ and $x\in E$, $$\left | f_{n}(x) \right | \leq M$$ I think this has to do with the supremum, but not quite sure.","Suppose that $f_{n} : E\rightarrow \mathbb{R}$ is a sequence of bounded functions that converge uniformly. Prove that there exists $M > 0$ such that for all $n \in \mathbb{N}$ and $x\in E$, $$\left | f_{n}(x) \right | \leq M$$ I think this has to do with the supremum, but not quite sure.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'uniform-convergence']"
47,A daunting double integral with a simple closed form,A daunting double integral with a simple closed form,,"In the following, I'll present a curious double integral that despite its daunting look has a very nice closed form, $$\int _0^{\pi/2}\int _0^{\pi/2}\cot (x) \csc ^2(y) \log (\cos (y)) \log \left(1-2 \sin (x)+\sin ^2(x) \csc ^2(y)\right)\textrm{d}x \textrm{d}y$$ $$=\frac{\pi^3}{6}-\pi.$$ The integral was recently proposed by C. I. Valean . He exploited a new double integral representation of the Dilogarithm he recently derived and presented in The Dilogarithm: A New Representation in Terms of a Double Integral . The solution flow (in large steps) is as follows: Exploiting that $\displaystyle \operatorname{Li}_2(u)=-\frac{1}{\pi}\int _0^{\pi/2}\int _0^{\pi/2}\frac{(1-u)\log \left(1-2 \sin (x)+ \csc ^2(y) \sin ^2(x)\right)}{\tan (x) \left(\cos ^2(y)+(1-u)^2 \sin ^2(y)\right)}\textrm{d}x \textrm{d}y, \ u<1$ , if we multiply both sides by $\pi$ and then integrate from $u=0$ to $u=1$ , we have $$\small \int _0^{\pi/2}\left(\int _0^{\pi/2}\cot(x)\log \left(1-2 \sin (x)+ \csc ^2(y) \sin ^2(x)\right)\left(-\int_0^1\frac{(1-u)}{\cos ^2(y)+(1-u)^2 \sin ^2(y)}\textrm{d}u\right)\textrm{d}x\right)\textrm{d}y$$ $$\small =\frac{1}{2}\int _0^{\pi/2}\left(\int _0^{\pi/2}\cot(x)\log \left(1-2 \sin (x)+ \csc ^2(y) \sin ^2(x)\right)\csc^2(y) \log(\cos^2(y)+(1-u)^2 \sin^2(y)\biggr|_{u=0}^{u=1}\textrm{d}x\right)\textrm{d}y$$ $$\small =\int _0^{\pi/2}\left(\int _0^{\pi/2}\cot (x) \csc ^2(y) \log (\cos (y)) \log \left(1-2 \sin (x)+\sin ^2(x) \csc ^2(y)\right)\textrm{d}x\right)\textrm{d}y$$ $$ =\pi \int_0^1 \operatorname{Li}_2(u) \textrm{d}u=\pi \int_0^1 u' \operatorname{Li}_2(u) \textrm{d}u=\pi \underbrace{u\operatorname{Li}_2(u)\biggr|_{u=0}^{u=1}}_{\displaystyle \pi^2/6} +\pi \underbrace{\int_0^1 \log(1-u)\textrm{d}u}_{\displaystyle -1}$$ $$=\frac{\pi^3}{6}-\pi.$$ I would enjoy a lot to see different approaches (possibly without using the Dilogarithm). Given the simplicity of the closed form one might be tempted to ponder over the possibility of getting other elegant ways of performing the calculations. EDIT_1 : Cornel says the double integral representation of $\operatorname{Li}_2$ exploited in the solution can be reduced to the following (fascinating) integral in one variable: $$\operatorname{Li}_2(u)=\frac{\pi ^2}{6}-\frac{1}{\pi }\int_0^{\pi/2} \arctan((1-u) \tan (x)) (\pi+2 \cot (x) \log (\cot (x))) \operatorname{d}x,$$ which, if you ask me, looks too beautiful to be true (how can such a beautiful thing exist?). The solution is straightforward at this point if we differentiate with respect to $u$ . EDIT_2 : The transformation from the double integral representation of $\operatorname{Li}_2$ to the single integral representation above takes place by proving and using that $$\int _0^{\pi/2}\cot (x) \log \left(1-2 \sin (x)+\sin ^2(x) \csc ^2(y)\right)\textrm{d}x$$ $$=-\frac{\pi ^2}{3}+\int_y^{\pi/2} (\pi +2 \cot (x) \log (\cot (x)) ) \textrm{d}x,$$ and at the same time, this fact can also be employed separately and directly to the main integral to get a second solution. EDIT_3 : Here is another example where the given double integral representation of $\operatorname{Li}_2$ plays a crucial part and immediately allows us to connect the integral with known resulting integrals: $$\int _0^{\pi/2}\int _0^{\pi/2} \operatorname{arctanh}(\sin(y))\csc (y)\cot (x) \log \left(1-2 \sin (x)+\sin ^2(x) \csc ^2(y)\right)\textrm{d}x \textrm{d}y$$ $$=2\log(2)\pi G-\frac{9}{8}\log^2(2)\pi^2-\frac{\pi^3}{6}-\frac{17}{96}\pi^4+12 \pi \Im\biggr\{\operatorname{Li}_3\left(\frac{1+i}{2}\right)\biggr\}.$$ EDIT_4 : More generally, we have the following polylogarithmic representation, $n\ge0$ , $$ \operatorname{Li}_{n+2}(u)$$ $$\small =(-1)^{n-1}\frac{1}{\pi}\frac{1}{n!}\int _0^{\pi/2}\int _0^{\pi/2}\frac{(1-u)\log^n(\sin(x))\log \left(1-2 \sin (x)+ \csc ^2(y) \sin ^2(x)\right)}{\tan (x) \left(\cos ^2(y)+(1-u)^2 \sin ^2(y)\right)}\textrm{d}x \textrm{d}y, \ u<1.$$ EDIT_5 : In view of the generalization above, which can be proved by exploiting a similar idea to the one stated at EDIT_2 , we obtain a double integral very similar to the initial one, and so nice: $$\small \int _0^{\pi/2}\int _0^{\pi/2}\cot (x) \csc ^2(y) \log(\sin(x))\log (\cos (y)) \log \left(1-2 \sin (x)+\sin ^2(x) \csc ^2(y)\right)\textrm{d}x \textrm{d}y$$ $$=\frac{\pi^3}{6}-\pi-\pi \zeta(3).$$ EDIT_6 : The Trilogarithmic version can also be approached like the Dilogarithm version, presented in the link attached in the beginning. Essentially, we combine $$\int_0^1 \frac{\log \left(\csc ^2(x) t^2 -2 t+1\right)\log(t)}{t} \textrm{d}t$$ $$=-\sum_{n=1}^{\infty}  \left(\frac{H_n^2}{n}+\frac{H_n^{(2)}}{n}+2\frac{H_n\overline{H}_n}{n}+2\frac{\overline{H}_n^{(2)}}{n}-2\frac{1}{n}\sum_{k=1}^n (-1)^{k-1}\frac{H_k}{k}\right)\cos(2 n x)$$ $$= -\sum_{n=1}^{\infty}\left(\frac{H_n^2}{n}+\frac{H_n^{(2)}}{n}+2\frac{1}{n}\sum_{k=1}^n\frac{\overline{H}_k}{k}\right)\cos(2 n x)$$ and $$-\operatorname{Li}_3\left(\frac{2x}{x-1}\right)$$ $$= \sum_{n=1}^{\infty}  x^n\left(\frac{1}{2}\frac{H_n^2}{n}+\frac{1}{2}\frac{H_n^{(2)}}{n}+\frac{1}{n}\sum_{k=1}^n\frac{\overline{H}_k}{k}\right)$$ $$=\sum_{n=1}^{\infty}  x^n\left(\frac{1}{2}\frac{H_n^2}{n}+\frac{1}{2}\frac{H_n^{(2)}}{n}+\frac{H_n\overline{H}_n}{n}+\frac{\overline{H}_n^{(2)}}{n}-\frac{1}{n}\sum_{k=1}^n (-1)^{k-1}\frac{H_k}{k}\right)$$ , which appear in Analogues of the established Landen-type identities in the form of series and some related Cauchy products by C.I. Valean and Deriving Special Fourier Series Involving the Inverse Tangent Integrals of Order Two and Three, and Other Curious Functions by C.I. Valean . EDIT_7 : Cornel says that we can perfectly consider and exploit the ideas in the paper The Dilogarithm: A New Representation in Terms of a Double Integral (for the Polylogarithm version) where the dilogarithmic version is proved. For higher order when the complicated coefficients of the key powers series and Fourier series pop up, we can write them generically by using say, $a_n$ and $c_n \cdot b_n$ , and this is enough (of course, they turn out to be complicated as we consider polylogarithms of higher orders). These details will be explained later in another paper. EDIT_8: Such strategies are very powerful in deriving difficult results, as we may see in More (Almost) Impossible Integrals, Sums, and Series: A New Collection of Fiendish Problems and Surprising Solutions (2023), pages 73-74 $$\int_0^{\pi/2}\frac{\log^4(\cos(\theta))}{\cos^2(\theta)+y^2 \sin^2(\theta)}\textrm{d}\theta=\frac{1}{16}\int_0^{\infty} \frac{\log^4(1+x^2)}{1+y^2 x^2}\textrm{d}x$$ $$= \frac{\pi}{96}(7\pi^4+24 \log^2(2)\pi^2+48\log^4(2)-288\log(2)\zeta(3))\frac{1}{y}$$ $$-\frac{1}{2}\log(2)\pi(4\log^2(2)+3\pi^2)\frac{1}{y}\log\left(\frac{2y}{1+y}\right)+\frac{\pi^3}{2}\frac{1}{y}\log^2\left(\frac{2y}{1+y}\right)$$ $$-3\log(2)\pi \frac{\log(y)}{y}\log^2\left(\frac{2y}{1+y}\right)+3\log(2)\pi \frac{\log(1-y)}{y}\log^2\left(\frac{2y}{1+y}\right)$$ $$+\frac{3}{2}\log(2)\pi \frac{1}{y}\log^3\left(\frac{2y}{1+y}\right)+\frac{\pi}{2}\frac{\log(y)}{y}\log^3\left(\frac{2y}{1+y}\right)-\frac{\pi}{2}\frac{\log(1-y)}{y}\log^3\left(\frac{2y}{1+y}\right)$$ $$ -\frac{\pi}{8}\frac{1}{y}\log^4\left(\frac{2y}{1+y}\right)+\frac{\pi}{4}(12\log^2(2)+\pi^2)\frac{1}{y}\operatorname{Li}_2\left(\frac{1-y}{1+y}\right)+3\log(2)\pi \frac{1}{y}\operatorname{Li}_3\left(\frac{1-y}{1+y}\right)$$ $$+6\log(2)\pi \frac{1}{y}\operatorname{Li}_3\left(\frac{2y}{1+y}\right)-\frac{3}{2}\pi\frac{1}{y} \operatorname{Li}_4\left(\frac{1-y}{1+y}\right)-3\pi\frac{1}{y} \operatorname{Li}_4\left(\frac{2y}{1+y}\right)$$ $$ -3\pi \frac{1}{y} \operatorname{Li}_4\left(\frac{y-1}{2y}\right), \ y>0.$$ EDIT_9: probably the final edit. Thanks Cornel for sharing fantastic calculations.","In the following, I'll present a curious double integral that despite its daunting look has a very nice closed form, The integral was recently proposed by C. I. Valean . He exploited a new double integral representation of the Dilogarithm he recently derived and presented in The Dilogarithm: A New Representation in Terms of a Double Integral . The solution flow (in large steps) is as follows: Exploiting that , if we multiply both sides by and then integrate from to , we have I would enjoy a lot to see different approaches (possibly without using the Dilogarithm). Given the simplicity of the closed form one might be tempted to ponder over the possibility of getting other elegant ways of performing the calculations. EDIT_1 : Cornel says the double integral representation of exploited in the solution can be reduced to the following (fascinating) integral in one variable: which, if you ask me, looks too beautiful to be true (how can such a beautiful thing exist?). The solution is straightforward at this point if we differentiate with respect to . EDIT_2 : The transformation from the double integral representation of to the single integral representation above takes place by proving and using that and at the same time, this fact can also be employed separately and directly to the main integral to get a second solution. EDIT_3 : Here is another example where the given double integral representation of plays a crucial part and immediately allows us to connect the integral with known resulting integrals: EDIT_4 : More generally, we have the following polylogarithmic representation, , EDIT_5 : In view of the generalization above, which can be proved by exploiting a similar idea to the one stated at EDIT_2 , we obtain a double integral very similar to the initial one, and so nice: EDIT_6 : The Trilogarithmic version can also be approached like the Dilogarithm version, presented in the link attached in the beginning. Essentially, we combine and , which appear in Analogues of the established Landen-type identities in the form of series and some related Cauchy products by C.I. Valean and Deriving Special Fourier Series Involving the Inverse Tangent Integrals of Order Two and Three, and Other Curious Functions by C.I. Valean . EDIT_7 : Cornel says that we can perfectly consider and exploit the ideas in the paper The Dilogarithm: A New Representation in Terms of a Double Integral (for the Polylogarithm version) where the dilogarithmic version is proved. For higher order when the complicated coefficients of the key powers series and Fourier series pop up, we can write them generically by using say, and , and this is enough (of course, they turn out to be complicated as we consider polylogarithms of higher orders). These details will be explained later in another paper. EDIT_8: Such strategies are very powerful in deriving difficult results, as we may see in More (Almost) Impossible Integrals, Sums, and Series: A New Collection of Fiendish Problems and Surprising Solutions (2023), pages 73-74 EDIT_9: probably the final edit. Thanks Cornel for sharing fantastic calculations.","\int _0^{\pi/2}\int _0^{\pi/2}\cot (x) \csc ^2(y) \log (\cos (y)) \log \left(1-2 \sin (x)+\sin ^2(x) \csc ^2(y)\right)\textrm{d}x \textrm{d}y =\frac{\pi^3}{6}-\pi. \displaystyle \operatorname{Li}_2(u)=-\frac{1}{\pi}\int _0^{\pi/2}\int _0^{\pi/2}\frac{(1-u)\log \left(1-2 \sin (x)+ \csc ^2(y) \sin ^2(x)\right)}{\tan (x) \left(\cos ^2(y)+(1-u)^2 \sin ^2(y)\right)}\textrm{d}x \textrm{d}y, \ u<1 \pi u=0 u=1 \small \int _0^{\pi/2}\left(\int _0^{\pi/2}\cot(x)\log \left(1-2 \sin (x)+ \csc ^2(y) \sin ^2(x)\right)\left(-\int_0^1\frac{(1-u)}{\cos ^2(y)+(1-u)^2 \sin ^2(y)}\textrm{d}u\right)\textrm{d}x\right)\textrm{d}y \small =\frac{1}{2}\int _0^{\pi/2}\left(\int _0^{\pi/2}\cot(x)\log \left(1-2 \sin (x)+ \csc ^2(y) \sin ^2(x)\right)\csc^2(y) \log(\cos^2(y)+(1-u)^2 \sin^2(y)\biggr|_{u=0}^{u=1}\textrm{d}x\right)\textrm{d}y \small =\int _0^{\pi/2}\left(\int _0^{\pi/2}\cot (x) \csc ^2(y) \log (\cos (y)) \log \left(1-2 \sin (x)+\sin ^2(x) \csc ^2(y)\right)\textrm{d}x\right)\textrm{d}y 
=\pi \int_0^1 \operatorname{Li}_2(u) \textrm{d}u=\pi \int_0^1 u' \operatorname{Li}_2(u) \textrm{d}u=\pi \underbrace{u\operatorname{Li}_2(u)\biggr|_{u=0}^{u=1}}_{\displaystyle \pi^2/6} +\pi \underbrace{\int_0^1 \log(1-u)\textrm{d}u}_{\displaystyle -1} =\frac{\pi^3}{6}-\pi. \operatorname{Li}_2 \operatorname{Li}_2(u)=\frac{\pi ^2}{6}-\frac{1}{\pi }\int_0^{\pi/2} \arctan((1-u) \tan (x)) (\pi+2 \cot (x) \log (\cot (x))) \operatorname{d}x, u \operatorname{Li}_2 \int _0^{\pi/2}\cot (x) \log \left(1-2 \sin (x)+\sin ^2(x) \csc ^2(y)\right)\textrm{d}x =-\frac{\pi ^2}{3}+\int_y^{\pi/2} (\pi +2 \cot (x) \log (\cot (x)) ) \textrm{d}x, \operatorname{Li}_2 \int _0^{\pi/2}\int _0^{\pi/2} \operatorname{arctanh}(\sin(y))\csc (y)\cot (x) \log \left(1-2 \sin (x)+\sin ^2(x) \csc ^2(y)\right)\textrm{d}x \textrm{d}y =2\log(2)\pi G-\frac{9}{8}\log^2(2)\pi^2-\frac{\pi^3}{6}-\frac{17}{96}\pi^4+12 \pi \Im\biggr\{\operatorname{Li}_3\left(\frac{1+i}{2}\right)\biggr\}. n\ge0  \operatorname{Li}_{n+2}(u) \small =(-1)^{n-1}\frac{1}{\pi}\frac{1}{n!}\int _0^{\pi/2}\int _0^{\pi/2}\frac{(1-u)\log^n(\sin(x))\log \left(1-2 \sin (x)+ \csc ^2(y) \sin ^2(x)\right)}{\tan (x) \left(\cos ^2(y)+(1-u)^2 \sin ^2(y)\right)}\textrm{d}x \textrm{d}y, \ u<1. \small \int _0^{\pi/2}\int _0^{\pi/2}\cot (x) \csc ^2(y) \log(\sin(x))\log (\cos (y)) \log \left(1-2 \sin (x)+\sin ^2(x) \csc ^2(y)\right)\textrm{d}x \textrm{d}y =\frac{\pi^3}{6}-\pi-\pi \zeta(3). \int_0^1 \frac{\log \left(\csc ^2(x) t^2 -2 t+1\right)\log(t)}{t} \textrm{d}t =-\sum_{n=1}^{\infty}  \left(\frac{H_n^2}{n}+\frac{H_n^{(2)}}{n}+2\frac{H_n\overline{H}_n}{n}+2\frac{\overline{H}_n^{(2)}}{n}-2\frac{1}{n}\sum_{k=1}^n (-1)^{k-1}\frac{H_k}{k}\right)\cos(2 n x) = -\sum_{n=1}^{\infty}\left(\frac{H_n^2}{n}+\frac{H_n^{(2)}}{n}+2\frac{1}{n}\sum_{k=1}^n\frac{\overline{H}_k}{k}\right)\cos(2 n x) -\operatorname{Li}_3\left(\frac{2x}{x-1}\right) = \sum_{n=1}^{\infty}  x^n\left(\frac{1}{2}\frac{H_n^2}{n}+\frac{1}{2}\frac{H_n^{(2)}}{n}+\frac{1}{n}\sum_{k=1}^n\frac{\overline{H}_k}{k}\right) =\sum_{n=1}^{\infty}  x^n\left(\frac{1}{2}\frac{H_n^2}{n}+\frac{1}{2}\frac{H_n^{(2)}}{n}+\frac{H_n\overline{H}_n}{n}+\frac{\overline{H}_n^{(2)}}{n}-\frac{1}{n}\sum_{k=1}^n (-1)^{k-1}\frac{H_k}{k}\right) a_n c_n \cdot b_n \int_0^{\pi/2}\frac{\log^4(\cos(\theta))}{\cos^2(\theta)+y^2 \sin^2(\theta)}\textrm{d}\theta=\frac{1}{16}\int_0^{\infty} \frac{\log^4(1+x^2)}{1+y^2 x^2}\textrm{d}x = \frac{\pi}{96}(7\pi^4+24 \log^2(2)\pi^2+48\log^4(2)-288\log(2)\zeta(3))\frac{1}{y} -\frac{1}{2}\log(2)\pi(4\log^2(2)+3\pi^2)\frac{1}{y}\log\left(\frac{2y}{1+y}\right)+\frac{\pi^3}{2}\frac{1}{y}\log^2\left(\frac{2y}{1+y}\right) -3\log(2)\pi \frac{\log(y)}{y}\log^2\left(\frac{2y}{1+y}\right)+3\log(2)\pi \frac{\log(1-y)}{y}\log^2\left(\frac{2y}{1+y}\right) +\frac{3}{2}\log(2)\pi \frac{1}{y}\log^3\left(\frac{2y}{1+y}\right)+\frac{\pi}{2}\frac{\log(y)}{y}\log^3\left(\frac{2y}{1+y}\right)-\frac{\pi}{2}\frac{\log(1-y)}{y}\log^3\left(\frac{2y}{1+y}\right) 
-\frac{\pi}{8}\frac{1}{y}\log^4\left(\frac{2y}{1+y}\right)+\frac{\pi}{4}(12\log^2(2)+\pi^2)\frac{1}{y}\operatorname{Li}_2\left(\frac{1-y}{1+y}\right)+3\log(2)\pi \frac{1}{y}\operatorname{Li}_3\left(\frac{1-y}{1+y}\right) +6\log(2)\pi \frac{1}{y}\operatorname{Li}_3\left(\frac{2y}{1+y}\right)-\frac{3}{2}\pi\frac{1}{y} \operatorname{Li}_4\left(\frac{1-y}{1+y}\right)-3\pi\frac{1}{y} \operatorname{Li}_4\left(\frac{2y}{1+y}\right) 
-3\pi \frac{1}{y} \operatorname{Li}_4\left(\frac{y-1}{2y}\right), \ y>0.","['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'definite-integrals']"
48,Show that $\sum_{n=1}^{\infty} \frac{(-1)^{n}}{\sqrt{n}}\sin(1 + \frac{x}{n})$ converges uniformly on $\mathbb{R}$,Show that  converges uniformly on,\sum_{n=1}^{\infty} \frac{(-1)^{n}}{\sqrt{n}}\sin(1 + \frac{x}{n}) \mathbb{R},"I am trying to show the followng: Show that $\sum_{n=1}^{\infty} \frac{(-1)^{n}}{\sqrt{n}}\sin(1 + \frac{x}{n})$ converges uniformly on $\mathbb{R}$ . I have shown it for a compact subset of $\mathbb{R}$ however, do not know how to extend it to the reals. Below is my proof for convergence on a compact subset. I break up the sum into two parts and attack each individually $\sum_{n=1}^{\infty} \frac{(-1)^{n}}{\sqrt{n}}\sin(1 + \frac{x}{n})$ = $\underbrace{\sum_{n=1}^{\infty} \frac{(-1)^{n}}{\sqrt{n}}\sin(1)}_{(1)}$ + $\underbrace{\sum_{n=1}^{\infty} \frac{(-1)^{n}}{\sqrt{n}}[\sin(1 + \frac{x}{n})-\sin(1)]}_{(2)}$ . $\textbf{Equation (1)}$ $\sum_{n=1}^{\infty} \frac{(-1)^{n}}{\sqrt{n}}\sin(1)$ converges because $\sum_{n=1}^{\infty} \frac{(-1)^{n}}{\sqrt{n}}$ converges by the alternating series test. $\textbf{Equation (2)}$ we will first bound $\sin(1 + \frac{x}{n})-\sin(1)$ as follows: \begin{align*} |\sin(1 + \frac{x}{n})-\sin(1)| &= \Big|\int_{1}^{1+\frac{x}{n}} \cos(t)dt\Big| \\                                 &\leq \Big|\int_{1}^{1+\frac{x}{n}} |\cos(t)| dt\Big| \\                                 &\leq \Big|\int_{1}^{1+\frac{x}{n}} dt\Big| \\                                 &\leq \Big|1+\frac{x}{n} - 1 \Big| \\                                 &= \frac{|x|}{n} \\ \end{align*} Therefore, on any compact interval $ x \in [-M, M]$ \begin{align*} |\sin(1 + \frac{x}{n})-\sin(1)| \leq \frac{M}{n} \end{align*} It follows that \begin{align*} \Big|\frac{(-1)^{n}}{\sqrt{n}} \Big( \sin(1 + \frac{x}{n})-\sin(1)\Big)\Big| &= \Big|\frac{(-1)^{n}}{\sqrt{n}}\Big|\Big|\sin(1 + \frac{x}{n})-\sin(1)\Big| \\ &= \frac{1}{\sqrt{n}}\Big|\sin(1 + \frac{x}{n})-\sin(1)\Big| \\ &\leq \frac{1}{\sqrt{n}}\frac{M}{n} \\ &= \frac{M}{n^{\frac{3}{2}}} \end{align*} Hence, \begin{align*} \sum_{n}^{\infty}\frac{1}{n^{\frac{3}{2}}} < \infty &\implies \sum_{n}^{\infty}\frac{M}{n^{\frac{3}{2}}} < \infty \\ &\implies \sum_{n=1}^{\infty}\Big|\frac{(-1)^{n}}{\sqrt{n}} \Big[ \sin(1 + \frac{x}{n})-\sin(1)\Big]\Big| < \infty \\ &\implies \sum_{n=1}^{\infty}\frac{(-1)^{n}}{\sqrt{n}} \Big[ \sin(1 + \frac{x}{n})-\sin(1)\Big] < \infty \end{align*} Hence, we have that both $(1)$ and $(2)$ converge on a compact interval $[-M,M]$ which implies our original equation of interest does also. I want to extend this proof to all $\mathbb{R}$ but I do not know how to. Could someone please help me with this. Anything is appreciated.","I am trying to show the followng: Show that converges uniformly on . I have shown it for a compact subset of however, do not know how to extend it to the reals. Below is my proof for convergence on a compact subset. I break up the sum into two parts and attack each individually = + . converges because converges by the alternating series test. we will first bound as follows: Therefore, on any compact interval It follows that Hence, Hence, we have that both and converge on a compact interval which implies our original equation of interest does also. I want to extend this proof to all but I do not know how to. Could someone please help me with this. Anything is appreciated.","\sum_{n=1}^{\infty} \frac{(-1)^{n}}{\sqrt{n}}\sin(1 + \frac{x}{n}) \mathbb{R} \mathbb{R} \sum_{n=1}^{\infty} \frac{(-1)^{n}}{\sqrt{n}}\sin(1 + \frac{x}{n}) \underbrace{\sum_{n=1}^{\infty} \frac{(-1)^{n}}{\sqrt{n}}\sin(1)}_{(1)} \underbrace{\sum_{n=1}^{\infty} \frac{(-1)^{n}}{\sqrt{n}}[\sin(1 + \frac{x}{n})-\sin(1)]}_{(2)} \textbf{Equation (1)} \sum_{n=1}^{\infty} \frac{(-1)^{n}}{\sqrt{n}}\sin(1) \sum_{n=1}^{\infty} \frac{(-1)^{n}}{\sqrt{n}} \textbf{Equation (2)} \sin(1 + \frac{x}{n})-\sin(1) \begin{align*}
|\sin(1 + \frac{x}{n})-\sin(1)| &= \Big|\int_{1}^{1+\frac{x}{n}} \cos(t)dt\Big| \\
                                &\leq \Big|\int_{1}^{1+\frac{x}{n}} |\cos(t)| dt\Big| \\
                                &\leq \Big|\int_{1}^{1+\frac{x}{n}} dt\Big| \\
                                &\leq \Big|1+\frac{x}{n} - 1 \Big| \\
                                &= \frac{|x|}{n} \\
\end{align*}  x \in [-M, M] \begin{align*}
|\sin(1 + \frac{x}{n})-\sin(1)| \leq \frac{M}{n}
\end{align*} \begin{align*}
\Big|\frac{(-1)^{n}}{\sqrt{n}} \Big( \sin(1 + \frac{x}{n})-\sin(1)\Big)\Big| &= \Big|\frac{(-1)^{n}}{\sqrt{n}}\Big|\Big|\sin(1 + \frac{x}{n})-\sin(1)\Big| \\
&= \frac{1}{\sqrt{n}}\Big|\sin(1 + \frac{x}{n})-\sin(1)\Big| \\
&\leq \frac{1}{\sqrt{n}}\frac{M}{n} \\
&= \frac{M}{n^{\frac{3}{2}}}
\end{align*} \begin{align*}
\sum_{n}^{\infty}\frac{1}{n^{\frac{3}{2}}} < \infty &\implies \sum_{n}^{\infty}\frac{M}{n^{\frac{3}{2}}} < \infty \\
&\implies \sum_{n=1}^{\infty}\Big|\frac{(-1)^{n}}{\sqrt{n}} \Big[ \sin(1 + \frac{x}{n})-\sin(1)\Big]\Big| < \infty \\
&\implies \sum_{n=1}^{\infty}\frac{(-1)^{n}}{\sqrt{n}} \Big[ \sin(1 + \frac{x}{n})-\sin(1)\Big] < \infty
\end{align*} (1) (2) [-M,M] \mathbb{R}","['real-analysis', 'sequences-and-series', 'functional-analysis', 'convergence-divergence']"
49,Limiting Behaviour of Mean Value Theorem ($\theta \to \frac12$ as $h \to 0$),Limiting Behaviour of Mean Value Theorem ( as ),\theta \to \frac12 h \to 0,"I have a function, $f$, which is continuous on $[a, a+h]$ and differentiable on $(a, a+h)$. By the mean value theorem, there exists a $\theta \in (0,1)$ such that, $$f(a+h) - f(a) = h f'(a+\theta h)$$ Clearly, $\theta$ generally depends on $h$. The goal is to prove that, given that $f''(a)$ exists and is non-zero , $$\lim_{h \to 0}  \theta = 1/2$$ Note: Using Taylor's theorem this is not too difficult, however , as per the source of the question, there is a solution that uses nothing more than the meant value theorem (including Cauchy's), l'Hopital's rule, and whatever else would generally be considered more 'basic' (e.g. definition of derivative, rules of limits, etc.). Edit: If the question is not too clear, an example of a specific case of what I seek to prove can be found here (the very last part of the post): http://www.stumblingrobot.com/2015/09/27/prove-an-alternate-expression-for-the-mean-value-formula/ .","I have a function, $f$, which is continuous on $[a, a+h]$ and differentiable on $(a, a+h)$. By the mean value theorem, there exists a $\theta \in (0,1)$ such that, $$f(a+h) - f(a) = h f'(a+\theta h)$$ Clearly, $\theta$ generally depends on $h$. The goal is to prove that, given that $f''(a)$ exists and is non-zero , $$\lim_{h \to 0}  \theta = 1/2$$ Note: Using Taylor's theorem this is not too difficult, however , as per the source of the question, there is a solution that uses nothing more than the meant value theorem (including Cauchy's), l'Hopital's rule, and whatever else would generally be considered more 'basic' (e.g. definition of derivative, rules of limits, etc.). Edit: If the question is not too clear, an example of a specific case of what I seek to prove can be found here (the very last part of the post): http://www.stumblingrobot.com/2015/09/27/prove-an-alternate-expression-for-the-mean-value-formula/ .",,"['calculus', 'real-analysis', 'limits', 'derivatives']"
50,"$f>0$ on $[0,1]$ implies $\int_0^1 f >0$",on  implies,"f>0 [0,1] \int_0^1 f >0","Someone made the remark on my old question (second-to-last comment on the answer from here ) that a integrable function $f>0$ on $[0,1]$ does not imply $\int_0^1 f >0$ since  limits do not preserve strict inequality. But I think it is true and I will try to give a proof. Since $\{f>0\} = \cup_{n=1}^{\infty}\{f>\frac{1}{n}\}$, from continuity of Lebesgue measure  $$ 1= m(\{f>0\}) = m\left(\bigcup_{n=1}^{\infty}\Big\{f>\frac{1}{n}\Big\}\right) = \lim_{n\rightarrow \infty} m\left(\Big\{f>\frac{1}{n}\Big\}\right),$$ this means there exists $N$ such that  $m\left(\Big\{f>\frac{1}{N}\Big\}\right)\geq 1/2$. Then $\frac{1}{N}\chi_{\{f>\frac{1}{N}\}} \leq f$ and  $$\int_0^1 f \geq \int_0^1 \frac{1}{N}\chi_{\{f>\frac{1}{N}\}} \geq \frac{1}{2N} > 0.$$ Is this correct?","Someone made the remark on my old question (second-to-last comment on the answer from here ) that a integrable function $f>0$ on $[0,1]$ does not imply $\int_0^1 f >0$ since  limits do not preserve strict inequality. But I think it is true and I will try to give a proof. Since $\{f>0\} = \cup_{n=1}^{\infty}\{f>\frac{1}{n}\}$, from continuity of Lebesgue measure  $$ 1= m(\{f>0\}) = m\left(\bigcup_{n=1}^{\infty}\Big\{f>\frac{1}{n}\Big\}\right) = \lim_{n\rightarrow \infty} m\left(\Big\{f>\frac{1}{n}\Big\}\right),$$ this means there exists $N$ such that  $m\left(\Big\{f>\frac{1}{N}\Big\}\right)\geq 1/2$. Then $\frac{1}{N}\chi_{\{f>\frac{1}{N}\}} \leq f$ and  $$\int_0^1 f \geq \int_0^1 \frac{1}{N}\chi_{\{f>\frac{1}{N}\}} \geq \frac{1}{2N} > 0.$$ Is this correct?",,"['real-analysis', 'proof-verification', 'lebesgue-integral']"
51,"Suppose $\sum x_n$ converges (not necessarily absolutely), does $\sum \sin x_n$ necessarily converge?","Suppose  converges (not necessarily absolutely), does  necessarily converge?",\sum x_n \sum \sin x_n,"The question is: If $\sum x_n$ converges, does $\sum \sin x_n$ converge? I know that if $\sum x_n$ converges absolutely, then $\sum \sin x_n$ converges. My intuition is that we cannot completely abandon absolute convergence, but I am struggling to come up with a counterexample.","The question is: If $\sum x_n$ converges, does $\sum \sin x_n$ converge? I know that if $\sum x_n$ converges absolutely, then $\sum \sin x_n$ converges. My intuition is that we cannot completely abandon absolute convergence, but I am struggling to come up with a counterexample.",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
52,"Show $\inf_f\int_0^1|f'(x)-f(x)|dx=1/e$ for continuously differentiable functions with $f(0)=0$, $f(1)=1$.","Show  for continuously differentiable functions with , .",\inf_f\int_0^1|f'(x)-f(x)|dx=1/e f(0)=0 f(1)=1,"Let $C$ be the class of all real-valued continuously differentiable functions $f$ on the interval $[0,1]$ with $f(0)=0$ and $f(1)=1$. How to show that $$\inf_{f\in C}\int_0^1|f'(x)-f(x)|dx=\frac{1}{e}?$$ I have been able to show that $1/e$ is a lower bound. Indeed, $$\begin{align*} \int_0^1|f'(x)-f(x)|dx &= \int_0^1|f'(x)e^{-x}-f(x)e^{-x}|e^xdx \\ &\geq \int_0^1\left(f'(x)e^{-x}-f(x)e^{-x}\right) dx \\ &= \int_0^1 \frac{d\left(f(x)e^{-x}\right)}{dx}dx  \\ &= f(1)e^{-1}-f(0)e^{0}\\ &=\frac{1}{e}. \end{align*}$$ But how to show this is the infimum? Is there a function $f\in C$ such that we get $\int_0^1|f'(x)-f(x)|dx=1/e$?","Let $C$ be the class of all real-valued continuously differentiable functions $f$ on the interval $[0,1]$ with $f(0)=0$ and $f(1)=1$. How to show that $$\inf_{f\in C}\int_0^1|f'(x)-f(x)|dx=\frac{1}{e}?$$ I have been able to show that $1/e$ is a lower bound. Indeed, $$\begin{align*} \int_0^1|f'(x)-f(x)|dx &= \int_0^1|f'(x)e^{-x}-f(x)e^{-x}|e^xdx \\ &\geq \int_0^1\left(f'(x)e^{-x}-f(x)e^{-x}\right) dx \\ &= \int_0^1 \frac{d\left(f(x)e^{-x}\right)}{dx}dx  \\ &= f(1)e^{-1}-f(0)e^{0}\\ &=\frac{1}{e}. \end{align*}$$ But how to show this is the infimum? Is there a function $f\in C$ such that we get $\int_0^1|f'(x)-f(x)|dx=1/e$?",,"['real-analysis', 'calculus-of-variations', 'supremum-and-infimum']"
53,A metric space such that all closed balls are compact is complete. [duplicate],A metric space such that all closed balls are compact is complete. [duplicate],,"This question already has answers here : Let $X$ be a metric space. Suppose there exists $r >0$ such that $\overline{B(x,r)}$ is compact for every $x \in X$. Show that $X$ is complete. (2 answers) Closed 1 year ago . I am trying to solve the following exercise: Let $(X,d)$ be a metric space that has the property that for any $x\in X$ and $r>0$, the closed ball   $$\bar{B}(x,r):=\{y\in X:d(x,y)\leq r\}$$   is compact. Show that $X$ is complete. I think I have a proof, but I am only using that the unit closed balls $\bar{B}(x,1)$ are compact. Maybe I am missing something? Attempt: Let $(x_n)_{n=1}^\infty$ be a Cauchy sequence in $(X,d)$. Then, $\exists N\in\mathbb{N}$ such that $\forall n\geq N$, $d(x_n,x_N)<1$. Thus, $\forall n\geq N$, $x_n\in\bar{B}(x_N,1)$ which is compact by assumption, so $(x_n)_{n=N}^\infty$ is a sequence in this compact set and thus has a convergent subsequence. Hence, $(x_n)_{n=1}^\infty$ is a Cauchy sequence that has a convergent subsequence, so it converges in $(X,d)$. Therefore, $(X,d)$ is complete.","This question already has answers here : Let $X$ be a metric space. Suppose there exists $r >0$ such that $\overline{B(x,r)}$ is compact for every $x \in X$. Show that $X$ is complete. (2 answers) Closed 1 year ago . I am trying to solve the following exercise: Let $(X,d)$ be a metric space that has the property that for any $x\in X$ and $r>0$, the closed ball   $$\bar{B}(x,r):=\{y\in X:d(x,y)\leq r\}$$   is compact. Show that $X$ is complete. I think I have a proof, but I am only using that the unit closed balls $\bar{B}(x,1)$ are compact. Maybe I am missing something? Attempt: Let $(x_n)_{n=1}^\infty$ be a Cauchy sequence in $(X,d)$. Then, $\exists N\in\mathbb{N}$ such that $\forall n\geq N$, $d(x_n,x_N)<1$. Thus, $\forall n\geq N$, $x_n\in\bar{B}(x_N,1)$ which is compact by assumption, so $(x_n)_{n=N}^\infty$ is a sequence in this compact set and thus has a convergent subsequence. Hence, $(x_n)_{n=1}^\infty$ is a Cauchy sequence that has a convergent subsequence, so it converges in $(X,d)$. Therefore, $(X,d)$ is complete.",,"['real-analysis', 'metric-spaces']"
54,Series with fractional part of $nx$,Series with fractional part of,nx,"Consider the function  $$ f(x)=\sum_{n=1}^{\infty}\frac{(nx)}{n^2} $$ where $(x)$ denotes the fractional part of $x$.  What are the points of discontinuity of $f$, and show they form a countable dense set and is $f$ Riemann-integrable on any bounded interval? I need hint for this one, not home work just solving problem for revision.","Consider the function  $$ f(x)=\sum_{n=1}^{\infty}\frac{(nx)}{n^2} $$ where $(x)$ denotes the fractional part of $x$.  What are the points of discontinuity of $f$, and show they form a countable dense set and is $f$ Riemann-integrable on any bounded interval? I need hint for this one, not home work just solving problem for revision.",,"['real-analysis', 'sequences-and-series']"
55,Continuous function satisfying $f^{k}(x)=f(x^k)$,Continuous function satisfying,f^{k}(x)=f(x^k),"How does one set out to find all continuous functions $f:\mathbb{R} \to \mathbb{R}$ which satisfy $f^{k}(x)=f(x^k)$ , where $k \in \mathbb{N}$? Motivation: Is $\sin(n^k) ≠ (\sin n)^k$ in general?","How does one set out to find all continuous functions $f:\mathbb{R} \to \mathbb{R}$ which satisfy $f^{k}(x)=f(x^k)$ , where $k \in \mathbb{N}$? Motivation: Is $\sin(n^k) ≠ (\sin n)^k$ in general?",,['real-analysis']
56,"Is $\ \{ \sin(p):\ p\ $ is prime $ \}\ $ a dense subset of $\ [-1,1]\ ?$",Is  is prime  a dense subset of,"\ \{ \sin(p):\ p\   \}\  \ [-1,1]\ ?","Is the set $A:= \{ \sin(p): \text{$p$ is prime} \}$ a dense subset of $[-1,1]$ ? Is it known whether or not $-1$ , $0$ or $1$ are limit points of $A$ ? I would imagine so, otherwise this means the primes are related to $\pi$ in some special way, which I have not heard of. I'm not sure how to prove the affirmative though: I think the unpredictable nature of the primes makes this a challenge. All I know is that $\{ \sin(n):n\in\mathbb{N}\}$ is a dense subset of $[-1,1]$ . For example, see here . There are many known inequalities and bounds for the prime counting function, for example, see here. However, I'm not sure these are useful for answering the above question.","Is the set a dense subset of ? Is it known whether or not , or are limit points of ? I would imagine so, otherwise this means the primes are related to in some special way, which I have not heard of. I'm not sure how to prove the affirmative though: I think the unpredictable nature of the primes makes this a challenge. All I know is that is a dense subset of . For example, see here . There are many known inequalities and bounds for the prime counting function, for example, see here. However, I'm not sure these are useful for answering the above question.","A:= \{ \sin(p): \text{p is prime} \} [-1,1] -1 0 1 A \pi \{ \sin(n):n\in\mathbb{N}\} [-1,1]","['real-analysis', 'number-theory', 'prime-numbers']"
57,Inequality of Incomplete Sum for $e^n$ Versus $e^{n}/2$,Inequality of Incomplete Sum for  Versus,e^n e^{n}/2,"Several years ago, I encountered a problem: Prove that, for all natural numbers $n\ge 1$ we have $$\sum_{k=0}^{n-1} \frac{n^k}{k!}<\frac{e^n}{2}<\sum_{k=0}^{n} \frac{n^k}{k!}.$$ The original solution is using induction, but that solution is false. Recently I am revisiting the problems and I found this one, so I am asking here. P.S. I have read the post how to ask a good question . I really don't know how to go on with this problem. Please don't regard this question as no-clue questions, thanks. To answer the commonly asked question ""what attempts have I tried."" I would like to say that I have tried to use a different induction, but it gives no result. Also, by graphing, the solution of $\frac{e^x}{2}=\sum_{k=0}^{n} \frac{x^k}{k!}$ I have observed is around $n+\frac 23$ (I know that it is not allowed to ask two questions in one post, but I will put this observation here and if this question is proved, I am going to ask a separate question for this observation.)","Several years ago, I encountered a problem: Prove that, for all natural numbers we have The original solution is using induction, but that solution is false. Recently I am revisiting the problems and I found this one, so I am asking here. P.S. I have read the post how to ask a good question . I really don't know how to go on with this problem. Please don't regard this question as no-clue questions, thanks. To answer the commonly asked question ""what attempts have I tried."" I would like to say that I have tried to use a different induction, but it gives no result. Also, by graphing, the solution of I have observed is around (I know that it is not allowed to ask two questions in one post, but I will put this observation here and if this question is proved, I am going to ask a separate question for this observation.)",n\ge 1 \sum_{k=0}^{n-1} \frac{n^k}{k!}<\frac{e^n}{2}<\sum_{k=0}^{n} \frac{n^k}{k!}. \frac{e^x}{2}=\sum_{k=0}^{n} \frac{x^k}{k!} n+\frac 23,"['real-analysis', 'sequences-and-series', 'inequality']"
58,Forming a subset of $\mathbb{R}$ by coin tossing,Forming a subset of  by coin tossing,\mathbb{R},"We form a subset $X$ of $\mathbb{R}$ as follows: for every $x\in\mathbb{R}$ we toss a coin. if heads then we put it in $X$ . Assume two tosses are independent of each other. What is the probability that $X$ is a (Lebesgue) measurable subset of $\mathbb{R}$ ? I just started learning probability theory so I am not sure how to formulate this properly, I mean I get uncountably many random variables ... . Will really appreciate any help with this. Feel free to use any advanced theorems, I will look them up. EDIT: Thanks to users angryavian & StephenMontgomery-Smith for pointing out that it makes sense only to talk about probability of $X$ being measurable. I have edited the same. Thanks","We form a subset of as follows: for every we toss a coin. if heads then we put it in . Assume two tosses are independent of each other. What is the probability that is a (Lebesgue) measurable subset of ? I just started learning probability theory so I am not sure how to formulate this properly, I mean I get uncountably many random variables ... . Will really appreciate any help with this. Feel free to use any advanced theorems, I will look them up. EDIT: Thanks to users angryavian & StephenMontgomery-Smith for pointing out that it makes sense only to talk about probability of being measurable. I have edited the same. Thanks",X \mathbb{R} x\in\mathbb{R} X X \mathbb{R} X,"['real-analysis', 'probability-theory', 'measure-theory', 'lebesgue-measure']"
59,why is the least square cost function for linear regression convex,why is the least square cost function for linear regression convex,,"I was looking at Andrew Ng's machine learning course and for linear regression he defined a hypothesis function to be $h(x) = \theta_0 + \theta_1x_1 + \dots + \theta_nx_n$ , where $x$ is a vector of values, so the goal of linear regression is to find $\theta$ that most closely estimates the real result in order to estimate how wrong the hypothesis is compared to how the data is actually distributed. He uses the least square $$ \mathrm{error} = (h(x) - y)^2,  $$ where $y$ is the real result. Since there are a total of $m$ training examples he needs to aggregate them such that all the errors get accounted for. So he defined a cost function $$ J(\theta) = \frac{1}{2m}\sum_{i=0}^{m}(h(x_i) - y_i)^2,  $$ where $x_i$ is a single training set. He states that $J(\theta)$ is convex with only $1$ local optimum. I want to know why is this function convex?","I was looking at Andrew Ng's machine learning course and for linear regression he defined a hypothesis function to be , where is a vector of values, so the goal of linear regression is to find that most closely estimates the real result in order to estimate how wrong the hypothesis is compared to how the data is actually distributed. He uses the least square where is the real result. Since there are a total of training examples he needs to aggregate them such that all the errors get accounted for. So he defined a cost function where is a single training set. He states that is convex with only local optimum. I want to know why is this function convex?","h(x) = \theta_0 + \theta_1x_1 + \dots + \theta_nx_n x \theta 
\mathrm{error} = (h(x) - y)^2, 
 y m 
J(\theta) = \frac{1}{2m}\sum_{i=0}^{m}(h(x_i) - y_i)^2, 
 x_i J(\theta) 1","['real-analysis', 'machine-learning', 'least-squares', 'linear-regression']"
60,Show $\int_0^\infty \log(1+x^2) \frac{\cosh \frac{\pi x}{4}}{\sinh^2 \frac{\pi x}{4}}dx=4\sqrt 2-\frac{16}{\pi}+\frac{8\sqrt 2}{\pi}\log(\sqrt 2+1)$,Show,\int_0^\infty \log(1+x^2) \frac{\cosh \frac{\pi x}{4}}{\sinh^2 \frac{\pi x}{4}}dx=4\sqrt 2-\frac{16}{\pi}+\frac{8\sqrt 2}{\pi}\log(\sqrt 2+1),I am trying to prove this interesting integral $$ I:=\int_0^\infty \log(1+x^2) \frac{\cosh \frac{\pi x}{4}}{\sinh^2 \frac{\pi x}{4}}dx=4\sqrt 2-\frac{16}{\pi}+\frac{8\sqrt 2}{\pi}\log(\sqrt 2+1). $$ I tried to write $$ \int_0^\infty \log(1+x)\frac{\cosh \frac{\pi x}{4}}{\sinh^2 \frac{\pi x}{4}}dx+\int_0^\infty \log(1-x)\frac{\cosh \frac{\pi x}{4}}{\sinh^2 \frac{\pi x}{4}}dx $$ and now using $$ \int_0^\infty \sum_{n=1}^\infty \frac{(-1)^{n+1}}{n}x^n\frac{\cosh \frac{\pi x}{4}}{\sinh^2 \frac{\pi x}{4}}dx-\int_0^\infty \sum_{n=1}^\infty \frac{x^n}{n}\frac{\cosh \frac{\pi x}{4}}{\sinh^2 \frac{\pi x}{4}}dx $$ and now introducing a parameter $$ I(a)=\int_0^\infty \sum_{n=1}^\infty \frac{(-1)^{n+1}}{n}x^n\frac{\cosh \frac{a \pi x}{4}}{\sinh^2 \frac{\pi x}{4}}dx-\int_0^\infty \sum_{n=1}^\infty \frac{x^n}{n}\frac{\cosh  \frac{a\pi x}{4}}{\sinh^2 \frac{\pi x}{4}}dx. $$ But writing $I'(a)$ didn't simplify much.  The substitution $y=\sinh \pi x/4$ also was of no use because of the $x^n $ factor.  So How can we prove this interesting integral?  Thanks,I am trying to prove this interesting integral $$ I:=\int_0^\infty \log(1+x^2) \frac{\cosh \frac{\pi x}{4}}{\sinh^2 \frac{\pi x}{4}}dx=4\sqrt 2-\frac{16}{\pi}+\frac{8\sqrt 2}{\pi}\log(\sqrt 2+1). $$ I tried to write $$ \int_0^\infty \log(1+x)\frac{\cosh \frac{\pi x}{4}}{\sinh^2 \frac{\pi x}{4}}dx+\int_0^\infty \log(1-x)\frac{\cosh \frac{\pi x}{4}}{\sinh^2 \frac{\pi x}{4}}dx $$ and now using $$ \int_0^\infty \sum_{n=1}^\infty \frac{(-1)^{n+1}}{n}x^n\frac{\cosh \frac{\pi x}{4}}{\sinh^2 \frac{\pi x}{4}}dx-\int_0^\infty \sum_{n=1}^\infty \frac{x^n}{n}\frac{\cosh \frac{\pi x}{4}}{\sinh^2 \frac{\pi x}{4}}dx $$ and now introducing a parameter $$ I(a)=\int_0^\infty \sum_{n=1}^\infty \frac{(-1)^{n+1}}{n}x^n\frac{\cosh \frac{a \pi x}{4}}{\sinh^2 \frac{\pi x}{4}}dx-\int_0^\infty \sum_{n=1}^\infty \frac{x^n}{n}\frac{\cosh  \frac{a\pi x}{4}}{\sinh^2 \frac{\pi x}{4}}dx. $$ But writing $I'(a)$ didn't simplify much.  The substitution $y=\sinh \pi x/4$ also was of no use because of the $x^n $ factor.  So How can we prove this interesting integral?  Thanks,,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
61,Two conjectured identities of sums involving $\sin(x\sin x)/x^2$,Two conjectured identities of sums involving,\sin(x\sin x)/x^2,"While tackling this question , I came up with the conjecture that the following identities hold: Conjecture. Let $f(x)=\dfrac{\sin(x\sin x)}{x^2}$ . Then $$ \sum_{k=-\infty}^{\infty} f(k\pi+x) = 1 \qquad\text{and}\qquad \sum_{k=-\infty}^{\infty} (-1)^k f(k\pi + x) = \cos x. $$ Unfortunately, I have absolutely no idea how to tackle this problems. The only evidence I have for this one is a numerical simulation: ( Figure. Left: A comparison between the partial sum $\sum_{|k|\leq n} (-1)^k f(k\pi + x)$ and $\cos x$ / Right: A comparison between the partial sum $\sum_{|k|\leq n} f(k\pi + x)$ and $1$ ) If the conjecture turns out to be true, then we can prove some fun identities, including the original posting that motivated this question. For example, we obtain the following identity as a by-product: $$ \int_{-\infty}^{\infty} \frac{\sin(x\sin x)}{x^2} \, \mathrm{d}x = \int_{0}^{\pi} \sum_{k=-\infty}^{\infty} f(k\pi + x) \, \mathrm{d}x = \int_{0}^{\pi} \mathrm{d}x = \pi $$","While tackling this question , I came up with the conjecture that the following identities hold: Conjecture. Let . Then Unfortunately, I have absolutely no idea how to tackle this problems. The only evidence I have for this one is a numerical simulation: ( Figure. Left: A comparison between the partial sum and / Right: A comparison between the partial sum and ) If the conjecture turns out to be true, then we can prove some fun identities, including the original posting that motivated this question. For example, we obtain the following identity as a by-product:","f(x)=\dfrac{\sin(x\sin x)}{x^2}  \sum_{k=-\infty}^{\infty} f(k\pi+x) = 1 \qquad\text{and}\qquad \sum_{k=-\infty}^{\infty} (-1)^k f(k\pi + x) = \cos x.  \sum_{|k|\leq n} (-1)^k f(k\pi + x) \cos x \sum_{|k|\leq n} f(k\pi + x) 1  \int_{-\infty}^{\infty} \frac{\sin(x\sin x)}{x^2} \, \mathrm{d}x
= \int_{0}^{\pi} \sum_{k=-\infty}^{\infty} f(k\pi + x) \, \mathrm{d}x
= \int_{0}^{\pi} \mathrm{d}x
= \pi ","['real-analysis', 'calculus', 'summation']"
62,"Is there a smooth strictly-increasing bijection $\mathbb{R} \to (0, 1)$ that maps $\mathbb{Q}$ onto $\mathbb{Q}\cap(0, 1)$?",Is there a smooth strictly-increasing bijection  that maps  onto ?,"\mathbb{R} \to (0, 1) \mathbb{Q} \mathbb{Q}\cap(0, 1)","I noticed that none of the ""nice"" sigmoid functions I know of sends rationals to rationals. I set out to find a sigmoid function that maps rationals into rationals.  I did find one, but it is not smooth.  I wonder if such a function exists, and if not, what is the connection between this non-existence and the requirement that it and its inverse map rationals to rationals. The function $$ x \mapsto \begin{cases}        \frac{1}{2 (1 - x)} &, \text {if} \ \ x < 0 \\       \frac{2 x + 1}{2 (1 + x)} &, \text {if} \ \  x \geq 0 \end{cases} $$ is a strictly increasing bijection from $\mathbb{R}$ onto $(0, 1)$ , and it maps $\mathbb{Q}$ into $\mathbb{Q}\cap(0, 1)$ . Furthermore, its inverse $$ y \mapsto \begin{cases}        \frac{2 y - 1}{2 y} &, \text {if} \ \  0 < y < \frac{1}{2} \\       \frac{2 y - 1}{2 (1 - y)} &, \text {if} \ \  \frac{1}{2} \leq y < 1 \end{cases} $$ maps $\mathbb{Q}\cap(0, 1)$ into $\mathbb{Q}$ . Therefore, the restriction of this function to $\mathbb{Q}$ is a monotone bijection from $\mathbb{Q}$ onto $\mathbb{Q}\cap(0, 1)$ . The original function (over $\mathbb{R}$ ) is not smooth, however: its second derivative does not exist at $x = 0$ . Does there exist a smooth monotone bijection from $\mathbb{R}$ onto $(0, 1)$ whose restriction to $\mathbb{Q}$ maps $\mathbb{Q}$ onto $\mathbb{Q}\cap(0, 1)$ ?  Is it possible to write down a formula for it? EDIT: Below is the function one gets if one subtracts $\frac{1}{2}$ from the expressions in the original definition. $$ x \mapsto \begin{cases}        \frac{x}{2 (1 - x)} &, \text {if} \ \ x < 0 \\       \frac{x}{2 (1 + x)} &, \text {if} \ \  x \geq 0 \end{cases} $$ (Informally, this ""translates"" the graph of the function down by $\frac{1}{2}$ , so that the function's value at $0$ is $0$ .) This ""translate"" of the original function more clearly shows its symmetry. EDIT2: I realized after the fact that the word ""monotone"" in the phrase ""smooth monotone bijection"" is probably redundant.  I cannot envision a smooth bijection on $\mathbb{R}$ that fails to be monotone.  (The sign of the first derivative must remain bounded away from zero if the function is going to be injective.)","I noticed that none of the ""nice"" sigmoid functions I know of sends rationals to rationals. I set out to find a sigmoid function that maps rationals into rationals.  I did find one, but it is not smooth.  I wonder if such a function exists, and if not, what is the connection between this non-existence and the requirement that it and its inverse map rationals to rationals. The function is a strictly increasing bijection from onto , and it maps into . Furthermore, its inverse maps into . Therefore, the restriction of this function to is a monotone bijection from onto . The original function (over ) is not smooth, however: its second derivative does not exist at . Does there exist a smooth monotone bijection from onto whose restriction to maps onto ?  Is it possible to write down a formula for it? EDIT: Below is the function one gets if one subtracts from the expressions in the original definition. (Informally, this ""translates"" the graph of the function down by , so that the function's value at is .) This ""translate"" of the original function more clearly shows its symmetry. EDIT2: I realized after the fact that the word ""monotone"" in the phrase ""smooth monotone bijection"" is probably redundant.  I cannot envision a smooth bijection on that fails to be monotone.  (The sign of the first derivative must remain bounded away from zero if the function is going to be injective.)","
x \mapsto
\begin{cases} 
      \frac{1}{2 (1 - x)} &, \text {if} \ \ x < 0 \\
      \frac{2 x + 1}{2 (1 + x)} &, \text {if} \ \  x \geq 0
\end{cases}
 \mathbb{R} (0, 1) \mathbb{Q} \mathbb{Q}\cap(0, 1) 
y \mapsto
\begin{cases} 
      \frac{2 y - 1}{2 y} &, \text {if} \ \  0 < y < \frac{1}{2} \\
      \frac{2 y - 1}{2 (1 - y)} &, \text {if} \ \  \frac{1}{2} \leq y < 1
\end{cases}
 \mathbb{Q}\cap(0, 1) \mathbb{Q} \mathbb{Q} \mathbb{Q} \mathbb{Q}\cap(0, 1) \mathbb{R} x = 0 \mathbb{R} (0, 1) \mathbb{Q} \mathbb{Q} \mathbb{Q}\cap(0, 1) \frac{1}{2} 
x \mapsto
\begin{cases} 
      \frac{x}{2 (1 - x)} &, \text {if} \ \ x < 0 \\
      \frac{x}{2 (1 + x)} &, \text {if} \ \  x \geq 0
\end{cases}
 \frac{1}{2} 0 0 \mathbb{R}",['real-analysis']
63,"Closed-form of $\mathbb E(\|G\|_\infty)$ where $G\sim\mathcal N(0,\mathbf{Id}_n)$.",Closed-form of  where .,"\mathbb E(\|G\|_\infty) G\sim\mathcal N(0,\mathbf{Id}_n)","Let $I_n = \mathbb E(\|G\|_\infty)$ , i.e. $$I_n = (2\pi)^{-\frac{n}{2}}\int_{x\in\mathbb R^n}\|x\|_\infty e^{-\frac{1}{2}\|x\|_2^2}\,dx.$$ I wonder if I can get its closed-form. By symmetry I got $$I_n = 2n\sqrt{\frac{2}{\pi}}\int_0^\infty xe^{-x^2}\operatorname{erf}(x)^{n-1}\,dx,$$ and then by integration by parts, for $n\ge2$ , $$I_n = \frac{2\sqrt2}{\pi}n(n-1)\int_0^\infty e^{-2x^2}\operatorname{erf}(x)^{n-2}\,dx,$$ where $\operatorname{erf}$ is the error function. These two formulas give me $$I_1 = \sqrt{\frac{2}{\pi}},\quad I_2 = 2\sqrt{\frac{1}{\pi}},\quad I_3 = \frac{12}{\pi\sqrt\pi}\arctan\frac{\sqrt2}{2}.$$ In this step, I think a general closed-form is almost impossible, so I post here to see if anyone has a better approach (at least for $I_4$ ). Update Series expansion of $I_4$ : $$I_4 = \frac{8\sqrt2}{\pi^2}\sum_{n=0}^{\infty}\left(\frac43\right)^n\frac{n!}{(2n+1)!}\,\Gamma(n+3/2)\,{}_2F_1(1/2,-n;3/2;1/4).$$ By the way $$I_n = \sqrt2n\int_0^1t^{n-1}\operatorname{erf}^{-1}(t)\,dt \,=\!\!\!?\; \sqrt2n\sum_{k=0}^\infty a_k \left(\frac{\sqrt\pi}{2}\right)^{2k+1}\frac1{2k+n+1},$$ where $a_k$ is the $k$ -th coefficient of the Maclaurin series of $\operatorname{erf}^{-1}(2x/\sqrt\pi)$ (see InverseErf ). Well, I don't really know the behavior of $(a_k)$ , but numerically the series does converge. I don't think this will lead to anything though. Let me explain a little about this problem. Imagine we have $n$ points to throw at 0 at the real axis, and the resulted position of one point is determined by $\mathcal N(0,1)$ . We want to study the behavior of the farthest distance from 0. This distance $D = \|G\|_\infty$ is determined by the density function defined below $$f:x \mapsto n\sqrt{\frac2\pi}\,\exp\left(-\frac{x^2}2\right) \operatorname{erf}^{n-1}\frac{x}{\sqrt2} \mathbb1_{x\ge0}.$$ (For fun one can check that $\int_0^\infty f(x)\,dx=1$ .) And now, what we want to know is, how to calculate $\mathbb E(D)$ (at least when $n=4$ )? @YuriNegometyanov has given a formula for $\mathbb E(\|G\|_2)$ . Even though it's not quite the topic, let's write it down as well: $$\mathbb E(\|G\|_2) =\sqrt2\,\frac{\Gamma\left(\dfrac{n+1}2\right)}{\Gamma\left(\dfrac n2\right)}.$$ A jupyter notebook to calculate numerical results. So from the series expansion of $I_4$ mentioned above (and tons of calculation), I got: $$I_4 = \frac{24}{\pi\sqrt\pi}\arctan\frac{1}{2\sqrt2}.$$ This is kind of interesting since the form is similar to $I_3$ . Maybe a general closed-form is in fact possible?","Let , i.e. I wonder if I can get its closed-form. By symmetry I got and then by integration by parts, for , where is the error function. These two formulas give me In this step, I think a general closed-form is almost impossible, so I post here to see if anyone has a better approach (at least for ). Update Series expansion of : By the way where is the -th coefficient of the Maclaurin series of (see InverseErf ). Well, I don't really know the behavior of , but numerically the series does converge. I don't think this will lead to anything though. Let me explain a little about this problem. Imagine we have points to throw at 0 at the real axis, and the resulted position of one point is determined by . We want to study the behavior of the farthest distance from 0. This distance is determined by the density function defined below (For fun one can check that .) And now, what we want to know is, how to calculate (at least when )? @YuriNegometyanov has given a formula for . Even though it's not quite the topic, let's write it down as well: A jupyter notebook to calculate numerical results. So from the series expansion of mentioned above (and tons of calculation), I got: This is kind of interesting since the form is similar to . Maybe a general closed-form is in fact possible?","I_n = \mathbb E(\|G\|_\infty) I_n = (2\pi)^{-\frac{n}{2}}\int_{x\in\mathbb R^n}\|x\|_\infty e^{-\frac{1}{2}\|x\|_2^2}\,dx. I_n = 2n\sqrt{\frac{2}{\pi}}\int_0^\infty xe^{-x^2}\operatorname{erf}(x)^{n-1}\,dx, n\ge2 I_n = \frac{2\sqrt2}{\pi}n(n-1)\int_0^\infty e^{-2x^2}\operatorname{erf}(x)^{n-2}\,dx, \operatorname{erf} I_1 = \sqrt{\frac{2}{\pi}},\quad I_2 = 2\sqrt{\frac{1}{\pi}},\quad I_3 = \frac{12}{\pi\sqrt\pi}\arctan\frac{\sqrt2}{2}. I_4 I_4 I_4 = \frac{8\sqrt2}{\pi^2}\sum_{n=0}^{\infty}\left(\frac43\right)^n\frac{n!}{(2n+1)!}\,\Gamma(n+3/2)\,{}_2F_1(1/2,-n;3/2;1/4). I_n = \sqrt2n\int_0^1t^{n-1}\operatorname{erf}^{-1}(t)\,dt \,=\!\!\!?\; \sqrt2n\sum_{k=0}^\infty a_k \left(\frac{\sqrt\pi}{2}\right)^{2k+1}\frac1{2k+n+1}, a_k k \operatorname{erf}^{-1}(2x/\sqrt\pi) (a_k) n \mathcal N(0,1) D = \|G\|_\infty f:x \mapsto n\sqrt{\frac2\pi}\,\exp\left(-\frac{x^2}2\right) \operatorname{erf}^{n-1}\frac{x}{\sqrt2} \mathbb1_{x\ge0}. \int_0^\infty f(x)\,dx=1 \mathbb E(D) n=4 \mathbb E(\|G\|_2) \mathbb E(\|G\|_2) =\sqrt2\,\frac{\Gamma\left(\dfrac{n+1}2\right)}{\Gamma\left(\dfrac n2\right)}. I_4 I_4 = \frac{24}{\pi\sqrt\pi}\arctan\frac{1}{2\sqrt2}. I_3","['real-analysis', 'probability', 'integration', 'probability-distributions']"
64,"Show $\tan(n) < n^q$, conjectured $q < 1.1$","Show , conjectured",\tan(n) < n^q q < 1.1,"Show $\tan(n) < n^q$ , $n \in \mathbb{N}$ , $n > 1$ . The argument of the $\tan$ -function is in radians. It is conjectured that $q < 1.1$ . In fact, search for the maximum of $q$ in $n\in [2, 10^9]$ gives $\tan(260515)= 383610.707744 = 260515^{1.031031}$ and then the next higher $q$ 's only at $\tan(122925461)= 326900723.479835 = 122925461^{1.052508}$ , and further $\tan(534483448)= 1914547468.536829 =  534483448^{1.063489}$ . It is known that $\tan(n)$ is unbounded (see math.stackexchange.com/questions/1056119 ). So it is clear that with increasing $n$ ,  ever larger $\tan(n)$ will eventually be found. While we know that $n\ne(k+1/2)\pi$ ,  that doesn't mean that we know how close $n$ comes to some $(k+1/2)\pi$ with growing $n$ . In the range of $n$ above, it appears that no higher values of $q$ are found, and the conjecture is that such high $n$ are required for higher values of $\tan(n)$ , that no higher $q$ will be attained. Possibly the limit for $q$ will have to be made more loose than $q < 1.1$ , where derivations of such looser bounds would certainly be appreciated. For possible relations to the irrationality measure of $\pi$ , which is unknown, see discussions in here: math.stackexchange.com/questions/2977461 .","Show , , . The argument of the -function is in radians. It is conjectured that . In fact, search for the maximum of in gives and then the next higher 's only at , and further . It is known that is unbounded (see math.stackexchange.com/questions/1056119 ). So it is clear that with increasing ,  ever larger will eventually be found. While we know that ,  that doesn't mean that we know how close comes to some with growing . In the range of above, it appears that no higher values of are found, and the conjecture is that such high are required for higher values of , that no higher will be attained. Possibly the limit for will have to be made more loose than , where derivations of such looser bounds would certainly be appreciated. For possible relations to the irrationality measure of , which is unknown, see discussions in here: math.stackexchange.com/questions/2977461 .","\tan(n) < n^q n \in \mathbb{N} n > 1 \tan q < 1.1 q n\in [2, 10^9] \tan(260515)= 383610.707744 = 260515^{1.031031} q \tan(122925461)= 326900723.479835 = 122925461^{1.052508} \tan(534483448)= 1914547468.536829 =  534483448^{1.063489} \tan(n) n \tan(n) n\ne(k+1/2)\pi n (k+1/2)\pi n n q n \tan(n) q q q < 1.1 \pi","['real-analysis', 'sequences-and-series', 'number-theory', 'trigonometry']"
65,What's the arc length of an implicit function?,What's the arc length of an implicit function?,,"While an explicit function $y(x)$'s arc length $s$ is easily obtained as $$s = \int \sqrt{1+|y'(x)|^2}\,dx,$$ is there any formula for implicit functions given by $f(x,y) = 0$? One can use the implicit differentiation $y'(x) = -\frac{\partial_y f}{\partial_x f}$ to obtain $$s = \int\sqrt{1 + |\partial_y f / \partial_x f|^2}\,dx,$$ but that still requires (locally) solving for $y(x)$. Is there any formulation that does not require this, e.g. another implicit equation involving $s$? Thoughts so far: One could rewrite $s$ as $$s = \int |\nabla f|\, |\partial_x f|dx,$$ or symmetrize to $$s = \int |\nabla f|\, \underbrace{(|\partial_x f|dx + |\partial_y f|dy)}_{(*)}/2$$ where $(*)$ might be strongly related to $|df|$ I guess (though it's not identical due to the $|\cdot|$), but then?","While an explicit function $y(x)$'s arc length $s$ is easily obtained as $$s = \int \sqrt{1+|y'(x)|^2}\,dx,$$ is there any formula for implicit functions given by $f(x,y) = 0$? One can use the implicit differentiation $y'(x) = -\frac{\partial_y f}{\partial_x f}$ to obtain $$s = \int\sqrt{1 + |\partial_y f / \partial_x f|^2}\,dx,$$ but that still requires (locally) solving for $y(x)$. Is there any formulation that does not require this, e.g. another implicit equation involving $s$? Thoughts so far: One could rewrite $s$ as $$s = \int |\nabla f|\, |\partial_x f|dx,$$ or symmetrize to $$s = \int |\nabla f|\, \underbrace{(|\partial_x f|dx + |\partial_y f|dy)}_{(*)}/2$$ where $(*)$ might be strongly related to $|df|$ I guess (though it's not identical due to the $|\cdot|$), but then?",,"['real-analysis', 'integration', 'implicit-differentiation', 'implicit-function-theorem']"
66,How does one show sin(x) is bounded using the power series?,How does one show sin(x) is bounded using the power series?,,"Define the real valued function  $$ \sin:\mathbb{R} \rightarrow \mathbb{R}, \qquad given ~~by \qquad \sin(x) := x-\frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \ldots $$ How does one show $\sin(x)$ is bounded using this definition? Note that you are not allowed to  use the power series of $\cos(x)$ and try to show $\sin^2(x) + \cos^2(x) =1$ and  then prove they are bounded. I want a direct proof using the power series of $\sin(x)$. Remark: I am looking for a proof that will allow me to modify/mimic the arguments if I am given some DIFFERENT power series that also happens to be bounded (but which doesn't have all the nice properties of sin(x) and cos(x) ). That is the motivation for the question. Take the power series of $\exp(-x^2)$ for example. Why is that bounded?","Define the real valued function  $$ \sin:\mathbb{R} \rightarrow \mathbb{R}, \qquad given ~~by \qquad \sin(x) := x-\frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \ldots $$ How does one show $\sin(x)$ is bounded using this definition? Note that you are not allowed to  use the power series of $\cos(x)$ and try to show $\sin^2(x) + \cos^2(x) =1$ and  then prove they are bounded. I want a direct proof using the power series of $\sin(x)$. Remark: I am looking for a proof that will allow me to modify/mimic the arguments if I am given some DIFFERENT power series that also happens to be bounded (but which doesn't have all the nice properties of sin(x) and cos(x) ). That is the motivation for the question. Take the power series of $\exp(-x^2)$ for example. Why is that bounded?",,"['real-analysis', 'ordinary-differential-equations', 'inequality', 'power-series']"
67,"Integral $\int_0^\infty \frac{\sqrt{\sqrt{\alpha^2+x^2}-\alpha}\,\exp\big({-\beta\sqrt{\alpha^2+x^2}\big)}}{\sqrt{\alpha^2+x^2}}\sin (\gamma x)\,dx$",Integral,"\int_0^\infty \frac{\sqrt{\sqrt{\alpha^2+x^2}-\alpha}\,\exp\big({-\beta\sqrt{\alpha^2+x^2}\big)}}{\sqrt{\alpha^2+x^2}}\sin (\gamma x)\,dx","I am having trouble showing this equality is true$$ \int_0^\infty \frac{\sqrt{\sqrt{\alpha^2+x^2}-\alpha}\,\exp\big({-\beta\sqrt{\alpha^2+x^2}\big)}}{\sqrt{\alpha^2+x^2}}\sin (\gamma x)\,dx=\sqrt\frac{\pi}{2}\frac{\gamma \exp\big(-\alpha\sqrt{\gamma^2+\beta^2}\big)}{\sqrt{\beta^2+\gamma^2}\sqrt{\beta+\sqrt{\beta^2+\gamma^2}}}, $$ $$ \mathcal{Re}(\alpha,\beta,\gamma> 0). $$ I do not know how to approach it because of all the square root functions. It seems if $x=\pm i\alpha \ $   we may have some convergence problems because of the denominator.  Perhaps there are ways to solve this using complex methods involving the branch cut from the square root singularity.   I just do not know what to choose $f(z)$ for a suitable complex function to represent the integrand. I also tried differentiating under the integral signs w.r.t $\alpha,\beta,\gamma$ but it did not simplify anything.  Thanks.   How can we calculate this integral?","I am having trouble showing this equality is true$$ \int_0^\infty \frac{\sqrt{\sqrt{\alpha^2+x^2}-\alpha}\,\exp\big({-\beta\sqrt{\alpha^2+x^2}\big)}}{\sqrt{\alpha^2+x^2}}\sin (\gamma x)\,dx=\sqrt\frac{\pi}{2}\frac{\gamma \exp\big(-\alpha\sqrt{\gamma^2+\beta^2}\big)}{\sqrt{\beta^2+\gamma^2}\sqrt{\beta+\sqrt{\beta^2+\gamma^2}}}, $$ $$ \mathcal{Re}(\alpha,\beta,\gamma> 0). $$ I do not know how to approach it because of all the square root functions. It seems if $x=\pm i\alpha \ $   we may have some convergence problems because of the denominator.  Perhaps there are ways to solve this using complex methods involving the branch cut from the square root singularity.   I just do not know what to choose $f(z)$ for a suitable complex function to represent the integrand. I also tried differentiating under the integral signs w.r.t $\alpha,\beta,\gamma$ but it did not simplify anything.  Thanks.   How can we calculate this integral?",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
68,Continuity of $\delta$ in the definition of continuity,Continuity of  in the definition of continuity,\delta,"When I was in the shower this morning a question went through my head about continuity of a function at a point. The simplest formulation of this question is: Let $f : \mathbb{R} \to \mathbb{R}$ be an unbounded continuous function with $f(0) = 0$. Define $\delta_f : (0, \infty) \to (0, \infty)$ by   $$\delta_f(\varepsilon) = \sup \{ \delta > 0\, :\, |x|< \delta \Rightarrow |f(x)| < \varepsilon \}$$   Under what conditions is $\delta_f$ a continuous function of $\varepsilon$? The answer to this probably involves monotonicity; for example, it seems that $\delta_f$ is continuous whenever $f$ is strictly monotone. I'd like to find (if possible) the weakest condition on $f$ to make $\delta_f$ continuous. My hunch is that the answer is that $\delta_f$ is continuous if and only if $|f| : \mathbb{R} \to [0,\infty)$ is strictly monotone, but I await counterexamples with open arms. More generally, the question can be formulated as follows: Let $f : V \to W$ be a continuous unbounded function between normed spaces with $f(0_V) = 0_W$. Define $\delta_f : (0, \infty) \to (0, \infty)$ by   $$\delta_f(\varepsilon) = \sup \{ \delta > 0 \, :\, \lVert x \rVert < \delta \Rightarrow \lVert f(x) \rVert < \varepsilon \}$$   Under what conditions is $\delta_f$ a continuous function of $\varepsilon$? My ultimate goal  is to prove that $\delta_f$ is continuous if and only if $\lVert f \rVert : V \to [0, \infty)$ is strictly monotone, or to find a counterexample.","When I was in the shower this morning a question went through my head about continuity of a function at a point. The simplest formulation of this question is: Let $f : \mathbb{R} \to \mathbb{R}$ be an unbounded continuous function with $f(0) = 0$. Define $\delta_f : (0, \infty) \to (0, \infty)$ by   $$\delta_f(\varepsilon) = \sup \{ \delta > 0\, :\, |x|< \delta \Rightarrow |f(x)| < \varepsilon \}$$   Under what conditions is $\delta_f$ a continuous function of $\varepsilon$? The answer to this probably involves monotonicity; for example, it seems that $\delta_f$ is continuous whenever $f$ is strictly monotone. I'd like to find (if possible) the weakest condition on $f$ to make $\delta_f$ continuous. My hunch is that the answer is that $\delta_f$ is continuous if and only if $|f| : \mathbb{R} \to [0,\infty)$ is strictly monotone, but I await counterexamples with open arms. More generally, the question can be formulated as follows: Let $f : V \to W$ be a continuous unbounded function between normed spaces with $f(0_V) = 0_W$. Define $\delta_f : (0, \infty) \to (0, \infty)$ by   $$\delta_f(\varepsilon) = \sup \{ \delta > 0 \, :\, \lVert x \rVert < \delta \Rightarrow \lVert f(x) \rVert < \varepsilon \}$$   Under what conditions is $\delta_f$ a continuous function of $\varepsilon$? My ultimate goal  is to prove that $\delta_f$ is continuous if and only if $\lVert f \rVert : V \to [0, \infty)$ is strictly monotone, or to find a counterexample.",,['real-analysis']
69,Axiomatizing convergent sequences and limits,Axiomatizing convergent sequences and limits,,"The usual exposition of limits of sequences starts with a definition and then derives properties like linearity. I'm curious if, conversely, a reasonable set of these properties characterize the subspace $C_{\text{std}} \subset \mathbf{R}^\mathbf{N}$ of convergent sequences and the map $L_{\text{std}}: C_{\text{std}} \to \mathbf{R}, (a_n) \mapsto \lim_{n \to \infty} a_n$ . Here is an example list of properties (when I say equivalently, I mean equivalent assuming the previous conditions): $C$ is a vector subspace and $L: C \to \mathbf{R}$ is $\mathbf{R}$ -linear. $C$ contains all monotone bounded sequences. If $a$ is eventually the constant $a_\infty$ , ie $a_n = a_\infty$ for large $n$ , then $La = a_\infty$ (equivalently, $L$ vanishes on the subspace of eventually $0$ sequences and takes the constant $1$ sequence to $1$ ). If $a \in (\mathbf{R}_{\ge 0})^\mathbf{N} \cap C$ then $La \ge 0$ (equivalently $L$ is monotonic). If $a, b \in C$ then $ab := (a_n b_n) \in C$ and $L(ab) = (L a)(L b)$ . For $a \in C$ every subsequence $a'$ of $a$ is also in $C$ and $La = La'$ . But all of these are also satisfied by $$C = \operatorname{span}\{a \text{ monotone bounded}\} = \{ a - b \mid a, b \text{ increasing and bounded} \}$$ (see https://math.stackexchange.com/a/4377050/32766 for an alternate characterization) and $L = L_{\text{std}}|C$ . So, is there a similar list of ""well-known"" properties of $(C_{\text{std}}, L_{\text{std}})$ that implies $C \supseteq C_{\text{std}}$ and $L$ is an extension of $L_{\text{std}}$ ? The implication should of course be ""non-trivial"", ie the list should not just include a definition of $\lim$ . Given such a list, we could define $(L_{\text{std}}, C_{\text{std}})$ as the minimal pair satisfying that list. Note that (1)-(4) are enough to show that $L$ restricted to $C \cap C_{\text{std}}$ agrees with $L_{\text{std}}$ : if $a_\infty = \lim a_n$ then $a_n$ is eventually bounded between $a_\infty - \epsilon$ and $a_\infty + \epsilon$ and hence so is $La$ . We don't even need all of (2), just that eventually constant sequences are in $C$ . (1), (3), (4) and (6) also imply that $C \subseteq C_{\text{std}}$ . First note that if $a \in C$ then it has to be bounded, otherwise for each $N > 0$ either $a$ or $-a$ has a subsequence above $N$ , so $|La| \ge N$ by monotonicity. Now if $a \in C$ is not convergent then it must have convergent subsequences with distinct limits, which forces a contradiction between (6) and that $L$ agrees with $L_{\text{std}}$ on $C \cap C_{\text{std}}$ , as shown above. However I'd be generally interested if an answer provides a list that allows for $C \supsetneq C_{\text{std}}$ . Remarks: (1), (3) and (5) can be combined to: $C$ is a unital subalgebra of $\mathbf{R}^\mathbf{N}$ containing the ideal $C_0$ of eventually $0$ sequences and $L: C \to C/C_0 \to \mathbf{R}$ is a unital algebra homomorphism. It is plausible to me that (4) is implied by the other conditions. It could also be replaced by the (not obviously equivalent) condition $a \in C\cap (\mathbf{R}_{\ge 0})^{\mathbf{N}} \implies (\sqrt{a_n}) \in C$ .","The usual exposition of limits of sequences starts with a definition and then derives properties like linearity. I'm curious if, conversely, a reasonable set of these properties characterize the subspace of convergent sequences and the map . Here is an example list of properties (when I say equivalently, I mean equivalent assuming the previous conditions): is a vector subspace and is -linear. contains all monotone bounded sequences. If is eventually the constant , ie for large , then (equivalently, vanishes on the subspace of eventually sequences and takes the constant sequence to ). If then (equivalently is monotonic). If then and . For every subsequence of is also in and . But all of these are also satisfied by (see https://math.stackexchange.com/a/4377050/32766 for an alternate characterization) and . So, is there a similar list of ""well-known"" properties of that implies and is an extension of ? The implication should of course be ""non-trivial"", ie the list should not just include a definition of . Given such a list, we could define as the minimal pair satisfying that list. Note that (1)-(4) are enough to show that restricted to agrees with : if then is eventually bounded between and and hence so is . We don't even need all of (2), just that eventually constant sequences are in . (1), (3), (4) and (6) also imply that . First note that if then it has to be bounded, otherwise for each either or has a subsequence above , so by monotonicity. Now if is not convergent then it must have convergent subsequences with distinct limits, which forces a contradiction between (6) and that agrees with on , as shown above. However I'd be generally interested if an answer provides a list that allows for . Remarks: (1), (3) and (5) can be combined to: is a unital subalgebra of containing the ideal of eventually sequences and is a unital algebra homomorphism. It is plausible to me that (4) is implied by the other conditions. It could also be replaced by the (not obviously equivalent) condition .","C_{\text{std}} \subset \mathbf{R}^\mathbf{N} L_{\text{std}}: C_{\text{std}} \to \mathbf{R}, (a_n) \mapsto \lim_{n \to \infty} a_n C L: C \to \mathbf{R} \mathbf{R} C a a_\infty a_n = a_\infty n La = a_\infty L 0 1 1 a \in (\mathbf{R}_{\ge 0})^\mathbf{N} \cap C La \ge 0 L a, b \in C ab := (a_n b_n) \in C L(ab) = (L a)(L b) a \in C a' a C La = La' C = \operatorname{span}\{a \text{ monotone bounded}\} = \{ a - b \mid a, b \text{ increasing and bounded} \} L = L_{\text{std}}|C (C_{\text{std}}, L_{\text{std}}) C \supseteq C_{\text{std}} L L_{\text{std}} \lim (L_{\text{std}}, C_{\text{std}}) L C \cap C_{\text{std}} L_{\text{std}} a_\infty = \lim a_n a_n a_\infty - \epsilon a_\infty + \epsilon La C C \subseteq C_{\text{std}} a \in C N > 0 a -a N |La| \ge N a \in C L L_{\text{std}} C \cap C_{\text{std}} C \supsetneq C_{\text{std}} C \mathbf{R}^\mathbf{N} C_0 0 L: C \to C/C_0 \to \mathbf{R} a \in C\cap (\mathbf{R}_{\ge 0})^{\mathbf{N}} \implies (\sqrt{a_n}) \in C","['real-analysis', 'sequences-and-series', 'limits']"
70,"If $g:[0,1] \to \Bbb{R}$ such that $g(x)=g(y) \implies g'(x)=g'(y)$ for all $x,y \in (0,1)$, then $g$ is monotonic?","If  such that  for all , then  is monotonic?","g:[0,1] \to \Bbb{R} g(x)=g(y) \implies g'(x)=g'(y) x,y \in (0,1) g","Is this true or false? Let $g:[0,1] \to \Bbb R$ be a function continuous on $[0,1]$ and differentiable on $(0,1)$, such that $g'$ is a function of $g$, i.e. for every $x, y \in (0,1)$, if $g(x) = g(y)$ then $g'(x) = g'(y)$. Then $g$ is monotonic. I posted my proof as a self-answer, and wonder if there are any easier proofs.","Is this true or false? Let $g:[0,1] \to \Bbb R$ be a function continuous on $[0,1]$ and differentiable on $(0,1)$, such that $g'$ is a function of $g$, i.e. for every $x, y \in (0,1)$, if $g(x) = g(y)$ then $g'(x) = g'(y)$. Then $g$ is monotonic. I posted my proof as a self-answer, and wonder if there are any easier proofs.",,"['real-analysis', 'alternative-proof', 'monotone-functions']"
71,"Does there exist a sequence $(a_n)$ such that, for all $n$, $a_0 +a_1 X +\cdots+a_nX^n$ has exactly $n$ distinct real roots?","Does there exist a sequence  such that, for all ,  has exactly  distinct real roots?",(a_n) n a_0 +a_1 X +\cdots+a_nX^n n,"Does there exist a sequence $(a_n)_{n≥0}$ such that, for all $n$, $a_0 +a_1 X +\cdots+a_nX^n$ has exactly $n$ distinct real roots ? I wonder if such a sequence exists. Maybe something with algebraically independent real numbers ? Is it possible to give an example of such a sequence ?","Does there exist a sequence $(a_n)_{n≥0}$ such that, for all $n$, $a_0 +a_1 X +\cdots+a_nX^n$ has exactly $n$ distinct real roots ? I wonder if such a sequence exists. Maybe something with algebraically independent real numbers ? Is it possible to give an example of such a sequence ?",,"['calculus', 'real-analysis']"
72,Generalized Harmonic Number Summation $ \sum_{n=1}^{\infty} {2^{-n}}{(H_{n}^{(2)})^2}$,Generalized Harmonic Number Summation, \sum_{n=1}^{\infty} {2^{-n}}{(H_{n}^{(2)})^2},"Prove That $$ \sum_{n=1}^{\infty} \dfrac{(H_{n}^{(2)})^2}{2^n} = \tfrac{1}{360}\pi^4 - \tfrac16\pi^2\ln^22 + \tfrac16\ln^42 + 2\mathrm{Li}_4(\tfrac12) + \zeta(3)\ln2 $$ Notation : $ \displaystyle H_{n}^{(2)} = \sum_{r=1}^{n} \dfrac{1}{r^2}$ We can solve the above problem using the generating function $\displaystyle \sum_{n=1}^{\infty} (H_{n}^{(2)})^2 x^n $ , but it gets rather tedious especially taking into account the indefinite polylogarithm integrals involved. Can we solve it using other methods like Euler Series Transform or properties of summation ?","Prove That Notation : We can solve the above problem using the generating function , but it gets rather tedious especially taking into account the indefinite polylogarithm integrals involved. Can we solve it using other methods like Euler Series Transform or properties of summation ?", \sum_{n=1}^{\infty} \dfrac{(H_{n}^{(2)})^2}{2^n} = \tfrac{1}{360}\pi^4 - \tfrac16\pi^2\ln^22 + \tfrac16\ln^42 + 2\mathrm{Li}_4(\tfrac12) + \zeta(3)\ln2   \displaystyle H_{n}^{(2)} = \sum_{r=1}^{n} \dfrac{1}{r^2} \displaystyle \sum_{n=1}^{\infty} (H_{n}^{(2)})^2 x^n ,"['real-analysis', 'sequences-and-series', 'generating-functions', 'harmonic-numbers']"
73,"Alternative ways to evaluate $\int^1_0 \frac{\text{Li}_2(x)^3}{x}\,dx$",Alternative ways to evaluate,"\int^1_0 \frac{\text{Li}_2(x)^3}{x}\,dx","In the following link here I found the integral & the evaluation of $$\displaystyle \int^1_0 \frac{\text{Li}_2(x)^3}{x}\,dx$$ I'll also include a simpler version together  with the question: is it possible to find some easy ways of computing both integrals without using complicated sums that require multiple zeta formulae and ""never-ending long"" generating functions? $$i). \displaystyle \int^1_0 \frac{\text{Li}_2(x)^2}{x}\,dx$$ $$ii). \displaystyle \int^1_0 \frac{\text{Li}_2(x)^3}{x}\,dx$$","In the following link here I found the integral & the evaluation of $$\displaystyle \int^1_0 \frac{\text{Li}_2(x)^3}{x}\,dx$$ I'll also include a simpler version together  with the question: is it possible to find some easy ways of computing both integrals without using complicated sums that require multiple zeta formulae and ""never-ending long"" generating functions? $$i). \displaystyle \int^1_0 \frac{\text{Li}_2(x)^2}{x}\,dx$$ $$ii). \displaystyle \int^1_0 \frac{\text{Li}_2(x)^3}{x}\,dx$$",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'special-functions']"
74,Online Course for Real Analysis,Online Course for Real Analysis,,"I noticed there are some good undergraduate calculus and linear algebra courses online (eg edx, MIT open courseware, Khan Academy, etc) and I'm taking some myself. But I'm now thinking about going the extra step afterwards and tackling Real Analysis. Are there any similar good online courses for introductory Real Analysis? I seem plenty of helpful recommendations for texts for self study, but no high quality video lectures outside of some stuff on youtube. Thanks very much.","I noticed there are some good undergraduate calculus and linear algebra courses online (eg edx, MIT open courseware, Khan Academy, etc) and I'm taking some myself. But I'm now thinking about going the extra step afterwards and tackling Real Analysis. Are there any similar good online courses for introductory Real Analysis? I seem plenty of helpful recommendations for texts for self study, but no high quality video lectures outside of some stuff on youtube. Thanks very much.",,[]
75,Discrete version of dominated convergence thm,Discrete version of dominated convergence thm,,"Let $f_1,f_2,\ldots,g\colon\mathbb{Z}\rightarrow\mathbb{R}$ be functions such that $|f_N(n)|\leq g(n)$, $\sum_{n=-\infty}^\infty g(n)<\infty$, and $\lim_{N\rightarrow\infty}f_N(n)=f(n)$. Then show that $$\lim_{N\rightarrow\infty}\sum_{n=-\infty}^\infty f_N(n)=\sum_{n=-\infty}^\infty f(n).$$ This looks like the dominated convergence theorem, but how can we prove it directly? Edit : As T. Bongers helpfully pointed out, this can be shown using the dominated convergence theorem. Is there a direct way to do it without the theorem?","Let $f_1,f_2,\ldots,g\colon\mathbb{Z}\rightarrow\mathbb{R}$ be functions such that $|f_N(n)|\leq g(n)$, $\sum_{n=-\infty}^\infty g(n)<\infty$, and $\lim_{N\rightarrow\infty}f_N(n)=f(n)$. Then show that $$\lim_{N\rightarrow\infty}\sum_{n=-\infty}^\infty f_N(n)=\sum_{n=-\infty}^\infty f(n).$$ This looks like the dominated convergence theorem, but how can we prove it directly? Edit : As T. Bongers helpfully pointed out, this can be shown using the dominated convergence theorem. Is there a direct way to do it without the theorem?",,['real-analysis']
76,"If $f^2$ is differentiable, how pathological can $f$ be?","If  is differentiable, how pathological can  be?",f^2 f,"Apologies for what's probably a dumb question from the perspective of someone who paid better attention in real analysis class. Let $I \subseteq \mathbb{R}$ be an interval and $f : I \to \mathbb{R}$ be a continuous function such that $f^2$ is differentiable.  It follows by elementary calculus that $f$ is differentiable wherever it is nonzero.  However, considering for instance $f(x) = |x|$ shows that $f$ is not necessarily differentiable at its zeroes. Can the situation with $f$ be any worse than a countable set of isolated singularities looking like the one that $f(x) = |x|$ has at the origin?","Apologies for what's probably a dumb question from the perspective of someone who paid better attention in real analysis class. Let $I \subseteq \mathbb{R}$ be an interval and $f : I \to \mathbb{R}$ be a continuous function such that $f^2$ is differentiable.  It follows by elementary calculus that $f$ is differentiable wherever it is nonzero.  However, considering for instance $f(x) = |x|$ shows that $f$ is not necessarily differentiable at its zeroes. Can the situation with $f$ be any worse than a countable set of isolated singularities looking like the one that $f(x) = |x|$ has at the origin?",,['real-analysis']
77,Uniform integrability and tightness.,Uniform integrability and tightness.,,"Definition: Let $(X,M,\mu)$ be a measure space and $\{f_n\}$ a sequence of measurable functions on $x$ that are integrable. Then $\{f_n\}$ is uniformly integrable if for every $\epsilon >0$, there is a $\delta >0$ such that if $E$ is a measurable subset of $X$ such that $\mu(E) < \delta$, then $$ \int_E |f_n|~d\mu < \epsilon\qquad\text{for every} ~n.$$ $\{f_n\}$ is said to be tight if for each $\epsilon >0$, there is a subset $X_0$ of $X$ such that $\mu(X_0)< \infty$ and $$\int_{X\setminus X_0} |f_n|~d\mu < \epsilon\qquad\text{for every} ~n.$$ Theorem:(Vitali Convergence) Let $(X,M,\mu)$ be a measure space. Let $\{f_n\}$ be a sequence of uniformly integrable functions that also forms a tight sequence. Suppose $f_n(x) \to f(x)$ a.e. on $X$. Then, $f$ is integrable and, $$ \lim_{n\to \infty} \int_X f_n~d\mu = \int_X f~ d\mu.$$ I wish to prove the following: Let $\{f_n\}$ be a sequence of non-negative integrable functions on $X$. Suppose that $\{f_n(x)\} \to 0$ for almost all $x\in X.$. Then $$ \lim_{n\to\infty} \int f_n~d\mu =0 \Leftrightarrow \{f_n\}~\text{is uniformly integrable and tight.}$$ This is my Attempt: $(\Leftarrow)$ Suppose $f_n \to 0$. If $\{f_n\}$ is uniformly integrable and tight, then by Vitali's Convergence theorem, $\lim_{n\to \infty} \int f_n~d\mu = 0$. $(\Rightarrow)$ Let $\lim_{n\to \infty} \int f_n~d\mu = 0$. Let $\epsilon >0$. Then $\exists$ an $N$ such that $\int_X f_n ~d\mu< \epsilon$ whenever $n\geq N.$ Also, since $f_n \geq 0$, if $E$ is a measurable subset of $X$ and $n\geq N$, then $\int _E f_n~d\mu < \epsilon.$ I know that if I have a finite sequence $\{f_k\}_{n=1}^N$ of non-negative integrable functions over $X$, then $\{f_k\}_{n=1}^N$ is uniformly integrable, since if $E\subset X$ and $\mu(E)<\delta_k>0$ then $\int_E  f_k~d\mu < \epsilon$. I can take $\delta=\min (\delta_1,\ldots, \delta_k)$ so that $\mu(E)< \delta$ and $$\int_E f_k~d\mu < \epsilon.$$ I'm afraid this is where I'm stuck and I don't know how to proceed. Any form of help will be very much appreciated. Thanks.","Definition: Let $(X,M,\mu)$ be a measure space and $\{f_n\}$ a sequence of measurable functions on $x$ that are integrable. Then $\{f_n\}$ is uniformly integrable if for every $\epsilon >0$, there is a $\delta >0$ such that if $E$ is a measurable subset of $X$ such that $\mu(E) < \delta$, then $$ \int_E |f_n|~d\mu < \epsilon\qquad\text{for every} ~n.$$ $\{f_n\}$ is said to be tight if for each $\epsilon >0$, there is a subset $X_0$ of $X$ such that $\mu(X_0)< \infty$ and $$\int_{X\setminus X_0} |f_n|~d\mu < \epsilon\qquad\text{for every} ~n.$$ Theorem:(Vitali Convergence) Let $(X,M,\mu)$ be a measure space. Let $\{f_n\}$ be a sequence of uniformly integrable functions that also forms a tight sequence. Suppose $f_n(x) \to f(x)$ a.e. on $X$. Then, $f$ is integrable and, $$ \lim_{n\to \infty} \int_X f_n~d\mu = \int_X f~ d\mu.$$ I wish to prove the following: Let $\{f_n\}$ be a sequence of non-negative integrable functions on $X$. Suppose that $\{f_n(x)\} \to 0$ for almost all $x\in X.$. Then $$ \lim_{n\to\infty} \int f_n~d\mu =0 \Leftrightarrow \{f_n\}~\text{is uniformly integrable and tight.}$$ This is my Attempt: $(\Leftarrow)$ Suppose $f_n \to 0$. If $\{f_n\}$ is uniformly integrable and tight, then by Vitali's Convergence theorem, $\lim_{n\to \infty} \int f_n~d\mu = 0$. $(\Rightarrow)$ Let $\lim_{n\to \infty} \int f_n~d\mu = 0$. Let $\epsilon >0$. Then $\exists$ an $N$ such that $\int_X f_n ~d\mu< \epsilon$ whenever $n\geq N.$ Also, since $f_n \geq 0$, if $E$ is a measurable subset of $X$ and $n\geq N$, then $\int _E f_n~d\mu < \epsilon.$ I know that if I have a finite sequence $\{f_k\}_{n=1}^N$ of non-negative integrable functions over $X$, then $\{f_k\}_{n=1}^N$ is uniformly integrable, since if $E\subset X$ and $\mu(E)<\delta_k>0$ then $\int_E  f_k~d\mu < \epsilon$. I can take $\delta=\min (\delta_1,\ldots, \delta_k)$ so that $\mu(E)< \delta$ and $$\int_E f_k~d\mu < \epsilon.$$ I'm afraid this is where I'm stuck and I don't know how to proceed. Any form of help will be very much appreciated. Thanks.",,"['real-analysis', 'measure-theory']"
78,A sequence with no converging subsequence that clusters everywhere,A sequence with no converging subsequence that clusters everywhere,,"Let $I = [0,1]$ be the compact unit interval and $T = I^I$ the Tychonoff cube. It is pretty standard to exhibit a sequence in $T$ with no convergent subsequence. It is also fairly standard to show that $T$ is separable (e.g. the polynomials with rational coefficients are dense). A simple cardinality argument can be used to show that this sequence of polynomials has cluster points which are not the limit of any of its subsequences. Now, I believe, but cannot prove, that we may be able to exhibit a sequence in $T$ that has no convergent subsequence, but that its closure is all of $T$ . In particular, I am interested constructing such a sequence $(f_n(t))_{n=1}^\infty$ and for each point $f \in T$ exhibiting a subnet of $(f_n(t))_{n=1}^\infty$ that converges to $f$ . Any ideas?","Let be the compact unit interval and the Tychonoff cube. It is pretty standard to exhibit a sequence in with no convergent subsequence. It is also fairly standard to show that is separable (e.g. the polynomials with rational coefficients are dense). A simple cardinality argument can be used to show that this sequence of polynomials has cluster points which are not the limit of any of its subsequences. Now, I believe, but cannot prove, that we may be able to exhibit a sequence in that has no convergent subsequence, but that its closure is all of . In particular, I am interested constructing such a sequence and for each point exhibiting a subnet of that converges to . Any ideas?","I = [0,1] T = I^I T T T T (f_n(t))_{n=1}^\infty f \in T (f_n(t))_{n=1}^\infty f","['real-analysis', 'sequences-and-series', 'general-topology', 'product-space']"
79,"What does it mean when two sets are ""adjoined"" in a metric space?","What does it mean when two sets are ""adjoined"" in a metric space?",,"I encountered the word ""adjoined"" in Baby Rudin, Chapter 2 concerning basic topology on Euclidean space. It appeared in the proof to Theorem 2.35 Theorem $\quad$ Closed subsets of compact sets are compact Proof $\quad$ Suppose $F\subset K\subset X$ , $F$ is closed (relative to $X$ ), and $K$ is compact. Let $\{V_{\alpha}\}$ be an open cover of $F$ . If $F^c$ is adjoined to $\{V_{\alpha}\}$ ,we obtain an open cover $\Omega$ of $K$ . Since $K$ is compact, there is a finite subcollection $\Phi$ of $\Omega$ which covers $K$ , and hence $F$ . If $F^c$ is a member of $\Phi$ , we may remove it from $\Phi$ and still retain an open cover of $F$ . We have thus shown that a finite subcollection of $\{V_{\alpha}\}$ covers $F$ . First, to be honest I don't understand the meaning of ""adjoined"", I guess it might mean that the two sets are ""complementary"" except for the ""boundary"" between them. If this is what ""adjoined"" means, then I am still confused. Because since $\{V_{\alpha}\}$ is an open cover of $F$ , and $F^c$ is open. If these two open sets are ""adjoined"", then neither of them includes $\partial F$ , which is absurd. So it might be that I just failed to understand the word ""adjoined"" properly. Without a proper understanding, I find it hard for me to process the whole proof. Can you help me? Thanks in advance!","I encountered the word ""adjoined"" in Baby Rudin, Chapter 2 concerning basic topology on Euclidean space. It appeared in the proof to Theorem 2.35 Theorem Closed subsets of compact sets are compact Proof Suppose , is closed (relative to ), and is compact. Let be an open cover of . If is adjoined to ,we obtain an open cover of . Since is compact, there is a finite subcollection of which covers , and hence . If is a member of , we may remove it from and still retain an open cover of . We have thus shown that a finite subcollection of covers . First, to be honest I don't understand the meaning of ""adjoined"", I guess it might mean that the two sets are ""complementary"" except for the ""boundary"" between them. If this is what ""adjoined"" means, then I am still confused. Because since is an open cover of , and is open. If these two open sets are ""adjoined"", then neither of them includes , which is absurd. So it might be that I just failed to understand the word ""adjoined"" properly. Without a proper understanding, I find it hard for me to process the whole proof. Can you help me? Thanks in advance!",\quad \quad F\subset K\subset X F X K \{V_{\alpha}\} F F^c \{V_{\alpha}\} \Omega K K \Phi \Omega K F F^c \Phi \Phi F \{V_{\alpha}\} F \{V_{\alpha}\} F F^c \partial F,"['calculus', 'real-analysis', 'metric-spaces', 'terminology']"
80,"Accumulation points of $ \{x_n \in \mathbb{R}, n \in \mathbb{N} \ \ | \ x_n = n\sin(n) \}$? [duplicate]",Accumulation points of ? [duplicate]," \{x_n \in \mathbb{R}, n \in \mathbb{N} \ \ | \ x_n = n\sin(n) \}","This question already has answers here : Is $n \sin n$ dense on the real line? (3 answers) Closed 9 years ago . A younger student asked me: What are accumulation points of the following set? $$ \{x_n \in \mathbb{R}, n \in \mathbb{N} \ \ | \ x_n = n\sin(n) \}$$ I really can't answer this question, could anyone help me?","This question already has answers here : Is $n \sin n$ dense on the real line? (3 answers) Closed 9 years ago . A younger student asked me: What are accumulation points of the following set? $$ \{x_n \in \mathbb{R}, n \in \mathbb{N} \ \ | \ x_n = n\sin(n) \}$$ I really can't answer this question, could anyone help me?",,"['real-analysis', 'diophantine-approximation']"
81,Convergence of a product series with one divergent factor,Convergence of a product series with one divergent factor,,"I'm currently struggling with the following problem: Let $\displaystyle \sum_{k=1}^{\infty} a_k$ be a convergent series with $a_k \in \mathbb{R} \setminus \{0\}$. Then is there always a sequence $\{b_k\}$ of real numbers with $\displaystyle \lim_{k \to \infty} b_k = \infty$ such that the series $\displaystyle \sum_{k=1}^{\infty} a_k b_k$ will still converge? My intuition of course says there is, as one should always be able to find some sequence that increases ""much slower"" than $a_k$ decreases. But how can I state this vague notion more precisely and actually prove my guess? I thought of choosing $b_k := -\log a_k$ or something, but that won't hold in all possible cases, won't it? Could you give any hints, please?","I'm currently struggling with the following problem: Let $\displaystyle \sum_{k=1}^{\infty} a_k$ be a convergent series with $a_k \in \mathbb{R} \setminus \{0\}$. Then is there always a sequence $\{b_k\}$ of real numbers with $\displaystyle \lim_{k \to \infty} b_k = \infty$ such that the series $\displaystyle \sum_{k=1}^{\infty} a_k b_k$ will still converge? My intuition of course says there is, as one should always be able to find some sequence that increases ""much slower"" than $a_k$ decreases. But how can I state this vague notion more precisely and actually prove my guess? I thought of choosing $b_k := -\log a_k$ or something, but that won't hold in all possible cases, won't it? Could you give any hints, please?",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
82,Prove $\frac{H(x^2)}{H(x)}$ increases.,Prove  increases.,\frac{H(x^2)}{H(x)},"For $x\in[0,1]$ , let $f(x):=-x\ln x$ and the two-sample entropy function $H(x)=f(x)+f(1-x)$ . Prove $h(x):=\displaystyle\frac{H(x^2)}{H(x)}$ increases. Here is my proof which is a bit cumbersome. I am seeking a much more elegant approach. The numerator of the derivative of the sought fraction is \begin{align} &H(x)^2\frac{dh(x)}{dx} \\ =&\frac d{dx}H(x^2)H(x)-H(x^2)\frac d{dx}H(x) \\ =& 2x\ln\frac{x^2}{1-x^2}\,\big(x\ln x+(1-x)\ln(1-x)\big)-\big(x^2\ln x^2+(1-x^2)\ln(1-x^2)\big)\ln\frac x{1-x} \\ =& 2x^2\ln^2 x+2x(2-x)\ln x\ln(1-x)-(x^2+1)\ln x\ln(1-x^2)+(1-x)^2\ln(1-x)\ln(1-x^2). \tag1\label1 \end{align} All four terms above except the third are positive. I combine the second and the third term together and divide it by $-\ln x$ which is positive, and get \begin{align} g(x):&=-2x(2-x)\ln(1-x)+(x^2+1)\ln(1-x^2) \\ &=(3x-1)(x-1)\ln(1-x)+(x^2+1)\ln(1+x) \tag2\label2 \\ &= \int_0^x \Big(g''(a)-\int_t^a g'''(s)ds\Big)(x-t)dt \end{align} for some $a\in[0,x]$ . So we only need to show $g(x)>0, \forall x\in\big(0,\frac13\big]$ . $$\frac{d^3g(x)}{dx^3}= \frac{4x(2x^3 +3x^2-2x-7)}{(1-x)^2(1+x)^3}.$$ Let $p(x):=2x^3+3x^2-2x-7$ . $p(x)\le p(1)=-4, \forall x\in[0,1]$ . This is true since $p(x)$ is convex as $p''(x)=12(x+\frac12)>0$ on that interval and $p(0)=-7<-4=p(1)$ . We can take $a=\frac13$ since we can show, with a bit of work, $g''(\frac13)>0$ . (to be continued)","For , let and the two-sample entropy function . Prove increases. Here is my proof which is a bit cumbersome. I am seeking a much more elegant approach. The numerator of the derivative of the sought fraction is All four terms above except the third are positive. I combine the second and the third term together and divide it by which is positive, and get for some . So we only need to show . Let . . This is true since is convex as on that interval and . We can take since we can show, with a bit of work, . (to be continued)","x\in[0,1] f(x):=-x\ln x H(x)=f(x)+f(1-x) h(x):=\displaystyle\frac{H(x^2)}{H(x)} \begin{align}
&H(x)^2\frac{dh(x)}{dx} \\
=&\frac d{dx}H(x^2)H(x)-H(x^2)\frac d{dx}H(x) \\
=& 2x\ln\frac{x^2}{1-x^2}\,\big(x\ln x+(1-x)\ln(1-x)\big)-\big(x^2\ln x^2+(1-x^2)\ln(1-x^2)\big)\ln\frac x{1-x} \\
=& 2x^2\ln^2 x+2x(2-x)\ln x\ln(1-x)-(x^2+1)\ln x\ln(1-x^2)+(1-x)^2\ln(1-x)\ln(1-x^2). \tag1\label1
\end{align} -\ln x \begin{align}
g(x):&=-2x(2-x)\ln(1-x)+(x^2+1)\ln(1-x^2) \\
&=(3x-1)(x-1)\ln(1-x)+(x^2+1)\ln(1+x) \tag2\label2 \\
&= \int_0^x \Big(g''(a)-\int_t^a g'''(s)ds\Big)(x-t)dt
\end{align} a\in[0,x] g(x)>0, \forall x\in\big(0,\frac13\big] \frac{d^3g(x)}{dx^3}= \frac{4x(2x^3 +3x^2-2x-7)}{(1-x)^2(1+x)^3}. p(x):=2x^3+3x^2-2x-7 p(x)\le p(1)=-4, \forall x\in[0,1] p(x) p''(x)=12(x+\frac12)>0 p(0)=-7<-4=p(1) a=\frac13 g''(\frac13)>0","['real-analysis', 'calculus', 'inequality', 'entropy']"
83,"How can we study $\lim_{t\to\infty}\int_{\mathbb{R}}\psi_t(x)dx$, when $\lim$ and $\int$ do not commute?","How can we study , when  and  do not commute?",\lim_{t\to\infty}\int_{\mathbb{R}}\psi_t(x)dx \lim \int,"I am dealing with the problem of studying $$\mathcal{I}= \lim_{t\to\infty}\int_{\mathbb{R}}\psi_t(x)dx$$ In my problem $\psi_t(x)$ is of the form $f_t(x)\mathbf{1}_{x>t}$ . Moreover, $f_t(x)=\frac{g(x)}{g(t)}\varphi(t-x+k)$ where $g> 0$ is the probability density function of a continuous unbounded support random variable and $\varphi$ is the probability density function of a standard normal ( $k$ being a constant). It is clear that $\lim_{t\to\infty}\psi_t(x)$ is pointwise $0$ . As a consequence, whenever I can bound $|\psi_t(x)|$ by an integrable function it's easy to conclude $\mathcal{I}=0$ by dominated convergence theorem. The ratio appearing in the integrand is always smaller than one (at least when $g$ is eventually decreasing) but, since the $f$ factor translates, it is difficult to find the desired bound. Actually, depending on shape of $g$ , this may not be possible.Indeed, numerical simulation suggest that, for many interesting choices of $g$ , we have $\mathcal{I}\neq 0$ . In these cases, it seems that it must not be possible to exchange integral and limit. How to proceed ? EDIT It is useful to notice that by de l'hospital $$-\lim_{t\to\infty}\frac{g'(t)}{g(t)}=\lim_{t\to\infty} \frac{g(t)}{1-G(t)}$$ which appears in the calculations below. Here Computation of $ \lim_{t\to\infty}\int_t^\infty\frac{g(x)}{\sigma g(t)}f(\frac{t-x}{\sigma}+k)dx$ with $g$ and $\varphi$ density functions. I attempt a probably flawed calculation.","I am dealing with the problem of studying In my problem is of the form . Moreover, where is the probability density function of a continuous unbounded support random variable and is the probability density function of a standard normal ( being a constant). It is clear that is pointwise . As a consequence, whenever I can bound by an integrable function it's easy to conclude by dominated convergence theorem. The ratio appearing in the integrand is always smaller than one (at least when is eventually decreasing) but, since the factor translates, it is difficult to find the desired bound. Actually, depending on shape of , this may not be possible.Indeed, numerical simulation suggest that, for many interesting choices of , we have . In these cases, it seems that it must not be possible to exchange integral and limit. How to proceed ? EDIT It is useful to notice that by de l'hospital which appears in the calculations below. Here Computation of $ \lim_{t\to\infty}\int_t^\infty\frac{g(x)}{\sigma g(t)}f(\frac{t-x}{\sigma}+k)dx$ with $g$ and $\varphi$ density functions. I attempt a probably flawed calculation.","\mathcal{I}= \lim_{t\to\infty}\int_{\mathbb{R}}\psi_t(x)dx \psi_t(x) f_t(x)\mathbf{1}_{x>t} f_t(x)=\frac{g(x)}{g(t)}\varphi(t-x+k) g>
0 \varphi k \lim_{t\to\infty}\psi_t(x) 0 |\psi_t(x)| \mathcal{I}=0 g f g g \mathcal{I}\neq 0 -\lim_{t\to\infty}\frac{g'(t)}{g(t)}=\lim_{t\to\infty} \frac{g(t)}{1-G(t)}","['real-analysis', 'calculus', 'probability', 'integration', 'measure-theory']"
84,An elementary function with asymptotic $f'(x)\sim2f(2x)$ for $x\to0^+$,An elementary function with asymptotic  for,f'(x)\sim2f(2x) x\to0^+,"We want to find an elementary function $f(x)$ that is smooth and strictly increasing on some interval $x\in(0,\epsilon)$ , satisfying $\lim\limits_{\,x\to0^+}f(x)=0$ , whose asymptotic for $x\to0^+$ is $f'(x)\sim2f(2x)$ , that is, $\lim\limits_{\,x\to0^+}\frac{f'(x)}{2f(2x)} = 1$ . By a lengthy series of trial and error I found a solution. You can click “Reveal spoiler” below to see it, unless you want to ponder on this problem on your own for a while before looking at my version. $\displaystyle\quad f(x)=\exp\left(-\left(\frac12+\frac1{\ln2}\right)\cdot\ln x-\frac1{\ln4}\cdot\ln^2\left(-\frac{\ln x}{x\cdot\ln2}\right)\right)$ Its derivative is rather cumbersome, but Mathematica says the limit $\lim\limits_{\,x\to0^+}\frac{f'(x)}{2f(2x)}$ is indeed $1$ , which is also confirmed by manual calculations. The solution is not unique, there are possible variations — I picked one that looked more readable. If we only impose a weaker condition $f'(x)=\mathcal O\left(f(2x)\right)$ , then the solution becomes simpler: $\displaystyle\quad f(x)=2^{\large-\frac{1}{2} \log _2^2\left(-\frac{\log_2\!x}{x}\right)}$ Questions: Is there a systematic approach to finding a solution to this problem? Is there a simpler elementary function with required properties?","We want to find an elementary function that is smooth and strictly increasing on some interval , satisfying , whose asymptotic for is , that is, . By a lengthy series of trial and error I found a solution. You can click “Reveal spoiler” below to see it, unless you want to ponder on this problem on your own for a while before looking at my version. Its derivative is rather cumbersome, but Mathematica says the limit is indeed , which is also confirmed by manual calculations. The solution is not unique, there are possible variations — I picked one that looked more readable. If we only impose a weaker condition , then the solution becomes simpler: Questions: Is there a systematic approach to finding a solution to this problem? Is there a simpler elementary function with required properties?","f(x) x\in(0,\epsilon) \lim\limits_{\,x\to0^+}f(x)=0 x\to0^+ f'(x)\sim2f(2x) \lim\limits_{\,x\to0^+}\frac{f'(x)}{2f(2x)} = 1 \displaystyle\quad f(x)=\exp\left(-\left(\frac12+\frac1{\ln2}\right)\cdot\ln x-\frac1{\ln4}\cdot\ln^2\left(-\frac{\ln x}{x\cdot\ln2}\right)\right) \lim\limits_{\,x\to0^+}\frac{f'(x)}{2f(2x)} 1 f'(x)=\mathcal O\left(f(2x)\right) \displaystyle\quad f(x)=2^{\large-\frac{1}{2} \log _2^2\left(-\frac{\log_2\!x}{x}\right)}","['real-analysis', 'calculus', 'limits', 'asymptotics', 'elementary-functions']"
85,"Does there exist a bijection that is not eventually equal to the identity, but has a finite number of ""crosses""?","Does there exist a bijection that is not eventually equal to the identity, but has a finite number of ""crosses""?",,"Consider a  bijection $f: \mathbb{N} \to \mathbb{N}$ : We define the (possible infinite) number of ""crosses"" or ""intersections"" by the following: For every pair $i,j$ with $i<j, $ define: $$ \alpha_{ij}=\begin{cases} 0&\ \text{if} \ f(j) > f(i)\\[8pt] 1&\ \text{if} \ f(j) < f(i)\\[8pt] \end{cases} $$ Then the number of (possibly infinite) intersections is $\ s(f) = \displaystyle \sum_{i=1}^\infty \left( \sum_{j=i+1}^\infty\alpha_{ij} \right)$ . (s depends only on the bijection $f$ ). Also, define a bijection eventually equal to the identity to be one in which $ \exists N$ such that $f(n) = n \quad \forall n \geq N$ . My conjecture is that a bijection $f: \mathbb{N} \to \mathbb{N}$ is eventually equal to the identity $ \iff s$ is finite, but I don't know how to prove this formally. Note that my conjecture would be false with bijections $f: \mathbb{Z} \to \mathbb{Z}$ , for example: The pigeonhole principle came to mind, but I'm not sure if it's useful or necessary for a proof.","Consider a  bijection : We define the (possible infinite) number of ""crosses"" or ""intersections"" by the following: For every pair with define: Then the number of (possibly infinite) intersections is . (s depends only on the bijection ). Also, define a bijection eventually equal to the identity to be one in which such that . My conjecture is that a bijection is eventually equal to the identity is finite, but I don't know how to prove this formally. Note that my conjecture would be false with bijections , for example: The pigeonhole principle came to mind, but I'm not sure if it's useful or necessary for a proof.","f: \mathbb{N} \to \mathbb{N} i,j i<j,  
\alpha_{ij}=\begin{cases}
0&\ \text{if} \ f(j) > f(i)\\[8pt]
1&\ \text{if} \ f(j) < f(i)\\[8pt]
\end{cases}
 \ s(f) = \displaystyle \sum_{i=1}^\infty \left( \sum_{j=i+1}^\infty\alpha_{ij} \right) f  \exists N f(n) = n \quad \forall n \geq N f: \mathbb{N} \to \mathbb{N}  \iff s f: \mathbb{Z} \to \mathbb{Z}","['real-analysis', 'sequences-and-series', 'analysis', 'pigeonhole-principle']"
86,Solve $f(x+f(2y))=f(x)+f(y)+y$,Solve,f(x+f(2y))=f(x)+f(y)+y,"Find all $f:\mathbb{R}^+\to \mathbb{R}^+$ such that for each $x$ and $y$ in $\mathbb{R}^+$, $$f(x+f(2y))=f(x)+f(y)+y$$ Note: $f(x)=x+b$ is a solution for all  $b\in\mathbb{R}^+$ but I can not prove it.","Find all $f:\mathbb{R}^+\to \mathbb{R}^+$ such that for each $x$ and $y$ in $\mathbb{R}^+$, $$f(x+f(2y))=f(x)+f(y)+y$$ Note: $f(x)=x+b$ is a solution for all  $b\in\mathbb{R}^+$ but I can not prove it.",,"['real-analysis', 'functions', 'functional-equations', 'real-numbers']"
87,"Let $(s_n)$ be a sequence of nonnegative numbers, and $\sigma_n=\frac{1}{n}(s_1+s_2+\cdots +s_n)$. Show that $\liminf s_n \le \liminf \sigma_n$.","Let  be a sequence of nonnegative numbers, and . Show that .",(s_n) \sigma_n=\frac{1}{n}(s_1+s_2+\cdots +s_n) \liminf s_n \le \liminf \sigma_n,"Let $(s_n)$ be a sequence of nonnegative numbers, and for each $n$ define $\sigma_n=\frac{1}{n}(s_1+s_2+\cdots +s_n)$. Show that $\liminf s_n \le \liminf \sigma_n$. Actually, I showed $\limsup \sigma_n \le \limsup s_n$ in the following way. For any $n \gt M \gt N$, we can get the following inequality, $\sup \{\sigma_n: n\gt M\}\le \frac{1}{M}(s_1+s_2+\cdots +s_N)+\sup\{s_n:n\gt N\}.$ So first taking the limit as $M\to \infty$ then as $N \to \infty$, we get the inequality. However, this method does not work out for the $\liminf$ case, since the above inequality was derived using the fact that $1/n \lt 1/M$. How can I show the inequality for the $\liminf$ case? I would greatly appreciate any suggestions or solutions.","Let $(s_n)$ be a sequence of nonnegative numbers, and for each $n$ define $\sigma_n=\frac{1}{n}(s_1+s_2+\cdots +s_n)$. Show that $\liminf s_n \le \liminf \sigma_n$. Actually, I showed $\limsup \sigma_n \le \limsup s_n$ in the following way. For any $n \gt M \gt N$, we can get the following inequality, $\sup \{\sigma_n: n\gt M\}\le \frac{1}{M}(s_1+s_2+\cdots +s_N)+\sup\{s_n:n\gt N\}.$ So first taking the limit as $M\to \infty$ then as $N \to \infty$, we get the inequality. However, this method does not work out for the $\liminf$ case, since the above inequality was derived using the fact that $1/n \lt 1/M$. How can I show the inequality for the $\liminf$ case? I would greatly appreciate any suggestions or solutions.",,"['calculus', 'real-analysis', 'analysis', 'limits', 'limsup-and-liminf']"
88,"Show all roots of $\sum_{k=0}^n 2^{k(n-k)} x^k$ are real (December 6, 2014 Putnam problem)","Show all roots of  are real (December 6, 2014 Putnam problem)",\sum_{k=0}^n 2^{k(n-k)} x^k,"Show that for each positive integer n, all roots of the polynomial $\sum_{k=0}^n 2^{k(n-k)} x^k$ are real numbers. I have no idea where to start. From this year's Putnam, problem B4.","Show that for each positive integer n, all roots of the polynomial $\sum_{k=0}^n 2^{k(n-k)} x^k$ are real numbers. I have no idea where to start. From this year's Putnam, problem B4.",,"['real-analysis', 'polynomials', 'summation', 'roots']"
89,Integral $\int_0^\infty \frac{\cos x}{x}\left(\int_0^x \frac{\sin t}{t}dt\right)^2dx=-\frac{7}{6}\zeta(3)$,Integral,\int_0^\infty \frac{\cos x}{x}\left(\int_0^x \frac{\sin t}{t}dt\right)^2dx=-\frac{7}{6}\zeta(3),"I am trying to prove this below. $$ I:=\int_0^\infty \frac{\cos x}{x}\left(\int_0^x \frac{\sin t}{t}dt\right)^2dx=-\frac{7}{6}\zeta(3) $$ where $$ \zeta(3)=\sum_{n=1}^\infty \frac{1}{n^3}. $$ I am not sure how to work with the integral over $t$ because it is from $0$ to $x$ .   If we can somehow write $$ \int_0^\infty \frac{\cos x}{x} \left(\int_0^\infty \frac{\sin t}{t}dt-\int_x^\infty \frac{\sin t}{t}dt     \right)^2dx=\int_0^\infty \frac{\cos x}{x}\left(\frac{\pi}{2}-\int_x^\infty \frac{\sin t}{t}dt\right)^2dx. $$ I do not want to use an asymptotic expansion on the integral over $t$ from $x$ to $\infty$ , I am looking for exact results.  Note we can use $\int_0^\infty \frac{\sin t}{t}dt=\int_0^\infty \mathcal{L}[\sin t(s)]ds=\frac{\pi}{2}.$ Other than this approach I am not really sure how to go about this.  Note by definition $$ \int_0^x \frac{\sin t}{t}dt\equiv Si(x), $$ but I'm not too sure what this definition can be used for in terms of a proof.  Also note $$ \int_0^\infty \frac{\cos x}{x}dx \to \infty. $$","I am trying to prove this below. where I am not sure how to work with the integral over because it is from to .   If we can somehow write I do not want to use an asymptotic expansion on the integral over from to , I am looking for exact results.  Note we can use Other than this approach I am not really sure how to go about this.  Note by definition but I'm not too sure what this definition can be used for in terms of a proof.  Also note","
I:=\int_0^\infty \frac{\cos x}{x}\left(\int_0^x \frac{\sin t}{t}dt\right)^2dx=-\frac{7}{6}\zeta(3)
 
\zeta(3)=\sum_{n=1}^\infty \frac{1}{n^3}.
 t 0 x 
\int_0^\infty \frac{\cos x}{x} \left(\int_0^\infty \frac{\sin t}{t}dt-\int_x^\infty \frac{\sin t}{t}dt     \right)^2dx=\int_0^\infty \frac{\cos x}{x}\left(\frac{\pi}{2}-\int_x^\infty \frac{\sin t}{t}dt\right)^2dx.
 t x \infty \int_0^\infty \frac{\sin t}{t}dt=\int_0^\infty \mathcal{L}[\sin t(s)]ds=\frac{\pi}{2}. 
\int_0^x \frac{\sin t}{t}dt\equiv Si(x),
 
\int_0^\infty \frac{\cos x}{x}dx \to \infty.
","['real-analysis', 'integration', 'complex-analysis', 'definite-integrals', 'special-functions']"
90,All nested partial sums of a sequence tend to $0$. Is the sequence constant?,All nested partial sums of a sequence tend to . Is the sequence constant?,0,"$S^0:\mathbb N\to \Bbb R$ is a function. For any $m\in \Bbb N$, we define $$S^m:\Bbb N\to \Bbb R$$ $$S^m(n)=\sum_{k=1}^n S^{m-1}(k)$$ For each $m\in \Bbb N$, we have: $$\lim_{n \to \infty}S^m(n)=0$$ Can we deduce $S^0\equiv 0?$","$S^0:\mathbb N\to \Bbb R$ is a function. For any $m\in \Bbb N$, we define $$S^m:\Bbb N\to \Bbb R$$ $$S^m(n)=\sum_{k=1}^n S^{m-1}(k)$$ For each $m\in \Bbb N$, we have: $$\lim_{n \to \infty}S^m(n)=0$$ Can we deduce $S^0\equiv 0?$",,['calculus']
91,Is there a well-ordering of the reals whose initial segments are all measurable?,Is there a well-ordering of the reals whose initial segments are all measurable?,,"Pretty much the question in the title: can there be a well-ordering the reals $\langle r_\alpha\mid \alpha<\kappa \rangle$ , all of whose initial segments $I_\beta := \{r_\alpha\mid \alpha<\beta<\kappa\}$ are Lebesgue-measurable? Observation: if the Continuum Hypothesis holds, then any well-ordering of the reals in ordertype $\omega_1$ will have this property. So it's consistent that such a well-ordering exists.","Pretty much the question in the title: can there be a well-ordering the reals , all of whose initial segments are Lebesgue-measurable? Observation: if the Continuum Hypothesis holds, then any well-ordering of the reals in ordertype will have this property. So it's consistent that such a well-ordering exists.",\langle r_\alpha\mid \alpha<\kappa \rangle I_\beta := \{r_\alpha\mid \alpha<\beta<\kappa\} \omega_1,"['real-analysis', 'measure-theory', 'set-theory', 'lebesgue-measure']"
92,"Prove that $16(a\sin a + \cos a - 1)^2 \le 2a^4 + a^3 \sin 2a, \ \forall a\ge 0$",Prove that,"16(a\sin a + \cos a - 1)^2 \le 2a^4 + a^3 \sin 2a, \ \forall a\ge 0","Problem 1 : Prove that $$16(a\sin a + \cos a - 1)^2 \le 2a^4 + a^3 \sin 2a, \ \forall a\ge 0.\tag{1}$$ This is the stronger version of the following Prove that $12(a\sin a+\cos a-1)^2\le 2a^4+a^3\sin(2a)$,$\forall a\in (0,\infty)$ : Problem 2 : Prove that $$12(a\sin a + \cos a - 1)^2 \le 2a^4 + a^3 \sin 2a, \ \forall a\ge 0. \tag{2}$$ For Problem 2, there is a very nice solution using Cauchy-Bunyakovsky-Schwarz inequality for integral Prove that $12(a\sin a+\cos a-1)^2\le 2a^4+a^3\sin(2a)$,$\forall a\in (0,\infty)$ . Indeed, the inequality (2) is nothing but $$12\left(\int_0^a x\cos x \mathrm{d} x \right)^2 \le 12\left(\int_0^a x^2 \mathrm{d} x\right) \left(\int_0^a \cos^2 x \mathrm{d}x\right).$$ For Problem 2, I gave a complicated proof. See Prove that $12(a\sin a+\cos a-1)^2\le 2a^4+a^3\sin(2a)$,$\forall a\in (0,\infty)$ Are there any nice solutions for Problem 1? Any comments and solutions are welcome and appreciated.","Problem 1 : Prove that This is the stronger version of the following Prove that $12(a\sin a+\cos a-1)^2\le 2a^4+a^3\sin(2a)$,$\forall a\in (0,\infty)$ : Problem 2 : Prove that For Problem 2, there is a very nice solution using Cauchy-Bunyakovsky-Schwarz inequality for integral Prove that $12(a\sin a+\cos a-1)^2\le 2a^4+a^3\sin(2a)$,$\forall a\in (0,\infty)$ . Indeed, the inequality (2) is nothing but For Problem 2, I gave a complicated proof. See Prove that $12(a\sin a+\cos a-1)^2\le 2a^4+a^3\sin(2a)$,$\forall a\in (0,\infty)$ Are there any nice solutions for Problem 1? Any comments and solutions are welcome and appreciated.","16(a\sin a + \cos a - 1)^2 \le 2a^4 + a^3 \sin 2a, \ \forall a\ge 0.\tag{1} 12(a\sin a + \cos a - 1)^2 \le 2a^4 + a^3 \sin 2a, \ \forall a\ge 0. \tag{2} 12\left(\int_0^a x\cos x \mathrm{d} x \right)^2 \le 12\left(\int_0^a x^2 \mathrm{d} x\right)
\left(\int_0^a \cos^2 x \mathrm{d}x\right).","['real-analysis', 'calculus', 'inequality']"
93,Irrationality of $\pi$ another proof,Irrationality of  another proof,\pi,"Proposition. Let $\alpha\in\mathbb{R}$. If there is a sequence of integers $a_n,b_n$ such that  $0<|b_n\alpha-a_n|\longrightarrow 0^+$ as $n\longrightarrow \infty$, then $\alpha$ is irrational. How to prove that $\pi$ is irrational using this proposition? I know several proof of the irrationality of π with complex analysis, but I think in this way is very difficult. For example to prove the irrationality of $e$ consider $$0<n!e-n!\left(1+\frac{1}{2!}+\frac{1}{3!}+\cdots +\frac{1}{n!}\right)\le\frac{1}{n}\longrightarrow0^+$$ Any hint would be appreciated.","Proposition. Let $\alpha\in\mathbb{R}$. If there is a sequence of integers $a_n,b_n$ such that  $0<|b_n\alpha-a_n|\longrightarrow 0^+$ as $n\longrightarrow \infty$, then $\alpha$ is irrational. How to prove that $\pi$ is irrational using this proposition? I know several proof of the irrationality of π with complex analysis, but I think in this way is very difficult. For example to prove the irrationality of $e$ consider $$0<n!e-n!\left(1+\frac{1}{2!}+\frac{1}{3!}+\cdots +\frac{1}{n!}\right)\le\frac{1}{n}\longrightarrow0^+$$ Any hint would be appreciated.",,"['real-analysis', 'irrational-numbers', 'transcendental-numbers']"
94,The limit of $\sin(n!)$,The limit of,\sin(n!),It is known that $\lim\limits_{n\to\infty}\sin n$ does not exist. $\lim\limits_{n\to\infty}\sin(n!)$ exists or not?,It is known that $\lim\limits_{n\to\infty}\sin n$ does not exist. $\lim\limits_{n\to\infty}\sin(n!)$ exists or not?,,"['real-analysis', 'limits', 'analysis', 'trigonometry', 'factorial']"
95,Property of a recursive integer sequence,Property of a recursive integer sequence,,"Loosely related to this question , I encountered a recursive sequence of integers $(b(j,n))_{j\in\mathbb Z,n\in\mathbb N}$ given by $$  b(0,1)=-1\qquad b(j,n)=0\text{ if }j<0\text{ or }j\geq n\\  b(j,n+1)=b(j,n)(2j-n)+b(j-1,n)(2j-3n-1)\text{ for all }n\in\mathbb N, j\in\lbrace 0,\ldots,n\rbrace.\tag{1} $$ Note that the non-vanishing terms of $(b(j,n))_{j\in\mathbb Z,n\in\mathbb N}$ form a triangle $$ \begin{matrix} &\underline{j=0}&\underline{j=1}&\ldots&&\\ n=1\,|&-1	&&&&\\ n=2\,|&\hphantom{-}1	&\hphantom{-}2&&&\\ \vdots&-2	&-5&-6&&\\ &\hphantom{-}6	&\hphantom{-}21	&\hphantom{-}24	&\hphantom{-}24&\\ &-24	&-108	&-189	&-120	&-120 \end{matrix} $$ Now explicit calculations suggest that one can attach polynomial weights in $j$ and $n$ to the $b(j,n)$ such that the sum over any row vanishes . More precisely: Conjecture. For any $n\in\mathbb N$   $$ \sum_{j=0}^{n-1} b(j,n)\big( 32j^3-32(2n-1)j^2 +2(22n^2-30n+13)j-(n-1)(2n-1)(5n-6) \big)=0 $$ I believe this to be true, but so far I was not able to prove it. The problem with induction here is that when using (1) in the induction step, the weight gets $j^4$-terms which then can't be directly connected to (2) anymore. The obvious way would be trying to find a closed form for the $b(j,n)$ but that seems quite difficult and I was hoping there would be an easier, ""intrinsic"" (only using the recursion or other properties of the sequence) way of proving this. I'm aware of the generating functions ansatz but as the ""weights"" in (1) depend on $j,n$ this seems to not work directly either. Thanks in advance for any answer or comment! Edit: just to vizualize the problem, the first non-vanishing elements of the sequence in question $\scriptstyle\big(b(j,n)(32j^3-32(2n-1)j^2 +2(22n^2-30n+13)j-(n-1)(2n-1)(5n-6))\big)_{j\in\mathbb Z,n\in\mathbb N}$ are given by $$ \begin{matrix} &\underline{j=0}&\underline{j=1}&\ldots&&\\ n=1\,|&0	&&&&\\ n=2\,|&-12&\hphantom{-}12&&&\\ \vdots&\hphantom{-}180	&-120&	-60&&\\ &-1764	&\hphantom{-}84	&\hphantom{-}1104	&\hphantom{-}576&\\ &\hphantom{-}16416	&\hphantom{-}12312	&-13608	&-7920	&-7200 \end{matrix} $$ Now it is evident that any row here sums up to 0.","Loosely related to this question , I encountered a recursive sequence of integers $(b(j,n))_{j\in\mathbb Z,n\in\mathbb N}$ given by $$  b(0,1)=-1\qquad b(j,n)=0\text{ if }j<0\text{ or }j\geq n\\  b(j,n+1)=b(j,n)(2j-n)+b(j-1,n)(2j-3n-1)\text{ for all }n\in\mathbb N, j\in\lbrace 0,\ldots,n\rbrace.\tag{1} $$ Note that the non-vanishing terms of $(b(j,n))_{j\in\mathbb Z,n\in\mathbb N}$ form a triangle $$ \begin{matrix} &\underline{j=0}&\underline{j=1}&\ldots&&\\ n=1\,|&-1	&&&&\\ n=2\,|&\hphantom{-}1	&\hphantom{-}2&&&\\ \vdots&-2	&-5&-6&&\\ &\hphantom{-}6	&\hphantom{-}21	&\hphantom{-}24	&\hphantom{-}24&\\ &-24	&-108	&-189	&-120	&-120 \end{matrix} $$ Now explicit calculations suggest that one can attach polynomial weights in $j$ and $n$ to the $b(j,n)$ such that the sum over any row vanishes . More precisely: Conjecture. For any $n\in\mathbb N$   $$ \sum_{j=0}^{n-1} b(j,n)\big( 32j^3-32(2n-1)j^2 +2(22n^2-30n+13)j-(n-1)(2n-1)(5n-6) \big)=0 $$ I believe this to be true, but so far I was not able to prove it. The problem with induction here is that when using (1) in the induction step, the weight gets $j^4$-terms which then can't be directly connected to (2) anymore. The obvious way would be trying to find a closed form for the $b(j,n)$ but that seems quite difficult and I was hoping there would be an easier, ""intrinsic"" (only using the recursion or other properties of the sequence) way of proving this. I'm aware of the generating functions ansatz but as the ""weights"" in (1) depend on $j,n$ this seems to not work directly either. Thanks in advance for any answer or comment! Edit: just to vizualize the problem, the first non-vanishing elements of the sequence in question $\scriptstyle\big(b(j,n)(32j^3-32(2n-1)j^2 +2(22n^2-30n+13)j-(n-1)(2n-1)(5n-6))\big)_{j\in\mathbb Z,n\in\mathbb N}$ are given by $$ \begin{matrix} &\underline{j=0}&\underline{j=1}&\ldots&&\\ n=1\,|&0	&&&&\\ n=2\,|&-12&\hphantom{-}12&&&\\ \vdots&\hphantom{-}180	&-120&	-60&&\\ &-1764	&\hphantom{-}84	&\hphantom{-}1104	&\hphantom{-}576&\\ &\hphantom{-}16416	&\hphantom{-}12312	&-13608	&-7920	&-7200 \end{matrix} $$ Now it is evident that any row here sums up to 0.",,"['real-analysis', 'sequences-and-series', 'analysis', 'recursion']"
96,Books to release our inner Ubermensch with calculus?,Books to release our inner Ubermensch with calculus?,,"I took calculus courses and have read some books about it. Most of them is similar: There are the definitions and some applications but not enough depth. For example, it is very common to find the problem of the box in calculus books: Having a sheet of paper of area $a^2$ , how can we fold it into a box in a way that maximizes the volume? This is a really interesting problem and I guess that the books could have a little bit more about it, in a way that almost suggests the study of optimization problems. The problem for me is that calculus by itself doesn't seems stimulating enough. But I have found several titles that make calculus more interesting. Take a look at this result from Chen's: Excursions in Classical Analysis. I think this is a very interesting and elegant result! One can have all these means as a function based on simple integrals and as a bonus, you also gain a very easy way to deduce inequalities between all them! I have also found Moll's: Numbers and Functions: From a classical-experimental mathematician’s point of view ; Moll/Boros: Irresistible Integrals: Symbolics, Analysis and Experiments in the Evaluation of Integrals ; Moll's books are filled with the study of polynomials, Riemann's $\zeta$ Function, Legendre polynomials, Chebyshev polynomials, Hermite polynomials, $\Gamma$ function, Logarithmic Integrals, etc. Kazarinoff's: Analytic inequalities ; Kazarinoff even makes an appeal in his book: Steele: The Cauchy-Schwarz Master Class: An Introduction to the Art of Mathematical Inequalities ; Gardiner's: Infinite processes , also sold by Dover as Understanding Infinite ; Gardiner's book is IMO an excellent choice to explain why we need a more rigorous analysis. Very soon in the book, he already presents some functions in which a naive usage of ideas in calculus can take one to hazardous consequences. Iosevich's: A View from the Top: Analysis, Combinatorics and Number Theory ; Spencer's: Asymptopia ; Shahriari's: Approximately Calculus . Shahriari's book is a real gem. Just take a look at its contents. It has a section on dynamical systems! Sasane's: The how and why of one variable calculus . What I call ""interesting"" here is the suggestion of using calculus to discover neat problems both in analysis and in others areas of mathematics or a decent and organic explanation of the whys. I felt that calculus was no stimulating because the derivative is basically ""a tool for finding lines tangent to functions"" and the integral is basically ""a tool for measuring the are under a curve"" , obviously: These are worthy, but where can we go from there? The question is, do you know more books that complement this list? Behind the scenes: The original title of the question was: ""Where to find books with interesting activities using calculus?"" but MSE suggested a change for a better title. I changed to this and the warning disappeared: If there is no warning, then this is a better title! (Also, I watched this today.)","I took calculus courses and have read some books about it. Most of them is similar: There are the definitions and some applications but not enough depth. For example, it is very common to find the problem of the box in calculus books: Having a sheet of paper of area , how can we fold it into a box in a way that maximizes the volume? This is a really interesting problem and I guess that the books could have a little bit more about it, in a way that almost suggests the study of optimization problems. The problem for me is that calculus by itself doesn't seems stimulating enough. But I have found several titles that make calculus more interesting. Take a look at this result from Chen's: Excursions in Classical Analysis. I think this is a very interesting and elegant result! One can have all these means as a function based on simple integrals and as a bonus, you also gain a very easy way to deduce inequalities between all them! I have also found Moll's: Numbers and Functions: From a classical-experimental mathematician’s point of view ; Moll/Boros: Irresistible Integrals: Symbolics, Analysis and Experiments in the Evaluation of Integrals ; Moll's books are filled with the study of polynomials, Riemann's Function, Legendre polynomials, Chebyshev polynomials, Hermite polynomials, function, Logarithmic Integrals, etc. Kazarinoff's: Analytic inequalities ; Kazarinoff even makes an appeal in his book: Steele: The Cauchy-Schwarz Master Class: An Introduction to the Art of Mathematical Inequalities ; Gardiner's: Infinite processes , also sold by Dover as Understanding Infinite ; Gardiner's book is IMO an excellent choice to explain why we need a more rigorous analysis. Very soon in the book, he already presents some functions in which a naive usage of ideas in calculus can take one to hazardous consequences. Iosevich's: A View from the Top: Analysis, Combinatorics and Number Theory ; Spencer's: Asymptopia ; Shahriari's: Approximately Calculus . Shahriari's book is a real gem. Just take a look at its contents. It has a section on dynamical systems! Sasane's: The how and why of one variable calculus . What I call ""interesting"" here is the suggestion of using calculus to discover neat problems both in analysis and in others areas of mathematics or a decent and organic explanation of the whys. I felt that calculus was no stimulating because the derivative is basically ""a tool for finding lines tangent to functions"" and the integral is basically ""a tool for measuring the are under a curve"" , obviously: These are worthy, but where can we go from there? The question is, do you know more books that complement this list? Behind the scenes: The original title of the question was: ""Where to find books with interesting activities using calculus?"" but MSE suggested a change for a better title. I changed to this and the warning disappeared: If there is no warning, then this is a better title! (Also, I watched this today.)",a^2 \zeta \Gamma,"['calculus', 'real-analysis', 'reference-request', 'book-recommendation']"
97,Why can't we interchange differentiation with taking a limit of a series of functions?,Why can't we interchange differentiation with taking a limit of a series of functions?,,"While learning a little Fourier analysis, I ran into this interesting phenomenon: Consider a series of sawtooth waves such that the height and width of the sawteeth shrinks to zero, but the slope of the sawteeth remains the same.  To be specific, let $$f_n(x) = \frac{nx - \lfloor nx\rfloor}{n}$$ Then define $$F(x) = \lim_{n\to\infty}f_n(x)$$ It seems intuitively clear that $F(x) = 0$ for all $x$ because the global maximum of $f_n$ is $\frac{1}{n}$. If $F(x) = 0$, then we should have $F'(x) = 0$ as well.  However, if we choose an irrational value of $x$, then $f'_n(x) = 1$ for all $n$, so if $F'(x)$ is found instead by taking $$F'(x) = \lim_{n\to\infty}f'_n(x)$$ we do not get $F'(x) = 0$. It seems like the derivative of a limit is not the same as the limit of a derivative, which is pretty counterintuitive to me. What's going on?","While learning a little Fourier analysis, I ran into this interesting phenomenon: Consider a series of sawtooth waves such that the height and width of the sawteeth shrinks to zero, but the slope of the sawteeth remains the same.  To be specific, let $$f_n(x) = \frac{nx - \lfloor nx\rfloor}{n}$$ Then define $$F(x) = \lim_{n\to\infty}f_n(x)$$ It seems intuitively clear that $F(x) = 0$ for all $x$ because the global maximum of $f_n$ is $\frac{1}{n}$. If $F(x) = 0$, then we should have $F'(x) = 0$ as well.  However, if we choose an irrational value of $x$, then $f'_n(x) = 1$ for all $n$, so if $F'(x)$ is found instead by taking $$F'(x) = \lim_{n\to\infty}f'_n(x)$$ we do not get $F'(x) = 0$. It seems like the derivative of a limit is not the same as the limit of a derivative, which is pretty counterintuitive to me. What's going on?",,"['real-analysis', 'limits']"
98,Inequality for Distribution of points in space,Inequality for Distribution of points in space,,"Consider the space $\mathbb{R}^2.$ We are given $n$ mutually different points $x_1,..,x_n \in \mathbb R^2.$ We can then introduce expressions $$f_i(x_1,...,x_n) =  n^2 \vert x_i \vert^2 + \sum_{j \neq i } \frac{1}{\vert x_j-x_i \vert^2 }.$$ I would like to know if there exists an explicit constant $c>0$ , independent of the points, (as large as possible) such that $$ n^2+ \sum_{i=1}^n f_i(x_1,...,x_n) \ge c n^3.$$ I will do two cases by hand to give you a feeling: $n=1$ : This case is clear, as $$1+ f_1(x_1) \ge 1 =c1^3$$ with $c=1.$ $n=2$ : This case is already more tricky, however $$4 + f_1(x_1,x_2) + f_2(x_1,x_2) =4+ 4(\vert x_1 \vert^2+\vert x_2 \vert^2) + \frac{2}{\vert x_1-x_2 \vert^2}$$ and thus using the parallelogram identity we find $$ 4 + f_1(x_1,x_2) + f_2(x_1,x_2)=4+ 2 (\vert x_1-x_2 \vert^2 + 1/\vert x_1-x_2 \vert^2) + 2 \vert x_1+x_2 \vert^2.$$ Now, we may use that $t+1/t  \ge 2$ for $t > 0$ to infer that $$ 4 + f_1(x_1,x_2) + f_2(x_1,x_2)\ge 4+ 4 = 2^3.$$ So somehow one could conjecture that $c=1$ is possible, but I don't know whether this is true in general.","Consider the space We are given mutually different points We can then introduce expressions I would like to know if there exists an explicit constant , independent of the points, (as large as possible) such that I will do two cases by hand to give you a feeling: : This case is clear, as with : This case is already more tricky, however and thus using the parallelogram identity we find Now, we may use that for to infer that So somehow one could conjecture that is possible, but I don't know whether this is true in general.","\mathbb{R}^2. n x_1,..,x_n \in \mathbb R^2. f_i(x_1,...,x_n) =  n^2 \vert x_i \vert^2 + \sum_{j \neq i } \frac{1}{\vert x_j-x_i \vert^2 }. c>0  n^2+ \sum_{i=1}^n f_i(x_1,...,x_n) \ge c n^3. n=1 1+ f_1(x_1) \ge 1 =c1^3 c=1. n=2 4 + f_1(x_1,x_2) + f_2(x_1,x_2) =4+ 4(\vert x_1 \vert^2+\vert x_2 \vert^2) + \frac{2}{\vert x_1-x_2 \vert^2}  4 + f_1(x_1,x_2) + f_2(x_1,x_2)=4+ 2 (\vert x_1-x_2 \vert^2 + 1/\vert x_1-x_2 \vert^2) + 2 \vert x_1+x_2 \vert^2. t+1/t  \ge 2 t > 0  4 + f_1(x_1,x_2) + f_2(x_1,x_2)\ge 4+ 4 = 2^3. c=1","['calculus', 'real-analysis']"
99,Intuition about the Bernstein polynomials proof of the Weierstrass approximation theorem,Intuition about the Bernstein polynomials proof of the Weierstrass approximation theorem,,"The Weierstrass approximation theorem can be stated as follows: Let $f\in C([a,b])$ . There exists a sequence $(p_n)_{n\in \mathbb{N}}$ of polynomials in $[a,b]$ such that $(p_n)$ converges uniformly to $f$ . One approach to prove this theorem is to notice that we just need to prove this for $[a,b]=[0,1]$ and then consider the Bernstein polynomials: $$B_n(f)(x)=\sum_{k=0}^n f\left(\dfrac{k}{n}\right)\binom{n}{k}x^k(1-x)^{n-k},$$ and then prove that $(B_n(f))$ converges uniformly to $f$ . The proof then indeed shows this convergence. My question here is regarding intuition. This is the kind of thing that I wonder how could anyone think of defining those polynomials so that they converge to $f$ . So: what is the intuition behind the Bernstein polynomials? Considering that we want to find a sequence of polynomials which converge uniformly to a continuous function, how could we ever think about defining those polynomials? Is there some intuition here?","The Weierstrass approximation theorem can be stated as follows: Let . There exists a sequence of polynomials in such that converges uniformly to . One approach to prove this theorem is to notice that we just need to prove this for and then consider the Bernstein polynomials: and then prove that converges uniformly to . The proof then indeed shows this convergence. My question here is regarding intuition. This is the kind of thing that I wonder how could anyone think of defining those polynomials so that they converge to . So: what is the intuition behind the Bernstein polynomials? Considering that we want to find a sequence of polynomials which converge uniformly to a continuous function, how could we ever think about defining those polynomials? Is there some intuition here?","f\in C([a,b]) (p_n)_{n\in \mathbb{N}} [a,b] (p_n) f [a,b]=[0,1] B_n(f)(x)=\sum_{k=0}^n f\left(\dfrac{k}{n}\right)\binom{n}{k}x^k(1-x)^{n-k}, (B_n(f)) f f","['real-analysis', 'sequences-and-series', 'functional-analysis', 'polynomials', 'uniform-convergence']"
