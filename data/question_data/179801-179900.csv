,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Why do we need a function to be defined in a neighbourhood around $a$ so that it's differentiable at $a$?,Why do we need a function to be defined in a neighbourhood around  so that it's differentiable at ?,a a,"Compare it to the definition of continuity of a function $f$ , if $x \in B(a,\delta)$ , then $f(x) \in B(f(a),\varepsilon)$ . This definition doesn't need $f$ to be defined around a neighbourhood around $a$ , in fact $a$ can be an isolated point in the domain of $f$ . (I just checked), when we talk about the (single-var case) derivative of $f$ at $x_0$ , we do not need $f$ to be defined in a neighbourhood around $x_0$ . $$ \lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0} = f'(x_0) \iff |x-x_0|< \delta \implies |\frac{f(x)-f(x_0)}{x-x_0} - f'(x_0)| < \varepsilon. $$ We just need that if $B(x_0,\delta)$ contains points, then the inequality above holds. However, in the $f:\mathbb{R}^p \to \mathbb{R}$ case, $f$ differentiable at $a \iff \exists l(x)$ linear function st. $f(x) = f(a) + l(x-a) + \varepsilon(x)\cdot|x-a|$ , where $\varepsilon(x) = 0$ if $x \to a$ . Why do we need $f$ to be defined in some neighbourhood here? The equivalent condition for differentiability of $f$ at $a$ is: $$ \lim_{x\to a} \frac{f(x)-f(a)-l(x-a)}{|x-a|} = 0 \iff \forall x \in B(a,\delta), |\frac{f(x)-f(a)-l(x-a)}{|x-a|}| < \kappa. $$ I don't understand why we need $f$ to be defined in some neighbourhood around $a$ .","Compare it to the definition of continuity of a function , if , then . This definition doesn't need to be defined around a neighbourhood around , in fact can be an isolated point in the domain of . (I just checked), when we talk about the (single-var case) derivative of at , we do not need to be defined in a neighbourhood around . We just need that if contains points, then the inequality above holds. However, in the case, differentiable at linear function st. , where if . Why do we need to be defined in some neighbourhood here? The equivalent condition for differentiability of at is: I don't understand why we need to be defined in some neighbourhood around .","f x \in B(a,\delta) f(x) \in B(f(a),\varepsilon) f a a f f x_0 f x_0 
\lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0} = f'(x_0) \iff |x-x_0|< \delta \implies |\frac{f(x)-f(x_0)}{x-x_0} - f'(x_0)| < \varepsilon.
 B(x_0,\delta) f:\mathbb{R}^p \to \mathbb{R} f a \iff \exists l(x) f(x) = f(a) + l(x-a) + \varepsilon(x)\cdot|x-a| \varepsilon(x) = 0 x \to a f f a 
\lim_{x\to a} \frac{f(x)-f(a)-l(x-a)}{|x-a|} = 0 \iff \forall x \in B(a,\delta), |\frac{f(x)-f(a)-l(x-a)}{|x-a|}| < \kappa.
 f a","['calculus', 'multivariable-calculus']"
1,"Multivariable chain rule for $f:\mathbb{R}^2\to\mathbb{R},\ f\ \ \mathcal{C}^2$ function",Multivariable chain rule for  function,"f:\mathbb{R}^2\to\mathbb{R},\ f\ \ \mathcal{C}^2","I am trying to solve the following problem. Suppose $f:\mathbb{R}^2\to\mathbb{R}$ is $\mathcal{C}^2.$ Let $F\begin{pmatrix}r\\\theta\end{pmatrix}=f\begin{pmatrix}r\cos\theta\\r\sin\theta\end{pmatrix}$ . Show that $\frac{\partial^2F}{\partial r^2}+\frac{1}{r}\frac{\partial F}{\partial r}+\frac{1}{r^2}\frac{\partial^2 F}{\partial \theta^2}=\frac{\partial^2 f}{\partial x^2}+\frac{\partial^2 f}{\partial y^2},$ here the lefthand side is evaluated at $\begin{bmatrix}r\\\theta\end{bmatrix}$ and the righthand is evaluated at $\begin{bmatrix}r\cos\theta\\r\sin\theta\end{bmatrix}.$ What I have done: $DF\begin{pmatrix}r\\\theta\end{pmatrix}=Df\left(\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}\right)D\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}=\left[\frac{\partial f}{\partial x}\left(\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}\right)\ \ \frac{\partial f}{\partial y}\left(\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}\right)\right] \begin{bmatrix}\cos\theta & -r\sin\theta\\ \sin\theta & r\cos\theta\end{bmatrix}=\begin{bmatrix}\frac{\partial f}{\partial x}\left(\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}\right)\cos\theta+\frac{\partial f}{\partial y}\left(\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}\right)\sin\theta & -\frac{\partial f}{\partial x}\left(\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}\right)r\sin\theta+\frac{\partial f}{\partial y}\left(\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}\right)r\cos\theta\end{bmatrix}=\begin{bmatrix}\frac{\partial F}{\partial r} & \frac{\partial F}{\partial\theta}\end{bmatrix}.$ It is not clear to me now how I should apply the chain rule to compute the left hand side terms like $\frac{\partial^2F}{\partial r^2}$ so I would appreciate an hint about how to do this.",I am trying to solve the following problem. Suppose is Let . Show that here the lefthand side is evaluated at and the righthand is evaluated at What I have done: It is not clear to me now how I should apply the chain rule to compute the left hand side terms like so I would appreciate an hint about how to do this.,"f:\mathbb{R}^2\to\mathbb{R} \mathcal{C}^2. F\begin{pmatrix}r\\\theta\end{pmatrix}=f\begin{pmatrix}r\cos\theta\\r\sin\theta\end{pmatrix} \frac{\partial^2F}{\partial r^2}+\frac{1}{r}\frac{\partial F}{\partial r}+\frac{1}{r^2}\frac{\partial^2 F}{\partial \theta^2}=\frac{\partial^2 f}{\partial x^2}+\frac{\partial^2 f}{\partial y^2}, \begin{bmatrix}r\\\theta\end{bmatrix} \begin{bmatrix}r\cos\theta\\r\sin\theta\end{bmatrix}. DF\begin{pmatrix}r\\\theta\end{pmatrix}=Df\left(\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}\right)D\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}=\left[\frac{\partial f}{\partial x}\left(\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}\right)\ \ \frac{\partial f}{\partial y}\left(\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}\right)\right] \begin{bmatrix}\cos\theta & -r\sin\theta\\ \sin\theta & r\cos\theta\end{bmatrix}=\begin{bmatrix}\frac{\partial f}{\partial x}\left(\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}\right)\cos\theta+\frac{\partial f}{\partial y}\left(\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}\right)\sin\theta & -\frac{\partial f}{\partial x}\left(\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}\right)r\sin\theta+\frac{\partial f}{\partial y}\left(\vec{g}\begin{pmatrix}r\\\theta\end{pmatrix}\right)r\cos\theta\end{bmatrix}=\begin{bmatrix}\frac{\partial F}{\partial r} & \frac{\partial F}{\partial\theta}\end{bmatrix}. \frac{\partial^2F}{\partial r^2}","['multivariable-calculus', 'derivatives', 'chain-rule']"
2,Any derivation map is a linear combination of directional derivatives,Any derivation map is a linear combination of directional derivatives,,"Fix a point $\mathbf p\in \mathbb R^n$ . Given $\mathbf v\in \mathbb R^n$ , define the directional derivative $\nabla_{\mathbf v}$ of a function $f$ at the point $\mathbf p$ as $$\nabla_{\mathbf v}f(\mathbf p)=\lim_{t\to 0}\frac{f(\mathbf p+t\mathbf v)-f(\mathbf p)}t$$ Note that this is a map $\nabla_{\mathbf v}:C^\infty(\mathbb R^n)\to C^\infty(\mathbb R^n)$ where $C^\infty(\mathbb R^n)$ is the set of all real valued smooth functions on $\mathbb R^n$ . Now, let us have a map $D:C^\infty(\mathbb R^n)\to C^\infty(\mathbb R^n)$ that satisfies the following properties- $D(f+g)(\mathbf p)=D(f)(\mathbf p)+D(g)(\mathbf p)$ $D(cf)(\mathbf p)=cD(f)(\mathbf p)$ $D(fg)(\mathbf p)=fD(g)(\mathbf p)+gD(f)(\mathbf p)$ Such a map is called a derivation at $\mathbf p$ . Prove that any such derivation maps can be written as a linear combination of $\{\nabla_{\mathbf e_i}\}_{i=1}^n$ . I have proved that the directional derivative map $\nabla_{\mathbf v}$ satisfies properties (1),(2) and (3). Using that, we can also very easily show that any linear combination of $\{\nabla_{\mathbf e_i}\}_{i=1}^n$ also satisfies properties (1),(2) and (3). As a part of a previous exercise (which may be of use in this part), I have proved $$\nabla_{\mathbf {v}+\mathbf{w}}=\nabla_{\mathbf v}\\\nabla_{c\mathbf v}=c\nabla_{\mathbf v}$$ But, I have no idea how to show that any derivation is a linear combination of directional derivatives.","Fix a point . Given , define the directional derivative of a function at the point as Note that this is a map where is the set of all real valued smooth functions on . Now, let us have a map that satisfies the following properties- Such a map is called a derivation at . Prove that any such derivation maps can be written as a linear combination of . I have proved that the directional derivative map satisfies properties (1),(2) and (3). Using that, we can also very easily show that any linear combination of also satisfies properties (1),(2) and (3). As a part of a previous exercise (which may be of use in this part), I have proved But, I have no idea how to show that any derivation is a linear combination of directional derivatives.",\mathbf p\in \mathbb R^n \mathbf v\in \mathbb R^n \nabla_{\mathbf v} f \mathbf p \nabla_{\mathbf v}f(\mathbf p)=\lim_{t\to 0}\frac{f(\mathbf p+t\mathbf v)-f(\mathbf p)}t \nabla_{\mathbf v}:C^\infty(\mathbb R^n)\to C^\infty(\mathbb R^n) C^\infty(\mathbb R^n) \mathbb R^n D:C^\infty(\mathbb R^n)\to C^\infty(\mathbb R^n) D(f+g)(\mathbf p)=D(f)(\mathbf p)+D(g)(\mathbf p) D(cf)(\mathbf p)=cD(f)(\mathbf p) D(fg)(\mathbf p)=fD(g)(\mathbf p)+gD(f)(\mathbf p) \mathbf p \{\nabla_{\mathbf e_i}\}_{i=1}^n \nabla_{\mathbf v} \{\nabla_{\mathbf e_i}\}_{i=1}^n \nabla_{\mathbf {v}+\mathbf{w}}=\nabla_{\mathbf v}\\\nabla_{c\mathbf v}=c\nabla_{\mathbf v},"['calculus', 'geometry', 'multivariable-calculus', 'derivatives', 'differential-geometry']"
3,Estimating the length of a curve from below by a bound on its curvature.,Estimating the length of a curve from below by a bound on its curvature.,,"I got stuck on a problem. Let $\alpha(s)$ be smooth simple closed curve  with curvature $k(s)$ and $0 <k(s) \leq c $ . Then show that the length of the curve is limited from below by $2\pi \frac{1}{c}$ . I can't formalize the solution very well: in a neighborhood of each point $\alpha(s)$ the curve $\alpha$ has length very near to the length of the osculating circle tangent at $\alpha(s),$ and the osculating circle has radius at least $\frac{1}{c},$ then we should be able to conclude by summing up all the local contributions.",I got stuck on a problem. Let be smooth simple closed curve  with curvature and . Then show that the length of the curve is limited from below by . I can't formalize the solution very well: in a neighborhood of each point the curve has length very near to the length of the osculating circle tangent at and the osculating circle has radius at least then we should be able to conclude by summing up all the local contributions.,"\alpha(s) k(s) 0 <k(s) \leq c  2\pi \frac{1}{c} \alpha(s) \alpha \alpha(s), \frac{1}{c},","['multivariable-calculus', 'differential-geometry', 'curves', 'curvature']"
4,Finding the derivative of $f(x)=x/||x||$ for $x \in \mathbb{R}^3$ [duplicate],Finding the derivative of  for  [duplicate],f(x)=x/||x|| x \in \mathbb{R}^3,"This question already has an answer here : Find the gradient of $f:\Bbb R^m\setminus\{0\}\to\Bbb R,\; x\mapsto |x|$ (1 answer) Closed 4 months ago . I'm trying to find the derivative of the function $f:\mathbb{R}^3 \to \mathbb{R}^3$ given by $f(x)=x/||x||$ for $x \in \mathbb{R}^3 \setminus \{0\}$ and $f(0)=0$ . I've actually already been given the following expression $$f'(x)(h)=\frac{h}{||x||} - \frac{\langle x,h \rangle x}{||x||^3}$$ but cannot find a nice way to prove that this is in fact the derivative of $f$ on $\mathbb{R}^3 \setminus \{0\}$ . What I've tried is to use the definition $$f(x+h)=f(x) + T(h) + ||h|| \varepsilon(h)$$ where $T(h)$ is a linear map that ends up being the derivative if it exists, and $\varepsilon(0)=0$ and is continuous at $0$ . Plugging the function and the given derivative in, it seems you have to show that for $x,h \in \mathbb{R}^3$ , $$\lim_{h \to 0} \frac{1}{||h||} \left(\frac{x+h}{||x+h||} - \frac{(x+h)}{||x||} + \frac{\langle x,h \rangle x}{||x||^3} \right)=0$$ and I can't see a nice way of showing this. I don't know if doing this is the intended way or if there is a much easier way. I've tried looking at the components of $f$ since $f$ is differentiable at $a$ if and only if its component functions are also differentiable at $a$ , but those expressions aren't any nicer than the one above. How do you show that the given function is the derivative of $f$ ?","This question already has an answer here : Find the gradient of $f:\Bbb R^m\setminus\{0\}\to\Bbb R,\; x\mapsto |x|$ (1 answer) Closed 4 months ago . I'm trying to find the derivative of the function given by for and . I've actually already been given the following expression but cannot find a nice way to prove that this is in fact the derivative of on . What I've tried is to use the definition where is a linear map that ends up being the derivative if it exists, and and is continuous at . Plugging the function and the given derivative in, it seems you have to show that for , and I can't see a nice way of showing this. I don't know if doing this is the intended way or if there is a much easier way. I've tried looking at the components of since is differentiable at if and only if its component functions are also differentiable at , but those expressions aren't any nicer than the one above. How do you show that the given function is the derivative of ?","f:\mathbb{R}^3 \to \mathbb{R}^3 f(x)=x/||x|| x \in \mathbb{R}^3 \setminus \{0\} f(0)=0 f'(x)(h)=\frac{h}{||x||} - \frac{\langle x,h \rangle x}{||x||^3} f \mathbb{R}^3 \setminus \{0\} f(x+h)=f(x) + T(h) + ||h|| \varepsilon(h) T(h) \varepsilon(0)=0 0 x,h \in \mathbb{R}^3 \lim_{h \to 0} \frac{1}{||h||} \left(\frac{x+h}{||x+h||} - \frac{(x+h)}{||x||} + \frac{\langle x,h \rangle x}{||x||^3} \right)=0 f f a a f","['real-analysis', 'multivariable-calculus']"
5,Tangential planes on a surface at the points of intersection with the sphere,Tangential planes on a surface at the points of intersection with the sphere,,"I was looking at a question in an old exam at it happens that someone has already asked it. However, I have one question regarding the formulation of the answer: Prove that tangent planes to the surface $a(xy+yz+xz)=xyz,a>0$ at the points of intersection with the sphere $x^2+y^2+z^2=r^2$ cut off segments on the coordinate axes whose sum is constant. In the accepted answer, it is suggested to rewrite the equation of the surface as $\frac1x+\frac1y+\frac1z=\frac1a$ in order to use $f(x,y,z)=\frac1x+\frac1y+\frac1z-\frac1a$ and get $\nabla f(x,y,z)=-\left(\frac1{x^2},\frac1{y^2},\frac1{z^2}\right)$ and the equation of the tangent plane at a point of intersection $(x_0,y_0,z_0)$ with the sphere is: $$\frac1{x_0^2}(x-x_0)+\frac1{y_0^2}(y-y_0)+\frac1{z_0^2}(z-z_0)=0.$$ It follows that the lengths of the segments are $$x=x_0^2\frac{x_0y_0+y_0z_0+x_0z_0}{x_0y_0z_0}\\y=y_0^2\frac{x_0y_0+y_0z_0+x_0z_0}{x_0y_0z_0}\\z=z_0^2\frac{x_0y_0+y_0z_0+x_0z_0}{x_0y_0z_0}$$ and $$\begin{aligned}x+y+z&=(x_0^2+y_0^2+z_0^2)\frac{x_0y_0+y_0z_0+x_0z_0}{x_0y_0z_0}\\&=\frac{r^2}a\end{aligned}$$ However, there are intersection points on the axes, so, $f(x,y,z)=a(xy+yz+xz)-xyz$ gives $\nabla f(x,y,z)=(ay+az-xz,ax+az-xz,ax+ay-xy).$ If $$(x_0,y_0,z_0)=(\pm r,0,0), \nabla f(x_0,y_0,z_0)=(0,\pm ar,\pm ar),$$ the tangential plane is parallel to the $x$ axis, moreover, the $x-$ axis lies in the plane. How is then the length of a segment on the axis defined? Should we isolate this as a special case?","I was looking at a question in an old exam at it happens that someone has already asked it. However, I have one question regarding the formulation of the answer: Prove that tangent planes to the surface at the points of intersection with the sphere cut off segments on the coordinate axes whose sum is constant. In the accepted answer, it is suggested to rewrite the equation of the surface as in order to use and get and the equation of the tangent plane at a point of intersection with the sphere is: It follows that the lengths of the segments are and However, there are intersection points on the axes, so, gives If the tangential plane is parallel to the axis, moreover, the axis lies in the plane. How is then the length of a segment on the axis defined? Should we isolate this as a special case?","a(xy+yz+xz)=xyz,a>0 x^2+y^2+z^2=r^2 \frac1x+\frac1y+\frac1z=\frac1a f(x,y,z)=\frac1x+\frac1y+\frac1z-\frac1a \nabla f(x,y,z)=-\left(\frac1{x^2},\frac1{y^2},\frac1{z^2}\right) (x_0,y_0,z_0) \frac1{x_0^2}(x-x_0)+\frac1{y_0^2}(y-y_0)+\frac1{z_0^2}(z-z_0)=0. x=x_0^2\frac{x_0y_0+y_0z_0+x_0z_0}{x_0y_0z_0}\\y=y_0^2\frac{x_0y_0+y_0z_0+x_0z_0}{x_0y_0z_0}\\z=z_0^2\frac{x_0y_0+y_0z_0+x_0z_0}{x_0y_0z_0} \begin{aligned}x+y+z&=(x_0^2+y_0^2+z_0^2)\frac{x_0y_0+y_0z_0+x_0z_0}{x_0y_0z_0}\\&=\frac{r^2}a\end{aligned} f(x,y,z)=a(xy+yz+xz)-xyz \nabla f(x,y,z)=(ay+az-xz,ax+az-xz,ax+ay-xy). (x_0,y_0,z_0)=(\pm r,0,0), \nabla f(x_0,y_0,z_0)=(0,\pm ar,\pm ar), x x-","['multivariable-calculus', 'tangent-spaces']"
6,How to calculate the volume of this set using spherical coordinates?,How to calculate the volume of this set using spherical coordinates?,,"Let $$K = \left\{(x, y, z) \in \mathbb R^3 ; \; x^2 + y^2 + z^2 < b^2, \, x^2 + y^2 > a^2 \right\}.$$ How to calculate the volume of $K$ using spherical coordinates? I know that the conditions translate to $r < b$ and $r \sin \theta > a$ but I really struggle to get the right integral. Could someone explain, please?","Let How to calculate the volume of using spherical coordinates? I know that the conditions translate to and but I really struggle to get the right integral. Could someone explain, please?","K = \left\{(x, y, z) \in \mathbb R^3 ; \; x^2 + y^2 + z^2 < b^2, \, x^2 + y^2 > a^2 \right\}. K r < b r \sin \theta > a","['real-analysis', 'integration', 'multivariable-calculus', 'volume', 'spherical-coordinates']"
7,Construction of derivative as ratio of determinants related to simultaneous system of implicit functions,Construction of derivative as ratio of determinants related to simultaneous system of implicit functions,,This is Example C in section 1.1 from Advanced Calculus (2nd ed) by David Widder. $$\begin{cases} v + \log u = xy \\ u + \log v = x - y \end{cases}$$ $$\begin{cases} \frac{1}{u} \frac{\partial u}{\partial x} + \frac{\partial v}{\partial x} = y \\ \frac{1}{v} \frac{\partial v}{\partial x} + \frac{\partial u}{\partial x} = 1 \end{cases}$$ $$\frac{\partial u}{\partial v} = \frac{\begin{vmatrix} yu & u \\ v & 1 \end{vmatrix}}{\begin{vmatrix} 1 & u \\ v & 1 \end{vmatrix}} = \frac{u(y-v)}{1-uv}$$ How does the author come up with the ratio of determinants in the third equation?,This is Example C in section 1.1 from Advanced Calculus (2nd ed) by David Widder. How does the author come up with the ratio of determinants in the third equation?,\begin{cases} v + \log u = xy \\ u + \log v = x - y \end{cases} \begin{cases} \frac{1}{u} \frac{\partial u}{\partial x} + \frac{\partial v}{\partial x} = y \\ \frac{1}{v} \frac{\partial v}{\partial x} + \frac{\partial u}{\partial x} = 1 \end{cases} \frac{\partial u}{\partial v} = \frac{\begin{vmatrix} yu & u \\ v & 1 \end{vmatrix}}{\begin{vmatrix} 1 & u \\ v & 1 \end{vmatrix}} = \frac{u(y-v)}{1-uv},"['multivariable-calculus', 'systems-of-equations', 'partial-derivative', 'determinant', 'implicit-differentiation']"
8,How to proof that smooth function vanishing on xy-coordinates cross must be of form $xyg$?,How to proof that smooth function vanishing on xy-coordinates cross must be of form ?,xyg,"I am reading Jet Nestruev's Smooth Manifolds and Observables book and I am struggling with the exercise 2.12. It states that any smooth function $f:\mathbb{R}^2\to\mathbb{R}$ vanishing on coordinate cross $K=\{x=0\}\cup\{y=0\}$ must be of form $f(x,y)=xyg(x,y)$ for some smooth function $g:\mathbb{R}^2\to\mathbb{R}$ . The exercise is states just after Hadamard's lemma , so I guess I should use it or related ideas somehow. EDIT. I just managed to prove that $$f(x,y)=x\int_0^1\frac{\partial f}{\partial x}(tx, y)dt \\ f(x,y)=y\int_0^1\frac{\partial f}{\partial y}(x, ty)dt$$ Thus $f(x,y)=xg_1(x,y)=yg_2(x,y)$ . However, I now stuck there. EDIT 2. Ok, I guess I know the answer. Since $f$ vanishes on the cross $K$ , $\frac{\partial f}{\partial x}$ vanishes on $x$ -axis. Thus we can apply the same argument as previously and obtain that $$f(x,y)=xy\int_0^1\int_0^1\frac{\partial^2f}{\partial x\partial y}(tx, sy)dsdt$$","I am reading Jet Nestruev's Smooth Manifolds and Observables book and I am struggling with the exercise 2.12. It states that any smooth function vanishing on coordinate cross must be of form for some smooth function . The exercise is states just after Hadamard's lemma , so I guess I should use it or related ideas somehow. EDIT. I just managed to prove that Thus . However, I now stuck there. EDIT 2. Ok, I guess I know the answer. Since vanishes on the cross , vanishes on -axis. Thus we can apply the same argument as previously and obtain that","f:\mathbb{R}^2\to\mathbb{R} K=\{x=0\}\cup\{y=0\} f(x,y)=xyg(x,y) g:\mathbb{R}^2\to\mathbb{R} f(x,y)=x\int_0^1\frac{\partial f}{\partial x}(tx, y)dt \\
f(x,y)=y\int_0^1\frac{\partial f}{\partial y}(x, ty)dt f(x,y)=xg_1(x,y)=yg_2(x,y) f K \frac{\partial f}{\partial x} x f(x,y)=xy\int_0^1\int_0^1\frac{\partial^2f}{\partial x\partial y}(tx, sy)dsdt","['real-analysis', 'multivariable-calculus', 'taylor-expansion', 'smooth-functions']"
9,Is $\limsup_{\epsilon \to 0^+}\frac{1}{\epsilon}\sup_{y \in B(x; \epsilon)}|f(y)-f(x)|$ related to any standard notion of gradient?,Is  related to any standard notion of gradient?,\limsup_{\epsilon \to 0^+}\frac{1}{\epsilon}\sup_{y \in B(x; \epsilon)}|f(y)-f(x)|,"Let $f:\mathbb R^d \to \mathbb R$ be a function and fix scalar $\epsilon > 0$ . Given a pint $x \in \mathbb R^d$ , define $$ \Delta_f(x;\epsilon) := \sup_{y \in B(x; \epsilon)}|f(y)-f(x)|, $$ where $B(x;\epsilon):=\{y \in \mathbb R^d \mid \|y-x\| \le \epsilon\}$ . Finally, define $\partial^{-} f(x) := \liminf_{\epsilon \to 0^+}\frac{\Delta_f(x;\epsilon)}{\epsilon}$ and $\partial^+ f(x) :=\limsup_{\epsilon \to 0^+}\frac{\Delta_f(x;\epsilon)}{\epsilon}$ Question 1. Is there a way to link the quantities $\partial^{\pm} f(x)$ with any known notion of derivative ? The strong slope of $f$ at $x$ , denoted $|\partial^+ | f(x)$ is defined by $$ |\partial^+| f(x) := \limsup_{y \to x}\frac{(f(y)-f(x))_+}{\|y-x\|}, $$ where $(t)_+ := \max(t,0)$ . This notion of gradient is common in gradient-flow literature, and can be defined for functions on arbitrary metric spaces (e.g spaces of probability measures). Question 2. Is $|\partial^+| f(x)$ related in any way to $\partial^{\pm} f(x)$ ?","Let be a function and fix scalar . Given a pint , define where . Finally, define and Question 1. Is there a way to link the quantities with any known notion of derivative ? The strong slope of at , denoted is defined by where . This notion of gradient is common in gradient-flow literature, and can be defined for functions on arbitrary metric spaces (e.g spaces of probability measures). Question 2. Is related in any way to ?","f:\mathbb R^d \to \mathbb R \epsilon > 0 x \in \mathbb R^d 
\Delta_f(x;\epsilon) := \sup_{y \in B(x; \epsilon)}|f(y)-f(x)|,
 B(x;\epsilon):=\{y \in \mathbb R^d \mid \|y-x\| \le \epsilon\} \partial^{-} f(x) := \liminf_{\epsilon \to 0^+}\frac{\Delta_f(x;\epsilon)}{\epsilon} \partial^+ f(x) :=\limsup_{\epsilon \to 0^+}\frac{\Delta_f(x;\epsilon)}{\epsilon} \partial^{\pm} f(x) f x |\partial^+ | f(x) 
|\partial^+| f(x) := \limsup_{y \to x}\frac{(f(y)-f(x))_+}{\|y-x\|},
 (t)_+ := \max(t,0) |\partial^+| f(x) \partial^{\pm} f(x)","['analysis', 'multivariable-calculus', 'derivatives', 'nonlinear-optimization', 'gradient-flows']"
10,Fundamental theorem of double integral,Fundamental theorem of double integral,,"Suppose $R$ is a ""reasonable"" region of $\mathbb{R}^2$ and $f(x,y)$ is a continuous function on $R$ . Then the double integral $\int\int_R f(x,y) dA$ exists (this is the usual limit of double sum as $\max{\Delta x_i}, \max{\Delta y_i} \to 0$ , in particular, the order of limit does not matter). Now suppose, in addition, that the region $R = \{(x,y) | a \le x \le b, g_1(x) \le y \le g_2(x) \}$ where $g_1(x), g_2(x)$ are differentiable functions when $x \in [a,b]$ . Then the double integral $\int \int_R f(x,y)dA = \int_{a}^{b} \int_{g_1(x)}^{g_2(x)} f(x,y) dydx$ . Is this theorem statement correct?","Suppose is a ""reasonable"" region of and is a continuous function on . Then the double integral exists (this is the usual limit of double sum as , in particular, the order of limit does not matter). Now suppose, in addition, that the region where are differentiable functions when . Then the double integral . Is this theorem statement correct?","R \mathbb{R}^2 f(x,y) R \int\int_R f(x,y) dA \max{\Delta x_i}, \max{\Delta y_i} \to 0 R = \{(x,y) | a \le x \le b, g_1(x) \le y \le g_2(x) \} g_1(x), g_2(x) x \in [a,b] \int \int_R f(x,y)dA = \int_{a}^{b} \int_{g_1(x)}^{g_2(x)} f(x,y) dydx","['real-analysis', 'analysis', 'multivariable-calculus']"
11,"Is it really in ""polar coordinates"" if there are no $\hat{r}$ and $\hat{\phi}$?","Is it really in ""polar coordinates"" if there are no  and ?",\hat{r} \hat{\phi},"Context. Let's say that I am asked to write a $d\vec{r}$ vector along a circular path. I will sometimes hear my classmates say, ""We should use polar coordinates,"" by which they usually mean $$\tag1 d\vec{r} = \langle -r \sin \phi \ d\phi , r \cos \phi \ d\phi \rangle,$$ as opposed to rectangular coordinates, meaning $$\tag2 d\vec{r} =\left\langle dx, \frac{-x \ dx}{\sqrt{r^2 - x^2}} \right\rangle.$$ But I guess I would be tempted to say that (1) is still in rectangular coordinates since $d\vec{r} = \langle -r \sin \phi \ d\phi , r \cos \phi \ d\phi \rangle$ implies $d\vec{r} = (-r \sin \phi \ d\phi) \hat{x} + (r \cos \phi \ d\phi) \hat{y}$ , which is still written in the $\{\hat{x}, \hat{y}\}$ basis. And that to truly write this $d\vec{r}$ in polar coordinates would be to write $$\tag3 d\vec{r} = 0 \hat{r} + r d\phi \ \hat{\phi},$$ i.e. written in the $\{\hat{r}, \hat{\phi}\}$ basis. (...or maybe is it now a coordinate frame? I'm not sure.) Question. I guess my question is: of (1), (2), and (3), what would each representation be called? Which is in ""polar coordinates""? Thanks so much!","Context. Let's say that I am asked to write a vector along a circular path. I will sometimes hear my classmates say, ""We should use polar coordinates,"" by which they usually mean as opposed to rectangular coordinates, meaning But I guess I would be tempted to say that (1) is still in rectangular coordinates since implies , which is still written in the basis. And that to truly write this in polar coordinates would be to write i.e. written in the basis. (...or maybe is it now a coordinate frame? I'm not sure.) Question. I guess my question is: of (1), (2), and (3), what would each representation be called? Which is in ""polar coordinates""? Thanks so much!","d\vec{r} \tag1 d\vec{r} = \langle -r \sin \phi \ d\phi , r \cos \phi \ d\phi \rangle, \tag2 d\vec{r} =\left\langle dx, \frac{-x \ dx}{\sqrt{r^2 - x^2}} \right\rangle. d\vec{r} = \langle -r \sin \phi \ d\phi , r \cos \phi \ d\phi \rangle d\vec{r} = (-r \sin \phi \ d\phi) \hat{x} + (r \cos \phi \ d\phi) \hat{y} \{\hat{x}, \hat{y}\} d\vec{r} \tag3 d\vec{r} = 0 \hat{r} + r d\phi \ \hat{\phi}, \{\hat{r}, \hat{\phi}\}","['multivariable-calculus', 'vector-analysis', 'polar-coordinates']"
12,Inequality involving Lipschitz derivative and Taylor's theorem,Inequality involving Lipschitz derivative and Taylor's theorem,,"I have come across an inequality that is supposed to follow from Taylor's theorem and I thought it was obvious until I realized I had the incorrect statement for the multi-variable version written down. How does the statement below follow from Taylors theorem? Let $f\in C^1(\mathbb{R}^n)$ and $|\nabla f(x) - \nabla f(y)| \leq L |x-y|$ , then by Taylor's theorem $$ f(x+h) \leq f(x) + \nabla f(x)\cdot h + \frac{L}{2}\Vert h\Vert^2 $$ Though we know the second derivative exists almost everywhere it doesn't seem like we can use it in Taylors theorem. Maybe a bound for the Peano remainder would work? Any thoughts?","I have come across an inequality that is supposed to follow from Taylor's theorem and I thought it was obvious until I realized I had the incorrect statement for the multi-variable version written down. How does the statement below follow from Taylors theorem? Let and , then by Taylor's theorem Though we know the second derivative exists almost everywhere it doesn't seem like we can use it in Taylors theorem. Maybe a bound for the Peano remainder would work? Any thoughts?","f\in C^1(\mathbb{R}^n) |\nabla f(x) - \nabla f(y)| \leq L |x-y| 
f(x+h) \leq f(x) + \nabla f(x)\cdot h + \frac{L}{2}\Vert h\Vert^2
","['real-analysis', 'multivariable-calculus']"
13,Compute the following integral: $\int_D \ln(x^2+y^2)dxdy$,Compute the following integral:,\int_D \ln(x^2+y^2)dxdy,"Compute the following integral: $\int_D \ln(x^2+y^2)dxdy$ where $D$ is the disc $x^2+y^2 \le1$ in $R$ into polar coordinates. What I have tried: $r^2\le1 \implies r\le1$ we also have $0 \le\theta\le2\pi$ This produces $$\int_0^{2\pi} d\theta\int_0^1\ln(r)\cdot r dr \implies 2\pi\int \ln(r)\cdot r dr$$ Taking the integral by parts to tackle the $dr$ integral we have $du = \frac{1}{r}$ and $v = \frac{r^2}{2}$ Which gives $$2\pi \left[\frac{\ln(r)r^2}{2}\Biggr|_0^1-\frac{1}{2}\int_0^1 rdr\right]\implies-\frac{2\pi}{4}$$ However, the right answer should be $-\pi$ any idea where I went wrong? Also, the actual question asks me to let "" $D_\delta$ be the annulus $\delta^2\le x^2+y^2\le1$ as I'm supposed to show that this integral has a finite limit which is $-\pi$ although I'm unsure on how to proceed with this and would really appreciate the communities support.","Compute the following integral: where is the disc in into polar coordinates. What I have tried: we also have This produces Taking the integral by parts to tackle the integral we have and Which gives However, the right answer should be any idea where I went wrong? Also, the actual question asks me to let "" be the annulus as I'm supposed to show that this integral has a finite limit which is although I'm unsure on how to proceed with this and would really appreciate the communities support.",\int_D \ln(x^2+y^2)dxdy D x^2+y^2 \le1 R r^2\le1 \implies r\le1 0 \le\theta\le2\pi \int_0^{2\pi} d\theta\int_0^1\ln(r)\cdot r dr \implies 2\pi\int \ln(r)\cdot r dr dr du = \frac{1}{r} v = \frac{r^2}{2} 2\pi \left[\frac{\ln(r)r^2}{2}\Biggr|_0^1-\frac{1}{2}\int_0^1 rdr\right]\implies-\frac{2\pi}{4} -\pi D_\delta \delta^2\le x^2+y^2\le1 -\pi,"['multivariable-calculus', 'multiple-integral']"
14,Understanding the proof of the Implicit Mapping Theorem,Understanding the proof of the Implicit Mapping Theorem,,"I am following Advanced Calculus of Several Variables by C.H. Edwards, Jr. I failed to build the logic of the theorem III- $3.4$ stated below, Theorem $3.4$ : Let the mapping $G: \mathscr{R}^{m+n} \rightarrow \mathscr{R}^{n}$ be $\mathscr{C}^{1}$ in a neighborhood of the point $(a,b)$ where $G(a,b)=0$ . If the partial derivative matrix $D_{2} G(a, b)$ is nonsingular,  then there exists a neighborhood $U$ of $a$ in $\mathscr{R}^{m}$ , a neighborhood $W$ of $(a, b)$ in $\mathscr{R}^{m+n}$ , and a $\mathscr{C}^{1}$ mapping $h: U \rightarrow \mathscr{R}^{n}$ , such that $y=h(x)$ solves the equation $G(x, y)=0$ in $W$ . In particular, the implicity defined mapping $h$ is the limit of the sequence of successive approximations defined inductively by, $$ \begin{aligned} &\qquad h_{0}(\mathbf{x})=\mathbf{b}, \quad h_{k+1}(\mathbf{x})=h_{k}(\mathbf{x})-D_{2} G(\mathbf{a}, \mathbf{b})^{-1} G\left(\mathbf{x}, h_{k}(\mathbf{x})\right) \end{aligned} $$ for $\mathbf{x} \in U$ . Theorem $3.3$ : Suppose that the mapping $f:\mathscr{R}^n\rightarrow\mathscr{R}^n$ is $\mathscr{C}^1$ in a neighborhood $W$ of the point $a$ , with the matrix $f'(a)\neq 0$ then $f$ is locally invertible - there exist neighborhoods $U\subset W$ of $a$ and $V$ of $b=f(a)$ and a one-to-one $\mathscr{C}^1$ mapping $g:V\rightarrow W$ such that $$g(f(x))=x\quad\text{for } x \in U,$$ $$f(g(y))=y\quad\text{for } y \in W$$ In particular, the local inverse $g$ is the limit of the sequence $\{g_k\}_{k=0}^\infty$ of successive approximations defined inductively by $$g_0(y)=a,\quad g_{k+1}(y)=g_k(y)-f'(a)^{-1}[f(g_k(y))-y]$$ Question $1$ : What I understand, inverse function theorem use implicit function theorem to guarantee there exist a relationship (function) of $y$ in term of $x$ (not explicitly). But the iterative form doesn't make sense to me. Like ""Why applying inverse Jocobian $(f'(a)^{-1})$ on $[f(g_k(y))-y]$ we get better and better approximation of $g(y)?$ "". Because What I know is, ""Jacobian approximate $f$ locally by a linear transformation"". Then what information encoded in the $f'(a)^{-1}$ for theorem $3.3$ and $D_{2} G(\mathbf{a}, \mathbf{b})^{-1}$ for theorem $3.4$ ? Question $2$ : What's the main difference/motivation/intuition between these two theorems? Maybe I am asking too many question for a single thread but as they are related to each other and pointed to understand only a single theorem, that's why I am put them all together. It will be great help if anyone explain those question.","I am following Advanced Calculus of Several Variables by C.H. Edwards, Jr. I failed to build the logic of the theorem III- stated below, Theorem : Let the mapping be in a neighborhood of the point where . If the partial derivative matrix is nonsingular,  then there exists a neighborhood of in , a neighborhood of in , and a mapping , such that solves the equation in . In particular, the implicity defined mapping is the limit of the sequence of successive approximations defined inductively by, for . Theorem : Suppose that the mapping is in a neighborhood of the point , with the matrix then is locally invertible - there exist neighborhoods of and of and a one-to-one mapping such that In particular, the local inverse is the limit of the sequence of successive approximations defined inductively by Question : What I understand, inverse function theorem use implicit function theorem to guarantee there exist a relationship (function) of in term of (not explicitly). But the iterative form doesn't make sense to me. Like ""Why applying inverse Jocobian on we get better and better approximation of "". Because What I know is, ""Jacobian approximate locally by a linear transformation"". Then what information encoded in the for theorem and for theorem ? Question : What's the main difference/motivation/intuition between these two theorems? Maybe I am asking too many question for a single thread but as they are related to each other and pointed to understand only a single theorem, that's why I am put them all together. It will be great help if anyone explain those question.","3.4 3.4 G: \mathscr{R}^{m+n} \rightarrow \mathscr{R}^{n} \mathscr{C}^{1} (a,b) G(a,b)=0 D_{2} G(a, b) U a \mathscr{R}^{m} W (a, b) \mathscr{R}^{m+n} \mathscr{C}^{1} h: U \rightarrow \mathscr{R}^{n} y=h(x) G(x, y)=0 W h 
\begin{aligned}
&\qquad h_{0}(\mathbf{x})=\mathbf{b}, \quad h_{k+1}(\mathbf{x})=h_{k}(\mathbf{x})-D_{2} G(\mathbf{a}, \mathbf{b})^{-1} G\left(\mathbf{x}, h_{k}(\mathbf{x})\right)
\end{aligned}
 \mathbf{x} \in U 3.3 f:\mathscr{R}^n\rightarrow\mathscr{R}^n \mathscr{C}^1 W a f'(a)\neq 0 f U\subset W a V b=f(a) \mathscr{C}^1 g:V\rightarrow W g(f(x))=x\quad\text{for } x \in U, f(g(y))=y\quad\text{for } y \in W g \{g_k\}_{k=0}^\infty g_0(y)=a,\quad g_{k+1}(y)=g_k(y)-f'(a)^{-1}[f(g_k(y))-y] 1 y x (f'(a)^{-1}) [f(g_k(y))-y] g(y)? f f'(a)^{-1} 3.3 D_{2} G(\mathbf{a}, \mathbf{b})^{-1} 3.4 2","['multivariable-calculus', 'implicit-function-theorem', 'inverse-function-theorem']"
15,differential volume form under two parameterizations,differential volume form under two parameterizations,,"I am trying to solidify my understanding of differential volume form by integrating the same section of the unit sphere using two separate parameterizations.  With the parameterization $\varphi^{-1}_1(\phi, \theta)=(\sin\phi\sin\theta, \sin\phi\cos\theta, \cos\phi)'$ , $A_1 := \{ (\phi, \theta): \phi\in (0, \pi/4), \theta \in (0,\pi)\}$ , I calculate the integral according to Amann (2009) : $$\text{Vol}_{g, U}(A) := \int_{\varphi(A)} \sqrt{\det(D {\varphi^{-1}}'D \varphi^{-1})} da$$ which here is just $$\text{Vol}_{g, U}(A_1) := \int_{0}^\pi \int_0^{\pi/4} |\sin \phi| ~d\phi~ d\theta \approx 0.920151.$$ Next, I use the parameterization $\varphi^{-1}_2(x,y)=(x, y, \sqrt{1-x^2-y^2})'$ , $A_1 := \{ (x,y): x\in \left(-\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}\right), y \in (0,\sqrt{\frac{1}{\sqrt{2}} - x^2})\}$ (which I think should be the same section of the sphere) to get $$\text{Vol}_{g, U}(A_2) := \int_{-\frac{1}{\sqrt{2}}}^\frac{1}{\sqrt{2}} \int_0^{\sqrt{\frac{1}{\sqrt{2}} - x^2}} \left|\frac{1}{\sqrt{1-x^2-y^2}}\right| ~dy~ dx \approx 1.30593.$$ So, two questions: Where am I going wrong here?  I think that the spherical coordinates one is okay.  I would really like an example where it matches the direct parameterization. I believe that $|\sin \phi|d\phi\theta$ in its entirety is the Riemannian volume form -- is this correct? Thanks! Edit Oct1: I've added a photo of the region.","I am trying to solidify my understanding of differential volume form by integrating the same section of the unit sphere using two separate parameterizations.  With the parameterization , , I calculate the integral according to Amann (2009) : which here is just Next, I use the parameterization , (which I think should be the same section of the sphere) to get So, two questions: Where am I going wrong here?  I think that the spherical coordinates one is okay.  I would really like an example where it matches the direct parameterization. I believe that in its entirety is the Riemannian volume form -- is this correct? Thanks! Edit Oct1: I've added a photo of the region.","\varphi^{-1}_1(\phi, \theta)=(\sin\phi\sin\theta, \sin\phi\cos\theta, \cos\phi)' A_1 := \{ (\phi, \theta): \phi\in (0, \pi/4), \theta \in (0,\pi)\} \text{Vol}_{g, U}(A) := \int_{\varphi(A)} \sqrt{\det(D {\varphi^{-1}}'D \varphi^{-1})} da \text{Vol}_{g, U}(A_1) := \int_{0}^\pi \int_0^{\pi/4} |\sin \phi| ~d\phi~ d\theta \approx 0.920151. \varphi^{-1}_2(x,y)=(x, y, \sqrt{1-x^2-y^2})' A_1 := \{ (x,y): x\in \left(-\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}\right), y \in (0,\sqrt{\frac{1}{\sqrt{2}} - x^2})\} \text{Vol}_{g, U}(A_2) := \int_{-\frac{1}{\sqrt{2}}}^\frac{1}{\sqrt{2}} \int_0^{\sqrt{\frac{1}{\sqrt{2}} - x^2}} \left|\frac{1}{\sqrt{1-x^2-y^2}}\right| ~dy~ dx \approx 1.30593. |\sin \phi|d\phi\theta","['geometry', 'multivariable-calculus', 'differential-geometry', 'riemannian-geometry']"
16,"How to Prove $\lim_{(x, y) \rightarrow (0, 0)}\frac{x^3-y^3}{x^2+y^2}=0$ [duplicate]",How to Prove  [duplicate],"\lim_{(x, y) \rightarrow (0, 0)}\frac{x^3-y^3}{x^2+y^2}=0","This question already has answers here : Evaluating $\lim\limits_{(x,y) \rightarrow (0,0)} \frac{x^3 - y^3}{x^2 + y^2}$ (2 answers) Closed 2 years ago . I wanted to prove how $$ \lim_{(x, y) \to (0, 0)} \frac{x^3-y^3}{x^2+y^2} = 0. $$ Specifically, I want to use the Squeeze Theorem for multivariable calculus. Then I know that I should pick an arbitrary function $g(x, y)$ where $|f(x, y)-L| \leq g(x, y)$ and the limit of $g(x, y)$ is $0$ . Since the limit of $f(x, y) =0$ , I just have to make $$ \Biggl\lvert \frac{x^3-y^3}{x^2+y^2} \Biggr\rvert \leq g(x, y), $$ where the limit of $g(x,y)=0$ . I was given a hint that $|x^3-y^3| \leq |x|^3+|y|^3$ . So I divided both sides by $|x^2+y^2|$ , $$ \frac{\bigl\lvert x^3-y^3 \bigr\rvert}{\bigl\lvert x^2+y^2 \bigr\rvert}  \leq \frac{\lvert x\rvert^3+ \lvert y\rvert^3}{\bigl\lvert x^2+y^2 \bigr\rvert}, $$ meaning that $$ \frac{\bigl\lvert x^3-y^3 \bigr\rvert}{\bigl\lvert x^2+y^2 \bigr\rvert}   \leq \lvert x\rvert + \lvert y\rvert.  $$ What do I do next?","This question already has answers here : Evaluating $\lim\limits_{(x,y) \rightarrow (0,0)} \frac{x^3 - y^3}{x^2 + y^2}$ (2 answers) Closed 2 years ago . I wanted to prove how Specifically, I want to use the Squeeze Theorem for multivariable calculus. Then I know that I should pick an arbitrary function where and the limit of is . Since the limit of , I just have to make where the limit of . I was given a hint that . So I divided both sides by , meaning that What do I do next?","
\lim_{(x, y) \to (0, 0)} \frac{x^3-y^3}{x^2+y^2} = 0.
 g(x, y) |f(x, y)-L| \leq g(x, y) g(x, y) 0 f(x, y) =0 
\Biggl\lvert \frac{x^3-y^3}{x^2+y^2} \Biggr\rvert \leq g(x, y),
 g(x,y)=0 |x^3-y^3| \leq |x|^3+|y|^3 |x^2+y^2| 
\frac{\bigl\lvert x^3-y^3 \bigr\rvert}{\bigl\lvert x^2+y^2 \bigr\rvert} 
\leq \frac{\lvert x\rvert^3+ \lvert y\rvert^3}{\bigl\lvert x^2+y^2 \bigr\rvert},
 
\frac{\bigl\lvert x^3-y^3 \bigr\rvert}{\bigl\lvert x^2+y^2 \bigr\rvert}  
\leq \lvert x\rvert + \lvert y\rvert. 
","['calculus', 'limits', 'multivariable-calculus', 'triangle-inequality']"
17,"Find volume of a solid figure that lies between: $x^2+y^2+z^2\leq 4$, $2x^2+2y^2-2z^2\geq 1$, $2z^2\geq x^2+y^2$","Find volume of a solid figure that lies between: , ,",x^2+y^2+z^2\leq 4 2x^2+2y^2-2z^2\geq 1 2z^2\geq x^2+y^2,"Find volume of a solid figure that lies between: $x^2+y^2+z^2\leq 4$ , $2x^2+2y^2-2z^2\geq 1$ , $2z^2\geq x^2+y^2$ I just really can't figure out the limits of integration... any hint would be great -----edit---- Spherical coordinates: $x=\rho \sin\varphi \cos\theta$ $y=\rho \sin\varphi \sin\theta$ $z = \rho \cos\varphi$ From sphere: $\rho ^2 \leq 4$ or $\rho \leq 2$ . From hyperboloid: $ 2\rho ^2( \sin^2\varphi - \cos^2 \varphi)\geq 1$ From cone: $\rho ^2(2 \cos^2\varphi - \sin^2 \varphi)\geq 0$ Intersection of sphere and hyperboloid, I got the ellipse: $\frac{x^2}{\frac{9}{4}}+\frac{y^2}{\frac{9}{4}}=1$ or in spherical coordinates: $\rho^2 \sin^2\varphi=\frac{9}{4}$","Find volume of a solid figure that lies between: , , I just really can't figure out the limits of integration... any hint would be great -----edit---- Spherical coordinates: From sphere: or . From hyperboloid: From cone: Intersection of sphere and hyperboloid, I got the ellipse: or in spherical coordinates:",x^2+y^2+z^2\leq 4 2x^2+2y^2-2z^2\geq 1 2z^2\geq x^2+y^2 x=\rho \sin\varphi \cos\theta y=\rho \sin\varphi \sin\theta z = \rho \cos\varphi \rho ^2 \leq 4 \rho \leq 2  2\rho ^2( \sin^2\varphi - \cos^2 \varphi)\geq 1 \rho ^2(2 \cos^2\varphi - \sin^2 \varphi)\geq 0 \frac{x^2}{\frac{9}{4}}+\frac{y^2}{\frac{9}{4}}=1 \rho^2 \sin^2\varphi=\frac{9}{4},"['multivariable-calculus', 'volume']"
18,"Caculate $\iint_D \frac{x^2+y^2}{\sqrt{4-(x^2+y^2)^2}}dxdy$, with D:$\frac{x^2}{2}+y^2\leq1$ [closed]","Caculate , with D: [closed]",\iint_D \frac{x^2+y^2}{\sqrt{4-(x^2+y^2)^2}}dxdy \frac{x^2}{2}+y^2\leq1,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I found some difficulty with this exercise: Calculate $$\iint_D \frac{x^2+y^2}{\sqrt{4-(x^2+y^2)^2}}dxdy$$ with $D := \left\{(x,y)\in\mathbb{R}^{2}\mid \dfrac{x^2}{2}+y^2\leq1\right\}$ I use change of Variables in Polar Coordinates, but the integral become so hard to calculate. I think maybe we change variables $u = x^2 + y^2$ , the integral will be easier, but I can't find $v(x,y$ ) to have the Jacobi easy to calculate.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I found some difficulty with this exercise: Calculate with I use change of Variables in Polar Coordinates, but the integral become so hard to calculate. I think maybe we change variables , the integral will be easier, but I can't find ) to have the Jacobi easy to calculate.","\iint_D \frac{x^2+y^2}{\sqrt{4-(x^2+y^2)^2}}dxdy D := \left\{(x,y)\in\mathbb{R}^{2}\mid \dfrac{x^2}{2}+y^2\leq1\right\} u = x^2 + y^2 v(x,y","['multivariable-calculus', 'multiple-integral']"
19,Calculate the volume of the set $M$,Calculate the volume of the set,M,"We have the set $M=\{(x,y,z)\in \mathbb{R}^3 : x^2+y^2-z^2\leq 1, \ 0 \leq z\leq 3\}$ . Draw $M$ and calculate the volume of $M$ . $$$$ I have done the following : \begin{equation*}\int_M\, dV=\int\int\int\, dx\, dy\, dy\end{equation*} Which are the boundaries of the integrals? Do we have to use spherical coordinates? Or do we set $x=r\cos\theta$ and $y=r\sin\theta$ and $z$ remains $z$ with $0\leq z\leq 3$ ?",We have the set . Draw and calculate the volume of . I have done the following : Which are the boundaries of the integrals? Do we have to use spherical coordinates? Or do we set and and remains with ?,"M=\{(x,y,z)\in \mathbb{R}^3 : x^2+y^2-z^2\leq 1, \ 0 \leq z\leq 3\} M M  \begin{equation*}\int_M\, dV=\int\int\int\, dx\, dy\, dy\end{equation*} x=r\cos\theta y=r\sin\theta z z 0\leq z\leq 3","['integration', 'multivariable-calculus', 'definite-integrals', 'volume']"
20,General form third degree multi variable Taylor polynomial,General form third degree multi variable Taylor polynomial,,"Using a second-degree multivariable Taylor polynomial for $x=(x_{1},...,x_{n})$ it follows that $$f(x+\Delta x)\approx f(x)+\nabla f(x)\Delta x+\frac{1}{2}\Delta x^{T}H(x)\Delta x$$ with $\nabla f$ the gradient and $H(x)$ the Hessian of $f$ . What is the general form of the third degree polynomial? My attempt would be something like $\frac{1}{6}\Delta x^{T}I(x)\Delta x^{2}$ , such that (e.g., for $n=2$ ) $$I(x)\Delta x^{2}=\begin{bmatrix}\frac{\partial^{3}f(x)}{\partial x_{1}^{3}}&3\frac{\partial f}{\partial x_{1}}\frac{\partial^{2}f}{\partial x_{2}^{2}}\\3\frac{\partial f}{\partial x_{2}}\frac{\partial^{2}f}{\partial x_{1}^{2}}&\frac{\partial^{3}f(x)}{\partial x_{2}^{3}}\end{bmatrix}\begin{bmatrix}\Delta x^{2}_{1}\\\Delta x^{2}_{2}\end{bmatrix}$$ and $$\frac{1}{6}\Delta xI(x)\Delta x^{2}=\frac{1}{6}\frac{\partial^{3}f(x)}{\partial x_{1}^{3}}\Delta x_{1}^{3}+\frac{1}{6}\frac{\partial^{3}f(x)}{\partial x_{2}^{3}}\Delta x_{2}^{3}+\frac{1}{2}\frac{\partial f}{\partial x_{1}}\frac{\partial^{2}f}{\partial x_{2}^{2}}\Delta x_{1}\Delta x_{2}^{2}+\frac{1}{2}\frac{\partial f}{\partial x_{2}}\frac{\partial^{1}f}{\partial x_{1}^{2}}\Delta x_{2}\Delta x_{1}^{2}$$ ; however, I know this to be wrong unfortunately.","Using a second-degree multivariable Taylor polynomial for it follows that with the gradient and the Hessian of . What is the general form of the third degree polynomial? My attempt would be something like , such that (e.g., for ) and ; however, I know this to be wrong unfortunately.","x=(x_{1},...,x_{n}) f(x+\Delta x)\approx f(x)+\nabla f(x)\Delta x+\frac{1}{2}\Delta x^{T}H(x)\Delta x \nabla f H(x) f \frac{1}{6}\Delta x^{T}I(x)\Delta x^{2} n=2 I(x)\Delta x^{2}=\begin{bmatrix}\frac{\partial^{3}f(x)}{\partial x_{1}^{3}}&3\frac{\partial f}{\partial x_{1}}\frac{\partial^{2}f}{\partial x_{2}^{2}}\\3\frac{\partial f}{\partial x_{2}}\frac{\partial^{2}f}{\partial x_{1}^{2}}&\frac{\partial^{3}f(x)}{\partial x_{2}^{3}}\end{bmatrix}\begin{bmatrix}\Delta x^{2}_{1}\\\Delta x^{2}_{2}\end{bmatrix} \frac{1}{6}\Delta xI(x)\Delta x^{2}=\frac{1}{6}\frac{\partial^{3}f(x)}{\partial x_{1}^{3}}\Delta x_{1}^{3}+\frac{1}{6}\frac{\partial^{3}f(x)}{\partial x_{2}^{3}}\Delta x_{2}^{3}+\frac{1}{2}\frac{\partial f}{\partial x_{1}}\frac{\partial^{2}f}{\partial x_{2}^{2}}\Delta x_{1}\Delta x_{2}^{2}+\frac{1}{2}\frac{\partial f}{\partial x_{2}}\frac{\partial^{1}f}{\partial x_{1}^{2}}\Delta x_{2}\Delta x_{1}^{2}",['multivariable-calculus']
21,"Implicit differentiation , leibniz notation- problem differential operator notation in multivariable calc","Implicit differentiation , leibniz notation- problem differential operator notation in multivariable calc",,"here are three examples of what im finding troublesome in understanding, i can workout the basics by myself but when it comes to iterating the differential operator i find the notation and the process convoluted. My biggest problem is i dont know in which logical step im making the mistake and so i dont know what to search so i can clear my confusion. example one change the equation $y'' +\frac{2}{x}y'+y=0 $ by taking "" $x$ as a function and $t=xy$ for a free variable"" my work- i take $\frac{\mathrm dy}{\mathrm dx}=\frac{\frac{\mathrm dy}{\mathrm dt}}{\frac{\mathrm dx}{\mathrm dt}}=\frac{1}{x\frac{\mathrm dx}{\mathrm dt}}$ but my book solution says $\frac{1}{x\frac{\mathrm dx}{\mathrm dt}}-\frac{1}{x^2}$ and i have no idea why since even if they used the chain rule $\frac{\mathrm d (\frac{t}{x})}{\mathrm dt}$ it would not give the book answer.even if we swapped in $y=\frac{t}{x}$ and then using the chain rule i dont get and extra $t$ as in $\frac{1}{x\frac{\mathrm dx}{\mathrm dt}}-\frac{t}{x^2}$ example two find $\mathrm d z , \mathrm d^2 z$ given implicit function (1) $\frac{x}{z}=\ln(\frac{z}{y})+1$ so i use the total differential on (1) to get $$\frac{z\mathrm dx-x \mathrm dz}{z^2}=\frac{y}{z}\frac{y \mathrm dz - z \mathrm dy}{y^2}$$ from which by rearranging we can get $\mathrm d z$ $$\mathrm d z=\frac{z(y\mathrm dx+z\mathrm dy)}{y(x+z)}$$ $(x\neq-z)$ by simple algebra manipulation, now we use total derivative on (2) $$ zy\mathrm dx-xy \mathrm dz -yz \mathrm dz-z^2 \mathrm dy=0$$ to get (book answer gives this) $$y(x+z)\mathrm d^2 z=z\mathrm dx \mathrm dy+(z \mathrm dy - x \mathrm dy)\mathrm dz -y^2 \mathrm dz^2$$ and i dont understand this process at all as by going by partial derivatives i dont get the second derivative to look like that(i dont know how i would use chain rule on for example $yz \mathrm dx=z\mathrm dx\mathrm dz+y\mathrm dx\mathrm dz$ ?) and im confused as to what $\mathrm dz^2$ is supposed to mean rigorously at this point. after rearranging and using $\mathrm dz$ the book answer gives $$\mathrm d^2 z=-\frac{z^2(y\mathrm dx-x\mathrm dy)^2}{y^2(x+z)^3}$$ example three transform $y'''=\frac{6y}{x^3}$ if new variable is $t=\ln(|x|)$ so i begin to calculate $y'$ as follows $$\frac{\mathrm dy}{\mathrm dx}=\frac{\mathrm dy}{\mathrm dt}\frac{\mathrm dt}{\mathrm dx}=\frac{1}{x}\frac{\mathrm dy}{\mathrm dt}$$ now to calculate the second derivative by t i do as follows $$\frac{\mathrm d^2y}{\mathrm dx^2}=\frac{1}{x}\frac{\mathrm d}{\mathrm dx}\left(\frac{\mathrm dy}{\mathrm dt}\right)=\frac{1}{x}\frac{\mathrm dt}{\mathrm dx}\frac{\mathrm d}{\mathrm dt}\left(\frac{\mathrm dy}{\mathrm dt}\right)=\frac{1}{x^2}\frac{\mathrm d}{\mathrm dt}\left(\frac{\mathrm dy}{\mathrm dt}\right)$$ and i thought that was it but the book solution gives this for the $\frac{\mathrm d}{\mathrm dt}(\frac{\mathrm dy}{\mathrm dt})=\frac{\mathrm d^2 y}{\mathrm dt^2}-\frac{\mathrm dy}{\mathrm dt}$ and i dont get how we got to this by using the  chain rule given the first derivative like so $\frac{(\mathrm dy)'\mathrm dt-\mathrm dy (dt)'}{(\mathrm dt)^2}$ im not sure what im getting wrong and i think i need a bit of ""hand holding"" so i would beg   please explain where im making the mistake or link me to a book/theory for this. i have no formal knowledge in differential forms or wedge products as im a second year math student.","here are three examples of what im finding troublesome in understanding, i can workout the basics by myself but when it comes to iterating the differential operator i find the notation and the process convoluted. My biggest problem is i dont know in which logical step im making the mistake and so i dont know what to search so i can clear my confusion. example one change the equation by taking "" as a function and for a free variable"" my work- i take but my book solution says and i have no idea why since even if they used the chain rule it would not give the book answer.even if we swapped in and then using the chain rule i dont get and extra as in example two find given implicit function (1) so i use the total differential on (1) to get from which by rearranging we can get by simple algebra manipulation, now we use total derivative on (2) to get (book answer gives this) and i dont understand this process at all as by going by partial derivatives i dont get the second derivative to look like that(i dont know how i would use chain rule on for example ?) and im confused as to what is supposed to mean rigorously at this point. after rearranging and using the book answer gives example three transform if new variable is so i begin to calculate as follows now to calculate the second derivative by t i do as follows and i thought that was it but the book solution gives this for the and i dont get how we got to this by using the  chain rule given the first derivative like so im not sure what im getting wrong and i think i need a bit of ""hand holding"" so i would beg   please explain where im making the mistake or link me to a book/theory for this. i have no formal knowledge in differential forms or wedge products as im a second year math student.","y'' +\frac{2}{x}y'+y=0  x t=xy \frac{\mathrm dy}{\mathrm dx}=\frac{\frac{\mathrm dy}{\mathrm dt}}{\frac{\mathrm dx}{\mathrm dt}}=\frac{1}{x\frac{\mathrm dx}{\mathrm dt}} \frac{1}{x\frac{\mathrm dx}{\mathrm dt}}-\frac{1}{x^2} \frac{\mathrm d (\frac{t}{x})}{\mathrm dt} y=\frac{t}{x} t \frac{1}{x\frac{\mathrm dx}{\mathrm dt}}-\frac{t}{x^2} \mathrm d z , \mathrm d^2 z \frac{x}{z}=\ln(\frac{z}{y})+1 \frac{z\mathrm dx-x \mathrm dz}{z^2}=\frac{y}{z}\frac{y \mathrm dz - z \mathrm dy}{y^2} \mathrm d z \mathrm d z=\frac{z(y\mathrm dx+z\mathrm dy)}{y(x+z)} (x\neq-z)  zy\mathrm dx-xy \mathrm dz -yz \mathrm dz-z^2 \mathrm dy=0 y(x+z)\mathrm d^2 z=z\mathrm dx \mathrm dy+(z \mathrm dy - x \mathrm dy)\mathrm dz -y^2 \mathrm dz^2 yz \mathrm dx=z\mathrm dx\mathrm dz+y\mathrm dx\mathrm dz \mathrm dz^2 \mathrm dz \mathrm d^2 z=-\frac{z^2(y\mathrm dx-x\mathrm dy)^2}{y^2(x+z)^3} y'''=\frac{6y}{x^3} t=\ln(|x|) y' \frac{\mathrm dy}{\mathrm dx}=\frac{\mathrm dy}{\mathrm dt}\frac{\mathrm dt}{\mathrm dx}=\frac{1}{x}\frac{\mathrm dy}{\mathrm dt} \frac{\mathrm d^2y}{\mathrm dx^2}=\frac{1}{x}\frac{\mathrm d}{\mathrm dx}\left(\frac{\mathrm dy}{\mathrm dt}\right)=\frac{1}{x}\frac{\mathrm dt}{\mathrm dx}\frac{\mathrm d}{\mathrm dt}\left(\frac{\mathrm dy}{\mathrm dt}\right)=\frac{1}{x^2}\frac{\mathrm d}{\mathrm dt}\left(\frac{\mathrm dy}{\mathrm dt}\right) \frac{\mathrm d}{\mathrm dt}(\frac{\mathrm dy}{\mathrm dt})=\frac{\mathrm d^2 y}{\mathrm dt^2}-\frac{\mathrm dy}{\mathrm dt} \frac{(\mathrm dy)'\mathrm dt-\mathrm dy (dt)'}{(\mathrm dt)^2}","['multivariable-calculus', 'derivatives', 'implicit-differentiation', 'differential-operators']"
22,Differentiating a Vector and a Matrix w.r.t. a Vector [Matrix Calculus],Differentiating a Vector and a Matrix w.r.t. a Vector [Matrix Calculus],,"I am studying matrix calculus for linear regression and machine learning and I would like to know exactly if the following calculations are correct: Let $y=\sin(x+yz)$ and $r=\begin{bmatrix}x\\y\\z\end{bmatrix}$ Then the following is the gradient of $y$ i.e., the derivative of $y$ with respect the vector $r$ in denominator layout: $$\frac{\partial y}{\partial r}=\frac{\partial\sin(x+yz)}{\partial r}=\begin{bmatrix}\frac{\partial \sin(x+yz)}{\partial x}\\\frac{\partial\sin(x+yz)}{\partial y}\\\frac{\partial \sin(x+yz}{\partial z}\end{bmatrix}=\begin{bmatrix} \cos(x+yz)\\z\cos(x+yz)\\y\cos(x+yz)\end{bmatrix}$$ In numerator layout it would be: $$[\cos(x+yz), z\cos(x+yz), y\cos(x+yz)]$$ Now, let $$\mathbf{y}=\begin{bmatrix} e^{xyz}\\x^2z\\yx\end{bmatrix}$$ So in numerator layout: $$\frac{\partial\mathbf{y}}{\partial r}=\begin{bmatrix} \frac{\partial e^{xyz}}{\partial x} & \frac{\partial e^{xyz}}{\partial y} & \frac{\partial e^{xyz}}{\partial z}\\ \frac{\partial x^2z}{\partial x} & \frac{\partial x^2z}{\partial y} & \frac{\partial x^2z}{\partial z}\\ \frac{\partial yx}{\partial x} & \frac{\partial yx}{\partial y} & \frac{\partial yx}{\partial z}\end{bmatrix}\\ =\begin{bmatrix} yze^{xyz} & xze^{xyz} & xye^{xyz}\\2xz& 0 & x^2 \\y & x & 0\end{bmatrix} $$ In denominator layout it would be the transpose of the above matrix? Now, let $$Y=\begin{bmatrix} x^2yz & xy^2z \\xyz^2 & \ln(xyz) \end{bmatrix}=\begin{bmatrix} Y_{11} & Y_{12} \\Y_{21} & Y_{22} \end{bmatrix}$$ Then $$\frac{\partial Y}{\partial r}=\begin{bmatrix} \frac{\partial Y_{11}}{\partial x} & \frac{\partial Y_{12}}{\partial x} & \frac{\partial Y_{11}}{\partial y} & \frac{\partial Y_{12}}{\partial y} & \frac{\partial Y_{11}}{\partial z} & \frac{\partial Y_{12}}{\partial z} \\ \frac{\partial Y_{21}}{\partial x} & \frac{\partial Y_{22}}{\partial x} & \frac{\partial Y_{21}}{\partial y} & \frac{\partial Y_{22}}{\partial y} & \frac{\partial Y_{21}}{\partial z} & \frac{\partial Y_{22}}{\partial z}\end{bmatrix}$$ Would this be correct? I'm worrying I mixed up the shape and/or the positions of the members of the matrices $\frac{\partial\mathbf{y}}{\partial r}$ and $\frac{\partial Y}{\partial r}$ .","I am studying matrix calculus for linear regression and machine learning and I would like to know exactly if the following calculations are correct: Let and Then the following is the gradient of i.e., the derivative of with respect the vector in denominator layout: In numerator layout it would be: Now, let So in numerator layout: In denominator layout it would be the transpose of the above matrix? Now, let Then Would this be correct? I'm worrying I mixed up the shape and/or the positions of the members of the matrices and .","y=\sin(x+yz) r=\begin{bmatrix}x\\y\\z\end{bmatrix} y y r \frac{\partial y}{\partial r}=\frac{\partial\sin(x+yz)}{\partial r}=\begin{bmatrix}\frac{\partial \sin(x+yz)}{\partial x}\\\frac{\partial\sin(x+yz)}{\partial y}\\\frac{\partial \sin(x+yz}{\partial z}\end{bmatrix}=\begin{bmatrix} \cos(x+yz)\\z\cos(x+yz)\\y\cos(x+yz)\end{bmatrix} [\cos(x+yz), z\cos(x+yz), y\cos(x+yz)] \mathbf{y}=\begin{bmatrix} e^{xyz}\\x^2z\\yx\end{bmatrix} \frac{\partial\mathbf{y}}{\partial r}=\begin{bmatrix} \frac{\partial e^{xyz}}{\partial x} & \frac{\partial e^{xyz}}{\partial y} & \frac{\partial e^{xyz}}{\partial z}\\ \frac{\partial x^2z}{\partial x} & \frac{\partial x^2z}{\partial y} & \frac{\partial x^2z}{\partial z}\\ \frac{\partial yx}{\partial x} & \frac{\partial yx}{\partial y} & \frac{\partial yx}{\partial z}\end{bmatrix}\\ =\begin{bmatrix} yze^{xyz} & xze^{xyz} & xye^{xyz}\\2xz& 0 & x^2 \\y & x & 0\end{bmatrix}  Y=\begin{bmatrix} x^2yz & xy^2z \\xyz^2 & \ln(xyz) \end{bmatrix}=\begin{bmatrix} Y_{11} & Y_{12} \\Y_{21} & Y_{22} \end{bmatrix} \frac{\partial Y}{\partial r}=\begin{bmatrix} \frac{\partial Y_{11}}{\partial x} & \frac{\partial Y_{12}}{\partial x} & \frac{\partial Y_{11}}{\partial y} & \frac{\partial Y_{12}}{\partial y} & \frac{\partial Y_{11}}{\partial z} & \frac{\partial Y_{12}}{\partial z} \\ \frac{\partial Y_{21}}{\partial x} & \frac{\partial Y_{22}}{\partial x} & \frac{\partial Y_{21}}{\partial y} & \frac{\partial Y_{22}}{\partial y} & \frac{\partial Y_{21}}{\partial z} & \frac{\partial Y_{22}}{\partial z}\end{bmatrix} \frac{\partial\mathbf{y}}{\partial r} \frac{\partial Y}{\partial r}","['calculus', 'multivariable-calculus', 'derivatives', 'vectors', 'matrix-calculus']"
23,Linearity of derivative sanity check,Linearity of derivative sanity check,,"Currently I'm reading about the formalization of the derivative as a linear operator and more specifically reinterpreting the chain rule.  Given $f_1, f_2:\mathbb{R}^n\rightarrow\mathbb{R}^p$ and $g(x_1,x_2):\mathbb{R}^p\times\mathbb{R}^p\rightarrow\mathbb{R}^p$ where $(x_1,x_2)\mapsto \lambda x_1+x_2$ , lets take the derivative of $g(f_1, f_2)$ or in other words $\lambda f_1+ f_2$ . I want to apply the chain rule without using Jacobian matrices or partial derivatives too directly, so I used linear operator form: $$D(g\circ f)(a)=Dg(f(a))\circ D(f(a))$$ or $$D(g\circ f)=((Dg)\circ f)\circ Df$$ Here we know from the limit definition that $Dg=g=\lambda x_1 +x_2$ is equal to itself. If $f=(f_1,f_2)$ , I get $(Dg)\circ f=\lambda f_1+f_2$ as the first term and so the total derivative is $(\lambda f_1+f_2)\circ (Df_1, Df_2)$ which is totally incorrect. But we know $Df=(Df_1, Df_2)$ , so must $(Dg)\circ f$ be $g$ instead? This should be simple but I am probably misinterpreting the chain rule here.","Currently I'm reading about the formalization of the derivative as a linear operator and more specifically reinterpreting the chain rule.  Given and where , lets take the derivative of or in other words . I want to apply the chain rule without using Jacobian matrices or partial derivatives too directly, so I used linear operator form: or Here we know from the limit definition that is equal to itself. If , I get as the first term and so the total derivative is which is totally incorrect. But we know , so must be instead? This should be simple but I am probably misinterpreting the chain rule here.","f_1, f_2:\mathbb{R}^n\rightarrow\mathbb{R}^p g(x_1,x_2):\mathbb{R}^p\times\mathbb{R}^p\rightarrow\mathbb{R}^p (x_1,x_2)\mapsto \lambda x_1+x_2 g(f_1, f_2) \lambda f_1+ f_2 D(g\circ f)(a)=Dg(f(a))\circ D(f(a)) D(g\circ f)=((Dg)\circ f)\circ Df Dg=g=\lambda x_1 +x_2 f=(f_1,f_2) (Dg)\circ f=\lambda f_1+f_2 (\lambda f_1+f_2)\circ (Df_1, Df_2) Df=(Df_1, Df_2) (Dg)\circ f g","['multivariable-calculus', 'derivatives']"
24,"The proof differentiability of multilinear function using ""product"" of function.","The proof differentiability of multilinear function using ""product"" of function.",,"In Kolk's Multidimensional Real Analysis I: Differentiation He used the following Corollary 2.4.3 to prove that a multilinear(k-linear) map $T \in Lin^k(\mathbf{R}^n,\mathbf{R})$ is differentiable.(In the proof of proposition 2.7.6) The whole argument is a single sentence: The differentiability of T follows from the fact that $T(a_1,\cdots, a_k) \in \mathbf{R}$ are polynomials in the coordinates of vectors $a_1,\cdots, a_k \in \mathbf{R}^n$ . See corollary 2.4.3. I know the fact that $T(a_1,\cdots, a_k) \in \mathbf{R}$ are polynomials in the coordinates of vectors $a_1,\cdots, a_k \in \mathbf{R}^n$ . It's a basic property of multilinear map. But how does the conclusion relate to corollary 2.4.3? Note that he defines the product $\langle f_1, f_2 \rangle$ as the composition: $ \begin{equation} \langle f_1, f_2 \rangle: x \mapsto (f_1(x),f_2(x)) \mapsto \langle f_1(x), f_2(x) \rangle : \mathbf{R}^n \rightarrow \mathbf{R}^p \times \mathbf{R}^p \rightarrow \mathbf{R} \end{equation} $",In Kolk's Multidimensional Real Analysis I: Differentiation He used the following Corollary 2.4.3 to prove that a multilinear(k-linear) map is differentiable.(In the proof of proposition 2.7.6) The whole argument is a single sentence: The differentiability of T follows from the fact that are polynomials in the coordinates of vectors . See corollary 2.4.3. I know the fact that are polynomials in the coordinates of vectors . It's a basic property of multilinear map. But how does the conclusion relate to corollary 2.4.3? Note that he defines the product as the composition:,"T \in Lin^k(\mathbf{R}^n,\mathbf{R}) T(a_1,\cdots, a_k) \in \mathbf{R} a_1,\cdots, a_k \in \mathbf{R}^n T(a_1,\cdots, a_k) \in \mathbf{R} a_1,\cdots, a_k \in \mathbf{R}^n \langle f_1, f_2 \rangle 
\begin{equation}
\langle f_1, f_2 \rangle: x \mapsto (f_1(x),f_2(x)) \mapsto \langle f_1(x), f_2(x) \rangle : \mathbf{R}^n \rightarrow \mathbf{R}^p \times \mathbf{R}^p \rightarrow \mathbf{R}
\end{equation}
","['multivariable-calculus', 'multilinear-algebra']"
25,"Show that $|e^{\dot{\imath}x\cdot y}-1|\leq c|y|$, for $|y|<1$ and $x\in \mathbb{R}^{n}$","Show that , for  and",|e^{\dot{\imath}x\cdot y}-1|\leq c|y| |y|<1 x\in \mathbb{R}^{n},"In the book ""Robert Strichartz, Guide to distribution theory"" last line of page 165 in the context of proving that $$I(x):=\int_{\mathbb{R}^n}\frac{|e^{\dot{\imath}x\cdot y}-1|}{|y|^{n+2s}}dy=c|x|^{2s}$$ it says near $y=0$ use that $|e^{\dot{\imath}x\cdot y}-1|\leq c|y|$ by the mean value theorem There is no condition or restriction on $x$ mentioned in this case. So, I tried both the one-dimensional mean-value theorem and the $n$ -dimensional mean value theorem. I always get $$|e^{\dot{\imath}x\cdot y}-1|\leq c|y||x|$$ Here are the details: Apply $1$ -dimensional the mean value theorem to the smooth function $f(t)= e^{\dot{\imath}t}$ on $[0,x\cdot y]$ if $x\cdot y>0$ 0r the integral $[x\cdot y,0]$ of $x\cdot y<0$ . We get $$|f(t)-f(0)|=|e^{\dot{\imath}x\cdot y}-1|=|t|=|x\cdot y|\leq |x||y|.$$ Now, apply the $n$ -dimensional the mean value theorem to the smooth function $f(y)= e^{\dot{\imath}y\cdot x}$ on the line segment $\lambda y+(1-\lambda)0$ , $\lambda\in [0,1]$ . We have $\nabla_{y}f(y)=\dot{\imath} x e^{\dot{\imath}y\cdot x}.$ So $$|e^{\dot{\imath}y\cdot x}-1|=\lambda |y||x|.\qquad (1)$$ If one regards $ e^{\dot{\imath}y\cdot x} $ as a function of $x$ in stead, and use $n$ -dimensional the mean value theorem on the line segment $\lambda x+(1-\lambda)0$ we also get (1) by symmetry. Any ideas?","In the book ""Robert Strichartz, Guide to distribution theory"" last line of page 165 in the context of proving that it says near use that by the mean value theorem There is no condition or restriction on mentioned in this case. So, I tried both the one-dimensional mean-value theorem and the -dimensional mean value theorem. I always get Here are the details: Apply -dimensional the mean value theorem to the smooth function on if 0r the integral of . We get Now, apply the -dimensional the mean value theorem to the smooth function on the line segment , . We have So If one regards as a function of in stead, and use -dimensional the mean value theorem on the line segment we also get (1) by symmetry. Any ideas?","I(x):=\int_{\mathbb{R}^n}\frac{|e^{\dot{\imath}x\cdot y}-1|}{|y|^{n+2s}}dy=c|x|^{2s} y=0 |e^{\dot{\imath}x\cdot y}-1|\leq c|y| x n |e^{\dot{\imath}x\cdot y}-1|\leq c|y||x| 1 f(t)= e^{\dot{\imath}t} [0,x\cdot y] x\cdot y>0 [x\cdot y,0] x\cdot y<0 |f(t)-f(0)|=|e^{\dot{\imath}x\cdot y}-1|=|t|=|x\cdot y|\leq |x||y|. n f(y)= e^{\dot{\imath}y\cdot x} \lambda y+(1-\lambda)0 \lambda\in [0,1] \nabla_{y}f(y)=\dot{\imath} x e^{\dot{\imath}y\cdot x}. |e^{\dot{\imath}y\cdot x}-1|=\lambda |y||x|.\qquad (1)  e^{\dot{\imath}y\cdot x}  x n \lambda x+(1-\lambda)0","['real-analysis', 'analysis', 'multivariable-calculus']"
26,"Need help with identifying dependent variable in change of variables, non-independent variables problem(s)","Need help with identifying dependent variable in change of variables, non-independent variables problem(s)",,"I understand the idea behind non-independent variables and have been able to solve problems of this form: we have a function $f$ , and a constraint equation that relates the variables of $f$ to each other. e.g. $w=x^2+y^2+z^2$ , where $z=x^2+y^2$ . To solve for $(w/z)_y$ we do chain rule: $(w/z)_y$ = $2x(x/z)_y + 2z$ . We then solve for $(x/z)_y$ and get the answer we want: $1+2z$ . But, I am attempting to do this problem and have been quite stuck. $$w=u^3-uv^2, u = xy, v = u + x$$ Find $(w/u)_x$ I'm stuck here because there are a lot more variables than I'm used to. Also, which is the dependent variable? I think this is quite similar to an example brought up in class that I didn't understand. $$f(x, y) = x + y, f/x = 1$$ Change of variables: $$x = u, y = u + v$$ Then, $$f = 2u + v, f/u = 2$$ $$x = u \text{, but } f/x = f/u$$ I think the problem I'm having with both examples is that I don't know what the dependent variable is. Is it x,y,u, or v? If someone can explain both examples, I'd appreciate it. Thank you.","I understand the idea behind non-independent variables and have been able to solve problems of this form: we have a function , and a constraint equation that relates the variables of to each other. e.g. , where . To solve for we do chain rule: = . We then solve for and get the answer we want: . But, I am attempting to do this problem and have been quite stuck. Find I'm stuck here because there are a lot more variables than I'm used to. Also, which is the dependent variable? I think this is quite similar to an example brought up in class that I didn't understand. Change of variables: Then, I think the problem I'm having with both examples is that I don't know what the dependent variable is. Is it x,y,u, or v? If someone can explain both examples, I'd appreciate it. Thank you.","f f w=x^2+y^2+z^2 z=x^2+y^2 (w/z)_y (w/z)_y 2x(x/z)_y + 2z (x/z)_y 1+2z w=u^3-uv^2, u = xy, v = u + x (w/u)_x f(x, y) = x + y, f/x = 1 x = u, y = u + v f = 2u + v, f/u = 2 x = u \text{, but } f/x = f/u",['multivariable-calculus']
27,"Is $\frac{x^3}{x^2+y^4}$ continuous at (0,0)?","Is  continuous at (0,0)?",\frac{x^3}{x^2+y^4},"In an exam I was asked to determine if a function $f:\mathbb R^2\to \mathbb R$ is continuous at a point $(x_0,y_0)$ , I needed to check if $$\lim_{(x,y) \to (x_0,y_0)} f(x,y) = f(x_0,y_0)$$ We consider the function given by $f(x,y)=\frac{x^3}{x^2+y^4}$ if $(x,y)\neq (0,0)$ and $f(0,0)=0$ . We want to know if $f$ is continuous at $(0,0)$ . What I did is the following : We have that for $(x,y) \neq (0,0)$ , $0\leq \frac{x^2}{x^2+y^4}\leq1$ and if $x>0$ $\implies 0\leq \frac{x^3}{x^2+y^4}\leq x$ . And if $x<0$ , $x\leq \frac{x^3}{x^2+y^4}\leq 0$ Then I used the squeeze theorem and said something very weird and stupid : $$""\lim_{(x,y) \to (0^+,0)} f(x,y)=\lim_{(x,y) \to (0^-,0)}f(x,y)=0""$$ First of all we never spoke or defined multivariable limits like that with $0^+$ and $0^-$ , I am starting to realize when I am writing this that I could have been smarter and said that $-\lvert x \rvert \leq \frac{x^3}{x^2+y^4} \leq \lvert x \rvert$ and squeeze theorem to give me the continuity in $(0,0)$ . The second point is that WolframAlpha tells me that the limit does not exist even though I tried many different paths such as $(t,t), (t^2,0)...$ to see if the function was discontinuous. Who is right ?","In an exam I was asked to determine if a function is continuous at a point , I needed to check if We consider the function given by if and . We want to know if is continuous at . What I did is the following : We have that for , and if . And if , Then I used the squeeze theorem and said something very weird and stupid : First of all we never spoke or defined multivariable limits like that with and , I am starting to realize when I am writing this that I could have been smarter and said that and squeeze theorem to give me the continuity in . The second point is that WolframAlpha tells me that the limit does not exist even though I tried many different paths such as to see if the function was discontinuous. Who is right ?","f:\mathbb R^2\to \mathbb R (x_0,y_0) \lim_{(x,y) \to (x_0,y_0)} f(x,y) = f(x_0,y_0) f(x,y)=\frac{x^3}{x^2+y^4} (x,y)\neq (0,0) f(0,0)=0 f (0,0) (x,y) \neq (0,0) 0\leq \frac{x^2}{x^2+y^4}\leq1 x>0 \implies 0\leq \frac{x^3}{x^2+y^4}\leq x x<0 x\leq \frac{x^3}{x^2+y^4}\leq 0 ""\lim_{(x,y) \to (0^+,0)} f(x,y)=\lim_{(x,y) \to (0^-,0)}f(x,y)=0"" 0^+ 0^- -\lvert x \rvert \leq \frac{x^3}{x^2+y^4} \leq \lvert x \rvert (0,0) (t,t), (t^2,0)...","['real-analysis', 'limits', 'multivariable-calculus', 'continuity', 'wolfram-alpha']"
28,"Showing that if the operator norm of $Dg$ is $< 1$, then $g$ has a fixed-point","Showing that if the operator norm of  is , then  has a fixed-point",Dg < 1 g,"I'm trying to solve this following question: Let $g:\mathbb{R^{n}\to R^{n}}$ be a differentiable function. Show that if exists $0\le r<1$ s.t. $\forall a\in \mathbb{R^{n}} \quad \vert\vert{(Dg)_a}\vert\vert_{op}\le r$ , when $\vert\vert{(Dg)_a}\vert\vert_{op} =\max \left\{\vert\vert(Dg)_a \vert\vert_2: \vert\vert x\vert\vert _2=1\right\} $ , then $g$ has a fixed-point in $\mathbb{R^{n}}.$ I wanted to show that $g$ is a contraction mapping, and since $\mathbb{R^{n}}$ is a complete metric space - g admits an unique fixed-point (Banach fixed-point theorem), but I couldn't prove that $g$ is indeed a contraction mapping. Any hint would be appreciated. Thank you!","I'm trying to solve this following question: Let be a differentiable function. Show that if exists s.t. , when , then has a fixed-point in I wanted to show that is a contraction mapping, and since is a complete metric space - g admits an unique fixed-point (Banach fixed-point theorem), but I couldn't prove that is indeed a contraction mapping. Any hint would be appreciated. Thank you!",g:\mathbb{R^{n}\to R^{n}} 0\le r<1 \forall a\in \mathbb{R^{n}} \quad \vert\vert{(Dg)_a}\vert\vert_{op}\le r \vert\vert{(Dg)_a}\vert\vert_{op} =\max \left\{\vert\vert(Dg)_a \vert\vert_2: \vert\vert x\vert\vert _2=1\right\}  g \mathbb{R^{n}}. g \mathbb{R^{n}} g,['multivariable-calculus']
29,"Calculate $\iint \sqrt{2-x^2-y^2} \, dx \, dy$ over the circle $x^2+y^2=x\sqrt{2}$",Calculate  over the circle,"\iint \sqrt{2-x^2-y^2} \, dx \, dy x^2+y^2=x\sqrt{2}","so as my question states I have to calculate $$\iint_\limits A \sqrt{2-x^2-y^2}\,dx\,dy$$ where $A$ is the circle defined by $$x^2+y^2=x\sqrt{2}$$ After introducing polar coordinates $(x = r\cos\theta, y= r\sin\theta)$ , I get that: The left side of the equation of the circle is always nonnegative, so the right side also has to be nonnegative. This means that $x \ge 0$ , or $$\theta \in \left[-\frac{\pi}{2}, \frac{\pi}{2}\right].$$ Also we have that $$r=\sqrt{2} \cos\theta$$ Here's where I have a slight problem. If I think about it, $2-x^2-y^2$ has to be $\ge 0$ for the square root to be real. Now, that means that $r \le \sqrt{2}$ . How do I determine the boundaries of $r$ ? I think that the boundaries for $r$ are $r \in [\sqrt{2}\cos\theta, \sqrt{2}]$ . If yes, could anyone explain why exactly? I'm confused because I don't have an inequality in $r=\sqrt{2}\cos\theta$ !","so as my question states I have to calculate where is the circle defined by After introducing polar coordinates , I get that: The left side of the equation of the circle is always nonnegative, so the right side also has to be nonnegative. This means that , or Also we have that Here's where I have a slight problem. If I think about it, has to be for the square root to be real. Now, that means that . How do I determine the boundaries of ? I think that the boundaries for are . If yes, could anyone explain why exactly? I'm confused because I don't have an inequality in !","\iint_\limits A \sqrt{2-x^2-y^2}\,dx\,dy A x^2+y^2=x\sqrt{2} (x = r\cos\theta, y= r\sin\theta) x \ge 0 \theta \in \left[-\frac{\pi}{2}, \frac{\pi}{2}\right]. r=\sqrt{2} \cos\theta 2-x^2-y^2 \ge 0 r \le \sqrt{2} r r r \in [\sqrt{2}\cos\theta, \sqrt{2}] r=\sqrt{2}\cos\theta","['integration', 'multivariable-calculus', 'polar-coordinates', 'multiple-integral']"
30,"How to calculate $ \intop_{\partial(B_{R}\left(0\right))}\frac{5z^{3}-12z}{|\left(\tau_{1},\tau_{2},\tau_{3}\right)-\left(x,y,z\right)|^{3}}dS_{2} $",How to calculate," \intop_{\partial(B_{R}\left(0\right))}\frac{5z^{3}-12z}{|\left(\tau_{1},\tau_{2},\tau_{3}\right)-\left(x,y,z\right)|^{3}}dS_{2} ","Conside a ball with center at $ (0,0,0) $ and radius $ 2 $ in $ \mathbb{R}^3 $ . Now let $ \tau=\left(\tau_{1},\tau_{2},\tau_{3}\right)\in\mathbb{R}^{3} $ be a constant point in the space. How can I calculate $ \intop_{\partial(B_{2}\left(0\right))}\frac{5z^{3}-12z}{|\left(\tau_{1},\tau_{2},\tau_{3}\right)-\left(x,y,z\right)|^{3}}dS_{2} $ Where $ dS_{2} $ means integrate by the 2 dimesional surface area of the sphere. Background Im trying to solve the following Laplace equation: $ \begin{cases} u_{xx}+u_{yy}+u_{zz}=0 & x^{2}+y^{2}+z^{2}<4\\ u\left(x,y,z\right)=5z^{3}-12z & x^{2}+y^{2}+z^{2}=4 \end{cases}$ Eearlier, I found a formula for homogenous Laplace equation's in a ball in $\mathbb{R}^n $ (using the right Green function), and it is given by: $ u\left(x\right)=\frac{1}{R|S^{N-1}|}\intop_{\partial\left(B_{R}\left(0\right)\right)}\frac{\left(R^{2}-|x|^{2}\right)}{|x-y|^{N}}\varphi\left(y\right)dS_{N-1}\left(y\right) $ Where $ x $ here in $ \mathbb{R}^N$ , $ S^{N-1}| $ is the $N-1 $ dimensional area of a unit sphere in $ \mathbb{R}^N$ , $ \varphi $ is the function for the boundary condition of the Laplace equation $$ \begin{cases} \varDelta u\left(x\right)=0 & inside\thinspace\thinspace the\thinspace\thinspace ball\\ u\left(x\right)=\varphi\left(x\right) & for\thinspace\thinspace\thinspace x\thinspace\thinspace\thinspace in\thinspace\thinspace\thinspace the\thinspace\thinspace\thinspace boundary\thinspace\thinspace\thinspace of\thinspace\thinspace\thinspace the\thinspace\thinspace\thinspace\thinspace ball \end{cases} $$ and $ dS_{N-1}(x) $ means integration by the $N-1$ dimesnional surface of the boudary. I cant see how to calculate this integral (I wrote it without the constatnts from the formula). I will highly appriciate any help. Thanks in advance.","Conside a ball with center at and radius in . Now let be a constant point in the space. How can I calculate Where means integrate by the 2 dimesional surface area of the sphere. Background Im trying to solve the following Laplace equation: Eearlier, I found a formula for homogenous Laplace equation's in a ball in (using the right Green function), and it is given by: Where here in , is the dimensional area of a unit sphere in , is the function for the boundary condition of the Laplace equation and means integration by the dimesnional surface of the boudary. I cant see how to calculate this integral (I wrote it without the constatnts from the formula). I will highly appriciate any help. Thanks in advance."," (0,0,0)   2   \mathbb{R}^3   \tau=\left(\tau_{1},\tau_{2},\tau_{3}\right)\in\mathbb{R}^{3}   \intop_{\partial(B_{2}\left(0\right))}\frac{5z^{3}-12z}{|\left(\tau_{1},\tau_{2},\tau_{3}\right)-\left(x,y,z\right)|^{3}}dS_{2}   dS_{2}   \begin{cases}
u_{xx}+u_{yy}+u_{zz}=0 & x^{2}+y^{2}+z^{2}<4\\
u\left(x,y,z\right)=5z^{3}-12z & x^{2}+y^{2}+z^{2}=4
\end{cases} \mathbb{R}^n   u\left(x\right)=\frac{1}{R|S^{N-1}|}\intop_{\partial\left(B_{R}\left(0\right)\right)}\frac{\left(R^{2}-|x|^{2}\right)}{|x-y|^{N}}\varphi\left(y\right)dS_{N-1}\left(y\right)   x   \mathbb{R}^N  S^{N-1}|  N-1   \mathbb{R}^N  \varphi   \begin{cases}
\varDelta u\left(x\right)=0 & inside\thinspace\thinspace the\thinspace\thinspace ball\\
u\left(x\right)=\varphi\left(x\right) & for\thinspace\thinspace\thinspace x\thinspace\thinspace\thinspace in\thinspace\thinspace\thinspace the\thinspace\thinspace\thinspace boundary\thinspace\thinspace\thinspace of\thinspace\thinspace\thinspace the\thinspace\thinspace\thinspace\thinspace ball
\end{cases}   dS_{N-1}(x)  N-1","['multivariable-calculus', 'partial-differential-equations']"
31,Clarification on definition of differentiability of vector-valued functions.,Clarification on definition of differentiability of vector-valued functions.,,"As it is normally stated, the definition of differentiability of vector-valued function is as follows: $\textbf{Definition:}$ Let $U \subset \Bbb{R}^n$ be open, and let $a \in U$ . A vector-valued function $f$ is differentiable at $a$ if there is a linear map $Df(a): \Bbb{R}^n \to \Bbb{R}^n$ so that $$\lim\limits_{h \to 0} \frac{f(a+h)-f(a)-Df(a)h}{\Vert h \Vert} = 0.$$ My question is, is the vector $h$ implicitly assumed to be arbitrary such that it is analogous for the case of real-valued functions where the direction of approaching the limit point should not matter?","As it is normally stated, the definition of differentiability of vector-valued function is as follows: Let be open, and let . A vector-valued function is differentiable at if there is a linear map so that My question is, is the vector implicitly assumed to be arbitrary such that it is analogous for the case of real-valued functions where the direction of approaching the limit point should not matter?",\textbf{Definition:} U \subset \Bbb{R}^n a \in U f a Df(a): \Bbb{R}^n \to \Bbb{R}^n \lim\limits_{h \to 0} \frac{f(a+h)-f(a)-Df(a)h}{\Vert h \Vert} = 0. h,"['multivariable-calculus', 'definition']"
32,Changing the order of integration of these two double integrals.,Changing the order of integration of these two double integrals.,,"$\int^2_1dx\int^{\sqrt{2x-x^2}}_{2-x}f(x,y)dy$ $\int^e_1dx\int^{ln(x)}_{0}f(x,y)dy$ My attempts: For the first Integral: I can see that $y=\sqrt{2x-x^2}$ defines half of a circle with radius $1$ . And $2-x$ is basically a line, I drew them, and I can see that if I wanted to change the order of integration, $y$ will be between $0$ and $1$ , and now my problem is $x$ , I need to get what is the upper $x$ from this equation $y=\sqrt{2x-x^2} \Longrightarrow y^2=2x-x^2$ , but how do I keep going? I'm not succeeding to get $x$ from that equation. For the second integral : I can see the area between $y=ln(x)$ , $x=e$ , $x=1$ . In order to change the order of integration, I must see the borders of $y$ , and that's the intersection between $y=ln(x)$ and $x=e$ , which is $y=1$ . So the integral will be: $\int^1_0dy\int^{e}_{e^y}f(x,y)dx$ Any help and feedback is really appreciated, thanks in advance!","My attempts: For the first Integral: I can see that defines half of a circle with radius . And is basically a line, I drew them, and I can see that if I wanted to change the order of integration, will be between and , and now my problem is , I need to get what is the upper from this equation , but how do I keep going? I'm not succeeding to get from that equation. For the second integral : I can see the area between , , . In order to change the order of integration, I must see the borders of , and that's the intersection between and , which is . So the integral will be: Any help and feedback is really appreciated, thanks in advance!","\int^2_1dx\int^{\sqrt{2x-x^2}}_{2-x}f(x,y)dy \int^e_1dx\int^{ln(x)}_{0}f(x,y)dy y=\sqrt{2x-x^2} 1 2-x y 0 1 x x y=\sqrt{2x-x^2} \Longrightarrow y^2=2x-x^2 x y=ln(x) x=e x=1 y y=ln(x) x=e y=1 \int^1_0dy\int^{e}_{e^y}f(x,y)dx","['integration', 'multivariable-calculus']"
33,Prove that this map isn't invertible on all of $\mathbb R^2$.,Prove that this map isn't invertible on all of .,\mathbb R^2,"Let $F(x,y)=(e^x\cos y, e^x\sin y)$ , show that $F$ isn't invertible on all of $\mathbb R^2$ , although it's locally invertible everywhere. It's obvious that $F$ is locally invertible everywhere, because $$J_F(x,y)=\pmatrix{e^x\cos y & -e^x\sin y\\ e^x\sin y & e^x\cos y}$$ so the determinant of the jacobian matrix is $$\Delta_F(x,y)=e^{2x}\cos^2x+e^{2x}\sin^2x=e^{2x}\ne0$$ Hence $F$ is locally invertible everywhere, but i can't prove that $F$ isn't invertible.","Let , show that isn't invertible on all of , although it's locally invertible everywhere. It's obvious that is locally invertible everywhere, because so the determinant of the jacobian matrix is Hence is locally invertible everywhere, but i can't prove that isn't invertible.","F(x,y)=(e^x\cos y, e^x\sin y) F \mathbb R^2 F J_F(x,y)=\pmatrix{e^x\cos y & -e^x\sin y\\ e^x\sin y & e^x\cos y} \Delta_F(x,y)=e^{2x}\cos^2x+e^{2x}\sin^2x=e^{2x}\ne0 F F","['matrices', 'multivariable-calculus', 'inverse', 'jacobian']"
34,Clarifying the definition of the directional derivative of a curve,Clarifying the definition of the directional derivative of a curve,,"I am reading Kuhnel's Differential Geometry - Curves, Surfaces, Manifolds and I feel like I am missing something about the definition of directional derivative (4.1). Kuhnel defines it as: Let $Y$ be a differentiable vector field defined on an open set of $\mathbb{R}^{n+1}$ , and let $(p, X) \in T_p\mathbb{R}^{n+1}$ . Then the directional derivative of $Y$ in the direction $X$ at $p$ is (the vector) $D_XY|_p := DY|_p(X)$ , where $DY|_p$ is the Jacobian matrix of $Y$ at $p$ . A few pages later, Kuhnel begins talking about geodesics and mentions the following: ""If $c(t)$ is the motion of the mass of a particle, then $D_{\dot{c}}\dot{c} = \ddot{c}$ is just the acceleration vector in Euclidean space."" But $\dot{c}$ is a curve, not a vector field, so how is the expression $D_{\dot{c}}\dot{c}$ even defined? Even if we consider $\dot{c}$ as a map $c(t) \mapsto \dot{c}(t)$ (i.e. vectors based at points on the curve), it is not defined on an open set of $\mathbb{R}^{n+1}$ , so I don't really know how to calculate $D_{\dot{c}}\dot{c}$ .","I am reading Kuhnel's Differential Geometry - Curves, Surfaces, Manifolds and I feel like I am missing something about the definition of directional derivative (4.1). Kuhnel defines it as: Let be a differentiable vector field defined on an open set of , and let . Then the directional derivative of in the direction at is (the vector) , where is the Jacobian matrix of at . A few pages later, Kuhnel begins talking about geodesics and mentions the following: ""If is the motion of the mass of a particle, then is just the acceleration vector in Euclidean space."" But is a curve, not a vector field, so how is the expression even defined? Even if we consider as a map (i.e. vectors based at points on the curve), it is not defined on an open set of , so I don't really know how to calculate .","Y \mathbb{R}^{n+1} (p, X) \in T_p\mathbb{R}^{n+1} Y X p D_XY|_p := DY|_p(X) DY|_p Y p c(t) D_{\dot{c}}\dot{c} = \ddot{c} \dot{c} D_{\dot{c}}\dot{c} \dot{c} c(t) \mapsto \dot{c}(t) \mathbb{R}^{n+1} D_{\dot{c}}\dot{c}","['multivariable-calculus', 'differential-geometry', 'definition']"
35,"Suppose $u(x,y)$ is a function $C^2$ from $\mathbb{R}^2$ to $\mathbb{R}$. After a change to polar coordinates $u(x,y)=u(r\cos \theta , r\sin \theta)$.",Suppose  is a function  from  to . After a change to polar coordinates .,"u(x,y) C^2 \mathbb{R}^2 \mathbb{R} u(x,y)=u(r\cos \theta , r\sin \theta)","Suppose $u(x,y)$ is a function $C^2$ from $\mathbb{R}^2$ to $\mathbb{R}$ . After a change to polar coordinates $u(x,y)=u(r\cos \theta , r\sin \theta)$ . We have \begin{align*} u_r=\frac{\partial u}{\partial x} \frac{\partial x}{\partial r} + \frac{\partial u}{\partial y} \frac{\partial y}{\partial r};\ u_\theta = \frac{\partial u}{\partial x} \frac{\partial x}{\partial \theta} + \frac{\partial u}{\partial y} \frac{\partial y}{\partial \theta} \end{align*} Use the chain rule to show that \begin{align*} u_{xx}+u_{yy}=u_{rr}+\frac{1}{r}u_\theta \end{align*} Attempt: \begin{align*} x&=r\cos \theta \\ y&=r\sin \theta \end{align*} \begin{align*} \frac{\partial x}{\partial r}=\cos \theta && \frac{\partial x}{\partial \theta}=-r\sin \theta \\ \frac{\partial y}{\partial r}=\sin \theta && \frac{\partial y}{\partial \theta}=r\cos \theta \end{align*} \begin{align*} u_r&=\frac{\partial u}{\partial x} \frac{\partial x}{\partial r} + \frac{\partial u}{\partial y} \frac{\partial y}{\partial r} \\ &=\frac{\partial u}{\partial x} \cos \theta + \frac{\partial u}{\partial y}\sin \theta \\ &=\cos \theta\frac{\partial u}{\partial x} + \sin \theta \frac{\partial u}{\partial y} \end{align*} \begin{align*} u_{rr}&=\cos \theta \frac{\partial }{\partial r}\frac{\partial u}{\partial x} + \sin \theta \frac{\partial }{\partial r}\frac{\partial u}{\partial y} \\ &= \cos \theta \left( \frac{\partial }{\partial x}\frac{\partial u}{\partial x}\frac{\partial x}{\partial r}+\frac{\partial }{\partial y}\frac{\partial u}{\partial x}\frac{\partial y}{\partial r} \right) + \sin \theta \left( \frac{\partial }{\partial x}\frac{\partial u}{\partial y}\frac{\partial x}{\partial r}+\frac{\partial }{\partial y}\frac{\partial u}{\partial y}\frac{\partial y}{\partial r} \right) \\ &=u_{xx}\cos ^2 \theta +u_{xy}2\cos \theta \sin \theta +u_{yy}\sin ^2 \theta  \end{align*} \begin{align*} u_\theta &= \frac{\partial u}{\partial x}\frac{\partial x}{\partial \theta}+\frac{\partial u}{\partial y}\frac{\partial y}{\partial \theta} \\ &= \frac{\partial u}{\partial x}(-r\sin \theta)+\frac{\partial u}{\partial y}(r\cos \theta) \\ &=r\cos \theta \frac{\partial u}{\partial y}-r\sin \theta \frac{\partial u}{\partial x} \end{align*} I already have this, but I don't know how to continue.","Suppose is a function from to . After a change to polar coordinates . We have Use the chain rule to show that Attempt: I already have this, but I don't know how to continue.","u(x,y) C^2 \mathbb{R}^2 \mathbb{R} u(x,y)=u(r\cos \theta , r\sin \theta) \begin{align*}
u_r=\frac{\partial u}{\partial x} \frac{\partial x}{\partial r} + \frac{\partial u}{\partial y} \frac{\partial y}{\partial r};\ u_\theta = \frac{\partial u}{\partial x} \frac{\partial x}{\partial \theta} + \frac{\partial u}{\partial y} \frac{\partial y}{\partial \theta}
\end{align*} \begin{align*}
u_{xx}+u_{yy}=u_{rr}+\frac{1}{r}u_\theta
\end{align*} \begin{align*}
x&=r\cos \theta \\
y&=r\sin \theta
\end{align*} \begin{align*}
\frac{\partial x}{\partial r}=\cos \theta && \frac{\partial x}{\partial \theta}=-r\sin \theta \\
\frac{\partial y}{\partial r}=\sin \theta && \frac{\partial y}{\partial \theta}=r\cos \theta
\end{align*} \begin{align*}
u_r&=\frac{\partial u}{\partial x} \frac{\partial x}{\partial r} + \frac{\partial u}{\partial y} \frac{\partial y}{\partial r} \\
&=\frac{\partial u}{\partial x} \cos \theta + \frac{\partial u}{\partial y}\sin \theta \\
&=\cos \theta\frac{\partial u}{\partial x} + \sin \theta \frac{\partial u}{\partial y}
\end{align*} \begin{align*}
u_{rr}&=\cos \theta \frac{\partial }{\partial r}\frac{\partial u}{\partial x} + \sin \theta \frac{\partial }{\partial r}\frac{\partial u}{\partial y} \\
&= \cos \theta \left( \frac{\partial }{\partial x}\frac{\partial u}{\partial x}\frac{\partial x}{\partial r}+\frac{\partial }{\partial y}\frac{\partial u}{\partial x}\frac{\partial y}{\partial r} \right) + \sin \theta \left( \frac{\partial }{\partial x}\frac{\partial u}{\partial y}\frac{\partial x}{\partial r}+\frac{\partial }{\partial y}\frac{\partial u}{\partial y}\frac{\partial y}{\partial r} \right) \\
&=u_{xx}\cos ^2 \theta +u_{xy}2\cos \theta \sin \theta +u_{yy}\sin ^2 \theta 
\end{align*} \begin{align*}
u_\theta &= \frac{\partial u}{\partial x}\frac{\partial x}{\partial \theta}+\frac{\partial u}{\partial y}\frac{\partial y}{\partial \theta} \\
&= \frac{\partial u}{\partial x}(-r\sin \theta)+\frac{\partial u}{\partial y}(r\cos \theta) \\
&=r\cos \theta \frac{\partial u}{\partial y}-r\sin \theta \frac{\partial u}{\partial x}
\end{align*}","['multivariable-calculus', 'vector-analysis']"
36,Volume integral over a cone,Volume integral over a cone,,"I'm trying to integrate $I = \rho\int_V(y^2+z^2)dv$ over a cone of base radius $R$ and height $H$ , where $\rho$ is a constant. The coordinates $y$ and $z$ are coordinates of the volume element. In cylindrical coordinates, we have $$dv = rdrd\theta dz$$ and $$y = r\sin \theta$$ Now the integral $I$ should be $$I = \int_0^R\int_0^{2\pi}\int_0^H(r^3\sin^2\theta+z^2r)drd\theta dz$$ Is this conversion to cylindrical coordinates correct? The background of the question is the moment of inertia tensor of said cone.","I'm trying to integrate over a cone of base radius and height , where is a constant. The coordinates and are coordinates of the volume element. In cylindrical coordinates, we have and Now the integral should be Is this conversion to cylindrical coordinates correct? The background of the question is the moment of inertia tensor of said cone.",I = \rho\int_V(y^2+z^2)dv R H \rho y z dv = rdrd\theta dz y = r\sin \theta I I = \int_0^R\int_0^{2\pi}\int_0^H(r^3\sin^2\theta+z^2r)drd\theta dz,"['integration', 'multivariable-calculus', 'multiple-integral', 'cylindrical-coordinates']"
37,Changing the order of integration to find the volume,Changing the order of integration to find the volume,,"My goal is to set up the triple integral that will solve the volume $\iiint \ xyz \ dV $ if S if the region bounded by the cylinders $x^2 + y^2 = 25$ and $ x^2+ z^2 = 25$ and the 1st octant with dV = dxdydz. With order dV = dzdydx, the value of the volume is $\frac{15625}{24}$ . My attempt is to set up the triple integral with order dV = dxdydz in which it should have the same answer with the triple integral with order dV = dzdydx. The S I have computed is S $\lbrace (x, y, z) \in \mathbb{R}^3: 0 \leq x \leq 5, 0 \leq y \leq \sqrt{25 - x^2}, 0 \leq z \leq \sqrt{25 - x^2} \rbrace$ this is for triple integral with order dV = dzdydx The S I have computed is S $\lbrace (x, y, z) \in \mathbb{R}^3: 0 \leq x \leq \sqrt{25 - \frac{y^2}{2} - \frac{z^2}{2}}, 0 \leq y \leq \sqrt{50 - z^2}, 0 \leq z \leq \sqrt{50} \rbrace$ this is for triple integral with order dV = dxdydz When I set up the triple integral, I found out that it should be $\frac{1}{2} \int_0^\sqrt{50} \int_0^\sqrt{50-z^2} \int_0^\sqrt{25 - \frac{y^2}{2} - \frac{z^2}{2}} xyz \ dxdydz$ and not simply $\int_0^\sqrt{50} \int_0^\sqrt{50-z^2} \int_0^\sqrt{25 - \frac{y^2}{2} - \frac{z^2}{2}} xyz \ dxdydz$ . What is the principle behind this? Why I should divide the volume of this triple integral to 2?","My goal is to set up the triple integral that will solve the volume if S if the region bounded by the cylinders and and the 1st octant with dV = dxdydz. With order dV = dzdydx, the value of the volume is . My attempt is to set up the triple integral with order dV = dxdydz in which it should have the same answer with the triple integral with order dV = dzdydx. The S I have computed is S this is for triple integral with order dV = dzdydx The S I have computed is S this is for triple integral with order dV = dxdydz When I set up the triple integral, I found out that it should be and not simply . What is the principle behind this? Why I should divide the volume of this triple integral to 2?","\iiint \ xyz \ dV  x^2 + y^2 = 25  x^2+ z^2 = 25 \frac{15625}{24} \lbrace (x, y, z) \in \mathbb{R}^3: 0 \leq x \leq 5, 0 \leq y \leq \sqrt{25 - x^2}, 0 \leq z \leq \sqrt{25 - x^2} \rbrace \lbrace (x, y, z) \in \mathbb{R}^3: 0 \leq x \leq \sqrt{25 - \frac{y^2}{2} - \frac{z^2}{2}}, 0 \leq y \leq \sqrt{50 - z^2}, 0 \leq z \leq \sqrt{50} \rbrace \frac{1}{2} \int_0^\sqrt{50} \int_0^\sqrt{50-z^2} \int_0^\sqrt{25 - \frac{y^2}{2} - \frac{z^2}{2}} xyz \ dxdydz \int_0^\sqrt{50} \int_0^\sqrt{50-z^2} \int_0^\sqrt{25 - \frac{y^2}{2} - \frac{z^2}{2}} xyz \ dxdydz","['calculus', 'multivariable-calculus', 'definite-integrals']"
38,Type of critical points in three dimensions,Type of critical points in three dimensions,,"I am facing an exercise about maxima and minima for the function $$f(x, y, z) = xye^x - xyz$$ So the gradient is $$\nabla f(x, y, z) = (ye^x + xye^x - yz, xe^x - xz, -xy)$$ The solutions I found are the points $$P = (0, 0, z)$$ $$Q = (0, y, 1)$$ $$K = (x, 0, e^x)$$ Now the Hessian matrix reads $$H =  \begin{pmatrix} 2ye^x + xye^x & e^x + xe^x - z & -y \\ e^x + xe^x - z & 0 & -x \\ -y & -x & 0 \end{pmatrix} $$ The problem is that when I evaluate the Hessian in the points I have found, in all the three cases I find one zero eigenvalue, which means I cannot say anything about the point. How is this possible? Is there a way to say anything about those points?","I am facing an exercise about maxima and minima for the function So the gradient is The solutions I found are the points Now the Hessian matrix reads The problem is that when I evaluate the Hessian in the points I have found, in all the three cases I find one zero eigenvalue, which means I cannot say anything about the point. How is this possible? Is there a way to say anything about those points?","f(x, y, z) = xye^x - xyz \nabla f(x, y, z) = (ye^x + xye^x - yz, xe^x - xz, -xy) P = (0, 0, z) Q = (0, y, 1) K = (x, 0, e^x) H = 
\begin{pmatrix}
2ye^x + xye^x & e^x + xe^x - z & -y \\
e^x + xe^x - z & 0 & -x \\
-y & -x & 0
\end{pmatrix}
","['multivariable-calculus', 'eigenvalues-eigenvectors', 'maxima-minima', 'hessian-matrix']"
39,Verify if the boundaries of the following double integrals are correct:,Verify if the boundaries of the following double integrals are correct:,,"I would appreciate if anyone can verify if I did the boundary right, I am trying to practice only this thing, I don't want to evaluate the integrals, just to learn how to do the boundary right. $\left(1\right)$ $$\int \:\int _D\:\sqrt{x^2-y^2}dxdy\:$$ $$\text{where }D \text{ is bounded by the sides of the triangle } OAB\: \text{ with }O\left(0,0\right),\:\:A\left(1,-1\right),\:B\left(1,1\right)$$ Here I found out that $y=-x \text{ and }y=\:-1$ from doing OA and AB hence my integral becomes bounded by: $$\int _0^1\:\left(\int _{-x}^{-1}\:\:\sqrt{x^2-y^2}dy\:\right)dx$$ $\left(2\right)$ $$\int \:\int _D\:\:xy \: dxdy$$ $$\text{where } D \text{ is bounded by } y=x^2,\:y=2x+3$$ For this one I did the following: $$\int _{-1}^{0}\:\:\left(\int _{x^2}^{2x+3}xy \: dy\right)\:dx$$ because i took $-1\le x\le 0$ $(3)$ $$\int \:\int _D \arcsin\sqrt{x+y}\:dxdy$$ $$\text{ where } D\text{ is bounded by } x+y=0,\:x+y=1,\:x=0,\:x=1$$ For this one I took: $0\le x\le 1$ and $y=-x\:,\:y=\:1-x$ so my integral will have the following boundary: $$ \int _0^1\:\left(\int _{-x}^{1-x} \arcsin\sqrt{x+y}\:dy\right)dx $$ I hope I didn't do any typos, I am a bit tired and also still not used to the typing format yet.","I would appreciate if anyone can verify if I did the boundary right, I am trying to practice only this thing, I don't want to evaluate the integrals, just to learn how to do the boundary right. Here I found out that from doing OA and AB hence my integral becomes bounded by: For this one I did the following: because i took For this one I took: and so my integral will have the following boundary: I hope I didn't do any typos, I am a bit tired and also still not used to the typing format yet.","\left(1\right) \int \:\int _D\:\sqrt{x^2-y^2}dxdy\: \text{where }D \text{ is bounded by the sides of the triangle } OAB\: \text{ with }O\left(0,0\right),\:\:A\left(1,-1\right),\:B\left(1,1\right) y=-x \text{ and }y=\:-1 \int _0^1\:\left(\int _{-x}^{-1}\:\:\sqrt{x^2-y^2}dy\:\right)dx \left(2\right) \int \:\int _D\:\:xy \: dxdy \text{where } D \text{ is bounded by } y=x^2,\:y=2x+3 \int _{-1}^{0}\:\:\left(\int _{x^2}^{2x+3}xy \: dy\right)\:dx -1\le x\le 0 (3) \int \:\int _D \arcsin\sqrt{x+y}\:dxdy \text{ where } D\text{ is bounded by } x+y=0,\:x+y=1,\:x=0,\:x=1 0\le x\le 1 y=-x\:,\:y=\:1-x  \int _0^1\:\left(\int _{-x}^{1-x} \arcsin\sqrt{x+y}\:dy\right)dx ","['calculus', 'multivariable-calculus', 'solution-verification', 'multiple-integral']"
40,"Global maxima and minima of $f(x,y)=x^2 + y^2 + \beta xy + x + 2y$",Global maxima and minima of,"f(x,y)=x^2 + y^2 + \beta xy + x + 2y","I am self-learning basic optimization theory and algorithms from An Introduction to Optimization by Chong and Zak. I would like someone to verify my solution to this problem, on finding the minimizer/maximizer of a function of two variables, or any tips/hint to proceed ahead. For each value of the scalar $\beta$ , find the set of all stationary points of the following two variables $x$ and $y$ $$f(x,y) = x^2 + y^2 + \beta xy + x + 2y$$ Which of those stationary points are local minima? Which are global minima and why? Does this function have a global maximum for some value of $\beta$ . Solution. We have: \begin{align*} f(x,y) &= x^2 + y^2 + \beta xy + x +2y\\ f_x(x,y) &= 2x+\beta y + 1\\ f_y(x,y) &= \beta x + 2y + 2\\ f_{xx}(x,y) &= 2\\ f_{xy}(x,y) &= \beta \\ f_{yy}(x,y) &= 2 \end{align*} By the first order necessary condition(FONC) for optimality, we know that if $\nabla f(\mathbf{x})=0$ , then $\mathbf{x}$ is a critical point. Thus, \begin{align*} f_x(x,y) &= 2x+\beta y + 1 = 0\\ f_y(x,y) &= \beta x + 2y + 2 = 0 \end{align*} Solving for $x$ and $y$ , we find that: \begin{align*} x = \frac{\begin{array}{|cc|} -1 & \beta \\ -2 & 2 \end{array}}{\begin{array}{|cc|} 2 & \beta \\ \beta & 2 \end{array}}=\frac{-2+2\beta}{4-\beta^2}=\frac{2\beta-2}{4 -\beta^2} \end{align*} \begin{align*} y = \frac{\begin{array}{|cc|} 2 & -1 \\ \beta & -2 \end{array}}{\begin{array}{|cc|} 2 & \beta \\ \beta & 2 \end{array}}=\frac{-4+\beta}{4-\beta^2}=\frac{\beta -4}{4 - \beta^2} \end{align*} The second order necessary and sufficient conditions for optimality are based on the sign of the quadratic form $Q(\mathbf{h})=\mathbf{h}^T \cdot Hf(\mathbf{a}) \cdot \mathbf{h}$ . The Hessian of $f$ is given by, $$Hf(\mathbf{x})=\begin{array}{|c c|} 2 & \beta \\ \beta & 2 \end{array}$$ Thus, $d_1 = 2 > 0$ and $d_2 = 4 - \beta^2$ . Thus, $f$ has a local minimizer if and only if $4 - \beta^2 > 0$ . $g(\beta) = 4 - \beta^2$ is a downward facing parabola. So, the values of this expression positive, if and only if $-2 < \beta < 2$ . The function $f$ has no global maximum. Question. How do I find the actual global minima?","I am self-learning basic optimization theory and algorithms from An Introduction to Optimization by Chong and Zak. I would like someone to verify my solution to this problem, on finding the minimizer/maximizer of a function of two variables, or any tips/hint to proceed ahead. For each value of the scalar , find the set of all stationary points of the following two variables and Which of those stationary points are local minima? Which are global minima and why? Does this function have a global maximum for some value of . Solution. We have: By the first order necessary condition(FONC) for optimality, we know that if , then is a critical point. Thus, Solving for and , we find that: The second order necessary and sufficient conditions for optimality are based on the sign of the quadratic form . The Hessian of is given by, Thus, and . Thus, has a local minimizer if and only if . is a downward facing parabola. So, the values of this expression positive, if and only if . The function has no global maximum. Question. How do I find the actual global minima?","\beta x y f(x,y) = x^2 + y^2 + \beta xy + x + 2y \beta \begin{align*}
f(x,y) &= x^2 + y^2 + \beta xy + x +2y\\
f_x(x,y) &= 2x+\beta y + 1\\
f_y(x,y) &= \beta x + 2y + 2\\
f_{xx}(x,y) &= 2\\
f_{xy}(x,y) &= \beta \\
f_{yy}(x,y) &= 2
\end{align*} \nabla f(\mathbf{x})=0 \mathbf{x} \begin{align*}
f_x(x,y) &= 2x+\beta y + 1 = 0\\
f_y(x,y) &= \beta x + 2y + 2 = 0
\end{align*} x y \begin{align*}
x = \frac{\begin{array}{|cc|}
-1 & \beta \\
-2 & 2
\end{array}}{\begin{array}{|cc|}
2 & \beta \\
\beta & 2
\end{array}}=\frac{-2+2\beta}{4-\beta^2}=\frac{2\beta-2}{4 -\beta^2}
\end{align*} \begin{align*}
y = \frac{\begin{array}{|cc|}
2 & -1 \\
\beta & -2
\end{array}}{\begin{array}{|cc|}
2 & \beta \\
\beta & 2
\end{array}}=\frac{-4+\beta}{4-\beta^2}=\frac{\beta -4}{4 - \beta^2}
\end{align*} Q(\mathbf{h})=\mathbf{h}^T \cdot Hf(\mathbf{a}) \cdot \mathbf{h} f Hf(\mathbf{x})=\begin{array}{|c c|}
2 & \beta \\
\beta & 2
\end{array} d_1 = 2 > 0 d_2 = 4 - \beta^2 f 4 - \beta^2 > 0 g(\beta) = 4 - \beta^2 -2 < \beta < 2 f","['multivariable-calculus', 'optimization', 'solution-verification', 'maxima-minima', 'nonlinear-optimization']"
41,Line integral of a Vector Field around a Closed path,Line integral of a Vector Field around a Closed path,,"This is just a little question. Suppose you want to evaluate an integral around a closed path formed by a curve $C(t) $ (only one curve), I suspect that the result would be $0$ , because you will do an integral from the point $P$ to the same point. so for example if $P=C(a)$ , then your integral is $$\int_CF=\int_a ^a F(C(t))\cdot C'(t)\,dt = 0$$ Is that true?","This is just a little question. Suppose you want to evaluate an integral around a closed path formed by a curve (only one curve), I suspect that the result would be , because you will do an integral from the point to the same point. so for example if , then your integral is Is that true?","C(t)  0 P P=C(a) \int_CF=\int_a ^a F(C(t))\cdot C'(t)\,dt = 0","['calculus', 'multivariable-calculus', 'vector-analysis']"
42,"A nonnegative function $f(a,b)\geq 0$",A nonnegative function,"f(a,b)\geq 0","Let $c$ be a fixed positive real  umber such that $c\geq 1.$ If $a\geq c^m, b\geq c, $ where $m$ is any positive integer, then is $$f(a,b)=c(ab+1)(a-c^m)(b-c)+c(ab+1)(a-c^m)(b+1)+c^m(b-c)(ab+1)(a+1)-(ab-c^{m+1})(a+1)(b+1)\geq 0?$$ We can check the validity for the two  cases $a=c^m,$ or $b=c$ easily. In fact the equality in the above claim  occurs at $(a, b)=(c^m,c).$ Kindly suggest me the way  to establish $f(a,b)\geq 0$ if it is true.","Let be a fixed positive real  umber such that If where is any positive integer, then is We can check the validity for the two  cases or easily. In fact the equality in the above claim  occurs at Kindly suggest me the way  to establish if it is true.","c c\geq 1. a\geq c^m, b\geq c,  m f(a,b)=c(ab+1)(a-c^m)(b-c)+c(ab+1)(a-c^m)(b+1)+c^m(b-c)(ab+1)(a+1)-(ab-c^{m+1})(a+1)(b+1)\geq 0? a=c^m, b=c (a, b)=(c^m,c). f(a,b)\geq 0","['multivariable-calculus', 'inequality', 'real-numbers']"
43,"Find the $\max$ and the $\min$ of $f(x,y)=xy$ subject to $g(x,y)=\frac{x^2}{a^2}+\frac{y^2}{b^2}-1=0$, using Lagrange multipliers","Find the  and the  of  subject to , using Lagrange multipliers","\max \min f(x,y)=xy g(x,y)=\frac{x^2}{a^2}+\frac{y^2}{b^2}-1=0","Find the $\max$ and the $\min$ of $$f(x,y)=xy$$ subject to the constraint $$g(x,y)=\frac{x^2}{a^2}+\frac{y^2}{b^2}-1=0$$ using Lagrange multipliers. My Attempt:",Find the and the of subject to the constraint using Lagrange multipliers. My Attempt:,"\max \min f(x,y)=xy g(x,y)=\frac{x^2}{a^2}+\frac{y^2}{b^2}-1=0","['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
44,Stokes' Theorem - cylindrical coordinates,Stokes' Theorem - cylindrical coordinates,,"I'm currently having an issue with verifying the validity of Stokes' Theorem on a particular problem. I can solve the problem by using Stokes' theorem to turn a surface integral of the curl of a vector into a line integral of a vector. However, when I try to check the validity of this by explicitly computing the surface integral of the curl, I am apparently faced with integrating a zero vector. We have the following where we use the cylindrical coordinate system $(\rho,\varphi,z)$ and $R,\Gamma$ are constants: \begin{align*}  \boldsymbol{u} &= u_\varphi(\rho)\hat{\boldsymbol{\varphi}}  \end{align*} \begin{align*} &u_\varphi(\rho) = \begin{cases} \frac{\Gamma \rho}{2 \pi R^2} &\rho \leq R \\ \frac{\Gamma}{2 \pi \rho} & \rho > R \\ \end{cases} \end{align*} \begin{align} \boldsymbol{\omega} &= \nabla \times \boldsymbol{u} = (0,0,\omega(\rho)) \end{align} \begin{align*}  &\omega(\rho) = \begin{cases} \frac{\Gamma}{2 \pi \rho} &\rho \leq R \\ 0 & \rho > R \\ \end{cases} \end{align*} We are given \begin{align} \Omega(\rho) = \int_{D(0,\rho)} \boldsymbol{\omega} \cdot \ dS \end{align} where $D(0,\rho)$ is a disc of radius $\rho$ centered at the origin. We are asked to show that: \begin{align*}  &\Omega(\rho) = \begin{cases} \Gamma(\frac{\rho}{R})^2 &\rho \leq R \\ \Gamma & \rho > R \\ \end{cases} \end{align*} where $D(0,\rho)$ is a disc of radius $\rho$ centered at the origin. While I can find $\Omega$ for $\rho \leq R$ , I'm having an issue directly computing it when $\rho > R$ . I have used Stokes' theorem to change the integral to $\int_{0}^{2 \pi} \boldsymbol{u} \cdot d\boldsymbol{x}$ which then gives me: \begin{align*} \Omega(\rho) &= \int_{0}^{2 \pi} (0, \frac{\Gamma }{2 \pi \rho},0) \cdot d \boldsymbol{x} \\  &= \int_{0}^{2 \pi} \frac{\Gamma }{2 \pi \rho} \rho d \varphi \\ &= \frac{\Gamma }{2 \pi } [ \varphi ]_{0}^{2 \pi} = \Gamma \end{align*} However, when I try to compute this directly using: \begin{align*} \Omega(\rho) = \int_{D(0,\rho)} \boldsymbol{\omega} \cdot \ dS = \int_{D(0,\rho)} (\nabla \times\boldsymbol{u}) \cdot \ dS \end{align*} I get $(\nabla \times \boldsymbol{u})=(0,0,0)$ which would give $\Omega(\rho)= 0 \quad \forall \ \rho > R$ We are also provided with a formula for computing the curl of a vector field expressed in cylindral polar coordiantes $(\rho, \varphi, z)$ : \begin{align*}   \nabla \times \boldsymbol{A} = \frac{1}{\rho} \begin{vmatrix} \hat{\boldsymbol{\rho}} & \rho \hat{\boldsymbol{\varphi}} & \hat{\boldsymbol{z}} \\ \partial_{\rho} & \partial_{\varphi} & \partial_{z} \\ A_{\rho} & \rho A_{\varphi} & A_{z} \end{vmatrix} \end{align*} Any help on how I can get the two methods to agree or observations on where I have gone wrong would be much appreciated, thank you!","I'm currently having an issue with verifying the validity of Stokes' Theorem on a particular problem. I can solve the problem by using Stokes' theorem to turn a surface integral of the curl of a vector into a line integral of a vector. However, when I try to check the validity of this by explicitly computing the surface integral of the curl, I am apparently faced with integrating a zero vector. We have the following where we use the cylindrical coordinate system and are constants: We are given where is a disc of radius centered at the origin. We are asked to show that: where is a disc of radius centered at the origin. While I can find for , I'm having an issue directly computing it when . I have used Stokes' theorem to change the integral to which then gives me: However, when I try to compute this directly using: I get which would give We are also provided with a formula for computing the curl of a vector field expressed in cylindral polar coordiantes : Any help on how I can get the two methods to agree or observations on where I have gone wrong would be much appreciated, thank you!","(\rho,\varphi,z) R,\Gamma \begin{align*} 
\boldsymbol{u} &= u_\varphi(\rho)\hat{\boldsymbol{\varphi}} 
\end{align*} \begin{align*}
&u_\varphi(\rho) = \begin{cases}
\frac{\Gamma \rho}{2 \pi R^2} &\rho \leq R \\
\frac{\Gamma}{2 \pi \rho} & \rho > R \\
\end{cases}
\end{align*} \begin{align}
\boldsymbol{\omega} &= \nabla \times \boldsymbol{u} = (0,0,\omega(\rho))
\end{align} \begin{align*} 
&\omega(\rho) = \begin{cases}
\frac{\Gamma}{2 \pi \rho} &\rho \leq R \\
0 & \rho > R \\
\end{cases}
\end{align*} \begin{align}
\Omega(\rho) = \int_{D(0,\rho)} \boldsymbol{\omega} \cdot \ dS
\end{align} D(0,\rho) \rho \begin{align*} 
&\Omega(\rho) = \begin{cases}
\Gamma(\frac{\rho}{R})^2 &\rho \leq R \\
\Gamma & \rho > R \\
\end{cases}
\end{align*} D(0,\rho) \rho \Omega \rho \leq R \rho > R \int_{0}^{2 \pi} \boldsymbol{u} \cdot d\boldsymbol{x} \begin{align*}
\Omega(\rho) &= \int_{0}^{2 \pi} (0, \frac{\Gamma }{2 \pi \rho},0) \cdot d \boldsymbol{x} \\
 &= \int_{0}^{2 \pi} \frac{\Gamma }{2 \pi \rho} \rho d \varphi \\
&= \frac{\Gamma }{2 \pi } [ \varphi ]_{0}^{2 \pi} = \Gamma
\end{align*} \begin{align*}
\Omega(\rho) = \int_{D(0,\rho)} \boldsymbol{\omega} \cdot \ dS = \int_{D(0,\rho)} (\nabla \times\boldsymbol{u}) \cdot \ dS
\end{align*} (\nabla \times \boldsymbol{u})=(0,0,0) \Omega(\rho)= 0 \quad \forall \ \rho > R (\rho, \varphi, z) \begin{align*} 
 \nabla \times \boldsymbol{A} = \frac{1}{\rho} \begin{vmatrix}
\hat{\boldsymbol{\rho}} & \rho \hat{\boldsymbol{\varphi}} & \hat{\boldsymbol{z}} \\
\partial_{\rho} & \partial_{\varphi} & \partial_{z} \\
A_{\rho} & \rho A_{\varphi} & A_{z}
\end{vmatrix}
\end{align*}","['multivariable-calculus', 'vector-analysis', 'stokes-theorem', 'cylindrical-coordinates']"
45,Bounding integrals on a `moving' domain,Bounding integrals on a `moving' domain,,"I've been reading through Schoen-Yau's proof of the positive mass theorem and find myself stuck on a particular estimate they prove. It seems to be simple multivariable calculus, but I haven't been able to prove it. Here's a simplified setup: Let $S\subset\mathbb{R}^3$ be a non-compact surface and consider the set of exhautions $S_{\sigma}=S\cap B_\sigma(0)$ , where $B_\sigma(0)$ is the ball of radius $\sigma$ with centre $0$ . Clearly, $S_{\sigma_1}\subset S_{\sigma_2}$ if $\sigma_1<\sigma_2$ and $\{S_\sigma\}$ exhausts $S$ as $\sigma\rightarrow\infty$ . They use the following argument to bound the integral, $$ \int_S \frac{1}{1+r^a}=\int_{S_{\sigma}}\frac{1}{1+r^a}+\int_{\sigma_0}^\infty\left(\frac{d}{dt}\int_{S_t}\frac{1}{1+r^a}\right)dt\leq Area(S_{\sigma_0})+\int_{\sigma_0}^\infty\frac{1}{1+t^a}\left(\frac{d}{dt}Area(S_t)\right)dt. $$ Here $a>2$ and $\sigma_0>0$ are constants. The first equality is just an application of the fundamental theorem of calculus. The $Area(S_\sigma)$ is clear. The term that is bothering me is the second term involving $\frac{d}{dt}Area(S_t)$ . I'm not sure how they got that bound. Are they using some kind of Leibniz rule type formula?","I've been reading through Schoen-Yau's proof of the positive mass theorem and find myself stuck on a particular estimate they prove. It seems to be simple multivariable calculus, but I haven't been able to prove it. Here's a simplified setup: Let be a non-compact surface and consider the set of exhautions , where is the ball of radius with centre . Clearly, if and exhausts as . They use the following argument to bound the integral, Here and are constants. The first equality is just an application of the fundamental theorem of calculus. The is clear. The term that is bothering me is the second term involving . I'm not sure how they got that bound. Are they using some kind of Leibniz rule type formula?","S\subset\mathbb{R}^3 S_{\sigma}=S\cap B_\sigma(0) B_\sigma(0) \sigma 0 S_{\sigma_1}\subset S_{\sigma_2} \sigma_1<\sigma_2 \{S_\sigma\} S \sigma\rightarrow\infty 
\int_S \frac{1}{1+r^a}=\int_{S_{\sigma}}\frac{1}{1+r^a}+\int_{\sigma_0}^\infty\left(\frac{d}{dt}\int_{S_t}\frac{1}{1+r^a}\right)dt\leq Area(S_{\sigma_0})+\int_{\sigma_0}^\infty\frac{1}{1+t^a}\left(\frac{d}{dt}Area(S_t)\right)dt.
 a>2 \sigma_0>0 Area(S_\sigma) \frac{d}{dt}Area(S_t)","['real-analysis', 'multivariable-calculus', 'differential-geometry', 'riemannian-geometry']"
46,"Differentiability of $f : \mathbb{R}^n \rightarrow \mathbb{R}^n$, where $|x-y - (f(x) -f(y))| \leq \frac{1}{2}|x-y|$","Differentiability of , where",f : \mathbb{R}^n \rightarrow \mathbb{R}^n |x-y - (f(x) -f(y))| \leq \frac{1}{2}|x-y|,"Question: If $f : \mathbb{R}^n \rightarrow \mathbb{R}^n$ is a continuous function that satifies $|x-y - (f(x) -f(y))| \leq \frac{1}{2}|x-y|$ for all $x,y \in \mathbb{R}^n$ , prove that $f$ is differentiable at 0. My attempt: Let $y=0$ . Then $|(f(x)-f(0)) - x| \leq \frac{1}{2}|x|$ . Then $\frac{|(f(x)-f(0)) - x|}{|x|} \leq \frac{1}{2}$ I know I need to show that $\frac{|(f(x)-f(0)) - Mx|}{|x|} \rightarrow 0$ for some linear transformation $M$ , but I'm at a loss as to how to use the condition. In particular, the $\frac{1}{2}$ is fixed, so I don't see how to proceed.","Question: If is a continuous function that satifies for all , prove that is differentiable at 0. My attempt: Let . Then . Then I know I need to show that for some linear transformation , but I'm at a loss as to how to use the condition. In particular, the is fixed, so I don't see how to proceed.","f : \mathbb{R}^n \rightarrow \mathbb{R}^n |x-y - (f(x) -f(y))| \leq \frac{1}{2}|x-y| x,y \in \mathbb{R}^n f y=0 |(f(x)-f(0)) - x| \leq \frac{1}{2}|x| \frac{|(f(x)-f(0)) - x|}{|x|} \leq \frac{1}{2} \frac{|(f(x)-f(0)) - Mx|}{|x|} \rightarrow 0 M \frac{1}{2}","['multivariable-calculus', 'derivatives']"
47,Gradient of norm [duplicate],Gradient of norm [duplicate],,"This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 3 years ago . How can I compute the gradient of f(x)= $||Ax-b||_{R^{-1}}^2$ I'm also confused about how to compute the gradient of g(x) = $||y-Ax||$ using the chain rule. I think the first step to take the gradient of g(x) would be $\nabla(g(x)) = \frac{d(y-Ax)^\top}{dx}(y-Ax)+ \frac{d(y-Ax)}{dx}(y-Ax)^\top$ . However, I'm unsure of how to take the derivative of (y-Ax) or the derivative of $(y-Ax)^\top$ . Any help is highly appreciated.","This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 3 years ago . How can I compute the gradient of f(x)= I'm also confused about how to compute the gradient of g(x) = using the chain rule. I think the first step to take the gradient of g(x) would be . However, I'm unsure of how to take the derivative of (y-Ax) or the derivative of . Any help is highly appreciated.",||Ax-b||_{R^{-1}}^2 ||y-Ax|| \nabla(g(x)) = \frac{d(y-Ax)^\top}{dx}(y-Ax)+ \frac{d(y-Ax)}{dx}(y-Ax)^\top (y-Ax)^\top,"['matrices', 'multivariable-calculus', 'derivatives', 'vector-analysis', 'matrix-calculus']"
48,"Bring the double integral to a single integral: $\iint\limits_{|x| + |y| \le 1} f(x + y) \, dx \, dy$",Bring the double integral to a single integral:,"\iint\limits_{|x| + |y| \le 1} f(x + y) \, dx \, dy","Bring the double integral to a single integral: $$\iint\limits_{|x| + |y| \le 1} f(x + y) \,dx \,dy$$ The substitution $u = x + y$ seems to be the only way since we should get rid of several variables in the function argument (also, I did $-x+y = v$ for the second variable). First, I split the integral into four: $$\iint\limits_{|x| + |y| \le 1} f(x + y) \,dx \,dy =  \iint\limits_{0\le x+y\le1} f(x+y)\,dx\,dy + \iint\limits_{-1\le x+y\le 0} f(x+y)\,dx\,dy +  \iint\limits_{0\le -x+y\le1} f(x+y)\,dx\,dy +  \iint\limits_{-1\le -x+y\le0} f(x+y)\,dx\,dy$$ Now, with the substitution, we can deal with the first two: $$\iint\limits_{0\le x+y\le1} f(x+y)\,dx\,dy +  \iint\limits_{-1\le x+y\le 0} f(x+y)\,dx\,dy =  \int_0^1 \frac 12 f(u)du + \int_{-1}^0 \frac 12 f(u)du = \frac 12 \int_{-1}^1 f(u)\,du$$ where $\frac{1}{2}$ is the Jacobian. I don't know if I'm going right and do not know how to deal with the next integrals.","Bring the double integral to a single integral: The substitution seems to be the only way since we should get rid of several variables in the function argument (also, I did for the second variable). First, I split the integral into four: Now, with the substitution, we can deal with the first two: where is the Jacobian. I don't know if I'm going right and do not know how to deal with the next integrals.","\iint\limits_{|x| + |y| \le 1} f(x + y) \,dx \,dy u = x + y -x+y = v \iint\limits_{|x| + |y| \le 1} f(x + y) \,dx \,dy = 
\iint\limits_{0\le x+y\le1} f(x+y)\,dx\,dy +
\iint\limits_{-1\le x+y\le 0} f(x+y)\,dx\,dy + 
\iint\limits_{0\le -x+y\le1} f(x+y)\,dx\,dy + 
\iint\limits_{-1\le -x+y\le0} f(x+y)\,dx\,dy \iint\limits_{0\le x+y\le1} f(x+y)\,dx\,dy + 
\iint\limits_{-1\le x+y\le 0} f(x+y)\,dx\,dy = 
\int_0^1 \frac 12 f(u)du + \int_{-1}^0 \frac 12 f(u)du = \frac 12 \int_{-1}^1 f(u)\,du \frac{1}{2}","['calculus', 'integration', 'multivariable-calculus', 'multiple-integral']"
49,Mapping continuously differentiable at a point,Mapping continuously differentiable at a point,,"I am reading Coleman's Calculus on Normed Vector Spaces. He defines what it means for a mapping $f: O \to F$ from an open subset $O$ of a normed vector space $E$ to a normed vector space $F$ to be differentiable at a point $x \in O$ . Also, such a mapping is differentiable if it is differentiable at every point in its domain. He than defines that such a mapping is continuously differentiable (or of class $C^1$ ) when it is differentiable and its differential mapping $f': O \to \mathcal{L}(E,F)$ is continuous. He does not define what it means for a mapping to be continuously differentiable at a point (at least I could not find a definition), so I am wondering what such definition would be like. This is what I first thought of: A mapping $f: O \to F$ is continuously differentiable at a point $x \in O$ if there is a neighborhood $N$ of $x$ such that $f$ is differentiable at $N$ and $f': N \to \mathcal{L}(E,F)$ is continuous at $x$ . My questions are: Is it true that a mapping is differentiable at $x$ if, and only if, it has all partial differentials defined at a neigborhood of $x$ and continuous at $x$ (but not necessarily continuous at a neighborhood of $x$ ? Is this definition useful in any other way? Is it necessary to have $f$ differentiable at a neighborhood to talk about continuity at $x$ ? This seemed more intuitive to me but I am not sure this is needed.","I am reading Coleman's Calculus on Normed Vector Spaces. He defines what it means for a mapping from an open subset of a normed vector space to a normed vector space to be differentiable at a point . Also, such a mapping is differentiable if it is differentiable at every point in its domain. He than defines that such a mapping is continuously differentiable (or of class ) when it is differentiable and its differential mapping is continuous. He does not define what it means for a mapping to be continuously differentiable at a point (at least I could not find a definition), so I am wondering what such definition would be like. This is what I first thought of: A mapping is continuously differentiable at a point if there is a neighborhood of such that is differentiable at and is continuous at . My questions are: Is it true that a mapping is differentiable at if, and only if, it has all partial differentials defined at a neigborhood of and continuous at (but not necessarily continuous at a neighborhood of ? Is this definition useful in any other way? Is it necessary to have differentiable at a neighborhood to talk about continuity at ? This seemed more intuitive to me but I am not sure this is needed.","f: O \to F O E F x \in O C^1 f': O \to \mathcal{L}(E,F) f: O \to F x \in O N x f N f': N \to \mathcal{L}(E,F) x x x x x f x","['multivariable-calculus', 'differential']"
50,Sphere falling in viscous fluid boundary condition,Sphere falling in viscous fluid boundary condition,,"A sphere, radius $a$ , falls at constant velocity $U_0$ through an incompressible viscous fluid. I am told to assume that \begin{equation} \mathbf{u} = U_0\mathbf{e}_z + \tilde{\mathbf{u}}(\mathbf{x}) \end{equation} where \begin{equation} \tilde{\mathbf{u}}(\mathbf{x})=\nabla\times\nabla\times(U_0f(r)\mathbf{e}_z). \end{equation} From this I have shown that \begin{equation} \tilde{\mathbf{u}}(\mathbf{x})=-U_0\Delta f \mathbf{e}_z \end{equation} and by taking the curl of the equations for Stokes flow, \begin{equation} \Delta^2 f= \text{ constant.} \end{equation} where $\Delta^2 f=\Delta(\Delta f)$ . I also know that \begin{equation} \Delta f\rightarrow0 \text{ as } r\rightarrow\infty. \end{equation} Here is where I am confused I need to show that $f(r)=Ar+\frac{B}{r}$ . I tried to do so by the Ansatz $f=r^a$ and found that $a=-1,0,1,2,4$ are all suitable for $\Delta^2 f=$ constant to hold. So $f(r)=Ar+\frac{B}{r}+C+Dr^2+Er^4.$ To satisfy $\Delta f\rightarrow0 \text{ as } r\rightarrow\infty$ , I set $D=E=0$ . But I cannot explain why $C=0$ . Why is this true? I also don't know how to set $A,B$ I think that $\textbf{u}=U_0\textbf{e}_z \text{ at } r=a$ for the no-slip condition, which sets $\tilde{\textbf{u}}=-U_0\Delta f\textbf{e}_z=0 \text{ at } r=a \implies A=0$ . So now I have $f(r)=\frac{B}{r}$ , and somehow I have to use the fact that $\nabla\times\textbf{e}_\phi=\frac{cot\theta}{r}\textbf{e}_r-\frac{1}{r}\textbf{e}_\theta$ to find $B$ . I am completely lost here.","A sphere, radius , falls at constant velocity through an incompressible viscous fluid. I am told to assume that where From this I have shown that and by taking the curl of the equations for Stokes flow, where . I also know that Here is where I am confused I need to show that . I tried to do so by the Ansatz and found that are all suitable for constant to hold. So To satisfy , I set . But I cannot explain why . Why is this true? I also don't know how to set I think that for the no-slip condition, which sets . So now I have , and somehow I have to use the fact that to find . I am completely lost here.","a U_0 \begin{equation}
\mathbf{u} = U_0\mathbf{e}_z + \tilde{\mathbf{u}}(\mathbf{x})
\end{equation} \begin{equation}
\tilde{\mathbf{u}}(\mathbf{x})=\nabla\times\nabla\times(U_0f(r)\mathbf{e}_z).
\end{equation} \begin{equation}
\tilde{\mathbf{u}}(\mathbf{x})=-U_0\Delta f \mathbf{e}_z
\end{equation} \begin{equation}
\Delta^2 f= \text{ constant.}
\end{equation} \Delta^2 f=\Delta(\Delta f) \begin{equation}
\Delta f\rightarrow0 \text{ as } r\rightarrow\infty.
\end{equation} f(r)=Ar+\frac{B}{r} f=r^a a=-1,0,1,2,4 \Delta^2 f= f(r)=Ar+\frac{B}{r}+C+Dr^2+Er^4. \Delta f\rightarrow0 \text{ as } r\rightarrow\infty D=E=0 C=0 A,B \textbf{u}=U_0\textbf{e}_z \text{ at } r=a \tilde{\textbf{u}}=-U_0\Delta f\textbf{e}_z=0 \text{ at } r=a \implies A=0 f(r)=\frac{B}{r} \nabla\times\textbf{e}_\phi=\frac{cot\theta}{r}\textbf{e}_r-\frac{1}{r}\textbf{e}_\theta B","['multivariable-calculus', 'mathematical-physics', 'fluid-dynamics']"
51,Shouldn't the chain rule be applied instead of the product rule in linear operators?,Shouldn't the chain rule be applied instead of the product rule in linear operators?,,"If I have the linear operators $A(x)$ and $B(x)$ , and i want to calculate the derivative of $A(B(x))$ , shouldn't it be: $$A(B(x))'= A'(B(x))B'(x)$$ Instead of $(AB(x))'= A'(x)B(x) + A(x)B'(x)$ ? This may not make much sense, but if you think about this linear operators as matrices, when we have $AB\vec{x} $ , should't we undestand it as "" $A(B(x))$ "", instead of "" $A(x)B(x)$ ""? After all A is acting on $x$ transformed by ""B"", not in $x$ itself. I am making this stupid question because I am trying to study QM through Functional analysis and operator theory, but when I came to this example I thought about something that I hadn't thought before and now I am in doubt, sorry for the silly question. It came from this check @littleO : ""Let $A(x)$ be an operator (It probably means a linear operator,but you know how physicists are!) dependant on a continuous variable x, define its variable by: $$\frac{dA(x)}{dx} \equiv  A'(x) = \lim_{\varepsilon \rightarrow 0} \frac{A(x+\varepsilon)-A(x)}{\varepsilon}$$ If $A$ has an inverse show that $$\frac{dA^{-1}}{dx}= - A^{-1}A'A^{-1}$$ Show also that $$\frac{dAB}{dx}= A'B+ AB'$$ ""","If I have the linear operators and , and i want to calculate the derivative of , shouldn't it be: Instead of ? This may not make much sense, but if you think about this linear operators as matrices, when we have , should't we undestand it as "" "", instead of "" ""? After all A is acting on transformed by ""B"", not in itself. I am making this stupid question because I am trying to study QM through Functional analysis and operator theory, but when I came to this example I thought about something that I hadn't thought before and now I am in doubt, sorry for the silly question. It came from this check @littleO : ""Let be an operator (It probably means a linear operator,but you know how physicists are!) dependant on a continuous variable x, define its variable by: If has an inverse show that Show also that """,A(x) B(x) A(B(x)) A(B(x))'= A'(B(x))B'(x) (AB(x))'= A'(x)B(x) + A(x)B'(x) AB\vec{x}  A(B(x)) A(x)B(x) x x A(x) \frac{dA(x)}{dx} \equiv  A'(x) = \lim_{\varepsilon \rightarrow 0} \frac{A(x+\varepsilon)-A(x)}{\varepsilon} A \frac{dA^{-1}}{dx}= - A^{-1}A'A^{-1} \frac{dAB}{dx}= A'B+ AB',"['linear-algebra', 'multivariable-calculus', 'operator-theory']"
52,Matrix function decomposition of the form $A(\theta)=B(\theta)^\top Q B(\theta)$,Matrix function decomposition of the form,A(\theta)=B(\theta)^\top Q B(\theta),"Suppose I have a matrix function $A:\mathbb{R}^n\to\mathbb{R}^{n\times n}$ , for which I know the following properties hold: $A(\theta)$ is real, symmetric and bounded for all $\theta$ $A(\theta)$ is a $\mathcal{C}_1$ function, i.e., $A(\theta)$ and $\frac{\partial A(\theta)}{\partial \theta}$ are continuous. There exist two constants $0<c_1\le c_2$ such that for all $\theta$ , $c_1I\preccurlyeq A(\theta)\preccurlyeq c_2 I$ . (so it is positive definite for all $\theta$ ) Is it true that this matrix can always be represented in the following form? $$A(\theta)=B(\theta)^\top Q B(\theta),$$ where $0\prec Q\in\mathbb{R}^{n\times n}$ , $B:\mathbb{R}^n\to\mathbb{R}^{n\times n}$ and $\det(B(\theta))\neq 0$ for all $\theta$ . My intuition says yes, just take power to a half (so $B(\theta) = A(\theta)^{\frac{1}{2}}$ and $Q=I$ . But, I cannot find any reference that 'exploits' this fact. So, my question is, could you give a reference that shows/claims this, or prove this yourself (:P)? Edit: I was also thinking about eigenfunction of the matrix function. However, I'm not sure how this connects exactly to the decomposition above...","Suppose I have a matrix function , for which I know the following properties hold: is real, symmetric and bounded for all is a function, i.e., and are continuous. There exist two constants such that for all , . (so it is positive definite for all ) Is it true that this matrix can always be represented in the following form? where , and for all . My intuition says yes, just take power to a half (so and . But, I cannot find any reference that 'exploits' this fact. So, my question is, could you give a reference that shows/claims this, or prove this yourself (:P)? Edit: I was also thinking about eigenfunction of the matrix function. However, I'm not sure how this connects exactly to the decomposition above...","A:\mathbb{R}^n\to\mathbb{R}^{n\times n} A(\theta) \theta A(\theta) \mathcal{C}_1 A(\theta) \frac{\partial A(\theta)}{\partial \theta} 0<c_1\le c_2 \theta c_1I\preccurlyeq A(\theta)\preccurlyeq c_2 I \theta A(\theta)=B(\theta)^\top Q B(\theta), 0\prec Q\in\mathbb{R}^{n\times n} B:\mathbb{R}^n\to\mathbb{R}^{n\times n} \det(B(\theta))\neq 0 \theta B(\theta) = A(\theta)^{\frac{1}{2}} Q=I","['matrices', 'multivariable-calculus', 'functions', 'reference-request', 'matrix-decomposition']"
53,Using a 'Similarity Variable' to transform a PDE into an ODE?,Using a 'Similarity Variable' to transform a PDE into an ODE?,,"I have a PDE: $$\frac{\partial y}{\partial t}=\alpha\frac{\partial^2y}{\partial x^2}\tag{1}$$ So we're looking for a function: $$y=f(x,t)$$ The following substitution with a Similarity Variable then transforms the PDE into a simple ODE: $$z=\frac{x}{2\sqrt{\alpha t}}\tag{2}$$ $$\frac{\mathrm{d}^2y(z)}{\mathrm{d}z^2}=-2z \frac{\mathrm{d}y(z)}{\mathrm{d}z}\tag{3}$$ The latter solves easily to: $$y=c_1\int e^{-z^2}\mathrm{d}z+c_2$$ The trouble is I can't seem to carry out this substitution to get from $(1)$ to $(3)$ , using $(2)$ . I thought of extracting $x$ and $t$ from $(2)$ and differentiating them as $\mathrm{d}t$ and $\mathrm{d}x^2$ but couldn't make that work. So how to make this substitution work?","I have a PDE: So we're looking for a function: The following substitution with a Similarity Variable then transforms the PDE into a simple ODE: The latter solves easily to: The trouble is I can't seem to carry out this substitution to get from to , using . I thought of extracting and from and differentiating them as and but couldn't make that work. So how to make this substitution work?","\frac{\partial y}{\partial t}=\alpha\frac{\partial^2y}{\partial x^2}\tag{1} y=f(x,t) z=\frac{x}{2\sqrt{\alpha t}}\tag{2} \frac{\mathrm{d}^2y(z)}{\mathrm{d}z^2}=-2z \frac{\mathrm{d}y(z)}{\mathrm{d}z}\tag{3} y=c_1\int e^{-z^2}\mathrm{d}z+c_2 (1) (3) (2) x t (2) \mathrm{d}t \mathrm{d}x^2","['multivariable-calculus', 'partial-differential-equations', 'partial-derivative']"
54,What Does the Antiderivative of This Multivariable Function Mean?,What Does the Antiderivative of This Multivariable Function Mean?,,"In this video, the instructor uses Undetermined Coefficients (or at least what would be called Undetermined Coefficients in the ODE world) to find a particular solution to the PDE $U_t + cU_x = f(x + ct)$ , where $c$ is a parameter present in the original Wave Equation. The guess used is $U = aF(x + ct)$ , where $a$ is the constant to be determined, and $F$ is said to be the antiderivative of $f$ .  However, I don't recall from MV Calc any single notion of an antiderivative of a multivariable function, only antiderivatives taken with respect to one of the function's inputs.  The closest thing I can think of would be a potential function, where one begins with a vector field and pieces together a single function whose gradient is that vector field, but I don't think that matches what's going on here. Later in the problem, he seems to take $\frac{\partial F}{\partial x}$ and $\frac{\partial F}{\partial t}$ separately, in both cases yielding $f$ , but it seems to me that baring certain exceptional cases, it is not possible to find a function whose partial with respect to each of its variables yield a single, given function.  What is going on here?","In this video, the instructor uses Undetermined Coefficients (or at least what would be called Undetermined Coefficients in the ODE world) to find a particular solution to the PDE , where is a parameter present in the original Wave Equation. The guess used is , where is the constant to be determined, and is said to be the antiderivative of .  However, I don't recall from MV Calc any single notion of an antiderivative of a multivariable function, only antiderivatives taken with respect to one of the function's inputs.  The closest thing I can think of would be a potential function, where one begins with a vector field and pieces together a single function whose gradient is that vector field, but I don't think that matches what's going on here. Later in the problem, he seems to take and separately, in both cases yielding , but it seems to me that baring certain exceptional cases, it is not possible to find a function whose partial with respect to each of its variables yield a single, given function.  What is going on here?",U_t + cU_x = f(x + ct) c U = aF(x + ct) a F f \frac{\partial F}{\partial x} \frac{\partial F}{\partial t} f,"['integration', 'multivariable-calculus', 'partial-differential-equations', 'partial-derivative', 'wave-equation']"
55,Function of class $C^1$ implies locally Lipschitz,Function of class  implies locally Lipschitz,C^1,"Let $A$ be open in $\mathbf{R}^{m} ;$ let $g: A \rightarrow \mathbf{R}^{n} .$ If $S$ is a subset of $A,$ we say that $g$ satisfies the Lipschitz condition on $S$ if the function $$ \lambda(\mathbf{x}, \mathbf{y})=|g(\mathbf{x})-g(\mathbf{y})| /|\mathbf{x}-\mathbf{y}| $$ is bounded for $\mathbf{x}, \mathbf{y}$ in $S$ and $\mathbf{x} \neq \mathbf{y}$ . We say that $g$ is locally Lipschitz if each point of $A$ has a neighborhood on which $g$ satisfies the Lipschitz condition. How do I go about showing that if $g$ is of class $C^{1}$ , then $g$ is locally Lipschitz? I know that the derivatives of $g$ are locally bounded and then I can apply the MVT, but am not sure how to formulate this. The $1-D$ case is easy, but I am struggling with the multidimensional case given.","Let be open in let If is a subset of we say that satisfies the Lipschitz condition on if the function is bounded for in and . We say that is locally Lipschitz if each point of has a neighborhood on which satisfies the Lipschitz condition. How do I go about showing that if is of class , then is locally Lipschitz? I know that the derivatives of are locally bounded and then I can apply the MVT, but am not sure how to formulate this. The case is easy, but I am struggling with the multidimensional case given.","A \mathbf{R}^{m} ; g: A \rightarrow \mathbf{R}^{n} . S A, g S 
\lambda(\mathbf{x}, \mathbf{y})=|g(\mathbf{x})-g(\mathbf{y})| /|\mathbf{x}-\mathbf{y}|
 \mathbf{x}, \mathbf{y} S \mathbf{x} \neq \mathbf{y} g A g g C^{1} g g 1-D","['real-analysis', 'analysis', 'multivariable-calculus', 'proof-writing', 'lipschitz-functions']"
56,Understanding higher order differentials and their properties,Understanding higher order differentials and their properties,,"In our course in calculus/real analysis we defined $n$ -th differential of multivariable function $f:\mathbb{R}^{m}\to\mathbb{R}$ as: $$d^{n}f(\vec{x},\vec{h})=\frac{d^{n}}{dt^{n}}f(\vec{x}+t\vec{h})\biggr\rvert_{t=0}   \text{    } (*)$$ There is an explicit formula for this differential in terms of partial derivatives which is quite similar to the multinomial theorem. Now we used this formula to find higher order partial derivatives all at once by finding higher order differentials. In order to find them we used the following properties: $$d(f+g)=df+dg$$ $$d(f\cdot g)=g\cdot df+f\cdot dg \text{ where $f$ and $g$ are differential expressions*}$$ $$d(dx)=0 \text{ where $x$ is an independent variable}$$ We used these properties to find higher-order differentials in the following way: Suppose we have $f(x,y)=xy$ . Then $df=y\cdot dx+x\cdot dy$ . Then we would find second-order differential: $$d^{2}f=d(df)=d(y\cdot dx+x\cdot dy)=d(y\cdot dx)+d(x\cdot dy)=dydx+yd(dx)+dxdy+xd(dy)=dxdy+y\cdot 0 + dxdy+ x\cdot0=2dxdy$$ We usually used this method to find all higher order partial derivatives at once by finding the differential of the same order. In the example above we can deduce that $\frac{\partial^2 f}{\partial x^2}=0$ , $\frac{\partial^2 f}{\partial y^2}=0$ and $\frac{\partial^2 f}{\partial x \partial y}=2/2=1$ While these properties were quite useful for finding higher-order differentials, we haven't proved them(moreover it was said that there is no textbook which covers such proof). And actually I can't imagine how is it possible to approach at proving this, because we haven't defined ""differential of a differential expression"". But after reading about higher order Frechet derivatives I realized that these differentials are just derivatives(which are multilinear functions) evaluated at the diagonal(i.e. $d^{n}f(\vec{x},\vec{h})=D^nf(\vec{x})(\vec{h},\vec{h},\dots,\vec{h})$ ). Is this the right way of viewing ""higher order differentials"" ? Is it possible to prove the properties mentinoned earlier using this definition ? What is the rigorous way of viewing this ""differential operator"" $d$ ? Is there some textbook which covers this topic in rigorous way ? *It was asked in the comments what ""differential expression"" actually means. We were not given a rigorous definition, but it seemed clear at the time that it meant something like ""sum of products of functions and $dx_i$ where $x_i$ is a variable"".","In our course in calculus/real analysis we defined -th differential of multivariable function as: There is an explicit formula for this differential in terms of partial derivatives which is quite similar to the multinomial theorem. Now we used this formula to find higher order partial derivatives all at once by finding higher order differentials. In order to find them we used the following properties: We used these properties to find higher-order differentials in the following way: Suppose we have . Then . Then we would find second-order differential: We usually used this method to find all higher order partial derivatives at once by finding the differential of the same order. In the example above we can deduce that , and While these properties were quite useful for finding higher-order differentials, we haven't proved them(moreover it was said that there is no textbook which covers such proof). And actually I can't imagine how is it possible to approach at proving this, because we haven't defined ""differential of a differential expression"". But after reading about higher order Frechet derivatives I realized that these differentials are just derivatives(which are multilinear functions) evaluated at the diagonal(i.e. ). Is this the right way of viewing ""higher order differentials"" ? Is it possible to prove the properties mentinoned earlier using this definition ? What is the rigorous way of viewing this ""differential operator"" ? Is there some textbook which covers this topic in rigorous way ? *It was asked in the comments what ""differential expression"" actually means. We were not given a rigorous definition, but it seemed clear at the time that it meant something like ""sum of products of functions and where is a variable"".","n f:\mathbb{R}^{m}\to\mathbb{R} d^{n}f(\vec{x},\vec{h})=\frac{d^{n}}{dt^{n}}f(\vec{x}+t\vec{h})\biggr\rvert_{t=0}   \text{    } (*) d(f+g)=df+dg d(f\cdot g)=g\cdot df+f\cdot dg \text{ where f and g are differential expressions*} d(dx)=0 \text{ where x is an independent variable} f(x,y)=xy df=y\cdot dx+x\cdot dy d^{2}f=d(df)=d(y\cdot dx+x\cdot dy)=d(y\cdot dx)+d(x\cdot dy)=dydx+yd(dx)+dxdy+xd(dy)=dxdy+y\cdot 0 + dxdy+ x\cdot0=2dxdy \frac{\partial^2 f}{\partial x^2}=0 \frac{\partial^2 f}{\partial y^2}=0 \frac{\partial^2 f}{\partial x \partial y}=2/2=1 d^{n}f(\vec{x},\vec{h})=D^nf(\vec{x})(\vec{h},\vec{h},\dots,\vec{h}) d dx_i x_i","['multivariable-calculus', 'derivatives', 'proof-writing']"
57,Find constant $c$ such that all intersection points of two spheres have perpendicular tangent planes,Find constant  such that all intersection points of two spheres have perpendicular tangent planes,c,"I have come to a problem in a multivariable calculus book that I'm having trouble with. The problem statement is : ""Find a constant $c$ such that for any point of intersection of the two spheres $(x-c)^{2} + y^{2} + z^{2} = 3$ and $x^{2} + (y-1)^{2} + z^{2} = 1$ , the corresponding tangent planes will be perpendicular to each other."" So I define two functions : \begin{align} f(x,y,z) & = (x-c)^{2} + y^{2} + z^{2} - 3 \\ g(x,y,z) & = x^{2} + (y-1)^{2} + z^{2} - 1 \end{align} We denote the partial derivative with respect to $x$ as $f_{x}$ and so on... We see : \begin{align} f_{x}(x,y,z) & = 2(x-c) = 2x - 2c\\ f_{y}(x,y,z) & = 2y\\ f_{z}(x,y,z) & = 2z \end{align} and : \begin{align} g_{x}(x,y,z) & = 2x \\ g_{y}(x,y,z) & = 2(y-1) = 2y - 2 \\ g_{z}(x,y,z) & = 2z \end{align} I assume that if the tangent planes of the two spheres are perpendicular at an intersection point, then the normals are also perpendicular. So for every intersection point (x,y,z) we have : \begin{equation} \bigtriangledown f(x,y,z) \cdot \bigtriangledown g(x,y,z) = 0 \end{equation} So : \begin{align} \require{cancel} (2x - 2c, 2y, 2z) \cdot (2x, 2y-2, 2z) & = 0\\ (2x-2c)2x + (2y)(2y) - 2(2y) + 4z^{2} & = 0\\ 4x^{2} - 4xc + 4y^{2} - 4y + 4z^{2} & = 0\\ 4(x^{2} + y^{2} + z^{2}) - 4(xc + y) & = 0\\ \cancel{4} \left[ (x^{2}+y^{2}+z^{2}) - (xc + y) \right] & = 0\\ (x^{2}+y^{2}+z^{2}) - (xc + y) & = 0 \\ x^{2} + y^{2} + z^{2} & = xc + y \end{align} We also see : \begin{align} \require{cancel} f(x,y,z) & = x^{2} - 2xc + c^{2} + y^{2} + z^{2} - 3 \\          & = (x^{2} + y^{2} + z^{2}) + (c^{2} - 2xc - 3) \\ g(x,y,z) & = x^{2} + y^{2} - 2y + \cancel{1} + z^{2} - \cancel{1} \\          & = (x^{2}+y^{2}+z^{2}) - 2y \end{align} and : \begin{align} f(x,y,z) = 0 & \Rightarrow (x^{2}+y^{2}+z^{2}) = -(c^{2}-2xc-3) = -c^{2} + 2xc + 3\\ g(x,y,z) = 0 & \Rightarrow (x^{2}+y^{2}+z^{2}) = 2y \end{align} So we have : \begin{equation} xc + y = -c^{2} + 2xc + 3 = 2y \end{equation} It is here that I am stuck. It seems that the goal here would be to obtain an expression for $c$ that doesn't include $x$ or $y$ , but I do not know how to obtain it. Can someone help with this ?","I have come to a problem in a multivariable calculus book that I'm having trouble with. The problem statement is : ""Find a constant such that for any point of intersection of the two spheres and , the corresponding tangent planes will be perpendicular to each other."" So I define two functions : We denote the partial derivative with respect to as and so on... We see : and : I assume that if the tangent planes of the two spheres are perpendicular at an intersection point, then the normals are also perpendicular. So for every intersection point (x,y,z) we have : So : We also see : and : So we have : It is here that I am stuck. It seems that the goal here would be to obtain an expression for that doesn't include or , but I do not know how to obtain it. Can someone help with this ?","c (x-c)^{2} + y^{2} + z^{2} = 3 x^{2} + (y-1)^{2} + z^{2} = 1 \begin{align}
f(x,y,z) & = (x-c)^{2} + y^{2} + z^{2} - 3 \\
g(x,y,z) & = x^{2} + (y-1)^{2} + z^{2} - 1
\end{align} x f_{x} \begin{align}
f_{x}(x,y,z) & = 2(x-c) = 2x - 2c\\
f_{y}(x,y,z) & = 2y\\
f_{z}(x,y,z) & = 2z
\end{align} \begin{align}
g_{x}(x,y,z) & = 2x \\
g_{y}(x,y,z) & = 2(y-1) = 2y - 2 \\
g_{z}(x,y,z) & = 2z
\end{align} \begin{equation}
\bigtriangledown f(x,y,z) \cdot \bigtriangledown g(x,y,z) = 0
\end{equation} \begin{align}
\require{cancel}
(2x - 2c, 2y, 2z) \cdot (2x, 2y-2, 2z) & = 0\\
(2x-2c)2x + (2y)(2y) - 2(2y) + 4z^{2} & = 0\\
4x^{2} - 4xc + 4y^{2} - 4y + 4z^{2} & = 0\\
4(x^{2} + y^{2} + z^{2}) - 4(xc + y) & = 0\\
\cancel{4} \left[ (x^{2}+y^{2}+z^{2}) - (xc + y) \right] & = 0\\
(x^{2}+y^{2}+z^{2}) - (xc + y) & = 0 \\
x^{2} + y^{2} + z^{2} & = xc + y
\end{align} \begin{align}
\require{cancel}
f(x,y,z) & = x^{2} - 2xc + c^{2} + y^{2} + z^{2} - 3 \\
         & = (x^{2} + y^{2} + z^{2}) + (c^{2} - 2xc - 3) \\
g(x,y,z) & = x^{2} + y^{2} - 2y + \cancel{1} + z^{2} - \cancel{1} \\
         & = (x^{2}+y^{2}+z^{2}) - 2y
\end{align} \begin{align}
f(x,y,z) = 0 & \Rightarrow (x^{2}+y^{2}+z^{2}) = -(c^{2}-2xc-3) = -c^{2} + 2xc + 3\\
g(x,y,z) = 0 & \Rightarrow (x^{2}+y^{2}+z^{2}) = 2y
\end{align} \begin{equation}
xc + y = -c^{2} + 2xc + 3 = 2y
\end{equation} c x y",['multivariable-calculus']
58,"On ""the Hessian is the Jacobian of the gradient""","On ""the Hessian is the Jacobian of the gradient""",,"According to Wikipedia , The Hessian matrix of a function $f$ is the Jacobian matrix of the gradient of the function $f$ ; that is: $H(f(x)) = J(\nabla f(x))$ . Suppose $f : \Bbb R^m \to \Bbb  R^n,x \mapsto f(x)$ and $f \in C^2 (\Bbb  R^m)$ . Here, I regard points in $\Bbb R^m, \Bbb R^n$ as column vectors, therefore $f$ sends column vectors to column vectors. When $n=1$ , we can define $\nabla f: \Bbb R^m \to (\Bbb R^m)^t,x\mapsto\nabla f(x)$ , which sends column vectors to row vectors. I  use $(\Bbb R^m)^t$ to denote row vector space, which is just a random notation. We do have a good definition for functions that sends column vectors to column vectors, but what can we say about functions that sends column vectors to row vectors? I discovered that if I manipulate $\nabla f(x)$ as a column vector, then I know how to calculate, and my calculation agree with Wiki. But I don't think we can ""manipulate $\nabla f(x)$ as a column vector"".","According to Wikipedia , The Hessian matrix of a function is the Jacobian matrix of the gradient of the function ; that is: . Suppose and . Here, I regard points in as column vectors, therefore sends column vectors to column vectors. When , we can define , which sends column vectors to row vectors. I  use to denote row vector space, which is just a random notation. We do have a good definition for functions that sends column vectors to column vectors, but what can we say about functions that sends column vectors to row vectors? I discovered that if I manipulate as a column vector, then I know how to calculate, and my calculation agree with Wiki. But I don't think we can ""manipulate as a column vector"".","f f H(f(x)) = J(\nabla f(x)) f : \Bbb R^m \to \Bbb  R^n,x \mapsto f(x) f \in C^2 (\Bbb  R^m) \Bbb R^m, \Bbb R^n f n=1 \nabla f: \Bbb R^m \to (\Bbb R^m)^t,x\mapsto\nabla f(x) (\Bbb R^m)^t \nabla f(x) \nabla f(x)","['multivariable-calculus', 'definition', 'vector-analysis', 'jacobian', 'hessian-matrix']"
59,Bounds on triple integral (Cartesian),Bounds on triple integral (Cartesian),,"I want to setup a triple integral for the volume of the surface in the ordering $dy \hspace{1mm} dx \hspace{1mm} dz$ : So far I have that for $0\leq z \leq 1, 0 \leq y \leq x$ and for $1 \leq z \leq 2, 0 \leq y \leq \sqrt{2-z}$ . I'm having trouble setting up bounds for $x$ . It looks from the projection like $0 \leq x \leq 1$ for both integrals, but it doesn't give me the right value for volume (should be $\frac{11}{12}$ based on the other differential orderings.)","I want to setup a triple integral for the volume of the surface in the ordering : So far I have that for and for . I'm having trouble setting up bounds for . It looks from the projection like for both integrals, but it doesn't give me the right value for volume (should be based on the other differential orderings.)","dy \hspace{1mm} dx \hspace{1mm} dz 0\leq z \leq 1, 0 \leq y \leq x 1 \leq z \leq 2, 0 \leq y \leq \sqrt{2-z} x 0 \leq x \leq 1 \frac{11}{12}",['multivariable-calculus']
60,Divergence of magnetic field $B = \frac{\mu_0 I}{2\pi r}$,Divergence of magnetic field,B = \frac{\mu_0 I}{2\pi r},"I have to show that the divergence of this magnetic field is 0. I can do this pretty easily using the divergence theorem; however, if I try using try computing the divergence directly $\nabla B$ does not equal $0$ . To solve it indirectly I used the definition that defines the divergence as the limit of a surface integral. I chose a cylindrical surface and then showed that the flux through that surface is zero as the field lines form circles.","I have to show that the divergence of this magnetic field is 0. I can do this pretty easily using the divergence theorem; however, if I try using try computing the divergence directly does not equal . To solve it indirectly I used the definition that defines the divergence as the limit of a surface integral. I chose a cylindrical surface and then showed that the flux through that surface is zero as the field lines form circles.",\nabla B 0,"['multivariable-calculus', 'vector-analysis', 'physics', 'dirac-delta']"
61,Triple Integral in Cylindrical Coordinates using x axis instead of z axis,Triple Integral in Cylindrical Coordinates using x axis instead of z axis,,"Today in my Calculus class my teacher made an example using change of variables using this problem: Find the volume of the solid inside the cylinder $y^{2} + z^{2} = 2y$ bounded by $x=0$ and the surface of the cone $x^{2}=y^{2} + z^{2}$ . With $x \geq 0$ . He wrote directly the change of variables like this: from (x, y, z) to $(\theta, r, x)$ which is, I think, the same as cylindrical coordinates but with rotated axis (I don't know how to call it)? because instead of $z$ he used $x$ for the height because it was convenient for the problem (if you make a graph, the cylinder lies in the $x$ axis). The thing is, I didn't understand the bounds for $(\theta, r, x)$ . He wrote the answer and it should be this: $$-\frac{\pi}2 \leq \theta\leq \frac{\pi}2,$$ $$0 \leq r\leq 2cos(\theta),$$ $$0 \leq x \leq r,$$ After the integration, the result should be 32/9. I've been trying to think all day but I still have troubles. Could you explain to me the change of variables, please? The $\theta$ angle bounds are the most confusing. Please. I'm sorry if I made a mistake with the equations format.","Today in my Calculus class my teacher made an example using change of variables using this problem: Find the volume of the solid inside the cylinder bounded by and the surface of the cone . With . He wrote directly the change of variables like this: from (x, y, z) to which is, I think, the same as cylindrical coordinates but with rotated axis (I don't know how to call it)? because instead of he used for the height because it was convenient for the problem (if you make a graph, the cylinder lies in the axis). The thing is, I didn't understand the bounds for . He wrote the answer and it should be this: After the integration, the result should be 32/9. I've been trying to think all day but I still have troubles. Could you explain to me the change of variables, please? The angle bounds are the most confusing. Please. I'm sorry if I made a mistake with the equations format.","y^{2} + z^{2} = 2y x=0 x^{2}=y^{2} + z^{2} x \geq 0 (\theta, r, x) z x x (\theta, r, x) -\frac{\pi}2 \leq \theta\leq \frac{\pi}2, 0 \leq r\leq 2cos(\theta), 0 \leq x \leq r, \theta","['integration', 'multivariable-calculus', 'multiple-integral', 'cylindrical-coordinates']"
62,"Help-me, please! Solve $\oint_{c} xy ds$ where $C$ is the intersection of the surfaces $x^2+y^2=4$ and $ y+z=8 $","Help-me, please! Solve  where  is the intersection of the surfaces  and",\oint_{c} xy ds C x^2+y^2=4  y+z=8 ,"Solve $\oint_{c} xy ds$ where $C$ is the intersection of the surfaces $x^2+y^2=4$ and $ y+z=8 $ Hi. I tried to resolve the issue below using the definition of line integral, but I couldnt solve it. Could someone appreciate my resolution and help me finish it? Below follows what I did: $$\oint_{c} xy ds = {\int }_{C}f\left(x,y,z\right)ds= \int_{a}^{b}f\left(\text{r}\left(t\right)\right)\sqrt{{\left({x}^{\prime }\left(t\right)\right)}^{2}+{\left({y}^{\prime }\left(t\right)\right)}^{2}+{\left({z}^{\prime }\left(t\right)\right)}^{2}}dt.$$ \begin{align*} 		x&=2\cos t \\ 		y&=2\sin t \\ 		z&=8-2\sin t 	\end{align*} \begin{align*} f\left(\text{r}\left(t\right)\right)&=\langle 2\cos t, 2\sin t, 8-2\sin t \rangle \\ ds = \left\Vert\left(\text{r}\left(t\right)\right)^{\prime} \right\Vert&= \sqrt{{\left({x}^{\prime }\left(t\right)\right)}^{2}+{\left({y}^{\prime }\left(t\right)\right)}^{2}+{\left({z}^{\prime }\left(t\right)\right)}^{2}}dt \end{align*} Thus, \begin{align*} \oint_{c} xy ds ={\int }_{C}f\left(x,y,z\right)ds&={\int_{0}^{2\pi}}(4\cos t\sin t)\sqrt{{\left({-2\sin t}\right)}^{2}+{\left({2\cos t}\right)}^{2}+{\left({-2\cos t}\right)}^{2}} dt \\ 	 &={\int_0^{2\pi}} 8\cos \left(t\right)\sin \left(t\right)\sqrt{-\sin ^2\left(t\right)+2} dt \\ 	 &= -\left[\frac{8}{3}\left(1+\cos ^2\left(t \right)\right)^{\frac{3}{2}}\right]_{_{0}}^{^{2\pi}} \\ 	 &=0. \end{align*} Thanks!","Solve where is the intersection of the surfaces and Hi. I tried to resolve the issue below using the definition of line integral, but I couldnt solve it. Could someone appreciate my resolution and help me finish it? Below follows what I did: Thus, Thanks!","\oint_{c} xy ds C x^2+y^2=4  y+z=8  \oint_{c} xy ds = {\int }_{C}f\left(x,y,z\right)ds= \int_{a}^{b}f\left(\text{r}\left(t\right)\right)\sqrt{{\left({x}^{\prime }\left(t\right)\right)}^{2}+{\left({y}^{\prime }\left(t\right)\right)}^{2}+{\left({z}^{\prime }\left(t\right)\right)}^{2}}dt. \begin{align*}
		x&=2\cos t \\
		y&=2\sin t \\
		z&=8-2\sin t
	\end{align*} \begin{align*}
f\left(\text{r}\left(t\right)\right)&=\langle 2\cos t, 2\sin t, 8-2\sin t \rangle \\
ds = \left\Vert\left(\text{r}\left(t\right)\right)^{\prime} \right\Vert&= \sqrt{{\left({x}^{\prime }\left(t\right)\right)}^{2}+{\left({y}^{\prime }\left(t\right)\right)}^{2}+{\left({z}^{\prime }\left(t\right)\right)}^{2}}dt
\end{align*} \begin{align*}
\oint_{c} xy ds ={\int }_{C}f\left(x,y,z\right)ds&={\int_{0}^{2\pi}}(4\cos t\sin t)\sqrt{{\left({-2\sin t}\right)}^{2}+{\left({2\cos t}\right)}^{2}+{\left({-2\cos t}\right)}^{2}} dt \\
	 &={\int_0^{2\pi}} 8\cos \left(t\right)\sin \left(t\right)\sqrt{-\sin ^2\left(t\right)+2} dt \\
	 &= -\left[\frac{8}{3}\left(1+\cos ^2\left(t \right)\right)^{\frac{3}{2}}\right]_{_{0}}^{^{2\pi}} \\
	 &=0.
\end{align*}","['calculus', 'multivariable-calculus', 'differential-geometry']"
63,Prove that if $f:M\rightarrow\Bbb R$ is a scalar function over a 1-manifold M without boundary then $\int_M df=0$,Prove that if  is a scalar function over a 1-manifold M without boundary then,f:M\rightarrow\Bbb R \int_M df=0,"Well James Munkres in the text Analysis on Manifolds prove the general Stoke's theorem for $k$ -form when $k>1$ and then he proves it for $k=1$ only when the bounary of the Manifold is not empty and he leaves as exercise to show that if $f$ is a scalar function defined over a compact $1$ -manifold $M$ then $$ \int_M df=0 $$ So let's start to try to prove it and precisely we will do to scalar function whose support is covered by a single coordinate chart becasue as Munkres showed in the general proof this is sufficent. Indeed if $\Phi:\{\phi_i:i=1,...,l\}$ is a partition of unity dominated by the coordinate patches of $M$ then the support of the forn $\phi_i\omega$ is contained in a single coordinate patch for each $i=1,..,l$ and thus $$ \int_Md\omega=\int_M0+\int_Md\omega=\int_M0\curlywedge\omega+\int_M\Biggl(\sum_{i=1}^l\phi_i\Biggl)\curlywedge d\omega= \\ \int_Md1\curlywedge\omega+\int_M\Biggl(\sum_{i=1}^l\phi_i\curlywedge d\omega\Bigg)= \\ \int_Md\Biggl(\sum_{i=1}^l\phi_i\Biggl)\curlywedge\omega+\sum_{i=1}^l\Biggl(\int_M\phi_i\curlywedge d\omega\Biggl)= \\ \sum_{i=1}^l\Biggl(\int_Md\phi_i\curlywedge\omega\Biggl)+\sum_{i=1}^l\Biggl(\int_M(-1)^0\phi_i\curlywedge d\omega\Biggl)=\sum_{i=1}^l\Biggl(\int_Md\phi_i\curlywedge\omega+\int_M(-1)^0\phi_i\curlywedge d\omega\Biggl) \\ \sum_{i=1}^l\Biggl(\int_Md\phi_i\curlywedge\omega+(-1)^0\phi_i\curlywedge d\omega\Biggl)=\sum_{i=1}^l\int_Md(\phi_i\omega)=0 $$ Well if $M$ is a compact $1$ -manifold without boundary it is possible to prove that for any $p\in M$ there exist a coordinate patch $\alpha$ defined in the unitary positive interval $(0,1)$ and thus if $f:M\rightarrow\Bbb R$ is a scalar function defined over M whose support $S_f$ is covered by a single coordinate patch then $$ \int_Mdf:=\int_{(0,1)}\alpha^*(df)=\int_{(0,1)}d(\alpha^*f)=\int_{(0,1)}d(\alpha\circ f) $$ So Unfortunately I do not able to prove that $$ \int_{(0,1)}d(\alpha\circ f)=0 $$ and thus I ask to do it. So could someone help me, please?","Well James Munkres in the text Analysis on Manifolds prove the general Stoke's theorem for -form when and then he proves it for only when the bounary of the Manifold is not empty and he leaves as exercise to show that if is a scalar function defined over a compact -manifold then So let's start to try to prove it and precisely we will do to scalar function whose support is covered by a single coordinate chart becasue as Munkres showed in the general proof this is sufficent. Indeed if is a partition of unity dominated by the coordinate patches of then the support of the forn is contained in a single coordinate patch for each and thus Well if is a compact -manifold without boundary it is possible to prove that for any there exist a coordinate patch defined in the unitary positive interval and thus if is a scalar function defined over M whose support is covered by a single coordinate patch then So Unfortunately I do not able to prove that and thus I ask to do it. So could someone help me, please?","k k>1 k=1 f 1 M 
\int_M df=0
 \Phi:\{\phi_i:i=1,...,l\} M \phi_i\omega i=1,..,l 
\int_Md\omega=\int_M0+\int_Md\omega=\int_M0\curlywedge\omega+\int_M\Biggl(\sum_{i=1}^l\phi_i\Biggl)\curlywedge d\omega=
\\
\int_Md1\curlywedge\omega+\int_M\Biggl(\sum_{i=1}^l\phi_i\curlywedge d\omega\Bigg)=
\\
\int_Md\Biggl(\sum_{i=1}^l\phi_i\Biggl)\curlywedge\omega+\sum_{i=1}^l\Biggl(\int_M\phi_i\curlywedge d\omega\Biggl)=
\\
\sum_{i=1}^l\Biggl(\int_Md\phi_i\curlywedge\omega\Biggl)+\sum_{i=1}^l\Biggl(\int_M(-1)^0\phi_i\curlywedge d\omega\Biggl)=\sum_{i=1}^l\Biggl(\int_Md\phi_i\curlywedge\omega+\int_M(-1)^0\phi_i\curlywedge d\omega\Biggl)
\\
\sum_{i=1}^l\Biggl(\int_Md\phi_i\curlywedge\omega+(-1)^0\phi_i\curlywedge d\omega\Biggl)=\sum_{i=1}^l\int_Md(\phi_i\omega)=0
 M 1 p\in M \alpha (0,1) f:M\rightarrow\Bbb R S_f 
\int_Mdf:=\int_{(0,1)}\alpha^*(df)=\int_{(0,1)}d(\alpha^*f)=\int_{(0,1)}d(\alpha\circ f)
 
\int_{(0,1)}d(\alpha\circ f)=0
","['calculus', 'multivariable-calculus', 'differential-geometry', 'stokes-theorem', 'compact-manifolds']"
64,Finding the function for a curve given its length and the length of its velocity?,Finding the function for a curve given its length and the length of its velocity?,,"I am working on an assignment and am trying to find a function for $\vec{r}(t)$ with constant length $x$ , whose velocity function also has a constant length $y$ . I'm not sure where to begin with this. I know that, say, $\langle \cos(t),\sin(t),0\rangle$ has a constant length of 1, but its velocity curve also has a constant length of 1. How can I scale the velocity without scaling the original function? Thanks!","I am working on an assignment and am trying to find a function for with constant length , whose velocity function also has a constant length . I'm not sure where to begin with this. I know that, say, has a constant length of 1, but its velocity curve also has a constant length of 1. How can I scale the velocity without scaling the original function? Thanks!","\vec{r}(t) x y \langle \cos(t),\sin(t),0\rangle","['multivariable-calculus', 'vectors']"
65,"How to evaluate $\lim_{(x,y)\to (0,0)} \frac{x^4+y^4}{2x-y}$?",How to evaluate ?,"\lim_{(x,y)\to (0,0)} \frac{x^4+y^4}{2x-y}","Find the limit : $$\lim_{(x,y)\to (0,0)} \frac{x^4+y^4}{2x-y}$$ I have tried using polar substitution but the problem is the denominator. Denominator can go to $0$ for some value of theta and is not dependent on only $r$ .",Find the limit : I have tried using polar substitution but the problem is the denominator. Denominator can go to for some value of theta and is not dependent on only .,"\lim_{(x,y)\to (0,0)} \frac{x^4+y^4}{2x-y} 0 r","['limits', 'multivariable-calculus', 'limits-without-lhopital']"
66,"Maximum value of $(x1)^2+ (y1)^2+ (z1)^2$ with constraint $x^2+y^2+z^2 2 , z1$",Maximum value of  with constraint,"(x1)^2+ (y1)^2+ (z1)^2 x^2+y^2+z^2 2 , z1","So the problem is that I have $D(f)=\{(x,y,z), x^2+y^2+z^2 2 , z1\}$ and I have to determine the maximum value for the function $(x1)^2+ (y1)^2+ (z1)^2$ in $D$ . I'm just confused as I don't actually know if $z\le1$ counts as a constraint as well, or is it just for me to sketch the area, which is actually a part of the question. Furthermore, I know that I have to use Lagrange multiplier method, but I honestly don't know how because $\le$ is making the question hard for me. Do I just calculate as usual and count $\le$ the same as $=$ ? appreciate all the feedback Edit: I have calculated the grad f =0 which is = $D(f)=(2(x-1), 2(y-1), 2(z-1))$ where I've got that $x=y=z= 1$ and $f(1, 1, 1)=0$ . (I don't know what to do with this though). Then I calculated $L(x, y, x, ) = (x1)^2+(y1)^2+(z1)^2 +(x^2+y^2+z^2-2)$ , then the four cases where I got the same value which is $-2= 2(x-1)/x = 2(y-1)/y = 2(z-1)/z$ . Which means that $x=y=z$ , put it in the $D$ function $x^2+ x^2+ x^2$ and ended up with $x=y=z= +2/3$ . I took the minus sign for the maximum distance from $(1,1,1)$ . which means that the answer is $x=y=z= 2/3$ . Is it correct?","So the problem is that I have and I have to determine the maximum value for the function in . I'm just confused as I don't actually know if counts as a constraint as well, or is it just for me to sketch the area, which is actually a part of the question. Furthermore, I know that I have to use Lagrange multiplier method, but I honestly don't know how because is making the question hard for me. Do I just calculate as usual and count the same as ? appreciate all the feedback Edit: I have calculated the grad f =0 which is = where I've got that and . (I don't know what to do with this though). Then I calculated , then the four cases where I got the same value which is . Which means that , put it in the function and ended up with . I took the minus sign for the maximum distance from . which means that the answer is . Is it correct?","D(f)=\{(x,y,z), x^2+y^2+z^2 2 , z1\} (x1)^2+ (y1)^2+ (z1)^2 D z\le1 \le \le = D(f)=(2(x-1), 2(y-1), 2(z-1)) x=y=z= 1 f(1, 1, 1)=0 L(x, y, x, ) = (x1)^2+(y1)^2+(z1)^2 +(x^2+y^2+z^2-2) -2= 2(x-1)/x = 2(y-1)/y = 2(z-1)/z x=y=z D x^2+ x^2+ x^2 x=y=z= +2/3 (1,1,1) x=y=z= 2/3",['multivariable-calculus']
67,compute $\iint_Y F.N\ dS $ with Gauss,compute  with Gauss,\iint_Y F.N\ dS ,"The question is: $$ \iint_Y F.N\ dS \quad Y=(x-z)^2+(y-z)^2=1+z^2,\ \  0\leq z\leq 1 \quad N=\text{pointing outward} $$ $$ F=(y,x,1+x^2z)$$ Here is how I have tried to solve it: I tried to solve it with Gauss theorem like this, $\gamma=\text{upper lid}$ , $\sigma=\text{lower lid} $ $$ \iint_{Y+\sigma+\gamma}F.N\ dS=\iiint_K \text{div}f\  dA \iff \iint_Y F.N\ dS=\iiint_k-\iint_\sigma-\iint_\gamma$$ $$ \iint_\sigma=-\pi, \iint_\gamma=2+\pi$$ But I have trouble to integrate this triple integral $$ \iiint_kx^2 dxdydz$$ I don't know if I have been  calculating right so far, and how should I proceed? Any suggestion would be great, thanks","The question is: Here is how I have tried to solve it: I tried to solve it with Gauss theorem like this, , But I have trouble to integrate this triple integral I don't know if I have been  calculating right so far, and how should I proceed? Any suggestion would be great, thanks"," \iint_Y F.N\ dS \quad Y=(x-z)^2+(y-z)^2=1+z^2,\ \  0\leq z\leq 1 \quad N=\text{pointing outward}
  F=(y,x,1+x^2z) \gamma=\text{upper lid} \sigma=\text{lower lid}   \iint_{Y+\sigma+\gamma}F.N\ dS=\iiint_K \text{div}f\  dA \iff \iint_Y F.N\ dS=\iiint_k-\iint_\sigma-\iint_\gamma  \iint_\sigma=-\pi, \iint_\gamma=2+\pi  \iiint_kx^2 dxdydz","['integration', 'multivariable-calculus', 'surface-integrals', 'line-integrals']"
68,Prove that $ \sum_{i=1}^{\infty}\frac{f_{i}}{2^{i}} $ is integrable function.,Prove that  is integrable function., \sum_{i=1}^{\infty}\frac{f_{i}}{2^{i}} ,"Let $ I\subseteq\mathbb{R}^{n} $ be a box.Let $ f_{i}:I\to[0,1] $ be integrable functions. Prove that $ \sum_{i=1}^{\infty}\frac{f_{i}}{2^{i}} $ is integrable function. My first intuition was to use Weierstrass M-test, but we never proved it for multivariable functions and I cant see why would it hold. The second intuition was to show that the set of discontinuities of $ \sum_{i=1}^{\infty}\frac{f_{i}}{2^{i}} $ is of measure zero, but im not sure how to show it. Any help would be appreaciated. Thanks in advance.","Let be a box.Let be integrable functions. Prove that is integrable function. My first intuition was to use Weierstrass M-test, but we never proved it for multivariable functions and I cant see why would it hold. The second intuition was to show that the set of discontinuities of is of measure zero, but im not sure how to show it. Any help would be appreaciated. Thanks in advance."," I\subseteq\mathbb{R}^{n}   f_{i}:I\to[0,1]   \sum_{i=1}^{\infty}\frac{f_{i}}{2^{i}}   \sum_{i=1}^{\infty}\frac{f_{i}}{2^{i}} ",['multivariable-calculus']
69,Do you find this proof convincing?,Do you find this proof convincing?,,"I must show that the function $f:\mathbb R^n \to \mathbb R$ defined as $ f(\mathbf x)=\sum_{j=1}^{k} \Vert \mathbf x -\mathbf{a}_j\Vert^2 $ for fixed $\mathbf a_1,...,\mathbf a_k \in\mathbb R^n$ has a global minimum. Here's my reasoning: The function $f$ is continuous. Pick $\delta \gt 0$ , by definition closed balls (neighborhoods) are compact; so define $\bar{B}_n(\mathbf 0,n\delta) =\{ \mathbf x\in\mathbb R^n :\Vert\mathbf x\Vert \le n\delta\}$ for $n \in \mathbb N$ , and define $f_n:\bar{B}_n(\mathbf 0,n\delta) \to \mathbb R$ as $f_n=\sum_{j=1}^{k} \Vert \mathbf x -\mathbf{a}_j\Vert^2$ . By the maximum/minimum value Theorem there is a $\mathbf y \in \bar{B}_n(\mathbf 0,n\delta) $ such that $f_n(\mathbf y)\le f_n(\mathbf x)$ for all $\mathbf x \in \bar{B}_n(\mathbf 0,n\delta)$ . Call $f_n(\mathbf y)= y_n$ . For all $k \in \mathbb N$ we have: $y_{n+1} \le y_n$ because $\bar{B}_k(\mathbf 0,k\delta) \subset \bar{B}_{k+1}(\mathbf 0,(k+1)\delta)$ , and $y_k \ge 0$ because $f_k(\mathbf x) \ge 0$ for all $\mathbf x$ s in the domain; so the sequence $\{y_n\}$ is bounded below and non increasing, hence it converges, as $n \to \infty$ , to a number $y$ . $y$ is the global minimum of $f(\mathbf x)=\sum_{j=1}^{k} \Vert \mathbf x -\mathbf{a}_j\Vert^2$ . I'm not so sure if I can claim that $y$ is indeed the global minimum of the function $f$ EDIT: I don't seem to be able to complete the proof with my method, also I'm thinking that if I prove there is a closed ball big enough (with all the $\mathbf a_i$ s in it) such that every element $\mathbf q$ outside the ball has its $f( \mathbf q)$ greater than the minimum inside then all the work above is superfluous. People in the comments and in the answers have provided a more efficient strategy: Let $f(\mathbf a_1)=\sum_{j=2}^{k} \Vert \mathbf a_1 - \mathbf a_j\Vert^2=M$ , and define the closed ball $\bar{B}(\mathbf a_1,\sqrt{M})= \{ \mathbf x \in \mathbb R^n: \Vert \mathbf x - \mathbf a_1 \Vert \le \sqrt{M} \}$ . By the minimum value theorem there is some $\mathbf y$ in $\bar{B}(\mathbf a_1,\sqrt{M})$ such that $f(\mathbf y) \le f(\mathbf x)$ for all $\mathbf x$ in $\bar{B}(\mathbf a_1,\sqrt{M})$ . Pick any element $\mathbf q \notin \bar{B}(\mathbf a_1,\sqrt{M})$ , which means that $0 \le \sqrt{M} \lt \Vert \mathbf q - \mathbf a_1 \Vert$ and thus $M \lt \Vert \mathbf q - \mathbf a_1 \Vert ^2$ . By definition we have $f(\mathbf y) \le f(\mathbf a_1) = M$ , but it's also true that $\Vert \mathbf q - \mathbf a_1 \Vert \le f(\mathbf q)$ . Therefore $f(\mathbf y) \lt f(\mathbf q)$ which completes the proof.","I must show that the function defined as for fixed has a global minimum. Here's my reasoning: The function is continuous. Pick , by definition closed balls (neighborhoods) are compact; so define for , and define as . By the maximum/minimum value Theorem there is a such that for all . Call . For all we have: because , and because for all s in the domain; so the sequence is bounded below and non increasing, hence it converges, as , to a number . is the global minimum of . I'm not so sure if I can claim that is indeed the global minimum of the function EDIT: I don't seem to be able to complete the proof with my method, also I'm thinking that if I prove there is a closed ball big enough (with all the s in it) such that every element outside the ball has its greater than the minimum inside then all the work above is superfluous. People in the comments and in the answers have provided a more efficient strategy: Let , and define the closed ball . By the minimum value theorem there is some in such that for all in . Pick any element , which means that and thus . By definition we have , but it's also true that . Therefore which completes the proof.","f:\mathbb R^n \to \mathbb R  f(\mathbf x)=\sum_{j=1}^{k} \Vert \mathbf x -\mathbf{a}_j\Vert^2  \mathbf a_1,...,\mathbf a_k \in\mathbb R^n f \delta \gt 0 \bar{B}_n(\mathbf 0,n\delta) =\{ \mathbf x\in\mathbb R^n :\Vert\mathbf x\Vert \le n\delta\} n \in \mathbb N f_n:\bar{B}_n(\mathbf 0,n\delta) \to \mathbb R f_n=\sum_{j=1}^{k} \Vert \mathbf x -\mathbf{a}_j\Vert^2 \mathbf y \in \bar{B}_n(\mathbf 0,n\delta)  f_n(\mathbf y)\le f_n(\mathbf x) \mathbf x \in \bar{B}_n(\mathbf 0,n\delta) f_n(\mathbf y)= y_n k \in \mathbb N y_{n+1} \le y_n \bar{B}_k(\mathbf 0,k\delta) \subset \bar{B}_{k+1}(\mathbf 0,(k+1)\delta) y_k \ge 0 f_k(\mathbf x) \ge 0 \mathbf x \{y_n\} n \to \infty y y f(\mathbf x)=\sum_{j=1}^{k} \Vert \mathbf x -\mathbf{a}_j\Vert^2 y f \mathbf a_i \mathbf q f( \mathbf q) f(\mathbf a_1)=\sum_{j=2}^{k} \Vert \mathbf a_1 - \mathbf a_j\Vert^2=M \bar{B}(\mathbf a_1,\sqrt{M})= \{ \mathbf x \in \mathbb R^n: \Vert \mathbf x - \mathbf a_1 \Vert \le \sqrt{M} \} \mathbf y \bar{B}(\mathbf a_1,\sqrt{M}) f(\mathbf y) \le f(\mathbf x) \mathbf x \bar{B}(\mathbf a_1,\sqrt{M}) \mathbf q \notin \bar{B}(\mathbf a_1,\sqrt{M}) 0 \le \sqrt{M} \lt \Vert \mathbf q - \mathbf a_1 \Vert M \lt \Vert \mathbf q - \mathbf a_1 \Vert ^2 f(\mathbf y) \le f(\mathbf a_1) = M \Vert \mathbf q - \mathbf a_1 \Vert \le f(\mathbf q) f(\mathbf y) \lt f(\mathbf q)","['calculus', 'analysis', 'multivariable-calculus', 'solution-verification']"
70,How to transform a multi (2) dimensional uniform random function to a given probability density function?,How to transform a multi (2) dimensional uniform random function to a given probability density function?,,"Well as per title, say I have the probability density function on domain $x \in [0,1] ; y \in [0,1]$ $$f(x,y) = \frac{12}{5} \left( x^2 + y^2 - xy \right)$$ Can I generate this density function from a given uniform (pseudo) random function on the same domain? When using a single variant it's slightly easy: Integrate the function to calculate the cumulative distribution function calculate the inverse of the CDF. plug in the uniform random function. However in multiple dimensions this can't be really done the ""inverse"" isn't clearly defined. - If I could split the variables it's a bit more trivial. But how can this be done in the generic case where the variables aren't independent? I could of course do it by rasterizing the function and getting linearizing the raster (just putting row behind row) and then using normal technologies for this. However this numerical approach seems inexact and arbitrary.","Well as per title, say I have the probability density function on domain Can I generate this density function from a given uniform (pseudo) random function on the same domain? When using a single variant it's slightly easy: Integrate the function to calculate the cumulative distribution function calculate the inverse of the CDF. plug in the uniform random function. However in multiple dimensions this can't be really done the ""inverse"" isn't clearly defined. - If I could split the variables it's a bit more trivial. But how can this be done in the generic case where the variables aren't independent? I could of course do it by rasterizing the function and getting linearizing the raster (just putting row behind row) and then using normal technologies for this. However this numerical approach seems inexact and arbitrary.","x \in [0,1] ; y \in [0,1] f(x,y) = \frac{12}{5} \left( x^2 + y^2 - xy \right)","['multivariable-calculus', 'probability-distributions']"
71,Question about meaning of derivative of vector-valued function,Question about meaning of derivative of vector-valued function,,"So I was wondering about the interpretation of the derivative of a vector-valued function in the sense of how it is analogous to a function with one variable input and one output. With the derivative in single variable calculus, it is interpreted as the slope of the line that is tangent to the curve. With a vector valued function, when we take the derivative, it is equivalent to taking the derivative of each of the components of the function's output. So are we saying that instead of a representing a line that is tangent to the curve, it is representing a vector that is tangent to the curve? But what is confusing me about the analogy is this. In the single variable situation, we can travel along the tangent line that is given by the derivative. What is the meaning of ""travelling along the tangent line"" when we have a tangent vector? Do we travel along the tangent vector?","So I was wondering about the interpretation of the derivative of a vector-valued function in the sense of how it is analogous to a function with one variable input and one output. With the derivative in single variable calculus, it is interpreted as the slope of the line that is tangent to the curve. With a vector valued function, when we take the derivative, it is equivalent to taking the derivative of each of the components of the function's output. So are we saying that instead of a representing a line that is tangent to the curve, it is representing a vector that is tangent to the curve? But what is confusing me about the analogy is this. In the single variable situation, we can travel along the tangent line that is given by the derivative. What is the meaning of ""travelling along the tangent line"" when we have a tangent vector? Do we travel along the tangent vector?",,"['multivariable-calculus', 'tangent-line']"
72,"How do you integrate $x ^ 2 + y ^ 2$ over the area where $x \geq 0, y \geq 0,$ and $3x + 4y <10$? [closed]",How do you integrate  over the area where  and ? [closed],"x ^ 2 + y ^ 2 x \geq 0, y \geq 0, 3x + 4y <10","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question How do you integrate $x ^ 2 + y ^ 2$ over the area where $x \geq 0, y \geq 0,$ and $3x + 4y <10$  I thought I should use polar coordinates because of the form of the function $x^2+y^2$ , but I can't set up the integral properly. How can I calculate this integral? Thanks in advance for your help!","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question How do you integrate over the area where and  I thought I should use polar coordinates because of the form of the function , but I can't set up the integral properly. How can I calculate this integral? Thanks in advance for your help!","x ^ 2 + y ^ 2 x \geq 0, y \geq 0, 3x + 4y <10 x^2+y^2","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
73,Multiple integration with Dirac delta,Multiple integration with Dirac delta,,"I am reading a paper and trying to solve collision integral. In Appendix A, there is an integral, $$ I^{(n)} = \int \frac{p^2_1\,dp_1 \,p^2_3\,dp_3\,p^2_4\,dp_4} {2E_12E_32E_4} 2\pi \delta (p_1  p_3  p_4)e^{E_1/T} I_\Omega,$$ where \begin{align} I_\Omega  &=\left( 4\pi \right)^3 \int d^3\lambda \, \frac{\sin p_1\lambda }{p_1\lambda}\frac{\sin p_3\lambda }{p_3\lambda}\frac{\sin p_4\lambda }{p_4\lambda} \\  &=\dfrac{32\pi^5}{p_1p_3p_4}\Bigg[\dfrac{\left( p_1-p_3+p_4\right) }{\left| p_1-p_3+p_4\right|}+\frac{(p_1+p_3-p_4) }{| p_1+p_3-p_4|}-\frac{(p_1-p_3-p_4) }{|p_1-p_3-p_4|}-\frac{(p_1+p_3+p_4) }{|p_1+p_3+p_4|}\Bigg]. \end{align} They get a result $$I^{(n)}=\int \frac{16\pi^6 p_1^2}{E_1}e^{-E_1/T} \, dp_1$$ after integrating out dirac delta. I can't seem to understand how do they remove both $p_3$ and $p_4$ using one delta integral. Also, since all the terms in brackets of $I_\Omega$ are positive, it seems that term goes to zero. Can anyone help? Note: $E_i$ and $p_i$ are energy and momentum of $i^{th}$ particle and here it is assumed that $m_3 =0$ and $m_4=0$ hence $E_3 = \sqrt{p_{3}^{2}+m_{3}^{2}}=p_{3} $ & similarly $E_4 = p_4$ .","I am reading a paper and trying to solve collision integral. In Appendix A, there is an integral, where They get a result after integrating out dirac delta. I can't seem to understand how do they remove both and using one delta integral. Also, since all the terms in brackets of are positive, it seems that term goes to zero. Can anyone help? Note: and are energy and momentum of particle and here it is assumed that and hence & similarly ."," I^{(n)} = \int \frac{p^2_1\,dp_1 \,p^2_3\,dp_3\,p^2_4\,dp_4}
{2E_12E_32E_4} 2\pi \delta (p_1  p_3  p_4)e^{E_1/T} I_\Omega, \begin{align}
I_\Omega 
&=\left( 4\pi \right)^3 \int d^3\lambda \, \frac{\sin p_1\lambda }{p_1\lambda}\frac{\sin p_3\lambda }{p_3\lambda}\frac{\sin p_4\lambda }{p_4\lambda} \\ 
&=\dfrac{32\pi^5}{p_1p_3p_4}\Bigg[\dfrac{\left( p_1-p_3+p_4\right) }{\left| p_1-p_3+p_4\right|}+\frac{(p_1+p_3-p_4) }{| p_1+p_3-p_4|}-\frac{(p_1-p_3-p_4) }{|p_1-p_3-p_4|}-\frac{(p_1+p_3+p_4) }{|p_1+p_3+p_4|}\Bigg].
\end{align} I^{(n)}=\int \frac{16\pi^6 p_1^2}{E_1}e^{-E_1/T} \, dp_1 p_3 p_4 I_\Omega E_i p_i i^{th} m_3 =0 m_4=0 E_3 = \sqrt{p_{3}^{2}+m_{3}^{2}}=p_{3}
 E_4 = p_4","['calculus', 'integration', 'multivariable-calculus', 'dirac-delta', 'multiple-integral']"
74,Let $V$ be the vector space polynomials of degree less than $n$ over $\mathbb{R}$,Let  be the vector space polynomials of degree less than  over,V n \mathbb{R},"Let $V$ be the vector space polynomials of degree less than $n$ over $\mathbb{R}$ . In other words $$V = \{p\in R[x] \mid \deg p < n\}.$$ Let $T : V \rightarrow V$ be the map $T =\frac{d}{dx}$ . From elementary calculus, $T$ is linear and $T^n=0$ for some positive integer $n$ . For $t\in \mathbb{R}$ , let $H_t : V \rightarrow V$ be the map $$H_t(p(x)) = p(x + t),$$ $p(x) \in V$ . We need to Show that $$e^{tT}=H_t.$$ So, should I start as follows? As, for any $p(x)\in \mathscr{C}^{\infty}$ and I should expand using Taylor expansion, $p(x+t)$ in the small neighborhood of t and which follows the desired result. Any help will be highly appreciated.","Let be the vector space polynomials of degree less than over . In other words Let be the map . From elementary calculus, is linear and for some positive integer . For , let be the map . We need to Show that So, should I start as follows? As, for any and I should expand using Taylor expansion, in the small neighborhood of t and which follows the desired result. Any help will be highly appreciated.","V n \mathbb{R} V = \{p\in R[x] \mid \deg p < n\}. T : V \rightarrow V T =\frac{d}{dx} T T^n=0 n t\in \mathbb{R} H_t : V \rightarrow V H_t(p(x)) = p(x + t), p(x) \in V e^{tT}=H_t. p(x)\in \mathscr{C}^{\infty} p(x+t)","['linear-algebra', 'multivariable-calculus']"
75,Change of Variable finding simple bounds,Change of Variable finding simple bounds,,"I have a very simple problem that I've solved before (and it gave me a lot of trouble then) and I somehow can not come up with the right bound again. In Michael Corral's book he uses a change of variable transformation to make $x = \frac{1}{2}(u+v)$ and $y = \frac{1}{2}(v-u)$ . The original substitutions are: $x - y = u$ $x + y = v$ The bounds on the $x,y$ plane are $(1,0)$ , $(0,1)$ and $x = 1-y$ Setting x to $0$ I can come up with the first bound of $u = -v$ I did so by plugging in $0$ for $x$ and $\frac{1}{2}(v-u)$ for $y$ and set it equal to $u$ i.e. $u = -\frac{1}{2}(v-u)$ It also works simply pluggin in the numbers. But the second bound for when $x = 1-y$ should be $u = v$ When plugging in I always get $0$ (i.e. the u's go away) or I get $v = 1$ which is a correct line but I have no bound for u on this? I'm doing something wrong and I'd be so grateful if you could help me out. I have a total black out here. I weirdly am super comfortable with polar and spherical substitution bounds but they don't apply here. Thank you very much! Edit: also if someone could recommend a ressource that has lot's of practice for finding these bounds that'd be great!","I have a very simple problem that I've solved before (and it gave me a lot of trouble then) and I somehow can not come up with the right bound again. In Michael Corral's book he uses a change of variable transformation to make and . The original substitutions are: The bounds on the plane are , and Setting x to I can come up with the first bound of I did so by plugging in for and for and set it equal to i.e. It also works simply pluggin in the numbers. But the second bound for when should be When plugging in I always get (i.e. the u's go away) or I get which is a correct line but I have no bound for u on this? I'm doing something wrong and I'd be so grateful if you could help me out. I have a total black out here. I weirdly am super comfortable with polar and spherical substitution bounds but they don't apply here. Thank you very much! Edit: also if someone could recommend a ressource that has lot's of practice for finding these bounds that'd be great!","x = \frac{1}{2}(u+v) y = \frac{1}{2}(v-u) x - y = u x + y = v x,y (1,0) (0,1) x = 1-y 0 u = -v 0 x \frac{1}{2}(v-u) y u u = -\frac{1}{2}(v-u) x = 1-y u = v 0 v = 1","['multivariable-calculus', 'multiple-integral', 'change-of-variable']"
76,"Extreme values of $f(x, y)=\frac{(x+y)^{2}}{2}+\frac{(x-y)^{3}}{3}$ on $D=\left\{(x, y) \in \mathbb{R}^{2}:|y| \leq 1-|x|\right\}$",Extreme values of  on,"f(x, y)=\frac{(x+y)^{2}}{2}+\frac{(x-y)^{3}}{3} D=\left\{(x, y) \in \mathbb{R}^{2}:|y| \leq 1-|x|\right\}","Extreme values of $f(x, y)=\frac{(x+y)^{2}}{2}+\frac{(x-y)^{3}}{3}$ on $D=\left\{(x, y) \in \mathbb{R}^{2}:|y| \leq 1-|x|\right\}$ I resolved by taking the partial derivatives and computing the Hessian matrix, so $(0,0)$ is a saddle point. For the first edge I tried to parametrize $D$ $$\begin{cases}x=1-t\\y=t\end{cases}$$ and resolve for $f(x,y)$ : $$f(t) = \frac{1}{2} + \frac{(2t-1)^3}{3}$$ $$f'(t)=4t-2=0 \implies t=\frac{1}{2}$$ and follows that $\left(\frac{1}{2},\frac{1}{2}\right)$ could be and extreme. It's clear that $(1,0)$ is a max value, but I can't find it. Can someone help me out in this?","Extreme values of on I resolved by taking the partial derivatives and computing the Hessian matrix, so is a saddle point. For the first edge I tried to parametrize and resolve for : and follows that could be and extreme. It's clear that is a max value, but I can't find it. Can someone help me out in this?","f(x, y)=\frac{(x+y)^{2}}{2}+\frac{(x-y)^{3}}{3} D=\left\{(x, y) \in \mathbb{R}^{2}:|y| \leq 1-|x|\right\} (0,0) D \begin{cases}x=1-t\\y=t\end{cases} f(x,y) f(t) = \frac{1}{2} + \frac{(2t-1)^3}{3} f'(t)=4t-2=0 \implies t=\frac{1}{2} \left(\frac{1}{2},\frac{1}{2}\right) (1,0)",['multivariable-calculus']
77,What is wrong with this procedure-writing Angular momentum operator in spherical coordinates,What is wrong with this procedure-writing Angular momentum operator in spherical coordinates,,"I am trying to write the first component of the quantum angular momentum operator $L_1$ in spherical coordinates. $$L_1=x_2p_3-x_3p_2=-i\hbar (x_2\dfrac{\partial}{\partial x_3}-x_3\dfrac{\partial}{\partial x_2})=$$ The defining equations of the spherical coordinates are: $x_1=r \sin\theta \cos\varphi,x_2=r\sin\theta\sin\varphi, x_3=r\cos\theta\tag{1}$ Then using the chain rule: \begin{align} L_1 &=-i\hbar (x_2\dfrac{\partial}{\partial x_3}-x_3\dfrac{\partial}{\partial x_2})\\ &=-i\hbar \Big[r\sin\theta\sin\varphi\left( \dfrac{\partial r}{\partial x_3}\dfrac{\partial}{\partial r} +\dfrac{\partial \theta}{\partial x_3}\dfrac{\partial}{\partial \theta} +\dfrac{\partial \varphi}{\partial x_3}\dfrac{\partial}{\partial \varphi}\right)\\ &\qquad\qquad\; -r\cos\theta\ \left( \dfrac{\partial r}{\partial x_2}\dfrac{\partial}{\partial r} +\dfrac{\partial \theta}{\partial x_2}\dfrac{\partial}{\partial \theta} +\dfrac{\partial \varphi}{\partial x_2}\dfrac{\partial}{\partial \varphi}\right) \Big] \tag{2} \end{align} Using (1): $\dfrac {\partial r}{\partial x_3}=\dfrac {1}{\frac{\partial x_3}{\partial r}}=\dfrac {1}{\cos \theta}$ $\dfrac {\partial \theta}{\partial x_3}=\dfrac {1}{\frac{\partial x_3}{\partial \theta}}=\dfrac {-1}{r \sin \theta}$ $\dfrac {\partial \varphi}{\partial x_3}=0$ $\dfrac {\partial r}{\partial x_2}=\dfrac {1}{\frac{\partial x_2}{\partial r}}=\dfrac {1}{\sin \theta \sin \varphi}$ $\dfrac {\partial \theta}{\partial x_2}=\dfrac {1}{\frac{\partial x_2}{\partial \theta}}=\dfrac {1}{r \cos \theta \sin \varphi}$ $\dfrac {\partial \varphi}{\partial x_2}=\dfrac {1}{\frac{\partial x_2}{\partial \varphi}}=\dfrac {1}{r \sin \theta \cos \varphi}$ Plugging these results in (2): $$L_1=-i\hbar [r\tan\theta\sin\varphi \dfrac{\partial}{\partial r} -\sin \varphi\dfrac{\partial}{\partial \theta} -r \dfrac{\cot\theta}{\sin \varphi}\dfrac{\partial}{\partial r} -\dfrac{1}{\sin \varphi}\dfrac{\partial}{\partial \theta} -\dfrac{\cot \theta}{ \cos \varphi}\dfrac{\partial}{\partial \varphi} ] \tag{3}$$ But the result should be $$L_1=+i\hbar [\sin \varphi\dfrac{\partial}{\partial \theta} +\cos \varphi \cot \theta \dfrac{\partial}{\partial \varphi} ] \tag{4}$$ What am I doing wrong?",I am trying to write the first component of the quantum angular momentum operator in spherical coordinates. The defining equations of the spherical coordinates are: Then using the chain rule: Using (1): Plugging these results in (2): But the result should be What am I doing wrong?,"L_1 L_1=x_2p_3-x_3p_2=-i\hbar (x_2\dfrac{\partial}{\partial x_3}-x_3\dfrac{\partial}{\partial x_2})= x_1=r \sin\theta \cos\varphi,x_2=r\sin\theta\sin\varphi, x_3=r\cos\theta\tag{1} \begin{align}
L_1
&=-i\hbar (x_2\dfrac{\partial}{\partial x_3}-x_3\dfrac{\partial}{\partial x_2})\\
&=-i\hbar \Big[r\sin\theta\sin\varphi\left(
\dfrac{\partial r}{\partial x_3}\dfrac{\partial}{\partial r}
+\dfrac{\partial \theta}{\partial x_3}\dfrac{\partial}{\partial \theta}
+\dfrac{\partial \varphi}{\partial x_3}\dfrac{\partial}{\partial \varphi}\right)\\
&\qquad\qquad\; -r\cos\theta\ \left(
\dfrac{\partial r}{\partial x_2}\dfrac{\partial}{\partial r}
+\dfrac{\partial \theta}{\partial x_2}\dfrac{\partial}{\partial \theta}
+\dfrac{\partial \varphi}{\partial x_2}\dfrac{\partial}{\partial \varphi}\right)
\Big] \tag{2}
\end{align} \dfrac {\partial r}{\partial x_3}=\dfrac {1}{\frac{\partial x_3}{\partial r}}=\dfrac {1}{\cos \theta} \dfrac {\partial \theta}{\partial x_3}=\dfrac {1}{\frac{\partial x_3}{\partial \theta}}=\dfrac {-1}{r \sin \theta} \dfrac {\partial \varphi}{\partial x_3}=0 \dfrac {\partial r}{\partial x_2}=\dfrac {1}{\frac{\partial x_2}{\partial r}}=\dfrac {1}{\sin \theta \sin \varphi} \dfrac {\partial \theta}{\partial x_2}=\dfrac {1}{\frac{\partial x_2}{\partial \theta}}=\dfrac {1}{r \cos \theta \sin \varphi} \dfrac {\partial \varphi}{\partial x_2}=\dfrac {1}{\frac{\partial x_2}{\partial \varphi}}=\dfrac {1}{r \sin \theta \cos \varphi} L_1=-i\hbar [r\tan\theta\sin\varphi
\dfrac{\partial}{\partial r}
-\sin \varphi\dfrac{\partial}{\partial \theta}
-r \dfrac{\cot\theta}{\sin \varphi}\dfrac{\partial}{\partial r}
-\dfrac{1}{\sin \varphi}\dfrac{\partial}{\partial \theta}
-\dfrac{\cot \theta}{ \cos \varphi}\dfrac{\partial}{\partial \varphi}
] \tag{3} L_1=+i\hbar [\sin \varphi\dfrac{\partial}{\partial \theta}
+\cos \varphi \cot \theta \dfrac{\partial}{\partial \varphi}
] \tag{4}","['multivariable-calculus', 'partial-derivative', 'quantum-mechanics']"
78,"Compute $\iint_D (x^4-y^4) \,dx\,dy$",Compute,"\iint_D (x^4-y^4) \,dx\,dy","How should I proceed? the question is : $$\iint_D (x^4-y^4) \,dx\,dy$$ $$D= \left\{(x,y):1<x^2-y^2<4, \quad \sqrt{17}<x^2+y^2<5,\quad x<0,\ \  y>0\right\}$$ I've tried to solve it with change of variable: $$u=x^2-y^2, \quad v=x^2+y^2$$ $$|J|=\frac{1}{8xy}, \quad \frac{1}{8}\iint uv\frac{1}{xy} \,du\,dv$$ How should I proceed? Any suggestion would be great, thanks","How should I proceed? the question is : I've tried to solve it with change of variable: How should I proceed? Any suggestion would be great, thanks","\iint_D (x^4-y^4) \,dx\,dy D= \left\{(x,y):1<x^2-y^2<4, \quad \sqrt{17}<x^2+y^2<5,\quad x<0,\ \  y>0\right\} u=x^2-y^2, \quad v=x^2+y^2 |J|=\frac{1}{8xy}, \quad \frac{1}{8}\iint uv\frac{1}{xy} \,du\,dv","['integration', 'multivariable-calculus', 'multiple-integral']"
79,"How to determine shape of level curves of $V(y,z)= \ln{ \left( \frac{y^2 + (z+z_o)^2}{y^2 + (z-z_o)^2}\right)} $?",How to determine shape of level curves of ?,"V(y,z)= \ln{ \left( \frac{y^2 + (z+z_o)^2}{y^2 + (z-z_o)^2}\right)} ","Context: I expect that a circle will be written as $$R^2 = (x-a)^2 + (y-b)^2,$$ where $R$ is the radius, and $(a,b)$ is the center of the circle. Question: How to determine analytically that the level lines of $$V(y,z)=  \ln{ \left(  \frac{y^2 + (z+z_o)^2}{y^2 + (z-z_o)^2}\right)}$$ are circles?","Context: I expect that a circle will be written as where is the radius, and is the center of the circle. Question: How to determine analytically that the level lines of are circles?","R^2 = (x-a)^2 + (y-b)^2, R (a,b) V(y,z)=  \ln{ \left(  \frac{y^2 + (z+z_o)^2}{y^2 + (z-z_o)^2}\right)}","['multivariable-calculus', 'visualization']"
80,Computing explicitly three integrals involving radial functions,Computing explicitly three integrals involving radial functions,,"How can I compute the following three (similar) integrals? $$ \int_{\mathbb R^N} \left(\frac{1}{1+|x|^2} \right)^{\beta} dx $$ $$ \int_{\mathbb R^N} \left(\frac{1}{1+|x|^2} \right)^{(N+2\alpha)/2} \left(\frac{1}{|x|} -\frac{1}{(1+|x|^2)^{1/2}} \right) dx $$ $$ \int_{\mathbb R^N} \left(\frac{1}{1+|x|^2} \right)^{(N+2\alpha)/2} |x|^{4\alpha - N} dx $$ where $\alpha, \beta >0$ . Since the involved functions are radial, one can make a change of variables to reduce the problems to 1-d problems, but then I don't know how the integral can be computed explicitly or it if can be done in an easy way at all or not.","How can I compute the following three (similar) integrals? where . Since the involved functions are radial, one can make a change of variables to reduce the problems to 1-d problems, but then I don't know how the integral can be computed explicitly or it if can be done in an easy way at all or not.","
\int_{\mathbb R^N} \left(\frac{1}{1+|x|^2} \right)^{\beta} dx
 
\int_{\mathbb R^N} \left(\frac{1}{1+|x|^2} \right)^{(N+2\alpha)/2} \left(\frac{1}{|x|} -\frac{1}{(1+|x|^2)^{1/2}} \right) dx
 
\int_{\mathbb R^N} \left(\frac{1}{1+|x|^2} \right)^{(N+2\alpha)/2} |x|^{4\alpha - N} dx
 \alpha, \beta >0","['calculus', 'integration', 'multivariable-calculus']"
81,Matrix derivative is invertible.,Matrix derivative is invertible.,,"I'm stuck with the next exercise. Let $f,g:U\subseteq\mathbb{R}^{2}\to\mathbb{R}$ a $C^1$ functions over $U$ . Consider $F:U\subseteq\mathbb{R}^{2}\to\mathbb{R}^{2}$ and $H:U\subseteq\mathbb{R}^{2}\to\mathbb{R}$ defined by $F(x,y)=(f(x,y),g(x,y))$ and $H(x,y)=||F(x,y)||^{2}$ . Prove that for all $x\in U$ , one of the next two conditions is not fulfilled. $H$ attains a local maximum at $x$ The derivative of $F$ at $x$ is invertible, i.e., the matrix $DF(x)$ is invertible. My idea was to proceed by contradiction, i.e., suposse that there exist $x_0\in U$ such that (1) and (2) holds. For this, (1) means that there exist $\varepsilon>0$ such that for all $z\in B(x_0,\varepsilon)\subseteq U$ , $H(z)\leq H(x_0)$ . Moreover, $\nabla H(x_0)=\overline{0}$ . Here, $$\nabla H(x_0)=\left(2 f(x_0)\dfrac{\partial f}{\partial x}(x_0)+2g(x_0)\dfrac{\partial g}{\partial x}(x_0), 2 f(x_0)\dfrac{\partial f}{\partial y}(x_0)+2g(x_0)\dfrac{\partial g}{\partial y}(x_0) \right)=(0,0)$$ By (2), $DF(x_0)$ is an invertible matrix. Then $$DF(x_0)=\begin{pmatrix}   \dfrac{\partial f}{\partial x}(x_0) & \dfrac{\partial f}{\partial y}(x_0)\\    \dfrac{\partial g}{\partial x}(x_0) & \dfrac{\partial g}{\partial y}(x_0) \end{pmatrix}$$ Beign invertible, then $\dfrac{\partial f}{\partial x}(x_0)\dfrac{\partial g}{\partial y}(x_0)-\dfrac{\partial f}{\partial y}(x_0)\dfrac{\partial g}{\partial x}(x_0)\neq 0$ . But, from here, I don't know how to continue. Maybe we can join the two conditions to derive a contradiction, but it's not clear to me. Any hint? Thanks for your help.","I'm stuck with the next exercise. Let a functions over . Consider and defined by and . Prove that for all , one of the next two conditions is not fulfilled. attains a local maximum at The derivative of at is invertible, i.e., the matrix is invertible. My idea was to proceed by contradiction, i.e., suposse that there exist such that (1) and (2) holds. For this, (1) means that there exist such that for all , . Moreover, . Here, By (2), is an invertible matrix. Then Beign invertible, then . But, from here, I don't know how to continue. Maybe we can join the two conditions to derive a contradiction, but it's not clear to me. Any hint? Thanks for your help.","f,g:U\subseteq\mathbb{R}^{2}\to\mathbb{R} C^1 U F:U\subseteq\mathbb{R}^{2}\to\mathbb{R}^{2} H:U\subseteq\mathbb{R}^{2}\to\mathbb{R} F(x,y)=(f(x,y),g(x,y)) H(x,y)=||F(x,y)||^{2} x\in U H x F x DF(x) x_0\in U \varepsilon>0 z\in B(x_0,\varepsilon)\subseteq U H(z)\leq H(x_0) \nabla H(x_0)=\overline{0} \nabla H(x_0)=\left(2 f(x_0)\dfrac{\partial f}{\partial x}(x_0)+2g(x_0)\dfrac{\partial g}{\partial x}(x_0), 2 f(x_0)\dfrac{\partial f}{\partial y}(x_0)+2g(x_0)\dfrac{\partial g}{\partial y}(x_0) \right)=(0,0) DF(x_0) DF(x_0)=\begin{pmatrix}
  \dfrac{\partial f}{\partial x}(x_0) & \dfrac{\partial f}{\partial y}(x_0)\\ 
  \dfrac{\partial g}{\partial x}(x_0) & \dfrac{\partial g}{\partial y}(x_0)
\end{pmatrix} \dfrac{\partial f}{\partial x}(x_0)\dfrac{\partial g}{\partial y}(x_0)-\dfrac{\partial f}{\partial y}(x_0)\dfrac{\partial g}{\partial x}(x_0)\neq 0","['calculus', 'multivariable-calculus']"
82,Definition of Differentiability (equivalent form),Definition of Differentiability (equivalent form),,"Suppose $E$ is an open set in $\mathbb R^n$ , $f$ maps $E$ into $\mathbb R^m$ , and $x\in E.$ If there exists a linear transformation $A$ of $\mathbb R^n$ into $\mathbb R^m$ such that $$ \lim_{h\to 0} \frac{|f(x+h)-f(x)-Ah|}{|h|}=0$$ then we say that $f$ is differentiable at $x$ , and we write $$f'(x)=A$$ Question: Can we rewrite the above definition to the following form: $$ f(x+h)-f(x)=f'(x)h + r(h)$$ where the remainder $r(h)$ satisfies $$ \lim_{h\to 0} \frac{|r(h)|}{|h|}=0. $$ If so, how to justify this.","Suppose is an open set in , maps into , and If there exists a linear transformation of into such that then we say that is differentiable at , and we write Question: Can we rewrite the above definition to the following form: where the remainder satisfies If so, how to justify this.",E \mathbb R^n f E \mathbb R^m x\in E. A \mathbb R^n \mathbb R^m  \lim_{h\to 0} \frac{|f(x+h)-f(x)-Ah|}{|h|}=0 f x f'(x)=A  f(x+h)-f(x)=f'(x)h + r(h) r(h)  \lim_{h\to 0} \frac{|r(h)|}{|h|}=0. ,"['real-analysis', 'multivariable-calculus']"
83,If $f$ is continuous then $f$ is uniformly continuous iff $|f|$ is uniformly continuous,If  is continuous then  is uniformly continuous iff  is uniformly continuous,f f |f|,"If $f:\Bbb R^n \to \Bbb R$ is continuous then $f$ is uniformly continuous iff $|f|$ is uniformly continuous. A map $f$ from a metric space $M=(M,d)$ to a metric space $N=(N,\rho)$ is said to be uniformly continuous if for every $\epsilon>0$ , there exists a $\delta>0$ such that $\rho(f(x),f(y))<\epsilon$ whenever $x,y \in M$ satisfy $d(x,y)<\delta$ . Clearly if $f:\Bbb R^n \to \Bbb R$ is uniformly continuous then $|f|$ is uniformly continuous as $|f|(x)-|f|(y)|\leq |f(x)-f(y)|$ but I am having a real trouble showing the converse part. In the region where $f$ is always positive or negative, we will not have any problem but how to deal with the points where $f$ is changing sign. If the zeros of $f$ are finite then also we can take a minimum of all $\delta$ s and conclude the result. What will happen if zeros of $f$ are infinite?","If is continuous then is uniformly continuous iff is uniformly continuous. A map from a metric space to a metric space is said to be uniformly continuous if for every , there exists a such that whenever satisfy . Clearly if is uniformly continuous then is uniformly continuous as but I am having a real trouble showing the converse part. In the region where is always positive or negative, we will not have any problem but how to deal with the points where is changing sign. If the zeros of are finite then also we can take a minimum of all s and conclude the result. What will happen if zeros of are infinite?","f:\Bbb R^n \to \Bbb R f |f| f M=(M,d) N=(N,\rho) \epsilon>0 \delta>0 \rho(f(x),f(y))<\epsilon x,y \in M d(x,y)<\delta f:\Bbb R^n \to \Bbb R |f| |f|(x)-|f|(y)|\leq |f(x)-f(y)| f f f \delta f","['real-analysis', 'multivariable-calculus', 'continuity', 'metric-spaces', 'uniform-continuity']"
84,Dirichlet's Integral to find Volume with gamma function values,Dirichlet's Integral to find Volume with gamma function values,,"Evaluate volume $\iiint_D dx dy dz$ over a domain $D$ where $D$ is the region bounded by $x\ge 0$ , $y\ge 0$ and $z\ge 0$ and ellipsoid $\frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2} = 1$ . My approach: I used the RMS $\ge$ AM inequality (since $x,y,z\ge 0$ and $a,b,c>0$ without loss of generality) to arrive at $\frac{x}{a}+\frac{y}{b}+\frac{z}{c}\le\sqrt{3}$ and then did a substitution $X=\frac{x}{a}$ , $Y=\frac{y}{b}$ and $Z=\frac{z}{c}$ And I got answer to be $\frac{abc(\cdot\Gamma(1)\Gamma(1)\Gamma(1)\cdot(\sqrt{3})^3)}{\Gamma(4)}$ ie $abc\sqrt{3}/2$ My friend's approach: Direct substitution of $X=\frac{x^2}{a^2}$ , $Y=\frac{y^2}{b^2}$ and $Z=\frac{z^2}{c^2}$ and his answer was $(\frac{abc}{8})\frac{(\Gamma(\frac{1}{2})\Gamma(\frac{1}{2})\Gamma(\frac{1}{2}))}{\Gamma(\frac{5}{2})}$ ie $\frac{abc\cdot\pi}{6}$ Professor says mine is wrong and friend's method is right but I am not satisfied without the reason.","Evaluate volume over a domain where is the region bounded by , and and ellipsoid . My approach: I used the RMS AM inequality (since and without loss of generality) to arrive at and then did a substitution , and And I got answer to be ie My friend's approach: Direct substitution of , and and his answer was ie Professor says mine is wrong and friend's method is right but I am not satisfied without the reason.","\iiint_D dx dy dz D D x\ge 0 y\ge 0 z\ge 0 \frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2} = 1 \ge x,y,z\ge 0 a,b,c>0 \frac{x}{a}+\frac{y}{b}+\frac{z}{c}\le\sqrt{3} X=\frac{x}{a} Y=\frac{y}{b} Z=\frac{z}{c} \frac{abc(\cdot\Gamma(1)\Gamma(1)\Gamma(1)\cdot(\sqrt{3})^3)}{\Gamma(4)} abc\sqrt{3}/2 X=\frac{x^2}{a^2} Y=\frac{y^2}{b^2} Z=\frac{z^2}{c^2} (\frac{abc}{8})\frac{(\Gamma(\frac{1}{2})\Gamma(\frac{1}{2})\Gamma(\frac{1}{2}))}{\Gamma(\frac{5}{2})} \frac{abc\cdot\pi}{6}","['integration', 'multivariable-calculus', 'inequality', 'volume', 'gamma-function']"
85,"Functions vanishing to infinite order ""in $1$-mean""","Functions vanishing to infinite order ""in -mean""",1,"When proving certain unique continuation theorems for classes of functions which satisfy some type of PDE inequality, people often talk about functions which vanish to infinite order at a point $x_0$ in the $q$ -mean in the sense that, if $u: \Omega \to \mathbb{C^n}$ is an $L^q$ -function, where $\Omega \subset \mathbb{R}^n$ is an open set: $$\int_{|x - x_0| \leq r} |u(x)|^q dx = O(r^k), \hspace{5pt} \forall k \in \mathbb{N}.$$ By Hlder's inequality, we know that if $u$ vanishes to infinite order in the $q$ -mean, and $p<q$ , then $u$ also vanishes to infinite order in the $p$ -mean. Vanishing to infinite order in the sense that we're used to for smooth functions (i.e, $|u(x)| = O(|x-x_0|^k)$ when $ x \to x_0$ for all $k$ ; this makes sense even for non-smooth functions) is equivalent to vanishing to infinite order in the $p$ -mean for $p = \infty.$ My question can be phrased in two ways: Does the converse hold? I.e, if $p<q$ , and $u$ vanishes to infinite order in the $p$ -mean, does it also vanish to infinite order in the $q$ -mean? A more specific version of the question which I'm interested in, for the purpose of $J$ -holomorphic curves: if $u$ is a smooth function and $n=2$ (for example, $\Omega$ can be a small open disk around $0$ ), does vanishing to infinite order in the $1$ -mean imply vanishing to infinite order in the usual sense, i.e. $D^{\alpha}u(0) = 0$ for all multi-indices $\alpha$ ? This is stated in Section 2.3. of the McDuff-Salamon book ""J-holomorphic curves and symplectic topology."" I was able to show this for $|\alpha| = 0,1$ , but I wasn't able to solve the general case, where I get stuck at an expression of the form $$\int_{|z| \leq r} |(D^k u)_0(z,\dots,z)|dz \leq M_Nr^N, \hspace{7pt} \forall N, \forall r<r_0.$$ I was also able to show that it's true when $u$ is holomorphic, due to the specific nature of the $k$ th derivative in this case.","When proving certain unique continuation theorems for classes of functions which satisfy some type of PDE inequality, people often talk about functions which vanish to infinite order at a point in the -mean in the sense that, if is an -function, where is an open set: By Hlder's inequality, we know that if vanishes to infinite order in the -mean, and , then also vanishes to infinite order in the -mean. Vanishing to infinite order in the sense that we're used to for smooth functions (i.e, when for all ; this makes sense even for non-smooth functions) is equivalent to vanishing to infinite order in the -mean for My question can be phrased in two ways: Does the converse hold? I.e, if , and vanishes to infinite order in the -mean, does it also vanish to infinite order in the -mean? A more specific version of the question which I'm interested in, for the purpose of -holomorphic curves: if is a smooth function and (for example, can be a small open disk around ), does vanishing to infinite order in the -mean imply vanishing to infinite order in the usual sense, i.e. for all multi-indices ? This is stated in Section 2.3. of the McDuff-Salamon book ""J-holomorphic curves and symplectic topology."" I was able to show this for , but I wasn't able to solve the general case, where I get stuck at an expression of the form I was also able to show that it's true when is holomorphic, due to the specific nature of the th derivative in this case.","x_0 q u: \Omega \to \mathbb{C^n} L^q \Omega \subset \mathbb{R}^n \int_{|x - x_0| \leq r} |u(x)|^q dx = O(r^k), \hspace{5pt} \forall k \in \mathbb{N}. u q p<q u p |u(x)| = O(|x-x_0|^k)  x \to x_0 k p p = \infty. p<q u p q J u n=2 \Omega 0 1 D^{\alpha}u(0) = 0 \alpha |\alpha| = 0,1 \int_{|z| \leq r} |(D^k u)_0(z,\dots,z)|dz \leq M_Nr^N, \hspace{7pt} \forall N, \forall r<r_0. u k","['integration', 'multivariable-calculus', 'symplectic-geometry']"
86,Almost Everywhere Conservative Vector Field,Almost Everywhere Conservative Vector Field,,"$$F(x,y) = \left \langle\frac{-y}{\sqrt{x^2 + y^2}}, \frac{x}{\sqrt{x^2 + y^2}},0 \right \rangle $$ Is a famous example of a vector field whose curl is zero away from the origin but does not have potential function associated with it. However, can a potential function be defined almost everywhere that works? I think this function can be increasing as it goes - say starting at zero on the x-axis. When it comes back, it will be at a higher value. But since I have specified that it be defined a.e., the value along the x-axis won't matter. I don't know how to explicitly write this function though.","Is a famous example of a vector field whose curl is zero away from the origin but does not have potential function associated with it. However, can a potential function be defined almost everywhere that works? I think this function can be increasing as it goes - say starting at zero on the x-axis. When it comes back, it will be at a higher value. But since I have specified that it be defined a.e., the value along the x-axis won't matter. I don't know how to explicitly write this function though.","F(x,y) = \left \langle\frac{-y}{\sqrt{x^2 + y^2}}, \frac{x}{\sqrt{x^2 + y^2}},0 \right \rangle ","['multivariable-calculus', 'differential-geometry']"
87,Geometric interpretation of differentiability,Geometric interpretation of differentiability,,"I know that the geometric interpretation of differentiability for a function $f:\mathbb{R}^2\to \mathbb{R}$ in a point $(x_0,y_0)$ is that it admits a tangent plane in the point $P=(x_0,y_0,f(x_0,y_0))$ . Thinking about the concept geometrically, I came up with the following """"conjecture"""". Let for simplicity $\ell_{v}$ the tangent line in the point $P$ to the graphic of $f$ in the direction of the versor $v$ , and let's suppose the function $f$ admits directional derivatives in every direction. $f \text{ differentiable in } (x_0,y_0)$ $\iff$ $\forall v,u \ \ \text{versors}$ , $\ell_{v}$ $\text{and}$ $\ell_{u}\ \text{are coplanar}$ I tried proving this using affine geometry but calculations get pretty messy. I would like if it's true or not(in the former case I would like a proof, in the latter a counterexample)","I know that the geometric interpretation of differentiability for a function in a point is that it admits a tangent plane in the point . Thinking about the concept geometrically, I came up with the following """"conjecture"""". Let for simplicity the tangent line in the point to the graphic of in the direction of the versor , and let's suppose the function admits directional derivatives in every direction. , I tried proving this using affine geometry but calculations get pretty messy. I would like if it's true or not(in the former case I would like a proof, in the latter a counterexample)","f:\mathbb{R}^2\to \mathbb{R} (x_0,y_0) P=(x_0,y_0,f(x_0,y_0)) \ell_{v} P f v f f \text{ differentiable in } (x_0,y_0) \iff \forall v,u \ \ \text{versors} \ell_{v} \text{and} \ell_{u}\ \text{are coplanar}","['multivariable-calculus', 'derivatives', 'affine-geometry', 'geometric-interpretation']"
88,Prove $\iiint_{\mathbb{R}^3}\left ( \frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}+\frac{\partial f}{\partial z} \right) dxdydz=0$,Prove,\iiint_{\mathbb{R}^3}\left ( \frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}+\frac{\partial f}{\partial z} \right) dxdydz=0,"Assume $f(x,y,z)$ is continuously differentiable on $\mathbb R^3$ , and both the integral $$ \iiint_{\mathbb{R}^3}{\left| f\left( x,y,z \right) \right|\text{d}}x\text{d}y\text{d}z $$ and $$ \iiint_{\mathbb{R}^3}{\left( \frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}+\frac{\partial f}{\partial z} \right) \text{d}}x\text{d}y\text{d}z $$ exists. Prove that $$\iiint_{\mathbb{R}^3}{\left( \frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}+\frac{\partial f}{\partial z} \right) \text{d}}x\text{d}y\text{d}z=0$$ I don't have any ideas about the problem. Can anyone help?","Assume is continuously differentiable on , and both the integral and exists. Prove that I don't have any ideas about the problem. Can anyone help?","f(x,y,z) \mathbb R^3 
\iiint_{\mathbb{R}^3}{\left| f\left( x,y,z \right) \right|\text{d}}x\text{d}y\text{d}z
 
\iiint_{\mathbb{R}^3}{\left( \frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}+\frac{\partial f}{\partial z} \right) \text{d}}x\text{d}y\text{d}z
 \iiint_{\mathbb{R}^3}{\left( \frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}+\frac{\partial f}{\partial z} \right) \text{d}}x\text{d}y\text{d}z=0","['real-analysis', 'analysis', 'multivariable-calculus']"
89,Relation between averages in the sphere and the ball,Relation between averages in the sphere and the ball,,"Let $\Omega$ be a domain in $\mathbb R^N$ and $u \in C(\Omega)$ . If $\omega_N$ denotes the volume of $B_1(0) \subset \mathbb R^N$ and $0 < r < d(x, \partial \Omega)$ , the averages are $$ A_b(x) = \frac{1}{\omega_N r^N} \int_{B_r(x)} u(y) \ dy $$ in the ball, and $$ A_s(x) = \frac{1}{N \omega_N r^{N - 1}} \int_{\partial B_r(x)} u(y) \ d\sigma(y) $$ in the sphere. We know that if $u$ is harmonic, then $A_b(x) = A_s(x) = u(x)$ . In general, can one say something like $A_s \leq (\geq) A_b$ ? I kindly ask for a hint, in case the answer is positive, or for examples, in case the answer is negative. EDIT : Ted's hint made me think of the following: take $u$ to be negative inside the ball and $0$ on the boundary. Then of course $A_b < 0 = A_s$ . On the other hand, if $u$ is positive, it is clear that $A_b > 0 = A_s$ . We conclude that the answer to the question is negative in general. This seems to be very simple, am I missing something?","Let be a domain in and . If denotes the volume of and , the averages are in the ball, and in the sphere. We know that if is harmonic, then . In general, can one say something like ? I kindly ask for a hint, in case the answer is positive, or for examples, in case the answer is negative. EDIT : Ted's hint made me think of the following: take to be negative inside the ball and on the boundary. Then of course . On the other hand, if is positive, it is clear that . We conclude that the answer to the question is negative in general. This seems to be very simple, am I missing something?","\Omega \mathbb R^N u \in C(\Omega) \omega_N B_1(0) \subset \mathbb R^N 0 < r < d(x, \partial \Omega) 
A_b(x) = \frac{1}{\omega_N r^N} \int_{B_r(x)} u(y) \ dy
 
A_s(x) = \frac{1}{N \omega_N r^{N - 1}} \int_{\partial B_r(x)} u(y) \ d\sigma(y)
 u A_b(x) = A_s(x) = u(x) A_s \leq (\geq) A_b u 0 A_b < 0 = A_s u A_b > 0 = A_s","['real-analysis', 'multivariable-calculus', 'partial-differential-equations']"
90,"Prove that $\iiint _{f^{-1}[a,b]}f(x,y,z)dxdydz=\int _a ^b tF'(t)dt$",Prove that,"\iiint _{f^{-1}[a,b]}f(x,y,z)dxdydz=\int _a ^b tF'(t)dt","Suppose $f\in C^1(\mathbb{R}^3)$ , for every $t\in \mathbb{R}$ , $f^{-1}(t)$ is a simple(1) closed surface, and let the volume of the 3-D object surrounded by $f^{-1}(t)$ be $F(t)$ . If $F:[0,+\infty] \to \mathbb{R}$ is continuously differentialble, prove that $$\iiint _{f^{-1}[a,b]}f(x,y,z)dxdydz=\int _a ^b tF'(t)dt.$$ (1):To be clear, here is the definition of a simple domain: My first thought was to use the coarea formula . Then $$ \iiint _{f^{-1}[a,b]}f(x,y,z)dxdydz=\int_a^b dt \int _{f^{-1}(t)} \frac{f}{|\nabla f|} d\sigma = \int _a ^b tdt \int _{f^{-1}(t)} \frac{1}{|\nabla f|} d\sigma. $$ Well, as you have seen the RHS is pretty close to $\int _a^b tF'(t)dt$ . But from here I met 2 major difficulties:(1)the conditions required by the coarea formula were not all satisfied: we need $f\in C^2(\mathbb{R})$ .(2) even if the coarea formula is allowed, I still cannot proceed. By definition $F(t)=\iiint _D 1dxdydz$ where $\partial D=f^{-1}(t)$ . But how should I get $F'(t)=\int _{f^{-1}(t)} \frac{1}{|\nabla f|} d\sigma$ ? Thank you in advance. Example: let $f(x,y,z)=x^2+y^2+z^2,[a,b]=[0,1]. F(t)=\frac{4\pi t^{3/2}}{3},F'(t)=2\pi \sqrt {t}.$ Hence, $$ \iiint _{f^{-1}[0,1]}f(x,y,z)dxdydz=\iiint _{x^2+y^2+z^2 \leq 1}(x^2+y^2+z^2)dxdydz= \\ \int _{[0,2\pi] \times [0,\pi] \times [0,1]} r^2 \cdot r^2 \sin \phi d\theta d\phi dr = 4\pi/5. $$ $$ \int _0 ^1 tF'(t)dt = \int _0 ^ 1 t \cdot 2\pi \sqrt {t} dt = 4\pi/5. $$","Suppose , for every , is a simple(1) closed surface, and let the volume of the 3-D object surrounded by be . If is continuously differentialble, prove that (1):To be clear, here is the definition of a simple domain: My first thought was to use the coarea formula . Then Well, as you have seen the RHS is pretty close to . But from here I met 2 major difficulties:(1)the conditions required by the coarea formula were not all satisfied: we need .(2) even if the coarea formula is allowed, I still cannot proceed. By definition where . But how should I get ? Thank you in advance. Example: let Hence,","f\in C^1(\mathbb{R}^3) t\in \mathbb{R} f^{-1}(t) f^{-1}(t) F(t) F:[0,+\infty] \to \mathbb{R} \iiint _{f^{-1}[a,b]}f(x,y,z)dxdydz=\int _a ^b tF'(t)dt. 
\iiint _{f^{-1}[a,b]}f(x,y,z)dxdydz=\int_a^b dt \int _{f^{-1}(t)} \frac{f}{|\nabla f|} d\sigma = \int _a ^b tdt \int _{f^{-1}(t)} \frac{1}{|\nabla f|} d\sigma.
 \int _a^b tF'(t)dt f\in C^2(\mathbb{R}) F(t)=\iiint _D 1dxdydz \partial D=f^{-1}(t) F'(t)=\int _{f^{-1}(t)} \frac{1}{|\nabla f|} d\sigma f(x,y,z)=x^2+y^2+z^2,[a,b]=[0,1]. F(t)=\frac{4\pi t^{3/2}}{3},F'(t)=2\pi \sqrt {t}. 
\iiint _{f^{-1}[0,1]}f(x,y,z)dxdydz=\iiint _{x^2+y^2+z^2 \leq 1}(x^2+y^2+z^2)dxdydz= \\ \int _{[0,2\pi] \times [0,\pi] \times [0,1]} r^2 \cdot r^2 \sin \phi d\theta d\phi dr = 4\pi/5.
 
\int _0 ^1 tF'(t)dt = \int _0 ^ 1 t \cdot 2\pi \sqrt {t} dt = 4\pi/5.
","['multivariable-calculus', 'multiple-integral', 'surface-integrals']"
91,"Does the limit exist? Limit going to $(0,0)$",Does the limit exist? Limit going to,"(0,0)","I tried to solve an exercise Does $\lim\limits_{x, y\to\ (0,0)} \frac{(e^{x+y} - 1)}{x^2+y^2}$ exist? I was thinking about looking at $(x, 0), (0,y)$ and $x=y=t$ and say it doesn't exist but i was told its not a proof. So could you please let me know how to solve this and this kind of questions? Thank you",I tried to solve an exercise Does exist? I was thinking about looking at and and say it doesn't exist but i was told its not a proof. So could you please let me know how to solve this and this kind of questions? Thank you,"\lim\limits_{x, y\to\ (0,0)} \frac{(e^{x+y} - 1)}{x^2+y^2} (x, 0), (0,y) x=y=t","['real-analysis', 'calculus', 'limits', 'multivariable-calculus', 'continuity']"
92,Prove that an irrotational vector field is conservative by showing the $\nabla$ of the scalar potential of $v$ written as an integral is equal to $v$,Prove that an irrotational vector field is conservative by showing the  of the scalar potential of  written as an integral is equal to,\nabla v v,"I am trying to prove that if $\vec v$ is a continuously differentiable vector field define on $\mathbb R^n$ and satisfies $$\frac {\partial v_i} {\partial x_j} = \frac {\partial v_j} {\partial x_i}$$ then $\nabla f(x) = \vec v(x)$ where $f(x) = \int_0^1 x \cdot \vec v(tx) \, \mathrm d t$ . This is my attempt: It suffices to prove $v_i(x) = \frac {\partial f(x)} {\partial x_i}$ . I shall use implicit summation over $j$ here when I apply the chain rule. I first use the fundamental theorem of calculus: $$\begin{align} v_i(x) & = \int _0^1  \frac {\partial v_i(tx)} {\partial t}\, \mathrm d t + v_i(0) \\ & = \int_0^1 \frac {\partial v_i(tx)} {\partial x_j} \frac {\partial x_j} {\partial t}\, \mathrm d t + v_i(0) \\ & = \int_0^1 \frac {\partial v_j(tx)} {\partial x_i}  \frac {\partial x_j} {\partial t}\, \mathrm d t + v_i(0) \end{align}$$ I shall also use implicit summation over $j$ here when I expand out the dot product: $$\begin{align} \frac {\partial f(x)} {\partial x_i} & = \int_0^1 \frac {\partial } {\partial x_i} \left( x_j v_j(tx)\right) \, \mathrm d t \\ & = \int_0^1 v_i(tx) + x_j \frac {\partial v_j(tx)} {\partial x_i}\, \mathrm d t\end{align}$$ I am rather stuck here though and not sure how to proceed. Any hints would be appreciated. Have I made any mistakes so far?",I am trying to prove that if is a continuously differentiable vector field define on and satisfies then where . This is my attempt: It suffices to prove . I shall use implicit summation over here when I apply the chain rule. I first use the fundamental theorem of calculus: I shall also use implicit summation over here when I expand out the dot product: I am rather stuck here though and not sure how to proceed. Any hints would be appreciated. Have I made any mistakes so far?,"\vec v \mathbb R^n \frac {\partial v_i} {\partial x_j} = \frac {\partial v_j} {\partial x_i} \nabla f(x) = \vec v(x) f(x) = \int_0^1 x \cdot \vec v(tx) \, \mathrm d t v_i(x) = \frac {\partial f(x)} {\partial x_i} j \begin{align} v_i(x) & = \int _0^1  \frac {\partial v_i(tx)} {\partial t}\, \mathrm d t + v_i(0) \\ & = \int_0^1 \frac {\partial v_i(tx)} {\partial x_j} \frac {\partial x_j} {\partial t}\, \mathrm d t + v_i(0) \\ & = \int_0^1 \frac {\partial v_j(tx)} {\partial x_i}  \frac {\partial x_j} {\partial t}\, \mathrm d t + v_i(0) \end{align} j \begin{align} \frac {\partial f(x)} {\partial x_i} & = \int_0^1 \frac {\partial } {\partial x_i} \left( x_j v_j(tx)\right) \, \mathrm d t \\ & = \int_0^1 v_i(tx) + x_j \frac {\partial v_j(tx)} {\partial x_i}\, \mathrm d t\end{align}","['multivariable-calculus', 'vector-fields', 'scalar-fields']"
93,Proving invariance of a functional under transformation,Proving invariance of a functional under transformation,,"Suppose we have a functional of the form $$ J(y)= \int_{x_{0}}^{x_{1}} f(x,y,y')dx $$ and a smooth transformation $X=\theta (x,y: \epsilon)$ , $Y=\psi (x,y: \epsilon)$ where $\epsilon$ is a parameter. I have the following definition of invariance under a transformation: Definition. The functional $J$ is invariant under the above transformation if, for all $\epsilon$ sufficiently small, in any subinterval $[a,b] \subset [x_{0},x_{1}]$ we have that $$ \int_{a}^{b} f(x,y,y')dx= \displaystyle \int_{a_{\epsilon}}^{b_{\epsilon}} f(X,Y,Y')dX $$ for all smooth functions $y$ defined on $[a,b]$ . Here, $a_{\epsilon}=\theta (a,y(a): \epsilon)$ , $b_{\epsilon}=\theta (b,y(b): \epsilon)$ Once said that, I'm interested in proving the invariance of the functional $$ J(y)=\int_{x_{0}}^{x_{1}} xy'^2dx $$ under the transformation $$ \begin{cases} X=x+2\epsilon x \ln(x)\\ Y=(1+\epsilon)y \end{cases} $$ My attempt: This would be easier if we could solve explicitly for $x$ in the first equation of the transformation, but, since we can't, suppose $x=\Theta (X; \epsilon)$ such inverse function exist(locally and for $\epsilon$ sufficiently small because of the inverse function theorem). On the other hand, we easily get $y=1/(1+\epsilon) Y$ . Therefore, $$ y'(x)=\displaystyle \frac{dy}{dx}=\displaystyle \frac{Y'(X)}{(1+ \epsilon) \Theta'(X)} $$ which implies that $$ xy'dx= \frac{\Theta(X)}{\Theta '(X)} \frac{Y'(X)^2}{(1+\epsilon)^2}dX $$ So, as you can see, if I could prove $$ \frac{\Theta(X)}{(1+\epsilon)^2 \Theta '(X)}=X $$ I'd be done, but I could not prove it, and actually, it doesn't seem true. Any help? Thanks in advance. Update: Of course we can also try to express $XY'(X)^2 dX$ in terms of $x$ and $y'(x)$ . In this case, we get $$ XY'(X)^2 dX= \displaystyle \frac{x+2 \epsilon x ln(x)}{1+ 2 \epsilon ln(x)+2 \epsilon} (1+ \epsilon)^2 y'(x)^2 dx  $$ In any case, it is not clear to me how to reconcile this expressions with the definition :( .","Suppose we have a functional of the form and a smooth transformation , where is a parameter. I have the following definition of invariance under a transformation: Definition. The functional is invariant under the above transformation if, for all sufficiently small, in any subinterval we have that for all smooth functions defined on . Here, , Once said that, I'm interested in proving the invariance of the functional under the transformation My attempt: This would be easier if we could solve explicitly for in the first equation of the transformation, but, since we can't, suppose such inverse function exist(locally and for sufficiently small because of the inverse function theorem). On the other hand, we easily get . Therefore, which implies that So, as you can see, if I could prove I'd be done, but I could not prove it, and actually, it doesn't seem true. Any help? Thanks in advance. Update: Of course we can also try to express in terms of and . In this case, we get In any case, it is not clear to me how to reconcile this expressions with the definition :( .","
J(y)= \int_{x_{0}}^{x_{1}} f(x,y,y')dx
 X=\theta (x,y: \epsilon) Y=\psi (x,y: \epsilon) \epsilon J \epsilon [a,b] \subset [x_{0},x_{1}] 
\int_{a}^{b} f(x,y,y')dx= \displaystyle \int_{a_{\epsilon}}^{b_{\epsilon}} f(X,Y,Y')dX
 y [a,b] a_{\epsilon}=\theta (a,y(a): \epsilon) b_{\epsilon}=\theta (b,y(b): \epsilon) 
J(y)=\int_{x_{0}}^{x_{1}} xy'^2dx
 
\begin{cases}
X=x+2\epsilon x \ln(x)\\
Y=(1+\epsilon)y
\end{cases}
 x x=\Theta (X; \epsilon) \epsilon y=1/(1+\epsilon) Y 
y'(x)=\displaystyle \frac{dy}{dx}=\displaystyle \frac{Y'(X)}{(1+ \epsilon) \Theta'(X)}
 
xy'dx= \frac{\Theta(X)}{\Theta '(X)} \frac{Y'(X)^2}{(1+\epsilon)^2}dX
 
\frac{\Theta(X)}{(1+\epsilon)^2 \Theta '(X)}=X
 XY'(X)^2 dX x y'(x) 
XY'(X)^2 dX= \displaystyle \frac{x+2 \epsilon x ln(x)}{1+ 2 \epsilon ln(x)+2 \epsilon} (1+ \epsilon)^2 y'(x)^2 dx 
","['functional-analysis', 'multivariable-calculus', 'calculus-of-variations']"
94,Constant derivative of a function implies is an affine map.,Constant derivative of a function implies is an affine map.,,"I am trying to prove the next: Let $f :\Omega\subset\mathbb{R}^n\rightarrow\mathbb{R}^m$ be differentiable, where $\Omega$ is an open connected subset of $\mathbb{R}^n$ . Suppose $D_{f}(x)$ is constant on $\Omega$ , that is, $D_{f}(x) = T$ for all $x\in\Omega.$ Show that $f$ is the restriction to $\Omega$ of an affine transformation. I was trying to prove that the set $A=\{x\in\Omega: f(x) = f(a) + T(x-a)\},$ where $a\in\Omega$ is fixed, is an open and closed subset of $\mathbb{R}^n,$ then for connected of $\Omega$ implies $\Omega = A$ and the proposition is true, but it seems quite hard to prove that; I was thinking about the definition of differentaiblity of $f$ in $a,$ however the best that we get of such definition is an inequality. I saw a proof of this using gradient theorem but I do not know yet integration; this proposition appears in the section of differential calculus only, hence it must be possible prove it without integration. Any kind of help is thanked in advanced.","I am trying to prove the next: Let be differentiable, where is an open connected subset of . Suppose is constant on , that is, for all Show that is the restriction to of an affine transformation. I was trying to prove that the set where is fixed, is an open and closed subset of then for connected of implies and the proposition is true, but it seems quite hard to prove that; I was thinking about the definition of differentaiblity of in however the best that we get of such definition is an inequality. I saw a proof of this using gradient theorem but I do not know yet integration; this proposition appears in the section of differential calculus only, hence it must be possible prove it without integration. Any kind of help is thanked in advanced.","f :\Omega\subset\mathbb{R}^n\rightarrow\mathbb{R}^m \Omega \mathbb{R}^n D_{f}(x) \Omega D_{f}(x) = T x\in\Omega. f \Omega A=\{x\in\Omega: f(x) = f(a) + T(x-a)\}, a\in\Omega \mathbb{R}^n, \Omega \Omega = A f a,",['multivariable-calculus']
95,Tree diagram Chain Rule,Tree diagram Chain Rule,,"Let $$ \begin{align} w &= f(x,y,z)\\ z &= z(x,y)\\ x &= x(t)\\ y &= y(t)\\ \end{align} $$ be differentiable functions. Find the formula for the derivative of $w$ with respect to $t$ . My answer: $w = f(x,y,z) \implies \dfrac{dw}{dt} = \dfrac{dw}{dx} \cdot \dfrac{dw}{dy} \cdot\dfrac{dw}{dz}$ $$ \begin{align} z &= z(x,y) \\ x &= x(t) \\ y &= y(t) \\ \end{align} \implies \frac{dz}{dt} = \frac{\partial z}{\partial x} \cdot \frac{dx}{dt} + \frac{\partial z}{\partial y}\cdot\frac{dy}{dt} $$ It is getting confusing to me. I used to make a tree diagram to make it a bit easier but this time, it is out of my reach. Can someone give me a hint?","Let be differentiable functions. Find the formula for the derivative of with respect to . My answer: It is getting confusing to me. I used to make a tree diagram to make it a bit easier but this time, it is out of my reach. Can someone give me a hint?","
\begin{align}
w &= f(x,y,z)\\
z &= z(x,y)\\
x &= x(t)\\
y &= y(t)\\
\end{align}
 w t w = f(x,y,z) \implies \dfrac{dw}{dt} = \dfrac{dw}{dx} \cdot \dfrac{dw}{dy} \cdot\dfrac{dw}{dz} 
\begin{align}
z &= z(x,y) \\
x &= x(t) \\
y &= y(t) \\
\end{align}
\implies \frac{dz}{dt} = \frac{\partial z}{\partial x} \cdot \frac{dx}{dt} + \frac{\partial z}{\partial y}\cdot\frac{dy}{dt}
",['multivariable-calculus']
96,How did the author take the gradient to get $\overline{W} \Leftarrow \overline{W} - \alpha \nabla_{W} L_i$?,How did the author take the gradient to get ?,\overline{W} \Leftarrow \overline{W} - \alpha \nabla_{W} L_i,"I am currently studying the textbook Neural Networks and Deep Learning by Charu C. Aggarwal. Chapter 1.2.1.1 What Objective Function Is the Perceptron Optimizing? says the following: Can we find a smooth loss function, whose gradient turns out to be the perceptron update? The number of classification errors in a binary classification problem can be written in the form of a $0/1$ loss function for training data point $(\overline{X_i}, y_i)$ as follows: $$L_i^{(0/1)} = \dfrac{1}{2} (y_i - \text{sign}\{ \overline{W} \cdot \overline{X_i} \})^2 = 1 - y_i \cdot \text{sign} \{ \overline{W} \cdot \overline{X_i} \} \tag{1.7}$$ The simplification to the right-hand side of the above objective function is obtained by setting both $y_i^2$ and $\text{sign}\{ \overline{W} \cdot \overline{X_i} \}^2$ to $1$ , since they are obtained by squaring a value drawn from $\{ -1, +1 \}$ . However, this objective function is not differentiable, because it has a staircase-like shape, especially when it is added over multiple points. Note that the $0/1$ loss above is dominated by the term $- y_i \cdot \text{sign} \{ \overline{W} \cdot \overline{X_i} \}$ , in which the sign function causes most of the problems associated with non-differentiability. Since neural networks are defined by gradient-based optimization, we need to define a smooth objective function that is responsible for the perceptron updates. It can be shown that the updates of the perceptron implicitly optimize the perceptron criterion. This objective function is defined by dropping the sign function in the above $0/1$ loss and setting negative values to $0$ in order to treat all correct predictions in a uniform and lossless way: $$L_i = \max\{ - y_i ( \overline{W} \cdot \overline{X_i} ), 0 \} \tag{1.8}$$ The reader is encouraged to use calculus to verify that the gradient of this smoothed objective function leads to the perceptron update, and the update of the perceptron is essentially $\overline{W} \Leftarrow \overline{W} - \alpha \nabla_{W} L_i$ . How did the author take the gradient to get $\overline{W} \Leftarrow \overline{W} - \alpha \nabla_{W} L_i$ ?","I am currently studying the textbook Neural Networks and Deep Learning by Charu C. Aggarwal. Chapter 1.2.1.1 What Objective Function Is the Perceptron Optimizing? says the following: Can we find a smooth loss function, whose gradient turns out to be the perceptron update? The number of classification errors in a binary classification problem can be written in the form of a loss function for training data point as follows: The simplification to the right-hand side of the above objective function is obtained by setting both and to , since they are obtained by squaring a value drawn from . However, this objective function is not differentiable, because it has a staircase-like shape, especially when it is added over multiple points. Note that the loss above is dominated by the term , in which the sign function causes most of the problems associated with non-differentiability. Since neural networks are defined by gradient-based optimization, we need to define a smooth objective function that is responsible for the perceptron updates. It can be shown that the updates of the perceptron implicitly optimize the perceptron criterion. This objective function is defined by dropping the sign function in the above loss and setting negative values to in order to treat all correct predictions in a uniform and lossless way: The reader is encouraged to use calculus to verify that the gradient of this smoothed objective function leads to the perceptron update, and the update of the perceptron is essentially . How did the author take the gradient to get ?","0/1 (\overline{X_i}, y_i) L_i^{(0/1)} = \dfrac{1}{2} (y_i - \text{sign}\{ \overline{W} \cdot \overline{X_i} \})^2 = 1 - y_i \cdot \text{sign} \{ \overline{W} \cdot \overline{X_i} \} \tag{1.7} y_i^2 \text{sign}\{ \overline{W} \cdot \overline{X_i} \}^2 1 \{ -1, +1 \} 0/1 - y_i \cdot \text{sign} \{ \overline{W} \cdot \overline{X_i} \} 0/1 0 L_i = \max\{ - y_i ( \overline{W} \cdot \overline{X_i} ), 0 \} \tag{1.8} \overline{W} \Leftarrow \overline{W} - \alpha \nabla_{W} L_i \overline{W} \Leftarrow \overline{W} - \alpha \nabla_{W} L_i","['multivariable-calculus', 'vector-analysis', 'machine-learning', 'gradient-descent', 'neural-networks']"
97,"Finding Grad of G,","Finding Grad of G,",,"OK, so I'm out of ideas. The question is: Let  f(x,y) be a function with continous partial derivatives that upholds: $$ \bigtriangledown f (0,-8) = (-3\widehat{i}, 5\widehat{j}) $$ let g(x,y) be the function : $$ g(x,y) = f(xy+x^2 , xy-y^2) $$ Calculate: (a Vector) $$ \bigtriangledown g(-2,2) = ? $$ My take on this: $$ \bigtriangledown g = \left(\frac{\partial \:g}{\partial \:x},\:\frac{\partial \:g}{\partial \:y}\right) $$ let u,v be: $$ u\left(x,y\right)\:=\:xy+x^2\:\:,\:v\left(x,y\right)\:=\:xy-y^{2\:} $$ and then By the chain Rule: $$ \frac{\partial g}{\partial x}\:=\:\frac{\partial g}{\partial u}\:\cdot \:\frac{\partial u}{\partial x}\:+\:\frac{\partial g}{\partial v}\cdot \frac{\partial v}{\partial x} $$ $$ \frac{\partial g}{\partial y}\:=\:\frac{\partial g}{\partial u}\:\cdot \:\frac{\partial u}{\partial y}\:+\:\frac{\partial g}{\partial v}\cdot \frac{\partial v}{\partial y} $$ Placing du/dx du/dy and dv/dx dv/dy , and evaluating at the point (-2,2) $$ \frac{\partial g}{\partial y}\:=\:+2\frac{\partial g}{\partial u}\:+6\:\frac{\partial g}{\partial v} $$ $$ \frac{\partial g}{\partial x}\:=\:+2\frac{\partial g}{\partial u}\:-2\:\frac{\partial g}{\partial v} $$ My question is how to procceed further?, What do i do with The partial derivatives of g with respect to u and v.","OK, so I'm out of ideas. The question is: Let  f(x,y) be a function with continous partial derivatives that upholds: let g(x,y) be the function : Calculate: (a Vector) My take on this: let u,v be: and then By the chain Rule: Placing du/dx du/dy and dv/dx dv/dy , and evaluating at the point (-2,2) My question is how to procceed further?, What do i do with The partial derivatives of g with respect to u and v.","
\bigtriangledown f (0,-8) = (-3\widehat{i}, 5\widehat{j})
 
g(x,y) = f(xy+x^2 , xy-y^2)
 
\bigtriangledown g(-2,2) = ?
 
\bigtriangledown g = \left(\frac{\partial \:g}{\partial \:x},\:\frac{\partial \:g}{\partial \:y}\right)
 
u\left(x,y\right)\:=\:xy+x^2\:\:,\:v\left(x,y\right)\:=\:xy-y^{2\:}
 
\frac{\partial g}{\partial x}\:=\:\frac{\partial g}{\partial u}\:\cdot \:\frac{\partial u}{\partial x}\:+\:\frac{\partial g}{\partial v}\cdot \frac{\partial v}{\partial x}
 
\frac{\partial g}{\partial y}\:=\:\frac{\partial g}{\partial u}\:\cdot \:\frac{\partial u}{\partial y}\:+\:\frac{\partial g}{\partial v}\cdot \frac{\partial v}{\partial y}
 
\frac{\partial g}{\partial y}\:=\:+2\frac{\partial g}{\partial u}\:+6\:\frac{\partial g}{\partial v}
 
\frac{\partial g}{\partial x}\:=\:+2\frac{\partial g}{\partial u}\:-2\:\frac{\partial g}{\partial v}
","['calculus', 'multivariable-calculus', 'vector-analysis', 'chain-rule', 'scalar-fields']"
98,Is the function injective if the Jacobian has full column rank?,Is the function injective if the Jacobian has full column rank?,,"Let $f:\mathbb{R}^m \to \mathbb{R}^n: x \to f(x)$ be a continuous and differentiable function with $m < n$ . If the Jacobian $J_f$ has full column rank (i.e., rank= $m$ ) $\forall x \in \mathbb{R}^m$ , does this imply that $f$ is an injective function? If yes, can I get a reference for this result?","Let be a continuous and differentiable function with . If the Jacobian has full column rank (i.e., rank= ) , does this imply that is an injective function? If yes, can I get a reference for this result?",f:\mathbb{R}^m \to \mathbb{R}^n: x \to f(x) m < n J_f m \forall x \in \mathbb{R}^m f,"['linear-algebra', 'multivariable-calculus']"
99,"Calculate $\lim _{ n\to \infty } \int _{ |x|<n }{ \int _{ |y|<n }{\sin(x^2+y^2)\,dx\,dy } } $",Calculate,"\lim _{ n\to \infty } \int _{ |x|<n }{ \int _{ |y|<n }{\sin(x^2+y^2)\,dx\,dy } } ","$$\lim _{ n\to \infty } \int _{ |x|<n }{ \int _{ |y|<n }{ \sin(x^2+y^2)\,dx\,dy } }$$ Seeing that $n\to \infty$ I deduce that yes $|x|<n$ and $|y|<n$ then $-\infty<x<\infty$ and $-\infty<y<\infty$ But I don't know if I can use these limits. Another observation I see is in the argument of the function $\sin(x^2+y^2)$ Which makes me think that I can use polar coordinates. Any suggestions on how to calculate such a limit? How could you write the limits of the integral?",Seeing that I deduce that yes and then and But I don't know if I can use these limits. Another observation I see is in the argument of the function Which makes me think that I can use polar coordinates. Any suggestions on how to calculate such a limit? How could you write the limits of the integral?,"\lim _{ n\to \infty } \int _{ |x|<n }{ \int _{ |y|<n }{ \sin(x^2+y^2)\,dx\,dy } } n\to \infty |x|<n |y|<n -\infty<x<\infty -\infty<y<\infty \sin(x^2+y^2)","['calculus', 'integration', 'complex-analysis', 'algebra-precalculus', 'multivariable-calculus']"
