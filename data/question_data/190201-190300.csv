,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How to apply the definition of a derivative with a piecewise function?,How to apply the definition of a derivative with a piecewise function?,,"Given the function: $$f(x) = \begin{cases} x^2+1 & \text{if $x\ge0$} \\ x^2-1 & \text{if $x < 0$} \end{cases}$$ Question: are we justified to say that the derivative at $f(0)$ exists? If so, what is $f'(0)$? And how do we justify it? Of course I do realize that the function isn't continuous at $x=0$ but still since the slope near $x=0$ seems equal near $0+$ and $0-$ I wondered why we can't say that $f'(0)=0$ What I tried is this: $f_+'(0)=\lim\limits_{h \to 0+}\frac{(x+h)^2+1-(x^2+1)}{h}=\lim\limits_{h \to 0+}\frac{(0+h)^2+1-(0^2+1)}{h}=\lim\limits_{h \to 0+}\frac{h^2}{h}=h=0$ $f_-'(0)=\lim\limits_{h \to 0-}\frac{(x+h)^2+1-(x^2+1)}{h}=\lim\limits_{h \to 0-}\frac{(0+h)^2+1-(0^2+1)}{h}=\lim\limits_{h \to 0-}\frac{h^2}{h}=h=0$ My conclusion is that since both the right and left limit using the definition of the derivative exist and generate the same answer the limit exists such that $f'(0)=0$. Apparently this is not true, so what is my mistake?","Given the function: $$f(x) = \begin{cases} x^2+1 & \text{if $x\ge0$} \\ x^2-1 & \text{if $x < 0$} \end{cases}$$ Question: are we justified to say that the derivative at $f(0)$ exists? If so, what is $f'(0)$? And how do we justify it? Of course I do realize that the function isn't continuous at $x=0$ but still since the slope near $x=0$ seems equal near $0+$ and $0-$ I wondered why we can't say that $f'(0)=0$ What I tried is this: $f_+'(0)=\lim\limits_{h \to 0+}\frac{(x+h)^2+1-(x^2+1)}{h}=\lim\limits_{h \to 0+}\frac{(0+h)^2+1-(0^2+1)}{h}=\lim\limits_{h \to 0+}\frac{h^2}{h}=h=0$ $f_-'(0)=\lim\limits_{h \to 0-}\frac{(x+h)^2+1-(x^2+1)}{h}=\lim\limits_{h \to 0-}\frac{(0+h)^2+1-(0^2+1)}{h}=\lim\limits_{h \to 0-}\frac{h^2}{h}=h=0$ My conclusion is that since both the right and left limit using the definition of the derivative exist and generate the same answer the limit exists such that $f'(0)=0$. Apparently this is not true, so what is my mistake?",,['derivatives']
1,Deriving Laplace Transform of Laguerre polynomial,Deriving Laplace Transform of Laguerre polynomial,,"I'm given this definition for the Laguerre polynomials: $$L_n(t)=\frac{e^t}{n!}\frac{d^n}{dt^n}\left[t^ne^{-t}\right],~\text{for }n=0,1,2...$$ and I have to show that the Laplace transform is $$\frac{1}{s}\left(\frac{s-1}{s}\right)^n$$ My first attempt was to find a formula for the $n$th derivative of $t^ne^{-t}$, which I found to be $$\frac{d^n}{dt^n}\left[t^ne^{-t}\right]=e^{-t}\sum_{k=0}^n\frac{n!}{(n-k)!}\binom nk (-t)^{n-k}$$ So from here I tried taking the transform: $$\begin{align*}\mathcal{L}_s\left\{\frac{e^t}{n!}\frac{d^n}{dt^n}\left[t^ne^{-t}\right]\right\}&=\frac{1}{n!}\mathcal{L}_{s-1}\left\{e^{-t}\sum_{k=0}^n\frac{n!}{(n-k)!}\binom nk (-t)^{n-k}\right\}\end{align*}$$ where $\mathcal{L}_s\{f(t)\}=F(s)$ and $\mathcal{L}_s\{e^tf(t)\}=\mathcal{L}_{s-1}\{f(t)\}=F(s-1)$ (in case the notation was unfamiliar). I'm not sure how to continue with this. The factor of $e^{-t}$ seems to undo the shift to $s-1$, and I'm not sure if that's a problem or not. I don't think I made a mistake with the $n$th derivative formula, but I think there might be a different way to express it that would make computation easier. Any help is appreciated!","I'm given this definition for the Laguerre polynomials: $$L_n(t)=\frac{e^t}{n!}\frac{d^n}{dt^n}\left[t^ne^{-t}\right],~\text{for }n=0,1,2...$$ and I have to show that the Laplace transform is $$\frac{1}{s}\left(\frac{s-1}{s}\right)^n$$ My first attempt was to find a formula for the $n$th derivative of $t^ne^{-t}$, which I found to be $$\frac{d^n}{dt^n}\left[t^ne^{-t}\right]=e^{-t}\sum_{k=0}^n\frac{n!}{(n-k)!}\binom nk (-t)^{n-k}$$ So from here I tried taking the transform: $$\begin{align*}\mathcal{L}_s\left\{\frac{e^t}{n!}\frac{d^n}{dt^n}\left[t^ne^{-t}\right]\right\}&=\frac{1}{n!}\mathcal{L}_{s-1}\left\{e^{-t}\sum_{k=0}^n\frac{n!}{(n-k)!}\binom nk (-t)^{n-k}\right\}\end{align*}$$ where $\mathcal{L}_s\{f(t)\}=F(s)$ and $\mathcal{L}_s\{e^tf(t)\}=\mathcal{L}_{s-1}\{f(t)\}=F(s-1)$ (in case the notation was unfamiliar). I'm not sure how to continue with this. The factor of $e^{-t}$ seems to undo the shift to $s-1$, and I'm not sure if that's a problem or not. I don't think I made a mistake with the $n$th derivative formula, but I think there might be a different way to express it that would make computation easier. Any help is appreciated!",,"['polynomials', 'derivatives', 'laplace-transform']"
2,Is it possible to have a function differentiable but not continuous in a given interval?,Is it possible to have a function differentiable but not continuous in a given interval?,,Is there any possible function that is not continuous but differentiable in a given interval. It sounds non-logical to me since differentiation is a special limit function in itself therefore non-continuous should be meaning non-differentiable either. Am I right?,Is there any possible function that is not continuous but differentiable in a given interval. It sounds non-logical to me since differentiation is a special limit function in itself therefore non-continuous should be meaning non-differentiable either. Am I right?,,"['derivatives', 'continuity']"
3,Functional derivatives in (Physics) Field Theory,Functional derivatives in (Physics) Field Theory,,"The functional or variational derivative as defined in several places like Wikipedia seems to be defined as a functional, $L$ that takes a single input function, say $f(x)$ and then we define a certain object $$\frac{\delta L}{\delta f(x)}$$ that is a functional derivative. However, in Classical or Quantum Field Theory we seem to define functionals which depend on more than one function, say $$\mathcal{L} (\dot{\phi}(x), \vec{\nabla}\phi(x), \phi(x))$$ or $$\mathcal{H} (p(x), \phi(x)),$$ and then we seem to do things like $$\frac{\delta L}{\delta \dot{\phi}(x)}.$$ How do I interpret these things?","The functional or variational derivative as defined in several places like Wikipedia seems to be defined as a functional, $L$ that takes a single input function, say $f(x)$ and then we define a certain object $$\frac{\delta L}{\delta f(x)}$$ that is a functional derivative. However, in Classical or Quantum Field Theory we seem to define functionals which depend on more than one function, say $$\mathcal{L} (\dot{\phi}(x), \vec{\nabla}\phi(x), \phi(x))$$ or $$\mathcal{H} (p(x), \phi(x)),$$ and then we seem to do things like $$\frac{\delta L}{\delta \dot{\phi}(x)}.$$ How do I interpret these things?",,"['derivatives', 'calculus-of-variations', 'classical-mechanics', 'functional-calculus']"
4,How can the derivative of $f:M\to \mathbb R$ where $M$ is a differentiable manifold can be well defined since it's not unique?,How can the derivative of  where  is a differentiable manifold can be well defined since it's not unique?,f:M\to \mathbb R M,"I have a doubt about the fact that a derivative of $f:M\to \mathbb R$ of a $\mathcal C^1$ manifold is well defined... Indeed, let $a\in M$ and $(U,\varphi )$ a chart from a $\mathbb C^1$ atlas s.t. $a\in A$ . Then, $$f'(a)=\left.\frac{d}{dx}\right|_{x=\varphi (a)}f(\varphi ^{-1}(x)).$$ But if $(\psi,V)$ is an other chart s.t. $a\in V$ , then, $$f'(a)=\left.\frac{d}{dx}\right|_{x=\psi(a)}f(\psi^{-1}(x)).$$ The thing is that I don't see any reason to have $$\left.\frac{d}{dx}\right|_{x=\varphi (a)}f(\varphi ^{-1}(x))=\left.\frac{d}{dx}\right|_{x=\psi(a)}f(\psi^{-1}(x)).$$","I have a doubt about the fact that a derivative of of a manifold is well defined... Indeed, let and a chart from a atlas s.t. . Then, But if is an other chart s.t. , then, The thing is that I don't see any reason to have","f:M\to \mathbb R \mathcal C^1 a\in M (U,\varphi ) \mathbb C^1 a\in A f'(a)=\left.\frac{d}{dx}\right|_{x=\varphi (a)}f(\varphi ^{-1}(x)). (\psi,V) a\in V f'(a)=\left.\frac{d}{dx}\right|_{x=\psi(a)}f(\psi^{-1}(x)). \left.\frac{d}{dx}\right|_{x=\varphi (a)}f(\varphi ^{-1}(x))=\left.\frac{d}{dx}\right|_{x=\psi(a)}f(\psi^{-1}(x)).","['derivatives', 'manifolds']"
5,Visualizing derivatives w.r.t another function,Visualizing derivatives w.r.t another function,,"I came across some derivatives w.r.t functions such as, $\frac{d(7^x)}{d(x^7)}$ , I tried plotting their graphs and seeing how we can relate the change in the value of $7^x$ as the function $x^7$ is changing . Can someone please provide visualization for this specific example or just a general intuitive explanation? Thank you!","I came across some derivatives w.r.t functions such as, , I tried plotting their graphs and seeing how we can relate the change in the value of as the function is changing . Can someone please provide visualization for this specific example or just a general intuitive explanation? Thank you!",\frac{d(7^x)}{d(x^7)} 7^x x^7,"['derivatives', 'related-rates']"
6,prove $-\frac{c' x}{c(x)}$ is decreasing. falling elasticity.,prove  is decreasing. falling elasticity.,-\frac{c' x}{c(x)},"Something was asserted without proof in an economics paper but I would like to see it proven. Suppose $$ c(x) = p + v\cdot t(x), $$ where $t>0$ , $t'<0$ , $t''>0$ , $p>0$ , $v>0$ and $c\rightarrow \infty$ as $x\rightarrow 0$ . It's claimed that $$ -\frac{c'(x)x}{c(x)} $$ is positive but decreasing---i.e., that the elasticity of $c(x)$ is falling in magnitude. Obviously it is positive (since $c'<0$ ) but I don't know how to prove it's decreasing. I guess that means $$ \frac{d}{dx} \frac{c'\cdot x}{c} = \frac{1}{c}\left[ c'' x + c' - \frac{(c')^2x}{c} \right]= \frac{1}{p+v t(x)}\left[ vt''(x) x + vt'(x) - \frac{(vt'(x))^2x}{p+vt(x)} \right] >0. $$ I am unable to prove this to be so.","Something was asserted without proof in an economics paper but I would like to see it proven. Suppose where , , , , and as . It's claimed that is positive but decreasing---i.e., that the elasticity of is falling in magnitude. Obviously it is positive (since ) but I don't know how to prove it's decreasing. I guess that means I am unable to prove this to be so.","
c(x) = p + v\cdot t(x),
 t>0 t'<0 t''>0 p>0 v>0 c\rightarrow \infty x\rightarrow 0 
-\frac{c'(x)x}{c(x)}
 c(x) c'<0 
\frac{d}{dx} \frac{c'\cdot x}{c} = \frac{1}{c}\left[
c'' x + c' - \frac{(c')^2x}{c}
\right]=
\frac{1}{p+v t(x)}\left[
vt''(x) x + vt'(x) - \frac{(vt'(x))^2x}{p+vt(x)}
\right]
>0.
","['derivatives', 'economics']"
7,Discrete Laplacian of Gaussian (LoG),Discrete Laplacian of Gaussian (LoG),,"In Image processing, one often uses the discrete Laplacian of Gaussian (LoG) to do edge detection with $LoG(x,y) = -\frac{1}{\pi \sigma^4}[1-\frac{x^2+y^2}{2\sigma^2}]\cdot e^{-\frac{x^2+y^2}{2\sigma^2}}$ Various sources here , here or here give discrete Kernels of the LoG to be convoluted with the input image to yield the filtered version. However, I do not understand the derivation of this Kernel from the function. One possible Kernel for $\sigma = 1.4$ is $K = \begin{bmatrix}0.0& 1.0& 1.0& 2.0&   2.0&   2.0&   1.0& 1.0& 0.0\\            1.0& 2.0& 4.0& 5.0&   5.0&   5.0&   4.0& 2.0& 1.0\\            1.0& 4.0& 5.0& 3.0&   0.0&   3.0&   5.0& 4.0& 1.0\\            2.0& 5.0& 3.0& -12.0& -24.0& -12.0& 3.0& 5.0& 2.0\\            2.0& 5.0& 0.0& -24.0& -40.0& -24.0& 0.0& 5.0& 2.0\\            2.0& 5.0& 3.0& -12.0& -24.0& -12.0& 3.0& 5.0& 2.0\\            1.0& 4.0& 5.0& 3.0&   0.0&   3.0&   5.0& 4.0& 1.0\\            1.0& 2.0& 4.0& 5.0&   5.0&   5.0&   4.0& 2.0& 1.0\\            0.0& 1.0& 1.0& 2.0&   2.0&   2.0&   1.0& 1.0& 0.0\end{bmatrix}$ My question is, how these discrete Kernels are derived from the $LoG(x,y)$ function. Is there any scaling factor involved? Is it an integration over the discrete pixel area? My calculated values for the central point (-40.0) are: $LoG(0,0) \approx -0.1624$ and $\iint\limits_{-0.5}^{0.5}{LoG(x,y)} dx dy \approx -0.0761$","In Image processing, one often uses the discrete Laplacian of Gaussian (LoG) to do edge detection with $LoG(x,y) = -\frac{1}{\pi \sigma^4}[1-\frac{x^2+y^2}{2\sigma^2}]\cdot e^{-\frac{x^2+y^2}{2\sigma^2}}$ Various sources here , here or here give discrete Kernels of the LoG to be convoluted with the input image to yield the filtered version. However, I do not understand the derivation of this Kernel from the function. One possible Kernel for $\sigma = 1.4$ is $K = \begin{bmatrix}0.0& 1.0& 1.0& 2.0&   2.0&   2.0&   1.0& 1.0& 0.0\\            1.0& 2.0& 4.0& 5.0&   5.0&   5.0&   4.0& 2.0& 1.0\\            1.0& 4.0& 5.0& 3.0&   0.0&   3.0&   5.0& 4.0& 1.0\\            2.0& 5.0& 3.0& -12.0& -24.0& -12.0& 3.0& 5.0& 2.0\\            2.0& 5.0& 0.0& -24.0& -40.0& -24.0& 0.0& 5.0& 2.0\\            2.0& 5.0& 3.0& -12.0& -24.0& -12.0& 3.0& 5.0& 2.0\\            1.0& 4.0& 5.0& 3.0&   0.0&   3.0&   5.0& 4.0& 1.0\\            1.0& 2.0& 4.0& 5.0&   5.0&   5.0&   4.0& 2.0& 1.0\\            0.0& 1.0& 1.0& 2.0&   2.0&   2.0&   1.0& 1.0& 0.0\end{bmatrix}$ My question is, how these discrete Kernels are derived from the $LoG(x,y)$ function. Is there any scaling factor involved? Is it an integration over the discrete pixel area? My calculated values for the central point (-40.0) are: $LoG(0,0) \approx -0.1624$ and $\iint\limits_{-0.5}^{0.5}{LoG(x,y)} dx dy \approx -0.0761$",,"['derivatives', 'convolution', 'laplacian', 'image-processing']"
8,'What is $\partial [x^\top x]/\partial x$' debate,'What is ' debate,\partial [x^\top x]/\partial x,"For $x \in \mathbb R^n$ , is $\frac{\partial[x^Tx]}{\partial[x]}$ equal to  $2x$ or $2x^T$? There are a lot of discrepancies about this, such as Vector derivation of $x^Tx$ . Another example of the discrepancy is the following, Computing matrix-vector calculus derivatives , where Jonas claimed that Par's answer's 4th line is incorrect. As a result, Par's answer was 'a' and Jonas' answer was 'a' transpose. Which answerer to the question of ""what's the derivative of x^T a with respect to x"" was correct?","For $x \in \mathbb R^n$ , is $\frac{\partial[x^Tx]}{\partial[x]}$ equal to  $2x$ or $2x^T$? There are a lot of discrepancies about this, such as Vector derivation of $x^Tx$ . Another example of the discrepancy is the following, Computing matrix-vector calculus derivatives , where Jonas claimed that Par's answer's 4th line is incorrect. As a result, Par's answer was 'a' and Jonas' answer was 'a' transpose. Which answerer to the question of ""what's the derivative of x^T a with respect to x"" was correct?",,['derivatives']
9,What is the definition of a cusp?,What is the definition of a cusp?,,"So there are 3 situations that a function is not differentiable 1. a vertical tangent, 2. a discontinuity and 3. at a cusp. Consider the function $f(x)=e^{-|x|}$ at the point $x=0$. It is continuous here but its first derivative is not does, and therefore it cannot be differential here can it? But I have read that a cusp is when the limit of the first derivative must tend to $+\infty$ when approaching the point from one direction and $-\infty$ when from the other. In this case it tends to $+1$ and $-1$ which should mean that it does not have a cusp here and does not fall into one of the non differentiable categories. But it is not differentiable, so is this definition of a cusp wrong, if so what is the actual definition? thanks.","So there are 3 situations that a function is not differentiable 1. a vertical tangent, 2. a discontinuity and 3. at a cusp. Consider the function $f(x)=e^{-|x|}$ at the point $x=0$. It is continuous here but its first derivative is not does, and therefore it cannot be differential here can it? But I have read that a cusp is when the limit of the first derivative must tend to $+\infty$ when approaching the point from one direction and $-\infty$ when from the other. In this case it tends to $+1$ and $-1$ which should mean that it does not have a cusp here and does not fall into one of the non differentiable categories. But it is not differentiable, so is this definition of a cusp wrong, if so what is the actual definition? thanks.",,['derivatives']
10,Maclaurin Series for $\arctan(x)$ by successive differentiation,Maclaurin Series for  by successive differentiation,\arctan(x),"I am trying to find a Maclaurin Series for $\arctan(x)$ up to the term with the fifth power of x and I have to use the method of successive differentiation. I know (from an example in my notes) the series found by integration of the $\dfrac{1}{1+x^2}$ series is $$x-\frac{x^3}{3}+\frac{x^5}{5}-\frac{x^7}{7}+\frac{x^9}{9}$$ When I try with successive differentiation, I cannot get this same series. Are both series correct, or is it a methodical error on my part? I checked the derivatives using WolframAlpha, so I can't see it being computational. BY SUCCESSIVE DIFFERENTIATION: To find coefficients of the power series, evaluate the function and it's successive derivatives at $x=0$ : $\arctan(0) =0$ $\arctan^{(1)}(x) = (1+x^2)^{-1}$ so $\arctan^{(1)}(0)= 1$ $\arctan^{(2)}(x) = -2x(1+x^2)^{-2}$ so $\arctan^{(2)}(0)= 0$ $\arctan^{(3)}(x) = (6x^2-2)(1+x^2)^{-3}$ so $\arctan^{(3)}(0) = -2$ $\arctan^{(4)}(x) = -24x(x^2-1)((1+x^2)^{-4}$ so $\arctan^{(4)}(0) = 0$ $\arctan^{(5)}(x) = 24(5x^4-10x^2+1)(1+x^2)^{-5}$ so $\arctan^{(5)}(0) = 24$ Which leads to the series $$x-2x^3+24x^5-\cdots$$ which captures the alternation of the series but the coefficients are all off. If this is correct and there are indeed the two series, an explanation of this oddity would be much appreciated, though I don't see how this can be the case.","I am trying to find a Maclaurin Series for up to the term with the fifth power of x and I have to use the method of successive differentiation. I know (from an example in my notes) the series found by integration of the series is When I try with successive differentiation, I cannot get this same series. Are both series correct, or is it a methodical error on my part? I checked the derivatives using WolframAlpha, so I can't see it being computational. BY SUCCESSIVE DIFFERENTIATION: To find coefficients of the power series, evaluate the function and it's successive derivatives at : so so so so so Which leads to the series which captures the alternation of the series but the coefficients are all off. If this is correct and there are indeed the two series, an explanation of this oddity would be much appreciated, though I don't see how this can be the case.",\arctan(x) \dfrac{1}{1+x^2} x-\frac{x^3}{3}+\frac{x^5}{5}-\frac{x^7}{7}+\frac{x^9}{9} x=0 \arctan(0) =0 \arctan^{(1)}(x) = (1+x^2)^{-1} \arctan^{(1)}(0)= 1 \arctan^{(2)}(x) = -2x(1+x^2)^{-2} \arctan^{(2)}(0)= 0 \arctan^{(3)}(x) = (6x^2-2)(1+x^2)^{-3} \arctan^{(3)}(0) = -2 \arctan^{(4)}(x) = -24x(x^2-1)((1+x^2)^{-4} \arctan^{(4)}(0) = 0 \arctan^{(5)}(x) = 24(5x^4-10x^2+1)(1+x^2)^{-5} \arctan^{(5)}(0) = 24 x-2x^3+24x^5-\cdots,"['derivatives', 'power-series']"
11,How to prove monotonicity in this case?,How to prove monotonicity in this case?,,"Let $0<a \le 1, \alpha<0$ and $\beta>0$ . How to prove that the function: $$f(x)=\frac{(\Gamma(a)-\Gamma(a,\alpha \ln(\beta x))) (\alpha\ln(x))^a}{(\alpha\ln(\beta x))^a (\Gamma(a)-\Gamma(a,\alpha \ln(x)))},$$ is decreasing for $\beta <1$ and increasing for $\beta>1$ . This question is motivated by the following inequality after drawing the graph for some values with wolfram. I tried the sign of derivative but it is more delicate.",Let and . How to prove that the function: is decreasing for and increasing for . This question is motivated by the following inequality after drawing the graph for some values with wolfram. I tried the sign of derivative but it is more delicate.,"0<a \le 1, \alpha<0 \beta>0 f(x)=\frac{(\Gamma(a)-\Gamma(a,\alpha \ln(\beta x))) (\alpha\ln(x))^a}{(\alpha\ln(\beta x))^a (\Gamma(a)-\Gamma(a,\alpha \ln(x)))}, \beta <1 \beta>1","['derivatives', 'monotone-functions']"
12,What is the difference between exact and partial differentiation?,What is the difference between exact and partial differentiation?,,"My understanding of partial $\left( \frac{\partial}{\partial} \right)$ and total  $\left( \frac{d}{d} \right)$ differentiation/derivative is that assuming $f(x_1, x_2, ...,x_n )$ where $x_i$s are not necessarily independent: $$\frac{d f}{ dx_i}=\sum^n_1 \left(\frac{\partial f}{\partial x_j}\frac{d x_j}{dx_i} \right)$$ Where $\frac{\partial f}{\partial x_i}$ is the symbolic derivative of the equation $f(x_1, x_2, ...,x_n )$ assuming all $x_j$s except $x_i$ are constants. Of course when $x_i$s are independent: $$\frac{\partial f}{\partial x_i}=\frac{d f}{ dx_i}$$ But in thermodynamics I see that they have this exact differential $$\left(\frac{\partial f}{\partial x_i} \right)_{x_j}$$ which to me looks exactly the same as partial differential. For example see these videos of thermodynamic lectures from MIT . I find this concept/notation redundant and confusing. I would appreciate if you could explain the difference between partial and exact differentials and give me a tangible example when they are not the same. P.S.1. This post also approves my point: In fact, the constancy of the other variables is implicit in the partial differential notation (∂/∂x) but it is customary to write the variables that are constant under the derivative when discussing thermodynamics, just to keep track of what other variables we were considering in that particular case. Which if true, is an awful idea. Partial differential equations are already long and confusing enough without these redundant notations. Why on earth should we make it even more difficult?","My understanding of partial $\left( \frac{\partial}{\partial} \right)$ and total  $\left( \frac{d}{d} \right)$ differentiation/derivative is that assuming $f(x_1, x_2, ...,x_n )$ where $x_i$s are not necessarily independent: $$\frac{d f}{ dx_i}=\sum^n_1 \left(\frac{\partial f}{\partial x_j}\frac{d x_j}{dx_i} \right)$$ Where $\frac{\partial f}{\partial x_i}$ is the symbolic derivative of the equation $f(x_1, x_2, ...,x_n )$ assuming all $x_j$s except $x_i$ are constants. Of course when $x_i$s are independent: $$\frac{\partial f}{\partial x_i}=\frac{d f}{ dx_i}$$ But in thermodynamics I see that they have this exact differential $$\left(\frac{\partial f}{\partial x_i} \right)_{x_j}$$ which to me looks exactly the same as partial differential. For example see these videos of thermodynamic lectures from MIT . I find this concept/notation redundant and confusing. I would appreciate if you could explain the difference between partial and exact differentials and give me a tangible example when they are not the same. P.S.1. This post also approves my point: In fact, the constancy of the other variables is implicit in the partial differential notation (∂/∂x) but it is customary to write the variables that are constant under the derivative when discussing thermodynamics, just to keep track of what other variables we were considering in that particular case. Which if true, is an awful idea. Partial differential equations are already long and confusing enough without these redundant notations. Why on earth should we make it even more difficult?",,['derivatives']
13,"How to solve $A_xB_y+A_yB_x=0$, $A,B \in \mathbb{C}[x,y]$?","How to solve , ?","A_xB_y+A_yB_x=0 A,B \in \mathbb{C}[x,y]","Let $A,B \in \mathbb{C}[x,y]$ be arbitrary polynomials of two variables with coefficients in $\mathbb{C}$ . Assume that $A_xB_y+A_yB_x=0$ , where $\cdot _x$ is the partial derivative with respect to $x$ and $\cdot_y$ is the partial derivative with respect to $y$ . Question 1: Is it possible to find a general solution to such equation? I really apologize if this is an easy question; I am not familiar with the theory of solving such equations. An example: $A=x+y, B=x-y$ . Here $A$ is symmetric and $B$ is skew-symmetric w.r.t. the involution $(x,y) \mapsto (x,-y)$ , but general $A$ symmetric and $B$ skew-symmetric do not work, for example $A=x^2+y^2, B=x-y$ . Question 2: Replace $A_xB_y+A_yB_x=0$ by $A_xB_y-A_yB_x=0$ . What is the answer in this case? Question 3: Let $h \in \mathbb{C}[x]$ , $h$ is any polynomial in one variable $x$ . What is the solotion to $(h-A_x)B_y+A_yB_x=0$ ? to $(h-A_x)B_y-A_yB_x=0$ ? Edit: If an answer for a general $h$ is too complicated, I do not mind to assume that $h=1$ , so Question 3 becomes: What is the solotion to $(1-A_x)B_y+A_yB_x=0$ ? to $(1-A_x)B_y-A_yB_x=0$ ? Now let us consider $A_xB_y-A_yB_x=-B_x$ , namely, $A_xB_y=(A_y-1)B_x$ (in qeustion 3, I have asked about $A_xB_y-A_yB_x=B_y$ , but these are analog cases). A possible solution is: $A=(x+y)^3+y, B=(x+y)^2$ . More generally, if I am not wrong, $A=f(C)+y, B=g(C)$ , where $C \in \mathbb{C}[x,y]$ , $f,g \in \mathbb{C}[T]$ . Question 4: Is it true that $A=f(C)+y, B=g(C)$ is a general solution to $A_xB_y=(A_y-1)B_x$ ? If not, is it possible to describe all additional solutions? See also this question. Thank you very much!","Let be arbitrary polynomials of two variables with coefficients in . Assume that , where is the partial derivative with respect to and is the partial derivative with respect to . Question 1: Is it possible to find a general solution to such equation? I really apologize if this is an easy question; I am not familiar with the theory of solving such equations. An example: . Here is symmetric and is skew-symmetric w.r.t. the involution , but general symmetric and skew-symmetric do not work, for example . Question 2: Replace by . What is the answer in this case? Question 3: Let , is any polynomial in one variable . What is the solotion to ? to ? Edit: If an answer for a general is too complicated, I do not mind to assume that , so Question 3 becomes: What is the solotion to ? to ? Now let us consider , namely, (in qeustion 3, I have asked about , but these are analog cases). A possible solution is: . More generally, if I am not wrong, , where , . Question 4: Is it true that is a general solution to ? If not, is it possible to describe all additional solutions? See also this question. Thank you very much!","A,B \in \mathbb{C}[x,y] \mathbb{C} A_xB_y+A_yB_x=0 \cdot _x x \cdot_y y A=x+y, B=x-y A B (x,y) \mapsto (x,-y) A B A=x^2+y^2, B=x-y A_xB_y+A_yB_x=0 A_xB_y-A_yB_x=0 h \in \mathbb{C}[x] h x (h-A_x)B_y+A_yB_x=0 (h-A_x)B_y-A_yB_x=0 h h=1 (1-A_x)B_y+A_yB_x=0 (1-A_x)B_y-A_yB_x=0 A_xB_y-A_yB_x=-B_x A_xB_y=(A_y-1)B_x A_xB_y-A_yB_x=B_y A=(x+y)^3+y, B=(x+y)^2 A=f(C)+y, B=g(C) C \in \mathbb{C}[x,y] f,g \in \mathbb{C}[T] A=f(C)+y, B=g(C) A_xB_y=(A_y-1)B_x","['derivatives', 'polynomials', 'commutative-algebra', 'partial-derivative', 'jacobian']"
14,Derivative of eigenvectors of a symmetric matrix-valued function,Derivative of eigenvectors of a symmetric matrix-valued function,,"Given a real symmetric $3\times3$ matrix $\mathsf{A}_{ij}$ and its derivative (w.r.t. some parameter, let's call it time ) $\dot{\mathsf{A}}_{ij}$, I want to measure/obtain the rotation (rate and direction) of the eigenvectors (the eigenvectors of a real symmetric matrix form an orthonormal matrix). How can this be done? Edit Since the eigenvectors of a real symmetric matrix are mutually orthogonal, the change of the eigenvectors can only be an overall rotation. An infinitesimal rotation is uniquely determined by the rate $\boldsymbol{\omega}$ such that $\dot{\boldsymbol{x}}=\boldsymbol{\omega}\times\boldsymbol{x}$ for any vector $\boldsymbol{x}$. My question then becomes how to obtain $\boldsymbol{\omega}$.","Given a real symmetric $3\times3$ matrix $\mathsf{A}_{ij}$ and its derivative (w.r.t. some parameter, let's call it time ) $\dot{\mathsf{A}}_{ij}$, I want to measure/obtain the rotation (rate and direction) of the eigenvectors (the eigenvectors of a real symmetric matrix form an orthonormal matrix). How can this be done? Edit Since the eigenvectors of a real symmetric matrix are mutually orthogonal, the change of the eigenvectors can only be an overall rotation. An infinitesimal rotation is uniquely determined by the rate $\boldsymbol{\omega}$ such that $\dot{\boldsymbol{x}}=\boldsymbol{\omega}\times\boldsymbol{x}$ for any vector $\boldsymbol{x}$. My question then becomes how to obtain $\boldsymbol{\omega}$.",,"['derivatives', 'eigenvalues-eigenvectors', 'matrix-calculus', 'symmetric-matrices']"
15,Criteria for smoothness of the pointwise limit of a sequence of functions,Criteria for smoothness of the pointwise limit of a sequence of functions,,"Let $\#$ denote cardinality, and fix $p\in[1,\infty]$. Let $(f_n)_{n\in\mathbb{N}}$ be a sequence of functions from $[0,1]$ to $\mathbb{R}$ with the properties $f_n\in C^{n-1}([0,1])$ $f^{(n)}_n$ is continuous on $[0,1]\setminus A_n$ where $\#A_n<\infty$ but $ \underset{n\to\infty}{\lim}\#A_n=\infty$ $\underset{n\to\infty}{\lim}||f_n^{(m)}||_p$ exists and is finite $\forall m\in\mathbb{N}_0$ $f_n$ converge pointwise to a function $f$ For what values of $p$ do these imply $f\in C^\infty([0,1])$? For those $p$ where a counterexample exists, how to construct such an example and/or what would be a convenient additional condition to avoid its existence? Attempt with $p=\infty$: Assume that $f^{(0)}$ is not continuous. Then there exists $x\in(0,1)$ and $\varepsilon>0$ such that however small $\delta>0$ is, there exists $x_0$ satisfying $0<|x-x_0|<\delta$ such that $|f(x_0)-f(x)|>\varepsilon$. Moreover $0<|(x_0(\delta)-(x+\delta))/2|<\delta$, and if $p=\infty$ then 3) implies $\infty>\underset{n\to\infty}{\lim}|f_n^{(1)}(x)|$. A contradiction follows: $$\infty>\underset{n\to\infty}{\lim}|3f_n^{(1)}(x)| =\underset{n\to\infty}{\lim}\left(\underset{\delta\to 0^+}{\lim}\left|\frac{f_n(x+\delta)-f_n(x)}{\delta}\right|+2\underset{\delta\to 0^+}{\lim}\left|\frac{f_n(x_0(\delta))-f_n(x+\delta)}{x_0(\delta)-(x+\delta)}\right|\right) \hspace{6.4cm}\text{ }\\\hspace{4cm} >\underset{n\to\infty}{\lim}\left(\underset{\delta\to 0^+}{\lim}\left|\frac{f_n(x+\delta)-f_n(x)}{\delta}\right|+\underset{\delta\to 0^+}{\limsup}\left|\frac{f_n(x_0(\delta))-f_n(x+\delta)}{\delta}\right|\right) \\\hspace{2.15cm} >\underset{n\to\infty}{\lim}\underset{\delta\to 0^+}{\limsup}\left|\frac{f_n(x+\delta)-f_n(x)}{\delta}+\frac{f_n(x_0(\delta))-f_n(x+\delta)}{\delta}\right| \\\hspace{2cm} =\underset{n\to\infty}{\lim}\underset{\delta\to 0^+}{\limsup}\left|\frac{f_n(x_0(\delta))-f_n(x)}{\delta}\right| \hspace{4.6cm}\text{ }\\ \overset{\text{???}}{=}\underset{\delta\to 0^+}{\limsup}\underset{n\to\infty}{\lim}\left|\frac{f_n(x_0(\delta))-f_n(x)}{\delta}\right| \hspace{2.6cm}\text{ }\\ =\underset{\delta\to 0^+}{\limsup}\frac{|f(x_0(\delta))-f(x)|}{\delta} >\underset{\delta\to 0^+}{\limsup}\frac{\varepsilon}{\delta}=\infty \hspace{0.5cm}\text{ }$$ so $f^{(0)}$ is continuous and replacing $f$ and $f_n$ by their derivative smoothness follows by induction. Unless the part with those ""???"" is wrong?!","Let $\#$ denote cardinality, and fix $p\in[1,\infty]$. Let $(f_n)_{n\in\mathbb{N}}$ be a sequence of functions from $[0,1]$ to $\mathbb{R}$ with the properties $f_n\in C^{n-1}([0,1])$ $f^{(n)}_n$ is continuous on $[0,1]\setminus A_n$ where $\#A_n<\infty$ but $ \underset{n\to\infty}{\lim}\#A_n=\infty$ $\underset{n\to\infty}{\lim}||f_n^{(m)}||_p$ exists and is finite $\forall m\in\mathbb{N}_0$ $f_n$ converge pointwise to a function $f$ For what values of $p$ do these imply $f\in C^\infty([0,1])$? For those $p$ where a counterexample exists, how to construct such an example and/or what would be a convenient additional condition to avoid its existence? Attempt with $p=\infty$: Assume that $f^{(0)}$ is not continuous. Then there exists $x\in(0,1)$ and $\varepsilon>0$ such that however small $\delta>0$ is, there exists $x_0$ satisfying $0<|x-x_0|<\delta$ such that $|f(x_0)-f(x)|>\varepsilon$. Moreover $0<|(x_0(\delta)-(x+\delta))/2|<\delta$, and if $p=\infty$ then 3) implies $\infty>\underset{n\to\infty}{\lim}|f_n^{(1)}(x)|$. A contradiction follows: $$\infty>\underset{n\to\infty}{\lim}|3f_n^{(1)}(x)| =\underset{n\to\infty}{\lim}\left(\underset{\delta\to 0^+}{\lim}\left|\frac{f_n(x+\delta)-f_n(x)}{\delta}\right|+2\underset{\delta\to 0^+}{\lim}\left|\frac{f_n(x_0(\delta))-f_n(x+\delta)}{x_0(\delta)-(x+\delta)}\right|\right) \hspace{6.4cm}\text{ }\\\hspace{4cm} >\underset{n\to\infty}{\lim}\left(\underset{\delta\to 0^+}{\lim}\left|\frac{f_n(x+\delta)-f_n(x)}{\delta}\right|+\underset{\delta\to 0^+}{\limsup}\left|\frac{f_n(x_0(\delta))-f_n(x+\delta)}{\delta}\right|\right) \\\hspace{2.15cm} >\underset{n\to\infty}{\lim}\underset{\delta\to 0^+}{\limsup}\left|\frac{f_n(x+\delta)-f_n(x)}{\delta}+\frac{f_n(x_0(\delta))-f_n(x+\delta)}{\delta}\right| \\\hspace{2cm} =\underset{n\to\infty}{\lim}\underset{\delta\to 0^+}{\limsup}\left|\frac{f_n(x_0(\delta))-f_n(x)}{\delta}\right| \hspace{4.6cm}\text{ }\\ \overset{\text{???}}{=}\underset{\delta\to 0^+}{\limsup}\underset{n\to\infty}{\lim}\left|\frac{f_n(x_0(\delta))-f_n(x)}{\delta}\right| \hspace{2.6cm}\text{ }\\ =\underset{\delta\to 0^+}{\limsup}\frac{|f(x_0(\delta))-f(x)|}{\delta} >\underset{\delta\to 0^+}{\limsup}\frac{\varepsilon}{\delta}=\infty \hspace{0.5cm}\text{ }$$ so $f^{(0)}$ is continuous and replacing $f$ and $f_n$ by their derivative smoothness follows by induction. Unless the part with those ""???"" is wrong?!",,"['derivatives', 'uniform-convergence', 'pointwise-convergence']"
16,Derivative of a characteristic polynomial at an eigenvalue,Derivative of a characteristic polynomial at an eigenvalue,,"Let $p(\lambda)$ be the characteristic polynomial of an $n\times n$ matrix $A$. We know that the roots of $p(\lambda)$ are the eigenvalues of $A$, hence the sum of the roots  of the polynomial (taking into account multiplicity) equals $\mathrm{tr}(A)$ and the product of the roots equals $|A|\equiv\mathrm{det}(A)$. Since $p(\lambda)=\prod_{i=1}^n(\lambda-\lambda_i)$, we have $p'(\lambda_1)=\prod_{i=2}^n(\lambda_1-\lambda_i)$ (arbitrary numbering of eigenvalues). Is there anyway that we can connect this value, i.e. the derivative of the characteristic polynomial at a root/eigenvalue, to other special quantities connected with $A$, like determinants and trace? I am sorry if the question is a little vague. Many thanks to all the responders in advance!","Let $p(\lambda)$ be the characteristic polynomial of an $n\times n$ matrix $A$. We know that the roots of $p(\lambda)$ are the eigenvalues of $A$, hence the sum of the roots  of the polynomial (taking into account multiplicity) equals $\mathrm{tr}(A)$ and the product of the roots equals $|A|\equiv\mathrm{det}(A)$. Since $p(\lambda)=\prod_{i=1}^n(\lambda-\lambda_i)$, we have $p'(\lambda_1)=\prod_{i=2}^n(\lambda_1-\lambda_i)$ (arbitrary numbering of eigenvalues). Is there anyway that we can connect this value, i.e. the derivative of the characteristic polynomial at a root/eigenvalue, to other special quantities connected with $A$, like determinants and trace? I am sorry if the question is a little vague. Many thanks to all the responders in advance!",,"['derivatives', 'polynomials', 'eigenvalues-eigenvectors']"
17,Link between two gradient definitions,Link between two gradient definitions,,"Let $M$ a $n$ dimensional embedded Riemannian manifold of $\mathbb{R}^{n+1}$ . There are two definitions of $\nabla_M f$ that seem different, but I think they should be related: Def 1: $\nabla_M f = \nabla f - \langle\nabla f, n\rangle n$ , where $n$ is the outer unit normal. Def 2: Definition through parametrization. By Def 1, one sees that $\nabla_M f \in \mathbb{R}^{n+1}$ , while using Def 2 (in practice), we get $\nabla_M f \in \mathbb{R}^{n}$ . How to relate this two definitions? (Maybe, behind my confusion there is some hidden identification). Edit 1: Take, e.g., $f(x,y,z)=\frac{1}{2}x^2$ on $\mathbb{R}^3$ . Then by Def 1: $\nabla_{S^2} f(x,y,z)=(x-x^3, -x^2y,-x^2z)$ and by Def 2: $\nabla_{S^2} f(\phi(\theta,\varphi))=(-\frac{1}{2} \sin(2\theta),\frac{1}{2} \sin(2\varphi) \cos^2 \theta)$ . But in Riemannian geometry, they said that they are the same, they even prove one from the other.","Let a dimensional embedded Riemannian manifold of . There are two definitions of that seem different, but I think they should be related: Def 1: , where is the outer unit normal. Def 2: Definition through parametrization. By Def 1, one sees that , while using Def 2 (in practice), we get . How to relate this two definitions? (Maybe, behind my confusion there is some hidden identification). Edit 1: Take, e.g., on . Then by Def 1: and by Def 2: . But in Riemannian geometry, they said that they are the same, they even prove one from the other.","M n \mathbb{R}^{n+1} \nabla_M f \nabla_M f = \nabla f - \langle\nabla f, n\rangle n n \nabla_M f \in \mathbb{R}^{n+1} \nabla_M f \in \mathbb{R}^{n} f(x,y,z)=\frac{1}{2}x^2 \mathbb{R}^3 \nabla_{S^2} f(x,y,z)=(x-x^3, -x^2y,-x^2z) \nabla_{S^2} f(\phi(\theta,\varphi))=(-\frac{1}{2} \sin(2\theta),\frac{1}{2} \sin(2\varphi) \cos^2 \theta)","['derivatives', 'differential-geometry', 'riemannian-geometry']"
18,Using chain rule to differentiate $f(x)=a(x)b(x)$?,Using chain rule to differentiate ?,f(x)=a(x)b(x),Why can I not apply the chain rule to a product in the following way. If we have some product: $$f(x)=a(x)b(x)$$ Consider the multiplication of b by a as another’s function so that: $$f(b(x))=ab$$ So that $\frac{df}{dx} = f’(b)b’(x)$ Something feels very wrong. But I can’t put my finger on it.,Why can I not apply the chain rule to a product in the following way. If we have some product: Consider the multiplication of b by a as another’s function so that: So that Something feels very wrong. But I can’t put my finger on it.,f(x)=a(x)b(x) f(b(x))=ab \frac{df}{dx} = f’(b)b’(x),"['derivatives', 'chain-rule']"
19,How do I calculate derivative of sgn(x),How do I calculate derivative of sgn(x),,"We know $|x| = \sqrt(x^2)$, determine the second derivative $\frac{d^2}{dx^2}|x|$, So the first derivative is sgn(x), but how do I get the second?","We know $|x| = \sqrt(x^2)$, determine the second derivative $\frac{d^2}{dx^2}|x|$, So the first derivative is sgn(x), but how do I get the second?",,['derivatives']
20,What is the difference between exponential symbol $a^x$ and $e^x$ in mathematics symbols?,What is the difference between exponential symbol  and  in mathematics symbols?,a^x e^x,I want to know the difference between the exponential symbol $a^x$ and $e^x$ in mathematics symbols and please give me some examples for both of them. I asked this question because of the derivative rules table below contain both exponential symbol $a^x$ and $e^x$ and I don't know when should I use one of them and when should I use the another one. Derivative rules table: [ Derivative rules table source ],I want to know the difference between the exponential symbol $a^x$ and $e^x$ in mathematics symbols and please give me some examples for both of them. I asked this question because of the derivative rules table below contain both exponential symbol $a^x$ and $e^x$ and I don't know when should I use one of them and when should I use the another one. Derivative rules table: [ Derivative rules table source ],,['derivatives']
21,Why isn’t Brownian motion differentiable?,Why isn’t Brownian motion differentiable?,,"Intuitively, if increments become infinitesimally small, why doesn’t Brownian motion become a differentiable function?","Intuitively, if increments become infinitesimally small, why doesn’t Brownian motion become a differentiable function?",,"['derivatives', 'stochastic-processes', 'stochastic-calculus', 'brownian-motion', 'differential-forms']"
22,How can we minimize a function of two variables?,How can we minimize a function of two variables?,,"Here we are more interested in the method to minimize the function, rather than what the actual result is. The function I currently have, which I may need to change, is: $$b / d + (1/6 \log{(b)})(1+\log{(b)})(1+2\log{(b)})m/(gd)$$ with: $$b=g \log_2(\log_2{(g)}+d\log_2{(2n^2a^2)}+\log_2{(nm)})$$ Here $a$, $m$, and $n$ are given and known.  I'm interested in knowing the method to find the $d$ and $g$ which minimize the function.  For example, can I simply take the derivative to find the minimum values? My question is, what method can I use to find the minimum values of a function of this type?  Can it be as simple as taking the derivative, assuming that finding the derivative is easy?","Here we are more interested in the method to minimize the function, rather than what the actual result is. The function I currently have, which I may need to change, is: $$b / d + (1/6 \log{(b)})(1+\log{(b)})(1+2\log{(b)})m/(gd)$$ with: $$b=g \log_2(\log_2{(g)}+d\log_2{(2n^2a^2)}+\log_2{(nm)})$$ Here $a$, $m$, and $n$ are given and known.  I'm interested in knowing the method to find the $d$ and $g$ which minimize the function.  For example, can I simply take the derivative to find the minimum values? My question is, what method can I use to find the minimum values of a function of this type?  Can it be as simple as taking the derivative, assuming that finding the derivative is easy?",,"['derivatives', 'partial-derivative', 'intuition', 'maxima-minima']"
23,Divergence in curvilinear coordinates,Divergence in curvilinear coordinates,,"In a curvilinear system with coordinates: $u_1, u_1, u_3$ . I want to understand why: $$\nabla\cdot \vec{V} \neq  \frac{1}{h_1} \frac{\partial V_1}{\partial u_1} +\frac{1}{h_2} \frac{\partial V_2}{\partial u_2} + \frac{1}{h_3}\frac{\partial V_3}{\partial u_3}. \tag1$$ My reasoning to explain why eq.(1) is not true is the following: Taking the first term of the dot product: $$\frac{\hat{e_1} }{h_1}\frac{\partial}{\partial u_1} (V_1 \hat{e_1}) =  \frac{\hat{e_1}}{h_1} \big( \hat{e_1} \frac{\partial V_1}{\partial u_1} + V_1 \frac{\partial \hat{e_1}}{\partial u_1} \big)$$ This seems rare to me since the definition of the dot product is: $$A \cdot B = A_1B_1 + A_2B_2 + A_3B_3 $$ And using that definition the divergence should be eq.(1) (with an equality). I would appreciate any references that explain this, since all I can find is the formula for the divergence in curvilinear systems but without a deduction.","In a curvilinear system with coordinates: . I want to understand why: My reasoning to explain why eq.(1) is not true is the following: Taking the first term of the dot product: This seems rare to me since the definition of the dot product is: And using that definition the divergence should be eq.(1) (with an equality). I would appreciate any references that explain this, since all I can find is the formula for the divergence in curvilinear systems but without a deduction.","u_1, u_1, u_3 \nabla\cdot \vec{V} \neq  \frac{1}{h_1} \frac{\partial V_1}{\partial u_1} +\frac{1}{h_2} \frac{\partial V_2}{\partial u_2} + \frac{1}{h_3}\frac{\partial V_3}{\partial u_3}. \tag1 \frac{\hat{e_1} }{h_1}\frac{\partial}{\partial u_1} (V_1 \hat{e_1}) =  \frac{\hat{e_1}}{h_1} \big( \hat{e_1} \frac{\partial V_1}{\partial u_1} + V_1 \frac{\partial \hat{e_1}}{\partial u_1} \big) A \cdot B = A_1B_1 + A_2B_2 + A_3B_3 ","['differential-geometry', 'coordinate-systems', 'derivatives', 'vector-fields']"
24,The derivative of $x!$,The derivative of,x!,I was trying to calculate the derivative of $x!$ but i ran into a large amount of numbers. Is it even possible to calculate it? Because in an app called GRAPHER when i type in $(x!)'$ it returnes the graph of this function. Here it is:,I was trying to calculate the derivative of $x!$ but i ran into a large amount of numbers. Is it even possible to calculate it? Because in an app called GRAPHER when i type in $(x!)'$ it returnes the graph of this function. Here it is:,,['derivatives']
25,why minimum of these functions happen at a special place?,why minimum of these functions happen at a special place?,,why minimum of these functions happen at a special place? how to use derivative to find the minimum of these functions? $$|x-1| + |x-2| + \dots + |x-9|$$ minimum is for $x = 5$ $$|x-1| + |x-2| + \dots + |x-99|$$ minimum is for $x = 50$ $$|x-1| + |x-2| + \dots + |x-999|$$ minimum is for $x = 500$ $$ \dots$$ why it happens?,why minimum of these functions happen at a special place? how to use derivative to find the minimum of these functions? $$|x-1| + |x-2| + \dots + |x-9|$$ minimum is for $x = 5$ $$|x-1| + |x-2| + \dots + |x-99|$$ minimum is for $x = 50$ $$|x-1| + |x-2| + \dots + |x-999|$$ minimum is for $x = 500$ $$ \dots$$ why it happens?,,"['derivatives', 'absolute-value']"
26,Equal partial derivatives,Equal partial derivatives,,"Suppose $\frac{\partial}{\partial x}F(x,y,z)=\frac{\partial}{\partial y}F(x,y,z)=\frac{\partial}{\partial z}F(x,y,z)$. Does this imply $F(x,y,z)=G(x+y+z)$ for some function G?","Suppose $\frac{\partial}{\partial x}F(x,y,z)=\frac{\partial}{\partial y}F(x,y,z)=\frac{\partial}{\partial z}F(x,y,z)$. Does this imply $F(x,y,z)=G(x+y+z)$ for some function G?",,"['derivatives', 'partial-derivative']"
27,Differentiability only in isolated point,Differentiability only in isolated point,,"Do functions exist, which are differentiable in a point, but not in a neighborhood of this point? Is $e^{\frac{1}{W(x)-2}}$, where W is the Weierstrass function ,  maybe an example of a such function?","Do functions exist, which are differentiable in a point, but not in a neighborhood of this point? Is $e^{\frac{1}{W(x)-2}}$, where W is the Weierstrass function ,  maybe an example of a such function?",,['derivatives']
28,Compact formula for the $n^\text{th}$ derivative of $\operatorname{sech}^2(x)$?,Compact formula for the  derivative of ?,n^\text{th} \operatorname{sech}^2(x),"In short, is there a clean formula for $\frac{d^n}{dx^n}\operatorname{sech}^2(x)?$ For convenience, let \begin{align} f(x)&=\operatorname{sech}^2(x),\\\\ g(x)&=\tanh(x). \end{align} Then \begin{align} f'(x)&=-2\operatorname{sech}^2(x)\tanh(x)& &=af(x)g(x),\\\\ g'(x)&=\operatorname{sech}^2(x)& &=f(x), \end{align} where $a=-2.$ We also have $g^2(x)=1-f(x).$ Hence $f^{(n)}(x)$ can always be written as a sum where each term is of the form $c_{k,p} [f(x)]^k[g(x)]^p$ where $p=0$ or $1$ . So the question becomes: is there a formula for the $c_{k,p}$ 's? I would expect some pattern to emerge from repeated uses of the power rule combined with the recursion of the above formulas, but I'm not sure how to proceed. Also, I know sometimes coefficients of special polynomials can encode information about $n^\text{th}$ derivatives (e.g., Hermite polynomials and $e^{-x^2}$ ), and I would find it acceptable to express the answer in terms of such coefficients if possible.","In short, is there a clean formula for For convenience, let Then where We also have Hence can always be written as a sum where each term is of the form where or . So the question becomes: is there a formula for the 's? I would expect some pattern to emerge from repeated uses of the power rule combined with the recursion of the above formulas, but I'm not sure how to proceed. Also, I know sometimes coefficients of special polynomials can encode information about derivatives (e.g., Hermite polynomials and ), and I would find it acceptable to express the answer in terms of such coefficients if possible.","\frac{d^n}{dx^n}\operatorname{sech}^2(x)? \begin{align}
f(x)&=\operatorname{sech}^2(x),\\\\
g(x)&=\tanh(x).
\end{align} \begin{align}
f'(x)&=-2\operatorname{sech}^2(x)\tanh(x)& &=af(x)g(x),\\\\
g'(x)&=\operatorname{sech}^2(x)& &=f(x),
\end{align} a=-2. g^2(x)=1-f(x). f^{(n)}(x) c_{k,p} [f(x)]^k[g(x)]^p p=0 1 c_{k,p} n^\text{th} e^{-x^2}","['derivatives', 'recurrence-relations', 'special-functions', 'hyperbolic-functions']"
29,Real life situation for an implicit function,Real life situation for an implicit function,,"What could be an example of a real life situation for which an implicit function may arouse? In real life, while plotting a value against the other, wouldn't it be the case that the function would not be implicitly defined? This relates to the understanding Why is the function always differentiated with respect to x? Books and tonnes of sites just let us plug in the rule. But, I haven't found a single resource that states why is the implicit function differentiated with respect to x? I don't know if it is just a convention.","What could be an example of a real life situation for which an implicit function may arouse? In real life, while plotting a value against the other, wouldn't it be the case that the function would not be implicitly defined? This relates to the understanding Why is the function always differentiated with respect to x? Books and tonnes of sites just let us plug in the rule. But, I haven't found a single resource that states why is the implicit function differentiated with respect to x? I don't know if it is just a convention.",,['derivatives']
30,Find the maximum value of $72\int\limits_{0}^{y}\sqrt{x^4+(y-y^2)^2}dx$,Find the maximum value of,72\int\limits_{0}^{y}\sqrt{x^4+(y-y^2)^2}dx,"Find the maximum value of $72\int\limits_{0}^{y}\sqrt{x^4+(y-y^2)^2}dx  $ for $y\in[0,1].$ I tried to differentiate the given function by using DUIS leibnitz rule but the calculations are messy and I tried to solve directly by integrating it but that also is not working.Can someone please help me in solving this question?","Find the maximum value of $72\int\limits_{0}^{y}\sqrt{x^4+(y-y^2)^2}dx  $ for $y\in[0,1].$ I tried to differentiate the given function by using DUIS leibnitz rule but the calculations are messy and I tried to solve directly by integrating it but that also is not working.Can someone please help me in solving this question?",,['derivatives']
31,"If $\int f(x) \sin{x} \cos{x}\,\mathrm dx = \frac {1}{2(b^2 - a^2)} \log f(x) +c $. Find $f(x)$",If . Find,"\int f(x) \sin{x} \cos{x}\,\mathrm dx = \frac {1}{2(b^2 - a^2)} \log f(x) +c  f(x)","Problem : If $\int f(x) \sin{x}  \cos{x}\,\mathrm  dx = \frac {1}{2(b^2 - a^2)} \log f(x) +c $ . Find $f(x)$ Solution : $\int f(x) \sin{x}  \cos{x}\,\mathrm  dx = \frac {1}{2(b^2 - a^2)} \log f(x) +c $ Differenting both sides,we get $ f(x) \sin{x}  \cos{x} = \frac {f'(x)}{2(b^2 - a^2)f(x)}  $ Am I doing right ?","Problem : If . Find Solution : Differenting both sides,we get Am I doing right ?","\int f(x) \sin{x}  \cos{x}\,\mathrm  dx = \frac {1}{2(b^2 - a^2)} \log f(x) +c  f(x) \int f(x) \sin{x}  \cos{x}\,\mathrm  dx = \frac {1}{2(b^2 - a^2)} \log f(x) +c   f(x) \sin{x}  \cos{x} = \frac {f'(x)}{2(b^2 - a^2)f(x)}  ",['derivatives']
32,Taking the derivative of x,Taking the derivative of x,,"Let's see I have the following equation $$ x=1 $$ I take the derivate of both sides with respect to $x$ : $$ \frac{\partial }{\partial x} x = \frac{\partial }{\partial x}1 $$ Therefore, $1=0$ . Clearly, that is not the right approach. So what is the right way to think of $x=1$ . What kind of object is it?","Let's see I have the following equation I take the derivate of both sides with respect to : Therefore, . Clearly, that is not the right approach. So what is the right way to think of . What kind of object is it?","
x=1
 x 
\frac{\partial }{\partial x} x = \frac{\partial }{\partial x}1
 1=0 x=1",['derivatives']
33,Optimal route consisting of rowing then walking,Optimal route consisting of rowing then walking,,"Problem You're in a boat on point A in the water, and you need to get to point B on land. Your rowing speed is 3km/h, and your walking speed 5km/h. See figure: Find the route that takes the least amount of time. My idea I started by marking an arbitrary route: From here, I figure the total time is going to be $$T = \frac{R}{3\mathrm{km/h}} + \frac{W}{5\mathrm{km/h}}$$ Since this is a function of two variables, I'm stuck. A general idea is to express $W$ in terms of $R$ to make it single-variable, and then apply the usual optimization tactics (with derivatives), but I'm having a hard time finding such an expression. Any help appreciated! EDIT - Alternative solution? Since the distance from A to the right angle (RA) is traveled 3/5 times as fast as the distance between RA and B, could I just scale the former up? That way, I get A-RA being a distance of $6\cdot\frac53 = 10\mathrm{km}$, which makes the hypotenuse $\sqrt{181}$ the shortest distance between A and B. And since we scaled it up, we can consider it traversable with walking speed rather than rowing speed! Thoughts?","Problem You're in a boat on point A in the water, and you need to get to point B on land. Your rowing speed is 3km/h, and your walking speed 5km/h. See figure: Find the route that takes the least amount of time. My idea I started by marking an arbitrary route: From here, I figure the total time is going to be $$T = \frac{R}{3\mathrm{km/h}} + \frac{W}{5\mathrm{km/h}}$$ Since this is a function of two variables, I'm stuck. A general idea is to express $W$ in terms of $R$ to make it single-variable, and then apply the usual optimization tactics (with derivatives), but I'm having a hard time finding such an expression. Any help appreciated! EDIT - Alternative solution? Since the distance from A to the right angle (RA) is traveled 3/5 times as fast as the distance between RA and B, could I just scale the former up? That way, I get A-RA being a distance of $6\cdot\frac53 = 10\mathrm{km}$, which makes the hypotenuse $\sqrt{181}$ the shortest distance between A and B. And since we scaled it up, we can consider it traversable with walking speed rather than rowing speed! Thoughts?",,"['trigonometry', 'derivatives', 'optimization']"
34,"prove that $f’(x)e^{\lambda x}$ is increasing if and only if $f’(x)+\lambda f(x)$ is increasing. Where $f\in C^1(0,\infty)$.",prove that  is increasing if and only if  is increasing. Where .,"f’(x)e^{\lambda x} f’(x)+\lambda f(x) f\in C^1(0,\infty)","prove that $f’(x)e^{\lambda x}$ is increasing if and only if $f’(x)+\lambda f(x)$ is increasing. Where $f\in C^1(0,\infty)$ , and $\lambda$ is a real number. I have tried to prove it by taking $0<x_1<x_2$ to make difference and to control each other. But it seems not that easy, I also have tried to prove it by contradiction, it failed as I need to find a small interval that $f’$ is monotone. But it isn’t always satisfied. Can anyone help me to figure it out or just give me some intention? Very appreciate it!","prove that is increasing if and only if is increasing. Where , and is a real number. I have tried to prove it by taking to make difference and to control each other. But it seems not that easy, I also have tried to prove it by contradiction, it failed as I need to find a small interval that is monotone. But it isn’t always satisfied. Can anyone help me to figure it out or just give me some intention? Very appreciate it!","f’(x)e^{\lambda x} f’(x)+\lambda f(x) f\in C^1(0,\infty) \lambda 0<x_1<x_2 f’","['derivatives', 'inequality', 'monotone-functions']"
35,Proving $dx.dy = r.drd\theta$ using x and y [duplicate],Proving  using x and y [duplicate],dx.dy = r.drd\theta,"This question already has answers here : how to get $dx\; dy=r\;dr\;d\theta$ [duplicate] (2 answers) Closed 7 years ago . I can prove that $dx dy = r.dr d\theta$ by drawing a circle and calculating the area of a small square in the polar coordinates, but when I try proving it using the equations below, I fail to prove it. What is my mistake? $x = r\cos(\theta) => dx = \cos(\theta).dr - \sin(\theta)r.d\theta$ $y = r\sin(\theta)=>dy=\sin(\theta).dr + \cos(\theta)r.d\theta$ $=> dxdy = r(\cos^2(\theta)-\sin^2(\theta))drd\theta=r\cos(2\theta).drd\theta$ I actually saw a similar question in this site: how to get $dx\; dy=r\;dr\;d\theta$ My problem was that I didn't understand why $drd\theta = -d\theta dr$","This question already has answers here : how to get $dx\; dy=r\;dr\;d\theta$ [duplicate] (2 answers) Closed 7 years ago . I can prove that $dx dy = r.dr d\theta$ by drawing a circle and calculating the area of a small square in the polar coordinates, but when I try proving it using the equations below, I fail to prove it. What is my mistake? $x = r\cos(\theta) => dx = \cos(\theta).dr - \sin(\theta)r.d\theta$ $y = r\sin(\theta)=>dy=\sin(\theta).dr + \cos(\theta)r.d\theta$ $=> dxdy = r(\cos^2(\theta)-\sin^2(\theta))drd\theta=r\cos(2\theta).drd\theta$ I actually saw a similar question in this site: how to get $dx\; dy=r\;dr\;d\theta$ My problem was that I didn't understand why $drd\theta = -d\theta dr$",,"['derivatives', 'polar-coordinates', 'jacobian']"
36,Math question please Rolle theorem?,Math question please Rolle theorem?,,"I have to prove that the equation $$x^5 +3x- 6$$ can't have more than one real root..so the function is continuous, has a derivative (both in $R$) . In $R$ there must be an interval where $f'(c)=0$, and if I prove this,than the equation has at least one real root. So $5x^4+3 =0$ ..this equation is only true for $x=0$. How to prove that this is the only root?","I have to prove that the equation $$x^5 +3x- 6$$ can't have more than one real root..so the function is continuous, has a derivative (both in $R$) . In $R$ there must be an interval where $f'(c)=0$, and if I prove this,than the equation has at least one real root. So $5x^4+3 =0$ ..this equation is only true for $x=0$. How to prove that this is the only root?",,['derivatives']
37,How to find the Frechet derivative at $A\rightarrow A^{-1}$ mapping?,How to find the Frechet derivative at  mapping?,A\rightarrow A^{-1},"I am reading on my own the Lectures on the Geometry of Manifolds (http://nd.edu/~lnicolae/Lectures.pdf ) , and got stuck in solving the exercise 1.1.3 (b) . The 1.1.3 (b) is : Let F: $U\rightarrow U$ be defined as $A\rightarrow A^{-1}$.  Show that $% D_{A}F(H)=-A^{-1}HA^{-1}$ for any $n\times n$ matrix $H.$ $D_{A}F(H)$ is the Frechet derivative of F at A.  H I guess should be the small ""drifting"", so that it is actually trying to calculate the derivative of $(A+tH)^{-1}$ I tried to expand $(A+H)^{-1}$ as $(I+A^{-1}H)^{-1}A^{-1}=I-A^{-1}H+\frac{% A^{-2}H^{2}}{2!}-\frac{A^{-3}H^{3}}{3!}+...,$ but it doesn't looks like $% -A^{-1}HA^{-1}$ Some one can give me a hint?","I am reading on my own the Lectures on the Geometry of Manifolds (http://nd.edu/~lnicolae/Lectures.pdf ) , and got stuck in solving the exercise 1.1.3 (b) . The 1.1.3 (b) is : Let F: $U\rightarrow U$ be defined as $A\rightarrow A^{-1}$.  Show that $% D_{A}F(H)=-A^{-1}HA^{-1}$ for any $n\times n$ matrix $H.$ $D_{A}F(H)$ is the Frechet derivative of F at A.  H I guess should be the small ""drifting"", so that it is actually trying to calculate the derivative of $(A+tH)^{-1}$ I tried to expand $(A+H)^{-1}$ as $(I+A^{-1}H)^{-1}A^{-1}=I-A^{-1}H+\frac{% A^{-2}H^{2}}{2!}-\frac{A^{-3}H^{3}}{3!}+...,$ but it doesn't looks like $% -A^{-1}HA^{-1}$ Some one can give me a hint?",,"['derivatives', 'manifolds', 'inverse']"
38,Is there a unified description of the geometric derivative?,Is there a unified description of the geometric derivative?,,"The exterior derivative ( $d$ or $\nabla\wedge$ ) is a very unifying concept, in that it subsumes the gradient of a scalar, the curl of a vector, and the ""divergence"" of a bivector, thus also lumping together the fundamental theorem of line integrals, Stokes' Theorem, and the divergence theorem. The interpretation of the exterior derivative might be ""the extent to which the field aligns with the boundary of an infinitesimal region"". The geometric derivative seems like it should offer further unification, because, at least for vectors, it combines the interior and exterior derivatives into one.  But the question is, how can I describe the meaning of the geometric derivative without resorting to describing each component separately? On the one hand, I'm under the impression this is kind of a common issue with geometric algebra: yes, it allows you to combine objects of different degrees, yet sometimes it appears impossible to conceive of the sum as an invariant whole. But not always.  For example, you can make a ""rotor"" by adding a scalar and a bivector.  Of course, the rotor is a transformation rather than a standalone object, but still it is a meaningful interpretation.  In 3D, this is simply an element of the even sub-algebra. Meanwhile, an even element of the 4D Clifford algebra can be construed as a transformation on bivectors, since it includes a duality rotation in addition to the spatial rotation. So, after taking the geometric derivative of a field of degree $k$ (or perhaps mixed degree), can we articulate what it is we have?  Is it some kind of transformation?  Or does it represent the way that the field varies in a generalized sense?  If so, what is that sense?","The exterior derivative ( or ) is a very unifying concept, in that it subsumes the gradient of a scalar, the curl of a vector, and the ""divergence"" of a bivector, thus also lumping together the fundamental theorem of line integrals, Stokes' Theorem, and the divergence theorem. The interpretation of the exterior derivative might be ""the extent to which the field aligns with the boundary of an infinitesimal region"". The geometric derivative seems like it should offer further unification, because, at least for vectors, it combines the interior and exterior derivatives into one.  But the question is, how can I describe the meaning of the geometric derivative without resorting to describing each component separately? On the one hand, I'm under the impression this is kind of a common issue with geometric algebra: yes, it allows you to combine objects of different degrees, yet sometimes it appears impossible to conceive of the sum as an invariant whole. But not always.  For example, you can make a ""rotor"" by adding a scalar and a bivector.  Of course, the rotor is a transformation rather than a standalone object, but still it is a meaningful interpretation.  In 3D, this is simply an element of the even sub-algebra. Meanwhile, an even element of the 4D Clifford algebra can be construed as a transformation on bivectors, since it includes a duality rotation in addition to the spatial rotation. So, after taking the geometric derivative of a field of degree (or perhaps mixed degree), can we articulate what it is we have?  Is it some kind of transformation?  Or does it represent the way that the field varies in a generalized sense?  If so, what is that sense?",d \nabla\wedge k,"['derivatives', 'vector-spaces', 'clifford-algebras', 'geometric-algebras', 'exterior-derivative']"
39,Three point numerical differentiation,Three point numerical differentiation,,"Is there any generalized way to calculate numerical differentiation using a certain number of points? I have found 2-point and 5-point methods, but could not find information about using any other number of points. I am interested in doing 3-point, but am not sure if this would be practical or possible.","Is there any generalized way to calculate numerical differentiation using a certain number of points? I have found 2-point and 5-point methods, but could not find information about using any other number of points. I am interested in doing 3-point, but am not sure if this would be practical or possible.",,"['derivatives', 'numerical-methods', 'finite-differences']"
40,Coordinate Free Proof of $ d^2 = 0$,Coordinate Free Proof of, d^2 = 0,"We all know that when the exterior derivative is applied twice to a $k$ form, it always yields the null $k+2$ form (i.e. $d^2 = 0$). However, the only proofs I've seen of this does it in some chart and using coordinate notation computes this. I was wondering if anyone has seen or knows of a proof of $d^2 = 0$ that doesn't use coordinates and instead proves this in a coordinate free way.","We all know that when the exterior derivative is applied twice to a $k$ form, it always yields the null $k+2$ form (i.e. $d^2 = 0$). However, the only proofs I've seen of this does it in some chart and using coordinate notation computes this. I was wondering if anyone has seen or knows of a proof of $d^2 = 0$ that doesn't use coordinates and instead proves this in a coordinate free way.",,"['differential-geometry', 'derivatives', 'differential-forms']"
41,Is there a nowhere differentiable norm on $\Bbb R^n$?,Is there a nowhere differentiable norm on ?,\Bbb R^n,Is there a nowhere differentiable norm on $\Bbb R^n$? The non differentiable norms that I know (e.g. the one norm $\|\cdot\|_1$  and the infinity norm $\|\cdot\|_{\infty}$) are non differentiable at a few points (basically the corners of the associated unit ball) but I can't find/build any example of norm which is differentiable nowhere...,Is there a nowhere differentiable norm on $\Bbb R^n$? The non differentiable norms that I know (e.g. the one norm $\|\cdot\|_1$  and the infinity norm $\|\cdot\|_{\infty}$) are non differentiable at a few points (basically the corners of the associated unit ball) but I can't find/build any example of norm which is differentiable nowhere...,,"['derivatives', 'examples-counterexamples', 'normed-spaces']"
42,Is the derivative of a quadratic related to the second difference of that quadratic?,Is the derivative of a quadratic related to the second difference of that quadratic?,,"Please do not judge me too harshly for my lack of knowledge, but at school we have gone over Quadratic functions recently. Now, these types of functions are not new to me, however when we viewed a table of one of these functions I found something interesting: $$ \begin{array}{c|lcr} x & f(x) & \text{1st Diff.} & \text{2nd Diff} \\ \hline -2 & 9 & - & - \\ -1 & 6 & -3 &- \\ 0 & 5 & -1 & 2 \\ 1 & 6 & 1 & 2 \\ 2 & 9 & 3 & 2 \end{array} $$ Now, as you might guess, the explicit function of this quadratic is  $$ f(x) = x^2 + 5 $$ But what was interesting was that the 2nd difference is constantly $+2$, which would be the slope of the derivative function, since $a = 1$: $$ f'(x) = 2(1)x $$ We've seen a few more and every time the $a$ coefficient of the quadratic was $1/2$ that of the second difference, which only added to the connection. So, what is the relationship between the 2nd difference and the derivative of this function? I looked online but nothing had the answer that I was looking for. Thank you! Gil Keidar","Please do not judge me too harshly for my lack of knowledge, but at school we have gone over Quadratic functions recently. Now, these types of functions are not new to me, however when we viewed a table of one of these functions I found something interesting: $$ \begin{array}{c|lcr} x & f(x) & \text{1st Diff.} & \text{2nd Diff} \\ \hline -2 & 9 & - & - \\ -1 & 6 & -3 &- \\ 0 & 5 & -1 & 2 \\ 1 & 6 & 1 & 2 \\ 2 & 9 & 3 & 2 \end{array} $$ Now, as you might guess, the explicit function of this quadratic is  $$ f(x) = x^2 + 5 $$ But what was interesting was that the 2nd difference is constantly $+2$, which would be the slope of the derivative function, since $a = 1$: $$ f'(x) = 2(1)x $$ We've seen a few more and every time the $a$ coefficient of the quadratic was $1/2$ that of the second difference, which only added to the connection. So, what is the relationship between the 2nd difference and the derivative of this function? I looked online but nothing had the answer that I was looking for. Thank you! Gil Keidar",,"['derivatives', 'quadratics']"
43,proper notation for derivatives and differential,proper notation for derivatives and differential,,"This is a simple question (perhaps pedantic) about basic differentiation in real coordinate space.  In practice, I don't ever have issues using or carrying out differentiation in $\mathbb{R}^m$ . But I'm pretty sure I frequently and severely abuse notation and I'm trying to improve. question 1: Let $f:\mathbb{R}^m \to \mathbb{R}^n$ be some smooth map between real coordinate spaces. It maps some point $x\in\mathbb{R}^m$ to $y\;\dot{=}\;f(x)\in\mathbb{R}^n$ .  What is the proper way to write the derivative of $f$ (Jacobian)? $$  df \quad, \quad \frac{\partial f}{\partial x} \quad, \quad \frac{\partial f(x)}{\partial x}  \quad, \quad  \frac{\partial y}{\partial x} \qquad ? $$ does $\frac{\partial f}{\partial x}$ even mean anything or do we need to ""feed"" $f$ some input before we can differentiate as $\frac{\partial f(x)}{\partial x}$ ? In the last of the above, $y$ is just a point, $y=f(x)\in\mathbb{R}^n$ , not a function, that can be expressed in terms of $x$ . Can we differentiate a point as $ \frac{\partial y}{\partial x}$ or is this simply a common abuse of notation? question 2: continuing the above, what is the proper way to write the derivative of $f$ at a particular point, say $p\in\mathbb{R}^m$ ?  which of the following are correct, incorrect, or equivalent? $$  df(p) \quad, \quad \frac{\partial f}{\partial x}\big|_p \quad, \quad \frac{\partial f(x)}{\partial x}\big|_p   \quad, \quad \frac{\partial f(p)}{\partial p}  \qquad?    $$ I have a hunch that the ""proper"" notation for the derivative of some $f:\mathbb{R}^m \to \mathbb{R}^n$ is just $df$ and this is defined such that, at any arbitrary point $x\in\mathbb{R}^m$ , it is given by $df(x)=\frac{\partial f(x)}{\partial x}$ .  Is this correct? context: I posed this question in the context of real coordinate space(s), but I'm asking it with differential geometry in mind. My background is not in math and I recently started teaching myself some differential geometry and quickly realized I don't have a great grasp of proper mathematical notation. edit: This question is related to one I asked on the physics page at this link","This is a simple question (perhaps pedantic) about basic differentiation in real coordinate space.  In practice, I don't ever have issues using or carrying out differentiation in . But I'm pretty sure I frequently and severely abuse notation and I'm trying to improve. question 1: Let be some smooth map between real coordinate spaces. It maps some point to .  What is the proper way to write the derivative of (Jacobian)? does even mean anything or do we need to ""feed"" some input before we can differentiate as ? In the last of the above, is just a point, , not a function, that can be expressed in terms of . Can we differentiate a point as or is this simply a common abuse of notation? question 2: continuing the above, what is the proper way to write the derivative of at a particular point, say ?  which of the following are correct, incorrect, or equivalent? I have a hunch that the ""proper"" notation for the derivative of some is just and this is defined such that, at any arbitrary point , it is given by .  Is this correct? context: I posed this question in the context of real coordinate space(s), but I'm asking it with differential geometry in mind. My background is not in math and I recently started teaching myself some differential geometry and quickly realized I don't have a great grasp of proper mathematical notation. edit: This question is related to one I asked on the physics page at this link","\mathbb{R}^m f:\mathbb{R}^m \to \mathbb{R}^n x\in\mathbb{R}^m y\;\dot{=}\;f(x)\in\mathbb{R}^n f 
 df \quad, \quad \frac{\partial f}{\partial x} \quad, \quad \frac{\partial f(x)}{\partial x}  \quad, \quad
 \frac{\partial y}{\partial x} \qquad ?
 \frac{\partial f}{\partial x} f \frac{\partial f(x)}{\partial x} y y=f(x)\in\mathbb{R}^n x  \frac{\partial y}{\partial x} f p\in\mathbb{R}^m 
 df(p) \quad, \quad \frac{\partial f}{\partial x}\big|_p \quad, \quad \frac{\partial f(x)}{\partial x}\big|_p   \quad, \quad \frac{\partial f(p)}{\partial p} 
\qquad?   
 f:\mathbb{R}^m \to \mathbb{R}^n df x\in\mathbb{R}^m df(x)=\frac{\partial f(x)}{\partial x}","['derivatives', 'differential-geometry', 'partial-derivative', 'jacobian']"
44,Show that there is a simultaneously tangent line to both the curves $y = e^x$ and $y = \ln x$,Show that there is a simultaneously tangent line to both the curves  and,y = e^x y = \ln x,I found the derivatives of both of the curves but I'm having trouble on how to move on from here: $y = e^x\implies y' = e^x$ $y = \ln x\implies y' = \frac{1}{x}$,I found the derivatives of both of the curves but I'm having trouble on how to move on from here:,y = e^x\implies y' = e^x y = \ln x\implies y' = \frac{1}{x},"['derivatives', 'tangent-line']"
45,$n$-th derivative of $\sin^k(x)$,-th derivative of,n \sin^k(x),"I would like to know if there is a general formula to calculate the $n$-th derivative of $\sin^k(x)$ evaluated at $x=0$, that is, $$\left.\frac{d^n}{d x^n} (\sin^k(x))\right|_{x=0}$$ with $0\leq k \leq n$.","I would like to know if there is a general formula to calculate the $n$-th derivative of $\sin^k(x)$ evaluated at $x=0$, that is, $$\left.\frac{d^n}{d x^n} (\sin^k(x))\right|_{x=0}$$ with $0\leq k \leq n$.",,['derivatives']
46,What is the term for whatever is being differentiated?,What is the term for whatever is being differentiated?,,"When we integrate a function: $ \int^b_a {2\over x^2} dx$ The expression to be integrated (is this case $ {2\over x^2} $) is referred to as the integrand . When we differentiate a function: $ {d \over dx } {2 \over x^2}$ Is there a word to describe the expression to to be differentiated? For instance, if I wanted to say ""In the above equation, two over x squared is the --------"", what would I fill in the blank?","When we integrate a function: $ \int^b_a {2\over x^2} dx$ The expression to be integrated (is this case $ {2\over x^2} $) is referred to as the integrand . When we differentiate a function: $ {d \over dx } {2 \over x^2}$ Is there a word to describe the expression to to be differentiated? For instance, if I wanted to say ""In the above equation, two over x squared is the --------"", what would I fill in the blank?",,"['derivatives', 'terminology']"
47,How to obtain (prove) 5-stencil formula for 2nd derivative?,How to obtain (prove) 5-stencil formula for 2nd derivative?,,"My question seems pretty easy. Prove the correctness of the following approximation: $$f(x)''= \frac{-f(x-2h)+16f(x-h)-30f(x)+16f(x-h)-f(x+2h)}{12h^2}$$ I rendered myself deeply saddened upon stumbling on this and being seemingly unable solve it by my own. I also failed to find proof anywhere online. Only final answer. The way I tried to it is via pretty common Taylor series expansion: $$f(x+h) = f(x) + f'(x)h+\frac{1}{2!}f''(x)h^2+\frac{1}{3!}f^{(3)}(x)h^3 + \sum_{n=4}^\infty \frac{1}{n!}f^{(n)}(x)h^n\quad (1)$$ I cut it off after $f^{(3)}$. I use this formula to get rest of the points to have 5-stencils, simply by substituting $h$ with $\{-h; 2h; -2h\}$ Thus I get: $$f(x-h) = f(x) - f'(x)h+\frac{1}{2!}f''(x)h^2-\frac{1}{3!}f^{(3)}(x)h^3\quad (2)$$  $$f(x+2h) = f(x) + 2f'(x)h+\frac{4}{2!}f''(x)h^2+\frac{8}{3!}f^{(3)}(x)h^3\quad (3)$$ $$f(x-2h) = f(x) - 2f'(x)h+\frac{4}{2!}f''(x)h^2-\frac{8}{3!}f^{(3)}(x)h^3\quad (4)$$ When I use equations $(1)$ and $(2)$, and add them by sides, I can get the 3-point formula: $$f(x+h) + f(x-h) = 2f(x) + f''(x)h^2\quad (5)$$ $$f''(x) = \frac{f(x-h) - f(2x) + f(x+h)}{h^2}$$ However when I try to do this with all the equations $(1)$-$(4)$ I get: $$f(x+h) + f(x-h) = 2f(x) + f''(x)h^2 \quad (6)$$ $$f(x+2h) + f(x-2h) = 2f(x) +4f''(x)h^2\quad (7)$$ Then I can try subtracting $(6)$ from $(7)$ and I get: $$f(x+h) + f(x-h) - f(x+2h) - f(x-2h) = 3 f''(x)h^2\quad (8)$$ which gives $$f''(x) = \frac{f(x+h) + f(x-h) - f(x+2h) - f(x-2h) } {3 h^2} \quad(9)$$ This is clearly different from what I am expecting. Also doing $(6)$+$(7)$ doesn't seem to yield correct coefficients, even though it preserves $f(x)$ term. Could you point out flaw in the approach and provide correct reasoning or any materials? All I found were very general or final answers with no explicit transformations. I feel kinda stupid being unable to get it right but I can't spot the flaw.","My question seems pretty easy. Prove the correctness of the following approximation: $$f(x)''= \frac{-f(x-2h)+16f(x-h)-30f(x)+16f(x-h)-f(x+2h)}{12h^2}$$ I rendered myself deeply saddened upon stumbling on this and being seemingly unable solve it by my own. I also failed to find proof anywhere online. Only final answer. The way I tried to it is via pretty common Taylor series expansion: $$f(x+h) = f(x) + f'(x)h+\frac{1}{2!}f''(x)h^2+\frac{1}{3!}f^{(3)}(x)h^3 + \sum_{n=4}^\infty \frac{1}{n!}f^{(n)}(x)h^n\quad (1)$$ I cut it off after $f^{(3)}$. I use this formula to get rest of the points to have 5-stencils, simply by substituting $h$ with $\{-h; 2h; -2h\}$ Thus I get: $$f(x-h) = f(x) - f'(x)h+\frac{1}{2!}f''(x)h^2-\frac{1}{3!}f^{(3)}(x)h^3\quad (2)$$  $$f(x+2h) = f(x) + 2f'(x)h+\frac{4}{2!}f''(x)h^2+\frac{8}{3!}f^{(3)}(x)h^3\quad (3)$$ $$f(x-2h) = f(x) - 2f'(x)h+\frac{4}{2!}f''(x)h^2-\frac{8}{3!}f^{(3)}(x)h^3\quad (4)$$ When I use equations $(1)$ and $(2)$, and add them by sides, I can get the 3-point formula: $$f(x+h) + f(x-h) = 2f(x) + f''(x)h^2\quad (5)$$ $$f''(x) = \frac{f(x-h) - f(2x) + f(x+h)}{h^2}$$ However when I try to do this with all the equations $(1)$-$(4)$ I get: $$f(x+h) + f(x-h) = 2f(x) + f''(x)h^2 \quad (6)$$ $$f(x+2h) + f(x-2h) = 2f(x) +4f''(x)h^2\quad (7)$$ Then I can try subtracting $(6)$ from $(7)$ and I get: $$f(x+h) + f(x-h) - f(x+2h) - f(x-2h) = 3 f''(x)h^2\quad (8)$$ which gives $$f''(x) = \frac{f(x+h) + f(x-h) - f(x+2h) - f(x-2h) } {3 h^2} \quad(9)$$ This is clearly different from what I am expecting. Also doing $(6)$+$(7)$ doesn't seem to yield correct coefficients, even though it preserves $f(x)$ term. Could you point out flaw in the approach and provide correct reasoning or any materials? All I found were very general or final answers with no explicit transformations. I feel kinda stupid being unable to get it right but I can't spot the flaw.",,"['numerical-methods', 'derivatives']"
48,"Given two positive numbers $x,\,y$ so that $32\,x^{6}+ 4\,y^{3}= 1$. Prove that $\frac{(2\,x^{2}+ y+ 3)^{5}}{3(x^{2}+ y^{2})- 3(x+ y)+ 2}\leqq 2048$ .",Given two positive numbers  so that . Prove that  .,"x,\,y 32\,x^{6}+ 4\,y^{3}= 1 \frac{(2\,x^{2}+ y+ 3)^{5}}{3(x^{2}+ y^{2})- 3(x+ y)+ 2}\leqq 2048","Given two positive numbers $x,\,y$ so that $32\,x^{6}+ 4\,y^{3}= 1$ . Prove that $$p(x)\equiv p= \frac{(2\,x^{2}+ y+ 3)^{5}}{3(x^{2}+ y^{2})- 3(x+ y)+ 2}\leqq 2048$$ My solution in VMF : (and I'm looking forward to seeing a nicer one(s), thanks for your interests !) $$32\,x^{6}+ 4\,y^{3}= 1\,\therefore\,(y- 2\,x^{2})\left ( x- \frac{1}{2} \right )\leqq 0,\,\left ( x- \frac{1}{2} \right )\left ( y- \frac{1}{2} \right )\leqq 0,\,{y}'= -\,\frac{16\,x^{5}}{y^{2}}$$ Thus, we have $${p}'(x)=$$ $$= \frac{{\left (\!(\!2 x^{2}+ y+ 3\!)^{5}\!\right )}'\{\!3(\!x^{2}+ y^{2}\!)- 3(x+ y)+ 2\!\}- {\left (\!3(\!x^{2}+ y^{2}\!)- 3(x+ y)+ 2\!\right )}'(\!2 x^{2}+ y+ 3\!)^{5}}{\left (\!3(\!x^{2}+ y^{2}\!)- 3(x+ y)+ 2\!\right )^{2}}=$$ $$\frac{5(\!2 x^{2}+ y+ 3\!)^{4}\left (\!4 x- \frac{16 y^{5}}{x^{2}}\!\right )\{\!3(\!x^{2}+ y^{2}\!)- 3(x+ y)+ 2\!\}- \left (\!6 x- 3- \frac{96 x^{5}}{y}+ \frac{48 x^{5}}{y^{2}}\!\right )(\!2 x^{2}+ y+ 3\!)^{5}}{\left (\!3(\!x^{2}+ y^{2}\!)- 3(x+ y)+ 2\!\right )^{2}}$$ $$= \frac{\left ( 20\,x(y- 2\,x^{2})(y+ 2\,x^{2})\{\!3(x^{2}+ y^{2})- 3(x+ y)+ 2\!\}- 3[x^{5}(16- 32\,y)+ y^{2}(2\,x- 1)] \right )}{y^{2}\left ( 3(x^{2}+ y^{2})- 3(x+ y)+ 2 \right )^{2}}$$ $$\times (2\,x^{2}+ y+ 3)^{4}$$ Using derivatives, the equation of the tangent line can be stated as follows: $$p(x)- p\left ( \frac{1}{2} \right )= {p}'(x)\left ( x- \frac{1}{2} \right )\leqq 0\,(\!easy\,to\,see\,immediately\,!\!)\,\therefore\,p(x)\leqq p\left ( \frac{1}{2} \right )\leqq 2048$$","Given two positive numbers so that . Prove that My solution in VMF : (and I'm looking forward to seeing a nicer one(s), thanks for your interests !) Thus, we have Using derivatives, the equation of the tangent line can be stated as follows:","x,\,y 32\,x^{6}+ 4\,y^{3}= 1 p(x)\equiv p= \frac{(2\,x^{2}+ y+ 3)^{5}}{3(x^{2}+ y^{2})- 3(x+ y)+ 2}\leqq 2048 32\,x^{6}+ 4\,y^{3}= 1\,\therefore\,(y- 2\,x^{2})\left ( x- \frac{1}{2} \right )\leqq 0,\,\left ( x- \frac{1}{2} \right )\left ( y- \frac{1}{2} \right )\leqq 0,\,{y}'= -\,\frac{16\,x^{5}}{y^{2}} {p}'(x)= = \frac{{\left (\!(\!2 x^{2}+ y+ 3\!)^{5}\!\right )}'\{\!3(\!x^{2}+ y^{2}\!)- 3(x+ y)+ 2\!\}- {\left (\!3(\!x^{2}+ y^{2}\!)- 3(x+ y)+ 2\!\right )}'(\!2 x^{2}+ y+ 3\!)^{5}}{\left (\!3(\!x^{2}+ y^{2}\!)- 3(x+ y)+ 2\!\right )^{2}}= \frac{5(\!2 x^{2}+ y+ 3\!)^{4}\left (\!4 x- \frac{16 y^{5}}{x^{2}}\!\right )\{\!3(\!x^{2}+ y^{2}\!)- 3(x+ y)+ 2\!\}- \left (\!6 x- 3- \frac{96 x^{5}}{y}+ \frac{48 x^{5}}{y^{2}}\!\right )(\!2 x^{2}+ y+ 3\!)^{5}}{\left (\!3(\!x^{2}+ y^{2}\!)- 3(x+ y)+ 2\!\right )^{2}} = \frac{\left ( 20\,x(y- 2\,x^{2})(y+ 2\,x^{2})\{\!3(x^{2}+ y^{2})- 3(x+ y)+ 2\!\}- 3[x^{5}(16- 32\,y)+ y^{2}(2\,x- 1)] \right )}{y^{2}\left ( 3(x^{2}+ y^{2})- 3(x+ y)+ 2 \right )^{2}} \times (2\,x^{2}+ y+ 3)^{4} p(x)- p\left ( \frac{1}{2} \right )= {p}'(x)\left ( x- \frac{1}{2} \right )\leqq 0\,(\!easy\,to\,see\,immediately\,!\!)\,\therefore\,p(x)\leqq p\left ( \frac{1}{2} \right )\leqq 2048","['derivatives', 'inequality']"
49,Second derivative of Kullback–Leibler divergence,Second derivative of Kullback–Leibler divergence,,"The Kullback-Leibler divergence is defined here . I have to find the second derivative of $\textrm{KL}(p(s, \theta)||\mu(s) q(\theta, s))$ regarding $p(s, \theta)$, where $p(s, \theta)$ is a joint probability (and therefore, $\frac{1}{p(s, \theta)} \geq 0$), and $\frac{\partial^2 p(s, \theta)}{\partial p(s,\theta)} = 0$. By definition, $\textrm{KL}(p(s, \theta)||\mu(s) q(\theta, s)) = \sum\limits_{i} P(i) \log \frac{P(i)}{Q(i)}$, where $Q(s,\theta)=\mu(s) q(\theta, s)$. The final result of the derivation must show that the second derivative of $\textrm{KL}(p(s, \theta)||\mu(s) q(\theta, s))$ is non-negative. My colleague scribbled these steps before leaving: $ \frac{\partial^2 \sum\limits_{i} p(s, \theta) \log \frac{p(s,\theta)}{Q(s, \theta)}}{\partial p(s,\theta)|s,\theta} = \\  P(s,\theta) \log P(s,\theta)-P(s,\theta) \log Q(s,\theta) = \\  \log P(s,\theta) * \frac{1}{P(s,\theta)} = \\  \log P(s,\theta) - \log Q(s,\theta) = \\  \frac{1}{P(s,\theta)} \geq 0.$ But this was really fast, and his handwriting isn't the best. Therefore, there might be some details wrong (and he might also have made a mistake). I'm guessing in the second line, some of terms must be derivated (as if it were a first derivation of a multiplication $(fg)' = f'g + fg'$), but the minus sign doesn't correspond and we are doing a second derivation. I'm also assuming he omitted the $\sum$. On the third line, I guess the second half of the equation equals $0$ and he derived the $\log$ into $\frac{1}{P(s,\theta)}$, but why is there a $\log$ still? On the fourth line there are other $\log$s, which I have no idea where they came from, but if we derive everything, we reach the final line, which we know is positive, and the proof ends. I tried to reach the solution on my own, like this $ \frac{\partial^2 \sum\limits_{i} P(i) \log \frac{P(i)}{Q(i)}}  {\partial P(i)} = \sum\limits_{i} P'' \log \frac{P}{Q} + 2 P' \log '\frac{P}{Q} + P \log '' \frac{P}{Q} = \sum\limits_{i} 0 + 2\frac{Q}{P} + P \frac{PQ'-QP'}{P^2} = \sum\limits_{i} 2\frac{Q}{P} + \frac{PQ'-QP'}{P} = \sum\limits_{i} 2\frac{Q}{P} - \frac{Q}{P} = \sum\limits_{i} \frac{Q}{P}  $ But I can't really move from here to the final solution. What did I do wrong? What was my colleague doing?","The Kullback-Leibler divergence is defined here . I have to find the second derivative of $\textrm{KL}(p(s, \theta)||\mu(s) q(\theta, s))$ regarding $p(s, \theta)$, where $p(s, \theta)$ is a joint probability (and therefore, $\frac{1}{p(s, \theta)} \geq 0$), and $\frac{\partial^2 p(s, \theta)}{\partial p(s,\theta)} = 0$. By definition, $\textrm{KL}(p(s, \theta)||\mu(s) q(\theta, s)) = \sum\limits_{i} P(i) \log \frac{P(i)}{Q(i)}$, where $Q(s,\theta)=\mu(s) q(\theta, s)$. The final result of the derivation must show that the second derivative of $\textrm{KL}(p(s, \theta)||\mu(s) q(\theta, s))$ is non-negative. My colleague scribbled these steps before leaving: $ \frac{\partial^2 \sum\limits_{i} p(s, \theta) \log \frac{p(s,\theta)}{Q(s, \theta)}}{\partial p(s,\theta)|s,\theta} = \\  P(s,\theta) \log P(s,\theta)-P(s,\theta) \log Q(s,\theta) = \\  \log P(s,\theta) * \frac{1}{P(s,\theta)} = \\  \log P(s,\theta) - \log Q(s,\theta) = \\  \frac{1}{P(s,\theta)} \geq 0.$ But this was really fast, and his handwriting isn't the best. Therefore, there might be some details wrong (and he might also have made a mistake). I'm guessing in the second line, some of terms must be derivated (as if it were a first derivation of a multiplication $(fg)' = f'g + fg'$), but the minus sign doesn't correspond and we are doing a second derivation. I'm also assuming he omitted the $\sum$. On the third line, I guess the second half of the equation equals $0$ and he derived the $\log$ into $\frac{1}{P(s,\theta)}$, but why is there a $\log$ still? On the fourth line there are other $\log$s, which I have no idea where they came from, but if we derive everything, we reach the final line, which we know is positive, and the proof ends. I tried to reach the solution on my own, like this $ \frac{\partial^2 \sum\limits_{i} P(i) \log \frac{P(i)}{Q(i)}}  {\partial P(i)} = \sum\limits_{i} P'' \log \frac{P}{Q} + 2 P' \log '\frac{P}{Q} + P \log '' \frac{P}{Q} = \sum\limits_{i} 0 + 2\frac{Q}{P} + P \frac{PQ'-QP'}{P^2} = \sum\limits_{i} 2\frac{Q}{P} + \frac{PQ'-QP'}{P} = \sum\limits_{i} 2\frac{Q}{P} - \frac{Q}{P} = \sum\limits_{i} \frac{Q}{P}  $ But I can't really move from here to the final solution. What did I do wrong? What was my colleague doing?",,"['derivatives', 'partial-derivative']"
50,Finite difference method,Finite difference method,,"I wanted to ask something regarding the finite difference approximation. I used the finite difference to calculate the numerical derivatives of my function. The finite difference is given by the following formula: \begin{equation} \frac{f(x+h)-f(x)}{h} \end{equation} The value of $h$ is questionable. In theory we should take it as small as possible, but I am not sure if we can just pick random different values for $h$ and try to see which one works better or if there is any ""rule"" or constraint to pick up a good value of $h$. With Thanks","I wanted to ask something regarding the finite difference approximation. I used the finite difference to calculate the numerical derivatives of my function. The finite difference is given by the following formula: \begin{equation} \frac{f(x+h)-f(x)}{h} \end{equation} The value of $h$ is questionable. In theory we should take it as small as possible, but I am not sure if we can just pick random different values for $h$ and try to see which one works better or if there is any ""rule"" or constraint to pick up a good value of $h$. With Thanks",,"['derivatives', 'optimization', 'numerical-methods']"
51,"What is the use of, and intuition behind, writing $\frac{d^2}{dx^2}$ for the second derivative?","What is the use of, and intuition behind, writing  for the second derivative?",\frac{d^2}{dx^2},"Is it possible to take a second derivative without taking the first derivative before? Why do we multiply the $d$ and $dx$ operators? Like, does $\dfrac{d^2}{dx^2}$ really mean $\dfrac{d}{dx} \cdot \dfrac{d}{dx}$? What's the intuitive understanding about this? Can it be represented in a graph? Like... 'Little change squared in $y$ over little change squared in $x$'?","Is it possible to take a second derivative without taking the first derivative before? Why do we multiply the $d$ and $dx$ operators? Like, does $\dfrac{d^2}{dx^2}$ really mean $\dfrac{d}{dx} \cdot \dfrac{d}{dx}$? What's the intuitive understanding about this? Can it be represented in a graph? Like... 'Little change squared in $y$ over little change squared in $x$'?",,"['notation', 'derivatives', 'intuition']"
52,Find the general form of $n$th derivative $f(x) = \ln(1+x)$,Find the general form of th derivative,n f(x) = \ln(1+x),I think I am doing it the correct way but I am not sure. Is it $$(-1)^{n+1}n!(1+x)^{-n} ?$$ Thank you guys.,I think I am doing it the correct way but I am not sure. Is it Thank you guys.,(-1)^{n+1}n!(1+x)^{-n} ?,['derivatives']
53,Method for estimating the $n^{th}$ derivative?,Method for estimating the  derivative?,n^{th},"When using numerical analysis, I often find that I am required to estimate a derivative (e.g. when using Newton Iteration for finding roots).  To estimate the first derivative of a function $f(x)$ at a point $x_0$ (assuming that $f(x)$ is continuous at $x_0$), one can use the slightly-modified (to avoid bias to one side) first principles formula for derivatives, shown below. For small $h$: $$f'(x_0)\approx\frac{f(x+h)-f(x-h)}{2h}\tag{1}$$ Using this method, we can estimate $f^{(n)}(x)$ recursively with, for sufficiently small $h$: $$f^{(n)}(x_0)\approx\frac{f^{(n-1)}(x+h)-f^{(n-1)}(x-h)}{2h}\tag{2}$$ The problem I have with $(2)$ is that each recursion produces a loss of accuracy that builds up.  As well, to estimate $f^{(n)}(x_0)$, the function $f(x)$ is required to be computed $2^n$ times. Is $(2)$ the best method for approximating the $n^{th}$ derivative of $f(x_0)$ numerically or are there more efficient methods?","When using numerical analysis, I often find that I am required to estimate a derivative (e.g. when using Newton Iteration for finding roots).  To estimate the first derivative of a function $f(x)$ at a point $x_0$ (assuming that $f(x)$ is continuous at $x_0$), one can use the slightly-modified (to avoid bias to one side) first principles formula for derivatives, shown below. For small $h$: $$f'(x_0)\approx\frac{f(x+h)-f(x-h)}{2h}\tag{1}$$ Using this method, we can estimate $f^{(n)}(x)$ recursively with, for sufficiently small $h$: $$f^{(n)}(x_0)\approx\frac{f^{(n-1)}(x+h)-f^{(n-1)}(x-h)}{2h}\tag{2}$$ The problem I have with $(2)$ is that each recursion produces a loss of accuracy that builds up.  As well, to estimate $f^{(n)}(x_0)$, the function $f(x)$ is required to be computed $2^n$ times. Is $(2)$ the best method for approximating the $n^{th}$ derivative of $f(x_0)$ numerically or are there more efficient methods?",,"['numerical-methods', 'approximation', 'derivatives', 'estimation']"
54,how to compute a differential?,how to compute a differential?,,"Let $$l:\text{Imm}(S^{1},\mathbb{R^2}) \to \mathbb{R}$$ be $$l(c)=\int_{S^1}{|c_{\theta}|d\theta}$$ and its diffrential is $$dl(c)(h)=\int_{S^1}{\frac{<h_{\theta},c_{\theta}>}{|c_{\theta}|}}d\theta$$ Can you please, explain how this differential is computed?","Let be and its diffrential is Can you please, explain how this differential is computed?","l:\text{Imm}(S^{1},\mathbb{R^2}) \to \mathbb{R} l(c)=\int_{S^1}{|c_{\theta}|d\theta} dl(c)(h)=\int_{S^1}{\frac{<h_{\theta},c_{\theta}>}{|c_{\theta}|}}d\theta","['derivatives', 'differential-geometry', 'differential']"
55,Conditions of the Taylor Theorem,Conditions of the Taylor Theorem,,I'm confused on the assumptions behind the Taylor Theorem because I found different versions of them across several books. Consider the function $f:\mathbb{R}\rightarrow \mathbb{R}$ (1) If and only if $f$ is infinitely many times differentiable at $a$ I can write $$f(x)=f(a)+\sum_{k=1}^{\infty}\frac{f^{(k)}(a)(x-a)^k}{k!}$$  Correct? (2) If and only if $f$ is $n$ times continuously differentiable at $a$ (which implies that $f$ is $n$ times differentiable in a neighbourhood of $a$) I can write $$ f(x)=f(a)+\sum_{k=1}^{n}\frac{f^{(k)}(a)(x-a)^k}{k!}+ o(||x-a||^n) $$ Correct? (3) If and only if $f$ is $n$ times continuously differentiable at each point between $x$ and $a$ I can write $$ f(x)=f(a)+\sum_{k=1}^{n}\frac{f^{(k)}(a)(x-a)^k}{k!}+ \frac{f^{(n+1)}(c)(x-a)^{n+1}}{(n+1)!} $$ for $c$ between $x$ ans $a$. Correct? My confusion is related in particular to the necessity of conditions.,I'm confused on the assumptions behind the Taylor Theorem because I found different versions of them across several books. Consider the function $f:\mathbb{R}\rightarrow \mathbb{R}$ (1) If and only if $f$ is infinitely many times differentiable at $a$ I can write $$f(x)=f(a)+\sum_{k=1}^{\infty}\frac{f^{(k)}(a)(x-a)^k}{k!}$$  Correct? (2) If and only if $f$ is $n$ times continuously differentiable at $a$ (which implies that $f$ is $n$ times differentiable in a neighbourhood of $a$) I can write $$ f(x)=f(a)+\sum_{k=1}^{n}\frac{f^{(k)}(a)(x-a)^k}{k!}+ o(||x-a||^n) $$ Correct? (3) If and only if $f$ is $n$ times continuously differentiable at each point between $x$ and $a$ I can write $$ f(x)=f(a)+\sum_{k=1}^{n}\frac{f^{(k)}(a)(x-a)^k}{k!}+ \frac{f^{(n+1)}(c)(x-a)^{n+1}}{(n+1)!} $$ for $c$ between $x$ ans $a$. Correct? My confusion is related in particular to the necessity of conditions.,,"['derivatives', 'taylor-expansion']"
56,Order of growth of derivatives at given x,Order of growth of derivatives at given x,,"Is there such an $f$ smooth function and $x\in D_f$, so that the sequence $f(x), f'(x), f''(x), ...$ grows faster than exponential? Can it grow at a factorial rate or faster?","Is there such an $f$ smooth function and $x\in D_f$, so that the sequence $f(x), f'(x), f''(x), ...$ grows faster than exponential? Can it grow at a factorial rate or faster?",,"['derivatives', 'asymptotics']"
57,Higher mixed partial derivatives of $e^{f(x)}$,Higher mixed partial derivatives of,e^{f(x)},"I have a function $g(x) = e^{-f(x)}$, $x = (x_1,x_2,...,x_n)$. Is there some compact and beautiful formula for the derivative $\frac{\partial^{|\alpha|}}{\partial x_1^{\alpha_1}...\partial x_n^{\alpha_n}}g(x)?$ Maybe in the case of $n=2$?","I have a function $g(x) = e^{-f(x)}$, $x = (x_1,x_2,...,x_n)$. Is there some compact and beautiful formula for the derivative $\frac{\partial^{|\alpha|}}{\partial x_1^{\alpha_1}...\partial x_n^{\alpha_n}}g(x)?$ Maybe in the case of $n=2$?",,['derivatives']
58,"Prove that $h(u)=\frac{2}{u^2}(u-\log(1+u))$ is decreasing on $(-1,\infty)$",Prove that  is decreasing on,"h(u)=\frac{2}{u^2}(u-\log(1+u)) (-1,\infty)","Prove that $h(u)=\frac{2}{u^2}(u-\log(1+u))$ is decreasing on $(-1,\infty)- \{0\}$ This function appears on the proof of Stirling's formula in Principles of Mathematical Analysis, by Walter Rudin. The classic way to approach the ""show a function is decreasing"" is to show that its derivative is negative on the given range. $$h'(u)= -\frac{2}{u^2}+\frac{4}{u^3}\log(1+u)-\frac{2}{u^2(1+u)}$$ And after we factor out the $-\frac{2}{u^2}$ we are left with showing that $$g(u):= 1-\frac{2}{u}\log(1+u)+\frac{1}{1+u}$$ is positive. We could try showing that $g$ has positive derivative and show that the limit of $g$ as $u$ approaches $-1$ is positive, but this seems like rhe derivative of $g$ will be something much more complicated.","Prove that is decreasing on This function appears on the proof of Stirling's formula in Principles of Mathematical Analysis, by Walter Rudin. The classic way to approach the ""show a function is decreasing"" is to show that its derivative is negative on the given range. And after we factor out the we are left with showing that is positive. We could try showing that has positive derivative and show that the limit of as approaches is positive, but this seems like rhe derivative of will be something much more complicated.","h(u)=\frac{2}{u^2}(u-\log(1+u)) (-1,\infty)- \{0\} h'(u)= -\frac{2}{u^2}+\frac{4}{u^3}\log(1+u)-\frac{2}{u^2(1+u)} -\frac{2}{u^2} g(u):= 1-\frac{2}{u}\log(1+u)+\frac{1}{1+u} g g u -1 g","['derivatives', 'monotone-functions']"
59,Prove this stronger inequality with $\frac{e^x}{x+1}-\frac{x-1}{\ln{x}}-\left(\frac{e-2}{2}\right)>0$,Prove this stronger inequality with,\frac{e^x}{x+1}-\frac{x-1}{\ln{x}}-\left(\frac{e-2}{2}\right)>0,"Let $x>1$ show that $$\dfrac{e^x}{x+1}-\dfrac{x-1}{\ln{x}}-\left(\dfrac{e-2}{2}\right)>0$$ It seem this inequality can use derivatives solve it,But it is ugly.can you  help？ $$\lim_{x\to 1}\left(\dfrac{e^x}{x+1}-\dfrac{x-1}{\ln{x}}-\left(\dfrac{e-2}{2}\right)\right)=0$$ and $f(x)=\dfrac{e^x}{x+1},g(x)=\dfrac{x-1}{\ln{x}}$,But $$f'(x)=\dfrac{xe^x}{(x+1)^2}>0, g'(x)=\dfrac{\ln{x}-\dfrac{x-1}{x}}{\ln^2{x}}>0$$ Unfortunately I don't know any nice expression for this 1-th derivative which could help. I'll be grateful for all useful suggestions.","Let $x>1$ show that $$\dfrac{e^x}{x+1}-\dfrac{x-1}{\ln{x}}-\left(\dfrac{e-2}{2}\right)>0$$ It seem this inequality can use derivatives solve it,But it is ugly.can you  help？ $$\lim_{x\to 1}\left(\dfrac{e^x}{x+1}-\dfrac{x-1}{\ln{x}}-\left(\dfrac{e-2}{2}\right)\right)=0$$ and $f(x)=\dfrac{e^x}{x+1},g(x)=\dfrac{x-1}{\ln{x}}$,But $$f'(x)=\dfrac{xe^x}{(x+1)^2}>0, g'(x)=\dfrac{\ln{x}-\dfrac{x-1}{x}}{\ln^2{x}}>0$$ Unfortunately I don't know any nice expression for this 1-th derivative which could help. I'll be grateful for all useful suggestions.",,"['derivatives', 'inequality']"
60,Do rational and irrational numbers flip-flop?,Do rational and irrational numbers flip-flop?,,"I have found out that between every 2 rational numbers there is an irrational number, and between every 2 irrational numbers, there is a rational number. Does this mean that the rational and irrational numbers are laid out in an alternating pattern along the real line?","I have found out that between every 2 rational numbers there is an irrational number, and between every 2 irrational numbers, there is a rational number. Does this mean that the rational and irrational numbers are laid out in an alternating pattern along the real line?",,"['derivatives', 'irrational-numbers', 'rational-numbers']"
61,Hyperbolic Functions (derivative of $\tanh x$),Hyperbolic Functions (derivative of ),\tanh x,$$\sinh(x) = \frac{1}{2(e^x - e^{-x})}$$ $$\cosh(x) = \frac{1}{2(e^x + e^{-x}}$$ $$\tanh(x) = \frac{\sinh (x)}{\cosh (x)}$$ Prove: $$\frac{d(\tanh(x))}{dx} = \frac{1}{(\cosh x)^2}$$ I got the derivative for $\tanh(x)$ as: $$\left[ \frac{1}{2(e^x + e^{-x})}\right]^2 - \frac{{[ \frac{1}{2(e^x + e^{-x})}]^2}}{[ \frac{1}{2(e^x + e^{-x})}]^2}$$,$$\sinh(x) = \frac{1}{2(e^x - e^{-x})}$$ $$\cosh(x) = \frac{1}{2(e^x + e^{-x}}$$ $$\tanh(x) = \frac{\sinh (x)}{\cosh (x)}$$ Prove: $$\frac{d(\tanh(x))}{dx} = \frac{1}{(\cosh x)^2}$$ I got the derivative for $\tanh(x)$ as: $$\left[ \frac{1}{2(e^x + e^{-x})}\right]^2 - \frac{{[ \frac{1}{2(e^x + e^{-x})}]^2}}{[ \frac{1}{2(e^x + e^{-x})}]^2}$$,,['derivatives']
62,How to prove Leibniz rule for exterior derivative using abstract index notation,How to prove Leibniz rule for exterior derivative using abstract index notation,,"I want to prove Leibniz rule for exterior derivative of wedge product using abstract index notation : For $\omega\in \Omega^k(U),\eta\in\Omega^l(U)$ , d $(\omega\wedge\eta)=\text{d}\omega\wedge\eta +(-1)^k\omega\wedge\text{d}\eta$ . My proof is given in the answer below.","I want to prove Leibniz rule for exterior derivative of wedge product using abstract index notation : For , d . My proof is given in the answer below.","\omega\in \Omega^k(U),\eta\in\Omega^l(U) (\omega\wedge\eta)=\text{d}\omega\wedge\eta +(-1)^k\omega\wedge\text{d}\eta","['derivatives', 'differential-geometry', 'solution-verification', 'exterior-algebra', 'index-notation']"
63,Solid angle relation between sinθ dϕdθ and d(cos(θ))dϕ,Solid angle relation between sinθ dϕdθ and d(cos(θ))dϕ,,"I am a bit confused with regards to the concept of solid angle. Why is the solid angle which is defined as $\sin \theta {\rm d}\phi\, {d\rm }\theta$ equal to $\sin\theta\,{\rm d}\theta {\rm d}\phi = {\rm d}\cos\theta{\rm d}\phi$","I am a bit confused with regards to the concept of solid angle. Why is the solid angle which is defined as $\sin \theta {\rm d}\phi\, {d\rm }\theta$ equal to $\sin\theta\,{\rm d}\theta {\rm d}\phi = {\rm d}\cos\theta{\rm d}\phi$",,"['derivatives', 'spherical-coordinates', 'spherical-geometry', 'solid-angle']"
64,Partial derivative definition,Partial derivative definition,,"What is the partial derivative of $$\frac{\partial x}{\partial y}$$ when $x$ and $y$ are a part of a function $f(x,y)$? Using an example of: $$f(x,y) = x+y$$ Given the definition of holding all else constant and varying x with respect to y (thus setting the function to zero), would it be corrrect to say that the partial derivative is simply negative one? One motivation for this is the formula (note that this is just re-arranged formula for total derivative or differential): $$\frac{dy}{dx} = -\frac {\partial F /\partial x}{\partial F / \partial y} +\frac{dF }{dx }\frac{\partial y}{\partial F}$$ And whether the Fs in the 2nd term can in fact be factored out. If that is the case, then: $$\frac{\partial x}{\partial y} = \frac {\partial F /\partial x}{\partial F / \partial y} =\frac{dF }{dx }\frac{\partial y}{\partial F}-\frac{dy}{dx} $$","What is the partial derivative of $$\frac{\partial x}{\partial y}$$ when $x$ and $y$ are a part of a function $f(x,y)$? Using an example of: $$f(x,y) = x+y$$ Given the definition of holding all else constant and varying x with respect to y (thus setting the function to zero), would it be corrrect to say that the partial derivative is simply negative one? One motivation for this is the formula (note that this is just re-arranged formula for total derivative or differential): $$\frac{dy}{dx} = -\frac {\partial F /\partial x}{\partial F / \partial y} +\frac{dF }{dx }\frac{\partial y}{\partial F}$$ And whether the Fs in the 2nd term can in fact be factored out. If that is the case, then: $$\frac{\partial x}{\partial y} = \frac {\partial F /\partial x}{\partial F / \partial y} =\frac{dF }{dx }\frac{\partial y}{\partial F}-\frac{dy}{dx} $$",,['derivatives']
65,"Find the derivative of $f$ if it exists, else, prove it doesn't exist","Find the derivative of  if it exists, else, prove it doesn't exist",f,"Let $f:  \mathbb R^+ \to  \mathbb R $ $$f=\mathop{\vcenter{\LARGE\mathrm K}}\limits_{j=1}^{+\infty}\frac{x}{x^j}=\cfrac{x}{x^1+\cfrac{x}{x^2+\cfrac{x}{x^3+\ddots}}}$$ I saw this problem in a math challenges magazine, I have no idea how this would be solved but thought it seemed really interesting.","Let $f:  \mathbb R^+ \to  \mathbb R $ $$f=\mathop{\vcenter{\LARGE\mathrm K}}\limits_{j=1}^{+\infty}\frac{x}{x^j}=\cfrac{x}{x^1+\cfrac{x}{x^2+\cfrac{x}{x^3+\ddots}}}$$ I saw this problem in a math challenges magazine, I have no idea how this would be solved but thought it seemed really interesting.",,"['derivatives', 'continued-fractions']"
66,Gradient of a summation,Gradient of a summation,,How to calculate the gradient of the following summation in terms of $x_i$ ? $$\sum_{i=0}^n(x_i-a)^2$$ is the following answer true? $$2 \sum_{i=0}^n(x_i-a)$$ Thank you,How to calculate the gradient of the following summation in terms of $x_i$ ? $$\sum_{i=0}^n(x_i-a)^2$$ is the following answer true? $$2 \sum_{i=0}^n(x_i-a)$$ Thank you,,"['derivatives', 'summation']"
67,Trace and Lie derivative of a tensor commute,Trace and Lie derivative of a tensor commute,,"How can we show that trace and Lie derivative of a $(1,1)$-tensor commute? That is, I want to prove that $$ \operatorname{tr}(L_X S) = L_X \operatorname{tr}S, $$ where $S$ is a $(1,1)$-tensor. This is Exercise 2.5.10 of Riemannian Geometry (Third Edition), by Peter Petersen . My idea is to consider the definition of a trace of a linear operator $L : V \to V$ (where $V$ is a vector space) given by $$ \operatorname{tr}L = \sum_{i=1}^n g(L(e_i),e_i), $$ where the $e_i$'s form an orthonormal basis. Also, Exercise 2.5.9 (the exercise preceding this one) asks to demonstrate that $\operatorname{tr} (\nabla_X S) = \nabla_X \operatorname{tr}S$, and perhaps this may be used in the proof of the current problem? I also wanted to type change the $(1,1)$-tensor $S$ into a $(0,2)$-tensor $\hat S$ via the fomula $\hat S(X,Y) = g(S(X),Y)$, so that I can use Proposition 2.1.2 of Petersen: If $X$ is a vector field and $T$ a $(0,k)$-tensor on a manifold $M$, then   $$ (L_X T)(Y_1,\ldots,Y_k)=D_X(T(Y_1,\ldots,Y_k))-\sum_{i=1}^k T(Y_1,\ldots,L_X Y_i,\ldots,Y_k). $$ Can I request any thoughts on this matter?","How can we show that trace and Lie derivative of a $(1,1)$-tensor commute? That is, I want to prove that $$ \operatorname{tr}(L_X S) = L_X \operatorname{tr}S, $$ where $S$ is a $(1,1)$-tensor. This is Exercise 2.5.10 of Riemannian Geometry (Third Edition), by Peter Petersen . My idea is to consider the definition of a trace of a linear operator $L : V \to V$ (where $V$ is a vector space) given by $$ \operatorname{tr}L = \sum_{i=1}^n g(L(e_i),e_i), $$ where the $e_i$'s form an orthonormal basis. Also, Exercise 2.5.9 (the exercise preceding this one) asks to demonstrate that $\operatorname{tr} (\nabla_X S) = \nabla_X \operatorname{tr}S$, and perhaps this may be used in the proof of the current problem? I also wanted to type change the $(1,1)$-tensor $S$ into a $(0,2)$-tensor $\hat S$ via the fomula $\hat S(X,Y) = g(S(X),Y)$, so that I can use Proposition 2.1.2 of Petersen: If $X$ is a vector field and $T$ a $(0,k)$-tensor on a manifold $M$, then   $$ (L_X T)(Y_1,\ldots,Y_k)=D_X(T(Y_1,\ldots,Y_k))-\sum_{i=1}^k T(Y_1,\ldots,L_X Y_i,\ldots,Y_k). $$ Can I request any thoughts on this matter?",,"['derivatives', 'manifolds', 'tensors', 'trace', 'lie-derivative']"
68,Intuition on second order partial derivatives,Intuition on second order partial derivatives,,"Inspired by smooth submanifolds of $\mathbb{R}^n$, I am looking for a good geometric way to think of second order partial derivatives of a locally smooth function $f:\mathbb{R}^n \rightarrow \mathbb{R}$. To clarify, I do not want to think of the first order partial derivatives as stand-alone functions $\mathbb{R}^n \rightarrow \mathbb{R}$, of which we then again take first-order derivatives, but I would like to see the connection to the original function. Thanks.","Inspired by smooth submanifolds of $\mathbb{R}^n$, I am looking for a good geometric way to think of second order partial derivatives of a locally smooth function $f:\mathbb{R}^n \rightarrow \mathbb{R}$. To clarify, I do not want to think of the first order partial derivatives as stand-alone functions $\mathbb{R}^n \rightarrow \mathbb{R}$, of which we then again take first-order derivatives, but I would like to see the connection to the original function. Thanks.",,"['derivatives', 'smooth-manifolds']"
69,Compute the differential of a smooth map,Compute the differential of a smooth map,,"Let $S\subseteq \mathbb{R}^3$ be an oriented regular surface and let $N$ be a field of normal unitary vector on $S$. We consider the map $F:S\times \mathbb{R}\rightarrow \mathbb{R}^3$ defined by $F(p,t):= p+tN_p$ and we want to calculate $DF_{(p,t)}:T_pS\times\mathbb{R}\rightarrow\mathbb{R}^3$. Can someone show me how to do this calculation?","Let $S\subseteq \mathbb{R}^3$ be an oriented regular surface and let $N$ be a field of normal unitary vector on $S$. We consider the map $F:S\times \mathbb{R}\rightarrow \mathbb{R}^3$ defined by $F(p,t):= p+tN_p$ and we want to calculate $DF_{(p,t)}:T_pS\times\mathbb{R}\rightarrow\mathbb{R}^3$. Can someone show me how to do this calculation?",,"['differential-geometry', 'derivatives', 'surfaces']"
70,Find the inequality with the best possible $k= constant$ (with the condition $x^{2}+ y^{2}\leq k$).,Find the inequality with the best possible  (with the condition ).,k= constant x^{2}+ y^{2}\leq k,"Find the inequality with the best possible $constant$ Given two non-negative numbers $x, y$ so that $x^{2}+ y^{2}\leq \frac{2}{7}$ . Prove that $$\frac{1}{1+ x^{2}}+ \frac{1}{1+ y^{2}}+ \frac{1}{1+ xy}\leq \frac{3}{1+ \left ( \frac{x+ y}{2} \right )^{2}}$$ where $constant= \frac{2}{7}$ is the best possible. Given two non-negative numbers $x, y$ so that $x^{2}+ y^{2}\leq \frac{2}{5}$ . Prove that $$\frac{1}{\sqrt{1+ x^{2}}}+ \frac{1}{\sqrt{1+ y^{2}}}+ \frac{1}{\sqrt{1+ xy}}\leq \frac{3}{\sqrt{1+ \left ( \frac{x+ y}{2} \right )^{2}}}$$ where $constant= \frac{2}{5}$ is the best possible. They are my two examples. I'm looking forward to seeing more many inequalities alike. Thanks for all your nice comments.",Find the inequality with the best possible Given two non-negative numbers so that . Prove that where is the best possible. Given two non-negative numbers so that . Prove that where is the best possible. They are my two examples. I'm looking forward to seeing more many inequalities alike. Thanks for all your nice comments.,"constant x, y x^{2}+ y^{2}\leq \frac{2}{7} \frac{1}{1+ x^{2}}+ \frac{1}{1+ y^{2}}+ \frac{1}{1+ xy}\leq \frac{3}{1+ \left ( \frac{x+ y}{2} \right )^{2}} constant= \frac{2}{7} x, y x^{2}+ y^{2}\leq \frac{2}{5} \frac{1}{\sqrt{1+ x^{2}}}+ \frac{1}{\sqrt{1+ y^{2}}}+ \frac{1}{\sqrt{1+ xy}}\leq \frac{3}{\sqrt{1+ \left ( \frac{x+ y}{2} \right )^{2}}} constant= \frac{2}{5}","['derivatives', 'inequality']"
71,Can we reparametrise a closed curve such that its derivative looks like the original curve?,Can we reparametrise a closed curve such that its derivative looks like the original curve?,,"When playing around with closed planar curves centered at the origin such as ellipses and circles in ""standard"" parametrisation (i.e. $(a \cos(t), b \sin(t))$ and period $2 \pi$ ) I noticed that they are their own derivatives. So I asked myself for which other closed curves this holds. For a curve like $(c, 2 \pi)$ , where $c(t) := (\cos(t), \sin(2t))$ , the derivative $c'$ obviously does not coincide with $c$ . But can we reparametrise $c$ (in a way that is its speed towards this bumps is slowed down) such that the derivative coincides with $c$ ? Another example: The derivative of unit circle in standard parametrisation coincides with the unit circle. But if we reparametrise the unit circle as $(\cos(t \cdot e^{t - 2 \pi}), \sin(t \cdot e^{t - 2 \pi}))$ with still period $2 \pi$ , the derivative doens't coincide. Another factor to consider is translated versions of curves. If a circle centered at the origin is translated to another position, its derivative will be centered at the origin. Thus my question is: Let $(c,p)$ be a closed planar curve.   Does there exists a reparametrisation of $c$ such that $c'$ and $c$ look the same modulo translation? Definition 1. A closed parametrised curve is a pair $(c, p)$ where $c: \mathbb R \to \mathbb R^n$ is parametrised curve with period $p$ , i.e. $c(t+p)=c(t)$ holds for all $t \in \mathbb R$ . Definition 2. A closed curve is an equivalence class of closed parametrised curves, where $(c,p) \sim (d,q)$ if and only if there exists a bijective smooth map $\phi: \mathbb R \to \mathbb R$ such that $d = c \circ \phi$ and $\phi'(t) > 0$ and $\phi(t + p) = \phi(t) + q$ hold for all for all $t \in \mathbb R$","When playing around with closed planar curves centered at the origin such as ellipses and circles in ""standard"" parametrisation (i.e. and period ) I noticed that they are their own derivatives. So I asked myself for which other closed curves this holds. For a curve like , where , the derivative obviously does not coincide with . But can we reparametrise (in a way that is its speed towards this bumps is slowed down) such that the derivative coincides with ? Another example: The derivative of unit circle in standard parametrisation coincides with the unit circle. But if we reparametrise the unit circle as with still period , the derivative doens't coincide. Another factor to consider is translated versions of curves. If a circle centered at the origin is translated to another position, its derivative will be centered at the origin. Thus my question is: Let be a closed planar curve.   Does there exists a reparametrisation of such that and look the same modulo translation? Definition 1. A closed parametrised curve is a pair where is parametrised curve with period , i.e. holds for all . Definition 2. A closed curve is an equivalence class of closed parametrised curves, where if and only if there exists a bijective smooth map such that and and hold for all for all","(a \cos(t), b \sin(t)) 2 \pi (c, 2 \pi) c(t) := (\cos(t), \sin(2t)) c' c c c (\cos(t \cdot e^{t - 2 \pi}), \sin(t \cdot e^{t - 2 \pi})) 2 \pi (c,p) c c' c (c, p) c: \mathbb R \to \mathbb R^n p c(t+p)=c(t) t \in \mathbb R (c,p) \sim (d,q) \phi: \mathbb R \to \mathbb R d = c \circ \phi \phi'(t) > 0 \phi(t + p) = \phi(t) + q t \in \mathbb R","['derivatives', 'differential-geometry', 'curves', 'plane-curves']"
72,An unexpected application of the Cauchy-Schwarz inequality for integrals,An unexpected application of the Cauchy-Schwarz inequality for integrals,,"I discovered this yesterday and I just want to know whether my solution is correct and whether there's a shortcut to it. Let $g(x)$ be a twice-differentiable continuous function that crosses the points $(0, t)$ and $(1, 1)$ and has stationary points there with $t\in(0,1)$. Then if  $$\text{sgn}\left(\int_0^1g(x)g''(x)\,dx\right)-\text{sgn}\left(\int_0^1\frac1{g(x)}\,dx\right)=\pm2$$ for some constant $K\in\mathbb{R}$, $$g(x)=Kg'(x)^2.$$ Furthermore, $g''(x)=\dfrac1{2K}$ when $x\in\mathbb{R}-\{0, 1\}$. Solution We use the Cauchy-Schwarz inequality for integrals with $a=0$ and $b=1$: $$\left[\int_0^1\frac{g'(x)}{\sqrt{g(x)}}\,dx\right]^2\le\left(\int_0^1g'(x)^2\,dx\right)\left(\int_0^1\frac1{g(x)}\,dx\right)\tag{1}$$ Firstly, consider the LHS. Let $u=g(x)\implies du=g'(x)\,dx$; thus LHS is $$\left[\int_0^1\frac{g'(x)}{\sqrt{g(x)}}\,dx\right]^2=\left[\int_{g(0)}^{g(1)}\frac1{\sqrt{u}}\,du\right]^2=\left(\left[2\sqrt{u}\right]_t^1\right)^2=4(1-\sqrt t)^2=T>0 \tag{2}$$ as $g(0)=t$ and $g(1)=1$. Now use integration by parts for the first integral on the RHS: $$\int_0^1g'(x)^2\,dx=\left[g(x)g'(x)\right]_0^1-\int_0^1g(x)g''(x)\,dx=-\int_0^1g(x)g''(x)\,dx\tag{3}$$ as $g'(0)=g'(1)=0$. Plugging $(2)$ and $(3)$ into $(1)$, we get $$-T\ge\left(\int_0^1g(x)g''(x)\,dx\right)\left(\int_0^1\frac1{g(x)}\,dx\right)\tag{4}$$ with equality holding if, and only if, $$\dfrac{g'(x)}{\sqrt{g(x)}}=k\implies g(x)=\frac1{k^2}g'(x)^2=Kg'(x)^2$$ where $k,K\in\mathbb{R}$ are constants. Differentiating this, we have $g'(x)=2Kg'(x)g''(x)$. We can divide by $g'(x)\neq0$ when $x\neq0, 1$ so for $x\in\mathbb{R}-\{0,1\}$, $$g''(x)=\frac1{2K}.$$ Now back to $(4)$. $$\int_0^1g(x)g''(x)\,dx>0\implies\int_0^1\frac1{g(x)}\,dx<0$$ and $$\int_0^1g(x)g''(x)\,dx<0\implies\int_0^1\frac1{g(x)}\,dx>0$$ This means that the sign of $\int_0^1g(x)g''(x)\,dx$ is always opposite that of $\int_0^1\frac1{g(x)}\,dx$. The result follows.","I discovered this yesterday and I just want to know whether my solution is correct and whether there's a shortcut to it. Let $g(x)$ be a twice-differentiable continuous function that crosses the points $(0, t)$ and $(1, 1)$ and has stationary points there with $t\in(0,1)$. Then if  $$\text{sgn}\left(\int_0^1g(x)g''(x)\,dx\right)-\text{sgn}\left(\int_0^1\frac1{g(x)}\,dx\right)=\pm2$$ for some constant $K\in\mathbb{R}$, $$g(x)=Kg'(x)^2.$$ Furthermore, $g''(x)=\dfrac1{2K}$ when $x\in\mathbb{R}-\{0, 1\}$. Solution We use the Cauchy-Schwarz inequality for integrals with $a=0$ and $b=1$: $$\left[\int_0^1\frac{g'(x)}{\sqrt{g(x)}}\,dx\right]^2\le\left(\int_0^1g'(x)^2\,dx\right)\left(\int_0^1\frac1{g(x)}\,dx\right)\tag{1}$$ Firstly, consider the LHS. Let $u=g(x)\implies du=g'(x)\,dx$; thus LHS is $$\left[\int_0^1\frac{g'(x)}{\sqrt{g(x)}}\,dx\right]^2=\left[\int_{g(0)}^{g(1)}\frac1{\sqrt{u}}\,du\right]^2=\left(\left[2\sqrt{u}\right]_t^1\right)^2=4(1-\sqrt t)^2=T>0 \tag{2}$$ as $g(0)=t$ and $g(1)=1$. Now use integration by parts for the first integral on the RHS: $$\int_0^1g'(x)^2\,dx=\left[g(x)g'(x)\right]_0^1-\int_0^1g(x)g''(x)\,dx=-\int_0^1g(x)g''(x)\,dx\tag{3}$$ as $g'(0)=g'(1)=0$. Plugging $(2)$ and $(3)$ into $(1)$, we get $$-T\ge\left(\int_0^1g(x)g''(x)\,dx\right)\left(\int_0^1\frac1{g(x)}\,dx\right)\tag{4}$$ with equality holding if, and only if, $$\dfrac{g'(x)}{\sqrt{g(x)}}=k\implies g(x)=\frac1{k^2}g'(x)^2=Kg'(x)^2$$ where $k,K\in\mathbb{R}$ are constants. Differentiating this, we have $g'(x)=2Kg'(x)g''(x)$. We can divide by $g'(x)\neq0$ when $x\neq0, 1$ so for $x\in\mathbb{R}-\{0,1\}$, $$g''(x)=\frac1{2K}.$$ Now back to $(4)$. $$\int_0^1g(x)g''(x)\,dx>0\implies\int_0^1\frac1{g(x)}\,dx<0$$ and $$\int_0^1g(x)g''(x)\,dx<0\implies\int_0^1\frac1{g(x)}\,dx>0$$ This means that the sign of $\int_0^1g(x)g''(x)\,dx$ is always opposite that of $\int_0^1\frac1{g(x)}\,dx$. The result follows.",,"['derivatives', 'definite-integrals', 'cauchy-schwarz-inequality', 'stationary-point']"
73,Is Cosine the only function satisfying $ f'(x)= f(x+\frac{\pi}{2})$?,Is Cosine the only function satisfying ?, f'(x)= f(x+\frac{\pi}{2}),"Basically most of us know that $\frac {\textrm{d}}{\textrm{d}x} \cos x = -\sin x$ . Also $ \cos (x+\frac{\pi}{2})=-\sin x$ That makes $$ \frac {\textrm{d}}{\textrm{d}x} \cos x = \cos (x+\frac{\pi}{2}) $$ So, out of curiosity I wondered what other functions could have this property. It's pretty interesting that by translating the graph of a function by some vector we get the graph of its derivative. Therefore, here's my attempt: I tried to find the defining property of this function. Here we go: $$ f'(x)= f(x+\frac{\pi}{2}) $$ That makes $$ f''(x)= \left( f'(x)\right)'= f(x+\frac{\pi}{2})'=f'(x+\frac{\pi}{2})=f(x+\pi)$$ Generally $$f^n(x)=f(x+n\frac{\pi}{2})$$ With $f^n(x) $ being the $n$-th derivative of $f$.  And I'm stuck at this step. What makes cosine adapt to this property is that $ \cos (x+\pi)=-\cos x$ which allows it to be the solution of the following differential equation: $$f''(x)+f(x)=0$$ Though I don't want to admit that $f(x+\pi)=-f(x) $ So, could you please help me? Isn't there any way to determine all the functions that satisfy the previously acknowledged property? Are they infinite?","Basically most of us know that $\frac {\textrm{d}}{\textrm{d}x} \cos x = -\sin x$ . Also $ \cos (x+\frac{\pi}{2})=-\sin x$ That makes $$ \frac {\textrm{d}}{\textrm{d}x} \cos x = \cos (x+\frac{\pi}{2}) $$ So, out of curiosity I wondered what other functions could have this property. It's pretty interesting that by translating the graph of a function by some vector we get the graph of its derivative. Therefore, here's my attempt: I tried to find the defining property of this function. Here we go: $$ f'(x)= f(x+\frac{\pi}{2}) $$ That makes $$ f''(x)= \left( f'(x)\right)'= f(x+\frac{\pi}{2})'=f'(x+\frac{\pi}{2})=f(x+\pi)$$ Generally $$f^n(x)=f(x+n\frac{\pi}{2})$$ With $f^n(x) $ being the $n$-th derivative of $f$.  And I'm stuck at this step. What makes cosine adapt to this property is that $ \cos (x+\pi)=-\cos x$ which allows it to be the solution of the following differential equation: $$f''(x)+f(x)=0$$ Though I don't want to admit that $f(x+\pi)=-f(x) $ So, could you please help me? Isn't there any way to determine all the functions that satisfy the previously acknowledged property? Are they infinite?",,['derivatives']
74,"How is an infinitesimal $dx$ different from $\Delta x\,$? [duplicate]",How is an infinitesimal  different from ? [duplicate],"dx \Delta x\,","This question already has answers here : Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? (27 answers) Closed 9 years ago . When I learned calc, I was always taught  $$\frac{df}{dx}= f'(x) = \lim_{h\rightarrow 0} \frac{f(x+h)-f(x)}{(x+h)-x}$$ But I have heard $dx$ is called an infinitesimal and I don't know what this means. In particular, I gather the validity of treating a ratio of differentials is a subtle issue and I'm not sure I get it.  Can someone explain the difference between $dx$ and $\Delta x$? EDIT: Here is a related thread: Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? I read that and this is what I don't understand: There is a way of getting around the logical difficulties with infinitesimals; this is called nonstandard analysis. It's pretty difficult to explain how one sets it up, but you can think of it as creating two classes of real numbers: the ones you are familiar with, that satisfy things like the Archimedean Property, the Supremum Property, and so on, and then you add another, separate class of real numbers that includes infinitesimals and a bunch of other things. Can someone explain what specifically are these two classes of real numbers and how they are different?","This question already has answers here : Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? (27 answers) Closed 9 years ago . When I learned calc, I was always taught  $$\frac{df}{dx}= f'(x) = \lim_{h\rightarrow 0} \frac{f(x+h)-f(x)}{(x+h)-x}$$ But I have heard $dx$ is called an infinitesimal and I don't know what this means. In particular, I gather the validity of treating a ratio of differentials is a subtle issue and I'm not sure I get it.  Can someone explain the difference between $dx$ and $\Delta x$? EDIT: Here is a related thread: Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? I read that and this is what I don't understand: There is a way of getting around the logical difficulties with infinitesimals; this is called nonstandard analysis. It's pretty difficult to explain how one sets it up, but you can think of it as creating two classes of real numbers: the ones you are familiar with, that satisfy things like the Archimedean Property, the Supremum Property, and so on, and then you add another, separate class of real numbers that includes infinitesimals and a bunch of other things. Can someone explain what specifically are these two classes of real numbers and how they are different?",,"['derivatives', 'definition', 'nonstandard-analysis', 'infinitesimals']"
75,"Are there ""differentiable manifolds"" that don't admit a $C^1$-structure","Are there ""differentiable manifolds"" that don't admit a -structure",C^1,"It is well known that every $C^1$ manifold admits a smooth manifold structure. What if we relax the definition of smooth manifold so the transition maps need only be differentiable? Does every such ""differentiable manifold"" admit a compatible $C^1$ atlas?","It is well known that every manifold admits a smooth manifold structure. What if we relax the definition of smooth manifold so the transition maps need only be differentiable? Does every such ""differentiable manifold"" admit a compatible atlas?",C^1 C^1,"['derivatives', 'manifolds', 'smooth-manifolds']"
76,How to express the covariant derivative on $TM$ by the covariant derivative on $M\times M$?,How to express the covariant derivative on  by the covariant derivative on ?,TM M\times M,"Let $(M,g,\Gamma)$ be a Riemannian manifold with the Levi-Civita connection. $M\times M$ and $TM$ have natural metric structrures inherited from $(M,g,\Gamma)$ . Let $\phi:TM \rightarrow M \times M$ be defined as follows: $$ \phi(z,u) = \big(\exp_z(u),\exp_z(-u)\big)$$ where $\exp_z$ is the exponential map $\exp_z : T_zM \rightarrow M $ . At least in some neighbourhood of the section $u=0$ it is invertible. Let $F$ be a tensor field on $M\times M$ . Let $G$ be a tensor field on $TM$ defined as $$ G = \phi^*F$$ where $\phi^*$ is the pullback/pushfoward of a tensor field (at least in the domain where $\phi$ is invertible). Let $ v \in T_{(z,u)}TM$ . My question is: how to write the covariant derivative $\nabla_{v} G$ in terms of the covariant derivative of $F$ ? My first thought was to just write $$ \nabla_{v} G = \phi^*(\nabla_{d\phi(v)} F) $$ but since $\phi$ doesn't seem to be a homomorphism of the metric structures on $TM$ and $M\times M$ , the covariant derivative on $TM$ is not a pullback of the covariant derivative on $M\times M$ , so I don't think that's the correct formula. Still, the metric structures on $TM$ and $M\times M$ both originate from the same structure on $M$ , so I think there must be some relation. EDIT: I was able to conclude that $$ \nabla_{v} G - \phi^*(\nabla_{d\phi(v)} F) = -\frac{d}{dt}\Big|_{t=0} \Big( (\phi^*\tilde P^{\phi(\gamma)}_t) (P^\gamma_t)^{-1} G\Big) $$ where $\gamma$ is any curve on $TM$ such that $\gamma'(0) = v$ , $P^\gamma_t$ is the parallel transport on $TM$ along $\gamma$ from point $\gamma(t)$ to point $\gamma(0)$ , and $\tilde P^{\phi(\gamma)}_t$ is the parallel transport on $M\times M$ along the curve $\phi(\gamma)$ from point $\phi(\gamma(t))$ to $\phi(\gamma(0))$ . I still don't know how to calculate this derivative $\frac{d}{dt}$ and express it in terms of, for example, the Riemann tensor, or at least the Synge's function.","Let be a Riemannian manifold with the Levi-Civita connection. and have natural metric structrures inherited from . Let be defined as follows: where is the exponential map . At least in some neighbourhood of the section it is invertible. Let be a tensor field on . Let be a tensor field on defined as where is the pullback/pushfoward of a tensor field (at least in the domain where is invertible). Let . My question is: how to write the covariant derivative in terms of the covariant derivative of ? My first thought was to just write but since doesn't seem to be a homomorphism of the metric structures on and , the covariant derivative on is not a pullback of the covariant derivative on , so I don't think that's the correct formula. Still, the metric structures on and both originate from the same structure on , so I think there must be some relation. EDIT: I was able to conclude that where is any curve on such that , is the parallel transport on along from point to point , and is the parallel transport on along the curve from point to . I still don't know how to calculate this derivative and express it in terms of, for example, the Riemann tensor, or at least the Synge's function.","(M,g,\Gamma) M\times M TM (M,g,\Gamma) \phi:TM \rightarrow M \times M  \phi(z,u) = \big(\exp_z(u),\exp_z(-u)\big) \exp_z \exp_z : T_zM \rightarrow M  u=0 F M\times M G TM  G = \phi^*F \phi^* \phi  v \in T_{(z,u)}TM \nabla_{v} G F  \nabla_{v} G = \phi^*(\nabla_{d\phi(v)} F)  \phi TM M\times M TM M\times M TM M\times M M  \nabla_{v} G - \phi^*(\nabla_{d\phi(v)} F) = -\frac{d}{dt}\Big|_{t=0} \Big( (\phi^*\tilde P^{\phi(\gamma)}_t) (P^\gamma_t)^{-1} G\Big)  \gamma TM \gamma'(0) = v P^\gamma_t TM \gamma \gamma(t) \gamma(0) \tilde P^{\phi(\gamma)}_t M\times M \phi(\gamma) \phi(\gamma(t)) \phi(\gamma(0)) \frac{d}{dt}","['derivatives', 'riemannian-geometry', 'tensors', 'connections', 'tangent-bundle']"
77,How is the Jacobian matrix computed in finite difference problems?,How is the Jacobian matrix computed in finite difference problems?,,"I have come across many papers which reference the Jacobian when solving certain finite difference inverse problems. And I have seen many articles and textbooks which discuss the mathematical properties of the Jacobian in an abstract sense. I have also seen examples for calculating the Jacobian when the functions are known and analytic. But, amidst all this, I still have no idea how to sit down at my computer and compute a Jacobian for real data and non-analytic functions. For example, I have some predicted data vector, $\mathbf{f}$ and some model vector, $\mathbf{m}$ . The Jacobian is defined as: $$\mathbf{J}_{ij} = \frac{\partial f_i}{\partial m_j} $$ But how can this actually be computed? Note, I have come across many answers that say that you can compute them using finite differences (for example, see this answer here ). In this case, we can say: $$\mathbf{F}(\mathbf{m}) = f_1(\mathbf{m})\mathbf{i}+f_2(\mathbf{m})\mathbf{j}+f_3(\mathbf{m})\mathbf{k}+...$$ and so the Jacobian is defined via finite difference with some model perturbation: $$\mathbf{J}_{ij} = \frac{f_i(m_j+\Delta m)-f_i(m_j)}{\Delta m}$$ However, this method requires me to solve $\mathbf{F}(\mathbf{m})$ for every predicted function for every model perturbation. My particular problem involves thousands of predicted points and millions of model parameters and each function evaluation takes minutes to hours. It is not feasible to use the method suggested. Any explanation or resources are appreciated.","I have come across many papers which reference the Jacobian when solving certain finite difference inverse problems. And I have seen many articles and textbooks which discuss the mathematical properties of the Jacobian in an abstract sense. I have also seen examples for calculating the Jacobian when the functions are known and analytic. But, amidst all this, I still have no idea how to sit down at my computer and compute a Jacobian for real data and non-analytic functions. For example, I have some predicted data vector, and some model vector, . The Jacobian is defined as: But how can this actually be computed? Note, I have come across many answers that say that you can compute them using finite differences (for example, see this answer here ). In this case, we can say: and so the Jacobian is defined via finite difference with some model perturbation: However, this method requires me to solve for every predicted function for every model perturbation. My particular problem involves thousands of predicted points and millions of model parameters and each function evaluation takes minutes to hours. It is not feasible to use the method suggested. Any explanation or resources are appreciated.",\mathbf{f} \mathbf{m} \mathbf{J}_{ij} = \frac{\partial f_i}{\partial m_j}  \mathbf{F}(\mathbf{m}) = f_1(\mathbf{m})\mathbf{i}+f_2(\mathbf{m})\mathbf{j}+f_3(\mathbf{m})\mathbf{k}+... \mathbf{J}_{ij} = \frac{f_i(m_j+\Delta m)-f_i(m_j)}{\Delta m} \mathbf{F}(\mathbf{m}),"['derivatives', 'partial-differential-equations', 'mathematical-modeling', 'jacobian', 'inverse-problems']"
78,"Proving that $q(A,S)>0$ if $A>0$, $S<0$ and $p_i(A,S)>0$","Proving that  if ,  and","q(A,S)>0 A>0 S<0 p_i(A,S)>0","Consider the polynomials $$p_1=3(56-56A+A^2)-112(-6+A)S-14(-36+A)S^2+112S^3+7S^4,$$ $$p_2=-112(-6+A)-28(-36+A)S+336S^2+28S^3,$$ $$p_3=-28(-36+A)+672S+84S^2,$$ $$p_4=672+168S,$$ $$p_5=168,$$ $$q=-168S-252S^2-84S^3-7S^4+84A+84AS-3A^2+14AS^2.$$ I want to prove that if $A>0$ , $S<0$ and $p_i>0$ then $q>0$ . I have observed that $\frac{\partial p_i}{\partial S}(A,S)=p_{i+1}(A,S)$ and $q=-p_1+p_2-p_3+p_4-p_5$ , so we can express $q$ as $$q=\sum_{k=0}^4(-1)^{k+1}\frac{\partial^k p_1}{\partial S^k},$$ but I don't know if it can be used to prove the above statement.","Consider the polynomials I want to prove that if , and then . I have observed that and , so we can express as but I don't know if it can be used to prove the above statement.","p_1=3(56-56A+A^2)-112(-6+A)S-14(-36+A)S^2+112S^3+7S^4, p_2=-112(-6+A)-28(-36+A)S+336S^2+28S^3, p_3=-28(-36+A)+672S+84S^2, p_4=672+168S, p_5=168, q=-168S-252S^2-84S^3-7S^4+84A+84AS-3A^2+14AS^2. A>0 S<0 p_i>0 q>0 \frac{\partial p_i}{\partial S}(A,S)=p_{i+1}(A,S) q=-p_1+p_2-p_3+p_4-p_5 q q=\sum_{k=0}^4(-1)^{k+1}\frac{\partial^k p_1}{\partial S^k},","['derivatives', 'inequality', 'polynomials']"
79,"Geometric Interpretation of the ""Half Derivative""","Geometric Interpretation of the ""Half Derivative""",,"I've been doing a little bit of research into fractional calculus involving fractional derivatives, and I was wondering what the geometric interpretations of such derivatives would be. As we know, the first derivative can represent slope or rate of change, and the second derivative can be used to investigate convexity or concavity. What about fractional derivatives, like the half-derivative? Is there any geometric interpretation or practical application for such a derivative? Furthermore, finding such a derivative seems to be quite elusive. It is often tempting to find a ""multiple derivative formula"" such as $$\frac{d^n}{dx^n} \ln (x)=\frac{(-1)^{n-1}(n-1)!}{x^n}$$ and then evaluate it at fractional values, but this does not work. Because such a formula can be proven only by induction, evaluating it at fractional values is not justified. Is there any way to find a fractional derivative without a formula proven by induction, or does a fractional derivative just have to be defined separately? TLDR: What is the geometric interpretation of a fractional derivative? What are some applications? How do I find a fractional derivative?","I've been doing a little bit of research into fractional calculus involving fractional derivatives, and I was wondering what the geometric interpretations of such derivatives would be. As we know, the first derivative can represent slope or rate of change, and the second derivative can be used to investigate convexity or concavity. What about fractional derivatives, like the half-derivative? Is there any geometric interpretation or practical application for such a derivative? Furthermore, finding such a derivative seems to be quite elusive. It is often tempting to find a ""multiple derivative formula"" such as $$\frac{d^n}{dx^n} \ln (x)=\frac{(-1)^{n-1}(n-1)!}{x^n}$$ and then evaluate it at fractional values, but this does not work. Because such a formula can be proven only by induction, evaluating it at fractional values is not justified. Is there any way to find a fractional derivative without a formula proven by induction, or does a fractional derivative just have to be defined separately? TLDR: What is the geometric interpretation of a fractional derivative? What are some applications? How do I find a fractional derivative?",,"['derivatives', 'fractional-calculus']"
80,Proof verification - A polynomial $P(x)$ has only real roots $\implies$ $P'(x)$ also has only real roots,Proof verification - A polynomial  has only real roots   also has only real roots,P(x) \implies P'(x),"Here's a problem that I've solved but I'm not very confident on my solution. Please check it there's any gap in my arguments. Also, is there a way to come up with a shorter proof ? Thank you. The Problem : If the polynomial $P(x)=a_nx^n+a_{n-1}x^{n-1}+\ldots+a_0,~a_n \neq 0,$ $(a_j \in \mathbb{R} ~\text{ for }~0 \leq j \leq n)$ has only real roots, then it's derivative $P'(x)$ has only real roots. My Solution : We know that, a polynomial of degree $n$ has exactly $n$ roots. Let the roots be denoted by $\alpha_1, \alpha_2,\ldots,\alpha_n$. Case-I : Let all the roots be distinct . WLOG let $\alpha_1<\alpha_2<\ldots<\alpha_n$. Then, by Rolle's theorem $\forall~ 0 \leq j \leq n, \exists~ c_j \in (\alpha_j,\alpha_{j+1})$ such that $P'(c_j)=0$. Note that we're free to use Rolles's theorem as $P$ is differentiable $($and hence also continuous $)$ throughout $\mathbb{R}$. Hence we have $n-1$ real roots of $P'(x)$ namely $c_1,c_2,\ldots,c_{n-1}$. Since $P'(x)$ is a polynomial of degree $n-1,$ and hence has exactly $($though at most suffices, in this case$) ~n-1$ roots. Hence all roots of $P'(x)$ are real. So this case was quite trivial (in a relative sense). Case-II : All the roots are NOT distinct . Suppose we have $m~(<n)$ distinct roots $\beta_1, \beta_2,\ldots,\beta_m$ with respective multiplicities $k_1,k_2,\ldots,k_m$. Clearly, $\sum_{j=1}^m k_j=n$. Claim : If $\beta$ is a root of $P(x)$ with multiplicity $k~(>1),$ then $\beta$ is a root of $P'(x)$ with multiplicity $k-1$. Proof of the claim : $\beta$ is a root of $P(x)$ with multiplicity $k~(>1)$ $\implies$ $P(x)=(x-\beta)^k Q(x)$ where $Q(x),$ is a polynomial of degree $n-k$ and $Q(\beta) \neq 0$. Then $P'(x)=k(x-\beta)^{k-1}Q(x)+(x-\beta)^kQ'(x)=(x-\beta)^{k-1}\{kQ(x)+(x-\beta)Q'(x)\}$ Hence $\beta$ is a root of $P'(x)$ with $k-1$ multiplicity $($Since $kQ(\beta)+(\beta-\beta)Q'(\beta)=kQ(\beta) \neq 0)$. End of Proof of the claim. Using the claim, for all $0 \leq j \leq m, \beta_j$ is a root of $P'(x)$ with multiplicity $k_j-1$. Like in Case-I , we use Rolle's theorem again to come up with $c_j \in (\beta_j,\beta_{j+1})$ such that $P(c_j)=0, ~1 \leq j \leq m-1$. So in total, as roots of $P'(x),$ we have $c_1,c_2,\ldots,c_{m-1}$ each with multiplicity $1,$ $($hence in total $m-1$ roots$),$ and $\beta_1,\beta_2,\ldots,\beta_m$ with respective multiplicities $k_1-1,k_2-1,\ldots,k_m-1$ $($which adds up to $\sum_{j=1}^m(k_j-1)=n-m$ roots$).$ Hence we have $m-1+n-m=n-1$ real roots of $P'(x)$. Since $P'(x)$ is a polynomial of degree $n-1,$ it has exactly $n-1$ roots. Hence all the roots of $P'(x)$ is real. $\blacksquare$","Here's a problem that I've solved but I'm not very confident on my solution. Please check it there's any gap in my arguments. Also, is there a way to come up with a shorter proof ? Thank you. The Problem : If the polynomial $P(x)=a_nx^n+a_{n-1}x^{n-1}+\ldots+a_0,~a_n \neq 0,$ $(a_j \in \mathbb{R} ~\text{ for }~0 \leq j \leq n)$ has only real roots, then it's derivative $P'(x)$ has only real roots. My Solution : We know that, a polynomial of degree $n$ has exactly $n$ roots. Let the roots be denoted by $\alpha_1, \alpha_2,\ldots,\alpha_n$. Case-I : Let all the roots be distinct . WLOG let $\alpha_1<\alpha_2<\ldots<\alpha_n$. Then, by Rolle's theorem $\forall~ 0 \leq j \leq n, \exists~ c_j \in (\alpha_j,\alpha_{j+1})$ such that $P'(c_j)=0$. Note that we're free to use Rolles's theorem as $P$ is differentiable $($and hence also continuous $)$ throughout $\mathbb{R}$. Hence we have $n-1$ real roots of $P'(x)$ namely $c_1,c_2,\ldots,c_{n-1}$. Since $P'(x)$ is a polynomial of degree $n-1,$ and hence has exactly $($though at most suffices, in this case$) ~n-1$ roots. Hence all roots of $P'(x)$ are real. So this case was quite trivial (in a relative sense). Case-II : All the roots are NOT distinct . Suppose we have $m~(<n)$ distinct roots $\beta_1, \beta_2,\ldots,\beta_m$ with respective multiplicities $k_1,k_2,\ldots,k_m$. Clearly, $\sum_{j=1}^m k_j=n$. Claim : If $\beta$ is a root of $P(x)$ with multiplicity $k~(>1),$ then $\beta$ is a root of $P'(x)$ with multiplicity $k-1$. Proof of the claim : $\beta$ is a root of $P(x)$ with multiplicity $k~(>1)$ $\implies$ $P(x)=(x-\beta)^k Q(x)$ where $Q(x),$ is a polynomial of degree $n-k$ and $Q(\beta) \neq 0$. Then $P'(x)=k(x-\beta)^{k-1}Q(x)+(x-\beta)^kQ'(x)=(x-\beta)^{k-1}\{kQ(x)+(x-\beta)Q'(x)\}$ Hence $\beta$ is a root of $P'(x)$ with $k-1$ multiplicity $($Since $kQ(\beta)+(\beta-\beta)Q'(\beta)=kQ(\beta) \neq 0)$. End of Proof of the claim. Using the claim, for all $0 \leq j \leq m, \beta_j$ is a root of $P'(x)$ with multiplicity $k_j-1$. Like in Case-I , we use Rolle's theorem again to come up with $c_j \in (\beta_j,\beta_{j+1})$ such that $P(c_j)=0, ~1 \leq j \leq m-1$. So in total, as roots of $P'(x),$ we have $c_1,c_2,\ldots,c_{m-1}$ each with multiplicity $1,$ $($hence in total $m-1$ roots$),$ and $\beta_1,\beta_2,\ldots,\beta_m$ with respective multiplicities $k_1-1,k_2-1,\ldots,k_m-1$ $($which adds up to $\sum_{j=1}^m(k_j-1)=n-m$ roots$).$ Hence we have $m-1+n-m=n-1$ real roots of $P'(x)$. Since $P'(x)$ is a polynomial of degree $n-1,$ it has exactly $n-1$ roots. Hence all the roots of $P'(x)$ is real. $\blacksquare$",,"['real-analysis', 'derivatives']"
81,Existence of a function whose derivative of inverse equals the inverse of the derivative,Existence of a function whose derivative of inverse equals the inverse of the derivative,,"I've been thinking about the calculation of inverse functions through Taylor series expansions. My hypothesis was that if we had: $$\ f(x) =\sum_{n=0}^\infty \frac{(x-x_0)}{n!}f^{n}(x_0),$$ then $$\ f^{-1}(x) =\sum_{n=0}^\infty \frac{(x-x_0)}{n!}({f^{-1}})^{(n)}(x_0).$$ Here it'd be possible to calculate the derivatives $\ ({f^{-1}})^{(n)}(x_0)$ through the inverse function derivation theorem but for this to work we should have the derivative of the inverse of $\ f$ equal the inverse of the derivative of $\ f$ , i.e. $$\ (f^{-1})' = (f')^{-1} \quad (1).$$ In general this does not hold; take for example $\ f(x) = x^2 $ . To formulate my question, assume we have a differentiable bijective map $\ f:A\rightarrow B$ with bijective derivative. Its inverse $\ f^{-1}$ is also a differentiable bijection. Assume the derivative of the inverse is also bijective. To try and find a function for which $\ (1) $ holds, I was able to deduce (from $\ (1) $ ) that if such a function exists, we must have $$\ f(x) = (f' \circ f')(x) \quad (2).$$ Clearly this does not hold for any polynomial nor (I'm pretty sure) for any other elementary functions. So, does there exist a function for which this condition holds? Deriving (2): $\ (f')^{-1}(x) = (f^{-1})'(x) = \frac{1}{(f' \circ f^{-1})(x)} \iff (f' \circ f^{-1})(x) = \frac{1}{(f')^{-1}(x)}$ . Then mapping by $\ f$ from the right gives $\ f'(x)=\frac{1}{(f')^{-1}(f(x))}$ and mapping by $\ \frac{1}{f'}$ from the left $\ \frac{1}{f(x)} = (\frac{1}{f'} \circ f')(x) \iff f(x)= \frac{1}{(\frac{1}{f'} \circ f')(x)}=\frac{1}{\frac{1}{(f' \circ f')(x)}}=(f' \circ f')(x)$","I've been thinking about the calculation of inverse functions through Taylor series expansions. My hypothesis was that if we had: then Here it'd be possible to calculate the derivatives through the inverse function derivation theorem but for this to work we should have the derivative of the inverse of equal the inverse of the derivative of , i.e. In general this does not hold; take for example . To formulate my question, assume we have a differentiable bijective map with bijective derivative. Its inverse is also a differentiable bijection. Assume the derivative of the inverse is also bijective. To try and find a function for which holds, I was able to deduce (from ) that if such a function exists, we must have Clearly this does not hold for any polynomial nor (I'm pretty sure) for any other elementary functions. So, does there exist a function for which this condition holds? Deriving (2): . Then mapping by from the right gives and mapping by from the left","\ f(x) =\sum_{n=0}^\infty \frac{(x-x_0)}{n!}f^{n}(x_0), \ f^{-1}(x) =\sum_{n=0}^\infty \frac{(x-x_0)}{n!}({f^{-1}})^{(n)}(x_0). \ ({f^{-1}})^{(n)}(x_0) \ f \ f \ (f^{-1})' = (f')^{-1} \quad (1). \ f(x) = x^2  \ f:A\rightarrow B \ f^{-1} \ (1)  \ (1)  \ f(x) = (f' \circ f')(x) \quad (2). \ (f')^{-1}(x) = (f^{-1})'(x) = \frac{1}{(f' \circ f^{-1})(x)} \iff (f' \circ f^{-1})(x) = \frac{1}{(f')^{-1}(x)} \ f \ f'(x)=\frac{1}{(f')^{-1}(f(x))} \ \frac{1}{f'} \ \frac{1}{f(x)} = (\frac{1}{f'} \circ f')(x) \iff f(x)= \frac{1}{(\frac{1}{f'} \circ f')(x)}=\frac{1}{\frac{1}{(f' \circ f')(x)}}=(f' \circ f')(x)","['derivatives', 'power-series', 'inverse-function']"
82,Is there a differentiable function on a closed subset of $\mathbb{R}^n$ that cannot be continued differentiably on an open superset?,Is there a differentiable function on a closed subset of  that cannot be continued differentiably on an open superset?,\mathbb{R}^n,"Let $A \subseteq \mathbb{R}^n$ be closed with no isolated points and $f:A \to \mathbb{R}^m$. Suppose that for every point $x_0 \in A$ we have (at least one) matrix $L_{x_0}$ such that $$ \lim_{x,y \to x_0} \frac{f(x)-f(y) - L_{x_0}(x-y)}{\Vert x-y\Vert} = 0 $$ The existence of this limit is always possible because $x_0$ is no isolated point of $A$. My question is: Is there an open set $U\subseteq \mathbb{R}^n$ with $A \subseteq U$ and a differentiable function $g$ on $U$ such that $g\vert_A = f$? What if we say that $L_{x_0}$ is unique for every $x_0 \in A$? What if we say that $x_0 \mapsto L_{x_0}$ is conitinuous? In a book about differentiable manifolds, I read that a function is called differentiable on a border point if it can be continued differenitably on a neighbourhood of that point. I was wondering what's the point. Why don't we use pointwise differentiability like usual?","Let $A \subseteq \mathbb{R}^n$ be closed with no isolated points and $f:A \to \mathbb{R}^m$. Suppose that for every point $x_0 \in A$ we have (at least one) matrix $L_{x_0}$ such that $$ \lim_{x,y \to x_0} \frac{f(x)-f(y) - L_{x_0}(x-y)}{\Vert x-y\Vert} = 0 $$ The existence of this limit is always possible because $x_0$ is no isolated point of $A$. My question is: Is there an open set $U\subseteq \mathbb{R}^n$ with $A \subseteq U$ and a differentiable function $g$ on $U$ such that $g\vert_A = f$? What if we say that $L_{x_0}$ is unique for every $x_0 \in A$? What if we say that $x_0 \mapsto L_{x_0}$ is conitinuous? In a book about differentiable manifolds, I read that a function is called differentiable on a border point if it can be continued differenitably on a neighbourhood of that point. I was wondering what's the point. Why don't we use pointwise differentiability like usual?",,"['derivatives', 'differential-topology']"
83,Prove that the Weierstrass-type function is nowhere differentiable,Prove that the Weierstrass-type function is nowhere differentiable,,"Given $0<\alpha\leq1$. Show that the function $$f(x)=\sum_{j=1}^\infty 2^{-j\alpha}\sin(2^jx)$$ is nowhere differentiable. I have solved the case $x=0$. Taking $t_l=2^{-l-1}\pi$, then $f(t_l)-f(0)=f(t_l)$ can be broken into three terms: the sum from $1$ to $l-1$, the single summand $j=l$, and the sum from $l+1$ to $\infty$. The third term is $0$. The middle term is $2^{-l\alpha}$, which dominates the first term as $l$ goes to $\infty$. This makes the Newton quotient blow up and shows nondifferentiability at $x=0.$ However, when $x$ is not a multiple of $\pi$ (or, rather, not $\frac{k\pi}{2^n}$) the (single-summand) middle term is very difficult to control for me (The third term is still $0$). Please help. This is not homework. Thank you very much.","Given $0<\alpha\leq1$. Show that the function $$f(x)=\sum_{j=1}^\infty 2^{-j\alpha}\sin(2^jx)$$ is nowhere differentiable. I have solved the case $x=0$. Taking $t_l=2^{-l-1}\pi$, then $f(t_l)-f(0)=f(t_l)$ can be broken into three terms: the sum from $1$ to $l-1$, the single summand $j=l$, and the sum from $l+1$ to $\infty$. The third term is $0$. The middle term is $2^{-l\alpha}$, which dominates the first term as $l$ goes to $\infty$. This makes the Newton quotient blow up and shows nondifferentiability at $x=0.$ However, when $x$ is not a multiple of $\pi$ (or, rather, not $\frac{k\pi}{2^n}$) the (single-summand) middle term is very difficult to control for me (The third term is still $0$). Please help. This is not homework. Thank you very much.",,"['real-analysis', 'reference-request', 'derivatives', 'convergence-divergence', 'examples-counterexamples']"
84,Generalisation of kth derivative to real values of k,Generalisation of kth derivative to real values of k,,"The answer to this question is most likely no, but I'm asking anyway: Assume that $f\in C^n(\mathbb {R,R})$. Is their any natural generalisation of the map $$\{1,2,\ldots,n\}\to C(\mathbb{R, R})\\k\mapsto f^{(k)}$$ to a map $$ [1, n]\to\{g\colon\mathbb{R\to R}\}\\ x\mapsto f^{(x)}? $$ That is, can we generalise ""the $k$th derivative"" to ""the $x$th derivative"" for real values of $x$? What I mean by ""natural"" is: Anything that has desirable properties and that has been explicitly formulated by someone, somewhere.","The answer to this question is most likely no, but I'm asking anyway: Assume that $f\in C^n(\mathbb {R,R})$. Is their any natural generalisation of the map $$\{1,2,\ldots,n\}\to C(\mathbb{R, R})\\k\mapsto f^{(k)}$$ to a map $$ [1, n]\to\{g\colon\mathbb{R\to R}\}\\ x\mapsto f^{(x)}? $$ That is, can we generalise ""the $k$th derivative"" to ""the $x$th derivative"" for real values of $x$? What I mean by ""natural"" is: Anything that has desirable properties and that has been explicitly formulated by someone, somewhere.",,['derivatives']
85,Understanding a Cosine Derivative,Understanding a Cosine Derivative,,Let $c$ be a constant. Why is it that $$ D_x \left(- \frac{\cos(cx)}{c} \right) = \sin(cx)? $$ I understand that $D_x \cos(x) = - \sin(x)$.  So what trigonometric identity is allowing us to infer the above?,Let $c$ be a constant. Why is it that $$ D_x \left(- \frac{\cos(cx)}{c} \right) = \sin(cx)? $$ I understand that $D_x \cos(x) = - \sin(x)$.  So what trigonometric identity is allowing us to infer the above?,,"['trigonometry', 'derivatives']"
86,When do polynomials eventually differentiate to zero?,When do polynomials eventually differentiate to zero?,,"Suppose I have a generic polynomial : $$f(x)=a_nx^n+a_{n-1}x^{n-1}+\dots+a_2x^2+a_1x+a_0$$ If I continually differentiate $f(x)$, when will it end up as $f^{(z)}(x)=0$? For example, given $f(x)=16x^3-x^2-3x+10$: $$f'(x)=48x^2-2x-3\\f''(x)=96x-2\\f'''(x)=96\\f''''(x)=\color{red}{0}$$ It ends up as $f^{(4)}=0$. I know that for the sine function this is not the case: $$f(x)=\sin(x)\\f'(x)=\cos(x)\\f''(x)=-\sin(x)\\f'''(x)=-\cos(x)\\f''''(x)=\sin(x)\\\text{loop!}$$ Edit: This isn't a polynomial. $x$ is not in the base, among other things. And if I define $$f(x)=\sin(x)=\frac{e^{-ix}}{2}+\frac{e^{ix}}{2}$$ Edit: Then we have sine still not as a polynomial. When differentiating it, we get: $$f'(x)=\frac{1}{2}ie^{ix}\left(-1+e^{2ix}\right)\\f''(x)=\frac{1}{2}e^{-ix}\left(1+e^{2ix}\right)\\f'''(x)=-\frac{1}{2}ie^{-ix}\left(-1+e^{2ix}\right)\\f''''(x)=\frac{e^{-ix}}{2}+\frac{e^{ix}}{2}\\\text{loop 2.0}!$$ What is ""special"" about the sine function that it does not eventually differentiate to $0$? As I investigated this question, I started with $i$ as a possible culprit: $$f(x)=10ix\\f'(x)=10i\\f''(x)=0\\\text{nope}$$ $$f(x)=10x^i\\f'(x)=10ix^{-1+i}\\f''(x)=(-10-10i)x^{-2+i}\\f^{(13)}(x)=(2716272000 - 8395946000i)x^{-13+i}\\\text{aha!}$$ I found that using $i$ in the exponent led to a polynomial that did not eventually differentiate to $0$. When do polynomials eventually differentiate to $f^{(z)}(x)=0$? Is using $i$ in the exponent the only case where it does not?","Suppose I have a generic polynomial : $$f(x)=a_nx^n+a_{n-1}x^{n-1}+\dots+a_2x^2+a_1x+a_0$$ If I continually differentiate $f(x)$, when will it end up as $f^{(z)}(x)=0$? For example, given $f(x)=16x^3-x^2-3x+10$: $$f'(x)=48x^2-2x-3\\f''(x)=96x-2\\f'''(x)=96\\f''''(x)=\color{red}{0}$$ It ends up as $f^{(4)}=0$. I know that for the sine function this is not the case: $$f(x)=\sin(x)\\f'(x)=\cos(x)\\f''(x)=-\sin(x)\\f'''(x)=-\cos(x)\\f''''(x)=\sin(x)\\\text{loop!}$$ Edit: This isn't a polynomial. $x$ is not in the base, among other things. And if I define $$f(x)=\sin(x)=\frac{e^{-ix}}{2}+\frac{e^{ix}}{2}$$ Edit: Then we have sine still not as a polynomial. When differentiating it, we get: $$f'(x)=\frac{1}{2}ie^{ix}\left(-1+e^{2ix}\right)\\f''(x)=\frac{1}{2}e^{-ix}\left(1+e^{2ix}\right)\\f'''(x)=-\frac{1}{2}ie^{-ix}\left(-1+e^{2ix}\right)\\f''''(x)=\frac{e^{-ix}}{2}+\frac{e^{ix}}{2}\\\text{loop 2.0}!$$ What is ""special"" about the sine function that it does not eventually differentiate to $0$? As I investigated this question, I started with $i$ as a possible culprit: $$f(x)=10ix\\f'(x)=10i\\f''(x)=0\\\text{nope}$$ $$f(x)=10x^i\\f'(x)=10ix^{-1+i}\\f''(x)=(-10-10i)x^{-2+i}\\f^{(13)}(x)=(2716272000 - 8395946000i)x^{-13+i}\\\text{aha!}$$ I found that using $i$ in the exponent led to a polynomial that did not eventually differentiate to $0$. When do polynomials eventually differentiate to $f^{(z)}(x)=0$? Is using $i$ in the exponent the only case where it does not?",,"['derivatives', 'polynomials', 'complex-numbers', 'exponentiation']"
87,Is My Proof that $\pi^e < e^{\pi}$ Valid? [duplicate],Is My Proof that  Valid? [duplicate],\pi^e < e^{\pi},"This question already has answers here : Comparing $\pi^e$ and $e^\pi$ without calculating them (15 answers) Closed 8 years ago . The other day, a math teacher at my college gave me a challenge problem: Prove that $$\pi^e < e^{\pi}$$ without using a calculator. The next day, I found a valid proof, but I used a log table instead of a calculator, so my proof is hardly satisfying. I went looking for another proof and I came up with something that might not be legitimate. Here's my proof: Suppose that is an $x$ such that $x^e < e^x$ Then taking the natural log of both sides, $$\ln(x^e) < \ln(e^x)$$ $$e\ln(x) < x$$ $$\ln(x) < \frac{x}{e}$$ Differentiate both sides, $$\frac{1}{x} < \frac{1}{e}$$ $$x>e$$ Thus, $x^e < x^e$, for $x > e$ Since $\pi > e$, therefore $\pi^e < e^{\pi}$ I think my proof is good except perhaps where I differentiated both sides of the inequality. I know that there are many cases where this is invalid, but I'm not sure about this case.","This question already has answers here : Comparing $\pi^e$ and $e^\pi$ without calculating them (15 answers) Closed 8 years ago . The other day, a math teacher at my college gave me a challenge problem: Prove that $$\pi^e < e^{\pi}$$ without using a calculator. The next day, I found a valid proof, but I used a log table instead of a calculator, so my proof is hardly satisfying. I went looking for another proof and I came up with something that might not be legitimate. Here's my proof: Suppose that is an $x$ such that $x^e < e^x$ Then taking the natural log of both sides, $$\ln(x^e) < \ln(e^x)$$ $$e\ln(x) < x$$ $$\ln(x) < \frac{x}{e}$$ Differentiate both sides, $$\frac{1}{x} < \frac{1}{e}$$ $$x>e$$ Thus, $x^e < x^e$, for $x > e$ Since $\pi > e$, therefore $\pi^e < e^{\pi}$ I think my proof is good except perhaps where I differentiated both sides of the inequality. I know that there are many cases where this is invalid, but I'm not sure about this case.",,"['inequality', 'derivatives', 'proof-verification']"
88,Is there a relation between fractional power and logarithm functions?,Is there a relation between fractional power and logarithm functions?,,"Something that has always bothered me is that there's no way to get $x^{-1}$ by differentiating $x^a$ for some $a$ , even though all other negative powers of $x$ can be achieved by differentiating some power of $x$ . So I began to mess around with equations trying to find some sort of connection. I realized that $$\lim_{a\rightarrow\infty}\left(\frac{d}{dx}\left( ax^{\frac{1}{a}}\right)\right) = x^{-1}$$ And, upon further delving that $$\lim_{a\rightarrow\infty} ax^{\frac{1}{a}} - a = \ln(x)$$ Is there a some sort of relation here? I understand that logarithms are intrinsically linked to powers, but I don't understand why a very small power is equal to a scaled, offset logarithm.","Something that has always bothered me is that there's no way to get by differentiating for some , even though all other negative powers of can be achieved by differentiating some power of . So I began to mess around with equations trying to find some sort of connection. I realized that And, upon further delving that Is there a some sort of relation here? I understand that logarithms are intrinsically linked to powers, but I don't understand why a very small power is equal to a scaled, offset logarithm.",x^{-1} x^a a x x \lim_{a\rightarrow\infty}\left(\frac{d}{dx}\left( ax^{\frac{1}{a}}\right)\right) = x^{-1} \lim_{a\rightarrow\infty} ax^{\frac{1}{a}} - a = \ln(x),"['derivatives', 'logarithms', 'exponentiation']"
89,"Give an example, with proof, of a function differentiable at a point, whose derivative is non-differentiable that point.","Give an example, with proof, of a function differentiable at a point, whose derivative is non-differentiable that point.",,"This question was labeled as either prove or disprove. I think that this is impossible, here is my reasoning: Any function $f(x)$ with asymptotic behavior at some point $x=a$ will hold this asymptotic behavior in the function $F(x)=\int f(x) \, dx$ . We have not yet covered any technical terms with integration, only differentiation. Help greatly appreciated!","This question was labeled as either prove or disprove. I think that this is impossible, here is my reasoning: Any function with asymptotic behavior at some point will hold this asymptotic behavior in the function . We have not yet covered any technical terms with integration, only differentiation. Help greatly appreciated!","f(x) x=a F(x)=\int f(x) \, dx","['real-analysis', 'derivatives']"
90,Chain rule with triple composition,Chain rule with triple composition,,"We are supposed to apply the chain rule on the following function $f$: $$ f(x) = \sqrt{x+\sqrt{2x+\sqrt{3x}}}   $$ I assumed we could rewrite this as $$ f(x) = g(h(j(x))) $$ However, I was not sure how to define the functions $$g(x), h(x), j(x) $$ Any help would be appreciated.","We are supposed to apply the chain rule on the following function $f$: $$ f(x) = \sqrt{x+\sqrt{2x+\sqrt{3x}}}   $$ I assumed we could rewrite this as $$ f(x) = g(h(j(x))) $$ However, I was not sure how to define the functions $$g(x), h(x), j(x) $$ Any help would be appreciated.",,"['derivatives', 'function-and-relation-composition']"
91,Relationship between factorial and derivatives,Relationship between factorial and derivatives,,I was wondering if there is any relationship between factorials and derivatives because I notice that if we had $x^n$ and we take the $n$-th derivative of this function it will be equal to the factorial of $n$: $$\frac{d^n}{dx^n}(x^n)=n!$$,I was wondering if there is any relationship between factorials and derivatives because I notice that if we had $x^n$ and we take the $n$-th derivative of this function it will be equal to the factorial of $n$: $$\frac{d^n}{dx^n}(x^n)=n!$$,,"['derivatives', 'factorial']"
92,"Is the function $f(x,y,z)=\sqrt{x^2 +y^2} +z^2$ smooth?",Is the function  smooth?,"f(x,y,z)=\sqrt{x^2 +y^2} +z^2","Let $f:\mathbb{R}^3 \rightarrow \mathbb{R}$ be defined by $$f(x,y,z)=\sqrt{x^2 +y^2} +z^2 .$$ Is this function smooth? My head is telling me there should be a problem when $x=y=0$, but I'm not sure. Can anybody help me out?","Let $f:\mathbb{R}^3 \rightarrow \mathbb{R}$ be defined by $$f(x,y,z)=\sqrt{x^2 +y^2} +z^2 .$$ Is this function smooth? My head is telling me there should be a problem when $x=y=0$, but I'm not sure. Can anybody help me out?",,"['real-analysis', 'differential-geometry', 'derivatives']"
93,Prove a convex and concave function can have at most 2 solutions,Prove a convex and concave function can have at most 2 solutions,,"Let $ a,b \in \mathbb{R}$ Let $f(x) : [a,b] \rightarrow \mathbb{R} $ is a continuous function in $ [a ,b] $, differentiable and strictly convex in $ (a, b) $ and $g(x) : [a,b] \rightarrow \mathbb{R} $ is a continuous function in $ [a ,b] $, differentiable and strictly concave in $ (a, b) $ How can I prove the intersection of $ f $ and $ g $ can have a maximum of two roots $f(x)-g(x)=0 $ ?","Let $ a,b \in \mathbb{R}$ Let $f(x) : [a,b] \rightarrow \mathbb{R} $ is a continuous function in $ [a ,b] $, differentiable and strictly convex in $ (a, b) $ and $g(x) : [a,b] \rightarrow \mathbb{R} $ is a continuous function in $ [a ,b] $, differentiable and strictly concave in $ (a, b) $ How can I prove the intersection of $ f $ and $ g $ can have a maximum of two roots $f(x)-g(x)=0 $ ?",,"['derivatives', 'proof-writing', 'roots']"
94,sum of polynoms of given property,sum of polynoms of given property,,"I have $P(x)$ a polynomial with degree $n$ ,$P(x) \ge 0$ for all $x \in$ real. I have to prove that: $f(x)=P(x)+P'(x)+P""(x)+......+P^{n}(x) \ge 0$ for all $x$. I tried different methods to solve it but I got stuck.Any suggestion or advice is welcomed.","I have $P(x)$ a polynomial with degree $n$ ,$P(x) \ge 0$ for all $x \in$ real. I have to prove that: $f(x)=P(x)+P'(x)+P""(x)+......+P^{n}(x) \ge 0$ for all $x$. I tried different methods to solve it but I got stuck.Any suggestion or advice is welcomed.",,"['polynomials', 'derivatives']"
95,A uniformly continuous function such that the derivative is not bounded and is not defined on a compact?,A uniformly continuous function such that the derivative is not bounded and is not defined on a compact?,,"Is there a uniformly continuous function values in $f: \mathbb{R}{\to}\mathbb{R}$ such that its first derivative is not bounded and is defined on a non-compact set? And if $f: X_{1}{\to}\mathbb{R}$? (let $(X_{1},d{1})$ metric space ). And if $f: X_{1}{\to}X_{2}$? (let $(X_{2},d{2})$ metric space ). I know that the first derivative is bounded f:I-->R (let I interval) iff f is a lipschitz function; If X1 is a compact thanks to the Heine-Cantor theorem f is uniformly continuous.","Is there a uniformly continuous function values in $f: \mathbb{R}{\to}\mathbb{R}$ such that its first derivative is not bounded and is defined on a non-compact set? And if $f: X_{1}{\to}\mathbb{R}$? (let $(X_{1},d{1})$ metric space ). And if $f: X_{1}{\to}X_{2}$? (let $(X_{2},d{2})$ metric space ). I know that the first derivative is bounded f:I-->R (let I interval) iff f is a lipschitz function; If X1 is a compact thanks to the Heine-Cantor theorem f is uniformly continuous.",,['real-analysis']
96,Why every point of a function where differentiation exists has only one tangent?,Why every point of a function where differentiation exists has only one tangent?,,Can anyone help me out? Why every point of a function where differentiation exists has only one tangent? I know the slope at any point of any function is defined by differentiation at that point.But there may be another straight line which touches the function at that point but slope is different from the differentiation value at that point. But this does not happen(I am not considering singular point).,Can anyone help me out? Why every point of a function where differentiation exists has only one tangent? I know the slope at any point of any function is defined by differentiation at that point.But there may be another straight line which touches the function at that point but slope is different from the differentiation value at that point. But this does not happen(I am not considering singular point).,,['derivatives']
97,Derivative of the variance wrt $x_i$,Derivative of the variance wrt,x_i,"As the title states, I want to find the derivative of $$\frac{1}{N}\sum_i (x_i - \mu)^2$$ w.r.t $x_i$ (note that $\mu$ is also another function of $x_i$, of course). I've tried solving it and got the following result $$\frac{2(N - 1)}{N^2}\sum_i (x_i - \mu)$$ Is this right? Am I doing something wrong?","As the title states, I want to find the derivative of $$\frac{1}{N}\sum_i (x_i - \mu)^2$$ w.r.t $x_i$ (note that $\mu$ is also another function of $x_i$, of course). I've tried solving it and got the following result $$\frac{2(N - 1)}{N^2}\sum_i (x_i - \mu)$$ Is this right? Am I doing something wrong?",,['derivatives']
98,Taylor Series in Fractional Calculus,Taylor Series in Fractional Calculus,,"I recently studied fractional calculus, namely the possibility to define fractional derivatives of some functions, like $$\frac{\text{d}^{1/2}}{\text{d}x^{1/2}}\ f(x) ~~~~~~~~~~~~~ \frac{\text{d}^{2/3}}{\text{d}x^{2/3}}\ f(x)$$ and so on. Now the question that came up into my mind is: if such a construction is possible, can we built "" new "" Taylor series for well known function in order to take into account (some) fractional derivatives too? I know the first problems that would arise would be: how could we take the whole possible derivatives of order between $0$ and $1$? They are infinite. And Yeah, that could be a really huge problem.. Are there any example of Fractional Taylor Series ? P.s. I've already read other similar questions, but they are too arid and I didn't find any good answer yet..","I recently studied fractional calculus, namely the possibility to define fractional derivatives of some functions, like $$\frac{\text{d}^{1/2}}{\text{d}x^{1/2}}\ f(x) ~~~~~~~~~~~~~ \frac{\text{d}^{2/3}}{\text{d}x^{2/3}}\ f(x)$$ and so on. Now the question that came up into my mind is: if such a construction is possible, can we built "" new "" Taylor series for well known function in order to take into account (some) fractional derivatives too? I know the first problems that would arise would be: how could we take the whole possible derivatives of order between $0$ and $1$? They are infinite. And Yeah, that could be a really huge problem.. Are there any example of Fractional Taylor Series ? P.s. I've already read other similar questions, but they are too arid and I didn't find any good answer yet..",,"['derivatives', 'taylor-expansion', 'fractional-calculus']"
99,Derivative Of $\ln(x)$,Derivative Of,\ln(x),"It is required to find the derivative of the natural logarithm of $x$: $\frac {d}{dx}\ln(x)$ My solution: Let $f(x)=\ln(x) $ then $f'(x)=\frac {d}{dx}\ln(x)    $ By definition:$$f'(x)= \lim_{h\to 0}\frac{f(x+h)- f(x)}{h} $$ By substitution:$$f'(x) = \lim_{h\to 0}\frac{\ln(x+h)- \ln(x)}{h} $$ Since $\ln(a)-\ln(b)=\ln(\frac ab)$, then: $$f'(x) = \lim_{h\to 0}\frac{\ln(\frac{x+h}x)}{h}$$ Since $a\times \ln(b) = \ln(a^b)$, then: $$f'(x) = \lim_{h\to 0}\ln((\frac{x+h}x)^\frac 1h)$$ Then:$$f'(x) = \lim_{h\to 0}\ln((1+\frac hx)^\frac 1h)$$ How to continue? Are there any other ways to find the derivative? Thanks in advance! Note: It is not allowed to use the fact that: $(e^x)'=e^x$ $$ e=\lim\limits_{n\to\infty} \left(1+\frac1n\right)^n$$","It is required to find the derivative of the natural logarithm of $x$: $\frac {d}{dx}\ln(x)$ My solution: Let $f(x)=\ln(x) $ then $f'(x)=\frac {d}{dx}\ln(x)    $ By definition:$$f'(x)= \lim_{h\to 0}\frac{f(x+h)- f(x)}{h} $$ By substitution:$$f'(x) = \lim_{h\to 0}\frac{\ln(x+h)- \ln(x)}{h} $$ Since $\ln(a)-\ln(b)=\ln(\frac ab)$, then: $$f'(x) = \lim_{h\to 0}\frac{\ln(\frac{x+h}x)}{h}$$ Since $a\times \ln(b) = \ln(a^b)$, then: $$f'(x) = \lim_{h\to 0}\ln((\frac{x+h}x)^\frac 1h)$$ Then:$$f'(x) = \lim_{h\to 0}\ln((1+\frac hx)^\frac 1h)$$ How to continue? Are there any other ways to find the derivative? Thanks in advance! Note: It is not allowed to use the fact that: $(e^x)'=e^x$ $$ e=\lim\limits_{n\to\infty} \left(1+\frac1n\right)^n$$",,"['derivatives', 'logarithms']"
