,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Prove that $\int_E |f_n-f|\to0 \iff \lim\limits_{n\to\infty}\int_E|f_n|=\int_E|f|.$,Prove that,\int_E |f_n-f|\to0 \iff \lim\limits_{n\to\infty}\int_E|f_n|=\int_E|f|.,"I'm reading Real Analysis by Royden 4th Edition. The entire problem statement is: Let $\{f_n\}_{n=1}^\infty$ be a sequence of integrable functions on $E$ for which $f_n\to f$ pointwise a.e. on $E$ and $f$ is integrable over $E$. Show that $\int_E |f_n-f|\to0 \iff \lim\limits_{n\to\infty}\int_E|f_n|=\int_E|f|.$ My attempt at the proof is: $(\Longrightarrow)$ Suppose $\int_E|f_n-f|\to0$ and let $\varepsilon>0$ be given. Then there exists an $N>0$ such that if $n\geq N$ then $|\int_E|f_n-f||<\varepsilon.$ Consider $$|\int_E|f_n|-\int_E|f||=|\int_E(|f_n|-|f|)|\leq|\int_E|f_n-f||<\varepsilon.$$ Thus, $\int_E|f_n|\to\int_E|f|.$ $(\Longleftarrow)$ Suppose now that $\int_E|f_n|\to\int_E|f|.$ Let $h_n=|f_n-f|$ and $g_n=|f_n|+|f|$. Then $h_n\to0$ pointwise a.e. on $E$ and $g_n\to2|f|$ pointwise a.e. on $E$. Moreover, since each $f_n$ and $f$ are integrable $\int_E g_n=\int_E|f_n|+|f|\to2\int_E|f|.$ Thus, by the General Lebesgue Dominated Convergence Theorem, $\int_E|f_n-f|\to\int_E0=0.$ I'm pretty sure I got this one down, but I was wondering if it was okay for $g_n$ to depend on $f$ or $f_n$ or does it need to be independent of them? Thanks for any help or feedback!","I'm reading Real Analysis by Royden 4th Edition. The entire problem statement is: Let $\{f_n\}_{n=1}^\infty$ be a sequence of integrable functions on $E$ for which $f_n\to f$ pointwise a.e. on $E$ and $f$ is integrable over $E$. Show that $\int_E |f_n-f|\to0 \iff \lim\limits_{n\to\infty}\int_E|f_n|=\int_E|f|.$ My attempt at the proof is: $(\Longrightarrow)$ Suppose $\int_E|f_n-f|\to0$ and let $\varepsilon>0$ be given. Then there exists an $N>0$ such that if $n\geq N$ then $|\int_E|f_n-f||<\varepsilon.$ Consider $$|\int_E|f_n|-\int_E|f||=|\int_E(|f_n|-|f|)|\leq|\int_E|f_n-f||<\varepsilon.$$ Thus, $\int_E|f_n|\to\int_E|f|.$ $(\Longleftarrow)$ Suppose now that $\int_E|f_n|\to\int_E|f|.$ Let $h_n=|f_n-f|$ and $g_n=|f_n|+|f|$. Then $h_n\to0$ pointwise a.e. on $E$ and $g_n\to2|f|$ pointwise a.e. on $E$. Moreover, since each $f_n$ and $f$ are integrable $\int_E g_n=\int_E|f_n|+|f|\to2\int_E|f|.$ Thus, by the General Lebesgue Dominated Convergence Theorem, $\int_E|f_n-f|\to\int_E0=0.$ I'm pretty sure I got this one down, but I was wondering if it was okay for $g_n$ to depend on $f$ or $f_n$ or does it need to be independent of them? Thanks for any help or feedback!",,"['real-analysis', 'integration', 'measure-theory', 'convergence-divergence', 'solution-verification']"
1,"continuous onto function from irrationals in [0,1] onto rationals in [0,1]","continuous onto function from irrationals in [0,1] onto rationals in [0,1]",,"Give a continuous surjective function from the irrationals in $[0,1]$ onto the rationals in $[0,1]$. Can we at least prove the existence of such a function? I couldn't see a function off the top of my head. Here we assume $[0,1]\setminus\mathbb Q$ with its usual metric.","Give a continuous surjective function from the irrationals in $[0,1]$ onto the rationals in $[0,1]$. Can we at least prove the existence of such a function? I couldn't see a function off the top of my head. Here we assume $[0,1]\setminus\mathbb Q$ with its usual metric.",,['real-analysis']
2,"Logarithm integral with golden ratio $\int^1_0 \frac{\log(1+\phi x^2)}{1+x}\, dx$",Logarithm integral with golden ratio,"\int^1_0 \frac{\log(1+\phi x^2)}{1+x}\, dx","How to evaluate the following integral $$ \int^1_0 \frac{\log(1+\phi x^2)}{1+x}\, dx$$ where $$\phi =\frac{\sqrt{5}+1}{2}$$ Is there a closed form for the integral ? My first attempt would involve relating the integral to dilogarithms but that seems complicated !","How to evaluate the following integral $$ \int^1_0 \frac{\log(1+\phi x^2)}{1+x}\, dx$$ where $$\phi =\frac{\sqrt{5}+1}{2}$$ Is there a closed form for the integral ? My first attempt would involve relating the integral to dilogarithms but that seems complicated !",,"['calculus', 'real-analysis', 'integration']"
3,Alternate proofs (other than diagonalization and topological nested sets) for uncountability of the reals?,Alternate proofs (other than diagonalization and topological nested sets) for uncountability of the reals?,,"I recently started studying set theory and am having quite a bit of difficulty accepting Cantor's diagonal proof for the uncountability of the reals. I also saw a topological proof via nested sets for uncountability which still does not satisfy me completely, given that just like the diagonal it relies on a never ending process. In fact, the nested sets proof sounds very much like the diagonalization proof to me. Do all proofs of the uncountability of the reals involve diagonalization? Are there any other proofs I can look at to understand? I couldn't find any on searching stack exchange. Thanks.","I recently started studying set theory and am having quite a bit of difficulty accepting Cantor's diagonal proof for the uncountability of the reals. I also saw a topological proof via nested sets for uncountability which still does not satisfy me completely, given that just like the diagonal it relies on a never ending process. In fact, the nested sets proof sounds very much like the diagonalization proof to me. Do all proofs of the uncountability of the reals involve diagonalization? Are there any other proofs I can look at to understand? I couldn't find any on searching stack exchange. Thanks.",,"['real-analysis', 'logic', 'set-theory']"
4,Help with evaluating a sum,Help with evaluating a sum,,"I am trying to evaluate the following sum: $$\sum_{n=1}^{\infty} \frac{1}{n(n+1)5^n}$$ So far I have written the sum as $$\sum_{n=1}^{\infty} \frac{1}{n(n+1)5^n} = \sum_{n=1}^{\infty} \left ( \frac{1}{n} - \frac{1}{n+1} \right ) \frac{1}{5^n} = \sum_{n=1}^{\infty} \frac{1}{n5^n} - \sum_{n=1}^{\infty} \frac{1}{(n+1)5^n}$$ I am stuck and I have not been able to find any similar examples. Wolfram Alpha gives the result $$\sum_{n=1}^{\infty} \frac{1}{n(n+1)5^n} = 1-4 \log(5/4)$$ I feel as though I should be writing the sum as an integral and evaluating the integral, but I do not know how to proceed. Any help appreciated. Thanks.","I am trying to evaluate the following sum: $$\sum_{n=1}^{\infty} \frac{1}{n(n+1)5^n}$$ So far I have written the sum as $$\sum_{n=1}^{\infty} \frac{1}{n(n+1)5^n} = \sum_{n=1}^{\infty} \left ( \frac{1}{n} - \frac{1}{n+1} \right ) \frac{1}{5^n} = \sum_{n=1}^{\infty} \frac{1}{n5^n} - \sum_{n=1}^{\infty} \frac{1}{(n+1)5^n}$$ I am stuck and I have not been able to find any similar examples. Wolfram Alpha gives the result $$\sum_{n=1}^{\infty} \frac{1}{n(n+1)5^n} = 1-4 \log(5/4)$$ I feel as though I should be writing the sum as an integral and evaluating the integral, but I do not know how to proceed. Any help appreciated. Thanks.",,['real-analysis']
5,Is every weak contraction a contraction?,Is every weak contraction a contraction?,,"A weak contraction is a function $f:M \to M$ such that for all $x \neq y$, $d(f(x), f(y)) < d(x, y)$. I don't think every weak contraction is a contraction, but I'm having a hard time finding a counterexample. Also, is it true that if $M$ is compact, then a weak contraction is a contraction? I think it might be true, but I'm not sure how to prove it.","A weak contraction is a function $f:M \to M$ such that for all $x \neq y$, $d(f(x), f(y)) < d(x, y)$. I don't think every weak contraction is a contraction, but I'm having a hard time finding a counterexample. Also, is it true that if $M$ is compact, then a weak contraction is a contraction? I think it might be true, but I'm not sure how to prove it.",,['real-analysis']
6,A proof of a property of limits,A proof of a property of limits,,"Today during lecture my lecturer showed us this property, but provided no proof. If $$\lim_{n\to\infty} {d_{n+1}\over d_n} >1$$ then $$\lim_{n\to\infty}d_{n}=\infty $$ Is this property legit? (not to be disrespectful to my lecturer but he tends to make a lot of mistakes) And if it is, what is the logic behind that property? How does it behave when the first limit tends to 1 or is less than 1?","Today during lecture my lecturer showed us this property, but provided no proof. If $$\lim_{n\to\infty} {d_{n+1}\over d_n} >1$$ then $$\lim_{n\to\infty}d_{n}=\infty $$ Is this property legit? (not to be disrespectful to my lecturer but he tends to make a lot of mistakes) And if it is, what is the logic behind that property? How does it behave when the first limit tends to 1 or is less than 1?",,"['real-analysis', 'limits', 'infinity']"
7,"If $x_n\ge0$ and $x_{n+1} \leq x_n + \frac1{n^2}$, show that: $\lim x_n$ converges.","If  and , show that:  converges.",x_n\ge0 x_{n+1} \leq x_n + \frac1{n^2} \lim x_n,"Let $x_n$ be a sequence of the type described above. It is not monotonic in general, so boundedness won't help. So, it seems as if I should show it's Cauchy. A wrong way to do this would be as follows (I'm on a mobile device, so I can't type absolute values. Bear with me.) $$\left|x_{n+1} - x_n\right| \leq \frac{1}{n^2}.$$ So, we have $$ \left|x_m -x_n\right| \leq \sum_{k=n}^{m} \frac{1}{k^2}$$ which is itself Cauchy, etc., etc. But, of course, I can't just use absolute values like that. One thing I have shown is that $x_n$ is bounded. Inductively, one may show $$ \limsup_{n \to \infty} x_n \leq x_k + \sum_{k=n}^{\infty} \frac{1}{k^2},$$ although I'm not sure this helps or matters at all. Thanks in advance. Disclaimer I've noticed that asking a large number of questions in quick succession on this site is often frowned upon, especially when little or no effort has been given by the asker. However, I am preparing for a large test in a few days and will be sifting through dozens of problems. Therefore, I may post a couple a day. I will only do so when I have made some initial, meaningful progress. Thanks.","Let $x_n$ be a sequence of the type described above. It is not monotonic in general, so boundedness won't help. So, it seems as if I should show it's Cauchy. A wrong way to do this would be as follows (I'm on a mobile device, so I can't type absolute values. Bear with me.) $$\left|x_{n+1} - x_n\right| \leq \frac{1}{n^2}.$$ So, we have $$ \left|x_m -x_n\right| \leq \sum_{k=n}^{m} \frac{1}{k^2}$$ which is itself Cauchy, etc., etc. But, of course, I can't just use absolute values like that. One thing I have shown is that $x_n$ is bounded. Inductively, one may show $$ \limsup_{n \to \infty} x_n \leq x_k + \sum_{k=n}^{\infty} \frac{1}{k^2},$$ although I'm not sure this helps or matters at all. Thanks in advance. Disclaimer I've noticed that asking a large number of questions in quick succession on this site is often frowned upon, especially when little or no effort has been given by the asker. However, I am preparing for a large test in a few days and will be sifting through dozens of problems. Therefore, I may post a couple a day. I will only do so when I have made some initial, meaningful progress. Thanks.",,"['real-analysis', 'sequences-and-series']"
8,Continuous extensions of continuous functions on dense subspaces,Continuous extensions of continuous functions on dense subspaces,,I thought that if I have a function $f: \mathbb Q \to \mathbb R$ that is continuous then I can (uniquely) extend it to a continuous function $F: \mathbb R \to \mathbb R$ as follows: for $r \in \mathbb R \setminus \mathbb Q$ pick a sequence $q_n$ converging to $r$ and then define $F(r) = \lim_{n \to \infty} f(q_n)$. So I thought there must be a theorem saying that given a continuous function $f: D \to Y$ where $D$ is a dense subset of a metric space $X$ one can uniquely extend it to $F: X \to Y$. Instead I found a theorem stating this but with the additional requirement that $f$ has to be uniformly continuous. Now I'm confused: is my example above wrong? Where does uniform continuity come in here? Thanks.,I thought that if I have a function $f: \mathbb Q \to \mathbb R$ that is continuous then I can (uniquely) extend it to a continuous function $F: \mathbb R \to \mathbb R$ as follows: for $r \in \mathbb R \setminus \mathbb Q$ pick a sequence $q_n$ converging to $r$ and then define $F(r) = \lim_{n \to \infty} f(q_n)$. So I thought there must be a theorem saying that given a continuous function $f: D \to Y$ where $D$ is a dense subset of a metric space $X$ one can uniquely extend it to $F: X \to Y$. Instead I found a theorem stating this but with the additional requirement that $f$ has to be uniformly continuous. Now I'm confused: is my example above wrong? Where does uniform continuity come in here? Thanks.,,"['real-analysis', 'metric-spaces']"
9,Evaluating $ \lim_{n\rightarrow\infty} n \int_{0}^{1} \frac{{x}^{n-2}}{{x}^{2n}+x^n+1} \mbox {d}x$,Evaluating, \lim_{n\rightarrow\infty} n \int_{0}^{1} \frac{{x}^{n-2}}{{x}^{2n}+x^n+1} \mbox {d}x,Evaluating $$L = \lim_{n\rightarrow\infty} n \int_{0}^{1} \frac{{x}^{n-2}}{{x}^{2n}+x^n+1} \mbox {d}x$$,Evaluating $$L = \lim_{n\rightarrow\infty} n \int_{0}^{1} \frac{{x}^{n-2}}{{x}^{2n}+x^n+1} \mbox {d}x$$,,"['calculus', 'real-analysis', 'integration', 'limits']"
10,Measure zero of the graph of a continuous function,Measure zero of the graph of a continuous function,,"If $A$ is a rectangle in $\mathbb{R}^n$ and if we let $f$ be continuous, then how can we show that the graph of $f$ has measure zero in $\mathbb{R}^{n+1}$? We may define that $A$ is a subset of $\mathbb{R}^n$ and the graph of $f: A\to \mathbb R$ is the set given $\mbox{graph}(f) := \{(x,y) \in \mathbb{R}^{n+1} : f(x) = y\}$.","If $A$ is a rectangle in $\mathbb{R}^n$ and if we let $f$ be continuous, then how can we show that the graph of $f$ has measure zero in $\mathbb{R}^{n+1}$? We may define that $A$ is a subset of $\mathbb{R}^n$ and the graph of $f: A\to \mathbb R$ is the set given $\mbox{graph}(f) := \{(x,y) \in \mathbb{R}^{n+1} : f(x) = y\}$.",,"['real-analysis', 'measure-theory']"
11,"Convergence of $\sqrt[n]{x^n+y^n}$ (for $x, y > 0$)",Convergence of  (for ),"\sqrt[n]{x^n+y^n} x, y > 0","How can I prove the convergence of the sequence $b_n=\sqrt[n]{x^n+y^n}$ where $x, y > 0$? Can I divide it in two cases? Case 1 : $x > y$. $$ b_n=\sqrt[n]{x^n+y^n} < \sqrt[n]{x^n+x^n} = \sqrt[n]{2 \cdot x^n}=x \cdot \sqrt[n]{2} $$ Case 2 : $x < y$. $$ b_n=\sqrt[n]{x^n+y^n} < \sqrt[n]{y^n+y^n} = \sqrt[n]{2 \cdot y^n}=y \cdot \sqrt[n]{2}$$ Result: Does the sequence converges to $\max(x,y)$?","How can I prove the convergence of the sequence $b_n=\sqrt[n]{x^n+y^n}$ where $x, y > 0$? Can I divide it in two cases? Case 1 : $x > y$. $$ b_n=\sqrt[n]{x^n+y^n} < \sqrt[n]{x^n+x^n} = \sqrt[n]{2 \cdot x^n}=x \cdot \sqrt[n]{2} $$ Case 2 : $x < y$. $$ b_n=\sqrt[n]{x^n+y^n} < \sqrt[n]{y^n+y^n} = \sqrt[n]{2 \cdot y^n}=y \cdot \sqrt[n]{2}$$ Result: Does the sequence converges to $\max(x,y)$?",,"['real-analysis', 'sequences-and-series', 'limits', 'radicals']"
12,Change of order of double limit of function sequence,Change of order of double limit of function sequence,,The more general quesion is under what conditions the folloing equality will hold (all functions are real valued): $$\lim_{x \rightarrow a} \ \lim_{j \rightarrow \infty} f_j(x) = \lim_{j \rightarrow \infty} \ \lim_{x \rightarrow a} f_j(x)$$ A more specific question is if it will hold for non-continuous functions $f_j$ that are uniformly convergent to a non-continuous limit function $f$ and all the single and iterated double limits exist. Also some references would be useful.,The more general quesion is under what conditions the folloing equality will hold (all functions are real valued): $$\lim_{x \rightarrow a} \ \lim_{j \rightarrow \infty} f_j(x) = \lim_{j \rightarrow \infty} \ \lim_{x \rightarrow a} f_j(x)$$ A more specific question is if it will hold for non-continuous functions $f_j$ that are uniformly convergent to a non-continuous limit function $f$ and all the single and iterated double limits exist. Also some references would be useful.,,"['real-analysis', 'sequences-and-series', 'functions', 'limits']"
13,$\Delta x$ in limit problem?,in limit problem?,\Delta x,I was working on some limit homework and everything was going fine until I reached this problem: $$\lim_{\Delta x \to 0} \frac{2(x + \Delta x) - 2x}{\Delta x}.$$ I am understanding limits but the triangle (delta?) before the x is throwing me off. I have never seen it used this way before and I have no idea what it means in this context. Could anyone help me out? Thanks.,I was working on some limit homework and everything was going fine until I reached this problem: $$\lim_{\Delta x \to 0} \frac{2(x + \Delta x) - 2x}{\Delta x}.$$ I am understanding limits but the triangle (delta?) before the x is throwing me off. I have never seen it used this way before and I have no idea what it means in this context. Could anyone help me out? Thanks.,,"['calculus', 'real-analysis', 'limits']"
14,The Multiplication Operator $M_f: L^2(\mu) \to L^2(\mu)$ such that $M_f g = fg$ (Rudin),The Multiplication Operator  such that  (Rudin),M_f: L^2(\mu) \to L^2(\mu) M_f g = fg,"This post is related but does not answer my question. Question 2 is partly unaddressed. Perhaps we can split into two cases, (i) where $\mu(\{f = 0\}) = 0$ and (ii) $\mu(\{f = 0\}) > 0$ ? If $\mu(\{f = 0\}) = 0$ , then $g = h/f$ a.e. Problem $5.17$ (paraphrased), Rudin's Real and Complex Analysis. If $\mu$ is a positive measure, each $f\in L^\infty(\mu)$ defines a multiplication operator $M_f$ on $L^2(\mu)$ into $L^2(\mu)$ , such that $M_f(g) = fg$ . Prove that $\|M_f\| \le \|f\|_\infty$ . I have shown that $\|M_f\| \le \|f\|_\infty$ . Question 1: For which measures $\mu$ is it true that $\|M_f\| = \|f\|_\infty$ for all $f\in L^\infty(\mu)$ ? Question 2: For which $f\in L^\infty(\mu)$ does $M_f$ map $L^2(\mu)$ onto $L^2(\mu)$ ? My work: We need to find measures $\mu$ for which the reverse inequality, i.e. $\|M_f\| \ge \|f\|_\infty$ also holds, for all $f\in L^\infty(\mu)$ . More explicitly, we need $$\sup\{\|M_f g\|_2: g\in L^2(\mu), \|g\|_2 \le 1\} = \|f\|_\infty \quad (f\in L^\infty(\mu))$$ where $$\|f\|_\infty = \inf\{a\in\mathbb R: \mu(\{|f| > a\}) = 0\}$$ Consider $h\in L^2(\mu)$ . We want $g\in L^2(\mu)$ such that $M_f g = fg = h$ . If $1/f \in L^\infty(\mu)$ , the map is surjective. Is the converse true? If not, could one help me get an iff condition without assuming anything on $\mu$ besides that it is a positive measure? I'd appreciate any help, thank you! Update: The proof below by Jose Avilez, and Jean L., Theorem $1.5$ tells us that $\|M_f\| = \|f\|_\infty$ for all $f\in L^\infty(\mu)$ iff $\mu$ is semi-finite.","This post is related but does not answer my question. Question 2 is partly unaddressed. Perhaps we can split into two cases, (i) where and (ii) ? If , then a.e. Problem (paraphrased), Rudin's Real and Complex Analysis. If is a positive measure, each defines a multiplication operator on into , such that . Prove that . I have shown that . Question 1: For which measures is it true that for all ? Question 2: For which does map onto ? My work: We need to find measures for which the reverse inequality, i.e. also holds, for all . More explicitly, we need where Consider . We want such that . If , the map is surjective. Is the converse true? If not, could one help me get an iff condition without assuming anything on besides that it is a positive measure? I'd appreciate any help, thank you! Update: The proof below by Jose Avilez, and Jean L., Theorem tells us that for all iff is semi-finite.","\mu(\{f = 0\}) = 0 \mu(\{f = 0\}) > 0 \mu(\{f = 0\}) = 0 g = h/f 5.17 \mu f\in L^\infty(\mu) M_f L^2(\mu) L^2(\mu) M_f(g) = fg \|M_f\| \le \|f\|_\infty \|M_f\| \le \|f\|_\infty \mu \|M_f\| = \|f\|_\infty f\in L^\infty(\mu) f\in L^\infty(\mu) M_f L^2(\mu) L^2(\mu) \mu \|M_f\| \ge \|f\|_\infty f\in L^\infty(\mu) \sup\{\|M_f g\|_2: g\in L^2(\mu), \|g\|_2 \le 1\} = \|f\|_\infty \quad (f\in L^\infty(\mu)) \|f\|_\infty = \inf\{a\in\mathbb R: \mu(\{|f| > a\}) = 0\} h\in L^2(\mu) g\in L^2(\mu) M_f g = fg = h 1/f \in L^\infty(\mu) \mu 1.5 \|M_f\| = \|f\|_\infty f\in L^\infty(\mu) \mu","['real-analysis', 'functional-analysis', 'measure-theory']"
15,Intuition on how $L^2$ difer from $L^1$ Lebesgue function spaces,Intuition on how  difer from  Lebesgue function spaces,L^2 L^1,"I have read the Wikipedia page on the topic, and the answers on this post focusing solely on $L^1$ . The integral definitions are very clear, and the fact that $L^2$ has a natural norm induced by the scalar product, which is important in Fourier tranforms, and other bits of knowledge around these definitions, still leave me wanting for a couple of plots of functions, one for each space, as well as a short definition in English, even if only approximate. To some degree, and leaving aside measure theory, which would likely have pre-empted this post to begin with, the absolute value of the functions may seem to serve a similar purpose to squaring. Yet, these are completely separate spaces. If it wasn't clear enough from the tone of my answer, or my self-deprecating profile, I have no formal training in mathematics, so I'm looking for answers along the vein of the notable Terry Tao's explanation here: I’ll start today with my article on “Function spaces“. Just as the analysis of numerical quantities relies heavily on the concept of magnitude or absolute value to measure the size of such quantities, or the extent to which two such quantities are close to each other, the analysis of functions relies on the concept of a norm to measure various “sizes” of such functions, as well as the extent to which two functions resemble to each other. But while numbers mainly have just one notion of magnitude (not counting the $p$ -adic valuations, which are of importance in number theory), functions have a wide variety of such magnitudes, such as “height” ( $L^\infty$ or $C^0$ norm), “mass” ( $L^1$ norm), “mean square” or “energy” ( $L^2$ or $H^1$ norms), “slope” (Lipschitz or $C^1$ norms), and so forth. In modern mathematics, we use the framework of function spaces to understand the properties of functions and their magnitudes; they provide a precise and rigorous way to formalise such “fuzzy” notions as a function being tall, thin, flat, smooth, oscillating, etc. Evidently my question here is a bit more concrete, but not far off from what Professor Tao is addressing in the most natural, uncondescending and didactic way to capture as broad a segment of his blog's readers as possible. Clearly he is not talking at the audience, seeking acclaim from the initiated, but rather communicating effectively. The first $1/2$ of the answer I am looking for would be this motivation for $L^2$ : Roughly speaking, $L^2$ space is the only functional space among $L^p$ spaces which is a Hilbert space, i.e. it has an inner product (and also complete)! One can imagine this spaces as a generalization of $\mathbb R^n$ to infinite dimensional cases. So many trends like finding minimum/maximum of function from $\mathbb R^n$ to $\mathbb R$ can be generalized to these spaces in a similar way... But I am looking for more motivation behind the comment, "" $L^1$ and $L^2$ are not ""completely separate"", in that $L^1 \cap L^2$ is a ""very large"" set."" Is $\{L^1\} > \{L^2\}$ ? How much bigger (or smaller)? What does it mean that the former measures the mean, while them latter, the energy? Etc. There is this post in the Physics SE , also delving into $L^2,$ but again, not making an intuitive comparison with $L^1.$","I have read the Wikipedia page on the topic, and the answers on this post focusing solely on . The integral definitions are very clear, and the fact that has a natural norm induced by the scalar product, which is important in Fourier tranforms, and other bits of knowledge around these definitions, still leave me wanting for a couple of plots of functions, one for each space, as well as a short definition in English, even if only approximate. To some degree, and leaving aside measure theory, which would likely have pre-empted this post to begin with, the absolute value of the functions may seem to serve a similar purpose to squaring. Yet, these are completely separate spaces. If it wasn't clear enough from the tone of my answer, or my self-deprecating profile, I have no formal training in mathematics, so I'm looking for answers along the vein of the notable Terry Tao's explanation here: I’ll start today with my article on “Function spaces“. Just as the analysis of numerical quantities relies heavily on the concept of magnitude or absolute value to measure the size of such quantities, or the extent to which two such quantities are close to each other, the analysis of functions relies on the concept of a norm to measure various “sizes” of such functions, as well as the extent to which two functions resemble to each other. But while numbers mainly have just one notion of magnitude (not counting the -adic valuations, which are of importance in number theory), functions have a wide variety of such magnitudes, such as “height” ( or norm), “mass” ( norm), “mean square” or “energy” ( or norms), “slope” (Lipschitz or norms), and so forth. In modern mathematics, we use the framework of function spaces to understand the properties of functions and their magnitudes; they provide a precise and rigorous way to formalise such “fuzzy” notions as a function being tall, thin, flat, smooth, oscillating, etc. Evidently my question here is a bit more concrete, but not far off from what Professor Tao is addressing in the most natural, uncondescending and didactic way to capture as broad a segment of his blog's readers as possible. Clearly he is not talking at the audience, seeking acclaim from the initiated, but rather communicating effectively. The first of the answer I am looking for would be this motivation for : Roughly speaking, space is the only functional space among spaces which is a Hilbert space, i.e. it has an inner product (and also complete)! One can imagine this spaces as a generalization of to infinite dimensional cases. So many trends like finding minimum/maximum of function from to can be generalized to these spaces in a similar way... But I am looking for more motivation behind the comment, "" and are not ""completely separate"", in that is a ""very large"" set."" Is ? How much bigger (or smaller)? What does it mean that the former measures the mean, while them latter, the energy? Etc. There is this post in the Physics SE , also delving into but again, not making an intuitive comparison with","L^1 L^2 p L^\infty C^0 L^1 L^2 H^1 C^1 1/2 L^2 L^2 L^p \mathbb R^n \mathbb R^n \mathbb R L^1 L^2 L^1 \cap L^2 \{L^1\} > \{L^2\} L^2, L^1.","['real-analysis', 'functional-analysis', 'vector-spaces', 'soft-question', 'normed-spaces']"
16,"Limit of $f:\mathbb{R}\to\mathbb{R}$ such that $f(x) = \frac1{q_n^5}$, if $x=\frac{p_n}{q_n}$ for $x \to \sqrt{2}$","Limit of  such that , if  for",f:\mathbb{R}\to\mathbb{R} f(x) = \frac1{q_n^5} x=\frac{p_n}{q_n} x \to \sqrt{2},"Let $p_n$ and $q_n$ two successions of integer numbers such that $q_n > 0$ and such that $(p_n, q_n) = 1$ for all indexes $n$ . Define $f(x) := \begin{cases}\frac1{q_n^5} \quad x=\frac{p_n}{q_n} \\[6pt] 0 \quad x \in \mathbb{R}-\mathbb{Q}\end{cases}$ Prove that $\lim_{x \to \sqrt{2}} \frac{f(x)}{(x-\sqrt2)^2}$ exists. Can you compute it? If we know the limit exists, then we might be able to apply Heine's criteria for limits of functions using limits of sequences to find the limit by using a rational sequence $(x_n) \to \sqrt2$ and using the definition of the function. However, I don't know how to approach proving the existence of the limit. Also, we know that if a sequence $(\frac{p_n}{q_n})$ , where $(p_n, q_n) = 1, p_n,q_n \in \mathbb{Z}, q_n > 0$ converges to an irrational number, then $\lim_{n \to \infty} q_n = \infty$ . Can you help me with this?","Let and two successions of integer numbers such that and such that for all indexes . Define Prove that exists. Can you compute it? If we know the limit exists, then we might be able to apply Heine's criteria for limits of functions using limits of sequences to find the limit by using a rational sequence and using the definition of the function. However, I don't know how to approach proving the existence of the limit. Also, we know that if a sequence , where converges to an irrational number, then . Can you help me with this?","p_n q_n q_n > 0 (p_n, q_n) = 1 n f(x) := \begin{cases}\frac1{q_n^5} \quad x=\frac{p_n}{q_n} \\[6pt]
0 \quad x \in \mathbb{R}-\mathbb{Q}\end{cases} \lim_{x \to \sqrt{2}} \frac{f(x)}{(x-\sqrt2)^2} (x_n) \to \sqrt2 (\frac{p_n}{q_n}) (p_n, q_n) = 1, p_n,q_n \in \mathbb{Z}, q_n > 0 \lim_{n \to \infty} q_n = \infty","['real-analysis', 'sequences-and-series', 'limits']"
17,"Prove that $S = \{ f: [0,1]\rightarrow \mathbb{R} \ \text{continuous} : x\in\mathbb{Q}\implies f(x) \in \mathbb{Q}\}$ is. uncountable",Prove that  is. uncountable,"S = \{ f: [0,1]\rightarrow \mathbb{R} \ \text{continuous} : x\in\mathbb{Q}\implies f(x) \in \mathbb{Q}\}","Prove that $$S = \{ f: [0,1]\rightarrow \mathbb{R} \ \text{continuous} : x\in\mathbb{Q}\implies f(x) \in \mathbb{Q}\}$$ is uncountable. I know that reals are uncountable,  so I look for injection from reals to set $S$ , i.e., if we can define a function as above for each real number, then we are done. Am I going in the right direction? Please help with some hint/solution.","Prove that is uncountable. I know that reals are uncountable,  so I look for injection from reals to set , i.e., if we can define a function as above for each real number, then we are done. Am I going in the right direction? Please help with some hint/solution.","S = \{ f: [0,1]\rightarrow \mathbb{R} \ \text{continuous} : x\in\mathbb{Q}\implies f(x) \in \mathbb{Q}\} S","['real-analysis', 'elementary-set-theory', 'continuity']"
18,How to evaluate $\int_0^{\pi/2} x\ln^2(\sin x)\textrm{d}x$ in a different way?,How to evaluate  in a different way?,\int_0^{\pi/2} x\ln^2(\sin x)\textrm{d}x,"The following problem \begin{align} &\int_{0}^{\pi/2} x\ln^{2}\left(\sin\left(x\right)\right)\,{\rm d}x \\[5mm] = & \ \frac{1}{2}\ln^{2}\left(2\right)\zeta\left(2\right) - \frac{19}{32}\,\zeta\left(4\right) + \frac{1}{24}\,\ln^{4}\left(2\right) + \operatorname{Li}_{4}\left(\frac{1}{2}\right) \label{1}\tag1 \end{align} was already solved in this solution. The question here is how to prove $(\ref{1})$ by utilizing the Fourier series of \begin{align} &\tan\left(x\right) \ln\left(\sin\left(x\right)\right) = -\sum_{n=1}^\infty\left[% \psi\left(\frac{n+1}{2}\right) - \psi\left(\frac{n}{2}\right)-\frac1n\right]\sin(2nx) \\[5mm] = & \ -\sum_{n=1}^\infty\left(\int_{0}^{1}\frac{1 - t}{1 + t}\,\,t^{n - 1}\,\,{\rm d}t\right) \sin\left(2nx\right),\quad 0 < x <\frac{\pi}{2} \end{align} I wonder what kind of clever manipulation we need to do to create the integrand in $(1)$ . I am sure it would be an amazing solution. Thank you in advance. This Fourier series can be found in the book, Almost Impossible Integrals, Sums and series , page $243$ ,  Eq $(3.281)$ .","The following problem was already solved in this solution. The question here is how to prove by utilizing the Fourier series of I wonder what kind of clever manipulation we need to do to create the integrand in . I am sure it would be an amazing solution. Thank you in advance. This Fourier series can be found in the book, Almost Impossible Integrals, Sums and series , page ,  Eq .","\begin{align}
&\int_{0}^{\pi/2}
x\ln^{2}\left(\sin\left(x\right)\right)\,{\rm d}x \\[5mm] = & \
\frac{1}{2}\ln^{2}\left(2\right)\zeta\left(2\right)
- \frac{19}{32}\,\zeta\left(4\right) +
\frac{1}{24}\,\ln^{4}\left(2\right)
+ \operatorname{Li}_{4}\left(\frac{1}{2}\right)
\label{1}\tag1
\end{align} (\ref{1}) \begin{align}
&\tan\left(x\right)
\ln\left(\sin\left(x\right)\right) =
-\sum_{n=1}^\infty\left[%
\psi\left(\frac{n+1}{2}\right) -
\psi\left(\frac{n}{2}\right)-\frac1n\right]\sin(2nx)
\\[5mm] = & \
-\sum_{n=1}^\infty\left(\int_{0}^{1}\frac{1 - t}{1 + t}\,\,t^{n - 1}\,\,{\rm d}t\right)
\sin\left(2nx\right),\quad 0 < x <\frac{\pi}{2}
\end{align} (1) 243 (3.281)","['real-analysis', 'integration', 'alternative-proof', 'polylogarithm', 'digamma-function']"
19,Intuition for $\lim\sup$ and $\lim\inf$,Intuition for  and,\lim\sup \lim\inf,"After reading several alternative definitions of $\lim\sup$ and $\lim\inf$ , such as $\lim\sup$ being the supremum of the set of all subsequential limits, I'm still having trouble building the intuition for $\lim\sup$ . One thing that I feel is true, but not sure, is that $\lim\sup$ represents the greatest real number that infinitely many $a_n$ gets close to, and $\lim\inf$ represents the smallest value that infinitely many $a_n$ gets close to. Are these correct statements? If so, how would one go about showing it?  Thanks","After reading several alternative definitions of and , such as being the supremum of the set of all subsequential limits, I'm still having trouble building the intuition for . One thing that I feel is true, but not sure, is that represents the greatest real number that infinitely many gets close to, and represents the smallest value that infinitely many gets close to. Are these correct statements? If so, how would one go about showing it?  Thanks",\lim\sup \lim\inf \lim\sup \lim\sup \lim\sup a_n \lim\inf a_n,['real-analysis']
20,Proving that: $\forall n \in \mathbb{N} :|f^{(n)}(x)|\leq \frac{1}{n+1}$,Proving that:,\forall n \in \mathbb{N} :|f^{(n)}(x)|\leq \frac{1}{n+1},"Suppose $x>0$ and we have function $f(x)=\frac{\sin x}{x}$ how can we show $$\forall n \in \mathbb{N} :|f^{(n)}(x)|\leq \frac{1}{n+1}$$ I need a hint to show this property .Thanks in advance . I tried for $n=1 ,2$  by finding maximum of $|f'| ,|f''|$ but I get stuck to show for $n$","Suppose $x>0$ and we have function $f(x)=\frac{\sin x}{x}$ how can we show $$\forall n \in \mathbb{N} :|f^{(n)}(x)|\leq \frac{1}{n+1}$$ I need a hint to show this property .Thanks in advance . I tried for $n=1 ,2$  by finding maximum of $|f'| ,|f''|$ but I get stuck to show for $n$",,"['calculus', 'real-analysis', 'derivatives', 'inequality', 'proof-writing']"
21,"Does, $\lim\limits_{x \to +\infty } f'(x) = + \infty \Leftrightarrow \lim\limits_{x \to +\infty } \frac{{f(x)}}{x} = + \infty $?","Does, ?",\lim\limits_{x \to +\infty } f'(x) = + \infty \Leftrightarrow \lim\limits_{x \to +\infty } \frac{{f(x)}}{x} = + \infty ,"Let $f:\Bbb R \to \Bbb R$ be a differentiable function. If $\mathop {\lim }\limits_{x \to  + \infty } \frac{{f(x)}}{x} =  + \infty $, it is always true that $\mathop {\lim }\limits_{x \to  + \infty } f'(x) =  + \infty $? How about the converse? For example, $\mathop {\lim }\limits_{x \to  + \infty } \frac{{\ln x}}{x} = 0$ is finite, then we can see $\mathop {\lim }\limits_{x \to  + \infty } (\ln x)' = 0$ is finite. $\mathop {\lim }\limits_{x \to  + \infty } \frac{{{x^2}}}{x} =  + \infty $ so $\mathop {\lim }\limits_{x \to  + \infty } ({x^2})' = \mathop {\lim }\limits_{x \to  + \infty } x =  + \infty $. So the claim seems good to me, but I don't know how to actually prove it. $\mathop {\lim }\limits_{x \to  + \infty } f'(x) = \mathop {\lim }\limits_{x \to \infty } \mathop {\lim }\limits_{h \to 0} \frac{{f(x + h) - f(x)}}{h}$, I don't know how to deal with this mixed limit. Also since the limits in the proposition diverges, it looks like mean value theorem sort of thing cannot apply here.","Let $f:\Bbb R \to \Bbb R$ be a differentiable function. If $\mathop {\lim }\limits_{x \to  + \infty } \frac{{f(x)}}{x} =  + \infty $, it is always true that $\mathop {\lim }\limits_{x \to  + \infty } f'(x) =  + \infty $? How about the converse? For example, $\mathop {\lim }\limits_{x \to  + \infty } \frac{{\ln x}}{x} = 0$ is finite, then we can see $\mathop {\lim }\limits_{x \to  + \infty } (\ln x)' = 0$ is finite. $\mathop {\lim }\limits_{x \to  + \infty } \frac{{{x^2}}}{x} =  + \infty $ so $\mathop {\lim }\limits_{x \to  + \infty } ({x^2})' = \mathop {\lim }\limits_{x \to  + \infty } x =  + \infty $. So the claim seems good to me, but I don't know how to actually prove it. $\mathop {\lim }\limits_{x \to  + \infty } f'(x) = \mathop {\lim }\limits_{x \to \infty } \mathop {\lim }\limits_{h \to 0} \frac{{f(x + h) - f(x)}}{h}$, I don't know how to deal with this mixed limit. Also since the limits in the proposition diverges, it looks like mean value theorem sort of thing cannot apply here.",,"['calculus', 'real-analysis', 'limits']"
22,What is the integral of a vector-valued function?,What is the integral of a vector-valued function?,,what does it mathematically mean an integral of a vector ? for example in physics we say that the impulse $\vec{I}$ is the time integral of force $\vec{f}$ : $\vec{I} = \int_{t_{1}}^{t_{2}} \vec{f} dt$ what does this object $~~ \int_{t_{1}}^{t_{2}} \vec{f} dt$ clearly mean ? is it a vector ?  does it have the same direction as $\vec{I}$ ?,what does it mathematically mean an integral of a vector ? for example in physics we say that the impulse $\vec{I}$ is the time integral of force $\vec{f}$ : $\vec{I} = \int_{t_{1}}^{t_{2}} \vec{f} dt$ what does this object $~~ \int_{t_{1}}^{t_{2}} \vec{f} dt$ clearly mean ? is it a vector ?  does it have the same direction as $\vec{I}$ ?,,['real-analysis']
23,continuity at isolated point [duplicate],continuity at isolated point [duplicate],,"This question already has answers here : Can a function with just one point in its domain be continuous? (6 answers) Closed 8 years ago . In our booklet it is written that : A function is  continuous at every isolated point. MY doubt :- Let us consider an example: let $f:\mathbb N \rightarrow \mathbb{R} $ such that $f(x)=x$ . As $\ \mathbb N =\{1,2,3,...\}$ and $1,2,3,..$ are all isolated points i.e $1,2,3... $ are not limit points of $\mathbb N $ . Now according to the above statement the function is continuous at every isolated point. But according to the definition of continuity, the continuity at point $a$ is $$\lim_{x\to a}f(x)=f(a)$$ Now take any number from  set $\mathbb N $ , for example take $ 2$ then $f(2)=2$ and $$\lim_{x\to 2}f(x)=\text{not  possible  to  determine or cannot be evaluate }$$ More precisely $2$ is not a limit point, so limit at $2$ cannot be calculated. So, the function is not continuous at all isolated points in this example. How the function can be continuous at all isolated points? Can anyone tell me?","This question already has answers here : Can a function with just one point in its domain be continuous? (6 answers) Closed 8 years ago . In our booklet it is written that : A function is  continuous at every isolated point. MY doubt :- Let us consider an example: let such that . As and are all isolated points i.e are not limit points of . Now according to the above statement the function is continuous at every isolated point. But according to the definition of continuity, the continuity at point is Now take any number from  set , for example take then and More precisely is not a limit point, so limit at cannot be calculated. So, the function is not continuous at all isolated points in this example. How the function can be continuous at all isolated points? Can anyone tell me?","f:\mathbb N \rightarrow \mathbb{R}  f(x)=x \ \mathbb N =\{1,2,3,...\} 1,2,3,.. 1,2,3...  \mathbb N  a \lim_{x\to a}f(x)=f(a) \mathbb N   2 f(2)=2 \lim_{x\to 2}f(x)=\text{not  possible  to  determine or cannot be evaluate } 2 2","['real-analysis', 'continuity']"
24,limit $ \lim \limits_{n \to \infty} {\left(\frac{z^{1/\sqrt n} + z^{-1/\sqrt n}}{2}\right)^n} $,limit, \lim \limits_{n \to \infty} {\left(\frac{z^{1/\sqrt n} + z^{-1/\sqrt n}}{2}\right)^n} ,"Calculate the limit $ \displaystyle \lim \limits_{n \to \infty} {\left(\frac{z^{1/\sqrt n} + z^{-1/\sqrt n}}{2}\right)^n} $ I now the answer, it is $ \displaystyle e^\frac{\log^2z}{2} $, but I don't know how to prove it. It seems like this notable limit $\displaystyle \lim \limits_{x \to \infty} {\left(1 + \frac{c}{x}\right)^x} = e^c$ should be useful here. For example I tried this way: $$ (z^{1/\sqrt n} + z^{-1/\sqrt n}) = (z^{1/(2 \sqrt n)} - z^{-1/(2 \sqrt n)})^2 + 2 $$ $$ \displaystyle \lim \limits_{n \to \infty} {\left(\frac{z^{1/\sqrt n} + z^{-1/\sqrt n}}{2}\right)^n} = \displaystyle \lim \limits_{n \to \infty} {\left(1 + \frac{(z^{1/(2 \sqrt n)} - z^{-1/(2 \sqrt n)})^2}{2}\right)^n} $$ where $ (z^{1/(2 \sqrt n)} - z^{-1/(2 \sqrt n)})^2 $ seems close to $ \frac{\log^2 z}{n} $. Also we can say that $$ \left(\frac{z^{1/\sqrt n} + z^{-1/\sqrt n}}{2}\right)^n = e^{n \log {\left(1 + \frac{\left(z^{1/(2 \sqrt n)} - z^{-1/(2 \sqrt n)}\right)^2}{2}\right)}}$$ and $ \log {\left(1 + \frac{(z^{1/(2 \sqrt n)} - z^{-1/(2 \sqrt n)})^2}{2}\right)} $ can be expand in the Taylor series. But I can't finish this ways. Thanks for the help!","Calculate the limit $ \displaystyle \lim \limits_{n \to \infty} {\left(\frac{z^{1/\sqrt n} + z^{-1/\sqrt n}}{2}\right)^n} $ I now the answer, it is $ \displaystyle e^\frac{\log^2z}{2} $, but I don't know how to prove it. It seems like this notable limit $\displaystyle \lim \limits_{x \to \infty} {\left(1 + \frac{c}{x}\right)^x} = e^c$ should be useful here. For example I tried this way: $$ (z^{1/\sqrt n} + z^{-1/\sqrt n}) = (z^{1/(2 \sqrt n)} - z^{-1/(2 \sqrt n)})^2 + 2 $$ $$ \displaystyle \lim \limits_{n \to \infty} {\left(\frac{z^{1/\sqrt n} + z^{-1/\sqrt n}}{2}\right)^n} = \displaystyle \lim \limits_{n \to \infty} {\left(1 + \frac{(z^{1/(2 \sqrt n)} - z^{-1/(2 \sqrt n)})^2}{2}\right)^n} $$ where $ (z^{1/(2 \sqrt n)} - z^{-1/(2 \sqrt n)})^2 $ seems close to $ \frac{\log^2 z}{n} $. Also we can say that $$ \left(\frac{z^{1/\sqrt n} + z^{-1/\sqrt n}}{2}\right)^n = e^{n \log {\left(1 + \frac{\left(z^{1/(2 \sqrt n)} - z^{-1/(2 \sqrt n)}\right)^2}{2}\right)}}$$ and $ \log {\left(1 + \frac{(z^{1/(2 \sqrt n)} - z^{-1/(2 \sqrt n)})^2}{2}\right)} $ can be expand in the Taylor series. But I can't finish this ways. Thanks for the help!",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
25,Question about proof: Uniform cauchy $\Rightarrow$ Uniform convergence,Question about proof: Uniform cauchy  Uniform convergence,\Rightarrow,"I have one quick question regarding the proof of a theorem contained in these notes. Theorem 5.13. A sequence $(f_n)$ of functions $f_n : A → R$ converges uniformly on $A$ if and only if it is uniformly Cauchy on $A$ . Question: In the triangle inequality part $|f_n(x) - f(x)| \leq |f_n(x) - f_m(x)| + |f_m(x) - f(x)|$ Why do we know that $f_m(x) \to f(x)$ as $m \to \infty$ so that $|f_m(x) - f(x)| < \frac{\epsilon}{2}$ I mean, isn't $|f_m(x) - f(x)|$ basically the same as $|f_n(x) - f(x)|$ on the left hand side? Just one is indexed by $m$ instead $n$ ? Why do we know that $|f_m(x) - f(x)| < \epsilon$ but we do not know immediately whether $|f_n(x) - f(x)| < \epsilon$","I have one quick question regarding the proof of a theorem contained in these notes. Theorem 5.13. A sequence of functions converges uniformly on if and only if it is uniformly Cauchy on . Question: In the triangle inequality part Why do we know that as so that I mean, isn't basically the same as on the left hand side? Just one is indexed by instead ? Why do we know that but we do not know immediately whether",(f_n) f_n : A → R A A |f_n(x) - f(x)| \leq |f_n(x) - f_m(x)| + |f_m(x) - f(x)| f_m(x) \to f(x) m \to \infty |f_m(x) - f(x)| < \frac{\epsilon}{2} |f_m(x) - f(x)| |f_n(x) - f(x)| m n |f_m(x) - f(x)| < \epsilon |f_n(x) - f(x)| < \epsilon,"['real-analysis', 'sequences-and-series', 'uniform-convergence']"
26,Open set containing rationals but complement non-denumerable,Open set containing rationals but complement non-denumerable,,"I am taking Real Analysis classes and I got a homework that asks me: Give an example of an open set $\mathcal{A}$ such that $\mathcal{A}\supset\mathbb{Q}$ but $\mathbb{R}-\mathcal{A}$ is non-denumerable. My attempt: First let $\mathcal{A} = \bigcup(r_n-1/2^n,r_n+1/2^n)$ where $r_n$ is the $n$-th rational, this is a union of open sets so $\mathbb{R}-\mathcal{A}$ is closed. I have reasons to believe that such set is also non-denumerable (as seen here: Uncountable closed set of irrational numbers but I have no experience in measure theory, is there other way to prove it's non-denumerability? Is that an answer at all? Please excuse my bad english, thank you.","I am taking Real Analysis classes and I got a homework that asks me: Give an example of an open set $\mathcal{A}$ such that $\mathcal{A}\supset\mathbb{Q}$ but $\mathbb{R}-\mathcal{A}$ is non-denumerable. My attempt: First let $\mathcal{A} = \bigcup(r_n-1/2^n,r_n+1/2^n)$ where $r_n$ is the $n$-th rational, this is a union of open sets so $\mathbb{R}-\mathcal{A}$ is closed. I have reasons to believe that such set is also non-denumerable (as seen here: Uncountable closed set of irrational numbers but I have no experience in measure theory, is there other way to prove it's non-denumerability? Is that an answer at all? Please excuse my bad english, thank you.",,"['real-analysis', 'general-topology', 'examples-counterexamples']"
27,"Let $f$ be a continuous and differentiable function such that $f(a)=f(b)=0$ , show that $f'(c)=\pi f(c)$ for some $c \in (a,b)$","Let  be a continuous and differentiable function such that  , show that  for some","f f(a)=f(b)=0 f'(c)=\pi f(c) c \in (a,b)","Let $f:[a,b] \rightarrow \mathbb{R} $  be a continuous function in $[a,b]$ and differentiable in $(a,b)$   such that $f(a)=f(b)=0$ . Show that $f'(c)=\pi f(c)$ for some $c \in (a,b)$ The initial hypothesis of that $f(a)=f(b)$ makes me think that I need to use the Rolle's Theorem of some way but I don't see how to use it. Any hint will be appreciated.","Let $f:[a,b] \rightarrow \mathbb{R} $  be a continuous function in $[a,b]$ and differentiable in $(a,b)$   such that $f(a)=f(b)=0$ . Show that $f'(c)=\pi f(c)$ for some $c \in (a,b)$ The initial hypothesis of that $f(a)=f(b)$ makes me think that I need to use the Rolle's Theorem of some way but I don't see how to use it. Any hint will be appreciated.",,"['real-analysis', 'derivatives']"
28,Counterexample for Interchange of Limits in integration,Counterexample for Interchange of Limits in integration,,"If $f_n$ converges to $f$ uniformly in $\mathbb{R}$, then \begin{equation*} \lim_{n\to\infty}\int_a^b f_n(x)\,dx =\int_a^b f(x)\,dx \end{equation*} but it's not true in general that \begin{equation*} \lim_{n\to\infty }\int_{-\infty}^\infty f_n(x)\,dx =\int_{-\infty}^\infty f(x) \, dx. \end{equation*} I cannot think of any counterexample.","If $f_n$ converges to $f$ uniformly in $\mathbb{R}$, then \begin{equation*} \lim_{n\to\infty}\int_a^b f_n(x)\,dx =\int_a^b f(x)\,dx \end{equation*} but it's not true in general that \begin{equation*} \lim_{n\to\infty }\int_{-\infty}^\infty f_n(x)\,dx =\int_{-\infty}^\infty f(x) \, dx. \end{equation*} I cannot think of any counterexample.",,['real-analysis']
29,Finding the closed form of $\int_0^1 \frac{(1-x+x\log x)\operatorname{Li}_3(x)}{x(x-1) \log x} \ dx$,Finding the closed form of,\int_0^1 \frac{(1-x+x\log x)\operatorname{Li}_3(x)}{x(x-1) \log x} \ dx,"$\def\Li{{\rm{Li}}}$Here I have a question I just received, and  still trying to find a proper starting point $$\int_0^1 \frac{(1-x+x\log x)\Li_3(x)}{x(x-1) \log x} \ dx$$ What starting point would you propose?","$\def\Li{{\rm{Li}}}$Here I have a question I just received, and  still trying to find a proper starting point $$\int_0^1 \frac{(1-x+x\log x)\Li_3(x)}{x(x-1) \log x} \ dx$$ What starting point would you propose?",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
30,Prove that $\int_{0}^{+\infty} u^{s-1} \cos (a u) \:e^{-b u}\:du=\frac{\Gamma(s)\cos\left(s\arctan \left(\frac{a}{b}\right)\right)}{(a^2+b^2)^{s/2}}$,Prove that,\int_{0}^{+\infty} u^{s-1} \cos (a u) \:e^{-b u}\:du=\frac{\Gamma(s)\cos\left(s\arctan \left(\frac{a}{b}\right)\right)}{(a^2+b^2)^{s/2}},"From the answer of this OP: Ramanujan log-trigonometric integrals , I found the following formula $$\begin{align} & \int_{0}^{+\infty} u^{s-1}  \cos (a u) \:e^{-b u}\:\mathrm{d}u = \Gamma (s)\frac{\cos \left(\! s \arctan \left(\frac{a}{b}\right)\right)}{(a^2+b^2)^{s/2}}, \, \left(\Re(s)>0, b>0, a>0 \right) \end{align}$$ How one proves that formula? I am interested in knowing the approach without using contour integration method. Thanks in advance.","From the answer of this OP: Ramanujan log-trigonometric integrals , I found the following formula $$\begin{align} & \int_{0}^{+\infty} u^{s-1}  \cos (a u) \:e^{-b u}\:\mathrm{d}u = \Gamma (s)\frac{\cos \left(\! s \arctan \left(\frac{a}{b}\right)\right)}{(a^2+b^2)^{s/2}}, \, \left(\Re(s)>0, b>0, a>0 \right) \end{align}$$ How one proves that formula? I am interested in knowing the approach without using contour integration method. Thanks in advance.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
31,Second Countability of Euclidean Spaces,Second Countability of Euclidean Spaces,,"Sorry I know this is a stupid question. However I got stuck on this for quite a while. I'm trying to prove that Euclidean spaces have a countable base, which can be constructed by taking all the open balls at all the rational points(points whose coordinates are rational) with all rational radius. However I find it hard to prove that any open set in the Euclidean space can be written as a union of such balls. Any help? Kind regards, Evariste","Sorry I know this is a stupid question. However I got stuck on this for quite a while. I'm trying to prove that Euclidean spaces have a countable base, which can be constructed by taking all the open balls at all the rational points(points whose coordinates are rational) with all rational radius. However I find it hard to prove that any open set in the Euclidean space can be written as a union of such balls. Any help? Kind regards, Evariste",,"['real-analysis', 'general-topology', 'analysis', 'metric-spaces']"
32,Value of $\sum_{k=1}^{\infty}\frac{1}{k^2+a^2}$,Value of,\sum_{k=1}^{\infty}\frac{1}{k^2+a^2},"So my question is to find the value of $\sum_{k=0}^\infty\frac{1}{k^2+1}$ and more generally $\sum_{k=0}^\infty\frac{1}{Q(k)}$  where Q is a quadratic polynomial with no zeroes on the integers. I can prove it converges, by the comparison test. I think I 've seen such a sum before, and I think it has a hyperbolic function solution. But the way they did it there involved complex analysis, and I'm not very comfortable with that.","So my question is to find the value of $\sum_{k=0}^\infty\frac{1}{k^2+1}$ and more generally $\sum_{k=0}^\infty\frac{1}{Q(k)}$  where Q is a quadratic polynomial with no zeroes on the integers. I can prove it converges, by the comparison test. I think I 've seen such a sum before, and I think it has a hyperbolic function solution. But the way they did it there involved complex analysis, and I'm not very comfortable with that.",,"['real-analysis', 'sequences-and-series']"
33,Proof for alternative definition of the derivative,Proof for alternative definition of the derivative,,"I read somewhere recently that you can define the derivative as follows: $$f'(x) = \lim_{h, k \to 0^+} \frac{f(x +h) - f(x - k)}{h + k}$$ I have been trying to prove this for about 2 hours, and can't seem to get it done. How should I proceed? Edit : Assume $f$ is differentiable for all $x \in (x - \delta, x + \delta)$ for some $\delta > 0$.","I read somewhere recently that you can define the derivative as follows: $$f'(x) = \lim_{h, k \to 0^+} \frac{f(x +h) - f(x - k)}{h + k}$$ I have been trying to prove this for about 2 hours, and can't seem to get it done. How should I proceed? Edit : Assume $f$ is differentiable for all $x \in (x - \delta, x + \delta)$ for some $\delta > 0$.",,"['real-analysis', 'analysis']"
34,Why is the Borel Algebra on R not equal the powerset?,Why is the Borel Algebra on R not equal the powerset?,,"The borel algebra on the topological space R is defined as the σ-algebra generated by the open sets (or, equivalently, by the closed sets). Logically, I thought that since this includes all the open sets (a,b) where a and b are real numbers, then, this would be equivalent to the power set. For example, the set (0.001, 0.0231) would be included as well as (-12, 19029) correct? I can't think of any set that would not be included. However, I have read that the Borel σ-algebra is not, in general, the whole power set. Can anyone give a gentle explanation as to why this is the case?","The borel algebra on the topological space R is defined as the σ-algebra generated by the open sets (or, equivalently, by the closed sets). Logically, I thought that since this includes all the open sets (a,b) where a and b are real numbers, then, this would be equivalent to the power set. For example, the set (0.001, 0.0231) would be included as well as (-12, 19029) correct? I can't think of any set that would not be included. However, I have read that the Borel σ-algebra is not, in general, the whole power set. Can anyone give a gentle explanation as to why this is the case?",,"['real-analysis', 'general-topology', 'measure-theory']"
35,Every real sequence is the derivative sequence of some function,Every real sequence is the derivative sequence of some function,,"I am looking for the proof of the following theorem: Let $(a_n)$ be a sequence of real   numbers.  Then there exists a function   $f$ which is infinitely differentiable   at 0, and  $$ \frac{d^nf}{dx^n}(0) =  a_n, \ \ \text{for all } n.$$ I would appreciate either a sketch of the proof or an online reference to it.  A general case is listed as Borel's lemma in Wikipedia, without proof. The hard part is when the power series $\sum_n \frac{a_n}{n!}x^n$ has a zero radius of convergence. Edit: Thanks for the answers!","I am looking for the proof of the following theorem: Let $(a_n)$ be a sequence of real   numbers.  Then there exists a function   $f$ which is infinitely differentiable   at 0, and  $$ \frac{d^nf}{dx^n}(0) =  a_n, \ \ \text{for all } n.$$ I would appreciate either a sketch of the proof or an online reference to it.  A general case is listed as Borel's lemma in Wikipedia, without proof. The hard part is when the power series $\sum_n \frac{a_n}{n!}x^n$ has a zero radius of convergence. Edit: Thanks for the answers!",,"['real-analysis', 'reference-request']"
36,Is the following piecewise-defined function smooth $\in C^\infty(\mathbb{R})$?,Is the following piecewise-defined function smooth ?,\in C^\infty(\mathbb{R}),"Is the function $$q(x)=\begin{cases} 1& x=0\\ 0& |x|\geq 1\\ \dfrac{1}{1+\exp\left(\dfrac{1-2|x|}{x^2-|x|}\right)}&\text{otherwise}\end{cases}$$ a smooth function class $C^\infty(\mathbb{R})$ ? I found this function as is shown in Wiki by choosing $f(x)=\exp\left(-\frac{1}{x}\right)\theta(x)$ with $\theta(x)$ the Heaviside step function , then making $g(x)=\frac{f(x)}{f(x)+f(1-x)}$ and then lets $q(x)=g\left(\frac{x-a}{b-a}\right)\cdot g\left(\frac{d-x}{d-c}\right)$ with $a=-1,\ b=c=0,\ d=1$ , as could be seen in Desmos . But later in this question I found in the answers and comments it have some issues at specific points were is undefined, so now I tried to fix them by defining the function piecewise. A simular construction is used in this answer in MSE to built an example of a smooth transition function so in the case of this $q(x)$ , in principle, the points were could be some issues should be only $x\in\{-1, 0,1\}$ , but since its defined piecewise now I am not sure if indeed this solve the situation making $q(x)\in C^\infty(\mathbb{R})$ a smooth function, or it still have issues that make it being non infinitely differentiable. I was able to check it possitively until the 7th derivative in Wolfram-Alpha . Motivation I am trying to understand the properties in order to figure out this other question .","Is the function a smooth function class ? I found this function as is shown in Wiki by choosing with the Heaviside step function , then making and then lets with , as could be seen in Desmos . But later in this question I found in the answers and comments it have some issues at specific points were is undefined, so now I tried to fix them by defining the function piecewise. A simular construction is used in this answer in MSE to built an example of a smooth transition function so in the case of this , in principle, the points were could be some issues should be only , but since its defined piecewise now I am not sure if indeed this solve the situation making a smooth function, or it still have issues that make it being non infinitely differentiable. I was able to check it possitively until the 7th derivative in Wolfram-Alpha . Motivation I am trying to understand the properties in order to figure out this other question .","q(x)=\begin{cases} 1& x=0\\ 0& |x|\geq 1\\ \dfrac{1}{1+\exp\left(\dfrac{1-2|x|}{x^2-|x|}\right)}&\text{otherwise}\end{cases} C^\infty(\mathbb{R}) f(x)=\exp\left(-\frac{1}{x}\right)\theta(x) \theta(x) g(x)=\frac{f(x)}{f(x)+f(1-x)} q(x)=g\left(\frac{x-a}{b-a}\right)\cdot g\left(\frac{d-x}{d-c}\right) a=-1,\ b=c=0,\ d=1 q(x) x\in\{-1, 0,1\} q(x)\in C^\infty(\mathbb{R})","['real-analysis', 'limits', 'continuity', 'smooth-functions', 'schwartz-space']"
37,Prove the existence of a point $c$.,Prove the existence of a point .,c,"Problem Let $f$ be a continuous function, $f:[0,1]\to\mathbb{R}$ with $\int_{0}^1 (2x-1)f(x) dx = 0$ . Prove that there exists a point c between $(0, 1)$ such that $\int_{0}^c (x-c)(f(x)-f(c)) dx = 0$ . This problem was given at a regional competition in Romania for $12$ th grade students. Attempt From Rolle's Theorem we have that there exists q such that $f(q) = (2q - 1)f(q) = 0$ , where we have $f(q)=0$ . My other attempt was: fix $p$ to $0.5$ and separate according to the Mean Value Theorem for Integrals $\int_{0}^1 (2x-1)f(x) dx = 0$ into $-f(c1) * \int_{0}^p (1-2x) dx$ $+ $ $f(c2) * \int_{p}^1 (2x-1) dx = 0$ where we get that $f(c1)=f(c2)$","Problem Let be a continuous function, with . Prove that there exists a point c between such that . This problem was given at a regional competition in Romania for th grade students. Attempt From Rolle's Theorem we have that there exists q such that , where we have . My other attempt was: fix to and separate according to the Mean Value Theorem for Integrals into where we get that","f f:[0,1]\to\mathbb{R} \int_{0}^1 (2x-1)f(x) dx = 0 (0, 1) \int_{0}^c (x-c)(f(x)-f(c)) dx = 0 12 f(q) = (2q - 1)f(q) = 0 f(q)=0 p 0.5 \int_{0}^1 (2x-1)f(x) dx = 0 -f(c1) * \int_{0}^p (1-2x) dx +  f(c2) * \int_{p}^1 (2x-1) dx = 0 f(c1)=f(c2)","['real-analysis', 'integration', 'definite-integrals', 'rolles-theorem']"
38,If $f$ is an infinitely differentiable function then does this hold?,If  is an infinitely differentiable function then does this hold?,f,"If $f$ is differentiable infinite number of times, then there exist $x\in (0,1)$ such that $$\frac{f(1)-f(x)}{x}=f'(x)$$ . I have tried to use Lagranges Mean Value Theorem . It lands me with $f(1)-f'(x)=(1-x)f'(x+\theta(1-x))$ where $\theta\in(0,1)$ . Then I thought that I could differentiate both sides again and again to get some form of $\theta^{n}$ but to no avail. Even the differential equation $xf'(x)+f(x)=f(1)$ admits a solution but I don't think that it will help me. Can anyone tell me how to proceed with this?","If is differentiable infinite number of times, then there exist such that . I have tried to use Lagranges Mean Value Theorem . It lands me with where . Then I thought that I could differentiate both sides again and again to get some form of but to no avail. Even the differential equation admits a solution but I don't think that it will help me. Can anyone tell me how to proceed with this?","f x\in (0,1) \frac{f(1)-f(x)}{x}=f'(x) f(1)-f'(x)=(1-x)f'(x+\theta(1-x)) \theta\in(0,1) \theta^{n} xf'(x)+f(x)=f(1)",['real-analysis']
39,Convex hull of a compact in $R^n$ is compact,Convex hull of a compact in  is compact,R^n,"Show that if $A \subset \mathbb{R}^n$ is compact then the convex hull of $A$ is compact. Solution: $A \subset \mathbb{R}^n$ is compact then is limited and closed, therefore $A \subset B_{[0,r]}$ (Ball closed center $0$ and radius $r$ ) for some $r>0$ . Since $B_{[0,r]}$ is convex and $Conv(A)$ is the least convex that contains $A$ : $Conv(A) \subset B_{[0,r]}$ . It remains to show that $Conv (A)$ is closed, how do I do this?","Show that if is compact then the convex hull of is compact. Solution: is compact then is limited and closed, therefore (Ball closed center and radius ) for some . Since is convex and is the least convex that contains : . It remains to show that is closed, how do I do this?","A \subset \mathbb{R}^n A A \subset \mathbb{R}^n A \subset B_{[0,r]} 0 r r>0 B_{[0,r]} Conv(A) A Conv(A) \subset B_{[0,r]} Conv (A)","['real-analysis', 'general-topology', 'functional-analysis']"
40,"If a set is open, does it mean that every point is an interior point? [duplicate]","If a set is open, does it mean that every point is an interior point? [duplicate]",,"This question already has answers here : Are ""if"" and ""iff"" interchangeable in definitions? (15 answers) Closed 6 years ago . In Walter Rudin's Principles of Mathematical Analysis he defines open set as: ""E is open if every point of E is an interior point of E."" So this can be translated in logic as "" If every point of E is an interior point of E, then E is open."" But does this mean that "" If E is open, then every point of E is an interior point of E?"" How is one sure when encountering these definitions that the converse also applies?","This question already has answers here : Are ""if"" and ""iff"" interchangeable in definitions? (15 answers) Closed 6 years ago . In Walter Rudin's Principles of Mathematical Analysis he defines open set as: ""E is open if every point of E is an interior point of E."" So this can be translated in logic as "" If every point of E is an interior point of E, then E is open."" But does this mean that "" If E is open, then every point of E is an interior point of E?"" How is one sure when encountering these definitions that the converse also applies?",,['real-analysis']
41,A converse proposition to the Mean Value Theorem [duplicate],A converse proposition to the Mean Value Theorem [duplicate],,"This question already has answers here : Converse of mean value theorem (2 answers) Closed 6 years ago . This is just a question that popped up in my head while going through basic real analysis. The ordinary Mean Value Theorem (MVT) is given as follows. Let $f:[a,b] \to \mathbb{R}$ be a function satisfying the following conditions: $(i)$ $f$ is continuous on $[a,b]$ and $(ii)$ $f$ is differentiable on $(a,b)$. Then $\exists c \in (a,b),$ such that $$f(b)-f(a)=f'(c) \cdot (b-a)$$ Of course, we can restrict $f$ to any interval $[x,y] \subset [a,b]$ and apply the theorem on $f|_{[c,d]}$ to state that $\exists z \in (x,y)$ such that $f(y)-f(x)=f'(z) \cdot (y-x)$ Can I formulate a converse proposition as follows? Let $f:[a,b] \to \mathbb{R}$ be a function satisfying the following conditions: $(i)$ $f$ is continuous on $[a,b]$ and $(ii)$ $f$ is differentiable on $(a,b)$. Then $\forall c \in (a,b), \exists x,y \in [a,b]$ such that $$f(y)-f(x)=f'(c) \cdot (y-x)$$ Does it hold? If yes, how do I prove it? (How does one find out the points $x$ and $y$, which need not be unique, that work for a given point $c?)$ Does it hold under weaker assumptions on the function? If no, can you provide me a counter example? Can I impose stronger conditions to make it work? Any help would be much appreciated. Thank you. EDIT : As pointed out by @Henrik, the proposition does not hold with it's current assumptions. On the other hand, @MANMAID claims that it does hold with the additional assumption that $f$ has no point of inflection. It seems very interesting. I require a formal proof, though.","This question already has answers here : Converse of mean value theorem (2 answers) Closed 6 years ago . This is just a question that popped up in my head while going through basic real analysis. The ordinary Mean Value Theorem (MVT) is given as follows. Let $f:[a,b] \to \mathbb{R}$ be a function satisfying the following conditions: $(i)$ $f$ is continuous on $[a,b]$ and $(ii)$ $f$ is differentiable on $(a,b)$. Then $\exists c \in (a,b),$ such that $$f(b)-f(a)=f'(c) \cdot (b-a)$$ Of course, we can restrict $f$ to any interval $[x,y] \subset [a,b]$ and apply the theorem on $f|_{[c,d]}$ to state that $\exists z \in (x,y)$ such that $f(y)-f(x)=f'(z) \cdot (y-x)$ Can I formulate a converse proposition as follows? Let $f:[a,b] \to \mathbb{R}$ be a function satisfying the following conditions: $(i)$ $f$ is continuous on $[a,b]$ and $(ii)$ $f$ is differentiable on $(a,b)$. Then $\forall c \in (a,b), \exists x,y \in [a,b]$ such that $$f(y)-f(x)=f'(c) \cdot (y-x)$$ Does it hold? If yes, how do I prove it? (How does one find out the points $x$ and $y$, which need not be unique, that work for a given point $c?)$ Does it hold under weaker assumptions on the function? If no, can you provide me a counter example? Can I impose stronger conditions to make it work? Any help would be much appreciated. Thank you. EDIT : As pointed out by @Henrik, the proposition does not hold with it's current assumptions. On the other hand, @MANMAID claims that it does hold with the additional assumption that $f$ has no point of inflection. It seems very interesting. I require a formal proof, though.",,['real-analysis']
42,Questions on Convergence of an Infinite series,Questions on Convergence of an Infinite series,,"If the sequence of partial sums of an infinite series $$\sum_{n=1}^∞ a_n$$ is bounded, show that $$\sum_{n=1}^∞ a_ne^{-nt}$$ is convergent for $t>0$ Request: Actually I don't need a direct answer, I'm studying infinite series since the last couple of days. So I don't fully understand this thing. That's why I want to understand the process to solve this type of problems. Thank you.","If the sequence of partial sums of an infinite series $$\sum_{n=1}^∞ a_n$$ is bounded, show that $$\sum_{n=1}^∞ a_ne^{-nt}$$ is convergent for $t>0$ Request: Actually I don't need a direct answer, I'm studying infinite series since the last couple of days. So I don't fully understand this thing. That's why I want to understand the process to solve this type of problems. Thank you.",,"['real-analysis', 'sequences-and-series', 'problem-solving']"
43,Prove $\csc(x)=\sum_{k=-\infty}^{\infty}\frac{(-1)^k}{x+k\pi}$,Prove,\csc(x)=\sum_{k=-\infty}^{\infty}\frac{(-1)^k}{x+k\pi},Prove $$\csc(x)=\sum_{k=-\infty}^{\infty}\frac{(-1)^k}{x+k\pi}$$ Hardy uses this fact without proof in a monograph on different ways to evaluate $\int_0^{\infty}\frac{\sin(x)}{x} dx$.,Prove $$\csc(x)=\sum_{k=-\infty}^{\infty}\frac{(-1)^k}{x+k\pi}$$ Hardy uses this fact without proof in a monograph on different ways to evaluate $\int_0^{\infty}\frac{\sin(x)}{x} dx$.,,"['real-analysis', 'sequences-and-series', 'analysis']"
44,"$f$ continuous on $(a,b)$ and $|f|$ differentiable on $(a,b)$; is $f$ differentiable in $(a,b)$?",continuous on  and  differentiable on ; is  differentiable in ?,"f (a,b) |f| (a,b) f (a,b)","Let $f:(a,b) \to \mathbb R$ be a continuous function such that $|f|$ is differentiable in $(a,b)$ ; then is $f$ differentiable in $(a,b)$ ?","Let $f:(a,b) \to \mathbb R$ be a continuous function such that $|f|$ is differentiable in $(a,b)$ ; then is $f$ differentiable in $(a,b)$ ?",,['real-analysis']
45,"If a separately continuous function $f : [0,1]^2 \to \mathbb{R}$ vanishes on a dense set, must it vanish on the whole set?","If a separately continuous function  vanishes on a dense set, must it vanish on the whole set?","f : [0,1]^2 \to \mathbb{R}","Assume $f(x,y)$ is defined on $D=[0,1]\times[0,1]$, and $f(x,y)$ is continuous of each separate variables(i.e. if we fix $y$ to $y_0$, then $f(x,y_0)$ is continuous and vice versa). If $f(x,y)$ vanishes on a dense subset of $D$. Does $f$ vanishes on $D$?","Assume $f(x,y)$ is defined on $D=[0,1]\times[0,1]$, and $f(x,y)$ is continuous of each separate variables(i.e. if we fix $y$ to $y_0$, then $f(x,y_0)$ is continuous and vice versa). If $f(x,y)$ vanishes on a dense subset of $D$. Does $f$ vanishes on $D$?",,"['real-analysis', 'analysis', 'multivariable-calculus', 'continuity']"
46,Largest Triangular Number less than a Given Natural Number,Largest Triangular Number less than a Given Natural Number,,"I want to determine the closest Triangular number a particular natural number is.  For example, the first 10 triangular numbers are $1,3,6,10,15,21,28,36,45,55$ and thus, the number $57$ can be written as  $$57=T_{10}+2$$ The number $54$ can be written as $$54=T_{9}+9\neq T_{10}-1$$ The second part highlights that I am looking for Triangular numbers larger than a particular positive, and not necessarily ""closer"" in terms of distance from Triangular numbers. My approach would be this; given the $n$-th Triangular number has the formula $$T_n=\frac{n(n+1)}{2}=\binom{n+1}{2}$$ If I'm looking for a particular breakdown and close Triangular number, my number, say, $M$ will be of the form $$M=T_k+r$$ where $0\le r\le k$, and thus $$2(M-r)=k^2+k$$ And thus $$k=\frac{-1\pm\sqrt{1+8(M-r)}}{2}$$ I'm lost here in trying to solve, given that $r$ varies.  I know that $r$ is less than or equal to $k$ but for sufficiently large $M$, how would I go about finishing solving?","I want to determine the closest Triangular number a particular natural number is.  For example, the first 10 triangular numbers are $1,3,6,10,15,21,28,36,45,55$ and thus, the number $57$ can be written as  $$57=T_{10}+2$$ The number $54$ can be written as $$54=T_{9}+9\neq T_{10}-1$$ The second part highlights that I am looking for Triangular numbers larger than a particular positive, and not necessarily ""closer"" in terms of distance from Triangular numbers. My approach would be this; given the $n$-th Triangular number has the formula $$T_n=\frac{n(n+1)}{2}=\binom{n+1}{2}$$ If I'm looking for a particular breakdown and close Triangular number, my number, say, $M$ will be of the form $$M=T_k+r$$ where $0\le r\le k$, and thus $$2(M-r)=k^2+k$$ And thus $$k=\frac{-1\pm\sqrt{1+8(M-r)}}{2}$$ I'm lost here in trying to solve, given that $r$ varies.  I know that $r$ is less than or equal to $k$ but for sufficiently large $M$, how would I go about finishing solving?",,"['real-analysis', 'algebra-precalculus', 'number-theory']"
47,What does Dini continuity mean?,What does Dini continuity mean?,,What does Dini continuity (the integral condition) mean visually? Description of Dini contuity: https://en.wikipedia.org/wiki/Dini_continuity,What does Dini continuity (the integral condition) mean visually? Description of Dini contuity: https://en.wikipedia.org/wiki/Dini_continuity,,"['real-analysis', 'uniform-continuity', 'visualization', 'geometric-interpretation']"
48,Techniques to prove a function is uniformly continuous,Techniques to prove a function is uniformly continuous,,"So I understand the definition of uniform continuity, but wanted some suggestions to prove that a function is or isn't uniformly continuous. I have looked ahead and have seen that if a function is continuous on a compact domain, then the function is also uniformly continuous. But for the purposes of the examples below, I cannot use compactness, or the fact that a function is uniformly continuous if it is continuous and has a bounded derivative. Here's one example:  prove $f(x)= \sqrt{x}$ is uniformly continuous on $(0, \infty)$ . I have seen this question asked before on this forum, and the respondents used the traditional definition. Ie. they found a delta such that if $|x-y|<\delta$ , then $|\sqrt{x}-\sqrt{y}|< \epsilon$ . I was wondering if I could try this in a slightly different way. Suppose I prove that $f$ is continuous at $x_0$ , and the choice of $\delta$ doesn't depend on $x_0$ . After some scratch work, I proved that $f(x)$ is continuous at $x_0$ provided that $|x-x_0|< \min\{1,(\sqrt{x_0+1} + \sqrt{x_0})*\epsilon\}$ . Since $x_0$ is always greater than $0$ , we can say that $\delta= \min\{1, \epsilon\}$ . Because I proved that our choice of $\delta$ doesn't depend on $x_0$ , is this a valid proof that $f(x)$ is uniformly continuous? Here's another example: I want to prove that $f(x)=x^2$ is not uniformly continuous on $(0, \infty)$ . After some work, I proved that $f(x)$ is continuous at $x_0$ (in dom $f$ ), provided that $\delta< min \{1, \frac {\epsilon}{ (2x_0+1)}\}$ . Here, since our choice of delta does depend on $x_0$ , we can say the function is not uniformly continuous. So am I approaching these problems correctly? I realize that I have not been using the definition of uniformly continuity. (that if $f$ is uniformly continuous, then if $|x-y|<\delta$ , then $|f(x)-f(y)|< \epsilon$ . Also, sorry about the formatting. I realize I should learn proper formatting soon!. Also, I have read the following post regarding this topic: $\sqrt x$ is uniformly continuous","So I understand the definition of uniform continuity, but wanted some suggestions to prove that a function is or isn't uniformly continuous. I have looked ahead and have seen that if a function is continuous on a compact domain, then the function is also uniformly continuous. But for the purposes of the examples below, I cannot use compactness, or the fact that a function is uniformly continuous if it is continuous and has a bounded derivative. Here's one example:  prove is uniformly continuous on . I have seen this question asked before on this forum, and the respondents used the traditional definition. Ie. they found a delta such that if , then . I was wondering if I could try this in a slightly different way. Suppose I prove that is continuous at , and the choice of doesn't depend on . After some scratch work, I proved that is continuous at provided that . Since is always greater than , we can say that . Because I proved that our choice of doesn't depend on , is this a valid proof that is uniformly continuous? Here's another example: I want to prove that is not uniformly continuous on . After some work, I proved that is continuous at (in dom ), provided that . Here, since our choice of delta does depend on , we can say the function is not uniformly continuous. So am I approaching these problems correctly? I realize that I have not been using the definition of uniformly continuity. (that if is uniformly continuous, then if , then . Also, sorry about the formatting. I realize I should learn proper formatting soon!. Also, I have read the following post regarding this topic: $\sqrt x$ is uniformly continuous","f(x)= \sqrt{x} (0, \infty) |x-y|<\delta |\sqrt{x}-\sqrt{y}|< \epsilon f x_0 \delta x_0 f(x) x_0 |x-x_0|< \min\{1,(\sqrt{x_0+1} + \sqrt{x_0})*\epsilon\} x_0 0 \delta= \min\{1, \epsilon\} \delta x_0 f(x) f(x)=x^2 (0, \infty) f(x) x_0 f \delta< min \{1, \frac {\epsilon}{ (2x_0+1)}\} x_0 f |x-y|<\delta |f(x)-f(y)|< \epsilon","['real-analysis', 'uniform-continuity', 'epsilon-delta']"
49,"What is Lévy measure? Why is it needed, and what is $(1\wedge|x^2|)$?","What is Lévy measure? Why is it needed, and what is ?",(1\wedge|x^2|),"A Borel measure $\nu$ on $\mathbb{R}$ is called a Lévy measure if   $\nu({0})=0$ and $\int_\mathbb{R}(1\wedge|x^2|) \, \nu(dx) < \infty .$ ( https://en.wikipedia.org/wiki/Financial_models_with_long-tailed_distributions_and_volatility_clustering#Infinitely_divisible_distributions ) So, what exactly is $(1\wedge|x^2|)$? (Or rather correctly, what is the definition of levy measure saying?) Edit: OK, but then why is levy measure needed?","A Borel measure $\nu$ on $\mathbb{R}$ is called a Lévy measure if   $\nu({0})=0$ and $\int_\mathbb{R}(1\wedge|x^2|) \, \nu(dx) < \infty .$ ( https://en.wikipedia.org/wiki/Financial_models_with_long-tailed_distributions_and_volatility_clustering#Infinitely_divisible_distributions ) So, what exactly is $(1\wedge|x^2|)$? (Or rather correctly, what is the definition of levy measure saying?) Edit: OK, but then why is levy measure needed?",,"['real-analysis', 'measure-theory', 'economics']"
50,Calculate : $\int_1^{\infty} \frac{1}{x} -\sin^{-1} \frac{1}{x}\ \mathrm{d}x $,Calculate :,\int_1^{\infty} \frac{1}{x} -\sin^{-1} \frac{1}{x}\ \mathrm{d}x ,"Find : $\displaystyle \int_1^{\infty} \frac{1}{x} -\sin^{-1} \frac{1}{x}\ \mathrm{d}x $. I've done some work but I've got stuck, you may try to help me continue or give me another way , in both cases try to give me just hints (not the full solution), Thank you. My  work : setting : $x^{-1}=t$, we get : $\displaystyle \int_{0}^{1} \frac{t-\sin^{-1}t}{t^2}\ \mathrm{d}t$. for $|t|\leq 1 $, we have : $\displaystyle \sin^{-1}t =\sum_{k=0}^{\infty}\frac{(2k)!z^{2k+1}}{4^k (k!)^2(2k+1)}.$ with some simplification : $\displaystyle \int_{0}^{1} \frac{t-\sin^{-1}t}{t^2}\ \mathrm{d}t=-\sum_{k=1}^{\infty} \frac{(2k-1)!}{4^k (k!)^2(2k+1)}$. The first problem is that I don't have any idea how to prove the Taylor series (in the general form ) for $\sin^{-1} t$ and it seem quite complicated, I just have it in my book. The second problem is that I don't know how to evaluate the last sum. I hope you can have an easier solution.","Find : $\displaystyle \int_1^{\infty} \frac{1}{x} -\sin^{-1} \frac{1}{x}\ \mathrm{d}x $. I've done some work but I've got stuck, you may try to help me continue or give me another way , in both cases try to give me just hints (not the full solution), Thank you. My  work : setting : $x^{-1}=t$, we get : $\displaystyle \int_{0}^{1} \frac{t-\sin^{-1}t}{t^2}\ \mathrm{d}t$. for $|t|\leq 1 $, we have : $\displaystyle \sin^{-1}t =\sum_{k=0}^{\infty}\frac{(2k)!z^{2k+1}}{4^k (k!)^2(2k+1)}.$ with some simplification : $\displaystyle \int_{0}^{1} \frac{t-\sin^{-1}t}{t^2}\ \mathrm{d}t=-\sum_{k=1}^{\infty} \frac{(2k-1)!}{4^k (k!)^2(2k+1)}$. The first problem is that I don't have any idea how to prove the Taylor series (in the general form ) for $\sin^{-1} t$ and it seem quite complicated, I just have it in my book. The second problem is that I don't know how to evaluate the last sum. I hope you can have an easier solution.",,"['real-analysis', 'sequences-and-series', 'integration', 'definite-integrals']"
51,"Spivak's Calculus (Chapter 5, Problem 41): Proof that $\lim_{x \to a} x^2 = a^2$","Spivak's Calculus (Chapter 5, Problem 41): Proof that",\lim_{x \to a} x^2 = a^2,"In Chapter 5, Problem 41, Spivak provides an alternative way to prove that $$\lim_{x \rightarrow a} x^2 = a^2\,\,,\,\,a > 0$$ Given $\,\epsilon > 0\,$ let $$\delta = \min\left\{\sqrt{a^2 + \epsilon} - a, a - \sqrt{a^2 - \epsilon}\right\}$$ Then $$|x - a| < \delta\Longrightarrow \sqrt{a^2 - \epsilon} < x < \sqrt{a^2 + \epsilon}\Longrightarrow a^2 - \epsilon < x^2 < a^2 + \epsilon\,\,,\, |x^2 - a^2| < \epsilon$$ Then he goes on to claim that this proof is fallacious. But wherein lies the fallacy?","In Chapter 5, Problem 41, Spivak provides an alternative way to prove that $$\lim_{x \rightarrow a} x^2 = a^2\,\,,\,\,a > 0$$ Given $\,\epsilon > 0\,$ let $$\delta = \min\left\{\sqrt{a^2 + \epsilon} - a, a - \sqrt{a^2 - \epsilon}\right\}$$ Then $$|x - a| < \delta\Longrightarrow \sqrt{a^2 - \epsilon} < x < \sqrt{a^2 + \epsilon}\Longrightarrow a^2 - \epsilon < x^2 < a^2 + \epsilon\,\,,\, |x^2 - a^2| < \epsilon$$ Then he goes on to claim that this proof is fallacious. But wherein lies the fallacy?",,"['calculus', 'real-analysis', 'limits', 'continuity', 'epsilon-delta']"
52,"If $\lim_{x\to+\infty}[f(x+1)-f(x)]= \ell,$ then $\lim\limits_{x\to+\infty}\frac{f(x)}x=\ell$.",If  then .,"\lim_{x\to+\infty}[f(x+1)-f(x)]= \ell, \lim\limits_{x\to+\infty}\frac{f(x)}x=\ell","Let $f:[0,+\infty)\to\Bbb R$ be a function bounded on each finite interval. I want to show that if $\lim\limits_{x\to+\infty}[f(x+1)-f(x)]= L,$ then also $\lim\limits_{x\to+\infty}\dfrac{f(x)}x = L$",Let be a function bounded on each finite interval. I want to show that if then also,"f:[0,+\infty)\to\Bbb R \lim\limits_{x\to+\infty}[f(x+1)-f(x)]= L, \lim\limits_{x\to+\infty}\dfrac{f(x)}x = L","['real-analysis', 'analysis', 'limits']"
53,A simpler proof of Jensen's inequality,A simpler proof of Jensen's inequality,,"Jensen's inequality states that if $(X,\mu)$ is a measure space with $\mu(X) = 1$, $\phi$ is convex, and $f:X \rightarrow \mathbb R$ is integrable, then $$\phi\left(\int fd\mu\right) \leq \int \phi \circ fd\mu.$$ I am trying to find an alternative proof which doesn't require the fact that convex functions are differentiable. I first prove the result for simple functions: if $f = \sum_{i=1} ^n a_i \chi_{A_i} $ and the $A_i$ to be disjoint with $X = \bigsqcup_{i=1} ^n A_i$. Then $\sum_{i=1} ^n \mu(A_i) = 1$, so $$\phi\left(\int_X \sum_{i=1} ^n a_i \chi_{A_i} d\mu\right) = \phi\left(\sum_{i=1} ^n a_i \mu(A_i ) \right) \leq \sum_{i=1} ^n \phi(a_i) \mu(A_i ) = \int_X \phi \circ f d\mu .$$ I want to extend this to the general case by approximating $f$ with a sequence of simple functions $(f_n)$. However, I run into a problem showing that the integrals of $\phi \circ f_n$ converge to the integral of $\phi \circ f$. Any suggestions?","Jensen's inequality states that if $(X,\mu)$ is a measure space with $\mu(X) = 1$, $\phi$ is convex, and $f:X \rightarrow \mathbb R$ is integrable, then $$\phi\left(\int fd\mu\right) \leq \int \phi \circ fd\mu.$$ I am trying to find an alternative proof which doesn't require the fact that convex functions are differentiable. I first prove the result for simple functions: if $f = \sum_{i=1} ^n a_i \chi_{A_i} $ and the $A_i$ to be disjoint with $X = \bigsqcup_{i=1} ^n A_i$. Then $\sum_{i=1} ^n \mu(A_i) = 1$, so $$\phi\left(\int_X \sum_{i=1} ^n a_i \chi_{A_i} d\mu\right) = \phi\left(\sum_{i=1} ^n a_i \mu(A_i ) \right) \leq \sum_{i=1} ^n \phi(a_i) \mu(A_i ) = \int_X \phi \circ f d\mu .$$ I want to extend this to the general case by approximating $f$ with a sequence of simple functions $(f_n)$. However, I run into a problem showing that the integrals of $\phi \circ f_n$ converge to the integral of $\phi \circ f$. Any suggestions?",,"['real-analysis', 'measure-theory', 'inequality']"
54,"Evaluating $\int_0^\infty \left| \frac{\sin t}{t} \right|^n \, \mathrm{d}t$ for $n = 3, 5, 7, \dots$",Evaluating  for,"\int_0^\infty \left| \frac{\sin t}{t} \right|^n \, \mathrm{d}t n = 3, 5, 7, \dots","I would like to determine the general term of the following sequence defined by an infinite integral: $$ I_n = \int_0^\infty \left| \frac{\sin t}{t} \right|^n \, \mathrm{d}t \, , $$ wherein $n =3, 5, 7, \dots$ is an odd integer. It can be checked that the integral is convergent for all values of $n$ in the prescribed range. The case of even $n$ is solved in A sine integral $\int_0^{\infty} \left(\frac{\sin x }{x }\right)^n\,\mathrm{d}x$ . Also, $I_1 = \infty$ . I have tried to use the method of multiple integrations by parts but in vein. I was wondering whether there exists a suitable approach to address this problem more effectively.","I would like to determine the general term of the following sequence defined by an infinite integral: wherein is an odd integer. It can be checked that the integral is convergent for all values of in the prescribed range. The case of even is solved in A sine integral $\int_0^{\infty} \left(\frac{\sin x }{x }\right)^n\,\mathrm{d}x$ . Also, . I have tried to use the method of multiple integrations by parts but in vein. I was wondering whether there exists a suitable approach to address this problem more effectively.","
I_n = \int_0^\infty \left| \frac{\sin t}{t} \right|^n \, \mathrm{d}t \, ,
 n =3, 5, 7, \dots n n I_1 = \infty","['real-analysis', 'calculus', 'integration', 'definite-integrals', 'improper-integrals']"
55,"If integral is 0 on any set of measure 1/pi, then the function is 0 a.e.","If integral is 0 on any set of measure 1/pi, then the function is 0 a.e.",,"This is a problem in my Qualifying Exam. ""Suppose $f:[0,1]\to \mathbb{R}$ is in $L^1$ (Lebesgue measure) and for    every measurable $A\subset [0,1]$ with $m(A)=\frac 1{\pi}$ we have $\int_A f dm=0$ . Prove that $f=0$ a.e."" I could not do it back then. I did my research and we have a similar problem here Integral vanishes on all intervals implies the function is a.e. zero . But the same method cannot be applied. Anyway, I cannot think of anything except for let $B$ be a set of measure $1/4$ and try to make the integral 0. However, I forgot that this is on the real line, so there is no monotonicity here. Anyone can help?","This is a problem in my Qualifying Exam. ""Suppose is in (Lebesgue measure) and for    every measurable with we have . Prove that a.e."" I could not do it back then. I did my research and we have a similar problem here Integral vanishes on all intervals implies the function is a.e. zero . But the same method cannot be applied. Anyway, I cannot think of anything except for let be a set of measure and try to make the integral 0. However, I forgot that this is on the real line, so there is no monotonicity here. Anyone can help?","f:[0,1]\to \mathbb{R} L^1 A\subset [0,1] m(A)=\frac 1{\pi} \int_A f dm=0 f=0 B 1/4","['real-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
56,Uniqueness of the maximizer of the Hardy inequality,Uniqueness of the maximizer of the Hardy inequality,,"CONVENTION . All functions in this post are nonnegative and defined on $[0, \infty)$ . The Hardy operator is $$ Hf(x)=\frac1x\int_0^x f(y)\, dy, $$ and, as it is well known, it satisfies the inequality $$\tag{H} \|Hf\|_{p}\le \frac{p}{p-1}\|f\|_{p},\quad \text{for }p>1, $$ where the constant $p/(p-1)$ is the best possible, in the sense that (H) fails if it is replaced by any strictly smaller one. In a vague sense, the function $f(x)=x^{-1/p}$ is a maximizer for (H), if one ""neglects a logarithmic divergence in both sides"" (as said in this answer of Terry Tao ). More precisely, as shown, for example, in the answers to this old question of mine , some sequences that approximate $f$ also saturate (H); it is the case of $$ f_n(x)=x^{-\frac1p -\frac1n}\mathbf 1_{(1, \infty)} $$ and of $$ f_n(x)=x^{-\frac1p}\mathbf 1_{(1, n)}.$$ My question is, roughly speaking: is $f(x)=x^{-1/p}$ the ""only"" maximizer to (H)? More precisely: Question . Suppose that $$ \frac{\|Hf_n\|_p}{\|f_n\|_p}\to \frac{p}{p-1}.$$ Does there exist a sequence $\lambda_n>0$ such that $$f_n(\lambda_nx)\to x^{-\frac1 p}\mathbf 1_{(1, \infty)},$$ almost everywhere on $(0, \infty)$ ? Remark . The presence of $\lambda_n$ is necessary to prevent trivial counterexamples; indeed, the ratio $\|Hf\|_p / \|f\|_p$ is invariant under the scaling transformation $$f\mapsto f_\lambda(x)=f(\lambda x).$$ FINAL NOTE . David C. Ullrich gave an exhaustive negative answer to the present question. His answer was edited heavily and so it may be a bit hard to read. Please see my Summary to David C. Ullrich's answer .","CONVENTION . All functions in this post are nonnegative and defined on . The Hardy operator is and, as it is well known, it satisfies the inequality where the constant is the best possible, in the sense that (H) fails if it is replaced by any strictly smaller one. In a vague sense, the function is a maximizer for (H), if one ""neglects a logarithmic divergence in both sides"" (as said in this answer of Terry Tao ). More precisely, as shown, for example, in the answers to this old question of mine , some sequences that approximate also saturate (H); it is the case of and of My question is, roughly speaking: is the ""only"" maximizer to (H)? More precisely: Question . Suppose that Does there exist a sequence such that almost everywhere on ? Remark . The presence of is necessary to prevent trivial counterexamples; indeed, the ratio is invariant under the scaling transformation FINAL NOTE . David C. Ullrich gave an exhaustive negative answer to the present question. His answer was edited heavily and so it may be a bit hard to read. Please see my Summary to David C. Ullrich's answer .","[0, \infty) 
Hf(x)=\frac1x\int_0^x f(y)\, dy,  \tag{H}
\|Hf\|_{p}\le \frac{p}{p-1}\|f\|_{p},\quad \text{for }p>1,  p/(p-1) f(x)=x^{-1/p} f 
f_n(x)=x^{-\frac1p -\frac1n}\mathbf 1_{(1, \infty)}  
f_n(x)=x^{-\frac1p}\mathbf 1_{(1, n)}. f(x)=x^{-1/p}  \frac{\|Hf_n\|_p}{\|f_n\|_p}\to \frac{p}{p-1}. \lambda_n>0 f_n(\lambda_nx)\to x^{-\frac1 p}\mathbf 1_{(1, \infty)}, (0, \infty) \lambda_n \|Hf\|_p / \|f\|_p f\mapsto f_\lambda(x)=f(\lambda x).","['real-analysis', 'analysis', 'inequality', 'optimization', 'harmonic-analysis']"
57,Function $f$ s.t. $\lim_{x\to\infty}\frac{f(e^x)}{f(x)}=1$,Function  s.t.,f \lim_{x\to\infty}\frac{f(e^x)}{f(x)}=1,"The questions are: 1) Does there exists some function $f$ s.t.   $\lim_{x\to\infty}\frac{f(e^x)}{f(x)}=1$ and   $\lim_{x\to\infty}f(x)=\infty$? 2) Is $\big(\sum_{k=n}^{2^n}a_k\big)_n\to0$ is sufficient to guarentee   the convergence of $\sum_{n=1}^\infty a_n$ for $(a_n)_{n\in\mathbb > N}\ge0$? I recently discovered that $(a_n)_{n\in\mathbb N}\ge 0$ and $\big(\sum_{k=n}^{2n}a_k\big)_{n\in\mathbb N}\to 0$ doesn't implies $\sum_{n=1}^\infty a_n\in\mathbb R$, which is done by setting $(a_n)_{n\ge2}=\frac{1}{n\ln n}$. By setting $(a_n)=\frac{1}{n\ln n \ln\ln n}$, we can't guarentee convergence of $\sum_{n=1}^\infty a_n$ even if  $\big(\sum_{k=n}^{n^2}a_k\big)_n\to 0$ But what after $2n$ and $n^2$ is $2^n$ (in some sense), so I would like to ask whether $\big(\sum_{k=n}^{2^n}a_k\big)_n\to0$ is sufficient to guarentee the convergence of $\sum_{n=1}^\infty a_n$ for $(a_n)_{n\in\mathbb N}\ge0$. Of course it is necessary, but I don't believe it is sufficience. And thus I raise question 1. Answering question 1 help question 2 of course, but is it sufficient? Perhaps we should consider functions (to be an indefinite integral of the integrand function-to-be) like the inverse of $g(x)=x^x$ (superlog?)? But tetration don't satisfy many properties that power have. And what is the derivative of it? Is it possible to make it simple? Or else, perhaps we should try other functions. Any help will be appreciate. Thank you!","The questions are: 1) Does there exists some function $f$ s.t.   $\lim_{x\to\infty}\frac{f(e^x)}{f(x)}=1$ and   $\lim_{x\to\infty}f(x)=\infty$? 2) Is $\big(\sum_{k=n}^{2^n}a_k\big)_n\to0$ is sufficient to guarentee   the convergence of $\sum_{n=1}^\infty a_n$ for $(a_n)_{n\in\mathbb > N}\ge0$? I recently discovered that $(a_n)_{n\in\mathbb N}\ge 0$ and $\big(\sum_{k=n}^{2n}a_k\big)_{n\in\mathbb N}\to 0$ doesn't implies $\sum_{n=1}^\infty a_n\in\mathbb R$, which is done by setting $(a_n)_{n\ge2}=\frac{1}{n\ln n}$. By setting $(a_n)=\frac{1}{n\ln n \ln\ln n}$, we can't guarentee convergence of $\sum_{n=1}^\infty a_n$ even if  $\big(\sum_{k=n}^{n^2}a_k\big)_n\to 0$ But what after $2n$ and $n^2$ is $2^n$ (in some sense), so I would like to ask whether $\big(\sum_{k=n}^{2^n}a_k\big)_n\to0$ is sufficient to guarentee the convergence of $\sum_{n=1}^\infty a_n$ for $(a_n)_{n\in\mathbb N}\ge0$. Of course it is necessary, but I don't believe it is sufficience. And thus I raise question 1. Answering question 1 help question 2 of course, but is it sufficient? Perhaps we should consider functions (to be an indefinite integral of the integrand function-to-be) like the inverse of $g(x)=x^x$ (superlog?)? But tetration don't satisfy many properties that power have. And what is the derivative of it? Is it possible to make it simple? Or else, perhaps we should try other functions. Any help will be appreciate. Thank you!",,"['real-analysis', 'sequences-and-series', 'limits', 'divergent-series', 'tetration']"
58,Is it possible to interchange order of supremum and supremum?,Is it possible to interchange order of supremum and supremum?,,"Prove that if A and B are arbitrary sets and f is a bounded real-valued function on $A\times B$ , then $$ \sup_{a \in A} \sup_{b \in B} f(a,b) = \sup_{b \in B} \sup_{a \in A} f(a,b) . $$ If it is possible, then does it change into one supremum like $\sup f(a,b)$ ? Thanks for help!","Prove that if A and B are arbitrary sets and f is a bounded real-valued function on , then If it is possible, then does it change into one supremum like ? Thanks for help!","A\times B  \sup_{a \in A} \sup_{b \in B} f(a,b) = \sup_{b \in B} \sup_{a \in A} f(a,b) .  \sup f(a,b)",['real-analysis']
59,Finding the sum of the series $\frac{1}{1!}+\frac{1+2}{2!}+\frac{1+2+3}{3!}+ \ldots$,Finding the sum of the series,\frac{1}{1!}+\frac{1+2}{2!}+\frac{1+2+3}{3!}+ \ldots,Deteremine the sum of the series $$\frac{1}{1!}+\frac{1+2}{2!}+\frac{1+2+3}{3!}+ \ldots$$ So I first write down the $n^{th}$ term $a_n=\frac{\frac{n(n+1)}{2}}{n!}=\frac{n+1}{2(n-1)!}$. So from there I can write the series as $$1+\frac{3}{2}+\frac{4}{2\times 2!}+\ldots +\frac{n+1}{2(n-1)!}+\ldots $$ I am quite sure I can do some sort of term by term integration or differentiation of some standard power series and crack this. Any leads?,Deteremine the sum of the series $$\frac{1}{1!}+\frac{1+2}{2!}+\frac{1+2+3}{3!}+ \ldots$$ So I first write down the $n^{th}$ term $a_n=\frac{\frac{n(n+1)}{2}}{n!}=\frac{n+1}{2(n-1)!}$. So from there I can write the series as $$1+\frac{3}{2}+\frac{4}{2\times 2!}+\ldots +\frac{n+1}{2(n-1)!}+\ldots $$ I am quite sure I can do some sort of term by term integration or differentiation of some standard power series and crack this. Any leads?,,"['real-analysis', 'sequences-and-series']"
60,Solving $(f(x))^2 = f(\sqrt{2}x)$,Solving,(f(x))^2 = f(\sqrt{2}x),"I would like to know how to solve this equation : $$f(x)^2 = f(\sqrt{2}x)$$ We assume that $f : \mathbb R \to \mathbb R$ is $\mathcal C^{2}$. The answer should be $f(x)=e^{-x^{2}/2}$, but I don't know how to show this.","I would like to know how to solve this equation : $$f(x)^2 = f(\sqrt{2}x)$$ We assume that $f : \mathbb R \to \mathbb R$ is $\mathcal C^{2}$. The answer should be $f(x)=e^{-x^{2}/2}$, but I don't know how to show this.",,"['calculus', 'real-analysis', 'functional-equations']"
61,Do constant functions have absolute or local max/mins?,Do constant functions have absolute or local max/mins?,,Let $f(x) = 4$ for every $x$ real. Is $4$ a local max and min and also an absolute max and min?,Let for every real. Is a local max and min and also an absolute max and min?,f(x) = 4 x 4,"['real-analysis', 'algebra-precalculus', 'optimization']"
62,Orthonormal basis for $L^2(\mathbb{R})$ by Hermite Polynomials,Orthonormal basis for  by Hermite Polynomials,L^2(\mathbb{R}),"Consider $L^2(\mathbb{R})$ as an Hilbert space with inner product $(\cdot ,\cdot)$ . Define $$\psi_n(x)=e^{-\frac{x^2}{2}}H_n(x),$$ where $H_n(x)$ is the Hermite Polynomials. How do you show that $\{\psi_n(x)\}$ is a orthogonal basis for $L^2(\mathbb{R})$ ? (We can make it orthonormal by normalize it). I showed $(\psi_i,\psi_j)=0$ for all $i\neq j$ . To conclude the $\{\psi_n(x)\}$ forms a basis. We need to show that $\text{span}(\psi_n(x))$ is a dense subset of $L^2$ . How do we prove that?",Consider as an Hilbert space with inner product . Define where is the Hermite Polynomials. How do you show that is a orthogonal basis for ? (We can make it orthonormal by normalize it). I showed for all . To conclude the forms a basis. We need to show that is a dense subset of . How do we prove that?,"L^2(\mathbb{R}) (\cdot ,\cdot) \psi_n(x)=e^{-\frac{x^2}{2}}H_n(x), H_n(x) \{\psi_n(x)\} L^2(\mathbb{R}) (\psi_i,\psi_j)=0 i\neq j \{\psi_n(x)\} \text{span}(\psi_n(x)) L^2","['real-analysis', 'functional-analysis', 'hilbert-spaces', 'banach-spaces']"
63,Integral involving the error function of log(x),Integral involving the error function of log(x),,"Looking for a closed form for the integral $$\int_0^{\infty } e^{-\left(\frac{a-\log (x)}{b}\right)^2} \left(\frac{1}{2} \text{erf}\left(\frac{a-\log (x)}{b}\right)+\frac{1}{2}\right) \, \mathrm{d}x,$$ where erf is the error function $erf (z)=\frac{2}{\sqrt{\pi }}\int _0^ze^{-t^2}\mathrm{d}t.$ I've tried all manner of tricks, to no avail.","Looking for a closed form for the integral $$\int_0^{\infty } e^{-\left(\frac{a-\log (x)}{b}\right)^2} \left(\frac{1}{2} \text{erf}\left(\frac{a-\log (x)}{b}\right)+\frac{1}{2}\right) \, \mathrm{d}x,$$ where erf is the error function $erf (z)=\frac{2}{\sqrt{\pi }}\int _0^ze^{-t^2}\mathrm{d}t.$ I've tried all manner of tricks, to no avail.",,"['calculus', 'real-analysis', 'definite-integrals', 'special-functions', 'error-function']"
64,Closed sets with empty interior measure zero,Closed sets with empty interior measure zero,,"Is the Lebesgue measure of a closed set with empty interior in $\mathbb{R}^{n}$ always zero? Trying to understand something in the math notes that I don't understand, and if the above is true, it would make more sense. Not sure if true though!","Is the Lebesgue measure of a closed set with empty interior in $\mathbb{R}^{n}$ always zero? Trying to understand something in the math notes that I don't understand, and if the above is true, it would make more sense. Not sure if true though!",,"['real-analysis', 'general-topology', 'measure-theory', 'lebesgue-measure']"
65,Convergence of the series $\sum \sqrt{a_n}/n$ given that $\sum a_n$ converges [duplicate],Convergence of the series  given that  converges [duplicate],\sum \sqrt{a_n}/n \sum a_n,"This question already has answers here : If $\sum_{n=1}^{\infty} a_n^{2}$ converges, then so does $\sum_{n=1}^{\infty} \frac {a_n}{n}$ (6 answers) Closed 6 years ago . I've been dealing with this problem for enough time and I don't even have a good idea nor a starting point. This is the problem: Let $\sum_{n=1}^\infty a_n$ be a convergent series. Show that $\sum_{n=1}^\infty \frac{\sqrt{a_n}}{n}$ is also convergent. Thanks for your kind and nice answers, I'm sure a good tip or starting point will suffice.","This question already has answers here : If $\sum_{n=1}^{\infty} a_n^{2}$ converges, then so does $\sum_{n=1}^{\infty} \frac {a_n}{n}$ (6 answers) Closed 6 years ago . I've been dealing with this problem for enough time and I don't even have a good idea nor a starting point. This is the problem: Let $\sum_{n=1}^\infty a_n$ be a convergent series. Show that $\sum_{n=1}^\infty \frac{\sqrt{a_n}}{n}$ is also convergent. Thanks for your kind and nice answers, I'm sure a good tip or starting point will suffice.",,"['real-analysis', 'sequences-and-series']"
66,"Compute $\lim_{n\to\infty}n^4\int_0^1\frac{x^n\ln^3x}{1+x^n}\ln(1-x)\,dx$",Compute,"\lim_{n\to\infty}n^4\int_0^1\frac{x^n\ln^3x}{1+x^n}\ln(1-x)\,dx","Compute \begin{equation} \lim_{n\to\infty}n^4\int_0^1\frac{x^n\ln^3x}{1+x^n}\ln(1-x)\,dx \end{equation} According to Wolfram Alpha , the limit is zero. I tried to make substitution $x=\frac{t}{n}$ and I got \begin{equation} \lim_{n\to\infty}\int_0^{\large\frac{1}{n}}\frac{t^n\ln^3t}{n^n+t^n}\ln\left(1-\frac{t}{n}\right)\,dx \to 0 \end{equation} but I am not sure this approach is correct. I also tried to make substitution $t=x^n$ and I got \begin{equation} \lim_{n\to\infty}n^4\int_0^1\frac{x^n\ln^3x}{1+x^n}\ln(1-x)\,dx=\lim_{n\to\infty}\int_0^1\frac{t^{\large\frac{1}{n}}\ln^3t}{1+t}\ln\left(1-t^{\large\frac{1}{n}}\right)\,dt \end{equation} I used the bound $$\left|\int_0^1\frac{t^{\large\frac{1}{n}}\ln^3t}{1+t}\ln\left(1-t^{\large\frac{1}{n}}\right)\,dt\right|\leq\int_0^1\left|t^{\large\frac{1}{n}}\ln^3t\ln\left(1-t^{\large\frac{1}{n}}\right)\right|\,dt$$ because  $1+t\ge1$, so $\frac{1}{1+t}\leq1$. I can compute the integral using Taylor series for the logarithm but after taking the limit I got the result was infinity. \begin{align} \lim_{n\to\infty}n^4\int_0^1\frac{x^n\ln^3x}{1+x^n}\ln(1-x)\,dx&=\lim_{n\to\infty}\int_0^1t^{\large\frac{1}{n}}\ln^3t\ln\left(1-t^{\large\frac{1}{n}}\right)\,dt\\ &=-\lim_{n\to\infty}\sum_{k=1}^\infty\frac{1}{k}\int_0^1t^{\large\frac{k+1}{n}}\ln^3t\,dt\\ &=\lim_{n\to\infty}\sum_{k=1}^\infty\frac{6}{k\left(\frac{k+1}{n}+1\right)^4}\to\infty \end{align} Could anyone here please help me? Any help would be greatly appreciated. Thank you.","Compute \begin{equation} \lim_{n\to\infty}n^4\int_0^1\frac{x^n\ln^3x}{1+x^n}\ln(1-x)\,dx \end{equation} According to Wolfram Alpha , the limit is zero. I tried to make substitution $x=\frac{t}{n}$ and I got \begin{equation} \lim_{n\to\infty}\int_0^{\large\frac{1}{n}}\frac{t^n\ln^3t}{n^n+t^n}\ln\left(1-\frac{t}{n}\right)\,dx \to 0 \end{equation} but I am not sure this approach is correct. I also tried to make substitution $t=x^n$ and I got \begin{equation} \lim_{n\to\infty}n^4\int_0^1\frac{x^n\ln^3x}{1+x^n}\ln(1-x)\,dx=\lim_{n\to\infty}\int_0^1\frac{t^{\large\frac{1}{n}}\ln^3t}{1+t}\ln\left(1-t^{\large\frac{1}{n}}\right)\,dt \end{equation} I used the bound $$\left|\int_0^1\frac{t^{\large\frac{1}{n}}\ln^3t}{1+t}\ln\left(1-t^{\large\frac{1}{n}}\right)\,dt\right|\leq\int_0^1\left|t^{\large\frac{1}{n}}\ln^3t\ln\left(1-t^{\large\frac{1}{n}}\right)\right|\,dt$$ because  $1+t\ge1$, so $\frac{1}{1+t}\leq1$. I can compute the integral using Taylor series for the logarithm but after taking the limit I got the result was infinity. \begin{align} \lim_{n\to\infty}n^4\int_0^1\frac{x^n\ln^3x}{1+x^n}\ln(1-x)\,dx&=\lim_{n\to\infty}\int_0^1t^{\large\frac{1}{n}}\ln^3t\ln\left(1-t^{\large\frac{1}{n}}\right)\,dt\\ &=-\lim_{n\to\infty}\sum_{k=1}^\infty\frac{1}{k}\int_0^1t^{\large\frac{k+1}{n}}\ln^3t\,dt\\ &=\lim_{n\to\infty}\sum_{k=1}^\infty\frac{6}{k\left(\frac{k+1}{n}+1\right)^4}\to\infty \end{align} Could anyone here please help me? Any help would be greatly appreciated. Thank you.",,"['calculus', 'real-analysis', 'integration', 'limits', 'convergence-divergence']"
67,Construction of a function which is not the pointwise limit of a sequence of continuous functions,Construction of a function which is not the pointwise limit of a sequence of continuous functions,,"This is somewhat linked to a prior question of mine which was looking to see if a proof of mine regarding the Dirichlet function was correct (it wasn't). I now have an answer to the question which can be answered without using directly using the Baire category theorem or the likes; as it is sufficiently different to my original approach, I feel that a new question would be the best way of going about this. The question is to Construct a function $f:[0,1] \rightarrow \mathbb{R}$ which is not the pointwise limit of any sequence $(f_n)$ of continuous functions to which I will have an answer below (while I didn't come up with much of this myself, I feel it's an interesting result to discuss). Finally, as a warning to those who are seeing this as a result of searching for answers to their example sheet/homework questions, please have a think about the question before reading the answer below.","This is somewhat linked to a prior question of mine which was looking to see if a proof of mine regarding the Dirichlet function was correct (it wasn't). I now have an answer to the question which can be answered without using directly using the Baire category theorem or the likes; as it is sufficiently different to my original approach, I feel that a new question would be the best way of going about this. The question is to Construct a function $f:[0,1] \rightarrow \mathbb{R}$ which is not the pointwise limit of any sequence $(f_n)$ of continuous functions to which I will have an answer below (while I didn't come up with much of this myself, I feel it's an interesting result to discuss). Finally, as a warning to those who are seeing this as a result of searching for answers to their example sheet/homework questions, please have a think about the question before reading the answer below.",,['real-analysis']
68,"Irrationals in $[0,1]$ does not have measure zero",Irrationals in  does not have measure zero,"[0,1]","Show that the set of irrationals in $[0,1]$ does not have measure zero in $\mathbb{R}$ By definition of measure zero, we must show that there exists $\epsilon>0$ such that the set $A$ of irrationals in $[0,1]$ cannot be covered by a countable collection of intervals $[a_1,b_1],[a_2,b_2],\ldots$ with total length less than $\epsilon$. So I take $\epsilon=1/2$, and suppose for contradiction that the countable collection of intervals $[a_1,b_1],[a_2,b_2],\ldots$ covers $A$ and $(b_1-a_1)+(b_2-a_2)+\ldots < 1/2$.  How can I continue? EDIT: Thanks for all the responses. Actually I haven't learned any measure theory, only the definition of measure zero in real analysis. Is it possible to proceed using only that? (By the way, sure, I can show $\mathbb{Q}\cap[0,1]$ has measure zero.)","Show that the set of irrationals in $[0,1]$ does not have measure zero in $\mathbb{R}$ By definition of measure zero, we must show that there exists $\epsilon>0$ such that the set $A$ of irrationals in $[0,1]$ cannot be covered by a countable collection of intervals $[a_1,b_1],[a_2,b_2],\ldots$ with total length less than $\epsilon$. So I take $\epsilon=1/2$, and suppose for contradiction that the countable collection of intervals $[a_1,b_1],[a_2,b_2],\ldots$ covers $A$ and $(b_1-a_1)+(b_2-a_2)+\ldots < 1/2$.  How can I continue? EDIT: Thanks for all the responses. Actually I haven't learned any measure theory, only the definition of measure zero in real analysis. Is it possible to proceed using only that? (By the way, sure, I can show $\mathbb{Q}\cap[0,1]$ has measure zero.)",,"['real-analysis', 'measure-theory']"
69,Minimizing sequence,Minimizing sequence,,This came up in a proof I was reading. Define $$\inf_{z \in K} \|x-z\| = d$$ Let $y_n\in K$ be a minimizing sequence How do we know that such a minimizing sequence exists? Here K is a closed convex subset of a Hilbert space H. Is this some axiom for from the reel numbers?,This came up in a proof I was reading. Define $$\inf_{z \in K} \|x-z\| = d$$ Let $y_n\in K$ be a minimizing sequence How do we know that such a minimizing sequence exists? Here K is a closed convex subset of a Hilbert space H. Is this some axiom for from the reel numbers?,,"['real-analysis', 'functional-analysis', 'hilbert-spaces']"
70,A characterization of functions from $\mathbb R^n$ to $\mathbb R^m$ which are continuous,A characterization of functions from  to  which are continuous,\mathbb R^n \mathbb R^m,"Greets I came up the other day with the following question: Is it true that $f:\mathbb{R}^n\longrightarrow{\mathbb{R}^m}$ is continuous if and only if $f$ maps compact sets onto compact sets and maps connected sets onto connected sets? I'm having trouble showing the ""if"", and I haven't found a counterexample to this part, so I would appreciate ideas to prove or disprove this. Thanks","Greets I came up the other day with the following question: Is it true that $f:\mathbb{R}^n\longrightarrow{\mathbb{R}^m}$ is continuous if and only if $f$ maps compact sets onto compact sets and maps connected sets onto connected sets? I'm having trouble showing the ""if"", and I haven't found a counterexample to this part, so I would appreciate ideas to prove or disprove this. Thanks",,['real-analysis']
71,Mean value property with fixed radius,Mean value property with fixed radius,,"I will focus on the real line. Let $f$ be a smooth function on $\mathbb{R}$, if $\forall x\in\mathbb{R}, r>0$, $$\frac{f(x-r)+f(x+r)}{2}=f(x),$$ we say that f has the spherical mean value property (MVP); if instead $$\frac{1}{2r}\int_{x-r}^{x+r}f(t)dt=f(x),$$ we say f has the ball MVP. It is easy to see that spherical MVP implies ball MVP. It is well known, and not hard to prove (though it is much harder in higher dimensions), that ball MVP implies that $f$ is harmonic, i.e. $f$ is of the form $ax+b$, where $a$ and $b$ are constants. Notice that in the definitions above, we require the radius $r$ to run over all the positive numbers. Out of curiosity, I tried to find non harmonic functions which satisfy the MVPs only for $r=1$. It turns out any $1$-periodic function will satisfy the spherical MVP with $r=1$, which is obvious. But it seems much harder to find one for ball MVP. So here is my question: Question: Does there exist a function $f$ which is not of the form $ax+b$, such that $\forall x\in\mathbb{R},$   $$\frac{1}{2}\int_{x-1}^{x+1}f(t)dt=f(x)?$$ Thank you!","I will focus on the real line. Let $f$ be a smooth function on $\mathbb{R}$, if $\forall x\in\mathbb{R}, r>0$, $$\frac{f(x-r)+f(x+r)}{2}=f(x),$$ we say that f has the spherical mean value property (MVP); if instead $$\frac{1}{2r}\int_{x-r}^{x+r}f(t)dt=f(x),$$ we say f has the ball MVP. It is easy to see that spherical MVP implies ball MVP. It is well known, and not hard to prove (though it is much harder in higher dimensions), that ball MVP implies that $f$ is harmonic, i.e. $f$ is of the form $ax+b$, where $a$ and $b$ are constants. Notice that in the definitions above, we require the radius $r$ to run over all the positive numbers. Out of curiosity, I tried to find non harmonic functions which satisfy the MVPs only for $r=1$. It turns out any $1$-periodic function will satisfy the spherical MVP with $r=1$, which is obvious. But it seems much harder to find one for ball MVP. So here is my question: Question: Does there exist a function $f$ which is not of the form $ax+b$, such that $\forall x\in\mathbb{R},$   $$\frac{1}{2}\int_{x-1}^{x+1}f(t)dt=f(x)?$$ Thank you!",,"['calculus', 'real-analysis', 'partial-differential-equations']"
72,Need Help: Any good textbook in undergrad multi-variable analysis/calculus?,Need Help: Any good textbook in undergrad multi-variable analysis/calculus?,,"This semester, I will be taking a senior undergrad course in advanced calculus ""real analysis of several variables"", and we will be covering topics like: -Differentiability. -Open mapping theorem. -Implicit function theorem. -Lagrange multipliers. Submanifolds. -Integrals. -Integration on surfaces. -Stokes theorem, Gauss theorem. I need to know if anyone of you guys know  good textbooks that contain practice problems with full solutions or hints that can be used to understand the material. Most of the textbooks I found are covering only the material with few examples.","This semester, I will be taking a senior undergrad course in advanced calculus ""real analysis of several variables"", and we will be covering topics like: -Differentiability. -Open mapping theorem. -Implicit function theorem. -Lagrange multipliers. Submanifolds. -Integrals. -Integration on surfaces. -Stokes theorem, Gauss theorem. I need to know if anyone of you guys know  good textbooks that contain practice problems with full solutions or hints that can be used to understand the material. Most of the textbooks I found are covering only the material with few examples.",,"['calculus', 'real-analysis', 'analysis', 'reference-request', 'multivariable-calculus']"
73,Does the Banach algebra $L^1(\mathbb{R})$ have zero divisors?,Does the Banach algebra  have zero divisors?,L^1(\mathbb{R}),"Assume that the functions $f,g: \mathbb{R}\rightarrow \mathbb{R}$ are integrable and equal to zero on $(-\infty,0)$, (i.e $f,g \in L^+$). Then by Titchmarsh's theorem: $f*g$ is zero almost everywhere iff $f$ or $g$ is zero almost everywhere. Hence the Banach subalgebra $L^+$ of $L^1(\mathbb{R})$ has no zero divisors. Is the same true for arbitrary integrable $f$ and $g$ on $\mathbb{R}$ ?","Assume that the functions $f,g: \mathbb{R}\rightarrow \mathbb{R}$ are integrable and equal to zero on $(-\infty,0)$, (i.e $f,g \in L^+$). Then by Titchmarsh's theorem: $f*g$ is zero almost everywhere iff $f$ or $g$ is zero almost everywhere. Hence the Banach subalgebra $L^+$ of $L^1(\mathbb{R})$ has no zero divisors. Is the same true for arbitrary integrable $f$ and $g$ on $\mathbb{R}$ ?",,"['real-analysis', 'functional-analysis', 'fourier-analysis', 'banach-algebras']"
74,Limit of Lebesgue integral $\int_F \; \frac{dx}{2 - \sin nx}$,Limit of Lebesgue integral,\int_F \; \frac{dx}{2 - \sin nx},"Let $F\subset\mathbb{R}$ be a measurable set of finite Lebesgue measure. Find the limit  $$ \lim_{n \to \infty} \int_{F} \frac{dx}{2-\sin nx}.$$ Let $f_{n}(x)=\frac{1}{2-\sin nx}$. This function is bounded and continuous. Thus, it is integrable. I even use Mathematica to compute the Riemann integral. However, since we don't really know what is $F$. So we don't know $\int_{F}\frac{dx}{2-\sin nx}$. Or this could be seen as a hint, suggesting that the limit does not exist. I want to use the dominated convergence  theorem to show that since $\lim \limits_{n \to  \infty}f_{n}(x)$ does not exist, $\lim \limits_{n \to \infty}\int_{F}\frac{dx}{2-\sin nx}$ does not exist. However, the condition of that theorem is that $\lim \limits_{n \to \infty}f_{n}(x)$ exists. So I guess I can not use this theorem.","Let $F\subset\mathbb{R}$ be a measurable set of finite Lebesgue measure. Find the limit  $$ \lim_{n \to \infty} \int_{F} \frac{dx}{2-\sin nx}.$$ Let $f_{n}(x)=\frac{1}{2-\sin nx}$. This function is bounded and continuous. Thus, it is integrable. I even use Mathematica to compute the Riemann integral. However, since we don't really know what is $F$. So we don't know $\int_{F}\frac{dx}{2-\sin nx}$. Or this could be seen as a hint, suggesting that the limit does not exist. I want to use the dominated convergence  theorem to show that since $\lim \limits_{n \to  \infty}f_{n}(x)$ does not exist, $\lim \limits_{n \to \infty}\int_{F}\frac{dx}{2-\sin nx}$ does not exist. However, the condition of that theorem is that $\lim \limits_{n \to \infty}f_{n}(x)$ exists. So I guess I can not use this theorem.",,['real-analysis']
75,"Calculating Dini derivatives for $f(x)=\begin{cases}x\,\sin{\left(\frac{1}{x}\right)} & x\neq 0\\ 0 & x=0\end{cases}$",Calculating Dini derivatives for,"f(x)=\begin{cases}x\,\sin{\left(\frac{1}{x}\right)} & x\neq 0\\ 0 & x=0\end{cases}","Define $D^+f(x) = \limsup\limits_{h\to 0^+}{\left(\dfrac{f(x+h)-f(x)}{h}\right)}$. Given the function $f(x)=\begin{cases}x\,\sin{\left(\frac{1}{x}\right)} & x\neq 0\\ 0 & x=0\end{cases}\,,$ find $D^+f(x)$. There are also three other approximate derivatives to work out (lim inf from the right, lim sup and lim inf from the left), but I figure if I can get one I can get the others. The problem is that, while I understand lim sup's for sequences, I still haven't grokked lim sup's for functions... The definition seems totally unclear. (This is also the first time I've ever had to calculate the lim sup of a function, somehow.) EDIT: Edited title for accuracy. Original title was ""Calculating an 'approximate' derivative""","Define $D^+f(x) = \limsup\limits_{h\to 0^+}{\left(\dfrac{f(x+h)-f(x)}{h}\right)}$. Given the function $f(x)=\begin{cases}x\,\sin{\left(\frac{1}{x}\right)} & x\neq 0\\ 0 & x=0\end{cases}\,,$ find $D^+f(x)$. There are also three other approximate derivatives to work out (lim inf from the right, lim sup and lim inf from the left), but I figure if I can get one I can get the others. The problem is that, while I understand lim sup's for sequences, I still haven't grokked lim sup's for functions... The definition seems totally unclear. (This is also the first time I've ever had to calculate the lim sup of a function, somehow.) EDIT: Edited title for accuracy. Original title was ""Calculating an 'approximate' derivative""",,['real-analysis']
76,"given that $h$ is differentiable and $h(3x)+h(2x)+h(x) = 0$ for all $x\in\mathbb{R},$ is $h$ necessarily identically zero?",given that  is differentiable and  for all  is  necessarily identically zero?,"h h(3x)+h(2x)+h(x) = 0 x\in\mathbb{R}, h","Let $h:\mathbb{R}\to\mathbb{R}$ be a differentiable function such that $h(3x)+h(2x)+h(x) = 0$ for all $x\in\mathbb{R}.$ Determine with proof whether $h\equiv 0.$ I think $h$ is identically zero. Let $(1)$ be the original equation. Suppose for a contradiction that there exists some $x$ for which $h(x)\neq 0.$ Then differentiating (1) gives $3 h'(3x) + 2h'(2x)+h'(x)=0$ for all $x\in \mathbb{R}.$ Clearly $h(0) = 0,$ which can be seen by substituting $x=0$ into $(1)$ . Claim 1: For any $\epsilon > 0,$ and any $t\neq 0, |\dfrac{h(t)}{t}| < \epsilon.$ I'm not sure how to prove claim 1, but if it's true, then it immediately follows that h is identically zero.","Let be a differentiable function such that for all Determine with proof whether I think is identically zero. Let be the original equation. Suppose for a contradiction that there exists some for which Then differentiating (1) gives for all Clearly which can be seen by substituting into . Claim 1: For any and any I'm not sure how to prove claim 1, but if it's true, then it immediately follows that h is identically zero.","h:\mathbb{R}\to\mathbb{R} h(3x)+h(2x)+h(x) = 0 x\in\mathbb{R}. h\equiv 0. h (1) x h(x)\neq 0. 3 h'(3x) + 2h'(2x)+h'(x)=0 x\in \mathbb{R}. h(0) = 0, x=0 (1) \epsilon > 0, t\neq 0, |\dfrac{h(t)}{t}| < \epsilon.","['real-analysis', 'calculus', 'derivatives', 'functional-equations']"
77,"Spivak Chapter 10, Problem 29 : why is this true?","Spivak Chapter 10, Problem 29 : why is this true?",,"The problem is as follows (verbatim) :  Prove that it is impossible to write $x = f(x)g(x)$ where $f$ and $g$ are differentiable and $f(0)=g(0)=0$ . I rephrased the question as I realized it wasn't optimally worded : Prove that it is impossible to write $x = f(x)g(x)$ for all $x \in \mathbb R$ , where $f$ are $g$ are differentiable on $\mathbb R$ and $f(0)=g(0)=0$ . I proved this by differentiating both sides of $x = f(x)g(x)$ , which gives $1 = f'(x)g(x) + f(x)g'(x)$ ; thus when $x=0, f'(0)g(0) + f(0)g'(0) = 0 \neq 1$ . However, this proof doesn't give me much insight on why this particular exercise is true. I am really interested in seeing a more intuitive explanation, but here are some of my attempts/hypotheses : $f'(x)g(x) + f(x)g'(x)$ is constant when $f$ or $g$ is constant. However, WLOG, if $f(x) = 0$ , then we will have to divide by zero in order to get a function $g$ such that $x = f(x)g(x)$ . If we define $f(x) = \frac{x}{g(x)},$ then clearly if $g(x) = 0$ then $f$ is not even continuous at $x=0$ , so $f$ would not be differentiable on $\mathbb R$ . I am more or less looking for a graphical(?), intuitive explanation. Thank you for your help!","The problem is as follows (verbatim) :  Prove that it is impossible to write where and are differentiable and . I rephrased the question as I realized it wasn't optimally worded : Prove that it is impossible to write for all , where are are differentiable on and . I proved this by differentiating both sides of , which gives ; thus when . However, this proof doesn't give me much insight on why this particular exercise is true. I am really interested in seeing a more intuitive explanation, but here are some of my attempts/hypotheses : is constant when or is constant. However, WLOG, if , then we will have to divide by zero in order to get a function such that . If we define then clearly if then is not even continuous at , so would not be differentiable on . I am more or less looking for a graphical(?), intuitive explanation. Thank you for your help!","x = f(x)g(x) f g f(0)=g(0)=0 x = f(x)g(x) x \in \mathbb R f g \mathbb R f(0)=g(0)=0 x = f(x)g(x) 1 = f'(x)g(x) + f(x)g'(x) x=0, f'(0)g(0) + f(0)g'(0) = 0 \neq 1 f'(x)g(x) + f(x)g'(x) f g f(x) = 0 g x = f(x)g(x) f(x) = \frac{x}{g(x)}, g(x) = 0 f x=0 f \mathbb R","['real-analysis', 'calculus']"
78,Density of test functions in the space of distributions -- a clarification,Density of test functions in the space of distributions -- a clarification,,"Let $U \subseteq \mathbb{R}^n$ be open and denote by $\mathcal{D}(U)$ the space of all compactly supported smooth functions $U \to \mathbb{R}$ . Let $\mathcal{D}^\prime(U)$ be the space of all distributions $\mathcal{D}(U) \to \mathbb{R}$ with the standard topology. Given a distribution $T$ , I would like to prove that there exists a sequence $(\psi_n)$ in $\mathcal{D}(U)$ such that \begin{equation}\label{eq:1}\tag{$\ast$} \lim_{n \to \infty} \left\langle \psi_n, \varphi \right\rangle  = \left\langle T , \varphi\right\rangle \end{equation} for all $\varphi \in \mathcal{D}(U)$ . I became interested in this question while the following paragraph from this Wikipedia article: The test functions are themselves locally integrable, and so define distributions. As such they are dense in $\mathcal{D}^\prime(U)$ with respect to the topology on $\mathcal{D}^\prime(U)$ in the sense that for any distribution $T \in \mathcal{D}^\prime(U)$ , there is a sequence $\psi_n \in \mathcal{D}(U)$ such that $$ \left\langle \psi_n, \varphi \right\rangle \to \left\langle T, \varphi \right\rangle $$ for all $\varphi \in \mathcal{D}(U)$ . This fact follows from the Hahn-Banach theorem, since the dual of $\mathcal{D}^\prime(U)$ with its weak*-topology is the space $\mathcal{D}(U)$ . My question is as follows: how does this follow from the Hahn-Banach theorem ? I understand why $(\mathcal{D}^\prime(U))^\ast \cong \mathcal{D}(U)$ when the former is given the weak*-topology, but I fail to see how \eqref{eq:1} follows from the Hahn-Banach theorem.","Let be open and denote by the space of all compactly supported smooth functions . Let be the space of all distributions with the standard topology. Given a distribution , I would like to prove that there exists a sequence in such that for all . I became interested in this question while the following paragraph from this Wikipedia article: The test functions are themselves locally integrable, and so define distributions. As such they are dense in with respect to the topology on in the sense that for any distribution , there is a sequence such that for all . This fact follows from the Hahn-Banach theorem, since the dual of with its weak*-topology is the space . My question is as follows: how does this follow from the Hahn-Banach theorem ? I understand why when the former is given the weak*-topology, but I fail to see how \eqref{eq:1} follows from the Hahn-Banach theorem.","U \subseteq \mathbb{R}^n \mathcal{D}(U) U \to \mathbb{R} \mathcal{D}^\prime(U) \mathcal{D}(U) \to \mathbb{R} T (\psi_n) \mathcal{D}(U) \begin{equation}\label{eq:1}\tag{\ast}
\lim_{n \to \infty} \left\langle \psi_n, \varphi \right\rangle  = \left\langle T , \varphi\right\rangle
\end{equation} \varphi \in \mathcal{D}(U) \mathcal{D}^\prime(U) \mathcal{D}^\prime(U) T \in \mathcal{D}^\prime(U) \psi_n \in \mathcal{D}(U) 
\left\langle \psi_n, \varphi \right\rangle \to \left\langle T, \varphi \right\rangle
 \varphi \in \mathcal{D}(U) \mathcal{D}^\prime(U) \mathcal{D}(U) (\mathcal{D}^\prime(U))^\ast \cong \mathcal{D}(U)","['real-analysis', 'functional-analysis']"
79,Infinitely differentiable functions with compact support are dense in $L^p$,Infinitely differentiable functions with compact support are dense in,L^p,"This problem is in Stein. In $L^p$ , $1\leq p<\infty$ on $\mathbb{R}^d$ with Lebesgue measure. (a) Continuous function with compact support are dense in $L^p$ . I have already proves this :) (b) Infinitely differentiable functions with compact support are dense in $L^p$ . How proves (b)? I read that this hard...","This problem is in Stein. In , on with Lebesgue measure. (a) Continuous function with compact support are dense in . I have already proves this :) (b) Infinitely differentiable functions with compact support are dense in . How proves (b)? I read that this hard...",L^p 1\leq p<\infty \mathbb{R}^d L^p L^p,"['real-analysis', 'functional-analysis', 'measure-theory', 'lebesgue-integral', 'lp-spaces']"
80,Two inequalities about using Fatou Lemma,Two inequalities about using Fatou Lemma,,"1) Let $\{f_n\}$ be a sequence of nonnegative measurable functions of $\mathbb R$ that converges pointwise on $\mathbb R$ to $f$ integrable. Show that $$\int_{\mathbb R} f = \lim_{n\to \infty}\int_{\mathbb R}f_n  \Rightarrow \int_{E} f = \lim_{n\to \infty}\int_{E}f_n $$ for any measurable set $E$ I know that $\int_{\mathbb R} f = \int_{\mathbb R \setminus E} f + \int_{E} f$ and $\int_{\mathbb R \setminus E} f \le \liminf_{n\to {\infty}}\biggr(\int_{\mathbb R \setminus E} f_n \biggr)$ from Fatau's Lemma. I couldn't obtain $\int_{E} f = \liminf_{n\to \infty}\int_{E}f_n = \limsup_{n\to \infty}\int_{E}f_n$ and I have seen that inequality below for obtaining it but I couldn't understand. Could someone explain me please? $$\liminf_{n\to \infty}\int_{\mathbb R \setminus E}f_n = \int_{\mathbb R}f-\limsup_{n\to \infty}\int_{E}f_n$$ 2) It has been written ""since $\int_Ef_n \le \int_Ef$ (this inequality from monotonicity I have understood) thus $$\limsup\int_Ef_n \le \int_Ef$$ in proof of The Monotone Convergence Theorem in Royden's Real Analysis. I couldn't see why that inequality obtains. Thanks for any help Regards","1) Let $\{f_n\}$ be a sequence of nonnegative measurable functions of $\mathbb R$ that converges pointwise on $\mathbb R$ to $f$ integrable. Show that $$\int_{\mathbb R} f = \lim_{n\to \infty}\int_{\mathbb R}f_n  \Rightarrow \int_{E} f = \lim_{n\to \infty}\int_{E}f_n $$ for any measurable set $E$ I know that $\int_{\mathbb R} f = \int_{\mathbb R \setminus E} f + \int_{E} f$ and $\int_{\mathbb R \setminus E} f \le \liminf_{n\to {\infty}}\biggr(\int_{\mathbb R \setminus E} f_n \biggr)$ from Fatau's Lemma. I couldn't obtain $\int_{E} f = \liminf_{n\to \infty}\int_{E}f_n = \limsup_{n\to \infty}\int_{E}f_n$ and I have seen that inequality below for obtaining it but I couldn't understand. Could someone explain me please? $$\liminf_{n\to \infty}\int_{\mathbb R \setminus E}f_n = \int_{\mathbb R}f-\limsup_{n\to \infty}\int_{E}f_n$$ 2) It has been written ""since $\int_Ef_n \le \int_Ef$ (this inequality from monotonicity I have understood) thus $$\limsup\int_Ef_n \le \int_Ef$$ in proof of The Monotone Convergence Theorem in Royden's Real Analysis. I couldn't see why that inequality obtains. Thanks for any help Regards",,"['real-analysis', 'lebesgue-integral', 'lebesgue-measure']"
81,How to make sense of complex coordinates on manifolds?,How to make sense of complex coordinates on manifolds?,,"I'm used to the following definition of a smooth manifold: An $n$ -dimensional chart for a topological space is a pair $(U,x)$ where $U$ is an open set of said space and $x : U\to \mathbb{R}^n$ is a homeomorphism. Two $n$ -dimensional charts $(U,x)$ , $(V,y)$ for a topological space are said $C^\infty$ compatible if $U\cap V=\emptyset$ or $U\cap V\neq\emptyset$ and $x\circ y^{-1} : y(U\cap V)\to x(U\cap V)$ and $y\circ x^{-1}: x(U\cap V)\to y(U\cap V)$ are $C^\infty$ functions on $\mathbb{R}^n$ . One $n$ -dimensional $C^\infty$ atlas for a topological space is a collection of $n$ -dimensional $C^\infty$ compatible charts whose domains cover the topological space. A smooth manifold is a Hausdorff, second-countable topological space together with one maximal $n$ -dimensional $C^\infty$ atlas. That's basicaly it. Now, it is quite common to see people doing something on the case of $S^2$ and manifolds related to it: like spacetimes with spherical symmetry. What is done is: people use stereographic projection to map $S^2\setminus \{N\}$ onto $\mathbb{R}^2$ , building the chart $$x(a,b,c)=\left(\dfrac{a}{1-c},\dfrac{b}{1-c}\right)$$ with two coordinate functions $x^1$ and $x^2$ . Now obviously since $\mathbb{C}$ is just $\mathbb{R}^2$ with a multiplication law, we can write $$x=(x^1,x^2)=x^1(1,0)+x^2(0,1)=x^1+ix^2.$$ That much is quite fine. Again, as a vector space over $\mathbb{R}$ , $\mathbb{C}$ is just $\mathbb{R}^2$ and so the chart can be seen as a mapping $x : S^2 \setminus \{N\}\to \mathbb{C}$ . Now here comes what troubles me. Instead of using coordinate functions $x^1=I^1\circ x$ and $x^2=I^2\circ x$ one switches to $z,\overline{z}$ . So people try to use the ""coordinate functions"" as $$z(a,b,c)=\dfrac{a}{1-c}+i\dfrac{b}{1-c},\quad \bar{z}(a,b,c)=\dfrac{a}{1-c}-i\dfrac{b}{1-c}$$ I can't get the point on how to rigorously make sense of this. I mean, the pair $(z,\bar{z})$ is a map $(z,\bar{z}) : S^2\setminus\{N\}\to \mathbb{C}^2$ , so it isn't a chart. Furthermore, as a real vector space $\mathbb{C}^2$ is $\mathbb{R}^4$ which is quite strange here. Finaly, all the information about the coordinates is already contained in $z$ above. Actualy, $\bar{z}$ is just one operation performed on $z$ . So what is the point with this? How does one rigorously in the context of the definitions I've mentioned, make sense of this approach? How is this a valid and rigorous thing to do, and what is the idea behind it?","I'm used to the following definition of a smooth manifold: An -dimensional chart for a topological space is a pair where is an open set of said space and is a homeomorphism. Two -dimensional charts , for a topological space are said compatible if or and and are functions on . One -dimensional atlas for a topological space is a collection of -dimensional compatible charts whose domains cover the topological space. A smooth manifold is a Hausdorff, second-countable topological space together with one maximal -dimensional atlas. That's basicaly it. Now, it is quite common to see people doing something on the case of and manifolds related to it: like spacetimes with spherical symmetry. What is done is: people use stereographic projection to map onto , building the chart with two coordinate functions and . Now obviously since is just with a multiplication law, we can write That much is quite fine. Again, as a vector space over , is just and so the chart can be seen as a mapping . Now here comes what troubles me. Instead of using coordinate functions and one switches to . So people try to use the ""coordinate functions"" as I can't get the point on how to rigorously make sense of this. I mean, the pair is a map , so it isn't a chart. Furthermore, as a real vector space is which is quite strange here. Finaly, all the information about the coordinates is already contained in above. Actualy, is just one operation performed on . So what is the point with this? How does one rigorously in the context of the definitions I've mentioned, make sense of this approach? How is this a valid and rigorous thing to do, and what is the idea behind it?","n (U,x) U x : U\to \mathbb{R}^n n (U,x) (V,y) C^\infty U\cap V=\emptyset U\cap V\neq\emptyset x\circ y^{-1} : y(U\cap V)\to x(U\cap V) y\circ x^{-1}: x(U\cap V)\to y(U\cap V) C^\infty \mathbb{R}^n n C^\infty n C^\infty n C^\infty S^2 S^2\setminus \{N\} \mathbb{R}^2 x(a,b,c)=\left(\dfrac{a}{1-c},\dfrac{b}{1-c}\right) x^1 x^2 \mathbb{C} \mathbb{R}^2 x=(x^1,x^2)=x^1(1,0)+x^2(0,1)=x^1+ix^2. \mathbb{R} \mathbb{C} \mathbb{R}^2 x : S^2 \setminus \{N\}\to \mathbb{C} x^1=I^1\circ x x^2=I^2\circ x z,\overline{z} z(a,b,c)=\dfrac{a}{1-c}+i\dfrac{b}{1-c},\quad \bar{z}(a,b,c)=\dfrac{a}{1-c}-i\dfrac{b}{1-c} (z,\bar{z}) (z,\bar{z}) : S^2\setminus\{N\}\to \mathbb{C}^2 \mathbb{C}^2 \mathbb{R}^4 z \bar{z} z","['real-analysis', 'complex-analysis', 'differential-geometry', 'smooth-manifolds', 'coordinate-systems']"
82,Little o notation in Taylor's formula : o(1),Little o notation in Taylor's formula : o(1),,"I see multiple versions of Taylor's formula (some call it Taylor's series ( I dont really know whats the difference ) but one I know pretty well is : $f(x)=f(a)+f'(a)(x-a)+f''(a)\frac{(x-a)^2}{2!}+f^{3}(a)\frac{(x-a)^3}{3!}+...+f^{n}(a)\frac{(x-a)^n}{n!}+(x-a)^n o(1)$ I get the idea of approximating a function using its derivatives , but I cant really understand what is the $(x-a)^n$ o(1) part doing . I know it means the left is negligble to 1 somewhere , but is it a function or other notation  ? Why choosing $(x-a)^n$ on the left ? And what does it really mean here? Thank you Edit: Converted formulas to Latex formatting - Felix","I see multiple versions of Taylor's formula (some call it Taylor's series ( I dont really know whats the difference ) but one I know pretty well is : $f(x)=f(a)+f'(a)(x-a)+f''(a)\frac{(x-a)^2}{2!}+f^{3}(a)\frac{(x-a)^3}{3!}+...+f^{n}(a)\frac{(x-a)^n}{n!}+(x-a)^n o(1)$ I get the idea of approximating a function using its derivatives , but I cant really understand what is the $(x-a)^n$ o(1) part doing . I know it means the left is negligble to 1 somewhere , but is it a function or other notation  ? Why choosing $(x-a)^n$ on the left ? And what does it really mean here? Thank you Edit: Converted formulas to Latex formatting - Felix",,"['real-analysis', 'taylor-expansion', 'definition']"
83,Baby Rudin Theorem 2.27c,Baby Rudin Theorem 2.27c,,"I feel like I'm missing something very simple here, but I'm confused at how Rudin proved Theorem 2.27 c: If $X$ is a metric space and $E\subset X$ , then $\overline{E}\subset F$ for every closed set $F\subset X$ such that $E\subset F$ . Note: $\overline{E}$ denotes the closure of $E$ ; in other words, $\overline{E} = E \cup E'$ , where $E'$ is the set of limit points of $E$ . Proof: If $F$ is closed and $F \supset E$ , then $F\supset F'$ , hence $F\supset E'$ . Thus $F \supset \overline{E}$ . What I'm confused about is how we know $F \supset E'$ from the previous facts?","I feel like I'm missing something very simple here, but I'm confused at how Rudin proved Theorem 2.27 c: If is a metric space and , then for every closed set such that . Note: denotes the closure of ; in other words, , where is the set of limit points of . Proof: If is closed and , then , hence . Thus . What I'm confused about is how we know from the previous facts?",X E\subset X \overline{E}\subset F F\subset X E\subset F \overline{E} E \overline{E} = E \cup E' E' E F F \supset E F\supset F' F\supset E' F \supset \overline{E} F \supset E',"['real-analysis', 'general-topology', 'metric-spaces']"
84,Proving that Null Cauchy Sequences Form A Maximal Ideal,Proving that Null Cauchy Sequences Form A Maximal Ideal,,"Suppose we have a field $F$ with norm function $|| \cdot ||.$ Let $\mathcal{F}$ be the set of Cauchy sequences in $F$ (with respect to the given norm), made into a ring in the usual way by addition and multiplication of corresponding terms in the sequence. That is, we define $$\left\{\alpha_n\right\}_{n\ge 0} + \left\{\beta_n\right\}_{n\ge 0} = \left\{\alpha_n+\beta_n\right\}_{n\ge 0}$$ and $$\left\{\alpha_n\right\}_{n\ge 0} \cdot \left\{\beta_n\right\}_{n\ge 0} = \left\{\alpha_n\cdot\beta_n\right\}_{n\ge 0}$$ for  any $\left\{\alpha_n\right\}_{n\ge 0}, \left\{\beta_n\right\}_{n\ge 0} \in \mathcal{F}.$ Finally, take $I$ to be the set of sequences $\left\{\alpha_n\right\}_{n\ge 0} \in \mathcal{F}$ satisfying $$\lim_{n\to \infty} ||\alpha_n|| = 0.$$ One can show that $I$ is an ideal. Is there a direct way to prove that $I$ is a maximal ideal? By direct method, I mean a proof that does not appeal to the fact that $\mathcal{F}/I$ is a field.","Suppose we have a field $F$ with norm function $|| \cdot ||.$ Let $\mathcal{F}$ be the set of Cauchy sequences in $F$ (with respect to the given norm), made into a ring in the usual way by addition and multiplication of corresponding terms in the sequence. That is, we define $$\left\{\alpha_n\right\}_{n\ge 0} + \left\{\beta_n\right\}_{n\ge 0} = \left\{\alpha_n+\beta_n\right\}_{n\ge 0}$$ and $$\left\{\alpha_n\right\}_{n\ge 0} \cdot \left\{\beta_n\right\}_{n\ge 0} = \left\{\alpha_n\cdot\beta_n\right\}_{n\ge 0}$$ for  any $\left\{\alpha_n\right\}_{n\ge 0}, \left\{\beta_n\right\}_{n\ge 0} \in \mathcal{F}.$ Finally, take $I$ to be the set of sequences $\left\{\alpha_n\right\}_{n\ge 0} \in \mathcal{F}$ satisfying $$\lim_{n\to \infty} ||\alpha_n|| = 0.$$ One can show that $I$ is an ideal. Is there a direct way to prove that $I$ is a maximal ideal? By direct method, I mean a proof that does not appeal to the fact that $\mathcal{F}/I$ is a field.",,"['real-analysis', 'abstract-algebra', 'analysis', 'ring-theory']"
85,Composition of real-analytic functions is real-analytic,Composition of real-analytic functions is real-analytic,,"Suppose $f,g: \mathbb{R} \to \mathbb{R}$ are real analytic, i.e, locally given by convergent power series. Then $g \circ f$ is real-analytic as well. How do I prove this? I guess the ""standard"" proof would be to extend $f$ and $g$ into some open subsets of $\mathbb{C}$ in the natural way via their power series, then notice that $g \circ f$ is complex differentiable, hence complex-analytic, and hence real-analytic when restricted to $\mathbb{R}$. But is there another proof of this, one that doesn't use complex-analytic extensions? I want a proof which can be extended to the multivariate case, i.e, if $f: \mathbb{R}^n \to \mathbb{R}^m$ and $g:\mathbb{R}^m \to \mathbb{R}^p$ are both analytic (i.e, their components are locally given by multivariate power series), then $g \circ f$ is also real-analytic. Is the standard proof for the multivariate case via complex-analytic extensions as well? Does anyone know a good book for this subject?","Suppose $f,g: \mathbb{R} \to \mathbb{R}$ are real analytic, i.e, locally given by convergent power series. Then $g \circ f$ is real-analytic as well. How do I prove this? I guess the ""standard"" proof would be to extend $f$ and $g$ into some open subsets of $\mathbb{C}$ in the natural way via their power series, then notice that $g \circ f$ is complex differentiable, hence complex-analytic, and hence real-analytic when restricted to $\mathbb{R}$. But is there another proof of this, one that doesn't use complex-analytic extensions? I want a proof which can be extended to the multivariate case, i.e, if $f: \mathbb{R}^n \to \mathbb{R}^m$ and $g:\mathbb{R}^m \to \mathbb{R}^p$ are both analytic (i.e, their components are locally given by multivariate power series), then $g \circ f$ is also real-analytic. Is the standard proof for the multivariate case via complex-analytic extensions as well? Does anyone know a good book for this subject?",,"['real-analysis', 'power-series', 'analyticity']"
86,Calculating a limit of integral,Calculating a limit of integral,,"Computing the limit: $$\lim_{n\rightarrow\infty}\left(\frac{1}{3\pi}\int_\pi^{2\pi}\frac{x}{\arctan(nx)} \ dx\right)^n$$ I made the substiution $t=nx$ then, we have: $$I=\frac{1}{n^2}\int_{n\pi}^{2n\pi}\frac{t}{\arctan t}dt$$ where $I$ is the inside integral. How would you continue?","Computing the limit: $$\lim_{n\rightarrow\infty}\left(\frac{1}{3\pi}\int_\pi^{2\pi}\frac{x}{\arctan(nx)} \ dx\right)^n$$ I made the substiution $t=nx$ then, we have: $$I=\frac{1}{n^2}\int_{n\pi}^{2n\pi}\frac{t}{\arctan t}dt$$ where $I$ is the inside integral. How would you continue?",,"['calculus', 'real-analysis', 'limits']"
87,Can Cauchy Schwarz inequality be proven using Jensen's inequality?,Can Cauchy Schwarz inequality be proven using Jensen's inequality?,,"After reading a comment on If $\mathrm{E} |X|^2$ exists, then $\mathrm{E} X$ also exists , I wonder if Cauchy Schwarz inequality can be proven using Jensen's inequality?","After reading a comment on If $\mathrm{E} |X|^2$ exists, then $\mathrm{E} X$ also exists , I wonder if Cauchy Schwarz inequality can be proven using Jensen's inequality?",,"['real-analysis', 'probability', 'cauchy-schwarz-inequality', 'jensen-inequality']"
88,How to decide about the convergence of $\sum(n\log n\log\log n)^{-1}$?,How to decide about the convergence of ?,\sum(n\log n\log\log n)^{-1},"In Baby Rudin, Theorem 3.27 on page 61 reads the following: Suppose $a_1 \geq a_2 \geq a_3 \geq \cdots \geq 0$ . Then the seires $\sum_{n=1}^\infty a_n$ converges if and only if the series $$ \sum_{k=0}^\infty 2^k a_{2^k} = a_1 + 2a_2 + 4a_4 + 8a_8 + \ldots$$ converges. Now using this result, Rudin gives Theorem 3.29 on page 62, which states that If $p>1$ , $$\sum_{n=2}^\infty \frac{1}{n (\log n)^p} $$ converges; if $p \leq 1$ , the series diverges. Right after the proof of Theorem 3.29, Rudin states on page 63: This procedure may evidently be continued. For instance, $$\sum_{n=3}^\infty \frac{1}{n \log n \log \log n}$$ diverges, whereas $$\sum_{n=3}^\infty \frac{1}{n \log n (\log \log n)^2}$$ converges. How do we derive these last two divergence and convergence conclusions  by continuing the above procedure as pointed out by Rudin? I mean how to prove the convergence of the seires $$\sum_{n=3}^\infty \frac{1}{n \log n (\log \log n)^2}?$$ And, how to prove the divergence of $$\sum_{n=3}^\infty \frac{1}{n \log n \log \log n}$$ using the line of argument suggested by Rudin?","In Baby Rudin, Theorem 3.27 on page 61 reads the following: Suppose . Then the seires converges if and only if the series converges. Now using this result, Rudin gives Theorem 3.29 on page 62, which states that If , converges; if , the series diverges. Right after the proof of Theorem 3.29, Rudin states on page 63: This procedure may evidently be continued. For instance, diverges, whereas converges. How do we derive these last two divergence and convergence conclusions  by continuing the above procedure as pointed out by Rudin? I mean how to prove the convergence of the seires And, how to prove the divergence of using the line of argument suggested by Rudin?",a_1 \geq a_2 \geq a_3 \geq \cdots \geq 0 \sum_{n=1}^\infty a_n  \sum_{k=0}^\infty 2^k a_{2^k} = a_1 + 2a_2 + 4a_4 + 8a_8 + \ldots p>1 \sum_{n=2}^\infty \frac{1}{n (\log n)^p}  p \leq 1 \sum_{n=3}^\infty \frac{1}{n \log n \log \log n} \sum_{n=3}^\infty \frac{1}{n \log n (\log \log n)^2} \sum_{n=3}^\infty \frac{1}{n \log n (\log \log n)^2}? \sum_{n=3}^\infty \frac{1}{n \log n \log \log n},"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
89,Computing in closed form $\sum_{n=1}^{\infty}\frac{\operatorname{Ci}\left(\frac{3}{4}\zeta(2) \space n\right)}{n^2}$,Computing in closed form,\sum_{n=1}^{\infty}\frac{\operatorname{Ci}\left(\frac{3}{4}\zeta(2) \space n\right)}{n^2},"What tools would you recommend me for computing the series below? $$\sum_{n=1}^{\infty}\frac{\operatorname{\displaystyle Ci\left(\frac{3}{4}\zeta(2) \space n\right)}}{n^2}$$ I lack the starting ideas, I need some. Thanks.","What tools would you recommend me for computing the series below? $$\sum_{n=1}^{\infty}\frac{\operatorname{\displaystyle Ci\left(\frac{3}{4}\zeta(2) \space n\right)}}{n^2}$$ I lack the starting ideas, I need some. Thanks.",,"['calculus', 'real-analysis', 'integration', 'sequences-and-series']"
90,"Is $C^{\infty}([0,1])$ a Banach space?",Is  a Banach space?,"C^{\infty}([0,1])","I have read that the answer is no, but I am unable to prove it. Give $C^{\infty}([0,1])$ the metric $$d(f,g) = \sum_{j=0}^{\infty} 2^{-j} \frac{||(f-g)^{(j)}||}{1 + ||(f-g)^{(j)}||}$$ associated to the collection of seminorms $||f^{(j)}||$, the sup-norm of the $j$-th derivative. This makes $C^{\infty}([0,1])$ into a complete metric space. How do you show that the topological vector space $C^{\infty}([0,1])$ is not a Banach space? That is, there does not exist a norm which induces the same topology. It does not appear to be enough to show merely that the metric does not arise from a norm, since it is very non-canonical.","I have read that the answer is no, but I am unable to prove it. Give $C^{\infty}([0,1])$ the metric $$d(f,g) = \sum_{j=0}^{\infty} 2^{-j} \frac{||(f-g)^{(j)}||}{1 + ||(f-g)^{(j)}||}$$ associated to the collection of seminorms $||f^{(j)}||$, the sup-norm of the $j$-th derivative. This makes $C^{\infty}([0,1])$ into a complete metric space. How do you show that the topological vector space $C^{\infty}([0,1])$ is not a Banach space? That is, there does not exist a norm which induces the same topology. It does not appear to be enough to show merely that the metric does not arise from a norm, since it is very non-canonical.",,"['real-analysis', 'functional-analysis', 'banach-spaces']"
91,Continuous probability distribution with no first moment but the characteristic function is differentiable,Continuous probability distribution with no first moment but the characteristic function is differentiable,,"I am looking for an example of a continuous distribution function where the first moment does not exist but the characteristic function is differentiable everywhere. Cauchy distributions do not fulfill this, as their characteristic functions are not differentiable at $0$. Does anybody here have an example?","I am looking for an example of a continuous distribution function where the first moment does not exist but the characteristic function is differentiable everywhere. Cauchy distributions do not fulfill this, as their characteristic functions are not differentiable at $0$. Does anybody here have an example?",,"['real-analysis', 'probability']"
92,"How to prove or is there any reference to ""integration-by-parts"" formula for difference quotients?","How to prove or is there any reference to ""integration-by-parts"" formula for difference quotients?",,"I found this identity in Lawrence C.Evans' book 'Partial Differential Equations' 2ed edition, page293, where $\phi \in C_{c}^{\infty}(V) \ and\ V\subset \subset U$, then $\int_{V}u(x)\frac{\phi(x+he_i)-\phi(x)}{h}dx=-\int_{V}[\frac{u(x)-u(x-he_i)}{h}]\phi(x)dx$ It says ""this is the 'integration-by-parts' formula for difference quotients."" I'd like to know is there any proof or reference of this  ""integration-by-parts"" formula for difference quotients?","I found this identity in Lawrence C.Evans' book 'Partial Differential Equations' 2ed edition, page293, where $\phi \in C_{c}^{\infty}(V) \ and\ V\subset \subset U$, then $\int_{V}u(x)\frac{\phi(x+he_i)-\phi(x)}{h}dx=-\int_{V}[\frac{u(x)-u(x-he_i)}{h}]\phi(x)dx$ It says ""this is the 'integration-by-parts' formula for difference quotients."" I'd like to know is there any proof or reference of this  ""integration-by-parts"" formula for difference quotients?",,"['real-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
93,Can $\mathbb{R}$ be written as an ascending union of proper additive subgroups?,Can  be written as an ascending union of proper additive subgroups?,\mathbb{R},Can the group $\mathbb{R}$ be written as countable ascending union of proper subgroups? (i.e. does there exists a series of proper subgroups $H_1\leq H_2\leq \cdots $ such that $\cup {H_i}=\mathbb{R}$?),Can the group $\mathbb{R}$ be written as countable ascending union of proper subgroups? (i.e. does there exists a series of proper subgroups $H_1\leq H_2\leq \cdots $ such that $\cup {H_i}=\mathbb{R}$?),,"['real-analysis', 'group-theory', 'axiom-of-choice', 'infinite-groups']"
94,Two functions agreeing except on set of measure zero,Two functions agreeing except on set of measure zero,,"Let $f,g:S\rightarrow\mathbb{R}$; assume $f$ and $g$ are integrable over $S$. Show that if $f$ and $g$ agree except on a set of measure zero, then $\int_Sf=\int_Sg$. Since $f$ and $g$ are integrable over $S$, we have $f-g$ also integrable over $S$. So $(f-g)(x)=0$ except for a set of measure zero. If $(f-g)$ were bounded, we could choose a partition so that the volume covering the points $x$ such that $(f-g)(x)\neq 0$ is less than any $\epsilon$, which will imply that $\int_S(f-g)=0$, and so $\int_S f=\int_S g$. But here we don't have boundedness. How can we go from here?","Let $f,g:S\rightarrow\mathbb{R}$; assume $f$ and $g$ are integrable over $S$. Show that if $f$ and $g$ agree except on a set of measure zero, then $\int_Sf=\int_Sg$. Since $f$ and $g$ are integrable over $S$, we have $f-g$ also integrable over $S$. So $(f-g)(x)=0$ except for a set of measure zero. If $(f-g)$ were bounded, we could choose a partition so that the volume covering the points $x$ such that $(f-g)(x)\neq 0$ is less than any $\epsilon$, which will imply that $\int_S(f-g)=0$, and so $\int_S f=\int_S g$. But here we don't have boundedness. How can we go from here?",,"['real-analysis', 'integration']"
95,Which is the better approximation to $e$?,Which is the better approximation to ?,e,"Let $a_n = (1+1/n)^n$ and $b_n = (1+1/n)^{n+1}$. Both $a_n \to e$ and $b_n \to e$, and $a_n < e < b_n$. A better approximation to $e$ is known to be $c_n = (1+1/n)^{n+1/2} = \sqrt{a_n b_n} $, the geometric mean aof $a_n$ and $b_n$. My question is: how about $d_n = (a_n+b_n)/2$, the arithmetic mean of $a_n$ and $b_n$? Is $d_n$ a better approximation to $e$ than $c_n$? More precisely, let $r_n = \frac{c_n-e}{d_n-e}$. Does $\lim_{n \to \infty} r_n$ exist? If so, is it $0$, $\infty$, or a finite value? If the limit is finite, what is it? Is there some other mean of $a_n$ and $b_n$ (such as the harmonic mean) which does better than either? No, I haven't tried to solve these yet. I thought they would be interesting questions. Extra points if the answer does $not$ use the expansions of $\ln(1\pm x)$ or $e^x$.","Let $a_n = (1+1/n)^n$ and $b_n = (1+1/n)^{n+1}$. Both $a_n \to e$ and $b_n \to e$, and $a_n < e < b_n$. A better approximation to $e$ is known to be $c_n = (1+1/n)^{n+1/2} = \sqrt{a_n b_n} $, the geometric mean aof $a_n$ and $b_n$. My question is: how about $d_n = (a_n+b_n)/2$, the arithmetic mean of $a_n$ and $b_n$? Is $d_n$ a better approximation to $e$ than $c_n$? More precisely, let $r_n = \frac{c_n-e}{d_n-e}$. Does $\lim_{n \to \infty} r_n$ exist? If so, is it $0$, $\infty$, or a finite value? If the limit is finite, what is it? Is there some other mean of $a_n$ and $b_n$ (such as the harmonic mean) which does better than either? No, I haven't tried to solve these yet. I thought they would be interesting questions. Extra points if the answer does $not$ use the expansions of $\ln(1\pm x)$ or $e^x$.",,"['real-analysis', 'limits']"
96,Triangle inequality applied to subtraction?,Triangle inequality applied to subtraction?,,So I checked out other questions and it appears my specific question wasn't answered I was wondering if the following is valid: $$ \lvert a - b \rvert \le \lvert a \rvert + \lvert -b \rvert = \lvert a \rvert + \lvert b \rvert \\ \therefore \; \lvert a - b \rvert \le \lvert a \rvert + \lvert b \rvert $$ Thanks in advanced!,So I checked out other questions and it appears my specific question wasn't answered I was wondering if the following is valid: $$ \lvert a - b \rvert \le \lvert a \rvert + \lvert -b \rvert = \lvert a \rvert + \lvert b \rvert \\ \therefore \; \lvert a - b \rvert \le \lvert a \rvert + \lvert b \rvert $$ Thanks in advanced!,,"['real-analysis', 'inequality']"
97,$f$ is integrable but has no indefinite integral,is integrable but has no indefinite integral,f,"Let $$f(x)=\cases{0,& $x\ne0$\cr 1, &$x=0.$}$$ Then $f$ is clearly integrable, yet has no antiderivative, on any interval containing $0,$ since any such antiderivative would have a constant value on each side of $0$ and have slope $1$ at $0$ —an impossibility. So does this mean that $f$ has no indefinite integral? EDIT My understanding is that the indefinite integral of $f$ is the family of all the antiderivatives of $f,$ and conceptually requires some antiderivative to be defined on the entire domain. Is this correct?","Let Then is clearly integrable, yet has no antiderivative, on any interval containing since any such antiderivative would have a constant value on each side of and have slope at —an impossibility. So does this mean that has no indefinite integral? EDIT My understanding is that the indefinite integral of is the family of all the antiderivatives of and conceptually requires some antiderivative to be defined on the entire domain. Is this correct?","f(x)=\cases{0,& x\ne0\cr 1, &x=0.} f 0, 0 1 0 f f f,","['calculus', 'real-analysis', 'analysis', 'integration']"
98,Rudin Theorem 3.27,Rudin Theorem 3.27,,"Theorem 3.27 of Rudin's book Principles of mathematical analysis at pages 61-62 states that, Suppose $a_1\ge a_2\ge a_3\ge \cdots \ge 0.$ Then the series $\sum_{n=1}^{\infty}a_{n}$ converges if and only if the series $\sum_{k=0}^{\infty}2^{k}a_{2^{k}}=a_{1}+2a_{2}+4a_{4}+8a_{8}+\cdots$ converges. I could follow all of the arguments except the the last sentence, which is By (8) and (9), the sequences $\left\{ s_{n}\right\}$ and $\left\{ t_{k}\right\}$ are either both bounded or both unbounded. Here (8) and (9) are (8) For $n<2^k$, $s_n \le t_k$. (9) For $n>2^k$, $2s_n \ge t_k$. where  $s_{n}=\sum_{i=1}^{n}a_{i}$, $t_{k}=\sum_{i=0}^{k}2^{i}a_{2^{i}}.$ Why are the two sequences either both bounded or both unbounded? The (8) seems to imply that if $t_k$ converges, then $s_n$ converges. The (9) seems to imply that if $s_n$ converges, then $t_k$ converges. I could not further more arguments to see how the last sentence works. Thank you for any help. BTW, for $n=2^k$, it seems like $s_n \le t_k$.","Theorem 3.27 of Rudin's book Principles of mathematical analysis at pages 61-62 states that, Suppose $a_1\ge a_2\ge a_3\ge \cdots \ge 0.$ Then the series $\sum_{n=1}^{\infty}a_{n}$ converges if and only if the series $\sum_{k=0}^{\infty}2^{k}a_{2^{k}}=a_{1}+2a_{2}+4a_{4}+8a_{8}+\cdots$ converges. I could follow all of the arguments except the the last sentence, which is By (8) and (9), the sequences $\left\{ s_{n}\right\}$ and $\left\{ t_{k}\right\}$ are either both bounded or both unbounded. Here (8) and (9) are (8) For $n<2^k$, $s_n \le t_k$. (9) For $n>2^k$, $2s_n \ge t_k$. where  $s_{n}=\sum_{i=1}^{n}a_{i}$, $t_{k}=\sum_{i=0}^{k}2^{i}a_{2^{i}}.$ Why are the two sequences either both bounded or both unbounded? The (8) seems to imply that if $t_k$ converges, then $s_n$ converges. The (9) seems to imply that if $s_n$ converges, then $t_k$ converges. I could not further more arguments to see how the last sentence works. Thank you for any help. BTW, for $n=2^k$, it seems like $s_n \le t_k$.",,['real-analysis']
99,seek a direct method to show $(a_k)\in l^2$,seek a direct method to show,(a_k)\in l^2,"Let $\{a_i\}, i=1, 2, \cdots$ be a nonincreasing sequence of positive numbers, and suppose $\sum_{k=1}^{\infty}\frac{a_k}{\sqrt{k}}<\infty~$, show that $\sum_{k=1}^{\infty}a_k^2<\infty.$ Could you give a direct proof by finding an inequality of the type $\sum_{k=1}^{\infty}a_k^2\leq f<\infty$, where $f$ is some expression related to the already known number $\sum_{k=1}^{\infty}\frac{a_k}{\sqrt{k}}$? Thanks in advance! Remark: I have solved this problem, but my method is indirect, I have used the uniformly bounded principal and the inequality $lim_{k\to\infty}\frac{1}{\sqrt{k}}\sum_{j=1}^ka_j=0,\forall (a_k)\in l^2$, so I really want to know how to solve the problem by finding a direct inequality.","Let $\{a_i\}, i=1, 2, \cdots$ be a nonincreasing sequence of positive numbers, and suppose $\sum_{k=1}^{\infty}\frac{a_k}{\sqrt{k}}<\infty~$, show that $\sum_{k=1}^{\infty}a_k^2<\infty.$ Could you give a direct proof by finding an inequality of the type $\sum_{k=1}^{\infty}a_k^2\leq f<\infty$, where $f$ is some expression related to the already known number $\sum_{k=1}^{\infty}\frac{a_k}{\sqrt{k}}$? Thanks in advance! Remark: I have solved this problem, but my method is indirect, I have used the uniformly bounded principal and the inequality $lim_{k\to\infty}\frac{1}{\sqrt{k}}\sum_{j=1}^ka_j=0,\forall (a_k)\in l^2$, so I really want to know how to solve the problem by finding a direct inequality.",,"['real-analysis', 'analysis', 'inequality', 'soft-question']"
