,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Inequality of Frobenius norm for skew matrices,Inequality of Frobenius norm for skew matrices,,"Let $A$ be a complex skew- symmetric $n \times n$ matrix, that is, $A^T = -A$. Denote by $\|\cdot\|_F$ the Frobenius norm, that is, $\|B\|_F^2 = \text{trace}(B^*B)$. I would like to prove that $$ \big\|A^*A\big\|_F^2 \leq \frac{1}{2}\big\|A\big\|_F^4. $$ Even better would be to prove a strict inequality, that is, to replace $\frac{1}{2}$ by a strictly smaller constant (possibly depending on $n$, possibly tending to $\frac{1}{2}$ as $n$ grows). The strange condition of $A$ being complex but skew-symmetric is unfortunately unavoidable, since this $A$ comes essentially from the Lie algebra of $SO(n, \mathbb{C})$. For reasons connected to this other point of view, I deduced from some ""too-well-known-to-give-a-reference"" facts in the literature that the inequality must hold, but I have never seen a proof of that. For arbitrary matrices $A$ a weaker inequality holds with $\frac{1}{2}$ replaced by $1$, by sub-multiplicativity. However, the equality is reached for a matrix made only of 1, which is as far from being skew-symmetric as possible. Motivated by this, I tried the computation for a skew matrix having 1 everywhere above the diagonal, but the ratio that one obtains in that case is $$ \frac{\|A^*A\|_F^2}{\|A\|_F^4} = \frac{1}{3} \frac{n^2+n-3}{n^2-n}, $$ which tends to $\frac{1}{3}$ and does not look great for proving a general bound. If this were a general bound, however, I would be happy! But I have no reason to believe that, other than I have no better guesses... Thanks in advance for any help! EDIT: I still do not know how to prove the inequality in general, but at least I can now prove that the inequality is optimal, since if one takes any (nonzero...) matrix of the form $$ A = \begin{pmatrix} 0 & \dots & 0 & v_1\\ \vdots & \ddots & \vdots & \vdots\\ 0 & \dots & 0 & v_{n-1}\\ -v_1 & \dots & -v_{n-1} & 0 \end{pmatrix} $$ then $\text{trace}(A^*A) = -2\|v\|^2$ and $\text{trace}\big((A^*A)^2\big) = 2\|v\|^4$, realizing the equality. Thus restricting to real-valued matrices does not seem to make you lose anything.","Let $A$ be a complex skew- symmetric $n \times n$ matrix, that is, $A^T = -A$. Denote by $\|\cdot\|_F$ the Frobenius norm, that is, $\|B\|_F^2 = \text{trace}(B^*B)$. I would like to prove that $$ \big\|A^*A\big\|_F^2 \leq \frac{1}{2}\big\|A\big\|_F^4. $$ Even better would be to prove a strict inequality, that is, to replace $\frac{1}{2}$ by a strictly smaller constant (possibly depending on $n$, possibly tending to $\frac{1}{2}$ as $n$ grows). The strange condition of $A$ being complex but skew-symmetric is unfortunately unavoidable, since this $A$ comes essentially from the Lie algebra of $SO(n, \mathbb{C})$. For reasons connected to this other point of view, I deduced from some ""too-well-known-to-give-a-reference"" facts in the literature that the inequality must hold, but I have never seen a proof of that. For arbitrary matrices $A$ a weaker inequality holds with $\frac{1}{2}$ replaced by $1$, by sub-multiplicativity. However, the equality is reached for a matrix made only of 1, which is as far from being skew-symmetric as possible. Motivated by this, I tried the computation for a skew matrix having 1 everywhere above the diagonal, but the ratio that one obtains in that case is $$ \frac{\|A^*A\|_F^2}{\|A\|_F^4} = \frac{1}{3} \frac{n^2+n-3}{n^2-n}, $$ which tends to $\frac{1}{3}$ and does not look great for proving a general bound. If this were a general bound, however, I would be happy! But I have no reason to believe that, other than I have no better guesses... Thanks in advance for any help! EDIT: I still do not know how to prove the inequality in general, but at least I can now prove that the inequality is optimal, since if one takes any (nonzero...) matrix of the form $$ A = \begin{pmatrix} 0 & \dots & 0 & v_1\\ \vdots & \ddots & \vdots & \vdots\\ 0 & \dots & 0 & v_{n-1}\\ -v_1 & \dots & -v_{n-1} & 0 \end{pmatrix} $$ then $\text{trace}(A^*A) = -2\|v\|^2$ and $\text{trace}\big((A^*A)^2\big) = 2\|v\|^4$, realizing the equality. Thus restricting to real-valued matrices does not seem to make you lose anything.",,"['matrices', 'inequality', 'lie-algebras', 'normed-spaces']"
1,Efficient low rank matrix-vector multiplication,Efficient low rank matrix-vector multiplication,,"If I have large matrix, but with very low rank, say 2. Is there an efficient way to multiply this matrix by vector (to achieve linear complexity)?","If I have large matrix, but with very low rank, say 2. Is there an efficient way to multiply this matrix by vector (to achieve linear complexity)?",,"['linear-algebra', 'matrices']"
2,Finding the dimension and basis of an orthogonal space,Finding the dimension and basis of an orthogonal space,,I am trying to find the basis and dimensions of the the space orthogonal $S$ which is in $\mathbb R^3$. $$S = \begin{bmatrix}1\\2\\3\end{bmatrix}$$ So the dimension would be two because it is $3 - 1$. The problem I am having is finding the basis. Would I do $$    1x+2y+3z = 0$$ Which would give me different basis. Am I on the correct track?,I am trying to find the basis and dimensions of the the space orthogonal $S$ which is in $\mathbb R^3$. $$S = \begin{bmatrix}1\\2\\3\end{bmatrix}$$ So the dimension would be two because it is $3 - 1$. The problem I am having is finding the basis. Would I do $$    1x+2y+3z = 0$$ Which would give me different basis. Am I on the correct track?,,"['linear-algebra', 'matrices', 'orthonormal']"
3,How many ways are there to represent a monomial order by term order via matrices?,How many ways are there to represent a monomial order by term order via matrices?,,"During the lecture, my professor brought up a list of project ideas to work on. One of the ideas I am interested and currently working on is term order via matrices. That is: I need to find the number of ways to order $n$-tuples with matrices in any type of orders. Here is the problem: $$\Large\textbf{Problem}$$ Let $>$ be a monomial ordering and $A$ be the following $m$-by-$n$ matrix: $$\begin{pmatrix} a_{11} & \cdots & a_{1m}\\ \vdots & \ddots & \vdots\\ a_{n1} & \cdots & a_{nm} \end{pmatrix}$$ such that the integers $m,n > 0$ and $a_{ij} \in \mathbb{R}$ for $1 \leq i \leq n$ and $1 \leq j \leq m$.  Define $>_A$ by $$\overline{a} = (a_1, \dots, a_n) > _A \overline{b} = (b_1, \dots, b_n)\text{ iff }\overline{a}A > \overline{b}A.$$ How many ways are there to represent a monomial order by term order via matrices?  Determine conditions such that $>_A$ defines a monomial ordering. $$\Large\textbf{My Current List of Term Orders Via Matrices}$$ Let $\overline{a} = (a_1, \dots, a_n)$ and $\overline{b} = (b_1, \dots, b_n)$ for each of the following orders.  Here are my results: $$\large\textbf{Lexicographic Order}$$ If $A = I$, then clearly $>_A$ is $>_{\text{lex}}$. $A = (1, \dots, 1)^T$ is the column matrix consisting of $n$ $1$'s, such that $\overline{a}A$ and $\overline{b}A$ exist. Next parts are not easy.  I found few of them. $$\large\textbf{Degree Lexicographic Order}$$ $A$ is the following $n$-by-$m$ matrix: $$A = \begin{pmatrix} 1 & 1 & \cdots & 1 & 1\\ 1 & 0 & 0 & \cdots & 0\\ 0 & 1 & 0 & \cdots & 0\\ \vdots & 0 & \ddots & 0 & 0\\ 0 & \vdots & 0 & 1 & 0\\ 0 & 0 & \vdots & 0 & 1 \end{pmatrix}$$ So if $\overline{a} > \overline{b}$, then $\overline{a}A >_{\text{deglex}} \overline{b}A$. If $A$ is the following square $n$-by-$n$ matrix $$A = \begin{pmatrix} 1 & 1 & \cdots & 1 & 1\\ 1 & 0 & 0 & \cdots & 0\\ 0 & 1 & 0 & \cdots & 0\\ \vdots & 0 & \ddots & 0 & 0\\ 0 & \cdots & 0 & 1 & 0 \end{pmatrix}$$ then $>_A$ is $>_{\text{deglex}}$ $A = (1, \dots, 1)^T$ might work for this order. $$\large\textbf{Degree Reversed Lexicographic Order}$$  1. If $A$ is the following $m$-by-$n$ matrix $$\begin{pmatrix} 1 & 1 & \cdots & 1 & 1\\ 0 & 0 & \cdots & 0 & -1\\ 0 & \cdots & 0 & -1 & 0\\ \vdots & 0 & \kern3mu\raise1mu{.}\kern3mu\raise6mu{.}\kern3mu\raise12mu{.} & 0 & \vdots\\ 0 & -1 & 0 & \vdots & 0\\ -1 & 0 & \cdots & 0 & 0 \end{pmatrix}$$ Then, $>_A$ is $>_{\text{degrev}}$. If $A$ is the following $n$-by-$m$ matrix $$\begin{pmatrix} 1 & 1 & \cdots & 1 & 1\\ 0 & 0 & \cdots & 0 & -1\\ 0 & \cdots & 0 & -1 & 0\\ \vdots & 0 & \kern3mu\raise1mu{.}\kern3mu\raise6mu{.}\kern3mu\raise12mu{.} & 0 & \vdots\\ 0 & -1 & 0 & \cdots & 0 \end{pmatrix}$$ Then, $>_A$ is also $>_{\text{degrev}}$. I think that $A = (1,\dots,1)^T$ also works for this order. $$\large\textbf{Inverse Lexicographic Order}$$ Same list of term orders via matrices as lexicographic order list. $$\Large\textbf{Results and Thoughts}$$ After hours of investigating the number of ways to represent types of order via matrices, I took down the following conditions I believe may be correct: Given $\overline{a} = (a_1, \dots,a_n)$, in order for $\overline{a}A$ to be defined, we need the row of a matrix $A$ to corresponds with the column of $\overline{a}$. In order to specify the order by matrices, each term in $\overline{a}$ must be multiplied by each nonzero term in the matrix $A$.  Otherwise, the ordering of the tuples is not defined very well.  In other words, for this case, if $\overline{a} >_A \overline{b}$, then we can't conclude that $\overline{a}A > \overline{b}A$ for an ordering $>$. I believe the list of matrix-orderings I have is not enough; there might be more types of matrix-ordering that works for orders, like $\text{lex}$, $\text{deglex}$ and $\text{invlex}$, that I haven't figured out yet. Any comments or thoughts you have for the problem I am working on?  I exhausted lots of tries to determine the number of matrix-orderings.","During the lecture, my professor brought up a list of project ideas to work on. One of the ideas I am interested and currently working on is term order via matrices. That is: I need to find the number of ways to order $n$-tuples with matrices in any type of orders. Here is the problem: $$\Large\textbf{Problem}$$ Let $>$ be a monomial ordering and $A$ be the following $m$-by-$n$ matrix: $$\begin{pmatrix} a_{11} & \cdots & a_{1m}\\ \vdots & \ddots & \vdots\\ a_{n1} & \cdots & a_{nm} \end{pmatrix}$$ such that the integers $m,n > 0$ and $a_{ij} \in \mathbb{R}$ for $1 \leq i \leq n$ and $1 \leq j \leq m$.  Define $>_A$ by $$\overline{a} = (a_1, \dots, a_n) > _A \overline{b} = (b_1, \dots, b_n)\text{ iff }\overline{a}A > \overline{b}A.$$ How many ways are there to represent a monomial order by term order via matrices?  Determine conditions such that $>_A$ defines a monomial ordering. $$\Large\textbf{My Current List of Term Orders Via Matrices}$$ Let $\overline{a} = (a_1, \dots, a_n)$ and $\overline{b} = (b_1, \dots, b_n)$ for each of the following orders.  Here are my results: $$\large\textbf{Lexicographic Order}$$ If $A = I$, then clearly $>_A$ is $>_{\text{lex}}$. $A = (1, \dots, 1)^T$ is the column matrix consisting of $n$ $1$'s, such that $\overline{a}A$ and $\overline{b}A$ exist. Next parts are not easy.  I found few of them. $$\large\textbf{Degree Lexicographic Order}$$ $A$ is the following $n$-by-$m$ matrix: $$A = \begin{pmatrix} 1 & 1 & \cdots & 1 & 1\\ 1 & 0 & 0 & \cdots & 0\\ 0 & 1 & 0 & \cdots & 0\\ \vdots & 0 & \ddots & 0 & 0\\ 0 & \vdots & 0 & 1 & 0\\ 0 & 0 & \vdots & 0 & 1 \end{pmatrix}$$ So if $\overline{a} > \overline{b}$, then $\overline{a}A >_{\text{deglex}} \overline{b}A$. If $A$ is the following square $n$-by-$n$ matrix $$A = \begin{pmatrix} 1 & 1 & \cdots & 1 & 1\\ 1 & 0 & 0 & \cdots & 0\\ 0 & 1 & 0 & \cdots & 0\\ \vdots & 0 & \ddots & 0 & 0\\ 0 & \cdots & 0 & 1 & 0 \end{pmatrix}$$ then $>_A$ is $>_{\text{deglex}}$ $A = (1, \dots, 1)^T$ might work for this order. $$\large\textbf{Degree Reversed Lexicographic Order}$$  1. If $A$ is the following $m$-by-$n$ matrix $$\begin{pmatrix} 1 & 1 & \cdots & 1 & 1\\ 0 & 0 & \cdots & 0 & -1\\ 0 & \cdots & 0 & -1 & 0\\ \vdots & 0 & \kern3mu\raise1mu{.}\kern3mu\raise6mu{.}\kern3mu\raise12mu{.} & 0 & \vdots\\ 0 & -1 & 0 & \vdots & 0\\ -1 & 0 & \cdots & 0 & 0 \end{pmatrix}$$ Then, $>_A$ is $>_{\text{degrev}}$. If $A$ is the following $n$-by-$m$ matrix $$\begin{pmatrix} 1 & 1 & \cdots & 1 & 1\\ 0 & 0 & \cdots & 0 & -1\\ 0 & \cdots & 0 & -1 & 0\\ \vdots & 0 & \kern3mu\raise1mu{.}\kern3mu\raise6mu{.}\kern3mu\raise12mu{.} & 0 & \vdots\\ 0 & -1 & 0 & \cdots & 0 \end{pmatrix}$$ Then, $>_A$ is also $>_{\text{degrev}}$. I think that $A = (1,\dots,1)^T$ also works for this order. $$\large\textbf{Inverse Lexicographic Order}$$ Same list of term orders via matrices as lexicographic order list. $$\Large\textbf{Results and Thoughts}$$ After hours of investigating the number of ways to represent types of order via matrices, I took down the following conditions I believe may be correct: Given $\overline{a} = (a_1, \dots,a_n)$, in order for $\overline{a}A$ to be defined, we need the row of a matrix $A$ to corresponds with the column of $\overline{a}$. In order to specify the order by matrices, each term in $\overline{a}$ must be multiplied by each nonzero term in the matrix $A$.  Otherwise, the ordering of the tuples is not defined very well.  In other words, for this case, if $\overline{a} >_A \overline{b}$, then we can't conclude that $\overline{a}A > \overline{b}A$ for an ordering $>$. I believe the list of matrix-orderings I have is not enough; there might be more types of matrix-ordering that works for orders, like $\text{lex}$, $\text{deglex}$ and $\text{invlex}$, that I haven't figured out yet. Any comments or thoughts you have for the problem I am working on?  I exhausted lots of tries to determine the number of matrix-orderings.",,"['matrices', 'commutative-algebra', 'order-theory', 'groebner-basis']"
4,"Show square matrix, then matrix is invertible","Show square matrix, then matrix is invertible",,"Question: Show that if a square matrix $A$ satisfies the equation $A^2 + 2A + I = 0$, then $A$ must be invertible. My work: Based on the section I read, I will treat I to be an identity matrix, which is a $1 \times 1$ matrix with a $1$ or as an square matrix with main diagonal is all ones and the rest is zero. I will also treat the $O$ as a zero matrix, which is a matrix with all zeros. So the question wants me to show that the square matrix $A$ will make the following equation true. Okay so I pick $A$ to be $[-1]$, a $1 \times 1$ matrix with a $-1$ inside. This was out of pure luck. This makes $A^2 = [1]$. This makes $2A = [-2]$. The identity matrix is $[1]$. $1 + -2 + 1 = 0$. I satisfied the equation with my choice of $A$ which makes my choice of the matrix $A$ an invertible matrix. I know matrix $A *$ the inverse of $A$ is the identity matrix. $[-1] * inverse = [1]$. So the inverse has to be $[-1]$. So the inverse of $A$ is $A$. It looks right mathematically speaking. Anyone can tell me how they would pick the square matrix A because I pick my matrix out of pure luck?","Question: Show that if a square matrix $A$ satisfies the equation $A^2 + 2A + I = 0$, then $A$ must be invertible. My work: Based on the section I read, I will treat I to be an identity matrix, which is a $1 \times 1$ matrix with a $1$ or as an square matrix with main diagonal is all ones and the rest is zero. I will also treat the $O$ as a zero matrix, which is a matrix with all zeros. So the question wants me to show that the square matrix $A$ will make the following equation true. Okay so I pick $A$ to be $[-1]$, a $1 \times 1$ matrix with a $-1$ inside. This was out of pure luck. This makes $A^2 = [1]$. This makes $2A = [-2]$. The identity matrix is $[1]$. $1 + -2 + 1 = 0$. I satisfied the equation with my choice of $A$ which makes my choice of the matrix $A$ an invertible matrix. I know matrix $A *$ the inverse of $A$ is the identity matrix. $[-1] * inverse = [1]$. So the inverse has to be $[-1]$. So the inverse of $A$ is $A$. It looks right mathematically speaking. Anyone can tell me how they would pick the square matrix A because I pick my matrix out of pure luck?",,"['linear-algebra', 'matrices']"
5,Need help in understanding how to find an elementary matrix,Need help in understanding how to find an elementary matrix,,"I read this chapter in my book and thought I understood it, but I don't.  I tried working a problem to test my understanding and I just don't know how to get started. Given the following matrices: $A=\begin{bmatrix} 1 & 2 & -3 \\ 0 & 1 & 2 \\ -1 & 2 & 0  \\ \end{bmatrix}$ $B=\begin{bmatrix} -1 & 2 & 0 \\ 0 & 1 & 2 \\ 1 & 2 & -3  \\ \end{bmatrix}$ Find an elementary matrix $E$ such that $EA = B$ What I think I understand... a matrix is elementary when a single row operation forms an $I_n$ matrix.  I don't understand how this applies though.  Please help!","I read this chapter in my book and thought I understood it, but I don't.  I tried working a problem to test my understanding and I just don't know how to get started. Given the following matrices: $A=\begin{bmatrix} 1 & 2 & -3 \\ 0 & 1 & 2 \\ -1 & 2 & 0  \\ \end{bmatrix}$ $B=\begin{bmatrix} -1 & 2 & 0 \\ 0 & 1 & 2 \\ 1 & 2 & -3  \\ \end{bmatrix}$ Find an elementary matrix $E$ such that $EA = B$ What I think I understand... a matrix is elementary when a single row operation forms an $I_n$ matrix.  I don't understand how this applies though.  Please help!",,"['linear-algebra', 'matrices']"
6,Common Factor in Matrices?,Common Factor in Matrices?,,"I am trying to solve some matrix multiplications, but I would like to know If I am allowed to take a common factor from matrices like this C - ABC = (1 - AB)*C where A is m*n and B is n*n. And if yes, what matrix will be the 1 matrix? It cannot be an identity as the AB is m*n. Thanks a lot","I am trying to solve some matrix multiplications, but I would like to know If I am allowed to take a common factor from matrices like this C - ABC = (1 - AB)*C where A is m*n and B is n*n. And if yes, what matrix will be the 1 matrix? It cannot be an identity as the AB is m*n. Thanks a lot",,['matrices']
7,"Given a matrix $A$, show that it is positive.","Given a matrix , show that it is positive.",A,"Show that   $$A := \begin{bmatrix}7 & 2 & -4\\2 & 4 & -2\\-4 & -2 & 7 \end{bmatrix}$$   is positive definite. Could this be proven by showing that each of the vectors of the standard basis gives a positive result, e.g.: $$\begin{bmatrix}1 & 0 & 0\end{bmatrix} \begin{bmatrix}7 & 2 & -4\\2 & 4 & -2\\-4 & -2  & 7 \end{bmatrix}\begin{bmatrix} 1 \\ 0 \\ 0\end{bmatrix} > 0.$$ The second part of the question asks me to diagonalize the matrix using an orthogonal matrix, which as I understand, is to use elementary matrices on the rows and columns of the matrix to get it to a diagonal form. Would it make a difference if Ifirstly only dealt with the rows and only afterward used the exact matrices only on the columns? Thanks for your time.","Show that   $$A := \begin{bmatrix}7 & 2 & -4\\2 & 4 & -2\\-4 & -2 & 7 \end{bmatrix}$$   is positive definite. Could this be proven by showing that each of the vectors of the standard basis gives a positive result, e.g.: $$\begin{bmatrix}1 & 0 & 0\end{bmatrix} \begin{bmatrix}7 & 2 & -4\\2 & 4 & -2\\-4 & -2  & 7 \end{bmatrix}\begin{bmatrix} 1 \\ 0 \\ 0\end{bmatrix} > 0.$$ The second part of the question asks me to diagonalize the matrix using an orthogonal matrix, which as I understand, is to use elementary matrices on the rows and columns of the matrix to get it to a diagonal form. Would it make a difference if Ifirstly only dealt with the rows and only afterward used the exact matrices only on the columns? Thanks for your time.",,"['linear-algebra', 'matrices']"
8,Matrix Multiplication: only one solution?,Matrix Multiplication: only one solution?,,"Let $A=\begin{bmatrix}6 & 5\\-7 & 9\end{bmatrix}$ and $C=\begin{bmatrix}1 & -2\\4 & -8\end{bmatrix}$ .  Find all matrices $B$ such that $AC=BC$ . $\begin{bmatrix}6&5\\-7&9\end{bmatrix} \times \begin{bmatrix}1&-2\\4&-8\end{bmatrix}$ $= \begin{bmatrix}6 \times 1 + 5 \times 4 & 6 \times (-2) + 5 \times (-8) \\ (-7) \times 1 + 9 \times 4 & (-7) \times (-2) + 9 \times (-8)\end{bmatrix}$ $= \begin{bmatrix}26 & -52 \\ 29 & -58\end{bmatrix}$ $\begin{bmatrix}6&5\\-7&9\end{bmatrix} \times \begin{bmatrix}b_1&b_2\\b_3&b_4\end{bmatrix} = \begin{bmatrix}26 & -52 \\ 29 & -58\end{bmatrix}$ $\begin{bmatrix}6b_1 + 5b_3 & 6b_2 + 5b_4 \\ -7b_1 + 9b_3 & -7b_2 + 9b_4 \end{bmatrix} = \begin{bmatrix}26 & -52 \\ 29 & -58\end{bmatrix}$ Equations: $6b_1 + 5b_3 = 26$ $-7b_1 + 9b_3 = 29$ Eliminate $b_1$ : $42b_1 + 35b_3 = 182$ $-42b_1 + 54b_3 = 174\implies 89b_3 = 356 \implies b_3 = 4$ $6b_1 + 20 = 26 \implies b_1 = 1$ $6b_2 + 5b_4 = -52$ $-7b_2 + 9b_4 = -58$ Eliminate $b_2$ : $42b_2 + 35b_4 = -364$ $-42b_2 + 54b_4 = -348\implies89b_4 = -712 \implies b_4 = -8$ $6b_2 + 5(-8) = -52 \implies b_2 = -2$ Therefore, is it true that the only solution is when $A = B$ ? Or am I missing something? EDIT: Cameron Williams You mentioned this, but it is not without problems. $\begin{bmatrix}1&-2\\4&-8\end{bmatrix} \times \begin{bmatrix}b_1&b_2\\b_3&b_4\end{bmatrix} = \begin{bmatrix}26&-52\\29&-58\end{bmatrix}$ $\begin{bmatrix}b_1-2b_3 & b_2 - 2b_4 \\ 4b_1 - 8b_3 & 4b_2 - 8b_4\end{bmatrix} = \begin{bmatrix}26&-52\\29&-58\end{bmatrix}$ New equations: $b_1 - 2b_3 = 26$ $4b_1 - 8b_3 = 29$ Eliminate $b_1$ and $b_3$ and make an invalid equation: $(4b_1 - 8b_3) - 4(b_1 - 2b_3) = 29 - 104$ $0 = -75$ What does this mean? I do not think I need to use the other 2 equations, but I do not understand what this invalid equality is supposed to mean.","Let and .  Find all matrices such that . Equations: Eliminate : Eliminate : Therefore, is it true that the only solution is when ? Or am I missing something? EDIT: Cameron Williams You mentioned this, but it is not without problems. New equations: Eliminate and and make an invalid equation: What does this mean? I do not think I need to use the other 2 equations, but I do not understand what this invalid equality is supposed to mean.",A=\begin{bmatrix}6 & 5\\-7 & 9\end{bmatrix} C=\begin{bmatrix}1 & -2\\4 & -8\end{bmatrix} B AC=BC \begin{bmatrix}6&5\\-7&9\end{bmatrix} \times \begin{bmatrix}1&-2\\4&-8\end{bmatrix} = \begin{bmatrix}6 \times 1 + 5 \times 4 & 6 \times (-2) + 5 \times (-8) \\ (-7) \times 1 + 9 \times 4 & (-7) \times (-2) + 9 \times (-8)\end{bmatrix} = \begin{bmatrix}26 & -52 \\ 29 & -58\end{bmatrix} \begin{bmatrix}6&5\\-7&9\end{bmatrix} \times \begin{bmatrix}b_1&b_2\\b_3&b_4\end{bmatrix} = \begin{bmatrix}26 & -52 \\ 29 & -58\end{bmatrix} \begin{bmatrix}6b_1 + 5b_3 & 6b_2 + 5b_4 \\ -7b_1 + 9b_3 & -7b_2 + 9b_4 \end{bmatrix} = \begin{bmatrix}26 & -52 \\ 29 & -58\end{bmatrix} 6b_1 + 5b_3 = 26 -7b_1 + 9b_3 = 29 b_1 42b_1 + 35b_3 = 182 -42b_1 + 54b_3 = 174\implies 89b_3 = 356 \implies b_3 = 4 6b_1 + 20 = 26 \implies b_1 = 1 6b_2 + 5b_4 = -52 -7b_2 + 9b_4 = -58 b_2 42b_2 + 35b_4 = -364 -42b_2 + 54b_4 = -348\implies89b_4 = -712 \implies b_4 = -8 6b_2 + 5(-8) = -52 \implies b_2 = -2 A = B \begin{bmatrix}1&-2\\4&-8\end{bmatrix} \times \begin{bmatrix}b_1&b_2\\b_3&b_4\end{bmatrix} = \begin{bmatrix}26&-52\\29&-58\end{bmatrix} \begin{bmatrix}b_1-2b_3 & b_2 - 2b_4 \\ 4b_1 - 8b_3 & 4b_2 - 8b_4\end{bmatrix} = \begin{bmatrix}26&-52\\29&-58\end{bmatrix} b_1 - 2b_3 = 26 4b_1 - 8b_3 = 29 b_1 b_3 (4b_1 - 8b_3) - 4(b_1 - 2b_3) = 29 - 104 0 = -75,['matrices']
9,Criteria for positive semi-definiteness - zero diagonal,Criteria for positive semi-definiteness - zero diagonal,,"I am currently doing a bit of background reading on some fundamental topics in preparation for a talk, and came across a question relating to positive definiteness. It is taken from Horn and Johnson's book entitled Matrix Analysis, and reads as follows: Show that if a positive semidefinite matrix has a zero entry on the main diagonal, then the entire row and column to which it belongs must be zero. (See page 400) At first glance, this seemed a straightforward problem, so i'm pretty certain i'm missing something obvious. I have tried writing out various expansions, and playing with different characterisations as given in previous exercises/corollaries - all to no avail. If anyone could point me in the right direction, it would be most helpful! Best, Chris","I am currently doing a bit of background reading on some fundamental topics in preparation for a talk, and came across a question relating to positive definiteness. It is taken from Horn and Johnson's book entitled Matrix Analysis, and reads as follows: Show that if a positive semidefinite matrix has a zero entry on the main diagonal, then the entire row and column to which it belongs must be zero. (See page 400) At first glance, this seemed a straightforward problem, so i'm pretty certain i'm missing something obvious. I have tried writing out various expansions, and playing with different characterisations as given in previous exercises/corollaries - all to no avail. If anyone could point me in the right direction, it would be most helpful! Best, Chris",,"['analysis', 'matrices']"
10,How to prove that the inverse of a persymmetric matrix is also persymmetric?,How to prove that the inverse of a persymmetric matrix is also persymmetric?,,"An exercise in a textbook I'm using to brush up on my linear algebra asks to prove that the inverse of a persymmetric matrix is also persymmetric. I have a colleague's old notes in front of me with a solution, but I can't understand his reasoning. It states: Let $A_S$ denote the persymmetric matrix. Then $A = SA_S = A_SS$, where $A$ is the anti-diagonal identity matrix (i.e.: the identity matrix rotated 90°). $$(SA_S)^{-1} = A_S^{-1}S^{-1}$$ In a previous question, we have proven that $A = A^{-1}$ ($A^2 = I$, where $I$ is the identity matrix, and thus $A^2 = AA = I$, and so multiplying both sides by the inverse $A^{-1}$ we have $A = A^{-1}$). Thus: $$(SA_S)^{-1} = A_S^{-1}S^{-1} = A_S^{-1}S$$ Therefore we need only prove $A_S^{-1}$ is symmetric. In a previous question we proved the transpose of the inverse of a matrix is equivalent to the inverse of its transpose, so $(A_S^{-1})^T = (A_S^T)^{-1}$. Since we know $A_S^T = A_S$, $(A_S^T)^{-1} = A_S^{-1}$, therefore $A_S^{-1}$ is symmetric. QED. I can't follow several bits of reasoning in this answer. First of all, I don't see how the second equation follows from the setup--how does $A = A^{-1}$ tell us that $A_S^{-1}S^{-1} = A_S^{-1}S$? Why is proving $A_S^{-1}$ to be symmetric sufficient to prove $A_S^{-1}$ is persymmetric? The author claims $A_S^T = A_S$, which would be true for a symmetric matrix, but not a persymmetric one (it is true that the transpose of a persymmetric matrix is also persymmetric). What is the basis for this claim? It's also possible that my colleague's answer is incorrect. I have an idea that perhaps using the facts that $A$ rotated by 90° is $I$ and $A_S$ rotated by 90° is a symmetric matrix in order to tackle the proof, but I'm stuck on the specifics. Can someone either explain my colleague's answer, give me a hint on how to prove this statement, or provide a proof of their own?","An exercise in a textbook I'm using to brush up on my linear algebra asks to prove that the inverse of a persymmetric matrix is also persymmetric. I have a colleague's old notes in front of me with a solution, but I can't understand his reasoning. It states: Let $A_S$ denote the persymmetric matrix. Then $A = SA_S = A_SS$, where $A$ is the anti-diagonal identity matrix (i.e.: the identity matrix rotated 90°). $$(SA_S)^{-1} = A_S^{-1}S^{-1}$$ In a previous question, we have proven that $A = A^{-1}$ ($A^2 = I$, where $I$ is the identity matrix, and thus $A^2 = AA = I$, and so multiplying both sides by the inverse $A^{-1}$ we have $A = A^{-1}$). Thus: $$(SA_S)^{-1} = A_S^{-1}S^{-1} = A_S^{-1}S$$ Therefore we need only prove $A_S^{-1}$ is symmetric. In a previous question we proved the transpose of the inverse of a matrix is equivalent to the inverse of its transpose, so $(A_S^{-1})^T = (A_S^T)^{-1}$. Since we know $A_S^T = A_S$, $(A_S^T)^{-1} = A_S^{-1}$, therefore $A_S^{-1}$ is symmetric. QED. I can't follow several bits of reasoning in this answer. First of all, I don't see how the second equation follows from the setup--how does $A = A^{-1}$ tell us that $A_S^{-1}S^{-1} = A_S^{-1}S$? Why is proving $A_S^{-1}$ to be symmetric sufficient to prove $A_S^{-1}$ is persymmetric? The author claims $A_S^T = A_S$, which would be true for a symmetric matrix, but not a persymmetric one (it is true that the transpose of a persymmetric matrix is also persymmetric). What is the basis for this claim? It's also possible that my colleague's answer is incorrect. I have an idea that perhaps using the facts that $A$ rotated by 90° is $I$ and $A_S$ rotated by 90° is a symmetric matrix in order to tackle the proof, but I'm stuck on the specifics. Can someone either explain my colleague's answer, give me a hint on how to prove this statement, or provide a proof of their own?",,"['linear-algebra', 'matrices', 'proof-verification', 'alternative-proof']"
11,How can one actually use Adjacency Matrix for understanding a graph?,How can one actually use Adjacency Matrix for understanding a graph?,,"I don't see any real reason why we would use an AM to represent a graph, beside visual appeal and ease. Generally, we would perform matrix operations on Matrices like |A|, Transpose and loads of other things but that magic doesn't seem to add up to what I am looking at, AM. Anyone care to explain or give me some intuition so I may respect AM a bit more and use it to study the graph at hand purely by playing around with the numbers. Obviously, you get the point.","I don't see any real reason why we would use an AM to represent a graph, beside visual appeal and ease. Generally, we would perform matrix operations on Matrices like |A|, Transpose and loads of other things but that magic doesn't seem to add up to what I am looking at, AM. Anyone care to explain or give me some intuition so I may respect AM a bit more and use it to study the graph at hand purely by playing around with the numbers. Obviously, you get the point.",,"['matrices', 'graph-theory']"
12,Why is the map bewteen a matrix and its characteristic polynomial continous?,Why is the map bewteen a matrix and its characteristic polynomial continous?,,"One may define the following map : $\begin{array}{l|rcl} f : & M_n(\mathbb R) & \longrightarrow & \mathbb R_n[X] \\     & A & \longmapsto & p_A \end{array}$ Why is $f$ continuous ? Which norms are convenient  for this problem ? Note that $f$ isn't linear. I tried to use the sequential way, without success...","One may define the following map : $\begin{array}{l|rcl} f : & M_n(\mathbb R) & \longrightarrow & \mathbb R_n[X] \\     & A & \longmapsto & p_A \end{array}$ Why is $f$ continuous ? Which norms are convenient  for this problem ? Note that $f$ isn't linear. I tried to use the sequential way, without success...",,"['linear-algebra', 'matrices']"
13,how to find all the solutions to $I+A+\cdots+A^n=0.$,how to find all the solutions to,I+A+\cdots+A^n=0.,"Let $GL_3(\mathbb{Z}[i])$ be the group of invertible $3\times 3$ matrices whose coefficients are Gaussian integers.I want to find all the pair $(A\in GL_3(\mathbb{Z}[i]),n\in\mathbb{Z})$ satisfying $$I+A+\cdots+A^n=0.$$ It is easy to dope out some obvious solutions.Say $(-I,1)$.But how to find all the solutions to this equation?","Let $GL_3(\mathbb{Z}[i])$ be the group of invertible $3\times 3$ matrices whose coefficients are Gaussian integers.I want to find all the pair $(A\in GL_3(\mathbb{Z}[i]),n\in\mathbb{Z})$ satisfying $$I+A+\cdots+A^n=0.$$ It is easy to dope out some obvious solutions.Say $(-I,1)$.But how to find all the solutions to this equation?",,"['linear-algebra', 'matrices', 'arithmetic', 'matrix-equations']"
14,When is the symmetric part of a matrix positive definite?,When is the symmetric part of a matrix positive definite?,,"Suppose there is a (non-symmetric) real square matrix $A$ with symmetric part $A+A^T$. What are some conditions on $A$ that are sufficient for $A+A^T$ to be positive definite? For example, if the eigenvalues of $A$ are strictly positive is $A+A^T$ positive definite? (EDIT: This part of the question is answered in the negative in the comments). This would then give the result I actually want which is that given two positive definite matrices $C$ and $D$ it follows that the symmetric part of $CD$ is also positive definite. (EDIT: But I think it is still not clear if $CD+DC>0$ - this is (perhaps) a slightly more special case than $A+A^T$ with $A$ having positive eigenvalues.)","Suppose there is a (non-symmetric) real square matrix $A$ with symmetric part $A+A^T$. What are some conditions on $A$ that are sufficient for $A+A^T$ to be positive definite? For example, if the eigenvalues of $A$ are strictly positive is $A+A^T$ positive definite? (EDIT: This part of the question is answered in the negative in the comments). This would then give the result I actually want which is that given two positive definite matrices $C$ and $D$ it follows that the symmetric part of $CD$ is also positive definite. (EDIT: But I think it is still not clear if $CD+DC>0$ - this is (perhaps) a slightly more special case than $A+A^T$ with $A$ having positive eigenvalues.)",,['matrices']
15,Square matrix and determinant inequality,Square matrix and determinant inequality,,"Let $A, B, C$ be invertible $n \times n$ square matrices with $AC=CA$ and $B^2C^2=I_n$ Is $\det(ABC +CBA +A^2+I_n)$  always $\geq 0$?","Let $A, B, C$ be invertible $n \times n$ square matrices with $AC=CA$ and $B^2C^2=I_n$ Is $\det(ABC +CBA +A^2+I_n)$  always $\geq 0$?",,['linear-algebra']
16,Determinant of a matrix with generalized binomial coefficients,Determinant of a matrix with generalized binomial coefficients,,Let $$ A= \begin{bmatrix}\binom{-1/2}{1}&\binom{-1/2}{0}&0&0&...&0\\ \binom{-1/2}{2}&\binom{-1/2}{1}&\binom{-1/2}{0}&0&&...\\...&&&\binom{-1/2}{0}\\ &&&...&&\binom{-1/2}{0}\\ \binom{-1/2}{n}&\binom{-1/2}{n-1}&\binom{-1/2}{n-2}&\binom{-1/2}{n-3}&...&\binom{-1/2}{1}\end{bmatrix}. $$ How can I calculate $\det A$? Thank you very much.,Let $$ A= \begin{bmatrix}\binom{-1/2}{1}&\binom{-1/2}{0}&0&0&...&0\\ \binom{-1/2}{2}&\binom{-1/2}{1}&\binom{-1/2}{0}&0&&...\\...&&&\binom{-1/2}{0}\\ &&&...&&\binom{-1/2}{0}\\ \binom{-1/2}{n}&\binom{-1/2}{n-1}&\binom{-1/2}{n-2}&\binom{-1/2}{n-3}&...&\binom{-1/2}{1}\end{bmatrix}. $$ How can I calculate $\det A$? Thank you very much.,,"['linear-algebra', 'matrices', 'determinant']"
17,"$T=-T^{*}$, show that $T+\alpha I$ is invertible.",", show that  is invertible.",T=-T^{*} T+\alpha I,"Please don't answer the question. Just tell me if I am in the right direction. I should be able to solve this. We are given $T=-T^{*}$, show that $T+\alpha I$ is invertibe for all real alphas that aren't zero. What I did: $det(T+\alpha I) = det(-T^{*}+\alpha I)=det(-\bar T+\alpha I) = \overline {det(-T+\alpha I)}$ And here I'm pretty much stuck. Am I in the right direction?","Please don't answer the question. Just tell me if I am in the right direction. I should be able to solve this. We are given $T=-T^{*}$, show that $T+\alpha I$ is invertibe for all real alphas that aren't zero. What I did: $det(T+\alpha I) = det(-T^{*}+\alpha I)=det(-\bar T+\alpha I) = \overline {det(-T+\alpha I)}$ And here I'm pretty much stuck. Am I in the right direction?",,"['linear-algebra', 'matrices', 'complex-numbers', 'determinant']"
18,Jordan Form Superdiagonal,Jordan Form Superdiagonal,,"How do you know how many of the super diagonal entries in the Jordan Form are zeros and how many are ones, and where they are placed? Thanks.","How do you know how many of the super diagonal entries in the Jordan Form are zeros and how many are ones, and where they are placed? Thanks.",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
19,Decompose symmetric matrix to scaling factors,Decompose symmetric matrix to scaling factors,,I have a symmetric square matrix $P$ composed by left- and right-multiplying another symmetric square matrix $Z$ with a diagonal matrix $Λ$: $$P = ΛZΛ$$ i.e. ($λ_i$ means $λ_{ii}$): $$ \begin{bmatrix} p_{11} & p_{12} & \cdots & p_{1n} \\ p_{21} & p_{22} & \cdots & p_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ p_{n1} & p_{n2} & \cdots & p_{nn} \\ \end{bmatrix} = \begin{bmatrix} λ_1λ_1z_{11} & λ_1λ_2z_{12} & \cdots & λ_1λ_nz_{1n} \\ λ_2λ_1z_{21} & λ_2λ_2z_{22} & \cdots & λ_2λ_nz_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ λ_nλ_1z_{n1} & λ_nλ_2z_{n2} & \cdots & λ_nλ_nz_{nn} \\ \end{bmatrix} $$ Is there a decomposition able to do the reverse operation (going from $Z$ and $P$ to $Λ$)?,I have a symmetric square matrix $P$ composed by left- and right-multiplying another symmetric square matrix $Z$ with a diagonal matrix $Λ$: $$P = ΛZΛ$$ i.e. ($λ_i$ means $λ_{ii}$): $$ \begin{bmatrix} p_{11} & p_{12} & \cdots & p_{1n} \\ p_{21} & p_{22} & \cdots & p_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ p_{n1} & p_{n2} & \cdots & p_{nn} \\ \end{bmatrix} = \begin{bmatrix} λ_1λ_1z_{11} & λ_1λ_2z_{12} & \cdots & λ_1λ_nz_{1n} \\ λ_2λ_1z_{21} & λ_2λ_2z_{22} & \cdots & λ_2λ_nz_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ λ_nλ_1z_{n1} & λ_nλ_2z_{n2} & \cdots & λ_nλ_nz_{nn} \\ \end{bmatrix} $$ Is there a decomposition able to do the reverse operation (going from $Z$ and $P$ to $Λ$)?,,"['linear-algebra', 'matrices']"
20,"If $(A-2I)^3(A+2I)^2=0$, then what are the possible Jordan canonical forms of $A$?","If , then what are the possible Jordan canonical forms of ?",(A-2I)^3(A+2I)^2=0 A,"Here is the exercise: Let $A$ be a $5\times5$ complex matrix such that $(A-2)^3(A+2)^2=0$, where we define $A-\mu:=A-\mu I$ for scalar $\mu$. Assume that $\lambda=2$ is an eigenvalue of $A$ and its geometric multiplicity is at least $2$. What are the possibilities for the Jordan canonical form (JCF)? What I know so far from the assumption is that the minimal polynomial of $A$ is of the form $f(x)=(x-2)^i(x+2)^j$ where $1\leq i\leq 3$ and $0\leq j\leq 2$. The number of blocks in the Jordan segment $J(2)$ is at least $2$. One can write the possible minimal polynomial one by one, which gives the information of the size of the largest block in each Jordan segment, and use the possible geometric multiplicity of $\lambda=2$ to find JCF. Here are my questions: Is there an alternative approach? Can we use the characteristic polynomial of $A$ here?","Here is the exercise: Let $A$ be a $5\times5$ complex matrix such that $(A-2)^3(A+2)^2=0$, where we define $A-\mu:=A-\mu I$ for scalar $\mu$. Assume that $\lambda=2$ is an eigenvalue of $A$ and its geometric multiplicity is at least $2$. What are the possibilities for the Jordan canonical form (JCF)? What I know so far from the assumption is that the minimal polynomial of $A$ is of the form $f(x)=(x-2)^i(x+2)^j$ where $1\leq i\leq 3$ and $0\leq j\leq 2$. The number of blocks in the Jordan segment $J(2)$ is at least $2$. One can write the possible minimal polynomial one by one, which gives the information of the size of the largest block in each Jordan segment, and use the possible geometric multiplicity of $\lambda=2$ to find JCF. Here are my questions: Is there an alternative approach? Can we use the characteristic polynomial of $A$ here?",,"['linear-algebra', 'matrices']"
21,Minors of a positive definite matrix are positive definite,Minors of a positive definite matrix are positive definite,,"All main minors of a positive definite matrix are positive definite as   well and therefore $A$ is  strictly invertible. All I know about positive-definiteness is that for the symmetric matrix $A$ the following inequality holds: $$x^TAx>0, \forall x(\neq 0) \in \mathbb R^3$$ Could you please give me some hints as to how can I prove the above theorem, specially its second part, what does  positive-definiteness have to do with being invertible?","All main minors of a positive definite matrix are positive definite as   well and therefore $A$ is  strictly invertible. All I know about positive-definiteness is that for the symmetric matrix $A$ the following inequality holds: $$x^TAx>0, \forall x(\neq 0) \in \mathbb R^3$$ Could you please give me some hints as to how can I prove the above theorem, specially its second part, what does  positive-definiteness have to do with being invertible?",,"['linear-algebra', 'matrices']"
22,Find minimal polynomial of a difficult transformation,Find minimal polynomial of a difficult transformation,,"we are asked to find the minimal polynomial of the transformation: $T: M_n(\mathbb C)\to M_n(\mathbb C)$ $T(A)=CA$ when $C$ is a diagonal matrix with the values $c_1,c_2,c_3,...,c_n$ on the diagonal and $0$ everywhere else. What I did: I found the matrix of the transformation with respect to the standard base. If I am not mistaken, it is a diagonal matrix with values $c_1,c_2,c_3,...c_n$ where every one of them is in the diagonal $n$ times. So the characteristic polynomial is: $(t-c_1)^n(t-c_2)^n...(t-c_n)^n$ How do I find the minimal polynomial from here? Obviously the minimal polynomial should be: $(t-c_1)^{\alpha_1}(t-c_2)^{\alpha_2}...(t-c_n)^{\alpha_n}$ but how do i find $\alpha_i$ for all $i$?","we are asked to find the minimal polynomial of the transformation: $T: M_n(\mathbb C)\to M_n(\mathbb C)$ $T(A)=CA$ when $C$ is a diagonal matrix with the values $c_1,c_2,c_3,...,c_n$ on the diagonal and $0$ everywhere else. What I did: I found the matrix of the transformation with respect to the standard base. If I am not mistaken, it is a diagonal matrix with values $c_1,c_2,c_3,...c_n$ where every one of them is in the diagonal $n$ times. So the characteristic polynomial is: $(t-c_1)^n(t-c_2)^n...(t-c_n)^n$ How do I find the minimal polynomial from here? Obviously the minimal polynomial should be: $(t-c_1)^{\alpha_1}(t-c_2)^{\alpha_2}...(t-c_n)^{\alpha_n}$ but how do i find $\alpha_i$ for all $i$?",,"['linear-algebra', 'matrices', 'polynomials', 'minimal-polynomials']"
23,How prove this two symmetric matrices $AB=0$,How prove this two symmetric matrices,AB=0,"Let $A,B$ be real symmetric matrices, and for any $n\in \Bbb N^{+}$, and for all $x,y\in \Bbb R$, we have $$tr(xA+yB)^n=x^ntr(A^n)+y^ntr(B^n).$$ Show that $AB=0$. My try:since $$(xA+yB)^n=x^nA^n+\binom{n}{1}x^{n-1}A^{n-1}yB+\cdots+\binom{n}{n-1}xA(yB)^{n-1}+y^nB^n$$ so $$tr\left(x^nA^n+\binom{n}{1}x^{n-1}A^{n-1}yB+\cdots+\binom{n}{n-1}xA(yB)^{n-1}+y^nB^n\right)=x^ntr(A^n)+y^ntr(B^n)$$ then  $$tr\left(\binom{n}{1}x^{n-1}A^{n-1}yB+\cdots+\binom{n}{n-1}xA(yB)^{n-1}\right)=0$$ so $$\binom{n}{1}x^{n-1}ytr(A^{n-1}B)+\cdots+\binom{n}{n-1}xy^{n-1}tr(AB^{n-1})=0$$ then I can't I found this is a nice result, maybe this is an odd problem, and I can't solve it. Thank you.","Let $A,B$ be real symmetric matrices, and for any $n\in \Bbb N^{+}$, and for all $x,y\in \Bbb R$, we have $$tr(xA+yB)^n=x^ntr(A^n)+y^ntr(B^n).$$ Show that $AB=0$. My try:since $$(xA+yB)^n=x^nA^n+\binom{n}{1}x^{n-1}A^{n-1}yB+\cdots+\binom{n}{n-1}xA(yB)^{n-1}+y^nB^n$$ so $$tr\left(x^nA^n+\binom{n}{1}x^{n-1}A^{n-1}yB+\cdots+\binom{n}{n-1}xA(yB)^{n-1}+y^nB^n\right)=x^ntr(A^n)+y^ntr(B^n)$$ then  $$tr\left(\binom{n}{1}x^{n-1}A^{n-1}yB+\cdots+\binom{n}{n-1}xA(yB)^{n-1}\right)=0$$ so $$\binom{n}{1}x^{n-1}ytr(A^{n-1}B)+\cdots+\binom{n}{n-1}xy^{n-1}tr(AB^{n-1})=0$$ then I can't I found this is a nice result, maybe this is an odd problem, and I can't solve it. Thank you.",,"['linear-algebra', 'matrices']"
24,"Show $\exp(A)=\cos(\sqrt{\det(A)})I+\frac{\sin(\sqrt{\det(A)})}{\sqrt{\det(A)}}A,A\in M(2,\mathbb{C})$",Show,"\exp(A)=\cos(\sqrt{\det(A)})I+\frac{\sin(\sqrt{\det(A)})}{\sqrt{\det(A)}}A,A\in M(2,\mathbb{C})","Show $$\exp(A)=\cos(\sqrt{\det(A)})I+\frac{\sin(\sqrt{\det(A)})}{\sqrt{\det(A)}}A$$   for $A\in M(2,\mathbb{C})$. In addition, $\operatorname{trace}(A)=0$. Can anyone give me a hint how this can connect with cosine and sine? Thanks!","Show $$\exp(A)=\cos(\sqrt{\det(A)})I+\frac{\sin(\sqrt{\det(A)})}{\sqrt{\det(A)}}A$$   for $A\in M(2,\mathbb{C})$. In addition, $\operatorname{trace}(A)=0$. Can anyone give me a hint how this can connect with cosine and sine? Thanks!",,"['linear-algebra', 'matrices', 'trace']"
25,Computing derivative of function between matrices,Computing derivative of function between matrices,,"Let $M_{k,n}$ be the set of all $k\times n$ matrices, $S_k$ be the set of all symmetric $k\times k$ matrices, and $I_k$ the identity $k\times k$ matrix. Let $\phi:M_{k,n}\rightarrow S_k$ be the map $\phi(A)=AA^t$. Show that $D\phi(A)$ can be identified with the map $M_{k,n}\rightarrow S_k$ with $B\rightarrow BA^t+AB^t$. I don't really understand how to compute the map $D\phi(A)$. Usually when there is a map $f:\mathbb{R}^s\rightarrow\mathbb{R}^t$, I compute the map $Df(x)$ by computing the partial derivatives $\partial f_i/\partial x_j$ for $i=1,\ldots,t$ and $j=1,\ldots,s$. But here we have a map from $M_{k,n}$ to $S_k$. How can we show that $D\phi(A)\cdot B=BA^t+AB^t$?","Let $M_{k,n}$ be the set of all $k\times n$ matrices, $S_k$ be the set of all symmetric $k\times k$ matrices, and $I_k$ the identity $k\times k$ matrix. Let $\phi:M_{k,n}\rightarrow S_k$ be the map $\phi(A)=AA^t$. Show that $D\phi(A)$ can be identified with the map $M_{k,n}\rightarrow S_k$ with $B\rightarrow BA^t+AB^t$. I don't really understand how to compute the map $D\phi(A)$. Usually when there is a map $f:\mathbb{R}^s\rightarrow\mathbb{R}^t$, I compute the map $Df(x)$ by computing the partial derivatives $\partial f_i/\partial x_j$ for $i=1,\ldots,t$ and $j=1,\ldots,s$. But here we have a map from $M_{k,n}$ to $S_k$. How can we show that $D\phi(A)\cdot B=BA^t+AB^t$?",,"['real-analysis', 'matrices', 'derivatives']"
26,Inequality with determinants problem,Inequality with determinants problem,,"Let $A,B \in M_{2}(\mathbb{R})$ with $AB=BA.$ Prove that: $$\det(A^{2}+AB+B^{2})\geq (\det(A)-\det(B))^{2}$$","Let $A,B \in M_{2}(\mathbb{R})$ with $AB=BA.$ Prove that: $$\det(A^{2}+AB+B^{2})\geq (\det(A)-\det(B))^{2}$$",,"['matrices', 'inequality', 'determinant']"
27,Calculating index of a subgroup,Calculating index of a subgroup,,"Compute the index $[Γ( 1 ) ′ : Γ_0 ( N ) ′ ]$ where $Γ(1)' := SL(2,\mathbb{Z})$ $Γ_0(N)':= \{ \begin{pmatrix}   a & b\\   c & d\\  \end{pmatrix} \in Γ(1)' : c \equiv 0 \mod{N} \} $ I'm basically stuck on how to get started.  Any help would be greatly appreciated!","Compute the index $[Γ( 1 ) ′ : Γ_0 ( N ) ′ ]$ where $Γ(1)' := SL(2,\mathbb{Z})$ $Γ_0(N)':= \{ \begin{pmatrix}   a & b\\   c & d\\  \end{pmatrix} \in Γ(1)' : c \equiv 0 \mod{N} \} $ I'm basically stuck on how to get started.  Any help would be greatly appreciated!",,"['group-theory', 'matrices', 'finite-groups', 'modular-forms']"
28,Why $Hx=x-(\rho u^Tx)u$?,Why ?,Hx=x-(\rho u^Tx)u,"A Householder reflection is  a matrix of the form $$H=I-\rho uu^T$$ with $\rho=2/\|u\|^2$. Obviously, $Hx=x-\rho uu^Tx$. Textbook http://www.mathworks.se/moler/leastsquares.pdf says that  $$Hx=x-(\rho u^Tx)u$$ However, I can not see the proof of this statement.","A Householder reflection is  a matrix of the form $$H=I-\rho uu^T$$ with $\rho=2/\|u\|^2$. Obviously, $Hx=x-\rho uu^Tx$. Textbook http://www.mathworks.se/moler/leastsquares.pdf says that  $$Hx=x-(\rho u^Tx)u$$ However, I can not see the proof of this statement.",,"['linear-algebra', 'matrices', 'reflection']"
29,Simple question about matrices,Simple question about matrices,,"My question is simple : If one replaces some of the entries of a matrix by 0, does he obtain necessarily a matrix with a lower norm? I have to precise that the norm I use is the maximum of the singular values of the matrix (which is an operator norm).","My question is simple : If one replaces some of the entries of a matrix by 0, does he obtain necessarily a matrix with a lower norm? I have to precise that the norm I use is the maximum of the singular values of the matrix (which is an operator norm).",,"['matrices', 'normed-spaces']"
30,Conjugacy classes and orders of matrices.,Conjugacy classes and orders of matrices.,,"The following are prime decompositions in $\Bbb{Z}_7[x]$: $x^8+1= (x^2-x-1)(x^2+x-1)(x^2+3x-1)(x^2+4x-1)$ $x^4+1= (x^2+3x+1)(x^2+4x+1)$ (a) Give representatives for the conjugacy classes of elements of order dividing 16 in $Gl_2(\Bbb{Z}_7)$ and give the order of each. (b) Show that there are no elements of order 32 in $Gl_2(\Bbb{Z}_7)$. (a) We can write the rational canonical forms as representatives. $x^2-x-1 \rightarrow \begin{bmatrix} 0 & 1 \\1 & 1 \end{bmatrix}$ $x^2+x-1  \rightarrow  \begin{bmatrix} 0 & 1 \\1 & -1 \end{bmatrix}$ $x^2+3x-1 \rightarrow \begin{bmatrix} 0 &  1 \\1 & -3 \end{bmatrix}$ $x^2+4x-1 \rightarrow \begin{bmatrix} 0 &  1 \\1 & -4 \end{bmatrix}$ $x^3+3x+1 \rightarrow \begin{bmatrix} 0 &  -1 \\1 & -3 \end{bmatrix}$ $x^2+4x+1 \rightarrow \begin{bmatrix} 0 &  -1 \\1 & -4 \end{bmatrix}$ $x^2+1 \rightarrow \begin{bmatrix} 0 &  -1 \\1 & 0 \end{bmatrix}$. The first four matrices have order 16, since they satisfy $x^8+1=0$ so $x^8=-1 \implies (x^8)^2 = 1$. The last two have order 8, since $x^4=-1 \implies x^8=1$. (b)We don’t know the factorization of $x^{16}+1$, so we can’t check whether or not the matrices that satisfy it actually belong to $Gl_2(\Bbb{Z}_7)$. I wasn’t able to tell whether this was even reducible in $Z_7[x]$ to begin with, so I was kind of stuck here, and I was wondering if anybody could give me a hint... Thanks in advance","The following are prime decompositions in $\Bbb{Z}_7[x]$: $x^8+1= (x^2-x-1)(x^2+x-1)(x^2+3x-1)(x^2+4x-1)$ $x^4+1= (x^2+3x+1)(x^2+4x+1)$ (a) Give representatives for the conjugacy classes of elements of order dividing 16 in $Gl_2(\Bbb{Z}_7)$ and give the order of each. (b) Show that there are no elements of order 32 in $Gl_2(\Bbb{Z}_7)$. (a) We can write the rational canonical forms as representatives. $x^2-x-1 \rightarrow \begin{bmatrix} 0 & 1 \\1 & 1 \end{bmatrix}$ $x^2+x-1  \rightarrow  \begin{bmatrix} 0 & 1 \\1 & -1 \end{bmatrix}$ $x^2+3x-1 \rightarrow \begin{bmatrix} 0 &  1 \\1 & -3 \end{bmatrix}$ $x^2+4x-1 \rightarrow \begin{bmatrix} 0 &  1 \\1 & -4 \end{bmatrix}$ $x^3+3x+1 \rightarrow \begin{bmatrix} 0 &  -1 \\1 & -3 \end{bmatrix}$ $x^2+4x+1 \rightarrow \begin{bmatrix} 0 &  -1 \\1 & -4 \end{bmatrix}$ $x^2+1 \rightarrow \begin{bmatrix} 0 &  -1 \\1 & 0 \end{bmatrix}$. The first four matrices have order 16, since they satisfy $x^8+1=0$ so $x^8=-1 \implies (x^8)^2 = 1$. The last two have order 8, since $x^4=-1 \implies x^8=1$. (b)We don’t know the factorization of $x^{16}+1$, so we can’t check whether or not the matrices that satisfy it actually belong to $Gl_2(\Bbb{Z}_7)$. I wasn’t able to tell whether this was even reducible in $Z_7[x]$ to begin with, so I was kind of stuck here, and I was wondering if anybody could give me a hint... Thanks in advance",,"['linear-algebra', 'group-theory']"
31,"Showing that if an equation has a unique solution for one variable, then it has unique solutions for all.","Showing that if an equation has a unique solution for one variable, then it has unique solutions for all.",,"I have a problem and a proposed solution. Please tell me if I'm correct. Problem: Let $A$ be a square matrix. Show that if the system $AX=B$ has a unique solution for some particular column vector B, then it has a unique solution for all $B$. Solution: If $AX=B$ has a unique solution for some column vector $B$, then $A$ in reduced row echelon form has a pivot in each column and $A$ can be reduced to $I_n$, for $A$,$\\ n \times n$. Since the number of equations = the number of unknowns, we will have column vector $(n \times 1)$ of $x_i$'s  = column vector $n \times 1$ of $b_i$'s. Hence, varying $B$ is equivalent to varying $X$ and will create a new solution for every change made to $B$. Thanks!","I have a problem and a proposed solution. Please tell me if I'm correct. Problem: Let $A$ be a square matrix. Show that if the system $AX=B$ has a unique solution for some particular column vector B, then it has a unique solution for all $B$. Solution: If $AX=B$ has a unique solution for some column vector $B$, then $A$ in reduced row echelon form has a pivot in each column and $A$ can be reduced to $I_n$, for $A$,$\\ n \times n$. Since the number of equations = the number of unknowns, we will have column vector $(n \times 1)$ of $x_i$'s  = column vector $n \times 1$ of $b_i$'s. Hence, varying $B$ is equivalent to varying $X$ and will create a new solution for every change made to $B$. Thanks!",,[]
32,Quadratic Operator Notation?,Quadratic Operator Notation?,,"I am dealing with functions that are linear combinations of: $[x_1^2, x_2^2... x_n^2, x_1x_2, x_1x_3... x_n-1x_n]$ spanned over a column. All these functions obey the law: $F(aX) = a^2F(X)$ for constant values a. Is there a notation for handling these? Similar to matrix notation for linear operators over $R^n$ (in fact matrices work just fine over $C^n$ as well) does there exist some type of concise ""box of numbers and stuff"" notation for handling quadratic operators? I tried designing my own notations which almost work fine except: If I allow both pure polynomials (ex: $x^3$ and $y^2$) and mixed polynomials (ex: $x^3y^2) handling composition using a box of numbers notation doesn't appear straightforward. If I allow only pure polynomials then I have a syntax for working with matrix-like objects that can handle composition just fine (it generalizes cleanly to higher powers) but I lose the ability to handle mixed expressions. The factoring of constant values and handling sum of expression are both fine in both notations. How do I create an all encompassing notation in case one does not exist?","I am dealing with functions that are linear combinations of: $[x_1^2, x_2^2... x_n^2, x_1x_2, x_1x_3... x_n-1x_n]$ spanned over a column. All these functions obey the law: $F(aX) = a^2F(X)$ for constant values a. Is there a notation for handling these? Similar to matrix notation for linear operators over $R^n$ (in fact matrices work just fine over $C^n$ as well) does there exist some type of concise ""box of numbers and stuff"" notation for handling quadratic operators? I tried designing my own notations which almost work fine except: If I allow both pure polynomials (ex: $x^3$ and $y^2$) and mixed polynomials (ex: $x^3y^2) handling composition using a box of numbers notation doesn't appear straightforward. If I allow only pure polynomials then I have a syntax for working with matrix-like objects that can handle composition just fine (it generalizes cleanly to higher powers) but I lose the ability to handle mixed expressions. The factoring of constant values and handling sum of expression are both fine in both notations. How do I create an all encompassing notation in case one does not exist?",,"['linear-algebra', 'matrices', 'polynomials', 'operator-theory']"
33,Set of permutation matrices,Set of permutation matrices,,I'm stuck in this problem. Prove the set $P$ of $n×n$ permutation matrices spans a subspace of dimension $(n−1)^2+1$,I'm stuck in this problem. Prove the set $P$ of $n×n$ permutation matrices spans a subspace of dimension $(n−1)^2+1$,,"['linear-algebra', 'matrices', 'permutations']"
34,Norm of a linear transformation,Norm of a linear transformation,,"Let $T:\mathbb R^2\to \mathbb R^2$ be given by the matrix $\begin{pmatrix}a&b\\ c& d\end{pmatrix}$.  Let $u:=a^2+b^2+c^2+d^2+2(ad-bc)$ and $v:=a^2+b^2+c^2+d^2-2(ad-bc)$. I need to show that $\mid\mid T\mid\mid=\frac 12 (\sqrt u+\sqrt v)$. I tried using the definition $\mid\mid T\mid\mid =\sup\{\mid \mid Tx\mid\mid\ : x\in \mathbb R^2, \mid\mid x\mid\mid=1 \}$, and trying to express each $x$ as $(\cos \theta,\sin \theta)$ for $\theta\in  [0,2\pi)$.  This just led to a big mess, though, eventually reaching that $$\mid\mid T(\cos\theta,\sin \theta)\mid\mid^2 < a^2+b^2+c^2+d^2+2(ab+cd)$$ This is kind of similar to $u$ and $v$ but not quite good enough, so I'm not sure now if this approach with $\theta$ and the trig functions is the right one. I also don't know how to get started for the other inequality.","Let $T:\mathbb R^2\to \mathbb R^2$ be given by the matrix $\begin{pmatrix}a&b\\ c& d\end{pmatrix}$.  Let $u:=a^2+b^2+c^2+d^2+2(ad-bc)$ and $v:=a^2+b^2+c^2+d^2-2(ad-bc)$. I need to show that $\mid\mid T\mid\mid=\frac 12 (\sqrt u+\sqrt v)$. I tried using the definition $\mid\mid T\mid\mid =\sup\{\mid \mid Tx\mid\mid\ : x\in \mathbb R^2, \mid\mid x\mid\mid=1 \}$, and trying to express each $x$ as $(\cos \theta,\sin \theta)$ for $\theta\in  [0,2\pi)$.  This just led to a big mess, though, eventually reaching that $$\mid\mid T(\cos\theta,\sin \theta)\mid\mid^2 < a^2+b^2+c^2+d^2+2(ab+cd)$$ This is kind of similar to $u$ and $v$ but not quite good enough, so I'm not sure now if this approach with $\theta$ and the trig functions is the right one. I also don't know how to get started for the other inequality.",,"['linear-algebra', 'matrices', 'functional-analysis', 'normed-spaces']"
35,Properties of an alternating bilinear form its coordinate matrix,Properties of an alternating bilinear form its coordinate matrix,,"I found that I lack many basic knowledge about linear algebra, so read the wiki article about Bilinear Forms. Especially this Paragraph . I tried to proof of ""Every alternating form is skew-symmetric."" was quite easy. And I found a counter-example for the inverse if $char(F) = 2$. However, I am currently trying to find a proof for this: A bilinear form is alternating if and only if its coordinate matrix is skew-symmetric and the diagonal entries are all zero I looked at the following equations, to understand that. $\begin{pmatrix}v_1 & v_2\end{pmatrix} \cdot \begin{pmatrix}a & b \\ c & d\end{pmatrix} \cdot \begin{pmatrix}v_1 \\ v_2\end{pmatrix} = 0$ $\begin{pmatrix}v_1 & v_2\end{pmatrix} \cdot \begin{pmatrix}a & b \\ c & d\end{pmatrix} \cdot \begin{pmatrix}w_1 \\ w_2\end{pmatrix} = - \begin{pmatrix}w_1 & w_2\end{pmatrix} \cdot \begin{pmatrix}a & b \\ c & d\end{pmatrix} \cdot \begin{pmatrix}v_1 \\ v_2\end{pmatrix}$ And they are both solvable for $a=0, d=0, c=-b$, thus the matrix $\begin{pmatrix}0 & b \\ -b & 0\end{pmatrix}$. However, I am not any nearer to a proof. Can someone please point me into the right direction?","I found that I lack many basic knowledge about linear algebra, so read the wiki article about Bilinear Forms. Especially this Paragraph . I tried to proof of ""Every alternating form is skew-symmetric."" was quite easy. And I found a counter-example for the inverse if $char(F) = 2$. However, I am currently trying to find a proof for this: A bilinear form is alternating if and only if its coordinate matrix is skew-symmetric and the diagonal entries are all zero I looked at the following equations, to understand that. $\begin{pmatrix}v_1 & v_2\end{pmatrix} \cdot \begin{pmatrix}a & b \\ c & d\end{pmatrix} \cdot \begin{pmatrix}v_1 \\ v_2\end{pmatrix} = 0$ $\begin{pmatrix}v_1 & v_2\end{pmatrix} \cdot \begin{pmatrix}a & b \\ c & d\end{pmatrix} \cdot \begin{pmatrix}w_1 \\ w_2\end{pmatrix} = - \begin{pmatrix}w_1 & w_2\end{pmatrix} \cdot \begin{pmatrix}a & b \\ c & d\end{pmatrix} \cdot \begin{pmatrix}v_1 \\ v_2\end{pmatrix}$ And they are both solvable for $a=0, d=0, c=-b$, thus the matrix $\begin{pmatrix}0 & b \\ -b & 0\end{pmatrix}$. However, I am not any nearer to a proof. Can someone please point me into the right direction?",,"['linear-algebra', 'matrices', 'bilinear-form']"
36,Solubility From Row Echelon Form,Solubility From Row Echelon Form,,"Here is the question I am attempting to solve Determine which values of $k$, if any, will give: a) A unique solution, b) No solution, c) Infinitely many solutions to the system of equations. $$\begin{align} x+y+kz& =2 \\ \\ 3x+4y+2z& =k \\ \\ 2x+3y-z& =1\end{align}$$ I transformed this into row-echelon form. I got. \begin{pmatrix} 1 & 1 & k &2 \\  0 & 1 & 2-3k &k-6 \\  0 & 0 & -3+k & 3-k \end{pmatrix} The answers say for a unique solution $k \neq 3$ a) So my reasoning is that if $k$ is not equal to $3$.Then we obviously have a unique solution, because if $k=3$, we would have everything in row $3$ equal to $0$. Hence we need to introduce parameters to solve the equation thus making us have infinitely many solutions. b) No solutions occurs if we have in the last row $(0,0,0|\alpha)$, where $alpha \neq 0$. We can't have any values of k that can attain this result. c) See a). Is this the right reasoning?","Here is the question I am attempting to solve Determine which values of $k$, if any, will give: a) A unique solution, b) No solution, c) Infinitely many solutions to the system of equations. $$\begin{align} x+y+kz& =2 \\ \\ 3x+4y+2z& =k \\ \\ 2x+3y-z& =1\end{align}$$ I transformed this into row-echelon form. I got. \begin{pmatrix} 1 & 1 & k &2 \\  0 & 1 & 2-3k &k-6 \\  0 & 0 & -3+k & 3-k \end{pmatrix} The answers say for a unique solution $k \neq 3$ a) So my reasoning is that if $k$ is not equal to $3$.Then we obviously have a unique solution, because if $k=3$, we would have everything in row $3$ equal to $0$. Hence we need to introduce parameters to solve the equation thus making us have infinitely many solutions. b) No solutions occurs if we have in the last row $(0,0,0|\alpha)$, where $alpha \neq 0$. We can't have any values of k that can attain this result. c) See a). Is this the right reasoning?",,"['linear-algebra', 'matrices']"
37,Square matrix $\|Ax-Ay\|\le \|x-y\|$,Square matrix,\|Ax-Ay\|\le \|x-y\|,"Could you give me an example of a square matrix $A\in \mathcal{M}_{2 \times 2}$ or $\mathcal{M}_{3 \times 3}$ for which we have $\|Ax-Ay\|\le \|x-y\|$, $ \ \ x, y \in \{0, e_1, . . . , e_n\}, \ \ e_1, . . . , e_n$ - canonical basis of $\mathbb{R}^n$, $\|\cdot\|$ is the Euclidean norm, but $\|Az\| > \|z\|$ for a certain vector $z$? I think $A =  \left( \begin{array}{ccc} 1 & \frac{1}{3}  \\ 0 & 1  \end{array} \right) $ will be good for the first case, but I have problem for the case where matrix has degree $3$.","Could you give me an example of a square matrix $A\in \mathcal{M}_{2 \times 2}$ or $\mathcal{M}_{3 \times 3}$ for which we have $\|Ax-Ay\|\le \|x-y\|$, $ \ \ x, y \in \{0, e_1, . . . , e_n\}, \ \ e_1, . . . , e_n$ - canonical basis of $\mathbb{R}^n$, $\|\cdot\|$ is the Euclidean norm, but $\|Az\| > \|z\|$ for a certain vector $z$? I think $A =  \left( \begin{array}{ccc} 1 & \frac{1}{3}  \\ 0 & 1  \end{array} \right) $ will be good for the first case, but I have problem for the case where matrix has degree $3$.",,"['matrices', 'metric-spaces', 'normed-spaces']"
38,Approximation of matrix in 2-norm,Approximation of matrix in 2-norm,,"Given a rank-$k$ matrix $A$, find a rank-$j$ matrix $B$, where $j<k$, such that $\|A-B\|_2$ is minimal. My idea was to choose, if $$A=P \operatorname{diag}(\sigma_1,\ldots,\sigma_k,0,\ldots) Q^H$$ then $$B=P \operatorname{diag}(\sigma_1,\ldots,\sigma_j,0,\ldots) Q^H$$ Is this approach correct? If so, then I would try to prove that this is actually the best approximation.","Given a rank-$k$ matrix $A$, find a rank-$j$ matrix $B$, where $j<k$, such that $\|A-B\|_2$ is minimal. My idea was to choose, if $$A=P \operatorname{diag}(\sigma_1,\ldots,\sigma_k,0,\ldots) Q^H$$ then $$B=P \operatorname{diag}(\sigma_1,\ldots,\sigma_j,0,\ldots) Q^H$$ Is this approach correct? If so, then I would try to prove that this is actually the best approximation.",,['linear-algebra']
39,Eigenvector of matrix of equal numbers,Eigenvector of matrix of equal numbers,,"For matrix the matrix $$A = \begin{bmatrix} 3&1&1\\ 1&3&1\\ 1&1&3\\ \end{bmatrix}$$ with eigenvalues $\lambda_1=5$, $\lambda_2=2$, $\lambda_3=2$, I am trying to find the corresponding eigenvector corresponding to the eigenvalue 2. I got $$(A - 2I_3) = \begin{bmatrix} 1&1&1\\ 1&1&1\\ 1&1&1\\ \end{bmatrix}$$ Reducing it (row reduced echelon form), I get: $$\left[ \begin{array} {ccc|c} 1&1&1&0\\ 0&0&0&0\\ 0&0&0&0\\ \end{array}\right]$$ Ending up with $x_1 + x_2 + x_3 = 0$. How would I find the eigenvector from there? Usually, I end up getting two equations and it's easy from there. How would you do it with one?","For matrix the matrix $$A = \begin{bmatrix} 3&1&1\\ 1&3&1\\ 1&1&3\\ \end{bmatrix}$$ with eigenvalues $\lambda_1=5$, $\lambda_2=2$, $\lambda_3=2$, I am trying to find the corresponding eigenvector corresponding to the eigenvalue 2. I got $$(A - 2I_3) = \begin{bmatrix} 1&1&1\\ 1&1&1\\ 1&1&1\\ \end{bmatrix}$$ Reducing it (row reduced echelon form), I get: $$\left[ \begin{array} {ccc|c} 1&1&1&0\\ 0&0&0&0\\ 0&0&0&0\\ \end{array}\right]$$ Ending up with $x_1 + x_2 + x_3 = 0$. How would I find the eigenvector from there? Usually, I end up getting two equations and it's easy from there. How would you do it with one?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
40,How Does One Find A Basis For The Orthogonal Complement of W given W?,How Does One Find A Basis For The Orthogonal Complement of W given W?,,"I've been doing some work in Linear Algebra for my course at school. I just want to be clear about how to find the orthogonal complement of a subspace. The basis for the subspace, W , is shown below, composed of 3 vectors:$$W = \begin{Bmatrix}\begin{bmatrix}1\\2\\3\\4 \end{bmatrix} \begin{bmatrix}-3\\4\\2\\6\end{bmatrix} \begin{bmatrix}2\\-2\\3\\5\end{bmatrix}\end{Bmatrix}$$ I would like to know if one simply sets $W*W^T$ $=0$ and takes the columns of the resulting matrix as the basis of the orthogonal complement of W , provided that row reduction has been performed to make sure the remaining columns are linearly independent. Please note that I have created an arbitrary set of vectors above that are not orthogonal, and so if they need to be for $W*W^T$ $=0$ please point that out. Thank you.","I've been doing some work in Linear Algebra for my course at school. I just want to be clear about how to find the orthogonal complement of a subspace. The basis for the subspace, W , is shown below, composed of 3 vectors:$$W = \begin{Bmatrix}\begin{bmatrix}1\\2\\3\\4 \end{bmatrix} \begin{bmatrix}-3\\4\\2\\6\end{bmatrix} \begin{bmatrix}2\\-2\\3\\5\end{bmatrix}\end{Bmatrix}$$ I would like to know if one simply sets $W*W^T$ $=0$ and takes the columns of the resulting matrix as the basis of the orthogonal complement of W , provided that row reduction has been performed to make sure the remaining columns are linearly independent. Please note that I have created an arbitrary set of vectors above that are not orthogonal, and so if they need to be for $W*W^T$ $=0$ please point that out. Thank you.",,"['linear-algebra', 'matrices', 'vector-spaces']"
41,"Revisited: How is $\phi:{\cal{L}}(V,W)\rightarrow M_{m\times n}(F)$ an isomophism of vector spaces?",Revisited: How is  an isomophism of vector spaces?,"\phi:{\cal{L}}(V,W)\rightarrow M_{m\times n}(F)","I'm told in lecture that if $V,W$ are vector spaces over $F$ and ${\cal{L}}(V,W)$ is the vector space of all linear maps $V\rightarrow W$ and ${\scr{B}}$ and ${\scr{C}}$ are bases for $V$ and $W$ respectively, then $\phi:{\cal{L}}(V,W)\rightarrow M_{m\times n}(F)$ defined by $\phi(T)=[T]^{\scr{C}}_{\scr{B}}$ is an isomorphism of vector spaces, where $T \in {\cal{L}}(V,W)$ and $[T]^{\scr{C}}_{\scr{B}}\in M_{m\times n}(F)$. Now, I'm trying to understand how this theorem completes the proof for the following theorem: Let $\{v_1,\dots,v_n\}$ be linearly independent set in a finite dimensional vector space $V$ and let $w_1,\dots,w_n$ be arbitrary vectors in a vector space $W$, then there is a linear map $T:V\rightarrow W$ such that $T(v_1)=w_1, T(v_2)=w_2, \dots$, or simply $T(v_i)=w_i$ for all $i=1,2,\dots,n$. My professor concludes at the end of the proof for the second theorem that ""[b]y theorem, such [a] $T$ exists and is unique."" Any help? Furthermore, how does the below outline conclude such a theorem? Let ${\scr{B}}=\{v_1,\dots,v_n\}$ be a basis for $V$ and ${\scr{C}}=\{u_1,\dots,u_m\}$ a basis for $W$. Then \begin{eqnarray} w_1 & = & a_{11}u_1+a_{21}u_2+\cdots+a_{m1}u_m & = & T(v_1)\\ w_1 & = & a_{12}u_1+a_{22}u_2+\cdots+a_{m2}u_m & = & T(v_2)\\ \vdots &&&&\vdots\\ w_n & = & a_{1n}u_1+a_{2n}u_2+\cdots+a_{mn}u_m & = & T(v_n), \end{eqnarray} which can be rewritten as \begin{eqnarray} \begin{pmatrix} u_1&\cdots&u_m \end{pmatrix} \begin{pmatrix} a_{11}&\cdots&a_{1n}\\ \vdots&\ddots&\vdots\\ a_{m1}&\cdots&a_{mn} \end{pmatrix}= \begin{pmatrix} T(v_1)\\ \vdots \\ T(v_n) \end{pmatrix}, \end{eqnarray} thus such a $T$ exists. Does this really suffice to prove?","I'm told in lecture that if $V,W$ are vector spaces over $F$ and ${\cal{L}}(V,W)$ is the vector space of all linear maps $V\rightarrow W$ and ${\scr{B}}$ and ${\scr{C}}$ are bases for $V$ and $W$ respectively, then $\phi:{\cal{L}}(V,W)\rightarrow M_{m\times n}(F)$ defined by $\phi(T)=[T]^{\scr{C}}_{\scr{B}}$ is an isomorphism of vector spaces, where $T \in {\cal{L}}(V,W)$ and $[T]^{\scr{C}}_{\scr{B}}\in M_{m\times n}(F)$. Now, I'm trying to understand how this theorem completes the proof for the following theorem: Let $\{v_1,\dots,v_n\}$ be linearly independent set in a finite dimensional vector space $V$ and let $w_1,\dots,w_n$ be arbitrary vectors in a vector space $W$, then there is a linear map $T:V\rightarrow W$ such that $T(v_1)=w_1, T(v_2)=w_2, \dots$, or simply $T(v_i)=w_i$ for all $i=1,2,\dots,n$. My professor concludes at the end of the proof for the second theorem that ""[b]y theorem, such [a] $T$ exists and is unique."" Any help? Furthermore, how does the below outline conclude such a theorem? Let ${\scr{B}}=\{v_1,\dots,v_n\}$ be a basis for $V$ and ${\scr{C}}=\{u_1,\dots,u_m\}$ a basis for $W$. Then \begin{eqnarray} w_1 & = & a_{11}u_1+a_{21}u_2+\cdots+a_{m1}u_m & = & T(v_1)\\ w_1 & = & a_{12}u_1+a_{22}u_2+\cdots+a_{m2}u_m & = & T(v_2)\\ \vdots &&&&\vdots\\ w_n & = & a_{1n}u_1+a_{2n}u_2+\cdots+a_{mn}u_m & = & T(v_n), \end{eqnarray} which can be rewritten as \begin{eqnarray} \begin{pmatrix} u_1&\cdots&u_m \end{pmatrix} \begin{pmatrix} a_{11}&\cdots&a_{1n}\\ \vdots&\ddots&\vdots\\ a_{m1}&\cdots&a_{mn} \end{pmatrix}= \begin{pmatrix} T(v_1)\\ \vdots \\ T(v_n) \end{pmatrix}, \end{eqnarray} thus such a $T$ exists. Does this really suffice to prove?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'vector-spaces']"
42,Inverse of matrices with 3 parts!,Inverse of matrices with 3 parts!,,"I just wonder if there is any closed form solution for the inverse of matrices with following form, or if it's possible to decompose them. $ \left[\begin{array}{cccccccccc} {\color{red}1} & {\color{red}x} & {\color{red}x} & {\color{red}x} & {\color{green}y} & {\color{green}y} & {\color{green}y} & {\color{green}y} & {\color{green}y} & {\color{green}y}\\ {\color{red}x} & {\color{red}1} & {\color{red}x} & {\color{red}x} & {\color{green}y} & {\color{green}y} & {\color{green}y} & {\color{green}y} & {\color{green}y} & {\color{green}y}\\ {\color{red}x} & {\color{red}x} & {\color{red}1} & {\color{red}x} & {\color{green}y} & {\color{green}y} & {\color{green}y} & {\color{green}y} & {\color{green}y} & {\color{green}y}\\ {\color{red}x} & {\color{red}x} & {\color{red}x} & {\color{red}1} & {\color{green}y} & {\color{green}y} & {\color{green}{\color{green}y}} & {\color{green}y} & {\color{green}y} & {\color{green}y}\\ {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}1} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z}\\ {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}1} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z}\\ {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}1} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z}\\ {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}1} & {\color{blue}z} & {\color{blue}z}\\ {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}1} & {\color{blue}z}\\ {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}1} \end{array}\right] $ The diagonal of the matrix is all 1, and matrix is consisted of three parts where the off diagonal entries of the matrix in each part are equal. Any comment or reference is appreciated.","I just wonder if there is any closed form solution for the inverse of matrices with following form, or if it's possible to decompose them. $ \left[\begin{array}{cccccccccc} {\color{red}1} & {\color{red}x} & {\color{red}x} & {\color{red}x} & {\color{green}y} & {\color{green}y} & {\color{green}y} & {\color{green}y} & {\color{green}y} & {\color{green}y}\\ {\color{red}x} & {\color{red}1} & {\color{red}x} & {\color{red}x} & {\color{green}y} & {\color{green}y} & {\color{green}y} & {\color{green}y} & {\color{green}y} & {\color{green}y}\\ {\color{red}x} & {\color{red}x} & {\color{red}1} & {\color{red}x} & {\color{green}y} & {\color{green}y} & {\color{green}y} & {\color{green}y} & {\color{green}y} & {\color{green}y}\\ {\color{red}x} & {\color{red}x} & {\color{red}x} & {\color{red}1} & {\color{green}y} & {\color{green}y} & {\color{green}{\color{green}y}} & {\color{green}y} & {\color{green}y} & {\color{green}y}\\ {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}1} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z}\\ {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}1} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z}\\ {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}1} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z}\\ {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}1} & {\color{blue}z} & {\color{blue}z}\\ {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}1} & {\color{blue}z}\\ {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}z} & {\color{blue}1} \end{array}\right] $ The diagonal of the matrix is all 1, and matrix is consisted of three parts where the off diagonal entries of the matrix in each part are equal. Any comment or reference is appreciated.",,"['linear-algebra', 'matrices', 'inverse']"
43,Eigenvalues of $A$ with $A^3 = A$,Eigenvalues of  with,A A^3 = A,"I just got a quick practice question here that I think should be simple but I can't find a definitive answer. Let $A$ be a square matrix such that $A^3 = A$. What can you say about the eigenvalues of $A$? It is multiple choice and all of the answers are combinations of $-1$, $1$, and $0$. I'm pretty sure $0$ and $1$ are possible but I'm not sure how to prove any of them. Thanks in advance.","I just got a quick practice question here that I think should be simple but I can't find a definitive answer. Let $A$ be a square matrix such that $A^3 = A$. What can you say about the eigenvalues of $A$? It is multiple choice and all of the answers are combinations of $-1$, $1$, and $0$. I'm pretty sure $0$ and $1$ are possible but I'm not sure how to prove any of them. Thanks in advance.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
44,Prove that function is bijective,Prove that function is bijective,,"Let $n \in \mathbb{N} \setminus \{ 0 \}  $ and $A \in M_n(\mathbb{R})$ with $m \in \mathbb{N} \setminus \{ 0 \}$ as $A^m= \alpha \times I_n$, with $ \alpha \in \mathbb{R} \setminus \{ -1,1 \}$. Prove that $f: M_n(\mathbb{R})\to M_n(\mathbb{R}), f(X)=X+AXA$ is bijective. I honestly don't have any idea how should I prove this, I'd be grateful for any help. I found the problem in the archives of a contest, but no hints or solving is provided.","Let $n \in \mathbb{N} \setminus \{ 0 \}  $ and $A \in M_n(\mathbb{R})$ with $m \in \mathbb{N} \setminus \{ 0 \}$ as $A^m= \alpha \times I_n$, with $ \alpha \in \mathbb{R} \setminus \{ -1,1 \}$. Prove that $f: M_n(\mathbb{R})\to M_n(\mathbb{R}), f(X)=X+AXA$ is bijective. I honestly don't have any idea how should I prove this, I'd be grateful for any help. I found the problem in the archives of a contest, but no hints or solving is provided.",,"['linear-algebra', 'matrices', 'functions', 'contest-math']"
45,What is the sufficient and necessary condition for $U$ and $V$ to be the same in the SVD?,What is the sufficient and necessary condition for  and  to be the same in the SVD?,U V,"As we know , SVD decomposites any matrix $M$ into the form: $$M=U\Sigma V^*,$$ where $U$ and $V$ are normally different. In here Wikipedia says that a matrix A is normal if and only if $U=V$. But in the very same article, it raised an example of a normal matrix $$M=\begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & 1 \\ 1 & 0 & 1 \end{pmatrix}=U\Sigma V^*,$$ where $$U= \left( \begin{array}{ccc}  -0.57735 & 0.816497 & 0. \\  -0.57735 & -0.408248 & -0.707107 \\  -0.57735 & -0.408248 & 0.707107 \\ \end{array} \right),$$ but $$V=\left( \begin{array}{ccc}  -0.57735 & 0.408248 & 0.707107 \\  -0.57735 & 0.408248 & -0.707107 \\  -0.57735 & -0.816497 & 0. \\ \end{array} \right),$$ according to mathematics software. They are not equal, why is that?","As we know , SVD decomposites any matrix $M$ into the form: $$M=U\Sigma V^*,$$ where $U$ and $V$ are normally different. In here Wikipedia says that a matrix A is normal if and only if $U=V$. But in the very same article, it raised an example of a normal matrix $$M=\begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & 1 \\ 1 & 0 & 1 \end{pmatrix}=U\Sigma V^*,$$ where $$U= \left( \begin{array}{ccc}  -0.57735 & 0.816497 & 0. \\  -0.57735 & -0.408248 & -0.707107 \\  -0.57735 & -0.408248 & 0.707107 \\ \end{array} \right),$$ but $$V=\left( \begin{array}{ccc}  -0.57735 & 0.408248 & 0.707107 \\  -0.57735 & 0.408248 & -0.707107 \\  -0.57735 & -0.816497 & 0. \\ \end{array} \right),$$ according to mathematics software. They are not equal, why is that?",,['matrices']
46,How to compute a matrix for rotating and centering rectangle in viewport?,How to compute a matrix for rotating and centering rectangle in viewport?,,"I have a rectangle given by 4 points. I'm trying to compute a transformation matrix such that the rectangle will appear straight and centered within my viewport. I'm not even sure where to begin. If we take the top left point to be $p0$, and work clockwise, $p1$ ... $p3$, then I think the angle of the rectangle can be computed with something like: $$ \theta = tan^{-1}(\frac{p1.y-p0.y}{p1.x-p0.x}) $$ Which I should then be able to use to rotate around the $z$ axis using this formula: But I'm still not sure how I get it to rotate around the center of the rectangle and then position and scale it correctly. In C# code, here's what I've got so far: var center = _drawingPoly.Aggregate((a, b) => a + b)/_drawingPoly.Count;  var r = _drawingPoly; var rect = new SizeF(     (_drawingPoly[1] - _drawingPoly[0]).Length,     (_drawingPoly[0] - _drawingPoly[3]).Length );  var cosT = (r[1].X - r[0].X) / rect.Width; var sinT = (r[0].Y - r[1].Y) / rect.Width;  var R = new Matrix4(     cosT, sinT, 0, 0,     -sinT, cosT, 0, 0,     0, 0, 1, 0,     0, 0, 0, 1 );  var rectAspect = rect.Width / rect.Height; var ctrlAspect = glControl.Width / (float)glControl.Height;  float sx, sy;  if (rectAspect < ctrlAspect) {      sy = -2f / rect.Height/1.1f;     sx = -sy / ctrlAspect; } else {     sx = 2f / rect.Width/1.1f;     sy = -sx * ctrlAspect;  }  float aspectRatio = glControl.Width / (float)glControl.Height;   var T = Matrix4.CreateTranslation(-center.X,-center.Y,0); var S = Matrix4.Scale(sx, sy, 1); var mat = T*R*S; This mostly works, but $p0$ always has to be the top left vertex otherwise the image will get rotated. Working on a solution for that. And here's a real life example. I'm trying to use the selection rectangle to orient this image:","I have a rectangle given by 4 points. I'm trying to compute a transformation matrix such that the rectangle will appear straight and centered within my viewport. I'm not even sure where to begin. If we take the top left point to be $p0$, and work clockwise, $p1$ ... $p3$, then I think the angle of the rectangle can be computed with something like: $$ \theta = tan^{-1}(\frac{p1.y-p0.y}{p1.x-p0.x}) $$ Which I should then be able to use to rotate around the $z$ axis using this formula: But I'm still not sure how I get it to rotate around the center of the rectangle and then position and scale it correctly. In C# code, here's what I've got so far: var center = _drawingPoly.Aggregate((a, b) => a + b)/_drawingPoly.Count;  var r = _drawingPoly; var rect = new SizeF(     (_drawingPoly[1] - _drawingPoly[0]).Length,     (_drawingPoly[0] - _drawingPoly[3]).Length );  var cosT = (r[1].X - r[0].X) / rect.Width; var sinT = (r[0].Y - r[1].Y) / rect.Width;  var R = new Matrix4(     cosT, sinT, 0, 0,     -sinT, cosT, 0, 0,     0, 0, 1, 0,     0, 0, 0, 1 );  var rectAspect = rect.Width / rect.Height; var ctrlAspect = glControl.Width / (float)glControl.Height;  float sx, sy;  if (rectAspect < ctrlAspect) {      sy = -2f / rect.Height/1.1f;     sx = -sy / ctrlAspect; } else {     sx = 2f / rect.Width/1.1f;     sy = -sx * ctrlAspect;  }  float aspectRatio = glControl.Width / (float)glControl.Height;   var T = Matrix4.CreateTranslation(-center.X,-center.Y,0); var S = Matrix4.Scale(sx, sy, 1); var mat = T*R*S; This mostly works, but $p0$ always has to be the top left vertex otherwise the image will get rotated. Working on a solution for that. And here's a real life example. I'm trying to use the selection rectangle to orient this image:",,"['matrices', 'transformation']"
47,Tensor Product proof,Tensor Product proof,,"I have to prove something for my matrix-algebra course. it's the following proof: I have to prove that $A\otimes B$ is invertible, if and only if $B\otimes B$ is invertible. Please explain this in simple language, I'm only a first year econometrics student.","I have to prove something for my matrix-algebra course. it's the following proof: I have to prove that $A\otimes B$ is invertible, if and only if $B\otimes B$ is invertible. Please explain this in simple language, I'm only a first year econometrics student.",,"['matrices', 'proof-writing', 'tensor-products']"
48,"If $\kappa (A) > \kappa (B)$, show $\kappa (B^{-1}A) < \kappa (A)$","If , show",\kappa (A) > \kappa (B) \kappa (B^{-1}A) < \kappa (A),"Let $A$ and $B$ be a toeplitz and symmetric positive definite $NxN$ matrices. If $\kappa (A)  > \kappa (B)$, how to show that: $$\kappa (B^{-1}A)  < \kappa (A)$$ where $\kappa $(X) is condition number of matrix $X$?","Let $A$ and $B$ be a toeplitz and symmetric positive definite $NxN$ matrices. If $\kappa (A)  > \kappa (B)$, how to show that: $$\kappa (B^{-1}A)  < \kappa (A)$$ where $\kappa $(X) is condition number of matrix $X$?",,"['matrices', 'condition-number', 'toeplitz-matrices']"
49,how prove for n≥2 $0≤\sum_{i=1}^n \sum_{j=1}^na_{ij}≤n $ that $A=[a_{ij}]\in M_n(R)$ be real matrix that A is symmetric and idempotent,how prove for n≥2  that  be real matrix that A is symmetric and idempotent,0≤\sum_{i=1}^n \sum_{j=1}^na_{ij}≤n  A=[a_{ij}]\in M_n(R),"let $A=[a_{ij}]\in M_n(R)$ be a real, symmetric and idempotent matrix (i.e.$A^2=A$). how can we prove for $n \geq 2$ that  $$0≤\sum_{i=1}^n \sum_{j=1}^na_{ij}≤n $$  thanks in advance.","let $A=[a_{ij}]\in M_n(R)$ be a real, symmetric and idempotent matrix (i.e.$A^2=A$). how can we prove for $n \geq 2$ that  $$0≤\sum_{i=1}^n \sum_{j=1}^na_{ij}≤n $$  thanks in advance.",,"['linear-algebra', 'matrices']"
50,prove that $\det T_{B} = |\det B|^{2n}$,prove that,\det T_{B} = |\det B|^{2n},"I'm doing an exercise in Kunze Hoffman book and be stucked in this exercise about calculating the determinant of a linear operator. Can anyone help me? Suppose $H$ is the vector space of all $n \times n$ Hermitan matrices over the field of real number. Prove that $T_B (A) = BAB^{*}$ is a lineat operator over the real vector space $H$ (I can prove this). Moreover, $\det T_B = |\det B|^{2n}$ Thanks every one.","I'm doing an exercise in Kunze Hoffman book and be stucked in this exercise about calculating the determinant of a linear operator. Can anyone help me? Suppose $H$ is the vector space of all $n \times n$ Hermitan matrices over the field of real number. Prove that $T_B (A) = BAB^{*}$ is a lineat operator over the real vector space $H$ (I can prove this). Moreover, $\det T_B = |\det B|^{2n}$ Thanks every one.",,"['linear-algebra', 'matrices', 'determinant']"
51,Differential equation involving matrices,Differential equation involving matrices,,"I want to show that $ Y(t)=\cos(At)$ and $Y(t)=\sin(At)$ satisfy the equation $$Y''+A^2Y=0 $$ subjected to the initial conditions $Y(0)=I, Y'(0)=0$ and $Y(0)=0, Y'(0)=I$ respictively where $A$ is an constant $n \times n$ matrix..","I want to show that $ Y(t)=\cos(At)$ and $Y(t)=\sin(At)$ satisfy the equation $$Y''+A^2Y=0 $$ subjected to the initial conditions $Y(0)=I, Y'(0)=0$ and $Y(0)=0, Y'(0)=I$ respictively where $A$ is an constant $n \times n$ matrix..",,"['linear-algebra', 'matrices', 'ordinary-differential-equations']"
52,How to be sure that the $k$th largest singular value is at least 1 of a matrix containing a k-by-k identity,How to be sure that the th largest singular value is at least 1 of a matrix containing a k-by-k identity,k,"In section 8.4 of the report of ID software , it says that the $k$th largest singular value of a $k \times n$ matrix $P$ is at least 1 if some subset of its columns makes up a $k\times k$ identity. I tried to figure it out but couldn't be sure of that. Any ideas on how to prove it?","In section 8.4 of the report of ID software , it says that the $k$th largest singular value of a $k \times n$ matrix $P$ is at least 1 if some subset of its columns makes up a $k\times k$ identity. I tried to figure it out but couldn't be sure of that. Any ideas on how to prove it?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
53,Determinant of $n\times n$ matrix,Determinant of  matrix,n\times n,"How can I calculate the determinant of the following $n\times n$ matrix, where $n$ is a multiple of $3$ ? $$\begin{pmatrix} 0 & 0 & 1 & & & & & & &\\ & & & 0 & 0 & 1 & & & &\\ & & & & & &\ddots\\ & & & & & & & 0 & 0 & 1\\ 0 & 1 & 0 & & & & & & &\\ & & & 0 & 1 & 0 & & & &\\ & & & & & &\ddots\\ & & & & & & & 0 & 1 & 0\\ 1 & 0 & 0 & & & & & & &\\ & & & 1 & 0 & 0 & & & &\\ & & & & & &\ddots\\ & & & & & & & 1 & 0 & 0 \end{pmatrix}$$","How can I calculate the determinant of the following $n\times n$ matrix, where $n$ is a multiple of $3$ ? $$\begin{pmatrix} 0 & 0 & 1 & & & & & & &\\ & & & 0 & 0 & 1 & & & &\\ & & & & & &\ddots\\ & & & & & & & 0 & 0 & 1\\ 0 & 1 & 0 & & & & & & &\\ & & & 0 & 1 & 0 & & & &\\ & & & & & &\ddots\\ & & & & & & & 0 & 1 & 0\\ 1 & 0 & 0 & & & & & & &\\ & & & 1 & 0 & 0 & & & &\\ & & & & & &\ddots\\ & & & & & & & 1 & 0 & 0 \end{pmatrix}$$",,"['linear-algebra', 'matrices', 'determinant']"
54,Product of positive definite matrices,Product of positive definite matrices,,"Let $X,Y,Z$ be positive definite matrices such that $XYZ$ is hermitian . How can we show that $XYZ$ is also a positive definite matrix?","Let $X,Y,Z$ be positive definite matrices such that $XYZ$ is hermitian . How can we show that $XYZ$ is also a positive definite matrix?",,"['linear-algebra', 'matrices', 'positive-definite']"
55,Geometric multiplicities of the same eigenvalue of $A$ and of $A^T$,Geometric multiplicities of the same eigenvalue of  and of,A A^T,"For a square complex/real matrix $A$, $A$ and $A^T$ have the same set of eigenvalues, each with same algebraic multiplicities, since their characteristic polynomials are the same. I wonder for each eigenvalue, are its geometric multiplicities for $A$ and for $A^T$ the same? Similar question for $A$ and $A^H$, where $H$ means conjugate and transpose, and the relation between their eigenvalues is conjugate. Thanks!","For a square complex/real matrix $A$, $A$ and $A^T$ have the same set of eigenvalues, each with same algebraic multiplicities, since their characteristic polynomials are the same. I wonder for each eigenvalue, are its geometric multiplicities for $A$ and for $A^T$ the same? Similar question for $A$ and $A^H$, where $H$ means conjugate and transpose, and the relation between their eigenvalues is conjugate. Thanks!",,['matrices']
56,Is $B^{-1}-A^{-1}$ a positive definite matrix?,Is  a positive definite matrix?,B^{-1}-A^{-1},"If $A$ and $B$ are positive definite matrices, and $A-B$ is also positive definite, is $B^{-1}-A^{-1}$ necessarily positive definite?","If $A$ and $B$ are positive definite matrices, and $A-B$ is also positive definite, is $B^{-1}-A^{-1}$ necessarily positive definite?",,"['linear-algebra', 'matrices']"
57,Convert a n by n matrix to upper triangular,Convert a n by n matrix to upper triangular,,How can we mathematically and algorithmic-ally  convert a $n\times n$ matrix to a upper triangular matrix,How can we mathematically and algorithmic-ally  convert a $n\times n$ matrix to a upper triangular matrix,,"['matrices', 'algorithms']"
58,Rewriting a Trace of Matrix products,Rewriting a Trace of Matrix products,,"Consider 3 $N \times N$ complex matrices $A$,$B$ and $X$. $A$ and $B$ are hermitian matrices. Let$X=[x_1,x_2..x_N]$ where $x_i$'s the $N\times 1$ column vectors of $X$. I am interested in the term $trace(AXBX^{H})$. Is there anyway, I can write it in terms of columns of $X$. To point out a example for another case, $trace(AX)=\sum_{i=1}^{N}a_i^{H}x_i$ where $a_i$ are the columns of $A$ (hermitian matrix). Similarly $trace(BX^{H})=trace(X^{H}B)=\sum_{i}^{N}x_i^Hb_i$ where $b_i$ are columns of $B$. Can anyone come up with a similar presentation for $trace{(AXBX^{H})}$.","Consider 3 $N \times N$ complex matrices $A$,$B$ and $X$. $A$ and $B$ are hermitian matrices. Let$X=[x_1,x_2..x_N]$ where $x_i$'s the $N\times 1$ column vectors of $X$. I am interested in the term $trace(AXBX^{H})$. Is there anyway, I can write it in terms of columns of $X$. To point out a example for another case, $trace(AX)=\sum_{i=1}^{N}a_i^{H}x_i$ where $a_i$ are the columns of $A$ (hermitian matrix). Similarly $trace(BX^{H})=trace(X^{H}B)=\sum_{i}^{N}x_i^Hb_i$ where $b_i$ are columns of $B$. Can anyone come up with a similar presentation for $trace{(AXBX^{H})}$.",,"['linear-algebra', 'matrices']"
59,"If the spectral radius of a matrix is less than 1, then the matrix has a norm which is less than 1.","If the spectral radius of a matrix is less than 1, then the matrix has a norm which is less than 1.",,"Let $A$ be an arbitrary square matrix and define $ \rho(A)$ to be the maximal eigenvalue of $A$ in absolute value. If $ \rho(A)<1,$  then there exists a norm of $A$ such that $ \| A \|<1.$ How to do this? Thanks.","Let $A$ be an arbitrary square matrix and define $ \rho(A)$ to be the maximal eigenvalue of $A$ in absolute value. If $ \rho(A)<1,$  then there exists a norm of $A$ such that $ \| A \|<1.$ How to do this? Thanks.",,"['linear-algebra', 'matrices']"
60,Integral involving Matrix Exponential to solve LTI system equation,Integral involving Matrix Exponential to solve LTI system equation,,"I am given that for $A$ that is $n \times n$ matrix of full rank, $$\int_{0}^{t}e^{A\sigma}d\sigma = (e^{At}-I)A^{-1}$$ Then I am using this to solve LTI system $$\dot{x}=Ax+Bu$$ Here, $x(0) = x_{0}$ and u is a constant vector. I went ahead and used the general solution for LTI system, $$x(t)=e^{A(t-t_{0})}x_{0}+\int_{t_0}^{t}e^{A(t-t_{0})}B(\tau)u(\tau) \, d\tau$$ I have $B$ and $U$ constant and time from 0 to t so this reduces to $$x(t)=e^{At}x_{0}+\int_{0}^{t}e^{A(t-t_{0})}Bu \, d\tau$$ I am kinda stuck here, what should I do with those constant matrix $B u$ to solve this system using $\int_{0}^{t}e^{A\sigma}d\sigma = (e^{At}-I)A^{-1}$ ? I know I am not allowed to just pull out $Bu$ outside of the integral because I am dealing with matrices. Any ideas?","I am given that for $A$ that is $n \times n$ matrix of full rank, $$\int_{0}^{t}e^{A\sigma}d\sigma = (e^{At}-I)A^{-1}$$ Then I am using this to solve LTI system $$\dot{x}=Ax+Bu$$ Here, $x(0) = x_{0}$ and u is a constant vector. I went ahead and used the general solution for LTI system, $$x(t)=e^{A(t-t_{0})}x_{0}+\int_{t_0}^{t}e^{A(t-t_{0})}B(\tau)u(\tau) \, d\tau$$ I have $B$ and $U$ constant and time from 0 to t so this reduces to $$x(t)=e^{At}x_{0}+\int_{0}^{t}e^{A(t-t_{0})}Bu \, d\tau$$ I am kinda stuck here, what should I do with those constant matrix $B u$ to solve this system using $\int_{0}^{t}e^{A\sigma}d\sigma = (e^{At}-I)A^{-1}$ ? I know I am not allowed to just pull out $Bu$ outside of the integral because I am dealing with matrices. Any ideas?",,"['linear-algebra', 'matrices']"
61,What is a necessary and sufficient condition of that $A$ has $n$ linearly independent eigenvectors?,What is a necessary and sufficient condition of that  has  linearly independent eigenvectors?,A n,"If $A$ is an $n$-by-$n$ matrix with complex entries, (i.e., $A\in M_n(\mathbb{C})$,) $A$ must have $n$ eigenvalues, counting algebraic multiples. But it is not always true that $A$ has $n$ linearly independent eigenvectors. So, what necessary and sufficient condition may be add, to ensure that $A$ has $n$ linearly independent eigenvectors? Of course, the simpler the better. I have another related question: I can't figure out why, intuitively, that algebraic multiple doesn't mean more than one linearly independent eigenvectors. I mean in my tuition, a multiple appear because there is a subspace with dimension>1  being scaled ""evenly"" in every direction. If the multiple is 2, how come I may fail to find 2 linearly independent vectors in this subspace?","If $A$ is an $n$-by-$n$ matrix with complex entries, (i.e., $A\in M_n(\mathbb{C})$,) $A$ must have $n$ eigenvalues, counting algebraic multiples. But it is not always true that $A$ has $n$ linearly independent eigenvectors. So, what necessary and sufficient condition may be add, to ensure that $A$ has $n$ linearly independent eigenvectors? Of course, the simpler the better. I have another related question: I can't figure out why, intuitively, that algebraic multiple doesn't mean more than one linearly independent eigenvectors. I mean in my tuition, a multiple appear because there is a subspace with dimension>1  being scaled ""evenly"" in every direction. If the multiple is 2, how come I may fail to find 2 linearly independent vectors in this subspace?",,['matrices']
62,sufficient conditions for linear combinations of indefinite matrices to be indefinite,sufficient conditions for linear combinations of indefinite matrices to be indefinite,,"In general, sum of indefinite matrices need not be indefinite. For example, $A=\begin{pmatrix}2&0\\0&-2\end{pmatrix}$, and $B=\begin{pmatrix}-1&0\\0&1\end{pmatrix}$ are indefinite matrices but matrix $A+B$ is positive definite. Do we have any sufficient conditions for the sum of general $n$th order indefinite matrices to be indefinite as well? Do we have conditions for any linear combination of those matrices to be indefinite as well? For example, the matrices $\begin{pmatrix}-1&0&0\\0&1&0\\0&0&1\end{pmatrix}$ and $\begin{pmatrix}1&0&0\\0&1&0\\0&0&-5\end{pmatrix}$ give indefinite matrix for any of its linear combinations.","In general, sum of indefinite matrices need not be indefinite. For example, $A=\begin{pmatrix}2&0\\0&-2\end{pmatrix}$, and $B=\begin{pmatrix}-1&0\\0&1\end{pmatrix}$ are indefinite matrices but matrix $A+B$ is positive definite. Do we have any sufficient conditions for the sum of general $n$th order indefinite matrices to be indefinite as well? Do we have conditions for any linear combination of those matrices to be indefinite as well? For example, the matrices $\begin{pmatrix}-1&0&0\\0&1&0\\0&0&1\end{pmatrix}$ and $\begin{pmatrix}1&0&0\\0&1&0\\0&0&-5\end{pmatrix}$ give indefinite matrix for any of its linear combinations.",,"['linear-algebra', 'matrices']"
63,A number of SVD components; understanding the relation,A number of SVD components; understanding the relation,,"Related to my work is the concept of Singular Value Decomposition (SVD). Namely, given some matrix $B\in\mathbb{R}^{n\times m}$, $n\geq m$, SVD can be written as $$B=U\Sigma V^T,$$ where $U\in\mathbb{R}^{n\times n}$ ($n$ left-singular vectors), $\Sigma \in\mathbb{R}^{n\times m}$ (matrix with $m$ singular values on its diagonal), and $V\in\mathbb{R}^{m\times m}$ ($m$ right-singular vectors). So, how I see it, given that there are $m$ singular values, only right-singular vectors and $m$ first left-singular vectors have the asociated singular value? To which singular value do the rest of left-singular vectors correspond to? In case one complements $\Sigma$ to contain zero singular values such that $\Sigma\in\mathbb{R}^{n\times n}$, then $V^T$ needs also to be complemented to be a $V^T\in\mathbb{R}^{n\times m}$ matrix. But, that would imply that a vector of all zeros is also a right-singular vector. Could someone clarify this? Furthermore, suppose that the rank of matrix $B$ is $c$, $c<m\leq n$. Does this imply that $B$ has only $d$ left- and right-singular vectors?","Related to my work is the concept of Singular Value Decomposition (SVD). Namely, given some matrix $B\in\mathbb{R}^{n\times m}$, $n\geq m$, SVD can be written as $$B=U\Sigma V^T,$$ where $U\in\mathbb{R}^{n\times n}$ ($n$ left-singular vectors), $\Sigma \in\mathbb{R}^{n\times m}$ (matrix with $m$ singular values on its diagonal), and $V\in\mathbb{R}^{m\times m}$ ($m$ right-singular vectors). So, how I see it, given that there are $m$ singular values, only right-singular vectors and $m$ first left-singular vectors have the asociated singular value? To which singular value do the rest of left-singular vectors correspond to? In case one complements $\Sigma$ to contain zero singular values such that $\Sigma\in\mathbb{R}^{n\times n}$, then $V^T$ needs also to be complemented to be a $V^T\in\mathbb{R}^{n\times m}$ matrix. But, that would imply that a vector of all zeros is also a right-singular vector. Could someone clarify this? Furthermore, suppose that the rank of matrix $B$ is $c$, $c<m\leq n$. Does this imply that $B$ has only $d$ left- and right-singular vectors?",,"['linear-algebra', 'matrices']"
64,"Minimizing the matrix norm, equivalence","Minimizing the matrix norm, equivalence",,"Given matrices $C\in \mathbb{R}^{n\times k}$ and $X\in \mathbb{R}^{n\times k}$, it is known that $X$ minimizes the following $$\|CC^T-XX^T\|_F^2$$ Can it be proved that such solution X minimizes $$\|C-X\|_F^2$$ Note that $\|\cdot\|_F$ corresponds to the Frobenius norm. Edit The question is as I posted it primarily. Note that the dimensions of $C$ and $X$ need to be the same; otherwise $\|C-X\|^2$ would not be possible. With your $X=CU$, what is obtained is $\|CC^T-CC^T\|$. Note that one could trivially set $X=C$ to annihilate Frobenius norm, but that's not the point. Matrix $X$ is supplied externally, and it is known that it minimizes $\|CC^T-XX^T\|^2$; question is: does it also minimize $\|C-X\|^2$? In the external algorithm, matrix $X$ is obtained as $X=Lsv(C)SV(C)^{1/2}$, where $Lsv(C)$ denotes left-singular vector of $C$, and $SV(C)^{1/2}$ a diagonal matrix with roots of singular values of C. Note that $||C−X||^2$ does not need to be 0 to be minimal.","Given matrices $C\in \mathbb{R}^{n\times k}$ and $X\in \mathbb{R}^{n\times k}$, it is known that $X$ minimizes the following $$\|CC^T-XX^T\|_F^2$$ Can it be proved that such solution X minimizes $$\|C-X\|_F^2$$ Note that $\|\cdot\|_F$ corresponds to the Frobenius norm. Edit The question is as I posted it primarily. Note that the dimensions of $C$ and $X$ need to be the same; otherwise $\|C-X\|^2$ would not be possible. With your $X=CU$, what is obtained is $\|CC^T-CC^T\|$. Note that one could trivially set $X=C$ to annihilate Frobenius norm, but that's not the point. Matrix $X$ is supplied externally, and it is known that it minimizes $\|CC^T-XX^T\|^2$; question is: does it also minimize $\|C-X\|^2$? In the external algorithm, matrix $X$ is obtained as $X=Lsv(C)SV(C)^{1/2}$, where $Lsv(C)$ denotes left-singular vector of $C$, and $SV(C)^{1/2}$ a diagonal matrix with roots of singular values of C. Note that $||C−X||^2$ does not need to be 0 to be minimal.",,"['matrices', 'linear-algebra']"
65,"(Graphics Gems IV, Shoemake) From matrix to euler angles explanation","(Graphics Gems IV, Shoemake) From matrix to euler angles explanation",,"I am trying to understand matrix to Euler angles conversion. So I read Graphics Gems IV, page 222 from Ken Shoemake. It states: ""Suppose we have code to convert a rotation matrix to XEDS angles, $R = R_z(c)R_y(b)R_x(a)$. If we are asked to extract XEDR, $R = R_z(a)R_y(b)R_x(c)$, we use our code as is, and swap $a$ and $c$ afterwards."" Restated (am I wrong??), it means that a $XYZ$ rotation of $(a, b, c)$ around static axes, is equivalent to a $XYZ$ rotation of $(c, b, a)$ around moving axes. This is exactly what they are doing in the source code. In wikipedia , they state instead: ""For either Euler or Tait-Bryan angles, it is very simple to convert from an intrinsic (rotating axes) to an extrinsic (static axes) convention, and vice-versa: just swap the order of the operations. An $(a, b, c)$ rotation using $X-Y-Z$ intrinsic convention is equivalent to a $(c, b, a)$ rotation using $Z-Y-X$ extrinsic convention; this is true for all Euler or Tait-Bryan axis combinations."" Restated (am I wrong 2 ? ), it means that a $XYZ$ rotation of $(a, b, c)$ around static axes, is equivalent to a $ZYX$ rotation of $(c, b, a)$ around moving axes which I can understand if I derive the sequence of rotation in static frames starting from the sequence of rotation in moving frames. If I take a simple example,with no rotation around $Y $nor $Z, R = Rx(a)$ in static frames, Shoemake states that $R = R_z(a)$ in moving frames while wikipedia states that $R = Rx(a)$ in moving frames. Moreover, taking it source code, and converting the same Euler angles with one rotation only and different rotation orders results in different quaternions. Intuitively, I believe that if we are rotating around one axis only, then order does not matter and it should always return the same quaternion. PROBLEM: Everybody is using Shoemake algo for a long time so it HAS to be right and I HAVE to be wrong. Someone could please help to see what I am doing wrong please? Best regards.","I am trying to understand matrix to Euler angles conversion. So I read Graphics Gems IV, page 222 from Ken Shoemake. It states: ""Suppose we have code to convert a rotation matrix to XEDS angles, $R = R_z(c)R_y(b)R_x(a)$. If we are asked to extract XEDR, $R = R_z(a)R_y(b)R_x(c)$, we use our code as is, and swap $a$ and $c$ afterwards."" Restated (am I wrong??), it means that a $XYZ$ rotation of $(a, b, c)$ around static axes, is equivalent to a $XYZ$ rotation of $(c, b, a)$ around moving axes. This is exactly what they are doing in the source code. In wikipedia , they state instead: ""For either Euler or Tait-Bryan angles, it is very simple to convert from an intrinsic (rotating axes) to an extrinsic (static axes) convention, and vice-versa: just swap the order of the operations. An $(a, b, c)$ rotation using $X-Y-Z$ intrinsic convention is equivalent to a $(c, b, a)$ rotation using $Z-Y-X$ extrinsic convention; this is true for all Euler or Tait-Bryan axis combinations."" Restated (am I wrong 2 ? ), it means that a $XYZ$ rotation of $(a, b, c)$ around static axes, is equivalent to a $ZYX$ rotation of $(c, b, a)$ around moving axes which I can understand if I derive the sequence of rotation in static frames starting from the sequence of rotation in moving frames. If I take a simple example,with no rotation around $Y $nor $Z, R = Rx(a)$ in static frames, Shoemake states that $R = R_z(a)$ in moving frames while wikipedia states that $R = Rx(a)$ in moving frames. Moreover, taking it source code, and converting the same Euler angles with one rotation only and different rotation orders results in different quaternions. Intuitively, I believe that if we are rotating around one axis only, then order does not matter and it should always return the same quaternion. PROBLEM: Everybody is using Shoemake algo for a long time so it HAS to be right and I HAVE to be wrong. Someone could please help to see what I am doing wrong please? Best regards.",,"['geometry', 'matrices', '3d', 'rotations']"
66,How to compute the change of basis matrix that conjugate a matrix to its rational canonical form,How to compute the change of basis matrix that conjugate a matrix to its rational canonical form,,"Let $A=\begin{pmatrix} 2&-2&14\\0&3&-7\\0&0&2\end{pmatrix}$, then its rational canonical form is $R=\begin{pmatrix}2&0&0\\0&0&-6\\0&1&5\end{pmatrix}$. How can I compute a matrix $P$ such that $P^{-1}AP=R$? And in general what is the algorithm?","Let $A=\begin{pmatrix} 2&-2&14\\0&3&-7\\0&0&2\end{pmatrix}$, then its rational canonical form is $R=\begin{pmatrix}2&0&0\\0&0&-6\\0&1&5\end{pmatrix}$. How can I compute a matrix $P$ such that $P^{-1}AP=R$? And in general what is the algorithm?",,"['linear-algebra', 'matrices']"
67,Modified Cholesky factorization and retrieving the usual LT matrix,Modified Cholesky factorization and retrieving the usual LT matrix,,"I have been looking at the modified Cholesky decomposition suggested by the following paper: Schnabel and Eskow, A Revised Modified Cholesky Factorization Algorithm , SIAM J. Optim. 9, pp. 1135-1148 (14 pages). The paper talks about an implementation of the Cholesky decomposition, modified by the fact that when the matrix is not positive definite, a diagonal perturbation is added before the decomposition takes place. The algorithm given in the paper (Algorithm 1) suggests that it finds factorization such that $LL^\top = A + E, E \ge 0$. But, when implemented as stated in the paper, what I get is a lower triangular factor matrix $L$, such that: $$P\cdot(L\cdot L^\top)\cdot P^\top = A + E$$ where, $P$ is a permutation matrix. This $L$ matrix is not the same as when using the normal Cholesky factorization for a PD matrix, where $A = L_1L_1^\top$, say. Now, I am using it in optimization context, specifically in trust region methods, where this decomposition is followed by inverting $L$ (to compute the trust region step), so it would be helpful to have factors in lower triangular form. Is there a way to get back $L_1$ (the original Cholesky factor) for a positive definite matrix from the $P$ and $L$ matrix obtained from the modified Cholesky factorization? I am a bit surprised that the algorithm misstates what it would produce, so maybe I am missing some step here. Related threads I have found so far: Cholesky factorization Cholesky decomposition and permutation matrix The second thread says that the relationship is not possible to find for any given set of matrix (not necessarily for a modified Cholesky decomposition as mentioned here). Not sure if the answer still holds in this case as well. Example: Consider the following $4\times4$ matrix which is PD. $$A = \begin{bmatrix}6 & 3 & 4 & 8 \\ 3 & 6 & 5 & 1 \\ 4 & 5 & 10 & 7 \\ 8 & 1 & 7 & 25 \end{bmatrix}$$ The vanilla Cholesky factor $L_1$, such that $L_1L_1^\top=A$ is: $$L_1 = \begin{bmatrix}2.4495 & 0 & 0 & 0 \\ 1.2247 & 2.1213 & 0 & 0 \\ 1.6330 & 1.4142 & 2.3094 & 0 \\ 3.2660 & -1.4142 & 1.5877 & 3.1325 \end{bmatrix}$$ Now, if I perform the modified Cholesky, I get: $$L=\begin{bmatrix}5 & 0 & 0 & 0 \\ 1.4 & 2.83549 & 0 & 0 \\ 0.2 & 1.66462 & 1.78579 & 0 \\ 1.6 & 0.62070 & 0.92215 & 1.48471 \end{bmatrix}$$ and $$P=\begin{bmatrix}0 & 0 & 0 & 1 \\ 0 & 0 & 1 & 0 \\ 0 & 1 & 0 & 0 \\ 1 & 0 & 0 & 0 \end{bmatrix}$$ Such that, $P\cdot (L\cdot L^\top)\cdot P^\top = A$. Of course, since $A$ is PD, $E=0$.","I have been looking at the modified Cholesky decomposition suggested by the following paper: Schnabel and Eskow, A Revised Modified Cholesky Factorization Algorithm , SIAM J. Optim. 9, pp. 1135-1148 (14 pages). The paper talks about an implementation of the Cholesky decomposition, modified by the fact that when the matrix is not positive definite, a diagonal perturbation is added before the decomposition takes place. The algorithm given in the paper (Algorithm 1) suggests that it finds factorization such that $LL^\top = A + E, E \ge 0$. But, when implemented as stated in the paper, what I get is a lower triangular factor matrix $L$, such that: $$P\cdot(L\cdot L^\top)\cdot P^\top = A + E$$ where, $P$ is a permutation matrix. This $L$ matrix is not the same as when using the normal Cholesky factorization for a PD matrix, where $A = L_1L_1^\top$, say. Now, I am using it in optimization context, specifically in trust region methods, where this decomposition is followed by inverting $L$ (to compute the trust region step), so it would be helpful to have factors in lower triangular form. Is there a way to get back $L_1$ (the original Cholesky factor) for a positive definite matrix from the $P$ and $L$ matrix obtained from the modified Cholesky factorization? I am a bit surprised that the algorithm misstates what it would produce, so maybe I am missing some step here. Related threads I have found so far: Cholesky factorization Cholesky decomposition and permutation matrix The second thread says that the relationship is not possible to find for any given set of matrix (not necessarily for a modified Cholesky decomposition as mentioned here). Not sure if the answer still holds in this case as well. Example: Consider the following $4\times4$ matrix which is PD. $$A = \begin{bmatrix}6 & 3 & 4 & 8 \\ 3 & 6 & 5 & 1 \\ 4 & 5 & 10 & 7 \\ 8 & 1 & 7 & 25 \end{bmatrix}$$ The vanilla Cholesky factor $L_1$, such that $L_1L_1^\top=A$ is: $$L_1 = \begin{bmatrix}2.4495 & 0 & 0 & 0 \\ 1.2247 & 2.1213 & 0 & 0 \\ 1.6330 & 1.4142 & 2.3094 & 0 \\ 3.2660 & -1.4142 & 1.5877 & 3.1325 \end{bmatrix}$$ Now, if I perform the modified Cholesky, I get: $$L=\begin{bmatrix}5 & 0 & 0 & 0 \\ 1.4 & 2.83549 & 0 & 0 \\ 0.2 & 1.66462 & 1.78579 & 0 \\ 1.6 & 0.62070 & 0.92215 & 1.48471 \end{bmatrix}$$ and $$P=\begin{bmatrix}0 & 0 & 0 & 1 \\ 0 & 0 & 1 & 0 \\ 0 & 1 & 0 & 0 \\ 1 & 0 & 0 & 0 \end{bmatrix}$$ Such that, $P\cdot (L\cdot L^\top)\cdot P^\top = A$. Of course, since $A$ is PD, $E=0$.",,"['matrices', 'numerical-linear-algebra', 'nonlinear-optimization', 'matrix-decomposition', 'cholesky-decomposition']"
68,Alternative way to do this eigenvector problem,Alternative way to do this eigenvector problem,,"Just wondering if there is a faster or better way of doing this question. I have 3 matrices: $A = {1\over2}\hbar\left( \begin{smallmatrix} 0&-i\\ i&0 \end{smallmatrix} \right), B = {1\over2}\hbar\left( \begin{smallmatrix} 0&1\\ 1&0 \end{smallmatrix} \right), C = {1\over2}\hbar\left( \begin{smallmatrix} 1&0\\ 0&-1 \end{smallmatrix} \right), $ Let $D = A^2+B^2+C^2$ and that $v$ is an eigenvector of $C$, (with eigenvalue $-{1\over2}\hbar$). Question: Show that $v$ is an eigenvector of $D$ with eigenvalue ${1\over2}({1\over2}+1)\hbar^2$. My take: $D={3\over4}\hbar^2\left( \begin{smallmatrix} 1&0\\ 0&1 \end{smallmatrix} \right)$. $Cv=-{1\over2}\hbar v \implies C^2v={1\over4}\hbar^2 v$. Since $C^2={1\over4}\hbar^2\left( \begin{smallmatrix} 1&0\\ 0&1 \end{smallmatrix} \right)=B^2=A^2$, Therefore $Dv = 3\times {1\over4}\hbar^2v$. So we are done. Though I do get the numerical answer, it is not immediately in the form asked for. Could anyone suggest another way of doing this? Thank you.","Just wondering if there is a faster or better way of doing this question. I have 3 matrices: $A = {1\over2}\hbar\left( \begin{smallmatrix} 0&-i\\ i&0 \end{smallmatrix} \right), B = {1\over2}\hbar\left( \begin{smallmatrix} 0&1\\ 1&0 \end{smallmatrix} \right), C = {1\over2}\hbar\left( \begin{smallmatrix} 1&0\\ 0&-1 \end{smallmatrix} \right), $ Let $D = A^2+B^2+C^2$ and that $v$ is an eigenvector of $C$, (with eigenvalue $-{1\over2}\hbar$). Question: Show that $v$ is an eigenvector of $D$ with eigenvalue ${1\over2}({1\over2}+1)\hbar^2$. My take: $D={3\over4}\hbar^2\left( \begin{smallmatrix} 1&0\\ 0&1 \end{smallmatrix} \right)$. $Cv=-{1\over2}\hbar v \implies C^2v={1\over4}\hbar^2 v$. Since $C^2={1\over4}\hbar^2\left( \begin{smallmatrix} 1&0\\ 0&1 \end{smallmatrix} \right)=B^2=A^2$, Therefore $Dv = 3\times {1\over4}\hbar^2v$. So we are done. Though I do get the numerical answer, it is not immediately in the form asked for. Could anyone suggest another way of doing this? Thank you.",,"['matrices', 'eigenvalues-eigenvectors']"
69,"Codes: Distance, number of codewords, etc","Codes: Distance, number of codewords, etc",,"Let $p\geq 3$ be any prime and consider the code $C = N(H)\subseteq\mathbb{Z}_p^2$, where $H =  \begin{pmatrix} 1 & 1 & 1 & \dots & 1 & 1 \\ 0 & 1 & 2 & \dots & p-2 & p-1 \end{pmatrix}\in \mathbb{Z}_p^{2\times p}$. a) How many codewords does the code $C$ contain? I've got an idea on this part. It should just be $p^2$, shouldn't it? b) Show that every selection of two distinct columns of $H$ results in a non-singular 2x2 matrix. c) Find a codeword in $C$ of weight 3 and use (b) to conclude that the code $C$ has distance 3. Parts b and c I have no clue on where to start. I'm sure that I could probably reason my way through c if I could figure out b, since finding the codeword of weight 3 should be trivial. However, I'm willing to take any assistance on this problem.","Let $p\geq 3$ be any prime and consider the code $C = N(H)\subseteq\mathbb{Z}_p^2$, where $H =  \begin{pmatrix} 1 & 1 & 1 & \dots & 1 & 1 \\ 0 & 1 & 2 & \dots & p-2 & p-1 \end{pmatrix}\in \mathbb{Z}_p^{2\times p}$. a) How many codewords does the code $C$ contain? I've got an idea on this part. It should just be $p^2$, shouldn't it? b) Show that every selection of two distinct columns of $H$ results in a non-singular 2x2 matrix. c) Find a codeword in $C$ of weight 3 and use (b) to conclude that the code $C$ has distance 3. Parts b and c I have no clue on where to start. I'm sure that I could probably reason my way through c if I could figure out b, since finding the codeword of weight 3 should be trivial. However, I'm willing to take any assistance on this problem.",,"['linear-algebra', 'matrices', 'coding-theory']"
70,What does matrix multiplication have to do with scalar multiplication?,What does matrix multiplication have to do with scalar multiplication?,,Why are matrix and scalar multiplication denoted the same way and treated as the same operation in standard mathematical notation?  This is always a source of confusion for me because they have completely different properties (specifically commutativity).  Multiplying a 1x1 matrix by an NxN matrix isn't even generally equivalent to multiplying an NxN matrix by a scalar.  (The former is not even always defined.)  Wouldn't it be clearer to consider these to be completely unrelated operations and use completely different notation to represent them?,Why are matrix and scalar multiplication denoted the same way and treated as the same operation in standard mathematical notation?  This is always a source of confusion for me because they have completely different properties (specifically commutativity).  Multiplying a 1x1 matrix by an NxN matrix isn't even generally equivalent to multiplying an NxN matrix by a scalar.  (The former is not even always defined.)  Wouldn't it be clearer to consider these to be completely unrelated operations and use completely different notation to represent them?,,"['linear-algebra', 'matrices', 'notation', 'math-history', 'faq']"
71,Prove that any 2x2 nilpotent matrix N is similar to zero matrix or 0 & 1 \\ 0 & 0,Prove that any 2x2 nilpotent matrix N is similar to zero matrix or 0 & 1 \\ 0 & 0,,"The question is to prove that any nilpotent matrix $N \in Mat_{2\times2}(\mathbb{C})$ is similar to \begin{equation} \begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix} \end{equation} or \begin{equation} \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix} \end{equation} I've known that $N^2=0$ , and the eigenvalues of $N$ are $0$ . But I'm still confused about how I can reach the conclusion of similarity. Thanks for any help!","The question is to prove that any nilpotent matrix is similar to or I've known that , and the eigenvalues of are . But I'm still confused about how I can reach the conclusion of similarity. Thanks for any help!","N \in Mat_{2\times2}(\mathbb{C}) \begin{equation}
\begin{pmatrix}
0 & 0 \\
0 & 0
\end{pmatrix}
\end{equation} \begin{equation}
\begin{pmatrix}
0 & 1 \\
0 & 0
\end{pmatrix}
\end{equation} N^2=0 N 0","['linear-algebra', 'matrices']"
72,How would understanding 'change of basis' explain where I went wrong?,How would understanding 'change of basis' explain where I went wrong?,,"(Sorry in advance if I don't get all the terminology right, and for the lengthy question!) Intro I've got two $2\times 2$ matrices called $\textbf{M}$ and $\textbf{N}$ , which represent the linear transformations M and N respectively. $$\textbf{M} =\begin{bmatrix}a & b\\c & d\end{bmatrix}; \quad \textbf{N} =\begin{bmatrix}e & f\\g & h\end{bmatrix}$$ I've got two methods (A and B) to try and find $\textbf{P}=\textbf{MN}$ but the results they give me are inconsistent! Can anyone explain why?? (A) The Long-Winded Way I know that the columns of a $2\times 2$ matrix are the images of $\hat{\textbf{i}}$ and $\hat{\textbf{j}}$ respectively when I apply the linear transformation represented by that matrix to a coordinate axes, S, with unit vectors $\hat{\textbf{i}}$ and $\hat{\textbf{j}}$ . And these images are described as column vectors 'as seen by' S - i.e. the column vectors are in 'the language of' $\hat{\textbf{i}}$ and $\hat{\textbf{j}}$ . (I'll call this my 'theorem'.) Let S' be the coordinate axes with unit vectors $\hat{\textbf{i}'}$ and $\hat{\textbf{j}'}$ and S'' be the coordinate axes with unit vectors $\hat{\textbf{i}''}$ and $\hat{\textbf{j}''}$ . I'll also say that S' is the image of S under the transformation N and S'' is the image of S' under the transformation M. (I'm applying N before M!) So using my theorem I know that: $$\hat{\textbf{i}'} = e\hat{\textbf{i}} + g\hat{\textbf{j}}$$ $$\hat{\textbf{j}'} = f\hat{\textbf{i}} + h\hat{\textbf{j}}$$ because the unit vectors of S' are the images of those of S, under the transformation that $\textbf{N}$ represents. Similarly, I can reason that: $$\hat{\textbf{i}''} = a\hat{\textbf{i}'} + c\hat{\textbf{j}'}$$ $$\hat{\textbf{j}''} = b\hat{\textbf{i}'} + d\hat{\textbf{j}'}$$ because the unit vectors of S'' are the images of those of S' under the transformation M, and these images are described as column vectors (in terms of the unit vectors of S') in the columns of $\textbf{M}$ . So, by substituting the top 2 equations into the bottom 2: $$\hat{\textbf{i}''} = a(e\hat{\textbf{i}} + g\hat{\textbf{j}}) + c(f\hat{\textbf{i}} + h\hat{\textbf{j}}) = (ae+cf)\hat{\textbf{i}} + (ag+ch)\hat{\textbf{j}}$$ $$\hat{\textbf{j}''} = b(e\hat{\textbf{i}} + g\hat{\textbf{j}}) + d(f\hat{\textbf{i}} + h\hat{\textbf{j}}) = (be+df)\hat{\textbf{i}} + (bg+dh)\hat{\textbf{j}}$$ Now that I have expressed the unit vectors of S'' in 'the language of' S, I should be able to construct a matrix (call it $\textbf{P}$ ) which describes the single transformation from S to S''. It's LHS and RHS columns will be the unit vectors $\hat{\textbf{i}''}$ and $\hat{\textbf{j}''}$ expressed as column vectors in terms of $\hat{\textbf{i}}$ and $\hat{\textbf{j}}$ . Therefore, $$\textbf{P} = \begin{bmatrix}(ae+cf) & (be+df)\\(ag+ch) & (bg+dh)\end{bmatrix}$$ But I also know that the matrix $\textbf{MN}$ describes the transformation N, followed by M. So if applied to S, it should transfrom it to S', then S'' - i.e. it would have the exact same effect as if we just applied the transfromation represented by $\textbf{P}$ to S! So $\textbf{MN} = \textbf{P}$ because they describe the same transformation, and therefore $$\textbf{MN} = \begin{bmatrix}(ae+cf) & (be+df)\\(ag+ch) & (bg+dh)\end{bmatrix}$$ (B) The Normal Matrix Multiplier Algorithm For this method I just use the normal method where you go left to right along the rth row of $\textbf{M}$ and down the qth column of $\textbf{N}$ , multiplying corresponding elements and adding all the products together, to get the element in the rth row and qth column of $\textbf{MN}$ . $$\textbf{MN} = \begin{bmatrix}a & b\\c & d\end{bmatrix}\times \begin{bmatrix}e & f\\g & h\end{bmatrix} = \begin{bmatrix}(ae+bg) & (af+bh)\\(ce+dg) & (cf+dh)\end{bmatrix}$$ Conclusion So why are the two methods giving me different results? Can anyone point out a mistake I made. I asked a friend studying maths at uni and he said it's got something to do with a 'change of basis' but I didn't really understand. (I fear that this might be a big concept so, if it is to do with this and it's too much to explain here, could anyone point me to good videos/websites/pdfs/online resources that explain it well?) Thank you!","(Sorry in advance if I don't get all the terminology right, and for the lengthy question!) Intro I've got two matrices called and , which represent the linear transformations M and N respectively. I've got two methods (A and B) to try and find but the results they give me are inconsistent! Can anyone explain why?? (A) The Long-Winded Way I know that the columns of a matrix are the images of and respectively when I apply the linear transformation represented by that matrix to a coordinate axes, S, with unit vectors and . And these images are described as column vectors 'as seen by' S - i.e. the column vectors are in 'the language of' and . (I'll call this my 'theorem'.) Let S' be the coordinate axes with unit vectors and and S'' be the coordinate axes with unit vectors and . I'll also say that S' is the image of S under the transformation N and S'' is the image of S' under the transformation M. (I'm applying N before M!) So using my theorem I know that: because the unit vectors of S' are the images of those of S, under the transformation that represents. Similarly, I can reason that: because the unit vectors of S'' are the images of those of S' under the transformation M, and these images are described as column vectors (in terms of the unit vectors of S') in the columns of . So, by substituting the top 2 equations into the bottom 2: Now that I have expressed the unit vectors of S'' in 'the language of' S, I should be able to construct a matrix (call it ) which describes the single transformation from S to S''. It's LHS and RHS columns will be the unit vectors and expressed as column vectors in terms of and . Therefore, But I also know that the matrix describes the transformation N, followed by M. So if applied to S, it should transfrom it to S', then S'' - i.e. it would have the exact same effect as if we just applied the transfromation represented by to S! So because they describe the same transformation, and therefore (B) The Normal Matrix Multiplier Algorithm For this method I just use the normal method where you go left to right along the rth row of and down the qth column of , multiplying corresponding elements and adding all the products together, to get the element in the rth row and qth column of . Conclusion So why are the two methods giving me different results? Can anyone point out a mistake I made. I asked a friend studying maths at uni and he said it's got something to do with a 'change of basis' but I didn't really understand. (I fear that this might be a big concept so, if it is to do with this and it's too much to explain here, could anyone point me to good videos/websites/pdfs/online resources that explain it well?) Thank you!",2\times 2 \textbf{M} \textbf{N} \textbf{M} =\begin{bmatrix}a & b\\c & d\end{bmatrix}; \quad \textbf{N} =\begin{bmatrix}e & f\\g & h\end{bmatrix} \textbf{P}=\textbf{MN} 2\times 2 \hat{\textbf{i}} \hat{\textbf{j}} \hat{\textbf{i}} \hat{\textbf{j}} \hat{\textbf{i}} \hat{\textbf{j}} \hat{\textbf{i}'} \hat{\textbf{j}'} \hat{\textbf{i}''} \hat{\textbf{j}''} \hat{\textbf{i}'} = e\hat{\textbf{i}} + g\hat{\textbf{j}} \hat{\textbf{j}'} = f\hat{\textbf{i}} + h\hat{\textbf{j}} \textbf{N} \hat{\textbf{i}''} = a\hat{\textbf{i}'} + c\hat{\textbf{j}'} \hat{\textbf{j}''} = b\hat{\textbf{i}'} + d\hat{\textbf{j}'} \textbf{M} \hat{\textbf{i}''} = a(e\hat{\textbf{i}} + g\hat{\textbf{j}}) + c(f\hat{\textbf{i}} + h\hat{\textbf{j}}) = (ae+cf)\hat{\textbf{i}} + (ag+ch)\hat{\textbf{j}} \hat{\textbf{j}''} = b(e\hat{\textbf{i}} + g\hat{\textbf{j}}) + d(f\hat{\textbf{i}} + h\hat{\textbf{j}}) = (be+df)\hat{\textbf{i}} + (bg+dh)\hat{\textbf{j}} \textbf{P} \hat{\textbf{i}''} \hat{\textbf{j}''} \hat{\textbf{i}} \hat{\textbf{j}} \textbf{P} = \begin{bmatrix}(ae+cf) & (be+df)\\(ag+ch) & (bg+dh)\end{bmatrix} \textbf{MN} \textbf{P} \textbf{MN} = \textbf{P} \textbf{MN} = \begin{bmatrix}(ae+cf) & (be+df)\\(ag+ch) & (bg+dh)\end{bmatrix} \textbf{M} \textbf{N} \textbf{MN} \textbf{MN} = \begin{bmatrix}a & b\\c & d\end{bmatrix}\times \begin{bmatrix}e & f\\g & h\end{bmatrix} = \begin{bmatrix}(ae+bg) & (af+bh)\\(ce+dg) & (cf+dh)\end{bmatrix},"['matrices', 'linear-transformations', 'change-of-basis']"
73,Prove that $T:B\mapsto AB+BA$ is bijective if and only if $\det(A)$ and $\text{tr}(A)$ are nonzero.,Prove that  is bijective if and only if  and  are nonzero.,T:B\mapsto AB+BA \det(A) \text{tr}(A),"I am trying to solve the following problem: Let $A$ be a $2\times 2$ complex matrix. Show that the linear map $$B\longmapsto AB + BA$$ is a bijection if and only if $\det(A)$ and $\text{tr}(A)$ are nonzero. My attempt: Let us call $T$ the linear map. The Jordan canonical form of $A$ can be of the form $\begin{pmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{pmatrix}$ or $\begin{pmatrix} \lambda & 1 \\ 0 & \lambda \end{pmatrix}$ . Suppose that $A$ is diagonalizable. Then, there exist vectors $v_1$ and $v_2$ in $\mathbb{C}^2$ such that $Av_1 = \lambda_1 v_1$ , $Av_2 = \lambda_2 v_2$ , and $\{v_1,v_2\}$ is a basis of $\mathbb{C}^2$ . It follows that the set of the matrices $$X_1 = \begin{pmatrix} v_1 & 0  \end{pmatrix}, ~~X_2 = \begin{pmatrix} 0&v_1  \end{pmatrix},~~ X_3 = \begin{pmatrix} v_2 & 0  \end{pmatrix},~~X_4 = \begin{pmatrix} 0 & v_2  \end{pmatrix}$$ is a basis of $M_2(\mathbb{C})$ . Notice that $$T(X_1) = AX_1 + X_1 A = \begin{pmatrix} \lambda_1v_1 & 0  \end{pmatrix} + \begin{pmatrix} a_{11}v_1 & a_{12}v_1  \end{pmatrix} = \begin{pmatrix} (\lambda_1+a_{11})v_1 & a_{12}v_1  \end{pmatrix} = (\lambda_1+a_{11})X_1 + a_{12}X_2$$ and $$T(X_2) = AX_2 + X_2 A = \begin{pmatrix} 0 & \lambda_1v_1  \end{pmatrix} + \begin{pmatrix} a_{21}v_1 & a_{22}v_1  \end{pmatrix} = \begin{pmatrix} a_{21}v_1 & (\lambda_1+a_{22})v_1  \end{pmatrix} = a_{21}X_1 + (\lambda_1+a_{22})X_2.$$ Likewise, we get that $T(X_3) = (\lambda_2+a_{11})X_3 + a_{12}X_4$ and $T(X_4) = a_{21}X_3 + (\lambda_2+a_{22})X_4$ . Thus, the representation of $T$ on the basis $\{X_1,X_2,X_3,X_4\}$ is given by the matrix: $$ \begin{pmatrix} \lambda_1+a_{11} & a_{21} & 0 & 0 \\ a_{12}& \lambda_1+a_{22} & 0 & 0 \\ 0 & 0 & \lambda_2+a_{11} & a_{21} \\ 0 & 0 & a_{12} & \lambda_2+a_{22} \\ \end{pmatrix}. $$ Since this matrix is diagonal by blocks, we have that $$\det(T) = \det\begin{pmatrix} \lambda_1+a_{11} & a_{21} \\ a_{12}& \lambda_1+a_{22}  \end{pmatrix}  \det\begin{pmatrix} \lambda_2+a_{11} & a_{21} \\ a_{12} & \lambda_2+a_{22}  \end{pmatrix} = (\lambda_1^2+\text{tr}(A)\lambda_1+\det(A))(\lambda_2^2+\text{tr}(A)\lambda_2+\det(A)).$$ Because $\lambda_1$ and $\lambda_2$ are roots of the polynomial $x^2-\text{tr}(A)x+\det(A)$ , it follows that $$\det(T) = (2\text{tr}(A)\lambda_1)(2\text{tr}(A)\lambda_2) = 4\text{tr}(A)^2\det(A).$$ Hence, $\det(T)=0$ if and only if $\det(A)=0$ or $\text{tr}(A)=0$ , which completes the proof of this case. However, I cannot apply the same idea to prove the statement when $A$ is not diagonalizable, so I am stuck. Can you give me some advice to complete the exercise? Thanks in advance. Edit: I just realized that for the case $J_A = \begin{pmatrix} \lambda & 1 \\ 0 & \lambda \end{pmatrix}$ we can use a similar idea. Indeed, there exists a basis $\{v_1,v_2\}$ of $\mathbb{C}^2$ such that $Av_1 = \lambda v_1$ and $Av_2 = v_1 + \lambda v_2$ . Repeating the previous construction with this basis, we get the following representation for T: $$ \begin{pmatrix} \lambda+a_{11} & a_{21} & 1 & 0 \\ a_{12}& \lambda+a_{22} & 0 & 1 \\ 0 & 0 & \lambda+a_{11} & a_{21} \\ 0 & 0 & a_{12} & \lambda+a_{22} \\ \end{pmatrix}. $$ This matrix also has determinant equal to $4\text{tr}(A)^2\det(A)$ , and so the problem is solved. I am still looking for a shorter approach.","I am trying to solve the following problem: Let be a complex matrix. Show that the linear map is a bijection if and only if and are nonzero. My attempt: Let us call the linear map. The Jordan canonical form of can be of the form or . Suppose that is diagonalizable. Then, there exist vectors and in such that , , and is a basis of . It follows that the set of the matrices is a basis of . Notice that and Likewise, we get that and . Thus, the representation of on the basis is given by the matrix: Since this matrix is diagonal by blocks, we have that Because and are roots of the polynomial , it follows that Hence, if and only if or , which completes the proof of this case. However, I cannot apply the same idea to prove the statement when is not diagonalizable, so I am stuck. Can you give me some advice to complete the exercise? Thanks in advance. Edit: I just realized that for the case we can use a similar idea. Indeed, there exists a basis of such that and . Repeating the previous construction with this basis, we get the following representation for T: This matrix also has determinant equal to , and so the problem is solved. I am still looking for a shorter approach.","A 2\times 2 B\longmapsto AB + BA \det(A) \text{tr}(A) T A \begin{pmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{pmatrix} \begin{pmatrix} \lambda & 1 \\ 0 & \lambda \end{pmatrix} A v_1 v_2 \mathbb{C}^2 Av_1 = \lambda_1 v_1 Av_2 = \lambda_2 v_2 \{v_1,v_2\} \mathbb{C}^2 X_1 = \begin{pmatrix} v_1 & 0  \end{pmatrix}, ~~X_2 = \begin{pmatrix} 0&v_1  \end{pmatrix},~~ X_3 = \begin{pmatrix} v_2 & 0  \end{pmatrix},~~X_4 = \begin{pmatrix} 0 & v_2  \end{pmatrix} M_2(\mathbb{C}) T(X_1) = AX_1 + X_1 A = \begin{pmatrix} \lambda_1v_1 & 0  \end{pmatrix} + \begin{pmatrix} a_{11}v_1 & a_{12}v_1  \end{pmatrix} = \begin{pmatrix} (\lambda_1+a_{11})v_1 & a_{12}v_1  \end{pmatrix} = (\lambda_1+a_{11})X_1 + a_{12}X_2 T(X_2) = AX_2 + X_2 A = \begin{pmatrix} 0 & \lambda_1v_1  \end{pmatrix} + \begin{pmatrix} a_{21}v_1 & a_{22}v_1  \end{pmatrix} = \begin{pmatrix} a_{21}v_1 & (\lambda_1+a_{22})v_1  \end{pmatrix} = a_{21}X_1 + (\lambda_1+a_{22})X_2. T(X_3) = (\lambda_2+a_{11})X_3 + a_{12}X_4 T(X_4) = a_{21}X_3 + (\lambda_2+a_{22})X_4 T \{X_1,X_2,X_3,X_4\} 
\begin{pmatrix}
\lambda_1+a_{11} & a_{21} & 0 & 0 \\
a_{12}& \lambda_1+a_{22} & 0 & 0 \\
0 & 0 & \lambda_2+a_{11} & a_{21} \\
0 & 0 & a_{12} & \lambda_2+a_{22} \\
\end{pmatrix}.
 \det(T) = \det\begin{pmatrix}
\lambda_1+a_{11} & a_{21} \\
a_{12}& \lambda_1+a_{22} 
\end{pmatrix}  \det\begin{pmatrix}
\lambda_2+a_{11} & a_{21} \\
a_{12} & \lambda_2+a_{22} 
\end{pmatrix} = (\lambda_1^2+\text{tr}(A)\lambda_1+\det(A))(\lambda_2^2+\text{tr}(A)\lambda_2+\det(A)). \lambda_1 \lambda_2 x^2-\text{tr}(A)x+\det(A) \det(T) = (2\text{tr}(A)\lambda_1)(2\text{tr}(A)\lambda_2) = 4\text{tr}(A)^2\det(A). \det(T)=0 \det(A)=0 \text{tr}(A)=0 A J_A = \begin{pmatrix} \lambda & 1 \\ 0 & \lambda \end{pmatrix} \{v_1,v_2\} \mathbb{C}^2 Av_1 = \lambda v_1 Av_2 = v_1 + \lambda v_2 
\begin{pmatrix}
\lambda+a_{11} & a_{21} & 1 & 0 \\
a_{12}& \lambda+a_{22} & 0 & 1 \\
0 & 0 & \lambda+a_{11} & a_{21} \\
0 & 0 & a_{12} & \lambda+a_{22} \\
\end{pmatrix}.
 4\text{tr}(A)^2\det(A)","['linear-algebra', 'matrices', 'diagonalization']"
74,"Let $U, W$ be subspaces of finite dimensional Vector Space $V$ over $K$ with $\dim U + \dim W > \dim V$. Show that $U \cap W \neq$ {$0$}.",Let  be subspaces of finite dimensional Vector Space  over  with . Show that  {}.,"U, W V K \dim U + \dim W > \dim V U \cap W \neq 0","Let $U, W$ be subspaces of  finite dimensional Vector Space $V$ over $K$ with $\dim U + \dim W > \dim V$ . Show that $U \cap W \neq$ { $0$ }. The first thing I thought about is this formula $$\dim(U+W) = \dim U + \dim W - \dim (U \cap W)$$ We know $\dim(U+W) \leq \dim V$ because $U,W$ are subspaces of $V$ , and the dimension of a subspace is always less than or equal to the dimension of the space they belong to. Using those two formulas, we have $$\dim V \geq \dim(U+W) = \dim U + \dim W - \dim (U \cap W)$$ If we rearrange, we have $$\dim (U \cap W) \geq \dim U + \dim W - \dim (V)$$ We are given that $$\dim U + \dim W > \dim V$$ so we know $$\dim U + \dim W - \dim (V) > 0$$ so we have $$\dim (U \cap W) \geq \dim U + \dim W - \dim (V) > 0$$ which implies $U \cap W \neq$ { $0$ }. Is this correct or did I do something wrong or forgot something ?","Let be subspaces of  finite dimensional Vector Space over with . Show that { }. The first thing I thought about is this formula We know because are subspaces of , and the dimension of a subspace is always less than or equal to the dimension of the space they belong to. Using those two formulas, we have If we rearrange, we have We are given that so we know so we have which implies { }. Is this correct or did I do something wrong or forgot something ?","U, W V K \dim U + \dim W > \dim V U \cap W \neq 0 \dim(U+W) = \dim U + \dim W - \dim (U \cap W) \dim(U+W) \leq \dim V U,W V \dim V \geq \dim(U+W) = \dim U + \dim W - \dim (U \cap W) \dim (U \cap W) \geq \dim U + \dim W - \dim (V) \dim U + \dim W > \dim V \dim U + \dim W - \dim (V) > 0 \dim (U \cap W) \geq \dim U + \dim W - \dim (V) > 0 U \cap W \neq 0","['linear-algebra', 'matrices', 'vector-spaces', 'vectors', 'vector-fields']"
75,Can $\operatorname{Tr}[A^{-1} BAB^\top]$ be shown to be always positive if $A$ is real and positive definite? [closed],Can  be shown to be always positive if  is real and positive definite? [closed],\operatorname{Tr}[A^{-1} BAB^\top] A,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 9 months ago . Improve this question Let $A$ be a real symmetric positive definite matrix and $B$ is a real matrix with all eigenvalues zero. Can we prove or disprove that $\operatorname{Tr}[A^{-1} BAB^\top]$ is a positive number?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 9 months ago . Improve this question Let be a real symmetric positive definite matrix and is a real matrix with all eigenvalues zero. Can we prove or disprove that is a positive number?",A B \operatorname{Tr}[A^{-1} BAB^\top],"['matrices', 'trace', 'positive-definite']"
76,What can we get by row/column addition?,What can we get by row/column addition?,,"Let $\Bbb K$ be a field and let ${\bf M} \in {\Bbb K}^{n \times n}$ be a full rank matrix. Applying elementary row and column operations, one can transform $\bf M$ into the identity matrix. What can we achieve when only row/column addition is allowed? By row/column addition, I mean replacing a row/column by the sum of that row/column and a multiple of another row/cloumn. It is clear that the determinant remains the same. But can any two matrices $n\times n$ of full rank and the same determinant be transformed into each other by row and column addition? Example In the comments, I was asked to transform the matrix $$\begin{pmatrix}2&0\\ 0&\frac{1}{2}\end{pmatrix}$$ into the identity. Here is how this can be done, though I am not sure if this is the optimal way. $$\begin{pmatrix}1&{0}\\ 1&{1}\end{pmatrix} \begin{pmatrix}1&{1}\\ 0&{1}\end{pmatrix} \begin{pmatrix}2&0\\ 0&\frac{1}{2}\end{pmatrix} \begin{pmatrix}1&0\\ -2&{1}\end{pmatrix} \begin{pmatrix}1&-\frac{1}{2}\\ 0&{1}\end{pmatrix}.$$","Let be a field and let be a full rank matrix. Applying elementary row and column operations, one can transform into the identity matrix. What can we achieve when only row/column addition is allowed? By row/column addition, I mean replacing a row/column by the sum of that row/column and a multiple of another row/cloumn. It is clear that the determinant remains the same. But can any two matrices of full rank and the same determinant be transformed into each other by row and column addition? Example In the comments, I was asked to transform the matrix into the identity. Here is how this can be done, though I am not sure if this is the optimal way.","\Bbb K {\bf M} \in {\Bbb K}^{n \times n} \bf M n\times n \begin{pmatrix}2&0\\ 0&\frac{1}{2}\end{pmatrix} \begin{pmatrix}1&{0}\\ 1&{1}\end{pmatrix}
\begin{pmatrix}1&{1}\\ 0&{1}\end{pmatrix}
\begin{pmatrix}2&0\\ 0&\frac{1}{2}\end{pmatrix}
\begin{pmatrix}1&0\\ -2&{1}\end{pmatrix}
\begin{pmatrix}1&-\frac{1}{2}\\ 0&{1}\end{pmatrix}.","['linear-algebra', 'matrices', 'gaussian-elimination']"
77,A question on matrices whose minimal polynomials has no repeated roots,A question on matrices whose minimal polynomials has no repeated roots,,"Question a) Prove that a matrix $A\in M_n(\mathbb{C})$ satisfying $A^3=A$ can be diagonalized. b) Does the statment in (a) remain true if one replaces $\mathbb{C}$ by an arbitrary algebraically closed field $F$ ? Answer First of all, we can see that the matrix $A$ satisfies the polynomial $p(X)=X(X-1)(X+1)$ . Since the minimal polynomial has to divide $p(X)$ , we can conclude that the minimal polynomial of $A$ has no repeated root. This would yield that $A$ can be diagonalized. However, I couldn't answer the part (b). Actually, I do not know what the crucial part $\mathbb{C}$ plays in part (a) either. Thanks a lot in advance...","Question a) Prove that a matrix satisfying can be diagonalized. b) Does the statment in (a) remain true if one replaces by an arbitrary algebraically closed field ? Answer First of all, we can see that the matrix satisfies the polynomial . Since the minimal polynomial has to divide , we can conclude that the minimal polynomial of has no repeated root. This would yield that can be diagonalized. However, I couldn't answer the part (b). Actually, I do not know what the crucial part plays in part (a) either. Thanks a lot in advance...",A\in M_n(\mathbb{C}) A^3=A \mathbb{C} F A p(X)=X(X-1)(X+1) p(X) A A \mathbb{C},"['linear-algebra', 'matrices', 'diagonalization', 'minimal-polynomials']"
78,Can I use a function argument to prove invertibility of matrices?,Can I use a function argument to prove invertibility of matrices?,,"If all matrices can represent a linear transformation, can I refer to some general notion of the transformation associated with the matrix to make conclusions about the matrix? For example, I want to prove a simple true/false statement, ""If A and B are square matrices such that AB is invertible, then both A and B must be invertible."" I believe that the statement is true for the following reason: Assuming AB is well-defined and A and B are square, A and B have the same dimension. The product, AB is square as well then. AB is invertible and square, so the transformation AB represents is an isomorphism, thus the transformation associated with AB is a bijection. The transformation associated with AB is a bijection, so the transformation B represents is surjective and the transformation A represents is injective. Since both A and B have square matrices representing their linear transformations, injective and surjective respectively, the transformations must also be isomorphisms. Hence the transformation associated with A and B has an inverse, and thus A and B (the matrices) are invertible. This feels convoluted and likely isn't the simplest solution. I am just wondering if it is OK to bring in the notion of the transformations the matrices could represent like this? Am I missing some subtlety?","If all matrices can represent a linear transformation, can I refer to some general notion of the transformation associated with the matrix to make conclusions about the matrix? For example, I want to prove a simple true/false statement, ""If A and B are square matrices such that AB is invertible, then both A and B must be invertible."" I believe that the statement is true for the following reason: Assuming AB is well-defined and A and B are square, A and B have the same dimension. The product, AB is square as well then. AB is invertible and square, so the transformation AB represents is an isomorphism, thus the transformation associated with AB is a bijection. The transformation associated with AB is a bijection, so the transformation B represents is surjective and the transformation A represents is injective. Since both A and B have square matrices representing their linear transformations, injective and surjective respectively, the transformations must also be isomorphisms. Hence the transformation associated with A and B has an inverse, and thus A and B (the matrices) are invertible. This feels convoluted and likely isn't the simplest solution. I am just wondering if it is OK to bring in the notion of the transformations the matrices could represent like this? Am I missing some subtlety?",,"['linear-algebra', 'matrices', 'linear-transformations']"
79,"Can we always find matrices $M,N$ such that $\ker(MA+NB) = \ker(A) \cap \ker(B)$?",Can we always find matrices  such that ?,"M,N \ker(MA+NB) = \ker(A) \cap \ker(B)","Suppose $A,B$ are $n\times n$ matrices with coefficients in a field $\mathbb{F}$ . Is it always possible to find $n\times n$ matrices $M,N$ with coefficients in the same field such that $$\ker(MA+NB) = \ker(A)\cap \ker(B)?$$ If $\mathbb{F} = \mathbb{R}$ or $\mathbb{C}$ then we can set $M = A^T, N = B^T$ and then the result follows because $$A^TAv + B^TBv = 0 \implies 0 = \langle v, A^TAv + B^TBv\rangle $$ $$= \langle v, A^TAv\rangle + \langle v,B^TBv\rangle = \|Av\|^2 + \|Bv\|^2 $$ which immediately tells us that $Av = Bv = 0$ (where the inner products are the normal dot product and sesquilinear products on $\mathbb{R}^n, \mathbb{C}^n$ depending on if we are working over $\mathbb{R}$ or $\mathbb{C}$ ). Notably, this approach catastrophically fails over fields not equal to $\mathbb{R}, \mathbb{C}$ as for example we can have $$A = \begin{pmatrix}1 & 1 \\\ 1 & 1\end{pmatrix} \implies A^TA = 0$$ if you are working over $\mathbb{F}_2$ . Is it still possible to find $M,N$ despite the fact that this ""obvious guess"" fails?","Suppose are matrices with coefficients in a field . Is it always possible to find matrices with coefficients in the same field such that If or then we can set and then the result follows because which immediately tells us that (where the inner products are the normal dot product and sesquilinear products on depending on if we are working over or ). Notably, this approach catastrophically fails over fields not equal to as for example we can have if you are working over . Is it still possible to find despite the fact that this ""obvious guess"" fails?","A,B n\times n \mathbb{F} n\times n M,N \ker(MA+NB) = \ker(A)\cap \ker(B)? \mathbb{F} = \mathbb{R} \mathbb{C} M = A^T, N = B^T A^TAv + B^TBv = 0 \implies 0 = \langle v, A^TAv + B^TBv\rangle  = \langle v, A^TAv\rangle + \langle v,B^TBv\rangle = \|Av\|^2 + \|Bv\|^2  Av = Bv = 0 \mathbb{R}^n, \mathbb{C}^n \mathbb{R} \mathbb{C} \mathbb{R}, \mathbb{C} A = \begin{pmatrix}1 & 1 \\\ 1 & 1\end{pmatrix} \implies A^TA = 0 \mathbb{F}_2 M,N","['linear-algebra', 'matrices', 'finite-fields']"
80,Centralizer of rotation matrix,Centralizer of rotation matrix,,"Let $\theta \in [0,2\pi), \theta \neq 0,\pi$ . What is $C_{\mathbb{R}^{2\times 2}}(R(\theta))$ , where $R(\theta) := \begin{pmatrix} \cos(\theta)&-\sin(\theta)\\ \sin(\theta)&\cos(\theta)\end{pmatrix}$ and $C_{\mathbb{R}^{2\times 2}}(R(\theta))$ is the centralizer of $R(\theta)$ in $\mathbb{R}^{2\times 2}$ ? I think one way to show that $C_{\mathbb{R}^{2\times 2}}(R(\theta)) = \mathbb{R}\cdot SO(2)$ is to take some $U \in U(2)$ with $R(\theta) = U \begin{pmatrix}e^{i\theta}& 0 \\ 0 & e^{-i\theta}\end{pmatrix}U^H$ . Then $C_{\mathbb{R}^{2\times 2}}(R(\theta)) = \mathbb{R}^{2\times 2}\cap U\text{diag}_{\mathbb{C}}(2)U^H$ and one has to show that these are exactly the matrices in $\mathbb{R}\cdot SO(2)$ . Solving the equations for real numbers and complex diagonalizing is a bit annoying. Is there a quick elegant way to show this? And is there some general approach for solving such problem?","Let . What is , where and is the centralizer of in ? I think one way to show that is to take some with . Then and one has to show that these are exactly the matrices in . Solving the equations for real numbers and complex diagonalizing is a bit annoying. Is there a quick elegant way to show this? And is there some general approach for solving such problem?","\theta \in [0,2\pi), \theta \neq 0,\pi C_{\mathbb{R}^{2\times 2}}(R(\theta)) R(\theta) := \begin{pmatrix} \cos(\theta)&-\sin(\theta)\\ \sin(\theta)&\cos(\theta)\end{pmatrix} C_{\mathbb{R}^{2\times 2}}(R(\theta)) R(\theta) \mathbb{R}^{2\times 2} C_{\mathbb{R}^{2\times 2}}(R(\theta)) = \mathbb{R}\cdot SO(2) U \in U(2) R(\theta) = U
\begin{pmatrix}e^{i\theta}& 0 \\ 0 & e^{-i\theta}\end{pmatrix}U^H C_{\mathbb{R}^{2\times 2}}(R(\theta)) = \mathbb{R}^{2\times 2}\cap U\text{diag}_{\mathbb{C}}(2)U^H \mathbb{R}\cdot SO(2)","['linear-algebra', 'matrices', 'group-theory', 'lie-groups']"
81,Looking for an optimisation algorithm,Looking for an optimisation algorithm,,"I have an optimisation problem and I'm looking for the best algorithm to solve it. I have a sparse matrix (only 0 and 1, R and C stay for rows and columns) such as: | C1 | C2 | C3 | C4 | C5 | C6 | ---------------------------------- R1 |  1 |  0 |  0 |  1 |  0 |  0 | R2 |  0 |  1 |  0 |  0 |  0 |  0 | R3 |  0 |  1 |  1 |  0 |  1 |  1 | R4 |  0 |  0 |  1 |  0 |  0 |  0 | R5 |  1 |  0 |  0 |  0 |  0 |  0 | I need to merge the rows so that the ""1"" appears only in at most one of the merged rows (for example, R2 and R3 cannot be merged because both rows have a ""1"" in column C2), and so to minimise the total number of zero in the whole matrix. The final goal is to find (and merge) the rows that do not share any ""1"" on the same column. Using the above example, I can do it in several ways. Solution 1. | C1 | C2 | C3 | C4 | C5 | C6 | ------------------------------------- R1+R2 |  1 |  1 |  0 |  1 |  0 |  0 | R3    |  0 |  1 |  1 |  0 |  1 |  1 | R4+R5 |  1 |  0 |  1 |  0 |  0 |  0 | Solution 2. | C1 | C2 | C3 | C4 | C5 | C6 | ------------------------------------- R1+R4+R2 |  1 |  1 |  1 |  1 |  0 |  0 | R3+R5    |  1 |  1 |  1 |  0 |  1 |  1 | Solution 3. | C1 | C2 | C3 | C4 | C5 | C6 | ------------------------------------------- R1+R3       |  1 |  1 |  1 |  1 |  1 |  1 | R2+R4+R5    |  1 |  1 |  1 |  0 |  0 |  0 | All the presented solutions cannot be reduced any more, but the second and the third options are clearly more efficient w.r.t. the first one. In my problem, I need to deal with a matrix MxN where M and N are more than some hundreds. Is there an algorithm that can find the optimal solution?","I have an optimisation problem and I'm looking for the best algorithm to solve it. I have a sparse matrix (only 0 and 1, R and C stay for rows and columns) such as: | C1 | C2 | C3 | C4 | C5 | C6 | ---------------------------------- R1 |  1 |  0 |  0 |  1 |  0 |  0 | R2 |  0 |  1 |  0 |  0 |  0 |  0 | R3 |  0 |  1 |  1 |  0 |  1 |  1 | R4 |  0 |  0 |  1 |  0 |  0 |  0 | R5 |  1 |  0 |  0 |  0 |  0 |  0 | I need to merge the rows so that the ""1"" appears only in at most one of the merged rows (for example, R2 and R3 cannot be merged because both rows have a ""1"" in column C2), and so to minimise the total number of zero in the whole matrix. The final goal is to find (and merge) the rows that do not share any ""1"" on the same column. Using the above example, I can do it in several ways. Solution 1. | C1 | C2 | C3 | C4 | C5 | C6 | ------------------------------------- R1+R2 |  1 |  1 |  0 |  1 |  0 |  0 | R3    |  0 |  1 |  1 |  0 |  1 |  1 | R4+R5 |  1 |  0 |  1 |  0 |  0 |  0 | Solution 2. | C1 | C2 | C3 | C4 | C5 | C6 | ------------------------------------- R1+R4+R2 |  1 |  1 |  1 |  1 |  0 |  0 | R3+R5    |  1 |  1 |  1 |  0 |  1 |  1 | Solution 3. | C1 | C2 | C3 | C4 | C5 | C6 | ------------------------------------------- R1+R3       |  1 |  1 |  1 |  1 |  1 |  1 | R2+R4+R5    |  1 |  1 |  1 |  0 |  0 |  0 | All the presented solutions cannot be reduced any more, but the second and the third options are clearly more efficient w.r.t. the first one. In my problem, I need to deal with a matrix MxN where M and N are more than some hundreds. Is there an algorithm that can find the optimal solution?",,"['matrices', 'optimization', 'coloring']"
82,Can all matrices be written as the sum of commuting matrices?,Can all matrices be written as the sum of commuting matrices?,,"Can all matrices be written as the sum of commuting matrices? All invertible matrices $A$ can be written as a sum of matrices $A=B+C$ such that $B$ and $C$ commute, $BC=CB$ . One example might be $$B=\frac{A+A^{-1}}{2}, \quad C=\frac{A-A^{-1}}{2},$$ but I'm not certain if this is the only solution. My question is: Can non-invertible matrices be written as the sum of commuting matrices? Edit: In response to Lulu's comment, I need to add another condition that neither of $B$ or $C$ commute with all matrices.","Can all matrices be written as the sum of commuting matrices? All invertible matrices can be written as a sum of matrices such that and commute, . One example might be but I'm not certain if this is the only solution. My question is: Can non-invertible matrices be written as the sum of commuting matrices? Edit: In response to Lulu's comment, I need to add another condition that neither of or commute with all matrices.","A A=B+C B C BC=CB B=\frac{A+A^{-1}}{2}, \quad C=\frac{A-A^{-1}}{2}, B C","['linear-algebra', 'matrices', 'linear-transformations']"
83,How to solve this least-squares-like problem?,How to solve this least-squares-like problem?,,"Given the finite set of matrices $\Bbb X := \{ {\bf X}_1, {\bf X}_2, \dots, {\bf X}_N \} \subset \mathbb{C}^{N_t \times L}$ and the matrix ${\bf Y} \in \mathbb{C}^{N_r \times L}$ , $$ \left( \hat{H}, \hat{X} \right) = \arg \min_{{\bf H} \in \mathbb{C}^{N_r \times N_t}, \\ {\bf X}\in\mathbb{X}} \| {\bf Y} - {\bf H} {\bf X} \|_{\text{F}}^2 $$ where $N_r$ and $N_t$ denote the number of antennas at the receiver and at the transmitter, respectively, and $L$ denotes the total number of time slots. I am trying to solve the optimization problem above in MATLAB. I do not understand how we proceed. Any help will be highly appreciated.","Given the finite set of matrices and the matrix , where and denote the number of antennas at the receiver and at the transmitter, respectively, and denotes the total number of time slots. I am trying to solve the optimization problem above in MATLAB. I do not understand how we proceed. Any help will be highly appreciated.","\Bbb X := \{ {\bf X}_1, {\bf X}_2, \dots, {\bf X}_N \} \subset \mathbb{C}^{N_t \times L} {\bf Y} \in \mathbb{C}^{N_r \times L}  \left( \hat{H}, \hat{X} \right) = \arg \min_{{\bf H} \in \mathbb{C}^{N_r \times N_t}, \\ {\bf X}\in\mathbb{X}} \| {\bf Y} - {\bf H} {\bf X} \|_{\text{F}}^2  N_r N_t L","['matrices', 'optimization', 'least-squares', 'maximum-likelihood']"
84,A magic basis of $\mathbb{C}^5$,A magic basis of,\mathbb{C}^5,"This is a a small $n$ restriction of another question . Find a $5\times 5$ matrix of unit vectors $\xi_{ij}\in \mathbb{C}^5$ such that: Entries along rows and columns are orthogonal, that is for $1\leq i,j,k,l\leq 5$ : $$\delta_{i,k}+\delta_{j,l}=1\implies \langle \xi_{ij},\xi_{kl}\rangle=0.$$ Entries not on a common row or column are neither parallel, anti-parallel, nor orthogonal, again for $1\leq i,j,k,l\leq 5$ :: $$\delta_{i,k}+\delta_{j,l}=0\implies 0<|\langle \xi_{ij},\xi_{kl}\rangle|<1$$ This is proving a real wicked problem. Motivation and an $n=4$ example in the original questions. Attempts thus far: I have started with the first two rows : $$\xi=\frac12\begin{bmatrix} \begin{pmatrix} 2 \\ 0 \\ 0 \\ 0\\ 0\end{pmatrix} & \begin{pmatrix} 0 \\ \star \\ \star \\ \star\\ \star\end{pmatrix} & \begin{pmatrix} 0 \\ \star \\ \star \\ \star\\ \star\end{pmatrix}  & \begin{pmatrix} 0 \\ \star \\ \star \\ \star\\ \star\end{pmatrix}  & \begin{pmatrix} 0 \\ \star \\ \star \\ \star\\ \star\end{pmatrix} \\  \begin{pmatrix} 0 \\ 2 \\ 0 \\ 0\\ 0\end{pmatrix} & \begin{pmatrix} \star \\ 0 \\ \star \\ \star\\ \star\end{pmatrix} & \begin{pmatrix} \star \\ 0 \\ \star \\ \star\\ \star\end{pmatrix}  & \begin{pmatrix} \star \\ 0 \\ \star \\ \star\\ \star\end{pmatrix}  & \begin{pmatrix} \star  \\ 0 \\ \star \\ \star\\ \star\end{pmatrix} \end{bmatrix}$$ To get orthogonality along the first row, we are multiplying four numbers, and along columns, just three. I started with 24th roots of unity but then this reduced to sixth roots, powers of $w=\exp(2\pi i/6)$ . You can get some of what you want with this by considering what combinations of four and three sixth roots give zero. It seems possible to get all but one of: orthogonal along rows one, two, three appropriate orthogonality between rows one and two, and one and three appropriate orthogonality between rows two and three But it seems to fail before it ever gets to rows four or five, or indeed the second condition of being non-orthogonal nor parallel, nor anti-parallel. This wasn't the best I did, but something like: $$\xi=\frac12\begin{bmatrix} \begin{pmatrix} 2 \\ 0 \\ 0 \\ 0\\ 0\end{pmatrix} & \begin{pmatrix} 0 \\ 1 \\ 1 \\ 1\\ 1\end{pmatrix} & \begin{pmatrix} 0 \\ 1 \\ 1 \\ -1\\ -1\end{pmatrix}  & \begin{pmatrix} 0 \\ w \\ w^4 \\ 1\\ -1\end{pmatrix}  & \begin{pmatrix} 0 \\ 1\\ -1 \\ w^2\\ w^5\end{pmatrix} \\  \begin{pmatrix} 0 \\ 2 \\ 0 \\ 0\\ 0\end{pmatrix} & \begin{pmatrix} w \\ 0 \\ 1 \\ w^2\\ w^4\end{pmatrix} & \begin{pmatrix} w \\ 0 \\ 1 \\ w^5\\ 1\end{pmatrix}  & \begin{pmatrix} w^2 \\ 0 \\ w^4 \\ w^2\\ w\end{pmatrix}  & \begin{pmatrix} w \\ 0 \\ -1 \\ w^4\\ -1\end{pmatrix} \\  \begin{pmatrix} 0 \\ 0 \\ 2 \\ 0\\ 0\end{pmatrix} & \begin{pmatrix} 1\\ w \\ 0 \\ w^5\\ -1\end{pmatrix} & \begin{pmatrix} w^2 \\ w \\ 0 \\ w^2\\ 1\end{pmatrix}  & \begin{pmatrix} -1\\ w^2 \\ 0 \\ w^5\\ 1\end{pmatrix}  & \begin{pmatrix} w^2 \\ w \\ 0 \\ w\\ w^2\end{pmatrix}  \end{bmatrix}$$ This attempt falls down on orthogonality in row three, and orthogonality in row 2 vs row 3.","This is a a small restriction of another question . Find a matrix of unit vectors such that: Entries along rows and columns are orthogonal, that is for : Entries not on a common row or column are neither parallel, anti-parallel, nor orthogonal, again for :: This is proving a real wicked problem. Motivation and an example in the original questions. Attempts thus far: I have started with the first two rows : To get orthogonality along the first row, we are multiplying four numbers, and along columns, just three. I started with 24th roots of unity but then this reduced to sixth roots, powers of . You can get some of what you want with this by considering what combinations of four and three sixth roots give zero. It seems possible to get all but one of: orthogonal along rows one, two, three appropriate orthogonality between rows one and two, and one and three appropriate orthogonality between rows two and three But it seems to fail before it ever gets to rows four or five, or indeed the second condition of being non-orthogonal nor parallel, nor anti-parallel. This wasn't the best I did, but something like: This attempt falls down on orthogonality in row three, and orthogonality in row 2 vs row 3.","n 5\times 5 \xi_{ij}\in \mathbb{C}^5 1\leq i,j,k,l\leq 5 \delta_{i,k}+\delta_{j,l}=1\implies \langle \xi_{ij},\xi_{kl}\rangle=0. 1\leq i,j,k,l\leq 5 \delta_{i,k}+\delta_{j,l}=0\implies 0<|\langle \xi_{ij},\xi_{kl}\rangle|<1 n=4 \xi=\frac12\begin{bmatrix} \begin{pmatrix} 2 \\ 0 \\ 0 \\ 0\\ 0\end{pmatrix} & \begin{pmatrix} 0 \\ \star \\ \star \\ \star\\ \star\end{pmatrix} & \begin{pmatrix} 0 \\ \star \\ \star \\ \star\\ \star\end{pmatrix}  & \begin{pmatrix} 0 \\ \star \\ \star \\ \star\\ \star\end{pmatrix}  & \begin{pmatrix} 0 \\ \star \\ \star \\ \star\\ \star\end{pmatrix}
\\  \begin{pmatrix} 0 \\ 2 \\ 0 \\ 0\\ 0\end{pmatrix} & \begin{pmatrix} \star \\ 0 \\ \star \\ \star\\ \star\end{pmatrix} & \begin{pmatrix} \star \\ 0 \\ \star \\ \star\\ \star\end{pmatrix}  & \begin{pmatrix} \star \\ 0 \\ \star \\ \star\\ \star\end{pmatrix}  & \begin{pmatrix} \star  \\ 0 \\ \star \\ \star\\ \star\end{pmatrix} \end{bmatrix} w=\exp(2\pi i/6) \xi=\frac12\begin{bmatrix} \begin{pmatrix} 2 \\ 0 \\ 0 \\ 0\\ 0\end{pmatrix} & \begin{pmatrix} 0 \\ 1 \\ 1 \\ 1\\ 1\end{pmatrix} & \begin{pmatrix} 0 \\ 1 \\ 1 \\ -1\\ -1\end{pmatrix}  & \begin{pmatrix} 0 \\ w \\ w^4 \\ 1\\ -1\end{pmatrix}  & \begin{pmatrix} 0 \\ 1\\ -1 \\ w^2\\ w^5\end{pmatrix}
\\  \begin{pmatrix} 0 \\ 2 \\ 0 \\ 0\\ 0\end{pmatrix} & \begin{pmatrix} w \\ 0 \\ 1 \\ w^2\\ w^4\end{pmatrix} & \begin{pmatrix} w \\ 0 \\ 1 \\ w^5\\ 1\end{pmatrix}  & \begin{pmatrix} w^2 \\ 0 \\ w^4 \\ w^2\\ w\end{pmatrix}  & \begin{pmatrix} w \\ 0 \\ -1 \\ w^4\\ -1\end{pmatrix}
\\  \begin{pmatrix} 0 \\ 0 \\ 2 \\ 0\\ 0\end{pmatrix} & \begin{pmatrix} 1\\ w \\ 0 \\ w^5\\ -1\end{pmatrix} & \begin{pmatrix} w^2 \\ w \\ 0 \\ w^2\\ 1\end{pmatrix}  & \begin{pmatrix} -1\\ w^2 \\ 0 \\ w^5\\ 1\end{pmatrix}  & \begin{pmatrix} w^2 \\ w \\ 0 \\ w\\ w^2\end{pmatrix}  \end{bmatrix}","['matrices', 'complex-numbers', 'inner-products', 'orthogonality', 'roots-of-unity']"
85,Can a symmetric positive definite matrix interpolation have linear trace and determinant?,Can a symmetric positive definite matrix interpolation have linear trace and determinant?,,"Definition. Suppose $A_0, A_1 \in S_+^N$ are real symmetric positive definite $N \times N$ matrices. An interpolation from $A_0$ to $A_1$ is a continuous / smooth function $A \colon [0, 1] \to S_+^N$ such that $A(0) = A_0$ and $A(1) = A_1$ . Example 1. The linear interpolation $A_{\text{lin}}(t) := (1 - t) A_0 + t A_1$ has linear trace , that is, $\text{tr}(A_{\text{lin}}(t)) = (1 - t) \text{tr}(A_0) + t \text{tr}(A_1)$ , but not linear determinant: $\det(A_{\text{lin}}(t)) \ne (1 - t) \det(A_0) + \det(A_1)$ . Example 2. The logarithmic interpolation $A_{\log}(t) := \exp\left( (1 - t) \log(A_0) + t \log(A_1)\right)$ has linear determinant but not linear trace. My question. Does there exist an interpolation from $A_0$ to $A_1$ with linear trace and determinant? My attempt. I tried to show that this is not possible for $N = 2$ (it clearly is possible for $N = 1$ ). Suppose $A_k = \begin{pmatrix} a_k & b_k \\ b_k & c_k \end{pmatrix}$ for $k \in \{ 0, 1 \}$ and $A(t) = \begin{pmatrix} a(t) & b(t) \\ b(t) & c(t) \end{pmatrix}$ . Then we require that \begin{gather*} a(t) + c(t) \overset{!}{=} (1 - t) (a_0 + c_0) + t (a_1 + c_1) \\ a(t) c(t) - b(t)^2 \overset{!}{=} (1 - t) [a_0 c_0 - b_0^2] + t [a_1 c_1 - b_1^2]. \end{gather*} We can't simply choose $a(t) = (1 - t) a_0 + t a_1$ and $c(t)$ similarly, because then we are in the case of Example 1 and thus don't have linear determinant. Since trace and determinant both only depend on the eigenvalues, there might be an argument using diagonalization. Plugging the first into the second equation of @AndreasLenz yields $$ \lambda_1(t) \big((1 - t) (\lambda_1(0) + \lambda_2(0)) + t (\lambda_1(1) + \lambda_2(1)) - \lambda_1(t) \big) = (1 - t) \lambda_1(0) \lambda_2(0) + t \lambda_1(1) \lambda_2(1) $$ and thus \begin{align} 2 \lambda_1(t) & = (1 - t)(\lambda_1(0) + \lambda_2(0)) + t (\lambda_1(1) + \lambda_2(1)) \\ & \quad \pm \sqrt{((1 - t)(\lambda_1(0) + \lambda_2(0)) + t (\lambda_1(1) + \lambda_2(1)))^2 - 4 \lambda_1(0) \lambda_2(0) (1 - t) - 4 \lambda_1(1) \lambda_2(1) t}.\end{align}","Definition. Suppose are real symmetric positive definite matrices. An interpolation from to is a continuous / smooth function such that and . Example 1. The linear interpolation has linear trace , that is, , but not linear determinant: . Example 2. The logarithmic interpolation has linear determinant but not linear trace. My question. Does there exist an interpolation from to with linear trace and determinant? My attempt. I tried to show that this is not possible for (it clearly is possible for ). Suppose for and . Then we require that We can't simply choose and similarly, because then we are in the case of Example 1 and thus don't have linear determinant. Since trace and determinant both only depend on the eigenvalues, there might be an argument using diagonalization. Plugging the first into the second equation of @AndreasLenz yields and thus","A_0, A_1 \in S_+^N N \times N A_0 A_1 A \colon [0, 1] \to S_+^N A(0) = A_0 A(1) = A_1 A_{\text{lin}}(t) := (1 - t) A_0 + t A_1 \text{tr}(A_{\text{lin}}(t)) = (1 - t) \text{tr}(A_0) + t \text{tr}(A_1) \det(A_{\text{lin}}(t)) \ne (1 - t) \det(A_0) + \det(A_1) A_{\log}(t) := \exp\left( (1 - t) \log(A_0) + t \log(A_1)\right) A_0 A_1 N = 2 N = 1 A_k = \begin{pmatrix} a_k & b_k \\ b_k & c_k \end{pmatrix} k \in \{ 0, 1 \} A(t) = \begin{pmatrix} a(t) & b(t) \\ b(t) & c(t) \end{pmatrix} \begin{gather*}
a(t) + c(t)
\overset{!}{=} (1 - t) (a_0 + c_0) + t (a_1 + c_1) \\
a(t) c(t) - b(t)^2
\overset{!}{=} (1 - t) [a_0 c_0 - b_0^2] + t [a_1 c_1 - b_1^2].
\end{gather*} a(t) = (1 - t) a_0 + t a_1 c(t) 
\lambda_1(t) \big((1 - t) (\lambda_1(0) + \lambda_2(0)) + t (\lambda_1(1) + \lambda_2(1)) - \lambda_1(t) \big) = (1 - t) \lambda_1(0) \lambda_2(0) + t \lambda_1(1) \lambda_2(1)
 \begin{align} 2 \lambda_1(t) & = (1 - t)(\lambda_1(0) + \lambda_2(0)) + t (\lambda_1(1) + \lambda_2(1)) \\ & \quad \pm \sqrt{((1 - t)(\lambda_1(0) + \lambda_2(0)) + t (\lambda_1(1) + \lambda_2(1)))^2 - 4 \lambda_1(0) \lambda_2(0) (1 - t) - 4 \lambda_1(1) \lambda_2(1) t}.\end{align}","['matrices', 'continuity', 'interpolation', 'symmetric-matrices', 'positive-definite']"
86,Show that $\det(xA+yB+zI_{n})=\det(yA+xB+zI_{n})$,Show that,\det(xA+yB+zI_{n})=\det(yA+xB+zI_{n}),"We have $A$ and $B$ $(n×n)$ matrices with complex entries. We know that $A-B=AB-BA$ . Show that $$\det(xA+yB+zI_{n})=\det(yA+xB+zI_{n})$$ for every $x,y,z$ complex numbers with $x+y≠0$ . We can see that $\operatorname{Tr}(A)=\operatorname{Tr}(B)$ . I tried to suppose $A$ or $B$ are invertible and try changing $A-B=AB-BA$ somehow, but I dont know if that helps. We can also see that $$\operatorname{Tr}(xA+yB)=\operatorname{Tr}(yA+xB)=(x+y)\operatorname{Tr}(A).$$ The conclusion looks like we need to show that $f(x,y)=f(y,x)$ . Maybe some calculus and we can use the continuity of polynomial functions? I also minded calculating the determinants using polynomial forms.","We have and matrices with complex entries. We know that . Show that for every complex numbers with . We can see that . I tried to suppose or are invertible and try changing somehow, but I dont know if that helps. We can also see that The conclusion looks like we need to show that . Maybe some calculus and we can use the continuity of polynomial functions? I also minded calculating the determinants using polynomial forms.","A B (n×n) A-B=AB-BA \det(xA+yB+zI_{n})=\det(yA+xB+zI_{n}) x,y,z x+y≠0 \operatorname{Tr}(A)=\operatorname{Tr}(B) A B A-B=AB-BA \operatorname{Tr}(xA+yB)=\operatorname{Tr}(yA+xB)=(x+y)\operatorname{Tr}(A). f(x,y)=f(y,x)","['matrices', 'eigenvalues-eigenvectors', 'contest-math', 'determinant', 'characteristic-polynomial']"
87,When can we continuously extend a linearly independent set to a basis?,When can we continuously extend a linearly independent set to a basis?,,"As we all know, any linearly independent subset of $\Bbb{R}^n$ (or vector spaces in general) can be extended to a (Hamel) basis. I'm wondering when this can be done continuously. To put it precisely: For which $m, n \in \Bbb{N}$ , where $m < n$ , does there exist a continuous map $\phi$ from the set of real $m \times n$ rectangular matrices of full row rank, to $\operatorname{GL}_n(\Bbb{R})$ , such that the submatrix made from the first $m$ rows of $\phi(A)$ is always $A$ ? My work so far: This is a generalisation of an older question that someone else asked, which I answered (twice). One of my answers shows that this cannot be done when $n$ is odd and $m = 1$ , using the hairy ball theorem. If such a map existed, then restriction to any row but the first would produce a continuous function mapping a non-zero vector to a linearly independent vector, which contradicts the hairy ball theorem. Of course, this doesn't work so well when $n$ is even. If $n$ is even, we can simply map $$(v_1, v_2, \ldots, v_{n-1}, v_n) \mapsto (v_2, -v_1, \ldots, v_n, -v_{n-1}),$$ i.e. swapping pairs of coordinates and negating one. As such, we always get a vector orthogonal to the original vector, and the map is clearly continuous. And, of course, one could simply pick other pairings as well, to get other vectors orthogonal to the original vector. However, there's no guarantee that we could build a whole basis from these maps! If $m = n - 1$ , then such a map exists. Indeed, all we need is a continuous map that takes $A$ and produces a vector linearly independent of all the rows. We can do that by generalising the cross product. We define: $$\psi(A) = \det\begin{pmatrix} \matrix{e_1 & e_2 & \ldots & e_n} \\ A \end{pmatrix},$$ where $e_1, \ldots, e_n$ are the standard basis vectors for $\Bbb{R}^n$ . We compute the ""determinant"" in much the same way as the usual mnemonic for the cross product: $$u \times v = \det \begin{pmatrix} \hat{i} & \hat{j} & \hat{k} \\ u_1 & u_2 & u_3 \\ v_1 & v_2 & v_3 \end{pmatrix} = \det\pmatrix{u_2 & u_3 \\ v_2 & v_3}\hat{i} - \det\pmatrix{u_1 & u_3 \\ v_1 & v_3}\hat{j} + \det\pmatrix{u_1 & u_2 \\ v_1 & v_2}\hat{k}.$$ This $\psi$ works in much the same way: taking $\psi(A) \cdot v$ for some $v \in \Bbb{R}^n$ , the result becomes $\det \pmatrix{v \\ A}$ , so if $v$ is already a row of $A$ , the result is $0$ . Thus, $\psi(A)$ is orthogonal to the rowspace of $A$ , and hence adding it as a row to $A$ will produce an invertible square matrix. Given the determinant is continuous, this is also a continuous choice. Searching yielded some other related posts , but both start with a one parameter family of vectors/rectnagular matrices, rather than considering a function on the full set of matrices. As such, they are asking something weaker. So, is there some characterisation of when such a map exists?","As we all know, any linearly independent subset of (or vector spaces in general) can be extended to a (Hamel) basis. I'm wondering when this can be done continuously. To put it precisely: For which , where , does there exist a continuous map from the set of real rectangular matrices of full row rank, to , such that the submatrix made from the first rows of is always ? My work so far: This is a generalisation of an older question that someone else asked, which I answered (twice). One of my answers shows that this cannot be done when is odd and , using the hairy ball theorem. If such a map existed, then restriction to any row but the first would produce a continuous function mapping a non-zero vector to a linearly independent vector, which contradicts the hairy ball theorem. Of course, this doesn't work so well when is even. If is even, we can simply map i.e. swapping pairs of coordinates and negating one. As such, we always get a vector orthogonal to the original vector, and the map is clearly continuous. And, of course, one could simply pick other pairings as well, to get other vectors orthogonal to the original vector. However, there's no guarantee that we could build a whole basis from these maps! If , then such a map exists. Indeed, all we need is a continuous map that takes and produces a vector linearly independent of all the rows. We can do that by generalising the cross product. We define: where are the standard basis vectors for . We compute the ""determinant"" in much the same way as the usual mnemonic for the cross product: This works in much the same way: taking for some , the result becomes , so if is already a row of , the result is . Thus, is orthogonal to the rowspace of , and hence adding it as a row to will produce an invertible square matrix. Given the determinant is continuous, this is also a continuous choice. Searching yielded some other related posts , but both start with a one parameter family of vectors/rectnagular matrices, rather than considering a function on the full set of matrices. As such, they are asking something weaker. So, is there some characterisation of when such a map exists?","\Bbb{R}^n m, n \in \Bbb{N} m < n \phi m \times n \operatorname{GL}_n(\Bbb{R}) m \phi(A) A n m = 1 n n (v_1, v_2, \ldots, v_{n-1}, v_n) \mapsto (v_2, -v_1, \ldots, v_n, -v_{n-1}), m = n - 1 A \psi(A) = \det\begin{pmatrix} \matrix{e_1 & e_2 & \ldots & e_n} \\ A \end{pmatrix}, e_1, \ldots, e_n \Bbb{R}^n u \times v = \det \begin{pmatrix} \hat{i} & \hat{j} & \hat{k} \\ u_1 & u_2 & u_3 \\ v_1 & v_2 & v_3 \end{pmatrix} = \det\pmatrix{u_2 & u_3 \\ v_2 & v_3}\hat{i} - \det\pmatrix{u_1 & u_3 \\ v_1 & v_3}\hat{j} + \det\pmatrix{u_1 & u_2 \\ v_1 & v_2}\hat{k}. \psi \psi(A) \cdot v v \in \Bbb{R}^n \det \pmatrix{v \\ A} v A 0 \psi(A) A A","['linear-algebra', 'matrices']"
88,Product of Inverse Hankel Matrix,Product of Inverse Hankel Matrix,,"Consider $H_n$ , the $n\times n$ Hankel matrix of the Catalan numbers starting from $2$ : $$H_n = \begin{bmatrix} 2 & 5 & 14 & 42 & 132\\ 5 & 14 & 42 & 132 & 429\\ 14 & 42 & 132 & 429 & 1430 & \cdots\\ 42 & 132 & 429 & 1430 & 4862\\ 132 & 429 & 1430 & 4862 & 16796\\ &&\vdots\end{bmatrix}$$ It is known that $\text{det}(H_n) = n + 1$ . (see Hankel Matrix ) Consider the column vector, $$c_n = \begin{bmatrix}1 \\ 2 \\ 5 \\ 14 \\ \vdots \end{bmatrix}$$ that contains the first $n$ Catalan numbers. I have found a pattern that I have checked up to $n=240$ , that $$(c_n)^T(H_n)^{-1}(c_n) = \frac{n}{n+1}$$ Is there any method I can take to prove this, or is there a counterexample? Note also that this product is the only non-zero eigenvalue of $(c_n)(c_n)^T(H_n)^{-1}$ .","Consider , the Hankel matrix of the Catalan numbers starting from : It is known that . (see Hankel Matrix ) Consider the column vector, that contains the first Catalan numbers. I have found a pattern that I have checked up to , that Is there any method I can take to prove this, or is there a counterexample? Note also that this product is the only non-zero eigenvalue of .","H_n n\times n 2 H_n = \begin{bmatrix}
2 & 5 & 14 & 42 & 132\\
5 & 14 & 42 & 132 & 429\\
14 & 42 & 132 & 429 & 1430 & \cdots\\
42 & 132 & 429 & 1430 & 4862\\
132 & 429 & 1430 & 4862 & 16796\\
&&\vdots\end{bmatrix} \text{det}(H_n) = n + 1 c_n = \begin{bmatrix}1 \\ 2 \\ 5 \\ 14 \\ \vdots \end{bmatrix} n n=240 (c_n)^T(H_n)^{-1}(c_n) = \frac{n}{n+1} (c_n)(c_n)^T(H_n)^{-1}","['linear-algebra', 'matrices', 'catalan-numbers', 'hankel-matrices']"
89,Cauchy-Schwarz inequality for positive semidefinite matrices,Cauchy-Schwarz inequality for positive semidefinite matrices,,"I have a vector space $V$ of finite dimension $n$ over the field $\mathbb R$ . I define the following product $$u\otimes v=\sum_{i=0}^\infty b_iu_iv_i^T$$ where $b_i\in\mathbb R^+$ and all $u_i$ and $v_i$ are elements of the vector space $V=\mathbb R^n$ . In the context of this formula, we use $u$ (and $v$ ) to mean the infinite list of $u_i$ (and $v_i$ ). This product defines a matrix in $\mathbb R^{n\times n}$ , given a choice of basis in $V$ . In particular, $u\otimes u$ will be (symmetrical) positive semidefinite. We can assume that $\sum b_i$ converges and that all $u_i$ and $v_i$ are bounded (the $L^2$ norm of each vector is bounded). That ensures that $u\otimes v$ converges. If we choose $n=1$ , the product $u\otimes v$ simplifies to a scalar product and we can easily show using Cauchy-Schwarz that: $$(u\otimes u)-(u\otimes v)(v\otimes v)^{-1}(v\otimes u)\ge0$$ How would I prove a similar inequality in general, for any finite $n$ ? As $b_i>0$ for all $i$ , we have both $u\otimes u$ and $v\otimes v$ (symmetrical) positive semidefinite matrices so the inequality should be understood as meaning that $(u\otimes u)-(u\otimes v)(v\otimes v)^{-1}(v\otimes u)$ is also (symmetrical) positive semidefinite.","I have a vector space of finite dimension over the field . I define the following product where and all and are elements of the vector space . In the context of this formula, we use (and ) to mean the infinite list of (and ). This product defines a matrix in , given a choice of basis in . In particular, will be (symmetrical) positive semidefinite. We can assume that converges and that all and are bounded (the norm of each vector is bounded). That ensures that converges. If we choose , the product simplifies to a scalar product and we can easily show using Cauchy-Schwarz that: How would I prove a similar inequality in general, for any finite ? As for all , we have both and (symmetrical) positive semidefinite matrices so the inequality should be understood as meaning that is also (symmetrical) positive semidefinite.",V n \mathbb R u\otimes v=\sum_{i=0}^\infty b_iu_iv_i^T b_i\in\mathbb R^+ u_i v_i V=\mathbb R^n u v u_i v_i \mathbb R^{n\times n} V u\otimes u \sum b_i u_i v_i L^2 u\otimes v n=1 u\otimes v (u\otimes u)-(u\otimes v)(v\otimes v)^{-1}(v\otimes u)\ge0 n b_i>0 i u\otimes u v\otimes v (u\otimes u)-(u\otimes v)(v\otimes v)^{-1}(v\otimes u),"['linear-algebra', 'matrices', 'cauchy-schwarz-inequality']"
90,Maximizing $\mbox{tr} \left( {\bf X}^{-1} {\bf A} {\bf X} {\bf B} + 2 {\bf X} {\bf C} \right)$ subject to ${\bf X} {\bf X}^\top = \gamma {\bf I}$,Maximizing  subject to,\mbox{tr} \left( {\bf X}^{-1} {\bf A} {\bf X} {\bf B} + 2 {\bf X} {\bf C} \right) {\bf X} {\bf X}^\top = \gamma {\bf I},"Can the following optimization statement be converted into a simpler form? $$\begin{array}{ll} \underset{{\bf X}}{\text{maximize}} & \mbox{tr} \left( {\bf X}^{-1} {\bf A} {\bf X} {\bf B} + 2 {\bf X} {\bf C} \right)\\ \text{subject to} & {\bf X} {\bf X}^\top = \gamma {\bf I}\end{array}$$ where $\bf A$ and $\bf B$ are symmetric matrices, and $\gamma > 0$ is a constant. I tried using Lagrange multipliers and ended up with the equation: ${\bf A} {\bf X} {\bf B} + {\bf C}^\top = {\bf X} {\bf \Lambda}$ , where ${\bf \Lambda}$ is the matrix with Lagrange multipliers along its diagonal. I'm not sure how to go about it after that. Please help.","Can the following optimization statement be converted into a simpler form? where and are symmetric matrices, and is a constant. I tried using Lagrange multipliers and ended up with the equation: , where is the matrix with Lagrange multipliers along its diagonal. I'm not sure how to go about it after that. Please help.",\begin{array}{ll} \underset{{\bf X}}{\text{maximize}} & \mbox{tr} \left( {\bf X}^{-1} {\bf A} {\bf X} {\bf B} + 2 {\bf X} {\bf C} \right)\\ \text{subject to} & {\bf X} {\bf X}^\top = \gamma {\bf I}\end{array} \bf A \bf B \gamma > 0 {\bf A} {\bf X} {\bf B} + {\bf C}^\top = {\bf X} {\bf \Lambda} {\bf \Lambda},"['matrices', 'optimization', 'orthogonal-matrices']"
91,"Does the Cauchy-Binet theorem simplify for matrices AWB, where W is a square matrix?","Does the Cauchy-Binet theorem simplify for matrices AWB, where W is a square matrix?",,"Suppose I have two matrices $A$ and $B$ , where $A$ is $m\times n$ and $B$ is $n\times m$ . The Cauchy-Binet theorem gives a way to calculate $\det(AB)$ : $$\det(AB) = \sum_S\det(A_S)\det(B_S),$$ where the sum is over all length- $m$ subsets of $\{1,2,...,n\}$ , $A_S$ is the $m\times m$ matrix whose columns are the columns in $A$ with indices $S$ , and $B_S$ is the $m\times m$ matrix whose rows are the rows in $B$ with indices $S$ . Now suppose I have a square $n\times n$ matrix $W$ . Is there a comparatively simple way to calculate $\det(AWB)$ ? It would be extra convenient if there is an expression for this in terms of $\det W$ , so it could be extended to $N$ square matrices, something like $\det(A\prod_k^NW_kB)$ . I expected to find a corollary to the Cauchy-Binet theorem for linear transformations but haven't come across anything. Maybe it's obvious and my linear algebra is just too rusty?","Suppose I have two matrices and , where is and is . The Cauchy-Binet theorem gives a way to calculate : where the sum is over all length- subsets of , is the matrix whose columns are the columns in with indices , and is the matrix whose rows are the rows in with indices . Now suppose I have a square matrix . Is there a comparatively simple way to calculate ? It would be extra convenient if there is an expression for this in terms of , so it could be extended to square matrices, something like . I expected to find a corollary to the Cauchy-Binet theorem for linear transformations but haven't come across anything. Maybe it's obvious and my linear algebra is just too rusty?","A B A m\times n B n\times m \det(AB) \det(AB) = \sum_S\det(A_S)\det(B_S), m \{1,2,...,n\} A_S m\times m A S B_S m\times m B S n\times n W \det(AWB) \det W N \det(A\prod_k^NW_kB)","['linear-algebra', 'matrices', 'linear-transformations', 'determinant', 'matrix-equations']"
92,When does a real symmetric matrix have $LDL^{T}$ decomposition? And when is the $LDL^{T}$ decomposition unique?,When does a real symmetric matrix have  decomposition? And when is the  decomposition unique?,LDL^{T} LDL^{T},"Suppose that $A$ is an $n \times n$ real symmetric indefinite matrix, ${\rm rank}(A) = k$ and $k \leq n$ . If the $LDL^{T}$ decomposition of $A$ exists, we denote it as $A=LDL^{T}$ , where $L$ is a lower unit triangular matrix and $D$ is a diagonal matrix. I have two questions about it: (1) What is the necessity and sufficiency of ""the $LDL^{T}$ decomposition of $A$ exists""? (2) What is the necessity and sufficiency of ""the $LDL^{T}$ decomposition of $A$ is unique""? I read the wiki and some books but I can't find the answer. I have an example as follows: \begin{align}     A_1 &=      \begin{bmatrix}         1 & 0 & 3 \\         0 & 0 & 3 \\         3 & 3 & 3 \\     \end{bmatrix} {\rm\ is\ invertible\ and\ has\ no\ } LDL^{T} {\rm\ decomposition},\\     A_2 &=      \begin{bmatrix}         1 & 1 & 3 \\         1 & 1 & 3 \\         3 & 3 & 3 \\     \end{bmatrix} =     \begin{bmatrix}         1 & 0 & 0 \\         1 & 1 & 0 \\         3 & 3 & 1 \\     \end{bmatrix}      \begin{bmatrix}         1 & 0 & 0 \\         0 & 0 & 0 \\         0 & 0 & -6 \\     \end{bmatrix}      \begin{bmatrix}         1 & 1 & 3 \\         0 & 1 & 3 \\         0 & 0 & 1 \\     \end{bmatrix}{\rm\ is\ singular}, \\     A_3 &=      \begin{bmatrix}         0 & 1 & 1 \\         1 & 1 & 1 \\         1 & 1 & 1 \\     \end{bmatrix} {\rm\ is\ singular\ and\ has\ no\ } LDL^{T} {\rm\ decomposition}.\\ \end{align}","Suppose that is an real symmetric indefinite matrix, and . If the decomposition of exists, we denote it as , where is a lower unit triangular matrix and is a diagonal matrix. I have two questions about it: (1) What is the necessity and sufficiency of ""the decomposition of exists""? (2) What is the necessity and sufficiency of ""the decomposition of is unique""? I read the wiki and some books but I can't find the answer. I have an example as follows:","A n \times n {\rm rank}(A) = k k \leq n LDL^{T} A A=LDL^{T} L D LDL^{T} A LDL^{T} A \begin{align}
    A_1 &= 
    \begin{bmatrix}
        1 & 0 & 3 \\
        0 & 0 & 3 \\
        3 & 3 & 3 \\
    \end{bmatrix} {\rm\ is\ invertible\ and\ has\ no\ } LDL^{T} {\rm\ decomposition},\\
    A_2 &= 
    \begin{bmatrix}
        1 & 1 & 3 \\
        1 & 1 & 3 \\
        3 & 3 & 3 \\
    \end{bmatrix} =
    \begin{bmatrix}
        1 & 0 & 0 \\
        1 & 1 & 0 \\
        3 & 3 & 1 \\
    \end{bmatrix} 
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 0 & 0 \\
        0 & 0 & -6 \\
    \end{bmatrix} 
    \begin{bmatrix}
        1 & 1 & 3 \\
        0 & 1 & 3 \\
        0 & 0 & 1 \\
    \end{bmatrix}{\rm\ is\ singular}, \\
    A_3 &= 
    \begin{bmatrix}
        0 & 1 & 1 \\
        1 & 1 & 1 \\
        1 & 1 & 1 \\
    \end{bmatrix} {\rm\ is\ singular\ and\ has\ no\ } LDL^{T} {\rm\ decomposition}.\\
\end{align}","['linear-algebra', 'matrices', 'matrix-decomposition']"
93,When will $AB$ and $B$ have the same column space?,When will  and  have the same column space?,AB B,"Suppose $A\in \mathbb{R}^{n\times n}$ and $B\in \mathbb{R}^{n\times m}$ with $n>m$ and $rank(B)=m$ . Now given $B$ , I would like to find all $A$ 's such that $AB$ and $B$ have the same column space. A trivial solution would be $A=I_n$ , but I would like to find all solutions (explicitly if possible? or any necessary and sufficient condition that $A$ need to satisify?)","Suppose and with and . Now given , I would like to find all 's such that and have the same column space. A trivial solution would be , but I would like to find all solutions (explicitly if possible? or any necessary and sufficient condition that need to satisify?)",A\in \mathbb{R}^{n\times n} B\in \mathbb{R}^{n\times m} n>m rank(B)=m B A AB B A=I_n A,"['linear-algebra', 'matrices', 'vector-spaces', 'matrix-calculus']"
94,What are the eigenvalues of $\begin{pmatrix} A & B \\ B &-A \end{pmatrix}$ in terms of $A$ and $B$?,What are the eigenvalues of  in terms of  and ?,\begin{pmatrix} A & B \\ B &-A \end{pmatrix} A B,"It is known that the set of eigenvalues of the following block matrix $$ C = \begin{pmatrix} A & B \\ B & A \end{pmatrix} $$ is the union of the eigenvalues of the matrices $A + B$ and $A - B$ . I am interested in the matrix of the following form $$ C = \begin{pmatrix} A & B \\ B &-A \end{pmatrix} $$ Is there a description of the eigenvalues of $C$ in terms of $A$ and $B$ ? Edit. If $AB=BA$ , then we can do the following. $$  C =  \begin{pmatrix}  A & B \\ B &-A \end{pmatrix} \begin{pmatrix}  v \\ u \end{pmatrix} =\lambda \begin{pmatrix}  v \\ u \end{pmatrix} $$ implies $$ \left\{   \begin{array}{l}     Av+Bu=\lambda v \\     Bv-Au=\lambda u    \end{array}. \right. $$ By multiplying the first equation by $B$ and assuming $AB=BA$ , we get $$ (A^2+B^2)u=\lambda^2u. $$ Therefore, $\lambda^2$ is an eigenvalue of $A^2+B^2$ , what is discussed in the comments.","It is known that the set of eigenvalues of the following block matrix is the union of the eigenvalues of the matrices and . I am interested in the matrix of the following form Is there a description of the eigenvalues of in terms of and ? Edit. If , then we can do the following. implies By multiplying the first equation by and assuming , we get Therefore, is an eigenvalue of , what is discussed in the comments."," C = \begin{pmatrix} A & B \\ B & A \end{pmatrix}  A + B A - B  C = \begin{pmatrix} A & B \\ B &-A \end{pmatrix}  C A B AB=BA  
C = 
\begin{pmatrix} 
A & B \\ B &-A \end{pmatrix}
\begin{pmatrix} 
v \\ u \end{pmatrix} =\lambda \begin{pmatrix} 
v \\ u \end{pmatrix}
 
\left\{
  \begin{array}{l}
    Av+Bu=\lambda v \\
    Bv-Au=\lambda u 
  \end{array}.
\right.
 B AB=BA 
(A^2+B^2)u=\lambda^2u.
 \lambda^2 A^2+B^2","['matrices', 'eigenvalues-eigenvectors', 'block-matrices']"
95,"Calculating the index of a subgroup of ${\rm PSL}(2,\mathbb{Z})$",Calculating the index of a subgroup of,"{\rm PSL}(2,\mathbb{Z})","Suppose I have a finitely generated subgroup of $G:={\rm PSL}(2,\mathbb{Z})$ defined by $H=\langle A,B,C\rangle$ where $$A = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix},\quad B = \begin{bmatrix} -1 & 0 \\ 3 & -1 \end{bmatrix},\quad C = \begin{bmatrix} -5 & 4 \\ 6 & -5 \end{bmatrix}.$$ I have reason to believe this subgroup has finite index in $G$ , and I'm trying to calculate it. Following a suggestion the top answer of this MO post , I first tried calculating the intersection of $H$ with $P\Gamma(2)$ , where $P\Gamma(2)$ are those matrices which are congruent to the identity matrix when we reduce elements mod $2$ . After some work I was able to show that $A^2=-I, B^2, C, (AB)^3, (BA)^3\in H\cap P\Gamma(2)$ . I think(?) these and their inverses are the smallest length words with this property. Does that mean $H\cap P\Gamma(2) = \langle B^2, C, (AB)^3, (BA)^3\rangle$ ? I'm not sure what else to do to find the intersection. After that, the answer seems to suggest rewriting the generators of the intersection in terms of the standard generators of $P\Gamma(2)$ , say $$x = \begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix}, \quad y = \begin{bmatrix} 1 & 0\\ -2 & 1 \end{bmatrix}.$$ It was indicated in the MO post that GAP could somehow rewrite the intersection in terms of these generators, and this would somehow be useful information. There was also a comment suggesting the Nielson-Schreier Index Formula could be used. Can someone spell out this argument in greater detail for me? Won't we just fine that the intersection is rank 4 and has index 3 in $P\Gamma(2)$ ? How does one find the index of $H$ in the entirety of ${\rm PSL}(2,\mathbb{Z})$ from there? I feel like I am missing something. One last note: I am aware that one can use Fuchsian group methods to calculate the index as well, but I am more interested in understanding the method proposed in the MO thread (or some other algebraic method, if anyone has something better).","Suppose I have a finitely generated subgroup of defined by where I have reason to believe this subgroup has finite index in , and I'm trying to calculate it. Following a suggestion the top answer of this MO post , I first tried calculating the intersection of with , where are those matrices which are congruent to the identity matrix when we reduce elements mod . After some work I was able to show that . I think(?) these and their inverses are the smallest length words with this property. Does that mean ? I'm not sure what else to do to find the intersection. After that, the answer seems to suggest rewriting the generators of the intersection in terms of the standard generators of , say It was indicated in the MO post that GAP could somehow rewrite the intersection in terms of these generators, and this would somehow be useful information. There was also a comment suggesting the Nielson-Schreier Index Formula could be used. Can someone spell out this argument in greater detail for me? Won't we just fine that the intersection is rank 4 and has index 3 in ? How does one find the index of in the entirety of from there? I feel like I am missing something. One last note: I am aware that one can use Fuchsian group methods to calculate the index as well, but I am more interested in understanding the method proposed in the MO thread (or some other algebraic method, if anyone has something better).","G:={\rm PSL}(2,\mathbb{Z}) H=\langle A,B,C\rangle A = \begin{bmatrix}
0 & -1 \\
1 & 0
\end{bmatrix},\quad
B = \begin{bmatrix}
-1 & 0 \\
3 & -1
\end{bmatrix},\quad
C = \begin{bmatrix}
-5 & 4 \\
6 & -5
\end{bmatrix}. G H P\Gamma(2) P\Gamma(2) 2 A^2=-I, B^2, C, (AB)^3, (BA)^3\in H\cap P\Gamma(2) H\cap P\Gamma(2) = \langle B^2, C, (AB)^3, (BA)^3\rangle P\Gamma(2) x = \begin{bmatrix}
1 & 2 \\
0 & 1
\end{bmatrix}, \quad
y = \begin{bmatrix}
1 & 0\\
-2 & 1
\end{bmatrix}. P\Gamma(2) H {\rm PSL}(2,\mathbb{Z})","['abstract-algebra', 'matrices', 'group-theory', 'modular-forms']"
96,"An example of a non-diagonalisable matrix in $\mathrm{SL}(n, \mathbb{Z})$ whose Jordan blocks don't have determinant $1$",An example of a non-diagonalisable matrix in  whose Jordan blocks don't have determinant,"\mathrm{SL}(n, \mathbb{Z}) 1","Does there exists a matrix $M \in \mathrm{SL}(n, \mathbb{Z})$ , such that: $M$ is not diagonalisable; when we put $M$ in its Jordan normal form, none of the Jordan blocks have a determinant with an absolute value of $1$ ? Where I got this question from : This is a follow-up question to the one I ask here . Thank you to those who provided the examples. So far, the examples are all made up of submatrices with determinant $1$ . I was wondering if there are matrices that are not of this form. So maybe a matrix with its Jordan normal form looks like this, with $|\lambda_1|^2 \ne 1$ and $|\lambda_2|^3 \ne 1$ . $$ M = \left( \begin{array}{ccc} \lambda_1  & 1  & 0 & 0& 0\\  0 &  \lambda_1 & 0 & 0& 0 \\  0 & 0 & \lambda_2 & 1& 0 \\  0 &  0 & 0 & \lambda_2& 1 \\  0 &  0 & 0 & 0& \lambda_2 \\ \end{array} \right). $$","Does there exists a matrix , such that: is not diagonalisable; when we put in its Jordan normal form, none of the Jordan blocks have a determinant with an absolute value of ? Where I got this question from : This is a follow-up question to the one I ask here . Thank you to those who provided the examples. So far, the examples are all made up of submatrices with determinant . I was wondering if there are matrices that are not of this form. So maybe a matrix with its Jordan normal form looks like this, with and .","M \in \mathrm{SL}(n, \mathbb{Z}) M M 1 1 |\lambda_1|^2 \ne 1 |\lambda_2|^3 \ne 1 
M =
\left(
\begin{array}{ccc}
\lambda_1  & 1  & 0 & 0& 0\\
 0 &  \lambda_1 & 0 & 0& 0 \\
 0 & 0 & \lambda_2 & 1& 0 \\
 0 &  0 & 0 & \lambda_2& 1 \\
 0 &  0 & 0 & 0& \lambda_2 \\
\end{array}
\right).
","['linear-algebra', 'abstract-algebra', 'matrices', 'group-theory', 'eigenvalues-eigenvectors']"
97,Polar decomposition of a linear combination of unitary matrices,Polar decomposition of a linear combination of unitary matrices,,"Consider a complex-valued square matrix $M$ of the form $$M = \frac{1}{2}\left(U_1 + e^{-i\phi}U_2\right),$$ where $U_1$ and $U_2$ are unitary matrices and $\phi$ is a real number. Moreover, consider the polar decomposition of $M$ : $$M = U P,$$ where $U$ is a unitary matrix and where $P$ is a positive semi-definite Hermitian matrix. I want to know if the matrices $U$ and $P$ can be directly expressed in closed form as a function of $U_1$ , $U_2$ , and $\phi$ . Is that possible? I somehow doubt it, but I thought I'd ask. We can assume that $M$ is nonsingular for simplicity. EDIT: Shortly after posting this question, I discovered that there are at least expressions for the polar decomposition of 2 by 2 real matrices and of 2 by 2 complex matrices . Though I'm still wondering if something more can be said for the structure above in any dimension.","Consider a complex-valued square matrix of the form where and are unitary matrices and is a real number. Moreover, consider the polar decomposition of : where is a unitary matrix and where is a positive semi-definite Hermitian matrix. I want to know if the matrices and can be directly expressed in closed form as a function of , , and . Is that possible? I somehow doubt it, but I thought I'd ask. We can assume that is nonsingular for simplicity. EDIT: Shortly after posting this question, I discovered that there are at least expressions for the polar decomposition of 2 by 2 real matrices and of 2 by 2 complex matrices . Though I'm still wondering if something more can be said for the structure above in any dimension.","M M = \frac{1}{2}\left(U_1 + e^{-i\phi}U_2\right), U_1 U_2 \phi M M = U P, U P U P U_1 U_2 \phi M","['linear-algebra', 'matrices', 'svd', 'positive-semidefinite', 'unitary-matrices']"
98,Prove that there exists $a\in \mathbb{C}$ such as $B=(1-a)A+aC$,Prove that there exists  such as,a\in \mathbb{C} B=(1-a)A+aC,"Let A,B and C be three distinct matrices $A_{2 \times 2},B_{2 \times 2},C_{2 \times 2}$ , with their traces equal to one another $TrA=TrB=TrC$ . Knowing that $AB+BC+CA=BA+CB+AC$ , prove thtat there exists $a\in \mathbb{C}$ such as $B=(1-a)A+aC$ . I tried applying Hamilton Caylay $A^{2}-Tr(A)A+detAI_{2}=O_{2}$ , the characteristic polynomial $det(xI_{2}-A)=x^{2}-Tr(A)x+detA$ and also $det(A+xB)=detA+(Tr(A)Tr(B)-Tr(AB))x+detBx^{2}$ . Moreover, I attempted to rearange $AB-BA+BC-CB+CA-AC=O_{2}$ and $B-A=a(C-A)$ . I think I am close but I can't find how to prove it. Can you give me some help? Thank you","Let A,B and C be three distinct matrices , with their traces equal to one another . Knowing that , prove thtat there exists such as . I tried applying Hamilton Caylay , the characteristic polynomial and also . Moreover, I attempted to rearange and . I think I am close but I can't find how to prove it. Can you give me some help? Thank you","A_{2 \times 2},B_{2 \times 2},C_{2 \times 2} TrA=TrB=TrC AB+BC+CA=BA+CB+AC a\in \mathbb{C} B=(1-a)A+aC A^{2}-Tr(A)A+detAI_{2}=O_{2} det(xI_{2}-A)=x^{2}-Tr(A)x+detA det(A+xB)=detA+(Tr(A)Tr(B)-Tr(AB))x+detBx^{2} AB-BA+BC-CB+CA-AC=O_{2} B-A=a(C-A)","['matrices', 'determinant']"
99,Bounding spectral radius of special matrix,Bounding spectral radius of special matrix,,"Let $A$ be an $n \times n$ matrix with all nonnegative entries and row sums strictly less than one, let $V$ be an $n \times n$ nonnegative diagonal matrix satisfying $V \leq I$ (entrywise), let $B\equiv\left(I-AV\right)^{-1}$ and finally let $X$ be a vector in the $n$ -dimensional simplex, i.e., $x_j \geq 0,\sum_j^n x_j=1$ . Consider the matrix $$M \equiv \left(\mathrm{diag}\left\{ B^{T}X\right\} \right)^{-1}B^{T}\left[ V\mathrm{diag}\left\{ X\right\} + (I-V) \mathrm{diag}\left\{ B^{T}X\right\} \right]B\mathrm{diag}(\iota-A \iota),$$ where $\mathrm{diag}\left(u\right)$ is the diagonal matrix formed from vector $u$ and $\iota$ is the vector of all ones. I want to show that the spectral radius of $M$ is (weakly) lower than one, $\rho(M)\leq 1$ . Two simple cases are illustrative. First, if $V = I $ then $M\iota = \iota$ and so $\rho(M)=1$ . Second, if $A$ is diagonal then $M$ would be diagonal and so we would just need to show that each diagonal element is lower than one. But each of diagonal element of $M$ would be of the form $$\left(v+\frac{1-v}{1-av}\right)\frac{1-a}{1-av},$$ which is readily shown to be lower than one. The problem above, namely showing that $\rho(M)\leq 1$ , comes from a more general problem, which I ultimately need to solve. Let $D_1$ and $D_2$ be two strictly positive diagonal $n \times n$ matrices and let $$\tilde{M}\equiv\left(\mathrm{diag}\left\{ B^{T}X\right\} \right)^{-1}B^{T}\left[V\mathrm{diag}\left\{ X\right\} +\left(I-V\right)\mathrm{diag}\left\{ B^{T}X\right\} \right]D_{1}B\mathrm{diag}\left(\iota-A\iota\right)D_{2}.$$ I want to show that $\rho(\tilde{M})\leq 1$ provided that $$ \tag{*} D_{1}\left(I-A\right)^{-1}\mathrm{diag}\left(\iota-A\iota\right)D_{2}\iota\leq\iota.$$ This is now posted as a separate question here: Bounding spectral radius of special matrix (extension) The simpler question stated above obtains from this more general question in the special case in which $D_{k}=d_{k}I,k=1,2$ with $d_1,d_2$ being positive scalars. In that case $$\tilde{M}=\left(\mathcal{\mathrm{diag}}\left\{ B^{T}X\right\} \right)^{-1}B^{T}\left[V\mathcal{\mathrm{diag}}\left\{ X\right\} +\left(I-V\right)\mathcal{\mathrm{diag}}\left\{ B^{T}X\right\} \right]B\mathrm{diag}\left(\iota-A\iota\right)d_{1}d_{2},$$ while condition (*) simply becomes $d_{1}d_{2} \leq 1$ , and so we can simply prove that $\rho(M)\leq 1$ .","Let be an matrix with all nonnegative entries and row sums strictly less than one, let be an nonnegative diagonal matrix satisfying (entrywise), let and finally let be a vector in the -dimensional simplex, i.e., . Consider the matrix where is the diagonal matrix formed from vector and is the vector of all ones. I want to show that the spectral radius of is (weakly) lower than one, . Two simple cases are illustrative. First, if then and so . Second, if is diagonal then would be diagonal and so we would just need to show that each diagonal element is lower than one. But each of diagonal element of would be of the form which is readily shown to be lower than one. The problem above, namely showing that , comes from a more general problem, which I ultimately need to solve. Let and be two strictly positive diagonal matrices and let I want to show that provided that This is now posted as a separate question here: Bounding spectral radius of special matrix (extension) The simpler question stated above obtains from this more general question in the special case in which with being positive scalars. In that case while condition (*) simply becomes , and so we can simply prove that .","A n \times n V n \times n V \leq I B\equiv\left(I-AV\right)^{-1} X n x_j \geq 0,\sum_j^n x_j=1 M \equiv \left(\mathrm{diag}\left\{ B^{T}X\right\} \right)^{-1}B^{T}\left[ V\mathrm{diag}\left\{ X\right\} + (I-V) \mathrm{diag}\left\{ B^{T}X\right\} \right]B\mathrm{diag}(\iota-A \iota), \mathrm{diag}\left(u\right) u \iota M \rho(M)\leq 1 V = I  M\iota = \iota \rho(M)=1 A M M \left(v+\frac{1-v}{1-av}\right)\frac{1-a}{1-av}, \rho(M)\leq 1 D_1 D_2 n \times n \tilde{M}\equiv\left(\mathrm{diag}\left\{ B^{T}X\right\} \right)^{-1}B^{T}\left[V\mathrm{diag}\left\{ X\right\} +\left(I-V\right)\mathrm{diag}\left\{ B^{T}X\right\} \right]D_{1}B\mathrm{diag}\left(\iota-A\iota\right)D_{2}. \rho(\tilde{M})\leq 1  \tag{*} D_{1}\left(I-A\right)^{-1}\mathrm{diag}\left(\iota-A\iota\right)D_{2}\iota\leq\iota. D_{k}=d_{k}I,k=1,2 d_1,d_2 \tilde{M}=\left(\mathcal{\mathrm{diag}}\left\{ B^{T}X\right\} \right)^{-1}B^{T}\left[V\mathcal{\mathrm{diag}}\left\{ X\right\} +\left(I-V\right)\mathcal{\mathrm{diag}}\left\{ B^{T}X\right\} \right]B\mathrm{diag}\left(\iota-A\iota\right)d_{1}d_{2}, d_{1}d_{2} \leq 1 \rho(M)\leq 1","['linear-algebra', 'matrices', 'inequality', 'upper-lower-bounds', 'spectral-radius']"
