,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"$g(\theta):= f(\theta {\bf y} + (1 − \theta){\bf x})$, find $g'$.",", find .",g(\theta):= f(\theta {\bf y} + (1 − \theta){\bf x}) g',"Let $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ and let  $g: \mathbb{R} \longrightarrow \mathbb{R}$ given by $g(\theta):= f(\theta {\bf y} + (1 − \theta){\bf x})$. I want to calculate the derivative $g'$. I set $z=\theta {\bf y}+ (1 − \theta){\bf x}$, then: $$ \frac{\mathrm{d}g}{\mathrm{d}\theta}= \frac{\mathrm{d}f}{\mathrm{d}\theta}= \frac{\mathrm{d}f}{\mathrm{d}z} \frac{\mathrm{d}z}{\mathrm{d}\theta}= \nabla f^\mathsf{T} ({\bf y}-{\bf x}) $$ Is this formally correct?  If it is not wrong, is it possible to write it better?","Let $f: \mathbb{R}^n \longrightarrow \mathbb{R}$ and let  $g: \mathbb{R} \longrightarrow \mathbb{R}$ given by $g(\theta):= f(\theta {\bf y} + (1 − \theta){\bf x})$. I want to calculate the derivative $g'$. I set $z=\theta {\bf y}+ (1 − \theta){\bf x}$, then: $$ \frac{\mathrm{d}g}{\mathrm{d}\theta}= \frac{\mathrm{d}f}{\mathrm{d}\theta}= \frac{\mathrm{d}f}{\mathrm{d}z} \frac{\mathrm{d}z}{\mathrm{d}\theta}= \nabla f^\mathsf{T} ({\bf y}-{\bf x}) $$ Is this formally correct?  If it is not wrong, is it possible to write it better?",,"['multivariable-calculus', 'derivatives', 'function-and-relation-composition']"
1,On the size of two integrals,On the size of two integrals,,"Let $A=\int_0^1 x^x dx, B= \int_0^1 \int_0^1 (xy)^{xy} dxdy$, which of the following is true: $A>B, A=B$ or $A<B$.","Let $A=\int_0^1 x^x dx, B= \int_0^1 \int_0^1 (xy)^{xy} dxdy$, which of the following is true: $A>B, A=B$ or $A<B$.",,"['real-analysis', 'multivariable-calculus']"
2,Show that dB/dt . B = 0,Show that dB/dt . B = 0,,"B is a binormal vector where B = T x N. T is the unit tangent and N is the principal normal vector. Basically, the derivative of the binormal vector is perpendicular to both the unit tangent and the binormal vector. I expanded everything out using B = T x N and the cross product differentiation rule but I ended up with (N' x T) . (T x N).","B is a binormal vector where B = T x N. T is the unit tangent and N is the principal normal vector. Basically, the derivative of the binormal vector is perpendicular to both the unit tangent and the binormal vector. I expanded everything out using B = T x N and the cross product differentiation rule but I ended up with (N' x T) . (T x N).",,['multivariable-calculus']
3,volume evaluated by triple integral,volume evaluated by triple integral,,"Let $\Omega:=\{(x,y,z)|x^2+y^2=1, 0\leq z \leq 2\}$, fix an $\alpha \in (-\frac{\pi}{2},\frac{\pi}{2})$ and given the transoformation $T(x,y,z):=(x,y+z\tan \alpha,z)$, find the volume of $T(\Omega)$. Remark: By the coordinate transformation formula for multiple integrals one can compute the corresponding Jacobian to be $1$, thus the volume of $T(\Omega)$ is the same as the volume of $\Omega$, am I right? Also, geometrically, the transformation only shifts the $xy$-cross section (at each $z$ level) to an extent (along the $y$ axis), thus the area of each cross section of the cylinder $\Omega$ remains unchanged after tranformation, so the volume of $T(\Omega)$ is equal to the one of $\Omega$. Is my explaination correct?","Let $\Omega:=\{(x,y,z)|x^2+y^2=1, 0\leq z \leq 2\}$, fix an $\alpha \in (-\frac{\pi}{2},\frac{\pi}{2})$ and given the transoformation $T(x,y,z):=(x,y+z\tan \alpha,z)$, find the volume of $T(\Omega)$. Remark: By the coordinate transformation formula for multiple integrals one can compute the corresponding Jacobian to be $1$, thus the volume of $T(\Omega)$ is the same as the volume of $\Omega$, am I right? Also, geometrically, the transformation only shifts the $xy$-cross section (at each $z$ level) to an extent (along the $y$ axis), thus the area of each cross section of the cylinder $\Omega$ remains unchanged after tranformation, so the volume of $T(\Omega)$ is equal to the one of $\Omega$. Is my explaination correct?",,"['integration', 'multivariable-calculus']"
4,parametric integration,parametric integration,,"Is there a mistake in the bottom of page 5 of this document? INTEGRATION: THE FEYNMAN WAY $$\frac{\partial}{\partial b} e^{be^{ix}}= e^{ix}e^{be^{ix}}$$ instead of $ib e^{be^{ix}}e^{ix}$? thank you very much. Also, i want to ask if there is any general rule to choose the parameter?","Is there a mistake in the bottom of page 5 of this document? INTEGRATION: THE FEYNMAN WAY $$\frac{\partial}{\partial b} e^{be^{ix}}= e^{ix}e^{be^{ix}}$$ instead of $ib e^{be^{ix}}e^{ix}$? thank you very much. Also, i want to ask if there is any general rule to choose the parameter?",,"['calculus', 'multivariable-calculus', 'integration']"
5,Integral from $0$ to Infinity of $e^{-3x^2}$? [duplicate],Integral from  to Infinity of ? [duplicate],0 e^{-3x^2},This question already has answers here : Evaluation of Gaussian integral $\int_{0}^{\infty} \mathrm{e}^{-x^2} dx$ (21 answers) Closed 11 years ago . How do you calculate the integral from $0$ to Infinity of $e^{-3x^2}$? I am supposed to use a double integral. Can someone please explain? Thanks in advance.,This question already has answers here : Evaluation of Gaussian integral $\int_{0}^{\infty} \mathrm{e}^{-x^2} dx$ (21 answers) Closed 11 years ago . How do you calculate the integral from $0$ to Infinity of $e^{-3x^2}$? I am supposed to use a double integral. Can someone please explain? Thanks in advance.,,"['integration', 'multivariable-calculus']"
6,Marginal Probability Density: Integrand Values,Marginal Probability Density: Integrand Values,,"I have a joint probability density function, $f(x,y)$. However, I have a constraint associated: $0< x < y < +\infty$. So, when I calculate the marginal probability densities, how do I factor in the constraints to the integrands for both $F_x$ and $F_y$? Should I draw out the shaded regions in $\mathbb R^2$? The function in question is $3e^{-2x-y}$. I can integrate - that's child's play.","I have a joint probability density function, $f(x,y)$. However, I have a constraint associated: $0< x < y < +\infty$. So, when I calculate the marginal probability densities, how do I factor in the constraints to the integrands for both $F_x$ and $F_y$? Should I draw out the shaded regions in $\mathbb R^2$? The function in question is $3e^{-2x-y}$. I can integrate - that's child's play.",,"['probability', 'multivariable-calculus', 'probability-distributions']"
7,Diagonals of parallelogram intersect at $90^\circ$ if and only if figure is rhombus,Diagonals of parallelogram intersect at  if and only if figure is rhombus,90^\circ,"How can we use vectors and dot products to show that the diagonals of a parallelogram intersect at $90^\circ$ if and only if the figure is a rhombus? I did the proof, but I realized my final answer would be a rectangle. (I know a rhombus is a type of rectangle, too). But I only want to prove the two diagonals are orthogonal.","How can we use vectors and dot products to show that the diagonals of a parallelogram intersect at $90^\circ$ if and only if the figure is a rhombus? I did the proof, but I realized my final answer would be a rectangle. (I know a rhombus is a type of rectangle, too). But I only want to prove the two diagonals are orthogonal.",,"['geometry', 'multivariable-calculus']"
8,Showing a function is differentiable using definition of derivative,Showing a function is differentiable using definition of derivative,,"Let $f: \mathbb{R}^2 \to \mathbb{R}^2$ be defined by $f(x,y)=(x^2-xy, x+y^2)$.  Use the definition of the derivative of a function to show that $f$ is differentiable at the point $p=(1,-1)$. My attempt is as follows: Definition of derivative: $\lim\limits_{x \to p}\frac{|f(x)-f(p)-df_p(x-p)|}{|x-p|}$.  So, $$\lim\limits_{(x,y)\to(1,-1)}\frac{|f((x,y))-f((1,-1))-df|_{(1,-1)}((x,y)-(1,-1)}{|(x,y)-(1,-1)|}.$$ Where $df|_{(1,-1)}=\begin{bmatrix}                      3 & -1 \\                      1 & -2                      \end{bmatrix}.$ Then, we have: $$\lim\limits_{(x,y) \to (1,-1)} \frac{|<x^2-xy, x+y^2>-<2,2>-<3x-y-4, x-2y-3>|}{|<x-1, y+1>|}$$ Simplified a bit: $$\lim\limits_{(x,y) \to (1,-1)} \frac{|<x^2-xy-3x+y+2, x+y^2-x+2y+1>|}{|<x-1, y+1>|} $$ $$\lim\limits_{(x,y) \to (1,-1)} \frac{\sqrt{(x^2-xy-3x+y+2)^2+(x+y^2-x+2y+1)^2}}{\sqrt{(x-1)^2+(y+1)^2}}$$ I still noticed that the bottom goes to zero.  Is there something that I am messing up computationally?","Let $f: \mathbb{R}^2 \to \mathbb{R}^2$ be defined by $f(x,y)=(x^2-xy, x+y^2)$.  Use the definition of the derivative of a function to show that $f$ is differentiable at the point $p=(1,-1)$. My attempt is as follows: Definition of derivative: $\lim\limits_{x \to p}\frac{|f(x)-f(p)-df_p(x-p)|}{|x-p|}$.  So, $$\lim\limits_{(x,y)\to(1,-1)}\frac{|f((x,y))-f((1,-1))-df|_{(1,-1)}((x,y)-(1,-1)}{|(x,y)-(1,-1)|}.$$ Where $df|_{(1,-1)}=\begin{bmatrix}                      3 & -1 \\                      1 & -2                      \end{bmatrix}.$ Then, we have: $$\lim\limits_{(x,y) \to (1,-1)} \frac{|<x^2-xy, x+y^2>-<2,2>-<3x-y-4, x-2y-3>|}{|<x-1, y+1>|}$$ Simplified a bit: $$\lim\limits_{(x,y) \to (1,-1)} \frac{|<x^2-xy-3x+y+2, x+y^2-x+2y+1>|}{|<x-1, y+1>|} $$ $$\lim\limits_{(x,y) \to (1,-1)} \frac{\sqrt{(x^2-xy-3x+y+2)^2+(x+y^2-x+2y+1)^2}}{\sqrt{(x-1)^2+(y+1)^2}}$$ I still noticed that the bottom goes to zero.  Is there something that I am messing up computationally?",,"['analysis', 'multivariable-calculus']"
9,How do I find $\frac{dy}{dx}$?,How do I find ?,\frac{dy}{dx},"I am given $y = (x_1,..., x_N)$ and $F  (y, x_1,..., x_N) = 0$ . I need to find $\frac{dy}{dx}$ in terms of the partials of $F$ . I have no idea at all after a lot of playing with it.",I am given and . I need to find in terms of the partials of . I have no idea at all after a lot of playing with it.,"y = (x_1,..., x_N) F  (y, x_1,..., x_N) = 0 \frac{dy}{dx} F","['multivariable-calculus', 'implicit-differentiation']"
10,Determining the Existence of Global Minimum/Maximum,Determining the Existence of Global Minimum/Maximum,,"Determine whether the function defined as $$f(x,y,z)=x+y+z$$ has a maximum or a minimum value on the set $xy+yz=1$, $xz+yz=4$, $x>0$, $y>0$, $z>0$. It is clear to me that it does have a minimum value as we have that  $f>{0}$; and calculations reveal that; $$f(\sqrt{3},2-\sqrt{3},2)=4$$ is a minimum value. Now, I suspect $f$ does not have a maximum value but I'm unsure how to properly show this.","Determine whether the function defined as $$f(x,y,z)=x+y+z$$ has a maximum or a minimum value on the set $xy+yz=1$, $xz+yz=4$, $x>0$, $y>0$, $z>0$. It is clear to me that it does have a minimum value as we have that  $f>{0}$; and calculations reveal that; $$f(\sqrt{3},2-\sqrt{3},2)=4$$ is a minimum value. Now, I suspect $f$ does not have a maximum value but I'm unsure how to properly show this.",,"['multivariable-calculus', 'optimization']"
11,Surface area of sphere with cylindrical cutout,Surface area of sphere with cylindrical cutout,,"Compute the total surface area of the remaining part of a solid ball of radius  11  after a cylindrical hole of radius  8  is drilled through the center of the ball. Change to cylindrical coordinates: $x^2+y^2+z^2=11^2$ , thus $z=\sqrt{121-x^2-y^2}=\sqrt{121-r^2}$ $SA=\int\int_R \sqrt{1+(\partial{z}/\partial{x})^2+(\partial{z}/\partial{y})^2}dA$ Here $(\partial{z}/\partial{x})=(121-x^2-y^2)^{-1/2}(-2x)=(-2x)(121-r^2)^{-1/2}$ and $(\partial{z}/\partial{y})=(121-x^2-y^2)^{-1/2}(-2y)=(-2y)(121-r^2)^{-1/2}$ $(\partial{z}/\partial{x})^2+(\partial{z}/\partial{y})^2=(4r^2)/(121-r^2)$ $SA=\int_0^{2\pi}\int_8^{11}\sqrt{1+(\partial{z}/\partial{x})^2+(\partial{z}/\partial{y})^2}rdrd\theta$ $=\int_0^{2\pi}\int_8^{11}\sqrt{1+(4r^2)/(121-r^2)}rdrd\theta$ which is some trig integral that I would rather not do, so at this point I passed it off to Wolfram Alpha's double integral widget http://www.wolframalpha.com/widget/widgetPopup.jsp?p=v&id=f45d619ba0a669dc4191493d49ac4898&title=Double+Integral+Calculator&theme=blue using $x$ as $r$ and $y$ as $\theta$, and using the following equation for $f(x,y)$: x(1+4x^2/(121-x^2))^(1/2) , using the ""dxdy"" method, $x$ going from 8 to 11, $y$ going from 0 to 2*pi. This gives me the answer 978.405, which of course is just the top ($z$-positive) half of the surface, so multiply by 2 to get 1956.81. But, homework says answer isn't right. It all looks right to me, where did I go wrong?","Compute the total surface area of the remaining part of a solid ball of radius  11  after a cylindrical hole of radius  8  is drilled through the center of the ball. Change to cylindrical coordinates: $x^2+y^2+z^2=11^2$ , thus $z=\sqrt{121-x^2-y^2}=\sqrt{121-r^2}$ $SA=\int\int_R \sqrt{1+(\partial{z}/\partial{x})^2+(\partial{z}/\partial{y})^2}dA$ Here $(\partial{z}/\partial{x})=(121-x^2-y^2)^{-1/2}(-2x)=(-2x)(121-r^2)^{-1/2}$ and $(\partial{z}/\partial{y})=(121-x^2-y^2)^{-1/2}(-2y)=(-2y)(121-r^2)^{-1/2}$ $(\partial{z}/\partial{x})^2+(\partial{z}/\partial{y})^2=(4r^2)/(121-r^2)$ $SA=\int_0^{2\pi}\int_8^{11}\sqrt{1+(\partial{z}/\partial{x})^2+(\partial{z}/\partial{y})^2}rdrd\theta$ $=\int_0^{2\pi}\int_8^{11}\sqrt{1+(4r^2)/(121-r^2)}rdrd\theta$ which is some trig integral that I would rather not do, so at this point I passed it off to Wolfram Alpha's double integral widget http://www.wolframalpha.com/widget/widgetPopup.jsp?p=v&id=f45d619ba0a669dc4191493d49ac4898&title=Double+Integral+Calculator&theme=blue using $x$ as $r$ and $y$ as $\theta$, and using the following equation for $f(x,y)$: x(1+4x^2/(121-x^2))^(1/2) , using the ""dxdy"" method, $x$ going from 8 to 11, $y$ going from 0 to 2*pi. This gives me the answer 978.405, which of course is just the top ($z$-positive) half of the surface, so multiply by 2 to get 1956.81. But, homework says answer isn't right. It all looks right to me, where did I go wrong?",,['multivariable-calculus']
12,compute line integral where line is boundary of $(x-5)^2 + (y-2)^2 \le 1$,compute line integral where line is boundary of,(x-5)^2 + (y-2)^2 \le 1,"Line integral of $$\frac{-y^3}{(x^2+y^2)^2} dx + \frac{xy^2}{(x^2+y^2)^2} dy$$ Using greene's theorem, this integral is equal to zero. Am i right? $P(x,y)$ and $Q(x,y)$ are not continuous at $(0,0)$. Since the circle does not contain $(0,0)$, green's theorem applies?","Line integral of $$\frac{-y^3}{(x^2+y^2)^2} dx + \frac{xy^2}{(x^2+y^2)^2} dy$$ Using greene's theorem, this integral is equal to zero. Am i right? $P(x,y)$ and $Q(x,y)$ are not continuous at $(0,0)$. Since the circle does not contain $(0,0)$, green's theorem applies?",,['multivariable-calculus']
13,Divergence Theorem to determine the flux,Divergence Theorem to determine the flux,,"Please how can I determine the flux of the vector field  $$F=(x+x^2 y){\bf e_x}+ (x y^2-y) {\bf e_y}$$ through the boundary which is formed by the the hyperbola $x^2 -81y^2=9$ and the lines $y=-2$ and $y=1$ I did the div of the vector field , and it is $$4xy$$ but now what to do , what is dv ? and also by which formulas should I represent the x and y? Thank you very much","Please how can I determine the flux of the vector field  $$F=(x+x^2 y){\bf e_x}+ (x y^2-y) {\bf e_y}$$ through the boundary which is formed by the the hyperbola $x^2 -81y^2=9$ and the lines $y=-2$ and $y=1$ I did the div of the vector field , and it is $$4xy$$ but now what to do , what is dv ? and also by which formulas should I represent the x and y? Thank you very much",,"['calculus', 'multivariable-calculus']"
14,What does this limit represent for this function?,What does this limit represent for this function?,,"Let $f: \mathbb{R}^2 \to \mathbb{R}^2$, suppose we have the limit $$\lim_{(x,y)\to(a,b)} \dfrac{|f(x,y) - f(a,b)|}{\sqrt{(x-a)^2+(y-b)^2}}$$ The absolute value bars on the top is optional, but I just wanted to make everything positive. What happens if the limit is equal to $1$? It looks as if the limit is a representation of some sort of derivative. Since both $x$ and $y$ seem to be both varying, I cannot tell which, perhaps it is some sort of gradient function? I am sure. Note that $f(x,y) = (f_1(x,y),f_2(x,y))$; $f(a,b) = (f_1(a,b),f_2(a,b))$. So the absolute value bars transform my limit to $$\lim_{(x,y)\to(a,b)} \dfrac{|f_1(x,y) - f_1(a,b)|}{\sqrt{(x-a)^2+(y-b)^2}} + \lim_{(x,y)\to(a,b)} \dfrac{|f_2(x,y) - f_2(a,b)|}{\sqrt{(x-a)^2+(y-b)^2}}$$","Let $f: \mathbb{R}^2 \to \mathbb{R}^2$, suppose we have the limit $$\lim_{(x,y)\to(a,b)} \dfrac{|f(x,y) - f(a,b)|}{\sqrt{(x-a)^2+(y-b)^2}}$$ The absolute value bars on the top is optional, but I just wanted to make everything positive. What happens if the limit is equal to $1$? It looks as if the limit is a representation of some sort of derivative. Since both $x$ and $y$ seem to be both varying, I cannot tell which, perhaps it is some sort of gradient function? I am sure. Note that $f(x,y) = (f_1(x,y),f_2(x,y))$; $f(a,b) = (f_1(a,b),f_2(a,b))$. So the absolute value bars transform my limit to $$\lim_{(x,y)\to(a,b)} \dfrac{|f_1(x,y) - f_1(a,b)|}{\sqrt{(x-a)^2+(y-b)^2}} + \lim_{(x,y)\to(a,b)} \dfrac{|f_2(x,y) - f_2(a,b)|}{\sqrt{(x-a)^2+(y-b)^2}}$$",,"['limits', 'multivariable-calculus']"
15,finding $\frac{dz}{dk}$ with differentiation,finding  with differentiation,\frac{dz}{dk},"Given  $$ \begin{align*} y=f(a,b)\\ a=a(u,v) &&b=b(u,v)\\ u=u(r)&&v=v(r) \end{align*} $$ Find $\frac{dy}{dr}$? I did it with this method so could somebody assist to confirm the answer? I have decided to find $\frac{da}{dr}$ and $\frac{db}{dr}$ so $$ \begin{align*} \frac{da}{dr} &= \frac{\partial a}{\partial u}\frac{du}{dr} + \frac{\partial a}{\partial v}\frac{dv}{dr}\\\frac{db}{dr} &= \frac{\partial b}{\partial u}\frac{du}{dr} + \frac{\partial b}{\partial v}\frac{dv}{dr} \end{align*} $$ Hence $$ \begin{align*} \frac{dy}{dr} &= \frac{\partial y}{\partial a}\frac{da}{dr}+\frac{\partial y}{\partial b}\frac{db}{dr}\\ &=\frac{\partial y}{\partial a}\begin{pmatrix}\frac{\partial a}{\partial u}\frac{du}{dr}+ \frac{\partial a}{\partial v}\frac{dv}{dr}\end{pmatrix}+\frac{\partial y}{\partial b}\begin{pmatrix}\frac{\partial b}{\partial u}\frac{du}{dr} + \frac{\partial b}{\partial v}\frac{dv}{dr}\end{pmatrix}\\&= \frac{\partial y}{\partial a}\frac{\partial a}{\partial u}\frac{du}{dr} + \frac{\partial y}{\partial a}\frac{\partial a}{\partial v}\frac{dv}{dr}+\frac{\partial y}{\partial b}\frac{\partial b}{\partial u}\frac{du}{dr} + \frac{\partial y}{\partial b}\frac{\partial b}{\partial v}\frac{dv}{dr} \end{align*} $$","Given  $$ \begin{align*} y=f(a,b)\\ a=a(u,v) &&b=b(u,v)\\ u=u(r)&&v=v(r) \end{align*} $$ Find $\frac{dy}{dr}$? I did it with this method so could somebody assist to confirm the answer? I have decided to find $\frac{da}{dr}$ and $\frac{db}{dr}$ so $$ \begin{align*} \frac{da}{dr} &= \frac{\partial a}{\partial u}\frac{du}{dr} + \frac{\partial a}{\partial v}\frac{dv}{dr}\\\frac{db}{dr} &= \frac{\partial b}{\partial u}\frac{du}{dr} + \frac{\partial b}{\partial v}\frac{dv}{dr} \end{align*} $$ Hence $$ \begin{align*} \frac{dy}{dr} &= \frac{\partial y}{\partial a}\frac{da}{dr}+\frac{\partial y}{\partial b}\frac{db}{dr}\\ &=\frac{\partial y}{\partial a}\begin{pmatrix}\frac{\partial a}{\partial u}\frac{du}{dr}+ \frac{\partial a}{\partial v}\frac{dv}{dr}\end{pmatrix}+\frac{\partial y}{\partial b}\begin{pmatrix}\frac{\partial b}{\partial u}\frac{du}{dr} + \frac{\partial b}{\partial v}\frac{dv}{dr}\end{pmatrix}\\&= \frac{\partial y}{\partial a}\frac{\partial a}{\partial u}\frac{du}{dr} + \frac{\partial y}{\partial a}\frac{\partial a}{\partial v}\frac{dv}{dr}+\frac{\partial y}{\partial b}\frac{\partial b}{\partial u}\frac{du}{dr} + \frac{\partial y}{\partial b}\frac{\partial b}{\partial v}\frac{dv}{dr} \end{align*} $$",,['multivariable-calculus']
16,Triple Integration,Triple Integration,,"The problem is as follows: Compute the intergal  $$I= \iiint_B \frac{x^4+2y^4}{x^4 +4y^4 +z^4 }\text{d}x\,\text{d}y\,\text{d}z $$  where $B$  is the unit ball defined by $B=$  {$(x,y,z)∣x^ 2 +y^ 2 +z^ 2 \leq 1$} . The official solution is tricky: The change of variable $(x,y,z) \mapsto (z,y,x)$  transforms the integral into  $$ I= \iiint_B \frac{z^4+2y^4}{x^4 +4y^4 +z^4 }\text{d}x\,\text{d}y\,\text{d}z \qquad(1)$$ hence $2I= \iiint_B 1\, \text{d}x\,\text{d}y\,\text{d}z = 4\pi/3$. Therefore, $I=2\pi/3$. My question is: what is meant by $(x,y,z)\mapsto (z,y,x)$ , isn't it ambiguous? Somebody have another solution saying that by symmetry, we have $I$ equals $(1)$ and adding gives the answer. I wanna ask if symmetric here means that the permutation of the variables preserves the domain $D$? Why if it is symmetric, then the two integrals are the same? The next question is to compute the integral $$J=\int_0^1 \int_0^1 \int_0^1 \cos^2(\frac{\pi}{6}(x+y+z)) \text{d}x\,\text{d}y\,\text{d}z$$ The solution uses similar technique in above: substitutes $x=1-u. y=1-v, z=1-w$, then we will get $$J=\int_1^0 \int_1^0\int_1^0  \cos^2(\frac{\pi}{2}-\frac{\pi}{6}(u+v+w)) \text{d}u\,\text{d}v\,\text{d}w$$ but why it also equals to $$\int_0^1 \int_0^1\int_0^1  \cos^2(\frac{\pi}{2}-\frac{\pi}{6}(u+v+w)) \text{d}u\,\text{d}v\,\text{d}w$$ did I do something wrong?? Thank you for answering such a dumb question!","The problem is as follows: Compute the intergal  $$I= \iiint_B \frac{x^4+2y^4}{x^4 +4y^4 +z^4 }\text{d}x\,\text{d}y\,\text{d}z $$  where $B$  is the unit ball defined by $B=$  {$(x,y,z)∣x^ 2 +y^ 2 +z^ 2 \leq 1$} . The official solution is tricky: The change of variable $(x,y,z) \mapsto (z,y,x)$  transforms the integral into  $$ I= \iiint_B \frac{z^4+2y^4}{x^4 +4y^4 +z^4 }\text{d}x\,\text{d}y\,\text{d}z \qquad(1)$$ hence $2I= \iiint_B 1\, \text{d}x\,\text{d}y\,\text{d}z = 4\pi/3$. Therefore, $I=2\pi/3$. My question is: what is meant by $(x,y,z)\mapsto (z,y,x)$ , isn't it ambiguous? Somebody have another solution saying that by symmetry, we have $I$ equals $(1)$ and adding gives the answer. I wanna ask if symmetric here means that the permutation of the variables preserves the domain $D$? Why if it is symmetric, then the two integrals are the same? The next question is to compute the integral $$J=\int_0^1 \int_0^1 \int_0^1 \cos^2(\frac{\pi}{6}(x+y+z)) \text{d}x\,\text{d}y\,\text{d}z$$ The solution uses similar technique in above: substitutes $x=1-u. y=1-v, z=1-w$, then we will get $$J=\int_1^0 \int_1^0\int_1^0  \cos^2(\frac{\pi}{2}-\frac{\pi}{6}(u+v+w)) \text{d}u\,\text{d}v\,\text{d}w$$ but why it also equals to $$\int_0^1 \int_0^1\int_0^1  \cos^2(\frac{\pi}{2}-\frac{\pi}{6}(u+v+w)) \text{d}u\,\text{d}v\,\text{d}w$$ did I do something wrong?? Thank you for answering such a dumb question!",,['multivariable-calculus']
17,how to solve this problem about the surface integral,how to solve this problem about the surface integral,,"The force per unit area acting on a sphere with radius $r$ is $F=r^2\vec r+cos\theta \vec \theta$, what is the net force on of the sphere? To solve this problem, i will use the surface integral and convert the force field in terms of standard basis in cartesian coordinate and do the surface integral but doing in this way completely ignored the meaning of spherical coordinates. My question is how to do the surface integral computating with the spherical coordinate instead of converting to the standard cartesian coordinate.","The force per unit area acting on a sphere with radius $r$ is $F=r^2\vec r+cos\theta \vec \theta$, what is the net force on of the sphere? To solve this problem, i will use the surface integral and convert the force field in terms of standard basis in cartesian coordinate and do the surface integral but doing in this way completely ignored the meaning of spherical coordinates. My question is how to do the surface integral computating with the spherical coordinate instead of converting to the standard cartesian coordinate.",,['multivariable-calculus']
18,"Obtain a formula for $\frac{dy_1}{dx}$ if $y_1$, $y_2$ are defined implicitly by:","Obtain a formula for  if ,  are defined implicitly by:",\frac{dy_1}{dx} y_1 y_2,"$G_1(x, y_1(x), y_2(x)) = 0$ $G_2(x, y_1(x), y_2(x)) = 0$ Since $G_1$ and $G_2$ are composite functions I can use the chain rule and split them up into partial derivatives. The derivative of both functions should be $0$ since they are both horizontal lines. I'm not sure what to do after that. Maybe just substitution?","$G_1(x, y_1(x), y_2(x)) = 0$ $G_2(x, y_1(x), y_2(x)) = 0$ Since $G_1$ and $G_2$ are composite functions I can use the chain rule and split them up into partial derivatives. The derivative of both functions should be $0$ since they are both horizontal lines. I'm not sure what to do after that. Maybe just substitution?",,['multivariable-calculus']
19,"Magnitude of a vector, cubed, in Einstein Summation Notation","Magnitude of a vector, cubed, in Einstein Summation Notation",,"Evaluate $\nabla \cdot (r^3v)$. The answer will be in terms of r. Where v represents the position vector and r represents the scalar magnitude of the position vector. I started by writing this in the notation, $d\over(dx_i)$$((r_jr_j)^3v)_i$ I'm not exactly sure on what ""evaluating"" will be in terms of doing it in summation notation? Since the instructions specify to evaluate, I'm assuming there's a tensor in there somewhere. Also, I am having trouble in expressing $r^3$ in summation notation. I do know that the magnitude of a vector is just $r \cdot r$, but how do i incorporate the cubed?","Evaluate $\nabla \cdot (r^3v)$. The answer will be in terms of r. Where v represents the position vector and r represents the scalar magnitude of the position vector. I started by writing this in the notation, $d\over(dx_i)$$((r_jr_j)^3v)_i$ I'm not exactly sure on what ""evaluating"" will be in terms of doing it in summation notation? Since the instructions specify to evaluate, I'm assuming there's a tensor in there somewhere. Also, I am having trouble in expressing $r^3$ in summation notation. I do know that the magnitude of a vector is just $r \cdot r$, but how do i incorporate the cubed?",,['multivariable-calculus']
20,Finding a limit with two variables,Finding a limit with two variables,,"If $(x,y)\neq (0,0)$, let $f(x,y)=(x^2-y^2)/(x^2+y^2)$. Find the limit of f(x,y) as $(x,y) \leftrightarrow (0,0)$ along the line y=mx. By replacing y=mx, I found $f(x,y)=(1-m^2)/(1+m^2)$ Is it possible to define f(0,0) so as to make f continuous at (0,0) This I didn't know Please help","If $(x,y)\neq (0,0)$, let $f(x,y)=(x^2-y^2)/(x^2+y^2)$. Find the limit of f(x,y) as $(x,y) \leftrightarrow (0,0)$ along the line y=mx. By replacing y=mx, I found $f(x,y)=(1-m^2)/(1+m^2)$ Is it possible to define f(0,0) so as to make f continuous at (0,0) This I didn't know Please help",,[]
21,Find smaller volume of solid bounded by plane and sphere,Find smaller volume of solid bounded by plane and sphere,,bounded by plane $x+y+z=1$ and $x^2 +y^2+z^2 = 1$. I need this part to solve https://math.stackexchange.com/questions/325143/change-of-variables-multiple-integrals-volume-question,bounded by plane $x+y+z=1$ and $x^2 +y^2+z^2 = 1$. I need this part to solve https://math.stackexchange.com/questions/325143/change-of-variables-multiple-integrals-volume-question,,['multivariable-calculus']
22,"Results of the multivariate function $f(x,y) = 3x^2 - 2xy + y^3 $",Results of the multivariate function,"f(x,y) = 3x^2 - 2xy + y^3 ","I was asked to derive 2 experessions for this multivariate function $$ f(x,y) = 3x^2 -2xy+y^3 $$ The first is $\large{\frac{f(x+h,y) - f(x,y)}{h}}$ and the other is $\large{\frac{f(x,y+k) - f(x,y)}{k}}$ The working is as follows: $$ \begin{align*} \frac{f(x+h,y) - f(x,y)}{h} &= \frac{3(x+h)^2 - 2y(x+h) + y^3 -(3x^2-2xy+y^3)}{h} \\&= \frac{3(x+h)^2 - 3x^2 -2y(x+h) + 2xy}{h} \\&=\frac{3(x+h+x)(x+h-x) - 2y(h)}{h} \\&=\frac{3(x+h+x)(x+h-x) - 2y(h)}{h} \\&= 3(2x+h) -2y \end{align*} $$ and $$ \begin{align*} \frac{f(x,y+k) - f(x,y)}{k} &= \frac{3x^2 - 2x(y+k) + (y+k)^3-(3x^2-2xy+y^3)}{k} \\&= \frac{-2x(y+k) +(y+k)^3 + 2xy-y^3}{k} \\&=\frac{2x(-y-k+y) + (y+k)^3 - y^3}{k} \\&=\frac{-2xk + (y+k-y)[(y+k)^2 + y(y+k) + y^2]}{k} \\&=\frac{-2xk + k[(y+k)^2 + y(y+k) + y^2]}{k} \\&= -2x + (y+k)^2 + y(y+k) + y^2 \end{align*} $$ Are the calculations correct? UPDATE Thanks to the community, I have verified the answer. For completeness, I shall continue to derive the partial derivatives using the above answers. $$ \begin{align*} \frac{\partial f}{\partial x}(x,y)=\lim_{h\to0}\frac{f(x+h,y)-f(x,y)}{h} &= \lim_{h \to 0}3(2x+h) - 2y \\&= 6x + 2y \end{align*} $$ and $$ \begin{align*} \frac{\partial f}{\partial y}(x,y)=\lim_{k\to0}\frac{f(x,y+k)-f(x,y)}{k} &= \lim_{k \to 0} -2x + (y+k)^2 + y(y+k) + y^2 \\&= -2x + y^2 + y^2 + y^2 \\&= -2x + 3y^2 \end{align*} $$","I was asked to derive 2 experessions for this multivariate function $$ f(x,y) = 3x^2 -2xy+y^3 $$ The first is $\large{\frac{f(x+h,y) - f(x,y)}{h}}$ and the other is $\large{\frac{f(x,y+k) - f(x,y)}{k}}$ The working is as follows: $$ \begin{align*} \frac{f(x+h,y) - f(x,y)}{h} &= \frac{3(x+h)^2 - 2y(x+h) + y^3 -(3x^2-2xy+y^3)}{h} \\&= \frac{3(x+h)^2 - 3x^2 -2y(x+h) + 2xy}{h} \\&=\frac{3(x+h+x)(x+h-x) - 2y(h)}{h} \\&=\frac{3(x+h+x)(x+h-x) - 2y(h)}{h} \\&= 3(2x+h) -2y \end{align*} $$ and $$ \begin{align*} \frac{f(x,y+k) - f(x,y)}{k} &= \frac{3x^2 - 2x(y+k) + (y+k)^3-(3x^2-2xy+y^3)}{k} \\&= \frac{-2x(y+k) +(y+k)^3 + 2xy-y^3}{k} \\&=\frac{2x(-y-k+y) + (y+k)^3 - y^3}{k} \\&=\frac{-2xk + (y+k-y)[(y+k)^2 + y(y+k) + y^2]}{k} \\&=\frac{-2xk + k[(y+k)^2 + y(y+k) + y^2]}{k} \\&= -2x + (y+k)^2 + y(y+k) + y^2 \end{align*} $$ Are the calculations correct? UPDATE Thanks to the community, I have verified the answer. For completeness, I shall continue to derive the partial derivatives using the above answers. $$ \begin{align*} \frac{\partial f}{\partial x}(x,y)=\lim_{h\to0}\frac{f(x+h,y)-f(x,y)}{h} &= \lim_{h \to 0}3(2x+h) - 2y \\&= 6x + 2y \end{align*} $$ and $$ \begin{align*} \frac{\partial f}{\partial y}(x,y)=\lim_{k\to0}\frac{f(x,y+k)-f(x,y)}{k} &= \lim_{k \to 0} -2x + (y+k)^2 + y(y+k) + y^2 \\&= -2x + y^2 + y^2 + y^2 \\&= -2x + 3y^2 \end{align*} $$",,"['calculus', 'multivariable-calculus']"
23,How to approximate using the differential?,How to approximate using the differential?,,"I'm working a practice problem for Calculus 3, and I'm given an equation to find the differential of: $z=3x^2+2y$ at the point $f(1,2)$ I've found the differential: $dz=6dx+2dy$ Next I'm supposed to use the differential to approximate $f(1.2,2.2)$, but I'm really not sure where to go this?","I'm working a practice problem for Calculus 3, and I'm given an equation to find the differential of: $z=3x^2+2y$ at the point $f(1,2)$ I've found the differential: $dz=6dx+2dy$ Next I'm supposed to use the differential to approximate $f(1.2,2.2)$, but I'm really not sure where to go this?",,"['calculus', 'multivariable-calculus']"
24,Calculate the volume of the solid that's above the plane $xy$ and is limited by the paraboloid $z=x^2+y^2$ and the cylinder $x^2+y^2=2y$,Calculate the volume of the solid that's above the plane  and is limited by the paraboloid  and the cylinder,xy z=x^2+y^2 x^2+y^2=2y,"I'm studying to my test of Calculus and I'm not sure about the result I got. This is how I've done it: Discover the interval of z: $$ 0 \leq z \leq x^2+y^2 $$ Setup the integral: $$ \iint_D \, \int_0^{x^2+y^2} \, dz \,\, dA $$ It equals to: $$ \iint_D x^2+y^2 \, dA $$ I've transformed the region $D$ to: $$ u = x \therefore x = u \\ v = y - 1 \therefore y = v + 1 \\ J = \begin{vmatrix} 1 & 0 \\ 0 & 1 \end{vmatrix} = 1 $$ And setup the integral to: $$ \iint_{u^2+v^2=1} u^2+(v+1)^2 \, du \, dv = \iint_{u^2+v^2=1} u^2+v^2+2v+1 \, du \, dv $$ Using polar coordinates: $$ \int_0^{2\pi} \int_0^1 \left( r^2 + 2 r \sin \theta + 1 \right) r \, dr \, d\theta = \int_0^{2\pi} \int_0^1 r^3 + 2 r^2 \sin \theta + r \, dr \, d\theta $$ Solving: $$ \int_0^{2\pi} \frac{3}{4} + \frac{2}{3} \sin \theta \, d\theta = \frac{3\pi}{2} $$ So, is it right? I'm not sure about the transformation I've done.","I'm studying to my test of Calculus and I'm not sure about the result I got. This is how I've done it: Discover the interval of z: $$ 0 \leq z \leq x^2+y^2 $$ Setup the integral: $$ \iint_D \, \int_0^{x^2+y^2} \, dz \,\, dA $$ It equals to: $$ \iint_D x^2+y^2 \, dA $$ I've transformed the region $D$ to: $$ u = x \therefore x = u \\ v = y - 1 \therefore y = v + 1 \\ J = \begin{vmatrix} 1 & 0 \\ 0 & 1 \end{vmatrix} = 1 $$ And setup the integral to: $$ \iint_{u^2+v^2=1} u^2+(v+1)^2 \, du \, dv = \iint_{u^2+v^2=1} u^2+v^2+2v+1 \, du \, dv $$ Using polar coordinates: $$ \int_0^{2\pi} \int_0^1 \left( r^2 + 2 r \sin \theta + 1 \right) r \, dr \, d\theta = \int_0^{2\pi} \int_0^1 r^3 + 2 r^2 \sin \theta + r \, dr \, d\theta $$ Solving: $$ \int_0^{2\pi} \frac{3}{4} + \frac{2}{3} \sin \theta \, d\theta = \frac{3\pi}{2} $$ So, is it right? I'm not sure about the transformation I've done.",,"['calculus', 'integration', 'multivariable-calculus']"
25,How is differential form different from ordinary calculus objects?,How is differential form different from ordinary calculus objects?,,"I am going to learn differential form soon, but after reading some introductory parts of my texts, I couldn't get why differential form is needed and how it is different from ordinary mathematics objects. Can anyone explain this?","I am going to learn differential form soon, but after reading some introductory parts of my texts, I couldn't get why differential form is needed and how it is different from ordinary mathematics objects. Can anyone explain this?",,"['differential-geometry', 'multivariable-calculus', 'differential-forms']"
26,Bounding a continuously differentiable function using Taylor given the function is bounded by the norm of x,Bounding a continuously differentiable function using Taylor given the function is bounded by the norm of x,,"Suppose $0 < r < 1$ and that $f \colon B_1(0) \to \mathbb R$ is continuously differentiable. If there is an $\alpha > 0$ s.t. $|f(x)<\Vert x\Vert^\alpha$ for all $x \in B_r(0)$, prove there is an $M > 0$ s.t. $|f(x)| \leq M\Vert x\Vert$ for $x \in B_r(0)$. I tried using a Taylor approximation to bound $|f(x)-f(a)|$, knowing that each is bounded above by $|f(x)<\Vert x\Vert^\alpha$ but this did not work. $F$ is continuously differentiable so all first order partials exist & are continuous, so we can only safely use Taylor for $n=1$. Any help or direction would be appreciated.","Suppose $0 < r < 1$ and that $f \colon B_1(0) \to \mathbb R$ is continuously differentiable. If there is an $\alpha > 0$ s.t. $|f(x)<\Vert x\Vert^\alpha$ for all $x \in B_r(0)$, prove there is an $M > 0$ s.t. $|f(x)| \leq M\Vert x\Vert$ for $x \in B_r(0)$. I tried using a Taylor approximation to bound $|f(x)-f(a)|$, knowing that each is bounded above by $|f(x)<\Vert x\Vert^\alpha$ but this did not work. $F$ is continuously differentiable so all first order partials exist & are continuous, so we can only safely use Taylor for $n=1$. Any help or direction would be appreciated.",,"['real-analysis', 'multivariable-calculus', 'taylor-expansion']"
27,Solid angle formula,Solid angle formula,,"How do you get from the equation on the left to the one on the right? $$  \Omega = \iint_S \frac { \vec{r} \cdot \hat{n} \,dS }{r^3}= \iint_S \sin\theta\,d\theta\,d\varphi . $$ Thanks.","How do you get from the equation on the left to the one on the right? $$  \Omega = \iint_S \frac { \vec{r} \cdot \hat{n} \,dS }{r^3}= \iint_S \sin\theta\,d\theta\,d\varphi . $$ Thanks.",,"['calculus', 'multivariable-calculus']"
28,Does there exist a diffeomorphism on $\mathbb{R^2}$ that flattens out the boundary of a compact set at a point?,Does there exist a diffeomorphism on  that flattens out the boundary of a compact set at a point?,\mathbb{R^2},"Given a compact subset of $\mathbb{R^2}$ with ${C^2}$ boundary $S$ and a point $x \in S$, can one find a diffeomorphism $f$ from $\mathbb{R^2}$ to $\mathbb{R^2}$ for which $f(x) = x$, the image $f(S)$ is a ${C^2}$ curve and such that, in a neighborhood of $f(x)$, the curve $f(S)$ coincides with the tangent line of $x$ in $S$? I think this is possible but still have no idea how to construct such a function $f$.","Given a compact subset of $\mathbb{R^2}$ with ${C^2}$ boundary $S$ and a point $x \in S$, can one find a diffeomorphism $f$ from $\mathbb{R^2}$ to $\mathbb{R^2}$ for which $f(x) = x$, the image $f(S)$ is a ${C^2}$ curve and such that, in a neighborhood of $f(x)$, the curve $f(S)$ coincides with the tangent line of $x$ in $S$? I think this is possible but still have no idea how to construct such a function $f$.",,"['differential-geometry', 'multivariable-calculus']"
29,Optimize vector function,Optimize vector function,,"this is a question about vector differentiation. I had to optimize the following function: $(b-Ax)^T(b-Ax)$ $A$ is matrix, $x$ and h are column vectors $$A(t) = A^T$$ I worked this out and computed the differential, $df$ which was: $$h(t)A(t)Ax + x(t)A(t)Ah - b(t)Ah - h(t)A(t)b$$ My question is, how do I compute the derivative $D$ from this  and get a solution of the optimal $x$?","this is a question about vector differentiation. I had to optimize the following function: $(b-Ax)^T(b-Ax)$ $A$ is matrix, $x$ and h are column vectors $$A(t) = A^T$$ I worked this out and computed the differential, $df$ which was: $$h(t)A(t)Ax + x(t)A(t)Ah - b(t)Ah - h(t)A(t)b$$ My question is, how do I compute the derivative $D$ from this  and get a solution of the optimal $x$?",,['multivariable-calculus']
30,Justification of use of delta functions/rigorous proof of Green's function for Poisson equation,Justification of use of delta functions/rigorous proof of Green's function for Poisson equation,,"I'm looking at the proofs of Helmholtz's theorem, but I'm having trouble justifying their interchange of integration and differentiation and subsequent treatment of the ""integrand"" as a delta function. Referring to http://www.ph.ed.ac.uk/~rhorsley/SI10-11_t+f/lec19.pdf , on page 2, this essentially boils down to showing that $v(r) = -\frac{1}{4\pi } \int \frac{a_i(r')}{|r-r'|} dr'$ solves the Poisson equation $ \nabla ^2 v_i = a_i$, ie. showing that $-\frac{1}{4\pi |r-r'|} $ is the Green's function for the Poisson equation. This seems somewhat familiar to me but I'm not sure how to prove it rigorously. The proofs I've seen of this again use Dirac delta arguments which again seem a bit hand-wavey. Could anyone help me sure up my logical understanding or direct me somewhere with a more careful treatment please? EDIT: Hmm, I found a proof for the validity of the Green's function that looks convincing at quick glance (Theorem 1.1 of Week 7 from http://math.mit.edu/~jspeck/18.152_Fall2011/18152_Schedule.html ).","I'm looking at the proofs of Helmholtz's theorem, but I'm having trouble justifying their interchange of integration and differentiation and subsequent treatment of the ""integrand"" as a delta function. Referring to http://www.ph.ed.ac.uk/~rhorsley/SI10-11_t+f/lec19.pdf , on page 2, this essentially boils down to showing that $v(r) = -\frac{1}{4\pi } \int \frac{a_i(r')}{|r-r'|} dr'$ solves the Poisson equation $ \nabla ^2 v_i = a_i$, ie. showing that $-\frac{1}{4\pi |r-r'|} $ is the Green's function for the Poisson equation. This seems somewhat familiar to me but I'm not sure how to prove it rigorously. The proofs I've seen of this again use Dirac delta arguments which again seem a bit hand-wavey. Could anyone help me sure up my logical understanding or direct me somewhere with a more careful treatment please? EDIT: Hmm, I found a proof for the validity of the Green's function that looks convincing at quick glance (Theorem 1.1 of Week 7 from http://math.mit.edu/~jspeck/18.152_Fall2011/18152_Schedule.html ).",,"['multivariable-calculus', 'partial-differential-equations', 'distribution-theory']"
31,Epsilon-delta proof at incorrect limit point,Epsilon-delta proof at incorrect limit point,,"I know that $$\lim_{(x,y)\to(0,0)}  {xy^2 \over \sqrt{x^2 + y^2}(x^2 + y^2)} \ne 0.$$ However, when I try to (wrongly) prove that the limit does equal $0$, using $$|x|, |y|, y  \le \sqrt{x^2 + y^2}$$ (I do realise that this is bad notation, not sure how I could write it here in a different way without using 3 equations), I get $$\left\vert\frac{xy^2}{(x^2 + y^2)^{3/2}} - 0\right\vert = \frac{|x|}{(x^2 + y^2)^{3/2}}y^2\le \frac{y^2}{(x^2 + y^2)} = \frac{|y||y|}{(x^2 + y^2)}\le $$ $$\frac{|y|}{\sqrt{x^2 + y^2}}\le |y|\le \sqrt{x^2 + y^2}< \delta < \epsilon$$ Since $$||(x,y)-(0,0)||= \sqrt{x^2 + y^2}< \delta$$ It's somewhat based on this answer. So now I wonder, at what step do I make a mistake, since this should (as far as I know) not be possible? Edit: If it has to do with dividing by zero, then why is that not a problem in the linked answer?","I know that $$\lim_{(x,y)\to(0,0)}  {xy^2 \over \sqrt{x^2 + y^2}(x^2 + y^2)} \ne 0.$$ However, when I try to (wrongly) prove that the limit does equal $0$, using $$|x|, |y|, y  \le \sqrt{x^2 + y^2}$$ (I do realise that this is bad notation, not sure how I could write it here in a different way without using 3 equations), I get $$\left\vert\frac{xy^2}{(x^2 + y^2)^{3/2}} - 0\right\vert = \frac{|x|}{(x^2 + y^2)^{3/2}}y^2\le \frac{y^2}{(x^2 + y^2)} = \frac{|y||y|}{(x^2 + y^2)}\le $$ $$\frac{|y|}{\sqrt{x^2 + y^2}}\le |y|\le \sqrt{x^2 + y^2}< \delta < \epsilon$$ Since $$||(x,y)-(0,0)||= \sqrt{x^2 + y^2}< \delta$$ It's somewhat based on this answer. So now I wonder, at what step do I make a mistake, since this should (as far as I know) not be possible? Edit: If it has to do with dividing by zero, then why is that not a problem in the linked answer?",,"['limits', 'multivariable-calculus']"
32,Change of Variable (Double Integral),Change of Variable (Double Integral),,"I have been trying to integrate the two following integrands; $$\int \int_{D}(x^{2}+y^{2})dxdy$$ where  $D=\{{x^{2}+xy+y^{2}\leq 1}\}$ and $$\int \int_{D}\sqrt{x^{2}+y^{2}}dxdy$$ where $D=\{{x^{2}+y^{2}\leq x}\}$. Now, I have been tempted to use change of variables (polar coordinates) in both cases. For example, in the first case, I completed the square so:  $$D=\{{(x+1/2y)^{2}+3/4y^{2}\leq 1}\}$$ I then set $u=r\cos{t}-(1/2)r\sin{t}$ and $v=r\sin{t}$, performed partial differentiation, formed the jacobin matrix, and calculated the determinant. I then set the new boundaries  $0\leq r\leq$1 and $0\leq t \leq 2\pi$ before calculating the whole expression. Now, the answer I obtain is $(53\pi)/96$, which is blatantly wrong as the textbook gives $(4\pi)/(3\sqrt{3})$. Since my approach to the second problem is similar, I fear, that the answer I obtain there is wrong as well. I would be exceedingly grateful if you could help me.","I have been trying to integrate the two following integrands; $$\int \int_{D}(x^{2}+y^{2})dxdy$$ where  $D=\{{x^{2}+xy+y^{2}\leq 1}\}$ and $$\int \int_{D}\sqrt{x^{2}+y^{2}}dxdy$$ where $D=\{{x^{2}+y^{2}\leq x}\}$. Now, I have been tempted to use change of variables (polar coordinates) in both cases. For example, in the first case, I completed the square so:  $$D=\{{(x+1/2y)^{2}+3/4y^{2}\leq 1}\}$$ I then set $u=r\cos{t}-(1/2)r\sin{t}$ and $v=r\sin{t}$, performed partial differentiation, formed the jacobin matrix, and calculated the determinant. I then set the new boundaries  $0\leq r\leq$1 and $0\leq t \leq 2\pi$ before calculating the whole expression. Now, the answer I obtain is $(53\pi)/96$, which is blatantly wrong as the textbook gives $(4\pi)/(3\sqrt{3})$. Since my approach to the second problem is similar, I fear, that the answer I obtain there is wrong as well. I would be exceedingly grateful if you could help me.",,"['integration', 'multivariable-calculus']"
33,partial derivative chain rules,partial derivative chain rules,,"Suppose that there is $f(a,b)$. Also suppose that $b = g(a, \text{and some other variables})$. By chain rule, it seems that $$\frac{\partial f}{\partial a} = \frac{\partial f}{\partial b}\frac{\partial b}{\partial a}+\frac{\partial f}{\partial a}\frac{\partial a}{\partial a}$$. Is this accurate? Edit: If this is not true, how is $\frac{\partial f}{\partial a}$ and $\frac{\partial f}{\partial b}$ related? Edit 2: OK. if $f(b,c)$ and $b=g(a, z, y, x...)$ and $c = a$. Now chain rule: $$\frac{\partial f}{\partial a} = \frac{\partial f}{\partial b}\frac{\partial b}{\partial a} + \frac{\partial f}{\partial c}\frac{\partial c}{\partial a} = \text{this equals to the above}.$$ So, what's wrong with this?","Suppose that there is $f(a,b)$. Also suppose that $b = g(a, \text{and some other variables})$. By chain rule, it seems that $$\frac{\partial f}{\partial a} = \frac{\partial f}{\partial b}\frac{\partial b}{\partial a}+\frac{\partial f}{\partial a}\frac{\partial a}{\partial a}$$. Is this accurate? Edit: If this is not true, how is $\frac{\partial f}{\partial a}$ and $\frac{\partial f}{\partial b}$ related? Edit 2: OK. if $f(b,c)$ and $b=g(a, z, y, x...)$ and $c = a$. Now chain rule: $$\frac{\partial f}{\partial a} = \frac{\partial f}{\partial b}\frac{\partial b}{\partial a} + \frac{\partial f}{\partial c}\frac{\partial c}{\partial a} = \text{this equals to the above}.$$ So, what's wrong with this?",,"['calculus', 'multivariable-calculus']"
34,Multivariable function derivative problem,Multivariable function derivative problem,,"Let $f : \mathbb{R}^2 \to \mathbb{R}$ be continuous on $\mathbb{R}^2$ and differentiable on $\mathbb{R}^2\setminus\{0\}$. If $Df(x)x = 0$ for all $x \in \mathbb{R}^2\setminus\{0\}$, show that $f$ is constant on $\mathbb{R}^2$. I tried to prove $Df(x) = 0$ however I cannot find an approach, here the mean value inequality seems not applicable.","Let $f : \mathbb{R}^2 \to \mathbb{R}$ be continuous on $\mathbb{R}^2$ and differentiable on $\mathbb{R}^2\setminus\{0\}$. If $Df(x)x = 0$ for all $x \in \mathbb{R}^2\setminus\{0\}$, show that $f$ is constant on $\mathbb{R}^2$. I tried to prove $Df(x) = 0$ however I cannot find an approach, here the mean value inequality seems not applicable.",,"['analysis', 'multivariable-calculus']"
35,How to approach graphing vector functions?,How to approach graphing vector functions?,,"Let's say I have the following vector function: $\mathbf{r}(t) = t \cos t\,\mathbf{i} + t\,\mathbf{j} + t \sin t\,\mathbf{k}$ What properties of this function will allow me to sketch a curve drawn by this function? I know that: $x = t\cos t$ $y = t$ $z = t \sin t$ What is a general approach that I can take to solve problems that want me to sketch curves drawn by vector functions?",Let's say I have the following vector function: What properties of this function will allow me to sketch a curve drawn by this function? I know that: What is a general approach that I can take to solve problems that want me to sketch curves drawn by vector functions?,"\mathbf{r}(t) = t \cos t\,\mathbf{i} + t\,\mathbf{j} + t \sin t\,\mathbf{k} x = t\cos t y = t z = t \sin t","['multivariable-calculus', 'vector-spaces']"
36,Find local and global minimizer(s)?,Find local and global minimizer(s)?,,"Consider the problem: minimize $f(x)=x_1$ subject to $(x_1-1)^2+x_2^2=1$, $(x_1+1)^2+x_2^2=1.$ Are there any local minimizers? Are there any global minimizers? Is $(-2,0)$ both the local and global minimizer?","Consider the problem: minimize $f(x)=x_1$ subject to $(x_1-1)^2+x_2^2=1$, $(x_1+1)^2+x_2^2=1.$ Are there any local minimizers? Are there any global minimizers? Is $(-2,0)$ both the local and global minimizer?",,"['multivariable-calculus', 'optimization']"
37,Nonlinear optimization question,Nonlinear optimization question,,"For (x,y) in $\mathbb R^2$, consider f(x,y) = $x^2 -2xy + \frac{4}{3}y^2 - 4y$ Find the local minimum of f. Is it a strict local minimum? Compute the $\lim\limits_{|(x,y)|\to \infty}$ f(x,y) to decide if the local minimum is a global minimum. My work: I found the critical point (6,6) and for any v (since it is in the interior of $\mathbb R^2$) is feasible. Therefore $\vec\nabla$F.v = 0 since (6,6) is in $\mathbb R^2$. I'm having trouble how taking the shown limit will prove whether the local min is a global min. Thanks,","For (x,y) in $\mathbb R^2$, consider f(x,y) = $x^2 -2xy + \frac{4}{3}y^2 - 4y$ Find the local minimum of f. Is it a strict local minimum? Compute the $\lim\limits_{|(x,y)|\to \infty}$ f(x,y) to decide if the local minimum is a global minimum. My work: I found the critical point (6,6) and for any v (since it is in the interior of $\mathbb R^2$) is feasible. Therefore $\vec\nabla$F.v = 0 since (6,6) is in $\mathbb R^2$. I'm having trouble how taking the shown limit will prove whether the local min is a global min. Thanks,",,"['multivariable-calculus', 'nonlinear-optimization']"
38,Symmetric polynomial optimization,Symmetric polynomial optimization,,"Recently I asked a stupid question here (there’s no harm in that, even Fields medalist Terence Tao advises to ask dumb questions once in a while). Here is a variant question that may be more difficult, perhaps even suitable for MathOverflow. Let $n\geq 3$, and $a,b$ be two positive numbers with $\frac{a}{n} \gt b^{\frac{1}{n}}$ ; then the set $$ K= \bigg\lbrace (x_1,x_2, \ldots ,x_n) \in (0,\infty)^n \bigg| \sum_{k=1}^nx_k=a, \  \prod_{k=1}^nx_k=b \bigg\rbrace $$ is nonempty and compact (it is included in $[0,a]^n$), and its boundary is homeomorphic to $S^{n-2}$ (a nice argument shows this here ).  Let $f$ be a symmetric polynomial in $x_1,x_2, \ldots,x_n$. Then $f$ attains a minimum $m$ on $K$. Let us call a point $p=(p_1,p_2, \ldots ,p_n)$ of $K$ extremist if all its coordinates are equal but one (in other words, there is an index $i$ such that the  set $\lbrace p_j\rbrace_{j \neq i}$ is a singleton). Also, call it weakly extremist if some two coordinates of $p$ are equal. Is it true that $m$ is always attained at an extremist point (we are of course not forbidding $f$ from also attaining $m$ at other, non-extremist points, which will certainly happen is $f$ is constant for example) ? Is it true that $m$ is always attained at a weakly extremist point? My progress so far on this problem : I can show that the answer is YES to both the questions when $n=3$. Indeed, by the fundamental theorem for symmetric polynomials it suffices to treat the special case $f(x_1,x_2,x_3)=x_1x_2+x_1x_3+x_2x_3$. Then for $(x_1,x_2,x_3) \in K$, $x_1$ varies in an interval $[\alpha,\beta]$ where $\alpha$ and $\beta$ are the two smallest roots of the polynomial $P=x(a-x)^2-4b$. Also, $$ f(x_1,x_2,x_3)=x_1(a-x_1)+\frac{b}{x_1}=g(x_1) $$ On the interval $[\alpha,\beta]$, the derivative $g’$ has two simple zeros, one at $\beta’=\frac{a-\beta}{2}$ and the other at $\alpha’=\frac{a-\alpha}{2}$. If we put $m=g(\beta)$ and $M=g(\alpha)$, then $M=g(\alpha)=g(\alpha’)$ and $m=g(\beta)=g(\beta’)$, so that the original interval $I=[\alpha,\beta]$ can be decomposed as $$ I=I_1 \cup I_2 \cup I_3, \ \ I_1=[\alpha,\beta’], \ I_2=[\beta’,\alpha’], \ I_3=[\alpha’,\beta] $$ and for $i=1,2,3$, we see that $g$ restricts to a bijection $I_i \to [m,M]$. So the minimum $m$ is attained at the extremist point $(\beta,\beta’,\beta’)$ and the maximum $M$ is attained at the extremist point $(\alpha,\alpha’,\alpha’)$.","Recently I asked a stupid question here (there’s no harm in that, even Fields medalist Terence Tao advises to ask dumb questions once in a while). Here is a variant question that may be more difficult, perhaps even suitable for MathOverflow. Let $n\geq 3$, and $a,b$ be two positive numbers with $\frac{a}{n} \gt b^{\frac{1}{n}}$ ; then the set $$ K= \bigg\lbrace (x_1,x_2, \ldots ,x_n) \in (0,\infty)^n \bigg| \sum_{k=1}^nx_k=a, \  \prod_{k=1}^nx_k=b \bigg\rbrace $$ is nonempty and compact (it is included in $[0,a]^n$), and its boundary is homeomorphic to $S^{n-2}$ (a nice argument shows this here ).  Let $f$ be a symmetric polynomial in $x_1,x_2, \ldots,x_n$. Then $f$ attains a minimum $m$ on $K$. Let us call a point $p=(p_1,p_2, \ldots ,p_n)$ of $K$ extremist if all its coordinates are equal but one (in other words, there is an index $i$ such that the  set $\lbrace p_j\rbrace_{j \neq i}$ is a singleton). Also, call it weakly extremist if some two coordinates of $p$ are equal. Is it true that $m$ is always attained at an extremist point (we are of course not forbidding $f$ from also attaining $m$ at other, non-extremist points, which will certainly happen is $f$ is constant for example) ? Is it true that $m$ is always attained at a weakly extremist point? My progress so far on this problem : I can show that the answer is YES to both the questions when $n=3$. Indeed, by the fundamental theorem for symmetric polynomials it suffices to treat the special case $f(x_1,x_2,x_3)=x_1x_2+x_1x_3+x_2x_3$. Then for $(x_1,x_2,x_3) \in K$, $x_1$ varies in an interval $[\alpha,\beta]$ where $\alpha$ and $\beta$ are the two smallest roots of the polynomial $P=x(a-x)^2-4b$. Also, $$ f(x_1,x_2,x_3)=x_1(a-x_1)+\frac{b}{x_1}=g(x_1) $$ On the interval $[\alpha,\beta]$, the derivative $g’$ has two simple zeros, one at $\beta’=\frac{a-\beta}{2}$ and the other at $\alpha’=\frac{a-\alpha}{2}$. If we put $m=g(\beta)$ and $M=g(\alpha)$, then $M=g(\alpha)=g(\alpha’)$ and $m=g(\beta)=g(\beta’)$, so that the original interval $I=[\alpha,\beta]$ can be decomposed as $$ I=I_1 \cup I_2 \cup I_3, \ \ I_1=[\alpha,\beta’], \ I_2=[\beta’,\alpha’], \ I_3=[\alpha’,\beta] $$ and for $i=1,2,3$, we see that $g$ restricts to a bijection $I_i \to [m,M]$. So the minimum $m$ is attained at the extremist point $(\beta,\beta’,\beta’)$ and the maximum $M$ is attained at the extremist point $(\alpha,\alpha’,\alpha’)$.",,"['polynomials', 'multivariable-calculus', 'optimization', 'symmetric-polynomials']"
39,supremum of a multivariable function,supremum of a multivariable function,,"Here is a question that I have been working on but having trouble with. Let $f(x)=e^{-|x|^2}$, where $x \in \mathbb{R}^n$ and $|x|$ the usual euclidean norm of $x$. Prove that for every $\epsilon >0$ there is a positive number $M$ such that $g(x,y):=f(x)g(y)|x-y|^2 < \epsilon$ whenever $|x|^2+|y|^2 >M$. I showed this Using the fact that $e^{-|x|^2}$ goes to zero as norm of $x$ goes to infinity. But I'm having trouble with the 2nd and 3rd part of the question. Show that $S:=\sup_{x,y\in \mathbb{R}^n}f(x)f(y)|x-y|^2$ is attained at some point in $\mathbb{R}^n \times \mathbb{R}^n$. Determine the value of S.","Here is a question that I have been working on but having trouble with. Let $f(x)=e^{-|x|^2}$, where $x \in \mathbb{R}^n$ and $|x|$ the usual euclidean norm of $x$. Prove that for every $\epsilon >0$ there is a positive number $M$ such that $g(x,y):=f(x)g(y)|x-y|^2 < \epsilon$ whenever $|x|^2+|y|^2 >M$. I showed this Using the fact that $e^{-|x|^2}$ goes to zero as norm of $x$ goes to infinity. But I'm having trouble with the 2nd and 3rd part of the question. Show that $S:=\sup_{x,y\in \mathbb{R}^n}f(x)f(y)|x-y|^2$ is attained at some point in $\mathbb{R}^n \times \mathbb{R}^n$. Determine the value of S.",,['multivariable-calculus']
40,Dimension of vector field in gauss divergence theorem,Dimension of vector field in gauss divergence theorem,,"In Gauss Divergence Theorem , $$\int \int_{\partial V} \textbf{F}\cdot\textbf{n}\,dS =\int \int \int_V \nabla\cdot\textbf{F}\,dx\,dy\,dz,$$ is there any restriction on the vector field $\textbf{F}$ ? Does it need to be in dimension three only ? Can I let it be $\textbf{F}=xyz$ where $x,y,z\in\mathbb{R}$ ?","In Gauss Divergence Theorem , $$\int \int_{\partial V} \textbf{F}\cdot\textbf{n}\,dS =\int \int \int_V \nabla\cdot\textbf{F}\,dx\,dy\,dz,$$ is there any restriction on the vector field $\textbf{F}$ ? Does it need to be in dimension three only ? Can I let it be $\textbf{F}=xyz$ where $x,y,z\in\mathbb{R}$ ?",,"['calculus', 'multivariable-calculus']"
41,"Calculating $\int_{\mathcal{S}}x_1^r \, \mathrm dx_1\ldots \, \mathrm dx_n$ [closed]",Calculating  [closed],"\int_{\mathcal{S}}x_1^r \, \mathrm dx_1\ldots \, \mathrm dx_n","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I need help with the calculation of the following integral $$ \int_{\mathcal{S}}x_1^r \, \mathrm dx_1\ldots  \, \mathrm dx_n $$ where $r>0$ and $$ \mathcal{S} = \left\{(x_1,\ldots,x_n):a-\epsilon\leq x_1+\ldots+x_n\leq a,\;x_1\ldots,x_n\geq0\right\} $$ for $a>0$ and $a-\epsilon>0$. Thank you","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I need help with the calculation of the following integral $$ \int_{\mathcal{S}}x_1^r \, \mathrm dx_1\ldots  \, \mathrm dx_n $$ where $r>0$ and $$ \mathcal{S} = \left\{(x_1,\ldots,x_n):a-\epsilon\leq x_1+\ldots+x_n\leq a,\;x_1\ldots,x_n\geq0\right\} $$ for $a>0$ and $a-\epsilon>0$. Thank you",,"['integration', 'multivariable-calculus', 'definite-integrals']"
42,A particular way of writing a polynomial.,A particular way of writing a polynomial.,,"Notice that by  Taylor's theorem  If the function $f : \mathbb{R}^{n} \rightarrow \mathbb{R}$  is $  k+1$ times continuously differentiable in the closed ball B, then one can derive an exact formula for the remainder in terms of (k+1)-th order partial derivatives of f in this neighborhood. Namely, \begin{align}& f( \boldsymbol{x} ) = \sum_{|\alpha|\leq k} \frac{\mathrm D^\alpha f(\boldsymbol{a})}{\alpha!} (\boldsymbol{x}-\boldsymbol{a})^\alpha + \sum_{|\beta|=k+1} R_\beta(\boldsymbol{x})(\boldsymbol{x}-\boldsymbol{a})^\beta, \\& R_\beta( \boldsymbol{x} ) = \frac{|\beta|}{\beta!} \int_0^1 (1-t)^{|\beta|-1} \mathrm D^\beta f \big(\boldsymbol{a}+t( \boldsymbol{x}-\boldsymbol{a} )\big) \, dt. \end{align} Until the degree two I Know that I can write the polynomial $  \sum_{|\alpha|\leq k} \frac{\mathrm D^\alpha f(\boldsymbol{a})}{\alpha!} (\boldsymbol{x}-\boldsymbol{a})^\alpha $ in this form  \begin{equation} f(0) + \mathrm Df(0) \cdot X + \frac{1}{2} X^{t} \mathrm D^{2}f(0)  X \end{equation} Can we write the polynomial above in a similar way for a degree grater than two?","Notice that by  Taylor's theorem  If the function $f : \mathbb{R}^{n} \rightarrow \mathbb{R}$  is $  k+1$ times continuously differentiable in the closed ball B, then one can derive an exact formula for the remainder in terms of (k+1)-th order partial derivatives of f in this neighborhood. Namely, \begin{align}& f( \boldsymbol{x} ) = \sum_{|\alpha|\leq k} \frac{\mathrm D^\alpha f(\boldsymbol{a})}{\alpha!} (\boldsymbol{x}-\boldsymbol{a})^\alpha + \sum_{|\beta|=k+1} R_\beta(\boldsymbol{x})(\boldsymbol{x}-\boldsymbol{a})^\beta, \\& R_\beta( \boldsymbol{x} ) = \frac{|\beta|}{\beta!} \int_0^1 (1-t)^{|\beta|-1} \mathrm D^\beta f \big(\boldsymbol{a}+t( \boldsymbol{x}-\boldsymbol{a} )\big) \, dt. \end{align} Until the degree two I Know that I can write the polynomial $  \sum_{|\alpha|\leq k} \frac{\mathrm D^\alpha f(\boldsymbol{a})}{\alpha!} (\boldsymbol{x}-\boldsymbol{a})^\alpha $ in this form  \begin{equation} f(0) + \mathrm Df(0) \cdot X + \frac{1}{2} X^{t} \mathrm D^{2}f(0)  X \end{equation} Can we write the polynomial above in a similar way for a degree grater than two?",,['multivariable-calculus']
43,Curvature $\kappa$ Proof,Curvature  Proof,\kappa,"The curvature of a curve (rate of change of the unit tangent vector with respect to arc length) is defined as $$\kappa = \frac{|\underline{r}'(t) \times \underline{r}''(t)|}{|\underline{r}'(t)|^3}$$ The proof of this is shown in my textbook, but I don't understand one step. They say: $\underline{r}'(t) = \underline{T}(t) |\underline{r}'(t)| = \underline{T}(t) s'(t)$ and then $ \underline{r}''(t) = \underline{T}'(t) s'(t) + \underline{T}(t)s''(t).$ First question: Would $s''(t)$ not equal $0$? My reasoning being since $s'(t) = |\underline{r}'(t)|,$ a constant and then subsequently differentiate again to get $0$? After the above step, they say $\underline{r}'(t) \times \underline{r}''(t) = [s'(t)]^2 \underline{T}(t) \times \underline{T}'(t)$. I am not sure how they get this. The expression I got when I simplified was:$$ \underline{T}(t)s'(t) \times (\underline{T}'(t)s'(t) + \underline{T}(t)s''(t)) = \underline{T}(t)s'(t) \times \underline{T}'(t)s'(t) + \underline{T}(t)s'(t) \times \underline{T}(t)s''(t)$$ And the latter term disappears because $\underline{T}(t) \times \underline{T}(t) = \underline{0}$ Many thanks.","The curvature of a curve (rate of change of the unit tangent vector with respect to arc length) is defined as $$\kappa = \frac{|\underline{r}'(t) \times \underline{r}''(t)|}{|\underline{r}'(t)|^3}$$ The proof of this is shown in my textbook, but I don't understand one step. They say: $\underline{r}'(t) = \underline{T}(t) |\underline{r}'(t)| = \underline{T}(t) s'(t)$ and then $ \underline{r}''(t) = \underline{T}'(t) s'(t) + \underline{T}(t)s''(t).$ First question: Would $s''(t)$ not equal $0$? My reasoning being since $s'(t) = |\underline{r}'(t)|,$ a constant and then subsequently differentiate again to get $0$? After the above step, they say $\underline{r}'(t) \times \underline{r}''(t) = [s'(t)]^2 \underline{T}(t) \times \underline{T}'(t)$. I am not sure how they get this. The expression I got when I simplified was:$$ \underline{T}(t)s'(t) \times (\underline{T}'(t)s'(t) + \underline{T}(t)s''(t)) = \underline{T}(t)s'(t) \times \underline{T}'(t)s'(t) + \underline{T}(t)s'(t) \times \underline{T}(t)s''(t)$$ And the latter term disappears because $\underline{T}(t) \times \underline{T}(t) = \underline{0}$ Many thanks.",,"['multivariable-calculus', 'vector-analysis']"
44,"Show that in a neighborhood of $(u_0, v_0, x_0, y_0)=(0,0,1,1), (u,v)$ can be written as differentiable functions of $x$ and $y$",Show that in a neighborhood of  can be written as differentiable functions of  and,"(u_0, v_0, x_0, y_0)=(0,0,1,1), (u,v) x y","I have the following problem: Considerate the following system: $$x=\cos(u)+\sin(v) $$ $$y=\sin(u)+\cos(v)$$ Show that in a neighborhood of $(u_0, v_0, x_0, y_0)=(0,0,1,1), (u,v)$ can be written as differentiable functions of $x$ and $y$. Furthermore, $$\left(\frac {\partial u}{\partial x}\right)^2 +\left(\frac {\partial u}{\partial y}\right)^2=\left(\frac {\partial v}{\partial x}\right)^2+\left(\frac {\partial v}{\partial y}\right)^2$$ I made some attempts however I did not succeed in the second part. But related to that , I will try again. I would appreciate any hints in the first part, which I believe is an application of implicit function theorem. Thanks in advance.","I have the following problem: Considerate the following system: $$x=\cos(u)+\sin(v) $$ $$y=\sin(u)+\cos(v)$$ Show that in a neighborhood of $(u_0, v_0, x_0, y_0)=(0,0,1,1), (u,v)$ can be written as differentiable functions of $x$ and $y$. Furthermore, $$\left(\frac {\partial u}{\partial x}\right)^2 +\left(\frac {\partial u}{\partial y}\right)^2=\left(\frac {\partial v}{\partial x}\right)^2+\left(\frac {\partial v}{\partial y}\right)^2$$ I made some attempts however I did not succeed in the second part. But related to that , I will try again. I would appreciate any hints in the first part, which I believe is an application of implicit function theorem. Thanks in advance.",,['multivariable-calculus']
45,Multivariable Calculus Word Problem,Multivariable Calculus Word Problem,,How do I solve the following problem. I'm not sure what it falls under so I don't know where to look in the book for help.... Find the three positive number with product 27 which minimizes the   sum of the third with twice the second with four times the first.,How do I solve the following problem. I'm not sure what it falls under so I don't know where to look in the book for help.... Find the three positive number with product 27 which minimizes the   sum of the third with twice the second with four times the first.,,['multivariable-calculus']
46,"Two part question, Area of Torus using disk/ washer method","Two part question, Area of Torus using disk/ washer method",,"a. A torus is formed by revolving the region bounded by the circle $(x-2)^2 + y^2 = 1$ about the y-axis. Use the disk/washer method to calculate the volume of the torus. Figure given, showing $r=2$ and with centroid at $(2,0)$ b. Use the disk/washer method to find the volume of the general torus if the circle has radius r and its center is R units from the axis of rotation. For part a, I started by rewriting equation as $x = 2 \pm \sqrt{1-y^2}$. I was using the washer setup, and simplified to $V= 8\pi \int_{-1}^1 \sqrt{1-y^2}dy$. The answer is given as $4\pi^2$. Just need to figure out the work to get there. Again, for part b answer is given as $2\pi^2 r^2 R$. I know I need the answer from part a to solve part b. Looking for any help on how to complete the steps that will give me the answer. Thanks!","a. A torus is formed by revolving the region bounded by the circle $(x-2)^2 + y^2 = 1$ about the y-axis. Use the disk/washer method to calculate the volume of the torus. Figure given, showing $r=2$ and with centroid at $(2,0)$ b. Use the disk/washer method to find the volume of the general torus if the circle has radius r and its center is R units from the axis of rotation. For part a, I started by rewriting equation as $x = 2 \pm \sqrt{1-y^2}$. I was using the washer setup, and simplified to $V= 8\pi \int_{-1}^1 \sqrt{1-y^2}dy$. The answer is given as $4\pi^2$. Just need to figure out the work to get there. Again, for part b answer is given as $2\pi^2 r^2 R$. I know I need the answer from part a to solve part b. Looking for any help on how to complete the steps that will give me the answer. Thanks!",,['multivariable-calculus']
47,Applying Green's Theorem,Applying Green's Theorem,,"So I'm practicing a few problems and I can't get this one - $$P(x,y) = e^x \sin(y) \\ Q(x,y) = e^x \cos(y)$$ $C$ is the right hand loop of the graph of the polar equation $r^2 = 4\cos(\theta)$ I want to evaluate: $$\int_{C}{P(x,y)\:dx+Q(x,y)\:dy}$$ Now I tried the right hand side of Green's theorem, but it's difficult because  $\frac{dp}{dy}$ in polar has a $\cos(r\sin(\theta))$ term in it. If I just try parametrizing with $x = a\sin(t)$ and $y = a\cos(t)$, where $-\pi/2 \leq t \leq -\pi/2$, then I get another ludicrous integral with $e^{a\cos(t)}\sin(a\sin(t))a\cos(t)\:dt$ as $P \:dx$, which seems insane to solve. So I think I may be missing some trick in doing this problem. What am I missing and how should I do this?","So I'm practicing a few problems and I can't get this one - $$P(x,y) = e^x \sin(y) \\ Q(x,y) = e^x \cos(y)$$ $C$ is the right hand loop of the graph of the polar equation $r^2 = 4\cos(\theta)$ I want to evaluate: $$\int_{C}{P(x,y)\:dx+Q(x,y)\:dy}$$ Now I tried the right hand side of Green's theorem, but it's difficult because  $\frac{dp}{dy}$ in polar has a $\cos(r\sin(\theta))$ term in it. If I just try parametrizing with $x = a\sin(t)$ and $y = a\cos(t)$, where $-\pi/2 \leq t \leq -\pi/2$, then I get another ludicrous integral with $e^{a\cos(t)}\sin(a\sin(t))a\cos(t)\:dt$ as $P \:dx$, which seems insane to solve. So I think I may be missing some trick in doing this problem. What am I missing and how should I do this?",,"['multivariable-calculus', 'polar-coordinates']"
48,Triple Integral of a cored apple,Triple Integral of a cored apple,,"Use a triple integral in spherical coordinates to ﬁnd the volume, $V$, of a cored apple, which consists of a sphere of radius $2$, $x^2 + y^2 + z^2 = 4$, and a cylindrical hole of radius one, $x^2 + y^2 = 1$. In other words, ﬁnd the volume of the sphere with the cylinder removed. I know that $\theta$ goes from $0$ to $2\pi$ since the sphere is complete. What I don't understand is how to: 1) Convert rectangular coordinates to spherical coordinates. My textbook gives a terrible explanation. 2) Find the bounds of the triple integral. As I said, $\theta$ goes from $0$ to $2\pi$. I have no clue as to how to find $p$ or $\varphi$. The center being removed is also throwing me. Could some explain as to how to go about this? I'm not necessarily looking for answer but a means to solve this problem. We have an exam next week and I would really love to be able to understand it.","Use a triple integral in spherical coordinates to ﬁnd the volume, $V$, of a cored apple, which consists of a sphere of radius $2$, $x^2 + y^2 + z^2 = 4$, and a cylindrical hole of radius one, $x^2 + y^2 = 1$. In other words, ﬁnd the volume of the sphere with the cylinder removed. I know that $\theta$ goes from $0$ to $2\pi$ since the sphere is complete. What I don't understand is how to: 1) Convert rectangular coordinates to spherical coordinates. My textbook gives a terrible explanation. 2) Find the bounds of the triple integral. As I said, $\theta$ goes from $0$ to $2\pi$. I have no clue as to how to find $p$ or $\varphi$. The center being removed is also throwing me. Could some explain as to how to go about this? I'm not necessarily looking for answer but a means to solve this problem. We have an exam next week and I would really love to be able to understand it.",,"['multivariable-calculus', 'integration']"
49,Line integral over ellipse in first quadrant,Line integral over ellipse in first quadrant,,"Evaluate $ \int_{C} xy\,ds $ where C is the arc of the ellipse $ \frac{x^2}{a^2} + \frac{y^2}{b^2} = 1 $ in the first quadrant. Let $x = a\cos t$ and $ y= b\sin t$ and use a substitution of $ u = a^2 \sin^2t + b^2\cos^2t $ to simply the expression under the sqrt root when dealing with $ ds$. I eventually get to $$\frac{ab}{2(a^2-b^2)} [\frac{2}{3} \sqrt{(a^2\sin^2t + b^2 \cos^2t)^3}]$$evaluated between $\pi/2$ and $0$. Should I take $\sqrt{a^6} = a^3$ here? (my thoughts being that $ a>0 $  in first quadrant)","Evaluate $ \int_{C} xy\,ds $ where C is the arc of the ellipse $ \frac{x^2}{a^2} + \frac{y^2}{b^2} = 1 $ in the first quadrant. Let $x = a\cos t$ and $ y= b\sin t$ and use a substitution of $ u = a^2 \sin^2t + b^2\cos^2t $ to simply the expression under the sqrt root when dealing with $ ds$. I eventually get to $$\frac{ab}{2(a^2-b^2)} [\frac{2}{3} \sqrt{(a^2\sin^2t + b^2 \cos^2t)^3}]$$evaluated between $\pi/2$ and $0$. Should I take $\sqrt{a^6} = a^3$ here? (my thoughts being that $ a>0 $  in first quadrant)",,['multivariable-calculus']
50,Rank of the differential of a composition,Rank of the differential of a composition,,"Let $U$ be an open subset of $\mathbb{R}^2$ and $f:U\to \mathbb{R}$ a differentiable function. Let $S=\{(x,y,f(x,y)): x\in U\}$ be the graph of $f$. Let $V$ be an open subset of $\mathbb{R}^2$ and $g:V\to \mathbb{R}^3$ be a one-to-one differentiable map such that $g(V)=S$. Assume that $dg$ is of rank 2 at each point of $V$. Let $\pi(x,y,z)=(x,y)$ be the projection. My question is: Must $\pi\circ dg$ be of rank 2 at each point of $V$?","Let $U$ be an open subset of $\mathbb{R}^2$ and $f:U\to \mathbb{R}$ a differentiable function. Let $S=\{(x,y,f(x,y)): x\in U\}$ be the graph of $f$. Let $V$ be an open subset of $\mathbb{R}^2$ and $g:V\to \mathbb{R}^3$ be a one-to-one differentiable map such that $g(V)=S$. Assume that $dg$ is of rank 2 at each point of $V$. Let $\pi(x,y,z)=(x,y)$ be the projection. My question is: Must $\pi\circ dg$ be of rank 2 at each point of $V$?",,"['real-analysis', 'differential-geometry', 'multivariable-calculus']"
51,Is there a measure,Is there a measure,,"Is there a measure $\nu$ on $[0,\infty)$ such that  $$ \ln x=\int_{0}^{\infty}d\nu\left(y\right)/\left(x+y-1\right)? $$ Thanks for any helpful answers!","Is there a measure $\nu$ on $[0,\infty)$ such that  $$ \ln x=\int_{0}^{\infty}d\nu\left(y\right)/\left(x+y-1\right)? $$ Thanks for any helpful answers!",,"['real-analysis', 'analysis', 'functional-analysis', 'measure-theory', 'multivariable-calculus']"
52,Line Integral set up,Line Integral set up,,"Integrate r = $\sqrt{x^2+y^2}$ from (0,0) to (1,1) along the path  (0,0) => (1,0) => (1,1) My professor tells me to let $dr = dxi +dyj$ where $i $ and $j$ are the standard unit vectors. I don't really see how this is possible with a scalar function. I have been parametrizing: $$\int_Cr(x,y)dr = \int_{t_0}^tr(x_{(t)},y_{(t)})\sqrt{(\frac{dx}{dt})^2 + (\frac{dy}{dt})^2}dt$$ and i get $C_1$ (0,0) => (1,0) $$x = t;dx/dt =1;y =0$$ But my main confusion is when I try to parameterize C2: (1,0)=>(1,1) I get an integral which I cannot solve by any conventional analytical methods:  x = 1, y = t, dy/dt = 1 $$\int\sqrt{1 + t^2}dt$$ So I have surely done something wrong, any help would be greatly appreciated.","Integrate r = $\sqrt{x^2+y^2}$ from (0,0) to (1,1) along the path  (0,0) => (1,0) => (1,1) My professor tells me to let $dr = dxi +dyj$ where $i $ and $j$ are the standard unit vectors. I don't really see how this is possible with a scalar function. I have been parametrizing: $$\int_Cr(x,y)dr = \int_{t_0}^tr(x_{(t)},y_{(t)})\sqrt{(\frac{dx}{dt})^2 + (\frac{dy}{dt})^2}dt$$ and i get $C_1$ (0,0) => (1,0) $$x = t;dx/dt =1;y =0$$ But my main confusion is when I try to parameterize C2: (1,0)=>(1,1) I get an integral which I cannot solve by any conventional analytical methods:  x = 1, y = t, dy/dt = 1 $$\int\sqrt{1 + t^2}dt$$ So I have surely done something wrong, any help would be greatly appreciated.",,['multivariable-calculus']
53,Jacobian with Chain rules,Jacobian with Chain rules,,"$$S\subseteq \mathbb{R}^N, U\subseteq \mathbb{R}^k, \phi\in C^1(U,\mathbb{R}^N) \ \text{ s.t.} \ \phi(U)=S.$$ $J_\psi$ is the Jacobian matrix. Let $V\subseteq\mathbb{R}^k$ be open, and let $\psi:V\to S$ a bijection so that $h:=\psi^{-1}\circ\phi\in C^1(U,V)$ and $h^{-1}=\phi^{-1}\circ\psi\in C^1(V,U)$ I'm trying to get to the end statement that $J_\psi(h(x))J_h(x)=J_\phi(x)$ for all $x\in U$. I'm thinking along the lines of chain rules, but the notation is strange as it is. I think that $J_\phi(x)$ would be the values of $\phi$ with respect to the partial derivatives $x_i$, but it's sending me in circles. Ideas?","$$S\subseteq \mathbb{R}^N, U\subseteq \mathbb{R}^k, \phi\in C^1(U,\mathbb{R}^N) \ \text{ s.t.} \ \phi(U)=S.$$ $J_\psi$ is the Jacobian matrix. Let $V\subseteq\mathbb{R}^k$ be open, and let $\psi:V\to S$ a bijection so that $h:=\psi^{-1}\circ\phi\in C^1(U,V)$ and $h^{-1}=\phi^{-1}\circ\psi\in C^1(V,U)$ I'm trying to get to the end statement that $J_\psi(h(x))J_h(x)=J_\phi(x)$ for all $x\in U$. I'm thinking along the lines of chain rules, but the notation is strange as it is. I think that $J_\phi(x)$ would be the values of $\phi$ with respect to the partial derivatives $x_i$, but it's sending me in circles. Ideas?",,"['real-analysis', 'matrices', 'multivariable-calculus']"
54,Proving the limit - multiple variable differentiation,Proving the limit - multiple variable differentiation,,"I'm working through an advanced calculus book and want to be certain I understand the idea behind proving limits. This is not homework, I'm just a statistician looking to learn more about mathematics. The exercise I'm concerned with proving is as follows: $$\begin{aligned} \lim_{(x,y)→(0,0)}  \frac{x^3y}{x^2 + y^4} \\\ \end{aligned}$$ My understanding is that I can choose a value to substitute in for y that allows for some easy cancellation that proves the limit equals 0.  For instance: $$\begin{aligned} x= y^2 \ ;   \frac{(y^2)^3y}{(y^2)^2 + y^4} \\\ \end{aligned}$$ From here, we have: $$\begin{aligned} \frac{y^7}{y^4(1 + 1)} \\\ \end{aligned}$$ Then as y→0 this simplifies to: $$\begin{aligned} \frac{0^3}{2} = 0 \\\ \end{aligned}$$ Is this how the limit could/would be proved?","I'm working through an advanced calculus book and want to be certain I understand the idea behind proving limits. This is not homework, I'm just a statistician looking to learn more about mathematics. The exercise I'm concerned with proving is as follows: $$\begin{aligned} \lim_{(x,y)→(0,0)}  \frac{x^3y}{x^2 + y^4} \\\ \end{aligned}$$ My understanding is that I can choose a value to substitute in for y that allows for some easy cancellation that proves the limit equals 0.  For instance: $$\begin{aligned} x= y^2 \ ;   \frac{(y^2)^3y}{(y^2)^2 + y^4} \\\ \end{aligned}$$ From here, we have: $$\begin{aligned} \frac{y^7}{y^4(1 + 1)} \\\ \end{aligned}$$ Then as y→0 this simplifies to: $$\begin{aligned} \frac{0^3}{2} = 0 \\\ \end{aligned}$$ Is this how the limit could/would be proved?",,"['limits', 'multivariable-calculus', 'proof-writing']"
55,Evaluate the surface integral,Evaluate the surface integral,,"I need to solve the following: $$\iint_S x^2 z ~d\rho,$$ where $S$ is part of the cylinder $x^2 + z^2 = 1$ that is above the $xy$-plane and between the planes $y = 0$ and $y = 2$. So it looks like I have portion of the cylinder... but again dont know how to setup the integral. I know I have to put the integrand in parametric form first and then I can plug that into the integrand and proceed to integrate.. but the issue is getting the integral limits and the parametric form!","I need to solve the following: $$\iint_S x^2 z ~d\rho,$$ where $S$ is part of the cylinder $x^2 + z^2 = 1$ that is above the $xy$-plane and between the planes $y = 0$ and $y = 2$. So it looks like I have portion of the cylinder... but again dont know how to setup the integral. I know I have to put the integrand in parametric form first and then I can plug that into the integrand and proceed to integrate.. but the issue is getting the integral limits and the parametric form!",,['multivariable-calculus']
56,Evaluate the triple integral.,Evaluate the triple integral.,,"Evaluate the triple integral of $x=y^2$  over the region bounded by $z=x$, $z=0$ and $x=1$  My order of integration was $dx\:dy\:dz$. I want to calculate the volume of this surface. I solved it for $dz\:dy\:dx$ and it was: $$V=\int_0^1\int_{-\sqrt{x}}^{\sqrt{x}}\int_{0}^{x}\:dz\:dy\:dx$$ And for $dz\:dx\:dy$ would be this: $$V=\int_{-1}^{1}\int_{y^2}^{1}\int_{0}^{x}dz\:dx\:dy$$ I tried to solve it and the result is this: $$V=\int_{0}^{1}\int_{-\sqrt{x}}^{\sqrt{x}}\int_{z}^{1}dx\:dy\:dz + \int_{0}^{1}\int_{-\frac{1}{2}}^{\frac{1}{2}}\int_{y^2}^{1}dx\:dy\:dz$$ But i think its wrong please advice me the best solution . I wanted to post the  shape of this surface in 3-dimensional region but I couldn't because I am new user.","Evaluate the triple integral of $x=y^2$  over the region bounded by $z=x$, $z=0$ and $x=1$  My order of integration was $dx\:dy\:dz$. I want to calculate the volume of this surface. I solved it for $dz\:dy\:dx$ and it was: $$V=\int_0^1\int_{-\sqrt{x}}^{\sqrt{x}}\int_{0}^{x}\:dz\:dy\:dx$$ And for $dz\:dx\:dy$ would be this: $$V=\int_{-1}^{1}\int_{y^2}^{1}\int_{0}^{x}dz\:dx\:dy$$ I tried to solve it and the result is this: $$V=\int_{0}^{1}\int_{-\sqrt{x}}^{\sqrt{x}}\int_{z}^{1}dx\:dy\:dz + \int_{0}^{1}\int_{-\frac{1}{2}}^{\frac{1}{2}}\int_{y^2}^{1}dx\:dy\:dz$$ But i think its wrong please advice me the best solution . I wanted to post the  shape of this surface in 3-dimensional region but I couldn't because I am new user.",,"['integration', 'multivariable-calculus']"
57,Determining limits on variable change,Determining limits on variable change,,"Ok, I've seen some questions similar to mine but it didn't really get me what I want so I figured I'd ask. I was given the following problem to solve by making the change of variables $ u = x-y, v = x+y$ in the following integral $ I = \int_0^1dy\int_0^{1-y}e^\frac{x-y}{x+y}dx$ This is an iterated integral over an region of the x-y plane. The Jacobian of the transformation is 1/2 so the integral becomes $I = \frac{1}{2}\int\int_Ae^\frac{u}{v}dudv $ At this point I had some difficulties. I eventually realized that if I played with the numbers, that u would have upper and lower bounds of 1 and -1 while v would have upper and lower bounds of 1 and 0. So my first attempt was this: $ I = \frac{1}{2} \int_0^1dv\int_{-1}^1e^\frac{u}{v}du$ however evaluating this integral was practically impossible (if you don't believe me, try finding an antiderivitive for $xsinh(1/x)$ with elementary techniques!), eventually I realized the integral would simplify if changed the limits as follows: $ I = \frac{1}{2} \int_0^1dv\int_{-v}^ve^\frac{u}{v}du$ this yielded the correct answer [$I=0.5sinh(1)$] however I would like to know how to determine how to find regions of integration on change of variables in ways other than guessing at the answer!  Is there an algorithmic procedure? Is there an easy graphical procedure? I know I could graph in the u-v plane but that seems like a lot more work than necessary. edit: the way I am thinking about this now is that the first set of boundaries are incorrect because they correspond to a region that is a box (rectangle) in the uv plane. Cleary, the region is triangular in the x-y plane, and I'm assuming it is also triangular in the uv plane. How do I determine the dependence between u and v in of this region?","Ok, I've seen some questions similar to mine but it didn't really get me what I want so I figured I'd ask. I was given the following problem to solve by making the change of variables $ u = x-y, v = x+y$ in the following integral $ I = \int_0^1dy\int_0^{1-y}e^\frac{x-y}{x+y}dx$ This is an iterated integral over an region of the x-y plane. The Jacobian of the transformation is 1/2 so the integral becomes $I = \frac{1}{2}\int\int_Ae^\frac{u}{v}dudv $ At this point I had some difficulties. I eventually realized that if I played with the numbers, that u would have upper and lower bounds of 1 and -1 while v would have upper and lower bounds of 1 and 0. So my first attempt was this: $ I = \frac{1}{2} \int_0^1dv\int_{-1}^1e^\frac{u}{v}du$ however evaluating this integral was practically impossible (if you don't believe me, try finding an antiderivitive for $xsinh(1/x)$ with elementary techniques!), eventually I realized the integral would simplify if changed the limits as follows: $ I = \frac{1}{2} \int_0^1dv\int_{-v}^ve^\frac{u}{v}du$ this yielded the correct answer [$I=0.5sinh(1)$] however I would like to know how to determine how to find regions of integration on change of variables in ways other than guessing at the answer!  Is there an algorithmic procedure? Is there an easy graphical procedure? I know I could graph in the u-v plane but that seems like a lot more work than necessary. edit: the way I am thinking about this now is that the first set of boundaries are incorrect because they correspond to a region that is a box (rectangle) in the uv plane. Cleary, the region is triangular in the x-y plane, and I'm assuming it is also triangular in the uv plane. How do I determine the dependence between u and v in of this region?",,"['multivariable-calculus', 'integration']"
58,Jacobian of $A^{-1}b$,Jacobian of,A^{-1}b,"I need to calculate the Jacobian $\frac{df}{dx}$ of $f=A^{-1}b$ where $A$ and $b$ are a function of $x$, the variable towards to differentiate. I thought $$\frac{df}{dx} = \frac{dA^{-1}}{dx} b  + A^{-1}\frac{db}{dx}$$ by the product rule,  and since $A^{-1}A=I$, $$\frac{dA^{-1}}{dx} = A^{-1} \frac{dA}{dx}  A^{-1}.$$ Now the last thing i thought is $$\frac{dA}{dx} = \frac{dA}{dx_1} + \frac{dA}{dx_2} + \frac{dA}{dx_3} + \cdots $$ The last step is to calculate the Jacobian of a matrix. However, if I try this for a simple example, I get a wrong answer. Can anyone see where I make the mistake? How can I calculate the Jacobian of $f=A^{-1}b$ correct if I cannot analytically invert $A$ (I can only do that numerically)?","I need to calculate the Jacobian $\frac{df}{dx}$ of $f=A^{-1}b$ where $A$ and $b$ are a function of $x$, the variable towards to differentiate. I thought $$\frac{df}{dx} = \frac{dA^{-1}}{dx} b  + A^{-1}\frac{db}{dx}$$ by the product rule,  and since $A^{-1}A=I$, $$\frac{dA^{-1}}{dx} = A^{-1} \frac{dA}{dx}  A^{-1}.$$ Now the last thing i thought is $$\frac{dA}{dx} = \frac{dA}{dx_1} + \frac{dA}{dx_2} + \frac{dA}{dx_3} + \cdots $$ The last step is to calculate the Jacobian of a matrix. However, if I try this for a simple example, I get a wrong answer. Can anyone see where I make the mistake? How can I calculate the Jacobian of $f=A^{-1}b$ correct if I cannot analytically invert $A$ (I can only do that numerically)?",,"['matrices', 'multivariable-calculus']"
59,Possible Calibration Equations [Misc],Possible Calibration Equations [Misc],,"this is my first post so hopefully this topic is considered OK. Background: In class we were using a laser (mounted on a planar robot) to measure various profiles of a sample underneath. The system was already calibrated when I used it but I started wondering how this calibration would be performed (i.e. the math behind it). Here is what I came up with for a setup (simplified): Setup : Assume the calibration gauge is a flat plate (blue, at angle $\alpha$ from x-axis and angle $\beta$ from y-axis, ideally it would be parallel to the x-y plane), then there will be: some initial height offset at the robot's origin ($\vec{Z_0}$, unknown), the laser's position attached to the planar robot ($\vec{P}$, known), the laser's measurement vector ($\vec{D_m}$, |$\vec{D_m}|$ known), the ""true"" height directly below the laser/robot ($\vec{D_t}$, unknown), and the error between the two ($\vec{E}$, unknown). Ideally, the laser is to be mounted such that the output beam is parallel to the z-axis but I'm sure there are some mounting angle errors (error angle $\phi$ from the z-axis in x-z plane and angle $\psi$ from the z-axis in the y-z plane). $i$ is the point where the laser makes contact with the plate (x,y,z). The ""known"" variables are |$\vec{D_m}$|, P_x, and P_y. The unknown variables I need to ""calibrate"" the system (so I can calculate $\vec{D_t}$) are $\vec{Z_0}$ and the angles $\alpha, \beta, \phi$, and $\psi$. My Attempt to Figure It Out : First, I found where the laser would intersect the plate/plane based on the robot's (x,y) position: $i = ( P_x + \Phi\frac{AP_x + BP_y + Z_0}{1-A\Phi-B\Psi}, P_y + \Psi\frac{AP_x + BP_y + Z_0}{1-A\Phi-B\Psi}, \frac{AP_x + BP_y + Z_0}{1-A\Phi-B\Psi})$ where $A$ is the slope of the plane along the x-direction, $B$ is the slope of the plane along the y-direction, $\Phi$ is the slope of $\vec{D_m}$'s x-component along the z-direction, and $\Psi$ is the slope of $\vec{D_m}$'s y-component along the z-direction. Using these instead of the angles made the equations cleaner. The measurement vector: $\vec{D_m} = (\Phi\frac{AP_x + BP_y + Z_0}{1-A\Phi-B\Psi},\Psi\frac{AP_x + BP_y + Z_0}{1-A\Phi-B\Psi}, \frac{AP_x + BP_y + Z_0}{1-A\Phi-B\Psi})$ The Calibration Equations (need 5) Take measurements along x-axis, regress to find slope, set equal to $\frac{\partial|\vec{D_m}|}{\partial P_x}$. Take measurements along y-axis, regress to find slope, set equal to $\frac{\partial|\vec{D_m}|}{\partial P_y}$. Move to origin ($P_x = P_y = 0$ to eliminates variables), take measurement set equal to |$\vec{D_m}$|. Have two points on the plane ($p_1, p_2$) with a known distance between them ($\Delta$). Move the robot so that the laser dot hits the first point, save ($P_{x1}, P_{y1}$), move the laser to the second point, save that position information. Use $P_{x1}, P_{y1}, P_{x2}, P_{y2}$ in the equation: $\Delta = |i(P_{x1}, P_{y1}) - i(P_{x2}, P_{y2})|$. ...? I'm stuck. Final Thoughts I thought this was pretty interesting and not being able to solve it has been bugging me. :) I have no idea how the system was actually calibrated but this is how I set it up. If there is a far easier way or if someone knows how these types of systems are actually calibrated please let me know (it would be an interesting read) but I would also like to figure out that fifth equation using my way. I'm not a math major so I figured this would be the place to go! Thanks!","this is my first post so hopefully this topic is considered OK. Background: In class we were using a laser (mounted on a planar robot) to measure various profiles of a sample underneath. The system was already calibrated when I used it but I started wondering how this calibration would be performed (i.e. the math behind it). Here is what I came up with for a setup (simplified): Setup : Assume the calibration gauge is a flat plate (blue, at angle $\alpha$ from x-axis and angle $\beta$ from y-axis, ideally it would be parallel to the x-y plane), then there will be: some initial height offset at the robot's origin ($\vec{Z_0}$, unknown), the laser's position attached to the planar robot ($\vec{P}$, known), the laser's measurement vector ($\vec{D_m}$, |$\vec{D_m}|$ known), the ""true"" height directly below the laser/robot ($\vec{D_t}$, unknown), and the error between the two ($\vec{E}$, unknown). Ideally, the laser is to be mounted such that the output beam is parallel to the z-axis but I'm sure there are some mounting angle errors (error angle $\phi$ from the z-axis in x-z plane and angle $\psi$ from the z-axis in the y-z plane). $i$ is the point where the laser makes contact with the plate (x,y,z). The ""known"" variables are |$\vec{D_m}$|, P_x, and P_y. The unknown variables I need to ""calibrate"" the system (so I can calculate $\vec{D_t}$) are $\vec{Z_0}$ and the angles $\alpha, \beta, \phi$, and $\psi$. My Attempt to Figure It Out : First, I found where the laser would intersect the plate/plane based on the robot's (x,y) position: $i = ( P_x + \Phi\frac{AP_x + BP_y + Z_0}{1-A\Phi-B\Psi}, P_y + \Psi\frac{AP_x + BP_y + Z_0}{1-A\Phi-B\Psi}, \frac{AP_x + BP_y + Z_0}{1-A\Phi-B\Psi})$ where $A$ is the slope of the plane along the x-direction, $B$ is the slope of the plane along the y-direction, $\Phi$ is the slope of $\vec{D_m}$'s x-component along the z-direction, and $\Psi$ is the slope of $\vec{D_m}$'s y-component along the z-direction. Using these instead of the angles made the equations cleaner. The measurement vector: $\vec{D_m} = (\Phi\frac{AP_x + BP_y + Z_0}{1-A\Phi-B\Psi},\Psi\frac{AP_x + BP_y + Z_0}{1-A\Phi-B\Psi}, \frac{AP_x + BP_y + Z_0}{1-A\Phi-B\Psi})$ The Calibration Equations (need 5) Take measurements along x-axis, regress to find slope, set equal to $\frac{\partial|\vec{D_m}|}{\partial P_x}$. Take measurements along y-axis, regress to find slope, set equal to $\frac{\partial|\vec{D_m}|}{\partial P_y}$. Move to origin ($P_x = P_y = 0$ to eliminates variables), take measurement set equal to |$\vec{D_m}$|. Have two points on the plane ($p_1, p_2$) with a known distance between them ($\Delta$). Move the robot so that the laser dot hits the first point, save ($P_{x1}, P_{y1}$), move the laser to the second point, save that position information. Use $P_{x1}, P_{y1}, P_{x2}, P_{y2}$ in the equation: $\Delta = |i(P_{x1}, P_{y1}) - i(P_{x2}, P_{y2})|$. ...? I'm stuck. Final Thoughts I thought this was pretty interesting and not being able to solve it has been bugging me. :) I have no idea how the system was actually calibrated but this is how I set it up. If there is a far easier way or if someone knows how these types of systems are actually calibrated please let me know (it would be an interesting read) but I would also like to figure out that fifth equation using my way. I'm not a math major so I figured this would be the place to go! Thanks!",,['multivariable-calculus']
60,Why these conditions make this map open?,Why these conditions make this map open?,,"If $A \subset \mathbb{R}^n$ is an open set and $g: A \to \mathbb{R}^n$ is an injective continuously differentiable function such that $\forall x \in A, \, \det g'(x) \neq 0$, does $g(U)$ is open for each $U \subset A$ open? Why? This is about p. 67 of Spivak's Calculus on Manifolds (down of the page), where he says: ""the collection of all $g(U)$ is an open cover of $g(A)$"".","If $A \subset \mathbb{R}^n$ is an open set and $g: A \to \mathbb{R}^n$ is an injective continuously differentiable function such that $\forall x \in A, \, \det g'(x) \neq 0$, does $g(U)$ is open for each $U \subset A$ open? Why? This is about p. 67 of Spivak's Calculus on Manifolds (down of the page), where he says: ""the collection of all $g(U)$ is an open cover of $g(A)$"".",,"['general-topology', 'multivariable-calculus']"
61,Angle between vectors when their dot product and norm of cross product are equal.,Angle between vectors when their dot product and norm of cross product are equal.,,I had a question in the final exam that asked what the angle between vectors a and b is if:   $$\vec a \cdot \vec b=|\vec a\times\vec b| $$ Any hints please.,I had a question in the final exam that asked what the angle between vectors a and b is if:   $$\vec a \cdot \vec b=|\vec a\times\vec b| $$ Any hints please.,,['multivariable-calculus']
62,Minimizing a two variable functions on a compact when one of the partial differentiate never gets null.,Minimizing a two variable functions on a compact when one of the partial differentiate never gets null.,,"I have a function $f$ defined on $]0,+\infty[\times[0,1] \rightarrow [0,1]$. For the moment let us say that it is smooth enough. I am looking to find a minimum of this function. What I was told to do (but I do not think it is right, though I cannot find a counter example) is to: Compute $\frac{\partial f}{\partial x}(x,y) = 0$, this gives me a unique $x_{\min} = h(y)$, then to study $g: y \mapsto f(h(y),y)$, and to see when it is minimum. My questions: Is it right? If yes, do you know the theorem I should look for? Otherwise, do you have a counter example (even if it means different hypothesis)? If it is true with a function smooth enough, can you tell me the minimal hypothesis needed (and a minimal counter example)? EDIT: I am especially interested in the minimal hypothesis that make this true, and a minimal counter example when those hypothesis are not matched (questions 3 and 4). Thanks Additional informations: there is $a,b>0$, $\frac{\partial f}{\partial y}(x,y) = a - bx$. (meaning the minimum seems to be necessarily for $y=0$ or $y=1$, depending on $x$ so here it works). I know of the theorem stating that we should look for every point $(x_0,y_0)$ such that $\frac{\partial f}{\partial x}(x_0,y_0) =\frac{\partial f}{\partial y}(x_0,y_0) = 0$, but in this example we never have this condition for the second variable. Do you know of another theorem valid on a compact?","I have a function $f$ defined on $]0,+\infty[\times[0,1] \rightarrow [0,1]$. For the moment let us say that it is smooth enough. I am looking to find a minimum of this function. What I was told to do (but I do not think it is right, though I cannot find a counter example) is to: Compute $\frac{\partial f}{\partial x}(x,y) = 0$, this gives me a unique $x_{\min} = h(y)$, then to study $g: y \mapsto f(h(y),y)$, and to see when it is minimum. My questions: Is it right? If yes, do you know the theorem I should look for? Otherwise, do you have a counter example (even if it means different hypothesis)? If it is true with a function smooth enough, can you tell me the minimal hypothesis needed (and a minimal counter example)? EDIT: I am especially interested in the minimal hypothesis that make this true, and a minimal counter example when those hypothesis are not matched (questions 3 and 4). Thanks Additional informations: there is $a,b>0$, $\frac{\partial f}{\partial y}(x,y) = a - bx$. (meaning the minimum seems to be necessarily for $y=0$ or $y=1$, depending on $x$ so here it works). I know of the theorem stating that we should look for every point $(x_0,y_0)$ such that $\frac{\partial f}{\partial x}(x_0,y_0) =\frac{\partial f}{\partial y}(x_0,y_0) = 0$, but in this example we never have this condition for the second variable. Do you know of another theorem valid on a compact?",,"['real-analysis', 'multivariable-calculus', 'partial-differential-equations']"
63,How to arrive at Stokes's theorem from Green's theorem?,How to arrive at Stokes's theorem from Green's theorem?,,I would like to verify the identity $$ \oint \vec F  \cdot (\hat i dx  + \hat j dy) + \oint \vec F \cdot (\hat i dx  + \hat j dy) + \oint \vec F  \cdot (\hat i dx  + \hat j dy)  = \oint \vec F \cdot (\hat i dx + \hat j dy + \hat k dz) $$ If it is incorrect then what would be the correct identity. Green's theorem is special case of Stokes's theorem. How do we arrive at Stokes's theorem using Green's theorem?,I would like to verify the identity $$ \oint \vec F  \cdot (\hat i dx  + \hat j dy) + \oint \vec F \cdot (\hat i dx  + \hat j dy) + \oint \vec F  \cdot (\hat i dx  + \hat j dy)  = \oint \vec F \cdot (\hat i dx + \hat j dy + \hat k dz) $$ If it is incorrect then what would be the correct identity. Green's theorem is special case of Stokes's theorem. How do we arrive at Stokes's theorem using Green's theorem?,,"['integration', 'multivariable-calculus']"
64,Calculate the volume between $z=x^2+y^2$ and $z=2ax+2by$,Calculate the volume between  and,z=x^2+y^2 z=2ax+2by,"I'm trying to calculate the volume between the surfaces $z=x^2+y^2$ and $z=2ax+2by$ where $a>0,b>0$. Here's what I've tried: First I noticed the projection of the volume to the xy plane is a circle: $(x-a)^2+(y-b)^2\leq a^2+b^2$. Using this I simplified the calculation of the integral for the volume a little. Marking $B$ as the circle we get that the volume is: $$\iint_{}^{B} (2ax+2ay-x^2-y^2) = \iint_{}^{B} (a^2+b^2)-\iint_{}^{B} ((x-a)^2+(y-b)^2) $$ Using the symmetry of the circle we get: $$\iint_{}^{B} ((x-a)^2+(y-b)^2) = 2\iint_{}^{B} ((x-a)^2)$$ And we can also use the formula for the area of a circle to get: $$\iint_{}^{B} (a^2+b^2) = \pi (a^2+b^2)^2$$ So all I have left to do is calculate $\iint_{}^{B} ((x-a)^2)$, but this is where I get stuck. Trying to do it using iterated integrals becomes too complex (we have only covered Cartesian coordinates, so I can't use something like polar coordinates here). I know the result is supposed to be $(1/2)\pi (a^2+b^2)^2$. Assistance would be appreciated. Thanks!","I'm trying to calculate the volume between the surfaces $z=x^2+y^2$ and $z=2ax+2by$ where $a>0,b>0$. Here's what I've tried: First I noticed the projection of the volume to the xy plane is a circle: $(x-a)^2+(y-b)^2\leq a^2+b^2$. Using this I simplified the calculation of the integral for the volume a little. Marking $B$ as the circle we get that the volume is: $$\iint_{}^{B} (2ax+2ay-x^2-y^2) = \iint_{}^{B} (a^2+b^2)-\iint_{}^{B} ((x-a)^2+(y-b)^2) $$ Using the symmetry of the circle we get: $$\iint_{}^{B} ((x-a)^2+(y-b)^2) = 2\iint_{}^{B} ((x-a)^2)$$ And we can also use the formula for the area of a circle to get: $$\iint_{}^{B} (a^2+b^2) = \pi (a^2+b^2)^2$$ So all I have left to do is calculate $\iint_{}^{B} ((x-a)^2)$, but this is where I get stuck. Trying to do it using iterated integrals becomes too complex (we have only covered Cartesian coordinates, so I can't use something like polar coordinates here). I know the result is supposed to be $(1/2)\pi (a^2+b^2)^2$. Assistance would be appreciated. Thanks!",,"['integration', 'multivariable-calculus']"
65,What's wrong in computing the gradient like this?,What's wrong in computing the gradient like this?,,"Say $u(x,y)=x^2+y^2$. Its gradient at (1,1) is (2,2). Since I'm sure the gradient is directed towards y=x direction. I set y=x. Then $u(x,y)=2x^2=2y^2$. Now compute the gradient again. It's $\nabla u=(\partial_x u,\partial_y u)=(4x,4y)=(4,4)$. So what goes wrong here?","Say $u(x,y)=x^2+y^2$. Its gradient at (1,1) is (2,2). Since I'm sure the gradient is directed towards y=x direction. I set y=x. Then $u(x,y)=2x^2=2y^2$. Now compute the gradient again. It's $\nabla u=(\partial_x u,\partial_y u)=(4x,4y)=(4,4)$. So what goes wrong here?",,['multivariable-calculus']
66,How do I apply the implicit function theorem to find the derivative wrt to a parameter in this specific equation?,How do I apply the implicit function theorem to find the derivative wrt to a parameter in this specific equation?,,$λ^2=\frac{p(2-2p)}{(p^2+a)}$ $p=\frac{(2λ-2+λb)}{(2λ+4)(λ-1)} $ Where $a$ and $b$ are positive parameters. How can I find $∂p/∂a$ and $∂λ/∂a$ ? A little background: this two equations are obtained from the first order conditions of a maximization problem. Thanks!,$λ^2=\frac{p(2-2p)}{(p^2+a)}$ $p=\frac{(2λ-2+λb)}{(2λ+4)(λ-1)} $ Where $a$ and $b$ are positive parameters. How can I find $∂p/∂a$ and $∂λ/∂a$ ? A little background: this two equations are obtained from the first order conditions of a maximization problem. Thanks!,,"['multivariable-calculus', 'implicit-differentiation']"
67,How can I prove that the inequality,How can I prove that the inequality,,"I want to prove that for any $g \in C_0^\infty$, $g\colon[0,\infty) \times \mathbb R \rightarrow \mathbb R$, and $g = g(t,x)$, $$\sup_{x\in \mathbb R} |g(t,x)| \leqslant \int_\mathbb R \hspace{2mm} \left(\left|g(t,x)\right| + \left| \frac{\partial g(t,x)}{\partial x} \right| \right) \hspace{2mm}dx$$ for any $t$.","I want to prove that for any $g \in C_0^\infty$, $g\colon[0,\infty) \times \mathbb R \rightarrow \mathbb R$, and $g = g(t,x)$, $$\sup_{x\in \mathbb R} |g(t,x)| \leqslant \int_\mathbb R \hspace{2mm} \left(\left|g(t,x)\right| + \left| \frac{\partial g(t,x)}{\partial x} \right| \right) \hspace{2mm}dx$$ for any $t$.",,"['real-analysis', 'multivariable-calculus']"
68,Verifying Stokes Theorem,Verifying Stokes Theorem,,"The first part of the question states prove, $$\oint_{c} r \wedge dr= 2 \iint_{s} ds$$ This I have done using stokes theorem, however stuck on the next part which states, verify this forumla is also applicable when s has a mild singularity, by evaluating both sides of the equation for the case when s is the cone, ${(x,y,z): (1-z)^{2}=x^{2}+y^{2}, x^{2}+y^{2} \leq 1, z\geq0}$ and c is the circle $x^{2}+y^{2}=1$ in the xy plane. Many thanks in advance.","The first part of the question states prove, $$\oint_{c} r \wedge dr= 2 \iint_{s} ds$$ This I have done using stokes theorem, however stuck on the next part which states, verify this forumla is also applicable when s has a mild singularity, by evaluating both sides of the equation for the case when s is the cone, ${(x,y,z): (1-z)^{2}=x^{2}+y^{2}, x^{2}+y^{2} \leq 1, z\geq0}$ and c is the circle $x^{2}+y^{2}=1$ in the xy plane. Many thanks in advance.",,"['calculus', 'multivariable-calculus']"
69,A quick question about the Hessian matrix,A quick question about the Hessian matrix,,A function $f$ that has continuous third order partial derivatives in $\mathbb{R}^n$. I'm just wondering that since the partial derivatives are continuous then the Hessian matrix is symmetric. Is that correct? Thanks.,A function $f$ that has continuous third order partial derivatives in $\mathbb{R}^n$. I'm just wondering that since the partial derivatives are continuous then the Hessian matrix is symmetric. Is that correct? Thanks.,,"['multivariable-calculus', 'hessian-matrix']"
70,Reducing the proof of the smoothness of a multivariable function to that of a $\mathbb{R} \rightarrow \mathbb{R}$ function,Reducing the proof of the smoothness of a multivariable function to that of a  function,\mathbb{R} \rightarrow \mathbb{R},"Let $$g_1 (x)=\frac{1}{e^{\frac{1}{x}}}, g_2 \equiv 0.$$ Can someone please explain  to me how to show, that the function $$f:\mathbb{R} \rightarrow \mathbb{R},\ x \mapsto \begin{cases} g_1 (x) & x>0\\ g_2 (x) & \text{else}\\ \end{cases} $$ is in $C^ \infty(\mathbb{R})$ ? I wasn't even able to manage to prove that $f|_{(0,\infty)}$ is in $C^ \infty(0,\infty)$ (let alone to prove that all derivatives exist in $0$, which actually seems to me to be the key point), since I wasn't able to guess a general formula for calculating the derivatives (which I did for some values using a CAS), because it just gets horrible complicated after the fourth derivative; my idea was to succesively calculate the derivatives using the chain, sum and product rule and to prove that way, that the function ought to be in $C^ \infty(\mathbb{R})$. Is there maybe a sleeker way to achieve this ? Afterswards I should use $f$ to prove that $$F:\mathbb{R}^k\rightarrow \mathbb{R}, \ (x_1,\ldots,x_k) \mapsto  \begin{cases} G_1 (x_1,\ldots,x_k) & |(x_1,\ldots,x_k)|<1\\ g_2 (x) & \text{else}\\ \end{cases} $$ is also in $C^ \infty(\mathbb{R^k})$ , for $G_1 (x_1,\ldots,x_k)=e^{-\frac{1}{1-|(x_1,\ldots,x_k)|^2}}$. The only thing that came to my mind for this, was to maybe try prove that all partial derivatives of all orders of $F$ are continuously differentiable, since that would imply that $F$ would be smooth and that $$F(x_1,\ldots,x_k)=f(1-|(x_1,\ldots,x_k)|^2),$$ but I'm not sure about that.","Let $$g_1 (x)=\frac{1}{e^{\frac{1}{x}}}, g_2 \equiv 0.$$ Can someone please explain  to me how to show, that the function $$f:\mathbb{R} \rightarrow \mathbb{R},\ x \mapsto \begin{cases} g_1 (x) & x>0\\ g_2 (x) & \text{else}\\ \end{cases} $$ is in $C^ \infty(\mathbb{R})$ ? I wasn't even able to manage to prove that $f|_{(0,\infty)}$ is in $C^ \infty(0,\infty)$ (let alone to prove that all derivatives exist in $0$, which actually seems to me to be the key point), since I wasn't able to guess a general formula for calculating the derivatives (which I did for some values using a CAS), because it just gets horrible complicated after the fourth derivative; my idea was to succesively calculate the derivatives using the chain, sum and product rule and to prove that way, that the function ought to be in $C^ \infty(\mathbb{R})$. Is there maybe a sleeker way to achieve this ? Afterswards I should use $f$ to prove that $$F:\mathbb{R}^k\rightarrow \mathbb{R}, \ (x_1,\ldots,x_k) \mapsto  \begin{cases} G_1 (x_1,\ldots,x_k) & |(x_1,\ldots,x_k)|<1\\ g_2 (x) & \text{else}\\ \end{cases} $$ is also in $C^ \infty(\mathbb{R^k})$ , for $G_1 (x_1,\ldots,x_k)=e^{-\frac{1}{1-|(x_1,\ldots,x_k)|^2}}$. The only thing that came to my mind for this, was to maybe try prove that all partial derivatives of all orders of $F$ are continuously differentiable, since that would imply that $F$ would be smooth and that $$F(x_1,\ldots,x_k)=f(1-|(x_1,\ldots,x_k)|^2),$$ but I'm not sure about that.",,['multivariable-calculus']
71,Derivative notation in linear algebra,Derivative notation in linear algebra,,"What is the difference between a gradient and a derivative?  The text I'm reading keeps mentioning that 'the gradient is the transpose of the derivative'. Does this mean that $$ \nabla f(v) = df(v)^T $$ and also that $$ \nabla f(v) = \frac{df(v)}{dx} $$","What is the difference between a gradient and a derivative?  The text I'm reading keeps mentioning that 'the gradient is the transpose of the derivative'. Does this mean that $$ \nabla f(v) = df(v)^T $$ and also that $$ \nabla f(v) = \frac{df(v)}{dx} $$",,"['linear-algebra', 'multivariable-calculus']"
72,Finding the angle between u and v.,Finding the angle between u and v.,,"Struggling with the following Suppose that $u$ and $v$ are non-parallel unit vectors, $a=u+sv$ and $b=u-sv$, where $s$ is a real number. If the angle $\theta$ between $u$ and $v$ is the same as the angle between $a$ and $b$, show that: $$\cos^{2}\theta= \frac {1-2s^{2}+s^{4}}{4s^{2}}$$ Hence find $s$ such that $\theta= \pi/4$ and $0<s<1$. Many thanks in advance.","Struggling with the following Suppose that $u$ and $v$ are non-parallel unit vectors, $a=u+sv$ and $b=u-sv$, where $s$ is a real number. If the angle $\theta$ between $u$ and $v$ is the same as the angle between $a$ and $b$, show that: $$\cos^{2}\theta= \frac {1-2s^{2}+s^{4}}{4s^{2}}$$ Hence find $s$ such that $\theta= \pi/4$ and $0<s<1$. Many thanks in advance.",,"['multivariable-calculus', 'vector-spaces']"
73,How to interpret a double integral,How to interpret a double integral,,"If you have an integral $\int_c^{d}\int_{a}^{b} f(x_1,x_2)\,dx_2\, dx_1$. I am not sure how to visualize this.I know that you are adding two dimensional rectangles but I cannot see the relationship between the formula and the visualization. Do you basically add all the rectangles in the x2 direction first and get a function of x1 and then add all the rectangles in the x2 direction to get a function of x2? It's easy to interpret a single variable integral but I am not sure what's actually being done in a double integral.I know it's the volume under a particular function in the xyz plane but I cannot determine the ""algorithm"" that is performed to actually compute that volume.","If you have an integral $\int_c^{d}\int_{a}^{b} f(x_1,x_2)\,dx_2\, dx_1$. I am not sure how to visualize this.I know that you are adding two dimensional rectangles but I cannot see the relationship between the formula and the visualization. Do you basically add all the rectangles in the x2 direction first and get a function of x1 and then add all the rectangles in the x2 direction to get a function of x2? It's easy to interpret a single variable integral but I am not sure what's actually being done in a double integral.I know it's the volume under a particular function in the xyz plane but I cannot determine the ""algorithm"" that is performed to actually compute that volume.",,['multivariable-calculus']
74,Extremum in $f:R^2\to R$ via partial differential?,Extremum in  via partial differential?,f:R^2\to R,"In my math lectures, I learnt that an extremum of a function $f:\mathbb R^2\to \mathbb R$ requires $\mathrm{grad}(f)=0$. So if $f$ was $f(x_1, x_2)$ that means $(∂f/∂x_1, ∂f/∂x_2) \cdot (x_1, x_2)^{\top}=0$. (Sorry for my poor Tex skills, am working to improve those). No I came across the following in an economics lecture and I can't figure out if what they do is correct: To find an extremum in $\pi=p*f(x_1, x_2) - w_1x_1 - w_2x_2$ they claimed it was sufficient to find a point satisfying $\frac{∂\pi}{∂x_1}=0$ and $\frac{∂\pi}{∂x_2}=0$ In contrast, the  normal gradient approach would yield $\frac{∂\pi}{∂x_1}x_1 + \frac{∂\pi}{∂x_2}x_2=0$ as the precondition for an extremum. It's pretty clear that $\frac{∂\pi}{∂x_1}=0$ and $\frac{∂\pi}{∂x_2}=0$ implies $\frac{∂\pi}{∂x_1}x_1 + \frac{∂\pi}{∂x_2}x_2=0$, but not the other way round. Because of that, I'd say that the approach used in the economics lecture might not find all interesting points for an extremum. Is that correct? Or is there anything I have overlooked?","In my math lectures, I learnt that an extremum of a function $f:\mathbb R^2\to \mathbb R$ requires $\mathrm{grad}(f)=0$. So if $f$ was $f(x_1, x_2)$ that means $(∂f/∂x_1, ∂f/∂x_2) \cdot (x_1, x_2)^{\top}=0$. (Sorry for my poor Tex skills, am working to improve those). No I came across the following in an economics lecture and I can't figure out if what they do is correct: To find an extremum in $\pi=p*f(x_1, x_2) - w_1x_1 - w_2x_2$ they claimed it was sufficient to find a point satisfying $\frac{∂\pi}{∂x_1}=0$ and $\frac{∂\pi}{∂x_2}=0$ In contrast, the  normal gradient approach would yield $\frac{∂\pi}{∂x_1}x_1 + \frac{∂\pi}{∂x_2}x_2=0$ as the precondition for an extremum. It's pretty clear that $\frac{∂\pi}{∂x_1}=0$ and $\frac{∂\pi}{∂x_2}=0$ implies $\frac{∂\pi}{∂x_1}x_1 + \frac{∂\pi}{∂x_2}x_2=0$, but not the other way round. Because of that, I'd say that the approach used in the economics lecture might not find all interesting points for an extremum. Is that correct? Or is there anything I have overlooked?",,['multivariable-calculus']
75,What am I actually doing when I integrate using spherical coordinates in $\mathbb{R}^3$?,What am I actually doing when I integrate using spherical coordinates in ?,\mathbb{R}^3,"When learning vector fields and using Green's Theorem with the Jacobian to find the area of a level surface, I actually realized that most of the examples shown in my book would be much easier to solve by using polar and spherical coordinates, but not multiplying by the representation of the radius. (Eg: In spherical coordinates I would eliminate the $\rho^2 \sin(\phi)$. I ended up discussing with my professor about it, and he said that the radius should be fixed. I disagreed, saying that the radius is changing, but you are not multiplying by it. Here is how I see the use of Spherical Coordinates for getting the volume: My idea is that, in spherical coordinates, if you take $z = \rho \cos(\theta)$, and look at the Cartesian plane from the z-axis, you are actually adding all the circles on the xy-plane. Then by looking from the y-axis, you have other sets of circles, and that's where you are getting your radius for multiplying $\rho^2 \sin(\phi)$. Is this right or I understood it wrongly? Thanks for your attention","When learning vector fields and using Green's Theorem with the Jacobian to find the area of a level surface, I actually realized that most of the examples shown in my book would be much easier to solve by using polar and spherical coordinates, but not multiplying by the representation of the radius. (Eg: In spherical coordinates I would eliminate the $\rho^2 \sin(\phi)$. I ended up discussing with my professor about it, and he said that the radius should be fixed. I disagreed, saying that the radius is changing, but you are not multiplying by it. Here is how I see the use of Spherical Coordinates for getting the volume: My idea is that, in spherical coordinates, if you take $z = \rho \cos(\theta)$, and look at the Cartesian plane from the z-axis, you are actually adding all the circles on the xy-plane. Then by looking from the y-axis, you have other sets of circles, and that's where you are getting your radius for multiplying $\rho^2 \sin(\phi)$. Is this right or I understood it wrongly? Thanks for your attention",,"['integration', 'multivariable-calculus', 'spherical-coordinates']"
76,geometric meaning behind line integrals,geometric meaning behind line integrals,,"What are some geometric meanings behind line integrals? I know if you have a curve on the xy plane and you are given a function $f(x,y)$ then the geometric meaning is a ""curtain drawn"" from the function (surface) to the curve below.  This is a planer curve. However what about when we just have a helix? Similarly what the geometric meaning (or explanation) for the line integral involving a vector field $F$ and a curve.  $$\int_c{F\cdot ds}$$ Here is an example: Let $c(t)= \sin{t}, \cos{t}, t$ from 0 to $2\pi$. let the vector field $F$ be defined by $$F(x,y,z)=x\hat{i} + y\hat{j} + z\hat{k}$$    Compute $\int_c{F\cdot ds}$ Any links to pdfs and other resources helping me understanding would be very helpful! Thanks!","What are some geometric meanings behind line integrals? I know if you have a curve on the xy plane and you are given a function $f(x,y)$ then the geometric meaning is a ""curtain drawn"" from the function (surface) to the curve below.  This is a planer curve. However what about when we just have a helix? Similarly what the geometric meaning (or explanation) for the line integral involving a vector field $F$ and a curve.  $$\int_c{F\cdot ds}$$ Here is an example: Let $c(t)= \sin{t}, \cos{t}, t$ from 0 to $2\pi$. let the vector field $F$ be defined by $$F(x,y,z)=x\hat{i} + y\hat{j} + z\hat{k}$$    Compute $\int_c{F\cdot ds}$ Any links to pdfs and other resources helping me understanding would be very helpful! Thanks!",,"['reference-request', 'multivariable-calculus']"
77,Symbolic computation of the derivative of dot product of 2 vectors,Symbolic computation of the derivative of dot product of 2 vectors,,"If I have two vectors $a$ and $b$, whose components are time varying, for example $$a=[a_x(t), a_y(t), a_z(t)]$$ $$b=[b_x(t), b_y(t), b_z(t)]$$ The dot product of these 2 vectors can be expressed as $$a\cdot b=\Vert{a}\Vert\cdot\Vert{b}\Vert\cdot \cos\phi_{ab}$$  I want to compute in closed form the variation of $\phi_{ab}$ with respect to time, that is, $\dot\phi_{ab}$. Is it possible to symbolically compute this? Can I also obtain in closed form the derivative of $a\cdot b$ with respect to time? Which software would you recommend to do these kind of calculations: Matlab, Mathematica or Maple? Any examples? In fact, I have derived closed-form expressions tediously by hand for several expressions of the type I have given here. I wish to cross-verify whether my derivations are correct.","If I have two vectors $a$ and $b$, whose components are time varying, for example $$a=[a_x(t), a_y(t), a_z(t)]$$ $$b=[b_x(t), b_y(t), b_z(t)]$$ The dot product of these 2 vectors can be expressed as $$a\cdot b=\Vert{a}\Vert\cdot\Vert{b}\Vert\cdot \cos\phi_{ab}$$  I want to compute in closed form the variation of $\phi_{ab}$ with respect to time, that is, $\dot\phi_{ab}$. Is it possible to symbolically compute this? Can I also obtain in closed form the derivative of $a\cdot b$ with respect to time? Which software would you recommend to do these kind of calculations: Matlab, Mathematica or Maple? Any examples? In fact, I have derived closed-form expressions tediously by hand for several expressions of the type I have given here. I wish to cross-verify whether my derivations are correct.",,"['linear-algebra', 'multivariable-calculus', 'math-software', 'mathematica', 'symbolic-computation']"
78,How to maximize this equation?,How to maximize this equation?,,"I am trying to maximize $P = 20q_a - 2q_a^2 + 16q_b-2q_b^2 – 1/4 (q_a + q_b)^2$, a profit equation, function of two separate quantities of products. I thought that what I need to do is to set both derivatives =0 and find where that equation holds for both variables, but I am having some trouble. Here's my work: $dP/dq_a = 20 – 4q_a – 1/2 (q_a + q_b) = 0$ $dP/dq_b = 16 – 4q_b – 1/2 (q_a + q_b)  = 0$ $4 – 4q_a +4q_b = 0$ $4 = 4 (q_a – q_b)$ $q_a = q_b + 1.$ As you can see from the last equation, I get $q_a = q_b + 1$, but when I plug a pair of 1, 0 ($q_a$ and $q_b$) into the derivative equations, I do not get 0's, so I guess I made a mistake somewhere (wolfram alpha gives a definitive answer). Please help, thanks!","I am trying to maximize $P = 20q_a - 2q_a^2 + 16q_b-2q_b^2 – 1/4 (q_a + q_b)^2$, a profit equation, function of two separate quantities of products. I thought that what I need to do is to set both derivatives =0 and find where that equation holds for both variables, but I am having some trouble. Here's my work: $dP/dq_a = 20 – 4q_a – 1/2 (q_a + q_b) = 0$ $dP/dq_b = 16 – 4q_b – 1/2 (q_a + q_b)  = 0$ $4 – 4q_a +4q_b = 0$ $4 = 4 (q_a – q_b)$ $q_a = q_b + 1.$ As you can see from the last equation, I get $q_a = q_b + 1$, but when I plug a pair of 1, 0 ($q_a$ and $q_b$) into the derivative equations, I do not get 0's, so I guess I made a mistake somewhere (wolfram alpha gives a definitive answer). Please help, thanks!",,"['multivariable-calculus', 'optimization']"
79,Transforming a sum in an integral,Transforming a sum in an integral,,"I have a second question about the article ""Imperfect Bose Gas with Hard-Sphere Interaction"". The authors begins with the sum: $$\frac{{E_2 }}{{E_0 }} = \frac{{16\pi ^2 a^2 \lambda ^2 }}{{V^2 }}\sum\limits_{} {'\frac{{\langle n_\alpha  \rangle \langle n_\gamma  \rangle \langle n_\lambda  \rangle }}{{\frac{1}{2}\left( {k_\alpha ^2  + k_\beta ^2  - k_\gamma ^2  - k_\lambda ^2 } \right)}}\delta \left( {\vec k{}_\alpha   + \vec k{}_\beta   - \vec k{}_\gamma   - \vec k{}_\lambda  } \right)}  $$ which represents the second order perturbation term of the energy. ${\langle n_\alpha  \rangle }$ is: $$\langle n_\alpha  \rangle  = \sum\limits_{n = 0}^\infty  {n\left( {ze^{ - \beta \varepsilon _\alpha  } } \right)} ^n /\sum\limits_{n = 0}^\infty  {\left( {ze^{ - \beta \varepsilon _\alpha  } } \right)} ^n  = \frac{{ze^{ - \beta \varepsilon _\alpha  } }}{{1 - ze^{ - \beta \varepsilon _\alpha  } }} $$ In the sum the terms with a vanishing denominator are omitted. There is also the restriction ${\vec k{}_\alpha   \ne \vec k{}_\beta  }\ \ $  and ${\vec k{}_\gamma   \ne \vec k{}_\lambda  }\ \ $. Now the passage that is unclear to me is the passage to the integral: $$\frac{{E_2 }}{{E_0 }} = \frac{{16\pi  a^2 \lambda ^2 }}{{V^2 }}\left( {\frac{{4V^3 }}{{\pi ^3 \lambda ^5 }}} \right)\sum\limits_{i,j,k}^\infty  {\frac{{z^{j + k + l} }}{{\left( {j + k + l} \right)^{1/2} \left( {j - k} \right)l}}} \frac{{\partial J}}{{\partial u}} $$ where: $$J = \int\limits_0^\infty  {\int\limits_0^\infty  {dqdq\frac{{\cosh \left( {upq} \right)}}{{q^2  - p^2 }}} } e^{ - vq^2  - wp^2 }  $$ and: $$u = \frac{{\hbar ^2 \beta }}{{2m}}\left( {\frac{{2\left( {j - k} \right)l}}{{j + k + l}}} \right) $$ $$v = \frac{{\hbar ^2 \beta }}{{2m}}\left( {\frac{{\left( {j + k} \right)l}}{{j + k + l}}} \right) $$ $$w = \frac{{\hbar ^2 \beta }}{{2m}}\left( {\frac{{\left( {j + k} \right)l + 4jk}}{{j + k + l}}} \right) $$ $\frac{{\partial J}}{{\partial u}}$ is calculated here: Evaluating the integral $I(u,v,w)=\iint_{(0,\infty)^2}\sinh(upq) e^{-vq^2 - wp^2}pq(q^2-p^2)^{-1}dpdq$ Do you have any suggestion?","I have a second question about the article ""Imperfect Bose Gas with Hard-Sphere Interaction"". The authors begins with the sum: $$\frac{{E_2 }}{{E_0 }} = \frac{{16\pi ^2 a^2 \lambda ^2 }}{{V^2 }}\sum\limits_{} {'\frac{{\langle n_\alpha  \rangle \langle n_\gamma  \rangle \langle n_\lambda  \rangle }}{{\frac{1}{2}\left( {k_\alpha ^2  + k_\beta ^2  - k_\gamma ^2  - k_\lambda ^2 } \right)}}\delta \left( {\vec k{}_\alpha   + \vec k{}_\beta   - \vec k{}_\gamma   - \vec k{}_\lambda  } \right)}  $$ which represents the second order perturbation term of the energy. ${\langle n_\alpha  \rangle }$ is: $$\langle n_\alpha  \rangle  = \sum\limits_{n = 0}^\infty  {n\left( {ze^{ - \beta \varepsilon _\alpha  } } \right)} ^n /\sum\limits_{n = 0}^\infty  {\left( {ze^{ - \beta \varepsilon _\alpha  } } \right)} ^n  = \frac{{ze^{ - \beta \varepsilon _\alpha  } }}{{1 - ze^{ - \beta \varepsilon _\alpha  } }} $$ In the sum the terms with a vanishing denominator are omitted. There is also the restriction ${\vec k{}_\alpha   \ne \vec k{}_\beta  }\ \ $  and ${\vec k{}_\gamma   \ne \vec k{}_\lambda  }\ \ $. Now the passage that is unclear to me is the passage to the integral: $$\frac{{E_2 }}{{E_0 }} = \frac{{16\pi  a^2 \lambda ^2 }}{{V^2 }}\left( {\frac{{4V^3 }}{{\pi ^3 \lambda ^5 }}} \right)\sum\limits_{i,j,k}^\infty  {\frac{{z^{j + k + l} }}{{\left( {j + k + l} \right)^{1/2} \left( {j - k} \right)l}}} \frac{{\partial J}}{{\partial u}} $$ where: $$J = \int\limits_0^\infty  {\int\limits_0^\infty  {dqdq\frac{{\cosh \left( {upq} \right)}}{{q^2  - p^2 }}} } e^{ - vq^2  - wp^2 }  $$ and: $$u = \frac{{\hbar ^2 \beta }}{{2m}}\left( {\frac{{2\left( {j - k} \right)l}}{{j + k + l}}} \right) $$ $$v = \frac{{\hbar ^2 \beta }}{{2m}}\left( {\frac{{\left( {j + k} \right)l}}{{j + k + l}}} \right) $$ $$w = \frac{{\hbar ^2 \beta }}{{2m}}\left( {\frac{{\left( {j + k} \right)l + 4jk}}{{j + k + l}}} \right) $$ $\frac{{\partial J}}{{\partial u}}$ is calculated here: Evaluating the integral $I(u,v,w)=\iint_{(0,\infty)^2}\sinh(upq) e^{-vq^2 - wp^2}pq(q^2-p^2)^{-1}dpdq$ Do you have any suggestion?",,"['integration', 'multivariable-calculus', 'physics']"
80,Bound for multi-index sum,Bound for multi-index sum,,"I have difficulties in evaluating the multi-index notation in the following context: Let $x \in R^n$ and let $i$ be a multi-index, $i=(i_1, \dots, i_n)$. Now I want to know the bound of the sum (knowing that each $x_i \leq a$, $\frac{1}{x_i} \leq b$) $$ \sum_{|i|=3} \frac{1}{i!} \frac{x^i}{x_l} \quad (\ast) $$ What I thought so far: There are three types of summands, (i) there is one $i_j=3$ (and the rest $i_k$, $k \neq j$ is 0) in $i$, then $i!$=6; (ii) there is one index $i_{j_1}=2$ and one $i_{j_2}$=1 (the others 0), then $i!=2$; and (iii) there are three components of $i$ 1, the rest 0, the faculty would be 1. (the types would be $(0, \dots, 3, \dots, 0)$, $(0, \dots, 0 , 2, 0, \dots, 0, 1, 0, \dots, 0)$ and $(0, \dots, 0 , 1, 0, \dots, 0, 1,  0, \dots, 0, 1, 0, \dots, 0)$). Now if I look at the first case, I would have $(n-1)$ times the expression $\frac{x_j^3}{x_l}$, $l \neq j$ and once $x_l^2$, so  $$ (\ast) = \frac{1}{6} ( \underbrace{\sum_{j \neq l} \frac{x_j^3}{x_l}}_{ \leq (n-1) a^3 b} \quad + \underbrace{x_l^2}_{\leq a^2}) + \frac{1}{2} (\text{???}) + \frac{1}{1} (\text{????}) $$ Q: How can I obtain the whole bound for $(\ast)$ ? Is this right so far? How can I continue for the second case (ii)? Is there a way to let it do mechanically be mathematica? (Does someone know how to put multi-index notation in Mathematica) Note: I write $x_i \leq a$, $\frac{1}{x_i} \leq b$ because in the application I look at $E(\frac{x^i}{x_l})$ and have bounds for the expectation values.","I have difficulties in evaluating the multi-index notation in the following context: Let $x \in R^n$ and let $i$ be a multi-index, $i=(i_1, \dots, i_n)$. Now I want to know the bound of the sum (knowing that each $x_i \leq a$, $\frac{1}{x_i} \leq b$) $$ \sum_{|i|=3} \frac{1}{i!} \frac{x^i}{x_l} \quad (\ast) $$ What I thought so far: There are three types of summands, (i) there is one $i_j=3$ (and the rest $i_k$, $k \neq j$ is 0) in $i$, then $i!$=6; (ii) there is one index $i_{j_1}=2$ and one $i_{j_2}$=1 (the others 0), then $i!=2$; and (iii) there are three components of $i$ 1, the rest 0, the faculty would be 1. (the types would be $(0, \dots, 3, \dots, 0)$, $(0, \dots, 0 , 2, 0, \dots, 0, 1, 0, \dots, 0)$ and $(0, \dots, 0 , 1, 0, \dots, 0, 1,  0, \dots, 0, 1, 0, \dots, 0)$). Now if I look at the first case, I would have $(n-1)$ times the expression $\frac{x_j^3}{x_l}$, $l \neq j$ and once $x_l^2$, so  $$ (\ast) = \frac{1}{6} ( \underbrace{\sum_{j \neq l} \frac{x_j^3}{x_l}}_{ \leq (n-1) a^3 b} \quad + \underbrace{x_l^2}_{\leq a^2}) + \frac{1}{2} (\text{???}) + \frac{1}{1} (\text{????}) $$ Q: How can I obtain the whole bound for $(\ast)$ ? Is this right so far? How can I continue for the second case (ii)? Is there a way to let it do mechanically be mathematica? (Does someone know how to put multi-index notation in Mathematica) Note: I write $x_i \leq a$, $\frac{1}{x_i} \leq b$ because in the application I look at $E(\frac{x^i}{x_l})$ and have bounds for the expectation values.",,"['real-analysis', 'combinatorics', 'multivariable-calculus', 'notation']"
81,Growth of Radially Symmetric Potential Fields,Growth of Radially Symmetric Potential Fields,,Suppose I have a function $F: R^3 \to R^3$ which satisfies: 1) There exists $\Psi: R^3 \to R$ such that $F = \nabla \Psi$ and 2) $F(x)$ depends only on $\|x\|$ Can I conclude that $\|F(x)\| = C\frac{1}{\|x\|^2}$ for some constant $C$? Or maybe at least $\|F(x)\| \le C\frac{1}{\|x\|^2}$? Maybe there is another condition to add that would let me make the conclusion? Thanks in advance.,Suppose I have a function $F: R^3 \to R^3$ which satisfies: 1) There exists $\Psi: R^3 \to R$ such that $F = \nabla \Psi$ and 2) $F(x)$ depends only on $\|x\|$ Can I conclude that $\|F(x)\| = C\frac{1}{\|x\|^2}$ for some constant $C$? Or maybe at least $\|F(x)\| \le C\frac{1}{\|x\|^2}$? Maybe there is another condition to add that would let me make the conclusion? Thanks in advance.,,['multivariable-calculus']
82,Proving $F(x)=F(a)+\sum_{\mu=1}^{n}(x^\mu -a^\mu )H_\mu (x)$,Proving,F(x)=F(a)+\sum_{\mu=1}^{n}(x^\mu -a^\mu )H_\mu (x),"Where $F:R^n \rightarrow R$, $a=(a^1,...,a^\mu)$ and $x=(x^1,...,x^\mu)$. Also, $H_\mu (a)=\frac{\partial F}{\partial x^\mu}|_{x=a}$ Hi there, this is a problem on General Relativity from Robert Wald. I'm trying to solve it for a few hours but still, it doesn't look that difficult.. I'm pretty sure the fundamental theorem of calculus is a good start: $F(x)-F(a)=∫_{a}^{x}F′(s)ds$ Then , with $ s=t(x-a)+a$ we have $F(x)-F(a)=(x-a)∫_{0}^{1}F′[t(x-a)+a]dt$ Which bares a nice similarity! But I don't know how to generalize from there. I was checking up on Stokes theorem to see if there's any connection. Every time I get stuck on a problem for more than one hour I know it must be something obvious. Weird.","Where $F:R^n \rightarrow R$, $a=(a^1,...,a^\mu)$ and $x=(x^1,...,x^\mu)$. Also, $H_\mu (a)=\frac{\partial F}{\partial x^\mu}|_{x=a}$ Hi there, this is a problem on General Relativity from Robert Wald. I'm trying to solve it for a few hours but still, it doesn't look that difficult.. I'm pretty sure the fundamental theorem of calculus is a good start: $F(x)-F(a)=∫_{a}^{x}F′(s)ds$ Then , with $ s=t(x-a)+a$ we have $F(x)-F(a)=(x-a)∫_{0}^{1}F′[t(x-a)+a]dt$ Which bares a nice similarity! But I don't know how to generalize from there. I was checking up on Stokes theorem to see if there's any connection. Every time I get stuck on a problem for more than one hour I know it must be something obvious. Weird.",,"['calculus', 'multivariable-calculus']"
83,Setting up and solving differential equation with The Euler Method,Setting up and solving differential equation with The Euler Method,,"I recently started this question and it gave me some insight into the world of differential equations. However the solution was not fit for my goals as I wanted a general method for calculating the position of a particle travelling through a vector field where the vectors in the field represents a force pushing on the particle. So this is kind of a follow-up from the information I got from my other question. So what do I have? A vector field that is defined by a function: $\vec{F}(x,y)$ I have left out the definition of the function since I wish to find a solution that works regardless off the function definition. But one example could be: $\vec{F}(x,y) = (\sin(x),\cos(y))$ A particle, which have some initial values for position and velocity $\vec{p}(0)=P$ $\dot{\vec{p}}(0)=Q$ I know wish to find the position of the particle after a specified time $\vec{P}(t)$ and velocity $\dot{\vec{P}}(t)$. From what I learned from the other question it's not possible to find a closed form solution to this, but you have to use some approximation with for example Euler's Method. However I don't know how to model this scenario as a differential equation and how to apply Euler's Method (or any other approximation technique) to my scenario. If someone could, step by step explain this process it would be much appreciated! I should also add, the goal is to implement this in a program, so keep that in mind! Thanks! P.S. I'm sure I missed stuff and you have questions about my logic here so I'd be happy to edit and answer any questions.","I recently started this question and it gave me some insight into the world of differential equations. However the solution was not fit for my goals as I wanted a general method for calculating the position of a particle travelling through a vector field where the vectors in the field represents a force pushing on the particle. So this is kind of a follow-up from the information I got from my other question. So what do I have? A vector field that is defined by a function: $\vec{F}(x,y)$ I have left out the definition of the function since I wish to find a solution that works regardless off the function definition. But one example could be: $\vec{F}(x,y) = (\sin(x),\cos(y))$ A particle, which have some initial values for position and velocity $\vec{p}(0)=P$ $\dot{\vec{p}}(0)=Q$ I know wish to find the position of the particle after a specified time $\vec{P}(t)$ and velocity $\dot{\vec{P}}(t)$. From what I learned from the other question it's not possible to find a closed form solution to this, but you have to use some approximation with for example Euler's Method. However I don't know how to model this scenario as a differential equation and how to apply Euler's Method (or any other approximation technique) to my scenario. If someone could, step by step explain this process it would be much appreciated! I should also add, the goal is to implement this in a program, so keep that in mind! Thanks! P.S. I'm sure I missed stuff and you have questions about my logic here so I'd be happy to edit and answer any questions.",,"['ordinary-differential-equations', 'numerical-methods', 'multivariable-calculus']"
84,Finding the maximum of a function,Finding the maximum of a function,,"I want to find the maximum of the following function, $$ f(x, y) = e^{m e^{-x}+n e^{-y}-x-y}(mrxe^y+nsye^x+mn(r+s)xy), 0 \le x, y \le 1 $$ where $r, s, m,$ and $n$ are positive constants. At the maximum point we have the following equations, $$ \begin{cases} \frac{\partial{f(x, y)}}{\partial{x}} = 0 \\  \frac{\partial{f(x, y)}}{\partial{y}} = 0 \end{cases} $$ Simplifying the above equations we will get, $$ \begin{cases} x = -W_0\left(\frac {s(ny+ye^y-e^y)} {m(nry+nsy+rye^y+sye^y-se^y)}\right) \\  y = -W_0\left(\frac {r(mx+xe^x-e^x)} {n(mrx+msx+rxe^x+sxe^x-re^x)}\right)  \end{cases} $$ Where $W_0(.)$ is the upper branch of the Lambert W function. We know that $W_0(.)$ cannot be expressed in terms of elementary functions. Does it imply that the system of equations cannot be solved analytically? If not, how can I solve it analytically?","I want to find the maximum of the following function, $$ f(x, y) = e^{m e^{-x}+n e^{-y}-x-y}(mrxe^y+nsye^x+mn(r+s)xy), 0 \le x, y \le 1 $$ where $r, s, m,$ and $n$ are positive constants. At the maximum point we have the following equations, $$ \begin{cases} \frac{\partial{f(x, y)}}{\partial{x}} = 0 \\  \frac{\partial{f(x, y)}}{\partial{y}} = 0 \end{cases} $$ Simplifying the above equations we will get, $$ \begin{cases} x = -W_0\left(\frac {s(ny+ye^y-e^y)} {m(nry+nsy+rye^y+sye^y-se^y)}\right) \\  y = -W_0\left(\frac {r(mx+xe^x-e^x)} {n(mrx+msx+rxe^x+sxe^x-re^x)}\right)  \end{cases} $$ Where $W_0(.)$ is the upper branch of the Lambert W function. We know that $W_0(.)$ cannot be expressed in terms of elementary functions. Does it imply that the system of equations cannot be solved analytically? If not, how can I solve it analytically?",,"['calculus', 'optimization', 'multivariable-calculus']"
85,3d axis rotation,3d axis rotation,,"I have a vector V= and several line segments Seg1, Seg2, Seg3, Seg4. I want to know how to rotate each of the line segments so that the X axis is parallel to my given vector. How can I do this? Note: I am aware that I can get the angle of rotation by taking the inner product of my vector and the x-unit vector (i.e. V DOT X) but I am unaware of what to do after this.","I have a vector V= and several line segments Seg1, Seg2, Seg3, Seg4. I want to know how to rotate each of the line segments so that the X axis is parallel to my given vector. How can I do this? Note: I am aware that I can get the angle of rotation by taking the inner product of my vector and the x-unit vector (i.e. V DOT X) but I am unaware of what to do after this.",,"['geometry', 'trigonometry', 'euclidean-geometry', 'multivariable-calculus']"
86,"Solving for a constant in a simple partial differential equation, using divergence/gradient","Solving for a constant in a simple partial differential equation, using divergence/gradient",,"From a bank of past master's exams: Let $u(x,y)$ satisfy  $$  -\left( \frac{\partial^2 u}{\partial x^2}+\frac{\partial^2 u}{\partial y^2}\right) = \lambda u $$  in a bounded region $\mathcal{D} \subset \mathbb{R}^2$ with smooth boundary $\mathcal B$. Assume $u=0$ on $\mathcal B$ (but $u \not \equiv 0$ on $\mathcal D$). Show that $$  \lambda = \frac{\iint_{\mathcal D} |\nabla u|^2dxdy}{\iint_{\mathcal D} u^2dxdy}. $$  (Suggestion: Compute the divergence $\operatorname{div}(u\nabla u)$.) So taking the suggestion, I get the following: $$ \begin{align} \nabla \cdot (u \nabla u) &= \nabla \cdot \left( u \frac{\partial u}{\partial x}, u \frac{\partial u}{\partial y} \right) \\ &= u \left( \frac{ \partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} \right) + \left( \frac{\partial u}{\partial x} \right)^2 + \left( \frac{\partial u}{\partial y} \right)^2 \\ &= -\lambda u^2 + |\nabla u|^2 \end{align} $$ If I get  $$ \iint_D \nabla \cdot (u \nabla u) dA = 0,$$ then I'll have $$\begin{align} \iint_D (|\nabla u|^2 - \lambda u^2) dA &= 0 \\ \iint_D |\nabla u|^2 dA = \iint_D \lambda u^2 dA &= \lambda \iint_D u^2 dA \\ \frac{\iint_D |\nabla u|^2 dA}{\iint_D u^2 dA} &= \lambda  \end{align}$$ But I don't know how to show that (I was thinking Stokes's Theorem, but I don't think the integrand is the curl of a vector field). It's probably something very simple (my vector calculus is quite rusty) -- what am I missing here?","From a bank of past master's exams: Let $u(x,y)$ satisfy  $$  -\left( \frac{\partial^2 u}{\partial x^2}+\frac{\partial^2 u}{\partial y^2}\right) = \lambda u $$  in a bounded region $\mathcal{D} \subset \mathbb{R}^2$ with smooth boundary $\mathcal B$. Assume $u=0$ on $\mathcal B$ (but $u \not \equiv 0$ on $\mathcal D$). Show that $$  \lambda = \frac{\iint_{\mathcal D} |\nabla u|^2dxdy}{\iint_{\mathcal D} u^2dxdy}. $$  (Suggestion: Compute the divergence $\operatorname{div}(u\nabla u)$.) So taking the suggestion, I get the following: $$ \begin{align} \nabla \cdot (u \nabla u) &= \nabla \cdot \left( u \frac{\partial u}{\partial x}, u \frac{\partial u}{\partial y} \right) \\ &= u \left( \frac{ \partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} \right) + \left( \frac{\partial u}{\partial x} \right)^2 + \left( \frac{\partial u}{\partial y} \right)^2 \\ &= -\lambda u^2 + |\nabla u|^2 \end{align} $$ If I get  $$ \iint_D \nabla \cdot (u \nabla u) dA = 0,$$ then I'll have $$\begin{align} \iint_D (|\nabla u|^2 - \lambda u^2) dA &= 0 \\ \iint_D |\nabla u|^2 dA = \iint_D \lambda u^2 dA &= \lambda \iint_D u^2 dA \\ \frac{\iint_D |\nabla u|^2 dA}{\iint_D u^2 dA} &= \lambda  \end{align}$$ But I don't know how to show that (I was thinking Stokes's Theorem, but I don't think the integrand is the curl of a vector field). It's probably something very simple (my vector calculus is quite rusty) -- what am I missing here?",,"['ordinary-differential-equations', 'multivariable-calculus']"
87,How does a bounded irrotational vector field imply continuity of the scalar potential,How does a bounded irrotational vector field imply continuity of the scalar potential,,Supose we have a vector field $E:R^3\rightarrow R^3$ with the property $\nabla\times E=0 \Longleftrightarrow E=-\nabla \phi$ where $\phi:R^3\rightarrow R$ How does the boundedness of $E$ imply the continuity of $\phi$ I can solve this physically for a certain case by assuming a rectangular curve through a surface across which $E$ is discontinuous as $\nabla\times E=0 \Longleftrightarrow \oint E.l dl=0$. So even though $E$ is discontinuous its associated scalar ($\phi$) is still continuous. But the argument above has been hinted to apply in general and I am having trouble getting it mathematically.,Supose we have a vector field $E:R^3\rightarrow R^3$ with the property $\nabla\times E=0 \Longleftrightarrow E=-\nabla \phi$ where $\phi:R^3\rightarrow R$ How does the boundedness of $E$ imply the continuity of $\phi$ I can solve this physically for a certain case by assuming a rectangular curve through a surface across which $E$ is discontinuous as $\nabla\times E=0 \Longleftrightarrow \oint E.l dl=0$. So even though $E$ is discontinuous its associated scalar ($\phi$) is still continuous. But the argument above has been hinted to apply in general and I am having trouble getting it mathematically.,,[]
88,Optimising closest approach with third order motion,Optimising closest approach with third order motion,,"I have recently come across this problem trying to visualise a certain economic model and I'm finding the solution is just beyond my reach. As far as I can tell there is a simplification of the problem which is easier and would still be good to have answered. There are two moving point particles A and B on a Cartesian plane. Particle A is trying to reach B as quickly as possible, it can do so by applying acceleration in any direction. Its acceleration and speed have constant upper limits. Particle A knows the position and velocity of itself and of B, and can continuously[1] adjust its acceleration (which doesn't have to be continuous). What function of the particles' position and velocity should A use to reach and stop at B in as little time as possible? In the simplest version B is moving with a constant velocity, which I think will produce a better result for the complete version too, in which B can have acceleration. For many nodes targeting each other my current approach of ""accelerate as fast as possible to B's current position"" quickly resembles chaos for a few particles targeting each other in a chain. I know why that's a poor approach, I just don't know how to make a better one. If I haven't been clear enough, I'd be happy to provide a visualisation of the problem and my not-working solution. EDIT: Pretty please work this all the way through. I've shown a lot of people this problem and the pattern has been for them solve it for a single dimension then tell me that it should be easy to just do it for both dimensions. It really isn't, there's almost certainly a derivation and optimisation equation in there somewhere since there are three unknowns: acceleration on each axis and time but only two equations to solve them with (position of both particles with respect to time). Another way of looking at it is that the dimensions can't be considered separately assuming they can move with maximum acceleration, since the magnitude of the acceleration is limited there is a trade-off there. As is probably clear from my fumbling explanations, I'm not math savvy enough to translate this idea into number and figures. [1] Not actually continuous, since it runs on a computer in discrete (but tiny) steps.","I have recently come across this problem trying to visualise a certain economic model and I'm finding the solution is just beyond my reach. As far as I can tell there is a simplification of the problem which is easier and would still be good to have answered. There are two moving point particles A and B on a Cartesian plane. Particle A is trying to reach B as quickly as possible, it can do so by applying acceleration in any direction. Its acceleration and speed have constant upper limits. Particle A knows the position and velocity of itself and of B, and can continuously[1] adjust its acceleration (which doesn't have to be continuous). What function of the particles' position and velocity should A use to reach and stop at B in as little time as possible? In the simplest version B is moving with a constant velocity, which I think will produce a better result for the complete version too, in which B can have acceleration. For many nodes targeting each other my current approach of ""accelerate as fast as possible to B's current position"" quickly resembles chaos for a few particles targeting each other in a chain. I know why that's a poor approach, I just don't know how to make a better one. If I haven't been clear enough, I'd be happy to provide a visualisation of the problem and my not-working solution. EDIT: Pretty please work this all the way through. I've shown a lot of people this problem and the pattern has been for them solve it for a single dimension then tell me that it should be easy to just do it for both dimensions. It really isn't, there's almost certainly a derivation and optimisation equation in there somewhere since there are three unknowns: acceleration on each axis and time but only two equations to solve them with (position of both particles with respect to time). Another way of looking at it is that the dimensions can't be considered separately assuming they can move with maximum acceleration, since the magnitude of the acceleration is limited there is a trade-off there. As is probably clear from my fumbling explanations, I'm not math savvy enough to translate this idea into number and figures. [1] Not actually continuous, since it runs on a computer in discrete (but tiny) steps.",,['multivariable-calculus']
89,Simple but Stuck: How do I find the point of intersection of two lines in Vector Calculus?,Simple but Stuck: How do I find the point of intersection of two lines in Vector Calculus?,,"Simple but Stuck: How do I find the point of intersection of two lines in Vector Calculus? Given symmetric equations and deriving parametric equations. Find the point of intersection of the lines L1 : $\frac{x}{2} =\frac{y+1}{3} = \frac{z-1}{2}$ and L2 : $\frac{x}{4} = \frac{y}{5} =\frac{z}{5}$ The parametric forms should be $x_1 = 2t$, $y_1 = -1+3t$, $z_1=1+2t$ and $x_2=4t$, $y_2=5t$, $z_2 = 5t$ The final solution is $(4, 5, 5)$. It almost looks like someone just grabbed the coefficients from the parametric form of the second line to make the point. Working backwards, how do I get this answer? Sincerely, In need of the process. Thank you very much.","Simple but Stuck: How do I find the point of intersection of two lines in Vector Calculus? Given symmetric equations and deriving parametric equations. Find the point of intersection of the lines L1 : $\frac{x}{2} =\frac{y+1}{3} = \frac{z-1}{2}$ and L2 : $\frac{x}{4} = \frac{y}{5} =\frac{z}{5}$ The parametric forms should be $x_1 = 2t$, $y_1 = -1+3t$, $z_1=1+2t$ and $x_2=4t$, $y_2=5t$, $z_2 = 5t$ The final solution is $(4, 5, 5)$. It almost looks like someone just grabbed the coefficients from the parametric form of the second line to make the point. Working backwards, how do I get this answer? Sincerely, In need of the process. Thank you very much.",,['calculus']
90,Verifying Stokes' theorem with a line integral,Verifying Stokes' theorem with a line integral,,"I have the following question: I've managed to do the surface integral part of stokes theorem (and got an answer of 0) however I'm having a lot of trouble doing the line integral part in order to verify the theorem. As I understand it, I should be doing a line integral around the part of the plane $z = 2-2x-y$ that's in the $x,y,z > 0$ quadrant of the $x$ $y$ and $z$ axes but I don't know how to parametrize that particular part seeing as it's in 3D. All the examples I've done so far have been solely in the $xy$ plane. Should I treat each side of the path I'm going to traverse as a separate line and then just do all four and add their answers up? Or am I missing something really simple?","I have the following question: I've managed to do the surface integral part of stokes theorem (and got an answer of 0) however I'm having a lot of trouble doing the line integral part in order to verify the theorem. As I understand it, I should be doing a line integral around the part of the plane $z = 2-2x-y$ that's in the $x,y,z > 0$ quadrant of the $x$ $y$ and $z$ axes but I don't know how to parametrize that particular part seeing as it's in 3D. All the examples I've done so far have been solely in the $xy$ plane. Should I treat each side of the path I'm going to traverse as a separate line and then just do all four and add their answers up? Or am I missing something really simple?",,['calculus']
91,Taylor expansion in time of the time component of a stress energy tensor,Taylor expansion in time of the time component of a stress energy tensor,,Perform a taylor expansion in 3 dimensions in time on the time compontent of of $T^{\alpha \beta}(t - r + n^{i} y_{i})$ given that $r$ is a contstant and $n^{i} y_{i}$ is the scalar product of a normal vector $n$ with a position vector $y$. I believe the formula is $\phi (\vec{r} + \vec{a}) = \sum^{\infty}_{n=0} \frac{1}{n!} (\vec{a}\cdot \nabla)^{n} \phi(\vec{r})$ in general and that the first term is $T^{\alpha \beta}(t -r)$ but would appreciate some other opinion about what the proceeding terms should be up to order $n=3$. Cheers,Perform a taylor expansion in 3 dimensions in time on the time compontent of of $T^{\alpha \beta}(t - r + n^{i} y_{i})$ given that $r$ is a contstant and $n^{i} y_{i}$ is the scalar product of a normal vector $n$ with a position vector $y$. I believe the formula is $\phi (\vec{r} + \vec{a}) = \sum^{\infty}_{n=0} \frac{1}{n!} (\vec{a}\cdot \nabla)^{n} \phi(\vec{r})$ in general and that the first term is $T^{\alpha \beta}(t -r)$ but would appreciate some other opinion about what the proceeding terms should be up to order $n=3$. Cheers,,"['calculus', 'taylor-expansion', 'multivariable-calculus', 'tensors']"
92,How to compute the subdifferential of a variational represention of the trace norm?,How to compute the subdifferential of a variational represention of the trace norm?,,"Let $f : {\cal S}_+^n \mapsto \mathbb{R}$ be a function defined as $f(Q) := {\rm tr} WQ^{-1}W + {\rm tr} Q$ where $W \in {\cal S}_{+}^{n}$ is a symmetric positive definite matrix. How to compute a subdifferential of $f$ at a point $Q \in {\cal S}_+^n$ for which $f(Q) < \infty$? Note that $f(Q) = \infty$ for $Q$ such that range of $W$ is not contained within the range of Q. Furthermore, we know that $\min_{Q \in {\cal S}_+^n} f(Q) = 2{\rm tr}W$ which is obtained at $Q = W$.","Let $f : {\cal S}_+^n \mapsto \mathbb{R}$ be a function defined as $f(Q) := {\rm tr} WQ^{-1}W + {\rm tr} Q$ where $W \in {\cal S}_{+}^{n}$ is a symmetric positive definite matrix. How to compute a subdifferential of $f$ at a point $Q \in {\cal S}_+^n$ for which $f(Q) < \infty$? Note that $f(Q) = \infty$ for $Q$ such that range of $W$ is not contained within the range of Q. Furthermore, we know that $\min_{Q \in {\cal S}_+^n} f(Q) = 2{\rm tr}W$ which is obtained at $Q = W$.",,"['multivariable-calculus', 'convex-analysis']"
93,How do I find the absolute maximum and minimum values of the Lamb-Oseen Vortex?,How do I find the absolute maximum and minimum values of the Lamb-Oseen Vortex?,,"I am researching alternative solutions to Stokes equations and I came across a problem with the Lamb-Oseen vortex I cannot solve that I hope will allow easier derivations of vortex functions with boundary conditions, such as restricting vortex growth and confining laminar radial flow. For example, if stirring coffee in a cup has a solution in cylindrical coordinates given ideal flow conditions, I want to find it mathematically and compare this behavior with experimental results. For an angular velocity function derived by Navier-Stokes, $$ \omega \left(r,t\right)=\frac{\omega _0R_0^2}{R\left(t\right)^2}exp\left(-\frac{r^2}{R\left(t\right)^2}\right)$$ from which the azimuthal velocity $u_\theta$ component of the vector field in cylindrical coordinates, $\vec{u}$ , derived by vorticity, $\vec{\omega }=\nabla \times \vec{u}= \nabla \times \vec{u_\theta}$ , is the vortex function, $$v_\theta (r,t)=\frac{\omega _0R_0^2}{r} \left ( 1- e^{-\frac{r^2}{R(t)^2}} \right )$$ This is equivalent to the Lamb Oseen equation for some $ \Gamma_0 = 2\omega_0 \pi R_{0}^2$ , where, $\Gamma_0$ is the initial circulation. Question: How do I find the max/min values of $u_\theta$ for all $R>0$ assuming that $R_{0}>0$ in $ R(t)=\sqrt{4\nu t +R_{0}^2}$ ? So far, to find the $r$ locating the max, I have taken the partial derivative of $u_\theta$ w.r.t $r$ , set this equal to zero, and attempted to solve for $r$ in, $$\frac{\partial u_{\theta }}{\partial r}=0$$ $$\frac{\partial u_{\theta }}{\partial r}=\omega _0R_0\left(-\frac{1}{r^2}+\frac{1}{r^2}e^{-\frac{r^2}{R^2}}+\frac{2}{R^2}e^{-\frac{r^2}{R^2}}\right)=0$$ which reduces down to a seemingly unsolvable form. $$\frac{r^2}{R^2}=ln\left(\frac{2r^2+R^2}{R^2}\right)$$ I looked into the Lambert W function, but haven't turned up algebraically exact solutions. Using the abs-max value for $\omega(r,t)$ , $$\left(\sqrt{\frac{R^2}{2}},\:\frac{\omega _0R_0^2}{R^2}\sqrt{\frac{R^2}{2}}\right),\left(-\sqrt{\frac{R^2}{2}},\:-\frac{\omega \:_0R_0^2}{R^2}\sqrt{\frac{R^2}{2}}\right)$$ I approximated for $u_\theta$ to be, $$\left(\pm \:\sqrt{\frac{R^2}{2}\left(\frac{5}{2}\right)},\pm \:\frac{\omega \:_0R_0^2}{R^2}\sqrt{\frac{R^2}{2}\left(\frac{4}{5}\right)}\right)$$ Is there an exact set of solutions to $\frac{\partial u_{\theta }}{\partial r}=0$ ?","I am researching alternative solutions to Stokes equations and I came across a problem with the Lamb-Oseen vortex I cannot solve that I hope will allow easier derivations of vortex functions with boundary conditions, such as restricting vortex growth and confining laminar radial flow. For example, if stirring coffee in a cup has a solution in cylindrical coordinates given ideal flow conditions, I want to find it mathematically and compare this behavior with experimental results. For an angular velocity function derived by Navier-Stokes, from which the azimuthal velocity component of the vector field in cylindrical coordinates, , derived by vorticity, , is the vortex function, This is equivalent to the Lamb Oseen equation for some , where, is the initial circulation. Question: How do I find the max/min values of for all assuming that in ? So far, to find the locating the max, I have taken the partial derivative of w.r.t , set this equal to zero, and attempted to solve for in, which reduces down to a seemingly unsolvable form. I looked into the Lambert W function, but haven't turned up algebraically exact solutions. Using the abs-max value for , I approximated for to be, Is there an exact set of solutions to ?"," \omega \left(r,t\right)=\frac{\omega _0R_0^2}{R\left(t\right)^2}exp\left(-\frac{r^2}{R\left(t\right)^2}\right) u_\theta \vec{u} \vec{\omega }=\nabla \times \vec{u}= \nabla \times \vec{u_\theta} v_\theta (r,t)=\frac{\omega _0R_0^2}{r} \left ( 1- e^{-\frac{r^2}{R(t)^2}} \right )  \Gamma_0 = 2\omega_0 \pi R_{0}^2 \Gamma_0 u_\theta R>0 R_{0}>0  R(t)=\sqrt{4\nu t +R_{0}^2} r u_\theta r r \frac{\partial u_{\theta }}{\partial r}=0 \frac{\partial u_{\theta }}{\partial r}=\omega _0R_0\left(-\frac{1}{r^2}+\frac{1}{r^2}e^{-\frac{r^2}{R^2}}+\frac{2}{R^2}e^{-\frac{r^2}{R^2}}\right)=0 \frac{r^2}{R^2}=ln\left(\frac{2r^2+R^2}{R^2}\right) \omega(r,t) \left(\sqrt{\frac{R^2}{2}},\:\frac{\omega _0R_0^2}{R^2}\sqrt{\frac{R^2}{2}}\right),\left(-\sqrt{\frac{R^2}{2}},\:-\frac{\omega \:_0R_0^2}{R^2}\sqrt{\frac{R^2}{2}}\right) u_\theta \left(\pm \:\sqrt{\frac{R^2}{2}\left(\frac{5}{2}\right)},\pm \:\frac{\omega \:_0R_0^2}{R^2}\sqrt{\frac{R^2}{2}\left(\frac{4}{5}\right)}\right) \frac{\partial u_{\theta }}{\partial r}=0","['calculus', 'multivariable-calculus', 'vector-fields', 'fluid-dynamics']"
94,Proof of second partials test for saddle points - why does Hessian need to be nonsingular?,Proof of second partials test for saddle points - why does Hessian need to be nonsingular?,,"Suppose $f:U \subseteq \mathbb{R}^n \to \mathbb{R}$ is $C^2$ on $U$ and $\nabla f(x)=0$ at $x \in U$ . If $Hf(x)$ is indefinite and nonsingular—that is, if it has both positive and negative eigenvalues and none of its eigenvalues are $0$ , then $f$ has a saddle point at $x$ by the Second Partial Derivative Test. I am trying to prove this result, but I do not understand where the part about $Hf(x)$ being nonsingular comes in. My attempt is as follows. With a change of coordinates, we can write $h^THf(x)h$ as $y^TDy$ , where $D$ is a diagonal matrix with all the eigenvalues of $Hf(x)$ as its entries, and $y$ being the coordinates of $h$ with respect to the eigenbasis. Let $\{v_1,...v_n\}$ be the eigenbasis, all of unit length. Then $$h^THf(x)h=y^TDy=\lambda_1y_1^2+\cdots+\lambda_ny_n^2.$$ Suppose $\lambda_1>0$ . Then $h^THf(x)h>0$ for any $y_1$ as long as $y_2,...y_n=0$ . Taking $h=y_1v_1$ , we have $$f(x+h)-f(x)=\frac{1}{2} hHf(x)h+E(h)=\frac{1}{2}\lambda_1y_1^2+E(h)$$ where $E(h)$ is the error of the quadratic approximation. If $E(h)\ge 0$ , then $f(x+h)-f(x)>0$ . Therefore assume $E(h)<0$ . Fix $\epsilon<\lambda_1/2$ . Then for all small enough $h$ , i.e., all small enough $y_1$ , $|E(h)|<\epsilon \lVert h \rVert^2=\epsilon y_1^2$ . We thus have $$f(x+h)-f(x)=\frac{1}{2}\lambda_1y_1^2-|E(h)|\ge\frac{1}{2}\lambda_1y_1^2-\epsilon y_1^2>0.$$ Assuming that $\lambda_2<0$ and employing a similar argument, we thus see that along $v_1$ , $f$ is growing, but along $v_2$ , $f$ is decreasing. This tells us $f$ has a saddle point at $x$ . Is my reasoning correct? Where does nonsingularity come in? As long as there is one positive eigenvalue and one negative eigenvalue the argument holds; why do we need to ensure that all eigenvalues are nonzero?","Suppose is on and at . If is indefinite and nonsingular—that is, if it has both positive and negative eigenvalues and none of its eigenvalues are , then has a saddle point at by the Second Partial Derivative Test. I am trying to prove this result, but I do not understand where the part about being nonsingular comes in. My attempt is as follows. With a change of coordinates, we can write as , where is a diagonal matrix with all the eigenvalues of as its entries, and being the coordinates of with respect to the eigenbasis. Let be the eigenbasis, all of unit length. Then Suppose . Then for any as long as . Taking , we have where is the error of the quadratic approximation. If , then . Therefore assume . Fix . Then for all small enough , i.e., all small enough , . We thus have Assuming that and employing a similar argument, we thus see that along , is growing, but along , is decreasing. This tells us has a saddle point at . Is my reasoning correct? Where does nonsingularity come in? As long as there is one positive eigenvalue and one negative eigenvalue the argument holds; why do we need to ensure that all eigenvalues are nonzero?","f:U \subseteq \mathbb{R}^n \to \mathbb{R} C^2 U \nabla f(x)=0 x \in U Hf(x) 0 f x Hf(x) h^THf(x)h y^TDy D Hf(x) y h \{v_1,...v_n\} h^THf(x)h=y^TDy=\lambda_1y_1^2+\cdots+\lambda_ny_n^2. \lambda_1>0 h^THf(x)h>0 y_1 y_2,...y_n=0 h=y_1v_1 f(x+h)-f(x)=\frac{1}{2} hHf(x)h+E(h)=\frac{1}{2}\lambda_1y_1^2+E(h) E(h) E(h)\ge 0 f(x+h)-f(x)>0 E(h)<0 \epsilon<\lambda_1/2 h y_1 |E(h)|<\epsilon \lVert h \rVert^2=\epsilon y_1^2 f(x+h)-f(x)=\frac{1}{2}\lambda_1y_1^2-|E(h)|\ge\frac{1}{2}\lambda_1y_1^2-\epsilon y_1^2>0. \lambda_2<0 v_1 f v_2 f f x","['real-analysis', 'linear-algebra', 'multivariable-calculus']"
95,Some kind of generalized Leibniz Rule,Some kind of generalized Leibniz Rule,,"I want to compute a modified version of the following: Defining $ \partial_i := \frac{\partial}{\partial q_i}$ we can write $$ \partial_{a_1} \cdots \partial_{a_n} \: q_{a_1} \cdots q_{a_n} = \sum_{\sigma \in S_n} \prod_{i=1}^n \delta_{a_i a_{\sigma(i)}} $$ Now if I sum over all $ \{a_i\}_{i=1}^n $ where every index ranges from 1 to N, then $$ \sum_{\{a\}}  \prod_{i=1}^n \delta_{a_i a_{\sigma(i)}} = Z(S_n; N, \dots , N) $$ where $Z(S_n)$ is the cycle index polynomial of the symmetric group $S_n$ and every cycle of any length will provide a factor of $N$ , since every cycle traces the identiy matrix. This case is straightforward because its just a permutation on the set of indices i.e. a bijection of them on themselves. Now my problem is the computation of a modified case like $$ (\prod_{j=1}^n\partial_{a_j}) \: (q_{a_1}^2 \prod_{i=3}^n q_{a_i}) $$ as you can see the index $a_1$ ""appears twice"" in the sense that if i go through with the derivatives it would be like a mapping $$ \{a_1,\dots,a_n\} \to \{a_1, a_1, a_3,\dots, a_n\} $$ but this is not a bijection and hence no permuation. Do you guys know how to give a closed form for this case and even cases where more than one $q_{a_i}$ appears quadratically?","I want to compute a modified version of the following: Defining we can write Now if I sum over all where every index ranges from 1 to N, then where is the cycle index polynomial of the symmetric group and every cycle of any length will provide a factor of , since every cycle traces the identiy matrix. This case is straightforward because its just a permutation on the set of indices i.e. a bijection of them on themselves. Now my problem is the computation of a modified case like as you can see the index ""appears twice"" in the sense that if i go through with the derivatives it would be like a mapping but this is not a bijection and hence no permuation. Do you guys know how to give a closed form for this case and even cases where more than one appears quadratically?"," \partial_i := \frac{\partial}{\partial q_i}  \partial_{a_1} \cdots \partial_{a_n} \: q_{a_1} \cdots q_{a_n} = \sum_{\sigma \in S_n} \prod_{i=1}^n \delta_{a_i a_{\sigma(i)}}   \{a_i\}_{i=1}^n   \sum_{\{a\}}  \prod_{i=1}^n \delta_{a_i a_{\sigma(i)}} = Z(S_n; N, \dots , N)  Z(S_n) S_n N  (\prod_{j=1}^n\partial_{a_j}) \: (q_{a_1}^2 \prod_{i=3}^n q_{a_i})  a_1  \{a_1,\dots,a_n\} \to \{a_1, a_1, a_3,\dots, a_n\}  q_{a_i}","['combinatorics', 'group-theory', 'multivariable-calculus', 'symmetric-groups']"
96,How to represent double antiderivative of a function as a definite integral,How to represent double antiderivative of a function as a definite integral,,"To represent the double antiderivative of a function $f(t)$ as a definite integral: First antiderivative : $F(x)$ can be expressed as one of antiderivative of $f(t)$ : $$    F(x) = \int_a^x f(t) \, dt    $$ I want to understand how to find a double antiderivative of a general fucntion as a definite integral. And understand in general for more than 2 antiderivatives too My attempt: Similar to how, with the Fundamental Theorem of Calculus for a single variable, $A(x) = \int_a^x f(t)\; dt$ is expressed as the area under the graph of $f(t)$ from $a$ to $x$ and $\frac{dA}{dx} = f(t)$ , in a similar way, I used the concept of volume under a graph for finding the double antiderivative of $f(t)$ as a definite integral. What I wanted to find is: $$ \int \left( \int f(t) \, dt \right) dt $$ Now I found it could be expressed as (I am not totally sure about this) : $$ V(t) = \int_a^t \left( \int_a^y f(x) \, dx \right) dy $$ This in the coordinate plane of $xy$ translates to a right-angled triangle where the area we are integrating lies between $x = a$ , $x = y$ , and $y = t$ . With this definition in mind, I am not able to express how differentiating $V(t)$ two times shall give me back my function $f(t)$ again. I can find $dV$ as: $$ dV = f(t) \cdot t \, dt + \frac{f(t)}{2} \, dt^2 $$ by considering how the volume changes for $V(t)$ with a small change $dt$ (Area of the trapezium formed by change $dt$ ). However, I am struggling to express how the rate of change of the rate of change of the volume $V(t)$ with respect to a small change $dt$ equals $f(t)$ .","To represent the double antiderivative of a function as a definite integral: First antiderivative : can be expressed as one of antiderivative of : I want to understand how to find a double antiderivative of a general fucntion as a definite integral. And understand in general for more than 2 antiderivatives too My attempt: Similar to how, with the Fundamental Theorem of Calculus for a single variable, is expressed as the area under the graph of from to and , in a similar way, I used the concept of volume under a graph for finding the double antiderivative of as a definite integral. What I wanted to find is: Now I found it could be expressed as (I am not totally sure about this) : This in the coordinate plane of translates to a right-angled triangle where the area we are integrating lies between , , and . With this definition in mind, I am not able to express how differentiating two times shall give me back my function again. I can find as: by considering how the volume changes for with a small change (Area of the trapezium formed by change ). However, I am struggling to express how the rate of change of the rate of change of the volume with respect to a small change equals .","f(t) F(x) f(t) 
   F(x) = \int_a^x f(t) \, dt
    A(x) = \int_a^x f(t)\; dt f(t) a x \frac{dA}{dx} = f(t) f(t) 
\int \left( \int f(t) \, dt \right) dt
 
V(t) = \int_a^t \left( \int_a^y f(x) \, dx \right) dy
 xy x = a x = y y = t V(t) f(t) dV 
dV = f(t) \cdot t \, dt + \frac{f(t)}{2} \, dt^2
 V(t) dt dt V(t) dt f(t)","['multivariable-calculus', 'definite-integrals', 'indefinite-integrals']"
97,Calculate the Integral $\iint x^{2}\:dydz + (x^2 + y^2 + z^2 )\:dx dy$ [closed],Calculate the Integral  [closed],\iint x^{2}\:dydz + (x^2 + y^2 + z^2 )\:dx dy,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 18 days ago . Improve this question The integral that I need to calculate is the integral of the second kind: $$I=\iint_{S} x^{2}\:dydz + (x^2 + y^2 + z^2 )\:dx dy$$ with $S$ being the boundary of the following object: $x^2 + y^2 \leq z \leq 1$ . I have not so much experience with multivariable calculus, so I am not so familiar with the notation. I figured (allegedly) that it is surface integral, and the region would be something like $$0\leq x^{2}+y^{2}\leq z\leq 1\implies \begin{cases} -1\leq x\leq 1 \\ -1\leq y\leq 1 \\ 0\leq z\leq 1 \end{cases}$$ Integrating the function itself is easy but the integral is what I do not know how to solve. Can someone help me with this? I believe that I understood the problem wrong. For those that want to close the question because of the lack of detail, that thing is the entire statement of the problem. Nothing else. And I am being thrown off because there's someone who also said that I have to instead solve the problem as $$I=\iint_{S} x^{2}\:dydz + \iint_{R} (x^2 + y^2 + z^2 )\:dx dy$$ for another object $R$ Attempt 1 Alright, let's try it again. For this problem, I think the surface would be $$x^{2}+y^{2}=z, 0\leq z\leq 1$$ This means that we can parameterize it as: $$r(v,u)=(v\cos{(u)}, v\sin{(u)}, v): \begin{cases} 0\leq u\leq 2\pi \\ 0\leq v\leq 1 \end{cases}$$ The integral is seems to be denoted in differential form, which is $$I=\iint_{S} x^{2}\:dy\wedge dz + (x^2 + y^2 + z^2 )\:dx\wedge dy$$ As provided by Kurt. G. Then, we can obtain the vector field $F=(x^{2},0,x^{2}+y^{2}+z^{2})$ . Then, I can calculate the surface integral now using: $$\iint_{S}Fdx \land dy + G dy\land dz+Hdz\land dx=\iint_{D}\left[ F \frac{\partial(x,y)}{\partial (u,v)} + G\frac{\partial(y,z)}{\partial (u,v)}+\frac{\partial(z,x)}{\partial (u,v)} \right] dudv$$ Am I right? Attempt 2 Okay, I was dumb. Now, the parameterization is wrong. if $(x,y)=v\cos(u),v\sin{(u)}$ , then $z=v^{2}$ . Hence, instead, we have... $$r(u,v)=(v\cos{(u)},v\sin{(u)},v^{2}),\begin{cases} 0\leq u\leq 2\pi \\ -1\leq v \leq 1 \end{cases}$$ Theoretical Concepts Since I want a bit of mind toward the theoretical base that I am working on, I'll just leave it here. Let $S$ be a smooth parameterized surface: $$\begin{cases} x=f(u,v) \\ y = g(u,v) \\ z = h(u,v) \\ (u,v)\in D \end{cases}$$ with orientation corresponding to the ordering $u,v$ . Then, we have $$dx=\frac{\partial x}{\partial u}+\frac{\partial x}{\partial v}dv$$ Similarly, $$dy=\frac{\partial y}{\partial u}+\frac{\partial y}{\partial v}dv$$ Hence, $$\begin{split} dx\land dy & = \left( \frac{\partial x}{\partial u}du + \frac{\partial x}{\partial v}dv \right)\land\left( \frac{\partial y}{\partial u}du + \frac{\partial y}{\partial v}dv \right)\\  & = \left( \frac{\partial x}{\partial u} \frac{\partial y}{\partial v}- \frac{\partial x}{\partial v} \frac{\partial y}{\partial u} \right) du \land dv \\& =\frac{\partial(x,y)}{\partial(u,v)}du\land dv \end{split}$$ Working all the same, we got $$\begin{align} dx\land dy = \left( \frac{\partial x}{\partial u} \frac{\partial y}{\partial v}- \frac{\partial x}{\partial v} \frac{\partial y}{\partial u} \right) du \land dv  =\frac{\partial(x,y)}{\partial(u,v)}du\land dv \\ dy\land dz = \left( \frac{\partial y}{\partial u} \frac{\partial z}{\partial v}- \frac{\partial y}{\partial v} \frac{\partial z}{\partial u} \right) du \land dv  =\frac{\partial(y,z)}{\partial(u,v)}du\land dv \\ dz\land dx = \left( \frac{\partial z}{\partial u} \frac{\partial x}{\partial v}- \frac{\partial z}{\partial v} \frac{\partial x}{\partial u} \right) du \land dv  =\frac{\partial(z,x)}{\partial(u,v)}du\land dv \end{align}$$ Then, we can use $$\iint_{S}Fdx \land dy + G dy\land dz+Hdz\land dx=\iint_{D}\left[ F \frac{\partial(x,y)}{\partial (u,v)} + G\frac{\partial(y,z)}{\partial (u,v)}+\frac{\partial(z,x)}{\partial (u,v)} \right] dudv$$ which is guaranteed by Stoke's theorem. Working out the whole thing is a problem, since it's quite long. Still, writing everything in order as $$I=\iint_{S} (x^{2} + y^{2} + z^{2} )\:dx \land dy +x^{2}\:dy\land dz$$ For the first term, converting to the new coordinate $(u,v)$ : $$\begin{cases} r_{1}=v^{2}\cos^{2}{(u)}+v^{2}\sin^{2}{(u)}+(v^{2})^{2}=v^{2}(1+v^{2})\\ r_{2}=v^{2}\cos^{2}{(u)} \end{cases} $$ For a vector field $F(r_{1},r_{2},0)$ . I think I will be able to solve this more, but it is quite long, and after this it is kind of trivial, so if I am right, this is the path of solution.","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 18 days ago . Improve this question The integral that I need to calculate is the integral of the second kind: with being the boundary of the following object: . I have not so much experience with multivariable calculus, so I am not so familiar with the notation. I figured (allegedly) that it is surface integral, and the region would be something like Integrating the function itself is easy but the integral is what I do not know how to solve. Can someone help me with this? I believe that I understood the problem wrong. For those that want to close the question because of the lack of detail, that thing is the entire statement of the problem. Nothing else. And I am being thrown off because there's someone who also said that I have to instead solve the problem as for another object Attempt 1 Alright, let's try it again. For this problem, I think the surface would be This means that we can parameterize it as: The integral is seems to be denoted in differential form, which is As provided by Kurt. G. Then, we can obtain the vector field . Then, I can calculate the surface integral now using: Am I right? Attempt 2 Okay, I was dumb. Now, the parameterization is wrong. if , then . Hence, instead, we have... Theoretical Concepts Since I want a bit of mind toward the theoretical base that I am working on, I'll just leave it here. Let be a smooth parameterized surface: with orientation corresponding to the ordering . Then, we have Similarly, Hence, Working all the same, we got Then, we can use which is guaranteed by Stoke's theorem. Working out the whole thing is a problem, since it's quite long. Still, writing everything in order as For the first term, converting to the new coordinate : For a vector field . I think I will be able to solve this more, but it is quite long, and after this it is kind of trivial, so if I am right, this is the path of solution.","I=\iint_{S} x^{2}\:dydz + (x^2 + y^2 + z^2 )\:dx dy S x^2 + y^2 \leq z \leq 1 0\leq x^{2}+y^{2}\leq z\leq 1\implies \begin{cases}
-1\leq x\leq 1 \\
-1\leq y\leq 1 \\
0\leq z\leq 1
\end{cases} I=\iint_{S} x^{2}\:dydz + \iint_{R} (x^2 + y^2 + z^2 )\:dx dy R x^{2}+y^{2}=z, 0\leq z\leq 1 r(v,u)=(v\cos{(u)}, v\sin{(u)}, v): \begin{cases}
0\leq u\leq 2\pi \\
0\leq v\leq 1
\end{cases} I=\iint_{S} x^{2}\:dy\wedge dz + (x^2 + y^2 + z^2 )\:dx\wedge dy F=(x^{2},0,x^{2}+y^{2}+z^{2}) \iint_{S}Fdx \land dy + G dy\land dz+Hdz\land dx=\iint_{D}\left[ F \frac{\partial(x,y)}{\partial (u,v)} + G\frac{\partial(y,z)}{\partial (u,v)}+\frac{\partial(z,x)}{\partial (u,v)} \right] dudv (x,y)=v\cos(u),v\sin{(u)} z=v^{2} r(u,v)=(v\cos{(u)},v\sin{(u)},v^{2}),\begin{cases}
0\leq u\leq 2\pi \\
-1\leq v \leq 1
\end{cases} S \begin{cases}
x=f(u,v) \\
y = g(u,v) \\
z = h(u,v) \\
(u,v)\in D
\end{cases} u,v dx=\frac{\partial x}{\partial u}+\frac{\partial x}{\partial v}dv dy=\frac{\partial y}{\partial u}+\frac{\partial y}{\partial v}dv \begin{split}
dx\land dy & = \left( \frac{\partial x}{\partial u}du + \frac{\partial x}{\partial v}dv \right)\land\left( \frac{\partial y}{\partial u}du + \frac{\partial y}{\partial v}dv \right)\\ 
& = \left( \frac{\partial x}{\partial u} \frac{\partial y}{\partial v}- \frac{\partial x}{\partial v} \frac{\partial y}{\partial u} \right) du \land dv
\\& =\frac{\partial(x,y)}{\partial(u,v)}du\land dv
\end{split} \begin{align}
dx\land dy = \left( \frac{\partial x}{\partial u} \frac{\partial y}{\partial v}- \frac{\partial x}{\partial v} \frac{\partial y}{\partial u} \right) du \land dv
 =\frac{\partial(x,y)}{\partial(u,v)}du\land dv \\
dy\land dz = \left( \frac{\partial y}{\partial u} \frac{\partial z}{\partial v}- \frac{\partial y}{\partial v} \frac{\partial z}{\partial u} \right) du \land dv
 =\frac{\partial(y,z)}{\partial(u,v)}du\land dv \\
dz\land dx = \left( \frac{\partial z}{\partial u} \frac{\partial x}{\partial v}- \frac{\partial z}{\partial v} \frac{\partial x}{\partial u} \right) du \land dv
 =\frac{\partial(z,x)}{\partial(u,v)}du\land dv
\end{align} \iint_{S}Fdx \land dy + G dy\land dz+Hdz\land dx=\iint_{D}\left[ F \frac{\partial(x,y)}{\partial (u,v)} + G\frac{\partial(y,z)}{\partial (u,v)}+\frac{\partial(z,x)}{\partial (u,v)} \right] dudv I=\iint_{S} (x^{2} + y^{2} + z^{2} )\:dx \land dy +x^{2}\:dy\land dz (u,v) \begin{cases}
r_{1}=v^{2}\cos^{2}{(u)}+v^{2}\sin^{2}{(u)}+(v^{2})^{2}=v^{2}(1+v^{2})\\
r_{2}=v^{2}\cos^{2}{(u)}
\end{cases}
 F(r_{1},r_{2},0)","['integration', 'multivariable-calculus']"
98,Change of coordinates and compatibility condition for an area constraint,Change of coordinates and compatibility condition for an area constraint,,"Let $x=x(a,b), y=y(a,b)$ and consider the change of coordinates generated by the infinitesimal transformation $\delta a = -b \delta \phi$ and $\delta b = a \delta \phi$ for some function $\delta\phi$ . This is an (rigid or isometric) rotation of the coordinates. Initially, we take $\delta \phi$ to be independent of $(a,b)$ . Let $(x,y)$ satisfy $$\frac{\partial(x,y)}{\partial(a,b)}=x_ay_b-x_by_a=J(a,b).$$ Are there any restrictions on the functions $(x,y)$ so that the function $J$ remains fixed under the coordinate transformation? I believe this implies that there is a compatibility condition restricting $x_{aa}+x_{bb}=y_{aa}+y_{bb}=0$ . What about the case where $\phi=\phi(b)$ , that is the angle of the rotation depends on one of the independent variables? Is there a corresponding compatibility condition?","Let and consider the change of coordinates generated by the infinitesimal transformation and for some function . This is an (rigid or isometric) rotation of the coordinates. Initially, we take to be independent of . Let satisfy Are there any restrictions on the functions so that the function remains fixed under the coordinate transformation? I believe this implies that there is a compatibility condition restricting . What about the case where , that is the angle of the rotation depends on one of the independent variables? Is there a corresponding compatibility condition?","x=x(a,b), y=y(a,b) \delta a = -b \delta \phi \delta b = a \delta \phi \delta\phi \delta \phi (a,b) (x,y) \frac{\partial(x,y)}{\partial(a,b)}=x_ay_b-x_by_a=J(a,b). (x,y) J x_{aa}+x_{bb}=y_{aa}+y_{bb}=0 \phi=\phi(b)","['calculus', 'multivariable-calculus', 'differential-geometry']"
99,Understanding the difference between df and $| \triangledown f|$,Understanding the difference between df and,| \triangledown f|,"I am seeking some clarification regarding a couple of vector calculus topics. Let's suppose z = f(x,y) is a surface in $\mathbb{R}^3$ , and for the sake of having something to hold onto that z represents money we make based on x (number of workers) and y (products we sell). As I understand it, $df = \frac{\partial f}{\partial x}dx + \frac{\partial f}{\partial y}dy$ . This represents the total differential of f(x,y). What I thought this represented was if I'm at some point on the surface $(x_o, y_o, z_o)$ , then I could somehow figure out the amount z would change if we modified our $x_o$ and $y_o$ a tiny bit. To be more precise with my thinking, since $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ represent the particular change in z we would experience if we wandered off a tiny bit in the x or y direction respectively, if we scale each rate by dx and dy respectively (our tiny bit of wandering) and add the results, we get the total change in z we would experience ( df ). Now we have this thing called the gradient which consists of $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ , specifically $\triangledown f = <\frac{\partial f}{\partial x}$ , $\frac{\partial f}{\partial y}>$ . And to top it off, the idea is that this is a vector that lies on the x-y plane and points in the direction of steepest ascent. Suppose the gradient at a particular point was $<4, 3>$ , I have been taking this to mean that if we increase the number of workers by 4 and the number of units sold by 3, then we will increase our revenue the quickest possible AND we would expect to make \$5 more as a result, since $|\triangledown f| = 5$ . So my questions are as follows: Is my intuition correct for both concepts? In what (simple) scenarios would I want to use the result of df vs. the result of $|\triangledown f|$ when talking about what to expect to happen to the z-values of my surface? I'm having trouble putting into words the lack of ability I'm having differentiating between the two concepts and uses. Not to mention that if df measures change in z based on the change in the x and y direction, but $|\triangledown f|$ also measures the change but then says it's the maximum change...I'm just a bit confused.","I am seeking some clarification regarding a couple of vector calculus topics. Let's suppose z = f(x,y) is a surface in , and for the sake of having something to hold onto that z represents money we make based on x (number of workers) and y (products we sell). As I understand it, . This represents the total differential of f(x,y). What I thought this represented was if I'm at some point on the surface , then I could somehow figure out the amount z would change if we modified our and a tiny bit. To be more precise with my thinking, since and represent the particular change in z we would experience if we wandered off a tiny bit in the x or y direction respectively, if we scale each rate by dx and dy respectively (our tiny bit of wandering) and add the results, we get the total change in z we would experience ( df ). Now we have this thing called the gradient which consists of and , specifically , . And to top it off, the idea is that this is a vector that lies on the x-y plane and points in the direction of steepest ascent. Suppose the gradient at a particular point was , I have been taking this to mean that if we increase the number of workers by 4 and the number of units sold by 3, then we will increase our revenue the quickest possible AND we would expect to make \$5 more as a result, since . So my questions are as follows: Is my intuition correct for both concepts? In what (simple) scenarios would I want to use the result of df vs. the result of when talking about what to expect to happen to the z-values of my surface? I'm having trouble putting into words the lack of ability I'm having differentiating between the two concepts and uses. Not to mention that if df measures change in z based on the change in the x and y direction, but also measures the change but then says it's the maximum change...I'm just a bit confused.","\mathbb{R}^3 df = \frac{\partial f}{\partial x}dx + \frac{\partial f}{\partial y}dy (x_o, y_o, z_o) x_o y_o \frac{\partial f}{\partial x} \frac{\partial f}{\partial y} \frac{\partial f}{\partial x} \frac{\partial f}{\partial y} \triangledown f = <\frac{\partial f}{\partial x} \frac{\partial f}{\partial y}> <4, 3> |\triangledown f| = 5 |\triangledown f| |\triangledown f|",['multivariable-calculus']
