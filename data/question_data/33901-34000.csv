,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Probability of Barcelona playing Real Madrid in Champions League Quarterfinals,Probability of Barcelona playing Real Madrid in Champions League Quarterfinals,,"The $2018$ Champions League quarterfinal draw will take place on Friday, March $16^{th}, 2018$ and I wanted to know what is the likelihood that Barcelona will get paired up with Real Madrid ? There are $8$ teams left in the pool so a total of $4$ draws will be made. I think that the number of ways to pairing Barcelona v Real Madrid is $^8C_2$ but I am stuck on how many possible draws there are. Isn't it $\dfrac{^8C_2}{\text{total # of draws}}$? If someone can walk me through the solution to this problem that would greatly be appreciated. Probability has always given me trouble...","The $2018$ Champions League quarterfinal draw will take place on Friday, March $16^{th}, 2018$ and I wanted to know what is the likelihood that Barcelona will get paired up with Real Madrid ? There are $8$ teams left in the pool so a total of $4$ draws will be made. I think that the number of ways to pairing Barcelona v Real Madrid is $^8C_2$ but I am stuck on how many possible draws there are. Isn't it $\dfrac{^8C_2}{\text{total # of draws}}$? If someone can walk me through the solution to this problem that would greatly be appreciated. Probability has always given me trouble...",,['probability']
1,A Probability question: last ball in the bag,A Probability question: last ball in the bag,,"Players are pulled up to pick a ball out of a hat containing $14$ red and $1$ blue. If the odds of drawing the blue ball are $1/15$ what are the odds of every person not drawing the blue ball and leaving it for the last person to draw? Initially I thought you would multiply the probability of not drawing the blue ball for each person and multiplying them together so i did $$\dfrac{14}{15} \times \dfrac{13}{14} \times \dfrac{12}{13} \times \cdots$$ But when I roughly calculate that, I get the same as $1/15$. Is this correct?","Players are pulled up to pick a ball out of a hat containing $14$ red and $1$ blue. If the odds of drawing the blue ball are $1/15$ what are the odds of every person not drawing the blue ball and leaving it for the last person to draw? Initially I thought you would multiply the probability of not drawing the blue ball for each person and multiplying them together so i did $$\dfrac{14}{15} \times \dfrac{13}{14} \times \dfrac{12}{13} \times \cdots$$ But when I roughly calculate that, I get the same as $1/15$. Is this correct?",,['probability']
2,as convergence of $n$ roots of product of uniform random variables,as convergence of  roots of product of uniform random variables,n,"Let $X_1,X_2,\dots$ be i.i.d. random variables with continuous uniform distribution $U(0,1)$. $\forall\ n\in\mathbb{N}:\text{we define } Y_n:=\prod\limits_{i=1}^nX_n$. Prove that the sequence $Y_1,\sqrt[2]{Y_2},\sqrt[3]{Y_3},\dots$ converges almost surely and find the limit. I am not sure what the limit is but I feel that maybe its $\frac{1}{2}$. I know that it is enough to show that $\sum\limits_{n=1}^\infty\Pr[|\sqrt[n]{Y_n}-\frac{1}{2}|\geq\epsilon]<\infty$. $\Pr[|\sqrt[n]{Y_n}-\frac{1}{2}|\geq\epsilon]=$ $\Pr[\{\sqrt[n]{Y_n}-\frac{1}{2}\geq\epsilon\}\cup\{\sqrt[n]{Y_n}-\frac{1}{2}\leq-\epsilon\}]=$ $\Pr[\{Y_n\geq(\epsilon+\frac{1}{2})^n\}\cup\{Y_n\leq(\frac{1}{2}-\epsilon)^n\}]\leq$ $\Pr[Y_n\geq(\epsilon+\frac{1}{2})^n]+\Pr[Y_n\leq(\frac{1}{2}-\epsilon)^n]\stackrel{\text{Markov}}{\leq}$ $\Big(\frac{1/2}{1/2+\epsilon}\Big)^n+\Pr[Y_n\leq(\frac{1}{2}-\epsilon)^n]$ What will I do with the second term? Is $\frac{1}{2}$ even the limit?","Let $X_1,X_2,\dots$ be i.i.d. random variables with continuous uniform distribution $U(0,1)$. $\forall\ n\in\mathbb{N}:\text{we define } Y_n:=\prod\limits_{i=1}^nX_n$. Prove that the sequence $Y_1,\sqrt[2]{Y_2},\sqrt[3]{Y_3},\dots$ converges almost surely and find the limit. I am not sure what the limit is but I feel that maybe its $\frac{1}{2}$. I know that it is enough to show that $\sum\limits_{n=1}^\infty\Pr[|\sqrt[n]{Y_n}-\frac{1}{2}|\geq\epsilon]<\infty$. $\Pr[|\sqrt[n]{Y_n}-\frac{1}{2}|\geq\epsilon]=$ $\Pr[\{\sqrt[n]{Y_n}-\frac{1}{2}\geq\epsilon\}\cup\{\sqrt[n]{Y_n}-\frac{1}{2}\leq-\epsilon\}]=$ $\Pr[\{Y_n\geq(\epsilon+\frac{1}{2})^n\}\cup\{Y_n\leq(\frac{1}{2}-\epsilon)^n\}]\leq$ $\Pr[Y_n\geq(\epsilon+\frac{1}{2})^n]+\Pr[Y_n\leq(\frac{1}{2}-\epsilon)^n]\stackrel{\text{Markov}}{\leq}$ $\Big(\frac{1/2}{1/2+\epsilon}\Big)^n+\Pr[Y_n\leq(\frac{1}{2}-\epsilon)^n]$ What will I do with the second term? Is $\frac{1}{2}$ even the limit?",,"['probability', 'probability-theory', 'probability-limit-theorems']"
3,Expected sum of cards until first Ace,Expected sum of cards until first Ace,,"I came across this question while preparing for an interview. You draw cards from a $52$-card deck until you get first Ace. After each card drawn, you discard three cards from the deck. What's the expected sum of cards until you get the first Ace? Note J, Q, K have point value 11, 12 and 13, and Ace has point value 1 discarded cards don't count towards the sum and if we don't get an Ace we shuffle the deck and continue when you shuffle, you shuffle all cards but you keep the sum, and when you draw a new card you add it to that sum (you don't start from zero after each shuffle) My thought so far: the expected sum is definitely between $73$ and $91$. $73$ is the expected sum if we don't discard any cards, so the problem simply becomes the expected sum until first Ace, that is, $(2+\dots+13) \cdot 4 \cdot \frac{1}{5}+1$. $91$ is the expected sum if we discard all $51$ remaining cards (shuffle the deck after each draw). In this case the number of draws needed to see the first Ace follows a Geometric distribution, so the answer is $(\frac{52}{4}-1) \cdot 7.5+1$ Any help is appreciated!!!","I came across this question while preparing for an interview. You draw cards from a $52$-card deck until you get first Ace. After each card drawn, you discard three cards from the deck. What's the expected sum of cards until you get the first Ace? Note J, Q, K have point value 11, 12 and 13, and Ace has point value 1 discarded cards don't count towards the sum and if we don't get an Ace we shuffle the deck and continue when you shuffle, you shuffle all cards but you keep the sum, and when you draw a new card you add it to that sum (you don't start from zero after each shuffle) My thought so far: the expected sum is definitely between $73$ and $91$. $73$ is the expected sum if we don't discard any cards, so the problem simply becomes the expected sum until first Ace, that is, $(2+\dots+13) \cdot 4 \cdot \frac{1}{5}+1$. $91$ is the expected sum if we discard all $51$ remaining cards (shuffle the deck after each draw). In this case the number of draws needed to see the first Ace follows a Geometric distribution, so the answer is $(\frac{52}{4}-1) \cdot 7.5+1$ Any help is appreciated!!!",,"['probability', 'probability-theory', 'statistics', 'card-games']"
4,Stochastic Processes and Trajectories,Stochastic Processes and Trajectories,,"I am having a hard time intuitively understanding the definition of a stochastic process when we fix $\omega$ and let $t$ vary. More specifically, we know that a stochastic process is a collection of random variables $\{ X(t,\omega)\}$, where $t \in T$ and $\omega \in \Omega$. It’s clear that for every $t$, $X(t, \dot)$ is a random variable. What I don’t see is how we get a “path” or “trajectory” when we fix $\omega$. For example, suppose we use a simple random walk where we toss a coin, and if the coin lands heads, then $X(heads)=1$ and we move one step up, and, if the coin lands tails, then $X(tails)=-1$ and we move one step down. Our initial point is 0. So, in this case, if I fixed my $\omega$ to be heads, then wouldn’t the trajectory, or path, be represented by the function $f(t)=t$? Alternatively, if we fixed $\omega$ to be tails, then the trajectory would be $f(t)=-t$. Obviously, there are infinite amount of trajectories, or realizations, (going up and down) that repesent this simple walk. So how do I find these deterministic simple paths, by fixing an $\omega$, for this example, when I only have two sample points.","I am having a hard time intuitively understanding the definition of a stochastic process when we fix $\omega$ and let $t$ vary. More specifically, we know that a stochastic process is a collection of random variables $\{ X(t,\omega)\}$, where $t \in T$ and $\omega \in \Omega$. It’s clear that for every $t$, $X(t, \dot)$ is a random variable. What I don’t see is how we get a “path” or “trajectory” when we fix $\omega$. For example, suppose we use a simple random walk where we toss a coin, and if the coin lands heads, then $X(heads)=1$ and we move one step up, and, if the coin lands tails, then $X(tails)=-1$ and we move one step down. Our initial point is 0. So, in this case, if I fixed my $\omega$ to be heads, then wouldn’t the trajectory, or path, be represented by the function $f(t)=t$? Alternatively, if we fixed $\omega$ to be tails, then the trajectory would be $f(t)=-t$. Obviously, there are infinite amount of trajectories, or realizations, (going up and down) that repesent this simple walk. So how do I find these deterministic simple paths, by fixing an $\omega$, for this example, when I only have two sample points.",,"['probability', 'stochastic-processes']"
5,Expectation of the product of two discrete random variables.,Expectation of the product of two discrete random variables.,,"Let $X_i, X_j$ be two random variables that can each assume the values $\zeta_1, ..., \zeta_m$. Then my book claims $E(X_iX_j) = \sum_{k = 1}^m\sum_{l = 1}^m\zeta_k\zeta_lP(X_i = \zeta_k \textrm{ and } X_j = \zeta_l$). I don't get how they acquired this result. To my understanding $E(X_iX_j) = \sum_{k = 1}^m\sum_{l = 1}^m\zeta_k\zeta_lP(X_i X_j = \zeta_k\zeta_l)$. So why is $P(X_iX_j = \zeta_k\zeta_l) = (X_i = \zeta_k \textrm{ and } X_j = \zeta_l)$?","Let $X_i, X_j$ be two random variables that can each assume the values $\zeta_1, ..., \zeta_m$. Then my book claims $E(X_iX_j) = \sum_{k = 1}^m\sum_{l = 1}^m\zeta_k\zeta_lP(X_i = \zeta_k \textrm{ and } X_j = \zeta_l$). I don't get how they acquired this result. To my understanding $E(X_iX_j) = \sum_{k = 1}^m\sum_{l = 1}^m\zeta_k\zeta_lP(X_i X_j = \zeta_k\zeta_l)$. So why is $P(X_iX_j = \zeta_k\zeta_l) = (X_i = \zeta_k \textrm{ and } X_j = \zeta_l)$?",,['probability']
6,Intro Probability Question,Intro Probability Question,,"Of $33$ people, $17$ like red, $14$ like green, and $11$ do not like either. What is the probability that a student likes red and green? What's the probability that exactly one of the following is true: the student likes red (call this event $A$) or the student likes green (call this event $B$).? So far I have that $P(A) = \frac{17}{33}$ and $P(B) = \frac{14}{33}$. I know we are looking for $P(A \cap B)$ for part one. I am wondering if for part two the formula would be $P(A) + P(B) -2P(A \cap  B)$.","Of $33$ people, $17$ like red, $14$ like green, and $11$ do not like either. What is the probability that a student likes red and green? What's the probability that exactly one of the following is true: the student likes red (call this event $A$) or the student likes green (call this event $B$).? So far I have that $P(A) = \frac{17}{33}$ and $P(B) = \frac{14}{33}$. I know we are looking for $P(A \cap B)$ for part one. I am wondering if for part two the formula would be $P(A) + P(B) -2P(A \cap  B)$.",,['probability']
7,Upper bound on the largest singular value of a stochastic matrix,Upper bound on the largest singular value of a stochastic matrix,,"A stochastic matrix is a positive matrix whose columns sum to one. The largest eigenvalue of a stochastic matrix is one. But the largest singular value can exceed one. Are there any known bounds on this largest singular value? (Note, when the matrix is doubly stochastic---with both rows and columns summing to one---then the largest singular equals one.)","A stochastic matrix is a positive matrix whose columns sum to one. The largest eigenvalue of a stochastic matrix is one. But the largest singular value can exceed one. Are there any known bounds on this largest singular value? (Note, when the matrix is doubly stochastic---with both rows and columns summing to one---then the largest singular equals one.)",,"['linear-algebra', 'probability', 'stochastic-processes', 'singular-values']"
8,How do I show that two random variables have the same cumulative distribution function?,How do I show that two random variables have the same cumulative distribution function?,,"Let $X$ be a random variable on $\mathbb N_0$ and $Y$ a uniformly distributed random variable on $[0,1]$, independent of $X$. Now define the random variable $$Z:=\inf\{n\in \mathbb N_0 : Y < \mathbb P(X \leq n)\}.$$ How can I show that the cdf of $X$ and $Z$ are the same? Or how do I approach problems like these in general?","Let $X$ be a random variable on $\mathbb N_0$ and $Y$ a uniformly distributed random variable on $[0,1]$, independent of $X$. Now define the random variable $$Z:=\inf\{n\in \mathbb N_0 : Y < \mathbb P(X \leq n)\}.$$ How can I show that the cdf of $X$ and $Z$ are the same? Or how do I approach problems like these in general?",,"['probability', 'probability-distributions']"
9,Finding the probability that $2$ socks are the same color,Finding the probability that  socks are the same color,2,"Question: A drawer contains $6$ blue socks and $4$ white socks. Two socks are chosen randomly without replacement. What is the probability that the $2$ socks are the same color? Should I approach this problem by adding the probabilities of selecting $2$ blue socks and selecting $2$ white socks? If so, is the formula unordered without replacement? Can somebody direct me with the right formula to solving this?","Question: A drawer contains $6$ blue socks and $4$ white socks. Two socks are chosen randomly without replacement. What is the probability that the $2$ socks are the same color? Should I approach this problem by adding the probabilities of selecting $2$ blue socks and selecting $2$ white socks? If so, is the formula unordered without replacement? Can somebody direct me with the right formula to solving this?",,"['probability', 'statistics']"
10,Expected Value Of Number of Removals From Urn,Expected Value Of Number of Removals From Urn,,"Here's the question: ""An urn contains b blue balls and r red balls. You repeatedly and independently remove balls from urn (without returning them) until the first blue ball is drawn. All balls currently in the urn have an equal probability of being selected each time a ball is removed. Define the random variable X as the number of balls that are drawn (number of red balls that are removed plus the first blue ball that is removed). Find E[X], the expected value of random variable X."" I'm having trouble finding a closed form expression for the expected value. Here's the work I've done:","Here's the question: ""An urn contains b blue balls and r red balls. You repeatedly and independently remove balls from urn (without returning them) until the first blue ball is drawn. All balls currently in the urn have an equal probability of being selected each time a ball is removed. Define the random variable X as the number of balls that are drawn (number of red balls that are removed plus the first blue ball that is removed). Find E[X], the expected value of random variable X."" I'm having trouble finding a closed form expression for the expected value. Here's the work I've done:",,"['probability', 'statistics']"
11,What does this notation mean p(x|y)?,What does this notation mean p(x|y)?,,"I was reading through a paper, when I encounter p(a|s). What does this mean? It was in context with log probability but I cant find this notation anywhere.","I was reading through a paper, when I encounter p(a|s). What does this mean? It was in context with log probability but I cant find this notation anywhere.",,"['probability', 'notation']"
12,how many times in average do I have to roll a dice to get a 1? [duplicate],how many times in average do I have to roll a dice to get a 1? [duplicate],,"This question already has answers here : On average, how many times must I roll a dice until I get a $6$? (6 answers) Closed 7 years ago . I've read that the answer to such a problem is the inverse probability. So here getting a one has probably 1/6, so the number of tries you would be expected to run in order to get a 1 is 6. I'm not sure I understand this.","This question already has answers here : On average, how many times must I roll a dice until I get a $6$? (6 answers) Closed 7 years ago . I've read that the answer to such a problem is the inverse probability. So here getting a one has probably 1/6, so the number of tries you would be expected to run in order to get a 1 is 6. I'm not sure I understand this.",,['probability']
13,Knock out tournament 1,Knock out tournament 1,,"8n players $P_1$, $P_2$, $P_3$, .....$P{_8}{_n}$ play a knock out tournament. It is known that all players are of equal strength. The tournament is held in three rounds where the players are paired at random in each round. If it is given that $P_1$ wins in the third round then what is the conditional probability that $P_2$ loses in the second round. I tried applying the concept of conditional probability followed by total probability theorem but somehow, there are far too many cases to consider. Any help/ suggestions/ solutions would be highly appreciated.","8n players $P_1$, $P_2$, $P_3$, .....$P{_8}{_n}$ play a knock out tournament. It is known that all players are of equal strength. The tournament is held in three rounds where the players are paired at random in each round. If it is given that $P_1$ wins in the third round then what is the conditional probability that $P_2$ loses in the second round. I tried applying the concept of conditional probability followed by total probability theorem but somehow, there are far too many cases to consider. Any help/ suggestions/ solutions would be highly appreciated.",,"['probability', 'combinatorics', 'combinations']"
14,"For iid random variables $Y_1, \ldots, Y_n$ with cdf $F$, what is $P(Y_n > \max(Y_1, \ldots, Y_{n-1})$?","For iid random variables  with cdf , what is ?","Y_1, \ldots, Y_n F P(Y_n > \max(Y_1, \ldots, Y_{n-1})","For iid random variables $Y_1, \ldots, Y_n$ with cdf $F$, assuming continuous rv's with $F$ strictly increasing on its support, what is $P \left(Y_n > \max(Y_1, \ldots, Y_{n-1}\right)$? My method is: $$ P\left(Y_n > \max(Y_1, \ldots, Y_{n-1}\right) = P(Y_n > Y_{n-1})\cdots P(Y_2>Y_1). $$ Now, intuitively it seems each of these is a half, but I'm not sure if thats correct?","For iid random variables $Y_1, \ldots, Y_n$ with cdf $F$, assuming continuous rv's with $F$ strictly increasing on its support, what is $P \left(Y_n > \max(Y_1, \ldots, Y_{n-1}\right)$? My method is: $$ P\left(Y_n > \max(Y_1, \ldots, Y_{n-1}\right) = P(Y_n > Y_{n-1})\cdots P(Y_2>Y_1). $$ Now, intuitively it seems each of these is a half, but I'm not sure if thats correct?",,"['probability', 'probability-theory']"
15,"Probability: there are $n$ rooms, and $m$ meetings, $m \leq n$, what's the probability of all meetings scheduled to a different room","Probability: there are  rooms, and  meetings, , what's the probability of all meetings scheduled to a different room",n m m \leq n,"Quite new in stats... definitely not my strong area. I came across this probability question, and I am not sure how to do this! The question goes: pretend that there's this meeting scheduling engine used by this   company and is not synced in real time, so when people schedule their   meetings online to book a room, there may be overlaps. let's say there   are $N$ rooms, and $M$ meetings, where $M \leq N$, what is the   probability that all meetings scheduled to a different room? My thought was that, the first meeting doesnt matter, can be in any room; then the 2nd meeting has $\frac{1}{N-1}$ chance of being in a room. so for two rooms not colliding, the chance of them being in separate rooms is $\frac{1}{N-1}$. Right? I am not confident about this one neither... Any hint/advice/guidance helps! update to clarify: 1) each room can only host up to one meeting 2) one meeting can only happen in one room","Quite new in stats... definitely not my strong area. I came across this probability question, and I am not sure how to do this! The question goes: pretend that there's this meeting scheduling engine used by this   company and is not synced in real time, so when people schedule their   meetings online to book a room, there may be overlaps. let's say there   are $N$ rooms, and $M$ meetings, where $M \leq N$, what is the   probability that all meetings scheduled to a different room? My thought was that, the first meeting doesnt matter, can be in any room; then the 2nd meeting has $\frac{1}{N-1}$ chance of being in a room. so for two rooms not colliding, the chance of them being in separate rooms is $\frac{1}{N-1}$. Right? I am not confident about this one neither... Any hint/advice/guidance helps! update to clarify: 1) each room can only host up to one meeting 2) one meeting can only happen in one room",,['probability']
16,Bayes Formula: Intuition,Bayes Formula: Intuition,,I found an intuitive explanation for Bayes' Theorem but I do not understand point 3: With $P(A|B) = \frac{P(A \cap B)}{P(B)}$ I see that P(B) is a scaling factor for partitions of B to be able to sum up to 1. But I can't seem to wrap my head around the meaning of the ratio in Bayes' Theorem.,I found an intuitive explanation for Bayes' Theorem but I do not understand point 3: With $P(A|B) = \frac{P(A \cap B)}{P(B)}$ I see that P(B) is a scaling factor for partitions of B to be able to sum up to 1. But I can't seem to wrap my head around the meaning of the ratio in Bayes' Theorem.,,"['probability', 'statistics']"
17,If $A$ is independent of $B$ and $C$ then why is it not necessarally independent of $B\cap C$?,If  is independent of  and  then why is it not necessarally independent of ?,A B C B\cap C,"I'm attempting to acquire an intuitive understanding of why the content in the question of the title is correct, however I am unable to do so. Is there way of thinking about the result that makes sense?","I'm attempting to acquire an intuitive understanding of why the content in the question of the title is correct, however I am unable to do so. Is there way of thinking about the result that makes sense?",,['probability']
18,About the Moment Generating Function,About the Moment Generating Function,,"Let $X \sim \exp(\lambda)$. Prove that : $E[X^n] = n!/\lambda n$ The hint in (Fundamentals of probability) gives the hint use the Moment Generating Function, but I can't see how I should use that to prove that this is the respective expression that follows.","Let $X \sim \exp(\lambda)$. Prove that : $E[X^n] = n!/\lambda n$ The hint in (Fundamentals of probability) gives the hint use the Moment Generating Function, but I can't see how I should use that to prove that this is the respective expression that follows.",,['probability']
19,Question on proof of linearity of expectation involving discrete random variables,Question on proof of linearity of expectation involving discrete random variables,,"Please see the proof below regarding the linearity of expectation given two discrete random variables $X$ and $Y$. I'm not understanding how the first highlighted step moves to the next highlighted step. I've looked online and seen mention that this relates to the law of total probability... but after looking up some more information on the law of total probability I can't see how this law is applied here. $$\begin{align*}E[X+Y]&=\sum_x\sum_y [(x+y)\cdot P(X=x, Y=y)]\\ &=\sum_x\sum_y [x\cdot P(X=x, Y=y)]+\sum_x\sum_y [y\cdot P(X=x, Y=y)]\\&=\bbox[yellow]{\sum_xx\sum_y P(X=x, Y=y)+\sum_xy\sum_y [P(X=x, Y=y)]}\\ &=\bbox[yellow]{\sum_x x \cdot P(X=x)+\sum_y y\cdot P(Y=y)}\\ &=E[X]+E[Y]\end{align*}$$","Please see the proof below regarding the linearity of expectation given two discrete random variables $X$ and $Y$. I'm not understanding how the first highlighted step moves to the next highlighted step. I've looked online and seen mention that this relates to the law of total probability... but after looking up some more information on the law of total probability I can't see how this law is applied here. $$\begin{align*}E[X+Y]&=\sum_x\sum_y [(x+y)\cdot P(X=x, Y=y)]\\ &=\sum_x\sum_y [x\cdot P(X=x, Y=y)]+\sum_x\sum_y [y\cdot P(X=x, Y=y)]\\&=\bbox[yellow]{\sum_xx\sum_y P(X=x, Y=y)+\sum_xy\sum_y [P(X=x, Y=y)]}\\ &=\bbox[yellow]{\sum_x x \cdot P(X=x)+\sum_y y\cdot P(Y=y)}\\ &=E[X]+E[Y]\end{align*}$$",,"['probability', 'proof-verification']"
20,Understanding a recurrence to solve the Coupon Collector problem?,Understanding a recurrence to solve the Coupon Collector problem?,,I recently came across this recurrence for the coupon collector problem: $$\text{draws}(n) = \text{draws}(n-1) \cdot\dfrac{n}{n-1} + 1$$ where $\text{draws}(n)$ is the expected number of coupons to be drawn to get all $n$ unique coupons. Why is this recurrence true? I would prefer an intuitive approach.,I recently came across this recurrence for the coupon collector problem: $$\text{draws}(n) = \text{draws}(n-1) \cdot\dfrac{n}{n-1} + 1$$ where $\text{draws}(n)$ is the expected number of coupons to be drawn to get all $n$ unique coupons. Why is this recurrence true? I would prefer an intuitive approach.,,"['probability', 'combinatorics', 'recurrence-relations']"
21,Distribution of the sum of binomial random variables,Distribution of the sum of binomial random variables,,"What is the distribution of the sum of random variables given by $$n = n_1 + n_2 + \cdots + n_k $$ where each $n_i$ is binomially distributed random variable define by $B(n_i,p)$. I'm not sure how to approach this problem. I believe that the random variable with mean $\eta = \eta_1 + \cdots + \eta_n$ and variance $\sigma^2 = \sigma_1^2 + \cdots +\sigma_n^2$ The central limit theorem states that under certain general conditions, the distribution $F(x)$ of $X$ approaches a normal distribution with the same mean and variance. So does this apply in this same situation? Thanks for your help in solving this, I really appreciate it! Update : The random variables can be treated as independent, which should help make the answer significantly easier","What is the distribution of the sum of random variables given by $$n = n_1 + n_2 + \cdots + n_k $$ where each $n_i$ is binomially distributed random variable define by $B(n_i,p)$. I'm not sure how to approach this problem. I believe that the random variable with mean $\eta = \eta_1 + \cdots + \eta_n$ and variance $\sigma^2 = \sigma_1^2 + \cdots +\sigma_n^2$ The central limit theorem states that under certain general conditions, the distribution $F(x)$ of $X$ approaches a normal distribution with the same mean and variance. So does this apply in this same situation? Thanks for your help in solving this, I really appreciate it! Update : The random variables can be treated as independent, which should help make the answer significantly easier",,"['probability', 'random-variables', 'binomial-distribution']"
22,Markov property misunderstanding,Markov property misunderstanding,,"I am reading 'Green, Brown & Probability' by Kai Lai Chung, and on chapter 5 (Markov Property) page 30, it says that the Markov property DOES NOT says that for $T=2$, $P\big(3.14< \lvert X_3 \rvert <3.15 \big| 3<\lvert X_1 \rvert<4 , 3.1 <\lvert X_2 \rvert< 3.2  \big)=P\big(3.14< \lvert X_3 \rvert <3.15 \big| 3.1 <\lvert X_2 \rvert< 3.2\big)$ 'namely that the past $3 < \lvert X_1 \rvert < 4$ may well have an after- effect on the future when the present \X2\ is given as shown'. But I don't see it, is this related to independence of increments?","I am reading 'Green, Brown & Probability' by Kai Lai Chung, and on chapter 5 (Markov Property) page 30, it says that the Markov property DOES NOT says that for $T=2$, $P\big(3.14< \lvert X_3 \rvert <3.15 \big| 3<\lvert X_1 \rvert<4 , 3.1 <\lvert X_2 \rvert< 3.2  \big)=P\big(3.14< \lvert X_3 \rvert <3.15 \big| 3.1 <\lvert X_2 \rvert< 3.2\big)$ 'namely that the past $3 < \lvert X_1 \rvert < 4$ may well have an after- effect on the future when the present \X2\ is given as shown'. But I don't see it, is this related to independence of increments?",,"['probability', 'stochastic-processes', 'markov-chains', 'markov-process']"
23,Picking certain number of balls without replacement and finding its probability distribution,Picking certain number of balls without replacement and finding its probability distribution,,"Suppose I am to choose three balls without replacement from a bag containing $5$ white and $4$ red balls. What will be the probability distribution of the red balls drawn ?. According to my book, probability function will be $$ {3\choose x}\left(\,{4 \over 9}\,\right)^{x}\left(\,{5 \over 9}\,\right)^{3 - x} $$ What I didn't understand is why my book is taking probability of choosing red ball to be $4/9$ and the probability of choosing a white ball to be $5/9$. I think the above probabilities are of choosing the red and the white balls in the first trial. In other trials the probability of the above two events will change as we are drawing balls without replacement.","Suppose I am to choose three balls without replacement from a bag containing $5$ white and $4$ red balls. What will be the probability distribution of the red balls drawn ?. According to my book, probability function will be $$ {3\choose x}\left(\,{4 \over 9}\,\right)^{x}\left(\,{5 \over 9}\,\right)^{3 - x} $$ What I didn't understand is why my book is taking probability of choosing red ball to be $4/9$ and the probability of choosing a white ball to be $5/9$. I think the above probabilities are of choosing the red and the white balls in the first trial. In other trials the probability of the above two events will change as we are drawing balls without replacement.",,"['probability', 'probability-distributions']"
24,Expected value of a geometric distribution with first step analysis.,Expected value of a geometric distribution with first step analysis.,,"I am trying to understand the ""story proof"" found in this lecture . I am a bit confused as how the expected value of a random variable differs from the the random variable itself when considering indicator functions. Say there is a geometric distribution . $X$ counts the number of the failures before the first success, and $E(X)$ is the expected number of failures. Now I want to compute the expected value given $p$, the probability of success,  and $q$ otherwise. Let $c=E(X)$. Now I do first-step analysis, $$c=0\cdot p+(1+c)q.$$ In this step, I don't understand the coefficient of $q$. In $(1+c)$, $1$ makes sense but why $c$? When computing the expected value: it is the $kp^kq^k$. So the coefficient $k$ is the value of the random variable. But in my example it is the expected value $c$ at the next step which confuses me.","I am trying to understand the ""story proof"" found in this lecture . I am a bit confused as how the expected value of a random variable differs from the the random variable itself when considering indicator functions. Say there is a geometric distribution . $X$ counts the number of the failures before the first success, and $E(X)$ is the expected number of failures. Now I want to compute the expected value given $p$, the probability of success,  and $q$ otherwise. Let $c=E(X)$. Now I do first-step analysis, $$c=0\cdot p+(1+c)q.$$ In this step, I don't understand the coefficient of $q$. In $(1+c)$, $1$ makes sense but why $c$? When computing the expected value: it is the $kp^kq^k$. So the coefficient $k$ is the value of the random variable. But in my example it is the expected value $c$ at the next step which confuses me.",,"['probability', 'probability-distributions', 'expectation']"
25,Finding large deviation bound for binomial distribution,Finding large deviation bound for binomial distribution,,"$S \sim Binomial(n, p)$. $\forall a > p$, find large deviation bound for $P( S \geq an)$ In the book, the large deviation bound definition is as follows: $\phi(t)$ is finite for some $t > 0$, $\forall a > \mu$, $P(S_n \geq an) \leq e^{-nI(a)}$, where $I(a) = sup\left \{at - log \phi(t): t > 0\right \} > 0$ My attempt at solving the problem: $P( S\geq an)$ $= P(e^{tS} \geq e^{tan})$ $ \leq \frac{E(e^{tS})}{e^{tan}}$ (by Markov inequality) $= \frac{e^{n log\phi_x(t))}}{e^{tan}}$ $= e^{[-n(at - log \phi_x(t)]}$ I know that $I(a) = sup\left \{at - log \phi_x(t): t > 0\right \} > 0$, and the answer key says that $I(a) = a log \frac{a}{p} + (1-a) log \frac{1-a}{1-p}$, but I have no idea how it arrived at that. I think I have to do something with the bernoulli mgf?","$S \sim Binomial(n, p)$. $\forall a > p$, find large deviation bound for $P( S \geq an)$ In the book, the large deviation bound definition is as follows: $\phi(t)$ is finite for some $t > 0$, $\forall a > \mu$, $P(S_n \geq an) \leq e^{-nI(a)}$, where $I(a) = sup\left \{at - log \phi(t): t > 0\right \} > 0$ My attempt at solving the problem: $P( S\geq an)$ $= P(e^{tS} \geq e^{tan})$ $ \leq \frac{E(e^{tS})}{e^{tan}}$ (by Markov inequality) $= \frac{e^{n log\phi_x(t))}}{e^{tan}}$ $= e^{[-n(at - log \phi_x(t)]}$ I know that $I(a) = sup\left \{at - log \phi_x(t): t > 0\right \} > 0$, and the answer key says that $I(a) = a log \frac{a}{p} + (1-a) log \frac{1-a}{1-p}$, but I have no idea how it arrived at that. I think I have to do something with the bernoulli mgf?",,"['probability', 'probability-distributions', 'stochastic-processes', 'large-deviation-theory']"
26,"Interpretation help: Showing that Riemann Hypothesis holds ""almost surely""","Interpretation help: Showing that Riemann Hypothesis holds ""almost surely""",,"I was perusing this textbook on algorithmic number theory , where I came across this page where they appear to prove that the Riemann Hypothesis holds almost surely . This seems like an odd statement for something that is not random (a Theorem). Yet they derive a probability measure for this and proceed forward... I've heard of the ""probabilistic method"" ala Erdos, but this seems different. It's not establishing the existence of something, but making a statement about the truth value of a theorem. Note: This question is similar to one I asked some time back under a now-defunct user name. That also referenced a work (Goldbach's conjecture) that used probability in number theory, but non in an the manner of Erdos. Question For the Riemann Hypothesis, how would I interpret the statement that this hypothesis is ""almost surely true"" ...would this have any weight in the mathematical community or is it mainly a useful heuristic for producing algorithms (aka...we can assume the hypothesis is true due to the high ""probability"" of it being true)...?","I was perusing this textbook on algorithmic number theory , where I came across this page where they appear to prove that the Riemann Hypothesis holds almost surely . This seems like an odd statement for something that is not random (a Theorem). Yet they derive a probability measure for this and proceed forward... I've heard of the ""probabilistic method"" ala Erdos, but this seems different. It's not establishing the existence of something, but making a statement about the truth value of a theorem. Note: This question is similar to one I asked some time back under a now-defunct user name. That also referenced a work (Goldbach's conjecture) that used probability in number theory, but non in an the manner of Erdos. Question For the Riemann Hypothesis, how would I interpret the statement that this hypothesis is ""almost surely true"" ...would this have any weight in the mathematical community or is it mainly a useful heuristic for producing algorithms (aka...we can assume the hypothesis is true due to the high ""probability"" of it being true)...?",,"['probability', 'number-theory']"
27,Number of steps in a 2D random walk return to origin,Number of steps in a 2D random walk return to origin,,"We have a random walk in 2D. In this many dimensions, we return to the origin with probability $1$. However, the number of steps it takes to do so seems to vary greatly from computer simulations I've ran. Thus, I'm curious about the distribution concerning the number of steps required for one to return to the origin in a 2D random walk. An explicit probability for each $n$ steps would be fantastic, but an expected value will do, too. I'm just curious about the topic in general. For what it's worth, I have looked into the 1D case, which seems a quite bit easier.","We have a random walk in 2D. In this many dimensions, we return to the origin with probability $1$. However, the number of steps it takes to do so seems to vary greatly from computer simulations I've ran. Thus, I'm curious about the distribution concerning the number of steps required for one to return to the origin in a 2D random walk. An explicit probability for each $n$ steps would be fantastic, but an expected value will do, too. I'm just curious about the topic in general. For what it's worth, I have looked into the 1D case, which seems a quite bit easier.",,"['probability', 'random-walk']"
28,What is the probability of dealing all $52$ cards in a standard well shuffled deck getting a single quad?,What is the probability of dealing all  cards in a standard well shuffled deck getting a single quad?,52,"I would like to know if you take a well shuffled deck of $52$ cards and then deal out all of them one at a time without replacement, what is the probability that they will be dealt such that there is exactly one quad dealt in order such as $4$ consecutive kings?  That run of $4$ has to be the only run of that length, there cannot be any others.  For example, 5, ... ,K,K,K,K,7,J,J,J,J is a ""loser"".  Other than computer simulation I am not sure how to solve this.","I would like to know if you take a well shuffled deck of $52$ cards and then deal out all of them one at a time without replacement, what is the probability that they will be dealt such that there is exactly one quad dealt in order such as $4$ consecutive kings?  That run of $4$ has to be the only run of that length, there cannot be any others.  For example, 5, ... ,K,K,K,K,7,J,J,J,J is a ""loser"".  Other than computer simulation I am not sure how to solve this.",,"['probability', 'inclusion-exclusion']"
29,"Probability: mathematically what does it mean to say ""let $X$ be a random variable WITH a cdf/pdf""","Probability: mathematically what does it mean to say ""let  be a random variable WITH a cdf/pdf""",X,"I don't quite understand what people mean by let ""$X$ be a random variable WITH a cdf/pdf"". For example, there is a question that says: ""Let X be a random variable with the 3-parameter Weibull pdf and cdf"" Suppose I say: Let $X$ be a random variable with Gaussian CDF. What does that mean exactly? $X$ is a function that maps from the event space to a real number. What does it mean to ""connect"" it, ""link"" it, ""equip"" it, WITH a CDF? It is already a function, what does it mean to let it hook up with another function, why don't we just deal with $X$ directly. Then I look at the CDF: $f_X(x) = \int_A \exp(-x^2/2)dx$ I ask myself: Is ""$\exp(-x^2/2)$"" part the random variable? No. Is ""$\int_A \exp(-x^2/2)dx$"" the random variable? No. Is little $x$ is the random variable? No. What is the difference if I wrote $f(x)$ instead of $f_X(x)$? Nothing happens. What does the CDF/PDF have to do with the random variable exactly? How do you know which CDF/PDF a random variable ""has""?","I don't quite understand what people mean by let ""$X$ be a random variable WITH a cdf/pdf"". For example, there is a question that says: ""Let X be a random variable with the 3-parameter Weibull pdf and cdf"" Suppose I say: Let $X$ be a random variable with Gaussian CDF. What does that mean exactly? $X$ is a function that maps from the event space to a real number. What does it mean to ""connect"" it, ""link"" it, ""equip"" it, WITH a CDF? It is already a function, what does it mean to let it hook up with another function, why don't we just deal with $X$ directly. Then I look at the CDF: $f_X(x) = \int_A \exp(-x^2/2)dx$ I ask myself: Is ""$\exp(-x^2/2)$"" part the random variable? No. Is ""$\int_A \exp(-x^2/2)dx$"" the random variable? No. Is little $x$ is the random variable? No. What is the difference if I wrote $f(x)$ instead of $f_X(x)$? Nothing happens. What does the CDF/PDF have to do with the random variable exactly? How do you know which CDF/PDF a random variable ""has""?",,"['probability', 'soft-question', 'terminology', 'random-variables', 'definition']"
30,When is random selection skewed (untrustworthy)?,When is random selection skewed (untrustworthy)?,,"Imagine there is a population of 100 people, out of which 3 are to be randomly selected each day for alcohol testing. After a month of such selections (after 20 selections), how many times somebody needs to be selected before I need to worry about the randomness in the process? Real data: I got two people selected 3 times each; seven people selected 2 times, and a bunch of them selected 1 time (and another bunch never selected) in the previous month. If the same person gets selected everyday for 20 days, there certainly ( about 99.9999999999% ) is something wrong. What's the chance that there's something when a person is selected 5 times? What is he's selected 4 times? 3 times? What if two people get selected 4 times each? ... When should I start thinking about making weighted selections? How do I go about making these kind of calculations?","Imagine there is a population of 100 people, out of which 3 are to be randomly selected each day for alcohol testing. After a month of such selections (after 20 selections), how many times somebody needs to be selected before I need to worry about the randomness in the process? Real data: I got two people selected 3 times each; seven people selected 2 times, and a bunch of them selected 1 time (and another bunch never selected) in the previous month. If the same person gets selected everyday for 20 days, there certainly ( about 99.9999999999% ) is something wrong. What's the chance that there's something when a person is selected 5 times? What is he's selected 4 times? 3 times? What if two people get selected 4 times each? ... When should I start thinking about making weighted selections? How do I go about making these kind of calculations?",,"['probability', 'statistics', 'random']"
31,"Probability and the ""out of"" thing""","Probability and the ""out of"" thing""",,"I have quite an odd question: I am not able to fully understand the concept of ""out of"". If I roll a dice once, from a total of $6$ possible outcomes, I'll get 1. Why does that mean a fraction $1\over 6$ = approx $16.67 \%$ and why does that mean that on average one out of $6$ rolls, I will get for example ""$1$"" on dice. Where does the fraction say that for every $6$ rolls, I'll get on average one roll I wanted to get. Why does $5$ out of $7$ mean $5\over 7$, why does that mean that it's on average $5$ per every $7$ people? Because when I want to get $5\over 7$ of something, I divide something into $7$ parts and get $5$. Is that the second look at this matter, that it can be seen like, for example: for every $7$ (divide some number by $7$ to find out how many $7$s are there) and then multiply by $5$, because for every seven that is included in the number it will be $5$. Are my thoughts correct? How is the correct way of seeing these things? Thanks for help in advance. dont answer like Maths 90-page long thesis, I just want an answer that is en explanation in your own words. What I struggle is probably the fractions, what does out of mean and why... and you explain everything but this.","I have quite an odd question: I am not able to fully understand the concept of ""out of"". If I roll a dice once, from a total of $6$ possible outcomes, I'll get 1. Why does that mean a fraction $1\over 6$ = approx $16.67 \%$ and why does that mean that on average one out of $6$ rolls, I will get for example ""$1$"" on dice. Where does the fraction say that for every $6$ rolls, I'll get on average one roll I wanted to get. Why does $5$ out of $7$ mean $5\over 7$, why does that mean that it's on average $5$ per every $7$ people? Because when I want to get $5\over 7$ of something, I divide something into $7$ parts and get $5$. Is that the second look at this matter, that it can be seen like, for example: for every $7$ (divide some number by $7$ to find out how many $7$s are there) and then multiply by $5$, because for every seven that is included in the number it will be $5$. Are my thoughts correct? How is the correct way of seeing these things? Thanks for help in advance. dont answer like Maths 90-page long thesis, I just want an answer that is en explanation in your own words. What I struggle is probably the fractions, what does out of mean and why... and you explain everything but this.",,"['probability', 'fractions']"
32,r distinct balls in N boxes,r distinct balls in N boxes,,"If r distinct balls are distributed at random into N (N ≤ r) boxes, what is the probability that box 1 will receive exactly j balls ( 0 ≤ 𝒋 ≤ r)? my solution is  [sample space] =$ N^r $ $$P=\frac{ 1}{N^r}\binom{r}{j}$$ I know there is something wrong. can you help me ?","If r distinct balls are distributed at random into N (N ≤ r) boxes, what is the probability that box 1 will receive exactly j balls ( 0 ≤ 𝒋 ≤ r)? my solution is  [sample space] =$ N^r $ $$P=\frac{ 1}{N^r}\binom{r}{j}$$ I know there is something wrong. can you help me ?",,['probability']
33,"Probability of choosing two real numbers $a$ and $b$ from $[1,4]$ such that $ ab>4$.",Probability of choosing two real numbers  and  from  such that .,"a b [1,4]  ab>4","What is the probability that when you pick two real numbers from the closed interval $[1,4]$, their product is greater than 4? I tried to solve it with integration but I couldn't get the right answer. And I think that this problem can be solved without integration.","What is the probability that when you pick two real numbers from the closed interval $[1,4]$, their product is greater than 4? I tried to solve it with integration but I couldn't get the right answer. And I think that this problem can be solved without integration.",,['probability']
34,probability density of the maximum of samples from a normalized uniform distribution,probability density of the maximum of samples from a normalized uniform distribution,,"Suppose $$X_1, X_2, \dots, X_n\sim Unif(0, 1), iid$$ and suppose $$\hat\theta = \max\{X_1, X_2, \dots, X_n\} / \sum_i^nX_i$$ How would I find the probability density of $\hat\theta$? I know the answer if it's iid. But I don't know how to formalize the fact that the sum is iqual to 1. a simiar question can be found here: probability density of the maximum of samples from a uniform distribution I arrive here: \begin{align} P(Y\leq x)&=P(\max(X_1,X_2 ,\cdots,X_n)/\sum_i^nX_i\leq x)\\&=P(X_1/\sum_i^nX_i\leq x,X_2/\sum_i^nX_i\leq x,\cdots,X_n/\sum_i^nX_i\leq x)\\ &\stackrel{ind}{=} \prod_{j=1}^nP(X_j/\sum_i^nX_i\leq x )\\& \ \ \ \ \  \end{align}","Suppose $$X_1, X_2, \dots, X_n\sim Unif(0, 1), iid$$ and suppose $$\hat\theta = \max\{X_1, X_2, \dots, X_n\} / \sum_i^nX_i$$ How would I find the probability density of $\hat\theta$? I know the answer if it's iid. But I don't know how to formalize the fact that the sum is iqual to 1. a simiar question can be found here: probability density of the maximum of samples from a uniform distribution I arrive here: \begin{align} P(Y\leq x)&=P(\max(X_1,X_2 ,\cdots,X_n)/\sum_i^nX_i\leq x)\\&=P(X_1/\sum_i^nX_i\leq x,X_2/\sum_i^nX_i\leq x,\cdots,X_n/\sum_i^nX_i\leq x)\\ &\stackrel{ind}{=} \prod_{j=1}^nP(X_j/\sum_i^nX_i\leq x )\\& \ \ \ \ \  \end{align}",,"['probability', 'probability-theory', 'probability-distributions']"
35,People sitting in a round table.,People sitting in a round table.,,"I'm practicing for the actuarial examination and I found this problem that I couldn't solve. If someone can help me, I will be happy :-). Five Americans, three Germans and four Italians go to dinner together.   They randomly sit at a round table with twelve chairs. What is the   probability that the people of the same nationality sit together? 5----American 3----German 4----Italian  Total=12  Chairs=12   Total outcomes= 12! Then, I was thinking about sitting first the Americans together and then the Germans and then the Italians together, but there are so many possibilities that I believe can't no be right. Are there any quick way? or correct one?","I'm practicing for the actuarial examination and I found this problem that I couldn't solve. If someone can help me, I will be happy :-). Five Americans, three Germans and four Italians go to dinner together.   They randomly sit at a round table with twelve chairs. What is the   probability that the people of the same nationality sit together? 5----American 3----German 4----Italian  Total=12  Chairs=12   Total outcomes= 12! Then, I was thinking about sitting first the Americans together and then the Germans and then the Italians together, but there are so many possibilities that I believe can't no be right. Are there any quick way? or correct one?",,['probability']
36,Prove that $\mathbb{P}(X > \lambda \mathbb{E}[X]) \geq (1-\lambda)^2\frac{\mathbb{E}[X]^2}{\mathbb{E}[X^2]}$,Prove that,\mathbb{P}(X > \lambda \mathbb{E}[X]) \geq (1-\lambda)^2\frac{\mathbb{E}[X]^2}{\mathbb{E}[X^2]},"I am trying to prove that $\mathbb{P}(X > \lambda \mathbb{E}[X]) \geq (1-\lambda)^2\frac{\mathbb{E}[X]^2}{\mathbb{E}[X^2]}$ for all non-negative random variables X with $0 \leq \lambda < 1$. I have tried many different paths, however the most successful one so far involves the Cauchy-Schwarz inequality, which states: $$\mathbb{E}[X Y] \leq \sqrt{\mathbb{E}[X^2]\mathbb{E}[Y^2]}$$ This implies: $$\mathbb{E}[Y^2] \geq \frac{\mathbb{E}[X Y]^2}{\mathbb{E}[X^2]}$$ Now I think with a proper choice for Y I might be able to obtain the statement I am trying to prove. I tried for instance $Y=1-\lambda$ which I believe leads to: $$(1-\lambda)^2 \geq (1-\lambda)^2\frac{\mathbb{E}[X]^2}{\mathbb{E}[X^2]}$$ This is the closest I got to the statement, however now I would have to prove that $\mathbb{P}(X > \lambda \mathbb{E}[X]) \geq (1-\lambda)^2$ at which I got stuck. Am I trying the right things here or am I completely going down a dead path? If so, what could be a sensible choice for $Y$? Thanks in advance!","I am trying to prove that $\mathbb{P}(X > \lambda \mathbb{E}[X]) \geq (1-\lambda)^2\frac{\mathbb{E}[X]^2}{\mathbb{E}[X^2]}$ for all non-negative random variables X with $0 \leq \lambda < 1$. I have tried many different paths, however the most successful one so far involves the Cauchy-Schwarz inequality, which states: $$\mathbb{E}[X Y] \leq \sqrt{\mathbb{E}[X^2]\mathbb{E}[Y^2]}$$ This implies: $$\mathbb{E}[Y^2] \geq \frac{\mathbb{E}[X Y]^2}{\mathbb{E}[X^2]}$$ Now I think with a proper choice for Y I might be able to obtain the statement I am trying to prove. I tried for instance $Y=1-\lambda$ which I believe leads to: $$(1-\lambda)^2 \geq (1-\lambda)^2\frac{\mathbb{E}[X]^2}{\mathbb{E}[X^2]}$$ This is the closest I got to the statement, however now I would have to prove that $\mathbb{P}(X > \lambda \mathbb{E}[X]) \geq (1-\lambda)^2$ at which I got stuck. Am I trying the right things here or am I completely going down a dead path? If so, what could be a sensible choice for $Y$? Thanks in advance!",,"['probability', 'probability-theory', 'inequality']"
37,What is the probability that a ﬁve-card poker hand contains cards of ﬁve different kinds?,What is the probability that a ﬁve-card poker hand contains cards of ﬁve different kinds?,,"The textbook also states the following: There are $13$ different kinds of cards, with four cards of each kind.   (Among the terms commonly used instead of “kind” are “rank,” “face   value,” “denomination,” and “value.”) These kinds are twos, threes,   fours, fives, sixes, sevens, eights, nines, tens, jacks, queens,   kings, and aces. There are also four suits: spades, clubs, hearts, and   diamonds, each containing 13 cards, with one card of each kind in a   suit. which describes it what it means by a 'kind'. So far I did the following: $$\Large\frac{\binom{13}{5}}{\binom{52}{5}}$$ I think I am missing something in the numerator however. Maybe $\binom{47}{5}$? But I am not sure how to justify it. Edit: Had to edit the question, so if somebody already started answering please check it.","The textbook also states the following: There are $13$ different kinds of cards, with four cards of each kind.   (Among the terms commonly used instead of “kind” are “rank,” “face   value,” “denomination,” and “value.”) These kinds are twos, threes,   fours, fives, sixes, sevens, eights, nines, tens, jacks, queens,   kings, and aces. There are also four suits: spades, clubs, hearts, and   diamonds, each containing 13 cards, with one card of each kind in a   suit. which describes it what it means by a 'kind'. So far I did the following: $$\Large\frac{\binom{13}{5}}{\binom{52}{5}}$$ I think I am missing something in the numerator however. Maybe $\binom{47}{5}$? But I am not sure how to justify it. Edit: Had to edit the question, so if somebody already started answering please check it.",,"['probability', 'combinatorics', 'discrete-mathematics']"
38,Are fractional/continuous central moments useful?,Are fractional/continuous central moments useful?,,"If the $k$th central moment for a continuous probability density function $f(x)$ is is defined by $$ m_k = \int (x-\mu)^k f(x) dx $$ We get the variance for $m_2$ and the unstandardized skew, kurtosis , etc... for $k=3,4,...$ Does it make sense to go from $k\in\mathbb{N}$ and generalize to fractions or real numbers $k\in\mathbb{Q^+}$ or $k\in\mathbb{R^+}$? To keep this an objective question, I'm looking for an application or use of fractional or continuous moments, either as defined above or in the spirit of the question.","If the $k$th central moment for a continuous probability density function $f(x)$ is is defined by $$ m_k = \int (x-\mu)^k f(x) dx $$ We get the variance for $m_2$ and the unstandardized skew, kurtosis , etc... for $k=3,4,...$ Does it make sense to go from $k\in\mathbb{N}$ and generalize to fractions or real numbers $k\in\mathbb{Q^+}$ or $k\in\mathbb{R^+}$? To keep this an objective question, I'm looking for an application or use of fractional or continuous moments, either as defined above or in the spirit of the question.",,"['probability', 'reference-request']"
39,"In a game of Bridge, what is the probability that all 4 players are dealt 13 cards of the same suit?","In a game of Bridge, what is the probability that all 4 players are dealt 13 cards of the same suit?",,"I was asked this question by a student at my college, and I answered it like this: Since Bridge is played with 4 players, and there are 4 suits per deck of 52 cards, and assuming the deck is a fair, properly shuffled deck of cards, then the probability of 1 player getting 13 of the same suit is $\dfrac {\left( \begin{matrix} 13\\ 13\end{matrix} \right) \left( \begin{matrix} 39\\ 0\end{matrix} \right) } {\left( \begin{matrix} 52\\ 13\end{matrix} \right) }\times4$ This simplifies down to $\dfrac {4} {( \begin{matrix} 52\\ 13\end{matrix} ) }$. But since the question is about the probability of all 4 players getting 13 cards of the same suit, then, since there are $4!$ ways of assigning suits to players, then, isn't the answer $\dfrac {4} {( \begin{matrix} 52\\ 13\end{matrix} ) }\times \dfrac {3} {( \begin{matrix} 39\\ 13\end{matrix} ) }\times \dfrac {2} {( \begin{matrix} 26\\ 13\end{matrix} ) }\times \dfrac {1} {( \begin{matrix} 13\\ 13\end{matrix} ) }$ ? I would like to know if I answered correctly or if there's another way of looking at this. Thanks!","I was asked this question by a student at my college, and I answered it like this: Since Bridge is played with 4 players, and there are 4 suits per deck of 52 cards, and assuming the deck is a fair, properly shuffled deck of cards, then the probability of 1 player getting 13 of the same suit is $\dfrac {\left( \begin{matrix} 13\\ 13\end{matrix} \right) \left( \begin{matrix} 39\\ 0\end{matrix} \right) } {\left( \begin{matrix} 52\\ 13\end{matrix} \right) }\times4$ This simplifies down to $\dfrac {4} {( \begin{matrix} 52\\ 13\end{matrix} ) }$. But since the question is about the probability of all 4 players getting 13 cards of the same suit, then, since there are $4!$ ways of assigning suits to players, then, isn't the answer $\dfrac {4} {( \begin{matrix} 52\\ 13\end{matrix} ) }\times \dfrac {3} {( \begin{matrix} 39\\ 13\end{matrix} ) }\times \dfrac {2} {( \begin{matrix} 26\\ 13\end{matrix} ) }\times \dfrac {1} {( \begin{matrix} 13\\ 13\end{matrix} ) }$ ? I would like to know if I answered correctly or if there's another way of looking at this. Thanks!",,['probability']
40,Calculating probability for forming a triangle,Calculating probability for forming a triangle,,"I am having trouble coming up with a solution for this problem: There is a stick of unit length. We break it into two parts.   Now, we pick the bigger one and break it into two parts.   I want to calculate the probability that the three pieces form a triangle. The problem is from ""Introduction to Probability, Charles M. Grinstead"", Chapter 2.2, Exercise 13","I am having trouble coming up with a solution for this problem: There is a stick of unit length. We break it into two parts.   Now, we pick the bigger one and break it into two parts.   I want to calculate the probability that the three pieces form a triangle. The problem is from ""Introduction to Probability, Charles M. Grinstead"", Chapter 2.2, Exercise 13",,['probability']
41,product of densities,product of densities,,"One can frequently read, that the product of the densities of two INDEPENDENT random variables is also a density - the joint density of the two variables. (see for example: http://en.wikipedia.org/wiki/Joint_probability_distribution#Joint_distribution_for_independent_variables ) One can also read, that IN GENERAL the product of two normal pdf is a Gaussian, but not a normal pdf.  i.e. one would have to multiply the product with a scaling factor (normalization constant) to get a normal pdf. (see for example: http://www.tina-vision.net/docs/memos/2003-003.pdf page 3, first paragraph) My naive interpretation of this would be, that in case of independence, the normalization constant equals 1.  But the formula given for the scaling factor in the second source does not seem to support this... Where is my misunderstanding? Now, for the multivariate case, i.e. the product of two joint pdf, each one being the joint pdf of a vector of jointly normal variables, but the two vectors being independent of each other, one finds: ""The vectors x1, x2 are statistically independent if their joint distribution is f(x1, x2) = f(x1)f(x2) or, equivalently, if f(x1|x2) = f(x1) and f(x2|x1) = f(x2)."" (I do not have enough reputation points to post more than two links, so I replace the ""tt"" in http with ""**"": h**p://www.le.ac.uk/users/dsgp1/COURSES/THIRDMET/MYLECTURES/5XMULTISTAT.pdf, the above quote can be found on page 3, number 6) On the other hand, it says in another source:  ""Suppose f(x) = N (x;1;1) and f(y) = N (x;2;2) are two INDEPENDENT d-dimensional Gaussian densities. Sometimes we want to compute the density which is proportional to the product of the two Gaussian densities, i.e. f(z) = cf(x)f(y), in which  c is a proper normalization constant to make f(z) a valid density function."" h**p://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=28A2A856531E5FF9FABEC67397A87B5D?doi=10.1.1.1.2635&rep=rep1&type=pdf"" - above quote is the first paragraph in section 7 on page 9 - here slightly modified notation. These two last sources seem to contradict each other.  Is there a reconciling fact I'm missing? Finally, the previous source given above provides an application in section 8.2 on page 11, where the Bayes theorem is applied to estimate the vector of mean values of a multivariate normal, the Bayes theorem is written there as follows: f(y|x) = cf(x|y)f(y) with c being a ""normalization constant"". more ""traditional"" descriptions of the Bayes theorem look more like this: f(y|x)f(x) = f(x|y)f(y) (see e.g.:  ""h**p://en.wikipedia.org/wiki/Bayes%27_theorem#For_random_variables"" (slightly rearranged here) This seems to imply, that the ""normalization constant"" is actually (always) 1/f(x). Is this correct? Best, JQ","One can frequently read, that the product of the densities of two INDEPENDENT random variables is also a density - the joint density of the two variables. (see for example: http://en.wikipedia.org/wiki/Joint_probability_distribution#Joint_distribution_for_independent_variables ) One can also read, that IN GENERAL the product of two normal pdf is a Gaussian, but not a normal pdf.  i.e. one would have to multiply the product with a scaling factor (normalization constant) to get a normal pdf. (see for example: http://www.tina-vision.net/docs/memos/2003-003.pdf page 3, first paragraph) My naive interpretation of this would be, that in case of independence, the normalization constant equals 1.  But the formula given for the scaling factor in the second source does not seem to support this... Where is my misunderstanding? Now, for the multivariate case, i.e. the product of two joint pdf, each one being the joint pdf of a vector of jointly normal variables, but the two vectors being independent of each other, one finds: ""The vectors x1, x2 are statistically independent if their joint distribution is f(x1, x2) = f(x1)f(x2) or, equivalently, if f(x1|x2) = f(x1) and f(x2|x1) = f(x2)."" (I do not have enough reputation points to post more than two links, so I replace the ""tt"" in http with ""**"": h**p://www.le.ac.uk/users/dsgp1/COURSES/THIRDMET/MYLECTURES/5XMULTISTAT.pdf, the above quote can be found on page 3, number 6) On the other hand, it says in another source:  ""Suppose f(x) = N (x;1;1) and f(y) = N (x;2;2) are two INDEPENDENT d-dimensional Gaussian densities. Sometimes we want to compute the density which is proportional to the product of the two Gaussian densities, i.e. f(z) = cf(x)f(y), in which  c is a proper normalization constant to make f(z) a valid density function."" h**p://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=28A2A856531E5FF9FABEC67397A87B5D?doi=10.1.1.1.2635&rep=rep1&type=pdf"" - above quote is the first paragraph in section 7 on page 9 - here slightly modified notation. These two last sources seem to contradict each other.  Is there a reconciling fact I'm missing? Finally, the previous source given above provides an application in section 8.2 on page 11, where the Bayes theorem is applied to estimate the vector of mean values of a multivariate normal, the Bayes theorem is written there as follows: f(y|x) = cf(x|y)f(y) with c being a ""normalization constant"". more ""traditional"" descriptions of the Bayes theorem look more like this: f(y|x)f(x) = f(x|y)f(y) (see e.g.:  ""h**p://en.wikipedia.org/wiki/Bayes%27_theorem#For_random_variables"" (slightly rearranged here) This seems to imply, that the ""normalization constant"" is actually (always) 1/f(x). Is this correct? Best, JQ",,"['probability', 'statistics']"
42,Which way to calculate this probability is correct?,Which way to calculate this probability is correct?,,"There is an urn that contains $N$ balls. Each ball might be either white or blue. I dont know how many white balls are in the urn, but my prior is that a ball is blue with probability $b$. Someone iterates through the blue balls and, for each ball independently, they either show it to me, with probability $p$, or they don't. What is the probability that I am shown $v$ balls? Solution 1: $Pr=\sum\limits_{i=0}^{N}{N\choose i}b^{i}(1-b)^{N-i}{i\choose v}p^{v}(1-p)^{i-v}.$ Solution 2: $Pr={N \choose v}(bp)^v(1-bp)^{N-v}.$ I wish the second one is correct, but it must be wrong. Why?","There is an urn that contains $N$ balls. Each ball might be either white or blue. I dont know how many white balls are in the urn, but my prior is that a ball is blue with probability $b$. Someone iterates through the blue balls and, for each ball independently, they either show it to me, with probability $p$, or they don't. What is the probability that I am shown $v$ balls? Solution 1: $Pr=\sum\limits_{i=0}^{N}{N\choose i}b^{i}(1-b)^{N-i}{i\choose v}p^{v}(1-p)^{i-v}.$ Solution 2: $Pr={N \choose v}(bp)^v(1-bp)^{N-v}.$ I wish the second one is correct, but it must be wrong. Why?",,['probability']
43,"Probability of one stock price rising, given probabilities of several prices rising/falling","Probability of one stock price rising, given probabilities of several prices rising/falling",,"So this is the problem: An investor is monitoring stocks from Company A and Company B, which   each either increase or decrease each day. On a given day, suppose   that there is a probability of 0.38 that both stocks will increase in   price, and a probability of 0.11 that both stocks will decrease in   price. Also, there is a probability of 0.16 that the stock from   Company A will decrease while the stock from Company B will increase.   What is the probability that the stock from Company A will increase   while the stock from Company B will decrease? What is the probability   that at least one company will have an increase in the stock price? Things I've written down If the probability for the price of both company's stock to go up is 0.38 then the probably for this to not happen, will be 0.62 & if this does not happen then would that mean at least one will decrease? Same thing for the probability for both to decrease since it's .11 then the probability for this not to happen, or in other words for at least one to increase will be .89? I know the respective answers should be .35 & .89, with .89 being the same as the second thing I wrote down but this seems rather semantic to me. I can also get the first answer by adding .38+.11+.16 = .65 then 1-.65 = .35 but I can't work out in my head why that would work. Some help please?","So this is the problem: An investor is monitoring stocks from Company A and Company B, which   each either increase or decrease each day. On a given day, suppose   that there is a probability of 0.38 that both stocks will increase in   price, and a probability of 0.11 that both stocks will decrease in   price. Also, there is a probability of 0.16 that the stock from   Company A will decrease while the stock from Company B will increase.   What is the probability that the stock from Company A will increase   while the stock from Company B will decrease? What is the probability   that at least one company will have an increase in the stock price? Things I've written down If the probability for the price of both company's stock to go up is 0.38 then the probably for this to not happen, will be 0.62 & if this does not happen then would that mean at least one will decrease? Same thing for the probability for both to decrease since it's .11 then the probability for this not to happen, or in other words for at least one to increase will be .89? I know the respective answers should be .35 & .89, with .89 being the same as the second thing I wrote down but this seems rather semantic to me. I can also get the first answer by adding .38+.11+.16 = .65 then 1-.65 = .35 but I can't work out in my head why that would work. Some help please?",,['probability']
44,Countrymen seated around a round table probability question,Countrymen seated around a round table probability question,,"Seated around the table are: - 2 Americans - 2 Canadians - 2 Mexicans - 2 Jamaicans. Each countryman is distinguishable. How many ways possible can all 8 people be seated such that AT LEAST TWO men from the same country sit next to each other? Rotations are considered the same. I know that I have to use PIE somehow on the two men, but it is slightly more confusing because there are four groups and two men in each group. Any hints? EDIT 12/1/2014: I checked the original problem statement and it said that ""In how many ways can all eight people be seated such that at least two pairs of countrymen are seated together?"". I mistakenly put ""at least two men"" rather than ""at least two pairs"". I'm terribly sorry for the confusion!","Seated around the table are: - 2 Americans - 2 Canadians - 2 Mexicans - 2 Jamaicans. Each countryman is distinguishable. How many ways possible can all 8 people be seated such that AT LEAST TWO men from the same country sit next to each other? Rotations are considered the same. I know that I have to use PIE somehow on the two men, but it is slightly more confusing because there are four groups and two men in each group. Any hints? EDIT 12/1/2014: I checked the original problem statement and it said that ""In how many ways can all eight people be seated such that at least two pairs of countrymen are seated together?"". I mistakenly put ""at least two men"" rather than ""at least two pairs"". I'm terribly sorry for the confusion!",,['probability']
45,"Rigorous proof of marginalization in probability, i.e. $P_{X}(x) = \sum_{\hat{y} \in \mathcal{Y}} P_{X,Y}(x,\hat{y})$","Rigorous proof of marginalization in probability, i.e.","P_{X}(x) = \sum_{\hat{y} \in \mathcal{Y}} P_{X,Y}(x,\hat{y})","How do you proof the marginalization rule of probability? i.e. what is the proof for: $$P_{X}(x) = \sum_{\hat{y} \in \mathcal{Y}} P_{X,Y}(x,\hat{y})$$ I managed to get a ""picture proof"" by drawing a venn diagram and then looking at the following equation: $$ P_{X}(x) = \sum_{\hat{y} \in \mathcal{Y}} P_{X,Y}(x,\hat{y}) = P_{X,Y}(x,y) + P_{X,Y}(x,\bar{y}) $$ Which by inspecting the venn diagram: One can notice that translating $P_{X,Y}(x,y) + P_{X,Y}(x,\bar{y})$ into sets gives: $$X \cap Y$$ and $$X \cap \bar{Y}$$ Which covers the whole ""area"" of X. I guess this makes sense in this case, but I feel that there are some issues with the ""proof"". Its a proof by picture (not a real proof) Does not generalize very well for an alphabet size of more values or more random variables. Not sure how to generalize this for continuous random variables Does not feel rigorous enough (probably because of the previous reasons). I was wondering, is this just an axiom of probability or can it be derived from more basic axioms? I am having a hard time generalizing this. Also, I thought this would have been a basic result in probability and should be in the web bus was unable to find any good rigorous reference. If possible I'd like an official reference too e.g. a textbook.","How do you proof the marginalization rule of probability? i.e. what is the proof for: I managed to get a ""picture proof"" by drawing a venn diagram and then looking at the following equation: Which by inspecting the venn diagram: One can notice that translating into sets gives: and Which covers the whole ""area"" of X. I guess this makes sense in this case, but I feel that there are some issues with the ""proof"". Its a proof by picture (not a real proof) Does not generalize very well for an alphabet size of more values or more random variables. Not sure how to generalize this for continuous random variables Does not feel rigorous enough (probably because of the previous reasons). I was wondering, is this just an axiom of probability or can it be derived from more basic axioms? I am having a hard time generalizing this. Also, I thought this would have been a basic result in probability and should be in the web bus was unable to find any good rigorous reference. If possible I'd like an official reference too e.g. a textbook.","P_{X}(x) = \sum_{\hat{y} \in \mathcal{Y}} P_{X,Y}(x,\hat{y})  P_{X}(x) = \sum_{\hat{y} \in \mathcal{Y}} P_{X,Y}(x,\hat{y}) = P_{X,Y}(x,y) + P_{X,Y}(x,\bar{y})  P_{X,Y}(x,y) + P_{X,Y}(x,\bar{y}) X \cap Y X \cap \bar{Y}","['probability', 'probability-theory']"
46,Flipping coins probability of $6$ flips having more heads than $5$ flips.,Flipping coins probability of  flips having more heads than  flips.,6 5,"I have $6$ fair coins and you have $5$ fair coins. We both flip our own coins and observe the number of heads we each have. What is the probability that I have more heads than you? Not sure how to start this, any help please?","I have $6$ fair coins and you have $5$ fair coins. We both flip our own coins and observe the number of heads we each have. What is the probability that I have more heads than you? Not sure how to start this, any help please?",,"['probability', 'discrete-mathematics']"
47,"Proof that if $Z$ is standard normal, then $Z^2$ is distributed Chi-Square (1).","Proof that if  is standard normal, then  is distributed Chi-Square (1).",Z Z^2,"Suppose that $Z\sim N(0,1)$ and let $V=Z^2$.  Prove that $V\sim \chi^2(1)$. I want to use the method of moment generating functions, because I already understand the proof using the method of distribution functions.  I will show my work, and then where I got stuck. Since $Z\sim N(0,1)$, then $\mu=0$ and $\sigma^2=1$, and we have $$M_V(t) =E[e^{tV}]=E[e^{tZ^2}]=\int_{-\infty}^\infty e^{tz^2}\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}z^2}dz=\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}e^{z^2(t-\frac{1}{2})}dz.$$ At this point, I'm out of ideas.  I want to eventually get something that looks like $\frac{1}{(1-2t)^{\frac{1}{2}}}$.  Could I get a hint please?","Suppose that $Z\sim N(0,1)$ and let $V=Z^2$.  Prove that $V\sim \chi^2(1)$. I want to use the method of moment generating functions, because I already understand the proof using the method of distribution functions.  I will show my work, and then where I got stuck. Since $Z\sim N(0,1)$, then $\mu=0$ and $\sigma^2=1$, and we have $$M_V(t) =E[e^{tV}]=E[e^{tZ^2}]=\int_{-\infty}^\infty e^{tz^2}\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}z^2}dz=\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}e^{z^2(t-\frac{1}{2})}dz.$$ At this point, I'm out of ideas.  I want to eventually get something that looks like $\frac{1}{(1-2t)^{\frac{1}{2}}}$.  Could I get a hint please?",,"['probability', 'statistics', 'normal-distribution', 'moment-generating-functions']"
48,Difference between Borel Sigma algebra and Cylindrical sigma algebra?,Difference between Borel Sigma algebra and Cylindrical sigma algebra?,,"I see that there are two differen concepts for Sigma Algebras on cartesian products over the real numbers. The first one is the Borel Sigma Algebra created by the product topology. The other one is the cylindrical sigma algebra.  Actually, when I read the definition of cylinder sets via projections see Wikipedia , I first thought that these concepts would be the same. Somehow, this seems to be wrong. So how are they related to each other. Is one of them coarser than the other or are they in general incomparable and why is the cylindrical one used in probability theory?","I see that there are two differen concepts for Sigma Algebras on cartesian products over the real numbers. The first one is the Borel Sigma Algebra created by the product topology. The other one is the cylindrical sigma algebra.  Actually, when I read the definition of cylinder sets via projections see Wikipedia , I first thought that these concepts would be the same. Somehow, this seems to be wrong. So how are they related to each other. Is one of them coarser than the other or are they in general incomparable and why is the cylindrical one used in probability theory?",,"['real-analysis', 'probability']"
49,Conditional Probabilities,Conditional Probabilities,,"I am faced with this question: $10$ % of all email you receive is spam. Your spam filter is $90$ % reliable, that is, $90$ % of the mails it marks as spam are indeed spam and $90$ % of spam mails are correctly labelled as spam. If you see a mail marked spam by your filter, what is the probability that it is really spam? This question was posed in a Chennai Mathematical Institute exam. This is how I'm trying it. $10$ % of all email you receive is spam $$ P(\text{spam}) = 0.1 $$ $90$ % of the mails it marks as spam are indeed spam $$ P(\text{spam|marked as spam}) = 0.9 $$ $90$ % of spam mails are correctly labelled as spam $$ P(\text{marked as spam|spam}) = 0.9 $$ By Bayes' Theorem, we have $$ P(\text{spam|marked as spam}) = \frac{P(\text{marked as spam|spam}) P(\text{spam})}{P(\text{marked as spam})} $$ $$ 0.9 = \frac{0.9 * 0.1}{P(\text{marked as spam})} $$ Which is incorrect. I am not able to figure out what exactly I did wrong. The official solution goes like this Out of 100 mails, 10 are spam. The filter will label 9 or 10 spam as   spam and 9 of 90 non-spam as spam. So 18 are labelled spam, of which 9   are actually spam. Can somebody show the right way to solve this using conditional probabilities?","I am faced with this question: % of all email you receive is spam. Your spam filter is % reliable, that is, % of the mails it marks as spam are indeed spam and % of spam mails are correctly labelled as spam. If you see a mail marked spam by your filter, what is the probability that it is really spam? This question was posed in a Chennai Mathematical Institute exam. This is how I'm trying it. % of all email you receive is spam % of the mails it marks as spam are indeed spam % of spam mails are correctly labelled as spam By Bayes' Theorem, we have Which is incorrect. I am not able to figure out what exactly I did wrong. The official solution goes like this Out of 100 mails, 10 are spam. The filter will label 9 or 10 spam as   spam and 9 of 90 non-spam as spam. So 18 are labelled spam, of which 9   are actually spam. Can somebody show the right way to solve this using conditional probabilities?","10 90 90 90 10 
P(\text{spam}) = 0.1
 90 
P(\text{spam|marked as spam}) = 0.9
 90 
P(\text{marked as spam|spam}) = 0.9
 
P(\text{spam|marked as spam}) = \frac{P(\text{marked as spam|spam}) P(\text{spam})}{P(\text{marked as spam})}
 
0.9 = \frac{0.9 * 0.1}{P(\text{marked as spam})}
",['probability']
50,Probability that random byte array is a valid UTF-8 string?,Probability that random byte array is a valid UTF-8 string?,,What is the probability that $n$-byte random byte array is a valid UTF-8 string? It doesn't care if it's NFC or NFD.,What is the probability that $n$-byte random byte array is a valid UTF-8 string? It doesn't care if it's NFC or NFD.,,['probability']
51,Probability that a geyser erupts,Probability that a geyser erupts,,"Lets say you have a geyser that has a 2/3 probability of erupting in a   50 minute interval? What is the probability that it will erupt in a 20   minute interval? The way I tried to solve it that a 20 minute interval is 2/5 of a 50 minute interval, so the probability is 2/3 * 2/5 = 4/15, but apparently this is worng. Where did I go wrong and what is the right method to solving it?","Lets say you have a geyser that has a 2/3 probability of erupting in a   50 minute interval? What is the probability that it will erupt in a 20   minute interval? The way I tried to solve it that a 20 minute interval is 2/5 of a 50 minute interval, so the probability is 2/3 * 2/5 = 4/15, but apparently this is worng. Where did I go wrong and what is the right method to solving it?",,['probability']
52,probability of no matching or exactly one matching and generalization,probability of no matching or exactly one matching and generalization,,"I had the following question in a midterm today: There are $10$ pairs of shoes. One randomly selects $8$ shoes. What is the probability that : $\textbf{a)}$ There are no matching pairs of shoes in the selected shoes. $\textbf{b)}$ There is exactly one matching pair of shoes. What I wrote as a solution is : $\textbf{a)}$ We randomly select one shoe, then two select the second one so that we don't violate the requirement we can select any of the 18 out of the 19 shoes left. Then for the third shoes that we will select we can select  any of the 16 out of the 18 shoes left and so on. So we get that : $$ P = \frac{18}{19}\frac{16}{18}\frac{14}{17}\frac{12}{16}\frac{10}{15}\frac{8}{14}\frac{6}{13}.$$ $\textbf{b)}$ The probability to pick exactly one pair of shoes is the probability of getting a pair of shoes and then getting no matching pairs for the remaining $18$ shoes(and to compute this we apply same logic as in part a) $$P=\frac{10}{\dbinom{20}{2}} \cdot \frac{16}{17}\frac{14}{16}\frac{12}{15}\frac{10}{14}\frac{8}{13}.$$ $\textbf{Firstly:}$ I know that my solution is quite ugly but is it right at least? If not where's the mistake? $\textbf{Secondly:}$ Searching for duplicates of this question I found many other question similar but with some different parameters so here I am proposing the general version: $\textbf{General Version:}$ There are $m$ pairs of shoes. One randomly selects $n$ shoes. What is the probability that there are exactly $k$ pairs of shoes in the selected ones? ($k < \lfloor n \rfloor$)","I had the following question in a midterm today: There are $10$ pairs of shoes. One randomly selects $8$ shoes. What is the probability that : $\textbf{a)}$ There are no matching pairs of shoes in the selected shoes. $\textbf{b)}$ There is exactly one matching pair of shoes. What I wrote as a solution is : $\textbf{a)}$ We randomly select one shoe, then two select the second one so that we don't violate the requirement we can select any of the 18 out of the 19 shoes left. Then for the third shoes that we will select we can select  any of the 16 out of the 18 shoes left and so on. So we get that : $$ P = \frac{18}{19}\frac{16}{18}\frac{14}{17}\frac{12}{16}\frac{10}{15}\frac{8}{14}\frac{6}{13}.$$ $\textbf{b)}$ The probability to pick exactly one pair of shoes is the probability of getting a pair of shoes and then getting no matching pairs for the remaining $18$ shoes(and to compute this we apply same logic as in part a) $$P=\frac{10}{\dbinom{20}{2}} \cdot \frac{16}{17}\frac{14}{16}\frac{12}{15}\frac{10}{14}\frac{8}{13}.$$ $\textbf{Firstly:}$ I know that my solution is quite ugly but is it right at least? If not where's the mistake? $\textbf{Secondly:}$ Searching for duplicates of this question I found many other question similar but with some different parameters so here I am proposing the general version: $\textbf{General Version:}$ There are $m$ pairs of shoes. One randomly selects $n$ shoes. What is the probability that there are exactly $k$ pairs of shoes in the selected ones? ($k < \lfloor n \rfloor$)",,"['probability', 'combinatorics']"
53,Hard Integral $\frac{1}{(1+x^2+y^2+z^2)^2}$,Hard Integral,\frac{1}{(1+x^2+y^2+z^2)^2},"Prove that $\displaystyle\int_{-\infty}^{\infty}\displaystyle\int_{-\infty}^{\infty}\displaystyle\int_{-\infty}^{\infty} \frac{1}{(1+x^2+y^2+z^2)^2}\, dx \, dy \, dz = \pi^2$ I tried substitution, trigonometric substitution, and partial fraction decomposition, but I can't solve this problem, I only know that $\frac{1}{(1+x^2+y^2+z^2)^2}$ is a even function :( then $$\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \frac{1}{(1+x^2+y^2+z^2)^2}\, dx \, dy \, dz = $$ $$ 2 \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \int_{0}^{\infty} \frac{1}{(1+x^2+y^2+z^2)^2}\, dx \, dy \, dz $$","Prove that $\displaystyle\int_{-\infty}^{\infty}\displaystyle\int_{-\infty}^{\infty}\displaystyle\int_{-\infty}^{\infty} \frac{1}{(1+x^2+y^2+z^2)^2}\, dx \, dy \, dz = \pi^2$ I tried substitution, trigonometric substitution, and partial fraction decomposition, but I can't solve this problem, I only know that $\frac{1}{(1+x^2+y^2+z^2)^2}$ is a even function :( then $$\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \frac{1}{(1+x^2+y^2+z^2)^2}\, dx \, dy \, dz = $$ $$ 2 \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \int_{0}^{\infty} \frac{1}{(1+x^2+y^2+z^2)^2}\, dx \, dy \, dz $$",,"['probability', 'integration', 'multivariable-calculus', 'definite-integrals', 'improper-integrals']"
54,Monty Hall vs. Card Example,Monty Hall vs. Card Example,,"In class, while illustrating the topic of conditional probability, my professor presented the following card example: You have 3 cards that have been randomly shuffled: card1, card2, and card3. One is an ace and the other two are non-aces. We are interested in the location of the ace. Thus, the sample space is S = {card1, card2, card3}.That is, the ace can either be card1, card2, or card3. We assume that each outcome is equally likely(ie classical probability formulation). Let event A1 = ""card1 is the ace"", thus P(A1) = 1/3. Let event B = ""turn over card3 and it is not an ace"". My professor says that once B occurs it becomes the sample space since it becomes the full set of possibilities given that it actually occurred. He says that A1 = {1} and B = {1,2}, so now the probability of the ace being the first card is $P(A1|B) = \frac{|A1\cap B|}{|B|} = \frac{1}{2}$. The answer to this problem made me uneasy because this problem looks very similar to the Monty Hall problem. Applying the ""Monty Hall problem"" reasoning to this problem would give the probability of the ace being card 1 as 1/3, since the probability of the ace being card 2 (ie ""switching your pick"") would be 2/3. Is the answer to the card example really 1/2? Are these problems the same? If not, what makes them different?","In class, while illustrating the topic of conditional probability, my professor presented the following card example: You have 3 cards that have been randomly shuffled: card1, card2, and card3. One is an ace and the other two are non-aces. We are interested in the location of the ace. Thus, the sample space is S = {card1, card2, card3}.That is, the ace can either be card1, card2, or card3. We assume that each outcome is equally likely(ie classical probability formulation). Let event A1 = ""card1 is the ace"", thus P(A1) = 1/3. Let event B = ""turn over card3 and it is not an ace"". My professor says that once B occurs it becomes the sample space since it becomes the full set of possibilities given that it actually occurred. He says that A1 = {1} and B = {1,2}, so now the probability of the ace being the first card is $P(A1|B) = \frac{|A1\cap B|}{|B|} = \frac{1}{2}$. The answer to this problem made me uneasy because this problem looks very similar to the Monty Hall problem. Applying the ""Monty Hall problem"" reasoning to this problem would give the probability of the ace being card 1 as 1/3, since the probability of the ace being card 2 (ie ""switching your pick"") would be 2/3. Is the answer to the card example really 1/2? Are these problems the same? If not, what makes them different?",,"['probability', 'intuition', 'monty-hall']"
55,"You roll a die until the sum of all your rolls is greater than 13. What number are you most likely to land on, on the last roll?","You roll a die until the sum of all your rolls is greater than 13. What number are you most likely to land on, on the last roll?",,"So I was thinking of doing this recursively: $f(x,i)$ is equal to the probability of rolling greater than $x$ and landing on $i$ on the last roll. $f(0,i) = 1/6$ for $i = \{1,2,..,6\}$. $f(1,i) = 1/6 + 1/6f(0,i)$ for $i = \{2,...,6\}$ and $f(1,1) = 1/6f(0,1)$. Finally, we list out this recursion until we get $f(13,i)$ and see for what value of $i$ is $f$ the largest. Is there a better way to approach this or an easy way to simplify this method?","So I was thinking of doing this recursively: $f(x,i)$ is equal to the probability of rolling greater than $x$ and landing on $i$ on the last roll. $f(0,i) = 1/6$ for $i = \{1,2,..,6\}$. $f(1,i) = 1/6 + 1/6f(0,i)$ for $i = \{2,...,6\}$ and $f(1,1) = 1/6f(0,1)$. Finally, we list out this recursion until we get $f(13,i)$ and see for what value of $i$ is $f$ the largest. Is there a better way to approach this or an easy way to simplify this method?",,"['probability', 'combinatorics', 'dice']"
56,Since when is 9/10 = 92%?,Since when is 9/10 = 92%?,,"This is probably a more basic question than this site is used to, probably because I'm only 13, and as such I'd appreciate if you gave a more basic and simple explanation than the norm for this site. I was reading a BBC News article this morning and I found myself questioning the three diagrams, in the 'Why are two tests better than one?' section. You do a first test and obtain nine heads and one tail... The probability that the coin is fair given this outcome is about 8%, [and the probability] that it is biased, about 92%. You do a second test, and this time you throw eight heads and two tails. Now the probability for a fair coin is about 16%, for a biased coin about 84%. So the naive thought might be that you haven't gained any certainty from this second test. But if you think about it differently, what you've really done is throw the coin 20 times and get 17 heads and three tails."" This means there's a probability of 98.5% that the coin is biased. I have three questions, all roughly along the same lines: Please see below paragraph, these questions are no longer 'active' - Since when is 9/10 heads a 92% probability, and why?  - Why is 8/10 an 84% probability, I always thought that 8/10 = 80%  - And finally, 17/20 is 85%, except when it's 98.5%. Why? After some help from the comments, I now realise that it's not talking about the probability of getting a head (or a tail) but the probability of the coin being biased. Can someone explain (preferably in layman's terms) how the article gets to 92%, 84% and 98.5% respectively? Thanks.","This is probably a more basic question than this site is used to, probably because I'm only 13, and as such I'd appreciate if you gave a more basic and simple explanation than the norm for this site. I was reading a BBC News article this morning and I found myself questioning the three diagrams, in the 'Why are two tests better than one?' section. You do a first test and obtain nine heads and one tail... The probability that the coin is fair given this outcome is about 8%, [and the probability] that it is biased, about 92%. You do a second test, and this time you throw eight heads and two tails. Now the probability for a fair coin is about 16%, for a biased coin about 84%. So the naive thought might be that you haven't gained any certainty from this second test. But if you think about it differently, what you've really done is throw the coin 20 times and get 17 heads and three tails."" This means there's a probability of 98.5% that the coin is biased. I have three questions, all roughly along the same lines: Please see below paragraph, these questions are no longer 'active' - Since when is 9/10 heads a 92% probability, and why?  - Why is 8/10 an 84% probability, I always thought that 8/10 = 80%  - And finally, 17/20 is 85%, except when it's 98.5%. Why? After some help from the comments, I now realise that it's not talking about the probability of getting a head (or a tail) but the probability of the coin being biased. Can someone explain (preferably in layman's terms) how the article gets to 92%, 84% and 98.5% respectively? Thanks.",,['probability']
57,"Rolling a fair die 4 times, what is the probability of getting an increasing sequence of numbers?","Rolling a fair die 4 times, what is the probability of getting an increasing sequence of numbers?",,"Game: I roll a die 4 times. What is the probability that I get a strictly increasing sequence of numbers. My initial thought is as follows: we condition on R1 (the first roll) being a 1, 2, or 3 (which happens with probability 1/2). Now, we look at R2 - there is a 1/6 probability that R1 = R2 and 5/6 probability that R2 is different from R1. In the case that R2 is not equal to R1, by symmetry we get that R2>R1 with a probability of 1/2. Therefore, the probability that R2>R1 is 5/12. Continuing in the same manner... There is a probability of 2/6 that R3 will be equal to R1 or R2. Therefore, there is a 4/6 probability that it is different. Using the same argument as above, there is a 1/6 probability that R3 > R2 > R1. Continuing similarly, P(R4 > R3 > R2 > R1) = (1/2) * (5/12) * (4/36) * (3/144) However, I have a feeling that I am doing something terribly wrong. Any help would be appreciated.","Game: I roll a die 4 times. What is the probability that I get a strictly increasing sequence of numbers. My initial thought is as follows: we condition on R1 (the first roll) being a 1, 2, or 3 (which happens with probability 1/2). Now, we look at R2 - there is a 1/6 probability that R1 = R2 and 5/6 probability that R2 is different from R1. In the case that R2 is not equal to R1, by symmetry we get that R2>R1 with a probability of 1/2. Therefore, the probability that R2>R1 is 5/12. Continuing in the same manner... There is a probability of 2/6 that R3 will be equal to R1 or R2. Therefore, there is a 4/6 probability that it is different. Using the same argument as above, there is a 1/6 probability that R3 > R2 > R1. Continuing similarly, P(R4 > R3 > R2 > R1) = (1/2) * (5/12) * (4/36) * (3/144) However, I have a feeling that I am doing something terribly wrong. Any help would be appreciated.",,"['probability', 'dice']"
58,Definition of atomic $\sigma$-field.,Definition of atomic -field.,\sigma,"Reading an article in probability theory I faced with phrase atomic $\sigma$-field . I tried to search for the definition, but google doesn't give any meaningful result. As a result I'm looking for the definition here.","Reading an article in probability theory I faced with phrase atomic $\sigma$-field . I tried to search for the definition, but google doesn't give any meaningful result. As a result I'm looking for the definition here.",,"['probability', 'measure-theory', 'probability-theory', 'random-variables']"
59,Lambda value of Poisson distribution,Lambda value of Poisson distribution,,"I'm a bit confused about the lambda value of a Poisson distribution. I know it means the average rate of success for a given interval. I'm confused about what this value exactly means through. For example, If I have 2.4/100,000 people contracting a disease over a period of two years, what is the lambda value if I'm trying to figure out the probability of at least 5 cases of the disease out of 100,000 in one year? I'm not sure if it would be either 2.4/100,000 or 1.2/100,000 since the original average is over a period of two years. I'm also pondering if lambda could be 2.4 or 1.2 since the question already states a sample of only 100,000. Thanks","I'm a bit confused about the lambda value of a Poisson distribution. I know it means the average rate of success for a given interval. I'm confused about what this value exactly means through. For example, If I have 2.4/100,000 people contracting a disease over a period of two years, what is the lambda value if I'm trying to figure out the probability of at least 5 cases of the disease out of 100,000 in one year? I'm not sure if it would be either 2.4/100,000 or 1.2/100,000 since the original average is over a period of two years. I'm also pondering if lambda could be 2.4 or 1.2 since the question already states a sample of only 100,000. Thanks",,"['probability', 'probability-distributions']"
60,Probability of having six side first,Probability of having six side first,,I have an exercise as follows: A and B alternately throw a dice (which has six sides numbered from 1 to 6). A starts firstly. What is the probability that A will be the first person who has side 6? Thanks for any help!,I have an exercise as follows: A and B alternately throw a dice (which has six sides numbered from 1 to 6). A starts firstly. What is the probability that A will be the first person who has side 6? Thanks for any help!,,['probability']
61,Geometric Distribution $P(X\ge Y)$,Geometric Distribution,P(X\ge Y),"I need to show that if $X$ and $Y$ are idd and geometrically distributed that the $P(X\ge Y)$ is $1\over{2-p}$. the joint pmf is $f_{xy}(xy)=p^2(1-p)^{x+y}$, and I think the only way to do this is to use a double sum: $\sum_{y=0}^{n}\sum_{x=y}^m p^2(1-p)^{x+y}$, which leads to me getting quite stuck. Any suggestions?","I need to show that if $X$ and $Y$ are idd and geometrically distributed that the $P(X\ge Y)$ is $1\over{2-p}$. the joint pmf is $f_{xy}(xy)=p^2(1-p)^{x+y}$, and I think the only way to do this is to use a double sum: $\sum_{y=0}^{n}\sum_{x=y}^m p^2(1-p)^{x+y}$, which leads to me getting quite stuck. Any suggestions?",,"['probability', 'statistics', 'probability-distributions', 'summation']"
62,What are the odds of cracking a cellphone pattern-lock?,What are the odds of cracking a cellphone pattern-lock?,,"In case someone don't know what a pattern lock is, they are like this: I am curious on the probability of randomly cracking one of these 'passwords', given that the length of the grid is 3x3 and that we know the pattern length (e.g number of lines). Also, you may or not noticed but the pattern can go in both + and x directions. Note: we can have multiple lines that go to a single dot, but we can never draw a line above another line (I think some apps allow this, but for this question lets assume we will not).","In case someone don't know what a pattern lock is, they are like this: I am curious on the probability of randomly cracking one of these 'passwords', given that the length of the grid is 3x3 and that we know the pattern length (e.g number of lines). Also, you may or not noticed but the pattern can go in both + and x directions. Note: we can have multiple lines that go to a single dot, but we can never draw a line above another line (I think some apps allow this, but for this question lets assume we will not).",,['probability']
63,What is the covariance of mixture Bernoulli distribution?,What is the covariance of mixture Bernoulli distribution?,,"For mixture of multivariate Bernoulli distribution we have that, $$p(x|\mu,\pi) =\sum_{k=1}^{K}\pi_kp(x|\mu_k)$$ where $$p(x|\mu_k) = \prod_{i=1}^{D}\mu_{ki}^{x_i}(1-\mu_{ki})^{1-x_i}$$ I read it from the book that $$E[x] = \sum_{i=1}^{K}\pi_k\mu_k$$ $$\operatorname{cov}[x] = \sum_{k=1}^{K}\pi_k(\Sigma_k+\mu_k\mu_k^T) - E[x]E[x]^T$$ The mean is trivial to prove, however I can't find proof for the covariance and I don't know how to prove it. Can anyone help?","For mixture of multivariate Bernoulli distribution we have that, where I read it from the book that The mean is trivial to prove, however I can't find proof for the covariance and I don't know how to prove it. Can anyone help?","p(x|\mu,\pi) =\sum_{k=1}^{K}\pi_kp(x|\mu_k) p(x|\mu_k) = \prod_{i=1}^{D}\mu_{ki}^{x_i}(1-\mu_{ki})^{1-x_i} E[x] = \sum_{i=1}^{K}\pi_k\mu_k \operatorname{cov}[x] = \sum_{k=1}^{K}\pi_k(\Sigma_k+\mu_k\mu_k^T) - E[x]E[x]^T","['linear-algebra', 'probability', 'probability-distributions', 'covariance']"
64,How can gender and class classification be dependent? [closed],How can gender and class classification be dependent? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. Want to improve this question? Update the question so it's on-topic for Mathematics Stack Exchange. Closed 11 years ago . Improve this question I got this question in my hw practice set In a class, there are 4 freshman boys, 6 freshman girls, and 6 sophomore boys. How many sophomore girls must be present if sex and class are to be independent when a student is selected at random? I solved the number (i believe is 9) that make class and gender independent . But it got me thinking: under what scenario, can class and gender be dependent ? (i mean, class and gender are totally unrelated things right? therefore, they should be independent correct?) I tried cook up some numbers to show that they're dependent(see table below). Mathematically, I've shown, through the following table, that gender and classification are indeed dependent. But how can gender and class classification be dependent? Male Female Total Freshman     18    20    38 Sophomore    12    16    28           Total        30    36    66","Closed. This question is off-topic . It is not currently accepting answers. Want to improve this question? Update the question so it's on-topic for Mathematics Stack Exchange. Closed 11 years ago . Improve this question I got this question in my hw practice set In a class, there are 4 freshman boys, 6 freshman girls, and 6 sophomore boys. How many sophomore girls must be present if sex and class are to be independent when a student is selected at random? I solved the number (i believe is 9) that make class and gender independent . But it got me thinking: under what scenario, can class and gender be dependent ? (i mean, class and gender are totally unrelated things right? therefore, they should be independent correct?) I tried cook up some numbers to show that they're dependent(see table below). Mathematically, I've shown, through the following table, that gender and classification are indeed dependent. But how can gender and class classification be dependent? Male Female Total Freshman     18    20    38 Sophomore    12    16    28           Total        30    36    66",,['probability']
65,Draw probability tree for drawing black & white cards (how to use $P(A|B)$),Draw probability tree for drawing black & white cards (how to use ),P(A|B),"I have a homework question that reads: You have a deck of $16$ cards, with: $4$ cards that are white on both sides; $7$ cards that are white on one side, black on the other; $5$ cards that are black on both sides. The cards are shuffled and randomly flipped. You draw one card from the    deck and look only at one side of it. (a) Draw a tree diagram with the probabilities of all possible card types.    Hint: Your tree should have two levels, one for the color of the top          of the card, and one for the color of the bottom card. (b) What is the probability that the top of your card is black? (c) If the top of your card is black, what is the probability that    the bottom is white? (d) If the top of your card is white, what is the probability that     the bottom is white? Now, here has been my approach thus far: first, I figured out how many total white sides and black sides there are: White sides $= 4 \times 2 + 7 = 15$ Black sides $= 5 \times 2 + 7 = 17$ Total sides $= 32$ Probability for top side being white $= 15/32 = 0.46875$ Probability for top side being black $= 1 - 15/32 = 0.53125$ Now, my problem is there is this $P(A | B)$ formula thing that I'm supposed to use to get the next level ones, but I don't know how to do that, so I skipped the probabilities for the second level in the tree for a bit. I went on to (c), where I said the probability was $7/12$, or about $0.58333$, because if your top is black, either you have a full black card (of which there are $5$), or you have a mixed card (of which there are $7$). You're looking for the probability of a mixed card, so you have a total of $7$ winning picks out of a total $12$ picks ($7 + 5$). I did a similar thing for (d) Then I went back and filled in the tree by doing the probability of the top side (say the top is black, which is $0.53125$) times the probability of the bottom for that branch (say bottom is white (top has already been black), so that is $0.58333$). So I went $0.53125 \times 0.58333 = 0.309894$, and did $0.53125 - 0.309894$ to get the probability of a full black card. My question is, how can I fill out the probability tree by using this $P(A|B)$ thing? I know that the formula for $P(A|B)$ is $P(A \cap B) / P(B)$. The thing is, I was getting an obviously wrong probability. Let $B$ be the event that the top of the card drawn is black. Let $A$ be the event that the bottom of the card drawn is white. $P(A|B) = P(A \cap B) / P(B) = (7/16) / (17/32) = 14/17 = 0.823529\ldots$? what? I did $7/16$ because that's the percentage of getting a mixed card ($A \cap B$), right? I don't understand UPDATE: Actually, $7/16$ should be $7/12$ right? Because we don't need to have the $4$ full whites in there. Now, $7/12 \times 17/32 = 0.309894$, which is the correct answer I believe. But isn't it $P(A \cap B) / P(B)$?","I have a homework question that reads: You have a deck of $16$ cards, with: $4$ cards that are white on both sides; $7$ cards that are white on one side, black on the other; $5$ cards that are black on both sides. The cards are shuffled and randomly flipped. You draw one card from the    deck and look only at one side of it. (a) Draw a tree diagram with the probabilities of all possible card types.    Hint: Your tree should have two levels, one for the color of the top          of the card, and one for the color of the bottom card. (b) What is the probability that the top of your card is black? (c) If the top of your card is black, what is the probability that    the bottom is white? (d) If the top of your card is white, what is the probability that     the bottom is white? Now, here has been my approach thus far: first, I figured out how many total white sides and black sides there are: White sides $= 4 \times 2 + 7 = 15$ Black sides $= 5 \times 2 + 7 = 17$ Total sides $= 32$ Probability for top side being white $= 15/32 = 0.46875$ Probability for top side being black $= 1 - 15/32 = 0.53125$ Now, my problem is there is this $P(A | B)$ formula thing that I'm supposed to use to get the next level ones, but I don't know how to do that, so I skipped the probabilities for the second level in the tree for a bit. I went on to (c), where I said the probability was $7/12$, or about $0.58333$, because if your top is black, either you have a full black card (of which there are $5$), or you have a mixed card (of which there are $7$). You're looking for the probability of a mixed card, so you have a total of $7$ winning picks out of a total $12$ picks ($7 + 5$). I did a similar thing for (d) Then I went back and filled in the tree by doing the probability of the top side (say the top is black, which is $0.53125$) times the probability of the bottom for that branch (say bottom is white (top has already been black), so that is $0.58333$). So I went $0.53125 \times 0.58333 = 0.309894$, and did $0.53125 - 0.309894$ to get the probability of a full black card. My question is, how can I fill out the probability tree by using this $P(A|B)$ thing? I know that the formula for $P(A|B)$ is $P(A \cap B) / P(B)$. The thing is, I was getting an obviously wrong probability. Let $B$ be the event that the top of the card drawn is black. Let $A$ be the event that the bottom of the card drawn is white. $P(A|B) = P(A \cap B) / P(B) = (7/16) / (17/32) = 14/17 = 0.823529\ldots$? what? I did $7/16$ because that's the percentage of getting a mixed card ($A \cap B$), right? I don't understand UPDATE: Actually, $7/16$ should be $7/12$ right? Because we don't need to have the $4$ full whites in there. Now, $7/12 \times 17/32 = 0.309894$, which is the correct answer I believe. But isn't it $P(A \cap B) / P(B)$?",,['probability']
66,"Joint probabilities, conditional probabilities with the chain rule.","Joint probabilities, conditional probabilities with the chain rule.",,"I'm reading through a book, and it walks through a problem. We need to compute $p(a | e, f)$. It says that by applying the chain rule we can see: $$p(a|e,f) = \frac{p(e,a|f)}{p(e|f)}$$ Looking at the chain rule, I do not understand how that was arrived at. I imagine there is a simple explanation (since no further working was shown in the book), is anyone able to provide one?","I'm reading through a book, and it walks through a problem. We need to compute $p(a | e, f)$. It says that by applying the chain rule we can see: $$p(a|e,f) = \frac{p(e,a|f)}{p(e|f)}$$ Looking at the chain rule, I do not understand how that was arrived at. I imagine there is a simple explanation (since no further working was shown in the book), is anyone able to provide one?",,['probability']
67,what is the best way to win: every 1000 submission will win,what is the best way to win: every 1000 submission will win,,"I have a question about probability. The game is like this: Every $1000$th submission will win, but the players don't know how many   submissions were made before. Is it better for a player to throw all of his $100$ credits in one time or is it better to throw one then wait then submit the next... and so on?","I have a question about probability. The game is like this: Every $1000$th submission will win, but the players don't know how many   submissions were made before. Is it better for a player to throw all of his $100$ credits in one time or is it better to throw one then wait then submit the next... and so on?",,['probability']
68,Extension of $3\sigma$ rule,Extension of  rule,3\sigma,"For the normally distributed r.v. $\xi$ there is a rule of $3\sigma$ which says that  $$ \mathsf P\{\xi\in (\mu-3\sigma,\mu+3\sigma)\}\geq 0.99. $$ Clearly, this rule not necessary holds for other distributions. I wonder if there are lower bounds for  $$ p(\lambda) = P\{\xi\in (\mu-\lambda\sigma,\mu+\lambda\sigma)\} $$ regardless of the distribution of real-valued random variable $\xi$. If we are focused only on  absolute continuous distributions, a naive approach is to consider the variational problem $$ \int\limits_{\int\limits xf(x)\,dx - \lambda\sqrt{\int\limits x^2f(x)\,dx-(\int\limits xf(x)\,dx)^2}}^{\int\limits xf(x)\,dx + \lambda\sqrt{\int\limits x^2f(x)\,dx-(\int\limits xf(x)\,dx)^2}} f(x)\,dx \to\inf\limits_f $$ which may be too naive. The other problem is that dsitributions can be not necessary absolutely continuous. So my question is if there are known lower bounds for $p(\lambda)$?","For the normally distributed r.v. $\xi$ there is a rule of $3\sigma$ which says that  $$ \mathsf P\{\xi\in (\mu-3\sigma,\mu+3\sigma)\}\geq 0.99. $$ Clearly, this rule not necessary holds for other distributions. I wonder if there are lower bounds for  $$ p(\lambda) = P\{\xi\in (\mu-\lambda\sigma,\mu+\lambda\sigma)\} $$ regardless of the distribution of real-valued random variable $\xi$. If we are focused only on  absolute continuous distributions, a naive approach is to consider the variational problem $$ \int\limits_{\int\limits xf(x)\,dx - \lambda\sqrt{\int\limits x^2f(x)\,dx-(\int\limits xf(x)\,dx)^2}}^{\int\limits xf(x)\,dx + \lambda\sqrt{\int\limits x^2f(x)\,dx-(\int\limits xf(x)\,dx)^2}} f(x)\,dx \to\inf\limits_f $$ which may be too naive. The other problem is that dsitributions can be not necessary absolutely continuous. So my question is if there are known lower bounds for $p(\lambda)$?",,"['probability', 'statistics']"
69,Rank of second ace after first ace is drawn,Rank of second ace after first ace is drawn,,"I have a probability problem with cards and the expected value of a card rank. I have a deck of 52 cards. I draw cards without replacement. While drawing cards from the deck, a first ace is drawn at rank $k$ (that is the $k^{th}$ card drawn is an ace, all previous were not). We want to find the expected number of additional draws until we get an ace. My idea is to follow this route: If I call $X$ the random variable of the rank of the second ace, $N=52$ the total number of cards, $p = N -k$ the remaining number of cards after the first ace is drawn, the idea is to plug the expectation value of $X$ $$E[X] = \sum_{i=1}^{p-3}i  P(X=i)$$ But the formula doesn't seem to simplify.  What would be your take at this?","I have a probability problem with cards and the expected value of a card rank. I have a deck of 52 cards. I draw cards without replacement. While drawing cards from the deck, a first ace is drawn at rank $k$ (that is the $k^{th}$ card drawn is an ace, all previous were not). We want to find the expected number of additional draws until we get an ace. My idea is to follow this route: If I call $X$ the random variable of the rank of the second ace, $N=52$ the total number of cards, $p = N -k$ the remaining number of cards after the first ace is drawn, the idea is to plug the expectation value of $X$ $$E[X] = \sum_{i=1}^{p-3}i  P(X=i)$$ But the formula doesn't seem to simplify.  What would be your take at this?",,['probability']
70,Confidence level in answer being correct when multiple people give that same answer,Confidence level in answer being correct when multiple people give that same answer,,"We are very rusty on our math, probability, statistics, permutations and combinations and need some help figuring out how to calculate some relative ""confidence level"" based on the number of people who give the same answer to a question. This is somewhat related to Wisdom of Crowds where if we ask a group of people the same question and many choose the same answer it is likely that answer is correct. Answer to a question is either ""a"" or ""b"". If we ask 1 person we would have 50% confidence that is the right answer. If we asked two people and they both said the same answer we would be 75% confident it is right I think?? If we keep extending this: Possible Answers = 4, we ask 2 people and they give the same answer ... it should be a higher confidence level but what? (87.5%?) The more possible answers for a question and the more people who give the same answer should lead to a higher confidence level - but what is the formula or algorithm to calculate it?","We are very rusty on our math, probability, statistics, permutations and combinations and need some help figuring out how to calculate some relative ""confidence level"" based on the number of people who give the same answer to a question. This is somewhat related to Wisdom of Crowds where if we ask a group of people the same question and many choose the same answer it is likely that answer is correct. Answer to a question is either ""a"" or ""b"". If we ask 1 person we would have 50% confidence that is the right answer. If we asked two people and they both said the same answer we would be 75% confident it is right I think?? If we keep extending this: Possible Answers = 4, we ask 2 people and they give the same answer ... it should be a higher confidence level but what? (87.5%?) The more possible answers for a question and the more people who give the same answer should lead to a higher confidence level - but what is the formula or algorithm to calculate it?",,"['probability', 'statistics']"
71,Distribution of the longest queue's length in parallel queues,Distribution of the longest queue's length in parallel queues,,"Considering $n$ people line up at $q$ queues. Let's say all people choose which queue to line up randomly, so each people has probability $1/q$ to choose a particular queue. Then the length of any particular queue is a Binomial distribution $B(n, 1/q)$ . Now, what is the the distribution of the longest queue of all $q$ queues? Specifically, if $q=2$ , what would be the result? Recall when $n$ is large, Binomial distribution $B(n, 1/q)$ becomes like a normal distribution, then if $q=n$ , and $n$ is large, is there a similar approximation of the longest queue? Side note: I ask this question as I ran into a computer algorithm question, for those familiar with Hash table , hashing function, mapping a key to a hash value, is just as a people line up to a queue. In the algorithm question, for a particular $j\in \{0,1,\cdots, n-1\}$ , $L_j$ denotes the length of the $j$ -th queue, and $W$ denotes the worst queue, i.e. the longest queue. The algorithm questions asks to prove $P(L_j > \frac{4\ln n}{\ln \ln n}) \le \frac{1}{n^2}$ and $P(W > \frac{4\ln n}{\ln \ln n}) \le \frac{1}{n}$ . While $P(L_j > \frac{4\ln n}{\ln \ln n}) \le \frac{1}{n^2}$ can be proved with the Cernoff bound, from it I couldn't find a clean way to $P(W > \frac{4\ln n}{\ln \ln n}) \le \frac{1}{n}$ . At first glance this is easy: $$P\left(W > \frac{4\ln n}{\ln \ln n}\right) = 1 - P\left(W \le \frac{4\ln n}{\ln \ln n}\right)  = 1 - \left[P\left(L_j \le \frac{4\ln n}{\ln \ln n}\right) \right]^n = 1 - \left[1-P\left(L_j > \frac{4\ln n}{\ln \ln n}\right) \right]^n < 1 - \left(1-\frac{1}{n^2}  \right)^n < \frac1n $$ However, there is a gap -- different queues are actually interfering with each other, for example, with a big $n$ , $P(L_j =0)=\left(1-\frac{1}{n}\right)^n\approx \frac1e$ , but $P(W=0)=0$ , we don't have $P(W=0)=P^n(L_j=0)$ . Hence I'm raising the question as the content.","Considering people line up at queues. Let's say all people choose which queue to line up randomly, so each people has probability to choose a particular queue. Then the length of any particular queue is a Binomial distribution . Now, what is the the distribution of the longest queue of all queues? Specifically, if , what would be the result? Recall when is large, Binomial distribution becomes like a normal distribution, then if , and is large, is there a similar approximation of the longest queue? Side note: I ask this question as I ran into a computer algorithm question, for those familiar with Hash table , hashing function, mapping a key to a hash value, is just as a people line up to a queue. In the algorithm question, for a particular , denotes the length of the -th queue, and denotes the worst queue, i.e. the longest queue. The algorithm questions asks to prove and . While can be proved with the Cernoff bound, from it I couldn't find a clean way to . At first glance this is easy: However, there is a gap -- different queues are actually interfering with each other, for example, with a big , , but , we don't have . Hence I'm raising the question as the content.","n q 1/q B(n, 1/q) q q=2 n B(n, 1/q) q=n n j\in \{0,1,\cdots, n-1\} L_j j W P(L_j > \frac{4\ln n}{\ln \ln n}) \le \frac{1}{n^2} P(W > \frac{4\ln n}{\ln \ln n}) \le \frac{1}{n} P(L_j > \frac{4\ln n}{\ln \ln n}) \le \frac{1}{n^2} P(W > \frac{4\ln n}{\ln \ln n}) \le \frac{1}{n} P\left(W > \frac{4\ln n}{\ln \ln n}\right) = 1 - P\left(W \le \frac{4\ln n}{\ln \ln n}\right)  = 1 - \left[P\left(L_j \le \frac{4\ln n}{\ln \ln n}\right) \right]^n = 1 - \left[1-P\left(L_j > \frac{4\ln n}{\ln \ln n}\right) \right]^n < 1 - \left(1-\frac{1}{n^2}  \right)^n < \frac1n  n P(L_j =0)=\left(1-\frac{1}{n}\right)^n\approx \frac1e P(W=0)=0 P(W=0)=P^n(L_j=0)","['probability', 'statistics', 'queueing-theory']"
72,Proof for Particular Fair Shuffle Algorithm,Proof for Particular Fair Shuffle Algorithm,,"I ran multiple simulations of the following function, and it seems to be fair shuffling, given that all permutations were roughly equal, but I don't understand why it works. It's just inserting at random positions within the current shuffled deck isn't it? I know about Fisher-Yates shuffling, for reference. def shuffle_deck(deck):     shuffled_deck = []     for card in deck:         r = random.randint(0, len(shuffled_deck))         shuffled_deck.insert(r, card)     return shuffled_deck I was expecting that it would be uneven probability permutation.","I ran multiple simulations of the following function, and it seems to be fair shuffling, given that all permutations were roughly equal, but I don't understand why it works. It's just inserting at random positions within the current shuffled deck isn't it? I know about Fisher-Yates shuffling, for reference. def shuffle_deck(deck):     shuffled_deck = []     for card in deck:         r = random.randint(0, len(shuffled_deck))         shuffled_deck.insert(r, card)     return shuffled_deck I was expecting that it would be uneven probability permutation.",,"['probability', 'combinatorics', 'algorithms', 'python']"
73,"Lower bound $\sum_{\{ s_{1},..,s_{d}\}\subset [n]} d!\frac{1}{s_{1}s_{2}\dots s_{d}} $",Lower bound,"\sum_{\{ s_{1},..,s_{d}\}\subset [n]} d!\frac{1}{s_{1}s_{2}\dots s_{d}} ","I want to lower bound the following expression of choosing $d$ numbers from $[n]:=\{ 1,2,\dots,n \}$ . $$ \sum_{\{ s_{1},..,s_{d}\}\subset [n]} d!\frac{1}{s_{1}s_{2}\dots s_{d}} $$ Here $s_{1},\dots,s_{d}$ are all distincts. I know I can upper bound this by $\left( 1+\frac{1}{2}+\dots+\frac{1}{n} \right)^{d} = H_{n}^{d} < (\ln n+1)^{d}$ . Is there a way to lower bound this quantity, perhaps to $\Theta((\ln n)^d)$ ? Or is a bound of $\Theta((\ln n)^d)$ even possible? I don't really know much about combinatorics, so not sure what tools I can utilize.","I want to lower bound the following expression of choosing numbers from . Here are all distincts. I know I can upper bound this by . Is there a way to lower bound this quantity, perhaps to ? Or is a bound of even possible? I don't really know much about combinatorics, so not sure what tools I can utilize.","d [n]:=\{ 1,2,\dots,n \} 
\sum_{\{ s_{1},..,s_{d}\}\subset [n]} d!\frac{1}{s_{1}s_{2}\dots s_{d}}
 s_{1},\dots,s_{d} \left( 1+\frac{1}{2}+\dots+\frac{1}{n} \right)^{d} = H_{n}^{d} < (\ln n+1)^{d} \Theta((\ln n)^d) \Theta((\ln n)^d)","['probability', 'combinatorics', 'graph-theory']"
74,Information-theoretic Inequality,Information-theoretic Inequality,,"If we have two discrete RVs, X, and Y. How can we show: $$\sum_{x,y} p(x|y)p(y|x) \geq 1.$$ The question goes further with finding a sufficient and necessary condition for equality. My attempt: For equality, assuming that X, Y are independent will enable us to sum over each variable PMF and get exactly one. However, I am stuck with showing how the inequality holds in general, and I appreciate any hints and tips.","If we have two discrete RVs, X, and Y. How can we show: The question goes further with finding a sufficient and necessary condition for equality. My attempt: For equality, assuming that X, Y are independent will enable us to sum over each variable PMF and get exactly one. However, I am stuck with showing how the inequality holds in general, and I appreciate any hints and tips.","\sum_{x,y} p(x|y)p(y|x) \geq 1.","['probability', 'inequality', 'probability-distributions', 'conditional-probability', 'information-theory']"
75,Card game Puzzle,Card game Puzzle,,"You have a special deck with 11 cards, with 10 red cards and 1 joker. starting with $1 we play a game: shuffle the deck we can choose to draw the top card. if it's red, we double our money, but if it's the joker, our money is multiplied by 1/2048. the card we drew is then discarded. repeat step 2 until you want to walk away: at any time, you can choose to stop drawing cards and take home however much you have. a) how would you play this game? what's the minimum amount of money you are guaranteed to take home? b) let's say you employ a strategy where you always draw n cards then stop. What value of n should you pick? how much money would you on average then have? c) what strategy maximizes the expected value, and what is this value? We now modify the game so that before drawing a card, you can bet any number between 0 and how much money you currently have. then, the bet is either doubled or multiplied by 1/2048 depending on whether you draw a red card or the joker. d) does your strategy change? how should you now play, and what are the associated expectations? e) what is the most amount of money you are guaranteed to win? My approach: Part a) If you ever draw a Joker, you continue to draw all the other cards. Hence minimum amount of money guaranteed is \$ 1/2 or 50 cents. b) We will stop drawing after n cards when $ E[n+1] <= E[n]:$ $$ \frac {10-n} {11-n} * 2^{n+1} <= \frac {10-n + 1} {11-n +1} * 2^{n} $$ This gives us $ n^2 -22n + 119 = 0 $ as our equation giving $ n <= 9.59 $ Hence we should stop after drawing 9 Red cards. However, if we get a Joker we obviously draw all cards then. c) Assuming same strategy as above $$ E[x] = \frac {2}{11} * 2^9 + \frac {9} {11} * \frac {1} {2} = 93.5 $$ Not sure if this strategy is correct/optimal that maximizes EV. Also no idea on the next part where we can bet any amount of the money we currently have","You have a special deck with 11 cards, with 10 red cards and 1 joker. starting with $1 we play a game: shuffle the deck we can choose to draw the top card. if it's red, we double our money, but if it's the joker, our money is multiplied by 1/2048. the card we drew is then discarded. repeat step 2 until you want to walk away: at any time, you can choose to stop drawing cards and take home however much you have. a) how would you play this game? what's the minimum amount of money you are guaranteed to take home? b) let's say you employ a strategy where you always draw n cards then stop. What value of n should you pick? how much money would you on average then have? c) what strategy maximizes the expected value, and what is this value? We now modify the game so that before drawing a card, you can bet any number between 0 and how much money you currently have. then, the bet is either doubled or multiplied by 1/2048 depending on whether you draw a red card or the joker. d) does your strategy change? how should you now play, and what are the associated expectations? e) what is the most amount of money you are guaranteed to win? My approach: Part a) If you ever draw a Joker, you continue to draw all the other cards. Hence minimum amount of money guaranteed is \$ 1/2 or 50 cents. b) We will stop drawing after n cards when This gives us as our equation giving Hence we should stop after drawing 9 Red cards. However, if we get a Joker we obviously draw all cards then. c) Assuming same strategy as above Not sure if this strategy is correct/optimal that maximizes EV. Also no idea on the next part where we can bet any amount of the money we currently have", E[n+1] <= E[n]:  \frac {10-n} {11-n} * 2^{n+1} <= \frac {10-n + 1} {11-n +1} * 2^{n}   n^2 -22n + 119 = 0   n <= 9.59   E[x] = \frac {2}{11} * 2^9 + \frac {9} {11} * \frac {1} {2} = 93.5 ,"['probability', 'probability-theory', 'expected-value', 'puzzle']"
76,Prove $\sum_{k=i}^{n} {k-1 \choose i-1} p^i (1-p)^{k-i} = \sum_{k=i}^{n} {n \choose k} p^k (1-p)^{n-k}$,Prove,\sum_{k=i}^{n} {k-1 \choose i-1} p^i (1-p)^{k-i} = \sum_{k=i}^{n} {n \choose k} p^k (1-p)^{n-k},"Prove that the following two summations are equal for any positive integers $i\leq n$ , and any real number $p$ between $0$ and $1$ : $$ \sum_{k=i}^{n} {k-1 \choose i-1} p^i (1-p)^{k-i} = \sum_{k=i}^{n} {n \choose k} p^k (1-p)^{n-k} $$ I know the equation is originated from Binomial distribution and its insight. That is, $$ P\{X(i,p) > n\} = P\{B(n,p) < i\}, $$ which implies $$ 1 - \sum_{k=i}^{n} {k-1 \choose i-1} p^i (1-p)^{k-i} = \sum_{k=0}^{i-1} {n \choose k} p^k (1-p)^{n-k}. $$ But I do not know how to prove it mathematically using some transformations in combinatorics.","Prove that the following two summations are equal for any positive integers , and any real number between and : I know the equation is originated from Binomial distribution and its insight. That is, which implies But I do not know how to prove it mathematically using some transformations in combinatorics.","i\leq n p 0 1 
\sum_{k=i}^{n} {k-1 \choose i-1} p^i (1-p)^{k-i} = \sum_{k=i}^{n} {n \choose k} p^k (1-p)^{n-k}
 
P\{X(i,p) > n\} = P\{B(n,p) < i\},
 
1 - \sum_{k=i}^{n} {k-1 \choose i-1} p^i (1-p)^{k-i} = \sum_{k=0}^{i-1} {n \choose k} p^k (1-p)^{n-k}.
","['probability', 'combinatorics', 'binomial-coefficients', 'binomial-distribution', 'geometric-distribution']"
77,Is the notion of random variable always necessary?,Is the notion of random variable always necessary?,,"I'm quite confused by the notion of random variable in the proper measure-theoretic framework. Let's first state the notation and definitions: Let $(\Omega, \Sigma, \operatorname{P})$ be a probability space. Then, a real-valued random variable is a measurable function $X \colon \Omega \to \mathbb{R}$ and its probability distribution is the pushforward measure $\operatorname{P}_{X} := \operatorname{P} \circ X^{-1}$ . If $\operatorname{P}_{X}$ is absolutely continuous with respect to the Lebesgue measure $\lambda$ we also know that there is a probability density function $f\colon \mathbb{R} \to \mathbb{R}$ such that $\operatorname{P}_{X}(B) = \int_B f \, \mathrm{d} \lambda$ for $B \in \mathcal{B}(\mathbb{R})$ (by the Radon–Nikodym theorem). Now let's see a simple example that is often used to illustrate the notion of random variable: Random variable that represents the sum of two dice. In this case $\Omega = \{1, 2, 3, 4, 5, 6\}^2$ , $\Sigma = \mathcal{P}(\Omega)$ , and $\operatorname{P}(A) = \frac{\#A}{36}$ for $A \in \Sigma$ , $X \colon (\omega_1, \omega_2) \mapsto \omega_1 + \omega_2$ and e.g. $\operatorname{P}_X(3) = \operatorname{P}(\{(1, 2), (2, 1)\}) = \frac{1}{18}$ . This is all crystal clear but the two examples below break my little mind: Normal random variable. What is $(\Omega, \Sigma, \operatorname{P})$ now? Others have given the answer that the underlying probability space is just abstract and unspecified. But why then, is it necessary to use the notion of random variable in the first place here? Wouldn't it be easier just to say that we are working with a probability space with $\Omega = \mathbb{R}$ , $\Sigma = \mathcal{B}(\mathbb{R})$ , and $\operatorname{P}(A) = \int_A \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2}\, \mathrm{d}\lambda(x)$ for $A \in \Sigma$ ? Random variable that represents the outcome of the toss of a fair coin. As explained here , the underlying probability space is again some abstract space of all conceivable futures. But why do we even need that? Why not directly use $\Omega = \{0, 1\}$ , $\Sigma = \mathcal{P}(\Omega)$ , and $\operatorname{P}(A) = \frac{\#A}{2}$ for $A \in \Sigma$ ? If it is indeed beneficial to introduce random variables in these two cases, what are the benefits?","I'm quite confused by the notion of random variable in the proper measure-theoretic framework. Let's first state the notation and definitions: Let be a probability space. Then, a real-valued random variable is a measurable function and its probability distribution is the pushforward measure . If is absolutely continuous with respect to the Lebesgue measure we also know that there is a probability density function such that for (by the Radon–Nikodym theorem). Now let's see a simple example that is often used to illustrate the notion of random variable: Random variable that represents the sum of two dice. In this case , , and for , and e.g. . This is all crystal clear but the two examples below break my little mind: Normal random variable. What is now? Others have given the answer that the underlying probability space is just abstract and unspecified. But why then, is it necessary to use the notion of random variable in the first place here? Wouldn't it be easier just to say that we are working with a probability space with , , and for ? Random variable that represents the outcome of the toss of a fair coin. As explained here , the underlying probability space is again some abstract space of all conceivable futures. But why do we even need that? Why not directly use , , and for ? If it is indeed beneficial to introduce random variables in these two cases, what are the benefits?","(\Omega, \Sigma, \operatorname{P}) X \colon \Omega \to \mathbb{R} \operatorname{P}_{X} := \operatorname{P} \circ X^{-1} \operatorname{P}_{X} \lambda f\colon \mathbb{R} \to \mathbb{R} \operatorname{P}_{X}(B) = \int_B f \, \mathrm{d} \lambda B \in \mathcal{B}(\mathbb{R}) \Omega = \{1, 2, 3, 4, 5, 6\}^2 \Sigma = \mathcal{P}(\Omega) \operatorname{P}(A) = \frac{\#A}{36} A \in \Sigma X \colon (\omega_1, \omega_2) \mapsto \omega_1 + \omega_2 \operatorname{P}_X(3) = \operatorname{P}(\{(1, 2), (2, 1)\}) = \frac{1}{18} (\Omega, \Sigma, \operatorname{P}) \Omega = \mathbb{R} \Sigma = \mathcal{B}(\mathbb{R}) \operatorname{P}(A) = \int_A \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2}\, \mathrm{d}\lambda(x) A \in \Sigma \Omega = \{0, 1\} \Sigma = \mathcal{P}(\Omega) \operatorname{P}(A) = \frac{\#A}{2} A \in \Sigma","['probability', 'probability-theory', 'measure-theory']"
78,Does the Central Limit Theorem Work for a Single Sample?,Does the Central Limit Theorem Work for a Single Sample?,,"This is a question that I am struggling to understand. Suppose there is a university with 100,000 students (population) I take a sample of 100 students and measure the height of each student Then, I take the average height of these 100 students. Finally, I calculate the variance of this sample average and calculate a 95% Confidence Interval for this sample average This leads me to my question: - Even though I interviewed 100 students - effectively, I only have a single sample As I understand, the Central Limit Theorem states that the mean of a (large enough) sequence of samples will be Normally Distributed - Thus, is it correct to use the Central Limit Theorem in this question to construct a Confidence Interval for the average height of the entire population? In this problem, I understand that I can easily calculate the Standard Deviation of the sample itself - that is, how much does the height of any given student in the sample deviate  on average from the average height of all students in the sample. But in this problem, is it really correct to use the Central Limit Theorem to calculate the Confidence Interval of the average height - when all I have is a single sample? I think the Central Limit Theorem would be more applicable if many people each took a single sample of size 100 - and then calculated the average height of students in the sample they took. Then, you would have a sequence of sample means - and in this case, the Central Limit Theorem could be used to calculate the Confidence Interval of the sample mean. Is my reasoning correct? Thanks! Note: I have attempted to illustrate both situations - in Case 1, I think the Central Limit Theorem does not apply. In Case 2, I think the Central Limit Theorem does apply:","This is a question that I am struggling to understand. Suppose there is a university with 100,000 students (population) I take a sample of 100 students and measure the height of each student Then, I take the average height of these 100 students. Finally, I calculate the variance of this sample average and calculate a 95% Confidence Interval for this sample average This leads me to my question: - Even though I interviewed 100 students - effectively, I only have a single sample As I understand, the Central Limit Theorem states that the mean of a (large enough) sequence of samples will be Normally Distributed - Thus, is it correct to use the Central Limit Theorem in this question to construct a Confidence Interval for the average height of the entire population? In this problem, I understand that I can easily calculate the Standard Deviation of the sample itself - that is, how much does the height of any given student in the sample deviate  on average from the average height of all students in the sample. But in this problem, is it really correct to use the Central Limit Theorem to calculate the Confidence Interval of the average height - when all I have is a single sample? I think the Central Limit Theorem would be more applicable if many people each took a single sample of size 100 - and then calculated the average height of students in the sample they took. Then, you would have a sequence of sample means - and in this case, the Central Limit Theorem could be used to calculate the Confidence Interval of the sample mean. Is my reasoning correct? Thanks! Note: I have attempted to illustrate both situations - in Case 1, I think the Central Limit Theorem does not apply. In Case 2, I think the Central Limit Theorem does apply:",,['probability']
79,Throwing two dice probability problem,Throwing two dice probability problem,,"We got the following problem from last week. They say my solution is not correct, but I don't know where I go wrong. Problem: We throw with two dice until the sum of the results equals $10$ two times. What is the probability that we throw exactly eight times a sum smaller than $10$ until this event occurs? (For example if we throw the first time $3$ , $4$ , second time $5$ , $5$ and third time $6$ , $4$ then we finish the experiment. The sums until the second $10$ were $7$ , $10$ and $10$ .) My solution: The probability of throwing exactly $10$ in one throw is $$p_{\left(=10\right)}\overset{\cdot}{=}\left(\frac{1}{6}\right)\left(\frac{1}{6}\right)+\left(\frac{1}{6}\right)\left(\frac{1}{6}\right)+\left(\frac{1}{6}\right)\left(\frac{1}{6}\right)=3\left(\frac{1}{6}\right)^{2}.$$ The probability of throwing more than $10$ in one throw is $$p_{\left(>10\right)}\overset{\cdot}{=}\left(\frac{1}{6}\right)\left(\frac{1}{6}\right)+\left(\frac{1}{6}\right)\left(\frac{1}{6}\right)+\left(\frac{1}{6}\right)\left(\frac{1}{6}\right)=3\left(\frac{1}{6}\right)^{2}.$$ So the probability of throwing less than $10$ in one round is $$p_{\left(<10\right)}\overset{\cdot}{=}1-3\left(\frac{1}{6}\right)^{2}-3\left(\frac{1}{6}\right)^{2}=1-6\left(\frac{1}{6}\right)^{2}.$$ The probability of throwing $\xi=1$ ""exactly $10$ "" and throwing $\eta=8$ ""less than $10$ "" from $N-1=n$ throwings is $$\mathbf{P}\left(\xi=1,\eta=8\right)=\frac{n!}{1!8!\left(n-1-8\right)!}p_{\left(=10\right)}^{1}p_{\left(<10\right)}^{8}\left(1-p_{\left(=10\right)}-p_{\left(<10\right)}\right)^{n-1-8}.$$ The distribution of the number of the last throwing can be calculated as follows: $$\mathbf{P}\left(N=m\right)=\frac{\left(m-1\right)!}{1!8!\left(\left(m-1\right)-1-8\right)!}p_{\left(=10\right)}^{1}p_{\left(<10\right)}^{8}\left(1-p_{\left(=10\right)}-p_{\left(<10\right)}\right)^{\left(m-1\right)-1-8}\cdot p_{\left(=10\right)}$$ So the asked probability using the total law of probabilities is: $$\sum_{n=9}^{\infty}\mathbf{P}\left(\left(\xi=1,\eta=8\right)\cap\left(N-1=n\right)\right)=\sum_{n=9}^{\infty}\mathbf{P}\left(\xi=1,\eta=8\mid N-1=n\right)\cdot\mathbf{P}\left(N-1=n\right)=$$ $$=\sum_{n=9}^{\infty}\frac{n!}{1!8!\left(n-1-8\right)!}p_{\left(=10\right)}^{1}p_{\left(<10\right)}^{8}\left(1-p_{\left(=10\right)}-p_{\left(<10\right)}\right)^{n-1-8}\cdot\frac{n!}{1!8!\left(n-1-8\right)!}p_{\left(=10\right)}^{1}p_{\left(<10\right)}^{8}\left(1-p_{\left(=10\right)}-p_{\left(<10\right)}\right)^{n-1-8}\cdot p_{\left(=10\right)}$$ , which is $\simeq0.0047101892$ according to WolframAlpha. Can you see my mistake?","We got the following problem from last week. They say my solution is not correct, but I don't know where I go wrong. Problem: We throw with two dice until the sum of the results equals two times. What is the probability that we throw exactly eight times a sum smaller than until this event occurs? (For example if we throw the first time , , second time , and third time , then we finish the experiment. The sums until the second were , and .) My solution: The probability of throwing exactly in one throw is The probability of throwing more than in one throw is So the probability of throwing less than in one round is The probability of throwing ""exactly "" and throwing ""less than "" from throwings is The distribution of the number of the last throwing can be calculated as follows: So the asked probability using the total law of probabilities is: , which is according to WolframAlpha. Can you see my mistake?","10 10 3 4 5 5 6 4 10 7 10 10 10 p_{\left(=10\right)}\overset{\cdot}{=}\left(\frac{1}{6}\right)\left(\frac{1}{6}\right)+\left(\frac{1}{6}\right)\left(\frac{1}{6}\right)+\left(\frac{1}{6}\right)\left(\frac{1}{6}\right)=3\left(\frac{1}{6}\right)^{2}. 10 p_{\left(>10\right)}\overset{\cdot}{=}\left(\frac{1}{6}\right)\left(\frac{1}{6}\right)+\left(\frac{1}{6}\right)\left(\frac{1}{6}\right)+\left(\frac{1}{6}\right)\left(\frac{1}{6}\right)=3\left(\frac{1}{6}\right)^{2}. 10 p_{\left(<10\right)}\overset{\cdot}{=}1-3\left(\frac{1}{6}\right)^{2}-3\left(\frac{1}{6}\right)^{2}=1-6\left(\frac{1}{6}\right)^{2}. \xi=1 10 \eta=8 10 N-1=n \mathbf{P}\left(\xi=1,\eta=8\right)=\frac{n!}{1!8!\left(n-1-8\right)!}p_{\left(=10\right)}^{1}p_{\left(<10\right)}^{8}\left(1-p_{\left(=10\right)}-p_{\left(<10\right)}\right)^{n-1-8}. \mathbf{P}\left(N=m\right)=\frac{\left(m-1\right)!}{1!8!\left(\left(m-1\right)-1-8\right)!}p_{\left(=10\right)}^{1}p_{\left(<10\right)}^{8}\left(1-p_{\left(=10\right)}-p_{\left(<10\right)}\right)^{\left(m-1\right)-1-8}\cdot p_{\left(=10\right)} \sum_{n=9}^{\infty}\mathbf{P}\left(\left(\xi=1,\eta=8\right)\cap\left(N-1=n\right)\right)=\sum_{n=9}^{\infty}\mathbf{P}\left(\xi=1,\eta=8\mid N-1=n\right)\cdot\mathbf{P}\left(N-1=n\right)= =\sum_{n=9}^{\infty}\frac{n!}{1!8!\left(n-1-8\right)!}p_{\left(=10\right)}^{1}p_{\left(<10\right)}^{8}\left(1-p_{\left(=10\right)}-p_{\left(<10\right)}\right)^{n-1-8}\cdot\frac{n!}{1!8!\left(n-1-8\right)!}p_{\left(=10\right)}^{1}p_{\left(<10\right)}^{8}\left(1-p_{\left(=10\right)}-p_{\left(<10\right)}\right)^{n-1-8}\cdot p_{\left(=10\right)} \simeq0.0047101892","['probability', 'probability-theory', 'probability-distributions']"
80,Creating an Example to Disprove the Central Limit Theorem?,Creating an Example to Disprove the Central Limit Theorem?,,"In class, we are always told that for the Central Limit Theorem to be applicable, observations have to be IID (Independent and Identically Distributed). However, we are not always told why this IID condition is so important for the Central Limit Theorem. This being said, I am trying to create an example where the IID condition is not met and thus show myself why it is required. Part 1: The first thing that comes to mind is an Autoregressive Process as by definition AR Processes are said not to be IID. For instance, suppose we have an AR(1) Process: $$y_t = \phi y_{t-1} + \epsilon_t$$ Based on this AR(1) process, I know the following: $E(\epsilon_t) = 0$ $E(\epsilon_t^2) = \sigma^2$ $E(\epsilon_t\epsilon_s) = 0$ $Var(y_t) = \frac{\sigma^2}{1-\phi^2}$ $E(y_t) = \phi E(y_{t-1}) = 0$ Part 2: As for the Central Limit Theorem, I know that in when $n$ is large, any Random Variable behaves as: $$\frac{\bar{x} - E(X)}{\sqrt{\frac{Var(X)}{n}}} \approx N(0,1)$$ Part 3: Putting this all together, I would now show that the above AR(1) Process DOES NOT converge to a Standard Normal Distribution: $$\frac{Y_t - E(Y_t)}{\sqrt{\frac{Var(Y_t)}{n}}} = \frac{Y_t - E(Y_t)}{\sqrt{\frac{\sigma^2}{1-\phi^2}\frac{1}{n}}} \not\approx N(0,1) $$ However, I am not sure if I am doing this correctly for the AR(1) Process and have in fact shown that in the absence of IID, the Central Limit Theorem is not necessarily valid. In general, can someone please show me an example where the IID condition is not met and as a result the Central Limit Theorem does not apply? Thanks! Note: I am aware that there are versions of the Central Limit Theorem that do not require the IID Condition (e.g. https://en.wikipedia.org/wiki/Central_limit_theorem#Lyapunov_CLT , https://en.wikipedia.org/wiki/Lindeberg%27s_condition ) - however, I am specifically interested in constructing an example that shows why the Classic Central Limit Theorem requires the IID condition.","In class, we are always told that for the Central Limit Theorem to be applicable, observations have to be IID (Independent and Identically Distributed). However, we are not always told why this IID condition is so important for the Central Limit Theorem. This being said, I am trying to create an example where the IID condition is not met and thus show myself why it is required. Part 1: The first thing that comes to mind is an Autoregressive Process as by definition AR Processes are said not to be IID. For instance, suppose we have an AR(1) Process: Based on this AR(1) process, I know the following: Part 2: As for the Central Limit Theorem, I know that in when is large, any Random Variable behaves as: Part 3: Putting this all together, I would now show that the above AR(1) Process DOES NOT converge to a Standard Normal Distribution: However, I am not sure if I am doing this correctly for the AR(1) Process and have in fact shown that in the absence of IID, the Central Limit Theorem is not necessarily valid. In general, can someone please show me an example where the IID condition is not met and as a result the Central Limit Theorem does not apply? Thanks! Note: I am aware that there are versions of the Central Limit Theorem that do not require the IID Condition (e.g. https://en.wikipedia.org/wiki/Central_limit_theorem#Lyapunov_CLT , https://en.wikipedia.org/wiki/Lindeberg%27s_condition ) - however, I am specifically interested in constructing an example that shows why the Classic Central Limit Theorem requires the IID condition.","y_t = \phi y_{t-1} + \epsilon_t E(\epsilon_t) = 0 E(\epsilon_t^2) = \sigma^2 E(\epsilon_t\epsilon_s) = 0 Var(y_t) = \frac{\sigma^2}{1-\phi^2} E(y_t) = \phi E(y_{t-1}) = 0 n \frac{\bar{x} - E(X)}{\sqrt{\frac{Var(X)}{n}}} \approx N(0,1) \frac{Y_t - E(Y_t)}{\sqrt{\frac{Var(Y_t)}{n}}} = \frac{Y_t - E(Y_t)}{\sqrt{\frac{\sigma^2}{1-\phi^2}\frac{1}{n}}} \not\approx N(0,1) ","['probability', 'central-limit-theorem']"
81,Expected “peripheral” points in the unit square,Expected “peripheral” points in the unit square,,"Background Grimmett & Stirzaker’s Probability and Random Processes (4th ed. 2020), exercise 4.2.5, reads: Peripheral points. Let $P_i = (X_i, Y_i)$ , $1\le  i \le n$ , be independent, uniformly distributed points in the unit square $[0,1]^2$ . A point $P_i$ is called peripheral if, for all $r=1, 2, \dots, n$ , either $X_r\le X_i$ or $Y_r\le Y_i$ , or both. Show that the mean number of peripheral points is $n\left(\frac{3}{4}\right)^{n-1}$ . The proof they give is easy: Define an indicator function $I_i$ that is 1 if the point $P_i$ is peripheral. Then $\mathbb{E}(I_i) = \mathbb{P}(I_i=1) = \left(\frac{3}{4}\right)^{n-1}$ , and setting the number of peripheral points $X:=\sum I_i$ , the result follows from the linearity of expectation. Here is a plot of $\mathbb{E}(X)= n\left(\frac{3}{4}\right)^{n-1}$ : Question Notably, for $n\ge 9$ , we have $\mathbb{E}(X) = n\left(\frac{3}{4}\right)^{n-1} < 1$ . This is a contradiction to the following argument: Assume $n$ points placed in the unit square (never mind how they are distributed). Since the set of points is finite, there exist at least one point with a maximal X coordinate, and at least one point with a maximal Y coordinate. (These points might be the same.) Therefore the number of peripheral points is $X \ge 1$ always, and thus $\mathbb{E}(X)\ge 1$ . Put differently, if for $n\ge 9$ we have $\mathbb{E}(X) < 1$ then there should exist a configuration of points such that no point is peripheral. I can’t see how that is true? Any help clearing up my confusion would be greatly appreciated, thanks!","Background Grimmett & Stirzaker’s Probability and Random Processes (4th ed. 2020), exercise 4.2.5, reads: Peripheral points. Let , , be independent, uniformly distributed points in the unit square . A point is called peripheral if, for all , either or , or both. Show that the mean number of peripheral points is . The proof they give is easy: Define an indicator function that is 1 if the point is peripheral. Then , and setting the number of peripheral points , the result follows from the linearity of expectation. Here is a plot of : Question Notably, for , we have . This is a contradiction to the following argument: Assume points placed in the unit square (never mind how they are distributed). Since the set of points is finite, there exist at least one point with a maximal X coordinate, and at least one point with a maximal Y coordinate. (These points might be the same.) Therefore the number of peripheral points is always, and thus . Put differently, if for we have then there should exist a configuration of points such that no point is peripheral. I can’t see how that is true? Any help clearing up my confusion would be greatly appreciated, thanks!","P_i = (X_i, Y_i) 1\le  i \le n [0,1]^2 P_i r=1, 2, \dots, n X_r\le X_i Y_r\le Y_i n\left(\frac{3}{4}\right)^{n-1} I_i P_i \mathbb{E}(I_i) = \mathbb{P}(I_i=1) = \left(\frac{3}{4}\right)^{n-1} X:=\sum I_i \mathbb{E}(X)= n\left(\frac{3}{4}\right)^{n-1} n\ge 9 \mathbb{E}(X) = n\left(\frac{3}{4}\right)^{n-1} < 1 n X \ge 1 \mathbb{E}(X)\ge 1 n\ge 9 \mathbb{E}(X) < 1","['probability', 'expected-value', 'uniform-distribution']"
82,How to solve COVID test validity using the Bayes theorem?,How to solve COVID test validity using the Bayes theorem?,,"The following information is given: The COVID tests are $70\%$ sensitive, i.e., ${\Bbb P} ( \text{Positive} \mid \text{COVID} ) = 0.7$ and $98\%$ specific, i.e. Pr(negative|no Covid)=0.98. We need to solve the probability of having covid when the test result is covid given that a)P(Covid)=0.5 b) P(Covid)=0.05. Can someone help me with making the joint probability distribution table and explain how they got it? The work I've done so far is: P(Covid|Positive) = P(Covid) x P(Positive|Covid) / P(Positive) P(Covid|Positive) = 0.5 x (0.7) / (0.7 + 0.02) I know the marginal probability is wrong but I don't think I understood the question fully. So having a table would help me understand it better. Thanks in advance.","The following information is given: The COVID tests are sensitive, i.e., and specific, i.e. Pr(negative|no Covid)=0.98. We need to solve the probability of having covid when the test result is covid given that a)P(Covid)=0.5 b) P(Covid)=0.05. Can someone help me with making the joint probability distribution table and explain how they got it? The work I've done so far is: P(Covid|Positive) = P(Covid) x P(Positive|Covid) / P(Positive) P(Covid|Positive) = 0.5 x (0.7) / (0.7 + 0.02) I know the marginal probability is wrong but I don't think I understood the question fully. So having a table would help me understand it better. Thanks in advance.",70\% {\Bbb P} ( \text{Positive} \mid \text{COVID} ) = 0.7 98\%,"['probability', 'bayesian', 'bayes-theorem']"
83,Find the variance for the number of runs,Find the variance for the number of runs,,"A biased coin is tossed $n$ times and heads shows up with probability $p$ on each toss.  Let us call  a sequence of throws which result in the same outcomes a run , so that for example, the sequence HHTHTTH contains five runs. If $R$ is a r.v. representing the number of runs then $\mathbb{E}(R) = 1+(n-1)2pq$ . I want to work out the variance $var(R)$ . To do this I would like to use that $var(R) = var(R-1) = \mathbb{E}(R-1)^2 - (\mathbb{E}(R-1))^2$ . Let $I_j$ be the indicator function of the event that the outcome of the $(j+1)$ th toss is different from the outcome of the $j$ th toss. $I_j$ and $I_k$ are independent if $|j-k| > 1$ , so that \begin{equation*} \begin{aligned} \mathbb{E}(R-1)^2 ={} & \mathbb{E}\left\{\left(\sum_{j=1}^{n-1}I_j\right)^2\right\} \\ = {} &\mathbb{E} \left(\sum_{j=1}^{n-1} I_{j}^{2}+2 \sum_{j=1}^{n-2} I_{j} I_{j+1}+2 \sum_{j=1}^{n-3} \sum_{k=j+2}^{n-1} I_{j} I_{k}\right). \end{aligned} \end{equation*} Now $\mathbb{E}(\sum_{j=1}^{n-1} I_{j}^{2}) = (n-1)2pq$ and $\mathbb{E}(2 \sum_{j=1}^{n-2} I_{j} I_{j+1}) = (n-2)2pq$ . We also have that $\mathbb{E}(2 \sum_{j=1}^{n-3} \sum_{k=j+2}^{n-1} I_{j} I_{k}) = (n-3)(n-4)(2pq)^2$ I believe. But now I have lost confidence and I am not sure how to get the final result for the variance. Is my approach correct and what should it be in the end?","A biased coin is tossed times and heads shows up with probability on each toss.  Let us call  a sequence of throws which result in the same outcomes a run , so that for example, the sequence HHTHTTH contains five runs. If is a r.v. representing the number of runs then . I want to work out the variance . To do this I would like to use that . Let be the indicator function of the event that the outcome of the th toss is different from the outcome of the th toss. and are independent if , so that Now and . We also have that I believe. But now I have lost confidence and I am not sure how to get the final result for the variance. Is my approach correct and what should it be in the end?","n p R \mathbb{E}(R) = 1+(n-1)2pq var(R) var(R) = var(R-1) = \mathbb{E}(R-1)^2 - (\mathbb{E}(R-1))^2 I_j (j+1) j I_j I_k |j-k| > 1 \begin{equation*}
\begin{aligned}
\mathbb{E}(R-1)^2 ={} & \mathbb{E}\left\{\left(\sum_{j=1}^{n-1}I_j\right)^2\right\} \\
= {} &\mathbb{E} \left(\sum_{j=1}^{n-1} I_{j}^{2}+2 \sum_{j=1}^{n-2} I_{j} I_{j+1}+2 \sum_{j=1}^{n-3} \sum_{k=j+2}^{n-1} I_{j} I_{k}\right).
\end{aligned}
\end{equation*} \mathbb{E}(\sum_{j=1}^{n-1} I_{j}^{2}) = (n-1)2pq \mathbb{E}(2 \sum_{j=1}^{n-2} I_{j} I_{j+1}) = (n-2)2pq \mathbb{E}(2 \sum_{j=1}^{n-3} \sum_{k=j+2}^{n-1} I_{j} I_{k}) = (n-3)(n-4)(2pq)^2",['probability']
84,Sum of i.i.d. random variables for which Chebyshev inequalities are tight,Sum of i.i.d. random variables for which Chebyshev inequalities are tight,,"Chebyshev's inequalities : Let $X$ be a random variable with finite expected value $\mu$ and finite non-zero variance $\sigma^{2}$ . Then for any real number $\delta > 0$ , $$ \Pr[|X - \mu| \geq \delta\sigma] \leq \frac {1}{\delta^{2}}$$ There is a tight example in wiki, $X_c$ is a random variable with $\sigma = 1/c$ : $$ \left\{ \begin{aligned} &\Pr[X_c = -1] = \frac{1}{2c^{2}}  \\ &\Pr[X_c = 0] = 1 - \frac{1}{c^{2}}  \\ &\Pr[X_c = 1] = \frac{1}{2c^{2}}  \\ \end{aligned} \right. $$ If $\delta = c$ , then we have $$ \Pr[|X - \mu| \geq \delta\sigma] = \Pr[|X| \geq 1] = \frac {1}{\delta^{2}}$$ But for $\delta > c$ , it is not tight. Is there another example that is tight for infinite large $\delta$ ? In addition, suppose $X_{1}, X_{2}, \ldots, X_{n}$ are i.i.d. random variables with finite expected value $\mu$ and finite non-zero variance $\sigma^{2}$ . According to Chebyshev's inequalities: $$\Pr\left[\left|\sum_{i}^{n}X_{i} - n\mu\right| \geq \delta n\sigma\right] \leq \frac{1}{n\delta^{2}}$$ Is there also an (asymptotic) tight example for $\{ X_{i} \}_{i}$ ?","Chebyshev's inequalities : Let be a random variable with finite expected value and finite non-zero variance . Then for any real number , There is a tight example in wiki, is a random variable with : If , then we have But for , it is not tight. Is there another example that is tight for infinite large ? In addition, suppose are i.i.d. random variables with finite expected value and finite non-zero variance . According to Chebyshev's inequalities: Is there also an (asymptotic) tight example for ?","X \mu \sigma^{2} \delta > 0  \Pr[|X - \mu| \geq \delta\sigma] \leq \frac {1}{\delta^{2}} X_c \sigma = 1/c 
\left\{
\begin{aligned}
&\Pr[X_c = -1] = \frac{1}{2c^{2}}  \\
&\Pr[X_c = 0] = 1 - \frac{1}{c^{2}}  \\
&\Pr[X_c = 1] = \frac{1}{2c^{2}}  \\
\end{aligned}
\right.
 \delta = c  \Pr[|X - \mu| \geq \delta\sigma] = \Pr[|X| \geq 1] = \frac {1}{\delta^{2}} \delta > c \delta X_{1}, X_{2}, \ldots, X_{n} \mu \sigma^{2} \Pr\left[\left|\sum_{i}^{n}X_{i} - n\mu\right| \geq \delta n\sigma\right] \leq \frac{1}{n\delta^{2}} \{ X_{i} \}_{i}","['probability', 'probability-distributions', 'random-variables', 'concentration-of-measure']"
85,You and your friend are each dealt two cards: hers face up and yours face down. Which of the following scenarios are you more likely to have a pair?,You and your friend are each dealt two cards: hers face up and yours face down. Which of the following scenarios are you more likely to have a pair?,,"I am self-teaching probability, could someone please help me determine if I've solved this problem correctly, as I don't have the answers to these exercises. The following question is taken from Dennis Sun's Probability course at Cal Poly SLO: ""You and your friend Amy are each dealt two cards: hers face up and yours face down. In which of the following scenarios are you more likely to have a pair: 1) When she has a pair of Queens, 2) Or when she has a Queen and a 5?"" https://dlsun.github.io/probability/conditional.html I modeled this the same way as if I were drawing 4 cards. Let A be the statement: I have a pair. B: Amy has Queens. C: Amy has Q5 I calculated the prior probability that Amy has Queens as $$P(B) = {{4 \choose 2}\over{52 \choose 2}} = 0.00452$$ And the prior that she has Q5 as: $$P(C) = {{4 \choose 1}{4 \choose 1}\over{52 \choose 2}} = 0.0121$$ Then the probability that Amy has Queens and I have a pair is $$P(AB) = {{4 \choose 4} + {4 \choose 2}{12 \choose 1}{4 \choose 2}\over{52 \choose 4}} = 0.00160$$ where the 1st term in the numerator is the ways that she can have Queens and I have the other 2 queens, and the 2nd term is the ways that she can have Queens and I have any other pair. Then the conditional probability would be: $$ P(A|B) = {{P(AB)}\over{P(B)}} = .353$$ And the probability that Amy has Q5 and I have a pair is $$P(AB) = {{4 \choose 1}{4 \choose 3}\cdot2 + {4 \choose 1}{4 \choose 1}{11 \choose 1}{4 \choose 2}\over{52 \choose 4}} = 0.00402$$ where the 1st term in the numerator is the probability that I have 5's or Q's. And the 2nd term is I have any other pair. And the conditional probability would be: $$ P(A|C) = {{P(AC)}\over{P(C)}} = .333$$ Did I make a mistake? It seems strange that the probability of me making a pair would go down if she has Q5.","I am self-teaching probability, could someone please help me determine if I've solved this problem correctly, as I don't have the answers to these exercises. The following question is taken from Dennis Sun's Probability course at Cal Poly SLO: ""You and your friend Amy are each dealt two cards: hers face up and yours face down. In which of the following scenarios are you more likely to have a pair: 1) When she has a pair of Queens, 2) Or when she has a Queen and a 5?"" https://dlsun.github.io/probability/conditional.html I modeled this the same way as if I were drawing 4 cards. Let A be the statement: I have a pair. B: Amy has Queens. C: Amy has Q5 I calculated the prior probability that Amy has Queens as And the prior that she has Q5 as: Then the probability that Amy has Queens and I have a pair is where the 1st term in the numerator is the ways that she can have Queens and I have the other 2 queens, and the 2nd term is the ways that she can have Queens and I have any other pair. Then the conditional probability would be: And the probability that Amy has Q5 and I have a pair is where the 1st term in the numerator is the probability that I have 5's or Q's. And the 2nd term is I have any other pair. And the conditional probability would be: Did I make a mistake? It seems strange that the probability of me making a pair would go down if she has Q5.",P(B) = {{4 \choose 2}\over{52 \choose 2}} = 0.00452 P(C) = {{4 \choose 1}{4 \choose 1}\over{52 \choose 2}} = 0.0121 P(AB) = {{4 \choose 4} + {4 \choose 2}{12 \choose 1}{4 \choose 2}\over{52 \choose 4}} = 0.00160  P(A|B) = {{P(AB)}\over{P(B)}} = .353 P(AB) = {{4 \choose 1}{4 \choose 3}\cdot2 + {4 \choose 1}{4 \choose 1}{11 \choose 1}{4 \choose 2}\over{52 \choose 4}} = 0.00402  P(A|C) = {{P(AC)}\over{P(C)}} = .333,"['probability', 'card-games']"
86,Does there exist an unbiased estimator for the absolute value of the mean?,Does there exist an unbiased estimator for the absolute value of the mean?,,"Given random variable $Z$ , with samples $\{z_1,\dots,z_n\}$ is there an unbiased estimator for $|\mathbb{E}[Z]|$ ? If $Z$ is positive (or negative) with probability 1 this is straightforward, as then the absolute value can be essentially ignored. This is a followup to my previous question here . Based on the answers provided and after having thought about it for a while, I am intuitively convinced that no such estimator can exist (for general random variables). However, I am lost on how to prove it.","Given random variable , with samples is there an unbiased estimator for ? If is positive (or negative) with probability 1 this is straightforward, as then the absolute value can be essentially ignored. This is a followup to my previous question here . Based on the answers provided and after having thought about it for a while, I am intuitively convinced that no such estimator can exist (for general random variables). However, I am lost on how to prove it.","Z \{z_1,\dots,z_n\} |\mathbb{E}[Z]| Z","['probability', 'statistics', 'random-variables']"
87,"why do we need a sequence of random variable, isn't one function sufficient?","why do we need a sequence of random variable, isn't one function sufficient?",,"In sampling, we have so many situations involving a sequence of random variables, what I am confusing is why do we need a sequence of random variables to describe the process? It feels like each function is only used once. Suppose $$X_i:\Omega\to\mathbb{R}\quad,i\in\mathbb{N}$$ $X_1,X_2,X_3...$ basically just the $\mathbb{R}$ -valued image, each image has its corresponding function. why don't we only use a single random variable to describe those images, this also seems to be sufficient to describe the process, if not, what is the problem?","In sampling, we have so many situations involving a sequence of random variables, what I am confusing is why do we need a sequence of random variables to describe the process? It feels like each function is only used once. Suppose basically just the -valued image, each image has its corresponding function. why don't we only use a single random variable to describe those images, this also seems to be sufficient to describe the process, if not, what is the problem?","X_i:\Omega\to\mathbb{R}\quad,i\in\mathbb{N} X_1,X_2,X_3... \mathbb{R}","['probability', 'probability-theory', 'random-variables', 'sampling']"
88,"Colored ball problem with $2$ colors, but each ball is always replaced with $1$ color","Colored ball problem with  colors, but each ball is always replaced with  color",2 1,"I'm trying to find a solution to this problem but I'm having a hard time thinking through how to solve it: You have one bag with $N$ balls, each being red or white. You draw some number of balls, each time replacing the ball with a red ball. IE both red and white balls are always replaced with red balls regardless of the color drawn. How do you solve for an expected number of red/white balls? Example: $100$ balls, $70$ red and $30$ white. You draw $15$ balls. How many red balls do you expect to draw? It would be pretty straightforward if the balls were replace with their own color, but I'm stumped here. Ideally I'd love to have something I could punch into a cell formula in Google Sheets, but if that's not possible then I'd be happy to have any kind of explanation.","I'm trying to find a solution to this problem but I'm having a hard time thinking through how to solve it: You have one bag with balls, each being red or white. You draw some number of balls, each time replacing the ball with a red ball. IE both red and white balls are always replaced with red balls regardless of the color drawn. How do you solve for an expected number of red/white balls? Example: balls, red and white. You draw balls. How many red balls do you expect to draw? It would be pretty straightforward if the balls were replace with their own color, but I'm stumped here. Ideally I'd love to have something I could punch into a cell formula in Google Sheets, but if that's not possible then I'd be happy to have any kind of explanation.",N 100 70 30 15,"['probability', 'combinatorics', 'statistics', 'balls-in-bins']"
89,"What is the probability of throwing all the even numbers with a dice, before you throw any odd number? [closed]","What is the probability of throwing all the even numbers with a dice, before you throw any odd number? [closed]",,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I came across this question via a friend, and I am still thinking about it. So we need to throw all even numbers before we throw any odd number. The first throw can be even or odd, that is simple. However, after that, we can throw the same even number, another even number or an odd number. This infinite loop of possibly throwing the same even number an infinite number of times, gets me confused in finding the answer. Could someone help me with finding the answer, using possibly recursion techniques? Kind regards, Clayton44","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I came across this question via a friend, and I am still thinking about it. So we need to throw all even numbers before we throw any odd number. The first throw can be even or odd, that is simple. However, after that, we can throw the same even number, another even number or an odd number. This infinite loop of possibly throwing the same even number an infinite number of times, gets me confused in finding the answer. Could someone help me with finding the answer, using possibly recursion techniques? Kind regards, Clayton44",,"['probability', 'dice']"
90,Proof Reference For Doob's First Stopping Time Theorem,Proof Reference For Doob's First Stopping Time Theorem,,"I am reading through a proof of Doob's First (Bounded) Stopping Time Theorem, and I am not really following. Can somebody either provide a good reference, or be able to prove it? For a martingale $X$ , and for stopping times $S, T$ , with $S \le T \le c$ , where $c > 0$ is a uniform bound, we have that $\mathbb{E}[X_T | \mathcal{F}_S] = X_S$ .","I am reading through a proof of Doob's First (Bounded) Stopping Time Theorem, and I am not really following. Can somebody either provide a good reference, or be able to prove it? For a martingale , and for stopping times , with , where is a uniform bound, we have that .","X S, T S \le T \le c c > 0 \mathbb{E}[X_T | \mathcal{F}_S] = X_S","['probability', 'martingales']"
91,Why are we using combination in probability question when the objects are identical?,Why are we using combination in probability question when the objects are identical?,,"There are $3$ blue , $4$ red ,and $5$ green identical balls in an urn. We draw $3$ balls without replacement. What is the probability that all of these $3$ balls have different colors? $\mathbf{\text{Please read until the end }}$ This is a basic probabilty question. I solved it such that $$3! \times \frac{3}{12} \times \frac{4}{11} \times \frac{5}{10}=\frac{3}{11}$$ However, there is also another approach such that $$\frac{\binom{3}{1}\binom{4}{1}\binom{5}{1}}{\binom{12}{3}}=\frac{3}{11}$$ $\mathbf{\text{My Question}:}$ We know that when we use combination , it is valid for selecting $r$ objects among $n$ $\color{red}{\text{distinct}}$ objects. However , as you see , the question says that the balls are identical among each types. Then , why did we use combination ? For example , i think that using generating functions for finding denominator is more sensible than using combination ,i.e , $\binom{12}{3}$ . When i ask it to someone , they said that ""oh okey , but it works"". Hence , i want a good explanation from you guys. Do not say me "" we use combination because it gives us all possible arrangemets,i.e $3!$ "" ,  because i think that it cannot be the reason to use combination. I think also that we betray the definition of combination by using it to select identical objects. Then what is the reason behind seeing these identical objects like ""distinct"" ? What is the reason behind using combination to select identical objects ? Thanks in advance !!","There are blue , red ,and green identical balls in an urn. We draw balls without replacement. What is the probability that all of these balls have different colors? This is a basic probabilty question. I solved it such that However, there is also another approach such that We know that when we use combination , it is valid for selecting objects among objects. However , as you see , the question says that the balls are identical among each types. Then , why did we use combination ? For example , i think that using generating functions for finding denominator is more sensible than using combination ,i.e , . When i ask it to someone , they said that ""oh okey , but it works"". Hence , i want a good explanation from you guys. Do not say me "" we use combination because it gives us all possible arrangemets,i.e "" ,  because i think that it cannot be the reason to use combination. I think also that we betray the definition of combination by using it to select identical objects. Then what is the reason behind seeing these identical objects like ""distinct"" ? What is the reason behind using combination to select identical objects ? Thanks in advance !!",3 4 5 3 3 \mathbf{\text{Please read until the end }} 3! \times \frac{3}{12} \times \frac{4}{11} \times \frac{5}{10}=\frac{3}{11} \frac{\binom{3}{1}\binom{4}{1}\binom{5}{1}}{\binom{12}{3}}=\frac{3}{11} \mathbf{\text{My Question}:} r n \color{red}{\text{distinct}} \binom{12}{3} 3!,"['probability', 'combinatorics', 'probability-theory', 'soft-question', 'combinations']"
92,Unexpected use of linearity of expectation with indicator random variable in problems,Unexpected use of linearity of expectation with indicator random variable in problems,,"Can people suggest some problems (probability puzzle type) where the use of linearity of expectation together with indicator random variable is unexpected/hard to see but it makes problems much easier? I have encountered a lot of questions asking similar types of problems in the various domains so I think combined use of linearity of expectation and indicator random variable deserves its own. This and This are the question I was motivated from, but some of problem in my second mentioned problem are not puzzle related.","Can people suggest some problems (probability puzzle type) where the use of linearity of expectation together with indicator random variable is unexpected/hard to see but it makes problems much easier? I have encountered a lot of questions asking similar types of problems in the various domains so I think combined use of linearity of expectation and indicator random variable deserves its own. This and This are the question I was motivated from, but some of problem in my second mentioned problem are not puzzle related.",,"['probability', 'soft-question', 'expected-value', 'intuition', 'big-list']"
93,What's more likely: $7$-digit number with no $1$'s or at least one $1$ among its digits?,What's more likely: -digit number with no 's or at least one  among its digits?,7 1 1,"A $7$ -digit number is chosen at random. Which is more likely: the number has no $1$ 's among its digits or the number has at least one $1$ among its digits? Here's how I did it: The question is asking whether $8(9)^6$ (the number of those with no $1$ 's among its digits) or $9(10)^6 - 8(9)^6$ (the number of those with at least one $1$ among its digits). Some tedious multiplying shows that $8(9)^6 = 4241528 < 4500000$ , which demonstrates that $9(10^6) - 8(9)^6$ i.e. the number having at least one $1$ 's among its digits is more likely. However, I am wondering if there is a slicker way to get the answer without having to do any tedious multipication.","A -digit number is chosen at random. Which is more likely: the number has no 's among its digits or the number has at least one among its digits? Here's how I did it: The question is asking whether (the number of those with no 's among its digits) or (the number of those with at least one among its digits). Some tedious multiplying shows that , which demonstrates that i.e. the number having at least one 's among its digits is more likely. However, I am wondering if there is a slicker way to get the answer without having to do any tedious multipication.",7 1 1 8(9)^6 1 9(10)^6 - 8(9)^6 1 8(9)^6 = 4241528 < 4500000 9(10^6) - 8(9)^6 1,"['probability', 'combinatorics']"
94,Odds of 3 identical digits in a row in a 6 digit number,Odds of 3 identical digits in a row in a 6 digit number,,"If I have a 6 digit random number what are the odds of me having 3 consecutive digits be identical?  examples $341117$ or $444628$ I thought of two ways to answer this and they give very different results. One (or both!) are incorrect.  Can someone explain to me what is wrong with the reasoning in the wrong one? First approach:  There are 4 possible positions for the 3 identical digits, and at each position there are ten possibilities $(000, 111, 222....999)$ . The remaining digits do not matter. So that sounds like out of the one million number combinations from 000000 all the way up to 999999 there are 40 ways this could happen, so this smells like 40 out of 1 million or one chance out of 25000. (and I think I now see a flaw in that reasoning) Second approach (and now I realize, gives the right answer): For each of the first 4 digits it does not matter what value they have.  There is then a one in 100 chance the next two digits will be the same. so the odds of this happening are 4 times 1 in 100 or one chance in 25. What I missed from the first approach is that for each set of three digits in a row, there are one thousand combinations for the other three digits, so for each of those 40 ways, there are actually 1000 possibilities (example 111432 and 111739 are two of the 1000 ways to have a 6 digit number starting with 111) . So 40,000 ways out of a million numbers is 1 in 25. I guess this is not even a question anymore, but it was when I started writing, so I will share.","If I have a 6 digit random number what are the odds of me having 3 consecutive digits be identical?  examples or I thought of two ways to answer this and they give very different results. One (or both!) are incorrect.  Can someone explain to me what is wrong with the reasoning in the wrong one? First approach:  There are 4 possible positions for the 3 identical digits, and at each position there are ten possibilities . The remaining digits do not matter. So that sounds like out of the one million number combinations from 000000 all the way up to 999999 there are 40 ways this could happen, so this smells like 40 out of 1 million or one chance out of 25000. (and I think I now see a flaw in that reasoning) Second approach (and now I realize, gives the right answer): For each of the first 4 digits it does not matter what value they have.  There is then a one in 100 chance the next two digits will be the same. so the odds of this happening are 4 times 1 in 100 or one chance in 25. What I missed from the first approach is that for each set of three digits in a row, there are one thousand combinations for the other three digits, so for each of those 40 ways, there are actually 1000 possibilities (example 111432 and 111739 are two of the 1000 ways to have a 6 digit number starting with 111) . So 40,000 ways out of a million numbers is 1 in 25. I guess this is not even a question anymore, but it was when I started writing, so I will share.","341117 444628 (000, 111, 222....999)","['probability', 'combinatorics']"
95,Finding expected total number of die rolls,Finding expected total number of die rolls,,"Question Ann and Bob take turns to roll a fair six-sided die. The game ends after a six or three consecutive fives come up, with the winner being the last person who threw the die. Ann will go first. $(a)\quad$ Find the probability that Ann will win. $(b)\quad$ Find the expected total number of rolls. My working Let $(A, 0)$ denote the state in which it is $A$ 's turn and the prior toss was not a $5$ or a $6$ , $(A, 5)$ the state in which it is $A$ 's turn and the prior toss was a $5$ and $(A, 55)$ the state in which it is $A$ 's turn and the prior two tosses were a $5$ . The states for $B$ are defined similarly and note also that $(A, 0)$ is the starting state. Now, for any state $S$ , let $P(S)$ denote the probability that $A$ will eventually win, given that we are now in state $S$ and we have the following relationships: $$\begin{aligned} P(A, 0) & = \frac 1 6 + \frac 1 6 P(B, 5) + \frac 4 6 P(B, 0) \\[5 mm] P(A, 5) & = \frac 1 6 + \frac 1 6 P(B, 55) + \frac 4 6 P(B, 0) \\[5 mm] P(A, 55) & = \frac 2 6 + \frac 4 6 P(B, 0) \\[5 mm] P(B, 0) & = \frac 1 6 P(A, 5) + \frac 4 6 P(A, 0) \\[5 mm] P(B, 5) & = \frac 1 6 P(A, 55) + \frac 4 6 P(A, 0) \\[5 mm] P(B, 55) & = \frac 4 6 P(A, 0) \end{aligned}$$ The system of linear equations above can be easily solved to give $P(A, 0) = \frac {93} {170}$ , which is the correct answer for $(a)$ . However, I am not sure how to approach $(b)$ , whose answer is $\frac {129} {22}$ . Any intuitive suggestions will be greatly appreciated :) Edit Following some hints Joe posted in an answer, I managed to solve $(b)$ :) Let $X$ be the number of dice rolls it takes for the game to stop, $A$ be the event that a $5$ is first rolled, $B$ the event that a $6$ is first rolled and $C$ the event that neither a $5$ nor a $6$ is first rolled. By the law of iterated expectation, we have $$\begin{aligned} \mathbb{E}(X) & = \mathbb{E}(X \mid A) \mathbb{P}(A) + \mathbb{E}(X \mid B) \mathbb{P}(B) + \mathbb{E}(X \mid C) \mathbb{P}(C) \\[5 mm] & = (1)\left(\frac 1 6\right) + \mathbb{E}(X \mid B) \mathbb{P}(B) + [\mathbb{E}(X) + 1]\left(\frac 4 6\right) \end{aligned}$$ To find $\mathbb{E}(X \mid B) \mathbb{P}(B)$ , I chose to consider different cases. The game ends with three $5$ s or two $5$ s and a $6$ , each case happening with probability $\left(\frac 1 6\right)^3$ and taking three turns to end. The game ends with one $5$ and one $6$ , which happens with probability $\left(\frac 1 6\right)^2$ and takes two turns to end. We must also not forget the cases where the game does not end (immediately). Two $5$ s are thrown, followed by neither a $5$ nor a $6$ , which happens with probability $\left(\frac 1 6\right)^2\left(\frac 4 6\right)$ and takes $\mathbb{E}(X + 3)$ turns to end. One $5$ is thrown, followed by neither a $5$ nor a $6$ , which happens with probability $\left(\frac 1 6\right)\left(\frac 4 6\right)$ and takes $\mathbb{E}(X + 2)$ turns to end. These four cases, when added up, will give $$\begin{aligned} \mathbb{E}(X \mid B) \mathbb{P}(B) & = \left(\frac 1 6\right)^3(3)(2) + \left(\frac 1 6\right)^2(2) + \left(\frac 1 6\right)^2\left(\frac 4 6\right)[\mathbb{E}(X + 3)] + \left(\frac 1 6\right)\left(\frac 4 6\right)[\mathbb{E}(X + 2)] \\[5 mm] & = \frac 1 {12} + \frac 1 {54} \mathbb{E}(X + 3) + \frac 1 9 \mathbb{E}(X + 2) \end{aligned}$$ Thus, $$\begin{aligned} \mathbb{E}(X) & = (1)\left(\frac 1 6\right) + \mathbb{E}(X \mid B) \mathbb{P}(B) + [\mathbb{E}(X) + 1]\left(\frac 4 6\right) \\[5 mm] & = \frac 1 6 + \frac 1 {12} + \frac 1 {54} \mathbb{E}(X + 3) + \frac 1 9 \mathbb{E}(X + 2) + \frac 4 6 \mathbb{E}(X + 1) \\[5 mm] & = \frac 1 4 + \frac 1 {54} \mathbb{E}(X + 3) + \frac 1 9 \mathbb{E}(X + 2) + \frac 4 6 \mathbb{E}(X + 1) \\[5 mm] & = \frac {43} {36} + \frac {43} {54} \mathbb{E}(X) \\[5 mm] \implies \mathbb{E}(X) & = \frac {129} {22} \end{aligned}$$","Question Ann and Bob take turns to roll a fair six-sided die. The game ends after a six or three consecutive fives come up, with the winner being the last person who threw the die. Ann will go first. Find the probability that Ann will win. Find the expected total number of rolls. My working Let denote the state in which it is 's turn and the prior toss was not a or a , the state in which it is 's turn and the prior toss was a and the state in which it is 's turn and the prior two tosses were a . The states for are defined similarly and note also that is the starting state. Now, for any state , let denote the probability that will eventually win, given that we are now in state and we have the following relationships: The system of linear equations above can be easily solved to give , which is the correct answer for . However, I am not sure how to approach , whose answer is . Any intuitive suggestions will be greatly appreciated :) Edit Following some hints Joe posted in an answer, I managed to solve :) Let be the number of dice rolls it takes for the game to stop, be the event that a is first rolled, the event that a is first rolled and the event that neither a nor a is first rolled. By the law of iterated expectation, we have To find , I chose to consider different cases. The game ends with three s or two s and a , each case happening with probability and taking three turns to end. The game ends with one and one , which happens with probability and takes two turns to end. We must also not forget the cases where the game does not end (immediately). Two s are thrown, followed by neither a nor a , which happens with probability and takes turns to end. One is thrown, followed by neither a nor a , which happens with probability and takes turns to end. These four cases, when added up, will give Thus,","(a)\quad (b)\quad (A, 0) A 5 6 (A, 5) A 5 (A, 55) A 5 B (A, 0) S P(S) A S \begin{aligned}
P(A, 0) & = \frac 1 6 + \frac 1 6 P(B, 5) + \frac 4 6 P(B, 0)
\\[5 mm] P(A, 5) & = \frac 1 6 + \frac 1 6 P(B, 55) + \frac 4 6 P(B, 0)
\\[5 mm] P(A, 55) & = \frac 2 6 + \frac 4 6 P(B, 0)
\\[5 mm] P(B, 0) & = \frac 1 6 P(A, 5) + \frac 4 6 P(A, 0)
\\[5 mm] P(B, 5) & = \frac 1 6 P(A, 55) + \frac 4 6 P(A, 0)
\\[5 mm] P(B, 55) & = \frac 4 6 P(A, 0)
\end{aligned} P(A, 0) = \frac {93} {170} (a) (b) \frac {129} {22} (b) X A 5 B 6 C 5 6 \begin{aligned}
\mathbb{E}(X) & = \mathbb{E}(X \mid A) \mathbb{P}(A) + \mathbb{E}(X \mid B) \mathbb{P}(B) + \mathbb{E}(X \mid C) \mathbb{P}(C)
\\[5 mm] & = (1)\left(\frac 1 6\right) + \mathbb{E}(X \mid B) \mathbb{P}(B) + [\mathbb{E}(X) + 1]\left(\frac 4 6\right)
\end{aligned} \mathbb{E}(X \mid B) \mathbb{P}(B) 5 5 6 \left(\frac 1 6\right)^3 5 6 \left(\frac 1 6\right)^2 5 5 6 \left(\frac 1 6\right)^2\left(\frac 4 6\right) \mathbb{E}(X + 3) 5 5 6 \left(\frac 1 6\right)\left(\frac 4 6\right) \mathbb{E}(X + 2) \begin{aligned}
\mathbb{E}(X \mid B) \mathbb{P}(B) & = \left(\frac 1 6\right)^3(3)(2) + \left(\frac 1 6\right)^2(2) + \left(\frac 1 6\right)^2\left(\frac 4 6\right)[\mathbb{E}(X + 3)] + \left(\frac 1 6\right)\left(\frac 4 6\right)[\mathbb{E}(X + 2)]
\\[5 mm] & = \frac 1 {12} + \frac 1 {54} \mathbb{E}(X + 3) + \frac 1 9 \mathbb{E}(X + 2)
\end{aligned} \begin{aligned}
\mathbb{E}(X) & = (1)\left(\frac 1 6\right) + \mathbb{E}(X \mid B) \mathbb{P}(B) + [\mathbb{E}(X) + 1]\left(\frac 4 6\right)
\\[5 mm] & = \frac 1 6 + \frac 1 {12} + \frac 1 {54} \mathbb{E}(X + 3) + \frac 1 9 \mathbb{E}(X + 2) + \frac 4 6 \mathbb{E}(X + 1)
\\[5 mm] & = \frac 1 4 + \frac 1 {54} \mathbb{E}(X + 3) + \frac 1 9 \mathbb{E}(X + 2) + \frac 4 6 \mathbb{E}(X + 1)
\\[5 mm] & = \frac {43} {36} + \frac {43} {54} \mathbb{E}(X)
\\[5 mm] \implies \mathbb{E}(X) & = \frac {129} {22}
\end{aligned}","['probability', 'statistics', 'expected-value', 'dice']"
96,Is $\mathbb{E}[X|X<Y]$ finite if $\mathbb{E}[Y]<\infty$?,Is  finite if ?,\mathbb{E}[X|X<Y] \mathbb{E}[Y]<\infty,"I'm afraid I may be overlooking an obvious answer to this question, but perhaps someone can provide some assistance, as probability is not my area of expertise. Suppose we have two independent random variables, $X$ and $Y$ , with $X$ finite almost surely, but $\mathbb{E}[X]=\infty$ and $\mathbb{E}[Y]<\infty$ . I'm trying to understand the quantity $\mathbb{E}[X\mid X<Y]$ . In particular, I would like to know if this conditional expectation is finite. I feel like I should be able to say $\mathbb{E}[X\mid X<Y] < \mathbb{E}[Y]$ , which gives the result, but then I got a bit caught up in the details. Any help, even just a nudge in the right direction, would be greatly appreciated. Edit: so sorry to have left this out, but $X$ and $Y$ are non-negative RV.","I'm afraid I may be overlooking an obvious answer to this question, but perhaps someone can provide some assistance, as probability is not my area of expertise. Suppose we have two independent random variables, and , with finite almost surely, but and . I'm trying to understand the quantity . In particular, I would like to know if this conditional expectation is finite. I feel like I should be able to say , which gives the result, but then I got a bit caught up in the details. Any help, even just a nudge in the right direction, would be greatly appreciated. Edit: so sorry to have left this out, but and are non-negative RV.",X Y X \mathbb{E}[X]=\infty \mathbb{E}[Y]<\infty \mathbb{E}[X\mid X<Y] \mathbb{E}[X\mid X<Y] < \mathbb{E}[Y] X Y,"['probability', 'random-variables', 'conditional-expectation']"
97,Ask a question on Wald statistic (George Casella 10.35 (b)),Ask a question on Wald statistic (George Casella 10.35 (b)),,"This question is from George Casella statistical inference textbook 10.35 (b). Let $X_1,...,X_n$ be a random sample from a $n(\mu,\sigma^2)$ population. If $\sigma^2$ is unknown and $\mu$ is known, find a Wald statistic for testing $H_0: \sigma =\sigma_0$ . My attempt: I refer to textbook page 493. First, I need to find the MLE of $\sigma^2$ . I find it and is the same as the solution. The log likelihood is $$-\frac{n}{2}log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\Sigma(x_i-\mu)^2$$ . Then taking derivatives with $\sigma^2$ , I get the MLE of $\sigma^2$ is $\frac{\Sigma(x_i-\mu)^2}{n}$ . Then I got problems. According to page 473, next I need to get the approximate variance of the estimator. First, I need to get the observed information number. Just taking second derivatives and then adding a minus sign). My frist derivative w.r.t $\sigma$ is: $$-\frac{n}{\sigma}+\frac{\Sigma(x_i-\mu)^2}{\sigma^3}$$ My second derivative w.r.t $\sigma$ is: $$\frac{n}{\sigma^2}-\frac{3\Sigma(x_i-\mu)^2}{\sigma^4}$$ Add a minus sign: $$-\frac{n}{\sigma^2}+\frac{3\Sigma(x_i-\mu)^2}{\sigma^4}$$ Now plug in $\sigma=\sqrt{\frac{\Sigma(x_i-\mu)^2}{n}}$ , I got the observed information number is $\frac{2n^2}{\Sigma(x_i-\mu)^2}$ . Hence, according to page 473 formula, my variance of the MLE of $\sigma$ is the reciprocal, that is $\frac{\Sigma(x_i-\mu)^2}{2n^2}$ . Hence compared with the. solution, our numerator is the same. But my denominator is $\sqrt{\frac{\Sigma(x_i-\mu)^2}{2n^2}}$ , different from the solution. What step did I get wrong? The solution is here: One comment below said I cannot take derivatives r.p.t $\sigma$ . I don't know why. I think it's okay for me to do this, because my goal is to get the approximate variance of $\sigma^2$ . But if I choose to take derivatives r.p.t $\sigma^2$ as suggested, then my first derivative of log likelihood is $$-\frac{n}{2\sigma^2}+\frac{\Sigma(x_i-\mu)^2}{2(\sigma^2)^2}$$ . My second derivative is $$\frac{n}{2(\sigma^2)^2}-\frac{\Sigma(x_i-\mu)^2}{(\sigma^2)^3}$$ Then add a minus sign is: $$-\frac{n}{2(\sigma^2)^2}+\frac{\Sigma(x_i-\mu)^2}{(\sigma^2)^3}$$ Now plug in MLE $\sigma^2=\frac{\Sigma(x_i-\mu)^2}{n}$ , I got the observed information number is $\frac{n^3}{2(\Sigma(x_i-\mu)^2)^2}$ . Now it is the same as the solution. But I am still confused why my previous method to take derivatives r.p.t $\sigma$ failed. My idea is if I take derivatives r.p.t $\sigma$ directly, then I don't need to use delta method to get the variance of $\sigma$ from the variance of of $\sigma^2$ Also, I am stuck how to get the next. According to the solution, now I should use delta method to get the variance of $\sigma$ . My preferred version of delta method is if $W_n \sim AN(a, b_n)$ , where $b_n$ goes to 0, and g is differentiable with $g'(a)$ not 0, then $g(W_n) \sim AN(g(a), [g'(a)]^2 b_n)$ . (AN denotes approximate normal). So my $W_n$ is $\frac{\Sigma(x_i-\mu)^2}{n}$ , my $a$ is $\sigma^2$ , my $b_n$ is $\frac{2(\Sigma(x_i-\mu)^2)^2}{n^3}$ . My g is $\sqrt{}$ . Hence, my approximated variance of $\sigma$ is $$[g'(a)]^2 b_n=(\frac{1}{2\sqrt{a}})^2 b_n=\frac{1}{4a} b_n=\frac{1}{4\sigma^2} \frac{2(\Sigma(x_i-\mu)^2)^2}{n^3}$$ . It's weird. My approximated variance of $\sigma$ contains $\sigma^2$ . Something in my attempt is wrong here.","This question is from George Casella statistical inference textbook 10.35 (b). Let be a random sample from a population. If is unknown and is known, find a Wald statistic for testing . My attempt: I refer to textbook page 493. First, I need to find the MLE of . I find it and is the same as the solution. The log likelihood is . Then taking derivatives with , I get the MLE of is . Then I got problems. According to page 473, next I need to get the approximate variance of the estimator. First, I need to get the observed information number. Just taking second derivatives and then adding a minus sign). My frist derivative w.r.t is: My second derivative w.r.t is: Add a minus sign: Now plug in , I got the observed information number is . Hence, according to page 473 formula, my variance of the MLE of is the reciprocal, that is . Hence compared with the. solution, our numerator is the same. But my denominator is , different from the solution. What step did I get wrong? The solution is here: One comment below said I cannot take derivatives r.p.t . I don't know why. I think it's okay for me to do this, because my goal is to get the approximate variance of . But if I choose to take derivatives r.p.t as suggested, then my first derivative of log likelihood is . My second derivative is Then add a minus sign is: Now plug in MLE , I got the observed information number is . Now it is the same as the solution. But I am still confused why my previous method to take derivatives r.p.t failed. My idea is if I take derivatives r.p.t directly, then I don't need to use delta method to get the variance of from the variance of of Also, I am stuck how to get the next. According to the solution, now I should use delta method to get the variance of . My preferred version of delta method is if , where goes to 0, and g is differentiable with not 0, then . (AN denotes approximate normal). So my is , my is , my is . My g is . Hence, my approximated variance of is . It's weird. My approximated variance of contains . Something in my attempt is wrong here.","X_1,...,X_n n(\mu,\sigma^2) \sigma^2 \mu H_0: \sigma =\sigma_0 \sigma^2 -\frac{n}{2}log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\Sigma(x_i-\mu)^2 \sigma^2 \sigma^2 \frac{\Sigma(x_i-\mu)^2}{n} \sigma -\frac{n}{\sigma}+\frac{\Sigma(x_i-\mu)^2}{\sigma^3} \sigma \frac{n}{\sigma^2}-\frac{3\Sigma(x_i-\mu)^2}{\sigma^4} -\frac{n}{\sigma^2}+\frac{3\Sigma(x_i-\mu)^2}{\sigma^4} \sigma=\sqrt{\frac{\Sigma(x_i-\mu)^2}{n}} \frac{2n^2}{\Sigma(x_i-\mu)^2} \sigma \frac{\Sigma(x_i-\mu)^2}{2n^2} \sqrt{\frac{\Sigma(x_i-\mu)^2}{2n^2}} \sigma \sigma^2 \sigma^2 -\frac{n}{2\sigma^2}+\frac{\Sigma(x_i-\mu)^2}{2(\sigma^2)^2} \frac{n}{2(\sigma^2)^2}-\frac{\Sigma(x_i-\mu)^2}{(\sigma^2)^3} -\frac{n}{2(\sigma^2)^2}+\frac{\Sigma(x_i-\mu)^2}{(\sigma^2)^3} \sigma^2=\frac{\Sigma(x_i-\mu)^2}{n} \frac{n^3}{2(\Sigma(x_i-\mu)^2)^2} \sigma \sigma \sigma \sigma^2 \sigma W_n \sim AN(a, b_n) b_n g'(a) g(W_n) \sim AN(g(a), [g'(a)]^2 b_n) W_n \frac{\Sigma(x_i-\mu)^2}{n} a \sigma^2 b_n \frac{2(\Sigma(x_i-\mu)^2)^2}{n^3} \sqrt{} \sigma [g'(a)]^2 b_n=(\frac{1}{2\sqrt{a}})^2 b_n=\frac{1}{4a} b_n=\frac{1}{4\sigma^2} \frac{2(\Sigma(x_i-\mu)^2)^2}{n^3} \sigma \sigma^2","['probability', 'statistics', 'statistical-inference']"
98,The Board Football Problem (Part I),The Board Football Problem (Part I),,"The original question is here ( The Board Football Problem (Probability) ) and part II is here( The Board Football Problem (Part II) ). I was told to segment the question in order to increase the chances of it being answered. As kids we used to play the game ""Board Football"". I'm not particularly sure of the popularity of the game but the rules were pretty simple to follow. The basic objective of the game was to score as many goals as possible and you played it with nothing more than dice and a notebook. It's a two player game, which goes on as long as the players don't bored. ie there isn't any way to win/finish the game.Both you (say A) and the opponent (say B) have a dice. Before starting the game you decide the minimum number of ""passes"" that need to be completed before a goal is scored(say p). Let's assume that A starts first.A rolls the dice and gets a certain number, say 5. This means that A completes 5 passes in that round and has to complete p-5 more passes in order to progress to the ""Final Stage"". However, before the round is completed the opponent(B) has the opportunity to ""intercept"" you and obtain possession of the ball. The way to do this is pretty simple. B rolls his dice and if he gets the same number as A, then B obtains possession of the ball and the round is completed. In the next round, B will start off from 0 passes. We weren't particularly finicky back then so even if we rolled a number to exceed the number of passes made, the player in possession was still allowed to progress to the ""Final Stage"". It didn't matter in what order the passes in each round were made as long as the required passes was obtained. Once the required passes were made and the player reached the ""Final Stage"", they had to flip a coin. If it turns up as heads,the player in possession scores and if it turns up as tails, he doesn't score. In both the cases, the possession is reverted back to B for the next round. We considered the coin-flipping part to take place in the same round as the round in which the player in possession completed the required passes. For example, let a 6 sided dice be used and the required number of passes be 9. (That is n=6,p=9) Let us say that A has the ball in the starting. In the first round, A rolls 4 and B rolls 5. A completes 4 passes within that round.A now has to complete at least 5 more passes in order to get an opportunity to shoot. In the second round, A rolls 3 and B rolls 3. Possession is overturned and now B controls the ball.B now has 0 passes completed. In the third round, B rolls 5 and A rolls 2.B has completed 5 passes in this round and needs to complete at least 4 more passes in order to get an opportunity to shoot. In the fourth round, B rolls 6 and A rolls 4. Although B has overshot the number of passes,  B is permitted to flip the coin. B tosses the coin and gets heads. B scores a goal.B leads by one goal to nil. Now A starts off with possession of the ball. So my question is as follows:If we use an n-sided dice and the number of required passes is p, the what is the probability that the player in possession of the ball(starting off from 0 passes completed) scores a goal without losing possession of the ball. Is it possible to calculate the value for this in a non-computational /formulaic manner? ( BONUS QUESTION ) Form a graph with the Y-Axis being the probability that the player in possession scores without losing possession of the ball and the number of required passes be the X-Axis. (Let a 10 sided dice be used for this case.) If possible, determine how different the graph would look [in a visual sense, no need for deeper calculations] if the number of sides on the dice is made to vary. Is the graph just shifted along the x-axis or do the peaks become more gradual?). PS This isn't a homework question/problem so please don't close it because an answer is being requested. It is an original question. Consider this to be a challenge of sorts.","The original question is here ( The Board Football Problem (Probability) ) and part II is here( The Board Football Problem (Part II) ). I was told to segment the question in order to increase the chances of it being answered. As kids we used to play the game ""Board Football"". I'm not particularly sure of the popularity of the game but the rules were pretty simple to follow. The basic objective of the game was to score as many goals as possible and you played it with nothing more than dice and a notebook. It's a two player game, which goes on as long as the players don't bored. ie there isn't any way to win/finish the game.Both you (say A) and the opponent (say B) have a dice. Before starting the game you decide the minimum number of ""passes"" that need to be completed before a goal is scored(say p). Let's assume that A starts first.A rolls the dice and gets a certain number, say 5. This means that A completes 5 passes in that round and has to complete p-5 more passes in order to progress to the ""Final Stage"". However, before the round is completed the opponent(B) has the opportunity to ""intercept"" you and obtain possession of the ball. The way to do this is pretty simple. B rolls his dice and if he gets the same number as A, then B obtains possession of the ball and the round is completed. In the next round, B will start off from 0 passes. We weren't particularly finicky back then so even if we rolled a number to exceed the number of passes made, the player in possession was still allowed to progress to the ""Final Stage"". It didn't matter in what order the passes in each round were made as long as the required passes was obtained. Once the required passes were made and the player reached the ""Final Stage"", they had to flip a coin. If it turns up as heads,the player in possession scores and if it turns up as tails, he doesn't score. In both the cases, the possession is reverted back to B for the next round. We considered the coin-flipping part to take place in the same round as the round in which the player in possession completed the required passes. For example, let a 6 sided dice be used and the required number of passes be 9. (That is n=6,p=9) Let us say that A has the ball in the starting. In the first round, A rolls 4 and B rolls 5. A completes 4 passes within that round.A now has to complete at least 5 more passes in order to get an opportunity to shoot. In the second round, A rolls 3 and B rolls 3. Possession is overturned and now B controls the ball.B now has 0 passes completed. In the third round, B rolls 5 and A rolls 2.B has completed 5 passes in this round and needs to complete at least 4 more passes in order to get an opportunity to shoot. In the fourth round, B rolls 6 and A rolls 4. Although B has overshot the number of passes,  B is permitted to flip the coin. B tosses the coin and gets heads. B scores a goal.B leads by one goal to nil. Now A starts off with possession of the ball. So my question is as follows:If we use an n-sided dice and the number of required passes is p, the what is the probability that the player in possession of the ball(starting off from 0 passes completed) scores a goal without losing possession of the ball. Is it possible to calculate the value for this in a non-computational /formulaic manner? ( BONUS QUESTION ) Form a graph with the Y-Axis being the probability that the player in possession scores without losing possession of the ball and the number of required passes be the X-Axis. (Let a 10 sided dice be used for this case.) If possible, determine how different the graph would look [in a visual sense, no need for deeper calculations] if the number of sides on the dice is made to vary. Is the graph just shifted along the x-axis or do the peaks become more gradual?). PS This isn't a homework question/problem so please don't close it because an answer is being requested. It is an original question. Consider this to be a challenge of sorts.",,"['probability', 'combinatorics', 'contest-math', 'recreational-mathematics', 'puzzle']"
99,Dartboard paradox and understanding independence,Dartboard paradox and understanding independence,,"By definition, events $A$ and $B$ are independent if $$P(A \cap     B) = P(A)\:P(B).$$ Thus if an event $A$ happens almost never $\left(P(A)=0\right),$ then $A$ is independent of all events, including itself. So, hitting the exact centre of a dartboard (happens almost never) and hitting within its inner ring are independent events. On the other hand, it is standard to characterise independence as follows: Two events are independent if the occurrence of one does not affect the probability of occurrence of the other. Since hitting the exact centre of a dartboard guarantees hitting within its inner ring (which is otherwise not guaranteed), the two events are dependent. Isn't this a contradiction?? If yes, then do we accept that the definition of independence is not meant to fully correspond to its verbal/intuitive characterisation? Is there any semantic difference between the following two versions? $(i)$ Two events are independent if the occurrence of one does not affect the probability of occurrence of the other. $(ii)$ Two events are independent if the occurrence of one does not affect the occurrence of the other. ADDENDUM On further pondering, I have resolved both questions: 2. Consider this experiment: flip two fair coins, letting $H_1$ be the event that the first coin lands on Heads, and $X$ be the event that the coins land on different sides. Then $$ P\left(H_1 \cap X\right)=\frac14=P(H_1)\:P(X);$$ i.e., $H_1$ and $X$ are independent events. Knowledge that $H_1$ happens reduces the possible number of ways that $X$ can eventuate—from $2$ (outcomes HT and TH) to $1$ (outcome HT)—but does not change the probability $\left(\frac12\right)$ of $X.$ 1. The following revision characterises pairwise independence more clearly and accurately: Let $P(A)\neq0.$ Events $A$ and $B$ are independent iff knowing that $A$ happens doesn't change $B$ 's probability. In this informal characterisation, almost-never events are now excluded from being conditioned on. (What does it even mean to say that an almost-never event has happened: in what sense have I hit the exact centre of a dartboard?) It motivates the definition of pairwise independence, which does allow both events to be impossible.","By definition, events and are independent if Thus if an event happens almost never then is independent of all events, including itself. So, hitting the exact centre of a dartboard (happens almost never) and hitting within its inner ring are independent events. On the other hand, it is standard to characterise independence as follows: Two events are independent if the occurrence of one does not affect the probability of occurrence of the other. Since hitting the exact centre of a dartboard guarantees hitting within its inner ring (which is otherwise not guaranteed), the two events are dependent. Isn't this a contradiction?? If yes, then do we accept that the definition of independence is not meant to fully correspond to its verbal/intuitive characterisation? Is there any semantic difference between the following two versions? Two events are independent if the occurrence of one does not affect the probability of occurrence of the other. Two events are independent if the occurrence of one does not affect the occurrence of the other. ADDENDUM On further pondering, I have resolved both questions: 2. Consider this experiment: flip two fair coins, letting be the event that the first coin lands on Heads, and be the event that the coins land on different sides. Then i.e., and are independent events. Knowledge that happens reduces the possible number of ways that can eventuate—from (outcomes HT and TH) to (outcome HT)—but does not change the probability of 1. The following revision characterises pairwise independence more clearly and accurately: Let Events and are independent iff knowing that happens doesn't change 's probability. In this informal characterisation, almost-never events are now excluded from being conditioned on. (What does it even mean to say that an almost-never event has happened: in what sense have I hit the exact centre of a dartboard?) It motivates the definition of pairwise independence, which does allow both events to be impossible.","A B P(A \cap
    B) = P(A)\:P(B). A \left(P(A)=0\right), A (i) (ii) H_1 X  P\left(H_1 \cap X\right)=\frac14=P(H_1)\:P(X); H_1 X H_1 X 2 1 \left(\frac12\right) X. P(A)\neq0. A B A B","['probability', 'terminology', 'definition']"
