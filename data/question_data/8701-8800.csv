,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Proving $\int_{1}^{\infty}\frac{\sin x}{\left(\log x\right)^{\frac{1}{2}}}dx$ converges,Proving  converges,\int_{1}^{\infty}\frac{\sin x}{\left(\log x\right)^{\frac{1}{2}}}dx,"Prove that the following improper integral converges $$\int_{1}^{\infty}\frac{\sin x}{\left(\log x\right)^{\frac{1}{2}}}dx.$$ I see that you can show this using Dirichlet's Convergence Test, but how would you show it not using this test?","Prove that the following improper integral converges $$\int_{1}^{\infty}\frac{\sin x}{\left(\log x\right)^{\frac{1}{2}}}dx.$$ I see that you can show this using Dirichlet's Convergence Test, but how would you show it not using this test?",,"['real-analysis', 'analysis']"
1,Are Tonelli's and Fubini's theorem equivalent?,Are Tonelli's and Fubini's theorem equivalent?,,"I can derive Fubini's theorem for interated integrals of complex functions from Tonelli's theorem for iterated integrals for unsigned functions.  I was wondering whether there is a way to go backwards.  I do not think so, because Fubini's theorem assumes the integrals are finite, whereas Tonelli's theorem allows the value of the integral to be $+\infty$. But maybe we can use a limiting argument? This is where I am not clear. So: is it possible to derive Tonelli's theorem from Fubini's theorem?  If so, I would appreciate a proof (or a outline of a proof).","I can derive Fubini's theorem for interated integrals of complex functions from Tonelli's theorem for iterated integrals for unsigned functions.  I was wondering whether there is a way to go backwards.  I do not think so, because Fubini's theorem assumes the integrals are finite, whereas Tonelli's theorem allows the value of the integral to be $+\infty$. But maybe we can use a limiting argument? This is where I am not clear. So: is it possible to derive Tonelli's theorem from Fubini's theorem?  If so, I would appreciate a proof (or a outline of a proof).",,"['real-analysis', 'measure-theory']"
2,Recommendations for real analysis,Recommendations for real analysis,,"I have completed two courses in real analysis that covered up to chapter 9 in Rudin's Principle of Mathematical Analysis (and one on complex analysis). So if I am interested in continuing on in analysis (real analysis and not complex analysis), what would be a good direction to go from here? What would be a good book to learn from? As my interest is primarily number theory I was wondering if there is a direction in analysis that would be helpful in this aspect.","I have completed two courses in real analysis that covered up to chapter 9 in Rudin's Principle of Mathematical Analysis (and one on complex analysis). So if I am interested in continuing on in analysis (real analysis and not complex analysis), what would be a good direction to go from here? What would be a good book to learn from? As my interest is primarily number theory I was wondering if there is a direction in analysis that would be helpful in this aspect.",,"['real-analysis', 'reference-request', 'advice']"
3,"If $ f \in C_0^\infty$, then is $f$ uniformly continuous?","If , then is  uniformly continuous?", f \in C_0^\infty f,"If $ f \in C_0^\infty=\{ g: g\in C^\infty, \lim_{|x|\rightarrow \infty}g(x)=0\}$, then is $f$ uniformly continuous on $\mathbb R$? ($ f : \mathbb R \to \mathbb R $)","If $ f \in C_0^\infty=\{ g: g\in C^\infty, \lim_{|x|\rightarrow \infty}g(x)=0\}$, then is $f$ uniformly continuous on $\mathbb R$? ($ f : \mathbb R \to \mathbb R $)",,['real-analysis']
4,Sequential characterization of closedness of the set,Sequential characterization of closedness of the set,,"Let $(X, d)$ be a metric space. A set $F\subset X$ is closed if and only if for every sequence $\left\{x_n\right\}\subset F$, if $x\in X$ and $x_n\rightarrow x$ then $x\in F$. Definition of closed set: Set is closed if and only if its complement is open. A Set $U$ is open if and only if $\forall_{x\in U}\exists_{r>0}B(x,r)\subset U$, where $B(x,r)$ is a ball with middle in $x$ and with radius $r$. It's rather a well-known fact that I used many times while solving problems, but just now I realized that I don't know how to prove it.","Let $(X, d)$ be a metric space. A set $F\subset X$ is closed if and only if for every sequence $\left\{x_n\right\}\subset F$, if $x\in X$ and $x_n\rightarrow x$ then $x\in F$. Definition of closed set: Set is closed if and only if its complement is open. A Set $U$ is open if and only if $\forall_{x\in U}\exists_{r>0}B(x,r)\subset U$, where $B(x,r)$ is a ball with middle in $x$ and with radius $r$. It's rather a well-known fact that I used many times while solving problems, but just now I realized that I don't know how to prove it.",,"['real-analysis', 'metric-spaces']"
5,The closure of $C^1$ in the  functions of bounded variation,The closure of  in the  functions of bounded variation,C^1,"Consider the space $(BV[0,1];||.||)$ with the norm $$||f||=|f(0)|+V_{f}[0,1]$$ Where $V_{f}[0,1]$ is the variation of $f$. My questions what is the closure of $C^1[0,1]$ with respect to this norm? Another question is how to prove that this norm is Banach?","Consider the space $(BV[0,1];||.||)$ with the norm $$||f||=|f(0)|+V_{f}[0,1]$$ Where $V_{f}[0,1]$ is the variation of $f$. My questions what is the closure of $C^1[0,1]$ with respect to this norm? Another question is how to prove that this norm is Banach?",,"['real-analysis', 'measure-theory', 'functional-analysis']"
6,"Show a $\sigma$-algebra contains the Borel sets : with $(a,\infty)$ or $(-\infty,b)$?",Show a -algebra contains the Borel sets : with  or ?,"\sigma (a,\infty) (-\infty,b)","For a certain $\sigma$-algebra $A$ on the real line, I would like to show that it contains the Borel sets. I can show that $A$ contains the left and right half-line $(a,\infty)$ and $(-\infty,b)$ for any real numbers $a$ and $b$. My question is : can I infer that $A$ contains the Borel sets by only prooving that it contains the left half-line or is it mandatory to show that $A$ contains both half-line? I'm not clear on how the Borel sets are generated from half-line and open intervals.","For a certain $\sigma$-algebra $A$ on the real line, I would like to show that it contains the Borel sets. I can show that $A$ contains the left and right half-line $(a,\infty)$ and $(-\infty,b)$ for any real numbers $a$ and $b$. My question is : can I infer that $A$ contains the Borel sets by only prooving that it contains the left half-line or is it mandatory to show that $A$ contains both half-line? I'm not clear on how the Borel sets are generated from half-line and open intervals.",,['real-analysis']
7,Uniqueness of power series,Uniqueness of power series,,"Is there two sequences of real numbers $a_i$ and $b_i\neq 8$, not depending on $x$, such that $x^8=\sum_{k=1}^{\infty}a_kx^{b_k}$ for all $x$? If $\displaystyle\sum_{k=1}^{\infty}a_kx^{b_k}=\sum_{k=1}^{\infty}c_kx^{d_k}$ for all $x>1$, and all coefficients are real and $b_k>b_{k+1}$, and $d_k>d_{k+1}$, is there a way to prove that $a_k=c_k$ and $b_k=d_k$ for all $k$? If $\displaystyle\sum_{k=1}^{\infty}a_kx^{b_k}=0$ for all $x>1$ and $b_i\neq b_j$, and all coefficients are real, must $a_k=0$?","Is there two sequences of real numbers $a_i$ and $b_i\neq 8$, not depending on $x$, such that $x^8=\sum_{k=1}^{\infty}a_kx^{b_k}$ for all $x$? If $\displaystyle\sum_{k=1}^{\infty}a_kx^{b_k}=\sum_{k=1}^{\infty}c_kx^{d_k}$ for all $x>1$, and all coefficients are real and $b_k>b_{k+1}$, and $d_k>d_{k+1}$, is there a way to prove that $a_k=c_k$ and $b_k=d_k$ for all $k$? If $\displaystyle\sum_{k=1}^{\infty}a_kx^{b_k}=0$ for all $x>1$ and $b_i\neq b_j$, and all coefficients are real, must $a_k=0$?",,"['real-analysis', 'sequences-and-series', 'power-series']"
8,Upper semi-continuity and lower semi-continuity of particular functions,Upper semi-continuity and lower semi-continuity of particular functions,,"Let $f:[a,b]\to \mathbb{R}$ be a bounded function. Let $$\bar{f}(x) = \inf_{\delta>0}\sup_{|x-y|<\delta} f(y)$$ $$\underline{f}(x) = \sup_{\delta>0}\inf_{|x-y|<\delta} f(y)$$ (small side question, does anyone know the latex code for a small bar underneath an item?) The problem (well it's got 4 parts, but I'll just put one and hopefully figure things out on the other 3 with a little nudge) Show that $\bar{f}$ is upper semi-continuous on $[a,b]$ The definition of upper semi-continuous we are using is: A function $f$ is upper semi-continuous at $x$ if whenever $f(x)<\alpha$ there is an open set $O$, containing $x$, such that $f(y)<\alpha$ for $y\in O$. I've tried showing this directly and by contradiction, but in both cases I'm simply finding I don't fully understand how to deal with these functions. For example, my direct argument: We must show that when $\bar{f}(x) < \alpha$, there is an open set $O\subset [a,b]$, containing $x$, such that $\bar{f}(z) < \alpha$ for all $z\in O$. But this means we must show if $$\inf_{\delta>0}\sup_{|x-y| < \delta} f(y) < \alpha,$$ then there is an open set $O\subset[a,b]$ containing $x$ such that $$\inf_{\delta>0}\sup_{|z-y| < \delta} f(y) < \alpha,$$ for all $z\in O$. Fix some $\delta'>0$. For all $\delta > \delta'$, $$ \inf \sup \{y:|z-y|<\delta\}f(y) \leq \inf \sup \{y:|z-y|<\delta'\} f(y).$$ so...? Some other thoughts: We note if we have $$ f(y) > \alpha$$ for all $y\neq x$ on some open neighbourhood of $x$, with $f(x) < \alpha$, then $\bar{f}(x) \geq \alpha$, as $\bar{f}(x)$ does not actually use the value of $f$ at $x$. Hence if $f$ has a removable discontinuity or singularity, $\bar{f}$ will not notice. Hence we may assume our function $f$ does not have a removable singularity. But then $f$ can only have jump discontinuity or an essential discontinuity. If $f$ has a jump discontinuity, perhaps we can find some open neighbourhood $U$ of $f$ containing $x$ such that $f(U)$ is homeomorphic to something connected apart from the jump, and well behaved. If $f$ has an essential discontinuity at $x$, the limit must not exist (as our function is bounded), thought $\bar{f}$ still exists. Again, we could `zoom in' to some connected component (apart from the discontinuity) and things should be nicer there? I'm not sure how to pick such an open neighbourhood of our point though... Even the slightest nudge would be appreciated :) Thanks!","Let $f:[a,b]\to \mathbb{R}$ be a bounded function. Let $$\bar{f}(x) = \inf_{\delta>0}\sup_{|x-y|<\delta} f(y)$$ $$\underline{f}(x) = \sup_{\delta>0}\inf_{|x-y|<\delta} f(y)$$ (small side question, does anyone know the latex code for a small bar underneath an item?) The problem (well it's got 4 parts, but I'll just put one and hopefully figure things out on the other 3 with a little nudge) Show that $\bar{f}$ is upper semi-continuous on $[a,b]$ The definition of upper semi-continuous we are using is: A function $f$ is upper semi-continuous at $x$ if whenever $f(x)<\alpha$ there is an open set $O$, containing $x$, such that $f(y)<\alpha$ for $y\in O$. I've tried showing this directly and by contradiction, but in both cases I'm simply finding I don't fully understand how to deal with these functions. For example, my direct argument: We must show that when $\bar{f}(x) < \alpha$, there is an open set $O\subset [a,b]$, containing $x$, such that $\bar{f}(z) < \alpha$ for all $z\in O$. But this means we must show if $$\inf_{\delta>0}\sup_{|x-y| < \delta} f(y) < \alpha,$$ then there is an open set $O\subset[a,b]$ containing $x$ such that $$\inf_{\delta>0}\sup_{|z-y| < \delta} f(y) < \alpha,$$ for all $z\in O$. Fix some $\delta'>0$. For all $\delta > \delta'$, $$ \inf \sup \{y:|z-y|<\delta\}f(y) \leq \inf \sup \{y:|z-y|<\delta'\} f(y).$$ so...? Some other thoughts: We note if we have $$ f(y) > \alpha$$ for all $y\neq x$ on some open neighbourhood of $x$, with $f(x) < \alpha$, then $\bar{f}(x) \geq \alpha$, as $\bar{f}(x)$ does not actually use the value of $f$ at $x$. Hence if $f$ has a removable discontinuity or singularity, $\bar{f}$ will not notice. Hence we may assume our function $f$ does not have a removable singularity. But then $f$ can only have jump discontinuity or an essential discontinuity. If $f$ has a jump discontinuity, perhaps we can find some open neighbourhood $U$ of $f$ containing $x$ such that $f(U)$ is homeomorphic to something connected apart from the jump, and well behaved. If $f$ has an essential discontinuity at $x$, the limit must not exist (as our function is bounded), thought $\bar{f}$ still exists. Again, we could `zoom in' to some connected component (apart from the discontinuity) and things should be nicer there? I'm not sure how to pick such an open neighbourhood of our point though... Even the slightest nudge would be appreciated :) Thanks!",,"['real-analysis', 'analysis']"
9,Quadratic iterative system,Quadratic iterative system,,"A general linear iteative system can be represented as a matrix: $$(x,y)\mapsto(ax+by,cx+dy)$$ is essentially the same as $$\left[\begin{array}{cc} a&b\\ c&d\\ \end{array}\right] \left[\begin{array}{c} x\\ y\\ \end{array} \right]$$ which is useful because it can be iterated quickly (matrix exponentiation) and enables various matrix techniques for determining asymptotic behavior and the like. (Of course the number of variables can be increased as needed.) Is there a similar tool for quadratic iterative systems like $$(x,y)\mapsto(ax^2+bxy+cy^2,dx^2+exy+fy^2)$$ ? I'm interested in computing the $n$th iterate ($n$ not too small), finding asymptotic behavior, and any other interesting things that can be determined for a given collection of constants $a,b,\ldots$. My immediate interest (genetics, oddly enough) does not use any of the diagonal terms $x^2,y^2$ so a treatment that ignores them would be fine (though I suspect including is more natural).","A general linear iteative system can be represented as a matrix: $$(x,y)\mapsto(ax+by,cx+dy)$$ is essentially the same as $$\left[\begin{array}{cc} a&b\\ c&d\\ \end{array}\right] \left[\begin{array}{c} x\\ y\\ \end{array} \right]$$ which is useful because it can be iterated quickly (matrix exponentiation) and enables various matrix techniques for determining asymptotic behavior and the like. (Of course the number of variables can be increased as needed.) Is there a similar tool for quadratic iterative systems like $$(x,y)\mapsto(ax^2+bxy+cy^2,dx^2+exy+fy^2)$$ ? I'm interested in computing the $n$th iterate ($n$ not too small), finding asymptotic behavior, and any other interesting things that can be determined for a given collection of constants $a,b,\ldots$. My immediate interest (genetics, oddly enough) does not use any of the diagonal terms $x^2,y^2$ so a treatment that ignores them would be fine (though I suspect including is more natural).",,"['real-analysis', 'dynamical-systems']"
10,justification on changing indexes in double sum,justification on changing indexes in double sum,,"I was wondering what is the justification for this step(changing the indexes)$\displaystyle\sum_{n=0}^{\infty}\frac{a^{n}}{n!}\sum_{m=0}^{\infty}\frac{b^{m}}{m!}=\sum_{k=0}^{\infty}\frac{1}{k!}\sum_{n=0}^{k}\frac{k!}{n!(k-n)!}a^{n}b^{k-n}$ , in Rudin's Real and complex analysis prolog to show $(\exp{a})( \exp{b})=\exp{(a+b)}$, is it the same principle that use fubini's theorem for integrals, I mean that one that says if given the domain of integration D=AxB=ExF then I can do something like $\int_{D}=\int_{A}\int_{B}=\int_{E}\int_{F}$ , I would appreciate any hint or reference to this, thanks in advance.","I was wondering what is the justification for this step(changing the indexes)$\displaystyle\sum_{n=0}^{\infty}\frac{a^{n}}{n!}\sum_{m=0}^{\infty}\frac{b^{m}}{m!}=\sum_{k=0}^{\infty}\frac{1}{k!}\sum_{n=0}^{k}\frac{k!}{n!(k-n)!}a^{n}b^{k-n}$ , in Rudin's Real and complex analysis prolog to show $(\exp{a})( \exp{b})=\exp{(a+b)}$, is it the same principle that use fubini's theorem for integrals, I mean that one that says if given the domain of integration D=AxB=ExF then I can do something like $\int_{D}=\int_{A}\int_{B}=\int_{E}\int_{F}$ , I would appreciate any hint or reference to this, thanks in advance.",,"['real-analysis', 'sequences-and-series']"
11,Level sets of a continuous function,Level sets of a continuous function,,Let $f$ be continuous on a compact subset $X$ of a metric space. If we put $A_h=\{x\in X:f(x)<h\}$ and $B_h=\{x\in X:f(x)\leq h\}$ - when is it true that $B_h = \overline{A_h}$? Is it true if and only if $A_h$ is not empty? Edited: Theo already showed that there are counterexamples. Is it true then that $A_h = B_h^\circ$?,Let $f$ be continuous on a compact subset $X$ of a metric space. If we put $A_h=\{x\in X:f(x)<h\}$ and $B_h=\{x\in X:f(x)\leq h\}$ - when is it true that $B_h = \overline{A_h}$? Is it true if and only if $A_h$ is not empty? Edited: Theo already showed that there are counterexamples. Is it true then that $A_h = B_h^\circ$?,,['real-analysis']
12,Big $O$ vs Big $\Theta$,Big  vs Big,O \Theta,"I am aware of the big theta notation $f = \Theta(g)$ if and only if there are positive constants $A, B$ and $x_0 > 0$ such that for all $x > x_0$ we have $$ A|g(x)| \leq |f(x)| \leq B |g(x)|. $$ What if the condition is the following: $$ C_1 + A|g(x)| \leq |f(x)| \leq C_2 + B |g(x)| $$ where $C_1, C_2$ are possibly negative? Certainly more can be said than just $f = O(g)$. Is there a generalized $\Theta$ notation which allows shifts (by, say $C_1, C_2$)? In particular, I'm interested in the special case: \begin{eqnarray} -C  \leq f(x) - g(x) \leq C \end{eqnarray} for some positive $C$. How does $f$ compare to $g$ in this case? If $f$ and $g$ are positive functions of $x$ which both diverge to $\infty$, is it true that $f(x) = -C + g(x) + \Theta(1)$? What is the appropriate asymptotic notation in this case? Update Thanks for the clarifying answers. Now here is a slightly harder question. Suppose $f$ is discrete and $g$ is continuous. Suppose further that as $x \to \infty$, the difference $f(x) - g(x)$ is asymptotically bounded in the interval $[-C,C]$ but does not necessarily converge to $0$. Does $f \sim g$ still make sense? Would it be more appropriate to use $\liminf_{x \to \infty} f(x) - g(x) = - C$ and $\limsup_{x \to \infty} f(x) - g(x) = C$?","I am aware of the big theta notation $f = \Theta(g)$ if and only if there are positive constants $A, B$ and $x_0 > 0$ such that for all $x > x_0$ we have $$ A|g(x)| \leq |f(x)| \leq B |g(x)|. $$ What if the condition is the following: $$ C_1 + A|g(x)| \leq |f(x)| \leq C_2 + B |g(x)| $$ where $C_1, C_2$ are possibly negative? Certainly more can be said than just $f = O(g)$. Is there a generalized $\Theta$ notation which allows shifts (by, say $C_1, C_2$)? In particular, I'm interested in the special case: \begin{eqnarray} -C  \leq f(x) - g(x) \leq C \end{eqnarray} for some positive $C$. How does $f$ compare to $g$ in this case? If $f$ and $g$ are positive functions of $x$ which both diverge to $\infty$, is it true that $f(x) = -C + g(x) + \Theta(1)$? What is the appropriate asymptotic notation in this case? Update Thanks for the clarifying answers. Now here is a slightly harder question. Suppose $f$ is discrete and $g$ is continuous. Suppose further that as $x \to \infty$, the difference $f(x) - g(x)$ is asymptotically bounded in the interval $[-C,C]$ but does not necessarily converge to $0$. Does $f \sim g$ still make sense? Would it be more appropriate to use $\liminf_{x \to \infty} f(x) - g(x) = - C$ and $\limsup_{x \to \infty} f(x) - g(x) = C$?",,"['real-analysis', 'asymptotics']"
13,Convergence of Series $\sum 1/(1+ n^2 x)$ Uniformly (Homework),Convergence of Series  Uniformly (Homework),\sum 1/(1+ n^2 x),"This is from the book Principles of Mathematical Analysis by Rudin,  number 4 of chapter 7. It says consider $$ f(x) = \sum\limits_{n=1}^{\infty}{ 1/(1+ n^2 x)  } $$ The question asks:  (1) For what values of x does the series converge absolutely. We got that the series converges when x $\not=$ 0 & x $\not= -1/k^2 $ when k is an integer since, there is a discontinuity when n reaches the value $k^2$ . However we don't understand how to do any of the following questions asked. Any hints would be greatly appreciated. We were told that this problem was suppose to be fairly hard for its position in the problem set (ie. 4th question in the Rudin book). (2) What interval does it converge uniformly? (3) On what intervals does it fail to converge uniformly ? (4) Is f continuous wherever the series converges? (5) Is f bounded?","This is from the book Principles of Mathematical Analysis by Rudin,  number 4 of chapter 7. It says consider $$ f(x) = \sum\limits_{n=1}^{\infty}{ 1/(1+ n^2 x)  } $$ The question asks:  (1) For what values of x does the series converge absolutely. We got that the series converges when x $\not=$ 0 & x $\not= -1/k^2 $ when k is an integer since, there is a discontinuity when n reaches the value $k^2$ . However we don't understand how to do any of the following questions asked. Any hints would be greatly appreciated. We were told that this problem was suppose to be fairly hard for its position in the problem set (ie. 4th question in the Rudin book). (2) What interval does it converge uniformly? (3) On what intervals does it fail to converge uniformly ? (4) Is f continuous wherever the series converges? (5) Is f bounded?",,"['real-analysis', 'sequences-and-series', 'analysis', 'uniform-convergence']"
14,Fejér's Theorem (Problem in Rudin),Fejér's Theorem (Problem in Rudin),,"Can you solve Problem 19 from Chapter 8 of Rudin's Principles of Mathematical Analysis, I'm having a lot of difficulty with it I've proven the first part, namely  $$\lim_{N\to\infty}\frac{1}{N}\sum_{n=1}^N \exp(ik(x+n\alpha))=\frac{1}{2\pi}\int_{-\pi}^\pi(\cdots) = \begin{cases} 1\text{ if }k=0\\0\text{ otherwise}\end{cases}$$ Now I want to prove that if $f$ is continuous in $\mathbb{R}$ and $f(x+2\pi)=f(x)$ for all $x$ then $$\lim_{N\to\infty} \sum_{n=1}^{N} \frac{1}{N} f(x+n\alpha)=\frac{1}{2\pi} \int\limits_{-\pi}^{\pi}f(t)\mathrm dt$$ for any $x$, where $\alpha/\pi$ is irrational. I've tried writing it as $$\lim_{N\to\infty}\frac{1}{N}\sum_{n=1}^N \sum_{k=0}^N\frac{1}{2\pi}\int_{-\pi}^\pi e^{ikt}f(x+n\alpha) $$ but that was not helpful.","Can you solve Problem 19 from Chapter 8 of Rudin's Principles of Mathematical Analysis, I'm having a lot of difficulty with it I've proven the first part, namely  $$\lim_{N\to\infty}\frac{1}{N}\sum_{n=1}^N \exp(ik(x+n\alpha))=\frac{1}{2\pi}\int_{-\pi}^\pi(\cdots) = \begin{cases} 1\text{ if }k=0\\0\text{ otherwise}\end{cases}$$ Now I want to prove that if $f$ is continuous in $\mathbb{R}$ and $f(x+2\pi)=f(x)$ for all $x$ then $$\lim_{N\to\infty} \sum_{n=1}^{N} \frac{1}{N} f(x+n\alpha)=\frac{1}{2\pi} \int\limits_{-\pi}^{\pi}f(t)\mathrm dt$$ for any $x$, where $\alpha/\pi$ is irrational. I've tried writing it as $$\lim_{N\to\infty}\frac{1}{N}\sum_{n=1}^N \sum_{k=0}^N\frac{1}{2\pi}\int_{-\pi}^\pi e^{ikt}f(x+n\alpha) $$ but that was not helpful.",,['real-analysis']
15,Limit of integral of sum of cosine functions by CLT?,Limit of integral of sum of cosine functions by CLT?,,"I want to show that $$\lim_{n\to \infty} (2\pi)^{-d}n^{d/2}d^{-2n}\int_{[-\pi, \pi]^d} (\cos(x_1)+\cdots +\cos(x_d))^{2n} dx_1\cdots dx_d =2(d/4\pi)^{d/2}$$ holds. How do I prove this? It seems that the central limit theorem can be used for this problem, but I don't know how to apply it. Or is there another good way to prove it?","I want to show that holds. How do I prove this? It seems that the central limit theorem can be used for this problem, but I don't know how to apply it. Or is there another good way to prove it?","\lim_{n\to \infty} (2\pi)^{-d}n^{d/2}d^{-2n}\int_{[-\pi, \pi]^d} (\cos(x_1)+\cdots +\cos(x_d))^{2n} dx_1\cdots dx_d =2(d/4\pi)^{d/2}","['real-analysis', 'integration', 'probability-theory', 'lebesgue-integral', 'central-limit-theorem']"
16,A question on proving an inequality involving a sequence of real numbers,A question on proving an inequality involving a sequence of real numbers,,"Let $a_n$ be a sequence of real numbers such that $1=a_1 \le a_2 \le a_3 \le \cdots \le a_n.$ Additionally, we have that $a_{i+1}-a_i \le \sqrt{a_i},$ for all $1 \le i <n.$ Then prove that $$\sum_{i=1}^{n-1}\frac{a_{i+1}-a_i}{a_i} \le 2 \log_2 n.$$ My attempt: Consider the LHS of the given inequality to be proved. $$\sum_{i=1}^{n-1}\frac{a_{i+1}-a_i}{a_i} \le \sum_{i=1}^{n-1}\frac{\sqrt{a_i}}{a_i} = \sum_{i=1}^{n-1}\frac{1}{\sqrt{a_i}}.$$ Since we have that $1=a_1 \le a_2 \le a_3 \le \cdots \le a_n,$ we get that $\frac{1}{\sqrt{a_i}} \le 1$ for all $1 \le i <n.$ Hence, $$\sum_{i=1}^{n-1}\frac{a_{i+1}-a_i}{a_i} \le  \sum_{i=1}^{n-1}\frac{1}{\sqrt{a_i}} \le n-1.$$ I know that this is very loose bound on the given sum. How can I possibly improve this? Please give any hints or suggestions. Thanks for all your inputs in advance.","Let be a sequence of real numbers such that Additionally, we have that for all Then prove that My attempt: Consider the LHS of the given inequality to be proved. Since we have that we get that for all Hence, I know that this is very loose bound on the given sum. How can I possibly improve this? Please give any hints or suggestions. Thanks for all your inputs in advance.","a_n 1=a_1 \le a_2 \le a_3 \le \cdots \le a_n. a_{i+1}-a_i \le \sqrt{a_i}, 1 \le i <n. \sum_{i=1}^{n-1}\frac{a_{i+1}-a_i}{a_i} \le 2 \log_2 n. \sum_{i=1}^{n-1}\frac{a_{i+1}-a_i}{a_i} \le \sum_{i=1}^{n-1}\frac{\sqrt{a_i}}{a_i} = \sum_{i=1}^{n-1}\frac{1}{\sqrt{a_i}}. 1=a_1 \le a_2 \le a_3 \le \cdots \le a_n, \frac{1}{\sqrt{a_i}} \le 1 1 \le i <n. \sum_{i=1}^{n-1}\frac{a_{i+1}-a_i}{a_i} \le  \sum_{i=1}^{n-1}\frac{1}{\sqrt{a_i}} \le n-1.","['real-analysis', 'sequences-and-series', 'inequality']"
17,Condition about functional equation has a solution,Condition about functional equation has a solution,,"Problem : Let $P(x, y)$ be a polynomial respect to $x, y$ . Find a condition about $P$ where functional equation of $f$ $$f(x+y)=f(x)+f(y)+P(x,y)$$ has a solution. (Where $f\colon \mathbb{R}\to\mathbb{R}$ be a differentiable function.) This problem came from some functional equation problems below : P1 : Find a differentiable $f$ which satisfies $$f(x+y)=f(x)+f(y)+2xy$$ S1 : Set $x=y=0$ then we get $f(0)=0$ . From definition of derivative, $$f'(x)=\lim_{y\to 0}\frac{f(x+y)-f(x)}{y} = \lim_{y\to 0} \frac{f(y)+2xy}{y} = 2x+f'(0)$$ This implies $$f(x)=x^2 + cx$$ for some constant $c$ . Plugging this to original functional equation, $$\text{LHS} = x^2 + 2xy + y^2 + cx + cy$$ $$\text{RHS} = x^2 + y^2 + cx + cy + 2xy$$ which are same, so we find a solution of original functional equation. P2 : Find a differentiable $f$ which satisfies $$f(x+y)=f(x)+f(y)+3x^2y$$ S2 : Similar way, $f(0)=0$ and from definition of derivative, $$f'(x)=f'(0)+3x^2$$ which implies $$f(x)=x^3 + cx$$ for some constant c. However If we plug this to original functional equation $$\text{LHS} = x^3 + 3x^2y + 3xy^2 + y^3 + cx + cy$$ $$\text{RHS} = x^3 + 3x^2y + y^3 +cx + cy$$ which are not same. So, I want to know a condition of $P(x, y)$ which makes a functional equation of $f$ (which likes below form) has a solution : $$f(x+y)=f(x)+f(y)+P(x,y)$$ Where $f$ is a differentiable function. I find that WLOG $P(0, 0) = 0$ Because, if $P(0, 0) = a \neq 0$ , add $a$ both side then we can write as $$(f(x+y)+a) = (f(x)+a) + (f(y)+a) + Q(x, y)$$ Where $Q(x, y)$ is a polynomial respect to $x, y$ which satisfies $Q(0, 0) = 0$ . Substitution $g(x) = f(x)+a$ changes this equation to : $$g(x+y)=g(x)+g(y)+Q(x, y)$$ So I can assume that WLOG $P(0,0)=0$ . Added Conjecture : $P(x,y)$ must be constant or middle term(Cross -terms) of binomial expansion, i.e. $$ (x+y)^2 = x^2 + y^2 + \mathbf{2xy}$$ or $$(x+y)^3 = x^3 + y^3 + \mathbf{3x^2y + 3xy^2}$$ I don't have a proof but I made some try and common thing of $P(x,y)$ is this. Thanks for your help!","Problem : Let be a polynomial respect to . Find a condition about where functional equation of has a solution. (Where be a differentiable function.) This problem came from some functional equation problems below : P1 : Find a differentiable which satisfies S1 : Set then we get . From definition of derivative, This implies for some constant . Plugging this to original functional equation, which are same, so we find a solution of original functional equation. P2 : Find a differentiable which satisfies S2 : Similar way, and from definition of derivative, which implies for some constant c. However If we plug this to original functional equation which are not same. So, I want to know a condition of which makes a functional equation of (which likes below form) has a solution : Where is a differentiable function. I find that WLOG Because, if , add both side then we can write as Where is a polynomial respect to which satisfies . Substitution changes this equation to : So I can assume that WLOG . Added Conjecture : must be constant or middle term(Cross -terms) of binomial expansion, i.e. or I don't have a proof but I made some try and common thing of is this. Thanks for your help!","P(x, y) x, y P f f(x+y)=f(x)+f(y)+P(x,y) f\colon \mathbb{R}\to\mathbb{R} f f(x+y)=f(x)+f(y)+2xy x=y=0 f(0)=0 f'(x)=\lim_{y\to 0}\frac{f(x+y)-f(x)}{y} = \lim_{y\to 0} \frac{f(y)+2xy}{y} = 2x+f'(0) f(x)=x^2 + cx c \text{LHS} = x^2 + 2xy + y^2 + cx + cy \text{RHS} = x^2 + y^2 + cx + cy + 2xy f f(x+y)=f(x)+f(y)+3x^2y f(0)=0 f'(x)=f'(0)+3x^2 f(x)=x^3 + cx \text{LHS} = x^3 + 3x^2y + 3xy^2 + y^3 + cx + cy \text{RHS} = x^3 + 3x^2y + y^3 +cx + cy P(x, y) f f(x+y)=f(x)+f(y)+P(x,y) f P(0, 0) = 0 P(0, 0) = a \neq 0 a (f(x+y)+a) = (f(x)+a) + (f(y)+a) + Q(x, y) Q(x, y) x, y Q(0, 0) = 0 g(x) = f(x)+a g(x+y)=g(x)+g(y)+Q(x, y) P(0,0)=0 P(x,y)  (x+y)^2 = x^2 + y^2 + \mathbf{2xy} (x+y)^3 = x^3 + y^3 + \mathbf{3x^2y + 3xy^2} P(x,y)","['real-analysis', 'calculus', 'polynomials', 'functional-equations']"
18,Do the terms of a divergent series eventually always become bigger than those of a convergent series?,Do the terms of a divergent series eventually always become bigger than those of a convergent series?,,"If $(a_n)_{n\geq 1}$ and $(b_n)_{n\geq 1}$ are decreasing, non-negative sequences of real numbers and $\sum_{n=1}^{\infty} a_n$ converges and $\sum_{n=1}^{\infty} b_n$ diverges, then is it always the case that there exists some positive integer $N$ such that for all $n\geq N$ we have $b_n \geq a_n$ ? I was originally trying to prove that this result is true as a lemma to prove that if $(a_n)_{n \geq 1}$ was positive and decreasing and $\sum_{n=1}^{\infty} a_n$ converges that $\lim_{n \to \infty} na_n=0$ . I originally wanted to prove the first statement as the second statement follows after taking $b_n = {1\over{n\ln(n)}}$ . I have since realized that there is another solution to the original question which doesn't involve this lemma, but I still am curious to know whether the lemma is true. I have figured out that the lemma is not true if you take away the requirement that both $a_n$ and $b_n$ are decreasing, but I believe with this restriction, the result should be true.","If and are decreasing, non-negative sequences of real numbers and converges and diverges, then is it always the case that there exists some positive integer such that for all we have ? I was originally trying to prove that this result is true as a lemma to prove that if was positive and decreasing and converges that . I originally wanted to prove the first statement as the second statement follows after taking . I have since realized that there is another solution to the original question which doesn't involve this lemma, but I still am curious to know whether the lemma is true. I have figured out that the lemma is not true if you take away the requirement that both and are decreasing, but I believe with this restriction, the result should be true.",(a_n)_{n\geq 1} (b_n)_{n\geq 1} \sum_{n=1}^{\infty} a_n \sum_{n=1}^{\infty} b_n N n\geq N b_n \geq a_n (a_n)_{n \geq 1} \sum_{n=1}^{\infty} a_n \lim_{n \to \infty} na_n=0 b_n = {1\over{n\ln(n)}} a_n b_n,"['real-analysis', 'calculus', 'sequences-and-series', 'alternative-proof']"
19,"Asymptotic of $_3F_2(1, \frac{1}{2}+d+n, 1+c+n; 1+2c, 2+2n;1)$ as $n\to \infty$",Asymptotic of  as,"_3F_2(1, \frac{1}{2}+d+n, 1+c+n; 1+2c, 2+2n;1) n\to \infty","Let $c,d$ be in a small neighborhood of $0$ ; I think the limit $$\begin{aligned}&\quad \lim_{n\to \infty} 4^{-n} n^{3c-d} {_3F_2}(1, \frac{1}{2}+d+n, 1+c+n; 1+2c, 2+2n;1) \\ &= \lim_{n\to \infty} 4^{-n} n^{3c-d} \sum_{k\geq 0} \frac{(1/2+d+n)_k (1+c+n)_k}{(1+2c)_k (2+2n)_k} := f(c,d) \end{aligned}$$ exists and is nonzero. I would like to find it explicitly in terms of $c,d$ . Question: how to find $f(c,d)$ ? Using integral representation of $_3F_2$ , we have (here $c>0$ ): $$\int_{[0,1]^2} (1-x)^{2c-1} y^c (1-y)^{-c} (1-xy)^{-1/2-d} \left(\frac{y(1-y)}{1-xy}\right)^n dxdy \sim \sqrt{\frac{\pi}{n}}\frac{1}{4c} n^{d-3c} f(c,d)$$ this seems amendable via Laplace's method: the maximum of $\frac{y(1-y)}{1-xy}$ in $[0,1]^2$ is $1$ ; however, this maximum occurs at boundary. I'm not so familiar with such techniques, but perhaps an expert on asymptotic analysis here can say something. Any suggestion is appreciated. When $c=0$ , we have $f(0,d) = \frac{2 \Gamma \left(\frac{1}{2}-d\right)}{\sqrt{\pi }}$ , because in this case, $_3F_2$ can be exactly evaluated in terms of gamma function.","Let be in a small neighborhood of ; I think the limit exists and is nonzero. I would like to find it explicitly in terms of . Question: how to find ? Using integral representation of , we have (here ): this seems amendable via Laplace's method: the maximum of in is ; however, this maximum occurs at boundary. I'm not so familiar with such techniques, but perhaps an expert on asymptotic analysis here can say something. Any suggestion is appreciated. When , we have , because in this case, can be exactly evaluated in terms of gamma function.","c,d 0 \begin{aligned}&\quad \lim_{n\to \infty} 4^{-n} n^{3c-d} {_3F_2}(1, \frac{1}{2}+d+n, 1+c+n; 1+2c, 2+2n;1) \\
&= \lim_{n\to \infty} 4^{-n} n^{3c-d} \sum_{k\geq 0} \frac{(1/2+d+n)_k (1+c+n)_k}{(1+2c)_k (2+2n)_k} := f(c,d)
\end{aligned} c,d f(c,d) _3F_2 c>0 \int_{[0,1]^2} (1-x)^{2c-1} y^c (1-y)^{-c} (1-xy)^{-1/2-d} \left(\frac{y(1-y)}{1-xy}\right)^n dxdy \sim \sqrt{\frac{\pi}{n}}\frac{1}{4c} n^{d-3c} f(c,d) \frac{y(1-y)}{1-xy} [0,1]^2 1 c=0 f(0,d) = \frac{2 \Gamma \left(\frac{1}{2}-d\right)}{\sqrt{\pi }} _3F_2","['real-analysis', 'sequences-and-series', 'limits', 'asymptotics', 'hypergeometric-function']"
20,Sum of subset converges to an irrational,Sum of subset converges to an irrational,,Let $(a_n)$ be a sequence such that $a_n>0$ for all $n$ and $\displaystyle \sum \limits _{n=1}^\infty a_n$ converges. Is it true that there exists some irrational number $0<\alpha <L$ and a subsequence $(a_{n_k})$ such that $\displaystyle \sum \limits _{k=1}^\infty a_{n_k}=\alpha$ ? Can we generalise this result and say that we can find such a subsequence for every number $0<\alpha <L$ large enough?,Let be a sequence such that for all and converges. Is it true that there exists some irrational number and a subsequence such that ? Can we generalise this result and say that we can find such a subsequence for every number large enough?,(a_n) a_n>0 n \displaystyle \sum \limits _{n=1}^\infty a_n 0<\alpha <L (a_{n_k}) \displaystyle \sum \limits _{k=1}^\infty a_{n_k}=\alpha 0<\alpha <L,"['real-analysis', 'sequences-and-series', 'summation']"
21,"Show that $f_n(x)$ converges uniformly to a continuous function on $[0,1 ]$",Show that  converges uniformly to a continuous function on,"f_n(x) [0,1 ]","Let $f_0(x): [0,1 ] \rightarrow \mathbb{R}$ , where $f_0(x)=x$ . Define $f_n(x)=-1 +\frac{1}{3} \int_0 ^1 \cos (3x^2+y^2+1) f_{n-1}(y) dy$ Show that $f_n(x)$ converges uniformly to a continuous function on $[0,1 ]$ . I thought of ways of solving this questions. First we want to find a function $f(x)$ that $f_n(x)$ converges to, then use the $\epsilon-\delta$ definition. But the integral part just makes it difficult to proceed this way. We can try the Weierestrass M-test, by first proving $f_n$ is continuous. Then use the Weierestrass M-test to prove uniform convergence. Then conclude that $f$ is continuous. But I had trouble finding a bound for the M-test. I think it's the recursive part that is making it difficult. But I'm sort of guessing $|f_n|\leq 1$ . But $\sum 1$ does not converge. Any help will be appreciated!","Let , where . Define Show that converges uniformly to a continuous function on . I thought of ways of solving this questions. First we want to find a function that converges to, then use the definition. But the integral part just makes it difficult to proceed this way. We can try the Weierestrass M-test, by first proving is continuous. Then use the Weierestrass M-test to prove uniform convergence. Then conclude that is continuous. But I had trouble finding a bound for the M-test. I think it's the recursive part that is making it difficult. But I'm sort of guessing . But does not converge. Any help will be appreciated!","f_0(x): [0,1 ] \rightarrow \mathbb{R} f_0(x)=x f_n(x)=-1 +\frac{1}{3} \int_0 ^1 \cos (3x^2+y^2+1) f_{n-1}(y) dy f_n(x) [0,1 ] f(x) f_n(x) \epsilon-\delta f_n f |f_n|\leq 1 \sum 1","['real-analysis', 'uniform-convergence']"
22,Closed form for definite integrals invovling Jacobi elliptic functions,Closed form for definite integrals invovling Jacobi elliptic functions,,"In a 1879 work, Glaisher proves the following closed forms $$\int_{0}^{K\left(k\right)}\log\left(\text{sn}\left(z;k\right)\right)dz=-\frac{1}{4}\pi K^{\prime}\left(k\right)-\frac{1}{2}K\left(k\right)\log\left(k\right)$$ $$\int_{0}^{K\left(k\right)}\log\left(\text{cn}\left(z;k\right)\right)dz=-\frac{1}{4}\pi K^{\prime}\left(k\right)+\frac{1}{2}K\left(k\right)\log\left(\frac{k}{k^{\prime}}\right)$$ $$\int_{0}^{K\left(k\right)}\log\left(\text{dn}\left(z;k\right)\right)dz=\frac{1}{2}K\left(k\right)\log\left(k^{\prime}\right)$$ where, $\text{sn}\left(z;k\right),\,\text{cn}\left(z;k\right),\,\text{dn}\left(z;k\right)$ are the Jacobi elliptic functions, $K(k)$ is the complete elliptic integral of the first kind and, as usual, $K^{\prime}(k)=K(k^{\prime})$ , where $k^{\prime}=\sqrt{1-k^{2}}.$ For the proof he use a product formula for the elliptic functions; I tried to understand what he did but the steps don't have many explanations and therefore I struggle to understand how to prove these identities. Question 1. How we can prove the previous identities? This is the link to the paper of Glaisher: https://royalsocietypublishing.org/doi/pdf/10.1098/rspl.1879.0056 I need to understand these identities because I would like to find a closed form for the following definite integrals: $$\int_{0}^{K\left(k\right)/2}\log\left(\text{sn}\left(z;k\right)\right)dz,\,\int_{0}^{K\left(k\right)/2}\log\left(\text{cn}\left(z;k\right)\right)dz,\,\int_{0}^{K\left(k\right)/2}\log\left(\text{dn}\left(z;k\right)\right)dz\tag{1}$$ Question 2. Is it possible to evaluate in a closed form (in the sense of Glaisher) the integrals in $(1)$ ? I tried some identites, like half argument formulas, hoping to fall back into one of the cases already considered by Glaisher but it seems that this approach does not work. Thank you.","In a 1879 work, Glaisher proves the following closed forms where, are the Jacobi elliptic functions, is the complete elliptic integral of the first kind and, as usual, , where For the proof he use a product formula for the elliptic functions; I tried to understand what he did but the steps don't have many explanations and therefore I struggle to understand how to prove these identities. Question 1. How we can prove the previous identities? This is the link to the paper of Glaisher: https://royalsocietypublishing.org/doi/pdf/10.1098/rspl.1879.0056 I need to understand these identities because I would like to find a closed form for the following definite integrals: Question 2. Is it possible to evaluate in a closed form (in the sense of Glaisher) the integrals in ? I tried some identites, like half argument formulas, hoping to fall back into one of the cases already considered by Glaisher but it seems that this approach does not work. Thank you.","\int_{0}^{K\left(k\right)}\log\left(\text{sn}\left(z;k\right)\right)dz=-\frac{1}{4}\pi K^{\prime}\left(k\right)-\frac{1}{2}K\left(k\right)\log\left(k\right) \int_{0}^{K\left(k\right)}\log\left(\text{cn}\left(z;k\right)\right)dz=-\frac{1}{4}\pi K^{\prime}\left(k\right)+\frac{1}{2}K\left(k\right)\log\left(\frac{k}{k^{\prime}}\right) \int_{0}^{K\left(k\right)}\log\left(\text{dn}\left(z;k\right)\right)dz=\frac{1}{2}K\left(k\right)\log\left(k^{\prime}\right) \text{sn}\left(z;k\right),\,\text{cn}\left(z;k\right),\,\text{dn}\left(z;k\right) K(k) K^{\prime}(k)=K(k^{\prime}) k^{\prime}=\sqrt{1-k^{2}}. \int_{0}^{K\left(k\right)/2}\log\left(\text{sn}\left(z;k\right)\right)dz,\,\int_{0}^{K\left(k\right)/2}\log\left(\text{cn}\left(z;k\right)\right)dz,\,\int_{0}^{K\left(k\right)/2}\log\left(\text{dn}\left(z;k\right)\right)dz\tag{1} (1)","['real-analysis', 'definite-integrals', 'closed-form', 'elliptic-integrals', 'elliptic-functions']"
23,"Upper and lower bounds for $\int_0^\infty e^{-u(u^\epsilon -1)}\,du$",Upper and lower bounds for,"\int_0^\infty e^{-u(u^\epsilon -1)}\,du","I am interested in reasonable to obtain upper and lower bounds, as well as techniques used in doing so, for $\int_0^\infty e^{-u(u^\epsilon -1)}\,du$ for epsilon in some right-neighborhood of zero, i.e. valid for all $0<\epsilon<\delta$ with $\delta>0$ . I believe I was able to prove the integral is $\mathcal{O}(1/\epsilon)$ as $\epsilon\to0^+$ by using the fact that $u^\epsilon -1\ge \epsilon\log u$ and then $\int_e^\infty e^{-\epsilon u\log u}\,du\le\int_e^\infty e^{-\epsilon u}\,du$ but didn't get much further. Edit: So it seems that thanks River Li's comment, we now have the bounds $$\frac{c}{\sqrt{\epsilon}}<\int_0^\infty e^{-u(u^\epsilon -1)}\,du < \frac{C}{\epsilon}$$ for some constant $C, c$ in some right neighborhood of zero. Can we do better?","I am interested in reasonable to obtain upper and lower bounds, as well as techniques used in doing so, for for epsilon in some right-neighborhood of zero, i.e. valid for all with . I believe I was able to prove the integral is as by using the fact that and then but didn't get much further. Edit: So it seems that thanks River Li's comment, we now have the bounds for some constant in some right neighborhood of zero. Can we do better?","\int_0^\infty e^{-u(u^\epsilon -1)}\,du 0<\epsilon<\delta \delta>0 \mathcal{O}(1/\epsilon) \epsilon\to0^+ u^\epsilon -1\ge \epsilon\log u \int_e^\infty e^{-\epsilon u\log u}\,du\le\int_e^\infty e^{-\epsilon u}\,du \frac{c}{\sqrt{\epsilon}}<\int_0^\infty e^{-u(u^\epsilon -1)}\,du < \frac{C}{\epsilon} C, c","['real-analysis', 'inequality', 'definite-integrals', 'upper-lower-bounds']"
24,"Is there a measurable function from $[0,1]$ to $ω_1$?",Is there a measurable function from  to ?,"[0,1] ω_1","Does there exist a measurable function from $[0,1]$ (with the Lebesgue measure) to $ω_1$ that induces the Dieudonné measure? Definitions: $ω_1$ is the set of all countable ordinals, equipped with its Borel $σ$ -algebra. The Dieudonné measure $\nu$ assigns measure $1$ to each subset of $ω_1$ that contains an unbounded closed set. For a measurable function $f : [0,1] \to ω_1$ , the induced measure assigns measure $\mu(f^{-1}(E))$ to each measurable set $E ⊆ ω_1$ . So to restate the question, in two parts: first, is there any measurable surjection from $[0,1]$ to the set of countable ordinals? Second, is there such a function $f$ such that for each Borel set of countable ordinals $E$ that contains an unbounded closed set, $\mu(f^{1}(E)) = 1$ ? Intuitively, it doesn't seem like this should be possible. For one thing, $f$ would have to map every subset $[0,1]$ with a measure other than $0$ or $1$ to a non-Borel subset of ordinals, which seems pretty pathological. The Lebesgue measure is non-atomic and regular, while the Dieudonné measure is neither. But I don't think I know enough analysis to prove it either way.","Does there exist a measurable function from (with the Lebesgue measure) to that induces the Dieudonné measure? Definitions: is the set of all countable ordinals, equipped with its Borel -algebra. The Dieudonné measure assigns measure to each subset of that contains an unbounded closed set. For a measurable function , the induced measure assigns measure to each measurable set . So to restate the question, in two parts: first, is there any measurable surjection from to the set of countable ordinals? Second, is there such a function such that for each Borel set of countable ordinals that contains an unbounded closed set, ? Intuitively, it doesn't seem like this should be possible. For one thing, would have to map every subset with a measure other than or to a non-Borel subset of ordinals, which seems pretty pathological. The Lebesgue measure is non-atomic and regular, while the Dieudonné measure is neither. But I don't think I know enough analysis to prove it either way.","[0,1] ω_1 ω_1 σ \nu 1 ω_1 f : [0,1] \to ω_1 \mu(f^{-1}(E)) E ⊆ ω_1 [0,1] f E \mu(f^{1}(E)) = 1 f [0,1] 0 1","['real-analysis', 'measure-theory', 'ordinals', 'borel-measures']"
25,Different definitions of the archimedean property,Different definitions of the archimedean property,,"In some textbooks I have seen the archimedean property defined as: for some positive real $x$ , real number $y$ , there exists a natural $n$ such that $nx>y$ . In other textbooks the archimedean property is defined as: for any real $x$ , we can find a natural $n$ such that $x \leq n$ I'm guessing I can prove that the two definitions are equivalent, but its just my guess, so if my reasoning is incorrect please correct me. Also if there are any other ways of looking at how the two definitions are equivalent I would love to learn more. My attempt at proving: To see how the first definition implies the second, we let $x=1$ in the first definition, then we have $n>y$ . But then this means for any real $y$ , we can find n such that n satisfies $n \geq y$ (since n satisfies $n>y$ ), which is exactly the second definition. To see how the the second definition implies the first, for any positive real $x$ , real $y$ , we pick a real $z$ such that $z=y/x$ . Then we can find $n$ such that $n \geq z=y/x$ . Since $n+1>n$ , we have $n+1>y/x$ . Since $x>0$ , we multiply both sides of the inequality and it does not change the order, to get $x(n+1)>y$ . Since $n+1$ is a positive integer, this is equivalent to the first definition.","In some textbooks I have seen the archimedean property defined as: for some positive real , real number , there exists a natural such that . In other textbooks the archimedean property is defined as: for any real , we can find a natural such that I'm guessing I can prove that the two definitions are equivalent, but its just my guess, so if my reasoning is incorrect please correct me. Also if there are any other ways of looking at how the two definitions are equivalent I would love to learn more. My attempt at proving: To see how the first definition implies the second, we let in the first definition, then we have . But then this means for any real , we can find n such that n satisfies (since n satisfies ), which is exactly the second definition. To see how the the second definition implies the first, for any positive real , real , we pick a real such that . Then we can find such that . Since , we have . Since , we multiply both sides of the inequality and it does not change the order, to get . Since is a positive integer, this is equivalent to the first definition.",x y n nx>y x n x \leq n x=1 n>y y n \geq y n>y x y z z=y/x n n \geq z=y/x n+1>n n+1>y/x x>0 x(n+1)>y n+1,"['real-analysis', 'proof-writing', 'solution-verification', 'real-numbers']"
26,"If $f(a)=f(b)$ then $\forall\alpha$, $f(x+\alpha)=f(x)$ for some $x$","If  then ,  for some",f(a)=f(b) \forall\alpha f(x+\alpha)=f(x) x,"Given that $f$ is continuous in $[a,b]$ with $f(a)=f(b)$ . Prove or disprove: given any $\alpha\in[0,{b-a\over2}]$ , there exists some $x\in[a,b]$ such that $f(x+\alpha)=f(x)$ . If this were true then I'd suppose we prove by contradiction. Suppose $f(x)\ne f(x+\alpha_0)$ for some $\alpha_0\in[0,{b-a\over2}]$ for all $x$ , then since $f$ is continuous, suppose w.l.o.g. $f(x+\alpha_0)-f(x)>0,\forall x\in[a,b]$ . How should I proceed?","Given that is continuous in with . Prove or disprove: given any , there exists some such that . If this were true then I'd suppose we prove by contradiction. Suppose for some for all , then since is continuous, suppose w.l.o.g. . How should I proceed?","f [a,b] f(a)=f(b) \alpha\in[0,{b-a\over2}] x\in[a,b] f(x+\alpha)=f(x) f(x)\ne f(x+\alpha_0) \alpha_0\in[0,{b-a\over2}] x f f(x+\alpha_0)-f(x)>0,\forall x\in[a,b]","['real-analysis', 'analysis', 'continuity']"
27,Find this somewhat unpleasant limit,Find this somewhat unpleasant limit,,"I am solving a problem and after quite some computations and almost 1 day spent on it, I decided to ask here. It all boils down to finding this limit $$\lim_{x \to 0+}C(x) = ?$$ where $$C(x) = \frac{x-\sqrt{x^2+1}\cdot \ln \big(\sqrt{x^2+1} + x \big)}{2x^2\sqrt{x^2+1}\cdot \ln\big(\sqrt{x^2+1} + x \big)}$$ I applied L'Hôpital's rule a few times to get to here. Now WA says this limit is $$-1/6$$ and this is correct. So I am trying to compute this and derive that $$\lim_{x \to 0+}C(x) = -1/6$$ by hand. And after studying the sub-expressions, I can see this limit is of the kind $0/0$ but if I try to apply L'Hôpital's rule again to the expression $C(x)$ , it doesn't get simpler, it gets more complicated. So there must be some trick here. Maybe I need to divide the numerator and denominator by some expression. I tried that too a few times but I don't succeed at making it simpler. Or... is this problem not solvable at all just by using L'Hôpital's rule? But I don't see what other theory to apply here. Any help or hint as to how to proceed? Side note: Here is the original problem which led me to this expression $C(x)$ . It asks us to find this limit. $$\lim_{x \to 0+} \left(\frac{\ln(x+\sqrt{1+x^2})}{x}\right)^\frac{1}{x^2} = ?$$","I am solving a problem and after quite some computations and almost 1 day spent on it, I decided to ask here. It all boils down to finding this limit where I applied L'Hôpital's rule a few times to get to here. Now WA says this limit is and this is correct. So I am trying to compute this and derive that by hand. And after studying the sub-expressions, I can see this limit is of the kind but if I try to apply L'Hôpital's rule again to the expression , it doesn't get simpler, it gets more complicated. So there must be some trick here. Maybe I need to divide the numerator and denominator by some expression. I tried that too a few times but I don't succeed at making it simpler. Or... is this problem not solvable at all just by using L'Hôpital's rule? But I don't see what other theory to apply here. Any help or hint as to how to proceed? Side note: Here is the original problem which led me to this expression . It asks us to find this limit.",\lim_{x \to 0+}C(x) = ? C(x) = \frac{x-\sqrt{x^2+1}\cdot \ln \big(\sqrt{x^2+1} + x \big)}{2x^2\sqrt{x^2+1}\cdot \ln\big(\sqrt{x^2+1} + x \big)} -1/6 \lim_{x \to 0+}C(x) = -1/6 0/0 C(x) C(x) \lim_{x \to 0+} \left(\frac{\ln(x+\sqrt{1+x^2})}{x}\right)^\frac{1}{x^2} = ?,"['real-analysis', 'calculus', 'limits']"
28,"Showing $\int_0^\infty\frac{Li_2\frac{(1-x^4)^2}{(1+x^4)^2}}{1+x^4}dx=\sqrt2\,\Re\left(\int_0^1\frac{Li_2\frac{(1+x^4)^2}{(1-x^4)^2}}{1+x^2}dx\right)$",Showing,"\int_0^\infty\frac{Li_2\frac{(1-x^4)^2}{(1+x^4)^2}}{1+x^4}dx=\sqrt2\,\Re\left(\int_0^1\frac{Li_2\frac{(1+x^4)^2}{(1-x^4)^2}}{1+x^2}dx\right)","A beautiful equality: $$\int_{0}^{\infty }\frac{\operatorname{Li}_{2}\left(\frac{(1-x^{4})^{2}}{(1+x^{4})^{2}}\right)}{1+x^{4}}dx=\sqrt{2} \Re \left(\int_{0}^{1 }\frac{\operatorname{Li}_{2}\left(\frac{(1+x^{4})^{2}}{(1-x^{4})^{2}}\right)}{1+x^{2}}dx\right)$$ This question was proposed by Sujeethan Balendran in RMM(Romanian Mathematical Magazine) I did by using complex number but i didn't by Real method by using $$\operatorname{Li}_{2}(a)=-a\int_{0}^{1}\frac{\ln x}{1-ax}dx$$ I believe some of you know some nice proofs of this, can you please share it with us?","A beautiful equality: This question was proposed by Sujeethan Balendran in RMM(Romanian Mathematical Magazine) I did by using complex number but i didn't by Real method by using I believe some of you know some nice proofs of this, can you please share it with us?",\int_{0}^{\infty }\frac{\operatorname{Li}_{2}\left(\frac{(1-x^{4})^{2}}{(1+x^{4})^{2}}\right)}{1+x^{4}}dx=\sqrt{2} \Re \left(\int_{0}^{1 }\frac{\operatorname{Li}_{2}\left(\frac{(1+x^{4})^{2}}{(1-x^{4})^{2}}\right)}{1+x^{2}}dx\right) \operatorname{Li}_{2}(a)=-a\int_{0}^{1}\frac{\ln x}{1-ax}dx,"['real-analysis', 'calculus', 'integration', 'complex-numbers']"
29,"Prove that $f(t)=0$ for almost every $t\in [0,1]$",Prove that  for almost every,"f(t)=0 t\in [0,1]","I would be glad if someone could help me to solve the following exercise. Let $f:[0,1]\to\Bbb R$ be a bounded measurable function such that $\int_0^1f(t)e^{nt}dt=0$ for every $n=0,1,2,\dots$ Prove that $f(t)=0$ for almost every $t\in [0,1]$ . I know that if $f$ is nonnegative then for $n=0$ the integral $\int_0^1f(t)dt=0$ implies that $f(t)=0$ for almost all $t\in[0,1]$ . Thanks!",I would be glad if someone could help me to solve the following exercise. Let be a bounded measurable function such that for every Prove that for almost every . I know that if is nonnegative then for the integral implies that for almost all . Thanks!,"f:[0,1]\to\Bbb R \int_0^1f(t)e^{nt}dt=0 n=0,1,2,\dots f(t)=0 t\in [0,1] f n=0 \int_0^1f(t)dt=0 f(t)=0 t\in[0,1]","['real-analysis', 'measure-theory']"
30,Conjecture with the CGM of a Circle,Conjecture with the CGM of a Circle,,"CGM = Continuous Geometric Mean. Heuristics and mathematics are described in: Subset as arithmetic mean of geometric means. Not really? A shortcut to the question as presented here is: $$ \operatorname{CGM}(\vec{r}) = \exp\left(-\!\!\!\!\!\!\int_0^1 \ln(\left|\vec{p}(t)-\vec{r}\right|^2)\,dt\right) $$ where $\,\vec{p}(t)\,$ is a circle in the plane and $\,\vec{r} = (x,y)\,$ is another point in the plane. A similar exercise for the circle as previously for a straight line segment leads to the following expression: $$ \operatorname{CGM}(\xi,\eta)= \exp\left( \frac{1}{2\pi}-\!\!\!\!\!\!\int_0^{2\pi}\ln\left|[\cos(t)-\xi]^2+[\sin(t)-\eta]^2\right|\,dt\right) $$ where dimensionless $\xi=x/R$ , $\eta=y/R$ ; $(x,y)=$ coordinates in the plane, $R=$ radius of circle. When this expression is fed into MAPLE (version 8), then surprisingly we get outcome one , independent of any $(x,y,R)$ values: > exp(int(ln((cos(t)-xi)^2+(sin(t)-eta)^2),t=0..2*Pi,'continuous')/(2*Pi)); 1 On the other hand trying to proceed by substituting polar coordinates: $$ x = r\cos(\phi) \quad ; \quad y = r\sin(\phi) \\ \exp\left(\frac{1}{2\pi}-\!\!\!\!\!\!\int_0^{2\pi} \ln\left|\left[\cos(t)-\frac{r}{R}\cos(\phi)\right]^2 +\left[\sin(t)-\frac{r}{R}\sin(\phi)\right]^2\right|\,dt\right) \quad \Longrightarrow \\ \operatorname{CGM}(\rho) = \exp\left(\frac{1}{2\pi}-\!\!\!\!\!\!\int_0^{2\pi} \ln\left|\rho^2+1-2\rho.\cos(t-\phi)\right|\,dt\right) \quad \mbox{with} \quad \rho=r/R $$ Because of circle symmetry we can choose $\,\phi=0\,$ without loss of generality. Care should be taken if a zero is present in the argument of the logarithm, resulting in a singularity at that place: $$ \rho^2+1-2\rho.\cos(t)=0 \quad \Longrightarrow \quad \rho = \cos(t)\pm\sqrt{\cos^2(t)-1} \\ \Longrightarrow \quad t = 0 \quad \mbox{and} \quad \rho = 1 $$ There are two approaches to this special case. The easiest one is to ignore the Cauchy principal value and remember that the special case represents the value zero, according to the heuristics in the referenced Q&A . The second approach is to accept the Cauchy principal value as being essential. With our numerical method, being the trapezium rule , integration at the interval $[0,\Delta t]$ is then calculated by: $$ \frac{f_1+f_2}{2}\Delta t \quad \mbox{with} \quad f_2 = \ln|2-2\cos(\Delta t)| \approx \ln|2-2(1-\Delta t^2/2)| = 2\ln|\Delta t| $$ On the other hand the integral at that place can be calculated more (or less) exactly: $$ \frac{f_1+f_2}{2}\Delta t \approx \int_0^{\Delta t}\ln|2-2\cos(t)|\,dt \approx \int_0^{\Delta t}2\ln|t|\,dt = 2(\Delta t\,\ln|\Delta t| - \Delta t) \\ \frac{f_1+2\ln|\Delta t|}{2}\Delta t \approx 2(\ln|\Delta t|-1)\Delta t \quad \Longrightarrow \quad f_1 \approx 2\ln|\Delta t|-4 $$ Having programmed all this (in Delphi Pascal) we get the following output. Mind the two bold printed values at $I(1.0)$ : the first one without and the second one with the Cauchy principal value. NUMERICALLY               CONJECTURED I(0.0) =  1.00000000000000E+0000 =  1.00000000000000E+0000 I(0.1) =  1.00000000000000E+0000 =  1.00000000000000E+0000 I(0.2) =  1.00000000000000E+0000 =  1.00000000000000E+0000 I(0.3) =  1.00000000000000E+0000 =  1.00000000000000E+0000 I(0.4) =  1.00000000000000E+0000 =  1.00000000000000E+0000 I(0.5) =  1.00000000000000E+0000 =  1.00000000000000E+0000 I(0.6) =  9.99999999999999E-0001 =  1.00000000000000E+0000 I(0.7) =  1.00000000000000E+0000 =  1.00000000000000E+0000 I(0.8) =  1.00000000000000E+0000 =  1.00000000000000E+0000 I(0.9) =  1.00000000000000E+0000 =  1.00000000000000E+0000 I(1.0) =  0.00000000000000E+0000 =  0.00000000000000E+0000 I(1.0) =  9.99967575938953E-0001 =  1.00000000000000E+0000 I(1.1) =  1.21000000000000E+0000 =  1.21000000000000E+0000 I(1.2) =  1.44000000000000E+0000 =  1.44000000000000E+0000 I(1.3) =  1.69000000000000E+0000 =  1.69000000000000E+0000 I(1.4) =  1.96000000000000E+0000 =  1.96000000000000E+0000 I(1.5) =  2.24999999999999E+0000 =  2.25000000000000E+0000 I(1.6) =  2.55999999999999E+0000 =  2.56000000000000E+0000 I(1.7) =  2.89000000000002E+0000 =  2.89000000000000E+0000 I(1.8) =  3.24000000000002E+0000 =  3.24000000000000E+0000 I(1.9) =  3.61000000000001E+0000 =  3.61000000000000E+0000 I(2.0) =  4.00000000000000E+0000 =  4.00000000000000E+0000 I(2.1) =  4.41000000000001E+0000 =  4.41000000000000E+0000 I(2.2) =  4.84000000000001E+0000 =  4.84000000000000E+0000 I(2.3) =  5.29000000000000E+0000 =  5.29000000000000E+0000 I(2.4) =  5.75999999999996E+0000 =  5.76000000000000E+0000 I(2.5) =  6.24999999999998E+0000 =  6.25000000000000E+0000 I(2.6) =  6.75999999999999E+0000 =  6.76000000000001E+0000 I(2.7) =  7.29000000000010E+0000 =  7.29000000000001E+0000 I(2.8) =  7.84000000000007E+0000 =  7.84000000000001E+0000 I(2.9) =  8.41000000000004E+0000 =  8.41000000000001E+0000 I(3.0) =  9.00000000000000E+0000 =  9.00000000000001E+0000 The numerical experiments give rise to the following conjecture (contradictory on purpose): $$ \operatorname{CGM}(\rho) = \begin{cases} 1 & \mbox{for} \quad \rho \le 1 \\ 0 & \mbox{for} \quad \rho = 1 \\ \rho^2 & \mbox{for} \quad \rho \ge 1 \end{cases} $$ Is there someone who can prove this conjecture analytically? I have seen something with complex analysis in sci.math a long time ago (2008) but didn't quite understand the argument. CGM for a Sphere In view of further development of the theory, it seems to be advantageous to define the Continuous Geometric Mean slightly different, namely as: $$ \operatorname{CGM}(\vec{r}) = \exp\left(- -\!\!\!\!\!\!\int_0^1 \ln(\left|\vec{p}(t)-\vec{r}\right|^2)\,dt\right) $$ For our unit circle (with Cauchy principal value) then we have instead: $$ \operatorname{CGM}(\rho) = \begin{cases} 1 & \mbox{for} \quad \rho \le 1 \\ 1/\rho^2 & \mbox{for} \quad \rho \ge 1 \end{cases} $$ We seek to generalize the Continuous Geometric Mean for a circle to the same for the surface of a unit sphere: $$ \operatorname{CGM}(\vec{r}) = \exp\left(-\iint \ln(\left|\vec{p}-\vec{r}\right|^2)\,dA/(4\pi)\right) $$ Expressed in spherical coordinates and specializing (without loss of generality) for $\,\vec{r} = (0,0,\rho)\,$ : $$ \vec{p}-\vec{r} = \begin{bmatrix} \sin(\theta)\cos(\phi) \\ \sin(\theta)\sin(\phi) \\ \cos(\theta)-\rho \end{bmatrix} \quad ; \quad dA = \sin(\theta)\,d\theta\,d\phi \\ \left|\vec{p}-\vec{r}\right|^2 = \sin^2(\theta)+\left[\,\cos(\theta)-\rho\,\right]^2 = 1-2\rho\cos(\theta)+\rho^2 \\ \iint \ln(\left|\vec{p}-\vec{r}\right|^2)\,dA / (4\pi)= \frac{2\pi}{4\pi} -\!\!\!\!\!\!\!\int_0^\pi \ln\left|\rho^2+1-2\rho\,\cos(\theta)\right|\,\sin(\theta)\,d\theta \\ \Longrightarrow \quad \operatorname{CGM}(\rho) = \exp\left(-\frac{1}{2} -\!\!\!\!\!\!\!\int_0^{\pi} \ln\left|\rho^2+1-2\rho\,\cos(\theta)\right|\,\sin(\theta)\,d\theta\right) $$ Surprisingly enough, integration is much easier in 3-D, when compared with the 2-D case. Substitution of $\,t = \cos(\theta)\,$ gives a short route to the solution: $$ -\!\!\!\!\!\!\int_0^{\pi} \ln\left|\rho^2+1-2\rho\,\cos(\theta)\right|\,\sin(\theta)\,d\theta = -\!\!\!\!\!\!\int_{-1}^{+1}\ln\left|\rho^2+1-2\rho\,t\right|\,dt = \\ \frac{1}{2\rho}\left[\;u\ln|u|-u\;\right]_{u=1+\rho^2-2\rho}^{u=1+\rho^2+2\rho} \quad \Longrightarrow \\ \operatorname{CGM}(\rho) = \exp\left(-\left[(1+\rho)^2\ln\left|(1+\rho)^2\right|-(1-\rho)^2\ln\left|(1-\rho)^2\right|-4\rho\right]/(4\rho)\right) $$ If we make a graph of the two functions - red for 2-D, black for 3-D - then there is another surprise: the graphs coincide for large values of the normed radius $\rho$ . Confirmation is found with MAPLE, series expansion for $q=1/\rho\to 0$ : > f(q) := exp(-((1+1/q)^2*ln((1+1/q)^2)-(1-1/q)^2*ln((1-1/q)^2)-4/q)/(4/q)); > series(f(q),q=0); Output: $$ ({q}^{2}-{\frac {1}{3}}{q}^{4}+O \left( {q}^{6} \right) ) $$","CGM = Continuous Geometric Mean. Heuristics and mathematics are described in: Subset as arithmetic mean of geometric means. Not really? A shortcut to the question as presented here is: where is a circle in the plane and is another point in the plane. A similar exercise for the circle as previously for a straight line segment leads to the following expression: where dimensionless , ; coordinates in the plane, radius of circle. When this expression is fed into MAPLE (version 8), then surprisingly we get outcome one , independent of any values: > exp(int(ln((cos(t)-xi)^2+(sin(t)-eta)^2),t=0..2*Pi,'continuous')/(2*Pi)); 1 On the other hand trying to proceed by substituting polar coordinates: Because of circle symmetry we can choose without loss of generality. Care should be taken if a zero is present in the argument of the logarithm, resulting in a singularity at that place: There are two approaches to this special case. The easiest one is to ignore the Cauchy principal value and remember that the special case represents the value zero, according to the heuristics in the referenced Q&A . The second approach is to accept the Cauchy principal value as being essential. With our numerical method, being the trapezium rule , integration at the interval is then calculated by: On the other hand the integral at that place can be calculated more (or less) exactly: Having programmed all this (in Delphi Pascal) we get the following output. Mind the two bold printed values at : the first one without and the second one with the Cauchy principal value. NUMERICALLY               CONJECTURED I(0.0) =  1.00000000000000E+0000 =  1.00000000000000E+0000 I(0.1) =  1.00000000000000E+0000 =  1.00000000000000E+0000 I(0.2) =  1.00000000000000E+0000 =  1.00000000000000E+0000 I(0.3) =  1.00000000000000E+0000 =  1.00000000000000E+0000 I(0.4) =  1.00000000000000E+0000 =  1.00000000000000E+0000 I(0.5) =  1.00000000000000E+0000 =  1.00000000000000E+0000 I(0.6) =  9.99999999999999E-0001 =  1.00000000000000E+0000 I(0.7) =  1.00000000000000E+0000 =  1.00000000000000E+0000 I(0.8) =  1.00000000000000E+0000 =  1.00000000000000E+0000 I(0.9) =  1.00000000000000E+0000 =  1.00000000000000E+0000 I(1.0) =  0.00000000000000E+0000 =  0.00000000000000E+0000 I(1.0) =  9.99967575938953E-0001 =  1.00000000000000E+0000 I(1.1) =  1.21000000000000E+0000 =  1.21000000000000E+0000 I(1.2) =  1.44000000000000E+0000 =  1.44000000000000E+0000 I(1.3) =  1.69000000000000E+0000 =  1.69000000000000E+0000 I(1.4) =  1.96000000000000E+0000 =  1.96000000000000E+0000 I(1.5) =  2.24999999999999E+0000 =  2.25000000000000E+0000 I(1.6) =  2.55999999999999E+0000 =  2.56000000000000E+0000 I(1.7) =  2.89000000000002E+0000 =  2.89000000000000E+0000 I(1.8) =  3.24000000000002E+0000 =  3.24000000000000E+0000 I(1.9) =  3.61000000000001E+0000 =  3.61000000000000E+0000 I(2.0) =  4.00000000000000E+0000 =  4.00000000000000E+0000 I(2.1) =  4.41000000000001E+0000 =  4.41000000000000E+0000 I(2.2) =  4.84000000000001E+0000 =  4.84000000000000E+0000 I(2.3) =  5.29000000000000E+0000 =  5.29000000000000E+0000 I(2.4) =  5.75999999999996E+0000 =  5.76000000000000E+0000 I(2.5) =  6.24999999999998E+0000 =  6.25000000000000E+0000 I(2.6) =  6.75999999999999E+0000 =  6.76000000000001E+0000 I(2.7) =  7.29000000000010E+0000 =  7.29000000000001E+0000 I(2.8) =  7.84000000000007E+0000 =  7.84000000000001E+0000 I(2.9) =  8.41000000000004E+0000 =  8.41000000000001E+0000 I(3.0) =  9.00000000000000E+0000 =  9.00000000000001E+0000 The numerical experiments give rise to the following conjecture (contradictory on purpose): Is there someone who can prove this conjecture analytically? I have seen something with complex analysis in sci.math a long time ago (2008) but didn't quite understand the argument. CGM for a Sphere In view of further development of the theory, it seems to be advantageous to define the Continuous Geometric Mean slightly different, namely as: For our unit circle (with Cauchy principal value) then we have instead: We seek to generalize the Continuous Geometric Mean for a circle to the same for the surface of a unit sphere: Expressed in spherical coordinates and specializing (without loss of generality) for : Surprisingly enough, integration is much easier in 3-D, when compared with the 2-D case. Substitution of gives a short route to the solution: If we make a graph of the two functions - red for 2-D, black for 3-D - then there is another surprise: the graphs coincide for large values of the normed radius . Confirmation is found with MAPLE, series expansion for : > f(q) := exp(-((1+1/q)^2*ln((1+1/q)^2)-(1-1/q)^2*ln((1-1/q)^2)-4/q)/(4/q)); > series(f(q),q=0); Output:","
\operatorname{CGM}(\vec{r}) = \exp\left(-\!\!\!\!\!\!\int_0^1 \ln(\left|\vec{p}(t)-\vec{r}\right|^2)\,dt\right)
 \,\vec{p}(t)\, \,\vec{r} = (x,y)\, 
\operatorname{CGM}(\xi,\eta)=
\exp\left( \frac{1}{2\pi}-\!\!\!\!\!\!\int_0^{2\pi}\ln\left|[\cos(t)-\xi]^2+[\sin(t)-\eta]^2\right|\,dt\right)
 \xi=x/R \eta=y/R (x,y)= R= (x,y,R) 
x = r\cos(\phi) \quad ; \quad y = r\sin(\phi) \\
\exp\left(\frac{1}{2\pi}-\!\!\!\!\!\!\int_0^{2\pi} \ln\left|\left[\cos(t)-\frac{r}{R}\cos(\phi)\right]^2
+\left[\sin(t)-\frac{r}{R}\sin(\phi)\right]^2\right|\,dt\right)
\quad \Longrightarrow \\ \operatorname{CGM}(\rho) =
\exp\left(\frac{1}{2\pi}-\!\!\!\!\!\!\int_0^{2\pi} \ln\left|\rho^2+1-2\rho.\cos(t-\phi)\right|\,dt\right)
\quad \mbox{with} \quad \rho=r/R
 \,\phi=0\, 
\rho^2+1-2\rho.\cos(t)=0 \quad \Longrightarrow \quad \rho = \cos(t)\pm\sqrt{\cos^2(t)-1}
\\ \Longrightarrow \quad t = 0 \quad \mbox{and} \quad \rho = 1
 [0,\Delta t] 
\frac{f_1+f_2}{2}\Delta t \quad \mbox{with} \quad f_2 = \ln|2-2\cos(\Delta t)| \approx \ln|2-2(1-\Delta t^2/2)| = 2\ln|\Delta t|
 
\frac{f_1+f_2}{2}\Delta t \approx \int_0^{\Delta t}\ln|2-2\cos(t)|\,dt \approx
\int_0^{\Delta t}2\ln|t|\,dt = 2(\Delta t\,\ln|\Delta t| - \Delta t) \\
\frac{f_1+2\ln|\Delta t|}{2}\Delta t \approx 2(\ln|\Delta t|-1)\Delta t \quad \Longrightarrow \quad f_1 \approx 2\ln|\Delta t|-4
 I(1.0) 
\operatorname{CGM}(\rho) = \begin{cases} 1 & \mbox{for} \quad \rho \le 1 \\ 0 & \mbox{for} \quad \rho = 1 \\
\rho^2 & \mbox{for} \quad \rho \ge 1 \end{cases}
 
\operatorname{CGM}(\vec{r}) = \exp\left(- -\!\!\!\!\!\!\int_0^1 \ln(\left|\vec{p}(t)-\vec{r}\right|^2)\,dt\right)
 
\operatorname{CGM}(\rho) = \begin{cases} 1 & \mbox{for} \quad \rho \le 1 \\ 1/\rho^2 & \mbox{for} \quad \rho \ge 1 \end{cases}
 
\operatorname{CGM}(\vec{r}) = \exp\left(-\iint \ln(\left|\vec{p}-\vec{r}\right|^2)\,dA/(4\pi)\right)
 \,\vec{r} = (0,0,\rho)\, 
\vec{p}-\vec{r} = \begin{bmatrix} \sin(\theta)\cos(\phi) \\ \sin(\theta)\sin(\phi) \\ \cos(\theta)-\rho \end{bmatrix}
\quad ; \quad dA = \sin(\theta)\,d\theta\,d\phi \\
\left|\vec{p}-\vec{r}\right|^2 = \sin^2(\theta)+\left[\,\cos(\theta)-\rho\,\right]^2 = 1-2\rho\cos(\theta)+\rho^2 \\
\iint \ln(\left|\vec{p}-\vec{r}\right|^2)\,dA / (4\pi)=
\frac{2\pi}{4\pi} -\!\!\!\!\!\!\!\int_0^\pi \ln\left|\rho^2+1-2\rho\,\cos(\theta)\right|\,\sin(\theta)\,d\theta
\\ \Longrightarrow \quad
\operatorname{CGM}(\rho) = \exp\left(-\frac{1}{2} -\!\!\!\!\!\!\!\int_0^{\pi} \ln\left|\rho^2+1-2\rho\,\cos(\theta)\right|\,\sin(\theta)\,d\theta\right)
 \,t = \cos(\theta)\, 
-\!\!\!\!\!\!\int_0^{\pi} \ln\left|\rho^2+1-2\rho\,\cos(\theta)\right|\,\sin(\theta)\,d\theta =
-\!\!\!\!\!\!\int_{-1}^{+1}\ln\left|\rho^2+1-2\rho\,t\right|\,dt = \\
\frac{1}{2\rho}\left[\;u\ln|u|-u\;\right]_{u=1+\rho^2-2\rho}^{u=1+\rho^2+2\rho} \quad \Longrightarrow \\
\operatorname{CGM}(\rho) = \exp\left(-\left[(1+\rho)^2\ln\left|(1+\rho)^2\right|-(1-\rho)^2\ln\left|(1-\rho)^2\right|-4\rho\right]/(4\rho)\right)
 \rho q=1/\rho\to 0 
({q}^{2}-{\frac {1}{3}}{q}^{4}+O \left( {q}^{6} \right) )
","['real-analysis', 'integration', 'complex-analysis']"
31,Understanding Rudin Theorem 3.3 (d),Understanding Rudin Theorem 3.3 (d),,"For reference, here is the full Theorem 3.3 (d) from Rudin. Suppose $\{s_n\}$ , $\{t_n\}$ are complex sequences, and $\lim\limits_{n \to \infty} s_n = s$ , $\lim\limits_{n \to \infty} t_n = t$ . Then (a) $\lim\limits_{n \to \infty} \left(s_n + t_n\right) = s + t$ ; (b) $\lim\limits_{n \to \infty} cs_n = cs$ , $\lim\limits_{n \to \infty} \left(c + s_n\right) = c + s$ , for any number $c$ ; (c) $\lim\limits_{n \to \infty} s_n t_n = st$ ; (d) $\lim\limits_{n \to \infty} \frac{1}{s_n} = \frac{1}{s}$ , provided $s_n \neq 0$ ( $n = 1, 2, 3, \ldots$ ), and $s \neq 0$ . There's one key step in the proof of (d) that I don't understand. First, Rudin uses convergence of $s_n$ to find an $m \in \mathbb{N}$ so that for all $n \geq m$ , we have $|s_n - s| < \frac{1}{2} |s|$ . He then asserts that for $n \geq m$ , we have $$  |s_n| > \frac{1}{2} |s|. $$ This is a very important step, but I cannot follow it. I've tried contradiction and the triangle inequality, but I can't get the inequality signs to line up. For example, I tried (for $n \geq m$ ), $$  |s_n| = |(s_n - s) + s| \leq |s_n - s| + |s| < \frac{1}{2} |s| + |s| = \frac{3}{2} |s|. $$ It seems as though I've bounded $|s_n|$ ""in the opposite direction."" If I knew $s$ were positive, expanding the absolute values might work, but we only know it's non-zero. The rest of the proof looks pretty straightforward to me, with one slight doubt. He asserts the existence of an $N$ (I don't know why he requires $N > m$ when he could just take the maximum of $N$ and $m$ ; does this make a difference?) so that $n \geq N$ implies $|s_n - s| < \frac{1}{2} |s|^2 \epsilon$ . As $|s_n| > \frac{1}{2} |s|$ , we have $\frac{1}{|s_n|} < \frac{2}{|s|}$ . We then have: \begin{align*} \left \lvert \frac{1}{s_n} - \frac{1}{s} \right \rvert & = \left \lvert \frac{s - s_n}{s_n \cdot s} \right \rvert \\ & = \frac{|s - s_n|}{|s||s_n|} \\ & < \frac{2|s - s_n|}{|s|^2} \\ & < \frac{2}{|s|^2} \cdot \frac{1}{2} |s|^2 \epsilon \\ & = \epsilon \end{align*} I would appreciate some feedback on the above and some help with those two questions (how Rudin deduces $|s_n| > \frac{1}{2} |s|$ and why he takes $N > m$ instead of $\max(N,m)$ .)","For reference, here is the full Theorem 3.3 (d) from Rudin. Suppose , are complex sequences, and , . Then (a) ; (b) , , for any number ; (c) ; (d) , provided ( ), and . There's one key step in the proof of (d) that I don't understand. First, Rudin uses convergence of to find an so that for all , we have . He then asserts that for , we have This is a very important step, but I cannot follow it. I've tried contradiction and the triangle inequality, but I can't get the inequality signs to line up. For example, I tried (for ), It seems as though I've bounded ""in the opposite direction."" If I knew were positive, expanding the absolute values might work, but we only know it's non-zero. The rest of the proof looks pretty straightforward to me, with one slight doubt. He asserts the existence of an (I don't know why he requires when he could just take the maximum of and ; does this make a difference?) so that implies . As , we have . We then have: I would appreciate some feedback on the above and some help with those two questions (how Rudin deduces and why he takes instead of .)","\{s_n\} \{t_n\} \lim\limits_{n \to \infty} s_n = s \lim\limits_{n \to \infty} t_n = t \lim\limits_{n \to \infty} \left(s_n + t_n\right) = s + t \lim\limits_{n \to \infty} cs_n = cs \lim\limits_{n \to \infty} \left(c + s_n\right) = c + s c \lim\limits_{n \to \infty} s_n t_n = st \lim\limits_{n \to \infty} \frac{1}{s_n} = \frac{1}{s} s_n \neq 0 n = 1, 2, 3, \ldots s \neq 0 s_n m \in \mathbb{N} n \geq m |s_n - s| < \frac{1}{2} |s| n \geq m  
|s_n| > \frac{1}{2} |s|.
 n \geq m  
|s_n| = |(s_n - s) + s| \leq |s_n - s| + |s| < \frac{1}{2} |s| + |s| = \frac{3}{2} |s|.
 |s_n| s N N > m N m n \geq N |s_n - s| < \frac{1}{2} |s|^2 \epsilon |s_n| > \frac{1}{2} |s| \frac{1}{|s_n|} < \frac{2}{|s|} \begin{align*}
\left \lvert \frac{1}{s_n} - \frac{1}{s} \right \rvert & = \left \lvert \frac{s - s_n}{s_n \cdot s} \right \rvert \\
& = \frac{|s - s_n|}{|s||s_n|} \\
& < \frac{2|s - s_n|}{|s|^2} \\
& < \frac{2}{|s|^2} \cdot \frac{1}{2} |s|^2 \epsilon \\
& = \epsilon
\end{align*} |s_n| > \frac{1}{2} |s| N > m \max(N,m)","['real-analysis', 'sequences-and-series', 'limits', 'proof-explanation']"
32,Show that $A \subset B \implies \overline{A} \subset \overline{B}$,Show that,A \subset B \implies \overline{A} \subset \overline{B},"NOTE: I know that a question asking for help to prove this same property already exists, but I would like an answer specifically based on the definition(s) and / or  remark below, please. Definition 1 : A point $x \in \mathbb{R}$ is a point of closure of a set $E \subset \mathbb{R}$ if $\quad \forall \ \delta>0,\; \ \exists \ y \in E \ \;$ s.t. $ \ |x-y| < \delta$ . Equivalently, $x$ is a point of closure of $E$ if every open interval containing $x$ also contains a point of $E.$ We call the set of all points of E the closure of $E$ and denote it by $\overline{E}.$ Remark: Every point in $E$ belongs to its closure. Particularly, $E \subset \overline{E}$ . Definition 2 : $E$ is closed if $E=\overline{E}$ . Question: Show that $A \subset B \implies \overline{A} \subset \overline{B}$ . Attempt: $A \subset B \implies A \subset B \subset \overline{B} \implies A \subset \overline{B}$ . If $A$ is closed then $A= \overline{A} \implies \overline{A} \subset \overline{B}.$ So I can do this when $A$ is closed but I'm not certain on how to use either of the definitions / remark to show that it holds when $A$ is open.","NOTE: I know that a question asking for help to prove this same property already exists, but I would like an answer specifically based on the definition(s) and / or  remark below, please. Definition 1 : A point is a point of closure of a set if s.t. . Equivalently, is a point of closure of if every open interval containing also contains a point of We call the set of all points of E the closure of and denote it by Remark: Every point in belongs to its closure. Particularly, . Definition 2 : is closed if . Question: Show that . Attempt: . If is closed then So I can do this when is closed but I'm not certain on how to use either of the definitions / remark to show that it holds when is open.","x \in \mathbb{R} E \subset \mathbb{R} \quad \forall \ \delta>0,\; \ \exists \ y \in E \ \;  \ |x-y| < \delta x E x E. E \overline{E}. E E \subset \overline{E} E E=\overline{E} A \subset B \implies \overline{A} \subset \overline{B} A \subset B \implies A \subset B \subset \overline{B} \implies A \subset \overline{B} A A= \overline{A} \implies \overline{A} \subset \overline{B}. A A",['real-analysis']
33,Is this set path-connected?,Is this set path-connected?,,"Consider $\left\{(e^{-x}\cos x, e^{-x}\sin x)\in \mathbb{R}^2: x\geq 0 \right\} \cup \left\{(0,0) \right\}.$ I need to see if this set is path connected. My guess is it is not primarily Because of the point $(0,0)$ . There needs to be a path from $(0,0)$ to any other point in the set. Since the other points are of the form $(e^{-\alpha}\cos \alpha, e^{-\alpha}\sin \alpha)$ for some $\alpha\geq 0,$ I must have that there is some $\alpha$ that will put the point $(e^{-\alpha}\cos \alpha, e^{-\alpha}\sin \alpha)$ arbitrarily close to $(0,0)$ . Since, $\cos x$ and $\sin x$ are never zero at the same time we need to send $\alpha$ to $\infty.$ What we seem to have is the closure of the set $$\left\{(e^{-x}\cos x, e^{-x}\sin x)\in \mathbb{R}^2: x\geq 0 \right\}.$$ Am I going in the right direction? How would I prove that there is no path from $(0,0)$ to any other point in $\left\{(e^{-x}\cos x, e^{-x}\sin x)\in \mathbb{R}^2: x\geq 0 \right\}$ ?","Consider I need to see if this set is path connected. My guess is it is not primarily Because of the point . There needs to be a path from to any other point in the set. Since the other points are of the form for some I must have that there is some that will put the point arbitrarily close to . Since, and are never zero at the same time we need to send to What we seem to have is the closure of the set Am I going in the right direction? How would I prove that there is no path from to any other point in ?","\left\{(e^{-x}\cos x, e^{-x}\sin x)\in \mathbb{R}^2: x\geq 0 \right\} \cup \left\{(0,0) \right\}. (0,0) (0,0) (e^{-\alpha}\cos \alpha, e^{-\alpha}\sin \alpha) \alpha\geq 0, \alpha (e^{-\alpha}\cos \alpha, e^{-\alpha}\sin \alpha) (0,0) \cos x \sin x \alpha \infty. \left\{(e^{-x}\cos x, e^{-x}\sin x)\in \mathbb{R}^2: x\geq 0 \right\}. (0,0) \left\{(e^{-x}\cos x, e^{-x}\sin x)\in \mathbb{R}^2: x\geq 0 \right\}","['real-analysis', 'general-topology', 'limits', 'connectedness', 'path-connected']"
34,"$f''(0)$ exists, what is $\lim_{x\to 0} f(x)/x^2$?","exists, what is ?",f''(0) \lim_{x\to 0} f(x)/x^2,"Given a function $f:\mathbb{R}\to\mathbb{R}$ twice-derivable at $x=0$ with $f(0)=0$ . Define $g(x):=\frac{f(x)}{x}$ for all $x\neq 0$ and $g(0)=f'(0)$ . Clearly, $g$ is continuous since $$ \lim_{x\to 0} g(x) =\lim_{x\to 0}  \frac{f(x)}{x} = \lim_{x\to 0}  \frac{f(x) - f(0)}{x-0} = f'(0).   $$ Solving a problem, I got the limit $\lim_{x\to 0}  \frac{g(x)}{x} =\lim_{x\to 0}\frac{f(x)}{x^2} $ , which I can't manage. I suppose that it exists and is related with $f''(0)$ . Could anyone show how to compute $\lim_{x\to 0}  \frac{g(x)}{x}$ ? Thanks in advance. EDIT: In a solution-book I read $g'(0)=f''(0) /2$ . Can this be  typo? Maybe they forgot some assumptions in the problem.","Given a function twice-derivable at with . Define for all and . Clearly, is continuous since Solving a problem, I got the limit , which I can't manage. I suppose that it exists and is related with . Could anyone show how to compute ? Thanks in advance. EDIT: In a solution-book I read . Can this be  typo? Maybe they forgot some assumptions in the problem.",f:\mathbb{R}\to\mathbb{R} x=0 f(0)=0 g(x):=\frac{f(x)}{x} x\neq 0 g(0)=f'(0) g  \lim_{x\to 0} g(x) =\lim_{x\to 0}  \frac{f(x)}{x} = \lim_{x\to 0}  \frac{f(x) - f(0)}{x-0} = f'(0).    \lim_{x\to 0}  \frac{g(x)}{x} =\lim_{x\to 0}\frac{f(x)}{x^2}  f''(0) \lim_{x\to 0}  \frac{g(x)}{x} g'(0)=f''(0) /2,"['real-analysis', 'limits', 'derivatives']"
35,"Prove $x^x+y^y\ge x^2+y^2$ for $x,y>0$ and $x+y\le 2$.",Prove  for  and .,"x^x+y^y\ge x^2+y^2 x,y>0 x+y\le 2","We may prove the inequality for $x,y\in (0,1]$ . Note that, for $0<x\le 1$ , it holds that \begin{align*}  x^x&=1+(x-1)+(x-1)^2+\frac{1}{2}(x-1)^3+\cdots\\ &\ge1+(x-1)+(x-1)^2+\frac{1}{2}(x-1)^3\\ &\ge x^2. \end{align*} Similarily, for $y \in (0,1]$ , it holds that $$y^y\ge y^2.$$ Thus $$x^x+y^y\ge x^2+y^2.$$ But how to prove under the condition $x+y\in (0,2]$ ?","We may prove the inequality for . Note that, for , it holds that Similarily, for , it holds that Thus But how to prove under the condition ?","x,y\in (0,1] 0<x\le 1 \begin{align*}
 x^x&=1+(x-1)+(x-1)^2+\frac{1}{2}(x-1)^3+\cdots\\
&\ge1+(x-1)+(x-1)^2+\frac{1}{2}(x-1)^3\\
&\ge x^2.
\end{align*} y \in (0,1] y^y\ge y^2. x^x+y^y\ge x^2+y^2. x+y\in (0,2]","['real-analysis', 'calculus', 'inequality']"
36,estimate of holder norm of a product,estimate of holder norm of a product,,"In a book 'Elliptic Partial Differential Equations of Second Order' by Gilbarg and Trudinger I stumbled upon the following inequality for two functions $f, g$ and their Holder norms in bounded domains $\Omega$ (eq 4.7): $$[f g]_{\gamma} \leq \max(1, d^{\alpha + \beta - 2\gamma}) [f]_{\alpha} [g]_{\beta} $$ where $\gamma = \min(\alpha, \beta)$ and $d = \text{diam}\ \Omega$ , $[f]_{\alpha} = \sup_{x \neq y} \frac{ f(x) - f(y) }{ \lvert x - y \rvert^{\alpha}}$ It's easy to prove that $fg$ is $\gamma$ -Holder (we use Holder embedding in compact spaces): $$[fg]_{\gamma} = \sup_{x \neq y} \frac{ \lvert f(y) g(y) - f(x) g(x) \rvert }{ \lvert x - y \rvert ^{\gamma} }\leq \sup_{x \neq y} \bigg( | f(x) | \frac{ |g(x) - g(y)| }{|x-y|^{\gamma}} + |g(x)|\frac{|f(x) - f(y)|}{|x-y|^{\gamma}} \bigg) \leq |f|_{\infty} [g]_{\gamma} + |g|_{\infty} [f]_{\gamma} $$ so $[fg]_{\gamma}$ is bounded by finite norms of $f,g$ . Then $|f|_{\infty} = \sup_{x \in \Omega} |f(x)| = \sup_{x \in \Omega} | f(x) - f(x_0) + f(x_0) | \leq |f(x_0)| + [f]_{\gamma} |x - x_0|^{\gamma}$ now if $x_0$ is taken such that $f(x_0) = 0$ , then we can bound it by: $$[fg]_{\gamma} \leq 2 [f]_{\gamma} [g]_{\gamma} d^{\gamma}$$ which is far from satisfying - not to mention that such $x_0$ does not necessarily have to exist.","In a book 'Elliptic Partial Differential Equations of Second Order' by Gilbarg and Trudinger I stumbled upon the following inequality for two functions and their Holder norms in bounded domains (eq 4.7): where and , It's easy to prove that is -Holder (we use Holder embedding in compact spaces): so is bounded by finite norms of . Then now if is taken such that , then we can bound it by: which is far from satisfying - not to mention that such does not necessarily have to exist.","f, g \Omega [f g]_{\gamma} \leq \max(1, d^{\alpha + \beta - 2\gamma}) [f]_{\alpha} [g]_{\beta}  \gamma = \min(\alpha, \beta) d = \text{diam}\ \Omega [f]_{\alpha} = \sup_{x \neq y} \frac{ f(x) - f(y) }{ \lvert x - y \rvert^{\alpha}} fg \gamma [fg]_{\gamma} = \sup_{x \neq y} \frac{ \lvert f(y) g(y) - f(x) g(x) \rvert }{ \lvert x - y \rvert ^{\gamma} }\leq \sup_{x \neq y} \bigg( | f(x) | \frac{ |g(x) - g(y)| }{|x-y|^{\gamma}} + |g(x)|\frac{|f(x) - f(y)|}{|x-y|^{\gamma}} \bigg) \leq |f|_{\infty} [g]_{\gamma} + |g|_{\infty} [f]_{\gamma}  [fg]_{\gamma} f,g |f|_{\infty} = \sup_{x \in \Omega} |f(x)| = \sup_{x \in \Omega} | f(x) - f(x_0) + f(x_0) | \leq |f(x_0)| + [f]_{\gamma} |x - x_0|^{\gamma} x_0 f(x_0) = 0 [fg]_{\gamma} \leq 2 [f]_{\gamma} [g]_{\gamma} d^{\gamma} x_0","['real-analysis', 'functional-analysis', 'holder-spaces']"
37,Sign of the first derivative,Sign of the first derivative,,"I am confused by the statement that a read, that says $f'(x_0)>0$ does not imply that $f$ is increasing in an open interval around $x_0$ . But the book also mentions that if $f(x)$ is differentiable at $x_0$ and $f'(x_0)>0$ then there is $h>0$ such that for all $x_1,x_2\in(x_0-h,x_0+h)$ if $x_1<x_0<x_2$ then $f(x_1)<f(x_0)<f(x_2)$ . I feel the statement contradicts one another. Any explanations will be appreciated.","I am confused by the statement that a read, that says does not imply that is increasing in an open interval around . But the book also mentions that if is differentiable at and then there is such that for all if then . I feel the statement contradicts one another. Any explanations will be appreciated.","f'(x_0)>0 f x_0 f(x) x_0 f'(x_0)>0 h>0 x_1,x_2\in(x_0-h,x_0+h) x_1<x_0<x_2 f(x_1)<f(x_0)<f(x_2)","['real-analysis', 'calculus']"
38,"If $\psi=x\sin(\frac{1}{x})$, and $f$ is integrable, is then $f\circ \psi$ also integrable?","If , and  is integrable, is then  also integrable?",\psi=x\sin(\frac{1}{x}) f f\circ \psi,"Let $$ \psi(x)= \left\{ \begin{array}{cll} x \sin\Big(\dfrac{1}{x}\Big) & \text{if} & x\in (0,1],\, \\ 0 & \text{if} & x=0, \end{array} \right. $$ and let $f:[-1,1]\rightarrow \mathbb{R}$ be Riemann integrable. How can I show that $f\circ \psi$ is Riemann integrable? I have several theorems in my book that could work if $\psi$ were a $C^1$ diffeomorphism or a homeomorphism with inverse satisfying Lipschitz condition. But $\psi$ is clearly neither of those. Any help with a zero set argument or reverting to definitions?",Let and let be Riemann integrable. How can I show that is Riemann integrable? I have several theorems in my book that could work if were a diffeomorphism or a homeomorphism with inverse satisfying Lipschitz condition. But is clearly neither of those. Any help with a zero set argument or reverting to definitions?,"
\psi(x)=
\left\{
\begin{array}{cll}
x \sin\Big(\dfrac{1}{x}\Big) & \text{if} & x\in (0,1],\, \\
0 & \text{if} & x=0,
\end{array}
\right.
 f:[-1,1]\rightarrow \mathbb{R} f\circ \psi \psi C^1 \psi","['real-analysis', 'calculus', 'integration', 'riemann-integration']"
39,"Deductions for the sets $A = \left\{(-1)^n + \frac{2}{n}:n=1,2,3,\ldots\right\}$ and $B = \{x \in \mathbf{Q}:0<x<1\}$",Deductions for the sets  and,"A = \left\{(-1)^n + \frac{2}{n}:n=1,2,3,\ldots\right\} B = \{x \in \mathbf{Q}:0<x<1\}","I would like to ask, if my proof and deductions for the below sets with respect to openness, closure checks out and is technically correct. Exercise 3.2.2 from Understanding Analysis by Stephen Abbot. Let \begin{align*} 	A = \left\{(-1)^n + \frac{2}{n}:n=1,2,3,\ldots\right\} \end{align*} and \begin{align*} 	B = \{x:\mathbf{Q}:0<x<1\} \end{align*} Answer the following questions for each set. (a) What are the limit points? (b) Is the set open? Closed? (c) Does the set contain any isolated points? (d) Find the closure of the set. Proof. (a) Writing out a few elements of the set $A$ , we have: \begin{align*} 	A = \left\{1,2,-\frac{1}{3},\frac{5}{4},-\frac{3}{5},\frac{4}{3},\ldots\right\} \end{align*} The points $-1$ and $1$ are the limit points of $A$ . There exists a subsequence $a_{2n-1} = -1 + \frac{2}{2n-1}$ in $A$ such that $\lim_{n\to\infty} a_{2n-1} = -1$ and $a_{2n-1} \ne 1$ for all $n \in \mathbf{N}$ . Similarly, look at the subsequence $(a_{2n})$ . $\lim_{n\to\infty} a_{2n} = 1$ . Every real number in $[0,1]$ is the limit of a sequence of rational numbers in $B$ . So, the closed interval $[0,1]$ is the set of all limit points of $B$ . (b) The set $A$ is open. For all $a \in A$ , there exists an $\epsilon$ -neighbourhood of $a$ , $V_\epsilon(a)$ that is contained in $A$ , $V_\epsilon(a) \subseteq A$ . The limit points of $A$ are not elements of the set $A$ . $A$ is not closed. The set $B$ is open. $B$ has no largest or smallest element. For every rational number $x \in B$ , there exists an $\epsilon$ -neighbourhood of $B$ , that intersects $B$ and contains points other than $x$ . The set $B$ is not closed, because $B$ does not contain its limit points. (c) Pick any point $a = (-1)^n + 2/n$ . Let $\epsilon = 2(\frac{1}{n} - \frac{1}{n+1})$ . Then, $V_\epsilon(a) \cap A = \{a\}\ \subseteq A$ . So, every point in $A$ is an isolated point. The set $B$ does not contain any isolated points. (d) \begin{align*} 	\bar{A} &= A \cup \{-1,1\}\\ 	\bar{B} &= [0,1] \end{align*}","I would like to ask, if my proof and deductions for the below sets with respect to openness, closure checks out and is technically correct. Exercise 3.2.2 from Understanding Analysis by Stephen Abbot. Let and Answer the following questions for each set. (a) What are the limit points? (b) Is the set open? Closed? (c) Does the set contain any isolated points? (d) Find the closure of the set. Proof. (a) Writing out a few elements of the set , we have: The points and are the limit points of . There exists a subsequence in such that and for all . Similarly, look at the subsequence . . Every real number in is the limit of a sequence of rational numbers in . So, the closed interval is the set of all limit points of . (b) The set is open. For all , there exists an -neighbourhood of , that is contained in , . The limit points of are not elements of the set . is not closed. The set is open. has no largest or smallest element. For every rational number , there exists an -neighbourhood of , that intersects and contains points other than . The set is not closed, because does not contain its limit points. (c) Pick any point . Let . Then, . So, every point in is an isolated point. The set does not contain any isolated points. (d)","\begin{align*}
	A = \left\{(-1)^n + \frac{2}{n}:n=1,2,3,\ldots\right\}
\end{align*} \begin{align*}
	B = \{x:\mathbf{Q}:0<x<1\}
\end{align*} A \begin{align*}
	A = \left\{1,2,-\frac{1}{3},\frac{5}{4},-\frac{3}{5},\frac{4}{3},\ldots\right\}
\end{align*} -1 1 A a_{2n-1} = -1 + \frac{2}{2n-1} A \lim_{n\to\infty} a_{2n-1} = -1 a_{2n-1} \ne 1 n \in \mathbf{N} (a_{2n}) \lim_{n\to\infty} a_{2n} = 1 [0,1] B [0,1] B A a \in A \epsilon a V_\epsilon(a) A V_\epsilon(a) \subseteq A A A A B B x \in B \epsilon B B x B B a = (-1)^n + 2/n \epsilon = 2(\frac{1}{n} - \frac{1}{n+1}) V_\epsilon(a) \cap A = \{a\}\ \subseteq A A B \begin{align*}
	\bar{A} &= A \cup \{-1,1\}\\
	\bar{B} &= [0,1]
\end{align*}","['real-analysis', 'general-topology', 'solution-verification']"
40,Asymptotic expansion of the inverse of $x\mapsto x+x^\phi$ near zero,Asymptotic expansion of the inverse of  near zero,x\mapsto x+x^\phi,"Consider a continuous real-valued monotone increasing function $f:\mathbb R^+\to\mathbb R^+$ satisfying $f\!\left(x+x^\phi\right)=x,$ where $\phi=\frac{1+\sqrt5}2$ is the golden ratio. Here is a plot of the function $f(z)$ : I need to find an asymptotic expansion of $f(z)$ for $z\to0^+$ in terms of powers of $z$ . By manually balancing coefficients, I was able to find a few first terms: $$f(z)=z-z^\phi\color{red}+\phi\,z^{2\,\phi-1}+\mathcal O\left(z^{3\,\phi-2}\right), \quad z\to0^+.$$ How can I find more terms and, ideally, a general formula for all terms of this series? Update : A wrong sign in formula is corrected (red).","Consider a continuous real-valued monotone increasing function satisfying where is the golden ratio. Here is a plot of the function : I need to find an asymptotic expansion of for in terms of powers of . By manually balancing coefficients, I was able to find a few first terms: How can I find more terms and, ideally, a general formula for all terms of this series? Update : A wrong sign in formula is corrected (red).","f:\mathbb R^+\to\mathbb R^+ f\!\left(x+x^\phi\right)=x, \phi=\frac{1+\sqrt5}2 f(z) f(z) z\to0^+ z f(z)=z-z^\phi\color{red}+\phi\,z^{2\,\phi-1}+\mathcal O\left(z^{3\,\phi-2}\right), \quad z\to0^+.","['real-analysis', 'sequences-and-series', 'asymptotics', 'inverse-function', 'golden-ratio']"
41,Is $\limsup_{t} \frac{a_t}{ h(t+1) - h(t) }$ finite when $0 \leq \sum_{i=t}^\infty a_i \leq h(t) = t^{-\alpha}$?,Is  finite when ?,\limsup_{t} \frac{a_t}{ h(t+1) - h(t) } 0 \leq \sum_{i=t}^\infty a_i \leq h(t) = t^{-\alpha},"Consider a monotone decreasing sequence $\{a_i\}_{i=1}^\infty$ , where $a_i  \geq 0$ for all $i$ . This sequence is assumed to satisfy the bound on its residual series $$ \sum_{i=t}^\infty a_i \leq h(t) = t^{-\alpha} \;\;\;\forall t\in\mathbb{N} \;, $$ where $\alpha \geq 1$ . From the above bound, it is clear that $\{a_i\}$ is summable since we have $\sum_{i=1}^\infty a_i \leq 1 < \infty$ . My objective is to determine whether $$\limsup_{i\rightarrow \infty} \frac{a_i}{-\Delta h_i} < \infty \,,$$ where $\Delta h_i = h(i+1) - h(i)$ . Thus far, I have only been able to prove the weaker property that $$ \liminf_{i\rightarrow\infty}  \frac{a_i}{-\Delta h_i} \leq 1 \;,$$ which is done by showing that $a_i \leq -\Delta h_i$ is true for infinitely many $i$ (see below for proof). I have found that the answer may be related to the Stolz-Cesaro Theorem or its converse , but I have been unable to make the exact requirements of the theorem line up with what I need. I have also found that this may be connected to the following post . Any help on this would be appreciated. I am also curious about the more general case when we have some convex function $h:(0,\infty)\rightarrow (0,\infty)$ satisfying $\lim_{i\rightarrow\infty} h(i) = 0$ . Proof that $a_i \leq -\Delta h_i$ for infinitely many $i$ : Assume the converse for all but finitely many $i$ . Then there exists a $T\in\mathbb{N}$ such that for all $t \geq T$ , $$ \sum_{i=t}^\infty a_i > - \sum_{i=t}^\infty \Delta h_i = h(t) \;, $$ which is a contradiction. Hence we must have that $a_i \leq -\Delta h_i$ holds infinitely often.","Consider a monotone decreasing sequence , where for all . This sequence is assumed to satisfy the bound on its residual series where . From the above bound, it is clear that is summable since we have . My objective is to determine whether where . Thus far, I have only been able to prove the weaker property that which is done by showing that is true for infinitely many (see below for proof). I have found that the answer may be related to the Stolz-Cesaro Theorem or its converse , but I have been unable to make the exact requirements of the theorem line up with what I need. I have also found that this may be connected to the following post . Any help on this would be appreciated. I am also curious about the more general case when we have some convex function satisfying . Proof that for infinitely many : Assume the converse for all but finitely many . Then there exists a such that for all , which is a contradiction. Hence we must have that holds infinitely often.","\{a_i\}_{i=1}^\infty a_i  \geq 0 i 
\sum_{i=t}^\infty a_i \leq h(t) = t^{-\alpha} \;\;\;\forall t\in\mathbb{N}
\;,
 \alpha \geq 1 \{a_i\} \sum_{i=1}^\infty a_i \leq 1 < \infty \limsup_{i\rightarrow \infty} \frac{a_i}{-\Delta h_i} < \infty \,, \Delta h_i = h(i+1) - h(i)  \liminf_{i\rightarrow\infty}  \frac{a_i}{-\Delta h_i} \leq 1 \;, a_i \leq -\Delta h_i i h:(0,\infty)\rightarrow (0,\infty) \lim_{i\rightarrow\infty} h(i) = 0 a_i \leq -\Delta h_i i i T\in\mathbb{N} t \geq T 
\sum_{i=t}^\infty a_i > - \sum_{i=t}^\infty \Delta h_i = h(t)
\;,
 a_i \leq -\Delta h_i","['real-analysis', 'sequences-and-series', 'upper-lower-bounds']"
42,About the roots of the derivative of a special polynomial,About the roots of the derivative of a special polynomial,,"Let $p$ be an odd prime, and let $n_1,\dots,n_{p-2}, m$ be even integers such that $n_1 < n_2 < \dots < n_{p-2}$ and \begin{equation} 2m > \sum_{i=1}^{p-2} n_i^2. \end{equation} Consider the polynomial \begin{equation} g(x)=(x^2 + m)(x - n_1) \dots (x - n_{p-2}). \end{equation} From Rolle's Theorem , we know that for each $i=1,2,\dots,p-3$ , there exists $x_i \in (n_i,n_{i+1})$ such that $g'(x_i)=0$ . So $g'(x)$ has at least $p-3$ distinct real zeroes. My question is: can $g'(x)$ have more than $p-3$ distinct real zeroes? I do not know the answer, but for sure the constraints on the parameters are relevant here. For example the polynomial $g(x)=(x^2+1)(x-4)(x-2)(x+2)$ has derivative $g'(x)=5x^4-16x^3-9x^2+24x-4=(x-1)(5x^3-11x^2-20x+4)$ which has four distinct real roots, as you can check on WolframAlpha . NOTE This strange polynomial arises in the construction given by R. Brauer of a polynomial $f(x) \in \mathbb{Q}[x]$ of degree $p$ whose Galois group over $\mathbb{Q}$ is isomorphic to the symmetric group $\mathcal{S}_p$ : see Jacobson, Basic Algebra I, $\S 4.10$ . The question I asked is clearly irrelevant for the construction, but has intrigued me, since I could not answer it in the negative nor I could find some counterexample.","Let be an odd prime, and let be even integers such that and Consider the polynomial From Rolle's Theorem , we know that for each , there exists such that . So has at least distinct real zeroes. My question is: can have more than distinct real zeroes? I do not know the answer, but for sure the constraints on the parameters are relevant here. For example the polynomial has derivative which has four distinct real roots, as you can check on WolframAlpha . NOTE This strange polynomial arises in the construction given by R. Brauer of a polynomial of degree whose Galois group over is isomorphic to the symmetric group : see Jacobson, Basic Algebra I, . The question I asked is clearly irrelevant for the construction, but has intrigued me, since I could not answer it in the negative nor I could find some counterexample.","p n_1,\dots,n_{p-2}, m n_1 < n_2 < \dots < n_{p-2} \begin{equation}
2m > \sum_{i=1}^{p-2} n_i^2.
\end{equation} \begin{equation}
g(x)=(x^2 + m)(x - n_1) \dots (x - n_{p-2}).
\end{equation} i=1,2,\dots,p-3 x_i \in (n_i,n_{i+1}) g'(x_i)=0 g'(x) p-3 g'(x) p-3 g(x)=(x^2+1)(x-4)(x-2)(x+2) g'(x)=5x^4-16x^3-9x^2+24x-4=(x-1)(5x^3-11x^2-20x+4) f(x) \in \mathbb{Q}[x] p \mathbb{Q} \mathcal{S}_p \S 4.10","['real-analysis', 'abstract-algebra', 'polynomials']"
43,"Are there maps $(0,\infty)\to (0,\infty)$ that are ${\mathbb Q}$-affine but not ${\mathbb R}$-affine?",Are there maps  that are -affine but not -affine?,"(0,\infty)\to (0,\infty) {\mathbb Q} {\mathbb R}","For any $F\subseteq {\mathbb R}$ , say that a map $f:(0,\infty)\to (0,\infty)$ is $F$ -affine when $f(tx+(1-t)y)=tf(x)+(1-t)f(y)$ whenever $t\in F$ , $x,y,tx+(1-t)y \gt 0$ . My question : can we construct a map $(0,\infty)\to (0,\infty)$ that's ${\mathbb Q}$ -affine but not ${\mathbb R}$ -affine ? My thoughts : $f$ cannot be continuous, obviously. If $\alpha_1,\alpha_2,\ldots,\alpha_r$ are $\mathbb Q$ -linearly independent positive real numbers and $C$ is the cone generated by them (so that $C$ is the set of all $\sum_{k=1}^{r} r_k\alpha_k $ where the $r_k$ are nonnegative rationals), then the map $f : C \to C$ defined by $$ f\bigg(\sum_{k=1}^{r} r_k\alpha_k\bigg)=\beta_0+\sum_{k=1}^{r} r_k\beta_k $$ (where $\beta_0,\beta_1,\ldots,\beta_r$ are positive constants), is $\mathbb Q$ -affine. Unfortunately, although there are Hamel bases of $\mathbb R$ over $\mathbb Q$ and we can certainly choose one to contain only positive numbers (replacing each negative base element by its opposite),  some positive real numbers will inevitably have some negative coordinates in this base so this seems to be a dead end. Also, if $f$ is a solution, $f$ can uniquely be extended to a $\mathbb Q$ -affine map ${\mathbb R}\to {\mathbb R}$ .","For any , say that a map is -affine when whenever , . My question : can we construct a map that's -affine but not -affine ? My thoughts : cannot be continuous, obviously. If are -linearly independent positive real numbers and is the cone generated by them (so that is the set of all where the are nonnegative rationals), then the map defined by (where are positive constants), is -affine. Unfortunately, although there are Hamel bases of over and we can certainly choose one to contain only positive numbers (replacing each negative base element by its opposite),  some positive real numbers will inevitably have some negative coordinates in this base so this seems to be a dead end. Also, if is a solution, can uniquely be extended to a -affine map .","F\subseteq {\mathbb R} f:(0,\infty)\to (0,\infty) F f(tx+(1-t)y)=tf(x)+(1-t)f(y) t\in F x,y,tx+(1-t)y \gt 0 (0,\infty)\to (0,\infty) {\mathbb Q} {\mathbb R} f \alpha_1,\alpha_2,\ldots,\alpha_r \mathbb Q C C \sum_{k=1}^{r} r_k\alpha_k  r_k f : C \to C 
f\bigg(\sum_{k=1}^{r} r_k\alpha_k\bigg)=\beta_0+\sum_{k=1}^{r} r_k\beta_k
 \beta_0,\beta_1,\ldots,\beta_r \mathbb Q \mathbb R \mathbb Q f f \mathbb Q {\mathbb R}\to {\mathbb R}","['real-analysis', 'examples-counterexamples', 'functional-equations', 'axiom-of-choice']"
44,Can we derive results from infinite sequences of integration by parts?,Can we derive results from infinite sequences of integration by parts?,,"I remember from my school days some old trick with integrals where if $\sin$ or $\cos$ were involved, we could sometimes apply the partial integration theorem once or twice and express the first integral in terms of itself in a way which let us solve it. An example: $$I=\int\sin(x)^2dx = x -\int \cos(x)^2dx = x - \sin(x)\cos(x) - \int \sin(x)^2dx$$ Now rearranging gives $$2I = x-\sin(x)\cos(x)$$ Which we can verify. Now to the question, can this be applied in a more general setting? For example if we do not after a finite set of steps end up with a closed form expression, could we use a series of integrals which might arise from iterating the integration-by-parts indefinitely? Are there any particular problems solvable this way which aren't solvable otherwise?","I remember from my school days some old trick with integrals where if or were involved, we could sometimes apply the partial integration theorem once or twice and express the first integral in terms of itself in a way which let us solve it. An example: Now rearranging gives Which we can verify. Now to the question, can this be applied in a more general setting? For example if we do not after a finite set of steps end up with a closed form expression, could we use a series of integrals which might arise from iterating the integration-by-parts indefinitely? Are there any particular problems solvable this way which aren't solvable otherwise?",\sin \cos I=\int\sin(x)^2dx = x -\int \cos(x)^2dx = x - \sin(x)\cos(x) - \int \sin(x)^2dx 2I = x-\sin(x)\cos(x),"['real-analysis', 'calculus', 'integration', 'soft-question']"
45,Computing $\sum_{n=1}^\infty\frac{(-1)^nH_n^2}{2n+1}$ in an alternative way,Computing  in an alternative way,\sum_{n=1}^\infty\frac{(-1)^nH_n^2}{2n+1},The following equality $$\sum_{n=1}^\infty(-1)^{n-1}\frac{H_n^2}{2n+1}=\frac{3}{16}\pi^3-\frac34\ln^2(2)\pi-8\Im\left\{\text{Li}_3\left(\frac{1+i}{2}\right)\right\}$$ can be proved if we are allowed to use the generating function ( see Eq $(3)$ ) $$\sum_{n=1}^\infty x^nH_n^2=\frac{\ln^2(1-x)+\text{Li}_2(x)}{1-x}$$ But the problem-proposer mentioned that the sum to be calculated without using this generating function. I have no clue how to approach it with such restriction. any idea ? Thanks in advance.,The following equality can be proved if we are allowed to use the generating function ( see Eq ) But the problem-proposer mentioned that the sum to be calculated without using this generating function. I have no clue how to approach it with such restriction. any idea ? Thanks in advance.,\sum_{n=1}^\infty(-1)^{n-1}\frac{H_n^2}{2n+1}=\frac{3}{16}\pi^3-\frac34\ln^2(2)\pi-8\Im\left\{\text{Li}_3\left(\frac{1+i}{2}\right)\right\} (3) \sum_{n=1}^\infty x^nH_n^2=\frac{\ln^2(1-x)+\text{Li}_2(x)}{1-x},"['real-analysis', 'integration', 'sequences-and-series', 'alternative-proof', 'harmonic-numbers']"
46,Step in proof on Riemann Sums from Spivak Calculus.,Step in proof on Riemann Sums from Spivak Calculus.,,"I was working out a proof in Spivak's Calculus (2008) - pg 279 . The following is a screenshot of the portion of the proof I'm having trouble with. My question is in working out combining steps 1,2, and 3 correctly. I want to arrive at $$\bigg|\sum_{i = 1}^{n}f(x_{i})(t_{i}-t_{i-1}) - \int_{a}^{b}f(x)dx \bigg| < \epsilon \\ \Rightarrow\ -\epsilon < \sum_{i = 1}^{n}f(x_{i})(t_{i}-t_{i-1}) - \int_{a}^{b}f(x)dx  < \epsilon$$ Fiddling around with equation 2, I would get something of the form $$ 0 \leq \sum_{i = 1}^{n}f(x_{i})(t_{i}-t_{i-1}) - L(f,P) \leq U(f,P) - L(f,P) < \epsilon$$ The same would occur for $\int_{a}^{b}f(x) dx$ .  Now using this idea I get something of the form: $$\epsilon > U(f,P) - L(f,P) \geq \sum_{i = 1}^{n}f(x_{i})(t_{i}-t_{i-1}) - L(f,P) \geq  \sum_{i = 1}^{n}f(x_{i})(t_{i}-t_{i-1}) - \int_{a}^{b}f(x) \geq ?? $$ Here is my issue, I can't say for sure that $\sum_{i = 1}^{n}f(x_{i})(t_{i}-t_{i-1}) - \int_{a}^{b}f(x) \geq 0$ . Nothing that I have can imply such and as a result of this I can't conclude that $\sum_{i = 1}^{n}f(x_{i})(t_{i}-t_{i-1}) - \int_{a}^{b}f(x) > -\epsilon$ . Which would allow me to finish this part of the proof. From experience I know it is a minor algebraic thing I'm missing, but I suppose I'm mentally fatigued and not seeing it. Some assistance would be nice.","I was working out a proof in Spivak's Calculus (2008) - pg 279 . The following is a screenshot of the portion of the proof I'm having trouble with. My question is in working out combining steps 1,2, and 3 correctly. I want to arrive at Fiddling around with equation 2, I would get something of the form The same would occur for .  Now using this idea I get something of the form: Here is my issue, I can't say for sure that . Nothing that I have can imply such and as a result of this I can't conclude that . Which would allow me to finish this part of the proof. From experience I know it is a minor algebraic thing I'm missing, but I suppose I'm mentally fatigued and not seeing it. Some assistance would be nice.","\bigg|\sum_{i = 1}^{n}f(x_{i})(t_{i}-t_{i-1}) - \int_{a}^{b}f(x)dx \bigg| < \epsilon \\ \Rightarrow\ -\epsilon < \sum_{i = 1}^{n}f(x_{i})(t_{i}-t_{i-1}) - \int_{a}^{b}f(x)dx  < \epsilon  0 \leq \sum_{i = 1}^{n}f(x_{i})(t_{i}-t_{i-1}) - L(f,P) \leq U(f,P) - L(f,P) < \epsilon \int_{a}^{b}f(x) dx \epsilon > U(f,P) - L(f,P) \geq \sum_{i = 1}^{n}f(x_{i})(t_{i}-t_{i-1}) - L(f,P) \geq  \sum_{i = 1}^{n}f(x_{i})(t_{i}-t_{i-1}) - \int_{a}^{b}f(x) \geq ??  \sum_{i = 1}^{n}f(x_{i})(t_{i}-t_{i-1}) - \int_{a}^{b}f(x) \geq 0 \sum_{i = 1}^{n}f(x_{i})(t_{i}-t_{i-1}) - \int_{a}^{b}f(x) > -\epsilon","['real-analysis', 'calculus', 'riemann-integration']"
47,"Is it true that $\int \hat{f}g \, dx=\int f\hat{g} \, dx$ for $f,g\in L^2$?",Is it true that  for ?,"\int \hat{f}g \, dx=\int f\hat{g} \, dx f,g\in L^2","It is a very well know fact that the formula holds in $L^1$ , I guess that the formula still  holds for $f,g\in L^2$ .","It is a very well know fact that the formula holds in , I guess that the formula still  holds for .","L^1 f,g\in L^2","['real-analysis', 'fourier-analysis', 'harmonic-analysis']"
48,Prove that $\sum_{n=1}^{\infty }\left ( \frac {\sin((2n-1)x)}{(2n-1)x)}\right )^k \frac{(-1)^{n-1}}{2n-1}=\frac π 4$ for $0\lt x\lt \frac \pi {2k} $,Prove that  for,\sum_{n=1}^{\infty }\left ( \frac {\sin((2n-1)x)}{(2n-1)x)}\right )^k \frac{(-1)^{n-1}}{2n-1}=\frac π 4 0\lt x\lt \frac \pi {2k} ,"Question :- Prove that $$ \sum_{ n =1}^{\infty } \left\{\frac{\sin\left(\left[2n - 1\right]x\right)} {\left(2n - 1\right)x}\right\}^{k}\ \frac{\left(-1\right)^{n - 1}}{2n - 1} = \frac{π}{4} \qquad\mbox{for}\quad 0\lt x\lt \frac{\pi}{2k} $$ While reading some papers , I came across this series.Unfortunately I do not have any link to the website since I took screenshot of It few months back . The Author claims that the above series is true for $0\lt x\lt \pi/\left(2k\right)$ . However he does not provide any mathematical proof instead he calculates the sum for different $x$ and $k$ like for $k = 100$ and $x = \pi/200$ the above sum up to $50$ terms is $$ 0.78539 81633 97448 30961 55824 $$ which is very close to $\pi/4$ . I verified It myself for $k = 1$ . Actually the author is working on various variations of the Gregory-Leibniz series and series of form $$ \frac{\sin\left(\mathrm{f}\left(x\right)\right)} {\mathrm{g}\left(x\right)} \quad\mbox{and}\quad  \frac{\cos\left(\mathrm{f}\left(x\right)\right)}{\mathrm{g}\left(x\right)} $$ ${\tt Mathematica}$ evaluates the series in terms of Lerch transcendent $\Phi$ function . I couldn't find any way to prove the  given series. Thank you for your help !!.","Question :- Prove that While reading some papers , I came across this series.Unfortunately I do not have any link to the website since I took screenshot of It few months back . The Author claims that the above series is true for . However he does not provide any mathematical proof instead he calculates the sum for different and like for and the above sum up to terms is which is very close to . I verified It myself for . Actually the author is working on various variations of the Gregory-Leibniz series and series of form evaluates the series in terms of Lerch transcendent function . I couldn't find any way to prove the  given series. Thank you for your help !!.","
\sum_{ n =1}^{\infty }
\left\{\frac{\sin\left(\left[2n - 1\right]x\right)}
{\left(2n - 1\right)x}\right\}^{k}\
\frac{\left(-1\right)^{n - 1}}{2n - 1} = \frac{π}{4}
\qquad\mbox{for}\quad 0\lt x\lt \frac{\pi}{2k}
 0\lt x\lt \pi/\left(2k\right) x k k = 100 x = \pi/200 50 
0.78539 81633 97448 30961 55824
 \pi/4 k = 1 
\frac{\sin\left(\mathrm{f}\left(x\right)\right)}
{\mathrm{g}\left(x\right)}
\quad\mbox{and}\quad 
\frac{\cos\left(\mathrm{f}\left(x\right)\right)}{\mathrm{g}\left(x\right)}
 {\tt Mathematica} \Phi","['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'complex-analysis']"
49,Find for which $\alpha$ the integral $\int_{0}^{1} \frac{1-x^{\alpha}}{1-x}dx$ converges,Find for which  the integral  converges,\alpha \int_{0}^{1} \frac{1-x^{\alpha}}{1-x}dx,"Find for which $\alpha$ the integral $\int_{0}^{1} \frac{1-x^{\alpha}}{1-x}dx$ converges. My Attempt: suppose $f(x) = \frac{1-x^{\alpha}}{1-x}$ . I think that the integral converges for $\alpha > -1$ . First I've tried to use the linearity of integrals, such that: $$\int_{0}^{1} \frac{1-x^{\alpha}}{1-x}dx = \int_{0}^{1} \frac{1}{1-x}dx - \int_{0}^{1} \frac{x^{\alpha}}{1-x}dx$$ as $$\int_{0}^{1} \frac{1}{1-x}dx = |_{0}^{1} \ln(1-x) $$ but because the first part of the integral diverges and is not dependent on $\alpha$ , then it's not helpful. the reason why I was foucsed in the point $x=1$ is because for $x=0$ , I'll apply the $\int_{0}^{1} \frac{1-x^{\alpha}}{1-x}dx$ limit comparison test with the function $g(x) = \frac{1}{1-x}$ , and lim $_{x\to 0} \frac{f(x)}{g(x)} = $ lim $_{x\to 0} 1-x^{\alpha} = 1$ , then for every $0<t<1: \ \int_{0}^{t}f(x)$ converges. I've checked different values of $\alpha$ and I'm wondering if the answer is connected to the fact that given $b>0$ , the integral $\int_{0}^{b} \frac{1}{x^\alpha}dx$ converges if and only if $\alpha < 1$ . I suppose that the easiest way to prove for which $\alpha$ the integral converges is by using the limit comparison test for improper integrals, but I can't find a function that will prove/disprove my hypothesis.","Find for which the integral converges. My Attempt: suppose . I think that the integral converges for . First I've tried to use the linearity of integrals, such that: as but because the first part of the integral diverges and is not dependent on , then it's not helpful. the reason why I was foucsed in the point is because for , I'll apply the limit comparison test with the function , and lim lim , then for every converges. I've checked different values of and I'm wondering if the answer is connected to the fact that given , the integral converges if and only if . I suppose that the easiest way to prove for which the integral converges is by using the limit comparison test for improper integrals, but I can't find a function that will prove/disprove my hypothesis.",\alpha \int_{0}^{1} \frac{1-x^{\alpha}}{1-x}dx f(x) = \frac{1-x^{\alpha}}{1-x} \alpha > -1 \int_{0}^{1} \frac{1-x^{\alpha}}{1-x}dx = \int_{0}^{1} \frac{1}{1-x}dx - \int_{0}^{1} \frac{x^{\alpha}}{1-x}dx \int_{0}^{1} \frac{1}{1-x}dx = |_{0}^{1} \ln(1-x)  \alpha x=1 x=0 \int_{0}^{1} \frac{1-x^{\alpha}}{1-x}dx g(x) = \frac{1}{1-x} _{x\to 0} \frac{f(x)}{g(x)} =  _{x\to 0} 1-x^{\alpha} = 1 0<t<1: \ \int_{0}^{t}f(x) \alpha b>0 \int_{0}^{b} \frac{1}{x^\alpha}dx \alpha < 1 \alpha,"['real-analysis', 'integration', 'convergence-divergence', 'improper-integrals']"
50,Integrate $\Omega=\int_{-\infty}^{\infty}\frac{\operatorname{arccot}(x)}{x^4+x^2+1}dx$,Integrate,\Omega=\int_{-\infty}^{\infty}\frac{\operatorname{arccot}(x)}{x^4+x^2+1}dx,"A friend of mine got me the problem proposed by Vasile Mircea Popa from Romania, which was published in the Romanian mathematical Magazine . The problem is to find: $$\Omega=\int_{-\infty}^{\infty}\frac{\operatorname{arccot}(x)}{x^4+x^2+1}dx$$ As per the Wolfram alpha the evaluated value is found to be $0$ . The reason is since $\operatorname{arccot}(x)=-\operatorname{arccot}(-x)$ for all $x\in\mathbb C^+$ is an odd function . However, the next answer obtained is $\frac{\pi^2}{ 2\sqrt{3}}$ where  the relations $\text{arccot}(x)=\frac{\pi}{2}-\operatorname{arctan}(x)\cdots(1)$ is used keeping in the view of principal branch of $\operatorname{arccot}(x)$ . The works is as follows: $$\Omega=\int_{-\infty}^{\infty}\frac{\frac{\pi}{2}-\operatorname{arctan}(x)}{x^4+x^2+1}dx=\frac{\pi}{2}\int_{-\infty}^{\infty}\frac{dx}{x^4+x^2+1}-\underbrace{\int_{-\infty}^{\infty}\frac{\operatorname{arctan}(x)}{x^4+x^2+1}}_{\text{odd function}}dx\\\overbrace{=}^{xy=1}\frac{\pi}{2}\int_{-\infty}^{\infty}\frac{x^2 dx}{x^4+x^2+1}=\frac{\pi}{2}\int_{-\infty}^{\infty}\frac{dx}{\left(x-\frac{1}{x}\right)^{2}+3}$$ then, by Cauchy Schlömilch transformation (Special case of Glasser's Masters  theorem) we obtain $$\Omega= \frac{\pi}{2}\int_{-\infty}^{\infty}\frac{dx}{x^2+3}=\frac{\pi^2}{2\sqrt{3}}$$ Note that former integral can be solved without using aforementioned theorem, by the partial fraction of $x^4+x^2+1=(x^2+x+1)(x^2-x+1)$ . My question is, Which of the above work is correct? In my view the first work is correct. In  the second working, Is the use of Maclaurin series done correctly ?","A friend of mine got me the problem proposed by Vasile Mircea Popa from Romania, which was published in the Romanian mathematical Magazine . The problem is to find: As per the Wolfram alpha the evaluated value is found to be . The reason is since for all is an odd function . However, the next answer obtained is where  the relations is used keeping in the view of principal branch of . The works is as follows: then, by Cauchy Schlömilch transformation (Special case of Glasser's Masters  theorem) we obtain Note that former integral can be solved without using aforementioned theorem, by the partial fraction of . My question is, Which of the above work is correct? In my view the first work is correct. In  the second working, Is the use of Maclaurin series done correctly ?",\Omega=\int_{-\infty}^{\infty}\frac{\operatorname{arccot}(x)}{x^4+x^2+1}dx 0 \operatorname{arccot}(x)=-\operatorname{arccot}(-x) x\in\mathbb C^+ \frac{\pi^2}{ 2\sqrt{3}} \text{arccot}(x)=\frac{\pi}{2}-\operatorname{arctan}(x)\cdots(1) \operatorname{arccot}(x) \Omega=\int_{-\infty}^{\infty}\frac{\frac{\pi}{2}-\operatorname{arctan}(x)}{x^4+x^2+1}dx=\frac{\pi}{2}\int_{-\infty}^{\infty}\frac{dx}{x^4+x^2+1}-\underbrace{\int_{-\infty}^{\infty}\frac{\operatorname{arctan}(x)}{x^4+x^2+1}}_{\text{odd function}}dx\\\overbrace{=}^{xy=1}\frac{\pi}{2}\int_{-\infty}^{\infty}\frac{x^2 dx}{x^4+x^2+1}=\frac{\pi}{2}\int_{-\infty}^{\infty}\frac{dx}{\left(x-\frac{1}{x}\right)^{2}+3} \Omega= \frac{\pi}{2}\int_{-\infty}^{\infty}\frac{dx}{x^2+3}=\frac{\pi^2}{2\sqrt{3}} x^4+x^2+1=(x^2+x+1)(x^2-x+1),"['real-analysis', 'integration', 'trigonometry', 'definite-integrals', 'inverse-function']"
51,"Real analytic set on a compact domain, no zeros on the boundary - isolated points only?","Real analytic set on a compact domain, no zeros on the boundary - isolated points only?",,"I have a feeling that the following must be true, but I cannot figure out a proof. I have two real analytic functions, $f$ , $g$ , both $[0,1]^2\rightarrow\mathbb{R}$ . I am interested in the set for which $f(x,y)=g(x,y)=0$ . Specifically, I would like to prove that this set is zero-dimensional. I know the following about these functions: For every $x_0 \in [0,1]$ , $f(x_0,y)=0$ has a solution. There exists $\epsilon \in(0,0.5)$ so that $g(x_0,y)=0$ has solutions only for $x_0\in [\epsilon,1-\epsilon]$ . Conversely, for every $y_0 \in [0,1]$ , $g(x,y_0)=0$ has a solution. There exists $\epsilon \in(0,0.5)$ so that $f(x,y_0)=0$ has solutions only for $y_0\in [\epsilon,1-\epsilon]$ . To put it into words, close to the boundary of the domain in $x$ -direction, $f$ has zeros but $g$ does not. Close to the boundary in $y$ -direction, $g$ has zeros but $f$ does not. This of course implies all solutions to the system are in the interior. My questions are: a) Is the information sufficient to conclude that solutions to $f=g=0$ are isolated points, i.e. there cannot be higher dimensional zero sets for the system? How would one show this? b) If it is false, what would be a counterexample? c) If it is true, is it also true for higher dimensions, say 3 functions $f,g,h:[0,1]^3\rightarrow\mathbb{R}$ with $f,g$ having zeros near and at $x=0,x=1$ , but not $h$ , $f,h$ having zeros near and at $y=0,y=1$ , but not $g$ , $g,h$ having zeros near and at $z=0,z=1$ , but not $f$ . Again the zero set for the system must lay in the interior. Can it contain paths? Any help is highly welcome. edit: I'll try to add a bit of thought we've had here. If the solution set does contain a path, this path can not be parallel to either the $x$ - or $y$ -axis - for then one could apply the identity theorem, and the restriction of both functions to that parallel would have to be the zero function. This contradicts either function not having a solution close to the boundaries this parallel intersects. So then, if such a path exists, it must be possible locally to give a parametrization of one coordinate by the other, say $\bar x(y)$ , with $f(\bar x(y),y)=g(\bar x(y),y)=0$ for $y$ in some open interval. But I fail to derive a contradiction from that and the assumptions quite yet. edit2: H. H. Rugh's answer and other users in the comments have pointed out how to construct counterexamples by taking any $f,g$ with the described properties and then multiplying both by some $h$ that has a 1-dimensional zero set in the interior. That was very helpful, since I realize now that the information described above is not sufficient to conclude what I wish to conclude (i.e. isolated zeros only). However, I am still convinced that it holds for the system I am interested in, but I obviously will have to rethink how to approach it. Any hints regarding which properties might be helpful to establish that such a system allows only isolated solutions would be highly welcome.","I have a feeling that the following must be true, but I cannot figure out a proof. I have two real analytic functions, , , both . I am interested in the set for which . Specifically, I would like to prove that this set is zero-dimensional. I know the following about these functions: For every , has a solution. There exists so that has solutions only for . Conversely, for every , has a solution. There exists so that has solutions only for . To put it into words, close to the boundary of the domain in -direction, has zeros but does not. Close to the boundary in -direction, has zeros but does not. This of course implies all solutions to the system are in the interior. My questions are: a) Is the information sufficient to conclude that solutions to are isolated points, i.e. there cannot be higher dimensional zero sets for the system? How would one show this? b) If it is false, what would be a counterexample? c) If it is true, is it also true for higher dimensions, say 3 functions with having zeros near and at , but not , having zeros near and at , but not , having zeros near and at , but not . Again the zero set for the system must lay in the interior. Can it contain paths? Any help is highly welcome. edit: I'll try to add a bit of thought we've had here. If the solution set does contain a path, this path can not be parallel to either the - or -axis - for then one could apply the identity theorem, and the restriction of both functions to that parallel would have to be the zero function. This contradicts either function not having a solution close to the boundaries this parallel intersects. So then, if such a path exists, it must be possible locally to give a parametrization of one coordinate by the other, say , with for in some open interval. But I fail to derive a contradiction from that and the assumptions quite yet. edit2: H. H. Rugh's answer and other users in the comments have pointed out how to construct counterexamples by taking any with the described properties and then multiplying both by some that has a 1-dimensional zero set in the interior. That was very helpful, since I realize now that the information described above is not sufficient to conclude what I wish to conclude (i.e. isolated zeros only). However, I am still convinced that it holds for the system I am interested in, but I obviously will have to rethink how to approach it. Any hints regarding which properties might be helpful to establish that such a system allows only isolated solutions would be highly welcome.","f g [0,1]^2\rightarrow\mathbb{R} f(x,y)=g(x,y)=0 x_0 \in [0,1] f(x_0,y)=0 \epsilon \in(0,0.5) g(x_0,y)=0 x_0\in [\epsilon,1-\epsilon] y_0 \in [0,1] g(x,y_0)=0 \epsilon \in(0,0.5) f(x,y_0)=0 y_0\in [\epsilon,1-\epsilon] x f g y g f f=g=0 f,g,h:[0,1]^3\rightarrow\mathbb{R} f,g x=0,x=1 h f,h y=0,y=1 g g,h z=0,z=1 f x y \bar x(y) f(\bar x(y),y)=g(\bar x(y),y)=0 y f,g h","['real-analysis', 'systems-of-equations', 'compactness']"
52,Probabilistic Proof of a Hausdorff-Young Type Inequality,Probabilistic Proof of a Hausdorff-Young Type Inequality,,"Let $1 \leq p <2$ and let $q$ be the Holder conjugate of $p$ so that $\frac{1}{p} + \frac{1}{q} = 1$ . Show that for any $\epsilon >0$ , there exists a Schwartz function $f \in S(\mathbb{R}^d)$ , such that: $$ \|\hat{f}\|_{L^{q}(\mathbb{R}^d)} \leq \epsilon \|f\|_{L^p(\mathbb{R}^d)} $$ The exercise suggests that as a hint, one should use Khintchine's inequality: If $\epsilon_{n}$ is a IID sequence of $\mathrm{Unif}(\{-1,1\})$ random variables (random choice of signs) and $x_n$ is (finite) sequence of complex numbers we have a constant $C(p) >0$ with: $$ \frac{1}{C(p)}\left(\sum_{n = 1}^{N}|x_n|^2\right)^{1/2} \leq \left(\mathbb{E}\left[\left(\sum_{n = 1}^N\epsilon_nx_n\right)^p\right]\right)^{1/p} \leq C(p)\left(\sum_{n = 1}^{N}|x_n|^2\right)^{1/2} $$ Does anyone have any ideas as to how one should properly apply this inequality?","Let and let be the Holder conjugate of so that . Show that for any , there exists a Schwartz function , such that: The exercise suggests that as a hint, one should use Khintchine's inequality: If is a IID sequence of random variables (random choice of signs) and is (finite) sequence of complex numbers we have a constant with: Does anyone have any ideas as to how one should properly apply this inequality?","1 \leq p <2 q p \frac{1}{p} + \frac{1}{q} = 1 \epsilon >0 f \in S(\mathbb{R}^d) 
\|\hat{f}\|_{L^{q}(\mathbb{R}^d)} \leq \epsilon \|f\|_{L^p(\mathbb{R}^d)}
 \epsilon_{n} \mathrm{Unif}(\{-1,1\}) x_n C(p) >0 
\frac{1}{C(p)}\left(\sum_{n = 1}^{N}|x_n|^2\right)^{1/2} \leq \left(\mathbb{E}\left[\left(\sum_{n = 1}^N\epsilon_nx_n\right)^p\right]\right)^{1/p} \leq C(p)\left(\sum_{n = 1}^{N}|x_n|^2\right)^{1/2}
","['real-analysis', 'probability-theory', 'harmonic-analysis']"
53,"$K=\{f \in C^1([0,1]): f(0)=0, |f'(x)|\leq 1 \; \forall x\}$ can be covered with $4^n$ balls of radius $1/n$",can be covered with  balls of radius,"K=\{f \in C^1([0,1]): f(0)=0, |f'(x)|\leq 1 \; \forall x\} 4^n 1/n","I am trying to solve the following problem: Let $ K=\{f \in C^1([0,1], \mathbb{R}): f(0)=0, |f'(x)|\leq 1 \; \forall x\} \subset C([0,1],\mathbb{R})$ Prove that $K$ is precompact in $C([0,1])$ Prove that, for every $n \in \mathbb{N}$ , $K$ can be covered with $4^n$ closed balls in $C([0,1])$ of radius $1/n$ (Hint: Consider balls centered at suitable piecewise affine functions with slope $\pm1$ ). I was able to prove point (1) by applying Arzelà–Ascoli theorem. I am struggling to prove point (2). I tried to construct the balls explicitly but with no success, even in the case $n=1$ .  I started by observing that for every $f \in K$ we have $$  -x \leq f(x) \leq x \quad \forall x \in [0,1] $$ and I thought that this could be useful. I then considered the balls of radius $1$ centered at $f_1(x) = x$ and $f_2(x) =-x$ but these do not seem enough. I thought that maybe I should consider vertical translations of $f_1$ and $f_2$ or some zig-zag looking functions, but I am unable to conclude. Also, maybe I do not have to construct the balls explicitly, but I do not have in mind other ways to prove their existence (the only thing that comes to my mind is to use the fact that compact metric spaces are totally bounded, but I do not know how). Could you please give me some help? Thank you. P.s. This problem is taken from a past entrance exam to a PhD in Mathematical Analysis. If you recognize that this is from some book, or if you have a source of similar problems, please tell me.","I am trying to solve the following problem: Let Prove that is precompact in Prove that, for every , can be covered with closed balls in of radius (Hint: Consider balls centered at suitable piecewise affine functions with slope ). I was able to prove point (1) by applying Arzelà–Ascoli theorem. I am struggling to prove point (2). I tried to construct the balls explicitly but with no success, even in the case .  I started by observing that for every we have and I thought that this could be useful. I then considered the balls of radius centered at and but these do not seem enough. I thought that maybe I should consider vertical translations of and or some zig-zag looking functions, but I am unable to conclude. Also, maybe I do not have to construct the balls explicitly, but I do not have in mind other ways to prove their existence (the only thing that comes to my mind is to use the fact that compact metric spaces are totally bounded, but I do not know how). Could you please give me some help? Thank you. P.s. This problem is taken from a past entrance exam to a PhD in Mathematical Analysis. If you recognize that this is from some book, or if you have a source of similar problems, please tell me."," K=\{f \in C^1([0,1], \mathbb{R}): f(0)=0, |f'(x)|\leq 1 \; \forall x\} \subset C([0,1],\mathbb{R}) K C([0,1]) n \in \mathbb{N} K 4^n C([0,1]) 1/n \pm1 n=1 f \in K  
-x \leq f(x) \leq x \quad \forall x \in [0,1]
 1 f_1(x) = x f_2(x) =-x f_1 f_2","['real-analysis', 'functional-analysis', 'metric-spaces', 'compactness']"
54,Prove that there are an infinite number of discontinuities on this function.,Prove that there are an infinite number of discontinuities on this function.,,"Show the  function $f:[0,1]\rightarrow\mathbb{R}$ given by $$ f(x)= \begin{cases} 1,&x=\frac{1}{n}\text{ for any positive integer $n$}\\ 0,&\text{otherwise} \end{cases} $$ has an infinite number of discontinuities. I have completed the proof, however I'm not sure if it is completely rigorous. I first showed that there is an infinite number of point that satisfy $$x=\frac{1}{n}$$ in the interval $[0,1]$ . I then defined $k_n$ as follows $$k_n \in \left(\frac{1}{n+1},\frac{1}{n}\right)\;\forall n\in \mathbb{N}$$ I then fixed $n$ and used the following interval for the function $$\left[\frac{1}{n+1},\frac{1}{n}\right)$$ rather than $[0,1]$ since it is a subinterval of $[0,1]$ . My logic was that if it is discontinuous on the subinterval then it must be discontinuous on $[0,1]$ but I am not quite sure how to say this rigorously so I simply stated it. I then negated the definition of continuity and took $$\epsilon=\frac{1}{2}$$ I then substituted $$f\left(k_n\right) = 0\text{ and }f\left(\frac{1}{n}\right) = 1$$ so, if $$\left|k_n-\frac{1}{n}\right|<\delta$$ then, $$\left|0-1\right|=1\geq\frac{1}{2}$$ This was the gist of my proof, I didn't write out everything explicitly for you but, can it be improved? have i missed anything? got a better method? any help is appreciated.","Show the  function given by has an infinite number of discontinuities. I have completed the proof, however I'm not sure if it is completely rigorous. I first showed that there is an infinite number of point that satisfy in the interval . I then defined as follows I then fixed and used the following interval for the function rather than since it is a subinterval of . My logic was that if it is discontinuous on the subinterval then it must be discontinuous on but I am not quite sure how to say this rigorously so I simply stated it. I then negated the definition of continuity and took I then substituted so, if then, This was the gist of my proof, I didn't write out everything explicitly for you but, can it be improved? have i missed anything? got a better method? any help is appreciated.","f:[0,1]\rightarrow\mathbb{R} 
f(x)=
\begin{cases}
1,&x=\frac{1}{n}\text{ for any positive integer n}\\
0,&\text{otherwise}
\end{cases}
 x=\frac{1}{n} [0,1] k_n k_n \in \left(\frac{1}{n+1},\frac{1}{n}\right)\;\forall n\in \mathbb{N} n \left[\frac{1}{n+1},\frac{1}{n}\right) [0,1] [0,1] [0,1] \epsilon=\frac{1}{2} f\left(k_n\right) = 0\text{ and }f\left(\frac{1}{n}\right) = 1 \left|k_n-\frac{1}{n}\right|<\delta \left|0-1\right|=1\geq\frac{1}{2}","['real-analysis', 'continuity']"
55,"If $f$ increasing, analytic on $\mathbb{R}$ and $\lim_{x\to +\infty}f(x)=1$, does it follows that $\lim_{x\to +\infty}f'(x)=0$?","If  increasing, analytic on  and , does it follows that ?",f \mathbb{R} \lim_{x\to +\infty}f(x)=1 \lim_{x\to +\infty}f'(x)=0,"Question: If $f$ strictly increasing, analytic on $\mathbb{R}$ and $\lim_{x\to +\infty}f(x)=1$ , does it follows that $\lim_{x\to +\infty}f'(x)=0$ ? If we drop the assumption that the function is increasing, an easy counterexample is $a(x)=\frac{\sin(x^2)}{x}+1$ . If we drop the analyticity requirement (but keep $C^{\infty}$ ) a counterexample can be constructed from $$h(x)=\begin{cases}0&x\le 0\\\exp\left(\frac{-\exp(-1/{(x-1)^2})}{x^2}\right)&x\in (0,1)\\ 1&x\ge 1\end{cases}$$ by setting $$b(x):=\text{sign}(x)\sum_{n=0}^{+\infty}\frac{h(2^n(|x|-n))}{2^{n+1}}$$ It is clear that, if $\lim_{x\to +\infty} f'$ exists, it must be $0$ : In fact, since $0=\lim_{x\to +\infty}\frac{f(x)-1}{x}=\lim_{x\to +\infty}f'(x)$ . Otherwise one can prove it by noting that, since $f'\ge 0$ and $1=\int_0^\infty f'(x)dx$ it is impossible to have $\lim_{x\to +\infty}f'(x)>0$ . However, I do not see how to prove the existence of the limit of how to construct a counterexample (as $b$ is not analytic)","Question: If strictly increasing, analytic on and , does it follows that ? If we drop the assumption that the function is increasing, an easy counterexample is . If we drop the analyticity requirement (but keep ) a counterexample can be constructed from by setting It is clear that, if exists, it must be : In fact, since . Otherwise one can prove it by noting that, since and it is impossible to have . However, I do not see how to prove the existence of the limit of how to construct a counterexample (as is not analytic)","f \mathbb{R} \lim_{x\to +\infty}f(x)=1 \lim_{x\to +\infty}f'(x)=0 a(x)=\frac{\sin(x^2)}{x}+1 C^{\infty} h(x)=\begin{cases}0&x\le 0\\\exp\left(\frac{-\exp(-1/{(x-1)^2})}{x^2}\right)&x\in (0,1)\\
1&x\ge 1\end{cases} b(x):=\text{sign}(x)\sum_{n=0}^{+\infty}\frac{h(2^n(|x|-n))}{2^{n+1}} \lim_{x\to +\infty} f' 0 0=\lim_{x\to +\infty}\frac{f(x)-1}{x}=\lim_{x\to +\infty}f'(x) f'\ge 0 1=\int_0^\infty f'(x)dx \lim_{x\to +\infty}f'(x)>0 b",['real-analysis']
56,"Two ""different"" definitions of $\sqrt{2}$","Two ""different"" definitions of",\sqrt{2},"In Walter Rudin's Principles of Mathematical Analysis (3rd edition) (page 10), it is proved that for every $x>0$ and every integer $n>0$ there is one and only one positive real $y$ such that $y^n=x$ . (This is number $y$ is then written $\sqrt[n]{x}$ .) In particular, this implies the existence of $\sqrt{2}$ . On the other hand, if one considers the polynomial $f(x)=x^2-2$ as an element in the ring $\mathbf{Q}[x]$ , one can adjoin a root of $f$ to $\mathbf{Q}$ . The procedure (see, for instance, Michael Artin's Algebra (2nd edition) page 456) is to form the quotient ring $K = \mathbf{Q}[x]/(f)$ of the polynomial ring $\mathbf{Q}[x]$ . This construction yields a ring $K$ and a homomorphism $F\to K$ , such that the residue $\overline{x}$ of $x$ satisfies the relation $f(\overline{x})=0$ . In the real analysis case, $\sqrt{2}$ can be approximated (or defined, depending on how one constructs the real numbers) by a Cauchy sequence of rational numbers: $$ 1, 1.4, 1.41, 1.414, 1.4142, \cdots $$ In the abstract algebra case, the set of real numbers is absent; one does not even need to define it. And there is no way to ""approximate"" $\overline{x}$ . These two ways to define the object $\sqrt{2}$ seems to be somewhat different in that the defined object has rather different properties. How should one understand the ""discrepancy"" here? Are there other relations/connections between these two definitions besides being a root of $f(x)=x^2-2$ ?","In Walter Rudin's Principles of Mathematical Analysis (3rd edition) (page 10), it is proved that for every and every integer there is one and only one positive real such that . (This is number is then written .) In particular, this implies the existence of . On the other hand, if one considers the polynomial as an element in the ring , one can adjoin a root of to . The procedure (see, for instance, Michael Artin's Algebra (2nd edition) page 456) is to form the quotient ring of the polynomial ring . This construction yields a ring and a homomorphism , such that the residue of satisfies the relation . In the real analysis case, can be approximated (or defined, depending on how one constructs the real numbers) by a Cauchy sequence of rational numbers: In the abstract algebra case, the set of real numbers is absent; one does not even need to define it. And there is no way to ""approximate"" . These two ways to define the object seems to be somewhat different in that the defined object has rather different properties. How should one understand the ""discrepancy"" here? Are there other relations/connections between these two definitions besides being a root of ?","x>0 n>0 y y^n=x y \sqrt[n]{x} \sqrt{2} f(x)=x^2-2 \mathbf{Q}[x] f \mathbf{Q} K = \mathbf{Q}[x]/(f) \mathbf{Q}[x] K F\to K \overline{x} x f(\overline{x})=0 \sqrt{2} 
1, 1.4, 1.41, 1.414, 1.4142, \cdots
 \overline{x} \sqrt{2} f(x)=x^2-2","['real-analysis', 'abstract-algebra']"
57,Lévy's metric on $\mathbb{R}^d$,Lévy's metric on,\mathbb{R}^d,"I know that a sequence of measures on $\mathbb{R}$ converges in distribution if and only if the corresponding Lévy's metric converges ( Relationship to weak toplogy (Lévy metric) ). According to this article : ""The concept of the Lévy metric can be extended to the case of distributions in $\mathbb{R}^d$ "". Let $\alpha=(1,...,1)$ ( $1$ repeated $d$ times), $\mathcal{P}$ the collection of probability measure on $\mathbb{R}^d$ and let's consider $d(F,H)=\inf(\epsilon>0;\forall x \in \mathbb{R}^d,F(x-\epsilon\alpha)-\epsilon\leq H(x) \leq F(x+\alpha\epsilon)+\epsilon),$ where $F$ and $H$ are two distribution functions on $\mathbb{R}^d.$ We can prove easily that $(d,\mathcal{P})$ is a metric space and that if $\lim_nd(F_n,F)=0$ then $F_n\Rightarrow F,$ so it remains to prove that if $F_n\Rightarrow F,$ then $\lim_nd(F_n,F)=0,$ so how can we do it?","I know that a sequence of measures on converges in distribution if and only if the corresponding Lévy's metric converges ( Relationship to weak toplogy (Lévy metric) ). According to this article : ""The concept of the Lévy metric can be extended to the case of distributions in "". Let ( repeated times), the collection of probability measure on and let's consider where and are two distribution functions on We can prove easily that is a metric space and that if then so it remains to prove that if then so how can we do it?","\mathbb{R} \mathbb{R}^d \alpha=(1,...,1) 1 d \mathcal{P} \mathbb{R}^d d(F,H)=\inf(\epsilon>0;\forall x \in \mathbb{R}^d,F(x-\epsilon\alpha)-\epsilon\leq H(x) \leq F(x+\alpha\epsilon)+\epsilon), F H \mathbb{R}^d. (d,\mathcal{P}) \lim_nd(F_n,F)=0 F_n\Rightarrow F, F_n\Rightarrow F, \lim_nd(F_n,F)=0,","['real-analysis', 'probability-theory', 'measure-theory', 'metric-spaces', 'weak-convergence']"
58,Intuitive motivation for limit computations,Intuitive motivation for limit computations,,"This is a Q&A pair concerning intuitive motivation for limit computations. Usually, my standard advice is to use asymptotic expansions to compute limits (especially for harder things like this or this ), but if we wish to do it without asymptotic expansions yet in a well-motivated way, we may want to have some intuitive explanation for why various elementary tricks work. For example, to prove that $\dfrac{1+2x-\sqrt[3]{1+6x}}{x^2} ≈ 4$ as $x → 0$ , an elegant way is to let $p = 1+2x$ and $r = \sqrt[3]{1+6x}$ , so as $x → 0$ we have $p,r → 1$ and hence $\dfrac{p-r}{x^2}$ $= \dfrac{p^3-r^3}{x^2·(p^2+p·r+r^2)}$ $= \dfrac{12+{?}x}{p^2+p·r+r^2}$ $≈ \dfrac{12}{1+1+1}$ . This trick may seem mysterious. After all, why did 'multiplying by the conjugate' work, and it is always possible to find such tricks? What if we are asked to find $\lim_{x→0} \dfrac{\sqrt{1+4x}-\sqrt[3]{1+6x}}{x^2}$ ? Is there a systematic yet intuitive way to figure out that we can apply the above trick to both parts? Personally, I prefer computing it by asymptotic expansion, namely that as $x → 0$ we clearly have $\sqrt{1+4x} ∈ 1+2x-2x^2+o(x^2)$ and $\sqrt[3]{1+6x} ∈ 1+2x-4x^2+o(x^2)$ , and so the result follows quickly. But the question remains: Can we find the asymptotic expansion intuitively without higher-power tools (such as Taylor series or binomial expansion for non-natural powers)? And better still, can we find an elementary solution without even rigorously proving the asymptotic expansion?","This is a Q&A pair concerning intuitive motivation for limit computations. Usually, my standard advice is to use asymptotic expansions to compute limits (especially for harder things like this or this ), but if we wish to do it without asymptotic expansions yet in a well-motivated way, we may want to have some intuitive explanation for why various elementary tricks work. For example, to prove that as , an elegant way is to let and , so as we have and hence . This trick may seem mysterious. After all, why did 'multiplying by the conjugate' work, and it is always possible to find such tricks? What if we are asked to find ? Is there a systematic yet intuitive way to figure out that we can apply the above trick to both parts? Personally, I prefer computing it by asymptotic expansion, namely that as we clearly have and , and so the result follows quickly. But the question remains: Can we find the asymptotic expansion intuitively without higher-power tools (such as Taylor series or binomial expansion for non-natural powers)? And better still, can we find an elementary solution without even rigorously proving the asymptotic expansion?","\dfrac{1+2x-\sqrt[3]{1+6x}}{x^2} ≈ 4 x → 0 p = 1+2x r = \sqrt[3]{1+6x} x → 0 p,r → 1 \dfrac{p-r}{x^2} = \dfrac{p^3-r^3}{x^2·(p^2+p·r+r^2)} = \dfrac{12+{?}x}{p^2+p·r+r^2} ≈ \dfrac{12}{1+1+1} \lim_{x→0} \dfrac{\sqrt{1+4x}-\sqrt[3]{1+6x}}{x^2} x → 0 \sqrt{1+4x} ∈ 1+2x-2x^2+o(x^2) \sqrt[3]{1+6x} ∈ 1+2x-4x^2+o(x^2)","['real-analysis', 'limits', 'soft-question', 'taylor-expansion', 'intuition']"
59,$\lim_{n \to \infty} \frac{1}{n} \sum_{r=1}^{\infty} e^{-\frac{r^2}{2n^2}}$,,\lim_{n \to \infty} \frac{1}{n} \sum_{r=1}^{\infty} e^{-\frac{r^2}{2n^2}},"Let $a_n = \frac{1}{n} \sum_{r=1}^{\infty} e^{-\frac{r^2}{2n^2}}$ . The sequence is well-defined by considering the ratio test. What then is $\lim_{n \to \infty} a_n$ ? I suspect it is $\sqrt{\pi/2}$ , by converting the ""riemann sum"" into a gaussian integral, but there were some slight details that I'm unable to justify. I've tried using another sequence, $b_{L,n} = \frac{1}{n} \sum_{r=1}^{nL} e^{-\frac{r^2}{2n^2}}$ , to help me in the process. $\lim_{n \to \infty} b_{L,n} = \int_0^L e^{-\frac{x^2}{2}} dx$ and $\lim_{L \to \infty} b_{L,n} = a_n$ . But to show that $\lim_{n \to \infty} \lim_{L \to \infty} b_{L,n} = \sqrt{\pi/2}$ , I can't just swap the order of the limits, can I?","Let . The sequence is well-defined by considering the ratio test. What then is ? I suspect it is , by converting the ""riemann sum"" into a gaussian integral, but there were some slight details that I'm unable to justify. I've tried using another sequence, , to help me in the process. and . But to show that , I can't just swap the order of the limits, can I?","a_n = \frac{1}{n} \sum_{r=1}^{\infty} e^{-\frac{r^2}{2n^2}} \lim_{n \to \infty} a_n \sqrt{\pi/2} b_{L,n} = \frac{1}{n} \sum_{r=1}^{nL} e^{-\frac{r^2}{2n^2}} \lim_{n \to \infty} b_{L,n} = \int_0^L e^{-\frac{x^2}{2}} dx \lim_{L \to \infty} b_{L,n} = a_n \lim_{n \to \infty} \lim_{L \to \infty} b_{L,n} = \sqrt{\pi/2}","['real-analysis', 'integration', 'improper-integrals', 'riemann-sum']"
60,$p$-adic metric,-adic metric,p,"This is a question from Robert Strichartz: the way of analysis, page 385. He defines a $p$ -adic metric on $\mathbb{Z}$ as follows. $p$ is a fixed prime. For any integer $z$ , we have $z = \pm \sum_{j=0}^N a_j p^j$ . $$|z|_p = p^{-k}$$ where $k$ is the smallest integer such that $a_k \neq 0$ . (a) Show that $d(x, y) = |x - y|_p$ is a metric. (b) Show that $d(x, z) \leq \max\left(d(x, y), d(y, z) \right)$ My understanding: First, I am assuming $x - y$ should be an integer. Second, from what he wrote, it does not follow that $|0|_p = 0$ so I am going to assume that. Any hints about how to start proving the triangle inequality? The answer to part (b) implies the triangle inequality only if I can solve part (b) without using the fact that $d$ is a metric.","This is a question from Robert Strichartz: the way of analysis, page 385. He defines a -adic metric on as follows. is a fixed prime. For any integer , we have . where is the smallest integer such that . (a) Show that is a metric. (b) Show that My understanding: First, I am assuming should be an integer. Second, from what he wrote, it does not follow that so I am going to assume that. Any hints about how to start proving the triangle inequality? The answer to part (b) implies the triangle inequality only if I can solve part (b) without using the fact that is a metric.","p \mathbb{Z} p z z = \pm \sum_{j=0}^N a_j p^j |z|_p = p^{-k} k a_k \neq 0 d(x, y) = |x - y|_p d(x, z) \leq \max\left(d(x, y), d(y, z) \right) x - y |0|_p = 0 d","['real-analysis', 'metric-spaces']"
61,Prove that $\lim_{h\to 0}\frac{e^h-1}{h}=1$ from the functional equation $f(x+y)=f(x)f(y)$.,Prove that  from the functional equation .,\lim_{h\to 0}\frac{e^h-1}{h}=1 f(x+y)=f(x)f(y),"If I have that $e^x$ is defined by the unique continuous function $\mathbb R\to \mathbb R^*$ s.t. $f(x+y)=f(x)f(y)$ for all $x,y\in\mathbb R$ and $f(1)=e$ , is it possible to prove that $$\lim_{h\to 0}\frac{e^h-1}{h}=1\ \ ?$$","If I have that is defined by the unique continuous function s.t. for all and , is it possible to prove that","e^x \mathbb R\to \mathbb R^* f(x+y)=f(x)f(y) x,y\in\mathbb R f(1)=e \lim_{h\to 0}\frac{e^h-1}{h}=1\ \ ?","['real-analysis', 'functional-equations']"
62,Pugh's definition of 'totally disconnected space',Pugh's definition of 'totally disconnected space',,"In Pugh's text Real Mathematical Analysis page 105, he defines a 'totally disconnected space' as follows: A metric space $M$ is totally disconnected if each point $p ∈ M$ has arbitrarily small clopen neighborhoods.    That is, given $\epsilon > 0, p ∈ M$ , there exists a clopen set $U$ such that $p ∈ U ⊂ M_{\epsilon}(p)$ Edit: $M_{\epsilon}(p)$ means 'an open ball of radius $\epsilon$ around p' The usual definition of a totally disconnected space is one where the singletons are the only connected subspaces. I can see how Pugh's definition implies the usual one, but not the other way around. There may be a contrived example where in fact the clopen subsets are ugly enough that they allow for total disconnectedness yet there aren't any clopen sets contained in a ball of radius $\epsilon$ . I can't find a counterexample either since all the metric spaces I've worked with thus far are 'nice'. So my question is - with regards to metric spaces, are these definitions equivalent?","In Pugh's text Real Mathematical Analysis page 105, he defines a 'totally disconnected space' as follows: A metric space is totally disconnected if each point has arbitrarily small clopen neighborhoods.    That is, given , there exists a clopen set such that Edit: means 'an open ball of radius around p' The usual definition of a totally disconnected space is one where the singletons are the only connected subspaces. I can see how Pugh's definition implies the usual one, but not the other way around. There may be a contrived example where in fact the clopen subsets are ugly enough that they allow for total disconnectedness yet there aren't any clopen sets contained in a ball of radius . I can't find a counterexample either since all the metric spaces I've worked with thus far are 'nice'. So my question is - with regards to metric spaces, are these definitions equivalent?","M p ∈ M \epsilon > 0, p ∈ M U p ∈ U ⊂ M_{\epsilon}(p) M_{\epsilon}(p) \epsilon \epsilon","['real-analysis', 'general-topology']"
63,Proof of integrability,Proof of integrability,,"The question states: Suppose $f$ is founded on $[a,b]$ . Suppose also that $f$ is integrable on every closed interval $[c,d]$ contained in the open interval $(a,b)$ . Show that $f$ is integrable on $[a,b]$ . So there are two possible courses of action. Either I can attempt to prove that $f$ must be piecewise monotonic on $[a,b]$ by proving that it is monotonic on every open subinterval, or I can attempt to write a proof via the Riemann criterion. I tried the former idea, but all I know is that $f$ is monotonic on every open subinterval of every [c,d], but I do not know how to bridge the gap between [c,d] and [a,b], despite trying to create a statement of the form $a<c<d<b$ and then finding $a$ and $c$ within $\epsilon$ of each other, and similarly for $b$ and $d$ . Trying to prove it using the Riemann criterion, I struggle to find good functions $s$ and $t$ to use. The textbook has not introduced limits or continuity yet, nor has it introduced the FTOC. So I must rely solely on basic analytical results involving the Riemann Criterion for the most part. Finally, I am not looking for a full proof but rather somewhat of an outline/hint as to how I can proceed.","The question states: Suppose is founded on . Suppose also that is integrable on every closed interval contained in the open interval . Show that is integrable on . So there are two possible courses of action. Either I can attempt to prove that must be piecewise monotonic on by proving that it is monotonic on every open subinterval, or I can attempt to write a proof via the Riemann criterion. I tried the former idea, but all I know is that is monotonic on every open subinterval of every [c,d], but I do not know how to bridge the gap between [c,d] and [a,b], despite trying to create a statement of the form and then finding and within of each other, and similarly for and . Trying to prove it using the Riemann criterion, I struggle to find good functions and to use. The textbook has not introduced limits or continuity yet, nor has it introduced the FTOC. So I must rely solely on basic analytical results involving the Riemann Criterion for the most part. Finally, I am not looking for a full proof but rather somewhat of an outline/hint as to how I can proceed.","f [a,b] f [c,d] (a,b) f [a,b] f [a,b] f a<c<d<b a c \epsilon b d s t","['real-analysis', 'calculus', 'integration', 'riemann-integration']"
64,How to prove this Bessel integral identity,How to prove this Bessel integral identity,,"I would like to prove the following identity which I found implemented in some code given to me: $$\int_0^\infty k e^{-k^2} J_0(kx)Y_0(kx)~\mathrm{d}k = -\frac{1}{2\pi} e^{-\frac{1}{2}x^2} K_0\left(\frac{1}{2}x^2\right)$$ where $J_0$ and $Y_0$ are the Bessel functions of the first and second kind, respectively, and $K_0$ is the modified Bessel function of the second kind. I am a physicist and not particularly familiar with Bessel functions or with their usual identities. Any particular ideas on how to prove this identity? I did a quick simulation in Python and the two sides of the equation indeed seem equal.","I would like to prove the following identity which I found implemented in some code given to me: where and are the Bessel functions of the first and second kind, respectively, and is the modified Bessel function of the second kind. I am a physicist and not particularly familiar with Bessel functions or with their usual identities. Any particular ideas on how to prove this identity? I did a quick simulation in Python and the two sides of the equation indeed seem equal.",\int_0^\infty k e^{-k^2} J_0(kx)Y_0(kx)~\mathrm{d}k = -\frac{1}{2\pi} e^{-\frac{1}{2}x^2} K_0\left(\frac{1}{2}x^2\right) J_0 Y_0 K_0,"['real-analysis', 'integration', 'bessel-functions']"
65,"How to prove that if $f:\mathbb{R} \rightarrow \mathbb{R}$ is continuous and $|f(x) - f(y)| > |x-y|$ for all $x,y$,then range of $f$ is $\mathbb{R}$ [duplicate]","How to prove that if  is continuous and  for all ,then range of  is  [duplicate]","f:\mathbb{R} \rightarrow \mathbb{R} |f(x) - f(y)| > |x-y| x,y f \mathbb{R}","This question already has answers here : Proving that a certain continuous function is surjective. (2 answers) Closed 4 years ago . Question: Let $f:\mathbb{R} \rightarrow \mathbb{R}$ be continuous and $|f(x) - f(y)| > |x-y|$ for all $x,y \in \mathbb{R}$ and $x \neq y$ . Prove that the range of $f$ is $\mathbb{R}$ Answer: I have a hard time writing this. For any [ $x_0, x_1$ ], it must be that the function within this interval is strictly increasing or decreasing. Otherwise, there is a max (or min) value $f(x_2)$ such that $x_2 \in (x_0,x_1)$ . Applying the Intermediate Value Theorem on $[x_0,x_2]$ and $[x_2,x_1]$ , then there exists a point $a \in [x_0,x_2]$ and $b \in [x_2,x_1]$ such that $f(a)=f(b)$ , contradicting $|f(x) - f(y)| > |x-y|$ for all $x,y \in \mathbb{R}$ . Furthermore, for arbitrary $y_0, y_1, y_2$ , if $|f(y_2)-f(y_1)| > |y_2-y_1|$ and $|f(y_1)-f(y_0)| > |y_1-y_0|$ then we have $|f(y_2)-f(y_0)| > |y_2-y_0|$ . Otherwise it means that the function on $[y_0,y_1]$ and $[y_1,y_2]$ were not both strictly increasing or strictly decreasing, which leads to the same contradiction, that there exist a $c \in [y_0,y_1]$ , $d\in [y_1,y_2]$ on each interval respectively where $f(c)=f(d)$ . Would I be able to then conclude that the range is $\mathbb{R}$ ? Logically it seems like I can, but it also doesn't seem to be based on any ""theorem"". I'm essentially just saying ""dot dot dot and so on"". I don't know if I'm missing anything, or if the arguments are not sound. Is there a more rigorous way to prove this?","This question already has answers here : Proving that a certain continuous function is surjective. (2 answers) Closed 4 years ago . Question: Let be continuous and for all and . Prove that the range of is Answer: I have a hard time writing this. For any [ ], it must be that the function within this interval is strictly increasing or decreasing. Otherwise, there is a max (or min) value such that . Applying the Intermediate Value Theorem on and , then there exists a point and such that , contradicting for all . Furthermore, for arbitrary , if and then we have . Otherwise it means that the function on and were not both strictly increasing or strictly decreasing, which leads to the same contradiction, that there exist a , on each interval respectively where . Would I be able to then conclude that the range is ? Logically it seems like I can, but it also doesn't seem to be based on any ""theorem"". I'm essentially just saying ""dot dot dot and so on"". I don't know if I'm missing anything, or if the arguments are not sound. Is there a more rigorous way to prove this?","f:\mathbb{R} \rightarrow \mathbb{R} |f(x) - f(y)| > |x-y| x,y \in \mathbb{R} x \neq y f \mathbb{R} x_0, x_1 f(x_2) x_2 \in (x_0,x_1) [x_0,x_2] [x_2,x_1] a \in [x_0,x_2] b \in [x_2,x_1] f(a)=f(b) |f(x) - f(y)| > |x-y| x,y \in \mathbb{R} y_0, y_1, y_2 |f(y_2)-f(y_1)| > |y_2-y_1| |f(y_1)-f(y_0)| > |y_1-y_0| |f(y_2)-f(y_0)| > |y_2-y_0| [y_0,y_1] [y_1,y_2] c \in [y_0,y_1] d\in [y_1,y_2] f(c)=f(d) \mathbb{R}","['real-analysis', 'continuity']"
66,Derivative of sign function $\operatorname{sgn}(x)$ (in distribution sense).,Derivative of sign function  (in distribution sense).,\operatorname{sgn}(x),"In the book of Schilling and Partzsch : Brownian motion (in the part of the Tanaka formula), they say that the derivative of $f(x)=\text{sgn}(x)$ is given by $f'(x)=\delta _0(x)$ (in distribution sense). But I find $f'(x)=2\delta _0(x)$ and I don't see where is my mistake : so let $\varphi$ a test function. $$\left<f',\varphi \right>=-\int_{\mathbb R}f\varphi '=\int_{-\infty }^0\varphi '-\int_0^\infty \varphi '=\varphi (0)+\varphi (0)=2\varphi (0)=\left<2\delta _0,\varphi \right>.$$ Did they do a mistake ?","In the book of Schilling and Partzsch : Brownian motion (in the part of the Tanaka formula), they say that the derivative of is given by (in distribution sense). But I find and I don't see where is my mistake : so let a test function. Did they do a mistake ?","f(x)=\text{sgn}(x) f'(x)=\delta _0(x) f'(x)=2\delta _0(x) \varphi \left<f',\varphi \right>=-\int_{\mathbb R}f\varphi '=\int_{-\infty }^0\varphi '-\int_0^\infty \varphi '=\varphi (0)+\varphi (0)=2\varphi (0)=\left<2\delta _0,\varphi \right>.",['real-analysis']
67,Baby Rudin chapter 2 exercise 8,Baby Rudin chapter 2 exercise 8,,"Exercise 2.8: Is every point of every open set $E\subset R^2$ a limit point of $E$ ? My Solution: Every point of every open set $E\subset R^2$ is a limit point of $E$ . [Notation: $N_r(p)$ is the set of all point x such that $0< d(x,p)< r $ ] Since $E$ is open, let point $x \in E$ , then x is an interior point of E. There is $r>0$ such that the deleted neighborhood $N_r(x) \subset E$ . For any $s>0$ , the deleted neighborhood $N_s(x)$ contains a point $z\in E$ , if $0<d(x,z)<min(s,r)$ . Thus $x$ is a limit point of $E$ . My question: I think my solution did not use any property of $R^2$ , so this conclusion should be true in other metric spaces. Is that right?","Exercise 2.8: Is every point of every open set a limit point of ? My Solution: Every point of every open set is a limit point of . [Notation: is the set of all point x such that ] Since is open, let point , then x is an interior point of E. There is such that the deleted neighborhood . For any , the deleted neighborhood contains a point , if . Thus is a limit point of . My question: I think my solution did not use any property of , so this conclusion should be true in other metric spaces. Is that right?","E\subset R^2 E E\subset R^2 E N_r(p) 0< d(x,p)< r  E x \in E r>0 N_r(x) \subset E s>0 N_s(x) z\in E 0<d(x,z)<min(s,r) x E R^2",['real-analysis']
68,Let $f(x)=\int_1^\infty \frac{\cos t}{x^2+t^2}dt$. Then which of the following are correct?,Let . Then which of the following are correct?,f(x)=\int_1^\infty \frac{\cos t}{x^2+t^2}dt,"Let $f(x)=\int_1^\infty \frac{\cos t}{x^2+t^2}dt$ . Then which of the following are correct? $f$ is bounded on $\mathbb R$ $f$ is continuous on $\mathbb R$ $f$ is not defined everywhere on $\mathbb R$ $f$ is not continuous on $\mathbb R$ My try: $|f(x)|=|\int_1^\infty \frac{\cos t}{x^2+t^2}dt|\leq \int_1^\infty |\frac{\cos t}{x^2+t^2}|dt\leq \int_1^\infty \frac{1}{x^2+t^2}dt=\frac{1}{x}\tan^{-1}(\frac{t}{x})|_1^\infty $ . Let $g(x)= \frac{1}{x}\tan^{-1}(\frac{t}{x})|_1^\infty$ is not continuous at $0$ . That doesn't mean that $f$ is not bounded. For continuity, $|f(x)-f(y)|=|\int_1^\infty \frac{\cos t}{x^2+t^2}dt-\int_1^\infty \frac{\cos t}{y^2+t^2}dt|\leq \int_1^\infty |\frac{1}{x^2+t^2}-\frac{1}{y^2+t^2}|dt=\int_1^\infty |\frac{y^2-x^2}{(x^2+t^2)(y^2+t^2)}|dt $ . I am not conclude from here.","Let . Then which of the following are correct? is bounded on is continuous on is not defined everywhere on is not continuous on My try: . Let is not continuous at . That doesn't mean that is not bounded. For continuity, . I am not conclude from here.",f(x)=\int_1^\infty \frac{\cos t}{x^2+t^2}dt f \mathbb R f \mathbb R f \mathbb R f \mathbb R |f(x)|=|\int_1^\infty \frac{\cos t}{x^2+t^2}dt|\leq \int_1^\infty |\frac{\cos t}{x^2+t^2}|dt\leq \int_1^\infty \frac{1}{x^2+t^2}dt=\frac{1}{x}\tan^{-1}(\frac{t}{x})|_1^\infty  g(x)= \frac{1}{x}\tan^{-1}(\frac{t}{x})|_1^\infty 0 f |f(x)-f(y)|=|\int_1^\infty \frac{\cos t}{x^2+t^2}dt-\int_1^\infty \frac{\cos t}{y^2+t^2}dt|\leq \int_1^\infty |\frac{1}{x^2+t^2}-\frac{1}{y^2+t^2}|dt=\int_1^\infty |\frac{y^2-x^2}{(x^2+t^2)(y^2+t^2)}|dt ,"['real-analysis', 'integration', 'improper-integrals']"
69,Proposition 5.4.4. in Tao,Proposition 5.4.4. in Tao,,"I am trying to prove the following proposition in Tao's analysis textbook. For ever real number $x$ , exactly one of the following three statements is true: (a) $x$ is zero; (b) $x$ is positive; (c) $x$ is negative. A real number $x$ is negative if and only if $-x$ is positive. If $x$ and $y$ are positive, then so are $x + y$ and $xy$ . I am unsure on how to approach the first part. Tao defines real numbers as limits of Cauchy sequences of rationals, though without defining just yet what a limit is. He defines a positive real number as one that can be written as the limit of a Cauchy sequence of rationals positively bounded away from $0$ and a negative real number as one that can be written as the limit of a Cauchy sequence negatively bounded away from $0$ . We have a law of trichotomy for the rationals, which could be extended to every element of the sequence, perhaps, to say that, upon throwing out a finite number of terms, the sequence is either identically zero, positively bounded away from zero, or negatively bounded away from $0$ , and thus $x$ is either $0$ , positive, or negative. I am still unsure on how to formalize this, though, or whether I am on the right track. The second statement seems rather straightforward. If $x$ is negative it is negatively bounded away from $0$ : we have $x = \text{LIM}_{n \to \infty} a_n$ , and $\exists - c < 0$ (- $c$ rational) such that $a_n \leq -c$ , Therefore, $-x = \text{LIM}_{n \to \infty} -a_n$ , where we have, multiplying through by $-1$ , that $\exists c > 0$ such that $-a_n \geq c$ , meaning $x$ is positively bounded away from $0$ and is therefore positive. The opposite implication is similar. As for the third part: let $x = \text{LIM}_{n \to \infty} a_n$ and $y = \text{LIM}_{n \to \infty} b_n$ . $x$ and $y$ are positive, meaning $a_n$ and $b_n$ are positively bounded away from $0$ , so $\exists c > 0, a_n \geq c$ and $\exists d > 0, b_n \geq d$ . Thus, for any $n$ , $a_n + b_n \geq c + d$ , and since the positive reals are closed under addition, $c + d > 0$ and $a_n + b_n$ is also positively bounded away from zero, so $x + y$ is also positive. The fourth part is similar, but with a product, $cd$ , in lieu of a sum. Assuming I have not made a mistake or omission, I believe that I understand how to write the later parts of the problem, but the first part is still quite confusing to me. Any help or insights would be greatly appreciated.","I am trying to prove the following proposition in Tao's analysis textbook. For ever real number , exactly one of the following three statements is true: (a) is zero; (b) is positive; (c) is negative. A real number is negative if and only if is positive. If and are positive, then so are and . I am unsure on how to approach the first part. Tao defines real numbers as limits of Cauchy sequences of rationals, though without defining just yet what a limit is. He defines a positive real number as one that can be written as the limit of a Cauchy sequence of rationals positively bounded away from and a negative real number as one that can be written as the limit of a Cauchy sequence negatively bounded away from . We have a law of trichotomy for the rationals, which could be extended to every element of the sequence, perhaps, to say that, upon throwing out a finite number of terms, the sequence is either identically zero, positively bounded away from zero, or negatively bounded away from , and thus is either , positive, or negative. I am still unsure on how to formalize this, though, or whether I am on the right track. The second statement seems rather straightforward. If is negative it is negatively bounded away from : we have , and (- rational) such that , Therefore, , where we have, multiplying through by , that such that , meaning is positively bounded away from and is therefore positive. The opposite implication is similar. As for the third part: let and . and are positive, meaning and are positively bounded away from , so and . Thus, for any , , and since the positive reals are closed under addition, and is also positively bounded away from zero, so is also positive. The fourth part is similar, but with a product, , in lieu of a sum. Assuming I have not made a mistake or omission, I believe that I understand how to write the later parts of the problem, but the first part is still quite confusing to me. Any help or insights would be greatly appreciated.","x x x x x -x x y x + y xy 0 0 0 x 0 x 0 x = \text{LIM}_{n \to \infty} a_n \exists - c < 0 c a_n \leq -c -x = \text{LIM}_{n \to \infty} -a_n -1 \exists c > 0 -a_n \geq c x 0 x = \text{LIM}_{n \to \infty} a_n y = \text{LIM}_{n \to \infty} b_n x y a_n b_n 0 \exists c > 0, a_n \geq c \exists d > 0, b_n \geq d n a_n + b_n \geq c + d c + d > 0 a_n + b_n x + y cd",['real-analysis']
70,$\sum_{n=1}^\infty\frac{H_n}{n}\left(\zeta(2)-H_{2n}^{(2)}\right)$,,\sum_{n=1}^\infty\frac{H_n}{n}\left(\zeta(2)-H_{2n}^{(2)}\right),"This problem was proposed by Cornel and he showed that $$S=\sum_{n=1}^\infty\frac{H_n}{n}\left(\zeta(2)-H_{2n}^{(2)}\right)=\frac13\ln^42-\frac12\ln^22\zeta(2)+\frac72\ln2\zeta(3)-\frac{21}4\zeta(4)+8\operatorname{Li}_4\left(\frac12\right)$$ here is my approach. we know that $\quad\zeta(2)-H_{2n}^{(2)}=\psi^{(1)}(2n+1)=-\displaystyle\int_0^1\frac{x^{2n}\ln x}{1-x}\ dx$ then $$S=-\int_0^1\frac{\ln x}{1-x}\sum_{n=1}^\infty\frac{x^{2n}H_n}{n}\ dx=-\int_0^1\frac{\ln x}{1-x}\left(\frac12\ln^2(1-x^2)+\operatorname{Li}_2(x^2)\right)\ dx$$ and by applying IBP , we get $$S=-2\int_0^1\frac{\operatorname{Li}_2(1-x)\ln(1-x^2)}{x(1-x^2)}\ dx$$ I applied the dilogarithmic identity $\quad\operatorname{Li}_2(1-x)=\zeta(2)-\ln x\ln(1-x)-\operatorname{Li}_2(x)$ but it was not that helpful. any idea?","This problem was proposed by Cornel and he showed that here is my approach. we know that then and by applying IBP , we get I applied the dilogarithmic identity but it was not that helpful. any idea?",S=\sum_{n=1}^\infty\frac{H_n}{n}\left(\zeta(2)-H_{2n}^{(2)}\right)=\frac13\ln^42-\frac12\ln^22\zeta(2)+\frac72\ln2\zeta(3)-\frac{21}4\zeta(4)+8\operatorname{Li}_4\left(\frac12\right) \quad\zeta(2)-H_{2n}^{(2)}=\psi^{(1)}(2n+1)=-\displaystyle\int_0^1\frac{x^{2n}\ln x}{1-x}\ dx S=-\int_0^1\frac{\ln x}{1-x}\sum_{n=1}^\infty\frac{x^{2n}H_n}{n}\ dx=-\int_0^1\frac{\ln x}{1-x}\left(\frac12\ln^2(1-x^2)+\operatorname{Li}_2(x^2)\right)\ dx S=-2\int_0^1\frac{\operatorname{Li}_2(1-x)\ln(1-x^2)}{x(1-x^2)}\ dx \quad\operatorname{Li}_2(1-x)=\zeta(2)-\ln x\ln(1-x)-\operatorname{Li}_2(x),"['real-analysis', 'integration', 'sequences-and-series', 'harmonic-numbers', 'polylogarithm']"
71,[True/False]The polynomial $x^4+7x^3−13x^2+11x$ has exactly one real root.,[True/False]The polynomial  has exactly one real root.,x^4+7x^3−13x^2+11x,"[True/False]The polynomial $x^4+7x^3−13x^2+11x$ has exactly one real   root. I want to solve it without drawing the graph. Here is my idea. Note that $f(1)=1+7-13+11=6>0$ and $f(-1)=1-7-13-11=-30<0$ So we have at least one real root. Now since degree is $4$ we have $4$ roots but rest three can not be complex as they occur in pairs, so we must have another real root. So the statement is False Am I right? Thanks for reading and all the help.","[True/False]The polynomial has exactly one real   root. I want to solve it without drawing the graph. Here is my idea. Note that and So we have at least one real root. Now since degree is we have roots but rest three can not be complex as they occur in pairs, so we must have another real root. So the statement is False Am I right? Thanks for reading and all the help.",x^4+7x^3−13x^2+11x f(1)=1+7-13+11=6>0 f(-1)=1-7-13-11=-30<0 4 4,"['real-analysis', 'functions']"
72,Example of an *associative* function $\mathbb{R}^2 \to \mathbb{R}$ which is continuous in both variables but fails to be continuous,Example of an *associative* function  which is continuous in both variables but fails to be continuous,\mathbb{R}^2 \to \mathbb{R},"A classical example of a function of two variables continuous in each but which fails to be continuous is this one . I wonder if someone can suggest an example of an associative counterexample of a binary operation continuous in each variable which fails to be continuous (it doesn't have to be on $\mathbb{R}$ but I expect that to be the easiest setting for analytic examples). Context: I am currently working with topological monoids, and have found (for a fixed multiplication) a canonical topology related to an action of the monoid which guarantees that the multiplication is continuous in each variable. I would of course like the topology to make it fully continuous, but a priori it is not clear that this holds.","A classical example of a function of two variables continuous in each but which fails to be continuous is this one . I wonder if someone can suggest an example of an associative counterexample of a binary operation continuous in each variable which fails to be continuous (it doesn't have to be on but I expect that to be the easiest setting for analytic examples). Context: I am currently working with topological monoids, and have found (for a fixed multiplication) a canonical topology related to an action of the monoid which guarantees that the multiplication is continuous in each variable. I would of course like the topology to make it fully continuous, but a priori it is not clear that this holds.",\mathbb{R},"['real-analysis', 'abstract-algebra', 'general-topology']"
73,Is $ f(x):= \frac{x}{ |x|^n }$ a gradient field?,Is  a gradient field?, f(x):= \frac{x}{ |x|^n },"for $ 2 \leq n $ let be $ f: \mathbb{R}^n \backslash \{ 0 \} \rightarrow \mathbb{R}^n $ $ f(x):= \frac{x}{|x|^n} $ , $ x\in \mathbb{R}^n \backslash \{ 0 \} $ Is f a gradient field? A vectorfield $ f $ ist a gradient field, if there is a function $ f= \nabla v $ $ \left[ \nabla f (x) = (\frac{ \partial f}{ \partial x_1} (x),...,\frac{ \partial f}{ \partial x_n} (x)) \right] $ So, I need to find a function $v$ so that $ f= \nabla v $ right? If I look for the case $n=2 $ so $ f(x):= \frac{x}{|x|^2} \in \mathbb{R}^n \backslash \{ 0 \} $ let be $ \partial_j v(x)= \frac{x_j}{|x|}$ then $v(x)= ln(|x|) $ and $$ \partial_j v(x)= \frac{1}{|x|} \frac{x_j}{|x|} = \frac{x_j}{|x^2|} $$ Is this the right idea? If yes, how can I proceed for larger $n$ ? If no, what would you suggest? Appreciate any help !","for let be , Is f a gradient field? A vectorfield ist a gradient field, if there is a function So, I need to find a function so that right? If I look for the case so let be then and Is this the right idea? If yes, how can I proceed for larger ? If no, what would you suggest? Appreciate any help !"," 2 \leq n   f: \mathbb{R}^n \backslash \{ 0 \} \rightarrow \mathbb{R}^n   f(x):= \frac{x}{|x|^n}   x\in \mathbb{R}^n \backslash \{ 0 \}   f   f= \nabla v   \left[ \nabla f (x) = (\frac{ \partial f}{ \partial x_1} (x),...,\frac{ \partial f}{ \partial x_n} (x)) \right]  v  f= \nabla v  n=2   f(x):= \frac{x}{|x|^2} \in \mathbb{R}^n \backslash \{ 0 \}   \partial_j v(x)= \frac{x_j}{|x|} v(x)= ln(|x|)   \partial_j v(x)= \frac{1}{|x|} \frac{x_j}{|x|} = \frac{x_j}{|x^2|}  n","['real-analysis', 'multivariable-calculus', 'vector-analysis']"
74,Showing a function is Frechet Differentiable？,Showing a function is Frechet Differentiable？,,"I just started learning the Frechet Derivatives. So I have a function $H:\mathbb{R}^{N\times n}\to\mathbb{R}^{N\times n}$ , i.e. $U^T\in\mathbb{R}^{N\times n}$ and $$H(U^T)=GW\times (F(U))^T+S\times U^T+C$$ with $G,W,S\in \mathbb{R}^{N \times N}$ are two matrices of size $N\times N$ , $F(\cdot)\in \mathbb{R}^n\to\mathbb{R}^n $ is a nonlinear function which maps each column vetor of $U$ to the corresponding column vector of $F(U)$ , and $C\in\mathbb{R}^{N\times n}$ . My question is what property should the nonlinear unknown function $F(\cdot)$ satisfy to ensure the function $H(\cdot)$ is Frechet differentiable? What does the Frechet derivative matrix looks like? What should I start?Thank you!","I just started learning the Frechet Derivatives. So I have a function , i.e. and with are two matrices of size , is a nonlinear function which maps each column vetor of to the corresponding column vector of , and . My question is what property should the nonlinear unknown function satisfy to ensure the function is Frechet differentiable? What does the Frechet derivative matrix looks like? What should I start?Thank you!","H:\mathbb{R}^{N\times n}\to\mathbb{R}^{N\times n} U^T\in\mathbb{R}^{N\times n} H(U^T)=GW\times (F(U))^T+S\times U^T+C G,W,S\in \mathbb{R}^{N \times N} N\times N F(\cdot)\in \mathbb{R}^n\to\mathbb{R}^n  U F(U) C\in\mathbb{R}^{N\times n} F(\cdot) H(\cdot)","['calculus', 'real-analysis', 'banach-spaces', 'fixed-point-theorems', 'frechet-derivative']"
75,"Suppose $f: [0,1] \rightarrow [0,1] $ and $f(x) \leq \int_0^x \sqrt{f(t)}dt$. Show that $f(x) \leq x^2$ for all $x \in [0,1]$.",Suppose  and . Show that  for all .,"f: [0,1] \rightarrow [0,1]  f(x) \leq \int_0^x \sqrt{f(t)}dt f(x) \leq x^2 x \in [0,1]","Let $f(x): [0,1] \rightarrow [0,1] $ such that $f(x) \leq \int_0^x \sqrt{f(t)}\,dt$ . Show that $f(x) \leq x^2$ for all $x \in [0,1]$ . I tried reiterating the inequality, obtaining $f(x) \leq \int_0^x1dt = x; f(x) \leq \int_0^x \sqrt{t}dt = \frac{2}{3}x^{3/2}$ etc... While it's easy to see that the exponent of $x$ tends to $2$ , it's more difficult to show that the coefficient is $1$ . Can somebody help me?","Let such that . Show that for all . I tried reiterating the inequality, obtaining etc... While it's easy to see that the exponent of tends to , it's more difficult to show that the coefficient is . Can somebody help me?","f(x): [0,1] \rightarrow [0,1]  f(x) \leq \int_0^x \sqrt{f(t)}\,dt f(x) \leq x^2 x \in [0,1] f(x) \leq \int_0^x1dt = x; f(x) \leq \int_0^x \sqrt{t}dt = \frac{2}{3}x^{3/2} x 2 1","['calculus', 'real-analysis', 'integration', 'inequality']"
76,"$f(x) = 1 - |1 - 2x|$, $a_{n+1} = f(a_n)$, prove convergence of subsequences",", , prove convergence of subsequences",f(x) = 1 - |1 - 2x| a_{n+1} = f(a_n),"Let $f(x) = 1 - |1 - 2x|$ , $a_1 = a$ , $a_{n+1} = f(a_n)$ . Prove there exists $a \in [0, 1]$ such that for every $x \in [0, 1]$ there exists a subsequence of $a_n$ convergent to $x$ . I've tried to analyze the graph of this function, but couldn't spot anything useful.","Let , , . Prove there exists such that for every there exists a subsequence of convergent to . I've tried to analyze the graph of this function, but couldn't spot anything useful.","f(x) = 1 - |1 - 2x| a_1 = a a_{n+1} = f(a_n) a \in [0, 1] x \in [0, 1] a_n x","['calculus', 'real-analysis', 'limits']"
77,"discuss about the differentiability of $g(x)=|f(x)|$, where $f$ is a differentiable function","discuss about the differentiability of , where  is a differentiable function",g(x)=|f(x)| f,"I want to discuss about the differentiability of $g(x)=|f(x)|$ , where $f$ is a differentiable function Example 1 Take $f(x)=|x|$ , function is clearly not differentiable at $x=0$ . Example 2 Take $f(x)=|\sin(x)|$ , function is clearly not differentiable at the point $x=n\pi$ After taking few more examples like $|x-1|, |\cos(x)|$ , it always seems to be the case that $|f(x)|$ is not differentiable at the points where $f(x)=0$ Observation: One thing is common in all the examples that some portion of $f(x)$ lies below $x$ axis. So I took another example $f(x)=x^2$ but $|f(x)|$ is differentiable at the point where $f(x)=0$ Question 1: Am I right in concluding that we can not just say in general setting that $|f(x)|$ is not differentiable at the points where $f(x)=0$ ? When can we(I mean under what conditions can we )conclude that $|f(x)|$ is differentiable at points where $f(x)=0$ . My hypothesis is that graph of $f$ should lie below $x$ axis. Question 2: Let $f(x)$ and $g(x)$ be two differentiable function, when can we conclude that $|f(x)|+|g(x)|$ is not differentiable at the  points where $f(x)=0$ and $g(x)=0$ Example $|sin(2-x)|+ |cos(x)| $ are not differentiable at $x=2+2\pi, x=(2n+1)\frac{k}{2}$ Edits As mentioned by @Torsten Schoeneberg in comments, my hypothesis fails!! Grand Edit: $f(x)=|x|$ then $f'(x)=\text{sign}{(x)}$ So let $f(x)$ be a differentiable function, and let $g(x)=|f(x)|$ , then $$g'(x) =\text{sign}(f(x))f'(x)$$ Note that $\text{sign}{(f(x))}=\begin{cases}{ -1 \quad \text{if } f(x)<0\\+1 \quad \text{if } f(x)>0 } \\{ 0 \quad \text{if } f(x)=0} \end{cases}$ Am I going in right direction?","I want to discuss about the differentiability of , where is a differentiable function Example 1 Take , function is clearly not differentiable at . Example 2 Take , function is clearly not differentiable at the point After taking few more examples like , it always seems to be the case that is not differentiable at the points where Observation: One thing is common in all the examples that some portion of lies below axis. So I took another example but is differentiable at the point where Question 1: Am I right in concluding that we can not just say in general setting that is not differentiable at the points where ? When can we(I mean under what conditions can we )conclude that is differentiable at points where . My hypothesis is that graph of should lie below axis. Question 2: Let and be two differentiable function, when can we conclude that is not differentiable at the  points where and Example are not differentiable at Edits As mentioned by @Torsten Schoeneberg in comments, my hypothesis fails!! Grand Edit: then So let be a differentiable function, and let , then Note that Am I going in right direction?","g(x)=|f(x)| f f(x)=|x| x=0 f(x)=|\sin(x)| x=n\pi |x-1|, |\cos(x)| |f(x)| f(x)=0 f(x) x f(x)=x^2 |f(x)| f(x)=0 |f(x)| f(x)=0 |f(x)| f(x)=0 f x f(x) g(x) |f(x)|+|g(x)| f(x)=0 g(x)=0 |sin(2-x)|+ |cos(x)|  x=2+2\pi, x=(2n+1)\frac{k}{2} f(x)=|x| f'(x)=\text{sign}{(x)} f(x) g(x)=|f(x)| g'(x) =\text{sign}(f(x))f'(x) \text{sign}{(f(x))}=\begin{cases}{ -1 \quad \text{if } f(x)<0\\+1 \quad \text{if } f(x)>0 } \\{ 0 \quad \text{if } f(x)=0} \end{cases}","['real-analysis', 'general-topology']"
78,How is 0 a limit point of $\{1/n\}_{n=1}^{\infty}$?,How is 0 a limit point of ?,\{1/n\}_{n=1}^{\infty},"I'm working through Principles of Topology and can't wrap my head around how $0$ is a limit point of $\{1/n\}_{n=1}^{\infty}$. For reference, this is on page 44 of the 2016 edition. The book provides the following definition/theorem & proof: Definition : A point x in $\mathbb{R}$ is a limit point or accumulation point of a subset A of $\mathbb{R}$ provided that every open set containing x contains a point of A distinct from x. Theorem 2.9: A real number x is a limit point of a subset A of $\mathbb{R}$ if and only if $d(x, A\setminus\{x\}) = 0$. Proof: Suppose first that x is a limit point of A and let $\epsilon$ be a positive number. Then the open set $(x-\epsilon, x+\epsilon)$ contains a point y of A distinct from x. Since $y\in(x-\epsilon,x+\epsilon)$, then $d(x,y) < \epsilon$ so $d(x, A\setminus\{x\}) < \epsilon$. Since the latter inequality holds for all $\epsilon>0$, then $d(x, A\setminus\{x\}) = 0$. Suppose now that $d(x, A\setminus\{x\}) = 0$ and consider an open set O containing x. Then O contains an open interval $(x-\delta, x+\delta)$ for some positive number $\delta$. Since $d(x, A\setminus\{x\}) < \delta$ and the interval $(x-\delta, x+\delta)$ consists precisely of all points at a distance less than $\delta$ from x, then $(x-\delta, x+\delta)$ must contain a point z in $A\setminus\{x\}$. Thus $$z \in (x-\delta, x+\delta) \subset O$$ and $z \neq x$ since $z\in$ $A\setminus\{x\}$. Hence every open set containing x contains a point of A distinct from x, and x is a limit point of A. $$\tag*{$\Box$}$$ Additionally, the book defines distance between a real number and a non-empty subset of $\mathbb{R}$ as: $$d(a, B) = glb\{|a-b|:b \in B\}$$ and provides a few examples, e.g.: $d(0, [1,2]) = d(0, (1,2)) = 1$ $d(1, [1,2]) = d(1, (1,2)) = 0$ So far, so good. The book then notes: $0$ is the only limit point of $\{1/n\}_{n=1}^{\infty}$. Note in this case that the limit point is outside the set. My question is... how? Say $x = 0$ and you choose $\epsilon = 3$, and you follow along the proof: $$(x-\epsilon, x+\epsilon) = \{x \in \mathbb{R}: (x-\epsilon) < x < (x+\epsilon)\}$$ which certainly does contain a point $y = 1$ where $y \in \{1/n\}_{n=1}^{\infty}$, with $y$ being distinct from $x$. What I don't understand: if $A = \{1/n\}_{n=1}^{\infty}$, how does $d(x, A\setminus\{x\}) = 0$? As far as I can tell, since $0 \notin A$, then $A\setminus\{x\}$ is equivalent to $\{1/n\}_{n=1}^{\infty}$, and the distance between $0$ and $A$ would be $1$: $$d(0, [1, \frac 12, \frac 13, \frac 14 ...)) = 1$$ What might I be missing here?","I'm working through Principles of Topology and can't wrap my head around how $0$ is a limit point of $\{1/n\}_{n=1}^{\infty}$. For reference, this is on page 44 of the 2016 edition. The book provides the following definition/theorem & proof: Definition : A point x in $\mathbb{R}$ is a limit point or accumulation point of a subset A of $\mathbb{R}$ provided that every open set containing x contains a point of A distinct from x. Theorem 2.9: A real number x is a limit point of a subset A of $\mathbb{R}$ if and only if $d(x, A\setminus\{x\}) = 0$. Proof: Suppose first that x is a limit point of A and let $\epsilon$ be a positive number. Then the open set $(x-\epsilon, x+\epsilon)$ contains a point y of A distinct from x. Since $y\in(x-\epsilon,x+\epsilon)$, then $d(x,y) < \epsilon$ so $d(x, A\setminus\{x\}) < \epsilon$. Since the latter inequality holds for all $\epsilon>0$, then $d(x, A\setminus\{x\}) = 0$. Suppose now that $d(x, A\setminus\{x\}) = 0$ and consider an open set O containing x. Then O contains an open interval $(x-\delta, x+\delta)$ for some positive number $\delta$. Since $d(x, A\setminus\{x\}) < \delta$ and the interval $(x-\delta, x+\delta)$ consists precisely of all points at a distance less than $\delta$ from x, then $(x-\delta, x+\delta)$ must contain a point z in $A\setminus\{x\}$. Thus $$z \in (x-\delta, x+\delta) \subset O$$ and $z \neq x$ since $z\in$ $A\setminus\{x\}$. Hence every open set containing x contains a point of A distinct from x, and x is a limit point of A. $$\tag*{$\Box$}$$ Additionally, the book defines distance between a real number and a non-empty subset of $\mathbb{R}$ as: $$d(a, B) = glb\{|a-b|:b \in B\}$$ and provides a few examples, e.g.: $d(0, [1,2]) = d(0, (1,2)) = 1$ $d(1, [1,2]) = d(1, (1,2)) = 0$ So far, so good. The book then notes: $0$ is the only limit point of $\{1/n\}_{n=1}^{\infty}$. Note in this case that the limit point is outside the set. My question is... how? Say $x = 0$ and you choose $\epsilon = 3$, and you follow along the proof: $$(x-\epsilon, x+\epsilon) = \{x \in \mathbb{R}: (x-\epsilon) < x < (x+\epsilon)\}$$ which certainly does contain a point $y = 1$ where $y \in \{1/n\}_{n=1}^{\infty}$, with $y$ being distinct from $x$. What I don't understand: if $A = \{1/n\}_{n=1}^{\infty}$, how does $d(x, A\setminus\{x\}) = 0$? As far as I can tell, since $0 \notin A$, then $A\setminus\{x\}$ is equivalent to $\{1/n\}_{n=1}^{\infty}$, and the distance between $0$ and $A$ would be $1$: $$d(0, [1, \frac 12, \frac 13, \frac 14 ...)) = 1$$ What might I be missing here?",,"['real-analysis', 'general-topology', 'metric-spaces']"
79,Does the domain of integration have to be convex?,Does the domain of integration have to be convex?,,"The following theorem is straightforward to prove if we ignore the convexity of the domain : Suppose that $\text{R}$ is a convex region in the plane and that the function $g : \text{R} \to \mathbb{R}$ has continuous bounded partial derivatives. Show that the surface $S = {(x, y, g(x, y)) \ | \ (x, y) \ \text{in} \ \text{R}}\}$ has area equal to that of $\text{R}$ if and only if the function $g : \text{R} \to \mathbb{R}$ is constant. For the proof I used the fact that $\iint_D \Big(\sqrt{1+f_x^2+f_y^2} -1\Big)\,dA = 0 \iff f_x=0 =f_y \iff f = \text{const.}$ But why it is supposed that the region to be convex? I can't see why it is necessary to use convexity of the region in the proof. Is there a counterexample i.e. a non-convex region that the mentioned theorem fails to hold?","The following theorem is straightforward to prove if we ignore the convexity of the domain : Suppose that $\text{R}$ is a convex region in the plane and that the function $g : \text{R} \to \mathbb{R}$ has continuous bounded partial derivatives. Show that the surface $S = {(x, y, g(x, y)) \ | \ (x, y) \ \text{in} \ \text{R}}\}$ has area equal to that of $\text{R}$ if and only if the function $g : \text{R} \to \mathbb{R}$ is constant. For the proof I used the fact that $\iint_D \Big(\sqrt{1+f_x^2+f_y^2} -1\Big)\,dA = 0 \iff f_x=0 =f_y \iff f = \text{const.}$ But why it is supposed that the region to be convex? I can't see why it is necessary to use convexity of the region in the proof. Is there a counterexample i.e. a non-convex region that the mentioned theorem fails to hold?",,"['real-analysis', 'integration', 'convex-analysis', 'examples-counterexamples']"
80,Example of a sequence of functions where the limit cannot be interchanged,Example of a sequence of functions where the limit cannot be interchanged,,"Give an example of a sequence of continuous functions $f_n$ on $[0,1]$ with $f_n$ converges pointwise to a continuous function $f$ such that the following relation does't hold: $$\lim_{n \rightarrow \infty} \lim_{x \rightarrow 0} f_n(x)=\lim_{x \rightarrow 0}\lim_{n \rightarrow \infty} f_n(x)$$ I know such a convergence is not uniform. I already tried with this one: $f_n(x)= 2nx e^{-nx^2}$ . Actually this one satisfies the given limit condition even though the convergence is not uniform! Any hint?",Give an example of a sequence of continuous functions on with converges pointwise to a continuous function such that the following relation does't hold: I know such a convergence is not uniform. I already tried with this one: . Actually this one satisfies the given limit condition even though the convergence is not uniform! Any hint?,"f_n [0,1] f_n f \lim_{n \rightarrow \infty} \lim_{x \rightarrow 0} f_n(x)=\lim_{x \rightarrow 0}\lim_{n \rightarrow \infty} f_n(x) f_n(x)= 2nx e^{-nx^2}",['real-analysis']
81,"Proof that the $\sup \left(a,b\right) = b$.",Proof that the .,"\sup \left(a,b\right) = b","I'm trying to prove that the least upper bound of the interval, $(a,b)$, is $b$, but am struggling with establishing that $b$ is the least such upper bound. So, using Rudin's definition, I want to prove both that $b$ is an upper bound of the interval and that it's the lowest such upper bound. First, by definition, $(a,b) = \{x \in \mathbb{R} \mid a < x < b\}$. So, $\forall x \in (a,b), x < b$, so $b$ is clearly an upper bound. Second -- where I find myself confused, though this is my best attempt -- let's assume, for a contradiction, that $\gamma \neq b$ is the least upper bound of $(a,b)$. So, $x \leq \gamma$ for all $x \in (a,b)$. Then, since $b$ is clearly an upper bound of $(a,b)$, $b \geq \gamma$. But, if $b > \gamma$, then $\gamma \in (a,b)$ by definition. But, if this is the case, then we can find some larger value in $(a,b)$, i.e., by taking the average of $b$ and $\gamma$, producing some value, $\alpha$, such that $\gamma < \alpha < b$. Thus, $\gamma$ is not an upper bound of this set, so it must be the case that $b = \gamma$. Thus, $b$ is the least upper bound of $(a,b)$. How does this look?","I'm trying to prove that the least upper bound of the interval, $(a,b)$, is $b$, but am struggling with establishing that $b$ is the least such upper bound. So, using Rudin's definition, I want to prove both that $b$ is an upper bound of the interval and that it's the lowest such upper bound. First, by definition, $(a,b) = \{x \in \mathbb{R} \mid a < x < b\}$. So, $\forall x \in (a,b), x < b$, so $b$ is clearly an upper bound. Second -- where I find myself confused, though this is my best attempt -- let's assume, for a contradiction, that $\gamma \neq b$ is the least upper bound of $(a,b)$. So, $x \leq \gamma$ for all $x \in (a,b)$. Then, since $b$ is clearly an upper bound of $(a,b)$, $b \geq \gamma$. But, if $b > \gamma$, then $\gamma \in (a,b)$ by definition. But, if this is the case, then we can find some larger value in $(a,b)$, i.e., by taking the average of $b$ and $\gamma$, producing some value, $\alpha$, such that $\gamma < \alpha < b$. Thus, $\gamma$ is not an upper bound of this set, so it must be the case that $b = \gamma$. Thus, $b$ is the least upper bound of $(a,b)$. How does this look?",,['real-analysis']
82,(Non-) Convergence of $\frac{1}{n} \sum_{k=0}^{n - 1} \exp\left(2i \pi [\frac{3 + \sqrt{5}}{2}]^k\right)$ when $n \to +\infty$,(Non-) Convergence of  when,\frac{1}{n} \sum_{k=0}^{n - 1} \exp\left(2i \pi [\frac{3 + \sqrt{5}}{2}]^k\right) n \to +\infty,"Let be $$\forall n > 0, S_n = \dfrac{1}{n} \sum\limits_{k=0}^{n - 1} \exp(2i\pi u_k),\quad  \forall k \geq 0,  u_k = \left(\dfrac{3 + \sqrt{5}}{2}\right)^k$$ I would like to prove or disprove the convergence of $S_n$ as $n \to +\infty$. What I have tried: First, I tried to express $u_k$ as $(\phi^{2k})_k$ with $\phi$ the golden ratio and use $\phi^2 = 1 + \phi$ in the exponential, but with no success. Second, I tried to establish lower / upper bounds of $S_n$ or study $S_{2n}, S_{2n + 1}$ with no success. I think I could make use of the irrationality of $\phi$ but would prefer to avoid a proof based on equipartition (as this is what I'm proving in the end). Also, this problem is whether $(\exp(2i\pi u_k))_k$ is Cesaro-summable.","Let be $$\forall n > 0, S_n = \dfrac{1}{n} \sum\limits_{k=0}^{n - 1} \exp(2i\pi u_k),\quad  \forall k \geq 0,  u_k = \left(\dfrac{3 + \sqrt{5}}{2}\right)^k$$ I would like to prove or disprove the convergence of $S_n$ as $n \to +\infty$. What I have tried: First, I tried to express $u_k$ as $(\phi^{2k})_k$ with $\phi$ the golden ratio and use $\phi^2 = 1 + \phi$ in the exponential, but with no success. Second, I tried to establish lower / upper bounds of $S_n$ or study $S_{2n}, S_{2n + 1}$ with no success. I think I could make use of the irrationality of $\phi$ but would prefer to avoid a proof based on equipartition (as this is what I'm proving in the end). Also, this problem is whether $(\exp(2i\pi u_k))_k$ is Cesaro-summable.",,"['real-analysis', 'convergence-divergence', 'golden-ratio', 'cesaro-summable']"
83,Linearity of indefinite integrals,Linearity of indefinite integrals,,"I am trying to make sense of 'linearity' of indefinite integrals. Let us restrict to the 1-dimensional case. My point is that $\int 0\, dx = C \in \mathbb{R}$, so I cannot really say that $\int$ is a linear operator. Indeed, linearity of $f \colon V \rightarrow W$ ($V,W$ vector spaces) implies $f(0) = 0$. In order to define $\int \colon V \rightarrow W$ in a good way, I should introduce an equivalence relation $\sim$ on $W$ saying that two elements are in the same equivalence class if they coincide up to a constant. So $\int$ becomes linear as operator $\int \colon V \rightarrow W_{/\sim}$. Assume a primitive of $f$ is $F$. Then $$\int f(x)\,dx = [F(x)] \in W_{/\sim}$$ or, as usual, $F(x)+C, C \in \mathbb{R}$. So I would say that the result of an indefinite integral is actually a coset in some quotient vector space. This even solves the problem that $\int \colon V \rightarrow W $ is not a well defined function. My question is: is this a good way to think, or is there a better one? I have never seen such a thing, neither in a course of Analysis, nor in the books I have read. I am wondering why. It seems a very natural thing to do when introducing indefinite integrals.","I am trying to make sense of 'linearity' of indefinite integrals. Let us restrict to the 1-dimensional case. My point is that $\int 0\, dx = C \in \mathbb{R}$, so I cannot really say that $\int$ is a linear operator. Indeed, linearity of $f \colon V \rightarrow W$ ($V,W$ vector spaces) implies $f(0) = 0$. In order to define $\int \colon V \rightarrow W$ in a good way, I should introduce an equivalence relation $\sim$ on $W$ saying that two elements are in the same equivalence class if they coincide up to a constant. So $\int$ becomes linear as operator $\int \colon V \rightarrow W_{/\sim}$. Assume a primitive of $f$ is $F$. Then $$\int f(x)\,dx = [F(x)] \in W_{/\sim}$$ or, as usual, $F(x)+C, C \in \mathbb{R}$. So I would say that the result of an indefinite integral is actually a coset in some quotient vector space. This even solves the problem that $\int \colon V \rightarrow W $ is not a well defined function. My question is: is this a good way to think, or is there a better one? I have never seen such a thing, neither in a course of Analysis, nor in the books I have read. I am wondering why. It seems a very natural thing to do when introducing indefinite integrals.",,"['real-analysis', 'linear-algebra', 'linear-transformations']"
84,Are there any studies on a sorting function on real line?,Are there any studies on a sorting function on real line?,,"Let assume we have an integer array $[3,4,1,0,7]$, we can sort it into $[0,1,3,4,7]$. There is a sorting function $S: \mathbb{N}^k \to \mathbb{N}^k$ that sort the integer in the array defined as above. If we see an integer array as a function that maps an index set $\mathcal{I} \to \mathbb{N}$, I want to generalize it to the case that $\mathbb{N}$ is replaced by $[0,1]$ of a real line. In this continuum case, are there any studies on the analog to the   sorting function $S$ but on real line in the sense that it ""sorts"" a real-valued function $f(x)$ defined on $[0,1]$? i.e. a function $S': F[0,1] \to M[0,1]$, where: $\mathcal{F}[0,1]$ are all function from $[0,1] \to [0,1]$, $\mathcal{M}[0,1]$ are all monotonic increasing (non-decreasing) function from $[0,1] \to [0,1]$. that somehow preserves the values of $f(x)$. I am not sure how to define what it preserves precisely now (I guess should be related to derivative), just a weird idea. EDIT followed by David C. Ullrich's answer What if we restrict $\mathcal{F}[0,1]$ to be some kind of ""nice"" function, such as differentiable almost everywhere on $[0,1]$?","Let assume we have an integer array $[3,4,1,0,7]$, we can sort it into $[0,1,3,4,7]$. There is a sorting function $S: \mathbb{N}^k \to \mathbb{N}^k$ that sort the integer in the array defined as above. If we see an integer array as a function that maps an index set $\mathcal{I} \to \mathbb{N}$, I want to generalize it to the case that $\mathbb{N}$ is replaced by $[0,1]$ of a real line. In this continuum case, are there any studies on the analog to the   sorting function $S$ but on real line in the sense that it ""sorts"" a real-valued function $f(x)$ defined on $[0,1]$? i.e. a function $S': F[0,1] \to M[0,1]$, where: $\mathcal{F}[0,1]$ are all function from $[0,1] \to [0,1]$, $\mathcal{M}[0,1]$ are all monotonic increasing (non-decreasing) function from $[0,1] \to [0,1]$. that somehow preserves the values of $f(x)$. I am not sure how to define what it preserves precisely now (I guess should be related to derivative), just a weird idea. EDIT followed by David C. Ullrich's answer What if we restrict $\mathcal{F}[0,1]$ to be some kind of ""nice"" function, such as differentiable almost everywhere on $[0,1]$?",,"['real-analysis', 'reference-request', 'monotone-functions']"
85,$\mathcal{C}^{\alpha}$ Besov spaces: Definition,Besov spaces: Definition,\mathcal{C}^{\alpha},"I'm reading an article for my future thesis (I'm a third-year undergraduate) where the authors define the generalized Holder Spaces as a special class of Besov Spaces. Define $\chi,\tilde{\chi}\in C_{c}^{\infty}(\mathbb{R}^d)$ such that Supp$(\chi)\subset B(0,8/3) \setminus B(0,3/4)$ and Supp$(\tilde{\chi}) \subset B(0,4/3),$ and such that $\tilde{\chi}(x)+\sum^{+ \infty}_{k=0} \chi(x/2^k)=1 \quad \forall x\in \mathbb{R}^d$. Define $\chi_{-1}:=\tilde{\chi},$ and $\chi_k(\cdot):=\chi(\cdot/2^k).$ Now, $\forall f \in C^{\infty}(\mathbb{T}^d),$ set $\delta_k(f):=\mathscr{F}^{-1}(\hat{f} \cdot \chi_k$), where $\hat{f}(k):=\int_{\mathbb{T}^d}f(x)e^{-2\pi i k\cdot x} dx$ and $\mathscr{F}^{-1}(g)(x):=\sum_{k\in \mathbb{z}^d} g(k)e^{2 \pi i k\cdot x}.$ Heuristically, $\delta_k(f)$ is just a part of the frequencies of the smooth function $f.$ Now, for every $\alpha \in \mathbb{R},$ we can define the $\mathcal{C}^{\alpha}$ norm of a smooth function, which is $\|f\|_{\mathcal{C}^{\alpha}}:=sup_{k\geq1} 2^{\alpha k} \|\delta_k(f)\|_{L^{\infty}}.$ The space $\mathcal{C}^{\alpha}$ is defined as the completion of $C^{\infty}(\mathbb{T}^d)$ with respect to this norm. The definition given is a bit different from the one which is mostly given in literature: the space of all tempered distributions such that the above-mentioned norm (which is well-defined because the Fourier antitransform of a compactly-supported distribution is a function) is finite. Now, me questions are: Why the $\mathcal{C}^{\alpha}$ norm is finite for every smooth function defined on the torus? Why does it coincide (well, I don't think they precisely coincide, but they should be equivalent or at least generate the same completion) with the classical $\alpha-$Holder norm ($\|f\|_{\mathcal{C}^{\alpha}}=sup_{x\neq y} \frac{|f(x)-f(y)|}{|x-y|^{\alpha}})$? Do the $\mathcal{C}^{\alpha}$ spaces respectively defined via completion of $C^{\infty}$ and via the tempered distributions coincide? The article can be found here .","I'm reading an article for my future thesis (I'm a third-year undergraduate) where the authors define the generalized Holder Spaces as a special class of Besov Spaces. Define $\chi,\tilde{\chi}\in C_{c}^{\infty}(\mathbb{R}^d)$ such that Supp$(\chi)\subset B(0,8/3) \setminus B(0,3/4)$ and Supp$(\tilde{\chi}) \subset B(0,4/3),$ and such that $\tilde{\chi}(x)+\sum^{+ \infty}_{k=0} \chi(x/2^k)=1 \quad \forall x\in \mathbb{R}^d$. Define $\chi_{-1}:=\tilde{\chi},$ and $\chi_k(\cdot):=\chi(\cdot/2^k).$ Now, $\forall f \in C^{\infty}(\mathbb{T}^d),$ set $\delta_k(f):=\mathscr{F}^{-1}(\hat{f} \cdot \chi_k$), where $\hat{f}(k):=\int_{\mathbb{T}^d}f(x)e^{-2\pi i k\cdot x} dx$ and $\mathscr{F}^{-1}(g)(x):=\sum_{k\in \mathbb{z}^d} g(k)e^{2 \pi i k\cdot x}.$ Heuristically, $\delta_k(f)$ is just a part of the frequencies of the smooth function $f.$ Now, for every $\alpha \in \mathbb{R},$ we can define the $\mathcal{C}^{\alpha}$ norm of a smooth function, which is $\|f\|_{\mathcal{C}^{\alpha}}:=sup_{k\geq1} 2^{\alpha k} \|\delta_k(f)\|_{L^{\infty}}.$ The space $\mathcal{C}^{\alpha}$ is defined as the completion of $C^{\infty}(\mathbb{T}^d)$ with respect to this norm. The definition given is a bit different from the one which is mostly given in literature: the space of all tempered distributions such that the above-mentioned norm (which is well-defined because the Fourier antitransform of a compactly-supported distribution is a function) is finite. Now, me questions are: Why the $\mathcal{C}^{\alpha}$ norm is finite for every smooth function defined on the torus? Why does it coincide (well, I don't think they precisely coincide, but they should be equivalent or at least generate the same completion) with the classical $\alpha-$Holder norm ($\|f\|_{\mathcal{C}^{\alpha}}=sup_{x\neq y} \frac{|f(x)-f(y)|}{|x-y|^{\alpha}})$? Do the $\mathcal{C}^{\alpha}$ spaces respectively defined via completion of $C^{\infty}$ and via the tempered distributions coincide? The article can be found here .",,"['real-analysis', 'functional-analysis', 'besov-space']"
86,"If $a_{nk} \leqslant n a_k$ for all $k \geqslant 0$ and $n > 0$, does $a_n/n$ converge?","If  for all  and , does  converge?",a_{nk} \leqslant n a_k k \geqslant 0 n > 0 a_n/n,"A sequence $a_n$ is called subadditive if $a_{n+k} \leqslant a_n + a_k$ for all $n, k \in \mathbb{N}$. Fekete's subadditive lemma states that if a sequence $a_n$ is subadditive, then the sequence $a_n/n$ converges.  What about a sequence satisfying the weaker condition $a_{nk} \leqslant n a_k$ for all $k \geqslant 0$ and $n > 0$?","A sequence $a_n$ is called subadditive if $a_{n+k} \leqslant a_n + a_k$ for all $n, k \in \mathbb{N}$. Fekete's subadditive lemma states that if a sequence $a_n$ is subadditive, then the sequence $a_n/n$ converges.  What about a sequence satisfying the weaker condition $a_{nk} \leqslant n a_k$ for all $k \geqslant 0$ and $n > 0$?",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
87,"If a function preserves integrability, must it have linear growth?","If a function preserves integrability, must it have linear growth?",,"This question is the converse of Sufficient conditions on $F$ such that $F(X)\in\mathcal{L}^{p}$ for all $X\in\mathcal{L}^{p}$ . Let $F : \mathbb{R} \to \mathbb{R}$ be a Borel function with the following property: for every Borel probability measure $\mu$ on $\mathbb{R}$ having finite first moment (i.e. $\int |x|\,d\mu < \infty$), we have $\int |F|\,d\mu < \infty$.  (In probability language, this says that whenever $X$ is an integrable random variable on any probability space, then $F(X)$ is integrable too; let $\mu$ be the law of $X$.) Must $F$ have linear growth?  That is, does there necessarily exist a constant $C$ such that $|F(x)| \le C(1+|x|)$ for all $x \in \mathbb{R}$? I thought about a closed graph theorem argument, but here the map $X \mapsto F(X)$ is a nonlinear mapping of $L^1$.","This question is the converse of Sufficient conditions on $F$ such that $F(X)\in\mathcal{L}^{p}$ for all $X\in\mathcal{L}^{p}$ . Let $F : \mathbb{R} \to \mathbb{R}$ be a Borel function with the following property: for every Borel probability measure $\mu$ on $\mathbb{R}$ having finite first moment (i.e. $\int |x|\,d\mu < \infty$), we have $\int |F|\,d\mu < \infty$.  (In probability language, this says that whenever $X$ is an integrable random variable on any probability space, then $F(X)$ is integrable too; let $\mu$ be the law of $X$.) Must $F$ have linear growth?  That is, does there necessarily exist a constant $C$ such that $|F(x)| \le C(1+|x|)$ for all $x \in \mathbb{R}$? I thought about a closed graph theorem argument, but here the map $X \mapsto F(X)$ is a nonlinear mapping of $L^1$.",,"['real-analysis', 'integration', 'probability-theory', 'measure-theory']"
88,"Inverse of a map $T_{(p,q)}(X \times Y) \to T_p X \times T_p Y$",Inverse of a map,"T_{(p,q)}(X \times Y) \to T_p X \times T_p Y","I'm trying to show that the maps defined in https://math.stackexchange.com/a/413846/500094 are mutual inverses. I have problem with one direction: $(g\circ f)(v)=g(d(\pi_X)_{(p,q)}v,d(\pi_Y)_{(p,q)}v)=d(\iota_X)_p(d(\pi_X)_{(p,q)}v)+d(\iota_Y)_q(d(\pi_X)_{(p,q)}v)=\\d(\iota_X\circ\pi_X)_{(p,q)}v+d(\iota_Y\circ\pi_Y)_{(p,q)}v=v+v=2v\ne v$ At which point am I mistaken?","I'm trying to show that the maps defined in https://math.stackexchange.com/a/413846/500094 are mutual inverses. I have problem with one direction: $(g\circ f)(v)=g(d(\pi_X)_{(p,q)}v,d(\pi_Y)_{(p,q)}v)=d(\iota_X)_p(d(\pi_X)_{(p,q)}v)+d(\iota_Y)_q(d(\pi_X)_{(p,q)}v)=\\d(\iota_X\circ\pi_X)_{(p,q)}v+d(\iota_Y\circ\pi_Y)_{(p,q)}v=v+v=2v\ne v$ At which point am I mistaken?",,"['real-analysis', 'functions', 'function-and-relation-composition']"
89,On the equivalence between the Kullback-Leiber divergence and the $L^2$ distance.,On the equivalence between the Kullback-Leiber divergence and the  distance.,L^2,"There is a question very similar to this one here , matter of fact I was looking to see if this equivalence was true and I stumbled upon that question. For the sake of completeness I quote the question: Let $P$ and $Q$ be two probability measures with densities $p$ and $q$ with respect to the Lebesgue measure on [0,1] such that: $0<a\leq p(x)\leq b$ , $0<a\leq q(x)\leq b$ $\forall x\in $ [0,1] $a$ , $b>0$ are constants. Show that the Kullback distance $K(P,Q)$ is equivalent to the squared $L^2$ distance between densities $p$ and $q$ . where I assume the Op is talking about the standard Kullback leiber divergence $$K(P,Q):= \int_{- \infty}^{+\infty} p(x) \log \frac{p(x)}{q(x)} \, dx$$ that is not, in fact, a distance (it is missing the symmetry property). Nonetheless we want to show there are two constants $\alpha$ and $\beta$ such that $\alpha||p-q||_{L^2}^{2}\leq K(P,Q)\leq \beta ||p-q||_{L^2}^{2}$ . How does one show the lower bound? I assume what the OP of the linked question is doing for the upper bound is that he is expanding $\log(q(x))$ at the point $p(x)$ resulting in $$\int_{0}^{1} p(x) \log \frac{p(x)}{q(x)} \, dx \le \int_{0}^{1}-p(x) ( q(x) - p(x)) \frac{1}{p(x)} + \frac{1}{2 p(x)^2}( q(x) - p(x))^2 \, dx \le $$ $$\int_{0}^{1} ( q(x) - p(x)) \, dx + \frac{1}{2 a^2} \int_{0}^{1}(q(x) - p(x))^2 \, dx = \frac{1}{2 a^2} \int_{0}^{1}(q(x) - p(x))^2 \, dx $$ where we have been assuming continuity of $q(x)$ . But the Op of the linked question obtains some extra constants before the $L^2$ norm so I am afraid my calculation are incorrect (as they often have been) could somebody point out my mistake?","There is a question very similar to this one here , matter of fact I was looking to see if this equivalence was true and I stumbled upon that question. For the sake of completeness I quote the question: Let and be two probability measures with densities and with respect to the Lebesgue measure on [0,1] such that: , [0,1] , are constants. Show that the Kullback distance is equivalent to the squared distance between densities and . where I assume the Op is talking about the standard Kullback leiber divergence that is not, in fact, a distance (it is missing the symmetry property). Nonetheless we want to show there are two constants and such that . How does one show the lower bound? I assume what the OP of the linked question is doing for the upper bound is that he is expanding at the point resulting in where we have been assuming continuity of . But the Op of the linked question obtains some extra constants before the norm so I am afraid my calculation are incorrect (as they often have been) could somebody point out my mistake?","P Q p q 0<a\leq p(x)\leq b 0<a\leq q(x)\leq b \forall x\in  a b>0 K(P,Q) L^2 p q K(P,Q):= \int_{- \infty}^{+\infty} p(x) \log \frac{p(x)}{q(x)} \, dx \alpha \beta \alpha||p-q||_{L^2}^{2}\leq K(P,Q)\leq \beta ||p-q||_{L^2}^{2} \log(q(x)) p(x) \int_{0}^{1} p(x) \log \frac{p(x)}{q(x)} \, dx \le \int_{0}^{1}-p(x) ( q(x) - p(x)) \frac{1}{p(x)} + \frac{1}{2 p(x)^2}( q(x) - p(x))^2 \, dx \le  \int_{0}^{1} ( q(x) - p(x)) \, dx + \frac{1}{2 a^2} \int_{0}^{1}(q(x) - p(x))^2 \, dx = \frac{1}{2 a^2} \int_{0}^{1}(q(x) - p(x))^2 \, dx  q(x) L^2","['real-analysis', 'functional-analysis', 'probability-theory', 'measure-theory']"
90,Analytical Way of Estimating Sums of Floor Functions,Analytical Way of Estimating Sums of Floor Functions,,"Hi Math Stack Exchange, I'm working on a problem that involves the difference between a sum series of floor functions. I have tried taking the more standard number theory approach by looking at remainder classes and modular arithmetic but haven't had real success so I'm hoping to take an analytical approach and was looking for help. Let, $$f(L) = \sum_{k=2}^{\frac{L}{2}}{\lfloor{\frac{L}{k}}\rfloor}$$ and $$\Delta(c,L) = f(L+c) - f(L) = \sum_{k=2}^{\frac{L}{2}}{\lfloor{\frac{L+c}{k}\rfloor - \lfloor\frac{L}{k}}\rfloor}$$ For the problem we can assume c is an integer and is very very small in comparison to L. So given the above equations I have a couple questions and any help on any of them would be great! 1) Does there already exist a quick identity for f(L) or $\Delta(c,L)$? 2) If there doesn't exist a nice identity, is there an analytical way to estimate f(L) or $\Delta(c,L)$ or approximate them? Thank you guys for taking the time to look over this!","Hi Math Stack Exchange, I'm working on a problem that involves the difference between a sum series of floor functions. I have tried taking the more standard number theory approach by looking at remainder classes and modular arithmetic but haven't had real success so I'm hoping to take an analytical approach and was looking for help. Let, $$f(L) = \sum_{k=2}^{\frac{L}{2}}{\lfloor{\frac{L}{k}}\rfloor}$$ and $$\Delta(c,L) = f(L+c) - f(L) = \sum_{k=2}^{\frac{L}{2}}{\lfloor{\frac{L+c}{k}\rfloor - \lfloor\frac{L}{k}}\rfloor}$$ For the problem we can assume c is an integer and is very very small in comparison to L. So given the above equations I have a couple questions and any help on any of them would be great! 1) Does there already exist a quick identity for f(L) or $\Delta(c,L)$? 2) If there doesn't exist a nice identity, is there an analytical way to estimate f(L) or $\Delta(c,L)$ or approximate them? Thank you guys for taking the time to look over this!",,"['real-analysis', 'number-theory', 'analytic-number-theory', 'ceiling-and-floor-functions', 'divisor-counting-function']"
91,Let $\{f_n\}^\infty_{n=1}$ be a sequence of continuous real valued functions defined on $\mathbb{R}$,Let  be a sequence of continuous real valued functions defined on,\{f_n\}^\infty_{n=1} \mathbb{R},"Let $\{f_n\}^\infty_{n=1}$  be a sequence of continuous real valued functions defined on $\mathbb{R}$ which converges pointwise to a continuous real valued function $f$. d Which of the following statements are true? a. If $0\le f_n \le f$ for all $n\in \mathbb{N}$, then $$\lim_{n\to \infty} \int_\infty^{-\infty} f_n(t)\,dt=\int_\infty^{-\infty } f(t) \, dt$$ b. If $|f_n(t)|\le |\sin t|$ for all $t\in \mathbb{R}$ and for all $n\in \mathbb{N}$, then   $$\lim_{n\to \infty} \int_\infty^{-\infty } f_n(t) \, dt=\int_\infty ^{-\infty } f(t) \, dt$$ c. If $|f_n(t)|\le e^t$ for all $t\in \mathbb{R}$ and for all $n\in \mathbb{N}$, then for all $a,b \in \mathbb{R}$ . $a<b$ $$\lim_{n\to \infty} \int_b^a f_n(t) \, dt=\int_b^a f(t) \, dt$$ since sequence of continuous real valued functions defined on $\mathbb{R}$ which converges pointwise to a continuous real valued function $f$ so 1 is true (but ia m not sure) can you hlep me with other options too..thank you","Let $\{f_n\}^\infty_{n=1}$  be a sequence of continuous real valued functions defined on $\mathbb{R}$ which converges pointwise to a continuous real valued function $f$. d Which of the following statements are true? a. If $0\le f_n \le f$ for all $n\in \mathbb{N}$, then $$\lim_{n\to \infty} \int_\infty^{-\infty} f_n(t)\,dt=\int_\infty^{-\infty } f(t) \, dt$$ b. If $|f_n(t)|\le |\sin t|$ for all $t\in \mathbb{R}$ and for all $n\in \mathbb{N}$, then   $$\lim_{n\to \infty} \int_\infty^{-\infty } f_n(t) \, dt=\int_\infty ^{-\infty } f(t) \, dt$$ c. If $|f_n(t)|\le e^t$ for all $t\in \mathbb{R}$ and for all $n\in \mathbb{N}$, then for all $a,b \in \mathbb{R}$ . $a<b$ $$\lim_{n\to \infty} \int_b^a f_n(t) \, dt=\int_b^a f(t) \, dt$$ since sequence of continuous real valued functions defined on $\mathbb{R}$ which converges pointwise to a continuous real valued function $f$ so 1 is true (but ia m not sure) can you hlep me with other options too..thank you",,"['real-analysis', 'sequences-and-series', 'uniform-convergence', 'uniform-continuity']"
92,How to find $\int \frac{e^{-x^2}}{x^2 + 1} dx$?,How to find ?,\int \frac{e^{-x^2}}{x^2 + 1} dx,"I have a question about improper integrals: How can we find $\lim_{n \rightarrow +\infty}\int_{-n}^{n} \frac{1 - e^{-nx^2}}{x^2(1+nx^2)}dx$? $\textbf{Some effort:}$ $\lim_{n \rightarrow +\infty}\int_{-n}^{n} \frac{1}{n} \frac{1 - e^{-nx^2}}{x^2(1+nx^2)}dx = \lim_{n \rightarrow +\infty} \frac{2}{n}\int_{0}^{n}  \frac{1 - e^{-nx^2}}{x^2(1+nx^2)}dx $ $~~~~~~~~~\textbf{(1)}$ By setting $nx^2 = u$, we have $dx = \frac{1}{2\sqrt{n}} \times \frac{1}{\sqrt{u}}$ and $x = \frac{\sqrt{u}}{\sqrt{n}}$. So by substituting these in $\textbf{(1)}$ we have (I will not put bounds and at the end will come back to the initial bounds and also I will drop the constant in integrals) $\textbf{(1)} = \lim_{n \rightarrow +\infty} \frac{2}{n} \int   \frac{1 - e^{-u}}{\frac{u}{n}(1+u)} \times \frac{1}{2 \sqrt{n}}\times \frac{1}{ \sqrt{u}} du = \lim_{n \rightarrow +\infty}  \frac{1}{\sqrt{n}} \int  \frac{1 - e^{-u}}{ u \sqrt{u}(1+u)}   du $ $= \lim_{n \rightarrow +\infty}  \frac{1}{\sqrt{n}} \int  ( \frac{1 }{ u \sqrt{u}(1+u)}  - \frac{e^{-u} }{ u \sqrt{u}(1+u)}) du$ $~~~~~~~~~\textbf{(2)}$ By setting $\sqrt{u} = v$, we have $\frac{1}{2\sqrt{u}}du = dv$. So by substituting these in $\textbf{(2)}$ we have $\textbf{(2)} = \lim_{n \rightarrow +\infty}  \frac{2}{\sqrt{n}} \int (\frac{1}{v^2(1+v^2)} - \frac{e^{-v^2}}{v^2(1+v^2)} dv) $ $~~~~~~~~~\textbf{(3)}$ $\textbf{(3)}= \lim_{n \rightarrow +\infty}  \frac{2}{\sqrt{n}} \int ( \frac{1}{v^2} - \frac{1}{1+v^2}  - \frac{e^{-v^2}}{v^2} +  \frac{e^{-v^2}}{v^2 + 1})  dv$ Now we will calculate each term separately. First part: For to find $\int \frac{1}{v^2} dv $, by setting $k_1=-\frac{1}{v}$, we have $dk_1 =\frac{1}{v^2} dv$ and so we have $\int \frac{1}{v^2} dv = \int dk_1= k_1= -\frac{1}{v}= -\frac{1}{\sqrt{u}}= -\frac{1}{\sqrt{nx^2}} = -\frac{1}{\sqrt{n}|x|} $ Second part: For to find $-\int \frac{1}{1+v^2} dv$, by setting  $k_2 = \arctan(v)$, we have $dk_2 = \frac{1}{1 + v^2}dv$ and so we have $-\int \frac{1}{1+v^2} dv = -int dk_2= -k_2= -\arctan(v) = -\arctan(\sqrt{u}) = -\arctan(\sqrt{nx^2}) = -\arctan(\sqrt{n}|x|) $ Third part: For to find $-\int \frac{e^{-v^2}}{v^2} dv $, by setting $\begin{cases}                k_3=e^{-v^2}\\                -\frac{1}{v^2}=dk_4             \end{cases}$ we will have $\begin{cases}                dk_3=-2ve^{-v^2}\\                k_4= \frac{1}{v}             \end{cases}$ and our integral will transform to $-\int \frac{e^{-v^2}}{v^2} dv = \frac{e^{-v^2}}{v} - 2 \int e^{-v^2} dv =  \frac{e^{-v^2}}{v} - 2(\frac{\sqrt{\pi}}{2}) = \frac{e^{-v^2}}{v} - \sqrt{\pi} =  \frac{e^{-(\sqrt{u})^2}}{\sqrt{u}} - \sqrt{\pi} = \frac{e^{-u}}{\sqrt{u}} - \sqrt{\pi} =\frac{e^{-nx^2}}{\sqrt{nx^2}} - \sqrt{\pi}=\frac{e^{-nx^2}}{\sqrt{n}|x|} - \sqrt{\pi}$ Forth part: For to find $\int \frac{e^{-v^2}}{v^2 + 1} dv$, I cannot find it! Can someone please help me to find $\int \frac{e^{-v^2}}{v^2 + 1} dv$? Thanks!","I have a question about improper integrals: How can we find $\lim_{n \rightarrow +\infty}\int_{-n}^{n} \frac{1 - e^{-nx^2}}{x^2(1+nx^2)}dx$? $\textbf{Some effort:}$ $\lim_{n \rightarrow +\infty}\int_{-n}^{n} \frac{1}{n} \frac{1 - e^{-nx^2}}{x^2(1+nx^2)}dx = \lim_{n \rightarrow +\infty} \frac{2}{n}\int_{0}^{n}  \frac{1 - e^{-nx^2}}{x^2(1+nx^2)}dx $ $~~~~~~~~~\textbf{(1)}$ By setting $nx^2 = u$, we have $dx = \frac{1}{2\sqrt{n}} \times \frac{1}{\sqrt{u}}$ and $x = \frac{\sqrt{u}}{\sqrt{n}}$. So by substituting these in $\textbf{(1)}$ we have (I will not put bounds and at the end will come back to the initial bounds and also I will drop the constant in integrals) $\textbf{(1)} = \lim_{n \rightarrow +\infty} \frac{2}{n} \int   \frac{1 - e^{-u}}{\frac{u}{n}(1+u)} \times \frac{1}{2 \sqrt{n}}\times \frac{1}{ \sqrt{u}} du = \lim_{n \rightarrow +\infty}  \frac{1}{\sqrt{n}} \int  \frac{1 - e^{-u}}{ u \sqrt{u}(1+u)}   du $ $= \lim_{n \rightarrow +\infty}  \frac{1}{\sqrt{n}} \int  ( \frac{1 }{ u \sqrt{u}(1+u)}  - \frac{e^{-u} }{ u \sqrt{u}(1+u)}) du$ $~~~~~~~~~\textbf{(2)}$ By setting $\sqrt{u} = v$, we have $\frac{1}{2\sqrt{u}}du = dv$. So by substituting these in $\textbf{(2)}$ we have $\textbf{(2)} = \lim_{n \rightarrow +\infty}  \frac{2}{\sqrt{n}} \int (\frac{1}{v^2(1+v^2)} - \frac{e^{-v^2}}{v^2(1+v^2)} dv) $ $~~~~~~~~~\textbf{(3)}$ $\textbf{(3)}= \lim_{n \rightarrow +\infty}  \frac{2}{\sqrt{n}} \int ( \frac{1}{v^2} - \frac{1}{1+v^2}  - \frac{e^{-v^2}}{v^2} +  \frac{e^{-v^2}}{v^2 + 1})  dv$ Now we will calculate each term separately. First part: For to find $\int \frac{1}{v^2} dv $, by setting $k_1=-\frac{1}{v}$, we have $dk_1 =\frac{1}{v^2} dv$ and so we have $\int \frac{1}{v^2} dv = \int dk_1= k_1= -\frac{1}{v}= -\frac{1}{\sqrt{u}}= -\frac{1}{\sqrt{nx^2}} = -\frac{1}{\sqrt{n}|x|} $ Second part: For to find $-\int \frac{1}{1+v^2} dv$, by setting  $k_2 = \arctan(v)$, we have $dk_2 = \frac{1}{1 + v^2}dv$ and so we have $-\int \frac{1}{1+v^2} dv = -int dk_2= -k_2= -\arctan(v) = -\arctan(\sqrt{u}) = -\arctan(\sqrt{nx^2}) = -\arctan(\sqrt{n}|x|) $ Third part: For to find $-\int \frac{e^{-v^2}}{v^2} dv $, by setting $\begin{cases}                k_3=e^{-v^2}\\                -\frac{1}{v^2}=dk_4             \end{cases}$ we will have $\begin{cases}                dk_3=-2ve^{-v^2}\\                k_4= \frac{1}{v}             \end{cases}$ and our integral will transform to $-\int \frac{e^{-v^2}}{v^2} dv = \frac{e^{-v^2}}{v} - 2 \int e^{-v^2} dv =  \frac{e^{-v^2}}{v} - 2(\frac{\sqrt{\pi}}{2}) = \frac{e^{-v^2}}{v} - \sqrt{\pi} =  \frac{e^{-(\sqrt{u})^2}}{\sqrt{u}} - \sqrt{\pi} = \frac{e^{-u}}{\sqrt{u}} - \sqrt{\pi} =\frac{e^{-nx^2}}{\sqrt{nx^2}} - \sqrt{\pi}=\frac{e^{-nx^2}}{\sqrt{n}|x|} - \sqrt{\pi}$ Forth part: For to find $\int \frac{e^{-v^2}}{v^2 + 1} dv$, I cannot find it! Can someone please help me to find $\int \frac{e^{-v^2}}{v^2 + 1} dv$? Thanks!",,"['real-analysis', 'integration', 'improper-integrals']"
93,The function $g$ is continuous in discontinuity point,The function  is continuous in discontinuity point,g,"Let $g \in V[a, b]$, where $V[a, b]$ is the set of functions of bounded variation on $[a,b]$.    Show that existence of the Riemann-Stieltjes integral $\int_a^b f dg$ implies that $g$ is continuous in any discontinuity point of $f$. May be I can show that the integral exist when $g$ is monotone increasing and continuous by substracting the upper and lower Riemann-Stieltjes sums and showing this quantity is smaller than any $\epsilon$?","Let $g \in V[a, b]$, where $V[a, b]$ is the set of functions of bounded variation on $[a,b]$.    Show that existence of the Riemann-Stieltjes integral $\int_a^b f dg$ implies that $g$ is continuous in any discontinuity point of $f$. May be I can show that the integral exist when $g$ is monotone increasing and continuous by substracting the upper and lower Riemann-Stieltjes sums and showing this quantity is smaller than any $\epsilon$?",,"['real-analysis', 'integration', 'stieltjes-integral']"
94,one to one correspondance between points on the number axis and real numbers,one to one correspondance between points on the number axis and real numbers,,"My question is arose by the three statements: An interval could be thought of as a line segment on the number axis according to this book . I think it is true that every line segment has two end points. The Cantor-Dedekind axiom : The points on a line can be put into a one-to-one correspondence with   the real numbers. so the unit interval [0,1] corresponds to a line segment AB on the number axis with its end points corresponding to real numbers 0 and 1 respectively , (0,1) is an interval different from [0,1], so I think it must correspond  to a line segment CD on the number axis different from AB, CD must have two end points different from the end points of AB, so what are the real numbers the two end points of CD respectively corresponding to ? Can we name them using some symbolic notations? Is there something wrong with my reasoning here ?","My question is arose by the three statements: An interval could be thought of as a line segment on the number axis according to this book . I think it is true that every line segment has two end points. The Cantor-Dedekind axiom : The points on a line can be put into a one-to-one correspondence with   the real numbers. so the unit interval [0,1] corresponds to a line segment AB on the number axis with its end points corresponding to real numbers 0 and 1 respectively , (0,1) is an interval different from [0,1], so I think it must correspond  to a line segment CD on the number axis different from AB, CD must have two end points different from the end points of AB, so what are the real numbers the two end points of CD respectively corresponding to ? Can we name them using some symbolic notations? Is there something wrong with my reasoning here ?",,"['calculus', 'real-analysis']"
95,Almost Finite Measurable Function is Almost Bounded,Almost Finite Measurable Function is Almost Bounded,,"Let $f$ be a measurable function on $E$ that is finite a.e. on $E$ and $m(E) < \infty$. For each $\epsilon > 0$, show that there is a measurable set $F$ contained in $E$ such that $f$ is bounded on $F$ and $m(E-F) < \epsilon$. Let $f : E \to \Bbb{R}$ and $E$ be as stated in the hypothesis. Since $f$ is finite almost everywhere, there exists an $E_0 \subseteq E$ for which $m(E_0) = 0$ and $f$ is finite over $E-E_0$. Let $\epsilon > 0$. Then there exists a closed set $C$ contained in $E$ such that $m(E-C) < \epsilon$. Note that $E-(C-E_0) = E_0 \cup (E-C)$ which implies $m(E-(C-E_0)) \le m(E_0) + m(E-C) < \epsilon$. Let $F = C-E_0$. Then $f$ will not be infinite over $C$, but it still could be unbounded; so it remains to show that it is bounded on $C$. By way of contradiction, suppose that $f$ is unbounded. Then for every $n \in \Bbb{N}$, there exists an $x_n \in C$ such that $|f(x_n)| \ge n$. Since $C$ and $E_0$ are measurable subsets of $E$, over which $f$ is a measurable function, $f$ is also measurable over $C-E_0 = F$. Moreover, since $|~|$ is continuous, the composition $|~| \circ f$ is measurable over $F$. Therefore, for every $n$, the measurable set $\{x \in F \mid |f(x)| \ge n \}$ is nonempty. This is where I get stuck...I feel like there is a contradiction lurking somewhere but I cannot spot it. I was trying to argue that the intersection $\bigcap_{n \in \Bbb{N}} \{x \in C \mid |f(x)| \ge n \} = \{x \in C \mid |f(x)| = \infty \}$ is not empty, which would give the desired contradiction, but this is very unlikely...I could use a hint.","Let $f$ be a measurable function on $E$ that is finite a.e. on $E$ and $m(E) < \infty$. For each $\epsilon > 0$, show that there is a measurable set $F$ contained in $E$ such that $f$ is bounded on $F$ and $m(E-F) < \epsilon$. Let $f : E \to \Bbb{R}$ and $E$ be as stated in the hypothesis. Since $f$ is finite almost everywhere, there exists an $E_0 \subseteq E$ for which $m(E_0) = 0$ and $f$ is finite over $E-E_0$. Let $\epsilon > 0$. Then there exists a closed set $C$ contained in $E$ such that $m(E-C) < \epsilon$. Note that $E-(C-E_0) = E_0 \cup (E-C)$ which implies $m(E-(C-E_0)) \le m(E_0) + m(E-C) < \epsilon$. Let $F = C-E_0$. Then $f$ will not be infinite over $C$, but it still could be unbounded; so it remains to show that it is bounded on $C$. By way of contradiction, suppose that $f$ is unbounded. Then for every $n \in \Bbb{N}$, there exists an $x_n \in C$ such that $|f(x_n)| \ge n$. Since $C$ and $E_0$ are measurable subsets of $E$, over which $f$ is a measurable function, $f$ is also measurable over $C-E_0 = F$. Moreover, since $|~|$ is continuous, the composition $|~| \circ f$ is measurable over $F$. Therefore, for every $n$, the measurable set $\{x \in F \mid |f(x)| \ge n \}$ is nonempty. This is where I get stuck...I feel like there is a contradiction lurking somewhere but I cannot spot it. I was trying to argue that the intersection $\bigcap_{n \in \Bbb{N}} \{x \in C \mid |f(x)| \ge n \} = \{x \in C \mid |f(x)| = \infty \}$ is not empty, which would give the desired contradiction, but this is very unlikely...I could use a hint.",,"['real-analysis', 'measure-theory']"
96,Can we find a periodic function $f$ with a non-zero smallest period such that $f(x^2)$ is also periodic?,Can we find a periodic function  with a non-zero smallest period such that  is also periodic?,f f(x^2),"Let $f$ be a periodic function such that it has a (non-zero) fundamental period (Smallest nonzero period). Can   $f(x^2)$ also be periodic? So the constant functions and Dirichlet function are not examples we want here. If $f$ is continuous, then it is impossible, because $f(x^2)$ fails to be uniformly continuous.","Let $f$ be a periodic function such that it has a (non-zero) fundamental period (Smallest nonzero period). Can   $f(x^2)$ also be periodic? So the constant functions and Dirichlet function are not examples we want here. If $f$ is continuous, then it is impossible, because $f(x^2)$ fails to be uniformly continuous.",,['real-analysis']
97,Euler squared sum of order 2 : $\sum \limits_{n=1}^{\infty} \frac{\left( \mathcal{H}_n^{(2)} \right)^2}{n^2}$,Euler squared sum of order 2 :,\sum \limits_{n=1}^{\infty} \frac{\left( \mathcal{H}_n^{(2)} \right)^2}{n^2},Let $\mathcal{H}_n$ denote the $n$ - th harmonic number. What techniques would one use to prove that $$\sum \limits_{n=1}^{\infty} \frac{\left( \mathcal{H}_n^{(2)} \right)^2}{n^2} = \zeta^2(3) + \frac{19 \zeta(6)}{24}$$ where $\zeta$ denotes the Riemann zeta function.,Let $\mathcal{H}_n$ denote the $n$ - th harmonic number. What techniques would one use to prove that $$\sum \limits_{n=1}^{\infty} \frac{\left( \mathcal{H}_n^{(2)} \right)^2}{n^2} = \zeta^2(3) + \frac{19 \zeta(6)}{24}$$ where $\zeta$ denotes the Riemann zeta function.,,"['real-analysis', 'sequences-and-series', 'euler-sums']"
98,Theorem 7.17 in Baby Rudin: How is a shorter proof possible if the continuity of the $f_n^\prime$ is assumed in addition?,Theorem 7.17 in Baby Rudin: How is a shorter proof possible if the continuity of the  is assumed in addition?,f_n^\prime,"Here is Theorem 7.17 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $\left\{ f_n \right\}$ is a sequence of functions, differentiable on $[a, b]$ and such that $\left\{ f_n \left( x_0 \right) \right\}$ converges for some point $x_0$ on $[a, b]$. If $\left\{ f_n^\prime \right\}$ converges uniformly on $[a, b]$, then $\left\{ f_n \right\}$ converges uniformly on $[a, b]$, to a function $f$, and    $$\tag{27} f^\prime(x) = \lim_{n \to \infty } f_n^\prime(x) \qquad \qquad (a \leq x \leq b). $$ And, here is Rudin's proof: Let $\varepsilon > 0$ be given. Choose $N$ such that $n \geq N$, $m \geq N$, implies    $$ \tag{28} \left\lvert f_n \left( x_0 \right) - f_m \left( x_0 \right) \right\rvert < \frac{\varepsilon}{2} $$   and    $$ \tag{29} \left\lvert f_n^\prime(t) - f_m^\prime(t) \right\rvert < \frac{\varepsilon}{2(b-a)} \qquad \qquad (a \leq t \leq b). $$ If we apply the mean value theorem 5.19 to the function $f_n - f_m$, (29) shows that    $$ \tag{30} \left\lvert f_n (x) - f_m (x) - f_n (t) + f_m (t) \right\rvert \leq \frac{ \lvert x-t \rvert \varepsilon }{ 2(b-a) } \leq \frac{ \varepsilon }{ 2 } $$   for any $x$ and $t$ on $[a, b]$, if $n \geq N$, $m \geq N$. The inequality    $$ \left\lvert f_n (x) - f_m (x) \right\rvert \leq \left\lvert f_n (x) - f_m (x) - f_n \left( x_0 \right) + f_m \left( x_0 \right) \right\rvert + \left\lvert f_n \left( x_0 \right) - f_m \left( x_0 \right) \right\rvert   $$   implies, by (28) and (30), that    $$ \left\lvert f_n(x) - f_m(x) \right\rvert < \varepsilon \qquad \qquad (a \leq x \leq b, n \geq N, m \geq N), $$   so that $\left\{ f_n \right\}$ converges uniformly on $[a, b]$. Let    $$ f(x) = \lim_{n \to \infty} f_n (x) \qquad (a \leq x \leq b). $$ Let us now fix a point $x$ on $[a, b]$ and define    $$ \tag{31} \phi_n(t) = \frac{ f_n(t) - f_n(x) }{ t-x}, \qquad \phi(t) = \frac{ f(t) - f(x) }{ t-x} $$   for $a \leq t \leq b$, $t \neq x$. Then    $$ \tag{32}  \lim_{ t \to x } \phi_n (t) = f_n^\prime(x) \qquad (n = 1, 2, 3, \ldots). $$   The first inequality in (30) shows that    $$ \left\lvert \phi_n(t) - \phi_m(t) \right\rvert \leq \frac{ \varepsilon}{2(b-a) } \qquad ( n \geq N, m \geq N), $$   so that $\left\{ \phi_n \right\}$ converges uniformly, for $t \neq x$. Since $\left\{ f_n \right\}$ converges to $f$, we conclude from (31) that    $$\tag{33} \lim_{n \to \infty } \phi_n (t) = \phi(t) $$   uniformly for $a \leq t \leq b$, $t \neq x$. If we now apply Theorem 7.11 to $\left\{ \phi_n \right\}$, (32) and (33) show that    $$ \lim_{t \to x} \phi(t) = \lim_{n \to \infty} f_n^\prime(x); $$   and this is (27), by the definition of $\phi(t)$. Here is Theorem 5.19 in Baby Rudin: Suppose $\mathbf{f}$ is a continuous mapping of $[a, b]$ into $\mathbb{R}^k$ and $\mathbf{f}$ is differentiable in $(a, b)$. Then there exists $x \in (a, b)$ such that    $$ \lvert \mathbf{f} (b) - \mathbf{f} (a) \rvert \leq (b-a) \left\lvert \mathbf{f}^\prime (x) \right\rvert. $$ And, here is Theorem 7.11: Suppose $f_n \to f$ uniformly on a set $E$ in a metric space. Let $x$ be a limit point of $E$, and suppose that    $$ \tag{15} \lim_{ t \to x } f_n (t) = A_n \qquad (n = 1, 2, 3, \ldots). $$    Then $\left\{ A_n \right\}$ converges, and    $$ \tag{16} \lim_{t \to x} f(t) = \lim_{n \to \infty} A_n. $$    In other words, the conclusion is that    $$ \tag{17} \lim_{t \to x} \lim_{ n \to \infty} f_n(t) = \lim_{ n \to \infty}  \lim_{t \to x} f_n(t). $$ Now here is the Remark following the proof of Theorem 7.17 in Baby Rudin: If the continuity of the functions $f_n^\prime$ is assumed in addition to the above hypotheses, then a much shorter proof of (27) can be based on Theorem 7.16 and the fundamental theorem of calculus. Here is Theorem 7.16 in Baby Rudin, 3rd edition: Let $\alpha$ be monotonically increasing on $[a, b]$. Suppose $f_n \in \mathscr{R}(\alpha)$ on $[a, b]$, for $n = 1, 2, 3, \ldots$, and suppose $f_n \to f$ uniformly on $[a, b]$. Then $f \in \mathscr{R}$ on $[a, b]$, and    $$ \tag{23} \int_a^b f \ \mathrm{d} \alpha = \lim_{n \to \infty} \int_a^b f_n \ \mathrm{d} \alpha. $$   (The existence of the limit is part of the conclusion.) And, here is Theorem 6.21 in Baby Rudin (i.e. the fundamental theorem of calculus): If $f \in \mathscr{R}$ on $[a, b]$ and if there is a differentiable function $F$ on $[a, b]$ such that $F^\prime = f$, then    $$ \int_a^b f(x) \ \mathrm{d} x = F(b) - F(a). $$ Although I've understood the proof of Theorem 7.17, I do not know how to give the much shorter proof of (27) that Rudin has asserted in the Remark above?","Here is Theorem 7.17 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $\left\{ f_n \right\}$ is a sequence of functions, differentiable on $[a, b]$ and such that $\left\{ f_n \left( x_0 \right) \right\}$ converges for some point $x_0$ on $[a, b]$. If $\left\{ f_n^\prime \right\}$ converges uniformly on $[a, b]$, then $\left\{ f_n \right\}$ converges uniformly on $[a, b]$, to a function $f$, and    $$\tag{27} f^\prime(x) = \lim_{n \to \infty } f_n^\prime(x) \qquad \qquad (a \leq x \leq b). $$ And, here is Rudin's proof: Let $\varepsilon > 0$ be given. Choose $N$ such that $n \geq N$, $m \geq N$, implies    $$ \tag{28} \left\lvert f_n \left( x_0 \right) - f_m \left( x_0 \right) \right\rvert < \frac{\varepsilon}{2} $$   and    $$ \tag{29} \left\lvert f_n^\prime(t) - f_m^\prime(t) \right\rvert < \frac{\varepsilon}{2(b-a)} \qquad \qquad (a \leq t \leq b). $$ If we apply the mean value theorem 5.19 to the function $f_n - f_m$, (29) shows that    $$ \tag{30} \left\lvert f_n (x) - f_m (x) - f_n (t) + f_m (t) \right\rvert \leq \frac{ \lvert x-t \rvert \varepsilon }{ 2(b-a) } \leq \frac{ \varepsilon }{ 2 } $$   for any $x$ and $t$ on $[a, b]$, if $n \geq N$, $m \geq N$. The inequality    $$ \left\lvert f_n (x) - f_m (x) \right\rvert \leq \left\lvert f_n (x) - f_m (x) - f_n \left( x_0 \right) + f_m \left( x_0 \right) \right\rvert + \left\lvert f_n \left( x_0 \right) - f_m \left( x_0 \right) \right\rvert   $$   implies, by (28) and (30), that    $$ \left\lvert f_n(x) - f_m(x) \right\rvert < \varepsilon \qquad \qquad (a \leq x \leq b, n \geq N, m \geq N), $$   so that $\left\{ f_n \right\}$ converges uniformly on $[a, b]$. Let    $$ f(x) = \lim_{n \to \infty} f_n (x) \qquad (a \leq x \leq b). $$ Let us now fix a point $x$ on $[a, b]$ and define    $$ \tag{31} \phi_n(t) = \frac{ f_n(t) - f_n(x) }{ t-x}, \qquad \phi(t) = \frac{ f(t) - f(x) }{ t-x} $$   for $a \leq t \leq b$, $t \neq x$. Then    $$ \tag{32}  \lim_{ t \to x } \phi_n (t) = f_n^\prime(x) \qquad (n = 1, 2, 3, \ldots). $$   The first inequality in (30) shows that    $$ \left\lvert \phi_n(t) - \phi_m(t) \right\rvert \leq \frac{ \varepsilon}{2(b-a) } \qquad ( n \geq N, m \geq N), $$   so that $\left\{ \phi_n \right\}$ converges uniformly, for $t \neq x$. Since $\left\{ f_n \right\}$ converges to $f$, we conclude from (31) that    $$\tag{33} \lim_{n \to \infty } \phi_n (t) = \phi(t) $$   uniformly for $a \leq t \leq b$, $t \neq x$. If we now apply Theorem 7.11 to $\left\{ \phi_n \right\}$, (32) and (33) show that    $$ \lim_{t \to x} \phi(t) = \lim_{n \to \infty} f_n^\prime(x); $$   and this is (27), by the definition of $\phi(t)$. Here is Theorem 5.19 in Baby Rudin: Suppose $\mathbf{f}$ is a continuous mapping of $[a, b]$ into $\mathbb{R}^k$ and $\mathbf{f}$ is differentiable in $(a, b)$. Then there exists $x \in (a, b)$ such that    $$ \lvert \mathbf{f} (b) - \mathbf{f} (a) \rvert \leq (b-a) \left\lvert \mathbf{f}^\prime (x) \right\rvert. $$ And, here is Theorem 7.11: Suppose $f_n \to f$ uniformly on a set $E$ in a metric space. Let $x$ be a limit point of $E$, and suppose that    $$ \tag{15} \lim_{ t \to x } f_n (t) = A_n \qquad (n = 1, 2, 3, \ldots). $$    Then $\left\{ A_n \right\}$ converges, and    $$ \tag{16} \lim_{t \to x} f(t) = \lim_{n \to \infty} A_n. $$    In other words, the conclusion is that    $$ \tag{17} \lim_{t \to x} \lim_{ n \to \infty} f_n(t) = \lim_{ n \to \infty}  \lim_{t \to x} f_n(t). $$ Now here is the Remark following the proof of Theorem 7.17 in Baby Rudin: If the continuity of the functions $f_n^\prime$ is assumed in addition to the above hypotheses, then a much shorter proof of (27) can be based on Theorem 7.16 and the fundamental theorem of calculus. Here is Theorem 7.16 in Baby Rudin, 3rd edition: Let $\alpha$ be monotonically increasing on $[a, b]$. Suppose $f_n \in \mathscr{R}(\alpha)$ on $[a, b]$, for $n = 1, 2, 3, \ldots$, and suppose $f_n \to f$ uniformly on $[a, b]$. Then $f \in \mathscr{R}$ on $[a, b]$, and    $$ \tag{23} \int_a^b f \ \mathrm{d} \alpha = \lim_{n \to \infty} \int_a^b f_n \ \mathrm{d} \alpha. $$   (The existence of the limit is part of the conclusion.) And, here is Theorem 6.21 in Baby Rudin (i.e. the fundamental theorem of calculus): If $f \in \mathscr{R}$ on $[a, b]$ and if there is a differentiable function $F$ on $[a, b]$ such that $F^\prime = f$, then    $$ \int_a^b f(x) \ \mathrm{d} x = F(b) - F(a). $$ Although I've understood the proof of Theorem 7.17, I do not know how to give the much shorter proof of (27) that Rudin has asserted in the Remark above?",,"['real-analysis', 'sequences-and-series', 'analysis', 'continuity', 'uniform-convergence']"
99,Find the limit if it exists of $S_{n+1} = \frac{1}{2}(S_n +\frac{A}{S_n})$ [duplicate],Find the limit if it exists of  [duplicate],S_{n+1} = \frac{1}{2}(S_n +\frac{A}{S_n}),"This question already has answers here : Proof of Convergence: Babylonian Method $x_{n+1}=\frac{1}{2}(x_n + \frac{a}{x_n})$ (9 answers) Closed 3 years ago . Suppose that $S_0$ and A are positive numbers, let $$S_{n+1} = \frac{1}{2}\left(S_n +\frac{A}{S_n}\right)$$ with $n \geq 0 $. (a)Show that $S_{n+1} \geq \sqrt{A} $ if $n \geq 0$ (b)Show that $S_{n+1} \leq S_n $ , if $n \geq 1$ (c) Show that $s= \lim\limits_{n \rightarrow \infty} S_n$ exists (d) find s (a) Show that $S_{n+1} \geq \sqrt{A} $ if $n \geq 0$ Given $$P_n:  S_{n+1} = \frac{1}{2}\left(S_n +\frac{A}{S_n}\right) \geq \sqrt{A}$$ $$P_0: S_{1} = \frac{1}{2}\left(S_0 +\frac{A}{S_0}\right) \geq \sqrt{A}  $$ We assume that $P_n$ is true $$P_{n+1}: S_{n+2}= \frac{1}{2}\left(S_{n+1} +\frac{A}{S_{n+1}}\right)$$ by assumption $$S_{n+2}= \frac{1}{2}\left(S_{n+1}\left(1 +\frac{A}{(S_{n+1})^2}\right)\right) \geq \frac{1}{2}\left(\sqrt{A}\left(1 +\frac{A}{(\sqrt{A})^2}\right)\right)$$ $$ S_{n+2}= \frac{1}{2}\left(S_{n+1} +\frac{A}{S_{n+1}}\right) \geq \sqrt{A}  $$ It follows that $S_{n+1} \geq \sqrt{A} $ (b) Show that $S_{n+1} \leq S_n $ , if $n \geq 1$ $$S_{n+1} \leq S_n$$ $$\frac{1}{2}\left(S_n +\frac{A}{S_n}\right) \leq S_n $$ Dividing by $S_n$ $$\frac{1}{2}\left(1 +\frac{A}{S_n^2}\right) \leq 1 $$ $$\frac{A}{2S_n^2} \leq \frac{1}{2}$$ $$A \leq S_n^2$$ $$S_n \geq \sqrt{A}$$ As  $S_{n+1} \leq S_n$ yields a true statement, it follows $S_{n+1} \leq S_n$ is true. (c) Show that $s= \lim\limits_{n \rightarrow \infty} S_n$ exists Since $S_{n+1} \leq S_n$, the sequence is non-increasing, using the non-increasing theorem stating that if $\{S_n\}$ is non-increasing then $$\lim\limits_{n \rightarrow > \infty} S_n = \inf\{S_n\} $$ (d) find s Is the argumentation in (a) and (b) appropriate? Also, I have to admit I m getting less confident in my argumentation (c) and (d). How to proceed in (c) and (d)?  Much appreciated for your input or help.","This question already has answers here : Proof of Convergence: Babylonian Method $x_{n+1}=\frac{1}{2}(x_n + \frac{a}{x_n})$ (9 answers) Closed 3 years ago . Suppose that $S_0$ and A are positive numbers, let $$S_{n+1} = \frac{1}{2}\left(S_n +\frac{A}{S_n}\right)$$ with $n \geq 0 $. (a)Show that $S_{n+1} \geq \sqrt{A} $ if $n \geq 0$ (b)Show that $S_{n+1} \leq S_n $ , if $n \geq 1$ (c) Show that $s= \lim\limits_{n \rightarrow \infty} S_n$ exists (d) find s (a) Show that $S_{n+1} \geq \sqrt{A} $ if $n \geq 0$ Given $$P_n:  S_{n+1} = \frac{1}{2}\left(S_n +\frac{A}{S_n}\right) \geq \sqrt{A}$$ $$P_0: S_{1} = \frac{1}{2}\left(S_0 +\frac{A}{S_0}\right) \geq \sqrt{A}  $$ We assume that $P_n$ is true $$P_{n+1}: S_{n+2}= \frac{1}{2}\left(S_{n+1} +\frac{A}{S_{n+1}}\right)$$ by assumption $$S_{n+2}= \frac{1}{2}\left(S_{n+1}\left(1 +\frac{A}{(S_{n+1})^2}\right)\right) \geq \frac{1}{2}\left(\sqrt{A}\left(1 +\frac{A}{(\sqrt{A})^2}\right)\right)$$ $$ S_{n+2}= \frac{1}{2}\left(S_{n+1} +\frac{A}{S_{n+1}}\right) \geq \sqrt{A}  $$ It follows that $S_{n+1} \geq \sqrt{A} $ (b) Show that $S_{n+1} \leq S_n $ , if $n \geq 1$ $$S_{n+1} \leq S_n$$ $$\frac{1}{2}\left(S_n +\frac{A}{S_n}\right) \leq S_n $$ Dividing by $S_n$ $$\frac{1}{2}\left(1 +\frac{A}{S_n^2}\right) \leq 1 $$ $$\frac{A}{2S_n^2} \leq \frac{1}{2}$$ $$A \leq S_n^2$$ $$S_n \geq \sqrt{A}$$ As  $S_{n+1} \leq S_n$ yields a true statement, it follows $S_{n+1} \leq S_n$ is true. (c) Show that $s= \lim\limits_{n \rightarrow \infty} S_n$ exists Since $S_{n+1} \leq S_n$, the sequence is non-increasing, using the non-increasing theorem stating that if $\{S_n\}$ is non-increasing then $$\lim\limits_{n \rightarrow > \infty} S_n = \inf\{S_n\} $$ (d) find s Is the argumentation in (a) and (b) appropriate? Also, I have to admit I m getting less confident in my argumentation (c) and (d). How to proceed in (c) and (d)?  Much appreciated for your input or help.",,"['real-analysis', 'sequences-and-series', 'proof-verification']"
