,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,When is a matrix triangularisable?,When is a matrix triangularisable?,,When is a matrix triangularisable? I don't seem to find much online regarding the triangularisability of matrices. What should I look for to prove if a matrix is triangularisable? What are the implications on its eigenvalues eigenvectors and eigenspaces? An example a simple triangularisable and one of a non-triangularisable matrix in $\mathbb{R}$ would also help.,When is a matrix triangularisable? I don't seem to find much online regarding the triangularisability of matrices. What should I look for to prove if a matrix is triangularisable? What are the implications on its eigenvalues eigenvectors and eigenspaces? An example a simple triangularisable and one of a non-triangularisable matrix in $\mathbb{R}$ would also help.,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
1,"Qualitatively, what is the difference between a matrix and a tensor?","Qualitatively, what is the difference between a matrix and a tensor?",,"Qualitatively (or mathematically ""light""), could someone describe the difference between a matrix and a tensor? I have only seen them used in the context of an undergraduate, upper level classical mechanics course, and within that context, I never understood the need to distinguish between matrices and tensors. They seemed like identical mathematical entities to me. Just as an aside, my math background is roughly the one of a typical undergraduate physics major (minus the linear algebra).","Qualitatively (or mathematically ""light""), could someone describe the difference between a matrix and a tensor? I have only seen them used in the context of an undergraduate, upper level classical mechanics course, and within that context, I never understood the need to distinguish between matrices and tensors. They seemed like identical mathematical entities to me. Just as an aside, my math background is roughly the one of a typical undergraduate physics major (minus the linear algebra).",,"['linear-algebra', 'mathematical-physics', 'tensors']"
2,Number of independent components of a unitary matrix,Number of independent components of a unitary matrix,,"By definition, a $n$ dimensional unitary matrix $U$ satisfies the condition $U^{\dagger}U=I$, and $UU^{\dagger}=I$. I'd like to ask if these two equations are independent. If so, there will be $n^2$ independent equations of constrain, which is equal to the number of independent components of a general $n$ dimensional matrix. This is obviously impossible. If not, how to prove it?","By definition, a $n$ dimensional unitary matrix $U$ satisfies the condition $U^{\dagger}U=I$, and $UU^{\dagger}=I$. I'd like to ask if these two equations are independent. If so, there will be $n^2$ independent equations of constrain, which is equal to the number of independent components of a general $n$ dimensional matrix. This is obviously impossible. If not, how to prove it?",,"['linear-algebra', 'matrices', 'unitary-matrices']"
3,How to construct change of basis matrix,How to construct change of basis matrix,,"How do I construct a change of basis matrix? For example in $\mathbb R^3$, how to construct matrix changing basis from $A$ to $B$? $A=\begin{pmatrix} 1  \\ 0 \\5 \end{pmatrix}\begin{pmatrix} 4  \\ 5 \\5 \end{pmatrix}\begin{pmatrix} 1  \\ 1 \\4 \end{pmatrix}$ $B=\begin{pmatrix} 1  \\ 3 \\2 \end{pmatrix}\begin{pmatrix} -2  \\ -1 \\1 \end{pmatrix}\begin{pmatrix} 1  \\ 2 \\3 \end{pmatrix}$","How do I construct a change of basis matrix? For example in $\mathbb R^3$, how to construct matrix changing basis from $A$ to $B$? $A=\begin{pmatrix} 1  \\ 0 \\5 \end{pmatrix}\begin{pmatrix} 4  \\ 5 \\5 \end{pmatrix}\begin{pmatrix} 1  \\ 1 \\4 \end{pmatrix}$ $B=\begin{pmatrix} 1  \\ 3 \\2 \end{pmatrix}\begin{pmatrix} -2  \\ -1 \\1 \end{pmatrix}\begin{pmatrix} 1  \\ 2 \\3 \end{pmatrix}$",,['linear-algebra']
4,Prove projection is self adjoint if and only if kernel and image are orthogonal complements,Prove projection is self adjoint if and only if kernel and image are orthogonal complements,,"Let $V$ be an IPS and suppose $\pi : V \to V$ is a projection so that $V = U \oplus W$ (ie $ V = U + W$ and $U \cap W = \left\{0\right\}$ ) $ \ $ where $U = \ker(\pi)$ and $W = \operatorname{im}(\pi)$ , and if $v = u + w \ $ (with $u \in U, \ w \in W$ ) then $\pi(v) = w$ . Prove $\pi$ is self adjoint if and only if $U$ and $W$ are orthogonal complements. I'm hoping someone can give me a few hints on how to begin this question.","Let be an IPS and suppose is a projection so that (ie and ) where and , and if (with ) then . Prove is self adjoint if and only if and are orthogonal complements. I'm hoping someone can give me a few hints on how to begin this question.","V \pi : V \to V V = U \oplus W  V = U + W U \cap W = \left\{0\right\}  \  U = \ker(\pi) W = \operatorname{im}(\pi) v = u + w \  u \in U, \ w \in W \pi(v) = w \pi U W","['linear-algebra', 'vector-spaces', 'inner-products']"
5,"Show that $\{1, \sqrt{2}, \sqrt{3}\}$ is linearly independent over $\mathbb{Q}$.",Show that  is linearly independent over .,"\{1, \sqrt{2}, \sqrt{3}\} \mathbb{Q}","My apologies if this question has been asked before, but a quick search gave no results.  This is not homework, but I would just like a hint please.  The question asks Show that $\{1, \sqrt{2}, \sqrt{3}\}$ is linearly independent over $\mathbb{Q}$. To begin, I consider some linear relation $a + b \sqrt{2} + c \sqrt{3} = 0$, where $a, b, c \in \mathbb{Q}$.  There are several cases to consider. If $c = 0$, then it must be the case that $a = b = 0$, because $\sqrt{2}$ is not rational.  Similarly, if $b = 0$, then $a = c = 0$ because $\sqrt{3}$ is not rational.  If $a = 0$, then we must have that $b = c = 0$ because $\frac{\sqrt{2}}{\sqrt{3}}$ is not rational (because $\sqrt{6}$ is not rational). My issue is drawing a contradiction when $a$, $b$, and $c$ are all nonzero.  It is not always the case that the sum of two irrational numbers is irrational (e.g. $1 - \sqrt{2}$ and $\sqrt{2}$).  Hints or suggestions would be greatly appreciated!","My apologies if this question has been asked before, but a quick search gave no results.  This is not homework, but I would just like a hint please.  The question asks Show that $\{1, \sqrt{2}, \sqrt{3}\}$ is linearly independent over $\mathbb{Q}$. To begin, I consider some linear relation $a + b \sqrt{2} + c \sqrt{3} = 0$, where $a, b, c \in \mathbb{Q}$.  There are several cases to consider. If $c = 0$, then it must be the case that $a = b = 0$, because $\sqrt{2}$ is not rational.  Similarly, if $b = 0$, then $a = c = 0$ because $\sqrt{3}$ is not rational.  If $a = 0$, then we must have that $b = c = 0$ because $\frac{\sqrt{2}}{\sqrt{3}}$ is not rational (because $\sqrt{6}$ is not rational). My issue is drawing a contradiction when $a$, $b$, and $c$ are all nonzero.  It is not always the case that the sum of two irrational numbers is irrational (e.g. $1 - \sqrt{2}$ and $\sqrt{2}$).  Hints or suggestions would be greatly appreciated!",,"['linear-algebra', 'radicals', 'irrational-numbers', 'rational-numbers']"
6,$AB-BA$ is a nilpotent matrix if it commutes with $A$,is a nilpotent matrix if it commutes with,AB-BA A,"I saw this in a MathOverflow post and am putting it here for posterity. Problem: Let $A$ and $B$ be square matrices and set $C=AB-BA$ . If $AC=CA$ , prove $C$ is nilpotent.","I saw this in a MathOverflow post and am putting it here for posterity. Problem: Let and be square matrices and set . If , prove is nilpotent.",A B C=AB-BA AC=CA C,"['linear-algebra', 'matrices']"
7,"Eigenvalues and determinant of conjugate, transpose and hermitian of a complex matrix.","Eigenvalues and determinant of conjugate, transpose and hermitian of a complex matrix.",,"For a strictly complex matrix $A$, 1) Can we comment on determinant of $A^{*}$ (conjugate of entries of $A$) , $A^{T}$ (transpose of A) and $A^{H}$ (hermitian of $A$). I know that for real matrices, $\det(A)=\det(A^{T})$. Does it carry over to complex matrices, i.e. does $\det(A)=\det(A^{T})$ in general? I understand $\det(A)=\det(A^{H})$ (from Schur triangularization). 2) The same question as first, now about eigenvalues of $A$. I would like to know about special cases, for instance what if $A$ is hermitian or positive definite and so on.","For a strictly complex matrix $A$, 1) Can we comment on determinant of $A^{*}$ (conjugate of entries of $A$) , $A^{T}$ (transpose of A) and $A^{H}$ (hermitian of $A$). I know that for real matrices, $\det(A)=\det(A^{T})$. Does it carry over to complex matrices, i.e. does $\det(A)=\det(A^{T})$ in general? I understand $\det(A)=\det(A^{H})$ (from Schur triangularization). 2) The same question as first, now about eigenvalues of $A$. I would like to know about special cases, for instance what if $A$ is hermitian or positive definite and so on.",,"['linear-algebra', 'matrices']"
8,Proving determinant product rule combinatorially,Proving determinant product rule combinatorially,,"One of definitions of the determinant is: $\det ({\mathbf C})       =\sum_{\lambda \in S_n} ({\operatorname {sgn} ({\lambda}) \prod_{k=1}^n C_{k \lambda ({k})}})$ I want to prove from this that $\det \left({\mathbf {AB}}\right) = \det({\mathbf A})\det({\mathbf B})$ What I have so far: $(AB)_{k\lambda ({k})} = \sum_{j=1}^n A_{kj}B_{j\lambda(k)}$ so we have for the determinant of $\mathbf {AB}$ $\det ({\mathbf {AB}})       =\sum_{\lambda \in S_n} ({\operatorname {sgn} ({\lambda}) \prod_{k=1}^n \sum_{j=1}^n A_{kj}B_{j\lambda(k)}})$ Now I'm not sure how to denote this, but the product of the sum I think is the sum over all combinations of n terms, each ranging from 1 to n, so I'll denote this set of all combinations $C_n(n)$ for n terms each ranging from 1 to n, analogous to the permutation set, but all combinations instead of permutations. then I get $\det ({\mathbf {AB}})       =\sum_{\lambda \in S_n} ({\operatorname {sgn} ({\lambda}) \sum_{\gamma \in C_n(n)} \prod_{k=1}^n A_{k\gamma(k)}B_{\gamma(k)\lambda(k)}} )$ then I can at least seperate the product: $\det ({\mathbf {AB}})       =\sum_{\lambda \in S_n} ({\operatorname {sgn} ({\lambda}) \sum_{\gamma \in C_n(n)} \prod_{k=1}^n A_{k\gamma(k)} \prod_{r=1}^n B_{\gamma(r)\lambda(r)}} )$ I changed the k to an r in one product because it's a dummy variable so I think it doesn't matter, I don't really know if this thing is helpful but this is my attempt at a solution so far. Thanks to anyone who helps!","One of definitions of the determinant is: $\det ({\mathbf C})       =\sum_{\lambda \in S_n} ({\operatorname {sgn} ({\lambda}) \prod_{k=1}^n C_{k \lambda ({k})}})$ I want to prove from this that $\det \left({\mathbf {AB}}\right) = \det({\mathbf A})\det({\mathbf B})$ What I have so far: $(AB)_{k\lambda ({k})} = \sum_{j=1}^n A_{kj}B_{j\lambda(k)}$ so we have for the determinant of $\mathbf {AB}$ $\det ({\mathbf {AB}})       =\sum_{\lambda \in S_n} ({\operatorname {sgn} ({\lambda}) \prod_{k=1}^n \sum_{j=1}^n A_{kj}B_{j\lambda(k)}})$ Now I'm not sure how to denote this, but the product of the sum I think is the sum over all combinations of n terms, each ranging from 1 to n, so I'll denote this set of all combinations $C_n(n)$ for n terms each ranging from 1 to n, analogous to the permutation set, but all combinations instead of permutations. then I get $\det ({\mathbf {AB}})       =\sum_{\lambda \in S_n} ({\operatorname {sgn} ({\lambda}) \sum_{\gamma \in C_n(n)} \prod_{k=1}^n A_{k\gamma(k)}B_{\gamma(k)\lambda(k)}} )$ then I can at least seperate the product: $\det ({\mathbf {AB}})       =\sum_{\lambda \in S_n} ({\operatorname {sgn} ({\lambda}) \sum_{\gamma \in C_n(n)} \prod_{k=1}^n A_{k\gamma(k)} \prod_{r=1}^n B_{\gamma(r)\lambda(r)}} )$ I changed the k to an r in one product because it's a dummy variable so I think it doesn't matter, I don't really know if this thing is helpful but this is my attempt at a solution so far. Thanks to anyone who helps!",,"['linear-algebra', 'combinatorics', 'matrices', 'determinant']"
9,Why does the largest Jordan block determine the degree for that factor in the minimal polynomial?,Why does the largest Jordan block determine the degree for that factor in the minimal polynomial?,,"Let $A$ be a square matrix, so $A$ has some Jordan Normal form. Then $A$ has a minimal polynomial, say $m(X)=\prod_{i=1}^k (t-\lambda_i)^{m_i}$. Wikipedia says The factors of the minimal polynomial $m$ are the elementary divisors of the largest degree corresponding to distinct eigenvalues. So $m_i$ is the size of the largest Jordan block of $\lambda_i$. Why is this exactly?","Let $A$ be a square matrix, so $A$ has some Jordan Normal form. Then $A$ has a minimal polynomial, say $m(X)=\prod_{i=1}^k (t-\lambda_i)^{m_i}$. Wikipedia says The factors of the minimal polynomial $m$ are the elementary divisors of the largest degree corresponding to distinct eigenvalues. So $m_i$ is the size of the largest Jordan block of $\lambda_i$. Why is this exactly?",,"['linear-algebra', 'matrices', 'jordan-normal-form', 'minimal-polynomials']"
10,How to find intersection of two lines in 3D?,How to find intersection of two lines in 3D?,,"Given two lines joining A,B and C, D in 3D how do I figure out if they intersect, where they intersect and the ratio along AB at which the intersection happens? I can quite hapilly work out the equation for the lines in different forms. I'm guessing that you need to change them to parametric form, equate the equations and do some algebra manipulation","Given two lines joining A,B and C, D in 3D how do I figure out if they intersect, where they intersect and the ratio along AB at which the intersection happens? I can quite hapilly work out the equation for the lines in different forms. I'm guessing that you need to change them to parametric form, equate the equations and do some algebra manipulation",,['linear-algebra']
11,Which integer matrices are $k$th powers for all $k$?,Which integer matrices are th powers for all ?,k k,"Problem 11401 of AMM (the American Mathematical Monthly ) states: Let $A$ be a nonsingular square matrix with integer entries. Suppose that for every positive integer $k$ , there is a matrix $X$ with integer entries such that $X^k = A$ . Show that $A$ must be the identity matrix. What if we assume $\det A=0$ ?  For example, any idempotent matrix $A=A^2$ is a $k$ th power for all $k$ , but are there any other matrices?","Problem 11401 of AMM (the American Mathematical Monthly ) states: Let be a nonsingular square matrix with integer entries. Suppose that for every positive integer , there is a matrix with integer entries such that . Show that must be the identity matrix. What if we assume ?  For example, any idempotent matrix is a th power for all , but are there any other matrices?",A k X X^k = A A \det A=0 A=A^2 k k,"['linear-algebra', 'matrices', 'contest-math']"
12,Basis of a vector space is a maximal linearly-independent set?,Basis of a vector space is a maximal linearly-independent set?,,"If $V$ is a vector space of finite dimension over $F$, then a basis of   $V$ is a maximal, linearly independent set in $V$. Is this conjecture true? If so, how to prove it?","If $V$ is a vector space of finite dimension over $F$, then a basis of   $V$ is a maximal, linearly independent set in $V$. Is this conjecture true? If so, how to prove it?",,"['linear-algebra', 'vector-spaces']"
13,"Is the set $ {\rm SL }(n, \mathbb {R}) $ connected in ${\rm M }(n, \mathbb {R}) $",Is the set  connected in," {\rm SL }(n, \mathbb {R})  {\rm M }(n, \mathbb {R}) ","Is the set $ {\rm SL }(n, \mathbb {R}) $ connected in ${\rm M }(n, \mathbb {R}) $? Can you  give me hints? I have only concept on  following tags.","Is the set $ {\rm SL }(n, \mathbb {R}) $ connected in ${\rm M }(n, \mathbb {R}) $? Can you  give me hints? I have only concept on  following tags.",,"['linear-algebra', 'general-topology', 'matrices', 'connectedness']"
14,Is there something deep in the fact that an endomorphism of a finite dimensional complex vector space has an eigenvector?,Is there something deep in the fact that an endomorphism of a finite dimensional complex vector space has an eigenvector?,,"In my course of linear algebra I studied that if $V$ is a finite dimensional vector space on the complex field, then every endomorphism of $V$ has an eigenvector. The proof is simple: taken a polynomial $f$ that is null on the endomorphism $A$ (it exists because of the finite dimension of $V$ ), exploiting the algebraic closure of $\mathbb{C}$ we are there. I gave the following geometrical interpretation: such a space can't be fully twisted by any of its linear operators, we always have at least an invariant subspace of dimenson 1. This fact still appears a bit magical to me. Is there some deep or different reason for this in other parts of mathematics such as geometry or algebra? Are there other interesting geometrical consequences? Does it remain true if the space is infinite dimensional?","In my course of linear algebra I studied that if is a finite dimensional vector space on the complex field, then every endomorphism of has an eigenvector. The proof is simple: taken a polynomial that is null on the endomorphism (it exists because of the finite dimension of ), exploiting the algebraic closure of we are there. I gave the following geometrical interpretation: such a space can't be fully twisted by any of its linear operators, we always have at least an invariant subspace of dimenson 1. This fact still appears a bit magical to me. Is there some deep or different reason for this in other parts of mathematics such as geometry or algebra? Are there other interesting geometrical consequences? Does it remain true if the space is infinite dimensional?",V V f A V \mathbb{C},[]
15,Can this determinant ever vanish?,Can this determinant ever vanish?,,"Let $a_j=1 +2\cos\left(\frac{2\pi j}q\right)$ for $j=1,\dots,q$ . Then consider the Hermitian matrix: $$A_q = \begin{pmatrix}             a_1 &      1 &      0 &      0 & \ldots &       0 & 1 \\               1 &    a_2 &      1 &      0 & \ldots &       0 & 0 \\               0 &      1 &    a_3 &      1 & \ldots &       0 & 0 \\              0 &      0 &      1 &    a_4 & \ldots &       0 & 0 \\          \vdots & \vdots & \vdots & \vdots & \ddots &  \vdots & \vdots \\              0 &      0 &      0 &      0 & \ldots & a_{q-1} & 1 \\               1 &      0 &      0 &      0 & \ldots &       1 & a_q         \end{pmatrix}$$ Using Mathematica, I find: $$\begin{align} \det(A_2)&=-4\\ \det(A_3)&=-1\\ \det(A_4)&=-7\\ \det(A_5)&=-\frac52(-5+\sqrt5) \end{align}$$ Is there any general formula that describes this determinant? And, more importantly, can the determinant ever be zero? The one answer I got so far, suggests no, but it is of course not a proof. This problem is motivated by a quantum mechanics problem, where the Hermitian matrix describes an observable in QM. In particular, it originates from the study of an electron on a lattice in a commensurable magnetic field.","Let for . Then consider the Hermitian matrix: Using Mathematica, I find: Is there any general formula that describes this determinant? And, more importantly, can the determinant ever be zero? The one answer I got so far, suggests no, but it is of course not a proof. This problem is motivated by a quantum mechanics problem, where the Hermitian matrix describes an observable in QM. In particular, it originates from the study of an electron on a lattice in a commensurable magnetic field.","a_j=1 +2\cos\left(\frac{2\pi j}q\right) j=1,\dots,q A_q = \begin{pmatrix} 
           a_1 &      1 &      0 &      0 & \ldots &       0 & 1 \\ 
             1 &    a_2 &      1 &      0 & \ldots &       0 & 0 \\ 
             0 &      1 &    a_3 &      1 & \ldots &       0 & 0 \\
             0 &      0 &      1 &    a_4 & \ldots &       0 & 0 \\ 
        \vdots & \vdots & \vdots & \vdots & \ddots &  \vdots & \vdots \\
             0 &      0 &      0 &      0 & \ldots & a_{q-1} & 1 \\ 
             1 &      0 &      0 &      0 & \ldots &       1 & a_q
        \end{pmatrix} \begin{align}
\det(A_2)&=-4\\
\det(A_3)&=-1\\
\det(A_4)&=-7\\
\det(A_5)&=-\frac52(-5+\sqrt5)
\end{align}","['linear-algebra', 'matrices', 'determinant']"
16,The definition of Determinant in the spirit of algebra and geometry,The definition of Determinant in the spirit of algebra and geometry,,"The concept of determinant is quite unmotivational topic to introduce. Textbooks use such ""strung out"" introductions like axiomatic definition, Laplace expansion, Leibniz'a permutation formula or something like signed volume. Question : is the following a possible way to introduce the determinant? Determinant is all about determing whether a given set of vectors are linearly independent, and a direct way to check this is to add scalar multiplications of column vectors to get the diagonal form: $$\begin{pmatrix}  a_{11} & a_{12} & a_{13} & a_{14} \\ a_{21} & a_{22}  & a_{23} & a_{24} \\ a_{31} & a_{32}  & a_{33} & a_{34} \\ a_{41} & a_{42}  & a_{43} & a_{44} \\ \end{pmatrix} \thicksim \begin{pmatrix}  d_1 & 0 & 0 & 0 \\ 0 & d_2  & 0 & 0 \\ 0 & 0  & d_3 & 0 \\ 0 & 0  & 0 & d_4 \\ \end{pmatrix}.$$ During the diagonalization process we demand that the information, i.e. the determinant, remains unchanged. Now it's clear that the vectors are linearly independent if every $d_i$ is nonzero, i.e. $\prod_{i=1}^n d_i\neq0$ . It may also be the case that two columns are equal and there is no diagonal form, so we must add a condition that annihilates the determinant (this is consistent with $\prod_{i=1}^n d_i=0$ ), since column vectors can't be linearly independent. If we want to have a real valued function that provides this information, then we simply introduce an ad hoc function $\det:\mathbb{R}^{n \times n} \rightarrow \mathbb{R}$ with following properties: $$\det (a_1,\ldots,a_i,\ldots,a_j,\ldots,a_n)=\det (a_1,\ldots,a_i,\ldots,k\cdot a_i+a_j,\ldots,a_n).$$ $$\det(d_1\cdot e_1,\ldots,d_n\cdot e_n)=\prod_{i=1}^n d_i.$$ $$\det (a_1,\ldots,a_i,\ldots,a_j,\ldots,a_n)=0, \space \space \text{if} \space \space a_i=a_j.$$ From the previous definition of determinant we can infer the multilinearity property: $$[a_1,\ldots,c_1 \cdot u+c_2 \cdot v,\ldots,a_n]\thicksim diag[d_1,\ldots,c_1 \cdot d'_i+c_2 \cdot d''_i ,\ldots,d_n],$$ so $$\det[a_1,\ldots,c_1 \cdot u+c_2 \cdot v,\ldots,a_n]=\prod_{j=1:j\neq i}^n d_j(c_1 \cdot d'_i+c_2 \cdot d''_i)$$ $$=c_1\det(diag[d_1,\ldots, d'_i,\ldots,d_n])+c_2\det(diag[d_1,\ldots, d''_i,\ldots,d_n])$$ $$=c_1\det[a_1,\ldots,u,\ldots,a_n]+c_2\det[a_1,\ldots, v,\ldots,a_n].$$ Note that previous multilinearity together with property $(1)$ gives the property $(2)$ , so we know from the literature that the determinant function $\det:\mathbb{R}^{n \times n} \rightarrow \mathbb{R}$ actually exists and it is unique. Obviously, the determinant offers information how orthogonal a set of vectors is. Thus, with Gram-Schmidt process we can form an orthogonal set of vectors form set $(a_1,\ldots, a_n)$ , and by multilinearity and property $(2)$ the absolute value of determinant is the volume of parallelepiped spanned by the set of vectors. Definition . Volume of parallelepiped formed by set of vectors $(a_1,\ldots, a_n)$ is $Vol(a_1,\ldots, a_n)=Vol(a_1,\ldots, a_{n-1})\cdot |a_{n}^{\bot}|=|a_{1}^{\bot}|\cdots |a_{n}^{\bot}|$ , where $a_{i}^{\bot} \bot span(a_1,\ldots, a_{i-1}).$ This approach to determinant works equally well if we begin with the volume of a parallelepiped (geometric approach) or with the search of invertibility (algebraic approach). I was motivated by the book Linear algebra and its applications by Lax on chapter 5: Rather than start with a formula for the determinant, we shall deduce it from the properties forced on it by the geometric properties of signed volume. This approach to determinants is due to E. Artin. $\det (a_1,\ldots,a_n)=0$ , if $a_i=a_j$ , $i\neq j.$ $\det (a_1,\ldots,a_n)$ is a multilinear function of its arguments, in the sense that if all $a_i, i \neq j$ are fixed, $\det$ is a linear function of the remaining argument $a_j.$ $\det(e_1,\ldots,e_n)=1.$","The concept of determinant is quite unmotivational topic to introduce. Textbooks use such ""strung out"" introductions like axiomatic definition, Laplace expansion, Leibniz'a permutation formula or something like signed volume. Question : is the following a possible way to introduce the determinant? Determinant is all about determing whether a given set of vectors are linearly independent, and a direct way to check this is to add scalar multiplications of column vectors to get the diagonal form: During the diagonalization process we demand that the information, i.e. the determinant, remains unchanged. Now it's clear that the vectors are linearly independent if every is nonzero, i.e. . It may also be the case that two columns are equal and there is no diagonal form, so we must add a condition that annihilates the determinant (this is consistent with ), since column vectors can't be linearly independent. If we want to have a real valued function that provides this information, then we simply introduce an ad hoc function with following properties: From the previous definition of determinant we can infer the multilinearity property: so Note that previous multilinearity together with property gives the property , so we know from the literature that the determinant function actually exists and it is unique. Obviously, the determinant offers information how orthogonal a set of vectors is. Thus, with Gram-Schmidt process we can form an orthogonal set of vectors form set , and by multilinearity and property the absolute value of determinant is the volume of parallelepiped spanned by the set of vectors. Definition . Volume of parallelepiped formed by set of vectors is , where This approach to determinant works equally well if we begin with the volume of a parallelepiped (geometric approach) or with the search of invertibility (algebraic approach). I was motivated by the book Linear algebra and its applications by Lax on chapter 5: Rather than start with a formula for the determinant, we shall deduce it from the properties forced on it by the geometric properties of signed volume. This approach to determinants is due to E. Artin. , if , is a multilinear function of its arguments, in the sense that if all are fixed, is a linear function of the remaining argument","\begin{pmatrix} 
a_{11} & a_{12} & a_{13} & a_{14} \\
a_{21} & a_{22}  & a_{23} & a_{24} \\
a_{31} & a_{32}  & a_{33} & a_{34} \\
a_{41} & a_{42}  & a_{43} & a_{44} \\
\end{pmatrix} \thicksim \begin{pmatrix} 
d_1 & 0 & 0 & 0 \\
0 & d_2  & 0 & 0 \\
0 & 0  & d_3 & 0 \\
0 & 0  & 0 & d_4 \\
\end{pmatrix}. d_i \prod_{i=1}^n d_i\neq0 \prod_{i=1}^n d_i=0 \det:\mathbb{R}^{n \times n} \rightarrow \mathbb{R} \det (a_1,\ldots,a_i,\ldots,a_j,\ldots,a_n)=\det (a_1,\ldots,a_i,\ldots,k\cdot a_i+a_j,\ldots,a_n). \det(d_1\cdot e_1,\ldots,d_n\cdot e_n)=\prod_{i=1}^n d_i. \det (a_1,\ldots,a_i,\ldots,a_j,\ldots,a_n)=0, \space \space \text{if} \space \space a_i=a_j. [a_1,\ldots,c_1 \cdot u+c_2 \cdot v,\ldots,a_n]\thicksim diag[d_1,\ldots,c_1 \cdot d'_i+c_2 \cdot d''_i ,\ldots,d_n], \det[a_1,\ldots,c_1 \cdot u+c_2 \cdot v,\ldots,a_n]=\prod_{j=1:j\neq i}^n d_j(c_1 \cdot d'_i+c_2 \cdot d''_i) =c_1\det(diag[d_1,\ldots, d'_i,\ldots,d_n])+c_2\det(diag[d_1,\ldots, d''_i,\ldots,d_n]) =c_1\det[a_1,\ldots,u,\ldots,a_n]+c_2\det[a_1,\ldots, v,\ldots,a_n]. (1) (2) \det:\mathbb{R}^{n \times n} \rightarrow \mathbb{R} (a_1,\ldots, a_n) (2) (a_1,\ldots, a_n) Vol(a_1,\ldots, a_n)=Vol(a_1,\ldots, a_{n-1})\cdot |a_{n}^{\bot}|=|a_{1}^{\bot}|\cdots |a_{n}^{\bot}| a_{i}^{\bot} \bot span(a_1,\ldots, a_{i-1}). \det (a_1,\ldots,a_n)=0 a_i=a_j i\neq j. \det (a_1,\ldots,a_n) a_i, i \neq j \det a_j. \det(e_1,\ldots,e_n)=1.","['linear-algebra', 'matrices', 'soft-question', 'determinant']"
17,Proving Holder's inequality for Schatten norms,Proving Holder's inequality for Schatten norms,,"Sticking to the finite dimensional case, Holder's inequality for Schatten norms is given by $$\left\|AB\right\|_{S^1}\leq\left\|A\right\|_{S^p}\left\|B\right\|_{S^q}$$ for $A,B$ $n\times n$ matrices, $p,q\in[1,\infty]$, and $\frac{1}{p}+\frac{1}{q}=1$. So using Young's inequality, the expression I have in mind is the following \begin{align} \frac{\left\|AB\right\|_{S^1}}{\left\|A\right\|_{S^p}\left\|B\right\|_{S^q}}=\frac{1}{\left\|A\right\|_{S^p}\left\|B\right\|_{S^q}}\sum_{i=1}^n|\sigma_i(AB)|&\overset{?}{\leq}\frac{1}{\left\|A\right\|_{S^p}\left\|B\right\|_{S^q}}\sum_{i=1}^n|\sigma_i(A)||\sigma_{\pi(i)}(B)|\\ &\overset{\text{YI}}{\leq}\frac{1}{p\left\|A\right\|_{S^p}^p}\sum_{i=1}^n|\sigma_i(A)|^p + \frac{1}{q\left\|A\right\|_{S^q}^q}\sum_{i=1}^n|\sigma_{\pi(i)}(B)|^q\\ &=\frac{1}{p} + \frac{1}{q}\\ &=1 \end{align} Focusing in on the inequality under question, $$\sum_{i=1}^n|\sigma_i(AB)|\overset{?}{\leq}\sum_{i=1}^n|\sigma_i(A)||\sigma_{\pi(i)}(B)|$$ we see that proving the Schatten version of Holder's inequality boils down to proving that there exists a permutation $\pi$ of the indices $\{1,...,n\}$ such that the above inequality holds.  Of course maybe this isn't true, but it's the hurdle I ran into when trying to adapt the standard proof of Holder's inequality to the Schatten case. Also I don't strictly need all the absolute values since singular values are always non-negative, but originally I was considering only Hermitian matrices, so I decided to include them. Edit: After doing some numerical tests it looks like permuting the indices isn't necessary.  Thus to win the bounty I'm either looking for a proof that $$\sum_{i=1}^n\sigma_i(AB)\leq\sum_{i=1}^n\sigma_i(A)\sigma_i(B)$$ or if you have some other proof of the Schatten Holder's inequality different than the one I've tried to adapt above, then that's fine too.","Sticking to the finite dimensional case, Holder's inequality for Schatten norms is given by $$\left\|AB\right\|_{S^1}\leq\left\|A\right\|_{S^p}\left\|B\right\|_{S^q}$$ for $A,B$ $n\times n$ matrices, $p,q\in[1,\infty]$, and $\frac{1}{p}+\frac{1}{q}=1$. So using Young's inequality, the expression I have in mind is the following \begin{align} \frac{\left\|AB\right\|_{S^1}}{\left\|A\right\|_{S^p}\left\|B\right\|_{S^q}}=\frac{1}{\left\|A\right\|_{S^p}\left\|B\right\|_{S^q}}\sum_{i=1}^n|\sigma_i(AB)|&\overset{?}{\leq}\frac{1}{\left\|A\right\|_{S^p}\left\|B\right\|_{S^q}}\sum_{i=1}^n|\sigma_i(A)||\sigma_{\pi(i)}(B)|\\ &\overset{\text{YI}}{\leq}\frac{1}{p\left\|A\right\|_{S^p}^p}\sum_{i=1}^n|\sigma_i(A)|^p + \frac{1}{q\left\|A\right\|_{S^q}^q}\sum_{i=1}^n|\sigma_{\pi(i)}(B)|^q\\ &=\frac{1}{p} + \frac{1}{q}\\ &=1 \end{align} Focusing in on the inequality under question, $$\sum_{i=1}^n|\sigma_i(AB)|\overset{?}{\leq}\sum_{i=1}^n|\sigma_i(A)||\sigma_{\pi(i)}(B)|$$ we see that proving the Schatten version of Holder's inequality boils down to proving that there exists a permutation $\pi$ of the indices $\{1,...,n\}$ such that the above inequality holds.  Of course maybe this isn't true, but it's the hurdle I ran into when trying to adapt the standard proof of Holder's inequality to the Schatten case. Also I don't strictly need all the absolute values since singular values are always non-negative, but originally I was considering only Hermitian matrices, so I decided to include them. Edit: After doing some numerical tests it looks like permuting the indices isn't necessary.  Thus to win the bounty I'm either looking for a proof that $$\sum_{i=1}^n\sigma_i(AB)\leq\sum_{i=1}^n\sigma_i(A)\sigma_i(B)$$ or if you have some other proof of the Schatten Holder's inequality different than the one I've tried to adapt above, then that's fine too.",,"['linear-algebra', 'inequality', 'normed-spaces']"
18,Determinant of exact sequence,Determinant of exact sequence,,"Let $0 \to A \to B \to C \to 0$ be an exact sequence of vector spaces.  I want to show that I have a canonical isomorphism $$\det(B)= \det(A) \otimes \det(C).$$ Here, ""det"" refers to the $n$-th exterior product where $n$ is the dimension of the vector space. Although it's easy to see that there is an isomorphism by fixing a basis, what is important to me is to show that this isomorphism is canonical.","Let $0 \to A \to B \to C \to 0$ be an exact sequence of vector spaces.  I want to show that I have a canonical isomorphism $$\det(B)= \det(A) \otimes \det(C).$$ Here, ""det"" refers to the $n$-th exterior product where $n$ is the dimension of the vector space. Although it's easy to see that there is an isomorphism by fixing a basis, what is important to me is to show that this isomorphism is canonical.",,"['linear-algebra', 'vector-spaces', 'determinant', 'multilinear-algebra']"
19,"Why is a matrix $A\in \operatorname{SL}(2,\mathbb{R})$ with $|\operatorname{tr}(A)|<2$ conjugate to a matrix of the following form?",Why is a matrix  with  conjugate to a matrix of the following form?,"A\in \operatorname{SL}(2,\mathbb{R}) |\operatorname{tr}(A)|<2","The trace $\operatorname{tr}(A)$ of a matrix $A$ is the sum of its diagonal entries.  Apparently if $A\in \operatorname{SL}(2,\mathbb{R})$ and $|\operatorname{tr}(A)|<2$, then $A$ is conjugate in $\operatorname{SL}(2,\mathbb{R})$ to a matrix of the form $$\left(\begin{array}{cc} \cos\theta & \sin\theta\\ -\sin\theta & \cos\theta \end{array}\right).$$ Why is this?  I seem to have forgotten my linear algebra.","The trace $\operatorname{tr}(A)$ of a matrix $A$ is the sum of its diagonal entries.  Apparently if $A\in \operatorname{SL}(2,\mathbb{R})$ and $|\operatorname{tr}(A)|<2$, then $A$ is conjugate in $\operatorname{SL}(2,\mathbb{R})$ to a matrix of the form $$\left(\begin{array}{cc} \cos\theta & \sin\theta\\ -\sin\theta & \cos\theta \end{array}\right).$$ Why is this?  I seem to have forgotten my linear algebra.",,['linear-algebra']
20,Is there an explicit relationship between the eigenvalues of a matrix and its derivative?,Is there an explicit relationship between the eigenvalues of a matrix and its derivative?,,"If we consider a matrix $A$ dependent of a variable $x$ , the eigenvalues and eigenvectors satisfying the equation $$ A \vec{v}=\lambda \vec{v} $$ will also depend on $x$ . If we consider the matrix $B$ such that $$B_{ij}=\frac{ \mathrm{d}}{ \mathrm{d} x} A_{ij}$$ Then, could we express the eigenvalues of $B$ in terms of the eigenvalues of $A$ ? I found the question very interesting and was not able to find a satisfying answer myself. For example in the case for $2\times2$ matrices of the form $$ A=\left ( \begin{matrix}  a(x) & b(x) \\   0 & c(x)  \end{matrix} \right ),\implies B=\left ( \begin{matrix}  a'(x) & b'(x) \\   0 & c'(x)  \end{matrix} \right ) $$ I noticed that $\lambda_B(x)= \lambda_A'(x)$ . But I cannot generalise it to general $2\times 2$ matrices. Not even thinking about $n\times n$ matrices... Thank you for your help and any idea!","If we consider a matrix dependent of a variable , the eigenvalues and eigenvectors satisfying the equation will also depend on . If we consider the matrix such that Then, could we express the eigenvalues of in terms of the eigenvalues of ? I found the question very interesting and was not able to find a satisfying answer myself. For example in the case for matrices of the form I noticed that . But I cannot generalise it to general matrices. Not even thinking about matrices... Thank you for your help and any idea!","A x 
A \vec{v}=\lambda \vec{v}
 x B B_{ij}=\frac{ \mathrm{d}}{ \mathrm{d} x} A_{ij} B A 2\times2 
A=\left (
\begin{matrix}
 a(x) & b(x) \\ 
 0 & c(x) 
\end{matrix}
\right ),\implies B=\left (
\begin{matrix}
 a'(x) & b'(x) \\ 
 0 & c'(x) 
\end{matrix}
\right )
 \lambda_B(x)= \lambda_A'(x) 2\times 2 n\times n","['linear-algebra', 'matrices', 'derivatives', 'eigenvalues-eigenvectors']"
21,Does a family of linearly independent injective maps have a vector with linearly independent images?,Does a family of linearly independent injective maps have a vector with linearly independent images?,,"Let $V,W$ be finite dimensional vector spaces over an infinite field $k$ . Fix a positive integer $n \leq \dim(W), \dim(V)$ . Given $n$ injective linear maps $f_i:V\rightarrow W$ , such that the $f_i$ are linearly independent as elements of $\operatorname{Hom}(V,W)$ , is there necessarily a vector $v\in V$ with $f_i(v)$ linearly independent in $W$ ? This holds for $n=1,2$ and if the $f_i$ commute and are diagonalisable, since then one may simultaneously diagonalise. In terms of the conditions, we clearly require linear independence of the maps for the conclusion, and injectivity is required to exclude the case of $\dim(V) > \dim(W)$ , where the images will never be linearly independent.","Let be finite dimensional vector spaces over an infinite field . Fix a positive integer . Given injective linear maps , such that the are linearly independent as elements of , is there necessarily a vector with linearly independent in ? This holds for and if the commute and are diagonalisable, since then one may simultaneously diagonalise. In terms of the conditions, we clearly require linear independence of the maps for the conclusion, and injectivity is required to exclude the case of , where the images will never be linearly independent.","V,W k n \leq \dim(W), \dim(V) n f_i:V\rightarrow W f_i \operatorname{Hom}(V,W) v\in V f_i(v) W n=1,2 f_i \dim(V) > \dim(W)","['linear-algebra', 'linear-transformations']"
22,Proof that Markov Matrix has Eigenvalue of 1,Proof that Markov Matrix has Eigenvalue of 1,,"Taken from “Introduction to Linear Algebra” by Gilbert Strang: I can verify this property of Markov matrices through computation on example matrices, but can someone please provide clarity on the final statement that 1 is an Eigenvalue?","Taken from “Introduction to Linear Algebra” by Gilbert Strang: I can verify this property of Markov matrices through computation on example matrices, but can someone please provide clarity on the final statement that 1 is an Eigenvalue?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
23,Is the number of linearly independent rows equal to the number of linearly independent columns?,Is the number of linearly independent rows equal to the number of linearly independent columns?,,"For any matrix the column rank and row rank are equal. As I understand it rank means the number of linearly independent vectors, where vectors is either the rows or columns of the matrix. This seems to mean that the number of linearly independent rows in a matrix is equal to the number of linearly independent columns? But my gut tells me this shouldn't be the case, at least intuitively I can't see why it would be the case.","For any matrix the column rank and row rank are equal. As I understand it rank means the number of linearly independent vectors, where vectors is either the rows or columns of the matrix. This seems to mean that the number of linearly independent rows in a matrix is equal to the number of linearly independent columns? But my gut tells me this shouldn't be the case, at least intuitively I can't see why it would be the case.",,"['linear-algebra', 'matrices', 'linear-transformations', 'matrix-rank']"
24,Proving replacement theorem?,Proving replacement theorem?,,"I want to see if I am understanding the proof of the replacement theorem correctly. Let $V$ be a vector space that is spanned by a set $G$ containing $n$ vectors.  Let $L \subseteq V$ be a linearly independent subset containing $m$ vectors.  Then $m\leq n$ and there exists a subset H of G containing exactly $n-m$ vectors s.t. $L \cup H$ generates $V$. The proof in the book says to start with $m=0$. In that case $L=\varnothing$, the empty set. Let's assume this theorem is true for some integer $m \geq 0$. Let $L = \{v_j\}_{j=1}^{m+1}$ and define it as a linearly independent subset of $V$ consisting of $m+1$ vectors. Since any subset of a linearly independent set is linearly independent as well ($S_1 \subseteq S_2 \subseteq V$), then $\{v_j\}_{j=1}^m$ is linearly independent also. It then says to use the induction hypothesis to say that $m \geq n$. Well, ok, but that seems sort of pulled out of nowhere to me. The next step is to say that there is therefore another subset, $\{u_k\}_{k=1}^{n-m}$ of G such that $\{v_j\}_{j=1}^m \cup \{u_k\}_{k=1}^{n-m}$ spans $V$. That being the case there are scalars $(a_j)_{j=1}^m$ and $(b_k)_{k=1}^{n-m}$ which you can multiply by the vectors $v_j$ and $u_k$. You can then add the two sets of vectors, yielding $$\sum_{j=1}^m a_j v_j + \sum_{k=1}^{n-m} b_k u_k = v_{m+1}$$ At this point I am not sure I understand things. Because we are assuming $n-m>0$ -- otherwise $v_{m+1}$ is linearly dependent. But then it says not only is $n>m $ but $n>m+1$, and that is where I am losing the plot to get to the next step.","I want to see if I am understanding the proof of the replacement theorem correctly. Let $V$ be a vector space that is spanned by a set $G$ containing $n$ vectors.  Let $L \subseteq V$ be a linearly independent subset containing $m$ vectors.  Then $m\leq n$ and there exists a subset H of G containing exactly $n-m$ vectors s.t. $L \cup H$ generates $V$. The proof in the book says to start with $m=0$. In that case $L=\varnothing$, the empty set. Let's assume this theorem is true for some integer $m \geq 0$. Let $L = \{v_j\}_{j=1}^{m+1}$ and define it as a linearly independent subset of $V$ consisting of $m+1$ vectors. Since any subset of a linearly independent set is linearly independent as well ($S_1 \subseteq S_2 \subseteq V$), then $\{v_j\}_{j=1}^m$ is linearly independent also. It then says to use the induction hypothesis to say that $m \geq n$. Well, ok, but that seems sort of pulled out of nowhere to me. The next step is to say that there is therefore another subset, $\{u_k\}_{k=1}^{n-m}$ of G such that $\{v_j\}_{j=1}^m \cup \{u_k\}_{k=1}^{n-m}$ spans $V$. That being the case there are scalars $(a_j)_{j=1}^m$ and $(b_k)_{k=1}^{n-m}$ which you can multiply by the vectors $v_j$ and $u_k$. You can then add the two sets of vectors, yielding $$\sum_{j=1}^m a_j v_j + \sum_{k=1}^{n-m} b_k u_k = v_{m+1}$$ At this point I am not sure I understand things. Because we are assuming $n-m>0$ -- otherwise $v_{m+1}$ is linearly dependent. But then it says not only is $n>m $ but $n>m+1$, and that is where I am losing the plot to get to the next step.",,"['linear-algebra', 'vector-spaces']"
25,"Linear Algebra, cube & dimensions > 3","Linear Algebra, cube & dimensions > 3",,"I have found interesting problem in Gilbert's Strang book, ,,Introduction to Linear Algebra'' (3rd edition): How many corners does a cube have in 4 dimensions? How many faces? How many edges? A typical corner is $(0,0,1,0)$ I have found the answer for corners: We know, that corner is $(x_1,x_2,x_3,x_4)$. For every $x_i$ we can use either $1$ or $0$. We can do this in $2 \cdot 2 \cdot 2 \cdot 2 = 2^4 = 16$ ways. The same method can be used for general problem of cube in $n$ dimensions (I suppose): Let's say, we have $n$-dimensional cube (I assume, that length of edge is $1$, but it can be some $a$, where $a \in \mathbb{R}$ [1]). Here, corner of this cube looks like this: $(x_1,x_2, \ldots , x_n)$. For every $x_i$ there are $2$ possibilities: $x_i = 0$ or $x_i = 1$ ($x_i = a$ in general). So, this cube has $2^n$ corners. It was pretty simple, I think. But now, there are also faces and edges. To be honest, I do not know, how to find the answer in this cases. I know, that solution for this problem is: A four-dimensional cube has $2^4 = 16$ corners and $2 \cdot 4 = 8$ three-dimensional sides and $24$ two-dimensional faces and $32$ on-dimensional edges. Could You somehow explain me, how to figure out this solution? I have found solution for corners by myself, using Linear Algebra methods & language. Could You show me, how to find the number of edges and faces, using Linear Algebra methods? Is there other method to find these numbers? (I suppose, that answer for this question is positive) I am also interested in articles/textbooks/etc. about space dimensions, if You know some interesting positions about that, share with me (and community). As I wrote: I am interested in mathematical explanations (in particular using Linear Algebra methods/language but other methods may be also interesting) and some intuitions (how to find solution using imagination etc. [2]). Thank You for help. [1] I am not sure of this assumption, because: (a) I am not sure, how edges (and faces) behave in $n$ dimensions (b) I am not sure, how should I think about the distance in $n$ dimensions. I mean, I know, that my intuition may play tricks here [2] I am not asking, how to imagine $4$ dimensional cube, but I think, that there is a way to find the solution, using reasoning, not only Linear Algebra. Addition My definition of face (there was a comment about that) is the same as definition here: http://en.wikipedia.org/wiki/Face_(geometry) , especially: In geometry, a face of a polyhedron is any of the polygons that make up its boundaries.","I have found interesting problem in Gilbert's Strang book, ,,Introduction to Linear Algebra'' (3rd edition): How many corners does a cube have in 4 dimensions? How many faces? How many edges? A typical corner is $(0,0,1,0)$ I have found the answer for corners: We know, that corner is $(x_1,x_2,x_3,x_4)$. For every $x_i$ we can use either $1$ or $0$. We can do this in $2 \cdot 2 \cdot 2 \cdot 2 = 2^4 = 16$ ways. The same method can be used for general problem of cube in $n$ dimensions (I suppose): Let's say, we have $n$-dimensional cube (I assume, that length of edge is $1$, but it can be some $a$, where $a \in \mathbb{R}$ [1]). Here, corner of this cube looks like this: $(x_1,x_2, \ldots , x_n)$. For every $x_i$ there are $2$ possibilities: $x_i = 0$ or $x_i = 1$ ($x_i = a$ in general). So, this cube has $2^n$ corners. It was pretty simple, I think. But now, there are also faces and edges. To be honest, I do not know, how to find the answer in this cases. I know, that solution for this problem is: A four-dimensional cube has $2^4 = 16$ corners and $2 \cdot 4 = 8$ three-dimensional sides and $24$ two-dimensional faces and $32$ on-dimensional edges. Could You somehow explain me, how to figure out this solution? I have found solution for corners by myself, using Linear Algebra methods & language. Could You show me, how to find the number of edges and faces, using Linear Algebra methods? Is there other method to find these numbers? (I suppose, that answer for this question is positive) I am also interested in articles/textbooks/etc. about space dimensions, if You know some interesting positions about that, share with me (and community). As I wrote: I am interested in mathematical explanations (in particular using Linear Algebra methods/language but other methods may be also interesting) and some intuitions (how to find solution using imagination etc. [2]). Thank You for help. [1] I am not sure of this assumption, because: (a) I am not sure, how edges (and faces) behave in $n$ dimensions (b) I am not sure, how should I think about the distance in $n$ dimensions. I mean, I know, that my intuition may play tricks here [2] I am not asking, how to imagine $4$ dimensional cube, but I think, that there is a way to find the solution, using reasoning, not only Linear Algebra. Addition My definition of face (there was a comment about that) is the same as definition here: http://en.wikipedia.org/wiki/Face_(geometry) , especially: In geometry, a face of a polyhedron is any of the polygons that make up its boundaries.",,['linear-algebra']
26,Definition of adjoint operator (asking for intuition),Definition of adjoint operator (asking for intuition),,"Definition of the adjoint operator: A linear operator T on an inner product space V is said to have an adjoint operator $T^{*}$ on V if $\langle T(u),v \rangle= \langle u,T^{*}(v) \rangle$ . Question: Why people come up with that definition? It does not sound intuitive to me. $T^{*}$ is the transpose conjugate of T right, and does that definition follow from definition of inner product space?","Definition of the adjoint operator: A linear operator T on an inner product space V is said to have an adjoint operator on V if . Question: Why people come up with that definition? It does not sound intuitive to me. is the transpose conjugate of T right, and does that definition follow from definition of inner product space?","T^{*} \langle T(u),v \rangle= \langle u,T^{*}(v) \rangle T^{*}","['linear-algebra', 'inner-products']"
27,Over which fields (besides $\mathbb{R}$) is every symmetric matrix potentially diagonalizable?,Over which fields (besides ) is every symmetric matrix potentially diagonalizable?,\mathbb{R},Over which fields (besides the well-known $\mathbb{R}$ ) is every symmetric matrix potentially diagonalizable? A matrix is potentially diagonalizable in a field $F$ if it is diagonalizable in the algebraic closure of $F$ . It appears to me that the fields $\mathbb{F}_2$ and $\mathbb{C}$ do not have this property. What about other finite fields?,Over which fields (besides the well-known ) is every symmetric matrix potentially diagonalizable? A matrix is potentially diagonalizable in a field if it is diagonalizable in the algebraic closure of . It appears to me that the fields and do not have this property. What about other finite fields?,\mathbb{R} F F \mathbb{F}_2 \mathbb{C},"['linear-algebra', 'abstract-algebra', 'finite-fields', 'diagonalization', 'symmetric-matrices']"
28,Jordan form step by step general algorithm,Jordan form step by step general algorithm,,"So I am trying to compile a summary of the procedure one should follow to find the Jordan basis and the Jordan form of a matrix, and I am on the lookout for free resources online where the algorithm to be followed is clearly explained in an amenable way. I have found some interesting youtube videos but what I am on the lookout for is a written thing. Alternatively, if you are so kind as to flesh out the procedure I would be happy to accept that as an answer.","So I am trying to compile a summary of the procedure one should follow to find the Jordan basis and the Jordan form of a matrix, and I am on the lookout for free resources online where the algorithm to be followed is clearly explained in an amenable way. I have found some interesting youtube videos but what I am on the lookout for is a written thing. Alternatively, if you are so kind as to flesh out the procedure I would be happy to accept that as an answer.",,"['linear-algebra', 'reference-request', 'jordan-normal-form']"
29,Example of product space isomorphic to sum of subspaces,Example of product space isomorphic to sum of subspaces,,"Here is the problem statement (from chapter $3$ of Axler's Linear Algebra Done Right ). Give an example of a vector space $V$ and subspaces $U_1,U_2$ of $V$ such that $U_1 \times U_2$ is isomorphic to $U_1 + U_2$ , but $U_1 + U_2$ is not a direct sum. Hint: the vector space $V$ must be infinite-dimensional. The only infinite-dimensional vector spaces mentioned in the book up to this point are $\mathbb{F}^{\infty}$ and $P(\mathbb{R})$ . I tried to construct an example using $\mathbb{F}^{\infty}$ by letting $U_1$ be the span of the standard bases $\{e_{2n}\}$ and $U_2$ the span of $\{e_{2n-1}\}$ . It seems that at least one of the $U_i$ must be infinite dimensional, or else the example could be constructed with $V$ finite dimensional. There is an isomorphism between $U_1 \times U_2$ and $U_1 + U_2$ , but $U_1 \cap U_2 = \{0\}$ so we have a direct sum, which is what we're trying to avoid. I'm not sure how to proceed from here.","Here is the problem statement (from chapter of Axler's Linear Algebra Done Right ). Give an example of a vector space and subspaces of such that is isomorphic to , but is not a direct sum. Hint: the vector space must be infinite-dimensional. The only infinite-dimensional vector spaces mentioned in the book up to this point are and . I tried to construct an example using by letting be the span of the standard bases and the span of . It seems that at least one of the must be infinite dimensional, or else the example could be constructed with finite dimensional. There is an isomorphism between and , but so we have a direct sum, which is what we're trying to avoid. I'm not sure how to proceed from here.","3 V U_1,U_2 V U_1 \times U_2 U_1 + U_2 U_1 + U_2 V \mathbb{F}^{\infty} P(\mathbb{R}) \mathbb{F}^{\infty} U_1 \{e_{2n}\} U_2 \{e_{2n-1}\} U_i V U_1 \times U_2 U_1 + U_2 U_1 \cap U_2 = \{0\}",['linear-algebra']
30,Is There a Basis Free Definition of the Pfaffian,Is There a Basis Free Definition of the Pfaffian,,"$\DeclareMathOperator{\pf}{pf}$ I recently came across a delightful fact that: The determinant of a $2n\times 2n$ skew-symmetric matrix is a the square of a certain polynomial called the pfaffian. I was looking for a ""conceptual proof"" of the above. So naturally I first wanted to understand pfaffians. The description of pfaffian I have seen ( here ) is not very satisfactory to me. Question. Is there a notion of the pfaffian of a linear operator? A promising description of the Pfaffian is available on the above mentioned article: Assume for simplicity that the entries of $M$ are complex numbers, and the $ij$-th entry be written as $a_{ij}$. Let $e_1, \ldots, e_{2n}$ be the standard basis of $\mathbf C^{2n}$. To $M$ we associate a bivector $\omega=\sum_{i<j}a_{ij}\ e_i\wedge e_j$ and let $\omega^n$ denote the wedging of $\omega$ with itself $n$ times. Then $$\frac{1}{n!}\omega^n= \pf(M)e_1\wedge \cdots \wedge e_{2n}$$ If you know a nice proof of the fact mentioned above then please share it.","$\DeclareMathOperator{\pf}{pf}$ I recently came across a delightful fact that: The determinant of a $2n\times 2n$ skew-symmetric matrix is a the square of a certain polynomial called the pfaffian. I was looking for a ""conceptual proof"" of the above. So naturally I first wanted to understand pfaffians. The description of pfaffian I have seen ( here ) is not very satisfactory to me. Question. Is there a notion of the pfaffian of a linear operator? A promising description of the Pfaffian is available on the above mentioned article: Assume for simplicity that the entries of $M$ are complex numbers, and the $ij$-th entry be written as $a_{ij}$. Let $e_1, \ldots, e_{2n}$ be the standard basis of $\mathbf C^{2n}$. To $M$ we associate a bivector $\omega=\sum_{i<j}a_{ij}\ e_i\wedge e_j$ and let $\omega^n$ denote the wedging of $\omega$ with itself $n$ times. Then $$\frac{1}{n!}\omega^n= \pf(M)e_1\wedge \cdots \wedge e_{2n}$$ If you know a nice proof of the fact mentioned above then please share it.",,"['linear-algebra', 'multilinear-algebra', 'bilinear-form', 'exterior-algebra', 'pfaffian']"
31,$SAS^{-1}=\lambda A$ - show $\lambda^n=1$or A is nilpotent,- show or A is nilpotent,SAS^{-1}=\lambda A \lambda^n=1,"I found a question im struggling with in http://www.math.upenn.edu/ugrad/calc/m240/240la.pdf Consider an $n*n$ matrix A with real or complex coefficients and $S$ an invertible matrix. show that if $SAS^{-1}=\lambda A$ then $\lambda^n=1\ or\ A\ is\ nilpotent$ now, showing that $\lambda^{n} = 1$ is easy when we take the determinant of both sides and $\det(A)\neq0$. but showing that the equality holds for $\det(A) = 0$ doesn't prove that $A$ is nilpotent, because not every singular matrix is nilpotent. Any help will be appreciated! Thank You!","I found a question im struggling with in http://www.math.upenn.edu/ugrad/calc/m240/240la.pdf Consider an $n*n$ matrix A with real or complex coefficients and $S$ an invertible matrix. show that if $SAS^{-1}=\lambda A$ then $\lambda^n=1\ or\ A\ is\ nilpotent$ now, showing that $\lambda^{n} = 1$ is easy when we take the determinant of both sides and $\det(A)\neq0$. but showing that the equality holds for $\det(A) = 0$ doesn't prove that $A$ is nilpotent, because not every singular matrix is nilpotent. Any help will be appreciated! Thank You!",,['linear-algebra']
32,Do determinants of binary matrices form a set of consecutive numbers?,Do determinants of binary matrices form a set of consecutive numbers?,,"While pondering a solution for the problem of generating random 0-1 matrices with small absolute determinants, I once again realise how little I know about 0-1 matrices. My initial idea was to pick a random determinant, construct a 0-1 matrix to match this determinant and then permute the matrix's rows and columns. But I quickly abandoned this idea because I simply know no way to construct a 0-1 matrix with a given determinant. In fact, I don't even know how large the determinant of a 0-1 matrix can be. The Hadamard's bound for the absolute determinant of an $n\times n$ 0-1 matrix is $\frac{(n+1)^{(n+1)/2}}{2^n}$ (online ref. 1 and ref. 2 ), and the bound is sharp if and only if there exists a Hadamard matrix of order $n+1$. Yet, to my knowledge, there is no known sharp upper bound for the absolute determinant of a general $n\times n$ 0-1 matrix. While I have abandoned the aforementioned idea, the determinants of 0-1 matrices still intrigues me. So, here is my question: Let ${\cal B}^{n\times n}$ denotes the set of all $n\times n$ 0-1 matrices and let $M=\max_{A\in B}\det A$. Is it true that for every $d\in\{0,1,\ldots,M\}$, there exists $A\in{\cal B}^{n\times n}$ such that $\det(A)=d$? For $n\le6$, the answer is positive, but I have no idea about the general case. Edit: The answer doesn't need to be complete. If this question has been recognised as an open problem in the literature, I am glad to know the references.","While pondering a solution for the problem of generating random 0-1 matrices with small absolute determinants, I once again realise how little I know about 0-1 matrices. My initial idea was to pick a random determinant, construct a 0-1 matrix to match this determinant and then permute the matrix's rows and columns. But I quickly abandoned this idea because I simply know no way to construct a 0-1 matrix with a given determinant. In fact, I don't even know how large the determinant of a 0-1 matrix can be. The Hadamard's bound for the absolute determinant of an $n\times n$ 0-1 matrix is $\frac{(n+1)^{(n+1)/2}}{2^n}$ (online ref. 1 and ref. 2 ), and the bound is sharp if and only if there exists a Hadamard matrix of order $n+1$. Yet, to my knowledge, there is no known sharp upper bound for the absolute determinant of a general $n\times n$ 0-1 matrix. While I have abandoned the aforementioned idea, the determinants of 0-1 matrices still intrigues me. So, here is my question: Let ${\cal B}^{n\times n}$ denotes the set of all $n\times n$ 0-1 matrices and let $M=\max_{A\in B}\det A$. Is it true that for every $d\in\{0,1,\ldots,M\}$, there exists $A\in{\cal B}^{n\times n}$ such that $\det(A)=d$? For $n\le6$, the answer is positive, but I have no idea about the general case. Edit: The answer doesn't need to be complete. If this question has been recognised as an open problem in the literature, I am glad to know the references.",,"['linear-algebra', 'reference-request', 'determinant']"
33,"Evaluate the determinant $\det\left[ \binom{2n}{n+i-j} \right]_{i,j=0}^{n-1}$",Evaluate the determinant,"\det\left[ \binom{2n}{n+i-j} \right]_{i,j=0}^{n-1}","I am trying to show that: \begin{equation} \det\left[ \binom{2n}{n+i-j} \right]_{i,j=0}^{n-1}=\prod_{i=0}^{n-1} \frac{\binom{2n+i}{n}}{\binom{n+i}{n}} \end{equation} I have tried playing with the algebra for some time. For example, if we fix $i$ and consider a particular row vector, we have: \begin{equation} \left[ \begin{array}{c}  \binom{2n}{n+i} & \binom{2n}{n+i-1} & \binom{2n}{n+i-2} & \dots & \binom{2n}{i+1}  \end{array} \right]  \end{equation} Which equals \begin{equation} \left[ \begin{array}{c}  \frac{(2n)!}{(n+i)!(n-i)!} & \frac{(2n)!}{(n+i-1)!(n-i+1)!} & \frac{(2n)!}{(n+i-2)!(n-i+2)!} & \dots & \frac{(2n)!}{(i+1)!(2n-i-1)!}  \end{array} \right]  \end{equation} It seems that our goal should be to factor out $\binom{2n+i}{n} / \binom{n+i}{n}$ and leave a matrix whose determinant evaluates to 1. Clearly: \begin{equation} \binom{2n+i}{n} / \binom{n+i}{n} = \frac{(2n+i)!}{n!(n+i)!} \cdot \frac{n!(i!)}{(n+i)!} =  \frac{(2n+i)!}{(n+i)!} \cdot \frac{i!}{(n+i)!} \end{equation} However, I am unsure of how to proceed. For those interested, the determinant given enumerates plane partitions contained within an $n\times n \times n$ cube, or equivalently, rhombic tilings of a regular hexagon with side length $n$.","I am trying to show that: \begin{equation} \det\left[ \binom{2n}{n+i-j} \right]_{i,j=0}^{n-1}=\prod_{i=0}^{n-1} \frac{\binom{2n+i}{n}}{\binom{n+i}{n}} \end{equation} I have tried playing with the algebra for some time. For example, if we fix $i$ and consider a particular row vector, we have: \begin{equation} \left[ \begin{array}{c}  \binom{2n}{n+i} & \binom{2n}{n+i-1} & \binom{2n}{n+i-2} & \dots & \binom{2n}{i+1}  \end{array} \right]  \end{equation} Which equals \begin{equation} \left[ \begin{array}{c}  \frac{(2n)!}{(n+i)!(n-i)!} & \frac{(2n)!}{(n+i-1)!(n-i+1)!} & \frac{(2n)!}{(n+i-2)!(n-i+2)!} & \dots & \frac{(2n)!}{(i+1)!(2n-i-1)!}  \end{array} \right]  \end{equation} It seems that our goal should be to factor out $\binom{2n+i}{n} / \binom{n+i}{n}$ and leave a matrix whose determinant evaluates to 1. Clearly: \begin{equation} \binom{2n+i}{n} / \binom{n+i}{n} = \frac{(2n+i)!}{n!(n+i)!} \cdot \frac{n!(i!)}{(n+i)!} =  \frac{(2n+i)!}{(n+i)!} \cdot \frac{i!}{(n+i)!} \end{equation} However, I am unsure of how to proceed. For those interested, the determinant given enumerates plane partitions contained within an $n\times n \times n$ cube, or equivalently, rhombic tilings of a regular hexagon with side length $n$.",,"['linear-algebra', 'combinatorics', 'determinant']"
34,Writing the Laplacian matrix of directed graphs as product?,Writing the Laplacian matrix of directed graphs as product?,,"The Laplacian matrix of a nondirected graph can be written as $M^TM$ with $M$ being the incidence matrix of the graph. This makes the (otherwise tedious) proof of Kirkhoff's theorem into a beautiful application of the Cauchy-Binet formula (and indeed, this is one of the proofs in ""Proofs from THE BOOK""). If the graph is directed, $M^TM$ does not work anymore; the diagonal of the resulting matrix contains the total degree of vertices, whereas for Kirkhoff's theorem to work, only the indegree should appear. So my question is this: can this approach still be salvaged by a slightly different definition of $M$ that eludes me, or is the ""tedious"" proof necessary and Cauchy-Binet simply can't be used here?","The Laplacian matrix of a nondirected graph can be written as $M^TM$ with $M$ being the incidence matrix of the graph. This makes the (otherwise tedious) proof of Kirkhoff's theorem into a beautiful application of the Cauchy-Binet formula (and indeed, this is one of the proofs in ""Proofs from THE BOOK""). If the graph is directed, $M^TM$ does not work anymore; the diagonal of the resulting matrix contains the total degree of vertices, whereas for Kirkhoff's theorem to work, only the indegree should appear. So my question is this: can this approach still be salvaged by a slightly different definition of $M$ that eludes me, or is the ""tedious"" proof necessary and Cauchy-Binet simply can't be used here?",,"['linear-algebra', 'graph-theory', 'graph-laplacian']"
35,Prove a symmetric complex matrix is non-singular,Prove a symmetric complex matrix is non-singular,,"As a compromise to this question , can we claim that the following symmetric complex matrix \begin{align*} S = \begin{pmatrix} 0 & 1 + 2i & 1 + 3i & \cdots & 1 + ni \\ 1 + 2i & 0 & 2 + 3i & \cdots & 2 + ni \\ 1 + 3i & 2 + 3i & 0 & \cdots & 3 + n i \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 + ni & 2 + ni & 3 + ni & \cdots & 0 \end{pmatrix} \end{align*} is non-singular? To clarify, the $(k, l)$ entry of $S$ is \begin{align*} s_{kl} = \begin{cases} k + il & l > k, \\ 0      & l = k, \\ s_{lk} & k > l. \end{cases} \end{align*} Through some row/column reduction, I have shown that \begin{align*} \det(S) =  \begin{vmatrix} 0 & 1 + 2i & i & \cdots & i & i \\ 1 + 2i & -2(1 + 2i) & 2 + 2i & & \\ i & 2 + 2i & -2(2 + 3i) & \ddots & \\ \vdots &   & \ddots & \ddots & \ddots & \\ i      &   &        & (n - 2) + (n - 2)i & -2[(n - 2) + (n - 1)i] & (n - 1)  + (n - 1)i \\ i      &   &        &                    & (n - 1) + (n - 1)i     & -2[(n - 1) + ni] \end{vmatrix}. \tag{$*$} \end{align*} Then I got stuck here. Is there any clever way to get around this? Or demonstrating the invertibility through other angles (say, linear independence or eigenvalues)? Result updates: Following @user8675309's tip, we can show by Levy-Desplanques theorem that the lower $(n - 1) \times (n - 1)$ submatrix of $(*)$ is non-singular, as it is row diagonally dominant. Thus $\mathrm{rank}(S) \geq n - 1$ . But the passage from $n - 1$ to $n$ seems quite difficult.","As a compromise to this question , can we claim that the following symmetric complex matrix is non-singular? To clarify, the entry of is Through some row/column reduction, I have shown that Then I got stuck here. Is there any clever way to get around this? Or demonstrating the invertibility through other angles (say, linear independence or eigenvalues)? Result updates: Following @user8675309's tip, we can show by Levy-Desplanques theorem that the lower submatrix of is non-singular, as it is row diagonally dominant. Thus . But the passage from to seems quite difficult.","\begin{align*}
S = \begin{pmatrix}
0 & 1 + 2i & 1 + 3i & \cdots & 1 + ni \\
1 + 2i & 0 & 2 + 3i & \cdots & 2 + ni \\
1 + 3i & 2 + 3i & 0 & \cdots & 3 + n i \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 + ni & 2 + ni & 3 + ni & \cdots & 0
\end{pmatrix}
\end{align*} (k, l) S \begin{align*}
s_{kl} = \begin{cases}
k + il & l > k, \\
0      & l = k, \\
s_{lk} & k > l.
\end{cases}
\end{align*} \begin{align*}
\det(S) = 
\begin{vmatrix}
0 & 1 + 2i & i & \cdots & i & i \\
1 + 2i & -2(1 + 2i) & 2 + 2i & & \\
i & 2 + 2i & -2(2 + 3i) & \ddots & \\
\vdots &   & \ddots & \ddots & \ddots & \\
i      &   &        & (n - 2) + (n - 2)i & -2[(n - 2) + (n - 1)i] & (n - 1)  + (n - 1)i \\
i      &   &        &                    & (n - 1) + (n - 1)i     & -2[(n - 1) + ni]
\end{vmatrix}. \tag{*}
\end{align*} (n - 1) \times (n - 1) (*) \mathrm{rank}(S) \geq n - 1 n - 1 n","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant', 'symmetric-matrices']"
36,$\mathrm{rank}(AB-BA)=1$ implies $A$ and $B$ are simultaneously triangularisable,implies  and  are simultaneously triangularisable,\mathrm{rank}(AB-BA)=1 A B,"Let $A$ and $B$ in $M_n(\mathbb C)$ such that the rank of $AB-BA$ is $1$ . Prove that $A$ and $B$ are simultaneously triangularisable. This generalizes the classical case $AB = BA$ . By induction on $n$ , it suffices to show that $A$ and $B$ have a common eigenvector. So, it would be sufficient to find a eigenspace of $A$ which is stable by $B$ since matrices are complex. Do you have ideas for that? Thank you.","Let and in such that the rank of is . Prove that and are simultaneously triangularisable. This generalizes the classical case . By induction on , it suffices to show that and have a common eigenvector. So, it would be sufficient to find a eigenspace of which is stable by since matrices are complex. Do you have ideas for that? Thank you.",A B M_n(\mathbb C) AB-BA 1 A B AB = BA n A B A B,['linear-algebra']
37,Problem book on linear algebra,Problem book on linear algebra,,"Please refer a problem book on linear algebra containing the following topics: Vector spaces, linear dependence of vectors, basis, dimension , linear transformations, matrix representation with respect to an ordered basis, range space and null space, rank-nullity theorem; eigenvalues and eigenvectors, Cayley-Hamilton theorem; symmetric, skew-symmetric, hermitian, skew-hermitian, orthogonal and unitary matrices. I have already done Schaum's 3000 solved problems on linear algebra , but I need one more problem book to solve in order to be confident to sit for my exam. I don't need a proof oriented problem book; my focus is to solve problems which are applications of theorems Please provide your suggestions. Thanks.","Please refer a problem book on linear algebra containing the following topics: Vector spaces, linear dependence of vectors, basis, dimension , linear transformations, matrix representation with respect to an ordered basis, range space and null space, rank-nullity theorem; eigenvalues and eigenvectors, Cayley-Hamilton theorem; symmetric, skew-symmetric, hermitian, skew-hermitian, orthogonal and unitary matrices. I have already done Schaum's 3000 solved problems on linear algebra , but I need one more problem book to solve in order to be confident to sit for my exam. I don't need a proof oriented problem book; my focus is to solve problems which are applications of theorems Please provide your suggestions. Thanks.",,"['linear-algebra', 'abstract-algebra', 'reference-request', 'soft-question', 'book-recommendation']"
38,Is the intersection between two $n$-spheres an $(n-1)$-sphere?,Is the intersection between two -spheres an -sphere?,n (n-1),Is it true that the intersection between two $n$ -sphere in $\mathbb{R}^n$ is a $(n-1)$ -sphere if is not empty or a single point? I have tried to prove it but my only idea is to work with equations and apparently is not a good idea.,Is it true that the intersection between two -sphere in is a -sphere if is not empty or a single point? I have tried to prove it but my only idea is to work with equations and apparently is not a good idea.,n \mathbb{R}^n (n-1),"['linear-algebra', 'geometry', 'hilbert-spaces']"
39,Canonical isomorphism between $\mathfrak{so}(3)$ and $\mathbb R^3$ with vector cross product,Canonical isomorphism between  and  with vector cross product,\mathfrak{so}(3) \mathbb R^3,"There is a well-known isomorphism between the Lie algebra $\mathfrak{so}(3)$ and $\mathbb{R}^3$ which maps the Lie bracket to the vector cross product. It looks like $$ \begin{pmatrix} 0 & -z &y\\ z & 0 & -x\\ -y & x & 0 \end{pmatrix} \mapsto \begin{pmatrix}x\\y\\z\end{pmatrix}. $$ There is a more geometric description to this isomorphism, which in broad strokes involves identifying $\mathbb{R}^3$ with $\Lambda^2(\mathbb{R}^3)$ via the Hodge star, and identifying $\mathfrak{so}(3)\subseteq \operatorname{End}(\mathbb{R}^3)\cong\mathbb{R}^3\otimes(\mathbb{R}^3)^*$ with $\Lambda^2(\mathbb{R}^3)$. If you work in $\mathbb{R}^3$ with its canonical basis, canonical inner product, and canonical orientation, it's easy to see that these isomorphisms yield the specified result. Now I'm trying to check the details of these identifications for a general vector space (with dim = 3 injected into the argument when necessary), giving all the isomorphisms explicitly and without choosing a basis. I found this excellent answer by Qiaochu Yuan to be helpful for some one of the steps. Let me walk through the steps, as I see them: First let's set notation. Let $V$ be an arbitrary real vector space with inner product $g$ and volume form $\Omega$. The automorphism group $SO(V)=\{O\in \operatorname{Aut}(V)\mid g(Ov,Ow)=g(v,w)\}$ has Lie algebra $\mathfrak{so}(V)=\{X\in \operatorname{End}(V)\mid g(Xv,w)+g(v,Xw)=0\}$ Therefore the map defined by $\tilde{\alpha}(X)=\operatorname{eval}(\flat\circ X\otimes\operatorname{id})\colon v\otimes w\mapsto g(Xv,w)$ is skew-symmetric. Here $\operatorname{eval}$ is the evaluation map $V\otimes V^*$ which takes $v\otimes\sigma\mapsto \sigma(v)$, and $\flat$ is the canonical isomorphism $V\to V^*$ induced by the non-degenerate bilinear form $g$, given by $u\mapsto (v\mapsto g(u,v))$. Since $\tilde{\alpha}(X)\colon V\otimes V\to \mathbb{R}$ is skew-symmetric, it factors to a map $\alpha(X)\colon\Lambda^2(V)\to\mathbb{R}$. I.e. $\alpha(X)\in\Lambda^2V^*$. Now apply the inverse map $\sharp\colon V^*\to V$ to the first tensor factor, we have $\beta(X)=(\sharp\wedge\operatorname{id})\circ\alpha(X)\in V\wedge V^*$. To make this construction a little more concrete, observe that given a one parameter group of rotations in the plane fixed by two vectors $v,w$, one can check that the corresponding Lie group element is $\beta(X)=w\wedge v^\flat$. Apply $\sharp$ to the second tensor factor to get an element of the standard exterior algebra $\gamma(X)=(\operatorname{id}\wedge\sharp)\circ\beta(X)\in\Lambda^2V$. Apply the Hodge star operator to get an element of the vector space $\delta(X)=*\gamma(X)\in V.$ Check that $\delta$ is an isomorphism of Lie algebras $\delta([X,Y])=\delta(X)\times\delta(Y)=*(\delta(X)\wedge\delta(Y))$ (Clearly it's linear. I suppose it should also be verified that $\delta$ is bijective) So I have several problems. Step 2 above seems very convoluted and inelegant, with all the raising and lowering operators just to check whether something is an antisymmetric map. Step 2 is not just convoluted, but of dubious construction. Usually one considers the exterior power of a vector space with itself, not a vector space wedged with its dual space. That construction seems nonstandard to me, so I'm in unfamiliar territory and wonder whether this is the wrong path. I can't really see how to do step 6. I can't push the Lie bracket through all these opaque constructions. The $\operatorname{eval}$ map used in step 2 is also somewhat mysterious, at least in one direction. That is, there's really only a canonical injection $\operatorname{eval}\colon V\otimes V^*\to \operatorname{End}(V)$ given by $v\otimes\sigma\mapsto (w\mapsto \sigma(w)v)$. In the case that $V$ is not finite dimensional, this will not be an isomorphism and does not have an inverse. In the finite dimensional case, we do have an inverse, any endomorphism can be written as a product of vectors and dual vectors, but not canonically. Mapping $\mathfrak{so}(V)$ to $\Lambda^2(V)$ seems to require as an intermediate step mapping $\operatorname{End}(V)$ to $V\otimes V^*$. Can this be done canonically? Given the concerns about the direction of the map $V\otimes V^*\to \operatorname{End}(V)$, perhaps I should try to go the other way, but I can't make any progress that way either, because I don't know any identity for the Hodge star operator that will allow me to evaluate expressions like $*(X\wedge Y)$. How does the Hodge star operator interact with the wedge product? Those are the issues I'm running into with my approach. I had somehow expected the I'd appreciate any resolutions to those issues, or alternatively if there is a better approach to this question, or some reason why the question itself is not a good one, I'd love to hear about it. Thanks.","There is a well-known isomorphism between the Lie algebra $\mathfrak{so}(3)$ and $\mathbb{R}^3$ which maps the Lie bracket to the vector cross product. It looks like $$ \begin{pmatrix} 0 & -z &y\\ z & 0 & -x\\ -y & x & 0 \end{pmatrix} \mapsto \begin{pmatrix}x\\y\\z\end{pmatrix}. $$ There is a more geometric description to this isomorphism, which in broad strokes involves identifying $\mathbb{R}^3$ with $\Lambda^2(\mathbb{R}^3)$ via the Hodge star, and identifying $\mathfrak{so}(3)\subseteq \operatorname{End}(\mathbb{R}^3)\cong\mathbb{R}^3\otimes(\mathbb{R}^3)^*$ with $\Lambda^2(\mathbb{R}^3)$. If you work in $\mathbb{R}^3$ with its canonical basis, canonical inner product, and canonical orientation, it's easy to see that these isomorphisms yield the specified result. Now I'm trying to check the details of these identifications for a general vector space (with dim = 3 injected into the argument when necessary), giving all the isomorphisms explicitly and without choosing a basis. I found this excellent answer by Qiaochu Yuan to be helpful for some one of the steps. Let me walk through the steps, as I see them: First let's set notation. Let $V$ be an arbitrary real vector space with inner product $g$ and volume form $\Omega$. The automorphism group $SO(V)=\{O\in \operatorname{Aut}(V)\mid g(Ov,Ow)=g(v,w)\}$ has Lie algebra $\mathfrak{so}(V)=\{X\in \operatorname{End}(V)\mid g(Xv,w)+g(v,Xw)=0\}$ Therefore the map defined by $\tilde{\alpha}(X)=\operatorname{eval}(\flat\circ X\otimes\operatorname{id})\colon v\otimes w\mapsto g(Xv,w)$ is skew-symmetric. Here $\operatorname{eval}$ is the evaluation map $V\otimes V^*$ which takes $v\otimes\sigma\mapsto \sigma(v)$, and $\flat$ is the canonical isomorphism $V\to V^*$ induced by the non-degenerate bilinear form $g$, given by $u\mapsto (v\mapsto g(u,v))$. Since $\tilde{\alpha}(X)\colon V\otimes V\to \mathbb{R}$ is skew-symmetric, it factors to a map $\alpha(X)\colon\Lambda^2(V)\to\mathbb{R}$. I.e. $\alpha(X)\in\Lambda^2V^*$. Now apply the inverse map $\sharp\colon V^*\to V$ to the first tensor factor, we have $\beta(X)=(\sharp\wedge\operatorname{id})\circ\alpha(X)\in V\wedge V^*$. To make this construction a little more concrete, observe that given a one parameter group of rotations in the plane fixed by two vectors $v,w$, one can check that the corresponding Lie group element is $\beta(X)=w\wedge v^\flat$. Apply $\sharp$ to the second tensor factor to get an element of the standard exterior algebra $\gamma(X)=(\operatorname{id}\wedge\sharp)\circ\beta(X)\in\Lambda^2V$. Apply the Hodge star operator to get an element of the vector space $\delta(X)=*\gamma(X)\in V.$ Check that $\delta$ is an isomorphism of Lie algebras $\delta([X,Y])=\delta(X)\times\delta(Y)=*(\delta(X)\wedge\delta(Y))$ (Clearly it's linear. I suppose it should also be verified that $\delta$ is bijective) So I have several problems. Step 2 above seems very convoluted and inelegant, with all the raising and lowering operators just to check whether something is an antisymmetric map. Step 2 is not just convoluted, but of dubious construction. Usually one considers the exterior power of a vector space with itself, not a vector space wedged with its dual space. That construction seems nonstandard to me, so I'm in unfamiliar territory and wonder whether this is the wrong path. I can't really see how to do step 6. I can't push the Lie bracket through all these opaque constructions. The $\operatorname{eval}$ map used in step 2 is also somewhat mysterious, at least in one direction. That is, there's really only a canonical injection $\operatorname{eval}\colon V\otimes V^*\to \operatorname{End}(V)$ given by $v\otimes\sigma\mapsto (w\mapsto \sigma(w)v)$. In the case that $V$ is not finite dimensional, this will not be an isomorphism and does not have an inverse. In the finite dimensional case, we do have an inverse, any endomorphism can be written as a product of vectors and dual vectors, but not canonically. Mapping $\mathfrak{so}(V)$ to $\Lambda^2(V)$ seems to require as an intermediate step mapping $\operatorname{End}(V)$ to $V\otimes V^*$. Can this be done canonically? Given the concerns about the direction of the map $V\otimes V^*\to \operatorname{End}(V)$, perhaps I should try to go the other way, but I can't make any progress that way either, because I don't know any identity for the Hodge star operator that will allow me to evaluate expressions like $*(X\wedge Y)$. How does the Hodge star operator interact with the wedge product? Those are the issues I'm running into with my approach. I had somehow expected the I'd appreciate any resolutions to those issues, or alternatively if there is a better approach to this question, or some reason why the question itself is not a good one, I'd love to hear about it. Thanks.",,"['linear-algebra', 'lie-algebras']"
40,clarification on Taylor's Formula,clarification on Taylor's Formula,,"In Linear Algrebra form Hoffman and Kunze, the Taylor's Formula is stated as follows: Theorem 5. (Taylor's Formula) (page 129) Let $\mathbb{F}$ be a field of characteristic zero, $c\in \mathbb{F}$, and $n$ a positive integer. If $f$ is a polynomial over $\mathbb{F}[X]$ with $\deg f \leq n$, then  $$f=\sum_{k=0}^{n}\frac{(D^{k}f)}{k!}(c)(x-c)^{k}.$$ After proving the Theorem, they make some comments.""Although we shall not give any details, it is possible worth mentioning at this point that with the proper interpretation Taylor's Formula is also valid for polynomials over fields of finite characteristics. If the field $\mathbb{F}$ has finite characteristics then we may $k!=0$ in $\mathbb{F}$, in which case the division of $(D^{k})f(c)$ by $k!$ is meaningless. Nevertheless, sense can made out of the division of $D^{k}f$ by $k!$, because every coefficient of $D^{k}f$ is an element of $\mathbb{F}$ multiplied by an integer divisible by $k!$."" Is this the ""proper interpretation""? I was hoping an interpretation over a field  like $GF(2)$. Thanks for your help.","In Linear Algrebra form Hoffman and Kunze, the Taylor's Formula is stated as follows: Theorem 5. (Taylor's Formula) (page 129) Let $\mathbb{F}$ be a field of characteristic zero, $c\in \mathbb{F}$, and $n$ a positive integer. If $f$ is a polynomial over $\mathbb{F}[X]$ with $\deg f \leq n$, then  $$f=\sum_{k=0}^{n}\frac{(D^{k}f)}{k!}(c)(x-c)^{k}.$$ After proving the Theorem, they make some comments.""Although we shall not give any details, it is possible worth mentioning at this point that with the proper interpretation Taylor's Formula is also valid for polynomials over fields of finite characteristics. If the field $\mathbb{F}$ has finite characteristics then we may $k!=0$ in $\mathbb{F}$, in which case the division of $(D^{k})f(c)$ by $k!$ is meaningless. Nevertheless, sense can made out of the division of $D^{k}f$ by $k!$, because every coefficient of $D^{k}f$ is an element of $\mathbb{F}$ multiplied by an integer divisible by $k!$."" Is this the ""proper interpretation""? I was hoping an interpretation over a field  like $GF(2)$. Thanks for your help.",,['linear-algebra']
41,Compute $\det(A^n+B^n)$,Compute,\det(A^n+B^n),"Let $A, B $ be two real $3\times 3 $ matrices, $AB=BA$, and $ \det(A-B)=\det(A^2+B^2)=1,\det(A+B)=3, \det(B)=0 $, then, what is ?   $$\det(A^n+B^n)$$  here $n$ is a positive integer. The problem looks very horrible. Any help will be appreciated","Let $A, B $ be two real $3\times 3 $ matrices, $AB=BA$, and $ \det(A-B)=\det(A^2+B^2)=1,\det(A+B)=3, \det(B)=0 $, then, what is ?   $$\det(A^n+B^n)$$  here $n$ is a positive integer. The problem looks very horrible. Any help will be appreciated",,"['linear-algebra', 'matrices', 'determinant']"
42,All Invariant Subspaces of a Linear Transformation,All Invariant Subspaces of a Linear Transformation,,"I got this problem: Let $T:\mathbb{R}^3 \to \mathbb{R}^3$ be a linear transformation such that all it's eigenvalues are 1, 2 and 3 and the corresponding eigenvectors are $v_1, v_2$ and $v_3$ respectively, Find all the T invariant subspaces of $\mathbb{R}^3$ I found that: All the T invariant subspace of dimension 0 are: $\{0\}$ All the T invariant subspace of dimension 1 are: $span\{v_1\}$ $span\{v_2\}$ and $span\{v_3\}$ All the T invariant subspace of dimension 3 are: $\mathbb{R}^3$ How do I show that all the T invariant subspace of dimension 2 are: $span\{v_1,v_2\}$ $span\{v_1,v_3\}$ and $span\{v_2,v_3\}$ In other words how can I show that this are ALL the 2 dimensional T invariant subspaces and that there are no other T invariant 2 dimensional subspaces?","I got this problem: Let $T:\mathbb{R}^3 \to \mathbb{R}^3$ be a linear transformation such that all it's eigenvalues are 1, 2 and 3 and the corresponding eigenvectors are $v_1, v_2$ and $v_3$ respectively, Find all the T invariant subspaces of $\mathbb{R}^3$ I found that: All the T invariant subspace of dimension 0 are: $\{0\}$ All the T invariant subspace of dimension 1 are: $span\{v_1\}$ $span\{v_2\}$ and $span\{v_3\}$ All the T invariant subspace of dimension 3 are: $\mathbb{R}^3$ How do I show that all the T invariant subspace of dimension 2 are: $span\{v_1,v_2\}$ $span\{v_1,v_3\}$ and $span\{v_2,v_3\}$ In other words how can I show that this are ALL the 2 dimensional T invariant subspaces and that there are no other T invariant 2 dimensional subspaces?",,"['linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors']"
43,Characteristic polynomial of a matrix with zeros on its diagonal,Characteristic polynomial of a matrix with zeros on its diagonal,,"Let $p(x)=x^n+a_{n-2}x^{n-2}+a_{n-3}x^{n-3}+\cdots+a_1x+a_0=(x-\lambda_1)\cdots(x-\lambda_n)$ be a polynomial with real coefficients such that every $\lambda_i$ is real. Is there always a symmetric real $n\times n$ matrix $M$, containing only zeros on its main diagonal such that its characteristic polynomial is $p$?","Let $p(x)=x^n+a_{n-2}x^{n-2}+a_{n-3}x^{n-3}+\cdots+a_1x+a_0=(x-\lambda_1)\cdots(x-\lambda_n)$ be a polynomial with real coefficients such that every $\lambda_i$ is real. Is there always a symmetric real $n\times n$ matrix $M$, containing only zeros on its main diagonal such that its characteristic polynomial is $p$?",,"['linear-algebra', 'matrices']"
44,Why some operations on tensors don't give a tensor?,Why some operations on tensors don't give a tensor?,,"The gradient is a tensor $\nabla f:\mathbf{V} \to \mathbf{R}$ where the partial derivatives are evaluated at some point $(x_0, y_0, z_0)$ . And evaluation of this linear form at some vector $v=(v_1,v_2,v_3)$ gives $$ (\nabla f)(\mathbf{v}) = \partial_x f v_1 + \partial_y f v_2 + \partial_z f v_3 $$ Furthermore in going to a new coordinate system these partial derivatives transform in the expected way. But what about a function $g:\mathbf{V} \to \mathbf{R}$ which is defined only using the partial derivative of $f$ in the $x$ direction. $$ g(\mathbf{v}) = \partial_x f v_1 + \partial_x f v_2 + \partial_x f v_3 $$ As I understand this is not considered a tensor because in moving to a new coordinate system it does not transform correctly. This has confused me endlessly. The strict definition considers a map such as $f:\mathbf{V} \to \mathbf{R}$ a tensor if linearity holds in each parameter. The function $g$ above certainly satisfies that. It seems to me that this definition is not used and that the definition of a tensor that is actually used consists of two parts. linearity in each parameter (i.e. multilinear form), and the algebraic structure of the coefficients is maintained in coordinate transformation Because once we have calculated $\partial_x f$ it is just a scalar and we just hit $(\partial_x f,\partial_x f,\partial_x f)$ with the usual transformation for a covariant vector to get the new coefficients for $g$ in the new coordinate system. That these new coefficients don't have the right algebraic structure doesn't make multilinearity of $g$ go away. Is this at all correct?","The gradient is a tensor where the partial derivatives are evaluated at some point . And evaluation of this linear form at some vector gives Furthermore in going to a new coordinate system these partial derivatives transform in the expected way. But what about a function which is defined only using the partial derivative of in the direction. As I understand this is not considered a tensor because in moving to a new coordinate system it does not transform correctly. This has confused me endlessly. The strict definition considers a map such as a tensor if linearity holds in each parameter. The function above certainly satisfies that. It seems to me that this definition is not used and that the definition of a tensor that is actually used consists of two parts. linearity in each parameter (i.e. multilinear form), and the algebraic structure of the coefficients is maintained in coordinate transformation Because once we have calculated it is just a scalar and we just hit with the usual transformation for a covariant vector to get the new coefficients for in the new coordinate system. That these new coefficients don't have the right algebraic structure doesn't make multilinearity of go away. Is this at all correct?","\nabla f:\mathbf{V} \to \mathbf{R} (x_0, y_0, z_0) v=(v_1,v_2,v_3) 
(\nabla f)(\mathbf{v}) = \partial_x f v_1 + \partial_y f v_2 + \partial_z f v_3
 g:\mathbf{V} \to \mathbf{R} f x 
g(\mathbf{v}) = \partial_x f v_1 + \partial_x f v_2 + \partial_x f v_3
 f:\mathbf{V} \to \mathbf{R} g \partial_x f (\partial_x f,\partial_x f,\partial_x f) g g","['linear-algebra', 'tensors', 'multilinear-algebra']"
45,Integers which are squared norm of 2 by 2 integer matrices,Integers which are squared norm of 2 by 2 integer matrices,,"Question : Which integers are of the form $\Vert A \Vert^2$ , with $A \in M_2(\mathbb{Z})$ . The code below provides the first such integers: $0, 1, 2, 4, 5, 8, 9, 10, 13, 16, 17, 18, 20, 25, 26$ . By searching this sequence on OEIS, we find: ""Numbers that are the sum of 2 squares"" A001481 . Are these integers exactly those which are the sum of two squares ? Research First, $\Vert A  \Vert^2$ is the largest eigenvalue of $A^*A$ , so for $A = \left( \begin{matrix} a & b \cr c & d \end{matrix} \right)$ and $a,b,c,d \in \mathbb{Z}$ , so we get: $$\Vert A  \Vert^2 = \frac{1}{2} \left(a^2+b^2+c^2+d^2+\sqrt{(a^2+b^2+c^2+d^2)^2 - 4(ad-bc)^2}\right)$$ Obviously, every sum of two squares is of the expected form , because by taking $c=d=0$ , we get $\Vert A \Vert^2=a^2+b^2$ . Then it remains to prove that there is no other integer (if true). Now, recall that: Sum of two square theorem An integer greater than one can be   written as a sum of two squares if and only if its prime decomposition   contains no prime congruent to 3 (mod 4) raised to an odd power. By taking $c=ra$ and $d=rb$ , we get that $\Vert A  \Vert^2 = (r^2+1)(a^2+b^2)$ , which is also a sum of two square because the following equation occurs ( proof here ): $$r^2 \not \equiv -1 \mod 4s+3$$ A necessary condition for $\Vert A  \Vert^2$ to be an integer, is that $(a^2+b^2+c^2+d^2)^2 - 4(ad-bc)^2$ must be a square $X^2$ , so that $(X,2(ad-bc),a^2+b^2+c^2+d^2)$ is a Pythagorean triple , so must be of the form $(k(m^2-n^2),2kmn,k(m^2+n^2)$ , and then $\Vert A  \Vert^2 = km^2$ . So it remains to prove that $k$ must be a sum of two squares. sage: L=[] ....: for a in range(-6,6): ....:     for b in range(-6,6): ....:         for c in range(-6,6): ....:             for d in range(-6,6): ....:                 n=numerical_approx(matrix([[a,b],[c,d]]).norm()^2,digits=10) ....:                 if n.is_integer(): ....:                     L.append(int(n)) ....: l=list(set(L)) ....: l.sort() ....: l[:20] ....: [0, 1, 2, 4, 5, 8, 9, 10, 13, 16, 17, 18, 20, 25, 26, 29, 32, 34, 36, 37]","Question : Which integers are of the form , with . The code below provides the first such integers: . By searching this sequence on OEIS, we find: ""Numbers that are the sum of 2 squares"" A001481 . Are these integers exactly those which are the sum of two squares ? Research First, is the largest eigenvalue of , so for and , so we get: Obviously, every sum of two squares is of the expected form , because by taking , we get . Then it remains to prove that there is no other integer (if true). Now, recall that: Sum of two square theorem An integer greater than one can be   written as a sum of two squares if and only if its prime decomposition   contains no prime congruent to 3 (mod 4) raised to an odd power. By taking and , we get that , which is also a sum of two square because the following equation occurs ( proof here ): A necessary condition for to be an integer, is that must be a square , so that is a Pythagorean triple , so must be of the form , and then . So it remains to prove that must be a sum of two squares. sage: L=[] ....: for a in range(-6,6): ....:     for b in range(-6,6): ....:         for c in range(-6,6): ....:             for d in range(-6,6): ....:                 n=numerical_approx(matrix([[a,b],[c,d]]).norm()^2,digits=10) ....:                 if n.is_integer(): ....:                     L.append(int(n)) ....: l=list(set(L)) ....: l.sort() ....: l[:20] ....: [0, 1, 2, 4, 5, 8, 9, 10, 13, 16, 17, 18, 20, 25, 26, 29, 32, 34, 36, 37]","\Vert A \Vert^2 A \in M_2(\mathbb{Z}) 0, 1, 2, 4, 5, 8, 9, 10, 13, 16, 17, 18, 20, 25, 26 \Vert A  \Vert^2 A^*A A = \left( \begin{matrix} a & b \cr c & d \end{matrix} \right) a,b,c,d \in \mathbb{Z} \Vert A  \Vert^2 = \frac{1}{2} \left(a^2+b^2+c^2+d^2+\sqrt{(a^2+b^2+c^2+d^2)^2 - 4(ad-bc)^2}\right) c=d=0 \Vert A \Vert^2=a^2+b^2 c=ra d=rb \Vert A  \Vert^2 = (r^2+1)(a^2+b^2) r^2 \not \equiv -1 \mod 4s+3 \Vert A  \Vert^2 (a^2+b^2+c^2+d^2)^2 - 4(ad-bc)^2 X^2 (X,2(ad-bc),a^2+b^2+c^2+d^2) (k(m^2-n^2),2kmn,k(m^2+n^2) \Vert A  \Vert^2 = km^2 k","['linear-algebra', 'matrices', 'elementary-number-theory']"
46,A matrix inequality involving trace norm of a matrix and its inverse,A matrix inequality involving trace norm of a matrix and its inverse,,"Let $A,B \succeq 0$ be two positive semidefinite matrices. Can we get a closed form expression for the following quantity? $$ \inf_{X \succ 0} \mathrm{tr}(XA) + \mathrm{tr}(X^{-1}B) $$ We assume all matrices involved are symmetric.","Let $A,B \succeq 0$ be two positive semidefinite matrices. Can we get a closed form expression for the following quantity? $$ \inf_{X \succ 0} \mathrm{tr}(XA) + \mathrm{tr}(X^{-1}B) $$ We assume all matrices involved are symmetric.",,"['linear-algebra', 'matrix-calculus', 'trace', 'semidefinite-programming']"
47,Category-Theoretic relation between Orbit-Stabilizer and Rank-Nullity Theorems,Category-Theoretic relation between Orbit-Stabilizer and Rank-Nullity Theorems,,"In linear algebra, the Rank-Nullity theorem states that given a vector space $V$ and an $n\times n$ matrix $A$, $$\text{rank}(A) + \text{null}(A) = n$$ or that $$\text{dim(image}(A)) + \text{dim(ker}(A)) = \text{dim}(V).$$ In abstract algebra, the Orbit-Stabilizer theorem states that given a group $G$ of order $n$, and an element $x$ of the set $G$ acts on, $$|\text{orb}(x)||\text{stab}(x)| = |G|.$$ Other than the visual similarity of the expressions, is there some deeper, perhaps category-theoretic connection between these two theorems? Is there, perhaps, a functor from the category of groups $\text{Grp}$ to some category where linear transformations are morphisms? Am I even using the words functor and morphism correctly in this context?","In linear algebra, the Rank-Nullity theorem states that given a vector space $V$ and an $n\times n$ matrix $A$, $$\text{rank}(A) + \text{null}(A) = n$$ or that $$\text{dim(image}(A)) + \text{dim(ker}(A)) = \text{dim}(V).$$ In abstract algebra, the Orbit-Stabilizer theorem states that given a group $G$ of order $n$, and an element $x$ of the set $G$ acts on, $$|\text{orb}(x)||\text{stab}(x)| = |G|.$$ Other than the visual similarity of the expressions, is there some deeper, perhaps category-theoretic connection between these two theorems? Is there, perhaps, a functor from the category of groups $\text{Grp}$ to some category where linear transformations are morphisms? Am I even using the words functor and morphism correctly in this context?",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'category-theory']"
48,On defining cross (vector) product.,On defining cross (vector) product.,,"This has been bugging me for years so I finally decided to ""derive"" (for lack of a better term) the definition of the cross product in $\mathbb R{^3}$. Here was my method for finding a vector: $\mathbf w = \mathbf u \times \mathbf v$ such that $\mathbf w \cdot \mathbf u = \mathbf w \cdot \mathbf v = 0$, where $\mathbf u = [$$         \begin{matrix}         a & b & c \\         \end{matrix}$$ ]$ and $\mathbf v = [$$         \begin{matrix}         d & e & f \\         \end{matrix}$$ ]$. This of course shows orthogonality between $\mathbf w$ and $\mathbf u$, as well as $\mathbf v$. I set up the 2x3 matrix to solve for $\mathbf w = [$$         \begin{matrix}         w_1 & w_2 & w_3 \\         \end{matrix}$$ ]$ as follows: $ $$         \begin{bmatrix}         a & b & c \\ d & e & f         \end{bmatrix}$$ \cdot \begin{bmatrix}         w_1 \\ w_2 \\ w_3         \end{bmatrix}$$ = \begin{bmatrix}         0 \\ 0         \end{bmatrix}$$ $ Of course this is 3 unknowns and 2 equations, so I knew there would have to be an arbitrary parameter. I was fine with this for the time being and after some dirty work, ended up with the following: $$\begin{bmatrix}         w_1 \\ w_2 \\ w_3         \end{bmatrix}  = t  \begin{bmatrix}         \frac{\begin{vmatrix}b & c \\ e & f\end{vmatrix}}{\begin{vmatrix}a & b \\ d & e\end{vmatrix}} \\ -\frac{\begin{vmatrix}a & c \\ d & f\end{vmatrix}}{\begin{vmatrix}a & b \\ d & e\end{vmatrix}} \\ 1         \end{bmatrix}$$ This looked very much like the ""traditional"" definition of the cross product, so I chose $t = \begin{vmatrix}a & b \\ d & e\end{vmatrix}$ and I finally ended up with $\mathbf w = $$\begin{pmatrix}         \begin{vmatrix}b & c \\ e & f\end{vmatrix}\\-{\begin{vmatrix}a & c \\ d & f\end{vmatrix}} \\ \begin{vmatrix}a & b \\ d & e\end{vmatrix}         \end{pmatrix}$$ $ which is the definition of the cross product that I've seen in pretty much all of my calculus and physics texts (also shown in determinant form with unit vectors). But where does that value for $t$ come from? Why does that particular value of $t$ work, besides my hunch to make it look like a definition that is universally accepted? Is the rationale behind $t$ being negative for $\mathbf w = \mathbf v \times \mathbf u$ just to satisfy the right-hand-rule? Sorry if anything is messed up, this is my first ever time using MathJax. By the way, I've checked similar questions which ask for the rationale for the cross product existing which I have learned from studying electromagnetics myself. But I wanted to see the rational behind the length of the vector, hence my value for $t$. Thanks for any help you can offer!","This has been bugging me for years so I finally decided to ""derive"" (for lack of a better term) the definition of the cross product in $\mathbb R{^3}$. Here was my method for finding a vector: $\mathbf w = \mathbf u \times \mathbf v$ such that $\mathbf w \cdot \mathbf u = \mathbf w \cdot \mathbf v = 0$, where $\mathbf u = [$$         \begin{matrix}         a & b & c \\         \end{matrix}$$ ]$ and $\mathbf v = [$$         \begin{matrix}         d & e & f \\         \end{matrix}$$ ]$. This of course shows orthogonality between $\mathbf w$ and $\mathbf u$, as well as $\mathbf v$. I set up the 2x3 matrix to solve for $\mathbf w = [$$         \begin{matrix}         w_1 & w_2 & w_3 \\         \end{matrix}$$ ]$ as follows: $ $$         \begin{bmatrix}         a & b & c \\ d & e & f         \end{bmatrix}$$ \cdot \begin{bmatrix}         w_1 \\ w_2 \\ w_3         \end{bmatrix}$$ = \begin{bmatrix}         0 \\ 0         \end{bmatrix}$$ $ Of course this is 3 unknowns and 2 equations, so I knew there would have to be an arbitrary parameter. I was fine with this for the time being and after some dirty work, ended up with the following: $$\begin{bmatrix}         w_1 \\ w_2 \\ w_3         \end{bmatrix}  = t  \begin{bmatrix}         \frac{\begin{vmatrix}b & c \\ e & f\end{vmatrix}}{\begin{vmatrix}a & b \\ d & e\end{vmatrix}} \\ -\frac{\begin{vmatrix}a & c \\ d & f\end{vmatrix}}{\begin{vmatrix}a & b \\ d & e\end{vmatrix}} \\ 1         \end{bmatrix}$$ This looked very much like the ""traditional"" definition of the cross product, so I chose $t = \begin{vmatrix}a & b \\ d & e\end{vmatrix}$ and I finally ended up with $\mathbf w = $$\begin{pmatrix}         \begin{vmatrix}b & c \\ e & f\end{vmatrix}\\-{\begin{vmatrix}a & c \\ d & f\end{vmatrix}} \\ \begin{vmatrix}a & b \\ d & e\end{vmatrix}         \end{pmatrix}$$ $ which is the definition of the cross product that I've seen in pretty much all of my calculus and physics texts (also shown in determinant form with unit vectors). But where does that value for $t$ come from? Why does that particular value of $t$ work, besides my hunch to make it look like a definition that is universally accepted? Is the rationale behind $t$ being negative for $\mathbf w = \mathbf v \times \mathbf u$ just to satisfy the right-hand-rule? Sorry if anything is messed up, this is my first ever time using MathJax. By the way, I've checked similar questions which ask for the rationale for the cross product existing which I have learned from studying electromagnetics myself. But I wanted to see the rational behind the length of the vector, hence my value for $t$. Thanks for any help you can offer!",,"['linear-algebra', 'cross-product']"
49,Connection between algebraic multiplicity and dimension of generalized eigenspace,Connection between algebraic multiplicity and dimension of generalized eigenspace,,"Assume $V$ to be a finite dimensional vector space. Define the algebraic multiplicity $am(\lambda)$ of an eigenvalue $\lambda$ of a linear operator $T:V\to V$ as the maximum index of the factor $(t-\lambda)$ appearing in the characteristic polynomial of $T$ . Also define $G_\lambda=\{v\in V:(T-\lambda I)^kv=0\}$ . I want to show that $\dim(G_\lambda)=am(\lambda)$ without using Jordan Form. Sheldon Axler in ""Linear Algebra Done Right"" specifically defined the ""multiplicity"" of $\lambda$ as $\dim(G_\lambda)$ , hence I could not get any help from it. I am not very conversant with the properties of the Jordan form, hence I would like a more elementary proof. Please note that I cannot use the decomposition of $V$ into a direct sum of generalized eigenspaces because I will need to prove that indeed, $am(\lambda)=\dim(G_\lambda)$ to prove this. I started by assuming that $f(t)=(t-\lambda)^kp(t)$ where $f$ is the characteristic polynomial of $T$ , $p$ is any other polynomial not containing the factor $(t-\lambda)$ . So I will have to show that $\dim(G_\lambda)=k$ . By Cayley Hamilton Theorem, $f(T)=0\implies (T-\lambda I)^kp(T)=0$ hence $p(T)v\in G_\lambda \forall v\in V$ . Now consider the collection $\{p(T)v,(T-\lambda I)p(T)v,...,(T-\lambda I)^{k-1}p(T)v\}$ for a nonzero $v\in V$ which I know is linearly independent (based on the previous exercise) and hence $\dim(G_\lambda)\geq k$ . How will the other direction follow?","Assume to be a finite dimensional vector space. Define the algebraic multiplicity of an eigenvalue of a linear operator as the maximum index of the factor appearing in the characteristic polynomial of . Also define . I want to show that without using Jordan Form. Sheldon Axler in ""Linear Algebra Done Right"" specifically defined the ""multiplicity"" of as , hence I could not get any help from it. I am not very conversant with the properties of the Jordan form, hence I would like a more elementary proof. Please note that I cannot use the decomposition of into a direct sum of generalized eigenspaces because I will need to prove that indeed, to prove this. I started by assuming that where is the characteristic polynomial of , is any other polynomial not containing the factor . So I will have to show that . By Cayley Hamilton Theorem, hence . Now consider the collection for a nonzero which I know is linearly independent (based on the previous exercise) and hence . How will the other direction follow?","V am(\lambda) \lambda T:V\to V (t-\lambda) T G_\lambda=\{v\in V:(T-\lambda I)^kv=0\} \dim(G_\lambda)=am(\lambda) \lambda \dim(G_\lambda) V am(\lambda)=\dim(G_\lambda) f(t)=(t-\lambda)^kp(t) f T p (t-\lambda) \dim(G_\lambda)=k f(T)=0\implies (T-\lambda I)^kp(T)=0 p(T)v\in G_\lambda \forall v\in V \{p(T)v,(T-\lambda I)p(T)v,...,(T-\lambda I)^{k-1}p(T)v\} v\in V \dim(G_\lambda)\geq k","['linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors']"
50,invertible if and only if bijective,invertible if and only if bijective,,"I need to show that a linear transformation $T \in L(W,W)$ is invertible if and only if $T$ is bijective. I'm not sure how to go about this, can someone give me an idea where to start? I know is injective if $T_u = T_w$ aka null $T = {0}$, surjective if range $T = W$, and invertible if there is an inverse that give you the identity","I need to show that a linear transformation $T \in L(W,W)$ is invertible if and only if $T$ is bijective. I'm not sure how to go about this, can someone give me an idea where to start? I know is injective if $T_u = T_w$ aka null $T = {0}$, surjective if range $T = W$, and invertible if there is an inverse that give you the identity",,['linear-algebra']
51,Is every square traceless matrix unitarily similar to a zero-diagonal matrix?,Is every square traceless matrix unitarily similar to a zero-diagonal matrix?,,"This question asks for the symmetric case, but after consideration I believe that any complex square matrix with zero trace is unitarily similar to a matrix with zero diagonal. This answer to another related question has a demonstration of the not necessarily unitary affirmative. Is the unitary case known true or false already? For reference, this is what makes me think it is true: Consider the set of values from the diagonal one pair at a time, say $d_0$ and $d_1$. We have the principal submatrix  $$\pmatrix{d_0 & x \\ y & d_1}$$ The general unitary transform (for any $c$ and $s$ such that $cc^* + ss^* = 1$) is \begin{align}   & \pmatrix{c & s \\ -s^* & c^*}\pmatrix{d_0 & x \\ y & d_1}\pmatrix{c^* & -s \\ s^* & c} \\  = & \pmatrix{cd_0 + sy& cx+sd_1 \\ -d_0s^* + c^*y & -s^*x+c^*d_1}\pmatrix{c^* & -s \\ s^* & c} \\  = & \pmatrix{\vert c \vert^2d_0 +\vert s\vert^2d_1 + cs^*x  + c^*sy & -csd_0 - s^2y + c^2x + csd_1\\ -c^*s^*d_0 +(c^*)^2y - (s^*)^2x + csd_1& \vert s \vert^2d_0 +\vert c\vert^2d_1 - c^*sy - cs^*x} \\ \end{align} The question at this point is if for some $c$ and $s$ can we have zero in the bottom right: $$\vert c \vert^2d_0 +\vert s\vert^2d_1 = (cs^*)x + (c^*s)y$$ From this point I visualize on the complex plane. The left side is in terms of only magnitudes. Parameterizing the magnitude ratio of $c$ and $s$ gives the value on a line between the points $d_0$ and $d_1$. The RHS (right hand side) is arbitrary in terms of complex angle. If $x$ and $y$ are large enough, then some angle for $c^*s$ (and opposite angle for $cs^*$) gives equality. The endpoints of the LHS line where $c=0$ or $s=0$ coincide with right hand side zero. At the middle points on the path between $d_0$ and $d_1$, the circle of angle possibilities for the right hand side grows, thus (if $x$ and $y$ are large enough) the possibility of equality exists with appropriate choice of angle for $c$ and $s$. For smaller values of $x$ and $y$, then a point closer to zero is attainable. For $x=0$ and $y=0$ a midpoint between $d_0$ and $d_1$ is closer to zero because the pair may be chosen as such due to the zero trace. Thus an iterative method converging to zero for all points is possible. As this argument is not terribly rigorous, I am wondering if the result is already known? Or would it be worth my time to formalize the argument?","This question asks for the symmetric case, but after consideration I believe that any complex square matrix with zero trace is unitarily similar to a matrix with zero diagonal. This answer to another related question has a demonstration of the not necessarily unitary affirmative. Is the unitary case known true or false already? For reference, this is what makes me think it is true: Consider the set of values from the diagonal one pair at a time, say $d_0$ and $d_1$. We have the principal submatrix  $$\pmatrix{d_0 & x \\ y & d_1}$$ The general unitary transform (for any $c$ and $s$ such that $cc^* + ss^* = 1$) is \begin{align}   & \pmatrix{c & s \\ -s^* & c^*}\pmatrix{d_0 & x \\ y & d_1}\pmatrix{c^* & -s \\ s^* & c} \\  = & \pmatrix{cd_0 + sy& cx+sd_1 \\ -d_0s^* + c^*y & -s^*x+c^*d_1}\pmatrix{c^* & -s \\ s^* & c} \\  = & \pmatrix{\vert c \vert^2d_0 +\vert s\vert^2d_1 + cs^*x  + c^*sy & -csd_0 - s^2y + c^2x + csd_1\\ -c^*s^*d_0 +(c^*)^2y - (s^*)^2x + csd_1& \vert s \vert^2d_0 +\vert c\vert^2d_1 - c^*sy - cs^*x} \\ \end{align} The question at this point is if for some $c$ and $s$ can we have zero in the bottom right: $$\vert c \vert^2d_0 +\vert s\vert^2d_1 = (cs^*)x + (c^*s)y$$ From this point I visualize on the complex plane. The left side is in terms of only magnitudes. Parameterizing the magnitude ratio of $c$ and $s$ gives the value on a line between the points $d_0$ and $d_1$. The RHS (right hand side) is arbitrary in terms of complex angle. If $x$ and $y$ are large enough, then some angle for $c^*s$ (and opposite angle for $cs^*$) gives equality. The endpoints of the LHS line where $c=0$ or $s=0$ coincide with right hand side zero. At the middle points on the path between $d_0$ and $d_1$, the circle of angle possibilities for the right hand side grows, thus (if $x$ and $y$ are large enough) the possibility of equality exists with appropriate choice of angle for $c$ and $s$. For smaller values of $x$ and $y$, then a point closer to zero is attainable. For $x=0$ and $y=0$ a midpoint between $d_0$ and $d_1$ is closer to zero because the pair may be chosen as such due to the zero trace. Thus an iterative method converging to zero for all points is possible. As this argument is not terribly rigorous, I am wondering if the result is already known? Or would it be worth my time to formalize the argument?",,"['linear-algebra', 'matrices']"
52,"Express $\mathrm{Tr}(X)$ in terms of $A$, given that $X=A^TX(I+X)^{-1}A$","Express  in terms of , given that",\mathrm{Tr}(X) A X=A^TX(I+X)^{-1}A,"Given real non-singular $n\times n$ matrix $A$ , with all eigenvalues larger that $1$ . Express $\mathrm{Tr}(X)$ in terms of $A$ , given that $X=A^TX(I+X)^{-1}A$ . $\quad$ ( $X$ is sym. pos. def.) It is allowed to assume that $A$ is in any special form that can be obtained using similarity transformation, i.e. $A$ can be changed by $\hat{A}$ , if $A=P^{-1}\hat{A}P$ , for some nonsingular $P$ . My attempt: For symmetric $A$ case, WLOG we can assume $A$ is diagonal, then \begin{align} X&=AX(I+X)^{-1}A\\ AX^{-1}AX&=I+X\\ AYA&=Y+I\\ (A\otimes A-I)\mathrm{vec}(Y)&=\mathrm{vec}(I)\\ \mathrm{vec}(Y)&=(A\otimes A-I)^{-1}\mathrm{vec}(I), \end{align} where $Y=X^{-1}$ and $\otimes$ denotes Kronecker product . If $\mathrm{vec}(Y)=(A\otimes A-I)^{-1}\mathrm{vec}(I)$ , then $\mathrm{vec}(X)=(A\otimes A-I)\mathrm{vec}(I)$ , using this result . From last eq. it is easy to see that $\mathrm{Tr}(X)=\sum_{i=1}^na^2_i-n$ , where $a_i$ are diagonal elements of $A$ . Assume that all eigenvalues of $(A)$ are equal to $\lambda$ and that Jordan canonical form of $A$ consists of single Jordan block $J$ . Then \begin{align} \mathrm{vec}(Y)&=(A\otimes A-I)^{-1}\mathrm{vec}(I)\\ &=\begin{bmatrix}  \lambda J-I & J &  &&  \\  &\lambda J-I&J& &\\  &  & \ddots & \ddots& \\ &  &  & \lambda J-I & J \\ & &  &  & \lambda J-I \end{bmatrix}^{-1}\mathrm{vec}(I)\\ &=\begin{bmatrix}  M & -MJM & MJMJM & \dots & (-1)^{n-1}MJM\cdots M \\   & \ddots & \ddots & \ddots & \ddots\\   &  & M & -MJM & MJMJM\\ &  &  & M& -MJM\\ &&&&M \end{bmatrix}\mathrm{vec}(I)\\ &=\begin{bmatrix}  TJ^{-1} & -T^2J^{-1} & T^3J^{-1} & \dots & (-1)^{n-1}T^nJ^{-1} \\   & \ddots & \ddots & \ddots & \ddots\\   &  & TJ^{-1} & -T^2J^{-1} & T^3J^{-1}\\ &  &  & TJ^{-1}& -T^2J^{-1}\\ &&&&TJ^{-1} \end{bmatrix}\mathrm{vec}(I) \end{align} where $M=(\lambda J-I)^{-1}$ and $T=MJ=(\lambda I-J^{-1})^{-1}$ . Second last equation is obtained using Block matrix inversion formula . I am not sure what to do next, the expression looks too complicated. Maybe there is an easier way to do it? EDIT: $\qquad$ For $A=\begin{bmatrix}  a_1 & 1\\   0&a_2 \end{bmatrix}$ , we will get $\mathrm{Tr}(X)=(a_1^2+a_2^2-2)+\frac{(a_1^2-1)(a_2^2-1)}{(a_1a_2-1)^2+1}$ . $\qquad$ While For $A=\begin{bmatrix}  a_1 & 0\\   0&a_2 \end{bmatrix}$ , we will get $\mathrm{Tr}(X)=(a_1^2+a_2^2-2)$ . The original problem is equivalent to the following problem: Find $\mathrm{Tr}(Z)$ , where $A'Z^{-1}A+Z=A'A+I$ and $Z-I>0.$","Given real non-singular matrix , with all eigenvalues larger that . Express in terms of , given that . ( is sym. pos. def.) It is allowed to assume that is in any special form that can be obtained using similarity transformation, i.e. can be changed by , if , for some nonsingular . My attempt: For symmetric case, WLOG we can assume is diagonal, then where and denotes Kronecker product . If , then , using this result . From last eq. it is easy to see that , where are diagonal elements of . Assume that all eigenvalues of are equal to and that Jordan canonical form of consists of single Jordan block . Then where and . Second last equation is obtained using Block matrix inversion formula . I am not sure what to do next, the expression looks too complicated. Maybe there is an easier way to do it? EDIT: For , we will get . While For , we will get . The original problem is equivalent to the following problem: Find , where and","n\times n A 1 \mathrm{Tr}(X) A X=A^TX(I+X)^{-1}A \quad X A A \hat{A} A=P^{-1}\hat{A}P P A A \begin{align}
X&=AX(I+X)^{-1}A\\
AX^{-1}AX&=I+X\\
AYA&=Y+I\\
(A\otimes A-I)\mathrm{vec}(Y)&=\mathrm{vec}(I)\\
\mathrm{vec}(Y)&=(A\otimes A-I)^{-1}\mathrm{vec}(I),
\end{align} Y=X^{-1} \otimes \mathrm{vec}(Y)=(A\otimes A-I)^{-1}\mathrm{vec}(I) \mathrm{vec}(X)=(A\otimes A-I)\mathrm{vec}(I) \mathrm{Tr}(X)=\sum_{i=1}^na^2_i-n a_i A (A) \lambda A J \begin{align}
\mathrm{vec}(Y)&=(A\otimes A-I)^{-1}\mathrm{vec}(I)\\
&=\begin{bmatrix} 
\lambda J-I & J &  &&  \\ 
&\lambda J-I&J& &\\
 &  & \ddots & \ddots& \\
&  &  & \lambda J-I & J \\
& &  &  & \lambda J-I
\end{bmatrix}^{-1}\mathrm{vec}(I)\\
&=\begin{bmatrix} 
M & -MJM & MJMJM & \dots & (-1)^{n-1}MJM\cdots M \\ 
 & \ddots & \ddots & \ddots & \ddots\\
  &  & M & -MJM & MJMJM\\
&  &  & M& -MJM\\
&&&&M
\end{bmatrix}\mathrm{vec}(I)\\
&=\begin{bmatrix} 
TJ^{-1} & -T^2J^{-1} & T^3J^{-1} & \dots & (-1)^{n-1}T^nJ^{-1} \\ 
 & \ddots & \ddots & \ddots & \ddots\\
  &  & TJ^{-1} & -T^2J^{-1} & T^3J^{-1}\\
&  &  & TJ^{-1}& -T^2J^{-1}\\
&&&&TJ^{-1}
\end{bmatrix}\mathrm{vec}(I)
\end{align} M=(\lambda J-I)^{-1} T=MJ=(\lambda I-J^{-1})^{-1} \qquad A=\begin{bmatrix} 
a_1 & 1\\ 
 0&a_2
\end{bmatrix} \mathrm{Tr}(X)=(a_1^2+a_2^2-2)+\frac{(a_1^2-1)(a_2^2-1)}{(a_1a_2-1)^2+1} \qquad A=\begin{bmatrix} 
a_1 & 0\\ 
 0&a_2
\end{bmatrix} \mathrm{Tr}(X)=(a_1^2+a_2^2-2) \mathrm{Tr}(Z) A'Z^{-1}A+Z=A'A+I Z-I>0.",['linear-algebra']
53,Meaning of $x^T A x$,Meaning of,x^T A x,"I've seen the term $x^T A x$ , where $A$ is a square and usually symmetric matrix, come up in a bunch of different areas of linear algebra. Places I've seen it include defining the Raleigh quotient, defining positive/negative semi-definite matrices, and in the derivation of PCA. I've also seen it sometimes referred to as describing a quadratic form. Is there some general definition/ intuitive description of what $x^T A x$ means with respect to a vector and a matrix? My sort of vague understanding is that it describes how a vector is changed under a linear transformation defined by $A$ (for example if $A$ causes x to rotate 90 $^\circ$ then $x^T A x = 0$ ) but I can't seem to come up with a more precise or insightful description of $x^T A x$ , and I'm surprised how little I could find online considering how often I see this term come up.","I've seen the term , where is a square and usually symmetric matrix, come up in a bunch of different areas of linear algebra. Places I've seen it include defining the Raleigh quotient, defining positive/negative semi-definite matrices, and in the derivation of PCA. I've also seen it sometimes referred to as describing a quadratic form. Is there some general definition/ intuitive description of what means with respect to a vector and a matrix? My sort of vague understanding is that it describes how a vector is changed under a linear transformation defined by (for example if causes x to rotate 90 then ) but I can't seem to come up with a more precise or insightful description of , and I'm surprised how little I could find online considering how often I see this term come up.",x^T A x A x^T A x A A ^\circ x^T A x = 0 x^T A x,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'quadratic-forms', 'positive-semidefinite']"
54,Matrix equation in characteristic 2,Matrix equation in characteristic 2,,"Let $n\geq 3$ be an odd integer and $K$ be a field with characteristic $2$. Let $A,B\in M_n(K)$ s.t. $A^2+B^2=I_n$; is it true that $AB+BA$ is a singular matrix? Remark. i) It is not difficult to see that the result is false when $n$ is even. ii) The proposition is true for $n=3$; but I do not know the answer if $n=5,7,\cdots$. EDIT. answer to @ i707107 . i) user1551 gave a counterexample for even $n$: $A=\begin{pmatrix}0&1\\1&0\end{pmatrix},B=\begin{pmatrix}0&1\\0&0\end{pmatrix}$. ii) To obtain the reported result for $n=3$, I did a PC computation using Grobner basis theory (the basis contains $234$ elements). The case $n=5$ has too large complexity. Remark. You are right; the problem is equivalent to show that $A+B$ admits $1$ as eigenvalue.","Let $n\geq 3$ be an odd integer and $K$ be a field with characteristic $2$. Let $A,B\in M_n(K)$ s.t. $A^2+B^2=I_n$; is it true that $AB+BA$ is a singular matrix? Remark. i) It is not difficult to see that the result is false when $n$ is even. ii) The proposition is true for $n=3$; but I do not know the answer if $n=5,7,\cdots$. EDIT. answer to @ i707107 . i) user1551 gave a counterexample for even $n$: $A=\begin{pmatrix}0&1\\1&0\end{pmatrix},B=\begin{pmatrix}0&1\\0&0\end{pmatrix}$. ii) To obtain the reported result for $n=3$, I did a PC computation using Grobner basis theory (the basis contains $234$ elements). The case $n=5$ has too large complexity. Remark. You are right; the problem is equivalent to show that $A+B$ admits $1$ as eigenvalue.",,['linear-algebra']
55,Do eigenvalues of a linear transformation over an infinite dimensional vector space appear in conjugate pairs?,Do eigenvalues of a linear transformation over an infinite dimensional vector space appear in conjugate pairs?,,"While attempting to answer a question here (namely, the finite dimensional case of the title question: Prove that if $\lambda$ is an eigenvalue of $T$, a linear transformation whose matrix representation has all real entries, then $\overline{\lambda}$ is an eigenvalue of $T$), I noticed the asker did not specify a finite dimensional vector space. Though the person who asked the question was satisfied with a finite-dimensional response, I was wondering if the analogue was true for infinite dimensional vector spaces. I have seen several proofs of this fact relying on $V$ being finite dimensional. One proof utilizes the roots of the characteristic polynomial; if the coefficients are real then the roots come in conjugate pairs. The second notable proof I've seen goes something like: $$(T-\lambda I)v = 0$$ $$\overline{(T-\lambda I)v} = 0$$ $$(\overline{T} - \overline{\lambda I})\overline{v} = 0$$ $$(T- \overline{\lambda}I)\overline{v} = 0$$ where we define $\overline{T}$ as taking the conjugate of each element of the matrix representation of $T$, and we define $\overline{v}$ as conjugating each entry in the n-tuple representation of $v$ with respect to a basis. Going backwards will give you that, given the conditions set earlier, $\lambda$ is an eigenvalue if and only if $\overline{\lambda}$ is an eigenvalue, and also, $v$ is an eigenvector with eigenvalue $\lambda$ if and only if $\overline{v}$ is an eigenvector with eigenvalue $\overline{\lambda}$. The first thing we would have to do is have some notion that is similar to the matrix representation of $T$ having all real entries. What exactly would that be? Would we have to work with infinite matrices, or (assuming the axiom of choice) could we define $T$ such that it takes basis vectors to linear combinations of basis vectors with real coefficients and that would suffice? If we assume the axiom of choice and take a basis of $V$, I am under the impression that the second proof I provided for the finite dimensional case could extend to the infinite dimensional case. Is it necessary to use the axiom of choice for a proof, though? Overall, my question is:  First, is there an analog of $T$ having all real matrix entries in an infinite dimensional case? Denote this property, if it exists, $P$. Second: Does anyone have a proof or counterexample of the following?: Let $V$ be an infinite dimensional complex vector space, and let $T$ be a linear transformation with $P$. $\lambda$ is an eigenvalue of $T$ if and only if $\overline{\lambda}$ is an eigenvalue of $T$. Can we also add: $v$ is an eigenvector with eigenvalue $\lambda$ if and only if $\overline{v}$ is an eigenvector with eigenvalue $\overline{\lambda}$? Whatever $\overline{v}$ may happen to mean in this case. If we can do this without infinite matrices, infinite basis, or assuming the axiom of choice, I would much prefer that! But I understand it may be necessary.","While attempting to answer a question here (namely, the finite dimensional case of the title question: Prove that if $\lambda$ is an eigenvalue of $T$, a linear transformation whose matrix representation has all real entries, then $\overline{\lambda}$ is an eigenvalue of $T$), I noticed the asker did not specify a finite dimensional vector space. Though the person who asked the question was satisfied with a finite-dimensional response, I was wondering if the analogue was true for infinite dimensional vector spaces. I have seen several proofs of this fact relying on $V$ being finite dimensional. One proof utilizes the roots of the characteristic polynomial; if the coefficients are real then the roots come in conjugate pairs. The second notable proof I've seen goes something like: $$(T-\lambda I)v = 0$$ $$\overline{(T-\lambda I)v} = 0$$ $$(\overline{T} - \overline{\lambda I})\overline{v} = 0$$ $$(T- \overline{\lambda}I)\overline{v} = 0$$ where we define $\overline{T}$ as taking the conjugate of each element of the matrix representation of $T$, and we define $\overline{v}$ as conjugating each entry in the n-tuple representation of $v$ with respect to a basis. Going backwards will give you that, given the conditions set earlier, $\lambda$ is an eigenvalue if and only if $\overline{\lambda}$ is an eigenvalue, and also, $v$ is an eigenvector with eigenvalue $\lambda$ if and only if $\overline{v}$ is an eigenvector with eigenvalue $\overline{\lambda}$. The first thing we would have to do is have some notion that is similar to the matrix representation of $T$ having all real entries. What exactly would that be? Would we have to work with infinite matrices, or (assuming the axiom of choice) could we define $T$ such that it takes basis vectors to linear combinations of basis vectors with real coefficients and that would suffice? If we assume the axiom of choice and take a basis of $V$, I am under the impression that the second proof I provided for the finite dimensional case could extend to the infinite dimensional case. Is it necessary to use the axiom of choice for a proof, though? Overall, my question is:  First, is there an analog of $T$ having all real matrix entries in an infinite dimensional case? Denote this property, if it exists, $P$. Second: Does anyone have a proof or counterexample of the following?: Let $V$ be an infinite dimensional complex vector space, and let $T$ be a linear transformation with $P$. $\lambda$ is an eigenvalue of $T$ if and only if $\overline{\lambda}$ is an eigenvalue of $T$. Can we also add: $v$ is an eigenvector with eigenvalue $\lambda$ if and only if $\overline{v}$ is an eigenvector with eigenvalue $\overline{\lambda}$? Whatever $\overline{v}$ may happen to mean in this case. If we can do this without infinite matrices, infinite basis, or assuming the axiom of choice, I would much prefer that! But I understand it may be necessary.",,"['linear-algebra', 'functional-analysis', 'eigenvalues-eigenvectors', 'infinite-matrices']"
56,An nth-order ODE has n linearly independent solutions,An nth-order ODE has n linearly independent solutions,,"MathWorld states: ""In general, an $n^\text{th}$-order ODE has $n$ linearly independent solutions"". Are they referring to linear ODEs? I only know why it should be true for ODEs with constant coefficients, by the following observations: The solutions to the differential equation $a_0f+\dots +a_nf^{(n)}=0$, where $a_n\ne 0$ form a vector space $V$ (check). Let $f\in C^n(\mathbb{R})$ s.t. $a_0f+\dots +a_nf^{(n)}=0$, where $a_n\ne 0$. Let $\vec{a}=(a_0,a_1,\dots,a_{n-1})$ and $\vec{f}=(f,f^{(1)},\dots,f^{(n-1)})$. $f^{(n)}=-a_n^{-1}(a_0f+\dots +a_{n-1}f^{(n-1)})=-a_n^{-1}\vec{a}\cdot\vec{f}$ is differentiable, and the $m^\text{th}$ derivative of $\vec{b}\cdot\vec{f}$ is: $$\vec b\left(\matrix{\vec{e_2}\\\vdots\\\vec{e_n}\\-a_n^{-1}\vec{a}}\right)^m\cdot \vec{f}$$ Hence $f$ is infinitely differentiable. Moreover, the coefficients above are bounded above by an exponential in $m$. For any closed interval $[-d,d]$, $\vec{f}$ is continuous and therefore bounded. This means that the Taylor series for $f$ converges to $f$ in the interval by Taylor's theorem for the expansion about $x=0$ (using the Lagrange form of the remainder on the whole interval). Hence the Taylor series for $f$ about $x=0$ converges to $f$ for all $\mathbb{R}$, i.e. $f$ is analytic. Now consider the linear transformation $L:V\to\mathbb{R}^n,f\mapsto (f(0),f^{(1)}(0),\dots,f^{(n-1)}(0))$. To prove surjectivity, use the differential equation to produce a Taylor series and show that it is a solution. Injectivity is proven by the below: If $L(f)=L(g)$ for some solutions $f,g$, then $\forall k=0,1,\dots,n-1, f^{(k)}(0)=g^{(k)}(0)$, and by the differential equation, this also holds for all $k\in\mathbb{N}$. $f$ and $g$ are analytic and since the Taylor series is unique, $f=g$. Hence, $V$ has dimension $n$. Is my proof correct? Is the theorem for general linear ODEs true, and how do I prove it?","MathWorld states: ""In general, an $n^\text{th}$-order ODE has $n$ linearly independent solutions"". Are they referring to linear ODEs? I only know why it should be true for ODEs with constant coefficients, by the following observations: The solutions to the differential equation $a_0f+\dots +a_nf^{(n)}=0$, where $a_n\ne 0$ form a vector space $V$ (check). Let $f\in C^n(\mathbb{R})$ s.t. $a_0f+\dots +a_nf^{(n)}=0$, where $a_n\ne 0$. Let $\vec{a}=(a_0,a_1,\dots,a_{n-1})$ and $\vec{f}=(f,f^{(1)},\dots,f^{(n-1)})$. $f^{(n)}=-a_n^{-1}(a_0f+\dots +a_{n-1}f^{(n-1)})=-a_n^{-1}\vec{a}\cdot\vec{f}$ is differentiable, and the $m^\text{th}$ derivative of $\vec{b}\cdot\vec{f}$ is: $$\vec b\left(\matrix{\vec{e_2}\\\vdots\\\vec{e_n}\\-a_n^{-1}\vec{a}}\right)^m\cdot \vec{f}$$ Hence $f$ is infinitely differentiable. Moreover, the coefficients above are bounded above by an exponential in $m$. For any closed interval $[-d,d]$, $\vec{f}$ is continuous and therefore bounded. This means that the Taylor series for $f$ converges to $f$ in the interval by Taylor's theorem for the expansion about $x=0$ (using the Lagrange form of the remainder on the whole interval). Hence the Taylor series for $f$ about $x=0$ converges to $f$ for all $\mathbb{R}$, i.e. $f$ is analytic. Now consider the linear transformation $L:V\to\mathbb{R}^n,f\mapsto (f(0),f^{(1)}(0),\dots,f^{(n-1)}(0))$. To prove surjectivity, use the differential equation to produce a Taylor series and show that it is a solution. Injectivity is proven by the below: If $L(f)=L(g)$ for some solutions $f,g$, then $\forall k=0,1,\dots,n-1, f^{(k)}(0)=g^{(k)}(0)$, and by the differential equation, this also holds for all $k\in\mathbb{N}$. $f$ and $g$ are analytic and since the Taylor series is unique, $f=g$. Hence, $V$ has dimension $n$. Is my proof correct? Is the theorem for general linear ODEs true, and how do I prove it?",,"['linear-algebra', 'ordinary-differential-equations', 'proof-verification']"
57,Other Euler characteristics?,Other Euler characteristics?,,"At the end of V.3.4 in Algebra: Chapter 0 , Aluffi describes the construction of a Grothendieck group over the category of finite dimensional $\operatorname{k}$-vector spaces, $K(\operatorname{k-vect^f})$. You first construct $F^{\operatorname{Ab}}([\operatorname{k-vect^f}])$, the free abelian group on the isomorphism classes of objects in $\operatorname{k-vect^f}$. Then you quotient out the subgroup generated by $[V]-[U]-[W]$ whenever there is an exact  \begin{equation} 0\longrightarrow U\longrightarrow V\longrightarrow W\longrightarrow 0. \end{equation} Although the construction works in more general settings ( over exact categories ), but let's just focus on this $K(\operatorname{k-vect^f})$. In particular, this Grothendieck group gives a generalized Euler characteristic \begin{equation} \chi_K(V_{\bullet})=\sum(-1)^j[V_j], \end{equation} where $V_{\bullet}$ is the complex\begin{equation} 0\longrightarrow V_{N}\longrightarrow V_{N-1}\longrightarrow V_{N-2}\longrightarrow\cdots\longrightarrow V_0\longrightarrow 0. \end{equation} Allufi says this this universal in the sense that if $\delta:\operatorname{k-vect^f}\to G$ is a function from $\operatorname{k-vect^f}$ to an abelian group satisfying two natural conditions: $\delta(V)=\delta(V')$ if $V\cong V'$, and $\delta(V/U)=\delta(V)-\delta(U)$, then there is a unique group homomorphism $\operatorname{k-vect^f}\to G$ that maps \begin{equation} \chi_{K}(V_\bullet)\mapsto \chi_{G}(V_\bullet):=\sum(-1)^j\delta(V_j). \end{equation} Note that the original Euler characteristic is obtained by taking $\delta(V)=\operatorname{dim}(V)$. All this seems impressive, but it would not be powerful if we did not have a supply of other $\delta$ functions other than $\operatorname{dim}$. Unfortunately I could not think of interesting examples of such functions. Can someone give some nice examples? Maybe $\operatorname{k-vect^f}$ is not so good because its Grothendieck group is too simple, so examples from other categories are also welcome. Thanks so much!","At the end of V.3.4 in Algebra: Chapter 0 , Aluffi describes the construction of a Grothendieck group over the category of finite dimensional $\operatorname{k}$-vector spaces, $K(\operatorname{k-vect^f})$. You first construct $F^{\operatorname{Ab}}([\operatorname{k-vect^f}])$, the free abelian group on the isomorphism classes of objects in $\operatorname{k-vect^f}$. Then you quotient out the subgroup generated by $[V]-[U]-[W]$ whenever there is an exact  \begin{equation} 0\longrightarrow U\longrightarrow V\longrightarrow W\longrightarrow 0. \end{equation} Although the construction works in more general settings ( over exact categories ), but let's just focus on this $K(\operatorname{k-vect^f})$. In particular, this Grothendieck group gives a generalized Euler characteristic \begin{equation} \chi_K(V_{\bullet})=\sum(-1)^j[V_j], \end{equation} where $V_{\bullet}$ is the complex\begin{equation} 0\longrightarrow V_{N}\longrightarrow V_{N-1}\longrightarrow V_{N-2}\longrightarrow\cdots\longrightarrow V_0\longrightarrow 0. \end{equation} Allufi says this this universal in the sense that if $\delta:\operatorname{k-vect^f}\to G$ is a function from $\operatorname{k-vect^f}$ to an abelian group satisfying two natural conditions: $\delta(V)=\delta(V')$ if $V\cong V'$, and $\delta(V/U)=\delta(V)-\delta(U)$, then there is a unique group homomorphism $\operatorname{k-vect^f}\to G$ that maps \begin{equation} \chi_{K}(V_\bullet)\mapsto \chi_{G}(V_\bullet):=\sum(-1)^j\delta(V_j). \end{equation} Note that the original Euler characteristic is obtained by taking $\delta(V)=\operatorname{dim}(V)$. All this seems impressive, but it would not be powerful if we did not have a supply of other $\delta$ functions other than $\operatorname{dim}$. Unfortunately I could not think of interesting examples of such functions. Can someone give some nice examples? Maybe $\operatorname{k-vect^f}$ is not so good because its Grothendieck group is too simple, so examples from other categories are also welcome. Thanks so much!",,"['linear-algebra', 'abstract-algebra', 'field-theory', 'category-theory', 'examples-counterexamples']"
58,Show trace is zero,Show trace is zero,,"Problem : We are given $n\times n$ square matrices $A$ and $B$ with $AB+BA=0$ and $A^2+B^2=I$. Show $tr(A)=tr(B)=0$. Thoughts : We have $tr(BA)=tr(AB)=-tr(BA)=0$. We also have the factorizations $(A+B)^2=I$ and $(A-B)^2=I$ by combining the two relations above. Let $\alpha_i$ denote the eigenvalues of $A$, and $\beta_i$ the eigenvalues of $B$. We have, by basic properties of trace, $\sum \alpha_i^2 +\sum \beta_i^2=n$ from $A^2+B^2=I$. I'm not sure where to go from here. I would prefer a small hint to a complete answer.","Problem : We are given $n\times n$ square matrices $A$ and $B$ with $AB+BA=0$ and $A^2+B^2=I$. Show $tr(A)=tr(B)=0$. Thoughts : We have $tr(BA)=tr(AB)=-tr(BA)=0$. We also have the factorizations $(A+B)^2=I$ and $(A-B)^2=I$ by combining the two relations above. Let $\alpha_i$ denote the eigenvalues of $A$, and $\beta_i$ the eigenvalues of $B$. We have, by basic properties of trace, $\sum \alpha_i^2 +\sum \beta_i^2=n$ from $A^2+B^2=I$. I'm not sure where to go from here. I would prefer a small hint to a complete answer.",,['linear-algebra']
59,Unexpectedly simple patterns for the determinants of some matrices,Unexpectedly simple patterns for the determinants of some matrices,,"Edit: ""Spoiler"" Since it's a pretty wordy question, here's a quick spoiler... Why is the following true? $$\det \begin{pmatrix} 0 & 1 & 2\\ 1 & 0 & 1 \\ 2 & 1 & 0 \end{pmatrix} =\det \begin{pmatrix} 0 & 1 & 2 & 0 & 1 & 2\\ 1 & 0 & 1 & 2 & 0 & 1\\ 2 & 1 & 0 & 1 & 2 & 0 \\ 0 & 2 & 1 & 0 & 1 & 2 \\ 1 & 0 & 2 & 1 & 0 & 1 \\ 2 & 1 & 0 & 2 & 1 & 0\end{pmatrix} = \det \begin{pmatrix} 0 & 1 & 2 & 0 & 1 & 2 & 0 & 1 & 2 \\ 1 & 0 & 1 & 2 & 0 & 1 & 2 & 0 & 1\\ 2 & 1 & 0 & 1 & 2 & 0 & 1 & 2 & 0 \\ 0 & 2 & 1 & 0 & 1 & 2 & 0 & 1 & 2\\ 1 & 0 & 2 & 1 & 0 & 1  & 2 & 0 & 1\\ 2 & 1 & 0 & 2 & 1 & 0 & 1 & 2 & 0 \\ 0& 2 & 1 & 0 & 2 & 1 & 0 & 1 & 2 \\ 1 & 0 & 2 & 1 & 0 & 2 & 1 & 0 & 1 \\ 2 & 1 & 0 & 2 & 1 & 0 & 2 & 1 & 0\end{pmatrix} = \dots $$ Consider the matrix $$A=\begin{pmatrix} 0 & 1 & 2\\ 1 & 0 & 1 \\ 2 & 1 & 0 \end{pmatrix}\,.$$ It can be easily evaluated that $\det A = 4$ . More in general it's easy to show (by direct calculation) that given $x\in\mathbb{R}$ and defining $$A(x) = \begin{pmatrix} x-1 & x & x+1 \\ x & x-1 & x \\ x+1 & x & x-1\end{pmatrix}$$ then $\det A(x) = 4x$ . The interesting fact is that these matrices can be ""expanded"" in a way such that the determinant is invariant. Additionally, for a larger class of matrices there seems to be some ""simple"" regular patterns concerning the determinant. Introducing some notation... First, I need to introduce some notation. Let $\mathbf{c} = \{c_1,c_2\dots c_n\}$ . I'll denote $T(\mathbf{c})$ the $n\times n$ symmetric Toeplix matrix whose principal and upper diagonals are given by the coefficients $c_1\dots c_n$ . I mean something like $$T(\{c_1,c_2,c_3,c_4\}) = \begin{pmatrix} c_1 & c_2 & c_3 & c_4\\c_2 & c_1 & c_2 & c_3 \\ c_3 & c_2 & c_1 & c_2 \\ c_4 & c_3 & c_2 & c_1  \end{pmatrix}\,.$$ If we call $\mathbf{v}(x) = \{x-1,x,x+1\}$ , then $A(x) = T(\mathbf{v}(x))$ . Finally, given a $n$ -dimensional vector $\mathbf{c} = \{c_1\dots c_n\}$ , I'll call $\mathbf{c}^k$ the $(k\cdot n)$ -dimensional vector obtained joining together $k$ copies of $\mathbf{c}$ . For example $$\{c_1,c_2,c_3,c_4\}^3 = \{c_1,c_2,c_3,c_4,c_1,c_2,c_3,c_4,c_1,c_2,c_3,c_4\}\,.$$ The main question I've stated at the beginning that $\det A(x) = 4x$ . With the above notation, $\det T(\mathbf{v}(x)) = 4x$ . Actually it seems to be true (at least for what I've tried with Mathematica) that for all positive integer $k$ $$\det T(\mathbf{v}^k(x)) = 4x\,.$$ I guess this result can be proven by induction on $k$ , but it seems to be a bit painful. I would expect some simple and clean proof for what seems to be such a neat result. Any ideas about what's going on and why the determinants are so simple? Going a bit further... Having noticed that things were so simple for $\mathbf{v}(x)=\{x-1,x,x+1\}$ , the first thing I've tried is to slightly change $\mathbf{v}$ . Let's now consider $T(\{x-2,x-1,x,x+1,x+2\}^k)$ . Unfortunatly in this case things get much more complicated. For $k=1$ the determinant is $16 x$ . But then for $k=2$ it's $113288 x$ , for $k=3$ $65157184 x$ and so on. Things are clearly much messier here. But... Let's define $\mathbf{w}(x) = \{x+2,x-1,x,x+1,x-2\}$ . Then the sequence of determinants seems to be very regular. \begin{align} &\det T(\mathbf{w}(x)) = 16 x\\ &\det T(\mathbf{w}^2(x)) = -8 x\\ &\det T(\mathbf{w}^3(x)) = 0\\ &\det T(\mathbf{w}^4(x)) = -8 x\\ &\det T(\mathbf{w}^5(x)) = 16 x\\ &\det T(\mathbf{w}^6(x)) = -8 x\\ &\det T(\mathbf{w}^7(x)) = 0\\ &\det T(\mathbf{w}^8(x)) = -8 x \end{align} and so on. So there is a clear pattern in the dependence on $k$ : $$\{16, -8, 0, -8, 16, -8, 0, -8, 16, -8, 0, -8, 16, -8, 0, -8, 16, -8, 0, -8,\dots\}\,.$$ Then we may look at $T(\{x-3,x+2,x-1,x,x+1,x-2,x+3\})$ and again there is a pattern: $$\{64, 12, 4, 0, 4, 12, 64, 12, 4, 0, 4, 12, 64, 12, 4, 0, 4, 12, 64, \dots\}\,.$$ And again for $T(\{x+4,x-3,x+2,x-1,x,x+1,x-2,x+3,x-4\})$ a new pattern: $$\{256, -16, 0, -16, 0, -16, 0, -16, 256, -16, 0, -16, 0, -16, 0, -16, 256, -16, 0, -16,\dots\}\,.$$ I would bet in the existence of a simple explanation for these patterns, but as for now I've really no clue. Any ideas?","Edit: ""Spoiler"" Since it's a pretty wordy question, here's a quick spoiler... Why is the following true? Consider the matrix It can be easily evaluated that . More in general it's easy to show (by direct calculation) that given and defining then . The interesting fact is that these matrices can be ""expanded"" in a way such that the determinant is invariant. Additionally, for a larger class of matrices there seems to be some ""simple"" regular patterns concerning the determinant. Introducing some notation... First, I need to introduce some notation. Let . I'll denote the symmetric Toeplix matrix whose principal and upper diagonals are given by the coefficients . I mean something like If we call , then . Finally, given a -dimensional vector , I'll call the -dimensional vector obtained joining together copies of . For example The main question I've stated at the beginning that . With the above notation, . Actually it seems to be true (at least for what I've tried with Mathematica) that for all positive integer I guess this result can be proven by induction on , but it seems to be a bit painful. I would expect some simple and clean proof for what seems to be such a neat result. Any ideas about what's going on and why the determinants are so simple? Going a bit further... Having noticed that things were so simple for , the first thing I've tried is to slightly change . Let's now consider . Unfortunatly in this case things get much more complicated. For the determinant is . But then for it's , for and so on. Things are clearly much messier here. But... Let's define . Then the sequence of determinants seems to be very regular. and so on. So there is a clear pattern in the dependence on : Then we may look at and again there is a pattern: And again for a new pattern: I would bet in the existence of a simple explanation for these patterns, but as for now I've really no clue. Any ideas?","\det \begin{pmatrix} 0 & 1 & 2\\ 1 & 0 & 1 \\ 2 & 1 & 0 \end{pmatrix} =\det \begin{pmatrix} 0 & 1 & 2 & 0 & 1 & 2\\ 1 & 0 & 1 & 2 & 0 & 1\\ 2 & 1 & 0 & 1 & 2 & 0 \\ 0 & 2 & 1 & 0 & 1 & 2 \\ 1 & 0 & 2 & 1 & 0 & 1 \\ 2 & 1 & 0 & 2 & 1 & 0\end{pmatrix} = \det \begin{pmatrix} 0 & 1 & 2 & 0 & 1 & 2 & 0 & 1 & 2 \\ 1 & 0 & 1 & 2 & 0 & 1 & 2 & 0 & 1\\ 2 & 1 & 0 & 1 & 2 & 0 & 1 & 2 & 0 \\ 0 & 2 & 1 & 0 & 1 & 2 & 0 & 1 & 2\\ 1 & 0 & 2 & 1 & 0 & 1  & 2 & 0 & 1\\ 2 & 1 & 0 & 2 & 1 & 0 & 1 & 2 & 0 \\ 0& 2 & 1 & 0 & 2 & 1 & 0 & 1 & 2 \\ 1 & 0 & 2 & 1 & 0 & 2 & 1 & 0 & 1 \\ 2 & 1 & 0 & 2 & 1 & 0 & 2 & 1 & 0\end{pmatrix} = \dots  A=\begin{pmatrix} 0 & 1 & 2\\ 1 & 0 & 1 \\ 2 & 1 & 0 \end{pmatrix}\,. \det A = 4 x\in\mathbb{R} A(x) = \begin{pmatrix} x-1 & x & x+1 \\ x & x-1 & x \\ x+1 & x & x-1\end{pmatrix} \det A(x) = 4x \mathbf{c} = \{c_1,c_2\dots c_n\} T(\mathbf{c}) n\times n c_1\dots c_n T(\{c_1,c_2,c_3,c_4\}) = \begin{pmatrix} c_1 & c_2 & c_3 & c_4\\c_2 & c_1 & c_2 & c_3 \\ c_3 & c_2 & c_1 & c_2 \\ c_4 & c_3 & c_2 & c_1  \end{pmatrix}\,. \mathbf{v}(x) = \{x-1,x,x+1\} A(x) = T(\mathbf{v}(x)) n \mathbf{c} = \{c_1\dots c_n\} \mathbf{c}^k (k\cdot n) k \mathbf{c} \{c_1,c_2,c_3,c_4\}^3 = \{c_1,c_2,c_3,c_4,c_1,c_2,c_3,c_4,c_1,c_2,c_3,c_4\}\,. \det A(x) = 4x \det T(\mathbf{v}(x)) = 4x k \det T(\mathbf{v}^k(x)) = 4x\,. k \mathbf{v}(x)=\{x-1,x,x+1\} \mathbf{v} T(\{x-2,x-1,x,x+1,x+2\}^k) k=1 16 x k=2 113288 x k=3 65157184 x \mathbf{w}(x) = \{x+2,x-1,x,x+1,x-2\} \begin{align}
&\det T(\mathbf{w}(x)) = 16 x\\
&\det T(\mathbf{w}^2(x)) = -8 x\\
&\det T(\mathbf{w}^3(x)) = 0\\
&\det T(\mathbf{w}^4(x)) = -8 x\\
&\det T(\mathbf{w}^5(x)) = 16 x\\
&\det T(\mathbf{w}^6(x)) = -8 x\\
&\det T(\mathbf{w}^7(x)) = 0\\
&\det T(\mathbf{w}^8(x)) = -8 x
\end{align} k \{16, -8, 0, -8, 16, -8, 0, -8, 16, -8, 0, -8, 16, -8, 0, -8, 16, -8, 0, -8,\dots\}\,. T(\{x-3,x+2,x-1,x,x+1,x-2,x+3\}) \{64, 12, 4, 0, 4, 12, 64, 12, 4, 0, 4, 12, 64, 12, 4, 0, 4, 12, 64, \dots\}\,. T(\{x+4,x-3,x+2,x-1,x,x+1,x-2,x+3,x-4\}) \{256, -16, 0, -16, 0, -16, 0, -16, 256, -16, 0, -16, 0, -16, 0, -16, 256, -16, 0, -16,\dots\}\,.","['linear-algebra', 'matrices', 'determinant', 'toeplitz-matrices']"
60,Kantorovich inequality and Cauchy-Schwarz inequality,Kantorovich inequality and Cauchy-Schwarz inequality,,"On the wikipedia site for the Kantorovich inequality, it is claimed ... the Kantorovich inequality is a particular case of the Cauchy–Schwarz inequality... Here, ""Kantorovich inequality"" refers to $$ (x^\top A \, x) \, (x^\top A^{-1} \, x) \le \frac{(m+M)^2}{4 \, m \, M} \, \|x\|^4 $$ for a symmetric, positive definite matrix $A$ and $m$ , $M$ denote the smallest and largest eigenvalue of $A$ . I was wondering what is meant by the above claim of wikipedia. Is it really true that the Kantorovich inequality is a particular case of CSI (in the sense of: ""can be easily derived from"")? The closest assertion I was able to find is from this paper : If we plug in $x = \sqrt{A} \, y$ , we arrive at $$ \| A \, y\|^2 \, \|y\|^2 \le \frac{(m+M)^2}{4 \, m \, M} \, (y^\top A \, y)^2 $$ which is a reverse of the special case $$ y^\top A \, y \le \|A \, y\| \, \|y\| $$ of CSI.","On the wikipedia site for the Kantorovich inequality, it is claimed ... the Kantorovich inequality is a particular case of the Cauchy–Schwarz inequality... Here, ""Kantorovich inequality"" refers to for a symmetric, positive definite matrix and , denote the smallest and largest eigenvalue of . I was wondering what is meant by the above claim of wikipedia. Is it really true that the Kantorovich inequality is a particular case of CSI (in the sense of: ""can be easily derived from"")? The closest assertion I was able to find is from this paper : If we plug in , we arrive at which is a reverse of the special case of CSI.","
(x^\top A \, x) \, (x^\top A^{-1} \, x) \le \frac{(m+M)^2}{4 \, m \, M} \, \|x\|^4
 A m M A x = \sqrt{A} \, y 
\| A \, y\|^2 \, \|y\|^2 \le \frac{(m+M)^2}{4 \, m \, M} \, (y^\top A \, y)^2
 
y^\top A \, y \le \|A \, y\| \, \|y\|
","['linear-algebra', 'inequality', 'eigenvalues-eigenvectors', 'cauchy-schwarz-inequality']"
61,Let $S$ be a diagonalizable matrix and $S+5T=I$. Then prove that $T$ is also diagonalizable.,Let  be a diagonalizable matrix and . Then prove that  is also diagonalizable.,S S+5T=I T,"My solution: Since $S$ is diagonalizable, so we can write $S=P^{-1}DP$ , where $P$ is an invertible matrix and $D$ is a diagonal matrix. Now $5T=I-S=P^{-1}P-P^{-1}DP=P^{-1}(I-D)P$ . So $T=P^{-1}\frac{1}{5}(I-D)P$ .     Since $I-D$ is also a diagonal matrix, hence $T$ is diagonalizable. Is my proof correct? Can it be done in another way? thanks.","My solution: Since is diagonalizable, so we can write , where is an invertible matrix and is a diagonal matrix. Now . So .     Since is also a diagonal matrix, hence is diagonalizable. Is my proof correct? Can it be done in another way? thanks.",S S=P^{-1}DP P D 5T=I-S=P^{-1}P-P^{-1}DP=P^{-1}(I-D)P T=P^{-1}\frac{1}{5}(I-D)P I-D T,"['linear-algebra', 'matrices', 'proof-verification', 'diagonalization']"
62,Variable leaving basis in linear programming - when does it happen?,Variable leaving basis in linear programming - when does it happen?,,"In the simplex algorithm in linear programming , what are conditions for a variable to leave a basis (not necessarily basis for the/an optimal solution) ? I'm supposed to list as many sufficient and necessary conditions as possible for some basic variable $x_q$ which could be slack, artificial or non-slack and non-artificial. Let $x_q$ be the s-th basic variable. Suppose the s-th row of some current simplex tableau has 1 in the column of $x_q$ and 0's everywhere else. Under what circumstances, if any, might $x_q$ leave the basis? Can any of the values in the s-th row of the tableau ever change? Well since it's a basic variable, I'm guessing the $x_q$ column already has 0's everywhere except in the s-th row. Now, the $x_q$ row has 0's everywhere in the column of $x_q$ like: This is in the context of the Big M Method and artificial variables . I'm not quite sure what the relationship is exactly, though. Edit: It looks like one of the constraints is the original (maximisation?) problem has something like $$x_2 = 10$$ or $$x_4 = 0$$ I guess the relationship would be Big M Method applies for equality constraint? But I think an equality constraint like for example $$x_4 = 0$$ would lead to $$x_4 + x_5 = 0$$ with $z$ being replaced with $z' = z - Mx_5$ So $$x_4 + x_5 = 0$$ doesn't exactly lead to a row of all but one zero entry? There are two non-zero entries? What I tried: $x_q$ leaves if there is some non-basic variable $x_r$ that enters because $$z_r - c_r < 0$$ $$z_r - c_r = \min_j (z_j - c_j)$$ $$\frac{b_q'}{a_{qr}'} = \min_i \{\frac{b_i'}{a_{ir}'} | a_{ir}' > 0 \}$$ Is that right? Any other sufficient or necessary conditions? What is the relevance of the 0's in the row? Edit: I guess an example would be something like \begin{bmatrix} 2 & 0 & 10\\  0 & 1 & 0\\  5 & 0 & 6 \end{bmatrix} If $x_q$ leaves and then $x_r$ enters where $x_q$'s column is the second column, and then $x_r$ is, say, the first column. What would be the EROs? $$\color{red}{\frac{1}{2}R_1 + R_2 \to R_2}$$ $$-2R_2 + R_1 \to R_1$$ $$-5R_2 + R_3 \to R_3$$ I have never had to make $\color{red}{\text{a zero entry to a non-zero number}}$ in the simplex method. I find this suspicious. Should I not? Perhaps the elements in the row can never change because $x_q$ can never leave? Or $x_q$ can never leave because row can never change?","In the simplex algorithm in linear programming , what are conditions for a variable to leave a basis (not necessarily basis for the/an optimal solution) ? I'm supposed to list as many sufficient and necessary conditions as possible for some basic variable $x_q$ which could be slack, artificial or non-slack and non-artificial. Let $x_q$ be the s-th basic variable. Suppose the s-th row of some current simplex tableau has 1 in the column of $x_q$ and 0's everywhere else. Under what circumstances, if any, might $x_q$ leave the basis? Can any of the values in the s-th row of the tableau ever change? Well since it's a basic variable, I'm guessing the $x_q$ column already has 0's everywhere except in the s-th row. Now, the $x_q$ row has 0's everywhere in the column of $x_q$ like: This is in the context of the Big M Method and artificial variables . I'm not quite sure what the relationship is exactly, though. Edit: It looks like one of the constraints is the original (maximisation?) problem has something like $$x_2 = 10$$ or $$x_4 = 0$$ I guess the relationship would be Big M Method applies for equality constraint? But I think an equality constraint like for example $$x_4 = 0$$ would lead to $$x_4 + x_5 = 0$$ with $z$ being replaced with $z' = z - Mx_5$ So $$x_4 + x_5 = 0$$ doesn't exactly lead to a row of all but one zero entry? There are two non-zero entries? What I tried: $x_q$ leaves if there is some non-basic variable $x_r$ that enters because $$z_r - c_r < 0$$ $$z_r - c_r = \min_j (z_j - c_j)$$ $$\frac{b_q'}{a_{qr}'} = \min_i \{\frac{b_i'}{a_{ir}'} | a_{ir}' > 0 \}$$ Is that right? Any other sufficient or necessary conditions? What is the relevance of the 0's in the row? Edit: I guess an example would be something like \begin{bmatrix} 2 & 0 & 10\\  0 & 1 & 0\\  5 & 0 & 6 \end{bmatrix} If $x_q$ leaves and then $x_r$ enters where $x_q$'s column is the second column, and then $x_r$ is, say, the first column. What would be the EROs? $$\color{red}{\frac{1}{2}R_1 + R_2 \to R_2}$$ $$-2R_2 + R_1 \to R_1$$ $$-5R_2 + R_3 \to R_3$$ I have never had to make $\color{red}{\text{a zero entry to a non-zero number}}$ in the simplex method. I find this suspicious. Should I not? Perhaps the elements in the row can never change because $x_q$ can never leave? Or $x_q$ can never leave because row can never change?",,"['linear-algebra', 'inequality', 'optimization', 'linear-programming', 'operations-research']"
63,"Prove that if positive-definite $f$ is continuous at $0$, it is continuous on $\mathbb{R}$","Prove that if positive-definite  is continuous at , it is continuous on",f 0 \mathbb{R},"Long story short, the question I'm stuck on is as follows: Let $f$ be a positive-definite function.  Prove that if $f$ is continuous at $0$, then it is continuous everywhere. Here's the long version: We say that a function $f:\mathbb{R}\to \mathbb{C}$ is positive definite if the matrix $A_f[\{t_1,t_2,\dots,t_n\}]$, whose entries are given by  $$ A_f[\{t_1,t_2,\dots,t_n\}]=[f(t_i-t_j)]_{i,j=1}^n $$ Is positive semidefinite for all choices of $t_1,\dots,t_n \in \mathbb{R}$.  In the whole problem, we are meant to show that $f$ has the following properties: $f(-t) = \overline{f(t)}$ $f(0) \in \mathbb{R}$ and $f(0) \geq 0$ $|f(t)|\leq f(0)$ for all $t \in \mathbb{R}$ if $f$ is continuous at $0$, then it is continuous everywhere The first three parts may all be solved by considering the $2\times 2$ matrix $A_f[0,t]$ where $t\in \mathbb{R}$ is arbitrary. Because $A_f[0,t]$ is Hermitian, the first statement holds.  Because $A_f[0,t]$ must have non-negative trace, we conclude that the second statement holds.  Becuase $A_f[0,t]$ has a non-negative determinant, we conclude that the third statement holds.  That fourth statement, however, has me stumped. As far as I can tell, there is no more insight to be gleaned from $2\times 2$ matrices.  Presumably, I need to find an upper bound for $|f(t) - f(t+\delta)|$ given that $|f(\delta) - f(0)|$ can be made arbitrarily small.  I've noticed that $\det A_f[0,t,t+\delta]$ can be finagled into something like $f(0)|f(t) - f(t+\delta)|^2$.  However, it's not clear to me how I would use this to the desired ends. There's also a good chance that I've managed to think myself into a hole, given that this one small part of one problem has given me more trouble than the rest of the assignment. The question claims that this problem can be solved using the fact that a semi-definite matrix has a non-negative trace and determinant, and that all principal submatrices have a non-negative determinant. I think that just about covers it.  If you've made it this far, thank you for your time; I tried not to make this a wall of text.  Any helpful nudges in the right direction would be very much appreciated; an attempt at an answer doubly so.","Long story short, the question I'm stuck on is as follows: Let $f$ be a positive-definite function.  Prove that if $f$ is continuous at $0$, then it is continuous everywhere. Here's the long version: We say that a function $f:\mathbb{R}\to \mathbb{C}$ is positive definite if the matrix $A_f[\{t_1,t_2,\dots,t_n\}]$, whose entries are given by  $$ A_f[\{t_1,t_2,\dots,t_n\}]=[f(t_i-t_j)]_{i,j=1}^n $$ Is positive semidefinite for all choices of $t_1,\dots,t_n \in \mathbb{R}$.  In the whole problem, we are meant to show that $f$ has the following properties: $f(-t) = \overline{f(t)}$ $f(0) \in \mathbb{R}$ and $f(0) \geq 0$ $|f(t)|\leq f(0)$ for all $t \in \mathbb{R}$ if $f$ is continuous at $0$, then it is continuous everywhere The first three parts may all be solved by considering the $2\times 2$ matrix $A_f[0,t]$ where $t\in \mathbb{R}$ is arbitrary. Because $A_f[0,t]$ is Hermitian, the first statement holds.  Because $A_f[0,t]$ must have non-negative trace, we conclude that the second statement holds.  Becuase $A_f[0,t]$ has a non-negative determinant, we conclude that the third statement holds.  That fourth statement, however, has me stumped. As far as I can tell, there is no more insight to be gleaned from $2\times 2$ matrices.  Presumably, I need to find an upper bound for $|f(t) - f(t+\delta)|$ given that $|f(\delta) - f(0)|$ can be made arbitrarily small.  I've noticed that $\det A_f[0,t,t+\delta]$ can be finagled into something like $f(0)|f(t) - f(t+\delta)|^2$.  However, it's not clear to me how I would use this to the desired ends. There's also a good chance that I've managed to think myself into a hole, given that this one small part of one problem has given me more trouble than the rest of the assignment. The question claims that this problem can be solved using the fact that a semi-definite matrix has a non-negative trace and determinant, and that all principal submatrices have a non-negative determinant. I think that just about covers it.  If you've made it this far, thank you for your time; I tried not to make this a wall of text.  Any helpful nudges in the right direction would be very much appreciated; an attempt at an answer doubly so.",,"['linear-algebra', 'matrices']"
64,Vorticity equation in index notation (curl of Navier-Stokes equation),Vorticity equation in index notation (curl of Navier-Stokes equation),,"I am trying to derive the vorticity equation and I got stuck when trying to prove the following relation using index notation: $$ {\rm curl}((\textbf{u}\cdot\nabla)\mathbf{u}) = (\mathbf{u}\cdot\nabla)\pmb\omega - ( \pmb\omega \cdot\nabla)\mathbf{u} $$ considering that the fluid is incompressible $\nabla\cdot\mathbf{u} = 0 $, $\pmb \omega = {\rm curl}(\mathbf{u})$ and that $\nabla \cdot \pmb \omega = 0.$ Here follows what I've done so far: $$ (\textbf{u}\cdot\nabla) \mathbf{u} = u_m\frac{\partial u_i}{\partial x_m} \mathbf{e}_i  = a_i  \mathbf{e}_i \\ {\rm curl}(\mathbf{a}) = \epsilon_{ijk} \frac{\partial a_k}{\partial x_j} \mathbf{e}_i = \epsilon_{ijk} \frac{\partial}{\partial x_j}\left( u_m\frac{\partial u_k}{\partial x_m} \right) \mathbf{e}_i = \\ = \epsilon_{ijk}\frac{\partial u_m}{\partial x_j}\frac{\partial u_k}{\partial x_m}  \mathbf{e}_i + \epsilon_{ijk}u_m \frac{\partial^2u_k}{\partial x_j \partial x_m} \mathbf{e}_i    \\ $$ the second term $\epsilon_{ijk}u_m \frac{\partial^2u_k}{\partial x_j \partial x_m} \mathbf{e}_i$ seems to be the first term ""$(\mathbf{u}\cdot\nabla)\pmb\omega$"" from the forementioned identity. Does anyone have an idea  how to get the second term?","I am trying to derive the vorticity equation and I got stuck when trying to prove the following relation using index notation: $$ {\rm curl}((\textbf{u}\cdot\nabla)\mathbf{u}) = (\mathbf{u}\cdot\nabla)\pmb\omega - ( \pmb\omega \cdot\nabla)\mathbf{u} $$ considering that the fluid is incompressible $\nabla\cdot\mathbf{u} = 0 $, $\pmb \omega = {\rm curl}(\mathbf{u})$ and that $\nabla \cdot \pmb \omega = 0.$ Here follows what I've done so far: $$ (\textbf{u}\cdot\nabla) \mathbf{u} = u_m\frac{\partial u_i}{\partial x_m} \mathbf{e}_i  = a_i  \mathbf{e}_i \\ {\rm curl}(\mathbf{a}) = \epsilon_{ijk} \frac{\partial a_k}{\partial x_j} \mathbf{e}_i = \epsilon_{ijk} \frac{\partial}{\partial x_j}\left( u_m\frac{\partial u_k}{\partial x_m} \right) \mathbf{e}_i = \\ = \epsilon_{ijk}\frac{\partial u_m}{\partial x_j}\frac{\partial u_k}{\partial x_m}  \mathbf{e}_i + \epsilon_{ijk}u_m \frac{\partial^2u_k}{\partial x_j \partial x_m} \mathbf{e}_i    \\ $$ the second term $\epsilon_{ijk}u_m \frac{\partial^2u_k}{\partial x_j \partial x_m} \mathbf{e}_i$ seems to be the first term ""$(\mathbf{u}\cdot\nabla)\pmb\omega$"" from the forementioned identity. Does anyone have an idea  how to get the second term?",,"['linear-algebra', 'tensors', 'fluid-dynamics']"
65,Can Hilbert spaces be defined over fields other than $\mathbb R$ and $\mathbb C$?,Can Hilbert spaces be defined over fields other than  and ?,\mathbb R \mathbb C,"Let $V$ be a vector space over a field $K$ . Suppose further that $K$ has the following structures: $K$ has a subfield $K_{\mathbb R}$ equipped with a field embedding $K_{\mathbb R}\hookrightarrow\mathbb R$ , so we can identity elements of $K_{\mathbb R}$ with elements of $\mathbb R$ . $K$ has an involution $*:K\to K,z\mapsto z^*$ , meaning $*$ is an automorphism and that $(z^*)^*=z$ for all $z\in K$ . Define a $(K,K_{\mathbb R},*,\hookrightarrow)$ -inner product space $\big(V,(K,K_{\mathbb R},*,\hookrightarrow),\langle\cdot,\cdot\rangle\big)$ as a $K$ -vector space (where $*,K_{\mathbb R},$ and $\hookrightarrow$ are the structures described above) together with a map $\langle\cdot,\cdot\rangle:V\times V\to K$ that satisfies the following properties: $\langle x,y\rangle=\langle y,x\rangle^*$ for all $x,y\in V$ . $\langle x,\cdot\rangle:V\to K$ is linear for all fixed $x\in V$ . $\forall x\in V\setminus\{0\}:\langle x,x\rangle>0$ . Condition (3) should be interpreted as saying $\langle x,x\rangle\in K_{\mathbb R}$ and the embedding (discussed above) identifies $\langle x,x\rangle$ with a positive real number. The idea of this definition is to allow for inner products to be defined in the most general setting possible and still agree with the usual definitions for $\mathbb R$ and $\mathbb C$ . Define a $(K,K_{\mathbb R},*,\hookrightarrow)$ -Hilbert space as a $(K,K_{\mathbb R},*,\hookrightarrow)$ -inner product space in which the induced norm $\lVert x\rVert =\sqrt{\langle x,x\rangle}$ makes $V$ into a complete metric space. Do there exist nontrivial $^\dagger$ $(K,K_{\mathbb R},*,\hookrightarrow)$ -Hilbert spaces for any fields $K\neq\mathbb R$ or $\mathbb C$ ? The "" $\neq$ "" should be read as ""not isomorphic to."" If yes, that would suggest interesting possible extensions of the usual definition of Hilbert space; if no, that would provide a justification for only ever defining or considering Hilbert spaces over $\mathbb R$ and $\mathbb C$ . $^\dagger$ By non-trivial, I mean $V\neq\{0\}$ , the single-element vector space.","Let be a vector space over a field . Suppose further that has the following structures: has a subfield equipped with a field embedding , so we can identity elements of with elements of . has an involution , meaning is an automorphism and that for all . Define a -inner product space as a -vector space (where and are the structures described above) together with a map that satisfies the following properties: for all . is linear for all fixed . . Condition (3) should be interpreted as saying and the embedding (discussed above) identifies with a positive real number. The idea of this definition is to allow for inner products to be defined in the most general setting possible and still agree with the usual definitions for and . Define a -Hilbert space as a -inner product space in which the induced norm makes into a complete metric space. Do there exist nontrivial -Hilbert spaces for any fields or ? The "" "" should be read as ""not isomorphic to."" If yes, that would suggest interesting possible extensions of the usual definition of Hilbert space; if no, that would provide a justification for only ever defining or considering Hilbert spaces over and . By non-trivial, I mean , the single-element vector space.","V K K K K_{\mathbb R} K_{\mathbb R}\hookrightarrow\mathbb R K_{\mathbb R} \mathbb R K *:K\to K,z\mapsto z^* * (z^*)^*=z z\in K (K,K_{\mathbb R},*,\hookrightarrow) \big(V,(K,K_{\mathbb R},*,\hookrightarrow),\langle\cdot,\cdot\rangle\big) K *,K_{\mathbb R}, \hookrightarrow \langle\cdot,\cdot\rangle:V\times V\to K \langle x,y\rangle=\langle y,x\rangle^* x,y\in V \langle x,\cdot\rangle:V\to K x\in V \forall x\in V\setminus\{0\}:\langle x,x\rangle>0 \langle x,x\rangle\in K_{\mathbb R} \langle x,x\rangle \mathbb R \mathbb C (K,K_{\mathbb R},*,\hookrightarrow) (K,K_{\mathbb R},*,\hookrightarrow) \lVert x\rVert =\sqrt{\langle x,x\rangle} V ^\dagger (K,K_{\mathbb R},*,\hookrightarrow) K\neq\mathbb R \mathbb C \neq \mathbb R \mathbb C ^\dagger V\neq\{0\}","['linear-algebra', 'field-theory', 'hilbert-spaces', 'inner-products', 'complete-spaces']"
66,Which matrices $A\in\text{Mat}_{n\times n}(\mathbb{K})$ are orthogonally diagonalizable over $\mathbb{K}$?,Which matrices  are orthogonally diagonalizable over ?,A\in\text{Mat}_{n\times n}(\mathbb{K}) \mathbb{K},"Update 1. I still need help with Question 1, Question 2' (as well as the bonus question under Question 2'), and Question 3'. Update 2. I believe that all questions have been answered if $\mathbb{K}$ is of characteristic not equal to $2$ .  The only thing remains to deal with is what happens when $\text{char}(\mathbb{K})=2$ . Let $\mathbb{K}$ be a field and $n$ a positive integer.  The notation $\text{Mat}_{n\times n}(\mathbb{K})$ represents the set of all $n$ -by- $n$ matrices with entries in $\mathbb{K}$ .  The subset $\text{GL}_n(\mathbb{K})$ of $\text{Mat}_{n\times n}(\mathbb{K})$ is composed by the invertible matrices.   Here, $(\_)^\top$ is the usual transpose operator.  Also, $\langle\_,\_\rangle$ is the standard nondegenerate bilinear form on $\mathbb{K}^n$ . Definition 1. A matrix $A\in\text{Mat}_{n\times n}(\mathbb{K})$ is said to be orthogonally diagonalizable over $\mathbb{K}$ if there exist matrices $D\in\text{Mat}_{n\times n}(\mathbb{K})$ and $Q\in\text{GL}_{n}(\mathbb{K})$ where $D$ is diagonal and $Q$ is orthogonal (i.e., $Q^\top=Q^{-1}$ ) such that $$A=QDQ^{\top}\,.$$ Definition 2. A matrix $A\in\text{Mat}_{n\times n}(\mathbb{K})$ is said to be seminormal if $$AA^\top=A^\top A\,.$$ For clarification, when $\mathbb{K}$ is $\mathbb{R}$ , seminormal matrices are the same as normal matrices.  However, when $\mathbb{K}$ is $\mathbb{C}$ , the terms seminormal and normal are different.  We have an obvious proposition. Proposition. Let $A\in\text{Mat}_{n\times n}(\mathbb{K})$ . (a) If $A$ is orthogonally diagonalizable over $\mathbb{K}$ , then $A$ is  symmetric. (b) If $A$ is symmetric, then $A$ is seminormal. The converse of (a) does not hold (but it does if $\mathbb{K}$ is $\mathbb{R}$ ).  For example, when $\mathbb{K}$ is the field $\mathbb{C}$ or any field with $\sqrt{-1}$ , we can take $$A:=\begin{bmatrix}1&\sqrt{-1}\\\sqrt{-1}&-1\end{bmatrix}\,.$$ Then, $A$ is symmetric, but being nilpotent, it is not diagonalizable.  The converse of (b) does not hold trivially (nonzero antisymmetric matrices are seminormal, but not symmetric). Here are my questions.  Crossed-out questions already have answers. Question 1. Is there a way to characterize all orthogonally diagonalizable matrices over an arbitrary field $\mathbb{K}$ ? As in Proposition (a), these matrices must be symmetric, but the counterexample above shows that this is not a sufficient condition.  Due to the answer by user277182 , I believe that this is a correct statement. Theorem. Suppose that $\text{char}(\mathbb{K})\neq 2$ .  A matrix $A\in\text{Mat}_{n\times n}(\mathbb{K})$ is orthogonally diagonalizable over $\mathbb{K}$ if and only if (a) $A$ is symmetric and diagonalizable over $\mathbb{K}$ , and (b) there exists a basis $\{v_1,v_2,\ldots,v_n\}$ of $\mathbb{K}^n$ consisting of eigenvectors of $A$ such that $\langle v_i,v_i\rangle$ is a nonzero perfect square element of $\mathbb{K}$ for each $i=1,2,\ldots,n$ . In the case where $\mathbb{K}$ contains all of its square roots (or when $\mathbb{K}$ is algebraically closed), the condition (b) in the theorem above is redundant.  This theorem also answers Question 2' below (in the case $\text{char}(\mathbb{K})\neq 2$ ). Question 2. If a symmetric matrix $A\in\text{Mat}_{n\times n}(\mathbb{K})$ is already known to be diagonalizable over $\mathbb{K}$ , is it also orthogonally diagonalizable over $\mathbb{K}$ ? The answer of Question 2 turns out to be no (see a counterexample in my answer below).  In light of this discovery, I propose a modified version of Question 2. Question 2'. Let $\mathbb{K}$ be an algebraically closed field.  If a symmetric matrix $A\in\text{Mat}_{n\times n}(\mathbb{K})$ is diagonalizable over $\mathbb{K}$ , is it also orthogonally diagonalizable over $\mathbb{K}$ ? Bonus. If $\mathbb{K}$ is not an algebraically closed field, what is a minimal requirement of $\mathbb{K}$ such that, if a symmetric matrix $A\in\text{Mat}_{n\times n}(\mathbb{K})$ is diagonalizable over $\mathbb{K}$ , it is always also orthogonally diagonalizable over $\mathbb{K}$ ?  This requirement may depend on $n$ . My guess for the bonus question is that, for every $x_1,x_2,\ldots,x_n\in\mathbb{K}$ , $x_1^2+x_2^2+\ldots+x_n^2$ has a square root in $\mathbb{K}$ .  For example, a minimal subfield of $\mathbb{R}$ with this property is the field of constructible real numbers .  Any field of characteristic $2$ automatically satisfies this condition. Edit. According to this paper and that paper , when $\mathbb{K}=\mathbb{C}$ , a symmetric matrix $A$ with an isotropic eigenvector $v$ (that is, $v^\top\,v=0$ ) is nonsemisimple (i.e., it is not diagonalizable).  Therefore, at least, when $\mathbb{K}$ is a subfield of $\mathbb{C}$ such that, for every $x_1,x_2,\ldots,x_n\in\mathbb{K}$ , $x_1^2+x_2^2+\ldots+x_n^2$ has a square root in $\mathbb{K}$ , then a symmetric matrix $A\in\text{Mat}_{n\times n}(\mathbb{K})$ is orthogonally diagonalizable over $\mathbb{K}$ if and only if it is diagonalizable over $\mathbb{K}$ .  The result for other fields is currently unknown (to me). Question 3. As a generalization of this question , suppose that $A\in\text{Mat}_{n\times n}(\mathbb{K})$ is diagonalizable over $\mathbb{K}$ . Does it hold that $A$ and $A^\top$ have the same set of eigenspaces if and only if $A$ is seminormal? Only the forward direction ( $\Rightarrow$ ) of this biconditional statement is known to be true.  It is clear, however, that when $A$ is orthogonally diagonalizable over $\mathbb{K}$ , then $A$ is symmetric, whence $A$ and $A^\top$ have the same eigenspaces.  As a result, the converse is true at least when $\mathbb{K}$ is a subfield of $\mathbb{R}$ because the seminormal (whence normal) matrices which is diagonalizable over $\mathbb{R}$ are the symmetric matrices. The answer to Question 3 is yes .  I forgot that diagonalizable matrices commute if and only if they can be simultaneously diagonalized.  See my answer in the other thread for a more detailed proof.   Therefore, I proposed a more generalized version of Question 3. Question 3'. Let $A\in\text{Mat}_{n\times n}(\mathbb{K})$ be such that all roots of the characteristic polynomial of $A$ lie in $\mathbb{K}$ .  What is a necessary and sufficient condition for $A$ and $A^\top$ to have the same set of generalized eigenspaces? Clearly, seminormality is not one such conditions.  Over any field $\mathbb{K}$ , the matrix $A:=\begin{bmatrix}0&1\\0&0\end{bmatrix}$ has the same set of generalized eigenspaces as does $A^\top$ .  (The only eigenvalue of $A$ is $0$ , and the generalized eigenspace associated to this eigenvalue is the whole $\mathbb{K}^2$ . The same goes with $A^\top$ .)  However, $$AA^\top=\begin{bmatrix}1&0\\0&0\end{bmatrix}\neq \begin{bmatrix}0&0\\0&1\end{bmatrix}=A^\top A\,.$$ In fact, any matrix $A\in\text{Mat}_{2\times 2}(\mathbb{K})$ which has an eigenvalue in $\mathbb{K}$ with multiplicity $2$ has $\mathbb{K}^2$ as its unique generalized eigenspace, and it follows immediately that $A$ and $A^\top$ have the same generalized eigenspace.","Update 1. I still need help with Question 1, Question 2' (as well as the bonus question under Question 2'), and Question 3'. Update 2. I believe that all questions have been answered if is of characteristic not equal to .  The only thing remains to deal with is what happens when . Let be a field and a positive integer.  The notation represents the set of all -by- matrices with entries in .  The subset of is composed by the invertible matrices.   Here, is the usual transpose operator.  Also, is the standard nondegenerate bilinear form on . Definition 1. A matrix is said to be orthogonally diagonalizable over if there exist matrices and where is diagonal and is orthogonal (i.e., ) such that Definition 2. A matrix is said to be seminormal if For clarification, when is , seminormal matrices are the same as normal matrices.  However, when is , the terms seminormal and normal are different.  We have an obvious proposition. Proposition. Let . (a) If is orthogonally diagonalizable over , then is  symmetric. (b) If is symmetric, then is seminormal. The converse of (a) does not hold (but it does if is ).  For example, when is the field or any field with , we can take Then, is symmetric, but being nilpotent, it is not diagonalizable.  The converse of (b) does not hold trivially (nonzero antisymmetric matrices are seminormal, but not symmetric). Here are my questions.  Crossed-out questions already have answers. Question 1. Is there a way to characterize all orthogonally diagonalizable matrices over an arbitrary field ? As in Proposition (a), these matrices must be symmetric, but the counterexample above shows that this is not a sufficient condition.  Due to the answer by user277182 , I believe that this is a correct statement. Theorem. Suppose that .  A matrix is orthogonally diagonalizable over if and only if (a) is symmetric and diagonalizable over , and (b) there exists a basis of consisting of eigenvectors of such that is a nonzero perfect square element of for each . In the case where contains all of its square roots (or when is algebraically closed), the condition (b) in the theorem above is redundant.  This theorem also answers Question 2' below (in the case ). Question 2. If a symmetric matrix is already known to be diagonalizable over , is it also orthogonally diagonalizable over ? The answer of Question 2 turns out to be no (see a counterexample in my answer below).  In light of this discovery, I propose a modified version of Question 2. Question 2'. Let be an algebraically closed field.  If a symmetric matrix is diagonalizable over , is it also orthogonally diagonalizable over ? Bonus. If is not an algebraically closed field, what is a minimal requirement of such that, if a symmetric matrix is diagonalizable over , it is always also orthogonally diagonalizable over ?  This requirement may depend on . My guess for the bonus question is that, for every , has a square root in .  For example, a minimal subfield of with this property is the field of constructible real numbers .  Any field of characteristic automatically satisfies this condition. Edit. According to this paper and that paper , when , a symmetric matrix with an isotropic eigenvector (that is, ) is nonsemisimple (i.e., it is not diagonalizable).  Therefore, at least, when is a subfield of such that, for every , has a square root in , then a symmetric matrix is orthogonally diagonalizable over if and only if it is diagonalizable over .  The result for other fields is currently unknown (to me). Question 3. As a generalization of this question , suppose that is diagonalizable over . Does it hold that and have the same set of eigenspaces if and only if is seminormal? Only the forward direction ( ) of this biconditional statement is known to be true.  It is clear, however, that when is orthogonally diagonalizable over , then is symmetric, whence and have the same eigenspaces.  As a result, the converse is true at least when is a subfield of because the seminormal (whence normal) matrices which is diagonalizable over are the symmetric matrices. The answer to Question 3 is yes .  I forgot that diagonalizable matrices commute if and only if they can be simultaneously diagonalized.  See my answer in the other thread for a more detailed proof.   Therefore, I proposed a more generalized version of Question 3. Question 3'. Let be such that all roots of the characteristic polynomial of lie in .  What is a necessary and sufficient condition for and to have the same set of generalized eigenspaces? Clearly, seminormality is not one such conditions.  Over any field , the matrix has the same set of generalized eigenspaces as does .  (The only eigenvalue of is , and the generalized eigenspace associated to this eigenvalue is the whole . The same goes with .)  However, In fact, any matrix which has an eigenvalue in with multiplicity has as its unique generalized eigenspace, and it follows immediately that and have the same generalized eigenspace.","\mathbb{K} 2 \text{char}(\mathbb{K})=2 \mathbb{K} n \text{Mat}_{n\times n}(\mathbb{K}) n n \mathbb{K} \text{GL}_n(\mathbb{K}) \text{Mat}_{n\times n}(\mathbb{K}) (\_)^\top \langle\_,\_\rangle \mathbb{K}^n A\in\text{Mat}_{n\times n}(\mathbb{K}) \mathbb{K} D\in\text{Mat}_{n\times n}(\mathbb{K}) Q\in\text{GL}_{n}(\mathbb{K}) D Q Q^\top=Q^{-1} A=QDQ^{\top}\,. A\in\text{Mat}_{n\times n}(\mathbb{K}) AA^\top=A^\top A\,. \mathbb{K} \mathbb{R} \mathbb{K} \mathbb{C} A\in\text{Mat}_{n\times n}(\mathbb{K}) A \mathbb{K} A A A \mathbb{K} \mathbb{R} \mathbb{K} \mathbb{C} \sqrt{-1} A:=\begin{bmatrix}1&\sqrt{-1}\\\sqrt{-1}&-1\end{bmatrix}\,. A \mathbb{K} \text{char}(\mathbb{K})\neq 2 A\in\text{Mat}_{n\times n}(\mathbb{K}) \mathbb{K} A \mathbb{K} \{v_1,v_2,\ldots,v_n\} \mathbb{K}^n A \langle v_i,v_i\rangle \mathbb{K} i=1,2,\ldots,n \mathbb{K} \mathbb{K} \text{char}(\mathbb{K})\neq 2 A\in\text{Mat}_{n\times n}(\mathbb{K}) \mathbb{K} \mathbb{K} \mathbb{K} A\in\text{Mat}_{n\times n}(\mathbb{K}) \mathbb{K} \mathbb{K} \mathbb{K} \mathbb{K} A\in\text{Mat}_{n\times n}(\mathbb{K}) \mathbb{K} \mathbb{K} n x_1,x_2,\ldots,x_n\in\mathbb{K} x_1^2+x_2^2+\ldots+x_n^2 \mathbb{K} \mathbb{R} 2 \mathbb{K}=\mathbb{C} A v v^\top\,v=0 \mathbb{K} \mathbb{C} x_1,x_2,\ldots,x_n\in\mathbb{K} x_1^2+x_2^2+\ldots+x_n^2 \mathbb{K} A\in\text{Mat}_{n\times n}(\mathbb{K}) \mathbb{K} \mathbb{K} A\in\text{Mat}_{n\times n}(\mathbb{K}) \mathbb{K} A A^\top A \Rightarrow A \mathbb{K} A A A^\top \mathbb{K} \mathbb{R} \mathbb{R} A\in\text{Mat}_{n\times n}(\mathbb{K}) A \mathbb{K} A A^\top \mathbb{K} A:=\begin{bmatrix}0&1\\0&0\end{bmatrix} A^\top A 0 \mathbb{K}^2 A^\top AA^\top=\begin{bmatrix}1&0\\0&0\end{bmatrix}\neq \begin{bmatrix}0&0\\0&1\end{bmatrix}=A^\top A\,. A\in\text{Mat}_{2\times 2}(\mathbb{K}) \mathbb{K} 2 \mathbb{K}^2 A A^\top","['linear-algebra', 'matrices', 'field-theory', 'diagonalization', 'orthogonal-matrices']"
67,Sum of singular values of a matrix,Sum of singular values of a matrix,,"Is there a ""trick"" to calculate the sum of singular values of a matrix $A$, without actually finding them? For example, the sum of the squared singular values is $\operatorname{trace}(A^TA)$.","Is there a ""trick"" to calculate the sum of singular values of a matrix $A$, without actually finding them? For example, the sum of the squared singular values is $\operatorname{trace}(A^TA)$.",,"['linear-algebra', 'matrices', 'svd', 'singular-values', 'nuclear-norm']"
68,Proof of a theorem on simultaneous diagonalization from Hoffman and Kunze.,Proof of a theorem on simultaneous diagonalization from Hoffman and Kunze.,,Now I am reading Linear Algebra from the book of Hoffman and Kunze second edition. I am trying to understand theorem $8$ on pg number $207$ which is based on Simultaneous diagonalization. I have seen plenty of proofs on this simultaneous diagonalization. But I couldn't understand what they had mentioned in the $1$st paragraph of the proof to adapt a previous lemma (lemma before theorem 7) for this theorem. I understand the proof they provided. But I am curious to know the modified lemma for the  diagonalizable case. If someone can help me I will be very happy. Many thanks.For your reference I add a photo.,Now I am reading Linear Algebra from the book of Hoffman and Kunze second edition. I am trying to understand theorem $8$ on pg number $207$ which is based on Simultaneous diagonalization. I have seen plenty of proofs on this simultaneous diagonalization. But I couldn't understand what they had mentioned in the $1$st paragraph of the proof to adapt a previous lemma (lemma before theorem 7) for this theorem. I understand the proof they provided. But I am curious to know the modified lemma for the  diagonalizable case. If someone can help me I will be very happy. Many thanks.For your reference I add a photo.,,"['linear-algebra', 'matrices', 'diagonalization', 'triangularization']"
69,On the sum of all elements of inverted correlation matrix,On the sum of all elements of inverted correlation matrix,,"Assume I have a correlation matrix,$A$  \begin{equation} A_{i,j} =   \begin{cases} 1,& \text{if}\ i=j\\ \rho_{i,j},& \text{otherwise}  \\ \end{cases} \end{equation}  Where $ 0\leq \rho_{i,j} \leq 1 $  and $A$ is positive definite. Now define a correlation matrix $\bar{A}$ as follows: \begin{equation} \overline{A} =   \begin{cases} 1,& \text{if}\ i=j\\ \overline{\rho},& \text{otherwise} \\ \end{cases} \end{equation} Where $\overline{\rho}=\frac{\sum_{i \neq j} \rho_{i,j}}{n^{2}-n}$ I want to prove that the sum of elements in the inverse matrix $\overline{A}^{-1}$ is less than the the sum of elements in the inverse matrix ${A}^{-1}$. Any ideas? I wrote a program that makes random matrices of this sort and it was true for all matrices in many different dimensions so it can't be just by chance. I just don't know how to prove it. Thanks","Assume I have a correlation matrix,$A$  \begin{equation} A_{i,j} =   \begin{cases} 1,& \text{if}\ i=j\\ \rho_{i,j},& \text{otherwise}  \\ \end{cases} \end{equation}  Where $ 0\leq \rho_{i,j} \leq 1 $  and $A$ is positive definite. Now define a correlation matrix $\bar{A}$ as follows: \begin{equation} \overline{A} =   \begin{cases} 1,& \text{if}\ i=j\\ \overline{\rho},& \text{otherwise} \\ \end{cases} \end{equation} Where $\overline{\rho}=\frac{\sum_{i \neq j} \rho_{i,j}}{n^{2}-n}$ I want to prove that the sum of elements in the inverse matrix $\overline{A}^{-1}$ is less than the the sum of elements in the inverse matrix ${A}^{-1}$. Any ideas? I wrote a program that makes random matrices of this sort and it was true for all matrices in many different dimensions so it can't be just by chance. I just don't know how to prove it. Thanks",,"['linear-algebra', 'probability']"
70,Is a symmetric positive definite matrix always diagonally dominant?,Is a symmetric positive definite matrix always diagonally dominant?,,"A Hermitian diagonally dominant matrix $A$ with real non-negative diagonal entries is positive semidefinite. Is it possible to have a Hermitian matrix be positive semidefinite/definite and not be diagonally dominant? In other words, if I know that a matrix $M$ is symmetric positive definite then can I ensure  $M - dI$, for a real number $d$, is positive definite only by ensuring $M - dI$ is diagonally dominant with non-negative diagonal entries? I am aware that $d \le \lambda_{min}$, $\lambda$ being eigenvalue, for the matrix to remain semidefinite, but I need to avoid eigenvalue computation. Thanks!","A Hermitian diagonally dominant matrix $A$ with real non-negative diagonal entries is positive semidefinite. Is it possible to have a Hermitian matrix be positive semidefinite/definite and not be diagonally dominant? In other words, if I know that a matrix $M$ is symmetric positive definite then can I ensure  $M - dI$, for a real number $d$, is positive definite only by ensuring $M - dI$ is diagonally dominant with non-negative diagonal entries? I am aware that $d \le \lambda_{min}$, $\lambda$ being eigenvalue, for the matrix to remain semidefinite, but I need to avoid eigenvalue computation. Thanks!",,"['linear-algebra', 'matrices', 'conic-sections', 'covariance']"
71,Is $(V_1\otimes\cdots\otimes V_k)^\ast \simeq V_1^\ast\otimes \cdots \otimes V_k^\ast$ true for infinite dimensional spaces?,Is  true for infinite dimensional spaces?,(V_1\otimes\cdots\otimes V_k)^\ast \simeq V_1^\ast\otimes \cdots \otimes V_k^\ast,"Suppose $V_1,\dots,V_k$ are vector spaces of finite dimension. Then I could prove easily that $(V_1\otimes\cdots\otimes V_k)^\ast\simeq V_1^\ast\otimes\cdots\otimes V_k^\ast$. My proof was like that: first of all, I've shown that if for each $i$ we have $W_i$ another vector space such that $V_i\simeq W_i$ then $V_1\otimes\cdots\otimes V_k \simeq W_1\otimes\cdots\otimes W_k$. Then, since I'm supposing each $V_i$ finite dimensional, each $V_i\simeq V_i^\ast$ and also, we have $V_1\otimes\cdots\otimes V_k$ also finite dimensional, so that $$(V_1\otimes\cdots\otimes V_k)^\ast \simeq V_1\otimes\cdots\otimes V_k\simeq V_1^\ast\otimes \cdots \otimes V_k^\ast$$ and so it is proved. Now, if the spaces are not finite dimensional this proof cannot be used. In that case, the property still holds? Is it possible to prove for infinite dimensional spaces? I've tried to prove it directly, constructing an isomorphism. I've picked first the mapping $\psi : V_1^\ast\times\cdots\times V_k^\ast \to \mathcal{L}(V_1,\dots,V_k;\mathbb{K})$ given by $$\psi(f_1,\dots,f_k)(v_1,\dots,v_k) = f_1(v_1)\cdots f_k(v_k)$$ this map is multilinear and hence by the universal property there corresponds a unique linear mapping $\phi : V_1^\ast\otimes \cdots \otimes V_k^\ast \to \mathcal{L}(V_1,\dots,V_k;\mathbb{K})$ such that: $$\phi(f_1\otimes\cdots\otimes f_k)(v_1,\dots,v_k) = f_1(v_1)\cdots f_k(v_k)$$ To show that this $\phi$ is isomorphis I would need to find an inverse, but I didn't have any idea. Is it possible to complete this proof? Thanks very much in advance.","Suppose $V_1,\dots,V_k$ are vector spaces of finite dimension. Then I could prove easily that $(V_1\otimes\cdots\otimes V_k)^\ast\simeq V_1^\ast\otimes\cdots\otimes V_k^\ast$. My proof was like that: first of all, I've shown that if for each $i$ we have $W_i$ another vector space such that $V_i\simeq W_i$ then $V_1\otimes\cdots\otimes V_k \simeq W_1\otimes\cdots\otimes W_k$. Then, since I'm supposing each $V_i$ finite dimensional, each $V_i\simeq V_i^\ast$ and also, we have $V_1\otimes\cdots\otimes V_k$ also finite dimensional, so that $$(V_1\otimes\cdots\otimes V_k)^\ast \simeq V_1\otimes\cdots\otimes V_k\simeq V_1^\ast\otimes \cdots \otimes V_k^\ast$$ and so it is proved. Now, if the spaces are not finite dimensional this proof cannot be used. In that case, the property still holds? Is it possible to prove for infinite dimensional spaces? I've tried to prove it directly, constructing an isomorphism. I've picked first the mapping $\psi : V_1^\ast\times\cdots\times V_k^\ast \to \mathcal{L}(V_1,\dots,V_k;\mathbb{K})$ given by $$\psi(f_1,\dots,f_k)(v_1,\dots,v_k) = f_1(v_1)\cdots f_k(v_k)$$ this map is multilinear and hence by the universal property there corresponds a unique linear mapping $\phi : V_1^\ast\otimes \cdots \otimes V_k^\ast \to \mathcal{L}(V_1,\dots,V_k;\mathbb{K})$ such that: $$\phi(f_1\otimes\cdots\otimes f_k)(v_1,\dots,v_k) = f_1(v_1)\cdots f_k(v_k)$$ To show that this $\phi$ is isomorphis I would need to find an inverse, but I didn't have any idea. Is it possible to complete this proof? Thanks very much in advance.",,"['linear-algebra', 'multilinear-algebra']"
72,Who first explicitly wrote the determinant identity $\det(1+AB) = \det(1+BA)$?,Who first explicitly wrote the determinant identity ?,\det(1+AB) = \det(1+BA),"Though this identity can be easily proved, I am wondering who first explicitly write it in such a simple and elegant form? I check several textbooks on linear algebra but find no evidence (see below the list of books I have checked). The entry on wikipedia calls it the Weinstein–Aronszajn identity now but previously attributed it to J. J. Sylvester . In the blog of Terence Tao , he calls it the Weinstein–Aronszajn identity with a link to the Wikipedia page, but tags and comments for that article imply that he used to call it the 'Sylvester determinant identity'. The wiki page does not give direct references to the explicit form. The book referred to in the proof calls this identity 'Sylvester's determinant theorem'. The old version of the wiki page refers to an 1851 paper by Sylvester . I have checked it and did not see the explicit form. In conclusion, no direct reference is given for the explicit form. So I am very confused. For the identity in the title, who should be attributed to? In case that asking about attribution may offend some mathematicians, I would like to focus on who first use the finite/infinite version of this identity to solve problems, especially in the explicit form. A direct reference would be a good answer. By 'direct reference' I mean a paper — a paper which (1) contains the explicit form of the identity, (2) applies it to solve problems and (3) is authored by whoever presumably be attributed to. Textbooks I have checked: C.D. Meyer, Matrix Analysis and Applied Linear Algebra (2000) R.A. Horn & C.R. Johnson, Matrix Analysis, 2nd ed. (2012)","Though this identity can be easily proved, I am wondering who first explicitly write it in such a simple and elegant form? I check several textbooks on linear algebra but find no evidence (see below the list of books I have checked). The entry on wikipedia calls it the Weinstein–Aronszajn identity now but previously attributed it to J. J. Sylvester . In the blog of Terence Tao , he calls it the Weinstein–Aronszajn identity with a link to the Wikipedia page, but tags and comments for that article imply that he used to call it the 'Sylvester determinant identity'. The wiki page does not give direct references to the explicit form. The book referred to in the proof calls this identity 'Sylvester's determinant theorem'. The old version of the wiki page refers to an 1851 paper by Sylvester . I have checked it and did not see the explicit form. In conclusion, no direct reference is given for the explicit form. So I am very confused. For the identity in the title, who should be attributed to? In case that asking about attribution may offend some mathematicians, I would like to focus on who first use the finite/infinite version of this identity to solve problems, especially in the explicit form. A direct reference would be a good answer. By 'direct reference' I mean a paper — a paper which (1) contains the explicit form of the identity, (2) applies it to solve problems and (3) is authored by whoever presumably be attributed to. Textbooks I have checked: C.D. Meyer, Matrix Analysis and Applied Linear Algebra (2000) R.A. Horn & C.R. Johnson, Matrix Analysis, 2nd ed. (2012)",,"['linear-algebra', 'math-history', 'random-matrices']"
73,Diagonalizable random matrix,Diagonalizable random matrix,,"Let $p_n$ the probability that a random matrix $M\in\mathcal{M}_n(\mathbb{R})$ such that its entries $(m_{i,j})_{1\leqslant i,j\leqslant n}$ are independant and following an uniform distribution over $[-1,1]$ , is diagonalizable. I was wondering  how to calculate $p_n$ and maybe how to find its limit or an equivalent. Diagonalization in $\mathbb{C}$ : I proved that $p_n=1$ for all $n\in\mathbb{N}$ if we talk about diagonalization in $\mathbb{C}$ : Let $$ \Phi_n : \left|\begin{aligned} &\ \ \ \ \ \ \ _ \ \ \ \ \mathbb{R}_{=n}[X] &\longrightarrow &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbb{C} \\ &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ P &\longmapsto &\prod_{1\leqslant i<j\leqslant n}\left(\lambda_i(P)-\lambda_j(P)\right)  \end{aligned}\right. $$ where $(\lambda_i(P))_{1\leqslant i\leqslant n}$ are the roots of $P$ ordered in the lexicographical order. For any $P\in\mathbb{R}_{=n}[X]$ , $\Phi_n(P)$ is, by a factor, the discriminant of $P$ and thus is a polynomial function of the coefficients of $P$ . Moreover, $\Phi_n(P)=0$ if and only if $P$ has a multiple root so that $$ p_n \geqslant \mathbb{P}(\Phi_n(\chi_M)\neq 0)=1-\mathbb{P}(\Phi_n(\chi_M)=0) $$ because $M$ is diagonalizable in $\mathbb{C}$ if $\chi_M$ has no multiple root. Moreover, if we denote $\lambda_n$ the Lebesgue measure on $\mathbb{R}^n$ , one can show that for any non-constant $P\in\mathbb{R}[X_1,\ldots,X_n]$ , if $$ \zeta(P) := \{ x\in\mathbb{R}^n\ |\ P(x)=0 \} $$ then $\lambda_n(\zeta(P))=0$ . We show it by induction on $n$ : if $n=1$ then $\zeta(P)$ is finite so that $\lambda_1(\zeta(P))=0$ . If $n\geqslant 2$ , we write $$ \zeta(P)=\bigcup_{t\in\mathbb{R}}\zeta(P(\cdot,t)) $$ where $P(\cdot,t):(x_1,\ldots,x_{n-1})\mapsto P(x_1,\ldots,x_{n-1},t)$ . By hypothesis $\lambda_{n-1}(\zeta(P(\cdot,t)))=0$ for all $t\in\mathbb{R}$ , thus using Fubini's theorem we have $$ \lambda_n(\zeta(P))=\int_{-\infty}^{+\infty}\lambda_{n-1}(\zeta(P(\cdot,t)))dt=0 $$ Finally, since $M\mapsto\Phi_n(\chi_M)$ is a polynomial function of the coefficients of $M$ (because $\Phi_n$ and $M\mapsto\chi_M$ are), the measure of the set $$ \{M\in\mathcal{M}_n(\mathbb{R})\ |\ \Phi_n(\chi_M)=0\} $$ is $0$ and $\mathbb{P}(\Phi_n(\chi_M)=0)=0$ and thus $p_n=1$ . Diagonalization in $\mathbb{R}$ : Because of what said above, for any $M\in\mathcal{M}_n(\mathbb{R})$ , $\chi_M$ has no multiple root almost surely so that $$ p_n=\mathbb{P}(\text{Sp}(M)\subset\mathbb{R}) $$ I believe that $\lim\limits_{n\rightarrow +\infty}p_n=0$ but I don't know how to prove it, and even less how to find an equivalent of $p_n$ . EDIT : While searching for papers I found out about the circular law. Notice that the entries of $M$ has zero mean and, since $A$ is diagonalizable if and only if $\lambda A$ is diagonalizable for all $\lambda\in\mathbb{R}$ , we can study the special case where the entries have a variance of $1$ (in our case we would study $\sqrt{\frac{3}{2}}A$ ). Let $\mu_n$ be the measure $$ \mu_n=\frac{1}{n}\sum_{k=1}^n \delta_{n^{-1/2}\lambda_k(M_n)} $$ with $M_n\in\mathcal{M}_n(\mathbb{R})$ a random matrix, $\lambda_k(M_n)$ its random eigenvalues and $\delta$ the Dirac measure. What is interesting is that $n\mu_n(\mathbb{R})$ is the number of real eigenvalues (counted with multiplicity) of $M_n$ so that $$ p_n=\mathbb{P}(\text{Sp}(M_n)\subset\mathbb{R})=\mathbb{P}(\mu_n(\mathbb{R})=1) $$ Since $\mu_n(\mathbb{R})\leqslant 1$ almost surely, we have using Markov's inequality $$ p_n=\mathbb{P}(\mu_n(\mathbb{R})\geqslant 1)\leqslant\mathbb{E}(\mu_n(\mathbb{R})) $$ If we prove that $\lim\limits_{n\rightarrow +\infty}\mu_n(\mathbb{R})=0$ almost surely, we can use the dominated convergence theorem with the domination $\mu_n(\mathbb{R})\leqslant 1$ almost surely to prove that $\lim\limits_{n\rightarrow +\infty}\mathbb{E}(\mu_n(\mathbb{R}))=0$ and this would finally show that $\lim\limits_{n\rightarrow +\infty}p_n=0$ . The circular law states that the sequence of measures $(\mu_n)_{n\in\mathbb{N}^*}$ converges in distribution to the uniform measure on the unit disk almost surely. This means that for all smooth function $f:\mathbb{C}\longrightarrow\mathbb{R}$ that has a compact support, we have $$ \lim\limits_{n\rightarrow +\infty}\int_{\mathbb{C}}f(z)d\mu_n(z)=\frac{1}{\pi}\int_{x^2+y^2\leqslant 1}f(x+iy)dxdy $$ Let $\varepsilon>0$ , $\beta>0$ and $f:\mathbb{C}\longrightarrow\mathbb{R}^+$ a smooth function such that $f(z)=1$ for all $z\in[-\beta,\beta]$ and $$\frac{1}{\pi}\int_{x^2+y^2\leqslant 1}f(x+iy)dxdy<\frac{\varepsilon}{2}$$ (such a function exists, $\varphi(x)=\mathbf{1}_{\{|x|\leqslant 1\}}+\left(1-e^{-\frac{x^2}{x^2-1}}\right)\mathbf{1}_{\{|x|>1\}}$ is a smooth function such that $\varphi(x)=1$ for all $x\in[-1,1]$ , we can use $f(x+iy)=\varphi(x/\beta)\varphi(y/\eta)$ with $\eta>0$ small enough). Thus $$ \limsup\limits_{n\rightarrow +\infty}\mu_n([-\beta,\beta])\leqslant\lim\limits_{n\rightarrow +\infty}\int_{\mathbb{C}}f(z)d\mu_n(z)=\frac{1}{\pi}\int_{x^2+y^2\leqslant 1}f(x+iy)dxdy<\frac{\varepsilon}{2} $$ Furthermore $$ \begin{aligned} \limsup\limits_{n\rightarrow +\infty}\mu_n(]-\infty,-\beta[\cup]\beta,+\infty[)&\leqslant\int_{-\infty}^{-\beta}\frac{t^2}{\beta^2}d\mu_n(t)+\int_{\beta}^{+\infty}\frac{t^2}{\beta^2}d\mu_n(t) \\ &\leqslant\frac{1}{\beta^2}\int_{\mathbb{C}}|z|^2 d\mu_n(z) \end{aligned}$$ However $$\sum_{k=1}^n{\lambda_k(M_n)^2}=\text{tr}({}^t M_n M_n)=\sum_{1\leqslant i,j\leqslant n}m_{i,j}^2$$ so that $$ \limsup\limits_{n\rightarrow+\infty}\int_{\mathbb{C}}|z|^2 d\mu_n(z)=\limsup\limits_{n\rightarrow +\infty}\frac{1}{n^2}\sum_{1\leqslant i,j\leqslant n}m_{i,j}^2\leqslant\mathbb{E}(m_{1,1}^2)=1 $$ almsot surely according to the law of large numbers. Thus there exists $C>0$ such that $$ \forall n\in\mathbb{N}^*,\int_{\mathbb{C}}|z|^2d\mu_n(z)\leqslant C $$ Finally $$ \limsup\limits_{n\rightarrow +\infty}\mu_n(]-\infty,-\beta[\cup]\beta,+\infty[)\leqslant\frac{C}{\beta^2} $$ and if we set $\beta=\sqrt{\frac{2C}{\varepsilon}}$ , we have $$ \limsup\limits_{n\rightarrow +\infty}\mu_n(\mathbb{R})=\limsup\limits_{n\rightarrow+\infty}\mu_n([-\beta,\beta])+\limsup\limits_{n\rightarrow +\infty}\mu_n(]-\infty,-\beta[\cup]\beta,+\infty[)<\varepsilon $$ Letting $\varepsilon\rightarrow 0$ gives $\limsup\limits_{n\rightarrow+\infty}\mu_n(\mathbb{R})=0$ almost surely and thus $\lim\limits_{n\rightarrow+\infty}p_n=0$ .","Let the probability that a random matrix such that its entries are independant and following an uniform distribution over , is diagonalizable. I was wondering  how to calculate and maybe how to find its limit or an equivalent. Diagonalization in : I proved that for all if we talk about diagonalization in : Let where are the roots of ordered in the lexicographical order. For any , is, by a factor, the discriminant of and thus is a polynomial function of the coefficients of . Moreover, if and only if has a multiple root so that because is diagonalizable in if has no multiple root. Moreover, if we denote the Lebesgue measure on , one can show that for any non-constant , if then . We show it by induction on : if then is finite so that . If , we write where . By hypothesis for all , thus using Fubini's theorem we have Finally, since is a polynomial function of the coefficients of (because and are), the measure of the set is and and thus . Diagonalization in : Because of what said above, for any , has no multiple root almost surely so that I believe that but I don't know how to prove it, and even less how to find an equivalent of . EDIT : While searching for papers I found out about the circular law. Notice that the entries of has zero mean and, since is diagonalizable if and only if is diagonalizable for all , we can study the special case where the entries have a variance of (in our case we would study ). Let be the measure with a random matrix, its random eigenvalues and the Dirac measure. What is interesting is that is the number of real eigenvalues (counted with multiplicity) of so that Since almost surely, we have using Markov's inequality If we prove that almost surely, we can use the dominated convergence theorem with the domination almost surely to prove that and this would finally show that . The circular law states that the sequence of measures converges in distribution to the uniform measure on the unit disk almost surely. This means that for all smooth function that has a compact support, we have Let , and a smooth function such that for all and (such a function exists, is a smooth function such that for all , we can use with small enough). Thus Furthermore However so that almsot surely according to the law of large numbers. Thus there exists such that Finally and if we set , we have Letting gives almost surely and thus .","p_n M\in\mathcal{M}_n(\mathbb{R}) (m_{i,j})_{1\leqslant i,j\leqslant n} [-1,1] p_n \mathbb{C} p_n=1 n\in\mathbb{N} \mathbb{C}  \Phi_n : \left|\begin{aligned} &\ \ \ \ \ \ \ _ \ \ \ \ \mathbb{R}_{=n}[X] &\longrightarrow &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbb{C} \\ &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ P &\longmapsto &\prod_{1\leqslant i<j\leqslant n}\left(\lambda_i(P)-\lambda_j(P)\right)  \end{aligned}\right.  (\lambda_i(P))_{1\leqslant i\leqslant n} P P\in\mathbb{R}_{=n}[X] \Phi_n(P) P P \Phi_n(P)=0 P  p_n \geqslant \mathbb{P}(\Phi_n(\chi_M)\neq 0)=1-\mathbb{P}(\Phi_n(\chi_M)=0)  M \mathbb{C} \chi_M \lambda_n \mathbb{R}^n P\in\mathbb{R}[X_1,\ldots,X_n]  \zeta(P) := \{ x\in\mathbb{R}^n\ |\ P(x)=0 \}  \lambda_n(\zeta(P))=0 n n=1 \zeta(P) \lambda_1(\zeta(P))=0 n\geqslant 2  \zeta(P)=\bigcup_{t\in\mathbb{R}}\zeta(P(\cdot,t))  P(\cdot,t):(x_1,\ldots,x_{n-1})\mapsto P(x_1,\ldots,x_{n-1},t) \lambda_{n-1}(\zeta(P(\cdot,t)))=0 t\in\mathbb{R}  \lambda_n(\zeta(P))=\int_{-\infty}^{+\infty}\lambda_{n-1}(\zeta(P(\cdot,t)))dt=0  M\mapsto\Phi_n(\chi_M) M \Phi_n M\mapsto\chi_M  \{M\in\mathcal{M}_n(\mathbb{R})\ |\ \Phi_n(\chi_M)=0\}  0 \mathbb{P}(\Phi_n(\chi_M)=0)=0 p_n=1 \mathbb{R} M\in\mathcal{M}_n(\mathbb{R}) \chi_M  p_n=\mathbb{P}(\text{Sp}(M)\subset\mathbb{R})  \lim\limits_{n\rightarrow +\infty}p_n=0 p_n M A \lambda A \lambda\in\mathbb{R} 1 \sqrt{\frac{3}{2}}A \mu_n  \mu_n=\frac{1}{n}\sum_{k=1}^n \delta_{n^{-1/2}\lambda_k(M_n)}  M_n\in\mathcal{M}_n(\mathbb{R}) \lambda_k(M_n) \delta n\mu_n(\mathbb{R}) M_n  p_n=\mathbb{P}(\text{Sp}(M_n)\subset\mathbb{R})=\mathbb{P}(\mu_n(\mathbb{R})=1)  \mu_n(\mathbb{R})\leqslant 1  p_n=\mathbb{P}(\mu_n(\mathbb{R})\geqslant 1)\leqslant\mathbb{E}(\mu_n(\mathbb{R}))  \lim\limits_{n\rightarrow +\infty}\mu_n(\mathbb{R})=0 \mu_n(\mathbb{R})\leqslant 1 \lim\limits_{n\rightarrow +\infty}\mathbb{E}(\mu_n(\mathbb{R}))=0 \lim\limits_{n\rightarrow +\infty}p_n=0 (\mu_n)_{n\in\mathbb{N}^*} f:\mathbb{C}\longrightarrow\mathbb{R}  \lim\limits_{n\rightarrow +\infty}\int_{\mathbb{C}}f(z)d\mu_n(z)=\frac{1}{\pi}\int_{x^2+y^2\leqslant 1}f(x+iy)dxdy  \varepsilon>0 \beta>0 f:\mathbb{C}\longrightarrow\mathbb{R}^+ f(z)=1 z\in[-\beta,\beta] \frac{1}{\pi}\int_{x^2+y^2\leqslant 1}f(x+iy)dxdy<\frac{\varepsilon}{2} \varphi(x)=\mathbf{1}_{\{|x|\leqslant 1\}}+\left(1-e^{-\frac{x^2}{x^2-1}}\right)\mathbf{1}_{\{|x|>1\}} \varphi(x)=1 x\in[-1,1] f(x+iy)=\varphi(x/\beta)\varphi(y/\eta) \eta>0  \limsup\limits_{n\rightarrow +\infty}\mu_n([-\beta,\beta])\leqslant\lim\limits_{n\rightarrow +\infty}\int_{\mathbb{C}}f(z)d\mu_n(z)=\frac{1}{\pi}\int_{x^2+y^2\leqslant 1}f(x+iy)dxdy<\frac{\varepsilon}{2}   \begin{aligned} \limsup\limits_{n\rightarrow +\infty}\mu_n(]-\infty,-\beta[\cup]\beta,+\infty[)&\leqslant\int_{-\infty}^{-\beta}\frac{t^2}{\beta^2}d\mu_n(t)+\int_{\beta}^{+\infty}\frac{t^2}{\beta^2}d\mu_n(t) \\
&\leqslant\frac{1}{\beta^2}\int_{\mathbb{C}}|z|^2 d\mu_n(z) \end{aligned} \sum_{k=1}^n{\lambda_k(M_n)^2}=\text{tr}({}^t M_n M_n)=\sum_{1\leqslant i,j\leqslant n}m_{i,j}^2  \limsup\limits_{n\rightarrow+\infty}\int_{\mathbb{C}}|z|^2 d\mu_n(z)=\limsup\limits_{n\rightarrow +\infty}\frac{1}{n^2}\sum_{1\leqslant i,j\leqslant n}m_{i,j}^2\leqslant\mathbb{E}(m_{1,1}^2)=1  C>0  \forall n\in\mathbb{N}^*,\int_{\mathbb{C}}|z|^2d\mu_n(z)\leqslant C   \limsup\limits_{n\rightarrow +\infty}\mu_n(]-\infty,-\beta[\cup]\beta,+\infty[)\leqslant\frac{C}{\beta^2}  \beta=\sqrt{\frac{2C}{\varepsilon}}  \limsup\limits_{n\rightarrow +\infty}\mu_n(\mathbb{R})=\limsup\limits_{n\rightarrow+\infty}\mu_n([-\beta,\beta])+\limsup\limits_{n\rightarrow +\infty}\mu_n(]-\infty,-\beta[\cup]\beta,+\infty[)<\varepsilon  \varepsilon\rightarrow 0 \limsup\limits_{n\rightarrow+\infty}\mu_n(\mathbb{R})=0 \lim\limits_{n\rightarrow+\infty}p_n=0","['linear-algebra', 'probability-theory', 'diagonalization']"
74,Linear algebra over a non-free f.g. module,Linear algebra over a non-free f.g. module,,"Suppose $M$ is a finitely generated module over a commutative ring $R$. I was thinking about the relation between $\operatorname{End}_R M$ and $M_n(R)$. Suppose I fix a generating set $m_1, \dotsc, m_n \in M$. Not all matrices $(a_{ij}) \in M_n(R)$ represent $R$ linear self-maps of $M$ via  $$m_i \mapsto \sum_j a_{ij}m_j,$$ because some of them don't respect the relations that may hold between the $m_i$. If a relation holds on $m_i$, the same had better hold for the columns of $a_{ij}$. For example, in $\mathbb{Z}_2 \oplus \mathbb{Z}_3$, with the usual two generators, the matrix  $$\begin{pmatrix} 0 & 1 \\ 1 & 0\end{pmatrix}$$ does not represent a map of abelian groups. So we need to look at some subalgebra $S <M_n(R)$ which does represent linear self-maps of $M$. Further, the fact that there are relations between the generators means that in general a lot of matrices correspond to the zero transformation. So it seems we should think of $\operatorname{End}_R M$ as a quotient of $S$, by the ideal of matrices that determine the zero transformation. I see no reason why this would be a two-sided ideal in $M_n(R)$, but it will be in $S$. So it seems that when we try to do matrix computations when working with f.g. modules, like when we prove Cayley-Hamilton and so forth, we are looking at $$M_n(R) \hookleftarrow S \twoheadrightarrow S/I \cong \operatorname{End}_R(M)$$ and we work so that we can do some matrix computations inside $S$ that work after we mod out by $I$. (I guess in proving Cayley Hamilton, like here , we're taking the ring to be $R[x]$.) Can anyone offer any more clarifying perspective? I've never seen sources that discuss it like this, so any references are welcome.","Suppose $M$ is a finitely generated module over a commutative ring $R$. I was thinking about the relation between $\operatorname{End}_R M$ and $M_n(R)$. Suppose I fix a generating set $m_1, \dotsc, m_n \in M$. Not all matrices $(a_{ij}) \in M_n(R)$ represent $R$ linear self-maps of $M$ via  $$m_i \mapsto \sum_j a_{ij}m_j,$$ because some of them don't respect the relations that may hold between the $m_i$. If a relation holds on $m_i$, the same had better hold for the columns of $a_{ij}$. For example, in $\mathbb{Z}_2 \oplus \mathbb{Z}_3$, with the usual two generators, the matrix  $$\begin{pmatrix} 0 & 1 \\ 1 & 0\end{pmatrix}$$ does not represent a map of abelian groups. So we need to look at some subalgebra $S <M_n(R)$ which does represent linear self-maps of $M$. Further, the fact that there are relations between the generators means that in general a lot of matrices correspond to the zero transformation. So it seems we should think of $\operatorname{End}_R M$ as a quotient of $S$, by the ideal of matrices that determine the zero transformation. I see no reason why this would be a two-sided ideal in $M_n(R)$, but it will be in $S$. So it seems that when we try to do matrix computations when working with f.g. modules, like when we prove Cayley-Hamilton and so forth, we are looking at $$M_n(R) \hookleftarrow S \twoheadrightarrow S/I \cong \operatorname{End}_R(M)$$ and we work so that we can do some matrix computations inside $S$ that work after we mod out by $I$. (I guess in proving Cayley Hamilton, like here , we're taking the ring to be $R[x]$.) Can anyone offer any more clarifying perspective? I've never seen sources that discuss it like this, so any references are welcome.",,"['linear-algebra', 'reference-request', 'commutative-algebra', 'soft-question', 'modules']"
75,"Vector Space Structures over ($\mathbb{R}$,+)","Vector Space Structures over (,+)",\mathbb{R},"Consider the abelian group ($\mathbb{R}$,+) of real numbers with the usual addition. Is there a scalar multiplication \begin{equation} \cdot : \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}, \end{equation} other than the usual multiplication, which makes ($\mathbb{R}$,+,$ \cdot $) a real vector space?","Consider the abelian group ($\mathbb{R}$,+) of real numbers with the usual addition. Is there a scalar multiplication \begin{equation} \cdot : \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}, \end{equation} other than the usual multiplication, which makes ($\mathbb{R}$,+,$ \cdot $) a real vector space?",,"['linear-algebra', 'abstract-algebra', 'modules', 'functional-equations']"
76,Confusion concerning Lemma 1.12 in Wiles's proof of Fermat's Last Theorem,Confusion concerning Lemma 1.12 in Wiles's proof of Fermat's Last Theorem,,"Let $k$ be a finite field of characteristic $p\neq 2$ (in fact, one only needs to consider the case $p\in\{3,5\}$ ), let $\Sigma$ be a finite set of primes containing $\infty$ and $p$ , and $$\rho_{0}:{\rm Gal}(\mathbb{Q}_{\Sigma}/\mathbb{Q})\rightarrow {\rm GL}_{2}(k)$$ an absolutely irreducible representation, meaning $\rho_{0}\otimes\overline{k}$ cannot be written as the direct sum of two one-dimensional subrepresentations, where $\mathbb{Q}_{\Sigma}$ is the largest Galois extension of $\mathbb{Q}$ unramified outside the primes in $\Sigma$ . Assume that if $\tau\in\operatorname{Gal}(\mathbb{Q}_{\Sigma}/\mathbb{Q})$ is complex conjugation, then $\det(\rho_{0}(\tau))=-1$ . Then $\rho$ induces a projective representation $$\tilde{\rho}_{0}:\operatorname{Gal}(\mathbb{Q}_{\Sigma}/\mathbb{Q})\longrightarrow\operatorname{PGL}_{2}(k).$$ Assume this projective representation has dihedral image, meaning $$\operatorname{image}(\tilde{\rho}_{0})\cong\left<s,r\mid s^{2}=r^{m}=(sr)^{2}=1\right>$$ for some $m\in\mathbb{N}$ , and assume further that $\rho_{0}|_{\mathbb{Q}(\sqrt{(-1)^{\frac{p-1}{2}}p})}$ is absolutely irreducible. The action of $\operatorname{Gal}(\mathbb{Q}_{\Sigma}/\mathbb{Q})$ on $k^{2}$ induces an action on $V_{\lambda}=\operatorname{Hom}(k^{2},k^{2})$ , namely by conjugation. (What that $\lambda$ stands for is of no significance here.) Since $p\neq 2$ , one has a direct sum of $k[\operatorname{Gal}(\mathbb{Q}_{\Sigma}/\mathbb{Q})]$ -modules $$V_{\lambda}=W_{\lambda}\oplus k\text{,}$$ where $W_{\lambda}$ denotes the space of $\operatorname{trace}$ - $0$ matrices and $k$ is the space of scalar multiplications. Let $K_{1}$ be the splitting field of $\rho_{0}$ (i.e. $\operatorname{Gal}(\mathbb{Q}_{\Sigma}/K_{1})=\operatorname{ker}(\rho_{0})$ ), and $$G:=\operatorname{Gal}(K_{1}/\mathbb{Q})=\operatorname{Gal}(\mathbb{Q}_{\Sigma}/\mathbb{Q})/\operatorname{ker}(\rho_{0})=\operatorname{image}(\rho_{0}).$$ Since $\overline{\rho}_{0}$ has dihedral image, $\rho_{0}\otimes\overline{k}=\operatorname{Ind}_{H}^{G}(\chi)$ for some character $\chi$ . Question : Wiles now makes the following claims. (1.) Under the above conditions, $W_{\lambda}\otimes\overline{k}=\delta\otimes\operatorname{Ind}_{H}^{G}(\chi/\chi')$ where $\chi'$ is the quadratic twist of $\chi$ by any element of $G\setminus H$ and $\delta$ is the quadratic character $G\longrightarrow G/H$ (what does that even mean - $W_{\lambda}\otimes\overline{k}$ decomposes as a quadratic character + something else?) (2.) since $M(\zeta_{p^{n}})$ is Abelian over $\mathbb{Q}$ , where $\mathbb{Q}\subseteq M\subseteq K_{1}$ such that $G/H=\operatorname{Gal}(M/\mathbb{Q})$ , one always finds for any $n\in\mathbb{N}$ an $x\in\operatorname{Gal}(\overline{\mathbb{Q}}/\mathbb{Q})$ which fixes $\mathbb{Q}(\zeta_{p^{n}})$ and $\tilde{\rho}_{0}(x)\neq 1$ as long as $m\neq 2$ (i.e. $\operatorname{image}(\rho_{0})\neq\mathbb{Z}/2\times\mathbb{Z}/2$ ). Why is that the case? Maybe I'm not seeing the wood for all the trees here, but who knows. EDIT : Also, should it not be $\operatorname{Gal}(\mathbb{Q}_{\Sigma}/\mathbb{Q})$ instead of $\operatorname{Gal}(\overline{\mathbb{Q}}/\mathbb{Q})$ ?","Let be a finite field of characteristic (in fact, one only needs to consider the case ), let be a finite set of primes containing and , and an absolutely irreducible representation, meaning cannot be written as the direct sum of two one-dimensional subrepresentations, where is the largest Galois extension of unramified outside the primes in . Assume that if is complex conjugation, then . Then induces a projective representation Assume this projective representation has dihedral image, meaning for some , and assume further that is absolutely irreducible. The action of on induces an action on , namely by conjugation. (What that stands for is of no significance here.) Since , one has a direct sum of -modules where denotes the space of - matrices and is the space of scalar multiplications. Let be the splitting field of (i.e. ), and Since has dihedral image, for some character . Question : Wiles now makes the following claims. (1.) Under the above conditions, where is the quadratic twist of by any element of and is the quadratic character (what does that even mean - decomposes as a quadratic character + something else?) (2.) since is Abelian over , where such that , one always finds for any an which fixes and as long as (i.e. ). Why is that the case? Maybe I'm not seeing the wood for all the trees here, but who knows. EDIT : Also, should it not be instead of ?","k p\neq 2 p\in\{3,5\} \Sigma \infty p \rho_{0}:{\rm Gal}(\mathbb{Q}_{\Sigma}/\mathbb{Q})\rightarrow {\rm GL}_{2}(k) \rho_{0}\otimes\overline{k} \mathbb{Q}_{\Sigma} \mathbb{Q} \Sigma \tau\in\operatorname{Gal}(\mathbb{Q}_{\Sigma}/\mathbb{Q}) \det(\rho_{0}(\tau))=-1 \rho \tilde{\rho}_{0}:\operatorname{Gal}(\mathbb{Q}_{\Sigma}/\mathbb{Q})\longrightarrow\operatorname{PGL}_{2}(k). \operatorname{image}(\tilde{\rho}_{0})\cong\left<s,r\mid s^{2}=r^{m}=(sr)^{2}=1\right> m\in\mathbb{N} \rho_{0}|_{\mathbb{Q}(\sqrt{(-1)^{\frac{p-1}{2}}p})} \operatorname{Gal}(\mathbb{Q}_{\Sigma}/\mathbb{Q}) k^{2} V_{\lambda}=\operatorname{Hom}(k^{2},k^{2}) \lambda p\neq 2 k[\operatorname{Gal}(\mathbb{Q}_{\Sigma}/\mathbb{Q})] V_{\lambda}=W_{\lambda}\oplus k\text{,} W_{\lambda} \operatorname{trace} 0 k K_{1} \rho_{0} \operatorname{Gal}(\mathbb{Q}_{\Sigma}/K_{1})=\operatorname{ker}(\rho_{0}) G:=\operatorname{Gal}(K_{1}/\mathbb{Q})=\operatorname{Gal}(\mathbb{Q}_{\Sigma}/\mathbb{Q})/\operatorname{ker}(\rho_{0})=\operatorname{image}(\rho_{0}). \overline{\rho}_{0} \rho_{0}\otimes\overline{k}=\operatorname{Ind}_{H}^{G}(\chi) \chi W_{\lambda}\otimes\overline{k}=\delta\otimes\operatorname{Ind}_{H}^{G}(\chi/\chi') \chi' \chi G\setminus H \delta G\longrightarrow G/H W_{\lambda}\otimes\overline{k} M(\zeta_{p^{n}}) \mathbb{Q} \mathbb{Q}\subseteq M\subseteq K_{1} G/H=\operatorname{Gal}(M/\mathbb{Q}) n\in\mathbb{N} x\in\operatorname{Gal}(\overline{\mathbb{Q}}/\mathbb{Q}) \mathbb{Q}(\zeta_{p^{n}}) \tilde{\rho}_{0}(x)\neq 1 m\neq 2 \operatorname{image}(\rho_{0})\neq\mathbb{Z}/2\times\mathbb{Z}/2 \operatorname{Gal}(\mathbb{Q}_{\Sigma}/\mathbb{Q}) \operatorname{Gal}(\overline{\mathbb{Q}}/\mathbb{Q})","['linear-algebra', 'proof-explanation', 'representation-theory', 'galois-theory', 'galois-representations']"
77,When are $GL_n$ and $GL_m$ equivalent... in characteristic 2?,When are  and  equivalent... in characteristic 2?,GL_n GL_m,"For fields $K$ and $L$ , I am interested in proving that "" $GL_n(K)$ and $GL_m(L)$ are isomorphic (as groups) if and only if $m=n$ and $K\simeq L$ "". I don't know how generally this is true, but: assume $K=L$ . In that case, if $char(K) \neq 2$ then there is a usual proof that $n=m$ by looking at the group of involutions of each, that yields $2^m = 2^n$ and this the result. if $char(K) = 2$ , I do not find any proof of this fact. is there a stronger result without supposing $K = L$ ? Can we at least conclude that $char(K) = char(L)$ ?","For fields and , I am interested in proving that "" and are isomorphic (as groups) if and only if and "". I don't know how generally this is true, but: assume . In that case, if then there is a usual proof that by looking at the group of involutions of each, that yields and this the result. if , I do not find any proof of this fact. is there a stronger result without supposing ? Can we at least conclude that ?",K L GL_n(K) GL_m(L) m=n K\simeq L K=L char(K) \neq 2 n=m 2^m = 2^n char(K) = 2 K = L char(K) = char(L),"['linear-algebra', 'group-theory', 'general-linear-group']"
78,"given the inverse of a matrix, is there an efficient way to find the determinant?","given the inverse of a matrix, is there an efficient way to find the determinant?",,"Suppose one has the inverse $A^{-1}$ of an $N\times N$ non-singular matrix $A$ .   Is there an ''efficient'' way to obtain $\det{A}$ ? With ''efficient'' I mean anything that has a better scaling than the standard $\mathcal{O}(N^3)$ .  Naively, one can argue that $\det{A}$ is already implicitly incorporated in the inverse via the adjugate $A^{-1}=\displaystyle\frac{1}{\det{A}}\operatorname{adj}A$ , so all the work has ""already been done"", and one only needs an efficient trick to distill the determinant. E.g.: for $N=2$ , we have $A=\left(\begin{array}{cc} a & b \\ c & d\end{array}\right)$ and $A^{-1}=\displaystyle\frac{1}{\det{A}}\left(\begin{array}{cc} d & -b \\ -c & a\end{array}\right)$ .  The determinant can efficiently be obtained from $\displaystyle\frac{A_{11}}{[A^{-1}]_{22}}=\det{A}$ in $\mathcal{O}(1)$ .","Suppose one has the inverse of an non-singular matrix .   Is there an ''efficient'' way to obtain ? With ''efficient'' I mean anything that has a better scaling than the standard .  Naively, one can argue that is already implicitly incorporated in the inverse via the adjugate , so all the work has ""already been done"", and one only needs an efficient trick to distill the determinant. E.g.: for , we have and .  The determinant can efficiently be obtained from in .",A^{-1} N\times N A \det{A} \mathcal{O}(N^3) \det{A} A^{-1}=\displaystyle\frac{1}{\det{A}}\operatorname{adj}A N=2 A=\left(\begin{array}{cc} a & b \\ c & d\end{array}\right) A^{-1}=\displaystyle\frac{1}{\det{A}}\left(\begin{array}{cc} d & -b \\ -c & a\end{array}\right) \displaystyle\frac{A_{11}}{[A^{-1}]_{22}}=\det{A} \mathcal{O}(1),"['linear-algebra', 'determinant', 'inverse']"
79,Limit of a sequence of determinants.,Limit of a sequence of determinants.,,"Let $\beta>0$ be given.  For each $n\geq 2$, let $\Delta_n=\det M_n$ denote the determinant of the following matrix: \begin{align}   M_n = \begin{pmatrix}     2+\epsilon^2 & -1 & 0 & 0 & 0 & -1 \\     -1 & 2+\epsilon^2 & -1 & 0 & 0 & \ddots \\     0 & -1 & 2+\epsilon^2 & -1 & 0 & \ddots \\     0 & 0 & -1 & 2+\epsilon^2 & -1 & \ddots \\     0 & 0 & 0 & -1 & 2+\epsilon^2 & \ddots \\     -1 & \ddots & \ddots & \ddots & \ddots & \ddots \\   \end{pmatrix}, \qquad   \epsilon = \frac{\beta}{n} \end{align} How can one evaluate the limit \begin{align}   \Delta = \lim_{n\to\infty}\Delta_n ? \end{align} This problem arose in the context of evaluating a certain path integral in a physics problem.  I know how to determine the eigenvalues of each $M_n$, but computing the determinant by taking the product of eigenvalues leads to a product I can't evaluate.  I also tried deriving a recursion relation for the $\Delta_n$ and showing that in the limit $n\to\infty$, the recursion relation can be regarded as a differential equation whose solution subject to certain initial data determines $\Delta$, but that failed as well. Any insights would be appreciated. Addendum. When I compute the eigenvalues $\lambda_k$ of each $M_n$, I obtain \begin{align}   \lambda_k = 4\sin^2\left(\frac{\pi k}{n}\right)+\left(\frac{\beta}{n}\right)^2, \qquad k=0,1,\dots, n-1 \end{align} from which it follows that \begin{align}   \Delta_n = \prod_{k=0}^{n-1}\left[4\sin^2\left(\frac{\pi k}{n}\right)+\left(\frac{\beta}{n}\right)^2\right] \end{align} which I don't have the foggiest idea of how to evaluate.  I do, however, have a conjecture for the answer which comes from the fact that this limit of determinants came from a path integral which can be computed in other ways.  My conjecture (which I have checked numerically using mathematica to some extent) is \begin{align}   \Delta = 4\sinh^2\frac{\beta}{2} \end{align}","Let $\beta>0$ be given.  For each $n\geq 2$, let $\Delta_n=\det M_n$ denote the determinant of the following matrix: \begin{align}   M_n = \begin{pmatrix}     2+\epsilon^2 & -1 & 0 & 0 & 0 & -1 \\     -1 & 2+\epsilon^2 & -1 & 0 & 0 & \ddots \\     0 & -1 & 2+\epsilon^2 & -1 & 0 & \ddots \\     0 & 0 & -1 & 2+\epsilon^2 & -1 & \ddots \\     0 & 0 & 0 & -1 & 2+\epsilon^2 & \ddots \\     -1 & \ddots & \ddots & \ddots & \ddots & \ddots \\   \end{pmatrix}, \qquad   \epsilon = \frac{\beta}{n} \end{align} How can one evaluate the limit \begin{align}   \Delta = \lim_{n\to\infty}\Delta_n ? \end{align} This problem arose in the context of evaluating a certain path integral in a physics problem.  I know how to determine the eigenvalues of each $M_n$, but computing the determinant by taking the product of eigenvalues leads to a product I can't evaluate.  I also tried deriving a recursion relation for the $\Delta_n$ and showing that in the limit $n\to\infty$, the recursion relation can be regarded as a differential equation whose solution subject to certain initial data determines $\Delta$, but that failed as well. Any insights would be appreciated. Addendum. When I compute the eigenvalues $\lambda_k$ of each $M_n$, I obtain \begin{align}   \lambda_k = 4\sin^2\left(\frac{\pi k}{n}\right)+\left(\frac{\beta}{n}\right)^2, \qquad k=0,1,\dots, n-1 \end{align} from which it follows that \begin{align}   \Delta_n = \prod_{k=0}^{n-1}\left[4\sin^2\left(\frac{\pi k}{n}\right)+\left(\frac{\beta}{n}\right)^2\right] \end{align} which I don't have the foggiest idea of how to evaluate.  I do, however, have a conjecture for the answer which comes from the fact that this limit of determinants came from a path integral which can be computed in other ways.  My conjecture (which I have checked numerically using mathematica to some extent) is \begin{align}   \Delta = 4\sinh^2\frac{\beta}{2} \end{align}",,"['linear-algebra', 'limits', 'determinant']"
80,"Given a matrix group, how to determine whether its connected , dense etc.","Given a matrix group, how to determine whether its connected , dense etc.",,"While going through Matrix groups , the problem I am facing is that how to determine different topological aspects of them, to be specific, Compactness is not a big problem due to Heine-Borel theorem , but dealing with other properties such as Connectedness and Density is not as easy. Firstly, I tried to think geometrically (like $SO(2) \cong S^1$ , $SU(2) \cong S^3$ , here $S^n$ denotes the group of the n-sphere in ${\Bbb R}^{n+1}$ , so $S^1$ denotes the circle group etc. ). But I am not able to do it for all the groups there on the table. Next, for connectedness, I tried to do it with by considering continuous functions from that group to $\{1,-1\}$ , but still am facing difficulty. And for density in particular, I have no clue. Please help me by providing some strategies on how to learn and solve problems on Connectedness and Density of Matrix groups.","While going through Matrix groups , the problem I am facing is that how to determine different topological aspects of them, to be specific, Compactness is not a big problem due to Heine-Borel theorem , but dealing with other properties such as Connectedness and Density is not as easy. Firstly, I tried to think geometrically (like $SO(2) \cong S^1$ , $SU(2) \cong S^3$ , here $S^n$ denotes the group of the n-sphere in ${\Bbb R}^{n+1}$ , so $S^1$ denotes the circle group etc. ). But I am not able to do it for all the groups there on the table. Next, for connectedness, I tried to do it with by considering continuous functions from that group to $\{1,-1\}$ , but still am facing difficulty. And for density in particular, I have no clue. Please help me by providing some strategies on how to learn and solve problems on Connectedness and Density of Matrix groups.",,"['linear-algebra', 'abstract-algebra']"
81,When do $n$ ants in cyclic pursuit with constant velocity converge?,When do  ants in cyclic pursuit with constant velocity converge?,n,"I'm reading a paper ( Ants, Crickets and Frogs in Cyclic Pursuit ) and trying to understand one of the simpler results. The following is a paraphrasing of the parts I'm using, but check the paper if more context is needed: Consider $n$ moving ants $\mathrm{x}_1(t)$ through $\mathrm{x}_n(t)$ in a vector space $\mathcal{L}$ . Each ant $\mathrm{x}_i$ moves at all times in the direction of the ant $\mathrm{x}_{i-1}$ (and $\mathrm{x}_1 \to \mathrm{x}_n$ ) with constant velocity $v$ . For $i = 1,...n$ , define the initial conditions as $\mathrm{x}_i(0) = \xi_i$ and the following system of $n$ differential equations: $$\frac{d}{dt} \mathrm{x}_i(t) = v \ \frac{\mathrm{x}_{i-1}(t) - \mathrm{x}_i(t)}{||\mathrm{x}_{i-1}(t) - \mathrm{x}_i(t)||}$$ We can construct an equivalent model in barycentric coordinates by defining $$\mathrm{y}_i(t) = \mathrm{x}_{i-1}(t) - \mathrm{x}_i(t)$$ Then for $i = 1,...,n$ , we have $\mathrm{y}_i(0) = \xi_{i-1} - \xi_i$ and the following system of equations: $$\frac{d}{dt} \mathrm{y}_i(t) = v \left(\frac{\mathrm{y}_{i-1}(t)}{||\mathrm{y}_{i-1}(t)||} - \frac{\mathrm{y}_i(t)}{||\mathrm{y}_i(t)||} \right)$$ For this system, we can prove the following: $\textbf{Lemma 1}$ The difference vectors $\mathrm{y}_i(t)$ have the following properties: $\sum \mathrm{y}_i = 0$ $\frac{d}{dt} ||\mathrm{y}_i|| = (\cos \alpha_i - 1) v$ Where $\alpha_i$ is the angle between $\mathrm{y}_i$ and $\mathrm{y}_{i-1}$ . $\textbf{Proof:}$ The first part follows by definition and the second comes from projecting $\mathrm{y}_{i-1}$ onto $\mathrm{y}_i$ . Call $T$ the termination time where all ants collide and define $Y(t) = \sum_i ||\mathrm{y}_i(t)||$ . Then comes the part I'm most curious about. They claim If the speeds are constant and equal, Lemma $1$ part 2 implies that $T \le Y(0)/v$ . I think this works by summing both sides of the second equality to get $$\sum_i \frac{d}{dt}||\mathrm{y}_i|| = v \sum_i (\cos \alpha_i - 1) \le -v$$ which would prove the claim, but the inequality is where I get stuck. I'm thinking we can bound the sum somehow by using the fact that $\sum_i \alpha_i \ge 2\pi$ , but I'm not quite sure how to do this. Edit: It appears this claim is false as pointed out by @stewbasic. I'd still like to get answers for the following: A general bound for the termination time $T$ . Under what conditions do the ants collide simultaneously? For simultaneity, a sufficient condition is fine provided it covers the specific case of cube. For example, if a cycle of $n$ ants all start a constant distance $\mathrm{y}(0) = ||\mathrm{y}_i(0)||$ from their pursuer and prey and all start the same distance $d$ from their centroid, is this enough to ensure simultaneity of collision? If it simplifies things, the time bound need only apply to cases that satisfy the sufficient condition for simultaneity. Specific case : Consider $8$ ants on the corners of a unit cube with pursuit cycle given by $$(0,0,0) \to (1,0,0) \to (1,1,0) \to (1,1,1) \to (1,0,1) \to (0,0,1) \to (0,1,1) \to (0,1,0)$$ I'd ideally like to get the bound $T \le 2/v$ , but even $T \le 8/v$ would acceptable. The actual value is about $1.96$ , which I can get from simulation where their convergence path looks like this: Here's an animated version I created: Edit: As @VictorZurkowski points out, the proof of Theorem $1$ includes a (not very strict) bound on $T$ which reduces in the constant velocity case to $$T \le \frac{Y(0)}{v\left(1 - \cos\left(\frac{2\pi}{m}\right)\right)}$$ where $m$ is the number of survivors. For the cube case with unit speed, this becomes $$T \le \frac{Y(0)}{1\left(1 - \cos\left(\frac{2\pi}{8}\right)\right)} = \frac{8}{\left(1 - \cos\left(\frac{\pi}{4}\right)\right)} = 8\left(2 + \sqrt{2}\right) \approx 27.3137$$ Using symmetry, I think I could get this down to $2 \left(2+\sqrt{2}\right) \approx 6.82843$ . All of the $\alpha_i$ follow one of these two curves where the $y$ -axis is in degrees and the $x$ -axis is $t$ in percent of of $T$ (at 100% they converge). We can see that they converge to the planar case where $\alpha_i = \frac{\pi}{4} = 45^{\circ}$ I believe the planar case is worst case in terms of convergence time and this website has some nice bounds for regular polygons with unit area. If my calculations are correct, an octagon with unit length sides would have path length $2\sqrt{4+3\sqrt{2}} \approx 5.742$ . [To be revised and continued...]","I'm reading a paper ( Ants, Crickets and Frogs in Cyclic Pursuit ) and trying to understand one of the simpler results. The following is a paraphrasing of the parts I'm using, but check the paper if more context is needed: Consider moving ants through in a vector space . Each ant moves at all times in the direction of the ant (and ) with constant velocity . For , define the initial conditions as and the following system of differential equations: We can construct an equivalent model in barycentric coordinates by defining Then for , we have and the following system of equations: For this system, we can prove the following: The difference vectors have the following properties: Where is the angle between and . The first part follows by definition and the second comes from projecting onto . Call the termination time where all ants collide and define . Then comes the part I'm most curious about. They claim If the speeds are constant and equal, Lemma part 2 implies that . I think this works by summing both sides of the second equality to get which would prove the claim, but the inequality is where I get stuck. I'm thinking we can bound the sum somehow by using the fact that , but I'm not quite sure how to do this. Edit: It appears this claim is false as pointed out by @stewbasic. I'd still like to get answers for the following: A general bound for the termination time . Under what conditions do the ants collide simultaneously? For simultaneity, a sufficient condition is fine provided it covers the specific case of cube. For example, if a cycle of ants all start a constant distance from their pursuer and prey and all start the same distance from their centroid, is this enough to ensure simultaneity of collision? If it simplifies things, the time bound need only apply to cases that satisfy the sufficient condition for simultaneity. Specific case : Consider ants on the corners of a unit cube with pursuit cycle given by I'd ideally like to get the bound , but even would acceptable. The actual value is about , which I can get from simulation where their convergence path looks like this: Here's an animated version I created: Edit: As @VictorZurkowski points out, the proof of Theorem includes a (not very strict) bound on which reduces in the constant velocity case to where is the number of survivors. For the cube case with unit speed, this becomes Using symmetry, I think I could get this down to . All of the follow one of these two curves where the -axis is in degrees and the -axis is in percent of of (at 100% they converge). We can see that they converge to the planar case where I believe the planar case is worst case in terms of convergence time and this website has some nice bounds for regular polygons with unit area. If my calculations are correct, an octagon with unit length sides would have path length . [To be revised and continued...]","n \mathrm{x}_1(t) \mathrm{x}_n(t) \mathcal{L} \mathrm{x}_i \mathrm{x}_{i-1} \mathrm{x}_1 \to \mathrm{x}_n v i = 1,...n \mathrm{x}_i(0) = \xi_i n \frac{d}{dt} \mathrm{x}_i(t) = v \ \frac{\mathrm{x}_{i-1}(t) - \mathrm{x}_i(t)}{||\mathrm{x}_{i-1}(t) - \mathrm{x}_i(t)||} \mathrm{y}_i(t) = \mathrm{x}_{i-1}(t) - \mathrm{x}_i(t) i = 1,...,n \mathrm{y}_i(0) = \xi_{i-1} - \xi_i \frac{d}{dt} \mathrm{y}_i(t) = v \left(\frac{\mathrm{y}_{i-1}(t)}{||\mathrm{y}_{i-1}(t)||} - \frac{\mathrm{y}_i(t)}{||\mathrm{y}_i(t)||} \right) \textbf{Lemma 1} \mathrm{y}_i(t) \sum \mathrm{y}_i = 0 \frac{d}{dt} ||\mathrm{y}_i|| = (\cos \alpha_i - 1) v \alpha_i \mathrm{y}_i \mathrm{y}_{i-1} \textbf{Proof:} \mathrm{y}_{i-1} \mathrm{y}_i T Y(t) = \sum_i ||\mathrm{y}_i(t)|| 1 T \le Y(0)/v \sum_i \frac{d}{dt}||\mathrm{y}_i|| = v \sum_i (\cos \alpha_i - 1) \le -v \sum_i \alpha_i \ge 2\pi T n \mathrm{y}(0) = ||\mathrm{y}_i(0)|| d 8 (0,0,0) \to (1,0,0) \to (1,1,0) \to (1,1,1) \to (1,0,1) \to (0,0,1) \to (0,1,1) \to (0,1,0) T \le 2/v T \le 8/v 1.96 1 T T \le \frac{Y(0)}{v\left(1 - \cos\left(\frac{2\pi}{m}\right)\right)} m T \le \frac{Y(0)}{1\left(1 - \cos\left(\frac{2\pi}{8}\right)\right)} = \frac{8}{\left(1 - \cos\left(\frac{\pi}{4}\right)\right)} = 8\left(2 + \sqrt{2}\right) \approx 27.3137 2 \left(2+\sqrt{2}\right) \approx 6.82843 \alpha_i y x t T \alpha_i = \frac{\pi}{4} = 45^{\circ} 2\sqrt{4+3\sqrt{2}} \approx 5.742","['linear-algebra', 'ordinary-differential-equations', 'dynamical-systems', 'proof-explanation']"
82,"Algebra defined by $a^2=a,b^2=b,c^2=c,(a+b+c)^2=a+b+c$",Algebra defined by,"a^2=a,b^2=b,c^2=c,(a+b+c)^2=a+b+c","Let $\cal A$ be the (noncommutative) unitary $\mathbb Z$-algebra defined by three generators $a,b,c$ and four relations $a^2=a,b^2=b,c^2=c,(a+b+c)^2=a+b+c$. Is it true that $ab\neq 0$ in $A$ ? This question is natural in the context of an older question here on MSE. My thoughts : The following two relations follow easily from the axioms : $$ \begin{array}{lcl} cb &=& -(ab+ac+ba+bc+ca) \\ cab &=& ca+ab+2(ba+ac+bc)+aba+abc+aca+bac+bca \\ \end{array}\tag{1} $$ Denote by $W$ the set of words on $a,b,c$ (they are called monomials in the algebra $\cal A$). We order $W$ with the shortlex $a<b<c$ ordering (which we denote by $\prec$). The two relations above express $cb$ or $cab$ in terms of $\prec$-smaller monomials. Iterating those two relations and using induction on $\prec$ ,any monomial can be transformed in $\cal A$ into a term whose monomials do not contain any of  $aa,bb,cc,cb,cab$. Denote by $W'$ the set of all monomials satisfying this condition. We therefore have a surjection $s : {\cal A}' \to {\cal A}$ where ${\cal A}'=\oplus_{w\in W'} {\mathbb Z}w$. Conjecture 1. The mapping $s$ is bijective, in other words $W'$ is a $\mathbb Z$-basis for $\cal A$. Note that the action of $a$ or $b$ on $W'$ is trivial to describe : for any monomial $w\in W'$, if $w$ does not start with an $a$ then $aw$ stays in $W'$, and $aw=w$ otherwise. Similarly for $b$. The action of $c$ is more complicated. Using the two relations in (1)  and induction on $\prec$ again, we see that there is a unique $\mathbb Z$-linear map $C:{\cal A}' \to {\cal A}'$ such that $C(1)=c,C(a)=ca$ and $$ \left\lbrace\begin{array}{lcl} C(abw) &=& (Ca+ab+2(ba+aC+bC)+aba+abC+aCa+baC+bCa)w \ ( \ \text{if} \ bw\in W') \\ C(acw) &=& cacw \ ( \ \text{if} \ acw\in W') \\ C(bw) &=& -(ab+aC+ba+bC+Ca)w \ ( \ \text{if} \ bw\in W') \\ C(cw) &=& cw \ ( \ \text{if} \ cw\in W') \end{array}\right.\tag{2} $$ Indeed, any monomial distinct from $1$ or $a$ starts with exactly one of $ab$, $ac$, $b$ or $c$. It is not clear however (at least to me) how to show that Conjecture 1 (equivalent form). This $C$ satisfies $C^2=C$ and $(a+b+C)^2=a+b+C$.","Let $\cal A$ be the (noncommutative) unitary $\mathbb Z$-algebra defined by three generators $a,b,c$ and four relations $a^2=a,b^2=b,c^2=c,(a+b+c)^2=a+b+c$. Is it true that $ab\neq 0$ in $A$ ? This question is natural in the context of an older question here on MSE. My thoughts : The following two relations follow easily from the axioms : $$ \begin{array}{lcl} cb &=& -(ab+ac+ba+bc+ca) \\ cab &=& ca+ab+2(ba+ac+bc)+aba+abc+aca+bac+bca \\ \end{array}\tag{1} $$ Denote by $W$ the set of words on $a,b,c$ (they are called monomials in the algebra $\cal A$). We order $W$ with the shortlex $a<b<c$ ordering (which we denote by $\prec$). The two relations above express $cb$ or $cab$ in terms of $\prec$-smaller monomials. Iterating those two relations and using induction on $\prec$ ,any monomial can be transformed in $\cal A$ into a term whose monomials do not contain any of  $aa,bb,cc,cb,cab$. Denote by $W'$ the set of all monomials satisfying this condition. We therefore have a surjection $s : {\cal A}' \to {\cal A}$ where ${\cal A}'=\oplus_{w\in W'} {\mathbb Z}w$. Conjecture 1. The mapping $s$ is bijective, in other words $W'$ is a $\mathbb Z$-basis for $\cal A$. Note that the action of $a$ or $b$ on $W'$ is trivial to describe : for any monomial $w\in W'$, if $w$ does not start with an $a$ then $aw$ stays in $W'$, and $aw=w$ otherwise. Similarly for $b$. The action of $c$ is more complicated. Using the two relations in (1)  and induction on $\prec$ again, we see that there is a unique $\mathbb Z$-linear map $C:{\cal A}' \to {\cal A}'$ such that $C(1)=c,C(a)=ca$ and $$ \left\lbrace\begin{array}{lcl} C(abw) &=& (Ca+ab+2(ba+aC+bC)+aba+abC+aCa+baC+bCa)w \ ( \ \text{if} \ bw\in W') \\ C(acw) &=& cacw \ ( \ \text{if} \ acw\in W') \\ C(bw) &=& -(ab+aC+ba+bC+Ca)w \ ( \ \text{if} \ bw\in W') \\ C(cw) &=& cw \ ( \ \text{if} \ cw\in W') \end{array}\right.\tag{2} $$ Indeed, any monomial distinct from $1$ or $a$ starts with exactly one of $ab$, $ac$, $b$ or $c$. It is not clear however (at least to me) how to show that Conjecture 1 (equivalent form). This $C$ satisfies $C^2=C$ and $(a+b+C)^2=a+b+C$.",,"['linear-algebra', 'abstract-algebra', 'examples-counterexamples', 'idempotents']"
83,What exactly is antieigenvalue analysis?,What exactly is antieigenvalue analysis?,,"I found a book in the library about antieigenvalue analysis and it is possibly the most unreadable piece of literature I have ever made an effort to understand. Unfortunately, every other resource I try inevitably takes you back to the same author. I should apologize for a lack of greater research effort, but beyond the line on the wikipedia page , The antieigenvectors $x$ are the vectors most turned by a matrix or operator $A$ I can't make heads or tails of anything else. Could someone explain why (or why not) this topic is useful or interesting? I've previously read about topics such as fractional calculus, harmonic analysis, non-standard analysis, product integration, quantum probability, higher order fourier analysis and other topics by dusting off a rarely read book off a shelf. I've always found something cool or interesting. I enjoy linear algebra quite a lot. The name antieigenvalue is very enticing to me. I really want to think that this topic is going to be neat. What is in antieigenvalue analysis that should excite me?","I found a book in the library about antieigenvalue analysis and it is possibly the most unreadable piece of literature I have ever made an effort to understand. Unfortunately, every other resource I try inevitably takes you back to the same author. I should apologize for a lack of greater research effort, but beyond the line on the wikipedia page , The antieigenvectors $x$ are the vectors most turned by a matrix or operator $A$ I can't make heads or tails of anything else. Could someone explain why (or why not) this topic is useful or interesting? I've previously read about topics such as fractional calculus, harmonic analysis, non-standard analysis, product integration, quantum probability, higher order fourier analysis and other topics by dusting off a rarely read book off a shelf. I've always found something cool or interesting. I enjoy linear algebra quite a lot. The name antieigenvalue is very enticing to me. I really want to think that this topic is going to be neat. What is in antieigenvalue analysis that should excite me?",,['linear-algebra']
84,An unusual type of linear algebra problem,An unusual type of linear algebra problem,,"I've come across the following linear algebra problem while trying to derive something in information theory. I'm looking both for numerical ways to solve this type of problem and for anything analytical that can be said about it. If there's a closed-form solution (e.g. in terms of matrix algebra) that would be great. I have a matrix $C$ (which might be singular) and vectors $a$ and $b$ such that $\sum_i a_i = \sum_j b_j$. I'm looking for vectors $\alpha$ and $\beta$ such the following conditions hold simultaneously: $$ \sum_i \alpha_i C_{ij} \beta_j = b_j\qquad\text{(for every $j$)} $$ and $$ \sum_j \alpha_i C_{ij} \beta_j = a_i\qquad\text{(for every $i$).} $$ Clearly there are cases where no solution exists, such as when $C$ is diagonal and $a\ne b$, but I suspect that in my case there always will be a solution. It looks like this should be easy to solve, but I can't quite see how to go about it. The following may or may not be relevant for answering the question, but in my case the numbers all relate to a joint probability distribution over variables $A$, $B$ and $X$: The elements of $C$ are the marginal distribution for $A$ and $B$. That is, $C_{ij} = p(A=i,B=i)$; $a$ and $b$ are the following marginal conditional probability distributions: $a_i = p(A=i \mathop{|} X=k)$ and $b_j = p(B=j \mathop{|} X=k)$, for some particular $k$; The numbers $\alpha_i C_{ij} \beta_j$ are the elements of an (unknown) conditional distribution $p(A=i,B=j\mathop{|}X=k)$, which is what I'm attempting to find. (Actually it's not the true conditional distribution but a minimum-information estimate of it, which is why it has this particular form.) In terms of the linear algebra problem, this means that $a$, $b$ and $C$ satisfy the following constraints: all the elements of $C$, $a$ and $b$ are real and between 0 and 1. $\sum_{ij}C_{ij} = 1$ $\sum_{i}a_{i} = \sum_j b_j = 1$. $a$ and $b$ can be expressed as the row and column sums of a matrix $D$ with real entries in $[0,1]$ such that $D_{ij}=0$ whenever $C_{ij}=0$, and $\sum_{ij}D_{ij}=1$. (The elements of $D$ are the ""true"" conditional distribution $p(A=i,B=j\mathop{|}X=k)$.)","I've come across the following linear algebra problem while trying to derive something in information theory. I'm looking both for numerical ways to solve this type of problem and for anything analytical that can be said about it. If there's a closed-form solution (e.g. in terms of matrix algebra) that would be great. I have a matrix $C$ (which might be singular) and vectors $a$ and $b$ such that $\sum_i a_i = \sum_j b_j$. I'm looking for vectors $\alpha$ and $\beta$ such the following conditions hold simultaneously: $$ \sum_i \alpha_i C_{ij} \beta_j = b_j\qquad\text{(for every $j$)} $$ and $$ \sum_j \alpha_i C_{ij} \beta_j = a_i\qquad\text{(for every $i$).} $$ Clearly there are cases where no solution exists, such as when $C$ is diagonal and $a\ne b$, but I suspect that in my case there always will be a solution. It looks like this should be easy to solve, but I can't quite see how to go about it. The following may or may not be relevant for answering the question, but in my case the numbers all relate to a joint probability distribution over variables $A$, $B$ and $X$: The elements of $C$ are the marginal distribution for $A$ and $B$. That is, $C_{ij} = p(A=i,B=i)$; $a$ and $b$ are the following marginal conditional probability distributions: $a_i = p(A=i \mathop{|} X=k)$ and $b_j = p(B=j \mathop{|} X=k)$, for some particular $k$; The numbers $\alpha_i C_{ij} \beta_j$ are the elements of an (unknown) conditional distribution $p(A=i,B=j\mathop{|}X=k)$, which is what I'm attempting to find. (Actually it's not the true conditional distribution but a minimum-information estimate of it, which is why it has this particular form.) In terms of the linear algebra problem, this means that $a$, $b$ and $C$ satisfy the following constraints: all the elements of $C$, $a$ and $b$ are real and between 0 and 1. $\sum_{ij}C_{ij} = 1$ $\sum_{i}a_{i} = \sum_j b_j = 1$. $a$ and $b$ can be expressed as the row and column sums of a matrix $D$ with real entries in $[0,1]$ such that $D_{ij}=0$ whenever $C_{ij}=0$, and $\sum_{ij}D_{ij}=1$. (The elements of $D$ are the ""true"" conditional distribution $p(A=i,B=j\mathop{|}X=k)$.)",,"['linear-algebra', 'probability-theory']"
85,Variety of pairs of product-zero matrices,Variety of pairs of product-zero matrices,,"Here's an old qualifying exam question I got stuck on. Consider the variety $X$ of pairs of matrices $(A,B)$ satisfying $AB = BA = 0$ (with entries in some field). What are the irreducible components of $X$? According to the question, they all have dimension $n^2$. A related question: are the irreducible components smooth away from their loci of intersection with other components?","Here's an old qualifying exam question I got stuck on. Consider the variety $X$ of pairs of matrices $(A,B)$ satisfying $AB = BA = 0$ (with entries in some field). What are the irreducible components of $X$? According to the question, they all have dimension $n^2$. A related question: are the irreducible components smooth away from their loci of intersection with other components?",,"['linear-algebra', 'algebraic-geometry']"
86,Determinant of a symmetric matrix,Determinant of a symmetric matrix,,"Given an $n\times n$ matrix $C= [c_{ij}]$ which is symmetric (i.e. $c_{ij}=c_{ji}\ \forall i,j$) calculate the determinant of the following matrix (assume $c_{ij} \neq 0\ \forall i,j$): $$\left(\begin{array}{cccccc}  \left(\sum_{i=1}^n\sum_{j=1}^n c_{ij}\right)^2 & \left(\sum_{j=1}^nc_{1j}\right)^2 & \left(\sum_{j=1}^nc_{2j}\right)^2 &\dots & \left(\sum_{j=1}^nc_{nj}\right)^2\\\left(\sum_{j=1}^nc_{1j}\right)^2 & c_{11}^2 & c_{12}^2 & \dots &c_{1n}^2\\ \left(\sum_{j=1}^nc_{2j}\right)^2 & c_{21}^2 & c_{22}^2 & \dots &c_{2n}^2\\ \vdots & \vdots &\vdots & \dots &\vdots \\ \left(\sum_{j=1}^nc_{nj}\right)^2 & c_{n1}^2 & c_{n2}^2 & \dots &c_{nn}^2   \end{array}\right)$$ Remark: For instance when $n=2$ the determinant is equal to $2(c_{11}c_{22}-c_{12}^2)^3$ (note $c_{12}=c_{21}$) which is of a nice form, but it seems hard to generalise this to the $(n+1) \times (n+1) $ case.","Given an $n\times n$ matrix $C= [c_{ij}]$ which is symmetric (i.e. $c_{ij}=c_{ji}\ \forall i,j$) calculate the determinant of the following matrix (assume $c_{ij} \neq 0\ \forall i,j$): $$\left(\begin{array}{cccccc}  \left(\sum_{i=1}^n\sum_{j=1}^n c_{ij}\right)^2 & \left(\sum_{j=1}^nc_{1j}\right)^2 & \left(\sum_{j=1}^nc_{2j}\right)^2 &\dots & \left(\sum_{j=1}^nc_{nj}\right)^2\\\left(\sum_{j=1}^nc_{1j}\right)^2 & c_{11}^2 & c_{12}^2 & \dots &c_{1n}^2\\ \left(\sum_{j=1}^nc_{2j}\right)^2 & c_{21}^2 & c_{22}^2 & \dots &c_{2n}^2\\ \vdots & \vdots &\vdots & \dots &\vdots \\ \left(\sum_{j=1}^nc_{nj}\right)^2 & c_{n1}^2 & c_{n2}^2 & \dots &c_{nn}^2   \end{array}\right)$$ Remark: For instance when $n=2$ the determinant is equal to $2(c_{11}c_{22}-c_{12}^2)^3$ (note $c_{12}=c_{21}$) which is of a nice form, but it seems hard to generalise this to the $(n+1) \times (n+1) $ case.",,"['linear-algebra', 'matrices', 'determinant']"
87,Estimating quality of projection,Estimating quality of projection,,"Suppose we are given a vector $v$ and vectors $\mu_i$ : $v = \mu_1+\mu_2+...+\mu_m$ , where $\mu_i \in R^n$ , all $\mu_i$ are of unit length. Oracle will give me $k$ vectors $\mu_{j_1}, \mu_{j_2},...\mu_{j_k}$ from the original set such that when I project $v$ onto subspace spanned by these vectors the length of the projection is highest possible. In other words, from the set of all combinations of $k$ vectors from $[\mu_1,...\mu_n]$ the $[\mu_{j_1}, \mu_{j_2},...\mu_{j_k}]$ give highest length of projection. Lets denote by $v_{\text{proj}}$ projection of $v$ onto $[\mu_{j_1}, \mu_{j_2},...\mu_{j_k}]$ I want to estimate quality of projection before oracle gives me this $k$ vectors. I want to give upper bound on $||v - v_{\text{proj}}|| $ As far as I understood it is very difficult to obtain these $k$ vectors by myself. However, I know  that for any two vectors $\mu_i, \mu_j$ , $||\mu_i-\mu_j|| \leq \alpha$ , where $\alpha$ is  a given positive number. Small values of $\alpha$ will tell me that all $\mu_i$ are close to each other and heading towards same direction. I would suspect then that projection will be good, and its length will be close to the length of original vector. How can I use this to give an upper bound $||v - v_{\text{proj}}|| $ ? My attempts : Without loss of generality lets assume that $k$ optimal vectors are first $k$ vectors in the list, i.e $\mu_1,\mu_2,...\mu_k$ . Lets denote by $P$ projection operator on the space spanned by $\mu_1,\mu_2,...\mu_k$ . $\|v - v_{\text{proj}}\|  = \|v - P(v)\| = \|v - P(\mu_1+\mu_2+...+\mu_m)\| = $ $\|v - P(\mu_1) - P(\mu_2) - ... - P(\mu_m)\| = $ $ \| v - \mu_1 - \mu_2 - ... - \mu_k - P(\mu_{k+1}) - P(\mu_{k+2}) - ... - P(\mu_m)\| = $ $\|\mu_{k+1} - P(\mu_{k+1}) + \mu_{k+2} - P(\mu_{k+2}) + ... + \mu_{m} - P(\mu_{m})\|$ $\|v - v_{\text{proj}}\| \leq \|\mu_{k+1} - P(\mu_{k+1})\| + \|\mu_{k+2} - P(\mu_{k+2}) + ... + \|\mu_{m} - P(\mu_{m})\|$ $\|v - v_{\text{proj}}\| \leq (m-k)\alpha$ So in order to make $\|v - v_{\text{proj}}\| \leq \epsilon$ , we need $k \geq \frac{m\alpha - \epsilon}{\alpha}$ I am not satisfied with this result because $k$ grows linearly with $m$ . I want it to grow much slower, something like $\log(m)$ . My goal is to show that under some constraints on $\mu_i$ , we need only approximately $\log(m)$ vectors to approximate $v$ . I think the bound can be improved substantially. First Cauchy inequality isn't very tight and second, I used $|\mu_{k+1} - P(\mu_{k+1})\| \leq \alpha$ which is also very loose. I am open for additional constraints on $\mu_1,...\mu_m$ to achieve logarithmic growth As Alex Ravsky has noted, we also need a constraint on $\alpha$ in order to achieve logarithmic growth. Assume that $m$ $\leq n$ , $\mu_i$ is th $i$ -th standard ort of the space $\mathbb{R}^n$ , and $\alpha = \sqrt{2}$ . Then $\|v -  v_{\text{proj}}\| = \sqrt{m-k}$","Suppose we are given a vector and vectors : , where , all are of unit length. Oracle will give me vectors from the original set such that when I project onto subspace spanned by these vectors the length of the projection is highest possible. In other words, from the set of all combinations of vectors from the give highest length of projection. Lets denote by projection of onto I want to estimate quality of projection before oracle gives me this vectors. I want to give upper bound on As far as I understood it is very difficult to obtain these vectors by myself. However, I know  that for any two vectors , , where is  a given positive number. Small values of will tell me that all are close to each other and heading towards same direction. I would suspect then that projection will be good, and its length will be close to the length of original vector. How can I use this to give an upper bound ? My attempts : Without loss of generality lets assume that optimal vectors are first vectors in the list, i.e . Lets denote by projection operator on the space spanned by . So in order to make , we need I am not satisfied with this result because grows linearly with . I want it to grow much slower, something like . My goal is to show that under some constraints on , we need only approximately vectors to approximate . I think the bound can be improved substantially. First Cauchy inequality isn't very tight and second, I used which is also very loose. I am open for additional constraints on to achieve logarithmic growth As Alex Ravsky has noted, we also need a constraint on in order to achieve logarithmic growth. Assume that , is th -th standard ort of the space , and . Then","v \mu_i v = \mu_1+\mu_2+...+\mu_m \mu_i \in R^n \mu_i k \mu_{j_1}, \mu_{j_2},...\mu_{j_k} v k [\mu_1,...\mu_n] [\mu_{j_1}, \mu_{j_2},...\mu_{j_k}] v_{\text{proj}} v [\mu_{j_1}, \mu_{j_2},...\mu_{j_k}] k ||v - v_{\text{proj}}||  k \mu_i, \mu_j ||\mu_i-\mu_j|| \leq \alpha \alpha \alpha \mu_i ||v - v_{\text{proj}}||  k k \mu_1,\mu_2,...\mu_k P \mu_1,\mu_2,...\mu_k \|v - v_{\text{proj}}\|  = \|v - P(v)\| = \|v - P(\mu_1+\mu_2+...+\mu_m)\| =  \|v - P(\mu_1) - P(\mu_2) - ... - P(\mu_m)\| =   \| v - \mu_1 - \mu_2 - ... - \mu_k - P(\mu_{k+1}) - P(\mu_{k+2}) - ... - P(\mu_m)\| =  \|\mu_{k+1} - P(\mu_{k+1}) + \mu_{k+2} - P(\mu_{k+2}) + ... + \mu_{m} - P(\mu_{m})\| \|v - v_{\text{proj}}\| \leq \|\mu_{k+1} - P(\mu_{k+1})\| + \|\mu_{k+2} - P(\mu_{k+2}) + ... + \|\mu_{m} - P(\mu_{m})\| \|v - v_{\text{proj}}\| \leq (m-k)\alpha \|v - v_{\text{proj}}\| \leq \epsilon k \geq \frac{m\alpha - \epsilon}{\alpha} k m \log(m) \mu_i \log(m) v |\mu_{k+1} - P(\mu_{k+1})\| \leq \alpha \mu_1,...\mu_m \alpha m \leq n \mu_i i \mathbb{R}^n \alpha = \sqrt{2} \|v -  v_{\text{proj}}\| = \sqrt{m-k}","['linear-algebra', 'inequality', 'projection']"
88,How do you solve linear least-squares modulo $2 \pi$?,How do you solve linear least-squares modulo ?,2 \pi,"I have an overdetermined system of $m$ equations ( $i = 1, 2, \dots, m$ ) $$ \sum_{j=1}^n A_{ij} \, x_j = y_i \pmod{2\pi} $$ where the $x$ coefficients are unknown, and $m > n$ . This is, essentially, the linear least squares problem but on $\mathbb{R}/\mathbb{Z}$ . But I have no idea if there is a good way to solve or work with this kind of problem. For context, I am trying to solve for phases of some set of $m$ equations involving complex variables, $$ y_i = \prod_{\{j\}} x_j $$ where $i = 1, 2, \dots, m$ , ${j}$ is some subset of $j = {1, 2, \dots, n}$ and $m > n$ . If you take the logarithm of this equations, $$ \ln(y_i) = \sum_{\{j\}} \ln(x_j) $$ which can be written as $$  \ln(y_i) = \sum_{j=1}^{n} A_{ij} \ln(x_j) $$ where $A_{ij}$ is a term that is either $0$ or $1$ depending on whether the corresponding $x_j$ showed up in the product above. Taking the imaginary part of this equation, I get a set of linear equations involving just the phases (arguments) of the complex variables but since phases wrap around every $2 \pi$ , I don't think I can solve these in the regular linear least squares manner.","I have an overdetermined system of equations ( ) where the coefficients are unknown, and . This is, essentially, the linear least squares problem but on . But I have no idea if there is a good way to solve or work with this kind of problem. For context, I am trying to solve for phases of some set of equations involving complex variables, where , is some subset of and . If you take the logarithm of this equations, which can be written as where is a term that is either or depending on whether the corresponding showed up in the product above. Taking the imaginary part of this equation, I get a set of linear equations involving just the phases (arguments) of the complex variables but since phases wrap around every , I don't think I can solve these in the regular linear least squares manner.","m i = 1, 2, \dots, m  \sum_{j=1}^n A_{ij} \, x_j = y_i \pmod{2\pi}  x m > n \mathbb{R}/\mathbb{Z} m  y_i = \prod_{\{j\}} x_j  i = 1, 2, \dots, m {j} j = {1, 2, \dots, n} m > n  \ln(y_i) = \sum_{\{j\}} \ln(x_j)    \ln(y_i) = \sum_{j=1}^{n} A_{ij} \ln(x_j)  A_{ij} 0 1 x_j 2 \pi","['linear-algebra', 'complex-numbers', 'modular-arithmetic', 'systems-of-equations', 'least-squares']"
89,Projective equivalence: Linear subspaces under the action of $PGL_n$,Projective equivalence: Linear subspaces under the action of,PGL_n,"A pair of ordered collections linear subspaces $\Lambda_1, \ldots, \Lambda_k$ and $\Lambda'_1, \ldots, \Lambda'_k$ of $\mathbb{P}^n$ are called projectively equivalent if there exists a regular automorphism $\phi : \mathbb{P}^n \to \mathbb{P}^n$ (equivalently, a member of $PGL_{n+1}$) such that $\phi(\Lambda_i) = \Lambda'_i$ for each $i$. It is well-known that two ordered sets of $n+2$ points in $\mathbb{P}^n$ in general position are projectively equivalent, and that any two ordered sets of three pairwise disjoint lines in $\mathbb{P}^3$ are projectively equivalent. Likewise, any two pairs of a hyperplane in $\mathbb{P}^n$ and a point outside of it are projectively equivalent. Three pairwise disjoint lines satisfies the condition that no two of them lies in a 2-plane, which is a condition on their relative position similar to points in general position. I have two questions: 1) In what sense does collections of linear subspaces lie in general position? Is it simply that any subset of them span a linear subspace of maximal dimension? 2) What kind of ""classifying theorems"" are there regarding which pairs of collections of ordered linear subspaces in $\mathbb{P}^n$ (in general position, or other types of conditions) are projectively equivalent? I'm looking for a sharp relation between the number $k$, the individual dimensions of each $\Lambda_i$, and the strictness of the condition of their relative position under which two such collections are projectively equivalent. Perhaps such a theorem is best formulated in terms of Grassmannians.","A pair of ordered collections linear subspaces $\Lambda_1, \ldots, \Lambda_k$ and $\Lambda'_1, \ldots, \Lambda'_k$ of $\mathbb{P}^n$ are called projectively equivalent if there exists a regular automorphism $\phi : \mathbb{P}^n \to \mathbb{P}^n$ (equivalently, a member of $PGL_{n+1}$) such that $\phi(\Lambda_i) = \Lambda'_i$ for each $i$. It is well-known that two ordered sets of $n+2$ points in $\mathbb{P}^n$ in general position are projectively equivalent, and that any two ordered sets of three pairwise disjoint lines in $\mathbb{P}^3$ are projectively equivalent. Likewise, any two pairs of a hyperplane in $\mathbb{P}^n$ and a point outside of it are projectively equivalent. Three pairwise disjoint lines satisfies the condition that no two of them lies in a 2-plane, which is a condition on their relative position similar to points in general position. I have two questions: 1) In what sense does collections of linear subspaces lie in general position? Is it simply that any subset of them span a linear subspace of maximal dimension? 2) What kind of ""classifying theorems"" are there regarding which pairs of collections of ordered linear subspaces in $\mathbb{P}^n$ (in general position, or other types of conditions) are projectively equivalent? I'm looking for a sharp relation between the number $k$, the individual dimensions of each $\Lambda_i$, and the strictness of the condition of their relative position under which two such collections are projectively equivalent. Perhaps such a theorem is best formulated in terms of Grassmannians.",,"['linear-algebra', 'geometry', 'algebraic-geometry']"
90,Inverse matrix and its zero entries,Inverse matrix and its zero entries,,"Let $A$ be an $N \times N$ square invertible matrix with inverse $A^{-1}$. Is it possible to know through information of $A$ alone (i.e. without actually calculating $A^{-1}$) Which entries of $A^{-1}$ will be zero? How many entries of $A^{-1}$ will be zero? Are there certain matrices for which we know which entries of the inverse are zero / the number of zero entries? Can something be said probabilistically if an entry of $A^{-1}$ is zero? For example, the inverse of an upper triangular matrix is also upper triangular so we know how many entries are zero ($N^2-N$) and which entries are zero (the upper triangle).","Let $A$ be an $N \times N$ square invertible matrix with inverse $A^{-1}$. Is it possible to know through information of $A$ alone (i.e. without actually calculating $A^{-1}$) Which entries of $A^{-1}$ will be zero? How many entries of $A^{-1}$ will be zero? Are there certain matrices for which we know which entries of the inverse are zero / the number of zero entries? Can something be said probabilistically if an entry of $A^{-1}$ is zero? For example, the inverse of an upper triangular matrix is also upper triangular so we know how many entries are zero ($N^2-N$) and which entries are zero (the upper triangle).",,"['linear-algebra', 'matrices']"
91,Computing the SVD factorization on C++ (using the proof of the existence of the SVD factorization),Computing the SVD factorization on C++ (using the proof of the existence of the SVD factorization),,"I am doing a C++ program that computes the SVD factorization of a real matrix A without using any known library of algebra that contains the implementation. In addition, QR descomposition is not allowed in any step (because one of my goals is to compare QR and SVD descomposition in a least squares problem). I know there are a lot of methods, with well tecniques, but my method is simple, far from being the fastest and better one, but it is based on a proof of the theorem about SVD factorization that I have done at University. I know that it is not the best way (I accept ideas and proposals if you want to tell me something better). The proof of the theorem (and the diagram of my algorithm) is: Teorem: Let $A \in M_{m,n} (\mathbb{R})$, with $m \geq n$. Then exists the $A$ =    $U \cdot \Sigma \cdot V^T $, with these properties: $U\in M_{m,n} (\mathbb{R})$ satisfies $U \cdot U^T = I_n$ (orthogonal   matrix) $\Sigma \in M_{n,n} (\mathbb{R})$ is a diagonal matrix with $\Sigma =$   diag$(\sigma_1, \sigma_2,..., \sigma_n)$ $V\in M_{n,n} (\mathbb{R})$ satisfies $V \cdot V^T = I_n$ (orthogonal   matrix) Proof: Construct $S = A^T \cdot A$, so $S \in M_{n,n} (\mathbb{R})$. We have   that $S$ is a symmetric and positive semi-definite matrix (you can   check that as an exercise, but if you want to know why it is true ask   me and then I'll write the proof of that). According to that, we can find an orthonormal basis of $\mathbb{R}^n$   of eigenvectors, $\mathbb{R}^n = [u_1,u_2,...,u_n]$, with   $<u_i,u_j>=d_{ij}$, with $d_{ij}=0$ if $j\neq i$ and $d_{ij}=1$ if $j > = i$. Let be $\lambda_1, \lambda_2, ..., \lambda_n$ the respective eigenvalues. Let be $1 \leq r \leq n$, with $\lambda_1, \lambda_2,..., > \lambda_r \geq 0$, and $\lambda_{r+1}, \lambda_{r+2}, \lambda_{n} = 0$   (remember that $S$ is a positive semi-definite matrix, so eigenvalues   can't be negative). Then, $\sigma_i = \sqrt{\lambda_i}$, so our $\Sigma = $diag$(\sigma_1, > \sigma_2,..., \sigma_n)$ it is now constructed. In addition, we will   say that $V=(v_1,v_2,...,v_n)$, and our matrix $V$ is now constructed   too. At the end we will see that all these definitions make sense and   the descomposition works. Finally, let's find the matrix U. We introduce the vectors $u_i = \frac{1}{\sigma_i} \cdot A \cdot v_i$,   for $1 \leq i \leq r$. These are unitary and two-by-two orthonormal   vectors (you can check that as an exercise, if you want to know the   proof tell me). Now we choose $u_{r+1}, u_{r+2},..., u_{n} \in  > \mathbb{R}^m$ that makes {$u_1, u_2,...,u_r,u_{r+1},u_{r+2},...,u_n$}   an orthonomal basis of $\mathbb{R}^m$. And we finally have our last   matrix $U = (u_1, u_2, ... , u_n)$. Let's check if the factorization   is correct. It is easy to see that $A \cdot V = U \cdot \Sigma$ : we have that $ A > \cdot v_j = \sigma_j  \cdot \frac{1}{\sigma_j} \cdot A \cdot v_j = > \sigma_j \cdot u_j$ for $1 \leq j \leq r$ and $A \cdot v_j = 0$ for   $r+1 \leq j \leq n$, so the equality it's true. Then, as the last   step, we see that $V$ is an orthogonal matrix (because the column   vectors make an orthonormal basis), so $V^{-1} = V^T$ and $A \cdot V > \cdot V^T = U \cdot \Sigma \cdot V^T \iff A = U \cdot \Sigma \cdot > V^T$, as we wanted to proof $\blacksquare$ First, I want to use a method like the power method to find the eigenvalues and eigenvectors of $S$, knowing that $S$ is a symmetric positive semi-definite matrix. The problem of using the power method is that it does not use the properties of $S$, and it finds only one eigenvector for one eigenvalue, and I want to find all the eigenvectors linked with a given eigenvalue... in fact, I want to know a basis of the eigenspace corresponding to a given eigenvalue. First question: Do you know a good algorithm (not too difficult) to find all the eigenvectors for each eigenvalue? (not only one, if more than one eigenvector have the same eigenvalue). Does this method give me the orthonormal basis of eigenvectors? I can't use the QR algorithm (I currently saw an algorithm to find the eigenspace of an eigenvalue using QR factorization). If I have this algorithm, it's very easy to find $V$, $\Sigma$ and the $u_1,u_2,...,u_r$ of the proof. The last step is to find $u_{r+1},...,u_{n}$ vectors that make {$u_1,...,u_n$} an orthonormal basis. Second (and last) question: Do you know a good algorithm to, given some two-by-two orthonormal vectors, complete them to a basis of the corresponding vectorial space? I don't know if it's useful to find linearly independent vectors and use the Gram-Schmidt algorithm to make an orthonormal basis: in fact, if I do this I'm using a half of a part of the QR implementation (that can be done with Gram-Schmidt), so if it is an alternative to this I will be grateful to read it. Lots of thanks and sorry for my English. From Spain, have a nice day.","I am doing a C++ program that computes the SVD factorization of a real matrix A without using any known library of algebra that contains the implementation. In addition, QR descomposition is not allowed in any step (because one of my goals is to compare QR and SVD descomposition in a least squares problem). I know there are a lot of methods, with well tecniques, but my method is simple, far from being the fastest and better one, but it is based on a proof of the theorem about SVD factorization that I have done at University. I know that it is not the best way (I accept ideas and proposals if you want to tell me something better). The proof of the theorem (and the diagram of my algorithm) is: Teorem: Let $A \in M_{m,n} (\mathbb{R})$, with $m \geq n$. Then exists the $A$ =    $U \cdot \Sigma \cdot V^T $, with these properties: $U\in M_{m,n} (\mathbb{R})$ satisfies $U \cdot U^T = I_n$ (orthogonal   matrix) $\Sigma \in M_{n,n} (\mathbb{R})$ is a diagonal matrix with $\Sigma =$   diag$(\sigma_1, \sigma_2,..., \sigma_n)$ $V\in M_{n,n} (\mathbb{R})$ satisfies $V \cdot V^T = I_n$ (orthogonal   matrix) Proof: Construct $S = A^T \cdot A$, so $S \in M_{n,n} (\mathbb{R})$. We have   that $S$ is a symmetric and positive semi-definite matrix (you can   check that as an exercise, but if you want to know why it is true ask   me and then I'll write the proof of that). According to that, we can find an orthonormal basis of $\mathbb{R}^n$   of eigenvectors, $\mathbb{R}^n = [u_1,u_2,...,u_n]$, with   $<u_i,u_j>=d_{ij}$, with $d_{ij}=0$ if $j\neq i$ and $d_{ij}=1$ if $j > = i$. Let be $\lambda_1, \lambda_2, ..., \lambda_n$ the respective eigenvalues. Let be $1 \leq r \leq n$, with $\lambda_1, \lambda_2,..., > \lambda_r \geq 0$, and $\lambda_{r+1}, \lambda_{r+2}, \lambda_{n} = 0$   (remember that $S$ is a positive semi-definite matrix, so eigenvalues   can't be negative). Then, $\sigma_i = \sqrt{\lambda_i}$, so our $\Sigma = $diag$(\sigma_1, > \sigma_2,..., \sigma_n)$ it is now constructed. In addition, we will   say that $V=(v_1,v_2,...,v_n)$, and our matrix $V$ is now constructed   too. At the end we will see that all these definitions make sense and   the descomposition works. Finally, let's find the matrix U. We introduce the vectors $u_i = \frac{1}{\sigma_i} \cdot A \cdot v_i$,   for $1 \leq i \leq r$. These are unitary and two-by-two orthonormal   vectors (you can check that as an exercise, if you want to know the   proof tell me). Now we choose $u_{r+1}, u_{r+2},..., u_{n} \in  > \mathbb{R}^m$ that makes {$u_1, u_2,...,u_r,u_{r+1},u_{r+2},...,u_n$}   an orthonomal basis of $\mathbb{R}^m$. And we finally have our last   matrix $U = (u_1, u_2, ... , u_n)$. Let's check if the factorization   is correct. It is easy to see that $A \cdot V = U \cdot \Sigma$ : we have that $ A > \cdot v_j = \sigma_j  \cdot \frac{1}{\sigma_j} \cdot A \cdot v_j = > \sigma_j \cdot u_j$ for $1 \leq j \leq r$ and $A \cdot v_j = 0$ for   $r+1 \leq j \leq n$, so the equality it's true. Then, as the last   step, we see that $V$ is an orthogonal matrix (because the column   vectors make an orthonormal basis), so $V^{-1} = V^T$ and $A \cdot V > \cdot V^T = U \cdot \Sigma \cdot V^T \iff A = U \cdot \Sigma \cdot > V^T$, as we wanted to proof $\blacksquare$ First, I want to use a method like the power method to find the eigenvalues and eigenvectors of $S$, knowing that $S$ is a symmetric positive semi-definite matrix. The problem of using the power method is that it does not use the properties of $S$, and it finds only one eigenvector for one eigenvalue, and I want to find all the eigenvectors linked with a given eigenvalue... in fact, I want to know a basis of the eigenspace corresponding to a given eigenvalue. First question: Do you know a good algorithm (not too difficult) to find all the eigenvectors for each eigenvalue? (not only one, if more than one eigenvector have the same eigenvalue). Does this method give me the orthonormal basis of eigenvectors? I can't use the QR algorithm (I currently saw an algorithm to find the eigenspace of an eigenvalue using QR factorization). If I have this algorithm, it's very easy to find $V$, $\Sigma$ and the $u_1,u_2,...,u_r$ of the proof. The last step is to find $u_{r+1},...,u_{n}$ vectors that make {$u_1,...,u_n$} an orthonormal basis. Second (and last) question: Do you know a good algorithm to, given some two-by-two orthonormal vectors, complete them to a basis of the corresponding vectorial space? I don't know if it's useful to find linearly independent vectors and use the Gram-Schmidt algorithm to make an orthonormal basis: in fact, if I do this I'm using a half of a part of the QR implementation (that can be done with Gram-Schmidt), so if it is an alternative to this I will be grateful to read it. Lots of thanks and sorry for my English. From Spain, have a nice day.",,"['linear-algebra', 'numerical-linear-algebra', 'eigenvalues-eigenvectors', 'svd']"
92,Show whether matrix is positive semidefinite or not,Show whether matrix is positive semidefinite or not,,"Background and motivation: When creating a Mercer Kernel Function we need to show that the Gram matrix defined by the function is positive semidefinite. Let $A_1, \ldots, A_n$ be subsets of $\{0, 1, \ldots, D\}$. Let $S(X)$ be the smallest $k$ elements of the set $X$. so with $k= 3$ we have $S(\{5,7,1,2,8,3\}) = \{1,2,3\}$. Let's construct a matrix $G$ with $$ G_{i,j} = |S(A_i)\cap S(A_j)\cap S(A_i\cup A_j)| = |S(A_i)\cap S(A_j)\cap S(S(A_i)\cup S(A_j))| $$ (The last equality is pretty intuitive and easy to show). Is $G$ Positive semidefinite? $D$, $n$ and $k$ are all given positive integers. It is clear that $G$ is symmetric with only positive entries. For inspiration it is pretty easy to show that the matrix $M$ defined by $M_{i,j} = |S(A_i) \cap S(A_j)|$ is Positive semidefinite. Create a vector $v_i$ that has $v_{i,j} = 1$ if $j\in S(A_i)$ and $0$ otherwise. each $v_i$ is then a $D+1$ dimensional vector and we have $M_{i,j} = \langle v_i, v_j\rangle$, so $M$ is the product of a matrix and its transpose.","Background and motivation: When creating a Mercer Kernel Function we need to show that the Gram matrix defined by the function is positive semidefinite. Let $A_1, \ldots, A_n$ be subsets of $\{0, 1, \ldots, D\}$. Let $S(X)$ be the smallest $k$ elements of the set $X$. so with $k= 3$ we have $S(\{5,7,1,2,8,3\}) = \{1,2,3\}$. Let's construct a matrix $G$ with $$ G_{i,j} = |S(A_i)\cap S(A_j)\cap S(A_i\cup A_j)| = |S(A_i)\cap S(A_j)\cap S(S(A_i)\cup S(A_j))| $$ (The last equality is pretty intuitive and easy to show). Is $G$ Positive semidefinite? $D$, $n$ and $k$ are all given positive integers. It is clear that $G$ is symmetric with only positive entries. For inspiration it is pretty easy to show that the matrix $M$ defined by $M_{i,j} = |S(A_i) \cap S(A_j)|$ is Positive semidefinite. Create a vector $v_i$ that has $v_{i,j} = 1$ if $j\in S(A_i)$ and $0$ otherwise. each $v_i$ is then a $D+1$ dimensional vector and we have $M_{i,j} = \langle v_i, v_j\rangle$, so $M$ is the product of a matrix and its transpose.",,['linear-algebra']
93,How to prove a Wronskian identity?,How to prove a Wronskian identity?,,"The following Wronskian identity can be proved by expanding both sides and checking that two sides are the same. But how to prove it more elegantly? Let $u_1(x), u_2(x), u_3(x), u_4(x)$ be four functions. Define q-shift Wronskian as follows: $$W(u_1, u_2, u_3)(x)=\det \begin{bmatrix} u_1(x) & u_1(xq^{-2}) & u_1(xq^{-4}) \\ u_2(x) & u_2(xq^{-2}) & u_2(xq^{-4}) \\ u_3(x) & u_3(xq^{-2}) & u_3(xq^{-4})  \end{bmatrix}.$$ Similarly for $W(u_1, u_2)(x)$ and $W(u_1, u_2, u_3, u_4)(x)$. Then we have a Wronskian identity: $$ W(W(u_1, u_3, u_4)(x), W(u_2, u_3, u_4)(x))(x) = W(u_1, u_2, u_3, u_4)(x) \cdot W(u_3, u_4)(xq^{-2}). $$ Thank you very much. Edit: The general version of the identity is the following. Let $W_s(i)=W(u_1, \ldots, \hat{u_i}, \ldots, u_{s+1})$, where $\hat{u_i}$ means without $u_i$. Given functions $u_1, \ldots, u_{s+1}$. $$ W_{k+1}(W_s(s-1)(x), W_s(s-2)(x), \ldots, W_{s}(s-k-1)(x))(x) = \\ \left(\prod_{j=1}^{k} W_{s+1}(u_1, \ldots, u_{s+1})(xq^{-2(j-1)})\right) \cdot W_{s-k}(u_{k+2}, \ldots, u_{s+1})(xq^{-2k}). $$","The following Wronskian identity can be proved by expanding both sides and checking that two sides are the same. But how to prove it more elegantly? Let $u_1(x), u_2(x), u_3(x), u_4(x)$ be four functions. Define q-shift Wronskian as follows: $$W(u_1, u_2, u_3)(x)=\det \begin{bmatrix} u_1(x) & u_1(xq^{-2}) & u_1(xq^{-4}) \\ u_2(x) & u_2(xq^{-2}) & u_2(xq^{-4}) \\ u_3(x) & u_3(xq^{-2}) & u_3(xq^{-4})  \end{bmatrix}.$$ Similarly for $W(u_1, u_2)(x)$ and $W(u_1, u_2, u_3, u_4)(x)$. Then we have a Wronskian identity: $$ W(W(u_1, u_3, u_4)(x), W(u_2, u_3, u_4)(x))(x) = W(u_1, u_2, u_3, u_4)(x) \cdot W(u_3, u_4)(xq^{-2}). $$ Thank you very much. Edit: The general version of the identity is the following. Let $W_s(i)=W(u_1, \ldots, \hat{u_i}, \ldots, u_{s+1})$, where $\hat{u_i}$ means without $u_i$. Given functions $u_1, \ldots, u_{s+1}$. $$ W_{k+1}(W_s(s-1)(x), W_s(s-2)(x), \ldots, W_{s}(s-k-1)(x))(x) = \\ \left(\prod_{j=1}^{k} W_{s+1}(u_1, \ldots, u_{s+1})(xq^{-2(j-1)})\right) \cdot W_{s-k}(u_{k+2}, \ldots, u_{s+1})(xq^{-2k}). $$",,"['linear-algebra', 'determinant', 'wronskian']"
94,Möbius transformations and the Lorentz group,Möbius transformations and the Lorentz group,,"In Tristan Needham's wonderful book Visual Complex Analysis , the chapter on Möbius transformations begins by asserting that they correspond one-to-one with the Lorentz transformations from special relativity. I tried to work out the details of this correspondence and I would like my results to be confirmed or corrected. I am pretty sure of my work but wanted to ask because it differs slightly from what Needham asserts. I am working with the following definitions: (1) The Möbius transformations are transformations of the complex plane of the form $z \mapsto \frac{az+b}{cz+d}, a,b,c,d \in \mathbb{C}, ad-bc \neq 0$. I'll refer to the group of these as $P(1,\mathbb{C})$, the notation in Ahlfors . (2) The Lorentz group is the group of linear transformations of $\mathbb{R}^4$ that preserve the Lorentz form $t^2-x^2-y^2-z^2$. Thus it is represented by the set of real 4x4 matrices satisfying $M^tI_{1,3}M=I_{1,3}$ where $I_{1,3}$ is the matrix of the Lorentz form, i.e. $1, -1, -1, -1$ on the main diagonal and zero elsewhere. For the group I'll use the notation $O_{1,3}$ from Artin . (3) A ""space-like interval"" is a vector in $\mathbb{R}^4$ on which the Lorentz form is negative; a ""time-like interval"" is a vector in $\mathbb{R}^4$ on which the Lorentz form is positive. I am adopting the convention that time is the first coordinate. After many hours of work I now believe the following: (a) The isomorphism with $P(1,\mathbb{C})$ really only belongs to a subgroup of $O_{1,3}$ of index 4, call it $M$. There is a subgroup of index 2 of $O_{1,3}$ that has a surjective homomorphism to $P(1,\mathbb{C})$, but it is not bijective because the kernel is $\{I,-I\}$. The Lorentz transformations in the coset of the index 2 subgroup actually map to orientation-reversing transformations of $\mathbb{C}$ which are thus not Möbius transformations. (b) $M$ is the path component containing $I$. (c) Also, $M$ is the set of Lorentz transformations that preserve the direction of time on time-like intervals and also preserve the orientation of 3-space given by the space-components of sets of 3 linearly independent space-like intervals. (d) However, even the transformations in this subgroup $M$ are capable of reversing time on space-like intervals or the orientation of space given by the space components of sets of 3 time-like intervals. Is all this correct? If/where incorrect, what is true instead?","In Tristan Needham's wonderful book Visual Complex Analysis , the chapter on Möbius transformations begins by asserting that they correspond one-to-one with the Lorentz transformations from special relativity. I tried to work out the details of this correspondence and I would like my results to be confirmed or corrected. I am pretty sure of my work but wanted to ask because it differs slightly from what Needham asserts. I am working with the following definitions: (1) The Möbius transformations are transformations of the complex plane of the form $z \mapsto \frac{az+b}{cz+d}, a,b,c,d \in \mathbb{C}, ad-bc \neq 0$. I'll refer to the group of these as $P(1,\mathbb{C})$, the notation in Ahlfors . (2) The Lorentz group is the group of linear transformations of $\mathbb{R}^4$ that preserve the Lorentz form $t^2-x^2-y^2-z^2$. Thus it is represented by the set of real 4x4 matrices satisfying $M^tI_{1,3}M=I_{1,3}$ where $I_{1,3}$ is the matrix of the Lorentz form, i.e. $1, -1, -1, -1$ on the main diagonal and zero elsewhere. For the group I'll use the notation $O_{1,3}$ from Artin . (3) A ""space-like interval"" is a vector in $\mathbb{R}^4$ on which the Lorentz form is negative; a ""time-like interval"" is a vector in $\mathbb{R}^4$ on which the Lorentz form is positive. I am adopting the convention that time is the first coordinate. After many hours of work I now believe the following: (a) The isomorphism with $P(1,\mathbb{C})$ really only belongs to a subgroup of $O_{1,3}$ of index 4, call it $M$. There is a subgroup of index 2 of $O_{1,3}$ that has a surjective homomorphism to $P(1,\mathbb{C})$, but it is not bijective because the kernel is $\{I,-I\}$. The Lorentz transformations in the coset of the index 2 subgroup actually map to orientation-reversing transformations of $\mathbb{C}$ which are thus not Möbius transformations. (b) $M$ is the path component containing $I$. (c) Also, $M$ is the set of Lorentz transformations that preserve the direction of time on time-like intervals and also preserve the orientation of 3-space given by the space-components of sets of 3 linearly independent space-like intervals. (d) However, even the transformations in this subgroup $M$ are capable of reversing time on space-like intervals or the orientation of space given by the space components of sets of 3 time-like intervals. Is all this correct? If/where incorrect, what is true instead?",,"['linear-algebra', 'complex-analysis']"
95,Does there exist a section of $GL_n(\mathbb{Z}/9\mathbb{Z}) \rightarrow GL_n(\mathbb{Z}/3\mathbb{Z})$?,Does there exist a section of ?,GL_n(\mathbb{Z}/9\mathbb{Z}) \rightarrow GL_n(\mathbb{Z}/3\mathbb{Z}),"There is the reduction map $r : GL_n(\mathbb{Z}/9\mathbb{Z}) \rightarrow GL_n(\mathbb{Z}/3\mathbb{Z})$ . When does there exist a group homomorphism $i : GL_n(\mathbb{Z}/3\mathbb{Z}) \rightarrow GL_n(\mathbb{Z}/9\mathbb{Z})$ such that $r \circ i = id$ ? After testing on Python I think that there exists such section for $n = 2$ but not for $n \geq 3$ , but I don't manage to prove it. I tried to use the fact that the group $GL_n(\mathbb{Z}/3\mathbb{Z})$ is generated by two matrices : an elementary matrix $E$ and a permutation matrix $P$ that acts as a $n$ -cycle on the basis (modulo a sign, depending if $n$ is odd or even). I wanted to prove that there doesn't exist any $(i(E),i(P))$ in $GL_n(\mathbb{Z}/9\mathbb{Z})$ which respect the group law. However according to Python for $n=3$ , there exists for instance $(i(E),i(P))$ which preserve at least the order of $E$ , $P$ , $PE$ , $P^2E$ , so it seems that an argument using this method would be quite complicated... I managed to prove that there doesn't exist a section of $GL_n(\mathbb{Z}/p^2\mathbb{Z}) \rightarrow GL_n(\mathbb{Z}/p\mathbb{Z})$ for any $n \geq 2$ if $p \geq 5$ , but the proof doesn't work if $p =3$ .","There is the reduction map . When does there exist a group homomorphism such that ? After testing on Python I think that there exists such section for but not for , but I don't manage to prove it. I tried to use the fact that the group is generated by two matrices : an elementary matrix and a permutation matrix that acts as a -cycle on the basis (modulo a sign, depending if is odd or even). I wanted to prove that there doesn't exist any in which respect the group law. However according to Python for , there exists for instance which preserve at least the order of , , , , so it seems that an argument using this method would be quite complicated... I managed to prove that there doesn't exist a section of for any if , but the proof doesn't work if .","r : GL_n(\mathbb{Z}/9\mathbb{Z}) \rightarrow GL_n(\mathbb{Z}/3\mathbb{Z}) i : GL_n(\mathbb{Z}/3\mathbb{Z}) \rightarrow GL_n(\mathbb{Z}/9\mathbb{Z}) r \circ i = id n = 2 n \geq 3 GL_n(\mathbb{Z}/3\mathbb{Z}) E P n n (i(E),i(P)) GL_n(\mathbb{Z}/9\mathbb{Z}) n=3 (i(E),i(P)) E P PE P^2E GL_n(\mathbb{Z}/p^2\mathbb{Z}) \rightarrow GL_n(\mathbb{Z}/p\mathbb{Z}) n \geq 2 p \geq 5 p =3","['linear-algebra', 'matrices', 'group-theory', 'finite-groups', 'finite-fields']"
96,Convergence of a linear recurrence equation,Convergence of a linear recurrence equation,,"Let $T \colon \mathbb{C}^n \to \mathbb{C}^n$ be a linear operator. Let $\{u_k\} \subset \mathbb{C}^n$ and $\{v_k\} \subset \mathbb{C}^n$ be two sequences of vectors. Suppose the spectral radius of $T$ is $\rho(T) = q <1$ but we don't know whether or not the spectral norm $\|T\|_2 = \sup_{\|x\|_2=1} \|Tx\|_2$ is smaller than $1$. I have a recurrence relation defined by \begin{align*} u_{k+1} = T u_k + \beta v_k, \end{align*} where we have the freedom to choose any fixed positive $\beta$. Furthermore, the $2$-norm of $v_k$ is bounded by \begin{align*} \|v_k\|_2 \le  \|u_k\|_2 +  \|u_{k-1}\|_2, \end{align*} The problem I am considering is to choose $\beta$ as large as possible but still guarantee $u_k \to 0$. I have an argument to show there exists some $\beta > 0$ such that $\|u_k\|_2 \to 0$. This argument is somehow involved and the choice of $\beta$ involves the condition number of $S$ where $S^{-1} J S = T$ is the Jordan decomposition. The argument goes as:    We know for any fixed $\varepsilon > 0$ we can define some vector norm $\|\cdot\|_v$ on $\mathbb{C}^n$ such that the induced operator norm of $T$ is $\|T\|_{op,v} = q + \varepsilon$. Then taking $\|\cdot\|_v$ norm on both sides of the recurrence, we get   \begin{align*} \|u_{k+1}\|_v \le (q+\varepsilon) \|u_k\|_v + \beta \|v_k\|_v. \end{align*}   By norm equivalence, we can bound $\|v_k\|_v \le a \|u_k\|_v + a \|u_{k-1}\|_v$ for some positive constant $a$. Now we have the second order recurrence $\|u_{k+1}\|_v \le (q+ \varepsilon+\beta a) \|u_k\|_v + \beta a \|u_{k-1}\|_v$, which can be solved explicitly. If $\beta$ is sufficiently small, $u_k \to 0$ can be guaranteed. To choose largest $\beta$ with this guarantee, I need the constants $c_1, c_2$ in $c_1\|\cdot\|_2 \le \|\cdot\|_v \le c_2 \|\cdot\|_2$, which is essentially the condition number of $S$. I am wondering whether there are more direct ways to my problem. I also considered expanding the recurrence \begin{align*} u_{k+1} = T^k u_1 + \beta( v_k + Tv_{k-1} + \dots + T^{k-1} v_1). \end{align*} Then we can apply the fact: for any matrix norm, $\|T^k\| \le c (\rho(T) + \varepsilon)^k$ for every $\varepsilon > 0$ where $c$ is some constant. By taking $2$-norm on both sides, we can avoid what I have done. But I haven't found a way to estimate the sum of $T^{k-j}v_j$ terms. Each sum is defined in a recursive manner with respect to $u_k$ and $u_{k-1}$.","Let $T \colon \mathbb{C}^n \to \mathbb{C}^n$ be a linear operator. Let $\{u_k\} \subset \mathbb{C}^n$ and $\{v_k\} \subset \mathbb{C}^n$ be two sequences of vectors. Suppose the spectral radius of $T$ is $\rho(T) = q <1$ but we don't know whether or not the spectral norm $\|T\|_2 = \sup_{\|x\|_2=1} \|Tx\|_2$ is smaller than $1$. I have a recurrence relation defined by \begin{align*} u_{k+1} = T u_k + \beta v_k, \end{align*} where we have the freedom to choose any fixed positive $\beta$. Furthermore, the $2$-norm of $v_k$ is bounded by \begin{align*} \|v_k\|_2 \le  \|u_k\|_2 +  \|u_{k-1}\|_2, \end{align*} The problem I am considering is to choose $\beta$ as large as possible but still guarantee $u_k \to 0$. I have an argument to show there exists some $\beta > 0$ such that $\|u_k\|_2 \to 0$. This argument is somehow involved and the choice of $\beta$ involves the condition number of $S$ where $S^{-1} J S = T$ is the Jordan decomposition. The argument goes as:    We know for any fixed $\varepsilon > 0$ we can define some vector norm $\|\cdot\|_v$ on $\mathbb{C}^n$ such that the induced operator norm of $T$ is $\|T\|_{op,v} = q + \varepsilon$. Then taking $\|\cdot\|_v$ norm on both sides of the recurrence, we get   \begin{align*} \|u_{k+1}\|_v \le (q+\varepsilon) \|u_k\|_v + \beta \|v_k\|_v. \end{align*}   By norm equivalence, we can bound $\|v_k\|_v \le a \|u_k\|_v + a \|u_{k-1}\|_v$ for some positive constant $a$. Now we have the second order recurrence $\|u_{k+1}\|_v \le (q+ \varepsilon+\beta a) \|u_k\|_v + \beta a \|u_{k-1}\|_v$, which can be solved explicitly. If $\beta$ is sufficiently small, $u_k \to 0$ can be guaranteed. To choose largest $\beta$ with this guarantee, I need the constants $c_1, c_2$ in $c_1\|\cdot\|_2 \le \|\cdot\|_v \le c_2 \|\cdot\|_2$, which is essentially the condition number of $S$. I am wondering whether there are more direct ways to my problem. I also considered expanding the recurrence \begin{align*} u_{k+1} = T^k u_1 + \beta( v_k + Tv_{k-1} + \dots + T^{k-1} v_1). \end{align*} Then we can apply the fact: for any matrix norm, $\|T^k\| \le c (\rho(T) + \varepsilon)^k$ for every $\varepsilon > 0$ where $c$ is some constant. By taking $2$-norm on both sides, we can avoid what I have done. But I haven't found a way to estimate the sum of $T^{k-j}v_j$ terms. Each sum is defined in a recursive manner with respect to $u_k$ and $u_{k-1}$.",,"['linear-algebra', 'convergence-divergence', 'recurrence-relations', 'rate-of-convergence']"
97,"Questions on color theory, expressed in linear algebra","Questions on color theory, expressed in linear algebra",,"I'm reading into color theory and there were a few questions which I asked myself along the way, maybe you can put me forward to some source where I can find answers or give them directly. The following paragraph contains some context, I'll try to summarize everything in purely mathematical terms below. The human eye has three main types of color reception cells (one for reddish light, one for greenish light and one for blueish light).  So every source of light (monochromatic, i.e. single wavelength, or not) induces a state in those cells, which can be described as a three tuple of non-negative, real numbers and it is assumed that this transition works semi-linearly, i.e. adding two sources of light induces the sum of the states induced by the single sources of light and same with multiplying by non-negative scalars.  I call this transition $T:S\rightarrow{\bf R}^3_{\ge 0}$ where $S$ is the space of spectral power distributions.  In the 1930s, as far as I understood, they didn't know exactly how $T$ looked like, so in order to build up a color space in accordance with human perception, they fixed three monochromatic sources of reg, green and blue light (they chose those wavelengths which were most easy to reproduce physically) and let a user mix it together in order to match the color of an arbitrary given monochromatic source of light.  This was not always possible, so they allowed the user to choose negative values for the primaries, which meant that instead of adding the wavelength to the mixture, it was added to the given source.  They succeeded to match every wavelength in the visible spectrum and took those values as a base of a first color space.  Again you get a semi-linear transition $T':S\rightarrow{\bf R}^3$ but this time non-negative values are included, yet it's not surjective on ${\bf R}^3$ .  What they did then was to choose a linear isomorphism which takes $T'(S)$ to ${\bf R}^3_{\ge 0}$ , so they can describe colors using non-negative values only. So in pure terms, there is the semi-linear space $S$ of spectral power distributions (which one can safely think as ${\bf R}^n_{\ge 0}$ for some large $n$ ) and a semi-linear transformation $T:S\rightarrow{\bf R}^3_{\ge 0}$ which is in general not known.  Also there are 'wavelengths' $p_1,p_2,p_3\in S$ and a semi-linear transformation $T':S\rightarrow{\bf R}^3$ such that for every $s\in S$ we have, denoting ${\sf max}(x,0)$ by $x^+$ and $(-x)^+$ by $x^-$ , the following: \begin{equation*}\tag{*}T(\sum_i T'(s)^+_i p_i)=T(s+\sum_i T'(s)^-_i p_i)\end{equation*} My questions related to this: Can we derive $T$ (or some of its properties) from $T'$ and (*)? Assuming we know $T$ , are there other triples of wavelengths which make this procedure possible, and if yes, What are their properties purely in terms of $T$ ? How does $T'(S)$ look like (geometrically) and How should it look like so we can find a linear isomorphism taking it to ${\bf R}^3_{\ge 0}$ ? My thoughts so far: (1) Here it helps to think the codomain of $T$ as whole ${\bf R}^3$ , then one can easily derive $T(s)=\sum_i T'(s)_i T(p_i)$ or equivalently $T=M\circ T'$ where $M$ is the $3\times 3$ -matrix $(T(p_1),T(p_2),T(p_3))$ .  If $T(S)$ is $3$ -dimensional, i.e. the set is not contained in a $2$ -dimensional subspace (which I assume), then $M$ is full rank, hence $T'$ is indeed semi-linear and we have that vice versa every full rank matrix $M$ with $M(T'(S))\subseteq{\bf R}^3_{\ge 0}$ produces a permissible $T$ which satisfies (*) via $T=M\circ T'$ . (2), (3) Constructing $T'$ should be possible for every triplet $p_1,p_2,p_3$ such that the $T(p_i)$ are linearly independent. (4) I realized that the semi-linearly closed subsets of ${\bf R}^n$ equal exactly the convex subsets which are closed under non-negative scalar multiplication.  For semi-linearly closed subsets of ${\bf R}^n_{\ge 0}$ these correspond exactly with the convex subsets of the $(n-1)$ -simplex (via projection along the semi-rays through the origin). (5) My intuition is that for a semi-linearly closed and topologically closed subset $A\subseteq{\bf R}^n$ , there exists a linear isomorphism taking it to ${\bf R}^n_{\ge 0}$ if and only if and only if there is no point $x\neq 0$ with $x,-x\in A$ . If someone can confirm my thoughts, they are encouraged to put them in an answer!","I'm reading into color theory and there were a few questions which I asked myself along the way, maybe you can put me forward to some source where I can find answers or give them directly. The following paragraph contains some context, I'll try to summarize everything in purely mathematical terms below. The human eye has three main types of color reception cells (one for reddish light, one for greenish light and one for blueish light).  So every source of light (monochromatic, i.e. single wavelength, or not) induces a state in those cells, which can be described as a three tuple of non-negative, real numbers and it is assumed that this transition works semi-linearly, i.e. adding two sources of light induces the sum of the states induced by the single sources of light and same with multiplying by non-negative scalars.  I call this transition where is the space of spectral power distributions.  In the 1930s, as far as I understood, they didn't know exactly how looked like, so in order to build up a color space in accordance with human perception, they fixed three monochromatic sources of reg, green and blue light (they chose those wavelengths which were most easy to reproduce physically) and let a user mix it together in order to match the color of an arbitrary given monochromatic source of light.  This was not always possible, so they allowed the user to choose negative values for the primaries, which meant that instead of adding the wavelength to the mixture, it was added to the given source.  They succeeded to match every wavelength in the visible spectrum and took those values as a base of a first color space.  Again you get a semi-linear transition but this time non-negative values are included, yet it's not surjective on .  What they did then was to choose a linear isomorphism which takes to , so they can describe colors using non-negative values only. So in pure terms, there is the semi-linear space of spectral power distributions (which one can safely think as for some large ) and a semi-linear transformation which is in general not known.  Also there are 'wavelengths' and a semi-linear transformation such that for every we have, denoting by and by , the following: My questions related to this: Can we derive (or some of its properties) from and (*)? Assuming we know , are there other triples of wavelengths which make this procedure possible, and if yes, What are their properties purely in terms of ? How does look like (geometrically) and How should it look like so we can find a linear isomorphism taking it to ? My thoughts so far: (1) Here it helps to think the codomain of as whole , then one can easily derive or equivalently where is the -matrix .  If is -dimensional, i.e. the set is not contained in a -dimensional subspace (which I assume), then is full rank, hence is indeed semi-linear and we have that vice versa every full rank matrix with produces a permissible which satisfies (*) via . (2), (3) Constructing should be possible for every triplet such that the are linearly independent. (4) I realized that the semi-linearly closed subsets of equal exactly the convex subsets which are closed under non-negative scalar multiplication.  For semi-linearly closed subsets of these correspond exactly with the convex subsets of the -simplex (via projection along the semi-rays through the origin). (5) My intuition is that for a semi-linearly closed and topologically closed subset , there exists a linear isomorphism taking it to if and only if and only if there is no point with . If someone can confirm my thoughts, they are encouraged to put them in an answer!","T:S\rightarrow{\bf R}^3_{\ge 0} S T T':S\rightarrow{\bf R}^3 {\bf R}^3 T'(S) {\bf R}^3_{\ge 0} S {\bf R}^n_{\ge 0} n T:S\rightarrow{\bf R}^3_{\ge 0} p_1,p_2,p_3\in S T':S\rightarrow{\bf R}^3 s\in S {\sf max}(x,0) x^+ (-x)^+ x^- \begin{equation*}\tag{*}T(\sum_i T'(s)^+_i p_i)=T(s+\sum_i T'(s)^-_i p_i)\end{equation*} T T' T T T'(S) {\bf R}^3_{\ge 0} T {\bf R}^3 T(s)=\sum_i T'(s)_i T(p_i) T=M\circ T' M 3\times 3 (T(p_1),T(p_2),T(p_3)) T(S) 3 2 M T' M M(T'(S))\subseteq{\bf R}^3_{\ge 0} T T=M\circ T' T' p_1,p_2,p_3 T(p_i) {\bf R}^n {\bf R}^n_{\ge 0} (n-1) A\subseteq{\bf R}^n {\bf R}^n_{\ge 0} x\neq 0 x,-x\in A","['linear-algebra', 'reference-request', 'linear-transformations', 'physics', 'book-recommendation']"
98,Physical or geometric meaning of the trace of a matrix,Physical or geometric meaning of the trace of a matrix,,"The geometric meaning of the determinant of a matrix as an area or a volume is dealt with in many textbooks. However, I don't know if the trace of a matrix has a geometric meaning too. Is there any geometric or physical (intuitive) significance related to   the trace of a matrix?","The geometric meaning of the determinant of a matrix as an area or a volume is dealt with in many textbooks. However, I don't know if the trace of a matrix has a geometric meaning too. Is there any geometric or physical (intuitive) significance related to   the trace of a matrix?",,"['linear-algebra', 'matrices', 'geometry', 'soft-question', 'physics']"
99,How to compute the determinant of this Toeplitz matrix?,How to compute the determinant of this Toeplitz matrix?,,"Given a positive integer $n$, express$$ f_n(x) = \left|\begin{array}{c c c c c}  1 & x & \cdots & x^{n - 1} & x^n\\ x & 1 & x & \cdots & x^{n - 1} \\ \vdots & x & \ddots & \ddots & \vdots\\ x^{n - 1} & \vdots & \ddots & 1 & x\\ x^n & x^{n - 1} & \cdots & x & 1 \end{array}\right| $$   as a polynomial of $x$. I tried to find a recurrence relation of $\{f_n\}_{n \geqslant 1}$ using Laplace expansion, but there seems to be no patterns in the minors in the expansion. Is there a somewhat simple recurrence relation of $\{f_n\}_{n \geqslant 1}$ or these determinants can be computed with other methods?","Given a positive integer $n$, express$$ f_n(x) = \left|\begin{array}{c c c c c}  1 & x & \cdots & x^{n - 1} & x^n\\ x & 1 & x & \cdots & x^{n - 1} \\ \vdots & x & \ddots & \ddots & \vdots\\ x^{n - 1} & \vdots & \ddots & 1 & x\\ x^n & x^{n - 1} & \cdots & x & 1 \end{array}\right| $$   as a polynomial of $x$. I tried to find a recurrence relation of $\{f_n\}_{n \geqslant 1}$ using Laplace expansion, but there seems to be no patterns in the minors in the expansion. Is there a somewhat simple recurrence relation of $\{f_n\}_{n \geqslant 1}$ or these determinants can be computed with other methods?",,"['linear-algebra', 'matrices', 'determinant', 'laplace-expansion', 'toeplitz-matrices']"
