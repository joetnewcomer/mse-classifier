,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,If $X\succ Y$ then is $X^{-1}\prec Y^{-1}$?,If  then is ?,X\succ Y X^{-1}\prec Y^{-1},"Let $X$ and $Y$ be symmetric positive definite matrices. Suppose that $X-Y$ is positive semidefinite. Does it follow that $Y^{-1}- X^{-1}$ is also positive semidefinite? If $X$ and $Y$ are simultaneously diagonalizable, then we may diagonalize both of them to get the result. After playing around with this a bit, I am starting to think that the answer is ""no"". But I didn't immediately see how to construct a counterexample. For example, it seems like there is no $2\times 2$ counterexample.","Let $X$ and $Y$ be symmetric positive definite matrices. Suppose that $X-Y$ is positive semidefinite. Does it follow that $Y^{-1}- X^{-1}$ is also positive semidefinite? If $X$ and $Y$ are simultaneously diagonalizable, then we may diagonalize both of them to get the result. After playing around with this a bit, I am starting to think that the answer is ""no"". But I didn't immediately see how to construct a counterexample. For example, it seems like there is no $2\times 2$ counterexample.",,"['matrices', 'inequality', 'positive-definite']"
1,Show that matrix $M$ is not orthogonal if it contains column of all ones.,Show that matrix  is not orthogonal if it contains column of all ones.,M,"Original problem: We have invertible matrix $M$ dimension $n$ over the $\mathbb F_2$. If $M$ contains column of all ones, then there exists two different rows $V,W$ of $M$ such that: $$<V,W>=\sum_{i}v_iw_i=1. $$ After sometime and with the help of my friend we find out that the problem is equal to the $$M*M^T\neq I,$$ where $I$ is the identical matrix. And this problem can be solved by solving this one: We have invertible matrix $M$ dimension $n$ over the $\mathbb F_2$. If $M$ contains column of all ones, then it is not orthogonal matrix. With $\mathbb F_2$ we denote the field with two elements: zero and one. Every suggestions and ideas are welcome. Edit: It's all ok when $n$ is even, but the odd $n$'s are problem.","Original problem: We have invertible matrix $M$ dimension $n$ over the $\mathbb F_2$. If $M$ contains column of all ones, then there exists two different rows $V,W$ of $M$ such that: $$<V,W>=\sum_{i}v_iw_i=1. $$ After sometime and with the help of my friend we find out that the problem is equal to the $$M*M^T\neq I,$$ where $I$ is the identical matrix. And this problem can be solved by solving this one: We have invertible matrix $M$ dimension $n$ over the $\mathbb F_2$. If $M$ contains column of all ones, then it is not orthogonal matrix. With $\mathbb F_2$ we denote the field with two elements: zero and one. Every suggestions and ideas are welcome. Edit: It's all ok when $n$ is even, but the odd $n$'s are problem.",,"['linear-algebra', 'matrices']"
2,Show that if $\nabla { {\bf{u}}}+(\nabla { {\bf{u}})^T}=0$ then $\bf{u}(x)=\bf{a}\times \bf{x}+\bf{b}$,Show that if  then,\nabla { {\bf{u}}}+(\nabla { {\bf{u}})^T}=0 \bf{u}(x)=\bf{a}\times \bf{x}+\bf{b},"I am working on this problem: Prove that if a smooth($C^1$) function $\bf{u}$ satisfies $\nabla { {\bf{u}}}+(\nabla { {\bf{u}})^T}=0$ then there exist constant vectors ${\bf{a},\bf{b}}\subset \Bbb R^3$, such that $\bf{u}(x)=\bf{a}\times \bf{x}+\bf{b}$, where $\bf{x} =<x_1,x_2,x_3>$. I really have no clue about this problem and I am not familiar with matrix calculations. So I just tried to verify that $\bf{u}(x)=\bf{a}\times \bf{x}+\bf{b}$ actually satisfy $\nabla { {\bf{u}}}+(\nabla { {\bf{u}})^T}=0$ . I tried to calculate $\nabla ({ {\bf{\bf{a}\times \bf{x}+\bf{b}}}}）$ and get $\bf{a}\times \nabla {\bf{x}}$. Then if I calculate $(\nabla ({ {\bf{\bf{a}\times \bf{x}+\bf{b}}}})^T$, is the result $(\bf{a}\times \nabla {\bf{x}})^T$?  It doesn't seem right to me, since I cannot conclude $\bf{a}\times \nabla {\bf{x}}+(\bf{a}\times \nabla {\bf{x}})^T=0$. Could anyone kindly provide some help? For example, some reference or theorem can be used here? Thanks very much！","I am working on this problem: Prove that if a smooth($C^1$) function $\bf{u}$ satisfies $\nabla { {\bf{u}}}+(\nabla { {\bf{u}})^T}=0$ then there exist constant vectors ${\bf{a},\bf{b}}\subset \Bbb R^3$, such that $\bf{u}(x)=\bf{a}\times \bf{x}+\bf{b}$, where $\bf{x} =<x_1,x_2,x_3>$. I really have no clue about this problem and I am not familiar with matrix calculations. So I just tried to verify that $\bf{u}(x)=\bf{a}\times \bf{x}+\bf{b}$ actually satisfy $\nabla { {\bf{u}}}+(\nabla { {\bf{u}})^T}=0$ . I tried to calculate $\nabla ({ {\bf{\bf{a}\times \bf{x}+\bf{b}}}}）$ and get $\bf{a}\times \nabla {\bf{x}}$. Then if I calculate $(\nabla ({ {\bf{\bf{a}\times \bf{x}+\bf{b}}}})^T$, is the result $(\bf{a}\times \nabla {\bf{x}})^T$?  It doesn't seem right to me, since I cannot conclude $\bf{a}\times \nabla {\bf{x}}+(\bf{a}\times \nabla {\bf{x}})^T=0$. Could anyone kindly provide some help? For example, some reference or theorem can be used here? Thanks very much！",,"['calculus', 'linear-algebra', 'matrices']"
3,$n \times n$ matrices galore.,matrices galore.,n \times n,"The group of $n \times n$ invertible matrices with entries in $\mathbb{R}$ is denoted $\text{GL}_n(\mathbb{R})$. Similarly, $\text{GL}_n(\mathbb{C})$ denotes the group of $n \times n$ invertible matrices with complex entries. Consider the following sets of matrices: $\text{SL}_n(\mathbb{R}) = \{M \in \text{GL}_n(\mathbb{R}) : \det(M) = 1\}$; $\text{SL}_n(\mathbb{C}) = \{M \in \text{GL}_n(\mathbb{C}) : \det(M) = 1\}$; $\text{O}_n(\mathbb{R}) = \{M \in \text{GL}_n(\mathbb{R}) : MM^\text{T} = M^\text{T}M = I_n\}$; $\text{SO}_n(\mathbb{R}) = \{M \in \text{O}_n(\mathbb{R}) : \det(M) = 1\}$; $\text{U}_n(\mathbb{C}) = \{M \in \text{GL}_n(\mathbb{C}) : MM^\dagger = M^\dagger M = I_n\}$; $\text{SU}_n(\mathbb{C}) = \{M \in \text{U}_n(\mathbb{C}) : \det(M) = 1\}$. Here $I_n$ stands for the $n \times n$ identity matrix, $M^\text{T}$ is the transpose of $M$, $M^\dagger$ is the conjugate transpose of $M$, and $\det(M)$ denotes the determinant of $M$. Find all possible inclusions among these sets, and prove that in every case the smaller set is a subgroup of the larger one.","The group of $n \times n$ invertible matrices with entries in $\mathbb{R}$ is denoted $\text{GL}_n(\mathbb{R})$. Similarly, $\text{GL}_n(\mathbb{C})$ denotes the group of $n \times n$ invertible matrices with complex entries. Consider the following sets of matrices: $\text{SL}_n(\mathbb{R}) = \{M \in \text{GL}_n(\mathbb{R}) : \det(M) = 1\}$; $\text{SL}_n(\mathbb{C}) = \{M \in \text{GL}_n(\mathbb{C}) : \det(M) = 1\}$; $\text{O}_n(\mathbb{R}) = \{M \in \text{GL}_n(\mathbb{R}) : MM^\text{T} = M^\text{T}M = I_n\}$; $\text{SO}_n(\mathbb{R}) = \{M \in \text{O}_n(\mathbb{R}) : \det(M) = 1\}$; $\text{U}_n(\mathbb{C}) = \{M \in \text{GL}_n(\mathbb{C}) : MM^\dagger = M^\dagger M = I_n\}$; $\text{SU}_n(\mathbb{C}) = \{M \in \text{U}_n(\mathbb{C}) : \det(M) = 1\}$. Here $I_n$ stands for the $n \times n$ identity matrix, $M^\text{T}$ is the transpose of $M$, $M^\dagger$ is the conjugate transpose of $M$, and $\det(M)$ denotes the determinant of $M$. Find all possible inclusions among these sets, and prove that in every case the smaller set is a subgroup of the larger one.",,"['linear-algebra', 'abstract-algebra']"
4,"Exist basis, simultaneously upper-triangular?","Exist basis, simultaneously upper-triangular?",,"Let $A, B \in M_n(\mathbb{C})$ be such that $\text{rank}(AB - BA) \le 1$. Does there exist a basis of $\mathbb{C}^n$ with respect to which $A$ and $B$ are simultaneously upper-triangular?","Let $A, B \in M_n(\mathbb{C})$ be such that $\text{rank}(AB - BA) \le 1$. Does there exist a basis of $\mathbb{C}^n$ with respect to which $A$ and $B$ are simultaneously upper-triangular?",,"['linear-algebra', 'matrices']"
5,Number of solutions in system of linear equations,Number of solutions in system of linear equations,,"I'm studying System of linear equations. When solving Ax=b , it is said that the system can behave in 3 ways. No solution Unique solution Infinitely many solutions And according to Wikipedia page, Usually, a system with fewer equations than unknowns has infinitely many solutions, but it may have no solution . Such a system is known as an underdetermined system. Usually, a system with the same number of equations and unknowns has a single unique solution . Usually, a system with more equations than unknowns has no solution . Such a system is also known as an overdetermined system. I think the word Usually implies that the system can behaves in 3 ways for any arbitrary matrices(it does not care the matrix is a square matrix or non-square matrix). I want to know the possible behaviors when the matrix A is non-square. My questions are Can non-square matrices(m < n) have a unique solution(unknowns > equations)? Can non-square matrices(m > n) have infinitely many solutions(unknowns < equations)?","I'm studying System of linear equations. When solving Ax=b , it is said that the system can behave in 3 ways. No solution Unique solution Infinitely many solutions And according to Wikipedia page, Usually, a system with fewer equations than unknowns has infinitely many solutions, but it may have no solution . Such a system is known as an underdetermined system. Usually, a system with the same number of equations and unknowns has a single unique solution . Usually, a system with more equations than unknowns has no solution . Such a system is also known as an overdetermined system. I think the word Usually implies that the system can behaves in 3 ways for any arbitrary matrices(it does not care the matrix is a square matrix or non-square matrix). I want to know the possible behaviors when the matrix A is non-square. My questions are Can non-square matrices(m < n) have a unique solution(unknowns > equations)? Can non-square matrices(m > n) have infinitely many solutions(unknowns < equations)?",,"['matrices', 'systems-of-equations']"
6,What is the necessary and sufficient condition when an $n \times n$ matrix over $\mathbb{Z} $ is invertible over $ \mathbb{Z}$,What is the necessary and sufficient condition when an  matrix over  is invertible over,n \times n \mathbb{Z}   \mathbb{Z},I read a proposition on the textbook. It says that an $n \times n$ matrix over $\mathbb{Z}$  is invertible over $\mathbb{Z}$ if and only if it can be expressed as a product of elementary matrices all of which are over $\mathbb{Z}$. The book left the proof to readers. But I do not think that the proposition holds. For example when matrix $A = \left( \begin{array}{cc}2 & 0\\ 0 & 1 \end{array}\right)$. It is elementary over $\mathbb{Z}$. But $inv(A) = \left( \begin{array}{cc}\frac{1}{2} & 0\\ 0 & 1 \end{array} \right)$ which is not over $\mathbb{Z}$. Guys where did I make the mistake?,I read a proposition on the textbook. It says that an $n \times n$ matrix over $\mathbb{Z}$  is invertible over $\mathbb{Z}$ if and only if it can be expressed as a product of elementary matrices all of which are over $\mathbb{Z}$. The book left the proof to readers. But I do not think that the proposition holds. For example when matrix $A = \left( \begin{array}{cc}2 & 0\\ 0 & 1 \end{array}\right)$. It is elementary over $\mathbb{Z}$. But $inv(A) = \left( \begin{array}{cc}\frac{1}{2} & 0\\ 0 & 1 \end{array} \right)$ which is not over $\mathbb{Z}$. Guys where did I make the mistake?,,"['linear-algebra', 'matrices']"
7,Sum of identity and idempotent (projection) matrix,Sum of identity and idempotent (projection) matrix,,"Let $P$ be an idempotent $n \times n$ matrix ($P^2 = P$). What is $(I + P)^{-1}$? I've been thinking about this problem for a while, but can't find an answer. I tried a few examples, but I'm not sure what the general pattern is.","Let $P$ be an idempotent $n \times n$ matrix ($P^2 = P$). What is $(I + P)^{-1}$? I've been thinking about this problem for a while, but can't find an answer. I tried a few examples, but I'm not sure what the general pattern is.",,"['linear-algebra', 'matrices']"
8,Cycles in the Fibonacci Sequence mod n with matrices,Cycles in the Fibonacci Sequence mod n with matrices,,"I was just looking at this question about Fibonacci sequence cycles modulo 5, and I happened to see a very nice solution that involved using matrices.  Using the matrix representation of the Fibonacci sequence, one can reduce the problem of finding cycles modulo $n$ to a certain problem concerning matrices.  Thus, my question is such: Given an integer $n$, does there exist an integer $x$ such that \begin{align} \begin{bmatrix} 1 & 1 \\ 1 & 0  \end{bmatrix}^x \equiv \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \pmod n \end{align} Surely there are some values of $n$ that have no associated $x$ satisfying the above equation.  In general, what values of $n$ have an associated $x$ that satisfies the above equation.  If you haven't looked at the linked answer, my question relates to the Fibonacci sequence in the following way:  If there exists an integer $n$ and an integer $x$ that satisfy the equation above, then the fibonacci sequence $ \pmod n$ repeats every $x$ terms.  That is,  $F_{i}\equiv F_{i+x}\pmod n$ To reiterate, can we characterize those integers $n$ for which there exists an associated integer $x$ that satisfies the matrix equation above? For $n=5$ we have the $x$ value of 20. For $n=6$ we have the $x$ value of 24. I am very interested to know if there are infinitely many solutions like this, or perhaps there is some largest $n$?  (I doubt the latter)  I am also interested to know what sorts of techniques can be used to solve the above matrix equation.  As always, please leave a comment to let me know if I messed something up or was unclear anywhere.","I was just looking at this question about Fibonacci sequence cycles modulo 5, and I happened to see a very nice solution that involved using matrices.  Using the matrix representation of the Fibonacci sequence, one can reduce the problem of finding cycles modulo $n$ to a certain problem concerning matrices.  Thus, my question is such: Given an integer $n$, does there exist an integer $x$ such that \begin{align} \begin{bmatrix} 1 & 1 \\ 1 & 0  \end{bmatrix}^x \equiv \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \pmod n \end{align} Surely there are some values of $n$ that have no associated $x$ satisfying the above equation.  In general, what values of $n$ have an associated $x$ that satisfies the above equation.  If you haven't looked at the linked answer, my question relates to the Fibonacci sequence in the following way:  If there exists an integer $n$ and an integer $x$ that satisfy the equation above, then the fibonacci sequence $ \pmod n$ repeats every $x$ terms.  That is,  $F_{i}\equiv F_{i+x}\pmod n$ To reiterate, can we characterize those integers $n$ for which there exists an associated integer $x$ that satisfies the matrix equation above? For $n=5$ we have the $x$ value of 20. For $n=6$ we have the $x$ value of 24. I am very interested to know if there are infinitely many solutions like this, or perhaps there is some largest $n$?  (I doubt the latter)  I am also interested to know what sorts of techniques can be used to solve the above matrix equation.  As always, please leave a comment to let me know if I messed something up or was unclear anywhere.",,"['matrices', 'modular-arithmetic', 'fibonacci-numbers']"
9,"An ""apparent"" contradiction for eigenvalues signs of $A=\left( \begin{array}{cc} a & -a \\ a-1 & 1-a \\ \end{array} \right)$.","An ""apparent"" contradiction for eigenvalues signs of .",A=\left( \begin{array}{cc} a & -a \\ a-1 & 1-a \\ \end{array} \right),"The following matrix $$A=\left(   \begin{array}{cc}     a & -a \\     a-1 & 1-a \\   \end{array} \right)$$ has eigenvalues $\lambda=0,1$ $\forall a\in\mathbb R$. Therefore $\lambda\geq 0$. So I would expect that $$\lambda=\frac{\vec x^T A \vec x}{\vec x^T \vec x}\geq 0, \ \ \forall \vec x=(x,y)\in\mathbb R^2.$$ Namely, $$\vec x^T A \vec x=x^2 a-xy+y^2(1-a), \ \ \ (1)$$ must be $\geq 0$. In order to study sign of $(1)$, I used the quadratic form theory, i.e. I study the associated symmetric matrix of $(1)$: $$\tilde A=\left(   \begin{array}{cc}     a & -\frac{1}{2} \\     -\frac{1}{2} & 1-a \\   \end{array} \right)$$ but it isn't a positive semi-definite matrix! Someone help me to explain this ""apparent"" contradiction, please?","The following matrix $$A=\left(   \begin{array}{cc}     a & -a \\     a-1 & 1-a \\   \end{array} \right)$$ has eigenvalues $\lambda=0,1$ $\forall a\in\mathbb R$. Therefore $\lambda\geq 0$. So I would expect that $$\lambda=\frac{\vec x^T A \vec x}{\vec x^T \vec x}\geq 0, \ \ \forall \vec x=(x,y)\in\mathbb R^2.$$ Namely, $$\vec x^T A \vec x=x^2 a-xy+y^2(1-a), \ \ \ (1)$$ must be $\geq 0$. In order to study sign of $(1)$, I used the quadratic form theory, i.e. I study the associated symmetric matrix of $(1)$: $$\tilde A=\left(   \begin{array}{cc}     a & -\frac{1}{2} \\     -\frac{1}{2} & 1-a \\   \end{array} \right)$$ but it isn't a positive semi-definite matrix! Someone help me to explain this ""apparent"" contradiction, please?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
10,Determinant of a matrix with binomial coefficients.,Determinant of a matrix with binomial coefficients.,,"Let $n \in\mathbb{N}$ and $A=(a_{ij})$ where  \begin{equation}a_{ij}=\binom{i+j}{i}\end{equation} for $0\leq i,j \leq n$. Show that $A$ has an inverse and that every element of $A^{-1}$ is an integer. I have shown that this $n\times n$ matrix is symmetric since,  \begin{equation} \binom{i+j}{i}=\binom{i+j}{j} \end{equation}  in order to try to get a nonzero determinant.  But i'm stuck in this step, suggestions would be appreciated.","Let $n \in\mathbb{N}$ and $A=(a_{ij})$ where  \begin{equation}a_{ij}=\binom{i+j}{i}\end{equation} for $0\leq i,j \leq n$. Show that $A$ has an inverse and that every element of $A^{-1}$ is an integer. I have shown that this $n\times n$ matrix is symmetric since,  \begin{equation} \binom{i+j}{i}=\binom{i+j}{j} \end{equation}  in order to try to get a nonzero determinant.  But i'm stuck in this step, suggestions would be appreciated.",,"['linear-algebra', 'matrices', 'binomial-coefficients', 'problem-solving']"
11,Is having $G^N = I$ enough to a matrix $G$ orthogonal?,Is having  enough to a matrix  orthogonal?,G^N = I G,"I have a matrix $G$ that is cyclic ($G^N = I$) and has determinant $\pm 1 $; is this enough to show that it is orthogonal? If not, what more could I add to make it so?","I have a matrix $G$ that is cyclic ($G^N = I$) and has determinant $\pm 1 $; is this enough to show that it is orthogonal? If not, what more could I add to make it so?",,"['linear-algebra', 'matrices', 'orthogonality']"
12,Lower bounding the eigenvalue of a matrix,Lower bounding the eigenvalue of a matrix,,"Suppose I have the following symmetric matrix  $$ A'=\begin{bmatrix} A + b b^T & b \\ b^T & 1 \end{bmatrix} $$ where $A$ is positive definite $n \times n$ symmetric matrix and $b$ is a $n \times 1$ vector. Suppose $\|b\|_2 \leq B_1$, and all eigenvalues of $A$ are between $[B_2, B_3]$. What is a bound in terms of $B_1$, $B_2$, and $B_3$ for the smallest eigenvalue of $A'$? (It is straight forward to show that $A'$ is positive definite.)","Suppose I have the following symmetric matrix  $$ A'=\begin{bmatrix} A + b b^T & b \\ b^T & 1 \end{bmatrix} $$ where $A$ is positive definite $n \times n$ symmetric matrix and $b$ is a $n \times 1$ vector. Suppose $\|b\|_2 \leq B_1$, and all eigenvalues of $A$ are between $[B_2, B_3]$. What is a bound in terms of $B_1$, $B_2$, and $B_3$ for the smallest eigenvalue of $A'$? (It is straight forward to show that $A'$ is positive definite.)",,"['matrices', 'eigenvalues-eigenvectors']"
13,Why does $D \det(I)B=\frac{d}{d t}\det(I+t B)|_{t=0}$ hold?,Why does  hold?,D \det(I)B=\frac{d}{d t}\det(I+t B)|_{t=0},"The following problem is from Taylor's PDE I. I do not get why $D \det(I)B=\frac{d}{d t}\det(I+t B)|_{t=0}$ hold. Since $\det(I)=1$ and $D$ is $n\times n$ matrix, the left side seems to be a matrix, while the right side is clearly a scalar. Could anyone tell me how I should correctly interpret $\det(I)$ in this case?","The following problem is from Taylor's PDE I. I do not get why $D \det(I)B=\frac{d}{d t}\det(I+t B)|_{t=0}$ hold. Since $\det(I)=1$ and $D$ is $n\times n$ matrix, the left side seems to be a matrix, while the right side is clearly a scalar. Could anyone tell me how I should correctly interpret $\det(I)$ in this case?",,"['matrices', 'partial-differential-equations']"
14,problem about symmetric positive semi-definite matrix,problem about symmetric positive semi-definite matrix,,"Let $A,B$ be symmetric positive semi-definite matrix with real entries  I have to show that $ Im(A) \subset Im(A+B)$ if $tr(AB)=0$ then $ AB=O $ I know that a symmetric matrix A is positive semi-definite iff any principle minors of A  $\geq 0$ (There are also other properties such as all eigenvalue is non negative ,for all $x\neq 0 ,x^t(A)x \geq 0$ etc.) also A+B is still symmetric positive semi-definite but I don't know how to apply this property to find $Im(A)$ and $Im(A+B)$  and for 2.  If I can show that A and B can be simultaneously diagonalized (I found that this is true? but cannot show it) then $X^{-1}AX=M ,X^{-1}BX=N$ for some diagonal matrix M,N with all entries nonnegative.This mean $X^{-1}ABX=MN$ therefore $0=tr(AB)=tr(MN) $but MN is diagonal matrix with all entries nonnegative so $MN=O$ so $AB=XMNX^{-1} =O$ Do I miss anything? Any ideas to complete this proof? and how to show 1.? Thanks for your help.","Let $A,B$ be symmetric positive semi-definite matrix with real entries  I have to show that $ Im(A) \subset Im(A+B)$ if $tr(AB)=0$ then $ AB=O $ I know that a symmetric matrix A is positive semi-definite iff any principle minors of A  $\geq 0$ (There are also other properties such as all eigenvalue is non negative ,for all $x\neq 0 ,x^t(A)x \geq 0$ etc.) also A+B is still symmetric positive semi-definite but I don't know how to apply this property to find $Im(A)$ and $Im(A+B)$  and for 2.  If I can show that A and B can be simultaneously diagonalized (I found that this is true? but cannot show it) then $X^{-1}AX=M ,X^{-1}BX=N$ for some diagonal matrix M,N with all entries nonnegative.This mean $X^{-1}ABX=MN$ therefore $0=tr(AB)=tr(MN) $but MN is diagonal matrix with all entries nonnegative so $MN=O$ so $AB=XMNX^{-1} =O$ Do I miss anything? Any ideas to complete this proof? and how to show 1.? Thanks for your help.",,"['linear-algebra', 'matrices', 'matrix-rank']"
15,"How can I prove that any matrix A can be expressed as the sum of two Hermitian matrices , B and C, in the form A = B + iC?","How can I prove that any matrix A can be expressed as the sum of two Hermitian matrices , B and C, in the form A = B + iC?",,"The question is in the title really.  Whether or not A must also be Hermitian is not clear to me.  Sorry, I am not very good with proofs of this nature.","The question is in the title really.  Whether or not A must also be Hermitian is not clear to me.  Sorry, I am not very good with proofs of this nature.",,"['linear-algebra', 'matrices']"
16,Dimension of set of Hermitian matrices commuting with a given matrix,Dimension of set of Hermitian matrices commuting with a given matrix,,"Given a Hermitian matrix $A$, what is the dimension of the set of all other Hermitian matrices $B$ such that $[A,B] = 0$. It is clearly not the same for all $A$, but how can one find it for a given $A$.","Given a Hermitian matrix $A$, what is the dimension of the set of all other Hermitian matrices $B$ such that $[A,B] = 0$. It is clearly not the same for all $A$, but how can one find it for a given $A$.",,"['matrices', 'lie-algebras']"
17,Matrix problem with inductive solution,Matrix problem with inductive solution,,"Yesterday I was at an interview and was given the following problem: Consider a matrix A that has dimensions NxM. Every element of the matrix is the average of its adjacent (up to 8) elements. Given that the element at position A[1][1]=1, find the element at A[N][M]. It was easy to notice that in such a matrix, all elements should be equal. Thus the element A[N][M]=1 . I proved it for a 2x2 matrix, by assigning x,y,z to the unknown elements, getting a system of 3 equations and solving it. The explanation to my answer A[N][M]=1 was that for any such NxM matrix you will get a system of n*m-1 equations with as many variables, which after solving will all be equal to the A[1][1] element. The interviewer requested an inductive solution. I claimed that this cannot be proved by induction as the P(n)(m) does not necessarily depend on P(n-1)(m-1). He proved it in the following steps. Prove it for the 2x2 matrix. (which I did) Assume that [N-1]x[M-1] is a matrix of all ones Enlarge the [N-1]x[M-1] matrix by one row and one column and start filling the final NxM matrix in the following way: Consider all the bottom and right 2x2 sub-matrices that include 2 known elements 1 and 2 unknown elements. As we had proved that 2x2 matrix can only have all ones, then the unknown elements should also be ones. Fill the Nth row and Mth column square by square to get the final matrix consisting of all 1s. I have doubts that this is a correct method of solving this problem. Could you share your opinions and tell me whether the inductive step is acceptable?","Yesterday I was at an interview and was given the following problem: Consider a matrix A that has dimensions NxM. Every element of the matrix is the average of its adjacent (up to 8) elements. Given that the element at position A[1][1]=1, find the element at A[N][M]. It was easy to notice that in such a matrix, all elements should be equal. Thus the element A[N][M]=1 . I proved it for a 2x2 matrix, by assigning x,y,z to the unknown elements, getting a system of 3 equations and solving it. The explanation to my answer A[N][M]=1 was that for any such NxM matrix you will get a system of n*m-1 equations with as many variables, which after solving will all be equal to the A[1][1] element. The interviewer requested an inductive solution. I claimed that this cannot be proved by induction as the P(n)(m) does not necessarily depend on P(n-1)(m-1). He proved it in the following steps. Prove it for the 2x2 matrix. (which I did) Assume that [N-1]x[M-1] is a matrix of all ones Enlarge the [N-1]x[M-1] matrix by one row and one column and start filling the final NxM matrix in the following way: Consider all the bottom and right 2x2 sub-matrices that include 2 known elements 1 and 2 unknown elements. As we had proved that 2x2 matrix can only have all ones, then the unknown elements should also be ones. Fill the Nth row and Mth column square by square to get the final matrix consisting of all 1s. I have doubts that this is a correct method of solving this problem. Could you share your opinions and tell me whether the inductive step is acceptable?",,"['matrices', 'induction']"
18,Kronecker product of two Hadamard matrices is Hadamard matrix,Kronecker product of two Hadamard matrices is Hadamard matrix,,"An $n\times n$ matrix $A$ is called a Hadamard matrix if all of its entries are $\pm 1$ and $AA^T=nI$. Show that the Kronecker product $A\otimes B$ of two $n \times n$ Hadamard matrices $A$ and $B$ is again a Hadamard matrix. This is an interesting result, but I don't know how to start with. It seems that I need to use induction on $n$? Any good idea?","An $n\times n$ matrix $A$ is called a Hadamard matrix if all of its entries are $\pm 1$ and $AA^T=nI$. Show that the Kronecker product $A\otimes B$ of two $n \times n$ Hadamard matrices $A$ and $B$ is again a Hadamard matrix. This is an interesting result, but I don't know how to start with. It seems that I need to use induction on $n$? Any good idea?",,"['matrices', 'hadamard-matrices']"
19,Convexity of Conjugate Function on Determinant,Convexity of Conjugate Function on Determinant,,"If $f: \mathbb R^n \to \mathbb R$ then $conj(f) : \mathbb R^n\to \mathbb R$ is defined as $conj(f)(y) = \sup_{x \in dom_f} (y^Tx - f(x))$ and it is called the conjugate function of $f$. if $f(X) = -\log(\det X)$ where $X$ is a symmetric positive definite matrix, what is the domain of $conj(f)$ and how can we show that it is convex? ($Y^TX$ is defined as $trace(YX)$).","If $f: \mathbb R^n \to \mathbb R$ then $conj(f) : \mathbb R^n\to \mathbb R$ is defined as $conj(f)(y) = \sup_{x \in dom_f} (y^Tx - f(x))$ and it is called the conjugate function of $f$. if $f(X) = -\log(\det X)$ where $X$ is a symmetric positive definite matrix, what is the domain of $conj(f)$ and how can we show that it is convex? ($Y^TX$ is defined as $trace(YX)$).",,"['matrices', 'functions', 'convex-optimization']"
20,Can I know all the elements of a matrix given that I know its sum along one dimension and the fact that it is axisymmetric?,Can I know all the elements of a matrix given that I know its sum along one dimension and the fact that it is axisymmetric?,,"For this discussion I will assume a 9x9 matrix but my question is for a general nxn matrix. I have a matrix which is not only symmetric along the vertical and the horizontal axis, but is axisymmetric about its center. So for a 9x9 matrix I have the following set of constraints: $$x_{i,j} = x_{k,l}$$ if $$ (i-5)^2+(j-5)^2 = (k-5)^2 + (l-5)^2.$$ It goes without saying that $(i,j)\equiv(5,5)$ is the center of the matrix. I also know the sum of the matrix along one dimension say $$ \begin{matrix} 0 & 1 & 4 & 9 & 12 &9 & 4 & 1 & 0. \end{matrix} $$ I was trying to solve it using the lsqlin function in Matlab. However the function accepts a vector $x$ while I have a 2D matrix. Also, I am not able to figure out a way to tell Matlab the set of constraints I have mentioned above (i.e. the constraints imposed by axisymmetry). Before I start digging deeper I wanted to know if such a set of equations will and certainly will have a unique solution. For instance, in the case of a 9x9 matrix I have 81 unknowns and 9 equations (defined by the sum along the dimesion). The axisymmetry provides a few more equations (though I am not sure exactly how many).","For this discussion I will assume a 9x9 matrix but my question is for a general nxn matrix. I have a matrix which is not only symmetric along the vertical and the horizontal axis, but is axisymmetric about its center. So for a 9x9 matrix I have the following set of constraints: $$x_{i,j} = x_{k,l}$$ if $$ (i-5)^2+(j-5)^2 = (k-5)^2 + (l-5)^2.$$ It goes without saying that $(i,j)\equiv(5,5)$ is the center of the matrix. I also know the sum of the matrix along one dimension say $$ \begin{matrix} 0 & 1 & 4 & 9 & 12 &9 & 4 & 1 & 0. \end{matrix} $$ I was trying to solve it using the lsqlin function in Matlab. However the function accepts a vector $x$ while I have a 2D matrix. Also, I am not able to figure out a way to tell Matlab the set of constraints I have mentioned above (i.e. the constraints imposed by axisymmetry). Before I start digging deeper I wanted to know if such a set of equations will and certainly will have a unique solution. For instance, in the case of a 9x9 matrix I have 81 unknowns and 9 equations (defined by the sum along the dimesion). The axisymmetry provides a few more equations (though I am not sure exactly how many).",,"['matrices', 'linear-programming', 'matrix-equations']"
21,Is there a Linear Map to represent transpose?,Is there a Linear Map to represent transpose?,,"For example, I want $AB = A^T$, is there such a matrix $B$ that does this? Where $A$ is $n \times m$ and $B$ is $m \times n$.","For example, I want $AB = A^T$, is there such a matrix $B$ that does this? Where $A$ is $n \times m$ and $B$ is $m \times n$.",,"['linear-algebra', 'matrices', 'matrix-equations']"
22,How to row reduce a matrix with complex entries?,How to row reduce a matrix with complex entries?,,"I have been doing some practice questions for university, and one of them is regarding row reducing a complex matrix. From what I can work out, I think (i could very well be wrong) that the first unknown (row 1) should be (1/32)(41i - 82) And as such, the second unknown should be (-3-2i) - (2 + 2i)((1/32)(41i - 82)) However this looks messy, and is making me think i may have done it wrong. If someone could please let me know if i am correct or not, and if not, where i went wrong, i would be hugely grateful. Thanks heaps in advance Corey","I have been doing some practice questions for university, and one of them is regarding row reducing a complex matrix. From what I can work out, I think (i could very well be wrong) that the first unknown (row 1) should be (1/32)(41i - 82) And as such, the second unknown should be (-3-2i) - (2 + 2i)((1/32)(41i - 82)) However this looks messy, and is making me think i may have done it wrong. If someone could please let me know if i am correct or not, and if not, where i went wrong, i would be hugely grateful. Thanks heaps in advance Corey",,"['linear-algebra', 'matrices', 'complex-numbers', 'gaussian-elimination']"
23,General Form of Orthogonal Upper Triangular Matrices,General Form of Orthogonal Upper Triangular Matrices,,"I have the following 2-part question: Find all $n \times n$ matrices that are both orthogonal and upper triangular, with positive diagonal entries. Show that the $QR$ factorization of an invertible $n \times n$ matrix is unique. Hint: if $A=Q_1R_1=Q_2R_2$, then the matrix $Q_2^{-1}Q_1=R_2R_1^{-1}$ is both orthogonal and upper triangular, with positive diagonal entries. I realize that the general form of the $R$ matrix is upper triangular, with diagonal entries as vector lengths, which by definition must be positive. I'm not sure about the big picture though. Anyone kind enough to nudge me in the right direction? Thanks!","I have the following 2-part question: Find all $n \times n$ matrices that are both orthogonal and upper triangular, with positive diagonal entries. Show that the $QR$ factorization of an invertible $n \times n$ matrix is unique. Hint: if $A=Q_1R_1=Q_2R_2$, then the matrix $Q_2^{-1}Q_1=R_2R_1^{-1}$ is both orthogonal and upper triangular, with positive diagonal entries. I realize that the general form of the $R$ matrix is upper triangular, with diagonal entries as vector lengths, which by definition must be positive. I'm not sure about the big picture though. Anyone kind enough to nudge me in the right direction? Thanks!",,"['linear-algebra', 'matrices', 'orthogonality']"
24,Derivative of a Matrix w.r.t. a Matrix,Derivative of a Matrix w.r.t. a Matrix,,"I have a matrix product with $\mathbf{X} \in \mathbb{R}^{m\times n}$ as $\mathbf{F(X)} = \mathbf{XAA}^T$ where $\mathbf{A}$ is a constant matrix w.r.t. $\mathbf{X}$. I see that I can write the following according to Wikipedia . $$ d\mathbf{F(X)} = (d\mathbf{X})\mathbf{AA}^T + \mathbf{X}d(\mathbf{AA}^T) = (d\mathbf{X})\mathbf{AA}^T $$ From here, can I write, $$ \frac{d\mathbf{F(X)}}{d\mathbf{X}} = \mathbf{I}_{m\times n}\mathbf{AA}^T = \mathbf{AA}^T $$ Note that I have taken the help of the fact that the derivative of an ${m\times n}$ matrix $\mathbf{A}$ with respect to itself is $\mathbf{I}_{m\times n}$, as found in page 4 of the Notes on Matrix Calculus by Paul L. Fackler . I'm not sure what exactly $\mathbf{I}_{m\times n}$ is, but I'm taking it as some sort of generalized identity matrix and assuming that premultiplying $\mathbf{AA}^T$ with $\mathbf{I}_{m\times n}$ results in $\mathbf{AA}^T$ only. So, in short, my question is can I write $\frac{d\mathbf{F(X)}}{d\mathbf{X}}$ as $\mathbf{AA}^T$, in this case?","I have a matrix product with $\mathbf{X} \in \mathbb{R}^{m\times n}$ as $\mathbf{F(X)} = \mathbf{XAA}^T$ where $\mathbf{A}$ is a constant matrix w.r.t. $\mathbf{X}$. I see that I can write the following according to Wikipedia . $$ d\mathbf{F(X)} = (d\mathbf{X})\mathbf{AA}^T + \mathbf{X}d(\mathbf{AA}^T) = (d\mathbf{X})\mathbf{AA}^T $$ From here, can I write, $$ \frac{d\mathbf{F(X)}}{d\mathbf{X}} = \mathbf{I}_{m\times n}\mathbf{AA}^T = \mathbf{AA}^T $$ Note that I have taken the help of the fact that the derivative of an ${m\times n}$ matrix $\mathbf{A}$ with respect to itself is $\mathbf{I}_{m\times n}$, as found in page 4 of the Notes on Matrix Calculus by Paul L. Fackler . I'm not sure what exactly $\mathbf{I}_{m\times n}$ is, but I'm taking it as some sort of generalized identity matrix and assuming that premultiplying $\mathbf{AA}^T$ with $\mathbf{I}_{m\times n}$ results in $\mathbf{AA}^T$ only. So, in short, my question is can I write $\frac{d\mathbf{F(X)}}{d\mathbf{X}}$ as $\mathbf{AA}^T$, in this case?",,"['calculus', 'linear-algebra', 'matrices']"
25,Showing that $\det(AB)=\det A \det B$ with the following identity.,Showing that  with the following identity.,\det(AB)=\det A \det B,"Given the following formulation of the determinant with Levi-Civita permutation symbols, show that $\det(AB)=\det A \det B$. $$\det A = \sum\limits_{ij\cdots l}\epsilon_{ij\cdots l} A_{i1}A_{j2}\cdots A_{ln}\,,\,\,\,\,\,\textrm{where A is an }n\times n \textrm{ matrix}$$ I have been trying to show this for so long, but I can't seem to get past a certain point. Here is my work so far. $$\begin{align*} \det (AB)&=\sum\limits_{ij\cdots l}\epsilon_{ij\cdots l}(AB)_{i1}(AB)_{j2}\cdots(AB)_{ln}\\ &=\sum\limits_{ij\cdots l}\epsilon_{ij\cdots l}\left(\sum\limits_{k_1}A_{ik_1}B_{k_11}\right)\left(\sum\limits_{k_2}A_{jk_2}B_{k_22}\right)\cdots \left(\sum\limits_{k_n}A_{lk_n}B_{k_nn}\right)\\ &=\sum\limits_{ij\cdots l}\epsilon_{ij\cdots l}\left(\sum\limits_{k_1,k_2,\cdots k_n} A_{ik_1}A_{jk_2}\cdots A_{lk_n}B_{k_11}B_{k_22}\cdots B_{k_nn}\right)\\ &=??? \end{align*} $$ Any tips on how to proceed? Have I made a mistake anywhere?","Given the following formulation of the determinant with Levi-Civita permutation symbols, show that $\det(AB)=\det A \det B$. $$\det A = \sum\limits_{ij\cdots l}\epsilon_{ij\cdots l} A_{i1}A_{j2}\cdots A_{ln}\,,\,\,\,\,\,\textrm{where A is an }n\times n \textrm{ matrix}$$ I have been trying to show this for so long, but I can't seem to get past a certain point. Here is my work so far. $$\begin{align*} \det (AB)&=\sum\limits_{ij\cdots l}\epsilon_{ij\cdots l}(AB)_{i1}(AB)_{j2}\cdots(AB)_{ln}\\ &=\sum\limits_{ij\cdots l}\epsilon_{ij\cdots l}\left(\sum\limits_{k_1}A_{ik_1}B_{k_11}\right)\left(\sum\limits_{k_2}A_{jk_2}B_{k_22}\right)\cdots \left(\sum\limits_{k_n}A_{lk_n}B_{k_nn}\right)\\ &=\sum\limits_{ij\cdots l}\epsilon_{ij\cdots l}\left(\sum\limits_{k_1,k_2,\cdots k_n} A_{ik_1}A_{jk_2}\cdots A_{lk_n}B_{k_11}B_{k_22}\cdots B_{k_nn}\right)\\ &=??? \end{align*} $$ Any tips on how to proceed? Have I made a mistake anywhere?",,"['linear-algebra', 'matrices', 'determinant']"
26,"Find the value of k, (if any), for which the system below has unique, infinite or no solution. [duplicate]","Find the value of k, (if any), for which the system below has unique, infinite or no solution. [duplicate]",,"This question already has answers here : System of Linear Equations - how many solutions? (3 answers) Closed 9 years ago . The system of equations are: $\begin{cases}x+y+kz = 1\\x+ky+z=1\\kx+y+z=1\\ \end{cases}$ I am looking to finding values of $k$, for which this system has either no solutions, infinite many solutions or a unique solution (if any). I've had a look around the web for some help, and know that this question has been asked, but in a way that I have not yet learnt (something to do with determinants I think). However, I have only been working with reducing the matrix to REF/RREF and thus I'm trying to find rows, such as $( 0 \ 0 \ 0 \mid 2)$ and $( 0 \ 0 \ 0 \mid 0)$ etc to show if we have no solutions or infinite many solutions, for instance. So far, I've got the following: $\left[\begin{array}{ccc|c}1&1&k&1\\1&k&1&1\\k&1&1&1\end{array}\right]$ Then, $R_2$$\mapsto$$R_2-R_1$ $\left[\begin{array}{ccc|c}1&1&k&1\\0&k-1&1-k&0\\k&1&1&1\end{array}\right]$ Then, $R_2$ $\leftrightarrow$ $R_3$ $\left[\begin{array}{ccc|c}1&1&k&1\\k&1&1&1\\0&k-1&1-k&0\end{array}\right]$ This is where I got stuck, and I'm not sure how to continue. Thank you in advance, and I'm sorry if this question has been asked and solved in the same way as presented above. Cheers.","This question already has answers here : System of Linear Equations - how many solutions? (3 answers) Closed 9 years ago . The system of equations are: $\begin{cases}x+y+kz = 1\\x+ky+z=1\\kx+y+z=1\\ \end{cases}$ I am looking to finding values of $k$, for which this system has either no solutions, infinite many solutions or a unique solution (if any). I've had a look around the web for some help, and know that this question has been asked, but in a way that I have not yet learnt (something to do with determinants I think). However, I have only been working with reducing the matrix to REF/RREF and thus I'm trying to find rows, such as $( 0 \ 0 \ 0 \mid 2)$ and $( 0 \ 0 \ 0 \mid 0)$ etc to show if we have no solutions or infinite many solutions, for instance. So far, I've got the following: $\left[\begin{array}{ccc|c}1&1&k&1\\1&k&1&1\\k&1&1&1\end{array}\right]$ Then, $R_2$$\mapsto$$R_2-R_1$ $\left[\begin{array}{ccc|c}1&1&k&1\\0&k-1&1-k&0\\k&1&1&1\end{array}\right]$ Then, $R_2$ $\leftrightarrow$ $R_3$ $\left[\begin{array}{ccc|c}1&1&k&1\\k&1&1&1\\0&k-1&1-k&0\end{array}\right]$ This is where I got stuck, and I'm not sure how to continue. Thank you in advance, and I'm sorry if this question has been asked and solved in the same way as presented above. Cheers.",,"['linear-algebra', 'matrices', 'systems-of-equations']"
27,Skew symmetric $4\times 4$ matrix of full-rank,Skew symmetric  matrix of full-rank,4\times 4,I have come across the fact that a $4\times 4$ skew-symmmetric matrix of full-rank is equivalent to  \begin{pmatrix} 0 &\theta_1& 0 &0 \\ -\theta_1& 0 &0 &0 \\ 0& 0&0 & \theta_2 \\ 0& 0& -\theta_2 & 0 \end{pmatrix} And I'm not sure why. Thanks for the help.,I have come across the fact that a $4\times 4$ skew-symmmetric matrix of full-rank is equivalent to  \begin{pmatrix} 0 &\theta_1& 0 &0 \\ -\theta_1& 0 &0 &0 \\ 0& 0&0 & \theta_2 \\ 0& 0& -\theta_2 & 0 \end{pmatrix} And I'm not sure why. Thanks for the help.,,"['linear-algebra', 'matrices']"
28,Courant minimax principle on block matrix,Courant minimax principle on block matrix,,"in going through some books about numerical mathematics I found the following exercise: Let $A,B \in \mathbb{R}^{n\times n}$ with $A$ symmetrical and rank( $A$ ) = rank(B) = $n$ . Define $M = \left[\begin{matrix} A & B \\ B^T & 0 \end{matrix}\right] $ . The statement now is, that $M$ has exactly $n$ positive and $n$ negative eigenvalues. And to prove it one should use the Courant minimax principle. First we can see, that $M$ ist symmetrical, so Courant minimax principle is actually applicable for $M$ . As the eigenvalues given by the minimax principle are ordered ( $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_{2n}$ ) we only have to consider two of them: $\lambda_n$ and $\lambda_{n+1}$ . If $\lambda_n$ is positive and $\lambda_{n+1}$ negative, then the statement is proven right. And this is where my problems begin. The Courant minimax principle gives that $\lambda_k = \max\limits_{U \in \mathcal{U}_k} \min\limits_{x \in U, ||x|| = 1}{x^T M x}$ , where $\mathcal{U}_k = \{ U \subset \mathbb{R}^{2n} | \dim{U} = 2n + 1 -k\}$ . I can't figure out how to make any kind of estimate for the value of $\lambda_k$ (in particular for $k = n$ and $k = n+1$ ) given how little we know about the matrix $M$ or it's components $A$ , and $B$ .","in going through some books about numerical mathematics I found the following exercise: Let with symmetrical and rank( ) = rank(B) = . Define . The statement now is, that has exactly positive and negative eigenvalues. And to prove it one should use the Courant minimax principle. First we can see, that ist symmetrical, so Courant minimax principle is actually applicable for . As the eigenvalues given by the minimax principle are ordered ( ) we only have to consider two of them: and . If is positive and negative, then the statement is proven right. And this is where my problems begin. The Courant minimax principle gives that , where . I can't figure out how to make any kind of estimate for the value of (in particular for and ) given how little we know about the matrix or it's components , and .","A,B \in \mathbb{R}^{n\times n} A A n M =
\left[\begin{matrix}
A & B \\
B^T & 0
\end{matrix}\right]
 M n n M M \lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_{2n} \lambda_n \lambda_{n+1} \lambda_n \lambda_{n+1} \lambda_k = \max\limits_{U \in \mathcal{U}_k} \min\limits_{x \in U, ||x|| = 1}{x^T M x} \mathcal{U}_k = \{ U \subset \mathbb{R}^{2n} | \dim{U} = 2n + 1 -k\} \lambda_k k = n k = n+1 M A B","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices', 'block-matrices']"
29,Factorise a matrix using the factor theorem,Factorise a matrix using the factor theorem,,"Can someone check this please?  $$ \begin{vmatrix} x&y&z\\ x^2&y^2&z^2\\ x^3&y^3&z^3\\ \end{vmatrix}$$ $$C_2=C_2-C_1\implies\quad \begin{vmatrix} x&y-x&z\\ x^2&y^2-x^2&z^2\\ x^3&y^3-x^3&z^3\\ \end{vmatrix}$$ $$(y-x) \begin{vmatrix} x&1&z\\ x^2&y+x&z^2\\ x^3&y^2+xy+x^2&z^3\\ \end{vmatrix}$$ $$(y-x)(z-x) \begin{vmatrix} x&1&1\\ x^2&y+x&z+x\\ x^3&y^2+xy+x^2&z^2+xz+x^2\\ \end{vmatrix}$$ $$R_2=R_2-xR_1\implies\quad (y-x)(z-x) \begin{vmatrix} x&1&1\\ 0&y&z\\ x^3&y^2+xy+x^2&z^2+xz+x^2\\ \end{vmatrix}$$ $$R_3=R_3-x^2R_1\implies\quad (y-x)(z-x) \begin{vmatrix} x&1&1\\ 0&y&z\\ 0&y^2+xy&z^2+xz\\ \end{vmatrix}$$ factor $x$$$\implies\quad x(y-x)(z-x) \begin{vmatrix} 1&1&1\\ 0&y&z\\ 0&y^2+xy&z^2+xz\\ \end{vmatrix}$$ $$\implies\quad x(y-x)(z-x)(yz^2-zy^2)$$ $$\implies\quad xyz(y-x)(z-x)(z-y)$$ Also I'd like practical tips on using the factor theorem for these types of questions. My understanding is that the determinant is $f(x,y,z)$ so if we hold $y$ and $z$ constant we could apply it somehow to $f(x)$ alone. I'm not that great spotting difference of squares etc and want a more fail safe alternative. Thanks in advance.","Can someone check this please?  $$ \begin{vmatrix} x&y&z\\ x^2&y^2&z^2\\ x^3&y^3&z^3\\ \end{vmatrix}$$ $$C_2=C_2-C_1\implies\quad \begin{vmatrix} x&y-x&z\\ x^2&y^2-x^2&z^2\\ x^3&y^3-x^3&z^3\\ \end{vmatrix}$$ $$(y-x) \begin{vmatrix} x&1&z\\ x^2&y+x&z^2\\ x^3&y^2+xy+x^2&z^3\\ \end{vmatrix}$$ $$(y-x)(z-x) \begin{vmatrix} x&1&1\\ x^2&y+x&z+x\\ x^3&y^2+xy+x^2&z^2+xz+x^2\\ \end{vmatrix}$$ $$R_2=R_2-xR_1\implies\quad (y-x)(z-x) \begin{vmatrix} x&1&1\\ 0&y&z\\ x^3&y^2+xy+x^2&z^2+xz+x^2\\ \end{vmatrix}$$ $$R_3=R_3-x^2R_1\implies\quad (y-x)(z-x) \begin{vmatrix} x&1&1\\ 0&y&z\\ 0&y^2+xy&z^2+xz\\ \end{vmatrix}$$ factor $x$$$\implies\quad x(y-x)(z-x) \begin{vmatrix} 1&1&1\\ 0&y&z\\ 0&y^2+xy&z^2+xz\\ \end{vmatrix}$$ $$\implies\quad x(y-x)(z-x)(yz^2-zy^2)$$ $$\implies\quad xyz(y-x)(z-x)(z-y)$$ Also I'd like practical tips on using the factor theorem for these types of questions. My understanding is that the determinant is $f(x,y,z)$ so if we hold $y$ and $z$ constant we could apply it somehow to $f(x)$ alone. I'm not that great spotting difference of squares etc and want a more fail safe alternative. Thanks in advance.",,"['matrices', 'factoring']"
30,Determinant of matrix whose entries are cosines,Determinant of matrix whose entries are cosines,,Let $n\geq3$ . How to find the determinant of the following matrix? $$ \begin{pmatrix}  \cos(\alpha_1 -\beta _1) & \cos(\alpha_1 -\beta _2) & \cdots & \cos(\alpha_1 -\beta _n)\\  \cos(\alpha_2 -\beta _1) & \cos(\alpha_2 -\beta _2)& \cdots & \cos(\alpha_2 -\beta _n)\\ \vdots & \vdots& \ddots& \vdots\\ \cos(\alpha_n -\beta _1)& \cos(\alpha_n -\beta _2)& \cdots & \cos(\alpha_n -\beta _n) \end{pmatrix} $$ I tried to use some trigonometric properties but it did not help. Any suggestions? Thanks!,Let . How to find the determinant of the following matrix? I tried to use some trigonometric properties but it did not help. Any suggestions? Thanks!,"n\geq3 
\begin{pmatrix} 
\cos(\alpha_1 -\beta _1) & \cos(\alpha_1 -\beta _2) & \cdots & \cos(\alpha_1 -\beta _n)\\ 
\cos(\alpha_2 -\beta _1) & \cos(\alpha_2 -\beta _2)& \cdots & \cos(\alpha_2 -\beta _n)\\
\vdots & \vdots& \ddots& \vdots\\
\cos(\alpha_n -\beta _1)& \cos(\alpha_n -\beta _2)& \cdots & \cos(\alpha_n -\beta _n)
\end{pmatrix}
","['linear-algebra', 'matrices', 'determinant']"
31,"Erwin Kreyszig's Introductory Functional Analysis With Applications, Section 2.6, Problem 11","Erwin Kreyszig's Introductory Functional Analysis With Applications, Section 2.6, Problem 11",,"Let $X$ be the vector space of all complex $n \times n$ matrices and define $T \colon X \to X$ by $Tx \colon= bx$, where $b \in X$ is fixed and $bx$ denotes the usual product of matrices. I know that $T$ is linear. Under what conditions does $T^{-1}$ exist? If $b$ is an invertible matrix, then of course $T^{-1}$ exists. But does the existence of $T^{-1}$ necessarily imply the invertibility of the matrix $b$? What condition(s) (other than invertibility), if any, should $b$ satisfy in order for $T^{-1}$ to exist?","Let $X$ be the vector space of all complex $n \times n$ matrices and define $T \colon X \to X$ by $Tx \colon= bx$, where $b \in X$ is fixed and $bx$ denotes the usual product of matrices. I know that $T$ is linear. Under what conditions does $T^{-1}$ exist? If $b$ is an invertible matrix, then of course $T^{-1}$ exists. But does the existence of $T^{-1}$ necessarily imply the invertibility of the matrix $b$? What condition(s) (other than invertibility), if any, should $b$ satisfy in order for $T^{-1}$ to exist?",,"['linear-algebra', 'analysis', 'matrices', 'functional-analysis', 'matrix-equations']"
32,How to determine if a 3x3 matrix is diagonalizable?,How to determine if a 3x3 matrix is diagonalizable?,,"The matrix is given as: $A=\begin{bmatrix} 0 & 1 & 1 \\0 & 0 & 4 \\ 0 & 0 & 3 \end{bmatrix}$ So the matrix has eigenvalues of $0$ ,$0$,and $3$. The matrix has a free variable for $x_1$ so there are only $2$ linear independent eigenvectors. So this matrix is not diagonalizable. What conditions would be necessary for $A$ to be diagonalizable? Is it simply all $3$ eigenvectors must be linearly independent? Or perhaps the opposite?","The matrix is given as: $A=\begin{bmatrix} 0 & 1 & 1 \\0 & 0 & 4 \\ 0 & 0 & 3 \end{bmatrix}$ So the matrix has eigenvalues of $0$ ,$0$,and $3$. The matrix has a free variable for $x_1$ so there are only $2$ linear independent eigenvectors. So this matrix is not diagonalizable. What conditions would be necessary for $A$ to be diagonalizable? Is it simply all $3$ eigenvectors must be linearly independent? Or perhaps the opposite?",,"['matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
33,"Determine when the system has a) no solution, b) 1 solution and c) infinitely many solutions","Determine when the system has a) no solution, b) 1 solution and c) infinitely many solutions",,"This question is not for an assignment, it was on the midterm and I am interested in figuring out how to solve it before the final exam. cheers, Determine when the system has a) no solution, b) 1 solution and c) infinitely many solutions Given the system \begin{align*} x + y + 7z & = -7\\ 2x + 3y +17z & = -16\\ x +2y +(a^2+1)z & = 3a \end{align*} Attempted Solution using matrix form $$\pmatrix{1&1&7&-7\cr2&3&17&-16\cr1&2&a^2+1&3a\cr}$$ using gaussian elimination to isolate eqn 3 for variable z (sorry about the missing a^2+1 up there html doesn't like me) line 1 - line 3 $$\pmatrix{1&1&7&-7\cr2&3&17&-16\cr 0&1&7-(a^2+1)&-7-3a\cr}$$ 2*line 1 - line 2 $$\pmatrix{1&1&7&-7\cr 0&-1&-3&2 \cr 0&-1&7-(a^2+1)&-7-3a\cr}$$ -1*line2 $$\pmatrix{1 & 1 &7 &-7\\ 0 & 1 & 3 & -2\\ 0 & -1 & 7 - (a^2 + 1) & -7 - 3a}$$ -1*line 2 + line 1 $$\pmatrix{1 & 0 & 4 & -5\\ 0 & 1 & 3 & -2\\ 0 & -1 & 7 - (a^2 + 1) & -7 - 3a}$$ line 2 + line 3 $$\pmatrix{1 & 0 & 4 & -5\\ 0 & 1 & 3 & -2\\ 0 & 0 & 10 - (a^2 + 1) & -9 - 3a}$$ so we get equation 3 $10-(a^2+1)z=-9-3a$ rearranging, $$(a^2+1)z=-19-3a \rightarrow$$ $$z=19/a^2+1 - 3a/a^2+1 \rightarrow $$ $$z=(19/a^2+1) - (-3/a+1)$$ $$z=19/a^2 + 1 - (-3/a) +1$$ $$z=19/a^2 + 2 + 3/a$$ multiply all terms by $a$ $$za=19/a + 2a + 3$$ $$za=(39/19)a + 3$$ $$za=3a+3$$ $$z=3+3/a$$ wut? I am unsure how to properly answer the question, please help stack exchange! My algebra skills are a little lacking, please point out helpfully where my thinking is wrong. I can solve for z but what does that prove","This question is not for an assignment, it was on the midterm and I am interested in figuring out how to solve it before the final exam. cheers, Determine when the system has a) no solution, b) 1 solution and c) infinitely many solutions Given the system \begin{align*} x + y + 7z & = -7\\ 2x + 3y +17z & = -16\\ x +2y +(a^2+1)z & = 3a \end{align*} Attempted Solution using matrix form $$\pmatrix{1&1&7&-7\cr2&3&17&-16\cr1&2&a^2+1&3a\cr}$$ using gaussian elimination to isolate eqn 3 for variable z (sorry about the missing a^2+1 up there html doesn't like me) line 1 - line 3 $$\pmatrix{1&1&7&-7\cr2&3&17&-16\cr 0&1&7-(a^2+1)&-7-3a\cr}$$ 2*line 1 - line 2 $$\pmatrix{1&1&7&-7\cr 0&-1&-3&2 \cr 0&-1&7-(a^2+1)&-7-3a\cr}$$ -1*line2 $$\pmatrix{1 & 1 &7 &-7\\ 0 & 1 & 3 & -2\\ 0 & -1 & 7 - (a^2 + 1) & -7 - 3a}$$ -1*line 2 + line 1 $$\pmatrix{1 & 0 & 4 & -5\\ 0 & 1 & 3 & -2\\ 0 & -1 & 7 - (a^2 + 1) & -7 - 3a}$$ line 2 + line 3 $$\pmatrix{1 & 0 & 4 & -5\\ 0 & 1 & 3 & -2\\ 0 & 0 & 10 - (a^2 + 1) & -9 - 3a}$$ so we get equation 3 $10-(a^2+1)z=-9-3a$ rearranging, $$(a^2+1)z=-19-3a \rightarrow$$ $$z=19/a^2+1 - 3a/a^2+1 \rightarrow $$ $$z=(19/a^2+1) - (-3/a+1)$$ $$z=19/a^2 + 1 - (-3/a) +1$$ $$z=19/a^2 + 2 + 3/a$$ multiply all terms by $a$ $$za=19/a + 2a + 3$$ $$za=(39/19)a + 3$$ $$za=3a+3$$ $$z=3+3/a$$ wut? I am unsure how to properly answer the question, please help stack exchange! My algebra skills are a little lacking, please point out helpfully where my thinking is wrong. I can solve for z but what does that prove",,"['linear-algebra', 'algebra-precalculus', 'matrices', 'systems-of-equations', 'gaussian-elimination']"
34,Proving Schur's Theorem can create both an upper and lower triangular matrix.,Proving Schur's Theorem can create both an upper and lower triangular matrix.,,"I've seen two very small distinctions in Schur's theorem, one is that for any $n\times n$ matrix $A, \exists$ a Unitary $U$ s.t. $U^*AU$ is upper triangular, the other is that $U^*AU$ is lower triangular. I've only learned the proof and can use the 'upper triangular' version and would like to show that Schur's theorem can also build a lower triangular matrix. I don't think I should have to go through the entire proof again, it seems I should be able to simply state something about ""By Schure's theorem..."" Any ideas would be appreciated. Thank you.","I've seen two very small distinctions in Schur's theorem, one is that for any $n\times n$ matrix $A, \exists$ a Unitary $U$ s.t. $U^*AU$ is upper triangular, the other is that $U^*AU$ is lower triangular. I've only learned the proof and can use the 'upper triangular' version and would like to show that Schur's theorem can also build a lower triangular matrix. I don't think I should have to go through the entire proof again, it seems I should be able to simply state something about ""By Schure's theorem..."" Any ideas would be appreciated. Thank you.",,"['linear-algebra', 'matrices', 'matrix-decomposition']"
35,How to prove the matrix inequality $\sqrt{\det{(A^2-AB+B^2)}}+\sqrt{\det{(A^2+AB+B^2)}}\ge |\det{(A)}+\det{(B)}|$,How to prove the matrix inequality,\sqrt{\det{(A^2-AB+B^2)}}+\sqrt{\det{(A^2+AB+B^2)}}\ge |\det{(A)}+\det{(B)}|,"Let $A,B\in M_{2}(R)$ be two square matrices so that $AB=BA$, prove the inequality $$\sqrt{\det{(A^2-AB+B^2)}}+\sqrt{\det{(A^2+AB+B^2)}}\ge |\det{(A)} +\det{(B)}|$$ I only know prove this $$\sqrt{a^2-ab+b^2}+\sqrt{a^2+ab+b^2}\ge |a+b|,a,b\in R$$ because $$\sqrt{x}+\sqrt{y}\ge\sqrt{x+y}$$ so $$LHS\ge\sqrt{2(a^2+b^2)}\ge (a+b)$$ But for matrices I can't prove it. Thank you","Let $A,B\in M_{2}(R)$ be two square matrices so that $AB=BA$, prove the inequality $$\sqrt{\det{(A^2-AB+B^2)}}+\sqrt{\det{(A^2+AB+B^2)}}\ge |\det{(A)} +\det{(B)}|$$ I only know prove this $$\sqrt{a^2-ab+b^2}+\sqrt{a^2+ab+b^2}\ge |a+b|,a,b\in R$$ because $$\sqrt{x}+\sqrt{y}\ge\sqrt{x+y}$$ so $$LHS\ge\sqrt{2(a^2+b^2)}\ge (a+b)$$ But for matrices I can't prove it. Thank you",,"['matrices', 'inequality']"
36,MATLAB Eigenvalues,MATLAB Eigenvalues,,"So I am trying to get matlab to output the correct eigenvalue/eigenvector of a matrix. The matrix is: \begin{array}{ccc} 0 & 1 \\ -3 & -4 \end{array} The eigenvalues are: $\lambda_1 = -1, \lambda_2=-3$ Therefore the eigenvector that I calculated and that wolframalpha verifies is: \begin{array}{ccc} 1 & 1 \\ -1 & -3 \end{array} However, when I run the following matlab code: A = [0 1; -3 -4;]; [T,lambda] = eig(A) I get the following as the output: T =      0.7071   -0.3162    -0.7071    0.9487 I understand that the ratios are correct (i.e $\frac{-.3162}{.9487}=-3$) but I want it to output the eigenvector as whole numbers like how I calculated above. Is there anyway to do that?","So I am trying to get matlab to output the correct eigenvalue/eigenvector of a matrix. The matrix is: \begin{array}{ccc} 0 & 1 \\ -3 & -4 \end{array} The eigenvalues are: $\lambda_1 = -1, \lambda_2=-3$ Therefore the eigenvector that I calculated and that wolframalpha verifies is: \begin{array}{ccc} 1 & 1 \\ -1 & -3 \end{array} However, when I run the following matlab code: A = [0 1; -3 -4;]; [T,lambda] = eig(A) I get the following as the output: T =      0.7071   -0.3162    -0.7071    0.9487 I understand that the ratios are correct (i.e $\frac{-.3162}{.9487}=-3$) but I want it to output the eigenvector as whole numbers like how I calculated above. Is there anyway to do that?",,"['matrices', 'eigenvalues-eigenvectors', 'matlab']"
37,Normal subgroup proof,Normal subgroup proof,,"I'm preparing for an algebra exam later this month and am trying out the exercises from my textbook. Sadly I got stuck with this one: Let $G$ be a group of all regular upper triangular matrices $2 \times 2$ over $\mathbb{Q}$. Let $H$ be its subgroup of matrices with positive numbers on the diagonal (anything can be in the upper right corner). Prove, that $H$ is a normal subgroup of $G$ and that $G/H$ is isomorphic with $\mathbb{Z}_2 \times \mathbb{Z}_2$. Could you please give me some directions on how to approach/start this problem? All help very appreciated. Sorry for my bad English, hope the problem is understandable.","I'm preparing for an algebra exam later this month and am trying out the exercises from my textbook. Sadly I got stuck with this one: Let $G$ be a group of all regular upper triangular matrices $2 \times 2$ over $\mathbb{Q}$. Let $H$ be its subgroup of matrices with positive numbers on the diagonal (anything can be in the upper right corner). Prove, that $H$ is a normal subgroup of $G$ and that $G/H$ is isomorphic with $\mathbb{Z}_2 \times \mathbb{Z}_2$. Could you please give me some directions on how to approach/start this problem? All help very appreciated. Sorry for my bad English, hope the problem is understandable.",,['matrices']
38,Singular Value Decomposition in Axler's book,Singular Value Decomposition in Axler's book,,"In Axler's ""Linear Algebra Done Right"", he gave the singular-value decomposition as: $Tv = s_1\langle v,e_1\rangle f_1 + \cdots + s_n\langle v,e_n\rangle f_n$, where  T is an operator; $s_1,\ldots,s_n$ are the singular values of $T$; $(e_1,\ldots,e_n)$ and $(f_1,\ldots,f_n)$ are orthonormal bases of $V$. Singular values of $T$ are defined as eigenvalues of $\sqrt{T^*T}$. $\langle ,\rangle $ is inner product. I am having trouble connecting this to the more generally used matrix representation $A=UΣV^T$.  Matrices in Axler's book are considered as linear maps, ""with respect to some bases"". How do I understand Axler's exposition using matrices? Thank you. Update: To clarify, I was thinking from the perspective where an operator is a matrix with respect to some basis (in his book...). So an operator can be written as $M(T,(b1,...bn),(b1,...bn))$. If I apply change of basis, I get $M(I,(f1,...fn),(b1,...bn))*M(T,(e1,...en),(f1,...fn))*M(I,b1,...bn),(e1,...en))$  where (e1,...en) is an orthonormal basis; (f1,...fn) is another orthonormal basis given by $fi=Sei$ given by isometry $S$ from polar decomposition. $M(T,(e1,...en),(f1,...fn))$ gives me the diagonol matrix $Σ$. But how about the rest? Thanks. Update 2 : Thanks for the answers below and I understand those. But my question is a bit specific to Axler's definition. In Axler's book, he thinks of matrix a bit differently: m*n matrix is defined as $M(T,(v_1...v_n),(w_1..w_m))$ where $T$ is a linear map from space $V$ to W, and $(v_1...v_n)$ and $(w_1...w_m)$ are bases for 2 spaces. For the $k$th column in the matrix, each entry $a_{ik}$ is defined as $Tv_k=a_{1k}w_1+...a_{mk}w_m$, so the matrix represents the linear map between the bases. So it's like ""you don't talk about a matrix without talking about $T$ and the bases"". Whether this is a good way to think about matrix is probably also a topic for debate... Anyway, now with that, I am trying to decompose: $M(T,(b_1,...b_n),(b_1,...b_n))$. I already know that the diagonol $Σ$ in SVD can be thought of as: A. $M(T,(e_1,...e_n),(f_1,...f_n))$, (where $f_i=Se_i$ given by isometry $S$ from polar decomposition $T=S\sqrt{T^*T}$), OR , B. $M(\sqrt{T^*T},(e_1,...e_n),(e_1,...e_n))$. So I would also like to know: 1. the operator 2. the bases, for the other 2 unitary matrices, no matter which one you choose for $Σ$. Thanks again.","In Axler's ""Linear Algebra Done Right"", he gave the singular-value decomposition as: $Tv = s_1\langle v,e_1\rangle f_1 + \cdots + s_n\langle v,e_n\rangle f_n$, where  T is an operator; $s_1,\ldots,s_n$ are the singular values of $T$; $(e_1,\ldots,e_n)$ and $(f_1,\ldots,f_n)$ are orthonormal bases of $V$. Singular values of $T$ are defined as eigenvalues of $\sqrt{T^*T}$. $\langle ,\rangle $ is inner product. I am having trouble connecting this to the more generally used matrix representation $A=UΣV^T$.  Matrices in Axler's book are considered as linear maps, ""with respect to some bases"". How do I understand Axler's exposition using matrices? Thank you. Update: To clarify, I was thinking from the perspective where an operator is a matrix with respect to some basis (in his book...). So an operator can be written as $M(T,(b1,...bn),(b1,...bn))$. If I apply change of basis, I get $M(I,(f1,...fn),(b1,...bn))*M(T,(e1,...en),(f1,...fn))*M(I,b1,...bn),(e1,...en))$  where (e1,...en) is an orthonormal basis; (f1,...fn) is another orthonormal basis given by $fi=Sei$ given by isometry $S$ from polar decomposition. $M(T,(e1,...en),(f1,...fn))$ gives me the diagonol matrix $Σ$. But how about the rest? Thanks. Update 2 : Thanks for the answers below and I understand those. But my question is a bit specific to Axler's definition. In Axler's book, he thinks of matrix a bit differently: m*n matrix is defined as $M(T,(v_1...v_n),(w_1..w_m))$ where $T$ is a linear map from space $V$ to W, and $(v_1...v_n)$ and $(w_1...w_m)$ are bases for 2 spaces. For the $k$th column in the matrix, each entry $a_{ik}$ is defined as $Tv_k=a_{1k}w_1+...a_{mk}w_m$, so the matrix represents the linear map between the bases. So it's like ""you don't talk about a matrix without talking about $T$ and the bases"". Whether this is a good way to think about matrix is probably also a topic for debate... Anyway, now with that, I am trying to decompose: $M(T,(b_1,...b_n),(b_1,...b_n))$. I already know that the diagonol $Σ$ in SVD can be thought of as: A. $M(T,(e_1,...e_n),(f_1,...f_n))$, (where $f_i=Se_i$ given by isometry $S$ from polar decomposition $T=S\sqrt{T^*T}$), OR , B. $M(\sqrt{T^*T},(e_1,...e_n),(e_1,...e_n))$. So I would also like to know: 1. the operator 2. the bases, for the other 2 unitary matrices, no matter which one you choose for $Σ$. Thanks again.",,"['linear-algebra', 'matrices', 'svd']"
39,Logarithm of Matrix exponential,Logarithm of Matrix exponential,,"Can we say  $\log(e^X e^Y)=X+Y \tag 1$ ? where $X$,$Y$ are general skew symmetric matrices of order $3 \times 3$ (Just mentioned skew symmetric matrices to indicate that these are rotational matrices)","Can we say  $\log(e^X e^Y)=X+Y \tag 1$ ? where $X$,$Y$ are general skew symmetric matrices of order $3 \times 3$ (Just mentioned skew symmetric matrices to indicate that these are rotational matrices)",,"['linear-algebra', 'matrices', 'exponential-function']"
40,Classify up to similarity all complex $3 \times 3$ matrices A such that A such that ${A^3} = 2{A^2} - A$,Classify up to similarity all complex  matrices A such that A such that,3 \times 3 {A^3} = 2{A^2} - A,"Classify up to similarity all complex $3 \times 3$ matrices A such that A such that ${A^3} = 2{A^2} - A$. Here is what I know: All matrices with complex entries have Jordan canonical forms. If matrices are similar, they have the same Jordan canonical form. This is what I've tried: Let $g(t) = {t^3} - 2{t^2} + t$. Then $g(A) = 0$. So, $p(t)$, the minimal polynomial of $A$, can be: $t, t-1, {(t-1)^2}, t(t-1), t{(t-1)^2}$. Now, I think that we classify all the matrices as similar to one of: $\begin{bmatrix}0&0&0\\0&0&0\\0&0&0\end{bmatrix}$, $\begin{bmatrix}1&0&0\\0&1&0\\0&0&1\end{bmatrix}$, $\begin{bmatrix}1&1&0\\0&1&0\\0&0&1\end{bmatrix}$, $\begin{bmatrix}1&0&0\\0&1&0\\0&0&0\end{bmatrix}$, $\begin{bmatrix}1&0&0\\0&0&0\\0&0&0\end{bmatrix}$, $\begin{bmatrix}1&1&0\\0&1&0\\0&0&0\end{bmatrix}$. And my reasoning for that was just taking each one of the possible minimal polynomials, and finding the possible JCF's for it. But, I'm not very confident in this strategy at all. Is this the right way to go about the problem? If not, what would you suggest trying? If so, is my reasoning correct - can I just find the possible minimal polynomials, and say that each matrix we are looking for is similar to the JCF that corresponds to these minimal polynomials? Is there anything else I would need to do to prove that this answer is ok?","Classify up to similarity all complex $3 \times 3$ matrices A such that A such that ${A^3} = 2{A^2} - A$. Here is what I know: All matrices with complex entries have Jordan canonical forms. If matrices are similar, they have the same Jordan canonical form. This is what I've tried: Let $g(t) = {t^3} - 2{t^2} + t$. Then $g(A) = 0$. So, $p(t)$, the minimal polynomial of $A$, can be: $t, t-1, {(t-1)^2}, t(t-1), t{(t-1)^2}$. Now, I think that we classify all the matrices as similar to one of: $\begin{bmatrix}0&0&0\\0&0&0\\0&0&0\end{bmatrix}$, $\begin{bmatrix}1&0&0\\0&1&0\\0&0&1\end{bmatrix}$, $\begin{bmatrix}1&1&0\\0&1&0\\0&0&1\end{bmatrix}$, $\begin{bmatrix}1&0&0\\0&1&0\\0&0&0\end{bmatrix}$, $\begin{bmatrix}1&0&0\\0&0&0\\0&0&0\end{bmatrix}$, $\begin{bmatrix}1&1&0\\0&1&0\\0&0&0\end{bmatrix}$. And my reasoning for that was just taking each one of the possible minimal polynomials, and finding the possible JCF's for it. But, I'm not very confident in this strategy at all. Is this the right way to go about the problem? If not, what would you suggest trying? If so, is my reasoning correct - can I just find the possible minimal polynomials, and say that each matrix we are looking for is similar to the JCF that corresponds to these minimal polynomials? Is there anything else I would need to do to prove that this answer is ok?",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
41,"Proving ($\left|\left|Ax\right|\right| = \left|\left|x\right|\right|$, for all $x\in\mathbb{C}^n$) $\implies A$ is unitary","Proving (, for all )  is unitary",\left|\left|Ax\right|\right| = \left|\left|x\right|\right| x\in\mathbb{C}^n \implies A,"As the title states, I'm trying to prove that $\left|\left|Ax\right|\right| = \left|\left|x\right|\right|$ for all $x\in\mathbb{C}^n\implies$ $A$ is unitary, where $A$ is a square matrix. This is part of a larger problem and the only thing I 'know' for the purposes of this proof is that $\left|\left|Ax\right|\right| = \left|\left|x\right|\right|$ for all $x\in\mathbb{C}^n$, nothing else, e.g. I don't know that the rows or columns of $A$ are orthonormal. Any hints on where I can start with this? I'm quite stuck. Thank you very much for your time.","As the title states, I'm trying to prove that $\left|\left|Ax\right|\right| = \left|\left|x\right|\right|$ for all $x\in\mathbb{C}^n\implies$ $A$ is unitary, where $A$ is a square matrix. This is part of a larger problem and the only thing I 'know' for the purposes of this proof is that $\left|\left|Ax\right|\right| = \left|\left|x\right|\right|$ for all $x\in\mathbb{C}^n$, nothing else, e.g. I don't know that the rows or columns of $A$ are orthonormal. Any hints on where I can start with this? I'm quite stuck. Thank you very much for your time.",,"['linear-algebra', 'matrices']"
42,Trace of vectors,Trace of vectors,,"Does that sound about right? Given that x is $m\times 1$ and y is $m\times 1$ vectors, show that  $ tr(\mathbf{xy'})=\mathbf{x'y}$. Attempt: By using the property of $tr(\mathbf{A'})=tr(\mathbf{A})$, $tr[\mathbf{xy'}]=tr[(\mathbf{xy'})']$, which is $tr[\mathbf{x'y}]$. But $\mathbf{x'y}$ is a scalar $1\times1$. Thus, $tr[\mathbf{xy'}]=\mathbf{x'y}$.","Does that sound about right? Given that x is $m\times 1$ and y is $m\times 1$ vectors, show that  $ tr(\mathbf{xy'})=\mathbf{x'y}$. Attempt: By using the property of $tr(\mathbf{A'})=tr(\mathbf{A})$, $tr[\mathbf{xy'}]=tr[(\mathbf{xy'})']$, which is $tr[\mathbf{x'y}]$. But $\mathbf{x'y}$ is a scalar $1\times1$. Thus, $tr[\mathbf{xy'}]=\mathbf{x'y}$.",,"['matrices', 'vectors']"
43,If $CD = -DC$ Show that either $C$ or $D$ has no inverse.,If  Show that either  or  has no inverse.,CD = -DC C D,"I'm probably missing something obvious, but how would I go about solving this?","I'm probably missing something obvious, but how would I go about solving this?",,"['linear-algebra', 'matrices']"
44,How do you find the change of coordinates matrix from a given matrix to the standard basis?,How do you find the change of coordinates matrix from a given matrix to the standard basis?,,"I'm not sure how to approach this problem.  The examples I've come across on the internet show how to find the change of coordinates matrix from a matrix to another matrix, such as B to C (for example). I came up with an answer but I'm not sure if it's correct. I started out with the matrix with those three vectors mentioned above: 3    2    1 0    2   -2 6   -4    3 Then I found the inverse which is the following: 1/21    5/21     1/7 2/7    -1/14    -1/7 2/7    -4/7     -1/7 Then I multiplied by the standard basis of a 3x1 vector: 1/21    5/21     1/7         1 2/7    -1/14    -1/7    *    0 2/7    -4/7     -1/7         0 And came up with the following answer: 1/21 2/7 2/7 Is this correct?  Something tells me I'm missing something or perhaps I approached the whole thing incorrectly.","I'm not sure how to approach this problem.  The examples I've come across on the internet show how to find the change of coordinates matrix from a matrix to another matrix, such as B to C (for example). I came up with an answer but I'm not sure if it's correct. I started out with the matrix with those three vectors mentioned above: 3    2    1 0    2   -2 6   -4    3 Then I found the inverse which is the following: 1/21    5/21     1/7 2/7    -1/14    -1/7 2/7    -4/7     -1/7 Then I multiplied by the standard basis of a 3x1 vector: 1/21    5/21     1/7         1 2/7    -1/14    -1/7    *    0 2/7    -4/7     -1/7         0 And came up with the following answer: 1/21 2/7 2/7 Is this correct?  Something tells me I'm missing something or perhaps I approached the whole thing incorrectly.",,"['linear-algebra', 'matrices']"
45,Is there a way to determine the matrix of $\Lambda^k(T)$ given the matrix of $T$?,Is there a way to determine the matrix of  given the matrix of ?,\Lambda^k(T) T,"Let $T$ be an endomorphism of a finite dimensional vector space $V$. Suppose that $(v_1,\ldots v_n)$ is an ordered basis of $V$. And let $[T]$ be the matrix of $T$ with respect to this basis. Is there a way to compute the matrix of $\Lambda^k(T)$ with respect to the obvious basis given $[T]$? The 'obvious' basis is the $\binom{n}{k}$ $k$-wedge tuples from $\{v_1,\ldots,v_n\}$ with increasing indices. There have been many questions on here about how to compute the characteristic polynomial and the coefficients $(-1)^k\text{tr}(\Lambda^k(T))$, but I haven't seen any interest in the matrix of $\Lambda^k(T)$ itself. I suspect it can be built from the minors of $[T]$, but I have no idea how to proceed.","Let $T$ be an endomorphism of a finite dimensional vector space $V$. Suppose that $(v_1,\ldots v_n)$ is an ordered basis of $V$. And let $[T]$ be the matrix of $T$ with respect to this basis. Is there a way to compute the matrix of $\Lambda^k(T)$ with respect to the obvious basis given $[T]$? The 'obvious' basis is the $\binom{n}{k}$ $k$-wedge tuples from $\{v_1,\ldots,v_n\}$ with increasing indices. There have been many questions on here about how to compute the characteristic polynomial and the coefficients $(-1)^k\text{tr}(\Lambda^k(T))$, but I haven't seen any interest in the matrix of $\Lambda^k(T)$ itself. I suspect it can be built from the minors of $[T]$, but I have no idea how to proceed.",,"['linear-algebra', 'abstract-algebra']"
46,What is the intutive explanation of why the notation of matrices is as it is?,What is the intutive explanation of why the notation of matrices is as it is?,,"If I want to solve a system of linear equations, like 2x-y=1 x+2y=4 Then the matrix notation for the same would be: $$ \begin{bmatrix} 2 & -1 \\ 1 & 2  \\ \end{bmatrix} \begin{bmatrix} X\\ Y\\ \end{bmatrix} =  \begin{bmatrix} 1\\ 2\\ \end{bmatrix}$$ I'd like to know how did this notation come into existence? Is this notation intutive for everyone? Or is there any significance of this notation? Or was this just proposed by someone (or a set of people) and then set as the standard?","If I want to solve a system of linear equations, like 2x-y=1 x+2y=4 Then the matrix notation for the same would be: $$ \begin{bmatrix} 2 & -1 \\ 1 & 2  \\ \end{bmatrix} \begin{bmatrix} X\\ Y\\ \end{bmatrix} =  \begin{bmatrix} 1\\ 2\\ \end{bmatrix}$$ I'd like to know how did this notation come into existence? Is this notation intutive for everyone? Or is there any significance of this notation? Or was this just proposed by someone (or a set of people) and then set as the standard?",,"['matrices', 'notation', 'math-history', 'matrix-equations']"
47,When does $x^TAx + c^Tx$ have a global minimum?,When does  have a global minimum?,x^TAx + c^Tx,"This question is closely related to my last question about extended quadratic forms. I figured out a nice criterion, when $$f : \mathbb R^n \rightarrow \mathbb R$$ $$f(x) = x^TAx + c^Tx$$ has a global minimum , where $A \in Mat(n,n,\mathbb R)$ is symmetric, $c \in Vec(n,\mathbb R )$ I want to know, when f(x) has a global minimum. It is clear, that A must be positive semi-definite. In this case, I figured out that f(x) has a global minimum, if and only if $c^T$ is orthogonal to the kernel of A. In other words, for any $x$ with $Ax=0$ , the equation $c^Tx=0$ must hold. Is there an easy proof for this criterion ? Can this result be understood geometrically ?","This question is closely related to my last question about extended quadratic forms. I figured out a nice criterion, when $$f : \mathbb R^n \rightarrow \mathbb R$$ $$f(x) = x^TAx + c^Tx$$ has a global minimum , where $A \in Mat(n,n,\mathbb R)$ is symmetric, $c \in Vec(n,\mathbb R )$ I want to know, when f(x) has a global minimum. It is clear, that A must be positive semi-definite. In this case, I figured out that f(x) has a global minimum, if and only if $c^T$ is orthogonal to the kernel of A. In other words, for any $x$ with $Ax=0$ , the equation $c^Tx=0$ must hold. Is there an easy proof for this criterion ? Can this result be understood geometrically ?",,"['linear-algebra', 'matrices', 'quadratic-forms']"
48,Newly Developed With Details - Describing orthographic projection using simple 2D transformations,Newly Developed With Details - Describing orthographic projection using simple 2D transformations,,"Thanks to Pedro for helping me further develop my question into something tangible. His (most recent) answer below clearly and formally outlines what I am asking. This is similar to this question, except in 3D and involving surfaces. I am doing some graphics work involving rotation matrices. The problem is that I am not just transforming points, I'm also transforming arbitrary surfaces (images, colors, etc.) These surfaces are represented by rectangles and only really support two main transformations, rotation and scaling (along the horizontal and vertical axes). If I have a rotation matrix: $$     M = \left(\begin{matrix}         m_0 & m_1 & m_2 \\         m_3 & m_4 & m_5 \\         m_6 & m_7 & m_8 \\         \end{matrix}\right) $$ That might produce some rotation like this: (just for illustration purposes) It could be said that in order to achieve this shape, a rectangle would go through a certain counter-clockwise rotation and then a certain scaling along the horizontal axis and the vertical axis. Is there any way to figure out what that counter-clockwise angle is and what the scale factors are? Keep in mind that I said that the rotation occurs first, though this is arbitrary and the rotation and scale transformations could happen in any order. EDIT: Hopefully these details grant more clarity. Here's exactly what's happening. There is a set of points (a, b, 0) that represent 3D coordinates. Each of these points have a related surface represented by a width and height. That surface could be an image, a color or any other shape potentially. These surfaces support two main transformations: rotation and scale transformations. The scale transformations occur in the horizontal and vertical axis and the rotations occur around an axis perpendicular to the screen. Assume the plane of the screen is the reference point from which all rotations occur. Also assume that there is a matrix M as defined above that represents the orientation of the 3D coordinate plane on which (a, b, 0) resides. What I am trying to do is project the point (a, b, 0) AND its related surface onto the screen using the aforementioned matrix. Taking that point (a, b, 0), as well as an orientation matrix in as input, I would like to figure out how I should transform the surface so that it appears as it would in 3D on the 2D screen (i.e. how I can transform it so that it looks projected on the screen). The image above is a good example of a surface that while looking 3D, is actually somehow represented by a set of 2D points. I would like to know how to come up with that projection given that I do not have the full point array. My assumption, which I now see may be incorrect, was that you could do that with a rotation and a scale alone. An answer below demonstrated that in fact you need at least another angle called the shear angle. Another piece of information that may be useful: As I mentioned, this is graphics work, so the coordinate axis are actually flipped. The x-axis is the same, but the y-axis is flipped so it becomes more positive as you go down. Hopefully making my question a bit more broad will lead to an answer: What transformations do I need to project a flat 3D surface onto a 2D plane (the screen)?","Thanks to Pedro for helping me further develop my question into something tangible. His (most recent) answer below clearly and formally outlines what I am asking. This is similar to this question, except in 3D and involving surfaces. I am doing some graphics work involving rotation matrices. The problem is that I am not just transforming points, I'm also transforming arbitrary surfaces (images, colors, etc.) These surfaces are represented by rectangles and only really support two main transformations, rotation and scaling (along the horizontal and vertical axes). If I have a rotation matrix: $$     M = \left(\begin{matrix}         m_0 & m_1 & m_2 \\         m_3 & m_4 & m_5 \\         m_6 & m_7 & m_8 \\         \end{matrix}\right) $$ That might produce some rotation like this: (just for illustration purposes) It could be said that in order to achieve this shape, a rectangle would go through a certain counter-clockwise rotation and then a certain scaling along the horizontal axis and the vertical axis. Is there any way to figure out what that counter-clockwise angle is and what the scale factors are? Keep in mind that I said that the rotation occurs first, though this is arbitrary and the rotation and scale transformations could happen in any order. EDIT: Hopefully these details grant more clarity. Here's exactly what's happening. There is a set of points (a, b, 0) that represent 3D coordinates. Each of these points have a related surface represented by a width and height. That surface could be an image, a color or any other shape potentially. These surfaces support two main transformations: rotation and scale transformations. The scale transformations occur in the horizontal and vertical axis and the rotations occur around an axis perpendicular to the screen. Assume the plane of the screen is the reference point from which all rotations occur. Also assume that there is a matrix M as defined above that represents the orientation of the 3D coordinate plane on which (a, b, 0) resides. What I am trying to do is project the point (a, b, 0) AND its related surface onto the screen using the aforementioned matrix. Taking that point (a, b, 0), as well as an orientation matrix in as input, I would like to figure out how I should transform the surface so that it appears as it would in 3D on the 2D screen (i.e. how I can transform it so that it looks projected on the screen). The image above is a good example of a surface that while looking 3D, is actually somehow represented by a set of 2D points. I would like to know how to come up with that projection given that I do not have the full point array. My assumption, which I now see may be incorrect, was that you could do that with a rotation and a scale alone. An answer below demonstrated that in fact you need at least another angle called the shear angle. Another piece of information that may be useful: As I mentioned, this is graphics work, so the coordinate axis are actually flipped. The x-axis is the same, but the y-axis is flipped so it becomes more positive as you go down. Hopefully making my question a bit more broad will lead to an answer: What transformations do I need to project a flat 3D surface onto a 2D plane (the screen)?",,['matrices']
49,Determine if a particular matrix is diagonalizable,Determine if a particular matrix is diagonalizable,,"my teacher gave me this exercise: Determine if this matrix is diagonalizable $ \begin{pmatrix} 1 & 1&1&1\\  1&2&3&4\\  1&-1&2&-2\\  0&0&1&-2  \end{pmatrix} $ I have tried to calculate the characteristic polynomial, that is $-13 + 10 x + x^2 - 3 x^3 + x^4$, but I don't know how to go on.  I tried to look at the roots of the characteristic polynomial with Wolfram Alpha and they are horrible! So I think that must exist an alternative way to do it! Have you got any ideas? Thank you!","my teacher gave me this exercise: Determine if this matrix is diagonalizable $ \begin{pmatrix} 1 & 1&1&1\\  1&2&3&4\\  1&-1&2&-2\\  0&0&1&-2  \end{pmatrix} $ I have tried to calculate the characteristic polynomial, that is $-13 + 10 x + x^2 - 3 x^3 + x^4$, but I don't know how to go on.  I tried to look at the roots of the characteristic polynomial with Wolfram Alpha and they are horrible! So I think that must exist an alternative way to do it! Have you got any ideas? Thank you!",,"['linear-algebra', 'matrices', 'diagonalization']"
50,$AB-BA$ is nilpotent matrix,is nilpotent matrix,AB-BA,"Let $A$ and $B$ be $n\times n$ nilpotent matrices. Show that $AB-BA$ is a nilpotent matrix if and only if $A$ and $B$ share the same eigenvector $\alpha$ . I tried if $A$ and $B$ share the same eigenvector $\alpha$ , then due to $A\alpha=0=B\alpha$ we have $(AB-BA)\alpha=0$ . Then I don't know how to continue ... :(","Let and be nilpotent matrices. Show that is a nilpotent matrix if and only if and share the same eigenvector . I tried if and share the same eigenvector , then due to we have . Then I don't know how to continue ... :(",A B n\times n AB-BA A B \alpha A B \alpha A\alpha=0=B\alpha (AB-BA)\alpha=0,"['linear-algebra', 'matrices']"
51,How to figure out the spectral radius of this matrix,How to figure out the spectral radius of this matrix,,"$$A=\begin{array}{ccc} 0 & 1/2 & 0 & \cdots & 0 \\ 1/2 & 0 & 1/2 &\cdots& 0\\ 0 & 1/2 & 0 & \cdots & 0\\ \vdots & \vdots & \vdots &\ddots & 1/2\\ 0 & 0 & 0 & 1/2 & 0 \end{array}$$ I need to know the spectral radius of $A$ in function of the dimension of the matrix $n$. I know only one way to know the spectral radius, using the definition I find all eigenvalues and then find the maximum. but it very hard if $n$ is changing. Some help please! I did some iteration by $$det(\lambda I-A)$$ $$n=2 \quad r=\frac{\sqrt{1}}{2}$$ $$n=3 \quad r=\frac{\sqrt{2}}{2}$$ for $n=2$ and $n=3$ are correct.  and if we take this rule, for $$n=5\quad \quad r=\frac{\sqrt{4}}{2}$$ but this isn´t true since I know radius for this matrix should be $< 1$ and if we take this rule  for $n=5$ we have  $$\frac{\sqrt4}{2}=1$$ Some hint?","$$A=\begin{array}{ccc} 0 & 1/2 & 0 & \cdots & 0 \\ 1/2 & 0 & 1/2 &\cdots& 0\\ 0 & 1/2 & 0 & \cdots & 0\\ \vdots & \vdots & \vdots &\ddots & 1/2\\ 0 & 0 & 0 & 1/2 & 0 \end{array}$$ I need to know the spectral radius of $A$ in function of the dimension of the matrix $n$. I know only one way to know the spectral radius, using the definition I find all eigenvalues and then find the maximum. but it very hard if $n$ is changing. Some help please! I did some iteration by $$det(\lambda I-A)$$ $$n=2 \quad r=\frac{\sqrt{1}}{2}$$ $$n=3 \quad r=\frac{\sqrt{2}}{2}$$ for $n=2$ and $n=3$ are correct.  and if we take this rule, for $$n=5\quad \quad r=\frac{\sqrt{4}}{2}$$ but this isn´t true since I know radius for this matrix should be $< 1$ and if we take this rule  for $n=5$ we have  $$\frac{\sqrt4}{2}=1$$ Some hint?",,"['matrices', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
52,Using matrices to calculate fibonacci?,Using matrices to calculate fibonacci?,,I have been told a couple of times it possible to calculate the fibonacci sequence much quicker using matrices but I never understood/they never elaborated. Would somebody be able to show how this technique works?,I have been told a couple of times it possible to calculate the fibonacci sequence much quicker using matrices but I never understood/they never elaborated. Would somebody be able to show how this technique works?,,"['matrices', 'fibonacci-numbers']"
53,Explicit formula for inverse of upper triangular Toeplitz matrix inverse,Explicit formula for inverse of upper triangular Toeplitz matrix inverse,,I have $n \times n$ upper triangular matrix $A$ such as  $$   \begin{bmatrix}     x_1 & x_2 & \ldots & x_n \\     0 & x_1 & \ldots & x_{n-1} \\     \vdots & \vdots & \ddots & \vdots \\     0 & 0 & \ldots & x_1    \end{bmatrix}; $$ then i want to obtain explicit form of $a_{ij}$ of $A^{-1}$ where $a_{ij}$ is the element of $i$th row and $j$th column of $A^{-1}$. Please answer or suggest related reference. Thanks in advance.,I have $n \times n$ upper triangular matrix $A$ such as  $$   \begin{bmatrix}     x_1 & x_2 & \ldots & x_n \\     0 & x_1 & \ldots & x_{n-1} \\     \vdots & \vdots & \ddots & \vdots \\     0 & 0 & \ldots & x_1    \end{bmatrix}; $$ then i want to obtain explicit form of $a_{ij}$ of $A^{-1}$ where $a_{ij}$ is the element of $i$th row and $j$th column of $A^{-1}$. Please answer or suggest related reference. Thanks in advance.,,"['linear-algebra', 'matrices', 'determinant', 'inverse']"
54,number of eigenvalues = dimension of eigenspace,number of eigenvalues = dimension of eigenspace,,"Is this true in general? What about: number of negative eigenvalues = dimension of span(eigenectors for the negative eigenvalues)? Or even more generally: number of eigenvalues greater than 4.3 = dimension of span(eigenvectors for the eigenvalues greater than 4.3)? Is (number of negative eigenvalues) here distinct eigenvalues or not? Any help would be much appreciated, as I cna't seem to find a proof anywhere (not even sure if it's true) Thanks!","Is this true in general? What about: number of negative eigenvalues = dimension of span(eigenectors for the negative eigenvalues)? Or even more generally: number of eigenvalues greater than 4.3 = dimension of span(eigenvectors for the eigenvalues greater than 4.3)? Is (number of negative eigenvalues) here distinct eigenvalues or not? Any help would be much appreciated, as I cna't seem to find a proof anywhere (not even sure if it's true) Thanks!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
55,Algorithm to compute maximum permutation sum in matrix,Algorithm to compute maximum permutation sum in matrix,,"Given a matrix $A_{n\times n}$ of real numbers, what fast algorithms do there exist to compute the maximum value of $a_{1,\sigma(1)}+a_{2,\sigma(2)}+\ldots+a_{n,\sigma(n)}$ over all permutations $\sigma$ of $1,2,\ldots,n$? Brute-force would take $O(n!)$ time. Is there a name to this problem/algorithm?","Given a matrix $A_{n\times n}$ of real numbers, what fast algorithms do there exist to compute the maximum value of $a_{1,\sigma(1)}+a_{2,\sigma(2)}+\ldots+a_{n,\sigma(n)}$ over all permutations $\sigma$ of $1,2,\ldots,n$? Brute-force would take $O(n!)$ time. Is there a name to this problem/algorithm?",,"['matrices', 'algorithms', 'permutations']"
56,2x2 Fibonacci matrix singular value decomposition,2x2 Fibonacci matrix singular value decomposition,,"$A = \left[\begin{array}[c]{rr}1 & 1\\1 & 0\end{array}\right]$ I am supposed to find all the eigenvalues and vectors for this matrix so that $Av=σu$ and then form a singular value decomposition $UΣVᵀ$ and show that $A=UΣVᵀ$. I have attempted this so many times and just keep getting so confused with what the question really wants, since I get something very complicated, as I get when I try to solve it. However, when I look at a similar markscheme it's quite simply put, but I don't understand how. I know that I can use $det(AᵀA)$ to get $σ^2$ or I can get σ with $det(A-λI)$. So those give me the same, which are the diagonals of $Σ$ and the rest of $Σ$ consists of zeros. So I get $$\Sigma = \left( \begin{array}{cc}  \sqrt{\frac{1}{2} \left(3+\sqrt{5}\right)} & 0 \\  0 & \sqrt{\frac{1}{2} \left(3-\sqrt{5}\right)} \\ \end{array} \right)$$ and $v_1$ and $v_2$ I calculated to be as follows $v_1$$=$$\left[\begin{array}[c]{r}(1+√5)/2\\1\end{array}\right]$ $v_2$$=$$\left[\begin{array}[c]{r}(1-√5)/2\\1\end{array}\right]$ After that am I not supposed to convert it to the unit vector? That comes out really messy... I just feel like I'm doing something wrong because it's not supposed to be a very complicated question, and if I continue from here it gets way ugly. I'm not sure how I am supposed to show what they're asking for. Also, I believe $u_1=v_1$ and $u_2=-v_2$, but how are those calculated? Another note - can this be done by keeping the eigenvectors as variables instead of substituting the values? Maybe that would not come out as complicated.","$A = \left[\begin{array}[c]{rr}1 & 1\\1 & 0\end{array}\right]$ I am supposed to find all the eigenvalues and vectors for this matrix so that $Av=σu$ and then form a singular value decomposition $UΣVᵀ$ and show that $A=UΣVᵀ$. I have attempted this so many times and just keep getting so confused with what the question really wants, since I get something very complicated, as I get when I try to solve it. However, when I look at a similar markscheme it's quite simply put, but I don't understand how. I know that I can use $det(AᵀA)$ to get $σ^2$ or I can get σ with $det(A-λI)$. So those give me the same, which are the diagonals of $Σ$ and the rest of $Σ$ consists of zeros. So I get $$\Sigma = \left( \begin{array}{cc}  \sqrt{\frac{1}{2} \left(3+\sqrt{5}\right)} & 0 \\  0 & \sqrt{\frac{1}{2} \left(3-\sqrt{5}\right)} \\ \end{array} \right)$$ and $v_1$ and $v_2$ I calculated to be as follows $v_1$$=$$\left[\begin{array}[c]{r}(1+√5)/2\\1\end{array}\right]$ $v_2$$=$$\left[\begin{array}[c]{r}(1-√5)/2\\1\end{array}\right]$ After that am I not supposed to convert it to the unit vector? That comes out really messy... I just feel like I'm doing something wrong because it's not supposed to be a very complicated question, and if I continue from here it gets way ugly. I'm not sure how I am supposed to show what they're asking for. Also, I believe $u_1=v_1$ and $u_2=-v_2$, but how are those calculated? Another note - can this be done by keeping the eigenvectors as variables instead of substituting the values? Maybe that would not come out as complicated.",,"['linear-algebra', 'matrices']"
57,"Conditions for ""$AA^T=A^TA$ implies $A$ symmetric"" to hold.","Conditions for "" implies  symmetric"" to hold.",AA^T=A^TA A,"This claim arose in this question Show that $A$ is symmetric, with $A \in M_n(\mathbb R)$ where it is assumed additionally that $AA^TA$ is symmetric. I'm considering weakening hypotheses. Let $A$ be a real matrix such that $AA^T=A^TA$. This implies that $A$ is normal. According to the spectral theorem (for complex-valued normal matrices), $$A=UDU^*$$ for some complex diagonal matrix $D$ and complex unitary matrix $U$. I am not sure I can infer from here that $D$ and $U$ must have real entries. Nevertheless, if $A$ has real eigenvalues, then $D$ must have real entries. Are real eigenvalues sufficient to prove that $U$ has real entries ? Is there a way to circumvent the use of the spectral theorem ? Can you think of other hypotheses for the result to hold ? Is the claim "" $AA^T=A^TA$ implies $A$ symmetric"" true for any real $A$ without conditions on eigenvalues ? ( EDIT : Answer is no.)","This claim arose in this question Show that $A$ is symmetric, with $A \in M_n(\mathbb R)$ where it is assumed additionally that $AA^TA$ is symmetric. I'm considering weakening hypotheses. Let $A$ be a real matrix such that $AA^T=A^TA$. This implies that $A$ is normal. According to the spectral theorem (for complex-valued normal matrices), $$A=UDU^*$$ for some complex diagonal matrix $D$ and complex unitary matrix $U$. I am not sure I can infer from here that $D$ and $U$ must have real entries. Nevertheless, if $A$ has real eigenvalues, then $D$ must have real entries. Are real eigenvalues sufficient to prove that $U$ has real entries ? Is there a way to circumvent the use of the spectral theorem ? Can you think of other hypotheses for the result to hold ? Is the claim "" $AA^T=A^TA$ implies $A$ symmetric"" true for any real $A$ without conditions on eigenvalues ? ( EDIT : Answer is no.)",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
58,How is the multiplication between a multidimensional tensor with a matrix defined?,How is the multiplication between a multidimensional tensor with a matrix defined?,,"I am thinking this calculation in the following way but I am wondering if it is correct. Can anybody explain to me please? For example, I have a 3-way tensor $T^{u×i×t}$. How do I multiply this tensor with another matrix $M^{j×h}$, and what constraints this matrix must have, say dimensions etc? Is it that the matrix must has one dimension whose length equals to the length of one dimension of the tensor? For example, the tensor is $T^{u×i×t}=T^{5×4×3}$. And a matrix is $M^{3×7}$. So I suppose for the tensor $T^{5×4×3}$ I can slice it into 5 matrices along the $u$ mode, each of which is a matrix $M^{3×4}$. Then I can apply matrix multiplication on each slice matrix $M^{4×3}$ and the matrix $M^{3×7}$. Then finally I get a tensor which is $T^{5×4×7}$ . So, $T^{u×i×t}×M^{t×h}=T^{u×i×h}$ Is this right? However I suppose I can also slice the original tensor $T^{5×4×3}$  into 4 matrices along the $i$ mode. Then I can also get a tensor $T^{5×4×7}$. Will the two tensors be equal to each other? Any references I can read more about it? How about more higher dimensional tensors, e.g. $T^{u×i×t×h×j}$ ?","I am thinking this calculation in the following way but I am wondering if it is correct. Can anybody explain to me please? For example, I have a 3-way tensor $T^{u×i×t}$. How do I multiply this tensor with another matrix $M^{j×h}$, and what constraints this matrix must have, say dimensions etc? Is it that the matrix must has one dimension whose length equals to the length of one dimension of the tensor? For example, the tensor is $T^{u×i×t}=T^{5×4×3}$. And a matrix is $M^{3×7}$. So I suppose for the tensor $T^{5×4×3}$ I can slice it into 5 matrices along the $u$ mode, each of which is a matrix $M^{3×4}$. Then I can apply matrix multiplication on each slice matrix $M^{4×3}$ and the matrix $M^{3×7}$. Then finally I get a tensor which is $T^{5×4×7}$ . So, $T^{u×i×t}×M^{t×h}=T^{u×i×h}$ Is this right? However I suppose I can also slice the original tensor $T^{5×4×3}$  into 4 matrices along the $i$ mode. Then I can also get a tensor $T^{5×4×7}$. Will the two tensors be equal to each other? Any references I can read more about it? How about more higher dimensional tensors, e.g. $T^{u×i×t×h×j}$ ?",,"['matrices', 'tensor-products', 'tensors']"
59,"Diagonalizing the matrix $A$, when $A^2$ is diagonalizable","Diagonalizing the matrix , when  is diagonalizable",A A^2,"If the matrix $A^2$ is diagonalizable and $A$ is invertible, is $A$ diagonalizable? I know it is not true if we leave out the invertibility. For example  if $ A=      \begin{pmatrix}         0 & 1  \\         0 & 0  \\         \end{pmatrix} $ (which is not diagonalizable), then  $A^2=               \begin{pmatrix}         0 & 0  \\         0 & 0  \\         \end{pmatrix} $ (and that is trivially diagonalizable). But what if we require $A$ to be invertible? I believe in that case, answer to my original question is yes, but I'm not able to prove that. Can someone please help me?","If the matrix $A^2$ is diagonalizable and $A$ is invertible, is $A$ diagonalizable? I know it is not true if we leave out the invertibility. For example  if $ A=      \begin{pmatrix}         0 & 1  \\         0 & 0  \\         \end{pmatrix} $ (which is not diagonalizable), then  $A^2=               \begin{pmatrix}         0 & 0  \\         0 & 0  \\         \end{pmatrix} $ (and that is trivially diagonalizable). But what if we require $A$ to be invertible? I believe in that case, answer to my original question is yes, but I'm not able to prove that. Can someone please help me?",,"['linear-algebra', 'matrices', 'diagonalization']"
60,Determinant by applying Gaussian Elimination,Determinant by applying Gaussian Elimination,,"I understand when using Gaussian Elimination you have to get it in ref form (upper triangle) and calculate the product of the diagonal. Additionally you have to keep track of the number of swaps to determine how many times to multiply by negative one. Let us say you have the following matrix:  $\begin{pmatrix} 2&-1\\ -1&-1\\ \end{pmatrix}$ and you scale the top row by 1/2 making $\begin{pmatrix} 1&-1/2\\ -1&-1\\ \end{pmatrix}$ <-this step is not needed, but let us say we do it then you do do P1+P2 into P2 giving $\begin{pmatrix} 1&-1/2\\ 0&-3/2\\ \end{pmatrix}$ so the determinant is -3/2 according to this, but the actual determinant is -3 (you can get that by skipping my unnecessary 2nd step as it is not needed to get the answer in REF. My question is why does scaling the top affect the determinant, I thought the only things that you had to look out for where row swaps to determine the amount of times you multiply by negative 1.","I understand when using Gaussian Elimination you have to get it in ref form (upper triangle) and calculate the product of the diagonal. Additionally you have to keep track of the number of swaps to determine how many times to multiply by negative one. Let us say you have the following matrix:  $\begin{pmatrix} 2&-1\\ -1&-1\\ \end{pmatrix}$ and you scale the top row by 1/2 making $\begin{pmatrix} 1&-1/2\\ -1&-1\\ \end{pmatrix}$ <-this step is not needed, but let us say we do it then you do do P1+P2 into P2 giving $\begin{pmatrix} 1&-1/2\\ 0&-3/2\\ \end{pmatrix}$ so the determinant is -3/2 according to this, but the actual determinant is -3 (you can get that by skipping my unnecessary 2nd step as it is not needed to get the answer in REF. My question is why does scaling the top affect the determinant, I thought the only things that you had to look out for where row swaps to determine the amount of times you multiply by negative 1.",,"['linear-algebra', 'matrices', 'determinant']"
61,Matrix of Linear Transformation by right multiplication,Matrix of Linear Transformation by right multiplication,,"I am trying to solve the following problem: Let $A$ be an $n\times n$ matrix, and let $V$ denote the space of $n$-dimensional row vectors. What is the matrix of the linear operator ‘‘right multiplication by $A$’’ with respect to the standard basis of $V$? I am not sure where to begin with this problem.","I am trying to solve the following problem: Let $A$ be an $n\times n$ matrix, and let $V$ denote the space of $n$-dimensional row vectors. What is the matrix of the linear operator ‘‘right multiplication by $A$’’ with respect to the standard basis of $V$? I am not sure where to begin with this problem.",,"['linear-algebra', 'abstract-algebra', 'matrices']"
62,to prove $f(P^{-1}AP)=P^{-1}f(A)P$ for an $n\times{n}$ square matrix?,to prove  for an  square matrix?,f(P^{-1}AP)=P^{-1}f(A)P n\times{n},"let $f(X)$ be a polynomial and let $A$ be $n\times n$ matrix.We have to show that for any $n\times n$ invertible matrix $P$, $f(P^{-1}AP)=P^{-1}f(A)P$ and that there exist a unitary matrix $U$ such that both $U^*AU$ and $U^*f(A)U$ are upper triangular, where $U^*$ is conjugate transpose of $U$ and $P^{-1}$ is inverse of $P$ ..(m having a little idea about its prove..probably we'll use a result here that is ""let A and B be n*n matrices s.that $AB=BA$ ..if all the eign values of A are distinct then $B$ can be expressed uniquely as a polynomial in $A$ with degree no more then $n-1$"").","let $f(X)$ be a polynomial and let $A$ be $n\times n$ matrix.We have to show that for any $n\times n$ invertible matrix $P$, $f(P^{-1}AP)=P^{-1}f(A)P$ and that there exist a unitary matrix $U$ such that both $U^*AU$ and $U^*f(A)U$ are upper triangular, where $U^*$ is conjugate transpose of $U$ and $P^{-1}$ is inverse of $P$ ..(m having a little idea about its prove..probably we'll use a result here that is ""let A and B be n*n matrices s.that $AB=BA$ ..if all the eign values of A are distinct then $B$ can be expressed uniquely as a polynomial in $A$ with degree no more then $n-1$"").",,"['linear-algebra', 'matrices']"
63,"For a given $A$, find a matrix $B$ such that $B^2=A$","For a given , find a matrix  such that",A B B^2=A,"Find a matrix $B$ such that $B^2=A$, where $$A = \begin{bmatrix}9 & 4 & -8 & 4\\4 & 9 & -4 & 8\\-8 &-4 & 9 & -4\\4 & 8 & -4 & 9\end{bmatrix}.$$ The matrix is symmetric and real, therefore is diagonizable with positive eigenvalues, so after the hard work of diagonizing the matrix the problem becomes trivial. My question is: there has to be an easier way to do it, but what is it? Thanks for your time.","Find a matrix $B$ such that $B^2=A$, where $$A = \begin{bmatrix}9 & 4 & -8 & 4\\4 & 9 & -4 & 8\\-8 &-4 & 9 & -4\\4 & 8 & -4 & 9\end{bmatrix}.$$ The matrix is symmetric and real, therefore is diagonizable with positive eigenvalues, so after the hard work of diagonizing the matrix the problem becomes trivial. My question is: there has to be an easier way to do it, but what is it? Thanks for your time.",,"['linear-algebra', 'matrices']"
64,Bareiss Algorithm,Bareiss Algorithm,,"I need to find the row Echelon form of a large, (sparse,) integer matrix. It seems that the Bareiss algorithm is a prime candidate, but I can't find any resources beyond the Wikipedia page that provide a simple explanation of the algorithm. Are there any resources/recommended books that I should look into?","I need to find the row Echelon form of a large, (sparse,) integer matrix. It seems that the Bareiss algorithm is a prime candidate, but I can't find any resources beyond the Wikipedia page that provide a simple explanation of the algorithm. Are there any resources/recommended books that I should look into?",,"['linear-algebra', 'matrices']"
65,Eigenvalues and eigenvectors of $v v^T$ matrix [duplicate],Eigenvalues and eigenvectors of  matrix [duplicate],v v^T,"This question already has answers here : eigenvalues and eigenvectors of $vv^T$ (2 answers) Closed 6 years ago . If $v$ is a column vector, then how many non-zero eigenvalues does the matrix $vv^T$ have? What are the eigenvalues? What are the corresponding eigenvectors? What are the eigenvectors corresponding to the zero eigenvalues? [Note: This is not homework]","This question already has answers here : eigenvalues and eigenvectors of $vv^T$ (2 answers) Closed 6 years ago . If $v$ is a column vector, then how many non-zero eigenvalues does the matrix $vv^T$ have? What are the eigenvalues? What are the corresponding eigenvectors? What are the eigenvectors corresponding to the zero eigenvalues? [Note: This is not homework]",,"['matrices', 'eigenvalues-eigenvectors']"
66,Rank of the sum of two matrices,Rank of the sum of two matrices,,"I have two square matrices $M$ and $N$ such that $M^2=M$ , $N^2=N$ and $MN=NM=0$. I'd like to prove that $\operatorname{rank} (M+N)=\operatorname{rank} (M)+\operatorname{rank} (N)$. I know that $\operatorname{rank} (M+N)\leq \operatorname{rank} (M)+\operatorname{rank} (N)$. But how will i prove $\operatorname{rank} (M+N)\geq \operatorname{rank} (M)+\operatorname{rank} (N)$? Thanks for any help","I have two square matrices $M$ and $N$ such that $M^2=M$ , $N^2=N$ and $MN=NM=0$. I'd like to prove that $\operatorname{rank} (M+N)=\operatorname{rank} (M)+\operatorname{rank} (N)$. I know that $\operatorname{rank} (M+N)\leq \operatorname{rank} (M)+\operatorname{rank} (N)$. But how will i prove $\operatorname{rank} (M+N)\geq \operatorname{rank} (M)+\operatorname{rank} (N)$? Thanks for any help",,"['linear-algebra', 'matrices', 'matrix-rank']"
67,Existence of Inverse of a Matrix,Existence of Inverse of a Matrix,,"Let $A_{m\times n}$ be an $m\times n$ matrix (over a field $k$). Suppose there exist matrices $B_{n\times m}$ and $C_{n\times m}$ such that $AB=I_{m}$ and $CA=I_{n}$. We should expect first that $m=n$ and then $B=C$. This can be proved by considering the matrices as linear transformations on appropriate vector spaces, and using rank-nullity theorem. But, while studying a book named ""Matrices"", I raised Question: Can we prove $m=n$ by considering ""only matrices"" and not linear transformations and vector spaces? (This question may have been appeared in stackexchange, but I didn't find this question. )","Let $A_{m\times n}$ be an $m\times n$ matrix (over a field $k$). Suppose there exist matrices $B_{n\times m}$ and $C_{n\times m}$ such that $AB=I_{m}$ and $CA=I_{n}$. We should expect first that $m=n$ and then $B=C$. This can be proved by considering the matrices as linear transformations on appropriate vector spaces, and using rank-nullity theorem. But, while studying a book named ""Matrices"", I raised Question: Can we prove $m=n$ by considering ""only matrices"" and not linear transformations and vector spaces? (This question may have been appeared in stackexchange, but I didn't find this question. )",,"['linear-algebra', 'matrices']"
68,Jacobian of an inverse,Jacobian of an inverse,,"Suppose that we have an invertible map $T(u,v)=(x,y)$ . The Jacobian of $T$ is given by $    \text{Jac}(T)=   \left| {\begin{array}{cc}    x_u & x_v \\    y_u & y_v \\   \end{array} } \right|$ . Now if we consider the inverse map of $T$ , i.e. $T^{-1}(x,y)=(u,v)$ , its Jacobian is given by $    \text{Jac}(T^{-1})=   \left| {\begin{array}{cc}    u_x & u_y \\    v_x & v_y \\   \end{array} } \right|$ . Since $H=TT^{-1}=I$ is the mapping $H(u,v)=(u,v)$ , we expect that $    \text{Jac}(H)=   \left| {\begin{array}{cc}    1 & 0 \\    0 & 1 \\   \end{array} } \right|$ . I know that $1=\frac{du}{du}=\frac{dv}{dv}$ and that $0=\frac{du}{dv}=\frac{dv}{du}$ but I cannot seem to figure out how multiplying the two matrices above would give the identity matrix. Doesn't look like the right chain rule to me...any idea where I went wrong?","Suppose that we have an invertible map . The Jacobian of is given by . Now if we consider the inverse map of , i.e. , its Jacobian is given by . Since is the mapping , we expect that . I know that and that but I cannot seem to figure out how multiplying the two matrices above would give the identity matrix. Doesn't look like the right chain rule to me...any idea where I went wrong?","T(u,v)=(x,y) T 
   \text{Jac}(T)=
  \left| {\begin{array}{cc}
   x_u & x_v \\
   y_u & y_v \\
  \end{array} } \right| T T^{-1}(x,y)=(u,v) 
   \text{Jac}(T^{-1})=
  \left| {\begin{array}{cc}
   u_x & u_y \\
   v_x & v_y \\
  \end{array} } \right| H=TT^{-1}=I H(u,v)=(u,v) 
   \text{Jac}(H)=
  \left| {\begin{array}{cc}
   1 & 0 \\
   0 & 1 \\
  \end{array} } \right| 1=\frac{du}{du}=\frac{dv}{dv} 0=\frac{du}{dv}=\frac{dv}{du}","['calculus', 'matrices', 'transformation']"
69,Find The Eigenvalues and Eigenvectors of the Hermitian Matrix,Find The Eigenvalues and Eigenvectors of the Hermitian Matrix,,"Find the eigenvalues and eigenvectors of the $2\times2$ hermitian matrix. $$\pmatrix{\epsilon_1&|V|e^{i\alpha}\\     |V|e^{-i\alpha}&\epsilon_2}$$ I know to find eigenvalues, you use $|A-\lambda I|$, but this is giving me difficult results to find an exact value for $\lambda$. $V$, $\epsilon_1$, $\epsilon_2$, $\alpha$ are all constants.","Find the eigenvalues and eigenvectors of the $2\times2$ hermitian matrix. $$\pmatrix{\epsilon_1&|V|e^{i\alpha}\\     |V|e^{-i\alpha}&\epsilon_2}$$ I know to find eigenvalues, you use $|A-\lambda I|$, but this is giving me difficult results to find an exact value for $\lambda$. $V$, $\epsilon_1$, $\epsilon_2$, $\alpha$ are all constants.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
70,Einstein Notation for product of stacked matrices,Einstein Notation for product of stacked matrices,,"Background Information: I recently started using the Einstein summation notation to express certain operations over an ""image"" $\mathbf{A}$ where to each pixel a square matrix is attached. That is, an array of the shape $\left[n_{row}\times n_{col}\times n \times n\right]$. Now, if I want to represent the multiplication of each ""submatrix"" $A_{pq}$ of size $n \times n$ by a matrix $\mathbf{B}$ of compatible size, I can express it as: $A^{'}_{pqij} = B_{ik}A_{pqkj}$. The problem: I have an array $\mathbf{B}$ of the same size as $\mathbf{A}$ and I want to compute a third array $\mathbf{A}^{'}$ where each pixel $\mathbf{A}_{pix}^{'}$ at the first two indices $p$ and $q$ corresponds to the matrix multiplication of matrix $\mathbf{A}_{pix}$ with $\mathbf{B}_{pix}$ in array $\mathbf{B}$ at the same indices $p$ and $q$. In the notation of linear algebra, for each $n \times n$ matrix indicized by $p$ and $q$ I want: $A_{pix}^{'} = B_{pix}A_{pix}$ Attempt : I initially tried something of the form: $A_{mnloij}^{'} = B_{mnik}A_{lokj}$ but if I think this will compute the submatrix multiplication at all combinations of indices $m,l,n,o$. The Question in Brief: Is there a way, using the Einstein summation notation, to compute the sum: $A_{pqij}^{'} = B_{mnik}A_{lokj}$ for the combinations of indices where $(m,n)=(l,o)=\left(p,q\right)$ only? Additional Attempt: In the worst case, I suppose that I have to first compute the expression above and then only slice the array in order to select the ""diagonals"" where $(m,n)=(l,o)$.","Background Information: I recently started using the Einstein summation notation to express certain operations over an ""image"" $\mathbf{A}$ where to each pixel a square matrix is attached. That is, an array of the shape $\left[n_{row}\times n_{col}\times n \times n\right]$. Now, if I want to represent the multiplication of each ""submatrix"" $A_{pq}$ of size $n \times n$ by a matrix $\mathbf{B}$ of compatible size, I can express it as: $A^{'}_{pqij} = B_{ik}A_{pqkj}$. The problem: I have an array $\mathbf{B}$ of the same size as $\mathbf{A}$ and I want to compute a third array $\mathbf{A}^{'}$ where each pixel $\mathbf{A}_{pix}^{'}$ at the first two indices $p$ and $q$ corresponds to the matrix multiplication of matrix $\mathbf{A}_{pix}$ with $\mathbf{B}_{pix}$ in array $\mathbf{B}$ at the same indices $p$ and $q$. In the notation of linear algebra, for each $n \times n$ matrix indicized by $p$ and $q$ I want: $A_{pix}^{'} = B_{pix}A_{pix}$ Attempt : I initially tried something of the form: $A_{mnloij}^{'} = B_{mnik}A_{lokj}$ but if I think this will compute the submatrix multiplication at all combinations of indices $m,l,n,o$. The Question in Brief: Is there a way, using the Einstein summation notation, to compute the sum: $A_{pqij}^{'} = B_{mnik}A_{lokj}$ for the combinations of indices where $(m,n)=(l,o)=\left(p,q\right)$ only? Additional Attempt: In the worst case, I suppose that I have to first compute the expression above and then only slice the array in order to select the ""diagonals"" where $(m,n)=(l,o)$.",,"['linear-algebra', 'matrices', 'multilinear-algebra']"
71,Find all matrices that satisfy $\mathrm B \mathrm A = \mathrm I_2$,Find all matrices that satisfy,\mathrm B \mathrm A = \mathrm I_2,"Given the matrix $$A=\begin{pmatrix}1&8\\3&5\\2&2\\ \end{pmatrix}$$ find all $2 \times 3$ matrices in $B \in M_{2 \times 3}(\mathbb R)$ with $BA=I_2$. Here's what I did: $$\begin{pmatrix}a&b&c\\d&e&f\\ \end{pmatrix} \begin{pmatrix}1&8\\3&5\\2&2\\ \end{pmatrix} = \begin{pmatrix}1&0\\0&1\\ \end{pmatrix}$$ and then multiplying things out: $$\begin{pmatrix} {a+3b+2c}&{8a+5b+2c}\\{d+3e+2f}&{8d+5e+2f}\\ \end{pmatrix} = \begin{pmatrix}1&0\\0&1\\ \end{pmatrix}$$ So would I just set $$a+3b+2c=1$$ $$8a+5b+2c=0$$ $$d+3e+2f=0$$ $$8d+5e+2f=1$$ But then this gives $4$ equations in $6$ unknowns, so wouldn't there be infinitely many solutions? Did I do this correctly? Matrices aren't my strong point... Thanks.","Given the matrix $$A=\begin{pmatrix}1&8\\3&5\\2&2\\ \end{pmatrix}$$ find all $2 \times 3$ matrices in $B \in M_{2 \times 3}(\mathbb R)$ with $BA=I_2$. Here's what I did: $$\begin{pmatrix}a&b&c\\d&e&f\\ \end{pmatrix} \begin{pmatrix}1&8\\3&5\\2&2\\ \end{pmatrix} = \begin{pmatrix}1&0\\0&1\\ \end{pmatrix}$$ and then multiplying things out: $$\begin{pmatrix} {a+3b+2c}&{8a+5b+2c}\\{d+3e+2f}&{8d+5e+2f}\\ \end{pmatrix} = \begin{pmatrix}1&0\\0&1\\ \end{pmatrix}$$ So would I just set $$a+3b+2c=1$$ $$8a+5b+2c=0$$ $$d+3e+2f=0$$ $$8d+5e+2f=1$$ But then this gives $4$ equations in $6$ unknowns, so wouldn't there be infinitely many solutions? Did I do this correctly? Matrices aren't my strong point... Thanks.",,"['linear-algebra', 'matrices', 'systems-of-equations', 'matrix-equations']"
72,Can you factor out vectors?,Can you factor out vectors?,,"My prof introduced eigenvalues to us today: Let $A$ be an $n \times n$ matrix. If there a scalar $\lambda$ and an $n-1$ non-zero column vector $u$ , then $$Au = \lambda u$$ then $\lambda$ is called an eigenvalue and $u$ is called an eigenvector. $$Au - \lambda u = 0$$ $$\implies (A - \lambda I)u = 0$$ $$\implies \det(A - \lambda I) = 0$$ How did he get from $Au - \lambda u = 0$ to $\implies (A - \lambda I)u = 0$ ? It looks like he factored out the vector, but I thought you could only factor out constants? If you can factor out vectors can you explain why?","My prof introduced eigenvalues to us today: Let be an matrix. If there a scalar and an non-zero column vector , then then is called an eigenvalue and is called an eigenvector. How did he get from to ? It looks like he factored out the vector, but I thought you could only factor out constants? If you can factor out vectors can you explain why?",A n \times n \lambda n-1 u Au = \lambda u \lambda u Au - \lambda u = 0 \implies (A - \lambda I)u = 0 \implies \det(A - \lambda I) = 0 Au - \lambda u = 0 \implies (A - \lambda I)u = 0,"['linear-algebra', 'matrices', 'vector-spaces', 'eigenvalues-eigenvectors']"
73,"A question regarding $\,3 \times 4$ matrices",A question regarding  matrices,"\,3 \times 4","Good day, I'm currently studying for an exam and need to learn about matrices too. Well, since I'm not good at English I'll just write what I've done so far. Below is a photo showing the full sheet of paper with the steps I did so far. The thing I'm wondering about is, that I'd have four unknown variables $(w, x, y, z)$ after writing down the equations, but my matrix has got 3 rows only. Therefor I'm wondering how I'm supposed to find a generic solution for A*x = b $$A=\begin{bmatrix}0&2&-2&3\\ 1&3&-1&2\\ 2&3&1&0 \end{bmatrix} , b=\begin{bmatrix}-1\\ -2\\ 0 \end{bmatrix}$$ $$A=\begin{bmatrix}1&3&-1&2&-2\\ 0&1&-1&3/2&-1/2\\ 0&0&0&1/2&-3/2 \end{bmatrix} $$ $$z=-3 $$  $$\operatorname{rank}(A)=3 $$ So I actually solved for $z$ but I can't seem to solve for the other unknown variables (or can I?). The thing that confuses me is the fact this matrix is $3 \times 4$ now instead of $3 \times 3$. Any tips would be very much appreciated! Click image with middle-mouse-button for full size.","Good day, I'm currently studying for an exam and need to learn about matrices too. Well, since I'm not good at English I'll just write what I've done so far. Below is a photo showing the full sheet of paper with the steps I did so far. The thing I'm wondering about is, that I'd have four unknown variables $(w, x, y, z)$ after writing down the equations, but my matrix has got 3 rows only. Therefor I'm wondering how I'm supposed to find a generic solution for A*x = b $$A=\begin{bmatrix}0&2&-2&3\\ 1&3&-1&2\\ 2&3&1&0 \end{bmatrix} , b=\begin{bmatrix}-1\\ -2\\ 0 \end{bmatrix}$$ $$A=\begin{bmatrix}1&3&-1&2&-2\\ 0&1&-1&3/2&-1/2\\ 0&0&0&1/2&-3/2 \end{bmatrix} $$ $$z=-3 $$  $$\operatorname{rank}(A)=3 $$ So I actually solved for $z$ but I can't seem to solve for the other unknown variables (or can I?). The thing that confuses me is the fact this matrix is $3 \times 4$ now instead of $3 \times 3$. Any tips would be very much appreciated! Click image with middle-mouse-button for full size.",,['matrices']
74,Differentiating second order term of Taylor polynomial (multivariable),Differentiating second order term of Taylor polynomial (multivariable),,"I am trying to derive Newton step in an iterative optimization . I know the step is: $$\Delta x=-H^{-1}g$$ where H is Hessian and $g$ is gradient of a vector function $f(x)$ at $x$. I also know the step is derived from second order Taylor expansion. For example, let's have a vector fuction $f(x)$ which is $\mathbb{R}^{3}\to\mathbb{R}$. Its second-order Taylor expansion is: $$f(x+\Delta x)\approx f(x)+g^{T}\Delta x+ \frac{1}{2}(\Delta x)^{T}H(\Delta x)$$ where $g$ is gradient and $H$ is Hessian of $f$ at $x$. Taking partial derivative with respect to $\Delta x$ should be: $$\frac{\partial f(x+\Delta x)}{\partial \Delta x}=g^{T}+H(\Delta x)$$ The first term is clear since $g^{T}$ is multiplier of $\Delta x$. But why $$\frac{\partial}{\partial\Delta x} \frac{1}{2}(\Delta x)^{T}H(\Delta x)=H(\Delta x)$$ ? Using product rule leads me to a different result (ommiting the $\frac{1}{2}$ for more clarity): $$\frac{\partial}{\partial\Delta x} (\Delta x)^{T}H(\Delta x)= \\ =\frac{\partial(\Delta x)^{T}}{\partial\Delta x}H(\Delta x)+(\Delta x)^{T}\frac{\partial H}{\partial\Delta x}(\Delta x)+(\Delta x)^{T}H\frac{\partial\Delta x}{\Delta x}= \\ =1\cdot H\cdot\Delta x + 0 + (\Delta x)^{T}H\cdot 1= \\ =H\cdot\Delta x + (\Delta x)^{T}H$$","I am trying to derive Newton step in an iterative optimization . I know the step is: $$\Delta x=-H^{-1}g$$ where H is Hessian and $g$ is gradient of a vector function $f(x)$ at $x$. I also know the step is derived from second order Taylor expansion. For example, let's have a vector fuction $f(x)$ which is $\mathbb{R}^{3}\to\mathbb{R}$. Its second-order Taylor expansion is: $$f(x+\Delta x)\approx f(x)+g^{T}\Delta x+ \frac{1}{2}(\Delta x)^{T}H(\Delta x)$$ where $g$ is gradient and $H$ is Hessian of $f$ at $x$. Taking partial derivative with respect to $\Delta x$ should be: $$\frac{\partial f(x+\Delta x)}{\partial \Delta x}=g^{T}+H(\Delta x)$$ The first term is clear since $g^{T}$ is multiplier of $\Delta x$. But why $$\frac{\partial}{\partial\Delta x} \frac{1}{2}(\Delta x)^{T}H(\Delta x)=H(\Delta x)$$ ? Using product rule leads me to a different result (ommiting the $\frac{1}{2}$ for more clarity): $$\frac{\partial}{\partial\Delta x} (\Delta x)^{T}H(\Delta x)= \\ =\frac{\partial(\Delta x)^{T}}{\partial\Delta x}H(\Delta x)+(\Delta x)^{T}\frac{\partial H}{\partial\Delta x}(\Delta x)+(\Delta x)^{T}H\frac{\partial\Delta x}{\Delta x}= \\ =1\cdot H\cdot\Delta x + 0 + (\Delta x)^{T}H\cdot 1= \\ =H\cdot\Delta x + (\Delta x)^{T}H$$",,"['linear-algebra', 'matrices', 'multivariable-calculus', 'taylor-expansion']"
75,Backward stable algorithm,Backward stable algorithm,,"Assume we have fixed unitary matrices $Q_1, \dots, Q_k \in \mathbb{C}^{m,m}$ and a matrix $A \in \mathbb{C}^{m,n}$ which can be perturbed. How can we proof that the algorithm on computing  the product $Q_k \cdots Q_1 A$ from left to right is backward stable? And what if the $Q$-matrices are not unitary anymore?","Assume we have fixed unitary matrices $Q_1, \dots, Q_k \in \mathbb{C}^{m,m}$ and a matrix $A \in \mathbb{C}^{m,n}$ which can be perturbed. How can we proof that the algorithm on computing  the product $Q_k \cdots Q_1 A$ from left to right is backward stable? And what if the $Q$-matrices are not unitary anymore?",,"['matrices', 'algorithms', 'numerical-linear-algebra', 'floating-point']"
76,Find the number of $n$ by $n$ matrices conjugate to a diagonal matrix [closed],Find the number of  by  matrices conjugate to a diagonal matrix [closed],n n,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question (a) Find the number of matrices of size $n$ by $n$ over the field of two elements which are conjugate to a diagonal matrix. What is the answer for $n = 4$? (b) What is the number of $n$ by $n$ matrices conjugate to a diagonal matrix over any finite field $F_q$? Any help or approaches to this problem would be very much appreciated!","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question (a) Find the number of matrices of size $n$ by $n$ over the field of two elements which are conjugate to a diagonal matrix. What is the answer for $n = 4$? (b) What is the number of $n$ by $n$ matrices conjugate to a diagonal matrix over any finite field $F_q$? Any help or approaches to this problem would be very much appreciated!",,"['linear-algebra', 'abstract-algebra']"
77,Finding the associated matrix of a linear transformation to calculate the characteristic polynomial,Finding the associated matrix of a linear transformation to calculate the characteristic polynomial,,"Let $T : M_{n \times n}(\Bbb R) \to M_{n \times n}(\Bbb R)$ be the function given by $T(A)=A^t$ (the transpose of $A$). I need to find the minimal polynomial and the characteristic polynomial of $T$. So, to find the characteristic polynomial, I'm trying to find the associated matrix of $T$. I did it for the case $n=2$. The coordinates of a matrix $\left( \begin{array}{ccc} a & b \\ c & d \end{array} \right)$ in the canonical basis $\beta$ is $[X]_\beta=\left( \begin{array}{c} a \\ b \\ c \\ d \end{array} \right)$. Let $$A = \left( \begin{array}{ccc} 1 & 0 & 0 & 0 \\ 0 & 0 & 1 &0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 &0 & 1 \end{array} \right)$$ Then $$A[X]_\beta=[A^t]_\beta==\left( \begin{array}{c} a \\ c \\ b \\ d \end{array} \right).$$ So, $A$ is the associated matrix of $T$. I know I can do the same for any $n$, but I don't know how to generalize it, and I need it to find the characteristic polynomial, solving $\det (\lambda I-A)=0$. Maybe there's an easier way to find the characteristic polynomial, if you know it, please let me know. If not, how can I generaize this matrix $A$ for any $n$ to find $\det (\lambda I-A)=0$? Thanks so much for your help,","Let $T : M_{n \times n}(\Bbb R) \to M_{n \times n}(\Bbb R)$ be the function given by $T(A)=A^t$ (the transpose of $A$). I need to find the minimal polynomial and the characteristic polynomial of $T$. So, to find the characteristic polynomial, I'm trying to find the associated matrix of $T$. I did it for the case $n=2$. The coordinates of a matrix $\left( \begin{array}{ccc} a & b \\ c & d \end{array} \right)$ in the canonical basis $\beta$ is $[X]_\beta=\left( \begin{array}{c} a \\ b \\ c \\ d \end{array} \right)$. Let $$A = \left( \begin{array}{ccc} 1 & 0 & 0 & 0 \\ 0 & 0 & 1 &0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 &0 & 1 \end{array} \right)$$ Then $$A[X]_\beta=[A^t]_\beta==\left( \begin{array}{c} a \\ c \\ b \\ d \end{array} \right).$$ So, $A$ is the associated matrix of $T$. I know I can do the same for any $n$, but I don't know how to generalize it, and I need it to find the characteristic polynomial, solving $\det (\lambda I-A)=0$. Maybe there's an easier way to find the characteristic polynomial, if you know it, please let me know. If not, how can I generaize this matrix $A$ for any $n$ to find $\det (\lambda I-A)=0$? Thanks so much for your help,",,"['linear-algebra', 'matrices', 'linear-transformations', 'characteristic-polynomial']"
78,What are non-orthogonal eigenvectors?,What are non-orthogonal eigenvectors?,,"Given a symmetric matrix $A$, the maximum of the trace, $Tr(Z^TAZ)$ under the assumption that $Z^TZ=I$ occurs when $Z$ has the eigenvectors of $A$, as $Tr(U^TAU)= \lambda_1 +\lambda_2+...\lambda_ d$ where $Z\in\mathbb{R}^{n\times d}$. I know that the eigen vectors being the solution is as a result of the Courant minimax principle. q1) Now, I have faintly heard about non-orthogonal eigenvectors, and am very curious to know, why they are called so? Is this because they form a basis of the eigen-space of $A$ and are still non-orthogonal? What is its relation with the Courant Minimax / Courant Fischer characterization, if any? q2) How does it fit into the trace maximization formulation given above? Especially, if there is no other constraint, when the orthogonality is dropped, doesn't the problem become unbounded or ill-posed? if so, what are these non-orthogonal eigenvectors, optimizing? q3) If the matrix $A$ was not symmetric or non-normal, then what are its non-orthogonal eigenvectors solving for? or may be is this connected it to an SVD instead in this case? q4) When i Google, for applications of non-orthogonal eigenvectors, i just find nothing. What are its uses!? Please restrict your answers to be within the matrix algebra setting as much as possible, unless it strictly requires discussing it through other fields of mathematics.","Given a symmetric matrix $A$, the maximum of the trace, $Tr(Z^TAZ)$ under the assumption that $Z^TZ=I$ occurs when $Z$ has the eigenvectors of $A$, as $Tr(U^TAU)= \lambda_1 +\lambda_2+...\lambda_ d$ where $Z\in\mathbb{R}^{n\times d}$. I know that the eigen vectors being the solution is as a result of the Courant minimax principle. q1) Now, I have faintly heard about non-orthogonal eigenvectors, and am very curious to know, why they are called so? Is this because they form a basis of the eigen-space of $A$ and are still non-orthogonal? What is its relation with the Courant Minimax / Courant Fischer characterization, if any? q2) How does it fit into the trace maximization formulation given above? Especially, if there is no other constraint, when the orthogonality is dropped, doesn't the problem become unbounded or ill-posed? if so, what are these non-orthogonal eigenvectors, optimizing? q3) If the matrix $A$ was not symmetric or non-normal, then what are its non-orthogonal eigenvectors solving for? or may be is this connected it to an SVD instead in this case? q4) When i Google, for applications of non-orthogonal eigenvectors, i just find nothing. What are its uses!? Please restrict your answers to be within the matrix algebra setting as much as possible, unless it strictly requires discussing it through other fields of mathematics.",,"['linear-algebra', 'matrices', 'optimization', 'vector-spaces', 'numerical-linear-algebra']"
79,Covariance matrix always positive semidefinite?,Covariance matrix always positive semidefinite?,,"I actually was perusing here right now to see if anything could explain a result I have been getting - of a covariance matrix which has a negative eigenvalue, yet the correlation matrix does not - but got a big surprise when I see all these statements that covariance matrices are always positive semidefinite. I have certainly not been getting this result. I get LOTS of negative eigenvalues when using the covariance matrix of things that are pairwise at least rather correlated. For instance, the correlation between various exponentially weighted averages of random walks with different time constants. I only need to get into the 30s of them with time constants increasing in a geometric sequence with a factor of 1.04 (in other words, 1, 1.04, 1.04^2) and BAM. Negative eigenvalues. Very SMALL negative eigenvalues, like 10^-6, but negative. And it's not matlab calculating eigenvalues and eigenvectors wrong either, I try premultiplying and postmultiplying the matrix with an eigenvector for a negative eigenvalue and the result is a negative number. And don't even get me started on covariance matrices derived from actual observations rather than abstract functions, so that the ""observed"" covariance and correlation, which is the covariance divided by the square root of the corresponding variances of the 2 variables it's a covariance between, is not the underlying covariance or correlation of the actual random variable. In other words, say you have a particle moving around, and it has some mean square change in its position after a given amount of time. When the value which is observed over a period of time is used as the actual value - for instance, the mean square amount it moved in a millisecond, averaged over a period of a second - then just by luck it may not be the real mean square change in position after a millisecond, maybe it moved a little more or less than usual over that second. Negative eigenvalues are just frigging everywhere. So what is the deal then? Is it all numerical subtraction error? If I did it all with enough precision, would matlab find it to be positive definite always, when there is nothing linearly dependent (eigenvalues can be 0, but not negative)?","I actually was perusing here right now to see if anything could explain a result I have been getting - of a covariance matrix which has a negative eigenvalue, yet the correlation matrix does not - but got a big surprise when I see all these statements that covariance matrices are always positive semidefinite. I have certainly not been getting this result. I get LOTS of negative eigenvalues when using the covariance matrix of things that are pairwise at least rather correlated. For instance, the correlation between various exponentially weighted averages of random walks with different time constants. I only need to get into the 30s of them with time constants increasing in a geometric sequence with a factor of 1.04 (in other words, 1, 1.04, 1.04^2) and BAM. Negative eigenvalues. Very SMALL negative eigenvalues, like 10^-6, but negative. And it's not matlab calculating eigenvalues and eigenvectors wrong either, I try premultiplying and postmultiplying the matrix with an eigenvector for a negative eigenvalue and the result is a negative number. And don't even get me started on covariance matrices derived from actual observations rather than abstract functions, so that the ""observed"" covariance and correlation, which is the covariance divided by the square root of the corresponding variances of the 2 variables it's a covariance between, is not the underlying covariance or correlation of the actual random variable. In other words, say you have a particle moving around, and it has some mean square change in its position after a given amount of time. When the value which is observed over a period of time is used as the actual value - for instance, the mean square amount it moved in a millisecond, averaged over a period of a second - then just by luck it may not be the real mean square change in position after a millisecond, maybe it moved a little more or less than usual over that second. Negative eigenvalues are just frigging everywhere. So what is the deal then? Is it all numerical subtraction error? If I did it all with enough precision, would matlab find it to be positive definite always, when there is nothing linearly dependent (eigenvalues can be 0, but not negative)?",,"['matrices', 'statistics']"
80,How is this matrix called (two diagonals)?,How is this matrix called (two diagonals)?,,I need to write an algorithm for solving this matrix but I wanted to first make a search online and that's why I need its name.,I need to write an algorithm for solving this matrix but I wanted to first make a search online and that's why I need its name.,,"['matrices', 'algorithms', 'diagonalization']"
81,"How show that if Matrix A is not square, it cannot have an inverse.","How show that if Matrix A is not square, it cannot have an inverse.",,"How to show that if How A is not square, it cannot have an inverse. Why is the the case and how can I prove it?","How to show that if How A is not square, it cannot have an inverse. Why is the the case and how can I prove it?",,['matrices']
82,Gradient of a scalar function with respect to a matrix,Gradient of a scalar function with respect to a matrix,,"I need to calculate $\dfrac{\partial}{\partial K}f(K)$, with: $$ f(K)=-\frac{1}{2}(u-Kx)^T\Sigma^{-1}(u-Kx)$$ $K$ and $\Sigma$ are $n\times n$ matrices, $\Sigma$ is symmetric, $u$ and $x$ are column vectors of size $n$. The result should be a matrix like: $$ \begin{bmatrix} \frac{\partial f(K)}{\partial k_{11}} & \frac{\partial f(K)}{\partial k_{12}} & \ldots \\ \frac{\partial f(K)}{\partial k_{21}} & \ldots & \ldots \\ \ldots & \ldots & \ldots \end{bmatrix} $$ Am I right? Following Petersen's Matrix Cookbook, I obtain the following matrix: $$ \Sigma^{-1}(u-Kx)x^T $$ My problem is that, choosing both $K$ and $\Sigma$ $2 \times 2$ diagonal, I get two different results: if I derive it step by step, that is finding the scalar $f(K)$ and then deriving wrt of all $k_{ij}$ I obtain this matrix: $$ \begin{pmatrix} \frac{(u_1-k_1x_1)x_1}{\sigma_1^2} & 0 \\ 0 & \frac{(u_2-k_2x_2)x_2}{\sigma_2^2} \end{pmatrix} $$ following Petersen's formula: $$ \begin{pmatrix} \frac{(u_1-k_1x_1)x_1}{\sigma_1^2} & \frac{(u_1-k_1x_1)x_2}{\sigma_1^2} \\ \frac{(u_2-k_2x_2)x_1}{\sigma_2^2} & \frac{(u_2-k_2x_2)x_2}{\sigma_2^2} \end{pmatrix} $$ What am I doing wrong?","I need to calculate $\dfrac{\partial}{\partial K}f(K)$, with: $$ f(K)=-\frac{1}{2}(u-Kx)^T\Sigma^{-1}(u-Kx)$$ $K$ and $\Sigma$ are $n\times n$ matrices, $\Sigma$ is symmetric, $u$ and $x$ are column vectors of size $n$. The result should be a matrix like: $$ \begin{bmatrix} \frac{\partial f(K)}{\partial k_{11}} & \frac{\partial f(K)}{\partial k_{12}} & \ldots \\ \frac{\partial f(K)}{\partial k_{21}} & \ldots & \ldots \\ \ldots & \ldots & \ldots \end{bmatrix} $$ Am I right? Following Petersen's Matrix Cookbook, I obtain the following matrix: $$ \Sigma^{-1}(u-Kx)x^T $$ My problem is that, choosing both $K$ and $\Sigma$ $2 \times 2$ diagonal, I get two different results: if I derive it step by step, that is finding the scalar $f(K)$ and then deriving wrt of all $k_{ij}$ I obtain this matrix: $$ \begin{pmatrix} \frac{(u_1-k_1x_1)x_1}{\sigma_1^2} & 0 \\ 0 & \frac{(u_2-k_2x_2)x_2}{\sigma_2^2} \end{pmatrix} $$ following Petersen's formula: $$ \begin{pmatrix} \frac{(u_1-k_1x_1)x_1}{\sigma_1^2} & \frac{(u_1-k_1x_1)x_2}{\sigma_1^2} \\ \frac{(u_2-k_2x_2)x_1}{\sigma_2^2} & \frac{(u_2-k_2x_2)x_2}{\sigma_2^2} \end{pmatrix} $$ What am I doing wrong?",,"['calculus', 'matrices', 'derivatives', 'partial-derivative']"
83,Does same characteristic polynomial and same rank imply similar?,Does same characteristic polynomial and same rank imply similar?,,Are two matrices with the same characteristic polynomial and the same rank necessarily similar? Where can I find the proof for such a thing?,Are two matrices with the same characteristic polynomial and the same rank necessarily similar? Where can I find the proof for such a thing?,,"['linear-algebra', 'matrices']"
84,Rational Canonical form,Rational Canonical form,,Could anyone explain me in detail how to find out a rational canonical form of any $n\times n$ matrix? Please explain with an example. P.S: I am an amateur in linear algebra. Thanks,Could anyone explain me in detail how to find out a rational canonical form of any $n\times n$ matrix? Please explain with an example. P.S: I am an amateur in linear algebra. Thanks,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
85,Skew-symmetric form as matrix,Skew-symmetric form as matrix,,"From Humphreys' Introduction to Lie Algebras and Representation Theory : $C_l$: Let $\dim V=2l$, with basis $(v_1,\ldots,v_{2l})$. Define a nondegenerate skew-symmetric form $f$ on $V$ by the matrix $S=\left| \begin{array}{cc} 0 & I_l \\ -I_l & 0  \end{array} \right|$. So, a skew-symmetric form is a function $B:V\times V\rightarrow F$ such that $B(v,w)=-B(w,v)$ for all $v,w\in V$. Why is it defined using the matrix $S$? What is the meaning of the matrix $S$ here?","From Humphreys' Introduction to Lie Algebras and Representation Theory : $C_l$: Let $\dim V=2l$, with basis $(v_1,\ldots,v_{2l})$. Define a nondegenerate skew-symmetric form $f$ on $V$ by the matrix $S=\left| \begin{array}{cc} 0 & I_l \\ -I_l & 0  \end{array} \right|$. So, a skew-symmetric form is a function $B:V\times V\rightarrow F$ such that $B(v,w)=-B(w,v)$ for all $v,w\in V$. Why is it defined using the matrix $S$? What is the meaning of the matrix $S$ here?",,"['abstract-algebra', 'matrices']"
86,Property of Subordinate Matrix Norm: $\|AB\| \leq \|A\|\|B\|$,Property of Subordinate Matrix Norm:,\|AB\| \leq \|A\|\|B\|,"I do not understand why the following property for Matrix subordinate norms holds: \begin{equation} \|AB\| \leq \|A\|\|B\| \end{equation} Please explain clearly as I know that it should be shown by the definition: \begin{equation} \|A\| = \sup\{\|A\|: u \in \mathbb{R}^n, \|u\|=1\} \end{equation} And the fact that: \begin{equation} \|Ax\| \leq \|A\|\|x\| \end{equation} I understand the definition and I understand the property. I just can't seem to put them together correctly. Hints only please! EDIT Is the following reasoning correct: Let $u$ be any unit vector: \begin{align} \|ABu\| \leq& \dfrac{\|ABx\|}{\|x\|}\\         \leq& \frac{\|A\|\|Bx\|}{\|x\|} \quad{\text{as $Bx$ is a vector}}\\         \leq& \frac{\|A\|\|B\|\|x\|}{\|x\|} \quad{\text{from above property}}\\         \leq& \|A\|\|B\| \end{align} Since $u$ is any unit vector then we have: \begin{equation} \|A\|\|B\| \geq \|AB\| \end{equation}","I do not understand why the following property for Matrix subordinate norms holds: \begin{equation} \|AB\| \leq \|A\|\|B\| \end{equation} Please explain clearly as I know that it should be shown by the definition: \begin{equation} \|A\| = \sup\{\|A\|: u \in \mathbb{R}^n, \|u\|=1\} \end{equation} And the fact that: \begin{equation} \|Ax\| \leq \|A\|\|x\| \end{equation} I understand the definition and I understand the property. I just can't seem to put them together correctly. Hints only please! EDIT Is the following reasoning correct: Let $u$ be any unit vector: \begin{align} \|ABu\| \leq& \dfrac{\|ABx\|}{\|x\|}\\         \leq& \frac{\|A\|\|Bx\|}{\|x\|} \quad{\text{as $Bx$ is a vector}}\\         \leq& \frac{\|A\|\|B\|\|x\|}{\|x\|} \quad{\text{from above property}}\\         \leq& \|A\|\|B\| \end{align} Since $u$ is any unit vector then we have: \begin{equation} \|A\|\|B\| \geq \|AB\| \end{equation}",,"['linear-algebra', 'matrices', 'normed-spaces']"
87,Property for Norms of Matrices,Property for Norms of Matrices,,"I am having trouble with the following problem: Show that the vector norm $||x||_1$ gives the subordinate matrix norm: \begin{equation} ||A||_1 = \max_{1\leq j\leq n}\sum_{i=1}^n|a_{ij}| \end{equation} I really do not have any starting point for this question. I though maybe we could use $||x||_1$ norm for the rows or columns of $A$ but I did not get anywhere with that. Note: \begin{equation} ||A|| = \sup{||Au||: u\in \mathbb{R}^n, ||u||=1} \end{equation} All help is greatly appreciated! EDIT: This is what I have so far: \begin{align} ||A||_1 =& \sup_{||u||_1=1}||Au|| \\         =& \sup_{||u||_1=1}||\sum\limits_{i=1}^n|(Au)_i|\,|| \end{align} Here is where I am having trouble. The maximum value depends on the entries in $A$. It just seems to make sense that we would pick the largest values in each row. 1.How do I finish the proof from here?","I am having trouble with the following problem: Show that the vector norm $||x||_1$ gives the subordinate matrix norm: \begin{equation} ||A||_1 = \max_{1\leq j\leq n}\sum_{i=1}^n|a_{ij}| \end{equation} I really do not have any starting point for this question. I though maybe we could use $||x||_1$ norm for the rows or columns of $A$ but I did not get anywhere with that. Note: \begin{equation} ||A|| = \sup{||Au||: u\in \mathbb{R}^n, ||u||=1} \end{equation} All help is greatly appreciated! EDIT: This is what I have so far: \begin{align} ||A||_1 =& \sup_{||u||_1=1}||Au|| \\         =& \sup_{||u||_1=1}||\sum\limits_{i=1}^n|(Au)_i|\,|| \end{align} Here is where I am having trouble. The maximum value depends on the entries in $A$. It just seems to make sense that we would pick the largest values in each row. 1.How do I finish the proof from here?",,"['linear-algebra', 'matrices', 'normed-spaces', 'matrix-norms']"
88,"Let $Q$ be a special orthogonal matrix. Show that $Q(u\times v)=Q(u)\times Q(v)$ for any vectors $u, v\in\mathbb R^3$.",Let  be a special orthogonal matrix. Show that  for any vectors .,"Q Q(u\times v)=Q(u)\times Q(v) u, v\in\mathbb R^3","Let $Q$ be a $3\times3$ special orthogonal matrix. Show that $Q(u\times v)=Q(u)\times Q(v)$ for any vectors $u, v\in\mathbb R^3$. I have no idea how to start. I'm not sure if $Q(u)\cdot Q(V)=Q(u\cdot v)$ would helps. Please give me some help. Thanks.","Let $Q$ be a $3\times3$ special orthogonal matrix. Show that $Q(u\times v)=Q(u)\times Q(v)$ for any vectors $u, v\in\mathbb R^3$. I have no idea how to start. I'm not sure if $Q(u)\cdot Q(V)=Q(u\cdot v)$ would helps. Please give me some help. Thanks.",,"['linear-algebra', 'matrices']"
89,Derivative of norm-infinity of vector,Derivative of norm-infinity of vector,,"So I know that $\frac{dX}{dX} = \mathbb{I}$ where $X \in \mathbb{R}^n$ and $\mathbb{I} \in \mathbb{R}^{n \times n}$ is the identity matrix. Now, what is the following derivative? $\frac{d|X|_\infty}{dX}$ where $|X|_\infty$ is the norm-infinity, i.e. $|X|_\infty = max(X)$ is a scalar?","So I know that $\frac{dX}{dX} = \mathbb{I}$ where $X \in \mathbb{R}^n$ and $\mathbb{I} \in \mathbb{R}^{n \times n}$ is the identity matrix. Now, what is the following derivative? $\frac{d|X|_\infty}{dX}$ where $|X|_\infty$ is the norm-infinity, i.e. $|X|_\infty = max(X)$ is a scalar?",,"['matrices', 'multivariable-calculus', 'derivatives']"
90,Is this fact about matrices and linear systems true?,Is this fact about matrices and linear systems true?,,"Let $A$ be a $m$-by-$n$ matrix and $B=A^TA$. If the columns of $A$ are linearly independent, then $Bx=0$ has a unique solution. If is true, can you help me prove it? If is false, could you give a counterexample? Thanks.","Let $A$ be a $m$-by-$n$ matrix and $B=A^TA$. If the columns of $A$ are linearly independent, then $Bx=0$ has a unique solution. If is true, can you help me prove it? If is false, could you give a counterexample? Thanks.",,"['linear-algebra', 'matrices']"
91,Determinant of Matrix Computed by Expanding Down the Diagonal?,Determinant of Matrix Computed by Expanding Down the Diagonal?,,"Going through this paper: http://dx.doi.org/10.1016/S0893-9659(00)00169-5 at the bottom of page 407, the authors seem to compute the determinant of a matrix by expanding down the diagonal. The authors discuss a matrix $A = \left[ \begin{array}{ccc} a_{11} a_{12} a_{13} \\ a_{21} a_{22} a_{23} \\ a_{31} a_{32} a_{33} \end{array}\right]$ and call $M_{ij}$ the cofactor of $a_{ij}$. Then at the bottom of page 407 they write $\det(A) = a_{11} M_{11} + a_{22} M_{22} + a_{33} M_{33}.$ Is expansion down the diagonal possible? Is it possible in some special cases?","Going through this paper: http://dx.doi.org/10.1016/S0893-9659(00)00169-5 at the bottom of page 407, the authors seem to compute the determinant of a matrix by expanding down the diagonal. The authors discuss a matrix $A = \left[ \begin{array}{ccc} a_{11} a_{12} a_{13} \\ a_{21} a_{22} a_{23} \\ a_{31} a_{32} a_{33} \end{array}\right]$ and call $M_{ij}$ the cofactor of $a_{ij}$. Then at the bottom of page 407 they write $\det(A) = a_{11} M_{11} + a_{22} M_{22} + a_{33} M_{33}.$ Is expansion down the diagonal possible? Is it possible in some special cases?",,"['linear-algebra', 'matrices', 'determinant']"
92,Matrices with at most one negative eigenvalue,Matrices with at most one negative eigenvalue,,Suppose a vector $y$ and a symmetric matrix $M$ are given. \begin{equation} \forall x; \quad x^Ty=0 \implies x^TMx \ge 0 \end{equation} Prove that $M$ has at most one negative eigenvalue.,Suppose a vector $y$ and a symmetric matrix $M$ are given. \begin{equation} \forall x; \quad x^Ty=0 \implies x^TMx \ge 0 \end{equation} Prove that $M$ has at most one negative eigenvalue.,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
93,"If $A$ is an $n \times n$ matrix such that $A^3 = O_{3}$, show that $I - A$ is invertible with inverse $I + A + A^2$","If  is an  matrix such that , show that  is invertible with inverse",A n \times n A^3 = O_{3} I - A I + A + A^2,"So this question is basically a proof. If $A$ is an $n \times n$ matrix (so square) which satisfies the condition $A^3 = O_{3}$ ($A^{3}$ gives the $3 \times 3$ zero matrix), then show that $(I - A)$ is invertible with inverse $(I + A + A^2)$. I have no idea where to start, all help welcome.","So this question is basically a proof. If $A$ is an $n \times n$ matrix (so square) which satisfies the condition $A^3 = O_{3}$ ($A^{3}$ gives the $3 \times 3$ zero matrix), then show that $(I - A)$ is invertible with inverse $(I + A + A^2)$. I have no idea where to start, all help welcome.",,"['linear-algebra', 'matrices', 'inverse']"
94,How many row operations separate two row-equivalent matrices $A$ and $B$?,How many row operations separate two row-equivalent matrices  and ?,A B,"This definition is from Hoffmann and Kunze. Definition. If $A$ and $B$ are $m \times n$ matrices over the field $F$, we   say that $B$ is row-equivalent to $A$ if $B$ can be obtained from $A$   by a finite sequence of elementary row operations. Suppose I have two different matrices and I perform 999 elementary row operations on $A$ on paper and cannot get $B$, so I conclude that they are not row equivalent, but the 1000th step (which I did not do) makes them row equivalent. My question is what is the meaning of ""finite sequence of elementary row operations"" in the above definition. Is it that Linear algebra is modern so we must resort to computers.","This definition is from Hoffmann and Kunze. Definition. If $A$ and $B$ are $m \times n$ matrices over the field $F$, we   say that $B$ is row-equivalent to $A$ if $B$ can be obtained from $A$   by a finite sequence of elementary row operations. Suppose I have two different matrices and I perform 999 elementary row operations on $A$ on paper and cannot get $B$, so I conclude that they are not row equivalent, but the 1000th step (which I did not do) makes them row equivalent. My question is what is the meaning of ""finite sequence of elementary row operations"" in the above definition. Is it that Linear algebra is modern so we must resort to computers.",,"['linear-algebra', 'matrices']"
95,Matrix Multiplication - Why Rows $\cdot$ Columns = Columns?,Matrix Multiplication - Why Rows  Columns = Columns?,\cdot,"I'm nearing the end of my first year of Calculus and am pretty confident in the parts of it I've learned, yet I still don't have a good understanding of matrices, which seem like they should be easier to understand and work with. They were never formally taught in any of the math courses my school has given me and were only ever briefly mentioned. I don't fully understand what they represent or the logic behind how they are multiplied. Why are the left matrix's rows multiplied by the second matrix's columns to form columns in the resulting matrix? Why not columns by rows to form rows? Was it an arbitrary decision that was made or does it fit what concept of the matrix? I could memorize how to perform operations on matrices and how to use them to solve certain problems, but what I'm trying to do is understand them conceptually. Specifically, I was reading a page about the matrix representation of 2d transformations as used by CSS3 and found it all very confusing.","I'm nearing the end of my first year of Calculus and am pretty confident in the parts of it I've learned, yet I still don't have a good understanding of matrices, which seem like they should be easier to understand and work with. They were never formally taught in any of the math courses my school has given me and were only ever briefly mentioned. I don't fully understand what they represent or the logic behind how they are multiplied. Why are the left matrix's rows multiplied by the second matrix's columns to form columns in the resulting matrix? Why not columns by rows to form rows? Was it an arbitrary decision that was made or does it fit what concept of the matrix? I could memorize how to perform operations on matrices and how to use them to solve certain problems, but what I'm trying to do is understand them conceptually. Specifically, I was reading a page about the matrix representation of 2d transformations as used by CSS3 and found it all very confusing.",,['matrices']
96,Elementary explanation of determinant,Elementary explanation of determinant,,I am teaching 9th graders about matrices and looking for an explanation of what a determinant is.  All of the explanations I find are too complex!  Is there someone that can give a 9th grade level explanation??,I am teaching 9th graders about matrices and looking for an explanation of what a determinant is.  All of the explanations I find are too complex!  Is there someone that can give a 9th grade level explanation??,,['matrices']
97,Prove that if $\mathrm{rank}(A) < n$ then $\det(A) = 0$?,Prove that if  then ?,\mathrm{rank}(A) < n \det(A) = 0,"If $A$ is an $n \times n$ matrix with $\DeclareMathOperator{\rank}{rank}$ $\rank(A) < n$, then I need to show that $\det(A) = 0$. Now I understand why this is - if $\rank(A) < n$ then when converted to reduced row echelon form, there will be a row/column of zeroes, thus $\det(A) = 0$ However, I have been told to use the fact that the determinant is multilinear and alternating and subsequently deduce that if $\det(A)$ is non-zero, $A$ is invertible. How do I use the properties of the determinant to prove these claims?","If $A$ is an $n \times n$ matrix with $\DeclareMathOperator{\rank}{rank}$ $\rank(A) < n$, then I need to show that $\det(A) = 0$. Now I understand why this is - if $\rank(A) < n$ then when converted to reduced row echelon form, there will be a row/column of zeroes, thus $\det(A) = 0$ However, I have been told to use the fact that the determinant is multilinear and alternating and subsequently deduce that if $\det(A)$ is non-zero, $A$ is invertible. How do I use the properties of the determinant to prove these claims?",,"['linear-algebra', 'matrices']"
98,Logarithm of a Gram matrix,Logarithm of a Gram matrix,,"Given a Gram matrix $K$, we are interested in calculating its matrix logarithm $\log(K)$, and in particular, to relate minus this logarithm to the Laplacian of a graph. We have noticed that $-\log(K)$ always has positive diagonal entries. Which is a good thing, but we would like to prove it. Any ideas?","Given a Gram matrix $K$, we are interested in calculating its matrix logarithm $\log(K)$, and in particular, to relate minus this logarithm to the Laplacian of a graph. We have noticed that $-\log(K)$ always has positive diagonal entries. Which is a good thing, but we would like to prove it. Any ideas?",,"['matrices', 'logarithms']"
99,Reversing the Gram Matrix,Reversing the Gram Matrix,,"Let $A$ be a $M\times N$  real matrix, then $B=A^TA$ is the gramian of $A$. Suppose $B$ is given, is $A$ unique? Can I say something on it depending on $M$ and $N$.","Let $A$ be a $M\times N$  real matrix, then $B=A^TA$ is the gramian of $A$. Suppose $B$ is given, is $A$ unique? Can I say something on it depending on $M$ and $N$.",,"['linear-algebra', 'matrices']"
