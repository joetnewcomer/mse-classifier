,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"If $A_n B_n$ converge to the identity matrix, is it true that $A_n^{1/2} B_n^{1/2} \to I$ as well?","If  converge to the identity matrix, is it true that  as well?",A_n B_n A_n^{1/2} B_n^{1/2} \to I,"If two sequences of positive definite symmetric matrices $(A_n)$ and $(B_n)$ are such that $A_n B_n \to I$ , is it necessarily true that $A_n^{1/2} B_n^{1/2}$ also converge to the identity? If not, what sort of additional assumptions are needed? (I can see that if $A_n$ and $B_n$ are simultaneously diagonalizable, the implication holds. Are there weaker conditions that also guarantee this?)","If two sequences of positive definite symmetric matrices and are such that , is it necessarily true that also converge to the identity? If not, what sort of additional assumptions are needed? (I can see that if and are simultaneously diagonalizable, the implication holds. Are there weaker conditions that also guarantee this?)",(A_n) (B_n) A_n B_n \to I A_n^{1/2} B_n^{1/2} A_n B_n,"['linear-algebra', 'matrices']"
1,Singular value of production of projection matrix and some other matrix,Singular value of production of projection matrix and some other matrix,,"if I have a matrix $X\in\mathbb{R}^{n\times d}$ , and some projection matrix $P\in\mathbb{R}^{n\times n}$ which projects things onto a $k$ -dim subspace of the column space of $X$ , is there some expression for the singular values of $PX$ ? Say WLOG that $\|X\|_{op}\leq1$ . Using Weyl's inequality we get an upperbound that $\sigma_i(PX)\leq \sigma_i(X)$ . But I suspect that this is too pessimistic since $P$ projects onto a subspace of $col(X)$ . For example, if $PX=X_k$ being the SVD of the top $k$ singular values, then $\sigma_1(PX)\leq \sigma_{k+1}(X)$ which can be very small. Thanks for any advice!","if I have a matrix , and some projection matrix which projects things onto a -dim subspace of the column space of , is there some expression for the singular values of ? Say WLOG that . Using Weyl's inequality we get an upperbound that . But I suspect that this is too pessimistic since projects onto a subspace of . For example, if being the SVD of the top singular values, then which can be very small. Thanks for any advice!",X\in\mathbb{R}^{n\times d} P\in\mathbb{R}^{n\times n} k X PX \|X\|_{op}\leq1 \sigma_i(PX)\leq \sigma_i(X) P col(X) PX=X_k k \sigma_1(PX)\leq \sigma_{k+1}(X),"['linear-algebra', 'matrices', 'matrix-decomposition', 'singular-values']"
2,countable dimensional vector space has uncountable eigenvalues?,countable dimensional vector space has uncountable eigenvalues?,,"Consider $\mathcal{R}^{\infty}$ , and linear map $\mathcal{L} \in L(\mathcal{R}^{\infty})$ , where $\mathcal{L}((x_1,x_2,...))=(x_2,x_3,...)$ . Now, any number $\lambda \in \mathcal{R}$ is an eigenvalue with eigenvector $(c,c\lambda,c \lambda^2,...)$ . But dimention of $\mathcal{R}^{\infty}$ is countable. Is this a problem?","Consider , and linear map , where . Now, any number is an eigenvalue with eigenvector . But dimention of is countable. Is this a problem?","\mathcal{R}^{\infty} \mathcal{L} \in L(\mathcal{R}^{\infty}) \mathcal{L}((x_1,x_2,...))=(x_2,x_3,...) \lambda \in \mathcal{R} (c,c\lambda,c \lambda^2,...) \mathcal{R}^{\infty}",['linear-algebra']
3,"Visualization of a ""Not so intuitive"" problem of linear algebra.","Visualization of a ""Not so intuitive"" problem of linear algebra.",,"I recently encountered a problem in Hoffmann-Kunze linear algebra: If $(.,.)$ is the standard inner product on $\mathbb C^2$ then show that $(Tv,v)=0 \forall v\in \mathbb C^2 \implies T=0$ , I think the proof is quite tricky and I saw its solution at last after trying myself through a long time. The solution is like this, we put $x+y$ and $x+iy$ for $v$ and prove that $(Tx,y)=0 \forall x,y\in \mathbb C^2$ , thus it follows that $Tx=0 \forall x\in \mathbb C^2$ , but I think it cannot be just a trick, there might be a deeper understanding within this simple 'just a trick' looking proof. I want to visualize why this fact is not true for $\mathbb R^2$ space but true when $\mathbb C$ replaces $\mathbb R$ . I am looking for an intuitive solution of the same problem or a suitable visualization that would help me in better understanding of this problem.","I recently encountered a problem in Hoffmann-Kunze linear algebra: If is the standard inner product on then show that , I think the proof is quite tricky and I saw its solution at last after trying myself through a long time. The solution is like this, we put and for and prove that , thus it follows that , but I think it cannot be just a trick, there might be a deeper understanding within this simple 'just a trick' looking proof. I want to visualize why this fact is not true for space but true when replaces . I am looking for an intuitive solution of the same problem or a suitable visualization that would help me in better understanding of this problem.","(.,.) \mathbb C^2 (Tv,v)=0 \forall v\in \mathbb C^2 \implies T=0 x+y x+iy v (Tx,y)=0 \forall x,y\in \mathbb C^2 Tx=0 \forall x\in \mathbb C^2 \mathbb R^2 \mathbb C \mathbb R","['linear-algebra', 'inner-products', 'intuition', 'visualization', 'motivation']"
4,Factorization of a linear map by multiple other maps,Factorization of a linear map by multiple other maps,,"Let $E$ be a vector space of finite dimension $n$ . Let $p\ge 1$ , $g, f_1, \ldots, f_p\in\mathcal{L}(E)$ be such that $$\bigcap_{i=1}^p\mathrm{ker}f_i\subseteq \mathrm{ker} g$$ Show that their exists $h_1, \ldots, h_p\in \mathcal{L}(E)$ such that $$ g = \sum_{i=1}^{p} h_i\circ f_i$$ I can show the result for $p=1$ , by studying $\varphi : h\in\mathcal{L}(E)\mapsto h\circ f$ . It is easy to see that $\ker\varphi = \{h\mid \mathrm{Im} f\subseteq\ker h\}$ so that $\dim\ker\varphi = (n-\mathrm{rk} f)\cdot n = n\cdot \dim\ker f$ . So that by rank theorem, $\mathrm{rank}\varphi = n(n - \dim\ker f) = n\cdot\mathrm{rank}(f)$ . To conclure, notice that $\mathrm{Im}(f) \subseteq \{h\mid \ker f\subseteq \ker h\}$ but the last vector-space is of dimension $n\cdot\mathrm{rank}(f)$ so we have equality. Therefore, $g\in \mathrm{Im}\varphi$ But how can one generalize that to $p$ linear maps ? The proof for $p=1$ can also be done by choosing a nice basis for example (which then also works in infinite dimension). To come back to the general case, I thought of looking at the following linear transformation : $$ \phi :x\mapsto (f_1(x), \ldots, f_p(x))$$ So that $\ker\phi = \displaystyle\bigcap_{i=1}^n \ker f_i$ but now, $\phi\in\mathcal{L}(E, E^p)$ . I thought of using $g':x\mapsto (g(x), \ldots, g(x))$ but the previous proof does not work. Any ideas ?","Let be a vector space of finite dimension . Let , be such that Show that their exists such that I can show the result for , by studying . It is easy to see that so that . So that by rank theorem, . To conclure, notice that but the last vector-space is of dimension so we have equality. Therefore, But how can one generalize that to linear maps ? The proof for can also be done by choosing a nice basis for example (which then also works in infinite dimension). To come back to the general case, I thought of looking at the following linear transformation : So that but now, . I thought of using but the previous proof does not work. Any ideas ?","E n p\ge 1 g, f_1, \ldots, f_p\in\mathcal{L}(E) \bigcap_{i=1}^p\mathrm{ker}f_i\subseteq \mathrm{ker} g h_1, \ldots, h_p\in \mathcal{L}(E)  g = \sum_{i=1}^{p} h_i\circ f_i p=1 \varphi : h\in\mathcal{L}(E)\mapsto h\circ f \ker\varphi = \{h\mid \mathrm{Im} f\subseteq\ker h\} \dim\ker\varphi = (n-\mathrm{rk} f)\cdot n = n\cdot \dim\ker f \mathrm{rank}\varphi = n(n - \dim\ker f) = n\cdot\mathrm{rank}(f) \mathrm{Im}(f) \subseteq \{h\mid \ker f\subseteq \ker h\} n\cdot\mathrm{rank}(f) g\in \mathrm{Im}\varphi p p=1  \phi :x\mapsto (f_1(x), \ldots, f_p(x)) \ker\phi = \displaystyle\bigcap_{i=1}^n \ker f_i \phi\in\mathcal{L}(E, E^p) g':x\mapsto (g(x), \ldots, g(x))","['linear-algebra', 'linear-transformations']"
5,Similar matrices over $\mathbb{Z}/2\mathbb{Z}$,Similar matrices over,\mathbb{Z}/2\mathbb{Z},"Given the following matrices $P=\left( \begin{array}{rrr} 1 & -1 & 0 \\ 0 & 2 & 5  \\ 0 & 0 & 3 \\ \end{array}\right), Q=\left( \begin{array}{rrr} 1 & 0 & 0 \\ -1 & 4 & 0  \\ 0 & 3 & 7 \\ \end{array}\right)$ , such that $P,Q \in M(3\times3, \mathbb{Z}/2\mathbb{Z})$ , I have to find out whether the two matrices are similar or not. One problem I'm facing is that $\mathbb{Z}/2\mathbb{Z} = {0,1}$ , which implies that $-1,2,5,3,4,7 \notin \mathbb{Z}/2\mathbb{Z}$ ,  hence I'm assuming it is meant that $P=\left( \begin{array}{rrr} 1 & (-1\mod2) & 0 \\ 0 & (2\mod2) & (5\mod2)  \\ 0 & 0 & (3\mod2) \\ \end{array}\right)\\ Q=\left( \begin{array}{rrr} 1 & 0 & 0 \\ (-1\mod2) & (4\mod2) & 0  \\ 0 & (3\mod2) & (7\mod2) \\ \end{array}\right)$ which would give us $P=\left( \begin{array}{rrr} 1 & 1 & 0 \\ 0 & 0 & 1  \\ 0 & 0 & 1 \\ \end{array}\right)\\ Q=\left( \begin{array}{rrr} 1 & 0 & 0 \\ 1 & 0 & 0  \\ 0 & 1 & 1 \\ \end{array}\right)$ But now the real problem arises, which is finding $R\in M(3\times3, \mathbb{Z}/2\mathbb{Z})$ , such that $P=RQR^{-1}$ . One attempt was to try and solve a huge system of equations, arising ,for example, from $P=RQR^{-1}$ but that does not seem like the proper way to do this (it didn't work out). Another thing I noticed is that $P, Q$ have the same rank, however, I was unable to use this in order to find a solution. My last resort so far would be to simply try out various matrices in $M(3\times3, \mathbb{Z}/2\mathbb{Z})$ , but this can't be the only way to do this. Any help is appreciated, thanks.","Given the following matrices , such that , I have to find out whether the two matrices are similar or not. One problem I'm facing is that , which implies that ,  hence I'm assuming it is meant that which would give us But now the real problem arises, which is finding , such that . One attempt was to try and solve a huge system of equations, arising ,for example, from but that does not seem like the proper way to do this (it didn't work out). Another thing I noticed is that have the same rank, however, I was unable to use this in order to find a solution. My last resort so far would be to simply try out various matrices in , but this can't be the only way to do this. Any help is appreciated, thanks.","P=\left( \begin{array}{rrr}
1 & -1 & 0 \\
0 & 2 & 5  \\
0 & 0 & 3 \\
\end{array}\right), Q=\left( \begin{array}{rrr}
1 & 0 & 0 \\
-1 & 4 & 0  \\
0 & 3 & 7 \\
\end{array}\right) P,Q \in M(3\times3, \mathbb{Z}/2\mathbb{Z}) \mathbb{Z}/2\mathbb{Z} = {0,1} -1,2,5,3,4,7 \notin \mathbb{Z}/2\mathbb{Z} P=\left( \begin{array}{rrr}
1 & (-1\mod2) & 0 \\
0 & (2\mod2) & (5\mod2)  \\
0 & 0 & (3\mod2) \\
\end{array}\right)\\ Q=\left( \begin{array}{rrr}
1 & 0 & 0 \\
(-1\mod2) & (4\mod2) & 0  \\
0 & (3\mod2) & (7\mod2) \\
\end{array}\right) P=\left( \begin{array}{rrr}
1 & 1 & 0 \\
0 & 0 & 1  \\
0 & 0 & 1 \\
\end{array}\right)\\ Q=\left( \begin{array}{rrr}
1 & 0 & 0 \\
1 & 0 & 0  \\
0 & 1 & 1 \\
\end{array}\right) R\in M(3\times3, \mathbb{Z}/2\mathbb{Z}) P=RQR^{-1} P=RQR^{-1} P, Q M(3\times3, \mathbb{Z}/2\mathbb{Z})","['linear-algebra', 'matrices']"
6,Does the tensor product respect semidefinite ordering in this way?,Does the tensor product respect semidefinite ordering in this way?,,"I'll use $\succeq$ to denote the positive semidefinite ordering: for square matrices $X,Y$ , one has $X \succeq Y$ iff $X - Y$ is positive semidefinite. It's a well known fact that if $X, Y \succeq 0$ then $X \otimes Y \succeq 0$ . However, if one has two pairs of matrices with $X \succeq Y$ and $X' \succeq Y'$ , it isn't necessarily the case that $X \otimes X' \succeq Y \otimes Y'$ (for instance, if $X = X' = I$ and $Y = Y' = -2I$ ). My question is: if we add that $Y, Y' \succeq 0$ , so our assumptions are $$ X \succeq Y \succeq 0,\ \ \ X' \succeq Y' \succeq 0 $$ Is it necessarily true that $X \otimes X' \succeq Y \otimes Y'$ ? I personally feel that this should be true (and I could swear I've seen this result before but can't find it anywhere), but I'm struggling to prove it. Does anyone have a reference for this fact, and/or know how to prove (or disprove) it?","I'll use to denote the positive semidefinite ordering: for square matrices , one has iff is positive semidefinite. It's a well known fact that if then . However, if one has two pairs of matrices with and , it isn't necessarily the case that (for instance, if and ). My question is: if we add that , so our assumptions are Is it necessarily true that ? I personally feel that this should be true (and I could swear I've seen this result before but can't find it anywhere), but I'm struggling to prove it. Does anyone have a reference for this fact, and/or know how to prove (or disprove) it?","\succeq X,Y X \succeq Y X - Y X, Y \succeq 0 X \otimes Y \succeq 0 X \succeq Y X' \succeq Y' X \otimes X' \succeq Y \otimes Y' X = X' = I Y = Y' = -2I Y, Y' \succeq 0  X \succeq Y \succeq 0,\ \ \ X' \succeq Y' \succeq 0  X \otimes X' \succeq Y \otimes Y'","['linear-algebra', 'tensor-products', 'positive-semidefinite', 'quantum-computation', 'quantum-information']"
7,Two Matrices $A$ and $B$ are similar if and only if their characteristic Matrices $xI_n-A$ and $xI_n-B$ are equivalent.,Two Matrices  and  are similar if and only if their characteristic Matrices  and  are equivalent.,A B xI_n-A xI_n-B,"As a part of an advanced junior undergraduate Linear Algebra course we're trying to prove the following statement: ""Two Matrices $A$ and $B$ are similar if and only if their characteristic Matrices $xI_n-A$ and $xI_n-B$ are equivalent."" where as $A,B \in K^{n \times n}$ , $xI_n-A \in K[x]$ , $K $ is a Field and $K[x]$ is the polynomial ring over $K$ . Equivalence of $xI_n-A$ and $xI_n-B$ in $K[x]$ is defined as: $xI_n-A$ and $xI_n-B$ are equivalent if there exists invertible matrices $S$ and $T$ in $K[x]$ such that $S(xI_n-B)T=xI_n-A$ The first direction of the proof is quite easy, the second one however is much more difficult, in the textbook the lecturer uses an algebraic proof, without explaining the motivation behind it. Here's an overview of the Proof "" $\Rightarrow$ "" There is an invertible Matrix $S$ such that $S^{-1}AS=B$ , multiplication of $S^{-1}(xI_n-A)S$ yields $xI_n-B$ "" $\Leftarrow$ ""  Let $S,T$ be invertible Matrices in $K[x]^{n\times n}$ such that $S(xI_n-B)T=xIn-A$ For every Matrix $C \in K[x]^{n \times n}$ there are Matrices $C_i \in K^{n \times n}$ such that $C= \sum_{x=0}^{m}x^iC_i$ Using this definition we get: $C(A)= \sum_{x=0}^{m}A^iC_i \in K^{n\times n}$ for $A \in K^{n \times n}$ Which implies that for Matrices $C,D \in K[x]^{n\times n}$ and $A \in K^{n \times n}$ : $$ \begin{align*}(C+D)(A) = C(A) + D(A)\end{align*} $$ $$ \begin{align*}(C \cdot D)(A) = (C(A)\cdot D)(A)\end{align*} $$ One can verify that $A \cdot S(A) = S(A) \cdot B \quad$ (1) and using induction we get $A^i \cdot S(A) = S(A) \cdot B^i \quad , \forall i \in \mathbb{N}$ We now prove that $S(A)$ is invertible. Because $S \in K[x]^{n\times n}$ is invertible there is $C \in K[x]^{n\times n}$ such that $S \cdot C= I_n$ using the previous results one can verify that $S(A) \cdot C(B)= I_n$ which implies that $S(A)$ is invertible which implies using (1) that $S(A)^{-1} \cdot A \cdot S(A) =B$ and thus the result. So here is my question: how could an undergraduate student come up with the idea for such proof, what are the main ideas/observations that could lead to it? If there is actually no intuition behind the proof, do you know of any alternative solutions to prove it? I've tried to work on it myself but I didn't come far, the reason is that we used this proof to use the Smith Normal form to build the theory of the rational/Frobenius normal form and then the Jordan normal form that's why I would appreciate a proof that doesn't assume the rational form","As a part of an advanced junior undergraduate Linear Algebra course we're trying to prove the following statement: ""Two Matrices and are similar if and only if their characteristic Matrices and are equivalent."" where as , , is a Field and is the polynomial ring over . Equivalence of and in is defined as: and are equivalent if there exists invertible matrices and in such that The first direction of the proof is quite easy, the second one however is much more difficult, in the textbook the lecturer uses an algebraic proof, without explaining the motivation behind it. Here's an overview of the Proof "" "" There is an invertible Matrix such that , multiplication of yields "" ""  Let be invertible Matrices in such that For every Matrix there are Matrices such that Using this definition we get: for Which implies that for Matrices and : One can verify that (1) and using induction we get We now prove that is invertible. Because is invertible there is such that using the previous results one can verify that which implies that is invertible which implies using (1) that and thus the result. So here is my question: how could an undergraduate student come up with the idea for such proof, what are the main ideas/observations that could lead to it? If there is actually no intuition behind the proof, do you know of any alternative solutions to prove it? I've tried to work on it myself but I didn't come far, the reason is that we used this proof to use the Smith Normal form to build the theory of the rational/Frobenius normal form and then the Jordan normal form that's why I would appreciate a proof that doesn't assume the rational form","A B xI_n-A xI_n-B A,B \in K^{n \times n} xI_n-A \in K[x] K  K[x] K xI_n-A xI_n-B K[x] xI_n-A xI_n-B S T K[x] S(xI_n-B)T=xI_n-A \Rightarrow S S^{-1}AS=B S^{-1}(xI_n-A)S xI_n-B \Leftarrow S,T K[x]^{n\times n} S(xI_n-B)T=xIn-A C \in K[x]^{n \times n} C_i \in K^{n \times n} C= \sum_{x=0}^{m}x^iC_i C(A)= \sum_{x=0}^{m}A^iC_i \in K^{n\times n} A \in K^{n \times n} C,D \in K[x]^{n\times n} A \in K^{n \times n} 
\begin{align*}(C+D)(A) = C(A) + D(A)\end{align*}
 
\begin{align*}(C \cdot D)(A) = (C(A)\cdot D)(A)\end{align*}
 A \cdot S(A) = S(A) \cdot B \quad A^i \cdot S(A) = S(A) \cdot B^i \quad , \forall i \in \mathbb{N} S(A) S \in K[x]^{n\times n} C \in K[x]^{n\times n} S \cdot C= I_n S(A) \cdot C(B)= I_n S(A) S(A)^{-1} \cdot A \cdot S(A) =B","['linear-algebra', 'abstract-algebra']"
8,Why are self-adjoint operators important?,Why are self-adjoint operators important?,,"I am learning about self-adjoint and normal operators. So far, they have come up in the Spectral theorem, which says self-adjoint operators have an eigenvalue basis and a corresponding diagonal matrix. Do self-adjoint (or normal) operators have any other useful properties? (i.e. why are they important in linear algebra?) Since it is fairly straightforward, at least from what i've learnt, to find eigenvalues with the characteristic equation, it seems to me that you wouldn’t first try to see if an operator is self-adjoint before you diagonalize it, but would just find the eigenvalues and vectors.","I am learning about self-adjoint and normal operators. So far, they have come up in the Spectral theorem, which says self-adjoint operators have an eigenvalue basis and a corresponding diagonal matrix. Do self-adjoint (or normal) operators have any other useful properties? (i.e. why are they important in linear algebra?) Since it is fairly straightforward, at least from what i've learnt, to find eigenvalues with the characteristic equation, it seems to me that you wouldn’t first try to see if an operator is self-adjoint before you diagonalize it, but would just find the eigenvalues and vectors.",,"['linear-algebra', 'adjoint-operators', 'self-adjoint-operators']"
9,Eigenvalues of rank-$1$ update,Eigenvalues of rank- update,1,"If I have a diagonal matrix with rank- $1$ update $$D + u v^T$$ what can I say about its eigenvalues? I know from Two matrices that are not similar have (almost) same eigenvalues that every eigenvalue of $D$ with multiplicity $m > 1$ will occur in $D + uv^T$ at least $m-1$ times. I am wondering what can be said about the remaining eigenvalues, and in particular, how do they scale with $u$ and/or $v$ . For example in the following Mathematica code: dim = 50; SeedRandom[1]  Diag = DiagonalMatrix[Flatten[RandomInteger[{0, 10}, {1, dim}]]];  u = ConstantArray[{1}, dim]; v = List /@ RandomReal[{0, 100}, {dim}]; vT = Transpose[v]; uvT = Transpose[u.vT];  Eigenvalues[Diag] Round[Eigenvalues[Diag + uvT], 0.01]  (*{10, 9, 9, 8, 8, 7, 6, 6, 6, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 3,  3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,  0, 0, 0, 0, 0}*)  (*{2340.33, 9.85, 9., 8.79, 8., 7.75, 6.98, 6., 6., 5.85, 5., 5., 5.,  5., 5., 4.68, 4., 4., 4., 4., 4., 3.59, 3., 3., 3., 3., 3., 3., 2.4,  2., 2., 2., 1.55, 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.3, 0., 0.,  0., 0., 0., 0., 0.}*) one can clearly see that the eigenvalue with multiplicity $m>1$ occurred in the perturbed case again at least $m-1$ times while every other eigenvalue lifted only slightly. If I now chose v to be of higher magnitude: v = List /@ RandomReal[{10^9, 10^10}, {dim}]; for some reason the eigenvalues change only insignificantly: (*{10, 9, 9, 8, 8, 7, 6, 6, 6, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 3,  3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,  0, 0, 0, 0, 0}*)  (*{2.60337*10^11, 9.86, 9., 8.79, 8., 7.76, 6.97, 6., 6., 5.84, 5., 5.,  5., 5., 5., 4.67, 4., 4., 4., 4., 4., 3.59, 3., 3., 3., 3., 3., 3.,  2.4, 2., 2., 2., 1.56, 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.31, 0.,  0., 0., 0., 0., 0., 0.}*) except for the first eigenvalue that blows up. Is there a mathematical argument for why most of the eigenvalues change only so slightly even when choosing $v$ to be so large?","If I have a diagonal matrix with rank- update what can I say about its eigenvalues? I know from Two matrices that are not similar have (almost) same eigenvalues that every eigenvalue of with multiplicity will occur in at least times. I am wondering what can be said about the remaining eigenvalues, and in particular, how do they scale with and/or . For example in the following Mathematica code: dim = 50; SeedRandom[1]  Diag = DiagonalMatrix[Flatten[RandomInteger[{0, 10}, {1, dim}]]];  u = ConstantArray[{1}, dim]; v = List /@ RandomReal[{0, 100}, {dim}]; vT = Transpose[v]; uvT = Transpose[u.vT];  Eigenvalues[Diag] Round[Eigenvalues[Diag + uvT], 0.01]  (*{10, 9, 9, 8, 8, 7, 6, 6, 6, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 3,  3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,  0, 0, 0, 0, 0}*)  (*{2340.33, 9.85, 9., 8.79, 8., 7.75, 6.98, 6., 6., 5.85, 5., 5., 5.,  5., 5., 4.68, 4., 4., 4., 4., 4., 3.59, 3., 3., 3., 3., 3., 3., 2.4,  2., 2., 2., 1.55, 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.3, 0., 0.,  0., 0., 0., 0., 0.}*) one can clearly see that the eigenvalue with multiplicity occurred in the perturbed case again at least times while every other eigenvalue lifted only slightly. If I now chose v to be of higher magnitude: v = List /@ RandomReal[{10^9, 10^10}, {dim}]; for some reason the eigenvalues change only insignificantly: (*{10, 9, 9, 8, 8, 7, 6, 6, 6, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 3,  3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,  0, 0, 0, 0, 0}*)  (*{2.60337*10^11, 9.86, 9., 8.79, 8., 7.76, 6.97, 6., 6., 5.84, 5., 5.,  5., 5., 5., 4.67, 4., 4., 4., 4., 4., 3.59, 3., 3., 3., 3., 3., 3.,  2.4, 2., 2., 2., 1.56, 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.31, 0.,  0., 0., 0., 0., 0., 0.}*) except for the first eigenvalue that blows up. Is there a mathematical argument for why most of the eigenvalues change only so slightly even when choosing to be so large?",1 D + u v^T D m > 1 D + uv^T m-1 u v m>1 m-1 v,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'mathematica']"
10,"If $f: \mathbb R^n \to \mathbb R^n$ satisfies $\|f(x - y)\| = \|f(x) - f(y)\|$, is $f$ additive?","If  satisfies , is  additive?",f: \mathbb R^n \to \mathbb R^n \|f(x - y)\| = \|f(x) - f(y)\| f,"Question Main question: Let $\| \cdot \|$ be a norm on a finite-dimensional real vector space $V$ . If $f : V \to V$ is a function satisfying $$ \| f(x-y)\| = \|f(x) - f(y)\| $$ for all $x, y \in V$ , does it follow that $f$ is additive? I.e., does it follow that $f(x + y) = f(x) + f(y)$ for all $x, y$ ? Follow-up: Does the answer change depending on the choice of $\| \cdot \|$ ? In particular, what if $\| \cdot \|$ is an inner product norm? Background Last week, user C.F.G. asked a very similar question , to which the answer was ""no for trivial reasons"": they asked whether the function $f$ was necessarily linear, but clearly all additive functions satisfy the condition, and there are non-linear additive functions. User Charlie Cunningham pointed out in the comments that the question is actually interesting if you remove the trivial reasons. Because the question had been answered, I tried to get an answer to the non-trivial question myself. The original question included the requirement that the vector space be finite-dimensional; I removed this requirement because it erroneously struck me as an irrelevant restriction (I thought you could just take the subspace containing $x, y, f(x), f(y)$ to get a finite-dimensional $V$ ). (The functional relation in those questions has a different form, but this is equivalent the formulation above, as pointed out by user Omnomnomnom in comments.) However, we then got a negative answer to my question -- an example of a non-additive $f$ satisfying the condition -- where the infinite-dimensionality of $V$ was crucial. This has left us with the tantalizing option that the finite-dimensionality of $V$ was crucial to the resolution of the question. I did not want to ask yet another very similar question here, but I also did not want to edit my question, as it had gotten a correct answer that did not deserve to be made irrelevant. Maybe third time's the charm?","Question Main question: Let be a norm on a finite-dimensional real vector space . If is a function satisfying for all , does it follow that is additive? I.e., does it follow that for all ? Follow-up: Does the answer change depending on the choice of ? In particular, what if is an inner product norm? Background Last week, user C.F.G. asked a very similar question , to which the answer was ""no for trivial reasons"": they asked whether the function was necessarily linear, but clearly all additive functions satisfy the condition, and there are non-linear additive functions. User Charlie Cunningham pointed out in the comments that the question is actually interesting if you remove the trivial reasons. Because the question had been answered, I tried to get an answer to the non-trivial question myself. The original question included the requirement that the vector space be finite-dimensional; I removed this requirement because it erroneously struck me as an irrelevant restriction (I thought you could just take the subspace containing to get a finite-dimensional ). (The functional relation in those questions has a different form, but this is equivalent the formulation above, as pointed out by user Omnomnomnom in comments.) However, we then got a negative answer to my question -- an example of a non-additive satisfying the condition -- where the infinite-dimensionality of was crucial. This has left us with the tantalizing option that the finite-dimensionality of was crucial to the resolution of the question. I did not want to ask yet another very similar question here, but I also did not want to edit my question, as it had gotten a correct answer that did not deserve to be made irrelevant. Maybe third time's the charm?","\| \cdot \| V f : V \to V 
\| f(x-y)\| = \|f(x) - f(y)\|
 x, y \in V f f(x + y) = f(x) + f(y) x, y \| \cdot \| \| \cdot \| f x, y, f(x), f(y) V f V V","['linear-algebra', 'normed-spaces', 'functional-equations']"
11,Set of linear functionals span the dual space iff intersection of their kernels is $\{0\}$.,Set of linear functionals span the dual space iff intersection of their kernels is .,\{0\},"I was wondering if anyone could offer some insight into the following problem: Let $\mathit{V}$ be a vector space over a field $\mathbb{F}$ . Assume that dim $\mathit{V}$ is finite. Let $f_1, \ldots, f_k$ $\in$ $\mathit{V}$ ' Show that $\{f_1, \ldots, f_k\}$ span $\mathit{V}$ ' iff $\bigcap_{i=1}^n \ker(f_i)$ = $\{0\}$",I was wondering if anyone could offer some insight into the following problem: Let be a vector space over a field . Assume that dim is finite. Let ' Show that span ' iff =,"\mathit{V} \mathbb{F} \mathit{V} f_1, \ldots, f_k \in \mathit{V} \{f_1, \ldots, f_k\} \mathit{V} \bigcap_{i=1}^n \ker(f_i) \{0\}","['linear-algebra', 'dual-spaces']"
12,"Prove that for every complex $2$x$2$ matrix $A$, there exists a matrix $X$ such that $X^3=A^2$.","Prove that for every complex x matrix , there exists a matrix  such that .",2 2 A X X^3=A^2,"This is a problem I've been stuck on for a while. I know that this statement is false if it had $A$ instead of $A^2$ , for example, the matrix $A = \begin{bmatrix} 0&1\\0&0 \end{bmatrix}$ can't be written as $X^3$ , but since $A^2$ equals $ \begin{bmatrix} 0&0\\0&0 \end{bmatrix}$ , it can be written as $X^3$ with $X$ being $ \begin{bmatrix} 0&0\\0&0 \end{bmatrix}$ for example. Other than this, I have no insight into why this statement could be true: nothing can be assumed about diagonalizability, invertibility or anything else. I might be overlooking something simple. Thanks in advance.","This is a problem I've been stuck on for a while. I know that this statement is false if it had instead of , for example, the matrix can't be written as , but since equals , it can be written as with being for example. Other than this, I have no insight into why this statement could be true: nothing can be assumed about diagonalizability, invertibility or anything else. I might be overlooking something simple. Thanks in advance.",A A^2 A = \begin{bmatrix} 0&1\\0&0 \end{bmatrix} X^3 A^2  \begin{bmatrix} 0&0\\0&0 \end{bmatrix} X^3 X  \begin{bmatrix} 0&0\\0&0 \end{bmatrix},"['linear-algebra', 'matrices', 'matrix-equations']"
13,Cardinality of certain subsets in vector spaces over finite fields,Cardinality of certain subsets in vector spaces over finite fields,,Assume that you have an $n$ -dimensional vector space over a finite field (therefore the number of elements in the vector space is finite.) and $F$ is a subset of this vector space which contains $m$ elements. Let's $A$ is a subset of this vector space when the intersection of $A+A$ and $F$ is empty. The question is this: What is a non trivial lower bound for the cardinality of $A$ ? Thank you.,Assume that you have an -dimensional vector space over a finite field (therefore the number of elements in the vector space is finite.) and is a subset of this vector space which contains elements. Let's is a subset of this vector space when the intersection of and is empty. The question is this: What is a non trivial lower bound for the cardinality of ? Thank you.,n F m A A+A F A,"['linear-algebra', 'abstract-algebra', 'combinatorics', 'extremal-combinatorics']"
14,Dual of $\mathbb{Q}$[x] is not isomorphic to $\mathbb{Q}$[x],Dual of [x] is not isomorphic to [x],\mathbb{Q} \mathbb{Q},Denote by $\mathbb{Q}$ the set of the rational numbers. Denote by $\mathbb{Q}[x]$ the vector space over $\mathbb{Q}$ of the polynomials with rational coefficients. Denote by $(\mathbb{Q}[x] )^{\star}$ the dual of $\mathbb{Q}$ [x] . I am trying to show that $(\mathbb{Q}[x] )^{\star}$ and $\mathbb{Q}[x]  $ are not isomorphic (that is: does not exist a linear transformation which is bijective). I really don't know how to start. Someone could help me ? Thanks in advance,Denote by the set of the rational numbers. Denote by the vector space over of the polynomials with rational coefficients. Denote by the dual of [x] . I am trying to show that and are not isomorphic (that is: does not exist a linear transformation which is bijective). I really don't know how to start. Someone could help me ? Thanks in advance,\mathbb{Q} \mathbb{Q}[x] \mathbb{Q} (\mathbb{Q}[x] )^{\star} \mathbb{Q} (\mathbb{Q}[x] )^{\star} \mathbb{Q}[x]  ,['linear-algebra']
15,"For any $2$ x $2$ matrix $A$, does there always exist a $2$ x $2$ matrix $B$ such that det($A+B$) = det($A$) + det($B$)?","For any  x  matrix , does there always exist a  x  matrix  such that det() = det() + det()?",2 2 A 2 2 B A+B A B,"For each invertible $2$ x $2$ matrix $A$ , does there exist an invertible $2$ x $2$ matrix $B$ such that the following conditions hold? (1) $A + B$ is invertible (2) det( $A+B$ ) = det( $A$ ) + det( $B$ ) I know that for $2$ x $2$ matrices det( $A+B$ ) = det( $A$ ) + det( $B$ ) + tr( $A$ )tr( $B$ ) - tr( $AB$ ). So this means tr( $A$ )tr( $B$ ) = tr( $AB$ ). Right now I am having trouble proving that there exists a $B$ that satisfies this equation as well as condition (1).","For each invertible x matrix , does there exist an invertible x matrix such that the following conditions hold? (1) is invertible (2) det( ) = det( ) + det( ) I know that for x matrices det( ) = det( ) + det( ) + tr( )tr( ) - tr( ). So this means tr( )tr( ) = tr( ). Right now I am having trouble proving that there exists a that satisfies this equation as well as condition (1).",2 2 A 2 2 B A + B A+B A B 2 2 A+B A B A B AB A B AB B,['linear-algebra']
16,Let $T:V\to V$ be a diagonalizable linear map on a vector space $V$. Does it hold that $T^{**}:V^{**}\to V^{**}$ is diagonalizable?,Let  be a diagonalizable linear map on a vector space . Does it hold that  is diagonalizable?,T:V\to V V T^{**}:V^{**}\to V^{**},"Edit: I found my own answer.  But I will be happy to see other answers as well. Question: Let $T:V\to V$ be a linear operator on a vector space $V$ . (a) Suppose that $T$ is diagonalizable. Does it hold that $T^{**}:V^{**}\to V^{**}$ is diagonalizable? (b) If $V$ has a generalized eigenspace decomposition with respect to $T$ , then does it also hold that $V^{**}$ has a generalized eigenspace decomposition with respect to $T^{**}$ ? Here, $V$ is defined over a field $F$ and $V^*$ denotes the algebraic dual $\operatorname{Hom}_F(V,F)$ of $V$ .  So, the double dual of $V$ is $V^{**}=(V^*)^*$ .  The dual map $T^*:V^*\to V^*$ is defined by $T^*(f)=f\circ T$ for each $f:V\to F$ .  Similarly, the double dual $T^{**}=(T^*)^*$ satisfies $$T^{**}\varphi(f)=(\varphi\circ T^*)(f)=\varphi(f\circ T)$$ for all $\varphi:V^*\to F$ and $f\in V^*$ . We say that $T$ is diagonalizable if $V$ has a direct sum decomposition into eigenspaces $V_\lambda$ of $T$ $$V=\bigoplus_{\lambda\in F} V_\lambda,$$ where $V_\lambda=\big\{v\in V:Tv=\lambda v\big\}$ .  If $V_\lambda\neq \{0\}$ , $\lambda$ is an eigenvalue of $T$ . We can also say that $T$ is Jordanizable if $V$ has a direct sum decomposition into generalized eigenspaces $V^\lambda$ of $T$ $$V=\bigoplus_{\lambda\in F} V^\lambda,$$ where $V^\lambda=\big\{v\in V:(T-\lambda)^dv=0\text{ for some integer }d>0\big\}$ .  If $V^\lambda\neq \{0\}$ , $\lambda$ is a generalized eigenvalue of $T$ .  A Jordan block of $T$ is a subspace $J$ of some $V^\lambda$ that is an indecomposable $F[X]$ -submodule with the action $X\cdot v=Tv$ for all $v\in V$ that is not a subspace of a larger indecomposable $F[X]$ -submodule of $V$ . Here is what is known. If $V$ is finite dimensional, then we have a natural identification $V=V^{**}$ , so $T$ and $T^{**}$ are basically the same operator under this identification.   Thus, the answers to both questions are positive. However, if $V$ is infinite dimensional, things are a little cloudy.  I know that if $T$ is diagonalizable and has finitely many eigenvalues, then so does $T^{**}$ .  Hence, the answer to (a) is yes in this case.  But what if $T$ has infinitely many eigenvalues? We can say something similar for (b).  Suppose $V$ can be decomposed into a direct sum of finitely many generalized eigenspaces of $T$ .  If for each generalized eigenvalue $\lambda$ , there exists a positive integer $d_\lambda$ such that any $(T-\lambda)^{d_\lambda}$ vanishes on the generalized eigenspace $V^\lambda$ , then $V^{**}$ also can be decomposed into a direct sum of finitely many generalized eigenspaces of $T^{**}$ .  But I don't have the answer if $T$ has infinitely many generalized eigenvalues or if $d_\lambda$ does not exist for some $\lambda$ . The idea of the proof of my partial results for (a) and (b) is that if $p(T)=0$ for some polynomial $p$ , then $p(T^{**})=0$ as well.  But this proof does not work if $T$ has infinitely many (generalized) eigenvalue or if there exist arbitrarily large Jordan blocks (in terms of dimensions).  I do not expect that the answer to (a) is yes (and so this will answer (b) as well), but I cannot find a way to construct a counterexample. In fact, let $V^{*m}$ denote the $m$ th dual of $V$ and $T^{*m}$ the $m$ th dual of $T$ (i.e., $V^{*m}=\big(V^{*(m-1)}\big)^*$ and $T^{*m}=\big(T^{*(m-1)}\big)^*$ , with $V^{*0}=V$ and $T^{*0}=T$ ).  If $T$ satisfies the polynomial equation $p(T)=0$ , then every $T^{*m}$ satisfies $p\big(T^{*m}\big)=0$ .  Hence, if $T$ is diagonalizable with finitely many eigenvalues, then so are all $T^{*m}$ .  If $V$ has a generalized eigenspace decomposition wrt $T$ with finitely many generalized eigenvalues and there is a universal upper bound on the dimension of every Jordan block, then $V^{*m}$ has a generalized eigenspace decomposition wrt $T^{*m}$ .","Edit: I found my own answer.  But I will be happy to see other answers as well. Question: Let be a linear operator on a vector space . (a) Suppose that is diagonalizable. Does it hold that is diagonalizable? (b) If has a generalized eigenspace decomposition with respect to , then does it also hold that has a generalized eigenspace decomposition with respect to ? Here, is defined over a field and denotes the algebraic dual of .  So, the double dual of is .  The dual map is defined by for each .  Similarly, the double dual satisfies for all and . We say that is diagonalizable if has a direct sum decomposition into eigenspaces of where .  If , is an eigenvalue of . We can also say that is Jordanizable if has a direct sum decomposition into generalized eigenspaces of where .  If , is a generalized eigenvalue of .  A Jordan block of is a subspace of some that is an indecomposable -submodule with the action for all that is not a subspace of a larger indecomposable -submodule of . Here is what is known. If is finite dimensional, then we have a natural identification , so and are basically the same operator under this identification.   Thus, the answers to both questions are positive. However, if is infinite dimensional, things are a little cloudy.  I know that if is diagonalizable and has finitely many eigenvalues, then so does .  Hence, the answer to (a) is yes in this case.  But what if has infinitely many eigenvalues? We can say something similar for (b).  Suppose can be decomposed into a direct sum of finitely many generalized eigenspaces of .  If for each generalized eigenvalue , there exists a positive integer such that any vanishes on the generalized eigenspace , then also can be decomposed into a direct sum of finitely many generalized eigenspaces of .  But I don't have the answer if has infinitely many generalized eigenvalues or if does not exist for some . The idea of the proof of my partial results for (a) and (b) is that if for some polynomial , then as well.  But this proof does not work if has infinitely many (generalized) eigenvalue or if there exist arbitrarily large Jordan blocks (in terms of dimensions).  I do not expect that the answer to (a) is yes (and so this will answer (b) as well), but I cannot find a way to construct a counterexample. In fact, let denote the th dual of and the th dual of (i.e., and , with and ).  If satisfies the polynomial equation , then every satisfies .  Hence, if is diagonalizable with finitely many eigenvalues, then so are all .  If has a generalized eigenspace decomposition wrt with finitely many generalized eigenvalues and there is a universal upper bound on the dimension of every Jordan block, then has a generalized eigenspace decomposition wrt .","T:V\to V V T T^{**}:V^{**}\to V^{**} V T V^{**} T^{**} V F V^* \operatorname{Hom}_F(V,F) V V V^{**}=(V^*)^* T^*:V^*\to V^* T^*(f)=f\circ T f:V\to F T^{**}=(T^*)^* T^{**}\varphi(f)=(\varphi\circ T^*)(f)=\varphi(f\circ T) \varphi:V^*\to F f\in V^* T V V_\lambda T V=\bigoplus_{\lambda\in F} V_\lambda, V_\lambda=\big\{v\in V:Tv=\lambda v\big\} V_\lambda\neq \{0\} \lambda T T V V^\lambda T V=\bigoplus_{\lambda\in F} V^\lambda, V^\lambda=\big\{v\in V:(T-\lambda)^dv=0\text{ for some integer }d>0\big\} V^\lambda\neq \{0\} \lambda T T J V^\lambda F[X] X\cdot v=Tv v\in V F[X] V V V=V^{**} T T^{**} V T T^{**} T V T \lambda d_\lambda (T-\lambda)^{d_\lambda} V^\lambda V^{**} T^{**} T d_\lambda \lambda p(T)=0 p p(T^{**})=0 T V^{*m} m V T^{*m} m T V^{*m}=\big(V^{*(m-1)}\big)^* T^{*m}=\big(T^{*(m-1)}\big)^* V^{*0}=V T^{*0}=T T p(T)=0 T^{*m} p\big(T^{*m}\big)=0 T T^{*m} V T V^{*m} T^{*m}","['linear-algebra', 'vector-spaces']"
17,"Center of a non-abelian subgroup of $GL(2, \mathbb{C})$",Center of a non-abelian subgroup of,"GL(2, \mathbb{C})","I'm trying to do the following exercise: Let be $G$ a non-abelian subgroup of $GL(2, \mathbb{C})$ . Prove that the center of $G$ is contained in the center of $GL(2, \mathbb{C})$ My (very partial) attempt: Suppose that  there is a matrix $A$ in the center of $G$ but not in that of $GL(2, \mathbb{C})$ . But the center is a normal subgroup, so it contains all the conjugates of $A$ . In particular the center of $GL(2, \mathbb{C})$ does not contain the matrix $J$ , the canonical Jordan form of $A$ . Instead there is a subgroup isomorphic to $G$ that contains $J$ . My idea is to prove that it is abelian against the hypothesis. I know that $J$ could have only one of these two form: diagonal (but not a multiple of identity) and a Jordan block. But I don't know how to continue... Any suggestion?  Thanks in advance.","I'm trying to do the following exercise: Let be a non-abelian subgroup of . Prove that the center of is contained in the center of My (very partial) attempt: Suppose that  there is a matrix in the center of but not in that of . But the center is a normal subgroup, so it contains all the conjugates of . In particular the center of does not contain the matrix , the canonical Jordan form of . Instead there is a subgroup isomorphic to that contains . My idea is to prove that it is abelian against the hypothesis. I know that could have only one of these two form: diagonal (but not a multiple of identity) and a Jordan block. But I don't know how to continue... Any suggestion?  Thanks in advance.","G GL(2, \mathbb{C}) G GL(2, \mathbb{C}) A G GL(2, \mathbb{C}) A GL(2, \mathbb{C}) J A G J J","['linear-algebra', 'abstract-algebra', 'matrices', 'group-theory', 'jordan-normal-form']"
18,Eigenvalues of the second exterior power of a linear operator,Eigenvalues of the second exterior power of a linear operator,,"Let $K$ be a field of characteristic zero and $V=K^n$ . Let $T: V \to V$ be a linear map with eigenvalues $\lambda_1,...,\lambda_n \in K$ , not necessarily all distinct. Let $\wedge^2 V$ be the second exterior power of $V$ and $\wedge^2T: \wedge^2 V \to \wedge^2 V$ is the linear map defined as $\wedge^2T(x\wedge y)=T(x)\wedge T(y),\forall x,y\in V$ and extend it to whole of $\wedge ^2V$ linearly. What are all the eigenvalues of $\wedge^2T$ ? I can easily show that  all $\lambda_i\lambda_j$ s , with $i\ne j$ , are eigenvalues of $\wedge^2T$ ; my question is: are there any other eigenvalues ? EDIT: I would accept an answer even if just for the $K=\mathbb C$ case","Let be a field of characteristic zero and . Let be a linear map with eigenvalues , not necessarily all distinct. Let be the second exterior power of and is the linear map defined as and extend it to whole of linearly. What are all the eigenvalues of ? I can easily show that  all s , with , are eigenvalues of ; my question is: are there any other eigenvalues ? EDIT: I would accept an answer even if just for the case","K V=K^n T: V \to V \lambda_1,...,\lambda_n \in K \wedge^2 V V \wedge^2T: \wedge^2 V \to \wedge^2 V \wedge^2T(x\wedge y)=T(x)\wedge T(y),\forall x,y\in V \wedge ^2V \wedge^2T \lambda_i\lambda_j i\ne j \wedge^2T K=\mathbb C","['linear-algebra', 'eigenvalues-eigenvectors', 'linear-transformations', 'exterior-algebra']"
19,Elementary Symmetric Polynomials and Determinant,Elementary Symmetric Polynomials and Determinant,,"I am trying to show that \begin{equation} \sqrt{\Delta} = \prod\limits_{1 \leq i < j \leq n} \left( x_i - x_j \right) = \det \begin{pmatrix} \dfrac{\partial \sigma_{n,1}}{\partial x_1} & \cdots & \dfrac{\partial \sigma_{n,n}}{\partial x_1} \\ \vdots & \ddots & \vdots \\ \dfrac{\partial \sigma_{n,1}}{\partial x_n} & \cdots & \dfrac{\partial \sigma_{n,n}}{\partial x_n} \end{pmatrix} \end{equation} where $\sigma_{n,j}$ are the elementary symmetric polynomials of $n$ variables $x_1, x_2, · · · , x_n$ Does anyone know of another method to do so other than induction since I am not sure how the inductive step works out~","I am trying to show that \begin{equation} \sqrt{\Delta} = \prod\limits_{1 \leq i < j \leq n} \left( x_i - x_j \right) = \det \begin{pmatrix} \dfrac{\partial \sigma_{n,1}}{\partial x_1} & \cdots & \dfrac{\partial \sigma_{n,n}}{\partial x_1} \\ \vdots & \ddots & \vdots \\ \dfrac{\partial \sigma_{n,1}}{\partial x_n} & \cdots & \dfrac{\partial \sigma_{n,n}}{\partial x_n} \end{pmatrix} \end{equation} where $\sigma_{n,j}$ are the elementary symmetric polynomials of $n$ variables $x_1, x_2, · · · , x_n$ Does anyone know of another method to do so other than induction since I am not sure how the inductive step works out~",,"['linear-algebra', 'abstract-algebra', 'galois-theory', 'symmetric-polynomials']"
20,Tensor norms on infinite Banach space,Tensor norms on infinite Banach space,,"Given two Banach spaces $V$ and $W$ and its tensor product $V \otimes W$.In Ryan's book ""Introduction into Tensor product of Banach space"", he said that it is natural to choose a norm for elementary tensors as $$\|v \otimes w\|_{V\otimes W} \leqslant \|v \|_{V}\| w \|_W$$ Now I don't understand why it is natural to require the above-mentioned inequality for a tensor norm?","Given two Banach spaces $V$ and $W$ and its tensor product $V \otimes W$.In Ryan's book ""Introduction into Tensor product of Banach space"", he said that it is natural to choose a norm for elementary tensors as $$\|v \otimes w\|_{V\otimes W} \leqslant \|v \|_{V}\| w \|_W$$ Now I don't understand why it is natural to require the above-mentioned inequality for a tensor norm?",,"['linear-algebra', 'tensor-products']"
21,Existence of a similar positive definite matrix,Existence of a similar positive definite matrix,,"Assume $A$ is a real (non-symmetric) positive definite matrix, that is, $$ x^T A x > 0 $$ for all real non-zero real $x$. It is easy to prove that the eigenvalues of $A$ have positive real part. Conversely, assume $A$ is a (non-symmetric) real matrix, whose eigenvalues have positive real part. It does not necessarily follow that $A$ is positive definite. Nonetheless, does there exist a similar matrix $B$ which is positive definite? More precisely, does there exist an invertible matrix $P$ such that $P^{-1} A P$ is positive definite?","Assume $A$ is a real (non-symmetric) positive definite matrix, that is, $$ x^T A x > 0 $$ for all real non-zero real $x$. It is easy to prove that the eigenvalues of $A$ have positive real part. Conversely, assume $A$ is a (non-symmetric) real matrix, whose eigenvalues have positive real part. It does not necessarily follow that $A$ is positive definite. Nonetheless, does there exist a similar matrix $B$ which is positive definite? More precisely, does there exist an invertible matrix $P$ such that $P^{-1} A P$ is positive definite?",,"['linear-algebra', 'positive-definite', 'change-of-basis']"
22,How to claim the subspace $AB-BA$ has dimension $n^2-1$?,How to claim the subspace  has dimension ?,AB-BA n^2-1,"There is a problem given in LINEAR ALGEBRA by  HOFFMAN & kUNZE PAGE $107$, Show that the trace functional on squre matrices of order $n$ is characterized in the following sense: If $W$ is the space of size $n$ matrices over the field $F$ and if $f$ is a linear functional on $W$ such that  $f(AB)=f(BA)$ for each $A$ and $B$ in $W$, then $f$ is a scalar multiple of the trace function. MY TRY: Put $C=AB-BA$. Clearly $\operatorname{Tr} C=0$. I claim that dimension of the subspace of such $C$ is $n^2-1$. So by the rank-nullity theorem $$\dim \ker f +\dim\operatorname{Im} f=n^2$$ i.e $n^2-1+\dim\operatorname{Im}=n^2$ ( as $f(C)=0$). Hence $\operatorname{rank} f=1$. Now since trace functional satisfies such $f$'s and it's scalar multiple is also a subspace of dimension $1$, $f$ needs to be scalar multiple of $\operatorname{Tr}$. MY PROBLEM: I claimed that subspace (the terrible fact is I'm not sure whether it's a subspace or not, can't prove it) of the matrices of the form $AB-BA$ has dimension with $n^2-1$. It means that there is no restriction on the entries of $AB-BA$ except the fact that their trace is $0$. How can I prove it? Any other approach or hint to solve that problem will also help me. Thanks for reading!","There is a problem given in LINEAR ALGEBRA by  HOFFMAN & kUNZE PAGE $107$, Show that the trace functional on squre matrices of order $n$ is characterized in the following sense: If $W$ is the space of size $n$ matrices over the field $F$ and if $f$ is a linear functional on $W$ such that  $f(AB)=f(BA)$ for each $A$ and $B$ in $W$, then $f$ is a scalar multiple of the trace function. MY TRY: Put $C=AB-BA$. Clearly $\operatorname{Tr} C=0$. I claim that dimension of the subspace of such $C$ is $n^2-1$. So by the rank-nullity theorem $$\dim \ker f +\dim\operatorname{Im} f=n^2$$ i.e $n^2-1+\dim\operatorname{Im}=n^2$ ( as $f(C)=0$). Hence $\operatorname{rank} f=1$. Now since trace functional satisfies such $f$'s and it's scalar multiple is also a subspace of dimension $1$, $f$ needs to be scalar multiple of $\operatorname{Tr}$. MY PROBLEM: I claimed that subspace (the terrible fact is I'm not sure whether it's a subspace or not, can't prove it) of the matrices of the form $AB-BA$ has dimension with $n^2-1$. It means that there is no restriction on the entries of $AB-BA$ except the fact that their trace is $0$. How can I prove it? Any other approach or hint to solve that problem will also help me. Thanks for reading!",,"['linear-algebra', 'linear-transformations']"
23,Sum of subspace and its orthogonal complement,Sum of subspace and its orthogonal complement,,"Let $V$ be a (not necessarily finite-dimensional) vector space (over a field $K$), $U$ a subspace of $V$ and $\beta\colon V \times V \to K$ a bilinear form. Define $U^\perp = \{v\in V : \beta(v,u)=0 \ \  \forall u \in U \}$ to be the (left) orthogonal complement of $U$. Which conditions do we need in order to get the identity $V = U + U^\perp$ or $V = U \oplus U^\perp$? I think one needs $U$ to be finite-dimensional and $\beta|_{U\times U}$ to be non-degenerate. Is it then possible for any given vector $v\in V$ to construct a (probably unique) vector $u\in U$ (dependent on $v$) such that $v-u\in U^\perp$, maybe via the map $v \mapsto \sum_{i=1}^m \beta(v,u_i)u_i $ for any given basis $\{u_1,\dotsc,u_m\}$ of $U$? A more general thought: The condition of $\beta|_{U\times U}$ being non-degenerate means that if $u\in U$ and $\beta(u,u')=0$ for all $u'\in U$ then $u=0$. So this is equivalent to $U\cap U^\perp = \{0\}$. If we ignore this condition, that means $U\cap U^\perp \ne \{0\}$, what do we still need for $V = U + U^\perp$?","Let $V$ be a (not necessarily finite-dimensional) vector space (over a field $K$), $U$ a subspace of $V$ and $\beta\colon V \times V \to K$ a bilinear form. Define $U^\perp = \{v\in V : \beta(v,u)=0 \ \  \forall u \in U \}$ to be the (left) orthogonal complement of $U$. Which conditions do we need in order to get the identity $V = U + U^\perp$ or $V = U \oplus U^\perp$? I think one needs $U$ to be finite-dimensional and $\beta|_{U\times U}$ to be non-degenerate. Is it then possible for any given vector $v\in V$ to construct a (probably unique) vector $u\in U$ (dependent on $v$) such that $v-u\in U^\perp$, maybe via the map $v \mapsto \sum_{i=1}^m \beta(v,u_i)u_i $ for any given basis $\{u_1,\dotsc,u_m\}$ of $U$? A more general thought: The condition of $\beta|_{U\times U}$ being non-degenerate means that if $u\in U$ and $\beta(u,u')=0$ for all $u'\in U$ then $u=0$. So this is equivalent to $U\cap U^\perp = \{0\}$. If we ignore this condition, that means $U\cap U^\perp \ne \{0\}$, what do we still need for $V = U + U^\perp$?",,"['linear-algebra', 'vector-spaces', 'orthogonality', 'bilinear-form']"
24,How would the condition for linearity for function with multiple variable be given?,How would the condition for linearity for function with multiple variable be given?,,"I know that a function is called linear if it satisfies the conditions $$f(x+y)=f(x)+f(y)$$ and $$f(ax)=af(x)$$ (i.e. it preserves the properties or operation). How is the condition for linearity given when there are multiple variable in the funciton? for ex $$f(x+y,z)$$ and  $$f(ax,z)$$ I know $$f(x+y,z)=f(x,z)+f(y,z)$$  is a condition for multilinear function, but can there be multivariable function which is linear and not multilinear,ie linear in all the variables at once and not seperately (like in multilinear function). How to make sense of multivariable function that is linear and that is multilinear?  Edit: Extra details. A function $f(x,y)$ is called multlinear(bilinear to be specific) if $$f(x+a,y)=f(x,y)+f(a,y)$$ $$f(x,y+b)=f(x,y)+f(x,b)$$ and also $$f(nx,y)=nf(x,y)=f(x,ny)$$. So is  $$f(x+a,y+b)=f(x,y+b)+f(a,y+b)+f(x+a,y)+f(x+a,b)+f(x,y)+f(a,b)+f(x,b)+f(a,y)$$ when $f(x,y)$ is multilinear and $$f(x+a,y+b)=f(x,y)+f(a,b)$$  when $f(x,y)$ is linear?","I know that a function is called linear if it satisfies the conditions $$f(x+y)=f(x)+f(y)$$ and $$f(ax)=af(x)$$ (i.e. it preserves the properties or operation). How is the condition for linearity given when there are multiple variable in the funciton? for ex $$f(x+y,z)$$ and  $$f(ax,z)$$ I know $$f(x+y,z)=f(x,z)+f(y,z)$$  is a condition for multilinear function, but can there be multivariable function which is linear and not multilinear,ie linear in all the variables at once and not seperately (like in multilinear function). How to make sense of multivariable function that is linear and that is multilinear?  Edit: Extra details. A function $f(x,y)$ is called multlinear(bilinear to be specific) if $$f(x+a,y)=f(x,y)+f(a,y)$$ $$f(x,y+b)=f(x,y)+f(x,b)$$ and also $$f(nx,y)=nf(x,y)=f(x,ny)$$. So is  $$f(x+a,y+b)=f(x,y+b)+f(a,y+b)+f(x+a,y)+f(x+a,b)+f(x,y)+f(a,b)+f(x,b)+f(a,y)$$ when $f(x,y)$ is multilinear and $$f(x+a,y+b)=f(x,y)+f(a,b)$$  when $f(x,y)$ is linear?",,"['linear-algebra', 'multilinear-algebra']"
25,Can an upper triangular matrix be put in jordan form using upper triangular matrices and permutations only.,Can an upper triangular matrix be put in jordan form using upper triangular matrices and permutations only.,,"Give an upper triangular matrix A does there always exist $P, P^{-1}$ and $U, U^{-1}$ such that $PUAU^{-1}P^{-1}$ is in Jordan canonical form and U is upper triangular and P is a permutation matrix. I believe the answer is yes as when we have distinct eigenvalues we can place them together and make diagonal block and that block can be diagonalized by an upper triangular matrix so we need only concern ourselves with the repeated eigenvalues case. when we have repeated eigenvalues we can use permutation matrices in a block say $B_j$ where $j=1,...,q$ where q is the number of distinct eigenvalues. if we put with the eigenvalues down the diagonals (which we can) then we consider $(B_j-\lambda )^i e_i$ for $i=1,2,...$  we have a collection of i generalized eigenvectors for each distinct eigenvalue. this will allow us to find n generalized eigenvectors (the distinct case is obvious) where in the $v_i$th vector will have 0's in the all components after the ith entry. I believe this is equivalent to the desired result. Field is obviously $\Bbb C$ arbitrary $n \times n $ matrix. Notice that it is not true for A not upper triangular. Edit!: I guess we could technically do this over an arbitrary field given that it is upper triangular at least over $\Bbb R $ would be ok as all the eigenvalues would already be real if it's in upper triangular form.","Give an upper triangular matrix A does there always exist $P, P^{-1}$ and $U, U^{-1}$ such that $PUAU^{-1}P^{-1}$ is in Jordan canonical form and U is upper triangular and P is a permutation matrix. I believe the answer is yes as when we have distinct eigenvalues we can place them together and make diagonal block and that block can be diagonalized by an upper triangular matrix so we need only concern ourselves with the repeated eigenvalues case. when we have repeated eigenvalues we can use permutation matrices in a block say $B_j$ where $j=1,...,q$ where q is the number of distinct eigenvalues. if we put with the eigenvalues down the diagonals (which we can) then we consider $(B_j-\lambda )^i e_i$ for $i=1,2,...$  we have a collection of i generalized eigenvectors for each distinct eigenvalue. this will allow us to find n generalized eigenvectors (the distinct case is obvious) where in the $v_i$th vector will have 0's in the all components after the ith entry. I believe this is equivalent to the desired result. Field is obviously $\Bbb C$ arbitrary $n \times n $ matrix. Notice that it is not true for A not upper triangular. Edit!: I guess we could technically do this over an arbitrary field given that it is upper triangular at least over $\Bbb R $ would be ok as all the eigenvalues would already be real if it's in upper triangular form.",,['linear-algebra']
26,Dual norm of a truncated and ordered (decreasing order) $\ell_1$-norm,Dual norm of a truncated and ordered (decreasing order) -norm,\ell_1,"I do not understand yet how the following dual-norm of a truncated and ordered (in decreasing fashion) $\ell_1$-norm $\lVert \mathbf{x}\rVert_{[k]}$ on $\mathbf{x} \in \mathbb{C}^n$ is: $$\lVert \mathbf{x}\rVert^*_{[k]}= \max\left\{\frac{1}{k} \| \mathbf{x} \|_1,\lVert \mathbf{x}\rVert_{\infty}\right\}$$ The  truncated $\ell_1$-norm is defined as the sum of the $k$ largest magnitudes of the entries in $\mathbf{x}$ vector, i.e., $\lVert \mathbf{x}\rVert_{[k]}=\lvert x_{i_1}\rvert+.....+\lvert x_{i_k}\rvert$ in which $\lvert x_{i_1}\rvert\geq\lvert x_{i_2}\rvert\geq.....\geq\lvert x_{i_n}\rvert$. Thank you","I do not understand yet how the following dual-norm of a truncated and ordered (in decreasing fashion) $\ell_1$-norm $\lVert \mathbf{x}\rVert_{[k]}$ on $\mathbf{x} \in \mathbb{C}^n$ is: $$\lVert \mathbf{x}\rVert^*_{[k]}= \max\left\{\frac{1}{k} \| \mathbf{x} \|_1,\lVert \mathbf{x}\rVert_{\infty}\right\}$$ The  truncated $\ell_1$-norm is defined as the sum of the $k$ largest magnitudes of the entries in $\mathbf{x}$ vector, i.e., $\lVert \mathbf{x}\rVert_{[k]}=\lvert x_{i_1}\rvert+.....+\lvert x_{i_k}\rvert$ in which $\lvert x_{i_1}\rvert\geq\lvert x_{i_2}\rvert\geq.....\geq\lvert x_{i_n}\rvert$. Thank you",,"['linear-algebra', 'optimization', 'convex-analysis', 'convex-optimization']"
27,When the Lights Out puzzle vectors over elementary abelian field of dimension $n^2$ form a basis,When the Lights Out puzzle vectors over elementary abelian field of dimension  form a basis,n^2,"Assume we have a $n\times n$ matrix, where $n$ is odd. The elements in this matrix are either $0$ or $1$. We can change the values of the elements in the matrix as follows: changing the value of position $\{i,j\}$ from $0$ to $1$ or vice versa also changes the values in positions $\{i-1,j\}$,$\{i+1,j\}$,$\{i,j-1\}$ and $\{i,j+1\}$ respectively. If any of these falls outside the matrix, we ignore it. Call these the basic operations. For example, if $n = 3$ and the matrix is all zeroes, changing the value of $\{1,2\}$ would result in $$ \left( \begin{array}{ccc}  1 & 1 & 1 \\  0 & 1 & 0 \\  0 & 0 & 0 \\ \end{array} \right) $$ and so on. We can turn the matrixes into vectors so that position $\{i,j\}$ of the matrix is the element $(i-1)n+j$ of the vector. The above example is thus the same as adding vector {1,1,1,0,1,0,0,0,0} to zero vector of length $9$. The addition is done modulo $2$. These vectors are elements of a $n^2$-dimensional vector space over the elementary abelian field of order $2$. The vectors associated with the basic operations form a basis of some subspace of this vector space. If we look at the set of these vectors when $n=3$, we have the following, written as matrix: $$\left( \begin{array}{ccccccccc}  1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\  1 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 \\  0 & 1 & 1 & 0 & 0 & 1 & 0 & 0 & 0 \\  1 & 0 & 0 & 1 & 1 & 0 & 1 & 0 & 0 \\  0 & 1 & 0 & 1 & 1 & 1 & 0 & 1 & 0 \\  0 & 0 & 1 & 0 & 1 & 1 & 0 & 0 & 1 \\  0 & 0 & 0 & 1 & 0 & 0 & 1 & 1 & 0 \\  0 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 1 \\  0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 1 \\ \end{array} \right)$$  These vectors are linearly independent and there are $9$ of them and thus form a basis of the whole space. This means that we can reach any configuration of zeroes and ones in the matrix by basic operations. What if $n=5$? We have, again, a set of $25$ vectors associated with the basic operations, very similar to the case $n=3$: $$\left( \begin{array}{ccccccccccccccccccccccccc}  1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 \\ \end{array} \right)$$ However, thse vectors are not linearly independent and thus do not form a basis of the whole space. In fact, they span a subspace of dimension $23$. This can be interpreted so that we can reach any configuration of zeroes and ones using basic operations, apart from two positions which are out of our control and may or may not have the desired values. If $n=7$, the set of vectors associated with the basic operations are again linearly independent. If $n=9$ or $n=11$, the vectors are not linearly independent, but are again independent if $n=13$. The question is why in the case of $n=3$ or $n=7$ we can reach all possible configurations of zeroes and ones in the matrix with the basic operations but not in the case of $n=5$. And further, for which values of $n$ are the vectors associated with the basic operations linearly independent? Testing some values of $n$, the list of such values of $n$ begins as $$\{1,3,7,13,15,21,25,27,31,37,43\}.$$  Looking the list up in OEIS returns only one result: A209839, which seems to have very little to do with this problem. There is really no reason to restrict ourselves with odd $n$s. For general $n$, the values for $n$ that produce vectors that span the whole space begins $\{1,2,3,6,7,8,10,12,13,15,18,20,21,22,25\}.$ This is sequence A076436 at OEIS. Edit: I had made an error in my program which resulted in wrong sequence and wrong dimensions of the subspaces. I have fixed that now. See comments for the correct differences in dimensions.","Assume we have a $n\times n$ matrix, where $n$ is odd. The elements in this matrix are either $0$ or $1$. We can change the values of the elements in the matrix as follows: changing the value of position $\{i,j\}$ from $0$ to $1$ or vice versa also changes the values in positions $\{i-1,j\}$,$\{i+1,j\}$,$\{i,j-1\}$ and $\{i,j+1\}$ respectively. If any of these falls outside the matrix, we ignore it. Call these the basic operations. For example, if $n = 3$ and the matrix is all zeroes, changing the value of $\{1,2\}$ would result in $$ \left( \begin{array}{ccc}  1 & 1 & 1 \\  0 & 1 & 0 \\  0 & 0 & 0 \\ \end{array} \right) $$ and so on. We can turn the matrixes into vectors so that position $\{i,j\}$ of the matrix is the element $(i-1)n+j$ of the vector. The above example is thus the same as adding vector {1,1,1,0,1,0,0,0,0} to zero vector of length $9$. The addition is done modulo $2$. These vectors are elements of a $n^2$-dimensional vector space over the elementary abelian field of order $2$. The vectors associated with the basic operations form a basis of some subspace of this vector space. If we look at the set of these vectors when $n=3$, we have the following, written as matrix: $$\left( \begin{array}{ccccccccc}  1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\  1 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 \\  0 & 1 & 1 & 0 & 0 & 1 & 0 & 0 & 0 \\  1 & 0 & 0 & 1 & 1 & 0 & 1 & 0 & 0 \\  0 & 1 & 0 & 1 & 1 & 1 & 0 & 1 & 0 \\  0 & 0 & 1 & 0 & 1 & 1 & 0 & 0 & 1 \\  0 & 0 & 0 & 1 & 0 & 0 & 1 & 1 & 0 \\  0 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 1 \\  0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 1 \\ \end{array} \right)$$  These vectors are linearly independent and there are $9$ of them and thus form a basis of the whole space. This means that we can reach any configuration of zeroes and ones in the matrix by basic operations. What if $n=5$? We have, again, a set of $25$ vectors associated with the basic operations, very similar to the case $n=3$: $$\left( \begin{array}{ccccccccccccccccccccccccc}  1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 \\ \end{array} \right)$$ However, thse vectors are not linearly independent and thus do not form a basis of the whole space. In fact, they span a subspace of dimension $23$. This can be interpreted so that we can reach any configuration of zeroes and ones using basic operations, apart from two positions which are out of our control and may or may not have the desired values. If $n=7$, the set of vectors associated with the basic operations are again linearly independent. If $n=9$ or $n=11$, the vectors are not linearly independent, but are again independent if $n=13$. The question is why in the case of $n=3$ or $n=7$ we can reach all possible configurations of zeroes and ones in the matrix with the basic operations but not in the case of $n=5$. And further, for which values of $n$ are the vectors associated with the basic operations linearly independent? Testing some values of $n$, the list of such values of $n$ begins as $$\{1,3,7,13,15,21,25,27,31,37,43\}.$$  Looking the list up in OEIS returns only one result: A209839, which seems to have very little to do with this problem. There is really no reason to restrict ourselves with odd $n$s. For general $n$, the values for $n$ that produce vectors that span the whole space begins $\{1,2,3,6,7,8,10,12,13,15,18,20,21,22,25\}.$ This is sequence A076436 at OEIS. Edit: I had made an error in my program which resulted in wrong sequence and wrong dimensions of the subspaces. I have fixed that now. See comments for the correct differences in dimensions.",,"['linear-algebra', 'vector-spaces']"
28,"$p,q,r$ three projections; show that if $p+q+r = 0$, then $p =q =r =0$","three projections; show that if , then","p,q,r p+q+r = 0 p =q =r =0","Let $\mathbb F$ be a field of zero characteristic. Let $E$ be a $\mathbb F$-vector-space. Let $p,q,r$ be three projections of $E$. Prove that if $p+q+r = 0$, then $p =q =r =0$. The case of finite dimension is really easy using the trace (since the rank or a projection is equal to its trace). How to deal with the case of infinite dimension? Some thoughts: One can see that $p(E) \cap q(E)= \{0\}$ since $-2$ is not an eigenvalue for $r$. Likewise, $p(E) \cap r(E)= q(E) \cap r(E) =\{0\}$. Otherwise, $\ker p \cap \ker q \subset \ker r$ $\ker p \cap \ker r \subset \ker q$ and $\ker q \cap \ker r \subset \ker p$ ...","Let $\mathbb F$ be a field of zero characteristic. Let $E$ be a $\mathbb F$-vector-space. Let $p,q,r$ be three projections of $E$. Prove that if $p+q+r = 0$, then $p =q =r =0$. The case of finite dimension is really easy using the trace (since the rank or a projection is equal to its trace). How to deal with the case of infinite dimension? Some thoughts: One can see that $p(E) \cap q(E)= \{0\}$ since $-2$ is not an eigenvalue for $r$. Likewise, $p(E) \cap r(E)= q(E) \cap r(E) =\{0\}$. Otherwise, $\ker p \cap \ker q \subset \ker r$ $\ker p \cap \ker r \subset \ker q$ and $\ker q \cap \ker r \subset \ker p$ ...",,['linear-algebra']
29,$T$ has no eigenvalues,has no eigenvalues,T,"Is the following Proof Correct? Given that $T\in\mathcal{L}(\mathbf{R}^2)$ defined by $T(x,y) =  (-3y,x)$. $T$ has no eigenvalues. Proof. Let $\sigma_T$ denote the set of all eigenvalues of $T$ and assume that $\sigma_T\neq\varnothing$ then for some $\lambda\in\sigma_T$ we have $T(x,y) = \lambda(x,y) = (-3y,x)$ where $(x,y)\neq (0,0)$, equivalently $\lambda x = -3y\text{ and }\lambda y = x$. but then $\lambda(\lambda y) = -3y$ equivalently $y(\lambda^2+3) = 0$. The equation $\lambda^2+3 = 0$ has no solutions in $\mathbf{R}$ consequently $y=0$ and then by equation $\lambda y  = x$ it follows that $x=0$  thus $(x,y) = (0,0)$ contradicting the fact that $(x,y)\neq (0,0)$. $\blacksquare$","Is the following Proof Correct? Given that $T\in\mathcal{L}(\mathbf{R}^2)$ defined by $T(x,y) =  (-3y,x)$. $T$ has no eigenvalues. Proof. Let $\sigma_T$ denote the set of all eigenvalues of $T$ and assume that $\sigma_T\neq\varnothing$ then for some $\lambda\in\sigma_T$ we have $T(x,y) = \lambda(x,y) = (-3y,x)$ where $(x,y)\neq (0,0)$, equivalently $\lambda x = -3y\text{ and }\lambda y = x$. but then $\lambda(\lambda y) = -3y$ equivalently $y(\lambda^2+3) = 0$. The equation $\lambda^2+3 = 0$ has no solutions in $\mathbf{R}$ consequently $y=0$ and then by equation $\lambda y  = x$ it follows that $x=0$  thus $(x,y) = (0,0)$ contradicting the fact that $(x,y)\neq (0,0)$. $\blacksquare$",,"['linear-algebra', 'proof-verification', 'eigenvalues-eigenvectors']"
30,geometric view of similar vs congruent matrices,geometric view of similar vs congruent matrices,,"I'm trying to understand similarity $(A \sim B \iff A = SBS^{-1})$ and congruence $(A \cong B \iff A = PBP^T,\, P\in GL(n, \mathbb{R}))$ through geometric analogies. For triangles: affine transformations (i.e., uniform scaling, rotation, reflection, translation[?]) preserve similarity. Two similar triangles have equivalent angles but may have different side lengths (i.e., they are measured in a different basis) unitary transformations (i.e., rotation, reflection, translation[?]) preserve congruence For matrices: similarity preserves the idea of a linear transformation; most ""geometrically"", eigenvalues are preserved, but angles between vectors are not preserved congruence preserves the above DOESN'T preserve the spectrum and but preserves angles between vectors (my interpretation of the bilinear form property) and the number of positive / negative / zero eigenvalues However, we only require $P$ to be invertible for matrix congruence $A = PBP^T$. Why don't we require $P$ to be unitary (i.e., why do we allow the spectrum to change)? Why is the adjoint $P^T$ important? Also, is there an analogy for thinking about (1) how angles between vectors are preserved by congruence but not similarity with matrices and (2) how angles between triangle legs are preserved with both similarity and congruence, or am I reading too much into the naming conventions?","I'm trying to understand similarity $(A \sim B \iff A = SBS^{-1})$ and congruence $(A \cong B \iff A = PBP^T,\, P\in GL(n, \mathbb{R}))$ through geometric analogies. For triangles: affine transformations (i.e., uniform scaling, rotation, reflection, translation[?]) preserve similarity. Two similar triangles have equivalent angles but may have different side lengths (i.e., they are measured in a different basis) unitary transformations (i.e., rotation, reflection, translation[?]) preserve congruence For matrices: similarity preserves the idea of a linear transformation; most ""geometrically"", eigenvalues are preserved, but angles between vectors are not preserved congruence preserves the above DOESN'T preserve the spectrum and but preserves angles between vectors (my interpretation of the bilinear form property) and the number of positive / negative / zero eigenvalues However, we only require $P$ to be invertible for matrix congruence $A = PBP^T$. Why don't we require $P$ to be unitary (i.e., why do we allow the spectrum to change)? Why is the adjoint $P^T$ important? Also, is there an analogy for thinking about (1) how angles between vectors are preserved by congruence but not similarity with matrices and (2) how angles between triangle legs are preserved with both similarity and congruence, or am I reading too much into the naming conventions?",,"['linear-algebra', 'geometry', 'linear-transformations', 'intuition']"
31,The degree of the minimal polynomial decreases to the limit,The degree of the minimal polynomial decreases to the limit,,"$A \mapsto \mu_A$ (minimal polynomial of the matrix $A$) is not continuous. Take for example $A_n = \begin{pmatrix}     0 & 1/n\\     0 & 0\\    \end{pmatrix}$. But I think that: if $(A_n)_{n \in \mathbb N}$ satisfies $A_n \to A$ (whatever the norm is since the dimension is finite) and if we suppose all $\mu_{A_n}$ and $\mu_A$ exist, then: $\exists \ n_0, \ \forall \ n ≥ n_0,$ deg$(\mu_A) ≤ $deg$(\mu_{A_n})$ Do you have a hint to prove that?","$A \mapsto \mu_A$ (minimal polynomial of the matrix $A$) is not continuous. Take for example $A_n = \begin{pmatrix}     0 & 1/n\\     0 & 0\\    \end{pmatrix}$. But I think that: if $(A_n)_{n \in \mathbb N}$ satisfies $A_n \to A$ (whatever the norm is since the dimension is finite) and if we suppose all $\mu_{A_n}$ and $\mu_A$ exist, then: $\exists \ n_0, \ \forall \ n ≥ n_0,$ deg$(\mu_A) ≤ $deg$(\mu_{A_n})$ Do you have a hint to prove that?",,"['linear-algebra', 'matrices']"
32,Show matrix is nilpotent,Show matrix is nilpotent,,"I have matrices $A,B$ of dimension $n$ with real coefficients which satisfy the following: $A^2-B^2=c(AB-BA)$ where $c$ is a real number. If $c\neq0$ , prove that $(AB-BA)^n = 0$. So far, I've been able to show that $AB-BA$ is singular. Can someone help?","I have matrices $A,B$ of dimension $n$ with real coefficients which satisfy the following: $A^2-B^2=c(AB-BA)$ where $c$ is a real number. If $c\neq0$ , prove that $(AB-BA)^n = 0$. So far, I've been able to show that $AB-BA$ is singular. Can someone help?",,"['linear-algebra', 'matrices']"
33,$A$ self adjoint matrix. Prove there is a real $c$ such that $cI+A$ is positive,self adjoint matrix. Prove there is a real  such that  is positive,A c cI+A,"This was partially asked before (1. There exists a real number $c$ such that $A+cI$ is positive when $A$ is symmetric , 2. Proving $aI+A$ is Positive Definite ) but I'd like revision of this proof using forms. Let $A$ be a self-adjoint $n \times n$  matrix. Prove that there is a real number $c$ such that the matrix $cI+A$ is positive This is my attempt: Let be $f$ the form with matrix $A$ in the canonical basis. Since $A$ is self adjoint there exists a unitary matrix $Q$ such that $$Q^*AQ = D$$ where $D$ is a diagonal matrix. But this is just the matrix of the form $f$ in the basis composed of the column vectors of $Q$. Moreover, since $A$ is self adjoint it follows that every entry of the diagonal matrix D is real. So, consider the matrix $A+cI$ wich is also self adjoint and the consider the form $f'$ associated to this matrix. Again, there exists $P$ unitary such that $P^*(A+cI)P = D'$ is diagonal. But $$P^*(A+cI)P = P^*AP + cP^*P = P^*AP + cI = D'$$ Choose $c$ so the entries of $D'$ are positive (for example, $c = 2\max \left\{  |(P^*AP)_{ii}|: i \in \left\{1,\ldots,n\right\}\right\}$) and then obviously  $$ X^*D'X > 0 \quad\forall X \in V$$ so the form $f'$ is positive and then $A+cI$ is positive (using the fact that if a form $f$ is positive then its matrix in every ordered basis is a positive matrix) Thanks a lot for your feedback!","This was partially asked before (1. There exists a real number $c$ such that $A+cI$ is positive when $A$ is symmetric , 2. Proving $aI+A$ is Positive Definite ) but I'd like revision of this proof using forms. Let $A$ be a self-adjoint $n \times n$  matrix. Prove that there is a real number $c$ such that the matrix $cI+A$ is positive This is my attempt: Let be $f$ the form with matrix $A$ in the canonical basis. Since $A$ is self adjoint there exists a unitary matrix $Q$ such that $$Q^*AQ = D$$ where $D$ is a diagonal matrix. But this is just the matrix of the form $f$ in the basis composed of the column vectors of $Q$. Moreover, since $A$ is self adjoint it follows that every entry of the diagonal matrix D is real. So, consider the matrix $A+cI$ wich is also self adjoint and the consider the form $f'$ associated to this matrix. Again, there exists $P$ unitary such that $P^*(A+cI)P = D'$ is diagonal. But $$P^*(A+cI)P = P^*AP + cP^*P = P^*AP + cI = D'$$ Choose $c$ so the entries of $D'$ are positive (for example, $c = 2\max \left\{  |(P^*AP)_{ii}|: i \in \left\{1,\ldots,n\right\}\right\}$) and then obviously  $$ X^*D'X > 0 \quad\forall X \in V$$ so the form $f'$ is positive and then $A+cI$ is positive (using the fact that if a form $f$ is positive then its matrix in every ordered basis is a positive matrix) Thanks a lot for your feedback!",,"['linear-algebra', 'matrices', 'proof-verification']"
34,Relationship between total support and fully indecomposability.,Relationship between total support and fully indecomposability.,,"I have to prove that if a nonnegative matrix $A$ has total support then there exist $P,Q$ permutation matrix such that $$PAQ=\bigoplus_{i=1}^k A_i$$ where $A_i$ is fully indecomposable $\forall i=1,\dots, k$ and the $\bigoplus$ symbol means ""direct sum"" of matrices. I found many reference on this theorem but no one is complete. Could you help me? Some papers I found cite , but I cannot find a link between the two ones.","I have to prove that if a nonnegative matrix $A$ has total support then there exist $P,Q$ permutation matrix such that $$PAQ=\bigoplus_{i=1}^k A_i$$ where $A_i$ is fully indecomposable $\forall i=1,\dots, k$ and the $\bigoplus$ symbol means ""direct sum"" of matrices. I found many reference on this theorem but no one is complete. Could you help me? Some papers I found cite , but I cannot find a link between the two ones.",,"['linear-algebra', 'matrices', 'permutations']"
35,Two definitions of the tensor product and their relation,Two definitions of the tensor product and their relation,,"I have seen two different definitions of the tensor product and I am asking about their relation. Definition (for modules in most abstract algebra books). Given two vector spaces $U$ and $V$ over $F$, the tensor product $T = U \otimes V$ is a vector space $T$ over $F$ together with a bilinear map $\pi : U \times V \to T$ such that every bilinear map on $U$ and $V$ factors through $T$ (by a unique linear map). This could easily be extended to the tensor product of an arbitrary but finite number of vector spaces. Definition (from Spivak, Calculus on Mainfolds and Munkres, Analysis on Mainfolds). Given a vector space $V$ over $F$, the $k$-fold tensor is the set of all multilinear maps from $V^k$ to $F$ (hence the tensor product of two spaces is the set of bilinear maps). What is the relation of Definition 1 and Definition 2? The first definition gives an isomorphism $\operatorname{Bilin}(U, V, F) \cong L(U \otimes V, F)$ as the correspondence giving the unique linear map is linear. In general this gives an isomorphism between the $k$-fold multilinear maps and the linear maps from the $k$-fold tensor product to $F$ (note that we have not used the property to its full extend, as it also holds for bilinear maps not having their image in $F$, but lets restrict to that case). But still this does not identifies the tensor product itself with the multilinear maps, but just those maps with the dual of the tensor product (which might in general not equal the tensor product itself)? When we start from Definition 2, I am not quite sure how to show the universal property from Definition one. So lets consider the $2$-fold tensor product of $V$ over $F$, then we have to show that $\operatorname{Bilin}(U,V, F)$ fulfills this (universal) property. The only possible way that comes to my mind is to define $$  \pi(u,v) = u^{\ast} \cdot v^{\ast} $$ where $u^{\ast}$ and $v^{\ast}$ denote the correspoding dual elements of $V^{\ast}$ (after choosing a basis in $V$ and thereby having a dual basis in $V^{\ast}$ according to the usual construction, see here ). And if $\varphi : U \times V \to F$ is another bilinear map, we have to show that $$  \varphi(u,v) = h(u^*\cdot v^*) $$ for a unique linear map, if we define $h$ that way (using that $u \mapsto u^{\ast}$ in injective) we could show that it is linear (using $\{ e_i^{\ast}\}$ and some computations). But in general $\{ u^{\ast} \mid u \in V \}\ne V^{\ast}$, so it is not clear to me how to extend $h$ to all of $\mbox{Bilin}(U,V,F)$, hence this only works in the case of reflexivity (which include the finite-dimensional vector spaces). So what I have done just works for reflexive vector spaces, but am I on the right track? Maybe there are simpler arguments, and Munkres does not restrict his definition to finite-dimensional vector spaces only, so does it makes sense to define the tensor product like in definition two, but then how does it relate to definition one?","I have seen two different definitions of the tensor product and I am asking about their relation. Definition (for modules in most abstract algebra books). Given two vector spaces $U$ and $V$ over $F$, the tensor product $T = U \otimes V$ is a vector space $T$ over $F$ together with a bilinear map $\pi : U \times V \to T$ such that every bilinear map on $U$ and $V$ factors through $T$ (by a unique linear map). This could easily be extended to the tensor product of an arbitrary but finite number of vector spaces. Definition (from Spivak, Calculus on Mainfolds and Munkres, Analysis on Mainfolds). Given a vector space $V$ over $F$, the $k$-fold tensor is the set of all multilinear maps from $V^k$ to $F$ (hence the tensor product of two spaces is the set of bilinear maps). What is the relation of Definition 1 and Definition 2? The first definition gives an isomorphism $\operatorname{Bilin}(U, V, F) \cong L(U \otimes V, F)$ as the correspondence giving the unique linear map is linear. In general this gives an isomorphism between the $k$-fold multilinear maps and the linear maps from the $k$-fold tensor product to $F$ (note that we have not used the property to its full extend, as it also holds for bilinear maps not having their image in $F$, but lets restrict to that case). But still this does not identifies the tensor product itself with the multilinear maps, but just those maps with the dual of the tensor product (which might in general not equal the tensor product itself)? When we start from Definition 2, I am not quite sure how to show the universal property from Definition one. So lets consider the $2$-fold tensor product of $V$ over $F$, then we have to show that $\operatorname{Bilin}(U,V, F)$ fulfills this (universal) property. The only possible way that comes to my mind is to define $$  \pi(u,v) = u^{\ast} \cdot v^{\ast} $$ where $u^{\ast}$ and $v^{\ast}$ denote the correspoding dual elements of $V^{\ast}$ (after choosing a basis in $V$ and thereby having a dual basis in $V^{\ast}$ according to the usual construction, see here ). And if $\varphi : U \times V \to F$ is another bilinear map, we have to show that $$  \varphi(u,v) = h(u^*\cdot v^*) $$ for a unique linear map, if we define $h$ that way (using that $u \mapsto u^{\ast}$ in injective) we could show that it is linear (using $\{ e_i^{\ast}\}$ and some computations). But in general $\{ u^{\ast} \mid u \in V \}\ne V^{\ast}$, so it is not clear to me how to extend $h$ to all of $\mbox{Bilin}(U,V,F)$, hence this only works in the case of reflexivity (which include the finite-dimensional vector spaces). So what I have done just works for reflexive vector spaces, but am I on the right track? Maybe there are simpler arguments, and Munkres does not restrict his definition to finite-dimensional vector spaces only, so does it makes sense to define the tensor product like in definition two, but then how does it relate to definition one?",,"['linear-algebra', 'definition', 'tensor-products', 'tensors', 'multilinear-algebra']"
36,"Find $A,B\in\Bbb K^{2\times 2}$ such that $AB\neq BA$ but $e^{A+B}=e^Ae^B$",Find  such that  but,"A,B\in\Bbb K^{2\times 2} AB\neq BA e^{A+B}=e^Ae^B","Find $A,B\in\Bbb K^{2\times 2}$ such that $AB\neq BA$ but $e^{A+B}=e^Ae^B$. Hint: $e^{2k\pi i}=1$ for all $k\in\Bbb Z$. This is the exercise 10 in page 146 of Analysis II of Amann and Escher. I dont know exactly what to do here more than just try things blindly (trying to guess what is the hint about). I know that $$A:=\begin{bmatrix}a&b\\0&c\end{bmatrix}\implies e^A=\begin{cases}\begin{bmatrix}e^{a}&\frac{b}{c-a}(e^{c}-e^{a})\\0&e^{c}\end{bmatrix},& c\neq a\\\begin{bmatrix}e^{a}&be^{a}\\0&e^{a}\end{bmatrix},& c= a\end{cases}\tag1$$ $$A:=\begin{bmatrix}0&-\omega\\\omega&0\end{bmatrix}\implies e^{A}=\begin{bmatrix}\cos \omega&-\sin\omega \\\sin\omega &\cos\omega \end{bmatrix}\tag2$$ What I thought about the hint is setup some matrices as in $(1)$ such that $i(A+B)=i2\pi I$, by example $$A:=\begin{bmatrix}a&b\\0&c\end{bmatrix},\quad B:=\begin{bmatrix}2\pi-a&-b\\0&2\pi-c\end{bmatrix}\implies e^{i(A+B)}=I$$ However we have that $AB=BA$, then I must find other way. Some help will be appreciated, thank you.","Find $A,B\in\Bbb K^{2\times 2}$ such that $AB\neq BA$ but $e^{A+B}=e^Ae^B$. Hint: $e^{2k\pi i}=1$ for all $k\in\Bbb Z$. This is the exercise 10 in page 146 of Analysis II of Amann and Escher. I dont know exactly what to do here more than just try things blindly (trying to guess what is the hint about). I know that $$A:=\begin{bmatrix}a&b\\0&c\end{bmatrix}\implies e^A=\begin{cases}\begin{bmatrix}e^{a}&\frac{b}{c-a}(e^{c}-e^{a})\\0&e^{c}\end{bmatrix},& c\neq a\\\begin{bmatrix}e^{a}&be^{a}\\0&e^{a}\end{bmatrix},& c= a\end{cases}\tag1$$ $$A:=\begin{bmatrix}0&-\omega\\\omega&0\end{bmatrix}\implies e^{A}=\begin{bmatrix}\cos \omega&-\sin\omega \\\sin\omega &\cos\omega \end{bmatrix}\tag2$$ What I thought about the hint is setup some matrices as in $(1)$ such that $i(A+B)=i2\pi I$, by example $$A:=\begin{bmatrix}a&b\\0&c\end{bmatrix},\quad B:=\begin{bmatrix}2\pi-a&-b\\0&2\pi-c\end{bmatrix}\implies e^{i(A+B)}=I$$ However we have that $AB=BA$, then I must find other way. Some help will be appreciated, thank you.",,"['linear-algebra', 'matrices', 'analysis']"
37,when do orthogonal projection $\ P_UP_V = P_VP_U$ [duplicate],when do orthogonal projection  [duplicate],\ P_UP_V = P_VP_U,"This question already has an answer here : Orthogonal projection and two subspaces (1 answer) Closed 6 years ago . U, V are subspaces of a finite dimensional vector space W, Let $\ P_U$ and $\ P_V$ be the orthogonal projections onto U and V respectively. When is it true that $\ P_UP_V = P_VP_U$?","This question already has an answer here : Orthogonal projection and two subspaces (1 answer) Closed 6 years ago . U, V are subspaces of a finite dimensional vector space W, Let $\ P_U$ and $\ P_V$ be the orthogonal projections onto U and V respectively. When is it true that $\ P_UP_V = P_VP_U$?",,"['linear-algebra', 'projection']"
38,Derivative of inner product via adjoint operator vs. complex derivatives,Derivative of inner product via adjoint operator vs. complex derivatives,,"Dear math enthusiasts, I need to take the derivative of an inner product involving an operator on one side and I'd like to do this via the adjoint operator. However, it seems I'm doing something wrong since things don't quite match for the complex-valued case (for real numbers it works). I'm assuming some mismatch between the (Wirtinger?) calculus I'm applying for the complex derivative and the inner products / operators but I'm not sure where exactly things go wrong. Here is the setting: I have an operator $T: \mathbb{C}^n \mapsto {\mathcal S_n}$, where $\mathcal S_n$ denotes the set of Hermitian symmetric complex $n \times n$ matrices. I am looking at the inner product $\langle A, T(x) \rangle$ and I need its derivative with respect to $x$. The inner products I'm using are $\langle x,y\rangle = x^{\rm H} y$ on $\mathbb{C}^n$ and $\langle X,Y\rangle = {\rm Trace}(X^{\rm H} Y)$ on $\mathcal S_n$, where $(\cdot)^{\rm H}$ denotes conjugate transpose. I was thinking to use the adjoint operator $T^*: {\mathcal S_n} \mapsto \mathbb{C}^n$ defined via $\langle A, T(x) \rangle = \langle T^*(A), x \rangle$ for all $A \in \mathcal S_n$, $x \in \mathbb{C}^n$. Then I was hoping to use some magic like this: $$\frac{{\rm d} \langle A, T(x) \rangle}{{\rm d} x} = \frac{{\rm d} \langle T^*(A), x \rangle}{{\rm d} x} = T^*(A). $$ Is this rule correct? Does it work in the complex domain? I was thinking it should work on any Hilbert space but I'm not sure. Anyways, I'm having trouble applying this. Here is a concrete simple example. The most simple form of it is the operator $T(x) = \begin{bmatrix} x_1 & x_2 \\ \bar{x}_2 & x_1 \end{bmatrix}$ [*] , where $\bar{x}$ denotes complex conjugation. Since $A \in \mathcal S_n$, let's say $A = \begin{bmatrix} a_1 & a_2 \\ \bar{a}_2 & a_1 \end{bmatrix}$, where $a_1$ is real. I'm computing the adjoint via $$[T^*(A)]_i = \langle T^*(A),e_i\rangle = \langle A,T(e_i) \rangle =  \begin{bmatrix}  \langle A, \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \rangle \\  \langle A, \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} \rangle  \end{bmatrix} = \begin{bmatrix} 2 a_1 \\ a_2 + \bar{a}_2\end{bmatrix}.$$ So far so good. At the same time, my inner product gives me $$ \langle A,T(x) \rangle = a_1 x_1 + \bar{a}_2 x_2 + a_2 \bar{x}_2 + a_1 x_1.$$ I would have said its derivative should be $$\frac{{\rm d} \langle A, T(x) \rangle}{{\rm d} x} = \begin{bmatrix} 2a_1 \\ \bar{a}_2  \end{bmatrix},$$ using $\frac{\partial \bar{x}_2}{x_2} = 0$. I'm assuming this is where I'm wrong. On the other hand, I would only get a result agreeing with $T^*(A)$ if this derivative would be equal to one, which I find hard to believe (unless $x$ is real). I'm not an expert in complex analysis but I was told that for functions $f: \mathbb{C}^n \mapsto \mathbb{R}$ we have $\frac{{\rm d}f}{{\rm d}x} = \overline{\frac{{\rm d}f}{{\rm d}\bar{x}}}$, since $f = \bar{f}$. This means that if we look for extrema it does not matter which of the derivatives we equate to zero. It works nicely in the above example if we treat $x_2$ and $\bar{x}_2$ independently (I belive this is called Wirtinger calculus or something?). At the same time this seems to break something with the inner products and/or adjoint operators. Could someone help me shed light on where is the incompatibility and what is the best way of treating these types of problems? What is the correct answer to the derivative? What makes me really sceptical about using $T^*(A)$ is that it always returns something real-valued (for symmetric $A$ on which it is defined) though the derivatives should be complex-valued. P.S.: [*] There is another minor issue here that I think has not directly to do with the problem but I welcome comments on: For this operator to map to $\mathcal S_n$ we actually need to make sure $x_1$ is real. So I have two options: (a) define it as $T(x) = \begin{bmatrix} {\rm Real}(x_1) & x_2 \\ \bar{x}_2 & {\rm Real}(x_1) \end{bmatrix}$ where ${\rm Real}(x_1) = \frac 12(x_1 + \bar{x}_1)$. Interestingly, this does not seem to change the adjoint operator, since $T(e_i)$ is unchanged. However, my inner product becomes $\langle 2 a_1 \frac 12 (x_1+\bar{x}_1) + a_2x_2 + \bar{a}_2 \bar{x}_2 \rangle$, which does seem to change the derivative again to $\begin{bmatrix} a_1 \\ a_2 \end{bmatrix}$. So even with this correction, the result is incompatible to the one we get via $T^*(A)$. (b) Alternatively, I could define the operator to map from $\mathbb{R} \times \mathbb{C}^{n-1}$ to $\mathcal S_n$. I'm not sure if this causes complications for the inner products though.","Dear math enthusiasts, I need to take the derivative of an inner product involving an operator on one side and I'd like to do this via the adjoint operator. However, it seems I'm doing something wrong since things don't quite match for the complex-valued case (for real numbers it works). I'm assuming some mismatch between the (Wirtinger?) calculus I'm applying for the complex derivative and the inner products / operators but I'm not sure where exactly things go wrong. Here is the setting: I have an operator $T: \mathbb{C}^n \mapsto {\mathcal S_n}$, where $\mathcal S_n$ denotes the set of Hermitian symmetric complex $n \times n$ matrices. I am looking at the inner product $\langle A, T(x) \rangle$ and I need its derivative with respect to $x$. The inner products I'm using are $\langle x,y\rangle = x^{\rm H} y$ on $\mathbb{C}^n$ and $\langle X,Y\rangle = {\rm Trace}(X^{\rm H} Y)$ on $\mathcal S_n$, where $(\cdot)^{\rm H}$ denotes conjugate transpose. I was thinking to use the adjoint operator $T^*: {\mathcal S_n} \mapsto \mathbb{C}^n$ defined via $\langle A, T(x) \rangle = \langle T^*(A), x \rangle$ for all $A \in \mathcal S_n$, $x \in \mathbb{C}^n$. Then I was hoping to use some magic like this: $$\frac{{\rm d} \langle A, T(x) \rangle}{{\rm d} x} = \frac{{\rm d} \langle T^*(A), x \rangle}{{\rm d} x} = T^*(A). $$ Is this rule correct? Does it work in the complex domain? I was thinking it should work on any Hilbert space but I'm not sure. Anyways, I'm having trouble applying this. Here is a concrete simple example. The most simple form of it is the operator $T(x) = \begin{bmatrix} x_1 & x_2 \\ \bar{x}_2 & x_1 \end{bmatrix}$ [*] , where $\bar{x}$ denotes complex conjugation. Since $A \in \mathcal S_n$, let's say $A = \begin{bmatrix} a_1 & a_2 \\ \bar{a}_2 & a_1 \end{bmatrix}$, where $a_1$ is real. I'm computing the adjoint via $$[T^*(A)]_i = \langle T^*(A),e_i\rangle = \langle A,T(e_i) \rangle =  \begin{bmatrix}  \langle A, \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \rangle \\  \langle A, \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} \rangle  \end{bmatrix} = \begin{bmatrix} 2 a_1 \\ a_2 + \bar{a}_2\end{bmatrix}.$$ So far so good. At the same time, my inner product gives me $$ \langle A,T(x) \rangle = a_1 x_1 + \bar{a}_2 x_2 + a_2 \bar{x}_2 + a_1 x_1.$$ I would have said its derivative should be $$\frac{{\rm d} \langle A, T(x) \rangle}{{\rm d} x} = \begin{bmatrix} 2a_1 \\ \bar{a}_2  \end{bmatrix},$$ using $\frac{\partial \bar{x}_2}{x_2} = 0$. I'm assuming this is where I'm wrong. On the other hand, I would only get a result agreeing with $T^*(A)$ if this derivative would be equal to one, which I find hard to believe (unless $x$ is real). I'm not an expert in complex analysis but I was told that for functions $f: \mathbb{C}^n \mapsto \mathbb{R}$ we have $\frac{{\rm d}f}{{\rm d}x} = \overline{\frac{{\rm d}f}{{\rm d}\bar{x}}}$, since $f = \bar{f}$. This means that if we look for extrema it does not matter which of the derivatives we equate to zero. It works nicely in the above example if we treat $x_2$ and $\bar{x}_2$ independently (I belive this is called Wirtinger calculus or something?). At the same time this seems to break something with the inner products and/or adjoint operators. Could someone help me shed light on where is the incompatibility and what is the best way of treating these types of problems? What is the correct answer to the derivative? What makes me really sceptical about using $T^*(A)$ is that it always returns something real-valued (for symmetric $A$ on which it is defined) though the derivatives should be complex-valued. P.S.: [*] There is another minor issue here that I think has not directly to do with the problem but I welcome comments on: For this operator to map to $\mathcal S_n$ we actually need to make sure $x_1$ is real. So I have two options: (a) define it as $T(x) = \begin{bmatrix} {\rm Real}(x_1) & x_2 \\ \bar{x}_2 & {\rm Real}(x_1) \end{bmatrix}$ where ${\rm Real}(x_1) = \frac 12(x_1 + \bar{x}_1)$. Interestingly, this does not seem to change the adjoint operator, since $T(e_i)$ is unchanged. However, my inner product becomes $\langle 2 a_1 \frac 12 (x_1+\bar{x}_1) + a_2x_2 + \bar{a}_2 \bar{x}_2 \rangle$, which does seem to change the derivative again to $\begin{bmatrix} a_1 \\ a_2 \end{bmatrix}$. So even with this correction, the result is incompatible to the one we get via $T^*(A)$. (b) Alternatively, I could define the operator to map from $\mathbb{R} \times \mathbb{C}^{n-1}$ to $\mathcal S_n$. I'm not sure if this causes complications for the inner products though.",,"['linear-algebra', 'complex-analysis', 'functional-analysis', 'adjoint-operators']"
39,Number of units in the ring of matrices,Number of units in the ring of matrices,,"I want to know about the number of units in the ring of all matrices of order $n\times n$ over the field of order $p$, where $p$ is a prime. I read in one my book that the order of $\mathit{GL}(n,\mathbb Z_p)$ is $(p^n-1)(p^n-p)(p^n-p^2)\dots (p^n-p^{n-1})$. I think this is the required number of units in the group $M_n(\mathbb Z_p)$. But I can't deduce it. Also I have a question what will be the result if p is not prime i.e composite.? Hope to get help. Thanks. Edit: My question for prime modulus $p$ is already been discussed here. So I would like to know about the case when the modulus is composite.","I want to know about the number of units in the ring of all matrices of order $n\times n$ over the field of order $p$, where $p$ is a prime. I read in one my book that the order of $\mathit{GL}(n,\mathbb Z_p)$ is $(p^n-1)(p^n-p)(p^n-p^2)\dots (p^n-p^{n-1})$. I think this is the required number of units in the group $M_n(\mathbb Z_p)$. But I can't deduce it. Also I have a question what will be the result if p is not prime i.e composite.? Hope to get help. Thanks. Edit: My question for prime modulus $p$ is already been discussed here. So I would like to know about the case when the modulus is composite.",,"['linear-algebra', 'matrices', 'ring-theory']"
40,Invertible matrix equals to unitary matrix times positive definite,Invertible matrix equals to unitary matrix times positive definite,,"Let $V$ be a finite-dimensional inner product space, and $T:V\to V$ be a invertible linear operator. Prove that there exists a unitary operator $U$ and a positive operator $P$ on $V$ such that $T=U\circ P$. How may I prove this theorem? Or its equivalent theorem in matrix form?","Let $V$ be a finite-dimensional inner product space, and $T:V\to V$ be a invertible linear operator. Prove that there exists a unitary operator $U$ and a positive operator $P$ on $V$ such that $T=U\circ P$. How may I prove this theorem? Or its equivalent theorem in matrix form?",,"['linear-algebra', 'linear-transformations']"
41,Solving many quadratic programs with the same objective matrix,Solving many quadratic programs with the same objective matrix,,"If one wants to solve many linear systems with the same matrix, $$\mathbf A\mathbf x_1 = \mathbf b_1, \quad \mathbf A\mathbf x_2 = \mathbf b_2, \quad \ldots, \quad \mathbf A\mathbf x_k = \mathbf b_k,$$ it is preferable to first precompute a factorization of the matrix $\mathbf A$, so that each system with different right-hand sides can be solved efficiently. In my case, $\mathbf A$ is symmetric and positive definite, so the Cholesky factorization works well. Of course, solving $\mathbf A\mathbf x_i = \mathbf b_i$ is equivalent to solving the unconstrained quadratic minimization $\min \tfrac12\mathbf x_i^T\mathbf A\mathbf x_i - \mathbf b_i^T\mathbf x_i$. I am interested in solving many such problems, but with additional inequality constraints, $$\begin{gather} \min & \tfrac12\mathbf x_i^T\mathbf A\mathbf x - \mathbf b_i^T\mathbf x_i \\ \text{s.t. } & \mathbf C_i\mathbf x \succeq \mathbf d_i. \end{gather}$$ The constraints and the linear term of the objective both vary between problems, but the quadratic term remains constant. Is there an analogous method for speeding up such problems by exploiting a factorization of $\mathbf A$? For equality constraints ($\mathbf C_i\mathbf x = \mathbf d_i$) we have tried the Uzawa method , but it does not support inequalities. More details: The matrix $\mathbf A$ is sparse, symmetric, and positive definite. Its sparse Cholesky factorization is available, but I can also precompute other factorizations if necessary. If a general solution is not possible, I am also interested in the special case where each constraint acts on only one degree of freedom, i.e. each row of $\mathbf C_i$ has only one nonzero entry.","If one wants to solve many linear systems with the same matrix, $$\mathbf A\mathbf x_1 = \mathbf b_1, \quad \mathbf A\mathbf x_2 = \mathbf b_2, \quad \ldots, \quad \mathbf A\mathbf x_k = \mathbf b_k,$$ it is preferable to first precompute a factorization of the matrix $\mathbf A$, so that each system with different right-hand sides can be solved efficiently. In my case, $\mathbf A$ is symmetric and positive definite, so the Cholesky factorization works well. Of course, solving $\mathbf A\mathbf x_i = \mathbf b_i$ is equivalent to solving the unconstrained quadratic minimization $\min \tfrac12\mathbf x_i^T\mathbf A\mathbf x_i - \mathbf b_i^T\mathbf x_i$. I am interested in solving many such problems, but with additional inequality constraints, $$\begin{gather} \min & \tfrac12\mathbf x_i^T\mathbf A\mathbf x - \mathbf b_i^T\mathbf x_i \\ \text{s.t. } & \mathbf C_i\mathbf x \succeq \mathbf d_i. \end{gather}$$ The constraints and the linear term of the objective both vary between problems, but the quadratic term remains constant. Is there an analogous method for speeding up such problems by exploiting a factorization of $\mathbf A$? For equality constraints ($\mathbf C_i\mathbf x = \mathbf d_i$) we have tried the Uzawa method , but it does not support inequalities. More details: The matrix $\mathbf A$ is sparse, symmetric, and positive definite. Its sparse Cholesky factorization is available, but I can also precompute other factorizations if necessary. If a general solution is not possible, I am also interested in the special case where each constraint acts on only one degree of freedom, i.e. each row of $\mathbf C_i$ has only one nonzero entry.",,"['linear-algebra', 'optimization']"
42,Representation theory and simultaneous block diagonalization of matrices with common symmetries,Representation theory and simultaneous block diagonalization of matrices with common symmetries,,"Group representation theory has applications in a lot of areas and problems in mathematics, one of such an example is the finest simultaneous block diagonalization of the set of matrices with common symmetries. According to Page 7 of this paper , let $\{T(g)\}$ be an orthogonal matrix representation of a group $G$, if a set of matrices $\{A_p\}$ all share the symmetry described by $G$, that is, $T(g)^\intercal A_p T(g) = A_p, \ \forall g\in G \ \text{and} \ \forall p,$ then a simultaneous block diagonalization of $A_p$ can be obtained through the decomposition of the representation $\{T(g)\}$ into irreducible representations. In particular, if $\{P^\intercal T(g) P\}$ is the direct sum of irreducible representations with dimensions $n_i$ and multiplicities $m_i$, then each element in $\{P^\intercal A_p P\}$ has a common block structure as the direct sum of blocks of dimensions $m_i$ and multiplicities $n_i$. Can anyone point me to references/theorems in group representation theory from which this claim is based? Thanks!","Group representation theory has applications in a lot of areas and problems in mathematics, one of such an example is the finest simultaneous block diagonalization of the set of matrices with common symmetries. According to Page 7 of this paper , let $\{T(g)\}$ be an orthogonal matrix representation of a group $G$, if a set of matrices $\{A_p\}$ all share the symmetry described by $G$, that is, $T(g)^\intercal A_p T(g) = A_p, \ \forall g\in G \ \text{and} \ \forall p,$ then a simultaneous block diagonalization of $A_p$ can be obtained through the decomposition of the representation $\{T(g)\}$ into irreducible representations. In particular, if $\{P^\intercal T(g) P\}$ is the direct sum of irreducible representations with dimensions $n_i$ and multiplicities $m_i$, then each element in $\{P^\intercal A_p P\}$ has a common block structure as the direct sum of blocks of dimensions $m_i$ and multiplicities $n_i$. Can anyone point me to references/theorems in group representation theory from which this claim is based? Thanks!",,"['linear-algebra', 'abstract-algebra', 'matrices', 'representation-theory', 'diagonalization']"
43,How to prove the existence of non-intersecting subspaces?,How to prove the existence of non-intersecting subspaces?,,"Let $V=\Bbb R^n$, $P,P'$ two $k$-dim subspaces, there must exist a $(n-k)$-dim subspace $Q$ whose intersection with both $P$ and $P'$ is zero. Although I have no idea how to prove this result, I feel it might be somehow connected to this problem . Thanks for any help. EDIT I tried arguing analogously to Steve's answer to the linked problem, but I was just not sure how to make a proper analogy, or these two problems are not actually related at all. By the way, geometric intuition tells me the result is still true for any finite collection of $k$-dim subspaces.","Let $V=\Bbb R^n$, $P,P'$ two $k$-dim subspaces, there must exist a $(n-k)$-dim subspace $Q$ whose intersection with both $P$ and $P'$ is zero. Although I have no idea how to prove this result, I feel it might be somehow connected to this problem . Thanks for any help. EDIT I tried arguing analogously to Steve's answer to the linked problem, but I was just not sure how to make a proper analogy, or these two problems are not actually related at all. By the way, geometric intuition tells me the result is still true for any finite collection of $k$-dim subspaces.",,"['linear-algebra', 'vector-spaces']"
44,Determining whether or not a vector is in a cone,Determining whether or not a vector is in a cone,,"Determine whether or not the vector $\langle 0,7,3 \rangle$ belongs to the cone generated by $$\langle 1,1,1\rangle \qquad \langle -1,2,1\rangle \qquad \langle 0,-1,1\rangle \qquad \langle 0,1,0\rangle$$ That is, I am asked to determine whether or not $\langle 0,7,3 \rangle$ is a linear combination of the other four listed vectors. I have a solution (which I now realize is not correct) $$ \langle 0,7,3 \rangle=(2)\langle 1,1,1\rangle+(2)\langle -1,2,1\rangle+(-1)\langle 0,-1,1\rangle+(0)\langle 0,1,0\rangle. $$ My question is more so how would I set up a linear system of equations for the question at hand? I could throw in several more vectors and multiply them all by $0$ as well to have $\langle 0,7,3 \rangle$ in a variety of cones, but that is rather trivial (just multiplying other vectors by $0$). How would I set up the original question here as an augmented matrix so I could row reduce it effectively? Question: Can anyone find nonegative weights for $\langle 1,1,1\rangle$, $\langle -1,2,1\rangle$, and $\langle 0,-1,1\rangle$ that will give $\langle 0,7,3 \rangle$?","Determine whether or not the vector $\langle 0,7,3 \rangle$ belongs to the cone generated by $$\langle 1,1,1\rangle \qquad \langle -1,2,1\rangle \qquad \langle 0,-1,1\rangle \qquad \langle 0,1,0\rangle$$ That is, I am asked to determine whether or not $\langle 0,7,3 \rangle$ is a linear combination of the other four listed vectors. I have a solution (which I now realize is not correct) $$ \langle 0,7,3 \rangle=(2)\langle 1,1,1\rangle+(2)\langle -1,2,1\rangle+(-1)\langle 0,-1,1\rangle+(0)\langle 0,1,0\rangle. $$ My question is more so how would I set up a linear system of equations for the question at hand? I could throw in several more vectors and multiply them all by $0$ as well to have $\langle 0,7,3 \rangle$ in a variety of cones, but that is rather trivial (just multiplying other vectors by $0$). How would I set up the original question here as an augmented matrix so I could row reduce it effectively? Question: Can anyone find nonegative weights for $\langle 1,1,1\rangle$, $\langle -1,2,1\rangle$, and $\langle 0,-1,1\rangle$ that will give $\langle 0,7,3 \rangle$?",,"['linear-algebra', 'vectors', 'linear-programming', 'convex-cone']"
45,Least Squares Fit to Find Transform Between Points,Least Squares Fit to Find Transform Between Points,,"We are using the OpenCV library estimateRigidTransform function to find a mapping between two 2D point sets.  The mapping supports rotation, uniform scaling, and translation. $$T=\left[     \begin{array}{cc|c}       s\cos(\theta)&-s\sin(\theta)&T_x\\       s\sin(\theta)&s\cos(\theta)&T_y     \end{array} \right] $$ The documentation claims the function solves the following problem. $$[A^*|b^*] = arg \min _{[A|b]} \sum _i \| \texttt{dst}[i] - A { \texttt{src}[i]}^T - b \| ^2$$ Here the left hand side is our transform $T$ and $dst$ and $src$ are the two point sets.  It appears to find the transform that minimizes the error (distance) between the $dst$ point set and the $src$ point set after transformation. The over defined problem of $N$ equations is converted to a system of 4 equations and 4 unknowns ($s\cos(\theta)$, $s\sin(\theta)$, $T_x$, $T_y$) as shown below.  $(a_x,a_y)$ is a point in the first point set $A$, and $(b_x,b_y)$ is a point in the second point set $B$.  Both point sets contain $N$ points.  The points have already been matched between the two sets, so the first point in set $A$ should map to the first point in set $B$ and so on. All the summations occur over the set of $N$ point pairs. $$\left[     \begin{array}{cccc}       \sum_{} (a_x^2+a_y^2)&0&\sum_{} a_x&\sum_{} a_y\\       0&\sum_{} (a_x^2+a_y^2)&-\sum_{} a_y&\sum_{} a_x\\       \sum_{} a_x&-\sum_{} a_y&N&0\\       \sum_{} a_y&\sum_{} a_x&0&N\\     \end{array} \right]\left[     \begin{array}{c}       s\cos(\theta)\\       s\sin(\theta)\\       T_x\\       T_y\\     \end{array} \right]=\left[     \begin{array}{c}       \sum_{} (a_xb_x+a_yb_y)\\       \sum_{} (a_xb_y-a_yb_x)\\       \sum_{} b_x\\       \sum_{} b_y\\     \end{array} \right]  $$ I got these equations from reverse engineering the source code.  The documentation then says that eigenvalue decomposition is used to solve the system.  I am trying to determine where this set of equations has come from.  The first and second rows are a mystery to me.  I recognize the third and fourth rows as summing both sides of the mapping equation for $x$ and $y$. $$s\cos(\theta)\sum_{} a_x-s\sin(\theta)\sum_{} a_y+NT_x=\sum_{} b_x$$ $$\sum_{} (a_xs\cos(\theta)-a_ys\sin(\theta)+T_x)=\sum_{} b_x$$ and $$s\cos(\theta)\sum_{} a_y+s\sin(\theta)\sum_{} a_x+NT_y=\sum_{} b_y$$ $$\sum_{} (a_xs\sin(\theta)+a_ys\cos(\theta)+T_y)=\sum_{} b_y$$ Does anyone know where the first and second rows come from?  I assume OpenCV is implementing some form of least squares fitting to find the transform that best maps the points. EDIT: After playing around further, I now recognize the first row as: $$a_x*row3+a_y*row4$$ And the second row as: $$-a_y*row3+a_x*row4$$ The first two rows are created by combining the last 2 in different ways. Doesn't this create of set of equations that are not independent any more? Or does the summations in the equations and the residual error somehow maintain their independence?","We are using the OpenCV library estimateRigidTransform function to find a mapping between two 2D point sets.  The mapping supports rotation, uniform scaling, and translation. $$T=\left[     \begin{array}{cc|c}       s\cos(\theta)&-s\sin(\theta)&T_x\\       s\sin(\theta)&s\cos(\theta)&T_y     \end{array} \right] $$ The documentation claims the function solves the following problem. $$[A^*|b^*] = arg \min _{[A|b]} \sum _i \| \texttt{dst}[i] - A { \texttt{src}[i]}^T - b \| ^2$$ Here the left hand side is our transform $T$ and $dst$ and $src$ are the two point sets.  It appears to find the transform that minimizes the error (distance) between the $dst$ point set and the $src$ point set after transformation. The over defined problem of $N$ equations is converted to a system of 4 equations and 4 unknowns ($s\cos(\theta)$, $s\sin(\theta)$, $T_x$, $T_y$) as shown below.  $(a_x,a_y)$ is a point in the first point set $A$, and $(b_x,b_y)$ is a point in the second point set $B$.  Both point sets contain $N$ points.  The points have already been matched between the two sets, so the first point in set $A$ should map to the first point in set $B$ and so on. All the summations occur over the set of $N$ point pairs. $$\left[     \begin{array}{cccc}       \sum_{} (a_x^2+a_y^2)&0&\sum_{} a_x&\sum_{} a_y\\       0&\sum_{} (a_x^2+a_y^2)&-\sum_{} a_y&\sum_{} a_x\\       \sum_{} a_x&-\sum_{} a_y&N&0\\       \sum_{} a_y&\sum_{} a_x&0&N\\     \end{array} \right]\left[     \begin{array}{c}       s\cos(\theta)\\       s\sin(\theta)\\       T_x\\       T_y\\     \end{array} \right]=\left[     \begin{array}{c}       \sum_{} (a_xb_x+a_yb_y)\\       \sum_{} (a_xb_y-a_yb_x)\\       \sum_{} b_x\\       \sum_{} b_y\\     \end{array} \right]  $$ I got these equations from reverse engineering the source code.  The documentation then says that eigenvalue decomposition is used to solve the system.  I am trying to determine where this set of equations has come from.  The first and second rows are a mystery to me.  I recognize the third and fourth rows as summing both sides of the mapping equation for $x$ and $y$. $$s\cos(\theta)\sum_{} a_x-s\sin(\theta)\sum_{} a_y+NT_x=\sum_{} b_x$$ $$\sum_{} (a_xs\cos(\theta)-a_ys\sin(\theta)+T_x)=\sum_{} b_x$$ and $$s\cos(\theta)\sum_{} a_y+s\sin(\theta)\sum_{} a_x+NT_y=\sum_{} b_y$$ $$\sum_{} (a_xs\sin(\theta)+a_ys\cos(\theta)+T_y)=\sum_{} b_y$$ Does anyone know where the first and second rows come from?  I assume OpenCV is implementing some form of least squares fitting to find the transform that best maps the points. EDIT: After playing around further, I now recognize the first row as: $$a_x*row3+a_y*row4$$ And the second row as: $$-a_y*row3+a_x*row4$$ The first two rows are created by combining the last 2 in different ways. Doesn't this create of set of equations that are not independent any more? Or does the summations in the equations and the residual error somehow maintain their independence?",,"['linear-algebra', 'geometry', 'systems-of-equations', 'linear-transformations']"
46,Tensor Product of dual linear maps,Tensor Product of dual linear maps,,"Suppose $V$ and $W$ are finite dimensional linear spaces and $V^*$ as well as $W^*$ are their appropriate linear duals. Now let $f: V \to W$ and $g: V \to W$ be linear maps. Is the following identity correct? $f^* \otimes g^* = (f \otimes g)^*$ That is the tensor product of the dual linear maps, is the linear dual of the tensor product of the maps. Can't find this neither on the Wikipedia page of the tensor product, nor on the Wikipedia page of the dual linear maps. Therefore its properly wrong? Don't think so.","Suppose $V$ and $W$ are finite dimensional linear spaces and $V^*$ as well as $W^*$ are their appropriate linear duals. Now let $f: V \to W$ and $g: V \to W$ be linear maps. Is the following identity correct? $f^* \otimes g^* = (f \otimes g)^*$ That is the tensor product of the dual linear maps, is the linear dual of the tensor product of the maps. Can't find this neither on the Wikipedia page of the tensor product, nor on the Wikipedia page of the dual linear maps. Therefore its properly wrong? Don't think so.",,"['linear-algebra', 'tensor-products']"
47,An $\mathbb R$-linear map which is also $\mathbb C$-linear,An -linear map which is also -linear,\mathbb R \mathbb C,"Suppose that $T : \mathbb C^2 \to \mathbb C^2$ is an $\mathbb R$ linear map. $T(1,0) = (1,0)$, $T(0,1)=(0,1)$ and $T$ maps the $\mathbb C$-subspaces of $\mathbb C^2$ to $\mathbb C$-subspaces of $\mathbb C^2$. What is $T$? My attempt: First I can show that $T$ is a surjective $R$ linear map, thus a bijective $R$ linear map. And I know that $T$ maps $(i,0)$ to $(k,0)$ where $k$ is a nonzero complex number, but I guess $k$ should either be $(i,0)$ or $(-i,0)$. I do not know how to prove it.","Suppose that $T : \mathbb C^2 \to \mathbb C^2$ is an $\mathbb R$ linear map. $T(1,0) = (1,0)$, $T(0,1)=(0,1)$ and $T$ maps the $\mathbb C$-subspaces of $\mathbb C^2$ to $\mathbb C$-subspaces of $\mathbb C^2$. What is $T$? My attempt: First I can show that $T$ is a surjective $R$ linear map, thus a bijective $R$ linear map. And I know that $T$ maps $(i,0)$ to $(k,0)$ where $k$ is a nonzero complex number, but I guess $k$ should either be $(i,0)$ or $(-i,0)$. I do not know how to prove it.",,"['linear-algebra', 'linear-transformations']"
48,Polar decomposition of composition of two $2 \times 2$ matrices,Polar decomposition of composition of two  matrices,2 \times 2,"In one of Ruelle`s papers ""Rotation Numbers for Flows and Diffeomorphisms"" Ruelle has the following calculation which I do not understand completely. Assume you have two invertible $2 \times 2$ matrices $A$ and $B$ with polar decompositions $A = U(\theta(A))|A|$ and $B = U(\theta(B))|B|$ where $U(\theta)$ is the planar rotation matrix by $\theta$ and $|B|=\sqrt(BB^T)$ etc. Then he says that $$ |\theta(AB)-\theta(A)-\theta(B)| \leq \pi $$ I don`t quite understand how he gets this result without a constant depending on norms of A and B. One can start by saying $$ AB = U(\theta(AB))|AB| = U(\theta(A))|A| U(\theta(B))|B| $$ $$ = U(\theta(A)+\theta(B))U(-\theta(B))|A|U(\theta(B))|B| $$ so that $$ U(\theta(AB)-\theta(A)-\theta(B)) = U(-\theta(B))|A|U(\theta(B))|B||AB|^{-1}. $$ Somewhere in the paper he gives as a hint $|\theta(PQ)| \leq \pi$ if $P$ and $Q$ are positive but I cant see how to use it.","In one of Ruelle`s papers ""Rotation Numbers for Flows and Diffeomorphisms"" Ruelle has the following calculation which I do not understand completely. Assume you have two invertible $2 \times 2$ matrices $A$ and $B$ with polar decompositions $A = U(\theta(A))|A|$ and $B = U(\theta(B))|B|$ where $U(\theta)$ is the planar rotation matrix by $\theta$ and $|B|=\sqrt(BB^T)$ etc. Then he says that $$ |\theta(AB)-\theta(A)-\theta(B)| \leq \pi $$ I don`t quite understand how he gets this result without a constant depending on norms of A and B. One can start by saying $$ AB = U(\theta(AB))|AB| = U(\theta(A))|A| U(\theta(B))|B| $$ $$ = U(\theta(A)+\theta(B))U(-\theta(B))|A|U(\theta(B))|B| $$ so that $$ U(\theta(AB)-\theta(A)-\theta(B)) = U(-\theta(B))|A|U(\theta(B))|B||AB|^{-1}. $$ Somewhere in the paper he gives as a hint $|\theta(PQ)| \leq \pi$ if $P$ and $Q$ are positive but I cant see how to use it.",,"['linear-algebra', 'dynamical-systems']"
49,"Matrices commuting with diagonal matrix, 3 distinct diagonal entries.","Matrices commuting with diagonal matrix, 3 distinct diagonal entries.",,"Hopefully I have the right idea? Let $A\in\mathbb{C}^{4\times 4}$ be a diagonal matrix with exactly 3 distinct entries on its main diagonal. What is the dimension of the vector space over $\mathbb{C}$ of matrices $B\in\mathbb{C}^{4\times4}$ such that $AB=BA$? If $B\in\mathbb{C}^{4\times4}$ is a diagonal matrix with exactly 3 distinct entries on its main diagonal, is $B$ similar to a polynomial in $A$? Another way to formulate this is to let $T_A\colon\mathbb{C}^4\to\mathbb{C}^4$ via $T_A(B) = AB-BA,$ and we wish to find the dimension of the null space $N$ of $T_A.$ For example, let's say $$A = \left[\begin{array}{cc|cc} a&&&\\ &a&&\\ \hline &&b&\\ &&&c \end{array}\right].$$ Then we need $B$ to commute with each $2\times 2$ block. In the upper left block, we have a scalar multiple of the identity, so everything commutes and this has dimension $4$. For the lower right block, we need $B$ to be diagonal there (dimension $2$). So then $\dim N = 6.$ As for the second question, I feel like the answer is no, but I am struggling to come up with a counterexample.","Hopefully I have the right idea? Let $A\in\mathbb{C}^{4\times 4}$ be a diagonal matrix with exactly 3 distinct entries on its main diagonal. What is the dimension of the vector space over $\mathbb{C}$ of matrices $B\in\mathbb{C}^{4\times4}$ such that $AB=BA$? If $B\in\mathbb{C}^{4\times4}$ is a diagonal matrix with exactly 3 distinct entries on its main diagonal, is $B$ similar to a polynomial in $A$? Another way to formulate this is to let $T_A\colon\mathbb{C}^4\to\mathbb{C}^4$ via $T_A(B) = AB-BA,$ and we wish to find the dimension of the null space $N$ of $T_A.$ For example, let's say $$A = \left[\begin{array}{cc|cc} a&&&\\ &a&&\\ \hline &&b&\\ &&&c \end{array}\right].$$ Then we need $B$ to commute with each $2\times 2$ block. In the upper left block, we have a scalar multiple of the identity, so everything commutes and this has dimension $4$. For the lower right block, we need $B$ to be diagonal there (dimension $2$). So then $\dim N = 6.$ As for the second question, I feel like the answer is no, but I am struggling to come up with a counterexample.",,['linear-algebra']
50,Geometry of $2 \times 2$ matrix mappings,Geometry of  matrix mappings,2 \times 2,"Suppose $\mathbf{A}$ is a $2 \times 2$ matrix. I'm interested in the geometry of the mapping $T(\mathbf{x}) = \mathbf{A}\mathbf{x}$ from $\mathbb{R}^2$ to $\mathbb{R}^2$. The effect on the vectors $\mathbf{e_1} = (1,0)$ and $\mathbf{e_2} = (0,1)$ is clear -- $T(\mathbf{e_1})$ and $T(\mathbf{e_2})$ are just the columns of $\mathbf{A}$. So, by linearity, a unit square gets mapped to a parallelogram. The green square gets mapped to the pink parallelogram in the pictures below. Now let's consider the effect on  a unit circle. It gets mapped to an ellipse, and I'm interested in how the geometry of this ellipse is related to the matrix $\mathbf{A}$. One case seems clear: if $\mathbf{A}$ is symmetric, then the axes of the ellipse are the eignevectors of $\mathbf{A}$, and its semi-axis lengths are the eigenvalues. The ""stretching"" of the circle to form the ellipse is nicely related to eigenvalues and eigenvectors. Fabulous. This is illustrated in the following picture: Another case is also clear: if the eigenvalues of $\mathbf{A}$ are not real, then presumably there is no relationship whatsoever to the geometry of the ellipse. Now the case that's puzzling me: what if $\mathbf{A}$ is not symmetric, but still has real eigenvalues. What sort of geometric relationship exists in this case, if any? This case is illustrated in the following picture:","Suppose $\mathbf{A}$ is a $2 \times 2$ matrix. I'm interested in the geometry of the mapping $T(\mathbf{x}) = \mathbf{A}\mathbf{x}$ from $\mathbb{R}^2$ to $\mathbb{R}^2$. The effect on the vectors $\mathbf{e_1} = (1,0)$ and $\mathbf{e_2} = (0,1)$ is clear -- $T(\mathbf{e_1})$ and $T(\mathbf{e_2})$ are just the columns of $\mathbf{A}$. So, by linearity, a unit square gets mapped to a parallelogram. The green square gets mapped to the pink parallelogram in the pictures below. Now let's consider the effect on  a unit circle. It gets mapped to an ellipse, and I'm interested in how the geometry of this ellipse is related to the matrix $\mathbf{A}$. One case seems clear: if $\mathbf{A}$ is symmetric, then the axes of the ellipse are the eignevectors of $\mathbf{A}$, and its semi-axis lengths are the eigenvalues. The ""stretching"" of the circle to form the ellipse is nicely related to eigenvalues and eigenvectors. Fabulous. This is illustrated in the following picture: Another case is also clear: if the eigenvalues of $\mathbf{A}$ are not real, then presumably there is no relationship whatsoever to the geometry of the ellipse. Now the case that's puzzling me: what if $\mathbf{A}$ is not symmetric, but still has real eigenvalues. What sort of geometric relationship exists in this case, if any? This case is illustrated in the following picture:",,"['linear-algebra', 'geometry', 'eigenvalues-eigenvectors', 'linear-transformations', 'conic-sections']"
51,For which dimensions is it possible to have $A \succeq B \succeq 0$ with $A^2 - B^2$ having $n-1$ negative eigenvalues?,For which dimensions is it possible to have  with  having  negative eigenvalues?,A \succeq B \succeq 0 A^2 - B^2 n-1,"For any dimension $n$, can we write down two symmetric, positive semi-definite matrices $A,B$ with $A \succeq B$ in the sense of the usual ordering (i.e., $A-B$ is positive semidefinite) such that $A^2 - B^2$ has $n-1$ negative eigenvalues? Notes: For $n=2$, there are examples of matrices $A,B$ such that $A \succeq B$ but it is not true that $A^2 \succeq B^2$. For example: $$A  = \left( \begin{array}{cc} 2 & 1 \\ 1 & 1 \end{array} \right), B = \left( \begin{array}{cc} 1 & 0 \\ 0 & 0 \end{array} \right)$$ For $n=2$, this pair of matrices provides an answer to this question. Since ${\rm tr}(A^2-B^2) \geq 0$, the matrix $A^2 - B^2$ has to have at least one nonnegative eigenvalue. My motivation: the fact that $A \succeq B$ does not imply $A^2 \succeq B^2$ is somewhat unintuitive to me. I was just wondering if one can construct an example where $A^2 - B^2$ is ``as close'' to a negative definite matrix as possible.","For any dimension $n$, can we write down two symmetric, positive semi-definite matrices $A,B$ with $A \succeq B$ in the sense of the usual ordering (i.e., $A-B$ is positive semidefinite) such that $A^2 - B^2$ has $n-1$ negative eigenvalues? Notes: For $n=2$, there are examples of matrices $A,B$ such that $A \succeq B$ but it is not true that $A^2 \succeq B^2$. For example: $$A  = \left( \begin{array}{cc} 2 & 1 \\ 1 & 1 \end{array} \right), B = \left( \begin{array}{cc} 1 & 0 \\ 0 & 0 \end{array} \right)$$ For $n=2$, this pair of matrices provides an answer to this question. Since ${\rm tr}(A^2-B^2) \geq 0$, the matrix $A^2 - B^2$ has to have at least one nonnegative eigenvalue. My motivation: the fact that $A \succeq B$ does not imply $A^2 \succeq B^2$ is somewhat unintuitive to me. I was just wondering if one can construct an example where $A^2 - B^2$ is ``as close'' to a negative definite matrix as possible.",,"['linear-algebra', 'matrices']"
52,Symmetrical and skew-symmetrical part of rotation matrix,Symmetrical and skew-symmetrical part of rotation matrix,,"Every matrix can be decomposed to symmetrical and skew-symmetrical part with the formula: $ A=\dfrac{1}{2}(A+A^T)+\dfrac{1}{2}(A-A^T)$. However if it is known only symmetrical part (we assume here that the whole matrix is unknown) it's impossible without additional information to reconstruct exactly skew-symmetrical part and vice versa. In the case of 3D rotation matrix we have additional constraints and probably such reconstruction is possible. Let's look at Rodrigues formula and two (symmetrical and skew-symetrical) parts  of rotation matrix: $R(v,\theta)= \{I+(1-cos(\theta))S^2(v)\} + sin(\theta)S(v)$ where $S(v)=\begin{bmatrix} v\times{i} &  v\times{j}&  v\times{k}  \end{bmatrix}^T$, skew-symetric matrix itself  ($3$ DOF set by components of an axis $v$) One can notice that having skew-symmetrical part of rotation matrix it is  relatively easy to reconstruct symmetrical part. Indeed $skew(R)=sin(\theta)S(v)$ and the whole expression $skew(R)$ can be decomposed to the product $kK$ in such a way that the sum of squares of matrix $K$ entries i.e. $ \begin{bmatrix} 1 &  1 & 1  \end{bmatrix}  (K\circ{K}) \begin{bmatrix} 1  &  1  & 1  \end{bmatrix}^T =2 $ ... then $k=sin(\theta)$ and $K=S(v)$ and we can calculate $ cos(\theta)$ and $ S(v)^2$ what makes possible reconstruction of symmetrical part. Exactly we have two solutions because $sin(\theta)=sin(\pi-\theta)$. In the second case when we want to reconstruct skew-symmetrical part the solution seems to be difficult to find (at least for me) so my question is: how to obtain skew-symmetrical part of rotation matrix $skew(R)$ knowing its symmetrical part $sym(R)$? additionally: why do such asymmetry in difficulty of solutions exist at all ? ( symmetry and skew symmetry in the first formula for the decomposition of any matrix $A$ seem not to differ too much ) what is the situation for higher dimensions? (when we don't have a Rodrigues formula)","Every matrix can be decomposed to symmetrical and skew-symmetrical part with the formula: $ A=\dfrac{1}{2}(A+A^T)+\dfrac{1}{2}(A-A^T)$. However if it is known only symmetrical part (we assume here that the whole matrix is unknown) it's impossible without additional information to reconstruct exactly skew-symmetrical part and vice versa. In the case of 3D rotation matrix we have additional constraints and probably such reconstruction is possible. Let's look at Rodrigues formula and two (symmetrical and skew-symetrical) parts  of rotation matrix: $R(v,\theta)= \{I+(1-cos(\theta))S^2(v)\} + sin(\theta)S(v)$ where $S(v)=\begin{bmatrix} v\times{i} &  v\times{j}&  v\times{k}  \end{bmatrix}^T$, skew-symetric matrix itself  ($3$ DOF set by components of an axis $v$) One can notice that having skew-symmetrical part of rotation matrix it is  relatively easy to reconstruct symmetrical part. Indeed $skew(R)=sin(\theta)S(v)$ and the whole expression $skew(R)$ can be decomposed to the product $kK$ in such a way that the sum of squares of matrix $K$ entries i.e. $ \begin{bmatrix} 1 &  1 & 1  \end{bmatrix}  (K\circ{K}) \begin{bmatrix} 1  &  1  & 1  \end{bmatrix}^T =2 $ ... then $k=sin(\theta)$ and $K=S(v)$ and we can calculate $ cos(\theta)$ and $ S(v)^2$ what makes possible reconstruction of symmetrical part. Exactly we have two solutions because $sin(\theta)=sin(\pi-\theta)$. In the second case when we want to reconstruct skew-symmetrical part the solution seems to be difficult to find (at least for me) so my question is: how to obtain skew-symmetrical part of rotation matrix $skew(R)$ knowing its symmetrical part $sym(R)$? additionally: why do such asymmetry in difficulty of solutions exist at all ? ( symmetry and skew symmetry in the first formula for the decomposition of any matrix $A$ seem not to differ too much ) what is the situation for higher dimensions? (when we don't have a Rodrigues formula)",,"['linear-algebra', 'matrices', 'rotations']"
53,Two different definitions of Jordan canonical form,Two different definitions of Jordan canonical form,,"I am currently reading two linear algebra books. One is Hoffman/Kunze's and the other one is Friedberg/Insel/Spence's.  They define Jordan canonical form of linear operator in different ways.  In Hoffman's book, Jordan forms are obtained by the primary decomposition and cyclic decomposition. In this case, 1's can appear some sub-diagonal entries. For example, $M = \begin{array}{cc} 2 & 0 & 0 & 0 & 0 \\ 1 & 2 & 0 & 0 & 0 \\ 0 & 0 & 3 & 0 & 0 \\ 0 & 0 & 1 & 3 & 0 \\ 0 & 0 & 0 & 0 & 3 \\ \end{array}$ But in Friedberg's (And almost any other linear algebra books for engineers..), 1's can appear some super-diagonal entries. For example, $M' = \begin{array}{cc} 2 & 1 & 0 & 0 & 0 \\ 0 & 2 & 0 & 0 & 0 \\ 0 & 0 & 3 & 1 & 0 \\ 0 & 0 & 0 & 3 & 0 \\ 0 & 0 & 0 & 0 & 3 \\ \end{array}$ Can someone explain why they define differently and relation of each other?","I am currently reading two linear algebra books. One is Hoffman/Kunze's and the other one is Friedberg/Insel/Spence's.  They define Jordan canonical form of linear operator in different ways.  In Hoffman's book, Jordan forms are obtained by the primary decomposition and cyclic decomposition. In this case, 1's can appear some sub-diagonal entries. For example, $M = \begin{array}{cc} 2 & 0 & 0 & 0 & 0 \\ 1 & 2 & 0 & 0 & 0 \\ 0 & 0 & 3 & 0 & 0 \\ 0 & 0 & 1 & 3 & 0 \\ 0 & 0 & 0 & 0 & 3 \\ \end{array}$ But in Friedberg's (And almost any other linear algebra books for engineers..), 1's can appear some super-diagonal entries. For example, $M' = \begin{array}{cc} 2 & 1 & 0 & 0 & 0 \\ 0 & 2 & 0 & 0 & 0 \\ 0 & 0 & 3 & 1 & 0 \\ 0 & 0 & 0 & 3 & 0 \\ 0 & 0 & 0 & 0 & 3 \\ \end{array}$ Can someone explain why they define differently and relation of each other?",,"['linear-algebra', 'jordan-normal-form']"
54,"If the sum $A_1 + A_2 + · · · + A_n$ is equal to the negative of the identity operator on $V$ , show that $dim_{\mathbb{R}} V$ is even.","If the sum  is equal to the negative of the identity operator on  , show that  is even.",A_1 + A_2 + · · · + A_n V dim_{\mathbb{R}} V,"Let $V$ be a finite-dimensional vector space over the real numbers $\mathbb{R}$. Suppose $A_1,A_2,....,A_n$ are finitely many pairwise commuting linear operators on $V$. Assume that none of the operators $A_i$ has a negative real eigenvalue. If the sum $A_1 + A_2 + · · · + A_n$ is equal to the negative of the identity operator on $V$ , show that $dim_{\mathbb{R}} V$ is even.","Let $V$ be a finite-dimensional vector space over the real numbers $\mathbb{R}$. Suppose $A_1,A_2,....,A_n$ are finitely many pairwise commuting linear operators on $V$. Assume that none of the operators $A_i$ has a negative real eigenvalue. If the sum $A_1 + A_2 + · · · + A_n$ is equal to the negative of the identity operator on $V$ , show that $dim_{\mathbb{R}} V$ is even.",,['linear-algebra']
55,Can a elementary row operation change the size of a matrix?,Can a elementary row operation change the size of a matrix?,,"My question is very simple - Can an elementary row operation change the size (eg: $2\times2$ or $3\times 2$) of a matrix? I think the answer should be no, but while reading Linear Algebra by Hoffman Kunze I stumbled upon this: Definition. An $m\times n$ matrix is said to be an elementary matrix if it can be obtained from the $m\times m$ identity matrix by means of a single elementary row operation. Now, I know of 3 elementary row operations, (adding a multiple of one row to another, multiplying throughout a row by a non zero constant and interchanging two rows) but none of them can change the size of a matrix. But since this is a highly praised book I can't trust myself as much as I'd like to.","My question is very simple - Can an elementary row operation change the size (eg: $2\times2$ or $3\times 2$) of a matrix? I think the answer should be no, but while reading Linear Algebra by Hoffman Kunze I stumbled upon this: Definition. An $m\times n$ matrix is said to be an elementary matrix if it can be obtained from the $m\times m$ identity matrix by means of a single elementary row operation. Now, I know of 3 elementary row operations, (adding a multiple of one row to another, multiplying throughout a row by a non zero constant and interchanging two rows) but none of them can change the size of a matrix. But since this is a highly praised book I can't trust myself as much as I'd like to.",,"['linear-algebra', 'matrices']"
56,Exponential of a symmetric matrix,Exponential of a symmetric matrix,,"Let $A$ be a real, symmetric and positive definite matrix and suppose $B$ is a real symmetric matrix such that $\exp(B) = A$. Is $B$ unique? The solution of my homework sheet says that $B$ is uniquely determined by $A$ since, for every eigenvalue $t \in \mathbb{R}$ of $B$, the $t$-eigenspace of $B$ equals the $e^t$-eigenspace of $A$. So far I was only able to show one inclusion of this equality. How do I prove  that if $r>0$ is an eigenvalue of $A$ and $v$ a vector such that $Av = rv$, then $Bv = \log(r)v$? Is this even true?","Let $A$ be a real, symmetric and positive definite matrix and suppose $B$ is a real symmetric matrix such that $\exp(B) = A$. Is $B$ unique? The solution of my homework sheet says that $B$ is uniquely determined by $A$ since, for every eigenvalue $t \in \mathbb{R}$ of $B$, the $t$-eigenspace of $B$ equals the $e^t$-eigenspace of $A$. So far I was only able to show one inclusion of this equality. How do I prove  that if $r>0$ is an eigenvalue of $A$ and $v$ a vector such that $Av = rv$, then $Bv = \log(r)v$? Is this even true?",,"['linear-algebra', 'matrices', 'exponentiation']"
57,"For every nonzero vector $v$ there exists a linear functional $f$, sucht that $f(v) \neq 0$.","For every nonzero vector  there exists a linear functional , sucht that .",v f f(v) \neq 0,"I want to prove that for all $v \in V$ with $v \neq 0 \implies \exists f \in V^{*} : f(v) \neq 0$. I know that if $V$ is finite-dimensional we can choose a basis $\{e_i\}$ of $V$ and construct the corresponding dual basis $\{e^{*}_i\}$. If $v \neq 0$ then necessarily at least one component of $v$ with respect to $\{e_i\}$ must be nonzero. WLOG let $v^{j} \neq 0$ then $e^{*}_j(v) = v^{j} \neq 0$ proving the theorem. However, I think this theorem should be true even in the infinite-dimensional case and I would like to see a basis-free proof (i.e. a proof that doesn't require a choice of basis). Apparently there's a proof in this question , but I'm pretty sure that the constructed functional $f$ is not linear. For the sake of argument let me reproduce the answer here: Let $v \neq 0$ and let $H$ be a subspace of $V$, such that $V = \operatorname{span}(v)\oplus H$. Define $f: V^{*} \to \mathbb{F}$ by $f(v) = 1$ and $f(h) = 0$ for all $h \in H$. Now, let's check whether $f$ is homogenous. Let $w \in \operatorname{span}(v)$, such that $w = \alpha v$ for some nonzero $\alpha \in \mathbb{F}$. We need to show that $f(w) = f(\alpha v)$ equals $\alpha f(v) = \alpha$. However, by definition $f$ is nonzero only for the vector $v$ and we get $f(w) = 0 \neq \alpha = \alpha f(v)$. Even if we interpret the definition of $f$ to mean that $f(s) = 1$ for all $s \in \operatorname{span}(v)$ it doesn't work out. How is this supposed to work out?","I want to prove that for all $v \in V$ with $v \neq 0 \implies \exists f \in V^{*} : f(v) \neq 0$. I know that if $V$ is finite-dimensional we can choose a basis $\{e_i\}$ of $V$ and construct the corresponding dual basis $\{e^{*}_i\}$. If $v \neq 0$ then necessarily at least one component of $v$ with respect to $\{e_i\}$ must be nonzero. WLOG let $v^{j} \neq 0$ then $e^{*}_j(v) = v^{j} \neq 0$ proving the theorem. However, I think this theorem should be true even in the infinite-dimensional case and I would like to see a basis-free proof (i.e. a proof that doesn't require a choice of basis). Apparently there's a proof in this question , but I'm pretty sure that the constructed functional $f$ is not linear. For the sake of argument let me reproduce the answer here: Let $v \neq 0$ and let $H$ be a subspace of $V$, such that $V = \operatorname{span}(v)\oplus H$. Define $f: V^{*} \to \mathbb{F}$ by $f(v) = 1$ and $f(h) = 0$ for all $h \in H$. Now, let's check whether $f$ is homogenous. Let $w \in \operatorname{span}(v)$, such that $w = \alpha v$ for some nonzero $\alpha \in \mathbb{F}$. We need to show that $f(w) = f(\alpha v)$ equals $\alpha f(v) = \alpha$. However, by definition $f$ is nonzero only for the vector $v$ and we get $f(w) = 0 \neq \alpha = \alpha f(v)$. Even if we interpret the definition of $f$ to mean that $f(s) = 1$ for all $s \in \operatorname{span}(v)$ it doesn't work out. How is this supposed to work out?",,['linear-algebra']
58,Understanding the definition of the direct sum of subspaces of a vector space,Understanding the definition of the direct sum of subspaces of a vector space,,I have a question regarding the definition of direct sum of a vector space in relation to subspaces. Definition: A vector space $V$ is called the direct sum of $W_1$ and $W_2$ if $W_1$ and $W_2$ are subspaces of $V$ such that $W_1\cap W_2 = \{0\}$ and $W_1 + W_2 = V$. We denote that $V$ is the direct sum of $W_1$ and $W_2$ by writing $V = W_1\oplus W_2$. Is this definition saying that any vector in $V$ can be written as a linear combination of the vectors in the set $W_1 + W_2$? Thanks!,I have a question regarding the definition of direct sum of a vector space in relation to subspaces. Definition: A vector space $V$ is called the direct sum of $W_1$ and $W_2$ if $W_1$ and $W_2$ are subspaces of $V$ such that $W_1\cap W_2 = \{0\}$ and $W_1 + W_2 = V$. We denote that $V$ is the direct sum of $W_1$ and $W_2$ by writing $V = W_1\oplus W_2$. Is this definition saying that any vector in $V$ can be written as a linear combination of the vectors in the set $W_1 + W_2$? Thanks!,,"['linear-algebra', 'definition', 'direct-sum']"
59,Find (linear) transformation matrix using the fact that the diagonals of a parallelogram bisect each other.,Find (linear) transformation matrix using the fact that the diagonals of a parallelogram bisect each other.,,"This is the first time I post something on this website. I'm on this question already for hours. I'm clearly not asking the community to do my homework, I'm hoping someone can explain me how I should solve the following question; Let $l$ be a line through the origin in $\mathbb{R}^2$ , $P_l$ the linear transformation that projects a vector onto $l$ , and $F_l$ the transformation that reflects a vector in $l$ . Draw diagrams to show that $F_l$ is linear. Diagrams? How does this look like? A standard matrix? Figure 3.14 (see image) suggests a way to find the matrix of $F_l$ , using the fact that the diagonals of a parallelogram bisect each other. Prove that $F_l = 2P_l(x) - x$ , and use this result to show that the standard matrix of $F_l$ is (see image). If the angle between $l$ and the positive $x$ -axis is $A$ , show that the matrix of $F_l$ is (see image). I attached the question as image Hopefully you can help. Thanks! Image: https://i.sstatic.net/vFkmM.jpg EDIT: Image shown here:","This is the first time I post something on this website. I'm on this question already for hours. I'm clearly not asking the community to do my homework, I'm hoping someone can explain me how I should solve the following question; Let be a line through the origin in , the linear transformation that projects a vector onto , and the transformation that reflects a vector in . Draw diagrams to show that is linear. Diagrams? How does this look like? A standard matrix? Figure 3.14 (see image) suggests a way to find the matrix of , using the fact that the diagonals of a parallelogram bisect each other. Prove that , and use this result to show that the standard matrix of is (see image). If the angle between and the positive -axis is , show that the matrix of is (see image). I attached the question as image Hopefully you can help. Thanks! Image: https://i.sstatic.net/vFkmM.jpg EDIT: Image shown here:",l \mathbb{R}^2 P_l l F_l l F_l F_l F_l = 2P_l(x) - x F_l l x A F_l,"['linear-algebra', 'matrices']"
60,"Some questions about S.Roman, ""Advanced Linear Algebra""","Some questions about S.Roman, ""Advanced Linear Algebra""",,"Question for those who have studied Roman's book ""Advanced Linear Algebra"". How self-contained is this book. Can I study determinants directly from this in context of exterior algebra and tensor products? How much one can understand if he didn't have a previous course in Linear Algebra. I want to study linear algebra but I want to do it properly with focus in abstract algebra. That is, I want the book to talk about modules, tensor products, exterior algebras. I tried Blyth's ""Module theory - an approach to linear algebra"" and Winitzki's ""Linear Algebra via Exterior Products"", but it didn't work out very well. Not beause the material was too hard, but because I simply don't like the style. It's not fully rigorous. Now I hope I can learn something from Roman's book.","Question for those who have studied Roman's book ""Advanced Linear Algebra"". How self-contained is this book. Can I study determinants directly from this in context of exterior algebra and tensor products? How much one can understand if he didn't have a previous course in Linear Algebra. I want to study linear algebra but I want to do it properly with focus in abstract algebra. That is, I want the book to talk about modules, tensor products, exterior algebras. I tried Blyth's ""Module theory - an approach to linear algebra"" and Winitzki's ""Linear Algebra via Exterior Products"", but it didn't work out very well. Not beause the material was too hard, but because I simply don't like the style. It's not fully rigorous. Now I hope I can learn something from Roman's book.",,"['linear-algebra', 'reference-request', 'modules', 'book-recommendation']"
61,Explicit construction of $n/2$ by $n$ circulant partial Hadamard matrices,Explicit construction of  by  circulant partial Hadamard matrices,n/2 n,"In Circulant partial Hadamard matrices by Craigen, Faucher, Low, and Wares it is stated in Theorem 9 that there is a $(p+1)$ by $2(p+1)$ circulant partial Hadamard matrix for every prime power $p$. This is very interesting but I would really like an explicit construction and I can't work out if or where one is given. Is there an explicit construction that gives a $(p+1)$ by $2(p+1)$   circulant partial Hadamard matrix for prime power $p$?","In Circulant partial Hadamard matrices by Craigen, Faucher, Low, and Wares it is stated in Theorem 9 that there is a $(p+1)$ by $2(p+1)$ circulant partial Hadamard matrix for every prime power $p$. This is very interesting but I would really like an explicit construction and I can't work out if or where one is given. Is there an explicit construction that gives a $(p+1)$ by $2(p+1)$   circulant partial Hadamard matrix for prime power $p$?",,['linear-algebra']
62,How to extend the parallelepiped volume formula to higher dimensions?,How to extend the parallelepiped volume formula to higher dimensions?,,"The volume of a parallelepiped $(V)$ is given by the triple scalar product: $$V=\mathbf{c}\cdot{}(\mathbf{a}\times\mathbf{b})$$ where $\mathbf{a}$, $\mathbf{b}$, and $\mathbf{c}$ are the vectors originating at one of the vertices of the parallelepiped. Is there a way to easily extend this formula to higher dimensions, analogous to the way that the volume of a sphere can be extended to higher dimensions? In linear algebra it would be useful to have a general formula for the volume of a hyper-parallelepiped because the determinant of a $3 \times 3$ Gram matrix (and metric tensor too, I think) is just $V^2$.  In general, the triple scalar product offers a formula for deriving the linearly independent basis vectors of a $3\times3$ matrix in terms of each other and the constant $V$.  It would be nice to extend this trick to $n \times n$ matrices.","The volume of a parallelepiped $(V)$ is given by the triple scalar product: $$V=\mathbf{c}\cdot{}(\mathbf{a}\times\mathbf{b})$$ where $\mathbf{a}$, $\mathbf{b}$, and $\mathbf{c}$ are the vectors originating at one of the vertices of the parallelepiped. Is there a way to easily extend this formula to higher dimensions, analogous to the way that the volume of a sphere can be extended to higher dimensions? In linear algebra it would be useful to have a general formula for the volume of a hyper-parallelepiped because the determinant of a $3 \times 3$ Gram matrix (and metric tensor too, I think) is just $V^2$.  In general, the triple scalar product offers a formula for deriving the linearly independent basis vectors of a $3\times3$ matrix in terms of each other and the constant $V$.  It would be nice to extend this trick to $n \times n$ matrices.",,"['linear-algebra', 'geometry', 'matrix-equations', 'tensors']"
63,How to find $P(-1)$ for $\frac{P(2x)}{P(x+1)}=8-\frac{56}{x+7}$ and $P(1)=1$?,How to find  for  and ?,P(-1) \frac{P(2x)}{P(x+1)}=8-\frac{56}{x+7} P(1)=1,$P(x)$ is a polynomial such that $P(1)=1$ and $\frac{P(2x)}{P(x+1)}=8-\frac{56}{x+7}$ and $P(-1)$ is rational. How to find $P(-1)$?,$P(x)$ is a polynomial such that $P(1)=1$ and $\frac{P(2x)}{P(x+1)}=8-\frac{56}{x+7}$ and $P(-1)$ is rational. How to find $P(-1)$?,,"['linear-algebra', 'algebra-precalculus', 'polynomials']"
64,A coordinate free book on linear and multilinear algebra defining determinants using exterior algebra,A coordinate free book on linear and multilinear algebra defining determinants using exterior algebra,,"I would like to find an advanced introduction to linear and multilinear algebra that is 1)Coordinate free 2)Use tensor products and exterior algebras to define determinants 3)DOES NOT assume a previous course in linear algebra, but only assumes some mathematical maturity and perhaps a little abstract algebra like group and field definitions and basic theorems.And definitely DOES NOT assume ANYTHING about determinants, so defines them from scratch. Thanks in advance.","I would like to find an advanced introduction to linear and multilinear algebra that is 1)Coordinate free 2)Use tensor products and exterior algebras to define determinants 3)DOES NOT assume a previous course in linear algebra, but only assumes some mathematical maturity and perhaps a little abstract algebra like group and field definitions and basic theorems.And definitely DOES NOT assume ANYTHING about determinants, so defines them from scratch. Thanks in advance.",,"['linear-algebra', 'matrices', 'tensor-products', 'multilinear-algebra', 'exterior-algebra']"
65,Basis for Skew Symmetric Matrix,Basis for Skew Symmetric Matrix,,"I'm trying to find a basis for the kernel for the following mapping:  Considering the linear transformation T: $M_{33} \rightarrow M_{33}  $ defined by $T(A) = .5(A + A^T)$. I know that this is basically asking for the basis under the condition that $T(A)=0$ which means that $A+A^T=0$ so $A^T = -A$.  I found that matrices that fit this condition are Skew Symmetric Matrices.  However, I'm not sure how to find the basis for the kernel of these matrices.","I'm trying to find a basis for the kernel for the following mapping:  Considering the linear transformation T: $M_{33} \rightarrow M_{33}  $ defined by $T(A) = .5(A + A^T)$. I know that this is basically asking for the basis under the condition that $T(A)=0$ which means that $A+A^T=0$ so $A^T = -A$.  I found that matrices that fit this condition are Skew Symmetric Matrices.  However, I'm not sure how to find the basis for the kernel of these matrices.",,"['linear-algebra', 'matrices']"
66,Source/explanation for this matrix inequality,Source/explanation for this matrix inequality,,"Here it is: $$z^\top M^{-1} M^{-1}z \le \|M^{-1}\| z^\top M^{-1} z.$$ Where $\pmb M$ is positive definite symmetric, $z$ is a vector in $\mathbb{R}^p$ (not necessarily normed!) and $||\pmb M||$ is the induced operator norm, i.e., $\sup_{∥x∥≤1}∥Mx∥$","Here it is: $$z^\top M^{-1} M^{-1}z \le \|M^{-1}\| z^\top M^{-1} z.$$ Where $\pmb M$ is positive definite symmetric, $z$ is a vector in $\mathbb{R}^p$ (not necessarily normed!) and $||\pmb M||$ is the induced operator norm, i.e., $\sup_{∥x∥≤1}∥Mx∥$",,"['linear-algebra', 'matrices', 'normed-spaces']"
67,Coordinate-free definition of elementary divisors,Coordinate-free definition of elementary divisors,,"There is a general mantra in math which says that what is independent of bases shall be defined independent of bases. Well, it is well known that the elementary divisors of a linear map $M\xrightarrow{\ \ f\ \ }N$ of finitely generated free modules over a principal ideal domain $R$ are independent of bases. So I wonder whether there is a simple definition, which does not mention chosen bases of $M$ and $N$. Perhaps such a characterization would help to make the theory more streamlined. Of course it should be possible to prove that given elementary divisors $\alpha_1,\dots,\alpha_s$ of $f$ there are bases of $M$ and $N$ such that the corresponding matrix of $f$ has Smith normal form whose entries are precisely the $\alpha_i$ and conversely that given bases such that the corresponding matrix has Smith normal form, its entries are elementary divisors.","There is a general mantra in math which says that what is independent of bases shall be defined independent of bases. Well, it is well known that the elementary divisors of a linear map $M\xrightarrow{\ \ f\ \ }N$ of finitely generated free modules over a principal ideal domain $R$ are independent of bases. So I wonder whether there is a simple definition, which does not mention chosen bases of $M$ and $N$. Perhaps such a characterization would help to make the theory more streamlined. Of course it should be possible to prove that given elementary divisors $\alpha_1,\dots,\alpha_s$ of $f$ there are bases of $M$ and $N$ such that the corresponding matrix of $f$ has Smith normal form whose entries are precisely the $\alpha_i$ and conversely that given bases such that the corresponding matrix has Smith normal form, its entries are elementary divisors.",,"['linear-algebra', 'abstract-algebra']"
68,Name for the module corresponding to a square matrix,Name for the module corresponding to a square matrix,,"I recently learned that for each $n \times n$ matrix $A$ with entries in some field $F$, there is a corresponding $F[x]$-module $M_A$. Namely, $M_A$ is the set $F^n$ with vector addition defined as usual, but with scalar multiplication defined by $f(x) \cdot v = f(A) \, v$ for each $f(x) \in F[x]$. My question is whether there is a name for the module $M_A$, and if so, what is it? I'm sorry that this is a bit of a trivial question, but I've looked around and have been unable to find an answer. If this question is inappropriate for this site please don't hesitate to let me know. Thanks.","I recently learned that for each $n \times n$ matrix $A$ with entries in some field $F$, there is a corresponding $F[x]$-module $M_A$. Namely, $M_A$ is the set $F^n$ with vector addition defined as usual, but with scalar multiplication defined by $f(x) \cdot v = f(A) \, v$ for each $f(x) \in F[x]$. My question is whether there is a name for the module $M_A$, and if so, what is it? I'm sorry that this is a bit of a trivial question, but I've looked around and have been unable to find an answer. If this question is inappropriate for this site please don't hesitate to let me know. Thanks.",,"['linear-algebra', 'abstract-algebra', 'functional-analysis', 'terminology', 'representation-theory']"
69,Berkeley Problems in Mathematics 7.5.22,Berkeley Problems in Mathematics 7.5.22,,"This is the problem: Let $A$ be a real symmetric $n \times n$ matrix with non negative entries. Prove that $A$ has an eigenvector with non-negative entries I looked at the answer key and don't quite understand it. In the expression containing max, why should it correspond to the eigenvalue $\lambda_0$? I thought that this may be because if Ax is parallel to x, then the dot product between $Ax$ and $x$ is maximised, but is it not possible that it still attains a large value if $A$ transforms $x$ in a way that scales x by so much that Ax is large enough to make $\langle Ax,x\rangle$ large even though they may not be parallel? Solution(as in answer key): Let $\lambda_0$ be the largest eigenvalue of $A$. We have $$\lambda_0 = \max{\{\langle Ax, x\rangle\mid x\in\mathbb{R}^n,\|x\| = 1\}}$$ and the maximum it attains precisely when $x$ is an eigenvector of $A$ with  eigenvalue $\lambda_0$. Suppose $v$ is a unit vector for which the maximum is attained, and let $u$ be the vector whose coordinates are the absolute values of the coordinates of $v$. Since the entries of $A$ are nonnegative, we have $$\langle Au,u \rangle \ge \langle Ax,x\rangle =\lambda_0$$ implying that $\langle Au,u\rangle = \lambda_0$, so that $u$ is an eigenvector of $A$ for the eigenvalue $\lambda_0$.","This is the problem: Let $A$ be a real symmetric $n \times n$ matrix with non negative entries. Prove that $A$ has an eigenvector with non-negative entries I looked at the answer key and don't quite understand it. In the expression containing max, why should it correspond to the eigenvalue $\lambda_0$? I thought that this may be because if Ax is parallel to x, then the dot product between $Ax$ and $x$ is maximised, but is it not possible that it still attains a large value if $A$ transforms $x$ in a way that scales x by so much that Ax is large enough to make $\langle Ax,x\rangle$ large even though they may not be parallel? Solution(as in answer key): Let $\lambda_0$ be the largest eigenvalue of $A$. We have $$\lambda_0 = \max{\{\langle Ax, x\rangle\mid x\in\mathbb{R}^n,\|x\| = 1\}}$$ and the maximum it attains precisely when $x$ is an eigenvector of $A$ with  eigenvalue $\lambda_0$. Suppose $v$ is a unit vector for which the maximum is attained, and let $u$ be the vector whose coordinates are the absolute values of the coordinates of $v$. Since the entries of $A$ are nonnegative, we have $$\langle Au,u \rangle \ge \langle Ax,x\rangle =\lambda_0$$ implying that $\langle Au,u\rangle = \lambda_0$, so that $u$ is an eigenvector of $A$ for the eigenvalue $\lambda_0$.",,['linear-algebra']
70,"If binomial expansion holds for $(A+B)^n$, does it follow that $A$ ad $B$ commute?","If binomial expansion holds for , does it follow that  ad  commute?",(A+B)^n A B,"Let $A, B$ be $2 \times 2$ real matrices. Is $AB=BA$ if $(A+B)^3=A^3+3A^2B+3AB^2+B^3$ ? Also, generalization for arbitrary positive integer $n$ is appreciated. This is not a homework problem, but something that interests me.","Let be real matrices. Is if ? Also, generalization for arbitrary positive integer is appreciated. This is not a homework problem, but something that interests me.","A, B 2 \times 2 AB=BA (A+B)^3=A^3+3A^2B+3AB^2+B^3 n","['linear-algebra', 'matrices']"
71,How to show the sum of the images of such $m$ projections is direct and is the whole space?,How to show the sum of the images of such  projections is direct and is the whole space?,m,"There are $m$ projections (whose square are themselves) $\phi_1,\cdots,\phi_m$ acting on a  finite-dimensional vector space $V$ such that $$\phi_i\phi_j=0\quad i\ne j\tag{1}$$ where $0$ denotes the zero transformation, and that $$\bigcap_{1}^m \text{Ker}\phi_i=0\tag{2}$$ where $0$ denotes the zero vector space. Prove that $$\bigoplus_{1}^m \text{Im}\phi_i=V$$ I have successfully shown that the sum is direct, but I failed to show the sum is exactly $V$. Since I only used  argument (1) to show the directness, it appears that I must use the (2) to show the sum is indeed the whole space. But for me I can do little about that. One possible approach is try to show that $\phi_1+\cdots+\phi_m=\text{id}$ or $\text{id}-\phi_1=\phi_2+\cdots+\phi_m$, since  $\text{Im}\phi_i\oplus\text{Ker}\phi_i(=\text{Im}(\text{id}-\phi_i))=V$ and maybe we can get $\text{Ker}$ involved somehow  in this way. But I've made no progress either. Any help or hint will be appreciated, thanks.","There are $m$ projections (whose square are themselves) $\phi_1,\cdots,\phi_m$ acting on a  finite-dimensional vector space $V$ such that $$\phi_i\phi_j=0\quad i\ne j\tag{1}$$ where $0$ denotes the zero transformation, and that $$\bigcap_{1}^m \text{Ker}\phi_i=0\tag{2}$$ where $0$ denotes the zero vector space. Prove that $$\bigoplus_{1}^m \text{Im}\phi_i=V$$ I have successfully shown that the sum is direct, but I failed to show the sum is exactly $V$. Since I only used  argument (1) to show the directness, it appears that I must use the (2) to show the sum is indeed the whole space. But for me I can do little about that. One possible approach is try to show that $\phi_1+\cdots+\phi_m=\text{id}$ or $\text{id}-\phi_1=\phi_2+\cdots+\phi_m$, since  $\text{Im}\phi_i\oplus\text{Ker}\phi_i(=\text{Im}(\text{id}-\phi_i))=V$ and maybe we can get $\text{Ker}$ involved somehow  in this way. But I've made no progress either. Any help or hint will be appreciated, thanks.",,"['linear-algebra', 'vector-spaces', 'linear-transformations', 'direct-sum']"
72,Finding an explicit formula from a recursive formula.,Finding an explicit formula from a recursive formula.,,"I have the recurrence relation: $$g(k, 0, x) = k,$$ $$g(k, n, x) = \dfrac{1}{2} \log_{k}{\left(\dfrac{k^{g(k, n - 1, x)}x}{g(k, n - 1, x)}\right)},$$ and I would like to solve it, if it is possible. By the way, $\lim_{n \to \infty}{g(k, n, x)} = f^{-1}(k, x), f(k, x) = k^{x}x$.","I have the recurrence relation: $$g(k, 0, x) = k,$$ $$g(k, n, x) = \dfrac{1}{2} \log_{k}{\left(\dfrac{k^{g(k, n - 1, x)}x}{g(k, n - 1, x)}\right)},$$ and I would like to solve it, if it is possible. By the way, $\lim_{n \to \infty}{g(k, n, x)} = f^{-1}(k, x), f(k, x) = k^{x}x$.",,"['linear-algebra', 'algebra-precalculus', 'recurrence-relations']"
73,"How do I prove that the inverse of the matrix, M, below has all elements greater than or equal to 0?","How do I prove that the inverse of the matrix, M, below has all elements greater than or equal to 0?",,$M=   \begin{bmatrix}     1 & \nu & 0 & ... & 0 & -\nu \\     -\nu & 1 & \nu & 0 & ... & 0 \\     0 & -\nu & 1 & \nu & ... & 0 \\     0 & 0 & -\nu & 1 & \nu.. & 0 \\      .&	 &		.&	.&	.&		 \\	     \nu & 0 & ... & 0 & -\nu & 1 \\   \end{bmatrix} $ It is basically a Circulant matrix . How do I prove that the inverse of this matrix will have all positive entries. For what conditions of $\nu$ will it be a non-negative matrix ?,$M=   \begin{bmatrix}     1 & \nu & 0 & ... & 0 & -\nu \\     -\nu & 1 & \nu & 0 & ... & 0 \\     0 & -\nu & 1 & \nu & ... & 0 \\     0 & 0 & -\nu & 1 & \nu.. & 0 \\      .&	 &		.&	.&	.&		 \\	     \nu & 0 & ... & 0 & -\nu & 1 \\   \end{bmatrix} $ It is basically a Circulant matrix . How do I prove that the inverse of this matrix will have all positive entries. For what conditions of $\nu$ will it be a non-negative matrix ?,,"['linear-algebra', 'matrices']"
74,A norm which is symmetric enough is induced by an inner product?,A norm which is symmetric enough is induced by an inner product?,,"$\newcommand{\<}{\langle} \newcommand{\>}{\rangle} $ It is a fact that for every norm $\| \|$ on a finite dimensional (real) vector  space, its isometry group $\text{ISO}(|| \cdot ||)$ is contained in some isometry group of a suitable inner product. ( see this question ). Now assume we have a norm $\| \|$ such that $\text{ISO}(|| \cdot ||)=\text{ISO}(\<,\>)$ for some inner product. Is $\| \|$ necessarily induced by an inner product? Update: The answer is yes. The key fact is the transitivity of the isometry group. Actually, as pointed out in this question in MO , the following statement is true: Let $X$ be finite-dimensional normed space whose isometry group acts transitively on the unit sphere (i.e, for every two unit-norm vectors $x,y∈X$ there exist a linear isometry from $(X, \| \|)$ to itself that sends $x$ to y). Then X is a Euclidean space (i.e., the norm comes from a scalar product). The proof is decomposed of two steps: (1) showing there exists an inner product $\< ,\>$ whose isometry group $\text{ISO}(|| \cdot ||)\subseteq \text{ISO}(\<,\>)$. The basic idea is this: $\text{ISO}(|| \cdot ||)$ is compact, hence it admits an invariant probability measure, hence (by an averaging argument) there exists a Euclidean structure preserved by it. ( The details can be found here ). (2) The transitivity of $\text{ISO}(|| \cdot ||)$ together with (1) impliy that the original norm is proportional to that Euclidean norm.","$\newcommand{\<}{\langle} \newcommand{\>}{\rangle} $ It is a fact that for every norm $\| \|$ on a finite dimensional (real) vector  space, its isometry group $\text{ISO}(|| \cdot ||)$ is contained in some isometry group of a suitable inner product. ( see this question ). Now assume we have a norm $\| \|$ such that $\text{ISO}(|| \cdot ||)=\text{ISO}(\<,\>)$ for some inner product. Is $\| \|$ necessarily induced by an inner product? Update: The answer is yes. The key fact is the transitivity of the isometry group. Actually, as pointed out in this question in MO , the following statement is true: Let $X$ be finite-dimensional normed space whose isometry group acts transitively on the unit sphere (i.e, for every two unit-norm vectors $x,y∈X$ there exist a linear isometry from $(X, \| \|)$ to itself that sends $x$ to y). Then X is a Euclidean space (i.e., the norm comes from a scalar product). The proof is decomposed of two steps: (1) showing there exists an inner product $\< ,\>$ whose isometry group $\text{ISO}(|| \cdot ||)\subseteq \text{ISO}(\<,\>)$. The basic idea is this: $\text{ISO}(|| \cdot ||)$ is compact, hence it admits an invariant probability measure, hence (by an averaging argument) there exists a Euclidean structure preserved by it. ( The details can be found here ). (2) The transitivity of $\text{ISO}(|| \cdot ||)$ together with (1) impliy that the original norm is proportional to that Euclidean norm.",,"['linear-algebra', 'normed-spaces', 'inner-products', 'isometry', 'metric-geometry']"
75,Linear Maps and Basis of Domain,Linear Maps and Basis of Domain,,"I don't understand the theorem from Axler's Linear Algebra Done Right: Suppose $v_1,v_2,...,v_n$ is a basis of $V$ and $w_1,....,w_n \in W$. Then there exists a unique linear map $T: V \to W$ such that $Tv_j = w_j$ for $j = 1,...,n$ What is this trying to say? I don't understand. In addition, I don't understand the proof of this by construction. How does this show it exists, and how does the uniqueness proof indeed show that it is unique?","I don't understand the theorem from Axler's Linear Algebra Done Right: Suppose $v_1,v_2,...,v_n$ is a basis of $V$ and $w_1,....,w_n \in W$. Then there exists a unique linear map $T: V \to W$ such that $Tv_j = w_j$ for $j = 1,...,n$ What is this trying to say? I don't understand. In addition, I don't understand the proof of this by construction. How does this show it exists, and how does the uniqueness proof indeed show that it is unique?",,"['linear-algebra', 'linear-transformations']"
76,Using linear algebra (e.g. matrix) methods to solve a system of linear inequalities,Using linear algebra (e.g. matrix) methods to solve a system of linear inequalities,,"Say we have the equation $Ax>b$, where $A$ is an M -by- N matrix, $b$ is a known vector of length N , x is an unknown vector of length N , and the inequality sign means that each element of $Ax$ is greater than the corresponding element of $b$. Is there some linear-algebra-based method, such as e.g. some modification of LU decomposition or something similar, that would be amenable to programming on a computer and which would allow me to generically solve this system for $x$, given numerical values for $A$ and $b$? I will also accept any method of determining merely whether a solution exists, if not any solutions themselves. EDIT: I should clarify that I'm mainly working with matrices with N and M both less than 10, so an exact method that works analogously to e.g. LU decomposition will probably be faster than an iterative method.","Say we have the equation $Ax>b$, where $A$ is an M -by- N matrix, $b$ is a known vector of length N , x is an unknown vector of length N , and the inequality sign means that each element of $Ax$ is greater than the corresponding element of $b$. Is there some linear-algebra-based method, such as e.g. some modification of LU decomposition or something similar, that would be amenable to programming on a computer and which would allow me to generically solve this system for $x$, given numerical values for $A$ and $b$? I will also accept any method of determining merely whether a solution exists, if not any solutions themselves. EDIT: I should clarify that I'm mainly working with matrices with N and M both less than 10, so an exact method that works analogously to e.g. LU decomposition will probably be faster than an iterative method.",,"['linear-algebra', 'inequality', 'systems-of-equations', 'linear-programming', 'numerical-linear-algebra']"
77,What does a linear equation with more than 2 variables represent?,What does a linear equation with more than 2 variables represent?,,"A linear equation with 2 variables, say $Ax+By+C = 0$, represents a line on a plane but what does a linear equation with 3 variables $Ax+By+Dz+c=0$ represent? A line in space, or something else? On a general note, what does a linear equation with $n$ variables represent?","A linear equation with 2 variables, say $Ax+By+C = 0$, represents a line on a plane but what does a linear equation with 3 variables $Ax+By+Dz+c=0$ represent? A line in space, or something else? On a general note, what does a linear equation with $n$ variables represent?",,"['linear-algebra', 'geometry', 'analytic-geometry']"
78,How to Formulate this Linear Algebra Fact in a Coordinate Free way?,How to Formulate this Linear Algebra Fact in a Coordinate Free way?,,"There is a result result given in the last paragraph of pg 15 in Hoffman And Kunze's Linear algebra (2nd Edition) which essentially says that THEOREM. Let $F_1$ be a subfield of a field $F$.   If the entries of an $m\times n$ matrix $M$ lie in $F_1$, and $\mathbf b\in F_1^m$, then the system of equations $M\mathbf x=\mathbf b$ has a solution $\mathbf x\in F_1^m$ if and only if it has a solution in $F^m$. In other words, the theorem says: THEOREM. Let $F_1$ be a subfield of a field $F$ and $V=F^m$ be a vector space over $F$.   Let $\mathbf A, \mathbf A_1,\ldots, \mathbf A_n$ be vectors in $V$ each having all of it's entries in $F_1$.   Let $x_1,\ldots,x_n\in F$ be such that $\sum_{i=1}^{n}x_i\mathbf A_i=\mathbf A$.   Then there exist $y_1,\ldots,y_n\in F_1$ such that $\sum_{i=1}^{n}y_i\mathbf A_i=\mathbf A$. I am looking for a ""coordinate free"" formulation of the above theorem, meaning, I don't want to take $F_1^m$ as my vector space $V$, whose elements are $m$-tuples. I'd like have a finite dimensional vector space $V$ over $F_1$. Can somebody see how to do that? Thanks.","There is a result result given in the last paragraph of pg 15 in Hoffman And Kunze's Linear algebra (2nd Edition) which essentially says that THEOREM. Let $F_1$ be a subfield of a field $F$.   If the entries of an $m\times n$ matrix $M$ lie in $F_1$, and $\mathbf b\in F_1^m$, then the system of equations $M\mathbf x=\mathbf b$ has a solution $\mathbf x\in F_1^m$ if and only if it has a solution in $F^m$. In other words, the theorem says: THEOREM. Let $F_1$ be a subfield of a field $F$ and $V=F^m$ be a vector space over $F$.   Let $\mathbf A, \mathbf A_1,\ldots, \mathbf A_n$ be vectors in $V$ each having all of it's entries in $F_1$.   Let $x_1,\ldots,x_n\in F$ be such that $\sum_{i=1}^{n}x_i\mathbf A_i=\mathbf A$.   Then there exist $y_1,\ldots,y_n\in F_1$ such that $\sum_{i=1}^{n}y_i\mathbf A_i=\mathbf A$. I am looking for a ""coordinate free"" formulation of the above theorem, meaning, I don't want to take $F_1^m$ as my vector space $V$, whose elements are $m$-tuples. I'd like have a finite dimensional vector space $V$ over $F_1$. Can somebody see how to do that? Thanks.",,"['linear-algebra', 'abstract-algebra']"
79,Proving that the elements of $A^n$ matrix are non-zero,Proving that the elements of  matrix are non-zero,A^n,"Let $ A = \begin{pmatrix} 1 & 2 \\-1 & 1 \\ \end{pmatrix}$. Prove that for every positive integer $n$ there exist integers $x_{n},y_{n}$ such that $A^n= \begin{pmatrix} x_{n} & -2y_{n} \\ y_{n} & x_{n} \\ \end{pmatrix}$. Prove that for all $n$ the numbers $x_{n}$ and $y_{n}$ are non-zero. I proved the existence of numbers $x_{n}$ and $y_{n}$, with the recurrence relations $x_{n+1}=x_{n}+2y_{n}$ and $y_{n+1}=y_{n}-x_{n}$. For proving that these numbers are non-zero, I assumed that if $x_{n}=0$, then $detA^n=3^n=2y_{n}^2$. Since $3^n$ is always an odd integer, and $2y_{n}$ is always an even integer, we reach a contradiction. However, I'm stuck at the case $y_{n}=0$. I've looked at other approaches to this problem, but no other has worked so far.","Let $ A = \begin{pmatrix} 1 & 2 \\-1 & 1 \\ \end{pmatrix}$. Prove that for every positive integer $n$ there exist integers $x_{n},y_{n}$ such that $A^n= \begin{pmatrix} x_{n} & -2y_{n} \\ y_{n} & x_{n} \\ \end{pmatrix}$. Prove that for all $n$ the numbers $x_{n}$ and $y_{n}$ are non-zero. I proved the existence of numbers $x_{n}$ and $y_{n}$, with the recurrence relations $x_{n+1}=x_{n}+2y_{n}$ and $y_{n+1}=y_{n}-x_{n}$. For proving that these numbers are non-zero, I assumed that if $x_{n}=0$, then $detA^n=3^n=2y_{n}^2$. Since $3^n$ is always an odd integer, and $2y_{n}$ is always an even integer, we reach a contradiction. However, I'm stuck at the case $y_{n}=0$. I've looked at other approaches to this problem, but no other has worked so far.",,"['linear-algebra', 'sequences-and-series', 'matrices', 'recurrence-relations']"
80,How to find all integer solutions for underdetermined sytsem of linear equations,How to find all integer solutions for underdetermined sytsem of linear equations,,I do have a system of n equations with m variables where m > n with integer coefficients. I wish to find a set of integer solutions to this system (In my case n = 2 and m = 4). Could somebody tell me how I can do it? I already solved this system with Mathematica but I would like to redo these calculations by hand to understand how their were obtained. The system is: $\left\{ \begin{array}{l l} 4u - 3v + 4w + 3z = 1\\ -4v - 3u - 4z + 3w = 0 \end{array} \right.$,I do have a system of n equations with m variables where m > n with integer coefficients. I wish to find a set of integer solutions to this system (In my case n = 2 and m = 4). Could somebody tell me how I can do it? I already solved this system with Mathematica but I would like to redo these calculations by hand to understand how their were obtained. The system is: $\left\{ \begin{array}{l l} 4u - 3v + 4w + 3z = 1\\ -4v - 3u - 4z + 3w = 0 \end{array} \right.$,,"['linear-algebra', 'number-theory', 'linear-diophantine-equations']"
81,Subspace of $L(V)$,Subspace of,L(V),"Suppose $V$ is finite dimensional and $E$ is a subspace of $L(V)$ such that $ST\in E$ and $TS \in E$ for all $S \in L(V)$ and $T\in E$. Prove that $E=\{0\}$ or $E =L(V)$. When $E$ is non-trivial, I want to prove that $1 \in E$. Then we can deduce that $L(V) \subset E$ and we are done. However, I don't know how to use the fact '$ST\in E$ and $TS \in E$'. To me, $L(V)$ is a ring and clearly $E$ is an ideal of ring. Still, why $1 \in E?$","Suppose $V$ is finite dimensional and $E$ is a subspace of $L(V)$ such that $ST\in E$ and $TS \in E$ for all $S \in L(V)$ and $T\in E$. Prove that $E=\{0\}$ or $E =L(V)$. When $E$ is non-trivial, I want to prove that $1 \in E$. Then we can deduce that $L(V) \subset E$ and we are done. However, I don't know how to use the fact '$ST\in E$ and $TS \in E$'. To me, $L(V)$ is a ring and clearly $E$ is an ideal of ring. Still, why $1 \in E?$",,"['linear-algebra', 'abstract-algebra']"
82,"Computing bases for direct, wedge, tensor products, etc., of given vector spaces","Computing bases for direct, wedge, tensor products, etc., of given vector spaces",,"I am filled with all kinds of vector space and I want to make sure I understand the basis for each kind of vector space. Suppose $\{v_i\}_{i=1}^n$ is the basis for vector space $V$, $\{w_j\}_{j=1}^m$ is the basis for vector space $W$. $V+W$: all linear combo of $\{v_i\}$, $\{w_j\}$, basis is $\{v_i,w_j\}$, dimension is $\dim(V)+\dim(W)-\text{number of overlap}$ $V\oplus W$: Same as $V\times W$? $V\times W$: all $(v,w)$ with $v\in V$ and $w\in W$, basis $(v_i,w_j)$, dimension is $\dim(V)\dim(W)$ $V\otimes W$: all linear combo of $v_i\otimes w_j$, basis $v_i\otimes w_j$, dimension is $\dim(V)\dim(W)$. By universal property of tensor, $V\times W$ is isomorphism to $V\otimes W$? $V\bigwedge V$ or $\bigwedge^rV$: all linear combo of $v_i\wedge v_j$, basis $v_i\wedge v_j$ and $i\neq j$, dimension is ${\dim V}\choose r$ $V^{`}\cdot V$ or $S_rV$:(symmetric power) all linear combo of $v_i^{`}\cdot v_j$, basis $v_i^{`}\cdot v_j$ and $i\neq j$, dimension is ${\dim V +r-1} \choose r$. So eventually, I want to understand what are the basis for the two vector spaces $\bigwedge^2(V\oplus W)$ and $ \bigwedge^2V\oplus(V\otimes W)\oplus\bigwedge^2W$(and finally I need to find an isomorphism between these two spaces, which need to prove it maps basis to basis, and that is why I need to make sure I know the basis). Let $\{(v_i,w_j)\}=\{e_k\}$, where $i=1,\cdots,n$, $j=1,\cdots,m$, $k=1,\cdots,nm$. Is $\{e_i\wedge e_j\}_{i,j=1}^{nm}$ with $i\neq j$ the basis for $\bigwedge^2(V\oplus W)$? OMG, I am so confused. BTW, it seems that these two spaces are not isomorphism, is the problem wrong?","I am filled with all kinds of vector space and I want to make sure I understand the basis for each kind of vector space. Suppose $\{v_i\}_{i=1}^n$ is the basis for vector space $V$, $\{w_j\}_{j=1}^m$ is the basis for vector space $W$. $V+W$: all linear combo of $\{v_i\}$, $\{w_j\}$, basis is $\{v_i,w_j\}$, dimension is $\dim(V)+\dim(W)-\text{number of overlap}$ $V\oplus W$: Same as $V\times W$? $V\times W$: all $(v,w)$ with $v\in V$ and $w\in W$, basis $(v_i,w_j)$, dimension is $\dim(V)\dim(W)$ $V\otimes W$: all linear combo of $v_i\otimes w_j$, basis $v_i\otimes w_j$, dimension is $\dim(V)\dim(W)$. By universal property of tensor, $V\times W$ is isomorphism to $V\otimes W$? $V\bigwedge V$ or $\bigwedge^rV$: all linear combo of $v_i\wedge v_j$, basis $v_i\wedge v_j$ and $i\neq j$, dimension is ${\dim V}\choose r$ $V^{`}\cdot V$ or $S_rV$:(symmetric power) all linear combo of $v_i^{`}\cdot v_j$, basis $v_i^{`}\cdot v_j$ and $i\neq j$, dimension is ${\dim V +r-1} \choose r$. So eventually, I want to understand what are the basis for the two vector spaces $\bigwedge^2(V\oplus W)$ and $ \bigwedge^2V\oplus(V\otimes W)\oplus\bigwedge^2W$(and finally I need to find an isomorphism between these two spaces, which need to prove it maps basis to basis, and that is why I need to make sure I know the basis). Let $\{(v_i,w_j)\}=\{e_k\}$, where $i=1,\cdots,n$, $j=1,\cdots,m$, $k=1,\cdots,nm$. Is $\{e_i\wedge e_j\}_{i,j=1}^{nm}$ with $i\neq j$ the basis for $\bigwedge^2(V\oplus W)$? OMG, I am so confused. BTW, it seems that these two spaces are not isomorphism, is the problem wrong?",,"['linear-algebra', 'vector-spaces', 'tensor-products']"
83,Minimal polynomial of a matrix satisfying $A^t=A^2$,Minimal polynomial of a matrix satisfying,A^t=A^2,"Let $A\in M_n(\mathbb R)$ be a non-zero non-identity matrix such that $A^t=A^2$. Then is it possible to find the minimal polynomial of $A$? My  try is: the given condition implies that $A^4=A$, hence the minimal polynomial $p(t)$ of $A$ divides $t^4-t=t(t-1)(t^2+t+1)$. So $p(t)$ can be some factor of $t(t-1)(t^2+t+1)$. Also if $\lambda$ is an eigenvalue of $A$ and $x$ is the corresponding eigenvector (considered as a column vector), then $Ax=\lambda x \implies \lambda x^t=x^t A^t\implies \lambda x^t=x^tA^2\implies \lambda(x^t x)=x^t(A^2x)\implies \lambda(x^t x)=\lambda^2(x^tx)\implies (\lambda^2-\lambda)(x^tx)=0\implies\lambda^2-\lambda=0,\text{ as } x^tx \text{ is non-zero}.$ Hence $0$ and $1$ are the only eigenvalues of $A$. So the minimal polynomial is of the form $p(t)=t(t-1).$ Is this correct or did I misunderstood something? P.S. $A^t$ denotes the transpose of the matrix $A$.","Let $A\in M_n(\mathbb R)$ be a non-zero non-identity matrix such that $A^t=A^2$. Then is it possible to find the minimal polynomial of $A$? My  try is: the given condition implies that $A^4=A$, hence the minimal polynomial $p(t)$ of $A$ divides $t^4-t=t(t-1)(t^2+t+1)$. So $p(t)$ can be some factor of $t(t-1)(t^2+t+1)$. Also if $\lambda$ is an eigenvalue of $A$ and $x$ is the corresponding eigenvector (considered as a column vector), then $Ax=\lambda x \implies \lambda x^t=x^t A^t\implies \lambda x^t=x^tA^2\implies \lambda(x^t x)=x^t(A^2x)\implies \lambda(x^t x)=\lambda^2(x^tx)\implies (\lambda^2-\lambda)(x^tx)=0\implies\lambda^2-\lambda=0,\text{ as } x^tx \text{ is non-zero}.$ Hence $0$ and $1$ are the only eigenvalues of $A$. So the minimal polynomial is of the form $p(t)=t(t-1).$ Is this correct or did I misunderstood something? P.S. $A^t$ denotes the transpose of the matrix $A$.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'minimal-polynomials']"
84,Perturbation theory of the eigenvalues about the symmetric matrix,Perturbation theory of the eigenvalues about the symmetric matrix,,"From Weyl's theorem, i.e.: Let $A$ and $E$ be $n\times n$ real symmetric matrices. Let $\alpha_1\geq\ldots\geq\alpha_n$ be the eigenvalues of $A$ and $\hat{\alpha}_1\geq\ldots\geq\hat{\alpha}_n$ be the eigenvalues of $\hat{A}=A+E$. Then $|\alpha_i-\hat{\alpha}_i|\leq\|E\|_2$ , we know that the perturbed eigenvalues of a symmetric matrix differ at most $\|E\|_2$ size, which means they can be calculated in a high accuracy. But here I have the question: what if we assume $E$ be a skew-symmetric matrix, i.e.: $E^T=-E$? Do we also have $|\alpha_i-\hat{\alpha}_i|\leq c\|E\|_2$, for some constant $c$? If it is, we may conclude that any perturbed matrix to $A$ (not only symmetric perturbation) changes little to $A$'s eigenvalues. If not, is there any example to show that such $c$ (of $O(1)$ order) does not exists?","From Weyl's theorem, i.e.: Let $A$ and $E$ be $n\times n$ real symmetric matrices. Let $\alpha_1\geq\ldots\geq\alpha_n$ be the eigenvalues of $A$ and $\hat{\alpha}_1\geq\ldots\geq\hat{\alpha}_n$ be the eigenvalues of $\hat{A}=A+E$. Then $|\alpha_i-\hat{\alpha}_i|\leq\|E\|_2$ , we know that the perturbed eigenvalues of a symmetric matrix differ at most $\|E\|_2$ size, which means they can be calculated in a high accuracy. But here I have the question: what if we assume $E$ be a skew-symmetric matrix, i.e.: $E^T=-E$? Do we also have $|\alpha_i-\hat{\alpha}_i|\leq c\|E\|_2$, for some constant $c$? If it is, we may conclude that any perturbed matrix to $A$ (not only symmetric perturbation) changes little to $A$'s eigenvalues. If not, is there any example to show that such $c$ (of $O(1)$ order) does not exists?",,"['linear-algebra', 'matrices', 'perturbation-theory']"
85,Symmetric matrices and commutativity,Symmetric matrices and commutativity,,"Q: Let $m, n$ be positive intergers. Let $A$ and $B$ real $n\times n$ matrices. Assume that $B$ is symmetric and positive definite. If $A$ commutes with $B^{m}$, prove that $A$ commutes with $B$. So I don't really know where to start with this other than the decomposition theorem for symmetric positive operators, so that I can write $B$ as diagonal with positive real entries. Perhaps taking the $m^{th}$ root could be useful which we can do since $B$ has positive entries, but I'm not sure. Many thanks!","Q: Let $m, n$ be positive intergers. Let $A$ and $B$ real $n\times n$ matrices. Assume that $B$ is symmetric and positive definite. If $A$ commutes with $B^{m}$, prove that $A$ commutes with $B$. So I don't really know where to start with this other than the decomposition theorem for symmetric positive operators, so that I can write $B$ as diagonal with positive real entries. Perhaps taking the $m^{th}$ root could be useful which we can do since $B$ has positive entries, but I'm not sure. Many thanks!",,"['linear-algebra', 'matrices']"
86,Schur decomposition of a matrix with distinct eigenvalues is almost unique,Schur decomposition of a matrix with distinct eigenvalues is almost unique,,"Let $M\in \mathbb C^{n,n}$ have $n$ distinct eigenvalues, and let $U_1, U_2$ be two Schur-forms of $M$. Show that if $U_1, U_2$ have equal diagonals, there is a hermitian diagonal matrix $Q$ such that $U_1=QU_2Q^H.$ My attempt: we have $M=Q_1U_1Q_1^H$ and $M=Q_2U_2Q_2^H$ for some hermitian $Q_1, Q_2$. So $$U_1 = \underbrace{Q_1^HQ_2}_{=:Q}U_2Q_2^HQ_1.$$ $Q$ is clearly hermitian but how do we show that it's diagonal? I was hoping to show that it's upper-triangular and then it would follow. Yet how do we use the setup? $U_1, U_2$ have equal diagonals, so $U_1 = D+N_1, U_2=D+N_2$ (for a diagonal $D$ and nilpotent $N_1, N_2$). From the distinctness of the eigenvalues follows that $M$ is diagonalizable but I don't quite see how to use it either. Any hints are hugely appreciated.","Let $M\in \mathbb C^{n,n}$ have $n$ distinct eigenvalues, and let $U_1, U_2$ be two Schur-forms of $M$. Show that if $U_1, U_2$ have equal diagonals, there is a hermitian diagonal matrix $Q$ such that $U_1=QU_2Q^H.$ My attempt: we have $M=Q_1U_1Q_1^H$ and $M=Q_2U_2Q_2^H$ for some hermitian $Q_1, Q_2$. So $$U_1 = \underbrace{Q_1^HQ_2}_{=:Q}U_2Q_2^HQ_1.$$ $Q$ is clearly hermitian but how do we show that it's diagonal? I was hoping to show that it's upper-triangular and then it would follow. Yet how do we use the setup? $U_1, U_2$ have equal diagonals, so $U_1 = D+N_1, U_2=D+N_2$ (for a diagonal $D$ and nilpotent $N_1, N_2$). From the distinctness of the eigenvalues follows that $M$ is diagonalizable but I don't quite see how to use it either. Any hints are hugely appreciated.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization', 'adjoint-operators']"
87,Constrained Optimizatoin: The Frank-Wolfe Method,Constrained Optimizatoin: The Frank-Wolfe Method,,A general convex optimization problem is framed as such: $$\min f(x) : x \in \Omega$$ where $\Omega$ is convex. The Frank-Wolfe method seeks a feasible descent direction $d_k$ (i.e. $x_k + d_k \in \Omega$) such that $\nabla(f_k)^Td_k < 0$ So I'm solving the subproblem: $$\min \nabla(f_k)^T(x-x_k) : x \in X$$ and in this instance am given the closed form solution: $$X = \left\{x:x \ge0 \sum x_i=b\right\}$$ where $b > 0$ The problem is to find (given an $x_k$) an explicit solution for $d_k$ to the subproblem. What I have done so far: Determined that $\sum d_i = 0$ since $x_k$ and $x_{k+1}$ must satisfy $\sum x_i=b$ Determined that $d_i \ge -x_i$ so that $x \ge0$ is satisfied at each iteration I know that if this were an unconstrained optimization problem the solution would be the steepest descent direction: $d_k = -\nabla f_k$ (I realize this is probably irrelevant) Any advice on how to solve this problem for the descent direction $d_k$ would be greatly appreciated. Thanks in advance!,A general convex optimization problem is framed as such: $$\min f(x) : x \in \Omega$$ where $\Omega$ is convex. The Frank-Wolfe method seeks a feasible descent direction $d_k$ (i.e. $x_k + d_k \in \Omega$) such that $\nabla(f_k)^Td_k < 0$ So I'm solving the subproblem: $$\min \nabla(f_k)^T(x-x_k) : x \in X$$ and in this instance am given the closed form solution: $$X = \left\{x:x \ge0 \sum x_i=b\right\}$$ where $b > 0$ The problem is to find (given an $x_k$) an explicit solution for $d_k$ to the subproblem. What I have done so far: Determined that $\sum d_i = 0$ since $x_k$ and $x_{k+1}$ must satisfy $\sum x_i=b$ Determined that $d_i \ge -x_i$ so that $x \ge0$ is satisfied at each iteration I know that if this were an unconstrained optimization problem the solution would be the steepest descent direction: $d_k = -\nabla f_k$ (I realize this is probably irrelevant) Any advice on how to solve this problem for the descent direction $d_k$ would be greatly appreciated. Thanks in advance!,,"['linear-algebra', 'optimization']"
88,Prove that the largest singular value of a matrix is greater than the largest eigenvalue,Prove that the largest singular value of a matrix is greater than the largest eigenvalue,,"Let $\sigma_1$ be the largest singular value of the matrix $A = (a_{ij})$.  Show that $\sigma_1 >= \lambda_{max}$, where $\lambda_{max}$ denotes the largest eigenvalue of $A$, and that $\sigma_1 \geq |a_{ij}|_{max}$. $A$ must be a square matrix, otherwise it would not have any eigenvalues.  One of the problems that I have worked so far includes a square matrix, and the above statement holds true for that problem. I am unsure of how to approach proving this for the general case however.  I know that $\sigma_1 = \sqrt{\lambda_{max}(A^TA)}$. Is there a relationship between $\lambda_{max}(A^TA)$ and $\lambda_{max}(A)$ that can be leveraged to prove this?","Let $\sigma_1$ be the largest singular value of the matrix $A = (a_{ij})$.  Show that $\sigma_1 >= \lambda_{max}$, where $\lambda_{max}$ denotes the largest eigenvalue of $A$, and that $\sigma_1 \geq |a_{ij}|_{max}$. $A$ must be a square matrix, otherwise it would not have any eigenvalues.  One of the problems that I have worked so far includes a square matrix, and the above statement holds true for that problem. I am unsure of how to approach proving this for the general case however.  I know that $\sigma_1 = \sqrt{\lambda_{max}(A^TA)}$. Is there a relationship between $\lambda_{max}(A^TA)$ and $\lambda_{max}(A)$ that can be leveraged to prove this?",,['linear-algebra']
89,Can the number of sign changes in a sequence of determinants tell us how many negative eigenvalues a symmetric matrix has?,Can the number of sign changes in a sequence of determinants tell us how many negative eigenvalues a symmetric matrix has?,,"From notes, I've gathered that given a symmetric matrix, the number of sign changes in its characteristic polynomial is equal to the number of positive eigenvalues of $A$. Proof: Let $p(x)$ be a real polynomial whose roots are all real. By Descarte’s rule, the number $\sigma$ of positive eigenvalues is bounded by the number of sign changes in $p(x)$. Similarly, the number $\sigma'$ of negative eigenvalues is bounded by the number of sign changes in $p(−x)$. Hence the total number of positive and negative eigenvalues is bounded by $\sigma+\sigma'$ Now  $\sigma + \sigma' \leq n$ and the fact that all eigenvalues of a symmetric real matrix are real imply that the bound of Descarte’s rule of signs holds with equality. How can I use this proof to show that the sign changes in a certain sequence of determinants tells us how many negative eigenvalues $A$ has?","From notes, I've gathered that given a symmetric matrix, the number of sign changes in its characteristic polynomial is equal to the number of positive eigenvalues of $A$. Proof: Let $p(x)$ be a real polynomial whose roots are all real. By Descarte’s rule, the number $\sigma$ of positive eigenvalues is bounded by the number of sign changes in $p(x)$. Similarly, the number $\sigma'$ of negative eigenvalues is bounded by the number of sign changes in $p(−x)$. Hence the total number of positive and negative eigenvalues is bounded by $\sigma+\sigma'$ Now  $\sigma + \sigma' \leq n$ and the fact that all eigenvalues of a symmetric real matrix are real imply that the bound of Descarte’s rule of signs holds with equality. How can I use this proof to show that the sign changes in a certain sequence of determinants tells us how many negative eigenvalues $A$ has?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
90,"If A is positive definite, can we prove that x>0 and Ax>0 always have a feasible solution?","If A is positive definite, can we prove that x>0 and Ax>0 always have a feasible solution?",,"I'm new here and couldn't find a similar question, so pardon me if it's already asked elsewhere. The question is literally simple: Suppose A is a positive definite matrix , could it be generally proved that the set $S=\left \{ x\in R^{n} |x>0,Ax>0 \right \}$ is nonempty? The inequalities are strict and by $x>0$, I mean $x_{i}>0$ for $i=1,2,...,n$. Honestly speaking, I haven't had any mentionable progress on the analytical side, except trying different types of decompostions (Cholesky, Spectral, etc ...) to no avail. so I tried solving this linear program for different choices of $A\succ 0$ using Matlab so that I could find a counterexample, without success. $$min w=\mathbf{1}^Ty+\mathbf{1}^Tz$$ $$s.t. \left\{\begin{matrix}x=\mathbf{1}+y\\Ax=\mathbf{1}+z \\ x,y,z\geq 0\end{matrix}\right.$$ where $\mathbf{1}=(1,1,...,1)^{T}$ I'd really appreciate if you would guide me on how could it be proven or negated.","I'm new here and couldn't find a similar question, so pardon me if it's already asked elsewhere. The question is literally simple: Suppose A is a positive definite matrix , could it be generally proved that the set $S=\left \{ x\in R^{n} |x>0,Ax>0 \right \}$ is nonempty? The inequalities are strict and by $x>0$, I mean $x_{i}>0$ for $i=1,2,...,n$. Honestly speaking, I haven't had any mentionable progress on the analytical side, except trying different types of decompostions (Cholesky, Spectral, etc ...) to no avail. so I tried solving this linear program for different choices of $A\succ 0$ using Matlab so that I could find a counterexample, without success. $$min w=\mathbf{1}^Ty+\mathbf{1}^Tz$$ $$s.t. \left\{\begin{matrix}x=\mathbf{1}+y\\Ax=\mathbf{1}+z \\ x,y,z\geq 0\end{matrix}\right.$$ where $\mathbf{1}=(1,1,...,1)^{T}$ I'd really appreciate if you would guide me on how could it be proven or negated.",,['linear-algebra']
91,set of almost complex structures on $\mathbb R^4$ as two disjoint spheres,set of almost complex structures on  as two disjoint spheres,\mathbb R^4,"The set of almost complex structures on $\mathbb R^{2n}$ is given by $$ M_n = \frac{GL(2n,\mathbb R)}{GL(n,\mathbb C)} = \mathcal C_+ \sqcup \mathcal C_-,$$ taking into account that $\det = \pm 1$ gives two disjoint sets. How do we show that $M_2=S^2 \sqcup S^2$? Moreover, I'm a bit confused by dimensions: how does $\dim_{\mathbb R}M_2=8$ relates with the latter decomposition? This is exercise 1.2.1 from the book Complex geometry by Huybrechts.","The set of almost complex structures on $\mathbb R^{2n}$ is given by $$ M_n = \frac{GL(2n,\mathbb R)}{GL(n,\mathbb C)} = \mathcal C_+ \sqcup \mathcal C_-,$$ taking into account that $\det = \pm 1$ gives two disjoint sets. How do we show that $M_2=S^2 \sqcup S^2$? Moreover, I'm a bit confused by dimensions: how does $\dim_{\mathbb R}M_2=8$ relates with the latter decomposition? This is exercise 1.2.1 from the book Complex geometry by Huybrechts.",,"['linear-algebra', 'differential-geometry', 'lie-groups', 'complex-geometry']"
92,Proof for a rank-one Decomposition theorem,Proof for a rank-one Decomposition theorem,,"Consider the following result which I recently came across in a research paper in my area (Signal Processing) Let $X$ be a $N\times N$ positive semidefinite (psd) matrix whose rank   is $r$. Let $A$ be any symmetric $N\times N$ matrix. Then, there exist   a set of vectors $x_1,\dots,x_r$ such that \begin{align} X & =  \sum_{i=1}^{r}x_ix_i^T \\ x_i^TAx_i &=  \frac{\mbox{trace}\{AX\}}{r},~~~\forall i \end{align} The following is the proof for it which I can't verify. Proof: Consider the following step-wise procedure whose inputs are $X$ and $A$. $~~$0.$~~$ Inputs are $X$ (given $X\geq 0$, $rank(X)=r$) and $A$ (symmetric). Decompose $X=RR^T$. Generate the eigen decomposition $R^TAR=U\Lambda U^T$. Let $h$ be any $N\times 1$ vector such that $\lvert h_i\rvert=1$ (each entry of $h$). Generate the vector $x_1$ and matrix $X_1$ as  \begin{align} x_1&=\frac{1}{\sqrt{r}}RUh \\ X_1&=X-x_1x_1^T  \end{align} Outputs are $X_1$ and $x_1$. The paper then claims that $X_1$ is psd and has rank $r-1$ $x_1^TAx_1=\frac{1}{r}trace(AX)$ While I am able to verify the second claim, am not able to verify the first one? How is it true? If this can be done, the rest of the proof is straight forward. I am looking for a rigorous proof. Read this if you are interested to know where this proof heads. Now do the stepwise algorithm earlier with inputs $X_1$ and $A$ to get $x_2$ and $X_2$ such that \begin{align}X_2&=X_1-x_2x_2^T\\&=X-x_1x_1^T-x_2x_2^T\end{align} and $$x_2^TAx_2=\frac{1}{r-1}trace(AX_1)=\frac{1}{r}trace(AX)$$ Then the result of the paper is that you can do this procedure $r$ times and get a rank-one decomposition $$X=\sum_{i=1}^{r}x_ix_i^T$$ with the property $$x_i^TAx_i\,=\,\frac{1}{r}trace(AX),~\forall i$$for any given psd X with rank $r$ and any symmetrix $A$.","Consider the following result which I recently came across in a research paper in my area (Signal Processing) Let $X$ be a $N\times N$ positive semidefinite (psd) matrix whose rank   is $r$. Let $A$ be any symmetric $N\times N$ matrix. Then, there exist   a set of vectors $x_1,\dots,x_r$ such that \begin{align} X & =  \sum_{i=1}^{r}x_ix_i^T \\ x_i^TAx_i &=  \frac{\mbox{trace}\{AX\}}{r},~~~\forall i \end{align} The following is the proof for it which I can't verify. Proof: Consider the following step-wise procedure whose inputs are $X$ and $A$. $~~$0.$~~$ Inputs are $X$ (given $X\geq 0$, $rank(X)=r$) and $A$ (symmetric). Decompose $X=RR^T$. Generate the eigen decomposition $R^TAR=U\Lambda U^T$. Let $h$ be any $N\times 1$ vector such that $\lvert h_i\rvert=1$ (each entry of $h$). Generate the vector $x_1$ and matrix $X_1$ as  \begin{align} x_1&=\frac{1}{\sqrt{r}}RUh \\ X_1&=X-x_1x_1^T  \end{align} Outputs are $X_1$ and $x_1$. The paper then claims that $X_1$ is psd and has rank $r-1$ $x_1^TAx_1=\frac{1}{r}trace(AX)$ While I am able to verify the second claim, am not able to verify the first one? How is it true? If this can be done, the rest of the proof is straight forward. I am looking for a rigorous proof. Read this if you are interested to know where this proof heads. Now do the stepwise algorithm earlier with inputs $X_1$ and $A$ to get $x_2$ and $X_2$ such that \begin{align}X_2&=X_1-x_2x_2^T\\&=X-x_1x_1^T-x_2x_2^T\end{align} and $$x_2^TAx_2=\frac{1}{r-1}trace(AX_1)=\frac{1}{r}trace(AX)$$ Then the result of the paper is that you can do this procedure $r$ times and get a rank-one decomposition $$X=\sum_{i=1}^{r}x_ix_i^T$$ with the property $$x_i^TAx_i\,=\,\frac{1}{r}trace(AX),~\forall i$$for any given psd X with rank $r$ and any symmetrix $A$.",,"['linear-algebra', 'matrices']"
93,Index notation for inverse matrices,Index notation for inverse matrices,,I have a question: There is an standard way to write the inverse of a matrix in index notation?. The reason is that I don't want to write $(A^{-1})_{ij}$  or $(A^{-1})_i^j$ or $(A^{-1})^{ij}$ using the exponent $^{-1}$. I working with a one-covariariant one-contravariant tensor with components $A_i^j$ and I only need to write the expression $y^i (A^{-1})^j_i=x^j$ without the exponent $^{-1}$.,I have a question: There is an standard way to write the inverse of a matrix in index notation?. The reason is that I don't want to write $(A^{-1})_{ij}$  or $(A^{-1})_i^j$ or $(A^{-1})^{ij}$ using the exponent $^{-1}$. I working with a one-covariariant one-contravariant tensor with components $A_i^j$ and I only need to write the expression $y^i (A^{-1})^j_i=x^j$ without the exponent $^{-1}$.,,"['linear-algebra', 'notation', 'tensors']"
94,Is the Steinitz exchange lemma necessary to establish invariance of 'basis-size'?,Is the Steinitz exchange lemma necessary to establish invariance of 'basis-size'?,,"I am going to answer my own question in some sense... In Beardon's ""Algebra and Geometry"" he proves ( Theorem 7.2.2 ) that if $v_1,\ldots,v_n$ and $u_1,\ldots,u_m$ are both bases for some $F$-vector space $V$, then $n=m$. He does so relying (essentially) only on commutativity in $F$. This seems strange as almost all other books on the subject stress the importance of the Steinitz Exchange Lemma in establishing the notion of dimension. So my question this: Why do they all bother with/make a fuss about Steinitz Exchange? Many thanks! Here is a sketch of Beardon's proof: Write $u_1$ in terms of the $v_j$ and write each $v_j$ in terms of the $u_i$.  Then by equating coefficients we obtain something like $1=\sum_{j=1}^n\lambda_{jk}\mu_{kj}$ for each $k=1,\ldots,m$ so by summing over $k$ we obtain $$m=\sum_{k=1}^m\sum_{j=1}^n\lambda_{jk}\mu_{kj}.$$ But by symmetry we also have $$n=\sum_{k=1}^n\sum_{j=1}^m\lambda_{jk}\mu_{kj}.$$ The result follows.","I am going to answer my own question in some sense... In Beardon's ""Algebra and Geometry"" he proves ( Theorem 7.2.2 ) that if $v_1,\ldots,v_n$ and $u_1,\ldots,u_m$ are both bases for some $F$-vector space $V$, then $n=m$. He does so relying (essentially) only on commutativity in $F$. This seems strange as almost all other books on the subject stress the importance of the Steinitz Exchange Lemma in establishing the notion of dimension. So my question this: Why do they all bother with/make a fuss about Steinitz Exchange? Many thanks! Here is a sketch of Beardon's proof: Write $u_1$ in terms of the $v_j$ and write each $v_j$ in terms of the $u_i$.  Then by equating coefficients we obtain something like $1=\sum_{j=1}^n\lambda_{jk}\mu_{kj}$ for each $k=1,\ldots,m$ so by summing over $k$ we obtain $$m=\sum_{k=1}^m\sum_{j=1}^n\lambda_{jk}\mu_{kj}.$$ But by symmetry we also have $$n=\sum_{k=1}^n\sum_{j=1}^m\lambda_{jk}\mu_{kj}.$$ The result follows.",,"['linear-algebra', 'vector-spaces']"
95,"Expressing a quadratic form, $\mathbf{x}^TA\mathbf{x}$ in terms of $\lVert\mathbf{x}\rVert^2$, $A$","Expressing a quadratic form,  in terms of ,",\mathbf{x}^TA\mathbf{x} \lVert\mathbf{x}\rVert^2 A,"EDIT: This question is actually an attempt to solve this . Please take a look. Let $A$ be a symmetric postive-definite $n\times n$ matrix, i.e. $A\in\mathbb{S}_{++}^{n}$ Also, let $\mathbf{x}\in\mathbb{R}^n$. Let $Q\colon\mathbf{R}^n\to\mathbb{R}^{*}_{+}$ be the following quadratic form $$ Q(\mathbf{x})=\mathbf{x}^TA\mathbf{x}. $$ If we apply  SVD (Singular Value Decomposition) on $A$, we have $$ A=P\Lambda P^T, $$ where $P$ is an orthogonal matrix, and $\Lambda=\operatorname{diag}\{\lambda_1,\ldots,\lambda_n\}$ is the diagonal matrix of the (positive) eigenvalues of $A$, $\lambda_i>0$, $i=1,\ldots,n$. I would like to express the above quadratic form, $Q(\mathbf{x})$, in terms of the $2$-norm of $\mathbf{x}$, as well as the matrix $A$ (in some way, for instance in terms of the $2$-norm of $\Lambda$, or something else). What I have thought so far is as follows: $$ Q(\mathbf{x}) = \mathbf{x}^TA\mathbf{x} = \mathbf{x}^T P \Lambda P^T \mathbf{x} = \Big(\mathbf{x}^T P \Lambda^{\frac{1}{2}}\Big)\Big(\Lambda^{\frac{1}{2}} P^T \mathbf{x}\Big) = \Big(\big(P \Lambda^{\frac{1}{2}}\big)^T\mathbf{x}\Big)^T \Big(\Lambda^{\frac{1}{2}} P^T \mathbf{x}\Big). $$ Now, if we set $\mathbf{x}_a=\big(P \Lambda^{\frac{1}{2}}\big)^T\mathbf{x}\in\mathbf{R}^n$, then the quadratic can be rewritten as $$ Q(\mathbf{x}) = \mathbf{x}_a^T\mathbf{x}_a = \big\lVert \mathbf{x}_a \big\rVert^2_2 = \Big\lVert \big(P \Lambda^{\frac{1}{2}}\big)^T\mathbf{x} \Big\rVert^2_2. $$ As far as I know (thanks to @DanielFischer - if I do not misunderstand his words), the following holds true $$ Q(\mathbf{x}) = \Big\lVert \big(P \Lambda^{\frac{1}{2}}\big)^T\mathbf{x} \Big\rVert^2_2 \leq \Big\lVert \big(P \Lambda^{\frac{1}{2}}\big)^T \Big\rVert^2_2 \Big\lVert \mathbf{x} \Big\rVert^2_2. $$ My question is: (a) Are all the above correct? (b) Is there any way of getting rid of the inequality, granted that $A$ is symmetric and positive-definite? Moreover, could we define a function $f\colon\mathbb{R}^n\times\mathbb{S}_{++}^{n}\to\mathbb{R}$, such that $f(\mathbf{x},A)=Q(\mathbf{x})$, where $f$ is expressed in terms of the $2$-norm of $\mathbf{x}$, as well as in terms of $A$ in some way (for instance, in terms of $\Lambda$, etc.)? (c) Any other suggestions? Thanks in advance!","EDIT: This question is actually an attempt to solve this . Please take a look. Let $A$ be a symmetric postive-definite $n\times n$ matrix, i.e. $A\in\mathbb{S}_{++}^{n}$ Also, let $\mathbf{x}\in\mathbb{R}^n$. Let $Q\colon\mathbf{R}^n\to\mathbb{R}^{*}_{+}$ be the following quadratic form $$ Q(\mathbf{x})=\mathbf{x}^TA\mathbf{x}. $$ If we apply  SVD (Singular Value Decomposition) on $A$, we have $$ A=P\Lambda P^T, $$ where $P$ is an orthogonal matrix, and $\Lambda=\operatorname{diag}\{\lambda_1,\ldots,\lambda_n\}$ is the diagonal matrix of the (positive) eigenvalues of $A$, $\lambda_i>0$, $i=1,\ldots,n$. I would like to express the above quadratic form, $Q(\mathbf{x})$, in terms of the $2$-norm of $\mathbf{x}$, as well as the matrix $A$ (in some way, for instance in terms of the $2$-norm of $\Lambda$, or something else). What I have thought so far is as follows: $$ Q(\mathbf{x}) = \mathbf{x}^TA\mathbf{x} = \mathbf{x}^T P \Lambda P^T \mathbf{x} = \Big(\mathbf{x}^T P \Lambda^{\frac{1}{2}}\Big)\Big(\Lambda^{\frac{1}{2}} P^T \mathbf{x}\Big) = \Big(\big(P \Lambda^{\frac{1}{2}}\big)^T\mathbf{x}\Big)^T \Big(\Lambda^{\frac{1}{2}} P^T \mathbf{x}\Big). $$ Now, if we set $\mathbf{x}_a=\big(P \Lambda^{\frac{1}{2}}\big)^T\mathbf{x}\in\mathbf{R}^n$, then the quadratic can be rewritten as $$ Q(\mathbf{x}) = \mathbf{x}_a^T\mathbf{x}_a = \big\lVert \mathbf{x}_a \big\rVert^2_2 = \Big\lVert \big(P \Lambda^{\frac{1}{2}}\big)^T\mathbf{x} \Big\rVert^2_2. $$ As far as I know (thanks to @DanielFischer - if I do not misunderstand his words), the following holds true $$ Q(\mathbf{x}) = \Big\lVert \big(P \Lambda^{\frac{1}{2}}\big)^T\mathbf{x} \Big\rVert^2_2 \leq \Big\lVert \big(P \Lambda^{\frac{1}{2}}\big)^T \Big\rVert^2_2 \Big\lVert \mathbf{x} \Big\rVert^2_2. $$ My question is: (a) Are all the above correct? (b) Is there any way of getting rid of the inequality, granted that $A$ is symmetric and positive-definite? Moreover, could we define a function $f\colon\mathbb{R}^n\times\mathbb{S}_{++}^{n}\to\mathbb{R}$, such that $f(\mathbf{x},A)=Q(\mathbf{x})$, where $f$ is expressed in terms of the $2$-norm of $\mathbf{x}$, as well as in terms of $A$ in some way (for instance, in terms of $\Lambda$, etc.)? (c) Any other suggestions? Thanks in advance!",,"['linear-algebra', 'quadratic-forms', 'matrix-decomposition']"
96,What good is the Commutator product?,What good is the Commutator product?,,"In geometric algebra, the commutator product is defined as $A \times B = \frac 1 2 (AB - BA)$. From linear algebra, I remember that the commutator of matrices is $[A, B] = AB - BA$ and the commutator of linear functions is likewise $[f,g]=fg-gf$.  Looking at my notes I see that skew transformations are closed under this operation.  But why is that important? Why do we need this product/ operation?  Can I use it to prove some cool theorems or is it useful in performing some algorithms?  What is the point of the commutator?","In geometric algebra, the commutator product is defined as $A \times B = \frac 1 2 (AB - BA)$. From linear algebra, I remember that the commutator of matrices is $[A, B] = AB - BA$ and the commutator of linear functions is likewise $[f,g]=fg-gf$.  Looking at my notes I see that skew transformations are closed under this operation.  But why is that important? Why do we need this product/ operation?  Can I use it to prove some cool theorems or is it useful in performing some algorithms?  What is the point of the commutator?",,['linear-algebra']
97,$M_n(\mathbb{Z}) = SL_n(\mathbb{Z}) + SL_n(\mathbb{Z})$,,M_n(\mathbb{Z}) = SL_n(\mathbb{Z}) + SL_n(\mathbb{Z}),"I came across the following problem and I'm stuck. Let $n>1$ be an even integer, and $A\in \mathcal{M}_n(\mathbb{Z})$. Show that there exist $B,C \in \mathrm{SL}_n(\mathbb{Z})$, such that  $A=B+C$. Any hint is appreciated.","I came across the following problem and I'm stuck. Let $n>1$ be an even integer, and $A\in \mathcal{M}_n(\mathbb{Z})$. Show that there exist $B,C \in \mathrm{SL}_n(\mathbb{Z})$, such that  $A=B+C$. Any hint is appreciated.",,['linear-algebra']
98,${\rm rank}(BA)={\rm rank}(B)$ if $A \in \mathbb{R}^{n \times n}$ is invertible?,if  is invertible?,{\rm rank}(BA)={\rm rank}(B) A \in \mathbb{R}^{n \times n},"I'm having some trouble with the following question: Let $A, B \in \mathbb{R}^{n \times n}$ and let $A$ be invertible. Is it true that in this case $rank(BA)=rank(B)$? I think that this statement is correct, but I'm unable to prove it. My thoughts so far: If $B$ is also invertible the statement clearly holds, since $GL_n(\mathbb{R})$ is a group. For $B$ not invertible we immediately have the inequality $rank(BA) \leq rank(B)$ because the columns of $BA$ are linear combinations of the columns of $B$. Now I've tried to prove the other inequality by contradiction, i.e. assuming that $rank(BA)<rank(B)$ and showing that this cannot be. But I can't complete this step. Thanks in advance for any help!","I'm having some trouble with the following question: Let $A, B \in \mathbb{R}^{n \times n}$ and let $A$ be invertible. Is it true that in this case $rank(BA)=rank(B)$? I think that this statement is correct, but I'm unable to prove it. My thoughts so far: If $B$ is also invertible the statement clearly holds, since $GL_n(\mathbb{R})$ is a group. For $B$ not invertible we immediately have the inequality $rank(BA) \leq rank(B)$ because the columns of $BA$ are linear combinations of the columns of $B$. Now I've tried to prove the other inequality by contradiction, i.e. assuming that $rank(BA)<rank(B)$ and showing that this cannot be. But I can't complete this step. Thanks in advance for any help!",,"['linear-algebra', 'matrices', 'matrix-rank']"
99,Projection onto the column space of an orthogonal matrix,Projection onto the column space of an orthogonal matrix,,"The projection of a vector $v$ onto the column space of A is $$A(A^T A)^{-1}A^T v$$ If the columns of $A$ are orthogonal, does the projection just become $A^Tv$? I think it should because geometrically you just want to take the dot product with each of the columns of $A$. But how can I show this is true?","The projection of a vector $v$ onto the column space of A is $$A(A^T A)^{-1}A^T v$$ If the columns of $A$ are orthogonal, does the projection just become $A^Tv$? I think it should because geometrically you just want to take the dot product with each of the columns of $A$. But how can I show this is true?",,"['linear-algebra', 'matrices']"
