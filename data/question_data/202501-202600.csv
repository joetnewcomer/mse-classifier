,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,About Affine planes,About Affine planes,,"I am studying about affine planes An affine plane can be defined as It is an ordered pair ($\mathcal{P}$ ,$\mathcal{L}$), P is non-empty set of points and and L is non-empty collection of the subsets of $\mathcal{P}$ called lines satisfying the following properties Given any two points, there is a unique line joining any two points. Given a point P and a line L not containing P, there is a unique line that contains P and does not intersect L. There are four points, no three of which are collinear. (This rule is just to eliminate the silly case where all of your points are on the same line.) Now what is the problem and what I am thinking about affine plane is the example of Rational affine plane where $$\mathcal{P}=\{(x,y)/x,y\in Q\}$$ and $$\mathcal{L}=\{(x,y)/ ax+by=c\}$$ But in this case the each line in rational affine plane seems to dots. means line is discontineous. Similalry in case of the finite affine planes I am thinking. Can anyone help to remove my this confusion. Thanks in advance","I am studying about affine planes An affine plane can be defined as It is an ordered pair ($\mathcal{P}$ ,$\mathcal{L}$), P is non-empty set of points and and L is non-empty collection of the subsets of $\mathcal{P}$ called lines satisfying the following properties Given any two points, there is a unique line joining any two points. Given a point P and a line L not containing P, there is a unique line that contains P and does not intersect L. There are four points, no three of which are collinear. (This rule is just to eliminate the silly case where all of your points are on the same line.) Now what is the problem and what I am thinking about affine plane is the example of Rational affine plane where $$\mathcal{P}=\{(x,y)/x,y\in Q\}$$ and $$\mathcal{L}=\{(x,y)/ ax+by=c\}$$ But in this case the each line in rational affine plane seems to dots. means line is discontineous. Similalry in case of the finite affine planes I am thinking. Can anyone help to remove my this confusion. Thanks in advance",,"['differential-geometry', 'euclidean-geometry', 'affine-geometry']"
1,The open disk and closed disk are a regular surfaces,The open disk and closed disk are a regular surfaces,,"this is the exercice 2-2 of Do Carmo's page 67 1- is the set $\{(x,y,z)\in \mathbb{R}^3 ; \;z=0 \text{ and } \;x^2+y^2\leq 1 \}$ a regular surface ? 2-is the set $\{(x,y,z)\in \mathbb{R}^3 ; \;z=0 \text{ and }\;x^2+y^2<1 \}$ a regular surface ? my answer 1- the closed disk is not a regular surface,if it's the case the closed disk wich is compacte in $\mathbb{R}^3 $ is homeomorphic to an open set of $\mathbb{R}^2$ and this is contradiction 2- the open disk is a regular surface because the  paramtrisation $X$from the open uniatry disk from $\mathbb{R}^2$ to the open disk in $\mathbb{R}^3 $ verify the definition of a regular sufraces $(x(u,v)=(u,v,0))$ For the second i 'am sur but for my first answer i have some doubt what do you think ?","this is the exercice 2-2 of Do Carmo's page 67 1- is the set $\{(x,y,z)\in \mathbb{R}^3 ; \;z=0 \text{ and } \;x^2+y^2\leq 1 \}$ a regular surface ? 2-is the set $\{(x,y,z)\in \mathbb{R}^3 ; \;z=0 \text{ and }\;x^2+y^2<1 \}$ a regular surface ? my answer 1- the closed disk is not a regular surface,if it's the case the closed disk wich is compacte in $\mathbb{R}^3 $ is homeomorphic to an open set of $\mathbb{R}^2$ and this is contradiction 2- the open disk is a regular surface because the  paramtrisation $X$from the open uniatry disk from $\mathbb{R}^2$ to the open disk in $\mathbb{R}^3 $ verify the definition of a regular sufraces $(x(u,v)=(u,v,0))$ For the second i 'am sur but for my first answer i have some doubt what do you think ?",,['differential-geometry']
2,Ad-invariant inner product on Lie algebra of compact Lie group.,Ad-invariant inner product on Lie algebra of compact Lie group.,,"I was reading about representations of compact Lie groups and came upon a certain construction of an $Ad$-invariant inner product on the Lie algebra of a compact Lie group. I'm having trouble showing that it is actually Ad-invariant. Here's my thinking so far: Let $G$ be a compact Lie group with Lie algebra $\mathfrak{g}$, and let $|dg|$ be a left-invariant (hence bi-invariant) smooth density on $G$. The claim is that $$B(x,y) := \frac{1}{\text{vol}(G)} \int_G B' (\text{Ad}_g (x), \text{Ad}_g(y))  \ |dg|$$ is an Ad-invariant inner product on $\mathfrak{g}$ (where $B': \mathfrak{g}\times \mathfrak{g}\to \mathbb{R}$ is an arbitrary inner product). Fixing $x,y\in \mathfrak{g}$ and letting $ \ \  f:G\to \mathbb{R}$, $g\mapsto  B'(\text{Ad}_g(x),\text{Ad}_g(y))$, we compute: \begin{align*} B(\text{Ad}_h (x), \text{Ad}_h(y)) &= \frac{1}{\text{vol}(G)} \int_G B'(\text{Ad}_g\text{Ad}_h (x), \text{Ad}_g\text{Ad}_h (y)) \ |dg| \\ &=  \frac{1}{\text{vol}(G)} \int_G B'(\text{Ad}_{gh} (x), \text{Ad}_{gh} (y)) \ |dg| \\ &= \frac{1}{\text{vol}(G)} \int_G f(gh) \ |dg| \\ \end{align*} So, I need to show that $\int_G f(gh) \ |dg| = \int_G f(g) \ |dg|$ for all $h\in G$ $(\star)$. Intuitively this is clear, since $|dg|$ is bi-invariant, and right multiplication by $h$ inside the argument of $f$ just ""shuffles"" the values of $f$ around to different points on $G$. However I can't seem to find an algebraic justification for why this should be true. Here is a thought I had: Let $\chi:G\to \mathbb{R}^*$, $g\mapsto |\det(\text{Ad}_g)|$ be the so-called ""modular function"" of $G$. I know that any left-invariant smooth density $\mu$ satisfies $$(R_h)^* \mu= \frac{1}{\chi(h^{-1})}\  \mu$$ Since $G$ is compact we have $\chi\equiv 1$, hence $(R_h)^* \mu= \mu$. Thus, to prove $(\star)$, it suffices to show that the density $f |dg|$ is left-invariant. However, I wasn't able to show this (and I'm not entirely convinced whether it's even true). Any suggestions would be greatly appreciated!","I was reading about representations of compact Lie groups and came upon a certain construction of an $Ad$-invariant inner product on the Lie algebra of a compact Lie group. I'm having trouble showing that it is actually Ad-invariant. Here's my thinking so far: Let $G$ be a compact Lie group with Lie algebra $\mathfrak{g}$, and let $|dg|$ be a left-invariant (hence bi-invariant) smooth density on $G$. The claim is that $$B(x,y) := \frac{1}{\text{vol}(G)} \int_G B' (\text{Ad}_g (x), \text{Ad}_g(y))  \ |dg|$$ is an Ad-invariant inner product on $\mathfrak{g}$ (where $B': \mathfrak{g}\times \mathfrak{g}\to \mathbb{R}$ is an arbitrary inner product). Fixing $x,y\in \mathfrak{g}$ and letting $ \ \  f:G\to \mathbb{R}$, $g\mapsto  B'(\text{Ad}_g(x),\text{Ad}_g(y))$, we compute: \begin{align*} B(\text{Ad}_h (x), \text{Ad}_h(y)) &= \frac{1}{\text{vol}(G)} \int_G B'(\text{Ad}_g\text{Ad}_h (x), \text{Ad}_g\text{Ad}_h (y)) \ |dg| \\ &=  \frac{1}{\text{vol}(G)} \int_G B'(\text{Ad}_{gh} (x), \text{Ad}_{gh} (y)) \ |dg| \\ &= \frac{1}{\text{vol}(G)} \int_G f(gh) \ |dg| \\ \end{align*} So, I need to show that $\int_G f(gh) \ |dg| = \int_G f(g) \ |dg|$ for all $h\in G$ $(\star)$. Intuitively this is clear, since $|dg|$ is bi-invariant, and right multiplication by $h$ inside the argument of $f$ just ""shuffles"" the values of $f$ around to different points on $G$. However I can't seem to find an algebraic justification for why this should be true. Here is a thought I had: Let $\chi:G\to \mathbb{R}^*$, $g\mapsto |\det(\text{Ad}_g)|$ be the so-called ""modular function"" of $G$. I know that any left-invariant smooth density $\mu$ satisfies $$(R_h)^* \mu= \frac{1}{\chi(h^{-1})}\  \mu$$ Since $G$ is compact we have $\chi\equiv 1$, hence $(R_h)^* \mu= \mu$. Thus, to prove $(\star)$, it suffices to show that the density $f |dg|$ is left-invariant. However, I wasn't able to show this (and I'm not entirely convinced whether it's even true). Any suggestions would be greatly appreciated!",,"['differential-geometry', 'representation-theory', 'lie-groups']"
3,Multivariable Substitution Rule VS Pullback of Volume Forms,Multivariable Substitution Rule VS Pullback of Volume Forms,,"Let $d\mathbf{x} = dx^1\wedge \cdots \wedge dx^n$ be the volume form on $U\subset\mathbb{R}^n$ and smooth $c : [a_1,b_1]\times\cdots\times [a_n,b_n] \to U$, $\mathbf{t} = (t_1,\dots,t_n) \mapsto \mathbf{x} = \mathbf{c}(\mathbf{t}) = (c^1(t_1,\dots,t_n),\dots,c^n(t_1,\dots,t_n))$. I’ve encountered the following two formulae: Multivariable Substitution Rule :  $$d\mathbf{x} = d\mathbf{c} = \vert det\left((D\mathbf{c})_\mathbf{t}\right) \vert d\mathbf{t}$$ Pullback of Volume Forms :  $$c^*(d\mathbf{x}) = d\mathbf{c} = det\left((D\mathbf{c})_\mathbf{t}\right) d\mathbf{t} $$ $(D\mathbf{c})_\mathbf{t}$ is the total derivative of $\mathbf{c}$ at $\mathbf{t}$. My question is, are these two formulae referring to different things? It feels like these two equations are talking about the same thing, but then why does one have an absolute value on the Jacobian determinant? I’m trying to understand this so I can prove that integrals of differential forms only flip signs under a re-parameterisation that reverses orientation.","Let $d\mathbf{x} = dx^1\wedge \cdots \wedge dx^n$ be the volume form on $U\subset\mathbb{R}^n$ and smooth $c : [a_1,b_1]\times\cdots\times [a_n,b_n] \to U$, $\mathbf{t} = (t_1,\dots,t_n) \mapsto \mathbf{x} = \mathbf{c}(\mathbf{t}) = (c^1(t_1,\dots,t_n),\dots,c^n(t_1,\dots,t_n))$. I’ve encountered the following two formulae: Multivariable Substitution Rule :  $$d\mathbf{x} = d\mathbf{c} = \vert det\left((D\mathbf{c})_\mathbf{t}\right) \vert d\mathbf{t}$$ Pullback of Volume Forms :  $$c^*(d\mathbf{x}) = d\mathbf{c} = det\left((D\mathbf{c})_\mathbf{t}\right) d\mathbf{t} $$ $(D\mathbf{c})_\mathbf{t}$ is the total derivative of $\mathbf{c}$ at $\mathbf{t}$. My question is, are these two formulae referring to different things? It feels like these two equations are talking about the same thing, but then why does one have an absolute value on the Jacobian determinant? I’m trying to understand this so I can prove that integrals of differential forms only flip signs under a re-parameterisation that reverses orientation.",,"['differential-geometry', 'differential-forms']"
4,"How to find distance between a point and 3D surface, solution to general quadric equation, and visualizing such a surface?","How to find distance between a point and 3D surface, solution to general quadric equation, and visualizing such a surface?",,"Goal: I am writing software to visualize 3-D objects in Python, using libraries such as sympy , numpy , and matplotlib.pyplot .  I would like to fit the best surface to a small number of points.  This is why I want to find the smallest distance between a point and a quadric surface ============================================================================== $$Ax^2 + By^2 + Cz^2 + Dxy + Exz + Fyz + Gx + Hy + Iz + J = 0$$ ============================================================================== Questions: 1)  How can I find a solution to this equation, given particular coefficients $A, B, ..., J$ ?  This is for the purpose of solving question 2, which is the key question: 2)  How can I find the smallest Euclidean distance between that surface and a point?  I have access to a computer; I'm using python2 and sympy at the moment. 3)  Is there software I can use to visualize such a surface? 4)  Are these surfaces known as ""manifolds"" in proper mathematical terms?  From brief reading, it sounds like a manifold is a more general mathematical object than the object the word ""surface"" I'm using denotes I've looked here , here , here , here , and here .","Goal: I am writing software to visualize 3-D objects in Python, using libraries such as sympy , numpy , and matplotlib.pyplot .  I would like to fit the best surface to a small number of points.  This is why I want to find the smallest distance between a point and a quadric surface ============================================================================== $$Ax^2 + By^2 + Cz^2 + Dxy + Exz + Fyz + Gx + Hy + Iz + J = 0$$ ============================================================================== Questions: 1)  How can I find a solution to this equation, given particular coefficients $A, B, ..., J$ ?  This is for the purpose of solving question 2, which is the key question: 2)  How can I find the smallest Euclidean distance between that surface and a point?  I have access to a computer; I'm using python2 and sympy at the moment. 3)  Is there software I can use to visualize such a surface? 4)  Are these surfaces known as ""manifolds"" in proper mathematical terms?  From brief reading, it sounds like a manifold is a more general mathematical object than the object the word ""surface"" I'm using denotes I've looked here , here , here , here , and here .",,"['differential-geometry', 'quadrics']"
5,"Determine conditions on $X$ and $Y$ that make span$(X,Y)$ an involutive distribution. How does this affect the maximal integral submanifolds?",Determine conditions on  and  that make span an involutive distribution. How does this affect the maximal integral submanifolds?,"X Y (X,Y)","I've been thinking about the following problem: Equip $\mathbb{R}^3$ with coordinates $(x,y,z)$ and define two vector fields $X$ and $Y$ by       $$ X=\frac{\partial}{\partial x}+f(x,y)\frac{\partial}{\partial z},\hspace{.5 in}\text{and}\hspace{.5 in}Y=\frac{\partial}{\partial y}+g(x,y)\frac{\partial}{\partial z}. $$       Define the distribution $\Delta\subset T\mathbb{R}^3$ by       $$ \Delta =\text{span}(X,Y). $$       Determine conditions on the functions $f(x,y)$ and $g(x,y)$ that imply $\Delta$ is involutive. What do your conditions imply about the maximal connected integral submanifolds of $\Delta$? I've mostly worked out this question, however I'm stuck on the last question: to find how my conditions affect the maximal connected integral submanifolds of $\Delta$. Here's my solution so far. ""Solution"" Note that     \begin{align*}     [X,Y]&=X(1)\frac{\partial}{\partial y}+X(g)\frac{\partial}{\partial z}-Y(1)\frac{\partial}{\partial x}-Y(f)\frac{\partial}{\partial z}\\     &=\left(\frac{\partial g}{\partial x}+f \frac{\partial g}{\partial z}\right)\frac{\partial}{\partial z}-\left(\frac{\partial f}{\partial y}+g\frac{\partial f}{\partial z}\right)\frac{\partial}{\partial z}\\     &=\left(\frac{\partial g}{\partial x}-\frac{\partial f}{\partial y}\right)\frac{\partial}{\partial z}, \end{align*}     where we have used that $f$ and $g$ are independent of $z$. Now $\Delta$ is involutive if and only if $[X,Y]\in\text{span}(X,Y)$, so we require the determinant of     $$ \begin{pmatrix} 1 & 0 & f\\ 0 & 1 & g\\  0 & 0 & \frac{\partial g}{\partial x}-\frac{\partial f}{\partial y} \end{pmatrix} $$     to vanish identically, i.e. we require     $$ \frac{\partial g}{\partial x}=\frac{\partial f}{\partial y}. $$ My problem is that any curve following $X$ would be of the form $\alpha(t)=(t+x_0,y_0,F(t))$, where $F$ is some antiderivative of $f(t+x_0,y_0)$, and similiarly for $Y$. Since in this form $f$ is constant in the $y$-direction I don't see how the condition above will come into play. Is my thinking incorrect? Is one of my calculations wrong? Any help would be greatly appreciated. Thanks.","I've been thinking about the following problem: Equip $\mathbb{R}^3$ with coordinates $(x,y,z)$ and define two vector fields $X$ and $Y$ by       $$ X=\frac{\partial}{\partial x}+f(x,y)\frac{\partial}{\partial z},\hspace{.5 in}\text{and}\hspace{.5 in}Y=\frac{\partial}{\partial y}+g(x,y)\frac{\partial}{\partial z}. $$       Define the distribution $\Delta\subset T\mathbb{R}^3$ by       $$ \Delta =\text{span}(X,Y). $$       Determine conditions on the functions $f(x,y)$ and $g(x,y)$ that imply $\Delta$ is involutive. What do your conditions imply about the maximal connected integral submanifolds of $\Delta$? I've mostly worked out this question, however I'm stuck on the last question: to find how my conditions affect the maximal connected integral submanifolds of $\Delta$. Here's my solution so far. ""Solution"" Note that     \begin{align*}     [X,Y]&=X(1)\frac{\partial}{\partial y}+X(g)\frac{\partial}{\partial z}-Y(1)\frac{\partial}{\partial x}-Y(f)\frac{\partial}{\partial z}\\     &=\left(\frac{\partial g}{\partial x}+f \frac{\partial g}{\partial z}\right)\frac{\partial}{\partial z}-\left(\frac{\partial f}{\partial y}+g\frac{\partial f}{\partial z}\right)\frac{\partial}{\partial z}\\     &=\left(\frac{\partial g}{\partial x}-\frac{\partial f}{\partial y}\right)\frac{\partial}{\partial z}, \end{align*}     where we have used that $f$ and $g$ are independent of $z$. Now $\Delta$ is involutive if and only if $[X,Y]\in\text{span}(X,Y)$, so we require the determinant of     $$ \begin{pmatrix} 1 & 0 & f\\ 0 & 1 & g\\  0 & 0 & \frac{\partial g}{\partial x}-\frac{\partial f}{\partial y} \end{pmatrix} $$     to vanish identically, i.e. we require     $$ \frac{\partial g}{\partial x}=\frac{\partial f}{\partial y}. $$ My problem is that any curve following $X$ would be of the form $\alpha(t)=(t+x_0,y_0,F(t))$, where $F$ is some antiderivative of $f(t+x_0,y_0)$, and similiarly for $Y$. Since in this form $f$ is constant in the $y$-direction I don't see how the condition above will come into play. Is my thinking incorrect? Is one of my calculations wrong? Any help would be greatly appreciated. Thanks.",,"['general-topology', 'differential-geometry', 'differential-topology']"
6,Parallel transport in two different polar coordinates,Parallel transport in two different polar coordinates,,"I need help with the basics of parallel transport. So I will write down what I have done in the plane $\mathbb{R}^2$ with non-cartesian coordinates, mixed with some small questions. First, I use polar coordinates $(r,\theta)$ and the coordinate frame for the tangent space, which is the plane itself, given by partial derivatives $(\partial_r,\partial_\theta)$. The operator $\partial_\theta$ generates rotations, in the sense that $e^{\phi\partial_\theta}$ is a rotation by angle $\phi$, independent of $r$. As a tangent vector, $\partial_\theta$ is not a unit vector, however, since $\partial_\theta=x\partial_y-y\partial_x$ so $||\partial_\theta||^2=x^2+y^2=r^2$. The vector $\partial_\theta$ increases in size with increasing $r$. This is in line with the fact that it generates rotation by a fixed angle, so the arc sweeped by such a rotation indeed increases linearly with $r$. [is this reasoning correct?] The translated vector $\vec{U}$ will have coordinates related to those of the initial vector $\vec{V}$ by $U^\mu=V^\mu-V^\lambda\Gamma^{\mu}_{\nu\lambda}dx^\nu$. Take a vector $\vec{V}=V^r\partial_r+V^\theta\partial_\theta$ with $V^r=V\cos\phi$ and $V^\theta=V\sin\phi/r$ for some $\phi$. If we transport it along $r$ by some amount $dr$, the new coordinates are $U^r=V^r$ and $U^\theta=V^\theta-V^\theta dr/r$. Therefore, $\Gamma^r_{ra}=0$, $\Gamma^\theta_{rr}=0$, $\Gamma^\theta_{r\theta}=1/r$. If we transport it along $\theta$ by some amount $d\theta$, the coordinates become $U^r=V^r+V^\theta rd\theta$ and $U^\theta=V^\theta-V^r d\theta/r$. Therefore, $\Gamma^r_{\theta r}=\Gamma^\theta_{\theta \theta}=0$, $\Gamma^r_{\theta \theta}=-r$, $\Gamma^\theta_{\theta r}=1/r$. Since $\Gamma^\mu_{\nu\lambda}=\Gamma^\mu_{\lambda\nu}$, there is no torsion. Now, I want to change the coordinate system in the tangent space, to test my understanding. I want to use $(\partial_r,\partial_s)$, where $s=r\theta$ is an arc coordinate. Since $\partial_s=\frac{1}{r}\partial_\theta$, the operator $\partial_s$ has unit norm (it is the versor $\hat\theta$) and also produces rotations, but by a fixed arc instead of a fixed angle. Notice that $\partial_r$ and $\partial_s$ do not commute. Now $\vec{V}=V^r\partial_r+V^s\partial_s$ with $V^r=V\cos\phi$ and $V^s=V\sin\phi$. When we transport it along $r$, the coordinates do not change at all, so $\Gamma^{\mu}_{r\lambda}=0$. When  we transport it along $s$ by some amount $ds$ the coordinates become $U^r=V^r+V^sds/r$ and $U^s=V^s-V^r ds/r$ so $\Gamma^r_{\theta r}=\Gamma^s_{s s}=0$, $\Gamma^r_{s s}=-1/r$, $\Gamma^s_{s r}=1/r$. This time $\Gamma$ is not symmetric, as $\Gamma^{s}_{rs}=0$ but $\Gamma^{s}_{sr}=1/r$. That means this way of doing it, which can be seen as a different connection on the plane from the usual, has torsion. Is this correct? I was expecting that things would turn out the same in the end. I mean, I thought I could choose whatever coordinate system I wanted and parallel transport and torsion would be invariant notions. I mean, I DEFINED the final vetor to be identical to the original vector, so... how can there be torsion??","I need help with the basics of parallel transport. So I will write down what I have done in the plane $\mathbb{R}^2$ with non-cartesian coordinates, mixed with some small questions. First, I use polar coordinates $(r,\theta)$ and the coordinate frame for the tangent space, which is the plane itself, given by partial derivatives $(\partial_r,\partial_\theta)$. The operator $\partial_\theta$ generates rotations, in the sense that $e^{\phi\partial_\theta}$ is a rotation by angle $\phi$, independent of $r$. As a tangent vector, $\partial_\theta$ is not a unit vector, however, since $\partial_\theta=x\partial_y-y\partial_x$ so $||\partial_\theta||^2=x^2+y^2=r^2$. The vector $\partial_\theta$ increases in size with increasing $r$. This is in line with the fact that it generates rotation by a fixed angle, so the arc sweeped by such a rotation indeed increases linearly with $r$. [is this reasoning correct?] The translated vector $\vec{U}$ will have coordinates related to those of the initial vector $\vec{V}$ by $U^\mu=V^\mu-V^\lambda\Gamma^{\mu}_{\nu\lambda}dx^\nu$. Take a vector $\vec{V}=V^r\partial_r+V^\theta\partial_\theta$ with $V^r=V\cos\phi$ and $V^\theta=V\sin\phi/r$ for some $\phi$. If we transport it along $r$ by some amount $dr$, the new coordinates are $U^r=V^r$ and $U^\theta=V^\theta-V^\theta dr/r$. Therefore, $\Gamma^r_{ra}=0$, $\Gamma^\theta_{rr}=0$, $\Gamma^\theta_{r\theta}=1/r$. If we transport it along $\theta$ by some amount $d\theta$, the coordinates become $U^r=V^r+V^\theta rd\theta$ and $U^\theta=V^\theta-V^r d\theta/r$. Therefore, $\Gamma^r_{\theta r}=\Gamma^\theta_{\theta \theta}=0$, $\Gamma^r_{\theta \theta}=-r$, $\Gamma^\theta_{\theta r}=1/r$. Since $\Gamma^\mu_{\nu\lambda}=\Gamma^\mu_{\lambda\nu}$, there is no torsion. Now, I want to change the coordinate system in the tangent space, to test my understanding. I want to use $(\partial_r,\partial_s)$, where $s=r\theta$ is an arc coordinate. Since $\partial_s=\frac{1}{r}\partial_\theta$, the operator $\partial_s$ has unit norm (it is the versor $\hat\theta$) and also produces rotations, but by a fixed arc instead of a fixed angle. Notice that $\partial_r$ and $\partial_s$ do not commute. Now $\vec{V}=V^r\partial_r+V^s\partial_s$ with $V^r=V\cos\phi$ and $V^s=V\sin\phi$. When we transport it along $r$, the coordinates do not change at all, so $\Gamma^{\mu}_{r\lambda}=0$. When  we transport it along $s$ by some amount $ds$ the coordinates become $U^r=V^r+V^sds/r$ and $U^s=V^s-V^r ds/r$ so $\Gamma^r_{\theta r}=\Gamma^s_{s s}=0$, $\Gamma^r_{s s}=-1/r$, $\Gamma^s_{s r}=1/r$. This time $\Gamma$ is not symmetric, as $\Gamma^{s}_{rs}=0$ but $\Gamma^{s}_{sr}=1/r$. That means this way of doing it, which can be seen as a different connection on the plane from the usual, has torsion. Is this correct? I was expecting that things would turn out the same in the end. I mean, I thought I could choose whatever coordinate system I wanted and parallel transport and torsion would be invariant notions. I mean, I DEFINED the final vetor to be identical to the original vector, so... how can there be torsion??",,"['differential-geometry', 'connections']"
7,Calculation to show $|\mathrm{d}r|^2_{\bar g} = 1$ implies sectional curvatures tend to $-1$.,Calculation to show  implies sectional curvatures tend to .,|\mathrm{d}r|^2_{\bar g} = 1 -1,"$\textbf{tl;dr:}$ Given that $r$ is a definining function for the boundary of a conformally compact manifold, how does one show that the sectional curvatures tend to $-1$ if $|\mathrm{d}r|^2_{\bar g} = 1$ on the boundary? $\textbf{Definitions:}$ Let $(M, g)$ be a pseudo-Riemannian manifold. Suppose $M$ can be identified with the interior of a smooth compact manifold with boundary $\overline M$. Let $\Sigma$ denote $\partial M$ so that $\overline M = \Sigma \sqcup M$. We say that $r$ is a defining function for $\Sigma$ if $r: M \to \mathbb{R}$ is smooth, $\mathcal Z(r) = \Sigma$, and $\mathrm{d}r \neq 0$ on $\Sigma$. ($\mathcal Z(r)$ denotes the zero locus of $r$, that is, the set of points in $M$ where $r$ vanishes.) We say that $(M, g)$ is conformally compact if there is a defining function $r$ for $\Sigma$ such that $$\bar g = r^2 g$$ on $M$, and $g$ is a metric on $\overline M$. We say that $g$ is asymptotically hyperbolic if $|\mathrm{d}r|_{\bar g}^2 = 1$. Question: My interpretation of asymptotically hyperbolic is that the sectional curvatures should tend to $-1$, so I set out to do some calculations. Does $|\mathrm{d}r|_{\bar g}^2 = 1$ really ensure that sectional curvatures tend to $-1$? I start by using the conformal transformation rule for the Riemann curvature, and write $$\bar R_{ijkl} = r^2(R_{ijkl} + (\Upsilon_{ij} - \Upsilon_i \Upsilon_j + \frac{1}{2}\Upsilon^2 g_{ij})\circledast g_{kl})$$ where I have used $\circledast$ to denote the Kulkarni-Nomizu product. Note that $\Upsilon_{i} = r^{-1}\partial_{i}r$. I now proceed to write each term on the right as $r^kA_{ijkl}$ where $A_{ijkl}$ is bounded (in the sense that each component of the tensor is finite). First we note the following: \begin{align*} \Upsilon_{ij} &= \nabla_{i}\Upsilon_j = \nabla_i(r^{-1}\partial_j r) = r^{-1}\Phi_{ij} - r^{-2}\Phi_i \Phi_j\\ \Upsilon_i \Upsilon_j &= r^{-2}\Phi_i \Phi_j\\ \Upsilon^2 &= r^{-2}g^{ij}\Phi_i \Phi_j = r^{-2}r^{2}\bar g^{ij}\Phi_i\Phi_j = |\mathrm{d}r|^2_{\bar g} \end{align*} where $\Phi_i = \partial_i r$. Therefore, we have \begin{align*} R_{ijkl} &= r^{-2}\bar R_{ijkl} - (\Upsilon_{ij} - \Upsilon_i \Upsilon_j + \frac{1}{2}\Upsilon^2 g_{ij})\circledast g_{kl}\\ &= r^{-2}\bar R_{ijkl} - (r^{-1}\Phi_{ij} - r^{-2}\Phi_i \Phi_j - r^{-2}\Phi_i \Phi_j + \frac{1}{2}|\mathrm{d}r|^2_{\bar g} g_{ij})\circledast g_{kl}\\ &= r^{-2}\bar R_{ijkl} - (r^{-3}\Phi_{ij} - r^{-4}\Phi_i \Phi_j - r^{-4}\Phi_i \Phi_j + r^{-4}\frac{1}{2}|\mathrm{d}r|^2_{\bar g} \bar g_{ij})\circledast \bar g_{kl}. \end{align*} If the two terms in the ""centre"" of the expression ($r^{-4}\Phi_i \Phi_j$) were to cancel, it now becomes clear that the $|\mathrm{d}r|^2_{\bar g}$ term dominates as $r$ vanishes, and the expression directly implies that the sectional curvature tends to $-|\mathrm{d}r|^2_{\bar g}$. However, the signs are negative for both terms and the limits don't work out. Did I make a calculation mistake somewhere? Any pointers would be appreciated!","$\textbf{tl;dr:}$ Given that $r$ is a definining function for the boundary of a conformally compact manifold, how does one show that the sectional curvatures tend to $-1$ if $|\mathrm{d}r|^2_{\bar g} = 1$ on the boundary? $\textbf{Definitions:}$ Let $(M, g)$ be a pseudo-Riemannian manifold. Suppose $M$ can be identified with the interior of a smooth compact manifold with boundary $\overline M$. Let $\Sigma$ denote $\partial M$ so that $\overline M = \Sigma \sqcup M$. We say that $r$ is a defining function for $\Sigma$ if $r: M \to \mathbb{R}$ is smooth, $\mathcal Z(r) = \Sigma$, and $\mathrm{d}r \neq 0$ on $\Sigma$. ($\mathcal Z(r)$ denotes the zero locus of $r$, that is, the set of points in $M$ where $r$ vanishes.) We say that $(M, g)$ is conformally compact if there is a defining function $r$ for $\Sigma$ such that $$\bar g = r^2 g$$ on $M$, and $g$ is a metric on $\overline M$. We say that $g$ is asymptotically hyperbolic if $|\mathrm{d}r|_{\bar g}^2 = 1$. Question: My interpretation of asymptotically hyperbolic is that the sectional curvatures should tend to $-1$, so I set out to do some calculations. Does $|\mathrm{d}r|_{\bar g}^2 = 1$ really ensure that sectional curvatures tend to $-1$? I start by using the conformal transformation rule for the Riemann curvature, and write $$\bar R_{ijkl} = r^2(R_{ijkl} + (\Upsilon_{ij} - \Upsilon_i \Upsilon_j + \frac{1}{2}\Upsilon^2 g_{ij})\circledast g_{kl})$$ where I have used $\circledast$ to denote the Kulkarni-Nomizu product. Note that $\Upsilon_{i} = r^{-1}\partial_{i}r$. I now proceed to write each term on the right as $r^kA_{ijkl}$ where $A_{ijkl}$ is bounded (in the sense that each component of the tensor is finite). First we note the following: \begin{align*} \Upsilon_{ij} &= \nabla_{i}\Upsilon_j = \nabla_i(r^{-1}\partial_j r) = r^{-1}\Phi_{ij} - r^{-2}\Phi_i \Phi_j\\ \Upsilon_i \Upsilon_j &= r^{-2}\Phi_i \Phi_j\\ \Upsilon^2 &= r^{-2}g^{ij}\Phi_i \Phi_j = r^{-2}r^{2}\bar g^{ij}\Phi_i\Phi_j = |\mathrm{d}r|^2_{\bar g} \end{align*} where $\Phi_i = \partial_i r$. Therefore, we have \begin{align*} R_{ijkl} &= r^{-2}\bar R_{ijkl} - (\Upsilon_{ij} - \Upsilon_i \Upsilon_j + \frac{1}{2}\Upsilon^2 g_{ij})\circledast g_{kl}\\ &= r^{-2}\bar R_{ijkl} - (r^{-1}\Phi_{ij} - r^{-2}\Phi_i \Phi_j - r^{-2}\Phi_i \Phi_j + \frac{1}{2}|\mathrm{d}r|^2_{\bar g} g_{ij})\circledast g_{kl}\\ &= r^{-2}\bar R_{ijkl} - (r^{-3}\Phi_{ij} - r^{-4}\Phi_i \Phi_j - r^{-4}\Phi_i \Phi_j + r^{-4}\frac{1}{2}|\mathrm{d}r|^2_{\bar g} \bar g_{ij})\circledast \bar g_{kl}. \end{align*} If the two terms in the ""centre"" of the expression ($r^{-4}\Phi_i \Phi_j$) were to cancel, it now becomes clear that the $|\mathrm{d}r|^2_{\bar g}$ term dominates as $r$ vanishes, and the expression directly implies that the sectional curvature tends to $-|\mathrm{d}r|^2_{\bar g}$. However, the signs are negative for both terms and the limits don't work out. Did I make a calculation mistake somewhere? Any pointers would be appreciated!",,"['differential-geometry', 'riemannian-geometry', 'conformal-geometry', 'general-relativity', 'compactification']"
8,Restrictions on Laplacian eigenvalues in 1 dimension,Restrictions on Laplacian eigenvalues in 1 dimension,,"Which finite sequences $0=\lambda_0<\lambda_1<\dots<\lambda_k$ can be obtained as the first $k+1$ eigenvalues of the Laplacian operator $\Delta_g$ on the circle $S^1$? Of course the metric $g$ is allowed to be arbitrary. Thoughts: In this case, the metric is specified by a positive function $S^1\to\mathbb{R}$, and by expanding this as a Fourier series, the Laplace equation $\Delta f=\lambda f$ becomes an infinite-dimensional matrix equation for the fourier coefficients of $f$, however, I am not sure how useful this viewpoint is. By way of motivation, it is a theorem of de Verdiere that any such sequence can be realized on any closed manifold of dimension $\ge 3$ by appropriate choice of metric .","Which finite sequences $0=\lambda_0<\lambda_1<\dots<\lambda_k$ can be obtained as the first $k+1$ eigenvalues of the Laplacian operator $\Delta_g$ on the circle $S^1$? Of course the metric $g$ is allowed to be arbitrary. Thoughts: In this case, the metric is specified by a positive function $S^1\to\mathbb{R}$, and by expanding this as a Fourier series, the Laplace equation $\Delta f=\lambda f$ becomes an infinite-dimensional matrix equation for the fourier coefficients of $f$, however, I am not sure how useful this viewpoint is. By way of motivation, it is a theorem of de Verdiere that any such sequence can be realized on any closed manifold of dimension $\ge 3$ by appropriate choice of metric .",,"['differential-geometry', 'partial-differential-equations', 'riemannian-geometry', 'laplacian']"
9,"Explicit complete metric on $\mathbb{C}\backslash \{ 0,1\} $ with Gaussian curvature $K \leqslant - 1$.",Explicit complete metric on  with Gaussian curvature .,"\mathbb{C}\backslash \{ 0,1\}  K \leqslant - 1","I wonder how to give a complete metric on $\mathbb{C}\backslash \{ 0,1\} $ with Gaussian curvature $K \leqslant  - 1$ explicitly. I tried to modify the Poincaré disk model since it has constant curvature $-1$ under the hyperbolic metric. However, I still could not see how to make it right. Is it possible to use cutoff functions to give an explicit expression? Any help will be appreciated.","I wonder how to give a complete metric on $\mathbb{C}\backslash \{ 0,1\} $ with Gaussian curvature $K \leqslant  - 1$ explicitly. I tried to modify the Poincaré disk model since it has constant curvature $-1$ under the hyperbolic metric. However, I still could not see how to make it right. Is it possible to use cutoff functions to give an explicit expression? Any help will be appreciated.",,"['differential-geometry', 'riemannian-geometry', 'riemann-surfaces', 'hyperbolic-geometry', 'curvature']"
10,Connected 1-manifolds,Connected 1-manifolds,,"I am reading the beginning of the first volume on the introduction to Differential Geometry by Spivak. In the first part he defines manifolds and states that 'the only connected 1-manifolds are the line $\mathbb{R}$ and the circle $\mathbb{S}^1$'. (1) Is that ' only ' a direct consequence of the definition of manifold? I do not really see the answer. It is also true that, at this point of the book, Spivak uses results from Algebraic topology/Homology (without proving them) to give a wider outlook on the world of manifolds. So it might well be that the answer is not trivial. If it actually is, I would like to see why. (2) Does this fact imply that the only surfaces of revolution in $\mathbb{R}^3$ which can be obtained by connected 1-manifolds are cylinders and 2-tori?","I am reading the beginning of the first volume on the introduction to Differential Geometry by Spivak. In the first part he defines manifolds and states that 'the only connected 1-manifolds are the line $\mathbb{R}$ and the circle $\mathbb{S}^1$'. (1) Is that ' only ' a direct consequence of the definition of manifold? I do not really see the answer. It is also true that, at this point of the book, Spivak uses results from Algebraic topology/Homology (without proving them) to give a wider outlook on the world of manifolds. So it might well be that the answer is not trivial. If it actually is, I would like to see why. (2) Does this fact imply that the only surfaces of revolution in $\mathbb{R}^3$ which can be obtained by connected 1-manifolds are cylinders and 2-tori?",,"['general-topology', 'differential-geometry', 'manifolds']"
11,A question about the shape operator of a minimal hypersurface.,A question about the shape operator of a minimal hypersurface.,,"I was reading a paper about minimal hypersurfaces. The author said that the Codazzi equation for a hypersurface $M$ in a manifold with constant sectional curvature is given by $(\nabla A)(X,Y)=(\nabla A)(Y,X)$ where $A$ is the shape operator of $M$ and $(\nabla A)(X,Y)=\nabla_X(AY)-A(\nabla_XY)$. Then the author said that if $M$ is a $minimal$ hypersurface then in a local orthonormal frame $\{e_1,e_2,...,e_n\}$, we have \begin{equation} (\nabla A)(e_i,e_i)=0 \end{equation} I couldn't figure out why this should be true. If $M$ is minimal then all I know is that in the orthonormal frame $\sum_{i=1}^n g(Ae_i,e_i)=0$. I tried to differentiate it and get the expression but I was unable to do so. Can you please help me with this ? Thanks","I was reading a paper about minimal hypersurfaces. The author said that the Codazzi equation for a hypersurface $M$ in a manifold with constant sectional curvature is given by $(\nabla A)(X,Y)=(\nabla A)(Y,X)$ where $A$ is the shape operator of $M$ and $(\nabla A)(X,Y)=\nabla_X(AY)-A(\nabla_XY)$. Then the author said that if $M$ is a $minimal$ hypersurface then in a local orthonormal frame $\{e_1,e_2,...,e_n\}$, we have \begin{equation} (\nabla A)(e_i,e_i)=0 \end{equation} I couldn't figure out why this should be true. If $M$ is minimal then all I know is that in the orthonormal frame $\sum_{i=1}^n g(Ae_i,e_i)=0$. I tried to differentiate it and get the expression but I was unable to do so. Can you please help me with this ? Thanks",,"['differential-geometry', 'riemannian-geometry', 'proof-explanation', 'minimal-surfaces']"
12,Vector fields on $S^3$,Vector fields on,S^3,"Does there exist a smooth vector field from $S^3$ to its tangent bundle with exactly $3$ zeros? The only vector field I can think was $v(x_1,x_2,x_3,x_4)=(-x_2,x_1,-x_4,x_3)$ which has no zeros. How could I think about a one that has zeros? Any help is appreciated.","Does there exist a smooth vector field from $S^3$ to its tangent bundle with exactly $3$ zeros? The only vector field I can think was $v(x_1,x_2,x_3,x_4)=(-x_2,x_1,-x_4,x_3)$ which has no zeros. How could I think about a one that has zeros? Any help is appreciated.",,"['differential-geometry', 'vector-bundles']"
13,Space curve: $\frac{\tau}{\kappa} =$ constant. Then all normal vectors $N(x)$ lie in a plane $A$.,Space curve:  constant. Then all normal vectors  lie in a plane .,\frac{\tau}{\kappa} = N(x) A,"Consider a space curve $c : I \rightarrow \mathbb{R^3}$ (regular) with $\frac{\tau}{\kappa}$ constant. (Torsion divided by curvature) Show: a) There is a plane $A$ with :           All normal vector $N(x)$ are in this plane $A$ . So I know that I have to find a vector $cos(\phi) \cdot T(x) + sin(\phi) \cdot B(x)$ with a constant $\phi$ to solve this exercise. But how can I find such a vector? And how can I use this vector to show a) ? Second task: Now the space curve is parameterized by arc length. Show: b) There is an isometry $J : \mathbb{R^3} \rightarrow \mathbb{R^3}$ with: $ J \circ c $ $(y)$ $=$ $(f(y),g(y), y \cdot cos(\phi))$ for two functions $f,g$ :$ I \rightarrow \mathbb{R} $. Remark : the $\phi$ is the $\phi$ from a) . So since $c$ is parameterized by arc length. I can use the following equations: $T' = \kappa \cdot  N $ and $ N' = - \kappa \cdot T + \tau \cdot B $ and $B' = - \tau \cdot N $, right? I think then we need them. Remark2 : Definition of $T,N,B$ $T(x)$ := $\frac{1}{||c'(x)||} \cdot c'(x) $. $N(x)$ := $\frac{1}{||T'(x)||} \cdot T'(x) $. $B(x)$ := $T(x) \times N(x)$ $\kappa(x)$ := $\frac{|| c' \times c''||(x)}{||c'(x)||^3} $. (curvature) $\tau(x)$ := $\frac{det(c',c'',c''')(x)}{|| c' \times c''||^2(x)} $ (torsion)","Consider a space curve $c : I \rightarrow \mathbb{R^3}$ (regular) with $\frac{\tau}{\kappa}$ constant. (Torsion divided by curvature) Show: a) There is a plane $A$ with :           All normal vector $N(x)$ are in this plane $A$ . So I know that I have to find a vector $cos(\phi) \cdot T(x) + sin(\phi) \cdot B(x)$ with a constant $\phi$ to solve this exercise. But how can I find such a vector? And how can I use this vector to show a) ? Second task: Now the space curve is parameterized by arc length. Show: b) There is an isometry $J : \mathbb{R^3} \rightarrow \mathbb{R^3}$ with: $ J \circ c $ $(y)$ $=$ $(f(y),g(y), y \cdot cos(\phi))$ for two functions $f,g$ :$ I \rightarrow \mathbb{R} $. Remark : the $\phi$ is the $\phi$ from a) . So since $c$ is parameterized by arc length. I can use the following equations: $T' = \kappa \cdot  N $ and $ N' = - \kappa \cdot T + \tau \cdot B $ and $B' = - \tau \cdot N $, right? I think then we need them. Remark2 : Definition of $T,N,B$ $T(x)$ := $\frac{1}{||c'(x)||} \cdot c'(x) $. $N(x)$ := $\frac{1}{||T'(x)||} \cdot T'(x) $. $B(x)$ := $T(x) \times N(x)$ $\kappa(x)$ := $\frac{|| c' \times c''||(x)}{||c'(x)||^3} $. (curvature) $\tau(x)$ := $\frac{det(c',c'',c''')(x)}{|| c' \times c''||^2(x)} $ (torsion)",,"['differential-geometry', 'curves', 'curvature', 'isometry']"
14,"Bott & Tu, Exercise 4.3.1","Bott & Tu, Exercise 4.3.1",,"I have some trouble computing the integral in Exercise 4.3.1 in Bott and Tu; Differential Forms in Algebraic Topology, and I was wondering if someone could help me with that. So we have the $n$-form $$\omega= \sum_{i=1}^{n}(-1)^{i-1}x_idx_1\ldots \widehat{dx_i}\ldots dx_{n+1}$$ (hat means omit) on the $n$ unit sphere $S^n$ and we have to integrate it over $S^n$. So, i did it over $S^1$ using the usual parametrization but higher dimensional analogues seem too complicated to compute. Is there an easier way? Thank you.","I have some trouble computing the integral in Exercise 4.3.1 in Bott and Tu; Differential Forms in Algebraic Topology, and I was wondering if someone could help me with that. So we have the $n$-form $$\omega= \sum_{i=1}^{n}(-1)^{i-1}x_idx_1\ldots \widehat{dx_i}\ldots dx_{n+1}$$ (hat means omit) on the $n$ unit sphere $S^n$ and we have to integrate it over $S^n$. So, i did it over $S^1$ using the usual parametrization but higher dimensional analogues seem too complicated to compute. Is there an easier way? Thank you.",,"['integration', 'differential-geometry', 'algebraic-topology', 'differential-forms']"
15,tangent of the evolute is normal to curve and compute the length of the evolute,tangent of the evolute is normal to curve and compute the length of the evolute,,"Let $s : (a,b) \rightarrow \mathbb{R^2} $ be a regular parameterized curve ( arc length parameterized) with curvature  $k(x) \neq 0 \forall x\in (a,b) $. We define the evolute of $s$ $e(x) :=s(x)+\frac{1}{k(x)}n(x),\quad x\in (a,b)$ $1)$ Show that the tangent at $x$ of the evolute of $s$ is the normal to $s$ at $x$. $2)$ Now assume that $k'(x) < 0 \forall x \in(a,b) $  and show that: $$L( e_{[i,j]}) =  \frac{1}{k(j)} - \frac{1}{k(i)}$$ for $ a < i \leq j < b $. For $1)$:  I know that we have to show: $\langle e'(x),s'(x)\rangle = 0 $. $e'(x) = s'(x)-\frac{k'(x)}{k(x)^2}n(x)+\frac{1}{k(x)}n'(x). $ Now I read that I have to use: $\langle s''(x), n(x) \rangle = - \langle s'(x),n'(x) \rangle $ and  $s''(x) = k(x)n(x) $ . I have already shown both equations but how can I use them to solve this exercise? For $2)$ $L( e_{[i,j]}) = \int_{i}^{j} ||e'(x)|| dx = \int_{i}^{j} ||s'(x)-\frac{k'(x)}{k(x)^2}n(x)+\frac{1}{k(x)}n'(x)|| dx $ but what can I do now? We know that $k'(x) < 0 $ and maybe we can use $1)$ ?","Let $s : (a,b) \rightarrow \mathbb{R^2} $ be a regular parameterized curve ( arc length parameterized) with curvature  $k(x) \neq 0 \forall x\in (a,b) $. We define the evolute of $s$ $e(x) :=s(x)+\frac{1}{k(x)}n(x),\quad x\in (a,b)$ $1)$ Show that the tangent at $x$ of the evolute of $s$ is the normal to $s$ at $x$. $2)$ Now assume that $k'(x) < 0 \forall x \in(a,b) $  and show that: $$L( e_{[i,j]}) =  \frac{1}{k(j)} - \frac{1}{k(i)}$$ for $ a < i \leq j < b $. For $1)$:  I know that we have to show: $\langle e'(x),s'(x)\rangle = 0 $. $e'(x) = s'(x)-\frac{k'(x)}{k(x)^2}n(x)+\frac{1}{k(x)}n'(x). $ Now I read that I have to use: $\langle s''(x), n(x) \rangle = - \langle s'(x),n'(x) \rangle $ and  $s''(x) = k(x)n(x) $ . I have already shown both equations but how can I use them to solve this exercise? For $2)$ $L( e_{[i,j]}) = \int_{i}^{j} ||e'(x)|| dx = \int_{i}^{j} ||s'(x)-\frac{k'(x)}{k(x)^2}n(x)+\frac{1}{k(x)}n'(x)|| dx $ but what can I do now? We know that $k'(x) < 0 $ and maybe we can use $1)$ ?",,"['differential-geometry', 'curves', 'curvature']"
16,Huisken's monotonicity formula,Huisken's monotonicity formula,,"In mean curvature flow there is an important tool, namely the Huisken's monotonicity formula: For a solution of the mean curvature flow $F: M^n \times [0,T) \rightarrow \mathbb{R}^m$ we have $$ \frac{d}{dt} \int_{M} \rho \: d \mu_t   =  - \int_{M} \left| H + \frac{\langle x, \nu \rangle}{2(T-t)} \right|^2 \rho \: d \mu_t, $$ where $$ \rho(x,t) =\frac{1}{(4\pi(T-t))^{\frac{n}{2}}} e^{-\frac{|x|^2}{4 (T-t)}}. $$ However in the literature of mean curvature flow I can find many statements of the formula where they all assume different conditions: Can the codimension of the solution, i.e. $m-n$ be bigger than $1$? Does $M$ have to be compact? If not, does $M$ have to be complete? Does $M$ have to be orientable? ... In every proof I have seen it seems that the codimension does not play a role and that it may be arbitrary. For the second: I know that Stoke's Theorem is used to show the monotonicity formula in many proofs and that for this we need orientability and compactness. However none of the references did explicitly require the orientability? The Stoke's theorem is used in the form: $$ \int_M \Delta \rho=0. $$ Is there any need to introduce the Hausdorff measure for a rigorous statement? EDIT: To make my question a little bit more precise: The question would be completely solved if someone can state (and maybe reference) a statement which allows noncompact manifolds in any codimension but possibly with additional assumptions: does $M$ have to be complete or orientable, does $F$ have to be an embedding,...? To give some specific references in the literature: Huisken proved the statement when $M$ is compact and in the codimension $1$ case in the paper Asymptotic behavior for singularities of the mean curvature flow ,   J. Differential Geometry, 31 (1990) 285-299. Later K. Ecker wrote a book where he proved a generalization to the noncompact case but where the integral of $\rho$ has to exist over $M$ (this is clearly natural), however in the beginning of the book he states that throughout the book he works in the case where $F$ is at each time a hypersurface which seems not to be necessary.","In mean curvature flow there is an important tool, namely the Huisken's monotonicity formula: For a solution of the mean curvature flow $F: M^n \times [0,T) \rightarrow \mathbb{R}^m$ we have $$ \frac{d}{dt} \int_{M} \rho \: d \mu_t   =  - \int_{M} \left| H + \frac{\langle x, \nu \rangle}{2(T-t)} \right|^2 \rho \: d \mu_t, $$ where $$ \rho(x,t) =\frac{1}{(4\pi(T-t))^{\frac{n}{2}}} e^{-\frac{|x|^2}{4 (T-t)}}. $$ However in the literature of mean curvature flow I can find many statements of the formula where they all assume different conditions: Can the codimension of the solution, i.e. $m-n$ be bigger than $1$? Does $M$ have to be compact? If not, does $M$ have to be complete? Does $M$ have to be orientable? ... In every proof I have seen it seems that the codimension does not play a role and that it may be arbitrary. For the second: I know that Stoke's Theorem is used to show the monotonicity formula in many proofs and that for this we need orientability and compactness. However none of the references did explicitly require the orientability? The Stoke's theorem is used in the form: $$ \int_M \Delta \rho=0. $$ Is there any need to introduce the Hausdorff measure for a rigorous statement? EDIT: To make my question a little bit more precise: The question would be completely solved if someone can state (and maybe reference) a statement which allows noncompact manifolds in any codimension but possibly with additional assumptions: does $M$ have to be complete or orientable, does $F$ have to be an embedding,...? To give some specific references in the literature: Huisken proved the statement when $M$ is compact and in the codimension $1$ case in the paper Asymptotic behavior for singularities of the mean curvature flow ,   J. Differential Geometry, 31 (1990) 285-299. Later K. Ecker wrote a book where he proved a generalization to the noncompact case but where the integral of $\rho$ has to exist over $M$ (this is clearly natural), however in the beginning of the book he states that throughout the book he works in the case where $F$ is at each time a hypersurface which seems not to be necessary.",,"['differential-geometry', 'riemannian-geometry', 'geometric-measure-theory', 'mean-curvature-flows']"
17,convention for interior product,convention for interior product,,"What is the convention that we take for the interior product of a function? i.e., if $X\in\mathfrak{X}(M)$ and $f\in C^\infty(M)$, how do we define $\iota(x)f$? Recall that $\iota(X)\alpha := \alpha(X,\cdot)$ for $\alpha\in\Omega^r(M)$ and then $\iota(X)\alpha\in\Omega^{r-1}(M)$ but this doesn't make sense for $f$. I suppose that we take the convention that $\iota(X)f$ is zero.","What is the convention that we take for the interior product of a function? i.e., if $X\in\mathfrak{X}(M)$ and $f\in C^\infty(M)$, how do we define $\iota(x)f$? Recall that $\iota(X)\alpha := \alpha(X,\cdot)$ for $\alpha\in\Omega^r(M)$ and then $\iota(X)\alpha\in\Omega^{r-1}(M)$ but this doesn't make sense for $f$. I suppose that we take the convention that $\iota(X)f$ is zero.",,"['differential-geometry', 'differential-forms']"
18,Divergence-free vectorfield has volume-preserving flow,Divergence-free vectorfield has volume-preserving flow,,"Recently I read that divergence-free vectorfields give rise to volume-preserving flows, but I fail to prove this statement. Let $M$ be an oriented,finite dimensional, smooth manifold equipped with a volume form $\omega$. Furthermore let $X$ be a divergence-free vectorfield on $M$ with respect to $\omega$ with compact support. We know that the vectorfield $X$ gives rise to a global flow $\phi_t(x)$. Claim:  This flow preserves the volumeform, i.e. for any fixed time $t\in \mathbb{R}$ the smooth function $\phi_t:M\rightarrow M$ fulfils $\phi^{*}_t \omega=\omega$, where $f^{*}$ denotes the pullback of a form for a smooth function $f:M\rightarrow N$ between two smooth manifolds $M$ and $N$. Attempt: So far I tried to work in local coordiantes, hoping that a straightforward calculation yields the desired result. In local coordinates we can write $\omega_x=f(x) dx^1 \wedge \dots \wedge dx^n$ for a smooth function $f:U\subset M\rightarrow \mathbb{R}$. Then: $\phi^{*}_t \omega_x=f(\phi_t(x)) d\phi^1_t \wedge \dots \wedge d\phi^n_t$, where $\phi^i_t=x^i\circ \phi_t$ and $x^i$ are the coordinate functions. Further $d\phi^1_t=\partial_i \phi^1_t dx^i$ (using the summation convention) and so on: $\Rightarrow \phi^{*}_t \omega_x=f(\phi_t(x)) \partial_{i_1}\phi^1_t \dots \partial_{i_n}\phi^n_t dx^{i_1} \wedge \dots \wedge dx^{i_n}=f(\phi_t(x)) \epsilon^{i_1 i_2\dots i_n}\partial_{i_1}\phi^1_t \dots \partial_{i_n}\phi^n_t dx^1\wedge \dots \wedge dx^n$ We want this to be equal to $\omega_x$ and so we need to show that $f(x)=f(\phi_t(x)) \epsilon^{i_1 i_2\dots i_n}\partial_{i_1}\phi^1_t \dots \partial_{i_n}\phi^n_t$. Since $X$ is divergence free, we have: $0=L_X \omega=d \iota_X \omega$, by Cartans magic formula and since $\omega$ is an $n$-form and therefore closed. Now to establish a connection between $X$ and $\phi_t$ we need to exploit the fact that $\phi_t$ is its flow: $\frac{d}{dt}\phi_t= X(\phi_t)$. In particular $\frac{d}{dt}\phi^i_t(x)=X(\phi_t(x))(x^i)$ for all $x\in M$. My problem now is that in order to establish a connection between the flow and the vectorfield, we need the time derivative of the flow, which does not occur in the calculations above so far. So how exactly can I exploit this connection? If it is of any help, we may assume that the manifold $M$ is compact and Riemannian and $\omega$ the Riemannian volume form. Thanks a lot in advance!","Recently I read that divergence-free vectorfields give rise to volume-preserving flows, but I fail to prove this statement. Let $M$ be an oriented,finite dimensional, smooth manifold equipped with a volume form $\omega$. Furthermore let $X$ be a divergence-free vectorfield on $M$ with respect to $\omega$ with compact support. We know that the vectorfield $X$ gives rise to a global flow $\phi_t(x)$. Claim:  This flow preserves the volumeform, i.e. for any fixed time $t\in \mathbb{R}$ the smooth function $\phi_t:M\rightarrow M$ fulfils $\phi^{*}_t \omega=\omega$, where $f^{*}$ denotes the pullback of a form for a smooth function $f:M\rightarrow N$ between two smooth manifolds $M$ and $N$. Attempt: So far I tried to work in local coordiantes, hoping that a straightforward calculation yields the desired result. In local coordinates we can write $\omega_x=f(x) dx^1 \wedge \dots \wedge dx^n$ for a smooth function $f:U\subset M\rightarrow \mathbb{R}$. Then: $\phi^{*}_t \omega_x=f(\phi_t(x)) d\phi^1_t \wedge \dots \wedge d\phi^n_t$, where $\phi^i_t=x^i\circ \phi_t$ and $x^i$ are the coordinate functions. Further $d\phi^1_t=\partial_i \phi^1_t dx^i$ (using the summation convention) and so on: $\Rightarrow \phi^{*}_t \omega_x=f(\phi_t(x)) \partial_{i_1}\phi^1_t \dots \partial_{i_n}\phi^n_t dx^{i_1} \wedge \dots \wedge dx^{i_n}=f(\phi_t(x)) \epsilon^{i_1 i_2\dots i_n}\partial_{i_1}\phi^1_t \dots \partial_{i_n}\phi^n_t dx^1\wedge \dots \wedge dx^n$ We want this to be equal to $\omega_x$ and so we need to show that $f(x)=f(\phi_t(x)) \epsilon^{i_1 i_2\dots i_n}\partial_{i_1}\phi^1_t \dots \partial_{i_n}\phi^n_t$. Since $X$ is divergence free, we have: $0=L_X \omega=d \iota_X \omega$, by Cartans magic formula and since $\omega$ is an $n$-form and therefore closed. Now to establish a connection between $X$ and $\phi_t$ we need to exploit the fact that $\phi_t$ is its flow: $\frac{d}{dt}\phi_t= X(\phi_t)$. In particular $\frac{d}{dt}\phi^i_t(x)=X(\phi_t(x))(x^i)$ for all $x\in M$. My problem now is that in order to establish a connection between the flow and the vectorfield, we need the time derivative of the flow, which does not occur in the calculations above so far. So how exactly can I exploit this connection? If it is of any help, we may assume that the manifold $M$ is compact and Riemannian and $\omega$ the Riemannian volume form. Thanks a lot in advance!",,"['differential-geometry', 'differential-topology', 'vector-fields']"
19,Dimension factor in definition of the Weyl tensor,Dimension factor in definition of the Weyl tensor,,"tl;dr Where do the $\frac{1}{n-2}$ and $\frac{1}{(n-1)(n-2)}$ factors come from in the definition of the Weyl tensor? The Weyl tensor is the trace-free component of the Riemann curvature tensor. Therefore, it can be simply computed by subtracting the traces. In dimension $1$, the Riemann curvature vanishes by its symmetries so the trace free part also vanishes. The Weyl tensor also vanishes in dimension $2$ and $3$, as at least two indices are always equal, so the Riemann curvature is entirely described by its traces. For dimensions $n \geq 3$, (apparently) $W$ is given by $$W_{ijkl} = R_{ijkl} - \frac{1}{n-2}\left(-g_{jl}R_{ik}+ g_{jk}R_{il}+ g_{il}R_{jk}- g_{ik}R_{jl}\right) - \frac{1}{(n-1)(n-2)}\left(g_{ik}g_{jl}R - g_{il}g_{jk}R\right)$$ However, I don't understand where the $\frac{1}{n-2}$ and $\frac{1}{(n-1)(n-2)}$ factors come from. Why is the formula not $$W_{ijkl} = R_{ijkl} - \frac{1}{n}\left(-g_{jl}R_{ik}+ g_{jk}R_{il}+ g_{il}R_{jk}- g_{ik}R_{jl}\right) - \frac{1}{n^2}\left(g_{ik}g_{jl}R - g_{il}g_{jk}R\right)$$ This seems sensible to me as you are dividing by the factor of the trace of the metric.","tl;dr Where do the $\frac{1}{n-2}$ and $\frac{1}{(n-1)(n-2)}$ factors come from in the definition of the Weyl tensor? The Weyl tensor is the trace-free component of the Riemann curvature tensor. Therefore, it can be simply computed by subtracting the traces. In dimension $1$, the Riemann curvature vanishes by its symmetries so the trace free part also vanishes. The Weyl tensor also vanishes in dimension $2$ and $3$, as at least two indices are always equal, so the Riemann curvature is entirely described by its traces. For dimensions $n \geq 3$, (apparently) $W$ is given by $$W_{ijkl} = R_{ijkl} - \frac{1}{n-2}\left(-g_{jl}R_{ik}+ g_{jk}R_{il}+ g_{il}R_{jk}- g_{ik}R_{jl}\right) - \frac{1}{(n-1)(n-2)}\left(g_{ik}g_{jl}R - g_{il}g_{jk}R\right)$$ However, I don't understand where the $\frac{1}{n-2}$ and $\frac{1}{(n-1)(n-2)}$ factors come from. Why is the formula not $$W_{ijkl} = R_{ijkl} - \frac{1}{n}\left(-g_{jl}R_{ik}+ g_{jk}R_{il}+ g_{il}R_{jk}- g_{ik}R_{jl}\right) - \frac{1}{n^2}\left(g_{ik}g_{jl}R - g_{il}g_{jk}R\right)$$ This seems sensible to me as you are dividing by the factor of the trace of the metric.",,"['differential-geometry', 'tensors', 'curvature', 'conformal-geometry']"
20,Holomorphic symplectic manifold,Holomorphic symplectic manifold,,"I have two basic questions regarding hyperkaehler manifolds. A holomorphic symplectic manifold is a complex manifold $X$ endowed with a $(2,0)$ -form $\omega$ . I know that a Hyperkaehler manifold can be seen as a holomorphic symplectic manifold. But is the converse also true? Is every holomorphic symplectic manifold always Hyperkaehler? If not, can you give a counterexample? Does a Hyperkaehler manifold possess a natural polarization? What is it? Thanks in advance for your answers.","I have two basic questions regarding hyperkaehler manifolds. A holomorphic symplectic manifold is a complex manifold endowed with a -form . I know that a Hyperkaehler manifold can be seen as a holomorphic symplectic manifold. But is the converse also true? Is every holomorphic symplectic manifold always Hyperkaehler? If not, can you give a counterexample? Does a Hyperkaehler manifold possess a natural polarization? What is it? Thanks in advance for your answers.","X (2,0) \omega","['differential-geometry', 'complex-geometry', 'symplectic-geometry', 'kahler-manifolds']"
21,Riemannian geometry identity,Riemannian geometry identity,,"Let $(M,g)$ be a Riemannian manifold and $f\in C^{\infty}(M)$. Why does the equality $$\nabla^j\nabla_i\nabla_jf-\nabla_i\nabla^j\nabla_jf=R_{ij}\nabla^jf$$ hold? I tried writing $$\nabla^j\nabla_i-\nabla_i\nabla^j=g^{jk}(\nabla_k\nabla_i-\nabla_i\nabla_k)=g^{jk}R(e_k,e_i)$$ assuming $[e_i,e_j]=0$, but now I'm stuck.","Let $(M,g)$ be a Riemannian manifold and $f\in C^{\infty}(M)$. Why does the equality $$\nabla^j\nabla_i\nabla_jf-\nabla_i\nabla^j\nabla_jf=R_{ij}\nabla^jf$$ hold? I tried writing $$\nabla^j\nabla_i-\nabla_i\nabla^j=g^{jk}(\nabla_k\nabla_i-\nabla_i\nabla_k)=g^{jk}R(e_k,e_i)$$ assuming $[e_i,e_j]=0$, but now I'm stuck.",,"['differential-geometry', 'riemannian-geometry', 'curvature']"
22,Software for computing geometirc data of parametric surfaces,Software for computing geometirc data of parametric surfaces,,Is there any (possibly free) software suitable for computing geometric quantities of parametric surfaces? Any suggestion will be very appreciated.,Is there any (possibly free) software suitable for computing geometric quantities of parametric surfaces? Any suggestion will be very appreciated.,,"['differential-geometry', 'math-software']"
23,Proof of Hopf's theorem (the one about cohomology of Lie groups being equal to the cohomology of a product of spheres),Proof of Hopf's theorem (the one about cohomology of Lie groups being equal to the cohomology of a product of spheres),,"Theorem (H. Hopf). Let $G$ be a compact connected Lie group. Then $G$ has the real cohomology of a product of odd dimensional spheres,   $H(G,\mathbb{R}) \approx H(\prod_q S^{2k_q - 1},\mathbb{R})$. The only place I could find the proof of this theorem was the book by Greub, Connections, curvature and cohomology . Do you guys know where else can I find a proof of this theorem? The original papers are in german and french, and I couldn't find any translations. Thanks in advance.","Theorem (H. Hopf). Let $G$ be a compact connected Lie group. Then $G$ has the real cohomology of a product of odd dimensional spheres,   $H(G,\mathbb{R}) \approx H(\prod_q S^{2k_q - 1},\mathbb{R})$. The only place I could find the proof of this theorem was the book by Greub, Connections, curvature and cohomology . Do you guys know where else can I find a proof of this theorem? The original papers are in german and french, and I couldn't find any translations. Thanks in advance.",,"['differential-geometry', 'reference-request', 'algebraic-topology', 'lie-groups', 'homology-cohomology']"
24,About the definition of Ehresmann connection,About the definition of Ehresmann connection,,"Jeffrey Lee in his book ""Manifolds and differential geometry"" defines the notion of Ehresmann connection as Definition 12.12. A (linear Ehresmann) connection on a vector bundle $\pi: E \to M$ is a smooth distribution $\mathcal{H}$ on the total space $E$ such that $\mathcal{H}$ is complementary to the vertical bundle: $TE = \mathcal{H} \oplus \mathcal{V} \ E$ $\mathcal{H}$ is homogenous: $T_y \mu_r (\mathcal{H}_y) = \mathcal{H}_{ry}$, where $\mu_r: E \to E $ is the multiplication map. He also gives it as an exercise to prove that there is a bijection between such connections and covariant derivatives (which he calls Koszul connections). However M. Postnikov in his book ""Semester IV. Differential Geometry"" gives a definition using local-coordinate representation of $\mathcal{H}$, such that the last condition above is replaced by: the condition that in the trivialization $E|_U \cong U \times V$, with $V$ - vector space, and $x_1,...,x_n$ - coordinates on the base,  $a_1,...,a_m$ - coordinates on the fiber, the forms $\theta_i$ defining $\mathcal{H}=Ann(\{\theta_i\})$, have the form $$\theta_i=da^i + \Gamma_{kj}^{i}(x) a^j dx^k \ \ (*)$$or in other words that if $$\theta_i=da^i + e_k^i(x,a)dx^k \ \ (**)$$ then functions $e_k^i(x,a)$ must be linear in the coordinates on the fiber. Now, it is easy to see that Postnikov's definition satisfies all the conditions of Lee's definition. However if one tries to write coordinate representation of Lee's definition, one can get to the form $(**)$ with functions $e_k^i(x,a)$ being only homogenous in the coordinates on the fiber. M. Postnikov then uses his second condition in the explicit construction showing the equivalence of his notion of connection to the usual covariant derivate   - basically, $\Gamma_{kj}^{i}(x) dx^k$ give connection forms $\omega_j^i$ on the base. He also later emphasizes, that, contrary to the case of principle bundles, one cannot make an easy coordinate free definition in the case of vector bundles. From that I suspect that it might be actually impossible to retrieve a covariant derivative out of Lee's definition - there seems to be no way of separating coordinates on the base from coordinates on the fiber in $e_k^i(x,a)dx^k$ to get connection forms on the base. So can you please clarify what is the right take on the notion of Ehresmann connection? Is there a clean coordinate-free way of defining it similar to Lee's approach, contrary to what M. Postnikov is suggesting? Remark. I am familiar with an approach of passing to the frame bundle and defining there a connection using an equivariant fundamental form. I am curious if it is possible to work only on the vector bundle itself.","Jeffrey Lee in his book ""Manifolds and differential geometry"" defines the notion of Ehresmann connection as Definition 12.12. A (linear Ehresmann) connection on a vector bundle $\pi: E \to M$ is a smooth distribution $\mathcal{H}$ on the total space $E$ such that $\mathcal{H}$ is complementary to the vertical bundle: $TE = \mathcal{H} \oplus \mathcal{V} \ E$ $\mathcal{H}$ is homogenous: $T_y \mu_r (\mathcal{H}_y) = \mathcal{H}_{ry}$, where $\mu_r: E \to E $ is the multiplication map. He also gives it as an exercise to prove that there is a bijection between such connections and covariant derivatives (which he calls Koszul connections). However M. Postnikov in his book ""Semester IV. Differential Geometry"" gives a definition using local-coordinate representation of $\mathcal{H}$, such that the last condition above is replaced by: the condition that in the trivialization $E|_U \cong U \times V$, with $V$ - vector space, and $x_1,...,x_n$ - coordinates on the base,  $a_1,...,a_m$ - coordinates on the fiber, the forms $\theta_i$ defining $\mathcal{H}=Ann(\{\theta_i\})$, have the form $$\theta_i=da^i + \Gamma_{kj}^{i}(x) a^j dx^k \ \ (*)$$or in other words that if $$\theta_i=da^i + e_k^i(x,a)dx^k \ \ (**)$$ then functions $e_k^i(x,a)$ must be linear in the coordinates on the fiber. Now, it is easy to see that Postnikov's definition satisfies all the conditions of Lee's definition. However if one tries to write coordinate representation of Lee's definition, one can get to the form $(**)$ with functions $e_k^i(x,a)$ being only homogenous in the coordinates on the fiber. M. Postnikov then uses his second condition in the explicit construction showing the equivalence of his notion of connection to the usual covariant derivate   - basically, $\Gamma_{kj}^{i}(x) dx^k$ give connection forms $\omega_j^i$ on the base. He also later emphasizes, that, contrary to the case of principle bundles, one cannot make an easy coordinate free definition in the case of vector bundles. From that I suspect that it might be actually impossible to retrieve a covariant derivative out of Lee's definition - there seems to be no way of separating coordinates on the base from coordinates on the fiber in $e_k^i(x,a)dx^k$ to get connection forms on the base. So can you please clarify what is the right take on the notion of Ehresmann connection? Is there a clean coordinate-free way of defining it similar to Lee's approach, contrary to what M. Postnikov is suggesting? Remark. I am familiar with an approach of passing to the frame bundle and defining there a connection using an equivariant fundamental form. I am curious if it is possible to work only on the vector bundle itself.",,"['differential-geometry', 'vector-bundles', 'connections']"
25,Tangent vectors on manifolds,Tangent vectors on manifolds,,"I have an elementary understanding of differential geometry, and I know that the concept of a vector on a manifold can be defined in several different ways. Perhaps the easiest to understand, is in terms of equivalence classes of curves $\gamma:M\rightarrow\mathbb{R}^{n}$ on a manifold $M$. In this case, vectors are defined at each point $p\in M$, as tangent vectors to curves at that point, ""living"" in the tangent space $T_{p}M$ to that point. A tangent vector at a point $p\in M$ is then an equivalence class of curves, mutually tangent, at that point. Another way is to construct the notion of a vector using derivations. My question is (and apologies if it's a silly one), can there exist vectors $v$ in a given tangent space $T_{p}M$ that are not tangent to a curve passing through $p$ (essentially, are there cases where $v^{i}\neq\frac{\mathrm{d}x^{i}}{\mathrm{d}t}$)?","I have an elementary understanding of differential geometry, and I know that the concept of a vector on a manifold can be defined in several different ways. Perhaps the easiest to understand, is in terms of equivalence classes of curves $\gamma:M\rightarrow\mathbb{R}^{n}$ on a manifold $M$. In this case, vectors are defined at each point $p\in M$, as tangent vectors to curves at that point, ""living"" in the tangent space $T_{p}M$ to that point. A tangent vector at a point $p\in M$ is then an equivalence class of curves, mutually tangent, at that point. Another way is to construct the notion of a vector using derivations. My question is (and apologies if it's a silly one), can there exist vectors $v$ in a given tangent space $T_{p}M$ that are not tangent to a curve passing through $p$ (essentially, are there cases where $v^{i}\neq\frac{\mathrm{d}x^{i}}{\mathrm{d}t}$)?",,"['differential-geometry', 'smooth-manifolds', 'tangent-bundle']"
26,Geodesic equation and arclength parametrization,Geodesic equation and arclength parametrization,,"Let $(M,g)$ be a Riemannian manifold and $\gamma\subset M$ a curve in $M$. Suppose I would like to show that $\gamma$ satisfies the geodesic equations $$\ddot{x}^k=-\Gamma_{ij}^k\dot{x}^i\dot{x}^j.$$ If $\gamma$ is not parametrized by arc length, $\gamma$ may not satisfy the geodesic equations, as this example demonstrates: Why is $\gamma(t)=(0,t)$ a geodesic in the hyperbolic plane? My question is why a geodesic, which is not parametrized by arc length, may not satisfy the geodesic equations (as in the above example). Do Carmo derives the geodesic equations on page 62 and states that a curve in $M$ is a geodesic iff it satisfies the geodesic equation. Where does he assume that the curve is parametrized by arc length?","Let $(M,g)$ be a Riemannian manifold and $\gamma\subset M$ a curve in $M$. Suppose I would like to show that $\gamma$ satisfies the geodesic equations $$\ddot{x}^k=-\Gamma_{ij}^k\dot{x}^i\dot{x}^j.$$ If $\gamma$ is not parametrized by arc length, $\gamma$ may not satisfy the geodesic equations, as this example demonstrates: Why is $\gamma(t)=(0,t)$ a geodesic in the hyperbolic plane? My question is why a geodesic, which is not parametrized by arc length, may not satisfy the geodesic equations (as in the above example). Do Carmo derives the geodesic equations on page 62 and states that a curve in $M$ is a geodesic iff it satisfies the geodesic equation. Where does he assume that the curve is parametrized by arc length?",,"['differential-geometry', 'riemannian-geometry']"
27,Intuitive meaning of attitude error function $\Psi$ defined over $SO(3)$. Is $\Psi$ a metric?,Intuitive meaning of attitude error function  defined over . Is  a metric?,\Psi SO(3) \Psi,"We define the attitude error function $\Psi(R, R_d)$ over $SO(3)$ as : $$ \Psi(R, R_d) = \frac{1}{2}tr(I - R_d^{\top}R)$$ This acts as a metric to define distance between two rotation matrices which otherwise can't be calculated using euclidean vector space subtraction. But we also know that $\Psi = \mathbb{cos(\phi)}$, where $\mathbb{\phi}$ is the rotation angle about the rotation axis as obtained through the axis-angle representation of rotation matrices. I wanted to ask if $\Psi$ is actually a metric (in a formal sense) or not? Meaning does it satisfy the triangle inequality : $$ \Psi(I, R) \le \Psi(I,R_d) + \Psi(R_d,R)$$ I tried working it out, also did brute force expansion, but couldn't really come to a conclusion. Since $\Psi$ corresponds to the rotation angle, I feel like this should be true as we can see by keeping an axis fixed, say $e_1$ and perform 2 successive rotations about the same axis corresponding to $R_d$ and $R$ respectively. I have very elementary knowledge regarding the above. Any help is appreciated.","We define the attitude error function $\Psi(R, R_d)$ over $SO(3)$ as : $$ \Psi(R, R_d) = \frac{1}{2}tr(I - R_d^{\top}R)$$ This acts as a metric to define distance between two rotation matrices which otherwise can't be calculated using euclidean vector space subtraction. But we also know that $\Psi = \mathbb{cos(\phi)}$, where $\mathbb{\phi}$ is the rotation angle about the rotation axis as obtained through the axis-angle representation of rotation matrices. I wanted to ask if $\Psi$ is actually a metric (in a formal sense) or not? Meaning does it satisfy the triangle inequality : $$ \Psi(I, R) \le \Psi(I,R_d) + \Psi(R_d,R)$$ I tried working it out, also did brute force expansion, but couldn't really come to a conclusion. Since $\Psi$ corresponds to the rotation angle, I feel like this should be true as we can see by keeping an axis fixed, say $e_1$ and perform 2 successive rotations about the same axis corresponding to $R_d$ and $R$ respectively. I have very elementary knowledge regarding the above. Any help is appreciated.",,"['general-topology', 'differential-geometry', 'manifolds', 'error-function']"
28,"""Continuity"" of volume function on hyperbolic tetrahedra","""Continuity"" of volume function on hyperbolic tetrahedra",,"Consider a sequence $T_i$ of tetrahedra in $\mathbb H^3$ whose   vertices tend to the vertices of a regular ideal tetrahedron $T$ in   $\partial \mathbb H^3$. Then $$Vol(T_i)\to v_3.$$ This should follow from Lebesgue dominated convergence if $T_i\subseteq T_{i+1}$ for (almost) all $i$, since, calling $\nu$ the volume form on $\mathbb H^3$, $$|\nu\chi_{T_i}|\leq|\nu\chi_T|$$ so the integrals converge. I think one can always suppose to be in this case by moving the $T_i$ by isometries: is this true? Is there a formally satisfying way to see it? Thank you in advance.","Consider a sequence $T_i$ of tetrahedra in $\mathbb H^3$ whose   vertices tend to the vertices of a regular ideal tetrahedron $T$ in   $\partial \mathbb H^3$. Then $$Vol(T_i)\to v_3.$$ This should follow from Lebesgue dominated convergence if $T_i\subseteq T_{i+1}$ for (almost) all $i$, since, calling $\nu$ the volume form on $\mathbb H^3$, $$|\nu\chi_{T_i}|\leq|\nu\chi_T|$$ so the integrals converge. I think one can always suppose to be in this case by moving the $T_i$ by isometries: is this true? Is there a formally satisfying way to see it? Thank you in advance.",,"['differential-geometry', 'hyperbolic-geometry', 'geometric-topology']"
29,Integral foliation identity,Integral foliation identity,,"I've been trying for days to solve the following identity, but feel like I really need a hint on how to start since I'm not yet much into how to work with submanifolds. Let $U\in \mathbb R^n $ and $f: U \to \mathbb R$ continuously differentiable with $df_x \neq 0$ for all $x\in U$ so that, for each $t \in \mathbb R$, $M_t := f^{-1} (\{ t \})$ is a $(n-1)$-dimensional submanifold of $\mathbb R^n$. Then, for every non-negative, $\mathcal B(U)$-measureable map $g: U \to \mathbb R$: $$\int_U g \,d\lambda_n = \int_\mathbb R \int_{M_t} \frac{g(x)}{\lVert \nabla f(x) \rVert} \omega^{M_t}(dx)\,dt,$$   where $\omega^{M_t}$ is the surface measure on $M_t$ defined by   $$\omega^{M_t}(A) := \omega^{\phi^*\sigma}(\phi ^{-1} (A)) := \int_{\phi ^{-1}(A) }\sqrt{\det (\phi^*\sigma)_x} \, \lambda_n(dx)$$   for a parametrization $\phi$ and the standard inner product $\sigma$. My attempt so far: My first idea was to apply the implicit function theorem to find a unique function with $f(x, g(x)) = t$ for each $x$ and some neighbourhood $U_x$ and then apply Fubini to split the left hand side in two integrals. However, I lack technical knowledge to do that. My other attempt was to use a characterization of submanifolds: Fix $x \in M_t$. We can find a neighbourhood $x \in U_x \subseteq U$ and a $C^1$-diffeomorphism $F: U_x \to W_x$, $W_x \subseteq \mathbb R^n$ open neighbourhood of $0$. However, in this case, I don't know of suitable ways to split up the integral. Any help appreciated.","I've been trying for days to solve the following identity, but feel like I really need a hint on how to start since I'm not yet much into how to work with submanifolds. Let $U\in \mathbb R^n $ and $f: U \to \mathbb R$ continuously differentiable with $df_x \neq 0$ for all $x\in U$ so that, for each $t \in \mathbb R$, $M_t := f^{-1} (\{ t \})$ is a $(n-1)$-dimensional submanifold of $\mathbb R^n$. Then, for every non-negative, $\mathcal B(U)$-measureable map $g: U \to \mathbb R$: $$\int_U g \,d\lambda_n = \int_\mathbb R \int_{M_t} \frac{g(x)}{\lVert \nabla f(x) \rVert} \omega^{M_t}(dx)\,dt,$$   where $\omega^{M_t}$ is the surface measure on $M_t$ defined by   $$\omega^{M_t}(A) := \omega^{\phi^*\sigma}(\phi ^{-1} (A)) := \int_{\phi ^{-1}(A) }\sqrt{\det (\phi^*\sigma)_x} \, \lambda_n(dx)$$   for a parametrization $\phi$ and the standard inner product $\sigma$. My attempt so far: My first idea was to apply the implicit function theorem to find a unique function with $f(x, g(x)) = t$ for each $x$ and some neighbourhood $U_x$ and then apply Fubini to split the left hand side in two integrals. However, I lack technical knowledge to do that. My other attempt was to use a characterization of submanifolds: Fix $x \in M_t$. We can find a neighbourhood $x \in U_x \subseteq U$ and a $C^1$-diffeomorphism $F: U_x \to W_x$, $W_x \subseteq \mathbb R^n$ open neighbourhood of $0$. However, in this case, I don't know of suitable ways to split up the integral. Any help appreciated.",,"['real-analysis', 'integration', 'differential-geometry', 'manifolds', 'lebesgue-integral']"
30,Good reference book for fibre bundles and principal bundles,Good reference book for fibre bundles and principal bundles,,"I want to read the theory of fibre bundle, vector bundle and principal G bundle. Also about classifying spaces. Can someone suggest me good references for this material.","I want to read the theory of fibre bundle, vector bundle and principal G bundle. Also about classifying spaces. Can someone suggest me good references for this material.",,"['differential-geometry', 'algebraic-topology', 'differential-topology']"
31,Intrinsic vs. Extrinsic notions of Harmonic maps.,Intrinsic vs. Extrinsic notions of Harmonic maps.,,"I have just began studying the theory of harmonic maps between manifolds recently. So far, I've come across 2 ways to define harmonic maps between (Riemannian) manifolds. Let $u:(M,\gamma)\to(N,g)$ be a map, says $C^1$ for simplicity, between 2 Riemannian manifolds of dimension $m$ and $n$ respectively. The metrics on them are given by $\gamma=(\gamma_{\alpha\beta})$ and $g=(g_{ij})$. Intrinsic definition : $u$ is said to be harmonic if, in coordinate chart,   $$ \Delta u^k=\gamma^{\alpha\beta}\Gamma^k_{ij} \frac{\partial u^{i}}{\partial x^{\alpha}}\frac{\partial u^{j}}{\partial x^{\beta}} $$   where $(\gamma^{\alpha\beta})=(\gamma_{\alpha\beta})^{-1}$. On the other hand, by Nash embedding theorem, we can view $N$ as an isometrically embedded submanifold of $\Bbb R^K$ some large $K$. In this case, we may define harmonicity as follow. Extrinsic definition : $u$ is said to be harmonic if   $$\begin{align} \Delta u =& A(u)(\nabla u,\nabla u)  \\ :=& \sum_{\alpha=1}^m A(u)(\frac{\partial u}{\partial x^{\alpha}},\frac{\partial u}{\partial x^{\alpha}}) \end{align}$$   where $A$ is the second fundamental form of $N$ in $\Bbb R^k$. Here are some of my questions: -In both cases, $\Delta$ is the Laplace-Beltrami operator right? -For the extrinsic definition, is $\frac{\partial u}{\partial x^{\alpha}}$ just the usual $(\frac{\partial u^1}{\partial x^{\alpha}},\dots,\frac{\partial u^K}{\partial x^{\alpha}})\in \Bbb R^K$? -How do we show that both expressions are equivalent? PS. My background is mostly in analysis (in Euclidean spaces) with some basic knowledge of Riemannian geometry and differential geometry. Note that I'm not quite good with differential geometry yet, as you can see, so I wouldn't be able to understand if the answer is too advanced, e.g. something involving Hodge isomorphism.","I have just began studying the theory of harmonic maps between manifolds recently. So far, I've come across 2 ways to define harmonic maps between (Riemannian) manifolds. Let $u:(M,\gamma)\to(N,g)$ be a map, says $C^1$ for simplicity, between 2 Riemannian manifolds of dimension $m$ and $n$ respectively. The metrics on them are given by $\gamma=(\gamma_{\alpha\beta})$ and $g=(g_{ij})$. Intrinsic definition : $u$ is said to be harmonic if, in coordinate chart,   $$ \Delta u^k=\gamma^{\alpha\beta}\Gamma^k_{ij} \frac{\partial u^{i}}{\partial x^{\alpha}}\frac{\partial u^{j}}{\partial x^{\beta}} $$   where $(\gamma^{\alpha\beta})=(\gamma_{\alpha\beta})^{-1}$. On the other hand, by Nash embedding theorem, we can view $N$ as an isometrically embedded submanifold of $\Bbb R^K$ some large $K$. In this case, we may define harmonicity as follow. Extrinsic definition : $u$ is said to be harmonic if   $$\begin{align} \Delta u =& A(u)(\nabla u,\nabla u)  \\ :=& \sum_{\alpha=1}^m A(u)(\frac{\partial u}{\partial x^{\alpha}},\frac{\partial u}{\partial x^{\alpha}}) \end{align}$$   where $A$ is the second fundamental form of $N$ in $\Bbb R^k$. Here are some of my questions: -In both cases, $\Delta$ is the Laplace-Beltrami operator right? -For the extrinsic definition, is $\frac{\partial u}{\partial x^{\alpha}}$ just the usual $(\frac{\partial u^1}{\partial x^{\alpha}},\dots,\frac{\partial u^K}{\partial x^{\alpha}})\in \Bbb R^K$? -How do we show that both expressions are equivalent? PS. My background is mostly in analysis (in Euclidean spaces) with some basic knowledge of Riemannian geometry and differential geometry. Note that I'm not quite good with differential geometry yet, as you can see, so I wouldn't be able to understand if the answer is too advanced, e.g. something involving Hodge isomorphism.",,"['differential-geometry', 'partial-differential-equations', 'riemannian-geometry']"
32,Proving the Lie derivative of a vector field with respect to another is a smooth vector field,Proving the Lie derivative of a vector field with respect to another is a smooth vector field,,"Suppose $M$ is a smooth manifold and $V,W$ are smooth vector fields on $M$. Let $L_V W$ denote the Lie derivative of $W$ with respect to $V$. Then $(L_V W)_p$ exists for every $p \in M$ and $(L_V W)$ is a smooth vector field. The proof begins as follows: Let $\theta$ be the flow of $V$. For arbitrary $p \in M$, let $(U,(x^i))$ be a smooth chart containing $p$. Choose an open interval $J_0$ containing 0 and an open subset $U_0 \subset U$ contatining $p$ such that $\theta$ maps $J_0 \times U_0$ into $U$. For $(t,x) \in J_0 \times U_0$, write the component functions of $\theta$ as $(\theta^1(t,x)...\theta^n(t,x))$. Then for any $(t,x) \in J_0 \times U_0$, the matrix of $d(\theta_{-t})_{\theta_{t}(x)}: T_{\theta_{t}(x)}M \rightarrow T_xM$ is $\bigg(\frac{\partial \theta^i}{\partial x^j}(-t,\theta(t,x))\bigg)$ I am confused how to compute this matrix. In particular, I know that the matrix of a map is the Jacobian of the (coordinate representation) of the map. But in particular how do I arrive at it being evaluated at $(-t,\theta(t,x))$? Help would be greatly appreciated !","Suppose $M$ is a smooth manifold and $V,W$ are smooth vector fields on $M$. Let $L_V W$ denote the Lie derivative of $W$ with respect to $V$. Then $(L_V W)_p$ exists for every $p \in M$ and $(L_V W)$ is a smooth vector field. The proof begins as follows: Let $\theta$ be the flow of $V$. For arbitrary $p \in M$, let $(U,(x^i))$ be a smooth chart containing $p$. Choose an open interval $J_0$ containing 0 and an open subset $U_0 \subset U$ contatining $p$ such that $\theta$ maps $J_0 \times U_0$ into $U$. For $(t,x) \in J_0 \times U_0$, write the component functions of $\theta$ as $(\theta^1(t,x)...\theta^n(t,x))$. Then for any $(t,x) \in J_0 \times U_0$, the matrix of $d(\theta_{-t})_{\theta_{t}(x)}: T_{\theta_{t}(x)}M \rightarrow T_xM$ is $\bigg(\frac{\partial \theta^i}{\partial x^j}(-t,\theta(t,x))\bigg)$ I am confused how to compute this matrix. In particular, I know that the matrix of a map is the Jacobian of the (coordinate representation) of the map. But in particular how do I arrive at it being evaluated at $(-t,\theta(t,x))$? Help would be greatly appreciated !",,"['differential-geometry', 'lie-derivative']"
33,finding a differential curve tangent to a distribution [closed],finding a differential curve tangent to a distribution [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I have the following distribution on $\mathbb{R}^3$ $${\cal{D}}_{(x,y,z)} = \langle\{\partial_x,\partial_y + x\partial_z\}\rangle$$ I want to show that for any $(x,y,z)$ in $\mathbb{R}^3$, there exists a path $\gamma$ from $0$ to $(x,y,z)$ tangent to $\cal{D}$ i.e. $\dot{\gamma}(t)\in{\cal{D}}_{\gamma(t)}$ Any hints?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I have the following distribution on $\mathbb{R}^3$ $${\cal{D}}_{(x,y,z)} = \langle\{\partial_x,\partial_y + x\partial_z\}\rangle$$ I want to show that for any $(x,y,z)$ in $\mathbb{R}^3$, there exists a path $\gamma$ from $0$ to $(x,y,z)$ tangent to $\cal{D}$ i.e. $\dot{\gamma}(t)\in{\cal{D}}_{\gamma(t)}$ Any hints?",,['differential-geometry']
34,How to Determine a clamped B-spline curve passes through a given point q,How to Determine a clamped B-spline curve passes through a given point q,,"Let P be a clamped B-spline curve of degree two defined by the control points, The control points are :    $\binom{-2}{-2},\binom{-2}{0},\binom{0}{2},\binom{2}{2},\binom{2}{0},\binom{0}{-2} $ and over the knot vector of $\tau$ := $ (0, 0, 0, \frac{1}{4},\frac{1}{2} ,\frac{3}{4}, 1, 1, 1)$. Does P pass through the point  $q := \Biggl(\begin{smallmatrix}     \frac{-1}{2} \\ \frac{1}{2} \end{smallmatrix} \Biggr)$  ? Could we get different results for other clamped (possibly non-uniform) knot vectors instead of $\tau $?","Let P be a clamped B-spline curve of degree two defined by the control points, The control points are :    $\binom{-2}{-2},\binom{-2}{0},\binom{0}{2},\binom{2}{2},\binom{2}{0},\binom{0}{-2} $ and over the knot vector of $\tau$ := $ (0, 0, 0, \frac{1}{4},\frac{1}{2} ,\frac{3}{4}, 1, 1, 1)$. Does P pass through the point  $q := \Biggl(\begin{smallmatrix}     \frac{-1}{2} \\ \frac{1}{2} \end{smallmatrix} \Biggr)$  ? Could we get different results for other clamped (possibly non-uniform) knot vectors instead of $\tau $?",,"['differential-geometry', 'curves', 'bezier-curve', 'spline']"
35,Proving $\nabla_X (\text{tr}\ F) = \text{tr} (\nabla_XF)$.,Proving .,\nabla_X (\text{tr}\ F) = \text{tr} (\nabla_XF),"Let $F\in\mathcal T^{k}_{l}(M)$ be a smooth section of a $(k,l)$-tensor bundle over the manifold $M$. The trace operator $$ \text{tr}:\mathcal T^{k}_{l}(M) \to\mathcal T^{k-1}_{l-1}(M) $$ is defined by $$ (\text{tr}\ F)(\omega^1,\dots,\omega^{l-1},Y_1,\dots,Y_{k-1}):=\text{tr}\left( F(\omega^1,\dots,\omega^{l-1},\omega,Y_1,\dots,Y_{k-1},Y) \right) $$ where trace on the RHS is the regular trace of the $(1,1)$-tensor in variables $\omega$ and $Y$. How do I show that $$\nabla_X (\text{tr} F) =  \text{tr} (\nabla_XF)$$   holds? Writing it out in coordinate seems like a real pain, is there a nicer way to prove this? I tried to do it in local coordinate and it looks quite messy. EDIT: The definition I am using is the computational one, i.e $$\begin{align} \nabla_X F(\omega^1,\dots,\omega^l,Y_1,\dots,Y_k):&=XF(\omega^1,\dots,Y_k)\\&-F(\nabla_X\omega^1,\omega^2,\dots,Y_k)-\dots -F(\omega^1,\dots,Y_{k-1},\nabla_X Y_k) \end{align}$$","Let $F\in\mathcal T^{k}_{l}(M)$ be a smooth section of a $(k,l)$-tensor bundle over the manifold $M$. The trace operator $$ \text{tr}:\mathcal T^{k}_{l}(M) \to\mathcal T^{k-1}_{l-1}(M) $$ is defined by $$ (\text{tr}\ F)(\omega^1,\dots,\omega^{l-1},Y_1,\dots,Y_{k-1}):=\text{tr}\left( F(\omega^1,\dots,\omega^{l-1},\omega,Y_1,\dots,Y_{k-1},Y) \right) $$ where trace on the RHS is the regular trace of the $(1,1)$-tensor in variables $\omega$ and $Y$. How do I show that $$\nabla_X (\text{tr} F) =  \text{tr} (\nabla_XF)$$   holds? Writing it out in coordinate seems like a real pain, is there a nicer way to prove this? I tried to do it in local coordinate and it looks quite messy. EDIT: The definition I am using is the computational one, i.e $$\begin{align} \nabla_X F(\omega^1,\dots,\omega^l,Y_1,\dots,Y_k):&=XF(\omega^1,\dots,Y_k)\\&-F(\nabla_X\omega^1,\omega^2,\dots,Y_k)-\dots -F(\omega^1,\dots,Y_{k-1},\nabla_X Y_k) \end{align}$$",,"['linear-algebra', 'differential-geometry', 'manifolds', 'riemannian-geometry', 'tensor-products']"
36,Is $\text{ker} (\delta_{\nabla^E}d_{\nabla^E})$ always non-zero?,Is  always non-zero?,\text{ker} (\delta_{\nabla^E}d_{\nabla^E}),"Let $E$ be a vector bundle over a smooth manifold $M$, equipped with a metric $\eta$ and a metric-compatible connection $\nabla$. Denote by $\delta_{\nabla^E}:\Omega^1(M,E) \to \Omega^{0}(M,E)=\Gamma(E)$ the adjoint of the connection $${\nabla^E}: \Gamma(E)\to \Omega^1(M,E).$$ Note that $\delta_{\nabla^E}\circ{\nabla^E}: \Gamma(E) \to  \Gamma(E)$. Question: Is $\text{ker} (\delta_{\nabla^E}\circ{\nabla^E})$ always non-zero? While it is known that $\text{ker} (\delta_{\nabla^E})$ is infinite-dimensional , it is not clear to me that there are non-zero elements in the $\text{ker} (\delta_{\nabla^E})$ which are in the image of $\nabla^E$. (Note: for a generic connection $\nabla^E$, $\text{ker}(\nabla^E)=0$. My motivation is trying to understand things about the minimizing properties of harmonic maps.","Let $E$ be a vector bundle over a smooth manifold $M$, equipped with a metric $\eta$ and a metric-compatible connection $\nabla$. Denote by $\delta_{\nabla^E}:\Omega^1(M,E) \to \Omega^{0}(M,E)=\Gamma(E)$ the adjoint of the connection $${\nabla^E}: \Gamma(E)\to \Omega^1(M,E).$$ Note that $\delta_{\nabla^E}\circ{\nabla^E}: \Gamma(E) \to  \Gamma(E)$. Question: Is $\text{ker} (\delta_{\nabla^E}\circ{\nabla^E})$ always non-zero? While it is known that $\text{ker} (\delta_{\nabla^E})$ is infinite-dimensional , it is not clear to me that there are non-zero elements in the $\text{ker} (\delta_{\nabla^E})$ which are in the image of $\nabla^E$. (Note: for a generic connection $\nabla^E$, $\text{ker}(\nabla^E)=0$. My motivation is trying to understand things about the minimizing properties of harmonic maps.",,"['differential-geometry', 'partial-differential-equations', 'riemannian-geometry', 'vector-bundles', 'adjoint-operators']"
37,The Differential of the Gauss map is the Negative of the Shape Operator?,The Differential of the Gauss map is the Negative of the Shape Operator?,,"$\newcommand{\sff}{II}$ $\newcommand{\mf}{\mathfrak}$ $\newcommand{\R}{\mathbf R}$ $\newcommand{\ab}[1]{\langle #1\rangle}$ Background Let $M$ be a Riemannian submanifold of a Riemannian manifold $\tilde M$ . Let $\tilde \nabla$ and $\nabla$ denote the Levi-Civita connections on $\tilde M$ and $M$ respectively. The $\tilde \nabla$ and $\nabla$ are related as follows: Let $X$ and $Y$ be smooth vector fields on $M$ . Then $\nabla_XY$ at a point $p\in M$ is the orthogonal projection of $\tilde \nabla_XY$ on $T_pM$ . The second fundamental form of $M$ is defined as the map $\sff:\mf X(M)\times \mf X(M)\to \mf X(M)$ which takes a pair of vector fields $(X, Y)$ to be the orthogonal projection of $\tilde \nabla_XY$ onto $T_pM^\perp$ . Thus $\tilde \nabla_X Y=\nabla_XY + \sff(X, Y)$ . Now suppose $M$ be a hypersurface in $\R^{n+1}$ , and $N$ be a smooth unit normal vector field along $M$ . Thus for any two smooth vector fields $X$ and $Y$ on $M$ , we have $\sff(X, Y)=h(X, Y)N$ , for some smooth map $h:\mf X(M)\times \mf X(M)\to \R$ . Define a map $G: M\to S^n$ as $G(q)=N_q$ for all $q\in M$ . The map $G$ is called the Gauss Map , and the Gaussian curvature of $M$ at $p$ is defined as $\det(dG_p)$ . Question In the theorem below, the minus sign attached to $h$ is bugging me. For this means that the shape operator is the negative of the differential of the Gauss map. While the shape operator is supposed to be the same as the differential of the Gauss map. I am unable to find a mistake in my proof. ""Theorem."" Let $M$ be a hypersurface in $\mathbf R^{n+1}$ , and let $G:M\to S^n$ be the Gauss map. Let $N$ be a unit normal vector field along $M$ . Then $\ab{dG_p u, v}= -h(u, v)$ for all $p\in M$ and $u, v\in T_pM$ . Proof. Let $\gamma:(-\varepsilon, \varepsilon)\to M$ be a smooth curve with $\gamma(0)=p$ and $\dot \gamma(0)=u$ . Let $V:I\to TM$ be the parallel vector field along $\gamma$ with $V(0)=v$ . Now we have $\ab{G\circ \gamma(t),\ V(t)}=0$ for all $t$ . Taking the derivative at $t=0$ , we have $$\ab{dG_p(\dot \gamma(0)),\ v} + \ab{G(\gamma(0)),\ \dot V(0)} =  0$$ Note that $\dot V$ is the covariant derivative of $V$ with respect to the Levi-Civita connection on $\R^{n+1}$ . Thus $\dot V(0) = D_{t}V(0) + h(\dot \gamma(0), V(0)) N_{\gamma(0)}$ . Substituting this in the equation above, we get $$\ab{dG_p(\dot \gamma(0)),\ v} + \ab{G(\gamma(0)),\ h(\dot \gamma(0), V(0))N_{\gamma(0)}} =  0$$ Putting $\dot \gamma(0)=u$ , $V(0)=v$ , and $G(\gamma(0))=N_p$ , we get $$\ab{dG_pu, v} + \ab{N_p,\ h(u, v) N_p} = \ab{dG_pu, v} + h(u, v) =  0$$ which proves the undesired result.","Background Let be a Riemannian submanifold of a Riemannian manifold . Let and denote the Levi-Civita connections on and respectively. The and are related as follows: Let and be smooth vector fields on . Then at a point is the orthogonal projection of on . The second fundamental form of is defined as the map which takes a pair of vector fields to be the orthogonal projection of onto . Thus . Now suppose be a hypersurface in , and be a smooth unit normal vector field along . Thus for any two smooth vector fields and on , we have , for some smooth map . Define a map as for all . The map is called the Gauss Map , and the Gaussian curvature of at is defined as . Question In the theorem below, the minus sign attached to is bugging me. For this means that the shape operator is the negative of the differential of the Gauss map. While the shape operator is supposed to be the same as the differential of the Gauss map. I am unable to find a mistake in my proof. ""Theorem."" Let be a hypersurface in , and let be the Gauss map. Let be a unit normal vector field along . Then for all and . Proof. Let be a smooth curve with and . Let be the parallel vector field along with . Now we have for all . Taking the derivative at , we have Note that is the covariant derivative of with respect to the Levi-Civita connection on . Thus . Substituting this in the equation above, we get Putting , , and , we get which proves the undesired result.","\newcommand{\sff}{II} \newcommand{\mf}{\mathfrak} \newcommand{\R}{\mathbf R} \newcommand{\ab}[1]{\langle #1\rangle} M \tilde M \tilde \nabla \nabla \tilde M M \tilde \nabla \nabla X Y M \nabla_XY p\in M \tilde \nabla_XY T_pM M \sff:\mf X(M)\times \mf X(M)\to \mf X(M) (X, Y) \tilde \nabla_XY T_pM^\perp \tilde \nabla_X Y=\nabla_XY + \sff(X, Y) M \R^{n+1} N M X Y M \sff(X, Y)=h(X, Y)N h:\mf X(M)\times \mf X(M)\to \R G: M\to S^n G(q)=N_q q\in M G M p \det(dG_p) h M \mathbf R^{n+1} G:M\to S^n N M \ab{dG_p u, v}= -h(u, v) p\in M u, v\in T_pM \gamma:(-\varepsilon, \varepsilon)\to M \gamma(0)=p \dot \gamma(0)=u V:I\to TM \gamma V(0)=v \ab{G\circ \gamma(t),\ V(t)}=0 t t=0 \ab{dG_p(\dot \gamma(0)),\ v} + \ab{G(\gamma(0)),\ \dot V(0)} =  0 \dot V V \R^{n+1} \dot V(0) = D_{t}V(0) + h(\dot \gamma(0), V(0)) N_{\gamma(0)} \ab{dG_p(\dot \gamma(0)),\ v} + \ab{G(\gamma(0)),\ h(\dot \gamma(0), V(0))N_{\gamma(0)}} =  0 \dot \gamma(0)=u V(0)=v G(\gamma(0))=N_p \ab{dG_pu, v} + \ab{N_p,\ h(u, v) N_p} = \ab{dG_pu, v} + h(u, v) =  0","['differential-geometry', 'riemannian-geometry', 'curvature']"
38,Trying to show that the orthogonal group is a manifold: why does this approach fail?,Trying to show that the orthogonal group is a manifold: why does this approach fail?,,"I'm trying to get a better feeling about differential geometry and I was trying to prove that $G = \{ x \in M_2(\mathbb{R}) : xx^t = I\}$ has a manifold structure.  My approach was to set $X = M_2(\mathbb{R})$ and consider the smooth map $f: X \rightarrow X, f(x) = xx^t - I$ .  The regular value theorem says that if the tangent space map $T_x(f): T_x(X) \rightarrow T_0(X)$ is surjective for all $x \in f^{-1}\{0\} = G$ , then $G$ is a submanifold of $X$ .  This is what I wanted to show in order to prove that $G$ has a manifold structure. Identifying $X$ with $\mathbb{R}^4$ via $\begin{pmatrix} a & b \\ c & d \end{pmatrix} = (a,b,c,d)$ , and taking the basis of the tangent space at a point $p$ to be $\frac{d}{dx_1}|_p, ... , \frac{d}{dx_4}|_p$ , the matrix of the tangent map at any $p \in f^{-1}\{0\}$ is just the Jacobian evaluated at $p$ .  Checking that $$f(x_1,x_2,x_3,x_4) = (x_1^2+x_2^2 -1, x_1x_3 + x_2x_4, x_1x_3 + x_2x_4, x_3^2 + x_4^2 - 1)$$ we see that the Jacobian of $f$ is $$\begin{pmatrix} 2x_1 & 2x_2 & 0 & 0 \\ x_3 & x_4 & x_1 & x_2 \\ x_3 & x_4 & x_1 &x_2 \\ 0 & 0 & 2x_1 & 2x_4 \end{pmatrix}$$ which is never invertible.  Hence the tangent space map is never surjective.  So this approach fails. On the other hand, this blog uses a similar approach with the regular value theorem, except it actually works.  It notably uses the space of symmetric matrices as the codomain, rather than the space of all matrices. My question : If I were more experienced in differential geometry, how can I know that my approach above is destined to fail?  How would I recognize that I need to shrink my codomain to a manifold of smaller dimension?  Does this have something to do with the fact that the orthogonal group itself has smaller dimension?","I'm trying to get a better feeling about differential geometry and I was trying to prove that has a manifold structure.  My approach was to set and consider the smooth map .  The regular value theorem says that if the tangent space map is surjective for all , then is a submanifold of .  This is what I wanted to show in order to prove that has a manifold structure. Identifying with via , and taking the basis of the tangent space at a point to be , the matrix of the tangent map at any is just the Jacobian evaluated at .  Checking that we see that the Jacobian of is which is never invertible.  Hence the tangent space map is never surjective.  So this approach fails. On the other hand, this blog uses a similar approach with the regular value theorem, except it actually works.  It notably uses the space of symmetric matrices as the codomain, rather than the space of all matrices. My question : If I were more experienced in differential geometry, how can I know that my approach above is destined to fail?  How would I recognize that I need to shrink my codomain to a manifold of smaller dimension?  Does this have something to do with the fact that the orthogonal group itself has smaller dimension?","G = \{ x \in M_2(\mathbb{R}) : xx^t = I\} X = M_2(\mathbb{R}) f: X \rightarrow X, f(x) = xx^t - I T_x(f): T_x(X) \rightarrow T_0(X) x \in f^{-1}\{0\} = G G X G X \mathbb{R}^4 \begin{pmatrix} a & b \\ c & d \end{pmatrix} = (a,b,c,d) p \frac{d}{dx_1}|_p, ... , \frac{d}{dx_4}|_p p \in f^{-1}\{0\} p f(x_1,x_2,x_3,x_4) = (x_1^2+x_2^2 -1, x_1x_3 + x_2x_4, x_1x_3 + x_2x_4, x_3^2 + x_4^2 - 1) f \begin{pmatrix} 2x_1 & 2x_2 & 0 & 0 \\ x_3 & x_4 & x_1 & x_2 \\ x_3 & x_4 & x_1 &x_2 \\ 0 & 0 & 2x_1 & 2x_4 \end{pmatrix}","['differential-geometry', 'lie-groups']"
39,What is $H_1(\operatorname{GL}_2(\mathbb{R}))$?,What is ?,H_1(\operatorname{GL}_2(\mathbb{R})),"I am trying to compute the cohomology groups of $\operatorname{GL}_2(\mathbb{R})$. I know that $\operatorname{GL}_2(\mathbb{R})$ has two connected components, so $H_0(\operatorname{GL}_2(\mathbb{R})) = \mathbb{R}^2$. Also, $\operatorname{GL}_2(\mathbb{R})$ is homotopically equivalent $O(2)$, the group of orthogonal matrices. Since $O(2)$ is 1 dimensional, this tells me that $H_k(\operatorname{GL}_2(\mathbb{R})) = 0$ for $k > 1$. But how do I compute $H_1(\operatorname{GL}_2(\mathbb{R})) = H_1(O(2))$?","I am trying to compute the cohomology groups of $\operatorname{GL}_2(\mathbb{R})$. I know that $\operatorname{GL}_2(\mathbb{R})$ has two connected components, so $H_0(\operatorname{GL}_2(\mathbb{R})) = \mathbb{R}^2$. Also, $\operatorname{GL}_2(\mathbb{R})$ is homotopically equivalent $O(2)$, the group of orthogonal matrices. Since $O(2)$ is 1 dimensional, this tells me that $H_k(\operatorname{GL}_2(\mathbb{R})) = 0$ for $k > 1$. But how do I compute $H_1(\operatorname{GL}_2(\mathbb{R})) = H_1(O(2))$?",,"['differential-geometry', 'lie-groups', 'homology-cohomology']"
40,"Definition of the term ""De Rham map""","Definition of the term ""De Rham map""",,"I am a PhD student working in the field of numerical simulation. In several papers, the term ""De Rham map"" pops up (for instance in the very good thesis by Jérôme Bonelle : https://tel.archives-ouvertes.fr/tel-01116527v2/document ). I am unsure about the definition of this term. I have come to believe that this term generally means ""operation that have continuous objects correspond to discrete ones"" i.e. ""means of defining the actual values of the degrees of freedom of a discrete object from a continuous object"", but I gradually suspect that it might rather mean ""result of the integration of a cochain on a differentiable manifold"". Of course, the two notions coincide in the litterature I have come accross. So my question is : What is the definition of the term ""De Rham map"" ? Regards,","I am a PhD student working in the field of numerical simulation. In several papers, the term ""De Rham map"" pops up (for instance in the very good thesis by Jérôme Bonelle : https://tel.archives-ouvertes.fr/tel-01116527v2/document ). I am unsure about the definition of this term. I have come to believe that this term generally means ""operation that have continuous objects correspond to discrete ones"" i.e. ""means of defining the actual values of the degrees of freedom of a discrete object from a continuous object"", but I gradually suspect that it might rather mean ""result of the integration of a cochain on a differentiable manifold"". Of course, the two notions coincide in the litterature I have come accross. So my question is : What is the definition of the term ""De Rham map"" ? Regards,",,"['integration', 'differential-geometry', 'manifolds', 'simulation']"
41,"How do I prove $d\omega = \frac{1}{p!}(\partial \omega_{i_1 \ldots i_p}/\partial x^j) \,dx^j \wedge dx^{i_1} \wedge \ldots \wedge dx^{i_p}$?",How do I prove ?,"d\omega = \frac{1}{p!}(\partial \omega_{i_1 \ldots i_p}/\partial x^j) \,dx^j \wedge dx^{i_1} \wedge \ldots \wedge dx^{i_p}","How do I prove $d\omega = \frac{1}{p!}(\partial \omega_{i_1 \ldots i_p}/\partial x^j) \, dx^j \wedge dx^{i_1} \wedge \ldots \wedge dx^{i_p}$ for $$\omega = \frac{1}{p!} \omega_{i_1 \ldots i_p} dx^{i_1} \wedge \ldots \wedge dx^{i_p} \text{?}$$ I can use the following: a) $d(\alpha + \beta) = d\alpha + d\beta$ b) $d^2 = 0$, c) $df = \frac{\partial f}{\partial x^j} \, dx^j$, d) $d(f\omega) = df \wedge \omega + f \, d\omega$, e) $d(dx^{i_1} \wedge \ldots \wedge dx^{i_p}) = 0$;","How do I prove $d\omega = \frac{1}{p!}(\partial \omega_{i_1 \ldots i_p}/\partial x^j) \, dx^j \wedge dx^{i_1} \wedge \ldots \wedge dx^{i_p}$ for $$\omega = \frac{1}{p!} \omega_{i_1 \ldots i_p} dx^{i_1} \wedge \ldots \wedge dx^{i_p} \text{?}$$ I can use the following: a) $d(\alpha + \beta) = d\alpha + d\beta$ b) $d^2 = 0$, c) $df = \frac{\partial f}{\partial x^j} \, dx^j$, d) $d(f\omega) = df \wedge \omega + f \, d\omega$, e) $d(dx^{i_1} \wedge \ldots \wedge dx^{i_p}) = 0$;",,"['differential-geometry', 'differential-forms', 'exterior-algebra']"
42,Representation of Metric in Normal Coordinates,Representation of Metric in Normal Coordinates,,"I'm trying to show that for coordinates $x:U \to \mathbb{R}^n$ centered at $p \in (M,g)$, the property $g_{ij}(0)x^j = g_{ij}(x)x^j$ characterizes normal coordinates - i.e. straight lines through the origin are geodesics. However, I don't know what $g_{ij}(0)x^j$ even means - is it just the matrix of coefficients of $g$ multiplied by some vector? Does it have any geometric meaning? $g$ is a map from $TM$ so it's weird that it's being applied to a point in the image of $M$ in $\mathbb{R}^n$. I get that $T_pM$ is diffeomorphic to some neighborhood of $p$, but I'm struggling to see the context for this property (it was shown in class and I wasn't in class). If the proof is just symbol manipulation - something like taking the derivative of $g_{ij}(c(t))c^j(t)$ where $c(t)$ is a parametrized line in the direction of some vector $c$, then that's fine, I'm just wondering if there's more geometric context in the property/condition.","I'm trying to show that for coordinates $x:U \to \mathbb{R}^n$ centered at $p \in (M,g)$, the property $g_{ij}(0)x^j = g_{ij}(x)x^j$ characterizes normal coordinates - i.e. straight lines through the origin are geodesics. However, I don't know what $g_{ij}(0)x^j$ even means - is it just the matrix of coefficients of $g$ multiplied by some vector? Does it have any geometric meaning? $g$ is a map from $TM$ so it's weird that it's being applied to a point in the image of $M$ in $\mathbb{R}^n$. I get that $T_pM$ is diffeomorphic to some neighborhood of $p$, but I'm struggling to see the context for this property (it was shown in class and I wasn't in class). If the proof is just symbol manipulation - something like taking the derivative of $g_{ij}(c(t))c^j(t)$ where $c(t)$ is a parametrized line in the direction of some vector $c$, then that's fine, I'm just wondering if there's more geometric context in the property/condition.",,"['differential-geometry', 'riemannian-geometry']"
43,Exterior covariant derivative and Lie derivative in Penrose abstract index notation?,Exterior covariant derivative and Lie derivative in Penrose abstract index notation?,,"How does one express the Lie derivative of tensors, and exterior covariant derivative for forms with values in a vector bundle in Penrose abstract index notation? I've tried looking through Penrose's negative dimensional tensors article but didn't see it written down. For say a vector-bundle valued 3-form $\omega_{bcd}^A$ where upper case indices correspond to the vector bundle and lower case indices are form indices,  would the exterior covariant derivative just be $\nabla_{[a}\omega_{bcd]}^A$, where square brackets indicate antisymmetrization?","How does one express the Lie derivative of tensors, and exterior covariant derivative for forms with values in a vector bundle in Penrose abstract index notation? I've tried looking through Penrose's negative dimensional tensors article but didn't see it written down. For say a vector-bundle valued 3-form $\omega_{bcd}^A$ where upper case indices correspond to the vector bundle and lower case indices are form indices,  would the exterior covariant derivative just be $\nabla_{[a}\omega_{bcd]}^A$, where square brackets indicate antisymmetrization?",,"['differential-geometry', 'tensors', 'vector-bundles']"
44,Show that on $\mathbb R^2$ the vector field $y^2\frac{\partial}{\partial x}+x^2\frac{\partial}{\partial y}$ is not complete,Show that on  the vector field  is not complete,\mathbb R^2 y^2\frac{\partial}{\partial x}+x^2\frac{\partial}{\partial y},"How to prove that on $\mathbb R^2$ the vector field  $$y^2\frac \partial{\partial x}+x^2\frac \partial{\partial y}$$ is not complete? I tried to solve the simultaneous ODE, but I realized that it is much complicated...","How to prove that on $\mathbb R^2$ the vector field  $$y^2\frac \partial{\partial x}+x^2\frac \partial{\partial y}$$ is not complete? I tried to solve the simultaneous ODE, but I realized that it is much complicated...",,"['differential-geometry', 'vector-fields']"
45,"If $i : N \rightarrow M$ is an injective immersion, find a $i$-linked vector field $Y$ to $X$","If  is an injective immersion, find a -linked vector field  to",i : N \rightarrow M i Y X,"I have an injective immersion $i:N\rightarrow M$ and a vector field $X$ on $N$. How to find a field $Y$ on $M$ which is $i$-linked to $X$? Reminder: $X,Y$ vector fields on $N,M$ respectively and $\psi:N\rightarrow M$ a differential map. Then $X$ and $Y$ are $\psi$ linked if $\psi_{*_p}(X_p) = Y_{\psi(p)}$ To do this, I define on $i(N), Y_{i(p)} := i_{*_p}(X_p)$ and this works. But I have to extend it to the whole of $M$ and I don't see how I can do that. I don't quite understand the argument extending a vector field defined on a closed submanifold , so if someone could help I would be gratefull.","I have an injective immersion $i:N\rightarrow M$ and a vector field $X$ on $N$. How to find a field $Y$ on $M$ which is $i$-linked to $X$? Reminder: $X,Y$ vector fields on $N,M$ respectively and $\psi:N\rightarrow M$ a differential map. Then $X$ and $Y$ are $\psi$ linked if $\psi_{*_p}(X_p) = Y_{\psi(p)}$ To do this, I define on $i(N), Y_{i(p)} := i_{*_p}(X_p)$ and this works. But I have to extend it to the whole of $M$ and I don't see how I can do that. I don't quite understand the argument extending a vector field defined on a closed submanifold , so if someone could help I would be gratefull.",,"['differential-geometry', 'differential-topology', 'smooth-manifolds', 'vector-fields']"
46,Showing that d$f(X)=X(f)$,Showing that d,f(X)=X(f),"Let $M$ be a manifold, $f:M\rightarrow\mathbb{R}$ a differentiable map and $X$ a differentiable vector field on $M$. I am struggling a bit understanding the equality $$\text{d}f(X)=X(f)$$ I believe it is equivalent to $$\text{d}_pf(v)=v(f)\quad\text{for every}\quad v\in T_pM$$ by making $v=X_p$ at each $p\in M$. The rhs is a real number and the lhs is a vector in $T_p\mathbb{R}$ (which obviously can be identified with a real number but technically, is not a real number). But if we take the identity chart $t:\mathbb{R}\rightarrow\mathbb{R}$, doing some computations I get that $$\text{d}_pf(v)=v(f)\left.\frac{\partial}{\partial t}\right|_{f(p)}\Rightarrow \text{d}f(v)=v(f)\frac{\partial}{\partial t}\Rightarrow \text{d}f(X)=X(f)\frac{\partial}{\partial t}$$ which makes more sense to me. So my question is, is my reasoning correct and this first formula is just an abuse of notation or am I missing something?","Let $M$ be a manifold, $f:M\rightarrow\mathbb{R}$ a differentiable map and $X$ a differentiable vector field on $M$. I am struggling a bit understanding the equality $$\text{d}f(X)=X(f)$$ I believe it is equivalent to $$\text{d}_pf(v)=v(f)\quad\text{for every}\quad v\in T_pM$$ by making $v=X_p$ at each $p\in M$. The rhs is a real number and the lhs is a vector in $T_p\mathbb{R}$ (which obviously can be identified with a real number but technically, is not a real number). But if we take the identity chart $t:\mathbb{R}\rightarrow\mathbb{R}$, doing some computations I get that $$\text{d}_pf(v)=v(f)\left.\frac{\partial}{\partial t}\right|_{f(p)}\Rightarrow \text{d}f(v)=v(f)\frac{\partial}{\partial t}\Rightarrow \text{d}f(X)=X(f)\frac{\partial}{\partial t}$$ which makes more sense to me. So my question is, is my reasoning correct and this first formula is just an abuse of notation or am I missing something?",,"['differential-geometry', 'smooth-manifolds']"
47,De Rham cohomology of a quotient manifold,De Rham cohomology of a quotient manifold,,"Let $X$ a manifold and $G$ a group that acts on X freely, properly, and smoothly. The quotient $X/G$ has a manifold structure such the projection $\pi:X\rightarrow X/G$ is a submersion. My question is: how are related the De Rham´s cohomology groups  of $X$ with the De Rham´s cohomology groups of the quotient? Thank you for your time.","Let $X$ a manifold and $G$ a group that acts on X freely, properly, and smoothly. The quotient $X/G$ has a manifold structure such the projection $\pi:X\rightarrow X/G$ is a submersion. My question is: how are related the De Rham´s cohomology groups  of $X$ with the De Rham´s cohomology groups of the quotient? Thank you for your time.",,['differential-geometry']
48,Notation of multiple covariant derivatives of a tensor,Notation of multiple covariant derivatives of a tensor,,"From this wikipedia page the laplacian of a tensor is defined by $$ \Delta T = g^{ij} \left(\nabla_{\partial_i} \nabla_{\partial_j} T - \nabla_{\nabla_{\partial_i} \partial_j} T\right). $$ I have an issue here with what they mean by $\nabla_{\partial_i} \nabla_{\partial_j} T $. Consider for simplicity the case that $T$ is a $(2,0)$-tensor (input 2 vectors, output a real number). There ware two ways to interpreet this. Interpreet $\nabla_{\partial_j} T$ first as the $(3,0)$-tensor $\nabla T$. Then we take the covariant derivative of $\nabla T$ and then insert the vectors $\partial_i$ and $\partial_j$. Interpreet $\nabla_{\partial_j} T$ as a $(2,0)$-tensor and take the covariant derivative with respect to $\partial_i$ of this expression. I know that the first interpretation is independent of the coordinates and the second isn't. When the first interpretation is written out in coordinates, then it equals the formula for $\Delta T$, but interpreted by the second interpretation listed. I feel like the second interpretation is the correct one?","From this wikipedia page the laplacian of a tensor is defined by $$ \Delta T = g^{ij} \left(\nabla_{\partial_i} \nabla_{\partial_j} T - \nabla_{\nabla_{\partial_i} \partial_j} T\right). $$ I have an issue here with what they mean by $\nabla_{\partial_i} \nabla_{\partial_j} T $. Consider for simplicity the case that $T$ is a $(2,0)$-tensor (input 2 vectors, output a real number). There ware two ways to interpreet this. Interpreet $\nabla_{\partial_j} T$ first as the $(3,0)$-tensor $\nabla T$. Then we take the covariant derivative of $\nabla T$ and then insert the vectors $\partial_i$ and $\partial_j$. Interpreet $\nabla_{\partial_j} T$ as a $(2,0)$-tensor and take the covariant derivative with respect to $\partial_i$ of this expression. I know that the first interpretation is independent of the coordinates and the second isn't. When the first interpretation is written out in coordinates, then it equals the formula for $\Delta T$, but interpreted by the second interpretation listed. I feel like the second interpretation is the correct one?",,"['differential-geometry', 'riemannian-geometry', 'tensors']"
49,"If I am told to reparameterize a curve by arc-length, what is the curve originally parameterized by?","If I am told to reparameterize a curve by arc-length, what is the curve originally parameterized by?",,"In differential geometry, you are often asked to reparameterize a curve using arc-length. I understand the process of how to do this, but I don't understand what we are reparameterizing from. What is the curve originally parameterized by (before we REparameterize it by arc-length)?","In differential geometry, you are often asked to reparameterize a curve using arc-length. I understand the process of how to do this, but I don't understand what we are reparameterizing from. What is the curve originally parameterized by (before we REparameterize it by arc-length)?",,['differential-geometry']
50,Arc length Parameterization,Arc length Parameterization,,If a given vector $\alpha$ is said to have unit speed such that $\Vert \alpha' \Vert$is equal to one. Does that imply that the vector is paramaterized with respect to arc length?,If a given vector $\alpha$ is said to have unit speed such that $\Vert \alpha' \Vert$is equal to one. Does that imply that the vector is paramaterized with respect to arc length?,,['differential-geometry']
51,Do two open diffeomorphic sets in $\mathbb{R^n}$ with the same boundary coincide?,Do two open diffeomorphic sets in  with the same boundary coincide?,\mathbb{R^n},"Let $A,B \subseteq \mathbb{R}^n$  be open diffeomorphic subsets such that $\partial A=\partial B$. Is it true that $A=B$? (I have seen this claim somewhere but with no proof). Edit: As commented by Noah, this is not true in general: e.g take $A,B$ to be the left and right half-planes in $\mathbb{R}^2$. The question might be more interesting if we require $A,B$ to be bounded.","Let $A,B \subseteq \mathbb{R}^n$  be open diffeomorphic subsets such that $\partial A=\partial B$. Is it true that $A=B$? (I have seen this claim somewhere but with no proof). Edit: As commented by Noah, this is not true in general: e.g take $A,B$ to be the left and right half-planes in $\mathbb{R}^2$. The question might be more interesting if we require $A,B$ to be bounded.",,"['general-topology', 'differential-geometry', 'smooth-manifolds']"
52,Example of a function in Riemannian manifold,Example of a function in Riemannian manifold,,"Is there any example of a function $f:M\rightarrow\mathbb{R}$ such that the following condition holds $$\nabla^2f_p(X,Y)\geq Ric_p(X,Y)f(p)\ \forall p\in M \text{ and } X,Y\in T_pM$$ where $\nabla^2$ is the Hessian operator and $Ric_p$ is the Ricci tensor at $p$. I know that every convex function in Euclidean space satisfies the condition but I can not find any other example in other Riemannian manifold. Please help.","Is there any example of a function $f:M\rightarrow\mathbb{R}$ such that the following condition holds $$\nabla^2f_p(X,Y)\geq Ric_p(X,Y)f(p)\ \forall p\in M \text{ and } X,Y\in T_pM$$ where $\nabla^2$ is the Hessian operator and $Ric_p$ is the Ricci tensor at $p$. I know that every convex function in Euclidean space satisfies the condition but I can not find any other example in other Riemannian manifold. Please help.",,"['differential-geometry', 'riemannian-geometry', 'examples-counterexamples', 'tensors']"
53,Complexification of homogeneous spaces,Complexification of homogeneous spaces,,"Let $X=G/H$ be a homogeneous space, where $G$ is a Lie group and $H$ is a closed subgroup of $G$. Assume that $G/H$ has a complexification $\hat X$. Is $\hat X$ also a homogeneous space i.e $\hat G/\hat H$? What is the relation between $G$ and $\hat G$ and $H$ and $\hat H$?","Let $X=G/H$ be a homogeneous space, where $G$ is a Lie group and $H$ is a closed subgroup of $G$. Assume that $G/H$ has a complexification $\hat X$. Is $\hat X$ also a homogeneous space i.e $\hat G/\hat H$? What is the relation between $G$ and $\hat G$ and $H$ and $\hat H$?",,"['differential-geometry', 'lie-groups', 'lie-algebras']"
54,The curve $y^2 - x^3=0$ isn't a differential submanifold of $\mathbb{R}^2$,The curve  isn't a differential submanifold of,y^2 - x^3=0 \mathbb{R}^2,"So, I would like to prove that the curve $\alpha :y^2 -x^3 =0$ is not a differential submanifold of $\mathbb{R}^2$ . My notes are quite messy about this, and at the time it was an argument that I really didn't get. Moreover, it's one of the first times I have to deal with manifolds. I know that I am supposed to use (I mean, the teacher used) the implicit function theorem, and see the curve as the locus of zeros of a differentiable function in $\mathbb{R}^2$ , because I need a submanifold of $\mathbb{R}^2$ . If you consider such a function, you can prove that in $(0,0)$ both partial derivatives are zero. Then you say you can't apply the implicit function theorem and so the curve is not a submanifold of $\mathbb{R}^2$ . There are a few things I am not sure about. However, the most troublesome is by far the application of the implicit function theorem. I mean, the theorem is great if you want to prove that some curve has a regular parametrization without bothering searching for an explicit one, which is great if I had wanted to prove that a curve is a differential submanifold. Here I cannot apply  the theorem in $(0,0)$ . How can you conclude then? Doesn't the implicit function theorem give only sufficient conditions? I mean, if you prove that every differentiable function in $\mathbb{R}^2$ with $\alpha$ as locus of zeros has partial derivatives equal to $0$ at the origin, why should it mean that no structure of differential submanifold is possible at all?","So, I would like to prove that the curve is not a differential submanifold of . My notes are quite messy about this, and at the time it was an argument that I really didn't get. Moreover, it's one of the first times I have to deal with manifolds. I know that I am supposed to use (I mean, the teacher used) the implicit function theorem, and see the curve as the locus of zeros of a differentiable function in , because I need a submanifold of . If you consider such a function, you can prove that in both partial derivatives are zero. Then you say you can't apply the implicit function theorem and so the curve is not a submanifold of . There are a few things I am not sure about. However, the most troublesome is by far the application of the implicit function theorem. I mean, the theorem is great if you want to prove that some curve has a regular parametrization without bothering searching for an explicit one, which is great if I had wanted to prove that a curve is a differential submanifold. Here I cannot apply  the theorem in . How can you conclude then? Doesn't the implicit function theorem give only sufficient conditions? I mean, if you prove that every differentiable function in with as locus of zeros has partial derivatives equal to at the origin, why should it mean that no structure of differential submanifold is possible at all?","\alpha :y^2 -x^3 =0 \mathbb{R}^2 \mathbb{R}^2 \mathbb{R}^2 (0,0) \mathbb{R}^2 (0,0) \mathbb{R}^2 \alpha 0","['differential-geometry', 'differential-topology']"
55,Why does the inverse metric of the hypersphere have discontinuities?,Why does the inverse metric of the hypersphere have discontinuities?,,"Consider the unit hypersphere in $\mathbb{R}^n$, i.e. with the Euclidean metric, using spherical coordinates. The metric tensor is then : $$ g_{11}=1 $$ $$ g_{ij}=\delta_{ij}\prod_{k=1}^{i-1}\sin^2(\theta_k) $$ where $\delta_{ij}$ is the Kronecker delta. The inverse metric is then: $$ g^{11}=1 $$ $$ g^{ij}=\delta_{ij}\prod_{k=1}^{i-1}\csc^2(\theta_k) $$ Notice that if any of $\theta_k=0$, then there will be a problem with $\csc(\theta_k)$. But $\theta_k=0$ seems to be a perfectly reasonable coordinate to be on. Questions: (1) Why does the inverse metric fail to exist when $\theta_k=0$? (2) How do I fix this so that I can use the inverse metric computationally in cases where $\theta_k$ are allowed to freely vary (and vanish) as they can for $g$? I'm hoping I'm making a silly mistake :) (Note: this is true for $n=3$ too)","Consider the unit hypersphere in $\mathbb{R}^n$, i.e. with the Euclidean metric, using spherical coordinates. The metric tensor is then : $$ g_{11}=1 $$ $$ g_{ij}=\delta_{ij}\prod_{k=1}^{i-1}\sin^2(\theta_k) $$ where $\delta_{ij}$ is the Kronecker delta. The inverse metric is then: $$ g^{11}=1 $$ $$ g^{ij}=\delta_{ij}\prod_{k=1}^{i-1}\csc^2(\theta_k) $$ Notice that if any of $\theta_k=0$, then there will be a problem with $\csc(\theta_k)$. But $\theta_k=0$ seems to be a perfectly reasonable coordinate to be on. Questions: (1) Why does the inverse metric fail to exist when $\theta_k=0$? (2) How do I fix this so that I can use the inverse metric computationally in cases where $\theta_k$ are allowed to freely vary (and vanish) as they can for $g$? I'm hoping I'm making a silly mistake :) (Note: this is true for $n=3$ too)",,"['differential-geometry', 'riemannian-geometry', 'tensors', 'spherical-coordinates', 'spherical-geometry']"
56,Real and complex nilpotent Lie groups,Real and complex nilpotent Lie groups,,"It is known that if $G$ is an abelian real Lie group then $\exp :(\mathfrak  g,+)\rightarrow G$ is a surjective homomorphism between two abelian groups. Thus, $G\cong \mathfrak g/\ker(\exp)\cong \mathbb R^n/\Gamma\cong \mathbb R^k\times(\mathbb S^1)^{n-k}$ where $\Gamma$ is a discrete subgroup of $\mathbb R^n$. Now the situation for the complex case is slightly different;  Let $G$ be an abelian complex Lie group, then $G\cong \mathbb C^m\times (\mathbb C^*)^n\times C$ where $C$ is a Toroidal group (i.e a complex connected Lie group that contains a connected normal subgroup $H$ which is a complexified torus (i.e., isomorphic to $(\mathbb C^*)^t$ such that $C/H$ is compact). My question is about mimicking this in the nilpotent case. So let $G$ be a connected real nilpotent Lie group, then $\exp:\mathfrak g\rightarrow G$ is a surjective smooth map between two manifolds. How to prove that $G$ is diffeomorphic to $\mathbb R^m\times (\mathbb S^1)^n$? Is this also true for the connected complex nilpotent Lie groups. I mean is $G$  holomorphic equivalent to $\mathbb C^m\times (\mathbb C^*)^n\times C$?","It is known that if $G$ is an abelian real Lie group then $\exp :(\mathfrak  g,+)\rightarrow G$ is a surjective homomorphism between two abelian groups. Thus, $G\cong \mathfrak g/\ker(\exp)\cong \mathbb R^n/\Gamma\cong \mathbb R^k\times(\mathbb S^1)^{n-k}$ where $\Gamma$ is a discrete subgroup of $\mathbb R^n$. Now the situation for the complex case is slightly different;  Let $G$ be an abelian complex Lie group, then $G\cong \mathbb C^m\times (\mathbb C^*)^n\times C$ where $C$ is a Toroidal group (i.e a complex connected Lie group that contains a connected normal subgroup $H$ which is a complexified torus (i.e., isomorphic to $(\mathbb C^*)^t$ such that $C/H$ is compact). My question is about mimicking this in the nilpotent case. So let $G$ be a connected real nilpotent Lie group, then $\exp:\mathfrak g\rightarrow G$ is a surjective smooth map between two manifolds. How to prove that $G$ is diffeomorphic to $\mathbb R^m\times (\mathbb S^1)^n$? Is this also true for the connected complex nilpotent Lie groups. I mean is $G$  holomorphic equivalent to $\mathbb C^m\times (\mathbb C^*)^n\times C$?",,"['differential-geometry', 'lie-groups', 'lie-algebras', 'complex-geometry']"
57,Sectional curvature of hypersurface,Sectional curvature of hypersurface,,"Let $M$ be a Riemannian hypersurface in $\mathbb{R}^{n+1}$, where $n \geq 2$. Let $\varSigma$ be a two dimensional subspace of the tangent space $T_{p}M$ of $M$ at $p$, and denote by $s_{p}$ the shape operator of $M$ at $p$. I believe I read on some reference (which currently I am not able to retrieve) the following statement: The sectional curvature $K(\varSigma)$ is equal to the determinant of the restriction of $s$ to $\varSigma$, provided $\varSigma$ is $s$-invariant, i.e. $s(\varSigma) \subset \varSigma$. Can somebody confirm that such statement is true? How would you prove it?","Let $M$ be a Riemannian hypersurface in $\mathbb{R}^{n+1}$, where $n \geq 2$. Let $\varSigma$ be a two dimensional subspace of the tangent space $T_{p}M$ of $M$ at $p$, and denote by $s_{p}$ the shape operator of $M$ at $p$. I believe I read on some reference (which currently I am not able to retrieve) the following statement: The sectional curvature $K(\varSigma)$ is equal to the determinant of the restriction of $s$ to $\varSigma$, provided $\varSigma$ is $s$-invariant, i.e. $s(\varSigma) \subset \varSigma$. Can somebody confirm that such statement is true? How would you prove it?",,['differential-geometry']
58,The gradient as a special case of the differential (or push-forward),The gradient as a special case of the differential (or push-forward),,"I am tripping over something elementary (I think). Given a smooth map $f\colon M\to N$ between smooth manifolds, the differential of $f$ at $p\in M$ is defined as \begin{align}\mathrm{d}_pf \colon T_pM &\to T_{f(p)}N\\ X&\mapsto X(-\circ f)\end{align} The gradient of $f\colon M\to \mathbb{R}$ at $p$ is, if I understand correctly, just the previous definition with $T_{f(p)}\mathbb{R}\cong\mathbb{R}$. But this is were I get confused, because immediately after the gradient of $f$ is defined by $$\mathrm{d}_pf(X):=X(f), \qquad \text{ with }X\in T_pM.$$ I understand this, since $\mathrm{d}_pf\colon T_pM\to\mathbb{R}$ (linearly) it is a covector and the right hand side works, but how can I see this as a special case of the above? (i.e. why does the $``\, -\ \circ\,''$ get dropped?) And how do the elements of $T_{f(p)}\mathbb{R}$ act on real functions? (perhaps this is more like what I am looking for).","I am tripping over something elementary (I think). Given a smooth map $f\colon M\to N$ between smooth manifolds, the differential of $f$ at $p\in M$ is defined as \begin{align}\mathrm{d}_pf \colon T_pM &\to T_{f(p)}N\\ X&\mapsto X(-\circ f)\end{align} The gradient of $f\colon M\to \mathbb{R}$ at $p$ is, if I understand correctly, just the previous definition with $T_{f(p)}\mathbb{R}\cong\mathbb{R}$. But this is were I get confused, because immediately after the gradient of $f$ is defined by $$\mathrm{d}_pf(X):=X(f), \qquad \text{ with }X\in T_pM.$$ I understand this, since $\mathrm{d}_pf\colon T_pM\to\mathbb{R}$ (linearly) it is a covector and the right hand side works, but how can I see this as a special case of the above? (i.e. why does the $``\, -\ \circ\,''$ get dropped?) And how do the elements of $T_{f(p)}\mathbb{R}$ act on real functions? (perhaps this is more like what I am looking for).",,['differential-geometry']
59,"Proof in do Carmo's ""Differential Forms and Applications""","Proof in do Carmo's ""Differential Forms and Applications""",,"I've trying to figure this out for a while and I am finally desperate enough to post to stack exchange In do Carmo's Differential Forms and Applications Proposition 2 on pg 92 (this is the proof that the Gaussian curvature is well-defined, independent of choice frame and coframe). I post the statement and proof for context: Proposition 2 Let $M^2$ be a Riemannian manifold of dimension two. For each $p\in M$, we define a number $K(p)$ by choosing a moving [orthonormal] frame $\{e_1,e_2\}$ around $p$ and setting   $$ d\omega_{12}(p):=-K(p)(\omega_1\wedge\omega_2)(p). $$   [Here $\{\omega_1,\omega_2\}$ is the coframe associated to $\{e_1,e_2\}$.] Then $K(p)$ does not depend on the choice of frames, and is called the Gaussian curvature of $M$ at $p$. Proof. Let $\{\bar{e}_1,\bar{e}_2\}$ be another moving [orthonormal] frame around $p$. Assume first that the orientations of the two moving frames are the same. Then   $$ \omega_{12}=\bar{\omega}_{12}-\tau. $$   [Here $\tau=fdg-gdf$, where $f$ and $g$ are differentiable functions such that $f^2+g^2=1$; this was shown in a earlier lemma -- Lemma 4 on pg 90.] Since $\tau =fdg-gdf$, $d\tau =0$, hence $d\omega_{12}=d\bar{\omega}_{12}$ [this is the part I don't understand; I will elaborate afterwards]. It follows that   $$ -K\omega_1\wedge\omega_2=d\omega_{12}=d\bar{\omega}_{12}=-\bar{K}\bar{\omega}_{1}\wedge\bar{\omega}=-\bar{K}\omega_1\wedge\omega_2 $$   hence $K=\bar{K}$, as we wished. If the orientations are opposite, we obtain   $$ d\omega_{12}=-d\bar{\omega}_{12},\hspace{.2 in}\omega_1\wedge\omega_{2}=-\bar{\omega}_1\wedge\bar{\omega}_2 $$   and the same conclusion holds. This is slightly embarrassing since I have been working with differential forms for a few years now, but I don't understand why $d\tau=0$. In my line of thinking (interpreting $fdg$ and $gdf$ as wedge products between 0-forms and 1-forms and writing out in full detail) \begin{align*} d\tau &=d(fdg-gdf)\\ &=df\wedge dg+f d^2g-dg\wedge df-gd^2f=2df\wedge dg\\ &=df\wedge dg+f\cdot 0-dg\wedge df-g\cdot 0\\ &=df\wedge  dg-dg\wedge df\\ &=2df\wedge dg \end{align*} which is not zero, unless there is some extra information about $f$ and $g$ I don't know about. It does seem like Differential Forms and Applications has quite a few typos, so I was thinking it was supposed to be $fdg+gdf$, but I went through the work where this ""$\tau$"" first popped up, and it seems like this is the correct form to be working with. Does anyone have any insight on what could be going wrong here? Thanks","I've trying to figure this out for a while and I am finally desperate enough to post to stack exchange In do Carmo's Differential Forms and Applications Proposition 2 on pg 92 (this is the proof that the Gaussian curvature is well-defined, independent of choice frame and coframe). I post the statement and proof for context: Proposition 2 Let $M^2$ be a Riemannian manifold of dimension two. For each $p\in M$, we define a number $K(p)$ by choosing a moving [orthonormal] frame $\{e_1,e_2\}$ around $p$ and setting   $$ d\omega_{12}(p):=-K(p)(\omega_1\wedge\omega_2)(p). $$   [Here $\{\omega_1,\omega_2\}$ is the coframe associated to $\{e_1,e_2\}$.] Then $K(p)$ does not depend on the choice of frames, and is called the Gaussian curvature of $M$ at $p$. Proof. Let $\{\bar{e}_1,\bar{e}_2\}$ be another moving [orthonormal] frame around $p$. Assume first that the orientations of the two moving frames are the same. Then   $$ \omega_{12}=\bar{\omega}_{12}-\tau. $$   [Here $\tau=fdg-gdf$, where $f$ and $g$ are differentiable functions such that $f^2+g^2=1$; this was shown in a earlier lemma -- Lemma 4 on pg 90.] Since $\tau =fdg-gdf$, $d\tau =0$, hence $d\omega_{12}=d\bar{\omega}_{12}$ [this is the part I don't understand; I will elaborate afterwards]. It follows that   $$ -K\omega_1\wedge\omega_2=d\omega_{12}=d\bar{\omega}_{12}=-\bar{K}\bar{\omega}_{1}\wedge\bar{\omega}=-\bar{K}\omega_1\wedge\omega_2 $$   hence $K=\bar{K}$, as we wished. If the orientations are opposite, we obtain   $$ d\omega_{12}=-d\bar{\omega}_{12},\hspace{.2 in}\omega_1\wedge\omega_{2}=-\bar{\omega}_1\wedge\bar{\omega}_2 $$   and the same conclusion holds. This is slightly embarrassing since I have been working with differential forms for a few years now, but I don't understand why $d\tau=0$. In my line of thinking (interpreting $fdg$ and $gdf$ as wedge products between 0-forms and 1-forms and writing out in full detail) \begin{align*} d\tau &=d(fdg-gdf)\\ &=df\wedge dg+f d^2g-dg\wedge df-gd^2f=2df\wedge dg\\ &=df\wedge dg+f\cdot 0-dg\wedge df-g\cdot 0\\ &=df\wedge  dg-dg\wedge df\\ &=2df\wedge dg \end{align*} which is not zero, unless there is some extra information about $f$ and $g$ I don't know about. It does seem like Differential Forms and Applications has quite a few typos, so I was thinking it was supposed to be $fdg+gdf$, but I went through the work where this ""$\tau$"" first popped up, and it seems like this is the correct form to be working with. Does anyone have any insight on what could be going wrong here? Thanks",,"['differential-geometry', 'differential-forms', 'curvature']"
60,Connection matrix for a vector bundle,Connection matrix for a vector bundle,,"Let $E \rightarrow M$ be a vector bundle.  In the book Differential Analysis on Complex Manifolds by R.O. Wells a connection on E is defined to be a map a linear map $D: \Omega(E) \rightarrow \Omega^1(E)$ such that $D(\phi s) = d\phi\otimes s + \phi Ds$. Now the book goes to choose a local basis of sections $f = (e_1,\ldots ,e_n)$(a frame) over U and shows how one can define a connection matrix with respect to a frame. It then shows if one changes the frame with a mapping $g : U \rightarrow GL(n)$ how the connection matrix changes. For example if A is the connection matrix one finds $A(fg)=g^{−1}dg+g^{-1}A(f)g$ and defining the curvature as $F(f)=A(f) \wedge A(f)+dA(f)$ one finds $F(fg)=g^{-1}F(f)g$. I noticed that if I considerd only transformaions $g: U \rightarrow G$  where $G$ is a matrix group and if $A(f)$ initially lies in the Lie Algebra of G then after a transformation $A(fg)=g^{−1}dg+g^{-1}A(f)g$ would also be in the Lie Algebra since the first term is the maurer cartan form and the second one is the adjoint representation. My question is if there is some sort of name or notion for only considering connections on a vector bundle such that the connection matrix lies in the Lie-algebra of a subgroup of $GL(n)$  ?  Does this have to do with structure groups ?","Let $E \rightarrow M$ be a vector bundle.  In the book Differential Analysis on Complex Manifolds by R.O. Wells a connection on E is defined to be a map a linear map $D: \Omega(E) \rightarrow \Omega^1(E)$ such that $D(\phi s) = d\phi\otimes s + \phi Ds$. Now the book goes to choose a local basis of sections $f = (e_1,\ldots ,e_n)$(a frame) over U and shows how one can define a connection matrix with respect to a frame. It then shows if one changes the frame with a mapping $g : U \rightarrow GL(n)$ how the connection matrix changes. For example if A is the connection matrix one finds $A(fg)=g^{−1}dg+g^{-1}A(f)g$ and defining the curvature as $F(f)=A(f) \wedge A(f)+dA(f)$ one finds $F(fg)=g^{-1}F(f)g$. I noticed that if I considerd only transformaions $g: U \rightarrow G$  where $G$ is a matrix group and if $A(f)$ initially lies in the Lie Algebra of G then after a transformation $A(fg)=g^{−1}dg+g^{-1}A(f)g$ would also be in the Lie Algebra since the first term is the maurer cartan form and the second one is the adjoint representation. My question is if there is some sort of name or notion for only considering connections on a vector bundle such that the connection matrix lies in the Lie-algebra of a subgroup of $GL(n)$  ?  Does this have to do with structure groups ?",,['differential-geometry']
61,Constructing a smooth bump function on a manifold,Constructing a smooth bump function on a manifold,,"In "" Loring W. Tu, An introduction to manifolds "" the following question exists: Let $q$ be a point of an $n$-dimensional manifold $M$ and $U$ any neighborhood of $q$. Construct a smooth bump function at $q$ supported in $U$. I answered that question but I want to make sure. Here is my answer: Let $q$ be arbitrary of $M$ that is contained in a neighborhood $U\subset M$. Then, there exists a coordinate chart $(V,\phi)$ in the maximal atlas of $M$ such that $q\in V \subset U$. In particular, there exists a smooth bump function $\rho:\mathbb{R}^n \to \mathbb{R}$ at $\phi(q)$ supported in $\phi(V)$ that is identically $1$ in a neighborhood $B_r(\phi(q)) \subset \phi(V)$, say, of $\phi(q)$. Define a map $f:M\to \mathbb{R}$ by  $$ f(p) =  \begin{cases} \rho\bigl(\phi(p)  \bigr), &\text{$p\in V$}, \\ 0, &\text{$p\not\in V$}. \end{cases} $$ Being the composite of two smooth function, $f$ is smooth on $V$ and hence on the whole manifold $M$. If $p\in \phi^{-1}\bigl( B_r(\phi(q)) \bigr)$, then $\phi(p) \in B_r\bigl( \phi(q) \bigr)$ and therefore, by the construction of $\rho$, $\rho\bigl( \phi(p) \bigr)=1$. That is, $f \equiv 1$ on the neighborhood $\phi^{-1}\bigl( B_r(\phi(q)) \bigr)$ of $q$. Clearly, by the definition of $f$, $supp\, f \subset V \subset U$. Hence, $f$ is a smooth bump function at $q$ supported in $U$. Can anyone please revise my proof ?. I appreciate your help. Thanks in advance. Note: A smooth bump function $f:M\to \mathbb{R}$ at a point $q\in M$ supported in $U\subset M$ is a non-negative smooth function such that $f\equiv 1$ on a neighborhood $V_q \subset U$ of $q$ and that $supp\, f \subset U$.","In "" Loring W. Tu, An introduction to manifolds "" the following question exists: Let $q$ be a point of an $n$-dimensional manifold $M$ and $U$ any neighborhood of $q$. Construct a smooth bump function at $q$ supported in $U$. I answered that question but I want to make sure. Here is my answer: Let $q$ be arbitrary of $M$ that is contained in a neighborhood $U\subset M$. Then, there exists a coordinate chart $(V,\phi)$ in the maximal atlas of $M$ such that $q\in V \subset U$. In particular, there exists a smooth bump function $\rho:\mathbb{R}^n \to \mathbb{R}$ at $\phi(q)$ supported in $\phi(V)$ that is identically $1$ in a neighborhood $B_r(\phi(q)) \subset \phi(V)$, say, of $\phi(q)$. Define a map $f:M\to \mathbb{R}$ by  $$ f(p) =  \begin{cases} \rho\bigl(\phi(p)  \bigr), &\text{$p\in V$}, \\ 0, &\text{$p\not\in V$}. \end{cases} $$ Being the composite of two smooth function, $f$ is smooth on $V$ and hence on the whole manifold $M$. If $p\in \phi^{-1}\bigl( B_r(\phi(q)) \bigr)$, then $\phi(p) \in B_r\bigl( \phi(q) \bigr)$ and therefore, by the construction of $\rho$, $\rho\bigl( \phi(p) \bigr)=1$. That is, $f \equiv 1$ on the neighborhood $\phi^{-1}\bigl( B_r(\phi(q)) \bigr)$ of $q$. Clearly, by the definition of $f$, $supp\, f \subset V \subset U$. Hence, $f$ is a smooth bump function at $q$ supported in $U$. Can anyone please revise my proof ?. I appreciate your help. Thanks in advance. Note: A smooth bump function $f:M\to \mathbb{R}$ at a point $q\in M$ supported in $U\subset M$ is a non-negative smooth function such that $f\equiv 1$ on a neighborhood $V_q \subset U$ of $q$ and that $supp\, f \subset U$.",,['differential-geometry']
62,Which planar curve has curvature linearly on arc length? [closed],Which planar curve has curvature linearly on arc length? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Which planar curve has natural equation $ k(s) = a*s $   ? where k(s) is curvature function on arc lenght parameterization and $ a \neq 0$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Which planar curve has natural equation $ k(s) = a*s $   ? where k(s) is curvature function on arc lenght parameterization and $ a \neq 0$",,['differential-geometry']
63,How to Prove a 3 dimensional curve isn't contained in a Quadratic Surface?,How to Prove a 3 dimensional curve isn't contained in a Quadratic Surface?,,"So I'm given a curve parametrically as $$ C = \begin{pmatrix} x = t \\ y  = t^2 \\ z = t^3 \end{pmatrix} $$ And  I wish to show that there doesn't exist a surface $S$ of the form $$ A + Bx + Cy + Dz + Exy + Fxz + Gyz + Hx^2 + Iy^2 + Kz^2 = 0 $$ That contains said curve. But I'm not really sure how to do this? One Idea: So in a simpler case of just showing that no surface $S$ of the form $A + Bx + Cy + Dz = 0$ contains the curve $C$, one can observe that $C$ has non-zero torsion, whereas every curve bounded in a plane $S$ will have to have $0$ torsion, so clearly $C$ is not contained in $S$. This motivates me to then ask: how do I generalize ""Torsion"" to a more general measure that is bounded for all quadratic surfaces?","So I'm given a curve parametrically as $$ C = \begin{pmatrix} x = t \\ y  = t^2 \\ z = t^3 \end{pmatrix} $$ And  I wish to show that there doesn't exist a surface $S$ of the form $$ A + Bx + Cy + Dz + Exy + Fxz + Gyz + Hx^2 + Iy^2 + Kz^2 = 0 $$ That contains said curve. But I'm not really sure how to do this? One Idea: So in a simpler case of just showing that no surface $S$ of the form $A + Bx + Cy + Dz = 0$ contains the curve $C$, one can observe that $C$ has non-zero torsion, whereas every curve bounded in a plane $S$ will have to have $0$ torsion, so clearly $C$ is not contained in $S$. This motivates me to then ask: how do I generalize ""Torsion"" to a more general measure that is bounded for all quadratic surfaces?",,"['differential-geometry', 'algebraic-geometry']"
64,Lie derivative characterisation of the Levi-Civita connection,Lie derivative characterisation of the Levi-Civita connection,,"In this note , equation (6) the author claimed that $(M,g)$ is a Riemannian manifold. Let $X,Y,K$ be vector fields defined in the vicinity of a point $p\in M$. Then the condition that $\nabla$ is Levi-Civita connection means that   $$L_K|_p(M\ni x\mapsto g_x(X_x,Y_x)\in\Bbb R)=g_p((\nabla_KX)_p,Y_p)+g_p(X_p,(\nabla_KY)_p)$$ I have rearranged the notations to make them look rigorous. In the original notations the equation looks like $$L_K\langle X,Y\rangle=\partial_K\langle X,Y\rangle=\langle \nabla_KX,Y\rangle+\langle X,\nabla_KY\rangle$$ Could anybody provide any references for a proof? The reference the author gave seems inaccessible (I couldn't find it online). Many thanks! EDIT In effect I'm looking for a proof for the characterisation of Killing fields found on Wikipedia , i.e., $X$ preserves metric (which I think is equivalent to $L_K|_p(M\ni x\mapsto g_x(X_x,Y_x)\in\Bbb R)=0$) means $$g(\nabla _{{Y}}X,Z)+g(Y,\nabla _{{Z}}X)=0\,$$ EDIT AGAIN Sorry I was so dumb, the equation I asked about in the main text is just the compatibility of the Levi-Civita connection. And in fact, this expression doesn't quite match the one characterising the Killing form (in the previous EDIT): for the one in the main text it's $\nabla_KX$ and for the one in the EDIT it's $\nabla_XK$. I have to give it quite a lot of thoughts how to make use of the relation $\nabla_KX-\nabla_XK=[K,X]$ etc.","In this note , equation (6) the author claimed that $(M,g)$ is a Riemannian manifold. Let $X,Y,K$ be vector fields defined in the vicinity of a point $p\in M$. Then the condition that $\nabla$ is Levi-Civita connection means that   $$L_K|_p(M\ni x\mapsto g_x(X_x,Y_x)\in\Bbb R)=g_p((\nabla_KX)_p,Y_p)+g_p(X_p,(\nabla_KY)_p)$$ I have rearranged the notations to make them look rigorous. In the original notations the equation looks like $$L_K\langle X,Y\rangle=\partial_K\langle X,Y\rangle=\langle \nabla_KX,Y\rangle+\langle X,\nabla_KY\rangle$$ Could anybody provide any references for a proof? The reference the author gave seems inaccessible (I couldn't find it online). Many thanks! EDIT In effect I'm looking for a proof for the characterisation of Killing fields found on Wikipedia , i.e., $X$ preserves metric (which I think is equivalent to $L_K|_p(M\ni x\mapsto g_x(X_x,Y_x)\in\Bbb R)=0$) means $$g(\nabla _{{Y}}X,Z)+g(Y,\nabla _{{Z}}X)=0\,$$ EDIT AGAIN Sorry I was so dumb, the equation I asked about in the main text is just the compatibility of the Levi-Civita connection. And in fact, this expression doesn't quite match the one characterising the Killing form (in the previous EDIT): for the one in the main text it's $\nabla_KX$ and for the one in the EDIT it's $\nabla_XK$. I have to give it quite a lot of thoughts how to make use of the relation $\nabla_KX-\nabla_XK=[K,X]$ etc.",,"['differential-geometry', 'riemannian-geometry', 'connections', 'lie-derivative']"
65,How to prove the distance isometry is a smooth on a Riemannian manifold?,How to prove the distance isometry is a smooth on a Riemannian manifold?,,"Suppose $(M,g)$ is a Riemannian manifold, $d$ the induced metric (by geodesic lengths), and $f$ is an isometry on $M$ in the metric space sense. I want to prove $f$ is a diffeomorphism using the exponential map. But the first thing is I want to prove smoothness. One fact I know is that $f$ sends geodesics to geodesics. So here I go. For each $p$, consider some $\epsilon>0$ so that we have a geodesic ball $\exp_p(\bar B_\epsilon(0))$ around $p$ (in which $\bar B$ means closed balls in $T_pM$). Within this ball, I want to show $f$ is smooth. For each $q\in \exp_p(\bar B_\epsilon(0))$ there exists a unique $v_q\in T_pM$ so that $\|v_q\|\le\epsilon$ and that $\exp_p(v_q)=q$. Thus $$f(q)=\exp_{F(p)}(d(f)_p v_q)$$ since $d(f)_p$ preserves lengths in tangent vectors. (Here $df$ denotes the differential of $f$, not to be confused with the metric $d$.) However, I'm having trouble showing why $q\mapsto v_q\in T_pM$ is smooth. Is there any explicit expression of this map? (I know it is continuous of course (and indeed an isometry) since $d(p,q)=\|v_q\|$. But how to show the direction of $v_q$ also varies smoothly with $q$?)","Suppose $(M,g)$ is a Riemannian manifold, $d$ the induced metric (by geodesic lengths), and $f$ is an isometry on $M$ in the metric space sense. I want to prove $f$ is a diffeomorphism using the exponential map. But the first thing is I want to prove smoothness. One fact I know is that $f$ sends geodesics to geodesics. So here I go. For each $p$, consider some $\epsilon>0$ so that we have a geodesic ball $\exp_p(\bar B_\epsilon(0))$ around $p$ (in which $\bar B$ means closed balls in $T_pM$). Within this ball, I want to show $f$ is smooth. For each $q\in \exp_p(\bar B_\epsilon(0))$ there exists a unique $v_q\in T_pM$ so that $\|v_q\|\le\epsilon$ and that $\exp_p(v_q)=q$. Thus $$f(q)=\exp_{F(p)}(d(f)_p v_q)$$ since $d(f)_p$ preserves lengths in tangent vectors. (Here $df$ denotes the differential of $f$, not to be confused with the metric $d$.) However, I'm having trouble showing why $q\mapsto v_q\in T_pM$ is smooth. Is there any explicit expression of this map? (I know it is continuous of course (and indeed an isometry) since $d(p,q)=\|v_q\|$. But how to show the direction of $v_q$ also varies smoothly with $q$?)",,"['differential-geometry', 'manifolds', 'riemannian-geometry', 'isometry']"
66,Integral curve is periodic if and only if it is compact,Integral curve is periodic if and only if it is compact,,"Suppose we have a complete, non-singular vector field on $M$. Take an integral curve $\phi: \mathbb{R} \rightarrow M$. If it is periodic than $\phi(\mathbb{R})$ is compact as an image of a compact set. Suppose $\phi(\mathbb{R})$ is compact, I have a problem with showing it is periodic, which I suppose is trivial and that even there is no arbitrary, continuous bijection from $\mathbb{R}$ to a compact space. EDIT: Since it may not be so trivial as I expected let me be very precise. M is compact smooth manifold, we have a smooth (necessarily complete since $M$ is compact) non-singular vector field on it. Is it true that an integral curve compact in induced topology must be periodic?","Suppose we have a complete, non-singular vector field on $M$. Take an integral curve $\phi: \mathbb{R} \rightarrow M$. If it is periodic than $\phi(\mathbb{R})$ is compact as an image of a compact set. Suppose $\phi(\mathbb{R})$ is compact, I have a problem with showing it is periodic, which I suppose is trivial and that even there is no arbitrary, continuous bijection from $\mathbb{R}$ to a compact space. EDIT: Since it may not be so trivial as I expected let me be very precise. M is compact smooth manifold, we have a smooth (necessarily complete since $M$ is compact) non-singular vector field on it. Is it true that an integral curve compact in induced topology must be periodic?",,"['general-topology', 'differential-geometry', 'differential-topology', 'dynamical-systems']"
67,$C^2$ isometric embedding of the flat torus into $\mathbb{R}^3$,isometric embedding of the flat torus into,C^2 \mathbb{R}^3,"What is the reason that there is no $C^2$ isometric embedding of the flat torus inside $\mathbb{R}^3$? Is there an explicit proof of this fact anywhere? The flat metric must violate some condition for the $C^2$ isometric embedding. And as we know, the condition for a local $C^2$ isometric embedding is given by the Gauss and Codazzi-Mainardi relations. I do not understand how these relations are getting violated by the flat metric on torus. Kindly cite some reference. Thanks in advance. Edit 1: I am following this paper: Han and Lin, On the isometric embedding of torus in $\mathbb{R}^3$, Methods and Applications of Analysis 15 , pp. 197-204, 2008. Here, the sufficient conditions for the existence of a global smooth isometric embedding of the torus of genus 1 $\mathbb{T}$ with a Riemannian metric $a$ $(\mathbb{T}, a)$ are given. But these conditions, as can be clearly seen, are given for the existence of the standard embedded torus in $\mathbb{R}^3$, which is too strict. The original question is about the existence of an isometrically embedded torus in $\mathbb{R}^3$, not necessarily the tadard torus. So there must be a different set of sufficient conditions than the ones given in the above reference. Can one of these sufficient conditions be the requirement that the the subset of $\mathbb{T}$ where the Gaussian curvature $K$ of $a$ is positive is non-empty ?","What is the reason that there is no $C^2$ isometric embedding of the flat torus inside $\mathbb{R}^3$? Is there an explicit proof of this fact anywhere? The flat metric must violate some condition for the $C^2$ isometric embedding. And as we know, the condition for a local $C^2$ isometric embedding is given by the Gauss and Codazzi-Mainardi relations. I do not understand how these relations are getting violated by the flat metric on torus. Kindly cite some reference. Thanks in advance. Edit 1: I am following this paper: Han and Lin, On the isometric embedding of torus in $\mathbb{R}^3$, Methods and Applications of Analysis 15 , pp. 197-204, 2008. Here, the sufficient conditions for the existence of a global smooth isometric embedding of the torus of genus 1 $\mathbb{T}$ with a Riemannian metric $a$ $(\mathbb{T}, a)$ are given. But these conditions, as can be clearly seen, are given for the existence of the standard embedded torus in $\mathbb{R}^3$, which is too strict. The original question is about the existence of an isometrically embedded torus in $\mathbb{R}^3$, not necessarily the tadard torus. So there must be a different set of sufficient conditions than the ones given in the above reference. Can one of these sufficient conditions be the requirement that the the subset of $\mathbb{T}$ where the Gaussian curvature $K$ of $a$ is positive is non-empty ?",,['differential-geometry']
68,Why is the stereographic projection a map from $S^2\to\mathbb{R}^2$?,Why is the stereographic projection a map from ?,S^2\to\mathbb{R}^2,"How can I understand that the stereographic projection $$X=\cot\left(\frac{\theta}{2}\right)\cos\phi,\hspace{0.5cm}Y=\cot\left(\frac{\theta}{2}\right)\sin\phi\tag{1}$$ is a map from the surface of a unit sphere to a plane? For (1) to be map from $S^2\to \mathbb{R}^2$, shouldn't $X,Y$ vary from $-\infty$ to $+\infty$ for the allowed ranges of $\theta,\phi$? Does it suffice to see that $X,Y$ are real? I think it's not because $\theta,\phi$ themselves were real. Then why is this a map from $S^2\to \mathbb{R}^2$ and not from $S^2\to \mathcal{M}$ where $\mathcal{M}$ is some other 2-dimensional manifold?","How can I understand that the stereographic projection $$X=\cot\left(\frac{\theta}{2}\right)\cos\phi,\hspace{0.5cm}Y=\cot\left(\frac{\theta}{2}\right)\sin\phi\tag{1}$$ is a map from the surface of a unit sphere to a plane? For (1) to be map from $S^2\to \mathbb{R}^2$, shouldn't $X,Y$ vary from $-\infty$ to $+\infty$ for the allowed ranges of $\theta,\phi$? Does it suffice to see that $X,Y$ are real? I think it's not because $\theta,\phi$ themselves were real. Then why is this a map from $S^2\to \mathbb{R}^2$ and not from $S^2\to \mathcal{M}$ where $\mathcal{M}$ is some other 2-dimensional manifold?",,"['differential-geometry', 'manifolds', 'riemannian-geometry', 'stereographic-projections', 'open-map']"
69,Dimensionality/Combinatorial Argument for Parallelizability of 1-Manifolds,Dimensionality/Combinatorial Argument for Parallelizability of 1-Manifolds,,"It just occurred to me that all one-dimensional manifolds are parallelizable. We clearly have examples of many, many (many) manifolds that are not, but since any one-dimensional manifold can be written as a countable disjoint union of $\mathbb{S}^1$ or $\mathbb{R}$, and both of these spaces are parallelizable, then any one-dimensional manifold is as well. I was wondering if there's anything in the classical literature or modern research literature that gives a proof of this simply based on dimensionality.  Is there a particular reason 1-manifolds all have this property even though it doesn't extend to higher dimensions?  Is there a particular property of manifolds this is indicative of?","It just occurred to me that all one-dimensional manifolds are parallelizable. We clearly have examples of many, many (many) manifolds that are not, but since any one-dimensional manifold can be written as a countable disjoint union of $\mathbb{S}^1$ or $\mathbb{R}$, and both of these spaces are parallelizable, then any one-dimensional manifold is as well. I was wondering if there's anything in the classical literature or modern research literature that gives a proof of this simply based on dimensionality.  Is there a particular reason 1-manifolds all have this property even though it doesn't extend to higher dimensions?  Is there a particular property of manifolds this is indicative of?",,"['differential-geometry', 'reference-request', 'manifolds']"
70,Product Einstein Manifolds,Product Einstein Manifolds,,In the book Einstein Manifolds by Besse it states the product of two Riemannian manifolds which are Einstein with the same constant $\lambda$ is an Einstein manifold with the same constant $\lambda$. Can someone provide a proof of this? Also what happens if the two manifolds are Einstein with different constants. Is the resulting product manifold still Einstein?,In the book Einstein Manifolds by Besse it states the product of two Riemannian manifolds which are Einstein with the same constant $\lambda$ is an Einstein manifold with the same constant $\lambda$. Can someone provide a proof of this? Also what happens if the two manifolds are Einstein with different constants. Is the resulting product manifold still Einstein?,,"['differential-geometry', 'manifolds']"
71,Universal bundle over $\mathbb P^{n}$ and direct sum of bundles,Universal bundle over  and direct sum of bundles,\mathbb P^{n},"Let $T(\mathbb{R}P^{n})$ be a tangent bundle to a $n$-dimensional projective space and let also denote $\gamma_{1}$ to be a trivial bundle of rank $1$. Moreover, let $E \rightarrow \mathbb{R}P^{n}$ be a universal (tautological) bundle. I would like to prove that $$T(\mathbb{R} P^{n}) \oplus \gamma_{1} = E \oplus E \oplus \ldots E = \bigoplus_{i=1}^{n} E$$ On the one hand, it may looks as if it follows from the existence of the Euler exact sequence of sheaves, written in the following way: $$ 0 \rightarrow \mathcal{O}_{\mathbb{P}^{n}} \rightarrow \mathcal{O}(1)^{\oplus (n+1)} \rightarrow \mathcal{T}_{\mathbb{P}^{n}} \rightarrow 0$$ One can show that this sequence splits (though not canonically???). Are there any ways to derive the statement from the propositon above or are there any easier ways to obtain the desired result (maybe, by pulling back the $T(\mathbb{R}P^{n})$ via $f: S^{n} \rightarrow RP^{n}$?)","Let $T(\mathbb{R}P^{n})$ be a tangent bundle to a $n$-dimensional projective space and let also denote $\gamma_{1}$ to be a trivial bundle of rank $1$. Moreover, let $E \rightarrow \mathbb{R}P^{n}$ be a universal (tautological) bundle. I would like to prove that $$T(\mathbb{R} P^{n}) \oplus \gamma_{1} = E \oplus E \oplus \ldots E = \bigoplus_{i=1}^{n} E$$ On the one hand, it may looks as if it follows from the existence of the Euler exact sequence of sheaves, written in the following way: $$ 0 \rightarrow \mathcal{O}_{\mathbb{P}^{n}} \rightarrow \mathcal{O}(1)^{\oplus (n+1)} \rightarrow \mathcal{T}_{\mathbb{P}^{n}} \rightarrow 0$$ One can show that this sequence splits (though not canonically???). Are there any ways to derive the statement from the propositon above or are there any easier ways to obtain the desired result (maybe, by pulling back the $T(\mathbb{R}P^{n})$ via $f: S^{n} \rightarrow RP^{n}$?)",,"['differential-geometry', 'vector-bundles', 'fiber-bundles', 'tangent-bundle']"
72,Is the set of all conformal structures on $\mathbb{R}^n$ a manifold? Does it have a name?,Is the set of all conformal structures on  a manifold? Does it have a name?,\mathbb{R}^n,"Question: Is the set of all conformal structures on $\mathbb{R}^n$ a manifold? Does it have a name? A pointer to a reference will suffice. Definition: A conformal structure on $\mathbb{R}^n$ is an equivalence class of inner products, with two inner products $f,g$ equivalent, $f \sim g$, if and only $f = \lambda g$ for some $\lambda >0$. In other words, it is an inner product ""up to positive scaling"". We need to specify an inner product to determine whether a set of vectors is orthonormal, but we only need to specify a conformal structure to determine whether a set of vectors is orthogonal. An inner product determines a notion of both length and angle, but a conformal structure only determines a notion of angle, not of length. This is analogous to how a norm only determines a notion of length, but not of angle -- in fact, it makes sense to think of a conformal structure as ""an inner product minus a choice of norm"". Attempt: Consider the following fact (cf. p. 201, Linear Algebra via Exterior Products ): If $\{ e_1, \dots, e_n\}$ is an arbitrary basis in $V$, then there exists an inner product with respect to which $\{e_1, \dots, e_n \}$ is an orthonormal basis. Thus, start with the non-compact $n$-Stiefel manifold , the set of all bases on $\mathbb{R}^n$. By the above fact, each element corresponds to an inner product (the inner product which makes the basis orthonormal). So we can impose an equivalence relation on the non-compact $n$-Stiefel manifold , saying that two bases are equivalent if and only if they are orthonormal with respect to the same inner product. Since the equivalence classes of this equivalence relation are (I imagine) diffeomorphic to the compact $n$-Stiefel manifold , this might be a homogeneous space . Now to get from this space (the space of all inner products) to the space of all conformal structures on $\mathbb{R}^n$ is simple -- we just quotient by the equivalence relation which says that two inner products are equivalent if and only if they are the same up to positive scaling, i.e. if and only if they belong to the same conformal structure . Since this is the same thing (I think) as quotienting by the action of $\mathbb{R}^{+}$, which is a Lie group, then, if the space of all inner products was a homogeneous space , the final space, the space of all conformal structures on $\mathbb{R}^n$, will also be a homogeneous space . Since homogeneous spaces are manifolds, and the space of all conformal structures on $\mathbb{R}^n$ might be a homogeneous space , then it might be a manifold. Obviously this says nothing about what the name of such a structure might be, except perhaps that is related to Stiefel manifolds via the actions of multiple Lie groups.","Question: Is the set of all conformal structures on $\mathbb{R}^n$ a manifold? Does it have a name? A pointer to a reference will suffice. Definition: A conformal structure on $\mathbb{R}^n$ is an equivalence class of inner products, with two inner products $f,g$ equivalent, $f \sim g$, if and only $f = \lambda g$ for some $\lambda >0$. In other words, it is an inner product ""up to positive scaling"". We need to specify an inner product to determine whether a set of vectors is orthonormal, but we only need to specify a conformal structure to determine whether a set of vectors is orthogonal. An inner product determines a notion of both length and angle, but a conformal structure only determines a notion of angle, not of length. This is analogous to how a norm only determines a notion of length, but not of angle -- in fact, it makes sense to think of a conformal structure as ""an inner product minus a choice of norm"". Attempt: Consider the following fact (cf. p. 201, Linear Algebra via Exterior Products ): If $\{ e_1, \dots, e_n\}$ is an arbitrary basis in $V$, then there exists an inner product with respect to which $\{e_1, \dots, e_n \}$ is an orthonormal basis. Thus, start with the non-compact $n$-Stiefel manifold , the set of all bases on $\mathbb{R}^n$. By the above fact, each element corresponds to an inner product (the inner product which makes the basis orthonormal). So we can impose an equivalence relation on the non-compact $n$-Stiefel manifold , saying that two bases are equivalent if and only if they are orthonormal with respect to the same inner product. Since the equivalence classes of this equivalence relation are (I imagine) diffeomorphic to the compact $n$-Stiefel manifold , this might be a homogeneous space . Now to get from this space (the space of all inner products) to the space of all conformal structures on $\mathbb{R}^n$ is simple -- we just quotient by the equivalence relation which says that two inner products are equivalent if and only if they are the same up to positive scaling, i.e. if and only if they belong to the same conformal structure . Since this is the same thing (I think) as quotienting by the action of $\mathbb{R}^{+}$, which is a Lie group, then, if the space of all inner products was a homogeneous space , the final space, the space of all conformal structures on $\mathbb{R}^n$, will also be a homogeneous space . Since homogeneous spaces are manifolds, and the space of all conformal structures on $\mathbb{R}^n$ might be a homogeneous space , then it might be a manifold. Obviously this says nothing about what the name of such a structure might be, except perhaps that is related to Stiefel manifolds via the actions of multiple Lie groups.",,"['differential-geometry', 'reference-request', 'terminology', 'homogeneous-spaces', 'stiefel-manifolds']"
73,sphere bundles isomorphic to vector bundle.,sphere bundles isomorphic to vector bundle.,,"Bredon claims that sphere bundles in certain cases are isomorphic to vector bundles. For example he says just replace $S^{n-1}$ with $R^n$. But for example the circle and the plane are not even isomorphic. How can we talk about bundle isomorphism when even the fibers are not isomorphic? ""A disk or sphere bundle gives rise to a vector bundle with the orthogonal group as the structure group just by replacing the fiber $D^n$ or $S^{n-1}$ with $R^n$ and using the same change of coordinate function"". We cannot just ""replace"" the fibers with something not isomorphic can we?","Bredon claims that sphere bundles in certain cases are isomorphic to vector bundles. For example he says just replace $S^{n-1}$ with $R^n$. But for example the circle and the plane are not even isomorphic. How can we talk about bundle isomorphism when even the fibers are not isomorphic? ""A disk or sphere bundle gives rise to a vector bundle with the orthogonal group as the structure group just by replacing the fiber $D^n$ or $S^{n-1}$ with $R^n$ and using the same change of coordinate function"". We cannot just ""replace"" the fibers with something not isomorphic can we?",,"['differential-geometry', 'algebraic-topology', 'vector-bundles', 'fiber-bundles']"
74,volume of geodesic balls for large Riemannian metrics,volume of geodesic balls for large Riemannian metrics,,"Let $M$ be a smooth compact manifold, let $p\in M$ and let $g_0$ be a fixed Riemannian metric on $M$ . Does there exists a constant $C>0$ such that for any Riemannian metric $g\ge g_0$ , the volume of the geodesic ball with respect to $g$ satisfies $Vol_g(B_g(1,p))\ge C$ ? In other words, can we expect that the volume of these balls does not shrink even after making the Riemannian metric really large? Attempt: All of the results I've researched on this topic involve a fixed Riemannian metric on $M$ and bounding volumes in terms of the injectivity/filling radius. Nothing I've seen so far allows for the Riemannian metric to change as well. I suppose the result has to be true for $\mathbb{S}^1$ because volume is the same as length, so for any large Riemannian metric, any geodesic ball will have a volume of $1$ as long as the volume of $\mathbb{S}^1$ itself is above $1$ .","Let be a smooth compact manifold, let and let be a fixed Riemannian metric on . Does there exists a constant such that for any Riemannian metric , the volume of the geodesic ball with respect to satisfies ? In other words, can we expect that the volume of these balls does not shrink even after making the Riemannian metric really large? Attempt: All of the results I've researched on this topic involve a fixed Riemannian metric on and bounding volumes in terms of the injectivity/filling radius. Nothing I've seen so far allows for the Riemannian metric to change as well. I suppose the result has to be true for because volume is the same as length, so for any large Riemannian metric, any geodesic ball will have a volume of as long as the volume of itself is above .","M p\in M g_0 M C>0 g\ge g_0 g Vol_g(B_g(1,p))\ge C M \mathbb{S}^1 1 \mathbb{S}^1 1",['differential-geometry']
75,Curvature and Torsion in terms of Geodesic Curvature,Curvature and Torsion in terms of Geodesic Curvature,,"Let $γ : I → S^2$ be a regular curve in the 2-sphere. Let $\kappa_g$ denote the geodesic curvature . Regarding the curve $γ$ as a space curve $S^2 ⊂ \mathbb R^3$ and assuming it to be Frenet, calculate its curvature $κ$ and torsion $τ$ in terms of $\kappa_g$. So we have $$\kappa_g = \langle T', \gamma \times T\rangle = \langle\gamma'', \gamma \times \gamma'\rangle$$ where the brackets denote the inner product, where $$T = \frac {d\gamma}{dt}, $$ and where the Frenet frame is the 3-dimensional orthonormal basis $(\gamma', \gamma \times \gamma', \gamma)$. The curvature of a space curve is given by $$\kappa = \frac{\lVert \gamma \times \gamma'\rVert}{\lVert\gamma'\rVert^3}$$ The Torsion is given by $$\tau = \frac{\langle\gamma' \times \gamma'',\gamma'''\rangle}{\lVert\gamma' \times \gamma''\rVert^3}$$ So basically the objective is to write $\kappa$ and $\tau$ in terms of $\kappa_g$? I'm not really sure how it's possible to get something like $\lVert\gamma'\rVert^3$ out of the definition of $\kappa_g$. On the other hand it sort of looks like we could obtain the numerator for $\tau$ by just differentiating $\kappa_g$ and noticing that both are of the form of a scalar triple product with the inner product between the two lower order derivatives and the highest order derivative with the numerator of $\kappa_g$ is one degree less than that of the numerator of $\tau$. Can anyone let me know if I'm on the right track or perhaps suggest how I might acquire the entire expression(s) for $\kappa$ and $\tau$ from $\kappa_g$?","Let $γ : I → S^2$ be a regular curve in the 2-sphere. Let $\kappa_g$ denote the geodesic curvature . Regarding the curve $γ$ as a space curve $S^2 ⊂ \mathbb R^3$ and assuming it to be Frenet, calculate its curvature $κ$ and torsion $τ$ in terms of $\kappa_g$. So we have $$\kappa_g = \langle T', \gamma \times T\rangle = \langle\gamma'', \gamma \times \gamma'\rangle$$ where the brackets denote the inner product, where $$T = \frac {d\gamma}{dt}, $$ and where the Frenet frame is the 3-dimensional orthonormal basis $(\gamma', \gamma \times \gamma', \gamma)$. The curvature of a space curve is given by $$\kappa = \frac{\lVert \gamma \times \gamma'\rVert}{\lVert\gamma'\rVert^3}$$ The Torsion is given by $$\tau = \frac{\langle\gamma' \times \gamma'',\gamma'''\rangle}{\lVert\gamma' \times \gamma''\rVert^3}$$ So basically the objective is to write $\kappa$ and $\tau$ in terms of $\kappa_g$? I'm not really sure how it's possible to get something like $\lVert\gamma'\rVert^3$ out of the definition of $\kappa_g$. On the other hand it sort of looks like we could obtain the numerator for $\tau$ by just differentiating $\kappa_g$ and noticing that both are of the form of a scalar triple product with the inner product between the two lower order derivatives and the highest order derivative with the numerator of $\kappa_g$ is one degree less than that of the numerator of $\tau$. Can anyone let me know if I'm on the right track or perhaps suggest how I might acquire the entire expression(s) for $\kappa$ and $\tau$ from $\kappa_g$?",,"['differential-geometry', 'curves', 'curvature', 'geodesic']"
76,Intuition for the Dual Space in Finite Dimensional Vector space,Intuition for the Dual Space in Finite Dimensional Vector space,,"Let me give you an example of how I think of linear operators first. Consider a linear operator $A: \mathcal{V} \to \mathcal{V}$. We have $A(\mu {\bf v} + \lambda {\bf w}) = \mu A({\bf v}) + \lambda A({\bf w})$ holding identically for $\mu, \lambda \in \mathbb{K}$ and ${\bf v}, {\bf w} \in \mathcal{V}$. Therefore, the set of all linear operators on $\mathcal{V}$ forms a vector space in its own right. Suppose $\mathcal{V} = \mathbb{R}^3$. To visualize this, I typically imagine $A$ as being represented by a 9D vector. This helps me quickly see various orthogonal decompositions into its symmetric and skew pieces or its spherical and deviatoric pieces. I have tended to do the same thing for the dual space, $\mathcal{V}^*$; that is, I have been imagining $\mathcal{V}$ and $\mathcal{V}^*$ as ${\it totally \ separate \ spaces}$. They are objects of different type after all. I have been thinking of elements of $\mathcal{V}$ as geometric vectors and elements of $\mathcal{V}^*$ as ""things that operate"" on those vectors. But I've just reached a hurdle. Say we are working in $\mathbb{R}^2$ with a set of geometric vectors. It is common to represent a vector ${\bf x} \in \mathbb{R}^2$ on either a covariant or contravariant basis: ${\bf x} = x_i {\bf a}^i = x^i {\bf a}_i$ with the property that ${\bf a}_i \cdot {\bf a}^j = \delta_i^j$. But wait a second. I've been doing some reading and is it true that ${\bf a}_i \in \mathbb{R}^2$ but ${\bf a}^j \in (\mathbb{R}^2)^*$? If so, then we are layering the two spaces on one. And the dual set is not to be thought of as a set of linear functionals but as a set of geometric vectors that are available which have a nice duality property. So is it true that in this interpretation, if we write ${\bf x} = x_j {\bf a}^j$ it is to be thought of as a linear functional, but if we write ${\bf x} = x^j {\bf a}_j$ then it is to be thought as a geometric vector? But ${\bf x}$ is supposed to represent the ${\it same}$ object, and elements of $\mathbb{R}^2$ and $(\mathbb{R}^2)^*$ are fundamentally different! How is this possible?","Let me give you an example of how I think of linear operators first. Consider a linear operator $A: \mathcal{V} \to \mathcal{V}$. We have $A(\mu {\bf v} + \lambda {\bf w}) = \mu A({\bf v}) + \lambda A({\bf w})$ holding identically for $\mu, \lambda \in \mathbb{K}$ and ${\bf v}, {\bf w} \in \mathcal{V}$. Therefore, the set of all linear operators on $\mathcal{V}$ forms a vector space in its own right. Suppose $\mathcal{V} = \mathbb{R}^3$. To visualize this, I typically imagine $A$ as being represented by a 9D vector. This helps me quickly see various orthogonal decompositions into its symmetric and skew pieces or its spherical and deviatoric pieces. I have tended to do the same thing for the dual space, $\mathcal{V}^*$; that is, I have been imagining $\mathcal{V}$ and $\mathcal{V}^*$ as ${\it totally \ separate \ spaces}$. They are objects of different type after all. I have been thinking of elements of $\mathcal{V}$ as geometric vectors and elements of $\mathcal{V}^*$ as ""things that operate"" on those vectors. But I've just reached a hurdle. Say we are working in $\mathbb{R}^2$ with a set of geometric vectors. It is common to represent a vector ${\bf x} \in \mathbb{R}^2$ on either a covariant or contravariant basis: ${\bf x} = x_i {\bf a}^i = x^i {\bf a}_i$ with the property that ${\bf a}_i \cdot {\bf a}^j = \delta_i^j$. But wait a second. I've been doing some reading and is it true that ${\bf a}_i \in \mathbb{R}^2$ but ${\bf a}^j \in (\mathbb{R}^2)^*$? If so, then we are layering the two spaces on one. And the dual set is not to be thought of as a set of linear functionals but as a set of geometric vectors that are available which have a nice duality property. So is it true that in this interpretation, if we write ${\bf x} = x_j {\bf a}^j$ it is to be thought of as a linear functional, but if we write ${\bf x} = x^j {\bf a}_j$ then it is to be thought as a geometric vector? But ${\bf x}$ is supposed to represent the ${\it same}$ object, and elements of $\mathbb{R}^2$ and $(\mathbb{R}^2)^*$ are fundamentally different! How is this possible?",,"['linear-algebra', 'differential-geometry']"
77,Only one chart to parameterize a unit-cylinder,Only one chart to parameterize a unit-cylinder,,"I want to parameterize a unit-cylinder $x^2+y^2=1$ with only one chart in a complete atlas (the sets must be open). The cylinder is in $\mathbb{R}^3$. One way to do the parametrization with two charts is: $$ \textbf{x}(u,v)=(\cos u, \sin u, v)$$ with $v\in \mathbb{R}$ and $0<u< 2\pi$ (this is the open set $U_1=(0,2\pi)\times \mathbb{R}$) and $$ \textbf{y}(\overline{u}, \overline{v}) = (\cos \overline{u}, \sin \overline{u}, \overline{v})$$ with $\overline{v}\equiv v\in \mathbb{R}$ and $-\pi < \overline{u} < \pi$ (this is the open set $U_2 = (-\pi,\pi)\times\mathbb{R}$). So the atlas is $\{ \textbf{x}, \textbf{y}\}$ into $U_1 \times U_2.$ So, it has two charts $\textbf{x}$ and $\textbf{y}$ defined in open sets (this is important: the set $(0,2 \pi]\times \mathbb{R}$ is not open , and a parametrization has to be defined into an open set). Do someone know an atlas with only one chart? For example, with a reparametrization. The only thing I know is that the atlas with one chart exists in this case$^*$. * Introduction to Topological Quantum Matter & Quantum Computation, Tudor D. Stanescu PD: Something similar in General Relativistic to changing Schwarzchild coordinates by Kruskal–Szekeres ones to extend the well-behaved domain.","I want to parameterize a unit-cylinder $x^2+y^2=1$ with only one chart in a complete atlas (the sets must be open). The cylinder is in $\mathbb{R}^3$. One way to do the parametrization with two charts is: $$ \textbf{x}(u,v)=(\cos u, \sin u, v)$$ with $v\in \mathbb{R}$ and $0<u< 2\pi$ (this is the open set $U_1=(0,2\pi)\times \mathbb{R}$) and $$ \textbf{y}(\overline{u}, \overline{v}) = (\cos \overline{u}, \sin \overline{u}, \overline{v})$$ with $\overline{v}\equiv v\in \mathbb{R}$ and $-\pi < \overline{u} < \pi$ (this is the open set $U_2 = (-\pi,\pi)\times\mathbb{R}$). So the atlas is $\{ \textbf{x}, \textbf{y}\}$ into $U_1 \times U_2.$ So, it has two charts $\textbf{x}$ and $\textbf{y}$ defined in open sets (this is important: the set $(0,2 \pi]\times \mathbb{R}$ is not open , and a parametrization has to be defined into an open set). Do someone know an atlas with only one chart? For example, with a reparametrization. The only thing I know is that the atlas with one chart exists in this case$^*$. * Introduction to Topological Quantum Matter & Quantum Computation, Tudor D. Stanescu PD: Something similar in General Relativistic to changing Schwarzchild coordinates by Kruskal–Szekeres ones to extend the well-behaved domain.",,['differential-geometry']
78,Why is the geometric locus of points equidistant to two other points in a two-dimensional Riemannian manifold a geodesic?,Why is the geometric locus of points equidistant to two other points in a two-dimensional Riemannian manifold a geodesic?,,"Let $M$ be a 2-dimensional Riemannian manifold, $x,y \in M$. Why is the set of points $\{z | d(z,x) = d(z,y)\}$ a geodesic? What can we say about higher-dimensional Riemannian manifolds?","Let $M$ be a 2-dimensional Riemannian manifold, $x,y \in M$. Why is the set of points $\{z | d(z,x) = d(z,y)\}$ a geodesic? What can we say about higher-dimensional Riemannian manifolds?",,"['differential-geometry', 'riemannian-geometry', 'geodesic']"
79,Smooth function between compact manifold and connected manifold,Smooth function between compact manifold and connected manifold,,"I have this problem and I don't know how to proceed with it: Let $M$ be a compact manifold $N$ a connected manifold and $f\colon M \longrightarrow N$ a smooth function. If $f$ doesn't have critical points, then prove that $f$ is surjective. What I know is that since $f$ doesn't have critical points, all the points in $N$ are regular. Since $M$ is compact, then $f(M)$ is compact too. Now, if $n\in N$ is a point such that $n\notin f(M)$ , then I can define the function $g\colon M \longrightarrow \mathbb{R}$ where $g$ defines the shortest length from $x\in M$ to $n$ , I know that this function must have a maximum and a minimum, but from this the only thing I can infer is that there is an open neighborhood for $n$ in $N$ . What can I do to finish the problem? EDIT: $M$ and $N$ are of the same dimension.","I have this problem and I don't know how to proceed with it: Let be a compact manifold a connected manifold and a smooth function. If doesn't have critical points, then prove that is surjective. What I know is that since doesn't have critical points, all the points in are regular. Since is compact, then is compact too. Now, if is a point such that , then I can define the function where defines the shortest length from to , I know that this function must have a maximum and a minimum, but from this the only thing I can infer is that there is an open neighborhood for in . What can I do to finish the problem? EDIT: and are of the same dimension.",M N f\colon M \longrightarrow N f f f N M f(M) n\in N n\notin f(M) g\colon M \longrightarrow \mathbb{R} g x\in M n n N M N,"['differential-geometry', 'smooth-functions', 'compact-manifolds']"
80,Can dual vector spaces be thought of as linear coordinate functions?,Can dual vector spaces be thought of as linear coordinate functions?,,"Note: This question seems like it might have been asked before, but the poster deleted it and I don't have enough reputation to see it. Let $V$ be a finite-dimensional real vector space. Such an object is, in a canonical way, a smooth manifold. Thus it makes sense to speak of smooth charts $(U,\varphi)$ for such a space (where $U \subseteq V$ is open of course). Let us restrict attention to those charts for which $U=V$. Then if the chart function $\varphi: V \to \mathbb{R}^n$ is linear, each of its component functions (which I call coordinate functions, since each specifies a different coordinate of the chart) is a linear function $V \to \mathbb{R}$, thus an element of $V^*$. (Obviously none of this works over arbitrary fields or for infinite-dimensional vector spaces, hence the restrictions I assume/gave above.) Question: Thus, does it make sense to interpret or even define dual bases to be linear charts from $V$ to $\mathbb{R}^n$, and $V^*$ to be the space of all linear coordinate functions? In particular, this would provide a ready explanation for why the components of a vector are written with the opposite index placement when using the Einstein summation convention. If $E = x^i E_i$ is a vector in $V$, then the components $x^i$ could be interpreted as short-hand for the evaluation of the linear coordinate functions (i.e. elements of $V^*$) evaluated at $E$ rather than just scalars, i.e. $$E = x^i(E) E_i \,.$$ Since the $x^i$ are elements of $V^*$, rather than scalars in $\mathbb{R}$, it becomes quite obvious why they should have the index placement they do. Also components/linear coordinates transform the same way that dual vectors do under changes of basis (i.e. they are covariant), so thinking about vectors/dual spaces/tensors the way physicists do, interpreting components/linear coordinate functions as being the same thing as elements of $V^*$ seems to make a certain amount of sense. It might also make dual vector spaces easier to understand for those who tend to think physically or geometrically ( e.g. ). However, I have not seen this practice mentioned in any texts on differential geometry or linear algebra which I have ever read, so I imagine that there are probably problems with it which I am not noticing or understanding yet.","Note: This question seems like it might have been asked before, but the poster deleted it and I don't have enough reputation to see it. Let $V$ be a finite-dimensional real vector space. Such an object is, in a canonical way, a smooth manifold. Thus it makes sense to speak of smooth charts $(U,\varphi)$ for such a space (where $U \subseteq V$ is open of course). Let us restrict attention to those charts for which $U=V$. Then if the chart function $\varphi: V \to \mathbb{R}^n$ is linear, each of its component functions (which I call coordinate functions, since each specifies a different coordinate of the chart) is a linear function $V \to \mathbb{R}$, thus an element of $V^*$. (Obviously none of this works over arbitrary fields or for infinite-dimensional vector spaces, hence the restrictions I assume/gave above.) Question: Thus, does it make sense to interpret or even define dual bases to be linear charts from $V$ to $\mathbb{R}^n$, and $V^*$ to be the space of all linear coordinate functions? In particular, this would provide a ready explanation for why the components of a vector are written with the opposite index placement when using the Einstein summation convention. If $E = x^i E_i$ is a vector in $V$, then the components $x^i$ could be interpreted as short-hand for the evaluation of the linear coordinate functions (i.e. elements of $V^*$) evaluated at $E$ rather than just scalars, i.e. $$E = x^i(E) E_i \,.$$ Since the $x^i$ are elements of $V^*$, rather than scalars in $\mathbb{R}$, it becomes quite obvious why they should have the index placement they do. Also components/linear coordinates transform the same way that dual vectors do under changes of basis (i.e. they are covariant), so thinking about vectors/dual spaces/tensors the way physicists do, interpreting components/linear coordinate functions as being the same thing as elements of $V^*$ seems to make a certain amount of sense. It might also make dual vector spaces easier to understand for those who tend to think physically or geometrically ( e.g. ). However, I have not seen this practice mentioned in any texts on differential geometry or linear algebra which I have ever read, so I imagine that there are probably problems with it which I am not noticing or understanding yet.",,"['linear-algebra', 'differential-geometry', 'soft-question', 'definition', 'intuition']"
81,One form of poincare duality imply the other?,One form of poincare duality imply the other?,,"I saw in my differential geometry class that Poincare duality states: $H_{DR}^k(X,\mathbb{R}) \times H_{DR}^{n - k}(X,\mathbb{R})  \rightarrow \mathbb{R}$ given by $([v],[r]) \mapsto \int_{X} n \wedge r$ is non-singular Prof mentioned that If X is compact oriented manifold of dimension n, then this is really the same as saying: $H_{DR}^i(X,\mathbb{R}) \cong H_{n - i}(X,\mathbb{R})$ where the right side of the equation we have singular homology. Can someone explain why this is the case ?","I saw in my differential geometry class that Poincare duality states: $H_{DR}^k(X,\mathbb{R}) \times H_{DR}^{n - k}(X,\mathbb{R})  \rightarrow \mathbb{R}$ given by $([v],[r]) \mapsto \int_{X} n \wedge r$ is non-singular Prof mentioned that If X is compact oriented manifold of dimension n, then this is really the same as saying: $H_{DR}^i(X,\mathbb{R}) \cong H_{n - i}(X,\mathbb{R})$ where the right side of the equation we have singular homology. Can someone explain why this is the case ?",,"['differential-geometry', 'algebraic-topology']"
82,Are vector field flows and Feller processes related?,Are vector field flows and Feller processes related?,,"This is a follow-up to a previous question of mine . In particular I know now that the terminology used for Lie groups which I referenced is a special case of the subject of flows of smooth vector fields on smooth manifolds, like that discussed in Lee's Introduction to Smooth Manifolds . Question: Is there a general theory encompassing both the one-parameter semigroups of Feller processes and the one-parameter groups of flows of smooth vector fields? Obviously the spaces involved are in general quite different, but there seems to be many analogies, in particular both involve infinitesimal generators and exponential maps. Perhaps an answer will be related to the theory of stochastic processes on Riemannian manifolds, I don't know. A reference will suffice for an answer.","This is a follow-up to a previous question of mine . In particular I know now that the terminology used for Lie groups which I referenced is a special case of the subject of flows of smooth vector fields on smooth manifolds, like that discussed in Lee's Introduction to Smooth Manifolds . Question: Is there a general theory encompassing both the one-parameter semigroups of Feller processes and the one-parameter groups of flows of smooth vector fields? Obviously the spaces involved are in general quite different, but there seems to be many analogies, in particular both involve infinitesimal generators and exponential maps. Perhaps an answer will be related to the theory of stochastic processes on Riemannian manifolds, I don't know. A reference will suffice for an answer.",,"['differential-geometry', 'reference-request', 'stochastic-processes']"
83,Show $e_1\wedge\cdots \wedge e_d$ is a non-zero vector in $\Lambda_d (V)$,Show  is a non-zero vector in,e_1\wedge\cdots \wedge e_d \Lambda_d (V),"Given $\{e_1,\cdots,e_d\}$ a basis of $V$, we have $e_1\wedge\cdots  \wedge e_d$ is a non-zero vector in the exterior algebra $\Lambda_d (V)$. Given a decomposable element which can be written as $v_1\wedge\cdots\wedge v_d$, it is the $0$ vector in $\Lambda_d (V)$ if $v_i = v_j$ for some $i,j$ between $1$ and $d$. So in general, the finite sum of decomposable elements like above are also zero. How would I argue that $e_1\wedge\cdots \wedge e_d$ can not be written as the sum of such elements.","Given $\{e_1,\cdots,e_d\}$ a basis of $V$, we have $e_1\wedge\cdots  \wedge e_d$ is a non-zero vector in the exterior algebra $\Lambda_d (V)$. Given a decomposable element which can be written as $v_1\wedge\cdots\wedge v_d$, it is the $0$ vector in $\Lambda_d (V)$ if $v_i = v_j$ for some $i,j$ between $1$ and $d$. So in general, the finite sum of decomposable elements like above are also zero. How would I argue that $e_1\wedge\cdots \wedge e_d$ can not be written as the sum of such elements.",,"['differential-geometry', 'differential-topology', 'exterior-algebra']"
84,Lie algebra of the two dimensional torus,Lie algebra of the two dimensional torus,,"In ""An Introduction To Manifold (2nd Edition)"" Remark 16.15, it says "" For the torus $\mathbb{R}^2 / \mathbb{Z}^2$, the Lie algebra $\mathfrak{g}$ has $\mathbb{R}^2$ as the underlying vector space and the one-dimensional Lie subalgebras are all the lines through the origin"". My questions are: What's the bracket of $\mathfrak{g}$ which has $\mathbb{R}^2$ as underlying vector space? To make $\mathbb{R}^2$ as a Lie algebra, we can define $[x,y] = 0, x,y\in \mathbb{R}^2$, or $[a,a]=[b,b]=0$ and $[a,b]=a$ where $a$ and $b$ are two basis of $\mathbb{R}^2$. Are both bracket definition valid for 2-torus? Are there possible other lines than those passing through the origin that are one-dimensional Lie subalgebras with bracket definitions other than above two? Thanks","In ""An Introduction To Manifold (2nd Edition)"" Remark 16.15, it says "" For the torus $\mathbb{R}^2 / \mathbb{Z}^2$, the Lie algebra $\mathfrak{g}$ has $\mathbb{R}^2$ as the underlying vector space and the one-dimensional Lie subalgebras are all the lines through the origin"". My questions are: What's the bracket of $\mathfrak{g}$ which has $\mathbb{R}^2$ as underlying vector space? To make $\mathbb{R}^2$ as a Lie algebra, we can define $[x,y] = 0, x,y\in \mathbb{R}^2$, or $[a,a]=[b,b]=0$ and $[a,b]=a$ where $a$ and $b$ are two basis of $\mathbb{R}^2$. Are both bracket definition valid for 2-torus? Are there possible other lines than those passing through the origin that are one-dimensional Lie subalgebras with bracket definitions other than above two? Thanks",,"['differential-geometry', 'manifolds', 'lie-algebras']"
85,Extending a basis to a symplectic basis,Extending a basis to a symplectic basis,,"Good afternoon! I tried to understand the following fact about symplectic linear algebra. Given a Lagrangian $L$ subspace of a symplectic vector space $(V,\Omega)$ , one can extend each basis of $L$ to a symplectic basis. I tried to do the proof by myself and by use of ""Ana Cannas da Silva-Lectures on Symplectic Geometry"" but I am still not sure whether it is ok. What do you think? The necessary condition that $\Omega(e_i, e_j)=0 ~\forall i,j=1,...,n$ is fullfilled bacause $L$ is a Lagrangian, meaning that $L = L^{\Omega} = \{v \in V: \Omega(v,l)=0~ \forall l \in L\}$ . Now we have to find $n$ elements ( $L$ Lagrangian, i.e. $\dim(V) = \frac{1}{2}\dim(L)$ ) $f_1, ..., f_n \in L^{\Omega}=L$ such that $\Omega(f_i, e_i) = 1$ , $\Omega(f_i, e_j) = 0$ and $\Omega(f_i, f_j)=0$ for all $i \neq j = 1,...,n$ . Let $\{e_1, ..., e_n\}$ be such a basis. (1) Define the set $W := span(e_2, e_3,...,e_n) \subset L$ . Since $\Omega$ is nondegenerate we can always find an element $\tilde{f}_1 \in W^{\Omega}=\{v \in V: \Omega(v,w)=0~ \forall w \in W\}$ with $\Omega(e_1, \tilde{f}_1) \neq 0$ . Take $f_1=\frac{\tilde{f}_1}{\Omega(e_1, \tilde{f}_1)}$ . Then $\Omega(f_1, e_1) = 1$ . Furthermore $\Omega(f_1,e_i)=0$ because $f_1 \in W^{\Omega}$ . Note that $V_1 := \text{span}(e_1, f_1) \subset W^{\Omega}$ and with some effort one can show that $V = V_1 \bigoplus V_1^{\Omega}$ . If $V_1^{\Omega} =\emptyset$ , we are ready. (2) $e_2 \in V_1^{\Omega}$ . Analogue to above because of the nondegeneracy of $\Omega$ and $e_2 \neq 0$ there exists an element $\tilde{f_2} \in V_1^{\Omega}$ with $\Omega(e_2, \tilde{f_2}) \neq 0$ . Take $f_2=\frac{\tilde{f}_2}{\Omega(e_1, \tilde{f}_2)}$ . Then $\Omega(e_2, f_2) = 1$ and $\Omega(e_1, f_2) = 0 = \Omega(f_1, f_2)$ . Again one can show that $V = (V_1 \bigoplus V_2) \bigoplus (V_1 \bigoplus V_2)^{\Omega}$ . If $(V_1 \bigoplus V_2)^{\Omega} =\emptyset$ , we are ready. (3) Now because L was Lagrangian and a subset of a finite vector space, this procedure ends. Thanks to all who have looked at this!","Good afternoon! I tried to understand the following fact about symplectic linear algebra. Given a Lagrangian subspace of a symplectic vector space , one can extend each basis of to a symplectic basis. I tried to do the proof by myself and by use of ""Ana Cannas da Silva-Lectures on Symplectic Geometry"" but I am still not sure whether it is ok. What do you think? The necessary condition that is fullfilled bacause is a Lagrangian, meaning that . Now we have to find elements ( Lagrangian, i.e. ) such that , and for all . Let be such a basis. (1) Define the set . Since is nondegenerate we can always find an element with . Take . Then . Furthermore because . Note that and with some effort one can show that . If , we are ready. (2) . Analogue to above because of the nondegeneracy of and there exists an element with . Take . Then and . Again one can show that . If , we are ready. (3) Now because L was Lagrangian and a subset of a finite vector space, this procedure ends. Thanks to all who have looked at this!","L (V,\Omega) L \Omega(e_i, e_j)=0 ~\forall i,j=1,...,n L L = L^{\Omega} = \{v \in V: \Omega(v,l)=0~ \forall l \in L\} n L \dim(V) = \frac{1}{2}\dim(L) f_1, ..., f_n \in L^{\Omega}=L \Omega(f_i, e_i) = 1 \Omega(f_i, e_j) = 0 \Omega(f_i, f_j)=0 i \neq j = 1,...,n \{e_1, ..., e_n\} W := span(e_2, e_3,...,e_n) \subset L \Omega \tilde{f}_1 \in W^{\Omega}=\{v \in V: \Omega(v,w)=0~ \forall w \in W\} \Omega(e_1, \tilde{f}_1) \neq 0 f_1=\frac{\tilde{f}_1}{\Omega(e_1, \tilde{f}_1)} \Omega(f_1, e_1) = 1 \Omega(f_1,e_i)=0 f_1 \in W^{\Omega} V_1 := \text{span}(e_1, f_1) \subset W^{\Omega} V = V_1 \bigoplus V_1^{\Omega} V_1^{\Omega} =\emptyset e_2 \in V_1^{\Omega} \Omega e_2 \neq 0 \tilde{f_2} \in V_1^{\Omega} \Omega(e_2, \tilde{f_2}) \neq 0 f_2=\frac{\tilde{f}_2}{\Omega(e_1, \tilde{f}_2)} \Omega(e_2, f_2) = 1 \Omega(e_1, f_2) = 0 = \Omega(f_1, f_2) V = (V_1 \bigoplus V_2) \bigoplus (V_1 \bigoplus V_2)^{\Omega} (V_1 \bigoplus V_2)^{\Omega} =\emptyset","['differential-geometry', 'symplectic-linear-algebra']"
86,A question regarding the vector bundle associated to the frame bundle,A question regarding the vector bundle associated to the frame bundle,,"Let $E\to M$ be a smooth real vector bundle of rank $n$ and $\mathcal{F}E\to M$ be the associated frame bundle. The frame bundle is a principal $GL(n,\mathbb{R})$-bundle and $GL(n,\mathbb{R})$ has a natural action on $\mathbb{R}^n$. Thus we can construct the associated vector bundle $\mathcal{F}E \times_{GL(n,\mathbb{R})} \mathbb{R}^n \to M$. This is again a vector bundle of rank $n$. A natural question is that : Is there a relation between the two bundles $\mathcal{F}E \times_{GL(n,\mathbb{R})} \mathbb{R}^n \to M$ and $E \to M$ ? I guess that they would be isomorphic, but I am unable to see if there is a natural isomorphism between them.","Let $E\to M$ be a smooth real vector bundle of rank $n$ and $\mathcal{F}E\to M$ be the associated frame bundle. The frame bundle is a principal $GL(n,\mathbb{R})$-bundle and $GL(n,\mathbb{R})$ has a natural action on $\mathbb{R}^n$. Thus we can construct the associated vector bundle $\mathcal{F}E \times_{GL(n,\mathbb{R})} \mathbb{R}^n \to M$. This is again a vector bundle of rank $n$. A natural question is that : Is there a relation between the two bundles $\mathcal{F}E \times_{GL(n,\mathbb{R})} \mathbb{R}^n \to M$ and $E \to M$ ? I guess that they would be isomorphic, but I am unable to see if there is a natural isomorphism between them.",,"['differential-geometry', 'manifolds', 'vector-bundles']"
87,Question about a lemma concerning vector fields in Do Carmo,Question about a lemma concerning vector fields in Do Carmo,,"In the book Differential Geometry of Curves and Surfaces by Do Carmo, he gives a proof of the following lemma: Let $w$ be a vector field in an open set $U \subset \mathbb{R}^{2}$ and let $p \in U$ be such that $w(p) \neq 0$. Then there exist $W \subset U$ of $p$ and a differentiable function $f: W \rightarrow \mathbb{R}$ such that $f$ is constant along each trajectory of $w$ and $df_{q} \neq 0$ for all $q \in W$ A trajectory is a curve in $U$ where the tangent vector at each point of the curve is valued of the vector field at that point. I think this is called an integral curve. The proof proceeds, by essentially assuming $p=(0,0)$. A previous theorem lets us assume that there exists an open set $V \subset U$, an interval $I$ and a differentiable function $\alpha: V \times I \rightarrow U$ where $\alpha(p,0)=p$ and $\frac{\partial{\alpha}}{\partial{t}}(p,t)=w(\alpha(p,t))$. We then let $\alpha'$ be the restriction of $\alpha$ to when $x=0$. The text then says that $d\alpha'_{(p,0)}(\text{unit vector in y direction})=\text{unit vector in y direction}$ I don't see how this follows from the definition of $\alpha$.","In the book Differential Geometry of Curves and Surfaces by Do Carmo, he gives a proof of the following lemma: Let $w$ be a vector field in an open set $U \subset \mathbb{R}^{2}$ and let $p \in U$ be such that $w(p) \neq 0$. Then there exist $W \subset U$ of $p$ and a differentiable function $f: W \rightarrow \mathbb{R}$ such that $f$ is constant along each trajectory of $w$ and $df_{q} \neq 0$ for all $q \in W$ A trajectory is a curve in $U$ where the tangent vector at each point of the curve is valued of the vector field at that point. I think this is called an integral curve. The proof proceeds, by essentially assuming $p=(0,0)$. A previous theorem lets us assume that there exists an open set $V \subset U$, an interval $I$ and a differentiable function $\alpha: V \times I \rightarrow U$ where $\alpha(p,0)=p$ and $\frac{\partial{\alpha}}{\partial{t}}(p,t)=w(\alpha(p,t))$. We then let $\alpha'$ be the restriction of $\alpha$ to when $x=0$. The text then says that $d\alpha'_{(p,0)}(\text{unit vector in y direction})=\text{unit vector in y direction}$ I don't see how this follows from the definition of $\alpha$.",,['differential-geometry']
88,Pseudosphere covered by upper half-plane,Pseudosphere covered by upper half-plane,,"Consider the (half) pseudosphere (with radius 1), which is the surface of revolution in $\mathbb{R}^3$ generated by the tractrix parametrized by (for $t \geqslant 0$) $$ t \mapsto (t -\operatorname{tanh} t, \operatorname{sech} t)~.$$ According to Wikipedia (see pseudosphere ), this surface is covered by the region $\{y \geqslant 1\}$ of the upper half-plane, with covering map given explicitely by: $$(x,y)\mapsto \big(v(\operatorname{arcosh} y)\cos x, v(\operatorname{arcosh} y) \sin x, u(\operatorname{arcosh} y)\big)$$ where $t \mapsto (u(t),v(t))$ is the arclength parametrization of the tractrix above. Ok. So in particular, we should have $$dX^2 + dY^2 + dZ^2 = \frac{dx^2 + dy^2}{y^2}~,$$ right? Here I have denoted of course $$X = v(\operatorname{arcosh} y)\cos x \qquad Y = v(\operatorname{arcosh} y) \sin x \qquad Z = u(\operatorname{arcosh} y)~.$$ But without even bothering to compute the arclength parametrization, isn't it the case that: $$dX^2 + dY^2 + dZ^2 = \big(v(\operatorname{arcosh} y)\big)^2 \, dx^2 \, + \, \frac{\big(u'(\operatorname{arcosh} y))^2 + \big(v'(\operatorname{arcosh} y)\big)^2}{y^2-1}dy^2$$ so in particular already the coefficient of $dy^2$, namely $\dfrac{1}{y^2-1}$, seems to be wrong. Am I going wrong somewhere, or is Wikipedia?","Consider the (half) pseudosphere (with radius 1), which is the surface of revolution in $\mathbb{R}^3$ generated by the tractrix parametrized by (for $t \geqslant 0$) $$ t \mapsto (t -\operatorname{tanh} t, \operatorname{sech} t)~.$$ According to Wikipedia (see pseudosphere ), this surface is covered by the region $\{y \geqslant 1\}$ of the upper half-plane, with covering map given explicitely by: $$(x,y)\mapsto \big(v(\operatorname{arcosh} y)\cos x, v(\operatorname{arcosh} y) \sin x, u(\operatorname{arcosh} y)\big)$$ where $t \mapsto (u(t),v(t))$ is the arclength parametrization of the tractrix above. Ok. So in particular, we should have $$dX^2 + dY^2 + dZ^2 = \frac{dx^2 + dy^2}{y^2}~,$$ right? Here I have denoted of course $$X = v(\operatorname{arcosh} y)\cos x \qquad Y = v(\operatorname{arcosh} y) \sin x \qquad Z = u(\operatorname{arcosh} y)~.$$ But without even bothering to compute the arclength parametrization, isn't it the case that: $$dX^2 + dY^2 + dZ^2 = \big(v(\operatorname{arcosh} y)\big)^2 \, dx^2 \, + \, \frac{\big(u'(\operatorname{arcosh} y))^2 + \big(v'(\operatorname{arcosh} y)\big)^2}{y^2-1}dy^2$$ so in particular already the coefficient of $dy^2$, namely $\dfrac{1}{y^2-1}$, seems to be wrong. Am I going wrong somewhere, or is Wikipedia?",,"['differential-geometry', 'riemannian-geometry', 'hyperbolic-geometry', 'covering-spaces', 'curvature']"
89,Definition of Frenet frame for curves in $\mathbb{R}^n$.,Definition of Frenet frame for curves in .,\mathbb{R}^n,"I have a question about the definition of Frenet frame here (pg 6) https://www.math.cuhk.edu.hk/~martinli/teaching/4030lectures.pdf . The definition above is summarized below: We are given a regular curve, $c$, in $\mathbb{R}^n$ parametrized by arc length, such that $c'(s),c''(s),\dots,c^{n-1}(s)$ are linearly independent for each $s$. A Frenet frame for $c$ is defined to be a positively oriented orthonormal basis $\{ e_1(s), \dots, e_n(s) \}$ such that $\text{Span}\{e_1(s),\dots,e_k(s)\} = \text{Span}\{c'(s),\dots,c^{k}(s)\}$ for $k=  1,\dots, \color{red} n$ and $\langle c^{k}(s) , e_k(s) \rangle > 0$. Condition 1. above does not look right to me because no assumptions about $c^{n}(s)$ are made. Should 1. be replaced by $\text{Span}\{e_1(s),\dots,e_k(s)\} =     \text{Span}\{c'(s),\dots,c^{k}(s)\}$ for $k=  1,\dots, \color{red} {n     - 1}?$ A similar assumption consistent with the document above is also made here http://www.cs.elte.hu/geometry/csikos/dif/dif2.pdf (pg 7. first definition on top of the page) so I wonder if I am missing something.","I have a question about the definition of Frenet frame here (pg 6) https://www.math.cuhk.edu.hk/~martinli/teaching/4030lectures.pdf . The definition above is summarized below: We are given a regular curve, $c$, in $\mathbb{R}^n$ parametrized by arc length, such that $c'(s),c''(s),\dots,c^{n-1}(s)$ are linearly independent for each $s$. A Frenet frame for $c$ is defined to be a positively oriented orthonormal basis $\{ e_1(s), \dots, e_n(s) \}$ such that $\text{Span}\{e_1(s),\dots,e_k(s)\} = \text{Span}\{c'(s),\dots,c^{k}(s)\}$ for $k=  1,\dots, \color{red} n$ and $\langle c^{k}(s) , e_k(s) \rangle > 0$. Condition 1. above does not look right to me because no assumptions about $c^{n}(s)$ are made. Should 1. be replaced by $\text{Span}\{e_1(s),\dots,e_k(s)\} =     \text{Span}\{c'(s),\dots,c^{k}(s)\}$ for $k=  1,\dots, \color{red} {n     - 1}?$ A similar assumption consistent with the document above is also made here http://www.cs.elte.hu/geometry/csikos/dif/dif2.pdf (pg 7. first definition on top of the page) so I wonder if I am missing something.",,['differential-geometry']
90,Why the chevalley $G/B$ and plucker $G/B$ are isomorphic $G$- projective varieties,Why the chevalley  and plucker  are isomorphic - projective varieties,G/B G/B G,"Let $G$ be an algebraic group acting transitively on closed projective varieties $X$ and $Y$.  Let $X \xrightarrow{f} Y$ be a bijective continuous map, that is $G-$equivariant.  Does it follow that $X \to Y$ is an isomorphism of algebraic varieties? It seems roughly intuitive because if the derivative of this map is zero somewhere then it should be zero everywhere by the equivariance. If there is a transversality proof that is easier for $G$-manifolds and diffeomorphisms, or if it easier to prove it for complex algebraic varieties and biholomorphisms, i'd be all for it. My motivation:  There are two structures as a projective variety, that one can put on $GL(n)/B$ where $B$ is the Borel subgroup(which I define as a maximal connnected solvable subgroup of $GL(n)$). Structure one:  One can use Chevalley's theorem to define what $G/B$ means as a quasiprojective variety:  its an orbit in $P(W)$ for some $G$ module $W$, which is therefore open in its closure.  Then one can show that its closed $P(W)$ because $B$ is solvable, and orbits of minimal dimension are closed.  Therefore $GL/B$ is a closed orbit in $P(W)$. Structure two:  There is a bijection of $G-$sets between $GL(n)/B$ and $Flag(\mathbb{C}^n)$.  There is a closed embedding of this into a product of grassmanians, and grassmanians are closed subvarieties of projective space by plucker.  So $GL(n)/B$ can be regarded as a closed subspace of $P(\wedge^1 \mathbb{C^n} \otimes ...\wedge^{n-1} \mathbb{C^n})$. I want to know 'Why are these $G$-varieties isomorphic as projective varieties?' and the question above would allow me to answer this. What I have got from professors:  A professor at tea told me that I need to use the correspondence between maps from projective space and line bundles over the domain. Brainstorming:  Maybe I need to use something about $G$ equivariant line bundles over $G/B$ since these will correspond to $G$-equivariant maps from $G/B$..  Borel-weil-bott theorem anyone?","Let $G$ be an algebraic group acting transitively on closed projective varieties $X$ and $Y$.  Let $X \xrightarrow{f} Y$ be a bijective continuous map, that is $G-$equivariant.  Does it follow that $X \to Y$ is an isomorphism of algebraic varieties? It seems roughly intuitive because if the derivative of this map is zero somewhere then it should be zero everywhere by the equivariance. If there is a transversality proof that is easier for $G$-manifolds and diffeomorphisms, or if it easier to prove it for complex algebraic varieties and biholomorphisms, i'd be all for it. My motivation:  There are two structures as a projective variety, that one can put on $GL(n)/B$ where $B$ is the Borel subgroup(which I define as a maximal connnected solvable subgroup of $GL(n)$). Structure one:  One can use Chevalley's theorem to define what $G/B$ means as a quasiprojective variety:  its an orbit in $P(W)$ for some $G$ module $W$, which is therefore open in its closure.  Then one can show that its closed $P(W)$ because $B$ is solvable, and orbits of minimal dimension are closed.  Therefore $GL/B$ is a closed orbit in $P(W)$. Structure two:  There is a bijection of $G-$sets between $GL(n)/B$ and $Flag(\mathbb{C}^n)$.  There is a closed embedding of this into a product of grassmanians, and grassmanians are closed subvarieties of projective space by plucker.  So $GL(n)/B$ can be regarded as a closed subspace of $P(\wedge^1 \mathbb{C^n} \otimes ...\wedge^{n-1} \mathbb{C^n})$. I want to know 'Why are these $G$-varieties isomorphic as projective varieties?' and the question above would allow me to answer this. What I have got from professors:  A professor at tea told me that I need to use the correspondence between maps from projective space and line bundles over the domain. Brainstorming:  Maybe I need to use something about $G$ equivariant line bundles over $G/B$ since these will correspond to $G$-equivariant maps from $G/B$..  Borel-weil-bott theorem anyone?",,"['differential-geometry', 'algebraic-geometry', 'reference-request', 'lie-groups', 'algebraic-groups']"
91,The Concept of Isometry under Riemannian Metric's Context,The Concept of Isometry under Riemannian Metric's Context,,"While reading the book Modern Geometry — Methods and Applications Part I , I have a problem reconciling the definition of isometries with the usual version (an isometry preserves the distance between two points). Quoted from the book, an isometry (or a motion of the given metric ) is a transformation preserving the Riemannian metric (Def. 4.1.1., page 24). That is, $x^i = x^i(z^1, \dots, z^n)$ is called an isometry if $g'_{ij}(z^1, \dots, z^n) = g_{ij}(x^1(z), \dots, x^n(z))$, where the $g_{ij}$ and $g'_{ij}$ are Riemannian metrics of corresponding coordinates systems. My problem is, how is the concept of distance connected with that of $g_{ij}$? Assuming that the distance between $x_1 = (x_1^1, \dots, x_1^n)$ and $x_2 = (x_2^1, \dots, x_2^n)$ is defined as $d(x_1, x_2) = \sqrt{\left< \xi, \xi \right>} = \sqrt{\sum_{i, j} g_{ij}(x_1) \xi^i \xi^j}$, where $\xi$ is the vector originating at $x_1$ and terminating at $x_2$. Let $z_1, z_2$ be points corresponding to $x_1, x_2$ under an arbitrarily chosen transformation, respectively, and let $\eta = \vec{z_1z_2}$. Then, by the definitions of vector and inner product in this book, $d(z_1, z_2) = \sqrt{\left< \eta, \eta \right>} = \sqrt{g'_{ij}(z_1) \eta^i \eta^j} = \sqrt{ \left[\frac{\partial x^k}{\partial z^i} g_{kl}\frac{\partial x^l}{\partial z^j}\right]_{z = z_1} \eta^i \eta^j} = \sqrt{g_{ij}(x_1) \xi^i \xi^j} = d(x_1, x_2)$. This suggests that every transformation is an isometry, which is obviously absurd. Thus, either my assumed definition of distance is false or I have misunderstood the definitions of vectors and inner products. Can anyone point out my fallacy? *This question is coming out when I am tackling with an exercise stating that every isometry of Euclidean n-space is affine , for which I have known a proof like in page 7~8 of this document . Unfortunately, I am unable to ""translate"" it into the context of Riemannian metric due to my shortage in concepts of distance.","While reading the book Modern Geometry — Methods and Applications Part I , I have a problem reconciling the definition of isometries with the usual version (an isometry preserves the distance between two points). Quoted from the book, an isometry (or a motion of the given metric ) is a transformation preserving the Riemannian metric (Def. 4.1.1., page 24). That is, $x^i = x^i(z^1, \dots, z^n)$ is called an isometry if $g'_{ij}(z^1, \dots, z^n) = g_{ij}(x^1(z), \dots, x^n(z))$, where the $g_{ij}$ and $g'_{ij}$ are Riemannian metrics of corresponding coordinates systems. My problem is, how is the concept of distance connected with that of $g_{ij}$? Assuming that the distance between $x_1 = (x_1^1, \dots, x_1^n)$ and $x_2 = (x_2^1, \dots, x_2^n)$ is defined as $d(x_1, x_2) = \sqrt{\left< \xi, \xi \right>} = \sqrt{\sum_{i, j} g_{ij}(x_1) \xi^i \xi^j}$, where $\xi$ is the vector originating at $x_1$ and terminating at $x_2$. Let $z_1, z_2$ be points corresponding to $x_1, x_2$ under an arbitrarily chosen transformation, respectively, and let $\eta = \vec{z_1z_2}$. Then, by the definitions of vector and inner product in this book, $d(z_1, z_2) = \sqrt{\left< \eta, \eta \right>} = \sqrt{g'_{ij}(z_1) \eta^i \eta^j} = \sqrt{ \left[\frac{\partial x^k}{\partial z^i} g_{kl}\frac{\partial x^l}{\partial z^j}\right]_{z = z_1} \eta^i \eta^j} = \sqrt{g_{ij}(x_1) \xi^i \xi^j} = d(x_1, x_2)$. This suggests that every transformation is an isometry, which is obviously absurd. Thus, either my assumed definition of distance is false or I have misunderstood the definitions of vectors and inner products. Can anyone point out my fallacy? *This question is coming out when I am tackling with an exercise stating that every isometry of Euclidean n-space is affine , for which I have known a proof like in page 7~8 of this document . Unfortunately, I am unable to ""translate"" it into the context of Riemannian metric due to my shortage in concepts of distance.",,['differential-geometry']
92,Parabolic Maximum Principle on Geometric Flows,Parabolic Maximum Principle on Geometric Flows,,"I am trying to understand the main aspects of geometric flows. The parabolic maximum principle (PMP) plays an important role to bound geometric quantities, however, I haven't found a reference in which they show (for beginners) how to apply it and they only mention that it is an application of the PMP. To be precise, in Huisken and Ilmanen's paper ""The inverse mean curvature flow and the Riemannian Penrose inequality"" ( https://projecteuclid.org/euclid.jdg/1090349447 ) they show that the evolution of the mean curvature is given by (equation 1.3): $$\dfrac{\partial}{\partial t}H=\dfrac{1}{H^2}\Delta H-2\dfrac{|D H|^2}{H^3} -\dfrac{1}{H}\text{Ric}(\nu,\nu)-\dfrac{1}{H}|A|^2$$ and they mention the following: ""This equation is cause for optimism, because in view of the fact that $|A|^2 \geq \dfrac{H^2}{n−1}$, the parabolic maximum principle yields the curvature bound $$\max\limits_{N_t} H^2 \leq \max_{N_0} H^2+C.$$ as long as the Ricci curvature is bounded below and the flow remains smooth."" It seems that the reason could be that from the evolution of H, $$\dfrac{\partial}{\partial t}H \leq \dfrac{1}{H^2}\Delta H+\frac{|c|}{H}, $$ which calls for an application of the PMC, however from the usual statements of the PMC I have seen, I am not sure how to deal with the $1/H^2$. I also tried writing the evolution of $H^2$, but I got similar issues. To add up, the constant $C$ also puzzles me, since all the conclusions or uses I have seem of the PMP, point out to a bound of the type $\max\limits_{N_0} H^2$ (no constant).","I am trying to understand the main aspects of geometric flows. The parabolic maximum principle (PMP) plays an important role to bound geometric quantities, however, I haven't found a reference in which they show (for beginners) how to apply it and they only mention that it is an application of the PMP. To be precise, in Huisken and Ilmanen's paper ""The inverse mean curvature flow and the Riemannian Penrose inequality"" ( https://projecteuclid.org/euclid.jdg/1090349447 ) they show that the evolution of the mean curvature is given by (equation 1.3): $$\dfrac{\partial}{\partial t}H=\dfrac{1}{H^2}\Delta H-2\dfrac{|D H|^2}{H^3} -\dfrac{1}{H}\text{Ric}(\nu,\nu)-\dfrac{1}{H}|A|^2$$ and they mention the following: ""This equation is cause for optimism, because in view of the fact that $|A|^2 \geq \dfrac{H^2}{n−1}$, the parabolic maximum principle yields the curvature bound $$\max\limits_{N_t} H^2 \leq \max_{N_0} H^2+C.$$ as long as the Ricci curvature is bounded below and the flow remains smooth."" It seems that the reason could be that from the evolution of H, $$\dfrac{\partial}{\partial t}H \leq \dfrac{1}{H^2}\Delta H+\frac{|c|}{H}, $$ which calls for an application of the PMC, however from the usual statements of the PMC I have seen, I am not sure how to deal with the $1/H^2$. I also tried writing the evolution of $H^2$, but I got similar issues. To add up, the constant $C$ also puzzles me, since all the conclusions or uses I have seem of the PMP, point out to a bound of the type $\max\limits_{N_0} H^2$ (no constant).",,"['differential-geometry', 'maximum-principle', 'mean-curvature-flows']"
93,"Gradient of a function on product $(U\times R, g_M \oplus ds^2)$",Gradient of a function on product,"(U\times R, g_M \oplus ds^2)","Consider $U$ an open subset of $\mathbb{R}^n$. It is clear that $U$ is a smooth submanifold of $\mathbb{R}^n$. Now, consider that $U$ is equipped with a Riemannian smooth metric $g$. The pair $(U,g)$ is a Riemannian manifold. Define a smooth function $f : U \times \mathbb{R} \to \mathbb{R}$. Consider that $\mathbb{R}$ is seen as a Riemannian manifold too, equipped with the canonical metric. The product $U \times \mathbb{R}$ can be equipped with the product metric $\tilde{g}$ defined as follows : Let $(p,t) \in U \times \mathbb{R}$. Then we have the following identification : $$ \mathrm{T}_{(p,t)}\big( U \times \mathbb{R} \big) \simeq \mathrm{T}_pU \oplus \mathrm{T}_{t}\mathbb{R} \simeq \mathrm{T}_pU \oplus \mathbb{R}$$ since $\mathrm{T}_{t}\mathbb{R} \simeq \mathbb{R}$. For all $(p,t) \in U \times \mathbb{R}$, for all $(u_{1}+v_{1},u_{2}+v_{2}) \in \mathrm{T}_{(p,t)}\big( U \times \mathbb{R} \big)$,  $$ \tilde{g}_{(p,t)}(u_{1}+v_{1},u_{2}+v_{2}) = g_{p}(u_{1},u_{2}) + v_{1}v_{2} $$ My question is : what is the gradient of $f$ (at a point $(p,t)$) ? Since $g$ is a smooth Riemannian metric on $M$, it is necessarily of the form : $\forall p \in U, \, \forall (u,v) \in \mathrm{T}_{p}U, \, g_{p}(u,v) = u^{\top}G(p)v$, with $G$ a smooth function such that $G(p)$ is a $n \times n$ symmetric positive definite matrix for all $p$. For $(p,t) \in U \times \mathbb{R}$, is the gradient $\nabla_{(p,t)}f$ of $f$ of the following form ?  $$ \nabla_{(p,t)}f = \begin{bmatrix} G(p)^{-1}\mathbf{A} \\ a \end{bmatrix} $$ with : $$ \mathbf{A} = \begin{bmatrix} \frac{\partial f}{\partial x_{1}}(p,t) \\ \vdots \\ \frac{\partial f}{\partial x_{n}}(p,t) \end{bmatrix} \quad \text{and} \quad a = \frac{\partial f}{\partial x_{n+1}}(p,t). $$","Consider $U$ an open subset of $\mathbb{R}^n$. It is clear that $U$ is a smooth submanifold of $\mathbb{R}^n$. Now, consider that $U$ is equipped with a Riemannian smooth metric $g$. The pair $(U,g)$ is a Riemannian manifold. Define a smooth function $f : U \times \mathbb{R} \to \mathbb{R}$. Consider that $\mathbb{R}$ is seen as a Riemannian manifold too, equipped with the canonical metric. The product $U \times \mathbb{R}$ can be equipped with the product metric $\tilde{g}$ defined as follows : Let $(p,t) \in U \times \mathbb{R}$. Then we have the following identification : $$ \mathrm{T}_{(p,t)}\big( U \times \mathbb{R} \big) \simeq \mathrm{T}_pU \oplus \mathrm{T}_{t}\mathbb{R} \simeq \mathrm{T}_pU \oplus \mathbb{R}$$ since $\mathrm{T}_{t}\mathbb{R} \simeq \mathbb{R}$. For all $(p,t) \in U \times \mathbb{R}$, for all $(u_{1}+v_{1},u_{2}+v_{2}) \in \mathrm{T}_{(p,t)}\big( U \times \mathbb{R} \big)$,  $$ \tilde{g}_{(p,t)}(u_{1}+v_{1},u_{2}+v_{2}) = g_{p}(u_{1},u_{2}) + v_{1}v_{2} $$ My question is : what is the gradient of $f$ (at a point $(p,t)$) ? Since $g$ is a smooth Riemannian metric on $M$, it is necessarily of the form : $\forall p \in U, \, \forall (u,v) \in \mathrm{T}_{p}U, \, g_{p}(u,v) = u^{\top}G(p)v$, with $G$ a smooth function such that $G(p)$ is a $n \times n$ symmetric positive definite matrix for all $p$. For $(p,t) \in U \times \mathbb{R}$, is the gradient $\nabla_{(p,t)}f$ of $f$ of the following form ?  $$ \nabla_{(p,t)}f = \begin{bmatrix} G(p)^{-1}\mathbf{A} \\ a \end{bmatrix} $$ with : $$ \mathbf{A} = \begin{bmatrix} \frac{\partial f}{\partial x_{1}}(p,t) \\ \vdots \\ \frac{\partial f}{\partial x_{n}}(p,t) \end{bmatrix} \quad \text{and} \quad a = \frac{\partial f}{\partial x_{n+1}}(p,t). $$",,"['differential-geometry', 'riemannian-geometry', 'smooth-manifolds']"
94,Degree of map $f \colon M \to N$ from compact manifold to noncompact manifold is $0$,Degree of map  from compact manifold to noncompact manifold is,f \colon M \to N 0,"The following is adapted from my professor's summary notes on Differential Geometry: Let $M$ and $N$ be two oriented connected manifolds of the same dimension $m$ , and let $f \colon M \to N$ be a proper map.  Let $I \colon H_{\mathrm{c}}^m(N) \to \mathbf{R}$ be the isomorphism given by integration from the $m^{\mathrm{th}}$ -degree compactly supported de Rahm cohomology space of $N$ to the real numbers, and similarly consider $I \colon H_{\mathrm{c}}^m(M) \to \mathbf{R}$ , as well as the map on cohomology $H_\mathrm{c}^m(f) \colon H_\mathrm{c}^m(N) \to H_\mathrm{c}^m(M)$ induced by $f$ . Then the linear map $I \circ H_\mathrm{c}^m(f) \circ I^{-1} \colon \mathbf{R} \to \mathbf{R}$ is multiplication by a real number $\mathrm{deg}(f)$ which we call the cohomological degree of the proper map $f \colon M \to N$ . .... He then continues to define the geometric degree $\mathrm{deg}_y(f)$ of $f$ at a regular value $y \in N$ in the usual way as the sum of local intersection numbers.  He then states: Theorem. The geometric degree $\mathrm{deg}_y(f)$ of $f \colon M \to N$ at each regular value $y \in N$ is equal to the cohomological degree $\mathrm{deg}(f)$ . ....[some corollaries] Corollary. If $M$ is compact and $N$ is non-compact, the degree $\mathrm{deg}(f)$ is $0$ . Again, these notes have statements only, no proofs. Could someone help me understand why the corollary is true?  I really have no idea.  Thanks.","The following is adapted from my professor's summary notes on Differential Geometry: Let and be two oriented connected manifolds of the same dimension , and let be a proper map.  Let be the isomorphism given by integration from the -degree compactly supported de Rahm cohomology space of to the real numbers, and similarly consider , as well as the map on cohomology induced by . Then the linear map is multiplication by a real number which we call the cohomological degree of the proper map . .... He then continues to define the geometric degree of at a regular value in the usual way as the sum of local intersection numbers.  He then states: Theorem. The geometric degree of at each regular value is equal to the cohomological degree . ....[some corollaries] Corollary. If is compact and is non-compact, the degree is . Again, these notes have statements only, no proofs. Could someone help me understand why the corollary is true?  I really have no idea.  Thanks.",M N m f \colon M \to N I \colon H_{\mathrm{c}}^m(N) \to \mathbf{R} m^{\mathrm{th}} N I \colon H_{\mathrm{c}}^m(M) \to \mathbf{R} H_\mathrm{c}^m(f) \colon H_\mathrm{c}^m(N) \to H_\mathrm{c}^m(M) f I \circ H_\mathrm{c}^m(f) \circ I^{-1} \colon \mathbf{R} \to \mathbf{R} \mathrm{deg}(f) f \colon M \to N \mathrm{deg}_y(f) f y \in N \mathrm{deg}_y(f) f \colon M \to N y \in N \mathrm{deg}(f) M N \mathrm{deg}(f) 0,"['integration', 'differential-geometry', 'differential-topology', 'smooth-manifolds']"
95,"""Approximate Isometry"" in Riemannian Geometry","""Approximate Isometry"" in Riemannian Geometry",,"I apologize if the notion I'm asking about is well known, I'm no expert in geometry (and I did not find an answer via google). Suppose $(X,g_X)$ and $(Y,g_Y)$ are (smooth) Riemannian manifolds. I'm wondering if one may describe a diffeomorphism $f:X \to Y$ as being ""close to an isometry."" Is there a criteria to measure this, and what would one call such a map? A natural idea is to call $f$ an $\epsilon$-isometry if for any $p\in X$, $u,v \in T_pX$, $$g^X_p(u,v) - g^Y_{f(p)}(dfu,dfv) < \epsilon$$ (This idea is analogous to that of an approximate isometry on Banach spaces). Does this concept exist and if so in what context is it useful? Thanks.","I apologize if the notion I'm asking about is well known, I'm no expert in geometry (and I did not find an answer via google). Suppose $(X,g_X)$ and $(Y,g_Y)$ are (smooth) Riemannian manifolds. I'm wondering if one may describe a diffeomorphism $f:X \to Y$ as being ""close to an isometry."" Is there a criteria to measure this, and what would one call such a map? A natural idea is to call $f$ an $\epsilon$-isometry if for any $p\in X$, $u,v \in T_pX$, $$g^X_p(u,v) - g^Y_{f(p)}(dfu,dfv) < \epsilon$$ (This idea is analogous to that of an approximate isometry on Banach spaces). Does this concept exist and if so in what context is it useful? Thanks.",,"['differential-geometry', 'riemannian-geometry', 'smooth-manifolds', 'conformal-geometry', 'isometry']"
96,Tangent vector of a curve,Tangent vector of a curve,,"Let $\vec{\sigma}:[a,b]\longrightarrow \mathbb{R}^2$ be a regular and closed curve of class $C^1$, parametrized respect to the arc lenght. Is true that the map $\vec{\sigma}':[a,b]\longrightarrow S^1$ is surjective?","Let $\vec{\sigma}:[a,b]\longrightarrow \mathbb{R}^2$ be a regular and closed curve of class $C^1$, parametrized respect to the arc lenght. Is true that the map $\vec{\sigma}':[a,b]\longrightarrow S^1$ is surjective?",,"['differential-geometry', 'plane-curves']"
97,Why must a function be independent of coordinates?,Why must a function be independent of coordinates?,,"What is the motivation for why a function should be independent of coordinates? In the case of a general manifold I kind of get why, since one (usually) defines a function $f$ as a map from the manifold to the reals, i.e. $f:M\rightarrow\mathbb{R}$, and so in this sense it is manifestly coordinate independent (since $f$ has been defined without introducing any particular coordinate system). However, is there a reason in general (both heuristically and technically) why a function should be coordinate independent? In physics, the standard argument seems to be that a function is a scalar and so has no directional dependence, thus it should be invariant under rotations of coordinate systems. However, I'm unsure how this extends in generality (for example, why should it be true for a coordinate translation. Is it simply that coordinates are an artefact of the observer and so the value of the scalar function should not depend on the coordinates chosen, much like a vector is coordinate independent and this requires that its individual components should transform under coordinate transformations)? I'm fairly new to the concept of differential geometry so I apologise for such a basic question, but hopefully someone can help me out.","What is the motivation for why a function should be independent of coordinates? In the case of a general manifold I kind of get why, since one (usually) defines a function $f$ as a map from the manifold to the reals, i.e. $f:M\rightarrow\mathbb{R}$, and so in this sense it is manifestly coordinate independent (since $f$ has been defined without introducing any particular coordinate system). However, is there a reason in general (both heuristically and technically) why a function should be coordinate independent? In physics, the standard argument seems to be that a function is a scalar and so has no directional dependence, thus it should be invariant under rotations of coordinate systems. However, I'm unsure how this extends in generality (for example, why should it be true for a coordinate translation. Is it simply that coordinates are an artefact of the observer and so the value of the scalar function should not depend on the coordinates chosen, much like a vector is coordinate independent and this requires that its individual components should transform under coordinate transformations)? I'm fairly new to the concept of differential geometry so I apologise for such a basic question, but hopefully someone can help me out.",,"['linear-algebra', 'differential-geometry', 'coordinate-systems']"
98,Do I have the right idea about affine connections?,Do I have the right idea about affine connections?,,"On a smooth manifold $M$, a vector field is a smooth map $X : M \to TM$, where $TM$ is the tangent bundle of $M$. If $\chi(M)$ denotes the space of vector fields on $M$, an affine connection $\nabla$ on $M$ is a mapping $\nabla : \chi(M) \times \chi(M) \to \chi(M)$ which (intuitively) creates a new vector field $\nabla_{X}Y$ given two vector fields $X$,$Y$. If I am correct, given $p \in M$, $\nabla_{X}Y(p)$ can be understood as the derivative of $X$ in the direction of $Y(p)$. 1) Consider that $M \subset \mathbb{R}^n$ to make things ""easier. Now, a vector field on $M$ is a smooth map $X : M \to \mathbb{R}^n$ (such that, for all $p$, $X(p) \in T_{p}M$). Given another vector field $Y$ and $p \in M$, the derivative of $X$ in the direction of $Y(p)$ could be (naively) $D_{p}X\big( Y(p) \big)$, where $D_{p}X : T_{p}M \to \mathbb{R}^n$ denotes the differential of $X$ at $p$. But this might not define an affine connection since $D_{p}X\big( Y(p) \big)$ is not necessarily a tangent vector, right ? 2) If $\gamma : I \subset \mathbb{R} \to M$ is a smooth curve and $X$ a vector field along $\gamma$, the covariant derivative $\frac{DX}{dt}$ of $X$ (namely $\nabla_{\frac{d\gamma}{dt}}X$) is defined to be the orthogonal projection on the linear space $T_{\gamma(t)}M$. However, this definition holds only because we implicitly assume that $M$ is equipped with the metric induced by $\mathbb{R}^n$, right ? In other words, if I'm correct and if $g$ is a Riemannian metric (which is not the metric induced by $\mathbb{R}^n$) on $M$, this definition of covariant derivative no longer makes sense.","On a smooth manifold $M$, a vector field is a smooth map $X : M \to TM$, where $TM$ is the tangent bundle of $M$. If $\chi(M)$ denotes the space of vector fields on $M$, an affine connection $\nabla$ on $M$ is a mapping $\nabla : \chi(M) \times \chi(M) \to \chi(M)$ which (intuitively) creates a new vector field $\nabla_{X}Y$ given two vector fields $X$,$Y$. If I am correct, given $p \in M$, $\nabla_{X}Y(p)$ can be understood as the derivative of $X$ in the direction of $Y(p)$. 1) Consider that $M \subset \mathbb{R}^n$ to make things ""easier. Now, a vector field on $M$ is a smooth map $X : M \to \mathbb{R}^n$ (such that, for all $p$, $X(p) \in T_{p}M$). Given another vector field $Y$ and $p \in M$, the derivative of $X$ in the direction of $Y(p)$ could be (naively) $D_{p}X\big( Y(p) \big)$, where $D_{p}X : T_{p}M \to \mathbb{R}^n$ denotes the differential of $X$ at $p$. But this might not define an affine connection since $D_{p}X\big( Y(p) \big)$ is not necessarily a tangent vector, right ? 2) If $\gamma : I \subset \mathbb{R} \to M$ is a smooth curve and $X$ a vector field along $\gamma$, the covariant derivative $\frac{DX}{dt}$ of $X$ (namely $\nabla_{\frac{d\gamma}{dt}}X$) is defined to be the orthogonal projection on the linear space $T_{\gamma(t)}M$. However, this definition holds only because we implicitly assume that $M$ is equipped with the metric induced by $\mathbb{R}^n$, right ? In other words, if I'm correct and if $g$ is a Riemannian metric (which is not the metric induced by $\mathbb{R}^n$) on $M$, this definition of covariant derivative no longer makes sense.",,"['differential-geometry', 'manifolds', 'riemannian-geometry']"
99,Find surface in $\mathbb{R}^3$ with certain tangent spaces,Find surface in  with certain tangent spaces,\mathbb{R}^3,"By Frobenius Theorem, in $\mathbb{R}^3$ there exists a smooth surface whose tangent space is spanned by the vector fields $V(x,y,z)=(x^2+y^2,0,-y)$ and $W(x,y,z)=(0,x^2+y^2,x)$.  How can I find this surface? Is there in general a way to find it when the vector fields are algebraic?","By Frobenius Theorem, in $\mathbb{R}^3$ there exists a smooth surface whose tangent space is spanned by the vector fields $V(x,y,z)=(x^2+y^2,0,-y)$ and $W(x,y,z)=(0,x^2+y^2,x)$.  How can I find this surface? Is there in general a way to find it when the vector fields are algebraic?",,['differential-geometry']
