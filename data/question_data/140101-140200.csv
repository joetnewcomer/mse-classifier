,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Solving this second order differential equation $m\frac{d^2x}{dt^2}+c\frac{dx}{dt}+k\sin{x}=0$,Solving this second order differential equation,m\frac{d^2x}{dt^2}+c\frac{dx}{dt}+k\sin{x}=0,I am investigating something in Physics and found out that I will have to solve this equation: $$m\frac{d^2x}{dt^2}+c\frac{dx}{dt}+k\sin{x}=0$$ I haven't even learned how to solve ones that involve $kx$ and so I am very troubled by the $k\sin{x}$. Is this possible to solve? If so how could it be solved for $x$? Edit: I can't assume that $\sin{x}\approx x$ since I am investigating cases in which such an assumption is not applicable (at large angles of $x$),I am investigating something in Physics and found out that I will have to solve this equation: $$m\frac{d^2x}{dt^2}+c\frac{dx}{dt}+k\sin{x}=0$$ I haven't even learned how to solve ones that involve $kx$ and so I am very troubled by the $k\sin{x}$. Is this possible to solve? If so how could it be solved for $x$? Edit: I can't assume that $\sin{x}\approx x$ since I am investigating cases in which such an assumption is not applicable (at large angles of $x$),,"['ordinary-differential-equations', 'trigonometry']"
1,Non linear to linear differential equation,Non linear to linear differential equation,,"Is there exist a non linear differential equation $$ y'(t) = f(t,y(t)) $$ such that a change of variable $z=\varphi(t,y)$ leads to a linear differential equation ?","Is there exist a non linear differential equation $$ y'(t) = f(t,y(t)) $$ such that a change of variable $z=\varphi(t,y)$ leads to a linear differential equation ?",,"['ordinary-differential-equations', 'vector-fields', 'change-of-variable']"
2,"Show that $\det(P(t))=\det(P_0) e^{\int_0^t \operatorname{Tr} A(s)\, ds}$",Show that,"\det(P(t))=\det(P_0) e^{\int_0^t \operatorname{Tr} A(s)\, ds}","Let $A(t)$ be a continuous family of $n \times n$ matrices and let $P(t)$ be the matrix solution to the initial value problem $P'(t)=A(t)P$, where $P(0)=P_0$. Show that $$\det(P(t))=\det(P_0) e^{\int_0^t \operatorname{Tr} A(s)\, ds}.$$ If we consider $A(t)=     \begin{pmatrix}     a & b  \\     c & d  \\        \end{pmatrix}$, $P(t)=     \begin{pmatrix}     x_1 & y_1  \\     x_2 & y_2  \\        \end{pmatrix}$, and $W(t)=\det (P(t))$ then $W'=\operatorname {Tr}(A) W$. My question is how do I solve the last system? And I just consider  $2\times 2$ matrix, but the result follows easily for an $n\times n$ matrix; am I right?","Let $A(t)$ be a continuous family of $n \times n$ matrices and let $P(t)$ be the matrix solution to the initial value problem $P'(t)=A(t)P$, where $P(0)=P_0$. Show that $$\det(P(t))=\det(P_0) e^{\int_0^t \operatorname{Tr} A(s)\, ds}.$$ If we consider $A(t)=     \begin{pmatrix}     a & b  \\     c & d  \\        \end{pmatrix}$, $P(t)=     \begin{pmatrix}     x_1 & y_1  \\     x_2 & y_2  \\        \end{pmatrix}$, and $W(t)=\det (P(t))$ then $W'=\operatorname {Tr}(A) W$. My question is how do I solve the last system? And I just consider  $2\times 2$ matrix, but the result follows easily for an $n\times n$ matrix; am I right?",,"['ordinary-differential-equations', 'derivatives']"
3,Fundamental matrix solution and Floquet theory,Fundamental matrix solution and Floquet theory,,"Let the following matrix problem be given $$y'=M(t)y$$ where $y\in\mathbb{R}^n$ and $M(t+a)=M(t)$ for some $a>0$. So $M$ is periodic. A fundamental matrix solution can be written as $X(t)=f(t)e^{At}$, where $f(t)$ is a $a$-periodic $n\times n$-matrix and $A$ a constant $n\times n$-matrix. For $n=1$ and $n=2$: 1. How do I find $A$? 2. What must hold for $M(t)$ such that all solutions remain bounded when $t\rightarrow \pm \infty$? 3.  What must hold for $M(t)$ such that all solutions are $a$-periodic? Here $M(t)=m(t)\begin{pmatrix}a&b\\c&d\end{pmatrix}$ for $n=2$ with $m(t)$ again $a$-periodic and $a,b,c,d$ constants. What I thought: For $n=1$ you want to find  $$X'=M(t)X$$ where $X(t)=f(t)e^{At}$. So this gives us $$f'(t)+Af(t)=M(t)f(t)$$ where $A$ is just a real number. How do I find it? And what must I do for the conditions in 2. and 3.? For $n=2$ I would appreciate any hints.","Let the following matrix problem be given $$y'=M(t)y$$ where $y\in\mathbb{R}^n$ and $M(t+a)=M(t)$ for some $a>0$. So $M$ is periodic. A fundamental matrix solution can be written as $X(t)=f(t)e^{At}$, where $f(t)$ is a $a$-periodic $n\times n$-matrix and $A$ a constant $n\times n$-matrix. For $n=1$ and $n=2$: 1. How do I find $A$? 2. What must hold for $M(t)$ such that all solutions remain bounded when $t\rightarrow \pm \infty$? 3.  What must hold for $M(t)$ such that all solutions are $a$-periodic? Here $M(t)=m(t)\begin{pmatrix}a&b\\c&d\end{pmatrix}$ for $n=2$ with $m(t)$ again $a$-periodic and $a,b,c,d$ constants. What I thought: For $n=1$ you want to find  $$X'=M(t)X$$ where $X(t)=f(t)e^{At}$. So this gives us $$f'(t)+Af(t)=M(t)f(t)$$ where $A$ is just a real number. How do I find it? And what must I do for the conditions in 2. and 3.? For $n=2$ I would appreciate any hints.",,"['ordinary-differential-equations', 'dynamical-systems', 'matrix-equations']"
4,Show that a real solution goes to infinity as t goes to infinty,Show that a real solution goes to infinity as t goes to infinty,,"Let $\phi(t)$ be a real, continuous, pi-periodic function. Show that there exists a real solution goes to infinity as t goes to infinity of: $x''-(cos^2t)x'+\phi(t)x=0$ I am having a hard time finding the solutions to this ode. I tried wolfram alpha with no luck. Am I missing a way to solve this by hand? Thanks.","Let $\phi(t)$ be a real, continuous, pi-periodic function. Show that there exists a real solution goes to infinity as t goes to infinity of: $x''-(cos^2t)x'+\phi(t)x=0$ I am having a hard time finding the solutions to this ode. I tried wolfram alpha with no luck. Am I missing a way to solve this by hand? Thanks.",,['ordinary-differential-equations']
5,Solving simple mass-spring ODE system,Solving simple mass-spring ODE system,,"Been a long time since I've solved ODE's analytically, and having trouble solving this simple system: \begin{align} \frac{dx}{dt} &= v \\ \frac{dv}{dt} &= -\frac{k}{m}\frac{\ln(x)}{x} \\ x(0) &= x_0, \ v(0) = v_0 \end{align} My approach: \begin{align} \frac{dv}{dt} &= \frac{dv}{dx}\frac{dx}{dt} \quad \quad \text{chain rule} \\ \implies \frac{dv}{dt} &= v\frac{dv}{dx} \quad \quad \text{definition of dx/dt} \\ \implies v\frac{dv}{dx} &= -\frac{k}{m}\frac{\ln(x)}{x} \quad \quad \text{definition of dv/dt} \\ \implies v dv &= -\frac{k}{m} \frac{\ln(x)}{x}dx \quad \quad \text{multiply each side by dx} \\ \implies \int vdv &= -\frac{k}{m} \int \frac{\ln(x)}{x} dx \quad \quad \text{integrate each side} \\ \implies \frac{v^2}{2} &= -\frac{k}{m} \frac{\ln^2(x)}{2} + C \quad \quad \text{result of integral} \\ \implies v &= \pm \sqrt{-\frac{k}{m} \ln^2(x)} + C \quad \quad \text{multiply each side by 2, take square root of each side} \end{align} The result clearly isn't right as the velocity is complex for all (real) displacements. Also checked by substituting in my numerical solution for $x$ and comparing the $v$ determined above to the numerical solution of $v$. Of course they are quite different. Anyways, was wondering if anyone could point out where I've gone wrong or give a hint as to a better way to determine $x$ and $v$. Thank you!","Been a long time since I've solved ODE's analytically, and having trouble solving this simple system: \begin{align} \frac{dx}{dt} &= v \\ \frac{dv}{dt} &= -\frac{k}{m}\frac{\ln(x)}{x} \\ x(0) &= x_0, \ v(0) = v_0 \end{align} My approach: \begin{align} \frac{dv}{dt} &= \frac{dv}{dx}\frac{dx}{dt} \quad \quad \text{chain rule} \\ \implies \frac{dv}{dt} &= v\frac{dv}{dx} \quad \quad \text{definition of dx/dt} \\ \implies v\frac{dv}{dx} &= -\frac{k}{m}\frac{\ln(x)}{x} \quad \quad \text{definition of dv/dt} \\ \implies v dv &= -\frac{k}{m} \frac{\ln(x)}{x}dx \quad \quad \text{multiply each side by dx} \\ \implies \int vdv &= -\frac{k}{m} \int \frac{\ln(x)}{x} dx \quad \quad \text{integrate each side} \\ \implies \frac{v^2}{2} &= -\frac{k}{m} \frac{\ln^2(x)}{2} + C \quad \quad \text{result of integral} \\ \implies v &= \pm \sqrt{-\frac{k}{m} \ln^2(x)} + C \quad \quad \text{multiply each side by 2, take square root of each side} \end{align} The result clearly isn't right as the velocity is complex for all (real) displacements. Also checked by substituting in my numerical solution for $x$ and comparing the $v$ determined above to the numerical solution of $v$. Of course they are quite different. Anyways, was wondering if anyone could point out where I've gone wrong or give a hint as to a better way to determine $x$ and $v$. Thank you!",,"['integration', 'ordinary-differential-equations', 'partial-differential-equations', 'logarithms']"
6,How to simplify solutions for $y'' + 4y = 2 \tan x$?,How to simplify solutions for ?,y'' + 4y = 2 \tan x,"The original equation is $$y'' + 4y = 2 \tan x$$ What I did so far: $$\lambda^2+4 = 0$$ $$\lambda_1 = -2i \quad \lambda_2 = 2i$$ $$y(x) = C_1\cos2x+C_2\sin2x$$ Write down the system: $$\begin{cases} C_1'\cos2x + C_2'\sin2x = 0 \\ 2C'_2\cos2 x - 2C_1'\sin2x = 2\tan x \end{cases}$$ Applying Krammer's formulas $$\Delta = \begin{vmatrix}\cos2x & \sin2x \\ 2\cos2x  & -2\sin2x \end{vmatrix} = -2\cos4x$$ $$\Delta_2 = \begin{vmatrix} 0 & \sin2x \\ 2\tan x & -2\sin2x\end{vmatrix} = -2\tan x\sin2x = -4\sin^2x$$ $$\Delta_3 = \begin{vmatrix}\cos2x & 0 \\ 2\cos2x & 2\tan x \end{vmatrix} = -2\cos2x\tan x$$ but when I tried to find solutions I got different expressions which I do not know how to integrate, what is the best way to simplify solutions here to get better integrals?","The original equation is $$y'' + 4y = 2 \tan x$$ What I did so far: $$\lambda^2+4 = 0$$ $$\lambda_1 = -2i \quad \lambda_2 = 2i$$ $$y(x) = C_1\cos2x+C_2\sin2x$$ Write down the system: $$\begin{cases} C_1'\cos2x + C_2'\sin2x = 0 \\ 2C'_2\cos2 x - 2C_1'\sin2x = 2\tan x \end{cases}$$ Applying Krammer's formulas $$\Delta = \begin{vmatrix}\cos2x & \sin2x \\ 2\cos2x  & -2\sin2x \end{vmatrix} = -2\cos4x$$ $$\Delta_2 = \begin{vmatrix} 0 & \sin2x \\ 2\tan x & -2\sin2x\end{vmatrix} = -2\tan x\sin2x = -4\sin^2x$$ $$\Delta_3 = \begin{vmatrix}\cos2x & 0 \\ 2\cos2x & 2\tan x \end{vmatrix} = -2\cos2x\tan x$$ but when I tried to find solutions I got different expressions which I do not know how to integrate, what is the best way to simplify solutions here to get better integrals?",,['ordinary-differential-equations']
7,Solving simple wave equation,Solving simple wave equation,,"This is a homework question for numerical analysis that I do not know how to find the solution to. $\frac{du}{dt}-\frac{du}{dx}=0$ $u(x,0)=e^x$ $u(1,t)=e^{t+1}$ $0\leq x \leq1,0\leq t \leq1$ I know the solution should be of the form $u=f(x)g(t)$ but I can't find any literature that explains what to do with the boundary conditions. My teacher said I don't need to have worked with PDEs before to solve this but I am lost.","This is a homework question for numerical analysis that I do not know how to find the solution to. $\frac{du}{dt}-\frac{du}{dx}=0$ $u(x,0)=e^x$ $u(1,t)=e^{t+1}$ $0\leq x \leq1,0\leq t \leq1$ I know the solution should be of the form $u=f(x)g(t)$ but I can't find any literature that explains what to do with the boundary conditions. My teacher said I don't need to have worked with PDEs before to solve this but I am lost.",,"['ordinary-differential-equations', 'partial-differential-equations', 'wave-equation', 'linear-pde']"
8,General solution to nth order linear inhomogeneous ode,General solution to nth order linear inhomogeneous ode,,"I am trying to understand Problem 3.21 in Teschl's book on ODE's. Prove that the solution to the inhomogeneous equation   $$ x^{(n)} + c_{n-1} x^{(n-1)} + \cdots + c_1 \dot{x} + c_0 x = g(t) $$   is given by   $$ x(t) = x_h(t) + \int_0^t u(t-s) g(s) ds $$   where $x_h(t)$ is an arbitrary solution of the homogeneous equation and $u(t)$ is the solution of the homogeneous equation corresponding to the initial condition $u(0) = \dot{u}= \cdots = u^{(n-2)} = 0$ and $u^{(n-1)}=1$. Hint: either reduce it to the general form of the inhomogeneous linear equation   $$ x(t) = Ax(t) + g(t) $$   which is given by   $$ x(t) = e^{tA} x_0 + \int_0^t e^{(t-s)A} g(s) ds $$   or verify it directly.  I recommend doing both. I am able to verify directly by repeated differentiation.  However, I cannot seem to manipulate the nth order equation solution into the form of the solution of the inhomogeneous linear system despite the fact that the solutions look quite similar.  I really want to understand this connection though and would very much appreciate some insight into how this reduction is done.","I am trying to understand Problem 3.21 in Teschl's book on ODE's. Prove that the solution to the inhomogeneous equation   $$ x^{(n)} + c_{n-1} x^{(n-1)} + \cdots + c_1 \dot{x} + c_0 x = g(t) $$   is given by   $$ x(t) = x_h(t) + \int_0^t u(t-s) g(s) ds $$   where $x_h(t)$ is an arbitrary solution of the homogeneous equation and $u(t)$ is the solution of the homogeneous equation corresponding to the initial condition $u(0) = \dot{u}= \cdots = u^{(n-2)} = 0$ and $u^{(n-1)}=1$. Hint: either reduce it to the general form of the inhomogeneous linear equation   $$ x(t) = Ax(t) + g(t) $$   which is given by   $$ x(t) = e^{tA} x_0 + \int_0^t e^{(t-s)A} g(s) ds $$   or verify it directly.  I recommend doing both. I am able to verify directly by repeated differentiation.  However, I cannot seem to manipulate the nth order equation solution into the form of the solution of the inhomogeneous linear system despite the fact that the solutions look quite similar.  I really want to understand this connection though and would very much appreciate some insight into how this reduction is done.",,"['linear-algebra', 'ordinary-differential-equations']"
9,Finding ordinary and singular points,Finding ordinary and singular points,,"I just have a quick question regarding the following ODE: $$P(x) y'' + Q(x) y' + R(x)=0.$$ To find the singular points of this ODE, is it correct to set $P(x)$ to $0$ and just solve for the $x$ terms? Or is there another method for finding the singular points. I understand you can take the limit of both $Q(x)$ and $R(x)$ over $P(x)$ respectively to check whether a given point is singular/ordinary. But my main question is, if I am asked to specifically FIND the point, is setting $P(x)$ equal to $0$ and solving for the x values the correct method? How about for finding ordinary points? Is there a method to find the point, or do I just look at $P(x)$ and think to myself, what values of $x$ can I plug into $P(x)$ such that $P(x)$ will not equal $0$? Thank you!","I just have a quick question regarding the following ODE: $$P(x) y'' + Q(x) y' + R(x)=0.$$ To find the singular points of this ODE, is it correct to set $P(x)$ to $0$ and just solve for the $x$ terms? Or is there another method for finding the singular points. I understand you can take the limit of both $Q(x)$ and $R(x)$ over $P(x)$ respectively to check whether a given point is singular/ordinary. But my main question is, if I am asked to specifically FIND the point, is setting $P(x)$ equal to $0$ and solving for the x values the correct method? How about for finding ordinary points? Is there a method to find the point, or do I just look at $P(x)$ and think to myself, what values of $x$ can I plug into $P(x)$ such that $P(x)$ will not equal $0$? Thank you!",,['ordinary-differential-equations']
10,Determine if the differential equation will have an unique solution on D,Determine if the differential equation will have an unique solution on D,,"Consider the following differential equation $$y'=xe^{-y^{2}}$$ the boundary condition is $y(1)=e$ and $D=\{(x,y):x,y\in R, 1\le x \le 2\}$ I need to figure out if this has an unique solution on $D$ I have solved the differential equation as far as I could to be $$\frac{1}{2} \sqrt{\pi}erfi(y)=\frac{x^2}{2}+c_1$$ Now from here I am unsure how to ""solve for y"" and how do I come to a conclusion if this has a unique solution or not.","Consider the following differential equation $$y'=xe^{-y^{2}}$$ the boundary condition is $y(1)=e$ and $D=\{(x,y):x,y\in R, 1\le x \le 2\}$ I need to figure out if this has an unique solution on $D$ I have solved the differential equation as far as I could to be $$\frac{1}{2} \sqrt{\pi}erfi(y)=\frac{x^2}{2}+c_1$$ Now from here I am unsure how to ""solve for y"" and how do I come to a conclusion if this has a unique solution or not.",,"['integration', 'ordinary-differential-equations']"
11,Existence of solution of $f'(x) = f(\phi(x))$,Existence of solution of,f'(x) = f(\phi(x)),"Let $\phi:[0,1]\to[0,1]$ be a none constant and continuous map. Prove that there exists a unique function $f\in C^1([0,1], \Bbb R)$ satisfying $$f'(x) = f(\phi(x))~~~f(0)=\alpha$$ where $\alpha\in\Bbb R$ is given. I attempted to use the Cauchy-Lipschitz Theorem but I couldn't go further. Any Hint?","Let $\phi:[0,1]\to[0,1]$ be a none constant and continuous map. Prove that there exists a unique function $f\in C^1([0,1], \Bbb R)$ satisfying $$f'(x) = f(\phi(x))~~~f(0)=\alpha$$ where $\alpha\in\Bbb R$ is given. I attempted to use the Cauchy-Lipschitz Theorem but I couldn't go further. Any Hint?",,"['functional-analysis', 'ordinary-differential-equations', 'analysis']"
12,"Why does dividing ""population"" by ""average life expectancy"" generate the ""death rate""?","Why does dividing ""population"" by ""average life expectancy"" generate the ""death rate""?",,I've a general basic question: why when we divide a population to the average life expectancy (average lifetime) creates the number of people who die at time t? I think this is related to the Little's law. but I can't really grasp why when we divide the number of people to its average lifetime creates the number of people who died. Can anyone explain in simple terms. I really appreciate your response. Regards.,I've a general basic question: why when we divide a population to the average life expectancy (average lifetime) creates the number of people who die at time t? I think this is related to the Little's law. but I can't really grasp why when we divide the number of people to its average lifetime creates the number of people who died. Can anyone explain in simple terms. I really appreciate your response. Regards.,,"['integration', 'ordinary-differential-equations']"
13,Solving Cauchy-Euler equations with substitution,Solving Cauchy-Euler equations with substitution,,"I have to solve the following equation as a Cauchy-Euler type for $1<3x$: $$(1-3x)^2y''-(3x-1)y'+3y=0 $$ I tried this: Take $$u=3x-1$$ then do $\frac{du}{dx}=3 $ $$du=3dx$$ So, $\frac{dy}{dx}= \frac{3dy}{du} $ Q. How would you do $\frac{d^2y}{du^2}$? Am I wrong? How would you proceed?","I have to solve the following equation as a Cauchy-Euler type for $1<3x$: $$(1-3x)^2y''-(3x-1)y'+3y=0 $$ I tried this: Take $$u=3x-1$$ then do $\frac{du}{dx}=3 $ $$du=3dx$$ So, $\frac{dy}{dx}= \frac{3dy}{du} $ Q. How would you do $\frac{d^2y}{du^2}$? Am I wrong? How would you proceed?",,"['calculus', 'ordinary-differential-equations', 'derivatives']"
14,This ODE has one general solutions or two solutions?,This ODE has one general solutions or two solutions?,,"The ODE $$2xy''-y'+\frac{1}{y'}=0$$ is a second order ODE , so it must have one general solution containing 2 constants  . I solved it by reduction of order as following but I have 2 general solutions each contains 2 constants , since I have positive and negative square root . I would like to know , do they represent one general solution since the sign can be included into the constant ? or do they represent as I said before 2 general solutions ? Here is my solution : let v=y' , v'=y'' $$2xv'=v-\frac{1}{v}$$ $$v^2-1=cx$$ $$v=\pm\sqrt{cx+1}$$ $$\frac{dy}{dx}=\pm\sqrt{cx+1}$$ $$y=\pm\frac{2}{3c}(cx+1)^{3/2}+k$$","The ODE $$2xy''-y'+\frac{1}{y'}=0$$ is a second order ODE , so it must have one general solution containing 2 constants  . I solved it by reduction of order as following but I have 2 general solutions each contains 2 constants , since I have positive and negative square root . I would like to know , do they represent one general solution since the sign can be included into the constant ? or do they represent as I said before 2 general solutions ? Here is my solution : let v=y' , v'=y'' $$2xv'=v-\frac{1}{v}$$ $$v^2-1=cx$$ $$v=\pm\sqrt{cx+1}$$ $$\frac{dy}{dx}=\pm\sqrt{cx+1}$$ $$y=\pm\frac{2}{3c}(cx+1)^{3/2}+k$$",,['ordinary-differential-equations']
15,Frobenius method on $x''+\frac{1}{t}x'+x=0$,Frobenius method on,x''+\frac{1}{t}x'+x=0,I am trying to use the Frobenius method on $x''+\frac{1}{t}x'+x=0$ where $x(0)=1$ and $x'(0)=0$. So far I have: Let $x(t)=\sum_{n=0}^\infty a_nt^n$ then $x'(t)=\sum_{n=0}^\infty na_nt^{n-1}\rightarrow \frac{1}{t}x'(t)=\sum_{n=0}^\infty \frac{1}{t}na_nt^{n-1}=\sum_{n=0}^\infty na_nt^{n-2}$ $x''(t)=\sum_{n=0}^\infty n^2a_nt^{n-2}$ So if I put this into the ODE I get $\sum_{n=0}^\infty \bigg[n^2a_nt^{n-2}+na_nt^{n-2}+a_nt^n \bigg]=\sum_{n=0}^\infty \bigg[a_n(n^2+n)t^{n-2}+a_nt^n \bigg] = 2a_1t^{-1}+\sum_{n=2}^\infty a_n(n^2+n)t^{n-2}+\sum_{n=0}^\infty a_nt^n = 2a_1t^{-1}+\sum_{n=0}^\infty a_{n+2}((n+2)^2+n+2)t^{n}+\sum_{n=0}^\infty a_nt^n$ I can't figure out how to finish this. Can someone help me out?,I am trying to use the Frobenius method on $x''+\frac{1}{t}x'+x=0$ where $x(0)=1$ and $x'(0)=0$. So far I have: Let $x(t)=\sum_{n=0}^\infty a_nt^n$ then $x'(t)=\sum_{n=0}^\infty na_nt^{n-1}\rightarrow \frac{1}{t}x'(t)=\sum_{n=0}^\infty \frac{1}{t}na_nt^{n-1}=\sum_{n=0}^\infty na_nt^{n-2}$ $x''(t)=\sum_{n=0}^\infty n^2a_nt^{n-2}$ So if I put this into the ODE I get $\sum_{n=0}^\infty \bigg[n^2a_nt^{n-2}+na_nt^{n-2}+a_nt^n \bigg]=\sum_{n=0}^\infty \bigg[a_n(n^2+n)t^{n-2}+a_nt^n \bigg] = 2a_1t^{-1}+\sum_{n=2}^\infty a_n(n^2+n)t^{n-2}+\sum_{n=0}^\infty a_nt^n = 2a_1t^{-1}+\sum_{n=0}^\infty a_{n+2}((n+2)^2+n+2)t^{n}+\sum_{n=0}^\infty a_nt^n$ I can't figure out how to finish this. Can someone help me out?,,['ordinary-differential-equations']
16,Extending numerical Euler method to higher order differential equations,Extending numerical Euler method to higher order differential equations,,"If it was a second order DE I can follow the logic however I get very confused when trying to numerically solve a third order DE. I have to estimate  $y(0.1)$ using step size 0.1 $$xy''' + xy'' + x^2y' + y = x,\quad with \quad y(0) = 1, y'(0) = 0, y''(0) = 1$$ where $$y_0 = y(x_0)$$  $$y_{n +1} = y_n + hf(x_n, y_n)$$ $$x_{n +1} = x_n + h$$ Now I can set $u_1=y, u_2=y', u_3=y''  $ which gives $u_1'=y'=u_2$ and $u_2'=y''=u_3$ and $u_3'=y'''$ So I can sub these in to get $$xu_3'+xu_3+x^2u_2+y=x\tag{*}$$ but I can also get  $$xu_3'+xu_2'+x^2u_1'+y=x\tag{**}$$ Would they both lead to the same answers? The first one does seem easier to work with. And assuming I go with the first one, I still do not know how to set up my iteration because I have no idea how to relate them. Approximate the value of $y (0.2)$ , where $y(x)$ is the solution of the initial-value problem $$y''+ xy' + y = 0,   y(0) = 1, y'(0) = 2$$ Substitute $u = y'$ to obtain the system $y' = u, u ' = −xu − y$ Euler’s scheme then gives $$y_{n +1}= y_n + hu_n$$  $$u_{n +1}= u_n + h( −x_nu_n − y_n)$$ If we use the step-size $h = 0.1$ and $y_0 = 1$, $u_0= 2$, we obtain $$y_1 = y_0 + (0.1)u_0 = 1 + (0.1)2 = 1.2$$ $$u_1 = u_0 + (0.1)( −x 0u 0 − y_0) = 2 + (0.1)( −1) = 1.9$$ $$y_2 = y_1 + (0.1)u_1 = 1.2 + (0.1)(1.9) = 1.39$$ $$u_2 = u_1 + (0.1)( −x_1 u_1 − y_1) = 1.9 − (0.1)1.39 = 1.761.1.9 + (0.1)$$ Thus we obtain $$y(0.2) ≈ 1.39$$ and $$y'(0.2) ≈ 1.761$$","If it was a second order DE I can follow the logic however I get very confused when trying to numerically solve a third order DE. I have to estimate  $y(0.1)$ using step size 0.1 $$xy''' + xy'' + x^2y' + y = x,\quad with \quad y(0) = 1, y'(0) = 0, y''(0) = 1$$ where $$y_0 = y(x_0)$$  $$y_{n +1} = y_n + hf(x_n, y_n)$$ $$x_{n +1} = x_n + h$$ Now I can set $u_1=y, u_2=y', u_3=y''  $ which gives $u_1'=y'=u_2$ and $u_2'=y''=u_3$ and $u_3'=y'''$ So I can sub these in to get $$xu_3'+xu_3+x^2u_2+y=x\tag{*}$$ but I can also get  $$xu_3'+xu_2'+x^2u_1'+y=x\tag{**}$$ Would they both lead to the same answers? The first one does seem easier to work with. And assuming I go with the first one, I still do not know how to set up my iteration because I have no idea how to relate them. Approximate the value of $y (0.2)$ , where $y(x)$ is the solution of the initial-value problem $$y''+ xy' + y = 0,   y(0) = 1, y'(0) = 2$$ Substitute $u = y'$ to obtain the system $y' = u, u ' = −xu − y$ Euler’s scheme then gives $$y_{n +1}= y_n + hu_n$$  $$u_{n +1}= u_n + h( −x_nu_n − y_n)$$ If we use the step-size $h = 0.1$ and $y_0 = 1$, $u_0= 2$, we obtain $$y_1 = y_0 + (0.1)u_0 = 1 + (0.1)2 = 1.2$$ $$u_1 = u_0 + (0.1)( −x 0u 0 − y_0) = 2 + (0.1)( −1) = 1.9$$ $$y_2 = y_1 + (0.1)u_1 = 1.2 + (0.1)(1.9) = 1.39$$ $$u_2 = u_1 + (0.1)( −x_1 u_1 − y_1) = 1.9 − (0.1)1.39 = 1.761.1.9 + (0.1)$$ Thus we obtain $$y(0.2) ≈ 1.39$$ and $$y'(0.2) ≈ 1.761$$",,['ordinary-differential-equations']
17,Solve the differential equations:,Solve the differential equations:,,$$y' = {y \over {\sin x}}+\tan{x \over 2}$$ I was trying to do this by substitution $u=y/x $ and it did not work and also with $$y' - {y \over {\sin x}} = 0$$ $${dy \over dx} = {y \over {\sin x}} $$ $${\ln(y) = \ln\left|\tan\frac{x}{2}\right|+c } $$ $${y = c\cdot\tan\frac{x}{2} } $$ but then when im trying to calculate $y'$ I have a problem and I have too many equations. Is there some easier way or am I making some mistakes.,$$y' = {y \over {\sin x}}+\tan{x \over 2}$$ I was trying to do this by substitution $u=y/x $ and it did not work and also with $$y' - {y \over {\sin x}} = 0$$ $${dy \over dx} = {y \over {\sin x}} $$ $${\ln(y) = \ln\left|\tan\frac{x}{2}\right|+c } $$ $${y = c\cdot\tan\frac{x}{2} } $$ but then when im trying to calculate $y'$ I have a problem and I have too many equations. Is there some easier way or am I making some mistakes.,,['ordinary-differential-equations']
18,How to find the general solution of a system of ODEs?,How to find the general solution of a system of ODEs?,,"I am trying to understand an example of Lawrance Perko, the problem being  \begin{align} \dot{x}_1 &=-x_1-3x_2\\ \dot{x}_2 &=2x_2 \end{align} Which can be written in the form $\dot{x}=Ax$ where : \begin{align} A= \begin{bmatrix} -1 & -3 \\ 0  & 2 \end{bmatrix} \end{align} The eigen values of A are $\lambda_{1}=-1$,$\lambda_{1}=2$ and the eigenvectors are $v_1=[1,0]$ and $v_2=[-1,1]$ respectively. The matrix P and the decoupling matrix $P^{-1}$ are given by, \begin{align} P=\begin{bmatrix} 1 &-1 \\ 0 & 1 \end{bmatrix},P^{-1}= \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} \end{align} Under the co-ordinate transform $y=P^{-1}x$ the linearly uncoupled system is given by: \begin{align} \dot{y_1}=-y_1\\ \dot{y_2}=2y_2 \end{align} Which has the general solution $y_1=c_1\exp(-t)$, $y_2=c_2\exp(2t)$. Now the general solution of the original system is iven by: \begin{align}\tag{1} x(t)=P\begin{bmatrix} e^{-t} & 0 \\ 0 & e^{2t} \end{bmatrix}P^{-1}c \end{align} Where $c=x(0)$, or equivalently by What I don't understand is the last step. According to my calculation $y=P^{-1}x$ so $x_1=y_1-y_2$, $x_2 = y_2$. So $x_1$ should be $c_1 e^{-t}-c_2e^{2t}$ and not $c_1 e^{-t}+c_2(e^{-t}-e^{2t})$ \ref{1}","I am trying to understand an example of Lawrance Perko, the problem being  \begin{align} \dot{x}_1 &=-x_1-3x_2\\ \dot{x}_2 &=2x_2 \end{align} Which can be written in the form $\dot{x}=Ax$ where : \begin{align} A= \begin{bmatrix} -1 & -3 \\ 0  & 2 \end{bmatrix} \end{align} The eigen values of A are $\lambda_{1}=-1$,$\lambda_{1}=2$ and the eigenvectors are $v_1=[1,0]$ and $v_2=[-1,1]$ respectively. The matrix P and the decoupling matrix $P^{-1}$ are given by, \begin{align} P=\begin{bmatrix} 1 &-1 \\ 0 & 1 \end{bmatrix},P^{-1}= \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} \end{align} Under the co-ordinate transform $y=P^{-1}x$ the linearly uncoupled system is given by: \begin{align} \dot{y_1}=-y_1\\ \dot{y_2}=2y_2 \end{align} Which has the general solution $y_1=c_1\exp(-t)$, $y_2=c_2\exp(2t)$. Now the general solution of the original system is iven by: \begin{align}\tag{1} x(t)=P\begin{bmatrix} e^{-t} & 0 \\ 0 & e^{2t} \end{bmatrix}P^{-1}c \end{align} Where $c=x(0)$, or equivalently by What I don't understand is the last step. According to my calculation $y=P^{-1}x$ so $x_1=y_1-y_2$, $x_2 = y_2$. So $x_1$ should be $c_1 e^{-t}-c_2e^{2t}$ and not $c_1 e^{-t}+c_2(e^{-t}-e^{2t})$ \ref{1}",,"['linear-algebra', 'ordinary-differential-equations', 'dynamical-systems']"
19,Prove that if $P^{-1}AP=diag[\lambda_j]$ then $e^{At}=Pdiag[e^{\lambda_j t}]P^{-1}$,Prove that if  then,P^{-1}AP=diag[\lambda_j] e^{At}=Pdiag[e^{\lambda_j t}]P^{-1},I'm stuck in this exercise. If $P^{-1}AP=diag[\lambda_j]$ then $e^{At}=Pdiag[e^{\lambda_j t}]P^{-1}$ This is what I've done: $$P^{-1}AP=diag[\lambda_j]$$ $$\implies AP=Pdiag[\lambda_j]$$ $$\implies A=Pdiag[\lambda_j]P^{-1}$$ $$\implies At=Pdiag[\lambda_jt]P^{-1}$$ $$\implies e^{At}=e^{Pdiag[\lambda_jt]P^{-1}}$$ And I need the last implication to be equal to $e^{At}=Pdiag[e^{\lambda_j t}P^{-1}]$ If anyone could help that would be great.,I'm stuck in this exercise. If then This is what I've done: And I need the last implication to be equal to If anyone could help that would be great.,P^{-1}AP=diag[\lambda_j] e^{At}=Pdiag[e^{\lambda_j t}]P^{-1} P^{-1}AP=diag[\lambda_j] \implies AP=Pdiag[\lambda_j] \implies A=Pdiag[\lambda_j]P^{-1} \implies At=Pdiag[\lambda_jt]P^{-1} \implies e^{At}=e^{Pdiag[\lambda_jt]P^{-1}} e^{At}=Pdiag[e^{\lambda_j t}P^{-1}],"['matrices', 'ordinary-differential-equations']"
20,What conditions are necessary or sufficient for this manipulation of differentials to be justified?,What conditions are necessary or sufficient for this manipulation of differentials to be justified?,,"This comes from a problem given in ""Physics"", Halliday-Resnick-Krane, Chapter 2, Problem 55. It asks to study a non-uniformly accelerated motion defined by $ a(t)=-3v(t)^2 $ and derive a numerical value for the time elapsed given the initial and final velocity (the initial velocity is $1.5$ and the final velocity is $0.75$). This becomes a differential equation, $ v'(t)=-3v(t)^2 $. Since, of course, first-year (standard) calculus doesn't provide, as far as I know, tools to solve this, after trying in vain on my own I looked for solutions on the web, and I found the following procedure in a physics forum: $$ \frac{dv}{dt} = -3v^2 \ \implies \ \frac{dv}{v^2}=-3dt \\ \implies \int_{v_0}^v\frac{dv}{v^2}=\int_0^t(-3dt) \implies \frac{1}{v_0}-\frac{1}{v}=-3t \ .$$ The numerical result stemming from this procedure is in perfect agreement with the numerical value given by the textbook itself (the textbook gives $0.2222$, my calculator gives $0. \bar 2 $). There are quite a lot of things that I don't understand here. There's the infamous multiplication by $dt$ and consequent cancellation of it in the LHS. What assumptions need to be made to justify this, if it is even possible? Apart from that, in the same passage they also divide by $v^2$. Is this only justified in this case because they know that both the initial and final velocity are greater than zero and the velocity is strictly decreasing? If not, how? In the next passage, they integrate the LHS as if $v$ was a variable, and I'm not sure if or how the variable substitution theorem and/or the chain rule can apply in this specific context, since we're coming from an expression (between the first and second passage) that either doesn't make sense or presents differential forms, which I haven't studied yet. In general , is there a theorem that somehow justifies this notational manipulations?, And in what conditions would such a theorem apply?","This comes from a problem given in ""Physics"", Halliday-Resnick-Krane, Chapter 2, Problem 55. It asks to study a non-uniformly accelerated motion defined by $ a(t)=-3v(t)^2 $ and derive a numerical value for the time elapsed given the initial and final velocity (the initial velocity is $1.5$ and the final velocity is $0.75$). This becomes a differential equation, $ v'(t)=-3v(t)^2 $. Since, of course, first-year (standard) calculus doesn't provide, as far as I know, tools to solve this, after trying in vain on my own I looked for solutions on the web, and I found the following procedure in a physics forum: $$ \frac{dv}{dt} = -3v^2 \ \implies \ \frac{dv}{v^2}=-3dt \\ \implies \int_{v_0}^v\frac{dv}{v^2}=\int_0^t(-3dt) \implies \frac{1}{v_0}-\frac{1}{v}=-3t \ .$$ The numerical result stemming from this procedure is in perfect agreement with the numerical value given by the textbook itself (the textbook gives $0.2222$, my calculator gives $0. \bar 2 $). There are quite a lot of things that I don't understand here. There's the infamous multiplication by $dt$ and consequent cancellation of it in the LHS. What assumptions need to be made to justify this, if it is even possible? Apart from that, in the same passage they also divide by $v^2$. Is this only justified in this case because they know that both the initial and final velocity are greater than zero and the velocity is strictly decreasing? If not, how? In the next passage, they integrate the LHS as if $v$ was a variable, and I'm not sure if or how the variable substitution theorem and/or the chain rule can apply in this specific context, since we're coming from an expression (between the first and second passage) that either doesn't make sense or presents differential forms, which I haven't studied yet. In general , is there a theorem that somehow justifies this notational manipulations?, And in what conditions would such a theorem apply?",,"['ordinary-differential-equations', 'physics']"
21,Wronskian identically zero or never zero?,Wronskian identically zero or never zero?,,Consider the equation $$y''- \dfrac{y'}{x} = 0~.$$ Solution is $$y=Cx^2 + d~.$$ The Wronskian of $x^2$ and $1$ turns out to be $-2x$ which is zero at $x = 0$ and non zero elsewhere. But the Wronskian of solutions to an equation of type $$y'' + p(x) y' + q(x) y = 0$$ should be identically zero or never zero. What am I missing?,Consider the equation Solution is The Wronskian of and turns out to be which is zero at and non zero elsewhere. But the Wronskian of solutions to an equation of type should be identically zero or never zero. What am I missing?,y''- \dfrac{y'}{x} = 0~. y=Cx^2 + d~. x^2 1 -2x x = 0 y'' + p(x) y' + q(x) y = 0,"['calculus', 'ordinary-differential-equations', 'wronskian']"
22,Math biology proof by induction question,Math biology proof by induction question,,"I am stuck on this question: Consider the delay-differential equation $\frac{d}{ dt}x(t) = x(t − 1)$, with $x(t) = 1$ for $t \in [−1, 0]$. The solution is given by $x(t) = \sum^{n}_{0}{\frac{[t − (k − 1)]^k}{k!}}$ for $n − 1 \leq t \leq n$, where $n$ is a non-negative integer. Use proof by induction (i.e., method of steps) to find the solution I have tried to use the sum of a geometric series on the series and then differentiating this and setting it equals to $x(t-1)$ also in the geometric series however when i tried to the basis case $k=1$ I ended up with $\frac{1-t^n}{t-1}=\frac{1+t^2(t-1)^n-nt^3(t-1)^{n-1}}{t^2}$ which I am unable to prove is true, can anyone see where I have gone wrong or suggest a better method of approach this? -edited so that n is the upper limit","I am stuck on this question: Consider the delay-differential equation $\frac{d}{ dt}x(t) = x(t − 1)$, with $x(t) = 1$ for $t \in [−1, 0]$. The solution is given by $x(t) = \sum^{n}_{0}{\frac{[t − (k − 1)]^k}{k!}}$ for $n − 1 \leq t \leq n$, where $n$ is a non-negative integer. Use proof by induction (i.e., method of steps) to find the solution I have tried to use the sum of a geometric series on the series and then differentiating this and setting it equals to $x(t-1)$ also in the geometric series however when i tried to the basis case $k=1$ I ended up with $\frac{1-t^n}{t-1}=\frac{1+t^2(t-1)^n-nt^3(t-1)^{n-1}}{t^2}$ which I am unable to prove is true, can anyone see where I have gone wrong or suggest a better method of approach this? -edited so that n is the upper limit",,"['ordinary-differential-equations', 'biology']"
23,Using variation of parameters on $2y'' - 3y' +y = (t^2 +1)e^t$,Using variation of parameters on,2y'' - 3y' +y = (t^2 +1)e^t,"Suppose $$2y'' - 3y' + y = (t^2 +1)e^t.$$ How do you solve this using variation of parameters? When I try solving this, I get the following answer:  $$c_1 e^{t/2} + (c_2+\frac{2}{3}t^3 - 4t^2 + 18t)e^t,$$ but the answer in the back of the book (and from WolframAlpha) is  $$c_1 e^{t/2} + (c_2+\frac{1}{3}t^3 - 2t^2 + 9t)e^t.$$ Here is an outline of my attempt. First, we solve $$2y'' -3y' +y = 0$$ and find that $y_1(t) = e^{t/2}$ and $y_2(t) = e^t$ are solutions. We next find $u_1(t)$ and $u_2(t)$ so that $y(t) = u_1(t)y_1(t) + u_2(t)y_2(t)$ is a solution to the original differential equation. The Wronskian is $W[y_1,y_2](t) = e^{3t/2}/2$. So, let  $g(t) = (t^2 + 1)e^t.$ We find that  $$u_1(t) = \int -\frac{g(t)y_2(t)}{W[y_1,y_2]}dt = -2\int(t^2+1)e^{t/2}dt.$$ Using integration by parts twice yields $$u_1(t) = -4(t^2+1)e^{t/2} + 16te^{t/2} - 32e^{t/2},$$ which matches with what WolframAlpha says the integral is (after some slight simplifying). Then,  $$u_2(t) = \int \frac{g(t)y_1(t)}{W[y_1,y_2]} dt = 2\int (t^2 + 1) dt.$$ And hence $u_2(t) = 2(t^3/3 + t)$. My solution is thus $y =(c_1 + u_1)y_1 + (c_2+u_2)y_2$, which after simplifying, gives what I wrote above (after absorbing the constant $-36$ into the constant $c_2$).","Suppose $$2y'' - 3y' + y = (t^2 +1)e^t.$$ How do you solve this using variation of parameters? When I try solving this, I get the following answer:  $$c_1 e^{t/2} + (c_2+\frac{2}{3}t^3 - 4t^2 + 18t)e^t,$$ but the answer in the back of the book (and from WolframAlpha) is  $$c_1 e^{t/2} + (c_2+\frac{1}{3}t^3 - 2t^2 + 9t)e^t.$$ Here is an outline of my attempt. First, we solve $$2y'' -3y' +y = 0$$ and find that $y_1(t) = e^{t/2}$ and $y_2(t) = e^t$ are solutions. We next find $u_1(t)$ and $u_2(t)$ so that $y(t) = u_1(t)y_1(t) + u_2(t)y_2(t)$ is a solution to the original differential equation. The Wronskian is $W[y_1,y_2](t) = e^{3t/2}/2$. So, let  $g(t) = (t^2 + 1)e^t.$ We find that  $$u_1(t) = \int -\frac{g(t)y_2(t)}{W[y_1,y_2]}dt = -2\int(t^2+1)e^{t/2}dt.$$ Using integration by parts twice yields $$u_1(t) = -4(t^2+1)e^{t/2} + 16te^{t/2} - 32e^{t/2},$$ which matches with what WolframAlpha says the integral is (after some slight simplifying). Then,  $$u_2(t) = \int \frac{g(t)y_1(t)}{W[y_1,y_2]} dt = 2\int (t^2 + 1) dt.$$ And hence $u_2(t) = 2(t^3/3 + t)$. My solution is thus $y =(c_1 + u_1)y_1 + (c_2+u_2)y_2$, which after simplifying, gives what I wrote above (after absorbing the constant $-36$ into the constant $c_2$).",,['ordinary-differential-equations']
24,Stability analysis for tracking controller - error dynamics?,Stability analysis for tracking controller - error dynamics?,,"say I have an inverted pendulum system like $$ \begin{split} \dot{x}_1 &= x_2 \\ \dot{x}_2 &= \sin(x_1) - 0.5x_2 + u \,. \end{split}  $$ I now want to design a controller that tracks a user defined reference for the system output $y = x_1$ such that the error $e := y_{set} - y$ gets zero. Remark : I know that it doesn't really make sense to try to track any other reference for $y$ other than one of the equilibria, but I am more interested in the method itself rather than the result (the pendulum is just a minimal example, in reality I would track the position of the cart carrying the pendulum). First I transform the system into error coordinates: $$ e = y_{set} - x_1 \rightarrow x_1 = y_{set} - e $$ and therefore, assuming constant references values $y_{set}$ we get the new system equations $$ \begin{split} \dot{e} &= -x_2 \\ \dot{x}_2 &= \sin(y_{set} - e) - 0.5x_2 + u \,. \end{split} $$ Correct so far? Question : Now here is my problem: If I want to perform a stability analysis using the standard Lyapunov function approach, how do I have to deal with the $y_{set}$ in the system equation above? Because for $y_{set} \neq 0$, the term $\dot{x}_2 \neq 0$ even if $e = x_2 = u = 0$. Since the reference can in principle be an arbitrary constant, how can I deal with it efficiently?","say I have an inverted pendulum system like $$ \begin{split} \dot{x}_1 &= x_2 \\ \dot{x}_2 &= \sin(x_1) - 0.5x_2 + u \,. \end{split}  $$ I now want to design a controller that tracks a user defined reference for the system output $y = x_1$ such that the error $e := y_{set} - y$ gets zero. Remark : I know that it doesn't really make sense to try to track any other reference for $y$ other than one of the equilibria, but I am more interested in the method itself rather than the result (the pendulum is just a minimal example, in reality I would track the position of the cart carrying the pendulum). First I transform the system into error coordinates: $$ e = y_{set} - x_1 \rightarrow x_1 = y_{set} - e $$ and therefore, assuming constant references values $y_{set}$ we get the new system equations $$ \begin{split} \dot{e} &= -x_2 \\ \dot{x}_2 &= \sin(y_{set} - e) - 0.5x_2 + u \,. \end{split} $$ Correct so far? Question : Now here is my problem: If I want to perform a stability analysis using the standard Lyapunov function approach, how do I have to deal with the $y_{set}$ in the system equation above? Because for $y_{set} \neq 0$, the term $\dot{x}_2 \neq 0$ even if $e = x_2 = u = 0$. Since the reference can in principle be an arbitrary constant, how can I deal with it efficiently?",,"['ordinary-differential-equations', 'dynamical-systems', 'control-theory', 'nonlinear-system', 'stability-theory']"
25,First Order DE Mixing Problem,First Order DE Mixing Problem,,"I've tried this problem quite a few times but I can't seem to get it right. A large tank contains 60 litres of water in which 23 grams of salt is dissolved. Brine containing 15 grams of salt per litre is pumped into the tank at a rate of 8 litres per minute. The well mixed solution is pumped out of the tank at a rate of 2 litres per minute. (a)    Find an expression for the amount of water in the tank after t minutes. (b)    Let x(t) be the amount of salt in the tank after t minutes. Which of the following is a differential equation for x(t)? EDIT: I totally misread part (a) and thought it was asking for an equation for the amount of salt in the tank. I do still need help finding x(t), since I need it for the next question. I'm assuming I have to find (b) before I can answer (a). I found the differential equation to be $$\frac{dx}{dt}=120-\frac{2 x(t)}{60+6t}$$ which was correct. Since this is a linear first order DE, I found the integrating factor $$m(x)=e^{\int\frac{2}{60+6t} dt}$$ $$m(x)=e^{\frac{1}{3}\int\frac{1}{10+t} dt}$$ $$m(x) = e^{\frac{1}{3} ln(10+t)}$$ $$m(x) = (10+t)^{\frac{1}{3}}$$ Multiplying the DE by this gives $$\int ((10+t)^{1/3}x(t))'= \int 120*(10+t)^{1/3}$$ $$(10+t)^{1/3}x(t)=120(\frac{3}{4}(10+t)^{4/3}+C)$$ $$\therefore x(t) = \frac{120(\frac{3}{4}(10+t)^{4/3}+C)}{10+t)^{1/3}}$$ Using the inital value $x(0)=23$ , since 23 grams of salt is dissolved in the tank initially, I found C to be $$23 = \frac{120(\frac{3}{4}(10+0)^{4/3}+C)}{10+0)^{1/3}}$$ $$C = \frac{23}{900}$$ Subbing this into the equation and simplifying gives the final answer $$x(t) = \frac{120(\frac{3}{4}(10+t)^{4/3}+\frac{23}{900})}{(10+t)^{1/3}}$$ $$x(t) = \frac{90(10+t)^{4/3}+\frac{46}{15}}{(10+t)^{1/3}}$$ Which wasn't correct. EDIT: Also, I should add, I only have one try left on the above question, but the next question is In Problem #8 above the size of the tank was not given. Now suppose that in Problem #8 the tank has an open top and has a total capacity of 192 litres. How much salt (in grams) will be in the tank at the instant that it begins to overflow? Since there is 60L in the tank initially, and the contents if increasing at 6 L/min, I found t with the equation $$60+6t = 192$$ $$t = 22$$ I never actually entered the above equation, so I'm not 100% sure that it's incorrect, but I entered $x(22) = 2880.97$ into this question and it was incorrect.","I've tried this problem quite a few times but I can't seem to get it right. A large tank contains 60 litres of water in which 23 grams of salt is dissolved. Brine containing 15 grams of salt per litre is pumped into the tank at a rate of 8 litres per minute. The well mixed solution is pumped out of the tank at a rate of 2 litres per minute. (a)    Find an expression for the amount of water in the tank after t minutes. (b)    Let x(t) be the amount of salt in the tank after t minutes. Which of the following is a differential equation for x(t)? EDIT: I totally misread part (a) and thought it was asking for an equation for the amount of salt in the tank. I do still need help finding x(t), since I need it for the next question. I'm assuming I have to find (b) before I can answer (a). I found the differential equation to be which was correct. Since this is a linear first order DE, I found the integrating factor Multiplying the DE by this gives Using the inital value , since 23 grams of salt is dissolved in the tank initially, I found C to be Subbing this into the equation and simplifying gives the final answer Which wasn't correct. EDIT: Also, I should add, I only have one try left on the above question, but the next question is In Problem #8 above the size of the tank was not given. Now suppose that in Problem #8 the tank has an open top and has a total capacity of 192 litres. How much salt (in grams) will be in the tank at the instant that it begins to overflow? Since there is 60L in the tank initially, and the contents if increasing at 6 L/min, I found t with the equation I never actually entered the above equation, so I'm not 100% sure that it's incorrect, but I entered into this question and it was incorrect.",\frac{dx}{dt}=120-\frac{2 x(t)}{60+6t} m(x)=e^{\int\frac{2}{60+6t} dt} m(x)=e^{\frac{1}{3}\int\frac{1}{10+t} dt} m(x) = e^{\frac{1}{3} ln(10+t)} m(x) = (10+t)^{\frac{1}{3}} \int ((10+t)^{1/3}x(t))'= \int 120*(10+t)^{1/3} (10+t)^{1/3}x(t)=120(\frac{3}{4}(10+t)^{4/3}+C) \therefore x(t) = \frac{120(\frac{3}{4}(10+t)^{4/3}+C)}{10+t)^{1/3}} x(0)=23 23 = \frac{120(\frac{3}{4}(10+0)^{4/3}+C)}{10+0)^{1/3}} C = \frac{23}{900} x(t) = \frac{120(\frac{3}{4}(10+t)^{4/3}+\frac{23}{900})}{(10+t)^{1/3}} x(t) = \frac{90(10+t)^{4/3}+\frac{46}{15}}{(10+t)^{1/3}} 60+6t = 192 t = 22 x(22) = 2880.97,['ordinary-differential-equations']
26,How to use a tangent circle in a numerical method for complex-valued differential equation,How to use a tangent circle in a numerical method for complex-valued differential equation,,"If you solve the equation $$\frac{dz}{dt}=-i\omega z$$ with initial value $z(0)=1$ you get $$z = e^{-i\omega t}$$ which traces a circle in the complex plane with angular speed $-\omega$.  If you try to the Euler method of approximating the solution, it diverges because $$z_1 = z_0 + h(-i\omega)z_0 = (1-ih\omega)z_0$$ and $$|z_1| = |1-ih\omega| = \sqrt{1+(h\omega)^2}$$ for $h,\omega >0$.  Likewise $|z_{n+1}|>|z_n|$ for each $n\in\mathbb{N}$. I'm trying to think of ways to fix this, so that $|z_{n+1}| \leq |z_n|$ and preferably with equality.  I tried using the second term of the Taylor series so that the approximation didn't merely make a linear approximation, but the same problem persisted. It occurs to me that a better method might be to approximate this not with a polynomial but with a circle.  I recall from calc 3 the osculating circle, but that was for a curve whose parameterization we already knew, so I'm not sure if I can leverage that.  Another thought is that rather than the scheme $$y_{n+1}=y_n+hy'_n$$ which moves from $y_n$ linearly by adding a multiple of $h$, I might want to rotate using complex multiplication, something like $z_{n+1}=z_nr_ne^{i\omega_nh}$ where $r_n\in\mathbb{R}$ would be the radius of the rotation, larger for when larger curvature is appropriate, and $\omega_n$ controlling the speed and direction--again, of course, $h$ serving as the step-size.  I probably would need to do something to deal with the issue of locating the center of the rotation. I know what I've done is not quite right because I'd have to also account for the center of the rotation, but before going down that road I wanted to know if what I'm attempting is even workable.  The big hurdle I can't quite figure out is how I might decide $r_n$ and $\omega_n$ at each stage, using only the derivative information.  If I understand it correctly, $\frac{dz}{dt}$ gives only the linear approximation of the direction of the complex number, like with a vector.","If you solve the equation $$\frac{dz}{dt}=-i\omega z$$ with initial value $z(0)=1$ you get $$z = e^{-i\omega t}$$ which traces a circle in the complex plane with angular speed $-\omega$.  If you try to the Euler method of approximating the solution, it diverges because $$z_1 = z_0 + h(-i\omega)z_0 = (1-ih\omega)z_0$$ and $$|z_1| = |1-ih\omega| = \sqrt{1+(h\omega)^2}$$ for $h,\omega >0$.  Likewise $|z_{n+1}|>|z_n|$ for each $n\in\mathbb{N}$. I'm trying to think of ways to fix this, so that $|z_{n+1}| \leq |z_n|$ and preferably with equality.  I tried using the second term of the Taylor series so that the approximation didn't merely make a linear approximation, but the same problem persisted. It occurs to me that a better method might be to approximate this not with a polynomial but with a circle.  I recall from calc 3 the osculating circle, but that was for a curve whose parameterization we already knew, so I'm not sure if I can leverage that.  Another thought is that rather than the scheme $$y_{n+1}=y_n+hy'_n$$ which moves from $y_n$ linearly by adding a multiple of $h$, I might want to rotate using complex multiplication, something like $z_{n+1}=z_nr_ne^{i\omega_nh}$ where $r_n\in\mathbb{R}$ would be the radius of the rotation, larger for when larger curvature is appropriate, and $\omega_n$ controlling the speed and direction--again, of course, $h$ serving as the step-size.  I probably would need to do something to deal with the issue of locating the center of the rotation. I know what I've done is not quite right because I'd have to also account for the center of the rotation, but before going down that road I wanted to know if what I'm attempting is even workable.  The big hurdle I can't quite figure out is how I might decide $r_n$ and $\omega_n$ at each stage, using only the derivative information.  If I understand it correctly, $\frac{dz}{dt}$ gives only the linear approximation of the direction of the complex number, like with a vector.",,"['ordinary-differential-equations', 'numerical-methods', 'approximation']"
27,PDE with strange Auxiliary Conditions,PDE with strange Auxiliary Conditions,,"I am currently trying to find an $\textit{explicit solution}$ to this question but am getting stuck. So the question reads: Find an explicit solution for the PDE:   $$u_t+u\cdot u_x=0, \>\> u(x,0)=g(x)=\begin{cases} 1 &= \text{ if } x\leq 0\\ 1-x &= \text{ if } 0\leq x \leq 1\\ 0 &= \text{ if } x\geq 1 \end{cases}$$    For all $x$ and $0\leq t\leq 1$. Now, I found the Characteristics: $$\frac{\partial x}{\partial t}=z(t), \>\>\> \frac{\partial z}{\partial t}=0$$ Where $z(t)$ is the term on the RHS of the PDE. Now, setting the initial conditions: $$x(0)=x_0, \>\> z(0)=g(x_0)$$ And the solutions are given by: $$z(t)=g(x_0), \>\>\>\> x(t)=g(x_0)\cdot t +x_0$$ Can I conclude that $u(x,t)=g(x_0)$? This seems a bit too easy, however I know that I can set $u=\frac{x-x_0}{t}$, so would I have to rearrange the solution in terms of $x_0$ for each case?","I am currently trying to find an $\textit{explicit solution}$ to this question but am getting stuck. So the question reads: Find an explicit solution for the PDE:   $$u_t+u\cdot u_x=0, \>\> u(x,0)=g(x)=\begin{cases} 1 &= \text{ if } x\leq 0\\ 1-x &= \text{ if } 0\leq x \leq 1\\ 0 &= \text{ if } x\geq 1 \end{cases}$$    For all $x$ and $0\leq t\leq 1$. Now, I found the Characteristics: $$\frac{\partial x}{\partial t}=z(t), \>\>\> \frac{\partial z}{\partial t}=0$$ Where $z(t)$ is the term on the RHS of the PDE. Now, setting the initial conditions: $$x(0)=x_0, \>\> z(0)=g(x_0)$$ And the solutions are given by: $$z(t)=g(x_0), \>\>\>\> x(t)=g(x_0)\cdot t +x_0$$ Can I conclude that $u(x,t)=g(x_0)$? This seems a bit too easy, however I know that I can set $u=\frac{x-x_0}{t}$, so would I have to rearrange the solution in terms of $x_0$ for each case?",,"['ordinary-differential-equations', 'partial-differential-equations']"
28,Model for spreading of rumors in a class,Model for spreading of rumors in a class,,"We just started learning about differential equations (just first order, ordinary differential equations so far). I have to solve an exercise whose translation is: There is a rumor among the students of this class that this problem   will be on the final exam in June. If there are 70 students enrolled   and the rumor is spreading in proportion to the number of students who   have not yet heard about it, how long will it take for 60 students to  know if by the second day 40 already know about it? Note: it is assumed  that in t = 0 no student knows the rumor. So I've tried to come up with a solution, but I realized that the number of unaware students doesn't come up in it. For starters, I'm working with a function S(t) (number of students who have heard about the rumor depending on how much time, in days, has passed). I followed this logic: The number of students who know about it must be equal to the initial number of aware students (zero) plus $\frac{dS(t)}{dt} t$ (the number of students who find out each day times the days passed). After solving, I get $A(t)=kt$, being $k$ a constant which should equal 20. Did I go wrong somewhere? Thanks in advance!","We just started learning about differential equations (just first order, ordinary differential equations so far). I have to solve an exercise whose translation is: There is a rumor among the students of this class that this problem   will be on the final exam in June. If there are 70 students enrolled   and the rumor is spreading in proportion to the number of students who   have not yet heard about it, how long will it take for 60 students to  know if by the second day 40 already know about it? Note: it is assumed  that in t = 0 no student knows the rumor. So I've tried to come up with a solution, but I realized that the number of unaware students doesn't come up in it. For starters, I'm working with a function S(t) (number of students who have heard about the rumor depending on how much time, in days, has passed). I followed this logic: The number of students who know about it must be equal to the initial number of aware students (zero) plus $\frac{dS(t)}{dt} t$ (the number of students who find out each day times the days passed). After solving, I get $A(t)=kt$, being $k$ a constant which should equal 20. Did I go wrong somewhere? Thanks in advance!",,[]
29,Integral curves,Integral curves,,"Suppose $f$ is a vector field. Let $g$ be an integral curve whose domain contains $[0,\infty)$. Now $\lim (g(t)) =p$ as $t \to \infty$. Then can we say that $f(p)$ is zero?","Suppose $f$ is a vector field. Let $g$ be an integral curve whose domain contains $[0,\infty)$. Now $\lim (g(t)) =p$ as $t \to \infty$. Then can we say that $f(p)$ is zero?",,"['geometry', 'ordinary-differential-equations', 'differential']"
30,Differential equation $\omega'+(1/2)rw=a/r^{n-1}$,Differential equation,\omega'+(1/2)rw=a/r^{n-1},I'm reading about the fundamental solution to heat equation and there's a technical step I don't understand. The text says that if we assume $\omega= \omega(r)$ is a solution to $\omega'+(1/2)rw=a/r^{n-1}$ satisfying $\lim_{r\to\infty} \omega=\lim_{r\to\infty} \omega' =0$ then $a$ must be equal to $0$. I tried to explain it but I don't realize why is it true. Does anyone have a suggestion about how to show it? Thank you.,I'm reading about the fundamental solution to heat equation and there's a technical step I don't understand. The text says that if we assume $\omega= \omega(r)$ is a solution to $\omega'+(1/2)rw=a/r^{n-1}$ satisfying $\lim_{r\to\infty} \omega=\lim_{r\to\infty} \omega' =0$ then $a$ must be equal to $0$. I tried to explain it but I don't realize why is it true. Does anyone have a suggestion about how to show it? Thank you.,,"['calculus', 'ordinary-differential-equations', 'limits']"
31,Euler central differences method,Euler central differences method,,"$$ \frac{\partial y}{\partial t} + c\frac{\partial y}{\partial x} = 0,$$ $$y = cos(x), t=0,$$$$\frac{\partial y}{\partial t}=csin(x), t=0  $$ has a solution $$y=cos(x−ct).$$ I wanted to expand both the derivatives as centered differences. So, in order to expand my derivatives, I did as shown below; $$\frac{y(i+1) - y(i-1)}{2Δt} +c(\frac{y(i+1) - y(i-1)}{2Δx}) = 0$$ $$\frac{y}{Δt} +c(\frac{y}{Δx})= 0$$ Now, I also intend to prove that the algebraic solution is an exact solution of the difference formula if I choose $Δx=cΔt$. How do I achieve this goal?","$$ \frac{\partial y}{\partial t} + c\frac{\partial y}{\partial x} = 0,$$ $$y = cos(x), t=0,$$$$\frac{\partial y}{\partial t}=csin(x), t=0  $$ has a solution $$y=cos(x−ct).$$ I wanted to expand both the derivatives as centered differences. So, in order to expand my derivatives, I did as shown below; $$\frac{y(i+1) - y(i-1)}{2Δt} +c(\frac{y(i+1) - y(i-1)}{2Δx}) = 0$$ $$\frac{y}{Δt} +c(\frac{y}{Δx})= 0$$ Now, I also intend to prove that the algebraic solution is an exact solution of the difference formula if I choose $Δx=cΔt$. How do I achieve this goal?",,"['ordinary-differential-equations', 'numerical-methods']"
32,"A mysterious way for solving ODE of type $P(x,y)dx+Q(x,y)dy=0$ where $P$ and $Q$ are homogenous function of degree $n$",A mysterious way for solving ODE of type  where  and  are homogenous function of degree,"P(x,y)dx+Q(x,y)dy=0 P Q n","Definition: We say $f(x,y)$ is homogenous of degree $n$ if for each $\lambda$, $f(\lambda x, \lambda y)=\lambda^n f(x,y)$ We call a differential equation of form $P(x,y)dx+Q(x,y)dy=0$ homogenous, If $P$ and $Q$ are two homogenous functions of degree $n$. I've read in a book that all of the homogenous ODE's can be solved by these substitutions: $y=vx$ , $dy=vdx+xdv$ Mysteriously, That seems to be working! I have two questions regarding this way of solving homogenous ODE's. $1$) How did they come up with this solution? They could do many other substitutions. Why this one? $2$) Why can we assume that there exists something like $v$ that $y=vx$? Maybe there is not a linear relation between $x$ and $y$! How are we sure? Note: I want to understand this method. That's the point.","Definition: We say $f(x,y)$ is homogenous of degree $n$ if for each $\lambda$, $f(\lambda x, \lambda y)=\lambda^n f(x,y)$ We call a differential equation of form $P(x,y)dx+Q(x,y)dy=0$ homogenous, If $P$ and $Q$ are two homogenous functions of degree $n$. I've read in a book that all of the homogenous ODE's can be solved by these substitutions: $y=vx$ , $dy=vdx+xdv$ Mysteriously, That seems to be working! I have two questions regarding this way of solving homogenous ODE's. $1$) How did they come up with this solution? They could do many other substitutions. Why this one? $2$) Why can we assume that there exists something like $v$ that $y=vx$? Maybe there is not a linear relation between $x$ and $y$! How are we sure? Note: I want to understand this method. That's the point.",,"['ordinary-differential-equations', 'homogeneous-equation']"
33,Trying out Green functions in simple context,Trying out Green functions in simple context,,"I am a physics student, and have had to use Green's function methods prior (in electrodynamics for example), but things were always badly explained. Now I am trying to brush up on things a bit and figured I'd try to solve some very simple ODEs (that can be otherwise integrated). Problem is I am not getting sensible results, and I don't even know how can I incorporate boundary conditions into the problem. Example calculation: Consider the linear ODE $ f'+\alpha f=g $ where $\alpha$ and $g$ are constants. The linear operator is then $$L=\frac{d}{dx}+\alpha,$$ moreover, since this is a translation-invariant operator, we will have $G(x,x')=G(x-x')$. Let's assume initial conditions are given by $f(0)=Q$. Then this equation can be integrated to give $$ f(x)=\left(Q-\frac{g}{\alpha}\right)e^{-\alpha x}+\frac{g}{\alpha}. $$ Now I'll try to solve this ODE using a Green's function method. Let $G(x-x')$ be the Green's function satisfying $$ L_{(x)}G(x-x')=\delta(x-x'), \\ \frac{d}{dx}G(x-x')+\alpha G(x-x')=\delta(x-x'). $$ I have no idea how to proceed, so I'm gonna take a Fourier transform. I have no idea if it is even well defined here or not, but physicists have done worse: $$ G(x-x')=\int G(k)e^{-ik(x-x')}\ dk, \\ \delta(x-x')= \int\frac{1}{2\pi}e^{-ik(x-x')}\ dk.$$ I get the following for Fourier-components: $$ (\alpha-ik)G(k)=\frac{1}{2\pi}, $$ so $$ G(x-x') =\frac{1}{2\pi}\int\frac{1}{\alpha-ik}e^{-ik(x-x')}\ dk.$$ This integral doesn't converge but I don't worry, as $G$ might just be a singular distribution. Instead, we have $$ f(x)=\int G(x-x')g\ dx'=\frac{g}{2\pi}\int dk\frac{1}{\alpha-ik}e^{-ikx}\int dx'e^{ikx'} \\ =g\int dk \frac{1}{\alpha-ik}e^{-ikx}\delta(k)=\frac{g}{\alpha}. $$ This certainly provides a solution of the equation, but not what I want. And I'm not surprised it didn't work, I didn't specify initial/boundary conditions. Probably when I took the Fourier transform, I accidentally gave some bogus boundary conditions, like $f$ should be zero in infinity or something, but I don't know. Question: What went wrong? I assume initial/boundary conditions, but how can I control them? How can I specify what initial/boundary conditions do I want? How do I find Green's function for proper initial/boundary conditions? Was taking the Fourier transform the wrong move there?","I am a physics student, and have had to use Green's function methods prior (in electrodynamics for example), but things were always badly explained. Now I am trying to brush up on things a bit and figured I'd try to solve some very simple ODEs (that can be otherwise integrated). Problem is I am not getting sensible results, and I don't even know how can I incorporate boundary conditions into the problem. Example calculation: Consider the linear ODE $ f'+\alpha f=g $ where $\alpha$ and $g$ are constants. The linear operator is then $$L=\frac{d}{dx}+\alpha,$$ moreover, since this is a translation-invariant operator, we will have $G(x,x')=G(x-x')$. Let's assume initial conditions are given by $f(0)=Q$. Then this equation can be integrated to give $$ f(x)=\left(Q-\frac{g}{\alpha}\right)e^{-\alpha x}+\frac{g}{\alpha}. $$ Now I'll try to solve this ODE using a Green's function method. Let $G(x-x')$ be the Green's function satisfying $$ L_{(x)}G(x-x')=\delta(x-x'), \\ \frac{d}{dx}G(x-x')+\alpha G(x-x')=\delta(x-x'). $$ I have no idea how to proceed, so I'm gonna take a Fourier transform. I have no idea if it is even well defined here or not, but physicists have done worse: $$ G(x-x')=\int G(k)e^{-ik(x-x')}\ dk, \\ \delta(x-x')= \int\frac{1}{2\pi}e^{-ik(x-x')}\ dk.$$ I get the following for Fourier-components: $$ (\alpha-ik)G(k)=\frac{1}{2\pi}, $$ so $$ G(x-x') =\frac{1}{2\pi}\int\frac{1}{\alpha-ik}e^{-ik(x-x')}\ dk.$$ This integral doesn't converge but I don't worry, as $G$ might just be a singular distribution. Instead, we have $$ f(x)=\int G(x-x')g\ dx'=\frac{g}{2\pi}\int dk\frac{1}{\alpha-ik}e^{-ikx}\int dx'e^{ikx'} \\ =g\int dk \frac{1}{\alpha-ik}e^{-ikx}\delta(k)=\frac{g}{\alpha}. $$ This certainly provides a solution of the equation, but not what I want. And I'm not surprised it didn't work, I didn't specify initial/boundary conditions. Probably when I took the Fourier transform, I accidentally gave some bogus boundary conditions, like $f$ should be zero in infinity or something, but I don't know. Question: What went wrong? I assume initial/boundary conditions, but how can I control them? How can I specify what initial/boundary conditions do I want? How do I find Green's function for proper initial/boundary conditions? Was taking the Fourier transform the wrong move there?",,"['ordinary-differential-equations', 'boundary-value-problem', 'greens-function', 'fundamental-solution']"
34,Solving ODE with power series,Solving ODE with power series,,"In some old notes, I found an exercise in which it was asked to solve this ODE, in a neighbourhood of $x_0=1$: $$xy''(x) - 3y(x) = 2x^2$$ I tried to solve it but I'm getting stuck. Let me show you what I tried: $x_0=1$ is an ordinary point the ODE, so the solution should be like $$y(x)=\sum_{n=0}^{+\infty} a_n(x-x_0)^n =\sum_{n=0}^{+\infty} a_n (x-1)^n$$ I computed the derivatives $$y(x)=\sum_{n=0}^{+\infty} a_n (x-1)^n$$ $$y'(x)=\sum_{n=0}^{+\infty} n a_n (x-1)^{n-1}=\sum_{n=1}^{+\infty} n a_n (x-1)^{n-1}$$ $$y''(x)=\sum_{n=0}^{+\infty} n(n-1) a_n (x-1)^{n-2}=\sum_{n=2}^{+\infty} n(n-1) a_n (x-1)^{n-2}$$ And I substituted in the original ODE: $$x \left(\sum_{n=2}^{+\infty} n(n-1) a_n (x-1)^{n-2}\right) -3 \left( \sum_{n=0}^{+\infty} a_n (x-1)^n\right)=0$$ I carried the costants inside the symbols of sum. $$\left(\sum_{n=2}^{+\infty} n(n-1) x a_n (x-1)^{n-2}\right) - \left( \sum_{n=0}^{+\infty} 3a_n (x-1)^n\right)=0$$ The obtained expression has two addenda. In order to keep the same power to each polynomial, I fixed some variables. $$\sum_{n=2}^{+\infty} n(n-1) x a_n (x-1)^{n-2} \implies t:=n-2 \implies \sum_{t=0}^{+\infty} (t+2)(t+1) x a_{t+2} (x-1)^{t} \implies \sum_{n=0}^{+\infty} (n+2)(n+1) x a_{n+2} (x-1)^{n}  $$ $$\sum_{n=0}^{+\infty} 3a_n (x-1)^n \quad \text{no transformation needed}$$ Then I wrote the addenda again: $$\sum_{n=0}^{+\infty} (n+2)(n+1) x a_{n+2} (x-1)^{n} - \sum_{n=0}^{+\infty} 3a_n (x-1)^n = 0$$ Now I checked the bounds of sums, in order to have them common for each sum, and   I noticed that they are already ok. The summation becomes $$\sum_{n=0}^{+\infty} \{(n+2)(n+1) x a_{n+2}  - 3a_n\} (x-1)^n = 0$$ But now, that $x$ in the first addendum is getting me stuck. Can anyone show me how to complete the exercise? Thanks in advance to everyone, even for having read my post.","In some old notes, I found an exercise in which it was asked to solve this ODE, in a neighbourhood of $x_0=1$: $$xy''(x) - 3y(x) = 2x^2$$ I tried to solve it but I'm getting stuck. Let me show you what I tried: $x_0=1$ is an ordinary point the ODE, so the solution should be like $$y(x)=\sum_{n=0}^{+\infty} a_n(x-x_0)^n =\sum_{n=0}^{+\infty} a_n (x-1)^n$$ I computed the derivatives $$y(x)=\sum_{n=0}^{+\infty} a_n (x-1)^n$$ $$y'(x)=\sum_{n=0}^{+\infty} n a_n (x-1)^{n-1}=\sum_{n=1}^{+\infty} n a_n (x-1)^{n-1}$$ $$y''(x)=\sum_{n=0}^{+\infty} n(n-1) a_n (x-1)^{n-2}=\sum_{n=2}^{+\infty} n(n-1) a_n (x-1)^{n-2}$$ And I substituted in the original ODE: $$x \left(\sum_{n=2}^{+\infty} n(n-1) a_n (x-1)^{n-2}\right) -3 \left( \sum_{n=0}^{+\infty} a_n (x-1)^n\right)=0$$ I carried the costants inside the symbols of sum. $$\left(\sum_{n=2}^{+\infty} n(n-1) x a_n (x-1)^{n-2}\right) - \left( \sum_{n=0}^{+\infty} 3a_n (x-1)^n\right)=0$$ The obtained expression has two addenda. In order to keep the same power to each polynomial, I fixed some variables. $$\sum_{n=2}^{+\infty} n(n-1) x a_n (x-1)^{n-2} \implies t:=n-2 \implies \sum_{t=0}^{+\infty} (t+2)(t+1) x a_{t+2} (x-1)^{t} \implies \sum_{n=0}^{+\infty} (n+2)(n+1) x a_{n+2} (x-1)^{n}  $$ $$\sum_{n=0}^{+\infty} 3a_n (x-1)^n \quad \text{no transformation needed}$$ Then I wrote the addenda again: $$\sum_{n=0}^{+\infty} (n+2)(n+1) x a_{n+2} (x-1)^{n} - \sum_{n=0}^{+\infty} 3a_n (x-1)^n = 0$$ Now I checked the bounds of sums, in order to have them common for each sum, and   I noticed that they are already ok. The summation becomes $$\sum_{n=0}^{+\infty} \{(n+2)(n+1) x a_{n+2}  - 3a_n\} (x-1)^n = 0$$ But now, that $x$ in the first addendum is getting me stuck. Can anyone show me how to complete the exercise? Thanks in advance to everyone, even for having read my post.",,"['analysis', 'ordinary-differential-equations', 'summation', 'power-series']"
35,behavior of non-linear differential equation,behavior of non-linear differential equation,,"The non-linear differential equation is defined as following $$\begin{cases}\frac{dx}{dt}=-\frac{1}{t^n}x+\frac{1}{t^m} (t>1)\\ x(1)=x_0 \end{cases}$$ where $n,m$ are positive integers. I want to find all pairs $(n,m)$ such that there exists $\lim_{t\rightarrow \infty}x(t)$. My idea: First I solved the linear differential equation  $$ \frac{dx}{dt}=-\frac{1}{t^n}x $$ If $n=1$, the solution is $x(t)=\frac{A}{t} (A\in \mathbb{R})$ . Regarding $A$ as the function of $t$ I substituted the solution and got $A=-\frac{1}{mt^m}$ and $\lim_{t\rightarrow \infty}x(t)=0$. I am in trouble when $n\geq 2$. Similarly I got  $$ x(t)=A\exp\left(\frac{1}{(n-1)t^{n-1}}\right) $$ as the solution of the above linear equation. Regarding $A$ as the function again, I got  $$ \frac{dA}{dt}=\frac{1}{t^m}\exp\left(-\frac{1}{(n-1)t^{n-1}}\right)$$ But I think this differential equation can't be solved generally. When $\lim_{t\rightarrow \infty}x(t)$ converges? Please give me some advice.","The non-linear differential equation is defined as following $$\begin{cases}\frac{dx}{dt}=-\frac{1}{t^n}x+\frac{1}{t^m} (t>1)\\ x(1)=x_0 \end{cases}$$ where $n,m$ are positive integers. I want to find all pairs $(n,m)$ such that there exists $\lim_{t\rightarrow \infty}x(t)$. My idea: First I solved the linear differential equation  $$ \frac{dx}{dt}=-\frac{1}{t^n}x $$ If $n=1$, the solution is $x(t)=\frac{A}{t} (A\in \mathbb{R})$ . Regarding $A$ as the function of $t$ I substituted the solution and got $A=-\frac{1}{mt^m}$ and $\lim_{t\rightarrow \infty}x(t)=0$. I am in trouble when $n\geq 2$. Similarly I got  $$ x(t)=A\exp\left(\frac{1}{(n-1)t^{n-1}}\right) $$ as the solution of the above linear equation. Regarding $A$ as the function again, I got  $$ \frac{dA}{dt}=\frac{1}{t^m}\exp\left(-\frac{1}{(n-1)t^{n-1}}\right)$$ But I think this differential equation can't be solved generally. When $\lim_{t\rightarrow \infty}x(t)$ converges? Please give me some advice.",,"['real-analysis', 'ordinary-differential-equations']"
36,Do periodic solutions of this system always exist?,Do periodic solutions of this system always exist?,,"Consider the following system of autonomous differential equations on $\mathbb{R}$: $$\frac{d}{dt}x(t)=f(x(t),y(t)) \ \ \ \ \ \text{and} \ \ \ \ \frac{d}{dt}y(t)=g(x(t),y(t)) \ \ \ \ (1)$$ where $f$ and $g$ satisfy local Lipschitz condition on both $x$ and $y$. While I was plotting some graphs modeling some ecological models, such as models of the interaction between populations of predators and preys, I noticed that this type of systems always have a periodic solution , provided it has a global bounded solution. Usually this periodic solution is a steady state (a constant solution). Is there any theorem which states this result? That is, is this theroem correct: Theorem. If $f$ and $g$ are locally Lipschitz with respect to both variables and if $(1)$ has a bounded solution on $t\in[0,\infty)$, then $(1)$  has a periodic solution.","Consider the following system of autonomous differential equations on $\mathbb{R}$: $$\frac{d}{dt}x(t)=f(x(t),y(t)) \ \ \ \ \ \text{and} \ \ \ \ \frac{d}{dt}y(t)=g(x(t),y(t)) \ \ \ \ (1)$$ where $f$ and $g$ satisfy local Lipschitz condition on both $x$ and $y$. While I was plotting some graphs modeling some ecological models, such as models of the interaction between populations of predators and preys, I noticed that this type of systems always have a periodic solution , provided it has a global bounded solution. Usually this periodic solution is a steady state (a constant solution). Is there any theorem which states this result? That is, is this theroem correct: Theorem. If $f$ and $g$ are locally Lipschitz with respect to both variables and if $(1)$ has a bounded solution on $t\in[0,\infty)$, then $(1)$  has a periodic solution.",,"['real-analysis', 'ordinary-differential-equations', 'dynamical-systems', 'periodic-functions']"
37,"Matrix Exponential: Solve $u''+2u'+u=0$ for $u(0)=u_0, u'(0)=u_1$",Matrix Exponential: Solve  for,"u''+2u'+u=0 u(0)=u_0, u'(0)=u_1","I am trying to solve the problem $u''+2u'+u=0$ for $u(0)=u_0, u'(0)=u_1$ using the matrix exponential. I first wrote the linear system as $x'_1=x_2$ and $x'_2=-x_1-2x_2$. I then found $e^{At}= $$ \left[   \begin{array}{ c c }      (t+1)e^{-t} & te^{-t} \\      -te^{-t} & (1-t)e^{-t}   \end{array} \right] $$ $ My book then says that $\vec{x}=e^{A(t-t_0)}x_0$. My question is how do I use this exponential matrix and the initial conditions to find the final answer? Thanks.","I am trying to solve the problem $u''+2u'+u=0$ for $u(0)=u_0, u'(0)=u_1$ using the matrix exponential. I first wrote the linear system as $x'_1=x_2$ and $x'_2=-x_1-2x_2$. I then found $e^{At}= $$ \left[   \begin{array}{ c c }      (t+1)e^{-t} & te^{-t} \\      -te^{-t} & (1-t)e^{-t}   \end{array} \right] $$ $ My book then says that $\vec{x}=e^{A(t-t_0)}x_0$. My question is how do I use this exponential matrix and the initial conditions to find the final answer? Thanks.",,['ordinary-differential-equations']
38,how to solve $y''=-\frac{1}{2y^2}?$ [closed],how to solve  [closed],y''=-\frac{1}{2y^2}?,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question i've been learning special kind of second order non-linear ODE,of the form f(y,y') and f(t,y').I came across a question which I'm not able to reach to the solution.Can someone help me with the method...","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question i've been learning special kind of second order non-linear ODE,of the form f(y,y') and f(t,y').I came across a question which I'm not able to reach to the solution.Can someone help me with the method...",,['ordinary-differential-equations']
39,Problem checking the solution of an ode,Problem checking the solution of an ode,,"I am doing an exercise, which asks to show that given a continuous function $f:[0,\infty)\to \mathbb{R}$, then $$y(x):=\alpha+\int_{0}^x f(t)\sin(x-t) dt$$ is the particular solution of $y''+y=f$ verifying $y(0)=\alpha$ and $y'(0)=0$ What I have It is obvious that $y(0)=\alpha$. Using that $\sin(x-t)=\sin(x)\cos(t)-\cos(x)\sin(t)$, and using differentiation under integral sign, I have $$y'(x)=\cos(x)\int_0^x f(t)\cos(t)dt+\sin(x)\int_0^xf(t)\sin(t)dt$$ and substituting $x=0$ we get $y'(0)=0$. However, here is my problem. When I compute $y''$, and I write $y''+y$, after the cancelations, I get $$y''+y=f+\alpha$$ (That $\alpha$ coming from $y$ doesn't go away). What am I doing wrong here? Thanks!!","I am doing an exercise, which asks to show that given a continuous function $f:[0,\infty)\to \mathbb{R}$, then $$y(x):=\alpha+\int_{0}^x f(t)\sin(x-t) dt$$ is the particular solution of $y''+y=f$ verifying $y(0)=\alpha$ and $y'(0)=0$ What I have It is obvious that $y(0)=\alpha$. Using that $\sin(x-t)=\sin(x)\cos(t)-\cos(x)\sin(t)$, and using differentiation under integral sign, I have $$y'(x)=\cos(x)\int_0^x f(t)\cos(t)dt+\sin(x)\int_0^xf(t)\sin(t)dt$$ and substituting $x=0$ we get $y'(0)=0$. However, here is my problem. When I compute $y''$, and I write $y''+y$, after the cancelations, I get $$y''+y=f+\alpha$$ (That $\alpha$ coming from $y$ doesn't go away). What am I doing wrong here? Thanks!!",,"['real-analysis', 'analysis', 'ordinary-differential-equations', 'definite-integrals']"
40,Why $f_x(x)=\frac1{3x^{2/3}}$ is not continuous at $x=0$?,Why  is not continuous at ?,f_x(x)=\frac1{3x^{2/3}} x=0,"Im reading a book of differential equations and at some point it says that the function defined by $f_x(x)=\frac1{3x^{2/3}}$ is not continuous at $x=0$, but $x=0$ is not a point of the domain of $f_x$ because the function is not defined at it. Then it is correct to say that if a function is not defined at some point then it is not continuous at this point either? Background: the function comes from the initial value problem defined by $$x'=x^{1/3}=:f(x),\quad x(0)=0\tag1$$ From here the book used the partial derivative of $f$ respect to $x$, that is $$f_x(x):=\partial_x f(x)=\frac1{3x^{2/3}}$$ and the fact that $f_x$ is not continuous at $x=0$ is used to show that, from some theorem about uniqueness of solutions, that the solutions to $(1)$ could be not unique, in this case it is shown that there are infinite solutions.","Im reading a book of differential equations and at some point it says that the function defined by $f_x(x)=\frac1{3x^{2/3}}$ is not continuous at $x=0$, but $x=0$ is not a point of the domain of $f_x$ because the function is not defined at it. Then it is correct to say that if a function is not defined at some point then it is not continuous at this point either? Background: the function comes from the initial value problem defined by $$x'=x^{1/3}=:f(x),\quad x(0)=0\tag1$$ From here the book used the partial derivative of $f$ respect to $x$, that is $$f_x(x):=\partial_x f(x)=\frac1{3x^{2/3}}$$ and the fact that $f_x$ is not continuous at $x=0$ is used to show that, from some theorem about uniqueness of solutions, that the solutions to $(1)$ could be not unique, in this case it is shown that there are infinite solutions.",,"['ordinary-differential-equations', 'continuity']"
41,"Separable equation, initial condition","Separable equation, initial condition",,"I am confused about this example that I am trying to understand. We solve the initial value problem $$x' = \frac{te^{x^2}}{x}, \space x(0) = 1,  $$ and we do it ""the usual way"" by writing the equation as $x'xe^{-x^2} = t $ and integrating both sides. We end up with $x^2 = \mathrm{ln}\frac{1}{e^{-1}-t^2} \iff x = \pm \sqrt{\mathrm{ln}\frac{1}{e^{-1}-t^2}}.$ Then the text says that we know, because of the initial condition, which sign to choose in front of the root expression, so it should be $+\sqrt{\mathrm{ln}\frac{1}{e^{-1}-t^2}}.$ That is what confuses me. I have learned that if there is a constant solution $x(t) \equiv k$, then any nonconstant solution cannot cross the line $x=k$. But from what I can gather, there is no constant solution to this equation. Because $x\equiv k \iff \frac{e^{x^2}}{x} = 0,$ which cannot happen for any $x$. Other than that I have no idea what the initial condition could possibly tell me about which sign to choose.","I am confused about this example that I am trying to understand. We solve the initial value problem $$x' = \frac{te^{x^2}}{x}, \space x(0) = 1,  $$ and we do it ""the usual way"" by writing the equation as $x'xe^{-x^2} = t $ and integrating both sides. We end up with $x^2 = \mathrm{ln}\frac{1}{e^{-1}-t^2} \iff x = \pm \sqrt{\mathrm{ln}\frac{1}{e^{-1}-t^2}}.$ Then the text says that we know, because of the initial condition, which sign to choose in front of the root expression, so it should be $+\sqrt{\mathrm{ln}\frac{1}{e^{-1}-t^2}}.$ That is what confuses me. I have learned that if there is a constant solution $x(t) \equiv k$, then any nonconstant solution cannot cross the line $x=k$. But from what I can gather, there is no constant solution to this equation. Because $x\equiv k \iff \frac{e^{x^2}}{x} = 0,$ which cannot happen for any $x$. Other than that I have no idea what the initial condition could possibly tell me about which sign to choose.",,"['ordinary-differential-equations', 'initial-value-problems']"
42,All Solutions to $ \frac{dy}{dx} = (y-1)e^x $,All Solutions to, \frac{dy}{dx} = (y-1)e^x ,$ \frac{dy}{dx} = (y-1)e^x $ a) Find all solutions to the above differential equation b) Find the solution of the differential equation above that satisfies $y(0) = 5 $ c) Find the solution of the differential equation above that satisfies $y(0) =1 $ Here is what I have done: a) $\int \frac{dy}{y-1} = \int e^x dx $ $ln|y-1| = e^x + C$ $y= Ce^{e^x} +1 $ Does the above cover all cases for the absolute value? The division by zero case: When $(y-1) = 0 \implies (y-1)e^x = 0 \implies (y-1) = 0 \implies y=1 $ b) $5= Ce^{e^0} + 1 \implies \frac{4}{e} = C \implies y = \frac{4}{e}e^{e^x} + 1 $ c) $ 1= Ce^{e^0} + 1 \implies C=0 \implies y = 1 $ Does this work look alright?,$ \frac{dy}{dx} = (y-1)e^x $ a) Find all solutions to the above differential equation b) Find the solution of the differential equation above that satisfies $y(0) = 5 $ c) Find the solution of the differential equation above that satisfies $y(0) =1 $ Here is what I have done: a) $\int \frac{dy}{y-1} = \int e^x dx $ $ln|y-1| = e^x + C$ $y= Ce^{e^x} +1 $ Does the above cover all cases for the absolute value? The division by zero case: When $(y-1) = 0 \implies (y-1)e^x = 0 \implies (y-1) = 0 \implies y=1 $ b) $5= Ce^{e^0} + 1 \implies \frac{4}{e} = C \implies y = \frac{4}{e}e^{e^x} + 1 $ c) $ 1= Ce^{e^0} + 1 \implies C=0 \implies y = 1 $ Does this work look alright?,,"['calculus', 'ordinary-differential-equations']"
43,Solve DFE $2y'^2 = 2x^2y' - 3xy$,Solve DFE,2y'^2 = 2x^2y' - 3xy,"I want to solve the DFE $$2y'^2 = 2x^2y' - 3xy.$$ I started with substituting $p = dy/dx$ and obtained $2p^2 = 2x^2p - 3xy$. I isolated $y$ and got $$y = \frac{2}{3} \left[xp - \frac{p^2}{x}\right].$$ Taking the derivative yields: $$y' = p'= \frac{2}{3}\left[p'x + p - \frac{p(2xp' - p)}{x^2}\right].$$ And that's where I draw blank. I tried factorizing, but at no avail. Any ideas?","I want to solve the DFE $$2y'^2 = 2x^2y' - 3xy.$$ I started with substituting $p = dy/dx$ and obtained $2p^2 = 2x^2p - 3xy$. I isolated $y$ and got $$y = \frac{2}{3} \left[xp - \frac{p^2}{x}\right].$$ Taking the derivative yields: $$y' = p'= \frac{2}{3}\left[p'x + p - \frac{p(2xp' - p)}{x^2}\right].$$ And that's where I draw blank. I tried factorizing, but at no avail. Any ideas?",,['ordinary-differential-equations']
44,Problems while solving the differential equation.,Problems while solving the differential equation.,,"$$x^2\frac{d^2y}{dx^2}+x^2\frac{dy}{dx}-2y=0$$ $x=0$ is a regular singular point. $$y=\sum_{n=0}^\infty c_nx^{n+r}$$ $$\frac{dy}{dx}=(n+r)\sum_{n=0}^\infty c_nx^{n+r-1}$$ $$\frac{d^2y}{dx^2}=(n+r)(n+r-1)\sum_{n=0}^\infty c_nx^{n+r-2}$$ $$(n+r)(n+r-1)\sum_{n=0}^\infty c_nx^{n+r}+(n+r-1)\sum_{n=1}^\infty c_{n-1}x^{n+r}-2\sum_{n=0}^\infty c_nx^{n+r}=0$$ Taking out a few terms $$(r)(r-1)c_0x^r+-2c_0x^r+(n+r)(n+r-1)\sum_{n=1}^\infty c_nx^{n+r}+ (n+r-1)\sum_{n=1}^\infty c_{n-1}x^{n+r}-2\sum_{n=1}^\infty c_nx^{n+r}=0$$ The incidal equation is $$r^2-r-2=0$$ The recurrence formula is, $$(n+r)(n+r-1)c_n+(n+r-1)c_{n-1}-2 c_n=0$$ Bigger roots Let $r=r_1=2$ $$(n+2)(n+1)c_n+(n+1)c_{n-1}-2c_n=0$$ $$c_n=\frac{-(n+1)c_{n-1}}{n^2+3n}$$ $$c_1=\frac{-c_0}{2},$$ $$c_2=\frac{-c_{1}}{6}$$ Taking the smaller root, $r=r_2=-1$ $$(n-1)(n-2)c_n+(n-2)c_{n-1}-2c_n=0$$ $$c_n=\frac{(2-n)c_{n-1}}{n^2-3n}$$ How shall I continue this any further? Any help would be appreciated. Can someone hint me on this question.","$$x^2\frac{d^2y}{dx^2}+x^2\frac{dy}{dx}-2y=0$$ $x=0$ is a regular singular point. $$y=\sum_{n=0}^\infty c_nx^{n+r}$$ $$\frac{dy}{dx}=(n+r)\sum_{n=0}^\infty c_nx^{n+r-1}$$ $$\frac{d^2y}{dx^2}=(n+r)(n+r-1)\sum_{n=0}^\infty c_nx^{n+r-2}$$ $$(n+r)(n+r-1)\sum_{n=0}^\infty c_nx^{n+r}+(n+r-1)\sum_{n=1}^\infty c_{n-1}x^{n+r}-2\sum_{n=0}^\infty c_nx^{n+r}=0$$ Taking out a few terms $$(r)(r-1)c_0x^r+-2c_0x^r+(n+r)(n+r-1)\sum_{n=1}^\infty c_nx^{n+r}+ (n+r-1)\sum_{n=1}^\infty c_{n-1}x^{n+r}-2\sum_{n=1}^\infty c_nx^{n+r}=0$$ The incidal equation is $$r^2-r-2=0$$ The recurrence formula is, $$(n+r)(n+r-1)c_n+(n+r-1)c_{n-1}-2 c_n=0$$ Bigger roots Let $r=r_1=2$ $$(n+2)(n+1)c_n+(n+1)c_{n-1}-2c_n=0$$ $$c_n=\frac{-(n+1)c_{n-1}}{n^2+3n}$$ $$c_1=\frac{-c_0}{2},$$ $$c_2=\frac{-c_{1}}{6}$$ Taking the smaller root, $r=r_2=-1$ $$(n-1)(n-2)c_n+(n-2)c_{n-1}-2c_n=0$$ $$c_n=\frac{(2-n)c_{n-1}}{n^2-3n}$$ How shall I continue this any further? Any help would be appreciated. Can someone hint me on this question.",,['ordinary-differential-equations']
45,Should I use the indefinite or the definite integral when working with differential equations?,Should I use the indefinite or the definite integral when working with differential equations?,,"I'm working through Pollard & Tenenbaum's ""Ordinary Differential Equations"". In their treatment of first-order differential equations they write that for an equation of form $$P(x)dx + Q(y)dy = 0$$ its solution is given by $$\int P(x)dx + \int Q(y)dy = c $$ In other words, they take indefinite integrals to obtain the solution. Later in the part on first-order equations, they arrive at exact differential equations, and use definite integrals in their proof of the exact differential equation's solution, to arrive at $$\int_{x_0}^x P(x, y)dx + \int_{y_0}^{y} Q(x_0, y)dy = c$$ as a solution of $$P(x, y)dx + Q(x, y)dy = 0$$ Where $P$ and $Q$ are partial derivatives of a multivariable function $f$. When do I use the definite integral $\int_{x_0}^x f(x)dx$ and when can I use $\int f(x)dx$, taking just the antiderivative and being done with it?","I'm working through Pollard & Tenenbaum's ""Ordinary Differential Equations"". In their treatment of first-order differential equations they write that for an equation of form $$P(x)dx + Q(y)dy = 0$$ its solution is given by $$\int P(x)dx + \int Q(y)dy = c $$ In other words, they take indefinite integrals to obtain the solution. Later in the part on first-order equations, they arrive at exact differential equations, and use definite integrals in their proof of the exact differential equation's solution, to arrive at $$\int_{x_0}^x P(x, y)dx + \int_{y_0}^{y} Q(x_0, y)dy = c$$ as a solution of $$P(x, y)dx + Q(x, y)dy = 0$$ Where $P$ and $Q$ are partial derivatives of a multivariable function $f$. When do I use the definite integral $\int_{x_0}^x f(x)dx$ and when can I use $\int f(x)dx$, taking just the antiderivative and being done with it?",,"['calculus', 'ordinary-differential-equations']"
46,What are the numerical methods for solving matrix differential equations?,What are the numerical methods for solving matrix differential equations?,,Which numerical methods can be used to solve matrix differential equations of the following form? $$\textbf{X}'(t)=\textbf{A}\textbf{X}(t)+\textbf{B}(t)$$ Do you have book or article suggestions about it?,Which numerical methods can be used to solve matrix differential equations of the following form? $$\textbf{X}'(t)=\textbf{A}\textbf{X}(t)+\textbf{B}(t)$$ Do you have book or article suggestions about it?,,"['ordinary-differential-equations', 'numerical-methods', 'matrix-equations']"
47,Finding the Derivative of an Integral,Finding the Derivative of an Integral,,"I am stuck trying to take the derivative of the following function: $$F(t) = \int_{-2t^2}^{t^{1/2}}f(xt^{-1},x)dx$$ I am aware the fundamental theorem of calculus is relevant, but I am not sure with how to deal with a generic function $f(xt^{-1},x)$ like this. Any help would be greatly appreciated!","I am stuck trying to take the derivative of the following function: $$F(t) = \int_{-2t^2}^{t^{1/2}}f(xt^{-1},x)dx$$ I am aware the fundamental theorem of calculus is relevant, but I am not sure with how to deal with a generic function $f(xt^{-1},x)$ like this. Any help would be greatly appreciated!",,"['calculus', 'integration', 'ordinary-differential-equations', 'multivariable-calculus', 'derivatives']"
48,Verification for the solution following differential equation!,Verification for the solution following differential equation!,,"Consider the following second order homogeneous differential equation! $$a_0(x)\frac{d^2y}{dx^2}+a_1(x)\frac{dy}{dx}+a_2(x)y=0$$ Suppose that $a_0,a_1,a_2$ are continuous for all  values of $[a,b]$. Let $f_1$ and  $f_2$ be two distinct solutions to the above differential equation for all $x$ on $a \leq x \leq b$. Further suppose that $f_2(x)\neq 0$. Let $W[f_1(x),f_2(x)]$ be Wronskian of $f_1$ & $f_2$ at $x$. Show that  $$\frac{\text{d}}{\text{d}x}\left[\frac{f_1(x)}{f_2(x)}\right]=-\frac{W[f_1(x),f_2(x)]}{[f_2(x)]^2}$$ My work is as follows $$y=\frac{f_1(x)}{f_2(x)}$$ Using product rule of differentiation we have $$\frac{\text{d}y}{\text{d}x}=\frac{f_2(x)f_1'(x)-f_2'(x)f_1(x)}{[f_2(x)]^2}$$ Know that $$W[f_1(x),f_2(x)]=|\begin{bmatrix}f_1(x) & f_2(x) \\f_1'(x) & f_2'(x) \end{bmatrix}|$$ $$W[f_1(x),f_2(x)]=f_1(x)f_2'(x)-f_1'(x)f_2(x)$$ $$-W[f_1(x),f_2(x)]=-[f_1'(x)f_2(x)-f_1(x)f_2'(x)]$$ $$\frac{\text{d}y}{\text{d}x}=-\frac{W[f_1(x),f_2(x)]}{[f_2(x)]^2}$$ for $x$ on $[a,b]$ Is is that simple? Use result in part 1 to show that if $W[f_1(x),f_2(x)]=0$ then, $f_1$ & $f_2$ are linearly dependent! My attempt Assume that $W[f_1(x),f_2(x)]=0$ $$\frac{\text{d}}{\text{d}x}[\frac{f_1(x)}{f_2(x)}]=-\frac{W[f_1(x),f_2(x)]}{[f_2(x)]^2}$$ $$\int{\text{d}}\left[\frac{f_1(x)}{f_2(x)}\right]=-\int\frac{W[f_1(x),f_2(x)]}{[f_2(x)]^2}{\text{d}x}$$ $$\int{\text{d}}\left[\frac{f_1(x)}{f_2(x)}\right]=-\int0{d}x$$ $$\frac{f_1(x)}{f_2(x)}=c$$ c is an arbitrary constant $${f_1(x)}=c{f_2(x)}$$ $f_1$ & $f_2$ just differ by a constant. Hence they are linearly dependent! Is this correct? Suppose that solution $f_1$ & $f_2$ are linearly independent on x on $a \leq x \leq b$ Hence, $f(x)=\frac{f_1(x)}{f_2(x)}$ and show that f is a monotonic function on $a \leq x \leq b$ I try to argue that The derivative of $f(x)$ is negative therefore monotonic! I am totally unsure for my work. I sincerely hope that someone will provide the rigorous and correct way of doing this question!","Consider the following second order homogeneous differential equation! $$a_0(x)\frac{d^2y}{dx^2}+a_1(x)\frac{dy}{dx}+a_2(x)y=0$$ Suppose that $a_0,a_1,a_2$ are continuous for all  values of $[a,b]$. Let $f_1$ and  $f_2$ be two distinct solutions to the above differential equation for all $x$ on $a \leq x \leq b$. Further suppose that $f_2(x)\neq 0$. Let $W[f_1(x),f_2(x)]$ be Wronskian of $f_1$ & $f_2$ at $x$. Show that  $$\frac{\text{d}}{\text{d}x}\left[\frac{f_1(x)}{f_2(x)}\right]=-\frac{W[f_1(x),f_2(x)]}{[f_2(x)]^2}$$ My work is as follows $$y=\frac{f_1(x)}{f_2(x)}$$ Using product rule of differentiation we have $$\frac{\text{d}y}{\text{d}x}=\frac{f_2(x)f_1'(x)-f_2'(x)f_1(x)}{[f_2(x)]^2}$$ Know that $$W[f_1(x),f_2(x)]=|\begin{bmatrix}f_1(x) & f_2(x) \\f_1'(x) & f_2'(x) \end{bmatrix}|$$ $$W[f_1(x),f_2(x)]=f_1(x)f_2'(x)-f_1'(x)f_2(x)$$ $$-W[f_1(x),f_2(x)]=-[f_1'(x)f_2(x)-f_1(x)f_2'(x)]$$ $$\frac{\text{d}y}{\text{d}x}=-\frac{W[f_1(x),f_2(x)]}{[f_2(x)]^2}$$ for $x$ on $[a,b]$ Is is that simple? Use result in part 1 to show that if $W[f_1(x),f_2(x)]=0$ then, $f_1$ & $f_2$ are linearly dependent! My attempt Assume that $W[f_1(x),f_2(x)]=0$ $$\frac{\text{d}}{\text{d}x}[\frac{f_1(x)}{f_2(x)}]=-\frac{W[f_1(x),f_2(x)]}{[f_2(x)]^2}$$ $$\int{\text{d}}\left[\frac{f_1(x)}{f_2(x)}\right]=-\int\frac{W[f_1(x),f_2(x)]}{[f_2(x)]^2}{\text{d}x}$$ $$\int{\text{d}}\left[\frac{f_1(x)}{f_2(x)}\right]=-\int0{d}x$$ $$\frac{f_1(x)}{f_2(x)}=c$$ c is an arbitrary constant $${f_1(x)}=c{f_2(x)}$$ $f_1$ & $f_2$ just differ by a constant. Hence they are linearly dependent! Is this correct? Suppose that solution $f_1$ & $f_2$ are linearly independent on x on $a \leq x \leq b$ Hence, $f(x)=\frac{f_1(x)}{f_2(x)}$ and show that f is a monotonic function on $a \leq x \leq b$ I try to argue that The derivative of $f(x)$ is negative therefore monotonic! I am totally unsure for my work. I sincerely hope that someone will provide the rigorous and correct way of doing this question!",,['ordinary-differential-equations']
49,Predicting growth with differential equations,Predicting growth with differential equations,,"Look at the differential equation $y'[x] = y[x] (2 + \sin[x]^2 - y[x])$ with $y[0] = 1$. How do you know before you do any plotting that as $x$ advances from $0$, the plot of the solution $y[x]$ goes up near $x$'s for which $2 + \sin[x]^2 > y[x]$ and the plot of the solution $y[x]$ goes down near $x$'s for which $2 + \sin[x]^2 < y[x]$ ? Why do you expect that the crests and dips of the plot of $y[x]$ are located at places where the $y[x]$ plot crosses the plot of $2 + \sin[x]^2$ ? My answer: The plot of $y[x]$ goes up near $x$'s for which $2+\sin[x]^2 > y[x]$ because if $2+\sin[x]^2 > y[x]$ then the expression in the parenthese will evaluate to a positive number times a positive number on the outside which results in a positve $y'[x]$ meaning it is growing. The plot of the solution $y[x]$ goes down near $x$'s for which $2+\sin[x]^2<y[x]$ because then the expression in the parentheses will evaluate to a negative number times a positive number on the outside which results in a negative $y'[x]$ meaning it is decreasing. We know that $y[x]$ cannot be a negative number because $2+\sin[x]^2$ never goes below 2. Meaning that whenever $y[x]$ crosses $2+\sin[x]^2$ it will hit a crest or dip because $2+\sin[x]^2$ is where $y[x]$ is not growing or decreasing. I feel like I am missing the big picture and my answer is wrong here. What is the correct way to approach this problem?","Look at the differential equation $y'[x] = y[x] (2 + \sin[x]^2 - y[x])$ with $y[0] = 1$. How do you know before you do any plotting that as $x$ advances from $0$, the plot of the solution $y[x]$ goes up near $x$'s for which $2 + \sin[x]^2 > y[x]$ and the plot of the solution $y[x]$ goes down near $x$'s for which $2 + \sin[x]^2 < y[x]$ ? Why do you expect that the crests and dips of the plot of $y[x]$ are located at places where the $y[x]$ plot crosses the plot of $2 + \sin[x]^2$ ? My answer: The plot of $y[x]$ goes up near $x$'s for which $2+\sin[x]^2 > y[x]$ because if $2+\sin[x]^2 > y[x]$ then the expression in the parenthese will evaluate to a positive number times a positive number on the outside which results in a positve $y'[x]$ meaning it is growing. The plot of the solution $y[x]$ goes down near $x$'s for which $2+\sin[x]^2<y[x]$ because then the expression in the parentheses will evaluate to a negative number times a positive number on the outside which results in a negative $y'[x]$ meaning it is decreasing. We know that $y[x]$ cannot be a negative number because $2+\sin[x]^2$ never goes below 2. Meaning that whenever $y[x]$ crosses $2+\sin[x]^2$ it will hit a crest or dip because $2+\sin[x]^2$ is where $y[x]$ is not growing or decreasing. I feel like I am missing the big picture and my answer is wrong here. What is the correct way to approach this problem?",,"['calculus', 'ordinary-differential-equations', 'derivatives']"
50,Exercise 1.12 in Tao's nonlinear dispersive equation,Exercise 1.12 in Tao's nonlinear dispersive equation,,Suppose $F$ is locally Lipschitz and has at most $x\log(x)$ growth i.e. $$\|F(u)\|_D\leq (1+\|u\|_D)\log(2+\|u\|_D).$$ Does solution to the Cauchy problem of ODE $$\partial_tu(t)=F(u)$$ (with some initial datum) exist classically for all time or is it possible to blow up?,Suppose $F$ is locally Lipschitz and has at most $x\log(x)$ growth i.e. $$\|F(u)\|_D\leq (1+\|u\|_D)\log(2+\|u\|_D).$$ Does solution to the Cauchy problem of ODE $$\partial_tu(t)=F(u)$$ (with some initial datum) exist classically for all time or is it possible to blow up?,,['ordinary-differential-equations']
51,Laplace transform of $x^{3/2}$,Laplace transform of,x^{3/2},"Solve the Laplace transform of $x^{3/2}$ $L[x^{3/2}]=L[x (x^{1/2})]=-\frac{d}{dp}L[x^{1/2}]=-\frac{d}{dp}L[x(x^{-1/2})]=-\frac{d}{dp}(-\frac{d}{dp})\sqrt{\frac{\pi}{p}}=\frac{1}{2p}\frac{1}{2p}\sqrt{\frac{\pi}{p}}=\frac{1}{4p^2}\sqrt{\frac{\pi}{p}}$ The correct answer is $\frac{3}{4p^2}\sqrt{\frac{\pi}{p}}$, could someone tell me where I missed the $3$ please?","Solve the Laplace transform of $x^{3/2}$ $L[x^{3/2}]=L[x (x^{1/2})]=-\frac{d}{dp}L[x^{1/2}]=-\frac{d}{dp}L[x(x^{-1/2})]=-\frac{d}{dp}(-\frac{d}{dp})\sqrt{\frac{\pi}{p}}=\frac{1}{2p}\frac{1}{2p}\sqrt{\frac{\pi}{p}}=\frac{1}{4p^2}\sqrt{\frac{\pi}{p}}$ The correct answer is $\frac{3}{4p^2}\sqrt{\frac{\pi}{p}}$, could someone tell me where I missed the $3$ please?",,['ordinary-differential-equations']
52,The method of undetermined coefficients: clarification,The method of undetermined coefficients: clarification,,"Please help me figure out the right side of the equation.  Find the general solution of the following equations. $$y''-2y'+2y=e^x\sin x$$ . I know that the general solution is $$y=e^x(c_1\cos x+c_2\sin x)$$ But I am confused about the particular solution due to the combination of the exponential and sine terms. I know that individually, $e^x$ will yield $y_p=Ae^{ax}$ and $\sin x$ will yield $y_p=A\sin bx + B \cos bx$ but I don't know how to combine these two. Also, if the combination of these two is the product of their particular solutions, then we fact another problem. Since we can't simply do $$y_p=Ae^{ax}(A\sin bx+B \cos bx)$$ since this looks like our general solution, perhaps we need to multiply this expression by an x, which will yield $$y_p=(x)Ae^{ax}(A\sin bx+B \cos bx)$$. Please let me know if I am on the correct track and help me finish the problem. I know that the final answer is $$y_p=-\frac{1}{2}xe^x\cos x$$ but I am not able to obtain it.","Please help me figure out the right side of the equation.  Find the general solution of the following equations. $$y''-2y'+2y=e^x\sin x$$ . I know that the general solution is $$y=e^x(c_1\cos x+c_2\sin x)$$ But I am confused about the particular solution due to the combination of the exponential and sine terms. I know that individually, $e^x$ will yield $y_p=Ae^{ax}$ and $\sin x$ will yield $y_p=A\sin bx + B \cos bx$ but I don't know how to combine these two. Also, if the combination of these two is the product of their particular solutions, then we fact another problem. Since we can't simply do $$y_p=Ae^{ax}(A\sin bx+B \cos bx)$$ since this looks like our general solution, perhaps we need to multiply this expression by an x, which will yield $$y_p=(x)Ae^{ax}(A\sin bx+B \cos bx)$$. Please let me know if I am on the correct track and help me finish the problem. I know that the final answer is $$y_p=-\frac{1}{2}xe^x\cos x$$ but I am not able to obtain it.",,['ordinary-differential-equations']
53,How do I solve the Riccati Differential Equation with fsolve in MATLAB / Octave?,How do I solve the Riccati Differential Equation with fsolve in MATLAB / Octave?,,"Let's say that I have this equation. This is the Riccati Differential Equation: $$P A + A^T P - P B R^{-1} B^T P + Q  = 0$$ I know $ A, B, Q, R$. My goal is to find $P$. How do I use the MATLAB / Octave command fsolve to find $P$ ? EDIT: $$ $$ Here is the answer - An example to get LQR matrix. >> fun = @(P, A, B, Q, R) P*A+A'*P-P*B*inv(R)*B'*P + Q fun =  @(P, A, B, Q, R) P * A + A' * P - P * B * inv (R) * B' * P + Q >> p0 = 0.1*ones(2) p0 =     0.10000   0.10000    0.10000   0.10000  >> P = fsolve(@(P) fun(P, Amat, Bmat, Q, R), p0) P =     55.67637    0.57241     0.57241    2.29935  >>  >> inv(R)*Bmat'*P ans =     2.2269e-02   2.2895e-04   -1.4350e-03  -5.7645e-03  >> lqr(Amat, Bmat, Q, R) ans =     2.2269e-02   2.2899e-04   -1.4353e-03  -5.7644e-03  >>","Let's say that I have this equation. This is the Riccati Differential Equation: $$P A + A^T P - P B R^{-1} B^T P + Q  = 0$$ I know $ A, B, Q, R$. My goal is to find $P$. How do I use the MATLAB / Octave command fsolve to find $P$ ? EDIT: $$ $$ Here is the answer - An example to get LQR matrix. >> fun = @(P, A, B, Q, R) P*A+A'*P-P*B*inv(R)*B'*P + Q fun =  @(P, A, B, Q, R) P * A + A' * P - P * B * inv (R) * B' * P + Q >> p0 = 0.1*ones(2) p0 =     0.10000   0.10000    0.10000   0.10000  >> P = fsolve(@(P) fun(P, Amat, Bmat, Q, R), p0) P =     55.67637    0.57241     0.57241    2.29935  >>  >> inv(R)*Bmat'*P ans =     2.2269e-02   2.2895e-04   -1.4350e-03  -5.7645e-03  >> lqr(Amat, Bmat, Q, R) ans =     2.2269e-02   2.2899e-04   -1.4353e-03  -5.7644e-03  >>",,"['matrices', 'ordinary-differential-equations', 'matlab', 'nonlinear-system', 'optimal-control']"
54,General solution for an improper node (ODE).,General solution for an improper node (ODE).,,"This might be a really simple question. Define $A=\begin{bmatrix}a&1\\0&a\end{bmatrix}$, $a\neq 0$; the eigenvalue $a$ has only one real associated eigenvector $v$ and then $\phi_1(t)=e^{at}v$ is a solution of the ODE $$\frac{dx}{dt}=Ax,$$ but in order to find the general solution, I need one more linearly independent solution. Is there a ""satisfactory"" way to obtain it? To explain the word ""satisfactory"", I've read somewhere that if $w\in \mathbb{R^2}$ is linearly independent of $v$, then $\phi_2(t):=e^{at}(w+tv)$ is the solution I seek, but the author didn't give further explanation (probably because anyone should be able to figure out where this came from) and I got confused. Writing $Aw=v+\beta w$, the matrix of $x\rightarrow Ax$ in the base $\{v,w\}$ is $\begin{bmatrix}a&1\\0&\beta\end{bmatrix}$, but since the eigenvalues are invariant on representation, $\beta=a$ and $Aw=v+aw$, then $$\frac{d}{dt} \phi_2=ae^ {at}(w+tv)+e^{at}v=e^{at}(atv+aw+v)=A(w+tv)=A\phi_2$$ that is, $\phi_2(t)$ is indeed a solution and there's something very clever about it that makes me think it makes sense, but it is still not a solution I could get on my own. Can anyone explain the meaning of this choice? Thanks in advance!","This might be a really simple question. Define $A=\begin{bmatrix}a&1\\0&a\end{bmatrix}$, $a\neq 0$; the eigenvalue $a$ has only one real associated eigenvector $v$ and then $\phi_1(t)=e^{at}v$ is a solution of the ODE $$\frac{dx}{dt}=Ax,$$ but in order to find the general solution, I need one more linearly independent solution. Is there a ""satisfactory"" way to obtain it? To explain the word ""satisfactory"", I've read somewhere that if $w\in \mathbb{R^2}$ is linearly independent of $v$, then $\phi_2(t):=e^{at}(w+tv)$ is the solution I seek, but the author didn't give further explanation (probably because anyone should be able to figure out where this came from) and I got confused. Writing $Aw=v+\beta w$, the matrix of $x\rightarrow Ax$ in the base $\{v,w\}$ is $\begin{bmatrix}a&1\\0&\beta\end{bmatrix}$, but since the eigenvalues are invariant on representation, $\beta=a$ and $Aw=v+aw$, then $$\frac{d}{dt} \phi_2=ae^ {at}(w+tv)+e^{at}v=e^{at}(atv+aw+v)=A(w+tv)=A\phi_2$$ that is, $\phi_2(t)$ is indeed a solution and there's something very clever about it that makes me think it makes sense, but it is still not a solution I could get on my own. Can anyone explain the meaning of this choice? Thanks in advance!",,"['ordinary-differential-equations', 'proof-verification']"
55,Solving $y''-\frac{1}{x \ln x}y'=12x^2\ln x$,Solving,y''-\frac{1}{x \ln x}y'=12x^2\ln x,"Could someone help me solve this differential equation? $$y''-\frac{1}{x\ln x}y'=12x^2\ln x$$ I tried doing $y'=z$ and that leads me to $$z'-\frac{1}{x\ln x}z=12x^2\ln x$$ and then I do $z=t\cdot x\:\:and\:z'=t'x+t$ which will lead me to $$t'x+t=12x^2\ln x+\frac{1}{x\ln x}\cdot t\cdot x$$ but trying to solve this(by parts) yields me $$t=x^{6x^2}-e^{3x^2}+\ln x+\frac{1}{x}$$ and to find $y$ I will have to integrate a pretty nasty equation, which leads me to believe I'm doing something wrong. I'm pretty sure I need to use substitution to get a first order linear equation but perhaps I am doing something wrong? Could someone take a look at this and point me in the right direction?","Could someone help me solve this differential equation? $$y''-\frac{1}{x\ln x}y'=12x^2\ln x$$ I tried doing $y'=z$ and that leads me to $$z'-\frac{1}{x\ln x}z=12x^2\ln x$$ and then I do $z=t\cdot x\:\:and\:z'=t'x+t$ which will lead me to $$t'x+t=12x^2\ln x+\frac{1}{x\ln x}\cdot t\cdot x$$ but trying to solve this(by parts) yields me $$t=x^{6x^2}-e^{3x^2}+\ln x+\frac{1}{x}$$ and to find $y$ I will have to integrate a pretty nasty equation, which leads me to believe I'm doing something wrong. I'm pretty sure I need to use substitution to get a first order linear equation but perhaps I am doing something wrong? Could someone take a look at this and point me in the right direction?",,['ordinary-differential-equations']
56,Asymptotic behavior of the solution,Asymptotic behavior of the solution,,"I just started research in physics, but along the way, I want to show something like $$\partial_x \beta(x) \sim -\frac{A}{x^2}[\alpha(x)+\beta(x)]\,\, as\,\, x\rightarrow\infty$$ where $\alpha(x)$ and $\beta(x)$ are non-negative monotonically decreasing much faster than $\frac{1}{x^n}$ (something like exponential decay or Gaussian) with a constant $A$. After doing method of integrating factor, I am putting a boundary condition that $\beta(x)$ at infinity is equal to 0. Then I tried to bound the integral or show some asymptotic behavior, but it seems like my math is not sufficient enough. Anyway, my goal is to show $\alpha$ is subdominant to $\beta$ at infinity or vice versa, how should I approach these kind of problem without losing generality (not assuming they are decaying like exp or gaus). Any suggestion for how to show asymptotic behavior of beta would be appreciated! thank you","I just started research in physics, but along the way, I want to show something like $$\partial_x \beta(x) \sim -\frac{A}{x^2}[\alpha(x)+\beta(x)]\,\, as\,\, x\rightarrow\infty$$ where $\alpha(x)$ and $\beta(x)$ are non-negative monotonically decreasing much faster than $\frac{1}{x^n}$ (something like exponential decay or Gaussian) with a constant $A$. After doing method of integrating factor, I am putting a boundary condition that $\beta(x)$ at infinity is equal to 0. Then I tried to bound the integral or show some asymptotic behavior, but it seems like my math is not sufficient enough. Anyway, my goal is to show $\alpha$ is subdominant to $\beta$ at infinity or vice versa, how should I approach these kind of problem without losing generality (not assuming they are decaying like exp or gaus). Any suggestion for how to show asymptotic behavior of beta would be appreciated! thank you",,"['calculus', 'ordinary-differential-equations', 'asymptotics']"
57,Change of variables for a linear ODE system,Change of variables for a linear ODE system,,"Given the solution $\vec x$ to a system of $n$ linear equations $$\dot x_i=f_i(\vec x)$$ for $i=1,\dots,n$, where each $f_i$ is known to be linear in $\vec x$ but is otherwise not known, is there a simple way to obtain the solution $\vec y$ to the system of $n$ linear equations $$\dot y_i=f_i(\vec y)+r_i y_i$$ in terms of $\vec x$ but not $f$, presumably via some change of variables with exponential scaling?","Given the solution $\vec x$ to a system of $n$ linear equations $$\dot x_i=f_i(\vec x)$$ for $i=1,\dots,n$, where each $f_i$ is known to be linear in $\vec x$ but is otherwise not known, is there a simple way to obtain the solution $\vec y$ to the system of $n$ linear equations $$\dot y_i=f_i(\vec y)+r_i y_i$$ in terms of $\vec x$ but not $f$, presumably via some change of variables with exponential scaling?",,"['linear-algebra', 'ordinary-differential-equations', 'systems-of-equations', 'dynamical-systems', 'change-of-variable']"
58,"For $X_t = t^aP(\frac{B_t}{t^b})$ to be martingale, find constants a,b and a differential equation satistfied by $P(\frac{B_t}{t^b})$","For  to be martingale, find constants a,b and a differential equation satistfied by",X_t = t^aP(\frac{B_t}{t^b}) P(\frac{B_t}{t^b}),"Here $B_t$ is Brownian motion and $P(x)$ is a polynomial of degree $n, n \ge 1$ Assuming $f(t,B_t) = X_t$, I used Ito lemma and used $f_t + f_{xx} = 0$ to equate the drift part of the SDE to $0$. So I get something like this: $$ \frac{a}{t}P(\frac{B_t}{t^b}) - \frac{b}{t^{b+1}}P'(\frac{B_t}{t^b}) + \frac{1}{t^{2b}}P''(\frac{B_t}{t^b}) = 0 $$ because $$ f_t = at^{a-1}P(\frac{B_t}{t^b}) - b\frac{t^a}{t^{b+1}}P'(\frac{B_t}{t^b})$$ and $$f_{xx} = \frac{t^a}{ t^{2b}}P''(\frac{B_t}{t^b}) $$ What should a and b be from this equation so that $X_t = t^a P(\dfrac{B_t}{t^b})$ is a martingale? Earlier I tried using the general form of a polynomial like this $P(x) = c_n x^n + c_{n-1} x^{n-1} + \dots c_1 x + c_0$ and tried differentiating that. But it gave me extremely complicated terms when I differentiate it twice. Edit: I further simpified the DE as follows: $$at^{2b -1}P(\frac{B_t}{t^b}) - b t^{b-1}P'(\frac{B_t}{t^b}) + P''(\frac{B_t}{t^b}) = 0  $$ A trivial solution would be that the polynomial $P$ is identically zero. Another solution is that $a=0, b=1$ along with $P' = 0$ Is that all? The second part of the question asks what $X_t$ looks like for $n = 2,3,4$. Shouldn't $n$ be equal to $1$ for $P' = 0$? Guys, anyone? I am probably doing this wrong but can't figure out what exactly.","Here $B_t$ is Brownian motion and $P(x)$ is a polynomial of degree $n, n \ge 1$ Assuming $f(t,B_t) = X_t$, I used Ito lemma and used $f_t + f_{xx} = 0$ to equate the drift part of the SDE to $0$. So I get something like this: $$ \frac{a}{t}P(\frac{B_t}{t^b}) - \frac{b}{t^{b+1}}P'(\frac{B_t}{t^b}) + \frac{1}{t^{2b}}P''(\frac{B_t}{t^b}) = 0 $$ because $$ f_t = at^{a-1}P(\frac{B_t}{t^b}) - b\frac{t^a}{t^{b+1}}P'(\frac{B_t}{t^b})$$ and $$f_{xx} = \frac{t^a}{ t^{2b}}P''(\frac{B_t}{t^b}) $$ What should a and b be from this equation so that $X_t = t^a P(\dfrac{B_t}{t^b})$ is a martingale? Earlier I tried using the general form of a polynomial like this $P(x) = c_n x^n + c_{n-1} x^{n-1} + \dots c_1 x + c_0$ and tried differentiating that. But it gave me extremely complicated terms when I differentiate it twice. Edit: I further simpified the DE as follows: $$at^{2b -1}P(\frac{B_t}{t^b}) - b t^{b-1}P'(\frac{B_t}{t^b}) + P''(\frac{B_t}{t^b}) = 0  $$ A trivial solution would be that the polynomial $P$ is identically zero. Another solution is that $a=0, b=1$ along with $P' = 0$ Is that all? The second part of the question asks what $X_t$ looks like for $n = 2,3,4$. Shouldn't $n$ be equal to $1$ for $P' = 0$? Guys, anyone? I am probably doing this wrong but can't figure out what exactly.",,"['probability', 'ordinary-differential-equations', 'stochastic-processes', 'stochastic-calculus', 'stochastic-integrals']"
59,Impossible Kinds Of Differential Equations,Impossible Kinds Of Differential Equations,,"Today my professor told me that there are some differential equations that cannot be solved. Is this true? If it is true, why can they not be solved? How complex would that kind of differential equation have to be?","Today my professor told me that there are some differential equations that cannot be solved. Is this true? If it is true, why can they not be solved? How complex would that kind of differential equation have to be?",,['ordinary-differential-equations']
60,Does the solution of differential equations exist for all times?,Does the solution of differential equations exist for all times?,,"I was trying to analyse the differential equation $\frac{dx}{dt} = -x + x^{3} , x(0) = x_{0}$,After solving I got the solution as $- \ln(|x|) + \frac{1}{2}\ln|x^2 - 1 | = t + (-\ln|x_{0}| + \frac{1}{2}\ln|x_{0}^2 - 1|)$,now by doing computational analysis I see that solution $x(t) $ diverges to infinity when the initial condition is $>1$ or $<1$ and the solution decreases and attains a steady stste when $x_0{ \in [-1,1]}$. But how do I prove analytically that when $x_{0} > 1$,$x_{0}<-1$,the solution diverges to infinity! and attains a steady state when $-1<x(0)<1$? And I think for negative time $t$,the solution exist for finitely many $t$,how can i intuitively think of this? Any help is great!","I was trying to analyse the differential equation $\frac{dx}{dt} = -x + x^{3} , x(0) = x_{0}$,After solving I got the solution as $- \ln(|x|) + \frac{1}{2}\ln|x^2 - 1 | = t + (-\ln|x_{0}| + \frac{1}{2}\ln|x_{0}^2 - 1|)$,now by doing computational analysis I see that solution $x(t) $ diverges to infinity when the initial condition is $>1$ or $<1$ and the solution decreases and attains a steady stste when $x_0{ \in [-1,1]}$. But how do I prove analytically that when $x_{0} > 1$,$x_{0}<-1$,the solution diverges to infinity! and attains a steady state when $-1<x(0)<1$? And I think for negative time $t$,the solution exist for finitely many $t$,how can i intuitively think of this? Any help is great!",,"['ordinary-differential-equations', 'dynamical-systems']"
61,Equicontinuity in Peano existence theorem,Equicontinuity in Peano existence theorem,,"I'm having some trouble understanding why the sequence of successive approximations in the proof of the Peano existence theorem is equicontinuous (which is needed to use Arzela-Ascoli theorem): Here's the theorem and proof as it goes in my textbook (and afterwards I'll lay out the part that I have the problem with): Peano's existence theorem: Let $f:[x_{0},x_{0}+a] \times\mathbb{R}\to\mathbb{R}$ be a continuous function. Then the equation $$y' = f(x,y)$$ has a solution which satisfies the initial condition $y(x_{0})=y_{0}.$ Proof: Let $n \in \mathbb{N}$ and $x_{j}=x_{0}+\frac{aj}{n}$ for $j \in \{0, ..., n\}$ . Let $y_{n}:[x_{0}, x_{0}+a] \to \mathbb{R}$ be a function defined by $y_{n}(x_{0})=y_{0}$ and $$y_{n}'(x) = f(x, y_{n}(x_{j})) \quad \text{for} \quad x_{j}<x\leq x_{j+1},$$ where $y_{n}(x_{j})$ is taken so that $y_{n}$ is continuous. $y_{n}$ satisfies the equation $$y_{n}(x) = y_{0}+ \int_{x_{0}}^{x} y_{n}'(t)dt = y_{0}+\int_{x_{0}}^{x} (f(t, y_{n}(t))+h_{n}(t))dt,$$ where $h_{n}(t)=f(t, y_{n}(x_{j}))-f(t, y_{n}(t))$ , where $x_{j}<t\leq x_{j+1}$ . Trivially, for large enough $n$ , we can approximate $t\in [x_{0}, x_{0}+a]$ as well as we want, and since $y_{n}$ is continuous, we have $||h_{n}||_{\infty} \to 0$ as $n \to \infty$ . Since $y_{n}$ is continuous on a compact interval, it's also bounded, so $||y_{n}||_{\infty}<b_{n}$ for some $b_{n} \in \mathbb{R}$ . Now, from $|y_{n}(u)-y_{n}(v)| \leq \int_{u}^{v} (|f(t, y_{n}(t))|+|h_{n}(t)|)dt<M|u-v|$ for some $M$ (since $f$ is continuous on the compact interval $[x_{0}, x_{0}+a] \times [-b_{n}, b_{n}]$ , it reaches its maximum and minimum value, and since $||h_{n}||_{\infty} \to 0$ as $n \to \infty$ , it's also bounded), I'm supposed to deduce a ""common Lipschitz constant $M$ "" for all $y_{n}$ , and deduce equicontinuity from there, but $b_{n}$ can go to infinity as $n \to \infty$ , and so $M$ depends on $n$ . Is the proof wrong? If so, how can I rectify this error? Or is there something I'm missing? I can't find a more detailed proof of the theorem in any other textbook I've found.","I'm having some trouble understanding why the sequence of successive approximations in the proof of the Peano existence theorem is equicontinuous (which is needed to use Arzela-Ascoli theorem): Here's the theorem and proof as it goes in my textbook (and afterwards I'll lay out the part that I have the problem with): Peano's existence theorem: Let be a continuous function. Then the equation has a solution which satisfies the initial condition Proof: Let and for . Let be a function defined by and where is taken so that is continuous. satisfies the equation where , where . Trivially, for large enough , we can approximate as well as we want, and since is continuous, we have as . Since is continuous on a compact interval, it's also bounded, so for some . Now, from for some (since is continuous on the compact interval , it reaches its maximum and minimum value, and since as , it's also bounded), I'm supposed to deduce a ""common Lipschitz constant "" for all , and deduce equicontinuity from there, but can go to infinity as , and so depends on . Is the proof wrong? If so, how can I rectify this error? Or is there something I'm missing? I can't find a more detailed proof of the theorem in any other textbook I've found.","f:[x_{0},x_{0}+a] \times\mathbb{R}\to\mathbb{R} y' = f(x,y) y(x_{0})=y_{0}. n \in \mathbb{N} x_{j}=x_{0}+\frac{aj}{n} j \in \{0, ..., n\} y_{n}:[x_{0}, x_{0}+a] \to \mathbb{R} y_{n}(x_{0})=y_{0} y_{n}'(x) = f(x, y_{n}(x_{j})) \quad \text{for} \quad x_{j}<x\leq x_{j+1}, y_{n}(x_{j}) y_{n} y_{n} y_{n}(x) = y_{0}+ \int_{x_{0}}^{x} y_{n}'(t)dt = y_{0}+\int_{x_{0}}^{x} (f(t, y_{n}(t))+h_{n}(t))dt, h_{n}(t)=f(t, y_{n}(x_{j}))-f(t, y_{n}(t)) x_{j}<t\leq x_{j+1} n t\in [x_{0}, x_{0}+a] y_{n} ||h_{n}||_{\infty} \to 0 n \to \infty y_{n} ||y_{n}||_{\infty}<b_{n} b_{n} \in \mathbb{R} |y_{n}(u)-y_{n}(v)| \leq \int_{u}^{v} (|f(t, y_{n}(t))|+|h_{n}(t)|)dt<M|u-v| M f [x_{0}, x_{0}+a] \times [-b_{n}, b_{n}] ||h_{n}||_{\infty} \to 0 n \to \infty M y_{n} b_{n} n \to \infty M n","['ordinary-differential-equations', 'proof-explanation']"
62,Solving differential equations by parametrization,Solving differential equations by parametrization,,In my book they solve the following two first-order differential equations by parametrization. 1) $x^4=y'^3-x^2y'$ 2) $y=y'^2+2lny'$ Notice that there is no y in the first equation and no x in the second equation. That's the signal that tells me to use parametrisation. Now for the first one they use $t=\frac{y'}{x}$ and for the second one they use $ t = y' $. How do I know which parametrization I should use? Thanks in advance!,In my book they solve the following two first-order differential equations by parametrization. 1) $x^4=y'^3-x^2y'$ 2) $y=y'^2+2lny'$ Notice that there is no y in the first equation and no x in the second equation. That's the signal that tells me to use parametrisation. Now for the first one they use $t=\frac{y'}{x}$ and for the second one they use $ t = y' $. How do I know which parametrization I should use? Thanks in advance!,,['ordinary-differential-equations']
63,"Solve the initial value problem $1+y\sinh(x)+(1+\cosh(x))y'=0,y(0)=\frac{1}{2}$",Solve the initial value problem,"1+y\sinh(x)+(1+\cosh(x))y'=0,y(0)=\frac{1}{2}","The question asks to solve the initial value problem: $$1+y\sinh(x)+(1+\cosh(x))y'=0,y(0)=\frac{1}{2}$$ I have tried substituting $1=\frac{\sinh(x)}{\sinh(x)}$ and $1=\frac{\cosh(x)}{\cosh(x)}$ but can't seem to find a workable form to start finding $y_h$","The question asks to solve the initial value problem: $$1+y\sinh(x)+(1+\cosh(x))y'=0,y(0)=\frac{1}{2}$$ I have tried substituting $1=\frac{\sinh(x)}{\sinh(x)}$ and $1=\frac{\cosh(x)}{\cosh(x)}$ but can't seem to find a workable form to start finding $y_h$",,"['ordinary-differential-equations', 'hyperbolic-functions', 'initial-value-problems']"
64,Finding solutions of $Ax(t)= x'(t)$ by linear algebra,Finding solutions of  by linear algebra,Ax(t)= x'(t),"Consider a system of $n$ first-order homogenous differential equations with real coefficients. We can write solutions in vector form: $Ax(t)= x'(t)$ where $A$ is the $n \times n$ coefficient matrix. Suppose $\lambda$ is an eigenvalue of $A$ with $\dim E_\lambda =1$ and the algebraic multiplicity of $\lambda$ equal to $2$. Let's guess that $y(t) =te^{\lambda t}u +e^{\lambda t}v$ is a solution to the system. I am asked to show that $u$ is an eigenvector of $A$ corresponding to $\lambda$ and that $v$ satisfies $(A-\lambda I)v=u$. Then I am asked to show that it's possible to solve for $v$. I know that $\dim G_\lambda =2$, where $G_\lambda$ is the generalized eigenspace of $A$ corresponding to $\lambda$. But I really have no idea how to proceed. (Even the wording of the problem confuses me.) I would appreciate some help understanding how to prove what I want.","Consider a system of $n$ first-order homogenous differential equations with real coefficients. We can write solutions in vector form: $Ax(t)= x'(t)$ where $A$ is the $n \times n$ coefficient matrix. Suppose $\lambda$ is an eigenvalue of $A$ with $\dim E_\lambda =1$ and the algebraic multiplicity of $\lambda$ equal to $2$. Let's guess that $y(t) =te^{\lambda t}u +e^{\lambda t}v$ is a solution to the system. I am asked to show that $u$ is an eigenvector of $A$ corresponding to $\lambda$ and that $v$ satisfies $(A-\lambda I)v=u$. Then I am asked to show that it's possible to solve for $v$. I know that $\dim G_\lambda =2$, where $G_\lambda$ is the generalized eigenspace of $A$ corresponding to $\lambda$. But I really have no idea how to proceed. (Even the wording of the problem confuses me.) I would appreciate some help understanding how to prove what I want.",,"['linear-algebra', 'ordinary-differential-equations']"
65,Reducing system of differential equations to first order.,Reducing system of differential equations to first order.,,"I have to solve following system of equations: $$ x''=-x-z+e^{-t},z'=-2x-2z+3e^{-t} $$ I would like to have formula for $x'$ in order to find fundamental matrix and so on. So from above quations I obtain $z'-2x''=e^{-t}$, integrate sides and have $2x'=z+e^{-t}$. So I have system of two first-order differential equations $$2x'=z+e^{-t}, z'=-2x-2z+3e^{-t}$$ Is it correct?","I have to solve following system of equations: $$ x''=-x-z+e^{-t},z'=-2x-2z+3e^{-t} $$ I would like to have formula for $x'$ in order to find fundamental matrix and so on. So from above quations I obtain $z'-2x''=e^{-t}$, integrate sides and have $2x'=z+e^{-t}$. So I have system of two first-order differential equations $$2x'=z+e^{-t}, z'=-2x-2z+3e^{-t}$$ Is it correct?",,['ordinary-differential-equations']
66,Show IVP $u'(t)= -u(t)\ln(u(t))$ has unique solution,Show IVP  has unique solution,u'(t)= -u(t)\ln(u(t)),"Consider the function  $$f:[0,e^{-1}]\rightarrow \mathbb{R}, u\mapsto \begin{cases}       -u\ln(u) & \text{if}\ u\in (0,e^{-1}] \\       0, & \text{if}\ u=0     \end{cases} $$ The IVP is given as $$\begin{cases}       u'(t)=f(u(t)) & \text{for}\ t\in[0,1] \\       u(0)=0      \end{cases} $$ For the first part of the problem I was supposed to show it is monotonically increasing and convex, which I did, furthermore I showed it satisfies $|f(u)-f(v)| \leq f(|u-v|) \quad \forall u,v \in [0,e^{-1}]$ Now I want to show that there exists a unique local solution. I tried the usual methods like separation of variables but the fact, that  $u'(t)=-u(t)\ln(u(t))$ would not let me solve the integral in that process due to always having one integral which is not solvable. So I thought I'd use Peano. As $f$ is a composition of continuous functions, $f$ is continous and Peano tells us there at least exists one solution in a local neighbourhood around $0$. Now I want to show it's unique by using a hint I was given to ""look at the difference of two solutions to the IVP"". So let's consider $u(t)-v(t)$ where both u and v are solutions to the IVP.   I could either check if that is zero ( I would not know how ) or maybe I could differentiate the expression and show that is zero, but again, I don't know how. Is the argument of existence correct? How can I show the uniqueness of the solution?","Consider the function  $$f:[0,e^{-1}]\rightarrow \mathbb{R}, u\mapsto \begin{cases}       -u\ln(u) & \text{if}\ u\in (0,e^{-1}] \\       0, & \text{if}\ u=0     \end{cases} $$ The IVP is given as $$\begin{cases}       u'(t)=f(u(t)) & \text{for}\ t\in[0,1] \\       u(0)=0      \end{cases} $$ For the first part of the problem I was supposed to show it is monotonically increasing and convex, which I did, furthermore I showed it satisfies $|f(u)-f(v)| \leq f(|u-v|) \quad \forall u,v \in [0,e^{-1}]$ Now I want to show that there exists a unique local solution. I tried the usual methods like separation of variables but the fact, that  $u'(t)=-u(t)\ln(u(t))$ would not let me solve the integral in that process due to always having one integral which is not solvable. So I thought I'd use Peano. As $f$ is a composition of continuous functions, $f$ is continous and Peano tells us there at least exists one solution in a local neighbourhood around $0$. Now I want to show it's unique by using a hint I was given to ""look at the difference of two solutions to the IVP"". So let's consider $u(t)-v(t)$ where both u and v are solutions to the IVP.   I could either check if that is zero ( I would not know how ) or maybe I could differentiate the expression and show that is zero, but again, I don't know how. Is the argument of existence correct? How can I show the uniqueness of the solution?",,"['real-analysis', 'ordinary-differential-equations', 'multivariable-calculus']"
67,Norm on the Space $H^{-1}$,Norm on the Space,H^{-1},"I have a question about the norm on the space $H^{-1}$, the dual space of the Sobolev space $H_0^1$. The canonical choice for the norm on $H^{-1}$ would be \begin{equation} \|T\|_{-1} = \sup_{v\in H_0^1, \|v\|_{H_0^1}=1} \frac{\lvert Tv \rvert}{\|v\|_{H_0^1}}, \end{equation} where $\|v\|_{H_0^1} = \left ( \|v\|_{L^2}^2 + \|v^{\prime}\|_{L^2}^2 \right )^{1/2}$ But in class we defined the norm to be \begin{equation} \|T\|_{-1} = \sup_{v\in H_0^1, \lvert v \rvert_{1,2}=1} \frac{\lvert Tv \rvert}{\lvert v \rvert_{1,2}}, \end{equation} where for $v\in H_0^1$: \begin{equation} \lvert v \rvert_{1,2} = \|v^{\prime}\|_{L^2}. \end{equation} Now my question: Is there a specific reason, why we devide by the $\lvert \cdot \rvert_{1,2}$-norm (it is indeed a norm on $H_0^1$) instead of the $\|\cdot\|_{H_0^1}$-norm?","I have a question about the norm on the space $H^{-1}$, the dual space of the Sobolev space $H_0^1$. The canonical choice for the norm on $H^{-1}$ would be \begin{equation} \|T\|_{-1} = \sup_{v\in H_0^1, \|v\|_{H_0^1}=1} \frac{\lvert Tv \rvert}{\|v\|_{H_0^1}}, \end{equation} where $\|v\|_{H_0^1} = \left ( \|v\|_{L^2}^2 + \|v^{\prime}\|_{L^2}^2 \right )^{1/2}$ But in class we defined the norm to be \begin{equation} \|T\|_{-1} = \sup_{v\in H_0^1, \lvert v \rvert_{1,2}=1} \frac{\lvert Tv \rvert}{\lvert v \rvert_{1,2}}, \end{equation} where for $v\in H_0^1$: \begin{equation} \lvert v \rvert_{1,2} = \|v^{\prime}\|_{L^2}. \end{equation} Now my question: Is there a specific reason, why we devide by the $\lvert \cdot \rvert_{1,2}$-norm (it is indeed a norm on $H_0^1$) instead of the $\|\cdot\|_{H_0^1}$-norm?",,"['ordinary-differential-equations', 'operator-theory', 'normed-spaces', 'sobolev-spaces']"
68,On Solution's of Initial Value Problems for ODE,On Solution's of Initial Value Problems for ODE,,"Consider the following IVP $\bullet $  $\;\;y'=2\sqrt{y}\;\;\;\; , y(0)=0$ The solution of the above IVP is given by ; $$ y(x) = \cases{0 & for $x \le x_0$\cr (x-x_0)^2 & for $x > x_0$}$$ where $x_0 \ge 0$. According to my understanding of IVP's, their solutions are curves that satisfy the ODE and also pass through the point specified in the initial conditions. Hence they must satisfy the initial conditions (say),$\;\;y(x_0)=y_0$. Now the above IVP has 2 family of solutions. But what confuses me is that, $y(x)= (x-x_0)^2$, for $x > x_0$ is a solution but, $y(0)=(-x_0)^2$ which need not be $0$. (So it fails to satisfy the initial condition $y(0)=0$) I feel I have misunderstood the concept. I have the same doubt with solutions of the following IVP too, $\bullet$ $\;\;y'=y^{1/3}\;\;\; , y(0)=0$ The solution of the above IVP is given by ; $$y(x)=\cases{0 & for $x\le x_0$\cr \pm \left(\frac{2}{3}(x-x_0)\right)^{3/2} & for $x> x_0$}$$ where $x_0 \ge 0$. Can anyone clear this misunderstanding of mine. And also providing a geometrical interpretation of IVP would help a lot.","Consider the following IVP $\bullet $  $\;\;y'=2\sqrt{y}\;\;\;\; , y(0)=0$ The solution of the above IVP is given by ; $$ y(x) = \cases{0 & for $x \le x_0$\cr (x-x_0)^2 & for $x > x_0$}$$ where $x_0 \ge 0$. According to my understanding of IVP's, their solutions are curves that satisfy the ODE and also pass through the point specified in the initial conditions. Hence they must satisfy the initial conditions (say),$\;\;y(x_0)=y_0$. Now the above IVP has 2 family of solutions. But what confuses me is that, $y(x)= (x-x_0)^2$, for $x > x_0$ is a solution but, $y(0)=(-x_0)^2$ which need not be $0$. (So it fails to satisfy the initial condition $y(0)=0$) I feel I have misunderstood the concept. I have the same doubt with solutions of the following IVP too, $\bullet$ $\;\;y'=y^{1/3}\;\;\; , y(0)=0$ The solution of the above IVP is given by ; $$y(x)=\cases{0 & for $x\le x_0$\cr \pm \left(\frac{2}{3}(x-x_0)\right)^{3/2} & for $x> x_0$}$$ where $x_0 \ge 0$. Can anyone clear this misunderstanding of mine. And also providing a geometrical interpretation of IVP would help a lot.",,"['ordinary-differential-equations', 'initial-value-problems']"
69,Euler's method approximating differential equations,Euler's method approximating differential equations,,"We have been told that $u'(t)= u(t)$ and $u(0)=1$ from this information we can conclude that $u(t) =e^x$ Show that  $ u_k =(1+h)^k,k=0,1,...$  is an approximation for $u(kh) $ using Euler's method My current thoughts are that h is the interval by which we increment, but I am not sure where to go from there.","We have been told that $u'(t)= u(t)$ and $u(0)=1$ from this information we can conclude that $u(t) =e^x$ Show that  $ u_k =(1+h)^k,k=0,1,...$  is an approximation for $u(kh) $ using Euler's method My current thoughts are that h is the interval by which we increment, but I am not sure where to go from there.",,"['calculus', 'ordinary-differential-equations', 'numerical-methods', 'approximation', 'approximation-theory']"
70,series solution laplace equation,series solution laplace equation,,"I need help finding th series solution to the laplace equation  $u_{xx} +u_{yy}=0\\$ in the infinite rectangle  $\Pi =(-\infty,0]$x$[0,\pi]$ in $R^2(x,y)$ provided that sup$_{(x,y)\in\Pi}|u(x,y)|<\infty$ and the function u=u(x,y) satisfy the boundary values  $u_y(x,0)=u_y(x,\pi)=0, u(0,y)=h(y).$ I have seperated the variables to be $\frac{X''(x)}{-X(x)}=\frac{Y''(y)}{Y(y)}=\lambda$ and split them up to two ODE, but i cant figure out what to do next to meet the boundry conditions when the conditions are with respect to derivative of y. I also dont quite understand the meaning of the notation with sup. All help is apperciated!","I need help finding th series solution to the laplace equation  $u_{xx} +u_{yy}=0\\$ in the infinite rectangle  $\Pi =(-\infty,0]$x$[0,\pi]$ in $R^2(x,y)$ provided that sup$_{(x,y)\in\Pi}|u(x,y)|<\infty$ and the function u=u(x,y) satisfy the boundary values  $u_y(x,0)=u_y(x,\pi)=0, u(0,y)=h(y).$ I have seperated the variables to be $\frac{X''(x)}{-X(x)}=\frac{Y''(y)}{Y(y)}=\lambda$ and split them up to two ODE, but i cant figure out what to do next to meet the boundry conditions when the conditions are with respect to derivative of y. I also dont quite understand the meaning of the notation with sup. All help is apperciated!",,"['ordinary-differential-equations', 'harmonic-functions']"
71,Laurent series and region of convergence of $\frac{z}{(z+2)(z+1)}$ at $z=-2$,Laurent series and region of convergence of  at,\frac{z}{(z+2)(z+1)} z=-2,I am trying to find the Laurent series and region of convergence $\frac{z}{(z+2)(z+1)}$ at $z=-2$. I found that $$ \frac{z}{(z+2)(z+1)}=\frac{2}{z+2}+\sum^\infty_{n=0}(z+2)^n $$ But I am confused because usually when I am asked to find the radius of convergence of Laurent series I just calculate the radius for the series but for the function above we have a series plus the term $\frac{2}{z+2}$ so do I just find the radius of convergence of the series and ignore the term $\frac{2}{z+2}$?,I am trying to find the Laurent series and region of convergence $\frac{z}{(z+2)(z+1)}$ at $z=-2$. I found that $$ \frac{z}{(z+2)(z+1)}=\frac{2}{z+2}+\sum^\infty_{n=0}(z+2)^n $$ But I am confused because usually when I am asked to find the radius of convergence of Laurent series I just calculate the radius for the series but for the function above we have a series plus the term $\frac{2}{z+2}$ so do I just find the radius of convergence of the series and ignore the term $\frac{2}{z+2}$?,,"['sequences-and-series', 'ordinary-differential-equations', 'laurent-series']"
72,Solving the advection equation with artificial damping,Solving the advection equation with artificial damping,,"I'm having trouble with the following system of partial differential equations: $$ \frac{\partial u_1}{\partial t} + a \frac{\partial u_1}{\partial x}=u_2(x,t)\;, \\ \frac{\partial u_2}{\partial t} +  b u_2(x,t) = ba \frac{\partial u_1}{\partial x}\;,  $$ where $a$ and $b$ are real positive constants. If $b=0$ , this reduces to the advection equation, so with the initial condition $u_1(x,0)=f(x)$ the solution is a right-running wave $u_1(x,t)=f(x-at)$ . This (simplified) system may be used in a finite-difference model with $b>0$ to artificially damp such a wave over time to simulate an infinite medium. However, I noticed that with $b>0$ , $u_1(x,0)=f(x)$ and $u_2(x,0)=0$ the solution for $u_1$ does not depend on time and becomes $u_1(x,t)\simeq f(x)$ . Would anyone be able to explain this? More specifically, I would like to find the initial condition $u_2(x,0)=g(f(x))$ so that $u_1(x,t)=0$ as $t\rightarrow\infty$ . Thanks!","I'm having trouble with the following system of partial differential equations: where and are real positive constants. If , this reduces to the advection equation, so with the initial condition the solution is a right-running wave . This (simplified) system may be used in a finite-difference model with to artificially damp such a wave over time to simulate an infinite medium. However, I noticed that with , and the solution for does not depend on time and becomes . Would anyone be able to explain this? More specifically, I would like to find the initial condition so that as . Thanks!","
\frac{\partial u_1}{\partial t} + a \frac{\partial u_1}{\partial x}=u_2(x,t)\;, \\
\frac{\partial u_2}{\partial t} +  b u_2(x,t) = ba \frac{\partial u_1}{\partial x}\;, 
 a b b=0 u_1(x,0)=f(x) u_1(x,t)=f(x-at) b>0 b>0 u_1(x,0)=f(x) u_2(x,0)=0 u_1 u_1(x,t)\simeq f(x) u_2(x,0)=g(f(x)) u_1(x,t)=0 t\rightarrow\infty","['ordinary-differential-equations', 'partial-differential-equations', 'systems-of-equations', 'initial-value-problems']"
73,nth derivative of a function to an arbitrary power,nth derivative of a function to an arbitrary power,,"How would one analytically write the following; $$\frac{d^n}{dx^n}\left(f(x)^k\right)$$ for integer $k$ and integer $n$? Writing it for a specific $n$ is easy, however, is there a finite series (similar to the Stirling or Bell numbers) that will generate a correct solution?","How would one analytically write the following; $$\frac{d^n}{dx^n}\left(f(x)^k\right)$$ for integer $k$ and integer $n$? Writing it for a specific $n$ is easy, however, is there a finite series (similar to the Stirling or Bell numbers) that will generate a correct solution?",,"['ordinary-differential-equations', 'derivatives']"
74,Applications of some Differential Equation,Applications of some Differential Equation,,"I was practicing some questions to get a grip of differential equation when I stumbled upon this problem. It looks like this: Tank A initially holds 100 gallons of brine that contains 100 pounds of salt, and tank B holds 100 gallons of water. Two gallons of water enter A each minute, and the mixture, assumed uniform, flows from A into tank B at the same rate. If the resulting mixture, also kept uniform, runs out of B at the rate of 2 gallons per minute, how much salt is in tank B at the end of 1 hour? It seems confusing......to build a differential equation that would describe that statement above. How do you answer it?","I was practicing some questions to get a grip of differential equation when I stumbled upon this problem. It looks like this: Tank A initially holds 100 gallons of brine that contains 100 pounds of salt, and tank B holds 100 gallons of water. Two gallons of water enter A each minute, and the mixture, assumed uniform, flows from A into tank B at the same rate. If the resulting mixture, also kept uniform, runs out of B at the rate of 2 gallons per minute, how much salt is in tank B at the end of 1 hour? It seems confusing......to build a differential equation that would describe that statement above. How do you answer it?",,['ordinary-differential-equations']
75,How to show that all solutions of$ x'' = -x$ is $\alpha sin(t) +\beta cos(t)$,How to show that all solutions of is, x'' = -x \alpha sin(t) +\beta cos(t),"I have this problem: Show that any solution of $x'' = -x$ is one of these: $αsin(t) + βcos(t)$ for some $(α, β)$. I'm not sure is I should use the Mean Value Theorem that says if $f '(x) = 0$ for all $x$, then $f$ is constant on that interval.","I have this problem: Show that any solution of $x'' = -x$ is one of these: $αsin(t) + βcos(t)$ for some $(α, β)$. I'm not sure is I should use the Mean Value Theorem that says if $f '(x) = 0$ for all $x$, then $f$ is constant on that interval.",,"['calculus', 'ordinary-differential-equations']"
76,"Why when ""fluxions"" are zero can equation be considered linear?","Why when ""fluxions"" are zero can equation be considered linear?",,"Reading an article form the 40's, there is a passage like this: Why can the equations be considered linear? (I assume ""fluxions"" are the derivatives.) P.S.: Article is ""Effects of Controls on Stability, Ashby, 1945"", p. 7 in Mechanisms of Intelligence, full PDF here .","Reading an article form the 40's, there is a passage like this: Why can the equations be considered linear? (I assume ""fluxions"" are the derivatives.) P.S.: Article is ""Effects of Controls on Stability, Ashby, 1945"", p. 7 in Mechanisms of Intelligence, full PDF here .",,['ordinary-differential-equations']
77,Why $U''(ξ) + 1/2\xi U' (\xi) + 1/2 U (\xi) = 0$ is invariant under $\xi = -\xi?$,Why  is invariant under,U''(ξ) + 1/2\xi U' (\xi) + 1/2 U (\xi) = 0 \xi = -\xi?,Why $U''(ξ) + \frac{1}{2}\xi U' (\xi) + \frac{1}{2} U (\xi) = 0$ is invariant wrt to change of variables $\xi = -\xi?$ I think there is no additional information about $U.$,Why $U''(ξ) + \frac{1}{2}\xi U' (\xi) + \frac{1}{2} U (\xi) = 0$ is invariant wrt to change of variables $\xi = -\xi?$ I think there is no additional information about $U.$,,['ordinary-differential-equations']
78,"Proving $f(x,y)$ is not Lipschitz but still has a unique solution to initial value problem",Proving  is not Lipschitz but still has a unique solution to initial value problem,"f(x,y)","Consider $g:\mathbb R\to\mathbb R$ defined by $g(y)=1+\sqrt |y|$. Let $h:\mathbb R\to \mathbb R$ be a continous function such that $h(x)\neq 0$ for all $x\neq 0$. I'm asked to prove the following: Prove the function $f(x,y)=\frac {h(x)}{g(y)}$ is not Lipschitz respect to $y$ around the origin. Prove the initial value problem associated with $(0,0)$ has a unique solution. Find the solution in the case $h(x)=1$. The first point is clear since $\partial f(x,y)/\partial y \to \infty $ as $y\to 0$. My problem lies on the second point. My only tool for proving uniqueness is the Picard theorem but $f(x,y)$ is not Lipschitz so I don't know how to tackle this problem. I tried using the method of separating variables but don't really know if it ensures uniqueness. Any help is appreciated.","Consider $g:\mathbb R\to\mathbb R$ defined by $g(y)=1+\sqrt |y|$. Let $h:\mathbb R\to \mathbb R$ be a continous function such that $h(x)\neq 0$ for all $x\neq 0$. I'm asked to prove the following: Prove the function $f(x,y)=\frac {h(x)}{g(y)}$ is not Lipschitz respect to $y$ around the origin. Prove the initial value problem associated with $(0,0)$ has a unique solution. Find the solution in the case $h(x)=1$. The first point is clear since $\partial f(x,y)/\partial y \to \infty $ as $y\to 0$. My problem lies on the second point. My only tool for proving uniqueness is the Picard theorem but $f(x,y)$ is not Lipschitz so I don't know how to tackle this problem. I tried using the method of separating variables but don't really know if it ensures uniqueness. Any help is appreciated.",,"['real-analysis', 'ordinary-differential-equations']"
79,Integrating Factors Proof Help,Integrating Factors Proof Help,,I am trying to learn the methodology behind this ODE technique.  I feel the source I am using (pic-related) leaves out something between Steps 2 and 3.  How does the original equation transform to that in Step 3 without any algebraic manipulation?  Help!?,I am trying to learn the methodology behind this ODE technique.  I feel the source I am using (pic-related) leaves out something between Steps 2 and 3.  How does the original equation transform to that in Step 3 without any algebraic manipulation?  Help!?,,"['ordinary-differential-equations', 'integrating-factor']"
80,$y'' + y' = \frac 43x - 1$,,y'' + y' = \frac 43x - 1,"I was able to get the solution to $y'' + y' = \frac 43x - 1$ by using variation of parameters. I got $y = \frac 23 x^2 - \frac 73 x+c_1+c_2e^{-x}.$ But I wanted to know how to solve this using undetermined coefficients since that method is easier. The characteristic polynomial is $r^2+r=r(r+1)=0$ which has roots $r=0,-1$ so $y_c = c_1+c_2 e^{-x}.$ What I learned is that if the ""forcing function"" ($\frac 43x -1$ in this problem) shares a term that is $x^k$ times a term in $y_c,$ where k is a nonnegative integer, then $y_p$ will contain a term that is $x^{k+1}$ times the shared term. Since $y_c$ and the forcing function have the term ""1"" in common, I thought this meant that $y_p$ would contain a term $x^{0+1}*1=x.$ So $y_p$ would have the form $Ax+B.$ Since B is accounted for in $y_c$, I tried to use $y_p=Ax.$ This gave me the wrong answer, which made me think I need to try a solution of the form $y_p=Ax^2+Bx.$ However, I don't know where the $x^2$ term comes from! If this is the right form, can someone explain where the quadratic term comes from? If I'm wrong, please correct me. Also, sorry if there are any typos. Thank you. :)","I was able to get the solution to $y'' + y' = \frac 43x - 1$ by using variation of parameters. I got $y = \frac 23 x^2 - \frac 73 x+c_1+c_2e^{-x}.$ But I wanted to know how to solve this using undetermined coefficients since that method is easier. The characteristic polynomial is $r^2+r=r(r+1)=0$ which has roots $r=0,-1$ so $y_c = c_1+c_2 e^{-x}.$ What I learned is that if the ""forcing function"" ($\frac 43x -1$ in this problem) shares a term that is $x^k$ times a term in $y_c,$ where k is a nonnegative integer, then $y_p$ will contain a term that is $x^{k+1}$ times the shared term. Since $y_c$ and the forcing function have the term ""1"" in common, I thought this meant that $y_p$ would contain a term $x^{0+1}*1=x.$ So $y_p$ would have the form $Ax+B.$ Since B is accounted for in $y_c$, I tried to use $y_p=Ax.$ This gave me the wrong answer, which made me think I need to try a solution of the form $y_p=Ax^2+Bx.$ However, I don't know where the $x^2$ term comes from! If this is the right form, can someone explain where the quadratic term comes from? If I'm wrong, please correct me. Also, sorry if there are any typos. Thank you. :)",,['ordinary-differential-equations']
81,Characterisation of $\left(-\frac{d^2}{dx^2}\right)^{1/2}$,Characterisation of,\left(-\frac{d^2}{dx^2}\right)^{1/2},"Let $T:=-\frac{d^2}{dx^2}$ on $L^2([0,1])$ (let's say with Dirichlet boundary conditions). This is a positive operator and therefore has a positive square-root $\left(-\frac{d^2}{dx^2}\right)^{1/2}$ My question: Is there a nice characterisation of this square-root? I know that in $n$ dimensions the operator $(-\Delta)^{1/2}$ is quite nasty, but I suspected that it might be better in one dimension. It would be particularly nice to have some relation between $i\frac{d}{dx}$ and $\left(-\frac{d^2}{dx^2}\right)^{1/2}$. Does anyone know whether such a thing exists? Maybe a more precise way to pose this question is the following: By the polar decomposition, there is a unitary operator $U$ such that  $$i\frac{d}{dx} = U\left(-\frac{d^2}{dx^2}\right)^{1/2}.$$ What's $U$?","Let $T:=-\frac{d^2}{dx^2}$ on $L^2([0,1])$ (let's say with Dirichlet boundary conditions). This is a positive operator and therefore has a positive square-root $\left(-\frac{d^2}{dx^2}\right)^{1/2}$ My question: Is there a nice characterisation of this square-root? I know that in $n$ dimensions the operator $(-\Delta)^{1/2}$ is quite nasty, but I suspected that it might be better in one dimension. It would be particularly nice to have some relation between $i\frac{d}{dx}$ and $\left(-\frac{d^2}{dx^2}\right)^{1/2}$. Does anyone know whether such a thing exists? Maybe a more precise way to pose this question is the following: By the polar decomposition, there is a unitary operator $U$ such that  $$i\frac{d}{dx} = U\left(-\frac{d^2}{dx^2}\right)^{1/2}.$$ What's $U$?",,"['functional-analysis', 'analysis', 'ordinary-differential-equations', 'spectral-theory']"
82,existence of solution of ODE with one sided Lipschitz right hand side,existence of solution of ODE with one sided Lipschitz right hand side,,"I know that in any ODE, the right hand side should be continuous or Lipschitz to guarantee the existence of a solution, in my research, I came across an ODE where the right hand side is one sided Lipschitz but I cannot find any resource about the way to prove that existence, can any one please guide me to a book or articles which could give me these information? The ODE is:  \begin{align*} \begin{cases} &\dfrac{d y(t)}{d t} = f(y(t))\\ &y(0) = x_{0} \end{cases} \end{align*} for almost every where $t\geq 0$ and $y(t)$ is absolutely continuous. Thanks very much for any one could help.","I know that in any ODE, the right hand side should be continuous or Lipschitz to guarantee the existence of a solution, in my research, I came across an ODE where the right hand side is one sided Lipschitz but I cannot find any resource about the way to prove that existence, can any one please guide me to a book or articles which could give me these information? The ODE is:  \begin{align*} \begin{cases} &\dfrac{d y(t)}{d t} = f(y(t))\\ &y(0) = x_{0} \end{cases} \end{align*} for almost every where $t\geq 0$ and $y(t)$ is absolutely continuous. Thanks very much for any one could help.",,['ordinary-differential-equations']
83,Solve a differential equation by a Laplace transform,Solve a differential equation by a Laplace transform,,I have this differential equation $\frac{d^4 y}{dx^4} - y = f(x)$ Where $y $ and the first three derivatives of $y$ disappear at $x = 0$. I have shown that $$\bar{y}(s)= \int_{0}^{\infty} f(\xi) \frac{e^{-s \xi}}{s^4 -1} d\xi $$ which is the first part of the question. I need to show that $$ y(x) = \frac{1}{2} \int_{0}^{x} f(\xi) [\sinh(x - \xi) - \sin(x - \xi)] d\xi$$ By breaking $\frac{1}{s^4 -1}$ into partial fractions we have $$\frac{1}{s^4 -1} = \frac{1}{2} \int_{0}^{\infty} (\sinh x - \sin x ) e^{-sx}  dx $$ And we have a double integral. How do I get to the solution from here?,I have this differential equation $\frac{d^4 y}{dx^4} - y = f(x)$ Where $y $ and the first three derivatives of $y$ disappear at $x = 0$. I have shown that $$\bar{y}(s)= \int_{0}^{\infty} f(\xi) \frac{e^{-s \xi}}{s^4 -1} d\xi $$ which is the first part of the question. I need to show that $$ y(x) = \frac{1}{2} \int_{0}^{x} f(\xi) [\sinh(x - \xi) - \sin(x - \xi)] d\xi$$ By breaking $\frac{1}{s^4 -1}$ into partial fractions we have $$\frac{1}{s^4 -1} = \frac{1}{2} \int_{0}^{\infty} (\sinh x - \sin x ) e^{-sx}  dx $$ And we have a double integral. How do I get to the solution from here?,,"['ordinary-differential-equations', 'laplace-transform']"
84,Solve $2x^2y'y'' - xy'' + y' =0$,Solve,2x^2y'y'' - xy'' + y' =0,"I'm solving the differential equation $2x^2y'y'' - xy'' + y' =0$ Can someone verify whether I'm correct? My attempt: We let $z = y'$ The differential equation becomes: $$2x^2zz' - xz' + z = 0$$ $$\Rightarrow 2x^2z\frac{dz}{dx} - x\frac{dz}{dx} + z = 0$$ $$\Rightarrow  -x + z\frac{dx}{dz} = -2x^2z$$ This is a Bernouilli equation in $x$. Let's substitute $u = 1/x, du/dz =-1/x^2dx/dz$ which yields: $$z\frac{du}{dz} +u = 2z$$ This is a first order linear differential equation, which has the solution (after separation of variables) $$u =c_1(z-2)$$ or $$\frac{1}{x} = c_1(y'-2)$$ and equivalently: $$y' = 2 + \frac{1}{c_1x}$$ and after integration: $$y = \frac{1}{c_1}\ln|c_1x| + 2x + c_2$$ I'm asking to verify this because the solution in my book says the solution should be $$1/2\ln|x| \pm 1/2 \sqrt{1_4c_1x^2} \mp 1/2\ln(1 + \sqrt{1-4c_1x^2}) \pm 1/2\ln(2\sqrt{|c_1|x} + c_2)$$","I'm solving the differential equation $2x^2y'y'' - xy'' + y' =0$ Can someone verify whether I'm correct? My attempt: We let $z = y'$ The differential equation becomes: $$2x^2zz' - xz' + z = 0$$ $$\Rightarrow 2x^2z\frac{dz}{dx} - x\frac{dz}{dx} + z = 0$$ $$\Rightarrow  -x + z\frac{dx}{dz} = -2x^2z$$ This is a Bernouilli equation in $x$. Let's substitute $u = 1/x, du/dz =-1/x^2dx/dz$ which yields: $$z\frac{du}{dz} +u = 2z$$ This is a first order linear differential equation, which has the solution (after separation of variables) $$u =c_1(z-2)$$ or $$\frac{1}{x} = c_1(y'-2)$$ and equivalently: $$y' = 2 + \frac{1}{c_1x}$$ and after integration: $$y = \frac{1}{c_1}\ln|c_1x| + 2x + c_2$$ I'm asking to verify this because the solution in my book says the solution should be $$1/2\ln|x| \pm 1/2 \sqrt{1_4c_1x^2} \mp 1/2\ln(1 + \sqrt{1-4c_1x^2}) \pm 1/2\ln(2\sqrt{|c_1|x} + c_2)$$",,[]
85,Find a particular solution of $\frac{1}{xD+1} (x^{-1}) $,Find a particular solution of,\frac{1}{xD+1} (x^{-1}) ,"I'm familiar with sums where the coefficient of D is a constant. I don't know how to solve ones like these, and I don't know what these sums are called. Can someone please suggest any reference material (available online) for this?","I'm familiar with sums where the coefficient of D is a constant. I don't know how to solve ones like these, and I don't know what these sums are called. Can someone please suggest any reference material (available online) for this?",,['ordinary-differential-equations']
86,"Differential equation, periodic solution, eigenvalues","Differential equation, periodic solution, eigenvalues",,"Sorry for my bad english. Let $u : \mathbb{R} \rightarrow \mathbb{R}^n$ a solution of a linear differential equation $ x'(t) = Ax(t)$ $(1)$ with $ A \in M_{nn}(\mathbb{R})$. We suppose A is a diagonalizable matrix. We want to prove that if this equation has a periodic solution (non-zero) with period $T > 0$, then the matrix $e^{TA}$ has the eigenvalue $1$ ; and to deduce that if all the eigenvalues of A are real, then $(1)$ hasn't periodic solution non constant. I don't see how to make explicit it. Someone could help me ? Thank you in advance...","Sorry for my bad english. Let $u : \mathbb{R} \rightarrow \mathbb{R}^n$ a solution of a linear differential equation $ x'(t) = Ax(t)$ $(1)$ with $ A \in M_{nn}(\mathbb{R})$. We suppose A is a diagonalizable matrix. We want to prove that if this equation has a periodic solution (non-zero) with period $T > 0$, then the matrix $e^{TA}$ has the eigenvalue $1$ ; and to deduce that if all the eigenvalues of A are real, then $(1)$ hasn't periodic solution non constant. I don't see how to make explicit it. Someone could help me ? Thank you in advance...",,['ordinary-differential-equations']
87,Prove that a differentiable function with a special propriety is a Isometry,Prove that a differentiable function with a special propriety is a Isometry,,"Hi folks! I'm trying to answer this one exercise: Let $f:\mathbb R^m\to\mathbb R^m$ be a $C^1$ function such that, for all $x\in\mathbb R^m$ $|f'(x)\cdot v|=||v||$ (where $||\cdot||$ is the euclidean norm). Show that $||f(x)-f(y)||=||x-y||$. $\bullet$ Using the Mean Value Inequality, it's easy to see that $||f(x)-f(y)||\leq||x-y||$ . $\bullet$ This exercise was on the Inverse Function Theorem section. I'm stuck trying to prove that $||f(x)-f(y)||\geq||x-y||$.","Hi folks! I'm trying to answer this one exercise: Let $f:\mathbb R^m\to\mathbb R^m$ be a $C^1$ function such that, for all $x\in\mathbb R^m$ $|f'(x)\cdot v|=||v||$ (where $||\cdot||$ is the euclidean norm). Show that $||f(x)-f(y)||=||x-y||$. $\bullet$ Using the Mean Value Inequality, it's easy to see that $||f(x)-f(y)||\leq||x-y||$ . $\bullet$ This exercise was on the Inverse Function Theorem section. I'm stuck trying to prove that $||f(x)-f(y)||\geq||x-y||$.",,"['real-analysis', 'ordinary-differential-equations', 'multivariable-calculus', 'inverse-function-theorem']"
88,Differential equation $y'=\frac{y}{2y\ln y+y+x}$,Differential equation,y'=\frac{y}{2y\ln y+y+x},"Solve the differential equation   $$y'=\frac{y}{2y\ln y+y+x}$$ This doesn't look hard, but the problem is how to spot the right substitution. This seems like a linear differential equation, but I couldn't transform it to the right form. This is what I did instead: Firstly, I transformed the equation to the following form: $$y-\left(2y\ln y+y+x\right)y'=0$$ Then I noticed that if I divide it with $y^2$ and recombine the terms in the numerator, I can end with the something like this: $$\frac{y-xy'-y(1+\ln y)y'-y\ln yy'}{y^2}=0$$ which is indeed $$\left(\frac{x-y\ln y\left(1+\ln y\right)}{y}\right)'=0$$ and thus the solution is $$x-y\ln y\left(1+\ln y\right)-c_1y=0$$ My approach works in this particular case. I spent a lot of time trying to figure out how to recombine the terms in the fraction in order to artificially create a derivative of a fraction. Of course, this is not how the general approach should look like. My question is how to deal with equations like this? Is there a more general algorithm which can be used to lead us to the solution with less effort? What substitution would be in this particular equation and how to spot the right substitution?","Solve the differential equation   $$y'=\frac{y}{2y\ln y+y+x}$$ This doesn't look hard, but the problem is how to spot the right substitution. This seems like a linear differential equation, but I couldn't transform it to the right form. This is what I did instead: Firstly, I transformed the equation to the following form: $$y-\left(2y\ln y+y+x\right)y'=0$$ Then I noticed that if I divide it with $y^2$ and recombine the terms in the numerator, I can end with the something like this: $$\frac{y-xy'-y(1+\ln y)y'-y\ln yy'}{y^2}=0$$ which is indeed $$\left(\frac{x-y\ln y\left(1+\ln y\right)}{y}\right)'=0$$ and thus the solution is $$x-y\ln y\left(1+\ln y\right)-c_1y=0$$ My approach works in this particular case. I spent a lot of time trying to figure out how to recombine the terms in the fraction in order to artificially create a derivative of a fraction. Of course, this is not how the general approach should look like. My question is how to deal with equations like this? Is there a more general algorithm which can be used to lead us to the solution with less effort? What substitution would be in this particular equation and how to spot the right substitution?",,"['integration', 'ordinary-differential-equations', 'functions', 'derivatives', 'logarithms']"
89,Differential equations solving,Differential equations solving,,"The number of organisms in a population at time t is denoted by x. Treating x as a continuous variable, the differential equation satisfied by x and t is $$\frac{dx}{dt} = \frac {xe^{-t}}{k + e^{-t}}$$ where k is  a positive constant. $i)$ Given that x = 10 when t = 0, solve the differential equation, obtaining a relation between x, k and t. I'm mainly stuck in getting the $x$ to one side and $t$ on another, if you could help me there I think I can do the rest of the question.","The number of organisms in a population at time t is denoted by x. Treating x as a continuous variable, the differential equation satisfied by x and t is $$\frac{dx}{dt} = \frac {xe^{-t}}{k + e^{-t}}$$ where k is  a positive constant. $i)$ Given that x = 10 when t = 0, solve the differential equation, obtaining a relation between x, k and t. I'm mainly stuck in getting the $x$ to one side and $t$ on another, if you could help me there I think I can do the rest of the question.",,['ordinary-differential-equations']
90,Constructing a Hamiltonian system with a given number of saddles and centers,Constructing a Hamiltonian system with a given number of saddles and centers,,"How can I construct a hamiltonian system with n saddles and 1 center? It is two dimensional system. I tried various polynomials, but none of it seemed to work. I always end up with at least two centers.","How can I construct a hamiltonian system with n saddles and 1 center? It is two dimensional system. I tried various polynomials, but none of it seemed to work. I always end up with at least two centers.",,['ordinary-differential-equations']
91,Solution of ordinary linear differential equation with variable coefficients,Solution of ordinary linear differential equation with variable coefficients,,"Is it possible to solve a second order ordinary linear differential equation with variable coefficients (Polynomials) by using Laplace transform method? If possible please guide. $$y''+2xy'+8y=0,     \\y(0)=3,  y'(0)=0. $$","Is it possible to solve a second order ordinary linear differential equation with variable coefficients (Polynomials) by using Laplace transform method? If possible please guide. $$y''+2xy'+8y=0,     \\y(0)=3,  y'(0)=0. $$",,['ordinary-differential-equations']
92,Solving first order linear pde,Solving first order linear pde,,"$$xv_x-yv_y=(x-y)\sin(x+y)$$ I am trying to using characteristic method $\dfrac{dx}{x}=\dfrac{dy}{-y}=\dfrac{dv}{(x-y)(\sin(x+y))}$ From first equality (I mean equation $(1)$ and $(2)$) , we get $xy=c_1$ From $(2)$ and $(3)$ , we get $\dfrac{dx+dy}{x-y}=\dfrac{dv}{(x-y)(\sin(x+y))}$ and so get $v=-\cos(x+y)$ Where Am I Wrong? or How can we solve another method?","$$xv_x-yv_y=(x-y)\sin(x+y)$$ I am trying to using characteristic method $\dfrac{dx}{x}=\dfrac{dy}{-y}=\dfrac{dv}{(x-y)(\sin(x+y))}$ From first equality (I mean equation $(1)$ and $(2)$) , we get $xy=c_1$ From $(2)$ and $(3)$ , we get $\dfrac{dx+dy}{x-y}=\dfrac{dv}{(x-y)(\sin(x+y))}$ and so get $v=-\cos(x+y)$ Where Am I Wrong? or How can we solve another method?",,"['ordinary-differential-equations', 'partial-differential-equations']"
93,"Since the $\omega$-limit is invariant, then it must contain only points with $\dot{V}(x)=0$","Since the -limit is invariant, then it must contain only points with",\omega \dot{V}(x)=0,"I am trying to understand the following proof: I do not fully get why he says: since $\Gamma^+$ (which I think is usually referred to as the $\omega$-limit) is an invariant set, then $\dot{V}(x)=0$ in $\Gamma^+$. Intuitively, it seems to me that, since $\omega$-limit is invariant, it must be composed of singularities or closed orbits (and I know that the $\omega$-limit is connected because the orbits for positive times are trapped inside a compact set). But closed orbits are impossible because $V$ is strictly decreasing, so $\omega(x)$ must only contain singularities. But even if this reasoning is correct, it seems to me that it is not rigorous as it is.","I am trying to understand the following proof: I do not fully get why he says: since $\Gamma^+$ (which I think is usually referred to as the $\omega$-limit) is an invariant set, then $\dot{V}(x)=0$ in $\Gamma^+$. Intuitively, it seems to me that, since $\omega$-limit is invariant, it must be composed of singularities or closed orbits (and I know that the $\omega$-limit is connected because the orbits for positive times are trapped inside a compact set). But closed orbits are impossible because $V$ is strictly decreasing, so $\omega(x)$ must only contain singularities. But even if this reasoning is correct, it seems to me that it is not rigorous as it is.",,"['ordinary-differential-equations', 'dynamical-systems', 'proof-explanation']"
94,Find the Integral curves of: ${dx\over(-1)}= {dy\over(3y+4z)}= {dz\over(2y+5z)}$,Find the Integral curves of:,{dx\over(-1)}= {dy\over(3y+4z)}= {dz\over(2y+5z)},"While going through the concept of simultaneous differential equation, I came across following problem: Find the Integral curves of: $${dx\over(-1)}= {dy\over(3y+4z)}= {dz\over(2y+5z)}$$ The solution says: Each of these fractions of the given system of equations is equal to : $${(dx-dy)\over(y-z)}\; \text{and}\; {(dy+2dz)\over7(y+2z)}$$ How each of these fractions can be equal to these fractions?Is there the use of componendo-dividendo concept?if yes, how we deduced that?","While going through the concept of simultaneous differential equation, I came across following problem: Find the Integral curves of: $${dx\over(-1)}= {dy\over(3y+4z)}= {dz\over(2y+5z)}$$ The solution says: Each of these fractions of the given system of equations is equal to : $${(dx-dy)\over(y-z)}\; \text{and}\; {(dy+2dz)\over7(y+2z)}$$ How each of these fractions can be equal to these fractions?Is there the use of componendo-dividendo concept?if yes, how we deduced that?",,"['calculus', 'ordinary-differential-equations']"
95,How to Solve this Particular Euler Differential Equation?,How to Solve this Particular Euler Differential Equation?,,I was asked to solve an Euler ODE: Here's my work so far: I'm asked to solve the following Euler DE using the method of variation of parameters:   $$x^2y''-y=(x^2+1)\sin{x}$$   I already determined the homogeneous solution which resulted in:   $$y_h=c_1 x^{\frac{\sqrt{5}+1}{2}}+c_2 x^{\frac{-\sqrt{5}+1}{2}}$$   I'm having trouble finding the particular solution though...   $$y_p=u_1 y_1+u_2 y_2$$   $$u_1=-\int \frac{R(x)\cdot y_2}{W}~dx \qquad \qquad  W=\text{wronskian}=-\sqrt{5}$$   $$u_1=\frac{1}{\sqrt{5}}\int (x^2+1)\sin{x}\cdot x^{\frac{-\sqrt{5}+1}{2}}~dx$$   But this integral is too complicated for me to solve. I computed it and found that it gives a gamma function and a really long answer. Is there anything I missed that could simplify this problem?,I was asked to solve an Euler ODE: Here's my work so far: I'm asked to solve the following Euler DE using the method of variation of parameters:   $$x^2y''-y=(x^2+1)\sin{x}$$   I already determined the homogeneous solution which resulted in:   $$y_h=c_1 x^{\frac{\sqrt{5}+1}{2}}+c_2 x^{\frac{-\sqrt{5}+1}{2}}$$   I'm having trouble finding the particular solution though...   $$y_p=u_1 y_1+u_2 y_2$$   $$u_1=-\int \frac{R(x)\cdot y_2}{W}~dx \qquad \qquad  W=\text{wronskian}=-\sqrt{5}$$   $$u_1=\frac{1}{\sqrt{5}}\int (x^2+1)\sin{x}\cdot x^{\frac{-\sqrt{5}+1}{2}}~dx$$   But this integral is too complicated for me to solve. I computed it and found that it gives a gamma function and a really long answer. Is there anything I missed that could simplify this problem?,,['ordinary-differential-equations']
96,How to find equilibria of a system of ODEs,How to find equilibria of a system of ODEs,,"Firstly, I am not asking anyone to solve my equations for me. I am just looking for guidance on how to find the equilibria of my system and point me in the correct direction with analysing stability. My system is as follows, \begin{equation} \frac{dy_i}{dx} = a_i y_i + \sum ^N_{j=1}(b_{ij}-c_{ij})y_i\cdot y_j\qquad i=1,\dots ,N \end{equation} Here $N$ can be quite large. $a_i$, $b_{ij}$ and $c_{ij}$ are just positive parameters. I understand that there are in general four types of equilibrium solution. $y_i = 0\qquad$    $\forall i\in N$ $y_i=y_i*\qquad$     $\forall i\in N$ $(y_1 = 0, \dots , y_{i-1} =0, y_i=y_i*, y_{i+1}=0,\dots,y_N =0)$ $(y_1 = 0, \dots , y_{i-1} =0, y_i=y_i*, y_{i+1}=0,\dots, y_{j-1} =0, y_j=y_j*, y_{j+1}=0,\dots,y_N =0)$ Where $y_i*$ is an equilibrium value. To find these we set each $dy_i/dx =0$ and where the nullclines intersect we have an equilibrium point. So in the case of simply re-arranging the above equation for $y_i$ we find that,  \begin{equation} y_i* = \frac{a_i - \sum ^N_{j\neq i} (c_{ij} - b_{ij})y_j}{(c_{ii} - b_{ii})} \end{equation} However I am a little lost as how to proceed from here! i.e how do I find the other equilibria?","Firstly, I am not asking anyone to solve my equations for me. I am just looking for guidance on how to find the equilibria of my system and point me in the correct direction with analysing stability. My system is as follows, \begin{equation} \frac{dy_i}{dx} = a_i y_i + \sum ^N_{j=1}(b_{ij}-c_{ij})y_i\cdot y_j\qquad i=1,\dots ,N \end{equation} Here $N$ can be quite large. $a_i$, $b_{ij}$ and $c_{ij}$ are just positive parameters. I understand that there are in general four types of equilibrium solution. $y_i = 0\qquad$    $\forall i\in N$ $y_i=y_i*\qquad$     $\forall i\in N$ $(y_1 = 0, \dots , y_{i-1} =0, y_i=y_i*, y_{i+1}=0,\dots,y_N =0)$ $(y_1 = 0, \dots , y_{i-1} =0, y_i=y_i*, y_{i+1}=0,\dots, y_{j-1} =0, y_j=y_j*, y_{j+1}=0,\dots,y_N =0)$ Where $y_i*$ is an equilibrium value. To find these we set each $dy_i/dx =0$ and where the nullclines intersect we have an equilibrium point. So in the case of simply re-arranging the above equation for $y_i$ we find that,  \begin{equation} y_i* = \frac{a_i - \sum ^N_{j\neq i} (c_{ij} - b_{ij})y_j}{(c_{ii} - b_{ii})} \end{equation} However I am a little lost as how to proceed from here! i.e how do I find the other equilibria?",,"['ordinary-differential-equations', 'dynamical-systems']"
97,Uniqueness of Solution for cauchy problem,Uniqueness of Solution for cauchy problem,,"I'm working on characteristics for partial differential equations and the example given is the equation $yu_x-xu_y=0$ with the condition $u|_{x=1}=y^2$. I could come up with one solution on the spot, but the book I'm using says the solution is not unique. So far I couldn't come up with another solution than $f(x,y)=x^2+y^2-1$. I understand that the methods of characteristics results in all soultions to be functions of $x^2+y^2$","I'm working on characteristics for partial differential equations and the example given is the equation $yu_x-xu_y=0$ with the condition $u|_{x=1}=y^2$. I could come up with one solution on the spot, but the book I'm using says the solution is not unique. So far I couldn't come up with another solution than $f(x,y)=x^2+y^2-1$. I understand that the methods of characteristics results in all soultions to be functions of $x^2+y^2$",,"['ordinary-differential-equations', 'partial-differential-equations', 'characteristics']"
98,How to solve $(y''')^3-xy'''+y''=0$,How to solve,(y''')^3-xy'''+y''=0,"I have no idea how to solve this example: $$(y''')^3-x\cdot y'''+y''=0$$ This ODE is third order, but, what to do with the power: $^3$??? Thanks in advance!","I have no idea how to solve this example: $$(y''')^3-x\cdot y'''+y''=0$$ This ODE is third order, but, what to do with the power: $^3$??? Thanks in advance!",,['ordinary-differential-equations']
99,Handling $\frac{dy}{dx}$ as a ratio. [duplicate],Handling  as a ratio. [duplicate],\frac{dy}{dx},"This question already has answers here : Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? (27 answers) Closed 7 years ago . I know that given a differential equation, one that is separable, it is not fully correct to handle the $\frac{dy}{dx}$ as a ratio. Meaning that it is not simply a small difference in $y$ over a small difference in $x$, as the difference is infinitesimal. It is simply a notation created by Leibnitz that is equivalent to $f'(x)$. My question is if this is the case then why is this method for ODE still used and taught? Is the answer always correct when dealing with ODE using this method or is does it lead to wrong results in some cases?","This question already has answers here : Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? (27 answers) Closed 7 years ago . I know that given a differential equation, one that is separable, it is not fully correct to handle the $\frac{dy}{dx}$ as a ratio. Meaning that it is not simply a small difference in $y$ over a small difference in $x$, as the difference is infinitesimal. It is simply a notation created by Leibnitz that is equivalent to $f'(x)$. My question is if this is the case then why is this method for ODE still used and taught? Is the answer always correct when dealing with ODE using this method or is does it lead to wrong results in some cases?",,"['ordinary-differential-equations', 'derivatives', 'notation']"
