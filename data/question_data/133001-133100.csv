,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Simple ODE problem in a 1964 paper of Peter Lax,Simple ODE problem in a 1964 paper of Peter Lax,,"The paper ""*Development of singularities of solutions of the nonlinear hyperbolic partial differential equation"" by Peter D.Lax (1964). It starts with a simple theorem about ODEs. Theorem: Let $z(t)$ be the solution of the initial value problem $$dz/dt =a(t)z^2,\ \ \ z(0)=m$$ in the interval $(0,T)$ . Suppose that the function $a(t)$ satisfies the inequality $$0<A<a(t),\ \ \ \ \ \ 0\leq t\leq T,$$ and suppose that $m$ is positive; then $$T<(mA)^{-1}$$ the proof of Lax following Let $z_0(t)$ be the solution of the comparision equation $$dz_0/dt=Az_0^2,\ \ \ z_0(0)=m.$$ Since A is lower bound for $a(t)$ , it follows easily that $z_0(t)$ is lower bound for $z(t)$ for all positive t. Since $z_0=m/(1-mAt)\rightarrow \infty\ \ at\ \ t=(mA)^{-1}$ , it follows that $z(t)$ cannot exist beyond this time. $Q.E.D$ I try to verify ""Since A is lower bound for $a(t)$ , it follows easily that $z_0(t)$ is lower bound for $z(t)$ for all positive t."" in this way. To prove this, I think a following lemma. (My lemma) let $g:[0,T)\rightarrow \mathbb{R}$ be a function which are following; i) $g(0)\geq 0$ ; ii) $g'(0)>0$ ;  i.e. $\forall \epsilon>0,\ \exists \delta>0$ s.t $0<t<\delta$ imply $|\frac{g(t)-g(0)}{t}-g'(0)|<\epsilon$ then $\exists T^*\in (0,T)$ s.t $g(t)\geq 0\ \ for\  t \in [0,T^*)$ proof) From my assumption, let $\epsilon= g'(0)/2$ then exist $\delta>0$ s.t $t\in(0,\delta)$ $$|\frac{g(t)-g(0)}{t}-g'(0)|<g'(0)/2$$ therefore $$g'(0)/2<[g(t)-g(0)]/t<3g'(0)/2\ \ \ for \ t \in (0,\delta)$$ $$g(t)>g'(0)t/2+g(0)>0\ \ for\ t\in(0,\delta)$$ because $g(0)\geq 0$ , $g(t)\geq0\ \ \ \ for\ \ \ t \in [0,\delta)$ . let $\delta= T*$ then proof is done. now prove ""Since A is lower bound for $a(t)$ , it follows easily that $z_0(t)$ is lower bound for $z(t)$ for all positive t."" claim) let $z(t),z_0(t)$ satisfy above suggestion then $z_0(t)\leq z(t)$ for $t \in [0,T)$ . (my proof) let $g(t)=z(t)-z_0(t)$ for $t \in [0,T) $ then $g(0)=m-m=0, g'(0)=(a(0)-A)m^2>0$ therefore $g(t)$ satisfy the suggestion of my lemma. therefore $\exists T^*\in (0,T)$ s.t $t \in [0,T^*)$ imply $g(t)\geq 0$ . actually, $g(t)>0$ for $t\in(0,T^*)$ . therefore $z(t)\geq z_0(t)$ for $t \in [0,T^*)$ , $z(t)> z_0(t)$ for $t\in(0,T^*)$ . Now for contradiction, suppose there exist $T_p \in [T^*,T)$ s.t $z_0(T_p)>z(T_p)$ ; i.e $g(T_p)<0$ . Then there exist $T_1\in [T^*,T_p)$ s.t i) $g(T_1)=0$ ; ii) $g(t)>0 $ for $t\in (0,T_1)$ iii) $\exists \eta>0$ s.t $g(t)<0$ for $t\in(T_1,T_1+\eta)$ i.e) there exist $T_1$ which $g(t)$ first pass through from positive value to negative value. then we can find contradiction, at $t=T_1$ , $g'(T_1)=a(T_1)z(T_1)^2-A z_0(T_1)^2=(a(T_1)-A)z_0(T_1)^2>0 $ note that $z_0(T_1)= m/(1-mAt)>0$ by my lemma,there exist $T^{**}\in (T_1,T)$ s.t $t\in [T_1,T^{**})$ imply $g(t)\geq 0$ ; therefore $z(t)-z_0(t)=g(t)\geq 0 \ \ \forall t\in [0,T)$ . $z_0 is lower bound of z.$ this is my proof, but I think there are some errors in my proof. first, z is the solution for $dz/dt=a(t)z^2, \ \ \ z(0)=m$ in $(0,T)$ but I use $z'(0)-z_0'(0)>0$ for my proof, however $z'$ is not define for $t=0$ second, I use ""there exist $T_1$ which $g(t)$ first pass through from positive value to negative value."", but actually, I don't know how to clarify this sentence. If you have some idea to clarify my proof,or If you have better idea to prove ""Since A is lower bound for $a(t)$ , it follows easily that $z_0(t)$ is lower bound for $z(t)$ for all positive t."", please give me some help.","The paper ""*Development of singularities of solutions of the nonlinear hyperbolic partial differential equation"" by Peter D.Lax (1964). It starts with a simple theorem about ODEs. Theorem: Let be the solution of the initial value problem in the interval . Suppose that the function satisfies the inequality and suppose that is positive; then the proof of Lax following Let be the solution of the comparision equation Since A is lower bound for , it follows easily that is lower bound for for all positive t. Since , it follows that cannot exist beyond this time. I try to verify ""Since A is lower bound for , it follows easily that is lower bound for for all positive t."" in this way. To prove this, I think a following lemma. (My lemma) let be a function which are following; i) ; ii) ;  i.e. s.t imply then s.t proof) From my assumption, let then exist s.t therefore because , . let then proof is done. now prove ""Since A is lower bound for , it follows easily that is lower bound for for all positive t."" claim) let satisfy above suggestion then for . (my proof) let for then therefore satisfy the suggestion of my lemma. therefore s.t imply . actually, for . therefore for , for . Now for contradiction, suppose there exist s.t ; i.e . Then there exist s.t i) ; ii) for iii) s.t for i.e) there exist which first pass through from positive value to negative value. then we can find contradiction, at , note that by my lemma,there exist s.t imply ; therefore . this is my proof, but I think there are some errors in my proof. first, z is the solution for in but I use for my proof, however is not define for second, I use ""there exist which first pass through from positive value to negative value."", but actually, I don't know how to clarify this sentence. If you have some idea to clarify my proof,or If you have better idea to prove ""Since A is lower bound for , it follows easily that is lower bound for for all positive t."", please give me some help.","z(t) dz/dt =a(t)z^2,\ \ \ z(0)=m (0,T) a(t) 0<A<a(t),\ \ \ \ \ \ 0\leq t\leq T, m T<(mA)^{-1} z_0(t) dz_0/dt=Az_0^2,\ \ \ z_0(0)=m. a(t) z_0(t) z(t) z_0=m/(1-mAt)\rightarrow \infty\ \ at\ \ t=(mA)^{-1} z(t) Q.E.D a(t) z_0(t) z(t) g:[0,T)\rightarrow \mathbb{R} g(0)\geq 0 g'(0)>0 \forall \epsilon>0,\ \exists \delta>0 0<t<\delta |\frac{g(t)-g(0)}{t}-g'(0)|<\epsilon \exists T^*\in (0,T) g(t)\geq 0\ \ for\  t \in [0,T^*) \epsilon= g'(0)/2 \delta>0 t\in(0,\delta) |\frac{g(t)-g(0)}{t}-g'(0)|<g'(0)/2 g'(0)/2<[g(t)-g(0)]/t<3g'(0)/2\ \ \ for \ t \in (0,\delta) g(t)>g'(0)t/2+g(0)>0\ \ for\ t\in(0,\delta) g(0)\geq 0 g(t)\geq0\ \ \ \ for\ \ \ t \in [0,\delta) \delta= T* a(t) z_0(t) z(t) z(t),z_0(t) z_0(t)\leq z(t) t \in [0,T) g(t)=z(t)-z_0(t) t \in [0,T)  g(0)=m-m=0, g'(0)=(a(0)-A)m^2>0 g(t) \exists T^*\in (0,T) t \in [0,T^*) g(t)\geq 0 g(t)>0 t\in(0,T^*) z(t)\geq z_0(t) t \in [0,T^*) z(t)> z_0(t) t\in(0,T^*) T_p \in [T^*,T) z_0(T_p)>z(T_p) g(T_p)<0 T_1\in [T^*,T_p) g(T_1)=0 g(t)>0  t\in (0,T_1) \exists \eta>0 g(t)<0 t\in(T_1,T_1+\eta) T_1 g(t) t=T_1 g'(T_1)=a(T_1)z(T_1)^2-A z_0(T_1)^2=(a(T_1)-A)z_0(T_1)^2>0  z_0(T_1)= m/(1-mAt)>0 T^{**}\in (T_1,T) t\in [T_1,T^{**}) g(t)\geq 0 z(t)-z_0(t)=g(t)\geq 0 \ \ \forall t\in [0,T) z_0 is lower bound of z. dz/dt=a(t)z^2, \ \ \ z(0)=m (0,T) z'(0)-z_0'(0)>0 z' t=0 T_1 g(t) a(t) z_0(t) z(t)","['real-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
1,Solve $yy' = \sqrt{y^2+y'^2}y''-y'y''$,Solve,yy' = \sqrt{y^2+y'^2}y''-y'y'',Solve $$yy' = \sqrt{y^2+y'^2}y''-y'y''$$ First I set $p = y'$ and $p' = \frac{dp}{dy}p$ to form: $$yp=\sqrt{y^2+p^2}\frac{dp}{dy}p-p\frac{dp}{dy}p \rightarrow y=\sqrt{y^2+p^2}\frac{dp}{dy}-\frac{dp}{dy}p$$ I am trying to come up with a clever substitution to deal with the square root and $p$ of : $$y=\frac{dp}{dy}(\sqrt{y^2+p^2}-p)$$,Solve First I set and to form: I am trying to come up with a clever substitution to deal with the square root and of :,yy' = \sqrt{y^2+y'^2}y''-y'y'' p = y' p' = \frac{dp}{dy}p yp=\sqrt{y^2+p^2}\frac{dp}{dy}p-p\frac{dp}{dy}p \rightarrow y=\sqrt{y^2+p^2}\frac{dp}{dy}-\frac{dp}{dy}p p y=\frac{dp}{dy}(\sqrt{y^2+p^2}-p),['ordinary-differential-equations']
2,A particular solution for $y''+a_1y' + a_2y = Ae^{i\omega x}$,A particular solution for,y''+a_1y' + a_2y = Ae^{i\omega x},"I'm trying to show that for the equation $y''+a_1y' + a_2y = Ae^{i\omega x}$ , there is a solution of $\phi(x)$ of the form $$\phi(x)=\frac{A}{\vert p(i\omega)\vert}e^{i(\omega x - \alpha)}$$ where $ p(i\omega)=\vert p(i\omega)\vert e^{ix}$ , and $p(i\omega) \neq 0$ . The last statement is confusing and I'm unsure how to interpret it. Why would $p(i\omega)=\vert p(i\omega)\vert e^{ix}$ ? I think that $$p(i \omega)=\omega^2+a_1i\omega+a_2$$ which is a complex number with $Re = \omega ^2+a_2$ , and $Im = a_1\omega$ . It does not seem like we will ever end up to such an identity, with the standard definition of complex norm. To be precise, we have that $$\vert p(i\omega)\vert=\sqrt{(\omega^2 +a_2)^2 + (a_1\omega)^2}$$ As for validating the solution, one would compute $L(\frac{A}{\vert p(i\omega)\vert}e^{i(\omega x - \alpha)})$ , take the term with the norm as a common factor. I imagine the norm is cancelled at some point. What am I missing here?","I'm trying to show that for the equation , there is a solution of of the form where , and . The last statement is confusing and I'm unsure how to interpret it. Why would ? I think that which is a complex number with , and . It does not seem like we will ever end up to such an identity, with the standard definition of complex norm. To be precise, we have that As for validating the solution, one would compute , take the term with the norm as a common factor. I imagine the norm is cancelled at some point. What am I missing here?",y''+a_1y' + a_2y = Ae^{i\omega x} \phi(x) \phi(x)=\frac{A}{\vert p(i\omega)\vert}e^{i(\omega x - \alpha)}  p(i\omega)=\vert p(i\omega)\vert e^{ix} p(i\omega) \neq 0 p(i\omega)=\vert p(i\omega)\vert e^{ix} p(i \omega)=\omega^2+a_1i\omega+a_2 Re = \omega ^2+a_2 Im = a_1\omega \vert p(i\omega)\vert=\sqrt{(\omega^2 +a_2)^2 + (a_1\omega)^2} L(\frac{A}{\vert p(i\omega)\vert}e^{i(\omega x - \alpha)}),"['ordinary-differential-equations', 'complex-numbers']"
3,Why do fixed point theorems appear all over mathematics?,Why do fixed point theorems appear all over mathematics?,,"For example, the Banach fixed-point theorem is applied in the proof of the Picard–Lindelöf theorem about the uniqueness of solutions of ordinary differential equations and the Lefschetz fixed-point theorem (or a modification of it) is used in the proof or in the context of the Weil conjectures. There are so many more examples. What is so special about the equation $f(x)=x$ ?","For example, the Banach fixed-point theorem is applied in the proof of the Picard–Lindelöf theorem about the uniqueness of solutions of ordinary differential equations and the Lefschetz fixed-point theorem (or a modification of it) is used in the proof or in the context of the Weil conjectures. There are so many more examples. What is so special about the equation ?",f(x)=x,"['ordinary-differential-equations', 'soft-question', 'fixed-point-theorems', 'motivation']"
4,Proving that the solution to $f^{\prime}(x) = \frac{1}{x^{2} + (f(x))^{2}}$ is bounded above.,Proving that the solution to  is bounded above.,f^{\prime}(x) = \frac{1}{x^{2} + (f(x))^{2}},"I am given that $f:[0,\infty)\to \mathbb{R}$ is the unique solution to the ODE: $$f^{\prime}(x) = \frac{1}{x^{2} + (f(x))^{2}}$$ with $f(0)=1$ and I must prove that it is bounded. I have already proven (using the fact that the derivative is always positive and the Mean Value Theorem) that it is monotonically increasing so is bounded below, specifically by $f(0)=1$ . However, I am struggling to figure out how to show it is bounded above. One thought I had was to rearrange the ode to get $(f(x))^2$ the subject, however this doesn't seem like a valid method for some reason, I may just be overthinking it though.","I am given that is the unique solution to the ODE: with and I must prove that it is bounded. I have already proven (using the fact that the derivative is always positive and the Mean Value Theorem) that it is monotonically increasing so is bounded below, specifically by . However, I am struggling to figure out how to show it is bounded above. One thought I had was to rearrange the ode to get the subject, however this doesn't seem like a valid method for some reason, I may just be overthinking it though.","f:[0,\infty)\to \mathbb{R} f^{\prime}(x) = \frac{1}{x^{2} + (f(x))^{2}} f(0)=1 f(0)=1 (f(x))^2","['real-analysis', 'calculus', 'ordinary-differential-equations', 'analysis', 'integral-inequality']"
5,The dimension of Jacobi field,The dimension of Jacobi field,,"The Jacobi field is defined as $$J^{''}(t)+R(\gamma^{'}(t),J(t))\gamma^{'}(t)=0$$ since it is a system of $2n$ order, spanned by $\{J,J^{'}\}$ , so it is dimension of $2n$ . But I don't get if we add a condition of $J(0)=0$ , then the dimension is $n$ . ( I can't see why the system is immeddiately reduced to $n$ linear independent equations which is expained by our teaching assistant ). Similarly, if we set the Jacobi field normal, that is $\langle J,\gamma ^{'}\rangle=0$ , then the dimension of normal Jacobi field is $2(n-1)$ . It seems to be a trivial consequence, but I really don' see it. Can anyone else help explain it ?","The Jacobi field is defined as since it is a system of order, spanned by , so it is dimension of . But I don't get if we add a condition of , then the dimension is . ( I can't see why the system is immeddiately reduced to linear independent equations which is expained by our teaching assistant ). Similarly, if we set the Jacobi field normal, that is , then the dimension of normal Jacobi field is . It seems to be a trivial consequence, but I really don' see it. Can anyone else help explain it ?","J^{''}(t)+R(\gamma^{'}(t),J(t))\gamma^{'}(t)=0 2n \{J,J^{'}\} 2n J(0)=0 n n \langle J,\gamma ^{'}\rangle=0 2(n-1)","['linear-algebra', 'ordinary-differential-equations', 'differential-geometry', 'riemannian-geometry', 'smooth-manifolds']"
6,Example of turning nonautonomous system of ODEs to autonomous system of ODEs,Example of turning nonautonomous system of ODEs to autonomous system of ODEs,,"I am reading Chicone's book and it mentions that you can reduce an $n$ th order differential equation into a system of first order equations, and gives an example. Nevertheless, there's no example of how you convert a nonautonomous differential equation to an autonomous one. How does this work, say with $x' = x + t$ ?","I am reading Chicone's book and it mentions that you can reduce an th order differential equation into a system of first order equations, and gives an example. Nevertheless, there's no example of how you convert a nonautonomous differential equation to an autonomous one. How does this work, say with ?",n x' = x + t,"['ordinary-differential-equations', 'dynamical-systems']"
7,Solving $f'(x)=f(x+1)$,Solving,f'(x)=f(x+1),I was wondering if it was possible to find functions $f$ such that $$ f'(x)=f(x+1) $$ for all $x \in \mathbb{R}$ . The only thing i've found so far is that it implies $$ f^{\left(n\right)}\left(x\right)=f\left(x+n\right) $$ is there any way to solve this ?,I was wondering if it was possible to find functions such that for all . The only thing i've found so far is that it implies is there any way to solve this ?,"f 
f'(x)=f(x+1)
 x \in \mathbb{R} 
f^{\left(n\right)}\left(x\right)=f\left(x+n\right)
","['ordinary-differential-equations', 'functions']"
8,Integral of $\int_0^{\infty} \frac{\sin^2(x)}{x^2+1}dx$ using Feynman integration.,Integral of  using Feynman integration.,\int_0^{\infty} \frac{\sin^2(x)}{x^2+1}dx,"Using $$I(t) = \int_0^\infty \frac{\sin^2(tx)}{x^2+1}dx$$ I want to know how to get an answer using Feynman integration and the Laplace transform of a differential equation. The correct answer is $\frac{(1-e^{-2})\pi}{4}$ , but I keep getting $(1-e^{-2})\pi$ , so I want to see where I have made a mistake. Here is the method: setting $t = 1$ provides the integral in question. By repeatedly differentiating $I(t)$ , you  can obtain the differential equation that $4  I'(t) = I'''(t)$ . Setting $J(t) = I'(t)$ , use a Laplace transform to obtain $J(t)$ . Now integrate $\int_0^1 J(t)dt$ ,  which is equal to $I(1) - I(0)$ from the second fundamental theorem of calculus. Since $I(0) = 0$ , solving for $I(1)$ yields the integral in question. Keep in mind that I'm still in high school, so all I really know how to do is partial derivatives and Laplace transforms to solve differential equations. If something is beyond the subjects of multivariable calculus, please continue to answer the question, but know that what I am looking for is an answer through the Feynman technique and Laplace transforms.","Using I want to know how to get an answer using Feynman integration and the Laplace transform of a differential equation. The correct answer is , but I keep getting , so I want to see where I have made a mistake. Here is the method: setting provides the integral in question. By repeatedly differentiating , you  can obtain the differential equation that . Setting , use a Laplace transform to obtain . Now integrate ,  which is equal to from the second fundamental theorem of calculus. Since , solving for yields the integral in question. Keep in mind that I'm still in high school, so all I really know how to do is partial derivatives and Laplace transforms to solve differential equations. If something is beyond the subjects of multivariable calculus, please continue to answer the question, but know that what I am looking for is an answer through the Feynman technique and Laplace transforms.",I(t) = \int_0^\infty \frac{\sin^2(tx)}{x^2+1}dx \frac{(1-e^{-2})\pi}{4} (1-e^{-2})\pi t = 1 I(t) 4  I'(t) = I'''(t) J(t) = I'(t) J(t) \int_0^1 J(t)dt I(1) - I(0) I(0) = 0 I(1),"['calculus', 'ordinary-differential-equations', 'multivariable-calculus', 'definite-integrals', 'laplace-transform']"
9,What exactly is a homogeneous equation?,What exactly is a homogeneous equation?,,"Ok this is when I'm learning about solving first order differential equations by substitution. I searched for answers but found mainly these points: It's homogeneous if it can be expressed as $dy/dx = f(x/y)$ ; It's homogeneous if $f(tx,ty)=t^{\alpha}\cdot f(x,y)$ ; homogeneity is about a ""scaling"" property of the function; But both my textbook and online videos are quite shallow with regard to the explanation of the meaning of these points. What exactly is $f(x/y)$ and how exactly does it relate to $t^{\alpha}$ ? Also, I have such an equation to solve and it's supposed to be homogeneous: $2yy′+5=y^2+5x$ I just don't see why it is homogeneous and how after transforming it into the form: $\displaystyle y'= \frac{y^2-5+5x}{2y}$ the right hand side can be seen as $f(x/y)$ ? Someone plz shed me some light thanks !","Ok this is when I'm learning about solving first order differential equations by substitution. I searched for answers but found mainly these points: It's homogeneous if it can be expressed as ; It's homogeneous if ; homogeneity is about a ""scaling"" property of the function; But both my textbook and online videos are quite shallow with regard to the explanation of the meaning of these points. What exactly is and how exactly does it relate to ? Also, I have such an equation to solve and it's supposed to be homogeneous: I just don't see why it is homogeneous and how after transforming it into the form: the right hand side can be seen as ? Someone plz shed me some light thanks !","dy/dx = f(x/y) f(tx,ty)=t^{\alpha}\cdot f(x,y) f(x/y) t^{\alpha} 2yy′+5=y^2+5x \displaystyle y'= \frac{y^2-5+5x}{2y} f(x/y)",['calculus']
10,A multiple choice question concerning first order linear differential equations,A multiple choice question concerning first order linear differential equations,,"Consider the initial value problem $$y'(t)=f(t)y(t)$$ with $y(0)=1$ where $f:\mathbb{R}\rightarrow \mathbb{R}$ is a  continuous function. Then this initial value problem has (1) Infinitely many solutions for some $f$ (2)a unique solution in $\mathbb{R}$ (3) no solution in $\mathbb{R}$ for some $f$ (4) a solution in an interval containing $0$ , but not on $\mathbb{R}$ for some $f$ My efforts $$\frac{dy(t)}{y(t)} = f(t)dt$$ $$\log y(t) = \int_0^tf(t)dt +C$$ Now at $t=0$ $y=1$ so we get $$0=0+C$$ So $$y(t)=\exp(\int_0^tf(t)dt)$$ So either $2$ is true or $4$ is true? I am not able to go further from here. What logic should I use now Edit: Since $\int_{0}^{t} f$ is continuous and $e^x$ is continuous and composition of continuous function is continuous so $2$ is true. Is this a correct way of thinking?","Consider the initial value problem with where is a  continuous function. Then this initial value problem has (1) Infinitely many solutions for some (2)a unique solution in (3) no solution in for some (4) a solution in an interval containing , but not on for some My efforts Now at so we get So So either is true or is true? I am not able to go further from here. What logic should I use now Edit: Since is continuous and is continuous and composition of continuous function is continuous so is true. Is this a correct way of thinking?",y'(t)=f(t)y(t) y(0)=1 f:\mathbb{R}\rightarrow \mathbb{R} f \mathbb{R} \mathbb{R} f 0 \mathbb{R} f \frac{dy(t)}{y(t)} = f(t)dt \log y(t) = \int_0^tf(t)dt +C t=0 y=1 0=0+C y(t)=\exp(\int_0^tf(t)dt) 2 4 \int_{0}^{t} f e^x 2,"['ordinary-differential-equations', 'initial-value-problems']"
11,"Find $\lim\limits_{t\to\infty}x(t)$ if $x'= (x-y)(1-x^2-y^2)$, $y' = (x+y)(1-x^2-y^2)$","Find  if ,",\lim\limits_{t\to\infty}x(t) x'= (x-y)(1-x^2-y^2) y' = (x+y)(1-x^2-y^2),"Suppose $x_0,y_0$ are reals such that $x_0^2+y_0^2>0.$ Suppose $x(t)$ and $y(t)$ satisfy the following: $\frac{dx}{dt} = (x-y)(1-x^2-y^2),$ $\frac{dy}{dt} = (x+y)(1-x^2-y^2),$ $x(0) = x_0,$ $y(0) = y_0.$ I am asked to find $\lim_{t\to\infty}x(t).$ Dividing the two differential equations, we have $$\frac{dy}{dx} = \frac{x+y}{x-y} = \frac{1 + \frac{y}{x}}{1-\frac{y}{x}}.$$ Let $v(x)=\frac{y}{x},$ so that $y = xv(x),$ and $$\frac{dy}{dx} = v + x\frac{dv}{dx}.$$ This implies that $$x\frac{dv}{dx} = \frac{1+v}{1-v} - v = \frac{1+v^2}{1-v}.$$ Rearranging and integrating, we get $$\log(x) = \arctan(v)-\frac{1}{2}\log(1+v^2)+c,$$ so $$\log(x) = \arctan\left(\frac{y}{x}\right)-\frac{1}{2}\log\left(1+\left(\frac{y}{x}\right)^2\right)+c.$$ By writing $1+\left(\frac{y}{x}\right)^2$ as $\frac{x^2+y^2}{x^2},$ we can simplify the above equation to $$\frac{1}{2}\log(x^2+y^2) = \arctan\left(\frac{y}{x}\right) + c.$$ However, I'm not sure how to proceed from this implicit equation. Any help would be much appreciated!","Suppose $x_0,y_0$ are reals such that $x_0^2+y_0^2>0.$ Suppose $x(t)$ and $y(t)$ satisfy the following: $\frac{dx}{dt} = (x-y)(1-x^2-y^2),$ $\frac{dy}{dt} = (x+y)(1-x^2-y^2),$ $x(0) = x_0,$ $y(0) = y_0.$ I am asked to find $\lim_{t\to\infty}x(t).$ Dividing the two differential equations, we have $$\frac{dy}{dx} = \frac{x+y}{x-y} = \frac{1 + \frac{y}{x}}{1-\frac{y}{x}}.$$ Let $v(x)=\frac{y}{x},$ so that $y = xv(x),$ and $$\frac{dy}{dx} = v + x\frac{dv}{dx}.$$ This implies that $$x\frac{dv}{dx} = \frac{1+v}{1-v} - v = \frac{1+v^2}{1-v}.$$ Rearranging and integrating, we get $$\log(x) = \arctan(v)-\frac{1}{2}\log(1+v^2)+c,$$ so $$\log(x) = \arctan\left(\frac{y}{x}\right)-\frac{1}{2}\log\left(1+\left(\frac{y}{x}\right)^2\right)+c.$$ By writing $1+\left(\frac{y}{x}\right)^2$ as $\frac{x^2+y^2}{x^2},$ we can simplify the above equation to $$\frac{1}{2}\log(x^2+y^2) = \arctan\left(\frac{y}{x}\right) + c.$$ However, I'm not sure how to proceed from this implicit equation. Any help would be much appreciated!",,"['ordinary-differential-equations', 'systems-of-equations']"
12,Method of Frobenius Why is There a Logartihmic Solution?,Method of Frobenius Why is There a Logartihmic Solution?,,"When solving a problem with the method of Frobenius, if the difference of the roots of the indicial equation differ by a natural number, the smaller root of the indicial equation does not produce a solution because there is no value of its $N$th term that will work where N is the natural number difference between the roots. However, there is another solution in the form  $$y_2(x) = u(x) -b_N y_1(x)\log x$$ where $y_1$ is the first solution, $u(x)$ is a Frobenius series with obtained with the smaller root, and $b_N$ is the Nth term of $u(x)$. (Ordinary Differential Equation by Morris Tenenbaum and Harry Pollard). I tried to use Reduction of Order, but couldn't figure out how that would work.","When solving a problem with the method of Frobenius, if the difference of the roots of the indicial equation differ by a natural number, the smaller root of the indicial equation does not produce a solution because there is no value of its $N$th term that will work where N is the natural number difference between the roots. However, there is another solution in the form  $$y_2(x) = u(x) -b_N y_1(x)\log x$$ where $y_1$ is the first solution, $u(x)$ is a Frobenius series with obtained with the smaller root, and $b_N$ is the Nth term of $u(x)$. (Ordinary Differential Equation by Morris Tenenbaum and Harry Pollard). I tried to use Reduction of Order, but couldn't figure out how that would work.",,['ordinary-differential-equations']
13,How is $y' = x^2$ a differential equation if it doesn't contain a function $y$?,How is  a differential equation if it doesn't contain a function ?,y' = x^2 y,"The way I learned differential equations is in the form $$y' = y$$ for example. So for example, $y' = x$ or $y'' = 0$ was never given as examples as differential equations. However, according to my new textbook $y' = x^2$ is given as an example of a differential equation. Can someone explain what's going on here?","The way I learned differential equations is in the form $$y' = y$$ for example. So for example, $y' = x$ or $y'' = 0$ was never given as examples as differential equations. However, according to my new textbook $y' = x^2$ is given as an example of a differential equation. Can someone explain what's going on here?",,"['ordinary-differential-equations', 'derivatives']"
14,Is this differential equation solvable when I cannot separate the variables?,Is this differential equation solvable when I cannot separate the variables?,,"Suppose $(x+y+1)^2 \frac{dy}{dx}+(x+y+1)^2+x^3=0$. How do I express x in terms of y? My thoughts: I don't think there is a clean way of separating the variables, especially once I expand the terms. So I try to work through without expanding. I can try dividing the entire equation by $(x+y+1)^2$ and this might give me a cleaner expression, but I still cannot separate the x from the y. It doesn't seem there is a way to separate the x from the y, so is this still solvable?","Suppose $(x+y+1)^2 \frac{dy}{dx}+(x+y+1)^2+x^3=0$. How do I express x in terms of y? My thoughts: I don't think there is a clean way of separating the variables, especially once I expand the terms. So I try to work through without expanding. I can try dividing the entire equation by $(x+y+1)^2$ and this might give me a cleaner expression, but I still cannot separate the x from the y. It doesn't seem there is a way to separate the x from the y, so is this still solvable?",,['ordinary-differential-equations']
15,"Show that the equilibrium point $(0,0)$ is asymptotically stable and an estimate of its basin of attraction",Show that the equilibrium point  is asymptotically stable and an estimate of its basin of attraction,"(0,0)","Consider the system $$\begin{aligned} \dot{x} &=-y-x^3+x^3y^2\\ \dot{y}&=x-y^3+x^2y^3\end{aligned}$$ Show that the equilibrium point $(0,0)$ is asymptotically stable and an estimate of its attractiveness basin. Clearly the point $(0,0)$ is a point of equilibrium and also $D_f(x,y)=\begin{pmatrix}-3x^2+3y^4 & -1+2yx^3\\ 1+2xy^3 &-3y^2+3y^2x^2 \end{pmatrix}$ , so $D_f(0,0)=\begin{pmatrix}0 & -1\\1  &0 \end{pmatrix}$ and has eigenvalues ​​ $ \pm i$ So, I can not conclude anything of stability for this point, I need a Liapunov function and I do not know what it is or how to find it, could someone help me please? Thank you very much.","Consider the system Show that the equilibrium point is asymptotically stable and an estimate of its attractiveness basin. Clearly the point is a point of equilibrium and also , so and has eigenvalues ​​ So, I can not conclude anything of stability for this point, I need a Liapunov function and I do not know what it is or how to find it, could someone help me please? Thank you very much.","\begin{aligned} \dot{x} &=-y-x^3+x^3y^2\\ \dot{y}&=x-y^3+x^2y^3\end{aligned} (0,0) (0,0) D_f(x,y)=\begin{pmatrix}-3x^2+3y^4 & -1+2yx^3\\ 1+2xy^3 &-3y^2+3y^2x^2 \end{pmatrix} D_f(0,0)=\begin{pmatrix}0 & -1\\1  &0 \end{pmatrix}  \pm i","['ordinary-differential-equations', 'systems-of-equations', 'dynamical-systems', 'basins-of-attraction']"
16,ODE for the Jacobian determinant of the flow map,ODE for the Jacobian determinant of the flow map,,"I'm trying to follow the steps starting at the last section of page 2 here . I'm asked to show that if $|J(x,y,z,t)|$ is the (spatial) Jacobian determinant of the flow map $\boldsymbol{\varphi}(x,y,z,t)$, then $$\frac{\partial |J|}{\partial t}=|J| \operatorname{div}(\mathbf{v}) $$ where $\mathbf{v}=\partial \boldsymbol{\varphi}/\partial t $ is the velocity field of the flow. I couldn't prove it for the 3D case, so I tried the 1D analogue, which boils down to the equation $$\frac{\partial^2 \varphi}{\partial t \partial x}= \frac{\partial \varphi}{\partial x} \frac{\partial^2 \varphi}{\partial x \partial t}.$$ I can't see why this simpler equation must hold either. I'd like some help with both of this cases please. Thank you.","I'm trying to follow the steps starting at the last section of page 2 here . I'm asked to show that if $|J(x,y,z,t)|$ is the (spatial) Jacobian determinant of the flow map $\boldsymbol{\varphi}(x,y,z,t)$, then $$\frac{\partial |J|}{\partial t}=|J| \operatorname{div}(\mathbf{v}) $$ where $\mathbf{v}=\partial \boldsymbol{\varphi}/\partial t $ is the velocity field of the flow. I couldn't prove it for the 3D case, so I tried the 1D analogue, which boils down to the equation $$\frac{\partial^2 \varphi}{\partial t \partial x}= \frac{\partial \varphi}{\partial x} \frac{\partial^2 \varphi}{\partial x \partial t}.$$ I can't see why this simpler equation must hold either. I'd like some help with both of this cases please. Thank you.",,"['ordinary-differential-equations', 'fluid-dynamics']"
17,"If $f$ is a real analytic function, then the solutions of $\dot{x} = f(x)$ are analytic as well","If  is a real analytic function, then the solutions of  are analytic as well",f \dot{x} = f(x),"Let $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$ be a real analytic function, i.e; for   all $x$ $\in$ $\mathbb{R}^n$ exists a neighbourhood $U_x \subset \mathbb{R}^n$ of $x$, satisfying $$f(y) = \sum_{i=0}^{\infty} \frac{1}{n!}f^{(n)}(x) \cdot(y-x)^n \hspace{0.1cm}\mbox{  for all }y\in U_x.$$ where $f^{(n)}$ is the $n$-th derivative of $f$, and $(y-x)^n = (y-x,\ldots,y-x)$. I want to prove that the solutions of the ODE $$\dot{x} = f(x) $$ are analytics as well. I have seen many books saying that this result is true, but I could not find the proof anywhere","Let $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$ be a real analytic function, i.e; for   all $x$ $\in$ $\mathbb{R}^n$ exists a neighbourhood $U_x \subset \mathbb{R}^n$ of $x$, satisfying $$f(y) = \sum_{i=0}^{\infty} \frac{1}{n!}f^{(n)}(x) \cdot(y-x)^n \hspace{0.1cm}\mbox{  for all }y\in U_x.$$ where $f^{(n)}$ is the $n$-th derivative of $f$, and $(y-x)^n = (y-x,\ldots,y-x)$. I want to prove that the solutions of the ODE $$\dot{x} = f(x) $$ are analytics as well. I have seen many books saying that this result is true, but I could not find the proof anywhere",,"['ordinary-differential-equations', 'dynamical-systems']"
18,"Why some functions $f(x,y)$ can be discontinuous but its partial derivatives still could exist?",Why some functions  can be discontinuous but its partial derivatives still could exist?,"f(x,y)","Why some functions  $f(x,y)$ can be discontinuous but its partial   derivatives still could exist? I am slightly confused,... the relationship between continuity, limits, partial derivatives and differentation. I don't understand that from the definition very well.","Why some functions  $f(x,y)$ can be discontinuous but its partial   derivatives still could exist? I am slightly confused,... the relationship between continuity, limits, partial derivatives and differentation. I don't understand that from the definition very well.",,"['analysis', 'ordinary-differential-equations']"
19,Nonlinear differential equation involving sine,Nonlinear differential equation involving sine,,"We know from the usual pendulum differential equation that it involves the sine function, and to solve it, usually we use the small angle approximation. But to fully solve it, the method used is to get the first integral, suppose we have $$\frac{d^2\theta}{dt^2} + B \sin(\theta) = 0$$ we can multiply both sides by $\frac{d\theta}{dt}$, so that $$\frac{d\theta}{dt}\frac{d^2\theta}{dt^2} + B \sin(\theta)\frac{d\theta}{dt} = 0$$ then the first integral is, $$\frac{d}{dt}\Bigg(\frac{1}{2} \Bigg(\frac{d\theta}{dt}\Bigg)^2 - B \cos(\theta) \Bigg) = 0 $$ Now, I have this nonlinear differential equation but it has a first order term in it instead of just the second order and the zeroth order, $$\frac{d^2\theta}{dt^2} + A \frac{d\theta}{dt} + B \sin(\theta) = 0$$ $A$ and $B$ are just constants. How do I go about solving this? Is there a technique or other way to solve this?","We know from the usual pendulum differential equation that it involves the sine function, and to solve it, usually we use the small angle approximation. But to fully solve it, the method used is to get the first integral, suppose we have $$\frac{d^2\theta}{dt^2} + B \sin(\theta) = 0$$ we can multiply both sides by $\frac{d\theta}{dt}$, so that $$\frac{d\theta}{dt}\frac{d^2\theta}{dt^2} + B \sin(\theta)\frac{d\theta}{dt} = 0$$ then the first integral is, $$\frac{d}{dt}\Bigg(\frac{1}{2} \Bigg(\frac{d\theta}{dt}\Bigg)^2 - B \cos(\theta) \Bigg) = 0 $$ Now, I have this nonlinear differential equation but it has a first order term in it instead of just the second order and the zeroth order, $$\frac{d^2\theta}{dt^2} + A \frac{d\theta}{dt} + B \sin(\theta) = 0$$ $A$ and $B$ are just constants. How do I go about solving this? Is there a technique or other way to solve this?",,"['ordinary-differential-equations', 'nonlinear-system']"
20,Modeling a Small Ball Rolling Off a Bigger ball,Modeling a Small Ball Rolling Off a Bigger ball,,"A friend gave a challenging math problem to solve for fun, but since I'm a high school calc student, it's too hard for me to figure out. If a small ball is at the top of a larger   stationary sphere (radius = $1$) and it starts to roll down   the side, at what point will the smaller ball lose contact with the   larger sphere? I got off to a good start, but then got stuck. Here's my work. If you don't care to read it all, you can skip to the last paragraph: I first aim to find the velocity. I start with the acceleration of the ball: $a = g\dot{}\sin(\theta)$ where theta is the angle of inclination the ball is rolling at any given instant. The derivative of the circle equation gives the slope of this incline. The circle is modeled by $f(x) = \sqrt{1-x^2}$ and $f'(x)= -x\dot{}(1-x^2)^{-1/2} \ $Therefore: $\ \theta = \tan^{-1}(f'(x))$. Substituting this back into the original equation, this is where it gets ugly: $$a = g\dot{}\sin\left(\tan^{-1}(f'(x))\right)$$ I figured I needed to put all of this is terms of time, so I change $x$, horizontal displacement of the ball, to $s(t)$, and $a$ to $a(t)$. Using my current knowledge of kinematics, I can relate displacement to acceleration such that $s(t) = \int_{}v(t)~dt \ $ and $\ v(t) = \int_{}a(t)~dt$ so if I'm not mistaken $$s(t) = \int_{}\int_{}a(t)\ dt\ dt$$ So, substituting this all back into my original equation, I get this: $$a(t) = g\dot{}\sin\left(\tan^{-1}\left(f'(\int_{}\int_{}a(t)\ dt\ dt)\right)\right)$$ Not sure if this is even correct syntax at this point. Anyways, I got everything in terms of $a$! But, I've never formally learned to solve DE's, so I'm not sure what to do next or if this DE is solvable, and if it is, it must be so complex that it's not a practical solution. There must be a simpler way that I'm missing, what do I do? After solving for the velocity, how would I use it to find when the ball loses contact with the larger sphere?","A friend gave a challenging math problem to solve for fun, but since I'm a high school calc student, it's too hard for me to figure out. If a small ball is at the top of a larger   stationary sphere (radius = $1$) and it starts to roll down   the side, at what point will the smaller ball lose contact with the   larger sphere? I got off to a good start, but then got stuck. Here's my work. If you don't care to read it all, you can skip to the last paragraph: I first aim to find the velocity. I start with the acceleration of the ball: $a = g\dot{}\sin(\theta)$ where theta is the angle of inclination the ball is rolling at any given instant. The derivative of the circle equation gives the slope of this incline. The circle is modeled by $f(x) = \sqrt{1-x^2}$ and $f'(x)= -x\dot{}(1-x^2)^{-1/2} \ $Therefore: $\ \theta = \tan^{-1}(f'(x))$. Substituting this back into the original equation, this is where it gets ugly: $$a = g\dot{}\sin\left(\tan^{-1}(f'(x))\right)$$ I figured I needed to put all of this is terms of time, so I change $x$, horizontal displacement of the ball, to $s(t)$, and $a$ to $a(t)$. Using my current knowledge of kinematics, I can relate displacement to acceleration such that $s(t) = \int_{}v(t)~dt \ $ and $\ v(t) = \int_{}a(t)~dt$ so if I'm not mistaken $$s(t) = \int_{}\int_{}a(t)\ dt\ dt$$ So, substituting this all back into my original equation, I get this: $$a(t) = g\dot{}\sin\left(\tan^{-1}\left(f'(\int_{}\int_{}a(t)\ dt\ dt)\right)\right)$$ Not sure if this is even correct syntax at this point. Anyways, I got everything in terms of $a$! But, I've never formally learned to solve DE's, so I'm not sure what to do next or if this DE is solvable, and if it is, it must be so complex that it's not a practical solution. There must be a simpler way that I'm missing, what do I do? After solving for the velocity, how would I use it to find when the ball loses contact with the larger sphere?",,"['calculus', 'ordinary-differential-equations', 'recreational-mathematics']"
21,Variation of parameters with nonconstant coefficients,Variation of parameters with nonconstant coefficients,,"In my class we learned how to solve DEs using the variation of parameters when the coefficients are constant, we use undetermined coefficients to get two homogeneous solutions then apply the method of variation of parameters to get a particular solution. But I was never taught how to use method of variation of parameters when the coefficients are non-constant. For example $$x^2y'' -3xy'+3y=12x^4$$ Has solutions $y_1=x$ and $y_2=x^3$. I found this example online but even it doesn't show how these homogenous solutions were found... But it uses them to find the particular solution.... Which I'm fine with. How do I find these solutions and is there a general strategy? Thanks!","In my class we learned how to solve DEs using the variation of parameters when the coefficients are constant, we use undetermined coefficients to get two homogeneous solutions then apply the method of variation of parameters to get a particular solution. But I was never taught how to use method of variation of parameters when the coefficients are non-constant. For example $$x^2y'' -3xy'+3y=12x^4$$ Has solutions $y_1=x$ and $y_2=x^3$. I found this example online but even it doesn't show how these homogenous solutions were found... But it uses them to find the particular solution.... Which I'm fine with. How do I find these solutions and is there a general strategy? Thanks!",,['ordinary-differential-equations']
22,Separation of variables on a second order ODE,Separation of variables on a second order ODE,,"I am attempting to grasp the basics of separation of variables for a second order separable differential equation, and am failing to do so: Given the equation: $$ x=\frac{d^2y}{dx^2}$$ I know from calculus class intuitively that the solution is: $$ y=\frac{x^3}{6}$$ But if I am to use separation of variables, why don't I get: $$ \frac{y^2}{2}=\frac{x^3}{6} $$ Since I should have had to integrate the $y$ side twice as well.","I am attempting to grasp the basics of separation of variables for a second order separable differential equation, and am failing to do so: Given the equation: $$ x=\frac{d^2y}{dx^2}$$ I know from calculus class intuitively that the solution is: $$ y=\frac{x^3}{6}$$ But if I am to use separation of variables, why don't I get: $$ \frac{y^2}{2}=\frac{x^3}{6} $$ Since I should have had to integrate the $y$ side twice as well.",,['ordinary-differential-equations']
23,$X: \mathbb{R} \to \mathbb{R}$ is not Lipschitz continuous,is not Lipschitz continuous,X: \mathbb{R} \to \mathbb{R},"Show that there doesn't exist any Lipschitz continuous function $X: \mathbb{R} \to \mathbb{R}$ with following property: The curve $\gamma : \mathbb{R} \to \mathbb{R}$, $\gamma(t)=\frac{t^2}{1+t^2}$ is a solution curve to differential equation $\dot{\gamma}(t)=X(\gamma(t)).$ This was as an exercise in previous exam in Analysis 2. But, in my all efforts I was capable to show that such a function $X$ is in fact Lipschitz-continuous. I have used the fact $\dot{\gamma}(t)=\frac{2t}{(1+t^2)^2}=X(\gamma(t))$ and by definition of Lipschitz continuity this was in fact bounded for all $t \in \mathbb{R}$. I am not sure if I understood real problem or not but I need your help for this question.","Show that there doesn't exist any Lipschitz continuous function $X: \mathbb{R} \to \mathbb{R}$ with following property: The curve $\gamma : \mathbb{R} \to \mathbb{R}$, $\gamma(t)=\frac{t^2}{1+t^2}$ is a solution curve to differential equation $\dot{\gamma}(t)=X(\gamma(t)).$ This was as an exercise in previous exam in Analysis 2. But, in my all efforts I was capable to show that such a function $X$ is in fact Lipschitz-continuous. I have used the fact $\dot{\gamma}(t)=\frac{2t}{(1+t^2)^2}=X(\gamma(t))$ and by definition of Lipschitz continuity this was in fact bounded for all $t \in \mathbb{R}$. I am not sure if I understood real problem or not but I need your help for this question.",,"['real-analysis', 'analysis', 'ordinary-differential-equations']"
24,Existence of a solution of a system of ODEs,Existence of a solution of a system of ODEs,,"Consider the following system of ODEs, \begin{equation} \begin{cases} x'=-x^2-xy+x\\ y'=-y^2-xy+y \end{cases} \end{equation} with initial conditions $x(0)=x_0>0$ and $y(0)=y_0>0$. I should prove that $(x(t),y(t))$ is defined for all $t\geq 0$ and that $x(t)>0$ and $y(t)>0$ for all $t\geq 0$. Any suggestion?","Consider the following system of ODEs, \begin{equation} \begin{cases} x'=-x^2-xy+x\\ y'=-y^2-xy+y \end{cases} \end{equation} with initial conditions $x(0)=x_0>0$ and $y(0)=y_0>0$. I should prove that $(x(t),y(t))$ is defined for all $t\geq 0$ and that $x(t)>0$ and $y(t)>0$ for all $t\geq 0$. Any suggestion?",,"['real-analysis', 'ordinary-differential-equations']"
25,Separation of Variables and Linear PDEs,Separation of Variables and Linear PDEs,,"Separation of variables is a powerful method which comes to our help for finding a closed form solution for a linear partial differential equation (PDE). For example, we all know that how the method works for the two dimensional Laplace equation in Cartesian Coordinates $$\nabla^2 \phi(x,y) = \partial^{2}_{x}\phi(x,y) + \partial^{2}_{y}\phi(x,y) = 0 \tag{1}$$ The steps are $1$. Consider a solution of the form $\phi(x,y)=X(x)Y(y) \tag{2}$ $2$. Put it into the equation, assuming $X(x) \ne 0,$ and $Y(y) \ne 0$, to get $$\dfrac{\text{D}^2 X}{X}+\dfrac{\text{D}^2 Y}{Y}=0 \tag{3}$$ where $\text{D}$ is the differential operator. $3$. Observe that the above functional equation $(3)$ is possible if and only if $$\dfrac{\text{D}^2 X}{X} = -\dfrac{\text{D}^2 Y}{Y} = \lambda \tag{4}$$ where $\lambda$ is some constant. $4$. Conclude that $X$ and $Y$ should satisfy the following ordinary differential equations (ODEs) $$\begin{align} \left[ \text{D}^2 - \lambda \right]X &= 0 \\ \left[ \text{D}^2 + \lambda \right]Y &= 0 \end{align} \tag{5}$$ However, I always had some questions about this method which is not addressed in the elementary books. Here are my questions $1$. What are the restrictions of this method? I mean when it is not going to work! or equivalently When a linear PDE cannot have a separable solution of the form $(2)$? $2$. Has the method been used for system of linear PDEs? If Yes , would you please give an example.","Separation of variables is a powerful method which comes to our help for finding a closed form solution for a linear partial differential equation (PDE). For example, we all know that how the method works for the two dimensional Laplace equation in Cartesian Coordinates $$\nabla^2 \phi(x,y) = \partial^{2}_{x}\phi(x,y) + \partial^{2}_{y}\phi(x,y) = 0 \tag{1}$$ The steps are $1$. Consider a solution of the form $\phi(x,y)=X(x)Y(y) \tag{2}$ $2$. Put it into the equation, assuming $X(x) \ne 0,$ and $Y(y) \ne 0$, to get $$\dfrac{\text{D}^2 X}{X}+\dfrac{\text{D}^2 Y}{Y}=0 \tag{3}$$ where $\text{D}$ is the differential operator. $3$. Observe that the above functional equation $(3)$ is possible if and only if $$\dfrac{\text{D}^2 X}{X} = -\dfrac{\text{D}^2 Y}{Y} = \lambda \tag{4}$$ where $\lambda$ is some constant. $4$. Conclude that $X$ and $Y$ should satisfy the following ordinary differential equations (ODEs) $$\begin{align} \left[ \text{D}^2 - \lambda \right]X &= 0 \\ \left[ \text{D}^2 + \lambda \right]Y &= 0 \end{align} \tag{5}$$ However, I always had some questions about this method which is not addressed in the elementary books. Here are my questions $1$. What are the restrictions of this method? I mean when it is not going to work! or equivalently When a linear PDE cannot have a separable solution of the form $(2)$? $2$. Has the method been used for system of linear PDEs? If Yes , would you please give an example.",,"['ordinary-differential-equations', 'multivariable-calculus', 'partial-differential-equations']"
26,Given the $n$th derivative of $f$ at $0$ find $f$.,Given the th derivative of  at  find .,n f 0 f,I want to find: $$\frac{1}{2} \sum_{n=0}^{\infty} \frac{(1/6)^n(2n+2)!}{n!(n+1)!}$$ Where the sum looks like a Taylor series of $f(x)$ at $0$: $$\sum_{n=0}^{\infty} \frac{x^n(2n+2)!}{n!(n+1)!}$$ Where: $$f^{(n)}(0)=\frac{(2n+2)!}{(n+1)!}$$ Is there a technique to retreat $f(x)$?,I want to find: $$\frac{1}{2} \sum_{n=0}^{\infty} \frac{(1/6)^n(2n+2)!}{n!(n+1)!}$$ Where the sum looks like a Taylor series of $f(x)$ at $0$: $$\sum_{n=0}^{\infty} \frac{x^n(2n+2)!}{n!(n+1)!}$$ Where: $$f^{(n)}(0)=\frac{(2n+2)!}{(n+1)!}$$ Is there a technique to retreat $f(x)$?,,"['calculus', 'ordinary-differential-equations']"
27,Extremizing a functional subject to an equality constraint,Extremizing a functional subject to an equality constraint,,"Question at hand is: Let $y\in\cal C^2([0,\pi])$ satisfying $y(0)=y(\pi)=0$ and $\int_0^\pi y^2(x)dx=1$ extremize the functional    $$J(y)=\int_0^\pi\left(y'(x)\right)^2dx$$ It's an MCQ, and from options using initial conditions, I could easily infer that solutions are $y(x)=\pm\sqrt{2\over\pi}\sin x$ (the other two wrong options being $y(x)=\pm\sqrt{2\over\pi}\cos x$. But Euler-Lagrange (only method I know to find extremals of functionals) leads me to $2y''(x)=0$. What am I missing/how to solve it directly?","Question at hand is: Let $y\in\cal C^2([0,\pi])$ satisfying $y(0)=y(\pi)=0$ and $\int_0^\pi y^2(x)dx=1$ extremize the functional    $$J(y)=\int_0^\pi\left(y'(x)\right)^2dx$$ It's an MCQ, and from options using initial conditions, I could easily infer that solutions are $y(x)=\pm\sqrt{2\over\pi}\sin x$ (the other two wrong options being $y(x)=\pm\sqrt{2\over\pi}\cos x$. But Euler-Lagrange (only method I know to find extremals of functionals) leads me to $2y''(x)=0$. What am I missing/how to solve it directly?",,"['ordinary-differential-equations', 'calculus-of-variations', 'boundary-value-problem']"
28,Derivation of $1 = x^2+y^2$ with respect to time [duplicate],Derivation of  with respect to time [duplicate],1 = x^2+y^2,This question already has answers here : Implicit Differentiation (2 answers) Closed 8 years ago . I am studying differential algebraic equations. Given the following equation: $1 = x^2+y^2$ Differentiate this equation with respect to time. The correct solution is: $0=2x \dot x + 2y \dot y $ But why?  My wrong idea was: $0=2x+2y$ Thank you very much!,This question already has answers here : Implicit Differentiation (2 answers) Closed 8 years ago . I am studying differential algebraic equations. Given the following equation: $1 = x^2+y^2$ Differentiate this equation with respect to time. The correct solution is: $0=2x \dot x + 2y \dot y $ But why?  My wrong idea was: $0=2x+2y$ Thank you very much!,,"['ordinary-differential-equations', 'derivatives']"
29,Unique solution for ode $y' = {\sqrt{1-y^2}} $,Unique solution for ode,y' = {\sqrt{1-y^2}} ,"I was given to solve the next ode: $y' = {\sqrt{1-y^2}} $ I found its solution: $y=sin(x+c)$ Now, I'm given that $y(0)=0$ and asked to show the only solution is $y=sin(x)$ in the region $(-\infty,\infty)$. I get that c=$\pi k$, and therefore infinite number of solutions. Can I get any help?","I was given to solve the next ode: $y' = {\sqrt{1-y^2}} $ I found its solution: $y=sin(x+c)$ Now, I'm given that $y(0)=0$ and asked to show the only solution is $y=sin(x)$ in the region $(-\infty,\infty)$. I get that c=$\pi k$, and therefore infinite number of solutions. Can I get any help?",,['ordinary-differential-equations']
30,Solve the differential equation $x^2u'=0$ in the sense of distributions,Solve the differential equation  in the sense of distributions,x^2u'=0,"Solve the differential equation in the sense of distribution:   $$x^{2}\frac{du}{dx}=0$$ This is from ""Principles of Applied Mathematics"" by Keener, problem 4.1.5. The solution in the back of the text is  $$u(x)=c_{1}+c_{2}H(x)+c_{3}\delta(x)$$ where $H(x)$ is the Heaviside function and $\delta(x)$ is the Dirac delta function. I think that I understand what the solution means (the action of $u$ on test functions), but I do not understand how to arrive at such a solution.","Solve the differential equation in the sense of distribution:   $$x^{2}\frac{du}{dx}=0$$ This is from ""Principles of Applied Mathematics"" by Keener, problem 4.1.5. The solution in the back of the text is  $$u(x)=c_{1}+c_{2}H(x)+c_{3}\delta(x)$$ where $H(x)$ is the Heaviside function and $\delta(x)$ is the Dirac delta function. I think that I understand what the solution means (the action of $u$ on test functions), but I do not understand how to arrive at such a solution.",,"['ordinary-differential-equations', 'distribution-theory', 'weak-derivatives']"
31,Runge Kutta stability,Runge Kutta stability,,"I am facing a problem solving a ODE with a Runge-Kutta 4th order method: The expression in order to solve is : \begin{equation} Ay^{''}+By^{'}+Cy= Cu \end{equation} \begin{equation} y =OUTPUT \end{equation} \begin{equation} u=INPUT \end{equation} We are using a sample time of 0.01s (with 0.001s sample time it does work), but the solver with some combinations of A,B and C does not converge. Then, we would like to know these combinations of A,B and C that make the method fail before start to using it. Thanks in advance. Case of study 1 (This crashes) $A=1.039\cdot10^3$ $B=5\cdot10^5$ $C=1.55\cdot10^7$ Case of study 2 (This works) $A=7.93\cdot10^3$ $B=5\cdot10^5$ $C=1.55\cdot10^5$","I am facing a problem solving a ODE with a Runge-Kutta 4th order method: The expression in order to solve is : \begin{equation} Ay^{''}+By^{'}+Cy= Cu \end{equation} \begin{equation} y =OUTPUT \end{equation} \begin{equation} u=INPUT \end{equation} We are using a sample time of 0.01s (with 0.001s sample time it does work), but the solver with some combinations of A,B and C does not converge. Then, we would like to know these combinations of A,B and C that make the method fail before start to using it. Thanks in advance. Case of study 1 (This crashes) $A=1.039\cdot10^3$ $B=5\cdot10^5$ $C=1.55\cdot10^7$ Case of study 2 (This works) $A=7.93\cdot10^3$ $B=5\cdot10^5$ $C=1.55\cdot10^5$",,"['ordinary-differential-equations', 'numerical-methods', 'matlab']"
32,Coupled differential equation arising in flow line.,Coupled differential equation arising in flow line.,,"So, I ran (certainly not literally) across these two coupled differential equation given by: $$x'(t)=\left(x(t)\right)^2-\left(y(t)\right)^2   $$    $$ y'(t)=2x(t)y(t)$$ These equation occurred when I was trying to figure flow lines, to a vector field given by. $\vec F:\mathbb{R}^2\to \mathbb{R}^2\;\; \mid \; \vec{F}(x,y)=(x^2-y^2,2xy) $ And the flow line is given by $\vec{r}'(t)=\vec{F}(\vec{r}(t))$  , where $t$ is any introduced parameter. And obviously $\vec{r}(t)=(x(t),y(t))$ , Right? Here's further background if it might be useful , http://ocw.mit.edu/courses/mathematics/18-022-calculus-of-several-variables-fall-2010/lecture-notes/MIT18_022F10_l_17.pdf . Example 17.10 And now for what have I tried. I differentiated equation in terms of $x'(t)$, to get, $$x''(t)=2x(t)x'(t)-2y(t)y'(t)$$ From original equation we have $y(t)=\pm \sqrt{(x(t))^2-x'(t)}$ Using this and above, to eliminate $y'(t)$ and $y(t)$ from the equation $y'(t)=2x(t)y(t)$, yet not really yielding good expression. Any help, to make the solution bit easier?","So, I ran (certainly not literally) across these two coupled differential equation given by: $$x'(t)=\left(x(t)\right)^2-\left(y(t)\right)^2   $$    $$ y'(t)=2x(t)y(t)$$ These equation occurred when I was trying to figure flow lines, to a vector field given by. $\vec F:\mathbb{R}^2\to \mathbb{R}^2\;\; \mid \; \vec{F}(x,y)=(x^2-y^2,2xy) $ And the flow line is given by $\vec{r}'(t)=\vec{F}(\vec{r}(t))$  , where $t$ is any introduced parameter. And obviously $\vec{r}(t)=(x(t),y(t))$ , Right? Here's further background if it might be useful , http://ocw.mit.edu/courses/mathematics/18-022-calculus-of-several-variables-fall-2010/lecture-notes/MIT18_022F10_l_17.pdf . Example 17.10 And now for what have I tried. I differentiated equation in terms of $x'(t)$, to get, $$x''(t)=2x(t)x'(t)-2y(t)y'(t)$$ From original equation we have $y(t)=\pm \sqrt{(x(t))^2-x'(t)}$ Using this and above, to eliminate $y'(t)$ and $y(t)$ from the equation $y'(t)=2x(t)y(t)$, yet not really yielding good expression. Any help, to make the solution bit easier?",,"['ordinary-differential-equations', 'multivariable-calculus']"
33,"Show that: a) $X^{-1}(t)$ is bounded in $[\beta,\infty)$. b)No system solution approaches zero solution when $t \rightarrow \infty.$",Show that: a)  is bounded in . b)No system solution approaches zero solution when,"X^{-1}(t) [\beta,\infty) t \rightarrow \infty.","Let a system $x' = A(t)x$ and suppose there are values positives $k, \beta$ such that a positive fundamental matrix $X(t)$ satisfies $\|X(t)\| \leq k$, $t \geq \beta$ and $$ \liminf_{t \rightarrow \infty} \int^t_\beta \operatorname{tr}(A(s))\,ds > - \infty.$$ Show that: a)$X^{-1}(t)$ is bounded in $[\beta,\infty)$. b)No system solution approaches zero solution when $t \rightarrow \infty.$","Let a system $x' = A(t)x$ and suppose there are values positives $k, \beta$ such that a positive fundamental matrix $X(t)$ satisfies $\|X(t)\| \leq k$, $t \geq \beta$ and $$ \liminf_{t \rightarrow \infty} \int^t_\beta \operatorname{tr}(A(s))\,ds > - \infty.$$ Show that: a)$X^{-1}(t)$ is bounded in $[\beta,\infty)$. b)No system solution approaches zero solution when $t \rightarrow \infty.$",,"['analysis', 'ordinary-differential-equations', 'systems-of-equations']"
34,Change of variables for heat equation,Change of variables for heat equation,,How to make a change of variables to turn the equation $$\frac{\partial{u}}{\partial{t}}=D\frac{\partial^2{u}}{\partial{x}^2}+cu$$ back to the heat equation? Where can I read about change of variables? Thank you.,How to make a change of variables to turn the equation $$\frac{\partial{u}}{\partial{t}}=D\frac{\partial^2{u}}{\partial{x}^2}+cu$$ back to the heat equation? Where can I read about change of variables? Thank you.,,"['calculus', 'ordinary-differential-equations', 'reference-request', 'partial-differential-equations', 'heat-equation']"
35,Solve differential equation $f'(z) = e^{-2} (f(z/e))^2$,Solve differential equation,f'(z) = e^{-2} (f(z/e))^2,"I'm curious if there's a simple closed solution to the following DE and, if so, what it is. $$\begin{align} f'(z) &= e^{-2} (f(z/e))^2 \\ f(0) &= 1. \end{align}$$","I'm curious if there's a simple closed solution to the following DE and, if so, what it is. $$\begin{align} f'(z) &= e^{-2} (f(z/e))^2 \\ f(0) &= 1. \end{align}$$",,"['ordinary-differential-equations', 'functional-equations']"
36,Any methods of solving this system of ODE's?,Any methods of solving this system of ODE's?,,"I try to solve this system of ODE's: $$ \frac{dQ_1 (t)}{dt} =  - a \sin (\omega t) Q_2(t) + b \cos(\omega t) Q_3(t) $$ $$ \frac{dQ_2 (t)}{dt} = - a \sin (\omega t) Q_1 (t) - c Q_3(t) $$ $$ \frac{dQ_3 (t)}{dt} = c Q_2(t) - b \cos(\omega t) Q_1(t) $$ with constant $a, b, c$. In terms of vectors RHS is just cross production of $P = \{ c, b \cos(\omega t), a \sin(\omega t)\}$ and $Q$ vectors. Is there any analytical method to solve this system, at least approximately?.. For example, if $a, b, c \ll \omega$.","I try to solve this system of ODE's: $$ \frac{dQ_1 (t)}{dt} =  - a \sin (\omega t) Q_2(t) + b \cos(\omega t) Q_3(t) $$ $$ \frac{dQ_2 (t)}{dt} = - a \sin (\omega t) Q_1 (t) - c Q_3(t) $$ $$ \frac{dQ_3 (t)}{dt} = c Q_2(t) - b \cos(\omega t) Q_1(t) $$ with constant $a, b, c$. In terms of vectors RHS is just cross production of $P = \{ c, b \cos(\omega t), a \sin(\omega t)\}$ and $Q$ vectors. Is there any analytical method to solve this system, at least approximately?.. For example, if $a, b, c \ll \omega$.",,"['ordinary-differential-equations', 'systems-of-equations']"
37,Green's function for Bessel ODE,Green's function for Bessel ODE,,"I want to compute the Green's function for the Bessel ODE. Its from Arfken (7th ed, Problem # 10.1.5) $ x^2y''(x) + xy'(x) + (k^2x^2 - 1)y(x) = 0 $, subject to the boundary condition, $y(0) = 0$, and $y(1) = 0$. The solution are $J_1(kx)$ and $Y_1(kx)$. But how can I combine to form a linear combination to satisfy the boundary condition? And then compute the Wronskian, for computing A?","I want to compute the Green's function for the Bessel ODE. Its from Arfken (7th ed, Problem # 10.1.5) $ x^2y''(x) + xy'(x) + (k^2x^2 - 1)y(x) = 0 $, subject to the boundary condition, $y(0) = 0$, and $y(1) = 0$. The solution are $J_1(kx)$ and $Y_1(kx)$. But how can I combine to form a linear combination to satisfy the boundary condition? And then compute the Wronskian, for computing A?",,['ordinary-differential-equations']
38,How many $f(x)$ are possible satisfying $f(x)=f'(x)$ and $f(0)=f(1)=0$.,How many  are possible satisfying  and .,f(x) f(x)=f'(x) f(0)=f(1)=0,"Let $f:[0,1]\to\Bbb{R}$ be a fixed continuous function such that $f$ is differentiable on $(0,1)$ and $f(0)=f(1)=0$ . Then the equation $f(x)=f'(x)$ admits how many solutions? The only solution that I am getting is $y=0$ . This is because $y=y'$ implies $y=Ae^x$ , and $A=0$ when accounting for boundary conditions. However, I am not sure if this is the right answer. Most other examinees were saying that multiple such functions are possible. An explanation of this would be great. EDIT: The options were A. No solution $x\in (0,1)$ B. More than one solution $x\in (0,1)$ C. Exactly one solution $x\in (0,1)$ D. At least one solution $x\in (0,1)$ I feel that if C is right, then so is D!","Let be a fixed continuous function such that is differentiable on and . Then the equation admits how many solutions? The only solution that I am getting is . This is because implies , and when accounting for boundary conditions. However, I am not sure if this is the right answer. Most other examinees were saying that multiple such functions are possible. An explanation of this would be great. EDIT: The options were A. No solution B. More than one solution C. Exactly one solution D. At least one solution I feel that if C is right, then so is D!","f:[0,1]\to\Bbb{R} f (0,1) f(0)=f(1)=0 f(x)=f'(x) y=0 y=y' y=Ae^x A=0 x\in (0,1) x\in (0,1) x\in (0,1) x\in (0,1)","['calculus', 'ordinary-differential-equations', 'functions']"
39,"Let $f: [0,1] \to \Bbb R^+$ be continuous map then is it possible to have $\int_0^x f(t)dt \geq f(x)$?",Let  be continuous map then is it possible to have ?,"f: [0,1] \to \Bbb R^+ \int_0^x f(t)dt \geq f(x)","Let $f: [0,1] \to \Bbb R^+$ be continuous map then is it possible to have $\int_0^x f(t)dt \geq f(x)$? If such functions exists then what will be the cardinality of the set having these kind of functions? If I have $f$ is differentiable then we get $f(x) \leq e^x$. But for $e^x$ the condition will not be satisfied. So how can we proceed?","Let $f: [0,1] \to \Bbb R^+$ be continuous map then is it possible to have $\int_0^x f(t)dt \geq f(x)$? If such functions exists then what will be the cardinality of the set having these kind of functions? If I have $f$ is differentiable then we get $f(x) \leq e^x$. But for $e^x$ the condition will not be satisfied. So how can we proceed?",,"['real-analysis', 'analysis', 'ordinary-differential-equations']"
40,On definition of gamma function.,On definition of gamma function.,,"We all know that gamma function's definition is $$\Gamma \left( x \right) = \int\limits_0^\infty {s^{x - 1} e^{ - s} ds}$$ and it is divergent for $x<0$. Yesterday, I was studying about Bessel function and i came up with a dilemma. In Bessel function for negative numbers, i.e. $J_{-\alpha}$ they use a term like $\Gamma(m-\alpha+1)$ and when $m<\alpha-1$ series is defined although gamma function is undefined. Also in class there were questions like finding $\Gamma\left(-\frac{1}{2}\right)$ where it is a definite value? $$\Gamma\left(-\frac{1}{2}\right)=-2\sqrt{\pi}$$ but how is it well-defined, isn't it diverging? And how is it a negative number? Summarizing, how exactly is gamma function defined?","We all know that gamma function's definition is $$\Gamma \left( x \right) = \int\limits_0^\infty {s^{x - 1} e^{ - s} ds}$$ and it is divergent for $x<0$. Yesterday, I was studying about Bessel function and i came up with a dilemma. In Bessel function for negative numbers, i.e. $J_{-\alpha}$ they use a term like $\Gamma(m-\alpha+1)$ and when $m<\alpha-1$ series is defined although gamma function is undefined. Also in class there were questions like finding $\Gamma\left(-\frac{1}{2}\right)$ where it is a definite value? $$\Gamma\left(-\frac{1}{2}\right)=-2\sqrt{\pi}$$ but how is it well-defined, isn't it diverging? And how is it a negative number? Summarizing, how exactly is gamma function defined?",,"['ordinary-differential-equations', 'special-functions', 'gamma-function', 'bessel-functions']"
41,How to solve the ODE $y(y'(x)+a)=bx$,How to solve the ODE,y(y'(x)+a)=bx,"I've been very frustrated attempting to separate this guy. Since there are three separate items when you multiply through by the y, it is very difficult to make all sides work out to forms $f(y)dy$ and $f(x)dx$. I've attempted the trick of getting it in the form $y'dx/y$ to get the integral to be $\log(y)$, but haven't successfully separated it yet. Any help would be appreciated, I want to understand your attempt, not just apply it if you know what I mean. Also, initial condition is: $ y(0)=0$ EDIT: we can try $y=Cx$ and find that it is a valid solution for specific C values.","I've been very frustrated attempting to separate this guy. Since there are three separate items when you multiply through by the y, it is very difficult to make all sides work out to forms $f(y)dy$ and $f(x)dx$. I've attempted the trick of getting it in the form $y'dx/y$ to get the integral to be $\log(y)$, but haven't successfully separated it yet. Any help would be appreciated, I want to understand your attempt, not just apply it if you know what I mean. Also, initial condition is: $ y(0)=0$ EDIT: we can try $y=Cx$ and find that it is a valid solution for specific C values.",,['ordinary-differential-equations']
42,Greens function of 1-d forced wave equation,Greens function of 1-d forced wave equation,,"[ORIGINAL PROBLEM] You are given hat the Green's function $g(x,t,\xi, \phi)$ is $\frac{\partial^2g}{\partial t^2} - \frac{\partial^2g}{\partial x^2}=\delta(t-\tau)\delta(x-\xi)$ with $g(x,0,\xi, \tau)= \frac{\partial }{\partial t} g(x,0,\xi, \phi)=0$ Show that the fourier transform in x of the Green's function is given by $G(x,t,\xi, \phi)=\frac{e^{ik\xi}sink(t-\tau)H(t-\tau)}{k} $ where H(x) is the Heaviside function. I get that $\frac{\partial^2 \tilde{g}}{\partial t^2}-k^2 \tilde{g} = \delta(t-\tau)e^{-ik\xi}    $ so $\tilde{g}=Ae^{kt}+Be^{-kt}+C.F$ but I cannot see what the complementary function should be or where to proceed from here.[ORIGINAL PROBLEM] I would like a full worked solution for all of this question and in return I am offering a bounty.","[ORIGINAL PROBLEM] You are given hat the Green's function $g(x,t,\xi, \phi)$ is $\frac{\partial^2g}{\partial t^2} - \frac{\partial^2g}{\partial x^2}=\delta(t-\tau)\delta(x-\xi)$ with $g(x,0,\xi, \tau)= \frac{\partial }{\partial t} g(x,0,\xi, \phi)=0$ Show that the fourier transform in x of the Green's function is given by $G(x,t,\xi, \phi)=\frac{e^{ik\xi}sink(t-\tau)H(t-\tau)}{k} $ where H(x) is the Heaviside function. I get that $\frac{\partial^2 \tilde{g}}{\partial t^2}-k^2 \tilde{g} = \delta(t-\tau)e^{-ik\xi}    $ so $\tilde{g}=Ae^{kt}+Be^{-kt}+C.F$ but I cannot see what the complementary function should be or where to proceed from here.[ORIGINAL PROBLEM] I would like a full worked solution for all of this question and in return I am offering a bounty.",,"['calculus', 'ordinary-differential-equations', 'partial-differential-equations', 'fourier-analysis', 'wave-equation']"
43,How can we construct a differential equation from a system of differential equation?,How can we construct a differential equation from a system of differential equation?,,Suppose we have a linear differential equation of order $n$. All of us know how to write it down as a system of linear differential equation as $X' = A_{n \times n} X_{n \times 1}$. My question is about the converse. I am not getting a satisfactory answer anywhere. Suppose an arbitrary $n \times n$ matrix $A$ is given. How can we construct a linear differential equation of order $n$ whose matrix representation is $X' = AX ? Is it always possible? My intuition is NO . But I can not explain it properly. If possible please give an example explaining the process. If not possible give a proof or counterexample. Thank you for your help.,Suppose we have a linear differential equation of order $n$. All of us know how to write it down as a system of linear differential equation as $X' = A_{n \times n} X_{n \times 1}$. My question is about the converse. I am not getting a satisfactory answer anywhere. Suppose an arbitrary $n \times n$ matrix $A$ is given. How can we construct a linear differential equation of order $n$ whose matrix representation is $X' = AX ? Is it always possible? My intuition is NO . But I can not explain it properly. If possible please give an example explaining the process. If not possible give a proof or counterexample. Thank you for your help.,,"['linear-algebra', 'analysis', 'ordinary-differential-equations']"
44,Free-fall according to Newton's gravitation law,Free-fall according to Newton's gravitation law,,"Most analysis of free-fall assume that bodies fall with constant acceleration. If however one analyses  free-fall according to Newton's gravitation law, one is lead to a differential equation which I don't know how to solve. The differential equation one is lead to[1] is $\frac{d^2x}{dt^2}=-\frac{1}{x^2}$. Does this equation have a closed form solution with given initial conditions $x_0,v_0$? [1] If one picks the units so that $MG=1$, fixes one mass at $x=0$ and places the falling mass at some $x>0$.","Most analysis of free-fall assume that bodies fall with constant acceleration. If however one analyses  free-fall according to Newton's gravitation law, one is lead to a differential equation which I don't know how to solve. The differential equation one is lead to[1] is $\frac{d^2x}{dt^2}=-\frac{1}{x^2}$. Does this equation have a closed form solution with given initial conditions $x_0,v_0$? [1] If one picks the units so that $MG=1$, fixes one mass at $x=0$ and places the falling mass at some $x>0$.",,"['ordinary-differential-equations', 'physics']"
45,System of two ODEs with reverse sign,System of two ODEs with reverse sign,,Consider the ordinary differential equations $$\dfrac{d}{dt}x_1(t)=x_2(t)$$ $$\dfrac{d}{dt}x_2(t)=-x_1(t)$$ for $t\in \mathbb{R}$. What are the solutions? We have $\dfrac{d^2}{dt^2}x_1(t)=\dfrac{d}{dt} x_2(t)=-x_1(t)$ and also $\dfrac{d^2}{dt^2}x_2(t)=-\dfrac{d}{dt} x_1(t)=-x_2(t)$. So $x_1(t)$ and $x_2(t)$ satisfy the equation $\dfrac{d^2}{dt^2}u(t)+u(t)=0$. They could be cosine and sine. What else could they be?,Consider the ordinary differential equations $$\dfrac{d}{dt}x_1(t)=x_2(t)$$ $$\dfrac{d}{dt}x_2(t)=-x_1(t)$$ for $t\in \mathbb{R}$. What are the solutions? We have $\dfrac{d^2}{dt^2}x_1(t)=\dfrac{d}{dt} x_2(t)=-x_1(t)$ and also $\dfrac{d^2}{dt^2}x_2(t)=-\dfrac{d}{dt} x_1(t)=-x_2(t)$. So $x_1(t)$ and $x_2(t)$ satisfy the equation $\dfrac{d^2}{dt^2}u(t)+u(t)=0$. They could be cosine and sine. What else could they be?,,['ordinary-differential-equations']
46,Differential equation: autonomous system,Differential equation: autonomous system,,"This isn't homework. I have no idea what theorems I should be looking at to solve this. Guidance, partial and total solutions are all welcomed. Let $f$ be a locally lipschitz function in an open set $G\subseteq \Bbb R^n$. Consider the autonomous system $y'=f(y)$. Let $y(x,\xi)$ be the value at point $x$ of the maximal solution that satisfies the initial condition $y(0)=\xi$. Prove that the domain of $y(\cdot, y(s, \xi))$ is $I-s$, where $I$ is the domain of $y(\cdot ,\xi)$. Prove that for all $s, t$ such that $y(s, \xi)$ and $y(t+s, \xi)$ exist, then $y(t,y(s,\xi))$ also exists and $y(t,y(s,\xi))=y(t+s, \xi)$. If $y$ is a maximal solution and there exists $T>0$ such that $y(0)=y(T)$ and $f(y(0))\neq 0$, then $y$ is a periodic solution and not constant. If $y$ is a solution whose domain is $(a,+\infty)$, if $\eta:=\lim _{x\to +\infty}y(x)$ and $\eta \in G$, then $f(\eta)=0$. EDIT I found an alternative solution for 3. Please check my proof and give feedback in comments: let $u$ be the restriction of $y$ to $[0,T]$, now let $\overline u$ be the periodic extension of $u$ to $\mathbb R$. It is easy to see that $\overline u$ is a solution to the given differential equation. But since $f$ is locally lipschitz, $\overline u$ must coincide with $y$ wherever they are both defined. Since $y$ is a maximal solution, it must be $\overline u$, so $y$ is defined on $\mathbb R$.","This isn't homework. I have no idea what theorems I should be looking at to solve this. Guidance, partial and total solutions are all welcomed. Let $f$ be a locally lipschitz function in an open set $G\subseteq \Bbb R^n$. Consider the autonomous system $y'=f(y)$. Let $y(x,\xi)$ be the value at point $x$ of the maximal solution that satisfies the initial condition $y(0)=\xi$. Prove that the domain of $y(\cdot, y(s, \xi))$ is $I-s$, where $I$ is the domain of $y(\cdot ,\xi)$. Prove that for all $s, t$ such that $y(s, \xi)$ and $y(t+s, \xi)$ exist, then $y(t,y(s,\xi))$ also exists and $y(t,y(s,\xi))=y(t+s, \xi)$. If $y$ is a maximal solution and there exists $T>0$ such that $y(0)=y(T)$ and $f(y(0))\neq 0$, then $y$ is a periodic solution and not constant. If $y$ is a solution whose domain is $(a,+\infty)$, if $\eta:=\lim _{x\to +\infty}y(x)$ and $\eta \in G$, then $f(\eta)=0$. EDIT I found an alternative solution for 3. Please check my proof and give feedback in comments: let $u$ be the restriction of $y$ to $[0,T]$, now let $\overline u$ be the periodic extension of $u$ to $\mathbb R$. It is easy to see that $\overline u$ is a solution to the given differential equation. But since $f$ is locally lipschitz, $\overline u$ must coincide with $y$ wherever they are both defined. Since $y$ is a maximal solution, it must be $\overline u$, so $y$ is defined on $\mathbb R$.",,"['ordinary-differential-equations', 'solution-verification']"
47,how do I solve this seperable equation with so many terms?,how do I solve this seperable equation with so many terms?,,"Solve given differential equation by separation of variables $$\frac{dy}{dx}=\frac{xy+3x-y-3}{xy-2x+4y-8}$$ I started by multiplying each side by the denominator to get $$(xy-2x+4y-8) dy = (xy+3x-y-3) dx$$ Now that there is $xy$ on each side of the equation do they cancel off or no because they are being multiplied by $dy$ or $dx$. How do I proceed? I'm getting a little discourage because this is the third separable differential equation I have come across and they all have taken a long time and involved lots of writing and are error prone. Is that right, in general differential equations are long and tedious (e.g. involve two integratoins and more)?","Solve given differential equation by separation of variables $$\frac{dy}{dx}=\frac{xy+3x-y-3}{xy-2x+4y-8}$$ I started by multiplying each side by the denominator to get $$(xy-2x+4y-8) dy = (xy+3x-y-3) dx$$ Now that there is $xy$ on each side of the equation do they cancel off or no because they are being multiplied by $dy$ or $dx$. How do I proceed? I'm getting a little discourage because this is the third separable differential equation I have come across and they all have taken a long time and involved lots of writing and are error prone. Is that right, in general differential equations are long and tedious (e.g. involve two integratoins and more)?",,['ordinary-differential-equations']
48,Solve first order matrix differential equation,Solve first order matrix differential equation,,"If I have a differential equation $ y'(t)=A y(t)$ where A is a constant square matrix that is not diagonalizable(although it is surely possible to calculate the eigenvalues) and no initial condition is given. And now I am interested in the fundamental matrix. Is there a general method to determine this matrix? I do not want to use the exponential function and the Jordan normal form, as this is quite exhausting. Maybe there is also an ansatz possible as it is for the special case, where this differential equation is equivalent to an n-th order ode.  I saw a method where they calculated the eigenvalues of the matrix and depending on the multiplicity n of this eigenvalue they used an exponential term(with the eigenvalue) and in each component an n-th order polynomial as a possible ansatz. Though they only did this, when they were interested in a initial value problem, so with an initial condition and not for a general solution. I was asked to deliver an example: so $y'(t)=\begin{pmatrix} 3 & -4 \\ 1 & -1 \end{pmatrix} y(t)$ If somebody can construct a fundamental matrix for this system, than this should be sufficient","If I have a differential equation $ y'(t)=A y(t)$ where A is a constant square matrix that is not diagonalizable(although it is surely possible to calculate the eigenvalues) and no initial condition is given. And now I am interested in the fundamental matrix. Is there a general method to determine this matrix? I do not want to use the exponential function and the Jordan normal form, as this is quite exhausting. Maybe there is also an ansatz possible as it is for the special case, where this differential equation is equivalent to an n-th order ode.  I saw a method where they calculated the eigenvalues of the matrix and depending on the multiplicity n of this eigenvalue they used an exponential term(with the eigenvalue) and in each component an n-th order polynomial as a possible ansatz. Though they only did this, when they were interested in a initial value problem, so with an initial condition and not for a general solution. I was asked to deliver an example: so $y'(t)=\begin{pmatrix} 3 & -4 \\ 1 & -1 \end{pmatrix} y(t)$ If somebody can construct a fundamental matrix for this system, than this should be sufficient",,['calculus']
49,How to verify the order of DOPRI Runge-Kutta method?,How to verify the order of DOPRI Runge-Kutta method?,,"I've written code in Fortran based on the RK8(7)-13 method by Dormand and Prince to solve the system $\mathbf{y}'=\mathbf{f}(t,\mathbf{y})$. The method is Runge-Kutta 8$^\text{th}$ order with an embedded 7$^\text{th}$ order method; I want to verify that the error on the output agrees with this. For a method of order $p$ and timestep $h$, we expect the local truncation error to be $\mathcal{O}(h^{p+1})$. Therefore, if I advance the program by one time step and calculate $\text{error} = |y_{\text{RK}}-y_{\text{exact}}|$ (say I know $y_{\text{exact}}$) and plot $\log(\text{error})$ vs. $\log(h)$ for several values of $h$, the slope of the line should be 9. Here's the problem: it isn't. I've tried simple polynomials like $f(t,y) = 1$ and $f(t,y) = 2t$, but the slope of the line always seems to be between 1 and 2; this is also the case for $f(t,y) = y$. However, for $y'' + y = 0$ (which translates into the coupled system $\mathbf{f}(t,y_{1}, y_{2}) = (y_{2}, -y_{1})$), I found that the slope is almost exact 9 for larger values of $h$ - for small values the slope goes back to ~1. Furthermore, the magnitude of the error is $~10^{-20}$ for this small $h$. I was wondering if anyone had any ideas as to what the issue could be. My ideas: I've been misinformed as to what order means, and the method I've described is not representative of the order of the RK scheme. This is entirely possible, but the fact that one of the slopes was 9 seems too coincidental. Something to do with precision. This could explain why the small $h$ values have unreliable errors since I'm ""only"" carrying 30 digits of precision, but this wouldn't explain the disagreement for the polynomial $f$. Something specific to RK methods that I'm unaware of. I'm hoping it's this. Programming error. I'm hoping it isn't this. Mistake entering in the Butcher table. I'm really hoping it isn't this. Side note: I've also tried plotting the global error as a function of $h$, but the results were even stranger, so I think the problem may be more fundamental.","I've written code in Fortran based on the RK8(7)-13 method by Dormand and Prince to solve the system $\mathbf{y}'=\mathbf{f}(t,\mathbf{y})$. The method is Runge-Kutta 8$^\text{th}$ order with an embedded 7$^\text{th}$ order method; I want to verify that the error on the output agrees with this. For a method of order $p$ and timestep $h$, we expect the local truncation error to be $\mathcal{O}(h^{p+1})$. Therefore, if I advance the program by one time step and calculate $\text{error} = |y_{\text{RK}}-y_{\text{exact}}|$ (say I know $y_{\text{exact}}$) and plot $\log(\text{error})$ vs. $\log(h)$ for several values of $h$, the slope of the line should be 9. Here's the problem: it isn't. I've tried simple polynomials like $f(t,y) = 1$ and $f(t,y) = 2t$, but the slope of the line always seems to be between 1 and 2; this is also the case for $f(t,y) = y$. However, for $y'' + y = 0$ (which translates into the coupled system $\mathbf{f}(t,y_{1}, y_{2}) = (y_{2}, -y_{1})$), I found that the slope is almost exact 9 for larger values of $h$ - for small values the slope goes back to ~1. Furthermore, the magnitude of the error is $~10^{-20}$ for this small $h$. I was wondering if anyone had any ideas as to what the issue could be. My ideas: I've been misinformed as to what order means, and the method I've described is not representative of the order of the RK scheme. This is entirely possible, but the fact that one of the slopes was 9 seems too coincidental. Something to do with precision. This could explain why the small $h$ values have unreliable errors since I'm ""only"" carrying 30 digits of precision, but this wouldn't explain the disagreement for the polynomial $f$. Something specific to RK methods that I'm unaware of. I'm hoping it's this. Programming error. I'm hoping it isn't this. Mistake entering in the Butcher table. I'm really hoping it isn't this. Side note: I've also tried plotting the global error as a function of $h$, but the results were even stranger, so I think the problem may be more fundamental.",,"['ordinary-differential-equations', 'numerical-methods', 'runge-kutta-methods']"
50,Integration and fundamental Theorem of Calculus,Integration and fundamental Theorem of Calculus,,"I need some help with the following integration/use of fundamental theorem of calculus: $\displaystyle x(t) =  \int_{0}^{t} \exp (-2s)a(s) \ ds$, where $a(x) = \left\{      \begin{array}{lr}        -1 & : t \in [0,1]\\        1& : t \in (1, \infty)      \end{array}    \right.$ How should I simplify the integral?","I need some help with the following integration/use of fundamental theorem of calculus: $\displaystyle x(t) =  \int_{0}^{t} \exp (-2s)a(s) \ ds$, where $a(x) = \left\{      \begin{array}{lr}        -1 & : t \in [0,1]\\        1& : t \in (1, \infty)      \end{array}    \right.$ How should I simplify the integral?",,['calculus']
51,Eigenvalues of a Sturm-Liouville problem,Eigenvalues of a Sturm-Liouville problem,,"Consider the Sturm-Liouville problem $$-u''(x) + V(x)u(x) = \lambda u(x)$$ for $u : \mathbb R \rightarrow \mathbb R$, $\lambda \in \mathbb R$. I am looking for a method to find the eigenvalues $\lambda$ which produce solutions $u \in L^2(\mathbb R)$ for various choices of $V$, e.g. $V(x) = c/\cosh^2(Cx)$ or $V(x) = e^x$. Is there a general procedure for doing this? If not, can someone provide suggestions for finding the eigenvalues at least in the two cases I mentioned?","Consider the Sturm-Liouville problem $$-u''(x) + V(x)u(x) = \lambda u(x)$$ for $u : \mathbb R \rightarrow \mathbb R$, $\lambda \in \mathbb R$. I am looking for a method to find the eigenvalues $\lambda$ which produce solutions $u \in L^2(\mathbb R)$ for various choices of $V$, e.g. $V(x) = c/\cosh^2(Cx)$ or $V(x) = e^x$. Is there a general procedure for doing this? If not, can someone provide suggestions for finding the eigenvalues at least in the two cases I mentioned?",,['ordinary-differential-equations']
52,How to find the general solution of $(1+x^2)y''+2xy'-2y=0$. How to express by means of elementary functions?,How to find the general solution of . How to express by means of elementary functions?,(1+x^2)y''+2xy'-2y=0,"Find the general solution of  $$(1+x^2)y''+2xy'-2y=0$$ in terms of power series in $x$.  Can you express this solution by means   of elementary functions? I know that $y= \displaystyle\sum_{n=0}^{ \infty } a_nx^n$ and $y'= \displaystyle\sum_{n=1}^{ \infty } a_nnx^{n-1}$ and  $y''=\displaystyle\sum_{n=2}^{ \infty } a_nn(n-1)x^{n-2}$. As far as I can tell I am to simply plug these summations in the original equation stated above. Also, I shifted the following to get $x^n$. Giving: $$\sum_{n=0}^{ \infty } a_{n+2}(n+2)(n+1)x^{n}+\sum_{n=2}^{ \infty } a_nn(n-1)x^{n}+\sum_{n=1}^{ \infty }2 a_nnx^{n}-\sum_{n=0}^{ \infty } 2a_nx^n=0$$ Now, I combine the equation into a single summation. $$\sum_{n=2}^{ \infty }\bigg[a_{n+2}(n+2)(n+1)+a_nn(n-1)+2a_nn-2a_n\bigg]x^n=0$$ Doing this, I am left over with $2(a_2-a_0)=0$ and $6a_3x=0$.  I can also form $a_{n+2}=a_n\frac{1-n}{n+1}$  Following this I plugged values in for the third equation, but this is where things start to get difficult.  I'm honestly not sure how to continue.  The question itself confuses me specifically ""in terms of power series in $x$.  Can you express this solution by means of elementary functions?"".","Find the general solution of  $$(1+x^2)y''+2xy'-2y=0$$ in terms of power series in $x$.  Can you express this solution by means   of elementary functions? I know that $y= \displaystyle\sum_{n=0}^{ \infty } a_nx^n$ and $y'= \displaystyle\sum_{n=1}^{ \infty } a_nnx^{n-1}$ and  $y''=\displaystyle\sum_{n=2}^{ \infty } a_nn(n-1)x^{n-2}$. As far as I can tell I am to simply plug these summations in the original equation stated above. Also, I shifted the following to get $x^n$. Giving: $$\sum_{n=0}^{ \infty } a_{n+2}(n+2)(n+1)x^{n}+\sum_{n=2}^{ \infty } a_nn(n-1)x^{n}+\sum_{n=1}^{ \infty }2 a_nnx^{n}-\sum_{n=0}^{ \infty } 2a_nx^n=0$$ Now, I combine the equation into a single summation. $$\sum_{n=2}^{ \infty }\bigg[a_{n+2}(n+2)(n+1)+a_nn(n-1)+2a_nn-2a_n\bigg]x^n=0$$ Doing this, I am left over with $2(a_2-a_0)=0$ and $6a_3x=0$.  I can also form $a_{n+2}=a_n\frac{1-n}{n+1}$  Following this I plugged values in for the third equation, but this is where things start to get difficult.  I'm honestly not sure how to continue.  The question itself confuses me specifically ""in terms of power series in $x$.  Can you express this solution by means of elementary functions?"".",,"['ordinary-differential-equations', 'power-series']"
53,Looking for a book on Differential Equations *with solutions*,Looking for a book on Differential Equations *with solutions*,,"I'm studying differential equations (specifically Laplace Transforms) right now with my college assigned 'Differential Equations with Application and Historical Notes'-George F Simmons. While I like the text, I'm not a big fan of the fact that there are not many solved examples and a solution manual isn't available. Can you guys suggest a decent book with a good range of questions (easy as well as difficult) which most importantly, has a solution manual available to refer to, after I've dwelled on a question for hours and still haven't figured it out.","I'm studying differential equations (specifically Laplace Transforms) right now with my college assigned 'Differential Equations with Application and Historical Notes'-George F Simmons. While I like the text, I'm not a big fan of the fact that there are not many solved examples and a solution manual isn't available. Can you guys suggest a decent book with a good range of questions (easy as well as difficult) which most importantly, has a solution manual available to refer to, after I've dwelled on a question for hours and still haven't figured it out.",,"['calculus', 'reference-request', 'ordinary-differential-equations']"
54,Time required to reach the goal when an object will be slowing down incrementally based on distance travelled?,Time required to reach the goal when an object will be slowing down incrementally based on distance travelled?,,"I was thinking about this when flying on the plane which was approaching and slowing down. Assume an object is approaching its target which is at a certain initial distance d at time t0. It starts at a speed that will allow it to reach the target in exactly one hour (e.g. d=100km, it starts at 100 km/h). Incrementally, it will slow down so that at every point in time, it will be exactly one hour far from reaching the target (after it had travelled 40 km being 60 km away, it will be travelling at 60 km/h). After reaching a predefined minimum speed (10 km/h), it will keep its velocity constant (and it will need exactly one additional hour to reach the target). How long will it take to reach the target? I somehow assume the answer should be 2 hours, independently of initial distance, but it does not fit (because then it would not matter what the original distance is (which it should not since we are travelling faster in the beginning), but any shorter path is included in the longer path and the resulting equation cannot possibly be true (or can it be?)) I think I am missing the mathematical apparatus that is needed to solve this (is it differential equations?) Can you please advise how to solve this?","I was thinking about this when flying on the plane which was approaching and slowing down. Assume an object is approaching its target which is at a certain initial distance d at time t0. It starts at a speed that will allow it to reach the target in exactly one hour (e.g. d=100km, it starts at 100 km/h). Incrementally, it will slow down so that at every point in time, it will be exactly one hour far from reaching the target (after it had travelled 40 km being 60 km away, it will be travelling at 60 km/h). After reaching a predefined minimum speed (10 km/h), it will keep its velocity constant (and it will need exactly one additional hour to reach the target). How long will it take to reach the target? I somehow assume the answer should be 2 hours, independently of initial distance, but it does not fit (because then it would not matter what the original distance is (which it should not since we are travelling faster in the beginning), but any shorter path is included in the longer path and the resulting equation cannot possibly be true (or can it be?)) I think I am missing the mathematical apparatus that is needed to solve this (is it differential equations?) Can you please advise how to solve this?",,['ordinary-differential-equations']
55,Finding equilibrium points of a differential equation,Finding equilibrium points of a differential equation,,"OK, this is a weak question, but I'm a bit rusty on differential equations, so here it is: I need to find the general solution of x'=ax+3 and then find the equilibrium points as well as which points are equilibria sinks and sources (a is a parameter). I found the general solution of x(t). It was: $x(t)=Ce^{at}-\frac{3}{a}$ Now, from the one or two lines in the text about equilibrium points, I'm seeing that that will be a ""value"" that makes x(t) = 0. Though, the way it's worded in the text makes it seem like a choice of ""a"" or ""C"" will be what makes an equilibrium solution. This is where I'm confused. Do I solve the general solution for t by setting x = 0? Do I solve the original equation finding setting x' to 0 and finding x (this is what I saw on some website). I'm just not sure what I'm supposed to do to find the equilibrium points, and then how to decide which are sinks and which are sources.","OK, this is a weak question, but I'm a bit rusty on differential equations, so here it is: I need to find the general solution of x'=ax+3 and then find the equilibrium points as well as which points are equilibria sinks and sources (a is a parameter). I found the general solution of x(t). It was: $x(t)=Ce^{at}-\frac{3}{a}$ Now, from the one or two lines in the text about equilibrium points, I'm seeing that that will be a ""value"" that makes x(t) = 0. Though, the way it's worded in the text makes it seem like a choice of ""a"" or ""C"" will be what makes an equilibrium solution. This is where I'm confused. Do I solve the general solution for t by setting x = 0? Do I solve the original equation finding setting x' to 0 and finding x (this is what I saw on some website). I'm just not sure what I'm supposed to do to find the equilibrium points, and then how to decide which are sinks and which are sources.",,['ordinary-differential-equations']
56,Proving $|x(t)|\lt |t|$ for the solutions $x$ of an ODE,Proving  for the solutions  of an ODE,|x(t)|\lt |t| x,"The problem is: Let $x(t)$ a solution of $$x'=-x(t^2-x^2),$$ so that $|x(t_0)|\lt |t_0|$. Show that for all $|t|\gt |t_0|$, $|x(t)|\lt |t|$. Suppose that $t_0\geq 0$. Consider Then $(t_0,x(t_0))$ is over the red line. I don't see why: $x$ solution of the above ODE implies that the graph of $x$ stay between the blue lines to the right of the red line and to the left of the orange line. Attempt. The attempt becomes an answer...","The problem is: Let $x(t)$ a solution of $$x'=-x(t^2-x^2),$$ so that $|x(t_0)|\lt |t_0|$. Show that for all $|t|\gt |t_0|$, $|x(t)|\lt |t|$. Suppose that $t_0\geq 0$. Consider Then $(t_0,x(t_0))$ is over the red line. I don't see why: $x$ solution of the above ODE implies that the graph of $x$ stay between the blue lines to the right of the red line and to the left of the orange line. Attempt. The attempt becomes an answer...",,"['ordinary-differential-equations', 'inequality']"
57,Continually Compounded Interest + Addition to Principal,Continually Compounded Interest + Addition to Principal,,"This is essentially the continually compounded version of this question . I want to know how much money I will have after continually compounding interest, plus continually adding a fixed amount to the principal. Let t be time in years, S be amount saved per month (added to the principal), R be APR, and x be the current amount of money. I'm going to assume no initial investment. I started off with this equation: $$ x + dx = 12 S dt + x e^{R dt} $$ Perhaps that is the wrong way to go about it. I don't even know what equations with bare dx's and dt's (instead of dx/dt's) are called, so I had trouble trying to figure this out. Moving things around, I got this equation, which I didn't know what to do with the dt in the denominator of the integral (Cancel out with the dt of the integral?). $$ \frac{dx}{dt} = 12 S - x \frac{1-e^{R dt}}{dt} $$ $$ x = 12 S t - \int{x \frac{1 - e^{R dt}}{dt}dt} $$","This is essentially the continually compounded version of this question . I want to know how much money I will have after continually compounding interest, plus continually adding a fixed amount to the principal. Let t be time in years, S be amount saved per month (added to the principal), R be APR, and x be the current amount of money. I'm going to assume no initial investment. I started off with this equation: $$ x + dx = 12 S dt + x e^{R dt} $$ Perhaps that is the wrong way to go about it. I don't even know what equations with bare dx's and dt's (instead of dx/dt's) are called, so I had trouble trying to figure this out. Moving things around, I got this equation, which I didn't know what to do with the dt in the denominator of the integral (Cancel out with the dt of the integral?). $$ \frac{dx}{dt} = 12 S - x \frac{1-e^{R dt}}{dt} $$ $$ x = 12 S t - \int{x \frac{1 - e^{R dt}}{dt}dt} $$",,"['calculus', 'ordinary-differential-equations', 'finance']"
58,What kind of ODE is this and how to solve it?,What kind of ODE is this and how to solve it?,,$$ (2x-4y+6)dx+(x+4y-3)dy=0 $$ I converted it to this form $ dy/dx = (2x-4y+6)/(x+4y-3) $ in the hope I can use $ z=y/x $ substitution for homogeneous ODEs but because of constants 6 and 3 I can't.,$$ (2x-4y+6)dx+(x+4y-3)dy=0 $$ I converted it to this form $ dy/dx = (2x-4y+6)/(x+4y-3) $ in the hope I can use $ z=y/x $ substitution for homogeneous ODEs but because of constants 6 and 3 I can't.,,['ordinary-differential-equations']
59,Essay about the art and applications of differential equations?,Essay about the art and applications of differential equations?,,"I teach a high school calculus class.  We've worked through the standard derivatives material, and I incorporated a discussion of antiderivatives throughout.  I've introduced solving ""area under a curve"" problems as solving differential equations.  Since it's easy to see the rate at which area is accumulating (the height of the function), we can write down a differential equation, take an antiderivative to find the area function, and solve for the constant. Anyhow, I find myself wanting to share with my students what differential equations are all about.  I don't know so much about them, but I have a sense that they are both a beautiful pure mathematics subject and a subject that has many, many applications. I'm hoping to hear suggestions for an essay or a chapter from a book--something like 3 to 10 pages--that would discuss differential equations as a subject, give some interesting examples, and point to some applications.  Any ideas?","I teach a high school calculus class.  We've worked through the standard derivatives material, and I incorporated a discussion of antiderivatives throughout.  I've introduced solving ""area under a curve"" problems as solving differential equations.  Since it's easy to see the rate at which area is accumulating (the height of the function), we can write down a differential equation, take an antiderivative to find the area function, and solve for the constant. Anyhow, I find myself wanting to share with my students what differential equations are all about.  I don't know so much about them, but I have a sense that they are both a beautiful pure mathematics subject and a subject that has many, many applications. I'm hoping to hear suggestions for an essay or a chapter from a book--something like 3 to 10 pages--that would discuss differential equations as a subject, give some interesting examples, and point to some applications.  Any ideas?",,"['calculus', 'reference-request', 'ordinary-differential-equations']"
60,Help in solving $xy'' -(1+x)y' + y = x^{2}e^{x}$,Help in solving,xy'' -(1+x)y' + y = x^{2}e^{x},"Can anyone help in solving the following problem: $$x \frac{d^{2}y}{dx^{2}} -(1+x) \cdot \frac{dy}{dx} + y = x^{2}\cdot e^{x}$$ I have gone through some methods where i reduce this to the standard form of $y"" + Py' + Qy = f(x)$ but didn't get how to solve this problem.","Can anyone help in solving the following problem: $$x \frac{d^{2}y}{dx^{2}} -(1+x) \cdot \frac{dy}{dx} + y = x^{2}\cdot e^{x}$$ I have gone through some methods where i reduce this to the standard form of $y"" + Py' + Qy = f(x)$ but didn't get how to solve this problem.",,['ordinary-differential-equations']
61,Why does it seem I can't apply the Radon transform to the Helmholtz equation?,Why does it seem I can't apply the Radon transform to the Helmholtz equation?,,"Say we a function $u$ and a bounded region $\Omega \subset \mathbb{R}^2$, such that $(\Delta+\lambda)u = 0$ everywhere, and $u=0$ on the boundary. We extend it to the entire plane by defining $u=0$ everywhere outside of $\Omega$. Wikipedia states that, for the Radon transform, $R\Delta = \partial^2 / \partial s^2 R$, hence we can derive the ODE $$ \left( \frac{\partial^2}{\partial s^2} + \lambda \right) Ru(\theta,s) = 0,$$ therefore we can reason that $$ Ru(\theta,s) = a(\theta) e^{i \sqrt{\lambda} s} + b(\theta) e^{-i \sqrt{\lambda} s}.$$ But this is periodic in $s$ with period $2\pi/\sqrt{\lambda}$, which would imply either that the above vanishes everywhere, or that it's possible to integrate $u$ over lines arbitrarily far away from $\Omega$ and still get a positive value, despite $u$ vanishing identically on said line. Contradiction. So does this mean: I can't use $R\Delta = \partial^2 / \partial s^2 R$ for functions that aren't sufficiently smooth, or The Radon transform is discontinuous at any line tangent to $\partial \Omega$?","Say we a function $u$ and a bounded region $\Omega \subset \mathbb{R}^2$, such that $(\Delta+\lambda)u = 0$ everywhere, and $u=0$ on the boundary. We extend it to the entire plane by defining $u=0$ everywhere outside of $\Omega$. Wikipedia states that, for the Radon transform, $R\Delta = \partial^2 / \partial s^2 R$, hence we can derive the ODE $$ \left( \frac{\partial^2}{\partial s^2} + \lambda \right) Ru(\theta,s) = 0,$$ therefore we can reason that $$ Ru(\theta,s) = a(\theta) e^{i \sqrt{\lambda} s} + b(\theta) e^{-i \sqrt{\lambda} s}.$$ But this is periodic in $s$ with period $2\pi/\sqrt{\lambda}$, which would imply either that the above vanishes everywhere, or that it's possible to integrate $u$ over lines arbitrarily far away from $\Omega$ and still get a positive value, despite $u$ vanishing identically on said line. Contradiction. So does this mean: I can't use $R\Delta = \partial^2 / \partial s^2 R$ for functions that aren't sufficiently smooth, or The Radon transform is discontinuous at any line tangent to $\partial \Omega$?",,"['analysis', 'ordinary-differential-equations', 'integral-transforms']"
62,Continuation of Solutions from Picard's existence of Solutions theory,Continuation of Solutions from Picard's existence of Solutions theory,,"Picard's existence and uniqueness theorem states that if a function satisfies some conditions on a particular interval then it has a solution in that interval. The function being f in the IVP: $$ \begin{cases} x'=f(x(t),t) \\ x(t_0)=x_0 \end{cases}$$ But then there exists a theorem that states that these solutions can be continued further by  repeating the argument with the old final time as the new initial time. I am looking for a detailed explanation and formulation of the proof for this theorem. Could somebody help me out? as I've been searching for quite a while now and can't seem to find anything.","Picard's existence and uniqueness theorem states that if a function satisfies some conditions on a particular interval then it has a solution in that interval. The function being f in the IVP: $$ \begin{cases} x'=f(x(t),t) \\ x(t_0)=x_0 \end{cases}$$ But then there exists a theorem that states that these solutions can be continued further by  repeating the argument with the old final time as the new initial time. I am looking for a detailed explanation and formulation of the proof for this theorem. Could somebody help me out? as I've been searching for quite a while now and can't seem to find anything.",,['ordinary-differential-equations']
63,Help with solving differential Equation using Exact Equation method,Help with solving differential Equation using Exact Equation method,,"I need to learn how to solve differential equations using either the Exact Equation Approach and or the Special Integrating Factor methods.  Below is a differential Equation to solve. $$(2xy^2 + \cos x) \text{d}x + (2x^2 y + \sin y)\text{d}y = 0$$ I would appreciate it if you would include comments to explain steps taken. Thanks in advance. Following your example I did the following Given $$ (2x + y)\mathrm dx + ( x - 2y)\mathrm dy = 0$$ a) $$ M(x,y)=2x + y, N(x,y)= x - 2y $$ b) check if the d.e is exact. $\frac{\partial M}{\partial y}=\frac{\partial}{\partial y}\left(2x + y\right)= 1 =\frac{\partial}{\partial x}\left(x - 2y\right)=\frac{dN}{\partial x}$ . c) $$ f\left(x,y\right)=\int M(x,y)\text{d}x =\int(2x + y)\text{d}x=x^{2} + xy +  g(y).$$ d) To find $g\left(y\right)$ $f_{y}\left(x,y\right)=\frac{\partial}{dy}\left(x^{2}+ xy + g(y)\right)=0 + x + g'\left(y\right).$ e) Upon  comparing with $N\left(x,y\right)$ , I find $g'\left(y\right)= - 2y$ which implies that $g\left(y\right)=- y^{2}+K$ Therefore, the general solution is $ x^2 + xy - y^2 =C$ .","I need to learn how to solve differential equations using either the Exact Equation Approach and or the Special Integrating Factor methods.  Below is a differential Equation to solve. I would appreciate it if you would include comments to explain steps taken. Thanks in advance. Following your example I did the following Given a) b) check if the d.e is exact. . c) d) To find e) Upon  comparing with , I find which implies that Therefore, the general solution is .","(2xy^2 + \cos x) \text{d}x + (2x^2 y + \sin y)\text{d}y = 0  (2x + y)\mathrm dx + ( x - 2y)\mathrm dy = 0  M(x,y)=2x + y, N(x,y)= x - 2y  \frac{\partial M}{\partial y}=\frac{\partial}{\partial y}\left(2x + y\right)= 1 =\frac{\partial}{\partial x}\left(x - 2y\right)=\frac{dN}{\partial x}  f\left(x,y\right)=\int M(x,y)\text{d}x =\int(2x + y)\text{d}x=x^{2} + xy +  g(y). g\left(y\right) f_{y}\left(x,y\right)=\frac{\partial}{dy}\left(x^{2}+ xy + g(y)\right)=0 + x + g'\left(y\right). N\left(x,y\right) g'\left(y\right)= - 2y g\left(y\right)=- y^{2}+K  x^2 + xy - y^2 =C","['calculus', 'integration', 'ordinary-differential-equations']"
64,"Does a ""Newton differential equation"" always have a solution?","Does a ""Newton differential equation"" always have a solution?",,"One reason why the iteration $$x_{n+1}=x_n-\tan\;x_n$$ converges quickly for appropriate starting values is that this is nothing more than the Newton-Raphson iteration for $\sin\;x$. This got me thinking: given some arbitrary function $g(x)$, is there always a function $f(x)$ such that $$\frac{f(x)}{f^\prime(x)}=g(x)$$ or are there restrictions on the nature of $g(x)$ so that the differential equation has a solution?","One reason why the iteration $$x_{n+1}=x_n-\tan\;x_n$$ converges quickly for appropriate starting values is that this is nothing more than the Newton-Raphson iteration for $\sin\;x$. This got me thinking: given some arbitrary function $g(x)$, is there always a function $f(x)$ such that $$\frac{f(x)}{f^\prime(x)}=g(x)$$ or are there restrictions on the nature of $g(x)$ so that the differential equation has a solution?",,[]
65,Correspondence between ODE and difference equation,Correspondence between ODE and difference equation,,"In Wikipedia about difference equations , there is some description about correspondence between ODE and difference equation: If you consider the Taylor series of   the solution to a linear differential   equation:$$\sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!} (x-a)^n$$ you   see that the coefficients of the   series are given by the nth derivative   of $f(x)$ evaluated at the point $a$. The   differential equation provides a   linear difference equation relating   these coefficients. The rule of thumb (for equations in   which the polynomial multiplying the   first term is non-zero at zero) is   that:$$ y^{[k]} \to f[n+k]$$and more   generally $$x^m*y^{[k]} \to n(n-1)(n-m+1)f[n+k-m]$$ Example: The recurrence relationship   for the Taylor series coefficients of   the equation: $$(x^2 + 3x -4)y^{[3]} -(3x+1)y^{[2]} + 2y = 0\,$$ is given by$$n(n-1)f[n+1] + 3nf[n+2] -4f[n+3] -3nf[n+1] -f[n+2]+ 2f[n] = 0\, $$ or $$-4f[n+3] +2nf[n+2] + n(n-4)f[n+1]  +2f[n] = 0.\,$$ My questions are: I was wondering what the rationale behind this conversion from an ODE to a difference equation is? Although having tried to read it several times, I was not able to understand it. In reverse direction, can a difference equation be converted to an ODE using this correspondence? How to? Is the conversion of an ODE into a difference equation in numerical methods for solving an ODE related to the correspondence between the two mentioned above? This equivalence can be used to   quickly solve for the recurrence   relationship for the coefficients in   the power series solution of a linear   differential equation. Problems generally solved using the   power series solution method taught in   normal differential equation classes   can be solved in a much easier way. I was wondering how exactly the correspondence can make solving an ODE or a difference equation easier? Thanks and regards!","In Wikipedia about difference equations , there is some description about correspondence between ODE and difference equation: If you consider the Taylor series of   the solution to a linear differential   equation:$$\sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!} (x-a)^n$$ you   see that the coefficients of the   series are given by the nth derivative   of $f(x)$ evaluated at the point $a$. The   differential equation provides a   linear difference equation relating   these coefficients. The rule of thumb (for equations in   which the polynomial multiplying the   first term is non-zero at zero) is   that:$$ y^{[k]} \to f[n+k]$$and more   generally $$x^m*y^{[k]} \to n(n-1)(n-m+1)f[n+k-m]$$ Example: The recurrence relationship   for the Taylor series coefficients of   the equation: $$(x^2 + 3x -4)y^{[3]} -(3x+1)y^{[2]} + 2y = 0\,$$ is given by$$n(n-1)f[n+1] + 3nf[n+2] -4f[n+3] -3nf[n+1] -f[n+2]+ 2f[n] = 0\, $$ or $$-4f[n+3] +2nf[n+2] + n(n-4)f[n+1]  +2f[n] = 0.\,$$ My questions are: I was wondering what the rationale behind this conversion from an ODE to a difference equation is? Although having tried to read it several times, I was not able to understand it. In reverse direction, can a difference equation be converted to an ODE using this correspondence? How to? Is the conversion of an ODE into a difference equation in numerical methods for solving an ODE related to the correspondence between the two mentioned above? This equivalence can be used to   quickly solve for the recurrence   relationship for the coefficients in   the power series solution of a linear   differential equation. Problems generally solved using the   power series solution method taught in   normal differential equation classes   can be solved in a much easier way. I was wondering how exactly the correspondence can make solving an ODE or a difference equation easier? Thanks and regards!",,"['ordinary-differential-equations', 'recurrence-relations']"
66,"How $\log(t)$ can be the limit of $t^r$, where $r\to 0$?","How  can be the limit of , where ?",\log(t) t^r r\to 0,"My class is solving the Cauchy-Euler differential equation $a t^2 y'' + b t y' + c y = 0$.  The solutions are powers of $t$, $y = t^r$ and then you solve for $r$ using the characteristic equation $a r^2 + (b-a) r + c = 0$.  This has two roots $r = \overline{r} \pm \Delta r$ and the general solution is $y = A t^{\overline{r} + \Delta r} + B t^{\overline{r} - \Delta r }$. What happens if we get double roots, i.e. $(b-a)^2 = 4ac$ or $\Delta r = 0$? We guess a solution of $y = t^r \ln t$.  Can derive this guess taking the limit of two distinct roots solution as $\Delta r$ tends to 0?  In some sense  $$ A t^{\overline{r} + \Delta r} + B t^{\overline{r} - \Delta r } = t^{\overline{r}} (  A t^{ \Delta r} + B t^{ - \Delta r }) \to  t^{\overline{r}} (C + D \ln t )$$ for some constants $A,B,C,D$. In a way, it is plausible the limit works this way because of the integral formula: $$ \int t^{r - 1}\mathrm dt = \begin{cases}  \frac{t^r}{r} & r \neq 0\\ \ln t  & r = 0 \end{cases}$$ And I expect this is coming into play in the Euler-Cauchy equation.  How to make it rigorous, though?","My class is solving the Cauchy-Euler differential equation $a t^2 y'' + b t y' + c y = 0$.  The solutions are powers of $t$, $y = t^r$ and then you solve for $r$ using the characteristic equation $a r^2 + (b-a) r + c = 0$.  This has two roots $r = \overline{r} \pm \Delta r$ and the general solution is $y = A t^{\overline{r} + \Delta r} + B t^{\overline{r} - \Delta r }$. What happens if we get double roots, i.e. $(b-a)^2 = 4ac$ or $\Delta r = 0$? We guess a solution of $y = t^r \ln t$.  Can derive this guess taking the limit of two distinct roots solution as $\Delta r$ tends to 0?  In some sense  $$ A t^{\overline{r} + \Delta r} + B t^{\overline{r} - \Delta r } = t^{\overline{r}} (  A t^{ \Delta r} + B t^{ - \Delta r }) \to  t^{\overline{r}} (C + D \ln t )$$ for some constants $A,B,C,D$. In a way, it is plausible the limit works this way because of the integral formula: $$ \int t^{r - 1}\mathrm dt = \begin{cases}  \frac{t^r}{r} & r \neq 0\\ \ln t  & r = 0 \end{cases}$$ And I expect this is coming into play in the Euler-Cauchy equation.  How to make it rigorous, though?",,"['calculus', 'real-analysis', 'ordinary-differential-equations']"
67,Periodic perturbation of ODE,Periodic perturbation of ODE,,"Recently, I have read Khasminskii's book (Stochastic Stability of Differential Equations). In Section 3.5, the author mentioned that the following is well-known in the theory of ODEs. If $x_0$ is an asymptotically stable equilibrium point of an autonomous system $\mathrm{d}x/\mathrm{d}t=F(x)$ and $f(t)$ is $\theta$ -periodic, then for sufficiently small $\epsilon$ the system $\mathrm{d}x/\mathrm{d}t=F(x)+\epsilon f(t)$ has a $\theta$ -periodic solution in a neighborhood of the equilibrium point. But I didn't find references about the fact. Could you give me some references or hints? Thanks!","Recently, I have read Khasminskii's book (Stochastic Stability of Differential Equations). In Section 3.5, the author mentioned that the following is well-known in the theory of ODEs. If is an asymptotically stable equilibrium point of an autonomous system and is -periodic, then for sufficiently small the system has a -periodic solution in a neighborhood of the equilibrium point. But I didn't find references about the fact. Could you give me some references or hints? Thanks!",x_0 \mathrm{d}x/\mathrm{d}t=F(x) f(t) \theta \epsilon \mathrm{d}x/\mathrm{d}t=F(x)+\epsilon f(t) \theta,"['ordinary-differential-equations', 'dynamical-systems', 'periodic-functions', 'perturbation-theory']"
68,Variation of parameters method is giving me the wrong answer.,Variation of parameters method is giving me the wrong answer.,,"I had a Differential Equations midterm yesterday with the problem: $$x^3y''+x^2y'-xy=\frac{x}{x+1}.$$ We were told beforehand that the homogeneous equation had a solution set $y_1(x)=x, y_2(x)=x^{-1}$ , so we were only supposed to find the particular solution using the Variation of parameters method. I started by solving for the Wronskian which should be $$W=\begin{vmatrix}x&\frac{1}{x}\\1&-\frac1{x^2}\end{vmatrix}=-\frac2x.$$ Now, as far as I know, we need to suppose that the particular solution is of the form $y=u_1(x)y_1(x)+u_2(x)y_2(x)$ , and we can calculate $u_1$ and $u_2$ in the following way: $$u_1=\int\frac{-y_2(x)g(x)}{W}dx\text{ and }u_2=\int\frac{y_1(x)g(x)}{W}dx,$$ giving me in this case that $$\begin{align}u_1&=\int\frac{-\frac{1}{x}\frac{x}{1+x}}{\frac{-2}{x}}dx\\&=\frac{1}{2}\int\frac{x}{1+x}dx=\frac{1}{2}(x+1-\ln|x+1|)\\u_2&=\int\frac{x\frac{x}{1+x}}{\frac{-2}{x}}dx\\&=-\frac12\int\frac{x^3}{1+x}dx\\&=-\frac12\left(\frac{(x+1)^3}{3}-\frac{3(x+1)^2}{2}+3(x+1)-\ln|x+1|\right).\end{align}$$ Thus, the particular solution should be $$y=\frac{x}{2}(x+1-\ln|x+1|)-\frac{1}{2x}\left(\frac{(x+1)^3}{3}-\frac{3(x+1)^2}{2}+3(x+1)\ln|x+1|\right).$$ I tried to check this solution using Mathematica, but when I plug it in the ODE, the expression simplifies to $\frac{x^4}{x+1}$ , not $\frac{x}{x+1}$ as the problem initially wanted. Also, in online calculators, the solution sometimes has a $\ln(x)$ that does not appear anywhere on my solution. I'm honestly really confused, I don't see at all where the mistake could be. Any help will be really appreciated.","I had a Differential Equations midterm yesterday with the problem: We were told beforehand that the homogeneous equation had a solution set , so we were only supposed to find the particular solution using the Variation of parameters method. I started by solving for the Wronskian which should be Now, as far as I know, we need to suppose that the particular solution is of the form , and we can calculate and in the following way: giving me in this case that Thus, the particular solution should be I tried to check this solution using Mathematica, but when I plug it in the ODE, the expression simplifies to , not as the problem initially wanted. Also, in online calculators, the solution sometimes has a that does not appear anywhere on my solution. I'm honestly really confused, I don't see at all where the mistake could be. Any help will be really appreciated.","x^3y''+x^2y'-xy=\frac{x}{x+1}. y_1(x)=x, y_2(x)=x^{-1} W=\begin{vmatrix}x&\frac{1}{x}\\1&-\frac1{x^2}\end{vmatrix}=-\frac2x. y=u_1(x)y_1(x)+u_2(x)y_2(x) u_1 u_2 u_1=\int\frac{-y_2(x)g(x)}{W}dx\text{ and }u_2=\int\frac{y_1(x)g(x)}{W}dx, \begin{align}u_1&=\int\frac{-\frac{1}{x}\frac{x}{1+x}}{\frac{-2}{x}}dx\\&=\frac{1}{2}\int\frac{x}{1+x}dx=\frac{1}{2}(x+1-\ln|x+1|)\\u_2&=\int\frac{x\frac{x}{1+x}}{\frac{-2}{x}}dx\\&=-\frac12\int\frac{x^3}{1+x}dx\\&=-\frac12\left(\frac{(x+1)^3}{3}-\frac{3(x+1)^2}{2}+3(x+1)-\ln|x+1|\right).\end{align} y=\frac{x}{2}(x+1-\ln|x+1|)-\frac{1}{2x}\left(\frac{(x+1)^3}{3}-\frac{3(x+1)^2}{2}+3(x+1)\ln|x+1|\right). \frac{x^4}{x+1} \frac{x}{x+1} \ln(x)",['ordinary-differential-equations']
69,what's wrong here ? in solving $y'+2y=1$,what's wrong here ? in solving,y'+2y=1,"I was dealing with Ordinary differential equation, $y'+2y=1$ In my book it is solved using seperable variable form, $\begin{equation}\begin{aligned}y'&=1-2y\\\frac{dy}{dx}&=1-2y\\\frac{dy}{1-2y}&=dx\\\int \frac{dy}{1-2y}&=\int dx\\\frac{\log(1-2y)}{-2}&=x+c\end{aligned}\end{equation}$ But what I did, $\begin{equation}\begin{aligned}y'&=1-2y\\\int y'dx&=\int 1-2ydx\\y&=x-2yx\\y+2xy&=x+c\\y(1+2x)&=x+c\\y&=\frac{x+c}{1+2x}\end{aligned}\end{equation}$ So I want to know, my way is also correct no? or where I am wrong..? Thanks in advance!!","I was dealing with Ordinary differential equation, In my book it is solved using seperable variable form, But what I did, So I want to know, my way is also correct no? or where I am wrong..? Thanks in advance!!",y'+2y=1 \begin{equation}\begin{aligned}y'&=1-2y\\\frac{dy}{dx}&=1-2y\\\frac{dy}{1-2y}&=dx\\\int \frac{dy}{1-2y}&=\int dx\\\frac{\log(1-2y)}{-2}&=x+c\end{aligned}\end{equation} \begin{equation}\begin{aligned}y'&=1-2y\\\int y'dx&=\int 1-2ydx\\y&=x-2yx\\y+2xy&=x+c\\y(1+2x)&=x+c\\y&=\frac{x+c}{1+2x}\end{aligned}\end{equation},['ordinary-differential-equations']
70,Linear independence in differential equations,Linear independence in differential equations,,"I'm currently taking linear algebra and differential equations class simultaneously. However, its not clear to me that why we need 2 linearly independent solutions for 2nd order linear equations. If it was 3rd order, would it be 3 and why? Like is it 2 dimensional when it becomes 2nd order? $$ay''+by'+cy=f(x)$$ Also, didn't get the reason behind solving for nonhomogenous functions, why do we only add 1 solution of  nonhomogenous to the homogenous solutions in order to get the general solution? . And is this limited with linear DE's?","I'm currently taking linear algebra and differential equations class simultaneously. However, its not clear to me that why we need 2 linearly independent solutions for 2nd order linear equations. If it was 3rd order, would it be 3 and why? Like is it 2 dimensional when it becomes 2nd order? Also, didn't get the reason behind solving for nonhomogenous functions, why do we only add 1 solution of  nonhomogenous to the homogenous solutions in order to get the general solution? . And is this limited with linear DE's?",ay''+by'+cy=f(x),"['linear-algebra', 'ordinary-differential-equations']"
71,Solve $y''+5y'+6y=e^{-3x}$,Solve,y''+5y'+6y=e^{-3x},"so I took the auxiliary equation and solved it: $$m^2+5m+6=0$$ gave me $$  m=-2,-3$$ and gives the complementary/general solution $$c_1e^{-2x}+c_2e^{-3x}$$ so I took $$axe^{-3x}$$ as the guess and got the particular solution as $$−xe^{−3x}$$ and adding that to the general solution gives $$c_1e^{-2x}+c_2e^{-3x}−xe^{−3x}$$",so I took the auxiliary equation and solved it: gave me and gives the complementary/general solution so I took as the guess and got the particular solution as and adding that to the general solution gives,"m^2+5m+6=0  
m=-2,-3 c_1e^{-2x}+c_2e^{-3x} axe^{-3x} −xe^{−3x} c_1e^{-2x}+c_2e^{-3x}−xe^{−3x}",['ordinary-differential-equations']
72,How to solve this ODE problem,How to solve this ODE problem,,"We have the following ODE: \begin{equation}     \begin{cases}     \frac{dy}{dx}=y(1-y)\\     y(0)=1/2     \end{cases} \end{equation} We can use the following equality: $$\frac{1}{y(1-y)}=1/2\bigg(\frac{1}{y}+\frac{1}{1-y}\bigg)$$ Then we do the following: $$\frac{dx}{dy}=\frac{1}{2y}+\frac{1}{2-2y}$$ $$dx=1/2\frac{1}{y}dy+\frac{1}{2-2y}dy$$ $$1/2\ln  y-1/2\ln (1-y)=x+C$$ $$C+x=\ln\frac{\sqrt{y}}{\sqrt{1-y}}$$ $$Ce^x=\frac{\sqrt{y}}{\sqrt{1-y}}$$ $$Ce^{2x}=\frac{y}{1-y}$$ But I am struggling to separate this. And then I find it even harder to answer the following: ""Note that y=1 is composed of critical points, determine with the use of the ODE if these CPs are stable"". This is difficult to know, because the system has no eigenmatrix, and without an eigenmatrix, the are no eigenvalues, so I cannot tell if the ""system"" is stable. Besides, it is not a ""system"" either. Any ideas? Thanks!","We have the following ODE: We can use the following equality: Then we do the following: But I am struggling to separate this. And then I find it even harder to answer the following: ""Note that y=1 is composed of critical points, determine with the use of the ODE if these CPs are stable"". This is difficult to know, because the system has no eigenmatrix, and without an eigenmatrix, the are no eigenvalues, so I cannot tell if the ""system"" is stable. Besides, it is not a ""system"" either. Any ideas? Thanks!","\begin{equation}
    \begin{cases}
    \frac{dy}{dx}=y(1-y)\\
    y(0)=1/2
    \end{cases}
\end{equation} \frac{1}{y(1-y)}=1/2\bigg(\frac{1}{y}+\frac{1}{1-y}\bigg) \frac{dx}{dy}=\frac{1}{2y}+\frac{1}{2-2y} dx=1/2\frac{1}{y}dy+\frac{1}{2-2y}dy 1/2\ln  y-1/2\ln (1-y)=x+C C+x=\ln\frac{\sqrt{y}}{\sqrt{1-y}} Ce^x=\frac{\sqrt{y}}{\sqrt{1-y}} Ce^{2x}=\frac{y}{1-y}",['ordinary-differential-equations']
73,Initial-value problem with a forcing function that has step discontinuity,Initial-value problem with a forcing function that has step discontinuity,,"Context I am solving an equation of motion derived from a Lagrangian that has a discontinuous step in its potential at the origin. In other words $$V(x) = \left(V_2-V_1\right)H(x) + V_1.$$ Based on this context, my question regards how to solve an initial-value problem to obtain $\dot{x}$ . Bare in mind that I can solve this problem using conservation of energy, but that won't teach me anything new with respect to how to utilize the Lagrangian formalism. Question I would like to solve the following initial value problem $$   m\,   \ddot{x}    =  - \left[V_2 - V_1\right]\delta(x)$$ with the initial conditions $$x(t_o) = x_o,~\text{where}~x_0<0,~\text{and}~\dot{x}(t_o) = v_o,~\text{where} ~v_0>0.$$ [In truth, I am interested only in $\dot x(t)$ ]. My Answer For all values $x$ excepting at $x=0$ , the mass is moving with constant velocity. As a consequence, I may write that $x=v\left(t-t_o\right)+x_o$ . Therefore, $$   m\,   \ddot{x}    =  - \left[V_2 - V_1\right]\delta\left(v\left(t-t_o\right)+x_o  \right).$$ Now, I integrate both sides with respect to time. I find $$ m\, \int    \ddot{x} \,dt   =  - \left[V_2 - V_1\right] \int   \delta\left(v\left(t-t_o\right)+x_0  \right)\,dt.$$ Upon integrating the left-hand side, and re-adjusting the right-hand side, I obtain $$ m\, \dot{x}  + k_1  =  - \left[V_2 - V_1\right] \int   \delta\left(v\,t-\left(v\,t_o-x_o\right)  \right)\,dt.$$ Now, I do a change of variables. Namely, that $s = v_o\,t$ . I find that $$ m\, \dot{x}  + k_1  =  - \left[V_2 - V_1\right] \int   \delta\left(s-\left(v_o\,t_o-x_o\right)  \right)\,\frac{ds}{v_o}.$$ Erroneously taking the integral on the right-hand side, I incorrectly find that $$ m\, \dot{x}  + k_1  =  - \frac{\left[V_2 - V_1\right]}{v_o} H\left(s-\left(v_o\,t_o-x_o\right)  \right) + k_2 .$$ Now, since $s = v_o\,t$ ,  I have that $$ m\, \dot{x}  + k_1  =  - \frac{\left[V_2 - V_1\right]}{v_o} H\left(v_o\left(t-t_o\right) +x_o \right) + k_2 .$$ Ultimately, for an as yet unknown constant $k$ , I arrive at $$  \dot{x}(t)    =  - \frac{\left[V_2 - V_1\right]}{m\,v_o} H\left(v_o\left(t-t_o\right) +x_o \right) + k  .$$ Upon satisfying the satisfy the boundary condition, $$ \dot{x}(t_o)    =  - \frac{\left[V_2 - V_1\right]}{m\,v_o} H\left(v_o\left(t -t_o\right) +x_o \right) + k = k = v_o .$$ So, I erroneously find that $$ \boxed{  \dot{x}(t)    =  v_o - \frac{\left[V_2 - V_1\right]}{m\,v_o} H\left(v_o\left(t-t_o\right) +x_o \right)  . } $$ Remarks I am not satisfied with my approach. It seems unwieldy, and is erroneous. If you have a more direct approach to solve the problem, then please consider posting it.","Context I am solving an equation of motion derived from a Lagrangian that has a discontinuous step in its potential at the origin. In other words Based on this context, my question regards how to solve an initial-value problem to obtain . Bare in mind that I can solve this problem using conservation of energy, but that won't teach me anything new with respect to how to utilize the Lagrangian formalism. Question I would like to solve the following initial value problem with the initial conditions [In truth, I am interested only in ]. My Answer For all values excepting at , the mass is moving with constant velocity. As a consequence, I may write that . Therefore, Now, I integrate both sides with respect to time. I find Upon integrating the left-hand side, and re-adjusting the right-hand side, I obtain Now, I do a change of variables. Namely, that . I find that Erroneously taking the integral on the right-hand side, I incorrectly find that Now, since ,  I have that Ultimately, for an as yet unknown constant , I arrive at Upon satisfying the satisfy the boundary condition, So, I erroneously find that Remarks I am not satisfied with my approach. It seems unwieldy, and is erroneous. If you have a more direct approach to solve the problem, then please consider posting it.","V(x) = \left(V_2-V_1\right)H(x) + V_1. \dot{x}    m\,   \ddot{x}    =  - \left[V_2 - V_1\right]\delta(x) x(t_o) = x_o,~\text{where}~x_0<0,~\text{and}~\dot{x}(t_o) = v_o,~\text{where} ~v_0>0. \dot x(t) x x=0 x=v\left(t-t_o\right)+x_o    m\,   \ddot{x}    =  - \left[V_2 - V_1\right]\delta\left(v\left(t-t_o\right)+x_o  \right).  m\, \int    \ddot{x} \,dt   =  - \left[V_2 - V_1\right] \int   \delta\left(v\left(t-t_o\right)+x_0  \right)\,dt.  m\, \dot{x}  + k_1  =  - \left[V_2 - V_1\right] \int   \delta\left(v\,t-\left(v\,t_o-x_o\right)  \right)\,dt. s = v_o\,t  m\, \dot{x}  + k_1  =  - \left[V_2 - V_1\right] \int   \delta\left(s-\left(v_o\,t_o-x_o\right)  \right)\,\frac{ds}{v_o}.  m\, \dot{x}  + k_1  =  - \frac{\left[V_2 - V_1\right]}{v_o} H\left(s-\left(v_o\,t_o-x_o\right)  \right) + k_2 . s = v_o\,t  m\, \dot{x}  + k_1  =  - \frac{\left[V_2 - V_1\right]}{v_o} H\left(v_o\left(t-t_o\right) +x_o \right) + k_2 . k   \dot{x}(t)    =  - \frac{\left[V_2 - V_1\right]}{m\,v_o} H\left(v_o\left(t-t_o\right) +x_o \right) + k  .  \dot{x}(t_o)    =  - \frac{\left[V_2 - V_1\right]}{m\,v_o} H\left(v_o\left(t -t_o\right) +x_o \right) + k = k = v_o . 
\boxed{
 \dot{x}(t)    =  v_o - \frac{\left[V_2 - V_1\right]}{m\,v_o} H\left(v_o\left(t-t_o\right) +x_o \right)  .
}
","['ordinary-differential-equations', 'physics', 'dirac-delta', 'initial-value-problems', 'euler-lagrange-equation']"
74,Can one separate variables as a sum in PDE?,Can one separate variables as a sum in PDE?,,"I always see solutions for some PDEs (e.g; wave equation) decomposed as a product, i.e; $u(x,y)=X(x)Y(y)$ for example. But I have never seen solutions written as $u(x,y)=X(x)+Y(y)$ for example. Is it possible to assume solutions of this kind and also construct the general solution using the superposition principle?","I always see solutions for some PDEs (e.g; wave equation) decomposed as a product, i.e; for example. But I have never seen solutions written as for example. Is it possible to assume solutions of this kind and also construct the general solution using the superposition principle?","u(x,y)=X(x)Y(y) u(x,y)=X(x)+Y(y)","['calculus', 'ordinary-differential-equations', 'partial-differential-equations']"
75,Possible Close-form solution for 2 dimensional $\frac{d\Sigma}{dt} = \mathbf{A}\Sigma + \Sigma \mathbf{A}^T + \mathbf{B}\mathbf{B}^T$,Possible Close-form solution for 2 dimensional,\frac{d\Sigma}{dt} = \mathbf{A}\Sigma + \Sigma \mathbf{A}^T + \mathbf{B}\mathbf{B}^T,"When $\mathbf{A},\mathbf{B}$ are 2x2 matrix and $\Sigma(t)$ are PSD, can we expect a close form solution for the following ODE, $$ \frac{d\Sigma}{dt} = \mathbf{A}\Sigma + \Sigma \mathbf{A}^T + \mathbf{B}\mathbf{B}^T. $$ The problem is from solving the covariance of a time-invariant SDE $$  x = \mathbf{A}x + \mathbf{B}dw. $$ When $x$ is in two dimensions, I guess there could be a close-form solution for $\Sigma$ . But I am not sure how to solve it.","When are 2x2 matrix and are PSD, can we expect a close form solution for the following ODE, The problem is from solving the covariance of a time-invariant SDE When is in two dimensions, I guess there could be a close-form solution for . But I am not sure how to solve it.","\mathbf{A},\mathbf{B} \Sigma(t) 
\frac{d\Sigma}{dt} = \mathbf{A}\Sigma + \Sigma \mathbf{A}^T + \mathbf{B}\mathbf{B}^T.
 
 x = \mathbf{A}x + \mathbf{B}dw.
 x \Sigma","['linear-algebra', 'ordinary-differential-equations', 'matrix-equations', 'matrix-calculus', 'stochastic-differential-equations']"
76,Showing there's no exotic solutions to this (kind of) differential equation,Showing there's no exotic solutions to this (kind of) differential equation,,"We want to solve $$ty'(t) = y$$ for all once-differentiable real functions. My attempt: It looks like a separable ODE, so first let's try solving it over an interval that doesn't contain $0$ , it should be easy then to extend it to 0 by continuity and differentiability. Unfortunately, we also have to divide by $y$ which could be $0$ . If we assume our solution doesn't pass through $0$ at $t\neq 0$ , it is easy to see the solutions are of form $$y = Ct, C\neq 0.$$ Another trivial solution would be of the above form with $C=0$ . Now we'd need to prove whether these are the only solutions, or if there is some other solution that passes through $0$ at some $t_0 \neq 0$ . What i've tried: Since $y(t_0)=0$ then plugging in the DE, $y'(t_0)=0$ . However now i have no idea how to proceed. Any hints? Is there a general technique for this kind of problem?","We want to solve for all once-differentiable real functions. My attempt: It looks like a separable ODE, so first let's try solving it over an interval that doesn't contain , it should be easy then to extend it to 0 by continuity and differentiability. Unfortunately, we also have to divide by which could be . If we assume our solution doesn't pass through at , it is easy to see the solutions are of form Another trivial solution would be of the above form with . Now we'd need to prove whether these are the only solutions, or if there is some other solution that passes through at some . What i've tried: Since then plugging in the DE, . However now i have no idea how to proceed. Any hints? Is there a general technique for this kind of problem?","ty'(t) = y 0 y 0 0 t\neq 0 y = Ct, C\neq 0. C=0 0 t_0 \neq 0 y(t_0)=0 y'(t_0)=0","['calculus', 'ordinary-differential-equations']"
77,Solving a Riccati equation - doesn't fit into any category I tried until now,Solving a Riccati equation - doesn't fit into any category I tried until now,,"I have this ODE: $$p = xp' - x(p^2 + x^2)$$ after dividing by $x \neq 0$ we get $$ p' = \frac{p}{x} + p^2 + x^2 $$ Which I recognized as a Riccati equation, and also confirmed this on WolframAlpha. However, here's the problem. We always dealt with three subtypes of the Riccati equation: a) For any Riccati equation $y' = P(x)y^2 + Q(x)y + R(x)$ , if P(x), Q(x) and R(x) are constants, then it is a separable equation b) If $y' = Ay^2 + \frac{B}{x}y + \frac{C}{x^2}$ , A,B,C are constants, we let $z=yx, z=z(x)$ c) If a particular solution $y_1$ is known, we let $y(x) = y_1(x) + \frac{1}{z(x)}$ However, my equation does not fall into any of the three categories I mentioned and I don't know how to proceed. Any help would be appreciated!","I have this ODE: after dividing by we get Which I recognized as a Riccati equation, and also confirmed this on WolframAlpha. However, here's the problem. We always dealt with three subtypes of the Riccati equation: a) For any Riccati equation , if P(x), Q(x) and R(x) are constants, then it is a separable equation b) If , A,B,C are constants, we let c) If a particular solution is known, we let However, my equation does not fall into any of the three categories I mentioned and I don't know how to proceed. Any help would be appreciated!","p = xp' - x(p^2 + x^2) x \neq 0  p' = \frac{p}{x} + p^2 + x^2  y' = P(x)y^2 + Q(x)y + R(x) y' = Ay^2 + \frac{B}{x}y + \frac{C}{x^2} z=yx, z=z(x) y_1 y(x) = y_1(x) + \frac{1}{z(x)}",['ordinary-differential-equations']
78,How do you go from an ODE to a Lagrangian?,How do you go from an ODE to a Lagrangian?,,"I know that given a Lagrangian, $L(t,x,v)$ you can write the Euler Lagrange equation $$L_x-\frac{d}{dt}L_v=0, \tag{1}$$ to minimize the functional $$\int_0^TL(t,x,v)dt.\tag{2}$$ I'm interested in the reverse direction - given an ODE say something like $$y''+f(t)y'+g(t)y+h(t)=0.\tag{3}$$ how can you find $L$ ?","I know that given a Lagrangian, you can write the Euler Lagrange equation to minimize the functional I'm interested in the reverse direction - given an ODE say something like how can you find ?","L(t,x,v) L_x-\frac{d}{dt}L_v=0, \tag{1} \int_0^TL(t,x,v)dt.\tag{2} y''+f(t)y'+g(t)y+h(t)=0.\tag{3} L","['ordinary-differential-equations', 'euler-lagrange-equation']"
79,Solving differential equation for density in a gas sphere,Solving differential equation for density in a gas sphere,,"I'm trying to derive the equation for density as a function of height in a gas sphere due to gravitational force, and I have derived the following equation: $$\frac{\mathrm{d}\rho}{\mathrm{d}z}=\frac{4 \pi G}{z^2RT}\rho(z)\int_{0}^{z}z'^2\rho(z')\mathrm{d}z'$$ Is there a way to solve it, even if numerically, for $\rho(z)$ ?","I'm trying to derive the equation for density as a function of height in a gas sphere due to gravitational force, and I have derived the following equation: Is there a way to solve it, even if numerically, for ?",\frac{\mathrm{d}\rho}{\mathrm{d}z}=\frac{4 \pi G}{z^2RT}\rho(z)\int_{0}^{z}z'^2\rho(z')\mathrm{d}z' \rho(z),"['calculus', 'integration', 'ordinary-differential-equations', 'derivatives']"
80,How does this implication follow? (Differential equations),How does this implication follow? (Differential equations),,"For an exam I have later today I am studying an old exam question which is accompanied by a solution. The question, and solution, are as follows: Consider the following initial value problem: $$x'(t) = (t + 1)e^{-x(t)}, t \geq 0, \text{with } x(0) = 1.$$ Multiplying both sides with $e^{x(t)}$ yields $x'(t)e^{x(t)} = t + 1$ and hence: $$\int_0^t x'(s)e^{x(s)}ds + c = \int_0^t(s + 1)ds \implies e^{x(t)} = \frac{(t + 1)^2 - 1}{2} + e^{x(0)}$$ Therefore, the solution of the initial value problem is: $$x(t) = \ln\biggl(\frac{(t + 1)^2 - 1}{2}\biggr), \enspace t \geq 0$$ The part that I don't understand is the ' $\implies$ ' part. How does this integral imply this $e^{x(t)} =$ ... solution? I apologize if this question is very trivial, however my professor has uploaded no examples where more steps are shown, and I was unable to find a similar question here or on other websites. Thank you for your time in advance!","For an exam I have later today I am studying an old exam question which is accompanied by a solution. The question, and solution, are as follows: Consider the following initial value problem: Multiplying both sides with yields and hence: Therefore, the solution of the initial value problem is: The part that I don't understand is the ' ' part. How does this integral imply this ... solution? I apologize if this question is very trivial, however my professor has uploaded no examples where more steps are shown, and I was unable to find a similar question here or on other websites. Thank you for your time in advance!","x'(t) = (t + 1)e^{-x(t)}, t \geq 0, \text{with } x(0) = 1. e^{x(t)} x'(t)e^{x(t)} = t + 1 \int_0^t x'(s)e^{x(s)}ds + c = \int_0^t(s + 1)ds \implies e^{x(t)} = \frac{(t + 1)^2 - 1}{2} + e^{x(0)} x(t) = \ln\biggl(\frac{(t + 1)^2 - 1}{2}\biggr), \enspace t \geq 0 \implies e^{x(t)} =",['ordinary-differential-equations']
81,A differential inequality with boundary values,A differential inequality with boundary values,,"Let $f:[0,1] \to \mathbb{R}$ be twice differentiable function on $(0,1)$ such that $f(0)=f(1)=0$ and $f''+2f'+f \ge 0$ Then which of the following values cannot be attained by $f$ ? $(a)\quad \pi$ $(b) \quad e$ $(c) \quad e^{\pi}$ $(d) \quad {\pi}^e$ My first thinking was to take equality with zero . Then $f(x)=(a+bx)e^{-x}$ whefe $a,b$ are arbitary constants With the boundary value , we have $a=0=b$ so $f=0$ . So no conclusiions Again taking the differential equation $f''+2f'+f=e^{\pi x}$ We have the general solution as $f(x)=(a+bx)e^{-x}+\frac{e^{\pi x  } }{(\pi+1)^2}$ With the boundary values $a=-\frac 1{(\pi+1)^2}$ and $b=\frac{e^{\pi+1}}{(\pi+1)^2}+   \frac 1{(\pi+1)^2} $ But what to conclude from this this? I am totally confused since I am new to this type of problem. Please help me solve this question. Thanks for your time.","Let be twice differentiable function on such that and Then which of the following values cannot be attained by ? My first thinking was to take equality with zero . Then whefe are arbitary constants With the boundary value , we have so . So no conclusiions Again taking the differential equation We have the general solution as With the boundary values and But what to conclude from this this? I am totally confused since I am new to this type of problem. Please help me solve this question. Thanks for your time.","f:[0,1] \to \mathbb{R} (0,1) f(0)=f(1)=0 f''+2f'+f \ge 0 f (a)\quad \pi (b) \quad e (c) \quad e^{\pi} (d) \quad {\pi}^e f(x)=(a+bx)e^{-x} a,b a=0=b f=0 f''+2f'+f=e^{\pi x} f(x)=(a+bx)e^{-x}+\frac{e^{\pi x 
} }{(\pi+1)^2} a=-\frac 1{(\pi+1)^2} b=\frac{e^{\pi+1}}{(\pi+1)^2}+ 
 \frac 1{(\pi+1)^2} ","['real-analysis', 'ordinary-differential-equations', 'inequality', 'boundary-value-problem']"
82,How to solve this system of ODE: $ u'= - \frac{2v}{t^2}$ and $v'=-u $?,How to solve this system of ODE:  and ?, u'= - \frac{2v}{t^2} v'=-u ,"I have this system of differential equations: $$ \left\{\begin{array}{ccc}u'&=& - \dfrac{2v}{t^2}\,, \\ v'&=&-u \,.\end{array}\right. $$ I want to find the general solution by deriving an Euler differential equation for $v$ and giving a fundamental system. So $v''= -u' \implies v''t^2-2v=0 $ . I am having issues solving this Euler differential equation.  How do I proceed?",I have this system of differential equations: I want to find the general solution by deriving an Euler differential equation for and giving a fundamental system. So . I am having issues solving this Euler differential equation.  How do I proceed?," \left\{\begin{array}{ccc}u'&=& - \dfrac{2v}{t^2}\,, \\ v'&=&-u
\,.\end{array}\right.  v v''= -u' \implies v''t^2-2v=0 ","['calculus', 'ordinary-differential-equations']"
83,solution to a general integral $\int_0^\infty \frac{\cos(tx)}{x^2+k^2}e^{-sx}dx$,solution to a general integral,\int_0^\infty \frac{\cos(tx)}{x^2+k^2}e^{-sx}dx,"I would like to find a general solution to the integral: $$I(s,t,k)=\int_0^\infty \frac{\cos(tx)}{x^2+k^2}e^{-sx}dx$$ so far using the substitution $u=\frac xk$ I have managed to reduce this to: $$I(s,t,k)=\frac 1k\int_0^\infty\frac{\cos(tku)}{u^2+1}e^{-sku}du$$ and then by defining $\alpha=tk,\beta=sk$ we can come up with a simpler integral: $$J(\alpha,\beta)=\int_0^\infty\frac{\cos(\alpha u)}{u^2+1}e^{-\beta u}du$$ We can calculate that: $$J_{\beta\beta}=\int_0^\infty\frac{u^2\cos(\alpha u)}{u^2+1}e^{-\beta u}du$$ $$=\int_0^\infty\cos(\alpha u)e^{-\beta u}du-J$$ $$=\frac{\beta}{\beta^2+\alpha^2}-J$$ $$J_{\alpha\alpha}=-J_{\beta\beta}$$ We now know that: $\nabla^2J=0$ Now to form a system of equations I found that: $$J(0,0)=\frac \pi2$$ $$J(\alpha,0)=\frac{\pi}{2}e^{-\alpha}$$ However I am struggling to find a solution to $J(0,\beta)$ although I know that it satisfies the equation: $$K''(\beta)+K(\beta)=\frac 1\beta,K(0)=\frac \pi2$$ It seems clear to me that $\lim_{\beta\to\infty}J(\alpha,\beta)=0$ so if I could solve for $K$ I should have everything I need to try and solve this problem. I think its obvious but I should add that: $$I(s,t,k)=\frac 1kJ(tk,sk)$$ Basically, could anyone help me find $J(0,\beta)$ or proceed with solving the pde I stated. Thanks! EDIT wolfram alpha gives: $$J(0,\beta)=\operatorname{Ci}(b)\sin(b)+\frac{\pi-2\operatorname{Si}(b)}{2}\cos(b)$$","I would like to find a general solution to the integral: so far using the substitution I have managed to reduce this to: and then by defining we can come up with a simpler integral: We can calculate that: We now know that: Now to form a system of equations I found that: However I am struggling to find a solution to although I know that it satisfies the equation: It seems clear to me that so if I could solve for I should have everything I need to try and solve this problem. I think its obvious but I should add that: Basically, could anyone help me find or proceed with solving the pde I stated. Thanks! EDIT wolfram alpha gives:","I(s,t,k)=\int_0^\infty \frac{\cos(tx)}{x^2+k^2}e^{-sx}dx u=\frac xk I(s,t,k)=\frac 1k\int_0^\infty\frac{\cos(tku)}{u^2+1}e^{-sku}du \alpha=tk,\beta=sk J(\alpha,\beta)=\int_0^\infty\frac{\cos(\alpha u)}{u^2+1}e^{-\beta u}du J_{\beta\beta}=\int_0^\infty\frac{u^2\cos(\alpha u)}{u^2+1}e^{-\beta u}du =\int_0^\infty\cos(\alpha u)e^{-\beta u}du-J =\frac{\beta}{\beta^2+\alpha^2}-J J_{\alpha\alpha}=-J_{\beta\beta} \nabla^2J=0 J(0,0)=\frac \pi2 J(\alpha,0)=\frac{\pi}{2}e^{-\alpha} J(0,\beta) K''(\beta)+K(\beta)=\frac 1\beta,K(0)=\frac \pi2 \lim_{\beta\to\infty}J(\alpha,\beta)=0 K I(s,t,k)=\frac 1kJ(tk,sk) J(0,\beta) J(0,\beta)=\operatorname{Ci}(b)\sin(b)+\frac{\pi-2\operatorname{Si}(b)}{2}\cos(b)","['integration', 'ordinary-differential-equations', 'partial-differential-equations', 'definite-integrals', 'leibniz-integral-rule']"
84,How to understand this polar coordinate transformation of ODE,How to understand this polar coordinate transformation of ODE,,"I am reading a textbook and it considers the following ODE: $$(u-x)u_x+u+x=0,$$ with $$u_x = \frac{\partial u}{\partial x}.$$ Use the following polar coordinate transformation $$x = r\cos\theta, \ \ \ \ \ u = r\sin\theta.$$ The ODE becomes $$\frac{dr}{d\theta}=r$$ How to obtain this final result?   Thanks in advanced.",I am reading a textbook and it considers the following ODE: with Use the following polar coordinate transformation The ODE becomes How to obtain this final result?   Thanks in advanced.,"(u-x)u_x+u+x=0, u_x = \frac{\partial u}{\partial x}. x = r\cos\theta, \ \ \ \ \ u = r\sin\theta. \frac{dr}{d\theta}=r","['ordinary-differential-equations', 'polar-coordinates']"
85,Can anyone solve this hard differential equation involving a derivative squared?,Can anyone solve this hard differential equation involving a derivative squared?,,"I have been trying to solve this diff. equation for quite some time now but haven't been able to do it correctly. It describes the lost height of the water $h$ at a certain time $t$ of a leaking reservoir. I have obtained this equation by using Torricelli's law as well as the law of continuity. At $t=0$ , the seal is removed and the reservoir starts leaking. Moreover, $A_O > A_G$ and to be less precise it can be assumed $A_O >> A_G$ , however, I would like to solve it as precisely as possible. The equation is: $$\frac{2gA_O^2}{A_O^2 - A_ G^2}h(t)+\frac{A_O^2}{A_G^2}\bigg(\frac{dh(t)}{dt}\bigg)^2 - \frac{2gA_O^2}{A_O^2 - A_G^2}H_0=0$$ with $h(0)=0$ . Additionally, $h(t)$ is bounded above by $H_0$ , is a strictly increasing function and thus also $\lim_{t \to \inf} h(t)=H_0$ The constants: $A_O$ is the surface of the top of the reservoir. $A_G$ is the surface area of the hole. $g$ is the gravitational acceleration. $H_0$ is the height of the water at t=0. The simplified formula would be: $$\lambda h(t)+\mu\left(\frac{dh(t)}{dt}\right)^2-\lambda H_0=0$$ And just to be clear $\bigg(\frac{dh(t)}{dt}\bigg)^2$ is simply the first derivative squared not the second derivative. I really hope someone can help me solve this! If there is anything unclear or you want more information please ask I will check this post regularly. Thanks in advance!","I have been trying to solve this diff. equation for quite some time now but haven't been able to do it correctly. It describes the lost height of the water at a certain time of a leaking reservoir. I have obtained this equation by using Torricelli's law as well as the law of continuity. At , the seal is removed and the reservoir starts leaking. Moreover, and to be less precise it can be assumed , however, I would like to solve it as precisely as possible. The equation is: with . Additionally, is bounded above by , is a strictly increasing function and thus also The constants: is the surface of the top of the reservoir. is the surface area of the hole. is the gravitational acceleration. is the height of the water at t=0. The simplified formula would be: And just to be clear is simply the first derivative squared not the second derivative. I really hope someone can help me solve this! If there is anything unclear or you want more information please ask I will check this post regularly. Thanks in advance!",h t t=0 A_O > A_G A_O >> A_G \frac{2gA_O^2}{A_O^2 - A_ G^2}h(t)+\frac{A_O^2}{A_G^2}\bigg(\frac{dh(t)}{dt}\bigg)^2 - \frac{2gA_O^2}{A_O^2 - A_G^2}H_0=0 h(0)=0 h(t) H_0 \lim_{t \to \inf} h(t)=H_0 A_O A_G g H_0 \lambda h(t)+\mu\left(\frac{dh(t)}{dt}\right)^2-\lambda H_0=0 \bigg(\frac{dh(t)}{dt}\bigg)^2,"['ordinary-differential-equations', 'physics']"
86,Periodic Solutions for the system,Periodic Solutions for the system,,"I'm having lots of troubles with proving that systems do not have periodic solutions. For example, PROBLEM: Prove that the system: $\dot x = A(t)x$ has no periodic solutions but the trivial , where $A(t) = \left [ \begin{matrix}     0 & 2+sin(t) \\     4+cos^{2}(t) & 0 \\    \end{matrix} \right ]$ My naive attempt has to try to calculate the solutions explicitly but I went no where. My other idea is find some criterion for perturbed systems (i.e $\dot x = (A+B(t))x$ ) which allows me to prove periodicity but I couldn't find any. Any help would be incredibly appreciated, thanks guys <3","I'm having lots of troubles with proving that systems do not have periodic solutions. For example, PROBLEM: Prove that the system: has no periodic solutions but the trivial , where My naive attempt has to try to calculate the solutions explicitly but I went no where. My other idea is find some criterion for perturbed systems (i.e ) which allows me to prove periodicity but I couldn't find any. Any help would be incredibly appreciated, thanks guys <3","\dot x = A(t)x A(t) = \left [ \begin{matrix}
    0 & 2+sin(t) \\
    4+cos^{2}(t) & 0 \\
   \end{matrix} \right ] \dot x = (A+B(t))x","['calculus', 'ordinary-differential-equations']"
87,How to solve the following ordinary differential equation?,How to solve the following ordinary differential equation?,,"Consider the ODE $$y' + x = \sqrt{x^2+y}.$$ I have no idea how to solve this equation. I have tried several ways but neither of them worked. Can anybody help me? Many thanks. I have tried substituting $u=\sqrt{x^2+y}$ , as well as squaring both sides of the ODE.","Consider the ODE I have no idea how to solve this equation. I have tried several ways but neither of them worked. Can anybody help me? Many thanks. I have tried substituting , as well as squaring both sides of the ODE.",y' + x = \sqrt{x^2+y}. u=\sqrt{x^2+y},['ordinary-differential-equations']
88,Differential Equations self-study,Differential Equations self-study,,"I have become a TA for a professor who teaches differential equations. The course is basically a self study course where students are to use their previous knowledge to explore and teach themselves differential equations. There is no book for the class which makes this hard for some students. I am reaching out to the community to see if there are any differential equation textbooks out there that really help a student self study differential equations. In doing some research I have found ""A First Course in Diﬀerential Equations with Modeling Applications by Dennis G. Zill"" as well as ""Fundamentals of Differential Equations by R. Kent Nagle"" to be somewhat decent. Thank you for your recommendations.","I have become a TA for a professor who teaches differential equations. The course is basically a self study course where students are to use their previous knowledge to explore and teach themselves differential equations. There is no book for the class which makes this hard for some students. I am reaching out to the community to see if there are any differential equation textbooks out there that really help a student self study differential equations. In doing some research I have found ""A First Course in Diﬀerential Equations with Modeling Applications by Dennis G. Zill"" as well as ""Fundamentals of Differential Equations by R. Kent Nagle"" to be somewhat decent. Thank you for your recommendations.",,"['ordinary-differential-equations', 'book-recommendation']"
89,List of definitions of the various types of ODE stability?,List of definitions of the various types of ODE stability?,,"I am studying numerical methods for solving ordinary differential equations (ODEs) and I keep on coming across different kinds of stability. I'm struggling to find the common thread between them. I was hoping that someone who knows this topic better than I do could draft a list of different kinds of stability of numerical ODE solvers along with their definitions. I'm guessing some are synonyms of each other. And others may be relevant only to the model problem. I'm just looking to discriminate which are which. Examples of types of stability I've come across include the following. However, this list is probably not exhaustive. Stability Zero-Stability A-Stability Absolute Stability Relative Stability Weak Stability Thank you!","I am studying numerical methods for solving ordinary differential equations (ODEs) and I keep on coming across different kinds of stability. I'm struggling to find the common thread between them. I was hoping that someone who knows this topic better than I do could draft a list of different kinds of stability of numerical ODE solvers along with their definitions. I'm guessing some are synonyms of each other. And others may be relevant only to the model problem. I'm just looking to discriminate which are which. Examples of types of stability I've come across include the following. However, this list is probably not exhaustive. Stability Zero-Stability A-Stability Absolute Stability Relative Stability Weak Stability Thank you!",,"['ordinary-differential-equations', 'numerical-methods', 'stability-in-odes', 'stability-theory']"
90,Where do we need absolute values when solving $u'(t) = \frac{u^2(t) - u(t)}{t}$,Where do we need absolute values when solving,u'(t) = \frac{u^2(t) - u(t)}{t},"Solve the differential equation $$ u'(t) = \frac{u^2(t) - u(t)}{t} $$ and then solve the initial value problem with $u(1) = \frac{1}{2}$ . I know the general solution is given by $u(t) = \frac{1}{c_1 t + 1}$ for all $t \in \mathbb{R}$ and some $c_1 \in \mathbb{R}$ determined by the initial condition, which is not always positive. My problem is that when I integrate $\frac{1}{t}$ when using separation of variables, is my result $\ln|t|$ or just $\ln(t)$ ? Here's what I've done so far: We exclude the constant solutions $u :\equiv 0$ and $u : \equiv 1$ Then our differential equation is equivalent to $$ \frac{u’(t)}{u^2(t) - u(t)} = \frac{1}{t}. $$ Integrating and substituting gives \begin{align*} \int_{u(t_0)}^{u(t)} \frac{1}{s(s - 1)} ds = \int_{t_0}^{t} \frac{1}{s} ds \implies & \int_{u(t_0)}^{u(t)} \frac{1}{s - 1}  - \frac{1}{s} ds = \ln\left| \frac{t}{t_0} \right| \\ \implies & \ln\left|\frac{u(t_0) ( u(t) - 1 )}{u(t) ( u(t_0) - 1)} \right| = \ln\left| \frac{t}{t_0} \right| \\ \implies & \frac{| u(t) |}{|u(t) - 1|} = \frac{|t_0| |u(t_0)|}{|t| | u(t_0) - 1|} \end{align*} Do I now do a case distinction or how can I solve for $u$ now?","Solve the differential equation and then solve the initial value problem with . I know the general solution is given by for all and some determined by the initial condition, which is not always positive. My problem is that when I integrate when using separation of variables, is my result or just ? Here's what I've done so far: We exclude the constant solutions and Then our differential equation is equivalent to Integrating and substituting gives Do I now do a case distinction or how can I solve for now?","
u'(t)
= \frac{u^2(t) - u(t)}{t}
 u(1) = \frac{1}{2} u(t) = \frac{1}{c_1 t + 1} t \in \mathbb{R} c_1 \in \mathbb{R} \frac{1}{t} \ln|t| \ln(t) u :\equiv 0 u : \equiv 1 
\frac{u’(t)}{u^2(t) - u(t)} = \frac{1}{t}.
 \begin{align*}
\int_{u(t_0)}^{u(t)} \frac{1}{s(s - 1)} ds
= \int_{t_0}^{t} \frac{1}{s} ds
\implies & \int_{u(t_0)}^{u(t)} \frac{1}{s - 1}  - \frac{1}{s} ds
= \ln\left| \frac{t}{t_0} \right| \\
\implies & \ln\left|\frac{u(t_0) ( u(t) - 1 )}{u(t) ( u(t_0) - 1)} \right|
= \ln\left| \frac{t}{t_0} \right| \\
\implies & \frac{| u(t) |}{|u(t) - 1|}
= \frac{|t_0| |u(t_0)|}{|t| | u(t_0) - 1|}
\end{align*} u","['integration', 'ordinary-differential-equations', 'logarithms']"
91,Does Gamma function a solution for known Ordinary differential equation?,Does Gamma function a solution for known Ordinary differential equation?,,"It is well known that gamma function's defined as : $$\Gamma \left( x \right) = \int\limits_0^\infty {s^{x - 1} e^{ - s} ds}$$ and it is divergent for $x<0$ . , Really I ask about differential equation which Gamma function satisfying it or by Other way : Does Gamma function a solution for known Ordinary differential equation and if yes what is it  ? For example if it obeyed any form of $F( \Gamma, \Gamma ', \dots, \Gamma^{(k)}) = 0$ ?","It is well known that gamma function's defined as : and it is divergent for . , Really I ask about differential equation which Gamma function satisfying it or by Other way : Does Gamma function a solution for known Ordinary differential equation and if yes what is it  ? For example if it obeyed any form of ?","\Gamma \left( x \right) = \int\limits_0^\infty {s^{x - 1} e^{ - s} ds} x<0 F( \Gamma, \Gamma ', \dots, \Gamma^{(k)}) = 0","['ordinary-differential-equations', 'gamma-function']"
92,"What is the solution of the equation $xyp^2 + (3x^2 - 2y^2)p - 6xy=0$, where $p = \frac{dx}{dy}$","What is the solution of the equation , where",xyp^2 + (3x^2 - 2y^2)p - 6xy=0 p = \frac{dx}{dy},"What is the solution of the equation $xyp^2 + (3x^2 - 2y^2)p - 6xy=0$ ,  where $p = \frac{dx}{dy}$ I was trying to solve it by dividing the whole equation by $xy$ and then integrate it $\frac{dy}{dx}[\frac{dy}{dx} + (\frac{3x}{y} - \frac{2y}{x})] = 6$ ,  but still this equation is non separable. Please tell me how to solve it.","What is the solution of the equation ,  where I was trying to solve it by dividing the whole equation by and then integrate it ,  but still this equation is non separable. Please tell me how to solve it.",xyp^2 + (3x^2 - 2y^2)p - 6xy=0 p = \frac{dx}{dy} xy \frac{dy}{dx}[\frac{dy}{dx} + (\frac{3x}{y} - \frac{2y}{x})] = 6,"['ordinary-differential-equations', 'nonlinear-system']"
93,Asymptotic behavior of solution to nonlinear ODE,Asymptotic behavior of solution to nonlinear ODE,,"I am trying to determine the asymptotic behavior as $t \to \infty$ of the solution to the IVP: $$y''(t) - y(t) + \frac{1}{[y(t)]^3} = 0\\y(0) = 1;\, y'(0) = 1$$ Without the nonlinearity, we have solutions of the form $e^{\pm t}$ .  I think the nonlinear solution has similar behavior, but I'm not sure how to show this or determine the precise asymptotic form.","I am trying to determine the asymptotic behavior as of the solution to the IVP: Without the nonlinearity, we have solutions of the form .  I think the nonlinear solution has similar behavior, but I'm not sure how to show this or determine the precise asymptotic form.","t \to \infty y''(t) - y(t) + \frac{1}{[y(t)]^3} = 0\\y(0) = 1;\, y'(0) = 1 e^{\pm t}",['ordinary-differential-equations']
94,Need to create a weird ODE,Need to create a weird ODE,,"Given $\epsilon>0$ , I need to create a first order ODE that is defined everywhere on $\mathbb{R}$ , such that all solutions to this ODE are only defined on an interval of length $\leq \epsilon$ . My attempt: $$x'(t) = \frac{1}{\log(10)}\left(\frac{1}{\epsilon/t -1}\right)\frac{\epsilon}{t}.$$ This has solutions $$ x(t) = \log_{10}\left( \frac{\epsilon}{t}-1 \right) + c $$ for $c\in\mathbb{R}$ . The problem is that my ODE and solutions are only defined on the interval $(0,\epsilon)$ , not all of $\mathbb{R}$ . My professor said that I was close, but my ODE should be autonomous so it is defined everywhere. There is no way that I can see how to extend my example to be what is desired. I do not think I am close at all. Any hints or help would be appreciated.","Given , I need to create a first order ODE that is defined everywhere on , such that all solutions to this ODE are only defined on an interval of length . My attempt: This has solutions for . The problem is that my ODE and solutions are only defined on the interval , not all of . My professor said that I was close, but my ODE should be autonomous so it is defined everywhere. There is no way that I can see how to extend my example to be what is desired. I do not think I am close at all. Any hints or help would be appreciated.","\epsilon>0 \mathbb{R} \leq \epsilon x'(t) = \frac{1}{\log(10)}\left(\frac{1}{\epsilon/t -1}\right)\frac{\epsilon}{t}.  x(t) = \log_{10}\left( \frac{\epsilon}{t}-1 \right) + c  c\in\mathbb{R} (0,\epsilon) \mathbb{R}","['real-analysis', 'ordinary-differential-equations', 'analysis']"
95,Is there any systematic way for transforming differential equation into Clairaut form?,Is there any systematic way for transforming differential equation into Clairaut form?,,"I have a few differential equations with their substitution to form Clairaut form, but is there any standard method to get the intuition of the substitution? examples: $y = 2xp + y^2 p^3$, for transforming , put $y^2 = Y$ $y + px = x^4 p^2$, for transforming , put $1/x = X$ $y = 3px + 6 y^2 p^2$,  for transforming , put $y^3 = Y$ and few other transformation cases. Note : $p = \frac{dy}{dx}$ I understand that reducing to Clairaut's form involves suitable substitution so as to bring it in the form of $Y = P X + f(P)$ but i am unable to form any intuition about what such substitutions might be , as the above equations seem complicated with more than one combination of variables and $p$. Help appreciated.","I have a few differential equations with their substitution to form Clairaut form, but is there any standard method to get the intuition of the substitution? examples: $y = 2xp + y^2 p^3$, for transforming , put $y^2 = Y$ $y + px = x^4 p^2$, for transforming , put $1/x = X$ $y = 3px + 6 y^2 p^2$,  for transforming , put $y^3 = Y$ and few other transformation cases. Note : $p = \frac{dy}{dx}$ I understand that reducing to Clairaut's form involves suitable substitution so as to bring it in the form of $Y = P X + f(P)$ but i am unable to form any intuition about what such substitutions might be , as the above equations seem complicated with more than one combination of variables and $p$. Help appreciated.",,['ordinary-differential-equations']
96,Relationship between fundamental matrix and exponential of a matrix,Relationship between fundamental matrix and exponential of a matrix,,"Let $A$ be an $n \times n$ matrix. I need to show that if $\Phi$ is a fundamental matrix of the system $$Y^{\prime}=AY $$ then there exists an invertible matrix B such that $\Phi = e^{At}B$, where $e^{At}$ is the exponential of the matrix $A$. I started out supposing I have $n$ linearly independent solutions $Y_{1},Y_{2},\cdots , Y_{n}$ to the system $Y^{\prime}=AY$. Then, $\Phi$ a fundamental matrix implies that $$\Phi = \begin{pmatrix} Y_{1} & Y_{2} & \cdots & Y_{n} \end{pmatrix}$$ But, I don't know where to go from here. Could somebody please help me with this proof? Thank you ahead of time for your time and patience.","Let $A$ be an $n \times n$ matrix. I need to show that if $\Phi$ is a fundamental matrix of the system $$Y^{\prime}=AY $$ then there exists an invertible matrix B such that $\Phi = e^{At}B$, where $e^{At}$ is the exponential of the matrix $A$. I started out supposing I have $n$ linearly independent solutions $Y_{1},Y_{2},\cdots , Y_{n}$ to the system $Y^{\prime}=AY$. Then, $\Phi$ a fundamental matrix implies that $$\Phi = \begin{pmatrix} Y_{1} & Y_{2} & \cdots & Y_{n} \end{pmatrix}$$ But, I don't know where to go from here. Could somebody please help me with this proof? Thank you ahead of time for your time and patience.",,"['linear-algebra', 'ordinary-differential-equations']"
97,$xy'=y\ln(xy)$ differential equations,differential equations,xy'=y\ln(xy),"I am really struggling with this problem. This section of my book is about solutions of differential equations through substitutions. We were taught about three methods of substitution in this section.  The first about Homogeneous equations. where you have something like  $$ M(x,y) + N(x,y) = 0$$ and  $$M(tx,ty) = t^\alpha *M(x,y)\; and\; N(tx,ty) = t^\alpha *N(x,y)$$ then using the substitution u=y/x or v=x/y the other method we learned was Bernoullis equation where you have the DE in the form  $$\frac{dy}{dx}+P(x)y = f(x)y^n$$ and the last we learned of was Reduction to separation of variables. Where you have $$\frac{dy}{dx}=f(Ax+By+C)$$ and you use subsitutions The problem states determine an appropriate substitution and solve $$xy'=yln(xy)$$ I found an answer on yahoo answers but I am lost in the steps. https://answers.yahoo.com/question/index?qid=20150420091352AA9lvDl The answer given is the same as the correct answer. Can anyone explain the steps especially the part where it transitions from $$xydu=xdy+ydx = y(u+1)dx$$ or tell me another approach.Thank you Edit: My teacher has told me that this problem was not of three types I mentioned above, which was confusing me as I was set on it having to be one of them. Thank you everyone for your time and solutions.","I am really struggling with this problem. This section of my book is about solutions of differential equations through substitutions. We were taught about three methods of substitution in this section.  The first about Homogeneous equations. where you have something like  $$ M(x,y) + N(x,y) = 0$$ and  $$M(tx,ty) = t^\alpha *M(x,y)\; and\; N(tx,ty) = t^\alpha *N(x,y)$$ then using the substitution u=y/x or v=x/y the other method we learned was Bernoullis equation where you have the DE in the form  $$\frac{dy}{dx}+P(x)y = f(x)y^n$$ and the last we learned of was Reduction to separation of variables. Where you have $$\frac{dy}{dx}=f(Ax+By+C)$$ and you use subsitutions The problem states determine an appropriate substitution and solve $$xy'=yln(xy)$$ I found an answer on yahoo answers but I am lost in the steps. https://answers.yahoo.com/question/index?qid=20150420091352AA9lvDl The answer given is the same as the correct answer. Can anyone explain the steps especially the part where it transitions from $$xydu=xdy+ydx = y(u+1)dx$$ or tell me another approach.Thank you Edit: My teacher has told me that this problem was not of three types I mentioned above, which was confusing me as I was set on it having to be one of them. Thank you everyone for your time and solutions.",,['ordinary-differential-equations']
98,Theta Method for system of ODEs,Theta Method for system of ODEs,,"So here is a question about a little Matlab code writing because I've never seen ""theta method""... Consider the $θ$-method   $$y_{n+1} = y_n + θhf_n + (1 − θ)hf_{n+1}$$   where $0 ≤ θ ≤ 1$ and $f_n = f(t_n, y_n)$. Write a MATLAB code to implement   the theta method for systems of ODEs. For $θ = 0$, $0.5$, $1$, use your code for solving   \begin{aligned} y_1' &= −y_1 \\ y_2' &= −100 \left(y_2 − \sin(t)\right) + \cos(t) \end{aligned}   for $0 ≤ t ≤ 1$, with initial value $y_1 = 1$, $y_2 = 2$. Try this for   stepsizes $h = .01$ and $h = .05$. Now I know how to do the Euler Code.... h = 0.05; %step size y1 = 1; %IC Conditions Y1 x1 = 2; %IC Condition Y2 t0 = 0; %Initial time t1 = 1; %Final time  nsteps = (t1-t0)/h; %number of steps  y(1) = y1; %Array for euler method Y1, -I.C Y1 x(1) = x1; %Array for euler method Y2  -I.C Y2 t(1) = t0 ; %array for euler method t  %% For loop and implementation of Euler Method  for n = 1:nsteps    t(n+1) = t(n)+h; %time stepping euler method, increment time    yprime(n+1) = -y(n); %Right hand side of diff eqy y1' = -y1    xprime(n+1) = -100*(x(n)-sin(t(n)))+cos(t(n)); %RHS of y2'= -100*(y2-sin(t))+cos(t)    y(n+1) = y(n)+yprime(n+1)*h;       x(n+1) = x(n)+xprime(n+1)*h; end Would I just be changing the last part of the code where I have this? y(n+1) = y(n)+yprime(n+1)*h;       x(n+1) = x(n)+xprime(n+1)*h; Essentially I would think I change that part above into the theta method of, $y_{n+1} = y_n + θhf_n + (1 − θ)hf_{n+1}$ If anyone is familiar with this method, please let me know how you would write this code, any feedback is helpful. Thank you","So here is a question about a little Matlab code writing because I've never seen ""theta method""... Consider the $θ$-method   $$y_{n+1} = y_n + θhf_n + (1 − θ)hf_{n+1}$$   where $0 ≤ θ ≤ 1$ and $f_n = f(t_n, y_n)$. Write a MATLAB code to implement   the theta method for systems of ODEs. For $θ = 0$, $0.5$, $1$, use your code for solving   \begin{aligned} y_1' &= −y_1 \\ y_2' &= −100 \left(y_2 − \sin(t)\right) + \cos(t) \end{aligned}   for $0 ≤ t ≤ 1$, with initial value $y_1 = 1$, $y_2 = 2$. Try this for   stepsizes $h = .01$ and $h = .05$. Now I know how to do the Euler Code.... h = 0.05; %step size y1 = 1; %IC Conditions Y1 x1 = 2; %IC Condition Y2 t0 = 0; %Initial time t1 = 1; %Final time  nsteps = (t1-t0)/h; %number of steps  y(1) = y1; %Array for euler method Y1, -I.C Y1 x(1) = x1; %Array for euler method Y2  -I.C Y2 t(1) = t0 ; %array for euler method t  %% For loop and implementation of Euler Method  for n = 1:nsteps    t(n+1) = t(n)+h; %time stepping euler method, increment time    yprime(n+1) = -y(n); %Right hand side of diff eqy y1' = -y1    xprime(n+1) = -100*(x(n)-sin(t(n)))+cos(t(n)); %RHS of y2'= -100*(y2-sin(t))+cos(t)    y(n+1) = y(n)+yprime(n+1)*h;       x(n+1) = x(n)+xprime(n+1)*h; end Would I just be changing the last part of the code where I have this? y(n+1) = y(n)+yprime(n+1)*h;       x(n+1) = x(n)+xprime(n+1)*h; Essentially I would think I change that part above into the theta method of, $y_{n+1} = y_n + θhf_n + (1 − θ)hf_{n+1}$ If anyone is familiar with this method, please let me know how you would write this code, any feedback is helpful. Thank you",,"['ordinary-differential-equations', 'numerical-methods', 'matlab']"
99,Possible to solve Algebraic Riccati Equation through ODE45?,Possible to solve Algebraic Riccati Equation through ODE45?,,"The Algebraic Riccati Equation is: $$A'X + XA - XBR^{-1}B'X + Q = 0$$ And the Algebraic Riccati Difference Equation is: $$A'X + XA - XBR^{-1}B'X + Q = \frac{dX}{dt}$$ Some times $\frac{dX}{dt}$ is just called $S$ in books. But is it not possible to solve the Algebraic Riccati Difference Equation through ODE45 and break the simulation when $-tol <= \frac{dX}{dt} <= tol$ ? You may wonder ""Why not use Schur's method to solve the Riccati equations?"". Well, Schur's method is a very bad method and there is a reason why MATLAB and Octave are not using it. Sure, the method find the solution $X$ to the riccati equation, but build a control law $L$ for the LQR controller by using that solution is not what I recommending. Right now I using fsolve, but that's a built in library in Octave and a function in Optimization package for MATLAB.","The Algebraic Riccati Equation is: $$A'X + XA - XBR^{-1}B'X + Q = 0$$ And the Algebraic Riccati Difference Equation is: $$A'X + XA - XBR^{-1}B'X + Q = \frac{dX}{dt}$$ Some times $\frac{dX}{dt}$ is just called $S$ in books. But is it not possible to solve the Algebraic Riccati Difference Equation through ODE45 and break the simulation when $-tol <= \frac{dX}{dt} <= tol$ ? You may wonder ""Why not use Schur's method to solve the Riccati equations?"". Well, Schur's method is a very bad method and there is a reason why MATLAB and Octave are not using it. Sure, the method find the solution $X$ to the riccati equation, but build a control law $L$ for the LQR controller by using that solution is not what I recommending. Right now I using fsolve, but that's a built in library in Octave and a function in Optimization package for MATLAB.",,"['ordinary-differential-equations', 'optimization', 'control-theory', 'optimal-control', 'linear-control']"
