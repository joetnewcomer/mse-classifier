,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Using Vieta's formula to evaluate $\frac{x_1}{x_2} + \frac{x_2}{x_3} + \frac{x_3}{x_1}$,Using Vieta's formula to evaluate,\frac{x_1}{x_2} + \frac{x_2}{x_3} + \frac{x_3}{x_1},"Using the cubic equation $ax^3 + bx^2 + cx +d = 0$ , $$x_1 + x_2 + x_3 = -\frac{b}{a},$$ $$x_1x_2 + x_2x_3 + x_1x_3 = \frac{c}{a},$$ $$x_1x_2x_3 = -\frac{d}{a},$$ How would one evaluate $\frac{x_1}{x_2} + \frac{x_2}{x_3} + \frac{x_3}{x_1}$ ? Edit: I'm at this point currently: $$\frac{x_1^2x_3 + x_1x_2^2 + x_2x_3^2}{x_1x_2x_3}$$ and don't know how to separate the single $x_1, x_2, x_3$ from the fraction.","Using the cubic equation , How would one evaluate ? Edit: I'm at this point currently: and don't know how to separate the single from the fraction.","ax^3 + bx^2 + cx +d = 0 x_1 + x_2 + x_3 = -\frac{b}{a}, x_1x_2 + x_2x_3 + x_1x_3 = \frac{c}{a}, x_1x_2x_3 = -\frac{d}{a}, \frac{x_1}{x_2} + \frac{x_2}{x_3} + \frac{x_3}{x_1} \frac{x_1^2x_3 + x_1x_2^2 + x_2x_3^2}{x_1x_2x_3} x_1, x_2, x_3",['analysis']
1,Why doesn't separate continuity imply continuity?,Why doesn't separate continuity imply continuity?,,"Suppose $f: U \rightarrow R$ for some open subset $U$ of $R^2$ is continuous in each variable ie. $f(- , y)$ continuous for each fixed y, and $f(x , -)$ continuous for each fixed x. I know the counterexample that $f = \frac{xy}{x^2 +y^2} $ for $(x,y) \neq (0,0)$ , $f = 0$ for $(x,y) = (0,0)$ is separately continuous but not continuous at the origin. Where does the following proof that it should be continuous fail? Suppose we try to show continuity at $(x_1 , y_1)$ . Then for any $(x_2, y_2)$ in $U$ , $|f(x_1 , y_1) -f(x_2 , y_2)| \leq |f(x_1 , y_1) -f(x_2 , y_1)| + |f(x_2 , y_1) -f(x_2 , y_2)|$ by the triangle inequality. Fix $\epsilon > 0$ . Then $\exists a>0$ s.t $|x_1 - x_2| < a \implies |f(x_1 , y_1) -f(x_2 , y_1)| < \epsilon $ . Similarly $\exists b>0$ s.t $|y_1 - y_2| < b \implies |f(x_2 , y_1) -f(x_2 , y_2)| < \epsilon $ . Let $\delta = min(a,b)$ , then for $|(x_1,y_1) - (x_2,y_2)|<\delta$ , we have $|f(x_1,y_1) - f(x_2,y_2)|< 2\epsilon$ . Done. Is it because whilst it may work for that particular choice of $(x_2, y_2)$ , there may be another choice, also within distance $\delta$ of $(x_1, y_1)$ , such that $|f(x_1,y_1) - f(x_2,y_2)| > \epsilon$ ? If I add the condition that $f$ is Lipschitz in $y$ , say, with Lipschitz constant independent of $y$ , how is this sufficient for continuity?","Suppose for some open subset of is continuous in each variable ie. continuous for each fixed y, and continuous for each fixed x. I know the counterexample that for , for is separately continuous but not continuous at the origin. Where does the following proof that it should be continuous fail? Suppose we try to show continuity at . Then for any in , by the triangle inequality. Fix . Then s.t . Similarly s.t . Let , then for , we have . Done. Is it because whilst it may work for that particular choice of , there may be another choice, also within distance of , such that ? If I add the condition that is Lipschitz in , say, with Lipschitz constant independent of , how is this sufficient for continuity?","f: U \rightarrow R U R^2 f(- , y) f(x , -) f = \frac{xy}{x^2 +y^2}  (x,y) \neq (0,0) f = 0 (x,y) = (0,0) (x_1 , y_1) (x_2, y_2) U |f(x_1 , y_1) -f(x_2 , y_2)| \leq |f(x_1 , y_1) -f(x_2 , y_1)| + |f(x_2 , y_1) -f(x_2 , y_2)| \epsilon > 0 \exists a>0 |x_1 - x_2| < a \implies |f(x_1 , y_1) -f(x_2 , y_1)| < \epsilon  \exists b>0 |y_1 - y_2| < b \implies |f(x_2 , y_1) -f(x_2 , y_2)| < \epsilon  \delta = min(a,b) |(x_1,y_1) - (x_2,y_2)|<\delta |f(x_1,y_1) - f(x_2,y_2)|< 2\epsilon (x_2, y_2) \delta (x_1, y_1) |f(x_1,y_1) - f(x_2,y_2)| > \epsilon f y y","['real-analysis', 'analysis', 'continuity', 'lipschitz-functions']"
2,"Show $(a, b) \sim\Bbb R$ for any interval $(a, b)$ [duplicate]",Show  for any interval  [duplicate],"(a, b) \sim\Bbb R (a, b)","This question already has an answer here : Prove that any two open intervals are equinumerous. (1 answer) Closed 3 months ago . Show $(a, b)$ has the same cardinality as $\Bbb R$ for any interval $(a, b)$ . From the previous chapters of the book ( Understanding Analysis by Stephen Abbott), I know that $(-1,1)$ has the same cardinality as $\Bbb R$ . I am willing to know is there any systematic way to show this or I have to guess some function $f: (a,b)\to(-1,1)$ that is 1-to-1 and onto?","This question already has an answer here : Prove that any two open intervals are equinumerous. (1 answer) Closed 3 months ago . Show has the same cardinality as for any interval . From the previous chapters of the book ( Understanding Analysis by Stephen Abbott), I know that has the same cardinality as . I am willing to know is there any systematic way to show this or I have to guess some function that is 1-to-1 and onto?","(a, b) \Bbb R (a, b) (-1,1) \Bbb R f: (a,b)\to(-1,1)","['analysis', 'real-numbers']"
3,$2$ equal improper integrals but one integral converges absolutely other is not,equal improper integrals but one integral converges absolutely other is not,2,I can show by parts $$\int_{0}^{\infty}\frac{\cos x}{1+x}dx=\int_{0}^{\infty}\frac{\sin x}{(1+x)^2}dx$$. But How to argue that one of them is converging absolutely and other is not? Is it possible to argue using integral test ? like $\int_{0}^{\infty}\frac{\sin x}{(1+x)^2}dx$ $\leq \int_{0}^{\infty}\frac{|\sin x|}{(1+x)^2}dx$ which is convergent  by integral test as $\sum_{i=0}^{\infty} \frac{1}{(1+x)^2}$ is convergent . Or there is other way to show .Any Help will be appreciated,I can show by parts $$\int_{0}^{\infty}\frac{\cos x}{1+x}dx=\int_{0}^{\infty}\frac{\sin x}{(1+x)^2}dx$$. But How to argue that one of them is converging absolutely and other is not? Is it possible to argue using integral test ? like $\int_{0}^{\infty}\frac{\sin x}{(1+x)^2}dx$ $\leq \int_{0}^{\infty}\frac{|\sin x|}{(1+x)^2}dx$ which is convergent  by integral test as $\sum_{i=0}^{\infty} \frac{1}{(1+x)^2}$ is convergent . Or there is other way to show .Any Help will be appreciated,,"['real-analysis', 'integration', 'analysis', 'improper-integrals', 'riemann-integration']"
4,"We square an integral, but why change a variable?","We square an integral, but why change a variable?",,"If we square an integral, we also change the integration variable in one of the integrals. But why is this actually correct? For example, say I have the following: Solve $\int_{-\infty}^\infty e^{-x^2} dx$. Let $I=\int_{-\infty}^\infty e^{-x^2} dx$, so \begin{align} I^2 &=\bigg(\int_{-\infty}^\infty e^{-x^2} dx\bigg)^2\\ &= \bigg(\int_{-\infty}^\infty e^{-x^2} dx\bigg) \times  \bigg( \underbrace{ \int_{-\infty}^{\infty} e^{-y^2}dy }_{\text{Why}?} \bigg)  \\ &=\int_{-\infty}^\infty\bigg(\int_{-\infty}^\infty e^{-(x^2+y^2)} dx\bigg)dy  \end{align} But why is the following wrong: \begin{align} I^2 &=\bigg(\int_{-\infty}^\infty e^{-x^2} dx\bigg)^2\\ &= \bigg(\int_{-\infty}^\infty e^{-x^2} dx\bigg) \times \bigg(\int_{-\infty}^{\infty} e^{-x^2}dx\bigg) \\ &=\int_{-\infty}^\infty\bigg(\int_{-\infty}^\infty e^{-x^2-x^2} dx\bigg)dx \\ &=\int_{-\infty}^\infty\bigg(\int_{-\infty}^\infty e^{-2x^2} dx\bigg)dx \qquad ? \end{align}","If we square an integral, we also change the integration variable in one of the integrals. But why is this actually correct? For example, say I have the following: Solve $\int_{-\infty}^\infty e^{-x^2} dx$. Let $I=\int_{-\infty}^\infty e^{-x^2} dx$, so \begin{align} I^2 &=\bigg(\int_{-\infty}^\infty e^{-x^2} dx\bigg)^2\\ &= \bigg(\int_{-\infty}^\infty e^{-x^2} dx\bigg) \times  \bigg( \underbrace{ \int_{-\infty}^{\infty} e^{-y^2}dy }_{\text{Why}?} \bigg)  \\ &=\int_{-\infty}^\infty\bigg(\int_{-\infty}^\infty e^{-(x^2+y^2)} dx\bigg)dy  \end{align} But why is the following wrong: \begin{align} I^2 &=\bigg(\int_{-\infty}^\infty e^{-x^2} dx\bigg)^2\\ &= \bigg(\int_{-\infty}^\infty e^{-x^2} dx\bigg) \times \bigg(\int_{-\infty}^{\infty} e^{-x^2}dx\bigg) \\ &=\int_{-\infty}^\infty\bigg(\int_{-\infty}^\infty e^{-x^2-x^2} dx\bigg)dx \\ &=\int_{-\infty}^\infty\bigg(\int_{-\infty}^\infty e^{-2x^2} dx\bigg)dx \qquad ? \end{align}",,"['calculus', 'real-analysis', 'integration', 'analysis', 'improper-integrals']"
5,How to expalin the pathologie of this zero distribution(or measure).,How to expalin the pathologie of this zero distribution(or measure).,,"Assume $\delta$ is the zero Dirac distribution (measure) on $\Bbb R$. namely, $$(\delta,f)= \int_\Bbb R f\delta(dx) =f(0)$$ We know if $T\in \mathcal {D'}(\Bbb R)$ is a distribution then for every $ \phi \in C^\infty(\Bbb R)$ we have that  $\phi T$ is also a distribution defines by $$(\phi T, f) = ( T, \phi f)$$  Moreover, $$(\phi T)'= \phi'T+\phi T' $$ My problem . If we consiser the particular case $\phi= x$ or $\phi=x^2$ and $T= \delta$ we have, $$(x\delta, f)= (xf)(0)=0~~~\forall ~~f$$ Question : does this means that $\color{red}{x\delta \equiv 0}$ in $ \mathcal {D'}(\Bbb R)$? If yes how to explain the fact that $$\color{blue}{0 =(x\delta)' = \delta +x\delta'}$$ If no what is $\color{red}{x\delta }$ as a distribution? Patently if one consider the measure $$\mu(dx) = x^2 \delta(dx)$$ then it happens that $$\int f\mu(dx) = 0~~~\forall ~~f$$ Why is $\mu$ a trivial measure.?","Assume $\delta$ is the zero Dirac distribution (measure) on $\Bbb R$. namely, $$(\delta,f)= \int_\Bbb R f\delta(dx) =f(0)$$ We know if $T\in \mathcal {D'}(\Bbb R)$ is a distribution then for every $ \phi \in C^\infty(\Bbb R)$ we have that  $\phi T$ is also a distribution defines by $$(\phi T, f) = ( T, \phi f)$$  Moreover, $$(\phi T)'= \phi'T+\phi T' $$ My problem . If we consiser the particular case $\phi= x$ or $\phi=x^2$ and $T= \delta$ we have, $$(x\delta, f)= (xf)(0)=0~~~\forall ~~f$$ Question : does this means that $\color{red}{x\delta \equiv 0}$ in $ \mathcal {D'}(\Bbb R)$? If yes how to explain the fact that $$\color{blue}{0 =(x\delta)' = \delta +x\delta'}$$ If no what is $\color{red}{x\delta }$ as a distribution? Patently if one consider the measure $$\mu(dx) = x^2 \delta(dx)$$ then it happens that $$\int f\mu(dx) = 0~~~\forall ~~f$$ Why is $\mu$ a trivial measure.?",,"['analysis', 'measure-theory', 'distribution-theory', 'dirac-delta']"
6,Finding the following integral.,Finding the following integral.,,"$f$ is continuous function $\forall x\in[0,a]$ $(a>0)$ $f(x)+f(a-x)\neq 0$ $\forall x\in[0,a]$ Find the following integral: $$ \int^a_0\frac{f(x)}{f(x)+f(a-x)}dx$$ So I'm thinking: $y:=a-x$, then $$ \int^a_0\frac{f(x)}{f(x)+f(a-x)}dx=\int^a_0\frac{f(a-y)}{f(a-y)+f(y)}d(a-y)=\int^a_0\frac{f(a-y)+f(y)-f(y)}{f(a-y)+f(y)}d(a-y)=\int^a_0\frac{f(a-y)+f(y)}{f(a-y)+f(y)}d(a-y)-\int^a_0\frac{f(y)}{f(a-y)+f(y)}d(a-y)= (1)$$ Now let's take $d(a-y)=-dy$ $$(1)=-\int^a_01dy+\int^a_0\frac{f(y)}{f(a-y)+f(y)}d(y)=-(y)\bigg|^a_0+\int^a_0\frac{f(y)}{f(a-y)+f(y)}d(y)=(2)$$ Now, should I replace $y=a-x$ and get $$ (2)=-a - \int^a_0\frac{f(a-x)}{f(x)+f(a-x)}dx$$ And finally combine the first and last equation? Or how should act? I'm stuck with at the last part. How should I finish it?","$f$ is continuous function $\forall x\in[0,a]$ $(a>0)$ $f(x)+f(a-x)\neq 0$ $\forall x\in[0,a]$ Find the following integral: $$ \int^a_0\frac{f(x)}{f(x)+f(a-x)}dx$$ So I'm thinking: $y:=a-x$, then $$ \int^a_0\frac{f(x)}{f(x)+f(a-x)}dx=\int^a_0\frac{f(a-y)}{f(a-y)+f(y)}d(a-y)=\int^a_0\frac{f(a-y)+f(y)-f(y)}{f(a-y)+f(y)}d(a-y)=\int^a_0\frac{f(a-y)+f(y)}{f(a-y)+f(y)}d(a-y)-\int^a_0\frac{f(y)}{f(a-y)+f(y)}d(a-y)= (1)$$ Now let's take $d(a-y)=-dy$ $$(1)=-\int^a_01dy+\int^a_0\frac{f(y)}{f(a-y)+f(y)}d(y)=-(y)\bigg|^a_0+\int^a_0\frac{f(y)}{f(a-y)+f(y)}d(y)=(2)$$ Now, should I replace $y=a-x$ and get $$ (2)=-a - \int^a_0\frac{f(a-x)}{f(x)+f(a-x)}dx$$ And finally combine the first and last equation? Or how should act? I'm stuck with at the last part. How should I finish it?",,"['real-analysis', 'analysis']"
7,An inequality concerning $\pi$,An inequality concerning,\pi,"Given $x_0=0,x_1,\ldots,x_n>0,x_1+\ldots+x_n=1$. We want to prove $$ \sum_{i=1}^n \frac{x_i}{\sqrt{1+x_0+\cdots +x_{i-1}}\sqrt{x_i+\cdots +x_n}}<\frac{\pi}2$$ My progress: Let $x_0+\cdots+x_i=\sin \alpha_i$ We have $0=\alpha_0<\alpha_1<\cdots<\alpha_n=\frac{\pi}2$ Then the inequality becomes $$ \sum_{i=1}^n \frac{\sin \alpha_i-\sin \alpha_{i-1}}{\cos \alpha_{i-1}}<\frac{\pi}2$$ Then I don't know how to deal with this trigonometric inequality.","Given $x_0=0,x_1,\ldots,x_n>0,x_1+\ldots+x_n=1$. We want to prove $$ \sum_{i=1}^n \frac{x_i}{\sqrt{1+x_0+\cdots +x_{i-1}}\sqrt{x_i+\cdots +x_n}}<\frac{\pi}2$$ My progress: Let $x_0+\cdots+x_i=\sin \alpha_i$ We have $0=\alpha_0<\alpha_1<\cdots<\alpha_n=\frac{\pi}2$ Then the inequality becomes $$ \sum_{i=1}^n \frac{\sin \alpha_i-\sin \alpha_{i-1}}{\cos \alpha_{i-1}}<\frac{\pi}2$$ Then I don't know how to deal with this trigonometric inequality.",,['analysis']
8,Find a $f$ function such that$f'(x)\geq 0$ but not continuous,Find a  function such that but not continuous,f f'(x)\geq 0,"I just started reading continuity, differentiability etc. So I was thinking of an example of following type: Let $f: [a,b]\rightarrow \mathbb{R}$ , where $f(x)$ is monotonically increasing continuous function and differentiable on $(a,b)$ or simply $f'(x)\geq 0$ for $x\in (a,b)$ . can we find such function for which $f'(x)$ is not continuous? I could not find any, whatever function I take $f'(x)$ is becoming continuous. Is there such function even exists!","I just started reading continuity, differentiability etc. So I was thinking of an example of following type: Let , where is monotonically increasing continuous function and differentiable on or simply for . can we find such function for which is not continuous? I could not find any, whatever function I take is becoming continuous. Is there such function even exists!","f: [a,b]\rightarrow \mathbb{R} f(x) (a,b) f'(x)\geq 0 x\in (a,b) f'(x) f'(x)","['calculus', 'real-analysis', 'analysis']"
9,"Show that $f(x) = \sqrt{x}$ for $0 \leq x \leq 1$ is absolutely continuous on $[0,1]$",Show that  for  is absolutely continuous on,"f(x) = \sqrt{x} 0 \leq x \leq 1 [0,1]","According to the Royden 4th p.119, Definition) A real-valued function $f$ on a closed, bounded interval $[a,b]$ is said to be absolutely continuous on $[a,b]$ provided for each $\epsilon >0$, there is a $\delta>0$ such that for every finite disjoint collection $\{ (a_k,b_k)\}_{k=1}^{n} $ of open intervals in $(a,b)$, if $\sum_{k=1}^{n}[b_k-a_k] < \delta$, then $\sum_{k=1}^n |f(b_k)-f(a_k) | < \epsilon$. I am trying to prove that $f(x) = \sqrt{x}$ for $0 \leq x \leq 1$ is absolutely continuous on $[0,1]$. I want to check that my approach is correct. Proof) Claim 1 For given $0<\epsilon<1$, $f(x) = \sqrt{x}$ is absolutely continuous on $[\epsilon,1]$. Let $0 < \epsilon <1$ and consider finite disjoint collection $\{ (a_k,b_k)\}_{k=1}^{n} $ of open intervals in$(\epsilon,1)$. Note that $\sum_{k=1}^{n} |f(b_k)-f(a_k)| $ =  $\sum_{k=1}^{n}|\sqrt{b_k}-\sqrt{a_k}|$= $\sum_{k=1}^{n} \frac{b_k-a_k}{\sqrt{b_k}+\sqrt{a_k}}$ < $\frac{1}{2\sqrt{\epsilon}}$ $\sum_{k=1}^{n}[b_k-a_k]$. Let $\delta = 2\epsilon \sqrt{\epsilon} = 2\epsilon^{\frac{3}{2}}$. Now, if $\sum_{k=1}^{n}[b_k - a_k] < \delta = 2\epsilon^{\frac{3}{2}}$, then $\sum_{k=1}^{n} |f(b_k)-f(a_k)| $ = $\sum_{k=1}^{n} \frac{b_k-a_k}{\sqrt{b_k}+\sqrt{a_k}}$ < $\frac{1}{2\sqrt{\epsilon}} (2\epsilon^{\frac{3}{2}})$ = $\epsilon$. Thus, $f(x) = \sqrt{x}$ is absolutely continuous on $[\epsilon,1]$. # Claim 2 $f(x) = \sqrt{x}$ is absolutely continuous on $[0,1]$. Let $\epsilon>0$ be given and choose $\epsilon'>0$ such that $0<\sqrt{\epsilon'}<\frac{\epsilon}{2}$. By Claim 1 , we can find $\delta>0$ as the response to the $\epsilon'$ challenge regarding the criterion for the absolute continuity of $f(x)=\sqrt{x}$ on $[\epsilon',1]$. Consider $\{ (a_k,b_k) \}_{k=1}^{n}$ such that $\sum[b_k-a_k]< \delta$. Divide a disjoint collection $\{ (a_k,b_k) \}_{k=1}^n$ in $(0,1)$ into two parts: $\{ (a_{k1},b_{k1}) \}_{k1=1}^{n1}$ in $(0,\epsilon')$ and $\{ (a_{k2},b_{k2}) \}_{k2=1}^{n2}$ in $(\epsilon',1)$, where $n1+n2=n$. (If some $(a_k,b_k)$ contains $\epsilon'$, then divide it into two parts: $(a_k,\epsilon')$ and $(\epsilon',b_k$).) Now, $\sum_{k2=1}^{n2}[b_{k2}-a_{k2}] < \sum_{k=1}^{n}[b_{k}-a_{k}] < \delta$. Since $\delta>0$ responds to the $\epsilon'>0$ challenge on $[\epsilon',1]$, $\sum_{k2=1}^{n2} |\sqrt{b_{k2}} - \sqrt{a_{k2}}| < \epsilon' < \frac{\epsilon}{2}$. Also, $\sum_{k1=1}^{n1} |\sqrt{b_{k1}} - \sqrt{a_{k1}}| < \sqrt{\epsilon'}-0 < \frac{\epsilon}{2}$. Thus, $\sum_{k=1}^{n} |\sqrt{b_{k}} - \sqrt{a_{k}}| \leq$ $\sum_{k1=1}^{n1} |\sqrt{b_{k1}} - \sqrt{a_{k1}}|$ + $\sum_{k2=1}^{n2} |\sqrt{b_{k2}} - \sqrt{a_{k2}}|$ < $\frac{\epsilon}{2}$ + $\frac{\epsilon}{2}$ < $\epsilon$. Hence, $f(x) = \sqrt{x}$ is absolutely continuous on $[0,1]$. #","According to the Royden 4th p.119, Definition) A real-valued function $f$ on a closed, bounded interval $[a,b]$ is said to be absolutely continuous on $[a,b]$ provided for each $\epsilon >0$, there is a $\delta>0$ such that for every finite disjoint collection $\{ (a_k,b_k)\}_{k=1}^{n} $ of open intervals in $(a,b)$, if $\sum_{k=1}^{n}[b_k-a_k] < \delta$, then $\sum_{k=1}^n |f(b_k)-f(a_k) | < \epsilon$. I am trying to prove that $f(x) = \sqrt{x}$ for $0 \leq x \leq 1$ is absolutely continuous on $[0,1]$. I want to check that my approach is correct. Proof) Claim 1 For given $0<\epsilon<1$, $f(x) = \sqrt{x}$ is absolutely continuous on $[\epsilon,1]$. Let $0 < \epsilon <1$ and consider finite disjoint collection $\{ (a_k,b_k)\}_{k=1}^{n} $ of open intervals in$(\epsilon,1)$. Note that $\sum_{k=1}^{n} |f(b_k)-f(a_k)| $ =  $\sum_{k=1}^{n}|\sqrt{b_k}-\sqrt{a_k}|$= $\sum_{k=1}^{n} \frac{b_k-a_k}{\sqrt{b_k}+\sqrt{a_k}}$ < $\frac{1}{2\sqrt{\epsilon}}$ $\sum_{k=1}^{n}[b_k-a_k]$. Let $\delta = 2\epsilon \sqrt{\epsilon} = 2\epsilon^{\frac{3}{2}}$. Now, if $\sum_{k=1}^{n}[b_k - a_k] < \delta = 2\epsilon^{\frac{3}{2}}$, then $\sum_{k=1}^{n} |f(b_k)-f(a_k)| $ = $\sum_{k=1}^{n} \frac{b_k-a_k}{\sqrt{b_k}+\sqrt{a_k}}$ < $\frac{1}{2\sqrt{\epsilon}} (2\epsilon^{\frac{3}{2}})$ = $\epsilon$. Thus, $f(x) = \sqrt{x}$ is absolutely continuous on $[\epsilon,1]$. # Claim 2 $f(x) = \sqrt{x}$ is absolutely continuous on $[0,1]$. Let $\epsilon>0$ be given and choose $\epsilon'>0$ such that $0<\sqrt{\epsilon'}<\frac{\epsilon}{2}$. By Claim 1 , we can find $\delta>0$ as the response to the $\epsilon'$ challenge regarding the criterion for the absolute continuity of $f(x)=\sqrt{x}$ on $[\epsilon',1]$. Consider $\{ (a_k,b_k) \}_{k=1}^{n}$ such that $\sum[b_k-a_k]< \delta$. Divide a disjoint collection $\{ (a_k,b_k) \}_{k=1}^n$ in $(0,1)$ into two parts: $\{ (a_{k1},b_{k1}) \}_{k1=1}^{n1}$ in $(0,\epsilon')$ and $\{ (a_{k2},b_{k2}) \}_{k2=1}^{n2}$ in $(\epsilon',1)$, where $n1+n2=n$. (If some $(a_k,b_k)$ contains $\epsilon'$, then divide it into two parts: $(a_k,\epsilon')$ and $(\epsilon',b_k$).) Now, $\sum_{k2=1}^{n2}[b_{k2}-a_{k2}] < \sum_{k=1}^{n}[b_{k}-a_{k}] < \delta$. Since $\delta>0$ responds to the $\epsilon'>0$ challenge on $[\epsilon',1]$, $\sum_{k2=1}^{n2} |\sqrt{b_{k2}} - \sqrt{a_{k2}}| < \epsilon' < \frac{\epsilon}{2}$. Also, $\sum_{k1=1}^{n1} |\sqrt{b_{k1}} - \sqrt{a_{k1}}| < \sqrt{\epsilon'}-0 < \frac{\epsilon}{2}$. Thus, $\sum_{k=1}^{n} |\sqrt{b_{k}} - \sqrt{a_{k}}| \leq$ $\sum_{k1=1}^{n1} |\sqrt{b_{k1}} - \sqrt{a_{k1}}|$ + $\sum_{k2=1}^{n2} |\sqrt{b_{k2}} - \sqrt{a_{k2}}|$ < $\frac{\epsilon}{2}$ + $\frac{\epsilon}{2}$ < $\epsilon$. Hence, $f(x) = \sqrt{x}$ is absolutely continuous on $[0,1]$. #",,"['real-analysis', 'analysis']"
10,"Prob. 2, Chap. 6, in Baby Rudin: If $f\geq 0$ and continuous on $[a,b]$ with $\int_a^bf(x)\ \mathrm{d}x=0$, then $f=0$","Prob. 2, Chap. 6, in Baby Rudin: If  and continuous on  with , then","f\geq 0 [a,b] \int_a^bf(x)\ \mathrm{d}x=0 f=0","Here is Prob. 2, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f \geq 0$, $f$ is continuous on $[a, b]$, and $\int_a^b f(x) \ \mathrm{d} x = 0$. Prove that $f(x) = 0$ for all $x \in [a, b]$. My Attempt: Let $c$ and $d$ be any points in $[a, b]$ such that $c \leq d$. As $f(x) \geq 0$ for all $x \in [a, b]$ and as $f$ is continuous on $[a, b]$, so $f(x) \geq 0$ for all $x \in [c, d]$ and $f$ is continuous on $[c, d]$; therefore $f \in \mathscr{R}$ on $[c, d]$ and    $$ \int_c^d f(x) \ \mathrm{d} x \geq \int_c^d \hat{0}(x) \ \mathrm{d} x = 0$$   by Theorem 6.12 (b) in Baby Rudin, where $\hat{0}$ denotes the zero function on $[c, d]$. That is,    $$ \int_c^d f(x) \ \mathrm{d} x \geq 0 \tag{0}$$   for any points $c$ and $d$ such that $a \leq c \leq d \leq b$. Suppose that there is a point $p \in [a, b]$ such that $f(p) >  0$. Then, as $f$ is continuous at $p$, so, for any real number $\varepsilon$ such that    $$ 0 < \varepsilon < \frac{  f(p) }{2}, $$   we can find a real number $\delta > 0$ such that    $$ \lvert f(x) - f(p) \rvert < \varepsilon < \frac{ f(p) }{2} $$   for all $x \in [a, b]$ for which    $ \lvert x-p \rvert < \delta$. Therefore, we can conclude that   $$ \frac{ f(p) }{2} < f(x) < \frac{3 f(p) }{2} \tag{1} $$   for all $x \in I$, where    $$I \colon= [a, b] \cap \left[ p- \frac{\delta}{2}, p+ \frac{\delta}{2} \right]. $$    Let's put $I \colon= [u, v]$. Then we see that    $$ \begin{align} \int_a^b f(x) \ \mathrm{d} x &= \int_a^u f(x) \ \mathrm{d} x + \int_u^v f(x) \ \mathrm{d} x + \int_v^b f(x) \ \mathrm{d} x \\ & \qquad \qquad \mbox{ [ by an extension of Theorem 6.12 (c) in Baby Rudin ] } \\ &\geq \int_u^v f(x) \ \mathrm{d} x \qquad \mbox{ [ by (0) above ] } \\  &\geq \int_u^v \frac{f(p)}{2}  \ \mathrm{d} x  \qquad \mbox{ [ using (1) and Theorem 6.12 (b) in Baby Rudin ] } \\ &= \frac{f(p)}{2} (v-u) \\ &> 0, \qquad \mbox{ [ by (2) and our choice of $u$ and $v$ above ] }  \end{align} $$   which contradicts our hypothesis that $\int_a^b f(x) \ \mathrm{d} x = 0$. Is this proof sound enough in terms of its logic and rigor? If so, then is it also lucid enough in its presentation?","Here is Prob. 2, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f \geq 0$, $f$ is continuous on $[a, b]$, and $\int_a^b f(x) \ \mathrm{d} x = 0$. Prove that $f(x) = 0$ for all $x \in [a, b]$. My Attempt: Let $c$ and $d$ be any points in $[a, b]$ such that $c \leq d$. As $f(x) \geq 0$ for all $x \in [a, b]$ and as $f$ is continuous on $[a, b]$, so $f(x) \geq 0$ for all $x \in [c, d]$ and $f$ is continuous on $[c, d]$; therefore $f \in \mathscr{R}$ on $[c, d]$ and    $$ \int_c^d f(x) \ \mathrm{d} x \geq \int_c^d \hat{0}(x) \ \mathrm{d} x = 0$$   by Theorem 6.12 (b) in Baby Rudin, where $\hat{0}$ denotes the zero function on $[c, d]$. That is,    $$ \int_c^d f(x) \ \mathrm{d} x \geq 0 \tag{0}$$   for any points $c$ and $d$ such that $a \leq c \leq d \leq b$. Suppose that there is a point $p \in [a, b]$ such that $f(p) >  0$. Then, as $f$ is continuous at $p$, so, for any real number $\varepsilon$ such that    $$ 0 < \varepsilon < \frac{  f(p) }{2}, $$   we can find a real number $\delta > 0$ such that    $$ \lvert f(x) - f(p) \rvert < \varepsilon < \frac{ f(p) }{2} $$   for all $x \in [a, b]$ for which    $ \lvert x-p \rvert < \delta$. Therefore, we can conclude that   $$ \frac{ f(p) }{2} < f(x) < \frac{3 f(p) }{2} \tag{1} $$   for all $x \in I$, where    $$I \colon= [a, b] \cap \left[ p- \frac{\delta}{2}, p+ \frac{\delta}{2} \right]. $$    Let's put $I \colon= [u, v]$. Then we see that    $$ \begin{align} \int_a^b f(x) \ \mathrm{d} x &= \int_a^u f(x) \ \mathrm{d} x + \int_u^v f(x) \ \mathrm{d} x + \int_v^b f(x) \ \mathrm{d} x \\ & \qquad \qquad \mbox{ [ by an extension of Theorem 6.12 (c) in Baby Rudin ] } \\ &\geq \int_u^v f(x) \ \mathrm{d} x \qquad \mbox{ [ by (0) above ] } \\  &\geq \int_u^v \frac{f(p)}{2}  \ \mathrm{d} x  \qquad \mbox{ [ using (1) and Theorem 6.12 (b) in Baby Rudin ] } \\ &= \frac{f(p)}{2} (v-u) \\ &> 0, \qquad \mbox{ [ by (2) and our choice of $u$ and $v$ above ] }  \end{align} $$   which contradicts our hypothesis that $\int_a^b f(x) \ \mathrm{d} x = 0$. Is this proof sound enough in terms of its logic and rigor? If so, then is it also lucid enough in its presentation?",,"['real-analysis', 'integration', 'analysis', 'proof-verification', 'definite-integrals']"
11,measure on sphere,measure on sphere,,"In a set of notes I am using (physics related), it states that ""we denote $$M = \{ (x,y,z)| x^2 + y^2 + z^2 = 1 \}$$ and $L^2(M)$ the space of square integrable functions on $M$ with measure $$\Omega = (\theta, \phi),~~0 \leq \theta \leq \pi, ~0 \leq \phi \leq 2 \pi$$ and $$d \Omega = \sin \theta d \theta d \phi.""$$ Is it clear how this defines a measure? As I know a measure is defined as a function from a $\sigma-$algebra to the real line, which satisfies the properties as in the link.","In a set of notes I am using (physics related), it states that ""we denote $$M = \{ (x,y,z)| x^2 + y^2 + z^2 = 1 \}$$ and $L^2(M)$ the space of square integrable functions on $M$ with measure $$\Omega = (\theta, \phi),~~0 \leq \theta \leq \pi, ~0 \leq \phi \leq 2 \pi$$ and $$d \Omega = \sin \theta d \theta d \phi.""$$ Is it clear how this defines a measure? As I know a measure is defined as a function from a $\sigma-$algebra to the real line, which satisfies the properties as in the link.",,['analysis']
12,Intermediate value theorem and the Riemann integration,Intermediate value theorem and the Riemann integration,,"Previously I posted this question asking for a review of the proof  , but I realize the proof it's wrong at all because I can't have the continuity of $f$. So here is another attempt for the proof. If $f\in R$ on $[a,b]$ and $g$ is a monotonous function on $[a,b],$ then there exist $\epsilon \in [a,b]$ such that $$\int_a^bfg=g(a)\int_a^{\epsilon}f+g(b)\int_{\epsilon}^bf..................   (*)$$ Proof: (attempt) Let $F(x)=g(a)\int_a^{x}f+g(b)\int_{x}^bf.$ Thus $F$ is continuous. Also, $F(a)=g(b)\int_a^bf$ and $F(b)=g(a)\int_a^bf$. Now if $\int_a^bfg$ is between $F(a)$ and $F(b)$ and If I apply the Intermediate value theorem then I'll get something like $(*)$. But how can I find a 'bound' for $\int_a^bfg$ such that $\int_a^bfg$ is between $F(a)$ and $F(b)$?. Is this idea for the proof correct? Note: I can't use mesure theory for the proof because I haven't seen nothing (haven't taken a course) about mesure theory.","Previously I posted this question asking for a review of the proof  , but I realize the proof it's wrong at all because I can't have the continuity of $f$. So here is another attempt for the proof. If $f\in R$ on $[a,b]$ and $g$ is a monotonous function on $[a,b],$ then there exist $\epsilon \in [a,b]$ such that $$\int_a^bfg=g(a)\int_a^{\epsilon}f+g(b)\int_{\epsilon}^bf..................   (*)$$ Proof: (attempt) Let $F(x)=g(a)\int_a^{x}f+g(b)\int_{x}^bf.$ Thus $F$ is continuous. Also, $F(a)=g(b)\int_a^bf$ and $F(b)=g(a)\int_a^bf$. Now if $\int_a^bfg$ is between $F(a)$ and $F(b)$ and If I apply the Intermediate value theorem then I'll get something like $(*)$. But how can I find a 'bound' for $\int_a^bfg$ such that $\int_a^bfg$ is between $F(a)$ and $F(b)$?. Is this idea for the proof correct? Note: I can't use mesure theory for the proof because I haven't seen nothing (haven't taken a course) about mesure theory.",,"['real-analysis', 'integration', 'analysis', 'proof-writing', 'riemann-integration']"
13,Is $|f(b)-f(a)| > |b-a|$ true for $f(x)=x+(1+e^x)^{-1}$?,Is  true for ?,|f(b)-f(a)| > |b-a| f(x)=x+(1+e^x)^{-1},"I'd like to use this as part of a proof, but I couldn't realize how to show this (and if it) is true. The function is: $f(x)=x+(1+e^x)^{-1}$","I'd like to use this as part of a proof, but I couldn't realize how to show this (and if it) is true. The function is: $f(x)=x+(1+e^x)^{-1}$",,"['real-analysis', 'analysis']"
14,"Using real analysis, how can we show that for all $x > 0$, $ \frac{2(e^x-(1+x))}{x^2} < e^x$","Using real analysis, how can we show that for all ,",x > 0  \frac{2(e^x-(1+x))}{x^2} < e^x,I was able to think that the numerator will always be positive and will overpower the denominator as well. But couldn't proceed from there.,I was able to think that the numerator will always be positive and will overpower the denominator as well. But couldn't proceed from there.,,"['real-analysis', 'analysis']"
15,Is there a smooth function which behaves like $x^n\sin(\frac{1}{x})$?,Is there a smooth function which behaves like ?,x^n\sin(\frac{1}{x}),"The function $x^n\sin(\frac{1}{x})$ is finite-order differentiable at $x=0$, and near $x=0$ it vanishes infinite times but not identically. I wonder if there is a smooth function like this. Any help will be appreciated.","The function $x^n\sin(\frac{1}{x})$ is finite-order differentiable at $x=0$, and near $x=0$ it vanishes infinite times but not identically. I wonder if there is a smooth function like this. Any help will be appreciated.",,"['calculus', 'real-analysis', 'analysis', 'functions']"
16,Spivak's Calculus - Chapter 1 Question 23,Spivak's Calculus - Chapter 1 Question 23,,"In Spivak's Calculus Chapter 1 Question 23: Replace the question marks in the following statement ing $\varepsilon, x_0$ and $y_0$ so that the conclusion will be true:   If $y_0\neq 0$ and $$|y-y_0|<?  \qquad\text{and}\qquad  |x-x_0|<?$$ then $y\neq 0$ and $$ \bigg| \frac{x}{y}-\frac{x_0}{y_0}\bigg|<\varepsilon.$$ The answer in its Solution Manual is $$|x-x_0|<\min\bigg(\frac{\varepsilon}{2(1/|y_0|+1)},1 \bigg)$$ and $$|y-y_0|<\min\bigg( \frac{|y_0|}{2},\frac{\varepsilon|y_0|^2}{4(|x_0|+1)} \bigg).$$ The latter is same as my solution, but the first is a bit different. So I wonder if $$|x-x_0|<\min\bigg(\frac{\varepsilon |y_0|}{4},1 \bigg)$$, is the solution still be true? Thanks in advance.","In Spivak's Calculus Chapter 1 Question 23: Replace the question marks in the following statement ing $\varepsilon, x_0$ and $y_0$ so that the conclusion will be true:   If $y_0\neq 0$ and $$|y-y_0|<?  \qquad\text{and}\qquad  |x-x_0|<?$$ then $y\neq 0$ and $$ \bigg| \frac{x}{y}-\frac{x_0}{y_0}\bigg|<\varepsilon.$$ The answer in its Solution Manual is $$|x-x_0|<\min\bigg(\frac{\varepsilon}{2(1/|y_0|+1)},1 \bigg)$$ and $$|y-y_0|<\min\bigg( \frac{|y_0|}{2},\frac{\varepsilon|y_0|^2}{4(|x_0|+1)} \bigg).$$ The latter is same as my solution, but the first is a bit different. So I wonder if $$|x-x_0|<\min\bigg(\frac{\varepsilon |y_0|}{4},1 \bigg)$$, is the solution still be true? Thanks in advance.",,"['calculus', 'analysis', 'epsilon-delta']"
17,Are there more types of critical points beyond maxima/minima/saddle points for higher dimensions?,Are there more types of critical points beyond maxima/minima/saddle points for higher dimensions?,,"I had a course on single variable calculus and at that point, we had minima and maxima. Now on several variables calculus, there is maxima, minima and saddle points. Certain books of several variables calculus  point only those three types of critical points but there is actually no guarantee that these are the only ones - other types could be more advanced and/or irrelevant for the study of calculus, perhaps? So are there more kinds of critical points for higher dimensional calculus? Supposing there are only these three, how can I know for sure that there are only these three? Supposing that there are more than three, how can I know that there are more than three?","I had a course on single variable calculus and at that point, we had minima and maxima. Now on several variables calculus, there is maxima, minima and saddle points. Certain books of several variables calculus  point only those three types of critical points but there is actually no guarantee that these are the only ones - other types could be more advanced and/or irrelevant for the study of calculus, perhaps? So are there more kinds of critical points for higher dimensional calculus? Supposing there are only these three, how can I know for sure that there are only these three? Supposing that there are more than three, how can I know that there are more than three?",,"['analysis', 'multivariable-calculus']"
18,Proofing that the exponential function is continuous in every $x_{0}$,Proofing that the exponential function is continuous in every,x_{0},"Given: $$\exp: \mathbb{R} \ni  x \mapsto \sum_{k=0}^{\infty } \frac{1}{k!} x^{k} \in \mathbb{R}$$ also $e = \exp(1)$. For all $x \in \mathbb{R}$ with $\left | x \right | \leq 1$:  $$\left | \exp(x) - 1 \right | \leq \left | x \right | \cdot (e-1)$$ and $\exp(0) = 1$ ........................................................................................................................... In order to proof that the exponential function is continuous for every $x_{0}$, it needs to be shown that it's continuous at all. This was shown here (it's continuous at $x_{0}$ = 0): Proving that the exponential function is continuous But I prefer this proof: $$ \exp(x) = \sum_{n=0}^\infty \frac{x^n}{n!} $$ apply some little changes $$ \exp(x) = 1 + x \sum_{n=1}^\infty \frac{x^{n-1}}{n!} $$ whence for $x\to 0$ $$ |\exp(x) - 1| \le |x| \sum_{n=1}^\infty{|x|^{n-1}} \le |x| \frac{1}{1-|x|} \to 0 $$ I would say in order to show that the exponential function is continuous for all $x_{0} = 0$, I just need to show it is continuous at $x_{0}$ = 0 (done) and then I can just conclude it is continuous everywhere, so at $x=x_{0}$? Not sure about this, is it really possible?","Given: $$\exp: \mathbb{R} \ni  x \mapsto \sum_{k=0}^{\infty } \frac{1}{k!} x^{k} \in \mathbb{R}$$ also $e = \exp(1)$. For all $x \in \mathbb{R}$ with $\left | x \right | \leq 1$:  $$\left | \exp(x) - 1 \right | \leq \left | x \right | \cdot (e-1)$$ and $\exp(0) = 1$ ........................................................................................................................... In order to proof that the exponential function is continuous for every $x_{0}$, it needs to be shown that it's continuous at all. This was shown here (it's continuous at $x_{0}$ = 0): Proving that the exponential function is continuous But I prefer this proof: $$ \exp(x) = \sum_{n=0}^\infty \frac{x^n}{n!} $$ apply some little changes $$ \exp(x) = 1 + x \sum_{n=1}^\infty \frac{x^{n-1}}{n!} $$ whence for $x\to 0$ $$ |\exp(x) - 1| \le |x| \sum_{n=1}^\infty{|x|^{n-1}} \le |x| \frac{1}{1-|x|} \to 0 $$ I would say in order to show that the exponential function is continuous for all $x_{0} = 0$, I just need to show it is continuous at $x_{0}$ = 0 (done) and then I can just conclude it is continuous everywhere, so at $x=x_{0}$? Not sure about this, is it really possible?",,"['analysis', 'exponential-function']"
19,Reference request: Inclusion of smooth maps into continuous maps between smooth manifolds is a weak homotopy equivalence.,Reference request: Inclusion of smooth maps into continuous maps between smooth manifolds is a weak homotopy equivalence.,,"Let $M,N$ be smooth manifolds. It seems to be well known that if the sets $C^0(M,N)$ and $C^\infty(M,N)$ are equipped with the appropriate topologies (I suppose the weak/strong Whitney topology), then the inclusion $$C^\infty(M,N) \hookrightarrow C^0(M,N)$$ is a weak homotopy equivalence, see e.g. here . However, I don't know of any reference where this is proved explicitly. Can anyone provide a reference that the above inclusion is a weak homotopy equivalence?","Let $M,N$ be smooth manifolds. It seems to be well known that if the sets $C^0(M,N)$ and $C^\infty(M,N)$ are equipped with the appropriate topologies (I suppose the weak/strong Whitney topology), then the inclusion $$C^\infty(M,N) \hookrightarrow C^0(M,N)$$ is a weak homotopy equivalence, see e.g. here . However, I don't know of any reference where this is proved explicitly. Can anyone provide a reference that the above inclusion is a weak homotopy equivalence?",,"['analysis', 'differential-geometry', 'algebraic-topology', 'differential-topology', 'homotopy-theory']"
20,Continuous functions are locally integrable?,Continuous functions are locally integrable?,,If $K\subset\mathbb{R}$ is compact and $f:K\rightarrow\mathbb{R}$ continuous then $f\in\mathbb{L}(K)$. In other words $f$ is integrable in $K$. So far i know that since $f$ is continuous then $f(K)$ is bounded. But I don't know how I can use this with the integral $\int_Kf$.,If $K\subset\mathbb{R}$ is compact and $f:K\rightarrow\mathbb{R}$ continuous then $f\in\mathbb{L}(K)$. In other words $f$ is integrable in $K$. So far i know that since $f$ is continuous then $f(K)$ is bounded. But I don't know how I can use this with the integral $\int_Kf$.,,"['analysis', 'measure-theory', 'lebesgue-integral']"
21,Relation between Meissel–Mertens constant and Euler–Mascheroni constant,Relation between Meissel–Mertens constant and Euler–Mascheroni constant,,"From the Wikipedia page , the Meissel–Mertens constant $M$ is defined as the limit: $$M:=\lim_{n\to\infty}\left(\sum_{p\leq n}\frac{1}{p}-\log\log n\right).$$ Why is it  equal to $\gamma+\sum_{p}\left(\log(1-\frac{1}{p})+\frac{1}{p}\right)$? where $\gamma$ is the  Euler–Mascheroni constant defined by $$\gamma:=\lim_{n\to\infty}\left(\sum_{k=1}^n\frac{1}{k}-\log n\right).$$ I tried to prove this, but cannot work it out.","From the Wikipedia page , the Meissel–Mertens constant $M$ is defined as the limit: $$M:=\lim_{n\to\infty}\left(\sum_{p\leq n}\frac{1}{p}-\log\log n\right).$$ Why is it  equal to $\gamma+\sum_{p}\left(\log(1-\frac{1}{p})+\frac{1}{p}\right)$? where $\gamma$ is the  Euler–Mascheroni constant defined by $$\gamma:=\lim_{n\to\infty}\left(\sum_{k=1}^n\frac{1}{k}-\log n\right).$$ I tried to prove this, but cannot work it out.",,"['analysis', 'analytic-number-theory']"
22,"Let $f:\mathbb{R}^m \to \mathbb{R}$ be differentiable s.t. $f(x/2)=f(x)/2, \forall x \in \mathbb{R}^m$. Show that $f$ is linear.",Let  be differentiable s.t. . Show that  is linear.,"f:\mathbb{R}^m \to \mathbb{R} f(x/2)=f(x)/2, \forall x \in \mathbb{R}^m f","Basically, I am not really sure how to start. I thought about going through induction to show for $\mathbb{N}$ and $\mathbb{Q}$, then use the completeness of $\mathbb{R}$, but I think it is a long way there. I am quite sure that there is a shortcut. Could you help me?","Basically, I am not really sure how to start. I thought about going through induction to show for $\mathbb{N}$ and $\mathbb{Q}$, then use the completeness of $\mathbb{R}$, but I think it is a long way there. I am quite sure that there is a shortcut. Could you help me?",,"['real-analysis', 'analysis', 'functional-equations']"
23,Proving two sets are equal,Proving two sets are equal,,"Suppose we have a claim: $A = \bigcup_{k \in \mathbb N} [-k, \frac{1}{k}) = (-\infty,1)$ = B. I aim to prove this by showing that the two sets are subsets of each other (i.e. $A \subset B$ and $B \subset A$). As for my strategy of showing this, I am going to grab an arbitrary element from the set A and show that this also belongs to B and vice versa. Now, my problem is that although I can easily show this in picture, I lack the mathematical argument that describes this picture in my head. Any suggestions?","Suppose we have a claim: $A = \bigcup_{k \in \mathbb N} [-k, \frac{1}{k}) = (-\infty,1)$ = B. I aim to prove this by showing that the two sets are subsets of each other (i.e. $A \subset B$ and $B \subset A$). As for my strategy of showing this, I am going to grab an arbitrary element from the set A and show that this also belongs to B and vice versa. Now, my problem is that although I can easily show this in picture, I lack the mathematical argument that describes this picture in my head. Any suggestions?",,[]
24,Showing quotient map $q$ is surjective and there exists another function $\bar{f}$ such that $f = \bar{f} \circ q$.,Showing quotient map  is surjective and there exists another function  such that .,q \bar{f} f = \bar{f} \circ q,"Problem: Let $\sim$ be an equivalence relation over a set $X$ and let $X / \sim $ be the corresponding quotient set. There is a function \begin{align*} q: X \rightarrow X / \sim \ : x \mapsto [x] \end{align*} which maps each element $x \in X$ to its corresponding equivalence class in $X / \sim$. This mapping is called the quotient map corresponding to $\sim$. 1) Show that $q$ is surjective. 2) Suppose that $f : X \rightarrow Y$ is a function with the property that \begin{align*} x_1 \sim x_2 \Rightarrow f(x_1) = f(x_2). \end{align*} Prove that there exists an unique function $\bar{f} : X/ \sim \rightarrow Y$ with the property that \begin{align*} f= \bar{f} \circ q. \end{align*} Attempt at proof: For part 1) I reasoned as follows: Let $[x] \in X/ \sim$ be arbitrary. Then we have to show that there exists an element $x \in X$ such that $q(x) = [x]$. Since no equivalence class in $X / \sim$ is empty, there always exists an $x \in [x]$ for each $x \in X$. This proves that $q$ is surjective. 2) For this part, I'm not sure how to proceed. We need to construct the function $\bar{f}$ I think. So I would let $[x_1] \in X / \sim$. Then $\bar{f} [x_1] = y_1$ for some $y_1 \in Y$. Then we need to show somehow that $f = \bar{f} \circ q$ holds? How can I do that?","Problem: Let $\sim$ be an equivalence relation over a set $X$ and let $X / \sim $ be the corresponding quotient set. There is a function \begin{align*} q: X \rightarrow X / \sim \ : x \mapsto [x] \end{align*} which maps each element $x \in X$ to its corresponding equivalence class in $X / \sim$. This mapping is called the quotient map corresponding to $\sim$. 1) Show that $q$ is surjective. 2) Suppose that $f : X \rightarrow Y$ is a function with the property that \begin{align*} x_1 \sim x_2 \Rightarrow f(x_1) = f(x_2). \end{align*} Prove that there exists an unique function $\bar{f} : X/ \sim \rightarrow Y$ with the property that \begin{align*} f= \bar{f} \circ q. \end{align*} Attempt at proof: For part 1) I reasoned as follows: Let $[x] \in X/ \sim$ be arbitrary. Then we have to show that there exists an element $x \in X$ such that $q(x) = [x]$. Since no equivalence class in $X / \sim$ is empty, there always exists an $x \in [x]$ for each $x \in X$. This proves that $q$ is surjective. 2) For this part, I'm not sure how to proceed. We need to construct the function $\bar{f}$ I think. So I would let $[x_1] \in X / \sim$. Then $\bar{f} [x_1] = y_1$ for some $y_1 \in Y$. Then we need to show somehow that $f = \bar{f} \circ q$ holds? How can I do that?",,"['abstract-algebra', 'analysis', 'proof-verification']"
25,Pullback of $1$-form in coordinates,Pullback of -form in coordinates,1,"Let $$\theta(p) = \sum_{i=1}^n f_i(p) \, dx_i$$ be a $1$-form in local coordinates. then we define $F^*(\omega(p))(X_1,\ldots,X_n) = \omega(F(p))(DF(p)(X_1),\ldots,DF(p)(X_n))$ as the pullback of a general n-form My question is now, can we also express the pullback of the $1$-form $\theta$ in local coordinates? I suspect that it would be something like $$F^*(\theta(p)) = \sum_{i=1}^n f_i(F(p)) \, d(F(x_i)),$$ but I am particularly uncertain how the coordinates $dx_i$ transform under pullback.","Let $$\theta(p) = \sum_{i=1}^n f_i(p) \, dx_i$$ be a $1$-form in local coordinates. then we define $F^*(\omega(p))(X_1,\ldots,X_n) = \omega(F(p))(DF(p)(X_1),\ldots,DF(p)(X_n))$ as the pullback of a general n-form My question is now, can we also express the pullback of the $1$-form $\theta$ in local coordinates? I suspect that it would be something like $$F^*(\theta(p)) = \sum_{i=1}^n f_i(F(p)) \, d(F(x_i)),$$ but I am particularly uncertain how the coordinates $dx_i$ transform under pullback.",,"['real-analysis', 'analysis', 'differential-geometry', 'differential-topology', 'differential-forms']"
26,Representing any number in $R_+$ by means of two numbers,Representing any number in  by means of two numbers,R_+,"I have the following question: Let $R_+$ denote the nonnegative reals. Let $0<a<1$ and $b>0$, and set $p = a^n b^m$. Is it possible to find a $n,m\in N$ such that $p$ can approximate any number in $R_+$ arbitrarily close?","I have the following question: Let $R_+$ denote the nonnegative reals. Let $0<a<1$ and $b>0$, and set $p = a^n b^m$. Is it possible to find a $n,m\in N$ such that $p$ can approximate any number in $R_+$ arbitrarily close?",,['analysis']
27,Failure of Newton-Leibniz formula,Failure of Newton-Leibniz formula,,"Suppose that $f : \mathbb{R} \rightarrow \mathbb{R}$ is differentiable but  $f \notin C^1 ( \mathbb{R} )$ . It means that $f'$ exist but it is not continuous. Question 1 Is function $f'$ locally integrable. I.e. does there exist for every $a , b \in \mathbb{R}$ $$ \int_{a}^{b} f'(x) dx $$ I think, I should ask about existence of Lebesgue integral. Question 2 If it exist, does the Newton-Leibniz formula holds? $$ \int_{a}^{b} f'(x) dx = f(b) - f(a) $$ Comment . I am asking because I wanted to prove Cauchy's integral theorem using Stokes' theorem. One told me that I am not allowed to use Stokes' theorem if derivatives are not continuous.. So I wonder whether it is important. The simplest case of Stokes' theorem is Newton-Leibniz formula.","Suppose that $f : \mathbb{R} \rightarrow \mathbb{R}$ is differentiable but  $f \notin C^1 ( \mathbb{R} )$ . It means that $f'$ exist but it is not continuous. Question 1 Is function $f'$ locally integrable. I.e. does there exist for every $a , b \in \mathbb{R}$ $$ \int_{a}^{b} f'(x) dx $$ I think, I should ask about existence of Lebesgue integral. Question 2 If it exist, does the Newton-Leibniz formula holds? $$ \int_{a}^{b} f'(x) dx = f(b) - f(a) $$ Comment . I am asking because I wanted to prove Cauchy's integral theorem using Stokes' theorem. One told me that I am not allowed to use Stokes' theorem if derivatives are not continuous.. So I wonder whether it is important. The simplest case of Stokes' theorem is Newton-Leibniz formula.",,"['integration', 'analysis', 'lebesgue-integral']"
28,Analysis Constructing a Sequence,Analysis Constructing a Sequence,,"I'm looking for a sequence of functions that is continuous and absolutely integrable, but pointwise divergent for every $z$ $\in [0,1]$. In other words, $ \int_0^1 |f_n(z)| dz \rightarrow 0$ as $n \rightarrow \infty$, but $(f_n(z))_n$ pointwise diverges for every $z$ $\in [0,1]$. I'm having no luck coming up with an example, and would appreciate any sort of insight on how to go about constructing such a sequence.","I'm looking for a sequence of functions that is continuous and absolutely integrable, but pointwise divergent for every $z$ $\in [0,1]$. In other words, $ \int_0^1 |f_n(z)| dz \rightarrow 0$ as $n \rightarrow \infty$, but $(f_n(z))_n$ pointwise diverges for every $z$ $\in [0,1]$. I'm having no luck coming up with an example, and would appreciate any sort of insight on how to go about constructing such a sequence.",,['analysis']
29,"Prove the triangle inequality for $d(x,y) = \min(|x−y|,1−|x−y|)$.",Prove the triangle inequality for .,"d(x,y) = \min(|x−y|,1−|x−y|)","Let $X$ be the set $[0,1)$ . Define a non-standard metric on X as follows: For two numbers $x,y ∈ X$ , take $d(x,y) = \min(|x−y|,1−|x−y|)$ . Show that this is a metric. In order to show this is a metric, I need to prove the triangle inequality for the metric. That is, for any $x, y, z \in X,\quad d(x,z) \le d(x,y)+d(y,z)$ . I try to prove this with the inequality $|x-y|+|y-z|\ge|x-z|$ , but I was lost when I reached the case: $d(x,z)=|x-z|,\,d(x,y)=|x-y|,\,d(y,z)=1-|y-z|$ . Could anyone give some hints?","Let be the set . Define a non-standard metric on X as follows: For two numbers , take . Show that this is a metric. In order to show this is a metric, I need to prove the triangle inequality for the metric. That is, for any . I try to prove this with the inequality , but I was lost when I reached the case: . Could anyone give some hints?","X [0,1) x,y ∈ X d(x,y) = \min(|x−y|,1−|x−y|) x, y, z \in X,\quad d(x,z) \le d(x,y)+d(y,z) |x-y|+|y-z|\ge|x-z| d(x,z)=|x-z|,\,d(x,y)=|x-y|,\,d(y,z)=1-|y-z|","['analysis', 'metric-spaces']"
30,Simplification of a combination of 6 values of the gamma function,Simplification of a combination of 6 values of the gamma function,,"I'm trying to simplify this combination of gamma functions: $$\frac{\Gamma\left(\frac{2}{25}\right)\Gamma\left(\frac{7}{25}\right)\Gamma\left(\frac{12}{25}\right)}{\Gamma\left(\frac{2}{5}\right)\Gamma\left(\frac{3}{25}\right)\Gamma\left(\frac{8}{25}\right)}$$ I tried to apply Gauss's multiplication formula , as it was done in this answer , but without any success. It is possible to simplify this expression at all?","I'm trying to simplify this combination of gamma functions: $$\frac{\Gamma\left(\frac{2}{25}\right)\Gamma\left(\frac{7}{25}\right)\Gamma\left(\frac{12}{25}\right)}{\Gamma\left(\frac{2}{5}\right)\Gamma\left(\frac{3}{25}\right)\Gamma\left(\frac{8}{25}\right)}$$ I tried to apply Gauss's multiplication formula , as it was done in this answer , but without any success. It is possible to simplify this expression at all?",,"['analysis', 'special-functions', 'gamma-function']"
31,How do I show that as $z \to \infty$ that $\int_0^\infty \frac{t - \lfloor t \rfloor - 1/2}{z + t} dt = O(z^{-1} )$??,How do I show that as  that ??,z \to \infty \int_0^\infty \frac{t - \lfloor t \rfloor - 1/2}{z + t} dt = O(z^{-1} ),"How do I show that as $z \to \infty$ we have $$ \int_0^\infty \frac{t - \lfloor t \rfloor - 1/2}{z + t} \,dt = O(z^{-1} )? $$ According to Serge Lang, the integral on the left is the error term for Stirling's asymptotic bound for $\log(\Gamma(z))$, but I read elsewhere that the integral is equivalent to the right hand side.  I am trying to find a rigorous way to show that the statement above is true, that is, without resorting to looking at a graph on WolframAlpha.","How do I show that as $z \to \infty$ we have $$ \int_0^\infty \frac{t - \lfloor t \rfloor - 1/2}{z + t} \,dt = O(z^{-1} )? $$ According to Serge Lang, the integral on the left is the error term for Stirling's asymptotic bound for $\log(\Gamma(z))$, but I read elsewhere that the integral is equivalent to the right hand side.  I am trying to find a rigorous way to show that the statement above is true, that is, without resorting to looking at a graph on WolframAlpha.",,"['integration', 'analysis', 'asymptotics', 'improper-integrals', 'gamma-function']"
32,The value of $\lim_{n\to \infty}\int_{-\infty}^{\infty}f(x)\cos^{2} nx dx.$,The value of,\lim_{n\to \infty}\int_{-\infty}^{\infty}f(x)\cos^{2} nx dx.,"Using the fact  $\lim_{n\to \infty}\int_{-\infty}^{\infty}f(x)\cos nx dx=0$ ,find the value of $$\lim_{n\to \infty}\int_{-\infty}^{\infty}f(x)\cos^{2} nx dx.$$ I tried through integrating by parts , also through the $1^{st}$ Mean Value Theorem of integral calculus but I c","Using the fact  $\lim_{n\to \infty}\int_{-\infty}^{\infty}f(x)\cos nx dx=0$ ,find the value of $$\lim_{n\to \infty}\int_{-\infty}^{\infty}f(x)\cos^{2} nx dx.$$ I tried through integrating by parts , also through the $1^{st}$ Mean Value Theorem of integral calculus but I c",,"['real-analysis', 'integration', 'analysis', 'definite-integrals', 'improper-integrals']"
33,"Let $n\geq 1$ and $\alpha \in [0,1)$ show that : $1\le \bigl(1+\frac{\alpha}{n}\bigr)^{n}\le \frac{1}{1-\alpha}$",Let  and  show that :,"n\geq 1 \alpha \in [0,1) 1\le \bigl(1+\frac{\alpha}{n}\bigr)^{n}\le \frac{1}{1-\alpha}","Let $n\geq 1$ and $\alpha \in [0,1)$ show that :   $$1\le \left(1+\dfrac{\alpha}{n}\right)^{n}\le \dfrac{1}{1-\alpha}$$ This question is related to that one Show that ${n \choose k}\leq n^k$ My thoughts: To prove that the following statement, which we will call P(n), holds for all natural numbers n: $$1\le \left(1+\dfrac{\alpha}{n}\right)^{n}\le \dfrac{1}{1-\alpha}$$ so my proof that P($n$) is true for each natural number $n$ proceeds as follows: Basis: Show that the statement holds for $n=1$. P($1$) amounts to the statement: $$1\le \left(1+\dfrac{\alpha}{1}\right)^{1}\le \dfrac{1}{1-\alpha}$$ $$\iff$$ $$1\le \left(1-{\alpha}^2\right)\le 1$$ since $\alpha \in [0,1) \implies 0\le \alpha < 1 \implies 0\le \alpha^2 < 1 \implies -1 \le -\alpha^2 < 0 \implies 0 \le 1-\alpha^2 < 1$ then the statement is true for $n=1$. Thus it has been shown that P($1$) holds Inductive step: Show that if P($n$) holds, then also P($n+1$) holds. This can be done as follows. Assume P($n$) holds. It must then be shown that P($n+1$) holds, that is: $$1\le \left(1+\dfrac{\alpha}{n+1}\right)^{n+1}\le \dfrac{1}{1-\alpha}$$ I can't manage is my reasoning correct and is there other ways to prove that Edit since there is probleme in the case of P(1) becuase i shouldn't write $1\le \left(1-{\alpha}^2\right)\le 1.$ since  the left inequality is not true. i have to break it in two separate case and do it then we ve : For $\left(1+\dfrac{\alpha}{n}\right)^{n}\le \dfrac{1}{1-\alpha}$ To prove that the following statement, which we will call P(n), holds for all natural numbers n: $$\left(1+\dfrac{\alpha}{n}\right)^{n}\le \dfrac{1}{1-\alpha}$$ so my proof that P($n$) is true for each natural number $n$ proceeds as follows: Basis: Show that the statement holds for $n=1$. P($1$) amounts to the statement: $\left(1+\dfrac{\alpha}{1}\right)^{1}\le \dfrac{1}{1-\alpha}$ since $\alpha \in [0,1) \implies 0\le \alpha < 1 \implies 0\le \alpha^2 < 1 \implies -1 \le -\alpha^2 < 0 \implies 0 \le 1-\alpha^2 < 1$ then the statement is true for $n=1$. Thus it has been shown that P($1$) holds Inductive step: Show that if P($n$) holds, then also P($n+1$) holds. This can be done as follows. Assume P($n$) holds. It must then be shown that P($n+1$) holds, that is: $$\left(1+\dfrac{\alpha}{n+1}\right)^{n+1}\le \dfrac{1}{1-\alpha}$$ i can't manage For $1\le \left(1+\dfrac{\alpha}{n}\right)^{n}$","Let $n\geq 1$ and $\alpha \in [0,1)$ show that :   $$1\le \left(1+\dfrac{\alpha}{n}\right)^{n}\le \dfrac{1}{1-\alpha}$$ This question is related to that one Show that ${n \choose k}\leq n^k$ My thoughts: To prove that the following statement, which we will call P(n), holds for all natural numbers n: $$1\le \left(1+\dfrac{\alpha}{n}\right)^{n}\le \dfrac{1}{1-\alpha}$$ so my proof that P($n$) is true for each natural number $n$ proceeds as follows: Basis: Show that the statement holds for $n=1$. P($1$) amounts to the statement: $$1\le \left(1+\dfrac{\alpha}{1}\right)^{1}\le \dfrac{1}{1-\alpha}$$ $$\iff$$ $$1\le \left(1-{\alpha}^2\right)\le 1$$ since $\alpha \in [0,1) \implies 0\le \alpha < 1 \implies 0\le \alpha^2 < 1 \implies -1 \le -\alpha^2 < 0 \implies 0 \le 1-\alpha^2 < 1$ then the statement is true for $n=1$. Thus it has been shown that P($1$) holds Inductive step: Show that if P($n$) holds, then also P($n+1$) holds. This can be done as follows. Assume P($n$) holds. It must then be shown that P($n+1$) holds, that is: $$1\le \left(1+\dfrac{\alpha}{n+1}\right)^{n+1}\le \dfrac{1}{1-\alpha}$$ I can't manage is my reasoning correct and is there other ways to prove that Edit since there is probleme in the case of P(1) becuase i shouldn't write $1\le \left(1-{\alpha}^2\right)\le 1.$ since  the left inequality is not true. i have to break it in two separate case and do it then we ve : For $\left(1+\dfrac{\alpha}{n}\right)^{n}\le \dfrac{1}{1-\alpha}$ To prove that the following statement, which we will call P(n), holds for all natural numbers n: $$\left(1+\dfrac{\alpha}{n}\right)^{n}\le \dfrac{1}{1-\alpha}$$ so my proof that P($n$) is true for each natural number $n$ proceeds as follows: Basis: Show that the statement holds for $n=1$. P($1$) amounts to the statement: $\left(1+\dfrac{\alpha}{1}\right)^{1}\le \dfrac{1}{1-\alpha}$ since $\alpha \in [0,1) \implies 0\le \alpha < 1 \implies 0\le \alpha^2 < 1 \implies -1 \le -\alpha^2 < 0 \implies 0 \le 1-\alpha^2 < 1$ then the statement is true for $n=1$. Thus it has been shown that P($1$) holds Inductive step: Show that if P($n$) holds, then also P($n+1$) holds. This can be done as follows. Assume P($n$) holds. It must then be shown that P($n+1$) holds, that is: $$\left(1+\dfrac{\alpha}{n+1}\right)^{n+1}\le \dfrac{1}{1-\alpha}$$ i can't manage For $1\le \left(1+\dfrac{\alpha}{n}\right)^{n}$",,"['calculus', 'real-analysis', 'analysis', 'inequality']"
34,What can we say about the rate of growth of a function growing faster than all polynomials?,What can we say about the rate of growth of a function growing faster than all polynomials?,,Suppose $f: \mathbb{R} \rightarrow \mathbb{R}$ satisfies the following: $$ \forall k \in \mathbb{N} \hspace{5pt} \lim_{t \rightarrow \infty} \frac{t^k}{f(t)} = 0.$$ Can we deduce a stronger growth rate for $f$? For instance (I know this is very hopeful - I am just giving an example of the sort of thing I mean) can we say $$ \limsup_{t \rightarrow \infty} \frac{e^t}{f(t)} < \infty \hspace{5pt}?$$ Many thanks for your help.,Suppose $f: \mathbb{R} \rightarrow \mathbb{R}$ satisfies the following: $$ \forall k \in \mathbb{N} \hspace{5pt} \lim_{t \rightarrow \infty} \frac{t^k}{f(t)} = 0.$$ Can we deduce a stronger growth rate for $f$? For instance (I know this is very hopeful - I am just giving an example of the sort of thing I mean) can we say $$ \limsup_{t \rightarrow \infty} \frac{e^t}{f(t)} < \infty \hspace{5pt}?$$ Many thanks for your help.,,"['real-analysis', 'analysis', 'asymptotics']"
35,Sequence which converges pointwise but not uniformly?,Sequence which converges pointwise but not uniformly?,,"it might be simple but I don't find a sequence $f_n: [0,1] \rightarrow \mathbb{R}, n \in \mathbb{N}$ that converges pointwise but not uniformly. First I thought it could be $f_n(x) = \frac{x}{n}$ but it is not right, is it? Thanks for help!","it might be simple but I don't find a sequence $f_n: [0,1] \rightarrow \mathbb{R}, n \in \mathbb{N}$ that converges pointwise but not uniformly. First I thought it could be $f_n(x) = \frac{x}{n}$ but it is not right, is it? Thanks for help!",,"['real-analysis', 'analysis', 'convergence-divergence']"
36,What is a solution of such equation concerning the arithmetic and integral means?,What is a solution of such equation concerning the arithmetic and integral means?,,"Let $f:[a,b] \rightarrow \mathbb R$ be integrable and satisfies  $$ f\left(\frac{x+y}{2}\right)=\frac{1}{y-x} \int_x^y f(t)dt  $$ for all $x \neq y$, $x,y \in [a,b]$. What about $f$? Is it affine function? Thanks","Let $f:[a,b] \rightarrow \mathbb R$ be integrable and satisfies  $$ f\left(\frac{x+y}{2}\right)=\frac{1}{y-x} \int_x^y f(t)dt  $$ for all $x \neq y$, $x,y \in [a,b]$. What about $f$? Is it affine function? Thanks",,['analysis']
37,Prove a sequence converges without knowing its limit,Prove a sequence converges without knowing its limit,,"Let $(X_n)$ be the sequence with $X_1=2$ and $X_n=\sqrt{5X_{n-1} + 6}$ for all $n\ge 2$. How can you prove that it is convergent? I know that its limit is $6$, but the question is how to rigorously prove its convergence without knowing it.","Let $(X_n)$ be the sequence with $X_1=2$ and $X_n=\sqrt{5X_{n-1} + 6}$ for all $n\ge 2$. How can you prove that it is convergent? I know that its limit is $6$, but the question is how to rigorously prove its convergence without knowing it.",,"['real-analysis', 'analysis', 'convergence-divergence', 'uniform-convergence', 'elementary-functions']"
38,Square Integrable and Continuous,Square Integrable and Continuous,,"I have come across the notation $L^2(\Omega) \cap C(\Omega)$; while I believe understand the resulting behavior, I can't get my head around the machinery.  When I think about $f \in C(\Omega)$ I am thinking out a single continuous function. However, for $f \in L^2(\Omega)$ I really mean a representative in an equivalence class. The upshot: $C(\Omega)$ seems like a collection of functions (maybe this is my flaw) $L^2(\Omega)$ is  a collection of equivalence classes because of this I am have a hard time understanding the meaning of $L^2(\Omega) \cap C(\Omega)$ and as a result what it means for $f \in L^2(\Omega) \cap C(\Omega)$. Can someone help shed some light on how to think of this? Thanks!","I have come across the notation $L^2(\Omega) \cap C(\Omega)$; while I believe understand the resulting behavior, I can't get my head around the machinery.  When I think about $f \in C(\Omega)$ I am thinking out a single continuous function. However, for $f \in L^2(\Omega)$ I really mean a representative in an equivalence class. The upshot: $C(\Omega)$ seems like a collection of functions (maybe this is my flaw) $L^2(\Omega)$ is  a collection of equivalence classes because of this I am have a hard time understanding the meaning of $L^2(\Omega) \cap C(\Omega)$ and as a result what it means for $f \in L^2(\Omega) \cap C(\Omega)$. Can someone help shed some light on how to think of this? Thanks!",,"['analysis', 'lebesgue-integral', 'lebesgue-measure']"
39,How find the minimum of the value $k$ such this intergral $\int_{0}^{1}f^2(x)dx\le k\left(\int_{0}^{1}f(x)dx\right)^2$,How find the minimum of the value  such this intergral,k \int_{0}^{1}f^2(x)dx\le k\left(\int_{0}^{1}f(x)dx\right)^2,"Find the minimum of the value $k$, such have    $$\int_{0}^{1}f^2(x)dx\le k\left(\int_{0}^{1}f(x)dx\right)^2$$   for any integrable function $f(x)$,and      $1\le f(x)\le 2,x\in(0,1)$ My idea: use Cauchy-Schwarz inequality we have $$\int_{0}^{1}f^2(x)dx\ge\left(\int_{0}^{1}f(x)dx\right)^2$$ I know this can't usefull to solve my problem. Thank you","Find the minimum of the value $k$, such have    $$\int_{0}^{1}f^2(x)dx\le k\left(\int_{0}^{1}f(x)dx\right)^2$$   for any integrable function $f(x)$,and      $1\le f(x)\le 2,x\in(0,1)$ My idea: use Cauchy-Schwarz inequality we have $$\int_{0}^{1}f^2(x)dx\ge\left(\int_{0}^{1}f(x)dx\right)^2$$ I know this can't usefull to solve my problem. Thank you",,"['analysis', 'integral-inequality']"
40,How to prove divergence of the integral $\int_{0}^{\infty}\frac{\sin(x)}{x^{2}}dx$,How to prove divergence of the integral,\int_{0}^{\infty}\frac{\sin(x)}{x^{2}}dx,"I want to show that the following integral diverges: $$\int_{0}^{\infty}\frac{\sin(x)}{x^{2}}dx$$ I used the substitution $ t = \frac{1}{x} $ to transform this integral into $$\int_{0}^{\infty}\sin \frac{1}{t}dt$$ It seems intuitive that for large $t$, $\sin \frac{1}{t} = O(\frac{1}{t})$ and so the last integral diverges and so does the original. Is this correct?","I want to show that the following integral diverges: $$\int_{0}^{\infty}\frac{\sin(x)}{x^{2}}dx$$ I used the substitution $ t = \frac{1}{x} $ to transform this integral into $$\int_{0}^{\infty}\sin \frac{1}{t}dt$$ It seems intuitive that for large $t$, $\sin \frac{1}{t} = O(\frac{1}{t})$ and so the last integral diverges and so does the original. Is this correct?",,"['real-analysis', 'integration', 'analysis', 'improper-integrals']"
41,help with showing completeness,help with showing completeness,,"Let $\left\{H_n\right\}_{n=1}^\infty$ be a sequence of Hilbert spaces and let $H=\left\{\left\{x_n\right\}:x_n\in H_n, \sum ||x_n||^2<\infty \right\}$. Define the inner product as $(\left\{x_n\right\}, \left\{y_n\right\})=\sum (x_n,y_n)$ Then $H$ is complete with respect to the induced norm $\left\|x_n \right\|=(\left\{x_n\right\}, \left\{x_n\right\})^\frac{1}{2}$. I want to consider a Cauchy sequence $\left\{ \left\{x_{i,m}  \right\}_{i=1}^\infty \right\}_{m=1}^\infty$ and use the fact that $\sum ||x_n||^2<\infty$, but here is where I run into a problem: $\displaystyle \sum_{m=1}^\infty ( \left\{x_i  \right\}_m , \left\{x_i  \right\}_m)= \sum_{m=1}^\infty \sum_{i=1}^\infty (x_{i,m}, x_{i,m})$. The sum above may not be necessarily finite, right? I want to show that it is finite so that this way I know that $\left\{ \left\{x_i  \right\}_m \right\}_{m=1}^\infty$ has a limit. What am I doing wrong? Thanks for your help.","Let $\left\{H_n\right\}_{n=1}^\infty$ be a sequence of Hilbert spaces and let $H=\left\{\left\{x_n\right\}:x_n\in H_n, \sum ||x_n||^2<\infty \right\}$. Define the inner product as $(\left\{x_n\right\}, \left\{y_n\right\})=\sum (x_n,y_n)$ Then $H$ is complete with respect to the induced norm $\left\|x_n \right\|=(\left\{x_n\right\}, \left\{x_n\right\})^\frac{1}{2}$. I want to consider a Cauchy sequence $\left\{ \left\{x_{i,m}  \right\}_{i=1}^\infty \right\}_{m=1}^\infty$ and use the fact that $\sum ||x_n||^2<\infty$, but here is where I run into a problem: $\displaystyle \sum_{m=1}^\infty ( \left\{x_i  \right\}_m , \left\{x_i  \right\}_m)= \sum_{m=1}^\infty \sum_{i=1}^\infty (x_{i,m}, x_{i,m})$. The sum above may not be necessarily finite, right? I want to show that it is finite so that this way I know that $\left\{ \left\{x_i  \right\}_m \right\}_{m=1}^\infty$ has a limit. What am I doing wrong? Thanks for your help.",,"['real-analysis', 'analysis', 'measure-theory', 'hilbert-spaces']"
42,Question about sup norm,Question about sup norm,,"Let $x \in \mathbb{R}^n$. Define $|x| = \max\{ |x_1|,...,|x_n|\} $. I want to show that this is a norm on $R^n$. This is my reasoning. First, notice $$ |x| = \max\{ |x_i| \} \geq |x_i| \; \forall i ,\;\; |x_i| \geq 0 \implies |x| \geq 0$$ $$ |cx| = \max\{ |cx_i| \} = \max\{ |c||x_i| \} = |c| \max\{ |x_i| \} = |c| |x|$$ $$ |x + y| = \max\{ |x_i + y_i| \} = |x_i +y_i| \; \text{for some i} \leq |x_i| + |y_i|  \leq \max\{|x_i|\} + \max\{|y_i|\} = |x| + |y| $$ Hence, this is indeed a norm on $R^n$. Is this correct? Thanks in advance for your help. Also, I have a bit of trouble seeing if this inequality is true: $$ \max\{ |x_1|,...,|x_n|\} \leq \sqrt{ \sum x_i^2} \leq \sqrt{n} \max\{ |x_1|,...,|x_n|\} $$ Can I show this by induction? Thanks","Let $x \in \mathbb{R}^n$. Define $|x| = \max\{ |x_1|,...,|x_n|\} $. I want to show that this is a norm on $R^n$. This is my reasoning. First, notice $$ |x| = \max\{ |x_i| \} \geq |x_i| \; \forall i ,\;\; |x_i| \geq 0 \implies |x| \geq 0$$ $$ |cx| = \max\{ |cx_i| \} = \max\{ |c||x_i| \} = |c| \max\{ |x_i| \} = |c| |x|$$ $$ |x + y| = \max\{ |x_i + y_i| \} = |x_i +y_i| \; \text{for some i} \leq |x_i| + |y_i|  \leq \max\{|x_i|\} + \max\{|y_i|\} = |x| + |y| $$ Hence, this is indeed a norm on $R^n$. Is this correct? Thanks in advance for your help. Also, I have a bit of trouble seeing if this inequality is true: $$ \max\{ |x_1|,...,|x_n|\} \leq \sqrt{ \sum x_i^2} \leq \sqrt{n} \max\{ |x_1|,...,|x_n|\} $$ Can I show this by induction? Thanks",,"['calculus', 'real-analysis']"
43,"Banach Measures: total, finitely-additive, isometry invariant extensions of Lebesgue Measure","Banach Measures: total, finitely-additive, isometry invariant extensions of Lebesgue Measure",,"I've been reading about paradoxical sets, mainly paradoxical subsets of the plane. As a consequence of this, I've been reading a couple of G.A. Sherman's papers on the subject. In his paper ""Properties of Paradoxical Sets in the Plane,"" an interesting result is that any subset of the plane with nonempty interior is not paradoxical (this is very interesting in that it is contrary to the the 3D analog where all bounded subsets with nonempty interior have to be paradoxical by the Banach-Tarski Paradox). However, to prove this, he used a total, finitely-additive, isometry-invariant extension of Lebesgue Measure, which he calls a Banach measure. His only references for this seem to be a paper by Banach, which is in French, and a theorem from ""The Banach-Tarski Paradox"" by Stan Wagon, which doesn't seem to mention Banach measures by name, nor does any of the surrounding material expound on this. I think I've been able to understand and fill in the details for most of his proofs for the main theorems, but I want to solidify my understanding by getting to know Banach measures better. Are there any textbooks or references that make specific use of Banach measures? What theorems can we take from the Lebesgue measure and put in terms of Banach measures? I assume, since he uses it, that nonempty interior implies positive measure for Banach measures, like it does for the Lebesgue measure, but is there anything else?","I've been reading about paradoxical sets, mainly paradoxical subsets of the plane. As a consequence of this, I've been reading a couple of G.A. Sherman's papers on the subject. In his paper ""Properties of Paradoxical Sets in the Plane,"" an interesting result is that any subset of the plane with nonempty interior is not paradoxical (this is very interesting in that it is contrary to the the 3D analog where all bounded subsets with nonempty interior have to be paradoxical by the Banach-Tarski Paradox). However, to prove this, he used a total, finitely-additive, isometry-invariant extension of Lebesgue Measure, which he calls a Banach measure. His only references for this seem to be a paper by Banach, which is in French, and a theorem from ""The Banach-Tarski Paradox"" by Stan Wagon, which doesn't seem to mention Banach measures by name, nor does any of the surrounding material expound on this. I think I've been able to understand and fill in the details for most of his proofs for the main theorems, but I want to solidify my understanding by getting to know Banach measures better. Are there any textbooks or references that make specific use of Banach measures? What theorems can we take from the Lebesgue measure and put in terms of Banach measures? I assume, since he uses it, that nonempty interior implies positive measure for Banach measures, like it does for the Lebesgue measure, but is there anything else?",,"['analysis', 'measure-theory', 'reference-request']"
44,Fredholm integral equation of first kind,Fredholm integral equation of first kind,,"I want to solve the Fredholm integral equation of first kind: $$ \int_L K(x,y)U(y)dy = f(x) $$ in these equation the function $U(y)$ is the unknown and the so-called kernel $K$ and the right hand side $f$ are given functions. I've used the set of orthogonal functions $\phi_i(x)$ i.e., $$\langle \phi_i,\phi_j \rangle = \left\{\begin{array}{c} 0 \ i\neq j \\ 1 \ i=j \end{array}\right. $$  and approximate the $f(x)$,$K(x,y)$ and $U(y)$ by these set functions.$$f(x)=\sum_i\alpha_i \phi_i \text{ where } \alpha_i= \langle f(x),\phi_i \rangle, \text{ and } U(y)=\sum_i\beta_i \phi_i \\K(x,y)=\sum_i\sum_j\omega_{i,j} \phi_i(x) \phi_j(y)$$ Now by substitution of these approximate into main equation and using the property of orthogonality, I obtained a linear equation in this form $A \beta=\alpha$, where $ \alpha=(\alpha_1,\alpha_2,...,\alpha_n)^T$, $\beta=(\beta_1,\beta_2,...,\beta_n)^T$ and $A=[\omega_{i,j}]$ but the determinant of $A$ matrix is zero and I can't solve the equation. What happened in this method?","I want to solve the Fredholm integral equation of first kind: $$ \int_L K(x,y)U(y)dy = f(x) $$ in these equation the function $U(y)$ is the unknown and the so-called kernel $K$ and the right hand side $f$ are given functions. I've used the set of orthogonal functions $\phi_i(x)$ i.e., $$\langle \phi_i,\phi_j \rangle = \left\{\begin{array}{c} 0 \ i\neq j \\ 1 \ i=j \end{array}\right. $$  and approximate the $f(x)$,$K(x,y)$ and $U(y)$ by these set functions.$$f(x)=\sum_i\alpha_i \phi_i \text{ where } \alpha_i= \langle f(x),\phi_i \rangle, \text{ and } U(y)=\sum_i\beta_i \phi_i \\K(x,y)=\sum_i\sum_j\omega_{i,j} \phi_i(x) \phi_j(y)$$ Now by substitution of these approximate into main equation and using the property of orthogonality, I obtained a linear equation in this form $A \beta=\alpha$, where $ \alpha=(\alpha_1,\alpha_2,...,\alpha_n)^T$, $\beta=(\beta_1,\beta_2,...,\beta_n)^T$ and $A=[\omega_{i,j}]$ but the determinant of $A$ matrix is zero and I can't solve the equation. What happened in this method?",,"['linear-algebra', 'integration', 'analysis', 'numerical-methods']"
45,convergence in $L^1$ for product of functions,convergence in  for product of functions,L^1,If $f_n$ converges to $f$ in $L^1$ and $g_n$ converges to $g$ in $L^1$. Does it necessarily mean that $f_ng_n$ converges to $fg$ in $L^1$ for finite measure spaces.,If $f_n$ converges to $f$ in $L^1$ and $g_n$ converges to $g$ in $L^1$. Does it necessarily mean that $f_ng_n$ converges to $fg$ in $L^1$ for finite measure spaces.,,"['real-analysis', 'analysis', 'measure-theory', 'integration', 'lp-spaces']"
46,schwarz class and $L^2(\mathbb{R})$,schwarz class and,L^2(\mathbb{R}),"Schwarz and $L^2$ both have the property that the Fourier transform is defined and bijective as a self-map of these spaces.  Are they related in anyway or is this coincidence? (i.e. dual in some sense, or perhaps both arising from some more general construction.)","Schwarz and $L^2$ both have the property that the Fourier transform is defined and bijective as a self-map of these spaces.  Are they related in anyway or is this coincidence? (i.e. dual in some sense, or perhaps both arising from some more general construction.)",,"['real-analysis', 'analysis', 'harmonic-analysis']"
47,Prime Harmonic Series $\sum\limits_{p\in\mathbb P}\frac1p$,Prime Harmonic Series,\sum\limits_{p\in\mathbb P}\frac1p,"We have following identity: ($p$ is a prime number) $$\left(1+\frac{1}{p}\right)\sum_{k=0}^n\frac{1}{p^{2k}}=\sum_{k=0}^{2n+1}\frac{1}{p^k}$$ Now, How to derive the following inequality from the above identity? Or use this identity to prove following inequality? $$\prod_{p<N}\left(1+\frac{1}{p}\right)\sum_{k<N}\frac{1}{k^2}\ge\sum_{n<N}\frac{1}{n}.$$ This inequality shows that $\sum\frac{1}{p}$ is divergent.","We have following identity: ($p$ is a prime number) $$\left(1+\frac{1}{p}\right)\sum_{k=0}^n\frac{1}{p^{2k}}=\sum_{k=0}^{2n+1}\frac{1}{p^k}$$ Now, How to derive the following inequality from the above identity? Or use this identity to prove following inequality? $$\prod_{p<N}\left(1+\frac{1}{p}\right)\sum_{k<N}\frac{1}{k^2}\ge\sum_{n<N}\frac{1}{n}.$$ This inequality shows that $\sum\frac{1}{p}$ is divergent.",,"['real-analysis', 'analysis', 'number-theory', 'prime-numbers', 'analytic-number-theory']"
48,Compute $\int_{\mathbb {R}}\sin \left(\frac {\pi x}{x^2+1}\right)\frac{1}{x^2+1} dx$,Compute,\int_{\mathbb {R}}\sin \left(\frac {\pi x}{x^2+1}\right)\frac{1}{x^2+1} dx,How do I compute this integral? $$\int_{\mathbb {R}}\sin \left(\frac {\pi x}{x^2+1}\right)\frac{1}{x^2+1} dx$$,How do I compute this integral? $$\int_{\mathbb {R}}\sin \left(\frac {\pi x}{x^2+1}\right)\frac{1}{x^2+1} dx$$,,"['real-analysis', 'analysis', 'fourier-analysis']"
49,Tough Legendre Integral,Tough Legendre Integral,,"I am currently fighting with the following integral. I have simplified it to this one here: $\int_{-1}^{\cos(\alpha)} P_l(t)P_{l'}(t) dt$, where $P_l$ is the l-th Legendre polynomial. unfortunately you cannot use orthogonality, so this is somewhat hard to do, but maybe somebody here has an idea.","I am currently fighting with the following integral. I have simplified it to this one here: $\int_{-1}^{\cos(\alpha)} P_l(t)P_{l'}(t) dt$, where $P_l$ is the l-th Legendre polynomial. unfortunately you cannot use orthogonality, so this is somewhat hard to do, but maybe somebody here has an idea.",,"['calculus', 'real-analysis', 'analysis', 'integration']"
50,Differentiable but not Absolutely continuous [duplicate],Differentiable but not Absolutely continuous [duplicate],,This question already has answers here : Does the everywhere differentiability of $f$ imply it is absolutely continuous on a compact interval? (2 answers) Closed 3 years ago . Please give an example (if it exists) for a function which is differentiable everywhere but not absolutely continuous.,This question already has answers here : Does the everywhere differentiability of $f$ imply it is absolutely continuous on a compact interval? (2 answers) Closed 3 years ago . Please give an example (if it exists) for a function which is differentiable everywhere but not absolutely continuous.,,"['real-analysis', 'analysis', 'partial-differential-equations']"
51,"Show $\langle x,\nabla f \rangle = pf(x)$",Show,"\langle x,\nabla f \rangle = pf(x)","I'm  trying to figure out this problem. Perhaps Someone could give me some hints/solve it for me? It would be much appreciated. Let $U$ be an open subset of $R^n$ and suppose $f:U\rightarrow R$. Then $f$ is homogeneous of degree $p$ if $f(\lambda x) = \lambda^p f(x)$ for all $x\in U$, $\lambda \in R$,and $\lambda x \in U$ prove that $\langle x,\nabla f(x) \rangle = pf(x)$ thanks a bunch guys","I'm  trying to figure out this problem. Perhaps Someone could give me some hints/solve it for me? It would be much appreciated. Let $U$ be an open subset of $R^n$ and suppose $f:U\rightarrow R$. Then $f$ is homogeneous of degree $p$ if $f(\lambda x) = \lambda^p f(x)$ for all $x\in U$, $\lambda \in R$,and $\lambda x \in U$ prove that $\langle x,\nabla f(x) \rangle = pf(x)$ thanks a bunch guys",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus']"
52,"$3\int_{0}^{1}(f'(x))^2dx \geq (2\int_{0}^{1}f(x)dx)^2 \impliedby 2\int_{0}^{\frac{1}{2}}f(x)\,\mathrm dx=\int_{\frac{1}{2}}^{1}f(x) \,\mathrm dx$",,"3\int_{0}^{1}(f'(x))^2dx \geq (2\int_{0}^{1}f(x)dx)^2 \impliedby 2\int_{0}^{\frac{1}{2}}f(x)\,\mathrm dx=\int_{\frac{1}{2}}^{1}f(x) \,\mathrm dx","Let $f : \mathbb{R} \to \mathbb{R} $ be a differentiable function.  Suppose that $2\int_{0}^{\frac{1}{2}}f(x)\,\mathrm dx=\int_{\frac{1}{2}}^{1}f(x) \,\mathrm dx$ Show that $$3\int_{0}^{1}(f'(x))^2 \,\mathrm dx \geq (2\int_{0}^{1}f(x)\,\mathrm dx)^2$$","Let $f : \mathbb{R} \to \mathbb{R} $ be a differentiable function.  Suppose that $2\int_{0}^{\frac{1}{2}}f(x)\,\mathrm dx=\int_{\frac{1}{2}}^{1}f(x) \,\mathrm dx$ Show that $$3\int_{0}^{1}(f'(x))^2 \,\mathrm dx \geq (2\int_{0}^{1}f(x)\,\mathrm dx)^2$$",,"['real-analysis', 'analysis', 'integration', 'inequality', 'integral-inequality']"
53,Uniform convergence in the Poisson equation...,Uniform convergence in the Poisson equation...,,"Let $K$ a compact set and $(u^j)_j$ a sequence of functions bounded uniformly such that $$\Delta u^j=f^j(u^j), \ \ \ \mbox{in} \ \ K,$$ where the sequence $(f^j)$ is bounded uniformly and equicontinuous, converging uniformly in the compact set $K$ to $\widehat f$. Can I say that exist a function $\widehat u$ such that $u^j$ converge uniformly to $\widehat u$ and $\Delta\widehat u=\widehat f(\widehat u)$??? Why?? In the Gilbarg-Trudinger (corollary $4.7$, page: $61$), we have the uniform convergence to a solution of poisson equation, but in this case, the function $f$ is the same for each $u^j$. My question is when the function $f$ changes for each $j$.","Let $K$ a compact set and $(u^j)_j$ a sequence of functions bounded uniformly such that $$\Delta u^j=f^j(u^j), \ \ \ \mbox{in} \ \ K,$$ where the sequence $(f^j)$ is bounded uniformly and equicontinuous, converging uniformly in the compact set $K$ to $\widehat f$. Can I say that exist a function $\widehat u$ such that $u^j$ converge uniformly to $\widehat u$ and $\Delta\widehat u=\widehat f(\widehat u)$??? Why?? In the Gilbarg-Trudinger (corollary $4.7$, page: $61$), we have the uniform convergence to a solution of poisson equation, but in this case, the function $f$ is the same for each $u^j$. My question is when the function $f$ changes for each $j$.",,"['analysis', 'partial-differential-equations']"
54,"If $f:\mathbb R\to\mathbb R$ is continuous and $f^3(x)=x$, then $ f(x)=x$ [duplicate]","If  is continuous and , then  [duplicate]",f:\mathbb R\to\mathbb R f^3(x)=x  f(x)=x,"This question already has answers here : Closed 11 years ago . Possible Duplicate: 3rd iterate of a continuous function equals identity function Assume $f:\mathbb R\to\mathbb R$  is  continuous and $f^3(x)=x $ $\forall x$. How can I prove that$$\forall x\in\mathbb R,\ f(x)=x$$","This question already has answers here : Closed 11 years ago . Possible Duplicate: 3rd iterate of a continuous function equals identity function Assume $f:\mathbb R\to\mathbb R$  is  continuous and $f^3(x)=x $ $\forall x$. How can I prove that$$\forall x\in\mathbb R,\ f(x)=x$$",,"['analysis', 'functions']"
55,Let $f:\mathbb R \rightarrow \mathbb R$ be a differentiable even function,Let  be a differentiable even function,f:\mathbb R \rightarrow \mathbb R,"I came across the above problem. I see that $G'(x)=f'(x)f(\sqrt{\tan(f(x))})$. Now since $f$ is a differentiable even function, $f(-x)=f(x)$ and so $-f'(-x)=f'(x)$ and thus $f'(0)=0$ and hence we can conclude $G'(0)=0.$ Am I going in the right direction?","I came across the above problem. I see that $G'(x)=f'(x)f(\sqrt{\tan(f(x))})$. Now since $f$ is a differentiable even function, $f(-x)=f(x)$ and so $-f'(-x)=f'(x)$ and thus $f'(0)=0$ and hence we can conclude $G'(0)=0.$ Am I going in the right direction?",,"['real-analysis', 'analysis']"
56,Can we define the limit using only rational nos.?,Can we define the limit using only rational nos.?,,Does the epsilon-delta definition require that the function $f$ is defined to map from $\mathbb R \rightarrow \mathbb R$? Would it be the same to define $f: \mathbb Q \rightarrow \mathbb Q$?,Does the epsilon-delta definition require that the function $f$ is defined to map from $\mathbb R \rightarrow \mathbb R$? Would it be the same to define $f: \mathbb Q \rightarrow \mathbb Q$?,,"['calculus', 'analysis']"
57,Show that the n-dimensional Hausdorff measure of an $n$-dimensional cube is positive and finite.,Show that the n-dimensional Hausdorff measure of an -dimensional cube is positive and finite.,n,"Show that the n-dimensional Hausdorff measure of an $n$-dimensional cube is positive and finite. I can easily show that if it is finite then the $n+1$ dimensional measure is $0$ and the $n-1$ dimensional measure is $\infty$, but I'm not sure how to show that it is at exactly $n$ that the positive finite case occurs. Can anyone provide any tips?","Show that the n-dimensional Hausdorff measure of an $n$-dimensional cube is positive and finite. I can easily show that if it is finite then the $n+1$ dimensional measure is $0$ and the $n-1$ dimensional measure is $\infty$, but I'm not sure how to show that it is at exactly $n$ that the positive finite case occurs. Can anyone provide any tips?",,"['real-analysis', 'analysis', 'measure-theory']"
58,Spivak Calculus 3rd ed. $|a + b| \leq |a| + |b|$,Spivak Calculus 3rd ed.,|a + b| \leq |a| + |b|,I'm working through the first chapter of Michael Spivak's Calculus 3rd ed. Towards the end of the chapter he proves $ |a + b| ≤ |a| + |b| $ using the observation that $|a|= \sqrt{ a^2 }$ when  $a$ is $ ≥ 0 $ . $ |a + b| ≤ |a| + |b| $ $$ (|a + b|)^2 = (a + b)^2 $$ $$= a^2 + 2ab + b^2 $$ $$ ≤ a^2 + 2|a|  |b| + b^2 $$  $$ = |a|^2 + 2|a|  |b| + |b|^2 $$ $$ = (|a| + |b|)^2 $$ I am unsure about what's going on with the equality sign. How does it go from $=$ to $≤$ on line 3 when a and b are changed to their absolute value and back to $=$ again on line 4 when $a^2$ and $b^2$ are changed to their absolute values?,I'm working through the first chapter of Michael Spivak's Calculus 3rd ed. Towards the end of the chapter he proves $ |a + b| ≤ |a| + |b| $ using the observation that $|a|= \sqrt{ a^2 }$ when  $a$ is $ ≥ 0 $ . $ |a + b| ≤ |a| + |b| $ $$ (|a + b|)^2 = (a + b)^2 $$ $$= a^2 + 2ab + b^2 $$ $$ ≤ a^2 + 2|a|  |b| + b^2 $$  $$ = |a|^2 + 2|a|  |b| + |b|^2 $$ $$ = (|a| + |b|)^2 $$ I am unsure about what's going on with the equality sign. How does it go from $=$ to $≤$ on line 3 when a and b are changed to their absolute value and back to $=$ again on line 4 when $a^2$ and $b^2$ are changed to their absolute values?,,"['calculus', 'analysis', 'proof-writing', 'absolute-value']"
59,Why is 'Antisymmetry' named so?,Why is 'Antisymmetry' named so?,,"So when we talk about order relations for the familiar number systems, we are always introduced to the antisymmetry property which is $x \le y, x \ge y \implies x=y$. When I think of the word 'antisymmetry', I think of something being the opposite of symmetry but not asymmetry. Is there any meaningful way to interpret it?","So when we talk about order relations for the familiar number systems, we are always introduced to the antisymmetry property which is $x \le y, x \ge y \implies x=y$. When I think of the word 'antisymmetry', I think of something being the opposite of symmetry but not asymmetry. Is there any meaningful way to interpret it?",,"['analysis', 'elementary-set-theory', 'terminology', 'order-theory', 'relations']"
60,harmonic function question,harmonic function question,,"Let $u$ and $v$ be real-valued harmonic functions on $U=\{z:|z|<1\}$.  Let $A=\{z\in U:u(z)=v(z)\}$.  Suppose $A$ contains a nonempty open set.  Prove $A=U$. Here is what I have so far: Let $h=u-v$.  Then $h$ is harmonic.  Let $X$ be the set of all $z$ such that $h(z)=0$ in some open neighborhood of $z$.  By our assumptions on $A$, $X$ is not empty.  Let $z\in X$.  Then $h(z)=0$ on some open set $V$ containing $z$.  If $x\in V$, then $h(w)=0$ in some open set containing $x$, namely $V$.  So $X$ is open. I want to show $X$ is also closed but I am having trouble doing so.  Any suggestions:","Let $u$ and $v$ be real-valued harmonic functions on $U=\{z:|z|<1\}$.  Let $A=\{z\in U:u(z)=v(z)\}$.  Suppose $A$ contains a nonempty open set.  Prove $A=U$. Here is what I have so far: Let $h=u-v$.  Then $h$ is harmonic.  Let $X$ be the set of all $z$ such that $h(z)=0$ in some open neighborhood of $z$.  By our assumptions on $A$, $X$ is not empty.  Let $z\in X$.  Then $h(z)=0$ on some open set $V$ containing $z$.  If $x\in V$, then $h(w)=0$ in some open set containing $x$, namely $V$.  So $X$ is open. I want to show $X$ is also closed but I am having trouble doing so.  Any suggestions:",,"['analysis', 'harmonic-functions']"
61,Radius of convergence of Power Series!,Radius of convergence of Power Series!,,"Given two power series $$\sum_{n=0}^{\infty} a_nx^n, \sum_{n=0}^{\infty} b_nx^n$$ with convergent radius $R_{1}$ and $R_{2}$ respectively. Suppose $R_{1}<R_{2}$,now what about the convergent radius of $\sum_{n=0}^{\infty} (a_n+b_n)x^n$","Given two power series $$\sum_{n=0}^{\infty} a_nx^n, \sum_{n=0}^{\infty} b_nx^n$$ with convergent radius $R_{1}$ and $R_{2}$ respectively. Suppose $R_{1}<R_{2}$,now what about the convergent radius of $\sum_{n=0}^{\infty} (a_n+b_n)x^n$",,"['real-analysis', 'analysis', 'power-series']"
62,Showing that the characteristic function of $\Bbb Q$ is not Riemann Integrable,Showing that the characteristic function of  is not Riemann Integrable,\Bbb Q,"So I know that the characteristic function of the rationals is not Riemann integrable and we can show this by showing that the upper and lower sums are different. But I have a theorem in my notes which states (we have not proved) that a function, $f$, is Riemann integrable iff the set of all points where $f$ is not continuous is a set of zero measure. So I was wondering if the following reasoning is ok: We know that $[0,1]$ has measure $1$ and the set $\mathbb{Q}$ is a set of zero measure. So it must be the case that $[0,1] - \mathbb{Q}_{[0,1]}$ is of measure $1$, so $f$ is not Riemann integrable. (I know that this question is not very well informed, but I have this theorem and so I can use this in my exam as a quick way to show a function is not Riemann integrable, I would be more than happy for more information though.) Thanks for any help","So I know that the characteristic function of the rationals is not Riemann integrable and we can show this by showing that the upper and lower sums are different. But I have a theorem in my notes which states (we have not proved) that a function, $f$, is Riemann integrable iff the set of all points where $f$ is not continuous is a set of zero measure. So I was wondering if the following reasoning is ok: We know that $[0,1]$ has measure $1$ and the set $\mathbb{Q}$ is a set of zero measure. So it must be the case that $[0,1] - \mathbb{Q}_{[0,1]}$ is of measure $1$, so $f$ is not Riemann integrable. (I know that this question is not very well informed, but I have this theorem and so I can use this in my exam as a quick way to show a function is not Riemann integrable, I would be more than happy for more information though.) Thanks for any help",,"['integration', 'analysis', 'measure-theory']"
63,Continuity of Laplace transform,Continuity of Laplace transform,,"I'm trying to prove the Riemann integral $$\int_0 ^ \infty \frac{\sin(x)} x \ dx = \frac{\pi}{2}$$ using tools that I, as a statistician, am likely not to forget. My general approach is Show that the Laplace transform $\psi(t) := \int_0 ^ \infty \frac{e^{-xt} \sin(x)} x \ dx$ is $-\arctan(t) + \frac \pi 2$ for $t \in (0, \infty)$ by differentiating under the integral (using dominated convergence stuff since the Lebesgue integral exists here) and solving the resulting differential equation. Argue that $\psi(t)$ is continuous at $0$. I've hit a snag at step 2. So, under what conditions do we have that the Laplace transform of a function is continuous at $t = 0$? Any trivial garuntees that it should be for $\frac {\sin x} x$ in particular? I feel like this would be easier if the function was Lebesgue integrable since when I'm trying to bound $|\psi(0) - \psi(t)|$ I am having a hard time controlling the tail of the integral of $\psi(t)$ for $t$ uniformly in $t$ near $0$. I'm also finding this somewhat interesting since it looks like $\frac{\sin x} x$ is sitting on a boundary of sorts: we are Lebesgue integrable up to a point, then Riemann integrable, and past that not integrable($t > 0, = 0, < 0$) So a bunch of Lebesgue integrable functions are converging to a function which has an improper Riemann integral but no Lebesgue integral. And yet interchanging limits produces the correct answer anyways when calculating the integral. EDIT: We already have an answer below that suggests how to do this for $\frac {\sin x} x$ but I'd like to get something on the more general question of under what conditions we have right continuity of the Laplace transform at $0$. Is it always continuous provided that the function has a finite improper Riemann integral?","I'm trying to prove the Riemann integral $$\int_0 ^ \infty \frac{\sin(x)} x \ dx = \frac{\pi}{2}$$ using tools that I, as a statistician, am likely not to forget. My general approach is Show that the Laplace transform $\psi(t) := \int_0 ^ \infty \frac{e^{-xt} \sin(x)} x \ dx$ is $-\arctan(t) + \frac \pi 2$ for $t \in (0, \infty)$ by differentiating under the integral (using dominated convergence stuff since the Lebesgue integral exists here) and solving the resulting differential equation. Argue that $\psi(t)$ is continuous at $0$. I've hit a snag at step 2. So, under what conditions do we have that the Laplace transform of a function is continuous at $t = 0$? Any trivial garuntees that it should be for $\frac {\sin x} x$ in particular? I feel like this would be easier if the function was Lebesgue integrable since when I'm trying to bound $|\psi(0) - \psi(t)|$ I am having a hard time controlling the tail of the integral of $\psi(t)$ for $t$ uniformly in $t$ near $0$. I'm also finding this somewhat interesting since it looks like $\frac{\sin x} x$ is sitting on a boundary of sorts: we are Lebesgue integrable up to a point, then Riemann integrable, and past that not integrable($t > 0, = 0, < 0$) So a bunch of Lebesgue integrable functions are converging to a function which has an improper Riemann integral but no Lebesgue integral. And yet interchanging limits produces the correct answer anyways when calculating the integral. EDIT: We already have an answer below that suggests how to do this for $\frac {\sin x} x$ but I'd like to get something on the more general question of under what conditions we have right continuity of the Laplace transform at $0$. Is it always continuous provided that the function has a finite improper Riemann integral?",,"['real-analysis', 'integration', 'analysis', 'lebesgue-integral']"
64,Quantifying convexity,Quantifying convexity,,"What methods exist to quantify convexity.  Yes, a set is convex if the the line between two points in the set is contained in the set, but is there a measure of how convex a set is?  If so, what is it?","What methods exist to quantify convexity.  Yes, a set is convex if the the line between two points in the set is contained in the set, but is there a measure of how convex a set is?  If so, what is it?",,"['analysis', 'convex-analysis']"
65,Conditions for the Convolution $f \ast g$ to be Continuous at a Point,Conditions for the Convolution  to be Continuous at a Point,f \ast g,"Let $f$ and $g$ be functions on $\mathbb{R}^n$.  Let $x_0$ be a given point in the unit ball $B(0,1)$.  I am looking for sufficient conditions for the convolution $$ (f \ast g)(x) = \int_{B(0,1)} f(y)g(x-y) dy $$ to be continuous at $x_0$. I would appreciate simple proofs or references to proofs that conditions given in an answer are sufficient. In my specific application, $f$ and $g$ are continuous in $B(0,1) \setminus \{0\}$ and $x_0 \neq 0$, but I would be very interested to see conditions for other (more general) situations as well. I would also be very interested to see conditions for the situation where $B(0,1)$ is replaced by $\mathbb{R}^n$. Thanks very much!","Let $f$ and $g$ be functions on $\mathbb{R}^n$.  Let $x_0$ be a given point in the unit ball $B(0,1)$.  I am looking for sufficient conditions for the convolution $$ (f \ast g)(x) = \int_{B(0,1)} f(y)g(x-y) dy $$ to be continuous at $x_0$. I would appreciate simple proofs or references to proofs that conditions given in an answer are sufficient. In my specific application, $f$ and $g$ are continuous in $B(0,1) \setminus \{0\}$ and $x_0 \neq 0$, but I would be very interested to see conditions for other (more general) situations as well. I would also be very interested to see conditions for the situation where $B(0,1)$ is replaced by $\mathbb{R}^n$. Thanks very much!",,"['real-analysis', 'analysis', 'fourier-analysis', 'convolution']"
66,Differentiation of generating function of Hermite's polynomials,Differentiation of generating function of Hermite's polynomials,,"The generating function of Hermite's polynomials is given by $G(x,t)=e^{2xt-t^2}$ for $x, t \in \mathbf{R}$. It is known that $\displaystyle G(x,t)=\sum_{n=0}^\infty H_n(x) \frac{t^n}{n!}$ for $x, t \in \mathbf{R}$, where $H_n(x)=(-1)^n e^{x^2} \dfrac{d^n}{dx^n}e^{-x^2}$ for $x \in \mathbf{R}$ -is the $n$-th Hermite's polynomial. By properties of power series, for fixed $x\in \mathbf{R}$ we have that  $$\frac{\partial G}{\partial t}= \sum_{n=1}^\infty \frac{\partial}{\partial t}\left[H_n(x) \frac{t^n}{n!}\right]=\sum_{n=1}^\infty H_n(x) \frac{t^{n-1}}{(n-1)!}\mbox{ for }t \in \mathbf{R}.$$ I have seen in some books that also $$\frac{\partial G}{\partial x}= \sum_{n=0}^\infty \frac{\partial}{\partial x}\left[H_n(x) \frac{t^n}{n!}\right]= \sum_{n=0}^\infty H_n'(x) \frac{t^n}{n!} \mbox{ for }x \in \mathbf{R}.$$ I don't understand, what is the reason that we can differentiate this series with respect to $x$ (maybe series $\sum_{n=0}^\infty \frac{\partial}{\partial x} (...)$ is uniformly convergent or locally uniformly convergent, but I don't see why). Thanks.","The generating function of Hermite's polynomials is given by $G(x,t)=e^{2xt-t^2}$ for $x, t \in \mathbf{R}$. It is known that $\displaystyle G(x,t)=\sum_{n=0}^\infty H_n(x) \frac{t^n}{n!}$ for $x, t \in \mathbf{R}$, where $H_n(x)=(-1)^n e^{x^2} \dfrac{d^n}{dx^n}e^{-x^2}$ for $x \in \mathbf{R}$ -is the $n$-th Hermite's polynomial. By properties of power series, for fixed $x\in \mathbf{R}$ we have that  $$\frac{\partial G}{\partial t}= \sum_{n=1}^\infty \frac{\partial}{\partial t}\left[H_n(x) \frac{t^n}{n!}\right]=\sum_{n=1}^\infty H_n(x) \frac{t^{n-1}}{(n-1)!}\mbox{ for }t \in \mathbf{R}.$$ I have seen in some books that also $$\frac{\partial G}{\partial x}= \sum_{n=0}^\infty \frac{\partial}{\partial x}\left[H_n(x) \frac{t^n}{n!}\right]= \sum_{n=0}^\infty H_n'(x) \frac{t^n}{n!} \mbox{ for }x \in \mathbf{R}.$$ I don't understand, what is the reason that we can differentiate this series with respect to $x$ (maybe series $\sum_{n=0}^\infty \frac{\partial}{\partial x} (...)$ is uniformly convergent or locally uniformly convergent, but I don't see why). Thanks.",,"['analysis', 'special-functions', 'orthogonal-polynomials']"
67,Proving the uniform convergence of $\sin(\frac{x}{n}) e^{-x^2}$,Proving the uniform convergence of,\sin(\frac{x}{n}) e^{-x^2},"How might one show that $\left(\sin(\frac{x}{n})e^{-x^2}\right)_n$ converges uniformly? I tried finding the supremum by setting the 1st derivative to $0$, but that gives a hard-to-solve equation. There must therefore be a looser bound, but I am not seeing it. Thanks.","How might one show that $\left(\sin(\frac{x}{n})e^{-x^2}\right)_n$ converges uniformly? I tried finding the supremum by setting the 1st derivative to $0$, but that gives a hard-to-solve equation. There must therefore be a looser bound, but I am not seeing it. Thanks.",,"['real-analysis', 'analysis']"
68,"If two normed spaces are Lipschitz equivalent, then one if complete iff the other is","If two normed spaces are Lipschitz equivalent, then one if complete iff the other is",,"Prove that if two normed spaces are Lipschitz equivalent, then one if complete iff the other is. My thoughts: Let $ (V_1, \Vert\cdot\Vert_1) $ and $ (V_2, \Vert\cdot\Vert_2) $ be Lipschitz equivalent normed vector spaces. Then there exists $f : V_1 \to V_2 $, and constants $h, k > 0 $, such that $ h\Vert f(x) - f(y)\Vert_2 \leq \Vert x-y\Vert_1 \leq k\Vert f(x) - f(y)\Vert_2 $ for all $ x,y, \in V_1 $. Suppose $(V_2, \Vert\cdot\Vert_2) $ is complete. Clearly everything is symmetrical, so we only really need to prove this in one direction. I can see that if $ (x_n) $ is a Cauchy sequence in $V_1$, then $(f(x_n))$ is Cauchy in $ V_2 $. I can also see that $ f $ is uniformly continuous. How can I turn this into a proof? Thanks","Prove that if two normed spaces are Lipschitz equivalent, then one if complete iff the other is. My thoughts: Let $ (V_1, \Vert\cdot\Vert_1) $ and $ (V_2, \Vert\cdot\Vert_2) $ be Lipschitz equivalent normed vector spaces. Then there exists $f : V_1 \to V_2 $, and constants $h, k > 0 $, such that $ h\Vert f(x) - f(y)\Vert_2 \leq \Vert x-y\Vert_1 \leq k\Vert f(x) - f(y)\Vert_2 $ for all $ x,y, \in V_1 $. Suppose $(V_2, \Vert\cdot\Vert_2) $ is complete. Clearly everything is symmetrical, so we only really need to prove this in one direction. I can see that if $ (x_n) $ is a Cauchy sequence in $V_1$, then $(f(x_n))$ is Cauchy in $ V_2 $. I can also see that $ f $ is uniformly continuous. How can I turn this into a proof? Thanks",,['analysis']
69,When does $\sum_{n=2}^\infty n^\alpha (\log n)^\beta$ converge?,When does  converge?,\sum_{n=2}^\infty n^\alpha (\log n)^\beta,"I need to find out for what $\alpha, \beta$ the following sum converges: $$\sum_{n=2}^\infty n^\alpha (\log n)^\beta$$ I thought I'd do that with the help of the integral criterion, that is to say I considered (after I had substituted $x = e^u$): $$\int_2^\infty e^{u(\alpha+1)} \cdot u^\beta \mathrm du$$ Then, I realised that for $\alpha + 1 > 0$, the exponential function will always dominate the monomial, so I only considered $\alpha + 1 \leq 0$. First, I let $\alpha + 1=0$, then I would be left with: $$\int_2^\infty u^\beta \mathrm du$$ I solved the integral and substituted back and ended up with: $$\lim_{b \to \infty} \frac{(\log b)^{\beta+1}}{\beta+1}$$ I figured this would only converge for $\beta+1 \leq 0$ and thought: ""Ok, now I have some cases, where the sum converges: $\alpha = -1 \land \beta \leq -1$"". To be sure, I checked the integral with the help of wolframalpha, and it told me the integral would not converge. What did I do wrong? Is there an easier way to find $\alpha$ and $\beta$ so that the sum converges?","I need to find out for what $\alpha, \beta$ the following sum converges: $$\sum_{n=2}^\infty n^\alpha (\log n)^\beta$$ I thought I'd do that with the help of the integral criterion, that is to say I considered (after I had substituted $x = e^u$): $$\int_2^\infty e^{u(\alpha+1)} \cdot u^\beta \mathrm du$$ Then, I realised that for $\alpha + 1 > 0$, the exponential function will always dominate the monomial, so I only considered $\alpha + 1 \leq 0$. First, I let $\alpha + 1=0$, then I would be left with: $$\int_2^\infty u^\beta \mathrm du$$ I solved the integral and substituted back and ended up with: $$\lim_{b \to \infty} \frac{(\log b)^{\beta+1}}{\beta+1}$$ I figured this would only converge for $\beta+1 \leq 0$ and thought: ""Ok, now I have some cases, where the sum converges: $\alpha = -1 \land \beta \leq -1$"". To be sure, I checked the integral with the help of wolframalpha, and it told me the integral would not converge. What did I do wrong? Is there an easier way to find $\alpha$ and $\beta$ so that the sum converges?",,['analysis']
70,Stone-Weierstrass Theorem,Stone-Weierstrass Theorem,,"In the proof of Stone-Weierstrass theorem provided in Rudin's Principles of Mathematical Analysis, why do we only need to show that there exists a sequence of polynomials $P_n$ that converges uniformly to the continuous complex function on $[0,1]$?","In the proof of Stone-Weierstrass theorem provided in Rudin's Principles of Mathematical Analysis, why do we only need to show that there exists a sequence of polynomials $P_n$ that converges uniformly to the continuous complex function on $[0,1]$?",,[]
71,A naive approach for nonlinear optimization on several real variables and one natural variable. Give examples of (potential) failure,A naive approach for nonlinear optimization on several real variables and one natural variable. Give examples of (potential) failure,,"Motivation: Example . To solve a problem on evaluating the maximum of a product of $n$ real variables subject to an equality constraint on its sum $S$ ($=100$), I used the Lagrange multipliers method (which can be improved by adding the adequate conditions on the second order partial derivatives). Doing so I got a ""solution"" $x^*=(x_1,x_2,\dots ,x_n)\in \mathbb{R}^{n}$ with $x_1=x_2=\dots=x_n=100/n$. I proceeded by maximizing the single real variable function $u(t)=(100/t)^t$: the maximum is at $36\lt t^*\lt 37$. Finally I computed $u(36)\lt u(37)$. Hence, the optimum occurs at $n=37$. In spite of having come to the correct solution to the problem, I was told that such an approach does not guarantee the correct solution, in the general case . Question: In order to see some limitations of treating optimization problems as illustrated by the above example but with greater generality, a few examples in which this method fails would be appreciated. Remark : I hope this edited text improves the question. EDIT: The case I have in mind is that of finding the maximum/minimum of an objective function $f(x_1,x_2,\dots,x_n)$ subject to at least one constraint $g(x_1,x_2,\dots,x_n)=0$ This is a kind  of generalization of the linear programming simplex problem , two differences being a nonlinear optimization and one integer variable .","Motivation: Example . To solve a problem on evaluating the maximum of a product of $n$ real variables subject to an equality constraint on its sum $S$ ($=100$), I used the Lagrange multipliers method (which can be improved by adding the adequate conditions on the second order partial derivatives). Doing so I got a ""solution"" $x^*=(x_1,x_2,\dots ,x_n)\in \mathbb{R}^{n}$ with $x_1=x_2=\dots=x_n=100/n$. I proceeded by maximizing the single real variable function $u(t)=(100/t)^t$: the maximum is at $36\lt t^*\lt 37$. Finally I computed $u(36)\lt u(37)$. Hence, the optimum occurs at $n=37$. In spite of having come to the correct solution to the problem, I was told that such an approach does not guarantee the correct solution, in the general case . Question: In order to see some limitations of treating optimization problems as illustrated by the above example but with greater generality, a few examples in which this method fails would be appreciated. Remark : I hope this edited text improves the question. EDIT: The case I have in mind is that of finding the maximum/minimum of an objective function $f(x_1,x_2,\dots,x_n)$ subject to at least one constraint $g(x_1,x_2,\dots,x_n)=0$ This is a kind  of generalization of the linear programming simplex problem , two differences being a nonlinear optimization and one integer variable .",,"['calculus', 'analysis', 'optimization', 'nonlinear-optimization', 'multivariable-calculus']"
72,"Can we find $f: [0,1] \to [0, \infty) \notin L^{\frac{q}{q-1}}[0,1]$ such that $\int_A f \le |A|^{1/q}$ for some $q \ge 2$ and all $A \subset [0,1]$?",Can we find  such that  for some  and all ?,"f: [0,1] \to [0, \infty) \notin L^{\frac{q}{q-1}}[0,1] \int_A f \le |A|^{1/q} q \ge 2 A \subset [0,1]","Let $f: [0,1] \to [0, \infty)$ be a measurable function satisfying $$\int_A f \le |A|^{1/q}$$ for some $q \ge 2$ and all measurable subsets $A \subset [0,1]$ . Show that $f\in L^p[0,1]$ for all $1 < p < \frac{q}{q-1}$ . Is $f$ necessarily in $L^{\frac{q}{q-1}}[0,1]$ ? I solved the first part. Apply the given inequality to the sets $A_t := \{f > t\}$ for $t\in \Bbb R$ , to get $$t |A_t| \le \int_{A_t} f \le |A_t|^{1/q},$$ i.e., $$|A_t| \le t^{\frac{q}{1-q}}.$$ Using $$\int_0^1 f^p = p\int_0^\infty t^{p-1} |A_t| \, dt$$ we get $$\int_0^1 f^p \le p + \frac{p}{\frac{q}{q-1} - p} < \infty$$ for $1 < p < \frac{q}{q-1}$ . Could someone help me produce a measurable function $f: [0,1] \to [0, \infty) \notin L^{\frac{q}{q-1}}[0,1]$ satisfying $$\int_A f \le |A|^{1/q}$$ for some $q \ge 2$ and all measurable subsets $A \subset [0,1]$ ?","Let be a measurable function satisfying for some and all measurable subsets . Show that for all . Is necessarily in ? I solved the first part. Apply the given inequality to the sets for , to get i.e., Using we get for . Could someone help me produce a measurable function satisfying for some and all measurable subsets ?","f: [0,1] \to [0, \infty) \int_A f \le |A|^{1/q} q \ge 2 A \subset [0,1] f\in L^p[0,1] 1 < p < \frac{q}{q-1} f L^{\frac{q}{q-1}}[0,1] A_t := \{f > t\} t\in \Bbb R t |A_t| \le \int_{A_t} f \le |A_t|^{1/q}, |A_t| \le t^{\frac{q}{1-q}}. \int_0^1 f^p = p\int_0^\infty t^{p-1} |A_t| \, dt \int_0^1 f^p \le p + \frac{p}{\frac{q}{q-1} - p} < \infty 1 < p < \frac{q}{q-1} f: [0,1] \to [0, \infty) \notin L^{\frac{q}{q-1}}[0,1] \int_A f \le |A|^{1/q} q \ge 2 A \subset [0,1]","['real-analysis', 'analysis', 'measure-theory', 'lp-spaces', 'weak-lp-spaces']"
73,Stone-Weierstrass theorem for non-polynomials,Stone-Weierstrass theorem for non-polynomials,,"Let $(X,\rho)$ be a compact metric space and let $C(X)$ be the set of the continuous real-valued functions on $X$ equiped with the maximum norm (which makes $C(X)$ a Banach Space). If a closed set subset $A\subset C(X)$ is such that: $f,g\in A\Rightarrow f+g\in A$ , $f\cdot g\in A$ ; $f\in A$ , $c\in\mathbb R\Rightarrow cf\in A$ ; There is some non-zero constant function in $A$ ; If $x,y\in X$ , $x\ne y$ , then there's some function $f\in A$ whith $f(x)\ne f(y)$ . Then we can conclude that $A=C(X)$ . This is how the Stone-Weierstrass theorem is stated on the book Foundations of Modern Analysis by Avner Friedman (Theorem 3.7.1). If we remove the closed hypothesis from $A$ , then the closure of $A$ is $C(X)$ , that is, every continuous function defined on $X$ can be approximated (uniformly) by functions from $A$ . The main application of this theorem is that every function defined on a compact subset $X\subset\mathbb R^n$ can be approximated by polynomials. My question is: Can we find a set $A$ of continuous functions defined on a compact subset $X\subset\mathbb R^n$ that has no polynomial functions, with the exception maybe of the constant ones, but still every continuous function defined on $X$ can be uniformly approximated by functions from $A$ .","Let be a compact metric space and let be the set of the continuous real-valued functions on equiped with the maximum norm (which makes a Banach Space). If a closed set subset is such that: , ; , ; There is some non-zero constant function in ; If , , then there's some function whith . Then we can conclude that . This is how the Stone-Weierstrass theorem is stated on the book Foundations of Modern Analysis by Avner Friedman (Theorem 3.7.1). If we remove the closed hypothesis from , then the closure of is , that is, every continuous function defined on can be approximated (uniformly) by functions from . The main application of this theorem is that every function defined on a compact subset can be approximated by polynomials. My question is: Can we find a set of continuous functions defined on a compact subset that has no polynomial functions, with the exception maybe of the constant ones, but still every continuous function defined on can be uniformly approximated by functions from .","(X,\rho) C(X) X C(X) A\subset C(X) f,g\in A\Rightarrow f+g\in A f\cdot g\in A f\in A c\in\mathbb R\Rightarrow cf\in A A x,y\in X x\ne y f\in A f(x)\ne f(y) A=C(X) A A C(X) X A X\subset\mathbb R^n A X\subset\mathbb R^n X A","['analysis', 'continuity', 'compactness']"
74,Did I use Stirling formula incorrectly?,Did I use Stirling formula incorrectly?,,I have to prove that $\frac{\sqrt{2\pi n} * e^{-n}*n^k}{k!}  \to e^{-x^2/2}$ when $\frac{k-n}{\sqrt{n}} \to x$ and $n \to \infty$ I used Stirling formula and get that $\frac{\sqrt{2\pi n} * e^{-n}*n^k}{k!} = e^{k-n} * (n/k)^k \frac{\sqrt{2\pi n}}{\sqrt{2\pi k}}$ From $\frac{k-n}{\sqrt{n}} \to x$ we get that $n/k \to 1$ than $\frac{\sqrt{2\pi n}}{\sqrt{2\pi k}} \to 1$ So we get $e^{k-n} * (n/k)^k$ But $(n/k)^k = (1 + (n-k)/k)^{k/(n-k) * (n-k)} = e^{n-k}$ because $(n-k) / k \to 0$ So everything tends to $e^{k-n} * e^{n-k} = 1$ What do I do wrong?(,I have to prove that when and I used Stirling formula and get that From we get that than So we get But because So everything tends to What do I do wrong?(,\frac{\sqrt{2\pi n} * e^{-n}*n^k}{k!}  \to e^{-x^2/2} \frac{k-n}{\sqrt{n}} \to x n \to \infty \frac{\sqrt{2\pi n} * e^{-n}*n^k}{k!} = e^{k-n} * (n/k)^k \frac{\sqrt{2\pi n}}{\sqrt{2\pi k}} \frac{k-n}{\sqrt{n}} \to x n/k \to 1 \frac{\sqrt{2\pi n}}{\sqrt{2\pi k}} \to 1 e^{k-n} * (n/k)^k (n/k)^k = (1 + (n-k)/k)^{k/(n-k) * (n-k)} = e^{n-k} (n-k) / k \to 0 e^{k-n} * e^{n-k} = 1,['analysis']
75,How can I show that this set has measure 0?,How can I show that this set has measure 0?,,"The Problem Suppose I have the measurable set $$E = \bigcup_{n = 1}^\infty\bigcap_{k = n}^\infty E_k$$ where $E_k \subseteq \mathbb{R}^d$ is measurable for all $k$ . I want to prove that if $\sum_{k = 1}^\infty m(E_k) < \infty$ then $m(E) = 0$ . What I tried I think it can be shown that $$E = \bigcup_{n = 1}^\infty\bigcap_{k = n}^\infty E_k \subseteq \bigcap_{n = 1}^\infty\bigcup_{k = n}^\infty E_k$$ Next let $B_N = \bigcup_{k = N}^\infty E_k$ . Clearly since the sets may not be disjoint, $$m(B_N) \leq \sum_{k = N}^\infty m(E_k)$$ and since $\sum_{k = 1}^\infty m(E_k) < \infty$ then for all $\epsilon > 0$ there exists $N$ such that $$m(B_N) \leq \sum_{k = N}^\infty m(E_k) < \epsilon$$ Furthermore, it's not hard to see that $\bigcap_{n = 1}^\infty B_n \subseteq B_N$ , so $$m(E) \leq m\left(\bigcap_{n = 1}^\infty\bigcup_{k = n}^\infty E_k\right) \leq m(B_N) < \epsilon$$ Since $\epsilon > 0$ , then $m(E) = 0$ . Have I missed anything? If so, how can I correct it? Thanks in advance!","The Problem Suppose I have the measurable set where is measurable for all . I want to prove that if then . What I tried I think it can be shown that Next let . Clearly since the sets may not be disjoint, and since then for all there exists such that Furthermore, it's not hard to see that , so Since , then . Have I missed anything? If so, how can I correct it? Thanks in advance!",E = \bigcup_{n = 1}^\infty\bigcap_{k = n}^\infty E_k E_k \subseteq \mathbb{R}^d k \sum_{k = 1}^\infty m(E_k) < \infty m(E) = 0 E = \bigcup_{n = 1}^\infty\bigcap_{k = n}^\infty E_k \subseteq \bigcap_{n = 1}^\infty\bigcup_{k = n}^\infty E_k B_N = \bigcup_{k = N}^\infty E_k m(B_N) \leq \sum_{k = N}^\infty m(E_k) \sum_{k = 1}^\infty m(E_k) < \infty \epsilon > 0 N m(B_N) \leq \sum_{k = N}^\infty m(E_k) < \epsilon \bigcap_{n = 1}^\infty B_n \subseteq B_N m(E) \leq m\left(\bigcap_{n = 1}^\infty\bigcup_{k = n}^\infty E_k\right) \leq m(B_N) < \epsilon \epsilon > 0 m(E) = 0,"['analysis', 'measure-theory', 'lebesgue-measure']"
76,Proving $\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}f(x)\sin{x}dx \geq \pi - 2$ given a twice differentiable function and an initial inequality condition,Proving  given a twice differentiable function and an initial inequality condition,\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}f(x)\sin{x}dx \geq \pi - 2,"Let $f : [-\frac{\pi}{2}, \frac{\pi}{2}] \rightarrow \mathbb{R}$ be a twice differentiable function such that $(f^{''}(x) - f(x))\tan{x} + 2f^{'}(x) \geq 1, \forall x \in (-\frac{\pi}{2}, \frac{\pi}{2})$ . Prove that $\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}f(x)\sin{x}dx \geq \pi - 2$ . The given inequality does not seem very generous, so what I tried to do, seeing that we need to obtain $\sin{x}$ and we have a $\tan{x}$ inside the inequality, was to process it by rearranging terms and eventually obtaining $f(x)\sin{x}$ on one side. Therefore: $(f^{''}(x) - f(x))\tan{x} \geq 1 - 2f^{'}(x) \Leftrightarrow (f^{''}(x) - f(x))\frac{\sin{x}}{\cos{x}} \geq 1 - 2f^{'}(x)$ At this stage, since we only really care about $I = (-\frac{\pi}{2}, \frac{\pi}{2}) \subset \mathbb{R}$ , we know that $\cos{x} > 0, \forall x \in I$ , but $\sin{x}$ takes both negative, positive and a null value, therefore we cannot multiply by it. We can, however, multiply by $cos{x}$ , unaffecting our inequality. We obtain: $(f^{''}(x) - f(x))\sin{x} \geq (1 - 2f^{'}(x))\cos{x} \Leftrightarrow f^{''}(x)\sin{x} - f(x)\sin{x} \geq \cos{x} - 2f^{'}(x)\cos{x}$ By rearranging and isolating the $f(x)\sin{x}$ terms, we finally obtain that: $f(x)\sin{x} \leq 2f^{'}(x)\cos{x} + f^{''}(x)\sin{x} - \cos{x}, \forall x \in I$ The problem with this inequality is that we have a $\leq$ , but we need a $\geq$ . Knowing this is true for all $x \in I$ , it must also remain true for every $-x \in I$ , therefore we map $x \rightarrow -x$ in our inequality, to obtain: $-f(-x)\sin{x} \leq 2f^{'}(-x)\cos{x} - f^{''}(-x)\sin{x} - \cos{x}$ Denote $J = \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}f(x)\sin{x}dx$ . It is known that $\int_{a}^{b}f(x)dx = \int_{a}^{b}f(a+b-x)dx$ . Therefore, substituting $u = -x$ in J, we obtain that $J = \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}f(-x)\sin{(-x)}dx = -\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}f(-x)\sin{x}dx$ , since sine is an odd function. However, this is where I am lost; I do not exactly know how to turn the sign from $\leq$ to $\geq$ , even with my substitution idea. Another thing I noticed is that the $f^{'}(x)\cos{x} + f^{''}(x)\sin{x}$ term can be written as $(f^{'}(x)\sin{x})^{'}$ , but the constant term $2$ is particularly annoying. Any help, hints, ideas, or solutions would be of tremendous use to me. Thank you very much for your time! :)","Let be a twice differentiable function such that . Prove that . The given inequality does not seem very generous, so what I tried to do, seeing that we need to obtain and we have a inside the inequality, was to process it by rearranging terms and eventually obtaining on one side. Therefore: At this stage, since we only really care about , we know that , but takes both negative, positive and a null value, therefore we cannot multiply by it. We can, however, multiply by , unaffecting our inequality. We obtain: By rearranging and isolating the terms, we finally obtain that: The problem with this inequality is that we have a , but we need a . Knowing this is true for all , it must also remain true for every , therefore we map in our inequality, to obtain: Denote . It is known that . Therefore, substituting in J, we obtain that , since sine is an odd function. However, this is where I am lost; I do not exactly know how to turn the sign from to , even with my substitution idea. Another thing I noticed is that the term can be written as , but the constant term is particularly annoying. Any help, hints, ideas, or solutions would be of tremendous use to me. Thank you very much for your time! :)","f : [-\frac{\pi}{2}, \frac{\pi}{2}] \rightarrow \mathbb{R} (f^{''}(x) - f(x))\tan{x} + 2f^{'}(x) \geq 1, \forall x \in (-\frac{\pi}{2}, \frac{\pi}{2}) \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}f(x)\sin{x}dx \geq \pi - 2 \sin{x} \tan{x} f(x)\sin{x} (f^{''}(x) - f(x))\tan{x} \geq 1 - 2f^{'}(x) \Leftrightarrow (f^{''}(x) - f(x))\frac{\sin{x}}{\cos{x}} \geq 1 - 2f^{'}(x) I = (-\frac{\pi}{2}, \frac{\pi}{2}) \subset \mathbb{R} \cos{x} > 0, \forall x \in I \sin{x} cos{x} (f^{''}(x) - f(x))\sin{x} \geq (1 - 2f^{'}(x))\cos{x} \Leftrightarrow f^{''}(x)\sin{x} - f(x)\sin{x} \geq \cos{x} - 2f^{'}(x)\cos{x} f(x)\sin{x} f(x)\sin{x} \leq 2f^{'}(x)\cos{x} + f^{''}(x)\sin{x} - \cos{x}, \forall x \in I \leq \geq x \in I -x \in I x \rightarrow -x -f(-x)\sin{x} \leq 2f^{'}(-x)\cos{x} - f^{''}(-x)\sin{x} - \cos{x} J = \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}f(x)\sin{x}dx \int_{a}^{b}f(x)dx = \int_{a}^{b}f(a+b-x)dx u = -x J = \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}f(-x)\sin{(-x)}dx = -\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}f(-x)\sin{x}dx \leq \geq f^{'}(x)\cos{x} + f^{''}(x)\sin{x} (f^{'}(x)\sin{x})^{'} 2","['real-analysis', 'calculus', 'integration', 'analysis', 'derivatives']"
77,What is meant by linearity in more than one dimension?,What is meant by linearity in more than one dimension?,,"I am slightly unclear on some terminology that I have come across in the following question: Is $\partial _vf(0,0)$ linear in $v$ ? My understanding of linearity for a single variable function is fine, although I'm a little unclear on what it means for a function that takes multiple variables? What is the generalisation here, and how should I interpret this word in these types of problems? I would be grateful for any clarification here regarding the meaning of linearity in this context.","I am slightly unclear on some terminology that I have come across in the following question: Is linear in ? My understanding of linearity for a single variable function is fine, although I'm a little unclear on what it means for a function that takes multiple variables? What is the generalisation here, and how should I interpret this word in these types of problems? I would be grateful for any clarification here regarding the meaning of linearity in this context.","\partial _vf(0,0) v","['real-analysis', 'calculus', 'analysis', 'multivariable-calculus', 'terminology']"
78,"$ f\in L^1(\mathbb{R}) $, $ f $ is continuous at $ 0 $, $ \widehat{f}\geq 0 $, show that $ \widehat{f}\in L^1(\mathbb{R}) $.",",  is continuous at , , show that .", f\in L^1(\mathbb{R})   f   0   \widehat{f}\geq 0   \widehat{f}\in L^1(\mathbb{R}) ,"Assume that $ f\in L^1(\mathbb{R}) $ and $ f $ is continuous at the point $ 0 $ . If $ \widehat{f}(\xi)=\int_{\mathbb{R}}f(x)e^{-2\pi ix\xi}dx\geq 0 $ for any $ \xi\in\mathbb{R} $ , show that $ \widehat{f}\in L^1(\mathbb{R}) $ . It can be get that $$ \int_{\mathbb{R}}|\widehat{f}(\xi)|d\xi=\int_{\mathbb{R}}\int_{\mathbb{R}}f(x)e^{-2\pi ix\xi}dxd\xi. $$ I do not how to use the condition that $ f $ is continuous at $ 0 $ . Can you give me some references or hints?","Assume that and is continuous at the point . If for any , show that . It can be get that I do not how to use the condition that is continuous at . Can you give me some references or hints?"," f\in L^1(\mathbb{R})   f   0   \widehat{f}(\xi)=\int_{\mathbb{R}}f(x)e^{-2\pi ix\xi}dx\geq 0   \xi\in\mathbb{R}   \widehat{f}\in L^1(\mathbb{R})  
\int_{\mathbb{R}}|\widehat{f}(\xi)|d\xi=\int_{\mathbb{R}}\int_{\mathbb{R}}f(x)e^{-2\pi ix\xi}dxd\xi.
  f   0 ","['real-analysis', 'analysis', 'fourier-analysis', 'harmonic-analysis']"
79,Intuition behind Legendre convex function,Intuition behind Legendre convex function,,"I came across the definition of Legendre functions and Legendre transformations in my studies (in the sense of convex analysis) and I started searching about it. I found a definition in Rockefellar's 1996 ""Convex analysis"" book. So let $\Psi$ be a proper and closed convex function, meaning $\text{dom}(\Psi) \neq \emptyset$ and $\forall \boldsymbol{x} \in\text{dom}(\Psi), \Psi(\boldsymbol{x}) >-\infty$ and $\Psi$ is semi-continuous. Let $\Theta=\text{int}(\text{dom}(\Psi))$ , where $\text{int}(\text{dom}(f))$ denotes the interior domain of $f$ . Then $(\Theta, \Psi)$ is said to be a Legendre-type convex function or simply Legendre if and only if: $\Theta \neq \emptyset$ . $\Psi$ is strictly convex and diferentiable in $\Theta$ . $\forall \theta_b \in bd(\Theta), \lim_{\theta\to\theta_b} || \nabla \Psi(\theta)|| = \infty $ , where $\boldsymbol{\theta} \in \Theta$ and $bd$ denotes the boundary. While the first two conditions are clear to me, the meaning of $\lim_{\theta\to\theta_b} || \nabla \Psi(\boldsymbol{\Theta})|| \to \infty $ is a bit obscure because I'm having trouble visualizing this to grasp the intuition behind it. Why is this precisely required or useful?","I came across the definition of Legendre functions and Legendre transformations in my studies (in the sense of convex analysis) and I started searching about it. I found a definition in Rockefellar's 1996 ""Convex analysis"" book. So let be a proper and closed convex function, meaning and and is semi-continuous. Let , where denotes the interior domain of . Then is said to be a Legendre-type convex function or simply Legendre if and only if: . is strictly convex and diferentiable in . , where and denotes the boundary. While the first two conditions are clear to me, the meaning of is a bit obscure because I'm having trouble visualizing this to grasp the intuition behind it. Why is this precisely required or useful?","\Psi \text{dom}(\Psi) \neq \emptyset \forall \boldsymbol{x} \in\text{dom}(\Psi), \Psi(\boldsymbol{x}) >-\infty \Psi \Theta=\text{int}(\text{dom}(\Psi)) \text{int}(\text{dom}(f)) f (\Theta, \Psi) \Theta \neq \emptyset \Psi \Theta \forall \theta_b \in bd(\Theta), \lim_{\theta\to\theta_b} || \nabla \Psi(\theta)|| = \infty  \boldsymbol{\theta} \in \Theta bd \lim_{\theta\to\theta_b} || \nabla \Psi(\boldsymbol{\Theta})|| \to \infty ","['analysis', 'functions', 'convex-analysis', 'legendre-functions', 'legendre-transformation']"
80,"Prove that for if for every $n \in \mathbb N$ and $x\in \mathbb R$ and $f'(x) = \frac{f(x+m)-f(x)}{m}$, then $f$ is linear affine.","Prove that for if for every  and  and , then  is linear affine.",n \in \mathbb N x\in \mathbb R f'(x) = \frac{f(x+m)-f(x)}{m} f,""" Prove that for $f:\mathbb R \to \mathbb R$ differenciable. If for every $m \in \mathbb N$ and $x\in \mathbb R$ , and $f'(x) = \frac{f(x+m)-f(x)}{m}$ . Then $f$ is linear affine."" This question appeared in one of my exams. I wasn't able to solve it back then. Now, I'm trying to solve it again, but I have been able to prove only for every $x \in \mathbb Z$ . Here is what I've done so far. For $n \in \mathbb N$ , $$ f'(0) = \frac{f(n) - f(0)}{n} \implies f'(0) n + f(0) = f(n). $$ Hence, $\forall x \in \mathbb N, f(x) = f'(0) \cdot x + f(0)$ is linear affine. Now, for a negative integer, we have $$ f'(-n) = \frac{f(0) - f(-n)}{n}\implies f(-n) = -f'(-n)n + f(0) $$ $$ f'(-n) = \frac{f(n)-f(-n)}{2n}\implies f(-n) = f(n) -2n f'(-n) = f'(0)n+f(0)-2nf'(-n) $$ Therefore, $f'(0) n = f'(-n)n \implies f'(0) = f'(-n)$ , which give us that $f$ is linear affine for every integer. This is how far I went.",""" Prove that for differenciable. If for every and , and . Then is linear affine."" This question appeared in one of my exams. I wasn't able to solve it back then. Now, I'm trying to solve it again, but I have been able to prove only for every . Here is what I've done so far. For , Hence, is linear affine. Now, for a negative integer, we have Therefore, , which give us that is linear affine for every integer. This is how far I went.","f:\mathbb R \to \mathbb R m \in \mathbb N x\in \mathbb R f'(x) = \frac{f(x+m)-f(x)}{m} f x \in \mathbb Z n \in \mathbb N 
f'(0) = \frac{f(n) - f(0)}{n} \implies f'(0) n + f(0) = f(n).
 \forall x \in \mathbb N, f(x) = f'(0) \cdot x + f(0) 
f'(-n) = \frac{f(0) - f(-n)}{n}\implies f(-n) = -f'(-n)n + f(0)
 
f'(-n) = \frac{f(n)-f(-n)}{2n}\implies f(-n) = f(n) -2n f'(-n) = f'(0)n+f(0)-2nf'(-n)
 f'(0) n = f'(-n)n \implies f'(0) = f'(-n) f","['real-analysis', 'analysis']"
81,Proof that if $X>Y$ then there exists some $\epsilon>0$ such that $x-\epsilon>y+\epsilon$,Proof that if  then there exists some  such that,X>Y \epsilon>0 x-\epsilon>y+\epsilon,"I am trying to prove that if $x>y$ , then there exists some $\epsilon>0$ such that $x-\epsilon>y+\epsilon$ . So far I have: Suppose, for a contradiction, that no such $\epsilon$ exists. That is, $\forall \epsilon>0$ , suppose that $x-\epsilon \leq y+\epsilon$ . Beyond this, I am not at all sure how to proceed. Must one make reference to the fact that limits preserve weak inequalities? Thank you.","I am trying to prove that if , then there exists some such that . So far I have: Suppose, for a contradiction, that no such exists. That is, , suppose that . Beyond this, I am not at all sure how to proceed. Must one make reference to the fact that limits preserve weak inequalities? Thank you.",x>y \epsilon>0 x-\epsilon>y+\epsilon \epsilon \forall \epsilon>0 x-\epsilon \leq y+\epsilon,"['real-analysis', 'analysis']"
82,Big O for error terms,Big O for error terms,,"The following link from wikipedia explains the Big O notation really good. I have only one problem, which is to formalize the usage of Big O notation for error terms in polynomials. In the  example give here  we have $$ e^x=1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+\frac{x^4}{4!}+...=1+x+\frac{x^2}{2!}+\mathcal{O}(x^3)=1+x+\mathcal{O}(x^2) $$ as $x\rightarrow 0$ . Now we find a similar notation also for the error terms in taylor polynomials. I would like to understand why this is right just formaly. 1.) Why is $\frac{x^3}{3!}+\frac{x^4}{4!}+... = \mathcal{O}(x^3)$ Is this because $$ \frac{x^3}{3!}\frac{1}{x^3}+\frac{x^4}{4!}\frac{1}{x^3}+\frac{x^5}{5!}\frac{1}{x^3}+...=\frac{1}{3!}+\frac{x}{4!}+\frac{x^2}{5!}+...\leq M, \quad x\rightarrow 0 $$ for some $M$ that has to be bigger than $\frac{1}{3!}$ or can one show this in a different way formaly? 2.) Why is $\frac{x^2}{2!}+\mathcal{O}(x^3) = \mathcal{O}(x^2)$ ? I appreciate your help! :)","The following link from wikipedia explains the Big O notation really good. I have only one problem, which is to formalize the usage of Big O notation for error terms in polynomials. In the  example give here  we have as . Now we find a similar notation also for the error terms in taylor polynomials. I would like to understand why this is right just formaly. 1.) Why is Is this because for some that has to be bigger than or can one show this in a different way formaly? 2.) Why is ? I appreciate your help! :)","
e^x=1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+\frac{x^4}{4!}+...=1+x+\frac{x^2}{2!}+\mathcal{O}(x^3)=1+x+\mathcal{O}(x^2)
 x\rightarrow 0 \frac{x^3}{3!}+\frac{x^4}{4!}+... = \mathcal{O}(x^3) 
\frac{x^3}{3!}\frac{1}{x^3}+\frac{x^4}{4!}\frac{1}{x^3}+\frac{x^5}{5!}\frac{1}{x^3}+...=\frac{1}{3!}+\frac{x}{4!}+\frac{x^2}{5!}+...\leq M, \quad x\rightarrow 0
 M \frac{1}{3!} \frac{x^2}{2!}+\mathcal{O}(x^3) = \mathcal{O}(x^2)","['real-analysis', 'calculus', 'analysis', 'asymptotics']"
83,A σ-algebra may have no non-empty atoms at all?,A σ-algebra may have no non-empty atoms at all?,,How to prove the fact that a $\sigma-$ algebra may have no non-empty atoms at all? Definition: An atom of a $\sigma$ -algebra $\mathscr{A}$ is a non-void set $\emptyset \neq A \in \mathscr{A}$ that contains no other set of $\mathscr{A}$ .,How to prove the fact that a algebra may have no non-empty atoms at all? Definition: An atom of a -algebra is a non-void set that contains no other set of .,\sigma- \sigma \mathscr{A} \emptyset \neq A \in \mathscr{A} \mathscr{A},"['real-analysis', 'analysis', 'measure-theory']"
84,Does anyone know some theorem to solve this integral?,Does anyone know some theorem to solve this integral?,,"Recently I have been studying calculus and I got stuck with this problem: I already know that the functions $f$ and $g$ are both real functions, defined and continuous in $[a,b]$ , such that \begin{equation} \int_{a}^{b}f(t)dt=2\int_{a}^{b}g(t)dt. \end{equation} I need to show that exists $c\in[a,b]$ , such that $f(c)=2g(c)$ . To solve that, I've started doing: \begin{equation} \int_{a}^{b}f(t)dt=F(b)-F(a)=2[G(b)-G(a)]=2\int_{a}^{b}g(t)dt, \end{equation} considering $F(t)$ and $G(t)$ antiderivatives for $f(t)$ and $g(t)$ . Multiplying all terms by $\frac{1}{(b-a)}$ , we have: \begin{equation} \frac{F(b)-F(a)}{b-a}=2\frac{[G(b)-G(a)]}{b-a}. \end{equation} I don't know the theorem, but I surely know that exists $c\in[a,b]$ such that \begin{equation} F'(c)=\frac{F(b)-F(a)}{b-a}\Leftrightarrow f(c)=\frac{F(b)-F(a)}{b-a}=2\frac{[G(b)-G(a)]}{b-a}, \end{equation} whitch gives me \begin{equation} f(c)=2\frac{[G(b)-G(a)]}{b-a}. \end{equation} I'm almost there, but I feel that I'm missing something. Please, help me.","Recently I have been studying calculus and I got stuck with this problem: I already know that the functions and are both real functions, defined and continuous in , such that I need to show that exists , such that . To solve that, I've started doing: considering and antiderivatives for and . Multiplying all terms by , we have: I don't know the theorem, but I surely know that exists such that whitch gives me I'm almost there, but I feel that I'm missing something. Please, help me.","f g [a,b] \begin{equation}
\int_{a}^{b}f(t)dt=2\int_{a}^{b}g(t)dt.
\end{equation} c\in[a,b] f(c)=2g(c) \begin{equation}
\int_{a}^{b}f(t)dt=F(b)-F(a)=2[G(b)-G(a)]=2\int_{a}^{b}g(t)dt,
\end{equation} F(t) G(t) f(t) g(t) \frac{1}{(b-a)} \begin{equation}
\frac{F(b)-F(a)}{b-a}=2\frac{[G(b)-G(a)]}{b-a}.
\end{equation} c\in[a,b] \begin{equation}
F'(c)=\frac{F(b)-F(a)}{b-a}\Leftrightarrow f(c)=\frac{F(b)-F(a)}{b-a}=2\frac{[G(b)-G(a)]}{b-a},
\end{equation} \begin{equation}
f(c)=2\frac{[G(b)-G(a)]}{b-a}.
\end{equation}","['calculus', 'integration', 'analysis']"
85,General knowledge regarding double convergence of sequences (and series).,General knowledge regarding double convergence of sequences (and series).,,"While proving a subspace of $\textit{L}^p(\mathbb{R}^n)$ is dense I stumbled upon a family of functions that was double indexed; this doesnt fall in our usual definition of sequence, it is merely a set of functions $\{f_{i,j}\}_{i,j\in \mathbb{N}}$ . The study of a collection of this type wasn't necessary for my objective, but i found a few papers online regarding double convergence and how it is defined. As a first thing i would like to know if anybody knows about this part of analysis, becouse although it sound interesting, it's probably not all that popular. To give some context to people who don't know what this is, ill give a first definition of convergence: Given a measure space $(X,\textit{A},\mu)$ and given $E\in \textit{A}$ with $|E|<+\infty$ ( $\star$ is this limitation necessary? $\star$ ), then we say that $\{f_{n,m}\}$ converges in the Pringsheim’s sense if for every $\epsilon > 0$ there exists $N∈\mathbb{N}$ such that $|f_{m,n}(x_0) −f|< \epsilon$ whenever $j$ , $k ≥N$ . And all the other kind of convergences can be defined as well (a.e., in measure,...) I now ask, what properties differenciate them from the normal sequences and convergence? By just thinking of examples i am pretty sure a major difference is that if a double sequence converges, it still can have subsequences converging elswhere, for example: $$f_{n,m}(x)=\frac{1}{n}+\frac{1}{m} \Longrightarrow \lim_{m\to +\infty}f_{n,m}=\frac{1}{n} \mbox{ and } \lim_{n\to +\infty}f_{n,m}=\frac{1}{m}$$ so when we fix one of the two between $n$ and $m$ we obtain a normal converging sequence: $$\forall \epsilon > 0\mbox{, } \exists N_1\in \mathbb{N} \mbox{ such that } \forall n\ge N_1 \Rightarrow |f_{n,m}-\frac{1}{m}|\le \frac{\epsilon}{2}$$ $$\forall \epsilon > 0\mbox{, } \exists N_2\in \mathbb{N} \mbox{ such that } \forall m\ge N_2 \Rightarrow \frac{1}{m}\le \frac{\epsilon}{2}$$ so that $$\forall \epsilon > 0\mbox{, } \exists N:=\max\{N_1,N_2\}\in \mathbb{N} \mbox{ such that } \forall n,m\ge N \Rightarrow |f_{n,m}-0|=|f_{n,m}-\frac{1}{m}+\frac{1}{m}-0| \le$$ $$\le|f_{n,m}-\frac{1}{m}|+\frac{1}{m}<\epsilon$$ We have obtained the ""double convergence of the double sequence"", but if i get the subsequence $\{f_{1,m}\}_{m\in \mathbb{N}}$ , this converges in the normal definition to $1$ ; actually you could find infinite sunsequences, $\forall n\in \mathbb{N}$ , $f_{n,m}\longrightarrow \frac{1}{n}$ , for $m\rightarrow +\infty$ . As you might have seen, i am just looking for insight on what to think of this topic, maybe also knowing what interesting results one can reach. Also I'm sorry if my english isn't that good.","While proving a subspace of is dense I stumbled upon a family of functions that was double indexed; this doesnt fall in our usual definition of sequence, it is merely a set of functions . The study of a collection of this type wasn't necessary for my objective, but i found a few papers online regarding double convergence and how it is defined. As a first thing i would like to know if anybody knows about this part of analysis, becouse although it sound interesting, it's probably not all that popular. To give some context to people who don't know what this is, ill give a first definition of convergence: Given a measure space and given with ( is this limitation necessary? ), then we say that converges in the Pringsheim’s sense if for every there exists such that whenever , . And all the other kind of convergences can be defined as well (a.e., in measure,...) I now ask, what properties differenciate them from the normal sequences and convergence? By just thinking of examples i am pretty sure a major difference is that if a double sequence converges, it still can have subsequences converging elswhere, for example: so when we fix one of the two between and we obtain a normal converging sequence: so that We have obtained the ""double convergence of the double sequence"", but if i get the subsequence , this converges in the normal definition to ; actually you could find infinite sunsequences, , , for . As you might have seen, i am just looking for insight on what to think of this topic, maybe also knowing what interesting results one can reach. Also I'm sorry if my english isn't that good.","\textit{L}^p(\mathbb{R}^n) \{f_{i,j}\}_{i,j\in \mathbb{N}} (X,\textit{A},\mu) E\in \textit{A} |E|<+\infty \star \star \{f_{n,m}\} \epsilon > 0 N∈\mathbb{N} |f_{m,n}(x_0) −f|< \epsilon j k ≥N f_{n,m}(x)=\frac{1}{n}+\frac{1}{m} \Longrightarrow \lim_{m\to +\infty}f_{n,m}=\frac{1}{n} \mbox{ and } \lim_{n\to +\infty}f_{n,m}=\frac{1}{m} n m \forall \epsilon > 0\mbox{, } \exists N_1\in \mathbb{N} \mbox{ such that } \forall n\ge N_1 \Rightarrow |f_{n,m}-\frac{1}{m}|\le \frac{\epsilon}{2} \forall \epsilon > 0\mbox{, } \exists N_2\in \mathbb{N} \mbox{ such that } \forall m\ge N_2 \Rightarrow \frac{1}{m}\le \frac{\epsilon}{2} \forall \epsilon > 0\mbox{, } \exists N:=\max\{N_1,N_2\}\in \mathbb{N} \mbox{ such that } \forall n,m\ge N \Rightarrow |f_{n,m}-0|=|f_{n,m}-\frac{1}{m}+\frac{1}{m}-0| \le \le|f_{n,m}-\frac{1}{m}|+\frac{1}{m}<\epsilon \{f_{1,m}\}_{m\in \mathbb{N}} 1 \forall n\in \mathbb{N} f_{n,m}\longrightarrow \frac{1}{n} m\rightarrow +\infty","['real-analysis', 'analysis', 'convergence-divergence', 'double-sequence']"
86,Mathematical problems which prompts the creation of new theories,Mathematical problems which prompts the creation of new theories,,"We spend a lot of time learning different theories (for instance, theory of differential forms, sobolev spaces, homology groups, distributions). Although (at least most parts of) these theories are very natural and understandable when we read them from books, they are very difficult to create at the first place: it could take tens of years of effort of a large number of excellent mathematicians. After learning those theories, we do exercises or solve problems, but most of the time, we are just using the tools stated in the book. Even the chance that we come up with a ""new"" definition ourselves is rare. (By ""new"", I mean ""have not learnt"", even if someone else have created it before.) So here is my question: What are some problems which prompts the creation of a new theory? EDIT: Just to clarify, I am looking for some problems which give everybody a chance to experience the process of creating new mathematics; so the problem need not be as difficult as Riemann conjecture. By ""new theory"", I just mean something that help us formulate the problem in a different way. For example, this video on a chess board puzzle has the idea of creating new theories, because unlike other less interesting puzzles about chess board which can be solved by just carefully counting the squares, this video mentions a new way of looking at the problem, namely the vertices of a hypercube. I have also seen other similar puzzles like this. Apparently, almost all of them are on discrete mathematics, so it would be really interesting if anyone could provide such a ""theory creating"" problem in other areas of mathematics (e.g. analysis). Of course, not all theories are created to tackle specific problems, so other ways of experiencing inventing new maths could also be suggested.","We spend a lot of time learning different theories (for instance, theory of differential forms, sobolev spaces, homology groups, distributions). Although (at least most parts of) these theories are very natural and understandable when we read them from books, they are very difficult to create at the first place: it could take tens of years of effort of a large number of excellent mathematicians. After learning those theories, we do exercises or solve problems, but most of the time, we are just using the tools stated in the book. Even the chance that we come up with a ""new"" definition ourselves is rare. (By ""new"", I mean ""have not learnt"", even if someone else have created it before.) So here is my question: What are some problems which prompts the creation of a new theory? EDIT: Just to clarify, I am looking for some problems which give everybody a chance to experience the process of creating new mathematics; so the problem need not be as difficult as Riemann conjecture. By ""new theory"", I just mean something that help us formulate the problem in a different way. For example, this video on a chess board puzzle has the idea of creating new theories, because unlike other less interesting puzzles about chess board which can be solved by just carefully counting the squares, this video mentions a new way of looking at the problem, namely the vertices of a hypercube. I have also seen other similar puzzles like this. Apparently, almost all of them are on discrete mathematics, so it would be really interesting if anyone could provide such a ""theory creating"" problem in other areas of mathematics (e.g. analysis). Of course, not all theories are created to tackle specific problems, so other ways of experiencing inventing new maths could also be suggested.",,"['analysis', 'discrete-mathematics', 'soft-question', 'problem-solving']"
87,"Integrability of $\sin \frac1x$ on $[0,1]$ using Darboux sums",Integrability of  on  using Darboux sums,"\sin \frac1x [0,1]","Prove that $f:[0,1] \rightarrow\mathbb{R}:f(x)= \left \{\begin {array}{ll} \sin \frac1x &, \textrm{if}~      x\in(0,1]\\ 0 &, \textrm{if}~~x =0 \end{array} \right.~~$ is Riemann integrable using Darboux sums. Attempt. The proof that I am aware of uses the classic result that extends integrability from every $[a,1]$ to $[0,1]$ (if $f$ is bounded on $[0,1]$ and integrable on $[a,1]$ for all $a\in (0,1)$ , then $f$ is integrable on $[0,1]$ ). I am interested in a proof that uses Darboux sums, especialy: $$U(f,P_n)-L(f,P_n) \to 0$$ for a specific sequence $(P_n)$ of partitions of $[0,1]$ . So far I haven't found one that does the job. Note : the fact that such a partition exists neither means that we are looking for a good looking one, nor that it could be written in a closed form. Thanks in advance.","Prove that is Riemann integrable using Darboux sums. Attempt. The proof that I am aware of uses the classic result that extends integrability from every to (if is bounded on and integrable on for all , then is integrable on ). I am interested in a proof that uses Darboux sums, especialy: for a specific sequence of partitions of . So far I haven't found one that does the job. Note : the fact that such a partition exists neither means that we are looking for a good looking one, nor that it could be written in a closed form. Thanks in advance.","f:[0,1] \rightarrow\mathbb{R}:f(x)= \left \{\begin {array}{ll}
\sin \frac1x &, \textrm{if}~      x\in(0,1]\\
0 &, \textrm{if}~~x =0
\end{array}
\right.~~ [a,1] [0,1] f [0,1] [a,1] a\in (0,1) f [0,1] U(f,P_n)-L(f,P_n) \to 0 (P_n) [0,1]","['real-analysis', 'integration', 'analysis', 'riemann-sum']"
88,"About $e^{i z} = \cos z + i \sin z$ in Michael Spivak ""Calculus 3rd Edition"".","About  in Michael Spivak ""Calculus 3rd Edition"".",e^{i z} = \cos z + i \sin z,"I am reading ""Calculus 3rd Edition"" by Michael Spivak. The author wrote as follows ( p. 555 ): Moreover, if we replace $z$ by $i z$ in the series for $e^z$ , and make a rearrangement of the terms (justified by absolute convergence), something particularly interesting happens: $$e^{i z} = 1 + i z + \frac{(iz)^2}{2!} + \frac{(iz)^3}{3!} + \frac{(iz)^4}{4!} + \frac{(iz)^5}{5!} + \cdots \\ =1 + iz - \frac{z^2}{2!} - \frac{i z^3}{3!} + \frac{z^4}{4!} + \frac{i z^5}{5!} + \cdots \\ = (1 - \frac{z^2}{2!} + \frac{z^4}{4!} - \cdots) + i (z - \frac{z^3}{3!} + \frac{z^5}{5!} + \cdots),$$ so $$e^{i z} = \cos z + i \sin z.$$ But I think the author didn't use a rearrangement of the terms at all. Am I right? Let $$c_n := 1 - \frac{z^2}{2!} + \frac{z^4}{4!} - \cdots + (-1)^n \frac{z^{2 n}}{(2 n)!},$$ $$s_n := z - \frac{z^3}{3!} + \frac{z^5}{5!} - \cdots + (-1)^n \frac{z^{2 n + 1}}{(2 n + 1)!},$$ $$e_n := 1 + i z + \frac{(iz)^2}{2!} + \frac{(iz)^3}{3!} + \frac{(iz)^4}{4!} + \frac{(iz)^5}{5!} + \cdots + \frac{(iz)^n}{n!}.$$ Then, $$e_{2 n + 1} = c_n + i s_n,$$ $$\lim_{n \to \infty} e_{2 n + 1} = e^{i z},$$ $$\lim_{n \to \infty} c_n + i s_n = \lim_{n \to \infty} c_n + i \lim_{n \to \infty} s_n = \cos z + i \sin z,$$ so $$e^{i z} = \cos z + i \sin z.$$","I am reading ""Calculus 3rd Edition"" by Michael Spivak. The author wrote as follows ( p. 555 ): Moreover, if we replace by in the series for , and make a rearrangement of the terms (justified by absolute convergence), something particularly interesting happens: so But I think the author didn't use a rearrangement of the terms at all. Am I right? Let Then, so","z i z e^z e^{i z} = 1 + i z + \frac{(iz)^2}{2!} + \frac{(iz)^3}{3!} + \frac{(iz)^4}{4!} + \frac{(iz)^5}{5!} + \cdots \\
=1 + iz - \frac{z^2}{2!} - \frac{i z^3}{3!} + \frac{z^4}{4!} + \frac{i z^5}{5!} + \cdots \\
= (1 - \frac{z^2}{2!} + \frac{z^4}{4!} - \cdots) + i (z - \frac{z^3}{3!} + \frac{z^5}{5!} + \cdots), e^{i z} = \cos z + i \sin z. c_n := 1 - \frac{z^2}{2!} + \frac{z^4}{4!} - \cdots + (-1)^n \frac{z^{2 n}}{(2 n)!}, s_n := z - \frac{z^3}{3!} + \frac{z^5}{5!} - \cdots + (-1)^n \frac{z^{2 n + 1}}{(2 n + 1)!}, e_n := 1 + i z + \frac{(iz)^2}{2!} + \frac{(iz)^3}{3!} + \frac{(iz)^4}{4!} + \frac{(iz)^5}{5!} + \cdots + \frac{(iz)^n}{n!}. e_{2 n + 1} = c_n + i s_n, \lim_{n \to \infty} e_{2 n + 1} = e^{i z}, \lim_{n \to \infty} c_n + i s_n = \lim_{n \to \infty} c_n + i \lim_{n \to \infty} s_n = \cos z + i \sin z, e^{i z} = \cos z + i \sin z.","['calculus', 'analysis', 'power-series', 'exponential-function']"
89,Are Sequences Continuous?,Are Sequences Continuous?,,"This might be a dumb question, but if so it will at least be easy to answer. Let $X$ and $Y$ be metric spaces. Let $A \subseteq X$ . Let $a \in A$ . Let $f : A \to Y$ be a map. Then, define the phrase "" $f$ is continuous at $a$ '' to mean the following: $$ \forall \varepsilon > 0, \hspace{1mm} \exists\hspace{1mm} \delta > 0 \ni \forall x\in A, \hspace{1mm}      d_X(x,a) < \delta \implies d_Y (f(x), f(a)) < \varepsilon $$ where $d_X$ and $d_Y$ denote the respective metrics of $X$ and $Y$ . Edit: We will apply this to a sequence $(a_n)$ in place of $f$ , $\mathbb{Z}^+$ in place of $A$ , and $n$ in place of $a$ (sorry for the confusion, the intent of the question refers to continuity of $(a_n)$ at $n$ ). Consider any real valued sequence $(a_n)$ , which is to say a map from $\mathbb{Z}^+$ to $\mathbb{R}$ . Let $n$ be any positive integer. Let $a_n$ denote the value of the map $(a_n)$ at $n$ . Let $\varepsilon > 0$ . Choose $\delta = \frac{1}{2}.$ Then (using conventional metric functions, etc.), the set of $x\in\mathbb{Z}^+$ satisfying $d(x,n) < \delta$ is the singleton $\{n\}$ . Moreover, we see that $x \in\{n\} \implies d(a_x, a_n) = 0 < \varepsilon$ . Now, forget we fixed $\varepsilon$ and $n$ , and we see that $(a_n)$ is continuous on $\mathbb{Z}^+.$ Is this wrong? Is it correct to say that $(a_n)$ is continuous at $n$ ? Must we preface this by saying that $(a_n)$ is continuous at $n$ as a function of $\mathbb{Z}^+,$ but not as a function of $\mathbb{R}$ ? Thanks/","This might be a dumb question, but if so it will at least be easy to answer. Let and be metric spaces. Let . Let . Let be a map. Then, define the phrase "" is continuous at '' to mean the following: where and denote the respective metrics of and . Edit: We will apply this to a sequence in place of , in place of , and in place of (sorry for the confusion, the intent of the question refers to continuity of at ). Consider any real valued sequence , which is to say a map from to . Let be any positive integer. Let denote the value of the map at . Let . Choose Then (using conventional metric functions, etc.), the set of satisfying is the singleton . Moreover, we see that . Now, forget we fixed and , and we see that is continuous on Is this wrong? Is it correct to say that is continuous at ? Must we preface this by saying that is continuous at as a function of but not as a function of ? Thanks/","X Y A \subseteq X a \in A f : A \to Y f a 
\forall \varepsilon > 0, \hspace{1mm} \exists\hspace{1mm} \delta > 0 \ni \forall x\in A, \hspace{1mm}
     d_X(x,a) < \delta \implies d_Y (f(x), f(a)) < \varepsilon
 d_X d_Y X Y (a_n) f \mathbb{Z}^+ A n a (a_n) n (a_n) \mathbb{Z}^+ \mathbb{R} n a_n (a_n) n \varepsilon > 0 \delta = \frac{1}{2}. x\in\mathbb{Z}^+ d(x,n) < \delta \{n\} x \in\{n\} \implies d(a_x, a_n) = 0 < \varepsilon \varepsilon n (a_n) \mathbb{Z}^+. (a_n) n (a_n) n \mathbb{Z}^+, \mathbb{R}","['real-analysis', 'analysis']"
90,How to justify taking the derivative operator inside of the integral?,How to justify taking the derivative operator inside of the integral?,,"I'm reading a PDE book, and the authors use this often, for $u = u(x,t)$ , and V an arbitrary volume on in the interior of a solid: $${\frac{d}{dt} \int_V u \space dx}$$ $$= {\int_V u_t \space dx}$$ How is differentiating under the integral w.r.t. to time $t$ justified? I imagine it must be so simple that I'm simply rusty with my introductory analysis coursework and don't see it right away. My attempt: Writing out the difference quotient and using linearity of the integral, we have that: $$ \lim_{h \to 0} \int_V \frac { u(x,t+h) - u(x,t)} {h}  \space dx   $$ and assuming I can use the dominated convergence theorem, we have that: $$ \lim_{h \to 0} \int_V \frac { u(x,t+h) - u(x,t)} {h}  \space dx = \int_V \lim_{h \to 0} \frac { u(x,t+h) - u(x,t)} {h}  \space dx $$ $$ = {\int_V u_t \space dx} $$ So, there seem to be some nice conditions on $u$ that the authors are assuming. Thanks,","I'm reading a PDE book, and the authors use this often, for , and V an arbitrary volume on in the interior of a solid: How is differentiating under the integral w.r.t. to time justified? I imagine it must be so simple that I'm simply rusty with my introductory analysis coursework and don't see it right away. My attempt: Writing out the difference quotient and using linearity of the integral, we have that: and assuming I can use the dominated convergence theorem, we have that: So, there seem to be some nice conditions on that the authors are assuming. Thanks,","u = u(x,t) {\frac{d}{dt} \int_V u \space dx} = {\int_V u_t \space dx} t  \lim_{h \to 0} \int_V \frac { u(x,t+h) - u(x,t)} {h}  \space dx     \lim_{h \to 0} \int_V \frac { u(x,t+h) - u(x,t)} {h}  \space dx = \int_V \lim_{h \to 0} \frac { u(x,t+h) - u(x,t)} {h}  \space dx   = {\int_V u_t \space dx}  u","['calculus', 'integration', 'analysis', 'derivatives', 'partial-differential-equations']"
91,An integral with logarithm and rational function,An integral with logarithm and rational function,,"Can someone solve: $ \int_0^\infty dx \frac{\log (x)}{(x-1)(x+a)}\;,$ for $a$ say a positive real (or presumably any non real-negative complex number). Note there is no pole at $x=1$ . I can easily find the solution on mathematica, but I can't find a `proof'. My instinct is to use some nice contour, or bring the integral limits to [0,1] and expand the log, but I can't make either work! Edit : Sorry I should have added the following. I know that the above integral is easily evaluated via partial fractions and using properties of dilogarithms, but I am wondering out of curiosity whether a simpler proof is possible. Certainly the result (see below) looks as if there should be an extremely elementary argument.","Can someone solve: for say a positive real (or presumably any non real-negative complex number). Note there is no pole at . I can easily find the solution on mathematica, but I can't find a `proof'. My instinct is to use some nice contour, or bring the integral limits to [0,1] and expand the log, but I can't make either work! Edit : Sorry I should have added the following. I know that the above integral is easily evaluated via partial fractions and using properties of dilogarithms, but I am wondering out of curiosity whether a simpler proof is possible. Certainly the result (see below) looks as if there should be an extremely elementary argument."," \int_0^\infty dx \frac{\log (x)}{(x-1)(x+a)}\;, a x=1","['integration', 'analysis', 'definite-integrals']"
92,Is there a set in which division of 0 by 0 is defined?,Is there a set in which division of 0 by 0 is defined?,,"The reason I ask this is that I've discovered that, even though they don't satisfy all field axioms, there are sets called projectively extended real number line and Riemann sphere, which are ℝ∪{∞} and ℂ∪{∞} where division of every nonzero number of the set by 0 is defined as ∞. However, the two sets' arithmetic operations aren't total and some operations are left undefined. These include: ∞+∞, ∞-∞, ∞·0, 0·∞, ∞/∞, and 0/0. My question is that if there is, or could be a field-like set that can also define the results of these operations. And could there be a logical definition for those operations, especially 0/0?","The reason I ask this is that I've discovered that, even though they don't satisfy all field axioms, there are sets called projectively extended real number line and Riemann sphere, which are ℝ∪{∞} and ℂ∪{∞} where division of every nonzero number of the set by 0 is defined as ∞. However, the two sets' arithmetic operations aren't total and some operations are left undefined. These include: ∞+∞, ∞-∞, ∞·0, 0·∞, ∞/∞, and 0/0. My question is that if there is, or could be a field-like set that can also define the results of these operations. And could there be a logical definition for those operations, especially 0/0?",,"['analysis', 'field-theory', 'infinity', 'binary-operations']"
93,Proof of a criterion for Bochner integrability,Proof of a criterion for Bochner integrability,,"The following comes from the Wikipedia article on Bochner integrals . Theorem . If $(X,\Sigma,\mu)$ is a measure space, then a Bochner-measurable function $f:X\to B$ is Bochner integrable if and only if $$\int_X\|f\|_B\,d\mu<\infty.$$ Here are the relevant definitions. Definition . A function $f:X\to B$ is called Bochner-measurable if it is equal $\mu$ -almost everywhere to a function $g$ taking values in a separable subspace $B_0$ of $B$ , and such that the inverse image $g^{−1}(U)$ of every open set $U$ in $B$ belongs to $\Sigma$ . Definition . A measurable function $f:X\to B$ is Bochner integrable if there exists a sequence of integrable simple functions $s_n$ such that $$\lim_{n\to\infty}\int_X\|f-s_n\|_B\,d\mu=0,$$ where the integral on the left-hand side is an ordinary Lebesgue integral. How do I prove the theorem above? I'm following Analysis III by H. Amann & J. Escher, and this theorem is indeed proved, but under the assumption that $\mu$ is complete and $\sigma$ -finite. The subtlety here is this (I'll stick to the notations and terminologies on the wikipedia article): Theorem . $f$ is Bochner measurable if and only if $f$ is limit $\mu$ -almost everywhere of a sequence of simple functions. Theorem . If $\mu$ is $\sigma$ -finite, $f$ is Bochner measurable if and only if $f$ is limit $\mu$ -almost everywhere of a sequence of integrable simple functions. The proof of the latter makes use of $\sigma$ -finiteness to ensure that the the simple functions $s_n$ are indeed integrable , i.e., $s_n^{-1}(B\setminus\{0\})$ has finite measure. On the other hand, I can prove this (by adapting the proof from Amann & Escher): Theorem . If $f$ is limit $\mu$ -almost everywhere of a sequence of simple functions, then there is a sequence of simple functions $s_n$ such that $$\lim_{n\to\infty}\int_X\|f-s_n\|_B\,d\mu=0.$$ Theorem . If $f$ is limit $\mu$ -almost everywhere of a sequence of integrable simple functions, then there is a sequence of integrable simple functions $s_n$ such that $$\lim_{n\to\infty}\int_X\|f-s_n\|_B\,d\mu=0.$$ (The reason is that, the proof assumes there is a sequence of simple functions $t_n\to f$ a.e., and then constructs $s_n$ out of $t_n$ . If $t_n$ are integrable, then $s_n$ are integrable by construction.) Therefore, assuming $\sigma$ -finiteness, we can show: $f$ is Bochner-measurable $\implies$ $f$ is limit $\mu$ -almost everywhere of a sequence of integrable simple functions $\implies$ there is a sequence of integrable simple functions $s_n$ such that $$\lim_{n\to\infty}\int_X\|f-s_n\|_B\,d\mu=0.$$ However, I don't know how I should prove this without $\sigma$ -finiteness.","The following comes from the Wikipedia article on Bochner integrals . Theorem . If is a measure space, then a Bochner-measurable function is Bochner integrable if and only if Here are the relevant definitions. Definition . A function is called Bochner-measurable if it is equal -almost everywhere to a function taking values in a separable subspace of , and such that the inverse image of every open set in belongs to . Definition . A measurable function is Bochner integrable if there exists a sequence of integrable simple functions such that where the integral on the left-hand side is an ordinary Lebesgue integral. How do I prove the theorem above? I'm following Analysis III by H. Amann & J. Escher, and this theorem is indeed proved, but under the assumption that is complete and -finite. The subtlety here is this (I'll stick to the notations and terminologies on the wikipedia article): Theorem . is Bochner measurable if and only if is limit -almost everywhere of a sequence of simple functions. Theorem . If is -finite, is Bochner measurable if and only if is limit -almost everywhere of a sequence of integrable simple functions. The proof of the latter makes use of -finiteness to ensure that the the simple functions are indeed integrable , i.e., has finite measure. On the other hand, I can prove this (by adapting the proof from Amann & Escher): Theorem . If is limit -almost everywhere of a sequence of simple functions, then there is a sequence of simple functions such that Theorem . If is limit -almost everywhere of a sequence of integrable simple functions, then there is a sequence of integrable simple functions such that (The reason is that, the proof assumes there is a sequence of simple functions a.e., and then constructs out of . If are integrable, then are integrable by construction.) Therefore, assuming -finiteness, we can show: is Bochner-measurable is limit -almost everywhere of a sequence of integrable simple functions there is a sequence of integrable simple functions such that However, I don't know how I should prove this without -finiteness.","(X,\Sigma,\mu) f:X\to B \int_X\|f\|_B\,d\mu<\infty. f:X\to B \mu g B_0 B g^{−1}(U) U B \Sigma f:X\to B s_n \lim_{n\to\infty}\int_X\|f-s_n\|_B\,d\mu=0, \mu \sigma f f \mu \mu \sigma f f \mu \sigma s_n s_n^{-1}(B\setminus\{0\}) f \mu s_n \lim_{n\to\infty}\int_X\|f-s_n\|_B\,d\mu=0. f \mu s_n \lim_{n\to\infty}\int_X\|f-s_n\|_B\,d\mu=0. t_n\to f s_n t_n t_n s_n \sigma f \implies f \mu \implies s_n \lim_{n\to\infty}\int_X\|f-s_n\|_B\,d\mu=0. \sigma","['real-analysis', 'analysis', 'measure-theory', 'banach-spaces']"
94,Is there any construction of real numbers that does not use a quotient space in the process?,Is there any construction of real numbers that does not use a quotient space in the process?,,I am working on an isomorphism (in terms of order and operations) from the power set of integers to R. I would like to know if anyone knows of any construction of the real numbers that uses such a simple set to provide it with the structure of the real number system. Thanks!,I am working on an isomorphism (in terms of order and operations) from the power set of integers to R. I would like to know if anyone knows of any construction of the real numbers that uses such a simple set to provide it with the structure of the real number system. Thanks!,,"['analysis', 'order-theory', 'real-numbers']"
95,"Proving $\| x + y \| = \| x \| + \| y \|$ implies $\|ax + by\| = \|a x \| + \|by\|$ for any $a,b \geq 0$",Proving  implies  for any,"\| x + y \| = \| x \| + \| y \| \|ax + by\| = \|a x \| + \|by\| a,b \geq 0","Consider $ (R^p, \| \cdot \|)$ a norm space and let $x$ and $y$ be vectors such that $\| x + y \| = \| x \| + \| y \|$. Now show that for any $\alpha$, $\beta$ $\geq 0$, $\|\alpha x + \beta y\| = \|\alpha x \| + \|\beta  y\|$. This is with regard to general norms, i.e. $$ ||\bar{x}|| \geq 0  $$ $$ ||\alpha\bar{x}|| = |\alpha | ||\bar{x}||  $$ $$ ||\bar{x} + \bar{y} || \leq ||\bar x|| + ||\bar y||  $$ $$ ||\bar{x}||= 0 \Leftrightarrow \bar x = 0 $$ With $|| \cdot ||_2$ the proof of this is simple, it implies colinearity of the vectors. For a general norm, I think it implies a somewhat weaker notion of ""colinearity"", in the hand-wavey sense. This is a homework question and I don't really want the answer---that will not be of much help. My assessment of the problem is to solve it by making strict use of the four properties of a norm that I've listed above but I am missing some key insight that tells me my maths toolbox is lacking. I would appreciate some problem solving guidance that will eventually lead me to figure this out.","Consider $ (R^p, \| \cdot \|)$ a norm space and let $x$ and $y$ be vectors such that $\| x + y \| = \| x \| + \| y \|$. Now show that for any $\alpha$, $\beta$ $\geq 0$, $\|\alpha x + \beta y\| = \|\alpha x \| + \|\beta  y\|$. This is with regard to general norms, i.e. $$ ||\bar{x}|| \geq 0  $$ $$ ||\alpha\bar{x}|| = |\alpha | ||\bar{x}||  $$ $$ ||\bar{x} + \bar{y} || \leq ||\bar x|| + ||\bar y||  $$ $$ ||\bar{x}||= 0 \Leftrightarrow \bar x = 0 $$ With $|| \cdot ||_2$ the proof of this is simple, it implies colinearity of the vectors. For a general norm, I think it implies a somewhat weaker notion of ""colinearity"", in the hand-wavey sense. This is a homework question and I don't really want the answer---that will not be of much help. My assessment of the problem is to solve it by making strict use of the four properties of a norm that I've listed above but I am missing some key insight that tells me my maths toolbox is lacking. I would appreciate some problem solving guidance that will eventually lead me to figure this out.",,"['real-analysis', 'analysis']"
96,Theorem 9.17 from Baby Rudin,Theorem 9.17 from Baby Rudin,,"I'm a bit confused about the final step in the proof the the theorem below from Baby Rudin. This theorem tells precisely that the value of the linear transformation $f'(x)$ at $e_j$ is the column-vector in $R^m$ which is the partial derivative of the vector $f=(f_1,\dots,f_m)^t$ with respect to the $j^{th}$ variable. Now, doesn't Eq. $(28)$ already imply this claim? The left-hand side of this equation is precisely the directional derivative of $f$ at $x$ in the direction of $e_j$, which is precisely what we need. Do I miss something? Furthermore, it seems to me that Rudin's reference to Theorem 4.10 is imprecise. This theorem says the following: However, I don't understand how this theorem is used to conclude the existence of the limits of each quotient.","I'm a bit confused about the final step in the proof the the theorem below from Baby Rudin. This theorem tells precisely that the value of the linear transformation $f'(x)$ at $e_j$ is the column-vector in $R^m$ which is the partial derivative of the vector $f=(f_1,\dots,f_m)^t$ with respect to the $j^{th}$ variable. Now, doesn't Eq. $(28)$ already imply this claim? The left-hand side of this equation is precisely the directional derivative of $f$ at $x$ in the direction of $e_j$, which is precisely what we need. Do I miss something? Furthermore, it seems to me that Rudin's reference to Theorem 4.10 is imprecise. This theorem says the following: However, I don't understand how this theorem is used to conclude the existence of the limits of each quotient.",,"['calculus', 'real-analysis', 'analysis']"
97,"$X^2 + X =A$ with $X, A\in \text{Mat}_{2,2} (\mathbb{R})$ . Show that there exists a solution $X$ for a given $A$",with  . Show that there exists a solution  for a given,"X^2 + X =A X, A\in \text{Mat}_{2,2} (\mathbb{R}) X A","Prove the following statement: There exists an $\epsilon > 0$ such that the following holds: If $A = (a_{ij}) \in \text{Mat}_{2,2} (\mathbb{R})$ a matrix with $(|a_{ij}|) < \epsilon$ for $i,j \in \{1,2\}$, then the following equation $$X^2 + X = A$$ has a solution $X \in \text{Mat}_{2,2} (\mathbb{R})$ My Idea on how to solve this: Let $X =  \begin{bmatrix} v& w \\ x & y  \end{bmatrix}$. Therefore $X^2 + X = \begin{bmatrix} v^2 + v + w x& v w + w y + w\\ v x + x y + x & w x + y^2 + y \end{bmatrix} = \begin{bmatrix} a_0 & a_1 \\ a_2 & a_3\end{bmatrix}$ Lets now define the function $$ 0=h(v,w,y,x,a_0,a_1,a_2,a_3) =\begin{cases} v^2 + v + w x - a_0 \\ v w + w y + w - a_1\\v x + x y + x -a_2 \\w x + y^2 + y-a_3 \end{cases} $$ We can now calculate the derivative of $h$: $$dh = \begin{bmatrix}  2v + 1 &  x & 0 & w & -1&0&0&0\\  w& v+y+1& w& 0& 0&-1&0&0\\ x & 0&x&v +1 & 0&0&-1&0 \\0&x&2y+1&w& 0&0&0&-1 \end{bmatrix}$$ The idea now would be to apply the implicit function theorem and show that there exists an $X$ which solves this equation. I am not sure though if this approach is correct. Last but not least.. this question comes from an analysis sheet, so I assume one should use the methods of analysis to solve it. Is my approach the correct way? And how does one proceed from here? Feel free to use another approach. Thank you for your time.","Prove the following statement: There exists an $\epsilon > 0$ such that the following holds: If $A = (a_{ij}) \in \text{Mat}_{2,2} (\mathbb{R})$ a matrix with $(|a_{ij}|) < \epsilon$ for $i,j \in \{1,2\}$, then the following equation $$X^2 + X = A$$ has a solution $X \in \text{Mat}_{2,2} (\mathbb{R})$ My Idea on how to solve this: Let $X =  \begin{bmatrix} v& w \\ x & y  \end{bmatrix}$. Therefore $X^2 + X = \begin{bmatrix} v^2 + v + w x& v w + w y + w\\ v x + x y + x & w x + y^2 + y \end{bmatrix} = \begin{bmatrix} a_0 & a_1 \\ a_2 & a_3\end{bmatrix}$ Lets now define the function $$ 0=h(v,w,y,x,a_0,a_1,a_2,a_3) =\begin{cases} v^2 + v + w x - a_0 \\ v w + w y + w - a_1\\v x + x y + x -a_2 \\w x + y^2 + y-a_3 \end{cases} $$ We can now calculate the derivative of $h$: $$dh = \begin{bmatrix}  2v + 1 &  x & 0 & w & -1&0&0&0\\  w& v+y+1& w& 0& 0&-1&0&0\\ x & 0&x&v +1 & 0&0&-1&0 \\0&x&2y+1&w& 0&0&0&-1 \end{bmatrix}$$ The idea now would be to apply the implicit function theorem and show that there exists an $X$ which solves this equation. I am not sure though if this approach is correct. Last but not least.. this question comes from an analysis sheet, so I assume one should use the methods of analysis to solve it. Is my approach the correct way? And how does one proceed from here? Feel free to use another approach. Thank you for your time.",,['linear-algebra']
98,Show that $\int_{0}^{1}\ln\left({1+x\over 1-x}\right){\mathrm dx\over 1+x^2}=C$,Show that,\int_{0}^{1}\ln\left({1+x\over 1-x}\right){\mathrm dx\over 1+x^2}=C,How do we show $(1)$ is correct? $$\int_{0}^{1}\ln\left({1+x\over 1-x}\right){\mathrm dx\over 1+x^2}=C\tag1$$ Where C is the Catalan constant $u={1+x\over 1-x}$ then $du={2dx\over (1-x)^2}$ $${1\over 2}\int_{1}^{\infty}(1-x)^2\ln(u){\mathrm du\over 1+x^2}=C\tag2$$ $x={u-1\over u+1}$ $${1\over 2}\int_{1}^{\infty}{4\over (u+1)^2}\ln(u){\mathrm (u+1)^2du\over 2(u^2+1)}=C\tag3$$ $$\int_{1}^{\infty}\ln(u){\mathrm du\over 1+u^2}=C\tag4$$,How do we show $(1)$ is correct? $$\int_{0}^{1}\ln\left({1+x\over 1-x}\right){\mathrm dx\over 1+x^2}=C\tag1$$ Where C is the Catalan constant $u={1+x\over 1-x}$ then $du={2dx\over (1-x)^2}$ $${1\over 2}\int_{1}^{\infty}(1-x)^2\ln(u){\mathrm du\over 1+x^2}=C\tag2$$ $x={u-1\over u+1}$ $${1\over 2}\int_{1}^{\infty}{4\over (u+1)^2}\ln(u){\mathrm (u+1)^2du\over 2(u^2+1)}=C\tag3$$ $$\int_{1}^{\infty}\ln(u){\mathrm du\over 1+u^2}=C\tag4$$,,"['real-analysis', 'integration', 'analysis', 'closed-form', 'catalans-constant']"
99,Example of a sequence with integral → 0 but gn(x) does not converge to zero and >=0.,Example of a sequence with integral → 0 but gn(x) does not converge to zero and >=0.,,"I need to think of an example of a sequence $g_n$ with $\int_0^1g_n → 0 $ but $g_n(x)$ does not converge to zero for any $x∈ [0,1]$ and $g_n(x) ≥ 0$ for all $x$ and $n$. Hints and help wanted! The fact $g_n(x)$ does not converge to zero is what has caused most of my attempts to fail.","I need to think of an example of a sequence $g_n$ with $\int_0^1g_n → 0 $ but $g_n(x)$ does not converge to zero for any $x∈ [0,1]$ and $g_n(x) ≥ 0$ for all $x$ and $n$. Hints and help wanted! The fact $g_n(x)$ does not converge to zero is what has caused most of my attempts to fail.",,"['real-analysis', 'integration', 'analysis']"
