,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Lebesgue Dominated Convergence example,Lebesgue Dominated Convergence example,,"How do I use Lebesgue Dominated Convergence Theorem to evaluate $$\lim_{n \to \infty}\int_{[0,1]}\frac{n\sin(x)}{1+n^2\sqrt x}dx$$ What dominating function to use here?","How do I use Lebesgue Dominated Convergence Theorem to evaluate $$\lim_{n \to \infty}\int_{[0,1]}\frac{n\sin(x)}{1+n^2\sqrt x}dx$$ What dominating function to use here?",,"['real-analysis', 'integration', 'measure-theory', 'convergence-divergence']"
1,Converse of Bolzano-Weierstrass theorem,Converse of Bolzano-Weierstrass theorem,,"Bolzano-Weierstrass theorem states that every bounded sequence has a limit point. But, the converse is not true. That is, there are some unbounded sequences which have a limit point. In my course book, I found an example for this claim, but it doesn't make sense. Here's the example give in the book: The set: {1, 2, 1, 4, 1, 6, ...} is unbounded, but has a limit point of 1 .  I can't understand how this set has a limit point as 1 .  According to the book definition of limit point, 'x' is the limit point of a sequence, if every neighborhood of 'x' has infinitely many elements of the sequence. If I apply it here, then I get only infinity as the limit point. Am I missing something?","Bolzano-Weierstrass theorem states that every bounded sequence has a limit point. But, the converse is not true. That is, there are some unbounded sequences which have a limit point. In my course book, I found an example for this claim, but it doesn't make sense. Here's the example give in the book: The set: {1, 2, 1, 4, 1, 6, ...} is unbounded, but has a limit point of 1 .  I can't understand how this set has a limit point as 1 .  According to the book definition of limit point, 'x' is the limit point of a sequence, if every neighborhood of 'x' has infinitely many elements of the sequence. If I apply it here, then I get only infinity as the limit point. Am I missing something?",,"['real-analysis', 'sequences-and-series', 'limits']"
2,A question on Lebesgue measure and its absolutely continuous measures,A question on Lebesgue measure and its absolutely continuous measures,,"Let $\lambda$ be the Lebesgue measure and $\mu$ be a measure absolutely continuous with respect to $\lambda$. Suppose for Lebesgue measurable sets $X_n$, $n=1,2,...$, $\lambda(X_n)=1/n$. Do we have  $\mu(X_n)\to 0$ as $n\to \infty$? Thanks.","Let $\lambda$ be the Lebesgue measure and $\mu$ be a measure absolutely continuous with respect to $\lambda$. Suppose for Lebesgue measurable sets $X_n$, $n=1,2,...$, $\lambda(X_n)=1/n$. Do we have  $\mu(X_n)\to 0$ as $n\to \infty$? Thanks.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
3,Why doesn't the limit of $\frac{e^{\frac1x}-1}{e^{\frac1x}+1}$ exist?,Why doesn't the limit of  exist?,\frac{e^{\frac1x}-1}{e^{\frac1x}+1},Consider $$\lim_{x \to 0}  \frac{e^{\frac1x}-1}{e^{\frac1x}+1}$$ Applying L'hospital's rule for the left hand limit and right hand limit gives the same answer. Why doesn't this limit exist?,Consider $$\lim_{x \to 0}  \frac{e^{\frac1x}-1}{e^{\frac1x}+1}$$ Applying L'hospital's rule for the left hand limit and right hand limit gives the same answer. Why doesn't this limit exist?,,"['real-analysis', 'calculus', 'limits', 'arithmetic', 'limits-without-lhopital']"
4,Why does the sequence of functions $f_n(x)=nx^n(1-x)$ not converge uniformly?,Why does the sequence of functions  not converge uniformly?,f_n(x)=nx^n(1-x),"Let $(f_n)_n$ be a sequence of functions defined by $$f_n : [0,1] \rightarrow \mathbb{R}: x \mapsto nx^n(1-x). $$ We have $\lim_{n \to \infty} f_n(x) = 0$ for all $x$. My textbook says the convergence isn't uniformly though, and I don't understand why. I computed $$\sup_{x \in [0,1]} |f_n(x) - 0 | = \sup_{x \in [0,1]} | nx^n (1-x) | = \ ? $$  How do I figure out this supremum? When I use the definition, I see that $$| nx^n(1-x) - 0 | = | nx^n - nx^{n+1}| \leq | nx^n | + | nx^{n+1}| \leq n + n = 2n. $$ And I can never get this smaller than $\epsilon$. But I couldn't find an explicit $\epsilon > 0$ and a $x \in [0,1]$ such that $ |f_n(x)| \geq \epsilon. $ Any help is appreciated.","Let $(f_n)_n$ be a sequence of functions defined by $$f_n : [0,1] \rightarrow \mathbb{R}: x \mapsto nx^n(1-x). $$ We have $\lim_{n \to \infty} f_n(x) = 0$ for all $x$. My textbook says the convergence isn't uniformly though, and I don't understand why. I computed $$\sup_{x \in [0,1]} |f_n(x) - 0 | = \sup_{x \in [0,1]} | nx^n (1-x) | = \ ? $$  How do I figure out this supremum? When I use the definition, I see that $$| nx^n(1-x) - 0 | = | nx^n - nx^{n+1}| \leq | nx^n | + | nx^{n+1}| \leq n + n = 2n. $$ And I can never get this smaller than $\epsilon$. But I couldn't find an explicit $\epsilon > 0$ and a $x \in [0,1]$ such that $ |f_n(x)| \geq \epsilon. $ Any help is appreciated.",,"['real-analysis', 'sequences-and-series', 'uniform-convergence']"
5,Prove that $\frac{x^n}{n!} < 1$ for $n$ sufficiently large,Prove that  for  sufficiently large,\frac{x^n}{n!} < 1 n,"Is this statement always true? For any real number $x$, there exists a natural number $n$, such that    $\frac{x^n}{n!} < 1$. I reach this conclusion observing the series form of '$e^x$'. I observe that for larger value of $n$, $\frac{x^n}{n!}$ changes the numeric value of $e^x$ right side of decimal point. Please provide the proof or link of the proof.","Is this statement always true? For any real number $x$, there exists a natural number $n$, such that    $\frac{x^n}{n!} < 1$. I reach this conclusion observing the series form of '$e^x$'. I observe that for larger value of $n$, $\frac{x^n}{n!}$ changes the numeric value of $e^x$ right side of decimal point. Please provide the proof or link of the proof.",,"['real-analysis', 'inequality']"
6,Find $\lim_{n \to \infty} \sqrt[n]{n!}$.,Find .,\lim_{n \to \infty} \sqrt[n]{n!},"I am playing around with the root/ratio test to practice with series. I just showed that $\sum \frac{1}{n!}$ converges by using the ratio test. I decided to see how things would go with the root test and I got stuck at something that I can't find on google. Right away while running the root test I encountered $\lim_{x \to +\infty}\left(\frac{1}{n!}\right)^\frac{1}{n}$. I have made the claim that $\left(n!\right)^\frac{1}{n}\geq 1 \hspace{3mm}\forall n\in \mathbb{N}.$ I have begun the proof and I think it is on the right track but some verification would be nice. Proof: We can see that for n=1, this obviously holds, $\left(1!\right)^1\geq 1.$ Now suppose that this is the case for some $n=k$, then we have $\left((k+1)!\right)^{\frac{1}{k+1}}= \left((k+1)k!\right)^\frac{1}{k+1}$. It is at this point that I start to have a little trouble. Can somebody give me a push in the right direction? Thanks!","I am playing around with the root/ratio test to practice with series. I just showed that $\sum \frac{1}{n!}$ converges by using the ratio test. I decided to see how things would go with the root test and I got stuck at something that I can't find on google. Right away while running the root test I encountered $\lim_{x \to +\infty}\left(\frac{1}{n!}\right)^\frac{1}{n}$. I have made the claim that $\left(n!\right)^\frac{1}{n}\geq 1 \hspace{3mm}\forall n\in \mathbb{N}.$ I have begun the proof and I think it is on the right track but some verification would be nice. Proof: We can see that for n=1, this obviously holds, $\left(1!\right)^1\geq 1.$ Now suppose that this is the case for some $n=k$, then we have $\left((k+1)!\right)^{\frac{1}{k+1}}= \left((k+1)k!\right)^\frac{1}{k+1}$. It is at this point that I start to have a little trouble. Can somebody give me a push in the right direction? Thanks!",,"['real-analysis', 'sequences-and-series', 'elementary-number-theory', 'limits', 'factorial']"
7,Prove that $\lim_{k\to\infty}{ \sqrt[n]{ \prod_{i=1}^{n}{(x_i+k)}} - k } = \frac{\sum_{i=1}^{n}{x_i}}{n}$,Prove that,\lim_{k\to\infty}{ \sqrt[n]{ \prod_{i=1}^{n}{(x_i+k)}} - k } = \frac{\sum_{i=1}^{n}{x_i}}{n},"I accidentally discovered this equality, in which I can prove numerically using python. $$\lim_{k\to\infty}{ \sqrt[n]{ \prod_{i=1}^{n}{(x_i+k)}} - k } = \frac{\sum_{i=1}^{n}{x_i}}{n}$$ But I need to prove this equality in an algebraic way and I could not get anywhere. The right-hand side is nothing more than an arithmetic mean, while the left-hand side is a modification of the geometric mean. I hope someone can help me at this point.","I accidentally discovered this equality, in which I can prove numerically using python. But I need to prove this equality in an algebraic way and I could not get anywhere. The right-hand side is nothing more than an arithmetic mean, while the left-hand side is a modification of the geometric mean. I hope someone can help me at this point.",\lim_{k\to\infty}{ \sqrt[n]{ \prod_{i=1}^{n}{(x_i+k)}} - k } = \frac{\sum_{i=1}^{n}{x_i}}{n},"['real-analysis', 'limits', 'means']"
8,"Function $g(x)$, such that $\int_{\mathbb{R}} f(g(x))dx = \int_{\mathbb{R}} f(x)dx$ for all $f\in L^1(\mathbb{R})$","Function , such that  for all",g(x) \int_{\mathbb{R}} f(g(x))dx = \int_{\mathbb{R}} f(x)dx f\in L^1(\mathbb{R}),"It's not too hard to show that for all $f\in L^1(\mathbb{R})$ and for $g(x) = x-\frac{1}{x}$ we have $\int_{\mathbb{R}} f(g(x))dx = \int_{\mathbb{R}} f(x)dx$:use the substitution $y = x - \frac{1}{x}$ and split the integral in two parts: $x<0$ and $x>0$. On one of them make the choice $x = \frac{y + \sqrt{y^2 + 4}}{2}$, on the other $x = \frac{y - \sqrt{y^2 + 4}}{2}$ and add the resulting integrals. The existence of even one function $g(x)$, such that the above works is astonishing. Are there other choices of $g$, such that $\int_{\mathbb{R}} f(g(x))dx = \int_{\mathbb{R}} f(x)dx$ for all $f\in L^1(\mathbb{R})$? Edit: $g(x) = x + a$ for any a would be a solution, but I was wondering if there are more complicated $g$ which work, or if $x - \frac{1}{x}$ is just a single example.","It's not too hard to show that for all $f\in L^1(\mathbb{R})$ and for $g(x) = x-\frac{1}{x}$ we have $\int_{\mathbb{R}} f(g(x))dx = \int_{\mathbb{R}} f(x)dx$:use the substitution $y = x - \frac{1}{x}$ and split the integral in two parts: $x<0$ and $x>0$. On one of them make the choice $x = \frac{y + \sqrt{y^2 + 4}}{2}$, on the other $x = \frac{y - \sqrt{y^2 + 4}}{2}$ and add the resulting integrals. The existence of even one function $g(x)$, such that the above works is astonishing. Are there other choices of $g$, such that $\int_{\mathbb{R}} f(g(x))dx = \int_{\mathbb{R}} f(x)dx$ for all $f\in L^1(\mathbb{R})$? Edit: $g(x) = x + a$ for any a would be a solution, but I was wondering if there are more complicated $g$ which work, or if $x - \frac{1}{x}$ is just a single example.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
9,An olympic mathematics problem regarding Cauchy-Schwarz,An olympic mathematics problem regarding Cauchy-Schwarz,,This question was asked in Turkish National Maths Olympiad in 2008. For all $xy=1$ we have $((x+y)^2+4)\cdot ((x+y)^2-2) \ge A\cdot(x-y)^2$ . What is the maximum value $A$ can get? My efforts regarding this problem; $(x+y)^2-8 \ge A\cdot(x-y)^2$ Using the property $xy=1$ ; $x^2+y^2-6\ge A\cdot (x^2+y^2-2)$ $\sqrt{\dfrac{x^2+y^2}{2}} \ge\sqrt{\dfrac{A\cdot(x^2+y^2-2)}{2}}$ Therefore $\sqrt{\dfrac{A\cdot(x^2+y^2-2)}{2}}=\dfrac{x+y}{2}$ Although moving further that doesn't work I applied Cauchy-Schwarz by the way if I didn't mention it before. How should I proceed? What are you suggestions?,This question was asked in Turkish National Maths Olympiad in 2008. For all we have . What is the maximum value can get? My efforts regarding this problem; Using the property ; Therefore Although moving further that doesn't work I applied Cauchy-Schwarz by the way if I didn't mention it before. How should I proceed? What are you suggestions?,xy=1 ((x+y)^2+4)\cdot ((x+y)^2-2) \ge A\cdot(x-y)^2 A (x+y)^2-8 \ge A\cdot(x-y)^2 xy=1 x^2+y^2-6\ge A\cdot (x^2+y^2-2) \sqrt{\dfrac{x^2+y^2}{2}} \ge\sqrt{\dfrac{A\cdot(x^2+y^2-2)}{2}} \sqrt{\dfrac{A\cdot(x^2+y^2-2)}{2}}=\dfrac{x+y}{2},"['real-analysis', 'algebra-precalculus', 'inequality', 'contest-math', 'cauchy-schwarz-inequality']"
10,"Prove that $\,f=0$ almost everywhere.",Prove that  almost everywhere.,"\,f=0","Let $f$ be a Lebesgue integrable function on $[0,1]$ such that, for any $0 \leq a < b \leq 1$, $$\biggl|\int^b_a f(x)\,dx\,\biggr| \leq (b-a)^2\,.$$ Prove that $f=0$ almost everywhere. I would be thankful if somebody tell me whether my attempt is correct or not: Consider the partition  $$ [0,1]=\left[0,\frac{1}{n}\right) \cup \left[\frac{1}{n},\frac{2}{n}\right)  \cup \cdots \cup \left[\frac{n-1}{n},1\right].  $$ I am going to use the fact that if $g$ is an integrable over a measurable set $E$, $g$ is finite almost everywhere on $E$, i.e. there is $M>0$ such that  $\vert g\rvert<M$ a.e. on E, moreover $$ \bigg|\int_Eg(x)dx\,\bigg|\leq M m(E) $$ Now using the hypothesis $$ \biggl|\int^{\frac{1}{n}}_{0} f(x)\,dx\biggr| \leq \biggl( \frac{1}{n} \biggr)^2  \Rightarrow |f(x)| \leq \frac{1}{n} \ \ a.e. \ on \ \biggl[0,\frac{1}{n} \biggr)  $$ $$\biggl|\int^{\frac{2}{n}}_{\frac{1}{n}} f(x)\,dx\biggr| \leq \biggl(\frac{1}{n}\biggr)^2  \Rightarrow |f(x)| \leq \frac{1}{n} \ \ a.e. \ on \ \biggl[\frac{1}{n},\frac{2}{n}\biggr) $$ and similarly $$\biggl|\int^{1}_{\frac{n-1}{n}} f(x)\,dx\biggr| \leq \biggl(\frac{1}{n}\biggr)^2  \Rightarrow |f(x)| \leq \frac{1}{n} \ \ a.e. \ on \ \biggl[\frac{n-1}{n},1\biggr]$$ Hence, $$ |f(x)| \leq \frac{1}{n} \ \ a.e. \ on \ [0,1]$$ we can let $n \rightarrow \infty$ which yields that $f=0$ almost everywhere on $[0,1]$.","Let $f$ be a Lebesgue integrable function on $[0,1]$ such that, for any $0 \leq a < b \leq 1$, $$\biggl|\int^b_a f(x)\,dx\,\biggr| \leq (b-a)^2\,.$$ Prove that $f=0$ almost everywhere. I would be thankful if somebody tell me whether my attempt is correct or not: Consider the partition  $$ [0,1]=\left[0,\frac{1}{n}\right) \cup \left[\frac{1}{n},\frac{2}{n}\right)  \cup \cdots \cup \left[\frac{n-1}{n},1\right].  $$ I am going to use the fact that if $g$ is an integrable over a measurable set $E$, $g$ is finite almost everywhere on $E$, i.e. there is $M>0$ such that  $\vert g\rvert<M$ a.e. on E, moreover $$ \bigg|\int_Eg(x)dx\,\bigg|\leq M m(E) $$ Now using the hypothesis $$ \biggl|\int^{\frac{1}{n}}_{0} f(x)\,dx\biggr| \leq \biggl( \frac{1}{n} \biggr)^2  \Rightarrow |f(x)| \leq \frac{1}{n} \ \ a.e. \ on \ \biggl[0,\frac{1}{n} \biggr)  $$ $$\biggl|\int^{\frac{2}{n}}_{\frac{1}{n}} f(x)\,dx\biggr| \leq \biggl(\frac{1}{n}\biggr)^2  \Rightarrow |f(x)| \leq \frac{1}{n} \ \ a.e. \ on \ \biggl[\frac{1}{n},\frac{2}{n}\biggr) $$ and similarly $$\biggl|\int^{1}_{\frac{n-1}{n}} f(x)\,dx\biggr| \leq \biggl(\frac{1}{n}\biggr)^2  \Rightarrow |f(x)| \leq \frac{1}{n} \ \ a.e. \ on \ \biggl[\frac{n-1}{n},1\biggr]$$ Hence, $$ |f(x)| \leq \frac{1}{n} \ \ a.e. \ on \ [0,1]$$ we can let $n \rightarrow \infty$ which yields that $f=0$ almost everywhere on $[0,1]$.",,"['real-analysis', 'integration', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
11,Infinite Series $\sum\limits_{n=1}^{\infty}\frac{1}{4^n\cos^2\frac{x}{2^n}}$,Infinite Series,\sum\limits_{n=1}^{\infty}\frac{1}{4^n\cos^2\frac{x}{2^n}},How to prove that  $$\sum_{n=1}^{\infty}\frac{1}{4^n\cos^2\frac{x}{2^n}}=\frac1{\sin^2x}-\frac1{x^2}$$,How to prove that  $$\sum_{n=1}^{\infty}\frac{1}{4^n\cos^2\frac{x}{2^n}}=\frac1{\sin^2x}-\frac1{x^2}$$,,"['calculus', 'real-analysis', 'sequences-and-series', 'closed-form', 'trigonometric-series']"
12,Convergence of a series implies the convergence of the squares,Convergence of a series implies the convergence of the squares,,Suppose $a_n\in\Bbb{R}$ and $\sum a_n$ converges. Is it necessarily true that $\sum (a_n)^2$ converges? I think the answer is yes but I was unable to prove it. It would be much easier if $\sum a_n$ was absolutely convergent. Can you help me to prove this?,Suppose $a_n\in\Bbb{R}$ and $\sum a_n$ converges. Is it necessarily true that $\sum (a_n)^2$ converges? I think the answer is yes but I was unable to prove it. It would be much easier if $\sum a_n$ was absolutely convergent. Can you help me to prove this?,,"['real-analysis', 'sequences-and-series']"
13,"Definition of ""not converging"" and proving $(-1)^n$ does not converge to $1$.","Definition of ""not converging"" and proving  does not converge to .",(-1)^n 1,"Remember that a sequence $x_n, n = 1,2,3\cdots$ is said to converge to $x$ as $n → ∞$ if for all $ε > 0$ there exists an $N ∈ \mathbb{N}$ such that $|x_n − x| < ε$ for all $n ≥ N$. (a) Complete the following statement: “If the sequence $x_n, n = 1,2,3\cdots$ does not converge to $x$ as $n → ∞$, that means that there exists an $ε > 0$ such that...” (b) Consider the sequence $x_n = (−1)^n, n = 1,2,3\cdots$ that is, the sequence is $(−1,1,−1,1,−1,...)$. Prove carefully, starting from your answer to part (a), that this sequence does not converge to 1. I am confused with the first part and what epsilon represents!","Remember that a sequence $x_n, n = 1,2,3\cdots$ is said to converge to $x$ as $n → ∞$ if for all $ε > 0$ there exists an $N ∈ \mathbb{N}$ such that $|x_n − x| < ε$ for all $n ≥ N$. (a) Complete the following statement: “If the sequence $x_n, n = 1,2,3\cdots$ does not converge to $x$ as $n → ∞$, that means that there exists an $ε > 0$ such that...” (b) Consider the sequence $x_n = (−1)^n, n = 1,2,3\cdots$ that is, the sequence is $(−1,1,−1,1,−1,...)$. Prove carefully, starting from your answer to part (a), that this sequence does not converge to 1. I am confused with the first part and what epsilon represents!",,"['real-analysis', 'sequences-and-series', 'analysis', 'limits']"
14,Can you take Dedekind Cuts of the real numbers?,Can you take Dedekind Cuts of the real numbers?,,"My professor defined Dedekind cuts in the following way:     ""Given two nonempty sets E,F $\subset \mathbb{R}$, we say that the pair (E,F) is a Dedekind cut of $\mathbb{R}$ if $E \cap F = \emptyset $ $E \cup F = \mathbb{R}$ $x < y$ for all $x \in E$ and for all $y \in F$"" However, I was under the impression that Dedekind cuts could only be taken of the rational numbers, in order to construct the real numbers.  Can you take a Dedekind cut of the Real line? Edit: I should mention that he defined a cut this way in order for him to state the following Dedekind Axiom: ""For every Dedekind cut (E,F) of $\mathbb{R}$ there exists a unique $L \in \mathbb{R}$ such that $x \leq L \leq y$ for all $x \in E$ and for all $y \in F$""","My professor defined Dedekind cuts in the following way:     ""Given two nonempty sets E,F $\subset \mathbb{R}$, we say that the pair (E,F) is a Dedekind cut of $\mathbb{R}$ if $E \cap F = \emptyset $ $E \cup F = \mathbb{R}$ $x < y$ for all $x \in E$ and for all $y \in F$"" However, I was under the impression that Dedekind cuts could only be taken of the rational numbers, in order to construct the real numbers.  Can you take a Dedekind cut of the Real line? Edit: I should mention that he defined a cut this way in order for him to state the following Dedekind Axiom: ""For every Dedekind cut (E,F) of $\mathbb{R}$ there exists a unique $L \in \mathbb{R}$ such that $x \leq L \leq y$ for all $x \in E$ and for all $y \in F$""",,['real-analysis']
15,Proof of greatest integer theorem: floor function is well-defined,Proof of greatest integer theorem: floor function is well-defined,,"I have to prove that $$\forall x \in \mathbb{R},\exists !\,n \in \mathbb{Z} \text{ s.t. }n \leq x < n+1\;.$$ where $\exists !\,n $ means there exists a unique (exactly one) $n$ . I'm done with proving that there are at least one integers for the solution. I couldn't prove the ""uniqueness"" of the solution, and so I looked up the internet, and here's what I found: Let $\hspace{2mm}n,m \in \mathbb{Z} \text{ s.t. }n \leq x < n+1$ and $m \leq x < m+1$ . Since $n \leq x \text{ and } -(m+1) < -x$ , by adding both, $n + (-m-1) < (x-x) = 0$ . And (some steps here) likewise, $(x-x) < n+m+1$ . Now, can I add up inequalities like that, even when the book is about real analysis (and in which assumptions are supposed to be really minimal)? Or should I also prove those addition of inequalities? Thank you :D","I have to prove that where means there exists a unique (exactly one) . I'm done with proving that there are at least one integers for the solution. I couldn't prove the ""uniqueness"" of the solution, and so I looked up the internet, and here's what I found: Let and . Since , by adding both, . And (some steps here) likewise, . Now, can I add up inequalities like that, even when the book is about real analysis (and in which assumptions are supposed to be really minimal)? Or should I also prove those addition of inequalities? Thank you :D","\forall x \in \mathbb{R},\exists !\,n \in \mathbb{Z} \text{ s.t. }n \leq x < n+1\;. \exists !\,n  n \hspace{2mm}n,m \in \mathbb{Z} \text{ s.t. }n \leq x < n+1 m \leq x < m+1 n \leq x \text{ and } -(m+1) < -x n + (-m-1) < (x-x) = 0 (x-x) < n+m+1","['real-analysis', 'ceiling-and-floor-functions']"
16,Limits of 2 variable functions,Limits of 2 variable functions,,"How would I prove that if $f: \mathbb R^2 \to \mathbb R$ is a function such that $$\lim_{(x,y)\to(a,b)} f(x,y) = L$$ and for every $y_0 \in \mathbb R$ $$ \lim_{x\to a} f(x,y_0) = L'_{y_0}$$ and for every $x_0 \in \mathbb R$ $$ \lim_{y\to b} f(x_0,y) = L''_{x_0}$$ then $$ \lim_{x\to a}\left(\lim_{y\to b} f(x,y)\right)  = \lim_{y\to b}\left(\lim_{x\to a} f(x,y)\right) = L$$","How would I prove that if $f: \mathbb R^2 \to \mathbb R$ is a function such that $$\lim_{(x,y)\to(a,b)} f(x,y) = L$$ and for every $y_0 \in \mathbb R$ $$ \lim_{x\to a} f(x,y_0) = L'_{y_0}$$ and for every $x_0 \in \mathbb R$ $$ \lim_{y\to b} f(x_0,y) = L''_{x_0}$$ then $$ \lim_{x\to a}\left(\lim_{y\to b} f(x,y)\right)  = \lim_{y\to b}\left(\lim_{x\to a} f(x,y)\right) = L$$",,"['real-analysis', 'limits']"
17,Show that unit circle is not homeomorphic to the real line,Show that unit circle is not homeomorphic to the real line,,"Show that $S^1$ is not homeomorphic to either $\mathbb{R}^1$ or $\mathbb{R}^2$ $\mathbf{My \ solution}$: So first we will show that $S^1$ is not homeomorphic to $\mathbb{R}^1$. To show that they are not homeomorphic we need to find a property that holds in $S^1$ but does not hold in $\mathbb{R}^1$ or vice-versa. $S^1$ is compact however $\mathbb{R}^1$ is not compact. The set $\{1\} $ is closed, and the map $$f: \Bbb R^2 \longrightarrow \Bbb R,$$ $$(x, y) \mapsto x^2 + y^2$$ is continuous. Therefore the circle $$\{(x,y) \in \Bbb R^2 : x^2 + y^2 = 1\} = f^{-1}(\{1\})$$ is closed in $\Bbb R^2$. Set $S^1$ is also bounded, since, for example, it is contained within the ball of radius $2$ centered at 0 of $\Bbb R^2$ (in the standard topology of $\Bbb R^2$). Hence it is also compact. However real line $\Bbb R^1$ is not because there is a cover of open intervals that does not have a finite subcover. For example, intervals (n−1, n+1) , where n takes all integer values in $\mathbb{Z}$, cover $\mathbb{R}$ but there is no finite subcover. Hence $S^1$ can not be isomorphic to $\mathbb{R}^1$. How to show now that $S^1$ is not homeomorphic to $\mathbb{R}^2$? Can i show it now in the same way? They can not be homeomorphic since $S^1$ is compact however $\mathbb{R}^2$ not. How to show that $\mathbb{R}^2$ is not compact?","Show that $S^1$ is not homeomorphic to either $\mathbb{R}^1$ or $\mathbb{R}^2$ $\mathbf{My \ solution}$: So first we will show that $S^1$ is not homeomorphic to $\mathbb{R}^1$. To show that they are not homeomorphic we need to find a property that holds in $S^1$ but does not hold in $\mathbb{R}^1$ or vice-versa. $S^1$ is compact however $\mathbb{R}^1$ is not compact. The set $\{1\} $ is closed, and the map $$f: \Bbb R^2 \longrightarrow \Bbb R,$$ $$(x, y) \mapsto x^2 + y^2$$ is continuous. Therefore the circle $$\{(x,y) \in \Bbb R^2 : x^2 + y^2 = 1\} = f^{-1}(\{1\})$$ is closed in $\Bbb R^2$. Set $S^1$ is also bounded, since, for example, it is contained within the ball of radius $2$ centered at 0 of $\Bbb R^2$ (in the standard topology of $\Bbb R^2$). Hence it is also compact. However real line $\Bbb R^1$ is not because there is a cover of open intervals that does not have a finite subcover. For example, intervals (n−1, n+1) , where n takes all integer values in $\mathbb{Z}$, cover $\mathbb{R}$ but there is no finite subcover. Hence $S^1$ can not be isomorphic to $\mathbb{R}^1$. How to show now that $S^1$ is not homeomorphic to $\mathbb{R}^2$? Can i show it now in the same way? They can not be homeomorphic since $S^1$ is compact however $\mathbb{R}^2$ not. How to show that $\mathbb{R}^2$ is not compact?",,['real-analysis']
18,How to evaluate $\int_{0}^{\infty} \int_{0}^{\infty} \frac{\sin(x) \sin(y) \sin(x+y)}{x y(x+y)} ~{\rm d}x ~{\rm d}y$?,How to evaluate ?,\int_{0}^{\infty} \int_{0}^{\infty} \frac{\sin(x) \sin(y) \sin(x+y)}{x y(x+y)} ~{\rm d}x ~{\rm d}y,"I came across the problem of evaluating the double integral: $$\int_{0}^{\infty} \int_{0}^{\infty} \frac{\sin(x) \sin(y) \sin(x+y)}{x y(x+y)} ~{\rm d}x ~{\rm d}y$$ My attempt at this was to substitute $\sin(x) \cos(y) + \cos(x) \sin(y)$ for $\sin(x+y)$, although I had no clue where to go from there. My second attempt was to use trigonometric identities to rewrite the integral as: $$\frac{1}{4} \int_{0}^{\infty} \int_{0}^{\infty} \frac{\sin(2x) + \sin(2y) - \sin(2x + 2y)}{x^2 y + x y^2} ~{\rm d}x ~{\rm d}y$$ Which didn't seem to help, so I continued to get: $$\frac{1}{2} \int_{0}^{\infty} \int_{0}^{\infty} \frac{\sin(2x) \sin^2 (y) + \sin(2y) \sin^2 (x)}{x^2 y + x y^2} ~{\rm d}x ~{\rm d}y$$ I am sure I am missing something, but I am unsure how to integrate this. Any help or advice is appreciated.","I came across the problem of evaluating the double integral: $$\int_{0}^{\infty} \int_{0}^{\infty} \frac{\sin(x) \sin(y) \sin(x+y)}{x y(x+y)} ~{\rm d}x ~{\rm d}y$$ My attempt at this was to substitute $\sin(x) \cos(y) + \cos(x) \sin(y)$ for $\sin(x+y)$, although I had no clue where to go from there. My second attempt was to use trigonometric identities to rewrite the integral as: $$\frac{1}{4} \int_{0}^{\infty} \int_{0}^{\infty} \frac{\sin(2x) + \sin(2y) - \sin(2x + 2y)}{x^2 y + x y^2} ~{\rm d}x ~{\rm d}y$$ Which didn't seem to help, so I continued to get: $$\frac{1}{2} \int_{0}^{\infty} \int_{0}^{\infty} \frac{\sin(2x) \sin^2 (y) + \sin(2y) \sin^2 (x)}{x^2 y + x y^2} ~{\rm d}x ~{\rm d}y$$ I am sure I am missing something, but I am unsure how to integrate this. Any help or advice is appreciated.",,"['real-analysis', 'integration', 'multivariable-calculus', 'improper-integrals', 'multiple-integral']"
19,Why does the commutative property of addition not hold for conditionally convergent series?,Why does the commutative property of addition not hold for conditionally convergent series?,,"I learned about the Riemann rearrangement theorem recently and I'm trying to develop an intuition as to why commutativity breaks down for conditionally convergent series. I understand the technique used in the theorem, but it just seems really odd that commutativity breaks down to me. It doesn't happen for other properties -- associativity holds for convergent series and commutativity holds for absolutely convergent series. What makes this particular property for this particular kind of convergent series ""special"" in this way? I'm aware of this question: why does commutativity of addition fail for infinite sums? but the answers haven't been helpful for me. JiK's answer is ""why would it?"", and goes on to talk about why you can't apply rules to infinite series, but this seems erroneous because associativity holds for convergent series and commutativity holds for absolutely convergent series. Fly by Night just explains conditionally vs absolutely convergent, josh314 says it applies to finite sums only which isn't true, and Denis just explains the theorem again, and Barry Cipra seems to have a similar kind of argument as JiK's and problematic for similar reasons. Is there a good way to understand why this is happening intuitively? or is this the wrong way to think about it and I should just accept that it's happening even though it's unintuitive? It's hard for me to just let it go without an intution because it seems like math starts to ""break"" here.. the theorem is sound but this property no longer holds in this case, which is really strange to me Does anyone know of any resources that go into depth on this kind of question?","I learned about the Riemann rearrangement theorem recently and I'm trying to develop an intuition as to why commutativity breaks down for conditionally convergent series. I understand the technique used in the theorem, but it just seems really odd that commutativity breaks down to me. It doesn't happen for other properties -- associativity holds for convergent series and commutativity holds for absolutely convergent series. What makes this particular property for this particular kind of convergent series ""special"" in this way? I'm aware of this question: why does commutativity of addition fail for infinite sums? but the answers haven't been helpful for me. JiK's answer is ""why would it?"", and goes on to talk about why you can't apply rules to infinite series, but this seems erroneous because associativity holds for convergent series and commutativity holds for absolutely convergent series. Fly by Night just explains conditionally vs absolutely convergent, josh314 says it applies to finite sums only which isn't true, and Denis just explains the theorem again, and Barry Cipra seems to have a similar kind of argument as JiK's and problematic for similar reasons. Is there a good way to understand why this is happening intuitively? or is this the wrong way to think about it and I should just accept that it's happening even though it's unintuitive? It's hard for me to just let it go without an intution because it seems like math starts to ""break"" here.. the theorem is sound but this property no longer holds in this case, which is really strange to me Does anyone know of any resources that go into depth on this kind of question?",,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'conditional-convergence']"
20,Integral over exponential involving reciprocial,Integral over exponential involving reciprocial,,"I want to show $$I := \int_{-\infty}^\infty \exp \left(-\left(x-\frac p x \right)^2\right) \, dx = \sqrt{\pi}$$ for any non-negative $p\geq 0$. I tried to prove $I^2=\pi$ using Fubini's theorem, but had no success.","I want to show $$I := \int_{-\infty}^\infty \exp \left(-\left(x-\frac p x \right)^2\right) \, dx = \sqrt{\pi}$$ for any non-negative $p\geq 0$. I tried to prove $I^2=\pi$ using Fubini's theorem, but had no success.",,"['calculus', 'real-analysis', 'integration']"
21,Prove compact subsets of metric spaces are closed,Prove compact subsets of metric spaces are closed,,"Prove compact subsets of metric spaces are closed Note, this question is more of analyzing an incorrect proof of mine rather than supplying a correct proof. My Attempted Proof Suppose $X$ is a metric space. Let $A \subset X$ be a compact subset of $X$ and let $\{V_{\alpha}\}$ be an open cover of $A$. Then there are finitely many indices $\alpha_{i}$ such that $A \subset V_{\alpha_{1}} \cup \ ... \ \cup V_{\alpha_{n}}$. Now let $x$ be a limit point of $A$. Assume $x \not\in A$. If $x \not\in A$ put $\delta = \inf \ \{\ d(x, y) \  | \ y \in A\}$. Take $\epsilon = \frac{\delta}{2}$, then $B_d(x, \epsilon) \cap A = \emptyset$ so that a neighbourhood of $x$ does not intersect $A$ asserting that $x$ cannot be a limit point of $A$, hence $x \in A$ so that $A$ is closed. $\square$. Now there must be something critically wrong in my proof, as I don't even use the condition that $A$ is compact anywhere in the contradiction that I establish. The above proof would assert that every subset of a metric space is closed . I think my error must be in the following argument : $\delta = \inf \ \{\ d(x, y) \  | \ y \in A\}$. For if we take $X = \mathbb{R}$ and $A = (0, 1) \subset \mathbb{R}$, then $\delta = 0$ if $x = 1$ or $x = 0$. Am I correct in analyzing this aspect of my proof?","Prove compact subsets of metric spaces are closed Note, this question is more of analyzing an incorrect proof of mine rather than supplying a correct proof. My Attempted Proof Suppose $X$ is a metric space. Let $A \subset X$ be a compact subset of $X$ and let $\{V_{\alpha}\}$ be an open cover of $A$. Then there are finitely many indices $\alpha_{i}$ such that $A \subset V_{\alpha_{1}} \cup \ ... \ \cup V_{\alpha_{n}}$. Now let $x$ be a limit point of $A$. Assume $x \not\in A$. If $x \not\in A$ put $\delta = \inf \ \{\ d(x, y) \  | \ y \in A\}$. Take $\epsilon = \frac{\delta}{2}$, then $B_d(x, \epsilon) \cap A = \emptyset$ so that a neighbourhood of $x$ does not intersect $A$ asserting that $x$ cannot be a limit point of $A$, hence $x \in A$ so that $A$ is closed. $\square$. Now there must be something critically wrong in my proof, as I don't even use the condition that $A$ is compact anywhere in the contradiction that I establish. The above proof would assert that every subset of a metric space is closed . I think my error must be in the following argument : $\delta = \inf \ \{\ d(x, y) \  | \ y \in A\}$. For if we take $X = \mathbb{R}$ and $A = (0, 1) \subset \mathbb{R}$, then $\delta = 0$ if $x = 1$ or $x = 0$. Am I correct in analyzing this aspect of my proof?",,"['real-analysis', 'general-topology', 'proof-verification', 'metric-spaces', 'proof-writing']"
22,Example for non-Riemann integrable functions,Example for non-Riemann integrable functions,,"According to Rudin (Principles of Mathematical Analysis) Riemann integrable functions are defined for bounded functions.For every bounded function defined on a closed interval $[a,b]$ Lower Riemann Sum and Upper Riemann sum are bounded .More mathematically $m(b-a) \leq L(P,f) \leq U(P,f) \leq M(b-a)$ where $m,M$ are lower and upper bounds of the function $f$ respectively. Rudin says that Upper Riemann Sum and Lower Riemann sum always exists,but their equality is the question. But while searching for non-examples we need to find a bounded function whose upper sum not equal to lower sum.One of the book is given example as $\frac{1}{x}$ in the interval $[0,b]$. But this function is not bounded. Can we use $\sin(\frac{1}{x})$ in the interval $[0,1]$. Explain how?","According to Rudin (Principles of Mathematical Analysis) Riemann integrable functions are defined for bounded functions.For every bounded function defined on a closed interval $[a,b]$ Lower Riemann Sum and Upper Riemann sum are bounded .More mathematically $m(b-a) \leq L(P,f) \leq U(P,f) \leq M(b-a)$ where $m,M$ are lower and upper bounds of the function $f$ respectively. Rudin says that Upper Riemann Sum and Lower Riemann sum always exists,but their equality is the question. But while searching for non-examples we need to find a bounded function whose upper sum not equal to lower sum.One of the book is given example as $\frac{1}{x}$ in the interval $[0,b]$. But this function is not bounded. Can we use $\sin(\frac{1}{x})$ in the interval $[0,1]$. Explain how?",,"['real-analysis', 'integration', 'riemannian-geometry', 'examples-counterexamples']"
23,What is set of measure zero? [closed],What is set of measure zero? [closed],,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 10 years ago . Improve this question What is the concept of set of measure zero? Please explain it in a easy language. Thank you.,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 10 years ago . Improve this question What is the concept of set of measure zero? Please explain it in a easy language. Thank you.,,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
24,A compact operator in $L^2(\mathbb R)$,A compact operator in,L^2(\mathbb R),"Let $g \in L^{\infty}(\mathbb R)$. Consider the operator    $$ \begin{split} T_g\colon & L^2(\mathbb R)\to L^2(\mathbb R) \\ & f \mapsto gf \end{split} $$   Prove that $T_g$ is compact (i.e., the image under $T_g$ of bounded closed sets is compact) if and only if $g=0$ a.e. I do not know how to start and I'm very puzzled. I know very little about compactness in $L^p$: of course they are complete metric spaces, therefore a subspace is compact if and only if it is closed (complete) and totally bounded. A singleton is of course totally bounded and I think it is closed: therefore I can say that if $g=0$ a.e. then the image of every subspace is $\{0\}$ which is compact, so the operator is compact. What about the inverse direction? It seems hard to prove.  Would you help me, please? Thanks.","Let $g \in L^{\infty}(\mathbb R)$. Consider the operator    $$ \begin{split} T_g\colon & L^2(\mathbb R)\to L^2(\mathbb R) \\ & f \mapsto gf \end{split} $$   Prove that $T_g$ is compact (i.e., the image under $T_g$ of bounded closed sets is compact) if and only if $g=0$ a.e. I do not know how to start and I'm very puzzled. I know very little about compactness in $L^p$: of course they are complete metric spaces, therefore a subspace is compact if and only if it is closed (complete) and totally bounded. A singleton is of course totally bounded and I think it is closed: therefore I can say that if $g=0$ a.e. then the image of every subspace is $\{0\}$ which is compact, so the operator is compact. What about the inverse direction? It seems hard to prove.  Would you help me, please? Thanks.",,"['real-analysis', 'functional-analysis', 'operator-theory', 'compact-operators']"
25,Weaker version of Cauchy sequence criteria [duplicate],Weaker version of Cauchy sequence criteria [duplicate],,"This question already has answers here : Closed 12 years ago . Possible Duplicate: Why doesn't $d(x_n,x_{n+1})\rightarrow 0$ as $n\rightarrow\infty$ imply ${x_n}$ is Cauchy? my question is this: The following definition is weaker than the definition of Cauchy sequences: $\forall \; \epsilon > 0, \;\exists N \in \mathbb{N} \;s.t.\; \forall\; n \geq N, \; |a_{n+1}-a_n | < \epsilon.$ Show that this is not equivalent to $(a_n)$ being a Cauchy sequence. The definition of Cauchy sequence is: A sequence $(s_n)$ is Cauchy if (and only if) for each $\epsilon > 0$ there exists an integer $N$ with the property that $|s_n-s_m| < \epsilon$ whenever $n\geq N$ and $m \geq N$. Note that a sequence (of real numbers) is convergent if and only if it is Cauchy. So I see an (the?) obvious difference between these two in that the Cauchy criteria demands that all values in a sequence above a certain index ($N$) are within a prescribed tolerance of each other, whether adjacent or not. This is where the question is weaker, in that it only requires the immediately adjacent values of the sequence to be within a tolerance of $\epsilon$. This would then allow, by taking successive differences of adjacent values, to accumulate a difference greater than $\epsilon$. This is seen as, $$\left| \sum_{i=1}^{n+1}\,a_i - \sum_{i=1}^{n}\,a_i\right| < \epsilon,\quad \left|\sum_{i=1}^{n+2}\,a_i - \sum_{i=1}^{n+1}\,a_i\right| < \epsilon,\quad \left| \sum_{i=1}^{n+3}\,a_i - \sum_{i=1}^{n+2}\,a_i\right| < \epsilon,$$ and summing each side of the inequalities gives (after reverting to sequence-notation and employing the triangle inequality), $$ \left(|a_{n+1}-a_n| + |a_{n+2}-a_{n+1}| + |a_{n+3}-a_{n+2}| + \cdots + |a_{K} - a_{K-1}| \right) \leq \left( \epsilon_{1,2} + \epsilon_{2,3} + \epsilon_{3,4} + \cdots + \epsilon_{K-1,K} \right) $$ which implies $$\left( \epsilon_{1,2} + \epsilon_{2,3} + \epsilon_{3,4} + \cdots + \epsilon_{K-1,K} \right)_{\textrm{ by weaker criteria }}   \geq |a_n - a_{n+K}|_{\textrm{ by Cauchy criteria }}$$ If I understand these differences correctly, then my main problem is putting these into a formal mathematical proof. Unless this would qualify? Thanks much for the help and the site!","This question already has answers here : Closed 12 years ago . Possible Duplicate: Why doesn't $d(x_n,x_{n+1})\rightarrow 0$ as $n\rightarrow\infty$ imply ${x_n}$ is Cauchy? my question is this: The following definition is weaker than the definition of Cauchy sequences: $\forall \; \epsilon > 0, \;\exists N \in \mathbb{N} \;s.t.\; \forall\; n \geq N, \; |a_{n+1}-a_n | < \epsilon.$ Show that this is not equivalent to $(a_n)$ being a Cauchy sequence. The definition of Cauchy sequence is: A sequence $(s_n)$ is Cauchy if (and only if) for each $\epsilon > 0$ there exists an integer $N$ with the property that $|s_n-s_m| < \epsilon$ whenever $n\geq N$ and $m \geq N$. Note that a sequence (of real numbers) is convergent if and only if it is Cauchy. So I see an (the?) obvious difference between these two in that the Cauchy criteria demands that all values in a sequence above a certain index ($N$) are within a prescribed tolerance of each other, whether adjacent or not. This is where the question is weaker, in that it only requires the immediately adjacent values of the sequence to be within a tolerance of $\epsilon$. This would then allow, by taking successive differences of adjacent values, to accumulate a difference greater than $\epsilon$. This is seen as, $$\left| \sum_{i=1}^{n+1}\,a_i - \sum_{i=1}^{n}\,a_i\right| < \epsilon,\quad \left|\sum_{i=1}^{n+2}\,a_i - \sum_{i=1}^{n+1}\,a_i\right| < \epsilon,\quad \left| \sum_{i=1}^{n+3}\,a_i - \sum_{i=1}^{n+2}\,a_i\right| < \epsilon,$$ and summing each side of the inequalities gives (after reverting to sequence-notation and employing the triangle inequality), $$ \left(|a_{n+1}-a_n| + |a_{n+2}-a_{n+1}| + |a_{n+3}-a_{n+2}| + \cdots + |a_{K} - a_{K-1}| \right) \leq \left( \epsilon_{1,2} + \epsilon_{2,3} + \epsilon_{3,4} + \cdots + \epsilon_{K-1,K} \right) $$ which implies $$\left( \epsilon_{1,2} + \epsilon_{2,3} + \epsilon_{3,4} + \cdots + \epsilon_{K-1,K} \right)_{\textrm{ by weaker criteria }}   \geq |a_n - a_{n+K}|_{\textrm{ by Cauchy criteria }}$$ If I understand these differences correctly, then my main problem is putting these into a formal mathematical proof. Unless this would qualify? Thanks much for the help and the site!",,"['real-analysis', 'sequences-and-series']"
26,Continuity of a function at an isolated point,Continuity of a function at an isolated point,,"Suppose $c$ is an isolated point in the domain $D$ of a function $f$. In the delta neighbourhood of $c$, does the function $f$ have the value $f(c)$?","Suppose $c$ is an isolated point in the domain $D$ of a function $f$. In the delta neighbourhood of $c$, does the function $f$ have the value $f(c)$?",,['real-analysis']
27,"If $0<A\le x_n\le B~~ (\forall n)$, and $\lim_{n\to\infty}\sqrt[n]{x_1x_2\cdots x_n}=A$. Prove $\lim_{n\to\infty}\frac{x_1+x_2+\cdots+x_n}{n}=A.$","If , and . Prove",0<A\le x_n\le B~~ (\forall n) \lim_{n\to\infty}\sqrt[n]{x_1x_2\cdots x_n}=A \lim_{n\to\infty}\frac{x_1+x_2+\cdots+x_n}{n}=A.,"Problem Assume $ 0<A \le x_n\le B~~ (\forall n)$ , and $\displaystyle \lim_{n \to \infty}\sqrt[n]{x_1x_2\cdots x_n}=A$ . Prove $$\displaystyle\lim_{n \to \infty}\frac{x_1+x_2+\cdots+x_n}{n}=A.$$ Is it true or not ? Probably it holds, but seems to be difficult to prove. If we consider using the squeezing theorem, then $$\sqrt[n]{x_1x_2\cdots x_n}\le \frac{x_1+x_2+\cdots+x_n}{n},$$ but how to estimate the upper bound?","Problem Assume , and . Prove Is it true or not ? Probably it holds, but seems to be difficult to prove. If we consider using the squeezing theorem, then but how to estimate the upper bound?"," 0<A \le x_n\le B~~ (\forall n) \displaystyle
\lim_{n \to \infty}\sqrt[n]{x_1x_2\cdots x_n}=A \displaystyle\lim_{n \to \infty}\frac{x_1+x_2+\cdots+x_n}{n}=A. \sqrt[n]{x_1x_2\cdots x_n}\le \frac{x_1+x_2+\cdots+x_n}{n},","['real-analysis', 'calculus', 'limits']"
28,Are there reasons to prefer one definition of the exponential function over the other?,Are there reasons to prefer one definition of the exponential function over the other?,,"This question is motivated by curiosity and  I haven't much background to exhibit . Going through a couple of books dealing with real analysis, I've noticed that 2 definitions can be given of the exponential function known in algebra as $f(x)= e^x$ . One definition says : The exponential function is the unique function defined on $\mathbb R$ such that $f(0)=1$ and $\forall (x) [   f'(x)= f(x)] $ . The other one defines the exponential function as the inverse of the natural logarithm function . More precisely $(1)$ $\exp_a (x)$ is defined as the inverse of $\log_a (x)$ , $(2)$ then , $\exp_a (x)$ is shown to be identical to $a^x$ , and finally $(3)$ every function of the form : $a^x$ is shown to be a "" special case"" of the $e^x$ function. My question : (1) Do these definitions exhaust the ways the exponential function can be defined? (2) Are these definitions actually different at least conceptually  ( though denoting in fact the same object)? (3) Is there a reason to prefer one definition over the other? What is each definition good for?","This question is motivated by curiosity and  I haven't much background to exhibit . Going through a couple of books dealing with real analysis, I've noticed that 2 definitions can be given of the exponential function known in algebra as . One definition says : The exponential function is the unique function defined on such that and . The other one defines the exponential function as the inverse of the natural logarithm function . More precisely is defined as the inverse of , then , is shown to be identical to , and finally every function of the form : is shown to be a "" special case"" of the function. My question : (1) Do these definitions exhaust the ways the exponential function can be defined? (2) Are these definitions actually different at least conceptually  ( though denoting in fact the same object)? (3) Is there a reason to prefer one definition over the other? What is each definition good for?",f(x)= e^x \mathbb R f(0)=1 \forall (x) [   f'(x)= f(x)]  (1) \exp_a (x) \log_a (x) (2) \exp_a (x) a^x (3) a^x e^x,"['real-analysis', 'calculus', 'exponential-function', 'definition']"
29,"Checking whether $\int_{B(0,r)} \int_{B(0,r)} \ln(\|x-y\|)dxdy > 0$?",Checking whether ?,"\int_{B(0,r)} \int_{B(0,r)} \ln(\|x-y\|)dxdy > 0","Today I was discussing with a classmate about the sign of the integral $$\int_{B(0,r)} \int_{B(0,r)} \ln(\|x-y\|)dxdy,$$ where $B(0,r)$ denotes the ball of center $0$ and radius $r$ in $\mathbb{R^2}$ . My friend said that this integral is negative for every $r>0$ because the function $\ln |x-y|$ is ""very negative"" at $x=y$ . I don't agree and I told him that I think there exists a critical $r$ from which the integral is positive. However, I don't know how to prove it. I rewrite it by using polar coordinates as $$\int_0^r\int_0^r\int_0^{2\pi}\int_0^{2\pi} \ln(\sqrt{r_1^2+r_2^2-2r_1r_2\cos(t_1-t_2)})r_1r_2dt_1dt_2dr_1dr_2.$$ I computed this integral with the software Mathematica and I obtained positive values with, for example, $r=2$ . However, this proof is not valid for my friend. Does anyone know how to prove it rigorously?","Today I was discussing with a classmate about the sign of the integral where denotes the ball of center and radius in . My friend said that this integral is negative for every because the function is ""very negative"" at . I don't agree and I told him that I think there exists a critical from which the integral is positive. However, I don't know how to prove it. I rewrite it by using polar coordinates as I computed this integral with the software Mathematica and I obtained positive values with, for example, . However, this proof is not valid for my friend. Does anyone know how to prove it rigorously?","\int_{B(0,r)} \int_{B(0,r)} \ln(\|x-y\|)dxdy, B(0,r) 0 r \mathbb{R^2} r>0 \ln |x-y| x=y r \int_0^r\int_0^r\int_0^{2\pi}\int_0^{2\pi} \ln(\sqrt{r_1^2+r_2^2-2r_1r_2\cos(t_1-t_2)})r_1r_2dt_1dt_2dr_1dr_2. r=2","['real-analysis', 'integration', 'multivariable-calculus', 'inequality', 'definite-integrals']"
30,"Fixed point of a monotone on [0,1].","Fixed point of a monotone on [0,1].",,"prove or disprove: Let $f:[0,1]\rightarrow[0,1]$ be a monotone (need not be strict) function then  f has a fixed point. Can I have a hint?","prove or disprove: Let $f:[0,1]\rightarrow[0,1]$ be a monotone (need not be strict) function then  f has a fixed point. Can I have a hint?",,['real-analysis']
31,Spivak or Apostol to prepare for learning Baby Rudin on my own?,Spivak or Apostol to prepare for learning Baby Rudin on my own?,,"I wish to learn real analysis on my own using the first 7 chapters from Baby Rudin. My goal is to have a good math background so as to study from PhD-level books in economics and finance on my own (e.g, stochastic calculus in finance, optimization, etc.). As an economics student, I find this kind of learning really challenging, so I appreciate it very much if you could give me some good suggestions/tips. I have the following: 1) Apostol's single-variable calculus book without solutions manual 2) Spivak's single-variable calculus book + solutions manual 3) Time constraints and a rusty knowledlege of calculus (single-variable/multivariable calculus/LA) If I have roughly 1-2 years ahead, I have to work full-time, and I have to choose only one of these 2 books, then which calculus book is better for self-study and for preparing for the first 7 chapters from Baby Rudin? Thanks much","I wish to learn real analysis on my own using the first 7 chapters from Baby Rudin. My goal is to have a good math background so as to study from PhD-level books in economics and finance on my own (e.g, stochastic calculus in finance, optimization, etc.). As an economics student, I find this kind of learning really challenging, so I appreciate it very much if you could give me some good suggestions/tips. I have the following: 1) Apostol's single-variable calculus book without solutions manual 2) Spivak's single-variable calculus book + solutions manual 3) Time constraints and a rusty knowledlege of calculus (single-variable/multivariable calculus/LA) If I have roughly 1-2 years ahead, I have to work full-time, and I have to choose only one of these 2 books, then which calculus book is better for self-study and for preparing for the first 7 chapters from Baby Rudin? Thanks much",,"['calculus', 'real-analysis']"
32,Why is adding the same as extending a length?,Why is adding the same as extending a length?,,"I've come to realize that the more I study some math subjects the more I question some results or ideas that seemed trivial or obvious to me. My question is about the real numbers and their geometric interpretation as a line. I get that real numbers being complete and ordered are naturally visualized as a number line. Now, consider two positive numbers $a$ and $b$ and find them in the number line. We define the binary operation $a•b$ like this: ""take a compass and open it from O to $b$, now draw a circle with such compass with $a$ as its center. The point on the right where the circle intersects the line is $a•b$"". This, of course, corresponds to $a+b$ but the reason is not that obvious to me. Intuitively we use addition when we ""add"" something so if I'm ""adding"" a line segment with length $a$ to another line segment with length $b$ the length of the resulting line must be $a+b$. But with a formal definition of addition (via Dedekind cuts for example) that property of addition doesn't seem that clear to me. I guess my question is: why must addition correspond to $a•b$? Why, if I have a line segment of length 3/2 and other one of length 5/3 can I be sure that if I arrange them so that one starts exactly when the other one ends, the length of the resulting line will be 19/6? Please forgive me if this question is too obvious or trivial. Thank you!","I've come to realize that the more I study some math subjects the more I question some results or ideas that seemed trivial or obvious to me. My question is about the real numbers and their geometric interpretation as a line. I get that real numbers being complete and ordered are naturally visualized as a number line. Now, consider two positive numbers $a$ and $b$ and find them in the number line. We define the binary operation $a•b$ like this: ""take a compass and open it from O to $b$, now draw a circle with such compass with $a$ as its center. The point on the right where the circle intersects the line is $a•b$"". This, of course, corresponds to $a+b$ but the reason is not that obvious to me. Intuitively we use addition when we ""add"" something so if I'm ""adding"" a line segment with length $a$ to another line segment with length $b$ the length of the resulting line must be $a+b$. But with a formal definition of addition (via Dedekind cuts for example) that property of addition doesn't seem that clear to me. I guess my question is: why must addition correspond to $a•b$? Why, if I have a line segment of length 3/2 and other one of length 5/3 can I be sure that if I arrange them so that one starts exactly when the other one ends, the length of the resulting line will be 19/6? Please forgive me if this question is too obvious or trivial. Thank you!",,"['real-analysis', 'geometry', 'analytic-geometry', 'real-numbers', 'geometric-construction']"
33,How to evaluate this monsterous integral?,How to evaluate this monsterous integral?,,How to prove this? $$\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}  \frac {2(x-x^2)e^{-(x^2+y^2+z^2)}}{((x-1)^2+(y-1)^2+(z-1)^2)^{3/2}}dxdydz=\frac {-4\pi e^{-3}}{3}$$ I tried converting to polar coordinates but still I couldn't evaluate.,How to prove this? $$\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}  \frac {2(x-x^2)e^{-(x^2+y^2+z^2)}}{((x-1)^2+(y-1)^2+(z-1)^2)^{3/2}}dxdydz=\frac {-4\pi e^{-3}}{3}$$ I tried converting to polar coordinates but still I couldn't evaluate.,,"['real-analysis', 'integration', 'definite-integrals', 'indefinite-integrals']"
34,Limit of definite integral of $f(x)\cos(mx)$,Limit of definite integral of,f(x)\cos(mx),"Source: Old comp./preliminary exam. Let $f(x)$ be a Riemann integrable function on $[0,1]$. Prove that $$\lim_{m\to\infty}\int_{0}^{1}f(x)\cos(mx) \, \,dx=0$$ Thought $(1)$ Because we don't know if $f(x)$ is differentiable, we can only use integration by part by setting $du=f(x)\, \,dx$ and $v=\cos(mx)$, the result is not quite helpful. $(2)$ We don't know whether $f(x)\cos(mx)$ converges as $m\rightarrow \infty$ so those convergence theorems cannot be applied. Side note: This exam assume no knowledge in measure theory and Lebesgue integral.","Source: Old comp./preliminary exam. Let $f(x)$ be a Riemann integrable function on $[0,1]$. Prove that $$\lim_{m\to\infty}\int_{0}^{1}f(x)\cos(mx) \, \,dx=0$$ Thought $(1)$ Because we don't know if $f(x)$ is differentiable, we can only use integration by part by setting $du=f(x)\, \,dx$ and $v=\cos(mx)$, the result is not quite helpful. $(2)$ We don't know whether $f(x)\cos(mx)$ converges as $m\rightarrow \infty$ so those convergence theorems cannot be applied. Side note: This exam assume no knowledge in measure theory and Lebesgue integral.",,"['calculus', 'real-analysis']"
35,Convergence of sequence: $f(n+1) = f(n) + \frac{f(n)^2}{n(n+1)}$,Convergence of sequence:,f(n+1) = f(n) + \frac{f(n)^2}{n(n+1)},"I am trying to work out a problem on some old analysis qual exam, and managed to reduce it to this question, but I can't seem to figure out this final step: Consider the sequence defined by the recurrence relation $$f(n+1) = f(n) + \frac{f(n)^2}{n(n+1)}$$ with initial condition $f(1) = 1$. Does this sequence converge or diverge?","I am trying to work out a problem on some old analysis qual exam, and managed to reduce it to this question, but I can't seem to figure out this final step: Consider the sequence defined by the recurrence relation $$f(n+1) = f(n) + \frac{f(n)^2}{n(n+1)}$$ with initial condition $f(1) = 1$. Does this sequence converge or diverge?",,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'recurrence-relations']"
36,Spectral theory - continuous spectrum,Spectral theory - continuous spectrum,,"imagine that I have some differential operator $D$ that is defined on an interval $[a,b]$. Now, assume that we take the boundary conditions in such a way that this operator is self-adjoint. Then, I have a few questions about this: 1.) Is it true that this operator cannot have a residual spectrum due to its self-adjointness. 2.) Where is the difference between continuous spectrum and a classical solution? Probably, I need to specify question (ii). The classical theory of ODEs tells me that for any eigenvalue $\lambda $ I will find two independet solutions that solve $D f = \lambda f$. The thing is of course, that these two solutions do not satisfy the boundary conditions of the operator. But what happens when I have a continuous spectrum? Will I find eigenvalues ( I heard that one gets a whole interval of eigenvalues, is this true?) ? Will I get solutions to my eigenvalue that satisfy the boundary conditions? I think I still did not understand what I get in the case of a continuous spectrum. Unfortunately, I am not a mathematician, so I am not so good at teaching this to myself from books in funtional analysis, so I would very much appreciate any assistance.","imagine that I have some differential operator $D$ that is defined on an interval $[a,b]$. Now, assume that we take the boundary conditions in such a way that this operator is self-adjoint. Then, I have a few questions about this: 1.) Is it true that this operator cannot have a residual spectrum due to its self-adjointness. 2.) Where is the difference between continuous spectrum and a classical solution? Probably, I need to specify question (ii). The classical theory of ODEs tells me that for any eigenvalue $\lambda $ I will find two independet solutions that solve $D f = \lambda f$. The thing is of course, that these two solutions do not satisfy the boundary conditions of the operator. But what happens when I have a continuous spectrum? Will I find eigenvalues ( I heard that one gets a whole interval of eigenvalues, is this true?) ? Will I get solutions to my eigenvalue that satisfy the boundary conditions? I think I still did not understand what I get in the case of a continuous spectrum. Unfortunately, I am not a mathematician, so I am not so good at teaching this to myself from books in funtional analysis, so I would very much appreciate any assistance.",,"['real-analysis', 'analysis']"
37,Subspaces of $L^p$,Subspaces of,L^p,"So studying Qualifying Exam problems in Analysis I cam across this one: For $1\lt r \lt p \lt s \lt \infty$  where $\mu$ denotes Lebesgue measure, a) Construct a subspace of $L^p([0,1],\mu)$  such that $ \forall r$ the subspace is dense in $L^r([0,1],\mu)$ but not $L^p$. b) Construct a subspace of $L^\infty([0,1,\mu)$ such that $ \forall s$ the subspace is dense in $L^p$ but not $L^s$. Now $L^s\subset L^p \subset L^r$ since $\mu([0,1])\lt\infty$, so the issue is recognizing the norms are not equivalent for this to be possible.  I'm just not terribly familiar with subspaces of $L^p$ for $p\neq2$.  Is this just a matter of better knowing $L^p$ spaces or is there something bigger that I'm missing?","So studying Qualifying Exam problems in Analysis I cam across this one: For $1\lt r \lt p \lt s \lt \infty$  where $\mu$ denotes Lebesgue measure, a) Construct a subspace of $L^p([0,1],\mu)$  such that $ \forall r$ the subspace is dense in $L^r([0,1],\mu)$ but not $L^p$. b) Construct a subspace of $L^\infty([0,1,\mu)$ such that $ \forall s$ the subspace is dense in $L^p$ but not $L^s$. Now $L^s\subset L^p \subset L^r$ since $\mu([0,1])\lt\infty$, so the issue is recognizing the norms are not equivalent for this to be possible.  I'm just not terribly familiar with subspaces of $L^p$ for $p\neq2$.  Is this just a matter of better knowing $L^p$ spaces or is there something bigger that I'm missing?",,"['real-analysis', 'functional-analysis', 'measure-theory', 'lp-spaces']"
38,"Suppose that $f(x)$ is continuous on $(0, \infty)$ such that for all $x > 0$,$f(x^2) = f(x)$. Prove that $f$ is a constant function.","Suppose that  is continuous on  such that for all ,. Prove that  is a constant function.","f(x) (0, \infty) x > 0 f(x^2) = f(x) f","Suppose that $f(x)$ is continuous on $(0, +\infty)$ such that for all $x > 0$,$f(x^2) = f(x)$. Prove that $f$ is a constant function. My attempt is to show that for any point $a \neq b$ , we have $f(a)=f(b)$. But I have no idea on how to get this. Anyone can help?","Suppose that $f(x)$ is continuous on $(0, +\infty)$ such that for all $x > 0$,$f(x^2) = f(x)$. Prove that $f$ is a constant function. My attempt is to show that for any point $a \neq b$ , we have $f(a)=f(b)$. But I have no idea on how to get this. Anyone can help?",,['real-analysis']
39,Proving that $x(1-e^{-1/x})$ is strictly increasing,Proving that  is strictly increasing,x(1-e^{-1/x}),"Prove that the function below is strictly increasing $$f(x)=x(1-e^{-1/x}), \quad x>0$$","Prove that the function below is strictly increasing $$f(x)=x(1-e^{-1/x}), \quad x>0$$",,"['calculus', 'real-analysis', 'contest-math']"
40,Field bigger than $\mathbb{R}$,Field bigger than,\mathbb{R},Is there any field containing $\mathbb{R}$ for which every non-empty subset has an infimum and a supremum in that field? I am trying to understand whether $\overline{\mathbb{R}}$ (which is not a field) is the best possible.,Is there any field containing $\mathbb{R}$ for which every non-empty subset has an infimum and a supremum in that field? I am trying to understand whether $\overline{\mathbb{R}}$ (which is not a field) is the best possible.,,['real-analysis']
41,"Example: Function sequence uniformly converges, its derivatives don't.","Example: Function sequence uniformly converges, its derivatives don't.",,"Could anyone give an example of a sequence of differentiable (real) functions that uniformly converge to a differentiable function, but the derivatives of which don't converge to the derivative of the limit function?","Could anyone give an example of a sequence of differentiable (real) functions that uniformly converge to a differentiable function, but the derivatives of which don't converge to the derivative of the limit function?",,"['calculus', 'real-analysis', 'intuition']"
42,How can I evaluate the Gaussian Integral using power series?,How can I evaluate the Gaussian Integral using power series?,,"It's a well known result that the Gaussian integral $$\int_0^\infty e^{-x^2} dx = \frac{\sqrt{\pi}}{2}$$ evaluates to $\frac{\sqrt{\pi}}{2}$ . This result can be obtained using double integrals with polar coordinates, among other things, but I'm particularly interested in evaluating this integral using power series. The power series of $e^{-x^2}$ is $$\sum_{n=0}^\infty \frac{(-1)^n x^{2n}}{n!}$$ and integrating this, we get $$\sum_{n=0}^\infty \frac{(-1)^n x^{2n+1}}{(n!) (2n+1)}$$ This expression is our integral (and it also has a radius of convergence of infinity, so we know it’ll approximate our integral function everywhere), so to evaluate it, we plug in our bounds. The expression is 0 for the bound of 0, so we're just looking for the limit of this power series as it tends to infinity. Using graphing software, with enough terms from the power series, I can see that the function does indeed spend a while at $\frac{\sqrt{\pi}}{2}$ before diverging. So, how can I analytically prove that the limit of the power series $$\lim_{x\to\infty} \sum_{n=0}^\infty \frac{(-1)^n x^{2n+1}}{(n!) (2n+1)}$$ as $x$ approaches infinity is $\frac{\sqrt{\pi}}{2}$ ? Thanks for your help!","It's a well known result that the Gaussian integral evaluates to . This result can be obtained using double integrals with polar coordinates, among other things, but I'm particularly interested in evaluating this integral using power series. The power series of is and integrating this, we get This expression is our integral (and it also has a radius of convergence of infinity, so we know it’ll approximate our integral function everywhere), so to evaluate it, we plug in our bounds. The expression is 0 for the bound of 0, so we're just looking for the limit of this power series as it tends to infinity. Using graphing software, with enough terms from the power series, I can see that the function does indeed spend a while at before diverging. So, how can I analytically prove that the limit of the power series as approaches infinity is ? Thanks for your help!",\int_0^\infty e^{-x^2} dx = \frac{\sqrt{\pi}}{2} \frac{\sqrt{\pi}}{2} e^{-x^2} \sum_{n=0}^\infty \frac{(-1)^n x^{2n}}{n!} \sum_{n=0}^\infty \frac{(-1)^n x^{2n+1}}{(n!) (2n+1)} \frac{\sqrt{\pi}}{2} \lim_{x\to\infty} \sum_{n=0}^\infty \frac{(-1)^n x^{2n+1}}{(n!) (2n+1)} x \frac{\sqrt{\pi}}{2},"['real-analysis', 'calculus']"
43,"If $f(x)=f(2x)$, then $f$ is differentiable","If , then  is differentiable",f(x)=f(2x) f,"Suppose $f$ is continuous from $(0,\infty)\to\mathbb{R}$ , and $f(x)=f(2x)$ . Then, can we conclude that $f$ is differentiable and uniformly continuous? I think yes, because, it is clear that $f(x)=f(2^kx)$ for any $x$ and $k\in\mathbb{N}$ . Also, this implies that for any $\epsilon>0$ , $f(x+\epsilon)=f(x)$ , for $x=\frac{\epsilon}{2^k-1}$ . This implies that $f$ is constant. Hence, the differentiability and uniform continuity is trivial. any counterexamples to this? Am I right here? Thanks beforehand.","Suppose is continuous from , and . Then, can we conclude that is differentiable and uniformly continuous? I think yes, because, it is clear that for any and . Also, this implies that for any , , for . This implies that is constant. Hence, the differentiability and uniform continuity is trivial. any counterexamples to this? Am I right here? Thanks beforehand.","f (0,\infty)\to\mathbb{R} f(x)=f(2x) f f(x)=f(2^kx) x k\in\mathbb{N} \epsilon>0 f(x+\epsilon)=f(x) x=\frac{\epsilon}{2^k-1} f","['real-analysis', 'calculus', 'functional-equations']"
44,What are the applications of sequence of functions?,What are the applications of sequence of functions?,,"This might be a ""silly"" question, but before starting my studies, I need a motivation. In Analysis books, there are the subjects such as ""sequence of functions, uniform convergence etc."" which deals with basically the sequence of functions, but up to now [I'm a 2. year physics & mathematics student], I haven't seen any real application of the concept of sequence of functions. I mean, for example, I have seen lots of application of sequences in defining continuity, compactness etc. (our instructor does the whole analysis based on sequences), but this is not the case for the sequence of function. Therefore, my question is that what are the applications of the concept of ""sequence of function"" in both mathematics and physics ?","This might be a ""silly"" question, but before starting my studies, I need a motivation. In Analysis books, there are the subjects such as ""sequence of functions, uniform convergence etc."" which deals with basically the sequence of functions, but up to now [I'm a 2. year physics & mathematics student], I haven't seen any real application of the concept of sequence of functions. I mean, for example, I have seen lots of application of sequences in defining continuity, compactness etc. (our instructor does the whole analysis based on sequences), but this is not the case for the sequence of function. Therefore, my question is that what are the applications of the concept of ""sequence of function"" in both mathematics and physics ?",,"['real-analysis', 'sequences-and-series', 'complex-analysis', 'uniform-convergence']"
45,Let $f\colon \mathbb R\to\mathbb R$ be a continuous function such that $f(i) = 0$,Let  be a continuous function such that,f\colon \mathbb R\to\mathbb R f(i) = 0,"Let $f\colon \mathbb R\to\mathbb R$ be a continuous function such that $f(i) = 0$, for all $i\in \mathbb Z$ . Which of the following statement is true$?$ $(A)$ $\operatorname{Image}(f)$ is closed in $\mathbb R$. $(B)$ $\operatorname{Image}(f)$ is open in $\mathbb R$. $(C)$ $f$ is uniformly continuous. $(D)$ None of the above. If we take $f(x) = 0$, for all $x\in \mathbb R$, then $\operatorname{Image}(f)$ is a singleton set. Also we know that every singleton set is closed in $\mathbb R$. So option $B$  is wrong. For option $(C)$, I take $f\colon \mathbb R\to\mathbb R$ such that $f(x) = x \sin\pi x $, this function $f$ is not uniformly continuous. So option $C$ is also wrong. Now my problem is that the answer of this question is option $D$ but i am not able to get any function to discard option $A$. So please help me. Thanks you.","Let $f\colon \mathbb R\to\mathbb R$ be a continuous function such that $f(i) = 0$, for all $i\in \mathbb Z$ . Which of the following statement is true$?$ $(A)$ $\operatorname{Image}(f)$ is closed in $\mathbb R$. $(B)$ $\operatorname{Image}(f)$ is open in $\mathbb R$. $(C)$ $f$ is uniformly continuous. $(D)$ None of the above. If we take $f(x) = 0$, for all $x\in \mathbb R$, then $\operatorname{Image}(f)$ is a singleton set. Also we know that every singleton set is closed in $\mathbb R$. So option $B$  is wrong. For option $(C)$, I take $f\colon \mathbb R\to\mathbb R$ such that $f(x) = x \sin\pi x $, this function $f$ is not uniformly continuous. So option $C$ is also wrong. Now my problem is that the answer of this question is option $D$ but i am not able to get any function to discard option $A$. So please help me. Thanks you.",,"['real-analysis', 'functions', 'continuity', 'uniform-continuity']"
46,Riemann integrable vs Lebesgue integrable,Riemann integrable vs Lebesgue integrable,,"Let $f$ be a real-valued function on $[a,b]$. Assume $f$ is Riemann integrable with strictly positive Riemann integral over $[a,b]$ then $f$ is strictly positive on some nonempty open interval. What if Lebesgue integrable instead of Riemann? *What I'm thinking is the following for Riemann. Assume otherwise, then for all open interval, say $(a',b')$,$\int_{a'}^{b'}f(x) dx\leq 0$. Since $f$ is Riemann-integrable with strictly positive Riemann integral over $[a,b]$, $\int_a^b f(x)dx< 0$. Let $a'\rightarrow a$, and $b'\rightarrow b $, then $\forall\epsilon>0$, $\exists\delta>0$ such that if $|a-a'|<\delta$ and $|b-b'|<\delta$, then $|\int_a^b f(x)dx-\int_{a'}^{b'}f(x)dx|<\epsilon$. By letting $\epsilon\rightarrow 0$, we'll have $\int_a^bf(x)dx\leq 0$, contradiction. Is this correct? and what about Lebesgue?","Let $f$ be a real-valued function on $[a,b]$. Assume $f$ is Riemann integrable with strictly positive Riemann integral over $[a,b]$ then $f$ is strictly positive on some nonempty open interval. What if Lebesgue integrable instead of Riemann? *What I'm thinking is the following for Riemann. Assume otherwise, then for all open interval, say $(a',b')$,$\int_{a'}^{b'}f(x) dx\leq 0$. Since $f$ is Riemann-integrable with strictly positive Riemann integral over $[a,b]$, $\int_a^b f(x)dx< 0$. Let $a'\rightarrow a$, and $b'\rightarrow b $, then $\forall\epsilon>0$, $\exists\delta>0$ such that if $|a-a'|<\delta$ and $|b-b'|<\delta$, then $|\int_a^b f(x)dx-\int_{a'}^{b'}f(x)dx|<\epsilon$. By letting $\epsilon\rightarrow 0$, we'll have $\int_a^bf(x)dx\leq 0$, contradiction. Is this correct? and what about Lebesgue?",,"['real-analysis', 'measure-theory']"
47,What are different ways to prove that $\sum_{n=1}^{\infty}\frac 1n$ is divergent? [duplicate],What are different ways to prove that  is divergent? [duplicate],\sum_{n=1}^{\infty}\frac 1n,"This question already has answers here : Why does the series $\sum_{n=1}^\infty\frac1n$ not converge? (26 answers) Closed 7 years ago . I have been studying Cauchy criterion for sequences, and have come across a rather simple proof for the harmonic series, and why it diverges. More so, we have the following: $$\sum_{n=1}^{\infty}\frac 1n=\infty\Rightarrow divergent$$ Here is my simple proof: Consider the sequence $\left\{a_n\right\}_{n=1}^{\infty}$ such that $a_n=\frac 1n$, then $\forall \epsilon>0,\exists \ N \in\mathbb{R}$ for $m,n\in \mathbb{N}$, such that, $$m,n>N\Rightarrow|a_m-a_n|<\epsilon$$ Pick $m=2n$, then we have the following: $$|a_{2n}-a_n|=\sum_{k=n+1}^{2n}\frac {1}{k}\geq \sum_{k=n+1}^{2n} \frac {1}{2n}=\frac 12$$ So pick $\epsilon=\frac 12\Rightarrow  \left\{a_n\right\}_{n=1}^{\infty}$ is not Cauchy and thus divergent. My question is, are there any other slick and easy proofs for the above claim, and if so, what are they? This series at first surprised me, as it initially doesn't seem divergent. Thanks in advance!","This question already has answers here : Why does the series $\sum_{n=1}^\infty\frac1n$ not converge? (26 answers) Closed 7 years ago . I have been studying Cauchy criterion for sequences, and have come across a rather simple proof for the harmonic series, and why it diverges. More so, we have the following: $$\sum_{n=1}^{\infty}\frac 1n=\infty\Rightarrow divergent$$ Here is my simple proof: Consider the sequence $\left\{a_n\right\}_{n=1}^{\infty}$ such that $a_n=\frac 1n$, then $\forall \epsilon>0,\exists \ N \in\mathbb{R}$ for $m,n\in \mathbb{N}$, such that, $$m,n>N\Rightarrow|a_m-a_n|<\epsilon$$ Pick $m=2n$, then we have the following: $$|a_{2n}-a_n|=\sum_{k=n+1}^{2n}\frac {1}{k}\geq \sum_{k=n+1}^{2n} \frac {1}{2n}=\frac 12$$ So pick $\epsilon=\frac 12\Rightarrow  \left\{a_n\right\}_{n=1}^{\infty}$ is not Cauchy and thus divergent. My question is, are there any other slick and easy proofs for the above claim, and if so, what are they? This series at first surprised me, as it initially doesn't seem divergent. Thanks in advance!",,"['real-analysis', 'sequences-and-series', 'harmonic-numbers']"
48,Limit of the nth power of certain partial sums,Limit of the nth power of certain partial sums,,Evaluate $$\lim_{n\to \infty}\left (\frac{6}{\pi^2}\sum_{k=1}^{n} \frac{1}{k^2} \right )^n.$$ The context is: I just thought it up and thought some members of MSE would like to try it.,Evaluate $$\lim_{n\to \infty}\left (\frac{6}{\pi^2}\sum_{k=1}^{n} \frac{1}{k^2} \right )^n.$$ The context is: I just thought it up and thought some members of MSE would like to try it.,,"['calculus', 'real-analysis', 'limits']"
49,Elementary Lebesgue measure problem,Elementary Lebesgue measure problem,,"Suppose $E_1,\cdots,E_n\subset [0,1]$ are Borel sets such that  $\sum_{i=1}^n\mu(E_i)>n-1$, in which $\mu$ denotes Lebesgue measure. Prove that $\cap_{i=1}^nE_i$ is nonempty. My attempts included using the famous equation, which is true as all sets in question are of finite lengths: $$\mu(\sum E_i)=\sum^1\mu(E_{i_1})+(-1)^1\sum^2\mu(E_{i_1}E_{i_2})+\cdots+(-1)^{n-2}\sum^{n-1}\mu(E_{i_1}\cdots E_{i_{n-1}})+(-1)^{n-1}\mu(E_1E_2\cdots E_n). $$ where $\sum^k$ denotes the $k$-th cyclic sum, and set addition and multiplication are used in place of union and intersection, for the sake of notational simplicity. Sadly, this came to no avail at all. Indeed I could do $n=2$ (which of course is too trivial to discuss here) but I couldn't  even do $n=3$. Whatever I think the ultimate goal is to show $\mu(\cap E_i)>0$, and some kind of elementary set operations must be involved. But now I'm at a loss of what to do.","Suppose $E_1,\cdots,E_n\subset [0,1]$ are Borel sets such that  $\sum_{i=1}^n\mu(E_i)>n-1$, in which $\mu$ denotes Lebesgue measure. Prove that $\cap_{i=1}^nE_i$ is nonempty. My attempts included using the famous equation, which is true as all sets in question are of finite lengths: $$\mu(\sum E_i)=\sum^1\mu(E_{i_1})+(-1)^1\sum^2\mu(E_{i_1}E_{i_2})+\cdots+(-1)^{n-2}\sum^{n-1}\mu(E_{i_1}\cdots E_{i_{n-1}})+(-1)^{n-1}\mu(E_1E_2\cdots E_n). $$ where $\sum^k$ denotes the $k$-th cyclic sum, and set addition and multiplication are used in place of union and intersection, for the sake of notational simplicity. Sadly, this came to no avail at all. Indeed I could do $n=2$ (which of course is too trivial to discuss here) but I couldn't  even do $n=3$. Whatever I think the ultimate goal is to show $\mu(\cap E_i)>0$, and some kind of elementary set operations must be involved. But now I'm at a loss of what to do.",,"['real-analysis', 'measure-theory', 'elementary-set-theory', 'lebesgue-measure']"
50,Determine convergence of the series $\sum_{n=1}^{\infty}\frac{(2n-1)!!}{(2n)!!}$,Determine convergence of the series,\sum_{n=1}^{\infty}\frac{(2n-1)!!}{(2n)!!},"I tried to use D'Alambert theorem to determine convergence of the series $\sum_{n=1}^{\infty}\frac{(2n-1)!!}{(2n)!!}$ . $$\lim_{n \to \infty} \frac{a_{n+1}}{a_{n}} = \lim_{n \to \infty} \frac{(2n-1)!!(2n+1)(2n)!!}{(2n-1)!!(2n)!!(2n+2)} = \lim_{n \to \infty} \frac{2n+1}{2n+2} = 1$$ but this test is inconclusive. I think a comparison test might give a result, but with which series should I compare it to?","I tried to use D'Alambert theorem to determine convergence of the series . but this test is inconclusive. I think a comparison test might give a result, but with which series should I compare it to?",\sum_{n=1}^{\infty}\frac{(2n-1)!!}{(2n)!!} \lim_{n \to \infty} \frac{a_{n+1}}{a_{n}} = \lim_{n \to \infty} \frac{(2n-1)!!(2n+1)(2n)!!}{(2n-1)!!(2n)!!(2n+2)} = \lim_{n \to \infty} \frac{2n+1}{2n+2} = 1,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
51,Query regarding Theorem 1.21 in Baby Rudin,Query regarding Theorem 1.21 in Baby Rudin,,"In the proof of Theorem - 1.21 (pg-10) in Rudin's Principles of Mathematical Analysis (Statement - For every real $x>0$ & every integer $n>0$, there is one & only one positive real $y$ s.t. $y^n = x$); The author says - ""Assume $y^n<x$. Choose h so that $0<h<1$ & $h<\frac{x-y^n}{n(y+1)^{n-1}}$"" $\space\space\space\space\space\space\space\space\space\space\space$ How does he get the last inequality? In the second part, how does the author get this value of k when he says - ""Assume $y^n>x$. Put $k=\frac{y^n-x}{ny^{n-1}}$"" ? Thanks in advance...","In the proof of Theorem - 1.21 (pg-10) in Rudin's Principles of Mathematical Analysis (Statement - For every real $x>0$ & every integer $n>0$, there is one & only one positive real $y$ s.t. $y^n = x$); The author says - ""Assume $y^n<x$. Choose h so that $0<h<1$ & $h<\frac{x-y^n}{n(y+1)^{n-1}}$"" $\space\space\space\space\space\space\space\space\space\space\space$ How does he get the last inequality? In the second part, how does the author get this value of k when he says - ""Assume $y^n>x$. Put $k=\frac{y^n-x}{ny^{n-1}}$"" ? Thanks in advance...",,['real-analysis']
52,Construct a compact set of real numbers whose limit points form a countable set.,Construct a compact set of real numbers whose limit points form a countable set.,,"I searched and found out that the below is a compact set of real numbers whose limit points form a countable set. I know the set in real number is compact if and only if it is bounded and closed. It's obvious it is bounded since $\,d(1/4, q) < 1\,$ for all $\,q \in E.$ However, I'm not sure how this is closed. Is there any simpler set that satisfies the above condition? Thank you! $$E = \left\{\frac 1{2^m}\left(1 - \frac 1n\right) \mid m,n \in \mathbb N\right\}.$$","I searched and found out that the below is a compact set of real numbers whose limit points form a countable set. I know the set in real number is compact if and only if it is bounded and closed. It's obvious it is bounded since $\,d(1/4, q) < 1\,$ for all $\,q \in E.$ However, I'm not sure how this is closed. Is there any simpler set that satisfies the above condition? Thank you! $$E = \left\{\frac 1{2^m}\left(1 - \frac 1n\right) \mid m,n \in \mathbb N\right\}.$$",,['real-analysis']
53,Check convergence of $\sum^{\infty}_{n=1} \frac{1}{(\ln\ln n)^{\ln n}}$,Check convergence of,\sum^{\infty}_{n=1} \frac{1}{(\ln\ln n)^{\ln n}},Check convergence of $$\sum^{\infty}_{n=1}\frac{1}{(\ln \ln n)^{\ln n}}.$$ Please verify my solution below.,Check convergence of $$\sum^{\infty}_{n=1}\frac{1}{(\ln \ln n)^{\ln n}}.$$ Please verify my solution below.,,"['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
54,How to justify term-by-term expansion to compute an integral,How to justify term-by-term expansion to compute an integral,,"In Titchmarsh's book on Fourier Transforms it states that the integral $$\int\nolimits_0^\infty \frac{x^{-a}}{1+x^2} dx,\quad 0<a<1$$ may be calculated by either contour integration or by series expansion. It certainly is easy to do this, technically. However, since the series for $\frac{1}{1+x^2}$ will only have radius of convergence equal to 1, due to the poles at $\pm i$, how does one justify doing the expansion then the integration? I know this must be simple but I'm not sure why it is true. Is it because the terms when combied with $x^{-a}$ form an absolutely convergent series? Thanks, Tom","In Titchmarsh's book on Fourier Transforms it states that the integral $$\int\nolimits_0^\infty \frac{x^{-a}}{1+x^2} dx,\quad 0<a<1$$ may be calculated by either contour integration or by series expansion. It certainly is easy to do this, technically. However, since the series for $\frac{1}{1+x^2}$ will only have radius of convergence equal to 1, due to the poles at $\pm i$, how does one justify doing the expansion then the integration? I know this must be simple but I'm not sure why it is true. Is it because the terms when combied with $x^{-a}$ form an absolutely convergent series? Thanks, Tom",,"['real-analysis', 'integration']"
55,Evaluate : $S=\frac{1}{1\cdot2\cdot3}+\frac{1}{5\cdot6\cdot7}+\frac{1}{9\cdot10\cdot11}+\cdots$,Evaluate :,S=\frac{1}{1\cdot2\cdot3}+\frac{1}{5\cdot6\cdot7}+\frac{1}{9\cdot10\cdot11}+\cdots,"Evaluate: $$S=\frac{1}{1\cdot2\cdot3}+\frac{1}{5\cdot6\cdot7} + \frac{1}{9\cdot10\cdot11}+\cdots$$ to infinite terms My Attempt: The given series $$S=\sum_{i=0}^\infty \frac{1}{(4i+1)(4i+2)(4i+3)} =\sum_{i=0}^\infty \left(\frac{1}{2(4i+1)}-\frac{1}{4i+2}+\frac{1}{2(4i+3)}\right)=\frac{1}{2}\sum_{i=0}^\infty \int_0^1 \left(x^{4i}-2x^{4i+1}+x^{4i+2}\right) \, dx$$ So, $$S=\frac{1}{2}\int_{0}^{1}\left(\frac{1}{1-x^4}-\frac{2x}{1-x^4} + \frac{x^2}{1-x^4}\right)dx=\frac{1}{2} \int_0^1 \left(\frac{1+x^2}{1-x^4}-\frac{2x}{1-x^4}\right)\,dx = \frac{1}{2} \int_0^1 \left(\frac{1}{1-x^2}-\frac{2x}{1-x^4}\right)\,dx$$ $$=\frac{1}{2}\int_{0}^1\frac{1}{1-x^2}dx-\int_{0}^{1}\frac{2x}{1-x^4}dx=\frac{1}{2}\int_{0}^1\frac{1}{1-x^2}dx-\frac{1}{2}\int_{0}^{1}\frac{1}{1-y^2}dy=0(y=x^2)$$ which is obviously absurd since all terms of $S$ are positive. But if I do like this then I am able to get the answer, $$S=\frac{1}{2}\int_{0}^{1}\left(\frac{1}{1-x^4}-\frac{2x}{1-x^4}+\frac{x^2}{1-x^4}\right)dx=\frac{1}{2}\int_{0}^{1}\frac{(1-x)^2}{1-x^4}dx=\frac{1}{2}\int_{0}^{1}\left(\frac{1}{1+x}-\frac{x}{1+x^2}\right)dx=\frac{\ln2}{4}$$ What is wrong with the previous approach","Evaluate: to infinite terms My Attempt: The given series So, which is obviously absurd since all terms of are positive. But if I do like this then I am able to get the answer, What is wrong with the previous approach","S=\frac{1}{1\cdot2\cdot3}+\frac{1}{5\cdot6\cdot7} + \frac{1}{9\cdot10\cdot11}+\cdots S=\sum_{i=0}^\infty \frac{1}{(4i+1)(4i+2)(4i+3)} =\sum_{i=0}^\infty \left(\frac{1}{2(4i+1)}-\frac{1}{4i+2}+\frac{1}{2(4i+3)}\right)=\frac{1}{2}\sum_{i=0}^\infty \int_0^1 \left(x^{4i}-2x^{4i+1}+x^{4i+2}\right) \, dx S=\frac{1}{2}\int_{0}^{1}\left(\frac{1}{1-x^4}-\frac{2x}{1-x^4} + \frac{x^2}{1-x^4}\right)dx=\frac{1}{2} \int_0^1 \left(\frac{1+x^2}{1-x^4}-\frac{2x}{1-x^4}\right)\,dx = \frac{1}{2} \int_0^1 \left(\frac{1}{1-x^2}-\frac{2x}{1-x^4}\right)\,dx =\frac{1}{2}\int_{0}^1\frac{1}{1-x^2}dx-\int_{0}^{1}\frac{2x}{1-x^4}dx=\frac{1}{2}\int_{0}^1\frac{1}{1-x^2}dx-\frac{1}{2}\int_{0}^{1}\frac{1}{1-y^2}dy=0(y=x^2) S S=\frac{1}{2}\int_{0}^{1}\left(\frac{1}{1-x^4}-\frac{2x}{1-x^4}+\frac{x^2}{1-x^4}\right)dx=\frac{1}{2}\int_{0}^{1}\frac{(1-x)^2}{1-x^4}dx=\frac{1}{2}\int_{0}^{1}\left(\frac{1}{1+x}-\frac{x}{1+x^2}\right)dx=\frac{\ln2}{4}","['real-analysis', 'integration', 'definite-integrals', 'summation']"
56,Limits and Infinite Integration by Parts,Limits and Infinite Integration by Parts,,"It is well known that $$\int \frac{\sin(x)}{x} \,dx$$ cannot be expressed in terms of elementary functions. However, if we repeatedly use integration by parts, we seem to be able to at least approximate the integral through the formula $$\int f(x) \,dx \approx \sum_{n=1}^a \frac{(-1)^{n-1}\cdot f^{(n-1)}(x)\cdot x^n}{n!}$$ where $a \in \mathbb{N}$ . When plugging this in to a graphing calculator, it converges, but very slowly. It also tends to converge more quickly for functions that tend to $0$ as $x \to \infty$ .  My guess is that $$\int f(x) \,dx = \lim_{a\to\infty}\sum_{n=1}^a \frac{(-1)^{n-1}\cdot f^{(n-1)}(x)\cdot x^n}{n!}$$ at least on a certain interval, but I am uncertain where to look  to learn more about these series. Any ideas? Thanks!","It is well known that cannot be expressed in terms of elementary functions. However, if we repeatedly use integration by parts, we seem to be able to at least approximate the integral through the formula where . When plugging this in to a graphing calculator, it converges, but very slowly. It also tends to converge more quickly for functions that tend to as .  My guess is that at least on a certain interval, but I am uncertain where to look  to learn more about these series. Any ideas? Thanks!","\int \frac{\sin(x)}{x} \,dx \int f(x) \,dx \approx \sum_{n=1}^a \frac{(-1)^{n-1}\cdot f^{(n-1)}(x)\cdot x^n}{n!} a \in \mathbb{N} 0 x \to \infty \int f(x) \,dx = \lim_{a\to\infty}\sum_{n=1}^a \frac{(-1)^{n-1}\cdot f^{(n-1)}(x)\cdot x^n}{n!}","['real-analysis', 'calculus', 'integration', 'sequences-and-series']"
57,There does not exist any continuous function $f : \mathbb R → \mathbb R$ such that $f(x)$ is rational if and only if $f(x + 1)$ is irrational,There does not exist any continuous function  such that  is rational if and only if  is irrational,f : \mathbb R → \mathbb R f(x) f(x + 1),Prove that there does not exist any continuous function $f : \mathbb R → \mathbb R$ such that $f(x)$ is rational if and only if $f(x + 1)$ is irrational. What theorems can I use to prove the statement?,Prove that there does not exist any continuous function $f : \mathbb R → \mathbb R$ such that $f(x)$ is rational if and only if $f(x + 1)$ is irrational. What theorems can I use to prove the statement?,,"['real-analysis', 'continuity']"
58,Property of a continuous function in the neighborhood of a point,Property of a continuous function in the neighborhood of a point,,"Let $f:\mathbb{R} \to \mathbb{R}$ be a differentiable function. Let $a \in \mathbb{R}$ and define the following sets:   \begin{align*} M= \{x>a \mid f(t)>f(a), \forall t \in (a,x] \}\\ N=\{x>a \mid f(t)=f(a), \forall t \in (a,x] \}\\ P=\{x>a \mid f(t)<f(a), \forall t \in (a,x] \} \end{align*}   Prove that at least one of these sets is not empty. I tried to prove this by contradiction. If they were all empty, then, for all $n \in \mathbb{N}_{\geq 1}$ there would be some numbers $a_n,b_n,c_n \in (a,a+\frac{1}{n})$ such that $f(a_n) \leq f(a), \: f(b_n) \neq f(a)$ and $f(c_n) \geq f(a)$. But these sequences that are formed seem independent, and I couldn't connect them such that I reach a contradiction. The fact that at least one of these sets is not empty is pretty obvious on a graph, but I didn't manage to prove it rigorously. I think I'm missing something obvious. Edit : Thank you for your counterexamples! As mentioned in one of the comments, I added the differentiability of the function.","Let $f:\mathbb{R} \to \mathbb{R}$ be a differentiable function. Let $a \in \mathbb{R}$ and define the following sets:   \begin{align*} M= \{x>a \mid f(t)>f(a), \forall t \in (a,x] \}\\ N=\{x>a \mid f(t)=f(a), \forall t \in (a,x] \}\\ P=\{x>a \mid f(t)<f(a), \forall t \in (a,x] \} \end{align*}   Prove that at least one of these sets is not empty. I tried to prove this by contradiction. If they were all empty, then, for all $n \in \mathbb{N}_{\geq 1}$ there would be some numbers $a_n,b_n,c_n \in (a,a+\frac{1}{n})$ such that $f(a_n) \leq f(a), \: f(b_n) \neq f(a)$ and $f(c_n) \geq f(a)$. But these sequences that are formed seem independent, and I couldn't connect them such that I reach a contradiction. The fact that at least one of these sets is not empty is pretty obvious on a graph, but I didn't manage to prove it rigorously. I think I'm missing something obvious. Edit : Thank you for your counterexamples! As mentioned in one of the comments, I added the differentiability of the function.",,"['real-analysis', 'continuity']"
59,Proof of connection between improper Riemann Integral and Lebesgue integral,Proof of connection between improper Riemann Integral and Lebesgue integral,,"""An improper Riemann integral is Lebesgue integrable if it is absolutely convergent. "" I've seen this statement quite often, but always without proof. I'd think if we have something like: $$ \int_{0}^\infty |f(x)|dx<\infty$$ there has to be a point $c\in\mathbb{R}$, where the integral doesn't ""grow"" anymore i.e. $$ \int_{0}^\infty |f(x)|dx \leq \int_{0}^c |f(x)|dx<\infty$$ otherwise the integral could not be convergent ( similar as to how an infinite sequence has to be a zero sequence in order for the infinite sum to be convergent ). But I'm having trouble formalising this or in general finding a proper proof. What is a simple way to prove this theorem?","""An improper Riemann integral is Lebesgue integrable if it is absolutely convergent. "" I've seen this statement quite often, but always without proof. I'd think if we have something like: $$ \int_{0}^\infty |f(x)|dx<\infty$$ there has to be a point $c\in\mathbb{R}$, where the integral doesn't ""grow"" anymore i.e. $$ \int_{0}^\infty |f(x)|dx \leq \int_{0}^c |f(x)|dx<\infty$$ otherwise the integral could not be convergent ( similar as to how an infinite sequence has to be a zero sequence in order for the infinite sum to be convergent ). But I'm having trouble formalising this or in general finding a proper proof. What is a simple way to prove this theorem?",,"['real-analysis', 'integration', 'improper-integrals', 'lebesgue-integral']"
60,Does the series $1-\frac12+\frac12-\frac1{2^2}+\frac13-\frac1{2^3}+\frac14-\frac1{2^4}+\frac15-\frac1{2^5}+\cdots$ converge or diverge?,Does the series  converge or diverge?,1-\frac12+\frac12-\frac1{2^2}+\frac13-\frac1{2^3}+\frac14-\frac1{2^4}+\frac15-\frac1{2^5}+\cdots,"$1-\frac{1}{2}+\frac{1}{2}-\frac{1}{2^2}+\frac{1}{3}-\frac{1}{2^3}+\frac{1}{4}-\frac{1}{2^4}+\frac{1}{5}-\frac{1}{2^5}+\cdots$ I've be trying to figure out how to write this series symbolically so I can examine it's limit, but I'm having trouble.  So far the best I've come up with is: $\sum_{n=0}^{\infty}(-1)^n\left(\frac{1}{2^n}-\frac{1}{2^{n+1}}\right)$ But obviously the above does not properly reproduce the series.","$1-\frac{1}{2}+\frac{1}{2}-\frac{1}{2^2}+\frac{1}{3}-\frac{1}{2^3}+\frac{1}{4}-\frac{1}{2^4}+\frac{1}{5}-\frac{1}{2^5}+\cdots$ I've be trying to figure out how to write this series symbolically so I can examine it's limit, but I'm having trouble.  So far the best I've come up with is: $\sum_{n=0}^{\infty}(-1)^n\left(\frac{1}{2^n}-\frac{1}{2^{n+1}}\right)$ But obviously the above does not properly reproduce the series.",,"['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
61,How many n-th Order Partial Derivatives Exist for a Function of k Variables?,How many n-th Order Partial Derivatives Exist for a Function of k Variables?,,"Example: Let's say for example I have a function, $f$, of $2$ variables : $f(x,y)$ For this function there exits $2$ first-order partial derivatives namely: $f_x = \frac{\partial f}{\partial x}$ $f_y = \frac{\partial f}{\partial y}$ Then if we are to differentiate further we will find that there are $4$ computable second-order partial derivatives. $f_{xx} = \frac{\partial^2 f}{\partial x^2}$ $f_{xy} = \frac{\partial^2 f}{{\partial x} {\partial y}}$ $f_{yy} = \frac{\partial^2 f}{\partial y^2}$ $f_{yx} = \frac{\partial^2 f}{{\partial y} {\partial x}}$ However due to the Equality of Mixed Partials ( https://en.wikipedia.org/wiki/Symmetry_of_second_derivatives ), two of those second-order partial derivatives are equivalent, $f_{xy} = f_{yx}$, and thus we are left with $3$ second-order partial derivatives for a function of 2 variables. $f_{xx} = \frac{\partial^2 f}{\partial x^2}$ $f_{xy} = \frac{\partial^2 f}{{\partial x} {\partial y}} \Leftrightarrow	 f_{yx} = \frac{\partial^2 f}{{\partial y} {\partial x}}$ $f_{yy} = \frac{\partial^2 f}{\partial y^2}$ Question: Given a function of k variables: $f(x_1 , x_2,x_3,\dots,x_{k-1},x_k)$ Is there a formula to find the number of $n^{th}$-order partial derivatives, (where $n$ is the order of the partial derivative), for a function of $k$ variables? For example where $n=1$ (i.e. the first-order derivatives), there would be $k$ partial derivatives, just as in the example above, for a function of $2$ variables there exists $2$ first-order derivatives.","Example: Let's say for example I have a function, $f$, of $2$ variables : $f(x,y)$ For this function there exits $2$ first-order partial derivatives namely: $f_x = \frac{\partial f}{\partial x}$ $f_y = \frac{\partial f}{\partial y}$ Then if we are to differentiate further we will find that there are $4$ computable second-order partial derivatives. $f_{xx} = \frac{\partial^2 f}{\partial x^2}$ $f_{xy} = \frac{\partial^2 f}{{\partial x} {\partial y}}$ $f_{yy} = \frac{\partial^2 f}{\partial y^2}$ $f_{yx} = \frac{\partial^2 f}{{\partial y} {\partial x}}$ However due to the Equality of Mixed Partials ( https://en.wikipedia.org/wiki/Symmetry_of_second_derivatives ), two of those second-order partial derivatives are equivalent, $f_{xy} = f_{yx}$, and thus we are left with $3$ second-order partial derivatives for a function of 2 variables. $f_{xx} = \frac{\partial^2 f}{\partial x^2}$ $f_{xy} = \frac{\partial^2 f}{{\partial x} {\partial y}} \Leftrightarrow	 f_{yx} = \frac{\partial^2 f}{{\partial y} {\partial x}}$ $f_{yy} = \frac{\partial^2 f}{\partial y^2}$ Question: Given a function of k variables: $f(x_1 , x_2,x_3,\dots,x_{k-1},x_k)$ Is there a formula to find the number of $n^{th}$-order partial derivatives, (where $n$ is the order of the partial derivative), for a function of $k$ variables? For example where $n=1$ (i.e. the first-order derivatives), there would be $k$ partial derivatives, just as in the example above, for a function of $2$ variables there exists $2$ first-order derivatives.",,"['calculus', 'real-analysis', 'combinatorics', 'multivariable-calculus', 'partial-derivative']"
62,Show that continuous functions on $\mathbb R$ are Borel-measurable,Show that continuous functions on  are Borel-measurable,\mathbb R,"Here are my thoughts. We want to show that $f^{-1}$ maps Borel sets to Borel sets. Let $f:\mathbb R\to\mathbb R$ be a continuous function and $\mathcal B(\mathbb R)$ a Borel $\sigma$-algebra. Let $B\in\mathcal B(\mathbb R)$ and $B$ open. Then $f^{-1}(B)$ is open as well, that is, there exists neighborhoods around every point of $f^{-1}(B)$ contained within. Since these neighborhoods are members of $\mathcal B(\mathbb R)$, their union are also members of $\mathcal B(\mathbb R)$. Hence $f^{-1}(B)$ is a member of $\mathcal B(\mathbb R)$. Here's where I'm stuck. I don't know how to show countably many such members can have their unions taken to give me $f^{-1}(B)$. I also don't know what to do with the closed sets in $\mathcal B(\mathbb R)$, especially singletons and those only generatable via complements. Hints?","Here are my thoughts. We want to show that $f^{-1}$ maps Borel sets to Borel sets. Let $f:\mathbb R\to\mathbb R$ be a continuous function and $\mathcal B(\mathbb R)$ a Borel $\sigma$-algebra. Let $B\in\mathcal B(\mathbb R)$ and $B$ open. Then $f^{-1}(B)$ is open as well, that is, there exists neighborhoods around every point of $f^{-1}(B)$ contained within. Since these neighborhoods are members of $\mathcal B(\mathbb R)$, their union are also members of $\mathcal B(\mathbb R)$. Hence $f^{-1}(B)$ is a member of $\mathcal B(\mathbb R)$. Here's where I'm stuck. I don't know how to show countably many such members can have their unions taken to give me $f^{-1}(B)$. I also don't know what to do with the closed sets in $\mathcal B(\mathbb R)$, especially singletons and those only generatable via complements. Hints?",,"['real-analysis', 'analysis', 'measure-theory', 'continuity']"
63,Sequence of rationals with an irrational limit have denominators going to infinity,Sequence of rationals with an irrational limit have denominators going to infinity,,"Let $\alpha$ be an irrational real number and let $a_j$ be a sequence of rational numbers converging to $\alpha$. Suppose that each $a_j$ is a fraction expressed in lowest terms: $a_j = \alpha_j / \beta_j$. Prove that the $\beta_j$ tend to $\infty$ Attempt: AFSOC that $\beta_j$ does not tend to infinity, then it is bounded by some $M$. We can also find an interval $\alpha \in (k, k+1)$. Let $\epsilon > 0$, $\exists N_0$ such that for all $N > N_0$ we have $|a_N - \alpha|  < \epsilon$. So $|\frac{p_N}{q_N} - \alpha| < \epsilon$. We try to use the fact that $q$ is bounded but fail to derive a contradiction. I do not know how to use the fact that $\alpha$ is between two integers, although it might not be relevant at all. Ideas?","Let $\alpha$ be an irrational real number and let $a_j$ be a sequence of rational numbers converging to $\alpha$. Suppose that each $a_j$ is a fraction expressed in lowest terms: $a_j = \alpha_j / \beta_j$. Prove that the $\beta_j$ tend to $\infty$ Attempt: AFSOC that $\beta_j$ does not tend to infinity, then it is bounded by some $M$. We can also find an interval $\alpha \in (k, k+1)$. Let $\epsilon > 0$, $\exists N_0$ such that for all $N > N_0$ we have $|a_N - \alpha|  < \epsilon$. So $|\frac{p_N}{q_N} - \alpha| < \epsilon$. We try to use the fact that $q$ is bounded but fail to derive a contradiction. I do not know how to use the fact that $\alpha$ is between two integers, although it might not be relevant at all. Ideas?",,"['real-analysis', 'epsilon-delta']"
64,Pointwise almost everywhere convergent subsequence of $\{\sin (nx)\}$,Pointwise almost everywhere convergent subsequence of,\{\sin (nx)\},"Can you prove or disprove that the sequence $\{\sin (nx)\}$ has a pointwise almost everywhere convergent subsequence with respect to the Lebesgue measure on $\mathbb{R}$ ? Edit: I am adding my thoughts here as a motivation, because otherwise this question will hit the bottom. So initially I thought whether $\{\sin{nx}\}$ is convergent in $L_1([0,\pi])$ or on other finite interval in $\mathbb R$. Then I computed the integral $\|\sin{nx}\|_1=\int\limits_{0}^{\pi}{|\sin{nx}|dx}=n\int\limits_{0}^{\pi/n}{\sin{nx}dx}=2$. Therefore $\|\sin{nx}\|_1\rightarrow 2$ but this doesn't tell me to which function converges. Then I thought about convergence in $L_2([0,\pi])$, but obviously it is not convergent there. This is because it is part of the orthonormal basis in $L_2$ (up to a constant), so it is not even Cauchy sequence. But still it is weakly convergent in $L_2$, because from Bessel's inequality it follows that $\langle f,\sin{nx}\rangle\rightarrow 0$ for each $f\in L_2([0,\pi])$. Finally, I decided to check whether $\{\sin{nx}\}$ has a convergent a.e subsequence. If there is no such sequence, then it can not be convergent in $L_1$. And if there is such subsequence, then by Lebesgue DCT it will follow that it converges in $L_1$ (and probably it might be the limit of the whole sequence). I just didn't see that LDCT will work again for $L_2$ : if there is a convergent a.e subsequence, then it should converge in $L_2$ also, but this is impossible since $\{\sin{nx}\}$ is orthonormal (up tp a constant) and  no subsequence of $\{\sin{nx}\}$ is Cauchy in $L_2$. This is actually the answer of @Julián Aguirre. Now I have another Question: Is it possible to prove that there is no convergent pointwise a.e subsequence of $\{\sin{nx}\}$ using only first year calculus and knowing of course what is a set with Lebesgue measure $0$ in $\mathbb R$ ? What is obvious is that for each $x$ there is a convergent subsequence since $\{\sin{nx}\}$ is bounded. But all $x\in [0,\pi]$ are uncountable set, so we can not apply for example Cantor diagonal argument.","Can you prove or disprove that the sequence $\{\sin (nx)\}$ has a pointwise almost everywhere convergent subsequence with respect to the Lebesgue measure on $\mathbb{R}$ ? Edit: I am adding my thoughts here as a motivation, because otherwise this question will hit the bottom. So initially I thought whether $\{\sin{nx}\}$ is convergent in $L_1([0,\pi])$ or on other finite interval in $\mathbb R$. Then I computed the integral $\|\sin{nx}\|_1=\int\limits_{0}^{\pi}{|\sin{nx}|dx}=n\int\limits_{0}^{\pi/n}{\sin{nx}dx}=2$. Therefore $\|\sin{nx}\|_1\rightarrow 2$ but this doesn't tell me to which function converges. Then I thought about convergence in $L_2([0,\pi])$, but obviously it is not convergent there. This is because it is part of the orthonormal basis in $L_2$ (up to a constant), so it is not even Cauchy sequence. But still it is weakly convergent in $L_2$, because from Bessel's inequality it follows that $\langle f,\sin{nx}\rangle\rightarrow 0$ for each $f\in L_2([0,\pi])$. Finally, I decided to check whether $\{\sin{nx}\}$ has a convergent a.e subsequence. If there is no such sequence, then it can not be convergent in $L_1$. And if there is such subsequence, then by Lebesgue DCT it will follow that it converges in $L_1$ (and probably it might be the limit of the whole sequence). I just didn't see that LDCT will work again for $L_2$ : if there is a convergent a.e subsequence, then it should converge in $L_2$ also, but this is impossible since $\{\sin{nx}\}$ is orthonormal (up tp a constant) and  no subsequence of $\{\sin{nx}\}$ is Cauchy in $L_2$. This is actually the answer of @Julián Aguirre. Now I have another Question: Is it possible to prove that there is no convergent pointwise a.e subsequence of $\{\sin{nx}\}$ using only first year calculus and knowing of course what is a set with Lebesgue measure $0$ in $\mathbb R$ ? What is obvious is that for each $x$ there is a convergent subsequence since $\{\sin{nx}\}$ is bounded. But all $x\in [0,\pi]$ are uncountable set, so we can not apply for example Cantor diagonal argument.",,"['real-analysis', 'sequences-and-series', 'lebesgue-measure', 'lp-spaces']"
65,How to find sum of the infinite series $\sum_{n=1}^{\infty} \frac{1}{ n(2n+1)}$,How to find sum of the infinite series,\sum_{n=1}^{\infty} \frac{1}{ n(2n+1)},$$\frac{1}{1 \times3} + \frac{1}{2\times5}+\frac{1}{3\times7} + \frac{1}{4\times9}+\cdots $$ How to find sum of this series? I tried this: its $n$ th term will be = $\frac{1}{n}-\frac{2}{2n+1}$ ; after that  I am not able to solve this.,How to find sum of this series? I tried this: its th term will be = ; after that  I am not able to solve this.,\frac{1}{1 \times3} + \frac{1}{2\times5}+\frac{1}{3\times7} + \frac{1}{4\times9}+\cdots  n \frac{1}{n}-\frac{2}{2n+1},"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'logarithms']"
66,Understanding the periodicity of a complex exponential function,Understanding the periodicity of a complex exponential function,,"In the reals, $e^{nx}$ explodes to infinity very fast. But, $e^{inx}$ is bounded and periodic. I am familiar with Euler's formula $e^{ix} = \cos x +i\sin x $. Yet, could you give me some intuition what's happening in the transition from the reals to the complex when multiplying by $e^i$? Thanks.","In the reals, $e^{nx}$ explodes to infinity very fast. But, $e^{inx}$ is bounded and periodic. I am familiar with Euler's formula $e^{ix} = \cos x +i\sin x $. Yet, could you give me some intuition what's happening in the transition from the reals to the complex when multiplying by $e^i$? Thanks.",,"['calculus', 'real-analysis', 'complex-analysis', 'trigonometry', 'periodic-functions']"
67,An asymptotic term for a finite sum involving Stirling numbers,An asymptotic term for a finite sum involving Stirling numbers,,"The question is a by-product at the end of this post . The following asymptotic term will ensure the convergence of some series. $$ \frac{1}{n!} \sum_{k = 1 }^{n } \frac{{n \brack k}}{k+1} = \mathcal{O} \left(\frac{1}{\ln n}\right), \quad \text{as} \quad n \rightarrow \infty. $$ Here $\displaystyle {n \brack k}$ are the unsigned Stirling numbers of the first kind . Can you find a proof for it? Update: A related interesting paper may be found here .","The question is a by-product at the end of this post . The following asymptotic term will ensure the convergence of some series. $$ \frac{1}{n!} \sum_{k = 1 }^{n } \frac{{n \brack k}}{k+1} = \mathcal{O} \left(\frac{1}{\ln n}\right), \quad \text{as} \quad n \rightarrow \infty. $$ Here $\displaystyle {n \brack k}$ are the unsigned Stirling numbers of the first kind . Can you find a proof for it? Update: A related interesting paper may be found here .",,"['real-analysis', 'sequences-and-series', 'asymptotics', 'stirling-numbers']"
68,"Showing $\rho (x,y)=\frac{d(x,y)}{1+d(x,y)}$ is a metric",Showing  is a metric,"\rho (x,y)=\frac{d(x,y)}{1+d(x,y)}","Show $\rho (x,y)=\dfrac{d(x,y)}{1+d(x,y)}$ is a metric on the metric space $X$, equipped with the Euclidean metric $d$. I've already shown that the positivity $\rho(x,y)\geq 0$, the symmetry $\rho(x,x)=\rho(y,y)=0$. I'm having trouble proving the last condition of a metric space though: the triangle inequality must be valid: $\rho(x,y)\leq \rho(x,p)+\rho (p,y)$, for $p\in X$. So, $\dfrac{d(x,y)}{1+d(x,y)}\leq \dfrac{d(p,x)}{1+d(p,x)}+\dfrac{d(p,y)}{1+d(p,y)}$ Could someone provide a hint as to get started? No complete solutions please. Thanks in advance.","Show $\rho (x,y)=\dfrac{d(x,y)}{1+d(x,y)}$ is a metric on the metric space $X$, equipped with the Euclidean metric $d$. I've already shown that the positivity $\rho(x,y)\geq 0$, the symmetry $\rho(x,x)=\rho(y,y)=0$. I'm having trouble proving the last condition of a metric space though: the triangle inequality must be valid: $\rho(x,y)\leq \rho(x,p)+\rho (p,y)$, for $p\in X$. So, $\dfrac{d(x,y)}{1+d(x,y)}\leq \dfrac{d(p,x)}{1+d(p,x)}+\dfrac{d(p,y)}{1+d(p,y)}$ Could someone provide a hint as to get started? No complete solutions please. Thanks in advance.",,"['real-analysis', 'metric-spaces']"
69,"If $f$ and $g$ are Riemann integrable, are $f\cdot g$ and $f/g$ Riemann integrable?","If  and  are Riemann integrable, are  and  Riemann integrable?",f g f\cdot g f/g,"I do not think they are, but I cannot seem to come up with a definitive answer. I have tried using the ""Cauchy criterion"" for integrability $$U(f,P)-L(f,P)<\varepsilon$$ Here, $U(f,P)$ is the upper Darboux sum and $L(f,P)$ is the lower Darboux sum. For $f\cdot g$ , I have tried using $$U(f,P)-L(f,P)<\sqrt\varepsilon$$ $$U(g,P)-L(g,P)<\sqrt\varepsilon$$ But I am not getting anywhere. I am not quite sure what to do for the $\frac{f}{g}$ . Edit: For $\frac{f}{g}$ , $g\neq0$","I do not think they are, but I cannot seem to come up with a definitive answer. I have tried using the ""Cauchy criterion"" for integrability Here, is the upper Darboux sum and is the lower Darboux sum. For , I have tried using But I am not getting anywhere. I am not quite sure what to do for the . Edit: For ,","U(f,P)-L(f,P)<\varepsilon U(f,P) L(f,P) f\cdot g U(f,P)-L(f,P)<\sqrt\varepsilon U(g,P)-L(g,P)<\sqrt\varepsilon \frac{f}{g} \frac{f}{g} g\neq0","['real-analysis', 'integration']"
70,How can one express $\sqrt{2+\sqrt{2}}$ without using the square root of a square root?,How can one express  without using the square root of a square root?,\sqrt{2+\sqrt{2}},"I was trying to review some analysis, and came across problem 3 from page 78 of Walter Rudin's Principles of Mathematical Analysis .  As part of the problem, I wanted to try to write $\sqrt{2+\sqrt{2}}$ without using the square root of a square root.  In other words, I wanted to express the number in the form $q_1+q_2{q_3}^s$, where $q_1, q_2, q_3, s \in \mathbb{Q}$ (or perhaps something similar).  I'm aware that this is probably a duplicate of another question, but I wasn't able to find it (I wasn't sure what to search for)...  Many thanks in advance! Edit (my work so far): I tried expressing it as the solution to the quartic $x^4-4x^2+2=0$, but this seemed futile... Edit 2 (the original problem): The original problem from the text states: If $s_1=\sqrt{2}$, and $$S_{n+1}=\sqrt{2+\sqrt{s_n}} (n=1,2,3,\ldots),$$   prove that $\{s_n\}$ converges, and that $s_n<2$ for $n=1,2,3,\ldots$. The problem is from the 3rd chapter of the book, which talks about sequences and series. Rudin provides numerous theorems on this topic, such as the comparison, ratio, and root tests for convergence.","I was trying to review some analysis, and came across problem 3 from page 78 of Walter Rudin's Principles of Mathematical Analysis .  As part of the problem, I wanted to try to write $\sqrt{2+\sqrt{2}}$ without using the square root of a square root.  In other words, I wanted to express the number in the form $q_1+q_2{q_3}^s$, where $q_1, q_2, q_3, s \in \mathbb{Q}$ (or perhaps something similar).  I'm aware that this is probably a duplicate of another question, but I wasn't able to find it (I wasn't sure what to search for)...  Many thanks in advance! Edit (my work so far): I tried expressing it as the solution to the quartic $x^4-4x^2+2=0$, but this seemed futile... Edit 2 (the original problem): The original problem from the text states: If $s_1=\sqrt{2}$, and $$S_{n+1}=\sqrt{2+\sqrt{s_n}} (n=1,2,3,\ldots),$$   prove that $\{s_n\}$ converges, and that $s_n<2$ for $n=1,2,3,\ldots$. The problem is from the 3rd chapter of the book, which talks about sequences and series. Rudin provides numerous theorems on this topic, such as the comparison, ratio, and root tests for convergence.",,"['real-analysis', 'abstract-algebra', 'field-theory', 'algebraic-number-theory']"
71,Limit of the series $\sum_{k=1}^\infty \frac{n}{n^2+k^2}.$,Limit of the series,\sum_{k=1}^\infty \frac{n}{n^2+k^2}.,"I am trying to evaluate $$\lim_{n\to \infty} \sum_{k=1}^\infty \frac{n}{n^2+k^2}.$$ Now I am aware that clearly $$\lim_{n\to \infty} \sum_{k=1}^n \frac{n}{n^2+k^2} = \int_0^1 \frac{1}{1+x^2}dx = \tan^{-1}(1) = \frac{\pi}{4},$$ but I do not know what to do if each sum is already sent to infinity. Im taking a limit of limits. I suppose I could rewrite my limit as $$\lim_{n\to \infty} \lim_{m\to \infty} \sum_{k=1}^m \frac{n}{n^2+k^2}?$$ But I am unaware if this is helpful at all. Any hints would be appreciated. Obviously, Wolfram calculates this as $\frac{\pi}{2}$ but I am unaware of the steps and logic to get there.","I am trying to evaluate Now I am aware that clearly but I do not know what to do if each sum is already sent to infinity. Im taking a limit of limits. I suppose I could rewrite my limit as But I am unaware if this is helpful at all. Any hints would be appreciated. Obviously, Wolfram calculates this as but I am unaware of the steps and logic to get there.","\lim_{n\to \infty} \sum_{k=1}^\infty \frac{n}{n^2+k^2}. \lim_{n\to \infty} \sum_{k=1}^n \frac{n}{n^2+k^2} = \int_0^1 \frac{1}{1+x^2}dx = \tan^{-1}(1) = \frac{\pi}{4}, \lim_{n\to \infty} \lim_{m\to \infty} \sum_{k=1}^m \frac{n}{n^2+k^2}? \frac{\pi}{2}","['real-analysis', 'calculus', 'sequences-and-series', 'summation']"
72,Iterative algorithm for $\pi$?,Iterative algorithm for ?,\pi,"Let $a_0=3$ and $a_n=a_{n-1}+\sin a_{n-1}$ . Then $$\pi =\lim_{n\to\infty}a_n.$$ I encountered this algorithm a long time ago and don't remember where. It converges very quickly, which I found fascinating (digits agreeing with $\pi$ are in green): $$\begin{align}a_1&\approx\color{green}{3.141}12,\\ a_2&\approx\color{green}{3.1415926535}722,\\ a_3&\approx \color{green}{3.14159265358979323846264338327950}19.\end{align}$$ Why does it compute $\pi$ ? And why is the convergence so fast?","Let and . Then I encountered this algorithm a long time ago and don't remember where. It converges very quickly, which I found fascinating (digits agreeing with are in green): Why does it compute ? And why is the convergence so fast?","a_0=3 a_n=a_{n-1}+\sin a_{n-1} \pi =\lim_{n\to\infty}a_n. \pi \begin{align}a_1&\approx\color{green}{3.141}12,\\
a_2&\approx\color{green}{3.1415926535}722,\\
a_3&\approx \color{green}{3.14159265358979323846264338327950}19.\end{align} \pi","['real-analysis', 'sequences-and-series', 'pi']"
73,Proof of $(1-x)x^n \leq \frac{n^n}{\left(n+1\right)^{n+1}}$ without use of derivatives,Proof of  without use of derivatives,(1-x)x^n \leq \frac{n^n}{\left(n+1\right)^{n+1}},"If $x \in \left[0,1\right]$ and $n \in \mathbb{Z}^+$ , is it possible to show $$(1-x)x^n \leq \frac{n^n}{\left(n+1\right)^{n+1}}$$ without use of derivatives? With derivative it's smooth: Let $f(x)=(1-x)x^n.$ Thus, $$f'(x)=nx^{n-1}-(n+1)x^n=(n+1)x^{n-1}\left(\frac{n}{n+1}-x\right),$$ which gives $x_{\max}=\dfrac{n}{n+1}$ and we are done!","If and , is it possible to show without use of derivatives? With derivative it's smooth: Let Thus, which gives and we are done!","x \in \left[0,1\right] n \in \mathbb{Z}^+ (1-x)x^n \leq \frac{n^n}{\left(n+1\right)^{n+1}} f(x)=(1-x)x^n. f'(x)=nx^{n-1}-(n+1)x^n=(n+1)x^{n-1}\left(\frac{n}{n+1}-x\right), x_{\max}=\dfrac{n}{n+1}","['real-analysis', 'inequality', 'a.m.-g.m.-inequality']"
74,How to prove that $\int_{1}^{\sqrt{2}+1}\frac{\ln{x}}{x^{2}-1}dx=\frac{\pi^{2}}{16}-\frac{\ln^{2}\left(\sqrt{2}+1\right)}{4}$,How to prove that,\int_{1}^{\sqrt{2}+1}\frac{\ln{x}}{x^{2}-1}dx=\frac{\pi^{2}}{16}-\frac{\ln^{2}\left(\sqrt{2}+1\right)}{4},"How to prove that $$\int_{1}^{\sqrt{2}+1}\frac{\ln{x}}{x^{2}-1}dx=\frac{\pi^{2}}{16}-\frac{\ln^{2}\left(\sqrt{2}+1\right)}{4}?$$ I have encountered this integral recently, and I am aware that one can show how this is true with a substitution $u=\ln{x}$ and then expanding it into a series of integrals of the form $\int_{0}^{\ln{(\sqrt{2}+1)}}ue^{-nu}du$ where each integral is then calculated individually, but it feels pretty brute-forced. Is there any other way to do this? Thanks.","How to prove that I have encountered this integral recently, and I am aware that one can show how this is true with a substitution and then expanding it into a series of integrals of the form where each integral is then calculated individually, but it feels pretty brute-forced. Is there any other way to do this? Thanks.",\int_{1}^{\sqrt{2}+1}\frac{\ln{x}}{x^{2}-1}dx=\frac{\pi^{2}}{16}-\frac{\ln^{2}\left(\sqrt{2}+1\right)}{4}? u=\ln{x} \int_{0}^{\ln{(\sqrt{2}+1)}}ue^{-nu}du,"['real-analysis', 'calculus']"
75,Definite integral: $\int_0^\infty \frac{x dx}{(1+x^2)(1+e^{\pi x})}$,Definite integral:,\int_0^\infty \frac{x dx}{(1+x^2)(1+e^{\pi x})},"I need help computing the value of the following definite improper integral: $$\int_0^\infty \frac{x dx}{(1+x^2)(1+e^{\pi x})}=\text{?}$$ Here are my thoughts and attempts: I tried using the Laplace Transform identity for definite integrals, with no luck (since I can only compute the Laplace Transform of $\frac{1}{e^{\pi x}+1}$ in terms of the digamma function... yuck) I can't use the residue theorem, since the integral is from $0$ to $\infty$ and the integrand is not an even function I would like to expand $\frac{x}{1+x^2}=\frac{1/x}{1-(-1/x^2)}$ as a geometric series, but it wouldn't always converge since $x$ goes from $0$ to $\infty$ CONTEXT: The integral came up in Jack D'Aurizio's answer to this question. Any ideas?","I need help computing the value of the following definite improper integral: $$\int_0^\infty \frac{x dx}{(1+x^2)(1+e^{\pi x})}=\text{?}$$ Here are my thoughts and attempts: I tried using the Laplace Transform identity for definite integrals, with no luck (since I can only compute the Laplace Transform of $\frac{1}{e^{\pi x}+1}$ in terms of the digamma function... yuck) I can't use the residue theorem, since the integral is from $0$ to $\infty$ and the integrand is not an even function I would like to expand $\frac{x}{1+x^2}=\frac{1/x}{1-(-1/x^2)}$ as a geometric series, but it wouldn't always converge since $x$ goes from $0$ to $\infty$ CONTEXT: The integral came up in Jack D'Aurizio's answer to this question. Any ideas?",,"['real-analysis', 'integration', 'complex-analysis', 'definite-integrals', 'improper-integrals']"
76,How to prove that $\lim_{k\to+\infty}\frac{\sin(kx)}{\pi x}=\delta(x)$?,How to prove that ?,\lim_{k\to+\infty}\frac{\sin(kx)}{\pi x}=\delta(x),"It is well-known that: $$\lim_{k\to+\infty}\frac{\sin(kx)}{\pi x}=\delta(x).$$ This can also be written as $$ 2\pi\delta(x)=\int^{+\infty}_{-\infty}e^{ikx}\,\mathrm dk.$$ However, I don't know how to prove this without using Fourier Transform . I have already searched google and looked for some books, but I just get nothing. In short, I want to know the proof of this equation: $$\lim_{k\to+\infty}\int^{+\infty}_{-\infty}\frac{\sin(kx)}{\pi x}f(x)dx=f(0).$$","It is well-known that: This can also be written as However, I don't know how to prove this without using Fourier Transform . I have already searched google and looked for some books, but I just get nothing. In short, I want to know the proof of this equation:","\lim_{k\to+\infty}\frac{\sin(kx)}{\pi x}=\delta(x).  2\pi\delta(x)=\int^{+\infty}_{-\infty}e^{ikx}\,\mathrm dk. \lim_{k\to+\infty}\int^{+\infty}_{-\infty}\frac{\sin(kx)}{\pi x}f(x)dx=f(0).","['real-analysis', 'calculus', 'distribution-theory', 'dirac-delta']"
77,"Another beautiful arctan integral $\int_{1/2}^1 \frac{\arctan\left(\frac{1-x^2}{7 x^2+10x+7}\right)}{1-x^2} \, dx$",Another beautiful arctan integral,"\int_{1/2}^1 \frac{\arctan\left(\frac{1-x^2}{7 x^2+10x+7}\right)}{1-x^2} \, dx","Do you think we can express the closed form of the integral below in a very nice and short way? As you already know, your opinions weighs much to me, so I need them! Calculate in closed-form $$\int_{1/2}^1 \frac{\arctan\left(\frac{1-x^2}{7 x^2+10x+7}\right)}{1-x^2} \, dx.$$ I'm looking forward to your precious feedback! Mathematica tells us the closed form is $$\frac{1}{4} i \text{Li}_2\left(\frac{1}{5}+\frac{2 i}{5}\right)+\frac{1}{4} i \text{Li}_2\left(\frac{2}{15}+\frac{2 i}{5}\right)-\frac{1}{4} i \text{Li}_2\left(\frac{1}{5}-\frac{2 i}{5}\right)-\frac{1}{4} i \text{Li}_2\left(\frac{2}{15}-\frac{2 i}{5}\right)-\frac{1}{4} i \text{Li}_2\left(\frac{1}{4}+\frac{i}{8}\right)+\frac{1}{4} i \text{Li}_2\left(\frac{1}{4}-\frac{i}{8}\right)-\frac{1}{4} i \text{Li}_2\left(\frac{1}{10}+\frac{3 i}{10}\right)+\frac{1}{4} i \text{Li}_2\left(\frac{1}{10}-\frac{3 i}{10}\right)+\frac{1}{4} i \text{Li}_2\left(\frac{1}{4}+\frac{i}{12}\right)-\frac{1}{4} i \text{Li}_2\left(\frac{1}{4}-\frac{i}{12}\right)-\frac{1}{4} i \text{Li}_2\left(\frac{4}{15}+\frac{8 i}{15}\right)+\frac{1}{4} i \text{Li}_2\left(\frac{4}{15}-\frac{8 i}{15}\right)+\frac{1}{4} \log (4) \tan ^{-1}(6)-\frac{1}{4} \log (4) \tan ^{-1}(9)+\frac{1}{4} \log (4) \tan ^{-1}\left(\frac{1}{6}\right)-\frac{1}{4} \log (4) \tan ^{-1}\left(\frac{1}{9}\right)-\frac{1}{4} \log (9) \tan ^{-1}(2)+\frac{1}{4} \log (9) \tan ^{-1}(3)-\frac{1}{4} \log (9) \tan ^{-1}(6)+\frac{1}{4} \log (9) \tan ^{-1}(9)-\frac{1}{4} \log (9) \tan ^{-1}\left(\frac{3}{55}\right)+\frac{1}{4} \log (16) \tan ^{-1}(2)-\frac{1}{4} \log (16) \tan ^{-1}(3).$$","Do you think we can express the closed form of the integral below in a very nice and short way? As you already know, your opinions weighs much to me, so I need them! Calculate in closed-form $$\int_{1/2}^1 \frac{\arctan\left(\frac{1-x^2}{7 x^2+10x+7}\right)}{1-x^2} \, dx.$$ I'm looking forward to your precious feedback! Mathematica tells us the closed form is $$\frac{1}{4} i \text{Li}_2\left(\frac{1}{5}+\frac{2 i}{5}\right)+\frac{1}{4} i \text{Li}_2\left(\frac{2}{15}+\frac{2 i}{5}\right)-\frac{1}{4} i \text{Li}_2\left(\frac{1}{5}-\frac{2 i}{5}\right)-\frac{1}{4} i \text{Li}_2\left(\frac{2}{15}-\frac{2 i}{5}\right)-\frac{1}{4} i \text{Li}_2\left(\frac{1}{4}+\frac{i}{8}\right)+\frac{1}{4} i \text{Li}_2\left(\frac{1}{4}-\frac{i}{8}\right)-\frac{1}{4} i \text{Li}_2\left(\frac{1}{10}+\frac{3 i}{10}\right)+\frac{1}{4} i \text{Li}_2\left(\frac{1}{10}-\frac{3 i}{10}\right)+\frac{1}{4} i \text{Li}_2\left(\frac{1}{4}+\frac{i}{12}\right)-\frac{1}{4} i \text{Li}_2\left(\frac{1}{4}-\frac{i}{12}\right)-\frac{1}{4} i \text{Li}_2\left(\frac{4}{15}+\frac{8 i}{15}\right)+\frac{1}{4} i \text{Li}_2\left(\frac{4}{15}-\frac{8 i}{15}\right)+\frac{1}{4} \log (4) \tan ^{-1}(6)-\frac{1}{4} \log (4) \tan ^{-1}(9)+\frac{1}{4} \log (4) \tan ^{-1}\left(\frac{1}{6}\right)-\frac{1}{4} \log (4) \tan ^{-1}\left(\frac{1}{9}\right)-\frac{1}{4} \log (9) \tan ^{-1}(2)+\frac{1}{4} \log (9) \tan ^{-1}(3)-\frac{1}{4} \log (9) \tan ^{-1}(6)+\frac{1}{4} \log (9) \tan ^{-1}(9)-\frac{1}{4} \log (9) \tan ^{-1}\left(\frac{3}{55}\right)+\frac{1}{4} \log (16) \tan ^{-1}(2)-\frac{1}{4} \log (16) \tan ^{-1}(3).$$",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'special-functions']"
78,A subspace $X$ is closed iff $X =( X^\perp)^\perp$,A subspace  is closed iff,X X =( X^\perp)^\perp,"Let $X$ be a subspace of a Hilbert space $H$. prove: $X$ is closed iff $X =( X^\perp)^\perp$ I do not know how to proceed. any hints would be appreciated. thanks. $X^\perp = ${ $y \in H$:$\langle x,y\rangle = 0 \forall x \in  X\}$.","Let $X$ be a subspace of a Hilbert space $H$. prove: $X$ is closed iff $X =( X^\perp)^\perp$ I do not know how to proceed. any hints would be appreciated. thanks. $X^\perp = ${ $y \in H$:$\langle x,y\rangle = 0 \forall x \in  X\}$.",,"['real-analysis', 'functional-analysis', 'hilbert-spaces', 'inner-products']"
79,Basic Open set question,Basic Open set question,,"In $\mathbb{R}^2$, if I have an open set, call it $U$, and a point $u_0\in U$. Is the set $U^{'} = U - \{ u_0 \}$ is open as well? Or perhaps it is non-open & non-closed? My intuition is that it still remains open, but I'm not sure on how to formalize this. Is it enough if I say that since for each $u\in U$ exists a neighbourhood $V_u$ such that $V_u\subseteq U$, so it is also true for each $u\in U- \{u_0 \}$, thus making $U^{'}$ open as well? Not sure if this is formal enough (or perhaps I missed something and it's wrong) And assuming this is correct, can I remove more than one point? i.e. remove any $u_0,...,u_n$ from $U$ and still it will remain an open set? Thanks!","In $\mathbb{R}^2$, if I have an open set, call it $U$, and a point $u_0\in U$. Is the set $U^{'} = U - \{ u_0 \}$ is open as well? Or perhaps it is non-open & non-closed? My intuition is that it still remains open, but I'm not sure on how to formalize this. Is it enough if I say that since for each $u\in U$ exists a neighbourhood $V_u$ such that $V_u\subseteq U$, so it is also true for each $u\in U- \{u_0 \}$, thus making $U^{'}$ open as well? Not sure if this is formal enough (or perhaps I missed something and it's wrong) And assuming this is correct, can I remove more than one point? i.e. remove any $u_0,...,u_n$ from $U$ and still it will remain an open set? Thanks!",,"['real-analysis', 'general-topology']"
80,"Integral $\int_0^1 \frac{x\log x+1-x}{x \log^2 x}\log(1+x)\, dx=\log\frac{4}{\pi}$",Integral,"\int_0^1 \frac{x\log x+1-x}{x \log^2 x}\log(1+x)\, dx=\log\frac{4}{\pi}","Hi I am trying to prove this $$ I:=\int_{0}^{1} {x\log\left(\,x\,\right) + 1 - x \over x\log^{2}\left(\,x\,\right)}\, \log\left(\,1 + x\,\right)\,{\rm d}x=\log\left(\,4 \over \pi\,\right). $$ Thanks. This is just a beautiful integral for many reasons.  Logs are everywhere and an inspirational solution!!!  I am not sure if breaking it up into three separate pieces is of any use, I  tried that by writing $$ I=\int_0^1\frac{ \log(1+x)}{\log x}dx+\int_0^1\frac{\log(1+x)}{x \log^2 x}dx-\int_0^1\frac{\log(1+x)}{\log^2 x}dx $$ but wasn't sure how to handle these.  Also  note that $$ \int_0^1 \frac{x\log x+1-x}{x \log^2 x}dx=1, $$ in case that happened to come up anywhere along the calculation.","Hi I am trying to prove this $$ I:=\int_{0}^{1} {x\log\left(\,x\,\right) + 1 - x \over x\log^{2}\left(\,x\,\right)}\, \log\left(\,1 + x\,\right)\,{\rm d}x=\log\left(\,4 \over \pi\,\right). $$ Thanks. This is just a beautiful integral for many reasons.  Logs are everywhere and an inspirational solution!!!  I am not sure if breaking it up into three separate pieces is of any use, I  tried that by writing $$ I=\int_0^1\frac{ \log(1+x)}{\log x}dx+\int_0^1\frac{\log(1+x)}{x \log^2 x}dx-\int_0^1\frac{\log(1+x)}{\log^2 x}dx $$ but wasn't sure how to handle these.  Also  note that $$ \int_0^1 \frac{x\log x+1-x}{x \log^2 x}dx=1, $$ in case that happened to come up anywhere along the calculation.",,"['real-analysis', 'integration', 'complex-analysis', 'definite-integrals', 'special-functions']"
81,Prove that $\int_{E}f =\lim \int_{E}f_{n}$,Prove that,\int_{E}f =\lim \int_{E}f_{n},"I'm doing exercise in Real Analysis of Folland, and got stuck on this problem. I try to use Fatou lemma but can't come to the conclusion. Can anyone help me. I really appreciate. Consider a measure space $(X, M, \mu)$.Suppose $\{f_{n}\} \in L^{+}, f_{n} \rightarrow f$ pointwise and $\int f = \lim \int f_{n} < \infty$. Then $\int_{E} f = \lim \int_{E} f_{n}$ for all $E \in M$. However, this need not be true if $\int f = \lim \int f_{n} = \infty$ Thanks so much.","I'm doing exercise in Real Analysis of Folland, and got stuck on this problem. I try to use Fatou lemma but can't come to the conclusion. Can anyone help me. I really appreciate. Consider a measure space $(X, M, \mu)$.Suppose $\{f_{n}\} \in L^{+}, f_{n} \rightarrow f$ pointwise and $\int f = \lim \int f_{n} < \infty$. Then $\int_{E} f = \lim \int_{E} f_{n}$ for all $E \in M$. However, this need not be true if $\int f = \lim \int f_{n} = \infty$ Thanks so much.",,"['real-analysis', 'integration', 'analysis', 'measure-theory']"
82,"If $\{x_n\}$ is a Cauchy sequence in a normed vector space, is $\frac{x_n}{\|x_n\|}$ Cauchy?","If  is a Cauchy sequence in a normed vector space, is  Cauchy?",\{x_n\} \frac{x_n}{\|x_n\|},"Let $\{x_n\}$ a Cauchy sequence in a normed vector space $X$. Is $$y_n = \frac{x_n}{\|x_n\|}$$ another Cauchy sequence in $D = \{x\in X : \|x\| = 1\}$? Remark : The idea is prove that if $D$ is complete, then $X$ is also complete. Thanks so much.","Let $\{x_n\}$ a Cauchy sequence in a normed vector space $X$. Is $$y_n = \frac{x_n}{\|x_n\|}$$ another Cauchy sequence in $D = \{x\in X : \|x\| = 1\}$? Remark : The idea is prove that if $D$ is complete, then $X$ is also complete. Thanks so much.",,"['real-analysis', 'sequences-and-series', 'normed-spaces']"
83,Compute: $\lim_{n\to\infty} \{ (\sqrt2+1)^{2n} \}$,Compute:,\lim_{n\to\infty} \{ (\sqrt2+1)^{2n} \},Compute the following limit: $$\lim_{n\to\infty} \{ (\sqrt2+1)^{2n} \}$$ where $\{x\}$ is the fractional part of $x$. I need some hints here. Thanks.,Compute the following limit: $$\lim_{n\to\infty} \{ (\sqrt2+1)^{2n} \}$$ where $\{x\}$ is the fractional part of $x$. I need some hints here. Thanks.,,"['real-analysis', 'limits']"
84,Function which has no fixed points,Function which has no fixed points,,"Problem: Can anyone come up with an explicit function $f \colon \mathbb R \to \mathbb R$ such that $| f(x) - f(y)| < |x-y|$ for all $x,y\in \mathbb R$ and $f$ has no fixed point? I could prove that such a function exists like a hyperpolic function which is below the $y=x$ axis and doesn't intersect it. But, I am looking for an explicit function that satisfies that.","Problem: Can anyone come up with an explicit function $f \colon \mathbb R \to \mathbb R$ such that $| f(x) - f(y)| < |x-y|$ for all $x,y\in \mathbb R$ and $f$ has no fixed point? I could prove that such a function exists like a hyperpolic function which is below the $y=x$ axis and doesn't intersect it. But, I am looking for an explicit function that satisfies that.",,"['real-analysis', 'general-topology', 'fixed-point-theorems', 'fixed-points']"
85,Integral of product of two functions in terms of the integral of the other function,Integral of product of two functions in terms of the integral of the other function,,"Problem: Let $f$ and $g$ be two continuous functions on $[ a,b ]$ and assume $g$ is positive. Prove that $$\int_{a}^{b}f(x)g(x)dx=f(\xi )\int_{a}^{b}g(x)dx$$ for some $\xi$ in $[ a,b ]$. Here is my solution: Since $f(x)$ and $g(x)$ are continuous, then $f(x) g(x)$ is continuous. Using the Mean value theorem, there exists a $\xi$ in $[ a,b ]$ such that  $\int_{a}^{b}f(x)g(x)dx= f(\xi)g(\xi) (b-a) $ and using the Mean value theorem again, we can get $g(\xi) (b-a)=\int_{a}^{b}g(x)dx$ which yields the required equality. Is my proof correct? If not, please let me know how to correct it.","Problem: Let $f$ and $g$ be two continuous functions on $[ a,b ]$ and assume $g$ is positive. Prove that $$\int_{a}^{b}f(x)g(x)dx=f(\xi )\int_{a}^{b}g(x)dx$$ for some $\xi$ in $[ a,b ]$. Here is my solution: Since $f(x)$ and $g(x)$ are continuous, then $f(x) g(x)$ is continuous. Using the Mean value theorem, there exists a $\xi$ in $[ a,b ]$ such that  $\int_{a}^{b}f(x)g(x)dx= f(\xi)g(\xi) (b-a) $ and using the Mean value theorem again, we can get $g(\xi) (b-a)=\int_{a}^{b}g(x)dx$ which yields the required equality. Is my proof correct? If not, please let me know how to correct it.",,"['calculus', 'real-analysis']"
86,Prove that the given set is bounded above in $\mathbb{Q}$ but does not have a Supremum in $\mathbb{Q}$,Prove that the given set is bounded above in  but does not have a Supremum in,\mathbb{Q} \mathbb{Q},"Let $S=\{r\in \mathbb{Q}\mid r\leq\sqrt{3}\}$ . Now prove that $S$ is bounded above in $\mathbb{Q}$ but it does not have a supremum in $\mathbb{Q}$ . The following is the proof I came up with, but I do not feel confident with it. Please let me know what is incorrect or could be written better. Proof. Let $m=\sqrt{3}$ . Then their exists an $m$ that belongs to the set of real numbers $\mathbb{R}$ such that $r\leq m$ for all $r$ that belong to the set $S$ . Hence, $S$ is bounded above in $\mathbb{Q}$ . Now, suppose that there exists a smaller upper bound in $S$ , $t$ . Then $t<\sqrt{3}$ , and as $t$ is an upper bound of $s$ , $t\geq s$ for all $s\in S$ . Therefore, $s\leq t<\sqrt{3}$ , and $s\leq \sqrt{3}\leq t$ , which implies $\sqrt{3}\leq t\leq \sqrt{3}$ , which is impossible. Therefore, $S$ does not have a supremum in $\mathbb{Q}$ . Edit (added by Arturo Magidin, taken from comment made by OP) 2nd Attempt: Proof Let $m=\sqrt{3}$ . Then $m\geq r$ for all $r$ that belong to $S$ . Hence, $S$ is bounded above by $m$ in $\mathbb{R}$ . Therefore, $S$ is bounded above in $\mathbb{Q}$ . Now, suppose $m=\sup(S)$ in $\mathbb{R}$ . Let $t$ be an upper bound in $\mathbb{Q}$ and $t\leq m$ . Hence, there exists $t$ that belongs to $\mathbb{Q}$ such that $m>t\geq r$ for every $r$ that belongs to $S$ . Then there would exist an $r$ that belongs to $S$ such that $r>t$ . Which contradicts $t\geq r$ . Therefore, $S$ doesn't have a supremum in $\mathbb{Q}$ . 3rd Attempt: Proof Since for every $r \in S$ $r<3$ and $3 \in \mathbb{Q}$ , we know $S$ is bounded above in $Q$ . To prove it doesn't have a supremum in $\mathbb{Q}$ , I will use contradiction. Suppose m were the supremum of $S$ in $\mathbb{Q}$ , then m does not equal $\sqrt{3}$ , and $m\in \mathbb{Q}$ . If $m\lt \sqrt{3}$ , by Archmedian's Property, their exists $t$ that belongs to $\mathbb{Q}$ such that $m\lt t\lt \sqrt{3}$ . This is a contradiction, because $m$ isn't an upper bound. If $m\gt\sqrt{3}$ , by Archmedian's Property, their exists $u$ that belongs to $\mathbb{Q}$ such that $\sqrt{3}\lt u\lt m$ This is also a contradiction. Hence, $S$ does not have a supremum in $\mathbb{Q}$","Let . Now prove that is bounded above in but it does not have a supremum in . The following is the proof I came up with, but I do not feel confident with it. Please let me know what is incorrect or could be written better. Proof. Let . Then their exists an that belongs to the set of real numbers such that for all that belong to the set . Hence, is bounded above in . Now, suppose that there exists a smaller upper bound in , . Then , and as is an upper bound of , for all . Therefore, , and , which implies , which is impossible. Therefore, does not have a supremum in . Edit (added by Arturo Magidin, taken from comment made by OP) 2nd Attempt: Proof Let . Then for all that belong to . Hence, is bounded above by in . Therefore, is bounded above in . Now, suppose in . Let be an upper bound in and . Hence, there exists that belongs to such that for every that belongs to . Then there would exist an that belongs to such that . Which contradicts . Therefore, doesn't have a supremum in . 3rd Attempt: Proof Since for every and , we know is bounded above in . To prove it doesn't have a supremum in , I will use contradiction. Suppose m were the supremum of in , then m does not equal , and . If , by Archmedian's Property, their exists that belongs to such that . This is a contradiction, because isn't an upper bound. If , by Archmedian's Property, their exists that belongs to such that This is also a contradiction. Hence, does not have a supremum in",S=\{r\in \mathbb{Q}\mid r\leq\sqrt{3}\} S \mathbb{Q} \mathbb{Q} m=\sqrt{3} m \mathbb{R} r\leq m r S S \mathbb{Q} S t t<\sqrt{3} t s t\geq s s\in S s\leq t<\sqrt{3} s\leq \sqrt{3}\leq t \sqrt{3}\leq t\leq \sqrt{3} S \mathbb{Q} m=\sqrt{3} m\geq r r S S m \mathbb{R} S \mathbb{Q} m=\sup(S) \mathbb{R} t \mathbb{Q} t\leq m t \mathbb{Q} m>t\geq r r S r S r>t t\geq r S \mathbb{Q} r \in S r<3 3 \in \mathbb{Q} S Q \mathbb{Q} S \mathbb{Q} \sqrt{3} m\in \mathbb{Q} m\lt \sqrt{3} t \mathbb{Q} m\lt t\lt \sqrt{3} m m\gt\sqrt{3} u \mathbb{Q} \sqrt{3}\lt u\lt m S \mathbb{Q},['real-analysis']
87,How to prove $\int_0^1\frac{1-x}{(\ln x)(1+x)}\ dx=\ln\left(\frac2{\pi}\right)$?,How to prove ?,\int_0^1\frac{1-x}{(\ln x)(1+x)}\ dx=\ln\left(\frac2{\pi}\right),"A friend asked me to prove $$\int_0^1\frac{1-x}{(\ln x)(1+x)}\ dx=\ln\left(\frac2{\pi}\right)$$ and here is my approach: \begin{align} I&=\int_0^1\frac{1-x}{\ln x}\frac1{1+x}\ dx\\ &=\int_0^1\left(-\int_0^1x^y\ dy\right)\frac1{1+x}\ dx\\ &=\int_0^1\left(-\int_0^1\frac{x^y}{1+x}\ dx\right)\ dy\\ &=\int_0^1\left((-1)^n\sum_{n=1}^\infty\int_0^1x^{y+n-1}\ dx\right)\ dy\\ &=\int_0^1\left(\sum_{n=1}^\infty\frac{(-1)^n}{y+n}\right)\ dy\\ &=\sum_{n=1}^\infty(-1)^n\int_0^1\frac1{y+n}\ dy\\ &=\sum_{n=1}^\infty(-1)^n\left[\ln(n+1)-\ln(n)\right]\tag{1} \end{align} Now how can we finish this alternating sum into $\ln\left(\frac2{\pi}\right)$ ? My idea was to use $$\operatorname{Li}_a(-1)=(1-2^{-a})\zeta(a)=\sum_{n=1}^\infty\frac{(-1)^n}{n^a}$$ and if we differentiate both sides with respect to $a$ we get $$2^{1-a}(\zeta^{'}(a)-\ln2\zeta(a))-\zeta^{'}(a)=\sum_{n=1}^\infty\frac{(-1)^{n-1}\ln(n)}{n^a}\tag{2}$$ wolfram says that $\sum_{n=1}^\infty(-1)^n\ln(n)$ is divergent which means we can not take the limit for (2) where $a\mapsto 0$ which means we can not break the sum in (1) into two sums. So any idea how to do (1)? Other question is, I tried to simplify the sum in (1) as follows \begin{align} S&=\sum_{n=1}^\infty(-1)^n\left[\ln(n+1)-\ln(n)\right]\\ &=-\left[\ln(2)-\ln(1)\right]+\left[\ln(3)-\ln(2)\right]-\left[\ln(4)-\ln(3)\right]+\left[\ln(5)-\ln(4)\right]-...\\ &=-2\ln(2)+2\ln(3)-2\ln(4)+...\\ &=2\sum_{n=1}^\infty(-1)^n\ln(n+1) \end{align} which is divergent again. what went wrong in my steps? Thanks.","A friend asked me to prove and here is my approach: Now how can we finish this alternating sum into ? My idea was to use and if we differentiate both sides with respect to we get wolfram says that is divergent which means we can not take the limit for (2) where which means we can not break the sum in (1) into two sums. So any idea how to do (1)? Other question is, I tried to simplify the sum in (1) as follows which is divergent again. what went wrong in my steps? Thanks.","\int_0^1\frac{1-x}{(\ln x)(1+x)}\ dx=\ln\left(\frac2{\pi}\right) \begin{align}
I&=\int_0^1\frac{1-x}{\ln x}\frac1{1+x}\ dx\\
&=\int_0^1\left(-\int_0^1x^y\ dy\right)\frac1{1+x}\ dx\\
&=\int_0^1\left(-\int_0^1\frac{x^y}{1+x}\ dx\right)\ dy\\
&=\int_0^1\left((-1)^n\sum_{n=1}^\infty\int_0^1x^{y+n-1}\ dx\right)\ dy\\
&=\int_0^1\left(\sum_{n=1}^\infty\frac{(-1)^n}{y+n}\right)\ dy\\
&=\sum_{n=1}^\infty(-1)^n\int_0^1\frac1{y+n}\ dy\\
&=\sum_{n=1}^\infty(-1)^n\left[\ln(n+1)-\ln(n)\right]\tag{1}
\end{align} \ln\left(\frac2{\pi}\right) \operatorname{Li}_a(-1)=(1-2^{-a})\zeta(a)=\sum_{n=1}^\infty\frac{(-1)^n}{n^a} a 2^{1-a}(\zeta^{'}(a)-\ln2\zeta(a))-\zeta^{'}(a)=\sum_{n=1}^\infty\frac{(-1)^{n-1}\ln(n)}{n^a}\tag{2} \sum_{n=1}^\infty(-1)^n\ln(n) a\mapsto 0 \begin{align}
S&=\sum_{n=1}^\infty(-1)^n\left[\ln(n+1)-\ln(n)\right]\\
&=-\left[\ln(2)-\ln(1)\right]+\left[\ln(3)-\ln(2)\right]-\left[\ln(4)-\ln(3)\right]+\left[\ln(5)-\ln(4)\right]-...\\
&=-2\ln(2)+2\ln(3)-2\ln(4)+...\\
&=2\sum_{n=1}^\infty(-1)^n\ln(n+1)
\end{align}","['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'definite-integrals']"
88,Counterexample to Bolzano-Weierstrass in infinite dimension,Counterexample to Bolzano-Weierstrass in infinite dimension,,"From Bolzano-Weirerstrass we can demonstrate that in a normed vector space $E$ of finite dimension, every bounded sequence admits a limit point. What are some counterexamples in infinite dimension? Does there exist a counterexample in every infinite dimensional normed space? I believe this one works: Let $E$ be the space of sequences of real numbers with finite support, equipped with the norm $\| (a_k)_{k \in  \mathbb{N} } \|=\sup |a_k|$. Then take define the sequence $(s_n)$ as follows: $s_n$ is the sequence whose $n$th term is $1$ and every other term is $0$. Then $(s_n)$ is bounded, and we can easily show that it has no limit point.","From Bolzano-Weirerstrass we can demonstrate that in a normed vector space $E$ of finite dimension, every bounded sequence admits a limit point. What are some counterexamples in infinite dimension? Does there exist a counterexample in every infinite dimensional normed space? I believe this one works: Let $E$ be the space of sequences of real numbers with finite support, equipped with the norm $\| (a_k)_{k \in  \mathbb{N} } \|=\sup |a_k|$. Then take define the sequence $(s_n)$ as follows: $s_n$ is the sequence whose $n$th term is $1$ and every other term is $0$. Then $(s_n)$ is bounded, and we can easily show that it has no limit point.",,"['real-analysis', 'vector-spaces']"
89,Theorem 3.3 (d) Rudin,Theorem 3.3 (d) Rudin,,"This from Rudin's Principle's of Mathematical Analysis I'm having trouble getting an intuitive picture of this proof. Can you please show me how Rudin gets to the inequality $|s_n -s|< \frac{1}{2} |s|^2 \epsilon$ algebraically? And also a description (I don't expect you draw it out, it's fine) of what this would look like in a pictorial?? I'm guessing it has something to do with the two triangles in the third one. Also point out if I interpreted the picture wrong in any way. Thanks in advance! This sites a life saver.","This from Rudin's Principle's of Mathematical Analysis I'm having trouble getting an intuitive picture of this proof. Can you please show me how Rudin gets to the inequality $|s_n -s|< \frac{1}{2} |s|^2 \epsilon$ algebraically? And also a description (I don't expect you draw it out, it's fine) of what this would look like in a pictorial?? I'm guessing it has something to do with the two triangles in the third one. Also point out if I interpreted the picture wrong in any way. Thanks in advance! This sites a life saver.",,"['real-analysis', 'sequences-and-series', 'general-topology', 'proof-explanation', 'epsilon-delta']"
90,In what way is the calculation of $\lim_{x \to \infty} \frac{4x^2}{x-2}$ wrong?,In what way is the calculation of  wrong?,\lim_{x \to \infty} \frac{4x^2}{x-2},"Is something wrong with the calculation below? $$ \begin{align} \lim_{x \to \infty} \frac{4x^2}{x-2} &= \lim_{x \to \infty} \frac{4x}{1-2/x} \\ & = \frac{(\lim_{x \to \infty} 4x)}{(\lim_{x \to \infty} 1-2/x)} \\ & = \frac{(\lim_{x \to \infty} 4x)}{1} \\ & = \lim_{x \to \infty} 4x \\ & = \infty. \end{align} $$ I ask because if there isn't then the following would seem correct, $$ \begin{align} \lim_{x \to \infty} \frac{4x^2}{x-2} - 4x &= \left( \lim_{x \to \infty} \frac{4x^2}{x-2} \right) - \left( \lim_{x \to \infty} 4x \right) \\ & = \left( \lim_{x \to \infty} 4x \right) - \left( \lim_{x \to \infty} 4x \right) \\ & = \lim_{x \to \infty} 4x - 4x \\ & = \lim_{x \to \infty} 0 \\ & = 0. \end{align} $$ But it is not, since $$ \begin{align} \lim_{x \to \infty} \frac{4x^2}{x-2} - 4x &= \lim_{x \to \infty} \frac{4x^2 - 4x(x-2)}{x-2} \\ &= \lim_{x \to \infty} \frac{8x}{x-2} \\ &= \lim_{x \to \infty} \frac{8}{1-2/x} \\ &= 8. \\ \end{align} $$ In what way is this wrong? Where is the mistake?","Is something wrong with the calculation below? $$ \begin{align} \lim_{x \to \infty} \frac{4x^2}{x-2} &= \lim_{x \to \infty} \frac{4x}{1-2/x} \\ & = \frac{(\lim_{x \to \infty} 4x)}{(\lim_{x \to \infty} 1-2/x)} \\ & = \frac{(\lim_{x \to \infty} 4x)}{1} \\ & = \lim_{x \to \infty} 4x \\ & = \infty. \end{align} $$ I ask because if there isn't then the following would seem correct, $$ \begin{align} \lim_{x \to \infty} \frac{4x^2}{x-2} - 4x &= \left( \lim_{x \to \infty} \frac{4x^2}{x-2} \right) - \left( \lim_{x \to \infty} 4x \right) \\ & = \left( \lim_{x \to \infty} 4x \right) - \left( \lim_{x \to \infty} 4x \right) \\ & = \lim_{x \to \infty} 4x - 4x \\ & = \lim_{x \to \infty} 0 \\ & = 0. \end{align} $$ But it is not, since $$ \begin{align} \lim_{x \to \infty} \frac{4x^2}{x-2} - 4x &= \lim_{x \to \infty} \frac{4x^2 - 4x(x-2)}{x-2} \\ &= \lim_{x \to \infty} \frac{8x}{x-2} \\ &= \lim_{x \to \infty} \frac{8}{1-2/x} \\ &= 8. \\ \end{align} $$ In what way is this wrong? Where is the mistake?",,"['real-analysis', 'calculus', 'limits', 'indeterminate-forms']"
91,Real Analysis advanced book suggestion,Real Analysis advanced book suggestion,,"I want to self study real analysis. So far, finished the first seven chapters of Baby Rudin (up to and including sequences and series of functions) and now want to proceed into more advanced books. I have couple options, including Stein&Shakarchi Folland Royden Rudin's Real&Complex Analysis Kolmogorov-Fomin Among these five (also happy to hear if you have further recommendations) which are more accessible and has better treatment of the material? I'm especially thinking among first three, so if there would be a comparative answer for the first three books, I would be really happy. Any help is appreciated. Thank you!","I want to self study real analysis. So far, finished the first seven chapters of Baby Rudin (up to and including sequences and series of functions) and now want to proceed into more advanced books. I have couple options, including Stein&Shakarchi Folland Royden Rudin's Real&Complex Analysis Kolmogorov-Fomin Among these five (also happy to hear if you have further recommendations) which are more accessible and has better treatment of the material? I'm especially thinking among first three, so if there would be a comparative answer for the first three books, I would be really happy. Any help is appreciated. Thank you!",,"['real-analysis', 'reference-request', 'book-recommendation']"
92,The Schwartz Class is dense in $L^p$,The Schwartz Class is dense in,L^p,Is there any hint to prove that for every $1 \le p < \infty $ the Schwartz Class is dense in $L^p$ ? Thanks so much.,Is there any hint to prove that for every the Schwartz Class is dense in ? Thanks so much.,1 \le p < \infty  L^p,"['real-analysis', 'functional-analysis', 'lebesgue-measure', 'lp-spaces', 'schwartz-space']"
93,All projections are continuous in a Banach space.,All projections are continuous in a Banach space.,,"I have this exercise: A linear operator $P: \Omega \rightarrow \Omega$ is called a   projection if both the range of P and $P^{-1}(\{0\})$ are closed and   $P(P(x))=P(x) \forall x$. Show that if $\Omega$ is a Banach space, then all projections of   $\Omega$ are continuous. This exercise is in the chapter of the open mapping theorem, and the closed graph theorem, so it is a pretty big hint that I am supposed to use one of these. My first attempt (open mapping): Since $P(\Omega)$ is closed, it is a Banach space. Then if we look at $P: \Omega \rightarrow P(\Omega)$, it is an open mapping. Let U be an open set of $\Omega$, then I need to show that $P^{-1}(U)$, is open. We have that $P^{-1}(U)=P^{-1}(U\cap P(\Omega))$, $U\cap P(\Omega)$ is an open set in $P(\Omega)$. So I need to show that $P^{-1}(U\cap P(\Omega))$ is open. I don't see how it beeing an open mapping helps me, because that is sadly the other way. I tried contradiction: Assume for contradiction that: $P^{-1}(U\cap P(\Omega))$ is not open. Then it must also contain a limit point for its complement.So then there exists x, so that $P(x)\in U$, but $z_n \rightarrow x$, $P(z_n)\ne U$. This implies among other things, that any image of any open ball around x, is both open(open mapping theorem), and contain elements from both U and its complement. But still, this is no contradiction as far as I see. Any tips? My second attempt (closed graph theorem). Since we are working in a Banach space, if I can show that P is a closed operator I will be done. So assume $\lim x_n = x$, then I must show that if $\lim T(x_n)$ converges, then $\lim T(x_n)=T(x)$. So assume that $\lim x_n = x$ and that $\lim T(x_n)=y$. Assume for contradiction that $y \ne T(x)$, then we are in the case where $\lim T(x_n)\ne T(x)\rightarrow \lim T(x_n-x)\ne 0$. Hence we are in the case where we have a sequnce $z_n\rightarrow 0$, but $T(z_n)$ converges, but it does not converge to 0. I can't get a contradiction in the case either. Can you guys please help me?","I have this exercise: A linear operator $P: \Omega \rightarrow \Omega$ is called a   projection if both the range of P and $P^{-1}(\{0\})$ are closed and   $P(P(x))=P(x) \forall x$. Show that if $\Omega$ is a Banach space, then all projections of   $\Omega$ are continuous. This exercise is in the chapter of the open mapping theorem, and the closed graph theorem, so it is a pretty big hint that I am supposed to use one of these. My first attempt (open mapping): Since $P(\Omega)$ is closed, it is a Banach space. Then if we look at $P: \Omega \rightarrow P(\Omega)$, it is an open mapping. Let U be an open set of $\Omega$, then I need to show that $P^{-1}(U)$, is open. We have that $P^{-1}(U)=P^{-1}(U\cap P(\Omega))$, $U\cap P(\Omega)$ is an open set in $P(\Omega)$. So I need to show that $P^{-1}(U\cap P(\Omega))$ is open. I don't see how it beeing an open mapping helps me, because that is sadly the other way. I tried contradiction: Assume for contradiction that: $P^{-1}(U\cap P(\Omega))$ is not open. Then it must also contain a limit point for its complement.So then there exists x, so that $P(x)\in U$, but $z_n \rightarrow x$, $P(z_n)\ne U$. This implies among other things, that any image of any open ball around x, is both open(open mapping theorem), and contain elements from both U and its complement. But still, this is no contradiction as far as I see. Any tips? My second attempt (closed graph theorem). Since we are working in a Banach space, if I can show that P is a closed operator I will be done. So assume $\lim x_n = x$, then I must show that if $\lim T(x_n)$ converges, then $\lim T(x_n)=T(x)$. So assume that $\lim x_n = x$ and that $\lim T(x_n)=y$. Assume for contradiction that $y \ne T(x)$, then we are in the case where $\lim T(x_n)\ne T(x)\rightarrow \lim T(x_n-x)\ne 0$. Hence we are in the case where we have a sequnce $z_n\rightarrow 0$, but $T(z_n)$ converges, but it does not converge to 0. I can't get a contradiction in the case either. Can you guys please help me?",,"['real-analysis', 'functional-analysis', 'banach-spaces']"
94,"Another integral $\int \frac{3 x^2+2 x+1}{ \left(x^3+x^2+x+2\right) \sqrt{1+\sqrt{x^3+x^2+x+2}}} \, dx$",Another integral,"\int \frac{3 x^2+2 x+1}{ \left(x^3+x^2+x+2\right) \sqrt{1+\sqrt{x^3+x^2+x+2}}} \, dx","Here is an indefinite integral that is similar to an integral I wanna propose for a contest. Apart from  using CAS, do you see any very easy way of calculating it? $$\int \frac{1+2x +3 x^2}{\left(2+x+x^2+x^3\right) \sqrt{1+\sqrt{2+x+x^2+x^3}}} \, dx$$ EDIT: It's a part from the generalization $$\int \frac{1+2x +3 x^2+\cdots n x^{n-1}}{\left(2+x+x^2+\cdots+ x^n\right) \sqrt{1\pm\sqrt{2+x+x^2+\cdots +x^n}}} \, dx$$ Supplementary question: How would you calculate the following integral using the generalization above? Would you prefer another way? $$\int_0^{1/2} \frac{1}{\left(x^2-3 x+2\right)\sqrt{\sqrt{\frac{x-2}{x-1}}+1} } \, dx$$ As a note, the generalization like the one you see above and slightly modified versions can be wisely used for calculating very hard integrals.","Here is an indefinite integral that is similar to an integral I wanna propose for a contest. Apart from  using CAS, do you see any very easy way of calculating it? $$\int \frac{1+2x +3 x^2}{\left(2+x+x^2+x^3\right) \sqrt{1+\sqrt{2+x+x^2+x^3}}} \, dx$$ EDIT: It's a part from the generalization $$\int \frac{1+2x +3 x^2+\cdots n x^{n-1}}{\left(2+x+x^2+\cdots+ x^n\right) \sqrt{1\pm\sqrt{2+x+x^2+\cdots +x^n}}} \, dx$$ Supplementary question: How would you calculate the following integral using the generalization above? Would you prefer another way? $$\int_0^{1/2} \frac{1}{\left(x^2-3 x+2\right)\sqrt{\sqrt{\frac{x-2}{x-1}}+1} } \, dx$$ As a note, the generalization like the one you see above and slightly modified versions can be wisely used for calculating very hard integrals.",,"['calculus', 'real-analysis', 'indefinite-integrals']"
95,Continuous and bounded imply uniform continuity?,Continuous and bounded imply uniform continuity?,,"I am thinking about this since couple hour. Is a continuous and bounded function $f:\mathbb R\to\mathbb R$ uniform continuous too? I didn't found a counter example and thus I tried to prove this like this: let $\epsilon>0$ and $a\in\mathbb R$. By continuity, there is a $\delta_a>0$ such that $$d(f(x),f(y))<d(f(x),f(a))+d(f(y),f(a))<\epsilon$$ if $d(x,y)\leq d(x,a)+d(y,a)<\delta_a.$ Then, if $\inf_a\delta_a>0$ I set $\delta=\inf_a\delta_a$ and we are done since $d(f(x),f(y))<\epsilon$ if $d(x,y)<\delta,$ but I didn't success to prove that $\delta\neq 0$.","I am thinking about this since couple hour. Is a continuous and bounded function $f:\mathbb R\to\mathbb R$ uniform continuous too? I didn't found a counter example and thus I tried to prove this like this: let $\epsilon>0$ and $a\in\mathbb R$. By continuity, there is a $\delta_a>0$ such that $$d(f(x),f(y))<d(f(x),f(a))+d(f(y),f(a))<\epsilon$$ if $d(x,y)\leq d(x,a)+d(y,a)<\delta_a.$ Then, if $\inf_a\delta_a>0$ I set $\delta=\inf_a\delta_a$ and we are done since $d(f(x),f(y))<\epsilon$ if $d(x,y)<\delta,$ but I didn't success to prove that $\delta\neq 0$.",,"['real-analysis', 'examples-counterexamples', 'uniform-continuity']"
96,Is there a set non-countable around zero but countable elsewhere?,Is there a set non-countable around zero but countable elsewhere?,,"Is there a set $A$ such that for every $a,r>0$ we have that $(0,r)\cap A$ is non-countable but $(a,a+r)\cap A$ is countable?",Is there a set such that for every we have that is non-countable but is countable?,"A a,r>0 (0,r)\cap A (a,a+r)\cap A","['real-analysis', 'general-topology']"
97,Why is the empty set considered an interval?,Why is the empty set considered an interval?,,What is the definition of an interval and why is the empty set an interval by that definition?,What is the definition of an interval and why is the empty set an interval by that definition?,,"['real-analysis', 'analysis', 'definition']"
98,Find $\lim_{n\to \infty}({1\over \sqrt{n^2+1}}+{1\over \sqrt{n^2+2}}+\cdots+{1\over \sqrt{n^2+n}})$ [closed],Find  [closed],\lim_{n\to \infty}({1\over \sqrt{n^2+1}}+{1\over \sqrt{n^2+2}}+\cdots+{1\over \sqrt{n^2+n}}),"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question strong text Find $\lim_\limits{n\to \infty}\left({1\over \sqrt{n^2+1}}+{1\over \sqrt{n^2+2}}+\cdots+{1\over \sqrt{n^2+n}}\right)$ . I do know it is bounded by $1$ . I tried using the sandwich rule with no success. How can I solve it?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question strong text Find . I do know it is bounded by . I tried using the sandwich rule with no success. How can I solve it?",\lim_\limits{n\to \infty}\left({1\over \sqrt{n^2+1}}+{1\over \sqrt{n^2+2}}+\cdots+{1\over \sqrt{n^2+n}}\right) 1,"['calculus', 'real-analysis', 'limits']"
99,"If two functions are in Schwartz space,then their convolution is also in Schwartz space","If two functions are in Schwartz space,then their convolution is also in Schwartz space",,"For the proposition (i), I don't know how to show the inequality in the second line of the proof, can someone help me?","For the proposition (i), I don't know how to show the inequality in the second line of the proof, can someone help me?",,"['real-analysis', 'schwartz-space']"
