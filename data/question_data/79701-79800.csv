,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"""Pivot step"" that Donald Knuth mentioned in TAOCP","""Pivot step"" that Donald Knuth mentioned in TAOCP",,"I stumbled upon the following operation on matrices in section 2.2.6 Arrays and Orthogonal Lists in the first volume of The Art of Computer Programming , in an example of working with sparse matrices: $$ \begin{pmatrix} &\vdots&&\vdots&\\ \dots&a&\dots&b&\dots\\ &\vdots&&\vdots&\\ \dots&c&\dots&d&\dots\\ &\vdots&&\vdots& \end{pmatrix} \leadsto \begin{pmatrix} &\vdots&&\vdots&\\ \dots&1/a&\dots&b/a&\dots\\ &\vdots&&\vdots&\\ \dots&-c/a&\dots&d-bc/a&\dots\\ &\vdots&&\vdots& \end{pmatrix} $$ (see it on Google Books ). This operation is called there a ""pivot step,"" the pivot in this case beeing the ""$a$"" entry in the first matrix. This operation is stated to be used in algorithms for inverting matrices and solving [systems of] linear equations, and in the simplex method. It is also cited here on page 141. I do not know the simplex method, but I know how to solve systems of linear equations, and usually all operations involved are elementary row transformations. However, this ""pivot step"" operation clearly cannot be obtained as a composition of elementary row transformations, neither does it preserve the rank of the matrix (consider the case of a $2\times 2$ matrix with $a = b = c = d$), and I do not understand its ""meaning."" What is this operation?","I stumbled upon the following operation on matrices in section 2.2.6 Arrays and Orthogonal Lists in the first volume of The Art of Computer Programming , in an example of working with sparse matrices: $$ \begin{pmatrix} &\vdots&&\vdots&\\ \dots&a&\dots&b&\dots\\ &\vdots&&\vdots&\\ \dots&c&\dots&d&\dots\\ &\vdots&&\vdots& \end{pmatrix} \leadsto \begin{pmatrix} &\vdots&&\vdots&\\ \dots&1/a&\dots&b/a&\dots\\ &\vdots&&\vdots&\\ \dots&-c/a&\dots&d-bc/a&\dots\\ &\vdots&&\vdots& \end{pmatrix} $$ (see it on Google Books ). This operation is called there a ""pivot step,"" the pivot in this case beeing the ""$a$"" entry in the first matrix. This operation is stated to be used in algorithms for inverting matrices and solving [systems of] linear equations, and in the simplex method. It is also cited here on page 141. I do not know the simplex method, but I know how to solve systems of linear equations, and usually all operations involved are elementary row transformations. However, this ""pivot step"" operation clearly cannot be obtained as a composition of elementary row transformations, neither does it preserve the rank of the matrix (consider the case of a $2\times 2$ matrix with $a = b = c = d$), and I do not understand its ""meaning."" What is this operation?",,"['matrices', 'numerical-linear-algebra', 'gaussian-elimination', 'sparse-matrices']"
1,Is there a generalization of matrices that allows uncountably many entries?,Is there a generalization of matrices that allows uncountably many entries?,,"For example, the matrix could have finitely many rows and columns, but each row/column has uncountably many elements and you can do the standard matrix multiplication by taking care to match up the entries with corresponding pairs of real number indices. Do such objects exist and has there been any work on them? Does","For example, the matrix could have finitely many rows and columns, but each row/column has uncountably many elements and you can do the standard matrix multiplication by taking care to match up the entries with corresponding pairs of real number indices. Do such objects exist and has there been any work on them? Does",,['matrices']
2,"Why, if principal ideals are different, they are not 2-sided?","Why, if principal ideals are different, they are not 2-sided?",,"There is an argument about ring ideals which I do not understand. Let $R$ be a ring with $1$,   $$a=\begin{bmatrix}1&0\\0&0\end{bmatrix},$$ $I=\{a\}\cdot M_2(R)$,   $J=M_2(R)\cdot\{a\}$. $I$ is a right ideal and $J$ is a left ideal of   $M_2(R)$. Observe that, if $R$ is not trivial, $I\not=J$ and hence   none of them is 2-sided. I do not understand this “hence”. How is $I\not=J$ relevant? I can prove that $J$ is not a right ideal in a concrete way: the right column of every member of $J$ is zero and $$\begin{bmatrix}1&0\\0&0\end{bmatrix}\cdot\begin{bmatrix}0&1\\0&0\end{bmatrix} = \begin{bmatrix}0&1\\0&0\end{bmatrix}.$$ I guess I need to prove something like “if $J=S\cdot\{a\} $ is a right ideal, then $J=\{a\}\cdot S $” for an abstract ring $S$ with $1$. I proved the following. $1\in S$, $1\cdot a\in J $, $a\in J $. [Let $s\in S$ be given. $a\cdot s\in J $ because $J$ is a right ideal.] Hence $\{a\}\cdot S\subseteq J $. What next? [Update 2017-09-12 19:48:28+03:00. It seems that this is a mistake in the textbook. I publish the parameters of the textbook so the authors can fix it. Menini, Claudia, and Freddy Van Oystaeyen. Abstract Algebra: A Comprehensive Treatment. Marcel Dekker, 2004.]","There is an argument about ring ideals which I do not understand. Let $R$ be a ring with $1$,   $$a=\begin{bmatrix}1&0\\0&0\end{bmatrix},$$ $I=\{a\}\cdot M_2(R)$,   $J=M_2(R)\cdot\{a\}$. $I$ is a right ideal and $J$ is a left ideal of   $M_2(R)$. Observe that, if $R$ is not trivial, $I\not=J$ and hence   none of them is 2-sided. I do not understand this “hence”. How is $I\not=J$ relevant? I can prove that $J$ is not a right ideal in a concrete way: the right column of every member of $J$ is zero and $$\begin{bmatrix}1&0\\0&0\end{bmatrix}\cdot\begin{bmatrix}0&1\\0&0\end{bmatrix} = \begin{bmatrix}0&1\\0&0\end{bmatrix}.$$ I guess I need to prove something like “if $J=S\cdot\{a\} $ is a right ideal, then $J=\{a\}\cdot S $” for an abstract ring $S$ with $1$. I proved the following. $1\in S$, $1\cdot a\in J $, $a\in J $. [Let $s\in S$ be given. $a\cdot s\in J $ because $J$ is a right ideal.] Hence $\{a\}\cdot S\subseteq J $. What next? [Update 2017-09-12 19:48:28+03:00. It seems that this is a mistake in the textbook. I publish the parameters of the textbook so the authors can fix it. Menini, Claudia, and Freddy Van Oystaeyen. Abstract Algebra: A Comprehensive Treatment. Marcel Dekker, 2004.]",,"['abstract-algebra', 'matrices', 'ring-theory', 'ideals']"
3,Dirac notation - Outer product representation of Normal Matrix,Dirac notation - Outer product representation of Normal Matrix,,"I'm studying some linear algebra applications in quantum mechanics, and I was told that a normal matrix can be written as: $$ M=\sum_{i=1}^{n}\theta_i |\theta_i\rangle \langle\theta_i| $$ where $|\theta_i\rangle$ is the eigenvector associated with it's eigenvalue $\theta_i$. The problem is that I can't properly visualize that summation as a normal matrix representation. Here's my attempt to visualize why that's true. I know, by spectrum theorem, that I can diagonalize that matrix M by some unitary matrices: $$ D = U^{\dagger}MU \Rightarrow U^{\dagger}\big(\sum_{i=1}^{n}\theta_i |\theta_i\rangle \langle\theta_i|\big)U $$ So if I manage to calculate the right relation, I'll get why the matrix $M$ can be written as it was said, but how can I do that? How can I include $U$ and $U^{\dagger}$ into that summation to calculate it? Can someone please show me what's really happening in that summation? What I've been able to get is: $$ \theta_i|\theta_i\rangle $$ Is a scalar times a ""column"" vector. $$ \langle\theta_i| $$ Is a bra, or a conjugate transpose ket. $$ \theta_i |\theta_i\rangle \langle\theta_i| $$ Is a matrix, and the summation is actually adding multiple matrices with previous outer product computation. Can someone please help me out? Thanks!","I'm studying some linear algebra applications in quantum mechanics, and I was told that a normal matrix can be written as: $$ M=\sum_{i=1}^{n}\theta_i |\theta_i\rangle \langle\theta_i| $$ where $|\theta_i\rangle$ is the eigenvector associated with it's eigenvalue $\theta_i$. The problem is that I can't properly visualize that summation as a normal matrix representation. Here's my attempt to visualize why that's true. I know, by spectrum theorem, that I can diagonalize that matrix M by some unitary matrices: $$ D = U^{\dagger}MU \Rightarrow U^{\dagger}\big(\sum_{i=1}^{n}\theta_i |\theta_i\rangle \langle\theta_i|\big)U $$ So if I manage to calculate the right relation, I'll get why the matrix $M$ can be written as it was said, but how can I do that? How can I include $U$ and $U^{\dagger}$ into that summation to calculate it? Can someone please show me what's really happening in that summation? What I've been able to get is: $$ \theta_i|\theta_i\rangle $$ Is a scalar times a ""column"" vector. $$ \langle\theta_i| $$ Is a bra, or a conjugate transpose ket. $$ \theta_i |\theta_i\rangle \langle\theta_i| $$ Is a matrix, and the summation is actually adding multiple matrices with previous outer product computation. Can someone please help me out? Thanks!",,"['linear-algebra', 'matrices', 'mathematical-physics', 'quantum-mechanics']"
4,Show that $M=\alpha I_n\iff$ no matrix in $S$ has a zero anywhere on its diagonal.,Show that  no matrix in  has a zero anywhere on its diagonal.,M=\alpha I_n\iff S,"Let $M$ be a square complex matrix, and let $S = \{XMX^{-1} | \det(X)\neq 0 \}$ i.e.  $S$ is the set of all matrices similar to $M$   . Show that $M=\alpha I$ for some $\alpha\neq 0$ if and only if no matrix in $S$ has a zero anywhere on its diagonal. The only if part is trivial. No idea on the other half...","Let $M$ be a square complex matrix, and let $S = \{XMX^{-1} | \det(X)\neq 0 \}$ i.e.  $S$ is the set of all matrices similar to $M$   . Show that $M=\alpha I$ for some $\alpha\neq 0$ if and only if no matrix in $S$ has a zero anywhere on its diagonal. The only if part is trivial. No idea on the other half...",,"['linear-algebra', 'matrices']"
5,Diagonal dominance preserved by row elimination,Diagonal dominance preserved by row elimination,,"I'm trying complete a proof I've seen in a class, which says that diagonally dominant matrices have an LU decomposition. For this, given a diagonally dominant matrix $A \in \mathbb{R}^{n \times n}$  we know that if sequence $A^{(0)}, \ \dots \ , \   A^{(n-1)} $ defined as $$ A^{(0)} = A \\  a_{ij}^{(k)} = a_{ij}^{(k-1)} -  \frac{a_{ik}^{(k-1)} \cdot a_{kj}^{(k-1)}}{a_{kk}^{(k-1)}}  $$ which consists of each step of gaussian elimination without pivoting is well defined, (i.e $\ a_{kk}^{(k-1)} \neq 0 $ for each $k$), then there is a LU decomposition. Therefore, it is sufficient to show the stronger statement: If $A^{(k)}$ is diagonally dominant, $A^{(k+1)}$ is as well. Now, here is where I've lost track of my notes. I know this has to be true but I've failed to succeed in proving it. We want to show that $$ \sum_{j \neq i}|a_{ij}^{(k)}| < |a_{ii}^{(k)}| $$ which is equivalent to $$ \sum_{j \neq i}|a_{ij}^{(k-1)} -  \frac{a_{ik}^{(k-1)} \cdot a_{kj}^{(k-1)}}{a_{kk}^{(k-1)}}| < |a_{ii}^{(k-1)} -  \frac{a_{ik}^{(k-1)} \cdot a_{ki}^{(k-1)}}{a_{kk}^{(k-1)}}| $$ Any thoughts?","I'm trying complete a proof I've seen in a class, which says that diagonally dominant matrices have an LU decomposition. For this, given a diagonally dominant matrix $A \in \mathbb{R}^{n \times n}$  we know that if sequence $A^{(0)}, \ \dots \ , \   A^{(n-1)} $ defined as $$ A^{(0)} = A \\  a_{ij}^{(k)} = a_{ij}^{(k-1)} -  \frac{a_{ik}^{(k-1)} \cdot a_{kj}^{(k-1)}}{a_{kk}^{(k-1)}}  $$ which consists of each step of gaussian elimination without pivoting is well defined, (i.e $\ a_{kk}^{(k-1)} \neq 0 $ for each $k$), then there is a LU decomposition. Therefore, it is sufficient to show the stronger statement: If $A^{(k)}$ is diagonally dominant, $A^{(k+1)}$ is as well. Now, here is where I've lost track of my notes. I know this has to be true but I've failed to succeed in proving it. We want to show that $$ \sum_{j \neq i}|a_{ij}^{(k)}| < |a_{ii}^{(k)}| $$ which is equivalent to $$ \sum_{j \neq i}|a_{ij}^{(k-1)} -  \frac{a_{ik}^{(k-1)} \cdot a_{kj}^{(k-1)}}{a_{kk}^{(k-1)}}| < |a_{ii}^{(k-1)} -  \frac{a_{ik}^{(k-1)} \cdot a_{ki}^{(k-1)}}{a_{kk}^{(k-1)}}| $$ Any thoughts?",,"['matrices', 'numerical-methods', 'gaussian-elimination']"
6,How to prove this $A$ has full rank and the number of positive eigenvalues of $A$ is one?,How to prove this  has full rank and the number of positive eigenvalues of  is one?,A A,"Suppose $A$ is a $3\times 3$ symmetric matrix such that $$\begin{bmatrix}x & y &1\end{bmatrix} A \begin{bmatrix}x \\y & \\1 \end{bmatrix}=xy-1$$ How to prove $A$ has full rank and the number of positive eigenvalues of $A$ is one? My try: $$\begin{bmatrix}x & y &1\end{bmatrix} \begin{bmatrix}a & b &c \\b & d & e \\c & e & f \end{bmatrix} \begin{bmatrix}x \\y  \\1 \end{bmatrix}=xy-1$$ $$\Rightarrow \begin{bmatrix}x & y &1\end{bmatrix} \begin{bmatrix}ax+by+c \\bx+dy+e  \\cx+ey+f \end{bmatrix}=xy-1$$ $$\Rightarrow ax^2+2bxy+2cx+dy^2+2ey+f=xy-1 $$ $$\Rightarrow a=c=d=e=0\; \text{and} \;b=1/2, f=-1$$ So the matrix becomes $$\begin{bmatrix}0 & 1/2 &0 \\1/2 & 0 & 0 \\0 & 0 & -1 \end{bmatrix}$$ which has only one positive eigenvalue, namely, $\frac 12$ . Also this matrix is invertible and, hence, full rank. Is this way correct or anything I'm doing wrong? What is the general idea?","Suppose is a symmetric matrix such that How to prove has full rank and the number of positive eigenvalues of is one? My try: So the matrix becomes which has only one positive eigenvalue, namely, . Also this matrix is invertible and, hence, full rank. Is this way correct or anything I'm doing wrong? What is the general idea?","A 3\times 3 \begin{bmatrix}x & y &1\end{bmatrix} A \begin{bmatrix}x \\y & \\1 \end{bmatrix}=xy-1 A A \begin{bmatrix}x & y &1\end{bmatrix} \begin{bmatrix}a & b &c \\b & d & e \\c & e & f \end{bmatrix} \begin{bmatrix}x \\y  \\1 \end{bmatrix}=xy-1 \Rightarrow \begin{bmatrix}x & y &1\end{bmatrix} \begin{bmatrix}ax+by+c \\bx+dy+e  \\cx+ey+f \end{bmatrix}=xy-1 \Rightarrow ax^2+2bxy+2cx+dy^2+2ey+f=xy-1  \Rightarrow a=c=d=e=0\; \text{and} \;b=1/2, f=-1 \begin{bmatrix}0 & 1/2 &0 \\1/2 & 0 & 0 \\0 & 0 & -1 \end{bmatrix} \frac 12","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'quadratic-forms', 'symmetric-matrices']"
7,"$X=ABA^T$, X, A are matrix and need to solve matrix B by enforcing it as a diagonal matrix.",", X, A are matrix and need to solve matrix B by enforcing it as a diagonal matrix.",X=ABA^T,"$X=ABA^T$,  X is a square matrix and A is rectangular matrix and I need to solve matrix B by enforcing it as a diagonal matrix. For non-singular matrix we can write B as $A^+XA^{+T}$, but B gets to be non-diagonal matrix. How can I get B as diagonal matrix. (Eigen decomposition of X gives different values of A).","$X=ABA^T$,  X is a square matrix and A is rectangular matrix and I need to solve matrix B by enforcing it as a diagonal matrix. For non-singular matrix we can write B as $A^+XA^{+T}$, but B gets to be non-diagonal matrix. How can I get B as diagonal matrix. (Eigen decomposition of X gives different values of A).",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'least-squares']"
8,"linear algebra: image, kernel and transpose","linear algebra: image, kernel and transpose",,"Given that $A$ is a $(n\times n)$ skew-symmetric matrix $(A^t = -A)$   and that $\mathrm{Img}(A) = \ker(A)$. Note here $n$ must be even from the rank nullity theorem, $\mathrm{rank}(A) =  \dim (\ker A )= n/2$. I need to show that there exists a $n/2$-dim subspace $V\subset  \mathbb{R}^n$ such that the bilinear form $\langle v, Aw\rangle$ is   non-degenerate. My question is that even on $\mathbb{R}^n$, since we know  $$\mathrm{Img}(A) = \ker(A^T)^\perp =\ker(- A)^\perp = \ker(A)^\perp $$ together with the given condition $\mathrm{Img}(A) = \ker(A)$, we would have $\ker(A)^\perp =  \ker(A)$ but this is impossible.","Given that $A$ is a $(n\times n)$ skew-symmetric matrix $(A^t = -A)$   and that $\mathrm{Img}(A) = \ker(A)$. Note here $n$ must be even from the rank nullity theorem, $\mathrm{rank}(A) =  \dim (\ker A )= n/2$. I need to show that there exists a $n/2$-dim subspace $V\subset  \mathbb{R}^n$ such that the bilinear form $\langle v, Aw\rangle$ is   non-degenerate. My question is that even on $\mathbb{R}^n$, since we know  $$\mathrm{Img}(A) = \ker(A^T)^\perp =\ker(- A)^\perp = \ker(A)^\perp $$ together with the given condition $\mathrm{Img}(A) = \ker(A)$, we would have $\ker(A)^\perp =  \ker(A)$ but this is impossible.",,"['linear-algebra', 'matrices', 'bilinear-form']"
9,Computing the partial trace of a matrix,Computing the partial trace of a matrix,,"I'm trying to understand the Wikipedia article on the partial trace. I do not understand their notation for the matrix elements of a tensor product of operators; that is, the object $\{a_{k l,ij}\}$ in the first section under the heading ""details"" at this link. I think a simple example will make this clear. Let $$A = \left[ \begin{array}{cc}1 & 2\\ 3 & 4 \end{array}\right], \quad B= \left[ \begin{array}{cc}8 & 9\\ 7 & 6 \end{array}\right].$$ And also let these matrices act on vector spaces $V = \mathbf{R}^2$ and $W = \mathbf{R}^2$ respectively. Then $$A \otimes B = \left[\begin{array}{cccc}8 & 9 & 16 & 18 \\ 7 & 6 & 14 & 12 \\ 24 & 27 & 32 & 36 \\ 21 & 18 & 28 & 24\end{array} \right] (""="" \{a_{k l,ij}\}).$$ Now clearly from the definitions of $A$ and $B$ we can tell that $$\operatorname{Tr}_W (A \otimes B ) = \operatorname{Tr}(B)\,A = 14\times \left[ \begin{array}{cc}1 & 2\\ 3 & 4 \end{array}\right] = \left[ \begin{array}{cc}14 & 28\\ 42 & 56 \end{array}\right](""="" \{b_{ki}\})$$ But I would like to see how this could be obtained directly from the matrix representation of $A \otimes B.$ In particular, how am I supposed to interpret the 4-index object $\{a_{k l,ij}\}$ in the wikipedia article here? How do I compute $b_{ki} = \sum_j a_{kj,ij}$? Thanks! Edit: I thought the solution was that: In fact the object $\{a_{kl,ij}\}$ is a two-index object, as we should expect it to be. The subscript contains products of the numbers $k, l$ and $i, j$. The comma separates the two indices. But this cannot be true. Because using this, we compute the off-diagonal element $b_{12} = a_{12} + a_{24} = 19 \neq 28$. We instead want the off-diagonal element $b_{12} = a_{13} + a_{24} = 28,$ but I do not see how the prescription from wikipedia furnishes this. Perhaps I am making some very simple arithmetic error.","I'm trying to understand the Wikipedia article on the partial trace. I do not understand their notation for the matrix elements of a tensor product of operators; that is, the object $\{a_{k l,ij}\}$ in the first section under the heading ""details"" at this link. I think a simple example will make this clear. Let $$A = \left[ \begin{array}{cc}1 & 2\\ 3 & 4 \end{array}\right], \quad B= \left[ \begin{array}{cc}8 & 9\\ 7 & 6 \end{array}\right].$$ And also let these matrices act on vector spaces $V = \mathbf{R}^2$ and $W = \mathbf{R}^2$ respectively. Then $$A \otimes B = \left[\begin{array}{cccc}8 & 9 & 16 & 18 \\ 7 & 6 & 14 & 12 \\ 24 & 27 & 32 & 36 \\ 21 & 18 & 28 & 24\end{array} \right] (""="" \{a_{k l,ij}\}).$$ Now clearly from the definitions of $A$ and $B$ we can tell that $$\operatorname{Tr}_W (A \otimes B ) = \operatorname{Tr}(B)\,A = 14\times \left[ \begin{array}{cc}1 & 2\\ 3 & 4 \end{array}\right] = \left[ \begin{array}{cc}14 & 28\\ 42 & 56 \end{array}\right](""="" \{b_{ki}\})$$ But I would like to see how this could be obtained directly from the matrix representation of $A \otimes B.$ In particular, how am I supposed to interpret the 4-index object $\{a_{k l,ij}\}$ in the wikipedia article here? How do I compute $b_{ki} = \sum_j a_{kj,ij}$? Thanks! Edit: I thought the solution was that: In fact the object $\{a_{kl,ij}\}$ is a two-index object, as we should expect it to be. The subscript contains products of the numbers $k, l$ and $i, j$. The comma separates the two indices. But this cannot be true. Because using this, we compute the off-diagonal element $b_{12} = a_{12} + a_{24} = 19 \neq 28$. We instead want the off-diagonal element $b_{12} = a_{13} + a_{24} = 28,$ but I do not see how the prescription from wikipedia furnishes this. Perhaps I am making some very simple arithmetic error.",,"['linear-algebra', 'matrices', 'linear-transformations', 'trace', 'index-notation']"
10,Does multiplying with a unitary matrix change the spectral norm of a matrix?,Does multiplying with a unitary matrix change the spectral norm of a matrix?,,"We know that the spectral norm of a matrix $A \in \Bbb K(n,n)$ $$\left \| A \right \| _2=\sqrt{\lambda_{\text{max}}(A^{^*}A)}$$ I need to prove that multiplying with an unitary matrix $U \in U(n)$ from the left or right does not change the value of the norm i.e. $$\left \| A \right \| _2 = \left \| UA \right \| _2 = \left \| AU \right \| _2$$ I was able to prove that  $$\left \| UA \right \| _2=\sqrt{\lambda_{\text{max}}((UA)^{^*}(UA))}$$ $$=\sqrt{\lambda_{\text{max}}(A^{^*}U^{^*}UA)}$$ $$=\sqrt{\lambda_{\text{max}}(A^{^*}A)}=\left \| A \right \| _2$$ since for unitary matrices $$U^{^*}U = I$$ I have no idea how to prove the the argument when multiplying from the right. This has been my approach. $$\left \| AU \right \| _2=\sqrt{\lambda_{\text{max}}((AU)^{^*}(AU))}$$ $$=\sqrt{\lambda_{\text{max}}(U^{^*}A^{^*}AU)}$$ I've really got no idea on what to do next. Any help is much appreciated.","We know that the spectral norm of a matrix $A \in \Bbb K(n,n)$ $$\left \| A \right \| _2=\sqrt{\lambda_{\text{max}}(A^{^*}A)}$$ I need to prove that multiplying with an unitary matrix $U \in U(n)$ from the left or right does not change the value of the norm i.e. $$\left \| A \right \| _2 = \left \| UA \right \| _2 = \left \| AU \right \| _2$$ I was able to prove that  $$\left \| UA \right \| _2=\sqrt{\lambda_{\text{max}}((UA)^{^*}(UA))}$$ $$=\sqrt{\lambda_{\text{max}}(A^{^*}U^{^*}UA)}$$ $$=\sqrt{\lambda_{\text{max}}(A^{^*}A)}=\left \| A \right \| _2$$ since for unitary matrices $$U^{^*}U = I$$ I have no idea how to prove the the argument when multiplying from the right. This has been my approach. $$\left \| AU \right \| _2=\sqrt{\lambda_{\text{max}}((AU)^{^*}(AU))}$$ $$=\sqrt{\lambda_{\text{max}}(U^{^*}A^{^*}AU)}$$ I've really got no idea on what to do next. Any help is much appreciated.",,['matrices']
11,"Given $A$, is there a quick way to compute the $(i,j)$th entry of $A^n$","Given , is there a quick way to compute the th entry of","A (i,j) A^n","I am studying elementary graph theory and there is a theorem which states that if you take the nth power of the adjacency matrix, the $(i,j)$ entry corresponds to the number of walks from $v_i$ to $v_j$ with length $n$. Is there a quick way to compute that one entry without having to compute the whole matrix? For example: $$A = \begin{bmatrix} 0 & 2 & 0\\  2 & 0 & 1\\  0 & 1 & 1 \end{bmatrix}$$ and $$A^2 = \begin{bmatrix} 4 & 0 & 2\\  0 & 5 & 1\\  2 & 1 & 2 \end{bmatrix}$$ This means that there are 5 walks of length 2 from v2 to v2 in the graph represented by A. Is there a method to directly compute the $(2,2)$ entry of $A^2$ without computing the whole matrix? What about other powers like $n = 3$ or $n = 4$?","I am studying elementary graph theory and there is a theorem which states that if you take the nth power of the adjacency matrix, the $(i,j)$ entry corresponds to the number of walks from $v_i$ to $v_j$ with length $n$. Is there a quick way to compute that one entry without having to compute the whole matrix? For example: $$A = \begin{bmatrix} 0 & 2 & 0\\  2 & 0 & 1\\  0 & 1 & 1 \end{bmatrix}$$ and $$A^2 = \begin{bmatrix} 4 & 0 & 2\\  0 & 5 & 1\\  2 & 1 & 2 \end{bmatrix}$$ This means that there are 5 walks of length 2 from v2 to v2 in the graph represented by A. Is there a method to directly compute the $(2,2)$ entry of $A^2$ without computing the whole matrix? What about other powers like $n = 3$ or $n = 4$?",,"['matrices', 'graph-theory']"
12,How to express complex multiplication with a linear operator on $\mathbb R^n$?,How to express complex multiplication with a linear operator on ?,\mathbb R^n,"I've recently been trying to find analogues between linear algebra ideas on $\mathbb R^2$ and $\mathbb C$, such as how the function $\textrm{Re}$ on $\mathbb C$ is equivalent to the linear operator given by the projection matrix $\begin{bmatrix}1 & 0 \\ 0 & 0\end{bmatrix}$ on $\mathbb R^2$. I now want to do the same thing with complex multiplication. One constraint is that I really want to find a matrix that gives complex multiplication that is not a function of one of the arguments. With the cross-product, for example, we can write $u \times v = Av$ but $A$ depends on $u$. I want to find a linear operator $A$ that encodes complex multiplication independent of the arguments, just like the projection matrix for $\textrm{Re}$. At first I was seeking a linear operator $A : \mathbb R^2 \times \mathbb R^2 \to \mathbb R^2$, but I don't know to do this with a matrix. The only things I can think of are $Av$ or $u^T A v$ but the first has the wrong domain and the second has the wrong codomain. My next idea was to consider $A: \mathbb R^4 \to \mathbb R^2$ so essentially I'd just stack the two, but this would require having $ac-bd = r_1a + r_2b + r_3c + r_4d$ which is impossible with my goal of fixed $r_i$ independent of the arguments. Am I wrong about this being linear? I know it's at least bilinear, but did I make a mistake here? To summarize all of this, I want to see if there's a way to represent complex multiplication using a fixed matrix $A$ between real-valued vector spaces.","I've recently been trying to find analogues between linear algebra ideas on $\mathbb R^2$ and $\mathbb C$, such as how the function $\textrm{Re}$ on $\mathbb C$ is equivalent to the linear operator given by the projection matrix $\begin{bmatrix}1 & 0 \\ 0 & 0\end{bmatrix}$ on $\mathbb R^2$. I now want to do the same thing with complex multiplication. One constraint is that I really want to find a matrix that gives complex multiplication that is not a function of one of the arguments. With the cross-product, for example, we can write $u \times v = Av$ but $A$ depends on $u$. I want to find a linear operator $A$ that encodes complex multiplication independent of the arguments, just like the projection matrix for $\textrm{Re}$. At first I was seeking a linear operator $A : \mathbb R^2 \times \mathbb R^2 \to \mathbb R^2$, but I don't know to do this with a matrix. The only things I can think of are $Av$ or $u^T A v$ but the first has the wrong domain and the second has the wrong codomain. My next idea was to consider $A: \mathbb R^4 \to \mathbb R^2$ so essentially I'd just stack the two, but this would require having $ac-bd = r_1a + r_2b + r_3c + r_4d$ which is impossible with my goal of fixed $r_i$ independent of the arguments. Am I wrong about this being linear? I know it's at least bilinear, but did I make a mistake here? To summarize all of this, I want to see if there's a way to represent complex multiplication using a fixed matrix $A$ between real-valued vector spaces.",,"['linear-algebra', 'matrices', 'complex-numbers', 'vectors']"
13,Norm of a skew symmetric unitary matrix,Norm of a skew symmetric unitary matrix,,"Let $U \in \mathbb{R}^{n \times n}$ be a unitary matrix, $U$ can be nonsymmetric, its eigenvalues can be complex numbers and all have modulus $1$. Is there an upper bound for the maximum singular value of its skew symmetric part (which is not necessarily unitary) depending on its eigenvalues? i.e.: Is there an $f$ such that $\left\|\frac{U - U^T}{2} \right\|_2 = \sigma_\text{max}\left(\frac{U - U^T}{2}\right) \le  f\left(\lambda_i\left(U\right)\right)$ ? More details: Observe that if $U=I$ (eigenvalues are real) $\Rightarrow \left\|\frac{U - U^T}{2} \right\|_2 = \sigma_\text{max}\left(\frac{U - U^T}{2}\right) = 0$, and if $U$ is skew-symmetric (eigenvalues purely imaginary) $\Rightarrow\left\|\frac{U - U^T}{2} \right\|_2 = \sigma_\text{max}\left(\frac{U - U^T}{2}\right) = 1$. Therefore there is a relationship between the norm $\left\|\frac{U - U^T}{2} \right\|_2 = \sigma_\text{max}\left(\frac{U - U^T}{2}\right)$ and the argument of the eigenvalues of $U$, i.e. $f\left(\lambda_i\left(U\right)\right) = f\left(\text{arg}(\lambda_i\left(U\right))\right)$. Further notes: in my work $U$ is the unitary factor of the polar decomposition of an M-matrix, but this may be irrelevant.","Let $U \in \mathbb{R}^{n \times n}$ be a unitary matrix, $U$ can be nonsymmetric, its eigenvalues can be complex numbers and all have modulus $1$. Is there an upper bound for the maximum singular value of its skew symmetric part (which is not necessarily unitary) depending on its eigenvalues? i.e.: Is there an $f$ such that $\left\|\frac{U - U^T}{2} \right\|_2 = \sigma_\text{max}\left(\frac{U - U^T}{2}\right) \le  f\left(\lambda_i\left(U\right)\right)$ ? More details: Observe that if $U=I$ (eigenvalues are real) $\Rightarrow \left\|\frac{U - U^T}{2} \right\|_2 = \sigma_\text{max}\left(\frac{U - U^T}{2}\right) = 0$, and if $U$ is skew-symmetric (eigenvalues purely imaginary) $\Rightarrow\left\|\frac{U - U^T}{2} \right\|_2 = \sigma_\text{max}\left(\frac{U - U^T}{2}\right) = 1$. Therefore there is a relationship between the norm $\left\|\frac{U - U^T}{2} \right\|_2 = \sigma_\text{max}\left(\frac{U - U^T}{2}\right)$ and the argument of the eigenvalues of $U$, i.e. $f\left(\lambda_i\left(U\right)\right) = f\left(\text{arg}(\lambda_i\left(U\right))\right)$. Further notes: in my work $U$ is the unitary factor of the polar decomposition of an M-matrix, but this may be irrelevant.",,"['linear-algebra', 'matrices', 'operator-theory']"
14,Is low-rank factorization another name for low-rank approximation?,Is low-rank factorization another name for low-rank approximation?,,"I need to learn low-rank factorization and its application in machine learning and digital image processing. But I have two questions: Is low-rank factorization another name for low-rank approximation? If the answer is no, what is the main difference between them? Would you please introduce me several references from which I can learn low-rank factorization?","I need to learn low-rank factorization and its application in machine learning and digital image processing. But I have two questions: Is low-rank factorization another name for low-rank approximation? If the answer is no, what is the main difference between them? Would you please introduce me several references from which I can learn low-rank factorization?",,"['linear-algebra', 'matrices', 'reference-request', 'matrix-decomposition']"
15,Problem involving span and homogeneous system of equations,Problem involving span and homogeneous system of equations,,I have a problem which is: Let $V$ $\subset$ $\mathbb{R}^5$ be spanned by $ \begin {bmatrix} 1\\0\\1\\1\\1\\ \end{bmatrix} $ and $ \begin {bmatrix} 0\\1\\-1\\0\\2\\ \end{bmatrix} $. Give a homogeneous system of equations having $V$ as its solution set. I really don't know how to go about this problem. What does it mean to be a solution set? I have the definition of a homogeneous set of equation meaning $\textbf{A}x=0$. I don't see how I would go about getting from that definition to a solution. Any hints would be appreciation. Thanks,I have a problem which is: Let $V$ $\subset$ $\mathbb{R}^5$ be spanned by $ \begin {bmatrix} 1\\0\\1\\1\\1\\ \end{bmatrix} $ and $ \begin {bmatrix} 0\\1\\-1\\0\\2\\ \end{bmatrix} $. Give a homogeneous system of equations having $V$ as its solution set. I really don't know how to go about this problem. What does it mean to be a solution set? I have the definition of a homogeneous set of equation meaning $\textbf{A}x=0$. I don't see how I would go about getting from that definition to a solution. Any hints would be appreciation. Thanks,,"['linear-algebra', 'matrices']"
16,"Prove that there exists $x \in \mathbb{R}^n$ such that $(Ax, x) > 0$ and $(Bx, x) < 0$",Prove that there exists  such that  and,"x \in \mathbb{R}^n (Ax, x) > 0 (Bx, x) < 0","Let $A$ and $B$ be two symmetric matrices from $\mathbb{R}^{n \times n}$ such that $\operatorname{tr}{A} > 0$ and $\operatorname{tr}B < 0$. How one can prove that there exists a vector $x \in \mathbb{R}^n$ such that $(Ax, x) > 0$ and $(Bx, x) < 0$? Since the matrices are real and symmetric we can select a basis in which $A$ is diagonal. In this case, its trace doesn't change. If $A = \operatorname{diag}(d_1, d_2, \dots, d_n)$, then we can note that  $$(Ax, x) = \sum\limits_{i=1}^n d_ix_i^2$$ So, it is easy to make this one positive by chosing appropriate $x$, say, $x = (1, 1, \dots, 1)^T$. But we also need to ensure that $(Bx, x) < 0$. So the question now is how to choose $x$ so that both inequalities hold.","Let $A$ and $B$ be two symmetric matrices from $\mathbb{R}^{n \times n}$ such that $\operatorname{tr}{A} > 0$ and $\operatorname{tr}B < 0$. How one can prove that there exists a vector $x \in \mathbb{R}^n$ such that $(Ax, x) > 0$ and $(Bx, x) < 0$? Since the matrices are real and symmetric we can select a basis in which $A$ is diagonal. In this case, its trace doesn't change. If $A = \operatorname{diag}(d_1, d_2, \dots, d_n)$, then we can note that  $$(Ax, x) = \sum\limits_{i=1}^n d_ix_i^2$$ So, it is easy to make this one positive by chosing appropriate $x$, say, $x = (1, 1, \dots, 1)^T$. But we also need to ensure that $(Bx, x) < 0$. So the question now is how to choose $x$ so that both inequalities hold.",,"['linear-algebra', 'matrices', 'expectation', 'diagonalization', 'trace']"
17,Inequality involving symmetric trace free matrices,Inequality involving symmetric trace free matrices,,"I am trying to prove the following statement: for $A$ a $n\times n$ symmetric matrix (with real coefficients) such that $\text{tr}(A)=0$, one has for every vector $e$ of euclidean norm 1 the following inequality $$ |Ae - (Ae,e)e| \le \frac{1}{\sqrt{2}}\left\| A \right\|_F, $$ where $|\cdot|$ denotes the usual euclidean norm on vectors (and $(\cdot,\cdot)$ is the associated scalar product) and $\left\| \cdot \right\|_F$ is the Frobenius norm on matrices, associated to the scalar product  $$ (B,C)_F = \sum_{i=1}^n \sum_{j=1}^n b_{i,j}c_{i,j}, $$ if $B = (b_{i,j})_{1\le i,j\le n}$ and $C = (c_{i,j})_{1\le i,j\le n}$. Here are several facts that I have collected: one can easily get the constant $1$ instead of $\frac{1}{\sqrt{2}}$, the fact $A$ is trace free is important (for example, the inequality is wrong when $A= \lambda I$, where $I$ is the identity matrix and $\lambda \neq 0$), the symmetry seems to play a role too. I have not been able to find a proof of this inequality (I found it without proof in a book).","I am trying to prove the following statement: for $A$ a $n\times n$ symmetric matrix (with real coefficients) such that $\text{tr}(A)=0$, one has for every vector $e$ of euclidean norm 1 the following inequality $$ |Ae - (Ae,e)e| \le \frac{1}{\sqrt{2}}\left\| A \right\|_F, $$ where $|\cdot|$ denotes the usual euclidean norm on vectors (and $(\cdot,\cdot)$ is the associated scalar product) and $\left\| \cdot \right\|_F$ is the Frobenius norm on matrices, associated to the scalar product  $$ (B,C)_F = \sum_{i=1}^n \sum_{j=1}^n b_{i,j}c_{i,j}, $$ if $B = (b_{i,j})_{1\le i,j\le n}$ and $C = (c_{i,j})_{1\le i,j\le n}$. Here are several facts that I have collected: one can easily get the constant $1$ instead of $\frac{1}{\sqrt{2}}$, the fact $A$ is trace free is important (for example, the inequality is wrong when $A= \lambda I$, where $I$ is the identity matrix and $\lambda \neq 0$), the symmetry seems to play a role too. I have not been able to find a proof of this inequality (I found it without proof in a book).",,"['matrices', 'inequality', 'trace', 'symmetric-matrices']"
18,Quadratic formula on matrices $aX^2 + bX + cI = 0$,Quadratic formula on matrices,aX^2 + bX + cI = 0,"PROBLEM STATEMENT I would like to solve the equation of type: $aX^2 + bX + cI = 0$ for $a,b,c$ ; where $X$ is some square $(N \times N)$ matrix (see footnote 1 below), $I$ is the identity matrix $(N \times N)$, $0$ is the zero matrix $(N \times N)$, and $a,b,c$ are some scalars FOOTNOTES/CAVEATS 1) I understand there are probably some nuances to $X$, such as invertibility or associativity. ATTEMPTED APPROACH It appears that there is some analogy to solving a similar algebra problem where instead of matrices, $X$ is some root to the 2nd degree polynomial and one can simply apply the quadratic formula to get $x$. However, I am stumped on how to proceed after the substitution (see question 2 below). QUESTIONS 1) What area in math should I investigate to understand this type of problem better? What relevant key words should I use in searching literature? 2) If applying the quadratic formula analogy is a valid step, how do I approach the result $X = {RHS}$, where ${RHS}$ is the right-hand side of the equation that is composed of some square roots of $I$, some scalar multiples $aI, bI, cI$, etc.? 3) If there are constraints to $X$, what are they to make this solvable?","PROBLEM STATEMENT I would like to solve the equation of type: $aX^2 + bX + cI = 0$ for $a,b,c$ ; where $X$ is some square $(N \times N)$ matrix (see footnote 1 below), $I$ is the identity matrix $(N \times N)$, $0$ is the zero matrix $(N \times N)$, and $a,b,c$ are some scalars FOOTNOTES/CAVEATS 1) I understand there are probably some nuances to $X$, such as invertibility or associativity. ATTEMPTED APPROACH It appears that there is some analogy to solving a similar algebra problem where instead of matrices, $X$ is some root to the 2nd degree polynomial and one can simply apply the quadratic formula to get $x$. However, I am stumped on how to proceed after the substitution (see question 2 below). QUESTIONS 1) What area in math should I investigate to understand this type of problem better? What relevant key words should I use in searching literature? 2) If applying the quadratic formula analogy is a valid step, how do I approach the result $X = {RHS}$, where ${RHS}$ is the right-hand side of the equation that is composed of some square roots of $I$, some scalar multiples $aI, bI, cI$, etc.? 3) If there are constraints to $X$, what are they to make this solvable?",,"['linear-algebra', 'matrices']"
19,CS231N Backpropagation gradient,CS231N Backpropagation gradient,,"I'm reading the Stanford course about Convolutional Neural Network and I don't understand how he backpropagates a 2 neural network. Actually, the thing I'm trying to understand is here: http://cs231n.github.io/neural-networks-case-study/ We know that: scores = np.dot(X, W) + b And to backpropagate he computes: dW = np.dot(X.T, dscores) db = np.sum(dscores, axis=0, keepdims=True) So mathematically, we can write ($F$ being scores) $$F = XW + b$$ and when he backpropagates he gets: $${dW} = X^\intercal {dF}$$ $${db} = \begin{bmatrix}        \sum\limits_{i=1} {dF}_{1i} \\        \sum\limits_{i=1} {dF}_{2i} \\        \vdots \\        \sum\limits_{i=1} {dF}_{ni}      \end{bmatrix}$$ I'm trying to do the math but I don't understand how to achieve those derivatives rigorously. I hope some of you will help me to understand how it works :s","I'm reading the Stanford course about Convolutional Neural Network and I don't understand how he backpropagates a 2 neural network. Actually, the thing I'm trying to understand is here: http://cs231n.github.io/neural-networks-case-study/ We know that: scores = np.dot(X, W) + b And to backpropagate he computes: dW = np.dot(X.T, dscores) db = np.sum(dscores, axis=0, keepdims=True) So mathematically, we can write ($F$ being scores) $$F = XW + b$$ and when he backpropagates he gets: $${dW} = X^\intercal {dF}$$ $${db} = \begin{bmatrix}        \sum\limits_{i=1} {dF}_{1i} \\        \sum\limits_{i=1} {dF}_{2i} \\        \vdots \\        \sum\limits_{i=1} {dF}_{ni}      \end{bmatrix}$$ I'm trying to do the math but I don't understand how to achieve those derivatives rigorously. I hope some of you will help me to understand how it works :s",,"['matrices', 'vector-analysis', 'neural-networks']"
20,Proof involving symmetric bilinear forms and matrices,Proof involving symmetric bilinear forms and matrices,,"Let $f$ $:$ $V$$\times$$V$ $\rightarrow$ $\Bbb R$ be a symmetric bilinear form. Let $A, B$ be the matrices of $f$ with respect to possibly different bases. I want to prove that there exists $P$ such that $B$ $=$ $P^T$$AP$. Please can anyone lend a hand here?","Let $f$ $:$ $V$$\times$$V$ $\rightarrow$ $\Bbb R$ be a symmetric bilinear form. Let $A, B$ be the matrices of $f$ with respect to possibly different bases. I want to prove that there exists $P$ such that $B$ $=$ $P^T$$AP$. Please can anyone lend a hand here?",,"['linear-algebra', 'matrices', 'bilinear-form', 'change-of-basis']"
21,Find matrix $P$ such that $P^{-1}AP=B$,Find matrix  such that,P P^{-1}AP=B,"Given $$A = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 2 & -3 \\ 1 & 3 & 2 \end{bmatrix}$$ $$B= \begin{bmatrix} 1 & 0 & 0 \\ 0 & 2 & -3 \\ 0 & 3 & 2 \end{bmatrix}$$ find $P$ such that $P^{-1} A P = B$. Firstly I said that $AP=PB$ Solved the 9 equations in 9 unknowns. and got that: $$P= \left( \begin{array}{ccc} -10x & 0 & 0 \\ 3x & y & z \\ x & -z & y \end{array} \right)$$ Then I used computer to find $P^{-1}$ in terms of those unknowns and plugged it back in to $P^{-1}AP=B$ Compared the coefficients and i end up with  $B= \left( \begin{array}{ccc} 1 & 0 & 0 \\ -z/10x & 2 & -3 \\ 1-y/10x & 3 & 2 \end{array} \right) $ Set $x=1$, $y=10$, $z=0$ and indeed $P^{-1}AP=B$ The doubts I am having is the fact that P is not unique. I could set x,y,z to different numbers. Can anyone explain this? Thank you.","Given $$A = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 2 & -3 \\ 1 & 3 & 2 \end{bmatrix}$$ $$B= \begin{bmatrix} 1 & 0 & 0 \\ 0 & 2 & -3 \\ 0 & 3 & 2 \end{bmatrix}$$ find $P$ such that $P^{-1} A P = B$. Firstly I said that $AP=PB$ Solved the 9 equations in 9 unknowns. and got that: $$P= \left( \begin{array}{ccc} -10x & 0 & 0 \\ 3x & y & z \\ x & -z & y \end{array} \right)$$ Then I used computer to find $P^{-1}$ in terms of those unknowns and plugged it back in to $P^{-1}AP=B$ Compared the coefficients and i end up with  $B= \left( \begin{array}{ccc} 1 & 0 & 0 \\ -z/10x & 2 & -3 \\ 1-y/10x & 3 & 2 \end{array} \right) $ Set $x=1$, $y=10$, $z=0$ and indeed $P^{-1}AP=B$ The doubts I am having is the fact that P is not unique. I could set x,y,z to different numbers. Can anyone explain this? Thank you.",,"['linear-algebra', 'matrices', 'matrix-equations']"
22,On matrix tridiagonalization,On matrix tridiagonalization,,"Can any matrix $X \in \mathbb R^{n \times n}$ be decomposed into a tridiagonal matrix, i.e., $$X = P^{-1}DP$$ where $P \in \mbox{SO}(n)$ and $D$ is tridiagonal ?","Can any matrix be decomposed into a tridiagonal matrix, i.e., where and is tridiagonal ?",X \in \mathbb R^{n \times n} X = P^{-1}DP P \in \mbox{SO}(n) D,"['linear-algebra', 'matrices', 'matrix-decomposition', 'orthogonal-matrices', 'tridiagonal-matrices']"
23,Eigenvalues of $A$ and $A A^T$,Eigenvalues of  and,A A A^T,"Let $A$ be a real $n\times n$ matrix. How are the eigenvalues of $A$ and $AA^T$ related? What I have come up with so far is that if we let $\lambda_1,\ldots,\lambda_n$ denote the eigenvalues of $A$, then since $$ \det(AA^T) = \det(A)^2 $$ we can deduce the product of the eigenvalues of $AA^T$ is the product of the eigenvalues of $A$ squared. This lead me to believe that $\lambda_1^2,\ldots,\lambda_n^2$ are the eigenvalues of $AA^T$, but I have not been able to prove it. Any help is appreciated!","Let $A$ be a real $n\times n$ matrix. How are the eigenvalues of $A$ and $AA^T$ related? What I have come up with so far is that if we let $\lambda_1,\ldots,\lambda_n$ denote the eigenvalues of $A$, then since $$ \det(AA^T) = \det(A)^2 $$ we can deduce the product of the eigenvalues of $AA^T$ is the product of the eigenvalues of $A$ squared. This lead me to believe that $\lambda_1^2,\ldots,\lambda_n^2$ are the eigenvalues of $AA^T$, but I have not been able to prove it. Any help is appreciated!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
24,$X$ is a basis for free abelian group $A_{n}$ if and only if $\det (M) = \pm 1$,is a basis for free abelian group  if and only if,X A_{n} \det (M) = \pm 1,"This question is related to another problem I asked a question about here . In fact, it is part (b) of a problem whose part (a) was Let $X = \{ x_{1}, x_{2}, \dots, x_{n}\}$ be a set of elements of the free abelian group $A_{n}$.  Let $M$ be the $n\times n$ matrix of coordinates of elements $x_{i}$ in terms of the basis $B=\{b_{1},b_{2},\dots, b_{n}\}$ (where $x_{i}=r_{i1}b_{1}+r_{i2}b_{2}+\cdots + r_{in}b_{n}$, and the coordinates $r_{ij}$ define the matrix $M=(r_{ij})$). Assume $M^{\prime}$ is a matrix obtained from $M$ by a sequence of elementary transformations. Prove that $M^{\prime}=ZMY$, where $Z$, $Y$ are some matrices in $GL(n, \mathbb{Z})$. For the part of the problem I'm asking about now, I need to show that $X$ is a basis for $A_{n}$ if and only if $\det (M) = \pm 1$. For the direction $(\Longleftarrow)$, I was given the hint: ""How is $\det(M)$ changed under transformations of $M$?"" Using this hint, I was able to get to the following point: Suppose $\det M = \pm 1$. By Part (a), if $M^{\prime}$ is a matrix   obtained by a sequence of elementary transformations on $M$, then   $M^{\prime}=ZMY$, where $Z$, $Y$ $\in GL(n,\mathbb{Z})$. Since $Z, Y  \in GL(n, \mathbb{Z})$, $\det(Z) = \pm 1$ and $\det(Y) = \pm 1$. From   linear algebra, we know that the determinant of a product of matrices   is equal to the product of the determinants, so $\det(Z) \cdot \det  (M) \cdot \det(Y) = (\pm 1) (\pm 1) (\pm 1) = \pm 1 = \det(M^{\prime})$. This was as far as I was able to get though, because I am having trouble understanding how to reconcile this with $X$ being a basis for $A_{n}$. For the $(\Longrightarrow)$ direction, I was able to get even less, primarily because of the fact that I don't understand how $X$ being a basis for $A_{n}$ has anything to do with  what the determinant of $M$ is. I started by saying that if $X$ is a basis for $A_{n}$, then $\forall a_{i} \in A_{n}$, $\exists c_{i1}, c_{i2}, \cdots , c_{in} \in  \mathbb{Z}$ such that $a_{i}=c_{i1}x_{1} + c_{i2}x_{2}+\cdots +  c_{in}x_{n}$, where the $c_{ij}$ are the entries in the matrix $M$.   And wasn't sure where to go from there. Could someone please tell me where to go from where I've stopped (in both directions)? I'm very much stuck and this point. (And be prepared to be patient and answer lots of follow-up questions!) Thank you! :)","This question is related to another problem I asked a question about here . In fact, it is part (b) of a problem whose part (a) was Let $X = \{ x_{1}, x_{2}, \dots, x_{n}\}$ be a set of elements of the free abelian group $A_{n}$.  Let $M$ be the $n\times n$ matrix of coordinates of elements $x_{i}$ in terms of the basis $B=\{b_{1},b_{2},\dots, b_{n}\}$ (where $x_{i}=r_{i1}b_{1}+r_{i2}b_{2}+\cdots + r_{in}b_{n}$, and the coordinates $r_{ij}$ define the matrix $M=(r_{ij})$). Assume $M^{\prime}$ is a matrix obtained from $M$ by a sequence of elementary transformations. Prove that $M^{\prime}=ZMY$, where $Z$, $Y$ are some matrices in $GL(n, \mathbb{Z})$. For the part of the problem I'm asking about now, I need to show that $X$ is a basis for $A_{n}$ if and only if $\det (M) = \pm 1$. For the direction $(\Longleftarrow)$, I was given the hint: ""How is $\det(M)$ changed under transformations of $M$?"" Using this hint, I was able to get to the following point: Suppose $\det M = \pm 1$. By Part (a), if $M^{\prime}$ is a matrix   obtained by a sequence of elementary transformations on $M$, then   $M^{\prime}=ZMY$, where $Z$, $Y$ $\in GL(n,\mathbb{Z})$. Since $Z, Y  \in GL(n, \mathbb{Z})$, $\det(Z) = \pm 1$ and $\det(Y) = \pm 1$. From   linear algebra, we know that the determinant of a product of matrices   is equal to the product of the determinants, so $\det(Z) \cdot \det  (M) \cdot \det(Y) = (\pm 1) (\pm 1) (\pm 1) = \pm 1 = \det(M^{\prime})$. This was as far as I was able to get though, because I am having trouble understanding how to reconcile this with $X$ being a basis for $A_{n}$. For the $(\Longrightarrow)$ direction, I was able to get even less, primarily because of the fact that I don't understand how $X$ being a basis for $A_{n}$ has anything to do with  what the determinant of $M$ is. I started by saying that if $X$ is a basis for $A_{n}$, then $\forall a_{i} \in A_{n}$, $\exists c_{i1}, c_{i2}, \cdots , c_{in} \in  \mathbb{Z}$ such that $a_{i}=c_{i1}x_{1} + c_{i2}x_{2}+\cdots +  c_{in}x_{n}$, where the $c_{ij}$ are the entries in the matrix $M$.   And wasn't sure where to go from there. Could someone please tell me where to go from where I've stopped (in both directions)? I'm very much stuck and this point. (And be prepared to be patient and answer lots of follow-up questions!) Thank you! :)",,"['abstract-algebra', 'matrices']"
25,Finding the matrix associated with a linear map,Finding the matrix associated with a linear map,,"Find the matrix associated with the linear map $f:R^2 \rightarrow R^2$ defined by $f(x,y)=(3x-y,y-x)$ with respect to the ordered basis ${(1,0),(1,1)}$ Let the matrix be $A$ and let $f(x)=AX$ where  $$X=\begin{bmatrix}  x \\ y  \end{bmatrix}$$ and  $$A=\begin{bmatrix}  a & b \\ c & d  \end{bmatrix}$$ I tried solving for $a,b,c,d$ by using the basis vectors as $(x,y)$. That is, I took $(x,y)=(1,0)$ and $(x,y)=(1,1)$ to find $a,b,c,d$. But I am not getting the given answer.","Find the matrix associated with the linear map $f:R^2 \rightarrow R^2$ defined by $f(x,y)=(3x-y,y-x)$ with respect to the ordered basis ${(1,0),(1,1)}$ Let the matrix be $A$ and let $f(x)=AX$ where  $$X=\begin{bmatrix}  x \\ y  \end{bmatrix}$$ and  $$A=\begin{bmatrix}  a & b \\ c & d  \end{bmatrix}$$ I tried solving for $a,b,c,d$ by using the basis vectors as $(x,y)$. That is, I took $(x,y)=(1,0)$ and $(x,y)=(1,1)$ to find $a,b,c,d$. But I am not getting the given answer.",,"['linear-algebra', 'matrices', 'linear-transformations']"
26,Why do eigenvalues of a real $2\times2$ matrices with positive determinant and negative trace have negative real parts?,Why do eigenvalues of a real  matrices with positive determinant and negative trace have negative real parts?,2\times2,"Consider the real matrix   $$ A=\begin{pmatrix}a & b\\c & d\end{pmatrix}. $$   It is said that the real parts of the Eigenvalues of $A$ are all negative if   $$ a+d<0\text{ and }ad-bc>0, $$   i.e.    $$ \text{trace }A<0\text{ and }\text{det }A>0. $$ How to verify this? If I consider the characteristic polynomial of $A$ which is $$ \lambda^2-(a+d)\lambda+ad-bc=0, $$ this has solutions $$ \lambda_{1,2}=\frac{a+d}{2}\pm\sqrt{\frac{(a+d)^2}{4}-(ad-bc)} $$ I think, we now have two possibilities. (1) $\lambda_{1,2}$ are real if  $$ ad-bc\leq\frac{(a+d)^2}{4} $$ In this case, we have $$ \lambda_{1,2}\leq a+d $$ and we need $a+d<0$ to have negative real parts. (2) $\lambda_{1,2}$ are complex. This is the case if the expression under the root is negative what only can be the case if $ad-bc>\frac{(a+d)^2}{4}>0$. The real part then is $\frac{a+d}{2}$ and this is negative exactly when $a+d<0$. Summarizing both cases, we have the two conditions $$ a+d<0\text{ and }ad-bc>0. $$","Consider the real matrix   $$ A=\begin{pmatrix}a & b\\c & d\end{pmatrix}. $$   It is said that the real parts of the Eigenvalues of $A$ are all negative if   $$ a+d<0\text{ and }ad-bc>0, $$   i.e.    $$ \text{trace }A<0\text{ and }\text{det }A>0. $$ How to verify this? If I consider the characteristic polynomial of $A$ which is $$ \lambda^2-(a+d)\lambda+ad-bc=0, $$ this has solutions $$ \lambda_{1,2}=\frac{a+d}{2}\pm\sqrt{\frac{(a+d)^2}{4}-(ad-bc)} $$ I think, we now have two possibilities. (1) $\lambda_{1,2}$ are real if  $$ ad-bc\leq\frac{(a+d)^2}{4} $$ In this case, we have $$ \lambda_{1,2}\leq a+d $$ and we need $a+d<0$ to have negative real parts. (2) $\lambda_{1,2}$ are complex. This is the case if the expression under the root is negative what only can be the case if $ad-bc>\frac{(a+d)^2}{4}>0$. The real part then is $\frac{a+d}{2}$ and this is negative exactly when $a+d<0$. Summarizing both cases, we have the two conditions $$ a+d<0\text{ and }ad-bc>0. $$",,"['matrices', 'eigenvalues-eigenvectors']"
27,Prove that $\rho(I - M^{-1}A)<1$,Prove that,\rho(I - M^{-1}A)<1,"Let we have real square matrices $M$ and $A$ such that $A = A^T$ and $(Mx, x) > \frac{1}{2} (Ax, x) > 0, \forall x \ne 0 $. Prove that $\rho(I - M^{-1}A)<1$ where $\rho(A)$ is spectral radius. It should be somehow connected with Neumann series. I have no proof, just some details. For example $\rho(I - M^{-1}A)<1$ equivalent to $\rho(\frac{1}{2}M^{-1}A) < 1$. And if $\rho(\frac{1}{2}M^{-1}A) < 1$ than series $\sum\limits_{n=0}^{\infty}(\frac{1}{2}M^{-1}A)^n$ converges and equals $(I - \frac{1}{2}M^{-1}A)^{-1} = M(M - \frac{1}{2}A)^{-1}$, where $M$ and $M - \frac{1}{2}A$ are positive definite by the statement. Thanks for any help or ideas!","Let we have real square matrices $M$ and $A$ such that $A = A^T$ and $(Mx, x) > \frac{1}{2} (Ax, x) > 0, \forall x \ne 0 $. Prove that $\rho(I - M^{-1}A)<1$ where $\rho(A)$ is spectral radius. It should be somehow connected with Neumann series. I have no proof, just some details. For example $\rho(I - M^{-1}A)<1$ equivalent to $\rho(\frac{1}{2}M^{-1}A) < 1$. And if $\rho(\frac{1}{2}M^{-1}A) < 1$ than series $\sum\limits_{n=0}^{\infty}(\frac{1}{2}M^{-1}A)^n$ converges and equals $(I - \frac{1}{2}M^{-1}A)^{-1} = M(M - \frac{1}{2}A)^{-1}$, where $M$ and $M - \frac{1}{2}A$ are positive definite by the statement. Thanks for any help or ideas!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
28,"Prove that for each $\phi \in S_n$, $\det A=\sum_{\sigma \in S_n}\mathrm{sgn}(\sigma)\prod_{j=1}^n a_{\phi(j)\sigma(j)}$","Prove that for each ,",\phi \in S_n \det A=\sum_{\sigma \in S_n}\mathrm{sgn}(\sigma)\prod_{j=1}^n a_{\phi(j)\sigma(j)},"Assume that  a permutation $\phi \in S_n$ is given. Prove that : $$\det A=\sum_{\sigma \in S_n}\operatorname{sgn} (\sigma)\space    a_{\phi(1)\sigma(1)}a_{\phi(2)\sigma(2)} \cdots a_{\phi(n)\sigma(n)}$$ My Try : The point is to choose an element from each row. So we have (am I correct?) : $$\forall \phi \in S_n\space \quad \det A=\sum_{\sigma \in S_n}\operatorname{sgn}(\sigma)\space    a_{\phi(1)\sigma(\phi(1))}a_{\phi(2)\sigma(\phi(2))}\dots a_{\phi(n)\sigma(\phi(n))}$$ So, It holds for $\sigma \phi^{-1}$ as $\phi$. Thus we have : $$\det A=\sum_{\sigma\phi^{-1} \in S_n}\operatorname{sgn}(\sigma\phi^{-1})\space    a_{\phi(1)\sigma\phi^{-1}(\phi(1))}a_{\phi(2)\sigma\phi^{-1}(\phi(2))}\dots a_{\phi(n)\sigma\phi^{-1}(\phi(n))}$$ $$=\sum_{\sigma\phi^{-1} \in S_n}\operatorname{sgn}(\sigma)\operatorname{sgn}\left(\phi^{-1}\right )\space   a_{\phi(1)\sigma(1)}a_{\phi(2)\sigma(2)}\dots a_{\phi(n)\sigma(n)}$$ I don't know what to do next.","Assume that  a permutation $\phi \in S_n$ is given. Prove that : $$\det A=\sum_{\sigma \in S_n}\operatorname{sgn} (\sigma)\space    a_{\phi(1)\sigma(1)}a_{\phi(2)\sigma(2)} \cdots a_{\phi(n)\sigma(n)}$$ My Try : The point is to choose an element from each row. So we have (am I correct?) : $$\forall \phi \in S_n\space \quad \det A=\sum_{\sigma \in S_n}\operatorname{sgn}(\sigma)\space    a_{\phi(1)\sigma(\phi(1))}a_{\phi(2)\sigma(\phi(2))}\dots a_{\phi(n)\sigma(\phi(n))}$$ So, It holds for $\sigma \phi^{-1}$ as $\phi$. Thus we have : $$\det A=\sum_{\sigma\phi^{-1} \in S_n}\operatorname{sgn}(\sigma\phi^{-1})\space    a_{\phi(1)\sigma\phi^{-1}(\phi(1))}a_{\phi(2)\sigma\phi^{-1}(\phi(2))}\dots a_{\phi(n)\sigma\phi^{-1}(\phi(n))}$$ $$=\sum_{\sigma\phi^{-1} \in S_n}\operatorname{sgn}(\sigma)\operatorname{sgn}\left(\phi^{-1}\right )\space   a_{\phi(1)\sigma(1)}a_{\phi(2)\sigma(2)}\dots a_{\phi(n)\sigma(n)}$$ I don't know what to do next.",,"['linear-algebra', 'matrices', 'permutations', 'determinant', 'symmetric-groups']"
29,Matrix calculus in multiple linear regression OLS estimate derivation,Matrix calculus in multiple linear regression OLS estimate derivation,,"The steps of the following derivation are from here Starting from $y= Xb +\epsilon $, which really is just the same as $\begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{N} \end{bmatrix} = \begin{bmatrix} 1 & x_{21} & \cdots & x_{K1} \\ 1 & x_{22} & \cdots & x_{K2} \\ \vdots & \ddots & \ddots & \vdots \\ 1 & x_{2N} & \cdots & x_{KN} \end{bmatrix} * \begin{bmatrix} b_{1} \\ b_{2} \\ \vdots \\ b_{K} \end{bmatrix} + \begin{bmatrix} \epsilon_{1} \\ \epsilon_{2} \\ \vdots \\ \epsilon_{N} \end{bmatrix} $ it all comes down to minimzing $e'e$: $\epsilon'\epsilon = \begin{bmatrix} e_{1} & e_{2} & \cdots & e_{N} \\ \end{bmatrix} \begin{bmatrix} e_{1} \\ e_{2} \\ \vdots \\ e_{N} \end{bmatrix} = \sum_{i=1}^{N}e_{i}^{2} $ So minimizing $e'e'$ gives us: $min_{b}$ $e'e = (y-Xb)'(y-Xb)$ $min_{b}$ $e'e = y'y - 2b'X'y + b'X'Xb$ (*) $\frac{\partial(e'e)}{\partial b} = -2X'y + 2X'Xb \stackrel{!}{=} 0$ $X'Xb=X'y$ $b=(X'X)^{-1}X'y$ I'm pretty new to matrix calculus, so I was a bit confused about (*). In step (*), $\frac{\partial(y'y)}{\partial b} = 0$, which makes sense. And then $\frac{\partial(-2b'X'y)}{\partial b} = -2X'y$, but why exactly is this true? If it were $\frac{\partial(-2b'X'y)}{\partial b'}$, then that would make perfect sense to me. Is taking the partial derivative with respect to $b$ the same as taking the partial derivative with respect to $b'$? Similarly, $\frac{\partial(b'X'Xb)}{\partial b} = X'Xb$ Why is this true? Shouldn't it be $= b'X'X$?","The steps of the following derivation are from here Starting from $y= Xb +\epsilon $, which really is just the same as $\begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{N} \end{bmatrix} = \begin{bmatrix} 1 & x_{21} & \cdots & x_{K1} \\ 1 & x_{22} & \cdots & x_{K2} \\ \vdots & \ddots & \ddots & \vdots \\ 1 & x_{2N} & \cdots & x_{KN} \end{bmatrix} * \begin{bmatrix} b_{1} \\ b_{2} \\ \vdots \\ b_{K} \end{bmatrix} + \begin{bmatrix} \epsilon_{1} \\ \epsilon_{2} \\ \vdots \\ \epsilon_{N} \end{bmatrix} $ it all comes down to minimzing $e'e$: $\epsilon'\epsilon = \begin{bmatrix} e_{1} & e_{2} & \cdots & e_{N} \\ \end{bmatrix} \begin{bmatrix} e_{1} \\ e_{2} \\ \vdots \\ e_{N} \end{bmatrix} = \sum_{i=1}^{N}e_{i}^{2} $ So minimizing $e'e'$ gives us: $min_{b}$ $e'e = (y-Xb)'(y-Xb)$ $min_{b}$ $e'e = y'y - 2b'X'y + b'X'Xb$ (*) $\frac{\partial(e'e)}{\partial b} = -2X'y + 2X'Xb \stackrel{!}{=} 0$ $X'Xb=X'y$ $b=(X'X)^{-1}X'y$ I'm pretty new to matrix calculus, so I was a bit confused about (*). In step (*), $\frac{\partial(y'y)}{\partial b} = 0$, which makes sense. And then $\frac{\partial(-2b'X'y)}{\partial b} = -2X'y$, but why exactly is this true? If it were $\frac{\partial(-2b'X'y)}{\partial b'}$, then that would make perfect sense to me. Is taking the partial derivative with respect to $b$ the same as taking the partial derivative with respect to $b'$? Similarly, $\frac{\partial(b'X'Xb)}{\partial b} = X'Xb$ Why is this true? Shouldn't it be $= b'X'X$?",,"['linear-algebra', 'matrices', 'derivatives', 'matrix-calculus', 'least-squares']"
30,Square root of a $2 \times 2$ matrix,Square root of a  matrix,2 \times 2,How can I find the square root of the following non-diagonalisable matrix? $$\begin{pmatrix}   5 & -1 \\   4 & 1  \end{pmatrix}$$ I have shown that $$A^{n}=3^{n-1}\begin{pmatrix}   2n+3 & -n \\   4n & 3-2n  \end{pmatrix}$$ though I'm not sure how to use this fact.,How can I find the square root of the following non-diagonalisable matrix? $$\begin{pmatrix}   5 & -1 \\   4 & 1  \end{pmatrix}$$ I have shown that $$A^{n}=3^{n-1}\begin{pmatrix}   2n+3 & -n \\   4n & 3-2n  \end{pmatrix}$$ though I'm not sure how to use this fact.,,"['linear-algebra', 'matrices']"
31,Getting a matrix lexicographically sorted along both directions,Getting a matrix lexicographically sorted along both directions,,"Let $A$ be some $2n\times 2n$ matrix such that each row and each column contains exactly $n$ times $0$ and $n$ times $1$. If I sort the rows of $A$ lexicographically (in the sense of the matrix treated as a list of words, its rows, not in the sense of rearranging each single row) and then its colums, will the rows stay sorted? If not, will the process of alternatingly sorting rows and colums eventually stabilize, giving some reordering of $A$ which is sorted along both directions? [EDIT] Jorge Fernández Hidalgo 's very nice answer works for arbitrary binary matrices and as well for matrices with values in $\{0,\dots,k-1\}$ by replacing $2$ with $k$ in the proof.  Since every $n\times n$ matrix can be replaced by an order equivalent matrix with values in $\{0,\dots,n^2-1\}$, the proof also works for arbitrary real valued matrices. However, I asked myself if it makes a difference whether one starts the process by sorting the rows or sorting the columns.  In more general terms, if $P_R,P_C,P_R',P_C'$ are permutation matrices such that $P_R AP_C$ and $P_R' AP_C'$ are both sorted by rows and columns, do we necessarily have $P_R AP_C=P_R' AP_C'$? The answer is (if my code is not wrong) that it makes a difference how we start, even when looking only at matrices of the type I described in the beginning (same number of $0$s and $1$s in each row and column).  The smallest counterexample I could find (of this type) has dimensions $6\times 6$. Here is some Matlab code, if anyone wants to fool around: function A=sortRowFirst(A)   B=sortrows(sortrows(A)')';   while(!isequal(A,B))     A=B;     B=sortrows(sortrows(A)')';   endwhile endfunction  function A=sortColFirst(A)   B=sortrows(sortrows(A')');   while(!isequal(A,B))     A=B;     B=sortrows(sortrows(A')');   endwhile endfunction  function A=randNice(n)   A=[];   for i=0:n-1     a=[ones(1,i),-ones(1,i)];     a=a(randperm(i*2));     A=[A;a;-a];     A=A';     a=[a(randperm(i*2)),[-1,1](randperm(2))];     A=[A;a;-a];   endfor endfunction","Let $A$ be some $2n\times 2n$ matrix such that each row and each column contains exactly $n$ times $0$ and $n$ times $1$. If I sort the rows of $A$ lexicographically (in the sense of the matrix treated as a list of words, its rows, not in the sense of rearranging each single row) and then its colums, will the rows stay sorted? If not, will the process of alternatingly sorting rows and colums eventually stabilize, giving some reordering of $A$ which is sorted along both directions? [EDIT] Jorge Fernández Hidalgo 's very nice answer works for arbitrary binary matrices and as well for matrices with values in $\{0,\dots,k-1\}$ by replacing $2$ with $k$ in the proof.  Since every $n\times n$ matrix can be replaced by an order equivalent matrix with values in $\{0,\dots,n^2-1\}$, the proof also works for arbitrary real valued matrices. However, I asked myself if it makes a difference whether one starts the process by sorting the rows or sorting the columns.  In more general terms, if $P_R,P_C,P_R',P_C'$ are permutation matrices such that $P_R AP_C$ and $P_R' AP_C'$ are both sorted by rows and columns, do we necessarily have $P_R AP_C=P_R' AP_C'$? The answer is (if my code is not wrong) that it makes a difference how we start, even when looking only at matrices of the type I described in the beginning (same number of $0$s and $1$s in each row and column).  The smallest counterexample I could find (of this type) has dimensions $6\times 6$. Here is some Matlab code, if anyone wants to fool around: function A=sortRowFirst(A)   B=sortrows(sortrows(A)')';   while(!isequal(A,B))     A=B;     B=sortrows(sortrows(A)')';   endwhile endfunction  function A=sortColFirst(A)   B=sortrows(sortrows(A')');   while(!isequal(A,B))     A=B;     B=sortrows(sortrows(A')');   endwhile endfunction  function A=randNice(n)   A=[];   for i=0:n-1     a=[ones(1,i),-ones(1,i)];     a=a(randperm(i*2));     A=[A;a;-a];     A=A';     a=[a(randperm(i*2)),[-1,1](randperm(2))];     A=[A;a;-a];   endfor endfunction",,"['linear-algebra', 'matrices', 'discrete-mathematics', 'sorting']"
32,Singular values of a matrix after scaling,Singular values of a matrix after scaling,,"Given a complex matrix $A$ and diagonal matrix $D$ with no zero elements on its diagonal, it is well-known that $A$ and $DAD^{-1}$ has the same eigenvalues, however not the same singular values. My question is, how far apart can the singular values be?  Specifically, consider a stochastic matrix $A$ of dimension $n$ with eigenvalues $|\lambda_1| \ge |\lambda_{2}| \ge \ldots \ge |\lambda_n|$ such that $\lambda_1 = 1$ and $|\lambda_{2}| < 1$. Denote its singular values by $\sigma_{1} \ge \sigma_{2} \ge \ldots \ge \sigma_{n}$, and we know that $\sigma_{1} = 1$ if $A$ is doubly stochastic and $\sigma_{1} > 1$ otherwise. Can one obtain a non-trivial upper bound on $\sigma_{2}(DAD^{-1})$? (I know that a lower bound can be achieved by majorization). In other words, might it be that $\sigma_{2}(DAD^{-1})=\sigma_{1}(DAD^{-1})$?","Given a complex matrix $A$ and diagonal matrix $D$ with no zero elements on its diagonal, it is well-known that $A$ and $DAD^{-1}$ has the same eigenvalues, however not the same singular values. My question is, how far apart can the singular values be?  Specifically, consider a stochastic matrix $A$ of dimension $n$ with eigenvalues $|\lambda_1| \ge |\lambda_{2}| \ge \ldots \ge |\lambda_n|$ such that $\lambda_1 = 1$ and $|\lambda_{2}| < 1$. Denote its singular values by $\sigma_{1} \ge \sigma_{2} \ge \ldots \ge \sigma_{n}$, and we know that $\sigma_{1} = 1$ if $A$ is doubly stochastic and $\sigma_{1} > 1$ otherwise. Can one obtain a non-trivial upper bound on $\sigma_{2}(DAD^{-1})$? (I know that a lower bound can be achieved by majorization). In other words, might it be that $\sigma_{2}(DAD^{-1})=\sigma_{1}(DAD^{-1})$?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
33,Let $\mathrm P$ be a $2 \times 2$ matrix such that $\mathrm P^{102}= \mathrm O$,Let  be a  matrix such that,\mathrm P 2 \times 2 \mathrm P^{102}= \mathrm O,Can someone guide me how to deal with these problems. Is there an organised way to proceed in solving these type of questions . Thank you.,Can someone guide me how to deal with these problems. Is there an organised way to proceed in solving these type of questions . Thank you.,,"['linear-algebra', 'matrices', 'nilpotence']"
34,Improper rotation matrix in $2D$,Improper rotation matrix in,2D,"The following is the related problem: Improper Rotations in Even Dimensions I want the simpler explanation. An improper rotation is rotation, followed by reflection in the plane perpendicular to the axis of rotation. Consider an improper rotation in $2D$: $$R_I=\begin{bmatrix} \cos\theta & \sin\theta  \\ \sin\theta   & -\cos\theta  \end{bmatrix} $$ Obviously, $\text {det}( R_I)=-1$. Consider the following simple case: $R_I\begin{bmatrix} 1\\ 0  \end{bmatrix}=\begin{bmatrix} \cos\theta \\ \sin\theta   \end{bmatrix}$. However, this is the same as $R\begin{bmatrix} 1\\ 0  \end{bmatrix}$, where $R$ is the proper rotation matrix. $$R=\begin{bmatrix} \cos\theta & -\sin\theta  \\ \sin\theta   & \cos\theta  \end{bmatrix}$$ $R_I\begin{bmatrix} 0\\ 1  \end{bmatrix}=\begin{bmatrix} \sin\theta \\ -\cos\theta    \end{bmatrix}$. and $R\begin{bmatrix} 0\\ 1  \end{bmatrix}=\begin{bmatrix} -\sin\theta  \\ \cos\theta   \end{bmatrix}$. Both are $180^o$ out of phase. I am confused about how to explain the definition of improper rotation by just two examples.","The following is the related problem: Improper Rotations in Even Dimensions I want the simpler explanation. An improper rotation is rotation, followed by reflection in the plane perpendicular to the axis of rotation. Consider an improper rotation in $2D$: $$R_I=\begin{bmatrix} \cos\theta & \sin\theta  \\ \sin\theta   & -\cos\theta  \end{bmatrix} $$ Obviously, $\text {det}( R_I)=-1$. Consider the following simple case: $R_I\begin{bmatrix} 1\\ 0  \end{bmatrix}=\begin{bmatrix} \cos\theta \\ \sin\theta   \end{bmatrix}$. However, this is the same as $R\begin{bmatrix} 1\\ 0  \end{bmatrix}$, where $R$ is the proper rotation matrix. $$R=\begin{bmatrix} \cos\theta & -\sin\theta  \\ \sin\theta   & \cos\theta  \end{bmatrix}$$ $R_I\begin{bmatrix} 0\\ 1  \end{bmatrix}=\begin{bmatrix} \sin\theta \\ -\cos\theta    \end{bmatrix}$. and $R\begin{bmatrix} 0\\ 1  \end{bmatrix}=\begin{bmatrix} -\sin\theta  \\ \cos\theta   \end{bmatrix}$. Both are $180^o$ out of phase. I am confused about how to explain the definition of improper rotation by just two examples.",,"['matrices', 'rotations', 'orthogonal-matrices']"
35,Jacobian of a matrix-valued function,Jacobian of a matrix-valued function,,"I am fairly sure the very reason my problem arises in the first place is that I am going about it wrong, or have gotten some basic concept wrong, so any answer to point out my misunderstanding would be most welcome. I think what I need amounts to the following: I want to get the 'Jacobian matrix' of a function $P:\mathbb{R}^n \rightarrow\mathbb{R}^{p\times q}$. As a simple example, if $U_1,\dots,U_n$ are known $\mathbb{R}^{p\times q}$ matrices, $P$ could be something such as: $$P(\mathbf{w}) = \sum_{i=1}^nw_iU_i$$ Where $\mathbf{w} = (w_1,\dots,w_n)$. The partial derivative of $P$ with regard to any coordinate of input $\mathbf{w}$ is straightforward enough, but my problem is that I would like to have a whole Jacobian matrix to perform further computation. Except a 'three-dimensional' $\mathbb{R}^{(p\times q) \times n}$ Jacobian seems wrong, and I would have no idea how to do anything with it anyway (e.g. a simple dot product with any regular, two-dimensional matrix). I had the intuition that the solution somehow relied on considering the Jacobian of $P$ as having $pq$ lines, but this would mean considering vectorized $(U_i)$'s. In my case, I don't see how this would work, as my $L$ function (see below) very much makes use of the $p\times q$ structure of its inputs. At the very least, I feel like it would then require to properly introduce the $vec$ transform and take it into account when computing the derivatives? Context: I have a function, $\mathcal{E}: \mathbb{R}^n \rightarrow\mathbb{R}$, whose gradient I want to compute. $\mathcal{E}$ itself can be written as: $$\mathcal{E}(\mathbf{w}) = L(P(\mathbf{w}), X)$$ Where $X\in\mathbb{R}^{p\times q}$ is a known constant matrix, $L:\left(\mathbb{R}^{p\times q},\mathbb{R}^{p\times q}\right)\rightarrow\mathbb{R}$, and $P:\mathbb{R}^n \rightarrow\mathbb{R}^{p\times q}$ as above. Now, if $P$ was a $\mathbb{R}^p$-valued function and $X\in\mathbb{R}^p$ instead, I would simply use the chain rule to get my gradient, i.e.: $$\nabla\mathcal{E}(\mathbf{w}) = \left[\partial P(\mathbf{w})\right]^T\nabla L(P(\mathbf{w}),X)$$ In which case the (transposed) Jacobian matrix in the first term would be in $\mathbb{R}^{n\times p}$, the second term in $\mathbb{R}^p$, and my gradient in $\mathbb{R}^n$ as expected. But since $P$ is matrix-valued here, I don't know how to proceed. Lastly, just in case this can help figuring out how I got there: in my case, I can think of $\mathbf{w}$ as a vector of weights, of the $(U_i)$ and $X$ as images and of $L$ as a loss function between my weighted transformation of $(U_i)$'s and a reference image $X$. $p$ would then be the number of pixels, and $q$ would be, say, $3$ if I am using RGB encoding - $L$ thus makes use of the $p\times q$ structure of its inputs, hence my reluctance to simply consider vectorized $(U_i)$.","I am fairly sure the very reason my problem arises in the first place is that I am going about it wrong, or have gotten some basic concept wrong, so any answer to point out my misunderstanding would be most welcome. I think what I need amounts to the following: I want to get the 'Jacobian matrix' of a function $P:\mathbb{R}^n \rightarrow\mathbb{R}^{p\times q}$. As a simple example, if $U_1,\dots,U_n$ are known $\mathbb{R}^{p\times q}$ matrices, $P$ could be something such as: $$P(\mathbf{w}) = \sum_{i=1}^nw_iU_i$$ Where $\mathbf{w} = (w_1,\dots,w_n)$. The partial derivative of $P$ with regard to any coordinate of input $\mathbf{w}$ is straightforward enough, but my problem is that I would like to have a whole Jacobian matrix to perform further computation. Except a 'three-dimensional' $\mathbb{R}^{(p\times q) \times n}$ Jacobian seems wrong, and I would have no idea how to do anything with it anyway (e.g. a simple dot product with any regular, two-dimensional matrix). I had the intuition that the solution somehow relied on considering the Jacobian of $P$ as having $pq$ lines, but this would mean considering vectorized $(U_i)$'s. In my case, I don't see how this would work, as my $L$ function (see below) very much makes use of the $p\times q$ structure of its inputs. At the very least, I feel like it would then require to properly introduce the $vec$ transform and take it into account when computing the derivatives? Context: I have a function, $\mathcal{E}: \mathbb{R}^n \rightarrow\mathbb{R}$, whose gradient I want to compute. $\mathcal{E}$ itself can be written as: $$\mathcal{E}(\mathbf{w}) = L(P(\mathbf{w}), X)$$ Where $X\in\mathbb{R}^{p\times q}$ is a known constant matrix, $L:\left(\mathbb{R}^{p\times q},\mathbb{R}^{p\times q}\right)\rightarrow\mathbb{R}$, and $P:\mathbb{R}^n \rightarrow\mathbb{R}^{p\times q}$ as above. Now, if $P$ was a $\mathbb{R}^p$-valued function and $X\in\mathbb{R}^p$ instead, I would simply use the chain rule to get my gradient, i.e.: $$\nabla\mathcal{E}(\mathbf{w}) = \left[\partial P(\mathbf{w})\right]^T\nabla L(P(\mathbf{w}),X)$$ In which case the (transposed) Jacobian matrix in the first term would be in $\mathbb{R}^{n\times p}$, the second term in $\mathbb{R}^p$, and my gradient in $\mathbb{R}^n$ as expected. But since $P$ is matrix-valued here, I don't know how to proceed. Lastly, just in case this can help figuring out how I got there: in my case, I can think of $\mathbf{w}$ as a vector of weights, of the $(U_i)$ and $X$ as images and of $L$ as a loss function between my weighted transformation of $(U_i)$'s and a reference image $X$. $p$ would then be the number of pixels, and $q$ would be, say, $3$ if I am using RGB encoding - $L$ thus makes use of the $p\times q$ structure of its inputs, hence my reluctance to simply consider vectorized $(U_i)$.",,"['linear-algebra', 'matrices', 'derivatives']"
36,Traces of powers of a matrix $A$ over an algebra are zero implies $A$ nilpotent.,Traces of powers of a matrix  over an algebra are zero implies  nilpotent.,A A,"I would like to have a result similar to ""Traces of all positive powers of a matrix are zero implies it is nilpotent"" . Namely: Let $R$ be a commutative $\mathbb{C}$-algebra, $A \in \mathcal{M}_n(R)$ such that $\mathrm{tr}(A)=\cdots=\mathrm{tr}(A^n)=0$. Does it follow that $A^n=0$? I have no idea weather or not the analogy of eigenvalue exists for matrices over algebra so I can't simply generalize considerations from cited post. Any help will be appreciated.","I would like to have a result similar to ""Traces of all positive powers of a matrix are zero implies it is nilpotent"" . Namely: Let $R$ be a commutative $\mathbb{C}$-algebra, $A \in \mathcal{M}_n(R)$ such that $\mathrm{tr}(A)=\cdots=\mathrm{tr}(A^n)=0$. Does it follow that $A^n=0$? I have no idea weather or not the analogy of eigenvalue exists for matrices over algebra so I can't simply generalize considerations from cited post. Any help will be appreciated.",,"['linear-algebra', 'abstract-algebra', 'matrices']"
37,Calculating the determinant of a matrix using its rank,Calculating the determinant of a matrix using its rank,,"Let A, B, C and D be real n×n matrices. If $$\operatorname{rank} \begin{bmatrix}        \ A & B \\[0.3em]        \ C & D \\[0.3em]      \end{bmatrix} = n$$ then show that $$\det \begin{bmatrix}        \det A & \det B \\[0.3em]        \det C & \det D \\[0.3em]      \end{bmatrix} = 0$$","Let A, B, C and D be real n×n matrices. If $$\operatorname{rank} \begin{bmatrix}        \ A & B \\[0.3em]        \ C & D \\[0.3em]      \end{bmatrix} = n$$ then show that $$\det \begin{bmatrix}        \det A & \det B \\[0.3em]        \det C & \det D \\[0.3em]      \end{bmatrix} = 0$$",,"['linear-algebra', 'matrices', 'determinant', 'matrix-rank']"
38,Block matrix of order $m$ with three block matrices,Block matrix of order  with three block matrices,m,"How to find eigenvalues of following block matrices? $M=\begin{bmatrix} A & B & O & O & O  & O & O & \cdots & O & O\\ B & A & B & O & O  & O & O & \cdots & O & O\\ O & B & A & B & O  & O & O & \cdots & O & O\\ O & O & B & A & B  & O & O & \cdots & O & O\\ O & O & O & B & A  & B & O & \cdots & O & O\\ \vdots & \vdots & \vdots & \vdots & \vdots  & \ddots & \ddots & \cdots & \vdots & \vdots\\ \vdots & \vdots & \vdots & \vdots & \vdots  & \vdots & \ddots & \ddots & \vdots & \vdots\\ \vdots & \vdots & \vdots & \vdots & \vdots  & \vdots & \vdots & \ddots & \ddots & \vdots\\ O & O & O & O & O  & O & O & O & A & B\\ O & O & O & O & O  & O & O & O & B & A\\ \end{bmatrix}_m$ Where, $A=\begin{bmatrix} 0 & 1 & 0 & 0 & 0  &\cdots & 0 & 1 \\  1 & 0 & 1 & 0 & 0 & \cdots & 0 & 0\\  0 & 1 & 0 & 1 & 0 & \cdots & 0 & 0\\ 0 & 0 & 1 & 0 & 1 & \cdots & 0 & 0\\ 0 & 0 & 0 & 1 & 0 & \ddots & 0 & 0\\ \vdots & \vdots & \vdots & \vdots & \ddots & \ddots & \ddots & \vdots\\ 0 & 0 & 0 & 0 & 0 & \ddots & 0 & 1\\ 1 & 0 & 0 & 0 & 0 & \cdots & 1 & 0\\ \end{bmatrix}_n$ $B=\begin{bmatrix} 1 & 0 & 0 & 0 & 0  &\cdots & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\  0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\ \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\ 0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\ \end{bmatrix}_n$ $O=\begin{bmatrix} 0 & 0 & 0 & 0 & 0  &\cdots & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\  0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\ \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\ 0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\ \end{bmatrix}_n$","How to find eigenvalues of following block matrices? $M=\begin{bmatrix} A & B & O & O & O  & O & O & \cdots & O & O\\ B & A & B & O & O  & O & O & \cdots & O & O\\ O & B & A & B & O  & O & O & \cdots & O & O\\ O & O & B & A & B  & O & O & \cdots & O & O\\ O & O & O & B & A  & B & O & \cdots & O & O\\ \vdots & \vdots & \vdots & \vdots & \vdots  & \ddots & \ddots & \cdots & \vdots & \vdots\\ \vdots & \vdots & \vdots & \vdots & \vdots  & \vdots & \ddots & \ddots & \vdots & \vdots\\ \vdots & \vdots & \vdots & \vdots & \vdots  & \vdots & \vdots & \ddots & \ddots & \vdots\\ O & O & O & O & O  & O & O & O & A & B\\ O & O & O & O & O  & O & O & O & B & A\\ \end{bmatrix}_m$ Where, $A=\begin{bmatrix} 0 & 1 & 0 & 0 & 0  &\cdots & 0 & 1 \\  1 & 0 & 1 & 0 & 0 & \cdots & 0 & 0\\  0 & 1 & 0 & 1 & 0 & \cdots & 0 & 0\\ 0 & 0 & 1 & 0 & 1 & \cdots & 0 & 0\\ 0 & 0 & 0 & 1 & 0 & \ddots & 0 & 0\\ \vdots & \vdots & \vdots & \vdots & \ddots & \ddots & \ddots & \vdots\\ 0 & 0 & 0 & 0 & 0 & \ddots & 0 & 1\\ 1 & 0 & 0 & 0 & 0 & \cdots & 1 & 0\\ \end{bmatrix}_n$ $B=\begin{bmatrix} 1 & 0 & 0 & 0 & 0  &\cdots & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\  0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\ \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\ 0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\ \end{bmatrix}_n$ $O=\begin{bmatrix} 0 & 0 & 0 & 0 & 0  &\cdots & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\  0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\ \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\ 0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\ 0 & 0 & 0 & 0 & 0 & \cdots & 0 & 0\\ \end{bmatrix}_n$",,"['matrices', 'eigenvalues-eigenvectors']"
39,Validity of a formula for the $n$-th power of a general $2 \times 2$ matrix,Validity of a formula for the -th power of a general  matrix,n 2 \times 2,"I am taking an optics course and at one point$^1$ we need to find the $n$-th power of the $2 \times 2$ matrix $$M := \begin{bmatrix} a & b \\ c & d\end{bmatrix}$$ where $a, b, c, d$ are real numbers.  My professor (who is not very mathematically rigorous) gives the following formula which he calls ""Sylvester's law"": $$\begin{bmatrix} a & b \\ c & d\end{bmatrix}^n = \frac{1}{\sin{\theta}} \begin{bmatrix} a \sin{(n \theta)} - \sin{((n-1) \theta)} & b \sin{(n \theta)} \\ c \sin{(n \theta)} & d \sin{(n \theta)} - \sin{((n-1)\theta)}\end{bmatrix} $$ where he defines $\cos{\theta} := \frac{1}{2}(a + d)$ (so presumably, $\theta = \arccos{(\frac{a+d}{2}}$).  He then claims that $M^n$ is finite if $-1 < \frac{1}{2} (a + d) < 1$, for then $\theta$ is well-defined and $\sin(\theta) \neq 0$.$^2$ I was pretty skeptical since I have never seen this formula before and a google search didn't return anything similar. I've tried this formula for a couple of simple cases, and I do not get the correct answer. For example, for $a = 0.1, b = 0.2, c = 0.3, d = 0.4$ and $n = 2$, the formula gives $$ \begin{bmatrix} -0.95 & 0.1 \\ 0.15 & -0.8 \end{bmatrix}$$ (note: not even positive) while the right value is $$M^2 = \begin{bmatrix} 0.07 & 0.1 \\ 0.15 & 0.22\end{bmatrix}.$$ Thus the formula cannot be correct as presented. However, I find it hard to believe that he just made the formula up, so I suspect that there is a similar, correct formula (or at least an approximation). My question: Is this formula correct? If not, is there a similar formula that is either correct or an approximation of the $n$-th power? If so, what is the range of validity? I realise that if the answer is no then it will be pretty hard to be certain of that. I'm hoping that someone here will have seen this before. $^1$For those interested: While discussing optical cavities using ray transfer matrices . $^2$ A weird statement, since the $n$-th power of a finite matrix can never be infinite. I suspect this just has to do with the range of validity of the formula.","I am taking an optics course and at one point$^1$ we need to find the $n$-th power of the $2 \times 2$ matrix $$M := \begin{bmatrix} a & b \\ c & d\end{bmatrix}$$ where $a, b, c, d$ are real numbers.  My professor (who is not very mathematically rigorous) gives the following formula which he calls ""Sylvester's law"": $$\begin{bmatrix} a & b \\ c & d\end{bmatrix}^n = \frac{1}{\sin{\theta}} \begin{bmatrix} a \sin{(n \theta)} - \sin{((n-1) \theta)} & b \sin{(n \theta)} \\ c \sin{(n \theta)} & d \sin{(n \theta)} - \sin{((n-1)\theta)}\end{bmatrix} $$ where he defines $\cos{\theta} := \frac{1}{2}(a + d)$ (so presumably, $\theta = \arccos{(\frac{a+d}{2}}$).  He then claims that $M^n$ is finite if $-1 < \frac{1}{2} (a + d) < 1$, for then $\theta$ is well-defined and $\sin(\theta) \neq 0$.$^2$ I was pretty skeptical since I have never seen this formula before and a google search didn't return anything similar. I've tried this formula for a couple of simple cases, and I do not get the correct answer. For example, for $a = 0.1, b = 0.2, c = 0.3, d = 0.4$ and $n = 2$, the formula gives $$ \begin{bmatrix} -0.95 & 0.1 \\ 0.15 & -0.8 \end{bmatrix}$$ (note: not even positive) while the right value is $$M^2 = \begin{bmatrix} 0.07 & 0.1 \\ 0.15 & 0.22\end{bmatrix}.$$ Thus the formula cannot be correct as presented. However, I find it hard to believe that he just made the formula up, so I suspect that there is a similar, correct formula (or at least an approximation). My question: Is this formula correct? If not, is there a similar formula that is either correct or an approximation of the $n$-th power? If so, what is the range of validity? I realise that if the answer is no then it will be pretty hard to be certain of that. I'm hoping that someone here will have seen this before. $^1$For those interested: While discussing optical cavities using ray transfer matrices . $^2$ A weird statement, since the $n$-th power of a finite matrix can never be infinite. I suspect this just has to do with the range of validity of the formula.",,"['linear-algebra', 'matrices']"
40,Intuition about orthogonal matrices,Intuition about orthogonal matrices,,"Currently I am studying orthogonal vectors and matrices. Although I got the theory, i.e. the connections between dot product, angle, orthonormal basis etc., I miss the intuition that lies behind certain formulas. My question : How could you interpret it (geometrically) that the transpose of the orthogonal matrix A equals its inverse, viz. $A^{t} = A^{-1}$ or that for the determinant it is $det(A) = 1$. Of course, one can prove all that analytically just fine. But how could I imagine the linear map and develop an intuition with respect to the 'right angle feature' of the vectors? I hope this is not too vague a question. If so, I will rephrase.","Currently I am studying orthogonal vectors and matrices. Although I got the theory, i.e. the connections between dot product, angle, orthonormal basis etc., I miss the intuition that lies behind certain formulas. My question : How could you interpret it (geometrically) that the transpose of the orthogonal matrix A equals its inverse, viz. $A^{t} = A^{-1}$ or that for the determinant it is $det(A) = 1$. Of course, one can prove all that analytically just fine. But how could I imagine the linear map and develop an intuition with respect to the 'right angle feature' of the vectors? I hope this is not too vague a question. If so, I will rephrase.",,"['matrices', 'orthogonality']"
41,Rank of upper triangular matrix,Rank of upper triangular matrix,,"Show that the rank of an upper triangular matrix is at least as large as the number of non-zero main diagonal entries. What I do not understand with this statement is how can one have a triangular matrix with more linearly independent vectors than non-zero main diagonal entries. If $T = [t_1 \quad t_2 \quad \dots t_n]$ is upper triangular ($t_i$ being the column vectors), cannot $t_j$ always be expressed as linear combination of the set $\{t_1, \dots, t_{j-1} \}$ if the diagonal element $t_{jj} = 0$? In that case the rank should be equal to the number of non-zero diagonal entries. Is there any counter example? Is it even meaningful to consider non-square matrices?","Show that the rank of an upper triangular matrix is at least as large as the number of non-zero main diagonal entries. What I do not understand with this statement is how can one have a triangular matrix with more linearly independent vectors than non-zero main diagonal entries. If $T = [t_1 \quad t_2 \quad \dots t_n]$ is upper triangular ($t_i$ being the column vectors), cannot $t_j$ always be expressed as linear combination of the set $\{t_1, \dots, t_{j-1} \}$ if the diagonal element $t_{jj} = 0$? In that case the rank should be equal to the number of non-zero diagonal entries. Is there any counter example? Is it even meaningful to consider non-square matrices?",,['matrices']
42,"If $A,B$ are invertible, show that $AB$ is invertible and express $(AB)^{-1}$ in terms of $A^{-1},B^{-1} $. [duplicate]","If  are invertible, show that  is invertible and express  in terms of . [duplicate]","A,B AB (AB)^{-1} A^{-1},B^{-1} ","This question already has an answer here : Invertibility of the Product of Two Matrices (1 answer) Closed 8 years ago . If $A,B$ are invertible, show that $AB$ is invertible and express $(AB)^{-1}$ in terms of $A^{-1},B^{-1} $. $($$A,B$ are matrices.$)$ I did the following: Suppose $AB$ is invertible. $$AB(AB)^{-1}=I$$ $$A^{-1}AB(AB)^{-1}=A^{-1}I$$ $$B(AB)^{-1}=A^{-1}$$ $$B^{-1}B(AB)^{-1}=B^{-1}A^{-1}$$ $$(AB)^{-1}=B^{-1}A^{-1}$$ I found what $(AB)^{-1}$ might be, now I need to show that $B^{-1}A^{-1}$ is really the inverse: $$ABB^{-1}A^{-1}=A[BB^{-1}]A^{-1}=[AA^{-1}]=I$$ $$B^{-1}A^{-1}AB=B^{-1}[A^{-1}A]B=[B^{-1}B]=I$$ Is it correct? Do I need these two steps? I feel that the first step shows me what the inverse would be if it exists and the second actually shows that it is the inverse. EDIT : This question is completely different of this one . In the aforementioned question, the person didnt know how to do it. I asked if mine is correct and asked if the first part would be required. It is sad that MSE users don't read the questions completely anymore.","This question already has an answer here : Invertibility of the Product of Two Matrices (1 answer) Closed 8 years ago . If $A,B$ are invertible, show that $AB$ is invertible and express $(AB)^{-1}$ in terms of $A^{-1},B^{-1} $. $($$A,B$ are matrices.$)$ I did the following: Suppose $AB$ is invertible. $$AB(AB)^{-1}=I$$ $$A^{-1}AB(AB)^{-1}=A^{-1}I$$ $$B(AB)^{-1}=A^{-1}$$ $$B^{-1}B(AB)^{-1}=B^{-1}A^{-1}$$ $$(AB)^{-1}=B^{-1}A^{-1}$$ I found what $(AB)^{-1}$ might be, now I need to show that $B^{-1}A^{-1}$ is really the inverse: $$ABB^{-1}A^{-1}=A[BB^{-1}]A^{-1}=[AA^{-1}]=I$$ $$B^{-1}A^{-1}AB=B^{-1}[A^{-1}A]B=[B^{-1}B]=I$$ Is it correct? Do I need these two steps? I feel that the first step shows me what the inverse would be if it exists and the second actually shows that it is the inverse. EDIT : This question is completely different of this one . In the aforementioned question, the person didnt know how to do it. I asked if mine is correct and asked if the first part would be required. It is sad that MSE users don't read the questions completely anymore.",,"['linear-algebra', 'matrices']"
43,Whether a nondegenerate skew-symmetric matrix is congruent to the matrix $\begin{bmatrix} 0 & I_{\ell} \\ -I_{\ell} & 0 \end{bmatrix}$,Whether a nondegenerate skew-symmetric matrix is congruent to the matrix,\begin{bmatrix} 0 & I_{\ell} \\ -I_{\ell} & 0 \end{bmatrix},"Let $A$ be a nondegenerate skew-symmetric matrix over complex field $\mathbb{C}$. Is there an invertible matrix $P$ such that  \begin{align*} P^{T}AP=\begin{bmatrix} 0 & I_{\ell} \\ -I_{\ell} & 0  \end{bmatrix}, \end{align*} where $P^{T}$ is the transpose of $P$, $I_{\ell}$ is the $\ell$ by $\ell$ identity matrix? If $A$ is an $n \times n$ nondegenerate skew-symmetric matrix over complex field $\mathbb{C}$, then $n$ is even. Since $\mathrm{det}(A)=\mathrm{det}(-A^{T})=(-1)^{n}\mathrm{det}(A)$.","Let $A$ be a nondegenerate skew-symmetric matrix over complex field $\mathbb{C}$. Is there an invertible matrix $P$ such that  \begin{align*} P^{T}AP=\begin{bmatrix} 0 & I_{\ell} \\ -I_{\ell} & 0  \end{bmatrix}, \end{align*} where $P^{T}$ is the transpose of $P$, $I_{\ell}$ is the $\ell$ by $\ell$ identity matrix? If $A$ is an $n \times n$ nondegenerate skew-symmetric matrix over complex field $\mathbb{C}$, then $n$ is even. Since $\mathrm{det}(A)=\mathrm{det}(-A^{T})=(-1)^{n}\mathrm{det}(A)$.",,"['linear-algebra', 'matrices']"
44,Does elementwise matrix inequality extend to norms?,Does elementwise matrix inequality extend to norms?,,"The elements of $A$ and $B$ are non-negative and $0 \le A_{ij} \le B_{ij}$ $\forall i,j$ . Is it true that $\Vert A \Vert_p \leq \Vert B \Vert_p$ ? The norm is the operator norm induced by the usual vector $p$ -norm. This is an exercise problem from A Brief Introduction to Numerical Analysis by E. Tyrtyshnikov and am using the book for self-study. My intuition is that it is true, when I think of vectors with all non-negative components, but am unable to give a proof in general.","The elements of and are non-negative and . Is it true that ? The norm is the operator norm induced by the usual vector -norm. This is an exercise problem from A Brief Introduction to Numerical Analysis by E. Tyrtyshnikov and am using the book for self-study. My intuition is that it is true, when I think of vectors with all non-negative components, but am unable to give a proof in general.","A B 0 \le A_{ij} \le B_{ij} \forall i,j \Vert A \Vert_p \leq \Vert B \Vert_p p","['linear-algebra', 'matrices', 'operator-theory', 'normed-spaces']"
45,Block matrix and spectral radius,Block matrix and spectral radius,,"I have a problem about a positive definite matrix. I cannot prove this. Let $B= [b_{ij}]$ be a $m \times m$ matrix. Let $\overline{B}^t$ be the conjugate transpose of $B$. If we have a strict inequality on the spectral radius $$\rho(\overline{B}^tB) < 1$$ show that the block matrix $$\begin{bmatrix} I_n & B \\  \overline{B}^t & I_n  \end{bmatrix}$$ is positive definite. If you don't mind, help me to solve this problem. Remake $\rho(A) = \max_i \lvert \lambda_i \rvert $ where $\lambda_i$ is eigenvalue of $A$. A matrix $A$ is positive definite if $\overline{x}^t A x >0$ for all $x\in \mathbb{C}^n$ and $ A = \overline{A}^t $.","I have a problem about a positive definite matrix. I cannot prove this. Let $B= [b_{ij}]$ be a $m \times m$ matrix. Let $\overline{B}^t$ be the conjugate transpose of $B$. If we have a strict inequality on the spectral radius $$\rho(\overline{B}^tB) < 1$$ show that the block matrix $$\begin{bmatrix} I_n & B \\  \overline{B}^t & I_n  \end{bmatrix}$$ is positive definite. If you don't mind, help me to solve this problem. Remake $\rho(A) = \max_i \lvert \lambda_i \rvert $ where $\lambda_i$ is eigenvalue of $A$. A matrix $A$ is positive definite if $\overline{x}^t A x >0$ for all $x\in \mathbb{C}^n$ and $ A = \overline{A}^t $.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices', 'spectral-radius']"
46,Showing a set of matrices is a subspace,Showing a set of matrices is a subspace,,"I have to show that a set of matrices that commute with the matrix $$\begin{bmatrix}  0&a_2  &a_3  &\cdots   & a_n\\   0& 0 &0  &\cdots  &0 \\   \vdots &\vdots  &\vdots  &\ddots   &\vdots \\   0&0  &0  &\cdots  &0  \end{bmatrix}$$  is a subspace and I have to find its basis. I know that for proving a subspace I have to show that the zero matrix is in this set and that if $A$ and $B$ is in the set than also $A+B$ is in the set and also for if $r \in \mathbb{R}$ and matrix A is from the set than also $rA$ is from this set. I also understand that two matrices commute if $AB=BA$ but I don't know with which matrix should I multiply the given one. Ok, so I took an arbitrary matrix from $\mathbb{R}^{n \times n}$ $$ \begin{bmatrix} e_{11} &e_{12} &e_{13} & \cdots & e_{1n} \\ e_{21} & \cdots&\cdots&\cdots&e_{2n} \\ \vdots & \vdots &\vdots &\ddots & \vdots \\ e_{n1} & \cdots & \cdots & \cdots &e_{nn} \end{bmatrix} $$ If I try to multiply it with the given one I get $$\begin{bmatrix} e_{11} &e_{12} &e_{13} & \cdots & e_{1n} \\ e_{21} & \cdots&\cdots&\cdots&e_{2n} \\ \vdots & \vdots &\vdots &\ddots & \vdots \\ e_{n1} & \cdots & \cdots & \cdots &e_{nn} \end{bmatrix} \begin{bmatrix}  0&a_2  &a_3  &\cdots   & a_n\\   0& 0 &0  &\cdots  &0 \\   \vdots &\vdots  &\vdots  &\ddots   &\vdots \\   0&0  &0  &\cdots  &0  \end{bmatrix}= \begin{bmatrix}  0&e_{11}a_2  &e_{11}a_3  &\cdots   &e_{11} a_n\\   0& 0 &0  &\cdots  &0 \\   \vdots &\vdots  &\vdots  &\ddots   &\vdots \\   0&0  &0  &\cdots  &0  \end{bmatrix} $$ 2. $$ \begin{bmatrix}  0&a_2  &a_3  &\cdots   & a_n\\   0& 0 &0  &\cdots  &0 \\   \vdots &\vdots  &\vdots  &\ddots   &\vdots \\   0&0  &0  &\cdots  &0  \end{bmatrix} \begin{bmatrix} e_{11} &e_{12} &e_{13} & \cdots & e_{1n} \\ e_{21} & \cdots&\cdots&\cdots&e_{2n} \\ \vdots & \vdots &\vdots &\ddots & \vdots \\ e_{n1} & \cdots & \cdots & \cdots &e_{nn} \end{bmatrix}  = \begin{bmatrix} a_2 e_{21}+a_3 e_{31}+ \cdots + a_n e_{n1} & a_2 e_{22}+\cdots+a_n e_{n2} & \cdots & \cdots & a_2 e_{2n}+ \cdots + a_n e_{nn} \\ 0&0&\cdots&\cdots&0 \\ \vdots & \vdots & \vdots & \ddots &0 \\ 0 &0&0 & \cdots &0 \end{bmatrix}$$ Now I have to compare the elements in the both computed matrices and I get a system of equations which I don't know how to solve.","I have to show that a set of matrices that commute with the matrix $$\begin{bmatrix}  0&a_2  &a_3  &\cdots   & a_n\\   0& 0 &0  &\cdots  &0 \\   \vdots &\vdots  &\vdots  &\ddots   &\vdots \\   0&0  &0  &\cdots  &0  \end{bmatrix}$$  is a subspace and I have to find its basis. I know that for proving a subspace I have to show that the zero matrix is in this set and that if $A$ and $B$ is in the set than also $A+B$ is in the set and also for if $r \in \mathbb{R}$ and matrix A is from the set than also $rA$ is from this set. I also understand that two matrices commute if $AB=BA$ but I don't know with which matrix should I multiply the given one. Ok, so I took an arbitrary matrix from $\mathbb{R}^{n \times n}$ $$ \begin{bmatrix} e_{11} &e_{12} &e_{13} & \cdots & e_{1n} \\ e_{21} & \cdots&\cdots&\cdots&e_{2n} \\ \vdots & \vdots &\vdots &\ddots & \vdots \\ e_{n1} & \cdots & \cdots & \cdots &e_{nn} \end{bmatrix} $$ If I try to multiply it with the given one I get $$\begin{bmatrix} e_{11} &e_{12} &e_{13} & \cdots & e_{1n} \\ e_{21} & \cdots&\cdots&\cdots&e_{2n} \\ \vdots & \vdots &\vdots &\ddots & \vdots \\ e_{n1} & \cdots & \cdots & \cdots &e_{nn} \end{bmatrix} \begin{bmatrix}  0&a_2  &a_3  &\cdots   & a_n\\   0& 0 &0  &\cdots  &0 \\   \vdots &\vdots  &\vdots  &\ddots   &\vdots \\   0&0  &0  &\cdots  &0  \end{bmatrix}= \begin{bmatrix}  0&e_{11}a_2  &e_{11}a_3  &\cdots   &e_{11} a_n\\   0& 0 &0  &\cdots  &0 \\   \vdots &\vdots  &\vdots  &\ddots   &\vdots \\   0&0  &0  &\cdots  &0  \end{bmatrix} $$ 2. $$ \begin{bmatrix}  0&a_2  &a_3  &\cdots   & a_n\\   0& 0 &0  &\cdots  &0 \\   \vdots &\vdots  &\vdots  &\ddots   &\vdots \\   0&0  &0  &\cdots  &0  \end{bmatrix} \begin{bmatrix} e_{11} &e_{12} &e_{13} & \cdots & e_{1n} \\ e_{21} & \cdots&\cdots&\cdots&e_{2n} \\ \vdots & \vdots &\vdots &\ddots & \vdots \\ e_{n1} & \cdots & \cdots & \cdots &e_{nn} \end{bmatrix}  = \begin{bmatrix} a_2 e_{21}+a_3 e_{31}+ \cdots + a_n e_{n1} & a_2 e_{22}+\cdots+a_n e_{n2} & \cdots & \cdots & a_2 e_{2n}+ \cdots + a_n e_{nn} \\ 0&0&\cdots&\cdots&0 \\ \vdots & \vdots & \vdots & \ddots &0 \\ 0 &0&0 & \cdots &0 \end{bmatrix}$$ Now I have to compare the elements in the both computed matrices and I get a system of equations which I don't know how to solve.",,"['linear-algebra', 'matrices']"
47,Constructing an explicit non-contractible path in $\text{GL}_n(\mathbb{R})$,Constructing an explicit non-contractible path in,\text{GL}_n(\mathbb{R}),"As can be seen here , the fundamental group of $\text{GL}_n(\mathbb{R})$ is $\mathbb{Z}/2\mathbb{Z}$ (for $n \ge 3$ ). (For $n=2$ it is $\mathbb{Z}$ ). Is there a way to find an explicit representing path for the non-trivial element of $\pi(\text{GL}_n(\mathbb{R}))$ ? i.e describing a non-contractible (closed) path? (I guess for $n=2$ it's just like in $SO(2)\cong \mathbb{S}^1$ ?)","As can be seen here , the fundamental group of is (for ). (For it is ). Is there a way to find an explicit representing path for the non-trivial element of ? i.e describing a non-contractible (closed) path? (I guess for it's just like in ?)",\text{GL}_n(\mathbb{R}) \mathbb{Z}/2\mathbb{Z} n \ge 3 n=2 \mathbb{Z} \pi(\text{GL}_n(\mathbb{R})) n=2 SO(2)\cong \mathbb{S}^1,"['matrices', 'algebraic-topology', 'lie-groups']"
48,When is the Frobenius norm equal to the spectral radius?,When is the Frobenius norm equal to the spectral radius?,,"I know that the spectral radius is $\rho(A) = max |\lambda_l| = \S_{max}^2$ and that the Frobenius norm is $||A|| = \sqrt{tr(A^*A)} = (\sum_{k}S_k^2)^{1/2}$, which means I want to find the matrix A for which the following is true $$ ||A||_F = \sqrt{tr(A^*A)} = (\sum_{k}S_k^2)^{1/2} = S_{max}^2 $$ So is the spectral radius equal to the frobenius norm if A is a square matrix with its largest eigenvalue equal to |1|? (S are the singular values)","I know that the spectral radius is $\rho(A) = max |\lambda_l| = \S_{max}^2$ and that the Frobenius norm is $||A|| = \sqrt{tr(A^*A)} = (\sum_{k}S_k^2)^{1/2}$, which means I want to find the matrix A for which the following is true $$ ||A||_F = \sqrt{tr(A^*A)} = (\sum_{k}S_k^2)^{1/2} = S_{max}^2 $$ So is the spectral radius equal to the frobenius norm if A is a square matrix with its largest eigenvalue equal to |1|? (S are the singular values)",,"['matrices', 'normed-spaces']"
49,Condition on output of Lyapunov equation being positive definite,Condition on output of Lyapunov equation being positive definite,,"I have Matlab code that solves the Lyapunov equation $$AX + XA^T + Q = 0$$ for a 3-D array of matrices, $A$ , using the Matlab function lyap(A,Q) . My problem is that sometimes the resultant matrix, $X$ , is positive definite and sometimes it is not. My question is given that the matrix $Q$ is positive definite, what is the condition on $A$ that the resultant matrix, $X$ , is always positive definite?","I have Matlab code that solves the Lyapunov equation for a 3-D array of matrices, , using the Matlab function lyap(A,Q) . My problem is that sometimes the resultant matrix, , is positive definite and sometimes it is not. My question is given that the matrix is positive definite, what is the condition on that the resultant matrix, , is always positive definite?",AX + XA^T + Q = 0 A X Q A X,"['linear-algebra', 'matrices']"
50,"""Extension"" of orthogonal group","""Extension"" of orthogonal group",,"I'm looking for a Lie group $G$, subgroup of $GL(n,\Bbb{R})$, and which contains $O(n,\Bbb{R})$ as a subgroup: $$ O(n,\Bbb{R}) \subseteq G \subseteq GL(n,\Bbb{R}). $$ Obvious examples: the conformal group, and the special linear group. Now, I would like $G$ to have exactly one dimension more than $O(n,\Bbb{R})$. Is the conformal group the only possibility, or are there any others? Thanks. (Feel free to edit tags appropriately.)","I'm looking for a Lie group $G$, subgroup of $GL(n,\Bbb{R})$, and which contains $O(n,\Bbb{R})$ as a subgroup: $$ O(n,\Bbb{R}) \subseteq G \subseteq GL(n,\Bbb{R}). $$ Obvious examples: the conformal group, and the special linear group. Now, I would like $G$ to have exactly one dimension more than $O(n,\Bbb{R})$. Is the conformal group the only possibility, or are there any others? Thanks. (Feel free to edit tags appropriately.)",,"['matrices', 'lie-groups', 'group-extensions']"
51,Can we form a basis for $2\times2$ matrix using only invertible matrices?,Can we form a basis for  matrix using only invertible matrices?,2\times2,"In contrast, can we form a basis for $2\times2$ matrix using only non-invertible matrices?","In contrast, can we form a basis for $2\times2$ matrix using only non-invertible matrices?",,"['linear-algebra', 'matrices']"
52,Not isomorphic graphs with same spectrum - exists?,Not isomorphic graphs with same spectrum - exists?,,"I am wondering if there exists two graphs, which are not isomorphic with the condition that both of them have the same spectrum. Two graphs are isomorphic when they may be drawn in the same way. Spectrum I mean set of all eigenvalues of the matrix of the graph. https://en.wikipedia.org/wiki/Spectrum_of_a_matrix Anybody may give me hint or some suggestions ? Or maybe this problem is already solved in math ? I have seen the closiest topic to mine: Given two non-isomorphic graphs with the same number of edges, vertices and degree, what is the most efficient way of proving they are not isomorphic? , however it is not direct answer to mine wonders, since the eigenvalues may be different in those graphs if I am correct.","I am wondering if there exists two graphs, which are not isomorphic with the condition that both of them have the same spectrum. Two graphs are isomorphic when they may be drawn in the same way. Spectrum I mean set of all eigenvalues of the matrix of the graph. https://en.wikipedia.org/wiki/Spectrum_of_a_matrix Anybody may give me hint or some suggestions ? Or maybe this problem is already solved in math ? I have seen the closiest topic to mine: Given two non-isomorphic graphs with the same number of edges, vertices and degree, what is the most efficient way of proving they are not isomorphic? , however it is not direct answer to mine wonders, since the eigenvalues may be different in those graphs if I am correct.",,"['matrices', 'graph-theory', 'eigenvalues-eigenvectors', 'graph-isomorphism']"
53,Taking derivative with respect to a vector x,Taking derivative with respect to a vector x,,"$\textbf{x}$ is a n-by-1 vector, $F(\textbf{x})$ is function from $R^{n} \rightarrow R^{n\times n}$. What is the derivative of $F(\textbf{x})\textbf{x}$ with respect to $\textbf{x}$? $\dfrac{dF(\textbf{x})\textbf{x}}{d\textbf{x}}$ = ?","$\textbf{x}$ is a n-by-1 vector, $F(\textbf{x})$ is function from $R^{n} \rightarrow R^{n\times n}$. What is the derivative of $F(\textbf{x})\textbf{x}$ with respect to $\textbf{x}$? $\dfrac{dF(\textbf{x})\textbf{x}}{d\textbf{x}}$ = ?",,['calculus']
54,Finding matrices with polynomials of an exact form,Finding matrices with polynomials of an exact form,,"Might be a bad title here, but I am looking through some old prelim questions and am curious if someone may assist with this one: Find a list of real matrices, as long as possible, such that: 1) The characteristic polynomial of each matrix $p(x) = (x-1)^{5}(x+1)$. 2) The minimal polynomial of each matrix is $m(x) = (x-1)^{2}(x+1)$. 3) No two matrices in the list are similar to each other. Any tips? Is this leading to Jordan normal form?","Might be a bad title here, but I am looking through some old prelim questions and am curious if someone may assist with this one: Find a list of real matrices, as long as possible, such that: 1) The characteristic polynomial of each matrix $p(x) = (x-1)^{5}(x+1)$. 2) The minimal polynomial of each matrix is $m(x) = (x-1)^{2}(x+1)$. 3) No two matrices in the list are similar to each other. Any tips? Is this leading to Jordan normal form?",,"['linear-algebra', 'matrices']"
55,Affine transformations - the meaning of contractivity,Affine transformations - the meaning of contractivity,,"An affine transformation $\omega \colon \mathbb{R}^2 \to \mathbb{R}^2$ is a linear mapping followed by a translation, in other words $$ \omega(x) = Ax+t =  \begin{pmatrix} a & b \\ c & d \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} + \begin{pmatrix} e \\ f \end{pmatrix}. $$ In general case, it is contractive (in terms of Euclidean metric) when the following is satisfied for some fixed $0 \le s < 1$ $$ \forall x,y \in \mathbb{R}^2 \quad d(\omega(x),\omega(y)) \le s \cdot d(x,y).  $$ Now according to ""Fractals everywhere"" by M.F.Barnsley (exercise III.6.4) apparently under the assumption $\det(A-I) \neq 0$ all this holds if the operator norm $|A| \le s <1$. Why is this assumption needed? Can't we have a contractive mapping whose fixed point $x_f = (I-A)^{-1}t$ is, like, at infinity? Another source provides the following conditions $$ a^2+c^2 < 1; \\  b^2+d^2 < 1; \\  a^2+b^2+c^2+d^2 - (ad-bc)(ad-bc) < 1; $$ but what's the meaning of these conditions? And is the inequality $\det(A-I) \neq 0$ automatically enforced in this case?","An affine transformation $\omega \colon \mathbb{R}^2 \to \mathbb{R}^2$ is a linear mapping followed by a translation, in other words $$ \omega(x) = Ax+t =  \begin{pmatrix} a & b \\ c & d \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} + \begin{pmatrix} e \\ f \end{pmatrix}. $$ In general case, it is contractive (in terms of Euclidean metric) when the following is satisfied for some fixed $0 \le s < 1$ $$ \forall x,y \in \mathbb{R}^2 \quad d(\omega(x),\omega(y)) \le s \cdot d(x,y).  $$ Now according to ""Fractals everywhere"" by M.F.Barnsley (exercise III.6.4) apparently under the assumption $\det(A-I) \neq 0$ all this holds if the operator norm $|A| \le s <1$. Why is this assumption needed? Can't we have a contractive mapping whose fixed point $x_f = (I-A)^{-1}t$ is, like, at infinity? Another source provides the following conditions $$ a^2+c^2 < 1; \\  b^2+d^2 < 1; \\  a^2+b^2+c^2+d^2 - (ad-bc)(ad-bc) < 1; $$ but what's the meaning of these conditions? And is the inequality $\det(A-I) \neq 0$ automatically enforced in this case?",,"['linear-algebra', 'matrices', 'geometry', 'affine-geometry']"
56,Inversion of a block matrix,Inversion of a block matrix,,Let $S$ to be a symmetric and positive semi-definite matrix of size $n$ . What is the inverse of the following block matrix $$ M_{2n\times 2n}= \begin{bmatrix} aI+S & -I\\ -I & aI+S \end{bmatrix} $$ where $a$ is an arbitrary real scalar?,Let to be a symmetric and positive semi-definite matrix of size . What is the inverse of the following block matrix where is an arbitrary real scalar?,"S n 
M_{2n\times 2n}=
\begin{bmatrix}
aI+S & -I\\ -I & aI+S
\end{bmatrix}
 a","['linear-algebra', 'matrices', 'inverse', 'block-matrices']"
57,"Orbits of $SL(3, \mathbb{C})/B$",Orbits of,"SL(3, \mathbb{C})/B","Let $B=  \Bigg\{\begin{bmatrix} * & *&* \\ 0 & *&*\\ 0&0&* \end{bmatrix} \Bigg\}< SL(3,\mathbb C)$. What is $SL(3,\mathbb C)/B$? Do we use these facts: Borel fixed point theorem. Algebraic actions of unipotent groups are cells $U_-\cdot x_0=U_-/H$? I actually need a detailed solution because I don't have enough background in this subject and I have to understand it! Any solutions or comments are highly appreciated as well. I also need to know the basic definitions and examples in flag manifolds which are essential to the solution. Particularity, in matrices case.","Let $B=  \Bigg\{\begin{bmatrix} * & *&* \\ 0 & *&*\\ 0&0&* \end{bmatrix} \Bigg\}< SL(3,\mathbb C)$. What is $SL(3,\mathbb C)/B$? Do we use these facts: Borel fixed point theorem. Algebraic actions of unipotent groups are cells $U_-\cdot x_0=U_-/H$? I actually need a detailed solution because I don't have enough background in this subject and I have to understand it! Any solutions or comments are highly appreciated as well. I also need to know the basic definitions and examples in flag manifolds which are essential to the solution. Particularity, in matrices case.",,"['matrices', 'algebraic-geometry', 'lie-groups', 'algebraic-groups', 'homogeneous-spaces']"
58,Order of group $GL_n(\mathbb{Z}_p)$,Order of group,GL_n(\mathbb{Z}_p),"Given $GL_n(\mathbb{Z}_p)$, a general linear group, where $p$ is a prime number, how does one determine the order of this group? We have $n^2$ elements in a matrix $A \in GL_n(\mathbb{Z}_p)$, and each element may have $p$ possibilities for numbers. But one condition has to be satisfied - that $\det A \in \mathbb{Z}^*_p$. So there will be $p$ determinants in total. That's where I get stuck.","Given $GL_n(\mathbb{Z}_p)$, a general linear group, where $p$ is a prime number, how does one determine the order of this group? We have $n^2$ elements in a matrix $A \in GL_n(\mathbb{Z}_p)$, and each element may have $p$ possibilities for numbers. But one condition has to be satisfied - that $\det A \in \mathbb{Z}^*_p$. So there will be $p$ determinants in total. That's where I get stuck.",,"['abstract-algebra', 'matrices', 'group-theory', 'finite-groups', 'prime-numbers']"
59,How to show that $\sum_{k=1}^n |a_{kk}| \le \sum_{k=1}^n |\lambda_k|$ for hermitian matrix $A$?,How to show that  for hermitian matrix ?,\sum_{k=1}^n |a_{kk}| \le \sum_{k=1}^n |\lambda_k| A,I see that this is not true in general. Is it true that $\sum_{k=1}^n |a_{kk}| \le \sum_{k=1}^n |\lambda_k|$ for any complex square matrix $A$? But it is true in hermitian matrix. How to prove that this is true?,I see that this is not true in general. Is it true that $\sum_{k=1}^n |a_{kk}| \le \sum_{k=1}^n |\lambda_k|$ for any complex square matrix $A$? But it is true in hermitian matrix. How to prove that this is true?,,"['linear-algebra', 'matrices']"
60,Is it true that $\sum_{k=1}^n |a_{kk}| \le \sum_{k=1}^n |\lambda_k|$ for any complex square matrix $A$?,Is it true that  for any complex square matrix ?,\sum_{k=1}^n |a_{kk}| \le \sum_{k=1}^n |\lambda_k| A,"(Note: this is not a duplicate as suggested. I am asking for an inequality.) We know that for any complex square matrix $A$, we have $$\sum_{k=1}^n a_{kk} = \sum_{k=1}^n \lambda_k.$$ I can see this relation in trace property that it is equal sum of diagonal elements. But how to show that the sum of absolute values of the diagonal elements is less than or equal to the sum of absolute values of the eigenvalues? That is, do we have $$\sum_{k=1}^n |a_{kk}| \le \sum_{k=1}^n |\lambda_k|\ ?$$","(Note: this is not a duplicate as suggested. I am asking for an inequality.) We know that for any complex square matrix $A$, we have $$\sum_{k=1}^n a_{kk} = \sum_{k=1}^n \lambda_k.$$ I can see this relation in trace property that it is equal sum of diagonal elements. But how to show that the sum of absolute values of the diagonal elements is less than or equal to the sum of absolute values of the eigenvalues? That is, do we have $$\sum_{k=1}^n |a_{kk}| \le \sum_{k=1}^n |\lambda_k|\ ?$$",,['matrices']
61,An identity involving matrix norm and vector norm in relation to a density matrix in physics,An identity involving matrix norm and vector norm in relation to a density matrix in physics,,"This question is self-contained. For those who are interested, it arises from my study of the paper: K Lendi (1987), Evolution matrix in a coherence vector formulation for quantum Markovian master equations of N-level systems , J. Phys. A: Math. Gen. , 20(1): 15-23. The question is about the squared Frobenius norm $\|\rho\|_F^2=\operatorname{Tr}(\rho^2)$ (see formula 2.32 in the paper) of a self-adjoint ""density matrix"" in physics. This density matrix has a representation $$ \rho = \frac{1}{N} \operatorname{id} + \sum_{k=1}^{N^2-1} v_k F_k \tag{2.16} $$ where the matrices $F_i$ form an orthonormal basis of the space of traceless Hermitian matrices. It follows that $v_i=\operatorname{Tr}(\rho F_i)$. Now consider the squared Euclidean norm $\|v\|_2^2 = \sum_{k=1}^{N^2-1} v_k^2$ (2.33). I want to know why $\|\rho\|_F^2 = \frac{1}{N} + \|v\|_2^2$ holds.","This question is self-contained. For those who are interested, it arises from my study of the paper: K Lendi (1987), Evolution matrix in a coherence vector formulation for quantum Markovian master equations of N-level systems , J. Phys. A: Math. Gen. , 20(1): 15-23. The question is about the squared Frobenius norm $\|\rho\|_F^2=\operatorname{Tr}(\rho^2)$ (see formula 2.32 in the paper) of a self-adjoint ""density matrix"" in physics. This density matrix has a representation $$ \rho = \frac{1}{N} \operatorname{id} + \sum_{k=1}^{N^2-1} v_k F_k \tag{2.16} $$ where the matrices $F_i$ form an orthonormal basis of the space of traceless Hermitian matrices. It follows that $v_i=\operatorname{Tr}(\rho F_i)$. Now consider the squared Euclidean norm $\|v\|_2^2 = \sum_{k=1}^{N^2-1} v_k^2$ (2.33). I want to know why $\|\rho\|_F^2 = \frac{1}{N} + \|v\|_2^2$ holds.",,"['linear-algebra', 'matrices', 'normed-spaces', 'mathematical-physics']"
62,Can the determinant of an integer matrix with $k$ given rows equal the gcd of the $k\times k$ minors of those rows?,Can the determinant of an integer matrix with  given rows equal the gcd of the  minors of those rows?,k k\times k,"I'm interested if the following is true: Let $n> k\geq1$ be integers, let $A\in\mathbb Z^{k\times n}$ and denote the $\binom nk$ $k\times k$ minors of $A$ by $A_1,\ldots,A_N$.   Then the equation   $$\left|\begin{array}{c}A\\\hline X\end{array}\right|=\gcd(A_1,\ldots,A_N)$$   always has a solution $X\in\mathbb Z^{n-k\times n}$. This is a generalisation of Can the determinant of an integer matrix with a given row be any multiple of the gcd of that row? (which is the case $k=1$). For $\style{text-decoration:line-through}{k=n}$ and $k=n-1$ the above conjecture is also (obviously) true. Note that, as in the linked question, finding the minimal positive value of that determinant is equivalent to finding the minimal hypervolume of an $n$-dimensional simplex whose vertices are in an integer lattice and $k+1$ vertices are fixed. Idea: The case $\gcd(A_1,\ldots,A_N)=1$ may follow from Elementary proof that if $A$ is a matrix map from $\mathbb{Z}^m$ to $\mathbb Z^n$, then the map is surjective iff the gcd of maximal minors is $1$","I'm interested if the following is true: Let $n> k\geq1$ be integers, let $A\in\mathbb Z^{k\times n}$ and denote the $\binom nk$ $k\times k$ minors of $A$ by $A_1,\ldots,A_N$.   Then the equation   $$\left|\begin{array}{c}A\\\hline X\end{array}\right|=\gcd(A_1,\ldots,A_N)$$   always has a solution $X\in\mathbb Z^{n-k\times n}$. This is a generalisation of Can the determinant of an integer matrix with a given row be any multiple of the gcd of that row? (which is the case $k=1$). For $\style{text-decoration:line-through}{k=n}$ and $k=n-1$ the above conjecture is also (obviously) true. Note that, as in the linked question, finding the minimal positive value of that determinant is equivalent to finding the minimal hypervolume of an $n$-dimensional simplex whose vertices are in an integer lattice and $k+1$ vertices are fixed. Idea: The case $\gcd(A_1,\ldots,A_N)=1$ may follow from Elementary proof that if $A$ is a matrix map from $\mathbb{Z}^m$ to $\mathbb Z^n$, then the map is surjective iff the gcd of maximal minors is $1$",,"['matrices', 'number-theory', 'diophantine-equations', 'determinant', 'conjectures']"
63,Product rule of the derivative of a matrix by a vector,Product rule of the derivative of a matrix by a vector,,"I am trying to express the derivative of the outer product of the $(n\times m)$-matrix  $\mathbf{A}=\mathbf{A}(\mathbf{x})$ with respect to the $p$-vector $\mathbf{x}$. This is, I want to rewrite $\frac{\partial \mathbf{A}\mathbf{A}^T}{\partial \mathbf{x}}$ using a product rule. My intuition tells me that I must have something like $$ \frac{\partial \mathbf{A}\mathbf{A}^T}{\partial \mathbf{x}}=\mathbf{A}\otimes\frac{\partial \mathbf{A}}{\partial \mathbf{x}}+\frac{\partial \mathbf{A}}{\partial \mathbf{x}}\otimes\mathbf{A}. $$ Any help or confirmation on this? Thanks!","I am trying to express the derivative of the outer product of the $(n\times m)$-matrix  $\mathbf{A}=\mathbf{A}(\mathbf{x})$ with respect to the $p$-vector $\mathbf{x}$. This is, I want to rewrite $\frac{\partial \mathbf{A}\mathbf{A}^T}{\partial \mathbf{x}}$ using a product rule. My intuition tells me that I must have something like $$ \frac{\partial \mathbf{A}\mathbf{A}^T}{\partial \mathbf{x}}=\mathbf{A}\otimes\frac{\partial \mathbf{A}}{\partial \mathbf{x}}+\frac{\partial \mathbf{A}}{\partial \mathbf{x}}\otimes\mathbf{A}. $$ Any help or confirmation on this? Thanks!",,"['calculus', 'matrices', 'matrix-calculus']"
64,Eigenvalues and eigenspaces of AB,Eigenvalues and eigenspaces of AB,,"Problem: Consider two matrices $A, B \in \mathbb{R}^{3 \times 3}$. Suppose $A$ has three distinct real eigenvalues $\lambda_1, \lambda_2$ and $\lambda_3$ with respective eigenspaces $E_{\lambda_1}, E_{\lambda_2}$ and $E_{\lambda_3}$. Suppose furthermore that $B$ has two distinct real eigenvalues $\mu_1$ and $\mu_2$ with respective eigenspaces $E_{\mu_1} = \text{span}(E_{\lambda_1}, E_{\lambda_2})$ (the space spanned by $E_{\lambda_1}$ and $E_{\lambda_2}$) and $E_{\mu_2} = E_{\lambda_3}$. 1) Determine the eigenvalues and corresponding eigenspaces of $AB$. 2) Show that $AB = BA$. Attempt at solution: I have no idea how to do this. I tried writing $\det(A - x \mathbb{I}_3) = (-1)^3 (x- \lambda_1) (x- \lambda_2) (x- \lambda_3)$ and then using the fact that $\det(AB) = \det(A) \det(B)$. But then I figured that the characteristic equation doesn't necessarily have to split like that ?","Problem: Consider two matrices $A, B \in \mathbb{R}^{3 \times 3}$. Suppose $A$ has three distinct real eigenvalues $\lambda_1, \lambda_2$ and $\lambda_3$ with respective eigenspaces $E_{\lambda_1}, E_{\lambda_2}$ and $E_{\lambda_3}$. Suppose furthermore that $B$ has two distinct real eigenvalues $\mu_1$ and $\mu_2$ with respective eigenspaces $E_{\mu_1} = \text{span}(E_{\lambda_1}, E_{\lambda_2})$ (the space spanned by $E_{\lambda_1}$ and $E_{\lambda_2}$) and $E_{\mu_2} = E_{\lambda_3}$. 1) Determine the eigenvalues and corresponding eigenspaces of $AB$. 2) Show that $AB = BA$. Attempt at solution: I have no idea how to do this. I tried writing $\det(A - x \mathbb{I}_3) = (-1)^3 (x- \lambda_1) (x- \lambda_2) (x- \lambda_3)$ and then using the fact that $\det(AB) = \det(A) \det(B)$. But then I figured that the characteristic equation doesn't necessarily have to split like that ?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
65,How do I find $\|T\|$ when given a matrix $T$?,How do I find  when given a matrix ?,\|T\| T,"How do I find the norm $\|T\|$ of T: $\mathbb{R}^3 \rightarrow  \mathbb{R}^3$ is defined by $T(x) := Ax$, where $A:= \begin{pmatrix} 0 & 2 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 3 \end{pmatrix} $. I was thinking of writing $T(x_1,x_2,x_3)=(2x_2,x_1,3x_3)$ and then $\|T\|$ =$((2x_2)^2+(x_1)^2+(3x_3)^2))^{1/2}$. Do I find $\|T\|$ like this or not?","How do I find the norm $\|T\|$ of T: $\mathbb{R}^3 \rightarrow  \mathbb{R}^3$ is defined by $T(x) := Ax$, where $A:= \begin{pmatrix} 0 & 2 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 3 \end{pmatrix} $. I was thinking of writing $T(x_1,x_2,x_3)=(2x_2,x_1,3x_3)$ and then $\|T\|$ =$((2x_2)^2+(x_1)^2+(3x_3)^2))^{1/2}$. Do I find $\|T\|$ like this or not?",,"['linear-algebra', 'matrices', 'analysis', 'normed-spaces']"
66,Proving generalized form of Laplace expansion along a row - determinant,Proving generalized form of Laplace expansion along a row - determinant,,"Definition: Let $A$ be an ($n \times n$)-matrix. Let $M_{ij}$ denote the matrix acquired from $A$ by deleting row $i$ and column $j$. For $n \geq 2$ we define the determinant of $A$ inductively as \begin{align*} \det(A) = \sum_{j=1}^n (-1)^{1+j} a_{1j} \det(M_{1j}). \end{align*} This is called Laplace expansion along the first row . I now want to proof the generalized form of this definition, i.e. using any row $i$ (not necessarily the first one). Proposition: Let $A$ be a ($n \times n$)-matrix, and let $i$ be an arbitrary rowindex. Then we have \begin{align*} \det(A) = \sum_{j=1}^n (-1)^{i+j} a_{ij} \det(M_{ij}). \end{align*} Proof: Let $C_1, C_2, \ldots, C_n$ be the rows of $A$. Thus we get \begin{align*} A = \begin{pmatrix} C_1 \\ \vdots \\ C_i \\ \vdots \\ C_n \end{pmatrix}. \end{align*} Now let $B$ be the matrix that is given as \begin{align*} B = \begin{pmatrix} C_i \\ C_1 \\ \vdots \\ C_{i-1} \\ C_{i+1} \\ \vdots \\ C_n \end{pmatrix}. \end{align*} Then $B$ can be acquired from $A$ by doing an ($i-1$)-amount of successive rowinterchanges. Hence we have $\det(B) = (-1)^{i-1} \det(A)$. Then I'm not sure how to continue. I know how to get $(-1)^{i+j}$. But if I just insert the definition then I'm stuck with the minor $M_{1j}$, and not the generalized form. Any ideas?","Definition: Let $A$ be an ($n \times n$)-matrix. Let $M_{ij}$ denote the matrix acquired from $A$ by deleting row $i$ and column $j$. For $n \geq 2$ we define the determinant of $A$ inductively as \begin{align*} \det(A) = \sum_{j=1}^n (-1)^{1+j} a_{1j} \det(M_{1j}). \end{align*} This is called Laplace expansion along the first row . I now want to proof the generalized form of this definition, i.e. using any row $i$ (not necessarily the first one). Proposition: Let $A$ be a ($n \times n$)-matrix, and let $i$ be an arbitrary rowindex. Then we have \begin{align*} \det(A) = \sum_{j=1}^n (-1)^{i+j} a_{ij} \det(M_{ij}). \end{align*} Proof: Let $C_1, C_2, \ldots, C_n$ be the rows of $A$. Thus we get \begin{align*} A = \begin{pmatrix} C_1 \\ \vdots \\ C_i \\ \vdots \\ C_n \end{pmatrix}. \end{align*} Now let $B$ be the matrix that is given as \begin{align*} B = \begin{pmatrix} C_i \\ C_1 \\ \vdots \\ C_{i-1} \\ C_{i+1} \\ \vdots \\ C_n \end{pmatrix}. \end{align*} Then $B$ can be acquired from $A$ by doing an ($i-1$)-amount of successive rowinterchanges. Hence we have $\det(B) = (-1)^{i-1} \det(A)$. Then I'm not sure how to continue. I know how to get $(-1)^{i+j}$. But if I just insert the definition then I'm stuck with the minor $M_{1j}$, and not the generalized form. Any ideas?",,"['linear-algebra', 'matrices', 'solution-verification', 'determinant']"
67,If $(I-A)(I+A)^{-1}$ is orthogonal then prove that A is skew symmetric.,If  is orthogonal then prove that A is skew symmetric.,(I-A)(I+A)^{-1},If $(I-A)(I+A)^{-1}$ is orthogonal then prove that A is skew symmetric. I can't solve this question.,If is orthogonal then prove that A is skew symmetric. I can't solve this question.,(I-A)(I+A)^{-1},"['matrices', 'determinant']"
68,Linear combination of matrices over finite and infinite fields,Linear combination of matrices over finite and infinite fields,,"Let $F\subset K$ be the fields. Let $A_1,\ldots, A_m$ be the $n\times n$ matrices over the field $F$, and  $c_1,\ldots,c_m\in K$  such that $c_1A_1+\cdots+c_mA_m$ is invertible. How to prove that for infinite $F$ there exists $f_1,\ldots,f_m\in F$ such that $f_1A_1+\cdots+f_mA_m$ is invertible and for finite $F$ it does not hold?","Let $F\subset K$ be the fields. Let $A_1,\ldots, A_m$ be the $n\times n$ matrices over the field $F$, and  $c_1,\ldots,c_m\in K$  such that $c_1A_1+\cdots+c_mA_m$ is invertible. How to prove that for infinite $F$ there exists $f_1,\ldots,f_m\in F$ such that $f_1A_1+\cdots+f_mA_m$ is invertible and for finite $F$ it does not hold?",,"['linear-algebra', 'matrices', 'vector-spaces', 'field-theory', 'extension-field']"
69,How to prove this identity involving characteristic polynomials on both sides?,How to prove this identity involving characteristic polynomials on both sides?,,"Suppose $A\in \Bbb C^{m\times n},B\in \Bbb C^{n\times m},m\ge n$, prove: $$\det(\lambda I_m-AB)=\lambda^{m-n}\det(\lambda I_n-BA)$$ I don't want to get into nasty determinant calculation. Instead, I think comparing the polynomial factors on both sides might help.  My attempt so far shows that $AB$ and $BA$ share the same non-zero eigenvalues, and that if $BA$ has 0 as eigenvalue, so does $AB$. I guess I'm on the right track but I can't proceed.  The multiplicities of $(\lambda-\lambda_i)$s on both sides seem to be big trouble. I cannot prove they are equal.  Can you help me? Thanks in advance.","Suppose $A\in \Bbb C^{m\times n},B\in \Bbb C^{n\times m},m\ge n$, prove: $$\det(\lambda I_m-AB)=\lambda^{m-n}\det(\lambda I_n-BA)$$ I don't want to get into nasty determinant calculation. Instead, I think comparing the polynomial factors on both sides might help.  My attempt so far shows that $AB$ and $BA$ share the same non-zero eigenvalues, and that if $BA$ has 0 as eigenvalue, so does $AB$. I guess I'm on the right track but I can't proceed.  The multiplicities of $(\lambda-\lambda_i)$s on both sides seem to be big trouble. I cannot prove they are equal.  Can you help me? Thanks in advance.",,"['linear-algebra', 'matrices', 'polynomials', 'eigenvalues-eigenvectors']"
70,Notation for replacing a matrix column with a vector,Notation for replacing a matrix column with a vector,,"Let $A$ be an $n\times n$ matrix. Let $v$ be an $n\times 1$ matrix. Is there a notation to signify replacing the $j$-th column of $A$ with $v$? If not, what is the accepted way to denote this?","Let $A$ be an $n\times n$ matrix. Let $v$ be an $n\times 1$ matrix. Is there a notation to signify replacing the $j$-th column of $A$ with $v$? If not, what is the accepted way to denote this?",,"['matrices', 'notation']"
71,Deducing that the inverse of a permutation matrix is its transpose,Deducing that the inverse of a permutation matrix is its transpose,,"I would like to verify that my proof below is sound. Let $A\in P$ where $P$ is the set of all permutation matrices (only one 1 in each row and column). Also, let $(A)_{ij}$ denote the entry of $A$ in row $i$ and column $j$ . Proof: $AA^{-1}=I$ So we must have: $$\sum_{i=1}^n\sum_{j=1}^n(A)_{ij}(A^{-1})_{ji}=1$$ For this to occur, the instances where $(A)_{ij}=1$ must exactly occur when the instances where $(A^{-1})_{ji}=1$ occur [otherwise our $1$ 's would multiply with zero, and the sum would be zero]. This will be in the first row and column, and then the second row and column, etc. $(A)_{ij}=(A^{-1})_{ji}=1$ and we can take the transpose of both sides here and see that: $$(A^T)_{ij}=(A^{-1})_{ij}$$ and hence $A^T = A^{-1}$","I would like to verify that my proof below is sound. Let where is the set of all permutation matrices (only one 1 in each row and column). Also, let denote the entry of in row and column . Proof: So we must have: For this to occur, the instances where must exactly occur when the instances where occur [otherwise our 's would multiply with zero, and the sum would be zero]. This will be in the first row and column, and then the second row and column, etc. and we can take the transpose of both sides here and see that: and hence",A\in P P (A)_{ij} A i j AA^{-1}=I \sum_{i=1}^n\sum_{j=1}^n(A)_{ij}(A^{-1})_{ji}=1 (A)_{ij}=1 (A^{-1})_{ji}=1 1 (A)_{ij}=(A^{-1})_{ji}=1 (A^T)_{ij}=(A^{-1})_{ij} A^T = A^{-1},"['abstract-algebra', 'matrices']"
72,Determinant of the sum of some special matrix,Determinant of the sum of some special matrix,,"$A,B$ are $3\times 3$ matrices. It is known that: $\det(A)=0$ $\forall i,j: b_{ij}=1$, where $b_{ij}$ is an element of matrix $B$ $\det(A+B)=1$ Find $\det(A+2014B)$ I don't know what to do. I found example in smaller dimensional of matrix $A = \begin{bmatrix} 6&3\\4&2\end{bmatrix}$,  but that's all. Please give me a hint.","$A,B$ are $3\times 3$ matrices. It is known that: $\det(A)=0$ $\forall i,j: b_{ij}=1$, where $b_{ij}$ is an element of matrix $B$ $\det(A+B)=1$ Find $\det(A+2014B)$ I don't know what to do. I found example in smaller dimensional of matrix $A = \begin{bmatrix} 6&3\\4&2\end{bmatrix}$,  but that's all. Please give me a hint.",,"['linear-algebra', 'matrices', 'determinant']"
73,Surprising result between determinant of a matrix and product of Euler totient function $\prod_{i=1}^n\phi(i)$ [duplicate],Surprising result between determinant of a matrix and product of Euler totient function  [duplicate],\prod_{i=1}^n\phi(i),"This question already has answers here : Let the matrix $A=[a_{ij}]_{n×n}$ be defined by $a_{ij}=\gcd(i,j )$. How prove that $A$ is invertible, and compute $\det(A)$? (3 answers) Closed 5 years ago . Let $A$ be the $n$ square matrix with as entries the greatest common divisors of the respective indices: $(A)_{i,j}=\gcd(i,j)$, $$A=\begin{Bmatrix}\gcd(1,1) & \gcd(1,2) & ... & \gcd(1,n) \\ \gcd(2,1) & \gcd(2,2) & ... & \gcd(2,n) \\ \vdots & \vdots & \ddots & \vdots \\\gcd(n-1,1) & \gcd(n-1,2) & ... & \gcd(n-1,n)\\\gcd(n,1) & \gcd(n,2) & ... & \gcd(n,n)\end{Bmatrix}$$ ok so $\phi(n)$ denotes Euler's totient function, then $$\det(A)=\phi(1)\phi(2)...\phi(n)=\prod_{i=1}^n\phi(i)\quad\quad(1)$$ this can permit us to determine $\phi(n)$ if we know the values of $\phi$ for the numbers less than $n$ because $$\underbrace{\det(A) = \sum_{\sigma \in S_n} \mathrm{sgn}(\sigma) \prod_{i=1}^n a_{i,\sigma_i}}_{\text{Leibniz formula}}\to\dfrac{\det(A)}{\phi(1)...\phi(n-1)}=\dfrac{\sum_{\sigma \in S_n}  \mathrm{sgn}(\sigma) \prod_{i=1}^n a_{i,\sigma_i}}{\phi(1)...\phi(n-1)}=\phi(n)$$ but idk if this is useful so (1) is a surprising result, but how to prove it? are there similar examples where $(A)_{i,j}=f(i,j)$ and $$\det(A)=\prod_{i=1}^n g(i)?$$","This question already has answers here : Let the matrix $A=[a_{ij}]_{n×n}$ be defined by $a_{ij}=\gcd(i,j )$. How prove that $A$ is invertible, and compute $\det(A)$? (3 answers) Closed 5 years ago . Let $A$ be the $n$ square matrix with as entries the greatest common divisors of the respective indices: $(A)_{i,j}=\gcd(i,j)$, $$A=\begin{Bmatrix}\gcd(1,1) & \gcd(1,2) & ... & \gcd(1,n) \\ \gcd(2,1) & \gcd(2,2) & ... & \gcd(2,n) \\ \vdots & \vdots & \ddots & \vdots \\\gcd(n-1,1) & \gcd(n-1,2) & ... & \gcd(n-1,n)\\\gcd(n,1) & \gcd(n,2) & ... & \gcd(n,n)\end{Bmatrix}$$ ok so $\phi(n)$ denotes Euler's totient function, then $$\det(A)=\phi(1)\phi(2)...\phi(n)=\prod_{i=1}^n\phi(i)\quad\quad(1)$$ this can permit us to determine $\phi(n)$ if we know the values of $\phi$ for the numbers less than $n$ because $$\underbrace{\det(A) = \sum_{\sigma \in S_n} \mathrm{sgn}(\sigma) \prod_{i=1}^n a_{i,\sigma_i}}_{\text{Leibniz formula}}\to\dfrac{\det(A)}{\phi(1)...\phi(n-1)}=\dfrac{\sum_{\sigma \in S_n}  \mathrm{sgn}(\sigma) \prod_{i=1}^n a_{i,\sigma_i}}{\phi(1)...\phi(n-1)}=\phi(n)$$ but idk if this is useful so (1) is a surprising result, but how to prove it? are there similar examples where $(A)_{i,j}=f(i,j)$ and $$\det(A)=\prod_{i=1}^n g(i)?$$",,"['linear-algebra', 'matrices', 'elementary-number-theory']"
74,What do we know if we know the determinant and trace of a matrix?,What do we know if we know the determinant and trace of a matrix?,,"Can we reconstruct an $n\times n$ matrix if all we know is the determinant and trace of that matrix (and its size: i.e. what $n$ is)?  I would think not, because two scalars wouldn't be enough to tell us about all $n^2$ entries.  But then this brings up three questions for me. What else do we need to specify a matrix exactly?  Obviously, I'm not talking about something for which we can immediately get back the matrix, like for instance, the transpose.  But what if we knew all of the eigenvectors and eigenvalues?  Or what if we know its Jordan form? If all we know is the trace and the determinant, what else can we figure out about the matrix?  Obviously we know whether it is invertible or not, but what about say the eigenvalues -- can we figure them out in the $n\ge 3$ case?  Can we figure something else out about the matrix? Are the trace and the determinant the only invariants of a matrix under change of basis? Basically, I'm just trying to find out exactly what information is encoded in the trace and determinant of a matrix. I know that the trace is the sum of the eigenvalues, the determinant is the product of the eigenvalues, and that the determinant of a matrix is the factor by which areas change under the linear transformation $x \mapsto Ax$, where $A$ is the matrix.  Is there anything thing else that knowing both of these invariants tell us?","Can we reconstruct an $n\times n$ matrix if all we know is the determinant and trace of that matrix (and its size: i.e. what $n$ is)?  I would think not, because two scalars wouldn't be enough to tell us about all $n^2$ entries.  But then this brings up three questions for me. What else do we need to specify a matrix exactly?  Obviously, I'm not talking about something for which we can immediately get back the matrix, like for instance, the transpose.  But what if we knew all of the eigenvectors and eigenvalues?  Or what if we know its Jordan form? If all we know is the trace and the determinant, what else can we figure out about the matrix?  Obviously we know whether it is invertible or not, but what about say the eigenvalues -- can we figure them out in the $n\ge 3$ case?  Can we figure something else out about the matrix? Are the trace and the determinant the only invariants of a matrix under change of basis? Basically, I'm just trying to find out exactly what information is encoded in the trace and determinant of a matrix. I know that the trace is the sum of the eigenvalues, the determinant is the product of the eigenvalues, and that the determinant of a matrix is the factor by which areas change under the linear transformation $x \mapsto Ax$, where $A$ is the matrix.  Is there anything thing else that knowing both of these invariants tell us?",,"['linear-algebra', 'matrices']"
75,Invertability of Non-Square Derivatives and Implicit Function Theorem,Invertability of Non-Square Derivatives and Implicit Function Theorem,,"In the statement of the Implicit Function Theorem from Rudin, we are asked to assume as a condition that for some function: $$ f: \mathbb{R}^{m+n} \rightarrow \mathbb{R}^n$$ The derivative at some point $Df(x)$ is invertible. My question is about the invertibility of this non-square matrix. We can see that $Df(x)$ is a linear operator from $\mathbb{R}^{n + m}$ to $\mathbb{R}^n$ so so its matrix representation is not square. This would mean that the the determinant is not defined and hence the definition of invertiblity does not apply. However, $Df(x)$ is still a linear mapping so it would make sense for us to talk about this function as having an inverse when it is 1-to-1 and onto. Just by visualizing the 1-dimensional case, my intuition says that this would be the case whenever $Df(x) \neq 0$. The proof for such a claim doesn't seem difficult but I was wondering if someone could contribute some clarification for the distinctions between these two definitions and how they relate to the Implicit Function Theorem.","In the statement of the Implicit Function Theorem from Rudin, we are asked to assume as a condition that for some function: $$ f: \mathbb{R}^{m+n} \rightarrow \mathbb{R}^n$$ The derivative at some point $Df(x)$ is invertible. My question is about the invertibility of this non-square matrix. We can see that $Df(x)$ is a linear operator from $\mathbb{R}^{n + m}$ to $\mathbb{R}^n$ so so its matrix representation is not square. This would mean that the the determinant is not defined and hence the definition of invertiblity does not apply. However, $Df(x)$ is still a linear mapping so it would make sense for us to talk about this function as having an inverse when it is 1-to-1 and onto. Just by visualizing the 1-dimensional case, my intuition says that this would be the case whenever $Df(x) \neq 0$. The proof for such a claim doesn't seem difficult but I was wondering if someone could contribute some clarification for the distinctions between these two definitions and how they relate to the Implicit Function Theorem.",,"['linear-algebra', 'matrices', 'inverse']"
76,Should a reflection matrix of a vector have the same form as a rotation matrix?,Should a reflection matrix of a vector have the same form as a rotation matrix?,,"According to the book: I know that it is not possible to write a reflection as a rotation, but from the text it seems that the matrix of the form $$ A=\left[   \begin{array}{ c c }      a & -b \\      b & a   \end{array} \right] $$ is the most general form of $O(2)$ since the way it results in $A$ doesn't show any specification of rotation; on the other hand, I can't find any pair of $a,b$ such that $$ A=\left[   \begin{array}{ c c }      1 & 0 \\      0 & -1   \end{array} \right]. $$ Why is that? Thank you.","According to the book: I know that it is not possible to write a reflection as a rotation, but from the text it seems that the matrix of the form $$ A=\left[   \begin{array}{ c c }      a & -b \\      b & a   \end{array} \right] $$ is the most general form of $O(2)$ since the way it results in $A$ doesn't show any specification of rotation; on the other hand, I can't find any pair of $a,b$ such that $$ A=\left[   \begin{array}{ c c }      1 & 0 \\      0 & -1   \end{array} \right]. $$ Why is that? Thank you.",,"['abstract-algebra', 'matrices']"
77,$B - A \in S^n_{++}$ and $I - A^{1/2}B^{-1}A^{1/2} \in S^n_{++}$ equivalent?,and  equivalent?,B - A \in S^n_{++} I - A^{1/2}B^{-1}A^{1/2} \in S^n_{++},"Define $S^n_{++}$ to be the set that contains all the positive definite matrices. That is, if $A \in S^n_{++}$, then $A$ is a positive definite matrix. Now suppose that $A,B \in S^n_{++}$ are two positive definite matrices. How to prove that $$B - A \in S^n_{++}$$ if and only if $$I - A^{1/2}B^{-1}A^{1/2} \in S^n_{++}?$$","Define $S^n_{++}$ to be the set that contains all the positive definite matrices. That is, if $A \in S^n_{++}$, then $A$ is a positive definite matrix. Now suppose that $A,B \in S^n_{++}$ are two positive definite matrices. How to prove that $$B - A \in S^n_{++}$$ if and only if $$I - A^{1/2}B^{-1}A^{1/2} \in S^n_{++}?$$",,"['linear-algebra', 'matrices', 'svd']"
78,Algorithm concerning orthogonal matrices,Algorithm concerning orthogonal matrices,,"Say I have a n-dimensional orthogonal matrix, with some of its elements given and these others unknown. Does there exist an effective algorithm to find out the unknown elements and restore the whole matrix (it only needs to find one solution if there are many, and gives an error when no solution exists)? Thanks!","Say I have a n-dimensional orthogonal matrix, with some of its elements given and these others unknown. Does there exist an effective algorithm to find out the unknown elements and restore the whole matrix (it only needs to find one solution if there are many, and gives an error when no solution exists)? Thanks!",,"['matrices', 'algorithms']"
79,Upper bound on trace of product of unitary and arbitrary matrix,Upper bound on trace of product of unitary and arbitrary matrix,,"Let the field be complex, $U$ be an $n\times n$ unitary matrix, $M$ be any $n\times n$ matrix, and $|M|$ denote the matrix formed by taking the absolute value of every entry of $M$. Edited Question: What is the general upper bound on $|trace(UM)|$? As asked previously, the following statement is false - There exists a permutation matrix $\Pi$, such that $|trace(UM)|\leq trace(\Pi|M|)$ Counter-example: $U=[[1/2,(1+i\sqrt2)/2],[(1-i\sqrt2)/2,-1/2]]$, $M=[[1.1,0],[1,0]]$","Let the field be complex, $U$ be an $n\times n$ unitary matrix, $M$ be any $n\times n$ matrix, and $|M|$ denote the matrix formed by taking the absolute value of every entry of $M$. Edited Question: What is the general upper bound on $|trace(UM)|$? As asked previously, the following statement is false - There exists a permutation matrix $\Pi$, such that $|trace(UM)|\leq trace(\Pi|M|)$ Counter-example: $U=[[1/2,(1+i\sqrt2)/2],[(1-i\sqrt2)/2,-1/2]]$, $M=[[1.1,0],[1,0]]$",,"['linear-algebra', 'matrices', 'trace']"
80,Algebra of matrix coefficient over a compact group is isomorphic to its dual.,Algebra of matrix coefficient over a compact group is isomorphic to its dual.,,"Let $K$ be a compact group. Then we have the following definition of matrix coefficient: Definition: $f: K \rightarrow \mathbb{C}$ is called a matrix coefficient if there is a finite dimensional semisimple $K$-module $V$ with $\alpha \in V^*, v\in V$ such that $f(k) = \alpha(kv)$ for all $k\in K$. Let $R(K)$ be the set of matrix coefficients of $K$. It is a subalgebra of the algebra $$C(K):=\{\text{Continuous functions } K\to \mathbb{C}\},$$ and isomorphic to $\oplus (F_{\gamma}^*\otimes F_{\gamma})$ as spaces (where $F_{\gamma}$ take over all finite dimensional simple modules). Set $R^v(K)$ to be the dual of $R(K)$.  Let $\zeta \in R^v(K)$ be defined by $\zeta(\alpha\otimes v) = \alpha(v)$ if $\gamma $ is trivial and $0$ otherwise. My Question is: Can we show that $R(K) \cong R^v(K)$ via $f \mapsto\zeta(f\cdot)$? Thanks very much!","Let $K$ be a compact group. Then we have the following definition of matrix coefficient: Definition: $f: K \rightarrow \mathbb{C}$ is called a matrix coefficient if there is a finite dimensional semisimple $K$-module $V$ with $\alpha \in V^*, v\in V$ such that $f(k) = \alpha(kv)$ for all $k\in K$. Let $R(K)$ be the set of matrix coefficients of $K$. It is a subalgebra of the algebra $$C(K):=\{\text{Continuous functions } K\to \mathbb{C}\},$$ and isomorphic to $\oplus (F_{\gamma}^*\otimes F_{\gamma})$ as spaces (where $F_{\gamma}$ take over all finite dimensional simple modules). Set $R^v(K)$ to be the dual of $R(K)$.  Let $\zeta \in R^v(K)$ be defined by $\zeta(\alpha\otimes v) = \alpha(v)$ if $\gamma $ is trivial and $0$ otherwise. My Question is: Can we show that $R(K) \cong R^v(K)$ via $f \mapsto\zeta(f\cdot)$? Thanks very much!",,"['matrices', 'lie-groups', 'lie-algebras']"
81,Matrix equation solution,Matrix equation solution,,"Does anybody know how to solve this matrix equation: $$P = P P^T R + X,$$ where $P, R,$ and $X$ are vectors with $n$ elements, and $P$ is the unknown vector?","Does anybody know how to solve this matrix equation: $$P = P P^T R + X,$$ where $P, R,$ and $X$ are vectors with $n$ elements, and $P$ is the unknown vector?",,"['linear-algebra', 'matrices', 'matrix-equations']"
82,Exponential restricts to special linear matrices,Exponential restricts to special linear matrices,,"Let's consider a field $k$ of characteristic $p$ and a matrix $M \in \mathfrak{sl}_n$ (the Lie algebra of trace $0$ matrices).  Assume $M^r = 0$ for some $r < p$ so that the exponential $$\exp(M) = I + M + \frac12M^2 + \cdots + \frac{1}{(r-1)!}M^{r - 1}$$ is well defined.  Then it's true that $\det(\exp(M)) = 1$. The easiest way to see this is to note that $\exp$ commutes with conjugation so you just have to prove it for matrices in Jordan normal form, and there it's obvious because all the eigenvalues are zero. My question is how can this be proven without using Jordan normal form?  I ask because I would like to be able to show that this is still true when the entries in $M$ lie in some $\mathbb F_p$-algebra which is not necessarily a field.","Let's consider a field $k$ of characteristic $p$ and a matrix $M \in \mathfrak{sl}_n$ (the Lie algebra of trace $0$ matrices).  Assume $M^r = 0$ for some $r < p$ so that the exponential $$\exp(M) = I + M + \frac12M^2 + \cdots + \frac{1}{(r-1)!}M^{r - 1}$$ is well defined.  Then it's true that $\det(\exp(M)) = 1$. The easiest way to see this is to note that $\exp$ commutes with conjugation so you just have to prove it for matrices in Jordan normal form, and there it's obvious because all the eigenvalues are zero. My question is how can this be proven without using Jordan normal form?  I ask because I would like to be able to show that this is still true when the entries in $M$ lie in some $\mathbb F_p$-algebra which is not necessarily a field.",,"['matrices', 'determinant']"
83,Proof of the conjecture that the kernel is of dimension 2,Proof of the conjecture that the kernel is of dimension 2,,"I already asked this question which has been answered. This question may seem very similar but the required matrix manipulations are probably very different here due to the addition of the matrix $\mathbf{P}$, which makes the problem more difficult. ""Experimentally"", I found that the kernel (null space) of the following matrix is of dimension 2. I'd like to prove it, but haven't managed yet: \begin{equation} \text{for almost all } t>0,\quad \text{dim}\,\text{ker}\left(\mathbf{Q}_2\mathbf{Q}_1(t)-\mathbf{Q}_1(t)^{-1}\mathbf{Q}_2\right)\overbrace{=}^?\;2  \end{equation} where: $\mathbf{Q}_2$ is the identity matrix everywhere except in $(2n,2n)$: \begin{equation} \mathbf{Q}_2=\begin{bmatrix} \mathbf{I}_n & \mathbf{0}_n \\  \mathbf{0}_n & \mathbf{P}^{-1}\begin{bmatrix}1 & && \\ & \ddots && \\ & & 1& \\ &&& -1 \end{bmatrix}\mathbf{P}  \end{bmatrix}\in\mathbb{R}^{2n\times2n} \end{equation} where $\mathbf{P}\in\mathbb{R}^{n\times n}$ is any invertible matrix. $\mathbf{Q}_1(t)$ is defined by: \begin{equation} \forall t>0,\quad\mathbf{Q}_1(t)=\begin{bmatrix}\textbf{cos}(\boldsymbol \Omega t) & \boldsymbol \Omega^{-1}\,\textbf{sin}(\boldsymbol \Omega t) \\  -\boldsymbol \Omega\,\textbf{sin}(\boldsymbol \Omega t) & \textbf{cos}(\boldsymbol \Omega t)\end{bmatrix}\in\mathbb{R}^{2n\times2n} \end{equation} where (... sorry...): \begin{equation} \boldsymbol\Omega=\begin{bmatrix} \omega_1 & & \\  & \ddots & \\  & & \omega_n  \end{bmatrix}\in\mathbb{R}^{n\times n},\quad \forall i\in\lbrace 1,\dots, n\rbrace, \omega_i>0 \end{equation} and the four blocks are diagonal, for example: \begin{equation} \mathbf{cos}(\boldsymbol\Omega t)=\begin{bmatrix} \cos(\omega_1t) & & \\  & \ddots & \\  & & \cos(\omega_n t)  \end{bmatrix}\in\mathbb{R}^{n\times n} \end{equation} Interesting properties of $\mathbf{Q}_1$ and $\mathbf{Q}_2$ : Obviously, $\mathbf{Q}_2$ is invertible and $\mathbf{Q}_2=\mathbf{Q}_2^{-1}$. Also, $\det(\mathbf{Q}_1)=1$ ($\omega_i>0$ and for proper $t>0$) and: \begin{equation} \mathbf{Q}_1(t)^{-1}=\begin{bmatrix}\textbf{cos}(\boldsymbol \Omega t) & -\boldsymbol \Omega^{-1}\,\textbf{sin}(\boldsymbol \Omega t) \\  \boldsymbol \Omega\,\textbf{sin}(\boldsymbol \Omega t) & \textbf{cos}(\boldsymbol \Omega t)\end{bmatrix} \end{equation} The initial equation can therefore also be written as: \begin{equation} \text{ker}\left(\mathbf{Q}_2\mathbf{Q}_1(t)-\mathbf{Q}_1(t)^{-1}\mathbf{Q}_2\right)=\text{ker}\left(\mathbf{Q}_2\mathbf{Q}_1(t)\mathbf{Q}_2\mathbf{Q}_1(t)-\mathbf{1}_{2n}\right) \end{equation} So another way of solving the problem is to prove that 1 is an eigenvalue of $\mathbf{Q}_2\mathbf{Q}_1(t)\mathbf{Q}_2\mathbf{Q}_1(t)$ with a multiplicity of 2. But I'm not sure this helps... Any clues would be greatly appreciated.","I already asked this question which has been answered. This question may seem very similar but the required matrix manipulations are probably very different here due to the addition of the matrix $\mathbf{P}$, which makes the problem more difficult. ""Experimentally"", I found that the kernel (null space) of the following matrix is of dimension 2. I'd like to prove it, but haven't managed yet: \begin{equation} \text{for almost all } t>0,\quad \text{dim}\,\text{ker}\left(\mathbf{Q}_2\mathbf{Q}_1(t)-\mathbf{Q}_1(t)^{-1}\mathbf{Q}_2\right)\overbrace{=}^?\;2  \end{equation} where: $\mathbf{Q}_2$ is the identity matrix everywhere except in $(2n,2n)$: \begin{equation} \mathbf{Q}_2=\begin{bmatrix} \mathbf{I}_n & \mathbf{0}_n \\  \mathbf{0}_n & \mathbf{P}^{-1}\begin{bmatrix}1 & && \\ & \ddots && \\ & & 1& \\ &&& -1 \end{bmatrix}\mathbf{P}  \end{bmatrix}\in\mathbb{R}^{2n\times2n} \end{equation} where $\mathbf{P}\in\mathbb{R}^{n\times n}$ is any invertible matrix. $\mathbf{Q}_1(t)$ is defined by: \begin{equation} \forall t>0,\quad\mathbf{Q}_1(t)=\begin{bmatrix}\textbf{cos}(\boldsymbol \Omega t) & \boldsymbol \Omega^{-1}\,\textbf{sin}(\boldsymbol \Omega t) \\  -\boldsymbol \Omega\,\textbf{sin}(\boldsymbol \Omega t) & \textbf{cos}(\boldsymbol \Omega t)\end{bmatrix}\in\mathbb{R}^{2n\times2n} \end{equation} where (... sorry...): \begin{equation} \boldsymbol\Omega=\begin{bmatrix} \omega_1 & & \\  & \ddots & \\  & & \omega_n  \end{bmatrix}\in\mathbb{R}^{n\times n},\quad \forall i\in\lbrace 1,\dots, n\rbrace, \omega_i>0 \end{equation} and the four blocks are diagonal, for example: \begin{equation} \mathbf{cos}(\boldsymbol\Omega t)=\begin{bmatrix} \cos(\omega_1t) & & \\  & \ddots & \\  & & \cos(\omega_n t)  \end{bmatrix}\in\mathbb{R}^{n\times n} \end{equation} Interesting properties of $\mathbf{Q}_1$ and $\mathbf{Q}_2$ : Obviously, $\mathbf{Q}_2$ is invertible and $\mathbf{Q}_2=\mathbf{Q}_2^{-1}$. Also, $\det(\mathbf{Q}_1)=1$ ($\omega_i>0$ and for proper $t>0$) and: \begin{equation} \mathbf{Q}_1(t)^{-1}=\begin{bmatrix}\textbf{cos}(\boldsymbol \Omega t) & -\boldsymbol \Omega^{-1}\,\textbf{sin}(\boldsymbol \Omega t) \\  \boldsymbol \Omega\,\textbf{sin}(\boldsymbol \Omega t) & \textbf{cos}(\boldsymbol \Omega t)\end{bmatrix} \end{equation} The initial equation can therefore also be written as: \begin{equation} \text{ker}\left(\mathbf{Q}_2\mathbf{Q}_1(t)-\mathbf{Q}_1(t)^{-1}\mathbf{Q}_2\right)=\text{ker}\left(\mathbf{Q}_2\mathbf{Q}_1(t)\mathbf{Q}_2\mathbf{Q}_1(t)-\mathbf{1}_{2n}\right) \end{equation} So another way of solving the problem is to prove that 1 is an eigenvalue of $\mathbf{Q}_2\mathbf{Q}_1(t)\mathbf{Q}_2\mathbf{Q}_1(t)$ with a multiplicity of 2. But I'm not sure this helps... Any clues would be greatly appreciated.",,"['linear-algebra', 'matrices', 'determinant']"
84,Prove that the kernel is of dimension 2,Prove that the kernel is of dimension 2,,"""Experimentally"", I found that the kernel (null space) of the following matrix is of dimension 2. I'd like to prove it, but haven't managed yet: \begin{equation} \text{for almost all } t>0,\quad \text{dim}\,\text{ker}\left(\mathbf{Q}_2\mathbf{Q}_1(t)-\mathbf{Q}_1(t)^{-1}\mathbf{Q}_2\right)\overbrace{=}^?\;2  \end{equation} where: $\mathbf{Q}_2$ is the identity matrix everywhere except in $(2n,2n)$: \begin{equation} \mathbf{Q}_2=\begin{bmatrix} 1 & & \\  & \ddots &   \\ & & 1 & \\ & & & -1  \end{bmatrix}\in\mathbb{R}^{2n\times2n} \end{equation} $\mathbf{Q}_1(t)$ is defined by: \begin{equation} \forall t>0,\quad\mathbf{Q}_1(t)=\begin{bmatrix}\textbf{cos}(\boldsymbol \Omega t) & \boldsymbol \Omega^{-1}\,\textbf{sin}(\boldsymbol \Omega t) \\  -\boldsymbol \Omega\,\textbf{sin}(\boldsymbol \Omega t) & \textbf{cos}(\boldsymbol \Omega t)\end{bmatrix}\in\mathbb{R}^{2n\times2n} \end{equation} where (... sorry...): \begin{equation} \boldsymbol\Omega=\begin{bmatrix} \omega_1 & & \\  & \ddots & \\  & & \omega_n  \end{bmatrix}\in\mathbb{R}^{n\times n},\quad \forall i\in\lbrace 1,\dots, n\rbrace, \omega_i>0 \end{equation} and the four blocks are diagonal, for example: \begin{equation} \mathbf{cos}(\boldsymbol\Omega t)=\begin{bmatrix} \cos(\omega_1t) & & \\  & \ddots & \\  & & \cos(\omega_n t)  \end{bmatrix}\in\mathbb{R}^{n\times n} \end{equation} Interesting properties of $\mathbf{Q}_1$ and $\mathbf{Q}_2$ : Obviously, $\mathbf{Q}_2$ is invertible and $\mathbf{Q}_2=\mathbf{Q}_2^{-1}$. Also, $\det(\mathbf{Q}_1)=1$ ($\omega_i>0$ and for proper $t>0$) and: \begin{equation} \mathbf{Q}_1(t)^{-1}=\begin{bmatrix}\textbf{cos}(\boldsymbol \Omega t) & -\boldsymbol \Omega^{-1}\,\textbf{sin}(\boldsymbol \Omega t) \\  \boldsymbol \Omega\,\textbf{sin}(\boldsymbol \Omega t) & \textbf{cos}(\boldsymbol \Omega t)\end{bmatrix} \end{equation} The initial equation can therefore also be written as: \begin{equation} \text{ker}\left(\mathbf{Q}_2\mathbf{Q}_1(t)-\mathbf{Q}_1(t)^{-1}\mathbf{Q}_2\right)=\text{ker}\left(\mathbf{Q}_2\mathbf{Q}_1(t)\mathbf{Q}_2\mathbf{Q}_1(t)-\mathbf{1}_{2n}\right) \end{equation} So another way of solving the problem is to prove that 1 is an eigenvalue of $\mathbf{Q}_2\mathbf{Q}_1(t)\mathbf{Q}_2\mathbf{Q}_1(t)$ with a multiplicity of 2. But I'm not sure this helps... Any clues would be greatly appreciated.","""Experimentally"", I found that the kernel (null space) of the following matrix is of dimension 2. I'd like to prove it, but haven't managed yet: \begin{equation} \text{for almost all } t>0,\quad \text{dim}\,\text{ker}\left(\mathbf{Q}_2\mathbf{Q}_1(t)-\mathbf{Q}_1(t)^{-1}\mathbf{Q}_2\right)\overbrace{=}^?\;2  \end{equation} where: $\mathbf{Q}_2$ is the identity matrix everywhere except in $(2n,2n)$: \begin{equation} \mathbf{Q}_2=\begin{bmatrix} 1 & & \\  & \ddots &   \\ & & 1 & \\ & & & -1  \end{bmatrix}\in\mathbb{R}^{2n\times2n} \end{equation} $\mathbf{Q}_1(t)$ is defined by: \begin{equation} \forall t>0,\quad\mathbf{Q}_1(t)=\begin{bmatrix}\textbf{cos}(\boldsymbol \Omega t) & \boldsymbol \Omega^{-1}\,\textbf{sin}(\boldsymbol \Omega t) \\  -\boldsymbol \Omega\,\textbf{sin}(\boldsymbol \Omega t) & \textbf{cos}(\boldsymbol \Omega t)\end{bmatrix}\in\mathbb{R}^{2n\times2n} \end{equation} where (... sorry...): \begin{equation} \boldsymbol\Omega=\begin{bmatrix} \omega_1 & & \\  & \ddots & \\  & & \omega_n  \end{bmatrix}\in\mathbb{R}^{n\times n},\quad \forall i\in\lbrace 1,\dots, n\rbrace, \omega_i>0 \end{equation} and the four blocks are diagonal, for example: \begin{equation} \mathbf{cos}(\boldsymbol\Omega t)=\begin{bmatrix} \cos(\omega_1t) & & \\  & \ddots & \\  & & \cos(\omega_n t)  \end{bmatrix}\in\mathbb{R}^{n\times n} \end{equation} Interesting properties of $\mathbf{Q}_1$ and $\mathbf{Q}_2$ : Obviously, $\mathbf{Q}_2$ is invertible and $\mathbf{Q}_2=\mathbf{Q}_2^{-1}$. Also, $\det(\mathbf{Q}_1)=1$ ($\omega_i>0$ and for proper $t>0$) and: \begin{equation} \mathbf{Q}_1(t)^{-1}=\begin{bmatrix}\textbf{cos}(\boldsymbol \Omega t) & -\boldsymbol \Omega^{-1}\,\textbf{sin}(\boldsymbol \Omega t) \\  \boldsymbol \Omega\,\textbf{sin}(\boldsymbol \Omega t) & \textbf{cos}(\boldsymbol \Omega t)\end{bmatrix} \end{equation} The initial equation can therefore also be written as: \begin{equation} \text{ker}\left(\mathbf{Q}_2\mathbf{Q}_1(t)-\mathbf{Q}_1(t)^{-1}\mathbf{Q}_2\right)=\text{ker}\left(\mathbf{Q}_2\mathbf{Q}_1(t)\mathbf{Q}_2\mathbf{Q}_1(t)-\mathbf{1}_{2n}\right) \end{equation} So another way of solving the problem is to prove that 1 is an eigenvalue of $\mathbf{Q}_2\mathbf{Q}_1(t)\mathbf{Q}_2\mathbf{Q}_1(t)$ with a multiplicity of 2. But I'm not sure this helps... Any clues would be greatly appreciated.",,"['linear-algebra', 'matrices']"
85,Density of orthogonal matrices with rational coefficients,Density of orthogonal matrices with rational coefficients,,"Is it true that ${O}_n({\mathbb R}) \cap {\mathbb Q}^n$ is dense in ${\cal O}_n({\mathbb R})$ for any $n\geq 2$ ? It obviously suffices to consider the density of ${SO}_n({\mathbb R}) \cap {\mathbb Q}^n$ in $SO_n({\mathbb R})$. This is easy for $n=2$ : consider the matrices of the form $$ \frac{1}{p^2+q^2}\left(\begin{array}{cc} p & -q \\ q & p \end{array}\right) $$ where $p,q$ are integers.","Is it true that ${O}_n({\mathbb R}) \cap {\mathbb Q}^n$ is dense in ${\cal O}_n({\mathbb R})$ for any $n\geq 2$ ? It obviously suffices to consider the density of ${SO}_n({\mathbb R}) \cap {\mathbb Q}^n$ in $SO_n({\mathbb R})$. This is easy for $n=2$ : consider the matrices of the form $$ \frac{1}{p^2+q^2}\left(\begin{array}{cc} p & -q \\ q & p \end{array}\right) $$ where $p,q$ are integers.",,"['real-analysis', 'matrices']"
86,"To find the volume dilation, integrate the determinant of the Jacobian","To find the volume dilation, integrate the determinant of the Jacobian",,"On the road toward proving the change of variables theorem in several variables, is there a painless way to show that  $$\text{Vol}(\phi(U))=\int_{U}|\text{det}(d\phi)|,$$ where $\phi$ is $C^1$, $d\phi$ is non-singular, and $U$ is some reasonable set (say open and connected)? Having done the single-variable version of change of variables when $f$ is discontinuous (as presented in baby Rudin, for example), I wanted to adapt the same method to this, but so far I haven't been successful. Many proofs I've seen have passed through several lemmas, but I think there should be a straightforward way. I am comfortable taking for granted that $|\text{det}(M)|$ is how much $M$ dilates the area of a parallelipiped.","On the road toward proving the change of variables theorem in several variables, is there a painless way to show that  $$\text{Vol}(\phi(U))=\int_{U}|\text{det}(d\phi)|,$$ where $\phi$ is $C^1$, $d\phi$ is non-singular, and $U$ is some reasonable set (say open and connected)? Having done the single-variable version of change of variables when $f$ is discontinuous (as presented in baby Rudin, for example), I wanted to adapt the same method to this, but so far I haven't been successful. Many proofs I've seen have passed through several lemmas, but I think there should be a straightforward way. I am comfortable taking for granted that $|\text{det}(M)|$ is how much $M$ dilates the area of a parallelipiped.",,"['integration', 'matrices', 'multivariable-calculus']"
87,Partial Derivative v/s Total Derivative,Partial Derivative v/s Total Derivative,,"I am bit confused regarding the geometrical/logical meaning of partial and total derivative. I have given my confusion with examples as follows Question Suppose we have a function $f(x,y)$ , then how do we write  the limit method of representing $ \frac{\partial  f(x,y)}{\partial x} \text{and} \frac{\mathrm{d} f(x,y)}{\mathrm{d} x}$  at (a,b)? What is the difference? Imagine I have a function $f(t,x,y)= t^4+x^2+y^3+2xy^9\tag 1$ what could be $ \frac{\partial  f(t,x,y)}{\partial x} , \frac{\mathrm{d} f(t,x,y)}{\mathrm{d} x},\frac{\partial  f(t,x,y)}{\partial t} , \frac{\mathrm{d} f(t,x,y)}{\mathrm{d} t}$? What is the difference between them in meaning? Imagine I have a function $f(t,x,y)= t^4+x^2+y^3+2xy^9,x=\psi(t),y=\tau(t)\tag 1$ what could be $ \frac{\partial  f(t,x,y)}{\partial x} , \frac{\mathrm{d} f(t,x,y)}{\mathrm{d} x},\frac{\partial  f(t,x,y)}{\partial t} , \frac{\mathrm{d} f(t,x,y)}{\mathrm{d} t}$? What is the difference between them in meaning? When we can say  $ \frac{\partial  f(x,y)}{\partial x} = \frac{\mathrm{d} f(x,y)}{\mathrm{d} x}$? Can we say  $\frac{\partial^2 f(t,x,y)}{\partial x dy} =\frac{\partial^2 f(t,x,y)}{ dy \partial x}$    if function $f(t,x,y)$  is $C^2$  continuous? We have a function curve ${f(t,g_1,g_2,g_3)}_{3 \times 1} \tag 2$ and we have a  $4 \times 1$ array  called $p=\begin{pmatrix}t\\  g_1\\  g_2\\  g_3 \end{pmatrix}$  All t and $g_i$ are variables, t is the curvilinear parameter . What is the meaning difference between $\frac{\partial f(t,g_1,g_2,g_3)}{\partial p} \text{and} \frac{d f(t,g_1,g_2,g_3)}{d p} $? How do we express the difference in limit notations? NB : Replies written with  question enumeration will be more helpul. These are basics, I know. But I get confused some time. Looking for interpretations beyond equations. Means geometrically or logically coherent one. This is not a homework problem just because it is simple. I made the example for expressing my issue so that I can learn from the result. Thanks for the time to read my question.","I am bit confused regarding the geometrical/logical meaning of partial and total derivative. I have given my confusion with examples as follows Question Suppose we have a function $f(x,y)$ , then how do we write  the limit method of representing $ \frac{\partial  f(x,y)}{\partial x} \text{and} \frac{\mathrm{d} f(x,y)}{\mathrm{d} x}$  at (a,b)? What is the difference? Imagine I have a function $f(t,x,y)= t^4+x^2+y^3+2xy^9\tag 1$ what could be $ \frac{\partial  f(t,x,y)}{\partial x} , \frac{\mathrm{d} f(t,x,y)}{\mathrm{d} x},\frac{\partial  f(t,x,y)}{\partial t} , \frac{\mathrm{d} f(t,x,y)}{\mathrm{d} t}$? What is the difference between them in meaning? Imagine I have a function $f(t,x,y)= t^4+x^2+y^3+2xy^9,x=\psi(t),y=\tau(t)\tag 1$ what could be $ \frac{\partial  f(t,x,y)}{\partial x} , \frac{\mathrm{d} f(t,x,y)}{\mathrm{d} x},\frac{\partial  f(t,x,y)}{\partial t} , \frac{\mathrm{d} f(t,x,y)}{\mathrm{d} t}$? What is the difference between them in meaning? When we can say  $ \frac{\partial  f(x,y)}{\partial x} = \frac{\mathrm{d} f(x,y)}{\mathrm{d} x}$? Can we say  $\frac{\partial^2 f(t,x,y)}{\partial x dy} =\frac{\partial^2 f(t,x,y)}{ dy \partial x}$    if function $f(t,x,y)$  is $C^2$  continuous? We have a function curve ${f(t,g_1,g_2,g_3)}_{3 \times 1} \tag 2$ and we have a  $4 \times 1$ array  called $p=\begin{pmatrix}t\\  g_1\\  g_2\\  g_3 \end{pmatrix}$  All t and $g_i$ are variables, t is the curvilinear parameter . What is the meaning difference between $\frac{\partial f(t,g_1,g_2,g_3)}{\partial p} \text{and} \frac{d f(t,g_1,g_2,g_3)}{d p} $? How do we express the difference in limit notations? NB : Replies written with  question enumeration will be more helpul. These are basics, I know. But I get confused some time. Looking for interpretations beyond equations. Means geometrically or logically coherent one. This is not a homework problem just because it is simple. I made the example for expressing my issue so that I can learn from the result. Thanks for the time to read my question.",,"['calculus', 'integration', 'matrices', 'derivatives', 'partial-derivative']"
88,how to find $(I + uv^T)^{-1}$,how to find,(I + uv^T)^{-1},"Let $u, v \in \mathbb{R}^N, v^Tu \neq -1$. Then I know that $I +uv^T \in \mathbb{R}^{N \times N}$ is invertible and I can verify that $$(I + uv^T)^{-1} = I - \frac{uv^T}{1+v^Tu}.$$ But I am not able to derive that inverse on my own. How to find it actually? That is if am given $A =  I +uv^T$ and I am asked to find $A^{-1}$, how to get the answer?","Let $u, v \in \mathbb{R}^N, v^Tu \neq -1$. Then I know that $I +uv^T \in \mathbb{R}^{N \times N}$ is invertible and I can verify that $$(I + uv^T)^{-1} = I - \frac{uv^T}{1+v^Tu}.$$ But I am not able to derive that inverse on my own. How to find it actually? That is if am given $A =  I +uv^T$ and I am asked to find $A^{-1}$, how to get the answer?",,"['linear-algebra', 'matrices']"
89,Does the upper or lower triangle of a matrix include the main diagonal?,Does the upper or lower triangle of a matrix include the main diagonal?,,"When one refers to the Upper or Lower Triangle of a square matrix, are the terms on the main diagonal included in one, both, or neither of the two matrices?","When one refers to the Upper or Lower Triangle of a square matrix, are the terms on the main diagonal included in one, both, or neither of the two matrices?",,"['linear-algebra', 'matrices']"
90,Show that a linear matrix transformation is bijective iff A is invertible.,Show that a linear matrix transformation is bijective iff A is invertible.,,Suppose a linear transformation $T: M_n(K) \rightarrow M_n(K)$ defined by $T(M) = A M$ for $M \in M_n(K)$. Show that it is bijective IFF $A$ is invertible. I was thinking then that I could show that it is surjective. So suppose there exists a  $B \in  M_n(K)$ such that $T(B) = ?$ What would it equal to show that?,Suppose a linear transformation $T: M_n(K) \rightarrow M_n(K)$ defined by $T(M) = A M$ for $M \in M_n(K)$. Show that it is bijective IFF $A$ is invertible. I was thinking then that I could show that it is surjective. So suppose there exists a  $B \in  M_n(K)$ such that $T(B) = ?$ What would it equal to show that?,,"['linear-algebra', 'matrices', 'vector-spaces', 'linear-transformations']"
91,Why is Doolittle Decomposition Algorithm failing and what should I try next?,Why is Doolittle Decomposition Algorithm failing and what should I try next?,,"I am trying to find the LU Decomposition of the following matrix: So far I have only tried the Doolittle Decomposition algorithm with partial pivoting (it's never failed me before!). As far as I can tell, that algorithm will not work because I end up with a zero on the diagonal that I cannot get rid of by swapping rows. After doing some research, I still do not understand: What is special about this matrix that makes the Doolittle method not work for it?   Would this be considered a ""sparse"" matrix? Is there a better algorithm I should use instead? For your information, I am doing this in the context of solving a system of equations in the form Ax = b, where A is the above matrix and b is something like [0 0 0 5 0 0 0 0 0]^T Thank you for your help!","I am trying to find the LU Decomposition of the following matrix: So far I have only tried the Doolittle Decomposition algorithm with partial pivoting (it's never failed me before!). As far as I can tell, that algorithm will not work because I end up with a zero on the diagonal that I cannot get rid of by swapping rows. After doing some research, I still do not understand: What is special about this matrix that makes the Doolittle method not work for it?   Would this be considered a ""sparse"" matrix? Is there a better algorithm I should use instead? For your information, I am doing this in the context of solving a system of equations in the form Ax = b, where A is the above matrix and b is something like [0 0 0 5 0 0 0 0 0]^T Thank you for your help!",,"['matrices', 'algorithms', 'matrix-equations', 'matrix-decomposition']"
92,Matrices with the same characteristic polynomial,Matrices with the same characteristic polynomial,,"For all the $n \times n$ matrices, let's define an equivalent relation that two matrices  are in the relation iff they have the same characteristic polynomial. How can we characterize the matrices that share the same characteristic polynomial? Why is the characteristic polynomial for a matrix called ""characteristic"", if there can be multiple matrices sharing the same characteristic polynomial? Compared to probability, in probability,  we also have a characteristic function for a probability distribution, and different distributions have different characteristic functions, so a characteristic function does characterize a distribution. Thanks.","For all the $n \times n$ matrices, let's define an equivalent relation that two matrices  are in the relation iff they have the same characteristic polynomial. How can we characterize the matrices that share the same characteristic polynomial? Why is the characteristic polynomial for a matrix called ""characteristic"", if there can be multiple matrices sharing the same characteristic polynomial? Compared to probability, in probability,  we also have a characteristic function for a probability distribution, and different distributions have different characteristic functions, so a characteristic function does characterize a distribution. Thanks.",,"['matrices', 'terminology']"
93,$rk(A)=n$ implies $rk(AB)=rk(B)$,implies,rk(A)=n rk(AB)=rk(B),"Let $A \in Mat_{m\times n}(\mathbb{R})$ and $B \in Mat_{n\times p}(\mathbb{R})$ . Assume $rk(A)=n$ . Prove that $rk(AB)=rk(B)$ . Lets start by proving $rk(B) \ge rk(AB)$ . Indeed, since the rows of $AB$ are linear combination of $B$ 's rows, $AB \subseteq B$ and therefore, $rk(B) \ge rk(AB)$ . Now, I wish to show the other part which is: $rk(AB) \ge rk(B)$ . How should I do that? Or maybe I should prove it with another approach.","Let and . Assume . Prove that . Lets start by proving . Indeed, since the rows of are linear combination of 's rows, and therefore, . Now, I wish to show the other part which is: . How should I do that? Or maybe I should prove it with another approach.",A \in Mat_{m\times n}(\mathbb{R}) B \in Mat_{n\times p}(\mathbb{R}) rk(A)=n rk(AB)=rk(B) rk(B) \ge rk(AB) AB B AB \subseteq B rk(B) \ge rk(AB) rk(AB) \ge rk(B),"['linear-algebra', 'matrices', 'vector-spaces']"
94,How to differentiate such matrix expression?,How to differentiate such matrix expression?,,"Given data $(y_1, ..., y_n)$ and each $y_i \in \mathbb{R}^p$, and $V$ is a $p \times q$ matrix with $q$ orthogonal unit vectors as columns. How do I differentiate the following ? And what is the solution ? $$\min_{V} \sum^N_{i=1} || y_i - V V^T y_i ||^2$$","Given data $(y_1, ..., y_n)$ and each $y_i \in \mathbb{R}^p$, and $V$ is a $p \times q$ matrix with $q$ orthogonal unit vectors as columns. How do I differentiate the following ? And what is the solution ? $$\min_{V} \sum^N_{i=1} || y_i - V V^T y_i ||^2$$",,"['linear-algebra', 'matrices', 'matrix-calculus']"
95,Find bases of Jordan Form for a $3\times 3$ matrix,Find bases of Jordan Form for a  matrix,3\times 3,"Here is my matrix: $A=\begin{bmatrix} 2 &2  &3 \\  1 &3  &3 \\  -1 &-2  &-2  \end{bmatrix}.$ And I know that the Jordan form is $J=\begin{bmatrix} 1 &0  &0 \\  0 &1  &1 \\  0 &0  &1  \end{bmatrix}.$ I have problems when I try to find the invertible matrix $P$ such that $P^{-1}AP=J.$ Here is my trial: Consider $AP=PJ$: $A\begin{bmatrix} p_1 &p_2  &p_3  \end{bmatrix}=\begin{bmatrix} p_1 &p_2  &p_3  \end{bmatrix}\begin{bmatrix} 1 &0  &0 \\  0 &1  &1 \\  0 &0  &1  \end{bmatrix}=\begin{bmatrix} p_1 &p_2  &p_2+p_3  \end{bmatrix}.$ Then, we have $(A-I)p_1=0, (A-I)p_2=0$ and $(A-I)p_2=p_3$. Then, I try to consider $$A-I=\begin{bmatrix} 1 &2  &3  \\  0 & 0& 0  \\   0& 0 &0    \end{bmatrix}.$$ I know that $(-2,1,0)^T$ and $(-3,0,1)^T$ are two linearly independent vectors corresponding to the eigenvalue 1. (I do now know I should do this step or not...) I have stuck for the whole days to compute different bases for different Jordan canonical form of matrices but I do not get a standard way to finish them. I am frustrated and could anyone give me a hand to do a step by step calculation for this example? Thank you so much.","Here is my matrix: $A=\begin{bmatrix} 2 &2  &3 \\  1 &3  &3 \\  -1 &-2  &-2  \end{bmatrix}.$ And I know that the Jordan form is $J=\begin{bmatrix} 1 &0  &0 \\  0 &1  &1 \\  0 &0  &1  \end{bmatrix}.$ I have problems when I try to find the invertible matrix $P$ such that $P^{-1}AP=J.$ Here is my trial: Consider $AP=PJ$: $A\begin{bmatrix} p_1 &p_2  &p_3  \end{bmatrix}=\begin{bmatrix} p_1 &p_2  &p_3  \end{bmatrix}\begin{bmatrix} 1 &0  &0 \\  0 &1  &1 \\  0 &0  &1  \end{bmatrix}=\begin{bmatrix} p_1 &p_2  &p_2+p_3  \end{bmatrix}.$ Then, we have $(A-I)p_1=0, (A-I)p_2=0$ and $(A-I)p_2=p_3$. Then, I try to consider $$A-I=\begin{bmatrix} 1 &2  &3  \\  0 & 0& 0  \\   0& 0 &0    \end{bmatrix}.$$ I know that $(-2,1,0)^T$ and $(-3,0,1)^T$ are two linearly independent vectors corresponding to the eigenvalue 1. (I do now know I should do this step or not...) I have stuck for the whole days to compute different bases for different Jordan canonical form of matrices but I do not get a standard way to finish them. I am frustrated and could anyone give me a hand to do a step by step calculation for this example? Thank you so much.",,['matrices']
96,Spectral Norm of $2\times 2$ symmetric matrix,Spectral Norm of  symmetric matrix,2\times 2,"Consider a $2\times 2$ symmetric matrix, in this case, is there some closed formula for its spectral norm ? By spectral norm I mean the induced 2-norm, there is a definition here . Thanks.","Consider a $2\times 2$ symmetric matrix, in this case, is there some closed formula for its spectral norm ? By spectral norm I mean the induced 2-norm, there is a definition here . Thanks.",,"['matrices', 'normed-spaces']"
97,Find an $n\times n$ integer matrix with determinant 1 and $n$ distinct eigenvalues,Find an  integer matrix with determinant 1 and  distinct eigenvalues,n\times n n,"Pretty much what the title suggests: for any positive integer $n$, I'm looking for an $n$-by-$n$ matrix with integer entries, determinant $1$ and $n$ eigenvalues. In case it is absolutely useless to come up with such a matrix, I'm looking for a proof that such a matrix exists.","Pretty much what the title suggests: for any positive integer $n$, I'm looking for an $n$-by-$n$ matrix with integer entries, determinant $1$ and $n$ eigenvalues. In case it is absolutely useless to come up with such a matrix, I'm looking for a proof that such a matrix exists.",,"['matrices', 'eigenvalues-eigenvectors', 'determinant']"
98,Formula for determinant of this matrix,Formula for determinant of this matrix,,"Let's have matrix $(n-1) \times (n-1)$ $$ \begin{pmatrix} 3 & 1& 1& \cdots& 1 \\ 1 & 4& 1& \cdots& 1 \\ 1 & 1& 5& \cdots& 1 \\ \vdots &\vdots &\vdots &\ddots &\vdots \\ 1 & 1& 1& \cdots& n+1 \\ \end{pmatrix} $$ Let's mark it's determinant $h_n$. $h_2 = 3$ is trivial, but what about formula for the rest? My GF suggested, that formula for the determinant could be $$ h_{n+1} = (n+1)h_n+n! $$ which seems to work just fine. But I have not idea how to go about proving it. Can someone push me in the right direction? Thanks in advance.","Let's have matrix $(n-1) \times (n-1)$ $$ \begin{pmatrix} 3 & 1& 1& \cdots& 1 \\ 1 & 4& 1& \cdots& 1 \\ 1 & 1& 5& \cdots& 1 \\ \vdots &\vdots &\vdots &\ddots &\vdots \\ 1 & 1& 1& \cdots& n+1 \\ \end{pmatrix} $$ Let's mark it's determinant $h_n$. $h_2 = 3$ is trivial, but what about formula for the rest? My GF suggested, that formula for the determinant could be $$ h_{n+1} = (n+1)h_n+n! $$ which seems to work just fine. But I have not idea how to go about proving it. Can someone push me in the right direction? Thanks in advance.",,"['matrices', 'determinant']"
99,The product of a matrix and its transpose can always be written as an exponential,The product of a matrix and its transpose can always be written as an exponential,,"For every real matrix $X$ with $\det X = 1$, there exists a real symmetric traceless matrix $Y$ such that $$ X^TX = e^Y $$ Is this true? If so, why?","For every real matrix $X$ with $\det X = 1$, there exists a real symmetric traceless matrix $Y$ such that $$ X^TX = e^Y $$ Is this true? If so, why?",,['matrices']
