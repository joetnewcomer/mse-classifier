,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Showing that $S_n -\lfloor S_n \rfloor \sim U[0,1]$",Showing that,"S_n -\lfloor S_n \rfloor \sim U[0,1]","$k \in \mathbb{N}$ is fixed $(X_n)_{n \geq 1}$ are all independent and follow an uniform law on $[0,k]$ We define $f(x)=x -\lfloor x \rfloor$ $S_n= \sum_{i=1}^{n} X_i$ $Z_n= f(S_n)$ We want to show that $\forall n \geq 1, S_n -\lfloor S_n \rfloor  \sim U[0,1]$ Here are the steps : I have found a density of $S_2$ Show that $Z_2 \sim U[0,1]$ 3.(a) Express $f(f(S_n) + X_{n+1})$ with $Z_{n+1}$ 3.(b) Deduce that $Z_n \sim U[0,1]$ My attempt: 1. $f_{S_2}(s)= \begin{cases} \frac{1}{k^2} s  \quad \text{si}  \quad  0 \leq s\leq k \\ \frac{1}{k} (2-\frac{s}{k}) \quad \text{si}  \quad k  \leq s \leq 2k\\ \end{cases} $ $F_{S_2}(s)= \begin{cases} \frac{s^2}{2 k^2}  \quad \text{si}  \quad 0 \leq s\leq k \\ 2\frac{s}{k}-\frac{s^2}{2 k^2} -1 \quad \text{si}  \quad  k  \leq s \leq 2k\\ \end{cases} $ For this question, let $Z=Z_2$ $0\leq Z \leq 1 $ For $a \leq 1$ $0\leq Z \leq a \iff Z \in  \bigcup_{j=0}^{j=k-1} [j,j+a]$ $F_Z(a)= \sum_{j=0}^{j=2k-1} F(j+a)-F(j)$ $ \begin{align*} f_Z(a) &= \sum_{j=0}^{j=2k-1} f_S(a+j)  \\ &= \sum_{j=0}^{j=k-1} f_S(a+j) + \sum_{j=k}^{j=2k-1} f_S(a+j) \\ &= \sum_{j=0}^{k-1} \big( \frac{a}{k^2} + \frac{j}{k^2} \big) + \sum_{j=k}^{2k-1} \big( \frac{2}{k} - \frac{a}{k^2} - \frac{j}{k^2}) \\ &=  \big( \sum_{j=0}^{k-1}  \frac{a}{k^2} - \sum_{j=k}^{2k-1}\frac{a}{k^2}  \big) +  \sum_{j=0}^{k-1} \frac{j}{k^2} -  \sum_{j=0}^{k-1} \frac{j+k}{k^2} + \sum_{j=k}^{2k-1}   \frac{2}{k} \\ &= -1 +\sum_{j=k}^{2k-1}  \ \frac{2}{k} \\ &=-1+2=1\\ \end{align*}   $ 3. $f ( f(S_n) + X_{n+1})= f( S_n - \lfloor S_n \rfloor + X_{n+1} )$ Let $Z_n= S_n - \lfloor  S_n\rfloor $ $S_{n+1} = S_n+ X_{n+1}  =  Z_n +  \lfloor  S_n\rfloor + X_{n+1}$ $ S_{n+1} -  \lfloor  S_{n+1}\rfloor = f(  Z_n + X_{n+1} )$ because $f(x+p)=f(x)$ for all integer $p$ so : $f ( f(S_n) + X_{n+1}) = Z_{n+1}$","is fixed are all independent and follow an uniform law on We define We want to show that Here are the steps : I have found a density of Show that 3.(a) Express with 3.(b) Deduce that My attempt: 1. For this question, let For 3. Let because for all integer so :","k \in \mathbb{N} (X_n)_{n \geq 1} [0,k] f(x)=x -\lfloor x \rfloor S_n= \sum_{i=1}^{n} X_i Z_n= f(S_n) \forall n \geq 1, S_n -\lfloor S_n \rfloor  \sim U[0,1] S_2 Z_2 \sim U[0,1] f(f(S_n) + X_{n+1}) Z_{n+1} Z_n \sim U[0,1] f_{S_2}(s)=
\begin{cases}
\frac{1}{k^2} s  \quad \text{si}  \quad  0 \leq s\leq k \\
\frac{1}{k} (2-\frac{s}{k}) \quad \text{si}  \quad k  \leq s \leq 2k\\
\end{cases}
 F_{S_2}(s)=
\begin{cases}
\frac{s^2}{2 k^2}  \quad \text{si}  \quad 0 \leq s\leq k \\
2\frac{s}{k}-\frac{s^2}{2 k^2} -1 \quad \text{si}  \quad  k  \leq s \leq 2k\\
\end{cases}
 Z=Z_2 0\leq Z \leq 1  a \leq 1 0\leq Z \leq a \iff Z \in  \bigcup_{j=0}^{j=k-1} [j,j+a] F_Z(a)= \sum_{j=0}^{j=2k-1} F(j+a)-F(j) 
\begin{align*}
f_Z(a) &= \sum_{j=0}^{j=2k-1} f_S(a+j)  \\
&= \sum_{j=0}^{j=k-1} f_S(a+j) + \sum_{j=k}^{j=2k-1} f_S(a+j) \\
&= \sum_{j=0}^{k-1} \big( \frac{a}{k^2} + \frac{j}{k^2} \big) + \sum_{j=k}^{2k-1} \big( \frac{2}{k} - \frac{a}{k^2} - \frac{j}{k^2}) \\
&=  \big( \sum_{j=0}^{k-1}  \frac{a}{k^2} - \sum_{j=k}^{2k-1}\frac{a}{k^2}  \big)
+  \sum_{j=0}^{k-1} \frac{j}{k^2} -  \sum_{j=0}^{k-1} \frac{j+k}{k^2} + \sum_{j=k}^{2k-1}   \frac{2}{k} \\
&= -1 +\sum_{j=k}^{2k-1}  \ \frac{2}{k} \\
&=-1+2=1\\
\end{align*}
   f ( f(S_n) + X_{n+1})= f( S_n - \lfloor S_n \rfloor + X_{n+1} ) Z_n= S_n - \lfloor  S_n\rfloor  S_{n+1} = S_n+ X_{n+1}  =  Z_n +  \lfloor  S_n\rfloor + X_{n+1}  S_{n+1} -  \lfloor  S_{n+1}\rfloor = f(  Z_n + X_{n+1} ) f(x+p)=f(x) p f ( f(S_n) + X_{n+1}) = Z_{n+1}","['probability', 'probability-distributions', 'uniform-distribution', 'ceiling-and-floor-functions']"
1,How to efficiently sample edges from a graph in relation to its spanning tree,How to efficiently sample edges from a graph in relation to its spanning tree,,"Consider a connected, unweighted, undirected graph $G$ . Let $m$ be the number of edges and $n$ be the number of nodes. Now consider the following random process. First sample a uniformly random spanning tree of $G$ and then pick an edge from this spanning tree uniformly at random. Our process returns the edge. If I want to sample many edges from $G$ from the probability distribution implied by this process, is there a more efficient  (in terms of computational complexity) method than sampling a new random spanning tree each time?","Consider a connected, unweighted, undirected graph . Let be the number of edges and be the number of nodes. Now consider the following random process. First sample a uniformly random spanning tree of and then pick an edge from this spanning tree uniformly at random. Our process returns the edge. If I want to sample many edges from from the probability distribution implied by this process, is there a more efficient  (in terms of computational complexity) method than sampling a new random spanning tree each time?",G m n G G,['probability']
2,Probability of Getting a Red Ball,Probability of Getting a Red Ball,,"I have a simple and straightforward question. A box contains $n$ balls, of which $r$ are red ( $r$ and $n$ are both positive integers, and $r \leq n$ ; suppose further that $n$ is even). Consider what happens when the balls are drawn from the box one at a time, at random without replacement. Determine: $\quad$ (a) The probability that the first ball drawn will be red; $\quad$ (b) The probability that the $\left(\frac{n}{2}\right)^{\text{th}}$ ball drawn will be red; $\quad$ (c) the probability that the last ball drawn will be red. I'm not sure how to approach questions (b) and (c) . I understand that (a) is $\frac rn$ because the probability of the very first ball being red is the ratio of all the red balls over the total number of balls, but I don't know how to extend this idea to the $i^{\, \text{th}}$ ball. Thank you for your time.","I have a simple and straightforward question. A box contains balls, of which are red ( and are both positive integers, and ; suppose further that is even). Consider what happens when the balls are drawn from the box one at a time, at random without replacement. Determine: (a) The probability that the first ball drawn will be red; (b) The probability that the ball drawn will be red; (c) the probability that the last ball drawn will be red. I'm not sure how to approach questions (b) and (c) . I understand that (a) is because the probability of the very first ball being red is the ratio of all the red balls over the total number of balls, but I don't know how to extend this idea to the ball. Thank you for your time.","n r r n r \leq n n \quad \quad \left(\frac{n}{2}\right)^{\text{th}} \quad \frac rn i^{\, \text{th}}",['probability']
3,Is it true that $\mathbb{E}[X^n] > \mathbb{E}[X^{n-1}]\mathbb{E}[X]$ for all $n \geq 2$?,Is it true that  for all ?,\mathbb{E}[X^n] > \mathbb{E}[X^{n-1}]\mathbb{E}[X] n \geq 2,"Let $X$ denote a random variable with a smooth distribution over $[0, 1]$ . Is it true that $$\mathbb{E}[X^n] > \mathbb{E}[X^{n-1}]\mathbb{E}[X]$$ for all integers $n = 2, 3, ...$ ? This seems to be a simple application of Jensen's inequality: however, the exact proof eludes me! My thoughts so far: In the case of $n = 2$ , we know that $$\mathbb{E}[X^2] > \mathbb{E}[X]\mathbb{E}[X] = \mathbb{E}[X]^2$$ by Jensen's inequality (since the function $f(x) = x^2$ is strictly convex on $[0, 1]$ ). Similarly, Jensen's inequality tells us that $$\mathbb{E}[X^n] > \mathbb{E}[X]^{n} = \mathbb{E}[X]^{n-1}\mathbb{E}[X]$$ but that is, unfortunately, not quite the inequality I am after. Finally, in the case where $X$ is uniformly distributed, one can calculate that $$ \mathbb{E}[X^n] = \frac{1}{n+1}$$ and similarly $$ \mathbb{E}[X^{n-1}] = \frac{1}{n}$$ $$ \mathbb{E}[X] = \frac{1}{2}$$ which allows one to verify that $$\mathbb{E}[X^n] = \frac{1}{n+1} > \mathbb{E}[X^{n-1}]\mathbb{E}[X] = \frac{1}{2n}$$ for all $n > 1$ . So this inequality holds in the uniform case, at least.","Let denote a random variable with a smooth distribution over . Is it true that for all integers ? This seems to be a simple application of Jensen's inequality: however, the exact proof eludes me! My thoughts so far: In the case of , we know that by Jensen's inequality (since the function is strictly convex on ). Similarly, Jensen's inequality tells us that but that is, unfortunately, not quite the inequality I am after. Finally, in the case where is uniformly distributed, one can calculate that and similarly which allows one to verify that for all . So this inequality holds in the uniform case, at least.","X [0, 1] \mathbb{E}[X^n] > \mathbb{E}[X^{n-1}]\mathbb{E}[X] n = 2, 3, ... n = 2 \mathbb{E}[X^2] > \mathbb{E}[X]\mathbb{E}[X] = \mathbb{E}[X]^2 f(x) = x^2 [0, 1] \mathbb{E}[X^n] > \mathbb{E}[X]^{n} = \mathbb{E}[X]^{n-1}\mathbb{E}[X] X  \mathbb{E}[X^n] = \frac{1}{n+1}  \mathbb{E}[X^{n-1}] = \frac{1}{n}  \mathbb{E}[X] = \frac{1}{2} \mathbb{E}[X^n] = \frac{1}{n+1} > \mathbb{E}[X^{n-1}]\mathbb{E}[X] = \frac{1}{2n} n > 1","['probability', 'probability-theory', 'jensen-inequality']"
4,Expected number of people seated between A and B,Expected number of people seated between A and B,,"$N (\ge 3)$ people sit at a round table. Every seating is equiprobable. Among those people are $3$ called $A$ , $B$ and $C$ . Let $X$ be a number of people siting between $A$ and $B$ (on the arc which does not include $C$ ). What is the expected value and variance of $X$ ? I've tried some combinatorics, but I can't seem to stop overcounting.","people sit at a round table. Every seating is equiprobable. Among those people are called , and . Let be a number of people siting between and (on the arc which does not include ). What is the expected value and variance of ? I've tried some combinatorics, but I can't seem to stop overcounting.",N (\ge 3) 3 A B C X A B C X,"['probability', 'combinatorics', 'expected-value', 'variance']"
5,"Is $\int_{\mathbb{R}} |F(x)-G(x)|^2(f(x)-g(x))dx =0$ true, for $f,g$ probability densities and $F,G$ the corresponding distribution functions?","Is  true, for  probability densities and  the corresponding distribution functions?","\int_{\mathbb{R}} |F(x)-G(x)|^2(f(x)-g(x))dx =0 f,g F,G","I came across the following inequality while doing a problem. I guess it should be true, but I am not able to prove it. Here is the situation. Let $f$ and $g$ be two probability densities on $\mathbb{R}.$ Let $F$ and $G$ be the distribution function corresponding to $f$ and $g,$ respectively. I am looking at the following integral $$I=\int\limits_{\mathbb{R}} |F(x)-G(x)|^2(f(x)-g(x))dx.$$ I guess that $I=0.$ The particular case I came across has further simplifications. For example, in my case, $g$ is uniform density on $[0, 1]$ and $f$ is a density supported on $[0, 1].$ The problem, in that case, reduces(?) to $$\int\limits_{0}^{1}|F(x)-x|^2(f(x)-1)dx=0.$$ Unfortunately, I could not come up with a proof. Any ideas?","I came across the following inequality while doing a problem. I guess it should be true, but I am not able to prove it. Here is the situation. Let and be two probability densities on Let and be the distribution function corresponding to and respectively. I am looking at the following integral I guess that The particular case I came across has further simplifications. For example, in my case, is uniform density on and is a density supported on The problem, in that case, reduces(?) to Unfortunately, I could not come up with a proof. Any ideas?","f g \mathbb{R}. F G f g, I=\int\limits_{\mathbb{R}} |F(x)-G(x)|^2(f(x)-g(x))dx. I=0. g [0, 1] f [0, 1]. \int\limits_{0}^{1}|F(x)-x|^2(f(x)-1)dx=0.","['real-analysis', 'probability', 'analysis', 'probability-theory']"
6,"Gaussian integration by parts, Wick's / Isserlis' theorem?","Gaussian integration by parts, Wick's / Isserlis' theorem?",,"Wikipedia's entry on Isserlis' theorem has the following identity: $$ \mathbb{E}\Big[ X_1 f(X_1, ..., X_n) \Big] = \sum_{i=1}^n \mathbb{E}[X_1 X_i] \mathbb{E}\Big[ \frac{\partial}{\partial X_i} f(X_1, ..., X_n) \Big],$$ where $X_1, ..., X_n $ is a zero-mean multivariate Gaussian random vector. My questions: What is this theorem called? Is this some incarnation of Wick's theorem? (I do not understand the notation on this page at all). What are the conditions required  on $f$ for this identity to hold?",Wikipedia's entry on Isserlis' theorem has the following identity: where is a zero-mean multivariate Gaussian random vector. My questions: What is this theorem called? Is this some incarnation of Wick's theorem? (I do not understand the notation on this page at all). What are the conditions required  on for this identity to hold?," \mathbb{E}\Big[ X_1 f(X_1, ..., X_n) \Big] = \sum_{i=1}^n \mathbb{E}[X_1 X_i] \mathbb{E}\Big[ \frac{\partial}{\partial X_i} f(X_1, ..., X_n) \Big], X_1, ..., X_n  f","['probability', 'integration', 'gaussian-integral']"
7,Probability that girl who answers the door is the eldest girl?,Probability that girl who answers the door is the eldest girl?,,"You know that a family has 3 children. You walk up to and knock on the front door of their house. A girl answers the door. What is the probability that the she is the eldest girl among the children? Assume that all 3 children are home and equally likely to answer the door. (I appreciate that with questions like this, the wording can be so crucial. If the question is in any way ambiguous, I would be delighted to have an explanation of why that is the case!)","You know that a family has 3 children. You walk up to and knock on the front door of their house. A girl answers the door. What is the probability that the she is the eldest girl among the children? Assume that all 3 children are home and equally likely to answer the door. (I appreciate that with questions like this, the wording can be so crucial. If the question is in any way ambiguous, I would be delighted to have an explanation of why that is the case!)",,"['probability', 'recreational-mathematics']"
8,"Let $\lambda \in \mathbb{R}, \lambda > 0$ and let $X, Y, Z \sim P(\lambda)$ (they have Poissons distribution) independent random variables..",Let  and let  (they have Poissons distribution) independent random variables..,"\lambda \in \mathbb{R}, \lambda > 0 X, Y, Z \sim P(\lambda)","Let $\lambda \in \mathbb{R}, \lambda > 0$ and let $X, Y, Z \sim P(\lambda)$ (they have Poissons distribution) independent random variables. Calculate $Var (XYZ) $ . I tried by calculating $ \mathbb{E} (XYZ) ^2 ( = \lambda ^6)$ because $X,Y,Z$ are independent and $(\mathbb{E} (XYZ) )^2 ( = \lambda ^6)$ (let $g$ be function so $g(X) = X^2$ and then because $X,Y,Z$ are independent so are $g(X), g(Y), g(Z))$ which means $Var(XYZ) =0$ . Is that correct?",Let and let (they have Poissons distribution) independent random variables. Calculate . I tried by calculating because are independent and (let be function so and then because are independent so are which means . Is that correct?,"\lambda \in \mathbb{R}, \lambda > 0 X, Y, Z \sim P(\lambda) Var (XYZ)   \mathbb{E} (XYZ) ^2 ( = \lambda ^6) X,Y,Z (\mathbb{E} (XYZ) )^2 ( = \lambda ^6) g g(X) = X^2 X,Y,Z g(X), g(Y), g(Z)) Var(XYZ) =0",['probability']
9,"$X$ is a random variable, if $\Bbb E(X^2)=1$ and $\Bbb E(X)\geq a>0$, prove that $\Bbb P(X\geq\lambda a)\geq(a-\lambda a)^2$ for $0\leq\lambda\leq 1$.","is a random variable, if  and , prove that  for .",X \Bbb E(X^2)=1 \Bbb E(X)\geq a>0 \Bbb P(X\geq\lambda a)\geq(a-\lambda a)^2 0\leq\lambda\leq 1,"This is a problem in KaiLai Chung's A Course in Probability Theory . Given a nonnegative random variable $X$ defined on $\Omega$ , if $\mathbb{E}(X^2)=1$ and $\mathbb{E}(X)\geq a >0$ , prove that $$\mathbb{P}(X\geq \lambda a)\geq (a-\lambda a)^2$$ for $0\leq\lambda \leq 1$ . Let $A=\{x\in \Omega:X(x)\geq \lambda a\}$ , we get $$\int_A (X-\lambda a)\geq a-\int_A\lambda a -\int_{A^c}X$$ and $$\int_A (X^2-\lambda^2 a^2)=1-\int_A\lambda^2a^2-\int_{A^c}X^2$$ I want to contrast $\int_A (X-\lambda a)$ and $\int_A (X^2-\lambda^2 a^2)$ , but I don't know how to do it, could anyone gives me some hints?","This is a problem in KaiLai Chung's A Course in Probability Theory . Given a nonnegative random variable defined on , if and , prove that for . Let , we get and I want to contrast and , but I don't know how to do it, could anyone gives me some hints?",X \Omega \mathbb{E}(X^2)=1 \mathbb{E}(X)\geq a >0 \mathbb{P}(X\geq \lambda a)\geq (a-\lambda a)^2 0\leq\lambda \leq 1 A=\{x\in \Omega:X(x)\geq \lambda a\} \int_A (X-\lambda a)\geq a-\int_A\lambda a -\int_{A^c}X \int_A (X^2-\lambda^2 a^2)=1-\int_A\lambda^2a^2-\int_{A^c}X^2 \int_A (X-\lambda a) \int_A (X^2-\lambda^2 a^2),"['probability', 'integration', 'lp-spaces']"
10,"In a right angled $\triangle ABC$, $DE$ and $DF$ are perpendicular to $AB$ and $BC$ respectively. What is the probability of $DE\cdot DF>3$?","In a right angled ,  and  are perpendicular to  and  respectively. What is the probability of ?",\triangle ABC DE DF AB BC DE\cdot DF>3,"In a right angled $\triangle ABC$ , $\angle B = 90^\circ$ , $\angle C = 15^\circ$ and $|AC| = 7.\;$ Let a point $D$ ( Random Point ) be taken on $AC$ and then perpendicular lines $DE$ and $DF$ are drawn on $AB$ and $AC$ respectively. What is the probability of $DE\cdot DF >3?$ Attempt : By trigonometry, I got the length of other two side from the hypotenuse $AC:$ $AB$ $\approx 1.8117$ $BC$ $\approx 6.7614$ . And than, I got the equation that $DE\cdot DF = (6.7614 - DE)\cdot AE\;$ (from the similarity of both $\triangle AED$ and $\triangle DFC$ ) Again, from right angled $\triangle AED$ , $\dfrac{AE}{DE} =  \tan 15^{\circ}\quad \implies \quad AE = DE\cdot \tan 15^\circ$ Here, I got stuck. I couldn't find a way out to proceed and skip that situation. I became lost and was unable to complete that process. Any kind of help or clue will be greatly helpful for me to step forward.","In a right angled , , and Let a point ( Random Point ) be taken on and then perpendicular lines and are drawn on and respectively. What is the probability of Attempt : By trigonometry, I got the length of other two side from the hypotenuse . And than, I got the equation that (from the similarity of both and ) Again, from right angled , Here, I got stuck. I couldn't find a way out to proceed and skip that situation. I became lost and was unable to complete that process. Any kind of help or clue will be greatly helpful for me to step forward.",\triangle ABC \angle B = 90^\circ \angle C = 15^\circ |AC| = 7.\; D AC DE DF AB AC DE\cdot DF >3? AC: AB \approx 1.8117 BC \approx 6.7614 DE\cdot DF = (6.7614 - DE)\cdot AE\; \triangle AED \triangle DFC \triangle AED \dfrac{AE}{DE} =  \tan 15^{\circ}\quad \implies \quad AE = DE\cdot \tan 15^\circ,"['probability', 'geometry', 'contest-math', 'triangles']"
11,Does there exist any probability density function ‎$‎f:‎\mathbb{R}\to‎\mathbb{R}‎$ ‎which is not Riemann integrable?,Does there exist any probability density function ‎ ‎which is not Riemann integrable?,‎f:‎\mathbb{R}\to‎\mathbb{R}‎,"Let‎  ‎ $‎‎f:‎\mathbb{R}\to‎\mathbb{R}‎$ be a probability density function.   Can ‎the following  be happened for ‎ $‎‎f$ ? (1) ‎‎ $‎‎f$ ‎is ‎not ‎integrable ‎on ‎an ‎(some) ‎interval ‎of ‎‎ $\mathbb{R}‎$ . ‎(2) ‎‎ $‎‎f$ ‎is ‎not ‎integrable ‎on every closed ‎interval ‎of ‎‎ $\mathbb{R}‎$ .‎‎‎ ‎ I know that ‎if $‎f‎$ ‎is a‎ ‎‎probability density function then (1) ‎ $‎‎f(x)‎\geq‎0 ‎\quad‎\text{for all} \; x$ , (2) ‎ $\int_{-‎\infty‎}^{+\infty}f(x)\,dx=1$ . but ‎here ‎we ‎have‎ Lebesgue integral not Riemann integral. Moreover if ‎ $‎‎f$ ‎wants ‎to ‎be‎ Riemann integrable on the whole $\mathbb{R}‎$ , it must hold in the following conditions ‎‎(a) ‎‎ $‎‎f$ ‎is ‎integrable ‎on every closed ‎interval ‎of ‎ $\mathbb{R},‎$ ‎ (b) the following integral is convergent‎ $$\int_{-‎\infty‎}^{+\infty}f(x)\,dx=‎‎\int_{-‎\infty‎}^{0}f(x)\,dx+\int_{0‎}^{+\infty}f(x)\,dx.$$ According the mentioned things, the most pdf  are ‎ Riemann integrable, and I could not find any example as I asked. Would anyone help me to find that. thanks a lot. ‎","Let‎  ‎ be a probability density function.   Can ‎the following  be happened for ‎ ? (1) ‎‎ ‎is ‎not ‎integrable ‎on ‎an ‎(some) ‎interval ‎of ‎‎ . ‎(2) ‎‎ ‎is ‎not ‎integrable ‎on every closed ‎interval ‎of ‎‎ .‎‎‎ ‎ I know that ‎if ‎is a‎ ‎‎probability density function then (1) ‎ , (2) ‎ . but ‎here ‎we ‎have‎ Lebesgue integral not Riemann integral. Moreover if ‎ ‎wants ‎to ‎be‎ Riemann integrable on the whole , it must hold in the following conditions ‎‎(a) ‎‎ ‎is ‎integrable ‎on every closed ‎interval ‎of ‎ ‎ (b) the following integral is convergent‎ According the mentioned things, the most pdf  are ‎ Riemann integrable, and I could not find any example as I asked. Would anyone help me to find that. thanks a lot. ‎","‎‎f:‎\mathbb{R}\to‎\mathbb{R}‎ ‎‎f ‎‎f \mathbb{R}‎ ‎‎f \mathbb{R}‎ ‎f‎ ‎‎f(x)‎\geq‎0 ‎\quad‎\text{for all} \; x \int_{-‎\infty‎}^{+\infty}f(x)\,dx=1 ‎‎f \mathbb{R}‎ ‎‎f \mathbb{R},‎ \int_{-‎\infty‎}^{+\infty}f(x)\,dx=‎‎\int_{-‎\infty‎}^{0}f(x)\,dx+\int_{0‎}^{+\infty}f(x)\,dx.","['probability', 'integration', 'probability-theory', 'probability-distributions']"
12,Probability of getting out of a circular area,Probability of getting out of a circular area,,"An airplane is moving (straight) within a circle of radius $R$ with constant speed $V$ for $t$ seconds. It can start at any place within the circle and move in each direction (uniform distributions).  What is the probability that it gets out of the circle (as a function of $V$ , $t$ and $R$ )?","An airplane is moving (straight) within a circle of radius with constant speed for seconds. It can start at any place within the circle and move in each direction (uniform distributions).  What is the probability that it gets out of the circle (as a function of , and )?",R V t V t R,"['probability', 'geometry', 'circles']"
13,Table tennis win probability,Table tennis win probability,,"This problem is from my teacher and I think their answer is wrong. The problem is in the context of table tennis. The players in the tournament final are Ani and Bertha. The score in the game is drawn at 20-20. The final game will continue until one player has scored two more points than the other. It is known from previous games between Ani and Bertha that the probability of Ani winning each point is 0.6. Find the probability that Ani will win the game after exactly 8 more points. I think that this means that, over the next 6 games on the 2nd, 4th and 6th game Ani and Bertha need to have a draw. For each draw there are two possible paths. Ani wins then Bertha or Bertha then Ani. After the 6th game Ani just needs to win twice to win after exactly 8 points. However my teacher says that: If Bertha wins the first game, it is not possible for Ani to win after exactly 8 points. and also asserts that there is only one path to the desired outcome. Using this they find the probability of Ani winning after exactly 8 points to be 0.005. ( Here is the linked image: I found an alternate answer using multiple paths. $P(\text{Ani winning a game})=0.6$ $P(\text{Bertha winning a game})=0.4$ $P(\text{Ani win after 8 points})=2\dot(0.4 \cdot 0.6)\cdot2\dot(0.4 \cdot 0.6)\cdot2\dot(0.4 \cdot 0.6)\cdot(0.6\cdot0.6)=0.040\ (2sf)$ After I found this answer I asked my teacher if the proposed answer was correct. My teacher replied saying that there was nothing wrong with it. Am I missing something painfully obvious and if so what? or is the teacher's answer incorrect?","This problem is from my teacher and I think their answer is wrong. The problem is in the context of table tennis. The players in the tournament final are Ani and Bertha. The score in the game is drawn at 20-20. The final game will continue until one player has scored two more points than the other. It is known from previous games between Ani and Bertha that the probability of Ani winning each point is 0.6. Find the probability that Ani will win the game after exactly 8 more points. I think that this means that, over the next 6 games on the 2nd, 4th and 6th game Ani and Bertha need to have a draw. For each draw there are two possible paths. Ani wins then Bertha or Bertha then Ani. After the 6th game Ani just needs to win twice to win after exactly 8 points. However my teacher says that: If Bertha wins the first game, it is not possible for Ani to win after exactly 8 points. and also asserts that there is only one path to the desired outcome. Using this they find the probability of Ani winning after exactly 8 points to be 0.005. ( Here is the linked image: I found an alternate answer using multiple paths. After I found this answer I asked my teacher if the proposed answer was correct. My teacher replied saying that there was nothing wrong with it. Am I missing something painfully obvious and if so what? or is the teacher's answer incorrect?",P(\text{Ani winning a game})=0.6 P(\text{Bertha winning a game})=0.4 P(\text{Ani win after 8 points})=2\dot(0.4 \cdot 0.6)\cdot2\dot(0.4 \cdot 0.6)\cdot2\dot(0.4 \cdot 0.6)\cdot(0.6\cdot0.6)=0.040\ (2sf),"['probability', 'decision-trees']"
14,Probability that exactly 2 balls are white,Probability that exactly 2 balls are white,,"What I did. I let $X$ be number of withdraws before $x$ white balls. We can call our succes in this case to be gettin a white ball and probability is of course $p = \frac{3}{6} = \frac{1}{2}$ . So we see $X$ is negative binomial r.v with $n=4$ trials. So, $$ P(X=2) = { 4 - 1 \choose 2 - 1} \left( \frac{1}{2} \right)^2 \left( \frac{1}{2} \right)^2 $$ Which gives $$ P(X=2) = \boxed{\dfrac{3}{16} }$$ Am I interpreting the problem correctly?","What I did. I let be number of withdraws before white balls. We can call our succes in this case to be gettin a white ball and probability is of course . So we see is negative binomial r.v with trials. So, Which gives Am I interpreting the problem correctly?",X x p = \frac{3}{6} = \frac{1}{2} X n=4  P(X=2) = { 4 - 1 \choose 2 - 1} \left( \frac{1}{2} \right)^2 \left( \frac{1}{2} \right)^2   P(X=2) = \boxed{\dfrac{3}{16} },['probability']
15,Random Bridge Hand w Cards of exactly two suits,Random Bridge Hand w Cards of exactly two suits,,"Question: What is the probability that a random bridge hand contains cards of exactly two suits? My Attempt At A Solution Bridge hands consist of $13$ cards, and a suit contains $52$ cards, so the way to pick a random bridge hand would be $$\frac{{26\choose 13}-2}{{52 \choose 13}} $$ As user @Lord Shark the Unknown hinted: there are $26\choose 13$ ways to choose hands of two suits but one of those suits is only say hearts, and another only spades, so we must compensate for those. Thank you for any corrections/hints.","Question: What is the probability that a random bridge hand contains cards of exactly two suits? My Attempt At A Solution Bridge hands consist of cards, and a suit contains cards, so the way to pick a random bridge hand would be As user @Lord Shark the Unknown hinted: there are ways to choose hands of two suits but one of those suits is only say hearts, and another only spades, so we must compensate for those. Thank you for any corrections/hints.",13 52 \frac{{26\choose 13}-2}{{52 \choose 13}}  26\choose 13,"['probability', 'combinatorics', 'combinations', 'card-games']"
16,Probability of a sequence of coin tosses ending in $HHT$ (I win) or $THH$(You win),Probability of a sequence of coin tosses ending in  (I win) or (You win),HHT THH,"I'm still learning undergraduate probability. I was asked this probability puzzle in a recent quantitative developer interview. I solved the first part of the question, using brute-force. I think, brute-force very quickly becomes unwieldy for the sequence ending $THH$ - not sure if my answer is correct. A coin is flipped infinitely until you or I win. If at any point, the last three tosses in the sequence are $HHT$, I win. If at any point, the last three tosses in the sequence are $THH$, you win. Which sequence is more likely? Solution. $\begin{aligned} P(xHHT)&=P(H^2T)+P(H^3T)+P(H^4T)+\ldots \\ &=\frac{1}{2^3}+\frac{1}{2^4} + \frac{1}{2^5} + \ldots \\ &=\frac{1/8}{1-1/2}\\ &=\frac{1}{4} \end{aligned}$ For the second part, $P(xTHH)$, I have drawn a state-diagram, but there are just too many possible combinations for a sequence ending in $THH$. Is there an easier, or perhaps an intuitive way to look at this? Any hints in the right direction would be great!","I'm still learning undergraduate probability. I was asked this probability puzzle in a recent quantitative developer interview. I solved the first part of the question, using brute-force. I think, brute-force very quickly becomes unwieldy for the sequence ending $THH$ - not sure if my answer is correct. A coin is flipped infinitely until you or I win. If at any point, the last three tosses in the sequence are $HHT$, I win. If at any point, the last three tosses in the sequence are $THH$, you win. Which sequence is more likely? Solution. $\begin{aligned} P(xHHT)&=P(H^2T)+P(H^3T)+P(H^4T)+\ldots \\ &=\frac{1}{2^3}+\frac{1}{2^4} + \frac{1}{2^5} + \ldots \\ &=\frac{1/8}{1-1/2}\\ &=\frac{1}{4} \end{aligned}$ For the second part, $P(xTHH)$, I have drawn a state-diagram, but there are just too many possible combinations for a sequence ending in $THH$. Is there an easier, or perhaps an intuitive way to look at this? Any hints in the right direction would be great!",,['probability']
17,"We have two coins, A and B. For each toss of coin A, the probability of getting head is 1/2...","We have two coins, A and B. For each toss of coin A, the probability of getting head is 1/2...",,"We have two coins, A and B. For each toss of coin A, the probability of getting head is 1/2 and for each toss of coin B, the probability of getting Heads is 1/3. All tosses of the same coin are independent. We select a coin at random and toss it till we get a head. The probability of selecting coin A is ¼ and coin B is 3/4. What is the expected number of tosses to get the first heads? The above problem is taken from the website https://www.analyticsvidhya.com/blog/2017/04/40-questions-on-probability-for-all-aspiring-data-scientists/ question 11 My solution is either 1/(1/4*1/2 + 3/4*1/3)=8/3, including the success toss, or 5/3, not including the success toss. My understanding is that it is geometric distribution question. But the solution provided is 2.75, with the following explanation: ""If coin A is selected then the number of times the coin would be tossed for a guaranteed Heads is 2, similarly, for coin B it is 3. Thus the number of times would be Tosses = 2 * (1/4)[probability of selecting coin A] + 3*(3/4)[probability of selecting coin B] = 2.75"" Is the solution provided incorrect? Or am I missing something?","We have two coins, A and B. For each toss of coin A, the probability of getting head is 1/2 and for each toss of coin B, the probability of getting Heads is 1/3. All tosses of the same coin are independent. We select a coin at random and toss it till we get a head. The probability of selecting coin A is ¼ and coin B is 3/4. What is the expected number of tosses to get the first heads? The above problem is taken from the website https://www.analyticsvidhya.com/blog/2017/04/40-questions-on-probability-for-all-aspiring-data-scientists/ question 11 My solution is either 1/(1/4*1/2 + 3/4*1/3)=8/3, including the success toss, or 5/3, not including the success toss. My understanding is that it is geometric distribution question. But the solution provided is 2.75, with the following explanation: ""If coin A is selected then the number of times the coin would be tossed for a guaranteed Heads is 2, similarly, for coin B it is 3. Thus the number of times would be Tosses = 2 * (1/4)[probability of selecting coin A] + 3*(3/4)[probability of selecting coin B] = 2.75"" Is the solution provided incorrect? Or am I missing something?",,"['probability', 'expected-value']"
18,"If we have infinite variance, can the expectation 'mean' anything?","If we have infinite variance, can the expectation 'mean' anything?",,If we have a random variable $X$ with infinite variance $Var(X)$ then how can the expectation $E(X)$ be useful to us? In the sense that our values vary so much from it that it holds no relevance. Maybe an example would clear things up?,If we have a random variable $X$ with infinite variance $Var(X)$ then how can the expectation $E(X)$ be useful to us? In the sense that our values vary so much from it that it holds no relevance. Maybe an example would clear things up?,,"['probability', 'expectation', 'variance']"
19,A disgruntled secretary problem - why my solution is incorrect,A disgruntled secretary problem - why my solution is incorrect,,"There are $n$ letters addressed to $n$ different people. The $n$ addresses are typed on $n$ envelopes. A disgruntled secretary shuffles the letters and puts them in the envelopes in random order, one letter per envelope. We are to find the probability that at least one letter is put in a correctly addressed envelope. I tried to solve this problem in the following way: I calculated that there are $n!$ possibilities of inserting the letters into the envelopes. I thought of the envelopes as boxes. So if the letter is put in the correct box that implicates that the letter will be given to the proper person. So now: imagine one person gets the right letter, that gives us: $${{n} \choose {1}} (n-1)!$$ different situations. ${n\choose1}$ because people are different, imagine two people get the right letter, this case leads to: $${{n} \choose {2}} (n-2)!$$ And so on... The results above suggest that the searched probability is equal to: $$P(A) = \frac{\sum_{k=1}^n{n\choose k}(n-k)!}{n!}$$ The answer is of course wrong. I've wondered why and I think that the problem lies in calculating the number of people who will be given the right letter, because when we imagine that one person gets the right one and we write ${n \choose 1}(n-1)!$ it doesn't mean that he or she is the ONLY person who gets the right letter, does it? What is the right answer? Is it: $\sum_{i=1}^n\binom{n}{i}(n-i)!(-1)^{i+1}$? If yes, why?","There are $n$ letters addressed to $n$ different people. The $n$ addresses are typed on $n$ envelopes. A disgruntled secretary shuffles the letters and puts them in the envelopes in random order, one letter per envelope. We are to find the probability that at least one letter is put in a correctly addressed envelope. I tried to solve this problem in the following way: I calculated that there are $n!$ possibilities of inserting the letters into the envelopes. I thought of the envelopes as boxes. So if the letter is put in the correct box that implicates that the letter will be given to the proper person. So now: imagine one person gets the right letter, that gives us: $${{n} \choose {1}} (n-1)!$$ different situations. ${n\choose1}$ because people are different, imagine two people get the right letter, this case leads to: $${{n} \choose {2}} (n-2)!$$ And so on... The results above suggest that the searched probability is equal to: $$P(A) = \frac{\sum_{k=1}^n{n\choose k}(n-k)!}{n!}$$ The answer is of course wrong. I've wondered why and I think that the problem lies in calculating the number of people who will be given the right letter, because when we imagine that one person gets the right one and we write ${n \choose 1}(n-1)!$ it doesn't mean that he or she is the ONLY person who gets the right letter, does it? What is the right answer? Is it: $\sum_{i=1}^n\binom{n}{i}(n-i)!(-1)^{i+1}$? If yes, why?",,['probability']
20,Volume of a high dimensional cone,Volume of a high dimensional cone,,"I would like to choose arbitrarily two vectors $a$ and $b$ $\in \mathbb{S}^{n-1}$ and I would like to calculate the probability they are at least some distance $\delta$ apart. This probability should be $$\frac{|\mathbb{S}^{n-1}|-|\{ x \in \mathbb{S}^{n-1}: \|x-a\| \leq \delta \}|}{|\mathbb{S}^{n-1}|}$$ So (I think), my problem consists of finding $|\{ x \in \mathbb{S}^{n-1}: \|x-a\| \leq \delta \}|$ ($|\cdot|$ being the surface measure in $\mathbb{R}^n$). I suppose $a$ can be taken as the unit vector $(1, 0, \cdots, 0)$. By symmetry, the points $\{ x \in \mathbb{S}^{n-1}: \|x-a\| = \delta \}$ should be a ""circle"" with center somewhere on the line between $(0, \cdots, 0)$ and $a$.  I need to find that center and the radius. I search for a point on the ""circle"" with only the first two components being non-zero: The center should have coordinates $(1-x, 0, \cdots, 0)$ and the point on the circle has coordinates $(1-x, y, 0, \cdots, 0)$. $x$ and $y$ need to verify $$x^2 + y^2 = \delta$$ and $$(1-x)^2 + y^2 = 1$$ This gives $x = \delta / 2$ and $y = (\sqrt{3}/4) \delta$. Going back to the surface we want to calculate: $$|\{ x \in \mathbb{S}^{n-1}: \|x-a\| \leq \delta \}| = |\{x \in \mathbb{S}^{n-1}\}\cap \{B((1-\delta / 2, 0, \cdots, 0), r = (\sqrt{3}/4) \delta)\}$$ This should just be the measure of the spherical cap of the sphere centered at $(1-\delta / 2, 0, \cdots, 0)$. To estimate this surface area, I wanted to use that surface area is the push-forward measure of a Gaussian measure on $\mathbb{R}^n$ under $x \rightarrow x/\|x\|$ (times the measure of $\mathbb{S}^{n-1}$ since the latter is a probability measure). So to estimate the above measure, we can also calculate the Gaussian measure of a cone (rays going from $0$ to points in $\{x \in \mathbb{S}^{n-1}: \|x-a\| \leq \delta\})$. Parametrizing this cone, I get $(t, \text{""disc of radius"" }t*\frac{\sqrt{3}\delta}{4(1-\delta/2)}) = (t, disc)$ $$(2\pi)^{-n/2}\int_{(t, disc)}e^{-\|x\|^2/2} = (2\pi)^{-n/2}\int_0^{\infty}e^{-t^2/2}\int_{(disc)}e^{(x_2^2 + x_3^2 + \cdots + x_n^2)/2}dxdt$$ The last integral is over a radial function so $$ = (2\pi)^{-n/2}\int_0^{\infty}e^{-t^2/2}\omega_{n-2}\int_0^{t*\frac{\sqrt{3}\delta}{4(1-\delta/2)} }e^{-s^2/2}dsdt$$ We can take $\delta$ as very small, so $$= (2\pi)^{-n/2}\int_0^{\infty}e^{-t^2/2}\omega_{n-2}t*\frac{\sqrt{3}\delta}{4(1-\delta/2)}dt \leq (2\pi)^{-n/2}\omega_{n-2}\sqrt{3}\delta$$ EDIT: In the end, we have to multiply with $|\mathbb{S}^{n-1}|$ since the above is only the uniform probability on $\mathbb{S}^{n-1}$ Are these calculations correct? I am a bit confused about the term $(2\pi)^{-n/2}$, then $\delta$ could be quite large and we would still not occupy a large amount of the mass.","I would like to choose arbitrarily two vectors $a$ and $b$ $\in \mathbb{S}^{n-1}$ and I would like to calculate the probability they are at least some distance $\delta$ apart. This probability should be $$\frac{|\mathbb{S}^{n-1}|-|\{ x \in \mathbb{S}^{n-1}: \|x-a\| \leq \delta \}|}{|\mathbb{S}^{n-1}|}$$ So (I think), my problem consists of finding $|\{ x \in \mathbb{S}^{n-1}: \|x-a\| \leq \delta \}|$ ($|\cdot|$ being the surface measure in $\mathbb{R}^n$). I suppose $a$ can be taken as the unit vector $(1, 0, \cdots, 0)$. By symmetry, the points $\{ x \in \mathbb{S}^{n-1}: \|x-a\| = \delta \}$ should be a ""circle"" with center somewhere on the line between $(0, \cdots, 0)$ and $a$.  I need to find that center and the radius. I search for a point on the ""circle"" with only the first two components being non-zero: The center should have coordinates $(1-x, 0, \cdots, 0)$ and the point on the circle has coordinates $(1-x, y, 0, \cdots, 0)$. $x$ and $y$ need to verify $$x^2 + y^2 = \delta$$ and $$(1-x)^2 + y^2 = 1$$ This gives $x = \delta / 2$ and $y = (\sqrt{3}/4) \delta$. Going back to the surface we want to calculate: $$|\{ x \in \mathbb{S}^{n-1}: \|x-a\| \leq \delta \}| = |\{x \in \mathbb{S}^{n-1}\}\cap \{B((1-\delta / 2, 0, \cdots, 0), r = (\sqrt{3}/4) \delta)\}$$ This should just be the measure of the spherical cap of the sphere centered at $(1-\delta / 2, 0, \cdots, 0)$. To estimate this surface area, I wanted to use that surface area is the push-forward measure of a Gaussian measure on $\mathbb{R}^n$ under $x \rightarrow x/\|x\|$ (times the measure of $\mathbb{S}^{n-1}$ since the latter is a probability measure). So to estimate the above measure, we can also calculate the Gaussian measure of a cone (rays going from $0$ to points in $\{x \in \mathbb{S}^{n-1}: \|x-a\| \leq \delta\})$. Parametrizing this cone, I get $(t, \text{""disc of radius"" }t*\frac{\sqrt{3}\delta}{4(1-\delta/2)}) = (t, disc)$ $$(2\pi)^{-n/2}\int_{(t, disc)}e^{-\|x\|^2/2} = (2\pi)^{-n/2}\int_0^{\infty}e^{-t^2/2}\int_{(disc)}e^{(x_2^2 + x_3^2 + \cdots + x_n^2)/2}dxdt$$ The last integral is over a radial function so $$ = (2\pi)^{-n/2}\int_0^{\infty}e^{-t^2/2}\omega_{n-2}\int_0^{t*\frac{\sqrt{3}\delta}{4(1-\delta/2)} }e^{-s^2/2}dsdt$$ We can take $\delta$ as very small, so $$= (2\pi)^{-n/2}\int_0^{\infty}e^{-t^2/2}\omega_{n-2}t*\frac{\sqrt{3}\delta}{4(1-\delta/2)}dt \leq (2\pi)^{-n/2}\omega_{n-2}\sqrt{3}\delta$$ EDIT: In the end, we have to multiply with $|\mathbb{S}^{n-1}|$ since the above is only the uniform probability on $\mathbb{S}^{n-1}$ Are these calculations correct? I am a bit confused about the term $(2\pi)^{-n/2}$, then $\delta$ could be quite large and we would still not occupy a large amount of the mass.",,"['probability', 'geometry', 'spheres', 'gaussian-integral', 'convex-cone']"
21,Characteristic function of a random vector,Characteristic function of a random vector,,"We consider the random vector  $ X\colon \Omega \to \mathbb {R}^n$ defined on the probability space $(\Omega, \mathfrak F, P)$. Let denote by $\Phi_{X}(x) = \mathbb E(e^{i\left<x, X\right>})$ its characteristic function. I would like to show the following equivalence: $X$ is a Gaussian vector if and only if $\Phi_{X}(x)$ is given by $$\Phi_{X}(x)= e^{i\left<m, x\right> -\frac{1}{2}\left<A x, x\right>} \qquad (*),$$ where $m=(\mathbb E(X_1), \dots, \mathbb E(X_n))$ and $A=Cov(X)$. I showed the direct sense (i.e. if $X$ is a Gaussian vector then $\Phi_{X}(x)$ is given as $(*)$). In fact, I have used the following $\Phi_{X}(x)=\Phi_{Z_x}(1) = \exp\{im_{x} - \frac{1}{2}\sigma_{x}^2\} = ...$, where $Z_x=\sum_{j=1}^{n}x_j X_j$ is a random variable in $N(m, \sigma^2)$ .... Now, I need help for the opposite direction. Thank you in advance","We consider the random vector  $ X\colon \Omega \to \mathbb {R}^n$ defined on the probability space $(\Omega, \mathfrak F, P)$. Let denote by $\Phi_{X}(x) = \mathbb E(e^{i\left<x, X\right>})$ its characteristic function. I would like to show the following equivalence: $X$ is a Gaussian vector if and only if $\Phi_{X}(x)$ is given by $$\Phi_{X}(x)= e^{i\left<m, x\right> -\frac{1}{2}\left<A x, x\right>} \qquad (*),$$ where $m=(\mathbb E(X_1), \dots, \mathbb E(X_n))$ and $A=Cov(X)$. I showed the direct sense (i.e. if $X$ is a Gaussian vector then $\Phi_{X}(x)$ is given as $(*)$). In fact, I have used the following $\Phi_{X}(x)=\Phi_{Z_x}(1) = \exp\{im_{x} - \frac{1}{2}\sigma_{x}^2\} = ...$, where $Z_x=\sum_{j=1}^{n}x_j X_j$ is a random variable in $N(m, \sigma^2)$ .... Now, I need help for the opposite direction. Thank you in advance",,"['probability', 'probability-theory', 'stochastic-processes', 'random-variables', 'characteristic-functions']"
22,What statistical test should be used to evaluate the efficiency of some treatment?,What statistical test should be used to evaluate the efficiency of some treatment?,,"We have two group of people (one of $n$ people, the other of $m$ people) that go through the same entertainment experience (think about a day at the amusement park or something like that). One group gets a special treatment over the other group at some moment during the day (a free drink for example). At the end of the day, each participant gives a satisfaction grade (an integer) $X_1, \cdots, X_n, Y_1, \cdots, Y_m$ that ranges from $0$ to $5$. We would like to know if the special treatment affects the overall satisfaction of the experience. I guess that we have to test if the means of the satisfaction of the two groups are equal ? What test should be used ?","We have two group of people (one of $n$ people, the other of $m$ people) that go through the same entertainment experience (think about a day at the amusement park or something like that). One group gets a special treatment over the other group at some moment during the day (a free drink for example). At the end of the day, each participant gives a satisfaction grade (an integer) $X_1, \cdots, X_n, Y_1, \cdots, Y_m$ that ranges from $0$ to $5$. We would like to know if the special treatment affects the overall satisfaction of the experience. I guess that we have to test if the means of the satisfaction of the two groups are equal ? What test should be used ?",,"['probability', 'statistics', 'hypothesis-testing']"
23,Expectation of positive semidefinite random matrix,Expectation of positive semidefinite random matrix,,Let $X$ be a random square symmetric matrix. Assume $X$ is positive semidefinite almost surely. Also assume that the expectation $E[X]$ exists. Does it follow that $E[X]$ is positive semidefinite? All my intuition tells me that this must be true but a rigorous proof eludes me.,Let $X$ be a random square symmetric matrix. Assume $X$ is positive semidefinite almost surely. Also assume that the expectation $E[X]$ exists. Does it follow that $E[X]$ is positive semidefinite? All my intuition tells me that this must be true but a rigorous proof eludes me.,,"['probability', 'random-matrices', 'positive-semidefinite']"
24,"Difficulty in understanding ""Find $E[N]$ , where $N=\min\{n>0: X_n=X_0\}$""","Difficulty in understanding ""Find  , where """,E[N] N=\min\{n>0: X_n=X_0\},"It is Question 30 on page 168 in Ross's book (Introduction to Probability Models-11th edition) Let $X_i, i\ge 0$ be independent and identically distributed random variables with probability mass function $p(j)=P(X_i=j), j=1,\ldots,m, \sum_{j=1}^m P(j) = 1$ Find $E[N]$, where $N=\min\{n>0: X_n = X_0\}$ The same question was here, but I don't have enough credit to comment on. Find $E[N]$, where $N = \min\{n>0: X_n = X_0\}$ Here are my questions: What is the sample space for it? Can anyone give me a simple example? or any references? What does $X_n = X_0$ mean for 2 random variables with the same distribution? Is it possible to resolve it with conditional probability? Are there any applications of such a question? What is the point behind the question? Any help would be greatly appreciated. Update 1: The Solution says: E[N] = $\sum_{j=1}^m$ E[N|Xo = j] * p(j) = $\sum_{j=1}^m (1/p(j)) * p(j) = m$ I am totally confused, so I want to confirm a few things. $N=\min\{n>0: X_n = X_0\}$: Is this the least index of indices of Xi that are equal to X0 (having the same j with X0)?","It is Question 30 on page 168 in Ross's book (Introduction to Probability Models-11th edition) Let $X_i, i\ge 0$ be independent and identically distributed random variables with probability mass function $p(j)=P(X_i=j), j=1,\ldots,m, \sum_{j=1}^m P(j) = 1$ Find $E[N]$, where $N=\min\{n>0: X_n = X_0\}$ The same question was here, but I don't have enough credit to comment on. Find $E[N]$, where $N = \min\{n>0: X_n = X_0\}$ Here are my questions: What is the sample space for it? Can anyone give me a simple example? or any references? What does $X_n = X_0$ mean for 2 random variables with the same distribution? Is it possible to resolve it with conditional probability? Are there any applications of such a question? What is the point behind the question? Any help would be greatly appreciated. Update 1: The Solution says: E[N] = $\sum_{j=1}^m$ E[N|Xo = j] * p(j) = $\sum_{j=1}^m (1/p(j)) * p(j) = m$ I am totally confused, so I want to confirm a few things. $N=\min\{n>0: X_n = X_0\}$: Is this the least index of indices of Xi that are equal to X0 (having the same j with X0)?",,"['probability', 'conditional-expectation']"
25,Two fair coins are tossed until both turn up heads,Two fair coins are tossed until both turn up heads,,"A penny and a dime are tossed together until both turn up heads, after which no more tosses are made. Find the expected number of times the penny comes up heads. What I've tried: Let $X$ and $Y$ be the number of times the penny and dime come up heads, respectively. Then, for $x = 1,2,3,\ldots$ $$P(X = x) = \sum_{y=1}^\infty P(X = x, Y = y) \\ = \sum_{y=1}^x P(X=x,Y=y) + \sum_{y=x+1}^\infty P(X=x, Y=y) \\ = \sum_{y=1}^x \sum_{n=x}^\infty \left( \frac{1}{4} \right) \binom{n-1}{x-1} \left( \frac{1}{2} \right)^{n-1} \binom{n-1}{y-1} \left( \frac{1}{2} \right)^{n-1} + \\ \sum_{y=x+1}^\infty \sum_{n=y}^\infty \left( \frac{1}{4} \right) \binom{n-1}{x-1} \left( \frac{1}{2} \right)^{n-1} \binom{n-1}{y-1} \left( \frac{1}{2} \right)^{n-1} \\ = \sum_{y=1}^x \sum_{n=x}^\infty \left( \frac{1}{4} \right)^n \binom{n-1}{x-1} \binom{n-1}{y-1} + \sum_{y=x+1}^\infty \sum_{n=y}^\infty \left( \frac{1}{4} \right)^n \binom{n-1}{x-1} \binom{n-1}{y-1}$$ Is there a way to simplify the above expression? Or is there an easier approach that I'm missing?","A penny and a dime are tossed together until both turn up heads, after which no more tosses are made. Find the expected number of times the penny comes up heads. What I've tried: Let $X$ and $Y$ be the number of times the penny and dime come up heads, respectively. Then, for $x = 1,2,3,\ldots$ $$P(X = x) = \sum_{y=1}^\infty P(X = x, Y = y) \\ = \sum_{y=1}^x P(X=x,Y=y) + \sum_{y=x+1}^\infty P(X=x, Y=y) \\ = \sum_{y=1}^x \sum_{n=x}^\infty \left( \frac{1}{4} \right) \binom{n-1}{x-1} \left( \frac{1}{2} \right)^{n-1} \binom{n-1}{y-1} \left( \frac{1}{2} \right)^{n-1} + \\ \sum_{y=x+1}^\infty \sum_{n=y}^\infty \left( \frac{1}{4} \right) \binom{n-1}{x-1} \left( \frac{1}{2} \right)^{n-1} \binom{n-1}{y-1} \left( \frac{1}{2} \right)^{n-1} \\ = \sum_{y=1}^x \sum_{n=x}^\infty \left( \frac{1}{4} \right)^n \binom{n-1}{x-1} \binom{n-1}{y-1} + \sum_{y=x+1}^\infty \sum_{n=y}^\infty \left( \frac{1}{4} \right)^n \binom{n-1}{x-1} \binom{n-1}{y-1}$$ Is there a way to simplify the above expression? Or is there an easier approach that I'm missing?",,"['probability', 'probability-distributions']"
26,Proof of the equation of the fundamental matrix in absorbing Markov chains,Proof of the equation of the fundamental matrix in absorbing Markov chains,,"The standard or canonical form of the transition matrix of an absorbing Markov chain is $$P = \begin{bmatrix}I & 0 \\ R & Q \end{bmatrix}$$ and the fundamental matrix is calculated as $$F=(I - Q)^{-1}$$ such that $FR$ spells out the probability of eventually landing on each absorbing states from different transient states. At the same time, $F$ provides the expected number of steps required. What is the proof that $F$ and $FR$ include this information?","The standard or canonical form of the transition matrix of an absorbing Markov chain is $$P = \begin{bmatrix}I & 0 \\ R & Q \end{bmatrix}$$ and the fundamental matrix is calculated as $$F=(I - Q)^{-1}$$ such that $FR$ spells out the probability of eventually landing on each absorbing states from different transient states. At the same time, $F$ provides the expected number of steps required. What is the proof that $F$ and $FR$ include this information?",,"['probability', 'stochastic-processes', 'markov-chains']"
27,Number of three-term arithmetic progressions in [n],Number of three-term arithmetic progressions in [n],,"Three numbers are chosen at random between 1 and $n$ (say $n=500$).What will be the probability of those numbers to be in arithmetic progression? I don't know how to count the number of favorable events. Sample space={(1,2,3),(4,5,6),(18,20,21)........}  favorable events={(2,4,6),(8,12,16),(10,20,30),(50,100,150... and many more)} I can count the sample space but how do i count the favorable events. Some insight could help?","Three numbers are chosen at random between 1 and $n$ (say $n=500$).What will be the probability of those numbers to be in arithmetic progression? I don't know how to count the number of favorable events. Sample space={(1,2,3),(4,5,6),(18,20,21)........}  favorable events={(2,4,6),(8,12,16),(10,20,30),(50,100,150... and many more)} I can count the sample space but how do i count the favorable events. Some insight could help?",,"['probability', 'combinatorics', 'arithmetic-progressions']"
28,Probability of a Gamma r.v. greater than another.,Probability of a Gamma r.v. greater than another.,,"I saw an interesting formula at: https://stats.stackexchange.com/questions/264861/probability-of-gamma-greater-than-exponential but I didn't know how to derive it. The question is: Let $X,Y$ be independent r.v.s and $X\sim\mathrm{Gamma}(k_1,\theta_1)$, $Y\sim\mathrm{Gamma}(k_2,\theta_2)$, with $\mathbb{E}[X]=k_1\theta_1$. How to derive $$P[X>Y]=IB(\frac{\theta_1}{\theta_1+\theta_2};k_2,k_1),$$ where $IB(x;a,b)$ is the regularized incomplete beta function defined as $$IB(x;a,b)=\frac{1}{B(a,b)}\int_0^xt^{a-1}(1-t)^{b-1}dt,$$ and $B(a,b)$ is the beta function.","I saw an interesting formula at: https://stats.stackexchange.com/questions/264861/probability-of-gamma-greater-than-exponential but I didn't know how to derive it. The question is: Let $X,Y$ be independent r.v.s and $X\sim\mathrm{Gamma}(k_1,\theta_1)$, $Y\sim\mathrm{Gamma}(k_2,\theta_2)$, with $\mathbb{E}[X]=k_1\theta_1$. How to derive $$P[X>Y]=IB(\frac{\theta_1}{\theta_1+\theta_2};k_2,k_1),$$ where $IB(x;a,b)$ is the regularized incomplete beta function defined as $$IB(x;a,b)=\frac{1}{B(a,b)}\int_0^xt^{a-1}(1-t)^{b-1}dt,$$ and $B(a,b)$ is the beta function.",,"['probability', 'probability-distributions', 'gamma-distribution']"
29,Probability that a point is closer to a side than a diagonal,Probability that a point is closer to a side than a diagonal,,So I have a rectangle in which a point is randomly chosen. One side is $a$ and the other is $b=a\sqrt3$. I am supposed to find the probability that a point is closer to a side than to the closest diagonal. I have found the probability that the point is closer to $a$ (0.71) and to $b$ (0.24). Now I was wondering how I can put these two probabilities together to form the asked-for probability. Thanks,So I have a rectangle in which a point is randomly chosen. One side is $a$ and the other is $b=a\sqrt3$. I am supposed to find the probability that a point is closer to a side than to the closest diagonal. I have found the probability that the point is closer to $a$ (0.71) and to $b$ (0.24). Now I was wondering how I can put these two probabilities together to form the asked-for probability. Thanks,,"['probability', 'geometry', 'probability-theory']"
30,Probability of truth in a chain of statements,Probability of truth in a chain of statements,,"Three individuals $A$ , $B$ and $C$ tell the truth with probability $1/3$ . (A) $C$ makes a statement and $A$ claims that it is true. What is the probability that the statement is true. (B) $C$ makes a statement and $A$ tells you that $B$ claims the statement is true. What is the probability that the statement is true. Let $T_A$ be the event that $A$ tells the truth and similarly $T_B \ \& \ T_C$ My answer in the first case $\frac{1}{3}\cdot \frac{1}{3} = \frac{1}{9}$ . I am confused with part $B$ of the question. My reasoning: Probability of $C$ telling the truth is $1/3$ . Now we don't know what $B$ said so we assume two cases. Therefore we would have the probability as $P(T_A \ T_B \ T_C \cup T_A \ T_B^C \ T_C^C) = \frac{1}{3}\cdot \frac{1}{3} \cdot \frac{1}{3}+\frac{1}{3}\cdot \frac{2}{3} \cdot \frac{2}{3}$ . Is this correct? This problem appears as an exercise to Bayes theorem so I am confused as to how the theorem is applicable. Thanks in advance for any assistance.","Three individuals , and tell the truth with probability . (A) makes a statement and claims that it is true. What is the probability that the statement is true. (B) makes a statement and tells you that claims the statement is true. What is the probability that the statement is true. Let be the event that tells the truth and similarly My answer in the first case . I am confused with part of the question. My reasoning: Probability of telling the truth is . Now we don't know what said so we assume two cases. Therefore we would have the probability as . Is this correct? This problem appears as an exercise to Bayes theorem so I am confused as to how the theorem is applicable. Thanks in advance for any assistance.",A B C 1/3 C A C A B T_A A T_B \ \& \ T_C \frac{1}{3}\cdot \frac{1}{3} = \frac{1}{9} B C 1/3 B P(T_A \ T_B \ T_C \cup T_A \ T_B^C \ T_C^C) = \frac{1}{3}\cdot \frac{1}{3} \cdot \frac{1}{3}+\frac{1}{3}\cdot \frac{2}{3} \cdot \frac{2}{3},['probability']
31,Probability that at least $2$ people will not receive any ace.,Probability that at least  people will not receive any ace.,2,"I've a deck with 52 french cards ($13$ values for each of $4$ suits) and $4$ players. Randomly dealing out all cards, what's the probability that at least   $2$ people will not receive any ace? My try : $$p=\frac{\frac{4!}{2!}\binom{48}{13,13,12,10}+\binom{4}{2}\binom{48}{13,13,11,11}+\frac{4!}{3!}\binom{48}{13,13,13,9}}{\binom{52}{13,13,13,13}}$$ Where: $\binom{48}{13,13,13,9}$ is the case $A$ has all $4$ aces, $\binom{48}{13,13,12,10}$ the case $A$ has $1$ ace $B$ has $3$ aces, $\binom{48}{13,13,11,11}$ the $A$ has $2$ aces and same for $B$, $\frac{4!}{2!}$ arrangements of $4$ people to be $A$ and $B$, $\binom{4}{2}$ combination of $4$ people to be $A$ and $B$, $\frac{4!}{3!}$ arrangements of $4$ people to be $A$ Am I right? If yes, is there a more elegant solution than mine?","I've a deck with 52 french cards ($13$ values for each of $4$ suits) and $4$ players. Randomly dealing out all cards, what's the probability that at least   $2$ people will not receive any ace? My try : $$p=\frac{\frac{4!}{2!}\binom{48}{13,13,12,10}+\binom{4}{2}\binom{48}{13,13,11,11}+\frac{4!}{3!}\binom{48}{13,13,13,9}}{\binom{52}{13,13,13,13}}$$ Where: $\binom{48}{13,13,13,9}$ is the case $A$ has all $4$ aces, $\binom{48}{13,13,12,10}$ the case $A$ has $1$ ace $B$ has $3$ aces, $\binom{48}{13,13,11,11}$ the $A$ has $2$ aces and same for $B$, $\frac{4!}{2!}$ arrangements of $4$ people to be $A$ and $B$, $\binom{4}{2}$ combination of $4$ people to be $A$ and $B$, $\frac{4!}{3!}$ arrangements of $4$ people to be $A$ Am I right? If yes, is there a more elegant solution than mine?",,"['probability', 'combinatorics', 'combinations']"
32,What is the reason that $p_i$ does not depend on $i$?,What is the reason that  does not depend on ?,p_i i,"Consider the following formula : $$p_i=\sum_{j=1}^n\frac{\binom{i-1}{j-1}\binom{n^2-i}{n-j}}{\binom{n^2}{n}},$$ where $i,n$ are positive integer and $i=1,\ldots,n^2$. Suppose there are $n$ boxes (the boxes are labeled with $1,2,\ldots,n$) each containing $n$ balls. The balls in each box are ordered according to their size. The $i$th $(i=1,2,\ldots,n^2)$ smallest ball among all $n^2$ balls is the $j$th smallest ball  $(j=1,2,\ldots,n)$ of a box if and only if  (1) the first $(i-1)$ values contain exactly $(j-1)$ 1's. This can be done in $\binom{i-1}{j-1}$ ways, $(2)$ the $i$th value is a $1$, (3) And therefore there are exactly $(n-j)$ 1's distributed among the remaining $(n^2-i)$ values. This can be done in $\binom{n^2-i}{n-j}$ ways. The total number of ways in which the $n^2$ balls can be randomly allocated to $n$ boxes is $\binom{n^2}{n}$. Then the probability that the $i$th smallest ball among all $n^2$ balls is the  $j$th smallest ball of a box is $$\frac{\binom{i-1}{j-1}\binom{n^2-i}{n-j}}{\binom{n^2}{n}}.$$ We select a ball if it is the $1$st smallest ball of the $1$st box, or  $2$nd smallest ball of the $2$nd box, thus $n$th smallest ball of the $n$th box. It is not possible that the $i$th smallest ball overall was in two boxes, but it had to be in some box. Therefore the event that ""the $i$th smallest ball was in box $j$"", is an exhaustive partition of the possibilities. Consequently there chances add up. Hence the probability that the $i$th smallest ball is selected is $$p_i=\sum_{j=1}^n\frac{\binom{i-1}{j-1}\binom{n^2-i}{n-j}}{\binom{n^2}{n}}.$$ I expected different value of $p_i$ for different $i$. But I was astonished that the $p_i$ is same for all $i$. It seems $$p_i=\sum_{j=1}^n\frac{\binom{i-1}{j-1}\binom{n^2-i}{n-j}}{\binom{n^2}{n}}=\frac{1}{n}.$$ I checked it by calculating $p_i$ for different $n$ such as $2$ or $3$. What is the  reason that $p_i$ does not depend on $i$? Why this complex formula is nothing but a uniform probability?","Consider the following formula : $$p_i=\sum_{j=1}^n\frac{\binom{i-1}{j-1}\binom{n^2-i}{n-j}}{\binom{n^2}{n}},$$ where $i,n$ are positive integer and $i=1,\ldots,n^2$. Suppose there are $n$ boxes (the boxes are labeled with $1,2,\ldots,n$) each containing $n$ balls. The balls in each box are ordered according to their size. The $i$th $(i=1,2,\ldots,n^2)$ smallest ball among all $n^2$ balls is the $j$th smallest ball  $(j=1,2,\ldots,n)$ of a box if and only if  (1) the first $(i-1)$ values contain exactly $(j-1)$ 1's. This can be done in $\binom{i-1}{j-1}$ ways, $(2)$ the $i$th value is a $1$, (3) And therefore there are exactly $(n-j)$ 1's distributed among the remaining $(n^2-i)$ values. This can be done in $\binom{n^2-i}{n-j}$ ways. The total number of ways in which the $n^2$ balls can be randomly allocated to $n$ boxes is $\binom{n^2}{n}$. Then the probability that the $i$th smallest ball among all $n^2$ balls is the  $j$th smallest ball of a box is $$\frac{\binom{i-1}{j-1}\binom{n^2-i}{n-j}}{\binom{n^2}{n}}.$$ We select a ball if it is the $1$st smallest ball of the $1$st box, or  $2$nd smallest ball of the $2$nd box, thus $n$th smallest ball of the $n$th box. It is not possible that the $i$th smallest ball overall was in two boxes, but it had to be in some box. Therefore the event that ""the $i$th smallest ball was in box $j$"", is an exhaustive partition of the possibilities. Consequently there chances add up. Hence the probability that the $i$th smallest ball is selected is $$p_i=\sum_{j=1}^n\frac{\binom{i-1}{j-1}\binom{n^2-i}{n-j}}{\binom{n^2}{n}}.$$ I expected different value of $p_i$ for different $i$. But I was astonished that the $p_i$ is same for all $i$. It seems $$p_i=\sum_{j=1}^n\frac{\binom{i-1}{j-1}\binom{n^2-i}{n-j}}{\binom{n^2}{n}}=\frac{1}{n}.$$ I checked it by calculating $p_i$ for different $n$ such as $2$ or $3$. What is the  reason that $p_i$ does not depend on $i$? Why this complex formula is nothing but a uniform probability?",,"['probability', 'probability-theory', 'statistics', 'uniform-distribution', 'hypergeometric-function']"
33,Bins and balls problem: expected number of balls placed given number of empty and non-empty bins?,Bins and balls problem: expected number of balls placed given number of empty and non-empty bins?,,"Given are $m$ bins with equal probability of choosing one of them. Unknown number of balls $n$ is placed into the bins, and, at the end of placement, we observe number of empty bins $m_e$ and non-empty bins $m_{n}$. Given $m$, $m_e$, $m_n$, what is the most likely number of balls $n$, which have been placed into bins? (UPD) possible additional information: number of bins with exactly one ball can be also known.","Given are $m$ bins with equal probability of choosing one of them. Unknown number of balls $n$ is placed into the bins, and, at the end of placement, we observe number of empty bins $m_e$ and non-empty bins $m_{n}$. Given $m$, $m_e$, $m_n$, what is the most likely number of balls $n$, which have been placed into bins? (UPD) possible additional information: number of bins with exactly one ball can be also known.",,"['probability', 'combinatorics', 'probability-distributions', 'balls-in-bins']"
34,What are some of the common techniques for density estimation?,What are some of the common techniques for density estimation?,,"I'm trying to estimate the probability density function of a real random variable given its iid realizations. What are some of the standard techniques to do this? Also, is there any reasonable assumptions on the smoothness of the probability density function that's being commonly used in this field? I'd greatly appreciate any of your pointers or references.","I'm trying to estimate the probability density function of a real random variable given its iid realizations. What are some of the standard techniques to do this? Also, is there any reasonable assumptions on the smoothness of the probability density function that's being commonly used in this field? I'd greatly appreciate any of your pointers or references.",,"['probability', 'statistics', 'machine-learning', 'parameter-estimation']"
35,what does $+$ mean in $f(x) = (1 - |x|)_{+}$?,what does  mean in ?,+ f(x) = (1 - |x|)_{+},"I'm doing homework and in one of the problem a density function is given by $f(x) = (1-|x|)_{+}$. I don't know if the $+$ is a mistypo or not. Anyone familiar with this? If it matters, $Y$ is a random variable with density $f(x) = (1-|x|)_{+}$ and I'm supposed to calculate the characteristic function of $Y$","I'm doing homework and in one of the problem a density function is given by $f(x) = (1-|x|)_{+}$. I don't know if the $+$ is a mistypo or not. Anyone familiar with this? If it matters, $Y$ is a random variable with density $f(x) = (1-|x|)_{+}$ and I'm supposed to calculate the characteristic function of $Y$",,"['probability', 'notation']"
36,Probability of original signal,Probability of original signal,,"Signal which can be green or red with probability $4/5$ and $1/5$ respectively, is received by station A and then transmitted to station B. The probability of each station receiving the signal correctly $3/4$. If the Signal received at station B is green, then the probability that the original signal was green is In this I think we have to use Bayes' theorem . but could not able to apply it .","Signal which can be green or red with probability $4/5$ and $1/5$ respectively, is received by station A and then transmitted to station B. The probability of each station receiving the signal correctly $3/4$. If the Signal received at station B is green, then the probability that the original signal was green is In this I think we have to use Bayes' theorem . but could not able to apply it .",,"['probability', 'probability-theory']"
37,Joint distribution of uniform variables,Joint distribution of uniform variables,,"$$X_{1}\sim \mathcal{Uniform}([0,2])$$ $$X_{2}\sim \mathcal{Uniform}([1,2])$$ $X_{1}$ and $X_{2}$ are independent random variables. My question is whether joint distribution of those variables is: $$f(x_{1},x_{2})=f_{1}(x_{1})\,f_2(x_{2})=1/2$$ ... for all $(x_1, x_2) \in\ [0,2]{\times}[1,2]$? If answer is incorrect, then how to approach this issue? If answer is correct, then I am thinking wheter exist more rigorous and formal approach to derive this?","$$X_{1}\sim \mathcal{Uniform}([0,2])$$ $$X_{2}\sim \mathcal{Uniform}([1,2])$$ $X_{1}$ and $X_{2}$ are independent random variables. My question is whether joint distribution of those variables is: $$f(x_{1},x_{2})=f_{1}(x_{1})\,f_2(x_{2})=1/2$$ ... for all $(x_1, x_2) \in\ [0,2]{\times}[1,2]$? If answer is incorrect, then how to approach this issue? If answer is correct, then I am thinking wheter exist more rigorous and formal approach to derive this?",,"['probability', 'probability-distributions', 'uniform-distribution']"
38,"Probability that sum of $3$ real numbers less than $2$, where each of them lies in $\left( 0,2 \right)$ .","Probability that sum of  real numbers less than , where each of them lies in  .","3 2 \left( 0,2 \right)","Edited : I have no problem in my solution at all, so please don't extend it further, all I want is explanation of another solution whose screenshot and URL I have mentioned. Suppose we select three real numbers X,Y and Z $\in \left( 0,2 \right)$. What's the probability that : $X+Y+Z\le 2$. I tried to proceed in this way, Let us assume 3-D coordinate axes $x,y$ and $z$. Now, the region bounded by $0< x <2$, $ 0< y <2$ and $0 < z< 2$  is nothing else but a cube of side length $2$, let it's volume be $V (=8)$.The region bounded by $X+Y+Z\le 2$ is a tetrahedron with $2$ as intercept on each $x,y$ and $z$ axis.Now, volume of this tetrahedron is $\frac { 1 }{ 6 } \begin{bmatrix} 2 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 2 \end{bmatrix}$ i.e. $\frac { V }{ 6 }$. Thus, the probability will be ratio of volume of these two i.e.  $\frac{1}{6}$. But, I found another solution too for this same question asked on this website, whose solution I am unable to understand.Can someone please explain me that solution : Click here for that previously asked question and it's solution. Or, take a look at the following screenshot : Any help will be Appreciated! P.S. - I am a high school student so please avoid use of higher Mathematics which is beyond my scope.","Edited : I have no problem in my solution at all, so please don't extend it further, all I want is explanation of another solution whose screenshot and URL I have mentioned. Suppose we select three real numbers X,Y and Z $\in \left( 0,2 \right)$. What's the probability that : $X+Y+Z\le 2$. I tried to proceed in this way, Let us assume 3-D coordinate axes $x,y$ and $z$. Now, the region bounded by $0< x <2$, $ 0< y <2$ and $0 < z< 2$  is nothing else but a cube of side length $2$, let it's volume be $V (=8)$.The region bounded by $X+Y+Z\le 2$ is a tetrahedron with $2$ as intercept on each $x,y$ and $z$ axis.Now, volume of this tetrahedron is $\frac { 1 }{ 6 } \begin{bmatrix} 2 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 2 \end{bmatrix}$ i.e. $\frac { V }{ 6 }$. Thus, the probability will be ratio of volume of these two i.e.  $\frac{1}{6}$. But, I found another solution too for this same question asked on this website, whose solution I am unable to understand.Can someone please explain me that solution : Click here for that previously asked question and it's solution. Or, take a look at the following screenshot : Any help will be Appreciated! P.S. - I am a high school student so please avoid use of higher Mathematics which is beyond my scope.",,"['probability', 'probability-distributions']"
39,How to show this estimator of variance is biased?,How to show this estimator of variance is biased?,,"It is known that the sample variance is an unbiased estimator: $$s^2 = \frac 1{n-1} \sum_{i=1}^n (X_i - \bar X)^2$$ I would like show that $\sigma '^2 = (X_1 - X_2)^2 $ is a biased estimator. My work: $$E((X_1 - X_2)^2)= E(X_1^2) - 2E(X_1 X_2) + E(X_2^2)$$ I wasn't taught of how to specifically simplify these kinds of expression, but I suspect that $E(X_1^2)=E(X_2^2)$ since it's symmetrical. I don't have any further ideas about how to show that the expected value is not the population variance. Please give me some hints to work on it. Thanks.","It is known that the sample variance is an unbiased estimator: $$s^2 = \frac 1{n-1} \sum_{i=1}^n (X_i - \bar X)^2$$ I would like show that $\sigma '^2 = (X_1 - X_2)^2 $ is a biased estimator. My work: $$E((X_1 - X_2)^2)= E(X_1^2) - 2E(X_1 X_2) + E(X_2^2)$$ I wasn't taught of how to specifically simplify these kinds of expression, but I suspect that $E(X_1^2)=E(X_2^2)$ since it's symmetrical. I don't have any further ideas about how to show that the expected value is not the population variance. Please give me some hints to work on it. Thanks.",,"['probability', 'statistics', 'expectation']"
40,the way of thinking of frequentist vs bayesian? [duplicate],the way of thinking of frequentist vs bayesian? [duplicate],,"This question already has answers here : Describing Bayesian Probability (3 answers) Closed 8 months ago . I'm learning about bayesian inference and I've heard that there's also another inference called frequentist inference. I still can't understand the difference the way frequentist and bayesian count the probability, can someone give me a simple example of a problem, and how frequentist and bayesian solve the problem?","This question already has answers here : Describing Bayesian Probability (3 answers) Closed 8 months ago . I'm learning about bayesian inference and I've heard that there's also another inference called frequentist inference. I still can't understand the difference the way frequentist and bayesian count the probability, can someone give me a simple example of a problem, and how frequentist and bayesian solve the problem?",,"['probability', 'statistics']"
41,Expectation value of trials needed to get $k$ consecutive outcomes,Expectation value of trials needed to get  consecutive outcomes,k,"Suppose that independent trials, each of which is equally likely to have any of $m$ possible outcome, are performed until the same outcome occurs $k$ consecutive times. If $N$ denotes the number of trials, show that $$E[N] = \frac{m^k-1}{m-1}$$ This is a homework question. I was trying to reverse engineer this into a GP but without success. I also tried an induction approach to $k$. For getting one outcome, the number of trials needed is always $1$, so $k=1$ is a trivial case. Now assume that the expectation value of trials needed for $k$ consecutive outcomes is $E[N]$, then how do I find the update to $E[N]$ for $k+1$. I am very confused on the approach. Looks like a one-liner would do. Please help.","Suppose that independent trials, each of which is equally likely to have any of $m$ possible outcome, are performed until the same outcome occurs $k$ consecutive times. If $N$ denotes the number of trials, show that $$E[N] = \frac{m^k-1}{m-1}$$ This is a homework question. I was trying to reverse engineer this into a GP but without success. I also tried an induction approach to $k$. For getting one outcome, the number of trials needed is always $1$, so $k=1$ is a trivial case. Now assume that the expectation value of trials needed for $k$ consecutive outcomes is $E[N]$, then how do I find the update to $E[N]$ for $k+1$. I am very confused on the approach. Looks like a one-liner would do. Please help.",,"['probability', 'probability-theory']"
42,How does a pity timer affects probabilities?,How does a pity timer affects probabilities?,,"In a game called hearthstone you buy packs to collect cards. They have four types of cards. Each type of cards has assigned a probability. So you open a pack and a random number is generated. The most valuable cards (called legendary) as you may expect are the least frequent. The owners of the game didnt want people to quit the game because of bad luck so they added a pity timer. If you didnt open a legendary in the previous 39 packs, when you open the number 40 you will always get a legendary card and the timer is set to zero. If you opened a legendary before the 40 pack the timer is also set to zero. The problem can be formulated as follows. If an event has probability p to happen and you ask for a condition that is if n-1 times in a row the event didnt happened, the event will always happen on the trial number n. What is the new probability of the event happening due to this condition? Thanks for reading!","In a game called hearthstone you buy packs to collect cards. They have four types of cards. Each type of cards has assigned a probability. So you open a pack and a random number is generated. The most valuable cards (called legendary) as you may expect are the least frequent. The owners of the game didnt want people to quit the game because of bad luck so they added a pity timer. If you didnt open a legendary in the previous 39 packs, when you open the number 40 you will always get a legendary card and the timer is set to zero. If you opened a legendary before the 40 pack the timer is also set to zero. The problem can be formulated as follows. If an event has probability p to happen and you ask for a condition that is if n-1 times in a row the event didnt happened, the event will always happen on the trial number n. What is the new probability of the event happening due to this condition? Thanks for reading!",,['probability']
43,How much area in a unit square is not covered by $k$ disks of area $1/k$ centered at random points within the square?,How much area in a unit square is not covered by  disks of area  centered at random points within the square?,k 1/k,"1 . Paint a $1\times 1$ square in blue. 2 . Take $k$ points randomly and uniformly from the square. 3 . Paint $k$ disks of area $1/k$ centered at each point in red. What is the expected remaining blue area? Remark: the sum of the areas of the disks is $k\cdot 1/k=1$ , the same as the area of the square, but we have to account for: Disks intersecting each other. Disks intersecting the outside of the square.","1 . Paint a square in blue. 2 . Take points randomly and uniformly from the square. 3 . Paint disks of area centered at each point in red. What is the expected remaining blue area? Remark: the sum of the areas of the disks is , the same as the area of the square, but we have to account for: Disks intersecting each other. Disks intersecting the outside of the square.",1\times 1 k k 1/k k\cdot 1/k=1,"['probability', 'expectation']"
44,Does Change of Numeraire same as Change of Measure?,Does Change of Numeraire same as Change of Measure?,,"Does Change of Numeraire same as Change of Measure? It is a bit confusing since both looks same. Do they have same meaning, or just mathematically alike.","Does Change of Numeraire same as Change of Measure? It is a bit confusing since both looks same. Do they have same meaning, or just mathematically alike.",,"['probability', 'stochastic-processes', 'finance']"
45,Variance of couples seated across from each other at a rectangular table.,Variance of couples seated across from each other at a rectangular table.,,"Let N couples be randomly seated at a rectangular table, men on one side and women on the other. Let X be the random variable describing the number of couples that end up being seated across from each other. What is the variance of X? I uncovered this problem in my book and have found the following: The seating arrangements are not independent. the $E(X)$ is 1 if you use indicator variables, ie. if $I_k$ is the indicator variable that is 1 if a couple k is seated across from one another and 0 otherwise. so $X=\sum_{k=1}^n I_k$ and the $E(X)$ is worked out to be 1. I believe I am correct so far, however, I am stuck here.  Since the each $E_k$ is not entirely independent, I tried using  $Var(X)= E(X^2) - (E(X))^2 = E(X^2) - 1$ So I need $E(X^2)$, which is a double sum. I do not know if this is solvable or if there is some term missing. Any direction would be appreciated. Edit: Okay, so I also know that $ p(I_k)=1/n $ and by the same rational $p(I_k*I_j)= 1/(n^2-n)$. So I am assuming I am missing some term because each k is not independent. I will try the other definition of variance, but I am almost sure (frustratingly so) I am missing something.","Let N couples be randomly seated at a rectangular table, men on one side and women on the other. Let X be the random variable describing the number of couples that end up being seated across from each other. What is the variance of X? I uncovered this problem in my book and have found the following: The seating arrangements are not independent. the $E(X)$ is 1 if you use indicator variables, ie. if $I_k$ is the indicator variable that is 1 if a couple k is seated across from one another and 0 otherwise. so $X=\sum_{k=1}^n I_k$ and the $E(X)$ is worked out to be 1. I believe I am correct so far, however, I am stuck here.  Since the each $E_k$ is not entirely independent, I tried using  $Var(X)= E(X^2) - (E(X))^2 = E(X^2) - 1$ So I need $E(X^2)$, which is a double sum. I do not know if this is solvable or if there is some term missing. Any direction would be appreciated. Edit: Okay, so I also know that $ p(I_k)=1/n $ and by the same rational $p(I_k*I_j)= 1/(n^2-n)$. So I am assuming I am missing some term because each k is not independent. I will try the other definition of variance, but I am almost sure (frustratingly so) I am missing something.",,"['probability', 'probability-theory', 'variance']"
46,Mean of $ \sum (X_i - \bar{X})^2$,Mean of, \sum (X_i - \bar{X})^2,"If $X_1,...,X_n$ are iid, what is the mean of the following variable:? $ \sum (X_i - \bar{X})^2$ I know the answer is $\sigma^2(n-1)$, but how is this calculated in general? If I expand, I get the variables squared, and that turns quite ugly. What's the trick here?","If $X_1,...,X_n$ are iid, what is the mean of the following variable:? $ \sum (X_i - \bar{X})^2$ I know the answer is $\sigma^2(n-1)$, but how is this calculated in general? If I expand, I get the variables squared, and that turns quite ugly. What's the trick here?",,"['probability', 'expected-value']"
47,Expected value of a coin toss,Expected value of a coin toss,,You flip a coin. If you get heads you win \$2 if you get tails you lose \$1. What is the expected value if you flip the coin 1000 times? I know that the expected value of flipping the coin once is $\frac{1}{2}(2) - \frac{1}{2}(1) =0.50$ Would the expected value be 500?,You flip a coin. If you get heads you win \$2 if you get tails you lose \$1. What is the expected value if you flip the coin 1000 times? I know that the expected value of flipping the coin once is $\frac{1}{2}(2) - \frac{1}{2}(1) =0.50$ Would the expected value be 500?,,['probability']
48,Probability of second card being an ace,Probability of second card being an ace,,"I have this task about cards: Consider choosing a card from a well-shuffled standard deck of 52   playing cards. Suppose that, after the first extraction, the card   is not reinserted in the deck. What is the probability that the second   card is an ace? Suppose that, after the first extraction, the card   is reinserted in the deck. What is the probability that the second   card is an ace? For the first case, I've been thinking about $$\left(\frac{4*3}{52*51}\right) + \left(\frac{48*4}{52*51}\right)$$ Where I counted two examples: a)the ace was drawn already, and b) it wasn't. For the second part of the task, would it be $$\left(\frac{4*4}{52*52}\right) + \left(\frac{52*4}{52*52}\right)$$ I'm pretty sure that I'm assuming wrong, but I can't come up with anything else. Any help would be great!","I have this task about cards: Consider choosing a card from a well-shuffled standard deck of 52   playing cards. Suppose that, after the first extraction, the card   is not reinserted in the deck. What is the probability that the second   card is an ace? Suppose that, after the first extraction, the card   is reinserted in the deck. What is the probability that the second   card is an ace? For the first case, I've been thinking about $$\left(\frac{4*3}{52*51}\right) + \left(\frac{48*4}{52*51}\right)$$ Where I counted two examples: a)the ace was drawn already, and b) it wasn't. For the second part of the task, would it be $$\left(\frac{4*4}{52*52}\right) + \left(\frac{52*4}{52*52}\right)$$ I'm pretty sure that I'm assuming wrong, but I can't come up with anything else. Any help would be great!",,"['probability', 'card-games']"
49,Two points are selected on a straight line of length 'a' units at random,Two points are selected on a straight line of length 'a' units at random,,"If two points are selected on a straight line of length 'a' units at random, then what is the probability that none of the three line segments formed by the two random points has length less than a/4.","If two points are selected on a straight line of length 'a' units at random, then what is the probability that none of the three line segments formed by the two random points has length less than a/4.",,"['probability', 'probability-distributions']"
50,Find the probability of $P_1$ winning the championship,Find the probability of  winning the championship,P_1,"Two players $P_1$ and $P_2$ are playing the final of a chess championship,which consists of a series of matches.Probability of $P_1$ winning a match is $\frac{2}{3}$ and that of $P_2$ is $\frac{1}{3}$.The winner will be the one who is ahead by two games as compared to the other player and wins atleast $6$ games.Now if the player $P_2$ wins first 4 matches,prove that the probability of $P_1$ winning the championship is $\frac{1088}{3645}$ I dont have thoughts as to how to tackle this problem.Because a player has to just ahead of 2 games as compared to another player,neither more than 2 nor less than 2.Please help me.","Two players $P_1$ and $P_2$ are playing the final of a chess championship,which consists of a series of matches.Probability of $P_1$ winning a match is $\frac{2}{3}$ and that of $P_2$ is $\frac{1}{3}$.The winner will be the one who is ahead by two games as compared to the other player and wins atleast $6$ games.Now if the player $P_2$ wins first 4 matches,prove that the probability of $P_1$ winning the championship is $\frac{1088}{3645}$ I dont have thoughts as to how to tackle this problem.Because a player has to just ahead of 2 games as compared to another player,neither more than 2 nor less than 2.Please help me.",,['probability']
51,$4$ integers are randomly selected from the numbers from $1$ to $10$. The chance that there are at least two successive numbers among those $4$ is,integers are randomly selected from the numbers from  to . The chance that there are at least two successive numbers among those  is,4 1 10 4,$4$ integers are randomly selected from the numbers from $1$ to $10$.  The chance that there are atleast two successive numbers among those $4$ selected is $(A)\frac{5}{6}\hspace{1cm}(B)\frac{3}{4}\hspace{1cm}(C)\frac{2}{3}\hspace{1cm}(D)\frac{1}{2}\hspace{1cm}$ I calculated answer as $\frac{24}{\binom{10}{4}}$ but this wrong.  Please help me find the right answer.,$4$ integers are randomly selected from the numbers from $1$ to $10$.  The chance that there are atleast two successive numbers among those $4$ selected is $(A)\frac{5}{6}\hspace{1cm}(B)\frac{3}{4}\hspace{1cm}(C)\frac{2}{3}\hspace{1cm}(D)\frac{1}{2}\hspace{1cm}$ I calculated answer as $\frac{24}{\binom{10}{4}}$ but this wrong.  Please help me find the right answer.,,"['probability', 'combinatorics']"
52,Is this a Conditional Probability?,Is this a Conditional Probability?,,I am very confused. Is the highlighted $ P(\text{Graduated} \cap \text{Studied})$ or $ P(\text{Graduated} \mid \text{Studied})$,I am very confused. Is the highlighted $ P(\text{Graduated} \cap \text{Studied})$ or $ P(\text{Graduated} \mid \text{Studied})$,,"['probability', 'statistics']"
53,Showing that infinite product of random variables goes to zero: $\prod^\infty X_i \rightarrow 0 \text{ a.s.}$,Showing that infinite product of random variables goes to zero:,\prod^\infty X_i \rightarrow 0 \text{ a.s.},"I am doing the following exercise: Let $X$ be a strictly positive rv with $\mathbb E[X]=1$ but $X \neq 1$ almost surely. Let $X_1, X_2 \dots$ be iid with same distribution as $X$. Now let $M_0=1$ and  $$M_n = \prod_{k=1}^{\infty} X_k$$ The task is to show that $M_n \rightarrow 0$ almost surely. Some sub-problems preceding showing this convergence was to show that $\mathbb E[\ln(X)]<0$ and that $\mathbb E[M_n]=1$. That is straightforward. Then one is asked to show the said convergence by considering  $$ M_n = e^{\sum^n \ln X_k} $$ So I have tried to reason that in order for $e^{\sum^n \ln X_k} \rightarrow 0$ almost surely, we need $\sum^n \ln X_k \rightarrow -\infty$ almost surely. Here I have some doubts as to how to proceed. I have tried to think in terms of Borel-Cantelli and show this by making an argument along the lines of: for any $N \in (-\infty, 0]$  $$ \mathbb P(M_n < N \text{ eventually}) = 1 $$ etc but it seems that this ends up requiring me to know things about the distribution of $M_n$ which I know nothing about (except its mean). Any help highly appreciated.","I am doing the following exercise: Let $X$ be a strictly positive rv with $\mathbb E[X]=1$ but $X \neq 1$ almost surely. Let $X_1, X_2 \dots$ be iid with same distribution as $X$. Now let $M_0=1$ and  $$M_n = \prod_{k=1}^{\infty} X_k$$ The task is to show that $M_n \rightarrow 0$ almost surely. Some sub-problems preceding showing this convergence was to show that $\mathbb E[\ln(X)]<0$ and that $\mathbb E[M_n]=1$. That is straightforward. Then one is asked to show the said convergence by considering  $$ M_n = e^{\sum^n \ln X_k} $$ So I have tried to reason that in order for $e^{\sum^n \ln X_k} \rightarrow 0$ almost surely, we need $\sum^n \ln X_k \rightarrow -\infty$ almost surely. Here I have some doubts as to how to proceed. I have tried to think in terms of Borel-Cantelli and show this by making an argument along the lines of: for any $N \in (-\infty, 0]$  $$ \mathbb P(M_n < N \text{ eventually}) = 1 $$ etc but it seems that this ends up requiring me to know things about the distribution of $M_n$ which I know nothing about (except its mean). Any help highly appreciated.",,['probability']
54,"Find an example where the random variables $X_1,X_2,X_3$ are pairwise independent, but not all together.","Find an example where the random variables  are pairwise independent, but not all together.","X_1,X_2,X_3","Find an example where the random variables $X_1,X_2,X_3$ are pairwise independent, but not all together.  I can't really understand how I am to do so. How is it done? There cannot be any multiplication. Is it a sum? I searched for it but didn't understand how it is interpreted mathematically. I could really use your help.","Find an example where the random variables $X_1,X_2,X_3$ are pairwise independent, but not all together.  I can't really understand how I am to do so. How is it done? There cannot be any multiplication. Is it a sum? I searched for it but didn't understand how it is interpreted mathematically. I could really use your help.",,['probability']
55,Four 6-sided dice are rolled. What is the probability that at least two dice show the same number?,Four 6-sided dice are rolled. What is the probability that at least two dice show the same number?,,"Am I doing this right? I split the problem up into the cases of 2 same, 3 same, 4 same, but I feel like something special has to be done for 2 of the same, because what if there are 2 pairs (like two 3's and two 4's)? This is what I have: For 2 of the same: $5\times 5\times 6\times {4\choose 2}=900$ For 3 of the same: $5\times 6\times {4\choose 3}=120$ For 4 of the same: $6\times {4\choose 4}=6$ Combined: $900+120+6=1026$ Total possibilities: $6^4=1296$ Probability of at least 2 die the same: $\frac {1026}{1296}\approx 79.17$% Confirmation that I'm right, or pointing out where I went wrong would be appreciated. Thanks! Sorry if the formatting could use work, still getting the hang of it.","Am I doing this right? I split the problem up into the cases of 2 same, 3 same, 4 same, but I feel like something special has to be done for 2 of the same, because what if there are 2 pairs (like two 3's and two 4's)? This is what I have: For 2 of the same: $5\times 5\times 6\times {4\choose 2}=900$ For 3 of the same: $5\times 6\times {4\choose 3}=120$ For 4 of the same: $6\times {4\choose 4}=6$ Combined: $900+120+6=1026$ Total possibilities: $6^4=1296$ Probability of at least 2 die the same: $\frac {1026}{1296}\approx 79.17$% Confirmation that I'm right, or pointing out where I went wrong would be appreciated. Thanks! Sorry if the formatting could use work, still getting the hang of it.",,"['probability', 'combinatorics', 'discrete-mathematics']"
56,What is wrong with ${13 \choose 1}{4 \choose 2} \cdot {12 \choose 1}{4 \choose 2}$ as combinations for two pair in poker?,What is wrong with  as combinations for two pair in poker?,{13 \choose 1}{4 \choose 2} \cdot {12 \choose 1}{4 \choose 2},Let's consider two pairs in a 52 cards deck of poker where every person gets five cards. My idea to approach this problem is to take following steps: First pair There are ${4 \choose 2}$ combinations getting two cards of the same rank There are ${13 \choose 1}$ combinations of having a specific rank out of a suit Second pair There are still ${4 \choose 2}$ combinations to get two cards of the same rank However as one card per suit is gone we only have ${12 \choose 1}$ for each combination out of a suit Any card There are ${4 \choose 1}$ combinations getting one card out of the same rank There are ${11 \choose 1}$ combinations to getting one card out of a suit This would yield in: $$P(TP) = \frac{{4 \choose 2}{13 \choose 1} \cdot {4 \choose 2}{12 \choose 1} \cdot {4 \choose 1}{11 \choose 1}}{{52 \choose 5}}$$ According to wikipedia the correct probability would be calculated as: $$P(TP) = \frac{{13 \choose 2}{4 \choose 2}{4 \choose 2} \cdot {4 \choose 1}{11 \choose 1}}{{52 \choose 5}}$$ What is the mistake in my model and how could I think of the one provided in wikipedia?,Let's consider two pairs in a 52 cards deck of poker where every person gets five cards. My idea to approach this problem is to take following steps: First pair There are ${4 \choose 2}$ combinations getting two cards of the same rank There are ${13 \choose 1}$ combinations of having a specific rank out of a suit Second pair There are still ${4 \choose 2}$ combinations to get two cards of the same rank However as one card per suit is gone we only have ${12 \choose 1}$ for each combination out of a suit Any card There are ${4 \choose 1}$ combinations getting one card out of the same rank There are ${11 \choose 1}$ combinations to getting one card out of a suit This would yield in: $$P(TP) = \frac{{4 \choose 2}{13 \choose 1} \cdot {4 \choose 2}{12 \choose 1} \cdot {4 \choose 1}{11 \choose 1}}{{52 \choose 5}}$$ According to wikipedia the correct probability would be calculated as: $$P(TP) = \frac{{13 \choose 2}{4 \choose 2}{4 \choose 2} \cdot {4 \choose 1}{11 \choose 1}}{{52 \choose 5}}$$ What is the mistake in my model and how could I think of the one provided in wikipedia?,,"['probability', 'binomial-coefficients', 'combinations']"
57,"In a tennis tournement of 128 players, elimination system, what is the probability that two twins will meet at some point?","In a tennis tournement of 128 players, elimination system, what is the probability that two twins will meet at some point?",,"Needs to be solved using conditional probability. I was thinking using the formula for total probability and having the hypothesis $H_i$ - they get to the $i$-th round and ($A$-they meet) $$P(A)=\sum P(A|H_i)P(H_i)$$ I reach a dilemma when trying to find these values, unsure whether to take into account which part of the tournament tree do they meet..  each player has an equal chance of winning.","Needs to be solved using conditional probability. I was thinking using the formula for total probability and having the hypothesis $H_i$ - they get to the $i$-th round and ($A$-they meet) $$P(A)=\sum P(A|H_i)P(H_i)$$ I reach a dilemma when trying to find these values, unsure whether to take into account which part of the tournament tree do they meet..  each player has an equal chance of winning.",,"['probability', 'probability-theory']"
58,Probability of $(a+b\omega+c\omega^{2})(a+b\omega^{2}+c\omega)=1$,Probability of,(a+b\omega+c\omega^{2})(a+b\omega^{2}+c\omega)=1,"A fair die is thrown three times. If $a$, $b$, $c$ are the numbers obtained on the die, then what is the probability that $$(a+b\omega+c\omega^{2})(a+b\omega^{2}+c\omega)=1$$   (where $\omega$ is a cube root of unity) My attempt: On simplification, $a^{2}+b^{2}+c^{2}=1+(ab+bc+ca)$. I can't figure how to find the number of cases where the condition is satisfied.","A fair die is thrown three times. If $a$, $b$, $c$ are the numbers obtained on the die, then what is the probability that $$(a+b\omega+c\omega^{2})(a+b\omega^{2}+c\omega)=1$$   (where $\omega$ is a cube root of unity) My attempt: On simplification, $a^{2}+b^{2}+c^{2}=1+(ab+bc+ca)$. I can't figure how to find the number of cases where the condition is satisfied.",,"['probability', 'complex-numbers']"
59,"calculating probability, people in a row","calculating probability, people in a row",,"There are total of n people, among whom A and B, stand in a row. Q1: P(exactly r people between A and B)=? Q2: Instead of in a row, they stand in a ring.      P(exactly r people between A and B)=? For Q1, I am only this far, which is not too far..(LOL): $\binom{n-2}{r}\times r!$ for the combination for people between A and B. (......A,r people,B...).But I don't know how to handle the combination for the two sides, to the left of A and to the right of B. For Q2, honestly I cannot tell how the combination could be different but I know they should be different. Thanks for your help!!","There are total of n people, among whom A and B, stand in a row. Q1: P(exactly r people between A and B)=? Q2: Instead of in a row, they stand in a ring.      P(exactly r people between A and B)=? For Q1, I am only this far, which is not too far..(LOL): $\binom{n-2}{r}\times r!$ for the combination for people between A and B. (......A,r people,B...).But I don't know how to handle the combination for the two sides, to the left of A and to the right of B. For Q2, honestly I cannot tell how the combination could be different but I know they should be different. Thanks for your help!!",,"['probability', 'combinatorics']"
60,Probability of having only 2 suits represented in a hand of 5 cards.,Probability of having only 2 suits represented in a hand of 5 cards.,,"I know the denominator will be (52 c 5) to represent the total number of 5 card hands with 52 cards. I'm having trouble with the numerator. I began with (4 c 2) for the total number of combinations of 2 suits of the 4. Then I multiplied this by 16 (the number of ways to arrange 2 suits in 5 slots, which would be 2 x 2 x 2 x 2 x 1). Am I on the right track so far? I know I'm going to have to multiply this by the probabilities of drawing those suits, like 13/52 and such, but I'm not sure how to do this correctly.","I know the denominator will be (52 c 5) to represent the total number of 5 card hands with 52 cards. I'm having trouble with the numerator. I began with (4 c 2) for the total number of combinations of 2 suits of the 4. Then I multiplied this by 16 (the number of ways to arrange 2 suits in 5 slots, which would be 2 x 2 x 2 x 2 x 1). Am I on the right track so far? I know I'm going to have to multiply this by the probabilities of drawing those suits, like 13/52 and such, but I'm not sure how to do this correctly.",,['probability']
61,Generalization of Bayes' Theorem,Generalization of Bayes' Theorem,,"Does anyone know of a generalization of Bayes' theorem to multiple conditions? From this answer I can see the definition of conditional probability with multiple conditions, but I couldn't find any reference to a  statement of Bayes' theorem with several conditions. I only need a generalization to 2 conditions for what I'm working on, but I'm also curious to its statement in $n$ conditions. Perhaps if we had the multiplication rule, too, we could derive Bayes' rule from the above?","Does anyone know of a generalization of Bayes' theorem to multiple conditions? From this answer I can see the definition of conditional probability with multiple conditions, but I couldn't find any reference to a  statement of Bayes' theorem with several conditions. I only need a generalization to 2 conditions for what I'm working on, but I'm also curious to its statement in conditions. Perhaps if we had the multiplication rule, too, we could derive Bayes' rule from the above?",n,"['probability', 'bayes-theorem']"
62,Probability a product of $n$ randomly chosen numbers from 1-9 is divisible by 10.,Probability a product of  randomly chosen numbers from 1-9 is divisible by 10.,n,"I'm working on a problem where each number is chosen randomly from 1-9. Given $n$ numbers chosen in this manner, we multiply all of these together. I'm looking for the probability that this product is divisible by 10. I reasoned that we only get a multiple of 10 when we multiply 5 and an even number together. So if either all $n$ numbers are odd or if none of the $n$ numbers is 5, then the product will not be divisible by 10, and we can take 1 - P(Not Divisible by 10). From here, I'm not sure how to come up with a numerical answer when there is an ambiguous number of $n$ randomly chosen numbers. Is there some way to get there? Or is this reasoning flawed?","I'm working on a problem where each number is chosen randomly from 1-9. Given $n$ numbers chosen in this manner, we multiply all of these together. I'm looking for the probability that this product is divisible by 10. I reasoned that we only get a multiple of 10 when we multiply 5 and an even number together. So if either all $n$ numbers are odd or if none of the $n$ numbers is 5, then the product will not be divisible by 10, and we can take 1 - P(Not Divisible by 10). From here, I'm not sure how to come up with a numerical answer when there is an ambiguous number of $n$ randomly chosen numbers. Is there some way to get there? Or is this reasoning flawed?",,"['probability', 'divisibility', 'random']"
63,Can the sum of two independent identical random variables be uniform? [closed],Can the sum of two independent identical random variables be uniform? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question $X$ and $Y$ are two independent and identically distributed random variables. Can $X+Y$ be uniformly distributed over interval $[0,1]$?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question $X$ and $Y$ are two independent and identically distributed random variables. Can $X+Y$ be uniformly distributed over interval $[0,1]$?",,"['probability', 'probability-theory', 'probability-distributions']"
64,"Optimal strategy for picking cards: win a dollar for red, lose one for black, and stop at any time","Optimal strategy for picking cards: win a dollar for red, lose one for black, and stop at any time",,"Suppose I have four cards: two black, two red. I draw them one by one. Every time I draw a red card, I win a dollar, and every time I draw a black card, I lose a dollar. I can choose to stop at any time I want. What is the optimal strategy for choosing when to stop, and what is its expected value? I did this by brute force to get $2/3$, but I don't know if there is a more clever way out there.","Suppose I have four cards: two black, two red. I draw them one by one. Every time I draw a red card, I win a dollar, and every time I draw a black card, I lose a dollar. I can choose to stop at any time I want. What is the optimal strategy for choosing when to stop, and what is its expected value? I did this by brute force to get $2/3$, but I don't know if there is a more clever way out there.",,['probability']
65,Coupon Collector Problem Extension,Coupon Collector Problem Extension,,"Humans have two copies of each of $23$ chromosome, for a total of $46$ chromosomes. If you want to sequence someone's DNA, you can just use a normal cell, since they have all the chromosomes. But if you have a sample of gamete cells, each one has only one copy of each chromosome. So, how would you find the expected number of gamete cells you'd have to sequence to get the person's entire genome? If humans only had $1$ chromosome, then this would be a simple coupon collector problem, which asks the expected number of times you'd choose randomly between two coupons until you get them both. In this case, there are $23$ pairs of coupons, and you randomly select one from each pair, and you want to know how many times you expect to do this until you get all $46$ coupons.","Humans have two copies of each of $23$ chromosome, for a total of $46$ chromosomes. If you want to sequence someone's DNA, you can just use a normal cell, since they have all the chromosomes. But if you have a sample of gamete cells, each one has only one copy of each chromosome. So, how would you find the expected number of gamete cells you'd have to sequence to get the person's entire genome? If humans only had $1$ chromosome, then this would be a simple coupon collector problem, which asks the expected number of times you'd choose randomly between two coupons until you get them both. In this case, there are $23$ pairs of coupons, and you randomly select one from each pair, and you want to know how many times you expect to do this until you get all $46$ coupons.",,['probability']
66,formula for infinite sum of a geometric series with increasing term,formula for infinite sum of a geometric series with increasing term,,"I'm looking for the Expectation of the discrete random variable X, E[X], with pmf: $$p(x)=(\frac 16)^{x+1}, x=0,1,2,3...$$ so what I tried is as follows... $$E[X]= \sum_{0}^\infty xp(x) =$$ so then $$=\sum_0^\infty x(\frac 16)(\frac 16)^x =     \frac 16\sum_{0}^\infty x(\frac 16)^x $$ which is $$ \frac 16 [0(\frac 16)^0+1(\frac 16)^1+2(\frac16)^2+3(\frac 16)^3+...] $$ so if we were to use the rule that: $$ \sum_0^\infty ar^x = \frac {a}{(1-r)} $$ when |r|<1. Then it seems like the difference between that formula and my problem is the increasing coefficient on the (1/6)^x... My math book (which doesn't really say anything more about it)... states that ""there is a general increasing geometric series relation which is $$1 + 2r + 3r^2 + 4r^3+...= \frac {1}{(1-r)^2} $$ is that what I need to know? and if so, can someone please show me why? Thanks!","I'm looking for the Expectation of the discrete random variable X, E[X], with pmf: $$p(x)=(\frac 16)^{x+1}, x=0,1,2,3...$$ so what I tried is as follows... $$E[X]= \sum_{0}^\infty xp(x) =$$ so then $$=\sum_0^\infty x(\frac 16)(\frac 16)^x =     \frac 16\sum_{0}^\infty x(\frac 16)^x $$ which is $$ \frac 16 [0(\frac 16)^0+1(\frac 16)^1+2(\frac16)^2+3(\frac 16)^3+...] $$ so if we were to use the rule that: $$ \sum_0^\infty ar^x = \frac {a}{(1-r)} $$ when |r|<1. Then it seems like the difference between that formula and my problem is the increasing coefficient on the (1/6)^x... My math book (which doesn't really say anything more about it)... states that ""there is a general increasing geometric series relation which is $$1 + 2r + 3r^2 + 4r^3+...= \frac {1}{(1-r)^2} $$ is that what I need to know? and if so, can someone please show me why? Thanks!",,"['calculus', 'probability', 'sequences-and-series']"
67,Variance of number of tails in a coin-toss experiment,Variance of number of tails in a coin-toss experiment,,"Let X be the random variable that equals the number of tails minus the number of heads when n fair coins are flipped. What is the variance of X? I've run a simulation and the answer seems to be n, but I can't get at it myself.  $V(X) = E(X^2) - E(X)^2$ and E(X) is zero, so $V(X) = E(X^2)$. Now $X = (2X_t - n)$ where $X_t$ is number of tails. $X^2 = 4X_t^2 + n^2 - 2X_tn$. Taking E() on both sides: $E(X^2) = 4E(X_t^2) + n^2 - 2nE(X_t)$ we know that $E(X_t)=n/2$ so, $E(X^2) = 4E(X_t^2) - n^2$ I tried to get $E(X_t^2)$ from a simulation and got the right answer again. So the question boils down to: How to find the expectation of $E(X_t^2)$ where $X_t$ is the number of tails in n coin tosses.","Let X be the random variable that equals the number of tails minus the number of heads when n fair coins are flipped. What is the variance of X? I've run a simulation and the answer seems to be n, but I can't get at it myself.  $V(X) = E(X^2) - E(X)^2$ and E(X) is zero, so $V(X) = E(X^2)$. Now $X = (2X_t - n)$ where $X_t$ is number of tails. $X^2 = 4X_t^2 + n^2 - 2X_tn$. Taking E() on both sides: $E(X^2) = 4E(X_t^2) + n^2 - 2nE(X_t)$ we know that $E(X_t)=n/2$ so, $E(X^2) = 4E(X_t^2) - n^2$ I tried to get $E(X_t^2)$ from a simulation and got the right answer again. So the question boils down to: How to find the expectation of $E(X_t^2)$ where $X_t$ is the number of tails in n coin tosses.",,"['probability', 'expectation']"
68,"Bayes' formula application to the probability of a horse winning, depending on the jockey","Bayes' formula application to the probability of a horse winning, depending on the jockey",,"I have a problem that I have been racking my brain to figure this out and I just don't have the background to know if I am correct or not so I am hoping that you can help me out, OK here it goes so bear with me.  I am going to try and make it as simple and clear as possible. I am going to start with a horse racing example and then go into my dilemma Horse A and Horse B are going to race against each other in a to horse race. They have raced each other 12 times before and Horse A has won five times and horse B has won seven times. So horse A has won 41.7% of the time but.... There are two Jockeys RED and BLUE 3 out of 5 of those wins came when Blue Jockey was riding horse A. However Blue Jockey rode him only once on the days horse A lost. Today Blue Jockey is riding horse A so we get a 60% chance if we just use that Blue jockey 3 out of five fact. But I know we need to consider the total amount of races as well So far I have this then P(horse A wins given blue jockey rides) =  .60 X .417 /.333 because 60% of 3 of the 5 wins were when Blue was riding. 41.7% win over all and .333 cause Blue Jockey has ridden the horse 4 times out of twelve races. Right? Now here   is where my question with changes Two horses A & B but five jockeys.  so lets just use some quick made up stats for the sake of time. Horse A  has a 66% chance to win but when Jockey  Black rides the horse A, horse A  wins 70.5% of the time. now the colors of the jockeys are put in a jar to be pulled out to see who is going to ride horse A.  the following are the chances in % of color being pulled out Red 19% Black 21% Green 13% Blue 37% Pink  10% Would my formula to see the actual percentage that horse A wins be: Horse A wins given Black rides = .705 X .66/.21 which then gives me 2.21 which is WHAT??  what does that mean? AND if it were a different rider lets say pink and for the sake of simplicity same win% would the denominator then be .10 This is causing me to lose sleep , just kidding(kinda) :)","I have a problem that I have been racking my brain to figure this out and I just don't have the background to know if I am correct or not so I am hoping that you can help me out, OK here it goes so bear with me.  I am going to try and make it as simple and clear as possible. I am going to start with a horse racing example and then go into my dilemma Horse A and Horse B are going to race against each other in a to horse race. They have raced each other 12 times before and Horse A has won five times and horse B has won seven times. So horse A has won 41.7% of the time but.... There are two Jockeys RED and BLUE 3 out of 5 of those wins came when Blue Jockey was riding horse A. However Blue Jockey rode him only once on the days horse A lost. Today Blue Jockey is riding horse A so we get a 60% chance if we just use that Blue jockey 3 out of five fact. But I know we need to consider the total amount of races as well So far I have this then P(horse A wins given blue jockey rides) =  .60 X .417 /.333 because 60% of 3 of the 5 wins were when Blue was riding. 41.7% win over all and .333 cause Blue Jockey has ridden the horse 4 times out of twelve races. Right? Now here   is where my question with changes Two horses A & B but five jockeys.  so lets just use some quick made up stats for the sake of time. Horse A  has a 66% chance to win but when Jockey  Black rides the horse A, horse A  wins 70.5% of the time. now the colors of the jockeys are put in a jar to be pulled out to see who is going to ride horse A.  the following are the chances in % of color being pulled out Red 19% Black 21% Green 13% Blue 37% Pink  10% Would my formula to see the actual percentage that horse A wins be: Horse A wins given Black rides = .705 X .66/.21 which then gives me 2.21 which is WHAT??  what does that mean? AND if it were a different rider lets say pink and for the sake of simplicity same win% would the denominator then be .10 This is causing me to lose sleep , just kidding(kinda) :)",,"['probability', 'bayes-theorem']"
69,Russian roulette should a player pull the trigger or spin the cylinder,Russian roulette should a player pull the trigger or spin the cylinder,,"Two men plays Russian roulette. In revolver there are 2 bullets in consecutive chambers( 2 bullets are in 2 chambers next to each other). One man spun the cylinder, pulled the trigger and he is fine, so the chamber was empty. What should the second man do: pull the trigger or spin the cylinder. My answer: Pull the trigger Explanation: If the man spins the cylinder there is 2 full chambers with the bullet, 4 empty chambers. The probability of drawing empty chamber is $\frac{4}{6} = \frac{2}{3}$ If the man just pull the trigger we have to remember that after 3 empty chambers there is empty chamber and after 1 empty chamber there is full chamber so the probability of that the next chamber is empty is $\frac{3}{4}$ $\frac{3}{4} \gt \frac{2}{3}$ Question: Am I right?","Two men plays Russian roulette. In revolver there are 2 bullets in consecutive chambers( 2 bullets are in 2 chambers next to each other). One man spun the cylinder, pulled the trigger and he is fine, so the chamber was empty. What should the second man do: pull the trigger or spin the cylinder. My answer: Pull the trigger Explanation: If the man spins the cylinder there is 2 full chambers with the bullet, 4 empty chambers. The probability of drawing empty chamber is $\frac{4}{6} = \frac{2}{3}$ If the man just pull the trigger we have to remember that after 3 empty chambers there is empty chamber and after 1 empty chamber there is full chamber so the probability of that the next chamber is empty is $\frac{3}{4}$ $\frac{3}{4} \gt \frac{2}{3}$ Question: Am I right?",,"['probability', 'puzzle']"
70,Gaussian processes are determined by their mean and covariance functions.,Gaussian processes are determined by their mean and covariance functions.,,"A stochastic process $X_t$ is called Gaussian if the random vectors $(X_{t_1},...,X_{t_n})$ are multivariate normal. Why are the finite dimensional distributions of a Gaussian process determined by its mean and covariance functions: $m(t) = EX_t$, $\rho(s,t) = E[(X_s-m(s))(X_t - m(t))^T]$? Thank you","A stochastic process $X_t$ is called Gaussian if the random vectors $(X_{t_1},...,X_{t_n})$ are multivariate normal. Why are the finite dimensional distributions of a Gaussian process determined by its mean and covariance functions: $m(t) = EX_t$, $\rho(s,t) = E[(X_s-m(s))(X_t - m(t))^T]$? Thank you",,"['probability', 'stochastic-processes', 'random-variables']"
71,"Finding optimal thresholds for ""guess if number is highest"" game","Finding optimal thresholds for ""guess if number is highest"" game",,"Consider the following game: five numbers are chosen randomly in the interval [0..1] with uniform distribution.  The player is shown each number in turn and asked if it is the highest.  The game proceeds until the player either answers incorrectly (false positive and false negative are both losing outcomes) or correctly guesses that the present number is the highest (correctly answering that the current number is the highest is an instant win, since the player could presumably answer correctly that one of the others is). If the player's goal is to maximize the probability of answering correctly whether each given number is the highest, the player should answer ""yes"" when shown a number which is larger than any seen thus far, and which is larger than the 1-1/2^(1/n), where n is the number of unseen numbers remaining (e.g. if the third number is sqrt(2), and the first two numbers were smaller than that, there's a 50% probability that both of the remaining numbers are less than that, so either ""yes"" and ""no"" would have a 50% probability of being correct; if the value is higher, then ""yes"" will more likely be correct; if it's lower, then ""no"" will be most likely. If the player's goal is to maximize the probability of winning , however, the calculations would seem to get more complicated.  When asked whether the fourth number is highest, the player should answer ""Yes"" if it's greater than 50%, and greater than all numbers seen thus far.  If the player answers correctly, the player wins (a player who correctly states that the fourth number is not the biggest will have no trouble answering that the fifth is).  When answering whether the ""third"" number is the biggest, however, the reward for correctly identifying that it is the biggest (i.e. an instant win) is larger than the reward for identifying that it is not (since the player will still risk a loss on the next number). Is there any nice way to determine the threshold for each number where the player should announce that it's the biggest (assuming it's larger than any values seen thus far)?  Attempting to partition a 3-dimensional space to decide what to do on the third guess seems awkward but not impossible, but going beyond that to figuring out the optimal strategy on the second guess would seem unworkable absent some simplification I'm not seeing.","Consider the following game: five numbers are chosen randomly in the interval [0..1] with uniform distribution.  The player is shown each number in turn and asked if it is the highest.  The game proceeds until the player either answers incorrectly (false positive and false negative are both losing outcomes) or correctly guesses that the present number is the highest (correctly answering that the current number is the highest is an instant win, since the player could presumably answer correctly that one of the others is). If the player's goal is to maximize the probability of answering correctly whether each given number is the highest, the player should answer ""yes"" when shown a number which is larger than any seen thus far, and which is larger than the 1-1/2^(1/n), where n is the number of unseen numbers remaining (e.g. if the third number is sqrt(2), and the first two numbers were smaller than that, there's a 50% probability that both of the remaining numbers are less than that, so either ""yes"" and ""no"" would have a 50% probability of being correct; if the value is higher, then ""yes"" will more likely be correct; if it's lower, then ""no"" will be most likely. If the player's goal is to maximize the probability of winning , however, the calculations would seem to get more complicated.  When asked whether the fourth number is highest, the player should answer ""Yes"" if it's greater than 50%, and greater than all numbers seen thus far.  If the player answers correctly, the player wins (a player who correctly states that the fourth number is not the biggest will have no trouble answering that the fifth is).  When answering whether the ""third"" number is the biggest, however, the reward for correctly identifying that it is the biggest (i.e. an instant win) is larger than the reward for identifying that it is not (since the player will still risk a loss on the next number). Is there any nice way to determine the threshold for each number where the player should announce that it's the biggest (assuming it's larger than any values seen thus far)?  Attempting to partition a 3-dimensional space to decide what to do on the third guess seems awkward but not impossible, but going beyond that to figuring out the optimal strategy on the second guess would seem unworkable absent some simplification I'm not seeing.",,"['probability', 'game-theory']"
72,Does X = Y in distribution and X being Y-measurable imply Y is X-measurable?,Does X = Y in distribution and X being Y-measurable imply Y is X-measurable?,,"Suppose $X,Y$ are random variables taking values in some Borel space, $X \overset {d}{=} Y$, and $X$ is $Y$-measurable. It follows from the fact that $X$ is $Y$-measurable that there exists a measurable $f$ such that $X = f(Y)$ a.s. Is it the case that there exists a measurable $g$ such that $Y = g(X)$ a.s.? It seems plausible, and is true for finite-valued, discrete random variables. EDIT: I've thought about it a bit, and one consequence is $X \overset {d}{=} f(X)$, which may or may not be helpful.","Suppose $X,Y$ are random variables taking values in some Borel space, $X \overset {d}{=} Y$, and $X$ is $Y$-measurable. It follows from the fact that $X$ is $Y$-measurable that there exists a measurable $f$ such that $X = f(Y)$ a.s. Is it the case that there exists a measurable $g$ such that $Y = g(X)$ a.s.? It seems plausible, and is true for finite-valued, discrete random variables. EDIT: I've thought about it a bit, and one consequence is $X \overset {d}{=} f(X)$, which may or may not be helpful.",,"['probability', 'measure-theory']"
73,Probability Questions! [duplicate],Probability Questions! [duplicate],,"This question already has answers here : Probability Dice Game Follow Up (3 answers) Closed 10 years ago . Alex, Bret, and Chloe repeatedly take turns tossing a fair die. Alex begins; Bret always follows Alex; Chloe always follows Bret; Alex always follows Chloe, and so on. Find the probability that Chloe will be the first one to toss a six.","This question already has answers here : Probability Dice Game Follow Up (3 answers) Closed 10 years ago . Alex, Bret, and Chloe repeatedly take turns tossing a fair die. Alex begins; Bret always follows Alex; Chloe always follows Bret; Alex always follows Chloe, and so on. Find the probability that Chloe will be the first one to toss a six.",,"['probability', 'dice']"
74,How to choose between an odd number of options with a fair coin,How to choose between an odd number of options with a fair coin,,"It is possible to choose between three equally desirable outcomes by tossing a fair coin as follows: Choose option 1 if the first head appears on an even toss Choose option 2 if the first tail appears on an even toss Choose option 3 if the first head and the first tail both appear on an odd toss Note that option 1 and option 2 are mutually exclusive, because we must have a head or a tail on the first toss. This works because the binary expansion $$\frac 13 =0.010101010101 \dots$$ has a $1$ in all the even places. Or equivalently because $$\frac 13=\frac 14+\frac1{16}+ \dots +\frac 1{4^n}+\dots$$ This is quite a simple procedure (though perhaps not as well known as it might be). I was pondering in the bath what might be the most convenient set of options if there were five or seven possibilities. For example I can pick off two fifths using the binary expansion of $\frac 15$ with heads and tails, and use my procedure for thirds to do the remaining three fifths - so it is possible. But can one do fifths or sevenths without resort to this kind of recursive procedure?","It is possible to choose between three equally desirable outcomes by tossing a fair coin as follows: Choose option 1 if the first head appears on an even toss Choose option 2 if the first tail appears on an even toss Choose option 3 if the first head and the first tail both appear on an odd toss Note that option 1 and option 2 are mutually exclusive, because we must have a head or a tail on the first toss. This works because the binary expansion $$\frac 13 =0.010101010101 \dots$$ has a $1$ in all the even places. Or equivalently because $$\frac 13=\frac 14+\frac1{16}+ \dots +\frac 1{4^n}+\dots$$ This is quite a simple procedure (though perhaps not as well known as it might be). I was pondering in the bath what might be the most convenient set of options if there were five or seven possibilities. For example I can pick off two fifths using the binary expansion of $\frac 15$ with heads and tails, and use my procedure for thirds to do the remaining three fifths - so it is possible. But can one do fifths or sevenths without resort to this kind of recursive procedure?",,"['probability', 'recreational-mathematics']"
75,Rational probabilities,Rational probabilities,,"A probability space is generally defined to be a sample space $\Omega$ with a sigma algebra $F\subseteq 2^{\Omega}$ and a probability function $P:F\to \mathbb{R}$. Is there anything inconsistent about taking the probability function to be $P:F\to\mathbb{Q}$ instead of $P:F\to \mathbb{R}$? That is, does it make sense to think of probabilities as rationals rather than reals? Generalizing slightly, for what spaces $X$ can you consider a function $P:F\to X$ and have the resulting system be something that we would recognize as a probability space?","A probability space is generally defined to be a sample space $\Omega$ with a sigma algebra $F\subseteq 2^{\Omega}$ and a probability function $P:F\to \mathbb{R}$. Is there anything inconsistent about taking the probability function to be $P:F\to\mathbb{Q}$ instead of $P:F\to \mathbb{R}$? That is, does it make sense to think of probabilities as rationals rather than reals? Generalizing slightly, for what spaces $X$ can you consider a function $P:F\to X$ and have the resulting system be something that we would recognize as a probability space?",,"['probability', 'probability-theory']"
76,Conditional covariance.,Conditional covariance.,,"Suppose we have two random variables, $X$ and $Y$, defined over nonnegative reals. Obviously, the following formula holds: $$\mathrm{Cov}(X,Y)=\mathbb{E}\left[XY\right]-\mathbb{E}\left[X\right]\mathbb{E}\left[Y\right].$$ However intuitional it may be, I am not so sure whether another formula that I stumbled upon holds: $$\mathrm{Cov}(X,Y|X>0\,\wedge\,Y>0)=\mathbb{E}\left[XY|X>0\,\wedge\,Y>0\right]-\mathbb{E}\left[X|X>0\,\wedge\,Y>0\right]\cdot\mathbb{E}\left[Y|X>0\,\wedge\,Y>0\right].$$ I am not looking for a proof, but I would be thankful for a short note why or why not the above formula holds. Thank you in advance.","Suppose we have two random variables, $X$ and $Y$, defined over nonnegative reals. Obviously, the following formula holds: $$\mathrm{Cov}(X,Y)=\mathbb{E}\left[XY\right]-\mathbb{E}\left[X\right]\mathbb{E}\left[Y\right].$$ However intuitional it may be, I am not so sure whether another formula that I stumbled upon holds: $$\mathrm{Cov}(X,Y|X>0\,\wedge\,Y>0)=\mathbb{E}\left[XY|X>0\,\wedge\,Y>0\right]-\mathbb{E}\left[X|X>0\,\wedge\,Y>0\right]\cdot\mathbb{E}\left[Y|X>0\,\wedge\,Y>0\right].$$ I am not looking for a proof, but I would be thankful for a short note why or why not the above formula holds. Thank you in advance.",,['probability']
77,A basic doubt on joint distribution,A basic doubt on joint distribution,,"How to calculate the following probability $P(X \leq x, Y=y)$ where $X$ is a continuous random variable and $Y$ is a discrete random variable. I have been given the distribution of $X$ and distribution of $P(Y=y|X=x)$.","How to calculate the following probability $P(X \leq x, Y=y)$ where $X$ is a continuous random variable and $Y$ is a discrete random variable. I have been given the distribution of $X$ and distribution of $P(Y=y|X=x)$.",,"['probability', 'probability-distributions']"
78,"Recursive formula for the probability that the starting player wins [DBertsekas & JTsitsiklis P57, 1.21]","Recursive formula for the probability that the starting player wins [DBertsekas & JTsitsiklis P57, 1.21]",,"Two players take turns removing a ball from a jar that initially contains   $w$ white and $b$ black balls. The first player to remove a white ball wins. Develop a   recursive formula that allows the convenient computation of the probability that the   starting player wins. Solution: Let $P(b, w)$ be the probability that the starting player wins when the jar initially contains $w$ white and $b$ black balls. We have, using the total probability theorem, $\Large{\color{red}{[}}$ $P(b, w) = \frac{w}{b + w} + \frac{b}{b + w}[ \, 1 - P(b - 1, w) \,] = 1- \frac{b}{b + w}P(b - 1, w) \, \Large{\color{red}{]}}$. The probabilities $P(1,w), P(2,w), ..., P(n,w)$ can be calculated sequentially using ths formula, starting with the initial condition $P(0,w) = 1$. How do you derive the expression inside the red brackets? Since the question postulates the starting player to win, I was thinking: $P(b, w) = P($a white ball is chosen on the 1st turn, or on the 3rd turn, or on the 5th or ...). Source: P57, 1.21, An Intro to Pr , 2nd Ed, D Bertsekas & J Tsitsiklis","Two players take turns removing a ball from a jar that initially contains   $w$ white and $b$ black balls. The first player to remove a white ball wins. Develop a   recursive formula that allows the convenient computation of the probability that the   starting player wins. Solution: Let $P(b, w)$ be the probability that the starting player wins when the jar initially contains $w$ white and $b$ black balls. We have, using the total probability theorem, $\Large{\color{red}{[}}$ $P(b, w) = \frac{w}{b + w} + \frac{b}{b + w}[ \, 1 - P(b - 1, w) \,] = 1- \frac{b}{b + w}P(b - 1, w) \, \Large{\color{red}{]}}$. The probabilities $P(1,w), P(2,w), ..., P(n,w)$ can be calculated sequentially using ths formula, starting with the initial condition $P(0,w) = 1$. How do you derive the expression inside the red brackets? Since the question postulates the starting player to win, I was thinking: $P(b, w) = P($a white ball is chosen on the 1st turn, or on the 3rd turn, or on the 5th or ...). Source: P57, 1.21, An Intro to Pr , 2nd Ed, D Bertsekas & J Tsitsiklis",,[]
79,Trying to work out the probabilities of a dice game I used to play,Trying to work out the probabilities of a dice game I used to play,,"At college, my friends and I would sometimes waste time playing a game with dice. We would roll 25 dice, pick out all the dice that landed on a 6, then roll the rest. This would carry on until all the dice were removed. The winner was the person who managed to remove all the dice in the least amount of turns. (A turn counts as rolling the dice - i.e. the first turn is rolling all 25 dice, the second turn is rolling all the dice which didn't roll six on the first turn, etc.) I want to find the expected amount of turns it would take to finish the game, but I am having difficulty modelling it. I know that if there was one dice then you could model it using the geometric distribution; and the number of sixes on each turn can be modeled using the binomial distribution, but I do not know how to combine the two (or if this is even useful). Please help, this problem has been bugging me at the back of my head for several years.","At college, my friends and I would sometimes waste time playing a game with dice. We would roll 25 dice, pick out all the dice that landed on a 6, then roll the rest. This would carry on until all the dice were removed. The winner was the person who managed to remove all the dice in the least amount of turns. (A turn counts as rolling the dice - i.e. the first turn is rolling all 25 dice, the second turn is rolling all the dice which didn't roll six on the first turn, etc.) I want to find the expected amount of turns it would take to finish the game, but I am having difficulty modelling it. I know that if there was one dice then you could model it using the geometric distribution; and the number of sixes on each turn can be modeled using the binomial distribution, but I do not know how to combine the two (or if this is even useful). Please help, this problem has been bugging me at the back of my head for several years.",,"['probability', 'discrete-mathematics', 'probability-distributions', 'recreational-mathematics', 'dice']"
80,"Does ""gerrymandering"" matter?","Does ""gerrymandering"" matter?",,"In the United States, it is often a raw point discussing the issue of redistricting, or so unfavorably called, ""Gerrymandering."" Background:  In the United States House of Representatives, each State is allotted a number of representatives based on the State's population.  Within the state, each of its representatives are assigned a geographical ""district.""  The point of contention comes with the fact that a states Governor may periodically get to redraw the districting lines. My question is: mathematically, probabilistically, can this work?  My intuition tells me that if one line is moved over to improve the chances of one side winning one district, won't that just hurt the chances of winning the other?","In the United States, it is often a raw point discussing the issue of redistricting, or so unfavorably called, ""Gerrymandering."" Background:  In the United States House of Representatives, each State is allotted a number of representatives based on the State's population.  Within the state, each of its representatives are assigned a geographical ""district.""  The point of contention comes with the fact that a states Governor may periodically get to redraw the districting lines. My question is: mathematically, probabilistically, can this work?  My intuition tells me that if one line is moved over to improve the chances of one side winning one district, won't that just hurt the chances of winning the other?",,['probability']
81,Compute the mean of $(1 + X)^{-1}$ where $X$ is Poisson$(\lambda)$,Compute the mean of  where  is Poisson,(1 + X)^{-1} X (\lambda),"Question Let $X$ be Poisson with parameter $\lambda$.  Compute the mean of $(1 + X)^{-1}$. (Introduction to Probability Theory, Hoel, pp. 104) Answer Key The answer key shows that the mean is $\lambda^{-1}(1 - e^{-\lambda})$ My Solution $$E(1 + X)^{-1} = \sum\limits_{j = 1}^\infty (1 + j)^{-1} \dfrac{\lambda^{(1 + j)^{-1}}e^{-\lambda}}{(1 + j)^{-1}!}$$ $$= e^{-\lambda} \sum\limits_{j = 1}^\infty \dfrac{\lambda^{(1 + j)^{-1}}}{((1 + j)^{-1} - 1)!}$$ $$= \lambda^{-1} e^{-\lambda} \sum\limits_{j = 1}^\infty \dfrac{\lambda^{(1 + j)^{-1} - 1}}{((1 + j)^{-1} - 1)!}$$ $$= \lambda^{-1} e^{-\lambda}(e^{\lambda} - 1)$$ $$= \lambda^{-1}(1 - e^{-\lambda})$$ EDIT: I found why I got the answer incorrect.  Thanks for those who answer the question! :)","Question Let $X$ be Poisson with parameter $\lambda$.  Compute the mean of $(1 + X)^{-1}$. (Introduction to Probability Theory, Hoel, pp. 104) Answer Key The answer key shows that the mean is $\lambda^{-1}(1 - e^{-\lambda})$ My Solution $$E(1 + X)^{-1} = \sum\limits_{j = 1}^\infty (1 + j)^{-1} \dfrac{\lambda^{(1 + j)^{-1}}e^{-\lambda}}{(1 + j)^{-1}!}$$ $$= e^{-\lambda} \sum\limits_{j = 1}^\infty \dfrac{\lambda^{(1 + j)^{-1}}}{((1 + j)^{-1} - 1)!}$$ $$= \lambda^{-1} e^{-\lambda} \sum\limits_{j = 1}^\infty \dfrac{\lambda^{(1 + j)^{-1} - 1}}{((1 + j)^{-1} - 1)!}$$ $$= \lambda^{-1} e^{-\lambda}(e^{\lambda} - 1)$$ $$= \lambda^{-1}(1 - e^{-\lambda})$$ EDIT: I found why I got the answer incorrect.  Thanks for those who answer the question! :)",,['probability']
82,Conditional expectation as expectation,Conditional expectation as expectation,,$X$ is a real-valued random variable. $B$ is an event. Is it right that the conditional expectation $\mathbb{E}[X\mid B]$ can be seen simply as an ordinary expectation over a new probability space having as a measure the corresponding conditional probability? Thanks,is a real-valued random variable. is an event. Is it right that the conditional expectation can be seen simply as an ordinary expectation over a new probability space having as a measure the corresponding conditional probability? Thanks,X B \mathbb{E}[X\mid B],"['probability', 'measure-theory', 'expected-value']"
83,Negative binomial distribution - deriving of the p.m.f. combinatorially,Negative binomial distribution - deriving of the p.m.f. combinatorially,,"Let $X$ be the number of trials preceding the $k$th success in a sequence of independent Bernoulli trials each with probability of success $p$. Then $X$ has a negative binomial distribution with p.m.f. $p(x) = \binom{x+k-1}{k-1}p^{k}(1-p)^{x}$. Please explain this pmf using a combinatorial argument. I get why we need to choose $k-1$ successes, but the rest of the pmf is eluding me.","Let $X$ be the number of trials preceding the $k$th success in a sequence of independent Bernoulli trials each with probability of success $p$. Then $X$ has a negative binomial distribution with p.m.f. $p(x) = \binom{x+k-1}{k-1}p^{k}(1-p)^{x}$. Please explain this pmf using a combinatorial argument. I get why we need to choose $k-1$ successes, but the rest of the pmf is eluding me.",,"['probability', 'combinatorics', 'probability-distributions', 'combinations']"
84,Form of the spectral density in Wiener Khinchin theorem?,Form of the spectral density in Wiener Khinchin theorem?,,"The Wiener–Khinchin theorem says the autocorrelation function of a wide sense stationary process can be written as a Stieltjes integral, where the integrator function is called the power spectral distribution function . When the power spectral distribution function is absolutely continuous, its derivative is called the power spectral density , and the power spectral density and the autocorrelation function are a Fourier transform pair. My question is when and how the power spectral density of the stationary stochastic process can be represented as in Wikipedia :  for a stationary process $x(t), t \geq 0$, the power spectral density can be defined as   $$     S_{xx}(\omega) := \lim_{T \rightarrow \infty} \mathbf{E} \left[ | \frac{1}{\sqrt{T}} \int_0^T x(t) e^{-i\omega t}\, dt | ^ 2 \right]. $$ Thanks and regards!","The Wiener–Khinchin theorem says the autocorrelation function of a wide sense stationary process can be written as a Stieltjes integral, where the integrator function is called the power spectral distribution function . When the power spectral distribution function is absolutely continuous, its derivative is called the power spectral density , and the power spectral density and the autocorrelation function are a Fourier transform pair. My question is when and how the power spectral density of the stationary stochastic process can be represented as in Wikipedia :  for a stationary process $x(t), t \geq 0$, the power spectral density can be defined as   $$     S_{xx}(\omega) := \lim_{T \rightarrow \infty} \mathbf{E} \left[ | \frac{1}{\sqrt{T}} \int_0^T x(t) e^{-i\omega t}\, dt | ^ 2 \right]. $$ Thanks and regards!",,"['probability', 'stochastic-processes', 'fourier-analysis', 'stationary-processes']"
85,"Two numbers are randomly selected from the set {0,1,2,3,4,5,6,7} without replacement","Two numbers are randomly selected from the set {0,1,2,3,4,5,6,7} without replacement",,What is the probability that the sum of the two numbers is 7? [what i did] 0+7  1+6 2+5  4+3 my ans:  4/8 Is this right or wrong? Does without replacement mean we can't use the same number twice? IE: 4+2 AND 2+4? Thanks for the help,What is the probability that the sum of the two numbers is 7? [what i did] 0+7  1+6 2+5  4+3 my ans:  4/8 Is this right or wrong? Does without replacement mean we can't use the same number twice? IE: 4+2 AND 2+4? Thanks for the help,,['probability']
86,"Probability, integers and reals (soft question)","Probability, integers and reals (soft question)",,"Given a random integer, is the probability of correctly guessing what it is exactly zero? What if it would be a real number, rather than an integer? Does the fact that the set of all integers is countable and the set of real numbers is uncountable change the probability value?","Given a random integer, is the probability of correctly guessing what it is exactly zero? What if it would be a real number, rather than an integer? Does the fact that the set of all integers is countable and the set of real numbers is uncountable change the probability value?",,['probability']
87,Compute value of $\pi$ up to 8 digits,Compute value of  up to 8 digits,\pi,"I am quite lost on how approximate the value of $\pi$ up to 8 digits with a confidence of 99% using Monte Carlo. I think this requires a large number of trials but how can I know how many trials? I know that a 99% confidence interval is 3 standard deviations away from the mean in a normal distribution. From the central limit theorem the standard deviation of the sample mean (or standard error) is proportional to the standard deviation of the population $\sigma_{\bar X} = \frac{\sigma}{\sqrt{n}}$ So I have something that relates the size of the sample (i.e. number of trials) with the standard deviation, but then I don't know how to proceed from here. How does the ""8 digit precision"" comes into play? UPDATE Ok I think I am close to understand it. From CLT we have $\displaystyle \sigma_{M} = \frac{\sigma}{\sqrt{N}}$ so in this case $\sigma = \sqrt{p(1-p)}$ therfore $\displaystyle \sigma_{M} = \frac{\sqrt{p(1-p)}}{\sqrt{N}}$ Then from the Bernoulli distribution, $\displaystyle \mu = p = \frac{\pi}{4}$ therefore $$\sigma_{M}=\frac{\sqrt{\pi(4-\pi)}}{\sqrt{N}}$$ but what would be the value of $\sigma_{M}$? and then I have $\pi$ in the formula but is the thing I am trying to approximate so how does this work? and still missing the role of the 8 digit precision.","I am quite lost on how approximate the value of $\pi$ up to 8 digits with a confidence of 99% using Monte Carlo. I think this requires a large number of trials but how can I know how many trials? I know that a 99% confidence interval is 3 standard deviations away from the mean in a normal distribution. From the central limit theorem the standard deviation of the sample mean (or standard error) is proportional to the standard deviation of the population $\sigma_{\bar X} = \frac{\sigma}{\sqrt{n}}$ So I have something that relates the size of the sample (i.e. number of trials) with the standard deviation, but then I don't know how to proceed from here. How does the ""8 digit precision"" comes into play? UPDATE Ok I think I am close to understand it. From CLT we have $\displaystyle \sigma_{M} = \frac{\sigma}{\sqrt{N}}$ so in this case $\sigma = \sqrt{p(1-p)}$ therfore $\displaystyle \sigma_{M} = \frac{\sqrt{p(1-p)}}{\sqrt{N}}$ Then from the Bernoulli distribution, $\displaystyle \mu = p = \frac{\pi}{4}$ therefore $$\sigma_{M}=\frac{\sqrt{\pi(4-\pi)}}{\sqrt{N}}$$ but what would be the value of $\sigma_{M}$? and then I have $\pi$ in the formula but is the thing I am trying to approximate so how does this work? and still missing the role of the 8 digit precision.",,"['probability', 'probability-distributions', 'probability-limit-theorems']"
88,Integrate over the uniform distribution on the simplex,Integrate over the uniform distribution on the simplex,,"Let $p=(p_1,\ldots,p_n)$ correspond to points in a simplex that add up to one, i.e. $p$ is a discrete probability distribution. I would like to compute an integral of the form $\int dp_1\ldots\int dp_n\sum_{i=1}^np_if(p_i)$ with $p$ uniformly distributed on the $n-1$ dimensional simplex. My question is, how can I parameterize $p_i$ such that the integral covers the simplex uniformly?","Let $p=(p_1,\ldots,p_n)$ correspond to points in a simplex that add up to one, i.e. $p$ is a discrete probability distribution. I would like to compute an integral of the form $\int dp_1\ldots\int dp_n\sum_{i=1}^np_if(p_i)$ with $p$ uniformly distributed on the $n-1$ dimensional simplex. My question is, how can I parameterize $p_i$ such that the integral covers the simplex uniformly?",,"['probability', 'probability-theory', 'probability-distributions']"
89,probability question shooting star,probability question shooting star,,"There is a 91% chance of seeing a shooting star in the next hour, what is the probability of seeing a shooting star in the next half hour? chance of seeing in an hour = .91 Chance of not seeing in an hour = .09 = (chance of not seeing in a half hour)^2 Chance of not seeing in a half hour = sqrt(.09) =0.3 Chance of seeing in a half hour = 0.7 Just out of curiosity, what exactly is the problem with saying that .455 is the prob of seeing a shooting star in a half hour? I realize that it's wrong because one can simply ask what is the prob of seeing a shooting star in 1.5 hours. You can't just add. Is the problem that we're double counting? What does .455+.455 even represent? Are we double counting the times we see 1 shooting star in the first half hour and 1 shooting star in the second? Once we solve for chance of seeing a star in a half hour, can we check our answer? P(shooting star in half hour) + P(shooting star in half hour) - P(2 shooting stars in 2 half hours) = .91? .7 + .7 - .49 = .91 Yes?","There is a 91% chance of seeing a shooting star in the next hour, what is the probability of seeing a shooting star in the next half hour? chance of seeing in an hour = .91 Chance of not seeing in an hour = .09 = (chance of not seeing in a half hour)^2 Chance of not seeing in a half hour = sqrt(.09) =0.3 Chance of seeing in a half hour = 0.7 Just out of curiosity, what exactly is the problem with saying that .455 is the prob of seeing a shooting star in a half hour? I realize that it's wrong because one can simply ask what is the prob of seeing a shooting star in 1.5 hours. You can't just add. Is the problem that we're double counting? What does .455+.455 even represent? Are we double counting the times we see 1 shooting star in the first half hour and 1 shooting star in the second? Once we solve for chance of seeing a star in a half hour, can we check our answer? P(shooting star in half hour) + P(shooting star in half hour) - P(2 shooting stars in 2 half hours) = .91? .7 + .7 - .49 = .91 Yes?",,['probability']
90,Truchet tiles on a flattened cube,Truchet tiles on a flattened cube,,"We have 2 Truchet tiles and a flattened cube as shown. We randomly place copies of the tiles into faces of the flattened cube. Find the probability that the circular arcs on the Truchet tiles will form one loop, two loops, three loops and four loops? If we divide each face of the flattened cube into 2x2 grid of squares, what will be the new probability for part (1)? I saw a journal about the three-dimensional Truchet tiles and they want the beginners to start with the basic question. I don't have any idea to start with. I'm thinking of defining a binary variable but it doesn't seem so promising.","We have 2 Truchet tiles and a flattened cube as shown. We randomly place copies of the tiles into faces of the flattened cube. Find the probability that the circular arcs on the Truchet tiles will form one loop, two loops, three loops and four loops? If we divide each face of the flattened cube into 2x2 grid of squares, what will be the new probability for part (1)? I saw a journal about the three-dimensional Truchet tiles and they want the beginners to start with the basic question. I don't have any idea to start with. I'm thinking of defining a binary variable but it doesn't seem so promising.",,"['probability', 'recreational-mathematics', 'geometric-probability']"
91,Variance of derangements,Variance of derangements,,"Suppose I choose a random permutation on n numbers. It is easy to prove that the mean of the number of fixed points (i.e. the numbers that get mapped to themselves) is 1. Is there an easy (constant) bound like this on the variance of this random variable ? Thanks,","Suppose I choose a random permutation on n numbers. It is easy to prove that the mean of the number of fixed points (i.e. the numbers that get mapped to themselves) is 1. Is there an easy (constant) bound like this on the variance of this random variable ? Thanks,",,"['probability', 'permutations', 'random-variables']"
92,3 Die Probability,3 Die Probability,,The number of times you have to roll a set of 3 die to see a 6 on each die appears to be around ~10.555 according to a implementation of this in c++ and c#. How would this be statistically proven as instinct would suggest it should be 6 ? Each die has 6 sides that are evenly distributed. All three die are thrown at the same time counting as one roll. Once all three die have displayed 6 at least once you can stop. What I am looking for is the statistical proof that you are required on average to roll the set 10.555 times to have seen 6 on each of the 3 die at least once. Many Thanks,The number of times you have to roll a set of 3 die to see a 6 on each die appears to be around ~10.555 according to a implementation of this in c++ and c#. How would this be statistically proven as instinct would suggest it should be 6 ? Each die has 6 sides that are evenly distributed. All three die are thrown at the same time counting as one roll. Once all three die have displayed 6 at least once you can stop. What I am looking for is the statistical proof that you are required on average to roll the set 10.555 times to have seen 6 on each of the 3 die at least once. Many Thanks,,"['probability', 'dice']"
93,Can we simplify an expression of random variables? (can we treat random variables as real numbers?),Can we simplify an expression of random variables? (can we treat random variables as real numbers?),,"Suppose that we have an expression of random variables including $X-X$ or $2X-X$ or $XY-XY$ and so on. can we treat random variables as real numbers? That is, can we delete $X-X$ or replace $2X-X$ by $X$? what about $X/X$?","Suppose that we have an expression of random variables including $X-X$ or $2X-X$ or $XY-XY$ and so on. can we treat random variables as real numbers? That is, can we delete $X-X$ or replace $2X-X$ by $X$? what about $X/X$?",,"['probability', 'statistics', 'probability-theory', 'random-variables']"
94,Expected value of a log Poisson distribution,Expected value of a log Poisson distribution,,"Suppose there is a sequence $(X_n)_n$ of independent random variables, $X_i \sim Poisson(\lambda)$. I order to almost surely compute $\lim\limits_{n\to \infty} \sqrt[n]{X_1X_2\dots X_n}$, I thought of using the law of large numbers for $\ln(X_1) + \ln(X_2) + \dots + \ln(X_n)\over n$. However, the natural logarithm is not defined for $0$, which is one value the random variables could take. I suppose that makes applying the law incorrect. Any other thoughts?","Suppose there is a sequence $(X_n)_n$ of independent random variables, $X_i \sim Poisson(\lambda)$. I order to almost surely compute $\lim\limits_{n\to \infty} \sqrt[n]{X_1X_2\dots X_n}$, I thought of using the law of large numbers for $\ln(X_1) + \ln(X_2) + \dots + \ln(X_n)\over n$. However, the natural logarithm is not defined for $0$, which is one value the random variables could take. I suppose that makes applying the law incorrect. Any other thoughts?",,"['probability', 'probability-theory', 'probability-distributions', 'law-of-large-numbers']"
95,Calculating $\mathbb{P}(A \mid B)$.,Calculating .,\mathbb{P}(A \mid B),"To confirm the formula for probabilities, given that an event has occured, I wonder if it is true that: $\mathbb{P}(A \mid B)=1-\mathbb{P}(A^{C} \mid B)$ where $\mathbb{P}(A)+\mathbb{P}(A^{C})=1$. $A$ and $B$ are events.","To confirm the formula for probabilities, given that an event has occured, I wonder if it is true that: $\mathbb{P}(A \mid B)=1-\mathbb{P}(A^{C} \mid B)$ where $\mathbb{P}(A)+\mathbb{P}(A^{C})=1$. $A$ and $B$ are events.",,['probability']
96,Probability of 15 consecutive green lights,Probability of 15 consecutive green lights,,"Introduction Upon a trip home, my mother and I were noticing a very peculiar occurrence: Traffic lights were almost continuously green. Indeed, exactly fifteen different traffic lights were green consecutively. Now, I am bad at probability, but this seems unlikely. Probability Application I reasoned that since there are three different options for all fifteen traffic lights, the probability of fifteen traffic lights being consecutively green was one in $_{15}P_{3}$. This is because there is only one sequence of traffic light configurations where all of them are green and we must count all of the possible traffic light configurations using a permutation since a sequence such as G,R,Y is not the same as Y,R,G. With that said, the probability of the event comes out to be approximately $0.0366\%$. Question Is this application of probability correct, incorrect, or somewhere in the middle? To make this question very precise, 'somewhere in the middle' means that my application of probability makes many underlying assumptions and ignores many factors. I am not aware what these specific assumptions and factors may be (if they exist), but that's why I ask this question. In what sense am I correct, and in what sense am I incorrect?","Introduction Upon a trip home, my mother and I were noticing a very peculiar occurrence: Traffic lights were almost continuously green. Indeed, exactly fifteen different traffic lights were green consecutively. Now, I am bad at probability, but this seems unlikely. Probability Application I reasoned that since there are three different options for all fifteen traffic lights, the probability of fifteen traffic lights being consecutively green was one in $_{15}P_{3}$. This is because there is only one sequence of traffic light configurations where all of them are green and we must count all of the possible traffic light configurations using a permutation since a sequence such as G,R,Y is not the same as Y,R,G. With that said, the probability of the event comes out to be approximately $0.0366\%$. Question Is this application of probability correct, incorrect, or somewhere in the middle? To make this question very precise, 'somewhere in the middle' means that my application of probability makes many underlying assumptions and ignores many factors. I am not aware what these specific assumptions and factors may be (if they exist), but that's why I ask this question. In what sense am I correct, and in what sense am I incorrect?",,"['probability', 'applications']"
97,Rolling die until number is greater than 100 [duplicate],Rolling die until number is greater than 100 [duplicate],,"This question already has answers here : Closed 11 years ago . Possible Duplicate: Probability of dice sum just greater than 100 A die is rolled several times and the number appearing is summed. We stop when this sum becomes greater than or equal to 100. What value of sum in the end is the most probable (out of 100, 101, 102, 103, 104, 105)? 105 can be generated in only 1 way : 99 + 6 104 can be generated in only 2 ways: 99 + 5, 98 + 6 and so on. Assuming that the dice rolling will reach either of numbers in range [94,99] is equally likely, 100 is the most likely. Am I correct?","This question already has answers here : Closed 11 years ago . Possible Duplicate: Probability of dice sum just greater than 100 A die is rolled several times and the number appearing is summed. We stop when this sum becomes greater than or equal to 100. What value of sum in the end is the most probable (out of 100, 101, 102, 103, 104, 105)? 105 can be generated in only 1 way : 99 + 6 104 can be generated in only 2 ways: 99 + 5, 98 + 6 and so on. Assuming that the dice rolling will reach either of numbers in range [94,99] is equally likely, 100 is the most likely. Am I correct?",,['probability']
98,$L_p$ complete for $p<1$,complete for,L_p p<1,"It is rather straight forward to show that $L_p$ is complete for $p\geqslant 1$, but I am having trouble showing the same thing when $p<1$. For the former case I have shown that every absolutely convergent sequence converges by constructing a a function in $L_p$ but bigger than the series and used the dominated convergence theorem (I can also do a similar thing using Cauchy sequences rather than absolutely convergent series). The problem is that in showing that my upper bound function is an element of $L_p$ I have used the triangle inequality which I cannot do for $p<1$. Does anyone have any ideas of a way around this? I noticed there are similar questions to this already, but they have either not been answered or closed.","It is rather straight forward to show that $L_p$ is complete for $p\geqslant 1$, but I am having trouble showing the same thing when $p<1$. For the former case I have shown that every absolutely convergent sequence converges by constructing a a function in $L_p$ but bigger than the series and used the dominated convergence theorem (I can also do a similar thing using Cauchy sequences rather than absolutely convergent series). The problem is that in showing that my upper bound function is an element of $L_p$ I have used the triangle inequality which I cannot do for $p<1$. Does anyone have any ideas of a way around this? I noticed there are similar questions to this already, but they have either not been answered or closed.",,"['real-analysis', 'probability', 'measure-theory', 'probability-theory', 'metric-spaces']"
99,"For two uncorrelated random variables $X,Y$, why does $\rho(X+Y,2X+2Y)=4?$","For two uncorrelated random variables , why does","X,Y \rho(X+Y,2X+2Y)=4?","Given two uncorrelated random variables $X,Y$ with the same variance $\sigma^2  $ I need to compute $\rho= \frac{COV(X,Y)}{\sigma(X)\sigma(Y)}$  between $X+Y$ and $2X+2Y$.  I know it should be a number between $-1$ and $1$ and I don't understand how come I get $4$. Here's what I did: $COV(X+Y,2X+2Y)=COV(X+Y,2X)+COV(X+Y,2Y)=COV(2X,X)+COV(2X,Y)+COV(2Y,Y)+COV(2Y,X)=2COV(X,X)+2COV(Y,Y)+4COV(X,Y)=2\sigma^2+2\sigma^2=4\sigma^2$ so final result is $\rho=4$ since $\sigma(X)=\sqrt{Var(x)}$. What's wrong with what I did?","Given two uncorrelated random variables $X,Y$ with the same variance $\sigma^2  $ I need to compute $\rho= \frac{COV(X,Y)}{\sigma(X)\sigma(Y)}$  between $X+Y$ and $2X+2Y$.  I know it should be a number between $-1$ and $1$ and I don't understand how come I get $4$. Here's what I did: $COV(X+Y,2X+2Y)=COV(X+Y,2X)+COV(X+Y,2Y)=COV(2X,X)+COV(2X,Y)+COV(2Y,Y)+COV(2Y,X)=2COV(X,X)+2COV(Y,Y)+4COV(X,Y)=2\sigma^2+2\sigma^2=4\sigma^2$ so final result is $\rho=4$ since $\sigma(X)=\sqrt{Var(x)}$. What's wrong with what I did?",,['probability']
