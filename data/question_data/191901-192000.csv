,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,The derivative of $f(x)=x|x|$,The derivative of,f(x)=x|x|,"We are tasked to compute the derivative of $f(x)=x|x|$ . $$f(x) = \begin{cases}        x^2 & x> 0 \\       0 & x=0\\       -x^2 & x<0     \end{cases} $$ We use the product rule to calculate the derivative of $f(x)$ . $$f'(x)=x(|x|)'+|x|$$ We know that $$(|x|)'= \begin{cases}        1 & x> 0 \\       \text{undefined}& x=0\\       -1 & x<0     \end{cases}$$ So $$f'(x) =\begin{cases}        2x & x> 0 \\       \text{undefined}& x=0\\       -2x & x<0  \end{cases}$$ But if we take the limit as $x \to 0$ , we find the left limit and the right limit equal $0$ . Then $$f'(x) =\begin{cases}        2x & x> 0 \\       0& x=0\\       -2x & x<0 \end{cases}  $$","We are tasked to compute the derivative of . We use the product rule to calculate the derivative of . We know that So But if we take the limit as , we find the left limit and the right limit equal . Then","f(x)=x|x| f(x) = \begin{cases} 
      x^2 & x> 0 \\
      0 & x=0\\
      -x^2 & x<0 
   \end{cases}
 f(x) f'(x)=x(|x|)'+|x| (|x|)'= \begin{cases} 
      1 & x> 0 \\
      \text{undefined}& x=0\\
      -1 & x<0 
   \end{cases} f'(x) =\begin{cases} 
      2x & x> 0 \\
      \text{undefined}& x=0\\
      -2x & x<0 
\end{cases} x \to 0 0 f'(x) =\begin{cases} 
      2x & x> 0 \\
      0& x=0\\
      -2x & x<0
\end{cases}  ",['real-analysis']
1,"Question on ""false"" application of chain rule","Question on ""false"" application of chain rule",,"Let's consider $K:\mathbb{R}^n\times [0,1]\to\mathbb{R}$ and $f:\mathbb{R}^n\to\mathbb{R}^n$ differentiable with $K(u,t):=\langle f(a+t(u-a)),(u-a)\rangle=\sum\limits_{i=1}^nf_i(a+t(u-a))(u_i-a_i)$ , where $a\in\mathbb{R}^n$ . Now let's take the first partial derivative (with respect to $u_1$ ) and apply the chain and product rule of differentiation: \begin{align*} &D_1K(u,t)=D_1\left(\sum\limits_{i=1}^nf_i(a+t(u-a))(u_i-a_i)\right)\\ &=\sum\limits_{i=1}^nD_1f_i(a+t(u-a))~\underset{=?}{\underbrace{D_1(a+t(u-a))}}~(u_i-a_i)+f_1(a+t(u-a)).\end{align*} As $(a+t(u-a))$ attains the form of $\begin{pmatrix}a_1+t(u_1-a_1)\\\vdots\\a_n+t(u_n-a_n) \end{pmatrix}$ , I thought that $D_1(a+t(u-a))=D_1\begin{pmatrix}a_1+t(u_1-a_1)\\a_2+t(u_2-a_2)\\ \vdots\\a_n+t(u_n-a_n) \end{pmatrix}=\begin{pmatrix}t\\0\\\vdots\\0 \end{pmatrix}$ but this doesn't match the dimension of the image of $K$ . Where is my mistake?","Let's consider and differentiable with , where . Now let's take the first partial derivative (with respect to ) and apply the chain and product rule of differentiation: As attains the form of , I thought that but this doesn't match the dimension of the image of . Where is my mistake?","K:\mathbb{R}^n\times [0,1]\to\mathbb{R} f:\mathbb{R}^n\to\mathbb{R}^n K(u,t):=\langle f(a+t(u-a)),(u-a)\rangle=\sum\limits_{i=1}^nf_i(a+t(u-a))(u_i-a_i) a\in\mathbb{R}^n u_1 \begin{align*}
&D_1K(u,t)=D_1\left(\sum\limits_{i=1}^nf_i(a+t(u-a))(u_i-a_i)\right)\\
&=\sum\limits_{i=1}^nD_1f_i(a+t(u-a))~\underset{=?}{\underbrace{D_1(a+t(u-a))}}~(u_i-a_i)+f_1(a+t(u-a)).\end{align*} (a+t(u-a)) \begin{pmatrix}a_1+t(u_1-a_1)\\\vdots\\a_n+t(u_n-a_n) \end{pmatrix} D_1(a+t(u-a))=D_1\begin{pmatrix}a_1+t(u_1-a_1)\\a_2+t(u_2-a_2)\\ \vdots\\a_n+t(u_n-a_n) \end{pmatrix}=\begin{pmatrix}t\\0\\\vdots\\0 \end{pmatrix} K","['real-analysis', 'calculus', 'derivatives', 'chain-rule']"
2,Product of a derivative and a continuous function is a Darboux function,Product of a derivative and a continuous function is a Darboux function,,"Let $I\subset\mathbb{R}$ be an interval. Suppose that $f(x)$ has an antiderivative on $I$ and $g(x)$ is continuous on $I$ , I have to prove that $f(x)g(x)$ is a Darboux function, having the intermediate value property. ( $f(x)$ is said to have the intermediate value property on $I$ , if for every $a,b\in I$ , $a<b$ , and $r\in [f(a),f(b)]$ (or $[f(b),f(a)]$ ), there exists $c\in [a,b]$ such that $f(c) = r$ .) If $f$ is nonzero everywhere, we actually know that $fg$ has an antiderivative (see here ). The same applies if $f$ is bounded above of below (even just locally) since we can add a constant to $f$ . But in general, $fg$ may not have an antiderivative. An example can be found here . I have no idea what to do with this problem. Any help appreciated.","Let be an interval. Suppose that has an antiderivative on and is continuous on , I have to prove that is a Darboux function, having the intermediate value property. ( is said to have the intermediate value property on , if for every , , and (or ), there exists such that .) If is nonzero everywhere, we actually know that has an antiderivative (see here ). The same applies if is bounded above of below (even just locally) since we can add a constant to . But in general, may not have an antiderivative. An example can be found here . I have no idea what to do with this problem. Any help appreciated.","I\subset\mathbb{R} f(x) I g(x) I f(x)g(x) f(x) I a,b\in I a<b r\in [f(a),f(b)] [f(b),f(a)] c\in [a,b] f(c) = r f fg f f fg","['real-analysis', 'calculus', 'derivatives']"
3,Finding the number of real roots of a polynomial,Finding the number of real roots of a polynomial,,"I want to find the number of real roots of $f(t)=t^4-2t^2+4t+1$ . As $f(0)=1>0, f(-1)=-4 <0$ and $f(-2)=1>0$ , I can say that there are two real roots since the polynomial is continuous. For the rest of the roots (which are complex), I think Rolle's theorem may be used but I could not find a way to show it. How can I proceed?","I want to find the number of real roots of . As and , I can say that there are two real roots since the polynomial is continuous. For the rest of the roots (which are complex), I think Rolle's theorem may be used but I could not find a way to show it. How can I proceed?","f(t)=t^4-2t^2+4t+1 f(0)=1>0, f(-1)=-4 <0 f(-2)=1>0","['calculus', 'derivatives', 'polynomials', 'rolles-theorem']"
4,Cauchy's mean value theorem conditions,Cauchy's mean value theorem conditions,,"Cauchy's mean value theorem conditions I have seen some statements of the Cauchy's mean value theorem that requires g′ is never 0 on (a,b), and some others that require f′ and g′ to never be simultaneously zero. Lemma: If functions f and g are both continuous on the closed interval [a,b], and differentiable on the open interval (a,b) and g′ is never zero on (a,b), then there exists some c∈(a,b), s.t f′(c)/g′(c)=f(b)−f(a)/(g(b)−g(a)). Why is it needed for g′ to never be 0 on all (a,b) , don't we just need g′ not to be zero on that value c ?","Cauchy's mean value theorem conditions I have seen some statements of the Cauchy's mean value theorem that requires g′ is never 0 on (a,b), and some others that require f′ and g′ to never be simultaneously zero. Lemma: If functions f and g are both continuous on the closed interval [a,b], and differentiable on the open interval (a,b) and g′ is never zero on (a,b), then there exists some c∈(a,b), s.t f′(c)/g′(c)=f(b)−f(a)/(g(b)−g(a)). Why is it needed for g′ to never be 0 on all (a,b) , don't we just need g′ not to be zero on that value c ?",,"['calculus', 'derivatives']"
5,Chain Rule Puzzle,Chain Rule Puzzle,,"Here is an old question: If $\boldsymbol{s_r}$ denote the sum of the $\boldsymbol{r}$ th powers of the roots of the equation $$\boldsymbol{x^n+p_1x^{n-1}+\cdots +p_n=0}$$ prove that if the coefficients be expressed in terms of $\boldsymbol{s_r}$ then will $$\boldsymbol{\frac{dp_{r+k}}{ds_r}=-\frac{p_k}{r}}$$ [Brioschi.] If we just consider the case $n=3$ and attempt to verify it for $\frac{dp_3}{ds_2}$ then we have Newton's identities, $$-3p_3=s_1p_2+s_2p_1+s_3$$ $$-2p_2=p_1s_1+s_2$$ $$-p_1=s_1$$ Which give $$\frac{dp_2}{ds_2}=-\frac{1}{2}$$ $$\frac{dp_3}{ds_2}=-\frac{p_1}{2}$$ So this seems to verify the question. However a different solution is possible, using the chain rule and this does not seem to work, $$\frac{dp_3}{ds_2}=\frac{dp_3}{dx_1}\frac{dx_1}{ds_2}+\frac{dp_3}{dx_2}\frac{dx_2}{ds_2}+\frac{dp_3}{dx_2}\frac{dx_3}{ds_2}$$ And taking $$\frac{dx_i}{ds_2}=1/\frac{ds_2}{dx_i}=\frac{1}{2x_i}$$ (this is legitimate right ?) I have the sum $$\frac{dp_3}{ds_2}=\frac{x_2x_3}{2x_1}+\frac{x_1x_3}{2x_2}+\frac{x_2x_2}{2x_3}$$ and this is not equal to the previous answer. What is the error in this second method ?","Here is an old question: If denote the sum of the th powers of the roots of the equation prove that if the coefficients be expressed in terms of then will [Brioschi.] If we just consider the case and attempt to verify it for then we have Newton's identities, Which give So this seems to verify the question. However a different solution is possible, using the chain rule and this does not seem to work, And taking (this is legitimate right ?) I have the sum and this is not equal to the previous answer. What is the error in this second method ?",\boldsymbol{s_r} \boldsymbol{r} \boldsymbol{x^n+p_1x^{n-1}+\cdots +p_n=0} \boldsymbol{s_r} \boldsymbol{\frac{dp_{r+k}}{ds_r}=-\frac{p_k}{r}} n=3 \frac{dp_3}{ds_2} -3p_3=s_1p_2+s_2p_1+s_3 -2p_2=p_1s_1+s_2 -p_1=s_1 \frac{dp_2}{ds_2}=-\frac{1}{2} \frac{dp_3}{ds_2}=-\frac{p_1}{2} \frac{dp_3}{ds_2}=\frac{dp_3}{dx_1}\frac{dx_1}{ds_2}+\frac{dp_3}{dx_2}\frac{dx_2}{ds_2}+\frac{dp_3}{dx_2}\frac{dx_3}{ds_2} \frac{dx_i}{ds_2}=1/\frac{ds_2}{dx_i}=\frac{1}{2x_i} \frac{dp_3}{ds_2}=\frac{x_2x_3}{2x_1}+\frac{x_1x_3}{2x_2}+\frac{x_2x_2}{2x_3},"['calculus', 'derivatives', 'chain-rule']"
6,Checking Differentiability Of g(x) Using Information Of f(x) (unsolved),Checking Differentiability Of g(x) Using Information Of f(x) (unsolved),,"Let $$f(x)=e^{x+1}-1, g(x)=p|f(x)|-\sum_{k=1}^n|f(x^k)|, n \in \mathbb N$$ It is given that $g(x)$ is a differentiable function over the complete real numbers. If value of $p=100$ , then sum of possible values of $n$ is $\dots$ I basically couldn't think of any way to start with this problem, Clearly $|f(x)|$ is not differentiable at $1$ point whereas in even powers ( $f(x^{2n}))$ of $f(x)$ , the function is differentiable at all points. Simply speaking, for odd values of $k$ , function will not be differentiable but for even values of $k$ , the function is differentiable at all points. But I have no clue on how to apply this concept in this question. The series is getting too long with none satisfactory results nor any hint for approaching this problem. EDIT: Can somebody please elaborate Ashish Ahuja's answer more? For checking differentiability at $x=-1$ (as suggested by below answer), how can we proceed? I suppose we can calculate the answer ( $n=19,20$ ) from here, but how? Also have a look at my answer below and kindly spot my mistake please. How can we confirm that the problem only occurs at $x=-1$ and not other numbers? Thank You","Let It is given that is a differentiable function over the complete real numbers. If value of , then sum of possible values of is I basically couldn't think of any way to start with this problem, Clearly is not differentiable at point whereas in even powers ( of , the function is differentiable at all points. Simply speaking, for odd values of , function will not be differentiable but for even values of , the function is differentiable at all points. But I have no clue on how to apply this concept in this question. The series is getting too long with none satisfactory results nor any hint for approaching this problem. EDIT: Can somebody please elaborate Ashish Ahuja's answer more? For checking differentiability at (as suggested by below answer), how can we proceed? I suppose we can calculate the answer ( ) from here, but how? Also have a look at my answer below and kindly spot my mistake please. How can we confirm that the problem only occurs at and not other numbers? Thank You","f(x)=e^{x+1}-1, g(x)=p|f(x)|-\sum_{k=1}^n|f(x^k)|, n \in \mathbb N g(x) p=100 n \dots |f(x)| 1 f(x^{2n})) f(x) k k x=-1 n=19,20 x=-1","['calculus', 'derivatives']"
7,Derivative of $v_{l}(r) = - \frac{1}{2} h(r)^T \left[H +\frac{Mr}{2} I \right]h(r) - \frac{M}{12}r^3$ w.r.t. $r$,Derivative of  w.r.t.,v_{l}(r) = - \frac{1}{2} h(r)^T \left[H +\frac{Mr}{2} I \right]h(r) - \frac{M}{12}r^3 r,"I am reading this paper (Section 5.1) and in some point it is stated that the derivative of $$v_{l}(r) = - \frac{1}{2}g^T \left[H +\frac{Mr}{2} I \right]^{-1}g - \frac{M}{12}r^3 \tag{1}$$ w.r.t. r is $$v_{l}'(r) = \frac{M}{4} (||h(r)||^2-r^2) \tag{2}$$ with $$g = -H h(r) - \frac{M}{2}r h(r). \tag{3}$$ and $$h(r) \stackrel{(3)}{=} -\left[ H+ \frac{Mr}{2}I\right]^{-1}g.\tag{4}$$ Using $(4)$ in $(1)$ , I have tried the following $$\begin{aligned}\partial_{r} v_{l}(r) =& \partial_r\left[-\frac{1}{2} h(r)^T\left(H+\frac{Mr}{2}I \right) h(r) -\frac{M}{12}r^3\right] \\ = & -\frac{1}{2} \left(\partial_{r} \left(h(r)^T H h(r)\right)+\frac{Mr}{2} \partial_{r} ||h(r)||^2\right)-\frac{M}{4}r^2 \\ =& -\frac{1}{2} \left[ h(r)^TH \partial_r h(r)+ \frac{Mr}{2} h(r)^T \partial_r h(r)\right]-\frac{M}{4}r^2 \\ =&  -\frac{1}{2} h(r)^T \left[H + \frac{Mr}{2}I \right]\partial_r h(r)-\frac{M}{4}r^2\end{aligned}$$ but I could not reach $(2)$ . Can you please give some directions? Any help is highly appreciated.","I am reading this paper (Section 5.1) and in some point it is stated that the derivative of w.r.t. r is with and Using in , I have tried the following but I could not reach . Can you please give some directions? Any help is highly appreciated.","v_{l}(r) = - \frac{1}{2}g^T \left[H +\frac{Mr}{2} I \right]^{-1}g - \frac{M}{12}r^3 \tag{1} v_{l}'(r) = \frac{M}{4} (||h(r)||^2-r^2) \tag{2} g = -H h(r) - \frac{M}{2}r h(r). \tag{3} h(r) \stackrel{(3)}{=} -\left[ H+ \frac{Mr}{2}I\right]^{-1}g.\tag{4} (4) (1) \begin{aligned}\partial_{r} v_{l}(r) =& \partial_r\left[-\frac{1}{2} h(r)^T\left(H+\frac{Mr}{2}I \right) h(r) -\frac{M}{12}r^3\right]
\\ = & -\frac{1}{2} \left(\partial_{r} \left(h(r)^T H h(r)\right)+\frac{Mr}{2} \partial_{r} ||h(r)||^2\right)-\frac{M}{4}r^2 \\ =& -\frac{1}{2} \left[ h(r)^TH \partial_r h(r)+ \frac{Mr}{2} h(r)^T \partial_r h(r)\right]-\frac{M}{4}r^2 \\ =&  -\frac{1}{2} h(r)^T \left[H + \frac{Mr}{2}I \right]\partial_r h(r)-\frac{M}{4}r^2\end{aligned} (2)","['calculus', 'linear-algebra', 'derivatives']"
8,Exist Contravariant derivative?,Exist Contravariant derivative?,,"I'm confused about the index representation, If the index up represents a contravariant tensor I can do this with the derivative and get it contravariant derivative?Or when it is said that, it is covariant or contravariant by ""essence""(before the contravariant and covariant formulation like the gradient Which is  transformed covariant). I can't get an image because I don't have a score or whatever then I'll try to write, (Contravariant) $T^{i'}=\dfrac{\partial x^{i'}}{\partial x_j}T^j$ (covariant). $V_{i'}=\dfrac{\partial x^j}{\partial x^{i'}}V_j$ Then $g^{ij}\partial_j=\partial^i$ , I can transformed it contravariant, For it has index above, But the derivative is always covariant, im confused, what is $\partial ^j$ .","I'm confused about the index representation, If the index up represents a contravariant tensor I can do this with the derivative and get it contravariant derivative?Or when it is said that, it is covariant or contravariant by ""essence""(before the contravariant and covariant formulation like the gradient Which is  transformed covariant). I can't get an image because I don't have a score or whatever then I'll try to write, (Contravariant) (covariant). Then , I can transformed it contravariant, For it has index above, But the derivative is always covariant, im confused, what is .",T^{i'}=\dfrac{\partial x^{i'}}{\partial x_j}T^j V_{i'}=\dfrac{\partial x^j}{\partial x^{i'}}V_j g^{ij}\partial_j=\partial^i \partial ^j,"['derivatives', 'differential-geometry', 'definition', 'tensors']"
9,What is the partial differential symbol (bent lowercase $d$) with an $x$ at the top?,What is the partial differential symbol (bent lowercase ) with an  at the top?,d x,"From a government website on global warming: $PT = LT / (1 - s)$ , where the sensitivity coefficient $s = ðln(LT) / ðln(B)$ and $B =$ burden. I have seen this before, yet there is never an explanation as to what it means.....","From a government website on global warming: , where the sensitivity coefficient and burden. I have seen this before, yet there is never an explanation as to what it means.....",PT = LT / (1 - s) s = ðln(LT) / ðln(B) B =,"['calculus', 'derivatives', 'partial-differential-equations', 'computational-science']"
10,Prove that set of increasing polynomials of degree $n$ is a connected subset of the set of all polynomials of degree $n.$,Prove that set of increasing polynomials of degree  is a connected subset of the set of all polynomials of degree,n n.,"Let $X$ be the space of all real polynomials $a_0 + a_1 t + a_2 t^2 + \cdots + a_n t^n$ of degree at most $n.$ We may think of $X$ as a topological space via it's identification with $\Bbb R^{n+1}$ given by $$a_0 + a_1 t + a_2 t^2 + \cdots + a_n t^n \longmapsto (a_0,a_1,a_2, \cdots , a_n).$$ Now consider the space $S$ consisting of all polynomials in $X$ that are increasing (as a function from $\Bbb R$ to $\Bbb R$ ). Is $S$ a connected subset of $X\ $ ? $\textbf{My Thoughts} :$ If $f(t) \in S$ then $f'(t) \gt 0,$ for all $t \in \Bbb R.$ Now for each fixed $t \in \Bbb R$ the equation $f'(t) = 0$ determines a plane passing through the origin in $\Bbb R^{n+1}$ and thus it's complement will have two connected components. How to determine the connected components in terms of $f\ $ ? I think one of them is determined by $f'(t) \gt 0$ and the other one is determined by $f'(t) \lt 0.$ Although I am struggling to prove this. With these things in mind what can we conclude about the connectedness of $S\ $ ? I got stuck at this stage. Any help will be highly solicited. Thanks in advance.",Let be the space of all real polynomials of degree at most We may think of as a topological space via it's identification with given by Now consider the space consisting of all polynomials in that are increasing (as a function from to ). Is a connected subset of ? If then for all Now for each fixed the equation determines a plane passing through the origin in and thus it's complement will have two connected components. How to determine the connected components in terms of ? I think one of them is determined by and the other one is determined by Although I am struggling to prove this. With these things in mind what can we conclude about the connectedness of ? I got stuck at this stage. Any help will be highly solicited. Thanks in advance.,"X a_0 + a_1 t + a_2 t^2 + \cdots + a_n t^n n. X \Bbb R^{n+1} a_0 + a_1 t + a_2 t^2 + \cdots + a_n t^n \longmapsto (a_0,a_1,a_2, \cdots , a_n). S X \Bbb R \Bbb R S X\  \textbf{My Thoughts} : f(t) \in S f'(t) \gt 0, t \in \Bbb R. t \in \Bbb R f'(t) = 0 \Bbb R^{n+1} f\  f'(t) \gt 0 f'(t) \lt 0. S\ ","['general-topology', 'derivatives', 'polynomials', 'connectedness']"
11,Why do these two equivalent equations have different implicit differentiations?,Why do these two equivalent equations have different implicit differentiations?,,"Using implicit differentiation, we have that the derivative of $4x^2y-3/y=0$ is $$\frac{dy}{dx} = -\frac{8xy^3}{4x^2y^2+3}.$$ But, if we multiply both sides of the original equation by y, we have the following equation $4x^2y^2-3=0$ which is seemingly equivalent for $y≠0$ . Taking the derivative of this new function yields $-y/x$ What is causing the difference here. Is it a rule of implicit differentiation or because the $0$ in the original equation that allows us to rewrite it in such a form? I am new to implicit differentiation and any help is appreciated.","Using implicit differentiation, we have that the derivative of is But, if we multiply both sides of the original equation by y, we have the following equation which is seemingly equivalent for . Taking the derivative of this new function yields What is causing the difference here. Is it a rule of implicit differentiation or because the in the original equation that allows us to rewrite it in such a form? I am new to implicit differentiation and any help is appreciated.",4x^2y-3/y=0 \frac{dy}{dx} = -\frac{8xy^3}{4x^2y^2+3}. 4x^2y^2-3=0 y≠0 -y/x 0,"['calculus', 'derivatives', 'implicit-differentiation']"
12,Why don't introductory calculus textbooks usually introduce the notion of differentiability on closed intervals?,Why don't introductory calculus textbooks usually introduce the notion of differentiability on closed intervals?,,"The title says it all. I find the apparent neglect of this idea rather unfortunate because (1) the notion of a function being differentiable on a closed interval is intuitively reasonable (and ought to be discussed), and (2) several theorems from calculus can be strengthened if they're stated in terms of closed interval differentiability. Before proceeding further, it's worth taking time to make sure we agree on what we mean we say that a function is ""differentiable over a closed interval"". Here's the definition I had in mind: A function $f:[a,b]\to\mathbb{R}$ is differentiable over $[a,b]$ if it is differentiable (in the ordinary sense) over $(a,b)$ , right-differentiable at $a$ , and left-differentiable at $b$ . As you can probably guess, a function $f$ is said to be left-differentiable at a number $x_0$ if the limit $$\lim_{h\to 0^-}\frac{f(x_0+h)-f(x_0)}{h}$$ exists, with a similar definition applying to right-differentiability. The limits are then defined to be the left-hand and right-hand derivatives of $f$ , respectively. With that said, here's an example of a theorem (FTC1) that, under its usual hypotheses, can be strengthened (albeit slightly) if the notion of closed-interval differentiability is used: If $f:[a,b]\to\mathbb{R}$ is continuous, then the function $F:[a,b]\to\mathbb{R}$ defined by $F(x)=\int_{a}^{x}f(t)\text{ }dt$ is differentiable over $[a,b]$ . Moreover, for all $x\in[a,b]$ , $F'(x)=f(x)$ (at the endpoints, $F'(x)$ is understood to be a left or right-hand derivative) The return you get by applying this notion to this example is obviously minimal. That said, I truly believe that making these extra definitions is worthwhile anyway, namely because the resulting concepts strongly align with our intuitions. To bring everything back together, I'll restate my question one last time: why don't introductory calculus textbooks usually introduce the notion of differentiability on closed intervals? Any and all responses are greatly appreciated. Note : the ""theorems"" I mentioned earlier do not include existence theorems like Rolle's theorem or the MVT. I understand why these only require differentiability in the interior and why we shouldn't strengthen that requirement.","The title says it all. I find the apparent neglect of this idea rather unfortunate because (1) the notion of a function being differentiable on a closed interval is intuitively reasonable (and ought to be discussed), and (2) several theorems from calculus can be strengthened if they're stated in terms of closed interval differentiability. Before proceeding further, it's worth taking time to make sure we agree on what we mean we say that a function is ""differentiable over a closed interval"". Here's the definition I had in mind: A function is differentiable over if it is differentiable (in the ordinary sense) over , right-differentiable at , and left-differentiable at . As you can probably guess, a function is said to be left-differentiable at a number if the limit exists, with a similar definition applying to right-differentiability. The limits are then defined to be the left-hand and right-hand derivatives of , respectively. With that said, here's an example of a theorem (FTC1) that, under its usual hypotheses, can be strengthened (albeit slightly) if the notion of closed-interval differentiability is used: If is continuous, then the function defined by is differentiable over . Moreover, for all , (at the endpoints, is understood to be a left or right-hand derivative) The return you get by applying this notion to this example is obviously minimal. That said, I truly believe that making these extra definitions is worthwhile anyway, namely because the resulting concepts strongly align with our intuitions. To bring everything back together, I'll restate my question one last time: why don't introductory calculus textbooks usually introduce the notion of differentiability on closed intervals? Any and all responses are greatly appreciated. Note : the ""theorems"" I mentioned earlier do not include existence theorems like Rolle's theorem or the MVT. I understand why these only require differentiability in the interior and why we shouldn't strengthen that requirement.","f:[a,b]\to\mathbb{R} [a,b] (a,b) a b f x_0 \lim_{h\to 0^-}\frac{f(x_0+h)-f(x_0)}{h} f f:[a,b]\to\mathbb{R} F:[a,b]\to\mathbb{R} F(x)=\int_{a}^{x}f(t)\text{ }dt [a,b] x\in[a,b] F'(x)=f(x) F'(x)","['calculus', 'derivatives', 'soft-question']"
13,Finding a function that satisfies this property.,Finding a function that satisfies this property.,,"Suppose that $f\in L^2([0,1],\mathbb{C})$ , that $0 < \alpha < 1$ and that $$(T_{\alpha}f)(x) = \int_0^{x^{\alpha}}f(y)dy$$ is continuous (which is actually something you can prove) I need to find an $f\not = 0$ a.e. such that \begin{equation}(1) \mbox{     ....... } \int_0^{x^{\alpha}}f(y)dy = (1-\alpha) f(x) .\end{equation} In tecnical terms I want to show $(1-\alpha)$ is an eigenvalue for the operator $T_{\alpha}$ by showing there is an associated eigenvector. I could only observe that since the left hand side in (1) is continuous, then $f$ (on the right) is too, so that going back on the left we have $f$ is $C^1$ ,... and so on; so I think we can deduce that $f\in C^\infty$ , but also deriving (1) I could not get anywhere. Would you be able to give me some hints or a way to find such a function? Thanks in advance","Suppose that , that and that is continuous (which is actually something you can prove) I need to find an a.e. such that In tecnical terms I want to show is an eigenvalue for the operator by showing there is an associated eigenvector. I could only observe that since the left hand side in (1) is continuous, then (on the right) is too, so that going back on the left we have is ,... and so on; so I think we can deduce that , but also deriving (1) I could not get anywhere. Would you be able to give me some hints or a way to find such a function? Thanks in advance","f\in L^2([0,1],\mathbb{C}) 0 < \alpha < 1 (T_{\alpha}f)(x) = \int_0^{x^{\alpha}}f(y)dy f\not = 0 \begin{equation}(1) \mbox{     ....... } \int_0^{x^{\alpha}}f(y)dy = (1-\alpha) f(x) .\end{equation} (1-\alpha) T_{\alpha} f f C^1 f\in C^\infty","['real-analysis', 'integration', 'derivatives', 'eigenvalues-eigenvectors', 'hilbert-spaces']"
14,"$(f_n)$ sequence of differentiable functions on $[0,1]$ and converge pointwise to $0$.",sequence of differentiable functions on  and converge pointwise to .,"(f_n) [0,1] 0","Let $(f_n)$ sequence of differentiable functions on $[0,1]$ converging pointwise to $0$ . Suppose $|f'_n(x)| \leqslant 2015 + \cos(x)$ $\forall x \in [0,1]$ and $\forall n$ . Show that $(f_n)$ converge uniformly to $0$ . ''Proof'' As supposed, $|f'_n(x)| \leqslant 2015 + \cos(x)$ . We can rewrite it: $|f'_n(x)| \leqslant 2015 + \cos(x) \leqslant 2016$ as $\cos(x)\leqslant1$ Since $(f_n)$ is a sequence of differentiable functions and the derivative is bounded, $(f_n)$ is Lipschitz. Hence, $\forall x,y \in [0,1]$ : $|f_n(x)-f_n(y)| \leqslant 2016|x-y|=\epsilon/3$ Moreover, as $(f_n)$ is Lipschitz, it implies that $(f_n)$ is uniformly continuous on $[0,1]$ : $\forall \epsilon>0$ $\exists \delta>0$ such that $\forall x,y \in [0,1]:$ $|x-y|<\delta \Rightarrow |f_n(x)-f_n(y)| \leqslant \epsilon/3$ And we want to show that $\forall \epsilon >0 \exists$ $n_0$ such that $\forall n\geqslant n_0 \forall x \in [0,1]$ : $|f_n(x)|<\epsilon$ . Then i don't really see how to apply the pointwise convergence(i.e create kind a subdivision on $[0,1]$ or?) and conclude the proof using triangular inequality. P.S Im stydying Analysis I right now so if you could explain with much details as possible it would be kind from your part. Thanks in advance ;)","Let sequence of differentiable functions on converging pointwise to . Suppose and . Show that converge uniformly to . ''Proof'' As supposed, . We can rewrite it: as Since is a sequence of differentiable functions and the derivative is bounded, is Lipschitz. Hence, : Moreover, as is Lipschitz, it implies that is uniformly continuous on : such that And we want to show that such that : . Then i don't really see how to apply the pointwise convergence(i.e create kind a subdivision on or?) and conclude the proof using triangular inequality. P.S Im stydying Analysis I right now so if you could explain with much details as possible it would be kind from your part. Thanks in advance ;)","(f_n) [0,1] 0 |f'_n(x)| \leqslant 2015 + \cos(x) \forall x \in [0,1] \forall n (f_n) 0 |f'_n(x)| \leqslant 2015 + \cos(x) |f'_n(x)| \leqslant 2015 + \cos(x) \leqslant 2016 \cos(x)\leqslant1 (f_n) (f_n) \forall x,y \in [0,1] |f_n(x)-f_n(y)| \leqslant 2016|x-y|=\epsilon/3 (f_n) (f_n) [0,1] \forall \epsilon>0 \exists \delta>0 \forall x,y \in [0,1]: |x-y|<\delta \Rightarrow |f_n(x)-f_n(y)| \leqslant \epsilon/3 \forall \epsilon >0 \exists n_0 \forall n\geqslant n_0 \forall x \in [0,1] |f_n(x)|<\epsilon [0,1]","['real-analysis', 'derivatives', 'uniform-convergence', 'lipschitz-functions', 'pointwise-convergence']"
15,"If $f$ is continuous at $c$ and $f ′(c) = 0$, then there exists an $h > 0$ such that $f$ is differentiable in the interval $(c – h, c + h)$.","If  is continuous at  and , then there exists an  such that  is differentiable in the interval .","f c f ′(c) = 0 h > 0 f (c – h, c + h)","My book states the following: if $f$ is continuous at $c$ and $f ′(c) = 0$ , then there exists an $h > 0$ such that $f$ is differentiable in the interval $(c – h, c + h)$ . But I don't understand this. It is not as if $f'$ is given to be continuous, rather $f$ is given continuous and differentiable at $x=c$ . So how can we possibly comment about the existence of $f'$ in the neighborhood of $c$ too? Note that f is defined on an open interval I and c belongs to I.","My book states the following: if is continuous at and , then there exists an such that is differentiable in the interval . But I don't understand this. It is not as if is given to be continuous, rather is given continuous and differentiable at . So how can we possibly comment about the existence of in the neighborhood of too? Note that f is defined on an open interval I and c belongs to I.","f c f ′(c) = 0 h > 0 f (c – h, c + h) f' f x=c f' c","['calculus', 'derivatives', 'maxima-minima']"
16,Finding the derivative- is my answer correct? Is the method correct?,Finding the derivative- is my answer correct? Is the method correct?,,"Question - find the derivative of $y$ with respect to $z$ $$y=  2^{z^3} $$ So what I’d do is Differentiate boths sides of the equation so $$\frac{dy}{dz}= \frac{d(2^{z^3 })}{dz}$$ Then I applied the chain rule by assuming, $$ u=z^3$$ which resulted in $$ 2^{z^3}\ln2\frac{dz^3}{dz}$$ Using the power rule I finally got the answer to be $$ 3z^2\cdot2^{z^3}\ln2$$ Is my answer correct?","Question - find the derivative of with respect to So what I’d do is Differentiate boths sides of the equation so Then I applied the chain rule by assuming, which resulted in Using the power rule I finally got the answer to be Is my answer correct?",y z y=  2^{z^3}  \frac{dy}{dz}= \frac{d(2^{z^3 })}{dz}  u=z^3  2^{z^3}\ln2\frac{dz^3}{dz}  3z^2\cdot2^{z^3}\ln2,"['calculus', 'derivatives']"
17,Show that $\Gamma\left(3^{-3}+1\right)<\Gamma \left(\operatorname{W}(3)^{-\operatorname{W}(3)}+1\right)$,Show that,\Gamma\left(3^{-3}+1\right)<\Gamma \left(\operatorname{W}(3)^{-\operatorname{W}(3)}+1\right),"Hi it's one of my old problem that I cannot solve let me propose it  : Prove that $$\Gamma\left(3^{-3}+1\right)<\Gamma \left(\operatorname{W}(3)^{-\operatorname{W}(3)}+1\right)\quad \tag{1}$$ where $W(\cdot)$ is the Lambert W function and $\Gamma(\cdot)$ is the gamma function. The central function is : $$f(x)=\Gamma\left(x^{-x}+1\right)$$ The derivative is : $$f'(x)= -x^{-x}(\ln(x)+1)\Gamma\left(x^{-x}+1\right)\psi(x^{-x}+1)$$ Unfortunately it doesn't helps since $f(x)$ is increasing around $x=3$ while $f\left(\operatorname{W}(x)\right)$ is decreasing around $x=3$ . Maybe I think we can do the same as the Claude Leibovici's answer  here Show that $\Gamma(\Omega)\leq \Gamma\Big(\operatorname{W}\Big(x^{x}\Big)\Big)<2$ on $(0,1]$ . My question : How to show $(1)$ ? Thanks in advance !","Hi it's one of my old problem that I cannot solve let me propose it  : Prove that where is the Lambert W function and is the gamma function. The central function is : The derivative is : Unfortunately it doesn't helps since is increasing around while is decreasing around . Maybe I think we can do the same as the Claude Leibovici's answer  here Show that $\Gamma(\Omega)\leq \Gamma\Big(\operatorname{W}\Big(x^{x}\Big)\Big)<2$ on $(0,1]$ . My question : How to show ? Thanks in advance !",\Gamma\left(3^{-3}+1\right)<\Gamma \left(\operatorname{W}(3)^{-\operatorname{W}(3)}+1\right)\quad \tag{1} W(\cdot) \Gamma(\cdot) f(x)=\Gamma\left(x^{-x}+1\right) f'(x)= -x^{-x}(\ln(x)+1)\Gamma\left(x^{-x}+1\right)\psi(x^{-x}+1) f(x) x=3 f\left(\operatorname{W}(x)\right) x=3 (1),"['derivatives', 'inequality', 'approximation', 'gamma-function', 'lambert-w']"
18,"If $a^2x^4+b^2y^4=c^6$, then the maximum value of $xy$","If , then the maximum value of",a^2x^4+b^2y^4=c^6 xy,"Simply subsisting the value of $y$ in $xy$ will not work, but I don’t know any other method to solve it. Can I get a hint?","Simply subsisting the value of in will not work, but I don’t know any other method to solve it. Can I get a hint?",y xy,"['derivatives', 'maxima-minima', 'absolute-value', 'a.m.-g.m.-inequality']"
19,Let $f$ be a differentiable function with no point $x$ such that $f(x)=0=f'(x)$ show that $f$ has finitely many zeros.,Let  be a differentiable function with no point  such that  show that  has finitely many zeros.,f x f(x)=0=f'(x) f,"Let $f: [0, 1] \rightarrow R$ be a differentiable function. Assume there is no point $x$ in $[0,1]$ such that $f(x) = 0 = f'(x)$ . Show that $f$ has only a finite number of zeros in $[0, 1]$ . My proof. Assume otherwise. Keep bisecting the interval choosing the subinterval with infinitely many zeros. (This is fairly. standard so I won't go into it). We obtain $(x_n)$ such that $f(x_n)=0$ for all $n$ . Moreover, $x_n\rightarrow x$ as $n\rightarrow \infty$ . We see immediately that $f(x)=0$ . Our goal is to show $f'(x)=0$ as well. We know, $$ \lim_{h\rightarrow 0}\dfrac{f(x+h)}{h}=L $$ There's a subsequence $x_{n_k}$ , $h_k = x_{n_k}-x\ge 0$ , (if not we will use $x-x_{n_k}$ and the proof will be similar) and we observe for every $h$ , there's $N$ such that if $k\ge N$ , $h_k=x_{n_k}-x\le h$ . Thus we observe, $$ \lim_{k\rightarrow \infty }\dfrac{f(x+h_k)}{h_k}=L=0 $$ The last part is due to the fact $f(x+h_k)=0$ . Contradiction! I am only looking for proof verification. $\textbf{Please only provide hints if my proof is wrong. Complete solutions won't benefit me at all!}$","Let be a differentiable function. Assume there is no point in such that . Show that has only a finite number of zeros in . My proof. Assume otherwise. Keep bisecting the interval choosing the subinterval with infinitely many zeros. (This is fairly. standard so I won't go into it). We obtain such that for all . Moreover, as . We see immediately that . Our goal is to show as well. We know, There's a subsequence , , (if not we will use and the proof will be similar) and we observe for every , there's such that if , . Thus we observe, The last part is due to the fact . Contradiction! I am only looking for proof verification.","f: [0, 1] \rightarrow R x [0,1] f(x) = 0 = f'(x) f [0, 1] (x_n) f(x_n)=0 n x_n\rightarrow x n\rightarrow \infty f(x)=0 f'(x)=0 
\lim_{h\rightarrow 0}\dfrac{f(x+h)}{h}=L
 x_{n_k} h_k = x_{n_k}-x\ge 0 x-x_{n_k} h N k\ge N h_k=x_{n_k}-x\le h 
\lim_{k\rightarrow \infty }\dfrac{f(x+h_k)}{h_k}=L=0
 f(x+h_k)=0 \textbf{Please only provide hints if my proof is wrong. Complete solutions won't benefit me at all!}",['derivatives']
20,Differentiability of compositions of non differentiable functions at a point.,Differentiability of compositions of non differentiable functions at a point.,,"If $f(x)$ and $g(x)$ are differentiable functions at all points in $[a,b]$ except a one point (c) in the interval and continuous everywhere on $[a,b]$ . Then it is guaranteed that $f(g(x))$ is differentiable wherever $f$ and $g$ are differentiable but is there an example for which $f(g(x))$ is also differentiable at c . Or can it be proven that something like this can’t happen? It is true that if they are nowhere differentiable then it is impossible to create an example but is it possible to create an example for such a case? What about 2 points, 3 points , infinitely many?","If and are differentiable functions at all points in except a one point (c) in the interval and continuous everywhere on . Then it is guaranteed that is differentiable wherever and are differentiable but is there an example for which is also differentiable at c . Or can it be proven that something like this can’t happen? It is true that if they are nowhere differentiable then it is impossible to create an example but is it possible to create an example for such a case? What about 2 points, 3 points , infinitely many?","f(x) g(x) [a,b] [a,b] f(g(x)) f g f(g(x))","['real-analysis', 'calculus', 'derivatives']"
21,Calculus: derivative of logarithm with respect to logarithm,Calculus: derivative of logarithm with respect to logarithm,,"I have this expression: $$ \dfrac{d\ln\left(\dfrac{x_2}{x_1}\right)}{d\ln(\theta)} $$ That I’m hoping to get some help solving, Where $\ln\left(\dfrac{x_2}{x_1}\right)= \ln\left(\dfrac{1-a}{a}\right)+\ln(\theta)$ and $\theta= \dfrac{a}{1-a} \cdot \dfrac{x_2}{x_1}$ My confusion stems from having a derivative with ln in both the numerator and denominator and I’m not sure how to correctly proceed. I know the answer is $1$ , however, I’m more interested in knowing the technique to get there. Any help would be appreciated","I have this expression: That I’m hoping to get some help solving, Where and My confusion stems from having a derivative with ln in both the numerator and denominator and I’m not sure how to correctly proceed. I know the answer is , however, I’m more interested in knowing the technique to get there. Any help would be appreciated", \dfrac{d\ln\left(\dfrac{x_2}{x_1}\right)}{d\ln(\theta)}  \ln\left(\dfrac{x_2}{x_1}\right)= \ln\left(\dfrac{1-a}{a}\right)+\ln(\theta) \theta= \dfrac{a}{1-a} \cdot \dfrac{x_2}{x_1} 1,"['calculus', 'derivatives']"
22,Function with the property $f(a+b)=f(a)+f(b)+2ab$,Function with the property,f(a+b)=f(a)+f(b)+2ab,"Let $f: \mathbb{R} \to \mathbb{R}$ and let's define $f$ such that the following property holds $$f(a+b)=f(a)+f(b)+2ab \text{ for all } a,b.$$ Also let the function be differentiable at $0$ and $f'(0) = 3.$ Show that the function is differentiable everywhere and determine the derivative $f'(x)$ . Since $f(0) = 0$ and we have that $\lim_{h\to0} \frac{f(h)}{h} = 3$ . So $$\frac{f(x+h)-f(h)}{h} = \frac{f(x)+f(h)+2xh}{h} = \frac{f(h)}{h}+2xh = 3+2xh$$ so it's differentiable and the derivative is $f'(x)=3+2xh$ ? I feel like i should have gotten rid of the $h$ ?",Let and let's define such that the following property holds Also let the function be differentiable at and Show that the function is differentiable everywhere and determine the derivative . Since and we have that . So so it's differentiable and the derivative is ? I feel like i should have gotten rid of the ?,"f: \mathbb{R} \to \mathbb{R} f f(a+b)=f(a)+f(b)+2ab \text{ for all } a,b. 0 f'(0) = 3. f'(x) f(0) = 0 \lim_{h\to0} \frac{f(h)}{h} = 3 \frac{f(x+h)-f(h)}{h} = \frac{f(x)+f(h)+2xh}{h} = \frac{f(h)}{h}+2xh = 3+2xh f'(x)=3+2xh h","['real-analysis', 'calculus']"
23,"$\forall x,y \in \mathbb{R^n}: x,y \in U => \left\lVert f(x) - f(y) \right\rVert \geq c \left\lVert x - y \right\rVert$ globally invertible",globally invertible,"\forall x,y \in \mathbb{R^n}: x,y \in U => \left\lVert f(x) - f(y) \right\rVert \geq c \left\lVert x - y \right\rVert","Let $f:U \subset \mathbb{R^n} \to \mathbb{R}^n$ be totally differentiable and there exists a constant $c > 0$ , so that $$\forall x,y \in \mathbb{R^n}: x,y \in U => \left\lVert f(x) - f(y) \right\rVert \geq c \left\lVert x - y \right\rVert$$ Prove that $f:U \to f(U)$ is globally invertible. Choose an random but constant $x$ or $y$ in $U$ . Rewrite $f(x) - f(y)$ and use the fact that a linear function of $\mathbb{R^n}$ to $\mathbb{R}^n$ is injective iff only the zero vector maps to the zero vector. I know that a function $f$ is globally invertible if $f$ is bijective. This must imply that one has to prove that the function is injective and surjective. This requires the inverse function and my guess is that the implicit function theorem can prove what is asked for above, but I don't know how to apply the theorem in this case. Can someone show how it's done?","Let be totally differentiable and there exists a constant , so that Prove that is globally invertible. Choose an random but constant or in . Rewrite and use the fact that a linear function of to is injective iff only the zero vector maps to the zero vector. I know that a function is globally invertible if is bijective. This must imply that one has to prove that the function is injective and surjective. This requires the inverse function and my guess is that the implicit function theorem can prove what is asked for above, but I don't know how to apply the theorem in this case. Can someone show how it's done?","f:U \subset \mathbb{R^n} \to \mathbb{R}^n c > 0 \forall x,y \in \mathbb{R^n}: x,y \in U => \left\lVert f(x) - f(y) \right\rVert \geq c \left\lVert x - y \right\rVert f:U \to f(U) x y U f(x) - f(y) \mathbb{R^n} \mathbb{R}^n f f","['calculus', 'derivatives', 'proof-writing', 'inverse-function', 'implicit-function-theorem']"
24,Find where $f(x)=\frac{x^2+4}{x^2-4}$ is concave upwards and concave downwards,Find where  is concave upwards and concave downwards,f(x)=\frac{x^2+4}{x^2-4},"Find where $f(x)=\frac{x^2+4}{x^2-4}$ is concave upwards and concave downwards Solution: We are going to take the second derivative, find the critical points, and then test each region. Lets get to the second derivative by doing the quotient rule twice: $f'(x) = \frac{(x^2-4)\frac{d}{dx}(x^2+4)-(x^2+4)\frac{d}{dx}(x^2-4)}{(x^2-4)^2}$ $=\frac{(x^2-4)(2x)-(x^2+4)(2x)}{(x^2-4)^2}$ $=\frac{-16x}{(x^2-4)^2}$ Okay, theres the first derivative. Let's do it again!! $f''(x)=\frac{(x^2-4)^2 \frac{d}{dx}(-16x) - (-16x)\frac{d}{dx}(x^2-4)^2}{(x^2-4)^4}$ Note that we have to the chain rule to evluate $\frac{d}{dx}(x^2-4)^2$ . $\rightarrow f''(x)=\frac{(x^2-4)^2 (-16) - (-16x)(2(x^2-4)\frac{d}{dx}(x^2-4))}{(x^2-4)^4}$ $\rightarrow =\frac{(x^2-4)^2 (-16) - (-16x)(2(x^2-4)(2x)}{(x^2-4)^4}$ $\rightarrow =\frac{(x^2-4)^2 (-16) + 64x^2(x^2-4)}{(x^2-4)^4}$ Canceling off one of the $(x^2-4)$ from each term in the numerator and the denominator... $\rightarrow =\frac{(x^2-4) (-16) + 64x^2}{(x^2-4)^3}$ $\rightarrow =\frac{-16x^2+64+64x^2}{(x^2-4)^3}$ $\rightarrow =\frac{64+48x^2}{(x^2-4)^3}$ Few.. okay the hard part is over. Now we need to find the points to chop our number line up from, so we have to solve $f''(x)=0$ and find where $f''(x)$ DNE. Let's solve where $f''(x)=0$ $f''(x) =\frac{64+48x^2}{(x^2-4)^3}=0$ $\rightarrow 64+48x^2 = 0$ $\rightarrow x= \pm \sqrt{-64/48}$ So $f''(x)=0$ only when $x$ is imaginary... so those values are irrelevant. On the other hand, $f''(x)$ DNE when $x = \pm 2$ , because the denominator will be zero. So these are the values we split our number line up by. Since $f''(-3)=\frac{496}{125}>0$ , we have that $f(x)$ is concave up on $(-\infty,-2)$ . Since $f''(0)=-1<0$ , we have that $f(x)$ is concave down on $(-2,2)$ . Since $f''(3)=\frac{496}{125}>0$ , we have that $f(x)$ is concave up on $(2,\infty)$ .","Find where is concave upwards and concave downwards Solution: We are going to take the second derivative, find the critical points, and then test each region. Lets get to the second derivative by doing the quotient rule twice: Okay, theres the first derivative. Let's do it again!! Note that we have to the chain rule to evluate . Canceling off one of the from each term in the numerator and the denominator... Few.. okay the hard part is over. Now we need to find the points to chop our number line up from, so we have to solve and find where DNE. Let's solve where So only when is imaginary... so those values are irrelevant. On the other hand, DNE when , because the denominator will be zero. So these are the values we split our number line up by. Since , we have that is concave up on . Since , we have that is concave down on . Since , we have that is concave up on .","f(x)=\frac{x^2+4}{x^2-4} f'(x) = \frac{(x^2-4)\frac{d}{dx}(x^2+4)-(x^2+4)\frac{d}{dx}(x^2-4)}{(x^2-4)^2} =\frac{(x^2-4)(2x)-(x^2+4)(2x)}{(x^2-4)^2} =\frac{-16x}{(x^2-4)^2} f''(x)=\frac{(x^2-4)^2 \frac{d}{dx}(-16x) - (-16x)\frac{d}{dx}(x^2-4)^2}{(x^2-4)^4} \frac{d}{dx}(x^2-4)^2 \rightarrow f''(x)=\frac{(x^2-4)^2 (-16) - (-16x)(2(x^2-4)\frac{d}{dx}(x^2-4))}{(x^2-4)^4} \rightarrow =\frac{(x^2-4)^2 (-16) - (-16x)(2(x^2-4)(2x)}{(x^2-4)^4} \rightarrow =\frac{(x^2-4)^2 (-16) + 64x^2(x^2-4)}{(x^2-4)^4} (x^2-4) \rightarrow =\frac{(x^2-4) (-16) + 64x^2}{(x^2-4)^3} \rightarrow =\frac{-16x^2+64+64x^2}{(x^2-4)^3} \rightarrow =\frac{64+48x^2}{(x^2-4)^3} f''(x)=0 f''(x) f''(x)=0 f''(x) =\frac{64+48x^2}{(x^2-4)^3}=0 \rightarrow 64+48x^2 = 0 \rightarrow x= \pm \sqrt{-64/48} f''(x)=0 x f''(x) x = \pm 2 f''(-3)=\frac{496}{125}>0 f(x) (-\infty,-2) f''(0)=-1<0 f(x) (-2,2) f''(3)=\frac{496}{125}>0 f(x) (2,\infty)",['calculus']
25,Two definitions of the half-derivative of $e^x$ don't match. Who's right?,Two definitions of the half-derivative of  don't match. Who's right?,e^x,"I find two main sources on how to compute the half-derivative of $e^x$ . Both make sense to me, but they give different answers. Firstly, people argue, that $$\begin{align} \frac{\mathrm{d}}{\mathrm{d} x} e^{k x} &= k e^{k x} \\[4pt] \frac{\mathrm{d}^2}{\mathrm{d} x^2} e^{k x} &= k^2 e^{k x} \\[4pt] \frac{\mathrm{d}^n}{\mathrm{d} x^n} e^{k x} &= k^n e^{k x} \end{align}$$ Therefore, it seems very reasonable, that $$\frac{\mathrm{d}^{1/2}}{\mathrm{d} x^{1/2}} e^{k x} = \sqrt{k} e^{k x}$$ But this is not what the usual formula gives $$ \frac{\mathrm{d}^{1/2}}{\mathrm{d} x^{1/2}} e^{k x} = \frac{1}{\Gamma (1/2)} \frac{\mathrm{d}}{\mathrm{d} x} \int \limits_0^x \mathrm{d} t \frac{e^{k t}}{\sqrt{x-t}} = \frac{1}{\Gamma (1/2)} \frac{\mathrm{d}}{\mathrm{d} x} e^{k x} \int \limits_0^x \mathrm{d} u \frac{e^{- k u}}{\sqrt{u}} = \\ = \frac{1}{\Gamma (1/2)} \frac{\mathrm{d}}{\mathrm{d} x} \frac{e^{k x}}{\sqrt{k}} \int \limits_0^{\sqrt{k x}} \mathrm{d} s \, e^{-s^2} $$ We can already see that this is not equal to $\sqrt{k} e^{k x}$ . So who is right? Why do we use the latter formula in almost all cases but somehow we settle for the simpler formula when it comes to the exponential? Note that both satisfy that if we apply the half-derivative twice, we get the usual first derivative. (In the first case, it's simply because $\sqrt{k} \sqrt{k} = k$ ; in the second case, there's a proof on Wiki using the properties of beta and gamma functions - plus I verified this numerically even though I couldn't express the integrals in a closed-form.) I also have hard time accepting this second, complicated, formula, mainly because any integer derivative of the exponential gives exponential, but for the half-derivative we get this weird monstrosity. On the other hand, it should be consistent with the formulas for the half-derivative for all powers of $x$ when it's put together in the infinite series for $e^{k x}$ . Can anyone shed some light on this issue for me, please?","I find two main sources on how to compute the half-derivative of . Both make sense to me, but they give different answers. Firstly, people argue, that Therefore, it seems very reasonable, that But this is not what the usual formula gives We can already see that this is not equal to . So who is right? Why do we use the latter formula in almost all cases but somehow we settle for the simpler formula when it comes to the exponential? Note that both satisfy that if we apply the half-derivative twice, we get the usual first derivative. (In the first case, it's simply because ; in the second case, there's a proof on Wiki using the properties of beta and gamma functions - plus I verified this numerically even though I couldn't express the integrals in a closed-form.) I also have hard time accepting this second, complicated, formula, mainly because any integer derivative of the exponential gives exponential, but for the half-derivative we get this weird monstrosity. On the other hand, it should be consistent with the formulas for the half-derivative for all powers of when it's put together in the infinite series for . Can anyone shed some light on this issue for me, please?","e^x \begin{align}
\frac{\mathrm{d}}{\mathrm{d} x} e^{k x} &= k e^{k x} \\[4pt]
\frac{\mathrm{d}^2}{\mathrm{d} x^2} e^{k x} &= k^2 e^{k x} \\[4pt]
\frac{\mathrm{d}^n}{\mathrm{d} x^n} e^{k x} &= k^n e^{k x}
\end{align} \frac{\mathrm{d}^{1/2}}{\mathrm{d} x^{1/2}} e^{k x} = \sqrt{k} e^{k x} 
\frac{\mathrm{d}^{1/2}}{\mathrm{d} x^{1/2}} e^{k x} = \frac{1}{\Gamma (1/2)} \frac{\mathrm{d}}{\mathrm{d} x} \int \limits_0^x \mathrm{d} t \frac{e^{k t}}{\sqrt{x-t}} = \frac{1}{\Gamma (1/2)} \frac{\mathrm{d}}{\mathrm{d} x} e^{k x} \int \limits_0^x \mathrm{d} u \frac{e^{- k u}}{\sqrt{u}} = \\
= \frac{1}{\Gamma (1/2)} \frac{\mathrm{d}}{\mathrm{d} x} \frac{e^{k x}}{\sqrt{k}} \int \limits_0^{\sqrt{k x}} \mathrm{d} s \, e^{-s^2}
 \sqrt{k} e^{k x} \sqrt{k} \sqrt{k} = k x e^{k x}","['derivatives', 'fractional-calculus']"
26,Partial derivatives: Why do results vary?,Partial derivatives: Why do results vary?,,"When I attempt to compute $f_{y}(0,0)$ , I first set $x = 0$ such that $f(0,y) = \frac{y^2}{y^2} = 1$ , and so $f_{y}(0,y) = 0$ . So its passes differentiability w.r.t.y near $(0,0,f(0,0))$ . However, if I compute this exact partial derivative using the definition of differentiation: $\lim_{h \to 0}\frac{f(0+h)-f(0)}{h}$ , I end up obtaining $\lim_{h \to 0}\frac{1}{h}$ which reveals that $f_{y}(0,0)$ does not exist. How can such contradicting result be explained?","When I attempt to compute , I first set such that , and so . So its passes differentiability w.r.t.y near . However, if I compute this exact partial derivative using the definition of differentiation: , I end up obtaining which reveals that does not exist. How can such contradicting result be explained?","f_{y}(0,0) x = 0 f(0,y) = \frac{y^2}{y^2} = 1 f_{y}(0,y) = 0 (0,0,f(0,0)) \lim_{h \to 0}\frac{f(0+h)-f(0)}{h} \lim_{h \to 0}\frac{1}{h} f_{y}(0,0)","['derivatives', 'partial-derivative']"
27,Is there a partial exterior derivative?,Is there a partial exterior derivative?,,The exterior derivative is a map $\text d:\Omega^k\rightarrow\Omega^{k+1}$ . We can divide two differential forms to get the ordinary derivative: $$\dfrac {\text df}{\text dx}=D_xf.$$ Is there an analogous map for partial derivatives? A map $\partial$ such that $$\dfrac {\partial f}{\partial x}=\partial_xf?$$,The exterior derivative is a map . We can divide two differential forms to get the ordinary derivative: Is there an analogous map for partial derivatives? A map such that,\text d:\Omega^k\rightarrow\Omega^{k+1} \dfrac {\text df}{\text dx}=D_xf. \partial \dfrac {\partial f}{\partial x}=\partial_xf?,"['derivatives', 'partial-derivative', 'differential-forms', 'exterior-algebra', 'exterior-derivative']"
28,"If $\lim\limits_{h\to 0^+}\frac{f(x+h)-f(x)}{h}=0$ for all $x\in I$, then $f$ is constant.","If  for all , then  is constant.",\lim\limits_{h\to 0^+}\frac{f(x+h)-f(x)}{h}=0 x\in I f,"So suppose $I\subset \mathbb{R}$ is an open interval and $f: I\to \mathbb{R}$ a continuous function such that $$\lim\limits_{h\to 0^+}\frac{f(x+h)-f(x)}{h}=0$$ for all $x\in I$ . Show that $f$ is constant. My ideas: Proof by contraposition. Suppose $f$ is not constant; thus there are $a,b\in I$ with $a<b$ such that $f(a)\neq f(b)$ . I am given the hint that I should check the $\sup$ of $$M:=\left \{ x\in [a,b] : \frac{f(x)-f(a)}{x-a} < \frac{f(b)-f(a)}{2(b-a)}\right\}.$$ I tried wrapping my head around the geometric meaning of $\frac{f(x)-f(a)}{x-a} $ and $\frac{f(b)-f(a)}{2(b-a)}$ . I came to the conclusion that $\frac{f(x)-f(a)}{x-a} $ approximates the slope of the curve at $a$ for sufficient $x$ . $\frac{f(b)-f(a)}{2(b-a)}$ is half of the slope of the secant line connecting $a$ with $b$ . But what does finding the $\sup$ of $M$ mean and how does it contradict the fact that $f$ is not constant?",So suppose is an open interval and a continuous function such that for all . Show that is constant. My ideas: Proof by contraposition. Suppose is not constant; thus there are with such that . I am given the hint that I should check the of I tried wrapping my head around the geometric meaning of and . I came to the conclusion that approximates the slope of the curve at for sufficient . is half of the slope of the secant line connecting with . But what does finding the of mean and how does it contradict the fact that is not constant?,"I\subset \mathbb{R} f: I\to \mathbb{R} \lim\limits_{h\to 0^+}\frac{f(x+h)-f(x)}{h}=0 x\in I f f a,b\in I a<b f(a)\neq f(b) \sup M:=\left \{ x\in [a,b] : \frac{f(x)-f(a)}{x-a} < \frac{f(b)-f(a)}{2(b-a)}\right\}. \frac{f(x)-f(a)}{x-a}  \frac{f(b)-f(a)}{2(b-a)} \frac{f(x)-f(a)}{x-a}  a x \frac{f(b)-f(a)}{2(b-a)} a b \sup M f","['real-analysis', 'derivatives', 'continuity', 'supremum-and-infimum']"
29,Two values of minima of a function by two methods.,Two values of minima of a function by two methods.,,"I had a problem of finding minima of a function $$f(x)=2^{x^2}-1+\frac{2}{2^{x^2}+1}$$ I solved it using AM-GM inequality, $$2^{x^2}-1+\frac{2}{2^{x^2}+1}$$ $$2^{x^2}+1+\frac{2}{2^{x^2}+1}-2$$ $$2^{x^2}+1+\frac{2}{2^{x^2}+1}\ge\ 2\sqrt2$$ $$2^{x^2}-1+\frac{2}{2^{x^2}+1}\ge\ 2\sqrt2-2$$ But in the solution answer was given as 1 and it was solved using differentiation, $$f'(x)=\frac{2x.ln2.2^{x^2}(2^{x^2}+1-\sqrt2)(2^{x^2}+1+\sqrt2)}{(2^{x^2}+1)^2}$$ $$2^{x^2}\ge1$$ $$2^{x^2}+1-\sqrt2\ge2-\sqrt2>0$$ At $x=0$ , $f(x)$ is least. Least value = $f(0)$ $=1$ I cannot understand how can there be two values by two different methods,please help me in the problem.","I had a problem of finding minima of a function I solved it using AM-GM inequality, But in the solution answer was given as 1 and it was solved using differentiation, At , is least. Least value = I cannot understand how can there be two values by two different methods,please help me in the problem.",f(x)=2^{x^2}-1+\frac{2}{2^{x^2}+1} 2^{x^2}-1+\frac{2}{2^{x^2}+1} 2^{x^2}+1+\frac{2}{2^{x^2}+1}-2 2^{x^2}+1+\frac{2}{2^{x^2}+1}\ge\ 2\sqrt2 2^{x^2}-1+\frac{2}{2^{x^2}+1}\ge\ 2\sqrt2-2 f'(x)=\frac{2x.ln2.2^{x^2}(2^{x^2}+1-\sqrt2)(2^{x^2}+1+\sqrt2)}{(2^{x^2}+1)^2} 2^{x^2}\ge1 2^{x^2}+1-\sqrt2\ge2-\sqrt2>0 x=0 f(x) f(0) =1,['derivatives']
30,"Calculus for the Practical Man: Chapter 4, Problem 16","Calculus for the Practical Man: Chapter 4, Problem 16",,"A rope 28 feet long is attached to a block on level ground and runs over a pulley 12 feet above the ground. The rope is stretched taut and the free end is drawn directly away from the block and pulley at the rate of 13 ft. per sec. How fast will the block be moving when it is 5 feet away from the point directly below the pulley? My solution: I start with relating the three sides of the triangle, holding one side constant at 12 feet. My aim is to find the rate of movement of the rope, that is, the rate of change of the hypotenuse, h , which determines, and is therefore identical to, the rate of change of the position of the box. $$h^2 = x^2 + y^2$$ $$h = \sqrt{x^2 + 144},$$ and since the rate of movement of the rope dictates the rate of movement of the box, $$\frac{dh}{dt} = \frac{x}{\sqrt{x^2 + 144}} \cdot \frac{dx}{dt}$$ Given that $h = 28 - 5 = 23$ feet when the box is 5 feet from the pulley, and that $$ x = \sqrt{h^2 - y^2} = \sqrt{23^2 - 12^2} = \sqrt{385},$$ it follows that $$\frac{dh}{dt} = \frac{\sqrt{385}}{\sqrt{385 + 144}} \cdot 13 \approx 11.1 \textrm{ ft/s}.$$ The book's answer is $8 \textrm{ ft^2/s}.$ Did I do something wrong? My answer is so close to the book's.","A rope 28 feet long is attached to a block on level ground and runs over a pulley 12 feet above the ground. The rope is stretched taut and the free end is drawn directly away from the block and pulley at the rate of 13 ft. per sec. How fast will the block be moving when it is 5 feet away from the point directly below the pulley? My solution: I start with relating the three sides of the triangle, holding one side constant at 12 feet. My aim is to find the rate of movement of the rope, that is, the rate of change of the hypotenuse, h , which determines, and is therefore identical to, the rate of change of the position of the box. and since the rate of movement of the rope dictates the rate of movement of the box, Given that feet when the box is 5 feet from the pulley, and that it follows that The book's answer is Did I do something wrong? My answer is so close to the book's.","h^2 = x^2 + y^2 h = \sqrt{x^2 + 144}, \frac{dh}{dt} = \frac{x}{\sqrt{x^2 + 144}} \cdot \frac{dx}{dt} h = 28 - 5 = 23  x = \sqrt{h^2 - y^2} = \sqrt{23^2 - 12^2} = \sqrt{385}, \frac{dh}{dt} = \frac{\sqrt{385}}{\sqrt{385 + 144}} \cdot 13 \approx 11.1 \textrm{ ft/s}. 8 \textrm{ ft^2/s}.","['calculus', 'derivatives', 'related-rates']"
31,Derivative of the antipodal map of $S^n$ in the context of smooth manifolds.,Derivative of the antipodal map of  in the context of smooth manifolds.,S^n,"I was reading a solution of a question here (see item $c$ on page $7$ ), but I don't understand why the antipodal map of $S^n$ has derivative equal to minus the identity on $T_pS^n$ for $p \in S^n$ . This is clear if we see $S^n \subset \mathbb{R}^{n+1}$ , then it's a simple computation of analysis in Euclidian spaces, but how to prove this in the context of smooth manifolds, where we don't have an ambient space to $S^n$ necessarily?","I was reading a solution of a question here (see item on page ), but I don't understand why the antipodal map of has derivative equal to minus the identity on for . This is clear if we see , then it's a simple computation of analysis in Euclidian spaces, but how to prove this in the context of smooth manifolds, where we don't have an ambient space to necessarily?",c 7 S^n T_pS^n p \in S^n S^n \subset \mathbb{R}^{n+1} S^n,"['derivatives', 'smooth-manifolds']"
32,Is there a compact expression for multiple chain rule derivatives?,Is there a compact expression for multiple chain rule derivatives?,,"Let $h(x) = f(g(x))$ . Is there an expression for $h^{(n)}(x)$ , which denotes the $n^{\text{th}}$ derivative of $h$ with respect to $x$ , in terms of derivatives of $f$ and $g$ ? For example, the first few derivatives are easily checked to be: $h'(x) = f' \cdot g'$ $h''(x) = f'' \cdot(g')^2 + f' \cdot g''$ $h'''(x) = f''' \cdot(g')^3 + 3f'' \cdot (g')^2\cdot g''+f'\cdot g'''$ $h^{(4)}(x)=f^{(4)} \cdot(g')^4 + 6f'''\cdot (g')^2\cdot g'' + 3f''\cdot(g'')^2+4g''\cdot f'\cdot f'' +g'\cdot f^{(4)}, \\$ where $f^{(n)}$ is understood to mean $f^{(n)}(g(x))$ . One might expect a nice pattern to emerge, but I'm not seeing anything obvious. Compare this to the Pascal's triangle pattern that emerges for $h(x)=f(x)\cdot g(x)$ upon taking $n$ derivatives of $h$ (x), allowing us to write very simply, $h^{(n)}(x)=\sum_{i=0}^n {{n}\choose{i}}f^{(n-i)}(x)\cdot g^{(i)}(x)$ .","Let . Is there an expression for , which denotes the derivative of with respect to , in terms of derivatives of and ? For example, the first few derivatives are easily checked to be: where is understood to mean . One might expect a nice pattern to emerge, but I'm not seeing anything obvious. Compare this to the Pascal's triangle pattern that emerges for upon taking derivatives of (x), allowing us to write very simply, .","h(x) = f(g(x)) h^{(n)}(x) n^{\text{th}} h x f g h'(x) = f' \cdot g' h''(x) = f'' \cdot(g')^2 + f' \cdot g'' h'''(x) = f''' \cdot(g')^3 + 3f'' \cdot (g')^2\cdot g''+f'\cdot g''' h^{(4)}(x)=f^{(4)} \cdot(g')^4 + 6f'''\cdot (g')^2\cdot g'' + 3f''\cdot(g'')^2+4g''\cdot f'\cdot f'' +g'\cdot f^{(4)}, \\ f^{(n)} f^{(n)}(g(x)) h(x)=f(x)\cdot g(x) n h h^{(n)}(x)=\sum_{i=0}^n {{n}\choose{i}}f^{(n-i)}(x)\cdot g^{(i)}(x)","['calculus', 'derivatives', 'chain-rule']"
33,Proof by induction for nth derivative,Proof by induction for nth derivative,,"Show the following hold by induction: $$\frac {d^n}{dx^n}\frac {e^x - 1}{x} = (-1)^n \frac{n!}{x^{n+1}} \left( e^x \left(\sum_{k=0}^{n} (-1)^{k} \frac{x^k}{k!}\right) - 1 \right)$$ Proof. It's not hard to show the base case hold. For inductive step, we can also write this as: $$\frac {d^n}{dx^n}\frac {e^x - 1}{x} = (-1)^n \frac{n!}{x^{n+1}} e^x \sum_{k=0}^{n} (-1)^{k} \frac{x^k}{k!} - (-1)^n \frac{n!}{x^{n+1}} $$ Take derivative on both side: $$\frac {d^{n+1}}{dx^{n+1}}\frac {e^x - 1}{x} = \frac {d}{dx}(-1)^n \frac{n!}{x^{n+1}} e^x \sum_{k=0}^{n} (-1)^{k} \frac{x^k}{k!} - \frac {d}{dx}(-1)^n \frac{n!}{x^{n+1}} $$ $$\frac {d^{n+1}}{dx^{n+1}}\frac {e^x - 1}{x} = \underbrace{\frac d{dx}(-1)^n\frac{n!}{x^{n+1}}e^x\sum_{k=0}^n(-1)^k\frac{x^k}{k!}}_? - \underbrace{(-1)^{n+1}\frac{(n+1)!}{x^{n+2}}}_{hold} $$ Therefore, my question is for the first part, how do I show the following hold: $$ \frac d{dx}(-1)^n\frac{n!}{x^{n+1}}e^x\sum_{k=0}^n(-1)^k\frac{x^k}{k!}  = (-1)^{n+1}\frac{(n+1)!}{x^{n+2}} e^x \sum_{k=0}^{n+1} (-1)^{k} \frac{x^k}{k!}$$","Show the following hold by induction: Proof. It's not hard to show the base case hold. For inductive step, we can also write this as: Take derivative on both side: Therefore, my question is for the first part, how do I show the following hold:","\frac {d^n}{dx^n}\frac {e^x - 1}{x} = (-1)^n \frac{n!}{x^{n+1}} \left( e^x \left(\sum_{k=0}^{n} (-1)^{k} \frac{x^k}{k!}\right) - 1 \right) \frac {d^n}{dx^n}\frac {e^x - 1}{x} = (-1)^n \frac{n!}{x^{n+1}} e^x \sum_{k=0}^{n} (-1)^{k} \frac{x^k}{k!} - (-1)^n \frac{n!}{x^{n+1}}  \frac {d^{n+1}}{dx^{n+1}}\frac {e^x - 1}{x} = \frac {d}{dx}(-1)^n \frac{n!}{x^{n+1}} e^x \sum_{k=0}^{n} (-1)^{k} \frac{x^k}{k!} - \frac {d}{dx}(-1)^n \frac{n!}{x^{n+1}}  \frac {d^{n+1}}{dx^{n+1}}\frac {e^x - 1}{x} = \underbrace{\frac d{dx}(-1)^n\frac{n!}{x^{n+1}}e^x\sum_{k=0}^n(-1)^k\frac{x^k}{k!}}_? - \underbrace{(-1)^{n+1}\frac{(n+1)!}{x^{n+2}}}_{hold}   \frac d{dx}(-1)^n\frac{n!}{x^{n+1}}e^x\sum_{k=0}^n(-1)^k\frac{x^k}{k!} 
= (-1)^{n+1}\frac{(n+1)!}{x^{n+2}} e^x \sum_{k=0}^{n+1} (-1)^{k} \frac{x^k}{k!}","['derivatives', 'summation', 'induction', 'factorial']"
34,Is a function differentiable at a point if its derivative is continuous at that point?,Is a function differentiable at a point if its derivative is continuous at that point?,,My professor said that the title statement might not always be the case and gave $$x^2 \sin\left(\frac{1}{x}\right)$$ at $x=0$ as a counter-example. But I don't seem to understand its differentiability and continuity at the point $x=0$ . Any explanation and if possible a better example are highly appreciated.,My professor said that the title statement might not always be the case and gave at as a counter-example. But I don't seem to understand its differentiability and continuity at the point . Any explanation and if possible a better example are highly appreciated.,x^2 \sin\left(\frac{1}{x}\right) x=0 x=0,"['calculus', 'derivatives', 'continuity']"
35,Right-sided derivative of $f(x)=\frac{\sin(\sqrt{x})}{\sqrt{x}}$,Right-sided derivative of,f(x)=\frac{\sin(\sqrt{x})}{\sqrt{x}},Let define $f:\mathbb{R}\rightarrow\mathbb{R}$ such as $f(x)=\frac{\sin(\sqrt{x})}{\sqrt{x}}$ for $x>0$ and $f(x)=1$ for $x=0$ Prove that $f$ has right-sided all-order derivatives in $0$ . My approach: I find out that $f$ can be written as the power series $$ f(x)=\sum_{n=0}^{\infty}\frac{(-1)^k}{(2k+1)!}x^k $$ which has an infinite radius of convergence so the $n$ -th derivative of $f$ in zero would be $$ f^{(n)}(0)=\frac{(-1)^n n!}{(2n+1)!} $$ but I do not know how to connect these facts with the right-sided derivative of this function,Let define such as for and for Prove that has right-sided all-order derivatives in . My approach: I find out that can be written as the power series which has an infinite radius of convergence so the -th derivative of in zero would be but I do not know how to connect these facts with the right-sided derivative of this function,"f:\mathbb{R}\rightarrow\mathbb{R} f(x)=\frac{\sin(\sqrt{x})}{\sqrt{x}} x>0 f(x)=1 x=0 f 0 f 
f(x)=\sum_{n=0}^{\infty}\frac{(-1)^k}{(2k+1)!}x^k
 n f 
f^{(n)}(0)=\frac{(-1)^n n!}{(2n+1)!}
","['real-analysis', 'derivatives', 'trigonometry', 'power-series']"
36,On the existence of a certain type of function,On the existence of a certain type of function,,I was asked in an analysis textbook the following question: Can there be a continuous and differentiable function $ f : \mathbb{R} \to \mathbb{R} $ such that $ f(0) = 1 $ and $ f'(x) \geq f^2(x) $ ? We are asked to prove inexistence or supply an example. ( $f^2$ is the squared function). I have tried coming up with such a function but to no avail and I also could not supply proof of non-existence. I was thinking of standard tricks like a factor of integration or composing $ f $ with another function to disprove using the inequality. I thank all helpers.,I was asked in an analysis textbook the following question: Can there be a continuous and differentiable function such that and ? We are asked to prove inexistence or supply an example. ( is the squared function). I have tried coming up with such a function but to no avail and I also could not supply proof of non-existence. I was thinking of standard tricks like a factor of integration or composing with another function to disprove using the inequality. I thank all helpers., f : \mathbb{R} \to \mathbb{R}   f(0) = 1   f'(x) \geq f^2(x)  f^2  f ,"['real-analysis', 'calculus', 'derivatives']"
37,Solving the one particle partition function,Solving the one particle partition function,,"We have $N$ oscillators and each of them is described by the Hamiltonian: $$H = \frac{p^2}{2m} + \frac{Kq^4}{4} $$ I have to compute the average total energy $\langle E\rangle$ of the $N$ oscillators. But to do so, first I have to compute the one particle partition function and to do so I have to solve the following integral: $$Z_1 (V,T) = \iint_{\mathbb{R}^2}  e ^{-\beta \,H_1(p,q)}\,dp\,dq.$$ So in this case: $$ Z_1 = \int_{-\infty}^{\infty}  e^{-\beta\frac{p^2}{2m}}\,dp \int_{-\infty}^{\infty}  e^{-\beta \frac{Kq^4}{4}}\,dq. $$ I know this integral can be solved by the Gauss method, knowing that: $$\int_{-\infty}^{\infty} e^{-x^2} dx = \sqrt{\pi}.$$ For the first integral I got: $$\int_{-\infty}^{\infty}  e^{-\beta\frac{p^2}{2m}}\,dp = \sqrt{\frac{2m\pi}{\beta}}$$ I am having difficulties solving the second one: $$\int_{-\infty}^{\infty}  e^{-\beta \frac{Kq^4}{4}}\,dq.$$ I have tried to make a the change of variables: $q^4 = a^2,$ but this does not simplify the calculation. What method should I use? Once you get $Z_1$ : $$Z_N = (Z_1)^N.$$ Then you just have to apply: $$\langle E \rangle = -\frac{ \partial \log(Z_N)}{\partial \beta}.$$ ANSWER $$\langle E \rangle = \frac{3N}{4\beta}.$$","We have oscillators and each of them is described by the Hamiltonian: I have to compute the average total energy of the oscillators. But to do so, first I have to compute the one particle partition function and to do so I have to solve the following integral: So in this case: I know this integral can be solved by the Gauss method, knowing that: For the first integral I got: I am having difficulties solving the second one: I have tried to make a the change of variables: but this does not simplify the calculation. What method should I use? Once you get : Then you just have to apply: ANSWER","N H = \frac{p^2}{2m} + \frac{Kq^4}{4}  \langle E\rangle N Z_1 (V,T) = \iint_{\mathbb{R}^2}  e ^{-\beta \,H_1(p,q)}\,dp\,dq.  Z_1 = \int_{-\infty}^{\infty}  e^{-\beta\frac{p^2}{2m}}\,dp \int_{-\infty}^{\infty}  e^{-\beta \frac{Kq^4}{4}}\,dq.  \int_{-\infty}^{\infty} e^{-x^2} dx = \sqrt{\pi}. \int_{-\infty}^{\infty}  e^{-\beta\frac{p^2}{2m}}\,dp = \sqrt{\frac{2m\pi}{\beta}} \int_{-\infty}^{\infty}  e^{-\beta \frac{Kq^4}{4}}\,dq. q^4 = a^2, Z_1 Z_N = (Z_1)^N. \langle E \rangle = -\frac{ \partial \log(Z_N)}{\partial \beta}. \langle E \rangle = \frac{3N}{4\beta}.","['calculus', 'integration', 'derivatives', 'mathematical-physics', 'statistical-mechanics']"
38,$\frac{d^2}{dx^2}f(x)>\frac{d^2}{d x^2}g(x) \implies \frac{d^2}{d x^2}\log(f(x))> \frac{d^2}{d x^2}\log(g(x))$?,?,\frac{d^2}{dx^2}f(x)>\frac{d^2}{d x^2}g(x) \implies \frac{d^2}{d x^2}\log(f(x))> \frac{d^2}{d x^2}\log(g(x)),"Does the following inequality hold? $$\frac{d^2}{dx^2}f(x)>\frac{d^2}{d x^2}g(x) \implies \frac{d^2}{d x^2}\log(f(x))> \frac{d^2}{d x^2}\log(g(x))$$ EDIT: as this does not hold i wonder whether it works with $\geq$ ? To give some insights why I was interested in this question: The fisher-information is defines as $$ I(\theta)=\int \frac{\partial^2}{\partial \theta ^2}log(f_\theta(x))dP_\theta(x)$$ and I was trying to figure out whether this cannot only be interpreted as the ""expected curvature"" of the log-likelihood but also as containing information about the expected curvature of the likelihood, but the relationship is at least not as simple as I hoped it would be.","Does the following inequality hold? EDIT: as this does not hold i wonder whether it works with ? To give some insights why I was interested in this question: The fisher-information is defines as and I was trying to figure out whether this cannot only be interpreted as the ""expected curvature"" of the log-likelihood but also as containing information about the expected curvature of the likelihood, but the relationship is at least not as simple as I hoped it would be.",\frac{d^2}{dx^2}f(x)>\frac{d^2}{d x^2}g(x) \implies \frac{d^2}{d x^2}\log(f(x))> \frac{d^2}{d x^2}\log(g(x)) \geq  I(\theta)=\int \frac{\partial^2}{\partial \theta ^2}log(f_\theta(x))dP_\theta(x),"['calculus', 'derivatives']"
39,"Find the Fourier series of the function: $f(x)=\begin{cases}x+1 & -1\leq x< 0,\\1-x &0\leq x< 1\end{cases},\;\;f(x+2)=f(x)$",Find the Fourier series of the function:,"f(x)=\begin{cases}x+1 & -1\leq x< 0,\\1-x &0\leq x< 1\end{cases},\;\;f(x+2)=f(x)","I want to find the Fourier series of the function. For now, I am clueless on how to handle the function, int that it has $f(x+2)=f(x).$ $$f(x)=\begin{cases}x+1 & -1\leq x< 0,\\1-x &0\leq x< 1\end{cases},\;\;f(x+2)=f(x)$$ Question: Can anyone explain what this term ($f(x+2)=f(x)$) means in relation to the function? Solution or references will be highly appreciated.","I want to find the Fourier series of the function. For now, I am clueless on how to handle the function, int that it has $f(x+2)=f(x).$ $$f(x)=\begin{cases}x+1 & -1\leq x< 0,\\1-x &0\leq x< 1\end{cases},\;\;f(x+2)=f(x)$$ Question: Can anyone explain what this term ($f(x+2)=f(x)$) means in relation to the function? Solution or references will be highly appreciated.",,"['calculus', 'derivatives', 'fourier-analysis', 'fourier-series', 'fourier-transform']"
40,"How are both of these true: $J = \nabla f ^T $, and also $\nabla f = J^T f$?","How are both of these true: , and also ?",J = \nabla f ^T  \nabla f = J^T f,"From questions such as this one: Gradient and Jacobian row and column conventions I understand that for cases where $f$ maps from $\mathbb{R}^n$ into $\mathbb{R}$ , i.e. $f: \mathbb{R}^n \rightarrow \mathbb{R}$,  the transpose of the gradient is equal to the jacobian:  $J = \nabla f ^T $.  Again, see Gradient and Jacobian row and column conventions as my resource. However, I am still occasionally confused by this, because when finding an expression of the gradient for when $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ I see expressions such as $\nabla f = J^T f$.   An example of this is in Nocedal and Wright, first edition on page 260: Question is how are both of these true:  $J = \nabla f ^T $, and also $\nabla f = J^T f$ ?","From questions such as this one: Gradient and Jacobian row and column conventions I understand that for cases where $f$ maps from $\mathbb{R}^n$ into $\mathbb{R}$ , i.e. $f: \mathbb{R}^n \rightarrow \mathbb{R}$,  the transpose of the gradient is equal to the jacobian:  $J = \nabla f ^T $.  Again, see Gradient and Jacobian row and column conventions as my resource. However, I am still occasionally confused by this, because when finding an expression of the gradient for when $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ I see expressions such as $\nabla f = J^T f$.   An example of this is in Nocedal and Wright, first edition on page 260: Question is how are both of these true:  $J = \nabla f ^T $, and also $\nabla f = J^T f$ ?",,"['derivatives', 'differential-geometry', 'differential', 'gradient-descent', 'jacobian']"
41,Gradient of the spectral norm of a matrix,Gradient of the spectral norm of a matrix,,"Let $X \in \mathbb{R}^{a \times b}$ and $$\|X\|_2 = \sigma_{\max}(X) = \sqrt{\lambda_{\max} \left( X^T X \right)}$$ How can I compute $\nabla_X \|AX\|_2$, where $A \in \mathbb{R}^{c \times a}$ is some known matrix?","Let $X \in \mathbb{R}^{a \times b}$ and $$\|X\|_2 = \sigma_{\max}(X) = \sqrt{\lambda_{\max} \left( X^T X \right)}$$ How can I compute $\nabla_X \|AX\|_2$, where $A \in \mathbb{R}^{c \times a}$ is some known matrix?",,"['linear-algebra', 'derivatives', 'matrix-calculus', 'singular-values', 'spectral-norm']"
42,Differentiation of a function defined over $\mathbb{R}^{m^{2}}$ (matrix spaces).,Differentiation of a function defined over  (matrix spaces).,\mathbb{R}^{m^{2}},"For $1\leq i,j\leq m$, let $f_{ij}:\mathbb{R}^{m^{2}} \to \mathbb{R}$ defined by $f_{ij}(X)V = (X^{2})_{ij}$. For any $X,V \in \mathbb{R}^{m^{2}}$, show that $df_{ij}(X)V$ is the $ij$-th element of the matrix $XV + VX$. Get a similar result with $X^{3}$. Generalize. I don't want an answer. Actually, I didn't understand how this function works. I would like an explanation to understand this function and a small hint to start solving.","For $1\leq i,j\leq m$, let $f_{ij}:\mathbb{R}^{m^{2}} \to \mathbb{R}$ defined by $f_{ij}(X)V = (X^{2})_{ij}$. For any $X,V \in \mathbb{R}^{m^{2}}$, show that $df_{ij}(X)V$ is the $ij$-th element of the matrix $XV + VX$. Get a similar result with $X^{3}$. Generalize. I don't want an answer. Actually, I didn't understand how this function works. I would like an explanation to understand this function and a small hint to start solving.",,"['real-analysis', 'derivatives']"
43,Prove to me that this answer is correct,Prove to me that this answer is correct,,"I'm doubting myself and my teacher, so I need some external verification. So, the graph on the left is given to me, and I'm told to derive it. The answer key is shown on the right. What I do not understand is why $x = -5$, $-1$ and $3$ are not inclusive. I understand that the limits at $x = -5$, $-1$ and $3$ do not exist, obviously. However, they are still derivable at those points, correct? Because in the original graph, they are clearly marked inclusive. It even says the domain is $[-5, \infty)$, so that completely means $-5$ is inclusive for $f(x)$. (That infinity one should really be a right parenthesis.) So anyway, is my teacher wrong, or am I wrong? Can anyone actually prove to me in the most clear, concise manner possible that $f'(x)$ at the points $-5$, $-1$ and $3$ are non-inclusive? Thank you for your time and help in advance. It is much appreciated.","I'm doubting myself and my teacher, so I need some external verification. So, the graph on the left is given to me, and I'm told to derive it. The answer key is shown on the right. What I do not understand is why $x = -5$, $-1$ and $3$ are not inclusive. I understand that the limits at $x = -5$, $-1$ and $3$ do not exist, obviously. However, they are still derivable at those points, correct? Because in the original graph, they are clearly marked inclusive. It even says the domain is $[-5, \infty)$, so that completely means $-5$ is inclusive for $f(x)$. (That infinity one should really be a right parenthesis.) So anyway, is my teacher wrong, or am I wrong? Can anyone actually prove to me in the most clear, concise manner possible that $f'(x)$ at the points $-5$, $-1$ and $3$ are non-inclusive? Thank you for your time and help in advance. It is much appreciated.",,"['calculus', 'derivatives']"
44,Boundary point & critical point of a function,Boundary point & critical point of a function,,Is it a must that all critical points are interior point for a function ?😕 My question is can a boundary point be critical point of a function??,Is it a must that all critical points are interior point for a function ?😕 My question is can a boundary point be critical point of a function??,,"['calculus', 'derivatives']"
45,Taylor's theorem implies existence of n+1 order derivative?,Taylor's theorem implies existence of n+1 order derivative?,,"From my  Calculus textbook: Usually the text is careful to make sure the description above a formula establishes all the necessary preconditions to exist for the formula to be true. I noticed here though that $f^ {(n+1)} $ is referred to, without any mention of a requirement that it exist (f being order n differentiable by itself doesn't imply n+1 differentiability). Does the continuity of all the earlier derivatives imply it or is there an unstated assumption?","From my  Calculus textbook: Usually the text is careful to make sure the description above a formula establishes all the necessary preconditions to exist for the formula to be true. I noticed here though that $f^ {(n+1)} $ is referred to, without any mention of a requirement that it exist (f being order n differentiable by itself doesn't imply n+1 differentiability). Does the continuity of all the earlier derivatives imply it or is there an unstated assumption?",,"['calculus', 'derivatives', 'taylor-expansion']"
46,How did product rule come about historically?,How did product rule come about historically?,,"Let $d(x) = f(x)g(x)$. Then: $$d'(x) = \lim_{h \to 0} \frac{d(x+h) - d(x)}{h}$$ $$d'(x) = \lim_{h \to 0} \frac{f(x+h)g(x+h) - f(x)g(x)}{h}$$ $$d'(x) = \lim_{h \to 0} \frac{f(x+h)g(x+h) - f(x+h)g(x) + f(x+h)g(x) - f(x)g(x)}{h}$$ $$d'(x) = \lim_{h \to 0} \frac{f(x+h)(g(x+h) - g(x)) + g(x)(f(x+h) - f(x))}{h}$$ $$d'(x) = \lim_{h \to 0} \frac{f(x+h)(g(x+h) - g(x))}{h} + \lim_{h \to 0}\frac{g(x)(f(x+h) - f(x))}{h}$$ $$d'(x) = \lim_{h \to 0} f(x+h)\frac{g(x+h) - g(x)}{h} + \lim_{h \to 0}g(x)\frac{f(x+h) - f(x)}{h}$$ $$d'(x) = f(x)g'(x) + g(x)f'(x)$$ It seems to be right but I only got here because I knew there was some slick ""closed-form"" ahead of time and I was just trying to figure out how to get there. When this rule was invented (many years ago, historically speaking), how did they get to the end state without necessarily knowing how it was going to end? It makes sense to ask ""What is the derivative of the product of two functions?"" but then it's very easy to get stuck at this step: $$d'(x) = \lim_{h \to 0} \frac{f(x+h)g(x+h) - f(x)g(x)}{h}$$ Before realizing you can add/subtract the same quantity and do some clever rearranging. How did anyone figure this out? Was there evidence this was a good way to go or did someone just trial-and-error it until they discovered it?","Let $d(x) = f(x)g(x)$. Then: $$d'(x) = \lim_{h \to 0} \frac{d(x+h) - d(x)}{h}$$ $$d'(x) = \lim_{h \to 0} \frac{f(x+h)g(x+h) - f(x)g(x)}{h}$$ $$d'(x) = \lim_{h \to 0} \frac{f(x+h)g(x+h) - f(x+h)g(x) + f(x+h)g(x) - f(x)g(x)}{h}$$ $$d'(x) = \lim_{h \to 0} \frac{f(x+h)(g(x+h) - g(x)) + g(x)(f(x+h) - f(x))}{h}$$ $$d'(x) = \lim_{h \to 0} \frac{f(x+h)(g(x+h) - g(x))}{h} + \lim_{h \to 0}\frac{g(x)(f(x+h) - f(x))}{h}$$ $$d'(x) = \lim_{h \to 0} f(x+h)\frac{g(x+h) - g(x)}{h} + \lim_{h \to 0}g(x)\frac{f(x+h) - f(x)}{h}$$ $$d'(x) = f(x)g'(x) + g(x)f'(x)$$ It seems to be right but I only got here because I knew there was some slick ""closed-form"" ahead of time and I was just trying to figure out how to get there. When this rule was invented (many years ago, historically speaking), how did they get to the end state without necessarily knowing how it was going to end? It makes sense to ask ""What is the derivative of the product of two functions?"" but then it's very easy to get stuck at this step: $$d'(x) = \lim_{h \to 0} \frac{f(x+h)g(x+h) - f(x)g(x)}{h}$$ Before realizing you can add/subtract the same quantity and do some clever rearranging. How did anyone figure this out? Was there evidence this was a good way to go or did someone just trial-and-error it until they discovered it?",,"['calculus', 'derivatives', 'soft-question', 'math-history']"
47,Understanding step in derivation of softmax function,Understanding step in derivation of softmax function,,"I'm reading Eli Bendersky's blog post that derives the softmax function and its associated loss function and am stuck on one of the first steps of the softmax function derivative [ link ]. His notation defines the softmax as follows: $$S_j = \frac{e^{a_i}}{ \sum_{k=1}^{N} e^{a_k} } $$ He then goes on to start the derivative: $$ \frac{\partial S_i}{\partial a_j} = \frac{ \partial \frac{e^{a_i} }{ \sum_{k=1}^N e^{a_k}} } {\partial a_j} $$ Here we are computing the derivative with respect to the $i$th output and the $j$th input. Because the numerator involves a quotient, he says one must apply the quotient rule from calculus: $$ f(x) = \frac{g(x)}{h(x)} $$ $$ f'(x) = \frac{ g'(x)h(x) - h'(x)g(x) } { (h(x))^2 } $$ In the case of the $S_j$ equations above: $$ g_i = e^{a_i} $$ $$ h_i = \sum_{k=1}^N e^{a_k} $$ So far so good. Here's where I get confused. He then says: ""Note that no matter which $a_j$ we compute the derivative of $h_i$ for, the answer will always be $e^{a_j}$"". If anyone could help me see why this is the case, I'd be very grateful.","I'm reading Eli Bendersky's blog post that derives the softmax function and its associated loss function and am stuck on one of the first steps of the softmax function derivative [ link ]. His notation defines the softmax as follows: $$S_j = \frac{e^{a_i}}{ \sum_{k=1}^{N} e^{a_k} } $$ He then goes on to start the derivative: $$ \frac{\partial S_i}{\partial a_j} = \frac{ \partial \frac{e^{a_i} }{ \sum_{k=1}^N e^{a_k}} } {\partial a_j} $$ Here we are computing the derivative with respect to the $i$th output and the $j$th input. Because the numerator involves a quotient, he says one must apply the quotient rule from calculus: $$ f(x) = \frac{g(x)}{h(x)} $$ $$ f'(x) = \frac{ g'(x)h(x) - h'(x)g(x) } { (h(x))^2 } $$ In the case of the $S_j$ equations above: $$ g_i = e^{a_i} $$ $$ h_i = \sum_{k=1}^N e^{a_k} $$ So far so good. Here's where I get confused. He then says: ""Note that no matter which $a_j$ we compute the derivative of $h_i$ for, the answer will always be $e^{a_j}$"". If anyone could help me see why this is the case, I'd be very grateful.",,"['calculus', 'derivatives', 'partial-derivative']"
48,Proving inequation with maximum and 2nd derivative,Proving inequation with maximum and 2nd derivative,,Let $f(x)$ be twice-differentiable and $f(a)=f(b)=0$ with $a<b$ How to prove that $$\max|f(x)| \le \frac 18(b-a)^2 \max|f''(x)|$$ (With $\max|f(x)|$ beeing maximum (or minimum) between a and b.) How would I use Taylor-Expansion and Mean Value Theorem here?,Let $f(x)$ be twice-differentiable and $f(a)=f(b)=0$ with $a<b$ How to prove that $$\max|f(x)| \le \frac 18(b-a)^2 \max|f''(x)|$$ (With $\max|f(x)|$ beeing maximum (or minimum) between a and b.) How would I use Taylor-Expansion and Mean Value Theorem here?,,"['calculus', 'real-analysis', 'derivatives', 'inequality']"
49,Function defined as determinant of polynomial matrix,Function defined as determinant of polynomial matrix,,"I have the following function: \begin{equation*} f_n(t) = \det\left(\begin{array}{ccc}p_0(t)&\dots&p_n(t)\\ p'_0(t)&\dots&p'_n(t)\\ p''_0(t)&\dots&p''_n(t)\\ \vdots&\ddots&\vdots\\ p^n_0(t)&\dots&p^n_n(t)\end{array}\right) \end{equation*} Where $p_i(t)$ are polynomials of degree $\leq n$ and $p^k_i(t)$ is the $k$-th derivative. I'm trying to show that $f(t) = f(0), \forall t$. Now, $f(0)$, as far as I understand, works out to be the determinant of the coefficient matrix multiplied by some constant. I decided to try this by induction, but I'm stuck on the inductive step. This is what I have so far: We prove the general case, for $n \geq 1$ via induction on $n$. For our base case, let $n = 1$ and let $p_i(t) = a_{i1}t+a_{i0}$ for $i \in \{0,1\}$. We have \begin{equation*} f(t) = \det\left(\begin{array}{cc}a_{01}t+a_{00} & a_{11}t+a_{10}\\ a_{01}&a_{11}\end{array}\right)= (a_{01}t+a_{00})a_{11} - (a_{11}t+a_{10})a_{01} \\ = a_{01}a_{11} t-a_{01}    a_{11}t-a_{01} a_{10}+a_{00}    a_{11} = a_{00} a_{11}-a_{01} a_{10}. \end{equation*} Clearly, $f(t)$ does not depend on $t$, and so, $f(t) = f(0)$ for all $t$. Now, we assume that the result holds for all $i < k$. We show that this implies that the result holds for $k$ as follows. Let $p_i(t) = a_{ik}t^k+ \dots +a_{i0}$ for $i \in \{0,k\}$. We have \begin{equation*} f(t) = \det\left(\begin{array}{ccc}p_0(t)&\dots&p_k(t)\\ \vdots&\ddots&\vdots\\ p^k_0(t)&\dots&p^k_k(t)\end{array}\right)= \sum_{i=0}^k (-1)^{i}p_i(t)\det(M_{1,i+1}) = \sum_{i=0}^k (-1)^{i}p_i(t)f_{{k-1}_{1,i+1}}(0) \end{equation*} where $M_{m,n}$ is the minor matrix of the $m$-th row and $n$-th column and $f_{{k-1}_{1,i+1}}$ is $\det(M_{1,i+1})$ evaluated at $t = 0$. In our case, since $p_i(t)$ has degree at most $k$, $p'_i(t)$ has degree at most $k-1$, and so, for any $i$, $M_{1,i+1}$ is a matrix of the given form for $n = k-1$. By the induction hypothesis, for all $i$, $\det(M_{1,i+1})$ is a constant that does not depend on $t$. I'm not really sure how to proceed from here, why does it follow that the resulting equation does not depend on $t$? I guess it has something to do with the fact that we're multiplying each polynomial by the determinant (times some constant) of the remaining polynomials' coefficients (except the constant coefficient, $a_{i0}$, which is lost in the first derivative). I don't see any other links here that would allow me to proceed.","I have the following function: \begin{equation*} f_n(t) = \det\left(\begin{array}{ccc}p_0(t)&\dots&p_n(t)\\ p'_0(t)&\dots&p'_n(t)\\ p''_0(t)&\dots&p''_n(t)\\ \vdots&\ddots&\vdots\\ p^n_0(t)&\dots&p^n_n(t)\end{array}\right) \end{equation*} Where $p_i(t)$ are polynomials of degree $\leq n$ and $p^k_i(t)$ is the $k$-th derivative. I'm trying to show that $f(t) = f(0), \forall t$. Now, $f(0)$, as far as I understand, works out to be the determinant of the coefficient matrix multiplied by some constant. I decided to try this by induction, but I'm stuck on the inductive step. This is what I have so far: We prove the general case, for $n \geq 1$ via induction on $n$. For our base case, let $n = 1$ and let $p_i(t) = a_{i1}t+a_{i0}$ for $i \in \{0,1\}$. We have \begin{equation*} f(t) = \det\left(\begin{array}{cc}a_{01}t+a_{00} & a_{11}t+a_{10}\\ a_{01}&a_{11}\end{array}\right)= (a_{01}t+a_{00})a_{11} - (a_{11}t+a_{10})a_{01} \\ = a_{01}a_{11} t-a_{01}    a_{11}t-a_{01} a_{10}+a_{00}    a_{11} = a_{00} a_{11}-a_{01} a_{10}. \end{equation*} Clearly, $f(t)$ does not depend on $t$, and so, $f(t) = f(0)$ for all $t$. Now, we assume that the result holds for all $i < k$. We show that this implies that the result holds for $k$ as follows. Let $p_i(t) = a_{ik}t^k+ \dots +a_{i0}$ for $i \in \{0,k\}$. We have \begin{equation*} f(t) = \det\left(\begin{array}{ccc}p_0(t)&\dots&p_k(t)\\ \vdots&\ddots&\vdots\\ p^k_0(t)&\dots&p^k_k(t)\end{array}\right)= \sum_{i=0}^k (-1)^{i}p_i(t)\det(M_{1,i+1}) = \sum_{i=0}^k (-1)^{i}p_i(t)f_{{k-1}_{1,i+1}}(0) \end{equation*} where $M_{m,n}$ is the minor matrix of the $m$-th row and $n$-th column and $f_{{k-1}_{1,i+1}}$ is $\det(M_{1,i+1})$ evaluated at $t = 0$. In our case, since $p_i(t)$ has degree at most $k$, $p'_i(t)$ has degree at most $k-1$, and so, for any $i$, $M_{1,i+1}$ is a matrix of the given form for $n = k-1$. By the induction hypothesis, for all $i$, $\det(M_{1,i+1})$ is a constant that does not depend on $t$. I'm not really sure how to proceed from here, why does it follow that the resulting equation does not depend on $t$? I guess it has something to do with the fact that we're multiplying each polynomial by the determinant (times some constant) of the remaining polynomials' coefficients (except the constant coefficient, $a_{i0}$, which is lost in the first derivative). I don't see any other links here that would allow me to proceed.",,"['linear-algebra', 'derivatives', 'polynomials', 'induction', 'determinant']"
50,Proof of Schwarz's theorem on partial derivatives for vector-valued functions,Proof of Schwarz's theorem on partial derivatives for vector-valued functions,,"I really appreciate your help in checking the below proof (especially the last section). Let $(E_{i})_{i=1 \dots n}$ and $F$ be normed vector spaces and $f: \prod{E_{i}}\rightarrow F$ a twice-differentiable function (no hypothesis is made on the continuity of partial derivatives). We want to prove that $\forall a,h,k \in \prod{E_{i}}, f''(a)(h)(k)=f''(a)(k)(h)$. We write $\Delta(a,h,k)=f(a+h+k)-f(a+h)-f(a+k)+f(a)$. Then: $||\Delta(a,h,k) - f''(a)(k)(h)|| \leq$ $||\Delta(a,h,k) - f'(a+k)(h)+f'(a)(h)||+||f'(a+k)(h)-f'(a)(h)-f''(a)(k)(h)||$ Let $(1)$ denote the first summand and $(2)$ the second. I can show that $(2)=o(||h||.||k||)$ using the definition of $f''$ therefore I will now try to do the same with $(1)$ (from there we can conclude by applying the same reasoning to $||\Delta(a,h,k) - f''(a)(h)(k)||$ and using the linearity of the derivatives: I am not interested in that part of the proof). $(1) = ||\Delta(a,h,k) - f'(a+k)(h)+f'(a)(h)||=$ $||\delta(a,h,k)-\delta(a,h,0)|| \leq $ $||k||.Sup_{x \in B(0,||k||)}{||f'(a+h+x)-f'(a+x)-f''(a+x)(h)||}$ where $\delta(a,h,k)= f(a+h+k)-f(a+k)-f'(a+k)(h)$, $B(0,||k||)$ the closed ball with center $0$ and radius $||k||$, and using the Mean Value theorem. Since $f'$ is differentiable it is continuous and the $Sup$ is reached at some point say $x0$. $(1) \leq ||k||.||f'(a+h+x0)-f'(a+x0)-f''(a+x0)(h)|| \leq $ $||k||.||f''(a+x0)(h)|| \leq$ $||k||.||h||.||f''(a+x0)|| = o(||h||.||k||)$ P.S.: Is there any useful, simple way to weaken the hypotheses?","I really appreciate your help in checking the below proof (especially the last section). Let $(E_{i})_{i=1 \dots n}$ and $F$ be normed vector spaces and $f: \prod{E_{i}}\rightarrow F$ a twice-differentiable function (no hypothesis is made on the continuity of partial derivatives). We want to prove that $\forall a,h,k \in \prod{E_{i}}, f''(a)(h)(k)=f''(a)(k)(h)$. We write $\Delta(a,h,k)=f(a+h+k)-f(a+h)-f(a+k)+f(a)$. Then: $||\Delta(a,h,k) - f''(a)(k)(h)|| \leq$ $||\Delta(a,h,k) - f'(a+k)(h)+f'(a)(h)||+||f'(a+k)(h)-f'(a)(h)-f''(a)(k)(h)||$ Let $(1)$ denote the first summand and $(2)$ the second. I can show that $(2)=o(||h||.||k||)$ using the definition of $f''$ therefore I will now try to do the same with $(1)$ (from there we can conclude by applying the same reasoning to $||\Delta(a,h,k) - f''(a)(h)(k)||$ and using the linearity of the derivatives: I am not interested in that part of the proof). $(1) = ||\Delta(a,h,k) - f'(a+k)(h)+f'(a)(h)||=$ $||\delta(a,h,k)-\delta(a,h,0)|| \leq $ $||k||.Sup_{x \in B(0,||k||)}{||f'(a+h+x)-f'(a+x)-f''(a+x)(h)||}$ where $\delta(a,h,k)= f(a+h+k)-f(a+k)-f'(a+k)(h)$, $B(0,||k||)$ the closed ball with center $0$ and radius $||k||$, and using the Mean Value theorem. Since $f'$ is differentiable it is continuous and the $Sup$ is reached at some point say $x0$. $(1) \leq ||k||.||f'(a+h+x0)-f'(a+x0)-f''(a+x0)(h)|| \leq $ $||k||.||f''(a+x0)(h)|| \leq$ $||k||.||h||.||f''(a+x0)|| = o(||h||.||k||)$ P.S.: Is there any useful, simple way to weaken the hypotheses?",,"['derivatives', 'proof-verification', 'normed-spaces']"
51,Applying the chain rule with the fundamental theorem of calculus 1,Applying the chain rule with the fundamental theorem of calculus 1,,"I have the following problem in which I have to apply both the chain rule and the FTC 1. I got the right answer, but i'm confused about what's really going. $$\frac{d}{dx} \int_1^{x^4} sec(t) \space dt $$ While trying to make sense of this, I have the following in mind. If  $g(x)=\int_a^{x}f(t)\space dt$ then     $g(x)^{'}=f(x)$ And $(f(g(x)){'}$= $f^{'}(g(x)) \times g^{'}(x)$ and $\frac{dy}{dx}= \frac{dy}{du} \times \frac{du}{dx}$ Applying these to the this integral, i'm trying to break everything down to see what is what. So that for example I know which function is nested in which function. I'm just looking for a clear and simple explanation of what's going on here.","I have the following problem in which I have to apply both the chain rule and the FTC 1. I got the right answer, but i'm confused about what's really going. $$\frac{d}{dx} \int_1^{x^4} sec(t) \space dt $$ While trying to make sense of this, I have the following in mind. If  $g(x)=\int_a^{x}f(t)\space dt$ then     $g(x)^{'}=f(x)$ And $(f(g(x)){'}$= $f^{'}(g(x)) \times g^{'}(x)$ and $\frac{dy}{dx}= \frac{dy}{du} \times \frac{du}{dx}$ Applying these to the this integral, i'm trying to break everything down to see what is what. So that for example I know which function is nested in which function. I'm just looking for a clear and simple explanation of what's going on here.",,"['calculus', 'integration', 'derivatives', 'chain-rule']"
52,"Show that $\exists m \in(k,\ell):f''(m)+f(m)=0$",Show that,"\exists m \in(k,\ell):f''(m)+f(m)=0","Let $f$ be twice differentiable on $\Bbb R$ and $f'(x)\not=0$, $f'(k)=f(\ell)$ and $f'(\ell)=f(k)$. Show that $\exists m \in(k,\ell):f''(m)+f(m)=0$. The only thing that I was able to do is to show that $\exists n \in (k,\ell):f'(n)-f(n)=0$. I'd like a hint.","Let $f$ be twice differentiable on $\Bbb R$ and $f'(x)\not=0$, $f'(k)=f(\ell)$ and $f'(\ell)=f(k)$. Show that $\exists m \in(k,\ell):f''(m)+f(m)=0$. The only thing that I was able to do is to show that $\exists n \in (k,\ell):f'(n)-f(n)=0$. I'd like a hint.",,"['calculus', 'derivatives']"
53,Truncation error and the second centered difference approximation $\frac{d^2u}{dx^2}$ at $x = x_{j}$ ?.,Truncation error and the second centered difference approximation  at  ?.,\frac{d^2u}{dx^2} x = x_{j},"The second centered difference approximation $\frac{d^2u}{dx^2}$ at $x = x_{j}$?. By expanding the terms $u(x_{j}+h)$ and $u(x_{j} - h)$ about the point $x_{j}$ with a Taylor series,we need to prove that the truncation error $T_{j}$ in the approximation satisfies $T_{j} =  -\frac{h^2}{12}\frac{d^4 u(x_{j})}{dx^4} +$ terms with higher powers of $h$. From central difference approximation $$u'(x_{j}) = \frac{u(x_{j}+h) - u(x_{j}-h)}{2h}$$ $$u''(x_{j}) = \frac{u'(x_{j}+h) - u'(x_{j}-h)}{2h}$$ $$u''(x_{j}) = \frac{u(x_{j}+2h) - 2u(x_{j}) + u(x_{j}-2h)}{4h^2}$$ From Taylor series expansion $u(x_{j}+2h) = u(x_{j}) + 2hu'(x_{j}) + \frac{(2h)^2}{2!}u''(x_{j})+ \frac{(2h)^3}{3!}u'''(x_{j}) + \frac{(2h)^4}{4!}u''''(x_{j})+h.o.t$ $u(x_{j}-2h) = u(x_{j}) - 2hu'(x_{j}) + \frac{(2h)^2}{2!}u''(x_{j})- \frac{(2h)^3}{3!}u'''(x_{j}) + \frac{(2h)^4}{4!}u''''(x_{j}) -h.o.t$ So that $$u(x_{j}+2h)+u(x_{j}-2h)-2u(x_{j}) = 4h^2u''(x_{j})+\frac{4}{3}h^4 u''''(x_{j})+...$$ $$u''(x_{j}) = \frac{u(x_{j}+2h)+u(x_{j}-2h)-2u(x_{j})}{4h^2} = u''(x_{j})+\frac{1}{3}h^2 u''''(x_{j})+...$$ Now i get that error $T_{j} = \frac{1}{3}h^2 u''''(x_{j})$  but I am not getting the required one $T_{j} = -\frac{h^2}{12} u''''(x_{j})$. Where am I doing wrong?","The second centered difference approximation $\frac{d^2u}{dx^2}$ at $x = x_{j}$?. By expanding the terms $u(x_{j}+h)$ and $u(x_{j} - h)$ about the point $x_{j}$ with a Taylor series,we need to prove that the truncation error $T_{j}$ in the approximation satisfies $T_{j} =  -\frac{h^2}{12}\frac{d^4 u(x_{j})}{dx^4} +$ terms with higher powers of $h$. From central difference approximation $$u'(x_{j}) = \frac{u(x_{j}+h) - u(x_{j}-h)}{2h}$$ $$u''(x_{j}) = \frac{u'(x_{j}+h) - u'(x_{j}-h)}{2h}$$ $$u''(x_{j}) = \frac{u(x_{j}+2h) - 2u(x_{j}) + u(x_{j}-2h)}{4h^2}$$ From Taylor series expansion $u(x_{j}+2h) = u(x_{j}) + 2hu'(x_{j}) + \frac{(2h)^2}{2!}u''(x_{j})+ \frac{(2h)^3}{3!}u'''(x_{j}) + \frac{(2h)^4}{4!}u''''(x_{j})+h.o.t$ $u(x_{j}-2h) = u(x_{j}) - 2hu'(x_{j}) + \frac{(2h)^2}{2!}u''(x_{j})- \frac{(2h)^3}{3!}u'''(x_{j}) + \frac{(2h)^4}{4!}u''''(x_{j}) -h.o.t$ So that $$u(x_{j}+2h)+u(x_{j}-2h)-2u(x_{j}) = 4h^2u''(x_{j})+\frac{4}{3}h^4 u''''(x_{j})+...$$ $$u''(x_{j}) = \frac{u(x_{j}+2h)+u(x_{j}-2h)-2u(x_{j})}{4h^2} = u''(x_{j})+\frac{1}{3}h^2 u''''(x_{j})+...$$ Now i get that error $T_{j} = \frac{1}{3}h^2 u''''(x_{j})$  but I am not getting the required one $T_{j} = -\frac{h^2}{12} u''''(x_{j})$. Where am I doing wrong?",,"['derivatives', 'numerical-methods', 'taylor-expansion', 'finite-differences', 'truncation-error']"
54,matrix differentiation of lp norm,matrix differentiation of lp norm,,"I'm trying to figure out what is the gradient [matrix differentiation] of the lp norm. I saw a few other posts regarding the specific case when for example when p=2 so I think that: $$\frac{\partial d}{\partial x} ||Ax - b||_2  = A$$ but what about for different arbitrary values of p? In that case, what is$\frac{\partial d}{\partial x} ||Ax - b||_p $, where subscript _p means lp-norm?","I'm trying to figure out what is the gradient [matrix differentiation] of the lp norm. I saw a few other posts regarding the specific case when for example when p=2 so I think that: $$\frac{\partial d}{\partial x} ||Ax - b||_2  = A$$ but what about for different arbitrary values of p? In that case, what is$\frac{\partial d}{\partial x} ||Ax - b||_p $, where subscript _p means lp-norm?",,"['derivatives', 'partial-derivative', 'normed-spaces', 'matrix-calculus', 'least-squares']"
55,Partial derivative of this summation?,Partial derivative of this summation?,,I'm kind of confused about the rules for partially differentiating summations...what would the partial derivative of this be wrt to beta? Thank you! $\frac{\partial}{\partial \beta} \sum_{i=1}^n -2x_iy_i+2\beta x_i^2$,I'm kind of confused about the rules for partially differentiating summations...what would the partial derivative of this be wrt to beta? Thank you! $\frac{\partial}{\partial \beta} \sum_{i=1}^n -2x_iy_i+2\beta x_i^2$,,"['calculus', 'derivatives', 'partial-derivative']"
56,How to find $n^{th}$ derivative of $e^{2x}\cos^2x \sin x$?,How to find  derivative of ?,n^{th} e^{2x}\cos^2x \sin x,Given : $y=e^{2x} \times \cos^2x \times \sin x$ and I have to find it's $n^{th}$ derivative I have managed to break $y$ down to: $y= \frac{1}{4} \times (e^{2x}.\sin 3x + e^{2x}.\sin x)$ But I don't know how to apply Leibnitz Rule in cases like $(e^{ax} \times \sin bx)$. Please guide,Given : $y=e^{2x} \times \cos^2x \times \sin x$ and I have to find it's $n^{th}$ derivative I have managed to break $y$ down to: $y= \frac{1}{4} \times (e^{2x}.\sin 3x + e^{2x}.\sin x)$ But I don't know how to apply Leibnitz Rule in cases like $(e^{ax} \times \sin bx)$. Please guide,,"['calculus', 'derivatives']"
57,Roots of $p(x)+xp'(x)$,Roots of,p(x)+xp'(x),"Let $p(x)$ be a polynomial of degree $n$ such that it has no real root (that is, $n$ is an even positive number). Can we say anything about the roots of the polynomial $p(x)+xp'(x)$? Here $p'(x)$ denotes the derivative of $p(x)$ with respect to $x$.","Let $p(x)$ be a polynomial of degree $n$ such that it has no real root (that is, $n$ is an even positive number). Can we say anything about the roots of the polynomial $p(x)+xp'(x)$? Here $p'(x)$ denotes the derivative of $p(x)$ with respect to $x$.",,"['derivatives', 'polynomials', 'roots']"
58,$f(x) = \arctan x | x\in R$.,.,f(x) = \arctan x | x\in R,$f(x) = \arctan x | x\in R$. Then 1) $f^{(n)}(0) = 0$ for all positive even integers n. 2)the sequence ${f^{(n)} (0)}$ is unbounded. Are they correct? How can derivative of more than three degree be computed?,$f(x) = \arctan x | x\in R$. Then 1) $f^{(n)}(0) = 0$ for all positive even integers n. 2)the sequence ${f^{(n)} (0)}$ is unbounded. Are they correct? How can derivative of more than three degree be computed?,,['calculus']
59,Derivative of vector product with respect to a vector [duplicate],Derivative of vector product with respect to a vector [duplicate],,"This question already has answers here : Vector derivative w.r.t its transpose $\frac{d(Ax)}{d(x^T)}$ (5 answers) Closed 7 years ago . I would like to derive the following expression with respect to a vector $x \in \mathcal{R}^N$: $a a^T b$  where $a, b$ are vectors in $\mathcal{R}^M$. My idea is to apply the following $ \frac{\partial a}{\partial x} a^T b + a (\frac{\partial a}{\partial x})^T b + a a^T \frac{\partial b}{\partial x}$ but it is dimensionally wrong. In fact: $\frac{\partial a}{\partial x} a^T b$ has dimension: $(MxN) (NxM) (Mx1)$ or $a (\frac{\partial a}{\partial x})^T b$ has dimension: $(Mx1) (NxM) (Mx1)$ that are really dimensionally wrong. I'm assuming that the derivative of a vector $a \in \mathcal{R}^M$ with respect to another vector $a \in \mathcal{R}^N$ is a matrix $a \in \mathcal{R}^{MxN}$. Moreover, even in the very simple example: I want to derive $a^T b$ with respect to $x$: $(\frac{\partial a}{\partial x})^T b + a^T \frac{\partial b}{\partial x}$, the first addendum seems to be a column vector in $\mathcal{R}^N$ while the second one is a row vector in $\mathcal{R}^N$. What am I doing wrong? I know I can fix this by transposing the function $a^T b$ when partially deriving with respect to $a$ (then the first addendum becomes $b^T \frac{\partial a}{\partial x}$) but I would like to know if there is a generic rule for that. Thank you to anyone that can provide to me some hints. Marco My question is a bit different with respect to this post since that is the differentiation about the transpose of a vector while here I'm questioning about the dimensionality of vector product derivative. Moreover, I add a dummy question that may help me in solving this: In case of a derivative of scalar-vector product, i.e. $\frac{\partial (c a)}{\partial x}$, with $c \in \mathcal{R}, a \in \mathcal{R}^M$ and $x \in \mathcal{R}^N$, I would say that the derivative is $\frac{\partial (c a)}{\partial x} = \frac{\partial c}{\partial x} a + c \frac{\partial a}{\partial x}$. Now, the first addendum has a wrong dimensionality: (1xM) (Nx1). Now, I know that the things works if I do: $\frac{\partial (c a)}{\partial x} = a \frac{\partial c}{\partial x} + c \frac{\partial a}{\partial x}$, i.e., if I set the scalar on the right side before deriving with respect to $c$. In this case I know that, for dimensionality, $a c$ is more correct than doing $c a$ but I would like to know if there are some rules that I'm aware of. Then my question is: why do I have to do this?","This question already has answers here : Vector derivative w.r.t its transpose $\frac{d(Ax)}{d(x^T)}$ (5 answers) Closed 7 years ago . I would like to derive the following expression with respect to a vector $x \in \mathcal{R}^N$: $a a^T b$  where $a, b$ are vectors in $\mathcal{R}^M$. My idea is to apply the following $ \frac{\partial a}{\partial x} a^T b + a (\frac{\partial a}{\partial x})^T b + a a^T \frac{\partial b}{\partial x}$ but it is dimensionally wrong. In fact: $\frac{\partial a}{\partial x} a^T b$ has dimension: $(MxN) (NxM) (Mx1)$ or $a (\frac{\partial a}{\partial x})^T b$ has dimension: $(Mx1) (NxM) (Mx1)$ that are really dimensionally wrong. I'm assuming that the derivative of a vector $a \in \mathcal{R}^M$ with respect to another vector $a \in \mathcal{R}^N$ is a matrix $a \in \mathcal{R}^{MxN}$. Moreover, even in the very simple example: I want to derive $a^T b$ with respect to $x$: $(\frac{\partial a}{\partial x})^T b + a^T \frac{\partial b}{\partial x}$, the first addendum seems to be a column vector in $\mathcal{R}^N$ while the second one is a row vector in $\mathcal{R}^N$. What am I doing wrong? I know I can fix this by transposing the function $a^T b$ when partially deriving with respect to $a$ (then the first addendum becomes $b^T \frac{\partial a}{\partial x}$) but I would like to know if there is a generic rule for that. Thank you to anyone that can provide to me some hints. Marco My question is a bit different with respect to this post since that is the differentiation about the transpose of a vector while here I'm questioning about the dimensionality of vector product derivative. Moreover, I add a dummy question that may help me in solving this: In case of a derivative of scalar-vector product, i.e. $\frac{\partial (c a)}{\partial x}$, with $c \in \mathcal{R}, a \in \mathcal{R}^M$ and $x \in \mathcal{R}^N$, I would say that the derivative is $\frac{\partial (c a)}{\partial x} = \frac{\partial c}{\partial x} a + c \frac{\partial a}{\partial x}$. Now, the first addendum has a wrong dimensionality: (1xM) (Nx1). Now, I know that the things works if I do: $\frac{\partial (c a)}{\partial x} = a \frac{\partial c}{\partial x} + c \frac{\partial a}{\partial x}$, i.e., if I set the scalar on the right side before deriving with respect to $c$. In this case I know that, for dimensionality, $a c$ is more correct than doing $c a$ but I would like to know if there are some rules that I'm aware of. Then my question is: why do I have to do this?",,"['derivatives', 'partial-derivative', 'vector-analysis']"
60,derivative of $e^x$ using the limit definition [duplicate],derivative of  using the limit definition [duplicate],e^x,"This question already has answers here : Using the Limit definition to find the derivative of $e^x$ (7 answers) Closed 7 years ago . Looking to prove  $ f(x) = e^x =f'(x) $ using the limit definition of the derivative. So far I have gotten to here: $e^x * \lim\limits_{h \to 0} \frac{e^h-1}{h}$ Any hints on what I can do next to get that entire limit to $1$? I can't use l'hopital's rule, otherwise that'd defeat the purpose.","This question already has answers here : Using the Limit definition to find the derivative of $e^x$ (7 answers) Closed 7 years ago . Looking to prove  $ f(x) = e^x =f'(x) $ using the limit definition of the derivative. So far I have gotten to here: $e^x * \lim\limits_{h \to 0} \frac{e^h-1}{h}$ Any hints on what I can do next to get that entire limit to $1$? I can't use l'hopital's rule, otherwise that'd defeat the purpose.",,"['calculus', 'derivatives', 'limits-without-lhopital']"
61,Prove that $\sum_{k=0}^n \frac{(-1)^{(n+k)}\sum_{r=0}^n a_rk^r}{k!(n-k)!}=a_n\quad \forall n \in \mathbb{N_0} $,Prove that,\sum_{k=0}^n \frac{(-1)^{(n+k)}\sum_{r=0}^n a_rk^r}{k!(n-k)!}=a_n\quad \forall n \in \mathbb{N_0} ,"Can Someone prove that the following equation is true? I tried to prove it using induction but I didn't get very far. $$\sum_{k=0}^n \frac{(-1)^{(n+k)}\sum_{r=0}^n a_rk^r}{k!(n-k)!}=a_n\quad \forall n \in \mathbb{N_0} $$ Where $\sum_{r=0}^n a_rk^r$ is a polynomial of degree $n$ and $a_n$ is the coefficient of the highest power of that polynomial. I know that for $k=0$ and $r=0$ you get $0^0$ which ofcourse is undefined. But can we just say for the sake of simplicity that $0^0=1$? $$\sum_{k=0}^n \frac{(-1)^{(n+k)}p(k)}{k!(n-k)!}=\frac{\partial^n p}{\partial k^nn!} \quad \forall n \in \mathbb{N_0} $$ Where $p(k)$ is a polynomial of degree $n$ should be an equivalent statement. This is my first question here by the way. If I made some mistakes, please let me know.","Can Someone prove that the following equation is true? I tried to prove it using induction but I didn't get very far. $$\sum_{k=0}^n \frac{(-1)^{(n+k)}\sum_{r=0}^n a_rk^r}{k!(n-k)!}=a_n\quad \forall n \in \mathbb{N_0} $$ Where $\sum_{r=0}^n a_rk^r$ is a polynomial of degree $n$ and $a_n$ is the coefficient of the highest power of that polynomial. I know that for $k=0$ and $r=0$ you get $0^0$ which ofcourse is undefined. But can we just say for the sake of simplicity that $0^0=1$? $$\sum_{k=0}^n \frac{(-1)^{(n+k)}p(k)}{k!(n-k)!}=\frac{\partial^n p}{\partial k^nn!} \quad \forall n \in \mathbb{N_0} $$ Where $p(k)$ is a polynomial of degree $n$ should be an equivalent statement. This is my first question here by the way. If I made some mistakes, please let me know.",,"['derivatives', 'summation', 'induction', 'interpolation', 'lagrange-interpolation']"
62,"$f:I\to \mathbb{R}$ is a differentiable path. If $a\in I$ is an accumulation point of $f^{-1}(v)$, then $f'(a) = 0$","is a differentiable path. If  is an accumulation point of , then",f:I\to \mathbb{R} a\in I f^{-1}(v) f'(a) = 0,"I need to show that: Let $f:I\to \mathbb{R}^n$ be a differentiable path. If $a\in I$ is an   accumulation point of the set $f^{-1}(v)$ for some $v\in  \mathbb{R}^n$, then $f'(a) = 0$ *ps: for a differentiable path we mean a function $f:I\subset \mathbb{R}\to \mathbb{R}^n$ such that its cordinates are differentiable If for a given $v$, it's set $f^{-1}(v)$ has an accumulation point, it means that there is more than one point $a$ such that $f(a) = v$, right? Because for definition, an accumulation point $a$ is a point $a$ not necessarily in $I$ (but here it's in $I$) such that every ball centered in $a$ also touches another point of $I$. So I imagine that there is at least one more point different from $a$ such that $f(a) = v$, and this point, or points, are near $a$. I know that since they're infinitely close, the derivative at point $a$ would be $0$ because it's like the function were constant at that point, but I don't know how to write it. Could somebody help me?","I need to show that: Let $f:I\to \mathbb{R}^n$ be a differentiable path. If $a\in I$ is an   accumulation point of the set $f^{-1}(v)$ for some $v\in  \mathbb{R}^n$, then $f'(a) = 0$ *ps: for a differentiable path we mean a function $f:I\subset \mathbb{R}\to \mathbb{R}^n$ such that its cordinates are differentiable If for a given $v$, it's set $f^{-1}(v)$ has an accumulation point, it means that there is more than one point $a$ such that $f(a) = v$, right? Because for definition, an accumulation point $a$ is a point $a$ not necessarily in $I$ (but here it's in $I$) such that every ball centered in $a$ also touches another point of $I$. So I imagine that there is at least one more point different from $a$ such that $f(a) = v$, and this point, or points, are near $a$. I know that since they're infinitely close, the derivative at point $a$ would be $0$ because it's like the function were constant at that point, but I don't know how to write it. Could somebody help me?",,"['calculus', 'real-analysis', 'derivatives']"
63,Prove $\lim_{x \rightarrow 0} \frac{f(x+g(x))-f(g(x))}{x}=f'(0)$,Prove,\lim_{x \rightarrow 0} \frac{f(x+g(x))-f(g(x))}{x}=f'(0),"Given $f:\mathbb{R}\rightarrow \mathbb{R}$ differentiable continuously, and $g:\mathbb{R}\rightarrow \mathbb{R} \ s.t.\lim_{x \rightarrow 0}g(x)=0$ $$\text{Prove:} \lim_{x \rightarrow 0} \frac{f(x+g(x))-f(g(x))}{x}=f'(0) $$ $f'(x)$ is continuous $\Rightarrow\lim_{x \rightarrow 0}f'(g(x))=f'(\lim_{x \rightarrow 0}g(x))=f'(0) $ By definition of the derivative: $\lim_{x \rightarrow 0} \frac{f(x+0)-f(0)}{x}=f'(0)$ How can I continue to get the result? Any help appreciated.","Given $f:\mathbb{R}\rightarrow \mathbb{R}$ differentiable continuously, and $g:\mathbb{R}\rightarrow \mathbb{R} \ s.t.\lim_{x \rightarrow 0}g(x)=0$ $$\text{Prove:} \lim_{x \rightarrow 0} \frac{f(x+g(x))-f(g(x))}{x}=f'(0) $$ $f'(x)$ is continuous $\Rightarrow\lim_{x \rightarrow 0}f'(g(x))=f'(\lim_{x \rightarrow 0}g(x))=f'(0) $ By definition of the derivative: $\lim_{x \rightarrow 0} \frac{f(x+0)-f(0)}{x}=f'(0)$ How can I continue to get the result? Any help appreciated.",,"['calculus', 'derivatives']"
64,Show that $f$ is differentiable at $0$ and $f'(0)=0$.,Show that  is differentiable at  and .,f 0 f'(0)=0,"A function $f: \mathbb{R}\rightarrow \mathbb{R}$ is defined by   $$     f(x)=  \begin{cases}     0,& \text{if } ~x=0 ~~\text{or}~~ x ~\text{is irrational} \\     \frac{1}{q^3}, &   \text{if}~~ x=\frac{p}{q} ~~ \text{where}~~ p\in \mathbb{Z}, q\in \mathbb{N}~~ \text{and} \gcd(p,q)=1.          \end{cases} $$ Show that $f$ is differentiable at $0$ and $f'(0)=0$. A hints to the problem is given as  $0\le |\frac{f(x)}{x}|\le x^2$ for $x \ne 0.$ I don't know how to solve this problem. Please help.","A function $f: \mathbb{R}\rightarrow \mathbb{R}$ is defined by   $$     f(x)=  \begin{cases}     0,& \text{if } ~x=0 ~~\text{or}~~ x ~\text{is irrational} \\     \frac{1}{q^3}, &   \text{if}~~ x=\frac{p}{q} ~~ \text{where}~~ p\in \mathbb{Z}, q\in \mathbb{N}~~ \text{and} \gcd(p,q)=1.          \end{cases} $$ Show that $f$ is differentiable at $0$ and $f'(0)=0$. A hints to the problem is given as  $0\le |\frac{f(x)}{x}|\le x^2$ for $x \ne 0.$ I don't know how to solve this problem. Please help.",,"['calculus', 'real-analysis', 'derivatives']"
65,Derivative of $e^y=x^{\ln x}$?,Derivative of ?,e^y=x^{\ln x},"How to obtain $y'$ from $e^{y}=x^{\ln x}$? This is what I did: $$\ln e^y = \ln x^{\ln x}$$ $$y = \ln ^2 x$$ $$y' = \frac{2 \ln x}{x}$$ Is this correct? When I compare it with an online derivative calculator, the result they gave was different, and they used implicit differentiation instead.","How to obtain $y'$ from $e^{y}=x^{\ln x}$? This is what I did: $$\ln e^y = \ln x^{\ln x}$$ $$y = \ln ^2 x$$ $$y' = \frac{2 \ln x}{x}$$ Is this correct? When I compare it with an online derivative calculator, the result they gave was different, and they used implicit differentiation instead.",,"['derivatives', 'implicit-differentiation']"
66,Proof for sum of this infinite derivative series....,Proof for sum of this infinite derivative series....,,"I was recently watching a Numberphile Video when I saw this 'formula' : $$S = \sum \limits_{n=1} ^ {\infty} n\,x^{n-1} = \dfrac {1} {\big(x-1\big)^2} \,\,\,\forall\,\,\, |x| < 1$$ I was amazed to see this and it immediately struck to my mind that it is really an infinite derivative series: $$S=\sum \limits_{n=1}^{\infty}\dfrac {d}{dx} x^n = \dfrac {d}{dx}\sum \limits_{n=1}^{\infty}x^n$$ Now applying the sum for infinite Geometric Series when $|r|<1$ ($r = $ common ratio)  : $$S = \dfrac{d}{dx}\dfrac{1}{1-x} = \dfrac{1}{\big(1-x\big)^2}$$ I am really a beginner at Calculus, so the first question I want to ask is : Is my derivation for the sum $S$ mathematically correct ? Though my final expression comes out good, there still might be chance of some mistake, so please help me. Also, Is there any other way of deriving this result besides Calculus? Is this result even linked to Calculus in any way? I can clearly see that the common ratio $r$ for $<nx^{n-1}>$ is $r=\dfrac{n+1}{n}x$, but unfortunately $n$ is variable and not a fixed constant and also $|r| < 1$ does not necessarily hold for me to be able to compute the infinte sum. Can anyone help ?","I was recently watching a Numberphile Video when I saw this 'formula' : $$S = \sum \limits_{n=1} ^ {\infty} n\,x^{n-1} = \dfrac {1} {\big(x-1\big)^2} \,\,\,\forall\,\,\, |x| < 1$$ I was amazed to see this and it immediately struck to my mind that it is really an infinite derivative series: $$S=\sum \limits_{n=1}^{\infty}\dfrac {d}{dx} x^n = \dfrac {d}{dx}\sum \limits_{n=1}^{\infty}x^n$$ Now applying the sum for infinite Geometric Series when $|r|<1$ ($r = $ common ratio)  : $$S = \dfrac{d}{dx}\dfrac{1}{1-x} = \dfrac{1}{\big(1-x\big)^2}$$ I am really a beginner at Calculus, so the first question I want to ask is : Is my derivation for the sum $S$ mathematically correct ? Though my final expression comes out good, there still might be chance of some mistake, so please help me. Also, Is there any other way of deriving this result besides Calculus? Is this result even linked to Calculus in any way? I can clearly see that the common ratio $r$ for $<nx^{n-1}>$ is $r=\dfrac{n+1}{n}x$, but unfortunately $n$ is variable and not a fixed constant and also $|r| < 1$ does not necessarily hold for me to be able to compute the infinte sum. Can anyone help ?",,"['calculus', 'sequences-and-series']"
67,Derivative and limit of $\sqrt[3]{x-\sin x}$,Derivative and limit of,\sqrt[3]{x-\sin x},"Given $f:R\rightarrow R, f(x) = \sqrt[3]{x-\sin x}$, compute $f'(0)$.Now, this can be done by using the definition of the derivative : $$f'(x_0) = \lim_{x\to{x_0}} \frac{f(x)-f(x_0)}{x-x_0}$$This yields the right answer.However, why can't we derive the function and then plug in 0? I mean: $$f'(x) = \frac{1-\cos x}{3\sqrt[3]{(x-\sin x)^2}}$$ And this is undefined for $x = 0$. Why does this happen?","Given $f:R\rightarrow R, f(x) = \sqrt[3]{x-\sin x}$, compute $f'(0)$.Now, this can be done by using the definition of the derivative : $$f'(x_0) = \lim_{x\to{x_0}} \frac{f(x)-f(x_0)}{x-x_0}$$This yields the right answer.However, why can't we derive the function and then plug in 0? I mean: $$f'(x) = \frac{1-\cos x}{3\sqrt[3]{(x-\sin x)^2}}$$ And this is undefined for $x = 0$. Why does this happen?",,['derivatives']
68,How to simplify derivatives,How to simplify derivatives,,The math problem asks to find the derivative of the function   $$y=(x+1)^4(x+5)^2$$ I get to the part $$(x+1)^4 \cdot 2(x+5) + (x+5)^2 \cdot 4(x+1)^3$$ How do they arrive at the answer $$2(x+1)^3(x+5)(3x+11) ?$$,The math problem asks to find the derivative of the function   $$y=(x+1)^4(x+5)^2$$ I get to the part $$(x+1)^4 \cdot 2(x+5) + (x+5)^2 \cdot 4(x+1)^3$$ How do they arrive at the answer $$2(x+1)^3(x+5)(3x+11) ?$$,,['derivatives']
69,Struggling with related rates,Struggling with related rates,,"I'm trying to understand how to deal with related rates and here is my problem. When a rocket is 2 km high it is moving vertically at 300 km/hr. At this time, how fast is the angle of elevation increasing as seen by a person on the ground 5km from the launchpad? la picture So we have $$\frac{dy}{dt}=300 km/hr$$ $$y=2 km$$ $$x=5km$$ And we have to find $$\frac{d\Theta}{dt}=?$$ My solution is next: $$\sin(\Theta)=\frac{y}{\sqrt{29}}$$ where $\sqrt{29}$ is the hypotenuse Then i take derivative of this thing $$\cos(\Theta)*\frac{d\Theta}{dt} = \frac{\frac{dy}{dt}}{\sqrt{29}}$$ We know, that $\cos(\Theta)$ is $\frac{5}{\sqrt{29}}$ and $\frac{dy}{dt}$ is $300$ so that we have $$\frac{5}{\sqrt{29}}*\frac{d\Theta}{dt}=\frac{300}{\sqrt{29}}$$ so that $$\frac{d\Theta}{dt}=\frac{300}{\sqrt{29}}*\frac{\sqrt{29}}{5}$$ $$\frac{d\Theta}{dt}=60$$ But it doesn't look like the right answer. Could you explain what did I do wrong, please?","I'm trying to understand how to deal with related rates and here is my problem. When a rocket is 2 km high it is moving vertically at 300 km/hr. At this time, how fast is the angle of elevation increasing as seen by a person on the ground 5km from the launchpad? la picture So we have $$\frac{dy}{dt}=300 km/hr$$ $$y=2 km$$ $$x=5km$$ And we have to find $$\frac{d\Theta}{dt}=?$$ My solution is next: $$\sin(\Theta)=\frac{y}{\sqrt{29}}$$ where $\sqrt{29}$ is the hypotenuse Then i take derivative of this thing $$\cos(\Theta)*\frac{d\Theta}{dt} = \frac{\frac{dy}{dt}}{\sqrt{29}}$$ We know, that $\cos(\Theta)$ is $\frac{5}{\sqrt{29}}$ and $\frac{dy}{dt}$ is $300$ so that we have $$\frac{5}{\sqrt{29}}*\frac{d\Theta}{dt}=\frac{300}{\sqrt{29}}$$ so that $$\frac{d\Theta}{dt}=\frac{300}{\sqrt{29}}*\frac{\sqrt{29}}{5}$$ $$\frac{d\Theta}{dt}=60$$ But it doesn't look like the right answer. Could you explain what did I do wrong, please?",,"['calculus', 'derivatives']"
70,Theorem of Cauchy,Theorem of Cauchy,,"Let $f:[a,b] \to \mathbb{R}$ be continuous and differentiable in $(a,b)$. Prove that there exists $ \alpha\in (a,b)$ such that $$\frac{b f(a) -a f(b)}{b-a} = f(\alpha) - \alpha f'(\alpha).$$","Let $f:[a,b] \to \mathbb{R}$ be continuous and differentiable in $(a,b)$. Prove that there exists $ \alpha\in (a,b)$ such that $$\frac{b f(a) -a f(b)}{b-a} = f(\alpha) - \alpha f'(\alpha).$$",,"['real-analysis', 'derivatives']"
71,How to prove that the following function is maximized when $p = q$?,How to prove that the following function is maximized when ?,p = q,"Given the following function: \begin{equation} f(q,p) = q\log(2p) + (1 - q)\log(2 - 2p) \end{equation} Given some $q \in [0,1]$ we need to prove that $f(q, p)$ is maximized when $p = q$. How to prove this?","Given the following function: \begin{equation} f(q,p) = q\log(2p) + (1 - q)\log(2 - 2p) \end{equation} Given some $q \in [0,1]$ we need to prove that $f(q, p)$ is maximized when $p = q$. How to prove this?",,"['derivatives', 'optimization', 'partial-derivative', 'maxima-minima']"
72,Differential Geometry (Curves): $\alpha(s)=\lambda(s)n(s)\implies \lambda$ differentiable?,Differential Geometry (Curves):  differentiable?,\alpha(s)=\lambda(s)n(s)\implies \lambda,"Here, differentiable means $C^\infty$. Let $I\subset \Bbb{R}$ be an open interval and $\alpha:I\to \Bbb{R}^3$ be a regular curve parameterized by arc length ($|\alpha'(s)|=1,\forall s\in I$) with curvature $|\alpha''(s)|=k(s)>0,\forall s\in I$. Define $n(s)$ as the normal vector, by $n(s)=\dfrac{1}{k(s)}\alpha''(s)$, which we also know to be differentiable, i.e. $n:I\to \Bbb{R}^3$ is differentiable. We also denote $b$ the binormal vector, $\tau$ the torsion, etc. Suppose then that all the normal lines to the curve meet together at the origin. This implies that, for each $s\in S$, there exists a unique $\lambda(s)\in \Bbb{R}$ such that $\alpha(s)=\lambda(s)n(s)$. So, I can define a function $\lambda:I\to \Bbb{R}$ such that each $\lambda(s)$ is as above. Question: Is $\lambda:I\to \Bbb{R}$ differentiable? Why? I know that $\alpha$ and $n$ are differentiable, but does this $\alpha=\lambda n$ alone suffice to know that $\lambda$ is differentiable? I need this in order to show that $\alpha$ is in fact a part of a circle. At Manfredo's Differentiable Geometry of Curves and Surfaces this is the exercise 4, of §1.5 and he suggests simply to take the derivative of $\alpha=\lambda n$, (which furthermore he claims to be constant!) to get $$(1-\lambda k)t+\lambda'n-\lambda \tau b=0,$$ but I don't know if it is allowed to take $\lambda'$... Edit: to say that $\alpha(s)=\lambda(s)n(s)=const.$ seems wrong, since I'm trying to prove that $\alpha(s)$ lies on a circle. Maybe the author did mean that $\lambda(s)=const.$ (?)","Here, differentiable means $C^\infty$. Let $I\subset \Bbb{R}$ be an open interval and $\alpha:I\to \Bbb{R}^3$ be a regular curve parameterized by arc length ($|\alpha'(s)|=1,\forall s\in I$) with curvature $|\alpha''(s)|=k(s)>0,\forall s\in I$. Define $n(s)$ as the normal vector, by $n(s)=\dfrac{1}{k(s)}\alpha''(s)$, which we also know to be differentiable, i.e. $n:I\to \Bbb{R}^3$ is differentiable. We also denote $b$ the binormal vector, $\tau$ the torsion, etc. Suppose then that all the normal lines to the curve meet together at the origin. This implies that, for each $s\in S$, there exists a unique $\lambda(s)\in \Bbb{R}$ such that $\alpha(s)=\lambda(s)n(s)$. So, I can define a function $\lambda:I\to \Bbb{R}$ such that each $\lambda(s)$ is as above. Question: Is $\lambda:I\to \Bbb{R}$ differentiable? Why? I know that $\alpha$ and $n$ are differentiable, but does this $\alpha=\lambda n$ alone suffice to know that $\lambda$ is differentiable? I need this in order to show that $\alpha$ is in fact a part of a circle. At Manfredo's Differentiable Geometry of Curves and Surfaces this is the exercise 4, of §1.5 and he suggests simply to take the derivative of $\alpha=\lambda n$, (which furthermore he claims to be constant!) to get $$(1-\lambda k)t+\lambda'n-\lambda \tau b=0,$$ but I don't know if it is allowed to take $\lambda'$... Edit: to say that $\alpha(s)=\lambda(s)n(s)=const.$ seems wrong, since I'm trying to prove that $\alpha(s)$ lies on a circle. Maybe the author did mean that $\lambda(s)=const.$ (?)",,"['differential-geometry', 'derivatives']"
73,Tangent plane to the surface equation,Tangent plane to the surface equation,,"Find an equation of the tangent plane to the surface $$(u,v)=\langle u^2+v^2,2uv,u^3\rangle$$at the point$$(5,4,1)$$ i know the tangent equation, you need the partial of x and y.. plug in the points. plug back into equation and bam, easy and done. but thats when you are given a normal function... why does this look so confusing? do i have to calculate something else? edit: I am not sure if i am over thinking it, or if what i did is correct. please help clarify $$ f_u=2u+2v+3u^2 $$ $$f_v=2v+2u$$plugged 5 and 4 in (for u and v) $$\langle 10,8,75\rangle\langle 8,10,0\rangle$$ took the cross product to find a vector.. and thats what i used in the tangent equation. Did i go about this correctly..?","Find an equation of the tangent plane to the surface $$(u,v)=\langle u^2+v^2,2uv,u^3\rangle$$at the point$$(5,4,1)$$ i know the tangent equation, you need the partial of x and y.. plug in the points. plug back into equation and bam, easy and done. but thats when you are given a normal function... why does this look so confusing? do i have to calculate something else? edit: I am not sure if i am over thinking it, or if what i did is correct. please help clarify $$ f_u=2u+2v+3u^2 $$ $$f_v=2v+2u$$plugged 5 and 4 in (for u and v) $$\langle 10,8,75\rangle\langle 8,10,0\rangle$$ took the cross product to find a vector.. and thats what i used in the tangent equation. Did i go about this correctly..?",,"['calculus', 'derivatives', 'partial-derivative']"
74,Find the $n$ derivative of $y= e^{2x}\sin^2 x$,Find the  derivative of,n y= e^{2x}\sin^2 x,"We have \begin{align*} y&= e^{2x}\sin^2 x\\ &= e^{2x}\left(\frac{1-\cos 2x}{2}\right)\\ &= \frac{e^{2x}}{2} - \frac{e^{2x}\cos 2x}{2} \end{align*} Then \begin{align*} y^{(n)} &= \left(\frac{e^{2x}}{2}\right)^{(n)} - \left(\frac{e^{2x}\cos 2x}{2}\right)^{(n)}\\ &= 2^{n-1}e^{2x} - \left(\frac{e^{2x}\cos 2x}{2}\right)^{(n)} \end{align*} I don't know how to proceed with the rightmost term. So far I've been applying the Leibniz Rule whenever I've had to find the $n$ derivative of a function of the form $f(x)g(x)$, because is clear that either $f(x)$ or $g(x)$ has a derivative a $k$ derivative ($1<k<n$) equal to zero, which simplifies the expression nicely. But here $$\frac{e^{2x}\cos 2x}{2}$$ both functions are infinitely differentiable on $\mathbb{R}$ which makes things a bit different. My only attempt was to write its n derivative in this form \begin{align*} &=\frac{1}{2}\sum_{k=0}^n{n \choose k} \big(2^k e^{2x}\big)\bigg(2^{n-k}\cos \left[2x + \frac{\pi(n-k)}{2}\right]\bigg)\\ &=\sum_{k=0}^n {n \choose k} 2^{n-1}e^{2x}\cos \left[2x + \frac{\pi(n-k)}{2}\right] \end{align*} So \begin{align*} y^{(n)} &= 2^{n-1}e^{2x} -\sum_{k=0}^n{n \choose k} 2^{n-1}e^{2x}\cos \left[2x + \frac{\pi(n-k)}{2}\right]\\ &= 2^{n-1}e^{2x}\left(1 -\sum_{k=0}^n{n \choose k} \cos \left[2x + \frac{\pi(n-k)}{2}\right]\right) \end{align*} But the textbook's answer is $$2^{n-1}e^{2x}\left(1 -2^{n/2}\cos \left[2x + \frac{\pi n}{4}\right]\right)$$ For some reason I have the feeling that a little of modular arithmetic has to be applied on $\frac{\pi(n-k)}{2}$","We have \begin{align*} y&= e^{2x}\sin^2 x\\ &= e^{2x}\left(\frac{1-\cos 2x}{2}\right)\\ &= \frac{e^{2x}}{2} - \frac{e^{2x}\cos 2x}{2} \end{align*} Then \begin{align*} y^{(n)} &= \left(\frac{e^{2x}}{2}\right)^{(n)} - \left(\frac{e^{2x}\cos 2x}{2}\right)^{(n)}\\ &= 2^{n-1}e^{2x} - \left(\frac{e^{2x}\cos 2x}{2}\right)^{(n)} \end{align*} I don't know how to proceed with the rightmost term. So far I've been applying the Leibniz Rule whenever I've had to find the $n$ derivative of a function of the form $f(x)g(x)$, because is clear that either $f(x)$ or $g(x)$ has a derivative a $k$ derivative ($1<k<n$) equal to zero, which simplifies the expression nicely. But here $$\frac{e^{2x}\cos 2x}{2}$$ both functions are infinitely differentiable on $\mathbb{R}$ which makes things a bit different. My only attempt was to write its n derivative in this form \begin{align*} &=\frac{1}{2}\sum_{k=0}^n{n \choose k} \big(2^k e^{2x}\big)\bigg(2^{n-k}\cos \left[2x + \frac{\pi(n-k)}{2}\right]\bigg)\\ &=\sum_{k=0}^n {n \choose k} 2^{n-1}e^{2x}\cos \left[2x + \frac{\pi(n-k)}{2}\right] \end{align*} So \begin{align*} y^{(n)} &= 2^{n-1}e^{2x} -\sum_{k=0}^n{n \choose k} 2^{n-1}e^{2x}\cos \left[2x + \frac{\pi(n-k)}{2}\right]\\ &= 2^{n-1}e^{2x}\left(1 -\sum_{k=0}^n{n \choose k} \cos \left[2x + \frac{\pi(n-k)}{2}\right]\right) \end{align*} But the textbook's answer is $$2^{n-1}e^{2x}\left(1 -2^{n/2}\cos \left[2x + \frac{\pi n}{4}\right]\right)$$ For some reason I have the feeling that a little of modular arithmetic has to be applied on $\frac{\pi(n-k)}{2}$",,"['calculus', 'real-analysis', 'derivatives']"
75,"Let $f:(0,1] \rightarrow \mathbb{R}$ be differentiable on $(0,1]$, with $|f'(x)| \leq 1$ let $a_n=f(1/n)$ Show that $(a_n)$converges.","Let  be differentiable on , with  let  Show that converges.","f:(0,1] \rightarrow \mathbb{R} (0,1] |f'(x)| \leq 1 a_n=f(1/n) (a_n)","Let $f:(0,1] \rightarrow \mathbb{R}$ be differentiable on $(0,1]$, with $|f'(x)| \leq 1$ for all $x$ in $(0,1].$ For each $n$ in $\mathbb{N}$, let $a_n=f(1/n)$ Show that $(a_n)_{n \in \mathbb{N}}$ converges. This is what I have so far: Since $f$ has a bounded derivative on $(0,1]$, $f$ is uniformly continuous on $(0,1]$. so the definition to be uniformly continuous is $\forall \epsilon >0, \exists \delta >0$ such that if $x,y \in (0,1]$ and $|x-y|<\delta$ then $|f(x)-f(y)|<\epsilon$. Now I have to apply the mean value theorem to show $a_n$ is cauchy, thus convergers. This is where i am stuck. I know the definition of a cauchy sequence is  $\forall \epsilon >0,$ $\exists n_0 \in \mathbb{N}$  such that if $n,m \geq n_0$ then $|x_n-x_m|<\epsilon$. How do i put this all together. Thanks for the help!","Let $f:(0,1] \rightarrow \mathbb{R}$ be differentiable on $(0,1]$, with $|f'(x)| \leq 1$ for all $x$ in $(0,1].$ For each $n$ in $\mathbb{N}$, let $a_n=f(1/n)$ Show that $(a_n)_{n \in \mathbb{N}}$ converges. This is what I have so far: Since $f$ has a bounded derivative on $(0,1]$, $f$ is uniformly continuous on $(0,1]$. so the definition to be uniformly continuous is $\forall \epsilon >0, \exists \delta >0$ such that if $x,y \in (0,1]$ and $|x-y|<\delta$ then $|f(x)-f(y)|<\epsilon$. Now I have to apply the mean value theorem to show $a_n$ is cauchy, thus convergers. This is where i am stuck. I know the definition of a cauchy sequence is  $\forall \epsilon >0,$ $\exists n_0 \in \mathbb{N}$  such that if $n,m \geq n_0$ then $|x_n-x_m|<\epsilon$. How do i put this all together. Thanks for the help!",,"['calculus', 'real-analysis', 'derivatives', 'cauchy-sequences', 'uniform-continuity']"
76,"If $f:\;[a,b]\rightarrow\mathbb{R}$ is continuously differentiable, then it is uniformly differentiable.","If  is continuously differentiable, then it is uniformly differentiable.","f:\;[a,b]\rightarrow\mathbb{R}","Hi I need help with this exercise: Let $f:\;[a,b]\rightarrow\mathbb{R}$ be a differentiable function such that  $f'$ is continuous on $(a,b)$. Prove for all $\epsilon>0$ there exist $\delta>0$ such that $$\left|\frac{f(t)-f(x)}{t-x}-f'(x)\right|<\epsilon$$ if $0<|t-x|<\delta$, $\ $ $a<x,t\leq b$. Please if you can, get me some ideas for do this. I know I need do it for definition, but I don't know how to start.","Hi I need help with this exercise: Let $f:\;[a,b]\rightarrow\mathbb{R}$ be a differentiable function such that  $f'$ is continuous on $(a,b)$. Prove for all $\epsilon>0$ there exist $\delta>0$ such that $$\left|\frac{f(t)-f(x)}{t-x}-f'(x)\right|<\epsilon$$ if $0<|t-x|<\delta$, $\ $ $a<x,t\leq b$. Please if you can, get me some ideas for do this. I know I need do it for definition, but I don't know how to start.",,"['real-analysis', 'derivatives', 'uniform-continuity']"
77,"Be $f:\;(a,b)\rightarrow\mathbb{R}$ a continuous function. Suppose $c\in(a,b)$ ...",Be  a continuous function. Suppose  ...,"f:\;(a,b)\rightarrow\mathbb{R} c\in(a,b)","I got a problem with this theorem: Theorem: Let $f:\;(a,b)\rightarrow\mathbb{R}$ be a continuous function. Suppose $c\in(a,b)$ is such that f is differentiable in $(a,c)$and in $(c,b)$ and $\lim_{x\rightarrow c}f'(x)=L$. Prove $f$ is differentiable in $c$ and $f'(c)=L$ Proof: $$\lim_{x\rightarrow c}\frac{f(x)-f(c)}{x-c}=\lim_{x\rightarrow c}\frac{f(c)-f(x)}{c-x}=\lim_{x\rightarrow c}f'(x)=L$$ Then $f'(c)$ exists and $f'(c)=L$ Is my proof fine?","I got a problem with this theorem: Theorem: Let $f:\;(a,b)\rightarrow\mathbb{R}$ be a continuous function. Suppose $c\in(a,b)$ is such that f is differentiable in $(a,c)$and in $(c,b)$ and $\lim_{x\rightarrow c}f'(x)=L$. Prove $f$ is differentiable in $c$ and $f'(c)=L$ Proof: $$\lim_{x\rightarrow c}\frac{f(x)-f(c)}{x-c}=\lim_{x\rightarrow c}\frac{f(c)-f(x)}{c-x}=\lim_{x\rightarrow c}f'(x)=L$$ Then $f'(c)$ exists and $f'(c)=L$ Is my proof fine?",,"['calculus', 'real-analysis', 'derivatives', 'real-numbers']"
78,horizontal asymptotes and derivatives,horizontal asymptotes and derivatives,,"Suppose that $f(x)$ has a horizontal asymptote. Must it be the case that the derivative approaches $0$ as $x$ tends to infinity? I do not think so, and I think I have a counter example, but I have yet to prove it. Of course, I know that the converse is not true (a derivative approaching $0$ need not come from a function with a horizontal asymptote... think $\ln x, \sqrt x$ , etc).","Suppose that has a horizontal asymptote. Must it be the case that the derivative approaches as tends to infinity? I do not think so, and I think I have a counter example, but I have yet to prove it. Of course, I know that the converse is not true (a derivative approaching need not come from a function with a horizontal asymptote... think , etc).","f(x) 0 x 0 \ln x, \sqrt x","['calculus', 'derivatives']"
79,Building a highway at the minimum cost,Building a highway at the minimum cost,,"I am asked the following question: You are responsible for building a highway that connects A to B. There's an old highway 50 miles south that can be restored at the cost of \$300.000,00 per mile. Building a new path requires \$500.000,00 per mile. Find the optimal path that minimizes the cost. So we can call the projections of A and B A' and B', and the midpoints between them X and Y. Using a straight path from A to B would require \$75.000.000,00 and going AA'B'B would require \$95.000.000,00. Sonlet's find an optimal point on the middle. The cost function is given by $$ C = 2 \cdot 500000 \sqrt{50^2+x^2}  + 300000(150-2x)\\ C' = \frac{10x}{\sqrt{x^2+2500}}-6=0 $$ If we say $C'=0$ we don't find any critical points. Did I do a miscalculation? And if not, what is the conclusion of the problem? Thank you.","I am asked the following question: You are responsible for building a highway that connects A to B. There's an old highway 50 miles south that can be restored at the cost of \$300.000,00 per mile. Building a new path requires \$500.000,00 per mile. Find the optimal path that minimizes the cost. So we can call the projections of A and B A' and B', and the midpoints between them X and Y. Using a straight path from A to B would require \$75.000.000,00 and going AA'B'B would require \$95.000.000,00. Sonlet's find an optimal point on the middle. The cost function is given by $$ C = 2 \cdot 500000 \sqrt{50^2+x^2}  + 300000(150-2x)\\ C' = \frac{10x}{\sqrt{x^2+2500}}-6=0 $$ If we say $C'=0$ we don't find any critical points. Did I do a miscalculation? And if not, what is the conclusion of the problem? Thank you.",,"['calculus', 'derivatives', 'optimization', 'applications', 'operations-research']"
80,"What are ""derivatives of $f$ at point $a \in \text{domain}$""?","What are ""derivatives of  at point ""?",f a \in \text{domain},"The 1994 paper ""Higher Order Derivatives and Differential Cryptanalysis"" uses the following unfamiliar (to me) syntax in a definition: Let $(S,+)$ and $(T,+)$ be Abelian groups. For a function $f : S \to T$, the derivatives of $f$ at point $a \in S$ is defined as   $$\Delta_af(x) = f(x+a) - f(x)$$ I don't understand this syntax, or how this could be true.  Consider the function $f(x) = x + 9$.  The derivative of this (expressed either as $\frac{d}{dx}$ or $f'(x)$) is 1.  Yet according to the definition given, it should be $$\Delta_2f(x) = f(x+2) - f(x) = 2$$ My questions are as follows. Does the syntax used indicate some different meaning of the term ""derivative""?  If so, what does ""derivative"" mean here? What does it mean to take a ""derivative of $f$ at point $a \in S$""?  Ordinarily I would assume that this meant to take the derivative, then substitute $x=a$ and solve, but that doesn't appear to be what is meant here.","The 1994 paper ""Higher Order Derivatives and Differential Cryptanalysis"" uses the following unfamiliar (to me) syntax in a definition: Let $(S,+)$ and $(T,+)$ be Abelian groups. For a function $f : S \to T$, the derivatives of $f$ at point $a \in S$ is defined as   $$\Delta_af(x) = f(x+a) - f(x)$$ I don't understand this syntax, or how this could be true.  Consider the function $f(x) = x + 9$.  The derivative of this (expressed either as $\frac{d}{dx}$ or $f'(x)$) is 1.  Yet according to the definition given, it should be $$\Delta_2f(x) = f(x+2) - f(x) = 2$$ My questions are as follows. Does the syntax used indicate some different meaning of the term ""derivative""?  If so, what does ""derivative"" mean here? What does it mean to take a ""derivative of $f$ at point $a \in S$""?  Ordinarily I would assume that this meant to take the derivative, then substitute $x=a$ and solve, but that doesn't appear to be what is meant here.",,"['calculus', 'derivatives']"
81,Is there a function $f(x)$ which is defined near $x = c$ and infinitely differentiable near $x = c$ and satisfy the following properties,Is there a function  which is defined near  and infinitely differentiable near  and satisfy the following properties,f(x) x = c x = c,"Is there a function $f(x)$ which is defined near $x = c$ and infinitely differentiable near $x = c$ and satisfy the following properties: For any positive real number $\delta$, there exist real numbers $x, x^{'}$ such that $c - \delta < x, x^{'} < c$ and $f(x) > f(c)$ and $f(x^{'}) < f(c)$ . For any positive real number $\delta$, there exist real numbers $x, x^{'}$ such that $c < x, x^{'} < c + \delta$ and $f(x) > f(c)$ and $f(x^{'}) < f(c)$ .","Is there a function $f(x)$ which is defined near $x = c$ and infinitely differentiable near $x = c$ and satisfy the following properties: For any positive real number $\delta$, there exist real numbers $x, x^{'}$ such that $c - \delta < x, x^{'} < c$ and $f(x) > f(c)$ and $f(x^{'}) < f(c)$ . For any positive real number $\delta$, there exist real numbers $x, x^{'}$ such that $c < x, x^{'} < c + \delta$ and $f(x) > f(c)$ and $f(x^{'}) < f(c)$ .",,['derivatives']
82,Prove that $\left(\forall x\in \mathbb R\left(f'(x)=f(x)^2\right)\land f(0)=0\right)\implies f=\bf 0$,Prove that,\left(\forall x\in \mathbb R\left(f'(x)=f(x)^2\right)\land f(0)=0\right)\implies f=\bf 0,"Let $f:\mathbb{R}\to\mathbb{R}$ differentiable, $f(0)=0$ and $f'(x)=f(x)^2\; \forall x\in\mathbb{R}$. Show that $f(x) = 0\; \forall x\in\mathbb{R}$. Some thoughts: It can be shown that $f^{(n)}(x) = n!f(x)^{n+1}$, therefore $f^{(n)}(0)=0$. I thought of using Taylor series but that is only useful if the function is analytical. I also tried something with $f(x) = \int_0^x f(t)^2$ but no luck. Solution Since $f'(x)=f(x)^2$, $f$ is increasing. Then $f(x) \geq 0$ for $x>0$ and $f(x) \leq 0$ for $x<0$. Suppose that for some $a>0$ we have $f(a) > 0$, then $\forall x\in(a,\infty),\; f(x)>0$. Then in this interval, we can proceed similarly to the answer provided by @zhw below, to get $f(x) = \frac{-1}{x+c}$, but for $x$ sufficiently large, $f(x)$ would be negative, contradiction. The case for the negative part is similar.","Let $f:\mathbb{R}\to\mathbb{R}$ differentiable, $f(0)=0$ and $f'(x)=f(x)^2\; \forall x\in\mathbb{R}$. Show that $f(x) = 0\; \forall x\in\mathbb{R}$. Some thoughts: It can be shown that $f^{(n)}(x) = n!f(x)^{n+1}$, therefore $f^{(n)}(0)=0$. I thought of using Taylor series but that is only useful if the function is analytical. I also tried something with $f(x) = \int_0^x f(t)^2$ but no luck. Solution Since $f'(x)=f(x)^2$, $f$ is increasing. Then $f(x) \geq 0$ for $x>0$ and $f(x) \leq 0$ for $x<0$. Suppose that for some $a>0$ we have $f(a) > 0$, then $\forall x\in(a,\infty),\; f(x)>0$. Then in this interval, we can proceed similarly to the answer provided by @zhw below, to get $f(x) = \frac{-1}{x+c}$, but for $x$ sufficiently large, $f(x)$ would be negative, contradiction. The case for the negative part is similar.",,"['calculus', 'derivatives']"
83,What does it mean when the second derivative at a point is infinite?,What does it mean when the second derivative at a point is infinite?,,What does it mean when the second derivative of a function at a certain point is infinite? What would be neat examples that illustrate what happens? When the first derivative is infinite you get a vertical slope. Does something similar happen with an infinite second derivative? In other words: can you see in $f(x)$ that $f''(x_{inf})\to\infty$?,What does it mean when the second derivative of a function at a certain point is infinite? What would be neat examples that illustrate what happens? When the first derivative is infinite you get a vertical slope. Does something similar happen with an infinite second derivative? In other words: can you see in $f(x)$ that $f''(x_{inf})\to\infty$?,,"['calculus', 'derivatives']"
84,Why is $\frac{d(x^n)}{d(x)}=nx^{n-1}$,Why is,\frac{d(x^n)}{d(x)}=nx^{n-1},"So I was thinking about what I have learnt and I realised that I kind of took the derivative of a function for granted. So I did some research as I wanted to find out how this was discovered and I stumbled upon this . More specially, here is a passage from it: Without going into too much complicated detail, Newton (and his contemporary Gottfried Leibniz independently) calculated a derivative function $f'(x)$ which gives the slope at any point of a function $f(x)$. This process of calculating the slope or derivative of a curve or function is called differential calculus or differentiation (or, in Newton’s terminology, the “method of fluxions” - he called the instantaneous rate of change at a particular point on a curve the ""fluxion"", and the changing values of x and y the ""fluents""). For instance, the derivative of a straight line of the type $f(x) = 4x$ is just $4$; the derivative of a squared function $f(x) = x^2$ is $2x$; the derivative of cubic function $f(x) = x^3$ is $3x^2$. I was wondering if anyone could explain (or point me to a resource) the ""complicated details"" (or hopefully, a rigorous proof) of how the derivative function $f'(x)$ was discovered?","So I was thinking about what I have learnt and I realised that I kind of took the derivative of a function for granted. So I did some research as I wanted to find out how this was discovered and I stumbled upon this . More specially, here is a passage from it: Without going into too much complicated detail, Newton (and his contemporary Gottfried Leibniz independently) calculated a derivative function $f'(x)$ which gives the slope at any point of a function $f(x)$. This process of calculating the slope or derivative of a curve or function is called differential calculus or differentiation (or, in Newton’s terminology, the “method of fluxions” - he called the instantaneous rate of change at a particular point on a curve the ""fluxion"", and the changing values of x and y the ""fluents""). For instance, the derivative of a straight line of the type $f(x) = 4x$ is just $4$; the derivative of a squared function $f(x) = x^2$ is $2x$; the derivative of cubic function $f(x) = x^3$ is $3x^2$. I was wondering if anyone could explain (or point me to a resource) the ""complicated details"" (or hopefully, a rigorous proof) of how the derivative function $f'(x)$ was discovered?",,['derivatives']
85,"Mean-value Theorem $f(x)=\sqrt{x+2}; [4,6]$",Mean-value Theorem,"f(x)=\sqrt{x+2}; [4,6]","Verify that the hypothesis of the mean-value theorem is satisfied for the given function on the indicated interval. Then find a suitable value for $c$ that satisfies the conclusion of the mean-value theorem. $$f(x)=\sqrt{x+2}; [4,6]$$ So, $$f'(x) = {1 \over 2} (x+2)^{-{1\over 2}}$$ $f(x)$ is differentiable for all x. Now, $$f'(c) = {f(b)-f(a)\over b- a} \\ = {f(6) - f(4)\over 6-4} \\ = {2\sqrt{2} - \sqrt{6}\over2}$$ Since $f'(c) = {2\sqrt{2} - \sqrt{6}\over2}$, $${1\over 2}(c+2)^{-{1\over 2}} = {2\sqrt{2} - \sqrt{6}\over2}$$ Then after, this will give me $c$ right? I just want to know if I did it right so far. I tried simplifying the last equation, but it wasn't right. Please let me know if there is anything wrong in my steps, if not could anyone help me solve for $c$ at the end? Thank you.","Verify that the hypothesis of the mean-value theorem is satisfied for the given function on the indicated interval. Then find a suitable value for $c$ that satisfies the conclusion of the mean-value theorem. $$f(x)=\sqrt{x+2}; [4,6]$$ So, $$f'(x) = {1 \over 2} (x+2)^{-{1\over 2}}$$ $f(x)$ is differentiable for all x. Now, $$f'(c) = {f(b)-f(a)\over b- a} \\ = {f(6) - f(4)\over 6-4} \\ = {2\sqrt{2} - \sqrt{6}\over2}$$ Since $f'(c) = {2\sqrt{2} - \sqrt{6}\over2}$, $${1\over 2}(c+2)^{-{1\over 2}} = {2\sqrt{2} - \sqrt{6}\over2}$$ Then after, this will give me $c$ right? I just want to know if I did it right so far. I tried simplifying the last equation, but it wasn't right. Please let me know if there is anything wrong in my steps, if not could anyone help me solve for $c$ at the end? Thank you.",,"['calculus', 'derivatives']"
86,Solve this calculus questions,Solve this calculus questions,,"Let a differentiable function $f(x) $ satisfy the rule $f(xy) = f(x) + f(y) + xy -x - y$ for all $x,y>0 $ Given $ f^1(1)=4 $ If $ f(x_o) = 0 $ then, Find the interval in which $ x_o$ lies in?","Let a differentiable function $f(x) $ satisfy the rule $f(xy) = f(x) + f(y) + xy -x - y$ for all $x,y>0 $ Given $ f^1(1)=4 $ If $ f(x_o) = 0 $ then, Find the interval in which $ x_o$ lies in?",,"['calculus', 'derivatives']"
87,"How to properly find supremum of a function $f(x,y,z)$ on a cube $[0,1]^3$?",How to properly find supremum of a function  on a cube ?,"f(x,y,z) [0,1]^3","Solving an applied problem I was faced with the need to find supremum of the following function  $$f(x,y,z)=\frac{(x-xyz)(y-xyz)(z-xyz)}{(1-xyz)^3}$$ where $f\colon\ [0,1]^3\backslash\{(1,1,1)\} \to\mathbb{R}$. I used Wolfram Mathematica and it resulted with ""there is no maximum in the region in which the objective function is defined"" and that the supremum is $\frac{8}{27}$ when $x,y,z\to 1$. I was able to verify some of it: $$\lim\limits_{x,y,z\to 1}f(x,y,z)  =  \lim\limits_{x\to 1}f(x,x,x) = \lim\limits_{x\to 1}\frac{(x-x^3)^3}{(1-x^3)^3} = \lim\limits_{x\to 1}\frac{x^3(x+1)^3}{(x^2+x+1)^3} = \frac{8}{27} $$  though I'm not sure how to justify the first equality. Can I neglect direction of derivative in this case? And secondly, I don't know how to show that $f(x,y,z)< \frac{8}{27}$ for all $x,y,z\in[0,1]^3\backslash\{(1,1,1)\}$. When I tried to do that straightforwardly I drowned in computations.","Solving an applied problem I was faced with the need to find supremum of the following function  $$f(x,y,z)=\frac{(x-xyz)(y-xyz)(z-xyz)}{(1-xyz)^3}$$ where $f\colon\ [0,1]^3\backslash\{(1,1,1)\} \to\mathbb{R}$. I used Wolfram Mathematica and it resulted with ""there is no maximum in the region in which the objective function is defined"" and that the supremum is $\frac{8}{27}$ when $x,y,z\to 1$. I was able to verify some of it: $$\lim\limits_{x,y,z\to 1}f(x,y,z)  =  \lim\limits_{x\to 1}f(x,x,x) = \lim\limits_{x\to 1}\frac{(x-x^3)^3}{(1-x^3)^3} = \lim\limits_{x\to 1}\frac{x^3(x+1)^3}{(x^2+x+1)^3} = \frac{8}{27} $$  though I'm not sure how to justify the first equality. Can I neglect direction of derivative in this case? And secondly, I don't know how to show that $f(x,y,z)< \frac{8}{27}$ for all $x,y,z\in[0,1]^3\backslash\{(1,1,1)\}$. When I tried to do that straightforwardly I drowned in computations.",,"['real-analysis', 'derivatives', 'inequality', 'supremum-and-infimum']"
88,Drones and Integrals Project,Drones and Integrals Project,,"Hello everyone and thanks for taking the time to read this post. So in my college calculus class we had the opportunity to fly a drone and get it's flight data. I have a spreadsheet featuring two columns (one is $x$-velocity and the other is time). I have graphed a velocity versus time graph given these points and I have also fitted a 6th degree polynomial trend line. I am trying to find the total displacement and the total distance that my drone flew. So I know that a velocity-v-time graph provides several important things about the drone. The area underneath the curve is the total displacement of the drone; the slope of the graph is the drone's acceleration. In order to find the total displacement of the graph I have to use a definite integral. After getting my trend line equation, I tried to use a definite integral, but received a displacement way to high to be correct. I was wondering if anyone could help me figure out what I am doing wrong. Some things to note: I have the actual displacement which was measured with measuring tape. The actual/theoretical displacement of my drone is $58\, cm$. When I graphed the trend line, my minimum $x$-point was $9.8$ because the drone started recording data even when it hovered and our professor said it was fine if we do not include that data. My maximum $x$-point is $18.7$ because this is when the drone completed its flight path and landed. Velocity was negative at some points during this flight. The equation I am supposed to use is as follows: $$\int_9^T v(t)dt$$ I started off by making a graph of the velocity versus time and fitted a trend line for it. The equation for the trend line was:  $$y = 1.3264x^6-116.14x^5+4179.3x^4-79079x^3+829395x^2-5000000x+10000000$$ After I integrated this equation for the time interval $[9.8, 18.7]$ I got a final answer of $-57519039.94$. This is obviously way wrong and I cannot seem to figure out what I am doing wrong. Thanks in advance for the help!","Hello everyone and thanks for taking the time to read this post. So in my college calculus class we had the opportunity to fly a drone and get it's flight data. I have a spreadsheet featuring two columns (one is $x$-velocity and the other is time). I have graphed a velocity versus time graph given these points and I have also fitted a 6th degree polynomial trend line. I am trying to find the total displacement and the total distance that my drone flew. So I know that a velocity-v-time graph provides several important things about the drone. The area underneath the curve is the total displacement of the drone; the slope of the graph is the drone's acceleration. In order to find the total displacement of the graph I have to use a definite integral. After getting my trend line equation, I tried to use a definite integral, but received a displacement way to high to be correct. I was wondering if anyone could help me figure out what I am doing wrong. Some things to note: I have the actual displacement which was measured with measuring tape. The actual/theoretical displacement of my drone is $58\, cm$. When I graphed the trend line, my minimum $x$-point was $9.8$ because the drone started recording data even when it hovered and our professor said it was fine if we do not include that data. My maximum $x$-point is $18.7$ because this is when the drone completed its flight path and landed. Velocity was negative at some points during this flight. The equation I am supposed to use is as follows: $$\int_9^T v(t)dt$$ I started off by making a graph of the velocity versus time and fitted a trend line for it. The equation for the trend line was:  $$y = 1.3264x^6-116.14x^5+4179.3x^4-79079x^3+829395x^2-5000000x+10000000$$ After I integrated this equation for the time interval $[9.8, 18.7]$ I got a final answer of $-57519039.94$. This is obviously way wrong and I cannot seem to figure out what I am doing wrong. Thanks in advance for the help!",,"['calculus', 'derivatives', 'definite-integrals']"
89,When do we have the formula $f(t)=e^{\lambda t}f(0)+\int_0^te^{\lambda (t-s)}g(s)ds$?,When do we have the formula ?,f(t)=e^{\lambda t}f(0)+\int_0^te^{\lambda (t-s)}g(s)ds,"Let $g:\mathbb{R}\to \mathbb{R}$ be a continuous  function. Consider the following integral equation $$f(t)=f(0)+\int_0^t\lambda f(s)ds+\int_0^tg(s)ds. \tag{1}$$ Since $g$ is continuous,  Thus the integral equation $(1)$ has a solution $f$ which is derivable everywhere and is given by: $$f(t)=e^{\lambda t}f(0)+\int_0^te^{\lambda (t-s)}g(s)ds,\tag{2}$$ because $(2)$ implies that  $f$  satisfies the following differential equation $$f'=\lambda f+g,$$ and then satisfies $(1)$. (To be correct, we have a lot of solutions, since changing $f(0)$ gives other solutions) Now if we assume that $g$ is only locally integrable. Do we still have a solution of the integral equation $(1)$ ? and can it be given also by the formula  $$f(t)=e^{\lambda t}f(0)+\int_0^te^{\lambda (t-s)}g(s)ds.$$","Let $g:\mathbb{R}\to \mathbb{R}$ be a continuous  function. Consider the following integral equation $$f(t)=f(0)+\int_0^t\lambda f(s)ds+\int_0^tg(s)ds. \tag{1}$$ Since $g$ is continuous,  Thus the integral equation $(1)$ has a solution $f$ which is derivable everywhere and is given by: $$f(t)=e^{\lambda t}f(0)+\int_0^te^{\lambda (t-s)}g(s)ds,\tag{2}$$ because $(2)$ implies that  $f$  satisfies the following differential equation $$f'=\lambda f+g,$$ and then satisfies $(1)$. (To be correct, we have a lot of solutions, since changing $f(0)$ gives other solutions) Now if we assume that $g$ is only locally integrable. Do we still have a solution of the integral equation $(1)$ ? and can it be given also by the formula  $$f(t)=e^{\lambda t}f(0)+\int_0^te^{\lambda (t-s)}g(s)ds.$$",,"['real-analysis', 'integration', 'derivatives', 'lebesgue-integral', 'almost-everywhere']"
90,Derivative of function with respect to $x$ where $x$ is the order of a derivative of another function.,Derivative of function with respect to  where  is the order of a derivative of another function.,x x,"When I learn math I always have lot of thoughts and ideas in my head and some of them are weird. But I came aross a question, which is... also kind of weird. How can a problem like this be solved? Is it possible? $$\frac{d}{dx}\Bigg(\frac{d^x}{dw^x}w^2\Bigg)$$","When I learn math I always have lot of thoughts and ideas in my head and some of them are weird. But I came aross a question, which is... also kind of weird. How can a problem like this be solved? Is it possible? $$\frac{d}{dx}\Bigg(\frac{d^x}{dw^x}w^2\Bigg)$$",,"['calculus', 'derivatives']"
91,Number of real roots of $f ' ( x )$,Number of real roots of,f ' ( x ),"Let $$f(x)=(x-a)(x-b)^3(x-c)^5(x-d)^7 $$ where $a,b,c,d$ are  real  numbers  with $a < b < c < d$ .  Thus $ f ( x )$ has $16$ real roots counting multiplicities and among them $4$ are distinct from each other.  Consider $f ' ( x )$, i.e.  the derivative of $f ( x )$.  Find the following: $(i)$  the  number  of  real  roots  of $f ' ( x )$,  counting  multiplicities, $(ii)$  the  number  of distinct real roots of $f ' ( x )$. This is a polynomial of degree $16$ hence the derivative will be of degree $15$ and hence it will have $15$ roots. But are they real ? How to find distinct real roots ? Rolle's theorem tells only about existence of root.","Let $$f(x)=(x-a)(x-b)^3(x-c)^5(x-d)^7 $$ where $a,b,c,d$ are  real  numbers  with $a < b < c < d$ .  Thus $ f ( x )$ has $16$ real roots counting multiplicities and among them $4$ are distinct from each other.  Consider $f ' ( x )$, i.e.  the derivative of $f ( x )$.  Find the following: $(i)$  the  number  of  real  roots  of $f ' ( x )$,  counting  multiplicities, $(ii)$  the  number  of distinct real roots of $f ' ( x )$. This is a polynomial of degree $16$ hence the derivative will be of degree $15$ and hence it will have $15$ roots. But are they real ? How to find distinct real roots ? Rolle's theorem tells only about existence of root.",,"['calculus', 'real-analysis', 'derivatives', 'polynomials', 'roots']"
92,$f'(a)=0$ then show that $f(a)=0$,then show that,f'(a)=0 f(a)=0,"Let $f \in \mathbb{R}\left[x\right]$ be a polynomial with real coefficients in one variable $x$ of degree $n > 0$ . Assume that $f$ has $n$ real roots (counted with multiplicities). Let $a \in \mathbb{R}$ be such that the derivative $f'$ of $f$ can be written as $f'\left(x\right) = \left(x-a\right)^2 g\left(x\right)$ for some polynomial $g$ . (In other words, $a$ is a double root of $f'$ .) Prove that $f\left(a\right) = 0$ (that is, $a$ is a root of $f$ ). I've seen this stated as an exercise with $n = 2016$ , but clearly this must be a general fact. This is not a purely algebraic problem; if we replaced $\mathbb{R}$ by $\mathbb{C}$ , then it would become false (for a counterexample, try $n = 3$ , $f = x^3 + 2$ and $a = 3$ ).","Let be a polynomial with real coefficients in one variable of degree . Assume that has real roots (counted with multiplicities). Let be such that the derivative of can be written as for some polynomial . (In other words, is a double root of .) Prove that (that is, is a root of ). I've seen this stated as an exercise with , but clearly this must be a general fact. This is not a purely algebraic problem; if we replaced by , then it would become false (for a counterexample, try , and ).",f \in \mathbb{R}\left[x\right] x n > 0 f n a \in \mathbb{R} f' f f'\left(x\right) = \left(x-a\right)^2 g\left(x\right) g a f' f\left(a\right) = 0 a f n = 2016 \mathbb{R} \mathbb{C} n = 3 f = x^3 + 2 a = 3,"['calculus', 'derivatives', 'polynomials', 'roots']"
93,Automatic differentiation-Reverse mode,Automatic differentiation-Reverse mode,,"I am trying to understand very basics material and a bit confused about steps. If we have function $z=f(x_1,x_1)=x_1x_2+sinx_1=w_1w_2+sinw_1=w_1+w_4=w_5$ we need  fix the dependent variable to be differentiated and computes the derivative with respect to each sub-expression recursively, according to chain rule we have - $\frac{dy}{dx}=\frac{dy}{dw_1}\frac{dw_1}{dx}=(\frac{dy}{dw_2}\frac{dw_2}{dw_1})\frac{dw_1}{dx}=(((\frac{dy}{dw_4}\frac{dw_4}{dw_3})\frac{dw_3}{dw_2})\frac{dw_2}{dw_1})\frac{dw_1}{dx}=((((\frac{dy}{dw_5}\frac{dw_5}{dw_4})\frac{dw_4}{dw_3})\frac{dw_3}{dw_2})\frac{dw_2}{dw_1})\frac{dw_1}{dx}$ $w_1=x_1, w_2=x_2$ $\bar w=\frac{dy}{dw}$ Then I have the next calculation- 1) $\bar f =\bar w_5 = 1 $ seed 2) $\bar w_4=$$\bar w_5$$\frac{dw_5}{dw_4}=$$\bar w_5 *1$ 2) $\bar w_3=$$\bar w_5$$\frac{dw_5}{dw_3}=$$\bar w_5 *1$ 3) $\bar w_2=$$\bar w_3$$\frac{dw_3}{dw_2}=$$\bar w_3 w_1$ I don't really see how we obtained these steps from the chain. Can somebody explain it more clearly, please!","I am trying to understand very basics material and a bit confused about steps. If we have function $z=f(x_1,x_1)=x_1x_2+sinx_1=w_1w_2+sinw_1=w_1+w_4=w_5$ we need  fix the dependent variable to be differentiated and computes the derivative with respect to each sub-expression recursively, according to chain rule we have - $\frac{dy}{dx}=\frac{dy}{dw_1}\frac{dw_1}{dx}=(\frac{dy}{dw_2}\frac{dw_2}{dw_1})\frac{dw_1}{dx}=(((\frac{dy}{dw_4}\frac{dw_4}{dw_3})\frac{dw_3}{dw_2})\frac{dw_2}{dw_1})\frac{dw_1}{dx}=((((\frac{dy}{dw_5}\frac{dw_5}{dw_4})\frac{dw_4}{dw_3})\frac{dw_3}{dw_2})\frac{dw_2}{dw_1})\frac{dw_1}{dx}$ $w_1=x_1, w_2=x_2$ $\bar w=\frac{dy}{dw}$ Then I have the next calculation- 1) $\bar f =\bar w_5 = 1 $ seed 2) $\bar w_4=$$\bar w_5$$\frac{dw_5}{dw_4}=$$\bar w_5 *1$ 2) $\bar w_3=$$\bar w_5$$\frac{dw_5}{dw_3}=$$\bar w_5 *1$ 3) $\bar w_2=$$\bar w_3$$\frac{dw_3}{dw_2}=$$\bar w_3 w_1$ I don't really see how we obtained these steps from the chain. Can somebody explain it more clearly, please!",,"['calculus', 'derivatives', 'algorithms']"
94,Show that the equation of the normal line with the minimum y-coordinate is $ y = \frac{-\sqrt{2}}{2}x + {1\over k}$,Show that the equation of the normal line with the minimum y-coordinate is, y = \frac{-\sqrt{2}}{2}x + {1\over k},"Question: The curve in the figure is the parabola $y=kx^2$ where $k>0$. Several normal lines to this parabola are also shown. Consider the points in the first quadrant from which the normal lines are drawn. Notice that as the $x$ coordinate gets smaller, the $y$-coordinate of the intersection of the normal with the other arm of the parabola also decreases until it reaches a minimum, and then it increases. The normal line with the minimum $y$ coordinate is dotted. $(a)$ Show that the equation of the normal to the parabola at a point $(x_0,y_0)$ is $y = {-x\over 2kx_0} + kx_0^2 + {1\over 2k}$ (b) Show that the equation of the normal line with the minimum y-coordinate is $ y =  \frac{-\sqrt{2}}{2}x + {1\over k} $ What I have done: $(a)$ Show that the equation of the normal to the parabola at a point $(x_0,y_0)$ is $y = {-x\over 2kx_0} + kx_0^2 + {1\over 2k}$ $$ f(x) = kx^2 $$ $$ f( x_{0}) = kx_{0}^2 $$ $$ f'(x) = 2kx $$ $$ f'(x_{0}) = 2kx_0 $$ $$ Normal = -1/m $$ $$ m= {-1\over 2kx_0} $$ $$ y-y_1 = m (x-x_1) $$ $$ y-kx_0^2 = {-1\over 2kx_0}(x-x_0) $$ $$ y = {-x\over 2kx_0} + kx_0^2 + {1\over 2k} $$ (b) Show that the equation of the normal line with the minimum y-coordinate is $ y =  \frac{-\sqrt{2}}{2}x + {1\over k} $ $$ y =  \frac{-\sqrt{2}}{2}x + {1\over k} $$ $$ {-1\over m} = \frac{-\sqrt{2}}{2} $$ $$ m = \sqrt{2} $$ $$ f'(x) = 2kx $$ $$ 2kx = \sqrt{2} $$ $$ x = {\sqrt{2}\over 2k} $$ $$ f({\sqrt{2}\over 2k}) =  {1\over 2k}$$ $$ y-y_1 = m (x-x_1) $$ $$ y - {1\over 2k} = \frac{-\sqrt{2}}{2} (x-{\sqrt{2}\over 2k}) $$ $$ y =  \frac{-\sqrt{2}}{2}x + {1\over k} $$ However I am stuck trying to prove that this is the minimum y coordinate. I've attempted $$ y = y$$ $$ kx^2 = {-x\over 2kx_0} + kx_0^2 + {1\over 2k}$$ $$ kx^2 + (\frac{1}{2kx_0})x -(kx_0^2 + \frac{1}{2k}) = 0$$ $$ x={-b\pm\sqrt{b^2-4ac} \over 2a}$$ $$ x={- (\frac{1}{2kx_0})\pm\sqrt{ (\frac{1}{4k^2x_0^2})-4(k)(-kx_0^2 - \frac{1}{2k})} \over 2k}$$ $$ x={- (\frac{1}{2kx_0})\pm\sqrt{ \frac{1}{4k^2x_0^2}-4(-k^2x_0^2 - \frac{k}{2k})} \over 2k}$$ $$ x={-\frac{1}{2kx_0}\pm\sqrt{ \frac{1}{4k^2x_0^2}+4k^2x_0^2 + 2} \over 2k}$$ $$ x={-\frac{1}{2kx_0}\pm\sqrt{ (2kx_0 + \frac{1}{2kx_0})^2} \over 2k}$$ $$ x={-\frac{1}{2kx_0}\pm { (2kx_0 + \frac{1}{2kx_0})} \over 2k}$$ $$ x = x_0 $$ $$ x = \frac{-1}{2k^2x_0} - x_0 $$ Now I am lost..","Question: The curve in the figure is the parabola $y=kx^2$ where $k>0$. Several normal lines to this parabola are also shown. Consider the points in the first quadrant from which the normal lines are drawn. Notice that as the $x$ coordinate gets smaller, the $y$-coordinate of the intersection of the normal with the other arm of the parabola also decreases until it reaches a minimum, and then it increases. The normal line with the minimum $y$ coordinate is dotted. $(a)$ Show that the equation of the normal to the parabola at a point $(x_0,y_0)$ is $y = {-x\over 2kx_0} + kx_0^2 + {1\over 2k}$ (b) Show that the equation of the normal line with the minimum y-coordinate is $ y =  \frac{-\sqrt{2}}{2}x + {1\over k} $ What I have done: $(a)$ Show that the equation of the normal to the parabola at a point $(x_0,y_0)$ is $y = {-x\over 2kx_0} + kx_0^2 + {1\over 2k}$ $$ f(x) = kx^2 $$ $$ f( x_{0}) = kx_{0}^2 $$ $$ f'(x) = 2kx $$ $$ f'(x_{0}) = 2kx_0 $$ $$ Normal = -1/m $$ $$ m= {-1\over 2kx_0} $$ $$ y-y_1 = m (x-x_1) $$ $$ y-kx_0^2 = {-1\over 2kx_0}(x-x_0) $$ $$ y = {-x\over 2kx_0} + kx_0^2 + {1\over 2k} $$ (b) Show that the equation of the normal line with the minimum y-coordinate is $ y =  \frac{-\sqrt{2}}{2}x + {1\over k} $ $$ y =  \frac{-\sqrt{2}}{2}x + {1\over k} $$ $$ {-1\over m} = \frac{-\sqrt{2}}{2} $$ $$ m = \sqrt{2} $$ $$ f'(x) = 2kx $$ $$ 2kx = \sqrt{2} $$ $$ x = {\sqrt{2}\over 2k} $$ $$ f({\sqrt{2}\over 2k}) =  {1\over 2k}$$ $$ y-y_1 = m (x-x_1) $$ $$ y - {1\over 2k} = \frac{-\sqrt{2}}{2} (x-{\sqrt{2}\over 2k}) $$ $$ y =  \frac{-\sqrt{2}}{2}x + {1\over k} $$ However I am stuck trying to prove that this is the minimum y coordinate. I've attempted $$ y = y$$ $$ kx^2 = {-x\over 2kx_0} + kx_0^2 + {1\over 2k}$$ $$ kx^2 + (\frac{1}{2kx_0})x -(kx_0^2 + \frac{1}{2k}) = 0$$ $$ x={-b\pm\sqrt{b^2-4ac} \over 2a}$$ $$ x={- (\frac{1}{2kx_0})\pm\sqrt{ (\frac{1}{4k^2x_0^2})-4(k)(-kx_0^2 - \frac{1}{2k})} \over 2k}$$ $$ x={- (\frac{1}{2kx_0})\pm\sqrt{ \frac{1}{4k^2x_0^2}-4(-k^2x_0^2 - \frac{k}{2k})} \over 2k}$$ $$ x={-\frac{1}{2kx_0}\pm\sqrt{ \frac{1}{4k^2x_0^2}+4k^2x_0^2 + 2} \over 2k}$$ $$ x={-\frac{1}{2kx_0}\pm\sqrt{ (2kx_0 + \frac{1}{2kx_0})^2} \over 2k}$$ $$ x={-\frac{1}{2kx_0}\pm { (2kx_0 + \frac{1}{2kx_0})} \over 2k}$$ $$ x = x_0 $$ $$ x = \frac{-1}{2k^2x_0} - x_0 $$ Now I am lost..",,"['calculus', 'derivatives', 'optimization', 'conic-sections', 'area']"
95,Is there a limit which characterizes twice differentiability?,Is there a limit which characterizes twice differentiability?,,"If $f$ is twice differentiable at $x=a$, then we have $$ f''(a) = \lim_{h \to 0} \frac{f(a+h)-2f(a)+f(a-h) }{h^2} $$ However there are functions which are not twice differentiable for which this limit exists (for example, the signum function). Is there a limit definition for $f''(a)$ which exists iff $f$ is twice differentiable?","If $f$ is twice differentiable at $x=a$, then we have $$ f''(a) = \lim_{h \to 0} \frac{f(a+h)-2f(a)+f(a-h) }{h^2} $$ However there are functions which are not twice differentiable for which this limit exists (for example, the signum function). Is there a limit definition for $f''(a)$ which exists iff $f$ is twice differentiable?",,"['calculus', 'derivatives']"
96,rate of change of area of circle per second w.r.t. radius,rate of change of area of circle per second w.r.t. radius,,"Find the rate of change of the area of a circle per second with respect to its radius when radius=5cm. Source $A= \pi r^2$ ⇒ $\frac{{\rm d}A}{{\rm d}r} =2rπ$ So when $r=5$ cm, $\frac{{\rm d}A}{{\rm d}r}= 10 \pi$ cm But the answer is $10\pi$ cm $^2$ /sec. I don't understand how time comes into picture when we are only talking about radius and area? Why is ""per second"" even mentioned in the question?","Find the rate of change of the area of a circle per second with respect to its radius when radius=5cm. Source ⇒ So when cm, cm But the answer is cm /sec. I don't understand how time comes into picture when we are only talking about radius and area? Why is ""per second"" even mentioned in the question?",A= \pi r^2 \frac{{\rm d}A}{{\rm d}r} =2rπ r=5 \frac{{\rm d}A}{{\rm d}r}= 10 \pi 10\pi ^2,"['calculus', 'derivatives']"
97,derivative of a projection matrix,derivative of a projection matrix,,"The projection onto a parametrised vector $v(\lambda)$ is $P_v = \frac{vv^{T}}{v^{T}v}.$ Its complement is $$P = I-\frac{vv^T}{v^{T}v}.$$ I've got an expression containing this complementary projection and I need its derivative. How do I calculate $$\frac{\partial P(v(\lambda))}{\partial \lambda} \text{ ?}$$ I started with $$\cfrac{\partial P(v(\lambda))}{\partial \lambda} = \cfrac{\partial P(v(\lambda))}{\partial v} \cfrac{\partial v(\lambda))}{\partial \lambda}$$ where only the expression $\cfrac{\partial vv^{T}}{\partial v}$ I can't handle. How can I find this derivative of a matrix with respect to a vector, or the original derivative with respect to the scalar parameter $\lambda$?","The projection onto a parametrised vector $v(\lambda)$ is $P_v = \frac{vv^{T}}{v^{T}v}.$ Its complement is $$P = I-\frac{vv^T}{v^{T}v}.$$ I've got an expression containing this complementary projection and I need its derivative. How do I calculate $$\frac{\partial P(v(\lambda))}{\partial \lambda} \text{ ?}$$ I started with $$\cfrac{\partial P(v(\lambda))}{\partial \lambda} = \cfrac{\partial P(v(\lambda))}{\partial v} \cfrac{\partial v(\lambda))}{\partial \lambda}$$ where only the expression $\cfrac{\partial vv^{T}}{\partial v}$ I can't handle. How can I find this derivative of a matrix with respect to a vector, or the original derivative with respect to the scalar parameter $\lambda$?",,"['linear-algebra', 'differential-geometry', 'derivatives']"
98,"What is the derivative of $\int_{-10}^{-3} e^{\tan(t)} \,dt$ with respect to x?",What is the derivative of  with respect to x?,"\int_{-10}^{-3} e^{\tan(t)} \,dt","We were learning about the Fundamental Theorem of Calculus today in my high school and the above integral came up as an example of an integral with a ""constant"" value. At first I accepted that the derivative of it was zero, but then I realized that since derivatives are defined for radian versions of the trigonometric functions, $e^{\tan(t)}$ does not satisfy the continuity requirement for it to be guaranteed to have an convergent integral on the interval $[-10, -3]$. I had my suspicions that the integral did not exist as a real number, so I checked it with the NINT tool on my graphing calculator and got $\infty$. I also cross-validated the result with WolframAlpha and reaffirmed that the integral does not converge to any real number. Yet a peer pointed out that the calculator also evaluated the derivative of $\infty$ as $0$, and WolframAlpha confirms that as well. My Question Is the derivative of $\infty$ truly $0$? It seems rediculous to me to treat it as a constant since it's only a concept, but has someone defined a use or reason for it to be that way?","We were learning about the Fundamental Theorem of Calculus today in my high school and the above integral came up as an example of an integral with a ""constant"" value. At first I accepted that the derivative of it was zero, but then I realized that since derivatives are defined for radian versions of the trigonometric functions, $e^{\tan(t)}$ does not satisfy the continuity requirement for it to be guaranteed to have an convergent integral on the interval $[-10, -3]$. I had my suspicions that the integral did not exist as a real number, so I checked it with the NINT tool on my graphing calculator and got $\infty$. I also cross-validated the result with WolframAlpha and reaffirmed that the integral does not converge to any real number. Yet a peer pointed out that the calculator also evaluated the derivative of $\infty$ as $0$, and WolframAlpha confirms that as well. My Question Is the derivative of $\infty$ truly $0$? It seems rediculous to me to treat it as a constant since it's only a concept, but has someone defined a use or reason for it to be that way?",,"['calculus', 'integration', 'derivatives', 'infinity']"
99,Theorems on substitution in indefinite integrals,Theorems on substitution in indefinite integrals,,"I can't understand some facts on the substitutions in indefinite integrals. On my textbook is reported only the ""standard"" case (integral of a composed function and the derivative of the inner function): Considered two intervals $I$ and $J$ let .$f: I\rightarrow \mathbb{R}$ be a function that has an antiderivative $F(x)$ on $I$ . $\phi : J\rightarrow I$  be a function differentiable on $J$. Then the function $f(\phi(x))\phi’(x)$ is integrable and we have $\int f(\phi(x))\phi’(x)dx=F(\phi(x))+c$ The theorem is clear, but, reading other books, I saw that there are at least two other cases of substitutions in which there are different (and more) conditions to impose, since it is necessary to use the inverse of the function $\phi$. I report the theorem(s) about these two cases Considered two intervals $I$ and $J$, let .$f: I\rightarrow \mathbb{R}$ be a function continuous $I$ .$\phi : J\rightarrow I$ be a function differentiable with continuous derivative on $J$ with $\phi’(x)\ne 0 \forall x \in J$ Some of the versions of the theorem now add the following condition: .$f(J)=I$ (i.e. $\phi$ surjective) Then $\int f(x) dx= \int f(\phi(t))\phi’(t)dt$ (using substitution $x=\phi(t)$, from which, since $\phi$ is invertible, we get back the $x$ with $t=\phi^{-1}(x)$) And $\int f(\phi(x)) dx= \int f(t)[\phi^{-1}(t)]’dt$ (using substitution $t=\phi(x)$, from which, since $\phi$ is invertible, we get the $dx$ to substitute in the integral) The condition of $f$ continuous implies (Fundamental Theorem of Calculus) that it has an antiderivative and $\phi’(x)\ne 0$ in $J$ means that $\phi$ is injective (which means invertible). Firstly I can't understand why it is required for $\phi(x)$ to have continuous derivative , instead of just being continuous (as in the first theorem, taken from my textbook). I see that in the same theorems regarding definite integrals the same condition ($\phi \in C^{1}$)is imposed, but I think that in that case it is strictly necessary to have the function that we want to integrate continuous, so that we can apply the fundamental theorem of calculus. But is it necessary to impose it also for indefinite integrals, or maybe just in the second and in the third case I listed? Secondly is it strictly necessary for $\phi(x)$ to be surjective besides being injective (so invertible)?. In other terms is it necessary that $\phi$ is a bijection between  $I$ and $J$ or is it enough for it to be invertible (i.e. $\phi^{-1}$ exists)? Thanks a lot in advice","I can't understand some facts on the substitutions in indefinite integrals. On my textbook is reported only the ""standard"" case (integral of a composed function and the derivative of the inner function): Considered two intervals $I$ and $J$ let .$f: I\rightarrow \mathbb{R}$ be a function that has an antiderivative $F(x)$ on $I$ . $\phi : J\rightarrow I$  be a function differentiable on $J$. Then the function $f(\phi(x))\phi’(x)$ is integrable and we have $\int f(\phi(x))\phi’(x)dx=F(\phi(x))+c$ The theorem is clear, but, reading other books, I saw that there are at least two other cases of substitutions in which there are different (and more) conditions to impose, since it is necessary to use the inverse of the function $\phi$. I report the theorem(s) about these two cases Considered two intervals $I$ and $J$, let .$f: I\rightarrow \mathbb{R}$ be a function continuous $I$ .$\phi : J\rightarrow I$ be a function differentiable with continuous derivative on $J$ with $\phi’(x)\ne 0 \forall x \in J$ Some of the versions of the theorem now add the following condition: .$f(J)=I$ (i.e. $\phi$ surjective) Then $\int f(x) dx= \int f(\phi(t))\phi’(t)dt$ (using substitution $x=\phi(t)$, from which, since $\phi$ is invertible, we get back the $x$ with $t=\phi^{-1}(x)$) And $\int f(\phi(x)) dx= \int f(t)[\phi^{-1}(t)]’dt$ (using substitution $t=\phi(x)$, from which, since $\phi$ is invertible, we get the $dx$ to substitute in the integral) The condition of $f$ continuous implies (Fundamental Theorem of Calculus) that it has an antiderivative and $\phi’(x)\ne 0$ in $J$ means that $\phi$ is injective (which means invertible). Firstly I can't understand why it is required for $\phi(x)$ to have continuous derivative , instead of just being continuous (as in the first theorem, taken from my textbook). I see that in the same theorems regarding definite integrals the same condition ($\phi \in C^{1}$)is imposed, but I think that in that case it is strictly necessary to have the function that we want to integrate continuous, so that we can apply the fundamental theorem of calculus. But is it necessary to impose it also for indefinite integrals, or maybe just in the second and in the third case I listed? Secondly is it strictly necessary for $\phi(x)$ to be surjective besides being injective (so invertible)?. In other terms is it necessary that $\phi$ is a bijection between  $I$ and $J$ or is it enough for it to be invertible (i.e. $\phi^{-1}$ exists)? Thanks a lot in advice",,"['calculus', 'integration', 'derivatives']"
