,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What's so special about radians? (Differentiation) [duplicate],What's so special about radians? (Differentiation) [duplicate],,"This question already has answers here : Why do we require radians in calculus? (11 answers) Closed 9 years ago . It seems to me that radians have lots of very special properties that allow us to do maths with trigonometric functions. When I first came across radians, I was led to believe that they were designed purely to make finding areas and arc lengths of sectors more easy. But then I discovered that: $$\frac{d}{dx}\sin x=\cos x$$ Clearly this only works when using radians. If you used degrees and found the gradient of $\sin x$ at $x=0$ you would get  that gradient $=\frac{\pi}{180}^{\circ}$. So my question is: why does differentiating $\sin x$ give you $cosx$ when you are using radians ? What makes radians so special in this respect?","This question already has answers here : Why do we require radians in calculus? (11 answers) Closed 9 years ago . It seems to me that radians have lots of very special properties that allow us to do maths with trigonometric functions. When I first came across radians, I was led to believe that they were designed purely to make finding areas and arc lengths of sectors more easy. But then I discovered that: $$\frac{d}{dx}\sin x=\cos x$$ Clearly this only works when using radians. If you used degrees and found the gradient of $\sin x$ at $x=0$ you would get  that gradient $=\frac{\pi}{180}^{\circ}$. So my question is: why does differentiating $\sin x$ give you $cosx$ when you are using radians ? What makes radians so special in this respect?",,"['calculus', 'derivatives']"
1,Find the solution to the differential equation,Find the solution to the differential equation,,"Assume $x\gt 0$ and let $$x(x+1)\frac{du}{dx} = u^2,$$ $$u(1) = 4.$$ I started off by doing some algebra to get: $$\frac{1}{u^2}du = \frac{1}{x^2+x}dx.$$ I then took the partial fraction of the right side of the equation: $$\frac{1}{u^2}du = \left(\frac{1}{x}-\frac{1}{x+1}\right).$$ I then took the integral of both sides: $$-\frac{1}{u} = \log{x}-\log{(x+1)}+C.$$ From here I don't know what to do because we are solving for $u(x)$ and I'm not sure how to get that from $-\frac{1}{u}$.","Assume $x\gt 0$ and let $$x(x+1)\frac{du}{dx} = u^2,$$ $$u(1) = 4.$$ I started off by doing some algebra to get: $$\frac{1}{u^2}du = \frac{1}{x^2+x}dx.$$ I then took the partial fraction of the right side of the equation: $$\frac{1}{u^2}du = \left(\frac{1}{x}-\frac{1}{x+1}\right).$$ I then took the integral of both sides: $$-\frac{1}{u} = \log{x}-\log{(x+1)}+C.$$ From here I don't know what to do because we are solving for $u(x)$ and I'm not sure how to get that from $-\frac{1}{u}$.",,"['calculus', 'integration', 'derivatives', 'logarithms', 'differential']"
2,Why are the differentiation/integration rules what they are?,Why are the differentiation/integration rules what they are?,,"So I understand what rules you use where, and the general forms of the rules like: $$\left(\frac{d}{dx}\right)^nx^k=\frac{k!}{(k-n)!}x^{k-n}$$ My question is why are these the formulas that give us the answers we want?  I learned integrals and derivatives using the limit method as the subdivisions got smaller and smaller, but I don't have a good conceptual understanding of the geometric manipulation taking place that allows the above rule to give us (slopes/areas) of exponential curves. For example, I learned that when you integrate you're finding an bunch of infinitesimal area rectangles under the curve $$dA=h*dx$$ where the height was the distance from the x-axis to the curve $$h=f(x)$$ so we get $$dA=f(x)dx$$ and then to go from an infinitesimal area to the whole area you integrate to get $$A=F(x)$$ Now from here, if $$f(x)=x$$ then $$F(x)=\int{x}dx=\frac{x^2}{2}+C$$ And my question is why does changing the exponent of the curve in this way (add one, divide by new exponent) give us the geometric area under the curve?","So I understand what rules you use where, and the general forms of the rules like: $$\left(\frac{d}{dx}\right)^nx^k=\frac{k!}{(k-n)!}x^{k-n}$$ My question is why are these the formulas that give us the answers we want?  I learned integrals and derivatives using the limit method as the subdivisions got smaller and smaller, but I don't have a good conceptual understanding of the geometric manipulation taking place that allows the above rule to give us (slopes/areas) of exponential curves. For example, I learned that when you integrate you're finding an bunch of infinitesimal area rectangles under the curve $$dA=h*dx$$ where the height was the distance from the x-axis to the curve $$h=f(x)$$ so we get $$dA=f(x)dx$$ and then to go from an infinitesimal area to the whole area you integrate to get $$A=F(x)$$ Now from here, if $$f(x)=x$$ then $$F(x)=\int{x}dx=\frac{x^2}{2}+C$$ And my question is why does changing the exponent of the curve in this way (add one, divide by new exponent) give us the geometric area under the curve?",,"['integration', 'derivatives']"
3,Implicit differentiation of $e^{x^2+y^2} = xy$,Implicit differentiation of,e^{x^2+y^2} = xy,I just want to reconfirm the steps needed to answer this question. Thank you Find $\dfrac{dy}{dx}$ in the followng: $$e^{\large x^2 + y^2}= xy$$ I got this so far. $\newcommand{\dd}{\mathrm{d}}\frac{\dd x}{\dd y} e^{x^2}\cdot e^{y^2} = \frac{\dd x}{\dd y} xy$ $u=e^{x^2}$: $\frac{\dd u}{\dd x}=2x(e^{x^2})$ $v=e^{y^2}$: $\frac{\dd v}{\dd x}=2y(e^{y^2})\frac{\dd y}{\dd x}$ $u=x$: $\frac{\dd u}{\dd x}=1$ $v=y$: $\frac{\dd v}{\dd x}=\frac{\dd y}{\dd x}$ $$\begin{align} e^{x^2}\cdot2y(e^{y^2})\frac{\dd y}{\dd x} + e^{y^2}\cdot 2x(e^{x^2}) &= x \frac{\dd y}{\dd x} + y\\ e^{x^2}\cdot2y(e^{y^2})\frac{\dd y}{\dd x} - x \frac{\dd y}{\dd x} &= y - e^{y^2}\cdot 2x(e^{x^2})\\ \frac{\dd y}{\dd x} \left[e^{x^2}\cdot2y(e^{y^2})-x\right] &= y - e^{y^2}\cdot 2x(e^{x^2})\\ \frac{\dd y}{\dd x} &= \frac{y - e^{y^2}\cdot 2x(e^{x^2})}{e^{x^2}\cdot2y(e^{y^2})-x} \end{align}$$,I just want to reconfirm the steps needed to answer this question. Thank you Find $\dfrac{dy}{dx}$ in the followng: $$e^{\large x^2 + y^2}= xy$$ I got this so far. $\newcommand{\dd}{\mathrm{d}}\frac{\dd x}{\dd y} e^{x^2}\cdot e^{y^2} = \frac{\dd x}{\dd y} xy$ $u=e^{x^2}$: $\frac{\dd u}{\dd x}=2x(e^{x^2})$ $v=e^{y^2}$: $\frac{\dd v}{\dd x}=2y(e^{y^2})\frac{\dd y}{\dd x}$ $u=x$: $\frac{\dd u}{\dd x}=1$ $v=y$: $\frac{\dd v}{\dd x}=\frac{\dd y}{\dd x}$ $$\begin{align} e^{x^2}\cdot2y(e^{y^2})\frac{\dd y}{\dd x} + e^{y^2}\cdot 2x(e^{x^2}) &= x \frac{\dd y}{\dd x} + y\\ e^{x^2}\cdot2y(e^{y^2})\frac{\dd y}{\dd x} - x \frac{\dd y}{\dd x} &= y - e^{y^2}\cdot 2x(e^{x^2})\\ \frac{\dd y}{\dd x} \left[e^{x^2}\cdot2y(e^{y^2})-x\right] &= y - e^{y^2}\cdot 2x(e^{x^2})\\ \frac{\dd y}{\dd x} &= \frac{y - e^{y^2}\cdot 2x(e^{x^2})}{e^{x^2}\cdot2y(e^{y^2})-x} \end{align}$$,,"['calculus', 'derivatives']"
4,Where do I make mistake on this derivative containing e^x^2,Where do I make mistake on this derivative containing e^x^2,,"My brother is preparing for the university and asked me the following multiple choice question. $$\frac{d}{dx}(x^3 * e^{x^2})$$ a)  $e^{x^2}*x^2*(1+2x)$ b)  $e^{x^2}*x^2*(3+2x)$ c)  $e^{x^2}*x^2*(3+2x^2)$ d)  $e^{x^2}*x^2*(3-2x)$ e)  $e^{x^2}*x^2*(3-2x^2)$ Even though I find $e^{x^2}*x^2*(3+2x^2)$, the answer is $e^{x^2}*x^2*(3+2x)$. I wonder where do I make the mistake. What I did is as follows: By product rule: $$(x^3 * e^{x^2})' \Rightarrow 3x^2 * e^{x^2} + x^3 * (e^{x^2})'$$ Since $(e^{x^2})' = 2x * e^{x^2}$, the equation becomes  $$3x^2 * e^{x^2} + x^3 * 2x * e^{x^2}$$ $$e^{x^2} * x^2 * (3 + 2x^2)$$ Thanks","My brother is preparing for the university and asked me the following multiple choice question. $$\frac{d}{dx}(x^3 * e^{x^2})$$ a)  $e^{x^2}*x^2*(1+2x)$ b)  $e^{x^2}*x^2*(3+2x)$ c)  $e^{x^2}*x^2*(3+2x^2)$ d)  $e^{x^2}*x^2*(3-2x)$ e)  $e^{x^2}*x^2*(3-2x^2)$ Even though I find $e^{x^2}*x^2*(3+2x^2)$, the answer is $e^{x^2}*x^2*(3+2x)$. I wonder where do I make the mistake. What I did is as follows: By product rule: $$(x^3 * e^{x^2})' \Rightarrow 3x^2 * e^{x^2} + x^3 * (e^{x^2})'$$ Since $(e^{x^2})' = 2x * e^{x^2}$, the equation becomes  $$3x^2 * e^{x^2} + x^3 * 2x * e^{x^2}$$ $$e^{x^2} * x^2 * (3 + 2x^2)$$ Thanks",,"['calculus', 'derivatives', 'exponential-function']"
5,Why is the derivative at a jump undefined even if the slope remains the same?,Why is the derivative at a jump undefined even if the slope remains the same?,,I've searched online and found almost nothing. What in the mathematical definition of a derivative makes it so that the derivative of the following is undefined at 0 . \begin{equation*} f(x) =\begin{cases} 3x  & \text{if } x < 0 \\ 3x+2  & \text{if } x \ge 0 \end{cases} \end{equation*},I've searched online and found almost nothing. What in the mathematical definition of a derivative makes it so that the derivative of the following is undefined at 0 . \begin{equation*} f(x) =\begin{cases} 3x  & \text{if } x < 0 \\ 3x+2  & \text{if } x \ge 0 \end{cases} \end{equation*},,"['calculus', 'derivatives', 'definition']"
6,Proving derivative of $e^x$ and $\ln x$,Proving derivative of  and,e^x \ln x,"If we define the number $e$ as $$e:=\lim_{n\to\infty}\left(1+{1\over n}\right)^n$$ then the only way I know to prove the derivatives of $e^x$ and it's inverse is to write $$\frac{\ln(x+h)-\ln x}{h}={1\over h}\ln\frac{x+h}{x}=\ln\left[\left(1+{h\over x}\right)^{1/h}\right]$$ and with some limit manipulations this can be shown to converge as $h\to 0$ to $$\ln(e^{1/x})={1\over x}$$ Now using the formula for the derivative of the inverse, $$\frac{d}{dx}e^x=\frac{1}{1/e^x}=e^x$$ Is there a way to get the derivative of $e^x$ directly from the definition of $e$ given above?","If we define the number $e$ as $$e:=\lim_{n\to\infty}\left(1+{1\over n}\right)^n$$ then the only way I know to prove the derivatives of $e^x$ and it's inverse is to write $$\frac{\ln(x+h)-\ln x}{h}={1\over h}\ln\frac{x+h}{x}=\ln\left[\left(1+{h\over x}\right)^{1/h}\right]$$ and with some limit manipulations this can be shown to converge as $h\to 0$ to $$\ln(e^{1/x})={1\over x}$$ Now using the formula for the derivative of the inverse, $$\frac{d}{dx}e^x=\frac{1}{1/e^x}=e^x$$ Is there a way to get the derivative of $e^x$ directly from the definition of $e$ given above?",,['calculus']
7,How does the chain rule work for more than one variable?,How does the chain rule work for more than one variable?,,"I know that that $$\dfrac{d\sqrt{x}}{dt} = \dfrac{d\sqrt{x}}{dx} \dfrac{dx}{dt}$$ In this equation there you only have 1 variable, namely $x$. But why is the following correct?: $$T = \frac{1}{2} m \left(v_{x}^2 + v_{y}^2 + v_{z}^2 \right)$$ $$\dfrac{dT}{dt} = m \left( v_{x}  \dfrac{dv_{x}}{dt}  + v_{y}  \dfrac{dv_{y}}{dt} + v_{z}  \dfrac{dv_{z}}{dt}                \right)$$ How do you use the chain rule with this 3 variables and what is the mathematical proof for that?","I know that that $$\dfrac{d\sqrt{x}}{dt} = \dfrac{d\sqrt{x}}{dx} \dfrac{dx}{dt}$$ In this equation there you only have 1 variable, namely $x$. But why is the following correct?: $$T = \frac{1}{2} m \left(v_{x}^2 + v_{y}^2 + v_{z}^2 \right)$$ $$\dfrac{dT}{dt} = m \left( v_{x}  \dfrac{dv_{x}}{dt}  + v_{y}  \dfrac{dv_{y}}{dt} + v_{z}  \dfrac{dv_{z}}{dt}                \right)$$ How do you use the chain rule with this 3 variables and what is the mathematical proof for that?",,['derivatives']
8,Is there a set formula for integration like there is for derivatives?,Is there a set formula for integration like there is for derivatives?,,"I know that the derivative of $f(x)$ must be $$f'(x)=\lim\limits_{h\to 0} \frac{f(x+h)-f(x)}{h}$$ We can use this formula to derive the derivatives of some functions like $\sin(x)$. Is there such a formula for an integral? In other words, is there a set formula for computing: $$\int f(x) \ dx$$ Thanks in advance for your answers!","I know that the derivative of $f(x)$ must be $$f'(x)=\lim\limits_{h\to 0} \frac{f(x+h)-f(x)}{h}$$ We can use this formula to derive the derivatives of some functions like $\sin(x)$. Is there such a formula for an integral? In other words, is there a set formula for computing: $$\int f(x) \ dx$$ Thanks in advance for your answers!",,"['calculus', 'integration', 'derivatives']"
9,variational derivative,variational derivative,,"Let $\Omega \subset \mathbb{R}^n,\ n=1,2 \mbox{ or } 3$. Define the following energy $$E=\int_{\Omega} \frac{1}{\varepsilon}\left[f(u)+\frac{\varepsilon^2}{2}|\gamma(n)\nabla u|^2\right]\,dx$$ where, $$n=\frac{\nabla u}{|\nabla u|}$$ this problem talking about the derivation of Cahn-Hilliard System, $\gamma$ defines the anisotropy function, we take it as general as we want e.g. $$\gamma(n)=1+\Gamma(n)$$  and $f \in C^3(\Omega)$. the point is that the article talk about the variational derivative of $E$ w.r.t. $u$ and he get the following $$\frac{\delta E}{\delta u}=f'(u)-\varepsilon^2 \nabla \cdot \textbf{m}$$ where, $$\textbf{m}= \gamma^2 \textbf{p}+\gamma\ |\textbf{p}|\ \textbf{P} \nabla_n \gamma$$ with $$\textbf{p}=\nabla u\ ,\ \textbf{P}=|\nabla u|\ \frac{\partial n}{\partial \textbf{p}}$$ i start working in the variational derivative i get the following \begin{eqnarray*} \delta E &=&\int_{\Omega} \frac{1}{\varepsilon}\left[f'(u) \delta u+\varepsilon^2 (\gamma(n) \nabla u)\delta(\gamma(n) \nabla u)\right]\,dx\\\\ &=& \int_{\Omega}\frac{1}{\varepsilon} \left[ f'(u) \delta u+\varepsilon^2(\gamma(n) \nabla u)\left(\delta(\gamma(n)) \nabla u+\gamma(n)\delta(\nabla u)\right)\right]\,dx \end{eqnarray*} my problem is that i don't know how to continue to get $\textbf{m}$. any help? $\textbf{Remark :}$ This problem mentioned in ""S. Torabi, S. Wise, J. Lowengrub , A New Method for Simulating Strongly Anisotropic Cahn-Hilliard Equations""","Let $\Omega \subset \mathbb{R}^n,\ n=1,2 \mbox{ or } 3$. Define the following energy $$E=\int_{\Omega} \frac{1}{\varepsilon}\left[f(u)+\frac{\varepsilon^2}{2}|\gamma(n)\nabla u|^2\right]\,dx$$ where, $$n=\frac{\nabla u}{|\nabla u|}$$ this problem talking about the derivation of Cahn-Hilliard System, $\gamma$ defines the anisotropy function, we take it as general as we want e.g. $$\gamma(n)=1+\Gamma(n)$$  and $f \in C^3(\Omega)$. the point is that the article talk about the variational derivative of $E$ w.r.t. $u$ and he get the following $$\frac{\delta E}{\delta u}=f'(u)-\varepsilon^2 \nabla \cdot \textbf{m}$$ where, $$\textbf{m}= \gamma^2 \textbf{p}+\gamma\ |\textbf{p}|\ \textbf{P} \nabla_n \gamma$$ with $$\textbf{p}=\nabla u\ ,\ \textbf{P}=|\nabla u|\ \frac{\partial n}{\partial \textbf{p}}$$ i start working in the variational derivative i get the following \begin{eqnarray*} \delta E &=&\int_{\Omega} \frac{1}{\varepsilon}\left[f'(u) \delta u+\varepsilon^2 (\gamma(n) \nabla u)\delta(\gamma(n) \nabla u)\right]\,dx\\\\ &=& \int_{\Omega}\frac{1}{\varepsilon} \left[ f'(u) \delta u+\varepsilon^2(\gamma(n) \nabla u)\left(\delta(\gamma(n)) \nabla u+\gamma(n)\delta(\nabla u)\right)\right]\,dx \end{eqnarray*} my problem is that i don't know how to continue to get $\textbf{m}$. any help? $\textbf{Remark :}$ This problem mentioned in ""S. Torabi, S. Wise, J. Lowengrub , A New Method for Simulating Strongly Anisotropic Cahn-Hilliard Equations""",,"['calculus', 'derivatives', 'partial-differential-equations', 'partial-derivative']"
10,"From the graph of the derivative $f'(x)$, make a sketch of the original function $f(x)$ and of the second derivative $f''(x)$","From the graph of the derivative , make a sketch of the original function  and of the second derivative",f'(x) f(x) f''(x),"I haven't started finding the derivatives of functions yet, so at the moment this is strictly about finding the right derivative graph to an original graph. The task was this: Look at the graph below of the derivative $f'(x)$ . From this, make a sketch of the original function $f(x)$ and of the second derivative $f''(x)$ . In a mail, I was told this as well: The important part about this question is to match up the features of $f(x)$ , $f'(x)$ , and $f''(x)$ , such as $x$ -intercepts, CP's, POI's.  There are relationships between these features when comparing a graph with its derivatives, this is what you must demonstrate in your solution. I'm not quite sure what he means by CP's and I'm guessing POI's means points of interest. Anyway, I ended up with this drawing: A closer look at the relevant two: Sorry that they're a little big. My scanner scans with pretty high resolution. Anyway, does this look right? Did I miss any of the qualities the teacher said it was important to demonstrate?","I haven't started finding the derivatives of functions yet, so at the moment this is strictly about finding the right derivative graph to an original graph. The task was this: Look at the graph below of the derivative . From this, make a sketch of the original function and of the second derivative . In a mail, I was told this as well: The important part about this question is to match up the features of , , and , such as -intercepts, CP's, POI's.  There are relationships between these features when comparing a graph with its derivatives, this is what you must demonstrate in your solution. I'm not quite sure what he means by CP's and I'm guessing POI's means points of interest. Anyway, I ended up with this drawing: A closer look at the relevant two: Sorry that they're a little big. My scanner scans with pretty high resolution. Anyway, does this look right? Did I miss any of the qualities the teacher said it was important to demonstrate?",f'(x) f(x) f''(x) f(x) f'(x) f''(x) x,"['calculus', 'derivatives', 'solution-verification']"
11,AP Calculus Extrema Help,AP Calculus Extrema Help,,"A particle moves along the t-axis with displacement $s(t) = t^4-8t^3+18t^2+60t-8$. Find the largest and smallest values of its velocity for $1<=t<=5$. So is this correct: $v(t) = s'(t) = 4t^3-24t^2+36t+60$ Now do I solve for $v(t) = 0$, so $4t^3-24t^2+36t+60 = 0$, so $t=-.958$ So, $s(1) = 63, s(5) = 367, s(-.958) = -41.084$ Would the maximum be s(5) and the minimum be s(-.958)?","A particle moves along the t-axis with displacement $s(t) = t^4-8t^3+18t^2+60t-8$. Find the largest and smallest values of its velocity for $1<=t<=5$. So is this correct: $v(t) = s'(t) = 4t^3-24t^2+36t+60$ Now do I solve for $v(t) = 0$, so $4t^3-24t^2+36t+60 = 0$, so $t=-.958$ So, $s(1) = 63, s(5) = 367, s(-.958) = -41.084$ Would the maximum be s(5) and the minimum be s(-.958)?",,"['calculus', 'derivatives']"
12,Prove that $\lim_{\Delta x\to 0} \frac{\Delta ^{n}f(x)}{\Delta x^{n}} = f^{(n)}(x).$,Prove that,\lim_{\Delta x\to 0} \frac{\Delta ^{n}f(x)}{\Delta x^{n}} = f^{(n)}(x).,"If $\Delta f(x)=f(x+\Delta x)-f(x)$, $(a)$ prove that $$\Delta\{\Delta f(x)\}=\Delta^2f(x)=f(x+2\Delta x)-2f(x+\Delta x)+f(x);$$ $(b)$ derive an expression for $\Delta^n f(x)$ where $n$ is any positive integer; and $(c)$ show that $$\lim\limits_{\Delta x\to0}\dfrac{\Delta^n f(x)}{(\Delta x)^n}=f^{(n)}(x)$$ if this limit exists. I was able to prove $(a)$, and this is the expression I derived for $(b)$ $$\Delta ^{n}f(x)=\sum_{i=0}^{n}(-1)^{n-i}\binom{n}{i}f(x+i\Delta x)$$ I an fairly sure that the above is correct. However, I am not sure how to prove, $$\lim_{\Delta x\rightarrow 0} \frac{\Delta ^{n}f(x)}{\Delta x^{n}} = \lim_{\Delta x\rightarrow 0} \frac{\sum_{i=0}^{n}(-1)^{n-i}\binom{n}{i}f(x+i\Delta x)}{\Delta x^{n}} = f^{(n)}(x)$$","If $\Delta f(x)=f(x+\Delta x)-f(x)$, $(a)$ prove that $$\Delta\{\Delta f(x)\}=\Delta^2f(x)=f(x+2\Delta x)-2f(x+\Delta x)+f(x);$$ $(b)$ derive an expression for $\Delta^n f(x)$ where $n$ is any positive integer; and $(c)$ show that $$\lim\limits_{\Delta x\to0}\dfrac{\Delta^n f(x)}{(\Delta x)^n}=f^{(n)}(x)$$ if this limit exists. I was able to prove $(a)$, and this is the expression I derived for $(b)$ $$\Delta ^{n}f(x)=\sum_{i=0}^{n}(-1)^{n-i}\binom{n}{i}f(x+i\Delta x)$$ I an fairly sure that the above is correct. However, I am not sure how to prove, $$\lim_{\Delta x\rightarrow 0} \frac{\Delta ^{n}f(x)}{\Delta x^{n}} = \lim_{\Delta x\rightarrow 0} \frac{\sum_{i=0}^{n}(-1)^{n-i}\binom{n}{i}f(x+i\Delta x)}{\Delta x^{n}} = f^{(n)}(x)$$",,"['calculus', 'derivatives', 'proof-writing']"
13,differentiate log Gamma function,differentiate log Gamma function,,"I am working with the log of negative binomial distribution $NegBin(r,p)$. I need to differentiate the following with respect to $r$ such that at the end, I am NOT left with $r$ in factorial form. $$\frac{d \;log( \Gamma(x + r) )}{dr} = \;\; ?$$ I read about digamma function, but I have $x+r$ within the bracket in my case. Can anyone show me how this can be done ?","I am working with the log of negative binomial distribution $NegBin(r,p)$. I need to differentiate the following with respect to $r$ such that at the end, I am NOT left with $r$ in factorial form. $$\frac{d \;log( \Gamma(x + r) )}{dr} = \;\; ?$$ I read about digamma function, but I have $x+r$ within the bracket in my case. Can anyone show me how this can be done ?",,"['derivatives', 'gamma-function']"
14,$f''$ bounded implies $f'$ is bounded,bounded implies  is bounded,f'' f',"Suppose that $f''$ exists on $[0,1]$ and that $f(0)=0=f(1)$. Suppose also that $|f''(x)|\le K$ for $x\in(0,1)$. Prove that $|f'(1/2)|\le K/4$ and that $|f'(x)|\le K/2$ for $x\in [0,1]$. The mean-value theorem might help, but I can't see how.","Suppose that $f''$ exists on $[0,1]$ and that $f(0)=0=f(1)$. Suppose also that $|f''(x)|\le K$ for $x\in(0,1)$. Prove that $|f'(1/2)|\le K/4$ and that $|f'(x)|\le K/2$ for $x\in [0,1]$. The mean-value theorem might help, but I can't see how.",,"['calculus', 'derivatives']"
15,Find the second derivative of $e^{x^{3}}+7x$,Find the second derivative of,e^{x^{3}}+7x,"$$e^{x^{3}}+7x$$ Here is what I have tried so far, $$y'=(e^{x^{3}})'+(7x)'$$ $$y'=3e^{x^{2}}e^x+7$$ $$y'=3e^{x^{3}+x}+7$$ I am supposed to find the second derivative but I believe I have already made a mistake. I am not sure how I would apply the chain rule to the $e^{x^{3}}$. I checked Wolframalpha and the derivative of $e^{x^{3}}$ was $$3e^{x^{3}}x^2$$ Did I leave out a step when I applied chain rule? I went over the problem a couple of times and couldn't catch my mistake.","$$e^{x^{3}}+7x$$ Here is what I have tried so far, $$y'=(e^{x^{3}})'+(7x)'$$ $$y'=3e^{x^{2}}e^x+7$$ $$y'=3e^{x^{3}+x}+7$$ I am supposed to find the second derivative but I believe I have already made a mistake. I am not sure how I would apply the chain rule to the $e^{x^{3}}$. I checked Wolframalpha and the derivative of $e^{x^{3}}$ was $$3e^{x^{3}}x^2$$ Did I leave out a step when I applied chain rule? I went over the problem a couple of times and couldn't catch my mistake.",,"['calculus', 'derivatives', 'wolfram-alpha']"
16,Functions with discontinuous derivative at the endpoints of an open interval,Functions with discontinuous derivative at the endpoints of an open interval,,"(a) Does there exist a function $f$ defined on the open interval $(a,b)$ such that $f'(b^-)$ exists, and $\lim_{x\to b-}f'(x)\neq f'(b^-)$, or (b) where $f'(b^-)$ exists and $\lim_{x\to b-}f'(x)$ does not exist? Since $f(b)$ is undefined, define $$f'(b^-)=\lim_{h\to0+}\frac{f(b-h)-f(b-2h)}h.$$ Are there any difficulties with this definition as compared to the standard definition of the one-sided derivative? Just reading my analysis textbook and thought this would make an interesting problem. Related: $f(x)=x^2\sin\frac1x$ has a derivative which is defined at $0$ (equal to $0$), but $\lim_{x\to 0}f'(x)$ does not exist. (c) Is it always true that if the limit exists, it is equal to $f'(0)$? Even more curiously, $\limsup_{x\to0}f'(x)+\liminf_{x\to0}f'(x)=2f'(0)$ for this function. (d) Is this always the case, when the quantity on the left side of the equality is defined?","(a) Does there exist a function $f$ defined on the open interval $(a,b)$ such that $f'(b^-)$ exists, and $\lim_{x\to b-}f'(x)\neq f'(b^-)$, or (b) where $f'(b^-)$ exists and $\lim_{x\to b-}f'(x)$ does not exist? Since $f(b)$ is undefined, define $$f'(b^-)=\lim_{h\to0+}\frac{f(b-h)-f(b-2h)}h.$$ Are there any difficulties with this definition as compared to the standard definition of the one-sided derivative? Just reading my analysis textbook and thought this would make an interesting problem. Related: $f(x)=x^2\sin\frac1x$ has a derivative which is defined at $0$ (equal to $0$), but $\lim_{x\to 0}f'(x)$ does not exist. (c) Is it always true that if the limit exists, it is equal to $f'(0)$? Even more curiously, $\limsup_{x\to0}f'(x)+\liminf_{x\to0}f'(x)=2f'(0)$ for this function. (d) Is this always the case, when the quantity on the left side of the equality is defined?",,"['real-analysis', 'derivatives']"
17,"Using Fundamental Theorem of Calculus ""Twice""","Using Fundamental Theorem of Calculus ""Twice""",,"Find $ F''(1)$ if $$ F(x) = \int_1^x f(t)dt $$ $$ f(t) = \int_1^{2t} \sqrt{1+u^3} du $$ My work: From the looks of it, it looks like the Fundamental Theorem of Calculus, twice. From FTC... $F'(x) = f(x) $ On the second equation, derive both sides and use chain rule: $f'(t)= d/dt*[ \int_1^{2t} \sqrt{1+u^3} du ] $ Using $z=2t$ $f'(t)=d/dz*[\int_1^{z} \sqrt{1+u^3} du]*dz/dt $ $f'(t) = 2 * \sqrt{1+(2t)^3} $ Just by gut feeling, I want to plug in $t=1$ and solve. Does $f'(t)$ represent $F''(x)$ ?   $t$ and $x$ are different variable. Am I plugging in $1$ too early?","Find $ F''(1)$ if $$ F(x) = \int_1^x f(t)dt $$ $$ f(t) = \int_1^{2t} \sqrt{1+u^3} du $$ My work: From the looks of it, it looks like the Fundamental Theorem of Calculus, twice. From FTC... $F'(x) = f(x) $ On the second equation, derive both sides and use chain rule: $f'(t)= d/dt*[ \int_1^{2t} \sqrt{1+u^3} du ] $ Using $z=2t$ $f'(t)=d/dz*[\int_1^{z} \sqrt{1+u^3} du]*dz/dt $ $f'(t) = 2 * \sqrt{1+(2t)^3} $ Just by gut feeling, I want to plug in $t=1$ and solve. Does $f'(t)$ represent $F''(x)$ ?   $t$ and $x$ are different variable. Am I plugging in $1$ too early?",,"['calculus', 'integration', 'derivatives']"
18,Derivative of a particular matrix valued function with respect to a vector,Derivative of a particular matrix valued function with respect to a vector,,"I am reading a section of a book regarding linear regression and came across a derivation that I could not follow. It starts with a loss function: $\mathcal{L}(\textbf{w},S) = (\textbf{y}-\textbf{X}\textbf{w})^\top(\textbf{y}-\textbf{X}\textbf{w})$ and then states that ""We can seek the optimal $\textbf{w}$ by taking the derivatives of the loss with respect to $\textbf{w}$ and setting them to the zero vector"" $\frac{\partial\mathcal{L}(\textbf{w},S)}{\partial\textbf{w}} = -2\textbf{X}^{\top}\textbf{y} + 2\textbf{X}^\top\textbf{X}\textbf{w} = \textbf{0}$ How is this derivative being calculated? I find that I have no idea how to take the derivative of vector or matrix valued functions, especially when the derivative is with respect to a vector, however I found a pdf ( http://orion.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf ) which appears to address some of my questions, yet my attempts at taking the derivative of the loss function seem to be missing a transpose and thus does not reduce as nicely as the books result.","I am reading a section of a book regarding linear regression and came across a derivation that I could not follow. It starts with a loss function: $\mathcal{L}(\textbf{w},S) = (\textbf{y}-\textbf{X}\textbf{w})^\top(\textbf{y}-\textbf{X}\textbf{w})$ and then states that ""We can seek the optimal $\textbf{w}$ by taking the derivatives of the loss with respect to $\textbf{w}$ and setting them to the zero vector"" $\frac{\partial\mathcal{L}(\textbf{w},S)}{\partial\textbf{w}} = -2\textbf{X}^{\top}\textbf{y} + 2\textbf{X}^\top\textbf{X}\textbf{w} = \textbf{0}$ How is this derivative being calculated? I find that I have no idea how to take the derivative of vector or matrix valued functions, especially when the derivative is with respect to a vector, however I found a pdf ( http://orion.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf ) which appears to address some of my questions, yet my attempts at taking the derivative of the loss function seem to be missing a transpose and thus does not reduce as nicely as the books result.",,['derivatives']
19,result involving derivative implies polynomial,result involving derivative implies polynomial,,"I've stumbled upon the following exercise and I have no clue as to how one would prove the following statement : let $f$ be a real valued function, such that: $f$ has a non positive fourth derivative on $\mathbb{R}$ $f$ is positive on $\mathbb{R}$ Then $f$ is a polynomial of degree strictly less than 3. Any help would be appreciated.","I've stumbled upon the following exercise and I have no clue as to how one would prove the following statement : let be a real valued function, such that: has a non positive fourth derivative on is positive on Then is a polynomial of degree strictly less than 3. Any help would be appreciated.",f f \mathbb{R} f \mathbb{R} f,"['derivatives', 'polynomials']"
20,Let $D:\mathbb{R}(x)\to\mathbb{R}(x)$ be the algebraic(!) differential operator such that $D(x)=1$. Is it true that $D(c)=0$ for all $c\in\mathbb{R}$?,Let  be the algebraic(!) differential operator such that . Is it true that  for all ?,D:\mathbb{R}(x)\to\mathbb{R}(x) D(x)=1 D(c)=0 c\in\mathbb{R},"Define the field of rational functions $\mathbb{R}(x)$ with algebraic differential operator $D:\mathbb{R}(x)\to\mathbb{R}(x)$ such that $D(x)=1$ . By algebraic differential operator, we mean a mapping such that the additive rule and product rule apply for all $f,g\in\mathbb{R}(x)$ : $$D(f+g)=D(f)+D(g),$$ $$D(fg)=fD(g)+gD(f).$$ Also, with this definition of the derivative, the following are easy to prove: $D(0)=D(1)=0$ ; $D(-f)=-D(f)$ ; $D\left(\frac{f}{g}\right)=\frac{gD(f)-fD(g)}{g^2}$ for $g\neq0$ ; $D(f^n)=nf^{n-1}D(f)$ for every $n\in\mathbb{Z}$ and $f\neq0$ ; If $c\in\mathbb{R}(x)$ is such that $D(c)=0$ , then $D(cf)=cD(f)$ . I want to prove that, for all $c\in\mathbb{R}$ , we have $D(c)=0$ . I've already proved that this is true if $c\in\mathbb{Q}$ . I wanted to use an argument that involved sequences, like using the fact that for every $c\in\mathbb{R}$ we could define a sequence $(c_n)\subseteq\mathbb{Q}$ such that $c_n\to c$ . However, this doesn't work, as the differential operator is (usually) not continuous, and we can't just do $$D(c)=D\left(\lim_{n\to\infty}c_n\right)=\lim_{n\to\infty}D(c_n)=\lim_{n\to\infty}0=0.$$ Hence, I'm stumped. For now I've been defining the field of constants of $\mathbb{R}(x)$ as $\mathbb{R}$ , but I would like to know if this result can be proved just using algebra and non-differential tools from analysis.","Define the field of rational functions with algebraic differential operator such that . By algebraic differential operator, we mean a mapping such that the additive rule and product rule apply for all : Also, with this definition of the derivative, the following are easy to prove: ; ; for ; for every and ; If is such that , then . I want to prove that, for all , we have . I've already proved that this is true if . I wanted to use an argument that involved sequences, like using the fact that for every we could define a sequence such that . However, this doesn't work, as the differential operator is (usually) not continuous, and we can't just do Hence, I'm stumped. For now I've been defining the field of constants of as , but I would like to know if this result can be proved just using algebra and non-differential tools from analysis.","\mathbb{R}(x) D:\mathbb{R}(x)\to\mathbb{R}(x) D(x)=1 f,g\in\mathbb{R}(x) D(f+g)=D(f)+D(g), D(fg)=fD(g)+gD(f). D(0)=D(1)=0 D(-f)=-D(f) D\left(\frac{f}{g}\right)=\frac{gD(f)-fD(g)}{g^2} g\neq0 D(f^n)=nf^{n-1}D(f) n\in\mathbb{Z} f\neq0 c\in\mathbb{R}(x) D(c)=0 D(cf)=cD(f) c\in\mathbb{R} D(c)=0 c\in\mathbb{Q} c\in\mathbb{R} (c_n)\subseteq\mathbb{Q} c_n\to c D(c)=D\left(\lim_{n\to\infty}c_n\right)=\lim_{n\to\infty}D(c_n)=\lim_{n\to\infty}0=0. \mathbb{R}(x) \mathbb{R}","['abstract-algebra', 'derivatives', 'field-theory', 'differential-algebra']"
21,"The integral solved $\int x^{\ln(x)-1}\ln(x)\,\mathrm dx$",The integral solved,"\int x^{\ln(x)-1}\ln(x)\,\mathrm dx","In the past time I was trying to find a common answer for $\displaystyle\int x^{\ln(x)-1}\ln(x)\,\mathrm dx$ , I actually answered the question which pattern is as follows. $\begin{align*} \displaystyle\int x^{\ln(x)-1}\ln(x)\,\mathrm dx&=\int x^{\ln(x)}x^{-1}\ln(x)\,\mathrm dx \tag*{$\alpha^{m+n}=\alpha^m\alpha^n$} \\ &=\int \frac{x^{\ln(x)}\ln(x)}{x}\,\mathrm dx\\ &=\int\frac{\left( \left( e^{\ln(x)} \right)^{\ln(x)} \right)\ln(x)}{x}\,\mathrm dx\\ &=\int\frac{e^{\ln^2(x)}\ln(x)}{x}\,\mathrm dx \end{align*}\tag*{1}$ Use the Integration by substitution Let $\begin{aligned} u&=\ln(x) \\\dfrac{\mathrm{d}u}{\mathrm{d}x}&=\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ \ln(x) \right] \\ \mathrm du &=\frac{1}{x}\,\mathrm dx \iff\mathrm dx=x\,\mathrm du \end{aligned}\tag*{2}$ $\text{We obtain $\displaystyle\int\frac{e^{\ln^2(x)}\ln(x)}{x}\,\mathrm dx =\int ue^{u^2}\,\mathrm du$}\tag*{3}$ Apply the integration by substitution twice and this time I would use $\alpha$ since I have used $u$ previously. $\begin{aligned} \alpha&=u^2\\\dfrac{\mathrm d\alpha}{\mathrm du} &=\dfrac{\mathrm d}{\mathrm du}\left[ u^2 \right] \\ \mathrm d\alpha&= 2u\,\mathrm du\implies\mathrm du=\frac{1}{2u}\,\mathrm d\alpha \end{aligned}\tag*{4}$ Hence $\begin{aligned} \displaystyle\int\frac{e^{\ln^2(x)}\ln(x)}{x}\,\mathrm dx =\int ue^{u^2}\,\mathrm du\implies\int ue^{u^2}\,\mathrm du&= \frac12\int e^{\alpha}\,\mathrm d\alpha \\ &=\frac{1}{2}e^{\alpha}+C\\ &=\frac{e^{u^2}}{2}+C\qquad \alpha\implies u\\ &=\frac{e^{{\ln^2(x)}}}{2}+C \qquad u\implies x \\&=\frac{e^{\ln(x)\ln(x)}}{2}+C\\ &=\frac{\left( e^{\ln(x)} \right)^{\ln(x)}}{2}+C\\&= \boxed{\frac{x^{\ln(x)}}{2}+C} \end{aligned}\tag*{5}$ Use logarithmic differentiation to tell that $\frac{e^{{\ln^2(x)}}}{2}+C$ is correct. Let $y=\frac{e^{{\ln^2(x)}}}{2}$ . $\begin{aligned} y&=e^{\ln^2(x)}\\ \ln(y) &=\ln\left( e^{\ln^2(x)} \right) \\ \ln(y) &=\ln^2(x)\ln(e)\\ \ln(y) &=\ln^2(x)\\\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ \ln(y) \right] &=\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ \ln^2(x) \right]\\ \dfrac{\mathrm{d}}{\mathrm{d}y}\left[ \ln(y) \right]\dfrac{\mathrm{d}y}{\mathrm{d}x}&=\dfrac{\mathrm{d}}{\mathrm{d}\alpha}\left[\alpha^2\right]\dfrac{\mathrm{d}}{\mathrm{d}x}[\ln(x)]\\ \frac1y\dfrac{\mathrm{d}y}{\mathrm{d}x}&=2\alpha\left( \frac{1}{x} \right)\\\frac1y\dfrac{\mathrm{d}y}{\mathrm{d}x}&=\frac{2\ln(x)}{x}\\ \dfrac{\mathrm{d}y}{\mathrm{d}x}&=y\left( \frac{2\ln(x)}{x} \right)\\ \dfrac{\mathrm{d}y}{\mathrm{d}x}&=\frac{2e^{\ln^2(x)}\ln(x)}{x} \end{aligned}\tag*{6}$ Since we have the $\frac{e^{\ln^2(x)}}{2}$ , then we can say that like this $\begin{aligned}\displaystyle y=\frac{e^{\ln^2(x)}}{2}&\implies\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ \frac{e^{\ln^2(x)}}{2} \right]\\&\implies\frac12\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ e^{\ln^2(x)} \right]\\&\implies\frac{1}{2}\left( \frac{2e^{\ln^2(x)}\ln(x)}{x} \right)\\& \implies\frac{e^{\ln^2(x)}\ln(x)}{x} \\ &\implies\frac{e^{\ln(x)\ln(x)}\ln(x)}{x}\\ &\implies\frac{\left( e^{\ln(x)} \right)^{\ln(x))}\ln(x)}{x}\\&\implies\frac{x^{\ln(x)}\ln(x)}{x}\\ &\implies x^{\ln(x)-1}\ln(x)\end{aligned}\tag*{7}$ The second version is by substituting $u=x^{\ln(x)}$ . $\displaystyle\int x^{\ln(x)-1}\ln(x)\,\mathrm dx \tag*{8}$ Let $\begin{aligned} u&=x^{\ln(x)}\\ \frac{\mathrm du}{\mathrm dx}&=\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ x^{\ln(x)} \right]^* \\\mathrm du &= 2x^{\ln(x)-1}\ln(x)\,\mathrm dx \implies\frac{\mathrm{d}u}{2x^{\ln(x)-1}\ln(x)}=\,\mathrm dx \end{aligned}\tag*{9}$ Then $\begin{aligned} \displaystyle \int x^{\ln(x)-1}\ln(x)\,\mathrm dx&=\int x^{\ln(x)-1}\ln(x)\cdot\frac{1}{2x^{\ln(x)-1}\ln(x)}\,\mathrm du\\ &=\frac12\int 1\,\mathrm du\\ &=\frac12[u]\\ &=\frac{x^{\ln(x)}}{2}+C \end{aligned}\tag*{10}$ *Proof $\frac{\mathrm{d}}{\mathrm{d}x}\left[ x^{\ln(x)} \right]=2x^{\ln(x)-1}\ln(x)$ To proof this let $y=x^{\ln(x)}$ $\begin{aligned} y&=x^{\ln(x)} \\ \ln(y)&=\ln\left( x^{\ln(x)} \right)\\ \ln(y)&=\ln(x)\ln(x)\\ \ln(y)&=\ln^2(x)\\\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ \ln(y) \right] &=\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ \ln^2(x) \right]\\ \dfrac{\mathrm{d}}{\mathrm{d}y}\left[ \ln(y) \right]\dfrac{\mathrm{d}y}{\mathrm{d}x}&=\dfrac{\mathrm{d}}{\mathrm{d}\alpha}\left[\alpha^2\right]\dfrac{\mathrm{d}}{\mathrm{d}x}[\ln(x)]\\ \frac1y\dfrac{\mathrm{d}y}{\mathrm{d}x}&=2\alpha\left( \frac{1}{x} \right)\\\frac1y\dfrac{\mathrm{d}y}{\mathrm{d}x}&=\frac{2\ln(x)}{x}\\ \dfrac{\mathrm{d}y}{\mathrm{d}x}&=y\left( \frac{2\ln(x)}{x} \right)\\&=\frac{2x^{\ln(x)-1}\ln(x)}{x}\\\dfrac{\mathrm{d}y}{\mathrm{d}x} &=2x^{\ln(x)-1}\ln(x)\end{aligned}\tag*{11}$ I tried asking various sources including AI sites whether there is a match between the functions $e^{\ln^2(x)}$ and $x^{\ln(x)}$ because the derivatives are the same and I checked the results my way of doing this with the help of Integral Calculator and looks as follows. $\displaystyle \int x^{\ln(x)-1}\ln(x)\,\mathrm dx$ "" /> $u= \ln(x)$ )"" /> $u= x^{\ln(x)}$ ) and the antiderivative results via maxima"" /> I also prove that what I'm doing is correct by differentiating the function for the result of the integral in the first way as shown in (5) using logarithmic differentiation as I did in (6) and how I get $x^{\ln (x)}$ from $e^{\ln^2(x)}$ I separate the $\ln^2(x)$ into $\ln(x)\ln(x)$ then because I use exponential rule that $\left(\alpha^{m}\right)^n =\alpha^{mn}$ and $e^{\ln(x)}= x$ because $e^{\ln(\alpha )}=1^{\alpha}=\alpha$ . So do you think what I'm doing is right?","In the past time I was trying to find a common answer for , I actually answered the question which pattern is as follows. Use the Integration by substitution Let Apply the integration by substitution twice and this time I would use since I have used previously. Hence Use logarithmic differentiation to tell that is correct. Let . Since we have the , then we can say that like this The second version is by substituting . Let Then *Proof To proof this let I tried asking various sources including AI sites whether there is a match between the functions and because the derivatives are the same and I checked the results my way of doing this with the help of Integral Calculator and looks as follows. $\displaystyle \int x^{\ln(x)-1}\ln(x)\,\mathrm dx$ "" /> $u= \ln(x)$ )"" /> $u= x^{\ln(x)}$ ) and the antiderivative results via maxima"" /> I also prove that what I'm doing is correct by differentiating the function for the result of the integral in the first way as shown in (5) using logarithmic differentiation as I did in (6) and how I get from I separate the into then because I use exponential rule that and because . So do you think what I'm doing is right?","\displaystyle\int x^{\ln(x)-1}\ln(x)\,\mathrm dx \begin{align*} \displaystyle\int x^{\ln(x)-1}\ln(x)\,\mathrm dx&=\int x^{\ln(x)}x^{-1}\ln(x)\,\mathrm dx \tag*{\alpha^{m+n}=\alpha^m\alpha^n} \\ &=\int \frac{x^{\ln(x)}\ln(x)}{x}\,\mathrm dx\\ &=\int\frac{\left( \left( e^{\ln(x)} \right)^{\ln(x)} \right)\ln(x)}{x}\,\mathrm dx\\ &=\int\frac{e^{\ln^2(x)}\ln(x)}{x}\,\mathrm dx \end{align*}\tag*{1} \begin{aligned} u&=\ln(x) \\\dfrac{\mathrm{d}u}{\mathrm{d}x}&=\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ \ln(x) \right] \\ \mathrm du &=\frac{1}{x}\,\mathrm dx \iff\mathrm dx=x\,\mathrm du \end{aligned}\tag*{2} \text{We obtain \displaystyle\int\frac{e^{\ln^2(x)}\ln(x)}{x}\,\mathrm dx =\int ue^{u^2}\,\mathrm du}\tag*{3} \alpha u \begin{aligned} \alpha&=u^2\\\dfrac{\mathrm d\alpha}{\mathrm du} &=\dfrac{\mathrm d}{\mathrm du}\left[ u^2 \right] \\ \mathrm d\alpha&= 2u\,\mathrm du\implies\mathrm du=\frac{1}{2u}\,\mathrm d\alpha \end{aligned}\tag*{4} \begin{aligned} \displaystyle\int\frac{e^{\ln^2(x)}\ln(x)}{x}\,\mathrm dx =\int ue^{u^2}\,\mathrm du\implies\int ue^{u^2}\,\mathrm du&= \frac12\int e^{\alpha}\,\mathrm d\alpha \\ &=\frac{1}{2}e^{\alpha}+C\\ &=\frac{e^{u^2}}{2}+C\qquad \alpha\implies u\\ &=\frac{e^{{\ln^2(x)}}}{2}+C \qquad u\implies x \\&=\frac{e^{\ln(x)\ln(x)}}{2}+C\\ &=\frac{\left( e^{\ln(x)} \right)^{\ln(x)}}{2}+C\\&= \boxed{\frac{x^{\ln(x)}}{2}+C} \end{aligned}\tag*{5} \frac{e^{{\ln^2(x)}}}{2}+C y=\frac{e^{{\ln^2(x)}}}{2} \begin{aligned} y&=e^{\ln^2(x)}\\ \ln(y) &=\ln\left( e^{\ln^2(x)} \right) \\ \ln(y) &=\ln^2(x)\ln(e)\\ \ln(y) &=\ln^2(x)\\\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ \ln(y) \right] &=\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ \ln^2(x) \right]\\ \dfrac{\mathrm{d}}{\mathrm{d}y}\left[ \ln(y) \right]\dfrac{\mathrm{d}y}{\mathrm{d}x}&=\dfrac{\mathrm{d}}{\mathrm{d}\alpha}\left[\alpha^2\right]\dfrac{\mathrm{d}}{\mathrm{d}x}[\ln(x)]\\ \frac1y\dfrac{\mathrm{d}y}{\mathrm{d}x}&=2\alpha\left( \frac{1}{x} \right)\\\frac1y\dfrac{\mathrm{d}y}{\mathrm{d}x}&=\frac{2\ln(x)}{x}\\ \dfrac{\mathrm{d}y}{\mathrm{d}x}&=y\left( \frac{2\ln(x)}{x} \right)\\ \dfrac{\mathrm{d}y}{\mathrm{d}x}&=\frac{2e^{\ln^2(x)}\ln(x)}{x} \end{aligned}\tag*{6} \frac{e^{\ln^2(x)}}{2} \begin{aligned}\displaystyle y=\frac{e^{\ln^2(x)}}{2}&\implies\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ \frac{e^{\ln^2(x)}}{2} \right]\\&\implies\frac12\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ e^{\ln^2(x)} \right]\\&\implies\frac{1}{2}\left( \frac{2e^{\ln^2(x)}\ln(x)}{x} \right)\\& \implies\frac{e^{\ln^2(x)}\ln(x)}{x} \\ &\implies\frac{e^{\ln(x)\ln(x)}\ln(x)}{x}\\ &\implies\frac{\left( e^{\ln(x)} \right)^{\ln(x))}\ln(x)}{x}\\&\implies\frac{x^{\ln(x)}\ln(x)}{x}\\ &\implies x^{\ln(x)-1}\ln(x)\end{aligned}\tag*{7} u=x^{\ln(x)} \displaystyle\int x^{\ln(x)-1}\ln(x)\,\mathrm dx \tag*{8} \begin{aligned} u&=x^{\ln(x)}\\ \frac{\mathrm du}{\mathrm dx}&=\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ x^{\ln(x)} \right]^* \\\mathrm du &= 2x^{\ln(x)-1}\ln(x)\,\mathrm dx \implies\frac{\mathrm{d}u}{2x^{\ln(x)-1}\ln(x)}=\,\mathrm dx \end{aligned}\tag*{9} \begin{aligned} \displaystyle \int x^{\ln(x)-1}\ln(x)\,\mathrm dx&=\int x^{\ln(x)-1}\ln(x)\cdot\frac{1}{2x^{\ln(x)-1}\ln(x)}\,\mathrm du\\ &=\frac12\int 1\,\mathrm du\\ &=\frac12[u]\\ &=\frac{x^{\ln(x)}}{2}+C \end{aligned}\tag*{10} \frac{\mathrm{d}}{\mathrm{d}x}\left[ x^{\ln(x)} \right]=2x^{\ln(x)-1}\ln(x) y=x^{\ln(x)} \begin{aligned} y&=x^{\ln(x)} \\ \ln(y)&=\ln\left( x^{\ln(x)} \right)\\ \ln(y)&=\ln(x)\ln(x)\\ \ln(y)&=\ln^2(x)\\\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ \ln(y) \right] &=\dfrac{\mathrm{d}}{\mathrm{d}x}\left[ \ln^2(x) \right]\\ \dfrac{\mathrm{d}}{\mathrm{d}y}\left[ \ln(y) \right]\dfrac{\mathrm{d}y}{\mathrm{d}x}&=\dfrac{\mathrm{d}}{\mathrm{d}\alpha}\left[\alpha^2\right]\dfrac{\mathrm{d}}{\mathrm{d}x}[\ln(x)]\\ \frac1y\dfrac{\mathrm{d}y}{\mathrm{d}x}&=2\alpha\left( \frac{1}{x} \right)\\\frac1y\dfrac{\mathrm{d}y}{\mathrm{d}x}&=\frac{2\ln(x)}{x}\\ \dfrac{\mathrm{d}y}{\mathrm{d}x}&=y\left( \frac{2\ln(x)}{x} \right)\\&=\frac{2x^{\ln(x)-1}\ln(x)}{x}\\\dfrac{\mathrm{d}y}{\mathrm{d}x} &=2x^{\ln(x)-1}\ln(x)\end{aligned}\tag*{11} e^{\ln^2(x)} x^{\ln(x)} x^{\ln (x)} e^{\ln^2(x)} \ln^2(x) \ln(x)\ln(x) \left(\alpha^{m}\right)^n =\alpha^{mn} e^{\ln(x)}= x e^{\ln(\alpha )}=1^{\alpha}=\alpha","['calculus', 'derivatives', 'solution-verification', 'logarithms', 'indefinite-integrals']"
22,Simplified expression for the nth derivative of the n+1th power of a function,Simplified expression for the nth derivative of the n+1th power of a function,,"$$\frac{d^n}{dx^n}(f(x))^n \text{ and similar expressions}$$ I was trying to work out the Taylor series for the solution to the general cubic using Lagrange Inversion (out of curiosity: I was wondering which of the roots the series would give for different cubics) and got stuck on finding the $n^{th}$ derivative of $ (\frac{1}{ax^2+bx+c})^{n+1} $ . First I tried working it out by hand but I couldn't see any obvious pattern that would allow me to compute the general case. Wolfram Alpha was able to calculate the next few values of n and I noticed some things but there was no obvious pattern for the general term. I realized I could do it by employing the Laplace Transform (or the Fourier Transform or maybe even directly from the Taylor Series) where taking n+1 derivatives is trivial but found it rather difficult to compute any of these in general since the powers of $ \frac{1}{ax^2+bx+c}$ are rather chaotic. I am a bit stuck on this. Also, if someone knows how to do this, I'm now curious if there is a way to get the general ${n-1}^{th}$ , $n^{th}$ , or ${n+1}^{th}$ derivatives of the $n^{th}$ or ${g(n)}^{th}$ powers of a function f(x) for some function g(n), and as to where the coefficients of these terms come from (some sort of analogue of Pascal's or one of Stirling's triangles maybe?). For example, the coefficients of the $n^{th}$ derivatives of $(f(x))^n$ can be arranged in a triangle like this when you arrange them in increasing order in each row: $$\text{1}$$ $$\text{2  2}$$ $$\text{3  6  18}$$ $$\text{4  24  36  48  144}$$ But I don't remember seeing these numbers anywhere and couldn't even find this sequence (read out by rows) in the OEIS.","I was trying to work out the Taylor series for the solution to the general cubic using Lagrange Inversion (out of curiosity: I was wondering which of the roots the series would give for different cubics) and got stuck on finding the derivative of . First I tried working it out by hand but I couldn't see any obvious pattern that would allow me to compute the general case. Wolfram Alpha was able to calculate the next few values of n and I noticed some things but there was no obvious pattern for the general term. I realized I could do it by employing the Laplace Transform (or the Fourier Transform or maybe even directly from the Taylor Series) where taking n+1 derivatives is trivial but found it rather difficult to compute any of these in general since the powers of are rather chaotic. I am a bit stuck on this. Also, if someone knows how to do this, I'm now curious if there is a way to get the general , , or derivatives of the or powers of a function f(x) for some function g(n), and as to where the coefficients of these terms come from (some sort of analogue of Pascal's or one of Stirling's triangles maybe?). For example, the coefficients of the derivatives of can be arranged in a triangle like this when you arrange them in increasing order in each row: But I don't remember seeing these numbers anywhere and couldn't even find this sequence (read out by rows) in the OEIS.",\frac{d^n}{dx^n}(f(x))^n \text{ and similar expressions} n^{th}  (\frac{1}{ax^2+bx+c})^{n+1}   \frac{1}{ax^2+bx+c} {n-1}^{th} n^{th} {n+1}^{th} n^{th} {g(n)}^{th} n^{th} (f(x))^n \text{1} \text{2  2} \text{3  6  18} \text{4  24  36  48  144},"['sequences-and-series', 'derivatives', 'taylor-expansion', 'laplace-transform', 'lagrange-inversion']"
23,Derivative of $y = \sin^3(\frac\pi 3(\cos(\frac\pi{3\sqrt2}(-4x^3 + 5x^2 + 1)^{3/2})))$ at $x=1$,Derivative of  at,y = \sin^3(\frac\pi 3(\cos(\frac\pi{3\sqrt2}(-4x^3 + 5x^2 + 1)^{3/2}))) x=1,"Today  I came across a problem: If $y = \sin^3\left(\frac\pi 3\left(\cos\left(\frac\pi{3\sqrt2}\left(-4x^3 + 5x^2 + 1\right)^{3/2}\right)\right)\right)$ , then at $x=1$ which of the following option is correct? $2y'+\sqrt{3} \pi^2 y = 0$ $2y' +3 \pi^2 y = 0$ $\sqrt2 y' - 3\pi^2 y = 0 $ $y' + 3\pi^2 y = 0$ This question is from JEE Main 2023 examination. My attempt: Firstly I used the identity $\sin^3(x) = \frac34 \sin(x) - \frac14 \sin(3x)$ and then by applying a brute force chain rule, I got that $f'(1) = \frac3{16} \pi^2$ and $f(1) = -\frac18$ . So, second option i.e. $2y' + 3\pi^2 y = 0$ is correct. But I don't think that this question is designed to be solved in this manner. Is there something which I'm missing?","Today  I came across a problem: If , then at which of the following option is correct? This question is from JEE Main 2023 examination. My attempt: Firstly I used the identity and then by applying a brute force chain rule, I got that and . So, second option i.e. is correct. But I don't think that this question is designed to be solved in this manner. Is there something which I'm missing?",y = \sin^3\left(\frac\pi 3\left(\cos\left(\frac\pi{3\sqrt2}\left(-4x^3 + 5x^2 + 1\right)^{3/2}\right)\right)\right) x=1 2y'+\sqrt{3} \pi^2 y = 0 2y' +3 \pi^2 y = 0 \sqrt2 y' - 3\pi^2 y = 0  y' + 3\pi^2 y = 0 \sin^3(x) = \frac34 \sin(x) - \frac14 \sin(3x) f'(1) = \frac3{16} \pi^2 f(1) = -\frac18 2y' + 3\pi^2 y = 0,"['calculus', 'derivatives']"
24,"If $f(0)=0$ and $f''(x)$ exists for all $x>0$, then show that","If  and  exists for all , then show that",f(0)=0 f''(x) x>0,"If $f(0)=0$ and $f''(x)$ exists for all $x>0$ , then show that $$ f'(x)-\frac{f(x)}{x}  = \frac{1}{2} xf''(\zeta),  \quad 0<\zeta <x $$ Also deduce that if $f''(x)$ is positive for positive values of $x$ , then $f(x)/x$ strictly increases as $x$ increases. For the first part i defined $ \phi(x) = xf'(x) -f(x)  $ . Applying Lagrange mean value theorem to it i got $ f'(x)-f(x)/x=xf''(\zeta)$ . The factor of 1/2 is missing. Can someone point out the mistake. Also kindly help with second part of question that talks about strictly increasing $f(x)/x$","If and exists for all , then show that Also deduce that if is positive for positive values of , then strictly increases as increases. For the first part i defined . Applying Lagrange mean value theorem to it i got . The factor of 1/2 is missing. Can someone point out the mistake. Also kindly help with second part of question that talks about strictly increasing","f(0)=0 f''(x) x>0 
f'(x)-\frac{f(x)}{x}
 = \frac{1}{2} xf''(\zeta),  \quad 0<\zeta <x
 f''(x) x f(x)/x x  \phi(x) = xf'(x) -f(x)    f'(x)-f(x)/x=xf''(\zeta) f(x)/x","['real-analysis', 'calculus', 'derivatives', 'mean-value-theorem']"
25,How does $\frac{{\rm d}u}{{\rm d} x}\Big{|}_{x+dx} = \frac{{\rm d} u}{{\rm d} x}\Big{|}_{x} + \frac{{\rm d}^2 u}{{\rm d}x^2}{{\rm d} x}$?,How does ?,\frac{{\rm d}u}{{\rm d} x}\Big{|}_{x+dx} = \frac{{\rm d} u}{{\rm d} x}\Big{|}_{x} + \frac{{\rm d}^2 u}{{\rm d}x^2}{{\rm d} x},"I am currently studying some equations related to a flexible beam bending problem, but I don't fully comprehend how this relation is being derived below: $$\frac{{\rm d}u}{{\rm d} x}\Big{|}_{x+dx} = \frac{{\rm d} u}{{\rm d} x}\Big{|}_{x} + \frac{{\rm d}^2 u}{{\rm d}x^2}{{\rm d} x}$$ I don't think its derived as a result of something specific to the beam bending problem, but I've attached an image below (from some lecture notes) of what the diagram shows, just in case. Below, $Q(x)$ is just the tension of the beam during an applied distributed load. . Can anyone explain to me how the differential on the left hand side is transformed into the right hand side? Maybe I'm missing something fundamental, I haven't looked at calculus in a while.","I am currently studying some equations related to a flexible beam bending problem, but I don't fully comprehend how this relation is being derived below: I don't think its derived as a result of something specific to the beam bending problem, but I've attached an image below (from some lecture notes) of what the diagram shows, just in case. Below, is just the tension of the beam during an applied distributed load. . Can anyone explain to me how the differential on the left hand side is transformed into the right hand side? Maybe I'm missing something fundamental, I haven't looked at calculus in a while.",\frac{{\rm d}u}{{\rm d} x}\Big{|}_{x+dx} = \frac{{\rm d} u}{{\rm d} x}\Big{|}_{x} + \frac{{\rm d}^2 u}{{\rm d}x^2}{{\rm d} x} Q(x),"['calculus', 'derivatives']"
26,Decomposing higher order derivatives of composite functions,Decomposing higher order derivatives of composite functions,,"I'm new to calculus, and just a little hazy on the skeleton of higher order derivatives when the chain rule is involved. I'm given a table of function and first derivative values, and I need to solve for the second derivative at a specific point. On an abstract level, that's fairly straightforward: (1) $$k(x) = f(g(x))$$ (2) $$\frac{d}{dx}k(x) = \frac{d}{dx}f(g(x))\frac{d}{dx}g(x)$$ (3) $$\frac{d^2}{dx^2}k(x)=\frac{d^2}{dx^2}f(g(x))\left[\frac{d}{dx}g(x)\right]^2+\frac{d}{dx}f(g(x))\frac{d^2}{dx^2}g(x)$$ But I don't know anything about the second derivative, so it's difficult to solve for a specific value of x (I could graph the function, but I'm looking for an exact solution). I'm curious, if I take $\frac{d^2}{dx^2}f(g(x))$ for example, is this the equivalent of $\frac{d}{dx}\left[\frac{d}{dx}f(g(x))\right]$ ? Of course, had I started with this expression, I would apply the chain rule, so I would find myself back at (3). In terms of one of the second-order derivatives in (3), however, since the chain rule was already applied, is it fair to assume $$\frac{d^2}{dx^2}f(g(A))=\frac{d^2}{dx^2}f(B)=\frac{d}{dx}\left[\frac{d}{dx}f(B)\right]=\frac{d}{dx}[C]$$ Edit: The point I'm trying to make here is breaking up the expression into first-order derivatives without applying the chain rule, since g(x) is essentially a constant in this context (to my understanding).","I'm new to calculus, and just a little hazy on the skeleton of higher order derivatives when the chain rule is involved. I'm given a table of function and first derivative values, and I need to solve for the second derivative at a specific point. On an abstract level, that's fairly straightforward: (1) (2) (3) But I don't know anything about the second derivative, so it's difficult to solve for a specific value of x (I could graph the function, but I'm looking for an exact solution). I'm curious, if I take for example, is this the equivalent of ? Of course, had I started with this expression, I would apply the chain rule, so I would find myself back at (3). In terms of one of the second-order derivatives in (3), however, since the chain rule was already applied, is it fair to assume Edit: The point I'm trying to make here is breaking up the expression into first-order derivatives without applying the chain rule, since g(x) is essentially a constant in this context (to my understanding).",k(x) = f(g(x)) \frac{d}{dx}k(x) = \frac{d}{dx}f(g(x))\frac{d}{dx}g(x) \frac{d^2}{dx^2}k(x)=\frac{d^2}{dx^2}f(g(x))\left[\frac{d}{dx}g(x)\right]^2+\frac{d}{dx}f(g(x))\frac{d^2}{dx^2}g(x) \frac{d^2}{dx^2}f(g(x)) \frac{d}{dx}\left[\frac{d}{dx}f(g(x))\right] \frac{d^2}{dx^2}f(g(A))=\frac{d^2}{dx^2}f(B)=\frac{d}{dx}\left[\frac{d}{dx}f(B)\right]=\frac{d}{dx}[C],"['calculus', 'derivatives', 'chain-rule', 'function-and-relation-composition']"
27,$\sum_{n=1}^\infty \frac{(-1)^{n+1}}{(2n-2)!}(2n+1)x^{2n}$ to function,to function,\sum_{n=1}^\infty \frac{(-1)^{n+1}}{(2n-2)!}(2n+1)x^{2n},"I want to calculate $3-\frac{5}{2}+\frac{7}{24}-\frac{9}{720}+...$ , while using $\sum_{n=1}^\infty \frac{(-1)^{n+1}}{(2n-2)!}(2n+1)x^{2n}$ . Here is my attempt: Firs I proved that the radius of converges for this series is $R= \infty$ . $$\sum_{n=1}^\infty \frac{(-1)^{n+1}}{(2n-2)!}(2n+1)x^{2n} =~_{k=n-1} ~ = \sum_{k=0}^\infty \frac{(-1)^{k+2}}{(2k)!}(2k+3)x^{2k+2} = \sum_{k=0}^\infty \frac{(-1)^{k}}{(2k)!}(x^{2k+3})'$$ $$= (\sum_{k=0}^\infty \frac{(-1)^{k}}{(2k)!}x^{2k+3})'= (x^3 \sum_{k=0}^\infty \frac{(-1)^{k}}{(2k)!}x^{2k})'=(x^3 \cos(x))'=3x^2 \cos(x)-x^3 \sin(x)$$ So $f(x)=\sum_{n=1}^\infty \frac{(-1)^{n+1}}{(2n-2)!}(2n+1)x^{2n}=3x^2 \cos(x)-x^3 \sin(x)$ . If I did everything correct, than I should find a value for $x$ which will help me to calculate the initial sum. Did I do it ok? Thanks a lot!","I want to calculate , while using . Here is my attempt: Firs I proved that the radius of converges for this series is . So . If I did everything correct, than I should find a value for which will help me to calculate the initial sum. Did I do it ok? Thanks a lot!",3-\frac{5}{2}+\frac{7}{24}-\frac{9}{720}+... \sum_{n=1}^\infty \frac{(-1)^{n+1}}{(2n-2)!}(2n+1)x^{2n} R= \infty \sum_{n=1}^\infty \frac{(-1)^{n+1}}{(2n-2)!}(2n+1)x^{2n} =~_{k=n-1} ~ = \sum_{k=0}^\infty \frac{(-1)^{k+2}}{(2k)!}(2k+3)x^{2k+2} = \sum_{k=0}^\infty \frac{(-1)^{k}}{(2k)!}(x^{2k+3})' = (\sum_{k=0}^\infty \frac{(-1)^{k}}{(2k)!}x^{2k+3})'= (x^3 \sum_{k=0}^\infty \frac{(-1)^{k}}{(2k)!}x^{2k})'=(x^3 \cos(x))'=3x^2 \cos(x)-x^3 \sin(x) f(x)=\sum_{n=1}^\infty \frac{(-1)^{n+1}}{(2n-2)!}(2n+1)x^{2n}=3x^2 \cos(x)-x^3 \sin(x) x,"['calculus', 'sequences-and-series', 'derivatives', 'solution-verification', 'power-series']"
28,"Prove that if $f^{(k)}(f^{-1}(x))$ exists, and is nonzero, then $(f^{-1})^{(k)}(x)$ exists.","Prove that if  exists, and is nonzero, then  exists.",f^{(k)}(f^{-1}(x)) (f^{-1})^{(k)}(x),"I think I found an error and just want to confirm. This is from Calculus by Michael Spivak, 3rd edition, Chapter 12 (Inverse Functions), problem 21. 12-21. Prove that if $f^{(k)}(f^{-1}(x))$ exists, and is nonzero, then $(f^{-1})^{(k)}(x)$ exists. This appears to be false. For example, take $f(x) = x^3$ . In this case $$f^{-1}(x) = x^{\frac{1}{3}}$$ $$f(0) = f^{-1}(0) = 0$$ $$f^{(3)}(0) = 6$$ If the statement is true, $(f^{-1})^{(3)}(0)$ would exist. However, $(f^{-1})'(0)$ doesn't exist (because $f'(0) = 0$ ) so higher order derivatives cannot. I think the question should instead say ""Prove that if $f^{(k)}(f^{-1}(x))$ exists, and $f'(f^{-1}(x))$ is nonzero, then $(f^{-1})^{(k)}(x)$ exists. Does that sound about right to y'all? Edit: As discussed with Paul Frost, the revised version of the statement Prove that if $f^{(k)}(f^{-1}(x))$ exists, and $f'(f^{-1}(x))$ is nonzero, then $(f^{-1})^{(k)}(x)$ exists is missing one other thing. We need $f$ to be one-one on some interval containing $f^{-1}(x)$ . For cases $k\geq 2$ , $f$ one-one is implied by $f'(f^{-1}(x))≠0$ , and the existence of $𝑓″(f^{-1}(x))$ (which gives us the existence of $f'$ for all points in some interval containing $f^{-1}(x)$ and continuity of $f'$ at $f^{-1}(x)$ ). We can prove the (corrected) statement for $k≥2$ . However, we don't know if $f$ is one-one for the case $k=1$ . As problem 11-63 shows, $f'(f^{-1}(x))\neq 0$ by itself isn't enough to guarantee $f$ is increasing (or decreasing) over some interval containing $f^{-1}(x)$ But that's ok. The higher derivative cases are the ones we're interested in anyway, so we can take $k = 2$ as the base case and use induction from there. The corrected corrected statement reads: ""Prove that if $f^{(k)}(f^{-1}(x))$ exists, and $f'(f^{-1}(x))$ is nonzero, then $(f^{-1})^{(k)}(x)$ exists for all $k\geq 2$ , $k\in \mathbb{N}$ For the $k=1$ case, we have theorem 12-5.","I think I found an error and just want to confirm. This is from Calculus by Michael Spivak, 3rd edition, Chapter 12 (Inverse Functions), problem 21. 12-21. Prove that if exists, and is nonzero, then exists. This appears to be false. For example, take . In this case If the statement is true, would exist. However, doesn't exist (because ) so higher order derivatives cannot. I think the question should instead say ""Prove that if exists, and is nonzero, then exists. Does that sound about right to y'all? Edit: As discussed with Paul Frost, the revised version of the statement Prove that if exists, and is nonzero, then exists is missing one other thing. We need to be one-one on some interval containing . For cases , one-one is implied by , and the existence of (which gives us the existence of for all points in some interval containing and continuity of at ). We can prove the (corrected) statement for . However, we don't know if is one-one for the case . As problem 11-63 shows, by itself isn't enough to guarantee is increasing (or decreasing) over some interval containing But that's ok. The higher derivative cases are the ones we're interested in anyway, so we can take as the base case and use induction from there. The corrected corrected statement reads: ""Prove that if exists, and is nonzero, then exists for all , For the case, we have theorem 12-5.",f^{(k)}(f^{-1}(x)) (f^{-1})^{(k)}(x) f(x) = x^3 f^{-1}(x) = x^{\frac{1}{3}} f(0) = f^{-1}(0) = 0 f^{(3)}(0) = 6 (f^{-1})^{(3)}(0) (f^{-1})'(0) f'(0) = 0 f^{(k)}(f^{-1}(x)) f'(f^{-1}(x)) (f^{-1})^{(k)}(x) f^{(k)}(f^{-1}(x)) f'(f^{-1}(x)) (f^{-1})^{(k)}(x) f f^{-1}(x) k\geq 2 f f'(f^{-1}(x))≠0 𝑓″(f^{-1}(x)) f' f^{-1}(x) f' f^{-1}(x) k≥2 f k=1 f'(f^{-1}(x))\neq 0 f f^{-1}(x) k = 2 f^{(k)}(f^{-1}(x)) f'(f^{-1}(x)) (f^{-1})^{(k)}(x) k\geq 2 k\in \mathbb{N} k=1,"['calculus', 'derivatives', 'solution-verification', 'inverse-function']"
29,Alternative Definition for the Derivative,Alternative Definition for the Derivative,,"In one class, we said that a function $f : \mathbb{R} \to \mathbb{R}$ is differentiable at $x_0$ if there exists a map $g : U\subset \mathbb{R} \to L(\mathbb{R}, \mathbb{R})$ continuous at $x_0$ such that $$ f(x) = f(x_0) + g(x)(x - x_0) $$ where $U$ is a neighborhood of $x_0$ . This is obviously just a restatement of the usual definition of a derivative with $f'(x_0) = g(x_0)$ , but it has distinct advantages. For example, the proof of the chain rule becomes much clearer. Can we play a similar game in multiple variables? That is, given a function $f : \mathbb{R}^n \to \mathbb{R}^m$ , can we say that it is differentiable at $x_0$ if there exists a map $g : U \subset \mathbb{R}^n \to L(\mathbb{R}^n , \mathbb{R}^m)$ continuous at $x_0$ such that $$ f(x) = f(x_0) + g(x)(x - x_0) $$ I tried to prove the equivalence myself, but it's not as obvious as one the one-variable case (if it's true at all), since in the one variable case we could use the fact that $\mathbb{R}$ is a field and $L(\mathbb{R} , \mathbb{R}) \cong \mathbb{R}$ .","In one class, we said that a function is differentiable at if there exists a map continuous at such that where is a neighborhood of . This is obviously just a restatement of the usual definition of a derivative with , but it has distinct advantages. For example, the proof of the chain rule becomes much clearer. Can we play a similar game in multiple variables? That is, given a function , can we say that it is differentiable at if there exists a map continuous at such that I tried to prove the equivalence myself, but it's not as obvious as one the one-variable case (if it's true at all), since in the one variable case we could use the fact that is a field and .","f : \mathbb{R} \to \mathbb{R} x_0 g : U\subset \mathbb{R} \to L(\mathbb{R}, \mathbb{R}) x_0 
f(x) = f(x_0) + g(x)(x - x_0)
 U x_0 f'(x_0) = g(x_0) f : \mathbb{R}^n \to \mathbb{R}^m x_0 g : U \subset \mathbb{R}^n \to L(\mathbb{R}^n , \mathbb{R}^m) x_0 
f(x) = f(x_0) + g(x)(x - x_0)
 \mathbb{R} L(\mathbb{R} , \mathbb{R}) \cong \mathbb{R}","['derivatives', 'definition']"
30,Proof explanation of an inverse function theorem,Proof explanation of an inverse function theorem,,"Let $I\neq\emptyset$ be an open interval and $f:I\to\mathbb{R}$ a continuous and injective function. If $f$ is differentiable at $x_0\in I$ and $f'(x_0)\neq 0$ , then $f^{-1}$ is differentiable at $f'(x_0):=y_0$ and we have that $$ (f^{-1})'(y_0)=\frac{1}{f'(x_0)}=\frac{1}{f'(f^{-1}(y_0))}.$$ Proof: Note that $f(I)$ is an interval, $f:I\to\mathbb{R}$ is an homomorphism, and $f^{-1}:J\to I$ is continuous. Now, $f$ and $f^{-1}$ are strictly monotonic (by a a lemma we previously proved). Let $(y_n)\in J\setminus\{y_0\}$ be any sequence such that $y_n\to y_0$ , and let $x_n=f^{-1}(y_n)$ . The continuity and injectivity of $f^{-1}$ implies that $(x_n)$ is a sequence in $I\setminus\{x_0\}$ such that $x_n\to x_0$ . Since $f$ is differentiable at $x_0$ and $f^{-1}(x_0)\neq 0$ , then we have $$\displaystyle\lim_{n\to\infty}\frac{f^{-1}(x_n)-f^{-1}(x_0)}{y_n-y_0}=\displaystyle\lim_{n\to\infty}\frac{x_n-x_0}{f(x_n)-f(x_0)}=\frac{1}{f'(x_0)}.$$ What I don't understand is how the continuity and injectivity of $f^{-1}$ implies that $(x_n)$ is a sequence in $I\setminus\{x_0\}$ such that $x_n\to x_0$ , and how to go from $\displaystyle\lim_{n\to\infty}\frac{x_n-x_0}{f(x_n)-f(x_0)}$ to $\frac{1}{f'(x_0)}$ . Any help is welcome","Let be an open interval and a continuous and injective function. If is differentiable at and , then is differentiable at and we have that Proof: Note that is an interval, is an homomorphism, and is continuous. Now, and are strictly monotonic (by a a lemma we previously proved). Let be any sequence such that , and let . The continuity and injectivity of implies that is a sequence in such that . Since is differentiable at and , then we have What I don't understand is how the continuity and injectivity of implies that is a sequence in such that , and how to go from to . Any help is welcome",I\neq\emptyset f:I\to\mathbb{R} f x_0\in I f'(x_0)\neq 0 f^{-1} f'(x_0):=y_0  (f^{-1})'(y_0)=\frac{1}{f'(x_0)}=\frac{1}{f'(f^{-1}(y_0))}. f(I) f:I\to\mathbb{R} f^{-1}:J\to I f f^{-1} (y_n)\in J\setminus\{y_0\} y_n\to y_0 x_n=f^{-1}(y_n) f^{-1} (x_n) I\setminus\{x_0\} x_n\to x_0 f x_0 f^{-1}(x_0)\neq 0 \displaystyle\lim_{n\to\infty}\frac{f^{-1}(x_n)-f^{-1}(x_0)}{y_n-y_0}=\displaystyle\lim_{n\to\infty}\frac{x_n-x_0}{f(x_n)-f(x_0)}=\frac{1}{f'(x_0)}. f^{-1} (x_n) I\setminus\{x_0\} x_n\to x_0 \displaystyle\lim_{n\to\infty}\frac{x_n-x_0}{f(x_n)-f(x_0)} \frac{1}{f'(x_0)},"['real-analysis', 'calculus', 'derivatives', 'proof-explanation', 'inverse-function']"
31,Question about the application of the Mean value Theorem [duplicate],Question about the application of the Mean value Theorem [duplicate],,"This question already has an answer here : Proving a different version of MVT (1 answer) Closed 3 years ago . Question: Let $f$ be a real-valued differentiable function in $(a, b)$ . Suppose that there exist $x, y ∈ (a, b)$ such that $x < y$ , $f'(x) = 0 = f'(y)$ and $(f(y)−f(x))/ (y−x) > 0$ . Prove that there exist a $ζ ∈ (x, y)$ such that $(f(ζ) − f(x))/(ζ − x) = f'(ζ)$ My Approach: I thought this is an easy question because the condition says that $f$ is differentiable, which means it is continuous. So it satisfies the condition for applying the mean value Theorem. Since $(x,y) ∈ (a,b)$ , the differentiability and continuity still holds for domain $(x,y)$ which means we can still apply the mean value theorem, and thus proving the statement true. However, according to this approach, I am not fully using the conditions giving in the question: $x < y$ , $f'(x) = 0 = f'(y)$ and $(f(y)−f(x))/ (y−x )> 0$ Am I missing something about proving this statement?","This question already has an answer here : Proving a different version of MVT (1 answer) Closed 3 years ago . Question: Let be a real-valued differentiable function in . Suppose that there exist such that , and . Prove that there exist a such that My Approach: I thought this is an easy question because the condition says that is differentiable, which means it is continuous. So it satisfies the condition for applying the mean value Theorem. Since , the differentiability and continuity still holds for domain which means we can still apply the mean value theorem, and thus proving the statement true. However, according to this approach, I am not fully using the conditions giving in the question: , and Am I missing something about proving this statement?","f (a, b) x, y ∈ (a, b) x < y f'(x) = 0 = f'(y) (f(y)−f(x))/
(y−x) > 0 ζ ∈ (x, y) (f(ζ) − f(x))/(ζ − x)
= f'(ζ) f (x,y) ∈ (a,b) (x,y) x < y f'(x) = 0 = f'(y) (f(y)−f(x))/
(y−x )> 0","['real-analysis', 'derivatives']"
32,Is there an elegant way to take a derivative of this linear algebra problem?,Is there an elegant way to take a derivative of this linear algebra problem?,,"I'm a biologist self-learning linear algebra for some applications in my work.  Taking derivatives in non-linear-algebra contexts is quite clear to me, as I can just chain-rule my way through problems... but doing so in a linear-algebra context is a bit of a mystery to me. I'm trying to take the derivative of the following equation $$y = \vec{d}^TP^TP\vec{\delta},$$ where $$\vec{d} = \left[ \begin{array}\\ d_1 \\ d_2 \\ d_3 \end{array} \right]$$ and $$\vec{\delta} = \left[ \begin{array}\\ o_1^2 -d_1^2 \\ o_2^2 - d_2^2 \\ o_3^2 - d_3^2 \end{array} \right].$$ $P$ is just a 3x3 matrix of constants. I'd like to find an elegant solution to finding the derivative $y^\prime \left( \vec{d} \right)$ , resulting in the Jacobian. The challenging part is that the vector $\vec{\delta}$ is a composite vector holding $\vec{\delta} = \vec{o} - \vec{d}^{\circ2}$ (not sure on my notation here.. but $\vec{d}^{\circ2}$ is supposed to indicate all the elements of $d$ are squared). So far, the only way I'm aware of doing this is by expanding everything out into one really large linear formula for the resulting single value, then taking the derivative of that. My end goal here is to implement this in code... so unnecessarily large formulas are something I'm trying to avoid to keep my code readable. Is there a more elegant solution to this?","I'm a biologist self-learning linear algebra for some applications in my work.  Taking derivatives in non-linear-algebra contexts is quite clear to me, as I can just chain-rule my way through problems... but doing so in a linear-algebra context is a bit of a mystery to me. I'm trying to take the derivative of the following equation where and is just a 3x3 matrix of constants. I'd like to find an elegant solution to finding the derivative , resulting in the Jacobian. The challenging part is that the vector is a composite vector holding (not sure on my notation here.. but is supposed to indicate all the elements of are squared). So far, the only way I'm aware of doing this is by expanding everything out into one really large linear formula for the resulting single value, then taking the derivative of that. My end goal here is to implement this in code... so unnecessarily large formulas are something I'm trying to avoid to keep my code readable. Is there a more elegant solution to this?","y = \vec{d}^TP^TP\vec{\delta}, \vec{d} = \left[ \begin{array}\\ d_1 \\ d_2 \\ d_3 \end{array} \right] \vec{\delta} = \left[ \begin{array}\\ o_1^2 -d_1^2 \\ o_2^2 - d_2^2 \\ o_3^2 - d_3^2 \end{array} \right]. P y^\prime \left( \vec{d} \right) \vec{\delta} \vec{\delta} = \vec{o} - \vec{d}^{\circ2} \vec{d}^{\circ2} d",['linear-algebra']
33,I don't understand this derivation,I don't understand this derivation,,"I try to unterstand a derivation and need help. There are given two functions $$ s=-cos(j\pi/n),s\in[-1,1] $$ and the nonlinear transformation $$ y(s)=C\tan[\frac{\pi(s+1)}{4}+\frac{s-1}{2}\arctan\frac{y^*}{C}]+y^*,y\in[0,\infty) $$ $y*$ and $C$ are constant parameters. The derivation and its solution is $$ \frac{ds}{dy(s)}=4C/[\pi+2\arctan(y^*/C)]/[C^2+(y(s)-y^*)^2] $$ but i don't understand how to reach this solution.",I try to unterstand a derivation and need help. There are given two functions and the nonlinear transformation and are constant parameters. The derivation and its solution is but i don't understand how to reach this solution.,"
s=-cos(j\pi/n),s\in[-1,1]
 
y(s)=C\tan[\frac{\pi(s+1)}{4}+\frac{s-1}{2}\arctan\frac{y^*}{C}]+y^*,y\in[0,\infty)
 y* C 
\frac{ds}{dy(s)}=4C/[\pi+2\arctan(y^*/C)]/[C^2+(y(s)-y^*)^2]
",['derivatives']
34,Critical points of $f(x)=\int_{x}^{x^2} e^{-{t^2}}dt$?,Critical points of ?,f(x)=\int_{x}^{x^2} e^{-{t^2}}dt,I understand that you need to take derivative of $f(x)$ and $f'(x)$ needs to be either equal to zero or undefined to find $f(x)$ 's critical points. However what happens if $f(x)$ is given as an integral as in the title? $$\frac{d}{dx} \int_{x}^{x^2} e^{-{t^2}}dt = F'(x^2)2x - F'(x)$$ $$F'(x^2)2x - F'(x)= 2xe^{-{x^4}} - e^{-{x^2}}$$ Is the calculation above correct for the question on the title? What should I do next? Thank you.,I understand that you need to take derivative of and needs to be either equal to zero or undefined to find 's critical points. However what happens if is given as an integral as in the title? Is the calculation above correct for the question on the title? What should I do next? Thank you.,f(x) f'(x) f(x) f(x) \frac{d}{dx} \int_{x}^{x^2} e^{-{t^2}}dt = F'(x^2)2x - F'(x) F'(x^2)2x - F'(x)= 2xe^{-{x^4}} - e^{-{x^2}},"['integration', 'derivatives']"
35,Find the relative maximum and relative minimum of the function $f(x) = 49x + \frac{4}{x}$,Find the relative maximum and relative minimum of the function,f(x) = 49x + \frac{4}{x},"Find the relative maximum and relative minimum of the function $f(x) = 49x + \frac{4}{x}=49x+4x^{-1}$ Solution: Step 1: Find the values of $x$ where $f'(x)=0$ and $f'(x)$ DNE. $f'(x)=49-4x^{-2}=49-\frac{4}{x^2}$ We can see that $f'(x)$ DNE when $x=0$ . Now lets find the values of $x$ that make $f'(x)=0$ . $f'(x)=49-4x^{-2}=49-\frac{4}{x^2}=0$ $\rightarrow x^2=\frac{4}{49}$ $\rightarrow x= \pm \sqrt{\frac{4}{49}}$ $\rightarrow x= \pm \frac{2}{7}$ Thus our critical points are $x=0, x= \frac{2}{7}, x= -\frac{2}{7}$ Step 2: Make a number line and plot the sign of $f'(x)$ to find where $f(x)$ is increasing and decreasing. $f'(-3)= 49-\frac{4}{9}>0$ $f'(\frac{-1}{7})=49-\frac{4}{(\frac{-1}{7})^2}=49-4(49)=-3(49)<0$ $f'(\frac{1}{7})=49-\frac{4}{(\frac{1}{7})^2}=49-4(49)=-3(49)<0$ $f'(3)=49-\frac{4}{3^2} >0$ Thus $f(x)$ is increasing as $x$ increases towards $\frac{-2}{7}$ and then $f(x)$ is decreasing as $x$ increases towards $0$ . Therefore $x=\frac{-2}{7}$ corresponds to a relative maximum. Thus $f(x)$ is decreasing $x$ increases towards $\frac{2}{7}$ and then $f(x)$ is increases as $x$ increases towards $\infty$ . Therefore $x=\frac{2}{7}$ corresponds to a relative minimum. Step 3: find the corresponding $y$ values. $f(\frac{2}{7})=28$ $f(\frac{-2}{7})=-28$ Therefore $(\frac{2}{7},28)$ is a relative minimum and $(\frac{-2}{7},-28)$ is a relative maximum. This may be weird that our relative maximum is smaller than our relative minimum, but since these are ""relative"" maximum and minimum, they are only being compared to points on the graph very near to it, so this is okay.","Find the relative maximum and relative minimum of the function Solution: Step 1: Find the values of where and DNE. We can see that DNE when . Now lets find the values of that make . Thus our critical points are Step 2: Make a number line and plot the sign of to find where is increasing and decreasing. Thus is increasing as increases towards and then is decreasing as increases towards . Therefore corresponds to a relative maximum. Thus is decreasing increases towards and then is increases as increases towards . Therefore corresponds to a relative minimum. Step 3: find the corresponding values. Therefore is a relative minimum and is a relative maximum. This may be weird that our relative maximum is smaller than our relative minimum, but since these are ""relative"" maximum and minimum, they are only being compared to points on the graph very near to it, so this is okay.","f(x) = 49x + \frac{4}{x}=49x+4x^{-1} x f'(x)=0 f'(x) f'(x)=49-4x^{-2}=49-\frac{4}{x^2} f'(x) x=0 x f'(x)=0 f'(x)=49-4x^{-2}=49-\frac{4}{x^2}=0 \rightarrow x^2=\frac{4}{49} \rightarrow x= \pm \sqrt{\frac{4}{49}} \rightarrow x= \pm \frac{2}{7} x=0, x= \frac{2}{7}, x= -\frac{2}{7} f'(x) f(x) f'(-3)= 49-\frac{4}{9}>0 f'(\frac{-1}{7})=49-\frac{4}{(\frac{-1}{7})^2}=49-4(49)=-3(49)<0 f'(\frac{1}{7})=49-\frac{4}{(\frac{1}{7})^2}=49-4(49)=-3(49)<0 f'(3)=49-\frac{4}{3^2} >0 f(x) x \frac{-2}{7} f(x) x 0 x=\frac{-2}{7} f(x) x \frac{2}{7} f(x) x \infty x=\frac{2}{7} y f(\frac{2}{7})=28 f(\frac{-2}{7})=-28 (\frac{2}{7},28) (\frac{-2}{7},-28)","['calculus', 'derivatives']"
36,Gradient of the Euclidean distance function,Gradient of the Euclidean distance function,,"If I calculate the Euclidean distance $d$ of two sets of $x,y$ points $A_{1} \in \mathbb{R}^{n\times2}$ and $B_{2} \in \mathbb{R}^{n\times2}$ as $d = \sqrt{(A_{1}-B_{1})^2 + (A_{2}-B_{2})^2}$ How would I calculate the gradient of $d$ ? Based on THIS answer I interpreted it as the gradient would be: $\nabla d =\frac{A-B}{\sqrt{(A_{1}-B_{1})^2 + (A_{2}-B_{2})^2}}$ However, this doesn't seem to give me the results I was expecting.","If I calculate the Euclidean distance of two sets of points and as How would I calculate the gradient of ? Based on THIS answer I interpreted it as the gradient would be: However, this doesn't seem to give me the results I was expecting.","d x,y A_{1} \in \mathbb{R}^{n\times2} B_{2} \in \mathbb{R}^{n\times2} d = \sqrt{(A_{1}-B_{1})^2 + (A_{2}-B_{2})^2} d \nabla d =\frac{A-B}{\sqrt{(A_{1}-B_{1})^2 + (A_{2}-B_{2})^2}}","['calculus', 'derivatives', 'euclidean-geometry', 'vector-analysis']"
37,"a line tangent to curve $y = \frac{x}{2−2x}$ goes through (1, -1)","a line tangent to curve  goes through (1, -1)",y = \frac{x}{2−2x},$y = \frac{x}{2−2x}$ $y = \frac{f(x)}{g(x)} \rightarrow y' = \frac{f'(x) \cdot g(x) - g'(x)\cdot f(x)}{{g(x)}^2}$ $y = \frac{x}{2−2x}$ $y' = \frac{(2-2x)-(x \cdot (-2))}{{(2−2x)}^2}$ $\rightarrow$ $y' = \frac{(2-2x+2x)}{{(2−2x)}^2} = \frac{2}{{(2-2x)}^2}$ why cant i find the gradient with derivative?,why cant i find the gradient with derivative?,y = \frac{x}{2−2x} y = \frac{f(x)}{g(x)} \rightarrow y' = \frac{f'(x) \cdot g(x) - g'(x)\cdot f(x)}{{g(x)}^2} y = \frac{x}{2−2x} y' = \frac{(2-2x)-(x \cdot (-2))}{{(2−2x)}^2} \rightarrow y' = \frac{(2-2x+2x)}{{(2−2x)}^2} = \frac{2}{{(2-2x)}^2},"['derivatives', 'curves']"
38,Clarifying the difference between differential 1-form and covariant derivatives,Clarifying the difference between differential 1-form and covariant derivatives,,"Upon trying to teach myself some basics of differential geometry I keep running into the notion of a differential 1-form , and the covariant derivative . I understand intuitively what the covariant derivative ( $\nabla_pu : TM \rightarrow TM$ ) aims to achieve. It provides a way to transport vectors in one tangent space to another over the manifold. If it is flat it has Christoffel symbol = 0. Now dual to the tangent space is the co-tangent space in which the differntial 1-form $dx : TM \rightarrow \mathbb{R}$ (for example) lives. (If any of the above definitions are incorrect please let me know) In elementary school calculus we used ""basic differentiation"" to solve all issues in regards to differentiability. Moreover, to find the total derivative in university we used a formula which involves $\partial / \partial x$ notation in conjunction with $dx$ . So they seemed to be doing the same job. The phrase everyone would throw around casually is: ""Use $dx$ when it's a function of one variable and $\partial / \partial x$ when there are multiple"". But at the end of the day they both achieve to specify some sort of ""rate of change"". In differential geometry these are clearly different beasts! I can read a book and ""get"" each definition, but I am struggling to put everything together neatly like jigsaw. Now my questions: In elementary calculus when I ""differentiated"" what did I do? Did I implicitly use a 1-form or the covariant derivative over the Euclidean space in Cart co-ords. If they are the same, why are they same? Since in diff geo on the abstract level they clearly work on different objects. So is integration ONLY concerned in working with 1-forms? (The $dx$ symbol is everywhere) When we solve DEs and PDEs there are derivatives involved. Do these stem from the covariant derivative, or from the differential 1-form? On the abstract level when I say ""I am going to find the derivative"", how do people (versed in differential geometry) interpret that statement? Am I going to be working with 1-forms or with the covariant derivative? Are the ordinary derivatives we see in elementary school calculus (e.g. for performing gradient descent on a surface) covariant, or contra-variant? What implications does this idea have for the entire framework (i.e. I think I heard it is a contravariant object, but we have this thing called a ""covariant derivative"". So do we not use covariant derivative for gradient descent??) Perhaps I have more questions to think of, but I feel if at least these are clarified it should be enough to help myself (and others) to continue with this self-learning journey!  :)","Upon trying to teach myself some basics of differential geometry I keep running into the notion of a differential 1-form , and the covariant derivative . I understand intuitively what the covariant derivative ( ) aims to achieve. It provides a way to transport vectors in one tangent space to another over the manifold. If it is flat it has Christoffel symbol = 0. Now dual to the tangent space is the co-tangent space in which the differntial 1-form (for example) lives. (If any of the above definitions are incorrect please let me know) In elementary school calculus we used ""basic differentiation"" to solve all issues in regards to differentiability. Moreover, to find the total derivative in university we used a formula which involves notation in conjunction with . So they seemed to be doing the same job. The phrase everyone would throw around casually is: ""Use when it's a function of one variable and when there are multiple"". But at the end of the day they both achieve to specify some sort of ""rate of change"". In differential geometry these are clearly different beasts! I can read a book and ""get"" each definition, but I am struggling to put everything together neatly like jigsaw. Now my questions: In elementary calculus when I ""differentiated"" what did I do? Did I implicitly use a 1-form or the covariant derivative over the Euclidean space in Cart co-ords. If they are the same, why are they same? Since in diff geo on the abstract level they clearly work on different objects. So is integration ONLY concerned in working with 1-forms? (The symbol is everywhere) When we solve DEs and PDEs there are derivatives involved. Do these stem from the covariant derivative, or from the differential 1-form? On the abstract level when I say ""I am going to find the derivative"", how do people (versed in differential geometry) interpret that statement? Am I going to be working with 1-forms or with the covariant derivative? Are the ordinary derivatives we see in elementary school calculus (e.g. for performing gradient descent on a surface) covariant, or contra-variant? What implications does this idea have for the entire framework (i.e. I think I heard it is a contravariant object, but we have this thing called a ""covariant derivative"". So do we not use covariant derivative for gradient descent??) Perhaps I have more questions to think of, but I feel if at least these are clarified it should be enough to help myself (and others) to continue with this self-learning journey!  :)",\nabla_pu : TM \rightarrow TM dx : TM \rightarrow \mathbb{R} \partial / \partial x dx dx \partial / \partial x dx,"['derivatives', 'differential-geometry', 'manifolds', 'differential-topology', 'partial-derivative']"
39,Understand a proof of general derivative test with Taylor-Formula.,Understand a proof of general derivative test with Taylor-Formula.,,"I'm trying to understand a proof of the general derivative test which states: Let $f:[a,b]\rightarrow \mathbb{R}$ be a function that is differenciable $n$ times, and let $x_0 \in]a,b[$ be point with $f^{\prime}(x_0)=0, f^{\prime\prime}(x_0)=0, ..., f^{(n-1)}(x_0)=0$ but $f^{(n)}(x_0)\neq 0$ . It follows, that: If n is uneven, $f$ does not have a local extremum in $x_0$ . If n is even, $f$ has a local extremum in $x_0$ (maximum if $f^{(n)}(x_0)<0$ and minimum if $f^{(n)}(x_0)<0$ ). $\underline{\text{Proof}}$ (From the Taylor-Formula): $f(x)=f(x_0)+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+R_n(x,x_0)$ with $\lim_{x\to x_0 }\frac{R_n(x,x_0)}{(x-x_0)^n}=0$ . (Which we can assume to have been proven). It follows, that for sufficiently small differences between $x$ and $x_0$ : $|\frac{R_n(x,x_0)}{(x-x_0)^n}|<\frac{1}{2}|\frac{f^{(n)}(x_0)}{n!}|$ (This is the first argument I can't understand). That's why $\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+R_n(x,x_0)$ has the same (negative/positive) sign as $\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n$ (Now I get completely lost). With uneven n, $\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+R_n(x,x_0)$ changes its sing in $x_0$ , which is why we have no local extremum. With even n, we have $(x-x_0)^n \geq 0$ and therefore $f(x) \geq f(x_0)$ if $f^{(n)}(x_0)>0$ and therefore a minimum, or $f(x) \leq f(x_0)$ if $f^{(n)}(x_0)<0$ and therefore a maximum. Now, I'm really not sure if theres an error in the script I'm reading or if it's just me missing something, but since I can't find that proof anywhere else, I wanted to ask whether anyone has any hints or suggestions on where I could look or a direct explanation.","I'm trying to understand a proof of the general derivative test which states: Let be a function that is differenciable times, and let be point with but . It follows, that: If n is uneven, does not have a local extremum in . If n is even, has a local extremum in (maximum if and minimum if ). (From the Taylor-Formula): with . (Which we can assume to have been proven). It follows, that for sufficiently small differences between and : (This is the first argument I can't understand). That's why has the same (negative/positive) sign as (Now I get completely lost). With uneven n, changes its sing in , which is why we have no local extremum. With even n, we have and therefore if and therefore a minimum, or if and therefore a maximum. Now, I'm really not sure if theres an error in the script I'm reading or if it's just me missing something, but since I can't find that proof anywhere else, I wanted to ask whether anyone has any hints or suggestions on where I could look or a direct explanation.","f:[a,b]\rightarrow \mathbb{R} n x_0 \in]a,b[ f^{\prime}(x_0)=0, f^{\prime\prime}(x_0)=0, ..., f^{(n-1)}(x_0)=0 f^{(n)}(x_0)\neq 0 f x_0 f x_0 f^{(n)}(x_0)<0 f^{(n)}(x_0)<0 \underline{\text{Proof}} f(x)=f(x_0)+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+R_n(x,x_0) \lim_{x\to x_0 }\frac{R_n(x,x_0)}{(x-x_0)^n}=0 x x_0 |\frac{R_n(x,x_0)}{(x-x_0)^n}|<\frac{1}{2}|\frac{f^{(n)}(x_0)}{n!}| \frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+R_n(x,x_0) \frac{f^{(n)}(x_0)}{n!}(x-x_0)^n \frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+R_n(x,x_0) x_0 (x-x_0)^n \geq 0 f(x) \geq f(x_0) f^{(n)}(x_0)>0 f(x) \leq f(x_0) f^{(n)}(x_0)<0","['real-analysis', 'derivatives', 'taylor-expansion']"
40,Show that $e^{-1/x^2}$ is not analytic around $x=0$.,Show that  is not analytic around .,e^{-1/x^2} x=0,"I have been working on the following question. Define a function \begin{align*} f(x)= \begin{cases} e^{-1/x^2}&\text{ for }x>0,\\ 0&\text{ for }x=0 \end{cases} \end{align*} Prove that $f$ is $C^\infty$ but not analytic around $x=0$ . And just in case some of you use a different notation, $C^\infty$ means that $f$ is infinitely differentiable. Since I am unlikely to find a formula for the $n^{th}$ derivative, my strategy was to use the series expansion for $e^x$ but the arguments are not working out well.","I have been working on the following question. Define a function Prove that is but not analytic around . And just in case some of you use a different notation, means that is infinitely differentiable. Since I am unlikely to find a formula for the derivative, my strategy was to use the series expansion for but the arguments are not working out well.","\begin{align*}
f(x)=
\begin{cases}
e^{-1/x^2}&\text{ for }x>0,\\
0&\text{ for }x=0
\end{cases}
\end{align*} f C^\infty x=0 C^\infty f n^{th} e^x","['derivatives', 'power-series', 'exponential-function', 'analyticity']"
41,The equation $ f'(x)=f(x)$ admits a solution,The equation  admits a solution, f'(x)=f(x),"let $f :[0,1]→\mathbb R$ be a fixed continous function such that f is differentiable on (0,1) and $ f(0)=f(1)=0$ .then the equation $ f'(x)=f(x)$ admits No solution $x\in (0,1)$ More than one solution $x\in (0,1)$ Exactly one solution $x\in (0,1)$ At least one solution $x\in (0,1)$ Actually, mean value theorem doesn't work for $f'(x)$ . The function $f$ has a fixed point and at that point $f'(x)=1$ and at some point $x_0 , f'(x)=0$ (by mean value theorem).but it doesn't get me anywhere! What am I missing? Obviously 1. Is false since f(x)=0 (also 3.) Is also false since $f(x)=\sin\pi x$ So 4. Is true, but I stuck to prove. Please help.","let be a fixed continous function such that f is differentiable on (0,1) and .then the equation admits No solution More than one solution Exactly one solution At least one solution Actually, mean value theorem doesn't work for . The function has a fixed point and at that point and at some point (by mean value theorem).but it doesn't get me anywhere! What am I missing? Obviously 1. Is false since f(x)=0 (also 3.) Is also false since So 4. Is true, but I stuck to prove. Please help.","f :[0,1]→\mathbb R  f(0)=f(1)=0  f'(x)=f(x) x\in (0,1) x\in (0,1) x\in (0,1) x\in (0,1) f'(x) f f'(x)=1 x_0 , f'(x)=0 f(x)=\sin\pi x","['real-analysis', 'derivatives', 'continuity', 'rolles-theorem']"
42,"if $f$ convex, $f$ has bounded first derivative iff $f$ uniformly continuous","if  convex,  has bounded first derivative iff  uniformly continuous",f f f,"Formal statement: if $f: \mathbb{R} \rightarrow \mathbb{R}$ is convex and differentiable, it is uniformly continuous iff there exists some $a > 0$ such that $|f'(x)| \leq a$ for all $x$. One direction seems slightly easier to prove: if $|f'(x)| \leq a$ for all $x$ and we know $f$ is differentiable, we know there's $c$ such that $|f(y) - f(x)| = c \cdot |x - y| \leq a \cdot |x - y|$ we apply Mean Value Theorem (edit: previously cited convexity in ADDITION to the MVT for this part and it was deemed unnecessary). Then we can just let $\delta = \epsilon / a$ to show uniform continuity. The other direction has me stumped. Any suggestion?","Formal statement: if $f: \mathbb{R} \rightarrow \mathbb{R}$ is convex and differentiable, it is uniformly continuous iff there exists some $a > 0$ such that $|f'(x)| \leq a$ for all $x$. One direction seems slightly easier to prove: if $|f'(x)| \leq a$ for all $x$ and we know $f$ is differentiable, we know there's $c$ such that $|f(y) - f(x)| = c \cdot |x - y| \leq a \cdot |x - y|$ we apply Mean Value Theorem (edit: previously cited convexity in ADDITION to the MVT for this part and it was deemed unnecessary). Then we can just let $\delta = \epsilon / a$ to show uniform continuity. The other direction has me stumped. Any suggestion?",,"['derivatives', 'convex-analysis', 'uniform-continuity']"
43,Motivation on Using Fourier Series to Solve Heat Equation,Motivation on Using Fourier Series to Solve Heat Equation,,"There is an approach to solve the heat equation $$ \begin{align*} \frac{\partial u}{\partial t}(x,t) &= \frac{\partial^2 u}{\partial x^2}(x,t) \quad \text{for $(x,t) \in \mathbb{R} \times (0, \infty)$}  \\ u(x,0) &= f(x) \quad \text{for $x \in \mathbb{R}$} \end{align*} $$ by applying Fourier Series. What is the motivation for that? How did Fourier see that his Fourier Series will help to resolve this equation? What makes Fourier Series to special to be applicable to this problem?","There is an approach to solve the heat equation $$ \begin{align*} \frac{\partial u}{\partial t}(x,t) &= \frac{\partial^2 u}{\partial x^2}(x,t) \quad \text{for $(x,t) \in \mathbb{R} \times (0, \infty)$}  \\ u(x,0) &= f(x) \quad \text{for $x \in \mathbb{R}$} \end{align*} $$ by applying Fourier Series. What is the motivation for that? How did Fourier see that his Fourier Series will help to resolve this equation? What makes Fourier Series to special to be applicable to this problem?",,"['derivatives', 'partial-differential-equations', 'fourier-series', 'heat-equation']"
44,Does value of differentiation as infinity implies continuity?,Does value of differentiation as infinity implies continuity?,,I come across Mean value theorem proof which is attached below Which has an assumption that f has derivative (finite or infinite ) at each interior point and continuity is assumed at endpoint. In proof assumption used that function f is continuous over whole interval. But I am not convinced with fact that f has infinite derivative and continuous at that point . I tried to use definition I get $|f(x)-f(y)|=|x-y|f'(c)$ one side is infinite how to show for continuity. Also Is it possible to have function which every where derivation infinity? I think question is wrong As I could not imagine function every where like a verticle line. But Asking for in case exist? Any help will be appreciated Edit: Sorry Everyone As In book already specified definition which already assume continuity of function to define the derivative.,I come across Mean value theorem proof which is attached below Which has an assumption that f has derivative (finite or infinite ) at each interior point and continuity is assumed at endpoint. In proof assumption used that function f is continuous over whole interval. But I am not convinced with fact that f has infinite derivative and continuous at that point . I tried to use definition I get $|f(x)-f(y)|=|x-y|f'(c)$ one side is infinite how to show for continuity. Also Is it possible to have function which every where derivation infinity? I think question is wrong As I could not imagine function every where like a verticle line. But Asking for in case exist? Any help will be appreciated Edit: Sorry Everyone As In book already specified definition which already assume continuity of function to define the derivative.,,"['real-analysis', 'derivatives', 'continuity']"
45,calculating a higher order derivative,calculating a higher order derivative,,"My task is to find the values $f^{(2017)}(0)$ and $f^{(2018)}(0)$ for $f(x)=\frac{\arccos(x)}{\sqrt{1-x^2}}$ . Basically, it's about finding the $n^{th}$ derivative of $f$ . So I noticed if I let $g(x)=arccos(x)$ , then $f(x)=-g(x)\cdot g'(x)$ . I was able to prove by induction that for all $n\geq 2$ and $k\in \Bbb{N}$ the $n^{th}$ derivative of $g'$ is $$[(g')^{k}]^{(n)}(0)=k\cdot(n-1)\cdot[(g')^{k+2}]^{(n-2)}(0)$$ But even with applying the Leibniz rule to $f=g\cdot g'$ I don't understand how to get the final result. Did I make the wrong approach to the problem or is the general formula above useful? If so, how should I apply it?","My task is to find the values and for . Basically, it's about finding the derivative of . So I noticed if I let , then . I was able to prove by induction that for all and the derivative of is But even with applying the Leibniz rule to I don't understand how to get the final result. Did I make the wrong approach to the problem or is the general formula above useful? If so, how should I apply it?",f^{(2017)}(0) f^{(2018)}(0) f(x)=\frac{\arccos(x)}{\sqrt{1-x^2}} n^{th} f g(x)=arccos(x) f(x)=-g(x)\cdot g'(x) n\geq 2 k\in \Bbb{N} n^{th} g' [(g')^{k}]^{(n)}(0)=k\cdot(n-1)\cdot[(g')^{k+2}]^{(n-2)}(0) f=g\cdot g',"['calculus', 'derivatives']"
46,Line integral and differentiability of $\Vert f \Vert^{p}$ where $f(x) = A(x)$,Line integral and differentiability of  where,\Vert f \Vert^{p} f(x) = A(x),"Problem. Let $f: \mathbb{R}^{n} \to \mathbb{R}^{n}$ , $n \geq 2$ , the function defined by $f(x) = Ax$ where $A$ is a matrix $n \times n$ . For each of the statements below, show if is true or give a counterexample if is false. (a) If $g(x) = \Vert Ax \Vert^{p}$ , with $p>1$ , then $g$ is differentiable in $\mathbb{R}^{n}$ (here $\Vert\;\Vert$ denote the euclidean norm in $\mathbb{R}^{n}$ ). (b) If $\det A = 1$ , then $\int_{\gamma}f\mathrm{d} \gamma$ not depends of the path $\gamma$ that connects the points $x_{0}$ e $x_{1}$ of $\mathbb{R}^{n}$ . (b) In my book, $\displaystyle \int_{\gamma}f\mathrm{d}\gamma$ is defined over a curve $\gamma: [a,b] \to \mathbb{R}^{n}$ of class $C^{1}$ and I proved that If $\Omega$ is connected open in $\mathbb{R}^{n}$ and $f: \Omega \to \mathbb{R}$ is a function of class $C^{1}$ , then $f'$ is a conservative field, that is, $$\int_{\gamma_{1}}f\mathrm{d}\gamma = \int_{\gamma_{2}}f\mathrm{d}\gamma \tag{*}\label{*}$$ for any differentiable curves that connect $x$ to $y$ for every $x,y \in \Omega$ . So, since $f$ is linear, $f$ satisfies the hypothesis of (*). Moreover, $f' \equiv f$ , then $f$ is conservative. But, I don't know if in this question, the definition of $\int_{\gamma}f\mathrm{d}\gamma$ is equal of my book. Moreover, if I'm right, the hypothesis $\det A = 1$ seems unnecessary. It made me think I'm totally wrong and try another way. So I tried to apply a linear change in the variables, but I think I couldn't use it correctly (a) I know that If $f:\mathbb{R}^{n} \to \mathbb{R}$ is given by $f(x) = \Vert x \Vert_{p}^{p}$ where $\Vert\;\Vert_{p} = (|x_{1}|^{p} + ... + |x_{n}|^{p})^{\frac{1}{p}}$ , then is differentiable and If $g:\mathbb{R}^{n} \to \mathbb{R}$ is differentiable, then $F(x) = g(Ax)$ (where $A$ is a $n \times n$ matrix) is differentiable and $F'(x) = A^{T}g'(Ax)$ . These two results make me think that (a) is true, because (a) would be like a generalization of them. But I couldn't prove. Can someone help me?","Problem. Let , , the function defined by where is a matrix . For each of the statements below, show if is true or give a counterexample if is false. (a) If , with , then is differentiable in (here denote the euclidean norm in ). (b) If , then not depends of the path that connects the points e of . (b) In my book, is defined over a curve of class and I proved that If is connected open in and is a function of class , then is a conservative field, that is, for any differentiable curves that connect to for every . So, since is linear, satisfies the hypothesis of (*). Moreover, , then is conservative. But, I don't know if in this question, the definition of is equal of my book. Moreover, if I'm right, the hypothesis seems unnecessary. It made me think I'm totally wrong and try another way. So I tried to apply a linear change in the variables, but I think I couldn't use it correctly (a) I know that If is given by where , then is differentiable and If is differentiable, then (where is a matrix) is differentiable and . These two results make me think that (a) is true, because (a) would be like a generalization of them. But I couldn't prove. Can someone help me?","f: \mathbb{R}^{n} \to \mathbb{R}^{n} n \geq 2 f(x) = Ax A n \times n g(x) = \Vert Ax \Vert^{p} p>1 g \mathbb{R}^{n} \Vert\;\Vert \mathbb{R}^{n} \det A = 1 \int_{\gamma}f\mathrm{d} \gamma \gamma x_{0} x_{1} \mathbb{R}^{n} \displaystyle \int_{\gamma}f\mathrm{d}\gamma \gamma: [a,b] \to \mathbb{R}^{n} C^{1} \Omega \mathbb{R}^{n} f: \Omega \to \mathbb{R} C^{1} f' \int_{\gamma_{1}}f\mathrm{d}\gamma = \int_{\gamma_{2}}f\mathrm{d}\gamma \tag{*}\label{*} x y x,y \in \Omega f f f' \equiv f f \int_{\gamma}f\mathrm{d}\gamma \det A = 1 f:\mathbb{R}^{n} \to \mathbb{R} f(x) = \Vert x \Vert_{p}^{p} \Vert\;\Vert_{p} = (|x_{1}|^{p} + ... + |x_{n}|^{p})^{\frac{1}{p}} g:\mathbb{R}^{n} \to \mathbb{R} F(x) = g(Ax) A n \times n F'(x) = A^{T}g'(Ax)","['real-analysis', 'integration', 'derivatives']"
47,partial derivatives and chain rule of functions defined on manifold,partial derivatives and chain rule of functions defined on manifold,,"For functions on $\mathbb{R}^{n}$, applying chain rule and taking partial derivatives is straight forward. I am a bit confused about how this concept can be extended to functions defined on manifolds. To better explain my question, first I'll give a, rather well known, example of a real valued function on $\mathbb{R}^2$, and then an example of a function on a mainfold, which I need to understand precisely. Let $f:\mathbb{R}^2\rightarrow\mathbb{R}$, its derivative can be written as, \begin{equation*} \label{eq:1} \begin{aligned}   \frac{d}{dt}f(x_{1},x_{2}) &= \frac{d}{dx_{1}}f(x_{1},x_{2}) \frac{d}{dt} x_{1} + \frac{d}{dx_{2}}f(x_{1},x_{2}) \frac{d}{dt} x_{2}\\   &= \left( d_{x_{1}}f \right) \dot{x}_{1} + \left( d_{x_{2}}f \right) \dot{x}_{2} \end{aligned} \end{equation*} One way to interpret $d_{x_{1}}f$ is that it represents how $f$ changes in the direction of the coordinate axis $x_{1}$, also known as Lie derivative. Now here is the actual question I want to ask. For a function on a manifold, in my case, I have a Lie algebra valued function $f: SO(3)\times SO(3) \rightarrow \mathfrak{so(3)}$, is this precisely correct to write the derivative in the following way? Let $R,R_d\in SO(3)$, \begin{equation*} \label{eq:2} \begin{aligned}   \frac{d}{dt}f(R,R_d) &= \frac{d}{d_R}f(R,R_d) \dot R + \frac{d}{dR_{d}}f(R,R_d) \dot R_d\\   &= \left( d_{R}f \right) \dot{R} + \left( d_{R_{d}}f \right) \dot{R}_{d}\\ &= \left( T_{R}f \right) \dot{R} + \left( T_{R_{d}}f \right) \dot{R}_{d}, \end{aligned} \end{equation*}  where $T_{R}f$ is the tangent space of $f$ at point $R$. This does not seem correct, or not mathematically precise because of the following two reasons, $d_R f$, strictly speaking, is not the change of $f$ in the direction of the ""coordinate"" $R$, as this function is geometric, and defined in a coordinate free way. This means the second line of the above equation does not make sense? $T_{R}f$ is the tangent space of $f$ at point $R$, and $T_{R_d}f$ is the tangent space of $f$ at point $R_d$, and we can't simply add vectors belonging to different tangent spaces. This means the third line of the equation does not make sense either? How can I write this derivative in a mathematically (geometrically) correct way? For some reason I don't want to merge $\dot R$, $\dot R_d$ with the term preceding it. In other words, I want to keep the term appearing because of chain rule separate with the rest of the terms. The reason I have introduced the notion of tangent spaces is because I think this is the right way to define derivatives of mainfolds. For example let $\mathcal{M}$, and $\mathcal{N}$, be manifolds, and let $f:\mathcal{M}\rightarrow \mathcal{N}$, the derivative, also called the push forward map, is defined as $D_f: T\mathcal{M}\rightarrow T\mathcal{N}$, where $T\mathcal{M}$, and $T\mathcal{N}$ are the tanget bundle of the manifolds $\mathcal{M}$, and $\mathcal{N}$, respectively. Even under this abstract definition of derivative, how can we defined derivative of a function on a manifold, while keeping the terms appearing because of chain rule separate?","For functions on $\mathbb{R}^{n}$, applying chain rule and taking partial derivatives is straight forward. I am a bit confused about how this concept can be extended to functions defined on manifolds. To better explain my question, first I'll give a, rather well known, example of a real valued function on $\mathbb{R}^2$, and then an example of a function on a mainfold, which I need to understand precisely. Let $f:\mathbb{R}^2\rightarrow\mathbb{R}$, its derivative can be written as, \begin{equation*} \label{eq:1} \begin{aligned}   \frac{d}{dt}f(x_{1},x_{2}) &= \frac{d}{dx_{1}}f(x_{1},x_{2}) \frac{d}{dt} x_{1} + \frac{d}{dx_{2}}f(x_{1},x_{2}) \frac{d}{dt} x_{2}\\   &= \left( d_{x_{1}}f \right) \dot{x}_{1} + \left( d_{x_{2}}f \right) \dot{x}_{2} \end{aligned} \end{equation*} One way to interpret $d_{x_{1}}f$ is that it represents how $f$ changes in the direction of the coordinate axis $x_{1}$, also known as Lie derivative. Now here is the actual question I want to ask. For a function on a manifold, in my case, I have a Lie algebra valued function $f: SO(3)\times SO(3) \rightarrow \mathfrak{so(3)}$, is this precisely correct to write the derivative in the following way? Let $R,R_d\in SO(3)$, \begin{equation*} \label{eq:2} \begin{aligned}   \frac{d}{dt}f(R,R_d) &= \frac{d}{d_R}f(R,R_d) \dot R + \frac{d}{dR_{d}}f(R,R_d) \dot R_d\\   &= \left( d_{R}f \right) \dot{R} + \left( d_{R_{d}}f \right) \dot{R}_{d}\\ &= \left( T_{R}f \right) \dot{R} + \left( T_{R_{d}}f \right) \dot{R}_{d}, \end{aligned} \end{equation*}  where $T_{R}f$ is the tangent space of $f$ at point $R$. This does not seem correct, or not mathematically precise because of the following two reasons, $d_R f$, strictly speaking, is not the change of $f$ in the direction of the ""coordinate"" $R$, as this function is geometric, and defined in a coordinate free way. This means the second line of the above equation does not make sense? $T_{R}f$ is the tangent space of $f$ at point $R$, and $T_{R_d}f$ is the tangent space of $f$ at point $R_d$, and we can't simply add vectors belonging to different tangent spaces. This means the third line of the equation does not make sense either? How can I write this derivative in a mathematically (geometrically) correct way? For some reason I don't want to merge $\dot R$, $\dot R_d$ with the term preceding it. In other words, I want to keep the term appearing because of chain rule separate with the rest of the terms. The reason I have introduced the notion of tangent spaces is because I think this is the right way to define derivatives of mainfolds. For example let $\mathcal{M}$, and $\mathcal{N}$, be manifolds, and let $f:\mathcal{M}\rightarrow \mathcal{N}$, the derivative, also called the push forward map, is defined as $D_f: T\mathcal{M}\rightarrow T\mathcal{N}$, where $T\mathcal{M}$, and $T\mathcal{N}$ are the tanget bundle of the manifolds $\mathcal{M}$, and $\mathcal{N}$, respectively. Even under this abstract definition of derivative, how can we defined derivative of a function on a manifold, while keeping the terms appearing because of chain rule separate?",,"['derivatives', 'differential-geometry', 'manifolds', 'lie-groups', 'pushforward']"
48,Notion of derivative used in Petersen & Pedersen's Matrix Cookbook,Notion of derivative used in Petersen & Pedersen's Matrix Cookbook,,"I am looking at the Matrix Cookbook . From my real analysis background, my understanding of calculating derivatives involving matrices is to use the Fréchet derivative on the normed space $(\mathbb{R}^{n \times n}, \|\cdot\|_{op})$ and whatever the target space is, but I am having a hard time linking this to what is used in this book. For example, consider the matrix trace $\text{Tr}: \mathbb{R}^{n \times n} \rightarrow \mathbb{R}$. This is a linear map so the Frechet derivative in direction ${\bf{V}} \in \mathbb{R}^{n \times n}$ is just the linear map itself independent of the point ${\bf{X}} \in \mathbb{R}^{n \times n}$ so $$\text{d}\text{Tr}({\bf{X}}){\bf{V}} = \text{Tr}({\bf{V}})$$ whereas in the Matrix Cookbook, the following identity is stated $$\displaystyle\frac{\partial}{\partial {\bf{X}}}\text{Tr}({\bf{X}}) = {\bf{I}}$$ Which I suppose has the same property of being independent of ${\bf{X}}$ but it's not the same. Another example is the function $f({\bf{X}}) = {\bf{X}}^{-1}$, which has Frechet derivative $\text{d}f({\bf{X}}){\bf{V}} = -{\bf{X}}^{-1}{\bf{V}}{\bf{X}}^{-1}$, MC states an extremely similar looking identity: $$\frac{\partial{\bf{X}}^{-1}}{\partial x} = -{\bf{X}}^{-1}\frac{\partial{\bf{X}}}{\partial x}{\bf{X}}^{-1}$$ My question is, what is the definition of $\displaystyle\frac{\partial}{\partial {\bf{X}}}$, $\partial{\bf{X}}$ and exotic expressions such as $\partial{\lambda_i}$ $\partial{\bf{v}}_i$ (where $\lambda_i, {\bf{v}}_i$ are the eigenvalues and vectors of a real symmetric matrix). I am also curious if there a nice geometric interpretation or analogue to derivatives in Banach spaces and why might these specialised derivatives be preferred over a Frechet derivative in applications. I have found a similar question with some answers here but I did not find these particularly enlightening, any insightful answers are much appreciated.","I am looking at the Matrix Cookbook . From my real analysis background, my understanding of calculating derivatives involving matrices is to use the Fréchet derivative on the normed space $(\mathbb{R}^{n \times n}, \|\cdot\|_{op})$ and whatever the target space is, but I am having a hard time linking this to what is used in this book. For example, consider the matrix trace $\text{Tr}: \mathbb{R}^{n \times n} \rightarrow \mathbb{R}$. This is a linear map so the Frechet derivative in direction ${\bf{V}} \in \mathbb{R}^{n \times n}$ is just the linear map itself independent of the point ${\bf{X}} \in \mathbb{R}^{n \times n}$ so $$\text{d}\text{Tr}({\bf{X}}){\bf{V}} = \text{Tr}({\bf{V}})$$ whereas in the Matrix Cookbook, the following identity is stated $$\displaystyle\frac{\partial}{\partial {\bf{X}}}\text{Tr}({\bf{X}}) = {\bf{I}}$$ Which I suppose has the same property of being independent of ${\bf{X}}$ but it's not the same. Another example is the function $f({\bf{X}}) = {\bf{X}}^{-1}$, which has Frechet derivative $\text{d}f({\bf{X}}){\bf{V}} = -{\bf{X}}^{-1}{\bf{V}}{\bf{X}}^{-1}$, MC states an extremely similar looking identity: $$\frac{\partial{\bf{X}}^{-1}}{\partial x} = -{\bf{X}}^{-1}\frac{\partial{\bf{X}}}{\partial x}{\bf{X}}^{-1}$$ My question is, what is the definition of $\displaystyle\frac{\partial}{\partial {\bf{X}}}$, $\partial{\bf{X}}$ and exotic expressions such as $\partial{\lambda_i}$ $\partial{\bf{v}}_i$ (where $\lambda_i, {\bf{v}}_i$ are the eigenvalues and vectors of a real symmetric matrix). I am also curious if there a nice geometric interpretation or analogue to derivatives in Banach spaces and why might these specialised derivatives be preferred over a Frechet derivative in applications. I have found a similar question with some answers here but I did not find these particularly enlightening, any insightful answers are much appreciated.",,"['linear-algebra', 'derivatives', 'soft-question', 'matrix-calculus', 'frechet-derivative']"
49,Inverse Function Theorem and Hessians Matrices,Inverse Function Theorem and Hessians Matrices,,"Let $f: V \to \mathbb{R}$ of class $C^{2}$ and $b \in V$ a critical point of $f$. If $\varphi: U \to V$ is a diffeomorphism $C^{2}$ with $\varphi(a) = b$, then the hessian of $f$ in the point $b$ and the hessian of $f \circ \varphi$ in the point $a$ has a same rank. I wanted a way to go. I tried to do some things, but I didn't have a good ideas. I know I need to use Inverse Function Theorem, but I don't know how to use it. I don't want a solution for the question, I just want an idea to continue. Thanks for the advance! EDIT (because of Daniele's comment). I have to use the Inverse Function Theorem and this implies that $\varphi$ is a local diffeomorphism iff $\varphi'(x): U \to V$ is an isomorphism $\forall x \in U$ so, I can understand that $\nabla\varphi: TU_{x} \to TV_{\varphi(x)}$ is an isomorphism. But I still cannot see how to use this to conclude the question. Is probably a little detail that I cannot see.","Let $f: V \to \mathbb{R}$ of class $C^{2}$ and $b \in V$ a critical point of $f$. If $\varphi: U \to V$ is a diffeomorphism $C^{2}$ with $\varphi(a) = b$, then the hessian of $f$ in the point $b$ and the hessian of $f \circ \varphi$ in the point $a$ has a same rank. I wanted a way to go. I tried to do some things, but I didn't have a good ideas. I know I need to use Inverse Function Theorem, but I don't know how to use it. I don't want a solution for the question, I just want an idea to continue. Thanks for the advance! EDIT (because of Daniele's comment). I have to use the Inverse Function Theorem and this implies that $\varphi$ is a local diffeomorphism iff $\varphi'(x): U \to V$ is an isomorphism $\forall x \in U$ so, I can understand that $\nabla\varphi: TU_{x} \to TV_{\varphi(x)}$ is an isomorphism. But I still cannot see how to use this to conclude the question. Is probably a little detail that I cannot see.",,"['real-analysis', 'derivatives', 'inverse-function-theorem']"
50,Prove that $\frac{dy}{dx}+\sec^2(\frac{\pi}{4}-x)=0$,Prove that,\frac{dy}{dx}+\sec^2(\frac{\pi}{4}-x)=0,"If $y=\sqrt{\frac{1-\sin 2x}{1+\sin 2x}}$, prove that $\frac{dy}{dx}+\sec^2(\frac{\pi}{4}-x)=0$ My attempts : Attempt 1 : $y=\sqrt{\frac{1-\sin 2x}{1+\sin 2x}}$ $\implies y=\sqrt{\frac{\sin^2 x+\cos^2 x-2\sin x \cos x}{\sin^2 x+\cos^2 x+2\sin x \cos x}}$ $\implies y=\sqrt{\frac{(\sin x -\cos x)^2}{(\sin x+\cos x)^2}}$ $\implies y=\frac{\sin x -\cos x}{\sin x +\cos x}$ $\therefore \frac{dy}{dx}=\frac{(\sin x +\cos x)(\cos x+\sin x)-(\sin x -\cos x)(\cos x -\sin x)}{(\sin x +\cos x)^2}$ $\implies \frac{dy}{dx}=\frac{(\sin x+\cos x)^2+(\sin x -\cos x)^2}{(\sin x+\cos x)^2}$ $\implies \frac{dy}{dx}=\frac{2(\sin^2 x+\cos^2 x)}{(\cos(\frac{\pi}{2}-x)+\cos x)^2}$ $\implies \frac{dy}{dx}=\frac{2}{(2\cos(\frac{\pi}{4})\cos(\frac{\pi}{4}-x))^2}$ $\implies \frac{dy}{dx}=\frac{2}{4\cos^2(\frac{\pi}{4})\cos^2(\frac{\pi}{4}-x)}$ $\implies \frac{dy}{dx}=\frac{2}{4\times\frac{1}{2}\cos^2(\frac{\pi}{4}-x)}$ $\implies \frac{dy}{dx}=\sec^2(\frac{\pi}{4}-x)$ $\implies \frac{dy}{dx}-\sec^2(\frac{\pi}{4}-x)=0$ But i have to prove $\frac{dy}{dx}+\sec^2(\frac{\pi}{4}-x)=0$ Attempt 2 : $y=\sqrt{\frac{1-\sin 2x}{1+\sin 2x}}$ $\implies y=\sqrt{\frac{\cos^2 x+\sin^2 x-2\cos x \sin x}{\cos^2 x+\sin^2 x+2\cos x \sin x}}$ $\implies y=\sqrt{\frac{(\cos x -\sin x)^2}{(\cos x+\sin x)^2}}$ $\implies y=\frac{\cos x -\sin x}{\cos x +\sin x}$ $\therefore \frac{dy}{dx}=\frac{(\cos x +\sin x)(-\sin x-\cos x)-(\cos x -\sin x)(-\sin x +\cos x)}{(\cos x +\sin x)^2}$ $\implies \frac{dy}{dx}=\frac{-(\cos x+\sin x)^2+(\cos x -\sin x)^2}{(\cos x+\sin x)^2}$ $\implies \frac{dy}{dx}=\frac{(\cos x -\sin x)^2-(\cos x+\sin x)^2}{(\cos x+\sin x)^2}$ $\implies \frac{dy}{dx}=\frac{-4\sin x\cos x}{(\cos x+\cos(\frac{\pi}{2}-x))^2}$ $\implies \frac{dy}{dx}=\frac{-4\sin x\cos x}{(2\cos(\frac{\pi}{4})\cos(x-\frac{\pi}{4}))^2}$ $\implies \frac{dy}{dx}=\frac{-4\sin x\cos x}{4\cos^2(\frac{\pi}{4})\cos^2(x-\frac{\pi}{4})}$ $\implies \frac{dy}{dx}=\frac{-4\sin x\cos x}{4\times \frac{1}{2}\cos^2(x-\frac{\pi}{4})}$ $\implies \frac{dy}{dx}=\frac{-2\sin x\cos x}{\cos^2(x-\frac{\pi}{4})}$ Attempt 3 : $y=\sqrt{\frac{1-\sin 2x}{1+\sin 2x}}$ $\implies y=\sqrt{\frac{(1-\sin 2x)(1-\sin 2x)}{(1+\sin 2x)(1-\sin 2x)}}$ $\implies y=\frac{1-\sin 2x}{\cos 2x}$ $\therefore \frac{dy}{dx}=\frac{\cos 2x(-2\cos 2x)-(1-\sin 2x)(-2\sin 2x)}{\cos^2 2x}$ $\implies \frac{dy}{dx}=\frac{-2\cos^2 2x+2\sin 2x(1-\sin 2x)}{\cos^2 2x}$ My questions : (i) Why does Attempt 2 give a result different from that given by Attempt 1 . (ii) How do i prove the result?","If $y=\sqrt{\frac{1-\sin 2x}{1+\sin 2x}}$, prove that $\frac{dy}{dx}+\sec^2(\frac{\pi}{4}-x)=0$ My attempts : Attempt 1 : $y=\sqrt{\frac{1-\sin 2x}{1+\sin 2x}}$ $\implies y=\sqrt{\frac{\sin^2 x+\cos^2 x-2\sin x \cos x}{\sin^2 x+\cos^2 x+2\sin x \cos x}}$ $\implies y=\sqrt{\frac{(\sin x -\cos x)^2}{(\sin x+\cos x)^2}}$ $\implies y=\frac{\sin x -\cos x}{\sin x +\cos x}$ $\therefore \frac{dy}{dx}=\frac{(\sin x +\cos x)(\cos x+\sin x)-(\sin x -\cos x)(\cos x -\sin x)}{(\sin x +\cos x)^2}$ $\implies \frac{dy}{dx}=\frac{(\sin x+\cos x)^2+(\sin x -\cos x)^2}{(\sin x+\cos x)^2}$ $\implies \frac{dy}{dx}=\frac{2(\sin^2 x+\cos^2 x)}{(\cos(\frac{\pi}{2}-x)+\cos x)^2}$ $\implies \frac{dy}{dx}=\frac{2}{(2\cos(\frac{\pi}{4})\cos(\frac{\pi}{4}-x))^2}$ $\implies \frac{dy}{dx}=\frac{2}{4\cos^2(\frac{\pi}{4})\cos^2(\frac{\pi}{4}-x)}$ $\implies \frac{dy}{dx}=\frac{2}{4\times\frac{1}{2}\cos^2(\frac{\pi}{4}-x)}$ $\implies \frac{dy}{dx}=\sec^2(\frac{\pi}{4}-x)$ $\implies \frac{dy}{dx}-\sec^2(\frac{\pi}{4}-x)=0$ But i have to prove $\frac{dy}{dx}+\sec^2(\frac{\pi}{4}-x)=0$ Attempt 2 : $y=\sqrt{\frac{1-\sin 2x}{1+\sin 2x}}$ $\implies y=\sqrt{\frac{\cos^2 x+\sin^2 x-2\cos x \sin x}{\cos^2 x+\sin^2 x+2\cos x \sin x}}$ $\implies y=\sqrt{\frac{(\cos x -\sin x)^2}{(\cos x+\sin x)^2}}$ $\implies y=\frac{\cos x -\sin x}{\cos x +\sin x}$ $\therefore \frac{dy}{dx}=\frac{(\cos x +\sin x)(-\sin x-\cos x)-(\cos x -\sin x)(-\sin x +\cos x)}{(\cos x +\sin x)^2}$ $\implies \frac{dy}{dx}=\frac{-(\cos x+\sin x)^2+(\cos x -\sin x)^2}{(\cos x+\sin x)^2}$ $\implies \frac{dy}{dx}=\frac{(\cos x -\sin x)^2-(\cos x+\sin x)^2}{(\cos x+\sin x)^2}$ $\implies \frac{dy}{dx}=\frac{-4\sin x\cos x}{(\cos x+\cos(\frac{\pi}{2}-x))^2}$ $\implies \frac{dy}{dx}=\frac{-4\sin x\cos x}{(2\cos(\frac{\pi}{4})\cos(x-\frac{\pi}{4}))^2}$ $\implies \frac{dy}{dx}=\frac{-4\sin x\cos x}{4\cos^2(\frac{\pi}{4})\cos^2(x-\frac{\pi}{4})}$ $\implies \frac{dy}{dx}=\frac{-4\sin x\cos x}{4\times \frac{1}{2}\cos^2(x-\frac{\pi}{4})}$ $\implies \frac{dy}{dx}=\frac{-2\sin x\cos x}{\cos^2(x-\frac{\pi}{4})}$ Attempt 3 : $y=\sqrt{\frac{1-\sin 2x}{1+\sin 2x}}$ $\implies y=\sqrt{\frac{(1-\sin 2x)(1-\sin 2x)}{(1+\sin 2x)(1-\sin 2x)}}$ $\implies y=\frac{1-\sin 2x}{\cos 2x}$ $\therefore \frac{dy}{dx}=\frac{\cos 2x(-2\cos 2x)-(1-\sin 2x)(-2\sin 2x)}{\cos^2 2x}$ $\implies \frac{dy}{dx}=\frac{-2\cos^2 2x+2\sin 2x(1-\sin 2x)}{\cos^2 2x}$ My questions : (i) Why does Attempt 2 give a result different from that given by Attempt 1 . (ii) How do i prove the result?",,"['calculus', 'trigonometry', 'derivatives', 'proof-verification']"
51,Smooth partition of unity: trouble verifying detail in Folland,Smooth partition of unity: trouble verifying detail in Folland,,"I have consulted the book Introduction to Partial Differential Equations by Folland (1995, Second Edition, Princeton University Press) for the proof of a certain theorem, but I am having trouble verifying a minor detail. Before I present what I am having trouble with, let me present the theorem and the proof as it is written by Folland on page 13 of the aforementioned book. (If $V$ is an open subset of $\mathbb{R}^n$, Folland writes $C_c^\infty(V)$ for the space of smooth functions on $\mathbf{R}^n$ whose support is compact and contained in $V$.) (0.19) Theorem Let $K \subset \mathbb{R}^n$ be compact and let $V_1, \dotsc, V_N$ be bounded open sets such that $K \subset \bigcup_1^N V_j$. Then there exist functions $\zeta_1, \dotsc, \zeta_N$ with $\zeta_j \in C_c^\infty(V_j)$ such that $\sum_1^N \zeta_j = 1$ on $K$. Proof Let $W_1, \dotsc, W_N$ be as in Lemma (0.18). [Comment: this means that the $W_j$ are open, cover $K$, and $\overline{W_j} \subset V_j$.] By Theorem (0.17), we can choose $\phi_j \in C_c^\infty(V_j)$ with $0 \le \phi_j \le 1$ and $\phi_j = 1$ on $\overline{W_j}$. Then $\Phi = \sum_1^N \phi_j \ge 1$ on $K$, so we can take $\zeta_j = \phi_j/\Phi$, with the understanding that $\zeta_j = 0$ wherever $\phi_j = 0$. For my applications I am only interested in demonstrating that the $\zeta_j$ belong to $C_c^1(V_j)$, and I am having trouble verifying that they are continuously differentiable ; everything else is fine. What is more, in my applications I do not assume that the $V_j$ are bounded, and I would appreciate an answer not relying upon their boundedness. It should be noted that the $\overline{W_j}$ may still be taken compact in this case. What follows is my progress. Pick $j \in I$, and define $A_j = \{ x \in \mathbb{R}^n : \phi_j(x) > 0 \}$, an open set. Note that $\text{supp } \phi_j = \overline{A_j}$, since $\phi_j \ge 0$ by construction. Firstly, $\phi_j = 0$ on the open set $\mathbf{R}^n \setminus \overline{A_j}$, hence the same is true of $\zeta_j$. We see that $\zeta_j$ is continuously differentiable on $\mathbf{R}^n \setminus \overline{A_j}$. Secondly, one has $\phi_j > 0$ and $\Phi > 0$ on $A_j$, and both of these functions are continuously differentiable. Therefore, the same must be true of $\zeta_j$ on the open set $A_j$. What remains is to prove that the partial derivatives of $\zeta_j$ exist and are continuous on the boundary $\partial A_j$, and this is where I am having trouble. However, I have demonstrated that $\zeta_j = 0$ on $\partial A_j$. For consider a point $x \in \partial A_j$, and pick a sequence $(x_m)_1^\infty$ in $\mathbb{R}^n \setminus A_j$ converging to $x$. Using continuity of $\phi_j$, one has $0 = \phi_j(x_m) \to \phi_j(x)$, and so $\phi_j(x) = 0$, thus $\zeta_j(x) = 0$. Can anyone help me verify that the partial derivatives of $\zeta_j$ exist and are continuous on $\partial A_j$?","I have consulted the book Introduction to Partial Differential Equations by Folland (1995, Second Edition, Princeton University Press) for the proof of a certain theorem, but I am having trouble verifying a minor detail. Before I present what I am having trouble with, let me present the theorem and the proof as it is written by Folland on page 13 of the aforementioned book. (If $V$ is an open subset of $\mathbb{R}^n$, Folland writes $C_c^\infty(V)$ for the space of smooth functions on $\mathbf{R}^n$ whose support is compact and contained in $V$.) (0.19) Theorem Let $K \subset \mathbb{R}^n$ be compact and let $V_1, \dotsc, V_N$ be bounded open sets such that $K \subset \bigcup_1^N V_j$. Then there exist functions $\zeta_1, \dotsc, \zeta_N$ with $\zeta_j \in C_c^\infty(V_j)$ such that $\sum_1^N \zeta_j = 1$ on $K$. Proof Let $W_1, \dotsc, W_N$ be as in Lemma (0.18). [Comment: this means that the $W_j$ are open, cover $K$, and $\overline{W_j} \subset V_j$.] By Theorem (0.17), we can choose $\phi_j \in C_c^\infty(V_j)$ with $0 \le \phi_j \le 1$ and $\phi_j = 1$ on $\overline{W_j}$. Then $\Phi = \sum_1^N \phi_j \ge 1$ on $K$, so we can take $\zeta_j = \phi_j/\Phi$, with the understanding that $\zeta_j = 0$ wherever $\phi_j = 0$. For my applications I am only interested in demonstrating that the $\zeta_j$ belong to $C_c^1(V_j)$, and I am having trouble verifying that they are continuously differentiable ; everything else is fine. What is more, in my applications I do not assume that the $V_j$ are bounded, and I would appreciate an answer not relying upon their boundedness. It should be noted that the $\overline{W_j}$ may still be taken compact in this case. What follows is my progress. Pick $j \in I$, and define $A_j = \{ x \in \mathbb{R}^n : \phi_j(x) > 0 \}$, an open set. Note that $\text{supp } \phi_j = \overline{A_j}$, since $\phi_j \ge 0$ by construction. Firstly, $\phi_j = 0$ on the open set $\mathbf{R}^n \setminus \overline{A_j}$, hence the same is true of $\zeta_j$. We see that $\zeta_j$ is continuously differentiable on $\mathbf{R}^n \setminus \overline{A_j}$. Secondly, one has $\phi_j > 0$ and $\Phi > 0$ on $A_j$, and both of these functions are continuously differentiable. Therefore, the same must be true of $\zeta_j$ on the open set $A_j$. What remains is to prove that the partial derivatives of $\zeta_j$ exist and are continuous on the boundary $\partial A_j$, and this is where I am having trouble. However, I have demonstrated that $\zeta_j = 0$ on $\partial A_j$. For consider a point $x \in \partial A_j$, and pick a sequence $(x_m)_1^\infty$ in $\mathbb{R}^n \setminus A_j$ converging to $x$. Using continuity of $\phi_j$, one has $0 = \phi_j(x_m) \to \phi_j(x)$, and so $\phi_j(x) = 0$, thus $\zeta_j(x) = 0$. Can anyone help me verify that the partial derivatives of $\zeta_j$ exist and are continuous on $\partial A_j$?",,"['calculus', 'real-analysis', 'derivatives']"
52,Solving $\cos(\pi(x-1))=0$,Solving,\cos(\pi(x-1))=0,"I have a second derivative that I need to use to find inflection points to create a graph. The second derivative is $$f^{\prime\prime}(x)=-4\pi^2\cos(\pi(x-1))$$ So I set the equation to $0$ and solve for $x$ $$-4\pi^2\cos(\pi(x-1))=0$$ I divide by the constant $-4\pi^2$ and get $$\cos(\pi(x-1))=0$$ But I am basically stuck at this point. I know I need to take the inverse cosine of both sides. The result I am getting is $x=3/2$, but the answer in the book is $x=1/2$, $3/2$. Can someone help me figure out how to solve the last steps of this problem?","I have a second derivative that I need to use to find inflection points to create a graph. The second derivative is $$f^{\prime\prime}(x)=-4\pi^2\cos(\pi(x-1))$$ So I set the equation to $0$ and solve for $x$ $$-4\pi^2\cos(\pi(x-1))=0$$ I divide by the constant $-4\pi^2$ and get $$\cos(\pi(x-1))=0$$ But I am basically stuck at this point. I know I need to take the inverse cosine of both sides. The result I am getting is $x=3/2$, but the answer in the book is $x=1/2$, $3/2$. Can someone help me figure out how to solve the last steps of this problem?",,"['trigonometry', 'derivatives']"
53,Is there any other function satisfying the system of equations involving integration?,Is there any other function satisfying the system of equations involving integration?,,"I've encountered this problem: Let $f(x)$ be a continuously differentiable (real) function on $[0,1]$ satisfying these equations: $$f(1)=0$$ $$\int_0^1 [f'(x)]^2 dx = 7$$ $$\int_0^1 x^2f(x) dx = \frac{1}{3}$$. Compute $\int_0^1f(x) dx$. I've managed to find a $f(x) = \frac{7}{4}(1-x^4)$ in a few trials. However, I cannot find any other solution (or at least any other elementary solution), which seems weird to me because these equations are not enough to uniquely define a function. Moreover, assume that there are some other solutions, how can the problem be so sure that $\int_0^1f(x) dx$ are all the same among those solutions? Is there any neat way to solve the problem without finding a solution? I highly doubt these two questions. I think the problem is wrong . But I'm not sure, so I post it here to discuss. Thanks in advance.","I've encountered this problem: Let $f(x)$ be a continuously differentiable (real) function on $[0,1]$ satisfying these equations: $$f(1)=0$$ $$\int_0^1 [f'(x)]^2 dx = 7$$ $$\int_0^1 x^2f(x) dx = \frac{1}{3}$$. Compute $\int_0^1f(x) dx$. I've managed to find a $f(x) = \frac{7}{4}(1-x^4)$ in a few trials. However, I cannot find any other solution (or at least any other elementary solution), which seems weird to me because these equations are not enough to uniquely define a function. Moreover, assume that there are some other solutions, how can the problem be so sure that $\int_0^1f(x) dx$ are all the same among those solutions? Is there any neat way to solve the problem without finding a solution? I highly doubt these two questions. I think the problem is wrong . But I'm not sure, so I post it here to discuss. Thanks in advance.",,"['calculus', 'real-analysis', 'integration', 'derivatives', 'definite-integrals']"
54,"Differentiate $f(x)=\frac{\cos^{-1}\frac{x}{2}}{\sqrt{2x+7}}$, $-2<x<2$","Differentiate ,",f(x)=\frac{\cos^{-1}\frac{x}{2}}{\sqrt{2x+7}} -2<x<2,"Differentiate $f(x)=\frac{\cos^{-1}\frac{x}{2}}{\sqrt{2x+7}}$, $-2<x<2$ My Attempt $$ \begin{align} f'(x)&=\frac{\sqrt{2x+7}\frac{d}{dx}\cos^{-1}\big(\frac{x}{2}\big)-\cos^{-1}\big(\frac{x}{2}\big)\frac{d}{dx}\sqrt{2x+7}}{(\sqrt{2x+7})^2}\\ &=\frac{\sqrt{2x+7}\frac{\frac{-1}{2}}{\sqrt{1-\frac{x^2}{4}}}-\cos^{-1}\big(\frac{x}{2}\big)\frac{2}{2\sqrt{2x+7}}}{(\sqrt{2x+7})^2}\\ &=\frac{\frac{-\sqrt{2x+7}.\sqrt{4}}{2\sqrt{4-x^2}}-\frac{\cos^{-1}\big(\frac{x}{2}\big)}{\sqrt{2x+7}}}{2x+7}\\ &=\frac{\pm2}{2.\sqrt{2x+7}\sqrt{4-x^2}}-\frac{\cos^{-1}\big(\frac{x}{2}\big)}{(2x+7)^{3/2}}\\ &\color{blue}{=\frac{\pm1}{\sqrt{2x+7}\sqrt{4-x^2}}-\frac{\cos^{-1}\big(\frac{x}{2}\big)}{(2x+7)^{3/2}}\\} \end{align} $$ Result by Mathematica D[(ArcCos[x/2]/Sqrt[2 x + 7]),x] $$ f'(x)={\frac{-1}{\sqrt{2-x}\sqrt{2+x}\sqrt{7+2x}}-\frac{\arccos\big(\frac{x}{2}\big)}{(7+2x)^{3/2}}}\\ \color{blue}{=\frac{-1}{\sqrt{2x+7}\sqrt{4-x^2}}-\frac{\cos^{-1}\big(\frac{x}{2}\big)}{(2x+7)^{3/2}}\\} $$ Mathematica seems to use only the principal root, ie. $\sqrt{4}=+2$ but in that case can I say the given solution is incomplete ? Or is there any other domain or range considerations which eliminate the other case ?","Differentiate $f(x)=\frac{\cos^{-1}\frac{x}{2}}{\sqrt{2x+7}}$, $-2<x<2$ My Attempt $$ \begin{align} f'(x)&=\frac{\sqrt{2x+7}\frac{d}{dx}\cos^{-1}\big(\frac{x}{2}\big)-\cos^{-1}\big(\frac{x}{2}\big)\frac{d}{dx}\sqrt{2x+7}}{(\sqrt{2x+7})^2}\\ &=\frac{\sqrt{2x+7}\frac{\frac{-1}{2}}{\sqrt{1-\frac{x^2}{4}}}-\cos^{-1}\big(\frac{x}{2}\big)\frac{2}{2\sqrt{2x+7}}}{(\sqrt{2x+7})^2}\\ &=\frac{\frac{-\sqrt{2x+7}.\sqrt{4}}{2\sqrt{4-x^2}}-\frac{\cos^{-1}\big(\frac{x}{2}\big)}{\sqrt{2x+7}}}{2x+7}\\ &=\frac{\pm2}{2.\sqrt{2x+7}\sqrt{4-x^2}}-\frac{\cos^{-1}\big(\frac{x}{2}\big)}{(2x+7)^{3/2}}\\ &\color{blue}{=\frac{\pm1}{\sqrt{2x+7}\sqrt{4-x^2}}-\frac{\cos^{-1}\big(\frac{x}{2}\big)}{(2x+7)^{3/2}}\\} \end{align} $$ Result by Mathematica D[(ArcCos[x/2]/Sqrt[2 x + 7]),x] $$ f'(x)={\frac{-1}{\sqrt{2-x}\sqrt{2+x}\sqrt{7+2x}}-\frac{\arccos\big(\frac{x}{2}\big)}{(7+2x)^{3/2}}}\\ \color{blue}{=\frac{-1}{\sqrt{2x+7}\sqrt{4-x^2}}-\frac{\cos^{-1}\big(\frac{x}{2}\big)}{(2x+7)^{3/2}}\\} $$ Mathematica seems to use only the principal root, ie. $\sqrt{4}=+2$ but in that case can I say the given solution is incomplete ? Or is there any other domain or range considerations which eliminate the other case ?",,"['calculus', 'derivatives']"
55,"Finding the point on $6x^2+y^2=262090$ that is nearest to the point $(1045,0)$",Finding the point on  that is nearest to the point,"6x^2+y^2=262090 (1045,0)","I know that to find the point on $6x^2+y^2=262090$ that is nearest to the point $(1045,0)$, we can try to minimize the squared distance  $S=(x-1045)^2+262090-6x^2$. However, calculus tells us that this function does not have a minimum point (instead only a maximum point exists). But if we try to minimize the distance function ( without squaring) then we can find the minimum. So, When exactly can we actually square the distance function to find the max/min point for distance problems?","I know that to find the point on $6x^2+y^2=262090$ that is nearest to the point $(1045,0)$, we can try to minimize the squared distance  $S=(x-1045)^2+262090-6x^2$. However, calculus tells us that this function does not have a minimum point (instead only a maximum point exists). But if we try to minimize the distance function ( without squaring) then we can find the minimum. So, When exactly can we actually square the distance function to find the max/min point for distance problems?",,"['calculus', 'derivatives']"
56,Characterizing discontinuous derivatives,Characterizing discontinuous derivatives,,"Apparently the set of discontinuity of derivatives is weird in its own sense. Following are the examples that I know so far: $1.$  $$g(x)=\left\{ \begin{array}{ll} x^2 \sin(\frac{1}{x})  &  x \in (0,1] \\ 0 &  x=0  \end{array}\right.$$ $g'$ is discontinuous at $x=0$. $2. $  The Volterra function defined on the ternary Cantor set is differentiable everywhere but the derivative is discontinuous on the whole of Cantor set ,that is on a nowhere dense set of measure zero. $3.$  The Volterra function defined on the Fat-Cantor set is differentiable everywhere but the derivative is discontinuous on the whole of Fat-Cantor set ,that is on a set of positive measure, but not full measure. $4.$  I am yet to find a derivative which is discontinuous on a set of full measure. Some good discussion about this can be found here and here. Questions: 1.What are some  examples of functions whose derivative is discontinuous on a dense set of zero measure , say on the rationals? 2.What are some  examples of functions whose derivative is discontinuous on a dense set of positive measure , say on the irrationals? Update : One can find a function which is differentiable everywhere but whose derivative is discontinuous on a dense set of zero measure here.","Apparently the set of discontinuity of derivatives is weird in its own sense. Following are the examples that I know so far: $1.$  $$g(x)=\left\{ \begin{array}{ll} x^2 \sin(\frac{1}{x})  &  x \in (0,1] \\ 0 &  x=0  \end{array}\right.$$ $g'$ is discontinuous at $x=0$. $2. $  The Volterra function defined on the ternary Cantor set is differentiable everywhere but the derivative is discontinuous on the whole of Cantor set ,that is on a nowhere dense set of measure zero. $3.$  The Volterra function defined on the Fat-Cantor set is differentiable everywhere but the derivative is discontinuous on the whole of Fat-Cantor set ,that is on a set of positive measure, but not full measure. $4.$  I am yet to find a derivative which is discontinuous on a set of full measure. Some good discussion about this can be found here and here. Questions: 1.What are some  examples of functions whose derivative is discontinuous on a dense set of zero measure , say on the rationals? 2.What are some  examples of functions whose derivative is discontinuous on a dense set of positive measure , say on the irrationals? Update : One can find a function which is differentiable everywhere but whose derivative is discontinuous on a dense set of zero measure here.",,"['real-analysis', 'derivatives', 'cantor-set']"
57,Domain of k in $f(x)=x^3+kx^2+5x+4\sin^2x$,Domain of k in,f(x)=x^3+kx^2+5x+4\sin^2x,The question says: Let $f(x)=x^3+kx^2+5x+4\sin^2x$ be an increasing function on $x \in R$. Then domain of k is? This is What I tried: I tried differentiating the expression but $f'(x)$ didn't turn out to be a quadratic(as it has that $\sin2x$ term) $$f'(x)=3x^2+2kx+5+4\sin2x>0$$ So how else am I supposed to solve this question?,The question says: Let $f(x)=x^3+kx^2+5x+4\sin^2x$ be an increasing function on $x \in R$. Then domain of k is? This is What I tried: I tried differentiating the expression but $f'(x)$ didn't turn out to be a quadratic(as it has that $\sin2x$ term) $$f'(x)=3x^2+2kx+5+4\sin2x>0$$ So how else am I supposed to solve this question?,,"['calculus', 'derivatives']"
58,"Find the extreme points of the function $g(x):=(x^4-2x^2+2)^{1/2}, x∈[-0.5,2]$",Find the extreme points of the function,"g(x):=(x^4-2x^2+2)^{1/2}, x∈[-0.5,2]","I need to find the extreme points of the function $$g(x):=(x^4-2x^2+2)^{1/2}, x∈[-0.5,2]$$ I first found $$f'(x)=\frac{{4x^3-4x}}{2\sqrt {x^4-2x^2+2}}$$ and made $f'(x)=0$ to find all the roots of the function, $x_1=0, x_2=1, x_3=-1$ but since $x_3$ is out of the domain I didn't consider it. Now I have $4$ candidates for the extreme points for this function, namely $x_1, x_2, r_1=-0.5, r_2=2$, where $r_1, r_2$ are the ends of the domain. I then put these candidates back into $f(x)$ and found that $$f(x_2)<f(r_1)<f(x_1)<f(r_2)$$ showing that $x_2$ is the global minimum and $r_2$ is the global maximum. But I can't seem to figure out the local maximum and local minimum of the function. I tried making a sign table for the function: But I have no idea how to determine that $x_1$ is the local maximum and $r_1$ is the local minimum. PS - Sorry for the terrible sign graph, I had to use an online graphing tool.","I need to find the extreme points of the function $$g(x):=(x^4-2x^2+2)^{1/2}, x∈[-0.5,2]$$ I first found $$f'(x)=\frac{{4x^3-4x}}{2\sqrt {x^4-2x^2+2}}$$ and made $f'(x)=0$ to find all the roots of the function, $x_1=0, x_2=1, x_3=-1$ but since $x_3$ is out of the domain I didn't consider it. Now I have $4$ candidates for the extreme points for this function, namely $x_1, x_2, r_1=-0.5, r_2=2$, where $r_1, r_2$ are the ends of the domain. I then put these candidates back into $f(x)$ and found that $$f(x_2)<f(r_1)<f(x_1)<f(r_2)$$ showing that $x_2$ is the global minimum and $r_2$ is the global maximum. But I can't seem to figure out the local maximum and local minimum of the function. I tried making a sign table for the function: But I have no idea how to determine that $x_1$ is the local maximum and $r_1$ is the local minimum. PS - Sorry for the terrible sign graph, I had to use an online graphing tool.",,"['calculus', 'real-analysis', 'derivatives', 'maxima-minima']"
59,Equivalence of an alternative definition of the derivative.,Equivalence of an alternative definition of the derivative.,,My calculus professor defined the derivative of a real function $f$ as follows : “Let $f\colon \mathbb{R} \longrightarrow \mathbb R$ be a function. Then $f$ is said to be differentiable at a point $a \in\mathbb R$ if $\exists$ a function $\psi$ such that : 1) $(\forall h \in\mathbb{R}): f(a+h) = f(a) + mh + hΨ(h)$ 2) $\psi$ is continuous at $0$ 3) $\psi(0) = 0$ Then we denote the derivative of $f(x)$ as $f'(x) = m$” My problem in this definition is as follows: I can see that by rearranging the terms $f(a)$ and then dividing by h we get the regular definition of the limit. I do not see what is the significance of $\psi$.,My calculus professor defined the derivative of a real function $f$ as follows : “Let $f\colon \mathbb{R} \longrightarrow \mathbb R$ be a function. Then $f$ is said to be differentiable at a point $a \in\mathbb R$ if $\exists$ a function $\psi$ such that : 1) $(\forall h \in\mathbb{R}): f(a+h) = f(a) + mh + hΨ(h)$ 2) $\psi$ is continuous at $0$ 3) $\psi(0) = 0$ Then we denote the derivative of $f(x)$ as $f'(x) = m$” My problem in this definition is as follows: I can see that by rearranging the terms $f(a)$ and then dividing by h we get the regular definition of the limit. I do not see what is the significance of $\psi$.,,"['calculus', 'derivatives', 'definition']"
60,Find Taylor series of: $x^x$,Find Taylor series of:,x^x,"Find the Taylor series polynomial form of: $$x^x$$ My attempt: I started calculating derivatives of $x^x$ for the series, $$f(x)=x^x$$ $$f'(x)=x^x(\ln x+1)$$ $$f''(x)=x^{x-1}+x^x(\ln x+1)$$ $$f'''(x)=\cdots$$ $$\vdots$$ But i could get only till certain terms and got frustrated, Attempt no. 2: $$f(x)=x^x$$ $$f(x)=e^{x\ln x}$$ $$f(x)=1+x\ln x+\frac{x^2\ln^2 x}{2!}+\cdots$$ $$f(x)=1+x({x-1}-\frac{(x-1)^2}{2}+\frac{(x-1)^3}{3}+\cdots)+\frac{x^2}{2}({x-1}-\frac{(x-1)^2}{2}+\frac{(x-2)^3}{3}+\cdots)^2+\cdots$$ Now let's try to get $x^n$ 's coefficient, $$C(x^0)=1$$ In the bracket multiplied to x, $$1+x(x-1-x^2+2x-1+x^3-3x+3x-1+\cdots)+\cdots$$ Following the lead $$1+x(g(x)-1-1-1-1\cdots)+\cdots$$ $$C(x^1)=-1-1-1-\cdots$$ So, $$C(x^1)=-\infty\cdots?$$ How do we write this equation? Attempt 3: $$f(x)=1+(x+1)\ln (x+1)+\frac{(x+1)^2}{2!}\ln^2(x+1)+\cdots$$ Now, $$f(x)=1+(x+1)(x+\frac{x^2}{2}+\frac{x^3}{3}\cdots)+\cdots$$ Now, $$C(x^0)=1$$ Considering $(1+x)^2,(1+x)^3\cdots$ contribution to $C(x^1)$ $$C(x^1)=1+1+1+1+\cdots$$ $$C(x^1)=\infty\cdots ?$$ Still.....stuck","Find the Taylor series polynomial form of: My attempt: I started calculating derivatives of for the series, But i could get only till certain terms and got frustrated, Attempt no. 2: Now let's try to get 's coefficient, In the bracket multiplied to x, Following the lead So, How do we write this equation? Attempt 3: Now, Now, Considering contribution to Still.....stuck","x^x x^x f(x)=x^x f'(x)=x^x(\ln x+1) f''(x)=x^{x-1}+x^x(\ln x+1) f'''(x)=\cdots \vdots f(x)=x^x f(x)=e^{x\ln x} f(x)=1+x\ln x+\frac{x^2\ln^2 x}{2!}+\cdots f(x)=1+x({x-1}-\frac{(x-1)^2}{2}+\frac{(x-1)^3}{3}+\cdots)+\frac{x^2}{2}({x-1}-\frac{(x-1)^2}{2}+\frac{(x-2)^3}{3}+\cdots)^2+\cdots x^n C(x^0)=1 1+x(x-1-x^2+2x-1+x^3-3x+3x-1+\cdots)+\cdots 1+x(g(x)-1-1-1-1\cdots)+\cdots C(x^1)=-1-1-1-\cdots C(x^1)=-\infty\cdots? f(x)=1+(x+1)\ln (x+1)+\frac{(x+1)^2}{2!}\ln^2(x+1)+\cdots f(x)=1+(x+1)(x+\frac{x^2}{2}+\frac{x^3}{3}\cdots)+\cdots C(x^0)=1 (1+x)^2,(1+x)^3\cdots C(x^1) C(x^1)=1+1+1+1+\cdots C(x^1)=\infty\cdots ?","['calculus', 'derivatives', 'taylor-expansion']"
61,Is completeness of the real line needed to show that if $\lim\limits_{x\to0} f'(x)$ exists and $f$ is continuous at $0$ then $f'(0)$ is the limit?,Is completeness of the real line needed to show that if  exists and  is continuous at  then  is the limit?,\lim\limits_{x\to0} f'(x) f 0 f'(0),"A question about existence of derivative of function at Zero The question linked above inspires another question. $f:\mathbb R\to\mathbb R$ is assumed to be continuous everywhere. It is assumed to be differentiable at all points besides $0.$ It is assumed that $\displaystyle\lim_{x\to0} f'(x)$ exists in $\mathbb R.$ The question was whether $f'(0)$ exists and is equal to that limit. A posted affirmative answer used L'Hopital's rule, and another used the mean value theorem directly. Either of those relies on the gaplessness of the real line. Can an affirmative answer be proved without completeness?  Could it be proved, for example, in the field of rational numbers? If not, what would be a counterexample in $\mathbb Q$?","A question about existence of derivative of function at Zero The question linked above inspires another question. $f:\mathbb R\to\mathbb R$ is assumed to be continuous everywhere. It is assumed to be differentiable at all points besides $0.$ It is assumed that $\displaystyle\lim_{x\to0} f'(x)$ exists in $\mathbb R.$ The question was whether $f'(0)$ exists and is equal to that limit. A posted affirmative answer used L'Hopital's rule, and another used the mean value theorem directly. Either of those relies on the gaplessness of the real line. Can an affirmative answer be proved without completeness?  Could it be proved, for example, in the field of rational numbers? If not, what would be a counterexample in $\mathbb Q$?",,"['real-analysis', 'derivatives', 'supremum-and-infimum']"
62,proving existence of a series $(x_n)$,proving existence of a series,(x_n),"I'm having trouble with this one, I don't know what is the right path to solve it. $f$ is a differentiable function in $(a,b)$, and let $x_0 \in (a,b)$. prove that there exists a sequence $(x_n)$ so that: $x_n \neq x_0$ for every $n$, $\lim_{n->\infty} x_n=x_0$ and $\lim_{n->\infty} f'(x_n) = f'(x_0)$ A hint I have: for every natural $n$, regard $[x_0,x_0+1/n]$ and use Lagrange's mean value theorem. I tried to use the property and definition of a derivative to prove that $\lim_{n->\infty} f'(x_n) = f'(x_0)$, but couldn't find a way to conclude that $\lim_{n->\infty} x_n=x_0$. I could assume that, but I don't know how to conclude that or how to prove it. Hoping you could help me. Thank you very much!","I'm having trouble with this one, I don't know what is the right path to solve it. $f$ is a differentiable function in $(a,b)$, and let $x_0 \in (a,b)$. prove that there exists a sequence $(x_n)$ so that: $x_n \neq x_0$ for every $n$, $\lim_{n->\infty} x_n=x_0$ and $\lim_{n->\infty} f'(x_n) = f'(x_0)$ A hint I have: for every natural $n$, regard $[x_0,x_0+1/n]$ and use Lagrange's mean value theorem. I tried to use the property and definition of a derivative to prove that $\lim_{n->\infty} f'(x_n) = f'(x_0)$, but couldn't find a way to conclude that $\lim_{n->\infty} x_n=x_0$. I could assume that, but I don't know how to conclude that or how to prove it. Hoping you could help me. Thank you very much!",,"['calculus', 'derivatives', 'limits-without-lhopital']"
63,Show that the derivative of this function is positive,Show that the derivative of this function is positive,,"Suppose that $n>1$, $g\in(0,1)$ and $f(g)\in(0,1)$. Suppose that    $\frac{df(g)}{dg}\geq0$. Define $B(g,n)$ as: $B(g,n)=\sum _{k=1}^n \frac{n!}{k!(n-k)!}g^{n-k}(1-g)^{k-1}(1-(1-f(g))^k)$ Show that: $\frac{dB(g,n)}{dg}>0$. Some of these assumptions can be relaxed (for example the $\frac{df(g)}{dg}\geq0$ I suspect), but I am not especially interested in that. What I have done: I have shown that this is the case for $n=2$, $n=3$ and $n=4$, but did not find any pattern that helped me generalize to $n$. I took the derivative with Mathematica and obtained that little monster: $ \frac{dB(g,n)}{dg}=-\frac{g^n \left(-n \left(\frac{(g-1) f(g)+1}{g}\right)^{n-1} \left(\frac{(g-1) f'(g)+f(g)}{g}-\frac{(g-1) f(g)+1}{g^2}\right)-n \left(\frac{1}{g}\right)^{n+1}\right)}{g-1}-\frac{n g^{n-1} \left(\left(\frac{1}{g}\right)^n-\left(\frac{(g-1) f(g)+1}{g}\right)^n\right)}{g-1}+\frac{g^n \left(\left(\frac{1}{g}\right)^n-\left(\frac{(g-1) f(g)+1}{g}\right)^n\right)}{(g-1)^2}$. (Here I might be wrong) I think that the problem can be slightly simplified by assuming that $\frac{df(g)}{dg}=0$. Because assuming $\frac{df(g)}{dg}>0$ only ""helps us"" in proving the that the derivative is positive, then it suffices to show that our desired result holds when $\frac{df(g)}{dg}=0$. Thanks in advance!","Suppose that $n>1$, $g\in(0,1)$ and $f(g)\in(0,1)$. Suppose that    $\frac{df(g)}{dg}\geq0$. Define $B(g,n)$ as: $B(g,n)=\sum _{k=1}^n \frac{n!}{k!(n-k)!}g^{n-k}(1-g)^{k-1}(1-(1-f(g))^k)$ Show that: $\frac{dB(g,n)}{dg}>0$. Some of these assumptions can be relaxed (for example the $\frac{df(g)}{dg}\geq0$ I suspect), but I am not especially interested in that. What I have done: I have shown that this is the case for $n=2$, $n=3$ and $n=4$, but did not find any pattern that helped me generalize to $n$. I took the derivative with Mathematica and obtained that little monster: $ \frac{dB(g,n)}{dg}=-\frac{g^n \left(-n \left(\frac{(g-1) f(g)+1}{g}\right)^{n-1} \left(\frac{(g-1) f'(g)+f(g)}{g}-\frac{(g-1) f(g)+1}{g^2}\right)-n \left(\frac{1}{g}\right)^{n+1}\right)}{g-1}-\frac{n g^{n-1} \left(\left(\frac{1}{g}\right)^n-\left(\frac{(g-1) f(g)+1}{g}\right)^n\right)}{g-1}+\frac{g^n \left(\left(\frac{1}{g}\right)^n-\left(\frac{(g-1) f(g)+1}{g}\right)^n\right)}{(g-1)^2}$. (Here I might be wrong) I think that the problem can be slightly simplified by assuming that $\frac{df(g)}{dg}=0$. Because assuming $\frac{df(g)}{dg}>0$ only ""helps us"" in proving the that the derivative is positive, then it suffices to show that our desired result holds when $\frac{df(g)}{dg}=0$. Thanks in advance!",,"['real-analysis', 'derivatives', 'binomial-distribution']"
64,Can the Taylor series be used to get polynomial roots?,Can the Taylor series be used to get polynomial roots?,,"I'm using this method: First, write the polynomial in this form: $$a_nx^n+a_{n-1}x^{n-1}+......a_2x^2+a_1x=c$$ Let the LHS of this expression be the function $f(x)$. I'm gonna write the Taylor series of $f^{-1}(x)$ around x=0 and then put $x=c$ in it to get $f^{-1}(c)$ which will be the value of $x$. Since, $f^{-1}(0)=0$ here, so we've got the first term of our Taylor series as $0$. Now, the only thing that remains is calculating the derivatives of $f^{-1}(x)$ at $x=0$. I'm using the fact that $$\frac{d(f^{-1}(x))}{dx}=\frac{1}{f'(f^{-1}(x))}$$ By differentiating this equation, we can get the second derivative of f−1(x) as: $$\frac{d^2(f^{-1}(x))}{dx^2}=-\frac{1}{(f'(f^{-1}(x)))^2}\cdot f''(f^{-1}(x))\cdot (f^{-1})'(x)$$ Similarly, we can get the other derivatives by further differentiation of this equation. Then we can evaluate all the derivatives at x=0 to get the Taylor series of $f^{-1}(x)$ and evaluate it at $x=c$ to get the value of $x$. I don't know the formula of $f^{-1}(x)$ but I know the value of $f^{-1}(x)$ at $x=0$. After doing all the formulas, what I have to do in the end in evaluating that expression at $x=0$ and I've the value of $f^{-1}(x)$ at $x=0$. For example, $$f'^{-1}(x)=\frac{d(f^{-1}(x))}{dx}=\frac{1}{f'(f(^{-1}(x))}$$ $$=\frac{1}{n*a_{n}(f(^{-1}(x)))^{n-1}+(n-1)a_{n-1}(f(^{-1}(x)))^{n-2}+.............2a_2f^{-1}(x)+a_1}$$ which gives $$\frac{1}{a_1}$$ at $x=0$ since $$f^{-1}(0)=0$$ Similarly, $$\frac{d^2(f^{-1}(x))}{dx^2}=-\frac{1}{(f'(f^{-1}(x)))^2}\cdot f''(f^{-1}(x))\cdot (f^{-1})'(x)$$ We already have the value of the first derivative at $x=0$ so we can substitute that here, so $$\frac{d^{2}}{dx^{2}}(f^{-1}(x))=-\frac{1}{a_1^{2}}\cdot 2a_2\cdot \frac{1}{a_1}$$ $$=-\frac{2a_2}{a_1^{3}}$$ I think this process can be continued to get more derivatives by the product rule. 1.Is this method correct? 2.Can something be done to make it better and remove the limitations (if there are any)? UPDATE: If I'm not wrong, then I think this method only works if none of the coefficients of the polynomial are zero. Is there some way to remove that limitation? UPDATE: Oh, I just figured out that we can obtain the Taylor series of the inverse of a polynomial around any point $x=a$ by this method. I also just found out about the Lagrange inversion theorem. It was also about getting Taylor series of inverse functions. I didn't understand much but it was the same series as mine except the coefficients. Are the coefficients also the same? Have I been doing the same thing?","I'm using this method: First, write the polynomial in this form: $$a_nx^n+a_{n-1}x^{n-1}+......a_2x^2+a_1x=c$$ Let the LHS of this expression be the function $f(x)$. I'm gonna write the Taylor series of $f^{-1}(x)$ around x=0 and then put $x=c$ in it to get $f^{-1}(c)$ which will be the value of $x$. Since, $f^{-1}(0)=0$ here, so we've got the first term of our Taylor series as $0$. Now, the only thing that remains is calculating the derivatives of $f^{-1}(x)$ at $x=0$. I'm using the fact that $$\frac{d(f^{-1}(x))}{dx}=\frac{1}{f'(f^{-1}(x))}$$ By differentiating this equation, we can get the second derivative of f−1(x) as: $$\frac{d^2(f^{-1}(x))}{dx^2}=-\frac{1}{(f'(f^{-1}(x)))^2}\cdot f''(f^{-1}(x))\cdot (f^{-1})'(x)$$ Similarly, we can get the other derivatives by further differentiation of this equation. Then we can evaluate all the derivatives at x=0 to get the Taylor series of $f^{-1}(x)$ and evaluate it at $x=c$ to get the value of $x$. I don't know the formula of $f^{-1}(x)$ but I know the value of $f^{-1}(x)$ at $x=0$. After doing all the formulas, what I have to do in the end in evaluating that expression at $x=0$ and I've the value of $f^{-1}(x)$ at $x=0$. For example, $$f'^{-1}(x)=\frac{d(f^{-1}(x))}{dx}=\frac{1}{f'(f(^{-1}(x))}$$ $$=\frac{1}{n*a_{n}(f(^{-1}(x)))^{n-1}+(n-1)a_{n-1}(f(^{-1}(x)))^{n-2}+.............2a_2f^{-1}(x)+a_1}$$ which gives $$\frac{1}{a_1}$$ at $x=0$ since $$f^{-1}(0)=0$$ Similarly, $$\frac{d^2(f^{-1}(x))}{dx^2}=-\frac{1}{(f'(f^{-1}(x)))^2}\cdot f''(f^{-1}(x))\cdot (f^{-1})'(x)$$ We already have the value of the first derivative at $x=0$ so we can substitute that here, so $$\frac{d^{2}}{dx^{2}}(f^{-1}(x))=-\frac{1}{a_1^{2}}\cdot 2a_2\cdot \frac{1}{a_1}$$ $$=-\frac{2a_2}{a_1^{3}}$$ I think this process can be continued to get more derivatives by the product rule. 1.Is this method correct? 2.Can something be done to make it better and remove the limitations (if there are any)? UPDATE: If I'm not wrong, then I think this method only works if none of the coefficients of the polynomial are zero. Is there some way to remove that limitation? UPDATE: Oh, I just figured out that we can obtain the Taylor series of the inverse of a polynomial around any point $x=a$ by this method. I also just found out about the Lagrange inversion theorem. It was also about getting Taylor series of inverse functions. I didn't understand much but it was the same series as mine except the coefficients. Are the coefficients also the same? Have I been doing the same thing?",,"['derivatives', 'polynomials']"
65,Nowhere differentiable Fourier transform,Nowhere differentiable Fourier transform,,Is there any $L_1$ function such that its fourier transform is in $L_1$ and the fourier transform is nowhere differentiable? Or every such Fourier transform must be differentiable a.e.?,Is there any $L_1$ function such that its fourier transform is in $L_1$ and the fourier transform is nowhere differentiable? Or every such Fourier transform must be differentiable a.e.?,,"['derivatives', 'fourier-analysis', 'harmonic-analysis']"
66,Finding a derivative using multiple chain rules,Finding a derivative using multiple chain rules,,"Find the derivative of the function. $y = [x + (x + \sin^2 x)^3]^4$ I know how to use the chain rule and I found the derivative to be: $$4[x+(x+\sin^2(x))^3]^3 \cdot (1 + 3(x + \sin^2(x))^2) \cdot (1+\sin (2x))$$ but my online homework says that this is wrong. I can't figure what what I've done wrong and I've tried it several times now. Can somebody help? Note: In the last term, I simplified $2\sin x\cos x$ to be $\sin(2x)$. I tried inputting both versions into my homework but it was wrong both ways.","Find the derivative of the function. $y = [x + (x + \sin^2 x)^3]^4$ I know how to use the chain rule and I found the derivative to be: $$4[x+(x+\sin^2(x))^3]^3 \cdot (1 + 3(x + \sin^2(x))^2) \cdot (1+\sin (2x))$$ but my online homework says that this is wrong. I can't figure what what I've done wrong and I've tried it several times now. Can somebody help? Note: In the last term, I simplified $2\sin x\cos x$ to be $\sin(2x)$. I tried inputting both versions into my homework but it was wrong both ways.",,"['calculus', 'derivatives', 'chain-rule']"
67,Is it possible for a continuous function to have a nowhere-continuous derivative? [duplicate],Is it possible for a continuous function to have a nowhere-continuous derivative? [duplicate],,This question already has answers here : Discontinuous derivative. [duplicate] (2 answers) How discontinuous can a derivative be? (1 answer) Closed 7 years ago . This is motivated by a question I saw elsewhere that asks whether there is a real-valued function on an interval that contains no monotone subintervals. Edit: Note that I am asking for a function whose derivative exists but is not continuous anywhere.,This question already has answers here : Discontinuous derivative. [duplicate] (2 answers) How discontinuous can a derivative be? (1 answer) Closed 7 years ago . This is motivated by a question I saw elsewhere that asks whether there is a real-valued function on an interval that contains no monotone subintervals. Edit: Note that I am asking for a function whose derivative exists but is not continuous anywhere.,,"['calculus', 'real-analysis', 'derivatives', 'continuity', 'examples-counterexamples']"
68,Convex functions converge uniformly. Then their derivatives too?,Convex functions converge uniformly. Then their derivatives too?,,"The following seems true to me: Question) Let $D$ be an open convex domain in $R^d$, and let $(f_n)$ be a sequence of convex functions which converge uniformly to $f$ in $D$. Then does $(\nabla f_n)$ uniformly converge to $\nabla f$ on any compact subset of $D$? Since functions might not be differentiable, then this means that  the distance of the set of subdifferentials converge to zero uniformly, that is, $\sup_{x \in K}$ dist$(\partial f_n (x), \partial f(x))$ goes to zero, where $K$ is a compact set in $D$ and $\partial f(x)$ is the subdifferential set of $f$ at $x$. (But you may just assume that everything is differentiable.) Is this true, and if so, how to prove it? As the counterexamples in the answers indicate, the statement above is wrong. But I think the following weaker statement is true: Question) Let $D$ be an open convex domain in $R^d$, and let $(f_n)$ be a sequence of convex functions which converge uniformly to $f$ in $D$. Then does $(\nabla f_n)$ converge pointwise to $\nabla f$ in $D$? (again in the sense that dist$(\partial f_n (x), \partial f(x)) \to 0$ for each $x \in D$) And in fact, I think that even pointwise convergence of $f_n$ to $f$ can still imply the pointwise convergence of the derivatives. Is this true, and if so, how to prove it?","The following seems true to me: Question) Let $D$ be an open convex domain in $R^d$, and let $(f_n)$ be a sequence of convex functions which converge uniformly to $f$ in $D$. Then does $(\nabla f_n)$ uniformly converge to $\nabla f$ on any compact subset of $D$? Since functions might not be differentiable, then this means that  the distance of the set of subdifferentials converge to zero uniformly, that is, $\sup_{x \in K}$ dist$(\partial f_n (x), \partial f(x))$ goes to zero, where $K$ is a compact set in $D$ and $\partial f(x)$ is the subdifferential set of $f$ at $x$. (But you may just assume that everything is differentiable.) Is this true, and if so, how to prove it? As the counterexamples in the answers indicate, the statement above is wrong. But I think the following weaker statement is true: Question) Let $D$ be an open convex domain in $R^d$, and let $(f_n)$ be a sequence of convex functions which converge uniformly to $f$ in $D$. Then does $(\nabla f_n)$ converge pointwise to $\nabla f$ in $D$? (again in the sense that dist$(\partial f_n (x), \partial f(x)) \to 0$ for each $x \in D$) And in fact, I think that even pointwise convergence of $f_n$ to $f$ can still imply the pointwise convergence of the derivatives. Is this true, and if so, how to prove it?",,"['derivatives', 'convex-analysis', 'uniform-convergence']"
69,"Dynamical systems, causality and derivatives order","Dynamical systems, causality and derivatives order",,"Talking about input/output representation of a dynamical system, the professor said that the equation(s) involved must satisfy this condition in order for the system to be qualified as ""causal"": the greatest derivatives order of the output should be lower than the greatest derivatives order of the input He explained the fact saying that if we dismiss the condition, we could imagine a system described by $$y(t) = \dot{u}(t)$$ that is  not feasible in our physical world where actions produce effects. He said ""if we knew the derivative of input, we'd know the future"". At first, I can't fully understand this assertion. If it is true, then why does the very knowledge of u(t) not imply the knowledge of the future? I think this misunderstanding is due to the fact that I have just started studying system, so maybe i'm not in the correct perspective. However there's another problem yet: in the quoted condition there is not written "" derivatives of input are not allowed"". They can appear, given that their max order is lower than (..). Why this constraint implies causality? Finally, I'd be very grateful if you could point out main concepts that link together systems, derivatives, future. Thanks a lot","Talking about input/output representation of a dynamical system, the professor said that the equation(s) involved must satisfy this condition in order for the system to be qualified as ""causal"": the greatest derivatives order of the output should be lower than the greatest derivatives order of the input He explained the fact saying that if we dismiss the condition, we could imagine a system described by $$y(t) = \dot{u}(t)$$ that is  not feasible in our physical world where actions produce effects. He said ""if we knew the derivative of input, we'd know the future"". At first, I can't fully understand this assertion. If it is true, then why does the very knowledge of u(t) not imply the knowledge of the future? I think this misunderstanding is due to the fact that I have just started studying system, so maybe i'm not in the correct perspective. However there's another problem yet: in the quoted condition there is not written "" derivatives of input are not allowed"". They can appear, given that their max order is lower than (..). Why this constraint implies causality? Finally, I'd be very grateful if you could point out main concepts that link together systems, derivatives, future. Thanks a lot",,"['derivatives', 'dynamical-systems', 'mathematical-modeling', 'systems-theory']"
70,Find $f$ if $f'(x)=\dfrac{x^2-1}{x}$ knowing that $f(1) = \dfrac{1}{2}$ and $f(-1) = 0$,Find  if  knowing that  and,f f'(x)=\dfrac{x^2-1}{x} f(1) = \dfrac{1}{2} f(-1) = 0,"I am asked the following problem: Find $f$ if $$f'(x)=\frac{x^2-1}{x}$$ I am not sure about my solution, which I will describe below: My solution: The first thing that I've done is separate the terms of $f'(x)$ \begin{align*} f'(x)&=\frac{x^2-1}{x}\\ &=x-\frac{1}{x}\\ \therefore  \quad f(x)&=\frac{x^2}{2}-\ln|x|+c \end{align*} For ( x > 0 ): \begin{align*} f(x)&=\frac{x^2}{2}-\ln x+c\\ f(1)&=\frac{1^2}{2}-\ln 1+c=\frac{1}{2} \quad \Rightarrow \quad c=0\\ f(x)&=\frac{x^2}{2}-\ln |x| \end{align*} For ( x < 0 ) \begin{align*} f(x)&=\frac{x^2}{2}-\ln (-x)+c\\ f(-1)&=\frac{(-1)^2}{2}-\ln [-(-1)]+c=0 \quad \Rightarrow \quad c=-\frac{1}{2}\\ f(x)&=\frac{x^2}{2}-\ln |x|-\frac{1}{2} \end{align*} Is my solution correct? Should I really find two different answers, one for $x > 0$ and another for $x < 0$? Thank you.","I am asked the following problem: Find $f$ if $$f'(x)=\frac{x^2-1}{x}$$ I am not sure about my solution, which I will describe below: My solution: The first thing that I've done is separate the terms of $f'(x)$ \begin{align*} f'(x)&=\frac{x^2-1}{x}\\ &=x-\frac{1}{x}\\ \therefore  \quad f(x)&=\frac{x^2}{2}-\ln|x|+c \end{align*} For ( x > 0 ): \begin{align*} f(x)&=\frac{x^2}{2}-\ln x+c\\ f(1)&=\frac{1^2}{2}-\ln 1+c=\frac{1}{2} \quad \Rightarrow \quad c=0\\ f(x)&=\frac{x^2}{2}-\ln |x| \end{align*} For ( x < 0 ) \begin{align*} f(x)&=\frac{x^2}{2}-\ln (-x)+c\\ f(-1)&=\frac{(-1)^2}{2}-\ln [-(-1)]+c=0 \quad \Rightarrow \quad c=-\frac{1}{2}\\ f(x)&=\frac{x^2}{2}-\ln |x|-\frac{1}{2} \end{align*} Is my solution correct? Should I really find two different answers, one for $x > 0$ and another for $x < 0$? Thank you.",,"['calculus', 'integration', 'derivatives', 'indefinite-integrals']"
71,"If $f^{(n)}(x)$ continous at a certain point ,then $f^{(n)}(x)$ continous over a neighbourhood of the point?","If  continous at a certain point ,then  continous over a neighbourhood of the point?",f^{(n)}(x) f^{(n)}(x),"Let $n$ is a non-negative integer,$f$ diferentiable $n$ times in a neighbourhood of $x_{0}\in \mathbf{R},(x_{0}-\delta ,x_{0}+\delta ),$ and  $f^{(n)}(x),$ the $n$th derivative of $f$ ,is continuous at $x=x_{0}.$ Is there a real number $\delta_{1},0<\delta_{1}<\delta,$ such that $f^{(n)}(x)$ is continous over $(x_{0}-\delta_{1} ,x_{0}+\delta_{1} )?$ When $n=0,$it is  not difficulty to find  a counterexample: Let $$x_{0}=0,\qquad f(x)=\begin{cases}  x^{2}& \text{if}\quad x \text { is a irratioanl number},\\   0& \text{if}\quad x \text { is a rational number}.  \end{cases} \quad $$ Go a step further，I need to find some counterexamples for higher orders, but I'm stumped for that .Can anyone give me any hints on how to start it?  Any help will be appreciated.","Let $n$ is a non-negative integer,$f$ diferentiable $n$ times in a neighbourhood of $x_{0}\in \mathbf{R},(x_{0}-\delta ,x_{0}+\delta ),$ and  $f^{(n)}(x),$ the $n$th derivative of $f$ ,is continuous at $x=x_{0}.$ Is there a real number $\delta_{1},0<\delta_{1}<\delta,$ such that $f^{(n)}(x)$ is continous over $(x_{0}-\delta_{1} ,x_{0}+\delta_{1} )?$ When $n=0,$it is  not difficulty to find  a counterexample: Let $$x_{0}=0,\qquad f(x)=\begin{cases}  x^{2}& \text{if}\quad x \text { is a irratioanl number},\\   0& \text{if}\quad x \text { is a rational number}.  \end{cases} \quad $$ Go a step further，I need to find some counterexamples for higher orders, but I'm stumped for that .Can anyone give me any hints on how to start it?  Any help will be appreciated.",,"['calculus', 'real-analysis', 'derivatives']"
72,Defining derivatives and integrals for hyperoperations > 2,Defining derivatives and integrals for hyperoperations > 2,,"Derivatives and Integrals are continuous generalizations of the Forward Difference and Summation additive operators respectively. We can do the same with multiplication and get multiplicative calculus with the geometric derivative (continuous forward ratio) and the product integral (continuous indefinite product). I propose a bigger generalization for all hyperoperations > 2. First we define the n-th Hyperoperation as $$H_{n}(a,b)=\begin{cases}b+1&\text{if }n=0\\a&\text{if }n=1\text{ and }b=0\\0&\text{if }n=2\text{ and }b=0\\1&\text{if }n\geq3\text{ and }b=0\\H_{n-1}(a,H_{n}(a,b-1)) &\text{otherwise}\end{cases}$$ $H_{-n}(a,b)$ being the corresponding inverse function (substarction, division, rooting, etc.) and $_{n}\mathrm{H}^{c}_{i=b}\,a_{i}$ the recursive use of the n-th hyperoperation over $a_{i}$. Using these concepts, we then define the n-th ''Hyperderivative'' operator as $$_{n}D_{x}\,f(x)=\lim_{h\to{0}}H_{-n+1}(H_{-n}(f(x+h),f(x)),h)$$ and the n-th ''Hyperintegral'' operator as $$_{n}J_{x}\,f(x)=\lim_{\Delta{x}\to{0}}H_{n+1}(_{n}\mathrm{H}_{i}\,f(x_{i}),\Delta{x})$$ Could this be used as a good definition? In which areas of maths would these continuous hyperoperations be useful? I appreciate any feedback.","Derivatives and Integrals are continuous generalizations of the Forward Difference and Summation additive operators respectively. We can do the same with multiplication and get multiplicative calculus with the geometric derivative (continuous forward ratio) and the product integral (continuous indefinite product). I propose a bigger generalization for all hyperoperations > 2. First we define the n-th Hyperoperation as $$H_{n}(a,b)=\begin{cases}b+1&\text{if }n=0\\a&\text{if }n=1\text{ and }b=0\\0&\text{if }n=2\text{ and }b=0\\1&\text{if }n\geq3\text{ and }b=0\\H_{n-1}(a,H_{n}(a,b-1)) &\text{otherwise}\end{cases}$$ $H_{-n}(a,b)$ being the corresponding inverse function (substarction, division, rooting, etc.) and $_{n}\mathrm{H}^{c}_{i=b}\,a_{i}$ the recursive use of the n-th hyperoperation over $a_{i}$. Using these concepts, we then define the n-th ''Hyperderivative'' operator as $$_{n}D_{x}\,f(x)=\lim_{h\to{0}}H_{-n+1}(H_{-n}(f(x+h),f(x)),h)$$ and the n-th ''Hyperintegral'' operator as $$_{n}J_{x}\,f(x)=\lim_{\Delta{x}\to{0}}H_{n+1}(_{n}\mathrm{H}_{i}\,f(x_{i}),\Delta{x})$$ Could this be used as a good definition? In which areas of maths would these continuous hyperoperations be useful? I appreciate any feedback.",,"['calculus', 'integration', 'derivatives']"
73,Statements about derivatives and integrals [closed],Statements about derivatives and integrals [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question My professor gave me one example. It's given one intervall  $I=\left [ a,b \right ]\subset \mathbb{R}$ and one function $f:I\mapsto \mathbb{R}$. There is also given 8 statements about derivatives and integrals, so here we go: A: f is Riemann integrable. B: For all $c\in I$ applies that $\lim_{x\mapsto c}f(x)=f(c)$. C: For all $c\in I$ applies that $f(x)= \sum_{k=0}^{\infty}a_kx^k$. D: $f$ is for I five times continuously differentiable. E: $f$ is continues for I. F: $f(x)=\int_{a}^{x}g(s)ds$ with one continuous function $g:\left [ a,b \right ] \mapsto \mathbb{R}$. G: $f$ is bounded. H: $f$ is for $I$ continuously differentiable. I had to sort by strength. Answer is: $C \implies D  \implies H \Leftrightarrow F\implies E \Leftrightarrow B \implies A\implies G$ Can somone explain my why is like this? I understand why everystatment is true, but I don't understand how to sort them by strength. I only understand why $ D  \implies H$ and that's it. Can someone explain me this?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question My professor gave me one example. It's given one intervall  $I=\left [ a,b \right ]\subset \mathbb{R}$ and one function $f:I\mapsto \mathbb{R}$. There is also given 8 statements about derivatives and integrals, so here we go: A: f is Riemann integrable. B: For all $c\in I$ applies that $\lim_{x\mapsto c}f(x)=f(c)$. C: For all $c\in I$ applies that $f(x)= \sum_{k=0}^{\infty}a_kx^k$. D: $f$ is for I five times continuously differentiable. E: $f$ is continues for I. F: $f(x)=\int_{a}^{x}g(s)ds$ with one continuous function $g:\left [ a,b \right ] \mapsto \mathbb{R}$. G: $f$ is bounded. H: $f$ is for $I$ continuously differentiable. I had to sort by strength. Answer is: $C \implies D  \implies H \Leftrightarrow F\implies E \Leftrightarrow B \implies A\implies G$ Can somone explain my why is like this? I understand why everystatment is true, but I don't understand how to sort them by strength. I only understand why $ D  \implies H$ and that's it. Can someone explain me this?",,"['integration', 'derivatives']"
74,Successive differentiation of an implicit function,Successive differentiation of an implicit function,,"If $y^{1/m}+y^{-1/m}=2x$, prove that $$(x^2-1)y'''+3xy''+(1-m^2)y'=0$$ I used brute force(kept on applying Product and Quotient rule) and after almost 3 pages of nasty calculations was able to get to the result. I am looking for a faster and better approach. Thanks.","If $y^{1/m}+y^{-1/m}=2x$, prove that $$(x^2-1)y'''+3xy''+(1-m^2)y'=0$$ I used brute force(kept on applying Product and Quotient rule) and after almost 3 pages of nasty calculations was able to get to the result. I am looking for a faster and better approach. Thanks.",,['calculus']
75,Parallel curve to a sine wave,Parallel curve to a sine wave,,"I've been trying to find the formula for the offset/parallel to a sine wave.  Not just the parametric equation, but the y = f(x) form. Here's what I've done so far: Read up on the parametric form and plugged in the x(t) and y(t) formulas. What I get is of course a parametric equation in terms of t. If $$ y =  \sin x $$ then the parameterization would be $$ x = t $$ $$ y = \sin  t $$ Plugging in the offset formula: $$ x_d(t)  = t + \frac{d\cdot\cos t}{\sqrt{1 + \cos^2 t}} $$ $$ y_d(t)  = \sin t  - \frac{d}{\sqrt {1 + \cos^2 t}} $$ Now, that's all accurate, but it doesn't put it into a function form.  According to my calculus book, the next step is to solve each of these for t and then set them equal to one another.  The problem is that they are kind of a mess, with those sinusoidal functions involved. My question is:  What's the y =  f(x) form for an offset curve of a sine wave? A little background:  I need this because I'm trying to find the intersection point when 3 offsets of three $\pi\over3$ -out-of-phase to each other sine waves intersect.  Basically where the green, blue and red intersect at the same time in the link below.  I can find it numerically, but I'd like it exactly because it's something of discovery to find out how the ancient people drew braids using just compass and straight edges. I can draw it no problem in C#: The intersection point was found using trial and error and is approximately 0.63. There are two blue lines, two red lines and two green lines, because I used +0.63 offset and -0.63 offset from the sine wave. Thank you in advance for any help.","I've been trying to find the formula for the offset/parallel to a sine wave.  Not just the parametric equation, but the y = f(x) form. Here's what I've done so far: Read up on the parametric form and plugged in the x(t) and y(t) formulas. What I get is of course a parametric equation in terms of t. If then the parameterization would be Plugging in the offset formula: Now, that's all accurate, but it doesn't put it into a function form.  According to my calculus book, the next step is to solve each of these for t and then set them equal to one another.  The problem is that they are kind of a mess, with those sinusoidal functions involved. My question is:  What's the y =  f(x) form for an offset curve of a sine wave? A little background:  I need this because I'm trying to find the intersection point when 3 offsets of three -out-of-phase to each other sine waves intersect.  Basically where the green, blue and red intersect at the same time in the link below.  I can find it numerically, but I'd like it exactly because it's something of discovery to find out how the ancient people drew braids using just compass and straight edges. I can draw it no problem in C#: The intersection point was found using trial and error and is approximately 0.63. There are two blue lines, two red lines and two green lines, because I used +0.63 offset and -0.63 offset from the sine wave. Thank you in advance for any help.", y =  \sin x   x = t   y = \sin  t   x_d(t)  = t + \frac{d\cdot\cos t}{\sqrt{1 + \cos^2 t}}   y_d(t)  = \sin t  - \frac{d}{\sqrt {1 + \cos^2 t}}  \pi\over3,"['calculus', 'derivatives']"
76,why $|x|$ in $\frac{d}{dx}\sec^{-1}x=\frac{1}{|x|\sqrt{x^2-1}}$,why  in,|x| \frac{d}{dx}\sec^{-1}x=\frac{1}{|x|\sqrt{x^2-1}},"I derived $$\frac{d}{dx}\sec^{-1}x$$ as follows: Let $$z=\sec^{-1}x$$ Then $$x=\sec z$$ differentiating both sides w.r.t $x$ we get $$1=\sec z \tan z \frac{dz}{dx}$$ so we get $$1=x\sqrt{x^2-1}\frac{dz}{dx}$$ so $$\frac{dz}{dx}=\frac{1}{x\sqrt{x^2-1}}$$ But why we need to introduce modulus to $x$, is it because slope of the tangent should be unique?","I derived $$\frac{d}{dx}\sec^{-1}x$$ as follows: Let $$z=\sec^{-1}x$$ Then $$x=\sec z$$ differentiating both sides w.r.t $x$ we get $$1=\sec z \tan z \frac{dz}{dx}$$ so we get $$1=x\sqrt{x^2-1}\frac{dz}{dx}$$ so $$\frac{dz}{dx}=\frac{1}{x\sqrt{x^2-1}}$$ But why we need to introduce modulus to $x$, is it because slope of the tangent should be unique?",,"['calculus', 'derivatives']"
77,"If the second derivative of a function is zero, why is the second derivative test inconclusive?","If the second derivative of a function is zero, why is the second derivative test inconclusive?",,"2nd derivative test gives three possibilities: 1) greater than zero (strict local min) 2) less than zero (strict local max) 3) equal to zero - no information It is this third case that I do not understand. If the second derivative at a stationary point is zero, doesn't that mean it is a stationary inflection point? I understand that another criteria for an inflection point is that it must change signs. However, I cannot think of an example where the first derivative is zero and the second derivative is zero, but the graph does not change signs. There is some information about undulation points available, but the descriptions are fairly vague: What is the difference between an undulation point and other critical values? I understand why the second derivative is zero at an inflection point: Why is the second derivative of an inflection point zero? I understand why the second derivative must change sign at an inflection point (the concavity/convexity changes).","2nd derivative test gives three possibilities: 1) greater than zero (strict local min) 2) less than zero (strict local max) 3) equal to zero - no information It is this third case that I do not understand. If the second derivative at a stationary point is zero, doesn't that mean it is a stationary inflection point? I understand that another criteria for an inflection point is that it must change signs. However, I cannot think of an example where the first derivative is zero and the second derivative is zero, but the graph does not change signs. There is some information about undulation points available, but the descriptions are fairly vague: What is the difference between an undulation point and other critical values? I understand why the second derivative is zero at an inflection point: Why is the second derivative of an inflection point zero? I understand why the second derivative must change sign at an inflection point (the concavity/convexity changes).",,['derivatives']
78,dx(t)/dx vs. dx/dx,dx(t)/dx vs. dx/dx,,"According to symbolic MATLAB and WolframAlpha, $\frac{\partial x(t)}{\partial x}  = 0, \frac{\partial x}{\partial x} = 1$ I came across this while trying to figure out how to do: $\frac {\partial} {\partial x} \int \dot x dt$ How can this be?","According to symbolic MATLAB and WolframAlpha, $\frac{\partial x(t)}{\partial x}  = 0, \frac{\partial x}{\partial x} = 1$ I came across this while trying to figure out how to do: $\frac {\partial} {\partial x} \int \dot x dt$ How can this be?",,"['calculus', 'derivatives']"
79,The time derivative of $ \|P(t)\|_{F}^{2} + \|\hat{x}(t)\|^2 $ is not well defined ?,The time derivative of  is not well defined ?, \|P(t)\|_{F}^{2} + \|\hat{x}(t)\|^2 ,"$\|.\|_{F}$ denotes the matrix frobenius norm. Both $P \in \mathbb{R}^{N X N} $ and $\hat{x} \in \mathbb{R}^{N}$ are functions of time. When I evaluate the derivative with respect to time , $\dfrac{\partial\|P\|_{F}^{2}}{\partial t} \in \mathbb{R}^{NXN}$ whereas $\dfrac{\partial \|\hat x\|^2}{\partial t} \in \mathbb{R}$ which means they can no longer be added directly !  I realize that matrix derivatives don't follow the chain rule, but since both functions output a scalar and the independent variable is a scalar too, I feel there should be a way to solve this. Am I missing something fundamental here ? Does this have anything to do with Tensors ? A Little background : I am trying to evaluate the gradient of a optimization cost on the covariance matrix $P$ and on the state estimate $\hat{x}$. (I have ignored the integral in the question for simplicity).","$\|.\|_{F}$ denotes the matrix frobenius norm. Both $P \in \mathbb{R}^{N X N} $ and $\hat{x} \in \mathbb{R}^{N}$ are functions of time. When I evaluate the derivative with respect to time , $\dfrac{\partial\|P\|_{F}^{2}}{\partial t} \in \mathbb{R}^{NXN}$ whereas $\dfrac{\partial \|\hat x\|^2}{\partial t} \in \mathbb{R}$ which means they can no longer be added directly !  I realize that matrix derivatives don't follow the chain rule, but since both functions output a scalar and the independent variable is a scalar too, I feel there should be a way to solve this. Am I missing something fundamental here ? Does this have anything to do with Tensors ? A Little background : I am trying to evaluate the gradient of a optimization cost on the covariance matrix $P$ and on the state estimate $\hat{x}$. (I have ignored the integral in the question for simplicity).",,"['linear-algebra', 'derivatives', 'tensor-products']"
80,The Derivative of polynomial function,The Derivative of polynomial function,,"Let $f$ be a generic polynomial function, defined by $$f(x) = a_nx^n + a_{n - 1}x^{n - 1} + a_{n - 2}x^{n - 2} + \dots + a_1x + a_0$$ ,$n \in \mathbb{N}$ If I define $f(x)$ using sigma notation, so I'll get: $$f(x) = \sum_{k =0}^{n} a_{n -k}x^{n - k}$$ But I want to calculate the derivative of this function, can i do this? $$\frac{d}{dx} f(x) = \frac{d}{dx} \sum_{k =0}^{n} a_{n -k}x^{n - k} = \sum_{k =0}^{n}(n - k) a_{n -k}x^{n - k -1}$$ I think this it's just saying that the derivative of a polynomial function i'ts given by the derivatives of the terms of the polynomial. Am I thinking correctly?","Let $f$ be a generic polynomial function, defined by $$f(x) = a_nx^n + a_{n - 1}x^{n - 1} + a_{n - 2}x^{n - 2} + \dots + a_1x + a_0$$ ,$n \in \mathbb{N}$ If I define $f(x)$ using sigma notation, so I'll get: $$f(x) = \sum_{k =0}^{n} a_{n -k}x^{n - k}$$ But I want to calculate the derivative of this function, can i do this? $$\frac{d}{dx} f(x) = \frac{d}{dx} \sum_{k =0}^{n} a_{n -k}x^{n - k} = \sum_{k =0}^{n}(n - k) a_{n -k}x^{n - k -1}$$ I think this it's just saying that the derivative of a polynomial function i'ts given by the derivatives of the terms of the polynomial. Am I thinking correctly?",,"['calculus', 'derivatives']"
81,Implicit Differentiation. Please help me understand why!,Implicit Differentiation. Please help me understand why!,,"I am trying to understand implicit differentiation; I understand what to do (that is no problem), but why I do it is another story. For example: $$3y^2=5x^3 $$ I understand that, if I take the derivative with respect to x of both sides of the equation, I'll get: $$\frac{d}{dx}(3y^2)=\frac{d}{dx}(5x^3)$$ $$6y\frac{d}{dx}(y)=15x^2\frac{d}{dx}(x)$$ $$6y\frac{dy}{dx}=15x^2\frac{dx}{dx}$$ $$6y\frac{dy}{dx}=15x^2$$ $$\frac{dy}{dx}=\frac{15x^2}{6y}$$ Unless I made some sort of error, this is what I am suppose to do. But why? Specifically, on the second line, I utilize the chain rule for the ""outer function"" and get 6y, but I still need to utilize the chain rule for the ""inner function"" which is the y. So why don't I go ahead and take the derivative of y and get 1? I know that I am not suppose to, but I don't really ""get it."" It seems to me that I only use the chain rule ""halfway"". Why isn't it an all or nothing? If it's all done with respect to x, it would seem to me that the 3y^2 should remain unchanged entirely. This is my problem. And I apologize if I got some of the terminology wrong.","I am trying to understand implicit differentiation; I understand what to do (that is no problem), but why I do it is another story. For example: $$3y^2=5x^3 $$ I understand that, if I take the derivative with respect to x of both sides of the equation, I'll get: $$\frac{d}{dx}(3y^2)=\frac{d}{dx}(5x^3)$$ $$6y\frac{d}{dx}(y)=15x^2\frac{d}{dx}(x)$$ $$6y\frac{dy}{dx}=15x^2\frac{dx}{dx}$$ $$6y\frac{dy}{dx}=15x^2$$ $$\frac{dy}{dx}=\frac{15x^2}{6y}$$ Unless I made some sort of error, this is what I am suppose to do. But why? Specifically, on the second line, I utilize the chain rule for the ""outer function"" and get 6y, but I still need to utilize the chain rule for the ""inner function"" which is the y. So why don't I go ahead and take the derivative of y and get 1? I know that I am not suppose to, but I don't really ""get it."" It seems to me that I only use the chain rule ""halfway"". Why isn't it an all or nothing? If it's all done with respect to x, it would seem to me that the 3y^2 should remain unchanged entirely. This is my problem. And I apologize if I got some of the terminology wrong.",,"['calculus', 'derivatives', 'implicit-differentiation']"
82,Derivative by Definition of $\frac{\sin^2(x)}{e^x-1}$,Derivative by Definition of,\frac{\sin^2(x)}{e^x-1},I have to prove the derivative by definition of $$\frac{\sin^2(x)}{e^x-1}$$ $$f^{\prime}(x)=\lim_{\Delta x \to 0}{\frac{f(x+\Delta x)-f(x)}{\Delta x}}$$ $$\large f^{\prime}(x)=\lim_{\Delta x \to 0}{\frac{\frac{\sin^2(x+\Delta x)}{e^{(x+\Delta x)}-1}-\frac{\sin^2(x)}{e^x-1}}{\Delta x}}$$ I tried to rid of ${e^x-1}$ in different ways but with no luck. Actually the problem is the limit of the last expression.,I have to prove the derivative by definition of $$\frac{\sin^2(x)}{e^x-1}$$ $$f^{\prime}(x)=\lim_{\Delta x \to 0}{\frac{f(x+\Delta x)-f(x)}{\Delta x}}$$ $$\large f^{\prime}(x)=\lim_{\Delta x \to 0}{\frac{\frac{\sin^2(x+\Delta x)}{e^{(x+\Delta x)}-1}-\frac{\sin^2(x)}{e^x-1}}{\Delta x}}$$ I tried to rid of ${e^x-1}$ in different ways but with no luck. Actually the problem is the limit of the last expression.,,['derivatives']
83,Circular definition of tangent line and derivative,Circular definition of tangent line and derivative,,"I'm trying to understand the deep relations between the tangent line to the graph of a function $f$ at a given point $P$, and the derivative of $f$ at the same point. Indeed, in many books the derivative is often defined as the slope of the ""tangent"", with only an intuitive definition of what a tangent line is. Then, once the concept of derivative is well assimilated, books define more precisely the tangent as... the line passing through $P = (a, f(a))$ with slope $f'(a)$ ! This seems a bit circular. So some authors use an other approach, starting by giving a geometric definition of a tangent, and then showing the connection with derivatives. That's what one can read in the paper "" What a tangent line is when it isn't a limit "", by Irl Bivens. But I am missing something in the proof of Theorem 1, when the author writes: it suffices to show that $|f(x)-L(x)|\leq (1/2)|L(x)-K(x)|$ where $f$ is a differentiable function, $L$ is the line of equation $L(x) = f'(a)(x-a)+f(a)$ and $K$ an other line, of equation $K(x)=p(x-a)+f(a)$ with $p \neq f'(a)$. So, basically, the goal is to show that $L$ is a better approximation of $f$ than any $K$. But where does the right-hand side come from ? It looks like, locally, $f(x) = 1/2 (L(x)+K(x))$, but why?","I'm trying to understand the deep relations between the tangent line to the graph of a function $f$ at a given point $P$, and the derivative of $f$ at the same point. Indeed, in many books the derivative is often defined as the slope of the ""tangent"", with only an intuitive definition of what a tangent line is. Then, once the concept of derivative is well assimilated, books define more precisely the tangent as... the line passing through $P = (a, f(a))$ with slope $f'(a)$ ! This seems a bit circular. So some authors use an other approach, starting by giving a geometric definition of a tangent, and then showing the connection with derivatives. That's what one can read in the paper "" What a tangent line is when it isn't a limit "", by Irl Bivens. But I am missing something in the proof of Theorem 1, when the author writes: it suffices to show that $|f(x)-L(x)|\leq (1/2)|L(x)-K(x)|$ where $f$ is a differentiable function, $L$ is the line of equation $L(x) = f'(a)(x-a)+f(a)$ and $K$ an other line, of equation $K(x)=p(x-a)+f(a)$ with $p \neq f'(a)$. So, basically, the goal is to show that $L$ is a better approximation of $f$ than any $K$. But where does the right-hand side come from ? It looks like, locally, $f(x) = 1/2 (L(x)+K(x))$, but why?",,"['calculus', 'real-analysis', 'derivatives']"
84,40th derivative of a function,40th derivative of a function,,"I would like to have some verification to see if my answer is correct. The given function is $f(x)=ln(1+x^2)$ and I need the 40th derivative at $x=0$. Here is my work: Using series one can manipulate $\frac{1}{1+x}=1-x+x^2-x^3+x^4...=SUMx^n(-1)^n$ into $\frac{1}{1+x^2}=1-x^2+x^4-x^6+x^8...=SUMx^{2n}(-1)^n$. Then $\frac{2x}{1+x}=2x-2x^3+2x^5-2x^7+2x^9...=2SUMx^{2n+1}(-1)^n$. Integrating gives $ln(1+x^2)=...=2SUM\frac{x^{2n+2}(-1)^n}{2n+2}$ where a $2$ cancels to arrive at a $n+1$ in that denominator. Now for the 40th derivative, $2n+2=40$ gives $n=19$ and thus I believe the answer is $\frac{-40!}{20}$ Do you concur? If not could you correct me? Thanks.","I would like to have some verification to see if my answer is correct. The given function is $f(x)=ln(1+x^2)$ and I need the 40th derivative at $x=0$. Here is my work: Using series one can manipulate $\frac{1}{1+x}=1-x+x^2-x^3+x^4...=SUMx^n(-1)^n$ into $\frac{1}{1+x^2}=1-x^2+x^4-x^6+x^8...=SUMx^{2n}(-1)^n$. Then $\frac{2x}{1+x}=2x-2x^3+2x^5-2x^7+2x^9...=2SUMx^{2n+1}(-1)^n$. Integrating gives $ln(1+x^2)=...=2SUM\frac{x^{2n+2}(-1)^n}{2n+2}$ where a $2$ cancels to arrive at a $n+1$ in that denominator. Now for the 40th derivative, $2n+2=40$ gives $n=19$ and thus I believe the answer is $\frac{-40!}{20}$ Do you concur? If not could you correct me? Thanks.",,"['derivatives', 'power-series', 'solution-verification']"
85,Differentiating the function $\arcsin(3x-4x^3)$,Differentiating the function,\arcsin(3x-4x^3),"When I have to differentiate the function $\arcsin(3x-4x^3)$ which of the following methods is more appropriate ? Putting $x=\sin θ$,simplifying and then differentiating for certain ranges of $x$. Directly differentiating using chain rule. Can the results obtained in these two techniques be shown to be same ?  BTW I really don't understand why most textbooks prefer the first method. Any ideas? Thank you. P.S:I know how to differentiate it.My question is something else ^ .","When I have to differentiate the function $\arcsin(3x-4x^3)$ which of the following methods is more appropriate ? Putting $x=\sin θ$,simplifying and then differentiating for certain ranges of $x$. Directly differentiating using chain rule. Can the results obtained in these two techniques be shown to be same ?  BTW I really don't understand why most textbooks prefer the first method. Any ideas? Thank you. P.S:I know how to differentiate it.My question is something else ^ .",,['trigonometry']
86,Lagrangian Method for Christoffel Symbol and (non-)holonomic basis,Lagrangian Method for Christoffel Symbol and (non-)holonomic basis,,"I rencently learned about the lagrangian/variational method for computing Christoffel symbols. Let $\mathcal{M}$ be a $m-$dimensional manifold with $g_{ij}$ being the metric tensor components and let $\gamma(\lambda) = (x^1(\lambda)\dots x^m(\lambda))$ a geodesic in $\mathcal{M}$. By assigning the following Lagrangian to the metric tensor $g_{ij}$ :   $$\mathcal{L} = g_{ij} \dot{x}^i \dot{x}^j,$$   with $\dot{x}^i = \dfrac{dx^i}{d\lambda}$ and using the least action principle one gets the Euler-Lagrange equations :   $$\dfrac{d}{d\lambda}\dfrac{\partial \mathcal{L}}{\partial \dot{x}^i} - \dfrac{\partial\mathcal{L}}{\partial x^i} = 0 \tag{1}$$   Remebering the geodesic equations written by means of Christoffel symbols :   $$\ddot{x}^k + \Gamma^k_{\ ij} \dot{x}^i \dot{x}^j = 0,\tag{2}$$   Christoffel symbols are obtained comparing $(1)$ and $(2)$. How does the fact the basis one uses is holonomic or not is taken into account ?","I rencently learned about the lagrangian/variational method for computing Christoffel symbols. Let $\mathcal{M}$ be a $m-$dimensional manifold with $g_{ij}$ being the metric tensor components and let $\gamma(\lambda) = (x^1(\lambda)\dots x^m(\lambda))$ a geodesic in $\mathcal{M}$. By assigning the following Lagrangian to the metric tensor $g_{ij}$ :   $$\mathcal{L} = g_{ij} \dot{x}^i \dot{x}^j,$$   with $\dot{x}^i = \dfrac{dx^i}{d\lambda}$ and using the least action principle one gets the Euler-Lagrange equations :   $$\dfrac{d}{d\lambda}\dfrac{\partial \mathcal{L}}{\partial \dot{x}^i} - \dfrac{\partial\mathcal{L}}{\partial x^i} = 0 \tag{1}$$   Remebering the geodesic equations written by means of Christoffel symbols :   $$\ddot{x}^k + \Gamma^k_{\ ij} \dot{x}^i \dot{x}^j = 0,\tag{2}$$   Christoffel symbols are obtained comparing $(1)$ and $(2)$. How does the fact the basis one uses is holonomic or not is taken into account ?",,"['differential-geometry', 'derivatives', 'riemannian-geometry']"
87,Is this proof that $g$ is continuous correct?,Is this proof that  is continuous correct?,g,"I have proved that $g$ is continuous on $(0,2)$ and I just wish to check if my solution for $g$ being right continuous at $0$ and hence continuous at $0$ is correct. $$\lim\limits_{x \to 0^+}g(x)     =   \lim\limits_{x \to 0^+} \frac{f(0)-f(2x)}{0-2x}= \lim\limits_{x \to 0^+} \frac{f(x)-f(0)}{x-0}=f'(0)$$ Using the fact that $f$ is differentiable, hence $f'_+(0)=f'(0)$ I switched the $2x$ for an $x$ because for small $x$ they're basically the same.","I have proved that $g$ is continuous on $(0,2)$ and I just wish to check if my solution for $g$ being right continuous at $0$ and hence continuous at $0$ is correct. $$\lim\limits_{x \to 0^+}g(x)     =   \lim\limits_{x \to 0^+} \frac{f(0)-f(2x)}{0-2x}= \lim\limits_{x \to 0^+} \frac{f(x)-f(0)}{x-0}=f'(0)$$ Using the fact that $f$ is differentiable, hence $f'_+(0)=f'(0)$ I switched the $2x$ for an $x$ because for small $x$ they're basically the same.",,"['derivatives', 'continuity', 'solution-verification']"
88,Showing that $f(x)=x^2$ for $x \in \mathbb{Q}$ and $f(x)=0$ for $x \not\in \mathbb{Q}$ is differentiable in $x=0$,Showing that  for  and  for  is differentiable in,f(x)=x^2 x \in \mathbb{Q} f(x)=0 x \not\in \mathbb{Q} x=0,I am supposed to show that $f(x) = x^2$ for $x$ in the rationals and $f(x) = 0$ for $x$ in the irrationals is differentiable at $x = 0$ and I am supposed to find the derivative of $f(x)$ at $x = 0$. Is my proof correct or not? My proof: consider limit as $h\rightarrow0$ of $\frac{f(0 + h) - f(0)}h$  then we have limit as $h\rightarrow0$ of $\frac{h^2 - 0^2}h$ and then we get limit as $h\rightarrow0$ of $\frac{h^2}h =$ limit as $h\rightarrow0$ of $h= 0 = $ the derivative of $f(x)$ at $0$,I am supposed to show that $f(x) = x^2$ for $x$ in the rationals and $f(x) = 0$ for $x$ in the irrationals is differentiable at $x = 0$ and I am supposed to find the derivative of $f(x)$ at $x = 0$. Is my proof correct or not? My proof: consider limit as $h\rightarrow0$ of $\frac{f(0 + h) - f(0)}h$  then we have limit as $h\rightarrow0$ of $\frac{h^2 - 0^2}h$ and then we get limit as $h\rightarrow0$ of $\frac{h^2}h =$ limit as $h\rightarrow0$ of $h= 0 = $ the derivative of $f(x)$ at $0$,,"['real-analysis', 'derivatives']"
89,There is a function which is continuous but not differentiable,There is a function which is continuous but not differentiable,,"I have a function which is a convergent series: $$f(x) = \sin(x) + \frac{1}{10}\sin(10x) + \frac{1}{100}\sin(100x) + \cdots \frac{1}{10^n}\sin(10^nx)$$ This function is convergent because for any E you care to specify, the function has a term which is smaller than E.  However, the function is not differentiable, and I don't understand why. $$\frac{d}{dx}f(x) = \cos(x)+\frac{10}{10}\cos(10x) + \frac{100}{100}\cos(100x) + \cdots \frac{10^n}{10^n}\cos(10^nx)$$ Is this a special case to A Continuous Nowhere-Differentiable Function ?","I have a function which is a convergent series: $$f(x) = \sin(x) + \frac{1}{10}\sin(10x) + \frac{1}{100}\sin(100x) + \cdots \frac{1}{10^n}\sin(10^nx)$$ This function is convergent because for any E you care to specify, the function has a term which is smaller than E.  However, the function is not differentiable, and I don't understand why. $$\frac{d}{dx}f(x) = \cos(x)+\frac{10}{10}\cos(10x) + \frac{100}{100}\cos(100x) + \cdots \frac{10^n}{10^n}\cos(10^nx)$$ Is this a special case to A Continuous Nowhere-Differentiable Function ?",,['derivatives']
90,Differentiate with product rule,Differentiate with product rule,,"Question: differentiate $x(x^2 +3x)^3$ I have gotten to the point where i've used the product rule and i've gotten $$(x^2 + 3x)^3 + x\cdot(3x+9)(x^2 + 3x)^2$$ but now that it comes to the simplifying im completely at a loss, any help would greatly appreciated","Question: differentiate $x(x^2 +3x)^3$ I have gotten to the point where i've used the product rule and i've gotten $$(x^2 + 3x)^3 + x\cdot(3x+9)(x^2 + 3x)^2$$ but now that it comes to the simplifying im completely at a loss, any help would greatly appreciated",,"['calculus', 'derivatives']"
91,How to show that $\int\limits_{-\infty}^{+\infty}(n-1)\Phi(x)^{n-2}\phi(x)^2dx$? decreases in $n$?,How to show that ? decreases in ?,\int\limits_{-\infty}^{+\infty}(n-1)\Phi(x)^{n-2}\phi(x)^2dx n,"I was working on a research project that involves taking the integral of $$(n-1)\int\limits_{-\infty}^{+\infty} \Phi\left(x\right)^{n-2}\phi\left(x\right)^2dx,$$ where $\Phi(.)$ is the CDF for standard normal, $\phi$ the PDF, $\alpha>0$ and $n\in\mathbb{Z}$ and $n>3$. Eventually, I wish to show that the whole expression decreases monotonically as $n$ increases. Any help on this will be greatly appreciated. Thanks!","I was working on a research project that involves taking the integral of $$(n-1)\int\limits_{-\infty}^{+\infty} \Phi\left(x\right)^{n-2}\phi\left(x\right)^2dx,$$ where $\Phi(.)$ is the CDF for standard normal, $\phi$ the PDF, $\alpha>0$ and $n\in\mathbb{Z}$ and $n>3$. Eventually, I wish to show that the whole expression decreases monotonically as $n$ increases. Any help on this will be greatly appreciated. Thanks!",,"['integration', 'derivatives', 'normal-distribution']"
92,Derivative in 0,Derivative in 0,,"I'm a highschool student and we don't learn maths in English. So please excuse me for my Math's English. I'm doing an exercise and I can't answer its final question. Can you help me? Thank you! Let $f$ a differentiable function on $\mathbb{R}$ which derivative is continuous on $\mathbb{R}$. Let $g$ a function defined on $\mathbb{R}$ by: $\forall x\in\mathbb{R}^*,\,\,g(x)=\dfrac{1}{2x}\int_{-x}^xf(t)\,dt$ and $g(0)=f(0)$. 1- Prove that $g$ is continuous at $0$. 2- For all $x\ne 0$ calculate $g'(x)$ according to $f(x)$ and $g(x)$. I answerd both questions. For the seconde one I found: $$\forall x\in\mathbb{R}^*,\,\,g'(x)=\dfrac{f(x)+f(-x)-2g(x)}{2x}$$ Here's the last question. 3- Prove that $g$ is differentiable at $0$ and that $g'(0)=0$. Thank you. edit: That's what I tried: $\lim_{x\to 0}\dfrac{g(x)-g(0)}{x}=\lim_{x\to 0}\dfrac{1}{2}\left(\dfrac{\frac{F(x)}{x}-f(0)}{x}+\dfrac{\frac{F(-x)}{-x}-f(0)}{x}\right) $ where $F(x)=\int_0^xf(t)dt$","I'm a highschool student and we don't learn maths in English. So please excuse me for my Math's English. I'm doing an exercise and I can't answer its final question. Can you help me? Thank you! Let $f$ a differentiable function on $\mathbb{R}$ which derivative is continuous on $\mathbb{R}$. Let $g$ a function defined on $\mathbb{R}$ by: $\forall x\in\mathbb{R}^*,\,\,g(x)=\dfrac{1}{2x}\int_{-x}^xf(t)\,dt$ and $g(0)=f(0)$. 1- Prove that $g$ is continuous at $0$. 2- For all $x\ne 0$ calculate $g'(x)$ according to $f(x)$ and $g(x)$. I answerd both questions. For the seconde one I found: $$\forall x\in\mathbb{R}^*,\,\,g'(x)=\dfrac{f(x)+f(-x)-2g(x)}{2x}$$ Here's the last question. 3- Prove that $g$ is differentiable at $0$ and that $g'(0)=0$. Thank you. edit: That's what I tried: $\lim_{x\to 0}\dfrac{g(x)-g(0)}{x}=\lim_{x\to 0}\dfrac{1}{2}\left(\dfrac{\frac{F(x)}{x}-f(0)}{x}+\dfrac{\frac{F(-x)}{-x}-f(0)}{x}\right) $ where $F(x)=\int_0^xf(t)dt$",,"['real-analysis', 'derivatives']"
93,Multivariate Fundamental theorem of calculus,Multivariate Fundamental theorem of calculus,,"If $\frac{\partial F}{\partial x_{1}}(x_{1},x_{2})=f(x_{1},x_{2})$ then $F(x_{1},x_{2})=\int^{x_{1}}f(u,x_{2})du+c(x_{2})?$ =============================================== Another question added: Actually this was what I intended to ask originally. If $\frac{\partial F}{\partial x_{1}}(x_{1},x_{2})=f(x_{1},x_{2})$, then $F(x_{1},x_{2})=\int^{}f(u,x_{2})du+c(x_{2})?$ Am I correct to put the integration constant to depend on $x_{2}$?","If $\frac{\partial F}{\partial x_{1}}(x_{1},x_{2})=f(x_{1},x_{2})$ then $F(x_{1},x_{2})=\int^{x_{1}}f(u,x_{2})du+c(x_{2})?$ =============================================== Another question added: Actually this was what I intended to ask originally. If $\frac{\partial F}{\partial x_{1}}(x_{1},x_{2})=f(x_{1},x_{2})$, then $F(x_{1},x_{2})=\int^{}f(u,x_{2})du+c(x_{2})?$ Am I correct to put the integration constant to depend on $x_{2}$?",,"['calculus', 'derivatives']"
94,Derivative of $\sqrt{\frac{9+x}{x}}$ using first principle,Derivative of  using first principle,\sqrt{\frac{9+x}{x}},"I am trying to find the derivative of $\sqrt{\dfrac{9+x}{x}}$ using first principle to differentiate. For sake of simplifying the formulas, I will omit the /h part to the first principles formula, unless someone is able to show me how to integrate it with these equations. $$\lim_{h\to 0} \frac {f(x+h)-f(x)}{h} $$ What I tried was rationalizing/simplifying first: $$\dfrac{\sqrt{9+x}}{x} \cdot \dfrac{\sqrt{9+x}}{\sqrt{9+x}}=\dfrac{9+x}{x\sqrt{9+x}}$$ Which when subbed into original first principle formula you get: $$\dfrac{(9+x+h)}{(x+h)\sqrt{9+x}} - \dfrac{(9+x)}{x\sqrt{9+x}}$$ I then tried to find a common denominator and this was the result: $$\dfrac{(9x+x^2+h)x\sqrt{9+x}}{x^2+18+2x+xh}-  \dfrac{(9x+9h+x^2+hx)\sqrt{9+x}}{x^2+18+2x+xh}$$ After collecting like terms I get: $$\dfrac{\dfrac{1}{x^2+18+2x+xh} - \dfrac{9h}{x^2+18+2x+xh}}{h}$$ I know it is close because the numerator is close to what the final product looks like: $$\dfrac{-9}{2 x^2 \sqrt{\dfrac{x+9}{x}}}$$","I am trying to find the derivative of $\sqrt{\dfrac{9+x}{x}}$ using first principle to differentiate. For sake of simplifying the formulas, I will omit the /h part to the first principles formula, unless someone is able to show me how to integrate it with these equations. $$\lim_{h\to 0} \frac {f(x+h)-f(x)}{h} $$ What I tried was rationalizing/simplifying first: $$\dfrac{\sqrt{9+x}}{x} \cdot \dfrac{\sqrt{9+x}}{\sqrt{9+x}}=\dfrac{9+x}{x\sqrt{9+x}}$$ Which when subbed into original first principle formula you get: $$\dfrac{(9+x+h)}{(x+h)\sqrt{9+x}} - \dfrac{(9+x)}{x\sqrt{9+x}}$$ I then tried to find a common denominator and this was the result: $$\dfrac{(9x+x^2+h)x\sqrt{9+x}}{x^2+18+2x+xh}-  \dfrac{(9x+9h+x^2+hx)\sqrt{9+x}}{x^2+18+2x+xh}$$ After collecting like terms I get: $$\dfrac{\dfrac{1}{x^2+18+2x+xh} - \dfrac{9h}{x^2+18+2x+xh}}{h}$$ I know it is close because the numerator is close to what the final product looks like: $$\dfrac{-9}{2 x^2 \sqrt{\dfrac{x+9}{x}}}$$",,"['calculus', 'derivatives']"
95,Is $\dot{f}(0)$ a function or a point?,Is  a function or a point?,\dot{f}(0),"Say \begin{align}  g: & \mathbb R^m \to \mathbb R^n \\ \implies g': & \mathbb R^m \to \mathcal{L}(\mathbb R^m,\mathbb R^n) \\ \implies g'(0): & \mathbb R^m \to \mathbb R^n \end{align} and it's known that for some $v \in \mathbb R^m$ holds $g'(0)v=w \in \mathbb R^n$. Let's define \begin{align}  f:  \mathbb R &\to \mathbb R^n \\ t &\mapsto g(tv) \end{align} Then \begin{align}  \dot{f}(t) & = \frac{d}{dt}f(t) \\ &=\frac{d}{dt}g(tv) \\ &=g'(tv)v \end{align} Therefore \begin{align}  \dot{f}(0) & = g'(0v)v \\ &=g'(0)v \\ &=w \end{align} It's from my lecture notes where they just conclude that $\dot{f}(0)=\partial g(0)v=w$. The attempt of derivation is by me. And the question is: isn't $\dot{f}(0) \in \mathcal{L}(\mathbb R,\mathbb R^n)$? How is it compatible with $w\in\mathbb R^n$? What went wrong?","Say \begin{align}  g: & \mathbb R^m \to \mathbb R^n \\ \implies g': & \mathbb R^m \to \mathcal{L}(\mathbb R^m,\mathbb R^n) \\ \implies g'(0): & \mathbb R^m \to \mathbb R^n \end{align} and it's known that for some $v \in \mathbb R^m$ holds $g'(0)v=w \in \mathbb R^n$. Let's define \begin{align}  f:  \mathbb R &\to \mathbb R^n \\ t &\mapsto g(tv) \end{align} Then \begin{align}  \dot{f}(t) & = \frac{d}{dt}f(t) \\ &=\frac{d}{dt}g(tv) \\ &=g'(tv)v \end{align} Therefore \begin{align}  \dot{f}(0) & = g'(0v)v \\ &=g'(0)v \\ &=w \end{align} It's from my lecture notes where they just conclude that $\dot{f}(0)=\partial g(0)v=w$. The attempt of derivation is by me. And the question is: isn't $\dot{f}(0) \in \mathcal{L}(\mathbb R,\mathbb R^n)$? How is it compatible with $w\in\mathbb R^n$? What went wrong?",,"['calculus', 'real-analysis', 'derivatives']"
96,Application of Mean Value Theorem and Interval,Application of Mean Value Theorem and Interval,,Using the mean value theorem establish the inequality $$7\frac{1}{4}<\sqrt{53}<7\frac{2}{7}$$ This is obviously a true statement but can you help me form the interval and what function I should use to prove this using the mean value theorem? I've only done problems where the interval is given. Thanks!,Using the mean value theorem establish the inequality $$7\frac{1}{4}<\sqrt{53}<7\frac{2}{7}$$ This is obviously a true statement but can you help me form the interval and what function I should use to prove this using the mean value theorem? I've only done problems where the interval is given. Thanks!,,['derivatives']
97,Derivatives of component maps,Derivatives of component maps,,"Given functions $f_1:\mathbb{R}^{a_1}\rightarrow\mathbb{R}^{b_1}$ and $f_2:\mathbb{R}^{a_2}\rightarrow\mathbb{R}^{b_2}$, and the function $f:\mathbb{R}^{a_1+a_2}\rightarrow\mathbb{R}^{b_1+b_2}$ is defined by $$f(x,y)=(f_1(x),f_2(y))$$ for $x\in\mathbb{R}^{a_1},y\in\mathbb{R}^{a_2}$. Take points $z=(z_1,z_2)$ and $w=(w_1,w_2)$, where $z_1,w_1\in\mathbb{R}^{a_1},z_2,w_2\in\mathbb{R}^{a_2}$. Is it true that $Df(z)w=(Df_1(z_1)w_1, Df_2(z_2)w_2)$?","Given functions $f_1:\mathbb{R}^{a_1}\rightarrow\mathbb{R}^{b_1}$ and $f_2:\mathbb{R}^{a_2}\rightarrow\mathbb{R}^{b_2}$, and the function $f:\mathbb{R}^{a_1+a_2}\rightarrow\mathbb{R}^{b_1+b_2}$ is defined by $$f(x,y)=(f_1(x),f_2(y))$$ for $x\in\mathbb{R}^{a_1},y\in\mathbb{R}^{a_2}$. Take points $z=(z_1,z_2)$ and $w=(w_1,w_2)$, where $z_1,w_1\in\mathbb{R}^{a_1},z_2,w_2\in\mathbb{R}^{a_2}$. Is it true that $Df(z)w=(Df_1(z_1)w_1, Df_2(z_2)w_2)$?",,"['calculus', 'real-analysis', 'derivatives']"
98,Proof of convolution,Proof of convolution,,I would like to know how I could prove the following convolution: $$ D (f*g) =D f* g =f* Dg $$,I would like to know how I could prove the following convolution: $$ D (f*g) =D f* g =f* Dg $$,,"['derivatives', 'convolution']"
99,property of function,property of function,,"Let $f _n (x)=x ^n$  . If I want to get $f_{n+1}'(x)$ , firstly I find $f _{n+1} (x)=x^ {n+1}$   and next differentiate $f _{n+1} (x)=x^ {n+1}$  , I obtain $f _ {n+1}' (x)=(n+1)x^ n $ . But in other ways, firstly differentiate $f _n (x)$ , obtain $f  _n '(x)=nx^ {n−1} $ , and substitute $n+1$  for $n$  in $f  _n '(x)=nx^ {n−1}$  , I get $f_  {n+1}' (x)=(n+1)x ^n$   too. I think second ways is not definition of $f ′ _{n+1} (x)$ . Is there any function which satisfying first ways is not equal to second ways?","Let $f _n (x)=x ^n$  . If I want to get $f_{n+1}'(x)$ , firstly I find $f _{n+1} (x)=x^ {n+1}$   and next differentiate $f _{n+1} (x)=x^ {n+1}$  , I obtain $f _ {n+1}' (x)=(n+1)x^ n $ . But in other ways, firstly differentiate $f _n (x)$ , obtain $f  _n '(x)=nx^ {n−1} $ , and substitute $n+1$  for $n$  in $f  _n '(x)=nx^ {n−1}$  , I get $f_  {n+1}' (x)=(n+1)x ^n$   too. I think second ways is not definition of $f ′ _{n+1} (x)$ . Is there any function which satisfying first ways is not equal to second ways?",,['derivatives']
