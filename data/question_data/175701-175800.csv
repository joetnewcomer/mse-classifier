,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Hessian under coordinate transformation product rule,Hessian under coordinate transformation product rule,,"Seeing this formula for the Hessian matrix under a coordinate transformation, I am confused as to why there is no product rule involved to give extra terms. As an example (2 dimensions) If $f:\mathbb{R^2}\to \mathbb{R}$ $(x,y) \to (u(x,y),v(x,y))$ is a coordinate transformation. Consider $f(u(x,y),v(x,y))$ and its Hessian w.r.t. $(x,y)$ and $(u,v)$. Then $H_{u,v} = J^t H_{x,y} J$ where $J$ is the Jacobian of the transformation. But assuming $x_i, x_j \in \{x,y\}$ shouldn't $f_{x_i} = f_u u_{x_i} + f_v v_{x_i}$ (total derivative), $f_{x_i x_j} = f_{uu}u_{x_i}u_{x_j} + f_{uv}u_{x_i}v_{x_j} + f_u u_{x_i x_j} + f_{vu}v_{x_i}u_{x_j} + f_{vv}v_{x_i}v_{x_j} + f_v v_{x_i x_j}$ (product rule)? Where do the $f_u u_{x_i x_j}$ and $f_v v_{x_i x_j}$ terms go?","Seeing this formula for the Hessian matrix under a coordinate transformation, I am confused as to why there is no product rule involved to give extra terms. As an example (2 dimensions) If $f:\mathbb{R^2}\to \mathbb{R}$ $(x,y) \to (u(x,y),v(x,y))$ is a coordinate transformation. Consider $f(u(x,y),v(x,y))$ and its Hessian w.r.t. $(x,y)$ and $(u,v)$. Then $H_{u,v} = J^t H_{x,y} J$ where $J$ is the Jacobian of the transformation. But assuming $x_i, x_j \in \{x,y\}$ shouldn't $f_{x_i} = f_u u_{x_i} + f_v v_{x_i}$ (total derivative), $f_{x_i x_j} = f_{uu}u_{x_i}u_{x_j} + f_{uv}u_{x_i}v_{x_j} + f_u u_{x_i x_j} + f_{vu}v_{x_i}u_{x_j} + f_{vv}v_{x_i}v_{x_j} + f_v v_{x_i x_j}$ (product rule)? Where do the $f_u u_{x_i x_j}$ and $f_v v_{x_i x_j}$ terms go?",,"['multivariable-calculus', 'vector-analysis']"
1,Why can I not combine integrals this way?,Why can I not combine integrals this way?,,"Evaluating the triple integral $\int^1_0 \int^{1-z}_0 \int^{1-y-z}_0 \text{dxdydz}$, I get $\frac 16$. Evaluating the triple integral $\int^1_0 \int^1_0 \int^1_0 \text{dxdydz}$, I get $1$. So I subtract them like: $$\int^1_0 \int^1_0 \int^1_0 \text{dxdydz} - \int^1_0 \int^{1-z}_0 \int^{1-y-z}_0 \text{dxdydz} = \int^1_0 \left(\int^1_0 \int^1_0 \text{dxdy} + \int^0_{1-z} \int^0_{1-y-z} \text{dxdy} \right)\text{dz}= \int^1_0 \int^1_{1-z} \int^1_{1-y-z} \text{dxdydz}$$ and I get $\frac 23$, not $1-\frac 16 = \frac 56$.  So I must be combining the integrals wrong.  Why can't I do it this way?","Evaluating the triple integral $\int^1_0 \int^{1-z}_0 \int^{1-y-z}_0 \text{dxdydz}$, I get $\frac 16$. Evaluating the triple integral $\int^1_0 \int^1_0 \int^1_0 \text{dxdydz}$, I get $1$. So I subtract them like: $$\int^1_0 \int^1_0 \int^1_0 \text{dxdydz} - \int^1_0 \int^{1-z}_0 \int^{1-y-z}_0 \text{dxdydz} = \int^1_0 \left(\int^1_0 \int^1_0 \text{dxdy} + \int^0_{1-z} \int^0_{1-y-z} \text{dxdy} \right)\text{dz}= \int^1_0 \int^1_{1-z} \int^1_{1-y-z} \text{dxdydz}$$ and I get $\frac 23$, not $1-\frac 16 = \frac 56$.  So I must be combining the integrals wrong.  Why can't I do it this way?",,"['integration', 'multivariable-calculus']"
2,A proof involving nested integrals and induction [duplicate],A proof involving nested integrals and induction [duplicate],,"This question already has answers here : Evaluate a Nested Integral (2 answers) Closed 9 years ago . Prove that $$\int_0^x dx_1 \int_0^{x_1}dx_2 \cdots \int_0^{x_{n-1}}f(x_n) \, dx_n =\frac{1}{(n-1)!}\int_0^x (x-t)^{n-1}f(t) \, dt$$ I'm trying induction over $n$. The case $n=1$ is trivial. When $n=2$ \begin{align}\int_0^x dx_1 \int_0^{x_1}f(x_{2})\,dx_2 = & \int_0^x\int_0^{x_1}f(x_2) \, dx_2 \, dx_1 \\ = & \int_0^x \int_{x_2}^{x} f(x_2) \, dx_1 \, dx_2 \\ =& \int_0^x f(x_2)(x_1-x_2) \, dx_2 = \frac{1}{(2-1)!}\int_0^x (x-t)f(t) \, dt\end{align} I couldn't take the induction step though. Any thoughts? I appreciate the help.","This question already has answers here : Evaluate a Nested Integral (2 answers) Closed 9 years ago . Prove that $$\int_0^x dx_1 \int_0^{x_1}dx_2 \cdots \int_0^{x_{n-1}}f(x_n) \, dx_n =\frac{1}{(n-1)!}\int_0^x (x-t)^{n-1}f(t) \, dt$$ I'm trying induction over $n$. The case $n=1$ is trivial. When $n=2$ \begin{align}\int_0^x dx_1 \int_0^{x_1}f(x_{2})\,dx_2 = & \int_0^x\int_0^{x_1}f(x_2) \, dx_2 \, dx_1 \\ = & \int_0^x \int_{x_2}^{x} f(x_2) \, dx_1 \, dx_2 \\ =& \int_0^x f(x_2)(x_1-x_2) \, dx_2 = \frac{1}{(2-1)!}\int_0^x (x-t)f(t) \, dt\end{align} I couldn't take the induction step though. Any thoughts? I appreciate the help.",,"['integration', 'multivariable-calculus']"
3,Is a sphere really a (differentiable) manifold?,Is a sphere really a (differentiable) manifold?,,"I am a beginning student in Differential Geometry. According to what I understand, the charts: $$\sigma_+^z(x,y) = (x,y, \sqrt{1 - x^2 - y^2} )$$ $$\sigma_+^x(u,v)  = (\sqrt{1 - u^2 - v^2},u,v )$$ defined on $U_+^z = \{ (x,y) : x^2 + y^2 <1 \} $ and $U_+^x = \{ (u,v) : u^2 + v^2 <1\}$ respectively will cover only a portion of the sphere and omitting the points $(0,\pm1,0).$ Now my question goes, how exactly does one find the transition map $$\sigma_+^z{^{-1}} \circ \sigma_+^x$$ when both coordinate charts/surface patches is a map from $\mathbb{R}^2 \to \mathbb{R}^3$? I already tried solving for an inverse, but I can't figure out what to do with the square root component $\sqrt{}$ for $\sigma_+^z$? I conjecture that the radius is actually a variable as well and that's why I am having so much trouble? Goal : My goal is to compute a transition map for a chart mapping from $\mathbb{R}^2 \to \mathbb{R}^3$. I've looked all over all the notes in differential geometry to find one example of a transition map calculation, I could not find any, that's why I am making one right now. If you could at least lead me to a simpler manifold (surface, I guess in this case), it would be great. I think I would be able to do the algebra from there. (sorry this post is so long) What I tried ""solving for variables"" : For $\sigma_+^z$, I set $s = x$, $t = y$, and $r = \sqrt{1 - x^2 - y^2}$, since the first two variables have been solved, I am not sure what to do with the last one. For $\sigma_-^x$, I set $a = \sqrt{1 - u^2 - v^2}, b  = u$ and $c = v$, does this mean the inverse will map back to $$ (\sqrt{1 - a^2 - c^2} , c)  \subset U_+^x$$","I am a beginning student in Differential Geometry. According to what I understand, the charts: $$\sigma_+^z(x,y) = (x,y, \sqrt{1 - x^2 - y^2} )$$ $$\sigma_+^x(u,v)  = (\sqrt{1 - u^2 - v^2},u,v )$$ defined on $U_+^z = \{ (x,y) : x^2 + y^2 <1 \} $ and $U_+^x = \{ (u,v) : u^2 + v^2 <1\}$ respectively will cover only a portion of the sphere and omitting the points $(0,\pm1,0).$ Now my question goes, how exactly does one find the transition map $$\sigma_+^z{^{-1}} \circ \sigma_+^x$$ when both coordinate charts/surface patches is a map from $\mathbb{R}^2 \to \mathbb{R}^3$? I already tried solving for an inverse, but I can't figure out what to do with the square root component $\sqrt{}$ for $\sigma_+^z$? I conjecture that the radius is actually a variable as well and that's why I am having so much trouble? Goal : My goal is to compute a transition map for a chart mapping from $\mathbb{R}^2 \to \mathbb{R}^3$. I've looked all over all the notes in differential geometry to find one example of a transition map calculation, I could not find any, that's why I am making one right now. If you could at least lead me to a simpler manifold (surface, I guess in this case), it would be great. I think I would be able to do the algebra from there. (sorry this post is so long) What I tried ""solving for variables"" : For $\sigma_+^z$, I set $s = x$, $t = y$, and $r = \sqrt{1 - x^2 - y^2}$, since the first two variables have been solved, I am not sure what to do with the last one. For $\sigma_-^x$, I set $a = \sqrt{1 - u^2 - v^2}, b  = u$ and $c = v$, does this mean the inverse will map back to $$ (\sqrt{1 - a^2 - c^2} , c)  \subset U_+^x$$",,"['multivariable-calculus', 'differential-geometry']"
4,Transformation rule for partial derivatives,Transformation rule for partial derivatives,,I can't fathom the step I have highlighted in green. Am I using the chain rule in 3 dimensions? What is it that I am transforming here?,I can't fathom the step I have highlighted in green. Am I using the chain rule in 3 dimensions? What is it that I am transforming here?,,['multivariable-calculus']
5,Use a double integral in polar coordinates to find the area,Use a double integral in polar coordinates to find the area,,"So the area is just an intersection of two circles Converting the two circles to polar coordinates, I get: $r(r-2\sin\theta)=0$, and $r(r-2\cos\theta)=0$ Ummm so $r =0$ and r = $2\sin\theta$ and r=$2\cos\theta$ ? are those the boundaries?","So the area is just an intersection of two circles Converting the two circles to polar coordinates, I get: $r(r-2\sin\theta)=0$, and $r(r-2\cos\theta)=0$ Ummm so $r =0$ and r = $2\sin\theta$ and r=$2\cos\theta$ ? are those the boundaries?",,"['calculus', 'integration', 'multivariable-calculus', 'polar-coordinates']"
6,Surface area of the part of the sphere $x^2+y^2+z^2=a^2$ that is inside the cylinder $x^2+y^2=ax$,Surface area of the part of the sphere  that is inside the cylinder,x^2+y^2+z^2=a^2 x^2+y^2=ax,"I've been solving some surface area problems lately, but I don't think that the same approach that I was using will work with this one (or at least will result in a lot work). So, I believe I should use spherical coordinates for the parameterization, I did: $$x=r\sin\phi\cos\theta, y=r\sin\phi\sin\theta, z=r\cos\phi$$ When I take the absolute value (is the term ""absolute value"" correct?) of the cross product of the partial derivatives of $r(\rho, \space \theta)$ I will get $\sin\phi$ (done it before). Therefore the integral I have to evaluate in order to get what is asked is: $$\iint_D\sin\phi \space d_\theta d_\phi$$ I'm just not sure of how to define the boundaries for $\theta$. What I did on other problems was define $R(r, \space \theta) = (r\cos\theta, r\sin\theta, z)$ (the cylinder parameterization) where $z$ would be the surface, will this work here? I have a feeling that taking the cross product of something like $\sqrt{a^2-x^2-y^2}$ will be pretty hairy and a more elegant way of finding $\theta$ should exist. If anyone can point out what I'm not seeing here I will be very glad. Thank you.","I've been solving some surface area problems lately, but I don't think that the same approach that I was using will work with this one (or at least will result in a lot work). So, I believe I should use spherical coordinates for the parameterization, I did: $$x=r\sin\phi\cos\theta, y=r\sin\phi\sin\theta, z=r\cos\phi$$ When I take the absolute value (is the term ""absolute value"" correct?) of the cross product of the partial derivatives of $r(\rho, \space \theta)$ I will get $\sin\phi$ (done it before). Therefore the integral I have to evaluate in order to get what is asked is: $$\iint_D\sin\phi \space d_\theta d_\phi$$ I'm just not sure of how to define the boundaries for $\theta$. What I did on other problems was define $R(r, \space \theta) = (r\cos\theta, r\sin\theta, z)$ (the cylinder parameterization) where $z$ would be the surface, will this work here? I have a feeling that taking the cross product of something like $\sqrt{a^2-x^2-y^2}$ will be pretty hairy and a more elegant way of finding $\theta$ should exist. If anyone can point out what I'm not seeing here I will be very glad. Thank you.",,"['multivariable-calculus', 'surfaces', 'spherical-coordinates']"
7,Lagrange multiplier - space probe,Lagrange multiplier - space probe,,"i am stuck on this question which uses the Lagrange multiplier. I am trying to construct the equations using the partial derivatives but the $x$'s and $y$'s cancel. can anyone help? A space probe in the shape of the ellipsoid $x^2 + y^2 + 3z^2 = 3$ enters a planet's atmosphere and begins to heat up. The temperature on its surface is found to be $T(x, y, z) = x^2 + 2y^2 + 6z$: Use the method of Lagrange multipliers to find the hottest points on the probe's surface.","i am stuck on this question which uses the Lagrange multiplier. I am trying to construct the equations using the partial derivatives but the $x$'s and $y$'s cancel. can anyone help? A space probe in the shape of the ellipsoid $x^2 + y^2 + 3z^2 = 3$ enters a planet's atmosphere and begins to heat up. The temperature on its surface is found to be $T(x, y, z) = x^2 + 2y^2 + 6z$: Use the method of Lagrange multipliers to find the hottest points on the probe's surface.",,"['calculus', 'multivariable-calculus', 'lagrange-multiplier']"
8,Finding Multivariable Limits,Finding Multivariable Limits,,"Is there any good way to find a multivariable limit other than switching to polar coordinates? For example, students each year are inundated with problems like $$\lim_{(x,y)\to (0,0)}{\frac{x^2y-xy^2}{\sqrt{x^2+y^2}}}$$ and, putting aside using the definition of the limit directly, the only good solution that seems to exist is to write $$\lim_{r\to 0}{\frac{r^3(\cos^2\theta\sin\theta-\cos\theta\sin^2\theta)}{r}}=\lim_{r\to 0}{\;r^2\cos\theta\sin\theta (\cos\theta -\sin\theta)}=0$$ Also, is there any instance in which this method fails? Don't feel obligated to provide a full-blown proof, I'm just wondering if there is a proof because I don't recall ever seeing one in a textbook.","Is there any good way to find a multivariable limit other than switching to polar coordinates? For example, students each year are inundated with problems like $$\lim_{(x,y)\to (0,0)}{\frac{x^2y-xy^2}{\sqrt{x^2+y^2}}}$$ and, putting aside using the definition of the limit directly, the only good solution that seems to exist is to write $$\lim_{r\to 0}{\frac{r^3(\cos^2\theta\sin\theta-\cos\theta\sin^2\theta)}{r}}=\lim_{r\to 0}{\;r^2\cos\theta\sin\theta (\cos\theta -\sin\theta)}=0$$ Also, is there any instance in which this method fails? Don't feel obligated to provide a full-blown proof, I'm just wondering if there is a proof because I don't recall ever seeing one in a textbook.",,['multivariable-calculus']
9,Hessian matrix of $g\circ f$,Hessian matrix of,g\circ f,"Say, $f:\mathbb R^n\to\mathbb R^k$ and $g:\mathbb R^k\to\mathbb R$ are both $C^2$. I'd like to express the Hessian matrix of $g\circ f$ $$\left( \frac{\partial^2(g\circ f)}{\partial x_i \partial x_j} (x) \right)_{i,j\in\{1,\ldots, n\}}$$ in terms of partial derivatives of $g$ and $f$. I know that $$(g\circ f)''(x)[v,w]=f''(g(x))\Big[g'(x)[v], g'(x)[w]\Big]+f'(g(x))\circ g''(x)[v,w]$$ yet I have problems writing it out in terms of partial derivatives.","Say, $f:\mathbb R^n\to\mathbb R^k$ and $g:\mathbb R^k\to\mathbb R$ are both $C^2$. I'd like to express the Hessian matrix of $g\circ f$ $$\left( \frac{\partial^2(g\circ f)}{\partial x_i \partial x_j} (x) \right)_{i,j\in\{1,\ldots, n\}}$$ in terms of partial derivatives of $g$ and $f$. I know that $$(g\circ f)''(x)[v,w]=f''(g(x))\Big[g'(x)[v], g'(x)[w]\Big]+f'(g(x))\circ g''(x)[v,w]$$ yet I have problems writing it out in terms of partial derivatives.",,"['multivariable-calculus', 'derivatives', 'tensors']"
10,Set up the limits and evaluate $\iiint ydv$,Set up the limits and evaluate,\iiint ydv,"Set up the limits and evaluate the integral. $$\displaystyle\iiint ydv$$ $$G$$ G is the region enclosed by the plane $z = y$, $xy-plane$ and the surface $y = 1 - x^2$ I need help finding the limits of integration. I have this so far, but I don't think it is correct. $$\displaystyle\int_{0}^{1}\int_{0}^{1 - x^2}\int_{0}^{y} y\;dz\;dy\;dx$$","Set up the limits and evaluate the integral. $$\displaystyle\iiint ydv$$ $$G$$ G is the region enclosed by the plane $z = y$, $xy-plane$ and the surface $y = 1 - x^2$ I need help finding the limits of integration. I have this so far, but I don't think it is correct. $$\displaystyle\int_{0}^{1}\int_{0}^{1 - x^2}\int_{0}^{y} y\;dz\;dy\;dx$$",,"['integration', 'multivariable-calculus', 'volume']"
11,Find the mistake in my proof of partial derivatives exisiting implying differentiability.,Find the mistake in my proof of partial derivatives exisiting implying differentiability.,,"Let $\Omega\subseteq \mathbb{R} ^2$ be open. Let the function $f: \mathbb{R} ^2 \mapsto \mathbb{R}$ have partial derivatives $f_x'$ and $f'_y$ at every point in $\Omega$. I will now try to prove that $f$ is differentiable on $\Omega$. What we want to show is the following: $$ f(x+h,y+k) = f(x,y) + Ah + Bk + \sqrt{h^2+k^2}R(h,k) \qquad \text{(1)} $$ for some $(x,y)\in\Omega$, $h$ and $k$ sufficiently small (so that $(x+h,y+k)\in\Omega$), some real numbers $A$ and $B$ and a function $R: \mathbb{R} ^2 \mapsto \mathbb{R}$ such that $R(h,k) \rightarrow 0$ when $(h,k) \rightarrow (0,0)$. We introduce the following functions from $\mathbb{R}$ to $\mathbb{R}$: $$ \phi(x)=f(x,y+k) $$ $$ \psi(y) = f(x,y). $$ (Here comes the crucial step, is this the mistake?) We realize that, because the partial derivatives of $f$ exist, $\phi$ and $\psi$ are differentiable. Therefore we have: $$ \phi(x+h) = \phi(x) + \phi'(x)h + h\rho_1(h) $$ $$ \psi(y+k) = \psi(y) + \psi'(y)k + k\rho_2(k), $$ where $\rho_1(h)\rightarrow 0$ when $h\rightarrow0$ and  $\rho_2(k)\rightarrow 0$ when $k\rightarrow0$. Adding these together, noting that $\phi(x) = \psi(y+k)$, gives: $$ \phi(x+h) = \psi(y) + \phi'(x)h + h\rho_1(h) + \psi'(y)k + k\rho_2(k), $$ or: $$ f(x+h,y+k) = f(x,y)  + f_x'(x,y+k)h + f_y'(x,y)k + h\rho_1(h) + k\rho_2(k). $$ The above is on the form $(1)$ because, since $\lvert\frac{h}{\sqrt{h^2+k^2}}\rvert\leq1$ (and the same thing for $k$), we can set $R(h,k)= \frac{h\rho_1(h) + k\rho_2(k)}{\sqrt{h^2+k^2}}$ and see that $R(h,k) \rightarrow 0$ when $(h,k) \rightarrow (0,0)$. (Right?) So we have shown that $f$ is differentiable. Now, I know this has to be wrong (I've seen counterexamples). So either I have used somewhere that the partial derivatives are continuous, or I have made some other sort of reasoning error. It's just that I can't find it, so I would be thankful if someone could point it out for me! Thanks! I also know that $A$ and $B$ should be the partial derivatives at $(x,y)$, but in the definition of differentiability (at least in my text book) it only says they need to be some reals. Only later it's proven that they are in fact the partial derivatives - which makes my proof seem all the more fishy.","Let $\Omega\subseteq \mathbb{R} ^2$ be open. Let the function $f: \mathbb{R} ^2 \mapsto \mathbb{R}$ have partial derivatives $f_x'$ and $f'_y$ at every point in $\Omega$. I will now try to prove that $f$ is differentiable on $\Omega$. What we want to show is the following: $$ f(x+h,y+k) = f(x,y) + Ah + Bk + \sqrt{h^2+k^2}R(h,k) \qquad \text{(1)} $$ for some $(x,y)\in\Omega$, $h$ and $k$ sufficiently small (so that $(x+h,y+k)\in\Omega$), some real numbers $A$ and $B$ and a function $R: \mathbb{R} ^2 \mapsto \mathbb{R}$ such that $R(h,k) \rightarrow 0$ when $(h,k) \rightarrow (0,0)$. We introduce the following functions from $\mathbb{R}$ to $\mathbb{R}$: $$ \phi(x)=f(x,y+k) $$ $$ \psi(y) = f(x,y). $$ (Here comes the crucial step, is this the mistake?) We realize that, because the partial derivatives of $f$ exist, $\phi$ and $\psi$ are differentiable. Therefore we have: $$ \phi(x+h) = \phi(x) + \phi'(x)h + h\rho_1(h) $$ $$ \psi(y+k) = \psi(y) + \psi'(y)k + k\rho_2(k), $$ where $\rho_1(h)\rightarrow 0$ when $h\rightarrow0$ and  $\rho_2(k)\rightarrow 0$ when $k\rightarrow0$. Adding these together, noting that $\phi(x) = \psi(y+k)$, gives: $$ \phi(x+h) = \psi(y) + \phi'(x)h + h\rho_1(h) + \psi'(y)k + k\rho_2(k), $$ or: $$ f(x+h,y+k) = f(x,y)  + f_x'(x,y+k)h + f_y'(x,y)k + h\rho_1(h) + k\rho_2(k). $$ The above is on the form $(1)$ because, since $\lvert\frac{h}{\sqrt{h^2+k^2}}\rvert\leq1$ (and the same thing for $k$), we can set $R(h,k)= \frac{h\rho_1(h) + k\rho_2(k)}{\sqrt{h^2+k^2}}$ and see that $R(h,k) \rightarrow 0$ when $(h,k) \rightarrow (0,0)$. (Right?) So we have shown that $f$ is differentiable. Now, I know this has to be wrong (I've seen counterexamples). So either I have used somewhere that the partial derivatives are continuous, or I have made some other sort of reasoning error. It's just that I can't find it, so I would be thankful if someone could point it out for me! Thanks! I also know that $A$ and $B$ should be the partial derivatives at $(x,y)$, but in the definition of differentiability (at least in my text book) it only says they need to be some reals. Only later it's proven that they are in fact the partial derivatives - which makes my proof seem all the more fishy.",,"['multivariable-calculus', 'proof-verification']"
12,Volume above cone and below paraboloid.,Volume above cone and below paraboloid.,,"I need to find the volume above the cone $z=\sqrt{x^2+y^2}$ and below the paraboloid $z=2-x^2-y^2$. I thought about using spherical coordinates and finding $p$, which would be (before simplification) : $$-z=-2+x^2+y^2$$ $$-p \cos(\theta) =-2+p^2 \sin^2 (\theta) \cos^2(\theta)+p^2 \sin^2(\theta) \sin^2(\theta)$$ But I can't seem to be able to isolate $p$ even knowing that $\sin^2(\theta)+\cos^2(\theta)=1$. Any hint would be greatly appreciated! Thanks.","I need to find the volume above the cone $z=\sqrt{x^2+y^2}$ and below the paraboloid $z=2-x^2-y^2$. I thought about using spherical coordinates and finding $p$, which would be (before simplification) : $$-z=-2+x^2+y^2$$ $$-p \cos(\theta) =-2+p^2 \sin^2 (\theta) \cos^2(\theta)+p^2 \sin^2(\theta) \sin^2(\theta)$$ But I can't seem to be able to isolate $p$ even knowing that $\sin^2(\theta)+\cos^2(\theta)=1$. Any hint would be greatly appreciated! Thanks.",,"['calculus', 'multivariable-calculus']"
13,Generalization of the Lagrange Multiplier,Generalization of the Lagrange Multiplier,,"Let's say that I have a submanifold cut out of $\mathbb{R}^{n+k}$ as $f^{-1}(0)$ where $f:\mathbb{R}^{n+k} \rightarrow \mathbb{R}^k$ is smooth and $0$ is a regular value. The Lagrange multiplier criterion tells me that if $x\in f^{-1}(0)$ is a critical point of $g|_{f^{-1}(0)}$ where $g:\mathbb{R}^{n+k} \rightarrow \mathbb{R}$, then there is a linear function $\lambda:\mathbb{R}^k \rightarrow \mathbb{R}$ such that $DG(x) + \lambda DF(x) = 0$. One can use this to probe for critical points of $g$ by first considering solutions of the system of equations $DG(x) + \lambda DF(x) = 0$ and $f(x) = 0$ for varying $\lambda$. Can you adapt this in a useful way to the scenario where $g$ is a vector valued function? i.e. $g:\mathbb{R}^{n+k} \rightarrow \mathbb{R}^{l}$ and $\lambda:\mathbb{R}^k \rightarrow \mathbb{R}^{l}$ If not, then what is a good approach to computing the sets of critical points of such vector valued functions.","Let's say that I have a submanifold cut out of $\mathbb{R}^{n+k}$ as $f^{-1}(0)$ where $f:\mathbb{R}^{n+k} \rightarrow \mathbb{R}^k$ is smooth and $0$ is a regular value. The Lagrange multiplier criterion tells me that if $x\in f^{-1}(0)$ is a critical point of $g|_{f^{-1}(0)}$ where $g:\mathbb{R}^{n+k} \rightarrow \mathbb{R}$, then there is a linear function $\lambda:\mathbb{R}^k \rightarrow \mathbb{R}$ such that $DG(x) + \lambda DF(x) = 0$. One can use this to probe for critical points of $g$ by first considering solutions of the system of equations $DG(x) + \lambda DF(x) = 0$ and $f(x) = 0$ for varying $\lambda$. Can you adapt this in a useful way to the scenario where $g$ is a vector valued function? i.e. $g:\mathbb{R}^{n+k} \rightarrow \mathbb{R}^{l}$ and $\lambda:\mathbb{R}^k \rightarrow \mathbb{R}^{l}$ If not, then what is a good approach to computing the sets of critical points of such vector valued functions.",,['multivariable-calculus']
14,Conservation Law for Heat Equation on Infinite Domain,Conservation Law for Heat Equation on Infinite Domain,,"Let $$ u_{t}(x,t) = \Delta u(x,t) \space \space \space \text{ for } \space t \ge 0 \space \space\space\space , \space x \in \mathbb{R}^{n} $$ and $$ u \rightarrow 0 \space \text{ as }  \space ||x|| \rightarrow \infty $$ Show $$I(t) =  \int_{\mathbb{R}^{n}} {u(x,t) } dx = constant $$ What i have attempted so far is to apply Lipschitz and the Divergence Theorem on a finite sphere and then try and show the limit comes out as zero. This works up until the limit part, then i get a bit  stuck. Maybe i'm missing something obvious. thanks for any hints/advice you can provide! EDIT: very grateful for the solutions you provided","Let $$ u_{t}(x,t) = \Delta u(x,t) \space \space \space \text{ for } \space t \ge 0 \space \space\space\space , \space x \in \mathbb{R}^{n} $$ and $$ u \rightarrow 0 \space \text{ as }  \space ||x|| \rightarrow \infty $$ Show $$I(t) =  \int_{\mathbb{R}^{n}} {u(x,t) } dx = constant $$ What i have attempted so far is to apply Lipschitz and the Divergence Theorem on a finite sphere and then try and show the limit comes out as zero. This works up until the limit part, then i get a bit  stuck. Maybe i'm missing something obvious. thanks for any hints/advice you can provide! EDIT: very grateful for the solutions you provided",,"['integration', 'multivariable-calculus', 'partial-differential-equations', 'heat-equation']"
15,Need help on Stokes Theorem in surface integral,Need help on Stokes Theorem in surface integral,,"Hello and how you doing today? I just came across a problem which need to use Stokes theorem. The problems says: Evaluate the surface integral $$ \int_{S}\nabla\times\vec{F}\cdot{\rm d}\vec{S} $$ where F(x,y,z)=$(y^2)i$ + $(2xy)j$+$(xz^2)k$ and S is the surface of the paraboloid $z= x^2+y^2$ bounded by the planes $x=0,y=0$ and $z=1$ in the first quadrant pointing upward. I got $\nabla F$ which is $(-z^2)j$ So, I stuck at here because I dont know the boundary C in order to use Stokes theorem. So could someone please help me to start? By the way thank you very much for taking your time and consideration to help me on this problem.","Hello and how you doing today? I just came across a problem which need to use Stokes theorem. The problems says: Evaluate the surface integral $$ \int_{S}\nabla\times\vec{F}\cdot{\rm d}\vec{S} $$ where F(x,y,z)=$(y^2)i$ + $(2xy)j$+$(xz^2)k$ and S is the surface of the paraboloid $z= x^2+y^2$ bounded by the planes $x=0,y=0$ and $z=1$ in the first quadrant pointing upward. I got $\nabla F$ which is $(-z^2)j$ So, I stuck at here because I dont know the boundary C in order to use Stokes theorem. So could someone please help me to start? By the way thank you very much for taking your time and consideration to help me on this problem.",,['multivariable-calculus']
16,Prove that the derivative of a scalar field cannot be greater than 0 for a fixed point and for every vector y,Prove that the derivative of a scalar field cannot be greater than 0 for a fixed point and for every vector y,,"The definition of the derivative of a scalar field with respect to a vector was given as the following: Given a scalar field $f: S \rightarrow R$, where $S \subseteq \mathbf{R}^n$. Let $a$ be an interior point of $S$ and $y$ an arbitrary point in $\mathbf{R}^n$. Then $f'(a;y)$ is the derivative of $f$ at $a$ with respect to $y$:   $$ f'(a;y) = \lim_{h\rightarrow 0}\frac{f(a+hy)-f(a)}{h} $$ Prove that there is no scalar field $f'(a;y)>0$ for a fixed vector $a$ and every nonzero vector $y$. I'm not really sure where to go with this. If $f'(a;y)>0$ for all $y$, then $f$ must be increasing in every direction around $a$, so $a$ is a minimum of $f$, maybe? I'm not even sure that's valid, so not too sure what to do, really.","The definition of the derivative of a scalar field with respect to a vector was given as the following: Given a scalar field $f: S \rightarrow R$, where $S \subseteq \mathbf{R}^n$. Let $a$ be an interior point of $S$ and $y$ an arbitrary point in $\mathbf{R}^n$. Then $f'(a;y)$ is the derivative of $f$ at $a$ with respect to $y$:   $$ f'(a;y) = \lim_{h\rightarrow 0}\frac{f(a+hy)-f(a)}{h} $$ Prove that there is no scalar field $f'(a;y)>0$ for a fixed vector $a$ and every nonzero vector $y$. I'm not really sure where to go with this. If $f'(a;y)>0$ for all $y$, then $f$ must be increasing in every direction around $a$, so $a$ is a minimum of $f$, maybe? I'm not even sure that's valid, so not too sure what to do, really.",,"['real-analysis', 'multivariable-calculus']"
17,When are the following multiple improper integrals convergent?,When are the following multiple improper integrals convergent?,,"This is a question from a past exam. For which $p, q\in \mathbb R$ do the following improper integrals converge? $$\begin{align*}I_1=\int_{D_1}\dfrac{dx}{(1-\cos(\|x\|_2))^p}\\I_2=\int_{D_2}\dfrac{dx}{\|x\|_2^q\ln(\|x\|_2)}\end{align*}$$, where $D_1=\{x\in \mathbb R^n|\|x\|_2\le1\}$, $D_2=\{x\in \mathbb R^n\mid\|x\|_2\ge\sqrt2\}$, and $\|x\|_2=\sqrt{\sum_1^nx_i^2}$. I have tried comparison test, but found no suitable comparison functions, and looked into my textbooks about improper integrals. But I found no useful information. Further, noting that $\cos x$ is approximately $1-x^2/2+x^4/4!-+...$, I conjecture that $I_1$ is convergent exactly when $p\lt n/2$. But I have no idea how to prove this rigorously. Finally, I tried using similar ideas for $\ln$, at least to give an approximate estimate for $I_2$, but in vain, for I found no expansion for $\ln$ that comes in handy. So any help or hint will be well appreciated.","This is a question from a past exam. For which $p, q\in \mathbb R$ do the following improper integrals converge? $$\begin{align*}I_1=\int_{D_1}\dfrac{dx}{(1-\cos(\|x\|_2))^p}\\I_2=\int_{D_2}\dfrac{dx}{\|x\|_2^q\ln(\|x\|_2)}\end{align*}$$, where $D_1=\{x\in \mathbb R^n|\|x\|_2\le1\}$, $D_2=\{x\in \mathbb R^n\mid\|x\|_2\ge\sqrt2\}$, and $\|x\|_2=\sqrt{\sum_1^nx_i^2}$. I have tried comparison test, but found no suitable comparison functions, and looked into my textbooks about improper integrals. But I found no useful information. Further, noting that $\cos x$ is approximately $1-x^2/2+x^4/4!-+...$, I conjecture that $I_1$ is convergent exactly when $p\lt n/2$. But I have no idea how to prove this rigorously. Finally, I tried using similar ideas for $\ln$, at least to give an approximate estimate for $I_2$, but in vain, for I found no expansion for $\ln$ that comes in handy. So any help or hint will be well appreciated.",,"['multivariable-calculus', 'improper-integrals']"
18,"Absolute extrema of the function $f(x,y)=2xy-x-y$",Absolute extrema of the function,"f(x,y)=2xy-x-y","Find the absolute extrema of the function $$f(x,y)=2xy-x-y$$ over the region of the $xy$-plane bounded by the parabola $y=x^2$ and the line $y=4.$ I was wondering if I needed to use Lagrange multipliers to solve this problem and if I do, how would I go about solving this problem? If someone could help me, that would be great! Thanks","Find the absolute extrema of the function $$f(x,y)=2xy-x-y$$ over the region of the $xy$-plane bounded by the parabola $y=x^2$ and the line $y=4.$ I was wondering if I needed to use Lagrange multipliers to solve this problem and if I do, how would I go about solving this problem? If someone could help me, that would be great! Thanks",,"['multivariable-calculus', 'optimization', 'maxima-minima']"
19,Spivak Calculus on Manifolds Exercise 2-9,Spivak Calculus on Manifolds Exercise 2-9,,"I'm kind of stumped on this exercise in two spots. First I'll state the problem: Two functions $f,g : \mathbb{R} \to \mathbb{R}$ are equal up to $n$th order at $a$ if $$\lim_{h \to 0} \frac{f(a + h) - g(a + h)}{h^n} = 0$$ (a) Show that $f$ is differentiable at $a$ if and only if there is a function $g$ of the form $g(x) = a_0 + a_1(x-1)$ such that $f$ and $g$ are equal up to the first order at $a$. (b) If $f'(a), \ldots , f^{(n)}(a)$ exist, show that $f$ and the function $g$ defined by $$g(x) = \sum_{i=0}^{n} \frac{f^{(i)}(a)}{i!}(x-a)^i$$ are equal up to $n$th order at $a$. Hint: The limit $$\lim_{x \to a} \frac{f(x) - \sum_{i=0}^{n-1} \frac{f^{(i)}(a)}{i!}(x-a)^i}{(x - a)^n}$$ can be evaluated by L'Hospital's rule. In part (a), ($\Rightarrow$) I have, but the converse I am just missing a detail. I can't see why $$\lim_{h \to 0} \frac{f(a + h) - a_0 - a_1h}{h} = 0$$ implies that $$a_0 = f(a)$$ I tried adding and subtracting $f(a)$ in the numerator of the limit to see if I could use the triangle inequality or some form of it to show that $$\lim_{h \to 0} \frac{f(a+h) - f(a) - a_1h}{h} \leq \lim_{h \to 0} \frac{f(a + h) - a_0 - a_1h}{h} = 0$$ but I haven't been able to prove that. On the other hand, for (b) I used the hint and got that $$\lim_{x \to a} \frac{f(x) - \sum_{i=0}^{n-1} \frac{f^{(i)}(a)}{i!}(x-a)^i}{(x - a)^n} = \lim_{x \to a} \frac{f^{n-1}(x) - f^{n-1}(a)}{(n-1)!(x-a)} = \frac{f^{(n)}(a)}{(n-1)!}$$ and then $$\lim_{x \to a} \frac{f(x) - \sum_{i=0}^{n} \frac{f^{(i)}(a)}{i!}(x-a)^i}{(x - a)^n} = \lim_{x \to a} \left[\frac{f(x) - \sum_{i=0}^{n-1} \frac{f^{(i)}(a)}{i!}(x-a)^i}{(x - a)^n}\right] - \frac{f^{(n)}(a)}{n!}\\ = \frac{f^{(n)}(a)}{(n-1)!} - \frac{f^{(n)}(a)}{n!} \neq 0$$ Can anyone tell me what I'm doing wrong? Edit : I calculated the factorial wrong in the denominator. Silly mistake. It is actually: $$\lim_{x \to a} \frac{f(x) - \sum_{i=0}^{n-1} \frac{f^{(i)}(a)}{i!}(x-a)^i}{(x - a)^n} = \lim_{x \to a} \frac{f^{n-1}(x) - f^{n-1}(a)}{n!(x-a)} = \frac{f^{(n)}(a)}{n!}$$ And then $$\lim_{x \to a} \frac{f(x) - \sum_{i=0}^{n} \frac{f^{(i)}(a)}{i!}(x-a)^i}{(x - a)^n} = \lim_{x \to a} \left[\frac{f(x) - \sum_{i=0}^{n-1} \frac{f^{(i)}(a)}{i!}(x-a)^i}{(x - a)^n}\right] - \frac{f^{(n)}(a)}{n!}\\ = \frac{f^{(n)}(a)}{n!} - \frac{f^{(n)}(a)}{n!} = 0$$","I'm kind of stumped on this exercise in two spots. First I'll state the problem: Two functions $f,g : \mathbb{R} \to \mathbb{R}$ are equal up to $n$th order at $a$ if $$\lim_{h \to 0} \frac{f(a + h) - g(a + h)}{h^n} = 0$$ (a) Show that $f$ is differentiable at $a$ if and only if there is a function $g$ of the form $g(x) = a_0 + a_1(x-1)$ such that $f$ and $g$ are equal up to the first order at $a$. (b) If $f'(a), \ldots , f^{(n)}(a)$ exist, show that $f$ and the function $g$ defined by $$g(x) = \sum_{i=0}^{n} \frac{f^{(i)}(a)}{i!}(x-a)^i$$ are equal up to $n$th order at $a$. Hint: The limit $$\lim_{x \to a} \frac{f(x) - \sum_{i=0}^{n-1} \frac{f^{(i)}(a)}{i!}(x-a)^i}{(x - a)^n}$$ can be evaluated by L'Hospital's rule. In part (a), ($\Rightarrow$) I have, but the converse I am just missing a detail. I can't see why $$\lim_{h \to 0} \frac{f(a + h) - a_0 - a_1h}{h} = 0$$ implies that $$a_0 = f(a)$$ I tried adding and subtracting $f(a)$ in the numerator of the limit to see if I could use the triangle inequality or some form of it to show that $$\lim_{h \to 0} \frac{f(a+h) - f(a) - a_1h}{h} \leq \lim_{h \to 0} \frac{f(a + h) - a_0 - a_1h}{h} = 0$$ but I haven't been able to prove that. On the other hand, for (b) I used the hint and got that $$\lim_{x \to a} \frac{f(x) - \sum_{i=0}^{n-1} \frac{f^{(i)}(a)}{i!}(x-a)^i}{(x - a)^n} = \lim_{x \to a} \frac{f^{n-1}(x) - f^{n-1}(a)}{(n-1)!(x-a)} = \frac{f^{(n)}(a)}{(n-1)!}$$ and then $$\lim_{x \to a} \frac{f(x) - \sum_{i=0}^{n} \frac{f^{(i)}(a)}{i!}(x-a)^i}{(x - a)^n} = \lim_{x \to a} \left[\frac{f(x) - \sum_{i=0}^{n-1} \frac{f^{(i)}(a)}{i!}(x-a)^i}{(x - a)^n}\right] - \frac{f^{(n)}(a)}{n!}\\ = \frac{f^{(n)}(a)}{(n-1)!} - \frac{f^{(n)}(a)}{n!} \neq 0$$ Can anyone tell me what I'm doing wrong? Edit : I calculated the factorial wrong in the denominator. Silly mistake. It is actually: $$\lim_{x \to a} \frac{f(x) - \sum_{i=0}^{n-1} \frac{f^{(i)}(a)}{i!}(x-a)^i}{(x - a)^n} = \lim_{x \to a} \frac{f^{n-1}(x) - f^{n-1}(a)}{n!(x-a)} = \frac{f^{(n)}(a)}{n!}$$ And then $$\lim_{x \to a} \frac{f(x) - \sum_{i=0}^{n} \frac{f^{(i)}(a)}{i!}(x-a)^i}{(x - a)^n} = \lim_{x \to a} \left[\frac{f(x) - \sum_{i=0}^{n-1} \frac{f^{(i)}(a)}{i!}(x-a)^i}{(x - a)^n}\right] - \frac{f^{(n)}(a)}{n!}\\ = \frac{f^{(n)}(a)}{n!} - \frac{f^{(n)}(a)}{n!} = 0$$",,"['calculus', 'multivariable-calculus']"
20,"Calculate the volume of $T = \{(x,y,z) \in \mathbb R^3 : 0 \leq z \leq x^2 + y^2, (x-1)^2 + y^2 \leq 1, y \geq 0\}$",Calculate the volume of,"T = \{(x,y,z) \in \mathbb R^3 : 0 \leq z \leq x^2 + y^2, (x-1)^2 + y^2 \leq 1, y \geq 0\}","Calculate the volume of $T = \{(x,y,z) \in \mathbb R^3 : 0 \leq z \leq x^2 + y^2, (x-1)^2 + y^2 \leq 1, y \geq 0\}$ so I said that the integral we need is $\iint_{D} {x^2 + y^2 dxdy}$. But when I drew $D$ I got this: Now I said I want to move the circle to the middle so I would have its' center at $(0,0)$ so I did a change of variables where : $$\begin{array}{11} x=u+1 \\ v = y\\ J(u,v) = 1 \\ u=r\cos\theta\\v=r\sin\theta\\0 \leq r \leq 1 \\ 0 \leq \theta \leq \pi  \end{array}$$ And we need $\iint_{D} {u^2 + 2u + 1 + v dudv}$ And the integral we finally need to calculate is: $$\int_{0}^{\pi} {d\theta {\int_{0}^{1} r^2\cos\theta + 2r\cos\theta + 1 + r\sin\theta du dv}} = 1 + \pi$$ but wolfram does not agree with my answer. What went wrong?","Calculate the volume of $T = \{(x,y,z) \in \mathbb R^3 : 0 \leq z \leq x^2 + y^2, (x-1)^2 + y^2 \leq 1, y \geq 0\}$ so I said that the integral we need is $\iint_{D} {x^2 + y^2 dxdy}$. But when I drew $D$ I got this: Now I said I want to move the circle to the middle so I would have its' center at $(0,0)$ so I did a change of variables where : $$\begin{array}{11} x=u+1 \\ v = y\\ J(u,v) = 1 \\ u=r\cos\theta\\v=r\sin\theta\\0 \leq r \leq 1 \\ 0 \leq \theta \leq \pi  \end{array}$$ And we need $\iint_{D} {u^2 + 2u + 1 + v dudv}$ And the integral we finally need to calculate is: $$\int_{0}^{\pi} {d\theta {\int_{0}^{1} r^2\cos\theta + 2r\cos\theta + 1 + r\sin\theta du dv}} = 1 + \pi$$ but wolfram does not agree with my answer. What went wrong?",,"['calculus', 'integration', 'multivariable-calculus', 'proof-verification']"
21,Which integral theorem to use to evaluate this triple integral?,Which integral theorem to use to evaluate this triple integral?,,"Take the normal pointing outwards from the surface. Use an appropriate integral theorem $$\iint_S \textbf{F}\cdot d\textbf{S} \space \space where \space \space \textbf{F} (x,y,z)=(x^3,3yz^2,3y^2z+10) $$ and $S$ is the surface $z=-\sqrt{4-x^2-y^2}$ My attempt I have tried using Gauss' divergence theorem which gives me $$ \iiint_V 3x^2+3y^2+3z^2 d\textbf{V}$$ and I end up with the integral $$ \int_0^2 \int_0^{2\pi} \int_{\frac{\pi}{2}}^\pi 3 r ^{4} \sin(\phi) \space \space d\theta \space d\phi \space  dr$$ The extra $r^2\sin(\theta)$ is the Jacobian in spherical coordinates.    Now when I integrate the above I get the incorrect answer. The correct answer is $\frac{-8\pi}{5}$. Where am i going wrong?","Take the normal pointing outwards from the surface. Use an appropriate integral theorem $$\iint_S \textbf{F}\cdot d\textbf{S} \space \space where \space \space \textbf{F} (x,y,z)=(x^3,3yz^2,3y^2z+10) $$ and $S$ is the surface $z=-\sqrt{4-x^2-y^2}$ My attempt I have tried using Gauss' divergence theorem which gives me $$ \iiint_V 3x^2+3y^2+3z^2 d\textbf{V}$$ and I end up with the integral $$ \int_0^2 \int_0^{2\pi} \int_{\frac{\pi}{2}}^\pi 3 r ^{4} \sin(\phi) \space \space d\theta \space d\phi \space  dr$$ The extra $r^2\sin(\theta)$ is the Jacobian in spherical coordinates.    Now when I integrate the above I get the incorrect answer. The correct answer is $\frac{-8\pi}{5}$. Where am i going wrong?",,"['integration', 'multivariable-calculus', 'vector-analysis']"
22,Triple integral over a cone,Triple integral over a cone,,"So I have to evaluate $$\iiint_S \! (y^2+z^2)\ \mathrm{d}x\ \mathrm{d}y\ \mathrm{d}z$$ where $S$ is a right circular cone of altitude $h$ with its base, of radius $a$, in the $xy$-plane and its axis along the $z$-axis, in cylindrical coordinates. Switching to cylindrical coordinates, we clearly have $0 \leq \theta \leq 2\pi$ and $0 \leq z \leq h$. I imagine a cross section of the cone as a right triangle with $\rho$ from the origin to the hypotenuse and a horizontal line at $z$ intersecting $\rho$ at the hypotenuse. Then $\rho=\sqrt{z^2+x^2}$, where $x$ is the length of the horizontal line. The horizontal line forms another triangle similar to the first, and so its length should be $$\frac{x}{h-z}=\frac{a}{h}$$ $$x=\frac{a(h-z)}{h}$$ so I have the inequality $$0 \leq \rho \leq \sqrt{\left(\frac{a(h-z)}{h}\right)^2+z^2}$$ but when I try to evaluate the integral with this bound, it ends up very wrong. I assume I've done something wrong finding the bounds for $\rho$, but I can't figure out what it is so I was hoping someone could tell me what I'm doing wrong. I tried doing another substitution that mapped the cone to the cone with $h=1$ and $a=1$, but that didn't really help. The book has the answer as $\frac{1}{60}\pi a^2h(3a^2+2h^2)$, but I'd like to know how to get there. Thanks for the help!","So I have to evaluate $$\iiint_S \! (y^2+z^2)\ \mathrm{d}x\ \mathrm{d}y\ \mathrm{d}z$$ where $S$ is a right circular cone of altitude $h$ with its base, of radius $a$, in the $xy$-plane and its axis along the $z$-axis, in cylindrical coordinates. Switching to cylindrical coordinates, we clearly have $0 \leq \theta \leq 2\pi$ and $0 \leq z \leq h$. I imagine a cross section of the cone as a right triangle with $\rho$ from the origin to the hypotenuse and a horizontal line at $z$ intersecting $\rho$ at the hypotenuse. Then $\rho=\sqrt{z^2+x^2}$, where $x$ is the length of the horizontal line. The horizontal line forms another triangle similar to the first, and so its length should be $$\frac{x}{h-z}=\frac{a}{h}$$ $$x=\frac{a(h-z)}{h}$$ so I have the inequality $$0 \leq \rho \leq \sqrt{\left(\frac{a(h-z)}{h}\right)^2+z^2}$$ but when I try to evaluate the integral with this bound, it ends up very wrong. I assume I've done something wrong finding the bounds for $\rho$, but I can't figure out what it is so I was hoping someone could tell me what I'm doing wrong. I tried doing another substitution that mapped the cone to the cone with $h=1$ and $a=1$, but that didn't really help. The book has the answer as $\frac{1}{60}\pi a^2h(3a^2+2h^2)$, but I'd like to know how to get there. Thanks for the help!",,['multivariable-calculus']
23,Finding global max./min.,Finding global max./min.,,"my task is to figure out the critical points of $f(x,y)=e^y(x^4-x^2+y)$, $\ $$\mathbb{R}^2 \rightarrow \mathbb{R}$, and show which of them is a maximum or minimum. As far as I got, I've shown that the critical points are: 1.: $(0,-1)$ which is neither max. nor min. (char. pol. of Hessian is indefinite) 2.: $\left(\frac{1}{ \sqrt2},-\frac{3}{4}\right)$, which is a local minimum and 3.: $\left(-\frac{1}{ \sqrt2},-\frac{3}{4}\right)$ which is the second local minimum. Moving towards my question, is there any way to easily show if any global maximum or minimum exists (in general and/or concerning this example)? I’ve used the char. poly. of the Hessian, is there any faster possibility to finding local min./max.?","my task is to figure out the critical points of $f(x,y)=e^y(x^4-x^2+y)$, $\ $$\mathbb{R}^2 \rightarrow \mathbb{R}$, and show which of them is a maximum or minimum. As far as I got, I've shown that the critical points are: 1.: $(0,-1)$ which is neither max. nor min. (char. pol. of Hessian is indefinite) 2.: $\left(\frac{1}{ \sqrt2},-\frac{3}{4}\right)$, which is a local minimum and 3.: $\left(-\frac{1}{ \sqrt2},-\frac{3}{4}\right)$ which is the second local minimum. Moving towards my question, is there any way to easily show if any global maximum or minimum exists (in general and/or concerning this example)? I’ve used the char. poly. of the Hessian, is there any faster possibility to finding local min./max.?",,['multivariable-calculus']
24,Any arbitrary closed smooth curve bounds a orientable surface?,Any arbitrary closed smooth curve bounds a orientable surface?,,"I've got a question that, given an arbitrary closed smooth curve $C:[0,1]\rightarrow\mathbb{R}^3$, can you always find a orientable surface $\Omega$ which satisfy $\partial\Omega=C[0,1]$ ? I have no idea on this question, and I suppose that the surface $\Omega$ has no restriction such as “smooth”. Thanks a lot for your help.","I've got a question that, given an arbitrary closed smooth curve $C:[0,1]\rightarrow\mathbb{R}^3$, can you always find a orientable surface $\Omega$ which satisfy $\partial\Omega=C[0,1]$ ? I have no idea on this question, and I suppose that the surface $\Omega$ has no restriction such as “smooth”. Thanks a lot for your help.",,"['real-analysis', 'multivariable-calculus']"
25,"Can I apply a Leibniz integration rule to $\frac{d}{dr} \int_{-\infty}^{\infty} \int_{h(r)}^\infty \int_{g(r)}^\infty {rf(x,y,z)\, dz\, dy\, dx}$?",Can I apply a Leibniz integration rule to ?,"\frac{d}{dr} \int_{-\infty}^{\infty} \int_{h(r)}^\infty \int_{g(r)}^\infty {rf(x,y,z)\, dz\, dy\, dx}","Suppose I need to compute the derivative $$ \frac{d}{dr} \int_{-\infty}^{\infty} \int_{h(r)}^\infty \int_{g(r)}^\infty {rf(x,y,z)\, dz\, dy\, dx} $$ Can I apply a Leibniz integration rule of some form? How?",Suppose I need to compute the derivative Can I apply a Leibniz integration rule of some form? How?,"
\frac{d}{dr} \int_{-\infty}^{\infty} \int_{h(r)}^\infty \int_{g(r)}^\infty {rf(x,y,z)\, dz\, dy\, dx}
","['integration', 'multivariable-calculus', 'leibniz-integral-rule']"
26,Integrate over the region bounded by two regions. Using Polar coordinates.,Integrate over the region bounded by two regions. Using Polar coordinates.,,Using Polar Coordinates integrate over the region bounded by the two circles: $$x^2+y^2=4$$ $$x^2+y^2=1$$ Evaluate the integral of $\int\int3x+8y^2 dx$ So what I did was said that as $x^2+y^2=4$ and $x^2+y^2=1$ That $1 \le r \le 2$. And as there is a symmetry in the four quadrants $0 \le \theta \le \frac{\pi}{2}$ which gave me $\int_0^\frac{\pi}{2}\int_1^2 3r^2\cos(\theta) +8r^3\sin^2(\theta) ~dr d\theta$ The answer it gives in the book is $30\pi$. I'm getting  $28 +30\pi$,Using Polar Coordinates integrate over the region bounded by the two circles: $$x^2+y^2=4$$ $$x^2+y^2=1$$ Evaluate the integral of $\int\int3x+8y^2 dx$ So what I did was said that as $x^2+y^2=4$ and $x^2+y^2=1$ That $1 \le r \le 2$. And as there is a symmetry in the four quadrants $0 \le \theta \le \frac{\pi}{2}$ which gave me $\int_0^\frac{\pi}{2}\int_1^2 3r^2\cos(\theta) +8r^3\sin^2(\theta) ~dr d\theta$ The answer it gives in the book is $30\pi$. I'm getting  $28 +30\pi$,,['multivariable-calculus']
27,How to handle the case of $0^0$.,How to handle the case of .,0^0,"I'm trying to find the maximum of $f(x,y) = x^ay^be^{-a-b}$ constrained to the triangle $x\geq 0$, $y\geq 0$, $x+y\leq 1$ where $a,b\geq 0$. I have split my problem up into several cases: Look for local maxima on the interior of the triangle. I am simply looking for critical points with the derivative and seeing if they satisfy my constraints. Then use quadratic forms to determine if any critical points are maxima. *EDIT: I realized I don't have to mess about with the quadratic form since the triangle is compact. Look for local maxima on the boundary. This gives me 3 subcases. i. On $x+y=1$. I'm using Lagrange multipliers. ii. We have $x=0$. This is where I am running into the issue. The hasty would conclude that $f(0,y)=0$. However, if $a=0$ we have $f(0,y) = 0^0y^be^{-b}$. I have been taught that $0^0$ is an indeterminant form, but have seen many things online stating that $0^0=1$. Depending on which is taken, the problem is affected. iii. We have $y=0$. Similar issue to ii. Any insights are greatly appreciated.","I'm trying to find the maximum of $f(x,y) = x^ay^be^{-a-b}$ constrained to the triangle $x\geq 0$, $y\geq 0$, $x+y\leq 1$ where $a,b\geq 0$. I have split my problem up into several cases: Look for local maxima on the interior of the triangle. I am simply looking for critical points with the derivative and seeing if they satisfy my constraints. Then use quadratic forms to determine if any critical points are maxima. *EDIT: I realized I don't have to mess about with the quadratic form since the triangle is compact. Look for local maxima on the boundary. This gives me 3 subcases. i. On $x+y=1$. I'm using Lagrange multipliers. ii. We have $x=0$. This is where I am running into the issue. The hasty would conclude that $f(0,y)=0$. However, if $a=0$ we have $f(0,y) = 0^0y^be^{-b}$. I have been taught that $0^0$ is an indeterminant form, but have seen many things online stating that $0^0=1$. Depending on which is taken, the problem is affected. iii. We have $y=0$. Similar issue to ii. Any insights are greatly appreciated.",,['calculus']
28,"Partial Derivative of $f(x,y) =\ln(x^{2} + y^{2}) + \sqrt{x^{2}\cdot y^{3}}$",Partial Derivative of,"f(x,y) =\ln(x^{2} + y^{2}) + \sqrt{x^{2}\cdot y^{3}}","$$f(x,y) = \ln(x^{2} + y^{2}) + \sqrt{x^{2}\cdot y^{3}}$$ What is the value of $f_{x}\left ( 0,1 \right )$ and $f_{y}\left ( 0,1 \right )$? I tried but I found the denominator as zero.","$$f(x,y) = \ln(x^{2} + y^{2}) + \sqrt{x^{2}\cdot y^{3}}$$ What is the value of $f_{x}\left ( 0,1 \right )$ and $f_{y}\left ( 0,1 \right )$? I tried but I found the denominator as zero.",,['multivariable-calculus']
29,"Pool of acid, optimal path","Pool of acid, optimal path",,"Evil Dr.Friendless have kidnapped Gregors family and attached them with a bomb. this bomb will is impossible to unwire unless you use a special key. Which of course Dr.Friendless have placed on inside a monkey at the bottom of a swimming pool he dumped acid into. The pool is given by the equation $$ f(x,y) = - \frac{2}{5}x - 1, \quad x,y \in (0,10) $$ And after a certain time the amount of acid at the point $x,y,z$ is given by the vectorfunction $$ A(x,y,z) = 10- \frac{x}{10} - y - \frac{z}{5} $$ As pointed out Gregor needs to swim from the end of the pool at $(0,0,0)$ to the bottom $(10,10,-5)$ to retrieve the key. If the tries anything fancy as walking around the pool, or using a diving mask Dr.Friendless instantly fries his family. The question now is: What is the optimal path Gregor can swim from $(0,0,0)$ to $(10,10,-5)$ to obtain the least amount of poison? That is to say to minimize the line integral $$ \int_{(0,0,0)}^{(10,10,-5)} A(\overrightarrow{r}(t)) \overrightarrow{\mathbf{r'}}(t)\,\mathrm{d}t $$ by finding the optimal parametrization. I tried a few paths, like a straight line and going along the edge, but was unable to see how I could work out the path which contained the least amount of acid.","Evil Dr.Friendless have kidnapped Gregors family and attached them with a bomb. this bomb will is impossible to unwire unless you use a special key. Which of course Dr.Friendless have placed on inside a monkey at the bottom of a swimming pool he dumped acid into. The pool is given by the equation $$ f(x,y) = - \frac{2}{5}x - 1, \quad x,y \in (0,10) $$ And after a certain time the amount of acid at the point $x,y,z$ is given by the vectorfunction $$ A(x,y,z) = 10- \frac{x}{10} - y - \frac{z}{5} $$ As pointed out Gregor needs to swim from the end of the pool at $(0,0,0)$ to the bottom $(10,10,-5)$ to retrieve the key. If the tries anything fancy as walking around the pool, or using a diving mask Dr.Friendless instantly fries his family. The question now is: What is the optimal path Gregor can swim from $(0,0,0)$ to $(10,10,-5)$ to obtain the least amount of poison? That is to say to minimize the line integral $$ \int_{(0,0,0)}^{(10,10,-5)} A(\overrightarrow{r}(t)) \overrightarrow{\mathbf{r'}}(t)\,\mathrm{d}t $$ by finding the optimal parametrization. I tried a few paths, like a straight line and going along the edge, but was unable to see how I could work out the path which contained the least amount of acid.",,"['integration', 'multivariable-calculus']"
30,Surface area of intersection of surfaces,Surface area of intersection of surfaces,,"Find the surface area of the portion of the cone $z^2=x^2+y^2$ that is inside the cylinder $z^2=2y$.I know how to write parametric equation of simple surfaces and calculate area,but how do we find ""area element"" in situation like this. I have solved this question but a few more question have come in my mind. Can the area of surface formed by intersection be calculated through surface integrals?(means is there any general method to write their parametric equation) Is surface area of inside  equal to area outside?(means area inside a ball and outside it) If yes then can 1. be solved by calculating area of two surfaces separately and then adding them?","Find the surface area of the portion of the cone $z^2=x^2+y^2$ that is inside the cylinder $z^2=2y$.I know how to write parametric equation of simple surfaces and calculate area,but how do we find ""area element"" in situation like this. I have solved this question but a few more question have come in my mind. Can the area of surface formed by intersection be calculated through surface integrals?(means is there any general method to write their parametric equation) Is surface area of inside  equal to area outside?(means area inside a ball and outside it) If yes then can 1. be solved by calculating area of two surfaces separately and then adding them?",,"['integration', 'multivariable-calculus']"
31,Does the gradient always point outward of a level surface?,Does the gradient always point outward of a level surface?,,"Let $f:\mathbb{R}^n\to \mathbb{R}$ be a differentiable function, with $a\in \mathbb{R}$ a regular value of $f$. Let $M=f^{-1}((-\infty,a])$. Then $M$ is an $n$-manifold with boundary, whose boundary is $\partial M=f^{-1}(a)$. Let $p\in \partial M$. Then $T_p\partial M=\{\nabla f(p)\}^{\perp}$. Question : Does $\nabla f(p)$ point outwards of $\partial M$? I know it's not true if we take the interval the other way around. E.g. $-1$ is a regular value of $f:\mathbb{R}^2\to \mathbb{R}$, $f(x,y)=-x^2-y^2$, but the gradient of $f$ points inwards of the boundary of $f^{-1}([-1,+\infty))$ which is $S^1$. In fact, by symmetry, if the answer to the question is positive, I suspect that if we take $M=f^{-1}([a,+\infty))$ then $\nabla f(p)$ always points inwards of $\partial M$.","Let $f:\mathbb{R}^n\to \mathbb{R}$ be a differentiable function, with $a\in \mathbb{R}$ a regular value of $f$. Let $M=f^{-1}((-\infty,a])$. Then $M$ is an $n$-manifold with boundary, whose boundary is $\partial M=f^{-1}(a)$. Let $p\in \partial M$. Then $T_p\partial M=\{\nabla f(p)\}^{\perp}$. Question : Does $\nabla f(p)$ point outwards of $\partial M$? I know it's not true if we take the interval the other way around. E.g. $-1$ is a regular value of $f:\mathbb{R}^2\to \mathbb{R}$, $f(x,y)=-x^2-y^2$, but the gradient of $f$ points inwards of the boundary of $f^{-1}([-1,+\infty))$ which is $S^1$. In fact, by symmetry, if the answer to the question is positive, I suspect that if we take $M=f^{-1}([a,+\infty))$ then $\nabla f(p)$ always points inwards of $\partial M$.",,"['differential-geometry', 'multivariable-calculus']"
32,Trouble with a simple Line Integral.,Trouble with a simple Line Integral.,,"Let $\gamma$ be a smooth Jordan curve in $\mathbb{R}^2\setminus\{(0,0)\}$ from $(1,0)$ to $(1,0)$, winding about the origin once in the clockwise direction. Compute: $$\int_{\gamma}\frac{y}{x^2+y^2}dx-\frac{x}{x^2+y^2}dy$$ I figured this problem suggested to use the Fundamental Theorem of Line Integrals, so  if $\gamma(t)=\langle x(t),y(t)\rangle$, then it seems natural to want to write \begin{align} &\int_{\gamma}\frac{y}{x^2+y^2}dx-\frac{x}{x^2+y^2}dy\\ &\qquad=\int_{0}^{1}\frac{y(t)x'(t)}{x^2(t)+y^2(t)}+\frac{x(t)y'(t)}{y^2(t)+x^2(t)}\ dt\\ &\qquad=\int_{0}^{1}\nabla f(x(t),y(t))\cdot \langle x'(t),y'(t)\rangle dt \end{align} where  $f(x,y)=\frac{-1}{2(x^2+y^2)}$. Then the fundamental theorem would imply that the line integral evaluates to $0$, since the gradient is obviously continuous on $\gamma$. But - the last inequality is wrong, insofar as the order is wrong. But I thought it might be more useful to show you where I'm stuck, and demonstrate I thought about this before posting. Sincerely, Someone who hasn't had to take a line integral in five years.","Let $\gamma$ be a smooth Jordan curve in $\mathbb{R}^2\setminus\{(0,0)\}$ from $(1,0)$ to $(1,0)$, winding about the origin once in the clockwise direction. Compute: $$\int_{\gamma}\frac{y}{x^2+y^2}dx-\frac{x}{x^2+y^2}dy$$ I figured this problem suggested to use the Fundamental Theorem of Line Integrals, so  if $\gamma(t)=\langle x(t),y(t)\rangle$, then it seems natural to want to write \begin{align} &\int_{\gamma}\frac{y}{x^2+y^2}dx-\frac{x}{x^2+y^2}dy\\ &\qquad=\int_{0}^{1}\frac{y(t)x'(t)}{x^2(t)+y^2(t)}+\frac{x(t)y'(t)}{y^2(t)+x^2(t)}\ dt\\ &\qquad=\int_{0}^{1}\nabla f(x(t),y(t))\cdot \langle x'(t),y'(t)\rangle dt \end{align} where  $f(x,y)=\frac{-1}{2(x^2+y^2)}$. Then the fundamental theorem would imply that the line integral evaluates to $0$, since the gradient is obviously continuous on $\gamma$. But - the last inequality is wrong, insofar as the order is wrong. But I thought it might be more useful to show you where I'm stuck, and demonstrate I thought about this before posting. Sincerely, Someone who hasn't had to take a line integral in five years.",,['multivariable-calculus']
33,Does this equation define a differentiable function?,Does this equation define a differentiable function?,,"I am solving past calculus exams, and I came across the following question. Does the equation: $$ F(x,y,z) = 2\sin(x^2yz) - 3x + 5y^2 - 2e^{yz} = 0 $$ define a differentiable function $z = f(x,y)$ in a neighborhood of $p = (1, 1, 0)$? At first, I thought this was a natural candidate for the implicit function theorem , but: $$ \left.\begin{matrix} \frac{\partial }{\partial z}F \end{matrix}\right|_{(1,1,0)} =  \begin{matrix} 2x^2y\cos(x^2yz) -2ye^{yz} \end{matrix}_{(1,1,0)} = 0 $$  hence, the theorem doesn't hold in this case. Ideas? Thanks!","I am solving past calculus exams, and I came across the following question. Does the equation: $$ F(x,y,z) = 2\sin(x^2yz) - 3x + 5y^2 - 2e^{yz} = 0 $$ define a differentiable function $z = f(x,y)$ in a neighborhood of $p = (1, 1, 0)$? At first, I thought this was a natural candidate for the implicit function theorem , but: $$ \left.\begin{matrix} \frac{\partial }{\partial z}F \end{matrix}\right|_{(1,1,0)} =  \begin{matrix} 2x^2y\cos(x^2yz) -2ye^{yz} \end{matrix}_{(1,1,0)} = 0 $$  hence, the theorem doesn't hold in this case. Ideas? Thanks!",,['multivariable-calculus']
34,Derivative of a composed function with several variables,Derivative of a composed function with several variables,,"I am struggling to find the first derivative of a composed function with several variables. I think the solution of the problem involves the chain rule or some generalised form, but I can not see how to do it. Any hint or help is appreciated. The situation: Lets assume we have variables $x_1, \dots, x_4$. These are transformed by a function $f$, which returns more than the given $4$ parameters. For example: $$ f: \mathbb{R}^4 \rightarrow \mathbb{R}^6, \quad x_1, x_2, x_3, x_4 \rightarrow (0, x_1 + x_3, 2 \cdot x_1 + x_4, 0, x_2 + x_3, 2 \cdot x_2 + x_4) $$ The 'output' of $f$, let us call it $\mathbf{y} = (y_1, y_2, \dots, y_6)$, is processed by another function $g$ which is defined as followed:  $$g:\mathbb{R}^6 \rightarrow \mathbb{R}^6, (y_1, y_2, \dots, y_6) \rightarrow (\exp(-y_1), \exp(-y_2), \dots, \exp(-y_6))$$ Then we have a third function $h$ which takes the 'output' of $g$, let us call this 'output' $\mathbf{z} = (z_1, z_2, \dots, z_6)$, and processes it further, returning some single value, so we have $h: \mathbb{R}^6 \rightarrow \mathbb{R}$. (I will not show the concrete definition of $h$, because I do not think it does help, but rather complicates things.) What I am looking for is the first derivative of $h$ with respect to the original four parameters $x_1, \dots, x_4$. What I tried is to apply the simple chain rule, like in the following. But I am not shure if this is correct or if I do miss something. Let's say we are interested in the first derivative of $h$ with respect to $x_2$. Then I got the following with the simple chain rule: $$ \frac{\partial h}{\partial x_4} = \frac{dh}{d\mathbf{z}} \cdot \frac{dg}{d\mathbf{y}} \cdot \frac{\partial f}{\partial x_2} $$ While writing this question, I ask myself if I need some 'partial derivative' of $h$ and $g$, but with respect to which parameter? All involving $x_2$? Any help or hints will be appreciated. Thank you! Best, Michael","I am struggling to find the first derivative of a composed function with several variables. I think the solution of the problem involves the chain rule or some generalised form, but I can not see how to do it. Any hint or help is appreciated. The situation: Lets assume we have variables $x_1, \dots, x_4$. These are transformed by a function $f$, which returns more than the given $4$ parameters. For example: $$ f: \mathbb{R}^4 \rightarrow \mathbb{R}^6, \quad x_1, x_2, x_3, x_4 \rightarrow (0, x_1 + x_3, 2 \cdot x_1 + x_4, 0, x_2 + x_3, 2 \cdot x_2 + x_4) $$ The 'output' of $f$, let us call it $\mathbf{y} = (y_1, y_2, \dots, y_6)$, is processed by another function $g$ which is defined as followed:  $$g:\mathbb{R}^6 \rightarrow \mathbb{R}^6, (y_1, y_2, \dots, y_6) \rightarrow (\exp(-y_1), \exp(-y_2), \dots, \exp(-y_6))$$ Then we have a third function $h$ which takes the 'output' of $g$, let us call this 'output' $\mathbf{z} = (z_1, z_2, \dots, z_6)$, and processes it further, returning some single value, so we have $h: \mathbb{R}^6 \rightarrow \mathbb{R}$. (I will not show the concrete definition of $h$, because I do not think it does help, but rather complicates things.) What I am looking for is the first derivative of $h$ with respect to the original four parameters $x_1, \dots, x_4$. What I tried is to apply the simple chain rule, like in the following. But I am not shure if this is correct or if I do miss something. Let's say we are interested in the first derivative of $h$ with respect to $x_2$. Then I got the following with the simple chain rule: $$ \frac{\partial h}{\partial x_4} = \frac{dh}{d\mathbf{z}} \cdot \frac{dg}{d\mathbf{y}} \cdot \frac{\partial f}{\partial x_2} $$ While writing this question, I ask myself if I need some 'partial derivative' of $h$ and $g$, but with respect to which parameter? All involving $x_2$? Any help or hints will be appreciated. Thank you! Best, Michael",,"['calculus', 'functions', 'multivariable-calculus', 'derivatives']"
35,"Same Div, different Curl, Equal on the boundary of some region?","Same Div, different Curl, Equal on the boundary of some region?",,"""Find a pair of fields having equal divergences in some region, having the same values on the boundary of that region, and yet having different curls"" Can anyone name a pair of fields that fit this requirement (It would be awesome if the region was the unit sphere)","""Find a pair of fields having equal divergences in some region, having the same values on the boundary of that region, and yet having different curls"" Can anyone name a pair of fields that fit this requirement (It would be awesome if the region was the unit sphere)",,"['multivariable-calculus', 'vector-analysis']"
36,Parametric integral with abs(),Parametric integral with abs(),,"I've got the integral $$\int_B|\sin(x)-y|dx dy$$ with $$B=\{0\leq x\leq \pi, 0\leq y\leq 1\}$$ My problem is how handle the abs in the integral. I'd probably go with two integrals (one when the abs is negative, one if it's positive), but I suppose there's a nicer solution. Note: I know how to deal with parametric integrals, the question is how to integrate the absolute value.","I've got the integral $$\int_B|\sin(x)-y|dx dy$$ with $$B=\{0\leq x\leq \pi, 0\leq y\leq 1\}$$ My problem is how handle the abs in the integral. I'd probably go with two integrals (one when the abs is negative, one if it's positive), but I suppose there's a nicer solution. Note: I know how to deal with parametric integrals, the question is how to integrate the absolute value.",,"['calculus', 'multivariable-calculus']"
37,Nested normal-distribution integral,Nested normal-distribution integral,,"Is there an analytical or approximate solution of the following integral? $$ \int_{-\infty}^{\infty}\int_{y-d}^{y+d}\exp\big(-{(x-\mu_1)^2}/{2\sigma^2}\big) \exp\big(-{(y-\mu_2)^2}/{2\sigma^2}\big) \,\mathrm{d}x\,\mathrm{d}y $$","Is there an analytical or approximate solution of the following integral? $$ \int_{-\infty}^{\infty}\int_{y-d}^{y+d}\exp\big(-{(x-\mu_1)^2}/{2\sigma^2}\big) \exp\big(-{(y-\mu_2)^2}/{2\sigma^2}\big) \,\mathrm{d}x\,\mathrm{d}y $$",,"['multivariable-calculus', 'normal-distribution']"
38,Notation - Two adjacent vectors?,Notation - Two adjacent vectors?,,"I'm studying multivariable calculus at the moment and have come across equations involving two bolded variables placed side by side, like so: $$ \nabla \mathbf{f}=\frac{\partial {{f}_{j}}}{\partial {{x}_{i}}}{{\mathbf{e}}_{i}}{{\mathbf{e}}_{j}} $$ Is this meant to be a dot product? Or something else?","I'm studying multivariable calculus at the moment and have come across equations involving two bolded variables placed side by side, like so: $$ \nabla \mathbf{f}=\frac{\partial {{f}_{j}}}{\partial {{x}_{i}}}{{\mathbf{e}}_{i}}{{\mathbf{e}}_{j}} $$ Is this meant to be a dot product? Or something else?",,"['notation', 'multivariable-calculus']"
39,Angular Velocity around a given axis,Angular Velocity around a given axis,,"Consider the problem of a point p rotating around a given axis. The axis of rotation is ω , |ω| = 1 , and q is a point on the axis. Assuming that the point rotates around the axis with unit angular velocity, then the velocity of the point p(t) is p'(t) = ω x ( p(t) - q ) I'm looking for a more detailed explanation for this equation, as it is not the same with the classic equation of angular velocity. I'm confused with the term p(t) - q in the right hand side. Thanks","Consider the problem of a point p rotating around a given axis. The axis of rotation is ω , |ω| = 1 , and q is a point on the axis. Assuming that the point rotates around the axis with unit angular velocity, then the velocity of the point p(t) is p'(t) = ω x ( p(t) - q ) I'm looking for a more detailed explanation for this equation, as it is not the same with the classic equation of angular velocity. I'm confused with the term p(t) - q in the right hand side. Thanks",,['multivariable-calculus']
40,"If $x\mapsto f(x,y)$ and $y\mapsto f(x,y)$ is continuous differentiable, then whether $f(x,y)$ is continous?","If  and  is continuous differentiable, then whether  is continous?","x\mapsto f(x,y) y\mapsto f(x,y) f(x,y)","Here is the question in my exercise. Let $f(x,y)$ be a function defined on a rectangle $[a,b]\times [c,d]$ . Assume that for each fixed $y \in[c,d]$ the function $x\mapsto f(x,y)$ is a continuously differentiable function of $x$ on $[a,b]$ ; and for each fixed $x \in[a,b]$ the function $y\mapsto f(x,y)$ is a continuously differentiable function of $y$ on $[c,d]$ . Can you conclude that $f(x,y)$ is a continuous function on the rectangle $[a,b]\times [c,d]$ ? If your answer is Yes then give a proof; if your answer is No then give a counterexample. Here is my thinking: Fixed any point $P_0$ in $[a,b]\times [c,d]$ . If $x\mapsto f(x,y)$ and $y\mapsto f(x,y)$ are both continuously differentiable, which means the partial deravatives $f_x$ and $f_y$ are continuous. So $f(x,y)$ is differentiable at $P_0$ . Then $f(x,y)$ is continuous. Is my thinking right? If it's wrong, what mistake I made?","Here is the question in my exercise. Let be a function defined on a rectangle . Assume that for each fixed the function is a continuously differentiable function of on ; and for each fixed the function is a continuously differentiable function of on . Can you conclude that is a continuous function on the rectangle ? If your answer is Yes then give a proof; if your answer is No then give a counterexample. Here is my thinking: Fixed any point in . If and are both continuously differentiable, which means the partial deravatives and are continuous. So is differentiable at . Then is continuous. Is my thinking right? If it's wrong, what mistake I made?","f(x,y) [a,b]\times [c,d] y \in[c,d] x\mapsto f(x,y) x [a,b] x \in[a,b] y\mapsto f(x,y) y [c,d] f(x,y) [a,b]\times [c,d] P_0 [a,b]\times [c,d] x\mapsto f(x,y) y\mapsto f(x,y) f_x f_y f(x,y) P_0 f(x,y)","['calculus', 'multivariable-calculus']"
41,Leibniz integral rule for an arbitrary number of dimensions,Leibniz integral rule for an arbitrary number of dimensions,,"Suppose we have $N$ particles whose coordinates are given by $\mathbf{r}_{i}$ . These coordinates are confined to be within a three-dimensional unit cell defined by $$\mathcal{V}=\left[0,L_{x}\right]\times\left[0,L_{y}\right]\times\left[0,L_{z}\right]$$ (For simplicity, we may assume $L_{x}=L_{y}=L_{z}\equiv L$ ). We shall also assume periodic boundary conditions, i.e., for example, $f\left(x_i=0\right)=f\left(x_i=L_{x}\right)$ (and same for the other Cartesian components and particles). Consider the integral $$ I = \int_{\Omega}dR\;f\left(R,t\right) $$ where $R$ represents the collection of coordinates $\mathbf{r}_1,\dots,\mathbf{r}_N$ and $dR=d\mathbf{r}_{1}\dots d\mathbf{r}_{N}$ . The volume of integration $\Omega$ is essentially the hyper-volume $\mathcal{V}^{N}$ formed by the unit cell $\mathcal{\mathcal{V}}$ . Now, suppose we perform the change of coordinates $\bar{\mathbf{r}}_{i}=c\left(t\right)\mathbf{r}_{i}$ , where $c\left(t\right)$ is some function of $t$ . For example, $\bar{\mathbf{r}}_{i}=t^{\alpha}\mathbf{r}_{i}$ , where $\alpha$ is some constant. I would like to calculate the derivative of the transformed $I$ with respect to $t$ , i.e., $$\frac{d}{dt}\int_{\bar{\Omega}\left(t\right)}d\bar{R}\;f\left(\bar{R},t\right) J(t)$$ where the transformed quantities are marked with a bar, and $J(t)$ is the Jacobian term that results from the change of coordinates. Note that after this substitution, the volume of integration depends on $t$ , because $\bar{\mathcal{V}}=\left[0,c(t) L_{x}\right]\times\left[0,c(t) L_{y}\right]\times\left[0, c(t) L_{z}\right]$ . According to Leibniz integral rule, $$\frac{d}{dt}\int_{\bar{\Omega}\left(t\right)}d\bar{R}\;f\left(\bar{R},t\right)J(t)=\int_{\bar{\Omega}\left(t\right)}d\bar{R}\;\frac{\partial}{\partial t}\left[f\left(\bar{R},t\right)J(t)\right]+\underbrace{\int_{\partial\bar{\Omega}\left(t\right)}f\left(\bar{R},t\right)J(t)\;U\cdot d\mathbf{\Sigma}}_{\text{boundary term}}$$ where $\partial\bar{\Omega}\left(t\right)$ is the boundary of the integration hyper-volume, $U$ is the velocity of the boundary (rate of change with $t$ ) and $d\mathbf{\Sigma}$ is a surface element. However, I have trouble evaluating $U\cdot d\mathbf{\Sigma}$ even for the simple cubic geometry provided here. For example, I am unsure how to calculate $U$ , or how to properly account for the orientation of the surface element.","Suppose we have particles whose coordinates are given by . These coordinates are confined to be within a three-dimensional unit cell defined by (For simplicity, we may assume ). We shall also assume periodic boundary conditions, i.e., for example, (and same for the other Cartesian components and particles). Consider the integral where represents the collection of coordinates and . The volume of integration is essentially the hyper-volume formed by the unit cell . Now, suppose we perform the change of coordinates , where is some function of . For example, , where is some constant. I would like to calculate the derivative of the transformed with respect to , i.e., where the transformed quantities are marked with a bar, and is the Jacobian term that results from the change of coordinates. Note that after this substitution, the volume of integration depends on , because . According to Leibniz integral rule, where is the boundary of the integration hyper-volume, is the velocity of the boundary (rate of change with ) and is a surface element. However, I have trouble evaluating even for the simple cubic geometry provided here. For example, I am unsure how to calculate , or how to properly account for the orientation of the surface element.","N \mathbf{r}_{i} \mathcal{V}=\left[0,L_{x}\right]\times\left[0,L_{y}\right]\times\left[0,L_{z}\right] L_{x}=L_{y}=L_{z}\equiv L f\left(x_i=0\right)=f\left(x_i=L_{x}\right) 
I = \int_{\Omega}dR\;f\left(R,t\right)
 R \mathbf{r}_1,\dots,\mathbf{r}_N dR=d\mathbf{r}_{1}\dots d\mathbf{r}_{N} \Omega \mathcal{V}^{N} \mathcal{\mathcal{V}} \bar{\mathbf{r}}_{i}=c\left(t\right)\mathbf{r}_{i} c\left(t\right) t \bar{\mathbf{r}}_{i}=t^{\alpha}\mathbf{r}_{i} \alpha I t \frac{d}{dt}\int_{\bar{\Omega}\left(t\right)}d\bar{R}\;f\left(\bar{R},t\right) J(t) J(t) t \bar{\mathcal{V}}=\left[0,c(t) L_{x}\right]\times\left[0,c(t) L_{y}\right]\times\left[0, c(t) L_{z}\right] \frac{d}{dt}\int_{\bar{\Omega}\left(t\right)}d\bar{R}\;f\left(\bar{R},t\right)J(t)=\int_{\bar{\Omega}\left(t\right)}d\bar{R}\;\frac{\partial}{\partial t}\left[f\left(\bar{R},t\right)J(t)\right]+\underbrace{\int_{\partial\bar{\Omega}\left(t\right)}f\left(\bar{R},t\right)J(t)\;U\cdot d\mathbf{\Sigma}}_{\text{boundary term}} \partial\bar{\Omega}\left(t\right) U t d\mathbf{\Sigma} U\cdot d\mathbf{\Sigma} U","['integration', 'multivariable-calculus', 'leibniz-integral-rule']"
42,Computing the integral of a certain surface,Computing the integral of a certain surface,,"I am given the surface $$S:=\{(x,y,z)\in\mathbb{R}^3|\sqrt{x^2+y^2}=2\cosh 2z,z\in[0,1]\}$$ They ask me to find a parametrization for this surface of revolution. This corresponds to the following $$\varphi(u,v)=(2\cosh(2u)\cos(v),2\cosh(2u)\sin(v),u)$$ with $(u,v)\in[0,1]\times[0,2\pi]$ . With this, they ask me to compute the integral over the 2-form $\omega=(z^2+2z)dx\wedge dy+(2x+2yz-z)dx\wedge dz$ if our surface is oriented by the normal vector $\nu(2,0,0)=(-1,0,0)$ . Since $\varphi(0,0)=(2,0,0)$ , $\varphi_u(0,0)=(0,0,1)$ and $\varphi_v(0,0)=(0,2,0)$ , we have that $\nu(0,0)=\varphi_u(0,0)\wedge\varphi_v(0,0)=(-2,0,0)$ and thus we have chosen the correct orientation. Now, $$\int_{S}\omega=\int_{[0,1]\times[0,2\pi]}\varphi^{*}((z^2+2z)dx\wedge dy+(2x+2yz-z)dx\wedge dz)$$ To compute the pullback we do the following $$(u^2+2u)d(2\cosh(2u)\cos(v))\wedge d(2\cosh(2u)\sin(v))+(4\cosh(2u)\cos(v)+4u\cosh(2u)\sin(v)-u)$$ $$d(2\cosh(2u)\cos(v))\wedge du$$ $$= 4(u^2+2u)(2\sinh(2u)\cos(v)du-\sin(v)\cosh(2u)dv)\wedge (2\cosh(2u)\cos(v)dv+4\sin(v)\sinh(2u)du)+2(4\cosh(2u)\cos(v)+4u\cosh(2u)\sin(v)-u)(2\sinh(2u)\cos(v)du-\sin(v)\cosh(2u)dv)\wedge du$$ $$=4(u^2+2u)(4\sinh(2u)\cosh(2u)\cos(v)^2 du\wedge dv + 4\sinh(2u)\cosh(2u)\sin(v)^2 du\wedge dv)+2\sin(v)\cosh(2u)(4\cosh(2u)\cos(v)+4u\cosh(2u)\sin(v)-u)du\wedge dv$$ $$=16(u^2+2u)\sinh(2u)\cosh(2u)du\wedge dv+2\sin(v)\cosh(2u)(4\cosh(2u)\cos(v)+4u\cosh(2u)\sin(v)-u)du\wedge dv$$ But this seemes a super long process, plus I don't really know if I will be able to integrate the expression. Is there a simpler way? Like using Stokes theorem or so?","I am given the surface They ask me to find a parametrization for this surface of revolution. This corresponds to the following with . With this, they ask me to compute the integral over the 2-form if our surface is oriented by the normal vector . Since , and , we have that and thus we have chosen the correct orientation. Now, To compute the pullback we do the following But this seemes a super long process, plus I don't really know if I will be able to integrate the expression. Is there a simpler way? Like using Stokes theorem or so?","S:=\{(x,y,z)\in\mathbb{R}^3|\sqrt{x^2+y^2}=2\cosh 2z,z\in[0,1]\} \varphi(u,v)=(2\cosh(2u)\cos(v),2\cosh(2u)\sin(v),u) (u,v)\in[0,1]\times[0,2\pi] \omega=(z^2+2z)dx\wedge dy+(2x+2yz-z)dx\wedge dz \nu(2,0,0)=(-1,0,0) \varphi(0,0)=(2,0,0) \varphi_u(0,0)=(0,0,1) \varphi_v(0,0)=(0,2,0) \nu(0,0)=\varphi_u(0,0)\wedge\varphi_v(0,0)=(-2,0,0) \int_{S}\omega=\int_{[0,1]\times[0,2\pi]}\varphi^{*}((z^2+2z)dx\wedge dy+(2x+2yz-z)dx\wedge dz) (u^2+2u)d(2\cosh(2u)\cos(v))\wedge d(2\cosh(2u)\sin(v))+(4\cosh(2u)\cos(v)+4u\cosh(2u)\sin(v)-u) d(2\cosh(2u)\cos(v))\wedge du = 4(u^2+2u)(2\sinh(2u)\cos(v)du-\sin(v)\cosh(2u)dv)\wedge (2\cosh(2u)\cos(v)dv+4\sin(v)\sinh(2u)du)+2(4\cosh(2u)\cos(v)+4u\cosh(2u)\sin(v)-u)(2\sinh(2u)\cos(v)du-\sin(v)\cosh(2u)dv)\wedge du =4(u^2+2u)(4\sinh(2u)\cosh(2u)\cos(v)^2 du\wedge dv + 4\sinh(2u)\cosh(2u)\sin(v)^2 du\wedge dv)+2\sin(v)\cosh(2u)(4\cosh(2u)\cos(v)+4u\cosh(2u)\sin(v)-u)du\wedge dv =16(u^2+2u)\sinh(2u)\cosh(2u)du\wedge dv+2\sin(v)\cosh(2u)(4\cosh(2u)\cos(v)+4u\cosh(2u)\sin(v)-u)du\wedge dv","['integration', 'multivariable-calculus', 'definite-integrals', 'differential-forms', 'surface-integrals']"
43,Apostol: How to use Stokes's theorem to compute $\int_C ydx+zdy+xdz=\pi a^2\sqrt{3}$ by using a portion of a sphere as the surface?,Apostol: How to use Stokes's theorem to compute  by using a portion of a sphere as the surface?,\int_C ydx+zdy+xdz=\pi a^2\sqrt{3},"The following is a problem in Apostol's Calculus , Vol II, section 12.13 Consider the line integral $$\int_C ydx+zdy+xdz=\pi  a^2\sqrt{3}\tag{1}$$ where $C$ is the curve of intersection of the sphere $$x^2+y^2+z^2=a^2\tag{2}$$ and the plane $$x+y+z=0\tag{3}$$ Use Stokes' theorem to show that the line integral (1) has the given value and explain how to traverse $C$ to arrive at the given answer. Solving (3) for $z$ $$z=-x-y\tag{4}$$ and subbing into (2) $$x^2+y^2+(-x-y)^2=a^2\tag{5}$$ we obtain $$x^2+xy+y^2=\frac{a^2}{2}\tag{6}$$ This equation represents the values of $x$ and $y$ of points $(x,y,z)$ that are on the intersection $C$ . Here is a plot of these points This ellipse is the projection of $C$ onto the xy-plane. Because the problem statement says to use Stokes' theorem, at this point I tried to compute the surface integral $$\iint\limits_S (\text{curl}F\cdot \hat{n})dS\tag{7}$$ $$=\iint\limits_S \text{curl}F(r(u,v))\cdot N(u,v)dudv\tag{8}$$ where $$N(u,v)= \frac{\partial r(u,v)}{\partial u}\times \frac{\partial r(u,v)}{\partial v}\tag{9}$$ is the fundamental vector product for a parametrization $r(u,v)$ of the surface $S$ (which is defined on some region $T$ in the uv plane that is the interior of a Jordan curve, the image under $r(u,v)$ of which is $C$ ), and $$F(x,y,z)=y\hat{i}+z\hat{j}+x\hat{k}\tag{10}$$ My question is about a non-optimal choice of this surface. Ideally, to facilitate calculations, we would use the plane in between $C$ as our surface. But I would like to use the portion of the sphere that has $C$ as its boundary. The sphere can be parametrized by spherical coordinates $$r(\phi,\theta)=a^2(\cos{\theta}\sin{\phi}\hat{i}+\sin{\theta}\sin{\phi}\hat{j}+\cos{\phi}\hat{k})\tag{11}$$ which gives us $$N(\phi,\theta)=\frac{\partial r(\phi,\theta)}{\partial \phi}\times \frac{\partial r(\phi,\theta)}{\partial \theta}=a^2(\sin^2{\phi}\cos{\theta}\hat{i}+\sin{\theta}\sin^2{\phi}\hat{j}+\cos{\phi}\sin{\phi}\hat{k})\tag{10}$$ and since we are interested only in the values of $\theta$ and $\phi$ that correspond to values on the surface that is above the ellipse, we can sub in the parametrization in (11) into the ellipse which should give us $\phi$ as a function of $\theta$ so that we know the limits of integration in the surface integral $$x^2+xy+y^2=\frac{a^2}{2}\tag{11}$$ $$a^2\cos^2{\theta}\sin^2{\phi}+a^2\sin{\theta}\cos{\theta}\sin^2{\phi}+a^2\sin^2{\theta}\sin^2{\phi}=\frac{a^2}{2}\tag{12}$$ $$\sin^2{\phi}=\frac{1}{2(1+\sin{\theta}\cos{\theta}})\tag{13}$$ $$\sin{\phi}=\frac{1}{\sqrt{2(1+\sin{\theta}\cos{\theta}})}\tag{14}$$ $$\phi(\theta)=\sin^{-1}{\left (\frac{1}{\sqrt{2(1+\sin{\theta}\cos{\theta}})}\right )}\tag{15}$$ Finally, $$\text{curl}F(x,y,z)=-\hat{i}-\hat{j}-\hat{k}\tag{16}$$ and the surface integral becomes $$\iint\limits_S (\text{curl}F\cdot \hat{n})dS\tag{17}$$ $$=\int_0^{2\pi}\int_0^{\phi(\theta)} \langle -1,-1,-1\rangle\cdot \langle a^2\sin^2{\phi}\cos{\theta},a^2\sin{\theta}\sin^2{\phi}, a^2\cos{\phi}\sin{\phi}\hat{k} \rangle d\phi d\theta\tag{18}$$ Should we expect (18) to equal $\pi a^2\sqrt{3}$ ? When I try to compute this in Maple, I don't really obtain the correct result.","The following is a problem in Apostol's Calculus , Vol II, section 12.13 Consider the line integral where is the curve of intersection of the sphere and the plane Use Stokes' theorem to show that the line integral (1) has the given value and explain how to traverse to arrive at the given answer. Solving (3) for and subbing into (2) we obtain This equation represents the values of and of points that are on the intersection . Here is a plot of these points This ellipse is the projection of onto the xy-plane. Because the problem statement says to use Stokes' theorem, at this point I tried to compute the surface integral where is the fundamental vector product for a parametrization of the surface (which is defined on some region in the uv plane that is the interior of a Jordan curve, the image under of which is ), and My question is about a non-optimal choice of this surface. Ideally, to facilitate calculations, we would use the plane in between as our surface. But I would like to use the portion of the sphere that has as its boundary. The sphere can be parametrized by spherical coordinates which gives us and since we are interested only in the values of and that correspond to values on the surface that is above the ellipse, we can sub in the parametrization in (11) into the ellipse which should give us as a function of so that we know the limits of integration in the surface integral Finally, and the surface integral becomes Should we expect (18) to equal ? When I try to compute this in Maple, I don't really obtain the correct result.","\int_C ydx+zdy+xdz=\pi
 a^2\sqrt{3}\tag{1} C x^2+y^2+z^2=a^2\tag{2} x+y+z=0\tag{3} C z z=-x-y\tag{4} x^2+y^2+(-x-y)^2=a^2\tag{5} x^2+xy+y^2=\frac{a^2}{2}\tag{6} x y (x,y,z) C C \iint\limits_S (\text{curl}F\cdot \hat{n})dS\tag{7} =\iint\limits_S \text{curl}F(r(u,v))\cdot N(u,v)dudv\tag{8} N(u,v)= \frac{\partial r(u,v)}{\partial u}\times \frac{\partial r(u,v)}{\partial v}\tag{9} r(u,v) S T r(u,v) C F(x,y,z)=y\hat{i}+z\hat{j}+x\hat{k}\tag{10} C C r(\phi,\theta)=a^2(\cos{\theta}\sin{\phi}\hat{i}+\sin{\theta}\sin{\phi}\hat{j}+\cos{\phi}\hat{k})\tag{11} N(\phi,\theta)=\frac{\partial r(\phi,\theta)}{\partial \phi}\times \frac{\partial r(\phi,\theta)}{\partial \theta}=a^2(\sin^2{\phi}\cos{\theta}\hat{i}+\sin{\theta}\sin^2{\phi}\hat{j}+\cos{\phi}\sin{\phi}\hat{k})\tag{10} \theta \phi \phi \theta x^2+xy+y^2=\frac{a^2}{2}\tag{11} a^2\cos^2{\theta}\sin^2{\phi}+a^2\sin{\theta}\cos{\theta}\sin^2{\phi}+a^2\sin^2{\theta}\sin^2{\phi}=\frac{a^2}{2}\tag{12} \sin^2{\phi}=\frac{1}{2(1+\sin{\theta}\cos{\theta}})\tag{13} \sin{\phi}=\frac{1}{\sqrt{2(1+\sin{\theta}\cos{\theta}})}\tag{14} \phi(\theta)=\sin^{-1}{\left (\frac{1}{\sqrt{2(1+\sin{\theta}\cos{\theta}})}\right )}\tag{15} \text{curl}F(x,y,z)=-\hat{i}-\hat{j}-\hat{k}\tag{16} \iint\limits_S (\text{curl}F\cdot \hat{n})dS\tag{17} =\int_0^{2\pi}\int_0^{\phi(\theta)} \langle -1,-1,-1\rangle\cdot \langle a^2\sin^2{\phi}\cos{\theta},a^2\sin{\theta}\sin^2{\phi}, a^2\cos{\phi}\sin{\phi}\hat{k} \rangle d\phi d\theta\tag{18} \pi a^2\sqrt{3}","['multivariable-calculus', 'stokes-theorem']"
44,"if $f=f(x,y)$ where $x=u^2 - v^2$ and $y = \frac{u}{v}$ what is $f_{uu}$ and $f_{vv}$",if  where  and  what is  and,"f=f(x,y) x=u^2 - v^2 y = \frac{u}{v} f_{uu} f_{vv}","I have tried to calculate this question and am not sure about my process being correct.  If you have the time, can you tell me if my answer is correct or not and if not where I have made my mistake(s).  Thanks in advance. $$ 	\begin{align*} 		f &= f(x.y), x = u^2 - v^2, y = \frac{u}{v}\\ 		&\text{find $f_{uu}$ and $f_{vv}$}\\ 		\frac{\partial x}{\partial u} &= 2u\\ 		\frac{\partial x}{\partial v} &= -2v\\ 		\frac{\partial y}{\partial u} &= \frac{1}{v}\\ 		\frac{\partial y}{\partial v} &= -\frac{u}{v^2}\\ 		\frac{\partial f}{\partial u} &= \frac{\partial f}{\partial x} \frac{\partial x}{\partial u} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial u}\\ 		&=2u\left(\frac{\partial f}{\partial x}\right) + \frac{1}{v}\left(\frac{\partial f}{\partial y}\right)\\ 		\frac{\partial}{\partial u}\left(f\right) &= 2u\frac{\partial}{\partial x}\left( f \right) + \frac{1}{v}\frac{\partial}{\partial y}\left( f \right) \tag{1} \label{equation0}\\ 		\frac{\partial^2 f}{\partial u^2} &= \left(\frac{\partial}{\partial u}\right) \left(\frac{\partial f}{\partial u}\right)\\ 		&=2\left(\frac{\partial f}{\partial x}\right) + 2u\left(\frac{\partial}{\partial u}\right)\left(\frac{\partial f}{\partial x}\right)  		+ \frac{1}{v}\left(\frac{\partial}{\partial u}\right)\left(\frac{\partial f}{\partial y}\right) \tag{2} \label {equation1}\\ 		&\text{Use equation $\eqref{equation0}$ to find $\frac{\partial}{\partial u} \left(\frac{\partial f}{\partial x} \right)$ and  			$\frac{\partial}{\partial u} \left(\frac{\partial f}{\partial y} \right)$.}\\ 		\frac{\partial}{\partial u} \left(\frac{\partial f}{\partial x}\right) 		&= 2u\left(\frac{\partial}{\partial x}\right) \left(\frac{\partial f}{\partial x}\right) +  		\frac{1}{v} \left(\frac{\partial}{\partial y}\right) \left(\frac{\partial f}{\partial x}\right) \\ 		&= 2u\frac{\partial^2 f}{\partial x^2} + \frac{1}{v} \frac{\partial^2 f}{\partial y \partial x} \tag{3} \label{equation2} \\ 		\frac{\partial}{\partial u} \left(\frac{\partial f}{\partial y}\right) 		&= 2u \left(\frac{\partial}{\partial x}\right) \left(\frac{\partial f}{\partial y}\right) +  		\frac{1}{v} \left(\frac{\partial}{\partial y}\right) \left(\frac{\partial f}{\partial y}\right) \\ 		&= 2u \frac{\partial^2 f}{\partial x \partial y} + \frac{1}{v} \frac{\partial^2 f}{\partial y^2} \tag{4} \label{equation3} \\		 		&\text{Substitute $\eqref{equation2}$ and $\eqref{equation3}$ into $\eqref{equation1}$.}\\ 		\frac{\partial^2 f}{\partial u^2} &= 2 \left(\frac{\partial f}{\partial x}\right) + 2u \left(2u \frac{\partial^2 f}{\partial x^2} + \frac{1}{v} \frac{\partial^2 f}{\partial y \partial x}\right) + \frac{1}{v} \left(2u \frac{\partial^2 f}{\partial x \partial y} + \frac{1}{v} \frac{\partial^2 f}{\partial y^2}\right)\\ 		&= 2 \frac{\partial f}{\partial x} + 4u^2 \frac{\partial^2 f}{\partial x^2} + \frac{2u}{v} \frac{\partial^2 f}{\partial y \partial x} + \frac{2u}{v} \frac{\partial^2 f}{\partial x \partial y} 		+ \frac{1}{v^2} \frac{\partial^2 f}{\partial y^2}\\ 		&\text{Assuming $\frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2 f}{\partial y \partial x}$}\\ 		&= 2 \frac{\partial f}{\partial x} + 4u^2 \frac{\partial^2 f}{\partial x^2} + \frac{4u}{v} \left( \frac{\partial^2 f}{\partial y \partial x} \right)	+ \frac{1}{v^2} \left(\frac{\partial^2 f}{\partial y^2}\right)\\ 		\frac{\partial f}{\partial v}  		&= \frac{\partial f}{\partial x} \frac{\partial x}{\partial v} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial v}\\ 		&=-2v\left(\frac{\partial f}{\partial x}\right) - \frac{u}{v^2} \left(\frac{\partial f}{\partial y}\right)\\ 		\frac{\partial}{\partial v}\left(f\right) &= -2v \frac{\partial}{\partial x}\left( f \right) -  \frac{u}{v^2}\frac{\partial}{\partial y}\left( f \right) \tag{5} \label{equation4}\\ 		\frac{\partial^2 f}{\partial v^2} &= \left(\frac{\partial}{\partial v}\right) \left(\frac{\partial f}{\partial v}\right)\\ 		&=-2\left(\frac{\partial f}{\partial x}\right) - 2v \left(\frac{\partial}{\partial v}\right)\left(\frac{\partial f}{\partial x}\right) + \frac{2u}{v^3}\left(\frac{\partial f}{\partial y}\right) - \\ 		& \frac{u}{v^2} \left(\frac{\partial}{\partial v}\right)\left(\frac{\partial f}{\partial y}\right) \tag{6} \label {equation5}\\ 		&\text{Use equation $\eqref{equation4}$ to find $\frac{\partial}{\partial v} \left(\frac{\partial f}{\partial x} \right)$ and $\frac{\partial}{\partial v} \left(\frac{\partial f}{\partial y} \right)$.}\\ 		\frac{\partial}{\partial v} \left(\frac{\partial f}{\partial x}\right) 		&= -2v \left(\frac{\partial}{\partial x}\right) \left(\frac{\partial f}{\partial x}\right) -  		\frac{u}{v^2} \left(\frac{\partial}{\partial y}\right) \left(\frac{\partial f}{\partial x}\right) \\ 		&= -2v \frac{\partial^2 f}{\partial x^2} - \frac{u}{v^2} \frac{\partial^2 f}{\partial y \partial x} \tag{7} \label{equation6} \\ 		\frac{\partial}{\partial v} \left(\frac{\partial f}{\partial y}\right) 		&= -2v \left(\frac{\partial}{\partial x}\right) \left(\frac{\partial f}{\partial y}\right) -  		\frac{u}{v^2} \left(\frac{\partial}{\partial y}\right) \left(\frac{\partial f}{\partial y}\right) \\ 		&= -2v \frac{\partial^2 f}{\partial x \partial y} - \frac{u}{v^2} \frac{\partial^2 f}{\partial y^2} \tag{8} \label{equation7} \\		 		&\text{Substitute $\eqref{equation6}$ and $\eqref{equation7}$ into $\eqref{equation5}$.}\\ 		\frac{\partial^2 f}{\partial v^2} &= -2 \left(\frac{\partial f}{\partial x}\right) - 2v \left(-2v \frac{\partial^2 f}{\partial x^2} - \frac{u}{v^2} \frac{\partial^2 f}{\partial y \partial x}\right) + 		\frac{2u}{v^3} \left(\frac{\partial f}{\partial y}\right) \\ 		&- \frac{u}{v^2} \left(-2v \frac{\partial^2 f}{\partial x \partial y} - \frac{u}{v^2} \frac{\partial^2 f}{\partial y^2}\right)\\ 		&= -2 \frac{\partial f}{\partial x} + 4v^2 \frac{\partial^2 f}{\partial x^2} + \frac{2u}{v} \frac{\partial^2 f}{\partial y \partial x} - \frac{2u}{v^3} \frac{\partial f}{\partial y} + \frac{2u}{v} \frac{\partial^2 f}{\partial x \partial y} \\ 		&+ \frac{u^2}{v^4} \frac{\partial^2 f}{\partial y^2}\\ 		&\text{Assuming $\frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2 f}{\partial y \partial x}$}\\ 		&= -2 \frac{\partial f}{\partial x} + 4v^2 \frac{\partial^2 f}{\partial x^2} - \frac{4u}{v} \left(\frac{\partial^2 f}{\partial y \partial x}\right) +\frac{2u}{v^3} \left(\frac{\partial f}{\partial y}\right) +  \frac{u^2}{v^4} \left(\frac{\partial^2 f}{\partial y^2}\right)\\ 	\end{align*} $$","I have tried to calculate this question and am not sure about my process being correct.  If you have the time, can you tell me if my answer is correct or not and if not where I have made my mistake(s).  Thanks in advance.","
	\begin{align*}
		f &= f(x.y), x = u^2 - v^2, y = \frac{u}{v}\\
		&\text{find f_{uu} and f_{vv}}\\
		\frac{\partial x}{\partial u} &= 2u\\
		\frac{\partial x}{\partial v} &= -2v\\
		\frac{\partial y}{\partial u} &= \frac{1}{v}\\
		\frac{\partial y}{\partial v} &= -\frac{u}{v^2}\\
		\frac{\partial f}{\partial u} &= \frac{\partial f}{\partial x} \frac{\partial x}{\partial u} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial u}\\
		&=2u\left(\frac{\partial f}{\partial x}\right) + \frac{1}{v}\left(\frac{\partial f}{\partial y}\right)\\
		\frac{\partial}{\partial u}\left(f\right) &= 2u\frac{\partial}{\partial x}\left( f \right) + \frac{1}{v}\frac{\partial}{\partial y}\left( f \right) \tag{1} \label{equation0}\\
		\frac{\partial^2 f}{\partial u^2} &= \left(\frac{\partial}{\partial u}\right) \left(\frac{\partial f}{\partial u}\right)\\
		&=2\left(\frac{\partial f}{\partial x}\right) + 2u\left(\frac{\partial}{\partial u}\right)\left(\frac{\partial f}{\partial x}\right) 
		+ \frac{1}{v}\left(\frac{\partial}{\partial u}\right)\left(\frac{\partial f}{\partial y}\right) \tag{2} \label {equation1}\\
		&\text{Use equation \eqref{equation0} to find \frac{\partial}{\partial u} \left(\frac{\partial f}{\partial x} \right) and 
			\frac{\partial}{\partial u} \left(\frac{\partial f}{\partial y} \right).}\\
		\frac{\partial}{\partial u} \left(\frac{\partial f}{\partial x}\right)
		&= 2u\left(\frac{\partial}{\partial x}\right) \left(\frac{\partial f}{\partial x}\right) + 
		\frac{1}{v} \left(\frac{\partial}{\partial y}\right) \left(\frac{\partial f}{\partial x}\right) \\
		&= 2u\frac{\partial^2 f}{\partial x^2} + \frac{1}{v} \frac{\partial^2 f}{\partial y \partial x} \tag{3} \label{equation2} \\
		\frac{\partial}{\partial u} \left(\frac{\partial f}{\partial y}\right)
		&= 2u \left(\frac{\partial}{\partial x}\right) \left(\frac{\partial f}{\partial y}\right) + 
		\frac{1}{v} \left(\frac{\partial}{\partial y}\right) \left(\frac{\partial f}{\partial y}\right) \\
		&= 2u \frac{\partial^2 f}{\partial x \partial y} + \frac{1}{v} \frac{\partial^2 f}{\partial y^2} \tag{4} \label{equation3} \\		
		&\text{Substitute \eqref{equation2} and \eqref{equation3} into \eqref{equation1}.}\\
		\frac{\partial^2 f}{\partial u^2} &= 2 \left(\frac{\partial f}{\partial x}\right) + 2u \left(2u \frac{\partial^2 f}{\partial x^2} + \frac{1}{v} \frac{\partial^2 f}{\partial y \partial x}\right) + \frac{1}{v} \left(2u \frac{\partial^2 f}{\partial x \partial y} + \frac{1}{v} \frac{\partial^2 f}{\partial y^2}\right)\\
		&= 2 \frac{\partial f}{\partial x} + 4u^2 \frac{\partial^2 f}{\partial x^2} + \frac{2u}{v} \frac{\partial^2 f}{\partial y \partial x} + \frac{2u}{v} \frac{\partial^2 f}{\partial x \partial y}
		+ \frac{1}{v^2} \frac{\partial^2 f}{\partial y^2}\\
		&\text{Assuming \frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2 f}{\partial y \partial x}}\\
		&= 2 \frac{\partial f}{\partial x} + 4u^2 \frac{\partial^2 f}{\partial x^2} + \frac{4u}{v} \left( \frac{\partial^2 f}{\partial y \partial x} \right)	+ \frac{1}{v^2} \left(\frac{\partial^2 f}{\partial y^2}\right)\\
		\frac{\partial f}{\partial v} 
		&= \frac{\partial f}{\partial x} \frac{\partial x}{\partial v} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial v}\\
		&=-2v\left(\frac{\partial f}{\partial x}\right) - \frac{u}{v^2} \left(\frac{\partial f}{\partial y}\right)\\
		\frac{\partial}{\partial v}\left(f\right) &= -2v \frac{\partial}{\partial x}\left( f \right) -  \frac{u}{v^2}\frac{\partial}{\partial y}\left( f \right) \tag{5} \label{equation4}\\
		\frac{\partial^2 f}{\partial v^2} &= \left(\frac{\partial}{\partial v}\right) \left(\frac{\partial f}{\partial v}\right)\\
		&=-2\left(\frac{\partial f}{\partial x}\right) - 2v \left(\frac{\partial}{\partial v}\right)\left(\frac{\partial f}{\partial x}\right) + \frac{2u}{v^3}\left(\frac{\partial f}{\partial y}\right) - \\
		& \frac{u}{v^2} \left(\frac{\partial}{\partial v}\right)\left(\frac{\partial f}{\partial y}\right) \tag{6} \label {equation5}\\
		&\text{Use equation \eqref{equation4} to find \frac{\partial}{\partial v} \left(\frac{\partial f}{\partial x} \right) and \frac{\partial}{\partial v} \left(\frac{\partial f}{\partial y} \right).}\\
		\frac{\partial}{\partial v} \left(\frac{\partial f}{\partial x}\right)
		&= -2v \left(\frac{\partial}{\partial x}\right) \left(\frac{\partial f}{\partial x}\right) - 
		\frac{u}{v^2} \left(\frac{\partial}{\partial y}\right) \left(\frac{\partial f}{\partial x}\right) \\
		&= -2v \frac{\partial^2 f}{\partial x^2} - \frac{u}{v^2} \frac{\partial^2 f}{\partial y \partial x} \tag{7} \label{equation6} \\
		\frac{\partial}{\partial v} \left(\frac{\partial f}{\partial y}\right)
		&= -2v \left(\frac{\partial}{\partial x}\right) \left(\frac{\partial f}{\partial y}\right) - 
		\frac{u}{v^2} \left(\frac{\partial}{\partial y}\right) \left(\frac{\partial f}{\partial y}\right) \\
		&= -2v \frac{\partial^2 f}{\partial x \partial y} - \frac{u}{v^2} \frac{\partial^2 f}{\partial y^2} \tag{8} \label{equation7} \\		
		&\text{Substitute \eqref{equation6} and \eqref{equation7} into \eqref{equation5}.}\\
		\frac{\partial^2 f}{\partial v^2} &= -2 \left(\frac{\partial f}{\partial x}\right) - 2v \left(-2v \frac{\partial^2 f}{\partial x^2} - \frac{u}{v^2} \frac{\partial^2 f}{\partial y \partial x}\right) +
		\frac{2u}{v^3} \left(\frac{\partial f}{\partial y}\right) \\
		&- \frac{u}{v^2} \left(-2v \frac{\partial^2 f}{\partial x \partial y} - \frac{u}{v^2} \frac{\partial^2 f}{\partial y^2}\right)\\
		&= -2 \frac{\partial f}{\partial x} + 4v^2 \frac{\partial^2 f}{\partial x^2} + \frac{2u}{v} \frac{\partial^2 f}{\partial y \partial x} - \frac{2u}{v^3} \frac{\partial f}{\partial y} + \frac{2u}{v} \frac{\partial^2 f}{\partial x \partial y} \\
		&+ \frac{u^2}{v^4} \frac{\partial^2 f}{\partial y^2}\\
		&\text{Assuming \frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2 f}{\partial y \partial x}}\\
		&= -2 \frac{\partial f}{\partial x} + 4v^2 \frac{\partial^2 f}{\partial x^2} - \frac{4u}{v} \left(\frac{\partial^2 f}{\partial y \partial x}\right) +\frac{2u}{v^3} \left(\frac{\partial f}{\partial y}\right) +  \frac{u^2}{v^4} \left(\frac{\partial^2 f}{\partial y^2}\right)\\
	\end{align*}
","['multivariable-calculus', 'partial-derivative', 'chain-rule']"
45,"How are Fourier Transforms, Green's Functions and the Poisson Equation related?","How are Fourier Transforms, Green's Functions and the Poisson Equation related?",,"So I have a question. It starts, as many good things in life do, with Fourier Transforms. I have been playing around with these things ever since I read somewhere that: $$ \mathcal{F} \left\{ \frac{d u}{dx} \right\}  = \omega \hat u $$ Where the notation $\hat u$ is the Fourier Transform of $u(\vec r)$ and $\omega = i \hat x$ where $\hat x$ is the reciprocal coordinate such that $\hat u = \hat u(\hat x)$ . I managed to, using the definition of Fourier Transform found in Wikipedia, convince myself that this identity is true. So I ran with it. If that thing is true, than this following expression must also be true: $$ \frac{d u}{dx} = \mathcal{F}^{-1} \{\omega \hat u\} $$ Which is a pretty weird way of differentiating something when you know how to do that symbolically, but by this point I had used discrete Fourier Transforms in a lot of grid-computations and I knew this to be quite a handy way of doing calculus with discretized functions for which I had no analytical form. So I got excited like a musician. You know what musicians do when they get excited and moved by the beauty in front of them? They compose . So I did, and here is what I got: $$ \frac{d }{dx} \left[ \frac{d u}{dx} \right] = \mathcal{F}^{-1}\{ \omega \mathcal{F}\{ \mathcal{F}^{-1} \{\omega \hat u\}\}\} $$ Which really is just: $$ \frac{d }{dx} \left[ \frac{d u}{dx} \right] = \mathcal{F}^{-1}\{ \omega^2 \hat u\} $$ This started looking a lot like this: $$ \frac{d^n u}{dx^n} = \mathcal{F}^{-1}\{ \omega^n \hat u\} $$ Which look like a general expression for the $n$ -th derivative of any function $u$ . Even one that has no analytical form. I googled for about ten minutes and figured out that, indeed. That is true. And not only that but when $n$ is negative , it turns out that it stop being a derivative and starts being an antiderivative , a definite integral. I was mindblown. Fourier, it turns out, had unified calculus under a single computation framework. It is no wonder that this operator on $u$ is called the Fourier Differintegral . Charming. Now, what else can we do with derivatives and integrals? In 1D, it turns out, not much. But things get juicy when you go full planar, adding a whole new dimension to play with. With it, comes a coordinate, $y$ , a reciprocal coordinate $\hat y$ and an imaginary reciprocal coordinate $\sigma = i \hat y$ . Ah, and also a position vector $\vec x = (x,y)$ , a reciprocal position vector $\hat {\vec x} = (\hat x,\hat y)$ and its imaginary counterpart ${\vec \omega} = i \hat {\vec x} = (\omega, \sigma)$ Now we arrive at things I pulled out of my ass. For partial derivatives, I'm doing it like this: $$ \mathcal{F}\left\{ \frac{\partial u}{\partial x} \right\} = \omega \hat u $$ Since $u$ is now a function of $\vec x$ I understand that $\hat u$ is a multidimensional Fourier Transform in 2D, and a function of $\vec \omega$ . Now, I guess that, equivalently, $$ \mathcal{F}\left\{ \frac{\partial u}{\partial y} \right\} = \sigma \hat u $$ Notice how the imaginary reciprocal variable that multiplies the transform is actually related to the variable with respect to which the derivative is being taken. I don't know for sure that this is the case. I just assumed this was right and then started computing things. One thing, in particular that I calculated was this: $$ \frac{\partial }{\partial y} \left[ \frac{\partial u}{\partial x} \right] = \mathcal{F}^{-1}\{ \sigma \mathcal{F}\{ \mathcal{F}^{-1} \{\omega \hat u\}\}\} $$ Which is really: $$ \frac{\partial }{\partial y} \left[ \frac{\partial u}{\partial x} \right] = \mathcal{F}^{-1}\{ \sigma \omega \hat u\} $$ I was then very confident to say that: $$ \frac{\partial^2 u}{\partial x^2} = \mathcal{F}^{-1}\{ \omega^2 \hat u\} $$ And likewise for $$ \frac{\partial^2 u}{\partial y^2} = \mathcal{F}^{-1}\{ \sigma^2 \hat u\} $$ And here I noticed that: $$ \nabla^2 u = \mathcal{F}^{-1} \{[\omega^2 + \sigma^2] \hat u\} = \mathcal{F}^{-1} \{ |\vec \omega|^2 \hat u\} $$ By this point I was really confident, so I decided to go up a notch and add a third dimension with coordinate $z$ , reciprocal $\hat z$ and imaginary reciprocal coordinate $\kappa$ . So in 3D I got: $$ \nabla^2 u = \mathcal{F}^{-1} \{[\omega^2 + \sigma^2 + \kappa^2] \hat u\} = |\vec \omega|^2 \hat u $$ And here I have grown arrogant (and potentially sloppy) and I thought I could solve the Poisson Equation just using this Fourier magic calculus I had uncovered, so the gods of mathematics punished me for my hubris. It went like this: Here is the Poisson Equation: $$ \nabla^2 \varphi = \rho $$ Now if I take the Fourier Transform on both sides, here is what I get: $$ |\vec \omega|^2 \hat \varphi = \hat \rho $$ Which suggests that I can just... $$ \hat \varphi = |\vec \omega|^{-2} \hat \rho $$ And taking the inverse Fourier Transform on both sides we get that: $$ \varphi = \mathcal{F}^{-1} \{ |\vec \omega|^{-2} \hat \rho \} $$ But then I found out that this type of equation is actually solved using Green's Functions, like so: $$ \varphi = -\int \int \int \frac{\rho({\vec r}')}{4\pi |\vec r - {\vec r}'|} d^3 \vec r' $$ Which looks rather... strange. Suddenly there is a 6D function ( $|\vec r - {\vec r}'|^{-1}$ ) multiplying $\rho$ and the entire thing looks like it was solved using volume integrals, which should be like this: $$ \int \int \int u d^3 \vec r = \mathcal{F}^{-1}\left\{\frac{1}{\omega \sigma \kappa}\hat u \right\} $$ And not this: $$ \varphi = \mathcal{F}^{-1} \left\{ \frac{1}{\omega^2 + \sigma^2 + \kappa^2} \hat \rho \right\} $$ ... which is the expression I had found using the Fourier Differintegral. So I can't help but ask: Where did I drop the ball? Where in my reasoning did I assume wrong things? I already know I dropped the ball somewhere, since my method did not arrive at something resembling Green's Functions, I just don't know where and I feel the answer will teach me a valuable lesson about the relationship between Fourier Transforms, Green's Functions, convolutions and Poisson's Equations. Maybe that was the lesson the gods were trying to teach me. May wisdom make humble people of us all. Cheers. EDIT 1 This edit is intended as a response to @martin 's answer. Martin, you said in your answer that: $$ \varphi = \mathcal{F}^-1\{|\vec \omega|^{-2}\} \ast \rho $$ So I went around trying to figure out what \mathcal{F}^-1{|\vec \omega|^{-2}} is and how to solve a convolution in 3D. Here's what I got: So formula 502 in this table shows me that: $$ \mathcal{F} \{ |\vec x|^{-a} \} = \frac{(2 \pi)^a}{c_{n,a}} |\vec \omega|^{-(n-a)}  $$ if $0 < \Re a < n$ where: $$ c_{n,a} = \pi^\frac{n}{2} 2^a \frac{\Gamma(a/2)}{\Gamma \left( \frac{n-a}{2} \right) } $$ I noticed that the pair $a=1$ and $n=3$ satisfies the constraint that $0 < \Re a < n$ and gives formula 502 a promising look: $$ \mathcal{F} \{ |\vec x|^{-1} \} = \frac{2 \pi}{c_{3,1}} |\vec \omega|^{-2}  $$ also, for this particular pair,  the complicated $c_{n,a}$ formula acquires a much simpler form, check it out: $$ c_{3,1} = \pi^\frac{3}{2} 2 \frac{\Gamma(1/2)}{\Gamma(1)} $$ since $\Gamma(1/2) = \sqrt{\pi}$ and $\Gamma(1) = 1$ we have that: $$ c_{3,1} = \pi^\frac{3}{2} 2 \sqrt{\pi}$ $$ And we can use the relationships between exponentiation and square roots to simplify it even further like so: $$ \pi^\frac{3}{2} \sqrt{\pi} = \pi^\frac{3}{2} \pi^\frac{1}{2} = \pi^\frac{3+1}{2} = \pi^\frac{4}{2} = \pi^2 $$ Which means that $c_{3,1}=2\pi^2$ ! How neat is that? -ahem- Anyway, we can plug that back into the formula as: $$ \mathcal{F} \{ |\vec x|^{-1} \} = \frac{2 \pi}{2\pi^2} |\vec \omega|^{-2}  $$ That warrants some simplification, which gives us $$ \mathcal{F} \{ |\vec x|^{-1} \} = \frac{1}{\pi} |\vec \omega|^{-2}  $$ Because the Fourier Transform and its inverse are integrals and $\frac{1}{\pi}$ is a constant, I assume I can do this: $$  \pi |\vec x|^{-1} = \mathcal{F}^{-1} \{|\vec \omega|^{-2} \}   $$ I can then define a function $f(\vec x) = \pi |\vec x|^{-1}$ and, going back to our Poisson equation $\nabla \varphi = \rho$ , we can state that $$ \varphi = f \ast \rho $$ Now, I didn't find any compact, understandable formula for a 3D integral, but I found this one: $$(f \ast g)(t)=\int_{-\infty}^{\infty} f(\tau) g(t-\tau) d \tau$$ And since convolutions are commutative then: $$(f \ast g)(t)=(g \ast f)(t)=\int_{-\infty}^{\infty} f(t-\tau)  g(\tau) d \tau$$ so now I'm praying that this thing, when blown to 3D dimensionality, looks like this: $$ (f \ast \rho)(\vec x)=\int\int\int_{-\infty}^{\infty} f({\vec x}-{\vec x}')  g(\vec x ') d^3 \vec x ' $$ When substituting $r=|{\vec x} - {\vec x}'|$ and $\rho(\vec x ') = \rho'$ we get that $$ \varphi = (f \ast \rho)(\vec x)=\int\int\int_{-\infty}^{\infty}\frac{\pi \rho'}{r} d^3 \vec x ' $$ Which looks different from the correct one: $$ \varphi = (f \ast \rho)(\vec x)=-\int\int\int_{-\infty}^{\infty}\frac{\rho'}{4 \pi r} d^3 \vec x ' $$ I'm thinking I dropped the ball somewhere around the transition from 1D convolutions to 3D convolutions, but IDK. Could be an incorrect usage of formula 502. Once again I have to ask Where did I drop the ball? but at least now I know where the 6D pairwise potential comes from.","So I have a question. It starts, as many good things in life do, with Fourier Transforms. I have been playing around with these things ever since I read somewhere that: Where the notation is the Fourier Transform of and where is the reciprocal coordinate such that . I managed to, using the definition of Fourier Transform found in Wikipedia, convince myself that this identity is true. So I ran with it. If that thing is true, than this following expression must also be true: Which is a pretty weird way of differentiating something when you know how to do that symbolically, but by this point I had used discrete Fourier Transforms in a lot of grid-computations and I knew this to be quite a handy way of doing calculus with discretized functions for which I had no analytical form. So I got excited like a musician. You know what musicians do when they get excited and moved by the beauty in front of them? They compose . So I did, and here is what I got: Which really is just: This started looking a lot like this: Which look like a general expression for the -th derivative of any function . Even one that has no analytical form. I googled for about ten minutes and figured out that, indeed. That is true. And not only that but when is negative , it turns out that it stop being a derivative and starts being an antiderivative , a definite integral. I was mindblown. Fourier, it turns out, had unified calculus under a single computation framework. It is no wonder that this operator on is called the Fourier Differintegral . Charming. Now, what else can we do with derivatives and integrals? In 1D, it turns out, not much. But things get juicy when you go full planar, adding a whole new dimension to play with. With it, comes a coordinate, , a reciprocal coordinate and an imaginary reciprocal coordinate . Ah, and also a position vector , a reciprocal position vector and its imaginary counterpart Now we arrive at things I pulled out of my ass. For partial derivatives, I'm doing it like this: Since is now a function of I understand that is a multidimensional Fourier Transform in 2D, and a function of . Now, I guess that, equivalently, Notice how the imaginary reciprocal variable that multiplies the transform is actually related to the variable with respect to which the derivative is being taken. I don't know for sure that this is the case. I just assumed this was right and then started computing things. One thing, in particular that I calculated was this: Which is really: I was then very confident to say that: And likewise for And here I noticed that: By this point I was really confident, so I decided to go up a notch and add a third dimension with coordinate , reciprocal and imaginary reciprocal coordinate . So in 3D I got: And here I have grown arrogant (and potentially sloppy) and I thought I could solve the Poisson Equation just using this Fourier magic calculus I had uncovered, so the gods of mathematics punished me for my hubris. It went like this: Here is the Poisson Equation: Now if I take the Fourier Transform on both sides, here is what I get: Which suggests that I can just... And taking the inverse Fourier Transform on both sides we get that: But then I found out that this type of equation is actually solved using Green's Functions, like so: Which looks rather... strange. Suddenly there is a 6D function ( ) multiplying and the entire thing looks like it was solved using volume integrals, which should be like this: And not this: ... which is the expression I had found using the Fourier Differintegral. So I can't help but ask: Where did I drop the ball? Where in my reasoning did I assume wrong things? I already know I dropped the ball somewhere, since my method did not arrive at something resembling Green's Functions, I just don't know where and I feel the answer will teach me a valuable lesson about the relationship between Fourier Transforms, Green's Functions, convolutions and Poisson's Equations. Maybe that was the lesson the gods were trying to teach me. May wisdom make humble people of us all. Cheers. EDIT 1 This edit is intended as a response to @martin 's answer. Martin, you said in your answer that: So I went around trying to figure out what \mathcal{F}^-1{|\vec \omega|^{-2}} is and how to solve a convolution in 3D. Here's what I got: So formula 502 in this table shows me that: if where: I noticed that the pair and satisfies the constraint that and gives formula 502 a promising look: also, for this particular pair,  the complicated formula acquires a much simpler form, check it out: since and we have that: And we can use the relationships between exponentiation and square roots to simplify it even further like so: Which means that ! How neat is that? -ahem- Anyway, we can plug that back into the formula as: That warrants some simplification, which gives us Because the Fourier Transform and its inverse are integrals and is a constant, I assume I can do this: I can then define a function and, going back to our Poisson equation , we can state that Now, I didn't find any compact, understandable formula for a 3D integral, but I found this one: And since convolutions are commutative then: so now I'm praying that this thing, when blown to 3D dimensionality, looks like this: When substituting and we get that Which looks different from the correct one: I'm thinking I dropped the ball somewhere around the transition from 1D convolutions to 3D convolutions, but IDK. Could be an incorrect usage of formula 502. Once again I have to ask Where did I drop the ball? but at least now I know where the 6D pairwise potential comes from.","
\mathcal{F} \left\{ \frac{d u}{dx} \right\}  = \omega \hat u
 \hat u u(\vec r) \omega = i \hat x \hat x \hat u = \hat u(\hat x) 
\frac{d u}{dx} = \mathcal{F}^{-1} \{\omega \hat u\}
 
\frac{d }{dx} \left[ \frac{d u}{dx} \right] = \mathcal{F}^{-1}\{ \omega \mathcal{F}\{ \mathcal{F}^{-1} \{\omega \hat u\}\}\}
 
\frac{d }{dx} \left[ \frac{d u}{dx} \right] = \mathcal{F}^{-1}\{ \omega^2 \hat u\}
 
\frac{d^n u}{dx^n} = \mathcal{F}^{-1}\{ \omega^n \hat u\}
 n u n u y \hat y \sigma = i \hat y \vec x = (x,y) \hat {\vec x} = (\hat x,\hat y) {\vec \omega} = i \hat {\vec x} = (\omega, \sigma) 
\mathcal{F}\left\{ \frac{\partial u}{\partial x} \right\} = \omega \hat u
 u \vec x \hat u \vec \omega 
\mathcal{F}\left\{ \frac{\partial u}{\partial y} \right\} = \sigma \hat u
 
\frac{\partial }{\partial y} \left[ \frac{\partial u}{\partial x} \right] = \mathcal{F}^{-1}\{ \sigma \mathcal{F}\{ \mathcal{F}^{-1} \{\omega \hat u\}\}\}
 
\frac{\partial }{\partial y} \left[ \frac{\partial u}{\partial x} \right] = \mathcal{F}^{-1}\{ \sigma \omega \hat u\}
 
\frac{\partial^2 u}{\partial x^2} = \mathcal{F}^{-1}\{ \omega^2 \hat u\}
 
\frac{\partial^2 u}{\partial y^2} = \mathcal{F}^{-1}\{ \sigma^2 \hat u\}
 
\nabla^2 u = \mathcal{F}^{-1} \{[\omega^2 + \sigma^2] \hat u\} = \mathcal{F}^{-1} \{ |\vec \omega|^2 \hat u\}
 z \hat z \kappa 
\nabla^2 u = \mathcal{F}^{-1} \{[\omega^2 + \sigma^2 + \kappa^2] \hat u\} = |\vec \omega|^2 \hat u
 
\nabla^2 \varphi = \rho
 
|\vec \omega|^2 \hat \varphi = \hat \rho
 
\hat \varphi = |\vec \omega|^{-2} \hat \rho
 
\varphi = \mathcal{F}^{-1} \{ |\vec \omega|^{-2} \hat \rho \}
 
\varphi = -\int \int \int \frac{\rho({\vec r}')}{4\pi |\vec r - {\vec r}'|} d^3 \vec r'
 |\vec r - {\vec r}'|^{-1} \rho 
\int \int \int u d^3 \vec r = \mathcal{F}^{-1}\left\{\frac{1}{\omega \sigma \kappa}\hat u \right\}
 
\varphi = \mathcal{F}^{-1} \left\{ \frac{1}{\omega^2 + \sigma^2 + \kappa^2} \hat \rho \right\}
 
\varphi = \mathcal{F}^-1\{|\vec \omega|^{-2}\} \ast \rho
 
\mathcal{F} \{ |\vec x|^{-a} \} = \frac{(2 \pi)^a}{c_{n,a}} |\vec \omega|^{-(n-a)} 
 0 < \Re a < n 
c_{n,a} = \pi^\frac{n}{2} 2^a \frac{\Gamma(a/2)}{\Gamma \left( \frac{n-a}{2} \right) }
 a=1 n=3 0 < \Re a < n 
\mathcal{F} \{ |\vec x|^{-1} \} = \frac{2 \pi}{c_{3,1}} |\vec \omega|^{-2} 
 c_{n,a} 
c_{3,1} = \pi^\frac{3}{2} 2 \frac{\Gamma(1/2)}{\Gamma(1)}
 \Gamma(1/2) = \sqrt{\pi} \Gamma(1) = 1 
c_{3,1} = \pi^\frac{3}{2} 2 \sqrt{\pi}
 
\pi^\frac{3}{2} \sqrt{\pi} = \pi^\frac{3}{2} \pi^\frac{1}{2} = \pi^\frac{3+1}{2} = \pi^\frac{4}{2} = \pi^2
 c_{3,1}=2\pi^2 
\mathcal{F} \{ |\vec x|^{-1} \} = \frac{2 \pi}{2\pi^2} |\vec \omega|^{-2} 
 
\mathcal{F} \{ |\vec x|^{-1} \} = \frac{1}{\pi} |\vec \omega|^{-2} 
 \frac{1}{\pi} 
 \pi |\vec x|^{-1} = \mathcal{F}^{-1} \{|\vec \omega|^{-2} \}  
 f(\vec x) = \pi |\vec x|^{-1} \nabla \varphi = \rho 
\varphi = f \ast \rho
 (f \ast g)(t)=\int_{-\infty}^{\infty} f(\tau) g(t-\tau) d \tau (f \ast g)(t)=(g \ast f)(t)=\int_{-\infty}^{\infty} f(t-\tau)  g(\tau) d \tau 
(f \ast \rho)(\vec x)=\int\int\int_{-\infty}^{\infty} f({\vec x}-{\vec x}')  g(\vec x ') d^3 \vec x '
 r=|{\vec x} - {\vec x}'| \rho(\vec x ') = \rho' 
\varphi = (f \ast \rho)(\vec x)=\int\int\int_{-\infty}^{\infty}\frac{\pi \rho'}{r} d^3 \vec x '
 
\varphi = (f \ast \rho)(\vec x)=-\int\int\int_{-\infty}^{\infty}\frac{\rho'}{4 \pi r} d^3 \vec x '
","['multivariable-calculus', 'partial-differential-equations', 'integral-transforms']"
46,Continuous function with partial derivative that is continuous in one variable but not the other,Continuous function with partial derivative that is continuous in one variable but not the other,,"Does there exist a function $f: \mathbb R^2 \to \mathbb R$ with the following property $f$ is continuous $\frac{\partial f}{\partial x}$ exists For each fixed $y$ , $\frac{\partial f}{\partial x}(x,y)$ is a continuous function of $x$ $\frac{\partial f}{\partial x}(0,y)$ is not a continuous function of $y$ I suspect the answer is yes, i.e. the first three properties do not imply the fourth since I don't see how to prove that, but I can't think of an easy example.","Does there exist a function with the following property is continuous exists For each fixed , is a continuous function of is not a continuous function of I suspect the answer is yes, i.e. the first three properties do not imply the fourth since I don't see how to prove that, but I can't think of an easy example.","f: \mathbb R^2 \to \mathbb R f \frac{\partial f}{\partial x} y \frac{\partial f}{\partial x}(x,y) x \frac{\partial f}{\partial x}(0,y) y","['multivariable-calculus', 'continuity', 'partial-derivative']"
47,Lagrange multiplier for QCQP with $1$ equality constraint,Lagrange multiplier for QCQP with  equality constraint,1,"I want to find the maximum of $f(x,y) = x^2 - y^2$ under the constraint $\frac12 x^2 + y^2 - 1 = 0$ . I defined Lagrange function: $$ L= x^2 - y^2 + \lambda \left( \frac12 x^2 + y^2 - 1 \right) $$ Then caculated $L'_x, L'_y, L'_{\lambda}$ and got the following equations system: $$ \begin{aligned} x (2 + \lambda) &= 0 \\ 2 y (\lambda - 1) &= 0 \\ \frac12 x^2 + y^2 - 1 &= 0 \end{aligned} $$ How to solve this system so I can find the maximizer $(x,y)$ ?",I want to find the maximum of under the constraint . I defined Lagrange function: Then caculated and got the following equations system: How to solve this system so I can find the maximizer ?,"f(x,y) = x^2 - y^2 \frac12 x^2 + y^2 - 1 = 0  L= x^2 - y^2 + \lambda \left( \frac12 x^2 + y^2 - 1 \right)  L'_x, L'_y, L'_{\lambda}  \begin{aligned} x (2 + \lambda) &= 0 \\ 2 y (\lambda - 1) &= 0 \\ \frac12 x^2 + y^2 - 1 &= 0 \end{aligned}  (x,y)","['calculus', 'multivariable-calculus']"
48,"Vector identity for $\dfrac{\mathrm d}{\mathrm dt}\nabla \varphi(\vec r(t),t) $",Vector identity for,"\dfrac{\mathrm d}{\mathrm dt}\nabla \varphi(\vec r(t),t) ","Suppose I have a scalar field $\varphi\left(\vec r(t), t\right)$ where $\vec r$ is a vector in $\mathbb R^3$ that depends on the parameter $t$ . I'd like to figure out what $\dfrac{\mathrm d}{\mathrm dt}\nabla \varphi$ is. I start as follows, $$\nabla \varphi = \partial_i \varphi \hat{e}^i $$ (which I attempt to write in Einstein notation) From there, I take the time derivative component-wise and using the directional derivative I get $$\dfrac{\mathrm d}{\mathrm dt}\nabla \varphi = \left( v_j \partial _j \partial_i \varphi   +\partial _t \partial _i \varphi  \right)\hat{e}^i$$ where $v^\nu = \dfrac{\mathrm d r^\nu }{\mathrm dt} $ . I can rewrite this as (where $^T$ denotes the transpose) $$\dfrac{\mathrm d\nabla\varphi }{\mathrm dt} = \pmatrix{\nabla \partial _x\varphi^T \\ \nabla \partial _y \varphi ^T \\ \nabla \partial_z \varphi ^T }\vec v+\dfrac{\partial}{\partial t}\nabla \varphi$$ and am wondering if there is a way for me to simplify this further, in particular if there is a notation or name for the $\pmatrix{\nabla \partial _x\varphi^T \\ \nabla \partial _y \varphi ^T \\ \nabla \partial_z \varphi ^T }$ matrix.","Suppose I have a scalar field where is a vector in that depends on the parameter . I'd like to figure out what is. I start as follows, (which I attempt to write in Einstein notation) From there, I take the time derivative component-wise and using the directional derivative I get where . I can rewrite this as (where denotes the transpose) and am wondering if there is a way for me to simplify this further, in particular if there is a notation or name for the matrix.","\varphi\left(\vec r(t), t\right) \vec r \mathbb R^3 t \dfrac{\mathrm d}{\mathrm dt}\nabla \varphi \nabla \varphi = \partial_i \varphi \hat{e}^i  \dfrac{\mathrm d}{\mathrm dt}\nabla \varphi = \left( v_j \partial _j \partial_i \varphi   +\partial _t \partial _i \varphi  \right)\hat{e}^i v^\nu = \dfrac{\mathrm d r^\nu }{\mathrm dt}  ^T \dfrac{\mathrm d\nabla\varphi }{\mathrm dt} = \pmatrix{\nabla \partial _x\varphi^T \\ \nabla \partial _y \varphi ^T \\ \nabla \partial_z \varphi ^T }\vec v+\dfrac{\partial}{\partial t}\nabla \varphi \pmatrix{\nabla \partial _x\varphi^T \\ \nabla \partial _y \varphi ^T \\ \nabla \partial_z \varphi ^T }","['calculus', 'linear-algebra', 'multivariable-calculus']"
49,True or False question for Fubini's Theorem,True or False question for Fubini's Theorem,,"a) Is it true that $$\iint_{R} f(x,y)dydx=\iint_{R} f(x,y)dxdy$$ I thought this would be true because of Fubini's Theorem however Fubini's Theorem requires $f(x,y)$ to be continuous on $R$ . There is no such condition in this problem. Therefore it is false? b) If $R$ is the rectangle $0\leq x\leq a, 0\leq y\leq b$ and $S$ is the rectangle $-a\leq x\leq 0, -b \leq y\leq 0$ then $$\iint_{R} f(x,y)dA=-\iint_{S} f(x,y) dA$$ I think this is false and the counter example I would use is $f(x,y)=1$ Then $\int_{0}^{b}\int_{0}^{a} dydx = \int_{-b}^{0}\int_{-a}^{0}dydx$ Is this correct?",a) Is it true that I thought this would be true because of Fubini's Theorem however Fubini's Theorem requires to be continuous on . There is no such condition in this problem. Therefore it is false? b) If is the rectangle and is the rectangle then I think this is false and the counter example I would use is Then Is this correct?,"\iint_{R} f(x,y)dydx=\iint_{R} f(x,y)dxdy f(x,y) R R 0\leq x\leq a, 0\leq y\leq b S -a\leq x\leq 0, -b \leq y\leq 0 \iint_{R} f(x,y)dA=-\iint_{S} f(x,y) dA f(x,y)=1 \int_{0}^{b}\int_{0}^{a} dydx = \int_{-b}^{0}\int_{-a}^{0}dydx",['multivariable-calculus']
50,Integration by parts with $\Delta^{-1}$ and $\nabla^{-1}$,Integration by parts with  and,\Delta^{-1} \nabla^{-1},"Let $u:\mathbb R^n \to \mathbb R$ be smooth and $A:\mathbb R \to \mathbb R^n$ be Lipschitz. How can we estimate the quantity $$\int_{\mathbb R^n} \nabla \cdot (A(u)) \Delta^{-1}udx$$ from below in terms of $\|\nabla^{-1}u\|_{2}$ and the Lipschitz norm of $A$ ? Is an estimate from above also available? In the simpler case when we replace $A(u)=a(x)u$ with a Lipschitz function $a:\mathbb R^n \to \mathbb R^n$ with $\mathrm{div}(a) = 0$ , I already know how to do it: let $\phi=\Delta^{-1} u$ and compute \begin{align*} \begin{aligned} \int a \cdot \nabla u \Delta^{-1} u d x &=\int \nabla \cdot(a u) \Delta^{-1} u \\ &=\int \nabla \cdot(a \Delta \phi) \phi \\ &=-\int \sum_{i, j} \partial_{i}\left(a^{i} \partial_{j j} \phi\right) \phi \\ &=-\int \sum_{i, j} a^{i} \partial_{j j} \phi \partial_{i} \phi \\ &=\int \sum_{i, j} \partial_{j}\left(a^{i} \partial_{i} \phi\right) \partial_{j} \phi \\ &=\int \sum_{i, j} \partial_{j} a^{i}\left|\partial_{i} \phi\right|^{2}+\int u^{i} \partial_{i} \frac{\left|\partial_{j} \phi\right|^{2}}{2} d x \\ &=\int \nabla^{-1} u\cdot \nabla a \cdot \nabla^{-1} u d x \end{aligned} \end{align*} which gives the required estimate in terms of $\|\nabla^{-1}u\|_{2}$ and $\|\nabla a\|_\infty$ , that is $... \ge - \|\nabla^{-1}u\|_{2}^2\|\nabla a\|_\infty$ and $... \le \|\nabla^{-1}u\|_{2}^2\|\nabla a\|_\infty$","Let be smooth and be Lipschitz. How can we estimate the quantity from below in terms of and the Lipschitz norm of ? Is an estimate from above also available? In the simpler case when we replace with a Lipschitz function with , I already know how to do it: let and compute which gives the required estimate in terms of and , that is and","u:\mathbb R^n \to \mathbb R A:\mathbb R \to \mathbb R^n \int_{\mathbb R^n} \nabla \cdot (A(u)) \Delta^{-1}udx \|\nabla^{-1}u\|_{2} A A(u)=a(x)u a:\mathbb R^n \to \mathbb R^n \mathrm{div}(a) = 0 \phi=\Delta^{-1} u \begin{align*}
\begin{aligned}
\int a \cdot \nabla u \Delta^{-1} u d x &=\int \nabla \cdot(a u) \Delta^{-1} u \\
&=\int \nabla \cdot(a \Delta \phi) \phi \\
&=-\int \sum_{i, j} \partial_{i}\left(a^{i} \partial_{j j} \phi\right) \phi \\
&=-\int \sum_{i, j} a^{i} \partial_{j j} \phi \partial_{i} \phi \\
&=\int \sum_{i, j} \partial_{j}\left(a^{i} \partial_{i} \phi\right) \partial_{j} \phi \\
&=\int \sum_{i, j} \partial_{j} a^{i}\left|\partial_{i} \phi\right|^{2}+\int u^{i} \partial_{i} \frac{\left|\partial_{j} \phi\right|^{2}}{2} d x \\
&=\int \nabla^{-1} u\cdot \nabla a \cdot \nabla^{-1} u d x
\end{aligned}
\end{align*} \|\nabla^{-1}u\|_{2} \|\nabla a\|_\infty ... \ge - \|\nabla^{-1}u\|_{2}^2\|\nabla a\|_\infty ... \le \|\nabla^{-1}u\|_{2}^2\|\nabla a\|_\infty","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'lebesgue-integral']"
51,How do you represent an axis as a vector?,How do you represent an axis as a vector?,,"I am Alice. In a question I am doing, it requires me to find the directional derivative of function $f$ at $(2, 3, 4)$ , in the direction of positive $z$ axis. So I trying to figure out, how to represent an axis as a vector in general. In case, I want the positive $z$ , how can I do that? $z = (0, 0, 1)$ Would this suffice? Why? I don't understand. Thanks all.","I am Alice. In a question I am doing, it requires me to find the directional derivative of function at , in the direction of positive axis. So I trying to figure out, how to represent an axis as a vector in general. In case, I want the positive , how can I do that? Would this suffice? Why? I don't understand. Thanks all.","f (2, 3, 4) z z z = (0, 0, 1)","['multivariable-calculus', 'vectors']"
52,Applying Leibniz's Rule to an Integral With Multiple Parameters,Applying Leibniz's Rule to an Integral With Multiple Parameters,,"I was given the following exercise and wasn't really able to make heads-or-tails out of it. It goes like so: Let $f: \mathbb{R}^3 \rightarrow \mathbb{R}$ be twice differentiable continuously , and statisfy the following: \begin{align*}     \quad & \forall (x,y,z) \in \mathbb{R}^3 \\     (*) \quad & \frac{\partial f}{\partial y}(x,y,z) = \frac{\partial^2 f}{\partial x^2}(x,y,z) \end{align*} Then define: \begin{gather*}     g: \mathbb{R}^2 \rightarrow \mathbb{R} \\     (x,y) \rightarrow \int_0^y f(x,y-z,z)dz \end{gather*} Prove for all $(x,y) \in \mathbb{R}^2$ \begin{gather*} \frac{\partial g}{\partial y}(x,y) = \frac{\partial^2 g}{\partial x^2}(x,y) + f(x,0,y) \end{gather*} I do know this question must involve Leibniz's Rule of Integration Under The Integral Sign , yet I am not sure how this is applicable here. The only sensible step to take is to fix $g$ for both variables and derive using Leibniz's rule (as we know $g$ is continuously differentiable twice in $\mathbb{R}^3$ ). So for the second variable: \begin{gather*}     \frac{\partial g}{\partial y} =  f(y,y,z)\cdot\frac{d}{dy}y + \int_0^y\frac{\partial f}{\partial y}(x,y-z,z)dz \end{gather*} For the first variable it seemed to me that I should derive twice using Leibniz's rule so I can apply $(*)$ , and I got: \begin{gather*} \frac{\partial^2 g}{\partial x^2} = \int_0^y\frac{\partial^2 f}{\partial x^2}(x,y-z,z)dz \end{gather*} We can easily see that by applying $(*)$ we get close to the required term, yet it is not quite there. I assume my evaluation of $\dfrac{\partial g}{\partial y}$ is incorrect. So first and foremost, how would one comprehensively justify using Leibniz's rule here in the preceding manner - it is not your usual case, there are two parameters in the integral represented by $g$ . Is it even justifiable here? And secondly, did I evaluate $\dfrac{\partial g}{\partial y}$ wrongly, or have I just missed something and the evaluation is incomplete? Any hint or explanation would be extremely appreciated. Thank you so much, and have a great day!","I was given the following exercise and wasn't really able to make heads-or-tails out of it. It goes like so: Let be twice differentiable continuously , and statisfy the following: Then define: Prove for all I do know this question must involve Leibniz's Rule of Integration Under The Integral Sign , yet I am not sure how this is applicable here. The only sensible step to take is to fix for both variables and derive using Leibniz's rule (as we know is continuously differentiable twice in ). So for the second variable: For the first variable it seemed to me that I should derive twice using Leibniz's rule so I can apply , and I got: We can easily see that by applying we get close to the required term, yet it is not quite there. I assume my evaluation of is incorrect. So first and foremost, how would one comprehensively justify using Leibniz's rule here in the preceding manner - it is not your usual case, there are two parameters in the integral represented by . Is it even justifiable here? And secondly, did I evaluate wrongly, or have I just missed something and the evaluation is incomplete? Any hint or explanation would be extremely appreciated. Thank you so much, and have a great day!","f: \mathbb{R}^3 \rightarrow \mathbb{R} \begin{align*}
    \quad & \forall (x,y,z) \in \mathbb{R}^3 \\
    (*) \quad & \frac{\partial f}{\partial y}(x,y,z) = \frac{\partial^2 f}{\partial x^2}(x,y,z)
\end{align*} \begin{gather*}
    g: \mathbb{R}^2 \rightarrow \mathbb{R} \\
    (x,y) \rightarrow \int_0^y f(x,y-z,z)dz
\end{gather*} (x,y) \in \mathbb{R}^2 \begin{gather*}
\frac{\partial g}{\partial y}(x,y) = \frac{\partial^2 g}{\partial x^2}(x,y) + f(x,0,y)
\end{gather*} g g \mathbb{R}^3 \begin{gather*}
    \frac{\partial g}{\partial y} =  f(y,y,z)\cdot\frac{d}{dy}y + \int_0^y\frac{\partial f}{\partial y}(x,y-z,z)dz
\end{gather*} (*) \begin{gather*}
\frac{\partial^2 g}{\partial x^2} = \int_0^y\frac{\partial^2 f}{\partial x^2}(x,y-z,z)dz
\end{gather*} (*) \dfrac{\partial g}{\partial y} g \dfrac{\partial g}{\partial y}","['real-analysis', 'integration', 'multivariable-calculus', 'leibniz-integral-rule']"
53,"Expanding $\int_{\Omega}|D^2u|^2(x,t)\;\mathrm{d}x$",Expanding,"\int_{\Omega}|D^2u|^2(x,t)\;\mathrm{d}x","I'm having some difficulty expanding the following integral: $$\int_{\Omega}|D^2u|^2(x,t)\;\mathrm{d}x=\int_{\Omega}(D^2u\cdot D^2u)(x,t)\;\mathrm{d}x$$ where $\Omega\subset\mathbb{R}^n$ and $u$ is in $\mathbb{R}^n$ . I want to use integration by parts (which I believe is the way to go here) but haven't been able to start anywhere. I'm very familiar with using integration by parts to solve the related integral $$\int_{\Omega}Du\cdot Dv\;dx=-\int_{\Omega} u\Delta v\;\mathrm{d}x+\int_{\partial\Omega}\frac{\partial v}{\partial\nu}u\;\mathrm{d}s$$ but I haven't been able to achieve the same success in this scenario. Could any of Green's Formulas help? Thanks!",I'm having some difficulty expanding the following integral: where and is in . I want to use integration by parts (which I believe is the way to go here) but haven't been able to start anywhere. I'm very familiar with using integration by parts to solve the related integral but I haven't been able to achieve the same success in this scenario. Could any of Green's Formulas help? Thanks!,"\int_{\Omega}|D^2u|^2(x,t)\;\mathrm{d}x=\int_{\Omega}(D^2u\cdot D^2u)(x,t)\;\mathrm{d}x \Omega\subset\mathbb{R}^n u \mathbb{R}^n \int_{\Omega}Du\cdot Dv\;dx=-\int_{\Omega} u\Delta v\;\mathrm{d}x+\int_{\partial\Omega}\frac{\partial v}{\partial\nu}u\;\mathrm{d}s","['calculus', 'multivariable-calculus', 'partial-differential-equations']"
54,Multivariable Taylor Expansion and Optimization Algorithms (Newton's Method / Steepest Descent / Conjugate Gradient),Multivariable Taylor Expansion and Optimization Algorithms (Newton's Method / Steepest Descent / Conjugate Gradient),,"I'm currently teaching myself about second-order optimization in the context of machine learning. One common application of second-order optimization is minimizing the cost function of say, a neural network or some other machine learning algorithm. This is just an optimization problem in some large nonlinear function, where the parameters to the function (the variables) are the weights and biases in the network. One common way of doing things is Newton's Method - to repeatedly create quadratic approximations of the nonlinear space using the multivariate Taylor Series to create a local quadratic approximation. Then, you directly minimize that. Then, create another approximation, until you reach the minima of the nonlinear function. The function that creates the approximation is exactly the multivariate Taylor's theorem, dictated by this: This is all fine and good, and makes sense. The problem is that optimizing this directly (by setting gradient to 0) involves taking the inverse of the Hessian, which is computationally expensive. That pure update rule is found by setting the gradient of this approximation w.r.t x as 0, and you get this: So, many algorithms, like Hessian-Free Optimization and L-BFGS opt to approximate the Hessian, to use some ""Quazi-newton Method"", where we still continuously make approximations and find the minimum, except this minimum is not calculated exactly. Often used is either the Steepest Descent or Conjugate Gradients methods of minimizing this approximated function (which avoids directly calculating the inverse hessian). The thing is, all the steepest descent and conjugate gradient things I'm learning about are built to minimize functions of the form Although it looks VERY similar to the Taylor Expansion quadratic approximation which I'm trying to apply steepest descent / conjugate gradients to, it's slightly different. a) all the vectors are $x$ instead of $(x-x_0)$ - how do I deal with that? b) there is a negative sign in front of $b^{T}x$ My question is: Since the use case for steepest descent / conjugate gradients is slightly different than the paraboloid created by the Taylor Expansion, how do I modify steepest descent / conjugate gradients OR the Taylor Expansion approximation to be able to apply conjugate gradients / steepest descent to the approximations created by the Taylor Expansion?","I'm currently teaching myself about second-order optimization in the context of machine learning. One common application of second-order optimization is minimizing the cost function of say, a neural network or some other machine learning algorithm. This is just an optimization problem in some large nonlinear function, where the parameters to the function (the variables) are the weights and biases in the network. One common way of doing things is Newton's Method - to repeatedly create quadratic approximations of the nonlinear space using the multivariate Taylor Series to create a local quadratic approximation. Then, you directly minimize that. Then, create another approximation, until you reach the minima of the nonlinear function. The function that creates the approximation is exactly the multivariate Taylor's theorem, dictated by this: This is all fine and good, and makes sense. The problem is that optimizing this directly (by setting gradient to 0) involves taking the inverse of the Hessian, which is computationally expensive. That pure update rule is found by setting the gradient of this approximation w.r.t x as 0, and you get this: So, many algorithms, like Hessian-Free Optimization and L-BFGS opt to approximate the Hessian, to use some ""Quazi-newton Method"", where we still continuously make approximations and find the minimum, except this minimum is not calculated exactly. Often used is either the Steepest Descent or Conjugate Gradients methods of minimizing this approximated function (which avoids directly calculating the inverse hessian). The thing is, all the steepest descent and conjugate gradient things I'm learning about are built to minimize functions of the form Although it looks VERY similar to the Taylor Expansion quadratic approximation which I'm trying to apply steepest descent / conjugate gradients to, it's slightly different. a) all the vectors are instead of - how do I deal with that? b) there is a negative sign in front of My question is: Since the use case for steepest descent / conjugate gradients is slightly different than the paraboloid created by the Taylor Expansion, how do I modify steepest descent / conjugate gradients OR the Taylor Expansion approximation to be able to apply conjugate gradients / steepest descent to the approximations created by the Taylor Expansion?",x (x-x_0) b^{T}x,"['multivariable-calculus', 'optimization', 'taylor-expansion', 'convex-optimization', 'newton-raphson']"
55,Characterizing (stationary) points by the number of valleys one can descent into,Characterizing (stationary) points by the number of valleys one can descent into,,"In non-convex optimizing of more than 2 times differentiable $f: \mathbb{R}^2 \mapsto \mathbb{R}$ we can encounter saddle points that have multiple valley one could descent into. At $(0,0)$ there are two direction of descent: (0,-1) and (0,1). However the number of valley one could of descent can be larger than the number of dimensions as the the monkey saddle illustrates: I want to distinguish descending into different valleys and from the number of directions as descent. If we look at a Unit Box of the Rastrigin function we see that the local maxima at (?.5,?.5) can descent in any direction but the axis aligned directions lead into saddle points, if we ignores those we have non connected open sets corresponding to each attractor of a ""not getting stuck in saddle points"" optimizers (Newton method, sufficiently perturbed gradient descent, ...). What is the name for the concept that would assign $(0,0)$ of the normal saddle a 2, the monkey saddle at (0,0) a 3 and any point (?.5, ?.5) a 4 on the Rastrigin function? Context: I am motivated to investigate this property over non stationary points. However i need a name for it first to see if there is existing literature. Any point along the x-axis of normal saddle point could be assigned a two, any point of the (strictly) positive x-axis of the monkey saddle could be assigned a 2 too. Any point of with only one component ending in .5 on the Rastrigin function would be a 2 too. I suspect that for an $f: \mathbb{R}^n \mapsto \mathbb{R}$ there exist a $\mathbb{R}^{n-1}$ dimensional (non connected) pseudo(?) manifold the contains all points that have more than two valleys next to them. All points which have exactly two neighboring valleys are part of n-dimensional hyper surfaces which divide different basins of attractions of local optima. Such surfaces might meet with other surfaces to from ""edges"" connecting the surfaces. I hope that finding that partioning structure can be used to accelerate global optimization of non-convex functions. A related but more Greedy (descents and forks but doesn't ascend) approach called "" Ridge Rider "" is already known in the literature and used to find more diverse solutions to optimization problems.","In non-convex optimizing of more than 2 times differentiable we can encounter saddle points that have multiple valley one could descent into. At there are two direction of descent: (0,-1) and (0,1). However the number of valley one could of descent can be larger than the number of dimensions as the the monkey saddle illustrates: I want to distinguish descending into different valleys and from the number of directions as descent. If we look at a Unit Box of the Rastrigin function we see that the local maxima at (?.5,?.5) can descent in any direction but the axis aligned directions lead into saddle points, if we ignores those we have non connected open sets corresponding to each attractor of a ""not getting stuck in saddle points"" optimizers (Newton method, sufficiently perturbed gradient descent, ...). What is the name for the concept that would assign of the normal saddle a 2, the monkey saddle at (0,0) a 3 and any point (?.5, ?.5) a 4 on the Rastrigin function? Context: I am motivated to investigate this property over non stationary points. However i need a name for it first to see if there is existing literature. Any point along the x-axis of normal saddle point could be assigned a two, any point of the (strictly) positive x-axis of the monkey saddle could be assigned a 2 too. Any point of with only one component ending in .5 on the Rastrigin function would be a 2 too. I suspect that for an there exist a dimensional (non connected) pseudo(?) manifold the contains all points that have more than two valleys next to them. All points which have exactly two neighboring valleys are part of n-dimensional hyper surfaces which divide different basins of attractions of local optima. Such surfaces might meet with other surfaces to from ""edges"" connecting the surfaces. I hope that finding that partioning structure can be used to accelerate global optimization of non-convex functions. A related but more Greedy (descents and forks but doesn't ascend) approach called "" Ridge Rider "" is already known in the literature and used to find more diverse solutions to optimization problems.","f: \mathbb{R}^2 \mapsto \mathbb{R} (0,0) (0,0) f: \mathbb{R}^n \mapsto \mathbb{R} \mathbb{R}^{n-1}","['multivariable-calculus', 'reference-request', 'perturbation-theory', 'non-convex-optimization', 'stationary-point']"
56,Show the inequality : $a^{(2(1-a))}+b^{(2(1-b))}+c^{(2(1-c))}+c\leq 1$,Show the inequality :,a^{(2(1-a))}+b^{(2(1-b))}+c^{(2(1-c))}+c\leq 1,"Claim : Let $0.5\geq a \geq b \geq 0.25\geq c\geq 0$ such that $a+b+c=1$ then we have : $$a^{(2(1-a))}+b^{(2(1-b))}+c^{(2(1-c))}+c\leq 1$$ To prove it I have tried Bernoulli's inequality . For $0\leq x\leq 0.25$ we have : $$x^{2(1-x)}\leq 2x^2$$ As in my previous posts we have the inequality $x\in[0,0.5]$ : $$x^{2(1-x)}\leq 2^{2x+1}x^2(1-x)$$ applying this for each variables $a,b$ we want to show : $$2^{2a+1}a^2(1-a)+2^{2b+1}b^2(1-b)+2c^2+c\leq 1$$ Now by Bernoulli's inequality we have: $$2^{2x+1}\leq 2(1+2x)$$ Remains to show : $$2(1+2a)a^2(1-a)+2(1+2b)b^2(1-b)+2c^2+c\leq 1\quad(1)$$ The function : $$f(x)=2(1+2x)x^2(1-x)$$ is concave for $x\in [\frac{1}{8}+\frac{\sqrt{\frac{19}{3}}}{8},0.5]$ So we can use Jensen's inequality remains to show : $$2\left(2(1+a+b)\left(\frac{a+b}{2}\right)^2\left(1-\left(\frac{a+b}{2}\right)\right)\right)+2c^2+c\leq 1$$ So it reduces to a one variable inequality and using derivatives it's not hard to show that : $$g(c)=2f\left(\frac{1-c}{2}\right)+2c^2+c\leq 1$$ For $c\in[0,1-2\left(\frac{1}{8}+\frac{\sqrt{\frac{19}{3}}}{8}\right)]$ It shows the equality case $a=b=0.5$ and $c=0$ but inequality $(1)$ is false for the other equality case $a=0.5$ and $b=c=0.25$ . We have also the inequality for $x\in[0.25,0.5]$ (we can prove it using logarithm and then derivative) $$x^{(2(1-x))}\leq x^22^{-5(x-0.25)(x-0.5)+1}$$ Using Bernoulli's inequality : $$x^22^{-5(x-0.25)(x-0.5)+1}\leq 2(x^2+x^2(-5(x-0.25)(x-0.5)))$$ So Remains to show : $$2(a^2+a^2(-5(a-0.25)(a-0.5)))+2(b^2+b^2(-5(b-0.25)(b-0.5)))+2c^2+c\leq 1\quad (2)$$ Question : Have you a proof ? How to show $(2)$ ? Thanks in advance !",Claim : Let such that then we have : To prove it I have tried Bernoulli's inequality . For we have : As in my previous posts we have the inequality : applying this for each variables we want to show : Now by Bernoulli's inequality we have: Remains to show : The function : is concave for So we can use Jensen's inequality remains to show : So it reduces to a one variable inequality and using derivatives it's not hard to show that : For It shows the equality case and but inequality is false for the other equality case and . We have also the inequality for (we can prove it using logarithm and then derivative) Using Bernoulli's inequality : So Remains to show : Question : Have you a proof ? How to show ? Thanks in advance !,"0.5\geq a \geq b \geq 0.25\geq c\geq 0 a+b+c=1 a^{(2(1-a))}+b^{(2(1-b))}+c^{(2(1-c))}+c\leq 1 0\leq x\leq 0.25 x^{2(1-x)}\leq 2x^2 x\in[0,0.5] x^{2(1-x)}\leq 2^{2x+1}x^2(1-x) a,b 2^{2a+1}a^2(1-a)+2^{2b+1}b^2(1-b)+2c^2+c\leq 1 2^{2x+1}\leq 2(1+2x) 2(1+2a)a^2(1-a)+2(1+2b)b^2(1-b)+2c^2+c\leq 1\quad(1) f(x)=2(1+2x)x^2(1-x) x\in [\frac{1}{8}+\frac{\sqrt{\frac{19}{3}}}{8},0.5] 2\left(2(1+a+b)\left(\frac{a+b}{2}\right)^2\left(1-\left(\frac{a+b}{2}\right)\right)\right)+2c^2+c\leq 1 g(c)=2f\left(\frac{1-c}{2}\right)+2c^2+c\leq 1 c\in[0,1-2\left(\frac{1}{8}+\frac{\sqrt{\frac{19}{3}}}{8}\right)] a=b=0.5 c=0 (1) a=0.5 b=c=0.25 x\in[0.25,0.5] x^{(2(1-x))}\leq x^22^{-5(x-0.25)(x-0.5)+1} x^22^{-5(x-0.25)(x-0.5)+1}\leq 2(x^2+x^2(-5(x-0.25)(x-0.5))) 2(a^2+a^2(-5(a-0.25)(a-0.5)))+2(b^2+b^2(-5(b-0.25)(b-0.5)))+2c^2+c\leq 1\quad (2) (2)","['multivariable-calculus', 'derivatives', 'inequality', 'exponentiation', 'jensen-inequality']"
57,Set of Discontinuities for Thomae's function in $\mathbb{R}^2$,Set of Discontinuities for Thomae's function in,\mathbb{R}^2,"For this question, Label: Part A : Is $f(x,y)$ integrable? Question $3$ - $7$ from Spivak's Calculus on Manifolds Part B : Prove $f:[0,1]\times[0,1]→\mathbb{R}$ is integrable. For part A and B question, I believe that the set of points of discontinuity is exactly equal to $[0,1]\times [0,1]\cap \mathbb{Q}\times \mathbb{Q}$ because part A is the extension of Thomae's funtion to $\mathbb{R}^2$ and part B is produced by taking a cross product of Thomae's funtion . But when I use the sequential continuity definition, I get a larger set. For Eg : For this part A; Let $p_n$ be a sequence of rational points converging to $\frac{1}{\sqrt{2}}$ but the sequence $f\left(p_n,\frac{1}{2}\right)=\frac{1}{2}$ doesn't converge to zero i.e. $f\left(\frac{1}{\sqrt{2}},\frac{1}{2}\right)=0$ Is there something wrong with the method? Else if the set is larger, what does the set look like? Any Help will be appreciated. Thanks in advance.","For this question, Label: Part A : Is integrable? Question - from Spivak's Calculus on Manifolds Part B : Prove is integrable. For part A and B question, I believe that the set of points of discontinuity is exactly equal to because part A is the extension of Thomae's funtion to and part B is produced by taking a cross product of Thomae's funtion . But when I use the sequential continuity definition, I get a larger set. For Eg : For this part A; Let be a sequence of rational points converging to but the sequence doesn't converge to zero i.e. Is there something wrong with the method? Else if the set is larger, what does the set look like? Any Help will be appreciated. Thanks in advance.","f(x,y) 3 7 f:[0,1]\times[0,1]→\mathbb{R} [0,1]\times [0,1]\cap \mathbb{Q}\times \mathbb{Q} \mathbb{R}^2 p_n \frac{1}{\sqrt{2}} f\left(p_n,\frac{1}{2}\right)=\frac{1}{2} f\left(\frac{1}{\sqrt{2}},\frac{1}{2}\right)=0","['real-analysis', 'multivariable-calculus', 'continuity', 'soft-question']"
58,Converse of inverse function theorem,Converse of inverse function theorem,,"Consider the following statement of the inverse function theorem: Let $f : W \to \mathbb{R}^n$ be a $C^1$ function from an open subset $W \subseteq \mathbb R^n$ and let $p \in W$ . $f$ has a local $C^1$ inverse if $Df(p)$ is invertible. This is effectively what one sees in Rudin's Principals of Mathematical Analysis . The question is: can the ""if"" can be replaced by ""iff""? It seems like the answer is obviously ""yes"": just apply the chain rule to $f \circ g = 1$ and $g \circ f = 1$ . Despite this, I can't find a statement that has the inverse function theorem in the ""iff"" form, so I feel like I must be making some kind of dumb mistake. For example this question has someone saying it only holds for ""low dimensions"" and someone else saying that it suffices to require the inverse to be surjective, which doesn't make sense to me given the straightforward proof I gave above. What am I missing?","Consider the following statement of the inverse function theorem: Let be a function from an open subset and let . has a local inverse if is invertible. This is effectively what one sees in Rudin's Principals of Mathematical Analysis . The question is: can the ""if"" can be replaced by ""iff""? It seems like the answer is obviously ""yes"": just apply the chain rule to and . Despite this, I can't find a statement that has the inverse function theorem in the ""iff"" form, so I feel like I must be making some kind of dumb mistake. For example this question has someone saying it only holds for ""low dimensions"" and someone else saying that it suffices to require the inverse to be surjective, which doesn't make sense to me given the straightforward proof I gave above. What am I missing?",f : W \to \mathbb{R}^n C^1 W \subseteq \mathbb R^n p \in W f C^1 Df(p) f \circ g = 1 g \circ f = 1,"['real-analysis', 'multivariable-calculus', 'derivatives', 'inverse-function-theorem']"
59,For $\omega$ and $\eta$ k-forms exist a $C^{1}$ function $f: \mathbb{R}^{3} \to \mathbb{R}$ such that $\eta = f\omega$.,For  and  k-forms exist a  function  such that .,\omega \eta C^{1} f: \mathbb{R}^{3} \to \mathbb{R} \eta = f\omega,"Let's consider a $k$ - form $\omega$ , $$\omega = \sum_{i_{1} < ... < i_{k}} \omega_{i_{1}, ..., i_{k}} dx^{i_{1}} \wedge ... \wedge dx^{i_{k}}$$ $\omega$ is $C^{r}$ if $\omega_{i_{1},... ,i_{k}}$ is $C^{r}$ . Consider $\Omega^k(U)$ the set of $C^{\infty}$ $k$ -forms in $U$ . My question is the following: Let $\omega,\ \eta\ \in \Omega^{1}(\mathbb{R}^{3})$ . If $\omega(x) \neq 0$ for all $x \in \mathbb{R}^{3}$ and $\omega \wedge \eta = 0$ , then exist a $C^{1}$ function $f: \mathbb{R}^{3} \to \mathbb{R}$ such that $\eta = f\omega$ . My attempt: consider $\omega = \omega_{1}dx + \omega_{2}dy +  \omega_{3}dz$ and $\eta = \eta_{1}dx + \eta_{2}dy + \eta_{3}dz$ , thus $$\omega \wedge \eta = (\omega_{1}\eta_{2} - \omega_{2}\eta_{1}) dx \wedge dy + (\omega_{1}\eta_{3} - \omega_{3}\eta_{1}) dx \wedge dz +\\ (\omega_{2}\eta_{3} - \omega_{3}\eta_{2}) dy \wedge dz$$ Using that $\omega \wedge \eta = 0$ and $\{dx \wedge dy, dx \wedge dz, dy \wedge dz \}$ is L.I. we have that $\omega_{1}\eta_{2} = \omega_{2}\eta_{1}$ ; $\omega_{1}\eta_{3} = \omega_{3}\eta_{1}$ ; $\omega_{2}\eta_{3}  = \omega_{3}\eta_{2}$ . Remember that we want a function $f$ such that $f\omega_{i} = \eta_{i}$ . Suppose that exist $p \in \mathbb{R}^{3}$ such that $\omega_{3}(p) = 0$ , then $\omega_{1}\eta_{3} = 0$ and $\omega_{2}\eta_{3} = 0$ and so $\omega_{1}(p) = 0$ or $\omega_{2}(p) = 0$ or $\eta_{3}(p) = 0$ . If $\eta_{3}(p) \neq 0$ , then $\omega(p) = 0$ , contradiction. So, we conclude that $\eta_{3}(p) = 0$ . Hence, we can define $f$ like $$f(p) = \begin{cases} \frac{\eta_{3}}{\omega_{3}}(p), &\text{ if }\omega_{3}(p) \neq 0, \\ 0, & \text{ if }\omega_{3}(p) = 0.\end{cases}$$ My problem is prove that $f$ is a $C^{1}$ function. Someone can help me?","Let's consider a - form , is if is . Consider the set of -forms in . My question is the following: Let . If for all and , then exist a function such that . My attempt: consider and , thus Using that and is L.I. we have that ; ; . Remember that we want a function such that . Suppose that exist such that , then and and so or or . If , then , contradiction. So, we conclude that . Hence, we can define like My problem is prove that is a function. Someone can help me?","k \omega \omega = \sum_{i_{1} < ... < i_{k}} \omega_{i_{1}, ..., i_{k}} dx^{i_{1}} \wedge ... \wedge dx^{i_{k}} \omega C^{r} \omega_{i_{1},... ,i_{k}} C^{r} \Omega^k(U) C^{\infty} k U \omega,\ \eta\ \in \Omega^{1}(\mathbb{R}^{3}) \omega(x) \neq 0 x \in \mathbb{R}^{3} \omega \wedge \eta = 0 C^{1} f: \mathbb{R}^{3} \to \mathbb{R} \eta = f\omega \omega = \omega_{1}dx + \omega_{2}dy + 
\omega_{3}dz \eta = \eta_{1}dx + \eta_{2}dy + \eta_{3}dz \omega \wedge \eta = (\omega_{1}\eta_{2} - \omega_{2}\eta_{1}) dx \wedge dy + (\omega_{1}\eta_{3} - \omega_{3}\eta_{1}) dx \wedge dz +\\ (\omega_{2}\eta_{3} - \omega_{3}\eta_{2}) dy \wedge dz \omega \wedge \eta = 0 \{dx \wedge dy, dx \wedge dz, dy \wedge dz \} \omega_{1}\eta_{2} = \omega_{2}\eta_{1} \omega_{1}\eta_{3} = \omega_{3}\eta_{1} \omega_{2}\eta_{3}  = \omega_{3}\eta_{2} f f\omega_{i} = \eta_{i} p \in \mathbb{R}^{3} \omega_{3}(p) = 0 \omega_{1}\eta_{3} = 0 \omega_{2}\eta_{3} = 0 \omega_{1}(p) = 0 \omega_{2}(p) = 0 \eta_{3}(p) = 0 \eta_{3}(p) \neq 0 \omega(p) = 0 \eta_{3}(p) = 0 f f(p) = \begin{cases} \frac{\eta_{3}}{\omega_{3}}(p), &\text{ if }\omega_{3}(p) \neq 0, \\ 0, & \text{ if }\omega_{3}(p) = 0.\end{cases} f C^{1}","['multivariable-calculus', 'differential-forms', 'exterior-algebra']"
60,Directional derivative and gradient of a differentiable function,Directional derivative and gradient of a differentiable function,,"Let $u = f(x,y,z)$ be a differentiable function in $\mathbb{R^3}$ Given the function satisfies: $f(x,y,x^2 + y^2) = 2x+y$ for all $x,y$ And the directional derivative of the point $(0,2,4)$ in the direction: $(-2,1,2)$ is equal to $-\frac{5}{3}$ . Calculate: $\nabla f(0,2,4)$ My try so far: I first normalized the vector: $\frac{(-2,1,2)}{||(-2,1,2)||} = (-\frac{2}{3}, \frac{1}{3}, \frac{2}{3})$ we know that $f(0,2,4) = f(x,y,x^2 + y^2)$ because $x=0, y=2 , x^2+y^2 = 4$ and so: $f(0,2,4) = 2 \cdot 0 + 2 = 2$ . By the definition: $\frac{\partial f}{\partial (-2,1,2)}(0,2,4) = -\frac{5}{3} \Rightarrow \nabla f(0,2,4) \cdot (-\frac{2}{3}, \frac{1}{3}, \frac{2}{3}) = -\frac{5}{3}$ Even by the definition I am stuck: $\nabla f(0,2,4) = \lim_{h \rightarrow 0} \frac{f( (0,2,4) + h(-2,1,2)) - f(0,2,4)}{h} = \lim_{h \rightarrow 0} \frac{f(-2h,2+h, 4+2h) - 2}{h}$ Because the point $(-2h,2+h, 4+2h)$ does not satisfy that $x^2 + y^2 = z$ (coordinates) ... The problem is that I don't know how to find the gradient of that point, because the function is not given in its explicit form.. I would appreciate your help, thank you!","Let be a differentiable function in Given the function satisfies: for all And the directional derivative of the point in the direction: is equal to . Calculate: My try so far: I first normalized the vector: we know that because and so: . By the definition: Even by the definition I am stuck: Because the point does not satisfy that (coordinates) ... The problem is that I don't know how to find the gradient of that point, because the function is not given in its explicit form.. I would appreciate your help, thank you!","u = f(x,y,z) \mathbb{R^3} f(x,y,x^2 + y^2) = 2x+y x,y (0,2,4) (-2,1,2) -\frac{5}{3} \nabla f(0,2,4) \frac{(-2,1,2)}{||(-2,1,2)||} = (-\frac{2}{3}, \frac{1}{3}, \frac{2}{3}) f(0,2,4) = f(x,y,x^2 + y^2) x=0, y=2 , x^2+y^2 = 4 f(0,2,4) = 2 \cdot 0 + 2 = 2 \frac{\partial f}{\partial (-2,1,2)}(0,2,4) = -\frac{5}{3} \Rightarrow \nabla f(0,2,4) \cdot (-\frac{2}{3}, \frac{1}{3}, \frac{2}{3}) = -\frac{5}{3} \nabla f(0,2,4) = \lim_{h \rightarrow 0} \frac{f( (0,2,4) + h(-2,1,2)) - f(0,2,4)}{h} = \lim_{h \rightarrow 0} \frac{f(-2h,2+h, 4+2h) - 2}{h} (-2h,2+h, 4+2h) x^2 + y^2 = z","['calculus', 'multivariable-calculus', 'vector-analysis']"
61,"$ f $ is differentiable in $ (0,0). $",is differentiable in," f   (0,0). ","Definition: Let $V\subseteq{\mathbb{R}^{m}}$ an open set, $a\in V$ y $f\colon V\to\mathbb{R}^{n}$ a function. We will say that $f$ is differentiable in $a,$ if exists a linear transformation $f'(a)\colon\mathbb{R}^{m}\to\mathbb{R}^{n}$ such that \begin{equation} f(a+h)=f(a)+f'(a)(h)+r(h),\qquad\lim_{h\rightarrow 0}{\dfrac{r(h)}{\lVert h\rVert}}=0. \end{equation} Let $ a \in \mathbb {R}$ be. Define the function $ f \colon \mathbb {R}^ {2} \to \mathbb {R} $ given by \begin{equation} f(x,y)=\left\{\begin{matrix} \dfrac{x\sin^{2}(x)+axy^{2}}{x^{2}+2y^{2}+3y^{4}} & (x,y)\neq(0,0)\\  0 & (x,y)=(0,0) \end{matrix}\right. \end{equation} Find the value of $ a $ so that $ f $ is differentiable by $ (0,0). $ My attempt: We observed that \begin{equation} \dfrac{\partial f}{\partial x}(0,0)=0=\dfrac{\partial f}{\partial y}(0,0). \end{equation} If $(x,y)\in\mathbb{R}^{2}\setminus\{(0,0)\},$ then \begin{equation} \dfrac{\partial f}{\partial x}(x,y)=\dfrac{\sin^{2}(x)(2y^{2}+3y^{4}-x^{2})+x\sin(2x)(x^{2}+2y^{2}+3y^{4})+ay^{2}(2y^{2}+3y^{4}-x^{2})}{(x^{2}+2y^{2}+3y^{4})^{2}} \end{equation} \begin{equation} \dfrac{\partial f}{\partial y}(x,y)=\dfrac{2axy(x^{2}-3y^{4})-4xy\sin^{2}(x)(1+3y^{2})}{(x^{2}+2y^{2}+3y^{4})^{2}} \end{equation} If $\dfrac{\partial f}{\partial y}(x,y)=0,$ then \begin{align} 2axy(x^{2}-3y^{4})-4xy\sin^{2}(x)(1+3y^{2})=0&\quad\Longleftrightarrow\quad a(x^{2}-3y^{4})=2\sin^{2}(x)(1+3y^{2})\\ &\quad\Longleftrightarrow\quad a=\dfrac{2\sin^{2}(x)(1+3y^{2})}{x^{2}-3y^{4}} \end{align} \begin{equation} f(x,y)=\left\{\begin{matrix} x\sin^{2}(x) & (x,y)\neq(0,0)\\  0 & (x,y)=(0,0) \end{matrix}\right. \end{equation} \begin{equation} \dfrac{\partial f}{\partial x}(0,0)=0=\dfrac{\partial f}{\partial y}(0,0) \end{equation} From this it follows that $\dfrac{\partial f}{\partial x}(x,y)$ and $\dfrac{\partial f}{\partial y}(x,y)$ are continuous by $(0,0)$ y $f$ is differentiable by $(0,0).$ Are my arguments correct? Any suggestion is welcome.","Definition: Let an open set, y a function. We will say that is differentiable in if exists a linear transformation such that Let be. Define the function given by Find the value of so that is differentiable by My attempt: We observed that If then If then From this it follows that and are continuous by y is differentiable by Are my arguments correct? Any suggestion is welcome.","V\subseteq{\mathbb{R}^{m}} a\in V f\colon V\to\mathbb{R}^{n} f a, f'(a)\colon\mathbb{R}^{m}\to\mathbb{R}^{n} \begin{equation}
f(a+h)=f(a)+f'(a)(h)+r(h),\qquad\lim_{h\rightarrow 0}{\dfrac{r(h)}{\lVert h\rVert}}=0.
\end{equation}  a \in \mathbb {R}  f \colon \mathbb {R}^ {2} \to \mathbb {R}  \begin{equation}
f(x,y)=\left\{\begin{matrix}
\dfrac{x\sin^{2}(x)+axy^{2}}{x^{2}+2y^{2}+3y^{4}} & (x,y)\neq(0,0)\\ 
0 & (x,y)=(0,0)
\end{matrix}\right.
\end{equation}  a   f   (0,0).  \begin{equation}
\dfrac{\partial f}{\partial x}(0,0)=0=\dfrac{\partial f}{\partial y}(0,0).
\end{equation} (x,y)\in\mathbb{R}^{2}\setminus\{(0,0)\}, \begin{equation}
\dfrac{\partial f}{\partial x}(x,y)=\dfrac{\sin^{2}(x)(2y^{2}+3y^{4}-x^{2})+x\sin(2x)(x^{2}+2y^{2}+3y^{4})+ay^{2}(2y^{2}+3y^{4}-x^{2})}{(x^{2}+2y^{2}+3y^{4})^{2}}
\end{equation} \begin{equation}
\dfrac{\partial f}{\partial y}(x,y)=\dfrac{2axy(x^{2}-3y^{4})-4xy\sin^{2}(x)(1+3y^{2})}{(x^{2}+2y^{2}+3y^{4})^{2}}
\end{equation} \dfrac{\partial f}{\partial y}(x,y)=0, \begin{align}
2axy(x^{2}-3y^{4})-4xy\sin^{2}(x)(1+3y^{2})=0&\quad\Longleftrightarrow\quad a(x^{2}-3y^{4})=2\sin^{2}(x)(1+3y^{2})\\
&\quad\Longleftrightarrow\quad a=\dfrac{2\sin^{2}(x)(1+3y^{2})}{x^{2}-3y^{4}}
\end{align} \begin{equation}
f(x,y)=\left\{\begin{matrix}
x\sin^{2}(x) & (x,y)\neq(0,0)\\ 
0 & (x,y)=(0,0)
\end{matrix}\right.
\end{equation} \begin{equation}
\dfrac{\partial f}{\partial x}(0,0)=0=\dfrac{\partial f}{\partial y}(0,0)
\end{equation} \dfrac{\partial f}{\partial x}(x,y) \dfrac{\partial f}{\partial y}(x,y) (0,0) f (0,0).","['real-analysis', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
62,Help understanding a creative definition of the derivative based purely on integrals,Help understanding a creative definition of the derivative based purely on integrals,,"I am trying to understand section 3 of Here , titled ""what is a derivative"". At equation (3.2) Hestenes defines the derivation in relation to the integral, as follows: $$ \partial A =\lim_{d\omega\to0} \frac{1}{d\omega} \oint_{\partial \mathcal{M}} d\sigma A \tag{1} $$ where $d\omega$ is a volume element. Hestenes suggests that this is a very good way to think of a derivative (perhaps even the best way). I am trying to apply this definition to the 1D case, but I am struggling to do so. Specifically, my goal is to start from (1) and obtain (2): $$ \frac{d A[x]}{d x} = \lim_{dx\to 0} \frac{A[x+dx]-A[x]}{dx} \tag{2} $$ My thoughts and assumptions are as follows. Since we are dealing with the 1D case, should I be using $Cl_1(\mathbb{R})$ ; that is, the Clifford algebra of dimension 1 over the reals with basis element $\{\hat{\mathbf{x}}_1\}$ ? Hestene claims $d\omega$ is m-vector-valued differential; that is, it is a pseudo-scalar from the tangent space of $\mathcal{M}$ evaluated at point $x \in \mathcal{M}$ . In our 1D case, $d\omega=Idx=\hat{\mathbf{x}}_1 dx$ where I is the unit pseudoscalar of $Cl_1(\mathbb{R})$ $A$ is a function of $x$ . Thus, I write $A[x]$ . Hestenes claims that $\partial=\partial_x$ is the derivated with respect to a vector $x$ . In 1D, therefore $\partial_x=\partial/\partial x$ . Hestenes claims that $d\sigma$ is a (m-1)-valued pseudoscalar also in the tangent space of $\partial \mathcal{M}$ evaluated at point $x$ . I am not sure how to downgrade $\mathcal{M}$ to $\partial \mathcal{M}$ such that it is $0$ -dimensional? Am I supposed to consider $d\sigma$ as a pseudoscalar of $Cl_0(\mathbb{R})$ ? If so then is the answer just $d\sigma=dx$ ? Finally, Hestenes claims (starting from equation 3.2) that one needs $d\omega \wedge \partial =0$ in order to get to the geometric product. In 1D, why is $\hat{\mathbf{x}}_1dx \wedge \partial=0$ ? Is $\partial$ assumed in the tangent space of $\mathcal{M}$ and thus parallel to $\hat{\mathbf{x}}_1$ ? What becomes of the counter integral in 1D... does it collapses to a simple definite integral? I hope I dont have to integrate from a to b then from b to a to get back to the original point and thus to complete the ""contour"". If so then the integrals would simply cancel each other: $\oint_R f(x)dx = \int_a^b f(x)dx + \int_b^a f(x)dx=0$ ... that can't be good :( Since the left-most term of (1) is a derivative of A and the right-most term contains $A$ and not $A'$ , then I feel the contour integral in 1D must collapse to a non-integral in order to avoid raising A to its anti-derivative. What is $\partial \mathcal{M}$ for a 1D manifold $\mathcal{M}$ - I am assuming it is simply an interval $[x,x+h]$ , where h is an infinitesimal element?","I am trying to understand section 3 of Here , titled ""what is a derivative"". At equation (3.2) Hestenes defines the derivation in relation to the integral, as follows: where is a volume element. Hestenes suggests that this is a very good way to think of a derivative (perhaps even the best way). I am trying to apply this definition to the 1D case, but I am struggling to do so. Specifically, my goal is to start from (1) and obtain (2): My thoughts and assumptions are as follows. Since we are dealing with the 1D case, should I be using ; that is, the Clifford algebra of dimension 1 over the reals with basis element ? Hestene claims is m-vector-valued differential; that is, it is a pseudo-scalar from the tangent space of evaluated at point . In our 1D case, where I is the unit pseudoscalar of is a function of . Thus, I write . Hestenes claims that is the derivated with respect to a vector . In 1D, therefore . Hestenes claims that is a (m-1)-valued pseudoscalar also in the tangent space of evaluated at point . I am not sure how to downgrade to such that it is -dimensional? Am I supposed to consider as a pseudoscalar of ? If so then is the answer just ? Finally, Hestenes claims (starting from equation 3.2) that one needs in order to get to the geometric product. In 1D, why is ? Is assumed in the tangent space of and thus parallel to ? What becomes of the counter integral in 1D... does it collapses to a simple definite integral? I hope I dont have to integrate from a to b then from b to a to get back to the original point and thus to complete the ""contour"". If so then the integrals would simply cancel each other: ... that can't be good :( Since the left-most term of (1) is a derivative of A and the right-most term contains and not , then I feel the contour integral in 1D must collapse to a non-integral in order to avoid raising A to its anti-derivative. What is for a 1D manifold - I am assuming it is simply an interval , where h is an infinitesimal element?","
\partial A =\lim_{d\omega\to0} \frac{1}{d\omega} \oint_{\partial \mathcal{M}} d\sigma A \tag{1}
 d\omega 
\frac{d A[x]}{d x} = \lim_{dx\to 0} \frac{A[x+dx]-A[x]}{dx} \tag{2}
 Cl_1(\mathbb{R}) \{\hat{\mathbf{x}}_1\} d\omega \mathcal{M} x \in \mathcal{M} d\omega=Idx=\hat{\mathbf{x}}_1 dx Cl_1(\mathbb{R}) A x A[x] \partial=\partial_x x \partial_x=\partial/\partial x d\sigma \partial \mathcal{M} x \mathcal{M} \partial \mathcal{M} 0 d\sigma Cl_0(\mathbb{R}) d\sigma=dx d\omega \wedge \partial =0 \hat{\mathbf{x}}_1dx \wedge \partial=0 \partial \mathcal{M} \hat{\mathbf{x}}_1 \oint_R f(x)dx = \int_a^b f(x)dx + \int_b^a f(x)dx=0 A A' \partial \mathcal{M} \mathcal{M} [x,x+h]","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'clifford-algebras']"
63,Deducing $f(x)=f(x_0)+f'(x_0)(x-x_0)+\frac12(x-x_0)^TH_f(x_0)(x-x_0)+o(\lVert x-x_0\rVert^2)$,Deducing,f(x)=f(x_0)+f'(x_0)(x-x_0)+\frac12(x-x_0)^TH_f(x_0)(x-x_0)+o(\lVert x-x_0\rVert^2),This was a task that gave $9$ points in an exam I failed. Since our professor doesn't provide solutions I thought I'd ask here. Let $f:\mathbb{R^2} \to \mathbb{R}$ be twice continuous partially differentiable and $x_0 \in \mathbb{R^2}$ random. Deduce the following formula for $x \in \mathbb{R^2}$ using Taylor's theorem: $$f(x)=f(x_0)+f'(x_0)(x-x_0)+\frac{1}{2}(x-x_0)^TH_f(x_0)(x-x_0)+o(\lVert x-x_0 \rVert^2).$$ Complete the remainder of the second order and show that for the remaining terms $T$ it holds that $$\lim_{x \to x_0} \frac{T(x)}{\lVert x-x_0 \rVert^2} = 0$$ Can someone tell us how to do this? The total derivate is defined as: $$\lim_{x \to \alpha} \frac{\lVert f(x)-f(a)- df_a(x-a)\rVert}{\lVert x-a \rVert} = 0$$ I think from this one can get the difference of the norm which was given an estimate of in the task...,This was a task that gave points in an exam I failed. Since our professor doesn't provide solutions I thought I'd ask here. Let be twice continuous partially differentiable and random. Deduce the following formula for using Taylor's theorem: Complete the remainder of the second order and show that for the remaining terms it holds that Can someone tell us how to do this? The total derivate is defined as: I think from this one can get the difference of the norm which was given an estimate of in the task...,9 f:\mathbb{R^2} \to \mathbb{R} x_0 \in \mathbb{R^2} x \in \mathbb{R^2} f(x)=f(x_0)+f'(x_0)(x-x_0)+\frac{1}{2}(x-x_0)^TH_f(x_0)(x-x_0)+o(\lVert x-x_0 \rVert^2). T \lim_{x \to x_0} \frac{T(x)}{\lVert x-x_0 \rVert^2} = 0 \lim_{x \to \alpha} \frac{\lVert f(x)-f(a)- df_a(x-a)\rVert}{\lVert x-a \rVert} = 0,"['multivariable-calculus', 'taylor-expansion']"
64,Fixed point and gradient descent,Fixed point and gradient descent,,"Suppose $f:[0,1]^d \to [0,1]^d$ is a smooth function. Is there any theorem comparing the convergence of the sequences obtained by (1) repeatedly applying (iterating) $f$ and (2) performing gradient descent on $\mathbf{x} \mapsto \|\mathbf{x}-f(\mathbf{x})\|^2$ starting from an arbitrary point $\mathbf{x}_0$ ?",Suppose is a smooth function. Is there any theorem comparing the convergence of the sequences obtained by (1) repeatedly applying (iterating) and (2) performing gradient descent on starting from an arbitrary point ?,"f:[0,1]^d \to [0,1]^d f \mathbf{x} \mapsto \|\mathbf{x}-f(\mathbf{x})\|^2 \mathbf{x}_0","['multivariable-calculus', 'convergence-divergence', 'optimization', 'dynamical-systems', 'fixed-point-theorems']"
65,Inequality in 4 variables Vasc's EV,Inequality in 4 variables Vasc's EV,,"Let $a,b,c,d\geq0$ satisfying $a+b+c+d=4$ . Prove $$\sqrt{a^3+b^3+c^3+d^3}+2(\sqrt3  -1)abcd\geq\sqrt{3(abc+abd+acd+bcd)}$$ Attempt: $a^3+b^3+c^3+d^3=(a+b+c+d)(a^2+b^2+c^2+d^2-ab-bc-cd-da-ac-bd)+3(abc+abd+acd+bcd)$ . Let's try it with Vasc's EV. Who knows?",Let satisfying . Prove Attempt: . Let's try it with Vasc's EV. Who knows?,"a,b,c,d\geq0 a+b+c+d=4 \sqrt{a^3+b^3+c^3+d^3}+2(\sqrt3
 -1)abcd\geq\sqrt{3(abc+abd+acd+bcd)} a^3+b^3+c^3+d^3=(a+b+c+d)(a^2+b^2+c^2+d^2-ab-bc-cd-da-ac-bd)+3(abc+abd+acd+bcd)","['multivariable-calculus', 'inequality', 'muirhead-inequality', 'buffalo-way']"
66,Continuity of $y \sin(\frac{1}{x})$,Continuity of,y \sin(\frac{1}{x}),"Problem: for which values of $\alpha \in \mathbb{R}$ is the function \begin{equation*} f: \mathbb{R}^2\to\mathbb{R}^2 \qquad \begin{pmatrix} x \\ y\end{pmatrix} \mapsto \begin{cases} y \sin(\frac{1}{x}) \quad &\text{if $x\neq 0$}\\0  \quad &\text{if $x= 0$}\end{cases} \end{equation*} continuous at $a = \begin{pmatrix} 0 \\ \alpha\end{pmatrix}$ ? Prove your answer. Attempted solution: I suppose the answer is: $\left\{ \begin{pmatrix} 0 \\ 0\end{pmatrix} \right\}$ . I tried to reason as follows, but I am not sure if this is technically correct. $f$ is continuous at $a$ $\iff \lim_{x\to 0} f\begin{pmatrix} x \\ \alpha\end{pmatrix} = \begin{pmatrix} 0 \\ 0\end{pmatrix}$ $\iff \lim_{x\to 0} \alpha \sin\left(\frac{1}{x} \right)= \begin{pmatrix} 0 \\ 0\end{pmatrix}$ $\iff \alpha = 0$ since $\sin$ is periodic, and thus diverges. Is this correct?","Problem: for which values of is the function continuous at ? Prove your answer. Attempted solution: I suppose the answer is: . I tried to reason as follows, but I am not sure if this is technically correct. is continuous at since is periodic, and thus diverges. Is this correct?","\alpha \in \mathbb{R} \begin{equation*}
f: \mathbb{R}^2\to\mathbb{R}^2 \qquad \begin{pmatrix} x \\ y\end{pmatrix} \mapsto \begin{cases} y \sin(\frac{1}{x}) \quad &\text{if x\neq 0}\\0  \quad &\text{if x= 0}\end{cases}
\end{equation*} a = \begin{pmatrix} 0 \\ \alpha\end{pmatrix} \left\{ \begin{pmatrix} 0 \\ 0\end{pmatrix} \right\} f a \iff \lim_{x\to 0} f\begin{pmatrix} x \\ \alpha\end{pmatrix} = \begin{pmatrix} 0 \\ 0\end{pmatrix} \iff \lim_{x\to 0} \alpha \sin\left(\frac{1}{x} \right)= \begin{pmatrix} 0 \\ 0\end{pmatrix} \iff \alpha = 0 \sin","['real-analysis', 'multivariable-calculus']"
67,How to prove function $f(X) := \text{tr} \left( X^{-1} A \right)$ is convex?,How to prove function  is convex?,f(X) := \text{tr} \left( X^{-1} A \right),"Let $S^{n}_{+}$ and $S^{n}_{++}$ denote the set of positive semidefinite and positive definite (symmetric) $n \times n$ matrices, respectively. Let function $f : S^{n}_{++} \to \mathbb R$ be defined by $$f(X) := \text{tr} \left( X^{-1} A \right)$$ where $A \in S^{n}_{+}$ . When $f$ is differentiable, given that $$\nabla_X f = - X^{-1}AX^{-1}$$ can we show that $f$ is convex via the use of the equivalent proposition of convexity $$\langle\nabla f(X)-\nabla f(Y),\ X-Y\rangle \ge 0$$ where $\langle \cdot, \cdot\rangle$ denotes the inner product of $S^{n}_{++}$ ? Or can it be proven that the function is convex in a simpler way?","Let and denote the set of positive semidefinite and positive definite (symmetric) matrices, respectively. Let function be defined by where . When is differentiable, given that can we show that is convex via the use of the equivalent proposition of convexity where denotes the inner product of ? Or can it be proven that the function is convex in a simpler way?","S^{n}_{+} S^{n}_{++} n \times n f : S^{n}_{++} \to \mathbb R f(X) := \text{tr} \left( X^{-1} A \right) A \in S^{n}_{+} f \nabla_X f = - X^{-1}AX^{-1} f \langle\nabla f(X)-\nabla f(Y),\ X-Y\rangle \ge 0 \langle \cdot, \cdot\rangle S^{n}_{++}","['multivariable-calculus', 'convex-analysis', 'matrix-calculus']"
68,"Is this ""one-sided"" version of the fundamental lemma of calculus of variations true?","Is this ""one-sided"" version of the fundamental lemma of calculus of variations true?",,"Let $\mathbb{D}^n \subseteq \mathbb{R}^n$ be the closed $n$ -dimensional unit ball, and let $A:\mathbb{D}^n \to \mathbb{R}^k \otimes \mathbb{R}^k$ be smooth. Suppose that $ \langle A , V \otimes V \rangle_{L^2} \ge 0$ for every smooth map $V:\mathbb{D}^n \to \mathbb{R}^k$ . Explicitly, I assume that $ \int_{\mathbb{D}^n} \langle A(x) , V(x) \otimes V(x) \rangle_{\mathbb{R}^k \otimes \mathbb{R}^k} dx \ge 0$ where $\langle , \rangle_{\mathbb{R}^k \otimes \mathbb{R}^k}$ is the tensor product metric on $\mathbb{R}^k$ , i.e. $$ \langle v_1 \otimes v_2 , w_1 \otimes w_2\rangle := \langle v_1, w_1 \rangle_{\mathbb{R}^k} \cdot \langle v_2 ,w_2 \rangle_{\mathbb{R}^k}.$$ Question: Is it true that $ \langle A(x) , v \otimes v \rangle_{\mathbb{R}^k \otimes \mathbb{R}^k} \ge 0$ for every $x \in \mathbb{D}^n$ and every $v \in \mathbb{R}^k$ ? This seems like a ""one-sided"" analogue of the fundamental lemma of the calculus of variations .","Let be the closed -dimensional unit ball, and let be smooth. Suppose that for every smooth map . Explicitly, I assume that where is the tensor product metric on , i.e. Question: Is it true that for every and every ? This seems like a ""one-sided"" analogue of the fundamental lemma of the calculus of variations .","\mathbb{D}^n \subseteq \mathbb{R}^n n A:\mathbb{D}^n \to \mathbb{R}^k \otimes \mathbb{R}^k  \langle A , V \otimes V \rangle_{L^2} \ge 0 V:\mathbb{D}^n \to \mathbb{R}^k  \int_{\mathbb{D}^n} \langle A(x) , V(x) \otimes V(x) \rangle_{\mathbb{R}^k \otimes \mathbb{R}^k} dx \ge 0 \langle , \rangle_{\mathbb{R}^k \otimes \mathbb{R}^k} \mathbb{R}^k  \langle v_1 \otimes v_2 , w_1 \otimes w_2\rangle := \langle v_1, w_1 \rangle_{\mathbb{R}^k} \cdot \langle v_2 ,w_2 \rangle_{\mathbb{R}^k}.  \langle A(x) , v \otimes v \rangle_{\mathbb{R}^k \otimes \mathbb{R}^k} \ge 0 x \in \mathbb{D}^n v \in \mathbb{R}^k","['real-analysis', 'multivariable-calculus', 'calculus-of-variations', 'variational-analysis']"
69,Can we approximate uniformly a function defined on a subset with smooth functions?,Can we approximate uniformly a function defined on a subset with smooth functions?,,"Let $\mathbb{D}^n \subseteq \mathbb{R}^n$ be the closed $n$ -dimensional unit ball. Suppose we are given an open subset $U \subseteq \mathbb{D}^n$ of full measure in $\mathbb{D}^n$ , and a smooth bounded function $f:U \to \mathbb{R}$ . Do there exist smooth functions $f_k:\mathbb{D}^n \to \mathbb{R}$ which converge uniformly to $f$ ? Note that that the derivatives of the original $f$ may explode when we approach $\partial U$ . In my specific case $U$ has Hausdorff dimension $\le n-1$ , but I am not sure if it matters.","Let be the closed -dimensional unit ball. Suppose we are given an open subset of full measure in , and a smooth bounded function . Do there exist smooth functions which converge uniformly to ? Note that that the derivatives of the original may explode when we approach . In my specific case has Hausdorff dimension , but I am not sure if it matters.",\mathbb{D}^n \subseteq \mathbb{R}^n n U \subseteq \mathbb{D}^n \mathbb{D}^n f:U \to \mathbb{R} f_k:\mathbb{D}^n \to \mathbb{R} f f \partial U U \le n-1,"['real-analysis', 'multivariable-calculus', 'approximation', 'uniform-convergence', 'singularity']"
70,Is orientation of Stokes theorem a convention?,Is orientation of Stokes theorem a convention?,,"Is orientation of Stokes theorem, that is, right hand rule, a convention? Can we also choose the left hand rule? But will not it create problems in Physics where the sign of our physical quantity (which is derived from Stokes theorem) will depend on this convention?","Is orientation of Stokes theorem, that is, right hand rule, a convention? Can we also choose the left hand rule? But will not it create problems in Physics where the sign of our physical quantity (which is derived from Stokes theorem) will depend on this convention?",,"['multivariable-calculus', 'physics', 'mathematical-physics', 'stokes-theorem', 'orientation']"
71,"Prove $F(x, y) = (f(x))^{g(y)}$ is integrable over $A = [0, 1] \times [0, 1].$",Prove  is integrable over,"F(x, y) = (f(x))^{g(y)} A = [0, 1] \times [0, 1].","I'm writing to ask for your help in the last step to finish the following question: Let $f, g: [0, 1] \to \mathbb{R}$ be integrables functions with $f(x) \geq m > 0,$ for all $x \in [0, 1]$ . Prove that $F(x, y) = (f(x))^{g(y)}$ is integrable over $A = [0, 1] \times [0, 1].$ My attempt: First, note that: $F(x, y) = (f(x))^{g(y)} = \exp(g(y)\cdot \ln(f(x)))$ . Then since $f([0, 1]) \subset [m, +\infty)$ and the logarithm is Lipschitz on $[m, +\infty)$ , so $\ln \circ f$ is integrable over $[0, 1]$ . Second, Defining $\phi : [0, 1] \times [0, 1] \to \mathbb{R}$ such that $\phi(x, y) = g(y)\cdot\ln(f(x))$ , it is possible to prove that $\phi$ is integrable due to the fact that $g$ and $\ln \circ f$ are. Finally, since $F = \exp \circ \phi,$ what is needed to finish the proof is to be show that $\exp \circ \phi$ is also integrable. But here is where I met my Waterloo. Any kind of help would be really appreciated. Thanks in advance.","I'm writing to ask for your help in the last step to finish the following question: Let be integrables functions with for all . Prove that is integrable over My attempt: First, note that: . Then since and the logarithm is Lipschitz on , so is integrable over . Second, Defining such that , it is possible to prove that is integrable due to the fact that and are. Finally, since what is needed to finish the proof is to be show that is also integrable. But here is where I met my Waterloo. Any kind of help would be really appreciated. Thanks in advance.","f, g: [0, 1] \to \mathbb{R} f(x) \geq m > 0, x \in [0, 1] F(x, y) = (f(x))^{g(y)} A = [0, 1] \times [0, 1]. F(x, y) = (f(x))^{g(y)} = \exp(g(y)\cdot \ln(f(x))) f([0, 1]) \subset [m, +\infty) [m, +\infty) \ln \circ f [0, 1] \phi : [0, 1] \times [0, 1] \to \mathbb{R} \phi(x, y) = g(y)\cdot\ln(f(x)) \phi g \ln \circ f F = \exp \circ \phi, \exp \circ \phi","['real-analysis', 'multivariable-calculus']"
72,Differential Equation for a Baseball seam,Differential Equation for a Baseball seam,,"Can we define differential equation and parametrization of a smooth continuous line on a unit ball with symmetry about two orthogonal axes ? And also have same geodesic curvature $k_g$ at four equidistant points on the curve on the two planes of symmetry? With a relation between curvature and torsion of the space curve seam? Can this space curve be unique? Imagine the seams of a common baseball or  tennis ball that divides surface area into two equal parts with two identical/congruent regions. However it has discontinuities at four locations... with curvature jumps at join of four small circles... these should translate to inflection points in the smooth curve sought. Looking for a relation between their curvature and torsion.. From an earlier question here we have a relation $$  \theta = (\pi/3)\sin 2\phi $$ that had been suggested (by @ minopret). It appears by visual inspection to me that not all $k_g$ are equal in magnitude and moreover there are six points of maximum geodesic curvature, not four. Thanks for all thoughts and hope would it be some fun too.","Can we define differential equation and parametrization of a smooth continuous line on a unit ball with symmetry about two orthogonal axes ? And also have same geodesic curvature at four equidistant points on the curve on the two planes of symmetry? With a relation between curvature and torsion of the space curve seam? Can this space curve be unique? Imagine the seams of a common baseball or  tennis ball that divides surface area into two equal parts with two identical/congruent regions. However it has discontinuities at four locations... with curvature jumps at join of four small circles... these should translate to inflection points in the smooth curve sought. Looking for a relation between their curvature and torsion.. From an earlier question here we have a relation that had been suggested (by @ minopret). It appears by visual inspection to me that not all are equal in magnitude and moreover there are six points of maximum geodesic curvature, not four. Thanks for all thoughts and hope would it be some fun too.",k_g   \theta = (\pi/3)\sin 2\phi  k_g,"['multivariable-calculus', 'differential-geometry']"
73,Differentation of vector,Differentation of vector,,"in the below equation, $\mathbf w$ is a vector with components $w_0$ and $w_1$ . $x^{(i)}$ and $y^{(i)}$ are constants. how to differentiate $j(\mathbf w)$ w.r.t. $w_0$ and $w_1$ $j(\mathbf w) = \frac{1}{10}\sum_{i=1}^5(x^{(i)}+w_1y^{(i)}-w_0)$","in the below equation, is a vector with components and . and are constants. how to differentiate w.r.t. and",\mathbf w w_0 w_1 x^{(i)} y^{(i)} j(\mathbf w) w_0 w_1 j(\mathbf w) = \frac{1}{10}\sum_{i=1}^5(x^{(i)}+w_1y^{(i)}-w_0),"['calculus', 'multivariable-calculus', 'vector-analysis']"
74,Calculus on Manifolds Theorem 3-14 (Sard's Theorem),Calculus on Manifolds Theorem 3-14 (Sard's Theorem),,"I have a question regarding Theorem 3-14 in Spivak's Calculus on Manifolds : The proof starts by considering a closed rectangle $U\subset A$ such that all the sides of $U$ are of the same length $l$ . Let $\epsilon>0$ . Spivak claims that if $N$ is sufficiently large and $U$ is divided into $N^n$ rectangles, with sides of length $l/N$ , then for each of these rectangles $S$ , if $x\in S$ we have $$|Dg(x)(y-x)-(g(y)-g(x))|<\epsilon|x-y|\leq \epsilon\sqrt{n}(l/N)$$ for all $y\in S$ . I don't get why this is necessarily true. My attempt to prove this is as follows. Take any $\epsilon>0$ . Since $g$ is differentiable on $U$ , for all $x\in U$ there exists $\delta'_x >0$ such that for all $y$ , $$|x-y|<\delta'_x\implies|Dg(x)(y-x)-(g(y)-g(x))|<\epsilon|x-y|.$$ Since $g$ is continuously differentiable, for all $x\in U$ there exists $\delta''_x$ such that for all $y$ and $v$ , $$|x-y|<\delta''_x\implies|Dg(x)(v)-Dg(y)(v)|<\epsilon|v|.$$ Let $\delta_x=\min\{\delta'_x,\, \delta''_x\}$ . Then $\{B(x;\delta_x/2)\,|\,x\in U\}$ is an open cover of $U$ . Since $U$ is compact, a finite number of these balls cover $U$ . Say these balls are centered around $x_1,\,x_2,\,\ldots,\,x_k$ . Let $\delta=\min\{\delta_{x_1}/2,\,\delta_{x_2}/2\,\ldots,\,\delta_{x_k}/2\}$ . Pick any arbitrary $x\in U$ . Then $x\in B(x_i,\delta_{x_i}/2)$ for some $1\leq i \leq k$ . Thus $$|Dg(x_i)(x-x_i)-(g(x)-g(x_i))|<\epsilon|x_i-x|.$$ For any $y\in B(x;\delta)$ we have $y\in B(x_i,\delta_{x_i})$ , ergo $$|Dg(x_i)(y-x_i)-(g(y)-g(x_i))|<\epsilon|x_i-y|.$$ Combining these two statements gives $$|Dg(x_i)(x-y)-(g(x)-g(y))|<\epsilon(|x_i-x|+|x_i-y|).$$ Again using the fact that $x\in B(x_i,\delta_{x_i}/2)$ , we have $$|Dg(x_i)(x-y)-Dg(x)(x-y)|<\epsilon|x-y|,$$ hence $$|Dg(x)(y-x)-(g(y)-g(x))|<\epsilon(|x-y|+|x_i-x|+|x_i-y|).$$ Unfortunately, I don't know how to progress from here. Any help would very much be appreciated.","I have a question regarding Theorem 3-14 in Spivak's Calculus on Manifolds : The proof starts by considering a closed rectangle such that all the sides of are of the same length . Let . Spivak claims that if is sufficiently large and is divided into rectangles, with sides of length , then for each of these rectangles , if we have for all . I don't get why this is necessarily true. My attempt to prove this is as follows. Take any . Since is differentiable on , for all there exists such that for all , Since is continuously differentiable, for all there exists such that for all and , Let . Then is an open cover of . Since is compact, a finite number of these balls cover . Say these balls are centered around . Let . Pick any arbitrary . Then for some . Thus For any we have , ergo Combining these two statements gives Again using the fact that , we have hence Unfortunately, I don't know how to progress from here. Any help would very much be appreciated.","U\subset A U l \epsilon>0 N U N^n l/N S x\in S |Dg(x)(y-x)-(g(y)-g(x))|<\epsilon|x-y|\leq \epsilon\sqrt{n}(l/N) y\in S \epsilon>0 g U x\in U \delta'_x >0 y |x-y|<\delta'_x\implies|Dg(x)(y-x)-(g(y)-g(x))|<\epsilon|x-y|. g x\in U \delta''_x y v |x-y|<\delta''_x\implies|Dg(x)(v)-Dg(y)(v)|<\epsilon|v|. \delta_x=\min\{\delta'_x,\, \delta''_x\} \{B(x;\delta_x/2)\,|\,x\in U\} U U U x_1,\,x_2,\,\ldots,\,x_k \delta=\min\{\delta_{x_1}/2,\,\delta_{x_2}/2\,\ldots,\,\delta_{x_k}/2\} x\in U x\in B(x_i,\delta_{x_i}/2) 1\leq i \leq k |Dg(x_i)(x-x_i)-(g(x)-g(x_i))|<\epsilon|x_i-x|. y\in B(x;\delta) y\in B(x_i,\delta_{x_i}) |Dg(x_i)(y-x_i)-(g(y)-g(x_i))|<\epsilon|x_i-y|. |Dg(x_i)(x-y)-(g(x)-g(y))|<\epsilon(|x_i-x|+|x_i-y|). x\in B(x_i,\delta_{x_i}/2) |Dg(x_i)(x-y)-Dg(x)(x-y)|<\epsilon|x-y|, |Dg(x)(y-x)-(g(y)-g(x))|<\epsilon(|x-y|+|x_i-x|+|x_i-y|).",['multivariable-calculus']
75,$d(I\omega) + I(d\omega) = \omega$ for differential forms,for differential forms,d(I\omega) + I(d\omega) = \omega,"Let $U$ be a convex connected and open set in $\mathbb{R}^n$ , such that $0\in U$ . For every $k$ -differential form $\omega$ , $$\omega =\sum_{i_1\lt\dots\lt i_k} c_{i_1,\dots,i_k}(x)dx{_{i_1}}\wedge\dots\wedge dx{_{i_k}},$$ we define a new $k-1$ differential form, $I\omega$ as follows - $$ \begin{split} (I\omega)  = \sum_{i_1\lt\dots\lt i_k}    \sum_{j=1}^kx_{i_j}    &\left( \int_0^1s^{k-1}c_{i_1,\dots,i_k}(sx)ds\right)\\ & dx_{i_1}\wedge\dots dx_{i_{j-1}}\wedge dx_{i_{j+1}}\wedge\dots\wedge dx_{i_k} \end{split} $$ Prove  that $d(I\omega) + I(d\omega) = \omega$ . So I know that - $d\omega = \sum_{i_1\lt\dots\lt i_k} \sum_{j=1}^n\frac{\partial c_{i_1,\dots,i_k}(x)}{\partial x_j}dx_j\wedge dx_{i_1}\wedge\dots\wedge dx_{i_k}$ and therefore - $I(d\omega) =  \sum_{i_1\lt\dots\lt i_k} \sum_{j=1}^k x_{i_j}(\int_0^1\frac{\partial c_{i_1,\dots,i_k}(sx)}{\partial x_j}ds)dx_{i_1}\wedge\dots dx_{i_{j-1}}\wedge dx_{i_{j+1}}\wedge\dots\wedge dx_{i_k}$ All I need now is to calculate $d(I\omega)$ , and put it into $d(I\omega) + I(d\omega)$ and show that it is equal to $\omega$ . However, I'm not sure - How can I calculate $d(I\omega)$ ? What is $d(I\omega)=\sum_{i_1\lt\dots\lt i_k}x_{i_j}\sum_{j=1}^n\frac{\partial(\int_0^1s^{k-1}c_{i_1,\dots,i_k}(sx)ds)}{\partial d_{x_j}}dx_{i_1}\wedge\dots dx_{i_{j-1}}\wedge dx_{i_{j+1}}\wedge\dots\wedge dx_{i_k}$ ? Or perhaps, is there any other way(some tricks) so I won't need to calculate it explicitly?","Let be a convex connected and open set in , such that . For every -differential form , we define a new differential form, as follows - Prove  that . So I know that - and therefore - All I need now is to calculate , and put it into and show that it is equal to . However, I'm not sure - How can I calculate ? What is ? Or perhaps, is there any other way(some tricks) so I won't need to calculate it explicitly?","U \mathbb{R}^n 0\in U k \omega \omega =\sum_{i_1\lt\dots\lt i_k} c_{i_1,\dots,i_k}(x)dx{_{i_1}}\wedge\dots\wedge dx{_{i_k}}, k-1 I\omega 
\begin{split}
(I\omega)
 = \sum_{i_1\lt\dots\lt i_k}
   \sum_{j=1}^kx_{i_j}
   &\left( \int_0^1s^{k-1}c_{i_1,\dots,i_k}(sx)ds\right)\\
&
dx_{i_1}\wedge\dots dx_{i_{j-1}}\wedge dx_{i_{j+1}}\wedge\dots\wedge dx_{i_k}
\end{split}
 d(I\omega) + I(d\omega) = \omega d\omega = \sum_{i_1\lt\dots\lt i_k} \sum_{j=1}^n\frac{\partial c_{i_1,\dots,i_k}(x)}{\partial x_j}dx_j\wedge dx_{i_1}\wedge\dots\wedge dx_{i_k} I(d\omega) =  \sum_{i_1\lt\dots\lt i_k} \sum_{j=1}^k x_{i_j}(\int_0^1\frac{\partial c_{i_1,\dots,i_k}(sx)}{\partial x_j}ds)dx_{i_1}\wedge\dots dx_{i_{j-1}}\wedge dx_{i_{j+1}}\wedge\dots\wedge dx_{i_k} d(I\omega) d(I\omega) + I(d\omega) \omega d(I\omega) d(I\omega)=\sum_{i_1\lt\dots\lt i_k}x_{i_j}\sum_{j=1}^n\frac{\partial(\int_0^1s^{k-1}c_{i_1,\dots,i_k}(sx)ds)}{\partial d_{x_j}}dx_{i_1}\wedge\dots dx_{i_{j-1}}\wedge dx_{i_{j+1}}\wedge\dots\wedge dx_{i_k}","['calculus', 'integration', 'multivariable-calculus', 'differential-forms']"
76,How to solve this system of equations systematically?,How to solve this system of equations systematically?,,"This might seem a trivial problem, but I have some trouble in arranging the data. So suppose you are given $f(x,y)=x^2y^2(1+x+2y)$ and you want to find it's critical points. Thus we find $$\frac{\partial f}{\partial x}(x,y)=xy^2(2+3x+4y)\textrm{ and }\frac{\partial f}{\partial y}(x,y)=2x^2y(1+x+3y).$$ Now we set $f_x= 0$ and $f_y=0.$ Thus we get a system $$ \begin{split} xy^2(2+3x+4y) &=0\\ 2x^2y(1+x+3y) &=0 \end{split} $$ Now we have a bunch of cases. The way I think about this is as follows: $$((x=0)\lor(y=0)\lor(3x+4y=-2))\land((x=0)\lor(y=0)\lor(x+3y=-1)).$$ Then I consider each possibility separately, but this seems to be slow and sometimes I forget some solutions. Thus I was wondering if there are any other methods which one can use to solve such type of problems.","This might seem a trivial problem, but I have some trouble in arranging the data. So suppose you are given and you want to find it's critical points. Thus we find Now we set and Thus we get a system Now we have a bunch of cases. The way I think about this is as follows: Then I consider each possibility separately, but this seems to be slow and sometimes I forget some solutions. Thus I was wondering if there are any other methods which one can use to solve such type of problems.","f(x,y)=x^2y^2(1+x+2y) \frac{\partial f}{\partial x}(x,y)=xy^2(2+3x+4y)\textrm{ and }\frac{\partial f}{\partial y}(x,y)=2x^2y(1+x+3y). f_x= 0 f_y=0. 
\begin{split}
xy^2(2+3x+4y) &=0\\
2x^2y(1+x+3y) &=0
\end{split}
 ((x=0)\lor(y=0)\lor(3x+4y=-2))\land((x=0)\lor(y=0)\lor(x+3y=-1)).","['multivariable-calculus', 'systems-of-equations', 'maxima-minima']"
77,What is the proof for the Vortex vector field equation?,What is the proof for the Vortex vector field equation?,,"I'm Struggling to understand why the vortex vector field is given by: Vortex vector field equation $\vec F(x,y) = (\frac{-y}{x^2+y^2}, \frac{x}{x^2+y^2})$ If anyone could explain why this is, I would be very grateful. Thank you.","I'm Struggling to understand why the vortex vector field is given by: Vortex vector field equation If anyone could explain why this is, I would be very grateful. Thank you.","\vec F(x,y) = (\frac{-y}{x^2+y^2}, \frac{x}{x^2+y^2})","['multivariable-calculus', 'vectors', 'vector-analysis']"
78,"Proving continuity of a multivariable function by ""reducing"" it to a single variable function","Proving continuity of a multivariable function by ""reducing"" it to a single variable function",,"My goal is to examine the continuity of a certain multivariable function. In that regard, (mainly because my math in the specific area is dusty) I find it somewhat tricky to either prove or disprove that property in the N -diensional space. However, I have come up with the following idea: let $f : \mathbb{R}^2 \rightarrow \mathbb{R} $ be the function that I want to examine at some point $(x_0, y_0)$ . Now I am using the following two functions $g: \mathbb{R}^2 \rightarrow \mathbb{R} $ and $h: \mathbb{R} \rightarrow \mathbb{R} $ so that I can write function $f$ as: $$ f(x,y) = h(g(x,y)),\quad \forall (x,y) \in \mathbb{R}^2 $$ I also know that function $g$ is indeed continuous in $\mathbb{R}^2$ . My assumption is that I can examine the continuity of $h$ at $x' = g(x_0,y_0)$ in order to decide the continuity of $f$ at $(x_0,y_0)$ - a much easier task. In mathimatical notation, I guess what I mean is: $$ \lim_{(x,y)->(x_0,y_0)} f(x,y) = f(x_0, y_0) \quad \iff \\ \lim_{x->g(x_0,y_0)}h(x) = h(g(x_0,y_0)) $$ Actual Questions: 1. Is my assumption correct? 2. If not, maybe do either of $\Rightarrow$ or $\Leftarrow$ hold? (Because if one of those holds then I can use the above to only prove or only disprove accoridnlgy.) 3. Does the same hold for proving/disproving differentiability as well? 4. Under what conditions do any of those hold for a (finite) sum of $h,g$ functions, i.e. $f(x,y) = h_1(g_1(x,y)) + h_2(g_2(x,y)) + ... + h_k(g_k(x,y))$ Note: I can actually post my original function and the transformation if the question is too broad or the answers depend greatly on the actual $f,g$ and $h$ definitions. However I believe a more general question may help more people find it and also make it easier to be understood.","My goal is to examine the continuity of a certain multivariable function. In that regard, (mainly because my math in the specific area is dusty) I find it somewhat tricky to either prove or disprove that property in the N -diensional space. However, I have come up with the following idea: let be the function that I want to examine at some point . Now I am using the following two functions and so that I can write function as: I also know that function is indeed continuous in . My assumption is that I can examine the continuity of at in order to decide the continuity of at - a much easier task. In mathimatical notation, I guess what I mean is: Actual Questions: 1. Is my assumption correct? 2. If not, maybe do either of or hold? (Because if one of those holds then I can use the above to only prove or only disprove accoridnlgy.) 3. Does the same hold for proving/disproving differentiability as well? 4. Under what conditions do any of those hold for a (finite) sum of functions, i.e. Note: I can actually post my original function and the transformation if the question is too broad or the answers depend greatly on the actual and definitions. However I believe a more general question may help more people find it and also make it easier to be understood.","f : \mathbb{R}^2 \rightarrow \mathbb{R}  (x_0, y_0) g: \mathbb{R}^2 \rightarrow \mathbb{R}  h: \mathbb{R} \rightarrow \mathbb{R}  f  f(x,y) = h(g(x,y)),\quad \forall (x,y) \in \mathbb{R}^2
 g \mathbb{R}^2 h x' = g(x_0,y_0) f (x_0,y_0) 
\lim_{(x,y)->(x_0,y_0)} f(x,y) = f(x_0, y_0) \quad \iff \\ \lim_{x->g(x_0,y_0)}h(x) = h(g(x_0,y_0))
 \Rightarrow \Leftarrow h,g f(x,y) = h_1(g_1(x,y)) + h_2(g_2(x,y)) + ... + h_k(g_k(x,y)) f,g h","['multivariable-calculus', 'derivatives', 'continuity']"
79,"Differentiability of the function $f(x,y)=\begin{cases} \frac{xy}{x^2+y^2}, & \text{if $(x,y)\ne(0,0)$}\\ 0, & \text{otherwise} \end{cases}$ [duplicate]","Differentiability of the function (x,y)\ne(0,0) [duplicate]","f(x,y)=\begin{cases} \frac{xy}{x^2+y^2}, & \text{if  }\\ 0, & \text{otherwise} \end{cases}","This question already has answers here : If $F(0,0)=0$ and $F(x,y)= \frac{xy}{x^2+y^2}$ for $(x,y)\neq (0,0)$ then $F$ is differentiable at $(0,0)$? [duplicate] (2 answers) Closed 5 years ago . Consider the function $f(x,y)=\begin{cases}     \frac{xy}{x^2+y^2}, & \text{if $(x,y)\ne(0,0)$}\\     0, & \text{otherwise}   \end{cases}$ Question : Is it differentiable everywhere? Does it have partial derivatives everywhere? My attempt : $f$ is differentiable on $\Bbb R^2\backslash\{(0,0)\}$ because it is the ratio between two differentiable functions and the denominator doesn't vanish. Is it still the case in $(0,0)$? If we write $x=r\cos\theta$ and $y=r\sin\theta$ then the function becomes $\begin{cases}     \frac{r^2\cos\theta\sin\theta}{r^2} & \text{if } r\ne0 & \\      0 &\text{if } r=0 \end{cases}$ = $\begin{cases} \cos\theta\sin\theta &  \end{cases}$ We realize that $f$ doesn't depend on the radius, so the fact that $f(1,1)=1/2\ne-1/2=f(-1,1)$ so we can approach zero by decreasing the radius and preserving the values $1/2$ and one hand and $-1/2$ on the other. So the function is discontinuous at zero, so not differentiable. Does it have partial derivatives everywhere? $$\frac{\partial f}{\partial x}=\lim_{h\to\ 0}\frac{f(h,0)}{h}=0$$ $$\frac{\partial f}{\partial y}=\lim_{h\to\ 0}\frac{f(0,h)}{h}=0$$ So the partial derivatives exist in zero and elsewhere.","This question already has answers here : If $F(0,0)=0$ and $F(x,y)= \frac{xy}{x^2+y^2}$ for $(x,y)\neq (0,0)$ then $F$ is differentiable at $(0,0)$? [duplicate] (2 answers) Closed 5 years ago . Consider the function $f(x,y)=\begin{cases}     \frac{xy}{x^2+y^2}, & \text{if $(x,y)\ne(0,0)$}\\     0, & \text{otherwise}   \end{cases}$ Question : Is it differentiable everywhere? Does it have partial derivatives everywhere? My attempt : $f$ is differentiable on $\Bbb R^2\backslash\{(0,0)\}$ because it is the ratio between two differentiable functions and the denominator doesn't vanish. Is it still the case in $(0,0)$? If we write $x=r\cos\theta$ and $y=r\sin\theta$ then the function becomes $\begin{cases}     \frac{r^2\cos\theta\sin\theta}{r^2} & \text{if } r\ne0 & \\      0 &\text{if } r=0 \end{cases}$ = $\begin{cases} \cos\theta\sin\theta &  \end{cases}$ We realize that $f$ doesn't depend on the radius, so the fact that $f(1,1)=1/2\ne-1/2=f(-1,1)$ so we can approach zero by decreasing the radius and preserving the values $1/2$ and one hand and $-1/2$ on the other. So the function is discontinuous at zero, so not differentiable. Does it have partial derivatives everywhere? $$\frac{\partial f}{\partial x}=\lim_{h\to\ 0}\frac{f(h,0)}{h}=0$$ $$\frac{\partial f}{\partial y}=\lim_{h\to\ 0}\frac{f(0,h)}{h}=0$$ So the partial derivatives exist in zero and elsewhere.",,"['multivariable-calculus', 'proof-verification', 'partial-derivative']"
80,Triple integral with multiple y boundaries.,Triple integral with multiple y boundaries.,,"Problem : I am trying to calculate the mass for the solid bounded by $z = y^5 + 1, \ z = 0, \ y = x, \ y = x^2, \ y = 1.$ The density at each point is directly proportional to the square of the distance from the yz-plane. I split the integral to simplify the calculation: Assuming $\rho=kx^2$, $k \cdot \int_{-1}^0 \int_{x^2}^1 \int_0^{y^5 + 1} x^2 dz \ dy \ dx + k \cdot \int_{0}^1 \int_x^{1} \int_0^{y^5 + 1} x^2 dz \ dy \ dx$ I am wondering if there is an easier way to set up the triple integral and if my limits of integration are correct?","Problem : I am trying to calculate the mass for the solid bounded by $z = y^5 + 1, \ z = 0, \ y = x, \ y = x^2, \ y = 1.$ The density at each point is directly proportional to the square of the distance from the yz-plane. I split the integral to simplify the calculation: Assuming $\rho=kx^2$, $k \cdot \int_{-1}^0 \int_{x^2}^1 \int_0^{y^5 + 1} x^2 dz \ dy \ dx + k \cdot \int_{0}^1 \int_x^{1} \int_0^{y^5 + 1} x^2 dz \ dy \ dx$ I am wondering if there is an easier way to set up the triple integral and if my limits of integration are correct?",,"['integration', 'multivariable-calculus', 'multiple-integral']"
81,"Find the volume bounded by the $xy$ plane, cylinder $x^2 + y^2 = 1$ and sphere $x^2 + y^2 +z^2 = 4$ [duplicate]","Find the volume bounded by the  plane, cylinder  and sphere  [duplicate]",xy x^2 + y^2 = 1 x^2 + y^2 +z^2 = 4,"This question already has answers here : Find the volume of the region inside both the sphere $x^2+y^2+z^2=4$ and the cylinder $x^2+y^2=1$ (2 answers) Closed 6 years ago . Find the volume bounded by the $xy$ plane, cylinder $x^2 + y^2 = 1$ and sphere $x^2 + y^2 +z^2 = 4$. I am struggling with setting up the bounds of integration. First, I will calculate the 'first-quadrant' piece of the volume. $z$  will traverse from $0$ to $2$. $x$ should start from the cylinder and go to the edge of the current circle of the sphere: $$\sqrt{1-y^2} \le x \le \sqrt{4-x^2-z^2}$$ However, the same applies to $y$: (I am only calculating half of the volume right now, where the smaller circle is the lower bound): $$\sqrt{1-x^2} \le y \le \sqrt{4-y^2-z^2}$$  However, this cannot work as both $x$ and $y$ are dependent. What is the error?","This question already has answers here : Find the volume of the region inside both the sphere $x^2+y^2+z^2=4$ and the cylinder $x^2+y^2=1$ (2 answers) Closed 6 years ago . Find the volume bounded by the $xy$ plane, cylinder $x^2 + y^2 = 1$ and sphere $x^2 + y^2 +z^2 = 4$. I am struggling with setting up the bounds of integration. First, I will calculate the 'first-quadrant' piece of the volume. $z$  will traverse from $0$ to $2$. $x$ should start from the cylinder and go to the edge of the current circle of the sphere: $$\sqrt{1-y^2} \le x \le \sqrt{4-x^2-z^2}$$ However, the same applies to $y$: (I am only calculating half of the volume right now, where the smaller circle is the lower bound): $$\sqrt{1-x^2} \le y \le \sqrt{4-y^2-z^2}$$  However, this cannot work as both $x$ and $y$ are dependent. What is the error?",,"['calculus', 'multivariable-calculus', 'volume']"
82,double integral with singularity (not exactly singularity),double integral with singularity (not exactly singularity),,"I'd like to solve $\iint_R \tan^{-1} \frac{y}{x}dA$, where $R=\{ (x,y) : x\ge0, y\ge 0, x^2+y^2 \le 4\}.$ I'm wondering if the following calculation is true: $$ \begin{split} \iint_R \tan^{-1} \frac{y}{x}dA   &=\int_0^{\pi/2}\int_0^2       \tan^{-1} \left(\frac{r \sin \theta}{r\cos\theta}\right)rdrd\theta\\   &=\int_0^{\pi/2}\int_0^2 \tan^{-1}(\tan\theta)rdrd\theta\\   &=\int_0^{\pi/2}\int_0^2\theta rdrd\theta\\   &=\frac{\pi^2}{4} \end{split} $$ It might be true, but I don't think it is a rigid proof. Apparently, $\tan^{-1} \frac{y}{x}$ is not defined for $x=0,$ but since $\lim_{x \to +0} \frac{y}{x} \to \infty$, we may extend or regard $\tan^{-1} \frac{y}{x}=\frac{\pi}{2}$ for $x=0.$ Please let me know if you have any comments for this problem. Thanks in advance!","I'd like to solve $\iint_R \tan^{-1} \frac{y}{x}dA$, where $R=\{ (x,y) : x\ge0, y\ge 0, x^2+y^2 \le 4\}.$ I'm wondering if the following calculation is true: $$ \begin{split} \iint_R \tan^{-1} \frac{y}{x}dA   &=\int_0^{\pi/2}\int_0^2       \tan^{-1} \left(\frac{r \sin \theta}{r\cos\theta}\right)rdrd\theta\\   &=\int_0^{\pi/2}\int_0^2 \tan^{-1}(\tan\theta)rdrd\theta\\   &=\int_0^{\pi/2}\int_0^2\theta rdrd\theta\\   &=\frac{\pi^2}{4} \end{split} $$ It might be true, but I don't think it is a rigid proof. Apparently, $\tan^{-1} \frac{y}{x}$ is not defined for $x=0,$ but since $\lim_{x \to +0} \frac{y}{x} \to \infty$, we may extend or regard $\tan^{-1} \frac{y}{x}=\frac{\pi}{2}$ for $x=0.$ Please let me know if you have any comments for this problem. Thanks in advance!",,"['calculus', 'multivariable-calculus', 'singularity', 'singular-integrals']"
83,"Carmo Curves and Surfaces, $f:V \subset S \to R$ no-where zero differentiable allows easier computation of curvature.","Carmo Curves and Surfaces,  no-where zero differentiable allows easier computation of curvature.",f:V \subset S \to R,"How can I solve this problem? I know that the curvature $K$ is given by $\det(dN_p)$ for any point $p$ in the surface. Here I will write capital $V$'s for the vectors in the problem statement. We should show that $$\langle d(fN)(V_1)\times d(fN)(V_2),fN\rangle=f^3\det(dN_p)$$I know that $$V_1=\alpha_1'(s)=x_uu_1'(s)+x_vv_1'(s)$$ $$V_2=\alpha_2'(s)=x_uu_2'(s)+x_vv_2'(s)$$ for curves $\alpha_1,\alpha_2$ mapping into $V$ from $R^2$. Therefore I think I can write $dfN(V_i)$ which I assume means $d(f \circ N)(V_i)$ as $$dfN(V_i)=dN(V_i)dfN(V_i)=(N_uu_i'(s)+N_vv_i'(s))dfN(V_i)$$ by the chain rule but obviously this doesn't make sense. I feel like I completely misunderstand something very fundamental, perhaps it should be $$dfN(V_i)=df(dN(V_i))=df(N_uu_i'(s)+N_vv_i'(s))$$ Furthermore I think that $N_uu_i'(s)+N_vv_i'(s)$ lies in the tangent plane and so $V_1,V_2$ are made up of the basis vectors $N_u,N_v$ so we can rewrite $$N_uu_i'(s)+N_vv_i'(s)=a_iV_1+b_iV_2$$ which would make sense given the hint in the book. Then can write $$dfN(V_i)=df(a_iV_1+b_iV_2)$$ assuming the second way I wrote the differential is correct. But this is probably completely wrong and I can't see how I could carry on from here anyway. Can anybody help me out?","How can I solve this problem? I know that the curvature $K$ is given by $\det(dN_p)$ for any point $p$ in the surface. Here I will write capital $V$'s for the vectors in the problem statement. We should show that $$\langle d(fN)(V_1)\times d(fN)(V_2),fN\rangle=f^3\det(dN_p)$$I know that $$V_1=\alpha_1'(s)=x_uu_1'(s)+x_vv_1'(s)$$ $$V_2=\alpha_2'(s)=x_uu_2'(s)+x_vv_2'(s)$$ for curves $\alpha_1,\alpha_2$ mapping into $V$ from $R^2$. Therefore I think I can write $dfN(V_i)$ which I assume means $d(f \circ N)(V_i)$ as $$dfN(V_i)=dN(V_i)dfN(V_i)=(N_uu_i'(s)+N_vv_i'(s))dfN(V_i)$$ by the chain rule but obviously this doesn't make sense. I feel like I completely misunderstand something very fundamental, perhaps it should be $$dfN(V_i)=df(dN(V_i))=df(N_uu_i'(s)+N_vv_i'(s))$$ Furthermore I think that $N_uu_i'(s)+N_vv_i'(s)$ lies in the tangent plane and so $V_1,V_2$ are made up of the basis vectors $N_u,N_v$ so we can rewrite $$N_uu_i'(s)+N_vv_i'(s)=a_iV_1+b_iV_2$$ which would make sense given the hint in the book. Then can write $$dfN(V_i)=df(a_iV_1+b_iV_2)$$ assuming the second way I wrote the differential is correct. But this is probably completely wrong and I can't see how I could carry on from here anyway. Can anybody help me out?",,"['multivariable-calculus', 'derivatives', 'differential-geometry']"
84,Diagram of the tangent vector at a point along a curve on a manifold in generalized coordinate system,Diagram of the tangent vector at a point along a curve on a manifold in generalized coordinate system,,"The interpretation of vectors as partial derivatives of the position vector and dual vectors as stacks is visually very appealing. I understand that in the book Gravitation by Misner the passage from one leave of the stack of a covector is made even more intuitive by pretending to be associated to some sort of sound effect (""bongs of bell""). Incredibly, though, I couldn't find any good diagrams of how this would work out to visualize the coordinates of a tangent vector $\vec v$ (in red below) to a point $P$ (central point below) on a manifold $M$ (in turquoise), where $P$ is located through the positional vector $\vec R = x^1(y_1,y_2,y_3)\vec e_1+x^2(y_1,y_2,y_3)\vec e_2+x^3(y_1,y_2,y_3)\vec e_3$ on a generalized curvilinear coordinate system. The vector $\vec v$ is NOT intended to belong to a vector field. It is a velocity vector tangent to the parameterized curve $f.$ Just dweling on one of the curvilinear coordinates in which only the $y_2$ component is allowed to change (in magenta) - $\vec R(c_1, y_2, c_3)$ - and with the stack (covector) represented as the orthogonal layers (also in magenta), I imagine that at point $P$ the tangent vector to a curve $f$ (in red) would be projected on one of the basis vectors, $\frac{\partial \vec R}{\partial y_2}\in T_pM$ (in black) as on the image to the left below. In the event that there is vector field (or scalar field?) defined on the manifold, the number of layers of the covector (1-form) pierced by the projected vector $\vec v$ on $\frac{\partial \vec R}{\partial y_2},$ if read-out directly on the stack (covector $\mathrm dy_2 \in T^*_pM$) as portrayed in the diagram (right side picture) it would amount to $2$ - i.e. the projection onto the black curvilinear coordinate basis vector pierces through exactly $2$ layers. This 1-forms underpin the integration along the curve. Is it correct to say that to introduce the 1-form (covector) a vector field on the manifold (or possibly a scalar field) needs to be defined (not drawn on the diagram)? Is it true that there is no role for 1-forms if all that was needed is to calculate the arc length of the curve $f$ on these generalized curvilinear coordinates, which would instead call for concepts like Lamé coefficients ? The question remains unanswered, likely due to the imprecisions in its formulation. However I tend to think that the correct answer is likely (I look forward to confirmation or corrections): In the case of integration to get the arc length, and since the differential distances, i.e. $dS= \sqrt{1 +\left(\frac{\partial \vec R}{\partial y_i}\right)^2}\mathrm dy,$ are 1-forms, the graphic correct (with recent modifications): the velocity vector $\vec v$ along the integration curve $f$ at point $P$ is part of the tangent space. The component along the curvilinear coordinate $\vec R(c_1,y_2,c_3)$ would give rise to the black vector - i.e. the projection of the vector $\vec v$ with its magnitude given by the number of magenta layers pierced orthogonal to the coordinate $\vec R(c_1,y_2,c_3)$. If there was a scalar or vector field defined on the manifold there would be a gradient or a field vector at point $P.$ The integration along the curve $f$ would implicitly involve for the tangent vector $\vec v$ to the curve $f$ at point $P$ to be dotted with that field vector as in any other point along the curve: $$\int_C \vec F \cdot \mathrm d\vec R =\int_a^b \vec F(\vec R(y_1(t),y_2(t),y_3(t))\cdot \vec R'(y_1(t),y_2(t),y_3(t))\mathrm dt.$$","The interpretation of vectors as partial derivatives of the position vector and dual vectors as stacks is visually very appealing. I understand that in the book Gravitation by Misner the passage from one leave of the stack of a covector is made even more intuitive by pretending to be associated to some sort of sound effect (""bongs of bell""). Incredibly, though, I couldn't find any good diagrams of how this would work out to visualize the coordinates of a tangent vector $\vec v$ (in red below) to a point $P$ (central point below) on a manifold $M$ (in turquoise), where $P$ is located through the positional vector $\vec R = x^1(y_1,y_2,y_3)\vec e_1+x^2(y_1,y_2,y_3)\vec e_2+x^3(y_1,y_2,y_3)\vec e_3$ on a generalized curvilinear coordinate system. The vector $\vec v$ is NOT intended to belong to a vector field. It is a velocity vector tangent to the parameterized curve $f.$ Just dweling on one of the curvilinear coordinates in which only the $y_2$ component is allowed to change (in magenta) - $\vec R(c_1, y_2, c_3)$ - and with the stack (covector) represented as the orthogonal layers (also in magenta), I imagine that at point $P$ the tangent vector to a curve $f$ (in red) would be projected on one of the basis vectors, $\frac{\partial \vec R}{\partial y_2}\in T_pM$ (in black) as on the image to the left below. In the event that there is vector field (or scalar field?) defined on the manifold, the number of layers of the covector (1-form) pierced by the projected vector $\vec v$ on $\frac{\partial \vec R}{\partial y_2},$ if read-out directly on the stack (covector $\mathrm dy_2 \in T^*_pM$) as portrayed in the diagram (right side picture) it would amount to $2$ - i.e. the projection onto the black curvilinear coordinate basis vector pierces through exactly $2$ layers. This 1-forms underpin the integration along the curve. Is it correct to say that to introduce the 1-form (covector) a vector field on the manifold (or possibly a scalar field) needs to be defined (not drawn on the diagram)? Is it true that there is no role for 1-forms if all that was needed is to calculate the arc length of the curve $f$ on these generalized curvilinear coordinates, which would instead call for concepts like Lamé coefficients ? The question remains unanswered, likely due to the imprecisions in its formulation. However I tend to think that the correct answer is likely (I look forward to confirmation or corrections): In the case of integration to get the arc length, and since the differential distances, i.e. $dS= \sqrt{1 +\left(\frac{\partial \vec R}{\partial y_i}\right)^2}\mathrm dy,$ are 1-forms, the graphic correct (with recent modifications): the velocity vector $\vec v$ along the integration curve $f$ at point $P$ is part of the tangent space. The component along the curvilinear coordinate $\vec R(c_1,y_2,c_3)$ would give rise to the black vector - i.e. the projection of the vector $\vec v$ with its magnitude given by the number of magenta layers pierced orthogonal to the coordinate $\vec R(c_1,y_2,c_3)$. If there was a scalar or vector field defined on the manifold there would be a gradient or a field vector at point $P.$ The integration along the curve $f$ would implicitly involve for the tangent vector $\vec v$ to the curve $f$ at point $P$ to be dotted with that field vector as in any other point along the curve: $$\int_C \vec F \cdot \mathrm d\vec R =\int_a^b \vec F(\vec R(y_1(t),y_2(t),y_3(t))\cdot \vec R'(y_1(t),y_2(t),y_3(t))\mathrm dt.$$",,"['multivariable-calculus', 'differential-geometry', 'differential-topology', 'differential-forms']"
85,"Triple integral between spheres $\iiint \sin (xy) - \sin (xz) + \sin (yz) \,dx \,dy \,dz$",Triple integral between spheres,"\iiint \sin (xy) - \sin (xz) + \sin (yz) \,dx \,dy \,dz","I'm hoping someone can tell me if this seems like a legitimate argument. Problem: compute $I:=\iiint_E \sin (xy) - \sin (xz) + \sin (yz) \,dx \,dy \,dz$ where $E = \{(x,y,z) \in \mathbb{R}^3 : 1 \le x^2 + y^2 + z^2 \le 4 \}$. Claim: $I = 0$. First we split it up $I = \iiint_E \sin (xy)  \,dx \,dy \,dz - \iiint_E  \sin (xz)  \,dx \,dy \,dz + \iiint_E \sin (yz) \,dx \,dy \,dz$.  Let's examine just the first of these three: we show $\iiint_E \sin (xy)  \,dx \,dy \,dz =0$.  The key is to decompose the region $E$ and exploit symmetry.  $E$ can be decomposed into several ""pieces:"" let $E_{x=0} = \{ (x,y,z) \in E : x = 0 \}$ $E_{y=0} =\{(x,y,z) \in E : y = 0\}$ $E_{++} = \{ (x,y,z) \in E : x,y >0 \}$ $E_{-+} = \{(x,y,z) \in E : x <0 , y >0 \}$ $E_{+-} = \{(x,y,z) \in E : x >0 , y <0 \}$ $E_{--} = \{(x,y,z) \in E : x <0 , y <0 \}$ and the union of the above is precisely $E$.  We establish a bijection between $E_{++}$ and $E_{-+}$ so that the values of the integrand at corresponding points differ by $-1$: given $(a,b,z) \in E_{++}$ so that $a,b >0$ we associate the point $(-a,b,z) \in E_{-+}$ and this defines a bijection.  Furthermore $\sin (ab) = - \sin (-ab)$ because $\sin$ is odd.  Thus $\sin (ab) + \sin (-ab) = 0$.  We establish a bijection between $E_{+-}$ and $E_{--}$ with the same feature by sending $(a,-b,z) \in E_{+-}$ to $(-a,-b,z)\in E_{--}$.  We note that on $E_{x=0}$ and $E_{y=0}$ the integrand $\sin(xy)$ is $0$.  All of this together shows that $\iiint_E \sin (xy)  \,dx \,dy \,dz =0$. Symmetric arguments show that the other two integrals are $0$, although the subdivisions into pieces will be oriented differently. Therefore $I=0$. Does this seem correct?  I really hope so, because otherwise I have no idea how to solve this problem.  Actually if someone is aware of another way I'd be interested to hear it.  Thank you!","I'm hoping someone can tell me if this seems like a legitimate argument. Problem: compute $I:=\iiint_E \sin (xy) - \sin (xz) + \sin (yz) \,dx \,dy \,dz$ where $E = \{(x,y,z) \in \mathbb{R}^3 : 1 \le x^2 + y^2 + z^2 \le 4 \}$. Claim: $I = 0$. First we split it up $I = \iiint_E \sin (xy)  \,dx \,dy \,dz - \iiint_E  \sin (xz)  \,dx \,dy \,dz + \iiint_E \sin (yz) \,dx \,dy \,dz$.  Let's examine just the first of these three: we show $\iiint_E \sin (xy)  \,dx \,dy \,dz =0$.  The key is to decompose the region $E$ and exploit symmetry.  $E$ can be decomposed into several ""pieces:"" let $E_{x=0} = \{ (x,y,z) \in E : x = 0 \}$ $E_{y=0} =\{(x,y,z) \in E : y = 0\}$ $E_{++} = \{ (x,y,z) \in E : x,y >0 \}$ $E_{-+} = \{(x,y,z) \in E : x <0 , y >0 \}$ $E_{+-} = \{(x,y,z) \in E : x >0 , y <0 \}$ $E_{--} = \{(x,y,z) \in E : x <0 , y <0 \}$ and the union of the above is precisely $E$.  We establish a bijection between $E_{++}$ and $E_{-+}$ so that the values of the integrand at corresponding points differ by $-1$: given $(a,b,z) \in E_{++}$ so that $a,b >0$ we associate the point $(-a,b,z) \in E_{-+}$ and this defines a bijection.  Furthermore $\sin (ab) = - \sin (-ab)$ because $\sin$ is odd.  Thus $\sin (ab) + \sin (-ab) = 0$.  We establish a bijection between $E_{+-}$ and $E_{--}$ with the same feature by sending $(a,-b,z) \in E_{+-}$ to $(-a,-b,z)\in E_{--}$.  We note that on $E_{x=0}$ and $E_{y=0}$ the integrand $\sin(xy)$ is $0$.  All of this together shows that $\iiint_E \sin (xy)  \,dx \,dy \,dz =0$. Symmetric arguments show that the other two integrals are $0$, although the subdivisions into pieces will be oriented differently. Therefore $I=0$. Does this seem correct?  I really hope so, because otherwise I have no idea how to solve this problem.  Actually if someone is aware of another way I'd be interested to hear it.  Thank you!",,"['multivariable-calculus', 'proof-verification']"
86,Surface Integral of a 2-form,Surface Integral of a 2-form,,"Let $\alpha=x\,dx+y\,dy+z\,dz, \gamma=xy\,dz$ Let $D$ be the square $0 \leq x \leq 1, 0 \leq y \leq 1, z=1$ oriented with the upward normal. Calculate $\iint_D \alpha \wedge \gamma$. My professor have a totally different approach then using the formula that is commonly found over the internet, which seems to work on all cases. My attempt: parameterize the surface $\sigma(u,v)=(u,v,1)$  such that it is a transformation from $\mathbb{R}^2$ to $\mathbb{R}^3$ $\alpha \wedge \gamma=-x^2y\,dz\,dx+xy^2\,dy\,dz$ $dX(u,v)=\frac{dX}{du}du+\frac{dX}{dv}dv=1du$ $dY(u,v)=\frac{dY}{du}du+\frac{dY}{dv}dv=1dv$ $dZ(u,v)=\frac{dZ}{du}du+\frac{dZ}{dv}dv=0$ $$\begin{align} \iint_D \alpha \wedge \gamma & = \iint_{D} -x^2y\,dz\,dx+xy^2\,dy\,dz \\ & = \int_0^1\int_0^1 -u^2v\,dZ\,dX+uv^2\,dY\,dZ\\ &= \int_0^1\int_0^1-u^2v(0)(1du)+uv^2(1dv)(0)\\ &= \int_0^1\int_0^1 0 \\ &= 0 \end{align}$$ Is it correct?","Let $\alpha=x\,dx+y\,dy+z\,dz, \gamma=xy\,dz$ Let $D$ be the square $0 \leq x \leq 1, 0 \leq y \leq 1, z=1$ oriented with the upward normal. Calculate $\iint_D \alpha \wedge \gamma$. My professor have a totally different approach then using the formula that is commonly found over the internet, which seems to work on all cases. My attempt: parameterize the surface $\sigma(u,v)=(u,v,1)$  such that it is a transformation from $\mathbb{R}^2$ to $\mathbb{R}^3$ $\alpha \wedge \gamma=-x^2y\,dz\,dx+xy^2\,dy\,dz$ $dX(u,v)=\frac{dX}{du}du+\frac{dX}{dv}dv=1du$ $dY(u,v)=\frac{dY}{du}du+\frac{dY}{dv}dv=1dv$ $dZ(u,v)=\frac{dZ}{du}du+\frac{dZ}{dv}dv=0$ $$\begin{align} \iint_D \alpha \wedge \gamma & = \iint_{D} -x^2y\,dz\,dx+xy^2\,dy\,dz \\ & = \int_0^1\int_0^1 -u^2v\,dZ\,dX+uv^2\,dY\,dZ\\ &= \int_0^1\int_0^1-u^2v(0)(1du)+uv^2(1dv)(0)\\ &= \int_0^1\int_0^1 0 \\ &= 0 \end{align}$$ Is it correct?",,"['multivariable-calculus', 'differential-forms', 'surface-integrals']"
87,Why are continuous partial derivatives a sufficient condition for differentiability?,Why are continuous partial derivatives a sufficient condition for differentiability?,,"$\mathbf {Theorem}$: if $\ f$ is a function of $\ x$ and $\ y$, where $\ f_{x}$ and $\ f_{y}$ are continuous in an open region $\ R$, then $\ f$ is differentiable on $\ R$. Proof:Let $\ S$ be a surface defined by $\ z=f(x,y)$, where $\ f$, $\ f_{x}$, and $\ f_{y}$ are continuous at $\ (x,y)$. Let $\Delta z$ be the total change in $\ f$ from two points on the surface $\ S$ , let $\Delta z_{x}$ be the change in $\ f$ when $\ y$ is held constant and let $\Delta z_{y}$ be the change in $\ f$ when x is held constant.  $$\Delta z=f(x+\Delta x, y+\Delta y)-f(x,y)$$ $$\Delta z= [f(x+\Delta x,y)-f(x,y)]+[f(x+\Delta x, y+\Delta y)-f(x+\Delta x,y)]$$ $$\Delta z= \Delta z_{x}+\Delta z_{y}$$ By the Mean Value Theorem there is some $\ x_{1}$ between $\ x$ and $\ x+\Delta x$ and some $\ y_{1}$ between $\ y$ and $\ y+\Delta y$ such that $$\Delta z_{x}=f_{x}(x_{1},y)\Delta x$$ $$\Delta z_{y}=f_{y}(x+\Delta x,y_{1})\Delta y$$ If you define $\epsilon_{1}$ and $\epsilon_{2}$ as $$\epsilon_{1}=f_{x}(x_{1},y)-f_{x}(x,y)$$ $$\epsilon_{2}=f_{y}(x+\Delta x, y_{1})-f_{y}(x,y)$$ It follows that $$\Delta z=\Delta z_{x}+\Delta z_{y}=[f_{x}(x,y)\Delta x + f_{y}(x,y)\Delta y]+\epsilon_{1}\Delta x + \epsilon_{2}\Delta y$$ By the continuity of $\ f_{x}$ and $\ f_{y}$ and the fact that $\ x\le x_{1}\le x+\Delta x$, and $\ y\le y_{1}\le y+\Delta y$, it follows that $$\epsilon_{1}\rightarrow 0\ \ \text{and}\ \ \epsilon_{2}\rightarrow 0\ \ \text{as}$$ $$\Delta x\rightarrow 0\ \ \text{and}\ \ \Delta y\rightarrow 0$$ Therefore, by definition, $\ f$ is differentiable. My question is, why do the continuity of the partial derivatives $\ f_{x}$ and $\ f_{y}$ and the intervals that $\ x_{1}$ and $\ y_{1}$ lie on imply that $\ f$ is differentiable?","$\mathbf {Theorem}$: if $\ f$ is a function of $\ x$ and $\ y$, where $\ f_{x}$ and $\ f_{y}$ are continuous in an open region $\ R$, then $\ f$ is differentiable on $\ R$. Proof:Let $\ S$ be a surface defined by $\ z=f(x,y)$, where $\ f$, $\ f_{x}$, and $\ f_{y}$ are continuous at $\ (x,y)$. Let $\Delta z$ be the total change in $\ f$ from two points on the surface $\ S$ , let $\Delta z_{x}$ be the change in $\ f$ when $\ y$ is held constant and let $\Delta z_{y}$ be the change in $\ f$ when x is held constant.  $$\Delta z=f(x+\Delta x, y+\Delta y)-f(x,y)$$ $$\Delta z= [f(x+\Delta x,y)-f(x,y)]+[f(x+\Delta x, y+\Delta y)-f(x+\Delta x,y)]$$ $$\Delta z= \Delta z_{x}+\Delta z_{y}$$ By the Mean Value Theorem there is some $\ x_{1}$ between $\ x$ and $\ x+\Delta x$ and some $\ y_{1}$ between $\ y$ and $\ y+\Delta y$ such that $$\Delta z_{x}=f_{x}(x_{1},y)\Delta x$$ $$\Delta z_{y}=f_{y}(x+\Delta x,y_{1})\Delta y$$ If you define $\epsilon_{1}$ and $\epsilon_{2}$ as $$\epsilon_{1}=f_{x}(x_{1},y)-f_{x}(x,y)$$ $$\epsilon_{2}=f_{y}(x+\Delta x, y_{1})-f_{y}(x,y)$$ It follows that $$\Delta z=\Delta z_{x}+\Delta z_{y}=[f_{x}(x,y)\Delta x + f_{y}(x,y)\Delta y]+\epsilon_{1}\Delta x + \epsilon_{2}\Delta y$$ By the continuity of $\ f_{x}$ and $\ f_{y}$ and the fact that $\ x\le x_{1}\le x+\Delta x$, and $\ y\le y_{1}\le y+\Delta y$, it follows that $$\epsilon_{1}\rightarrow 0\ \ \text{and}\ \ \epsilon_{2}\rightarrow 0\ \ \text{as}$$ $$\Delta x\rightarrow 0\ \ \text{and}\ \ \Delta y\rightarrow 0$$ Therefore, by definition, $\ f$ is differentiable. My question is, why do the continuity of the partial derivatives $\ f_{x}$ and $\ f_{y}$ and the intervals that $\ x_{1}$ and $\ y_{1}$ lie on imply that $\ f$ is differentiable?",,"['multivariable-calculus', 'continuity', 'partial-derivative']"
88,Is there a simple proof for the (co)area formula in the case of equal dimensions?,Is there a simple proof for the (co)area formula in the case of equal dimensions?,,"Let $\Omega \subseteq \mathbb{R}^n$ be a nice domain (say a ball), and let $f:\Omega \to \mathbb{R}^n$ be smooth. Then $$ \int_{\Omega} |\det df|=\int_{\mathbb{R}^n} |f^{-1}(y)|dy, \tag{1}$$ where $|f^{-1}(y)|$ is the cardinality of the set $\{f^{-1}(y)\}$. Formula $(1)$ is a special case of the area and co-area formulas , when the dimensions of the source and target are equal. In general, the area\co-area formulas: Deal with domains of different dimensions. Allow the domain to be any measurable set. Allow $f$ to be less regular (require only Lipschitzity). The proofs for the general claims are not very short or easy. Question: Is there a simpler proof for the specific case mentioned above? (Equal dimensions, $f$ smooth, nice smooth domain). A reference would suffice (I couldn't find one).","Let $\Omega \subseteq \mathbb{R}^n$ be a nice domain (say a ball), and let $f:\Omega \to \mathbb{R}^n$ be smooth. Then $$ \int_{\Omega} |\det df|=\int_{\mathbb{R}^n} |f^{-1}(y)|dy, \tag{1}$$ where $|f^{-1}(y)|$ is the cardinality of the set $\{f^{-1}(y)\}$. Formula $(1)$ is a special case of the area and co-area formulas , when the dimensions of the source and target are equal. In general, the area\co-area formulas: Deal with domains of different dimensions. Allow the domain to be any measurable set. Allow $f$ to be less regular (require only Lipschitzity). The proofs for the general claims are not very short or easy. Question: Is there a simpler proof for the specific case mentioned above? (Equal dimensions, $f$ smooth, nice smooth domain). A reference would suffice (I couldn't find one).",,"['real-analysis', 'multivariable-calculus', 'area', 'jacobian', 'change-of-variable']"
89,Finding the right dimensions of a physics-motivated equation,Finding the right dimensions of a physics-motivated equation,,"I have been trying to find the right dimensions of some functions in the equation $$\begin{cases}-\rho U\cdot \nabla u = -\nabla p + \nabla^2u \\ \nabla \cdot u = 0 \end{cases}$$ as seen here , in this Wikipedia article about the Oseen equations. $u$ and $U$ are two velocities, which makes me think they are both vectors of the same dimension $d$ (the dimension we are working on) and they depend on the spatial coordinates (and maybe on time as well). So I think $u, U: \mathbb{R}^d\to \Bbb{R}^d$ or $u,U:\Bbb{R}^{d+1}\to\Bbb{R}^d$. $p$ is the pressure, so I would say it is a scalar function that depends on the spatial coordinates as well, but not on time, so $p: \Bbb{R}^d \to \Bbb{R}$. I believe my struggle has to do with the fact that I don't know what are the meanings of the operators in the context they are being used. If $u,U$ and $p$ have the dimensions I think they have, then  $\nabla \cdot u$ is the only thing that I understand. What does the gradient, $\nabla u$, mean? When $u$ is scalar, $\nabla u$ is a vector. What if $u$ is a vector already? Is $\nabla u$ a matrix? While trying to understand what is going on I read about dyadics in Wikipedia. It felt like it could make sense here, but then what is the meaning of $U\cdot \nabla u$, if $U$ is a vector and $\nabla u$ is a matrix? And of course, $\nabla^2$ which I know is the Laplace operator, with which I am also familiar when applied to a scalar function. If $u$ is a vector, then what is $\nabla^2 u$? Is it a vector as well? I think it should be, in order to be summable with $-\nabla p$... If anyone could point me to something to read/learn in order to be able to understand this, I would be glad. The problem is, I don't even know where to go looking for answers.","I have been trying to find the right dimensions of some functions in the equation $$\begin{cases}-\rho U\cdot \nabla u = -\nabla p + \nabla^2u \\ \nabla \cdot u = 0 \end{cases}$$ as seen here , in this Wikipedia article about the Oseen equations. $u$ and $U$ are two velocities, which makes me think they are both vectors of the same dimension $d$ (the dimension we are working on) and they depend on the spatial coordinates (and maybe on time as well). So I think $u, U: \mathbb{R}^d\to \Bbb{R}^d$ or $u,U:\Bbb{R}^{d+1}\to\Bbb{R}^d$. $p$ is the pressure, so I would say it is a scalar function that depends on the spatial coordinates as well, but not on time, so $p: \Bbb{R}^d \to \Bbb{R}$. I believe my struggle has to do with the fact that I don't know what are the meanings of the operators in the context they are being used. If $u,U$ and $p$ have the dimensions I think they have, then  $\nabla \cdot u$ is the only thing that I understand. What does the gradient, $\nabla u$, mean? When $u$ is scalar, $\nabla u$ is a vector. What if $u$ is a vector already? Is $\nabla u$ a matrix? While trying to understand what is going on I read about dyadics in Wikipedia. It felt like it could make sense here, but then what is the meaning of $U\cdot \nabla u$, if $U$ is a vector and $\nabla u$ is a matrix? And of course, $\nabla^2$ which I know is the Laplace operator, with which I am also familiar when applied to a scalar function. If $u$ is a vector, then what is $\nabla^2 u$? Is it a vector as well? I think it should be, in order to be summable with $-\nabla p$... If anyone could point me to something to read/learn in order to be able to understand this, I would be glad. The problem is, I don't even know where to go looking for answers.",,"['multivariable-calculus', 'physics', 'tensors']"
90,"Why use Bordered Hessian than ""simple"" Hessian as second derivative test?","Why use Bordered Hessian than ""simple"" Hessian as second derivative test?",,"Why use Bordered Hessian than ""simple"" Hessian as second derivative test in a multi constrained optimization problem? The critical points are found from the Lagrangian so they follow the constraints. All you have to do is to check the convexity/concavity with Hessian in these critical points and tell whether it is local minimum or maximum. I find it much easier in calculations and to remember the rules of convexity/concavity, than these of bordered Hessian...","Why use Bordered Hessian than ""simple"" Hessian as second derivative test in a multi constrained optimization problem? The critical points are found from the Lagrangian so they follow the constraints. All you have to do is to check the convexity/concavity with Hessian in these critical points and tell whether it is local minimum or maximum. I find it much easier in calculations and to remember the rules of convexity/concavity, than these of bordered Hessian...",,"['multivariable-calculus', 'optimization', 'constraints', 'hessian-matrix']"
91,Integrate Product of Matrix Exponentials,Integrate Product of Matrix Exponentials,,"I'm trying to compute $$I(t) = \int_0^t e^{ A \tau} e^{A^T \tau} \ d \tau $$ where $A$ is a real matrix, and $A^T$ its transpose. I know that if $A$ was symmetric and $B = A + A^T$  nonsingular, I could use  the rule for the Integral of matrix exponential and the result would be $$ I(t) = B^{-1} \left( e^{t B } - I \right) $$ What if $A$ is not symmetric, though?","I'm trying to compute $$I(t) = \int_0^t e^{ A \tau} e^{A^T \tau} \ d \tau $$ where $A$ is a real matrix, and $A^T$ its transpose. I know that if $A$ was symmetric and $B = A + A^T$  nonsingular, I could use  the rule for the Integral of matrix exponential and the result would be $$ I(t) = B^{-1} \left( e^{t B } - I \right) $$ What if $A$ is not symmetric, though?",,"['multivariable-calculus', 'definite-integrals', 'matrix-equations', 'matrix-calculus', 'matrix-exponential']"
92,Switching Iterated Integrals from dxdy to dydx,Switching Iterated Integrals from dxdy to dydx,,"I am having issues switching an iterated integral from dxdy to dydx : Switch the integral $\int_0^2 \int_y^{2y}6xy$ dx dy to a dy dx integral in the form of $\int_0^2 \int_{...}^{...}6xydydx$ + $\int_2^4 \int_{...}^{...}6xydydx$ What I'm having issues with here is trying to break it up into pieces-- is the first one just from 0 to y, and the next one from y to 2y?","I am having issues switching an iterated integral from dxdy to dydx : Switch the integral $\int_0^2 \int_y^{2y}6xy$ dx dy to a dy dx integral in the form of $\int_0^2 \int_{...}^{...}6xydydx$ + $\int_2^4 \int_{...}^{...}6xydydx$ What I'm having issues with here is trying to break it up into pieces-- is the first one just from 0 to y, and the next one from y to 2y?",,"['calculus', 'integration', 'multivariable-calculus']"
93,"Calculate the triple integral $\iiint_D \sqrt{x^2+y^2+z^2}\, dV$.",Calculate the triple integral .,"\iiint_D \sqrt{x^2+y^2+z^2}\, dV","Calculate the triple integral of $$\iiint_D \sqrt{x^2+y^2+z^2}\, dV$$ where $D$ is bounded by (1) $x^2+y^2+z^2=2ay$ and (2) $y=\sqrt{x^2+z^2}$ . So far I was thinking that if I made $y$ turn to $z$ and $z$ turn to $y$ I could use cilyndrical coordinates So (1) $a^2=r^2+(z-a)^2$ and (2) $z=r$ and both intersect at $z=a$ It is correct to propose the integral $$\int_0^{2\pi}\int_0^a\int_0^{\sqrt{a^2-(z-a)^2}}r\sqrt{z^2+r^2} \,dr\,dz\,d\theta$$ ? And if so I evaluate and get $a^3\pi$ but Im suspicious of this result",Calculate the triple integral of where is bounded by (1) and (2) . So far I was thinking that if I made turn to and turn to I could use cilyndrical coordinates So (1) and (2) and both intersect at It is correct to propose the integral ? And if so I evaluate and get but Im suspicious of this result,"\iiint_D \sqrt{x^2+y^2+z^2}\, dV D x^2+y^2+z^2=2ay y=\sqrt{x^2+z^2} y z z y a^2=r^2+(z-a)^2 z=r z=a \int_0^{2\pi}\int_0^a\int_0^{\sqrt{a^2-(z-a)^2}}r\sqrt{z^2+r^2} \,dr\,dz\,d\theta a^3\pi","['integration', 'multivariable-calculus', 'definite-integrals', 'volume', 'multiple-integral']"
94,A map is differentiable if and only if its pullback preserves differentiablity,A map is differentiable if and only if its pullback preserves differentiablity,,"Given $A,B$ two normed finite dimensional vector spaces and $f: U\subset A \to B$ a continuous map (where $U$ is open in $A$); show that, $f$ is differentiable ($C^{\infty}$) if and only if $f^{*}(C^\infty(B))\subseteq C^{\infty}(U)$. Here, $f^*(g)= g\circ f$. It's easy to see that if $f$ is $C^\infty$ then, $f^{*}(C^\infty(B))\subseteq C^{\infty}(U)$ because composition of differentiable maps is differentiable. However, I have no clue in the other direction.","Given $A,B$ two normed finite dimensional vector spaces and $f: U\subset A \to B$ a continuous map (where $U$ is open in $A$); show that, $f$ is differentiable ($C^{\infty}$) if and only if $f^{*}(C^\infty(B))\subseteq C^{\infty}(U)$. Here, $f^*(g)= g\circ f$. It's easy to see that if $f$ is $C^\infty$ then, $f^{*}(C^\infty(B))\subseteq C^{\infty}(U)$ because composition of differentiable maps is differentiable. However, I have no clue in the other direction.",,['multivariable-calculus']
95,"What is the domain and range of $f(x,y)=3x^2+2y^2-5$",What is the domain and range of,"f(x,y)=3x^2+2y^2-5","What is the domain and range of $f(x,y)=3x^2+2y^2-5$ I'm just starting to learn multivariable, and need help on this question. Let $z=3x^2+2y^2-5$ The domain is $S=\{(x,y)\in\mathbb{R}^2|x,y\in(-\infty,\infty)\}$ There is no restriction on the domain. What about the range? I can't really imagine this curve and so I don't know for which values of $z$ it can cover. How can I solve this?","What is the domain and range of $f(x,y)=3x^2+2y^2-5$ I'm just starting to learn multivariable, and need help on this question. Let $z=3x^2+2y^2-5$ The domain is $S=\{(x,y)\in\mathbb{R}^2|x,y\in(-\infty,\infty)\}$ There is no restriction on the domain. What about the range? I can't really imagine this curve and so I don't know for which values of $z$ it can cover. How can I solve this?",,"['calculus', 'real-analysis', 'multivariable-calculus']"
96,How does this application of the chain rule work?,How does this application of the chain rule work?,,"Let $\bar{x_1} = x_1 \cos(x_2)$ and $ \bar{x_2} = x_1 \sin(x_2)$ Suppose that $f:\mathbb{R}^{2} \to \mathbb{R}^{2}$ is a smooth function of $\bar{x_1}$ and $\bar{x_2}.$ Show that: $(\frac{\partial{f}}{\partial\bar{x_1}})^2+(\frac{\partial{f}}{\partial\bar{x_2}})^2 = (\frac{\partial{f}}{\partial{x_1}})^2+ \frac{1}{x_1^2}(\frac{\partial{f}}{\partial{x_2}})^2$. Using the Chain Rule: $\frac{\partial{f}}{\partial{x_1}} = \frac{\partial{f}}{\partial{\bar{x_1}}} \frac{\partial{\bar{x_1}}}{\partial{x_1}}  + \frac{\partial{f}}{\partial{\bar{x_2}}}\frac{\partial{\bar{x_2}}}{\partial{x_1}}$  and $\frac{\partial{f}}{\partial{x_2}} = \frac{\partial{f}}{\partial{\bar{x_1}}} \frac{\partial{\bar{x_1}}}{\partial{x_2}}  + \frac{\partial{f}}{\partial{\bar{x_2}}}\frac{\partial{\bar{x_2}}}{\partial{x_2}}$ $\frac{\partial{\bar{x_1}}}{\partial{x_1}} = \cos{x_2}$, $\frac{\partial{\bar{x_2}}}{\partial{x_1}}=  \sin{x_2 }$, $\frac{\partial{\bar{x_1}}}{\partial{x_2}} = -x_{1}\sin{x_2}$, $\frac{\partial{\bar{x_2}}}{\partial{x_2}} = x_{1}\cos{x_2}$ $\partial_{x_1}{f} = \cos{x_2}\partial_{\bar{x_1}}{f}+\sin{x_2}\partial_{\bar{x_2}}{f}$ $\partial_{x_2}{f} = -x_{1}\sin{x_2}\partial_{\bar{x_1}}{f}+x_{1}\cos{x_2}\partial_{\bar{x_2}}{f}$ $(\partial_{x_1}{f})^{2} = (\frac{\partial f}{\partial \bar{x}_1} \cos{x_2}+\frac{\partial f}{\partial \bar{x}_2} \sin{x}_2) ^{2} = (\frac{\partial f}{\partial \bar{x}_1}) ^{2} \cos^{2}x_2 + 2 \frac{\partial f}{\partial \bar{x}_1} \frac{\partial f}{\partial \bar{x}_2} \cos{x}_2 \sin{x}_2+(\frac{\partial f}{\partial \bar{x}_2})^{2}\sin^{2}x_2$ $(\partial_{x_2}{f})^{2} = (\frac{\partial f}{\partial \bar{x}_1} (-x_1\sin x_2) + \frac{\partial f}{\partial \bar{x}_2}x_1\cos x_2)^{2} =  \frac{\partial f}{\partial \bar{x}_1}^{2}(x_1^{2}\sin^{2}x_2)+2\frac{\partial f}{\partial \bar{x}_1}\frac{\partial f}{\partial \bar{x}_2}(-x_1\sin x_2)(x_2\cos x_2) + (\frac{\partial f}{\partial \bar{x}_2})^{2}x_1^{2}\cos^{2}x_2$ $\frac{\partial f}{\partial \bar{x}_2}^{2} = (x_1)^{2}[(\frac{\partial f}{\partial \bar{x}_1})^{2}\sin^{2}x_{2}-2\sin x_2\cos x_2 \frac{\partial f}{\partial \bar{x}_1}\frac{\partial f}{\partial \bar{x}_2}+(\frac{\partial f}{\partial \bar{x}_2})^{2}\cos^{2}x_2]$ So $(\frac{\partial f}{\partial x_1})^{2}+ \frac{1}{x_1^{2}}(\frac{\partial f}{\partial x_2})^{2} = \frac{\partial f}{\partial \bar{x}_1}^{2}(\cos^2 x_2 + \sin^2 x_2) + (\frac{\partial f}{\partial \bar x_2})^2(\sin^2 x_2+\cos^2 x_2)$ $= (\frac{\partial f}{\partial \bar{x}_1})^{2}+(\frac{\partial f}{\partial \bar{x}_2})^{2}$ When solving for $\frac{\partial f}{\partial x_1}$, isn't $x_1$ dependent on $x_2$ and $\bar{x}_1$ and $\bar{x}_2$? So why is his $\frac{\partial f}{\partial x_1}$ formulated in a way that does not include $x_2$? And also My professor only showed the right side of the equation without expanding the LHS. How would someone know that both the RHS and LHS are equivalent without the answer? Would they need to expand the LHS to see? and if you were to expand the LHS, how would $\frac{\partial f}{\partial \bar{x}_1}$ look?","Let $\bar{x_1} = x_1 \cos(x_2)$ and $ \bar{x_2} = x_1 \sin(x_2)$ Suppose that $f:\mathbb{R}^{2} \to \mathbb{R}^{2}$ is a smooth function of $\bar{x_1}$ and $\bar{x_2}.$ Show that: $(\frac{\partial{f}}{\partial\bar{x_1}})^2+(\frac{\partial{f}}{\partial\bar{x_2}})^2 = (\frac{\partial{f}}{\partial{x_1}})^2+ \frac{1}{x_1^2}(\frac{\partial{f}}{\partial{x_2}})^2$. Using the Chain Rule: $\frac{\partial{f}}{\partial{x_1}} = \frac{\partial{f}}{\partial{\bar{x_1}}} \frac{\partial{\bar{x_1}}}{\partial{x_1}}  + \frac{\partial{f}}{\partial{\bar{x_2}}}\frac{\partial{\bar{x_2}}}{\partial{x_1}}$  and $\frac{\partial{f}}{\partial{x_2}} = \frac{\partial{f}}{\partial{\bar{x_1}}} \frac{\partial{\bar{x_1}}}{\partial{x_2}}  + \frac{\partial{f}}{\partial{\bar{x_2}}}\frac{\partial{\bar{x_2}}}{\partial{x_2}}$ $\frac{\partial{\bar{x_1}}}{\partial{x_1}} = \cos{x_2}$, $\frac{\partial{\bar{x_2}}}{\partial{x_1}}=  \sin{x_2 }$, $\frac{\partial{\bar{x_1}}}{\partial{x_2}} = -x_{1}\sin{x_2}$, $\frac{\partial{\bar{x_2}}}{\partial{x_2}} = x_{1}\cos{x_2}$ $\partial_{x_1}{f} = \cos{x_2}\partial_{\bar{x_1}}{f}+\sin{x_2}\partial_{\bar{x_2}}{f}$ $\partial_{x_2}{f} = -x_{1}\sin{x_2}\partial_{\bar{x_1}}{f}+x_{1}\cos{x_2}\partial_{\bar{x_2}}{f}$ $(\partial_{x_1}{f})^{2} = (\frac{\partial f}{\partial \bar{x}_1} \cos{x_2}+\frac{\partial f}{\partial \bar{x}_2} \sin{x}_2) ^{2} = (\frac{\partial f}{\partial \bar{x}_1}) ^{2} \cos^{2}x_2 + 2 \frac{\partial f}{\partial \bar{x}_1} \frac{\partial f}{\partial \bar{x}_2} \cos{x}_2 \sin{x}_2+(\frac{\partial f}{\partial \bar{x}_2})^{2}\sin^{2}x_2$ $(\partial_{x_2}{f})^{2} = (\frac{\partial f}{\partial \bar{x}_1} (-x_1\sin x_2) + \frac{\partial f}{\partial \bar{x}_2}x_1\cos x_2)^{2} =  \frac{\partial f}{\partial \bar{x}_1}^{2}(x_1^{2}\sin^{2}x_2)+2\frac{\partial f}{\partial \bar{x}_1}\frac{\partial f}{\partial \bar{x}_2}(-x_1\sin x_2)(x_2\cos x_2) + (\frac{\partial f}{\partial \bar{x}_2})^{2}x_1^{2}\cos^{2}x_2$ $\frac{\partial f}{\partial \bar{x}_2}^{2} = (x_1)^{2}[(\frac{\partial f}{\partial \bar{x}_1})^{2}\sin^{2}x_{2}-2\sin x_2\cos x_2 \frac{\partial f}{\partial \bar{x}_1}\frac{\partial f}{\partial \bar{x}_2}+(\frac{\partial f}{\partial \bar{x}_2})^{2}\cos^{2}x_2]$ So $(\frac{\partial f}{\partial x_1})^{2}+ \frac{1}{x_1^{2}}(\frac{\partial f}{\partial x_2})^{2} = \frac{\partial f}{\partial \bar{x}_1}^{2}(\cos^2 x_2 + \sin^2 x_2) + (\frac{\partial f}{\partial \bar x_2})^2(\sin^2 x_2+\cos^2 x_2)$ $= (\frac{\partial f}{\partial \bar{x}_1})^{2}+(\frac{\partial f}{\partial \bar{x}_2})^{2}$ When solving for $\frac{\partial f}{\partial x_1}$, isn't $x_1$ dependent on $x_2$ and $\bar{x}_1$ and $\bar{x}_2$? So why is his $\frac{\partial f}{\partial x_1}$ formulated in a way that does not include $x_2$? And also My professor only showed the right side of the equation without expanding the LHS. How would someone know that both the RHS and LHS are equivalent without the answer? Would they need to expand the LHS to see? and if you were to expand the LHS, how would $\frac{\partial f}{\partial \bar{x}_1}$ look?",,['multivariable-calculus']
97,"If $f:\mathbb{R}^n \to \mathbb{R}^m$ is $C^1$ with $a \in \mathbb{R}^n$, and rank$(Df_a) = m$, prove or disprove the following:","If  is  with , and rank, prove or disprove the following:",f:\mathbb{R}^n \to \mathbb{R}^m C^1 a \in \mathbb{R}^n (Df_a) = m,"If $f:\mathbb{R}^n \to \mathbb{R}^m$ is $C^1$ with $a \in \mathbb{R}^n$, and  rank$(Df_a) = m$, then there exists $\epsilon > 0$, such that $B_\epsilon (f(a)) \subseteq f(\mathbb{R}^n)$. My first instinct is to use Taylor's Theorem and try $f(a+h)-f(a) = Df_a (h) + r(h)$ and see if I can turn $f(a+h) - f(a)$ into some ball of radius epsilon using the fact that $Df_a$ is onto but I really have no clue how to proceed from here.","If $f:\mathbb{R}^n \to \mathbb{R}^m$ is $C^1$ with $a \in \mathbb{R}^n$, and  rank$(Df_a) = m$, then there exists $\epsilon > 0$, such that $B_\epsilon (f(a)) \subseteq f(\mathbb{R}^n)$. My first instinct is to use Taylor's Theorem and try $f(a+h)-f(a) = Df_a (h) + r(h)$ and see if I can turn $f(a+h) - f(a)$ into some ball of radius epsilon using the fact that $Df_a$ is onto but I really have no clue how to proceed from here.",,"['real-analysis', 'multivariable-calculus']"
98,continuity of function defined by surface integral,continuity of function defined by surface integral,,"I'm considering this function for positive $r$ $$ \varphi(r) = \int\limits_{\partial B(0,r)} f \, dS. $$ where $f$ is a $C^1(\mathbb{R}^n)$ function and $\partial B(0,r)$ is the surface of the $n$-sphere. I think this function should be continuous (even if $f$ is only continuous) but I am having a hard time proving it, I should find a small enough $h>0$ so $$ \left| \partial B(0,r+h) - \partial B(0,r) \right| < \varepsilon. $$ My failed ideas -This would be routine if I had $\partial B(0,r) \subset \partial B(0,r+h)$ but that is false, however if I could somehow transform the integral from the boundary to the interior of the ball that would solve it but I don't know if that's possible. -The other idea was to show that the $\varphi(r) = f(c_r) \operatorname{Measure}(\partial B(0,r))$ for a $c_r$ depending on r and going from there, but then I need assume that $\lim\limits_{h \to 0} c_{r+h} = c_r$ but that's pretty much assuming what I'm trying to prove. I'm probably missing something important here, so any hints would be greatly appreciated.","I'm considering this function for positive $r$ $$ \varphi(r) = \int\limits_{\partial B(0,r)} f \, dS. $$ where $f$ is a $C^1(\mathbb{R}^n)$ function and $\partial B(0,r)$ is the surface of the $n$-sphere. I think this function should be continuous (even if $f$ is only continuous) but I am having a hard time proving it, I should find a small enough $h>0$ so $$ \left| \partial B(0,r+h) - \partial B(0,r) \right| < \varepsilon. $$ My failed ideas -This would be routine if I had $\partial B(0,r) \subset \partial B(0,r+h)$ but that is false, however if I could somehow transform the integral from the boundary to the interior of the ball that would solve it but I don't know if that's possible. -The other idea was to show that the $\varphi(r) = f(c_r) \operatorname{Measure}(\partial B(0,r))$ for a $c_r$ depending on r and going from there, but then I need assume that $\lim\limits_{h \to 0} c_{r+h} = c_r$ but that's pretty much assuming what I'm trying to prove. I'm probably missing something important here, so any hints would be greatly appreciated.",,"['real-analysis', 'multivariable-calculus']"
99,A question about gradient field,A question about gradient field,,"Let $\vec{F}=(xy,x^2+y^2)$ be a vector field.  Is there exist a function $f(x,y)$ such that $\vec{\nabla}f=\vec{F}$? My attempt: if $f_x=xy$ and $f_y=x^2+y^2$, then $f(x,y)=\frac{x^2y}{2}+g(y)$. Therefore $f_y=\frac{x^2}{2}+g'(y)$. Hence $\frac{x^2}{2}+g'(y)=x^2+y^2$, so $g'(y)=\frac{x^2}{2}+y^2$. But $g(y)$ is a function of $y$,  a contradiction. On the other hand, if you take any circle $C(t)=(R\cos t,R\sin t)$, $t\in[0,2\pi]$ then  $$ \oint_{C}f\cdot d\vec{l}=\int_{0}^{2\pi}(R^2\cos t\sin t,R^2)\cdot(-R\sin t,R\cos t)\,dt=R^3\int_{0}^{2\pi}\cos^3t\,dt=0 $$  Why this does not contradict the first claim? Thanks","Let $\vec{F}=(xy,x^2+y^2)$ be a vector field.  Is there exist a function $f(x,y)$ such that $\vec{\nabla}f=\vec{F}$? My attempt: if $f_x=xy$ and $f_y=x^2+y^2$, then $f(x,y)=\frac{x^2y}{2}+g(y)$. Therefore $f_y=\frac{x^2}{2}+g'(y)$. Hence $\frac{x^2}{2}+g'(y)=x^2+y^2$, so $g'(y)=\frac{x^2}{2}+y^2$. But $g(y)$ is a function of $y$,  a contradiction. On the other hand, if you take any circle $C(t)=(R\cos t,R\sin t)$, $t\in[0,2\pi]$ then  $$ \oint_{C}f\cdot d\vec{l}=\int_{0}^{2\pi}(R^2\cos t\sin t,R^2)\cdot(-R\sin t,R\cos t)\,dt=R^3\int_{0}^{2\pi}\cos^3t\,dt=0 $$  Why this does not contradict the first claim? Thanks",,['multivariable-calculus']
