,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"A closed form of $\int_0^1\log (- \log x)\log \left(\frac{1+x}{1-x}\right)\,dx$",A closed form of,"\int_0^1\log (- \log x)\log \left(\frac{1+x}{1-x}\right)\,dx","Is it possible to obtain a closed form of the following integral? $$\int_0^1\log (- \log x)\log \left(\frac{1+x}{1-x}\right)\,dx$$ I've made the change of variable $$t=\frac{1+x}{1-x} $$ but I feel like I'm turning in circles...","Is it possible to obtain a closed form of the following integral? $$\int_0^1\log (- \log x)\log \left(\frac{1+x}{1-x}\right)\,dx$$ I've made the change of variable $$t=\frac{1+x}{1-x} $$ but I feel like I'm turning in circles...",,"['calculus', 'integration', 'definite-integrals']"
1,Derivation of Gradshteyn and Ryzhik integral 3.876.1 (in question),Derivation of Gradshteyn and Ryzhik integral 3.876.1 (in question),,"In the Gradshteyn and Ryzhik Table of Integrals, the following integral appears (3.876.1, page 486 in the 8th edition): \begin{equation} \int_0^{\infty} \frac{\sin (p \sqrt{x^2 + a^2})}{\sqrt{x^2 + a^2}} \cos (bx) dx = \begin{cases} \frac{\pi}{2} J_0 \left( a \sqrt{p^2 - b^2} \right) & 0 < b < p \\ 0 & b > p > 0 \end{cases} \end{equation} for $a > 0$, where $J_0$ is the Bessel function. I am interested how this result could be derived (not necessarily rigorously proved), especially the range of $p$ such that the value is $0$. Thank you.","In the Gradshteyn and Ryzhik Table of Integrals, the following integral appears (3.876.1, page 486 in the 8th edition): \begin{equation} \int_0^{\infty} \frac{\sin (p \sqrt{x^2 + a^2})}{\sqrt{x^2 + a^2}} \cos (bx) dx = \begin{cases} \frac{\pi}{2} J_0 \left( a \sqrt{p^2 - b^2} \right) & 0 < b < p \\ 0 & b > p > 0 \end{cases} \end{equation} for $a > 0$, where $J_0$ is the Bessel function. I am interested how this result could be derived (not necessarily rigorously proved), especially the range of $p$ such that the value is $0$. Thank you.",,"['calculus', 'definite-integrals', 'bessel-functions', 'integration']"
2,What exactly is the difference between Gateaux derivative and directional derivative?,What exactly is the difference between Gateaux derivative and directional derivative?,,"The definition of the limit looks very similar between the two derivatives. It seems that directional derivative is the ""amount"" of the function going in the direction of a vector (arrow), whereas the Gateaux derivative is the ""amount"" of a function going in the direction of another function - can someone verify if I have this correctly? Is there anything beyond this? In addition can someone explain why it is necessary to state that the Gateaux derivative is defined on Banach spaces? Let's say I have a set of function that does not form a Banach space, intuitively why would that cause difficulty to define a derivative? And when do I use Gateaux derivative when I am told to calculate the derivative of a function?","The definition of the limit looks very similar between the two derivatives. It seems that directional derivative is the ""amount"" of the function going in the direction of a vector (arrow), whereas the Gateaux derivative is the ""amount"" of a function going in the direction of another function - can someone verify if I have this correctly? Is there anything beyond this? In addition can someone explain why it is necessary to state that the Gateaux derivative is defined on Banach spaces? Let's say I have a set of function that does not form a Banach space, intuitively why would that cause difficulty to define a derivative? And when do I use Gateaux derivative when I am told to calculate the derivative of a function?",,"['calculus', 'limits', 'derivatives', 'gateaux-derivative']"
3,Exchanging $\lim$ and $\inf$?,Exchanging  and ?,\lim \inf,"Suppose we have a sequence of functions $f_n(x)$ that converge to a limiting function $f(x) = \lim_{n \to \infty} f_n(x)$ for $\forall x \in [a,b]$. I was wondering under what conditions the following holds? \begin{equation*} \lim_{n \to \infty} \inf_{x \in [a,b]} f_n(x) = \inf_{x \in [a,b]} f(x). \end{equation*}","Suppose we have a sequence of functions $f_n(x)$ that converge to a limiting function $f(x) = \lim_{n \to \infty} f_n(x)$ for $\forall x \in [a,b]$. I was wondering under what conditions the following holds? \begin{equation*} \lim_{n \to \infty} \inf_{x \in [a,b]} f_n(x) = \inf_{x \in [a,b]} f(x). \end{equation*}",,"['calculus', 'limits', 'convergence-divergence', 'optimization']"
4,Solve the integral $\int_0^\infty x/(x^3+1) dx$,Solve the integral,\int_0^\infty x/(x^3+1) dx,"I'm new here! The problem: integrate from zero to infinity x over the quantity x cubed plus one dx. I checked on wolfram alpha and the answer is that the indefinite integral is this: $$\int \frac{x}{1+x^3} dx = \frac{1}{6}\left(\log(x^2-x+1)-2 \log(x+1)+2 \sqrt{3} \arctan((2 x-1)/\sqrt{3})\right)+\text{constant}$$ and the definite integral is this: $$\int_0^\infty \frac{x}{1+x^3} dx = \frac{2 \pi}{3 \sqrt{3}}\approx 1.2092$$ I am trying to figure out all the steps in between. I see that there are logs, which are equivalent to the ln variant that i am more familiar with, which means it was integrating $1/x$ at some point; I also see an inverse tangent in there. I started with long division to simplify and I got (which could be wrong because I am very tired right now) $x/(x^3+1) = x^2+ 1/(x^3+1)$ which seems to be a step in the right direction. Wolfram Alpha thinks I definitely did that step wrong. The two equations do not evaluate as equal. Then I cheated and took the wolfram alpha factorization of $(x^3+1) = (x+1)(x^2-x+1)$...I probably should have known that but didn't offhand. Now it is looking like $x^2$ plus the partial fraction decomposition $1/(x^3+1) = A/(x+1)+(Bx+C)/(x^2-x+1)$. Am I heading in the right direction with this? At this point do I just plug in and crunch?","I'm new here! The problem: integrate from zero to infinity x over the quantity x cubed plus one dx. I checked on wolfram alpha and the answer is that the indefinite integral is this: $$\int \frac{x}{1+x^3} dx = \frac{1}{6}\left(\log(x^2-x+1)-2 \log(x+1)+2 \sqrt{3} \arctan((2 x-1)/\sqrt{3})\right)+\text{constant}$$ and the definite integral is this: $$\int_0^\infty \frac{x}{1+x^3} dx = \frac{2 \pi}{3 \sqrt{3}}\approx 1.2092$$ I am trying to figure out all the steps in between. I see that there are logs, which are equivalent to the ln variant that i am more familiar with, which means it was integrating $1/x$ at some point; I also see an inverse tangent in there. I started with long division to simplify and I got (which could be wrong because I am very tired right now) $x/(x^3+1) = x^2+ 1/(x^3+1)$ which seems to be a step in the right direction. Wolfram Alpha thinks I definitely did that step wrong. The two equations do not evaluate as equal. Then I cheated and took the wolfram alpha factorization of $(x^3+1) = (x+1)(x^2-x+1)$...I probably should have known that but didn't offhand. Now it is looking like $x^2$ plus the partial fraction decomposition $1/(x^3+1) = A/(x+1)+(Bx+C)/(x^2-x+1)$. Am I heading in the right direction with this? At this point do I just plug in and crunch?",,['calculus']
5,Function which derivative at $0$ is $1$ but is not monotonic increasing,Function which derivative at  is  but is not monotonic increasing,0 1,"Please, I need help in order to understand the following assumption that I've found in Bartle's book Introduction to Real Analysis page 171. It says: One might suppose that, if the derivative is strictly positive at a point, then the function is increasing at this point. However, this supposition is false; indeed, the differentiable function defined by \begin{equation}   g(x)=\begin{cases}     x+2x^2\sin(\frac{1}{x}), & \text{if $x\neq 0$},\\     0, & \text{$x=0$}.   \end{cases} \end{equation} Ok. So I've got that $g$ is continuous at zero and  $$g'(0)=1, g'\left(\frac{1}{2n\pi}\right)=-1<0, \text{ and } g'\left(\frac{1}{(2n+1)\pi}\right)=3>0.$$ So this function has positive derivative at zero but $g$ is not increasing in any neighborhood of $x = 0$? What am I doing wrong here? So far what I know is that if a function has positive derivative at a point we can find a neighborhood of that point where the function is increasing. Thanks for any help on this.","Please, I need help in order to understand the following assumption that I've found in Bartle's book Introduction to Real Analysis page 171. It says: One might suppose that, if the derivative is strictly positive at a point, then the function is increasing at this point. However, this supposition is false; indeed, the differentiable function defined by \begin{equation}   g(x)=\begin{cases}     x+2x^2\sin(\frac{1}{x}), & \text{if $x\neq 0$},\\     0, & \text{$x=0$}.   \end{cases} \end{equation} Ok. So I've got that $g$ is continuous at zero and  $$g'(0)=1, g'\left(\frac{1}{2n\pi}\right)=-1<0, \text{ and } g'\left(\frac{1}{(2n+1)\pi}\right)=3>0.$$ So this function has positive derivative at zero but $g$ is not increasing in any neighborhood of $x = 0$? What am I doing wrong here? So far what I know is that if a function has positive derivative at a point we can find a neighborhood of that point where the function is increasing. Thanks for any help on this.",,"['calculus', 'functions', 'derivatives']"
6,Convergence of composition of functions sequences,Convergence of composition of functions sequences,,"Let $X$ be a metric space, $f_n: X \to X$, $g_n: X \to X$, $f_n(x) \to f(x)$, $g_n(x) \to g(x)$  ($n \to \infty$). Is $f_n(g_n(x)) \to f(g(x))$ ? Here: 1) pointwise convergence; 2) uniform convergence. So, there are 2 cases in my question. In the first case $f_n \to f$ and $g_n \to g$ pointwise. In the second case $f_n \to f$ and $g_n \to g$ uniformly. Thank you very much!","Let $X$ be a metric space, $f_n: X \to X$, $g_n: X \to X$, $f_n(x) \to f(x)$, $g_n(x) \to g(x)$  ($n \to \infty$). Is $f_n(g_n(x)) \to f(g(x))$ ? Here: 1) pointwise convergence; 2) uniform convergence. So, there are 2 cases in my question. In the first case $f_n \to f$ and $g_n \to g$ pointwise. In the second case $f_n \to f$ and $g_n \to g$ uniformly. Thank you very much!",,['calculus']
7,"Showing that the function given by $f(x,y)=\frac{xy}{\sqrt{x^2+y^2}}$ and $f(0,0)=0$ is continuous but not differentiable",Showing that the function given by  and  is continuous but not differentiable,"f(x,y)=\frac{xy}{\sqrt{x^2+y^2}} f(0,0)=0","Let $$  f(x,y) = \begin{cases} \dfrac{xy}{\sqrt{x^2+y^2}}   & \text{if $(x,y)\neq(0,0)$ } \\[2ex] 0 & \text{if $(x,y)=(0,0)$ }  \\ \end{cases} $$ Show that this function is continuous but not differentiable at $(0,0),$ although it has both partial derivatives existing there. I can show this function is continous and the partial derivatives exist. But how can I show that this function is not differentiable ? Is showing that the function is differentiable similar to showing that a derivative exists?",Let Show that this function is continuous but not differentiable at although it has both partial derivatives existing there. I can show this function is continous and the partial derivatives exist. But how can I show that this function is not differentiable ? Is showing that the function is differentiable similar to showing that a derivative exists?,"
 f(x,y) =
\begin{cases}
\dfrac{xy}{\sqrt{x^2+y^2}}   & \text{if (x,y)\neq(0,0) } \\[2ex]
0 & \text{if (x,y)=(0,0) }  \\
\end{cases}
 (0,0),","['calculus', 'multivariable-calculus', 'derivatives', 'continuity', 'scalar-fields']"
8,"Solving for upper limit of integration, given a lower limit and integrand","Solving for upper limit of integration, given a lower limit and integrand",,"Consider $f$ a real integrable function, usually we want to evaluate the integral $\int_a^bf(x)dx$ for some $a<b$ given. Now, suppose we know $a$ but we don't know $b$, further, we know the value of this integral, let $\int_a^bf(x)dx=\lambda$. My question is, in what conditions we can find $b$, and how? A simple case is when $f$ has a known primitive $F$ and $F$ has inverse $F^{-1}$. $$ \int_a^bf(x)dx=\lambda\implies F(b)-F(a)=\lambda\implies F(b)=F(a)+\lambda\implies b=F^{-1}(F(a)+\lambda)$$ In this case we can evaluate $b$, but if this is not the case, what can be done? Thanks. PS: More approaches are welcome, approaches not relying on $f$ primitive.","Consider $f$ a real integrable function, usually we want to evaluate the integral $\int_a^bf(x)dx$ for some $a<b$ given. Now, suppose we know $a$ but we don't know $b$, further, we know the value of this integral, let $\int_a^bf(x)dx=\lambda$. My question is, in what conditions we can find $b$, and how? A simple case is when $f$ has a known primitive $F$ and $F$ has inverse $F^{-1}$. $$ \int_a^bf(x)dx=\lambda\implies F(b)-F(a)=\lambda\implies F(b)=F(a)+\lambda\implies b=F^{-1}(F(a)+\lambda)$$ In this case we can evaluate $b$, but if this is not the case, what can be done? Thanks. PS: More approaches are welcome, approaches not relying on $f$ primitive.",,"['calculus', 'integration', 'definite-integrals']"
9,Find $\lim_{n\to\infty}2^n\underbrace{\sqrt{2-\sqrt{2+\sqrt{2+\dots+\sqrt2}}}}_{n \textrm{ square roots}}$.,Find .,\lim_{n\to\infty}2^n\underbrace{\sqrt{2-\sqrt{2+\sqrt{2+\dots+\sqrt2}}}}_{n \textrm{ square roots}},"Find $\displaystyle \lim_{n\to\infty}2^n\underbrace{\sqrt{2-\sqrt{2+\sqrt{2+\dots+\sqrt2}}}}_{n \textrm{ square roots}}$. By geometry method, I know that this is $\pi$. But is there algebraic method to find this ? Thank you.","Find $\displaystyle \lim_{n\to\infty}2^n\underbrace{\sqrt{2-\sqrt{2+\sqrt{2+\dots+\sqrt2}}}}_{n \textrm{ square roots}}$. By geometry method, I know that this is $\pi$. But is there algebraic method to find this ? Thank you.",,"['calculus', 'limits']"
10,How do I get good at Math? [closed],How do I get good at Math? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 8 months ago . Improve this question How do I get good at math? I'm a freshman in college, and I've always done OK in math. I never had any good teachers in High school, and I always have done the bare minimum. Over the course of this year, I really came to understand how important math is and how much potential I have to enjoy it. I'm taking a discrete 2 course and I love it. (I'm a Computer Science major) Even though I passed Calculus 3, I don't feel I understand Calculus. I have an excellent memory and was able to memorize formulas and apply them when I was supposed to. I don't understand anything though. I came to the realization that I should put some effort into getting better at math. Over spring break, I went through almost all of Khan Academy's exercises just to make sure that I have a good grasp on the very basics. I'm going to finish them up in a couple of days. what should I do next? My plans are to start at the beginning of my Calculus book (Thomson's Early Transcendental) and work my way through the book (one part a day), reading everything and doing every problem.  I don't know how good of a book this is though. I've done some googling and a lot of people recommend Spivak's and Apostle's Calculus books. Is it worth purchasing one of these (or something else) if I really want to comprehend the material? Are they good for self-teaching? I usually have some sort of project like this going on (except never math based), and I'm completely willing to put time into this goal. Some direction would be nice though.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 8 months ago . Improve this question How do I get good at math? I'm a freshman in college, and I've always done OK in math. I never had any good teachers in High school, and I always have done the bare minimum. Over the course of this year, I really came to understand how important math is and how much potential I have to enjoy it. I'm taking a discrete 2 course and I love it. (I'm a Computer Science major) Even though I passed Calculus 3, I don't feel I understand Calculus. I have an excellent memory and was able to memorize formulas and apply them when I was supposed to. I don't understand anything though. I came to the realization that I should put some effort into getting better at math. Over spring break, I went through almost all of Khan Academy's exercises just to make sure that I have a good grasp on the very basics. I'm going to finish them up in a couple of days. what should I do next? My plans are to start at the beginning of my Calculus book (Thomson's Early Transcendental) and work my way through the book (one part a day), reading everything and doing every problem.  I don't know how good of a book this is though. I've done some googling and a lot of people recommend Spivak's and Apostle's Calculus books. Is it worth purchasing one of these (or something else) if I really want to comprehend the material? Are they good for self-teaching? I usually have some sort of project like this going on (except never math based), and I'm completely willing to put time into this goal. Some direction would be nice though.",,"['calculus', 'soft-question', 'learning']"
11,Integral and derivative of $\lfloor x\rfloor$ and $x - \lfloor x\rfloor$,Integral and derivative of  and,\lfloor x\rfloor x - \lfloor x\rfloor,"I've always assumed by graphical inspection that $\int (x - \lfloor x\rfloor)\mathrm dx = \dfrac{(x - \lfloor x\rfloor)^2 + \lfloor x\rfloor}{2}$ ( W|A ) and $\int \lfloor x\rfloor\mathrm dx = x\lfloor x\rfloor - \dfrac{\lfloor x\rfloor(\lfloor x\rfloor + 1)}{2}$ ( W|A ) Why does Wolfram|Alpha say for each integral, "" no result found in terms of standard mathematical functions ""? I also assumed that $\frac{\mathrm d}{\mathrm dx} \lfloor x \rfloor = 0$, yet according to Wolfram|Alpha $\frac{\mathrm d}{\mathrm dx} \lfloor x \rfloor = \mathop {\rm floor}'(x)$—which is not not explained, but the graph looks very strange .  What is going on here?","I've always assumed by graphical inspection that $\int (x - \lfloor x\rfloor)\mathrm dx = \dfrac{(x - \lfloor x\rfloor)^2 + \lfloor x\rfloor}{2}$ ( W|A ) and $\int \lfloor x\rfloor\mathrm dx = x\lfloor x\rfloor - \dfrac{\lfloor x\rfloor(\lfloor x\rfloor + 1)}{2}$ ( W|A ) Why does Wolfram|Alpha say for each integral, "" no result found in terms of standard mathematical functions ""? I also assumed that $\frac{\mathrm d}{\mathrm dx} \lfloor x \rfloor = 0$, yet according to Wolfram|Alpha $\frac{\mathrm d}{\mathrm dx} \lfloor x \rfloor = \mathop {\rm floor}'(x)$—which is not not explained, but the graph looks very strange .  What is going on here?",,"['calculus', 'integration', 'derivatives', 'ceiling-and-floor-functions']"
12,Another Sophomore's Dream: $\int_{-\infty}^{\infty}\binom{n}{x}dx=\sum_{i=0}^{n} \binom{n}{i}$,Another Sophomore's Dream:,\int_{-\infty}^{\infty}\binom{n}{x}dx=\sum_{i=0}^{n} \binom{n}{i},"I found an identity $$\int_{-\infty}^{\infty}\binom{n}{x}dx=\sum_{i=0}^{n} \binom{n}{i}$$ where LHS can be calculated by the Reflection relation and Dirichlet integral . The result is $2^n$ , which is apparently equal to RHS. This identity is in the form ""integration=summation"", which is similar to the ""Sophomore's dream"" , $\int_0^1 x^{-x}dx = \sum_{n=1}^\infty n^{-n}$ . Is this another coincidence? If not, what is the reason behind it to make it true.","I found an identity where LHS can be calculated by the Reflection relation and Dirichlet integral . The result is , which is apparently equal to RHS. This identity is in the form ""integration=summation"", which is similar to the ""Sophomore's dream"" , . Is this another coincidence? If not, what is the reason behind it to make it true.",\int_{-\infty}^{\infty}\binom{n}{x}dx=\sum_{i=0}^{n} \binom{n}{i} 2^n \int_0^1 x^{-x}dx = \sum_{n=1}^\infty n^{-n},"['calculus', 'integration', 'binomial-coefficients', 'binomial-theorem']"
13,When would we want to use uneven subintervals in a Riemann integral?,When would we want to use uneven subintervals in a Riemann integral?,,"The formal definition of a Riemann Integral is written such that you can have uneven subintervals and it still works.  Why do we need to generalize to the case of uneven subintervals?  Why not insist our subintervals are always of equal length, and $\Delta x$ is the same for all of them? Here's the full definition from Wiki: The Riemann Integral of $f$ equals $s$ if: For all $\epsilon > 0$ , there exists a $\delta > 0$ such that for any tagged partition $x_0 \dots x_n$ and $t_0 \dots t_{n-1}$ whose mesh is less than $\delta$ we have $$\left|  \left(\sum_{i=0}^{n} f(t_i) (x_{i+1}-x_{i})\right) - s \right| < \epsilon$$ Notice that they have to reference the ""mesh"" in this definition, i.e. the length of the longest subinterval.  Wouldn't we get a simpler definition by just requiring equal subintervals? E.g [my version] The Riemann Integral of $f$ equals $s$ if: For all $\epsilon > 0$ , there exists a $\delta > 0$ such that for any tagged partition $x_0 \dots x_n$ and $t_0 \dots t_{n-1}$ [with equal subintervals] less than $\delta$ we have $$\left|  \left(\sum_{i=0}^{n} f(x_i)\Delta x_i \right) - s \right| < \epsilon$$ Or is there a use for having different subinterval lengths?","The formal definition of a Riemann Integral is written such that you can have uneven subintervals and it still works.  Why do we need to generalize to the case of uneven subintervals?  Why not insist our subintervals are always of equal length, and is the same for all of them? Here's the full definition from Wiki: The Riemann Integral of equals if: For all , there exists a such that for any tagged partition and whose mesh is less than we have Notice that they have to reference the ""mesh"" in this definition, i.e. the length of the longest subinterval.  Wouldn't we get a simpler definition by just requiring equal subintervals? E.g [my version] The Riemann Integral of equals if: For all , there exists a such that for any tagged partition and [with equal subintervals] less than we have Or is there a use for having different subinterval lengths?",\Delta x f s \epsilon > 0 \delta > 0 x_0 \dots x_n t_0 \dots t_{n-1} \delta \left|  \left(\sum_{i=0}^{n} f(t_i) (x_{i+1}-x_{i})\right) - s \right| < \epsilon f s \epsilon > 0 \delta > 0 x_0 \dots x_n t_0 \dots t_{n-1} \delta \left|  \left(\sum_{i=0}^{n} f(x_i)\Delta x_i \right) - s \right| < \epsilon,"['calculus', 'riemann-integration', 'riemann-sum', 'partitions-for-integration']"
14,Prove $f(x)$ is constant.,Prove  is constant.,f(x),"Suppose $ f: \mathbb{R} \mapsto \mathbb{R}$ be continous. For any interval $[a,b]$ , there exists $x_0\in (a,b)$ such that either $f(x_0)=\max\limits_{a\le x \le b} f(x)$ or $f(x_0)=\min\limits_{a\le x \le b} f(x)$ . Prove $f(x)$ is constant. We may consider apply Reductio ad Absurdum . Suppose $f(x)$ is not constant, then there exist $\alpha<\beta$ such that $f(\alpha)\neq f(\beta).$ But how to reduce the contradiction?","Suppose be continous. For any interval , there exists such that either or . Prove is constant. We may consider apply Reductio ad Absurdum . Suppose is not constant, then there exist such that But how to reduce the contradiction?"," f: \mathbb{R} \mapsto \mathbb{R} [a,b] x_0\in (a,b) f(x_0)=\max\limits_{a\le x \le b} f(x) f(x_0)=\min\limits_{a\le x \le b} f(x) f(x) f(x) \alpha<\beta f(\alpha)\neq f(\beta).","['calculus', 'analysis', 'continuity']"
15,Solution verification:$\lim_{x\to 2}\frac{\ln(x-1)}{3^{x-2}-5^{-x+2}}$,Solution verification:,\lim_{x\to 2}\frac{\ln(x-1)}{3^{x-2}-5^{-x+2}},Evaluate without L'Hospital: $$\lim_{x\to  2}\frac{\ln(x-1)}{3^{x-2}-5^{-x+2}}$$ My attempt: I used: $$\lim_{f(x)\to 0}\frac{\ln(1+f(x))}{f(x)}=1\;\&\;\lim_{f(x)\to 0}\frac{a^{f(x)}-1}{f(x)}=\ln a$$ $$ \begin{split} L &= \lim_{x\to 2} \frac{\ln(x-1)}{3^{x-2}-5^{-x+2}} \\   &= \lim_{x\to 2} \frac{\dfrac{\ln(1+(x-2))}{x-2}\cdot(x-2)}                         {(x-2)\cdot\dfrac{3^{x-2}-1+1-5^{-x+2}}{x-2}} \\   &= \lim_{x\to 2} \frac{\dfrac{\ln(1+(x-2))}{x-2}}                         {\dfrac{3^{x-2}-1}{x-2}+\dfrac{5^{2-x}-1}{2-x}} \\   &=\frac{1}{\ln3+\ln5} \\   &=\frac{1}{\ln(15)} \end{split} $$ Is this correct?,Evaluate without L'Hospital: My attempt: I used: Is this correct?,"\lim_{x\to
 2}\frac{\ln(x-1)}{3^{x-2}-5^{-x+2}} \lim_{f(x)\to 0}\frac{\ln(1+f(x))}{f(x)}=1\;\&\;\lim_{f(x)\to 0}\frac{a^{f(x)}-1}{f(x)}=\ln a 
\begin{split}
L &= \lim_{x\to 2} \frac{\ln(x-1)}{3^{x-2}-5^{-x+2}} \\
  &= \lim_{x\to 2} \frac{\dfrac{\ln(1+(x-2))}{x-2}\cdot(x-2)}
                        {(x-2)\cdot\dfrac{3^{x-2}-1+1-5^{-x+2}}{x-2}} \\
  &= \lim_{x\to 2} \frac{\dfrac{\ln(1+(x-2))}{x-2}}
                        {\dfrac{3^{x-2}-1}{x-2}+\dfrac{5^{2-x}-1}{2-x}} \\
  &=\frac{1}{\ln3+\ln5} \\
  &=\frac{1}{\ln(15)}
\end{split}
","['calculus', 'limits-without-lhopital', 'solution-verification']"
16,Find $\lim_{n \to \infty}\frac1{\ln^2n}\left( \frac{\ln 2}{2} + \frac{\ln 3}{3} +\cdots + \frac{\ln n}{n}\right)$,Find,\lim_{n \to \infty}\frac1{\ln^2n}\left( \frac{\ln 2}{2} + \frac{\ln 3}{3} +\cdots + \frac{\ln n}{n}\right),"I have to find the following limit: $$\lim_{n \to \infty} \frac{\frac{\ln 2}{2} + \frac{\ln 3}{3} + \cdots + \frac{\ln n}{n}}{\ln^2n}$$ I tried splitting this limit like so: $$\lim\limits_{n \to \infty} \dfrac{\frac{\ln 2}{2} + \frac{\ln 3}{3} + \cdots + \frac{\ln n}{n}}{\ln n} \cdot \dfrac{1}{\ln n}$$ Because $\dfrac{1}{\ln n} \rightarrow 0$ as $n \rightarrow \infty$ , I concluded that the limit is $0$ . I know that in order to use this I firstly would have to show that $$\lim\limits_{n \to \infty} \dfrac{\frac{\ln 2}{2} + \frac{\ln 3}{3} + \cdots +\frac{\ln n}{n}}{\ln n}$$ is bounded, but I didn't know how to do that and kinda hoped for the best. It turns out that my hopes were in vain, since the limit is actually $\dfrac{1}{2}$ and not $0$ , like I got. I also tried using Stolz-Cesaro, resulting in: $$\lim\limits_{n \to \infty} \dfrac{\frac{\ln (n+1)}{n+1}}{\ln^2 (n + 1) - \ln^2 n} = \lim\limits_{n \to \infty} \dfrac{\frac{\ln (n+1)}{n + 1}}{(\ln (n+1)-\ln n)(\ln (n + 1) + \ln n)}$$ $$= \lim\limits_{n \to \infty} \dfrac{\frac{\ln (n + 1)}{n + 1}}{\ln (\frac{n + 1}{n}) \cdot \ln(n(n + 1))}$$ Aaand I got stuck. So how should I approach this and get $\dfrac{1}{2}$ as the final answer?","I have to find the following limit: I tried splitting this limit like so: Because as , I concluded that the limit is . I know that in order to use this I firstly would have to show that is bounded, but I didn't know how to do that and kinda hoped for the best. It turns out that my hopes were in vain, since the limit is actually and not , like I got. I also tried using Stolz-Cesaro, resulting in: Aaand I got stuck. So how should I approach this and get as the final answer?",\lim_{n \to \infty} \frac{\frac{\ln 2}{2} + \frac{\ln 3}{3} + \cdots + \frac{\ln n}{n}}{\ln^2n} \lim\limits_{n \to \infty} \dfrac{\frac{\ln 2}{2} + \frac{\ln 3}{3} + \cdots + \frac{\ln n}{n}}{\ln n} \cdot \dfrac{1}{\ln n} \dfrac{1}{\ln n} \rightarrow 0 n \rightarrow \infty 0 \lim\limits_{n \to \infty} \dfrac{\frac{\ln 2}{2} + \frac{\ln 3}{3} + \cdots +\frac{\ln n}{n}}{\ln n} \dfrac{1}{2} 0 \lim\limits_{n \to \infty} \dfrac{\frac{\ln (n+1)}{n+1}}{\ln^2 (n + 1) - \ln^2 n} = \lim\limits_{n \to \infty} \dfrac{\frac{\ln (n+1)}{n + 1}}{(\ln (n+1)-\ln n)(\ln (n + 1) + \ln n)} = \lim\limits_{n \to \infty} \dfrac{\frac{\ln (n + 1)}{n + 1}}{\ln (\frac{n + 1}{n}) \cdot \ln(n(n + 1))} \dfrac{1}{2},['calculus']
17,"Evaluating $\int_0^{\pi/2} \log \left| \sin^2 x - a \right|$ where $a\in [0,1]$.",Evaluating  where .,"\int_0^{\pi/2} \log \left| \sin^2 x - a \right| a\in [0,1]","How to evaluate $$ \displaystyle\int_0^{\pi/2} \log \left| \sin^2 x - a \right|\,dx $$ where $a\in[0,1]$ ? I think of this problem as a generalization of the following proposition $$ \displaystyle\int_0^{\pi/2} \log  \left(\sin x\right)\,dx =-\frac12\pi\log2 $$ My try Put $$ I(a)=\displaystyle\int_0^{\pi/2} \log \left| \sin^2 x - a \right|\,dx $$ From the substitution $x \to \frac{\pi}{2}-x$ , we get $$ \displaystyle\int_0^{\pi/2} \log \left| \sin^2 x - a \right|\,dx = \displaystyle\int_0^{\pi/2} \log \left| \cos^2 x - a \right|\,dx $$ Thus $$ \displaystyle\int_0^{\pi/2} \log \left| \sin^2 x - a \right|\,dx = \displaystyle\int_0^{\pi/2} \log \left| \sin^2 x - (1-a) \right|\,dx $$ which means $$I(a)=I(1-a) \tag{1}$$ On the other hand, \begin{align} 2I(a) &= \displaystyle\int_0^{\pi/2} \log \left| (\sin^2 x - a)(\cos^2 x -a) \right|\,dx \\ &= \displaystyle\int_0^{\pi/2} \log \left| a^2-a+\sin^2 x \cos^2 x \right|\,dx \\ &= \displaystyle\int_0^{\pi/2} \log \left| 4(a^2-a)+\sin^2 (2x) \right|\,dx -\pi \log 2 \\ &= \frac{1}{2}\displaystyle\int_0^{\pi} \log \left| 4(a^2-a)+\sin^2 x \right|\,dx -\pi \log 2 \\ &= \displaystyle\int_0^{\pi/2} \log \left| 4(a^2-a)+\sin^2 x \right|\,dx -\pi \log 2 \\ &= \displaystyle\int_0^{\pi/2} \log \left| 1+4(a^2-a)-\sin^2 x \right|\,dx -\pi \log 2 \\ &= I((2a-1)^2) -\pi \log 2 \end{align} Thus $$ 2I(a)=I((2a-1)^2)-\pi \log 2 \tag{2} $$ Let $a=0$ we get the proposition mentioned above $\displaystyle\int_0^{\pi/2} \log  \left(\sin x\right)\,dx =-\frac12\pi\log2.$ But how to move on ? Can we solve the problem only by $(1)$ and $(2)$ ? Or what other properties should we use to evaluate that? Looking forward to your new solutions as well. Thank you in advance! Added: As pointed out in the comments, it seems like that the integral is identical to $-\pi\log 2$ . From $(1)$ and $(2)$ we can also find many numbers such that $I(a)=-\pi\log 2$ .","How to evaluate where ? I think of this problem as a generalization of the following proposition My try Put From the substitution , we get Thus which means On the other hand, Thus Let we get the proposition mentioned above But how to move on ? Can we solve the problem only by and ? Or what other properties should we use to evaluate that? Looking forward to your new solutions as well. Thank you in advance! Added: As pointed out in the comments, it seems like that the integral is identical to . From and we can also find many numbers such that .","
\displaystyle\int_0^{\pi/2} \log \left| \sin^2 x - a \right|\,dx
 a\in[0,1] 
\displaystyle\int_0^{\pi/2} \log  \left(\sin x\right)\,dx =-\frac12\pi\log2
 
I(a)=\displaystyle\int_0^{\pi/2} \log \left| \sin^2 x - a \right|\,dx
 x \to \frac{\pi}{2}-x 
\displaystyle\int_0^{\pi/2} \log \left| \sin^2 x - a \right|\,dx
=
\displaystyle\int_0^{\pi/2} \log \left| \cos^2 x - a \right|\,dx
 
\displaystyle\int_0^{\pi/2} \log \left| \sin^2 x - a \right|\,dx
=
\displaystyle\int_0^{\pi/2} \log \left| \sin^2 x - (1-a) \right|\,dx
 I(a)=I(1-a) \tag{1} \begin{align}
2I(a) &=
\displaystyle\int_0^{\pi/2} \log \left| (\sin^2 x - a)(\cos^2 x -a) \right|\,dx
\\ &=
\displaystyle\int_0^{\pi/2} \log \left| a^2-a+\sin^2 x \cos^2 x \right|\,dx
\\ &=
\displaystyle\int_0^{\pi/2} \log \left| 4(a^2-a)+\sin^2 (2x) \right|\,dx
-\pi \log 2
\\ &=
\frac{1}{2}\displaystyle\int_0^{\pi} \log \left| 4(a^2-a)+\sin^2 x \right|\,dx
-\pi \log 2
\\ &=
\displaystyle\int_0^{\pi/2} \log \left| 4(a^2-a)+\sin^2 x \right|\,dx
-\pi \log 2
\\ &=
\displaystyle\int_0^{\pi/2} \log \left| 1+4(a^2-a)-\sin^2 x \right|\,dx
-\pi \log 2
\\ &=
I((2a-1)^2)
-\pi \log 2
\end{align} 
2I(a)=I((2a-1)^2)-\pi \log 2 \tag{2}
 a=0 \displaystyle\int_0^{\pi/2} \log  \left(\sin x\right)\,dx =-\frac12\pi\log2. (1) (2) -\pi\log 2 (1) (2) I(a)=-\pi\log 2","['calculus', 'integration', 'trigonometry', 'definite-integrals', 'logarithms']"
18,How to solve a second order partial differential equation involving a delta Dirac function?,How to solve a second order partial differential equation involving a delta Dirac function?,,"In a mathematical physical problem, I came across the following partial differential equation involving a delta Dirac function: $$ a \, \frac{\partial^2 w}{\partial x^2} + b \, \frac{\partial^2 w}{\partial y^2} + \delta^2(x,y) = 0 \, ,  $$ subject to the boundary conditions $w(x = \pm 1, y) = w(x, y = \pm 1) = 0$ . Here $a, b \in \mathbb{R}_+$ and $\delta^2(x,y) = \delta(x)\delta(y)$ is the two-dimensional delta Dirac function. While solutions for ODEs with delta Dirac functions can readily be obtained using the standard approach, I am not aware of any resolution recipe for PDEs with delta Dirac functions. Any help or hint is highly desirable and appreciated. Thank you","In a mathematical physical problem, I came across the following partial differential equation involving a delta Dirac function: subject to the boundary conditions . Here and is the two-dimensional delta Dirac function. While solutions for ODEs with delta Dirac functions can readily be obtained using the standard approach, I am not aware of any resolution recipe for PDEs with delta Dirac functions. Any help or hint is highly desirable and appreciated. Thank you","
a \, \frac{\partial^2 w}{\partial x^2}
+ b \, \frac{\partial^2 w}{\partial y^2}
+ \delta^2(x,y) = 0 \, , 
 w(x = \pm 1, y) = w(x, y = \pm 1) = 0 a, b \in \mathbb{R}_+ \delta^2(x,y) = \delta(x)\delta(y)","['calculus', 'partial-differential-equations', 'dirac-delta', 'greens-function', 'elliptic-equations']"
19,"On the integrals $\int_0^\infty [1-x^n \operatorname{arccot}^n(x)] \, \mathrm{d} x$",On the integrals,"\int_0^\infty [1-x^n \operatorname{arccot}^n(x)] \, \mathrm{d} x","When answering this question I came across the integrals $$ I_n \equiv \int \limits_0^\infty [1-x^n \operatorname{arccot}^n(x)] \, \mathrm{d} x \, , ~n \in \mathbb{N} \, . $$ I needed $I_1 = \frac{\pi}{4}$ , $I_2 = \frac{\pi}{6}[2 \ln(2)+1]$ and $I_3 = \frac{\pi}{32}[32 \ln(2)+ 4 -\pi^2]$ in my answers. They can be evaluated by writing $$ I_n = \lim_{r \to \infty} \left[r - \int \limits_0^r x^n \operatorname{arccot}^n (x) \, \mathrm{d} x \right] $$ and using repeated integration by parts to reduce the remaining integral to a few terms that cancel the $r$ in the limit and some well-known integrals. This calculation should work for any $n \in \mathbb{N}$, but it gets more tedious for larger values of course. Mathematica gives reasonably nice expressions for the integrals in terms of $\pi$ , $\ln(2)$ and values of the zeta function (for example $I_4 = \frac{\pi}{40} [4 + 80 \ln(2) - \pi^2 (3 + 4 \ln(2)) + 18 \zeta(3)]$), so it might be possible to evaluate the integrals in terms of suitable special functions in general. However, I have not yet found a way to transform them into such an expression. The obvious substitution $x = \cot (t)$ leads to $$ I_n = \int \limits_0^{\pi/2} \frac{\sin^n (t) - t^n \cos^n (t)}{\sin^{n+2}(t)} \, \mathrm{d} t \, ,$$ which does not seem to help much. We can use $$ 1 - a^n = (1-a) \sum \limits_{k=0}^{n-1} a^k = \sum \limits_{k=0}^{n-1} \sum \limits_{l=0}^k (-1)^l {k \choose l} (1-a)^{l+1} $$ for $n \in \mathbb{N}$ and $a \in \mathbb{R}$ to obtain $$ I_n = \sum \limits_{k=0}^{n-1} \sum \limits_{l=0}^k (-1)^l {k \choose l} J_{l+1} $$ in terms of $$ J_n \equiv \int \limits_0^\infty [1-x \operatorname{arccot}(x)]^n \, \mathrm{d} x \, , ~n \in \mathbb{N} \, . $$ Interchanging the sums, using $\sum_{k=l}^{n-1} {k \choose l} = {n \choose l+1}$ (which apparently is known as the hockey-stick identity ) and defining $I_0 = J_0 = 0$ , we find that the two sequences are binomial transforms of each other (except for a minus sign): $$I_n = - \sum \limits_{m=0}^n (-1)^m {n \choose m} J_m \, , \, n \in \mathbb{N}_0 \, . $$ I do not know which of the two families of integrals is easier to evaluate though. Note that the same method enables us to compute $$ \int \limits_0^\infty [1-x \operatorname{arccot}(x)] P[x \operatorname{arccot}(x)] \, \mathrm{d} x  $$ for any polynomial $P$ once we know $(I_n)_{n \in \mathbb{N}}$ or $(J_n)_{n \in \mathbb{N}}$. I am also interested in the asymptotic behaviour of the integrals. Numerical calculations and plots suggest that we have \begin{align} I_n &\sim \sqrt{\frac{\pi n}{3}} \, , \, n \to \infty \, , \\  J_n &\sim \frac{2}{\pi n} \, , \, n \to \infty \, , \end{align} but I have no idea how to prove that. Therefore I am left with the following two questions: How can we find a closed-form expression for $I_n$ or $J_n$ , $n \in \mathbb{N}$ ? What can we say about the asymptotics of $I_n$ or $J_n$ as $n \to \infty$ ? Any hints or (partial) solutions to either of them would be greatly appreciated.","When answering this question I came across the integrals $$ I_n \equiv \int \limits_0^\infty [1-x^n \operatorname{arccot}^n(x)] \, \mathrm{d} x \, , ~n \in \mathbb{N} \, . $$ I needed $I_1 = \frac{\pi}{4}$ , $I_2 = \frac{\pi}{6}[2 \ln(2)+1]$ and $I_3 = \frac{\pi}{32}[32 \ln(2)+ 4 -\pi^2]$ in my answers. They can be evaluated by writing $$ I_n = \lim_{r \to \infty} \left[r - \int \limits_0^r x^n \operatorname{arccot}^n (x) \, \mathrm{d} x \right] $$ and using repeated integration by parts to reduce the remaining integral to a few terms that cancel the $r$ in the limit and some well-known integrals. This calculation should work for any $n \in \mathbb{N}$, but it gets more tedious for larger values of course. Mathematica gives reasonably nice expressions for the integrals in terms of $\pi$ , $\ln(2)$ and values of the zeta function (for example $I_4 = \frac{\pi}{40} [4 + 80 \ln(2) - \pi^2 (3 + 4 \ln(2)) + 18 \zeta(3)]$), so it might be possible to evaluate the integrals in terms of suitable special functions in general. However, I have not yet found a way to transform them into such an expression. The obvious substitution $x = \cot (t)$ leads to $$ I_n = \int \limits_0^{\pi/2} \frac{\sin^n (t) - t^n \cos^n (t)}{\sin^{n+2}(t)} \, \mathrm{d} t \, ,$$ which does not seem to help much. We can use $$ 1 - a^n = (1-a) \sum \limits_{k=0}^{n-1} a^k = \sum \limits_{k=0}^{n-1} \sum \limits_{l=0}^k (-1)^l {k \choose l} (1-a)^{l+1} $$ for $n \in \mathbb{N}$ and $a \in \mathbb{R}$ to obtain $$ I_n = \sum \limits_{k=0}^{n-1} \sum \limits_{l=0}^k (-1)^l {k \choose l} J_{l+1} $$ in terms of $$ J_n \equiv \int \limits_0^\infty [1-x \operatorname{arccot}(x)]^n \, \mathrm{d} x \, , ~n \in \mathbb{N} \, . $$ Interchanging the sums, using $\sum_{k=l}^{n-1} {k \choose l} = {n \choose l+1}$ (which apparently is known as the hockey-stick identity ) and defining $I_0 = J_0 = 0$ , we find that the two sequences are binomial transforms of each other (except for a minus sign): $$I_n = - \sum \limits_{m=0}^n (-1)^m {n \choose m} J_m \, , \, n \in \mathbb{N}_0 \, . $$ I do not know which of the two families of integrals is easier to evaluate though. Note that the same method enables us to compute $$ \int \limits_0^\infty [1-x \operatorname{arccot}(x)] P[x \operatorname{arccot}(x)] \, \mathrm{d} x  $$ for any polynomial $P$ once we know $(I_n)_{n \in \mathbb{N}}$ or $(J_n)_{n \in \mathbb{N}}$. I am also interested in the asymptotic behaviour of the integrals. Numerical calculations and plots suggest that we have \begin{align} I_n &\sim \sqrt{\frac{\pi n}{3}} \, , \, n \to \infty \, , \\  J_n &\sim \frac{2}{\pi n} \, , \, n \to \infty \, , \end{align} but I have no idea how to prove that. Therefore I am left with the following two questions: How can we find a closed-form expression for $I_n$ or $J_n$ , $n \in \mathbb{N}$ ? What can we say about the asymptotics of $I_n$ or $J_n$ as $n \to \infty$ ? Any hints or (partial) solutions to either of them would be greatly appreciated.",,"['calculus', 'integration', 'definite-integrals', 'asymptotics']"
20,Prove $\int _0^\infty f^2 dx \leq \cdots $ for $f$ convex,Prove  for  convex,\int _0^\infty f^2 dx \leq \cdots  f,"Prove $$\int _0^\infty f^2(x) dx \leq \frac{2}{3}\cdot  \max_{x \in \mathbb R^+} f(x) \cdot  \int _0^\infty f(x) dx$$ for $f(x) \geq 0$ and convex. I know via Holder's we can get without the constant $\frac{2}{3}$ but that should also be true for $f$ not convex. I tried some approaches using convexity but didn't get the desired result. A hint or reference would also help. Also, it would be interesting to know when equality holds.","Prove $$\int _0^\infty f^2(x) dx \leq \frac{2}{3}\cdot  \max_{x \in \mathbb R^+} f(x) \cdot  \int _0^\infty f(x) dx$$ for $f(x) \geq 0$ and convex. I know via Holder's we can get without the constant $\frac{2}{3}$ but that should also be true for $f$ not convex. I tried some approaches using convexity but didn't get the desired result. A hint or reference would also help. Also, it would be interesting to know when equality holds.",,"['calculus', 'inequality', 'convex-analysis', 'integral-inequality']"
21,"Integrate $\int \arcsin\left(\frac{x+2}{x^2+4x+13}\right)\,\mathrm dx$",Integrate,"\int \arcsin\left(\frac{x+2}{x^2+4x+13}\right)\,\mathrm dx","$\def\d{\mathrm{d}}$Any hint on how to compute this integration?$$\int \arcsin\left(\frac{x+2}{x^2+4x+13}\right)\,\d x$$ This is what I have done:$$ \int \arcsin\left(\frac{x+2}{x^2+4x+13}\right)\,\d x=\int \arcsin\left(\frac{x+2}{(x+2)^2+9}\right)\,\d x. $$ Substitute $x+2=3 \tan\theta$ and $\d x=3\sec^2\theta\,\d\theta$,\begin{align*} &\mathrel{\phantom{=}} \int \arcsin\left(\frac{x+2}{(x+2)^2+9}\right)\,\d x\\ &=3\int \arcsin\left(\frac{3\tan\theta}{9\tan^2\theta+9}\right)\sec^2\theta\,\d\theta\\ &=3\int \arcsin\left(\frac{3\tan\theta}{9\sec^2\theta}\right)\sec^2\theta\,\d\theta\\ &=3\int \arcsin\left(\frac{\sin\theta\cos\theta}{3}\right)\sec^2\theta\,\d\theta\\ &=3\int \arcsin\left(\frac{\sin2\theta}{6}\right)\sec^2\theta\,\d\theta. \end{align*} What can I do from here?","$\def\d{\mathrm{d}}$Any hint on how to compute this integration?$$\int \arcsin\left(\frac{x+2}{x^2+4x+13}\right)\,\d x$$ This is what I have done:$$ \int \arcsin\left(\frac{x+2}{x^2+4x+13}\right)\,\d x=\int \arcsin\left(\frac{x+2}{(x+2)^2+9}\right)\,\d x. $$ Substitute $x+2=3 \tan\theta$ and $\d x=3\sec^2\theta\,\d\theta$,\begin{align*} &\mathrel{\phantom{=}} \int \arcsin\left(\frac{x+2}{(x+2)^2+9}\right)\,\d x\\ &=3\int \arcsin\left(\frac{3\tan\theta}{9\tan^2\theta+9}\right)\sec^2\theta\,\d\theta\\ &=3\int \arcsin\left(\frac{3\tan\theta}{9\sec^2\theta}\right)\sec^2\theta\,\d\theta\\ &=3\int \arcsin\left(\frac{\sin\theta\cos\theta}{3}\right)\sec^2\theta\,\d\theta\\ &=3\int \arcsin\left(\frac{\sin2\theta}{6}\right)\sec^2\theta\,\d\theta. \end{align*} What can I do from here?",,"['calculus', 'integration', 'indefinite-integrals']"
22,Is there a flaw in the theory of fractional calculus?,Is there a flaw in the theory of fractional calculus?,,"Let's talk about the function $f(x)=x^n$. It's derivative of $k^{th}$ order can be expressed by the formula: $$\frac{d^k}{dx^k}x^n=\frac{n!}{(n-k)!}x^{n-k}$$ Similarly, the $k^{th}$ integral (integral operator applied $k$ times) can be expressed as: $$\frac{n!}{(n+k)!}x^{n+k}$$ According the the Wikipedia article https://en.wikipedia.org/wiki/Fractional_calculus , we can replace the factorial with the Gamma function to get derivatives of fractional order. So, applying the derivative of half order twice to $\frac{x^{n+1}}{n+1}+C$, should get us to $x^n$. Applying the half-ordered derivative once gives: $$\frac{d^{1/2}}{{dx^{1/2}}}\left(\frac{x^{n+1}}{n+1}+Cx^0\right)=\frac{1}{n+1}\frac{\Pi(n+1)}{\Pi(n+1/2)}x^{n+1/2}+C\frac{1}{\Pi(-1/2)}x^{-1/2}$$ where $\Pi(x)$ is the generalization of the factorial function, and $\Pi(x)=\Gamma(1+x)$ Again, applying the half-ordered derivative gives: $$\frac{1}{n+1}\frac{\Pi(n+1)}{\Pi(n)}x^n+\frac{C}{\Pi(-1)}x^{-1}=x^n$$ which works fine because $\frac{C}{\Pi(-1)}\rightarrow 0$. So, the derivative works good but that's not the case with fractional-ordered integration. Applying the half-ordered integral operator twice to $x^n$ should give us $\frac{x^{n+1}}{n+1}+C$. Applying the half-ordered integral once means finding a function whose half-ordered derivative is $x^n$. So, applying it once gives: $$\frac{\Pi(n)}{\Pi{(n+1/2)}}x^{n+1/2}+C\frac{1}{\Pi(-1/2)}x^{-1/2}$$ Again, applying the half ordered derivative to this function should give a function whose half-ordered derivative is this function. So, again applying the half-integral operator gives: $$\frac{x^{n+1}}{n+1}+C+C'\frac{1}{\Pi(-1/2)}x^{-1/2}\neq \frac{x^{n+1}}{n+1}+C$$ where $C'$ is another constant. So, why does this additional term containing $C'$ get introduced? Is the theory of fractional derivatives flawed? Is there any way to get a single constant $C$ in the end by applying the half-integral operator two times?","Let's talk about the function $f(x)=x^n$. It's derivative of $k^{th}$ order can be expressed by the formula: $$\frac{d^k}{dx^k}x^n=\frac{n!}{(n-k)!}x^{n-k}$$ Similarly, the $k^{th}$ integral (integral operator applied $k$ times) can be expressed as: $$\frac{n!}{(n+k)!}x^{n+k}$$ According the the Wikipedia article https://en.wikipedia.org/wiki/Fractional_calculus , we can replace the factorial with the Gamma function to get derivatives of fractional order. So, applying the derivative of half order twice to $\frac{x^{n+1}}{n+1}+C$, should get us to $x^n$. Applying the half-ordered derivative once gives: $$\frac{d^{1/2}}{{dx^{1/2}}}\left(\frac{x^{n+1}}{n+1}+Cx^0\right)=\frac{1}{n+1}\frac{\Pi(n+1)}{\Pi(n+1/2)}x^{n+1/2}+C\frac{1}{\Pi(-1/2)}x^{-1/2}$$ where $\Pi(x)$ is the generalization of the factorial function, and $\Pi(x)=\Gamma(1+x)$ Again, applying the half-ordered derivative gives: $$\frac{1}{n+1}\frac{\Pi(n+1)}{\Pi(n)}x^n+\frac{C}{\Pi(-1)}x^{-1}=x^n$$ which works fine because $\frac{C}{\Pi(-1)}\rightarrow 0$. So, the derivative works good but that's not the case with fractional-ordered integration. Applying the half-ordered integral operator twice to $x^n$ should give us $\frac{x^{n+1}}{n+1}+C$. Applying the half-ordered integral once means finding a function whose half-ordered derivative is $x^n$. So, applying it once gives: $$\frac{\Pi(n)}{\Pi{(n+1/2)}}x^{n+1/2}+C\frac{1}{\Pi(-1/2)}x^{-1/2}$$ Again, applying the half ordered derivative to this function should give a function whose half-ordered derivative is this function. So, again applying the half-integral operator gives: $$\frac{x^{n+1}}{n+1}+C+C'\frac{1}{\Pi(-1/2)}x^{-1/2}\neq \frac{x^{n+1}}{n+1}+C$$ where $C'$ is another constant. So, why does this additional term containing $C'$ get introduced? Is the theory of fractional derivatives flawed? Is there any way to get a single constant $C$ in the end by applying the half-integral operator two times?",,"['calculus', 'integration']"
23,Find the maximum and minimum values of $\sin^2\theta+\sin^2\phi$ when $\theta+\phi=\alpha$,Find the maximum and minimum values of  when,\sin^2\theta+\sin^2\phi \theta+\phi=\alpha,"Find the maximum and minimum values of $\sin^2\theta+\sin^2\phi$ when $\theta+\phi=\alpha$(a constant). $\theta+\phi=\alpha\implies\phi=\alpha-\theta$ $\sin^2\theta+\sin^2\phi=\sin^2\theta+\sin^2(\alpha-\theta)$ Let $f(\theta)=\sin^2\theta+\sin^2(\alpha-\theta)$ $f'(\theta)=2\sin\theta\cos\theta-2\sin(\alpha-\theta)\cos(\alpha-\theta)$ Putting $f'(\theta)=0$ gives $\sin2\theta=\sin2(\alpha-\theta)$ $2\theta=2\alpha-2\theta\implies \alpha=2\theta$ If $\alpha=2\theta$,then by $\theta+\phi=\alpha$ gives $\phi=\theta$ I am stuck here,the answer given is maximum $1+\cos\alpha$ and minimum $1-\cos\alpha$.But i have found only one critical value(when $\phi=\theta$) and that too i cannot decide whether it will give maximum or minimum value. Please help.","Find the maximum and minimum values of $\sin^2\theta+\sin^2\phi$ when $\theta+\phi=\alpha$(a constant). $\theta+\phi=\alpha\implies\phi=\alpha-\theta$ $\sin^2\theta+\sin^2\phi=\sin^2\theta+\sin^2(\alpha-\theta)$ Let $f(\theta)=\sin^2\theta+\sin^2(\alpha-\theta)$ $f'(\theta)=2\sin\theta\cos\theta-2\sin(\alpha-\theta)\cos(\alpha-\theta)$ Putting $f'(\theta)=0$ gives $\sin2\theta=\sin2(\alpha-\theta)$ $2\theta=2\alpha-2\theta\implies \alpha=2\theta$ If $\alpha=2\theta$,then by $\theta+\phi=\alpha$ gives $\phi=\theta$ I am stuck here,the answer given is maximum $1+\cos\alpha$ and minimum $1-\cos\alpha$.But i have found only one critical value(when $\phi=\theta$) and that too i cannot decide whether it will give maximum or minimum value. Please help.",,"['calculus', 'trigonometry', 'optimization', 'supremum-and-infimum']"
24,"Evaluate $\sum_{n=1}^{\infty }\int_{0}^{\frac{1}{n}}\frac{\sqrt{x}}{1+x^{2}}\,\mathrm{d}x$",Evaluate,"\sum_{n=1}^{\infty }\int_{0}^{\frac{1}{n}}\frac{\sqrt{x}}{1+x^{2}}\,\mathrm{d}x","I have some trouble in evaluating this series $$\sum_{n=1}^{\infty }\int_{0}^{\frac{1}{n}}\frac{\sqrt{x}}{1+x^{2}}\mathrm{d}x$$ I tried to calculate the integral first, but after that I found the series become so complicated. Besides, I found maybe the series equals to  $$\sum_{k=0}^{\infty }\frac{\left ( -1 \right )^{k}\zeta \left ( 2k+\dfrac{3}{2} \right )}{2k+\dfrac{3}{2}}$$ this is definitely a monster to me. So I want to know is there a good way to solve this integral-series.","I have some trouble in evaluating this series $$\sum_{n=1}^{\infty }\int_{0}^{\frac{1}{n}}\frac{\sqrt{x}}{1+x^{2}}\mathrm{d}x$$ I tried to calculate the integral first, but after that I found the series become so complicated. Besides, I found maybe the series equals to  $$\sum_{k=0}^{\infty }\frac{\left ( -1 \right )^{k}\zeta \left ( 2k+\dfrac{3}{2} \right )}{2k+\dfrac{3}{2}}$$ this is definitely a monster to me. So I want to know is there a good way to solve this integral-series.",,"['calculus', 'integration', 'sequences-and-series', 'analysis', 'zeta-functions']"
25,A limit about $\int_0^\infty x^{-x}e^{tx}dx$,A limit about,\int_0^\infty x^{-x}e^{tx}dx,"How to prove $$\lim_{t\to +\infty}\frac{\int_0^{+\infty} x^{-x}e^{tx}dx}{e^{\frac12(t-1)+e^{t-1}}}=\sqrt{2\pi}.$$ Someone asked this difficult question, I have tried Taylor formula of $e^x$ but failed, could you show  me the method?","How to prove $$\lim_{t\to +\infty}\frac{\int_0^{+\infty} x^{-x}e^{tx}dx}{e^{\frac12(t-1)+e^{t-1}}}=\sqrt{2\pi}.$$ Someone asked this difficult question, I have tried Taylor formula of $e^x$ but failed, could you show  me the method?",,"['calculus', 'integration', 'combinatorics', 'analysis', 'limits']"
26,What is $\lim_{n\to\infty}2^n\sqrt{2-\sqrt{2+\sqrt{2+\dots+\sqrt{p}}}}$ for $negative$ and other $p$?,What is  for  and other ?,\lim_{n\to\infty}2^n\sqrt{2-\sqrt{2+\sqrt{2+\dots+\sqrt{p}}}} negative p,"This was inspired by similar posts like this one . Define the function, $$F(p) = \lim_{n\to\infty}2^n\sqrt{2-\underbrace{\sqrt{2+\sqrt{2+\dots+\sqrt{p}}}}_{n \textrm{ square roots}}}$$ We know that, $$F(2) = \frac{\pi}{2},\quad F(3) = \frac{\pi}{3}$$ I was wondering what it evaluates to if we use other integers . Some numerical computation and the Inverse Symbolic Calculator suggests that, $$\begin{aligned} F(5) &= 2\ln\big(\tfrac{1+\sqrt{5}}{2}\big)\,i\\ F(6) &= \ln\big(2+\sqrt{3}\big)\,i\\ F(7) &= \ln\big(\tfrac{5+\sqrt{3\times7}}{2}\big)\,i\\ \vdots\\ F(11) &= \ln\big(\tfrac{9+\sqrt{7\times11}}{2}\big)\,i\\ \vdots\\ F(17) &= \ln\big(\tfrac{15+\sqrt{13\times17}}{2}\big)\,i \end{aligned}$$ Note that the radical arguments are fundamental units . If we use negative $p$, $$\begin{aligned} F(-1) &= \pi-2\ln\big(\tfrac{1+\sqrt{5}}{2}\big)\,i\\ F(-2) &= \pi-\ln\big(2+\sqrt{3}\big)\,i\\ F(-3) &= \pi-\ln\big(\tfrac{5+\sqrt{3\times7}}{2}\big)\,i\\ \vdots\\ F(-7) &= \pi-\ln\big(\tfrac{9+\sqrt{7\times11}}{2}\big)\,i\\ \vdots\\ F(-13) &= \pi-\ln\big(\tfrac{15+\sqrt{13\times17}}{2}\big)\,i \end{aligned}$$ and so on. It seems $F(2+m)+F(2-m) = \pi$. I also observed that if $m\pm2$ are primes, then, $$F(2+m) = \pi-F(2-m) = \ln\Big(\tfrac{m+\sqrt{(m-2)(m+2)}}{2}\Big)\,i\tag1$$ though the form of $(1)$ is only conjectural. Question: What is then the formula for $F(p)$ using general $p$?","This was inspired by similar posts like this one . Define the function, $$F(p) = \lim_{n\to\infty}2^n\sqrt{2-\underbrace{\sqrt{2+\sqrt{2+\dots+\sqrt{p}}}}_{n \textrm{ square roots}}}$$ We know that, $$F(2) = \frac{\pi}{2},\quad F(3) = \frac{\pi}{3}$$ I was wondering what it evaluates to if we use other integers . Some numerical computation and the Inverse Symbolic Calculator suggests that, $$\begin{aligned} F(5) &= 2\ln\big(\tfrac{1+\sqrt{5}}{2}\big)\,i\\ F(6) &= \ln\big(2+\sqrt{3}\big)\,i\\ F(7) &= \ln\big(\tfrac{5+\sqrt{3\times7}}{2}\big)\,i\\ \vdots\\ F(11) &= \ln\big(\tfrac{9+\sqrt{7\times11}}{2}\big)\,i\\ \vdots\\ F(17) &= \ln\big(\tfrac{15+\sqrt{13\times17}}{2}\big)\,i \end{aligned}$$ Note that the radical arguments are fundamental units . If we use negative $p$, $$\begin{aligned} F(-1) &= \pi-2\ln\big(\tfrac{1+\sqrt{5}}{2}\big)\,i\\ F(-2) &= \pi-\ln\big(2+\sqrt{3}\big)\,i\\ F(-3) &= \pi-\ln\big(\tfrac{5+\sqrt{3\times7}}{2}\big)\,i\\ \vdots\\ F(-7) &= \pi-\ln\big(\tfrac{9+\sqrt{7\times11}}{2}\big)\,i\\ \vdots\\ F(-13) &= \pi-\ln\big(\tfrac{15+\sqrt{13\times17}}{2}\big)\,i \end{aligned}$$ and so on. It seems $F(2+m)+F(2-m) = \pi$. I also observed that if $m\pm2$ are primes, then, $$F(2+m) = \pi-F(2-m) = \ln\Big(\tfrac{m+\sqrt{(m-2)(m+2)}}{2}\Big)\,i\tag1$$ though the form of $(1)$ is only conjectural. Question: What is then the formula for $F(p)$ using general $p$?",,"['calculus', 'limits', 'logarithms', 'pi', 'nested-radicals']"
27,gradient flow on $SU(n)$,gradient flow on,SU(n),"Define the following cost functions $f_1, f_2 :SU(n) \rightarrow \mathbb{R}$ by $f_1(U) = Re \left( \text{Tr}\left(G^{\dagger} U \right) \right)$ and $f_2(U) = \left| \left( \text{Tr}\left(G^{\dagger} U \right) \right) \right|^2$ for some fixed $G \in SU(n)$. What are the gradients $\nabla f_1, \nabla f_2$ of these functions and how can they be expressed?","Define the following cost functions $f_1, f_2 :SU(n) \rightarrow \mathbb{R}$ by $f_1(U) = Re \left( \text{Tr}\left(G^{\dagger} U \right) \right)$ and $f_2(U) = \left| \left( \text{Tr}\left(G^{\dagger} U \right) \right) \right|^2$ for some fixed $G \in SU(n)$. What are the gradients $\nabla f_1, \nabla f_2$ of these functions and how can they be expressed?",,"['calculus', 'differential-geometry', 'optimization', 'lie-groups', 'lie-algebras']"
28,Integration of $\frac{1}{\sin x+\cos x}$,Integration of,\frac{1}{\sin x+\cos x},"I'm given this $\int\frac{1}{\sin x+\cos x}dx$. My attempt, $\sin x+\cos x=R\cos (x-\alpha)$ $R\cos \alpha=1$ and $R\sin \alpha=1$ $R=\sqrt{1^2+1^2}=\sqrt{2}$, $\tan\alpha=1$ $\alpha=\frac{\pi}{4}$ So, $\sin x+\cos x=\sqrt{2}\cos (x-\frac{\pi}{4})$ $\int\frac{1}{\sin x+\cos x}dx=\int \frac{1}{\sqrt{2}\cos (x-\frac{\pi}{4})}dx$ $=\frac{1}{\sqrt{2}}\int \sec (x-\frac{\pi}{4})dx$ $=\frac{1}{\sqrt{2}} \ln \left | \sec (x-\frac{\pi}{4})+\tan (x-\frac{\pi}{4}) \right |+c$ Am I correct? Is there another way to solve this integral? Thanks in advance.","I'm given this $\int\frac{1}{\sin x+\cos x}dx$. My attempt, $\sin x+\cos x=R\cos (x-\alpha)$ $R\cos \alpha=1$ and $R\sin \alpha=1$ $R=\sqrt{1^2+1^2}=\sqrt{2}$, $\tan\alpha=1$ $\alpha=\frac{\pi}{4}$ So, $\sin x+\cos x=\sqrt{2}\cos (x-\frac{\pi}{4})$ $\int\frac{1}{\sin x+\cos x}dx=\int \frac{1}{\sqrt{2}\cos (x-\frac{\pi}{4})}dx$ $=\frac{1}{\sqrt{2}}\int \sec (x-\frac{\pi}{4})dx$ $=\frac{1}{\sqrt{2}} \ln \left | \sec (x-\frac{\pi}{4})+\tan (x-\frac{\pi}{4}) \right |+c$ Am I correct? Is there another way to solve this integral? Thanks in advance.",,"['calculus', 'integration', 'indefinite-integrals']"
29,What intuition stands behind implicit differentiation,What intuition stands behind implicit differentiation,,"I'm trying to undestand implicit differentation Let's take as a an example equation y^2 + x^2 = 1 1. How i think about how the equation works I think the function as : if x changes then the y term have to hold value of ""y^2 + x^2"" equal 1. Therefore the equation defines some set of numbers at x cordinates and y cordinates. 2. How i think about how differentate the equation If i want to know how the equation changes as x changes, i'm taking derivative with respect to x $\frac{d}{dx}y^2+\frac{d}{dx}x^2=\frac{d}{dx}1$ We can consider $y$ as a function, $y = f(x)$ Therefore: $\frac{d}{dx}(f(x))^2+\frac{d}{dx}x^2=\frac{d}{dx}1$ We can calculate how (f(x))^2 changes as f(x) changes, using chain rule. $\frac{df(x)}{dx}\frac{d}{df(x)}(f(x))^2+\frac{d}{dx}x^2=\frac{d}{dx}1$ This is equal: $2f(x)\frac{df(x)}{dx}f(x)+\frac{d}{dx}x^2=\frac{d}{dx}1$ As $x$ changes, $x^2$ changes as $2x$, therefore $2f(x)\frac{df(x)}{dx}f(x)+2x=\frac{d}{dx}1$ As x changes, 1 doesn't changes, therefore it is 0.  $2f(x)\frac{df(x)}{dx}f(x)+2x=0$ We don't know derivative of $f(x)$ but we can solve it If we solve the derivative, we get $f'(x) = -\frac xy$ 3. Questions My way of thinking is right? What does mean the final answer? It looks strange, it doesn't tell me nothing comparing to norma, explicit derivative of a function. There is a difference between $\frac{dy}{dx}$ and $\frac{d}{dx}y$ ? Why i want to know ? Because i want to know how to interpretate steps and solution, not only algorithmically solve some book's problems. PS. I'm barely after highschool - Therefore i don't know yet set theorem and other high level math things. I'm learning calculus on my own.","I'm trying to undestand implicit differentation Let's take as a an example equation y^2 + x^2 = 1 1. How i think about how the equation works I think the function as : if x changes then the y term have to hold value of ""y^2 + x^2"" equal 1. Therefore the equation defines some set of numbers at x cordinates and y cordinates. 2. How i think about how differentate the equation If i want to know how the equation changes as x changes, i'm taking derivative with respect to x $\frac{d}{dx}y^2+\frac{d}{dx}x^2=\frac{d}{dx}1$ We can consider $y$ as a function, $y = f(x)$ Therefore: $\frac{d}{dx}(f(x))^2+\frac{d}{dx}x^2=\frac{d}{dx}1$ We can calculate how (f(x))^2 changes as f(x) changes, using chain rule. $\frac{df(x)}{dx}\frac{d}{df(x)}(f(x))^2+\frac{d}{dx}x^2=\frac{d}{dx}1$ This is equal: $2f(x)\frac{df(x)}{dx}f(x)+\frac{d}{dx}x^2=\frac{d}{dx}1$ As $x$ changes, $x^2$ changes as $2x$, therefore $2f(x)\frac{df(x)}{dx}f(x)+2x=\frac{d}{dx}1$ As x changes, 1 doesn't changes, therefore it is 0.  $2f(x)\frac{df(x)}{dx}f(x)+2x=0$ We don't know derivative of $f(x)$ but we can solve it If we solve the derivative, we get $f'(x) = -\frac xy$ 3. Questions My way of thinking is right? What does mean the final answer? It looks strange, it doesn't tell me nothing comparing to norma, explicit derivative of a function. There is a difference between $\frac{dy}{dx}$ and $\frac{d}{dx}y$ ? Why i want to know ? Because i want to know how to interpretate steps and solution, not only algorithmically solve some book's problems. PS. I'm barely after highschool - Therefore i don't know yet set theorem and other high level math things. I'm learning calculus on my own.",,"['calculus', 'derivatives', 'education', 'implicit-differentiation']"
30,Infinite product related to the Wallis product,Infinite product related to the Wallis product,,"Some time ago I heard  this math question on the radio: The Wallis product $$\frac{2}{1}*\frac{2}{3}*\frac{4}{3}*\frac{4}{5}*\frac{6}{5}*\frac{6}{7}* \dotsb$$ is known to converge to $\pi/2$, but what does this infinite product converge to: $$ \frac{\sqrt[2]{2}}{\sqrt[1]{1}}*\frac{\sqrt[2]{2}}{\sqrt[3]{3}}*\frac{\sqrt[4]{4}}{\sqrt[3]{3}}*\frac{\sqrt[4]{4}}{\sqrt[5]{5}}*\frac{\sqrt[6]{6}}{\sqrt[5]{5}}*\frac{\sqrt[6]{6}}{\sqrt[7]{7}} *\dotsb $$ All I know about it is that it is aproximatly equal to $1.37676673907$ Wolfram Alpha doesn't give me much more How does one attack such a problem? I know how the value for the Wallis product was found, but none of these techniques seem to work on this seemingly related product. Tell me anything you find or know about it! Edit: Thanks to the link lucian gave me I found the closed form: $$2^{2\gamma-\ln{(2)}}$$ I checked 50 digits and they are all correct, now how would you arrive at this or how would you prove this?","Some time ago I heard  this math question on the radio: The Wallis product $$\frac{2}{1}*\frac{2}{3}*\frac{4}{3}*\frac{4}{5}*\frac{6}{5}*\frac{6}{7}* \dotsb$$ is known to converge to $\pi/2$, but what does this infinite product converge to: $$ \frac{\sqrt[2]{2}}{\sqrt[1]{1}}*\frac{\sqrt[2]{2}}{\sqrt[3]{3}}*\frac{\sqrt[4]{4}}{\sqrt[3]{3}}*\frac{\sqrt[4]{4}}{\sqrt[5]{5}}*\frac{\sqrt[6]{6}}{\sqrt[5]{5}}*\frac{\sqrt[6]{6}}{\sqrt[7]{7}} *\dotsb $$ All I know about it is that it is aproximatly equal to $1.37676673907$ Wolfram Alpha doesn't give me much more How does one attack such a problem? I know how the value for the Wallis product was found, but none of these techniques seem to work on this seemingly related product. Tell me anything you find or know about it! Edit: Thanks to the link lucian gave me I found the closed form: $$2^{2\gamma-\ln{(2)}}$$ I checked 50 digits and they are all correct, now how would you arrive at this or how would you prove this?",,"['calculus', 'infinite-product']"
31,The fastest trajectory that a particle should follow through two different mediums,The fastest trajectory that a particle should follow through two different mediums,,"Hello, after 3 failed attempts to solve this problem, I decided to start a bounty for this question. Please, I need a complete answer with an interpreation of the final result. Thank you in advance. Problem: A particle travels at speed $v_a$ in a medium $A$ and at speed $v_b$ in an medium $B$. The particle departs on time $t=0$ from the point $P_i$ and has to arrive in the minimal time to the point $P_f$, as in the picture. Determine the trajectory that the particle has to follow to arrive from to the point $P_f$ in the minimal time. Attempt 1: I know that the fastest way to go from a point to another point is the segment of straight line that joins the two points, but I don't know how to consider the velocities $v_a$ and $v_b$ of the problem, or if they're just distractors to confuse me and there's no need to consider them. Am I right or not? Attempt 2: Let $P_c$ be the point where the particle crosses the boundary between $A$ and $B$, and let $r$ be the distance from the left point of the boundary to $P_c$. We have $t_{P_i P_c}=d_{P_i P_c} /v_a=\frac{\sqrt{h^2+r^2}}{v_a}$ $t_{P_c P_f}=d_{P_c P_f} /v_b = \frac{\sqrt{m^2+(s-r)^2}}{v_b}$ So, $t_{P_i P_f}=t_{P_i P_c}+t_{P_c P_f}=\frac{\sqrt{h^2+r^2}}{v_a}+\frac{\sqrt{m^2+(s-r)^2}}{v_b}$. Now, we have to consider the function: $t(r)=\frac{\sqrt{h^2+r^2}}{v_a}+\frac{\sqrt{m^2+(s-r)^2}}{v_b}$, differentiating with respect to $r$ and setting to zero, we get: $t'(r)=\frac{r}{v_a \sqrt{h^2+r^2}}-\frac{s-r}{v_b \sqrt{m^2+(s-r)^2}}=0$ So, $rv_b \sqrt{m^2+(s-r)^2}=(s-r)v_a \sqrt{h^2+r^2}$ and $r^2v_b^2 (m^2+(s-r)^2)=(s-r)^2v_a^2 (h^2+r^2)$ Rearranging, we get the following 4th degree polynomial in $r$: $(v_b^2-v_a^2)r^4+2s(v_a^2-v_b^2)r^3+(v_b^2m^2-v_a^2h^2+v_b^2s^2-s^2v_a^2)r^2+(2sv_a^2h^2)r-(s^2v_a^2h^2)=0.$ So, now do I have to solve this equation? Attempt 3: Let $P_c$ be the point where the particle crosses the boundary between $A$ and $B$, let $r$ be the distance from the left point of the boundary to $P_c$, and let $\alpha$ and $\beta$ be the angles in the following picture: We have $t_{P_i P_c}=d_{P_i P_c} /v_a=\frac{r \sin \alpha}{v_a}$ $t_{P_c P_f}=d_{P_c P_f} /v_b = \frac{(s-r) \sin \beta}{v_b}$ So, $t_{P_i P_f}=t_{P_i P_c}+t_{P_c P_f}= \frac{r \sin \alpha}{v_a} +\frac{(s-r) \sin \beta}{v_b}.$ Now, we have to consider the function: $t(r)=\frac{r \sin \alpha}{v_a} +\frac{(s-r) \sin \beta}{v_b},$ differentiating with respect to $r$ and setting to zero, we get: $t'(r)=\frac{\sin \alpha}{v_a}-\frac{r \sin \beta}{v_b}=0.$ So, $\frac{\sin \alpha}{v_a}=\frac{r \sin \beta}{v_b},$ and then $r=\frac{\sin \alpha}{\sin \beta}\frac{v_b}{v_a}=1,$ ?? by Snell's law.","Hello, after 3 failed attempts to solve this problem, I decided to start a bounty for this question. Please, I need a complete answer with an interpreation of the final result. Thank you in advance. Problem: A particle travels at speed $v_a$ in a medium $A$ and at speed $v_b$ in an medium $B$. The particle departs on time $t=0$ from the point $P_i$ and has to arrive in the minimal time to the point $P_f$, as in the picture. Determine the trajectory that the particle has to follow to arrive from to the point $P_f$ in the minimal time. Attempt 1: I know that the fastest way to go from a point to another point is the segment of straight line that joins the two points, but I don't know how to consider the velocities $v_a$ and $v_b$ of the problem, or if they're just distractors to confuse me and there's no need to consider them. Am I right or not? Attempt 2: Let $P_c$ be the point where the particle crosses the boundary between $A$ and $B$, and let $r$ be the distance from the left point of the boundary to $P_c$. We have $t_{P_i P_c}=d_{P_i P_c} /v_a=\frac{\sqrt{h^2+r^2}}{v_a}$ $t_{P_c P_f}=d_{P_c P_f} /v_b = \frac{\sqrt{m^2+(s-r)^2}}{v_b}$ So, $t_{P_i P_f}=t_{P_i P_c}+t_{P_c P_f}=\frac{\sqrt{h^2+r^2}}{v_a}+\frac{\sqrt{m^2+(s-r)^2}}{v_b}$. Now, we have to consider the function: $t(r)=\frac{\sqrt{h^2+r^2}}{v_a}+\frac{\sqrt{m^2+(s-r)^2}}{v_b}$, differentiating with respect to $r$ and setting to zero, we get: $t'(r)=\frac{r}{v_a \sqrt{h^2+r^2}}-\frac{s-r}{v_b \sqrt{m^2+(s-r)^2}}=0$ So, $rv_b \sqrt{m^2+(s-r)^2}=(s-r)v_a \sqrt{h^2+r^2}$ and $r^2v_b^2 (m^2+(s-r)^2)=(s-r)^2v_a^2 (h^2+r^2)$ Rearranging, we get the following 4th degree polynomial in $r$: $(v_b^2-v_a^2)r^4+2s(v_a^2-v_b^2)r^3+(v_b^2m^2-v_a^2h^2+v_b^2s^2-s^2v_a^2)r^2+(2sv_a^2h^2)r-(s^2v_a^2h^2)=0.$ So, now do I have to solve this equation? Attempt 3: Let $P_c$ be the point where the particle crosses the boundary between $A$ and $B$, let $r$ be the distance from the left point of the boundary to $P_c$, and let $\alpha$ and $\beta$ be the angles in the following picture: We have $t_{P_i P_c}=d_{P_i P_c} /v_a=\frac{r \sin \alpha}{v_a}$ $t_{P_c P_f}=d_{P_c P_f} /v_b = \frac{(s-r) \sin \beta}{v_b}$ So, $t_{P_i P_f}=t_{P_i P_c}+t_{P_c P_f}= \frac{r \sin \alpha}{v_a} +\frac{(s-r) \sin \beta}{v_b}.$ Now, we have to consider the function: $t(r)=\frac{r \sin \alpha}{v_a} +\frac{(s-r) \sin \beta}{v_b},$ differentiating with respect to $r$ and setting to zero, we get: $t'(r)=\frac{\sin \alpha}{v_a}-\frac{r \sin \beta}{v_b}=0.$ So, $\frac{\sin \alpha}{v_a}=\frac{r \sin \beta}{v_b},$ and then $r=\frac{\sin \alpha}{\sin \beta}\frac{v_b}{v_a}=1,$ ?? by Snell's law.",,"['calculus', 'optimization']"
32,"How to determine if a function is decreasing, constant or increasing (in a given interval) if its derivative function has no zeroes?","How to determine if a function is decreasing, constant or increasing (in a given interval) if its derivative function has no zeroes?",,"Let us suppose you have a certain function $f(x)$ and you want to find out in which intervals this function is decreasing, constant or increasing. I know you need to follow these steps: Find out $f'(x)$. Find out the values for which $f'(x)=0$. In other words, we need to find out the zeroes of $f'(x)$. Let's suppose we find out two values which are $x=a$ and $x=b$, and that $a<b$. Now we need to choose a random value $r$ from the interval $(-\infty,a]$ or $(-\infty,a)$ (I don't remember exactly) and calculate $f'(x)$ for $x=r$. If $f'(x)<0$ for $x=r$, then $f(x)$ is decreasing in the mentioned interval. If $f'(x)=0$ for $x=r$, then $f(x)$ is constant in that interval. If $f'(x)>0$ for $x=r$, then $f(x)$ is increasing in that interval. We need to repeat steps 3 and 4 for the other intervals. Now, what happens if $f'(x)$ doesn't have any zeroes? What should I do? Example of a (derivative) function that doesn't have any zeroes: $e^x/x$. Thanks in advance!","Let us suppose you have a certain function $f(x)$ and you want to find out in which intervals this function is decreasing, constant or increasing. I know you need to follow these steps: Find out $f'(x)$. Find out the values for which $f'(x)=0$. In other words, we need to find out the zeroes of $f'(x)$. Let's suppose we find out two values which are $x=a$ and $x=b$, and that $a<b$. Now we need to choose a random value $r$ from the interval $(-\infty,a]$ or $(-\infty,a)$ (I don't remember exactly) and calculate $f'(x)$ for $x=r$. If $f'(x)<0$ for $x=r$, then $f(x)$ is decreasing in the mentioned interval. If $f'(x)=0$ for $x=r$, then $f(x)$ is constant in that interval. If $f'(x)>0$ for $x=r$, then $f(x)$ is increasing in that interval. We need to repeat steps 3 and 4 for the other intervals. Now, what happens if $f'(x)$ doesn't have any zeroes? What should I do? Example of a (derivative) function that doesn't have any zeroes: $e^x/x$. Thanks in advance!",,"['calculus', 'functions', 'derivatives']"
33,On Magnitudes of Sums of Roots of Unity and a Simple Trigonometric Inequality,On Magnitudes of Sums of Roots of Unity and a Simple Trigonometric Inequality,,"The Problem Let $r,q,m$ be positive integers such that $4 \leq r$ and $1<m,q\leq r/2$. Is it the case that $$\left |  \sum_{k=0}^{q-1} \zeta^{km}\right | < \left |\sum_{k=0}^{q-1} \zeta^{k}\right |,$$ where $\zeta$ is a primitive $r^{th}$ root of unity. Continuous Variant Through use of a non-standard trigonometric identity which may be found here we can change the previous inequality to $$|\sin(mq\pi/r)\sin(\pi/r)| < |\sin(m\pi/r)\sin(q\pi/r)|.$$ Then we may substitute real variables. Let $\theta \in (0,\pi/4]$ and $a,b \in \mathbb R$ so that $0< a\theta, b \theta \leq \pi$. The we have the inequality $$| \sin(ab \theta) \sin(\theta) |< | \sin(a \theta)\sin(b \theta) |. $$ Fix $\theta$ and consider the function $$f(x,y)=\frac{\sin(xy\theta)\sin(\theta)}{\sin(x\theta)\sin(y\theta)}$$ where $(x,y) \in [1,\pi/(2\theta)]^2=X$. So if we can show that $f$ has a unique maximum on $X$ at $(1,1)$ we would also have our result. One can show with a somewhat painful computation that the critical points of $f$ are on the line $x=y$, so we can consider instead the function $$g(x)=\frac{\sin(x^2\theta)\sin(\theta)}{\sin^2(x\theta)}$$ and the corresponding inequality $$|\sin(x^2\theta)\sin(\theta)|<|\sin^2(x\theta)|.$$ I have some partial results in this case where essentially I can control the inequality depending on where $x^2\theta$ is modulo $2\pi$. Which comes down to everything except when the residue of $x^2\theta$ modulo $2\pi$ is in the interval $(2x\theta-\theta,2x\theta)$. Motivation Let the generalized binomial coefficient $C_q(n,k)$ be the coefficient of $x^k$ in the polynomial $(1+x+\cdots+x^{q-1})^k$, sometimes this appears in the literature as $\binom{n}{k}_q$. Define $$PC_q(n,r,k)=\sum_{j \in \mathbb Z} C_q(n,k+rj)$$ then the sums in the statement of the problem arise naturally as the coefficients of a discrete Fourier expansion of $PC_q$. In particular proving the desired inequality shows that $PC_q$ behaves as a sine function as $n \rightarrow \infty$. There's some more background on $C_q$ and $PC_q$ is this answer .","The Problem Let $r,q,m$ be positive integers such that $4 \leq r$ and $1<m,q\leq r/2$. Is it the case that $$\left |  \sum_{k=0}^{q-1} \zeta^{km}\right | < \left |\sum_{k=0}^{q-1} \zeta^{k}\right |,$$ where $\zeta$ is a primitive $r^{th}$ root of unity. Continuous Variant Through use of a non-standard trigonometric identity which may be found here we can change the previous inequality to $$|\sin(mq\pi/r)\sin(\pi/r)| < |\sin(m\pi/r)\sin(q\pi/r)|.$$ Then we may substitute real variables. Let $\theta \in (0,\pi/4]$ and $a,b \in \mathbb R$ so that $0< a\theta, b \theta \leq \pi$. The we have the inequality $$| \sin(ab \theta) \sin(\theta) |< | \sin(a \theta)\sin(b \theta) |. $$ Fix $\theta$ and consider the function $$f(x,y)=\frac{\sin(xy\theta)\sin(\theta)}{\sin(x\theta)\sin(y\theta)}$$ where $(x,y) \in [1,\pi/(2\theta)]^2=X$. So if we can show that $f$ has a unique maximum on $X$ at $(1,1)$ we would also have our result. One can show with a somewhat painful computation that the critical points of $f$ are on the line $x=y$, so we can consider instead the function $$g(x)=\frac{\sin(x^2\theta)\sin(\theta)}{\sin^2(x\theta)}$$ and the corresponding inequality $$|\sin(x^2\theta)\sin(\theta)|<|\sin^2(x\theta)|.$$ I have some partial results in this case where essentially I can control the inequality depending on where $x^2\theta$ is modulo $2\pi$. Which comes down to everything except when the residue of $x^2\theta$ modulo $2\pi$ is in the interval $(2x\theta-\theta,2x\theta)$. Motivation Let the generalized binomial coefficient $C_q(n,k)$ be the coefficient of $x^k$ in the polynomial $(1+x+\cdots+x^{q-1})^k$, sometimes this appears in the literature as $\binom{n}{k}_q$. Define $$PC_q(n,r,k)=\sum_{j \in \mathbb Z} C_q(n,k+rj)$$ then the sums in the statement of the problem arise naturally as the coefficients of a discrete Fourier expansion of $PC_q$. In particular proving the desired inequality shows that $PC_q$ behaves as a sine function as $n \rightarrow \infty$. There's some more background on $C_q$ and $PC_q$ is this answer .",,"['calculus', 'combinatorics', 'inequality']"
34,"Evaluating $\int_{0}^{x} e^t \sqrt{2 + \sin(2t)} \, dt$",Evaluating,"\int_{0}^{x} e^t \sqrt{2 + \sin(2t)} \, dt","I was recently asked to evaluate the following integral: $$\int_0^x e^t \sqrt{2 + \sin(2t)} \, dt$$ It was beyond the ken of WolframAlpha, which I find quite discouraging. Does anyone have an idea of how to tackle this problem?","I was recently asked to evaluate the following integral: $$\int_0^x e^t \sqrt{2 + \sin(2t)} \, dt$$ It was beyond the ken of WolframAlpha, which I find quite discouraging. Does anyone have an idea of how to tackle this problem?",,"['calculus', 'integration', 'derivatives', 'problem-solving']"
35,Why doesn't this proof work?,Why doesn't this proof work?,,"In my youthful naiveté (i.e. fifteen minutes ago), I was trying to prove that if $X$ was a random variable with zero mean, then $XY$ also had zero mean for any random variable $Y$.  My proof used integration by parts: $\int_{\Omega} x(\omega)y(\omega)f(\omega)d\omega = y(\omega) \int_{\Omega}x(\omega)f(\omega)d\omega - \int_{\Omega}[\int_{\Omega}x(\omega)f(\omega)d\omega] dy(\omega)$ Since $X$ has zero mean, $\int_{\Omega} x(\omega)f(\omega)d\omega$ should be $0$, so the entire expression should be $0$.  However, I don't think this is true.  In fact, if $X \sim N(0, 1)$, then $E(X) = 0$ but $E(X^2) = 1$.  What am I doing wrong? Thanks!","In my youthful naiveté (i.e. fifteen minutes ago), I was trying to prove that if $X$ was a random variable with zero mean, then $XY$ also had zero mean for any random variable $Y$.  My proof used integration by parts: $\int_{\Omega} x(\omega)y(\omega)f(\omega)d\omega = y(\omega) \int_{\Omega}x(\omega)f(\omega)d\omega - \int_{\Omega}[\int_{\Omega}x(\omega)f(\omega)d\omega] dy(\omega)$ Since $X$ has zero mean, $\int_{\Omega} x(\omega)f(\omega)d\omega$ should be $0$, so the entire expression should be $0$.  However, I don't think this is true.  In fact, if $X \sim N(0, 1)$, then $E(X) = 0$ but $E(X^2) = 1$.  What am I doing wrong? Thanks!",,"['calculus', 'probability']"
36,Different type of function composition,Different type of function composition,,"When I first heard about function composition in school, I also expected the graphs of the composed function to 'behave' like both simple functions that are composed. Suppose we have f(x) = $x^2$ and $g(x) = \cos(x)$ The graph of $y = f(g(x))$ looks like in the first image: But my instinct in school was to think that the graph looks like the second image: Red curve represents f(x) = $x^2$ and the blue curve represents the result of the special composition $y = f(g(x))$ . Think of this special type of composition as molding/transposing the X-axis on the graph/curve of the outer function in the composition and then draw the graph of the inner function on previously modified coordinates plane. In the example above think of the parabola $x^2$ as the X-axis and then draw $ \cos(x)$ around this new X-axis (which is no longer a straight line). My questions are: Does this type of function composition have a name? Has it been studied? Are there any papers I can read on it? If yes, how can I search about them? I know that in this type of composition, the result might not be a function that can be written in explicit form $y=f(x)$ , but when can it be? As a final note, here is $\cos(x)$ composed with $\frac{1}{6}\cos(6x)$ . It looks nice, like the beginning of a fractal series. Red curve represents $g(x) = \cos(x)$ and the blue curve represents the result of the special composition.","When I first heard about function composition in school, I also expected the graphs of the composed function to 'behave' like both simple functions that are composed. Suppose we have f(x) = and The graph of looks like in the first image: But my instinct in school was to think that the graph looks like the second image: Red curve represents f(x) = and the blue curve represents the result of the special composition . Think of this special type of composition as molding/transposing the X-axis on the graph/curve of the outer function in the composition and then draw the graph of the inner function on previously modified coordinates plane. In the example above think of the parabola as the X-axis and then draw around this new X-axis (which is no longer a straight line). My questions are: Does this type of function composition have a name? Has it been studied? Are there any papers I can read on it? If yes, how can I search about them? I know that in this type of composition, the result might not be a function that can be written in explicit form , but when can it be? As a final note, here is composed with . It looks nice, like the beginning of a fractal series. Red curve represents and the blue curve represents the result of the special composition.",x^2 g(x) = \cos(x) y = f(g(x)) x^2 y = f(g(x)) x^2  \cos(x) y=f(x) \cos(x) \frac{1}{6}\cos(6x) g(x) = \cos(x),"['calculus', 'functions', 'graphing-functions', 'function-and-relation-composition']"
37,How to find the height of sand in an hourglass?,How to find the height of sand in an hourglass?,,"EDIT: I have made an important correction to the ""previous question"" link below... it was accidentally pointing to an unrelated question before. I would also like to emphasize that I welcome any completely different approach to the question (solving for height as a function of ""P"") -- not just a correction to/extension of my potentially flawed approach... the simpler the better, of course. Also, I'm realizing that it's unclear whether I'm asking for a general solution for any shape that conforms to the described properties, or whether I'm asking for a specific, perhaps more elegant, solution specifically for the lemniscate of Bernoulli... or thirdly, for help completing my specific attempt at a solution.  I am interested in all those things, so, honestly I don't really know what to do about that other than to break the question up into multiple separate questions.  Hopefully it's OK as is. Let's say we have an hourglass/""container-type"" shape that when viewed in 2 dimensions is both horizontally and vertically symmetrical around its center... with its 3-dimensional ""counterpart"" being radially symmetric around its vertical axis.  I have chosen the lemniscate of Bernoulli: We imagine the ""top"" half is ""full"" of ""sand"" -- or some imaginary substance that has a perfectly level surface. Anyway, given that some percentage of the sand, P, has fallen to the bottom, how can we calculate the height of that sand as a function of P? So that is the basic question.  It's been years since the calculus days (assuming the answer can't be found an easier way), and I have spend days getting extremely confused by this. My attempt at an answer and the reasoning for it: Apparently it is true that it makes no difference if we use a 2 or 3-dimensional world in which to make this calculation (I have verified this with a previous question ). Further, since the shape is symmetrical, we can literally, cut this problem in half by plotting half of the lemniscate in the following way (in fact, we probably only need a quarter of it): Setting a = 1: Because we have cut the problem in half, for our purposes now, the the total amount of sand is equal to one quarter the area of the original lemniscate -- the area under the curve of the above function on the interval (0,sqrt(2)): This area = 0.5 because the area of a lemniscate is: Area squared = 2 * a squared. (and a=1 as established already). So let's say some amount of sand has fallen: Now, we should be able to find the height, s, of that percent, P, of the sand in the bottom half... by solving for the upper limit of integration of: Where f(x) is the ""y ="" function shown earlier. However I'm stuck here because I can't find the integral of that function, presumably because it's not possible?  I tried using integral-calculator.com and it says it's not possible. To summarize, my general approach (for any shape) is: Establish a ""f(x)"" function representing half of the shape in its horizontal orientation Calculate the ""total area of sand"" For a given percentage of sand that has fallen, solve for the height of sand by using a limit of integration. More importantly, though, I'm sure there is a better way to do this whole problem ... maybe an ultra-simple geometric way exists for the lemniscate due to its unique properties?","EDIT: I have made an important correction to the ""previous question"" link below... it was accidentally pointing to an unrelated question before. I would also like to emphasize that I welcome any completely different approach to the question (solving for height as a function of ""P"") -- not just a correction to/extension of my potentially flawed approach... the simpler the better, of course. Also, I'm realizing that it's unclear whether I'm asking for a general solution for any shape that conforms to the described properties, or whether I'm asking for a specific, perhaps more elegant, solution specifically for the lemniscate of Bernoulli... or thirdly, for help completing my specific attempt at a solution.  I am interested in all those things, so, honestly I don't really know what to do about that other than to break the question up into multiple separate questions.  Hopefully it's OK as is. Let's say we have an hourglass/""container-type"" shape that when viewed in 2 dimensions is both horizontally and vertically symmetrical around its center... with its 3-dimensional ""counterpart"" being radially symmetric around its vertical axis.  I have chosen the lemniscate of Bernoulli: We imagine the ""top"" half is ""full"" of ""sand"" -- or some imaginary substance that has a perfectly level surface. Anyway, given that some percentage of the sand, P, has fallen to the bottom, how can we calculate the height of that sand as a function of P? So that is the basic question.  It's been years since the calculus days (assuming the answer can't be found an easier way), and I have spend days getting extremely confused by this. My attempt at an answer and the reasoning for it: Apparently it is true that it makes no difference if we use a 2 or 3-dimensional world in which to make this calculation (I have verified this with a previous question ). Further, since the shape is symmetrical, we can literally, cut this problem in half by plotting half of the lemniscate in the following way (in fact, we probably only need a quarter of it): Setting a = 1: Because we have cut the problem in half, for our purposes now, the the total amount of sand is equal to one quarter the area of the original lemniscate -- the area under the curve of the above function on the interval (0,sqrt(2)): This area = 0.5 because the area of a lemniscate is: Area squared = 2 * a squared. (and a=1 as established already). So let's say some amount of sand has fallen: Now, we should be able to find the height, s, of that percent, P, of the sand in the bottom half... by solving for the upper limit of integration of: Where f(x) is the ""y ="" function shown earlier. However I'm stuck here because I can't find the integral of that function, presumably because it's not possible?  I tried using integral-calculator.com and it says it's not possible. To summarize, my general approach (for any shape) is: Establish a ""f(x)"" function representing half of the shape in its horizontal orientation Calculate the ""total area of sand"" For a given percentage of sand that has fallen, solve for the height of sand by using a limit of integration. More importantly, though, I'm sure there is a better way to do this whole problem ... maybe an ultra-simple geometric way exists for the lemniscate due to its unique properties?",,['calculus']
38,Cubic addition and differentiablility,Cubic addition and differentiablility,,"It came to my thought that if we define $a\oplus b = (a^3 + b^3)^{\frac13}$ then $\Bbb R$ endowed with $\oplus$ and $\cdot$ the latter being the usual multiplication is a field, with usual $0$ and $1$ being the zero and the unity of this field respectively. Not really knowing what to do next with this, I decided to check how will new derivatives be different from the old ones. If I am not mistaken, it holds that $$ \lim_{h\to 0}\frac{f(x\oplus h)\ominus f(x)}{h} = \left(f'(x)\frac{f^2(x)}{x^2}\right)^{\frac13} $$ if $x\neq 0$ , while the direct evaluation of the limit when $x = 0$ gives that indeed it is infinite unless $f(0) = 0$ . In the latter case we get the new derivative (namely, the limit) being exactly the old derivative $f'(0)$ . This looks very odd to me: why $0$ should be any special point of this field? So my questions are: Did I make a mistake somewhere? If yes, please point me to it. If not, why in this new field differentiability at $0$ is something special?","It came to my thought that if we define then endowed with and the latter being the usual multiplication is a field, with usual and being the zero and the unity of this field respectively. Not really knowing what to do next with this, I decided to check how will new derivatives be different from the old ones. If I am not mistaken, it holds that if , while the direct evaluation of the limit when gives that indeed it is infinite unless . In the latter case we get the new derivative (namely, the limit) being exactly the old derivative . This looks very odd to me: why should be any special point of this field? So my questions are: Did I make a mistake somewhere? If yes, please point me to it. If not, why in this new field differentiability at is something special?","a\oplus b = (a^3 + b^3)^{\frac13} \Bbb R \oplus \cdot 0 1 
\lim_{h\to 0}\frac{f(x\oplus h)\ominus f(x)}{h} = \left(f'(x)\frac{f^2(x)}{x^2}\right)^{\frac13}
 x\neq 0 x = 0 f(0) = 0 f'(0) 0 0","['calculus', 'limits', 'derivatives']"
39,Continuous water-filling,Continuous water-filling,,"Disclaimer : I x-posted this question on OR-stackexchange as suggested in the comment below. I reposted it there since I did not receive any satisfactory answer on math-stackexchange up to now. Let $x\in\mathbb{R}^n$ be an optimization variable and $\alpha\in\mathbb{R}^n$ be a given $n$ -dimensional vector. The standard water-filling problem is formulated as \begin{equation} \begin{array}{ll} \underset{x}{\operatorname{minimize}} & -\displaystyle\sum_{i=1}^{n} \log \left(\alpha_{i}+x_{i}\right) \\ \text { subject to } & x \succeq 0, \quad \mathbf{1}^{T} x=1, \end{array} \end{equation} and it has a well-known solution (see Boyd & Vandenberghe page 245). I was thinking about the case in which we have continuous communication channel slots. Intuitively this may be thought of as the case when the sizes of the communication channels approach zero. How is it possible to generalize this optimization problem to this case? I believe that $\alpha$ and $x$ , which from now on we will denote by $\alpha(x)$ and $f(x)$ respectively, would in this case be continuous real function and the problem would be: \begin{equation} \begin{array}{ll} \underset{f \in \mathcal{F}}{\operatorname{minimize}} & -\displaystyle\int_{x} \log \left(\alpha(x)+f(x)\right)dx \\ \text { subject to } & f(x) \geq 0\; \forall x, \quad \int_x f(x)dx=1 \end{array} \end{equation} where $\mathcal{F}$ is a given class of functions (e.g. Hölder continuous, Lipschitz, etc). We can also assume that the domain of the functions $f(x)$ and $\alpha(x)$ is compact, i.e. $x\in\mathcal{X}\subseteq \mathbb{R}$ , with $\mathcal{X}$ a compact subspace of $\mathbb{R}$ . Is this the right path to follow? Do you have any other on how to solve these types of problems? It looks it is related to the calculus of variations , but I have never seen these types of problems and I have no idea how to solve them. Thanks!","Disclaimer : I x-posted this question on OR-stackexchange as suggested in the comment below. I reposted it there since I did not receive any satisfactory answer on math-stackexchange up to now. Let be an optimization variable and be a given -dimensional vector. The standard water-filling problem is formulated as and it has a well-known solution (see Boyd & Vandenberghe page 245). I was thinking about the case in which we have continuous communication channel slots. Intuitively this may be thought of as the case when the sizes of the communication channels approach zero. How is it possible to generalize this optimization problem to this case? I believe that and , which from now on we will denote by and respectively, would in this case be continuous real function and the problem would be: where is a given class of functions (e.g. Hölder continuous, Lipschitz, etc). We can also assume that the domain of the functions and is compact, i.e. , with a compact subspace of . Is this the right path to follow? Do you have any other on how to solve these types of problems? It looks it is related to the calculus of variations , but I have never seen these types of problems and I have no idea how to solve them. Thanks!","x\in\mathbb{R}^n \alpha\in\mathbb{R}^n n \begin{equation}
\begin{array}{ll}
\underset{x}{\operatorname{minimize}} & -\displaystyle\sum_{i=1}^{n} \log \left(\alpha_{i}+x_{i}\right) \\
\text { subject to } & x \succeq 0, \quad \mathbf{1}^{T} x=1,
\end{array}
\end{equation} \alpha x \alpha(x) f(x) \begin{equation}
\begin{array}{ll}
\underset{f \in \mathcal{F}}{\operatorname{minimize}} & -\displaystyle\int_{x} \log \left(\alpha(x)+f(x)\right)dx \\
\text { subject to } & f(x) \geq 0\; \forall x, \quad \int_x f(x)dx=1
\end{array}
\end{equation} \mathcal{F} f(x) \alpha(x) x\in\mathcal{X}\subseteq \mathbb{R} \mathcal{X} \mathbb{R}","['calculus', 'optimization', 'convex-optimization', 'calculus-of-variations']"
40,"Why does $\int_0^R 2 \pi r \,\mathrm d r$ give the area of a circle?",Why does  give the area of a circle?,"\int_0^R 2 \pi r \,\mathrm d r","There's a method of computing the area of a circle by dividing it in concentric rings with infinitesimal width. Let $R$ be the radius of the circle and $r$ be the radius of the rings. The area of the circle is $$\int_0^R 2 \pi r \,\mathrm d r$$ My questions are: I do not understand, though, how to justify the $2 \pi r \,\mathrm d r$ approximation for the area of each ring. Its actual area would be $$\pi (r + \mathrm d r)^2 - \pi r^2 = 2 \pi r \,\mathrm d r + \pi \left( \mathrm d r \right)^2$$ right? Could I use this more precise formula if I wanted to? How? The area on the integral above looks more like the lateral area of a cylinder of height $\mathrm d r$ , which is different from the actual area between two concentric circles. So why does that work? Would the lateral area of a truncated cone (which seems to be the intermediate between the ring area and the cylinder lateral area) also work as an approximation? Also, how do you come up with such an idea for an approximation that makes the calculation so beautifully simple (i.e. adopting the lateral area of a cylinder as the area of the rings)? It is considered a trivial integral, but there is a huge and mostly ignored step to be taken there.","There's a method of computing the area of a circle by dividing it in concentric rings with infinitesimal width. Let be the radius of the circle and be the radius of the rings. The area of the circle is My questions are: I do not understand, though, how to justify the approximation for the area of each ring. Its actual area would be right? Could I use this more precise formula if I wanted to? How? The area on the integral above looks more like the lateral area of a cylinder of height , which is different from the actual area between two concentric circles. So why does that work? Would the lateral area of a truncated cone (which seems to be the intermediate between the ring area and the cylinder lateral area) also work as an approximation? Also, how do you come up with such an idea for an approximation that makes the calculation so beautifully simple (i.e. adopting the lateral area of a cylinder as the area of the rings)? It is considered a trivial integral, but there is a huge and mostly ignored step to be taken there.","R r \int_0^R 2 \pi r \,\mathrm d r 2 \pi r \,\mathrm d r \pi (r + \mathrm d r)^2 - \pi r^2 = 2 \pi r \,\mathrm d r + \pi \left( \mathrm d r \right)^2 \mathrm d r","['calculus', 'integration', 'definite-integrals', 'area', 'polar-coordinates']"
41,Proving $\int_0^\infty\frac{e^x-1}{x(e^x+1)^2}dx=6\ln A-\frac12-\frac16\ln2-\frac12\ln\pi$,Proving,\int_0^\infty\frac{e^x-1}{x(e^x+1)^2}dx=6\ln A-\frac12-\frac16\ln2-\frac12\ln\pi,"I want to prove $$I=\int_0^\infty\frac{e^x-1}{x(e^x+1)^2}dx=6\ln A-\frac12-\frac16\ln2-\frac12\ln\pi$$ where $A$ denotes the Glaisher's constant. My attempt: Knowing that $$\int_0^\infty\frac{e^{-ax}-e^{-bx}}{x}dx=\ln b-\ln a$$ So I tried $$I=\int_0^\infty\frac{e^x-1}x\sum_{n=0}^\infty(-1)^{n+1}ne^{-(n+1)x}dx$$ But unfortunately $f_n(x)=(-1)^{n+1}ne^{-(n+1)x}$ does't uniform converge in $[0,+\infty)$ and I can't go further. (I also noticed the relation between integrals containing $e^x\pm1$ and $\zeta$ function. $\zeta'(-1)=\frac1{12}-\ln A$. So I added ""riemann-zeta"" in the tags)","I want to prove $$I=\int_0^\infty\frac{e^x-1}{x(e^x+1)^2}dx=6\ln A-\frac12-\frac16\ln2-\frac12\ln\pi$$ where $A$ denotes the Glaisher's constant. My attempt: Knowing that $$\int_0^\infty\frac{e^{-ax}-e^{-bx}}{x}dx=\ln b-\ln a$$ So I tried $$I=\int_0^\infty\frac{e^x-1}x\sum_{n=0}^\infty(-1)^{n+1}ne^{-(n+1)x}dx$$ But unfortunately $f_n(x)=(-1)^{n+1}ne^{-(n+1)x}$ does't uniform converge in $[0,+\infty)$ and I can't go further. (I also noticed the relation between integrals containing $e^x\pm1$ and $\zeta$ function. $\zeta'(-1)=\frac1{12}-\ln A$. So I added ""riemann-zeta"" in the tags)",,"['calculus', 'integration', 'definite-integrals', 'riemann-zeta']"
42,"How is it ""easily checked"" that $[1-s(\cos\theta + i \sin \theta)] \sum_{n=0}^\infty s^n [\cos(n\theta)+i \sin(n\theta)] = 1$","How is it ""easily checked"" that",[1-s(\cos\theta + i \sin \theta)] \sum_{n=0}^\infty s^n [\cos(n\theta)+i \sin(n\theta)] = 1,"This is from a derivation of de Moivre's theorem on p.148 of Grimmett & Stirzaker's Probability and Random Processes . The setup: The sequence $a_n = (\cos \theta + i \sin \theta)^n$ has generating function   $$G_a(s) = \sum_{n=0}^\infty \left[ s(\cos\theta + i \sin\theta) \right]^n = \frac{1}{1 - s(\cos\theta + i \sin\theta)}$$   if $|s| < 1$; here $i = \sqrt{-1}$. The part that I don't follow: It is easily checked by examining the coefficient of $s^n$ that $$\left[1-s(\cos\theta + i \sin \theta) \right] \sum_{n=0}^\infty s^n \left[\cos(n\theta)+i \sin(n\theta) \right] = 1 $$ when $|s| < 1$. The rest is clear to me, but in case you're interested: Thus $$\sum_{n=0}^\infty s^n \left[\cos(n\theta)+i \sin(n\theta) \right] = \frac{1}{1-s(\cos\theta + i \sin \theta)}$$ if $|s| < 1$. Equating the coefficients of $s_n$ we obtain the well-known fact that $\cos(n\theta)+i \sin(n\theta) = (\cos\theta + i \sin\theta)^n$. Thanks for any help!","This is from a derivation of de Moivre's theorem on p.148 of Grimmett & Stirzaker's Probability and Random Processes . The setup: The sequence $a_n = (\cos \theta + i \sin \theta)^n$ has generating function   $$G_a(s) = \sum_{n=0}^\infty \left[ s(\cos\theta + i \sin\theta) \right]^n = \frac{1}{1 - s(\cos\theta + i \sin\theta)}$$   if $|s| < 1$; here $i = \sqrt{-1}$. The part that I don't follow: It is easily checked by examining the coefficient of $s^n$ that $$\left[1-s(\cos\theta + i \sin \theta) \right] \sum_{n=0}^\infty s^n \left[\cos(n\theta)+i \sin(n\theta) \right] = 1 $$ when $|s| < 1$. The rest is clear to me, but in case you're interested: Thus $$\sum_{n=0}^\infty s^n \left[\cos(n\theta)+i \sin(n\theta) \right] = \frac{1}{1-s(\cos\theta + i \sin \theta)}$$ if $|s| < 1$. Equating the coefficients of $s_n$ we obtain the well-known fact that $\cos(n\theta)+i \sin(n\theta) = (\cos\theta + i \sin\theta)^n$. Thanks for any help!",,"['calculus', 'complex-analysis', 'probability-theory', 'generating-functions']"
43,Show that $\int_0^1\ln(-\ln{x})\cdot{\mathrm dx\over 1+x^2}=-\sum\limits_{n=0}^\infty{1\over 2n+1}\cdot{2\pi\over e^{\pi(2n+1)}+1}$ and evaluate it,Show that  and evaluate it,\int_0^1\ln(-\ln{x})\cdot{\mathrm dx\over 1+x^2}=-\sum\limits_{n=0}^\infty{1\over 2n+1}\cdot{2\pi\over e^{\pi(2n+1)}+1},"Considering this integral and sum are equal, how can we show that and evaluate its closed form? $$\int_{0}^{1}\ln{(-\ln{x})}\cdot{\mathrm dx\over 1+x^2}=-\sum_{n=0}^{\infty}{1\over 2n+1}\cdot{2\pi\over e^{\pi(2n+1)}+1}=I_S\tag1$$ Note: $$\int_{0}^{1}{\mathrm dx\over 1+x^2}=\sum_{n=0}^{\infty}{(-1)^n\over 2n+1}\tag2$$ An attempt: $u=-\ln{x}$ then $xdu=-dx$ $(1)$ becomes $$\int_{0}^{\infty}\ln{u}\cdot{\mathrm du\over e^u+e^{-u}}={1\over 2}\int_{0}^{\infty}\ln{u}\cdot{\mathrm du\over \cosh{u}}\tag3$$ $(3)$ , seem complicate.","Considering this integral and sum are equal, how can we show that and evaluate its closed form? Note: An attempt: then becomes , seem complicate.",\int_{0}^{1}\ln{(-\ln{x})}\cdot{\mathrm dx\over 1+x^2}=-\sum_{n=0}^{\infty}{1\over 2n+1}\cdot{2\pi\over e^{\pi(2n+1)}+1}=I_S\tag1 \int_{0}^{1}{\mathrm dx\over 1+x^2}=\sum_{n=0}^{\infty}{(-1)^n\over 2n+1}\tag2 u=-\ln{x} xdu=-dx (1) \int_{0}^{\infty}\ln{u}\cdot{\mathrm du\over e^u+e^{-u}}={1\over 2}\int_{0}^{\infty}\ln{u}\cdot{\mathrm du\over \cosh{u}}\tag3 (3),"['calculus', 'integration', 'sequences-and-series', 'definite-integrals']"
44,Show us how to prove that $\int_{0}^{\infty}{\sin(e^{-\gamma}x)}\cdot{\ln{x}\over x}\mathrm dx=0$,Show us how to prove that,\int_{0}^{\infty}{\sin(e^{-\gamma}x)}\cdot{\ln{x}\over x}\mathrm dx=0,I saw these two intgerals and would like to know if they are correct. $$\int_{0}^{\infty}{\sin(e^{-\gamma}x)}\cdot{\ln{x}\over x}\mathrm dx=0\tag1$$ $$\int_{0}^{\infty}{\sin\left(\sqrt{x}^{\sqrt{2}}\right)}\cdot{\ln{x}\over x}\mathrm dx=-\pi\gamma\tag2$$ Where $\gamma$ is the Euler Mascheroni constant Here I ignored the limits I apply sub: to $(1)$ $u=\ln{x}$ $xdu=dx$ $$I = \int{ue^{-u}\sin(e^{u-\gamma})}du\tag3$$ Apply integration by parts to $(3)$ $$\int{ue^{-u}}du=-e^{-u}(1+u)$$ $$I={-e^{-u}(1+u)\sin(e^{u-\gamma})}+\int{e^{-u}(1+u)\cos(e^{u-\gamma})}du$$ Encounter more harder than before. Please show us how to prove $(1)$ and $(2)$,I saw these two intgerals and would like to know if they are correct. Where is the Euler Mascheroni constant Here I ignored the limits I apply sub: to Apply integration by parts to Encounter more harder than before. Please show us how to prove and,\int_{0}^{\infty}{\sin(e^{-\gamma}x)}\cdot{\ln{x}\over x}\mathrm dx=0\tag1 \int_{0}^{\infty}{\sin\left(\sqrt{x}^{\sqrt{2}}\right)}\cdot{\ln{x}\over x}\mathrm dx=-\pi\gamma\tag2 \gamma (1) u=\ln{x} xdu=dx I = \int{ue^{-u}\sin(e^{u-\gamma})}du\tag3 (3) \int{ue^{-u}}du=-e^{-u}(1+u) I={-e^{-u}(1+u)\sin(e^{u-\gamma})}+\int{e^{-u}(1+u)\cos(e^{u-\gamma})}du (1) (2),"['calculus', 'integration']"
45,"How to show an infinite number of algebraic numbers $\alpha$ and $\beta$ for $_2F_1\left(\frac14,\frac14;\frac34;-\alpha\right)=\beta\,$?",How to show an infinite number of algebraic numbers  and  for ?,"\alpha \beta _2F_1\left(\frac14,\frac14;\frac34;-\alpha\right)=\beta\,","( Note : This is the case $a=\frac14$ of ${_2F_1\left(a ,a ;a +\tfrac12;-u\right)}=2^{a}\frac{\Gamma\big(a+\tfrac12\big)}{\sqrt\pi\,\Gamma(a)}\int_0^\infty\frac{dx}{(1+2u+\cosh x)^a}.\,$ There is also $a=\frac13$ and $a=\frac16$ .) In a post , Reshetnikov considered some integrals and their equivalent forms, $$\frac{1}{2\sqrt2\,K(k_1)}\,\int_0^1 \frac{dx}{\sqrt{1-x}\,\sqrt[4]{x^3+\color{blue}{3}x^4}}=\,_2F_1\big(\tfrac{1}{4},\tfrac{1}{4};\tfrac{3}{4};-\color{blue}3\big) = \frac{2}{3^{3/4}}\tag1$$ $$\frac{1}{2\sqrt2\,K(k_1)}\,\int_0^1 \frac{dx}{\sqrt{1-x}\,\sqrt[4]{x^3+\color{blue}{80}x^4}}=\,_2F_1\big(\tfrac{1}{4},\tfrac{1}{4};\tfrac{3}{4};-\color{blue}{80}\big) = \frac35\tag2$$ Just like for the case $a=\tfrac13$ , we postulate these are just the first of an infinite family of algebraic numbers $\alpha$ and $\beta$ such that, $$_2F_1\left(\frac14,\frac14;\frac34;-\alpha\right)=\beta\tag3$$ The table below summarizes results and show some quartic trends, $$\begin{array}{|c|c|c|c|c|} \hline p&\tau&\alpha(\tau)&\beta(\tau)&\text{Deg}\\ \hline 3&\frac{1+3\sqrt{-1}}2&4\cdot1^4-1=\color{blue}{3}& \large\frac2{3^{3/4}}  &1\\ 5&\frac{1+5\sqrt{-1}}2&3^4-1=\color{blue}{80}& \large\frac35  &1\\ 7&\frac{1+7\sqrt{-1}}2&4(2+\sqrt7)^4-1& \large\frac4{7^{7/8}} \frac1{(8+3\sqrt7)^{1/4}} &2\\ 11&\frac{1+11\sqrt{-1}}2&4u^4 - 1 &\large \frac6{11^{11/12}}\Big(\frac{11^{2/3}-(14-6\sqrt3)^{1/3}-(14+6\sqrt3)^{1/3}}{3}\Big) &3 \\ 13&\frac{1+13\sqrt{-1}}2& v^4-1 &\large \frac7{13^{2/3}}\Big(\frac{13^{-1/3}+(-2+6\sqrt3)^{1/3}-(2+6\sqrt3)^{1/3}}{3}\Big) &3 \\ 17&\frac{1+17\sqrt{-1}}2& x_1^4 - 1 & \large \frac9{17}x_2 &4 \\ 19&\frac{1+19\sqrt{-1}}2& 4y_1^4 - 1 & \large \frac{10}{19}y_2^{1/4} &5 \\ \hline \end{array}$$ and fundamental unit $U_7=8+3\sqrt7$ , with $u,v$ as the real root of the cubics, $$u^3 - 23u^2 + 15u - 9=0\\v^3 - 67v^2 - 159v - 99=0$$ and $x_i$ as  roots of quartics, and $y_i$ as  roots of quintics, and so on. $\text{Deg}$ is degree of $\alpha(\tau)$ and $\beta^4(\tau)$ . Conjecture: Let $\tau = \frac{1+p\sqrt{-1}}{2}$ . To find $\alpha$ , define the eta quotient $\displaystyle\lambda=\frac{\sqrt2\,\eta(2\tau)}{\zeta_{48}\,\eta(\tau)}$ with $\zeta_{48} = e^{2\pi i/48}$ . Then, like the case $a=\tfrac13$ , $\alpha$ also is a quadratic of similar form, $$16\cdot64\,\alpha(1+\alpha)=\left( \lambda^{12} -64\, \lambda^{-12} \right)^2$$ with analogous root, $$\alpha= \frac1{4\sqrt{64}}\big(\lambda^6-\sqrt{64}\,\lambda^{-6}\big)^2\tag4$$ And if $p=4k\pm1$ is a prime, then $\alpha$ and $\beta^4$ of $(3)$ are algebraic numbers of degree $k$ . Questions: How do we prove the conjecture? (And the analogous one in the other post ?) Is there a third family of this group with infinitely many algebraic numbers $\alpha_3$ and $\beta_3$ ? $\color{green}{Update}:$ It seems there is a third family. The obvious choices are $a=\frac12$ and $a=\frac16$ . For the latter, and using a better search method, I found, $$_2F_1\Big(\frac{1}{6},\frac{1}{6};\frac{2}{3};-\frac{125}3\Big) = \frac{2}{3^{5/6}}$$ See case $a=\frac16$ here .","( Note : This is the case of There is also and .) In a post , Reshetnikov considered some integrals and their equivalent forms, Just like for the case , we postulate these are just the first of an infinite family of algebraic numbers and such that, The table below summarizes results and show some quartic trends, and fundamental unit , with as the real root of the cubics, and as  roots of quartics, and as  roots of quintics, and so on. is degree of and . Conjecture: Let . To find , define the eta quotient with . Then, like the case , also is a quadratic of similar form, with analogous root, And if is a prime, then and of are algebraic numbers of degree . Questions: How do we prove the conjecture? (And the analogous one in the other post ?) Is there a third family of this group with infinitely many algebraic numbers and ? It seems there is a third family. The obvious choices are and . For the latter, and using a better search method, I found, See case here .","a=\frac14 {_2F_1\left(a ,a ;a +\tfrac12;-u\right)}=2^{a}\frac{\Gamma\big(a+\tfrac12\big)}{\sqrt\pi\,\Gamma(a)}\int_0^\infty\frac{dx}{(1+2u+\cosh x)^a}.\, a=\frac13 a=\frac16 \frac{1}{2\sqrt2\,K(k_1)}\,\int_0^1 \frac{dx}{\sqrt{1-x}\,\sqrt[4]{x^3+\color{blue}{3}x^4}}=\,_2F_1\big(\tfrac{1}{4},\tfrac{1}{4};\tfrac{3}{4};-\color{blue}3\big) = \frac{2}{3^{3/4}}\tag1 \frac{1}{2\sqrt2\,K(k_1)}\,\int_0^1 \frac{dx}{\sqrt{1-x}\,\sqrt[4]{x^3+\color{blue}{80}x^4}}=\,_2F_1\big(\tfrac{1}{4},\tfrac{1}{4};\tfrac{3}{4};-\color{blue}{80}\big) = \frac35\tag2 a=\tfrac13 \alpha \beta _2F_1\left(\frac14,\frac14;\frac34;-\alpha\right)=\beta\tag3 \begin{array}{|c|c|c|c|c|}
\hline
p&\tau&\alpha(\tau)&\beta(\tau)&\text{Deg}\\
\hline
3&\frac{1+3\sqrt{-1}}2&4\cdot1^4-1=\color{blue}{3}& \large\frac2{3^{3/4}}  &1\\
5&\frac{1+5\sqrt{-1}}2&3^4-1=\color{blue}{80}& \large\frac35  &1\\
7&\frac{1+7\sqrt{-1}}2&4(2+\sqrt7)^4-1& \large\frac4{7^{7/8}} \frac1{(8+3\sqrt7)^{1/4}} &2\\
11&\frac{1+11\sqrt{-1}}2&4u^4 - 1 &\large \frac6{11^{11/12}}\Big(\frac{11^{2/3}-(14-6\sqrt3)^{1/3}-(14+6\sqrt3)^{1/3}}{3}\Big) &3 \\
13&\frac{1+13\sqrt{-1}}2& v^4-1 &\large \frac7{13^{2/3}}\Big(\frac{13^{-1/3}+(-2+6\sqrt3)^{1/3}-(2+6\sqrt3)^{1/3}}{3}\Big) &3 \\
17&\frac{1+17\sqrt{-1}}2& x_1^4 - 1 & \large \frac9{17}x_2 &4 \\
19&\frac{1+19\sqrt{-1}}2& 4y_1^4 - 1 & \large \frac{10}{19}y_2^{1/4} &5 \\
\hline
\end{array} U_7=8+3\sqrt7 u,v u^3 - 23u^2 + 15u - 9=0\\v^3 - 67v^2 - 159v - 99=0 x_i y_i \text{Deg} \alpha(\tau) \beta^4(\tau) \tau = \frac{1+p\sqrt{-1}}{2} \alpha \displaystyle\lambda=\frac{\sqrt2\,\eta(2\tau)}{\zeta_{48}\,\eta(\tau)} \zeta_{48} = e^{2\pi i/48} a=\tfrac13 \alpha 16\cdot64\,\alpha(1+\alpha)=\left( \lambda^{12} -64\, \lambda^{-12} \right)^2 \alpha= \frac1{4\sqrt{64}}\big(\lambda^6-\sqrt{64}\,\lambda^{-6}\big)^2\tag4 p=4k\pm1 \alpha \beta^4 (3) k \alpha_3 \beta_3 \color{green}{Update}: a=\frac12 a=\frac16 _2F_1\Big(\frac{1}{6},\frac{1}{6};\frac{2}{3};-\frac{125}3\Big) = \frac{2}{3^{5/6}} a=\frac16","['calculus', 'definite-integrals', 'radicals', 'hypergeometric-function', 'conjectures']"
46,"Simple proof of area of ""rectangled"" circle","Simple proof of area of ""rectangled"" circle",,"Here is a simple problem which I would occasionally assign to my precalculus students and to my calculus students. The precalculus students always found a simpler answer. Sometimes it is possible to know too much. :) Construct a simple proof that the area of the shaded region of the circle is $$ \frac{1}{2}\pi r^2+2ab $$ Caution! Mousing over the yellow region will reveal the answer. Bonus: For those who got the answer or who revealed the answer, what does the dashed line represent? What is its equation?","Here is a simple problem which I would occasionally assign to my precalculus students and to my calculus students. The precalculus students always found a simpler answer. Sometimes it is possible to know too much. :) Construct a simple proof that the area of the shaded region of the circle is Caution! Mousing over the yellow region will reveal the answer. Bonus: For those who got the answer or who revealed the answer, what does the dashed line represent? What is its equation?", \frac{1}{2}\pi r^2+2ab ,"['calculus', 'algebra-precalculus', 'circles']"
47,Find all $f:\mathbb {R} \rightarrow \mathbb {R}$ where $f(f(x))=f'(x)f(x)+c$,Find all  where,f:\mathbb {R} \rightarrow \mathbb {R} f(f(x))=f'(x)f(x)+c,"Recently, while studying calculus, I have come across multiples problems which asked the following: If $f(x)$ is a polynomial, find all $f(x)$ that $f(f(x))=f'(x)f(x)+c$, where $c$ is a constant. This problem can be solved as so: If $deg(f(x))=n$, the upper equation implies that $n^2=2n-1$, or that $n=1$. This implies that $f(x)=ax+b$, and it is just a matter of calculation from here. But how does one find all $f:\mathbb {R} \rightarrow \mathbb {R}$ where $f(f(x))=f'(x)f(x)+c$? I am asking for solutions whe $f(x)$ is not necessarily a polynomial . Because of my above method, I thought that $f'(x)$ would be a constant. However, I was not able to prove this. Differentiating both sides gave me that $(f'(f(x))-f'(x))(f'(x))=f''(x)f(x)$. This proved no help at all. Since I am young, please use methods that are comprehensible to a high-school student. Any help would be appreciated.","Recently, while studying calculus, I have come across multiples problems which asked the following: If $f(x)$ is a polynomial, find all $f(x)$ that $f(f(x))=f'(x)f(x)+c$, where $c$ is a constant. This problem can be solved as so: If $deg(f(x))=n$, the upper equation implies that $n^2=2n-1$, or that $n=1$. This implies that $f(x)=ax+b$, and it is just a matter of calculation from here. But how does one find all $f:\mathbb {R} \rightarrow \mathbb {R}$ where $f(f(x))=f'(x)f(x)+c$? I am asking for solutions whe $f(x)$ is not necessarily a polynomial . Because of my above method, I thought that $f'(x)$ would be a constant. However, I was not able to prove this. Differentiating both sides gave me that $(f'(f(x))-f'(x))(f'(x))=f''(x)f(x)$. This proved no help at all. Since I am young, please use methods that are comprehensible to a high-school student. Any help would be appreciated.",,"['calculus', 'derivatives', 'functional-equations']"
48,Integrate $ \int_{0}^{1} \frac{\tan^{-1}(x)}{\sqrt{1-x^2}} dx$ without using Hyperbolic Functions,Integrate  without using Hyperbolic Functions, \int_{0}^{1} \frac{\tan^{-1}(x)}{\sqrt{1-x^2}} dx,"I originally had to solve this Integral: $$ \int_{0}^{1} \frac{\tan^{-1}(x)}{\sqrt{1-x^2}} dx$$ It was suggested to me that I introduce the parameter $a$ and then try Differentiation Under the Integral Sign. I thus rewrote the Integral as  $$ I(a)=\int_{0}^{1} \frac{\tan^{-1}(ax)}{\sqrt{1-x^2}} dx$$ $$\Longrightarrow I'(a)= \int_0^1\dfrac{y}{(1+(ay)^2)(\sqrt{1-y^2})}dy$$ I then thought I might try Integration By Parts with $\sqrt{1-y^2}$ in the denominator as the derivative of $\sin^{-1}(y)$. However, I don't understand how this would help. My friend suggested using hyperbolic functions but I don't know anything about them. $$$$ Would somebody please be so kind as to show me how to solve this problem? Many, many thanks in advance!","I originally had to solve this Integral: $$ \int_{0}^{1} \frac{\tan^{-1}(x)}{\sqrt{1-x^2}} dx$$ It was suggested to me that I introduce the parameter $a$ and then try Differentiation Under the Integral Sign. I thus rewrote the Integral as  $$ I(a)=\int_{0}^{1} \frac{\tan^{-1}(ax)}{\sqrt{1-x^2}} dx$$ $$\Longrightarrow I'(a)= \int_0^1\dfrac{y}{(1+(ay)^2)(\sqrt{1-y^2})}dy$$ I then thought I might try Integration By Parts with $\sqrt{1-y^2}$ in the denominator as the derivative of $\sin^{-1}(y)$. However, I don't understand how this would help. My friend suggested using hyperbolic functions but I don't know anything about them. $$$$ Would somebody please be so kind as to show me how to solve this problem? Many, many thanks in advance!",,"['calculus', 'integration', 'definite-integrals']"
49,Moment method for weak convergence,Moment method for weak convergence,,"Say, we have a sequence of probability distributions $\mu_n$ on $\mathbb R$, that are uniformly subgaussian in the sense that $$\mu_n(\mathbb R\setminus[-R,R])\leq Ce^{-CR^2}$$ for some positive constant $C$. Assume that all moments of these distributions exist and that they converge to those of a standard normal distribution, i.e., $$\lim_{n\to\infty}\int x^k~d\mu_n(x)=\begin{cases}(k-1)!!&\text{if }k\text{ is even}\\ 0 &\text{else}\end{cases}.$$ Then $\mu_n$ converges weakly to a standard-normal distribution $\mu$, i.e., $$\lim_{n\to\infty} \int f~d\mu_n=\int f~d\mu$$ for all bounded continuous $f$. The above claim is clearly true: The uniform subgaussian hypothesis implies that the $k$-th moments are uniformly bounded by $(Ck)^{k/2}$ and therefore the characteristic functions can be written as $$\int e^{itx}~d\mu_n(x)=\sum_{k=0}^\infty \frac{(it)^k}{k!}\int x^k~d\mu_n(x),$$ which converges point wise to the characteristic function of the normal distribution. The result now follows from Levy's Continuity Theorem. Question: Is there a way to prove the above statement directly? (I am concerned with something related where the measures are random themselves and I can't argue with the characteristic function) Thoughts: Fix $\epsilon>0$ and $f\in C_b(\mathbb R)$ and choose $\lambda<\infty$ such that $\mu_n(\mathbb R\setminus[-\lambda,\lambda])\leq \epsilon$. From the Weierstraß approximation theorem, we find a polynomial $p$ that approximates $f$ on the interval $[-\lambda,\lambda]$ up to an error of $\epsilon$. Thus we find  $$\big\lvert\int f~d\mu_n-\int f~d\mu\big\lvert\leq \int_{[-\lambda,\lambda]}\lvert f-p\lvert~d\mu_n+\big\lvert\int_{[-\lambda,\lambda]^c} f-p~d\mu_n\big\lvert+\big\lvert\int p~d(\mu_n-\mu)\big\lvert+\int \lvert p-f\lvert~d\mu.$$ The first term is bounded by $\epsilon$ and the third term converges to zero by assumption. The fourth term can be bound by $\epsilon$ and the integral outside the finite interval. Since $f$ is bounded we can estimate $\lvert f(x)-p(x)\lvert\leq c x^{2k}$ outside the interval for some $c,k\in\mathbb N$. It would therefore remain to show that $$\int_{[-\lambda,\lambda]^c} x^{2k}~d(\mu_n+\mu)$$ is small. Does this somehow follow from the subgaussian assumption (which wasn't really used till now)?","Say, we have a sequence of probability distributions $\mu_n$ on $\mathbb R$, that are uniformly subgaussian in the sense that $$\mu_n(\mathbb R\setminus[-R,R])\leq Ce^{-CR^2}$$ for some positive constant $C$. Assume that all moments of these distributions exist and that they converge to those of a standard normal distribution, i.e., $$\lim_{n\to\infty}\int x^k~d\mu_n(x)=\begin{cases}(k-1)!!&\text{if }k\text{ is even}\\ 0 &\text{else}\end{cases}.$$ Then $\mu_n$ converges weakly to a standard-normal distribution $\mu$, i.e., $$\lim_{n\to\infty} \int f~d\mu_n=\int f~d\mu$$ for all bounded continuous $f$. The above claim is clearly true: The uniform subgaussian hypothesis implies that the $k$-th moments are uniformly bounded by $(Ck)^{k/2}$ and therefore the characteristic functions can be written as $$\int e^{itx}~d\mu_n(x)=\sum_{k=0}^\infty \frac{(it)^k}{k!}\int x^k~d\mu_n(x),$$ which converges point wise to the characteristic function of the normal distribution. The result now follows from Levy's Continuity Theorem. Question: Is there a way to prove the above statement directly? (I am concerned with something related where the measures are random themselves and I can't argue with the characteristic function) Thoughts: Fix $\epsilon>0$ and $f\in C_b(\mathbb R)$ and choose $\lambda<\infty$ such that $\mu_n(\mathbb R\setminus[-\lambda,\lambda])\leq \epsilon$. From the Weierstraß approximation theorem, we find a polynomial $p$ that approximates $f$ on the interval $[-\lambda,\lambda]$ up to an error of $\epsilon$. Thus we find  $$\big\lvert\int f~d\mu_n-\int f~d\mu\big\lvert\leq \int_{[-\lambda,\lambda]}\lvert f-p\lvert~d\mu_n+\big\lvert\int_{[-\lambda,\lambda]^c} f-p~d\mu_n\big\lvert+\big\lvert\int p~d(\mu_n-\mu)\big\lvert+\int \lvert p-f\lvert~d\mu.$$ The first term is bounded by $\epsilon$ and the third term converges to zero by assumption. The fourth term can be bound by $\epsilon$ and the integral outside the finite interval. Since $f$ is bounded we can estimate $\lvert f(x)-p(x)\lvert\leq c x^{2k}$ outside the interval for some $c,k\in\mathbb N$. It would therefore remain to show that $$\int_{[-\lambda,\lambda]^c} x^{2k}~d(\mu_n+\mu)$$ is small. Does this somehow follow from the subgaussian assumption (which wasn't really used till now)?",,"['calculus', 'probability', 'probability-theory', 'probability-distributions', 'weak-convergence']"
50,"For which integers $1\leq{m}\leq{10}$ is it true that $\int_0^\pi{(\cos{x})(\cos{2x})\cdots(\cos{mx})}\,dx = 0$?",For which integers  is it true that ?,"1\leq{m}\leq{10} \int_0^\pi{(\cos{x})(\cos{2x})\cdots(\cos{mx})}\,dx = 0","I could only solve this problem via brute force, trying every value from $m = 1$ to $10$... What is the more efficient and proper method of approach? (Note: my method involved repeated usage of the product-to-sum trig identity to achieve a simplified result of the integrand; but is there a way to generalize this simplified result for all $m \in [1,10]$?) Thanks","I could only solve this problem via brute force, trying every value from $m = 1$ to $10$... What is the more efficient and proper method of approach? (Note: my method involved repeated usage of the product-to-sum trig identity to achieve a simplified result of the integrand; but is there a way to generalize this simplified result for all $m \in [1,10]$?) Thanks",,"['calculus', 'integration', 'trigonometry']"
51,Fractional derivative of exponential function,Fractional derivative of exponential function,,"With the $n$th order derivative ($n$ as a positive integer) of $e^{ax}$ given by $$D^{n}e^{ax}=a^ne^{ax},$$ is the generalized (or fractional) derivative the same? Does it apply for any arbitrary $\alpha$? That is $$D^{\alpha}e^{ax}=a^{\alpha}e^{ax}?$$ For example...$$D^{\frac{1}{2}}e^{2x}=2^{0.5}e^{2x}?$$ Do you know of a study or paper showing how the integer order derivative of an exponential function is generalized into a fractional order? Thank you.","With the $n$th order derivative ($n$ as a positive integer) of $e^{ax}$ given by $$D^{n}e^{ax}=a^ne^{ax},$$ is the generalized (or fractional) derivative the same? Does it apply for any arbitrary $\alpha$? That is $$D^{\alpha}e^{ax}=a^{\alpha}e^{ax}?$$ For example...$$D^{\frac{1}{2}}e^{2x}=2^{0.5}e^{2x}?$$ Do you know of a study or paper showing how the integer order derivative of an exponential function is generalized into a fractional order? Thank you.",,"['calculus', 'functions', 'exponential-function', 'fractional-calculus']"
52,"Newton's ""Famous Blunder""?","Newton's ""Famous Blunder""?",,"On page $225$ of Isaac Newton on Mathematical Certainty and Method by Niccolo Guicciardini (see here for a link ), I read In the following demonstration... Newton made a famous blunder... He wrote, ""Case 1. Any rectangle as $AB$ increased by a continual motion, when the halves of the moments ${1\over 2}a$ and ${1\over 2}b$, were lacking from the sides $A$ and $B$, was $A-{1\over 2}a$ multiplied by $B-{1\over 2}b$, or $AB-{1\over 2}aB-{1\over 2}bA+{1\over 4}ab$, and as soon as the sides $A$ and $B$ have been increased by the other halves of the moments; it comes out $A+{1\over 2}a$ multiplied by $B+{1\over 2}b$, or $AB+{1\over 2}aB+{1\over 2}bA+{1\over 4}ab$. Subtract the former rectangle from this rectangle, and there will remain the excess $aB+bA$. Therefore by the total increments $a$ and $b$ of the sides, there is generated the increment $aB+bA$ of the rectangle. Q.E.D. "" This reasoning seems perfectly fine to me $-$ what is the ""momentous mistake""?","On page $225$ of Isaac Newton on Mathematical Certainty and Method by Niccolo Guicciardini (see here for a link ), I read In the following demonstration... Newton made a famous blunder... He wrote, ""Case 1. Any rectangle as $AB$ increased by a continual motion, when the halves of the moments ${1\over 2}a$ and ${1\over 2}b$, were lacking from the sides $A$ and $B$, was $A-{1\over 2}a$ multiplied by $B-{1\over 2}b$, or $AB-{1\over 2}aB-{1\over 2}bA+{1\over 4}ab$, and as soon as the sides $A$ and $B$ have been increased by the other halves of the moments; it comes out $A+{1\over 2}a$ multiplied by $B+{1\over 2}b$, or $AB+{1\over 2}aB+{1\over 2}bA+{1\over 4}ab$. Subtract the former rectangle from this rectangle, and there will remain the excess $aB+bA$. Therefore by the total increments $a$ and $b$ of the sides, there is generated the increment $aB+bA$ of the rectangle. Q.E.D. "" This reasoning seems perfectly fine to me $-$ what is the ""momentous mistake""?",,['calculus']
53,Computing $\int {\dfrac{\csc^{2014}x-2014}{\cos^{2014}x} dx}$,Computing,\int {\dfrac{\csc^{2014}x-2014}{\cos^{2014}x} dx},"I don't know how to compute: $$\int {\dfrac{\csc^{2014}x-2014}{\cos^{2014}x} dx}$$ I have tried substituting $t=\tan ^{2} x$ but got nothing out of it. I know there's some trick involved, but can't figure it out. Also, how does one frame such questions involving numbers like the current year, next year or previous year? Is there a general theme to attack such problems?","I don't know how to compute: $$\int {\dfrac{\csc^{2014}x-2014}{\cos^{2014}x} dx}$$ I have tried substituting $t=\tan ^{2} x$ but got nothing out of it. I know there's some trick involved, but can't figure it out. Also, how does one frame such questions involving numbers like the current year, next year or previous year? Is there a general theme to attack such problems?",,"['calculus', 'integration', 'problem-solving', 'indefinite-integrals']"
54,How to estimate the limit $\lim_{x\to+\infty}\frac{\int_0^x|\sin(s)|ds}{x}?$,How to estimate the limit,\lim_{x\to+\infty}\frac{\int_0^x|\sin(s)|ds}{x}?,"I have come across the problem of estimation of the limit $$\lim_{x\to+\infty}\frac{\int_0^x|\sin(s)|ds}{x}.$$ Since it is easy to check that the method of l'Hopital's Rule is incapable of it. I have tried the following:  Since for every $x>\pi,$ there exist unique $n\in\mathbb{N}$ and $\theta\in[0, \pi),$ such that $$x=n\pi+\theta,$$ and $$x\to+\infty \Longleftrightarrow n\to\infty.$$ On account of the periodicity of the mapping $s\to |\sin(s)|,$  \begin{gather*} \begin{aligned} &\lim_{x\to+\infty}\frac{\int_0^x|\sin(s)|ds}{x}=\lim_{n\to\infty}\frac{\int_0^{n\pi+\theta}|\sin(s)|ds}{n\pi+\theta}=\lim_{n\to\infty}\frac{\int_0^{n\pi}|\sin(s)|ds+\int_{n\pi}^{n\pi+\theta}|\sin(s)|ds}{n\pi+\theta}\\ =&\lim_{n\to\infty}\frac{2n+\int_0^{\theta}|\sin(s)|ds}{n\pi+\theta}=\lim_{n\to\infty}\frac{2+\frac{1}{n}\cdot\int_0^{\theta}|\sin(s)|ds}{\pi+\frac{1}{n}\cdot\theta}\\ =&\frac{2}{\pi}. \end{aligned} \end{gather*} I am not very sure that my trial is sound. Can anyone help me to check my method, or find another  way to estimate this limit?","I have come across the problem of estimation of the limit $$\lim_{x\to+\infty}\frac{\int_0^x|\sin(s)|ds}{x}.$$ Since it is easy to check that the method of l'Hopital's Rule is incapable of it. I have tried the following:  Since for every $x>\pi,$ there exist unique $n\in\mathbb{N}$ and $\theta\in[0, \pi),$ such that $$x=n\pi+\theta,$$ and $$x\to+\infty \Longleftrightarrow n\to\infty.$$ On account of the periodicity of the mapping $s\to |\sin(s)|,$  \begin{gather*} \begin{aligned} &\lim_{x\to+\infty}\frac{\int_0^x|\sin(s)|ds}{x}=\lim_{n\to\infty}\frac{\int_0^{n\pi+\theta}|\sin(s)|ds}{n\pi+\theta}=\lim_{n\to\infty}\frac{\int_0^{n\pi}|\sin(s)|ds+\int_{n\pi}^{n\pi+\theta}|\sin(s)|ds}{n\pi+\theta}\\ =&\lim_{n\to\infty}\frac{2n+\int_0^{\theta}|\sin(s)|ds}{n\pi+\theta}=\lim_{n\to\infty}\frac{2+\frac{1}{n}\cdot\int_0^{\theta}|\sin(s)|ds}{\pi+\frac{1}{n}\cdot\theta}\\ =&\frac{2}{\pi}. \end{aligned} \end{gather*} I am not very sure that my trial is sound. Can anyone help me to check my method, or find another  way to estimate this limit?",,"['calculus', 'integration', 'limits']"
55,A Handwaving Proof of a Specific Existence and Uniqueness Theorem,A Handwaving Proof of a Specific Existence and Uniqueness Theorem,,"My problem is as follows: Given the second order homogeneous linear differential equation with constant coefficients   $$a\frac{d^2y}{dx^2}+b\frac{dy}{dx}+c\,y(x)=0,$$   is there a good heuristic that explains why the solution set is of the form $\{Ae^{\alpha x}+Be^{\beta x}\}$ or $\{Ae^{\alpha x}+Bxe^{\alpha x}\}$. The background is that I am teaching engineers the method of solving these equations but like everything else I like to give them a reason why the method works. I can explain why we might look for solutions of the form $e^{rx}$, why something like $Ae^{rx}$ will be a solution and why if $y_1$ and $y_2$ are solutions then so is $y_1+y_2$. I can explain the non-homogeneous case and why occasionally we have to look at test solutions of the form $xy_H$ --- where $y_H$ is a solution of the homogeneous equation. The problem occurs when I try and explain to them why the solutions have to be two dimensional and that we don't need three linearly independent solutions (in the homogeneous case). My best hand-waving argument thus far is that in a solution we will have to integrate twice somewhere and so we will end up with two constants of integration say $C_1$ and $C_2$ so our solution will be $$y_H=y(x,C_1,C_2)$$ but I have had to wave very hard indeed to turn this into $y_H=Ay_1+By_2$. These are not maths students but I still tried to make various bad arguments along the lines of the 'kernel' of the operator $\displaystyle D^2=\frac{d^2y}{dx^2}$ being two dimensional and that the addition of $bD$ and $cI$ distorts the 'kernel' but not the dimension of it (I wonder can this argument be made rigorous). Have any of ye any better ideas? I understand that we can show from the Uniqueness and Existence Equation that the solutions must have this form... the irony is that I am happy to sketch an argument of plausibility of that fact --- which is left without proof in most ODE classes --- but the journey from there to the conclusion, which is done in these classes, is beyond the scope and interest of this class. I fully expect a comment along the lines of they're engineers, who cares? I do!","My problem is as follows: Given the second order homogeneous linear differential equation with constant coefficients   $$a\frac{d^2y}{dx^2}+b\frac{dy}{dx}+c\,y(x)=0,$$   is there a good heuristic that explains why the solution set is of the form $\{Ae^{\alpha x}+Be^{\beta x}\}$ or $\{Ae^{\alpha x}+Bxe^{\alpha x}\}$. The background is that I am teaching engineers the method of solving these equations but like everything else I like to give them a reason why the method works. I can explain why we might look for solutions of the form $e^{rx}$, why something like $Ae^{rx}$ will be a solution and why if $y_1$ and $y_2$ are solutions then so is $y_1+y_2$. I can explain the non-homogeneous case and why occasionally we have to look at test solutions of the form $xy_H$ --- where $y_H$ is a solution of the homogeneous equation. The problem occurs when I try and explain to them why the solutions have to be two dimensional and that we don't need three linearly independent solutions (in the homogeneous case). My best hand-waving argument thus far is that in a solution we will have to integrate twice somewhere and so we will end up with two constants of integration say $C_1$ and $C_2$ so our solution will be $$y_H=y(x,C_1,C_2)$$ but I have had to wave very hard indeed to turn this into $y_H=Ay_1+By_2$. These are not maths students but I still tried to make various bad arguments along the lines of the 'kernel' of the operator $\displaystyle D^2=\frac{d^2y}{dx^2}$ being two dimensional and that the addition of $bD$ and $cI$ distorts the 'kernel' but not the dimension of it (I wonder can this argument be made rigorous). Have any of ye any better ideas? I understand that we can show from the Uniqueness and Existence Equation that the solutions must have this form... the irony is that I am happy to sketch an argument of plausibility of that fact --- which is left without proof in most ODE classes --- but the journey from there to the conclusion, which is done in these classes, is beyond the scope and interest of this class. I fully expect a comment along the lines of they're engineers, who cares? I do!",,"['calculus', 'linear-algebra', 'ordinary-differential-equations', 'education']"
56,Separate incomplete elliptic integral into real and imaginary parts,Separate incomplete elliptic integral into real and imaginary parts,,"I am working in a problem that involves Incomplete Elliptic Integrals of the First and Second kind of the form $F(\sin^{-1}x~|~m)$ and $E(\sin^{-1}x~|~m)$ where the parameters $m$, $x$ are real numbers in the range $m>1$ and $1/\sqrt{m} \le x \le 1$. ($x$ and $m$ are related to the commonly used argument $\phi$ and modulus $m$ by $x \equiv \sin \phi$ and $m \equiv k^2$) As can be seen by plotting them, in this range the real part of the integrals is independent of $x$ while the imaginary part isn't. As an example, see this plot for F and this other for E for a value $m=5$. What I would like to do is to separate the real and imaginary part of this integrals, at least in this particular range. In other words, finding the real valued functions $f_{re} (m)$, $g_{re} (m)$, $f_{im} (x,m)$ and $g_{im} (x,m)$ that satisfy: $$ F(\sin^{-1}x~|~m) \equiv f_{re} (m) + \text{i} f_{im} (x,m) $$ $$ E(\sin^{-1}x~|~m) \equiv g_{re} (m) + \text{i} g_{im} (x,m) $$ in the range $m>1$ and $1/\sqrt{m} \le x \le 1$. By using the Reciprocal Modulus Transformations (see DLMF section 19.7) and taking the limit $x\rightarrow 1/\sqrt{m}$, I have found the real parts to be: $$ f_{re}(m) \equiv \frac{1}{\sqrt{m}} K\left(\frac{1}{m}\right) $$ $$ g_{re}(m) \equiv \sqrt{m} \left[ E \left( \frac{1}{m} \right) - K \left( \frac{1}{m} \right) \right] + \frac{1}{\sqrt{m}} K\left(\frac{1}{m}\right) $$ However, the imaginary parts $f_{im} (x,m)$, $g_{im} (x,m)$ escape me. I reckon there should be a way of expressing them in terms of incomplete elliptic integrals with parameters in the real valued range. If I use the reciprocal modulus transformations I will bring the parameter inside the range $0<m<1$ but the argument will now be complex as I will have $x>1$. I have looked everywhere in the literature but I can't seem to find any identity that solves the problem. I could perhaps do something if there was a way of expressing elliptic integrals of complex argument as a combination of elliptic integrals of real argument and imaginary pure argument, but I don't know how it can be done. Does someone have any insight on how those imaginary parts could be found?","I am working in a problem that involves Incomplete Elliptic Integrals of the First and Second kind of the form $F(\sin^{-1}x~|~m)$ and $E(\sin^{-1}x~|~m)$ where the parameters $m$, $x$ are real numbers in the range $m>1$ and $1/\sqrt{m} \le x \le 1$. ($x$ and $m$ are related to the commonly used argument $\phi$ and modulus $m$ by $x \equiv \sin \phi$ and $m \equiv k^2$) As can be seen by plotting them, in this range the real part of the integrals is independent of $x$ while the imaginary part isn't. As an example, see this plot for F and this other for E for a value $m=5$. What I would like to do is to separate the real and imaginary part of this integrals, at least in this particular range. In other words, finding the real valued functions $f_{re} (m)$, $g_{re} (m)$, $f_{im} (x,m)$ and $g_{im} (x,m)$ that satisfy: $$ F(\sin^{-1}x~|~m) \equiv f_{re} (m) + \text{i} f_{im} (x,m) $$ $$ E(\sin^{-1}x~|~m) \equiv g_{re} (m) + \text{i} g_{im} (x,m) $$ in the range $m>1$ and $1/\sqrt{m} \le x \le 1$. By using the Reciprocal Modulus Transformations (see DLMF section 19.7) and taking the limit $x\rightarrow 1/\sqrt{m}$, I have found the real parts to be: $$ f_{re}(m) \equiv \frac{1}{\sqrt{m}} K\left(\frac{1}{m}\right) $$ $$ g_{re}(m) \equiv \sqrt{m} \left[ E \left( \frac{1}{m} \right) - K \left( \frac{1}{m} \right) \right] + \frac{1}{\sqrt{m}} K\left(\frac{1}{m}\right) $$ However, the imaginary parts $f_{im} (x,m)$, $g_{im} (x,m)$ escape me. I reckon there should be a way of expressing them in terms of incomplete elliptic integrals with parameters in the real valued range. If I use the reciprocal modulus transformations I will bring the parameter inside the range $0<m<1$ but the argument will now be complex as I will have $x>1$. I have looked everywhere in the literature but I can't seem to find any identity that solves the problem. I could perhaps do something if there was a way of expressing elliptic integrals of complex argument as a combination of elliptic integrals of real argument and imaginary pure argument, but I don't know how it can be done. Does someone have any insight on how those imaginary parts could be found?",,"['calculus', 'complex-analysis', 'special-functions', 'elliptic-integrals']"
57,Non-equivalence of D'Alembert's and Cauchy's criterion?,Non-equivalence of D'Alembert's and Cauchy's criterion?,,"Is there a simple example where D'Alembert's and Cauchy's criterion (the root test) for convergence of infinite series don't agree, i.e. one of them proves inconclusive? Can you explain why that happens? Intuitive explanations along with rigorous reasoning are more than welcome!","Is there a simple example where D'Alembert's and Cauchy's criterion (the root test) for convergence of infinite series don't agree, i.e. one of them proves inconclusive? Can you explain why that happens? Intuitive explanations along with rigorous reasoning are more than welcome!",,['calculus']
58,Find a Continuous Function [duplicate],Find a Continuous Function [duplicate],,"This question already has answers here : Closed 11 years ago . Possible Duplicate: Universal Chord Theorem I am having a problem with this exercise. Could someone help? Suppose $a \in (0,1)$ is a real number which is not of the form $\frac{1}{n}$ for any natural number n n. Find a function f which is continuous on $[0, 1]$ and such that $f (0) = f (1)$ but which does not satisfy $f (x) = f (x + a)$ for any x with $x$, $x + a \in [0, 1]$. I noticed that this condition is satisfied if and only if $f(x) \geq f(0)$ Thank you in advance","This question already has answers here : Closed 11 years ago . Possible Duplicate: Universal Chord Theorem I am having a problem with this exercise. Could someone help? Suppose $a \in (0,1)$ is a real number which is not of the form $\frac{1}{n}$ for any natural number n n. Find a function f which is continuous on $[0, 1]$ and such that $f (0) = f (1)$ but which does not satisfy $f (x) = f (x + a)$ for any x with $x$, $x + a \in [0, 1]$. I noticed that this condition is satisfied if and only if $f(x) \geq f(0)$ Thank you in advance",,[]
59,Deriving the addition formula for the lemniscate functions from a total differential equation,Deriving the addition formula for the lemniscate functions from a total differential equation,,"The lemniscate of Bernoulli $C$ is a plane curve defined as follows. Let $a > 0$ be a real number. Let $F_1 = (a, 0)$ and $F_2 = (-a, 0)$ be two points of $\mathbb{R}^2$. Let $C = \{P \in \mathbb{R}^2; PF_1\cdot PF_2 = a^2\}$. Then the equation of $C$ in the polar coordinates is: $r^2 = 2a^2\cos 2\theta$ Let $P$ be a point of $C$ in the first quadrant. Let $u$ be the arc length between $O = (0, 0)$ and $P$. Then, by this question , $u = \int_{0}^{r} \frac{2a^2dr}{\sqrt{4a^4 - r^4}}$ Let $2a^2 = 1$ and $x = r$. Then $u = \int_{0}^{x} \frac{dx}{\sqrt{1 - x^4}}$ $u = u(x)$ is defined in $0 \le x \le 1$. However, the above integral can be defined on $[-1, 1]$. So we extend the domain of $u(x)$ to $[-1, 1]$ by the above integral. Since $\frac{1}{\sqrt{1 - x^4}}$ is invariant under the substitution $x \rightarrow -x$, $u(-x) = -u(x)$ for every $x \in [-1, 1]$. Since $u'(x) = \frac{1}{\sqrt{1 - x^4}} > 0$ on $(-1, 1)$, $u(x)$ is strctly increasing on $[-1, 1]$. Hence there exists the inverse function of $u(x)$. We denote the inverse function of $u(x)$ by $s(u)$. We call $s(u)$ lemniscate sine. Since arcsin $x = \int_{0}^{x} \frac{dx}{\sqrt{1 - x^2}}$, $s(u)$ is analogous to $\sin u$. We denote $u(1) = \int_{0}^{1} \frac{dx}{\sqrt{1 - x^4}}$ by $\omega$. $s(u)$ is defined on $[-\omega, \omega]$. $\omega$ corresponds to $\frac{\pi}{2}$ in the analogy of $s(u)$ with $\sin u$. Since $u(-x) = -u(x)$, $s(-u) = -s(u)$ We define a function $c(u)$ by $c(u) = s(\omega - u)$ and call it lemniscate cosine. $c(u)$ is defined on $[0, 2\omega]$. Pursuing the analogy with $\sin u$ and being motivated by this question , we consider the following total differential equation. $$\frac{dx}{\sqrt{1 - x^4}} + \frac{dy}{\sqrt{1 - y^4}} = 0$$ Let $u = \int_{0}^{x}\frac{dx}{\sqrt{1 - x^4}}$ Then $x = s(u)$ Let $v = \int_{0}^{y}\frac{dy}{\sqrt{1 - y^4}}$ Then $y = s(v)$ Let $c$ be a constant. Then $u + v = c$ is a solution of this equation. Then we get $$s(u + v) = \frac{x\sqrt{1 - y^4} + y\sqrt{1 - x^4}}{1 + x^2y^2}$$ Substituting $u = \omega$, $v = -u$, we get $x = s(\omega) = 1, y = s(-u) = -s(u)$. Hence $s(\omega - u) = \frac{\sqrt{1 - y^4}}{1+y^2} = \sqrt{\frac{1 - y^2}{1 + y^2}}$ Hence $c(u) = \sqrt{\frac{1 - s^2(u)}{1 + s^2(u)}}$ Hence $$s(u+v) = \frac{s(u)c(v) + s(v)c(u)}{1 - s(u)s(v)c(u)c(v)}$$ Since $c(u+v) = s(\omega - u - v) = s((\omega - u) + (-v))$, $$c(u+v) = \frac{c(u)c(v) - s(u)s(v)}{1 + s(u)s(v)c(u)c(v)}$$ Remark Since $c(u) = \sqrt{\frac{1 - s^2(u)}{1 + s^2(u)}}$ and $s(-u) = -s(u)$, $c(-u) = c(u)$. My question How do we prove the following equation? $$s(u + v) = \frac{x\sqrt{1 - y^4} + y\sqrt{1 - x^4}}{1 + x^2y^2}$$","The lemniscate of Bernoulli $C$ is a plane curve defined as follows. Let $a > 0$ be a real number. Let $F_1 = (a, 0)$ and $F_2 = (-a, 0)$ be two points of $\mathbb{R}^2$. Let $C = \{P \in \mathbb{R}^2; PF_1\cdot PF_2 = a^2\}$. Then the equation of $C$ in the polar coordinates is: $r^2 = 2a^2\cos 2\theta$ Let $P$ be a point of $C$ in the first quadrant. Let $u$ be the arc length between $O = (0, 0)$ and $P$. Then, by this question , $u = \int_{0}^{r} \frac{2a^2dr}{\sqrt{4a^4 - r^4}}$ Let $2a^2 = 1$ and $x = r$. Then $u = \int_{0}^{x} \frac{dx}{\sqrt{1 - x^4}}$ $u = u(x)$ is defined in $0 \le x \le 1$. However, the above integral can be defined on $[-1, 1]$. So we extend the domain of $u(x)$ to $[-1, 1]$ by the above integral. Since $\frac{1}{\sqrt{1 - x^4}}$ is invariant under the substitution $x \rightarrow -x$, $u(-x) = -u(x)$ for every $x \in [-1, 1]$. Since $u'(x) = \frac{1}{\sqrt{1 - x^4}} > 0$ on $(-1, 1)$, $u(x)$ is strctly increasing on $[-1, 1]$. Hence there exists the inverse function of $u(x)$. We denote the inverse function of $u(x)$ by $s(u)$. We call $s(u)$ lemniscate sine. Since arcsin $x = \int_{0}^{x} \frac{dx}{\sqrt{1 - x^2}}$, $s(u)$ is analogous to $\sin u$. We denote $u(1) = \int_{0}^{1} \frac{dx}{\sqrt{1 - x^4}}$ by $\omega$. $s(u)$ is defined on $[-\omega, \omega]$. $\omega$ corresponds to $\frac{\pi}{2}$ in the analogy of $s(u)$ with $\sin u$. Since $u(-x) = -u(x)$, $s(-u) = -s(u)$ We define a function $c(u)$ by $c(u) = s(\omega - u)$ and call it lemniscate cosine. $c(u)$ is defined on $[0, 2\omega]$. Pursuing the analogy with $\sin u$ and being motivated by this question , we consider the following total differential equation. $$\frac{dx}{\sqrt{1 - x^4}} + \frac{dy}{\sqrt{1 - y^4}} = 0$$ Let $u = \int_{0}^{x}\frac{dx}{\sqrt{1 - x^4}}$ Then $x = s(u)$ Let $v = \int_{0}^{y}\frac{dy}{\sqrt{1 - y^4}}$ Then $y = s(v)$ Let $c$ be a constant. Then $u + v = c$ is a solution of this equation. Then we get $$s(u + v) = \frac{x\sqrt{1 - y^4} + y\sqrt{1 - x^4}}{1 + x^2y^2}$$ Substituting $u = \omega$, $v = -u$, we get $x = s(\omega) = 1, y = s(-u) = -s(u)$. Hence $s(\omega - u) = \frac{\sqrt{1 - y^4}}{1+y^2} = \sqrt{\frac{1 - y^2}{1 + y^2}}$ Hence $c(u) = \sqrt{\frac{1 - s^2(u)}{1 + s^2(u)}}$ Hence $$s(u+v) = \frac{s(u)c(v) + s(v)c(u)}{1 - s(u)s(v)c(u)c(v)}$$ Since $c(u+v) = s(\omega - u - v) = s((\omega - u) + (-v))$, $$c(u+v) = \frac{c(u)c(v) - s(u)s(v)}{1 + s(u)s(v)c(u)c(v)}$$ Remark Since $c(u) = \sqrt{\frac{1 - s^2(u)}{1 + s^2(u)}}$ and $s(-u) = -s(u)$, $c(-u) = c(u)$. My question How do we prove the following equation? $$s(u + v) = \frac{x\sqrt{1 - y^4} + y\sqrt{1 - x^4}}{1 + x^2y^2}$$",,"['calculus', 'ordinary-differential-equations', 'special-functions', 'elliptic-functions']"
60,Integrating a product of exponential and complementary error function with square-root of variable in the denominator,Integrating a product of exponential and complementary error function with square-root of variable in the denominator,,"I need to evaluate \begin{equation} \int_a^\infty \mathrm{erfc}\left( \frac{b}{\sqrt{c\cdot h}}  \right) e^{-d\cdot h} dh \end{equation} where $\mathrm{erfc}(s) = \frac{2}{\sqrt{\pi}} \int_{s}^{\infty} \exp(-t^2) dt$. A closed-form expression is appreciated since ultimately, I need to do \begin{equation} \int_0^\infty \left( \int_{k\cdot y}^\infty \mathrm{erfc}\left( \frac{b}{\sqrt{c\cdot h}}  \right) e^{-d\cdot h} dh \right) e^{-m \cdot y} dy \end{equation} I've noticed that a similar function - the Q-function - such that \begin{align}   Q(s) &= \frac{1}{\sqrt{2\pi}} \int_s^\infty e^{-\frac{x^2}{2}}dx \\        &=\frac{1}{2} \mathrm{erfc}(\frac{x}{\sqrt{2}}) \end{align} and the Q-function has an alternative representation \begin{align}   Q(s) = \frac{1}{\pi} \int_0^{\frac{\pi}{2}} \exp{\left(\frac{-s^2}{2\sin^2{\phi}} \right)}d\phi \end{align} but I'm not sure if this helps.","I need to evaluate \begin{equation} \int_a^\infty \mathrm{erfc}\left( \frac{b}{\sqrt{c\cdot h}}  \right) e^{-d\cdot h} dh \end{equation} where $\mathrm{erfc}(s) = \frac{2}{\sqrt{\pi}} \int_{s}^{\infty} \exp(-t^2) dt$. A closed-form expression is appreciated since ultimately, I need to do \begin{equation} \int_0^\infty \left( \int_{k\cdot y}^\infty \mathrm{erfc}\left( \frac{b}{\sqrt{c\cdot h}}  \right) e^{-d\cdot h} dh \right) e^{-m \cdot y} dy \end{equation} I've noticed that a similar function - the Q-function - such that \begin{align}   Q(s) &= \frac{1}{\sqrt{2\pi}} \int_s^\infty e^{-\frac{x^2}{2}}dx \\        &=\frac{1}{2} \mathrm{erfc}(\frac{x}{\sqrt{2}}) \end{align} and the Q-function has an alternative representation \begin{align}   Q(s) = \frac{1}{\pi} \int_0^{\frac{\pi}{2}} \exp{\left(\frac{-s^2}{2\sin^2{\phi}} \right)}d\phi \end{align} but I'm not sure if this helps.",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'error-function']"
61,Lie group heuristics for a raising operator for $(-1)^n \frac{d^n}{d\beta^n}\frac{x^\beta}{\beta!}|_{\beta=0}$,Lie group heuristics for a raising operator for,(-1)^n \frac{d^n}{d\beta^n}\frac{x^\beta}{\beta!}|_{\beta=0},"Consider the fractional integro-derivative $\displaystyle\frac{d^{\beta}}{dx^\beta}\frac{x^{\alpha}}{\alpha!}=FP\frac{1}{2\pi i}\displaystyle\oint_{|z-x|=|x|}\frac{z^{\alpha}}{\alpha!}\frac{\beta!}{(z-x)^{\beta+1}}dz=FP\displaystyle\int_{0}^{x}\frac{z^{\alpha}}{\alpha!}\frac{(x-z)^{-\beta-1}}{(-\beta-1)!} dz$ $= \displaystyle\frac{x^{\alpha-\beta}}{(\alpha-\beta)!}$ where  FP  denotes a Hadamard-type finite part, $x>0$, and $\alpha$ and $\beta$ are real. Identify Lie group elements and multiplication as $\displaystyle(\frac{x^{\alpha}}{\alpha!},\frac{x^{\beta}}{\beta!})=FP \displaystyle\int_{0}^{\infty}\frac{z^{\alpha}}{\alpha!}\frac{d}{dx}H(x-z)\frac{(x-z)^{\beta}}{\beta!}dz= \frac{x^{\alpha+\beta}}{(\alpha+\beta)!}$ with $H(x)$ as the Heaviside step function. The complex contour integral gives a continuation of the multiplication rule to the identity element $\beta=0$, so assume taking its derivative w.r.t. $\beta$ at $\beta=0$ gives a convolutional “infinitesimal generator” $R$ leading to $\displaystyle(1-\epsilon R)\frac{x^{\alpha}}{\alpha!}=\frac{x^{\alpha}}{\alpha!}-\epsilon\frac{1}{2\pi i}\displaystyle\oint_{|z-x|=|x|}\frac{-ln(z-x)+\lambda}{z-x}\frac{z^{\alpha}}{\alpha!} dz$ approximating $\frac{x^{\alpha+\epsilon }}{(\alpha+\epsilon)!}$ for small $\epsilon$ where $\lambda=d\beta!/d\beta|_{\beta=0}$. Then, analogous to $(1+tA/n)^n$ tending to $exp(tA)$ as n tends to infinity, assume (letting $\alpha=0$ and $tA/n =-\beta R/n=-\epsilon R$) $\displaystyle\frac{x^\beta}{\beta!} = exp(-\beta R) 1$. Here $R^n$ represents repeated convolution initially acting on 1. If this is true, then, equivalently, $R$ represents a raising operator for $\psi_{n}(x)=(-1)^n \frac{d^n}{d\beta^n}\frac{x^\beta}{\beta!}|_{\beta=0}$; that is, $\psi_{n+1}(x)=R\psi_{n}(x)=\frac{1}{2\pi i}\displaystyle\oint_{|z-x|=|x|}\frac{-ln(z-x)+\lambda}{z-x}\psi_{n}(z) dz$. Update : The contour can be collapsed to the real line to obtain $\psi_{n+1}(x)=R\psi_{n}(x)=(-ln(x)+\lambda)\psi_{n}(x)+\displaystyle\int_{0}^{x}\frac{\psi_{n}\left (  x\right )-\psi_n(u)}{x-u}du$. I have basically two questions about the validity of these relations: A) Can the Lie group argument be made rigorous, and if so, how? B) Can anyone provide a proof of the raising operation independent of group theoretic arguments? Any history on these relations would be appreciated also. PS: By considering the limit of  $\displaystyle\frac{1}{2}[\frac{(-1+a)!}{(z-x)^a}+\frac{(-1-a)!}{(z-x)^{-a}}]$ as $a$ tends to zero, using $\displaystyle\frac{sin(\pi u)}{\pi u}=\frac{1}{u!(-u)!}$, you can show that $I_x=[R,x]=Rx-xR$  is the raising operator for $\displaystyle\frac{x^{\alpha}}{\alpha!}$  ; i.e., $\displaystyle I_x\frac{x^{\alpha}}{\alpha!}=\frac{1}{2\pi i}\displaystyle\oint_{|z-x|=|x|}(-ln(z-x)+\lambda)\frac{z^{\alpha}}{\alpha!}dz=\frac{x^{\alpha+1}}{(\alpha+1)!}$. So, $\displaystyle\frac{x^\beta}{\beta!} = \frac{1}{1+I_{\beta}R} 1$  also, eliminating all factorials, for $\beta>0$.","Consider the fractional integro-derivative $\displaystyle\frac{d^{\beta}}{dx^\beta}\frac{x^{\alpha}}{\alpha!}=FP\frac{1}{2\pi i}\displaystyle\oint_{|z-x|=|x|}\frac{z^{\alpha}}{\alpha!}\frac{\beta!}{(z-x)^{\beta+1}}dz=FP\displaystyle\int_{0}^{x}\frac{z^{\alpha}}{\alpha!}\frac{(x-z)^{-\beta-1}}{(-\beta-1)!} dz$ $= \displaystyle\frac{x^{\alpha-\beta}}{(\alpha-\beta)!}$ where  FP  denotes a Hadamard-type finite part, $x>0$, and $\alpha$ and $\beta$ are real. Identify Lie group elements and multiplication as $\displaystyle(\frac{x^{\alpha}}{\alpha!},\frac{x^{\beta}}{\beta!})=FP \displaystyle\int_{0}^{\infty}\frac{z^{\alpha}}{\alpha!}\frac{d}{dx}H(x-z)\frac{(x-z)^{\beta}}{\beta!}dz= \frac{x^{\alpha+\beta}}{(\alpha+\beta)!}$ with $H(x)$ as the Heaviside step function. The complex contour integral gives a continuation of the multiplication rule to the identity element $\beta=0$, so assume taking its derivative w.r.t. $\beta$ at $\beta=0$ gives a convolutional “infinitesimal generator” $R$ leading to $\displaystyle(1-\epsilon R)\frac{x^{\alpha}}{\alpha!}=\frac{x^{\alpha}}{\alpha!}-\epsilon\frac{1}{2\pi i}\displaystyle\oint_{|z-x|=|x|}\frac{-ln(z-x)+\lambda}{z-x}\frac{z^{\alpha}}{\alpha!} dz$ approximating $\frac{x^{\alpha+\epsilon }}{(\alpha+\epsilon)!}$ for small $\epsilon$ where $\lambda=d\beta!/d\beta|_{\beta=0}$. Then, analogous to $(1+tA/n)^n$ tending to $exp(tA)$ as n tends to infinity, assume (letting $\alpha=0$ and $tA/n =-\beta R/n=-\epsilon R$) $\displaystyle\frac{x^\beta}{\beta!} = exp(-\beta R) 1$. Here $R^n$ represents repeated convolution initially acting on 1. If this is true, then, equivalently, $R$ represents a raising operator for $\psi_{n}(x)=(-1)^n \frac{d^n}{d\beta^n}\frac{x^\beta}{\beta!}|_{\beta=0}$; that is, $\psi_{n+1}(x)=R\psi_{n}(x)=\frac{1}{2\pi i}\displaystyle\oint_{|z-x|=|x|}\frac{-ln(z-x)+\lambda}{z-x}\psi_{n}(z) dz$. Update : The contour can be collapsed to the real line to obtain $\psi_{n+1}(x)=R\psi_{n}(x)=(-ln(x)+\lambda)\psi_{n}(x)+\displaystyle\int_{0}^{x}\frac{\psi_{n}\left (  x\right )-\psi_n(u)}{x-u}du$. I have basically two questions about the validity of these relations: A) Can the Lie group argument be made rigorous, and if so, how? B) Can anyone provide a proof of the raising operation independent of group theoretic arguments? Any history on these relations would be appreciated also. PS: By considering the limit of  $\displaystyle\frac{1}{2}[\frac{(-1+a)!}{(z-x)^a}+\frac{(-1-a)!}{(z-x)^{-a}}]$ as $a$ tends to zero, using $\displaystyle\frac{sin(\pi u)}{\pi u}=\frac{1}{u!(-u)!}$, you can show that $I_x=[R,x]=Rx-xR$  is the raising operator for $\displaystyle\frac{x^{\alpha}}{\alpha!}$  ; i.e., $\displaystyle I_x\frac{x^{\alpha}}{\alpha!}=\frac{1}{2\pi i}\displaystyle\oint_{|z-x|=|x|}(-ln(z-x)+\lambda)\frac{z^{\alpha}}{\alpha!}dz=\frac{x^{\alpha+1}}{(\alpha+1)!}$. So, $\displaystyle\frac{x^\beta}{\beta!} = \frac{1}{1+I_{\beta}R} 1$  also, eliminating all factorials, for $\beta>0$.",,"['calculus', 'analysis', 'complex-analysis', 'lie-groups', 'operator-theory']"
62,Separation of variables for partial differential equations,Separation of variables for partial differential equations,,What class of Partial Differential Equations can be solved using the method of separation of variables?,What class of Partial Differential Equations can be solved using the method of separation of variables?,,['calculus']
63,"Showing sequence of trigonometric integrals is positive, decreasing","Showing sequence of trigonometric integrals is positive, decreasing",,"For integer $n\geq 0$ , consider the integral $$I_n := 2\int_{-\infty}^{\infty}\cos^n(x)\sin(x)xe^{-x^2}dx.$$ Show that each $I_n>0$ and $I_{n+1}< I_n$ . I believe this should be true based on computing values of $n$ in Mathematica, but I do not know how to prove this. For $n=0$ , it's straightforward from integration by parts and using the Fourier transform of $e^{-x^2}$ . For $n=1$ , one can write $\cos(x)\sin(x)=\frac12\sin(2x)$ and use integration by parts again plus the the Fourier transform of $e^{-x^2}$ . If it helps, one can use integration by parts and trig identities to rewrite $$I_n = (n+1)\int_{-\infty}^{\infty}\cos^{n+1}(x)e^{-x^2}dx - n\int_{-\infty}^{\infty}\cos^{n-1}(x)e^{-x^2}dx. $$","For integer , consider the integral Show that each and . I believe this should be true based on computing values of in Mathematica, but I do not know how to prove this. For , it's straightforward from integration by parts and using the Fourier transform of . For , one can write and use integration by parts again plus the the Fourier transform of . If it helps, one can use integration by parts and trig identities to rewrite",n\geq 0 I_n := 2\int_{-\infty}^{\infty}\cos^n(x)\sin(x)xe^{-x^2}dx. I_n>0 I_{n+1}< I_n n n=0 e^{-x^2} n=1 \cos(x)\sin(x)=\frac12\sin(2x) e^{-x^2} I_n = (n+1)\int_{-\infty}^{\infty}\cos^{n+1}(x)e^{-x^2}dx - n\int_{-\infty}^{\infty}\cos^{n-1}(x)e^{-x^2}dx. ,"['calculus', 'integration', 'trigonometric-integrals']"
64,"When is $\int_0^\infty dk \int_0^\infty dq \int_0^R dt \,f(k,q,t,r)\stackrel{?}{=}g(r)$ where both $f$ and $g$ are known functions",When is  where both  and  are known functions,"\int_0^\infty dk \int_0^\infty dq \int_0^R dt \,f(k,q,t,r)\stackrel{?}{=}g(r) f g","I would like to know under which circumstances the following triple integral can be evaluated analytically as $$ \int_{k=0}^{k=\infty} \int_{q=0}^{q=\infty} \int_{t=0}^{t=R} f(k,q,t,r) \,\mathrm{d}t \,\mathrm{d}q \,\mathrm{d}k \stackrel{?}{=} g(r)   \qquad\qquad (0 < t,r < R) \, ,  $$ where the two function $f$ and $g$ are given explicitely by $$ f(k,q,t,r) = \frac{4qk}{\pi \alpha^4} \, \left( Q-q\right) \left( K+k \right)  \left( e^{-k} - e^{-K} \right)  \sin(qt) \sin(kt) J_1(qr)  \, ,  $$ and $$ g(r) = \frac{r}{s^3} \left( \frac{6}{\alpha^2 s^2}  -2e^{-\alpha s} \left( 1 + \frac{3}{\alpha s} + \frac{3}{\alpha^2 s^2} \right) \right) \, . $$ Here, we have defined for the sake of convenience the abbreviations $K = \sqrt{k^2+\alpha^2}$ , $Q = \sqrt{q^2+\alpha^2}$ , and $s=\sqrt{1+r^2}$ . Here, $\alpha$ is a positive real number. Please note that $t \in [0,R]$ and that $k,q \in [0,\infty)$ . A numerical evaluation shows that the result is surprisingly accurate up to a very small additive constant that I do not succeed to evaluate exactly yet. As $R\to\infty$ , it can be checked that the identity is exact. How I proceeded is to use inverse Hankel transform wrt the variable $q$ and inverse sine Fourier transform wrt the variable $k$ . Even though I know that $$ \int_0^\infty r g(r) J_1(qr) \, \mathrm{d}r = \frac{2q}{\alpha^2} \left(  e^{-q} - e^{-Q}\right) \, , $$ I was unable to rigorously prove that. In particular, in the limit $\alpha \to 0$ , this can easily be shown upon using the identity $$ \int_0^\infty \sin(qt) J_1(qr) \, \mathrm{d}q = \frac{t H(r-t)}{r \left( r^2-t^2\right)^\frac{1}{2}} \, . $$ Any insight is highly appreciated. Thank you.","I would like to know under which circumstances the following triple integral can be evaluated analytically as where the two function and are given explicitely by and Here, we have defined for the sake of convenience the abbreviations , , and . Here, is a positive real number. Please note that and that . A numerical evaluation shows that the result is surprisingly accurate up to a very small additive constant that I do not succeed to evaluate exactly yet. As , it can be checked that the identity is exact. How I proceeded is to use inverse Hankel transform wrt the variable and inverse sine Fourier transform wrt the variable . Even though I know that I was unable to rigorously prove that. In particular, in the limit , this can easily be shown upon using the identity Any insight is highly appreciated. Thank you.","
\int_{k=0}^{k=\infty} \int_{q=0}^{q=\infty} \int_{t=0}^{t=R} f(k,q,t,r) \,\mathrm{d}t \,\mathrm{d}q \,\mathrm{d}k \stackrel{?}{=} g(r)  
\qquad\qquad (0 < t,r < R) \, , 
 f g 
f(k,q,t,r) = \frac{4qk}{\pi \alpha^4} \, \left( Q-q\right) \left( K+k \right)  \left( e^{-k} - e^{-K} \right)  \sin(qt) \sin(kt) J_1(qr)  \, , 
 
g(r) = \frac{r}{s^3} \left( \frac{6}{\alpha^2 s^2}  -2e^{-\alpha s} \left( 1 + \frac{3}{\alpha s} + \frac{3}{\alpha^2 s^2} \right) \right) \, .
 K = \sqrt{k^2+\alpha^2} Q = \sqrt{q^2+\alpha^2} s=\sqrt{1+r^2} \alpha t \in [0,R] k,q \in [0,\infty) R\to\infty q k 
\int_0^\infty r g(r) J_1(qr) \, \mathrm{d}r =
\frac{2q}{\alpha^2} \left(  e^{-q} - e^{-Q}\right) \, ,
 \alpha \to 0 
\int_0^\infty \sin(qt) J_1(qr) \, \mathrm{d}q
= \frac{t H(r-t)}{r \left( r^2-t^2\right)^\frac{1}{2}} \, .
","['calculus', 'integration', 'fourier-analysis', 'fourier-transform', 'integral-transforms']"
65,On the limit of a sum equalling $\pi$,On the limit of a sum equalling,\pi,"I've been looking at the following sum: $$S=\lim_{n\to\infty}\left(\frac{8}{n^2}\sum_{k=1}^n k\sqrt{\frac{n}{k}-1}\right)$$ Which I have proved converges to $\pi$ . My proof remains relatively simple. It goes as follows: We begin by developing the following expression: \begin{align} S &=\lim_{n\to\infty}\left(\frac{4}{n}\sum_{k=1}^{\infty}\frac{2k}{n}\sqrt{\frac{n}{k}-1}\right)\\ &=\lim_{n\to\infty}\left(\frac{4}{n}\sum_{k=1}^{\infty}\sqrt{\frac{4k}{n}-\frac{4k^2}{n^2}}\right)\\ &=2\lim_{n\to\infty}\left(\frac{2}{n}\sum_{k=1}^{\infty}\sqrt{1-\left(-1+\frac{2k}{n}\right)^2}\right) \end{align} One can notice that the expression within the limit is just the Riemann representation of the following integral: $$\lim_{n\to\infty}\left(\frac{2}{n}\sum_{k=1}^{\infty}\sqrt{1-\left(-1+\frac{2k}{n}\right)^2}\right)=\int_{-1}^1\sqrt{1-x^2}\ dx$$ Which is simply the area of the unit circle in the top half of the plane, equalling $\frac{\pi}{2}$ . hence we have: \begin{align} S&=2\lim_{n\to\infty}\left(\frac{2}{n}\sum_{k=1}^{\infty}\sqrt{1-\left(-1+\frac{2k}{n}\right)^2}\right)\\&=2\int_{-1}^1\sqrt{1-x^2}\ dx=\pi\end{align} My question to you is; if you were asked to solve this limit problem, how would you go about it? This is a purely recreational question and i'm curious as to whether there exists even more elementary and elegant ways to prove that this sum is indeed equal to $\pi$ . I'm purely looking for beautiful proofs to this limit problem!","I've been looking at the following sum: Which I have proved converges to . My proof remains relatively simple. It goes as follows: We begin by developing the following expression: One can notice that the expression within the limit is just the Riemann representation of the following integral: Which is simply the area of the unit circle in the top half of the plane, equalling . hence we have: My question to you is; if you were asked to solve this limit problem, how would you go about it? This is a purely recreational question and i'm curious as to whether there exists even more elementary and elegant ways to prove that this sum is indeed equal to . I'm purely looking for beautiful proofs to this limit problem!","S=\lim_{n\to\infty}\left(\frac{8}{n^2}\sum_{k=1}^n k\sqrt{\frac{n}{k}-1}\right) \pi \begin{align}
S
&=\lim_{n\to\infty}\left(\frac{4}{n}\sum_{k=1}^{\infty}\frac{2k}{n}\sqrt{\frac{n}{k}-1}\right)\\
&=\lim_{n\to\infty}\left(\frac{4}{n}\sum_{k=1}^{\infty}\sqrt{\frac{4k}{n}-\frac{4k^2}{n^2}}\right)\\
&=2\lim_{n\to\infty}\left(\frac{2}{n}\sum_{k=1}^{\infty}\sqrt{1-\left(-1+\frac{2k}{n}\right)^2}\right)
\end{align} \lim_{n\to\infty}\left(\frac{2}{n}\sum_{k=1}^{\infty}\sqrt{1-\left(-1+\frac{2k}{n}\right)^2}\right)=\int_{-1}^1\sqrt{1-x^2}\ dx \frac{\pi}{2} \begin{align}
S&=2\lim_{n\to\infty}\left(\frac{2}{n}\sum_{k=1}^{\infty}\sqrt{1-\left(-1+\frac{2k}{n}\right)^2}\right)\\&=2\int_{-1}^1\sqrt{1-x^2}\ dx=\pi\end{align} \pi","['calculus', 'integration', 'limits']"
66,Is it circular to use L'Hospital's rule to find derivative?,Is it circular to use L'Hospital's rule to find derivative?,,"I was killing time by doing some random math on paper and came up with the idea of trying to use L'Hospital on the limit definition of the derivative. If I choose the function $f(x)=\ln x$ then I can do $$f'(x)=\lim_{h\to 0}\frac{\ln(x+h) - \ln x}{h}=\lim_{h\to 0}\frac{\ln(\frac{x+h}{x})}{h}=\lim_{h\to 0}\frac{\ln(1 + \frac hx)}{h}$$ The numerator approaches $\ln 1=0$ and the denominator approaches $0$ , so I take the derivative of both with respect to $h$ . $$f'(x)=\lim_{h\to 0}\frac{f'(1 + \frac hx)\cdot \frac 1x}{1}= \frac 1x \cdot \lim_{h\to 0}f'(1+\frac hx)$$ $$f'(x)=\frac{f'(1)}{x}$$ This just so happens to match the truth, but I suspect that this is a flawed argument. Can you tell me specifically where any holes in the logic might be?","I was killing time by doing some random math on paper and came up with the idea of trying to use L'Hospital on the limit definition of the derivative. If I choose the function then I can do The numerator approaches and the denominator approaches , so I take the derivative of both with respect to . This just so happens to match the truth, but I suspect that this is a flawed argument. Can you tell me specifically where any holes in the logic might be?","f(x)=\ln x f'(x)=\lim_{h\to 0}\frac{\ln(x+h) - \ln x}{h}=\lim_{h\to 0}\frac{\ln(\frac{x+h}{x})}{h}=\lim_{h\to 0}\frac{\ln(1 + \frac hx)}{h} \ln 1=0 0 h f'(x)=\lim_{h\to 0}\frac{f'(1 + \frac hx)\cdot \frac 1x}{1}=
\frac 1x \cdot \lim_{h\to 0}f'(1+\frac hx) f'(x)=\frac{f'(1)}{x}","['calculus', 'limits']"
67,The expected distance between two points on a sphere and on a circle,The expected distance between two points on a sphere and on a circle,,"Two points are randomly selected on the circle. What is the expected distance between them? And what will be the expected distance between the two points on a sphere? An interesting problem, I had several ideas: we can generate a uniform distribution in an $n$ -dimensional cube described around a unit ball, remove points outside the ball from the sample, and obtain a uniform distribution of vectors in the ball. We normalize the vectors - we get on the sphere. And we use Monte Carlo.... Of course, there are a lot of iterations, and the accuracy is low, but for a rough estimate and for checking the exact calculations, it will do well. I also reasoned like this: since the task does not change when rotating, I can take one point fixed. We get the expectation of the distance from a random point of the circle to a fixed one. This is an obvious integral: $$ \frac{1}{2 \pi} \int_{-\pi}^{\pi} \sqrt{(1-\cos x)^{2}+\sin ^{2} x}\,dx=\frac{1}{\pi} \int_{-\pi}^{\pi}\left|\sin \frac{x}{2}\right|dx=\frac{2}{\pi} \int_{0}^{\pi} \sin \frac{x}{2}\,dx=\frac{4}{\pi}$$ - for the unit circle, of course. $$ \begin{aligned} &\frac{1}{4 \pi} \int_{0}^{\pi} \int_{-\pi}^{\pi} \sin \theta \sqrt{(1-\cos \theta)^{2}+\sin ^{2} \theta \cos ^{2} \varphi+\sin ^{2} \theta \sin ^{2} \varphi}\, d \varphi d \theta= \\ &=\frac{1}{2} \int_{0}^{\pi} 2 \sin \frac{\theta}{2} \sin \theta\, d \theta=\frac{1}{2} \int_{0}^{\pi} \cos \frac{\theta}{2}-\cos \frac{3 \theta}{2} d \theta=\left.\left(\sin \frac{\theta}{2}-\frac{1}{3} \sin \frac{3 \theta}{2}\right)\right|_{0} ^{\pi}=\frac{4}{3} \end{aligned}.$$ -and this is for the sphere. Parameterization of the sphere: $z=\cos(\theta)$ , $x=\sin(\theta)\cos(φ)$ , $y=\sin(\theta)\sin(φ)$ . As a fixed point we take $(0,0,1)$ . Jacobian $\sin(\theta)$ . I took a slightly non-standard parameterization relative to theta, so that later I would not mess with things like $\sin (\pi/4-\theta/2)$ . Here are my thoughts. I ask you to double-check me and, if possible, write your own version.","Two points are randomly selected on the circle. What is the expected distance between them? And what will be the expected distance between the two points on a sphere? An interesting problem, I had several ideas: we can generate a uniform distribution in an -dimensional cube described around a unit ball, remove points outside the ball from the sample, and obtain a uniform distribution of vectors in the ball. We normalize the vectors - we get on the sphere. And we use Monte Carlo.... Of course, there are a lot of iterations, and the accuracy is low, but for a rough estimate and for checking the exact calculations, it will do well. I also reasoned like this: since the task does not change when rotating, I can take one point fixed. We get the expectation of the distance from a random point of the circle to a fixed one. This is an obvious integral: - for the unit circle, of course. -and this is for the sphere. Parameterization of the sphere: , , . As a fixed point we take . Jacobian . I took a slightly non-standard parameterization relative to theta, so that later I would not mess with things like . Here are my thoughts. I ask you to double-check me and, if possible, write your own version.","n  \frac{1}{2 \pi} \int_{-\pi}^{\pi} \sqrt{(1-\cos x)^{2}+\sin ^{2} x}\,dx=\frac{1}{\pi} \int_{-\pi}^{\pi}\left|\sin \frac{x}{2}\right|dx=\frac{2}{\pi} \int_{0}^{\pi} \sin \frac{x}{2}\,dx=\frac{4}{\pi}  \begin{aligned} &\frac{1}{4 \pi} \int_{0}^{\pi} \int_{-\pi}^{\pi} \sin \theta \sqrt{(1-\cos \theta)^{2}+\sin ^{2} \theta \cos ^{2} \varphi+\sin ^{2} \theta \sin ^{2} \varphi}\, d \varphi d \theta= \\ &=\frac{1}{2} \int_{0}^{\pi} 2 \sin \frac{\theta}{2} \sin \theta\, d \theta=\frac{1}{2} \int_{0}^{\pi} \cos \frac{\theta}{2}-\cos \frac{3 \theta}{2} d \theta=\left.\left(\sin \frac{\theta}{2}-\frac{1}{3} \sin \frac{3 \theta}{2}\right)\right|_{0} ^{\pi}=\frac{4}{3} \end{aligned}. z=\cos(\theta) x=\sin(\theta)\cos(φ) y=\sin(\theta)\sin(φ) (0,0,1) \sin(\theta) \sin (\pi/4-\theta/2)","['calculus', 'probability', 'geometry', 'probability-distributions']"
68,Probability that a Particle which moves Unit distance in a Random direction on each step will be inside the Unit Sphere after $n$ steps,Probability that a Particle which moves Unit distance in a Random direction on each step will be inside the Unit Sphere after  steps,n,"The following integral equation arises while calculating the probability that, a particle which starts at the origin and moves a unit distance in a random direction on each ‘move’, will be within the unit sphere after $n$ moves: $$ f_n(x) = \begin{cases}  \int_{1-x}^{1+x} \frac{df_{n-1}(t)}{dt}  \left( \frac{x^2-(t-1)^2}{4t} \right) dt, & 0\le x\lt 1    \\ f_{n-1} (x-1) + \int_{x-1}^{x+1} \frac{df_{n-1}(t)}{dt}  \left( \frac{x^2 -(t-1)^2}{4t} \right) dt, & 1 \le x \le n-2  \\ f_{n-1} (x-1) + \int_{x-1}^{n-1} \frac{df_{n-1}(t)}{dt}  \left( \frac{x^2 -(t-1)^2}{4t} \right) dt, & n-2\lt x\lt n  \\ 1, & x\ge n \end{cases} $$ Here, $n \ge 3$ . At first glance, this looks quite unsolvable, as it is a mixture of a recurrence relation and an integral equation, that too with differing arguments in $x$ . But just to make sure, is there a way to solve for $f_n(x)$ ? I’m ultimately looking for $f_n(1)$ so it’s also fine if that can be obtained without actually solving the equation. Note: $f_n(x)$ is defined to be the probability that the particle is inside the sphere of radius $x$ centered at the origin after $n$ moves. As the ‘base case’, $$f_2(x) =\begin{cases} \frac{x^2}{4}, & 0\le x\le 2 \\ 1, & x\gt 2 \end{cases}$$ Here are the graphs of $\color{blue}{f_2(x)}, \color{green}{f_3(x)}, \color{red}{f_4(x)} $ , and some initial values: $$f_0(1) = 1 \\ f_1(1) = 0 \\ f_2(1) = \frac 14 \\ f_3(1)= \frac 16 \\ f_4(1) = \frac{23}{192} \\ f_5(1) =\frac{11}{120} $$","The following integral equation arises while calculating the probability that, a particle which starts at the origin and moves a unit distance in a random direction on each ‘move’, will be within the unit sphere after moves: Here, . At first glance, this looks quite unsolvable, as it is a mixture of a recurrence relation and an integral equation, that too with differing arguments in . But just to make sure, is there a way to solve for ? I’m ultimately looking for so it’s also fine if that can be obtained without actually solving the equation. Note: is defined to be the probability that the particle is inside the sphere of radius centered at the origin after moves. As the ‘base case’, Here are the graphs of , and some initial values:","n 
f_n(x) = \begin{cases} 
\int_{1-x}^{1+x} \frac{df_{n-1}(t)}{dt}  \left( \frac{x^2-(t-1)^2}{4t} \right) dt, & 0\le x\lt 1  
 \\ f_{n-1} (x-1) + \int_{x-1}^{x+1} \frac{df_{n-1}(t)}{dt}  \left( \frac{x^2 -(t-1)^2}{4t} \right) dt, & 1 \le x \le n-2 
\\ f_{n-1} (x-1) + \int_{x-1}^{n-1} \frac{df_{n-1}(t)}{dt}  \left( \frac{x^2 -(t-1)^2}{4t} \right) dt, & n-2\lt x\lt n 
\\ 1, & x\ge n
\end{cases}
 n \ge 3 x f_n(x) f_n(1) f_n(x) x n f_2(x) =\begin{cases} \frac{x^2}{4}, & 0\le x\le 2 \\ 1, & x\gt 2 \end{cases} \color{blue}{f_2(x)}, \color{green}{f_3(x)}, \color{red}{f_4(x)}  f_0(1) = 1 \\ f_1(1) = 0 \\ f_2(1) = \frac 14 \\ f_3(1)= \frac 16 \\ f_4(1) = \frac{23}{192} \\ f_5(1) =\frac{11}{120} ","['calculus', 'probability', 'recurrence-relations', 'integral-equations', 'geometric-probability']"
69,Geometric Similarity of Functions,Geometric Similarity of Functions,,"I am a 16 year old high school student and recently I have written a paper on a numerical approximation of distinct functions. I have shown my teachers this and they do not understand it. My questions: Is this a valid theorem to use to estimate functions with differently based functions? Has something similar already been created? Is it all useful/publishable? Any tips on how to improve? I will give an outline but you can find it here: https://www.overleaf.com/read/xjqhfgvrcrbj Definitions Geometric similarity refers to the dilation of a particular shape in all its dimensions. Proofs of geometric similarity are included in congruence proofs of triangles with AAA (Angle-Angle-Angle) proofs. Knowing the sizes of all sides of both triangles: $\triangle{ABC}$ and $\triangle{A'B'C'}$ , to find the dilation factor and prove geometric similarity the following must be true: $\frac{\mid A' \mid}{\mid A \mid} =\frac{\mid B' \mid}{\mid B \mid}=\frac{\mid C' \mid}{\mid C \mid}$ . Interpreting functions as shapes on the Cartesian plane and using geometry, geometrically similar functions can be calculated. Analytically this would imply for a function $y=f(x)\; \{x_0\leq x \leq x_1\}$ a geometrically similar function would be of the form $ny=f(nx)\;\{\frac{x_0}{n}\leq x \leq \frac{x_1}{n}\}$ where $n\in {\rm I\!R}$ . This is because the function is scaled by the same factor in the $x$ and $y$ direction thus would be geometrically similar. $y_1=\sin(x)\;\{0\leq x \leq 2\pi\}$ and $y_2=\frac{1}{2}\sin(2x)\; \{0 \leq x\leq \pi\}$ "" /> However to compare two functions which are distinct, multiplying $x$ and $y$ by $n$ will not suffice for proving similarity. The formula to find the dilation factor can be used to prove similarity between two functions. By describing a function geometrically it has three superficial 'edges' which can be represented as sets. Two of the edges are the two axis $x$ and $y$ . The length of the side ' $y$ ' is the $\max \{ f(x) : x = 1 .. n \}-\min \{ f(x) : x = 1 .. n \}$ and the length of the side $x$ is $b_1$ - $a_1$ where $b_1$ is the upper bound and $a_1$ is the lower bound. Finally the third side of the function will be the arc length over the interval $\{a_1\leq x\leq b_1\}$ . Another characteristic for two shapes to be geometrically similar is the area is increased by the dilation factor squared.Thus from the formula for the dilation factor for two similar triangles the following theorem can be derived: Theorem Let $y_1\;\{a_1\leq x \leq b_1\}$ and $y_2\;\{a_2\leq x \leq b_2\}$ be functions whose derivative exists in every point. If both functions are geometrically similar then the following system holds: \begin{equation}     \frac{1}{\big(b_1-a_1\big)}\int_{a_1}^{b_1}  \sqrt{1+\bigg( \frac{dy_1}{dx}  \bigg) ^{2} } dx= \frac{ 1 }{ \big(b_2-a_2\big) } \int_{a_2}^{b_2}  \sqrt{1+\bigg( \frac{dy_2}{dx}  \bigg) ^{2} } dx \end{equation} \begin{equation}     \frac{1}{\big(b_1-a_1\big)^2} \int_{a_1}^{b_1} y_1 dx= \frac{1}{\big(b_2-a_2\big)^2}\int_{a_2}^{b_2} y_2dx  \end{equation} Similarity Between Distinct Functions When describing a function as distinct it denotes that the functions have different bases, i.e. sinusoidal and exponential. As mentioned above, for geometric similarity to exist of a function $y=f(x)$ the resultant function will become $ny=f(nx)$ . However if comparing functions of different bases, equations (1) and (2) are necessary to find the bounds of similarity. For example, the problem: Find the bounds $b$ and $a$ where $e^x\;\{0\leq x\leq 1\}$ is similar to $x^2 $ . To see examples go to the above link. Any help would be much appreciated and apologies if this is crude mathematics.","I am a 16 year old high school student and recently I have written a paper on a numerical approximation of distinct functions. I have shown my teachers this and they do not understand it. My questions: Is this a valid theorem to use to estimate functions with differently based functions? Has something similar already been created? Is it all useful/publishable? Any tips on how to improve? I will give an outline but you can find it here: https://www.overleaf.com/read/xjqhfgvrcrbj Definitions Geometric similarity refers to the dilation of a particular shape in all its dimensions. Proofs of geometric similarity are included in congruence proofs of triangles with AAA (Angle-Angle-Angle) proofs. Knowing the sizes of all sides of both triangles: and , to find the dilation factor and prove geometric similarity the following must be true: . Interpreting functions as shapes on the Cartesian plane and using geometry, geometrically similar functions can be calculated. Analytically this would imply for a function a geometrically similar function would be of the form where . This is because the function is scaled by the same factor in the and direction thus would be geometrically similar. $y_1=\sin(x)\;\{0\leq x \leq 2\pi\}$ and "" /> However to compare two functions which are distinct, multiplying and by will not suffice for proving similarity. The formula to find the dilation factor can be used to prove similarity between two functions. By describing a function geometrically it has three superficial 'edges' which can be represented as sets. Two of the edges are the two axis and . The length of the side ' ' is the and the length of the side is - where is the upper bound and is the lower bound. Finally the third side of the function will be the arc length over the interval . Another characteristic for two shapes to be geometrically similar is the area is increased by the dilation factor squared.Thus from the formula for the dilation factor for two similar triangles the following theorem can be derived: Theorem Let and be functions whose derivative exists in every point. If both functions are geometrically similar then the following system holds: Similarity Between Distinct Functions When describing a function as distinct it denotes that the functions have different bases, i.e. sinusoidal and exponential. As mentioned above, for geometric similarity to exist of a function the resultant function will become . However if comparing functions of different bases, equations (1) and (2) are necessary to find the bounds of similarity. For example, the problem: Find the bounds and where is similar to . To see examples go to the above link. Any help would be much appreciated and apologies if this is crude mathematics.","\triangle{ABC} \triangle{A'B'C'} \frac{\mid A' \mid}{\mid A \mid} =\frac{\mid B' \mid}{\mid B \mid}=\frac{\mid C' \mid}{\mid C \mid} y=f(x)\; \{x_0\leq x \leq x_1\} ny=f(nx)\;\{\frac{x_0}{n}\leq x \leq \frac{x_1}{n}\} n\in {\rm I\!R} x y y_2=\frac{1}{2}\sin(2x)\; \{0 \leq x\leq \pi\} x y n x y y \max \{ f(x) : x = 1 .. n \}-\min \{ f(x) : x = 1 .. n \} x b_1 a_1 b_1 a_1 \{a_1\leq x\leq b_1\} y_1\;\{a_1\leq x \leq b_1\} y_2\;\{a_2\leq x \leq b_2\} \begin{equation}
    \frac{1}{\big(b_1-a_1\big)}\int_{a_1}^{b_1}  \sqrt{1+\bigg( \frac{dy_1}{dx}  \bigg) ^{2} } dx= \frac{ 1 }{ \big(b_2-a_2\big) } \int_{a_2}^{b_2}  \sqrt{1+\bigg( \frac{dy_2}{dx}  \bigg) ^{2} } dx
\end{equation} \begin{equation}
    \frac{1}{\big(b_1-a_1\big)^2} \int_{a_1}^{b_1} y_1 dx= \frac{1}{\big(b_2-a_2\big)^2}\int_{a_2}^{b_2} y_2dx 
\end{equation} y=f(x) ny=f(nx) b a e^x\;\{0\leq x\leq 1\} x^2 ","['calculus', 'numerical-methods']"
70,"How to prove that $\int_0^1 f(x)\,dx = f(0) + \frac{1}{2}f'(c)$ for some $ c \in [0,1]$?",How to prove that  for some ?,"\int_0^1 f(x)\,dx = f(0) + \frac{1}{2}f'(c)  c \in [0,1]","Given that ${f}$ is differentiable on the interval $[0,1]$ I need to prove that $\int_0^1 f(x)dx = f(0) + \frac{1}{2} f'(c)$ for some $ c \in [0,1]$ . I'm aware of integral mean value theorem, which gives us the following: Exists point $c \in [0,1]: {f}(c) = \frac{1}{1} \int_0^1 f(x)\,\mathrm{d}x$ But I can't get further and it looks like that's not the right way at all. I'll be happy to get any tips or key statements that will lead me to the solution, please.","Given that is differentiable on the interval I need to prove that for some . I'm aware of integral mean value theorem, which gives us the following: Exists point But I can't get further and it looks like that's not the right way at all. I'll be happy to get any tips or key statements that will lead me to the solution, please.","{f} [0,1] \int_0^1 f(x)dx = f(0) + \frac{1}{2} f'(c)  c \in [0,1] c \in [0,1]: {f}(c) = \frac{1}{1} \int_0^1 f(x)\,\mathrm{d}x","['calculus', 'definite-integrals']"
71,Prove the following inequality $\sum_{k=0}^{n}(-1)^{k}f(a_{k})\geq f ( \sum _ { k = 0 } ^ { n } ( - 1 ) ^ { k } a _ { k } )$,Prove the following inequality,\sum_{k=0}^{n}(-1)^{k}f(a_{k})\geq f ( \sum _ { k = 0 } ^ { n } ( - 1 ) ^ { k } a _ { k } ),"Suppose that a function $f$ is convex and increasing on $[0,+\infty)$ and $f(0)=0$ .Show that $$\sum _ { k = 0 } ^ { n } ( - 1 ) ^ { k } f ( a_ { k } ) \geq f \left( \sum _ { k = 0 } ^ { n } ( - 1 ) ^ { k } a _ { k } \right)$$ For any number $a _ { 0 } \geq a _ { 1 } \geq \ldots \geq a _ { n } \geq 0$ Please help me to solve this I have try using Jensen’s inequality. But I have thought it many times. I still can’t do this. Thank beforehand!",Suppose that a function is convex and increasing on and .Show that For any number Please help me to solve this I have try using Jensen’s inequality. But I have thought it many times. I still can’t do this. Thank beforehand!,"f [0,+\infty) f(0)=0 \sum _ { k = 0 } ^ { n } ( - 1 ) ^ { k } f ( a_ { k } ) \geq f \left( \sum _ { k = 0 } ^ { n } ( - 1 ) ^ { k } a _ { k } \right) a _ { 0 } \geq a _ { 1 } \geq \ldots \geq a _ { n } \geq 0","['calculus', 'derivatives']"
72,Name for this kind of derivative?,Name for this kind of derivative?,,"In a problem I was working on, I found it convenient to use the notation $ dX/dA_{i \rightarrow j} $ to represent the marginal change in $X$ from redistributing a marginal amount of $A_i$ to $A_j$. Is there a name for this, and can it even be called a ""derivative""? Is there a better or more conventional way to write this? Context: $\{ A_i \}$ is a finite sequence of loan payments and $X$ could be something like the associated internal rate of return. There are many valid payment sequences that satisfy a set of constraints, each having a different $X$.","In a problem I was working on, I found it convenient to use the notation $ dX/dA_{i \rightarrow j} $ to represent the marginal change in $X$ from redistributing a marginal amount of $A_i$ to $A_j$. Is there a name for this, and can it even be called a ""derivative""? Is there a better or more conventional way to write this? Context: $\{ A_i \}$ is a finite sequence of loan payments and $X$ could be something like the associated internal rate of return. There are many valid payment sequences that satisfy a set of constraints, each having a different $X$.",,"['calculus', 'optimization', 'soft-question', 'notation', 'finance']"
73,Compute the distance from a point in $\textbf{R}^3$ to Boy's surface,Compute the distance from a point in  to Boy's surface,\textbf{R}^3,"We're given a parametric surface $S\subset\textbf{R}^3$, and an arbitrary $\boldsymbol x\in\textbf{R}^3$ where $\boldsymbol x := (x_1,x_2,x_3)$. We'd like to compute the distance from $\boldsymbol x$ to $S$. Note. By distance I mean smallest (Euclidean) distance . Example. Let $S\subset\textbf{R}^3$ be the image of the map $$ \begin{align} [0;2\pi] \times [0;\pi] & \longrightarrow \textbf{R}^3 \\ (\theta, \varphi) & \longmapsto (s_1, s_2, s_3) \\ (\theta, \varphi) & \longmapsto (\cos(\theta)\sin(\varphi),\ \sin(\theta)\sin(\varphi),\ \cos(\varphi)) \end{align} $$ (ie. $S$ is the unit sphere centered at $\boldsymbol 0\in\textbf{R}^3$). Then the (smallest Euclidean) distance $d$ from $\boldsymbol x \in \textbf{R}^3$ to $S\subset\textbf{R}^3$, is (I think) $$ d = |\boldsymbol x - \boldsymbol 0| - 1 = |\boldsymbol x| - 1,$$ where $|\boldsymbol a|$ is the norm of $\boldsymbol a$. The case of interest is the distance from $\boldsymbol x\in\textbf{R}^3$ to the Kusner-Bryant parametrization of Boy's surface (an immersion of $\textbf{R}P^2$ inside $\textbf{R}^3$), which is a function of a complex parameter $w\in\textbf{C}$ in the complex unit disk $\textbf{D}$ (ie. $|w| \leq 1$). Now $S\subset\textbf{R}^3$ is the image of the map $$ \begin{align} \textbf{D}\subset\textbf{C} & \longrightarrow \textbf{R}^3 \\ w & \longmapsto (s_1, s_2, s_3) \\ w & \longmapsto (gg_1, gg_2, gg_3) \end{align} $$ where $$ \begin{align} g1 & := -{3 \over 2} \text{Im}\left[{w(1-w^4) \over g_4}\right] \\ g2 & := -{3 \over 2} \text{Re}\left[{w(1+w^4) \over g_4}\right] \\ g3 & := \text{Im}\left[{1 + w^6 \over g_4}\right] - {1 \over 2} \\ g4 & := w^3(w^3 + \sqrt5) - 1\\ g & := {1 \over g_1^2 + g_2^2 + g_3^2} \end{align} $$ What is an expression for the distance from $\boldsymbol x$ to $S$? (An application of this is to render the surface )","We're given a parametric surface $S\subset\textbf{R}^3$, and an arbitrary $\boldsymbol x\in\textbf{R}^3$ where $\boldsymbol x := (x_1,x_2,x_3)$. We'd like to compute the distance from $\boldsymbol x$ to $S$. Note. By distance I mean smallest (Euclidean) distance . Example. Let $S\subset\textbf{R}^3$ be the image of the map $$ \begin{align} [0;2\pi] \times [0;\pi] & \longrightarrow \textbf{R}^3 \\ (\theta, \varphi) & \longmapsto (s_1, s_2, s_3) \\ (\theta, \varphi) & \longmapsto (\cos(\theta)\sin(\varphi),\ \sin(\theta)\sin(\varphi),\ \cos(\varphi)) \end{align} $$ (ie. $S$ is the unit sphere centered at $\boldsymbol 0\in\textbf{R}^3$). Then the (smallest Euclidean) distance $d$ from $\boldsymbol x \in \textbf{R}^3$ to $S\subset\textbf{R}^3$, is (I think) $$ d = |\boldsymbol x - \boldsymbol 0| - 1 = |\boldsymbol x| - 1,$$ where $|\boldsymbol a|$ is the norm of $\boldsymbol a$. The case of interest is the distance from $\boldsymbol x\in\textbf{R}^3$ to the Kusner-Bryant parametrization of Boy's surface (an immersion of $\textbf{R}P^2$ inside $\textbf{R}^3$), which is a function of a complex parameter $w\in\textbf{C}$ in the complex unit disk $\textbf{D}$ (ie. $|w| \leq 1$). Now $S\subset\textbf{R}^3$ is the image of the map $$ \begin{align} \textbf{D}\subset\textbf{C} & \longrightarrow \textbf{R}^3 \\ w & \longmapsto (s_1, s_2, s_3) \\ w & \longmapsto (gg_1, gg_2, gg_3) \end{align} $$ where $$ \begin{align} g1 & := -{3 \over 2} \text{Im}\left[{w(1-w^4) \over g_4}\right] \\ g2 & := -{3 \over 2} \text{Re}\left[{w(1+w^4) \over g_4}\right] \\ g3 & := \text{Im}\left[{1 + w^6 \over g_4}\right] - {1 \over 2} \\ g4 & := w^3(w^3 + \sqrt5) - 1\\ g & := {1 \over g_1^2 + g_2^2 + g_3^2} \end{align} $$ What is an expression for the distance from $\boldsymbol x$ to $S$? (An application of this is to render the surface )",,"['calculus', 'linear-algebra', 'multivariable-calculus', 'differential-geometry', 'optimization']"
74,Surface Area vs. Volume of Solid of Revolution,Surface Area vs. Volume of Solid of Revolution,,"Surface Area and Volume of Solid of Revolution: Why does $\int 2 \times \pi y \, dx$ not work for surface area, but $\int \pi \times y^2 \, dx$ works for volume? I know that for surface area, it’s because the function is slanted so it couldn’t be written as Riemann sum but also for volume, it can be written as Riemann sum while the function is also slanted.","Surface Area and Volume of Solid of Revolution: Why does $\int 2 \times \pi y \, dx$ not work for surface area, but $\int \pi \times y^2 \, dx$ works for volume? I know that for surface area, it’s because the function is slanted so it couldn’t be written as Riemann sum but also for volume, it can be written as Riemann sum while the function is also slanted.",,"['calculus', 'integration']"
75,A floor function appeared after integration. What happended?,A floor function appeared after integration. What happended?,,"I have laplace's equation, $\nabla^2f=0$, inside a circle (radius $a$) for which the boundary condition (polar coordinates) is $$ f(a,\phi) = \begin{cases} 1\quad \text{for $0< \phi < \pi/2$}\\ -1\quad \text{for $-\pi < \phi < -\pi/2$}\\ 0\quad \text{otherwise} \end{cases} $$ I went to solve it using the formula (for $r<a$): $$ f(r,\phi) = \frac{a^2 - r^2}{2\pi} \int^\pi_{-\pi} \frac{f(a,\phi)}{a^2 + r^2 - 2ar \cos(\phi-t)}dt $$ $$ f(r,\phi) = \frac{a^2 - r^2}{2\pi} \left[ \int^{-\pi/2}_{-\pi} \frac{-1}{a^2 + r^2 - 2ar \cos(\phi-t)}dt + \int^{\pi/2}_{0} \frac{1}{a^2 + r^2 - 2ar \cos(\phi-t)}dt\right] $$ By using the universal trigonometric substitution , I arrived at $$ f(r,\phi) = \frac{1}{\pi}\left[h\left(r,\phi,\frac{-\pi}{2}\right) - h\left(r,\phi,-\pi\right) - h\left(r,\phi,\frac{\pi}{2}\right) + h\left(r,\phi,0\right)\right] $$ Where $$ h(r,\phi,t) = \arctan\left(\frac{a+r}{a-r} \tan\left(\frac{\phi - t}{2}\right)\right) $$ Which agrees with wolfram alpha's result . But then the plotting for various values of $r$ gave me When I did the symbolic integration using a hp 50g calculator, a $floor$ function appeared adding to $h$. $$ g(r,\phi,t) = h(r,\phi,t) + \pi\, \text{floor} \left( \frac{\phi-t}{2\pi} +\frac{1}{2} \right) $$ The final result being $$ f(r,\phi) = \frac{1}{\pi}\left[g\left(r,\phi,\frac{-\pi}{2}\right) - g\left(r,\phi,-\pi\right) - g\left(r,\phi,\frac{\pi}{2}\right) + g\left(r,\phi,0\right)\right] $$ Which plotting gave me the expected result Now I cannot understand what happenned. Why and how did (must) that floor function appear?","I have laplace's equation, $\nabla^2f=0$, inside a circle (radius $a$) for which the boundary condition (polar coordinates) is $$ f(a,\phi) = \begin{cases} 1\quad \text{for $0< \phi < \pi/2$}\\ -1\quad \text{for $-\pi < \phi < -\pi/2$}\\ 0\quad \text{otherwise} \end{cases} $$ I went to solve it using the formula (for $r<a$): $$ f(r,\phi) = \frac{a^2 - r^2}{2\pi} \int^\pi_{-\pi} \frac{f(a,\phi)}{a^2 + r^2 - 2ar \cos(\phi-t)}dt $$ $$ f(r,\phi) = \frac{a^2 - r^2}{2\pi} \left[ \int^{-\pi/2}_{-\pi} \frac{-1}{a^2 + r^2 - 2ar \cos(\phi-t)}dt + \int^{\pi/2}_{0} \frac{1}{a^2 + r^2 - 2ar \cos(\phi-t)}dt\right] $$ By using the universal trigonometric substitution , I arrived at $$ f(r,\phi) = \frac{1}{\pi}\left[h\left(r,\phi,\frac{-\pi}{2}\right) - h\left(r,\phi,-\pi\right) - h\left(r,\phi,\frac{\pi}{2}\right) + h\left(r,\phi,0\right)\right] $$ Where $$ h(r,\phi,t) = \arctan\left(\frac{a+r}{a-r} \tan\left(\frac{\phi - t}{2}\right)\right) $$ Which agrees with wolfram alpha's result . But then the plotting for various values of $r$ gave me When I did the symbolic integration using a hp 50g calculator, a $floor$ function appeared adding to $h$. $$ g(r,\phi,t) = h(r,\phi,t) + \pi\, \text{floor} \left( \frac{\phi-t}{2\pi} +\frac{1}{2} \right) $$ The final result being $$ f(r,\phi) = \frac{1}{\pi}\left[g\left(r,\phi,\frac{-\pi}{2}\right) - g\left(r,\phi,-\pi\right) - g\left(r,\phi,\frac{\pi}{2}\right) + g\left(r,\phi,0\right)\right] $$ Which plotting gave me the expected result Now I cannot understand what happenned. Why and how did (must) that floor function appear?",,"['calculus', 'integration', 'definite-integrals']"
76,Compute $\lim_{x \to 0^+}x\int_{x}^1 \frac{f(t)}{t^2}dt $,Compute,\lim_{x \to 0^+}x\int_{x}^1 \frac{f(t)}{t^2}dt ,"If $f$ is integrable on $[0,1]$ and $\displaystyle \lim_{x \to 0^+}f(x)$ exists, compute $\displaystyle \lim_{x \to 0^+}x\int_{x}^1 \dfrac{f(t)}{t^2} dt $. We can't really use L'Hospital's rule, but for $f(t) = 1, \forall t$, we get the limit to be $1$. How do we compute the limit?","If $f$ is integrable on $[0,1]$ and $\displaystyle \lim_{x \to 0^+}f(x)$ exists, compute $\displaystyle \lim_{x \to 0^+}x\int_{x}^1 \dfrac{f(t)}{t^2} dt $. We can't really use L'Hospital's rule, but for $f(t) = 1, \forall t$, we get the limit to be $1$. How do we compute the limit?",,['calculus']
77,Prove whether series converges or not?,Prove whether series converges or not?,,Does anyone know how to determine with proof whether the series $$\sum_{n=1}^\infty\frac{1}{n^{2+\cos(2\pi\ln(n)) }}$$ converges?,Does anyone know how to determine with proof whether the series $$\sum_{n=1}^\infty\frac{1}{n^{2+\cos(2\pi\ln(n)) }}$$ converges?,,['calculus']
78,Solve integral $\int \frac{\sqrt{x+1}+2}{(x+1)^2 - \sqrt{x+1}}dx$,Solve integral,\int \frac{\sqrt{x+1}+2}{(x+1)^2 - \sqrt{x+1}}dx,"This is how i solved it: first i used substitution $x+1 = t^2 \Rightarrow dx=2tdt$ so integral becomes $I=\int \frac{t+2}{t^4-t}2tdt = 2\int \frac{t+2}{t^3-1}dt  =2\int\frac{t+2}{(t-1)(t^2-t+1)}dt $ usic partial fraction decomposition i have: $\frac{t+2}{(t+1)(t^2-t+1)}=\frac{A}{t+1} + \frac{Bt+C}{t^2-t+1}=\frac{At^2-At+A+Bt^2+Bt+Ct+C}{(t+1)(t^2-t+1)}$ from here, we have that $A=\frac{1}{3} , B=-\frac{1}{3}, C=\frac{5}{3}$ so integral becomes $I=\frac{1}{3} \int \frac{dt}{t+1}-\frac{1}{3}\int\frac{(t-5)dt}{t^2-t+1} = \frac{1}{3}ln|t+1|-\frac{1}{3}I_1 $ Now, for the $I_1$ $I_1=\int\frac{(t-5)dt}{t^2-t+1} = \int\frac{tdt}{t^2-t+1} - \int\frac{5dt}{t^2-t+1}= \frac{1}{2}\int\frac{2t+1-1}{t^2-t+1}dt - 5\int\frac{dt}{t^2-t+1}=\int\frac{2t+1}{t^2-t+1}dt - \frac{9}{2}\int\frac{dt}{t^2-t+1}= ln|t^2-t+1|-\frac{9}{2}I_2$ Now, for $I_2$ $I_2=\int\frac{1}{t^2-t+1}dt= \int\frac{1}{t^2-t+\frac{1}{4} + \frac{3}{4}}dt= \int\frac{1}{(t+\frac{1}{2})^2 + \frac{3}{4}}dt=\frac{4}{3} \int\frac{1}{(\frac{2t+1}{\sqrt{3}})^2 + 1}dt$ Now, we can use substitution: $\frac{2t+1}{\sqrt{3}}=z \Rightarrow dt=\frac{\sqrt{3} dz}{2}$ So we have: $I_2=\frac{2\sqrt{3}}{3}\int\frac{dz}{1+z^2} =\frac{2\sqrt{3}}{3}\arctan z $ Now, going back to $I_1$ $I_1=ln|t^2-t+1|-3\sqrt{3} \arctan \frac{2t+1}{\sqrt{3}}$ and if we go back to $I$ $I=\frac{1}{3}ln|t+1|-\frac{1}{3} ln|t^2-t+1|-\sqrt{3} \arctan \frac{2t+1}{\sqrt{3}}$ in terms of $x$ $I=\frac{1}{3}ln|\sqrt{x+1}+1|-\frac{1}{3} ln|x+2-\sqrt{x+1}|-\sqrt{3} \arctan \frac{2\sqrt{x+1}+1}{\sqrt{3}}$ Yet, in my workbook i have a different solution, but i can't find any mistakes here, any help?","This is how i solved it: first i used substitution $x+1 = t^2 \Rightarrow dx=2tdt$ so integral becomes $I=\int \frac{t+2}{t^4-t}2tdt = 2\int \frac{t+2}{t^3-1}dt  =2\int\frac{t+2}{(t-1)(t^2-t+1)}dt $ usic partial fraction decomposition i have: $\frac{t+2}{(t+1)(t^2-t+1)}=\frac{A}{t+1} + \frac{Bt+C}{t^2-t+1}=\frac{At^2-At+A+Bt^2+Bt+Ct+C}{(t+1)(t^2-t+1)}$ from here, we have that $A=\frac{1}{3} , B=-\frac{1}{3}, C=\frac{5}{3}$ so integral becomes $I=\frac{1}{3} \int \frac{dt}{t+1}-\frac{1}{3}\int\frac{(t-5)dt}{t^2-t+1} = \frac{1}{3}ln|t+1|-\frac{1}{3}I_1 $ Now, for the $I_1$ $I_1=\int\frac{(t-5)dt}{t^2-t+1} = \int\frac{tdt}{t^2-t+1} - \int\frac{5dt}{t^2-t+1}= \frac{1}{2}\int\frac{2t+1-1}{t^2-t+1}dt - 5\int\frac{dt}{t^2-t+1}=\int\frac{2t+1}{t^2-t+1}dt - \frac{9}{2}\int\frac{dt}{t^2-t+1}= ln|t^2-t+1|-\frac{9}{2}I_2$ Now, for $I_2$ $I_2=\int\frac{1}{t^2-t+1}dt= \int\frac{1}{t^2-t+\frac{1}{4} + \frac{3}{4}}dt= \int\frac{1}{(t+\frac{1}{2})^2 + \frac{3}{4}}dt=\frac{4}{3} \int\frac{1}{(\frac{2t+1}{\sqrt{3}})^2 + 1}dt$ Now, we can use substitution: $\frac{2t+1}{\sqrt{3}}=z \Rightarrow dt=\frac{\sqrt{3} dz}{2}$ So we have: $I_2=\frac{2\sqrt{3}}{3}\int\frac{dz}{1+z^2} =\frac{2\sqrt{3}}{3}\arctan z $ Now, going back to $I_1$ $I_1=ln|t^2-t+1|-3\sqrt{3} \arctan \frac{2t+1}{\sqrt{3}}$ and if we go back to $I$ $I=\frac{1}{3}ln|t+1|-\frac{1}{3} ln|t^2-t+1|-\sqrt{3} \arctan \frac{2t+1}{\sqrt{3}}$ in terms of $x$ $I=\frac{1}{3}ln|\sqrt{x+1}+1|-\frac{1}{3} ln|x+2-\sqrt{x+1}|-\sqrt{3} \arctan \frac{2\sqrt{x+1}+1}{\sqrt{3}}$ Yet, in my workbook i have a different solution, but i can't find any mistakes here, any help?",,"['calculus', 'integration', 'indefinite-integrals']"
79,Non-circular proof of $\lim_{\theta \to 0}\frac{\sin\theta}{\theta} = 1$,Non-circular proof of,\lim_{\theta \to 0}\frac{\sin\theta}{\theta} = 1,"$$\lim_{\theta \to 0}\frac{\sin\theta}{\theta} = 1$$ The above limit is fundamental to studies of introductory calculus. I know that this limit could be proven by the squeeze theorem and the length of sector , i.e. $$s = r\theta$$ where r is radius and $\theta$ the angle. However, it is claimed that the proof of this limit is circular. I can't bring myself to agree to that, but apparently the length of sector is a corollary of the limit , which is proven by the inequality $$\cos \theta < \frac{\sin\theta}{\theta} < 1$$ Can anyone point me to other proofs of the limit?","$$\lim_{\theta \to 0}\frac{\sin\theta}{\theta} = 1$$ The above limit is fundamental to studies of introductory calculus. I know that this limit could be proven by the squeeze theorem and the length of sector , i.e. $$s = r\theta$$ where r is radius and $\theta$ the angle. However, it is claimed that the proof of this limit is circular. I can't bring myself to agree to that, but apparently the length of sector is a corollary of the limit , which is proven by the inequality $$\cos \theta < \frac{\sin\theta}{\theta} < 1$$ Can anyone point me to other proofs of the limit?",,"['calculus', 'trigonometry']"
80,Does the minimum of two decreasing divergent series diverge? [duplicate],Does the minimum of two decreasing divergent series diverge? [duplicate],,"This question already has answers here : Find examples of two series $\sum a_n$ and $\sum b_n$ both of which diverge but for which $\sum \min(a_n, b_n)$ converges (3 answers) Closed 8 years ago . If $\displaystyle \sum_{n=1}^{\infty}a_n$ and $\displaystyle \sum_{n=1}^{\infty}b_n$ are both divergent series with $a_n\downarrow0$ and $b_n\downarrow0$, [so $(a_n)$ and $(b_n)$ are decreasing sequences which converge to 0], and if $c_n=\min\{a_n,b_n\},\;\;$ does the series $\displaystyle \sum_{n=1}^{\infty}c_n$ necessarily diverge? (I was led to ask this question after reading this question and math110's solution to it: $a_n\downarrow 0, \sum\limits_{n=1}^{\infty}a_n=+\infty, b_n=min\{a_n,1/n\}$, prove $\sum b_n $ diverges. .)","This question already has answers here : Find examples of two series $\sum a_n$ and $\sum b_n$ both of which diverge but for which $\sum \min(a_n, b_n)$ converges (3 answers) Closed 8 years ago . If $\displaystyle \sum_{n=1}^{\infty}a_n$ and $\displaystyle \sum_{n=1}^{\infty}b_n$ are both divergent series with $a_n\downarrow0$ and $b_n\downarrow0$, [so $(a_n)$ and $(b_n)$ are decreasing sequences which converge to 0], and if $c_n=\min\{a_n,b_n\},\;\;$ does the series $\displaystyle \sum_{n=1}^{\infty}c_n$ necessarily diverge? (I was led to ask this question after reading this question and math110's solution to it: $a_n\downarrow 0, \sum\limits_{n=1}^{\infty}a_n=+\infty, b_n=min\{a_n,1/n\}$, prove $\sum b_n $ diverges. .)",,"['calculus', 'sequences-and-series']"
81,How find limit $\displaystyle \lim_{n\to\infty}n\left(1-\tfrac{\ln n}{n}\right)^n$,How find limit,\displaystyle \lim_{n\to\infty}n\left(1-\tfrac{\ln n}{n}\right)^n,How find this limit $$\displaystyle \lim_{n\to\infty}n\left(1-\dfrac{\ln n}{n}\right)^n$$,How find this limit $$\displaystyle \lim_{n\to\infty}n\left(1-\dfrac{\ln n}{n}\right)^n$$,,"['calculus', 'algebra-precalculus', 'limits']"
82,general solution of the equation $\frac{dy}{dx} =\exp(y/x)$,general solution of the equation,\frac{dy}{dx} =\exp(y/x),"How can i get the general solution of the equation a) $\frac{dy}{dx} = \exp(y/x)$ b) $\frac{dy}{dx} = \exp(x-y)$ and $y=2$ when $x = 0$ I tried b) first: This is a first-order nonlinear ordinary differential equation, which is separable.  General solution:  $y(x) = \ln(C+e^x)$ Finding C , we have that: $$2 = \ln(C + e^0)$$ $$ 2 = \ln(C + 1) $$  $$e^2 = C+1 $$  $$C = e^2 - 1 $$  Particular solution:  $$y(x) = \ln(e^2 -1 + e^x)$$ Is that correct the solution for b)? , I stuck with a), some help please.","How can i get the general solution of the equation a) $\frac{dy}{dx} = \exp(y/x)$ b) $\frac{dy}{dx} = \exp(x-y)$ and $y=2$ when $x = 0$ I tried b) first: This is a first-order nonlinear ordinary differential equation, which is separable.  General solution:  $y(x) = \ln(C+e^x)$ Finding C , we have that: $$2 = \ln(C + e^0)$$ $$ 2 = \ln(C + 1) $$  $$e^2 = C+1 $$  $$C = e^2 - 1 $$  Particular solution:  $$y(x) = \ln(e^2 -1 + e^x)$$ Is that correct the solution for b)? , I stuck with a), some help please.",,"['calculus', 'ordinary-differential-equations', 'derivatives']"
83,Problem of limit of power function,Problem of limit of power function,,"I have a problem : Find $$\lim_{x\to 0}\left(\dfrac{x^2-2x+3}{x^2-3x+2}\right)^{\dfrac{\sin x}{x}}$$ Here is my argument : $$\lim_{x\rightarrow 0}\left(\dfrac{x^2-2x+3}{x^2-3x+2}\right)^{\dfrac{\sin x}{x}}= \lim_{x\rightarrow 0}e^{\ln\left(\dfrac{x^2-2x+3}{x^2-3x+2}\right)\dfrac{\sin x}{x}}$$ On the other hand, $$\lim_{x\rightarrow 0}\text{ln}\left(\dfrac{x^2-2x+3}{x^2-3x+2}\right)=\ln\left(\lim_{x\rightarrow 0}\dfrac{x^2-2x+3}{x^2-3x+2}\right)=\text{ln}\dfrac{3}{2}$$ and $$\lim_{x\rightarrow 0}\dfrac{\sin x}{x}=1$$ therefore $$\lim_{x\rightarrow 0}\left(\dfrac{x^2-2x+3}{x^2-3x+2}\right)^{\dfrac{\sin x}{x}}=e^{\ln\frac{3}{2}}=\dfrac{3}{2}$$ Am I wrong ? If I am wrong, please show me how to do this problem. Thanks !","I have a problem : Find $$\lim_{x\to 0}\left(\dfrac{x^2-2x+3}{x^2-3x+2}\right)^{\dfrac{\sin x}{x}}$$ Here is my argument : $$\lim_{x\rightarrow 0}\left(\dfrac{x^2-2x+3}{x^2-3x+2}\right)^{\dfrac{\sin x}{x}}= \lim_{x\rightarrow 0}e^{\ln\left(\dfrac{x^2-2x+3}{x^2-3x+2}\right)\dfrac{\sin x}{x}}$$ On the other hand, $$\lim_{x\rightarrow 0}\text{ln}\left(\dfrac{x^2-2x+3}{x^2-3x+2}\right)=\ln\left(\lim_{x\rightarrow 0}\dfrac{x^2-2x+3}{x^2-3x+2}\right)=\text{ln}\dfrac{3}{2}$$ and $$\lim_{x\rightarrow 0}\dfrac{\sin x}{x}=1$$ therefore $$\lim_{x\rightarrow 0}\left(\dfrac{x^2-2x+3}{x^2-3x+2}\right)^{\dfrac{\sin x}{x}}=e^{\ln\frac{3}{2}}=\dfrac{3}{2}$$ Am I wrong ? If I am wrong, please show me how to do this problem. Thanks !",,['calculus']
84,Continuity proof.,Continuity proof.,,"I want to prove that $\exp x$ and $\sin x$ are continuous. This means I want to show that $$\lim\limits_{x\to a}e^x=e^a$$ $$\lim\limits_{x\to a}\sin x=\sin a$$ for any fixed $a \in \Bbb R$. Then I need to show that for any $\epsilon >0$ there exists a $\delta >0$ such that whenever $|x-a|<\delta$, then $|\sin x -\sin a|<\epsilon$ and similarily $|e^x-e^a|<\epsilon$. For the first case I have that $$\left| {\sin x - \sin a} \right| = 2\left| {\sin \frac{{x - a}}{2}} \right|\left| {\cos \frac{{x + a}}{2}} \right|$$ for the second case I use $$\left| {{e^x} - {e^a}} \right| = \left| {{e^a}} \right|\left| {{e^{x - a}} - 1} \right|$$ ADD Just for the sake of a simple solution: Let $\epsilon>0$ and $a\in \bf R$ be given. Choose $\delta = \epsilon$. Then we have that $$\eqalign{   \left| {\sin x - \sin a} \right| = 2\left| {\sin \left( {\frac{{x - a}}{2}} \right)\cos \left( {\frac{{x + a}}{2}} \right)} \right| &\cr     = 2\sin \left| {\frac{{x - a}}{2}} \right|\cos \left| {\frac{{x + a}}{2}} \right| &\cr     \leqslant 2\left| {\frac{{x - a}}{2}} \right| = \left| {x - a} \right| < \delta  = \epsilon  &\cr} $$ For the exponential, we have that $\log:(0,+\infty)\to \Bbb R$ is continuous, one-one, onto, and differentiable on $(0,+\infty)$. It follows that $\exp:\mathbb R\to (0,\infty)$ defined by $\exp x =y \iff \log y = x$ is also continuous and differentiable. Definitions : $y=e^x$ is the inverse of $y=\log x $ defined as $$\log x = \lim_{k \to 0} \frac{x^k-1}{k}$$ Here I gave a list of the elementary functions of $\log$. I would probably choose to define $\sin$ like this Checking Apostol's Calculus I found he proves the continuity of $\sin x$ by first proving that if $f$ is (Riemann) integrable in $[a,x]$ for all $x\in [a,b]$ then $$F(x) = \int_a^x f(t) dt$$ is continuous for all $x\in [a,b]$. Under this light it is evident $$\sin x = \int_0^x \cos t dt $$ $$\cos x = 1-\int_0^x \sin t dt $$ $$e^x = \int_0^x e^t dt+1$$ are all continuous.","I want to prove that $\exp x$ and $\sin x$ are continuous. This means I want to show that $$\lim\limits_{x\to a}e^x=e^a$$ $$\lim\limits_{x\to a}\sin x=\sin a$$ for any fixed $a \in \Bbb R$. Then I need to show that for any $\epsilon >0$ there exists a $\delta >0$ such that whenever $|x-a|<\delta$, then $|\sin x -\sin a|<\epsilon$ and similarily $|e^x-e^a|<\epsilon$. For the first case I have that $$\left| {\sin x - \sin a} \right| = 2\left| {\sin \frac{{x - a}}{2}} \right|\left| {\cos \frac{{x + a}}{2}} \right|$$ for the second case I use $$\left| {{e^x} - {e^a}} \right| = \left| {{e^a}} \right|\left| {{e^{x - a}} - 1} \right|$$ ADD Just for the sake of a simple solution: Let $\epsilon>0$ and $a\in \bf R$ be given. Choose $\delta = \epsilon$. Then we have that $$\eqalign{   \left| {\sin x - \sin a} \right| = 2\left| {\sin \left( {\frac{{x - a}}{2}} \right)\cos \left( {\frac{{x + a}}{2}} \right)} \right| &\cr     = 2\sin \left| {\frac{{x - a}}{2}} \right|\cos \left| {\frac{{x + a}}{2}} \right| &\cr     \leqslant 2\left| {\frac{{x - a}}{2}} \right| = \left| {x - a} \right| < \delta  = \epsilon  &\cr} $$ For the exponential, we have that $\log:(0,+\infty)\to \Bbb R$ is continuous, one-one, onto, and differentiable on $(0,+\infty)$. It follows that $\exp:\mathbb R\to (0,\infty)$ defined by $\exp x =y \iff \log y = x$ is also continuous and differentiable. Definitions : $y=e^x$ is the inverse of $y=\log x $ defined as $$\log x = \lim_{k \to 0} \frac{x^k-1}{k}$$ Here I gave a list of the elementary functions of $\log$. I would probably choose to define $\sin$ like this Checking Apostol's Calculus I found he proves the continuity of $\sin x$ by first proving that if $f$ is (Riemann) integrable in $[a,x]$ for all $x\in [a,b]$ then $$F(x) = \int_a^x f(t) dt$$ is continuous for all $x\in [a,b]$. Under this light it is evident $$\sin x = \int_0^x \cos t dt $$ $$\cos x = 1-\int_0^x \sin t dt $$ $$e^x = \int_0^x e^t dt+1$$ are all continuous.",,"['calculus', 'limits', 'continuity']"
85,Integral of the derivative of a function of bounded variation,Integral of the derivative of a function of bounded variation,,"Let $f\colon [a,b] \to \mathbb R$ be of bounded variation. Must it be the case that $|\int_a ^b f' (x) |\leq |TV(f)|$, where $TV(f)$ is the total variation of $f$ over $[a,b]$? If so, how can one prove this? In the standard proof of the monotone differentiation theorem, it is shown tat this holds for increasing functions: if $f$ is increasing, then $\int_a ^b f'(x) \leq f(b) - f(a) = TV(f)$. I am trying to generalize this to functions of bounded variation.","Let $f\colon [a,b] \to \mathbb R$ be of bounded variation. Must it be the case that $|\int_a ^b f' (x) |\leq |TV(f)|$, where $TV(f)$ is the total variation of $f$ over $[a,b]$? If so, how can one prove this? In the standard proof of the monotone differentiation theorem, it is shown tat this holds for increasing functions: if $f$ is increasing, then $\int_a ^b f'(x) \leq f(b) - f(a) = TV(f)$. I am trying to generalize this to functions of bounded variation.",,"['calculus', 'measure-theory', 'functional-analysis', 'bounded-variation']"
86,evaluation of $\sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n} H_{n+1}^{(2)}}{(n+1)^{2}}$ and other Euler sums,evaluation of  and other Euler sums,\sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n} H_{n+1}^{(2)}}{(n+1)^{2}},"I was trying to evaluate this famous integral $$\int_{0}^{1} \frac{\ln (x) \ln^{2}(1+x) \ln(1-x)}{x} \ dx $$ Here is my attempt so solve the integral \begin{align} &\int_{0}^{1} \frac{\ln (x) \ln^{2}(1+x) \ln(1-x)}{x} \ dx = 2 \int_{0}^{1} \frac{\ln (x) \ln(1-x)}{x} \sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n}}{n+1} x^{n+1} \ dx \\ &= 2 \sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n}}{n+1} \int_{0}^{1} x^{n} \ln(x) \ln(1-x) \ dx \\ &= 2 \sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n}}{n+1} \frac{\partial }{\partial a \partial b} B(a,b) \Big|_{(a=n+1,b=1)} \\ &= 2 \sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n}}{n+1} B(n+1,1) \Big [ \Big( \psi(n+1)-\psi(n+2) \Big) \Big(\psi(1)-\psi(n+2) \Big)-\psi'(n+2)\Big] \\ &= 2 \sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n}}{(n+1)^{2}} \Big[ (H_{n} -H_{n+1} ) (-H_{n+1}) - \frac{\pi^{2}}{6} + H_{n+1}^{(2)} \Big] \\ &= 2 \sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n}}{(n+1)^{2}} \Big[\Big(-\frac{1}{n+1} \Big) (-H_{n+1}) - \frac{\pi^{2}}{6} + H_{n+1}^{(2)} \Big] \\ &= 2 \sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n} H_{n+1}}{(n+1)^{3}} - \frac{\pi^{2}}{3} \sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n}}{(n+1)^{2}} + 2 \sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n} H_{n+1}^{(2)}}{(n+1)^{2}} \\ &= 2 \sum_{n=1}^{\infty} (-1)^{n-1} \frac{(H_{n})^{2}}{(n+1)^{3}} + 2 \sum_{n=1}^{\infty} (-1)^{k-1} \frac{H_{n}}{(n+1)^{4}} - \frac{\pi^{2}}{3} \sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n}}{(n+1)^{2}} + 2 \sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n} H_{n+1}^{(2)}}{(n+1)^{2}} \end{align} Evaluating the first Euler sum is probably quite difficult, and the fourth sum seems crazy. Maybe I made a mistake. Note that: this is not a duplicate of the question because I want to know the evaluation of the last 4 Euler sums, not the main integral. Thank for reading. edit; wrong link","I was trying to evaluate this famous integral Here is my attempt so solve the integral Evaluating the first Euler sum is probably quite difficult, and the fourth sum seems crazy. Maybe I made a mistake. Note that: this is not a duplicate of the question because I want to know the evaluation of the last 4 Euler sums, not the main integral. Thank for reading. edit; wrong link","\int_{0}^{1} \frac{\ln (x) \ln^{2}(1+x) \ln(1-x)}{x} \ dx  \begin{align}
&\int_{0}^{1} \frac{\ln (x) \ln^{2}(1+x) \ln(1-x)}{x} \ dx = 2 \int_{0}^{1} \frac{\ln (x) \ln(1-x)}{x} \sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n}}{n+1} x^{n+1} \ dx \\
&= 2 \sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n}}{n+1} \int_{0}^{1} x^{n} \ln(x) \ln(1-x) \ dx \\
&= 2 \sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n}}{n+1} \frac{\partial }{\partial a \partial b} B(a,b) \Big|_{(a=n+1,b=1)} \\
&= 2 \sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n}}{n+1} B(n+1,1) \Big [ \Big( \psi(n+1)-\psi(n+2) \Big) \Big(\psi(1)-\psi(n+2) \Big)-\psi'(n+2)\Big] \\
&= 2 \sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n}}{(n+1)^{2}} \Big[ (H_{n} -H_{n+1} ) (-H_{n+1}) - \frac{\pi^{2}}{6} + H_{n+1}^{(2)} \Big] \\
&= 2 \sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n}}{(n+1)^{2}} \Big[\Big(-\frac{1}{n+1} \Big) (-H_{n+1}) - \frac{\pi^{2}}{6} + H_{n+1}^{(2)} \Big] \\
&= 2 \sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n} H_{n+1}}{(n+1)^{3}} - \frac{\pi^{2}}{3} \sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n}}{(n+1)^{2}} + 2 \sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n} H_{n+1}^{(2)}}{(n+1)^{2}} \\
&= 2 \sum_{n=1}^{\infty} (-1)^{n-1} \frac{(H_{n})^{2}}{(n+1)^{3}} + 2 \sum_{n=1}^{\infty} (-1)^{k-1} \frac{H_{n}}{(n+1)^{4}} - \frac{\pi^{2}}{3} \sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n}}{(n+1)^{2}} + 2 \sum_{n=1}^{\infty} (-1)^{n-1} \frac{H_{n} H_{n+1}^{(2)}}{(n+1)^{2}}
\end{align}","['calculus', 'integration']"
87,Integral of $\int_{-r}^r e^{\beta y} \arctan\left(\frac{\sqrt{r^2-y^2}}{d}\right) dy$,Integral of,\int_{-r}^r e^{\beta y} \arctan\left(\frac{\sqrt{r^2-y^2}}{d}\right) dy,"I'm looking for anything that might help me solve the integral below: $$\int_{-r}^r e^{\beta y} \arctan\left(\frac{\sqrt{r^2-y^2}}{d}\right) dy$$ With $\\{r,d,\beta,y\\}\in\mathbb{R}$ , $\beta<0$ , $\\{r,d\\}>0$ The function is essentially the angle $\phi=\arctan\left(\frac{\sqrt{r^2-y^2}}{d}\right)$ multiplied by an exponential decay, $e^{\beta x}$ . In the figure below, you can see that $\phi=\arctan\left(\frac{a}{d}\right)$ and $a=\sqrt{r^2-y^2}$ . The only indication I have that the integral has an analytic solution is that the region is closed and well defined. The function is not analytic at $y=\pm r$ but is is analytical around it. I've attempted a number of substitutions, integration by parts, and residue analysis. I've also tried to find it in some integral tables but I'm not sure what form to search it in. The obvious substitution is $y=r\sin({\theta})$ . This leads to $$\int_{-\pi/2}^{\pi/2}e^{\beta r \sin({\theta})}\arctan\left({\frac{r \cos\left(\theta\right)}{d}}\right)r\cos\left(\theta\right)d\theta$$ Which doesn't seem any easier to solve.","I'm looking for anything that might help me solve the integral below: With , , The function is essentially the angle multiplied by an exponential decay, . In the figure below, you can see that and . The only indication I have that the integral has an analytic solution is that the region is closed and well defined. The function is not analytic at but is is analytical around it. I've attempted a number of substitutions, integration by parts, and residue analysis. I've also tried to find it in some integral tables but I'm not sure what form to search it in. The obvious substitution is . This leads to Which doesn't seem any easier to solve.","\int_{-r}^r e^{\beta y} \arctan\left(\frac{\sqrt{r^2-y^2}}{d}\right) dy \\{r,d,\beta,y\\}\in\mathbb{R} \beta<0 \\{r,d\\}>0 \phi=\arctan\left(\frac{\sqrt{r^2-y^2}}{d}\right) e^{\beta x} \phi=\arctan\left(\frac{a}{d}\right) a=\sqrt{r^2-y^2} y=\pm r y=r\sin({\theta}) \int_{-\pi/2}^{\pi/2}e^{\beta r \sin({\theta})}\arctan\left({\frac{r \cos\left(\theta\right)}{d}}\right)r\cos\left(\theta\right)d\theta","['calculus', 'integration', 'definite-integrals', 'exponential-function']"
88,"Suppose $f(x)=(\tan x)^{\frac{3}{2}}-3\tan x+(\tan x)^{\frac{1}{2}}$. Then, how can we compare the given integrals?","Suppose . Then, how can we compare the given integrals?",f(x)=(\tan x)^{\frac{3}{2}}-3\tan x+(\tan x)^{\frac{1}{2}},"Let $f(x)=(\tan x)^\frac32-3\tan x+\sqrt{\tan x}$ . Consider the integrals $$I_1=\int_0^1f(x)dx$$ $$I_2=\int_{0.3}^{1.3}f(x)dx$$ $$I_3=\int_{0.5}^{1.5}f(x)dx$$ Then, prove that $I_1>I_3>I_2$ I tried out a lot of things. This question has been repeated twice at this site but no proper solution was found. Some found solutions using graphs. However, This is a question from a high school exam that does not permit the use of graphing softwares. Here are a few of my approaches that failed: Creating a function $g(x)$ such that: $$ g(x)=\int_{x}^{x+1} f(t)dt$$ and differentiate it using Newton-Leibnitz. Nothing came out.It gives $g(x)=f(x+1)-f(x)$ . The derivative of $f(x)$ does not reveal much to comment about $f(x+1)$ and $f(x)$ . Integrating the expression and finding out the primitive. I tried substituting $t=\sqrt{\tan x}$ and proceeding further using algebraic twins. I did find a primitive and here it is: $$2(\ln(\frac{\sqrt{\tan x}+\sqrt{\cot x}-\sqrt{2}}{\sqrt{\tan x}+\sqrt{\cot x}+\sqrt{2}})+ \sqrt{\tan x})-3\ln{\sec x}$$ Go ahead and put the limits and compare them if you dare. I won't stop you. If you find something out,please do tell me. Graphing by converting it into a reduced cubic and seeing if one or more areas are comparable enough.(It is a hand graph, so no use).But I got to see something interesting. I found out that the roots of the reduced cubic are $\frac{3+\sqrt{5}}{2}$ and $\frac{3-\sqrt{5}}{2}$ which can be written as $4cos^2 \frac{\pi}{10}$ and its reciprocal. But in the end the, it did not help me. This is a problem from Bstat paper of ISI $2009$ and this problem has got me drunk.","Let . Consider the integrals Then, prove that I tried out a lot of things. This question has been repeated twice at this site but no proper solution was found. Some found solutions using graphs. However, This is a question from a high school exam that does not permit the use of graphing softwares. Here are a few of my approaches that failed: Creating a function such that: and differentiate it using Newton-Leibnitz. Nothing came out.It gives . The derivative of does not reveal much to comment about and . Integrating the expression and finding out the primitive. I tried substituting and proceeding further using algebraic twins. I did find a primitive and here it is: Go ahead and put the limits and compare them if you dare. I won't stop you. If you find something out,please do tell me. Graphing by converting it into a reduced cubic and seeing if one or more areas are comparable enough.(It is a hand graph, so no use).But I got to see something interesting. I found out that the roots of the reduced cubic are and which can be written as and its reciprocal. But in the end the, it did not help me. This is a problem from Bstat paper of ISI and this problem has got me drunk.",f(x)=(\tan x)^\frac32-3\tan x+\sqrt{\tan x} I_1=\int_0^1f(x)dx I_2=\int_{0.3}^{1.3}f(x)dx I_3=\int_{0.5}^{1.5}f(x)dx I_1>I_3>I_2 g(x)  g(x)=\int_{x}^{x+1} f(t)dt g(x)=f(x+1)-f(x) f(x) f(x+1) f(x) t=\sqrt{\tan x} 2(\ln(\frac{\sqrt{\tan x}+\sqrt{\cot x}-\sqrt{2}}{\sqrt{\tan x}+\sqrt{\cot x}+\sqrt{2}})+ \sqrt{\tan x})-3\ln{\sec x} \frac{3+\sqrt{5}}{2} \frac{3-\sqrt{5}}{2} 4cos^2 \frac{\pi}{10} 2009,"['calculus', 'integration']"
89,"Spivak, Ch. 23, ""Infinite Series"", Problem 21c","Spivak, Ch. 23, ""Infinite Series"", Problem 21c",,"The following problem is from Chapter 23 ""Infinite Series"" of Spivak's Calculus . Note that there are questions about convergence of the binomial series such as this one , but the current question concerns a specific proof. Namely, the one presented in the problem below. In this problem we will establish the ""binomial series"" $$(1+x)^\alpha=\sum\limits_{k=0}^\infty \binom{\alpha}{k}x^k, |x|<1$$ for any $\alpha$ , by showing that $R_{n,0}(x)=0$ . The proof is in several steps, and uses the Cauchy and Lagrange forms as found in problem 10-21. (a) Use the ratio test to show that the series $\sum\limits_{k=0}^\infty \binom{\alpha}{k} r^k$ does indeed converge for  > $|r|<1$ . (b) Suppose first that $0\leq x<1$ . Show that $\lim\limits_{n\to\infty}  R_{n,0}(x)=0$ , but using Lagrange's form of the remainder, noticing that $(1+t)^{\alpha-n-1}\leq 1$ for $n+1>\alpha$ . (c) Now suppose that $-1<x<0$ ; the number $t$ in Cauchy's form of the remainder satisfies $-1<x_t\leq 0$ . Show that $$|x(1+t)^{\alpha-1}|\leq |x|M, \text{ where }  M=\max(1,(1+x)^{\alpha-1}),$$ and $$\left | \frac{x-t}{1+t} \right |=|x|\left ( \frac{1-t/x}{1+t} \right  )\leq |x|\tag{1}$$ Using Cauchy's form of the remainder, and the fact that $$(n+1)\binom{\alpha}{n+1} = \alpha \binom{\alpha-1}{n}$$ show that $\lim\limits_{n\to\infty} R_{n,0}(x)=0$ I was able to prove (a) and (b) and most of (c), up to (1). The issue is proving the last part, namely that $\lim\limits_{n\to\infty} R_{n,0}(x)=0$ . The solution manual has the following $$|R_{n,0}(0)|=\left | (n+1)\binom{\alpha}{n+1}x(1+t)^{\alpha-1} \left(\frac{x-t}{1+t}\right )^n \right |\tag{2}$$ $$\leq |x\alpha M|\cdot \left |\binom{\alpha-1}{n}x^n\right | \to 0  \text{ by part } a$$ First off, the $R_{n,0}(0)$ seems to be a typo. This would be the n-th order remainder evaluated at $0$ . What I have is the following $$R_{n,0}(x)=\binom{\alpha}{n+1}(1+t)^{\alpha-(n+1)}x^{n+1}$$ $$=\binom{\alpha}{n+1}\cdot x\cdot (1+t)^{\alpha-1}\cdot \left ( \frac{x}{1+t}\right )^n$$ How is (2) obtained?","The following problem is from Chapter 23 ""Infinite Series"" of Spivak's Calculus . Note that there are questions about convergence of the binomial series such as this one , but the current question concerns a specific proof. Namely, the one presented in the problem below. In this problem we will establish the ""binomial series"" for any , by showing that . The proof is in several steps, and uses the Cauchy and Lagrange forms as found in problem 10-21. (a) Use the ratio test to show that the series does indeed converge for  > . (b) Suppose first that . Show that , but using Lagrange's form of the remainder, noticing that for . (c) Now suppose that ; the number in Cauchy's form of the remainder satisfies . Show that and Using Cauchy's form of the remainder, and the fact that show that I was able to prove (a) and (b) and most of (c), up to (1). The issue is proving the last part, namely that . The solution manual has the following First off, the seems to be a typo. This would be the n-th order remainder evaluated at . What I have is the following How is (2) obtained?","(1+x)^\alpha=\sum\limits_{k=0}^\infty \binom{\alpha}{k}x^k, |x|<1 \alpha R_{n,0}(x)=0 \sum\limits_{k=0}^\infty \binom{\alpha}{k} r^k |r|<1 0\leq x<1 \lim\limits_{n\to\infty}
 R_{n,0}(x)=0 (1+t)^{\alpha-n-1}\leq 1 n+1>\alpha -1<x<0 t -1<x_t\leq 0 |x(1+t)^{\alpha-1}|\leq |x|M, \text{ where }
 M=\max(1,(1+x)^{\alpha-1}), \left | \frac{x-t}{1+t} \right |=|x|\left ( \frac{1-t/x}{1+t} \right
 )\leq |x|\tag{1} (n+1)\binom{\alpha}{n+1} = \alpha \binom{\alpha-1}{n} \lim\limits_{n\to\infty} R_{n,0}(x)=0 \lim\limits_{n\to\infty} R_{n,0}(x)=0 |R_{n,0}(0)|=\left | (n+1)\binom{\alpha}{n+1}x(1+t)^{\alpha-1} \left(\frac{x-t}{1+t}\right )^n \right |\tag{2} \leq |x\alpha M|\cdot \left |\binom{\alpha-1}{n}x^n\right | \to 0
 \text{ by part } a R_{n,0}(0) 0 R_{n,0}(x)=\binom{\alpha}{n+1}(1+t)^{\alpha-(n+1)}x^{n+1} =\binom{\alpha}{n+1}\cdot x\cdot (1+t)^{\alpha-1}\cdot \left ( \frac{x}{1+t}\right )^n","['calculus', 'sequences-and-series', 'proof-explanation']"
90,"If $\lim_{x\to x_0} f(x)g(x)=0$, then $\lim f(x)=0$ or $\lim g(x)=0$ at $x_0$. Is my counterexample fine?","If , then  or  at . Is my counterexample fine?",\lim_{x\to x_0} f(x)g(x)=0 \lim f(x)=0 \lim g(x)=0 x_0,"True/false: If $\lim_{x\to x_0} f(x)g(x)=0$ , then $\lim f(x)=0$ or $\lim g(x)=0$ at $x_0$ . I think the claim is not true! Can we define functions $f, g$ as follows: $$ f = \begin{cases}     1 & \text{if $x \in Q$}\\     0 & \text{otherwise} \end{cases} \\ g = \begin{cases}       0 & \text{if $x \in Q$}\\       1 & \text{otherwise} \end{cases} $$ So both $f, g$ diverge as Dirichlet function, but $fg=0$ which converges to $0$ at any $x_0$ . Is this counterexample right? Thanks!","True/false: If , then or at . I think the claim is not true! Can we define functions as follows: So both diverge as Dirichlet function, but which converges to at any . Is this counterexample right? Thanks!","\lim_{x\to x_0} f(x)g(x)=0 \lim f(x)=0 \lim g(x)=0 x_0 f, g 
f = \begin{cases}
    1 & \text{if x \in Q}\\
    0 & \text{otherwise}
\end{cases}
\\
g = \begin{cases}
      0 & \text{if x \in Q}\\
      1 & \text{otherwise}
\end{cases}
 f, g fg=0 0 x_0",['calculus']
91,Is there anything wrong with this proof that $\lim_{x\to0} \frac{\sin(x)}{x} = 1$?,Is there anything wrong with this proof that ?,\lim_{x\to0} \frac{\sin(x)}{x} = 1,"Definitions Since I'm asking for verification of a proof, I'll start with some basic definitions. I believe these definitions are  more or less equivalent to the ""standard"" definitions of curve length, radian, and sine/cosine: Let the length of a continuous curve $f(t): [a,b] \rightarrow \mathbb{R}^2$ be defined as $\lim_{N\to\infty}\sum_{i=1}^N \sqrt{(x(t_{i})-x(t_{i-1}))^2 + (y(t_{i})-y(t_{i-1}))^2}$ where $(x(t_{i}),y(t_{i}))=f(t_{i})$ and $t_{i} = a+i(b-a)/N$ . Let the angle measured by the counter-clockwise rotation of a point $P$ on a unit circle $C$ , such that the locus of $P$ generates a curve of length $\theta$ , be defined as $\theta$ radians. This angle measured by a clockwise rotation can be defined as $-\theta$ radians. Let the point on the Cartesian plane reached by rotating the point $(1,0)$ on the unit circle centered at $(0,0)$ by an angle of $\theta$ radians be defined as $(\cos(\theta), \sin(\theta))$ . Detailed Proof Consider the curve $f(t) = (\cos(t), \sin(t))$ for $t \in [0,1]$ . On the one hand, this is just a circular arc that subtends 1 radian from the center of a unit circle, and must therefore have length 1, by the definition of a radian. On the other hand, we can compute the length of the curve by the definition of curve length, which means that: $$\lim_{N\to\infty}\sum_{i=1}^N \sqrt{(\cos(i/N)-\cos((i-1)/N))^2 + (\sin(i/N)-\sin((i-1)/N))^2} = 1$$ Using basic trigonometric properties and algebra (see appendix for details), we can get: $$\lim_{N\to\infty}2N\sin(\frac{1}{2N}) = \lim_{N\to\infty}\frac{\sin(\frac{1}{2N})}{\frac{1}{2N}} = 1$$ Since we can always choose an $x\in\mathbb{R}$ such that $0<x<\frac{1}{2N}$ , we have that: $$\lim_{x\to0^+}\frac{\sin(x)}{x} = 1$$ Now we just observe that $\sin(x)$ is an odd function since we rotate starting from the x-axis, and therefore a counter-rotation reflects the y-coordinate along the x-axis, so we have: $$\lim_{x\to0^-}\frac{\sin(x)}{x} = \lim_{x\to0^+}\frac{\sin(-x)}{-x} = \lim_{x\to0^+}\frac{-\sin(x)}{-x} = \lim_{x\to0^+}\frac{\sin(x)}{x} = 1$$ Therefore: $$\lim_{x\to0}\frac{\sin(x)}{x} = 1$$ QED Note, however, that to show that the length of the curve is $\lim_{N\to\infty}2N\sin(\frac{1}{2N})$ , we don't need to use trigonometric properties or algebra at all. Consider the following diagram where we break a circular arc of length 1 into N=3 parts: The summation in the definition of the curve length is just the sum of the lengths of the orange segments in the above diagram. Notice that all the orange segments are of equal length since all the triangles are congruent isosceles triangles with two side lengths of 1 joined at an angle of 1/N = 1/3 radians. Therefore, the sum of the lengths of all segments is 2N multiplied by half the length of a single segment. To find half the length of a single orange segment, we split the bottom triangle into two right triangles and rotate the bottom half like so: This shows that half the length of a segment is $\sin(1/6)$ for N=3 and $\sin(\frac{1}{2N})$ in general. Therefore, the general the sum of the lengths is $2N\sin(\frac{1}{2N})$ . TL;DR Proof If I was showing this as a basic proof, with the same level of rigor as I usually see the proof of $\lim_{x\to0} \frac{\sin(x)}{x} = 1$ thrown around everywhere, it would be much more straightforward: Here's a diagram splitting an arc of length 1 into N equal parts: Observe that there are N parts each of length $2\sin(\frac{1}{2N})$ , so the total length is $2N\sin(\frac{1}{2N})$ . This implies that: $$\lim_{N\to\infty}2N\sin(\frac{1}{2N}) = \lim_{N\to\infty}\frac{\sin(\frac{1}{2N})}{\frac{1}{2N}} = 1$$ Now let $x = \frac{1}{2N}$ and we get: $$\lim_{x\to0}\frac{\sin(x)}{x} = 1$$ QED Motivation Why do I bring up this proof? If it's valid, I honestly think this is a much better proof than the common proof that relies on the squeeze theorem and geometric areas demonstrating that $\frac{\sin(\theta)}{2}\leq\frac{\theta}{2}\leq\frac{\tan(\theta)}{2}$ . The common proof hides the fact that we ultimately need to find the limit of an infinite process. To do this, it relies on the area of a circular sector. Ironically, many common proofs that we are taught for the area of a circle, if expressed more rigorously, are actually relying on this very same limit. You can even see this in the above diagrams. If we split a sector of arc length $\theta$ into N triangles, each one has area $\frac{1}{2}\sin(\frac{\theta}{N})$ and therefore the total area is: $$\lim_{N\to\infty}\frac{1}{2}N\sin(\frac{\theta}{N}) = \lim_{N\to\infty}\frac{1}{2}\frac{\sin(\frac{\theta}{N})}{\frac{1}{N}} = \frac{1}{2}\lim_{x\to0}\frac{\sin(\theta x)}{x}$$ And it is precisely the proof shown here that can be trivially extended to show that: $$\lim_{x\to0}\frac{\sin(\theta x)}{x} = \theta$$ So in short, I believe the advantages of using the proof here, if valid, as the typical proof for demonstrating that $\lim_{x\to0} \frac{\sin(x)}{x} = 1$ are: It avoids circular reasoning if we're relying on a modern notion of Archimedes' proof for the area of a circle. It avoids dealing with areas at all and instead directly deals with the ratio of two measures of length. It easily extends to a more general result, namely that $\lim_{x\to0} \frac{\sin(\theta x)}{x} = \theta$ . Appendix $$\lim_{N\to\infty}\sum_{i=1}^N \sqrt{(\cos(i/N)-\cos((i-1)/N))^2 + (\sin(i/N)-\sin((i-1)/N))^2}$$ The above expression will contain $(\cos(i/N)^2 + \sin(i/N)^2)$ and $(\cos((i-1)/N)^2 + \sin((i-1)/N)^2)$ so once we simplify we will get the expression: $$\lim_{N\to\infty}\sum_{i=1}^N \sqrt{2 - 2(\cos(i/N)\cos((i-1)/N) + \sin(i/N)\sin((i-1)/N))}$$ Since cosine is an even function and sine is an odd function, this is equivalent to: $$\lim_{N\to\infty}\sum_{i=1}^N \sqrt{2 - 2(\cos(i/N)\cos((1-i)/N) - \sin(i/N)\sin((1-i)/N))}$$ Since $\cos(a+b) = \cos(a)\cos(b) - \sin(a)\sin(b)$ , we get the expression: $$\lim_{N\to\infty}\sum_{i=1}^N \sqrt{2 - 2(\cos(i/N + (1-i)/N))} = \lim_{N\to\infty}\sum_{i=1}^N \sqrt{2 - 2\cos(1/N)}$$ Since $1 - \cos(\theta) = 2\sin^2(\theta / 2)$ , we get: $$\lim_{N\to\infty} N\sqrt{2*2\sin^2(\frac{1}{2N})}$$ And the final result: $$\lim_{N\to\infty} 2N\sin(\frac{1}{2N})$$","Definitions Since I'm asking for verification of a proof, I'll start with some basic definitions. I believe these definitions are  more or less equivalent to the ""standard"" definitions of curve length, radian, and sine/cosine: Let the length of a continuous curve be defined as where and . Let the angle measured by the counter-clockwise rotation of a point on a unit circle , such that the locus of generates a curve of length , be defined as radians. This angle measured by a clockwise rotation can be defined as radians. Let the point on the Cartesian plane reached by rotating the point on the unit circle centered at by an angle of radians be defined as . Detailed Proof Consider the curve for . On the one hand, this is just a circular arc that subtends 1 radian from the center of a unit circle, and must therefore have length 1, by the definition of a radian. On the other hand, we can compute the length of the curve by the definition of curve length, which means that: Using basic trigonometric properties and algebra (see appendix for details), we can get: Since we can always choose an such that , we have that: Now we just observe that is an odd function since we rotate starting from the x-axis, and therefore a counter-rotation reflects the y-coordinate along the x-axis, so we have: Therefore: QED Note, however, that to show that the length of the curve is , we don't need to use trigonometric properties or algebra at all. Consider the following diagram where we break a circular arc of length 1 into N=3 parts: The summation in the definition of the curve length is just the sum of the lengths of the orange segments in the above diagram. Notice that all the orange segments are of equal length since all the triangles are congruent isosceles triangles with two side lengths of 1 joined at an angle of 1/N = 1/3 radians. Therefore, the sum of the lengths of all segments is 2N multiplied by half the length of a single segment. To find half the length of a single orange segment, we split the bottom triangle into two right triangles and rotate the bottom half like so: This shows that half the length of a segment is for N=3 and in general. Therefore, the general the sum of the lengths is . TL;DR Proof If I was showing this as a basic proof, with the same level of rigor as I usually see the proof of thrown around everywhere, it would be much more straightforward: Here's a diagram splitting an arc of length 1 into N equal parts: Observe that there are N parts each of length , so the total length is . This implies that: Now let and we get: QED Motivation Why do I bring up this proof? If it's valid, I honestly think this is a much better proof than the common proof that relies on the squeeze theorem and geometric areas demonstrating that . The common proof hides the fact that we ultimately need to find the limit of an infinite process. To do this, it relies on the area of a circular sector. Ironically, many common proofs that we are taught for the area of a circle, if expressed more rigorously, are actually relying on this very same limit. You can even see this in the above diagrams. If we split a sector of arc length into N triangles, each one has area and therefore the total area is: And it is precisely the proof shown here that can be trivially extended to show that: So in short, I believe the advantages of using the proof here, if valid, as the typical proof for demonstrating that are: It avoids circular reasoning if we're relying on a modern notion of Archimedes' proof for the area of a circle. It avoids dealing with areas at all and instead directly deals with the ratio of two measures of length. It easily extends to a more general result, namely that . Appendix The above expression will contain and so once we simplify we will get the expression: Since cosine is an even function and sine is an odd function, this is equivalent to: Since , we get the expression: Since , we get: And the final result:","f(t): [a,b] \rightarrow \mathbb{R}^2 \lim_{N\to\infty}\sum_{i=1}^N \sqrt{(x(t_{i})-x(t_{i-1}))^2 + (y(t_{i})-y(t_{i-1}))^2} (x(t_{i}),y(t_{i}))=f(t_{i}) t_{i} = a+i(b-a)/N P C P \theta \theta -\theta (1,0) (0,0) \theta (\cos(\theta), \sin(\theta)) f(t) = (\cos(t), \sin(t)) t \in [0,1] \lim_{N\to\infty}\sum_{i=1}^N \sqrt{(\cos(i/N)-\cos((i-1)/N))^2 + (\sin(i/N)-\sin((i-1)/N))^2} = 1 \lim_{N\to\infty}2N\sin(\frac{1}{2N}) = \lim_{N\to\infty}\frac{\sin(\frac{1}{2N})}{\frac{1}{2N}} = 1 x\in\mathbb{R} 0<x<\frac{1}{2N} \lim_{x\to0^+}\frac{\sin(x)}{x} = 1 \sin(x) \lim_{x\to0^-}\frac{\sin(x)}{x} = \lim_{x\to0^+}\frac{\sin(-x)}{-x} = \lim_{x\to0^+}\frac{-\sin(x)}{-x} = \lim_{x\to0^+}\frac{\sin(x)}{x} = 1 \lim_{x\to0}\frac{\sin(x)}{x} = 1 \lim_{N\to\infty}2N\sin(\frac{1}{2N}) \sin(1/6) \sin(\frac{1}{2N}) 2N\sin(\frac{1}{2N}) \lim_{x\to0} \frac{\sin(x)}{x} = 1 2\sin(\frac{1}{2N}) 2N\sin(\frac{1}{2N}) \lim_{N\to\infty}2N\sin(\frac{1}{2N}) = \lim_{N\to\infty}\frac{\sin(\frac{1}{2N})}{\frac{1}{2N}} = 1 x = \frac{1}{2N} \lim_{x\to0}\frac{\sin(x)}{x} = 1 \frac{\sin(\theta)}{2}\leq\frac{\theta}{2}\leq\frac{\tan(\theta)}{2} \theta \frac{1}{2}\sin(\frac{\theta}{N}) \lim_{N\to\infty}\frac{1}{2}N\sin(\frac{\theta}{N}) = \lim_{N\to\infty}\frac{1}{2}\frac{\sin(\frac{\theta}{N})}{\frac{1}{N}} = \frac{1}{2}\lim_{x\to0}\frac{\sin(\theta x)}{x} \lim_{x\to0}\frac{\sin(\theta x)}{x} = \theta \lim_{x\to0} \frac{\sin(x)}{x} = 1 \lim_{x\to0} \frac{\sin(\theta x)}{x} = \theta \lim_{N\to\infty}\sum_{i=1}^N \sqrt{(\cos(i/N)-\cos((i-1)/N))^2 + (\sin(i/N)-\sin((i-1)/N))^2} (\cos(i/N)^2 + \sin(i/N)^2) (\cos((i-1)/N)^2 + \sin((i-1)/N)^2) \lim_{N\to\infty}\sum_{i=1}^N \sqrt{2 - 2(\cos(i/N)\cos((i-1)/N) + \sin(i/N)\sin((i-1)/N))} \lim_{N\to\infty}\sum_{i=1}^N \sqrt{2 - 2(\cos(i/N)\cos((1-i)/N) - \sin(i/N)\sin((1-i)/N))} \cos(a+b) = \cos(a)\cos(b) - \sin(a)\sin(b) \lim_{N\to\infty}\sum_{i=1}^N \sqrt{2 - 2(\cos(i/N + (1-i)/N))} = \lim_{N\to\infty}\sum_{i=1}^N \sqrt{2 - 2\cos(1/N)} 1 - \cos(\theta) = 2\sin^2(\theta / 2) \lim_{N\to\infty} N\sqrt{2*2\sin^2(\frac{1}{2N})} \lim_{N\to\infty} 2N\sin(\frac{1}{2N})","['calculus', 'geometry', 'solution-verification', 'alternative-proof', 'limits-without-lhopital']"
92,A “general definition” of Riemann sum,A “general definition” of Riemann sum,,"Suppose $f$ is Riemann integerable in $[0,1]$ .Prove that $$\lim_{n\rightarrow \infty}\frac{1}{\phi(n)}\sum_{1\leq k\leq n,(k,n)=1}f\left(\frac{k}{n}\right)=\int_0^1f(x)dx$$ Here $\phi(n)$ is Euler's function My attempt: Let $\mu$ be the Möbius function. $$\begin{align} LHS-RHS&=\frac{\sum_{k=1}^{n}\sum_{d|(k,n)}\mu(d)f(\frac{k}{n})}{n\sum_{d|n}\frac{\mu(d)}{d}}-\frac{1}{n}\sum_{k=1}^{n}f(\frac{k}{n})\\ &=\frac{1}{n}\frac{\sum_{k=1}^{n}(\sum_{d|(k,n)}\mu(d)-\sum_{d|n}\frac{\mu(d)}{d})f(\frac{k}{n})}{\sum_{d|n}\frac{\mu(d)}{d}} \end{align} $$ But it seems not to work.Does anyone know how to prove it?Thank you",Suppose is Riemann integerable in .Prove that Here is Euler's function My attempt: Let be the Möbius function. But it seems not to work.Does anyone know how to prove it?Thank you,"f [0,1] \lim_{n\rightarrow \infty}\frac{1}{\phi(n)}\sum_{1\leq k\leq n,(k,n)=1}f\left(\frac{k}{n}\right)=\int_0^1f(x)dx \phi(n) \mu \begin{align}
LHS-RHS&=\frac{\sum_{k=1}^{n}\sum_{d|(k,n)}\mu(d)f(\frac{k}{n})}{n\sum_{d|n}\frac{\mu(d)}{d}}-\frac{1}{n}\sum_{k=1}^{n}f(\frac{k}{n})\\
&=\frac{1}{n}\frac{\sum_{k=1}^{n}(\sum_{d|(k,n)}\mu(d)-\sum_{d|n}\frac{\mu(d)}{d})f(\frac{k}{n})}{\sum_{d|n}\frac{\mu(d)}{d}}
\end{align}
",['calculus']
93,Extremising $\int_0^1 f(x) f(1-x) \ \mathrm{d}x$ subject to length of $f$ and endpoints,Extremising  subject to length of  and endpoints,\int_0^1 f(x) f(1-x) \ \mathrm{d}x f,"I have recently learnt some Calculus of Variations and was trying to apply this to a question I made: Over all functions $f: [0, 1] \to \mathbb{R}$ satisfying $f(0) = f(1) = 0$ with fixed curve length $\ell \geq 1$ (i.e. $\int_0^1 \sqrt{1 + (f'(x))^2} \ \mathrm{d}x = \ell$ ), find $f$ which maximise and minimise \begin{align*} \int_0^1 f(x) f(1 - x) \ \mathrm{d}x. \end{align*} Ordinarily, I would proceed by Lagrange Multipliers and use Euler-Lagrange equations to solve for $f$ , but I'm not sure how this would work with $f$ being shifted above. I considered rederiving the Euler-Lagrange equation for this as well, but the fact that it is a shifted argument makes me think this would likely not be nice to work with. Any help would be appreciated, thanks!","I have recently learnt some Calculus of Variations and was trying to apply this to a question I made: Over all functions satisfying with fixed curve length (i.e. ), find which maximise and minimise Ordinarily, I would proceed by Lagrange Multipliers and use Euler-Lagrange equations to solve for , but I'm not sure how this would work with being shifted above. I considered rederiving the Euler-Lagrange equation for this as well, but the fact that it is a shifted argument makes me think this would likely not be nice to work with. Any help would be appreciated, thanks!","f: [0, 1] \to \mathbb{R} f(0) = f(1) = 0 \ell \geq 1 \int_0^1 \sqrt{1 + (f'(x))^2} \ \mathrm{d}x = \ell f \begin{align*}
\int_0^1 f(x) f(1 - x) \ \mathrm{d}x.
\end{align*} f f","['calculus', 'optimization', 'calculus-of-variations', 'constraints', 'euler-lagrange-equation']"
94,Question about finding roots of a polynomial and studying the nature,Question about finding roots of a polynomial and studying the nature,,"The number of real roots of the equation $1+\frac{x}{1}+\frac{x^{2}}{2}+\frac{x^{2}}{3}+\cdots+\frac{x^{7}}{7}=0$ (without factorial) is My work Let, $\mathrm{f}(\mathrm{x})=1+\frac{x}{1}+\frac{x^{2}}{2}+\frac{x^{3}}{3}+\cdots+\frac{x^{6}}{6}$ [ Let, f has a minimum at $x=x_{0},$ where then $f^{\prime}\left(x_{0}\right)=0$ ] $\Rightarrow 1+x_{0}+x_{0}^{2}+x_{0}^{3}+x_{0}^{4}+x_{0}^{5}=0$ $\Rightarrow \frac{x_{0}^{6}-1}{x_{0}-1}=0$ $\Rightarrow \frac{\left(x_{0}^{3}-1\right)\left(x_{0}^{3}+1\right)}{x_{0}-1}=0$ $\Rightarrow\left(x_{0}^{2}+x_{0}+1\right)\left(x_{0}^{2}-x_{0}+1\right)\left(x_{0}+1\right)=0$ Which has a real root $x_{0}=-1$ But, $f(-1)=1-1+\left(\frac{1}{2}-\frac{1}{3}\right)+\left(\frac{1}{4}-\frac{1}{5}\right)+\frac{1}{6}>0$ The $f(x)>0$ and hence $f$ has no real zeros. Now let, $g(x)=1+\frac{x}{1}+\frac{x^{2}}{2}+\frac{x^{3}}{3}+\cdots+\frac{x^{7}}{7}$ An odd degree polynomial has at least one real root. If our polynomial g has more than one zero, say $x_{1}, x_{2}$ Then by Role's theorem in $\left(x_{1}, x_{2}\right)$ we have $^{\prime} x_{3}$ ' such that $\mathrm{g}^{\prime}\left(x_{3}\right)=0$ $\Rightarrow 1+x_{3}+x_{3}^{2}+\cdots+x_{3}^{6}=0$ But this has no real zeros. Hence the given polynomial has exactly one real zero. correct me if i Am wrong","The number of real roots of the equation (without factorial) is My work Let, [ Let, f has a minimum at where then ] Which has a real root But, The and hence has no real zeros. Now let, An odd degree polynomial has at least one real root. If our polynomial g has more than one zero, say Then by Role's theorem in we have ' such that But this has no real zeros. Hence the given polynomial has exactly one real zero. correct me if i Am wrong","1+\frac{x}{1}+\frac{x^{2}}{2}+\frac{x^{2}}{3}+\cdots+\frac{x^{7}}{7}=0 \mathrm{f}(\mathrm{x})=1+\frac{x}{1}+\frac{x^{2}}{2}+\frac{x^{3}}{3}+\cdots+\frac{x^{6}}{6} x=x_{0}, f^{\prime}\left(x_{0}\right)=0 \Rightarrow 1+x_{0}+x_{0}^{2}+x_{0}^{3}+x_{0}^{4}+x_{0}^{5}=0 \Rightarrow \frac{x_{0}^{6}-1}{x_{0}-1}=0 \Rightarrow \frac{\left(x_{0}^{3}-1\right)\left(x_{0}^{3}+1\right)}{x_{0}-1}=0 \Rightarrow\left(x_{0}^{2}+x_{0}+1\right)\left(x_{0}^{2}-x_{0}+1\right)\left(x_{0}+1\right)=0 x_{0}=-1 f(-1)=1-1+\left(\frac{1}{2}-\frac{1}{3}\right)+\left(\frac{1}{4}-\frac{1}{5}\right)+\frac{1}{6}>0 f(x)>0 f g(x)=1+\frac{x}{1}+\frac{x^{2}}{2}+\frac{x^{3}}{3}+\cdots+\frac{x^{7}}{7} x_{1}, x_{2} \left(x_{1}, x_{2}\right) ^{\prime} x_{3} \mathrm{g}^{\prime}\left(x_{3}\right)=0 \Rightarrow 1+x_{3}+x_{3}^{2}+\cdots+x_{3}^{6}=0",['calculus']
95,Integrating $x^{x^x}$,Integrating,x^{x^x},"Although one cannot find an elementary antiderivative of $f(x)=x^x$ , we can still give a series representation for $\int_0^1 x^x dx$ , namely: $$I_1=\sum_{n=1}^\infty \frac{(-1)^{n+1}}{n^n}=0.78343\ldots$$ One can even find an expression for the complete antiderivative in terms of infinite sums and the incomplete gamma function $\Gamma(a,x)$ : $$\int x^x dx =\sum_{n=1}^\infty \left(\frac{(-1)^{n+1}\Gamma(-n\ln(x),n)}{n^n \Gamma(n)}\right)+C$$ Considering special, non-elementary function, series, infinite products, etc. , is this also possible for $\int_0^1 x^{x^x} dx$ ? Thank you in advance!","Although one cannot find an elementary antiderivative of , we can still give a series representation for , namely: One can even find an expression for the complete antiderivative in terms of infinite sums and the incomplete gamma function : Considering special, non-elementary function, series, infinite products, etc. , is this also possible for ? Thank you in advance!","f(x)=x^x \int_0^1 x^x dx I_1=\sum_{n=1}^\infty \frac{(-1)^{n+1}}{n^n}=0.78343\ldots \Gamma(a,x) \int x^x dx =\sum_{n=1}^\infty \left(\frac{(-1)^{n+1}\Gamma(-n\ln(x),n)}{n^n \Gamma(n)}\right)+C \int_0^1 x^{x^x} dx","['calculus', 'integration', 'definite-integrals', 'gamma-function', 'tetration']"
96,Irresistible: $T(p)=\int_0^{\pi/2}x\tan(x)^p\mathrm dx$ for $-2<p<1$,Irresistible:  for,T(p)=\int_0^{\pi/2}x\tan(x)^p\mathrm dx -2<p<1,"I am working on a challenge posed to me by the book Irresistible Integrals , finding a closed form for the integral $$T(p)=\int_0^{\pi/2}x\tan(x)^p\mathrm dx,\qquad -2<p<1$$ I am confident that a closed form exists, because the book tells me to find it. My efforts: $$T(p)=\int_0^{\pi/2}x\tan(x)^p\mathrm dx$$ $x=\arctan u$ : $$I(p)=\int_0^\infty \frac{u^p}{1+u^2}\arctan u\,\mathrm du$$ $u=\frac1t$ : $$T(p)=\int_0^{\infty}\frac{t^{-p}}{1+\frac{1}{t^2}}\arctan(1/t)\frac{\mathrm dt}{t^2}$$ $$T(p)=\frac\pi2\int_0^{\infty}\frac{t^{2-p}}{1+t^2}\mathrm dt-\int_0^\infty \frac{t^{2-p}}{1+t^2}\arctan t\,\mathrm dt$$ $$T(p)=\frac\pi2J(p)-T(2-p)$$ Then focusing on $$J(p)=\int_0^{\infty}\frac{t^{2-p}}{1+t^2}\mathrm dt$$ As I have shown before, this integral relates to the Beta function: $$\int_0^{\infty}\frac{t^{2b-1}\mathrm dt}{(1+t^2)^{a+b}}=\frac12\mathrm{B}(a,b)=\frac{\Gamma(a)\Gamma(b)}{2\Gamma(a+b)}$$ So $$J(p)=\frac12\Gamma\left(\frac{p-1}2\right)\Gamma\left(\frac{3-p}2\right)$$ But this only works for $p\in(1,3)$ , so I'm thinking that my functional equation is entirely false. Does anyone know how to evaluate this integral? Thanks. Edit: As was noted in the comments, we have $$T(p)=\frac\pi2\int_0^\infty \frac{t^{-p}}{1+t^2}\mathrm dt-\int_0^\infty \frac{t^{-p}}{1+t^2}\arctan t\,\mathrm dt$$ So redefining $J(p)=\int_0^\infty \frac{t^{-p}}{1+t^2}\mathrm dt$ We have that $$J(p)=\frac12\Gamma\left(\frac{1-p}2\right)\Gamma\left(\frac{1+p}2\right)$$ Then if we recall that $$\Gamma(s)\Gamma(1-s)=\frac\pi{\sin\pi s}$$ $$\Gamma(s/2)\Gamma(1-s/2)=\frac\pi{\sin\frac{\pi s}2}$$ $$\Gamma\left(\frac{1+s}2\right)\Gamma\left(\frac{1-s}2\right)=\frac\pi{\sin\frac{\pi(s+1)}2}=\frac\pi{\cos\frac{\pi s}2}$$ So $$T(p)+T(-p)=\frac{\pi^2}{4\cos\frac{\pi p}2}$$","I am working on a challenge posed to me by the book Irresistible Integrals , finding a closed form for the integral I am confident that a closed form exists, because the book tells me to find it. My efforts: : : Then focusing on As I have shown before, this integral relates to the Beta function: So But this only works for , so I'm thinking that my functional equation is entirely false. Does anyone know how to evaluate this integral? Thanks. Edit: As was noted in the comments, we have So redefining We have that Then if we recall that So","T(p)=\int_0^{\pi/2}x\tan(x)^p\mathrm dx,\qquad -2<p<1 T(p)=\int_0^{\pi/2}x\tan(x)^p\mathrm dx x=\arctan u I(p)=\int_0^\infty \frac{u^p}{1+u^2}\arctan u\,\mathrm du u=\frac1t T(p)=\int_0^{\infty}\frac{t^{-p}}{1+\frac{1}{t^2}}\arctan(1/t)\frac{\mathrm dt}{t^2} T(p)=\frac\pi2\int_0^{\infty}\frac{t^{2-p}}{1+t^2}\mathrm dt-\int_0^\infty \frac{t^{2-p}}{1+t^2}\arctan t\,\mathrm dt T(p)=\frac\pi2J(p)-T(2-p) J(p)=\int_0^{\infty}\frac{t^{2-p}}{1+t^2}\mathrm dt \int_0^{\infty}\frac{t^{2b-1}\mathrm dt}{(1+t^2)^{a+b}}=\frac12\mathrm{B}(a,b)=\frac{\Gamma(a)\Gamma(b)}{2\Gamma(a+b)} J(p)=\frac12\Gamma\left(\frac{p-1}2\right)\Gamma\left(\frac{3-p}2\right) p\in(1,3) T(p)=\frac\pi2\int_0^\infty \frac{t^{-p}}{1+t^2}\mathrm dt-\int_0^\infty \frac{t^{-p}}{1+t^2}\arctan t\,\mathrm dt J(p)=\int_0^\infty \frac{t^{-p}}{1+t^2}\mathrm dt J(p)=\frac12\Gamma\left(\frac{1-p}2\right)\Gamma\left(\frac{1+p}2\right) \Gamma(s)\Gamma(1-s)=\frac\pi{\sin\pi s} \Gamma(s/2)\Gamma(1-s/2)=\frac\pi{\sin\frac{\pi s}2} \Gamma\left(\frac{1+s}2\right)\Gamma\left(\frac{1-s}2\right)=\frac\pi{\sin\frac{\pi(s+1)}2}=\frac\pi{\cos\frac{\pi s}2} T(p)+T(-p)=\frac{\pi^2}{4\cos\frac{\pi p}2}","['calculus', 'integration', 'closed-form']"
97,Prove that $2\left(\frac{6}{e}\right)^e>17$,Prove that,2\left(\frac{6}{e}\right)^e>17,"I was doing some exercises in my calculus textbook and to finish one, I need to prove that $$2\left(\frac{6}{e}\right)^e>17.$$ This is supposed to be simple (,,easy to notice'', the textbook says). However, I cannot figure it out withour a calculator (17.2 is about the exact value of the left hand side). My only idea was to use Bernoulli inequality: $$2\left(\frac{6}{e}\right)^e=2\left(1+\left(\frac{6}{e}-1\right)\right)^e\ge 2\left(1+e\left(\frac{6}{e}-1\right)\right)=2(7-e)=14-2e$$ but this is to weak. Could anybody give me some hints?","I was doing some exercises in my calculus textbook and to finish one, I need to prove that $$2\left(\frac{6}{e}\right)^e>17.$$ This is supposed to be simple (,,easy to notice'', the textbook says). However, I cannot figure it out withour a calculator (17.2 is about the exact value of the left hand side). My only idea was to use Bernoulli inequality: $$2\left(\frac{6}{e}\right)^e=2\left(1+\left(\frac{6}{e}-1\right)\right)^e\ge 2\left(1+e\left(\frac{6}{e}-1\right)\right)=2(7-e)=14-2e$$ but this is to weak. Could anybody give me some hints?",,"['calculus', 'inequality']"
98,Substitution in integration,Substitution in integration,,"When we write: $$ \int f(y)\frac {dy}{dx}dx = \int f(y) dy$$ to me the intuition is that the $dx$s cancel (I am aware this is not a fully mathematically correct way to think about it). This arises every time we solve a separable first order ordinary differential equation. But in changing variables of the integration, there must be a substitution. If there were limits on the first integral they would need to change for the second. So what substitution are we technically making?","When we write: $$ \int f(y)\frac {dy}{dx}dx = \int f(y) dy$$ to me the intuition is that the $dx$s cancel (I am aware this is not a fully mathematically correct way to think about it). This arises every time we solve a separable first order ordinary differential equation. But in changing variables of the integration, there must be a substitution. If there were limits on the first integral they would need to change for the second. So what substitution are we technically making?",,"['calculus', 'ordinary-differential-equations']"
99,Accurate identities related to $\sum\limits_{n=0}^{\infty}\frac{(2n)!}{(n!)^3}x^n$ and $\sum\limits_{n=0}^{\infty}\frac{(2n)!}{(n!)^4}x^n$,Accurate identities related to  and,\sum\limits_{n=0}^{\infty}\frac{(2n)!}{(n!)^3}x^n \sum\limits_{n=0}^{\infty}\frac{(2n)!}{(n!)^4}x^n,"Some time ago while playing around with maths, I derived the following pair of formulae: $$\sum_{n=0}^{\infty}\frac{(2n)!}{(n!)^3}x^n=\frac{2}{\pi}\int_0^{\frac{\pi}{2}}e^{4x\cos^2\theta}\;d\theta\tag{1}$$ $$\sum_{n=0}^{\infty}\frac{(2n)!}{(n!)^4}x^n=\frac{2}{\pi}\int_0^{\frac{\pi}{2}}I_{0}\left(4\sqrt{x}\cos\theta\right)\;d\theta\tag{2}$$ as well as the following generalizations: $$\sum_{n=0}^{\infty}\frac{(2n)!^m}{(n!)^{2m+1}}x^n=\left(\frac{2}{\pi}\right)^m\int_0^{\frac{\pi}{2}}\ldots\int_0^{\frac{\pi}{2}}e^{4^{m}x\prod_{k=1}^{m}\cos^2\theta_k}\;d\theta_1\ldots\;d\theta_m\tag{3}$$ $$\sum_{n=0}^{\infty}\frac{(2n)!^m}{(n!)^{2m+2}}x^n=\left(\frac{2}{\pi}\right)^m\int_0^{\frac{\pi}{2}}\ldots\int_0^{\frac{\pi}{2}}I_0\left(2^{m+1}\sqrt{x}\prod_{k=1}^{m}\cos\theta_k\right)\;d\theta_1\ldots\;d\theta_m\tag{4}$$ (In what follows $J_0$ and $I_0$ and Bessel are modified Bessel functions respectively of the first kind). The method I used to find these was extremely formal (in the second sense of this answer) and I know that such derivations can give spurious results, so my question is: are these formulae correct? (Note that I have asked them in the same question because my derivation below proved them together, requiring one to prove the other. These series could be written in terms of binomial coefficients but this page does not seem to contain any formulae like these involving powers of binomial coefficients in the numerator of the summand.) [ Edit : I have just noticed this question which appears to mention a special case of $(2)$ (modified by the formal identity $J_0(ix)=I_0(x)$ ).] Outline of formal derivation : I started with the Laplace transform formula $L\left[t^{\frac{n}{2}}\int_{0}^{\infty}u^{-\frac{n}{2}}J_n(2\sqrt{ut})f(u)\;du\right](s)=\frac{1}{s^{n+1}}L[f(t)]\left(\frac{1}{s}\right)$ from Borelli and Coleman's Differential Equations: a modelling perspective (the proof is not too difficult), which formally implies that if $f(t)=\sum\limits_{n=0}^{\infty}a_{n}t^n$ then: $$\sum_{n=0}^{\infty}\frac{a_{n}t^n}{n!}=L^{-1}\left[\frac{f(\frac{1}{s})}{s}\right](t)=\int_0^\infty J_0(2\sqrt{ut})L^{-1}[f(s)](u)\;du\tag{5}$$ Taking $f(t)=\sqrt{\frac{1}{1-t}}=\sum\limits_{n=0}^{\infty}\frac{(2n)!t^n}{(n!)^{2}4^n}$ (a binomial series ) in $(5)$ , by convolution and shifting we get $\sum\limits_{n=0}^{\infty}\frac{(2n)!t^n}{(n!)^{3}4^n}=L^{-1}\left[\frac{1}{s}\sqrt{\frac{1}{1-\frac{1}{s}}}\right](t)=\frac{1}{\pi}\int_0^t \frac{e^u}{\sqrt{u(t-u)}}\;du$ , and $\color{#ff0000}{(1)}$ follows by setting $u=\frac{t\left(\cos(2\theta)+1\right)}{2}$ . Taking $f(t)=\sum\limits_{n=0}^{\infty}\frac{(-1)^n(2n)!}{(n!)^3}t^n$ in $(5)$ and using $(1)$ and elementary trigonometric identities gives $\sum\limits_{n=0}^{\infty}\frac{(-1)^n(2n)!}{(n!)^4}t^n=\frac{1}{\pi}\int_0^\infty J_0(2\sqrt{ut})L^{-1}\left[e^{-2s}\int_0^\pi e^{-2s\cos\theta}\;d\theta\right](u)\;du$ . But making the transformation $\theta=\cos^{-1}\left(\frac{t-2}{2}\right)$ , the argument of the inverse Laplace transform becomes $\int_0^4\frac{e^{-st}}{\sqrt{4t-t^2}}\;dt$ which is the Laplace transform of $\frac{1-H(t-4)}{\sqrt{4t-t^2}}$ ( $H(t)$ is the Heaviside function ) so our formula reduces to $\frac{1}{\pi}\int_0^\infty J_0(2\sqrt{ut})\frac{1-H(u-4)}{\sqrt{4u-u^2}}\;du$ which by taking $u=2(1+\cos2\theta)$ and formally setting $J_0(ix)=I_0(x)$ gives $\color{#ff0000}{(2)}$ . Using the resulting formula $\sum\limits_{n=0}^{\infty}\frac{(2n)!}{(n!)^4}(-1)^n t^{2n}=\frac{2}{\pi}\int_0^{\frac{\pi}{2}}J_{0}\left(4t\cos\theta\right)\;d\theta$ and taking the Laplace transform of both sides and interchanging integral and Laplace transform gives $\sum\limits_{n=0}^{\infty}\frac{(2n)!^2}{(n!)^4}\frac{(-1)^n}{ s^{2n+1}}=\frac{2}{\pi}\int_0^{\frac{\pi}{2}}\frac{1}{\sqrt{s^2+16\cos^2\theta}}\;d\theta$ , which gives: $$\sum_{n=0}^{\infty}\frac{(2n)!^2}{(n!)^4}(-1)^n t^n=\frac{2}{\pi}\int_0^{\frac{\pi}{2}}\frac{1}{\sqrt{1+16t\cos^2\theta}}\;d\theta\tag{6}$$ This is an elliptic integral I believe. Applying $(5)$ to this, interchanging integral and Laplace transform, applying convolution and shifting, and taking $u=\frac{t}{2}(1+\cos2\varphi)$ lets us arrive at: $$\sum_{n=0}^\infty \frac{(2n)!^2}{(n!)^5}x^n=\frac{4}{\pi^2}\int_0^{\frac{\pi}{2}}\int_0^{\frac{\pi}{2}}e^{16x\cos^2\theta\cos^2\varphi}\;d\theta\;d\varphi\tag{7}$$ The methods of deriving $(2)$ from $(1)$ and of deriving $(7)$ from $(2)$ may be easily extended to an induction deriving $\color{#ff0000}{(3)}$ and $\color{#ff0000}{(4)}$ . Accuracy : The error of $(1)$ for $x=0.02$ according to Wolfram|Alpha is on the order of $10^{-16}$ which seems extremely small although I am not sure about the sizes of errors in Wolfram|Alpha (and there are very accurate near identities ; I have also derived elsewhere identities with errors of $10^{-16}$ which appear to be mere approximations). I am not sure about the accuracy of $(2)$ , $(3)$ , or $(4)$ but noting the relation of $(6)$ to elliptic integrals we get the following $AGM$ formula: $$\sum_{n=0}^{\infty}\frac{(2n)!^2}{(n!)^4}x^n=\frac{1}{AGM(\sqrt{1-16x},1)}\tag{8}$$ whose error at $x=0.02$ appears to be also on the order of $10^{-16}$ . This seems to imply that the derivation may still be on the right track when $(6)$ is shown. So my question is: Are some or all of the above (highlighted) identities correct?","Some time ago while playing around with maths, I derived the following pair of formulae: as well as the following generalizations: (In what follows and and Bessel are modified Bessel functions respectively of the first kind). The method I used to find these was extremely formal (in the second sense of this answer) and I know that such derivations can give spurious results, so my question is: are these formulae correct? (Note that I have asked them in the same question because my derivation below proved them together, requiring one to prove the other. These series could be written in terms of binomial coefficients but this page does not seem to contain any formulae like these involving powers of binomial coefficients in the numerator of the summand.) [ Edit : I have just noticed this question which appears to mention a special case of (modified by the formal identity ).] Outline of formal derivation : I started with the Laplace transform formula from Borelli and Coleman's Differential Equations: a modelling perspective (the proof is not too difficult), which formally implies that if then: Taking (a binomial series ) in , by convolution and shifting we get , and follows by setting . Taking in and using and elementary trigonometric identities gives . But making the transformation , the argument of the inverse Laplace transform becomes which is the Laplace transform of ( is the Heaviside function ) so our formula reduces to which by taking and formally setting gives . Using the resulting formula and taking the Laplace transform of both sides and interchanging integral and Laplace transform gives , which gives: This is an elliptic integral I believe. Applying to this, interchanging integral and Laplace transform, applying convolution and shifting, and taking lets us arrive at: The methods of deriving from and of deriving from may be easily extended to an induction deriving and . Accuracy : The error of for according to Wolfram|Alpha is on the order of which seems extremely small although I am not sure about the sizes of errors in Wolfram|Alpha (and there are very accurate near identities ; I have also derived elsewhere identities with errors of which appear to be mere approximations). I am not sure about the accuracy of , , or but noting the relation of to elliptic integrals we get the following formula: whose error at appears to be also on the order of . This seems to imply that the derivation may still be on the right track when is shown. So my question is: Are some or all of the above (highlighted) identities correct?","\sum_{n=0}^{\infty}\frac{(2n)!}{(n!)^3}x^n=\frac{2}{\pi}\int_0^{\frac{\pi}{2}}e^{4x\cos^2\theta}\;d\theta\tag{1} \sum_{n=0}^{\infty}\frac{(2n)!}{(n!)^4}x^n=\frac{2}{\pi}\int_0^{\frac{\pi}{2}}I_{0}\left(4\sqrt{x}\cos\theta\right)\;d\theta\tag{2} \sum_{n=0}^{\infty}\frac{(2n)!^m}{(n!)^{2m+1}}x^n=\left(\frac{2}{\pi}\right)^m\int_0^{\frac{\pi}{2}}\ldots\int_0^{\frac{\pi}{2}}e^{4^{m}x\prod_{k=1}^{m}\cos^2\theta_k}\;d\theta_1\ldots\;d\theta_m\tag{3} \sum_{n=0}^{\infty}\frac{(2n)!^m}{(n!)^{2m+2}}x^n=\left(\frac{2}{\pi}\right)^m\int_0^{\frac{\pi}{2}}\ldots\int_0^{\frac{\pi}{2}}I_0\left(2^{m+1}\sqrt{x}\prod_{k=1}^{m}\cos\theta_k\right)\;d\theta_1\ldots\;d\theta_m\tag{4} J_0 I_0 (2) J_0(ix)=I_0(x) L\left[t^{\frac{n}{2}}\int_{0}^{\infty}u^{-\frac{n}{2}}J_n(2\sqrt{ut})f(u)\;du\right](s)=\frac{1}{s^{n+1}}L[f(t)]\left(\frac{1}{s}\right) f(t)=\sum\limits_{n=0}^{\infty}a_{n}t^n \sum_{n=0}^{\infty}\frac{a_{n}t^n}{n!}=L^{-1}\left[\frac{f(\frac{1}{s})}{s}\right](t)=\int_0^\infty J_0(2\sqrt{ut})L^{-1}[f(s)](u)\;du\tag{5} f(t)=\sqrt{\frac{1}{1-t}}=\sum\limits_{n=0}^{\infty}\frac{(2n)!t^n}{(n!)^{2}4^n} (5) \sum\limits_{n=0}^{\infty}\frac{(2n)!t^n}{(n!)^{3}4^n}=L^{-1}\left[\frac{1}{s}\sqrt{\frac{1}{1-\frac{1}{s}}}\right](t)=\frac{1}{\pi}\int_0^t \frac{e^u}{\sqrt{u(t-u)}}\;du \color{#ff0000}{(1)} u=\frac{t\left(\cos(2\theta)+1\right)}{2} f(t)=\sum\limits_{n=0}^{\infty}\frac{(-1)^n(2n)!}{(n!)^3}t^n (5) (1) \sum\limits_{n=0}^{\infty}\frac{(-1)^n(2n)!}{(n!)^4}t^n=\frac{1}{\pi}\int_0^\infty J_0(2\sqrt{ut})L^{-1}\left[e^{-2s}\int_0^\pi e^{-2s\cos\theta}\;d\theta\right](u)\;du \theta=\cos^{-1}\left(\frac{t-2}{2}\right) \int_0^4\frac{e^{-st}}{\sqrt{4t-t^2}}\;dt \frac{1-H(t-4)}{\sqrt{4t-t^2}} H(t) \frac{1}{\pi}\int_0^\infty J_0(2\sqrt{ut})\frac{1-H(u-4)}{\sqrt{4u-u^2}}\;du u=2(1+\cos2\theta) J_0(ix)=I_0(x) \color{#ff0000}{(2)} \sum\limits_{n=0}^{\infty}\frac{(2n)!}{(n!)^4}(-1)^n t^{2n}=\frac{2}{\pi}\int_0^{\frac{\pi}{2}}J_{0}\left(4t\cos\theta\right)\;d\theta \sum\limits_{n=0}^{\infty}\frac{(2n)!^2}{(n!)^4}\frac{(-1)^n}{ s^{2n+1}}=\frac{2}{\pi}\int_0^{\frac{\pi}{2}}\frac{1}{\sqrt{s^2+16\cos^2\theta}}\;d\theta \sum_{n=0}^{\infty}\frac{(2n)!^2}{(n!)^4}(-1)^n t^n=\frac{2}{\pi}\int_0^{\frac{\pi}{2}}\frac{1}{\sqrt{1+16t\cos^2\theta}}\;d\theta\tag{6} (5) u=\frac{t}{2}(1+\cos2\varphi) \sum_{n=0}^\infty \frac{(2n)!^2}{(n!)^5}x^n=\frac{4}{\pi^2}\int_0^{\frac{\pi}{2}}\int_0^{\frac{\pi}{2}}e^{16x\cos^2\theta\cos^2\varphi}\;d\theta\;d\varphi\tag{7} (2) (1) (7) (2) \color{#ff0000}{(3)} \color{#ff0000}{(4)} (1) x=0.02 10^{-16} 10^{-16} (2) (3) (4) (6) AGM \sum_{n=0}^{\infty}\frac{(2n)!^2}{(n!)^4}x^n=\frac{1}{AGM(\sqrt{1-16x},1)}\tag{8} x=0.02 10^{-16} (6)","['calculus', 'sequences-and-series', 'laplace-transform', 'bessel-functions', 'elliptic-integrals']"
