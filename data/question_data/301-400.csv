,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Is this function nowhere analytic?,Is this function nowhere analytic?,,"One usually sees $f(x):=\exp\frac{-1}{x^2}$ as an example of a $C^\infty$ function that is not analytic, having one point of non-analyticity (the point $0$). The Fabius function is a canonical example of a $C^\infty$ function that is non-analytic on a continuum. Consider now the (real) function $f(x)=\exp\frac{-1}{x^2}$ from above. With the understanding that $f$ is a bounded function and all derivatives of $f$ are bounded, define $$g(x):=\sum_n 2^{-n}\ f(x-a_n)$$ Where $a_n$ is an enumeration of $\mathbb Q$. We get again a $C^\infty$ function, as uniform convergence of the sum and of the sum of derivatives follows from all derivatives (of $f$) being bounded. It looks like $g$ is also nowhere analytic, since the points of non-analyticity of all the summands together is $\mathbb Q$, which is dense in $\mathbb R$ (if a function is analytic at $p$, there exists an open neighbourhood of $p$ on which it is also analytic). But a proof is something different, and maybe, since we are putting non-analyticities arbitrarily close together, the non-analytic parts cancel at some points. Is $g$ nowhere analytic?","One usually sees $f(x):=\exp\frac{-1}{x^2}$ as an example of a $C^\infty$ function that is not analytic, having one point of non-analyticity (the point $0$). The Fabius function is a canonical example of a $C^\infty$ function that is non-analytic on a continuum. Consider now the (real) function $f(x)=\exp\frac{-1}{x^2}$ from above. With the understanding that $f$ is a bounded function and all derivatives of $f$ are bounded, define $$g(x):=\sum_n 2^{-n}\ f(x-a_n)$$ Where $a_n$ is an enumeration of $\mathbb Q$. We get again a $C^\infty$ function, as uniform convergence of the sum and of the sum of derivatives follows from all derivatives (of $f$) being bounded. It looks like $g$ is also nowhere analytic, since the points of non-analyticity of all the summands together is $\mathbb Q$, which is dense in $\mathbb R$ (if a function is analytic at $p$, there exists an open neighbourhood of $p$ on which it is also analytic). But a proof is something different, and maybe, since we are putting non-analyticities arbitrarily close together, the non-analytic parts cancel at some points. Is $g$ nowhere analytic?",,"['real-analysis', 'complex-analysis', 'analyticity']"
1,"Uniform continuity, uniform convergence, and translation","Uniform continuity, uniform convergence, and translation",,"Let $f:\mathbb R \to \mathbb R$ be a continuous function. Define $f_n:\mathbb R \to \mathbb R$ by $$ f_n(x) := f(x+1/n). $$ Suppose that $(f_n)_{n=1}^\infty$ converges uniformly to $f$. Does it follow that $f$ is uniformly continuous? Note: the answer is clearly no if we don't assume that $f$ is continuous. I suspect there is a counterexample, showing that the answer is no even if $f$ is continuous. Edit : The following observation might help. There exists a continuous, non-uniformly continuous function $f:\mathbb R \to \mathbb R$ such that $(f_n)_{n=1}^\infty$ converges uniformly to $f$, if and only if there exists a continuous, non-uniformly continuous function $g:\mathbb N \times \mathbb R \to \mathbb R$, such that $(g_n)_{n=1}^\infty$ converges uniformly to $g$, where $g_n(k,x) = g(k,x+1/n)$ and the metric on $\mathbb N \times \mathbb R$ comes from viewing it as a subspace of $\mathbb R \times \mathbb R$ (with the Euclidean metric, say). Proof : Given the function $g$, there is some $\epsilon>0$ which witnesses non-uniform continuity. That is, for each $n$, there exists $m \in \mathbb N$ and $x,y \in \mathbb R$ such that $|x-y|<1/n$ and $$|g(m,x)-g(m,y)|\geq\epsilon.$$ By moving things around, we can assume that for each $k$, there exists $y \in (0,1/k)$ such that $$ |g(k,0)-g(k,y)| \geq \epsilon. $$ Next, we can also assume that $g(k,x)=0$ for all $|x|>2$. This is achieved by multiplying $g$ by the function $h$ which is $1$ on $\mathbb N \times [-1,1]$, $0$ on $\mathbb N \times (\mathbb R \setminus (-2,2)$, and linear elsewhere. That $(gh)_n \to gh$ uniformly can be proven by the same argument that the product of uniformly continuous functions is uniformly continuous. Now, we just define $f$ piecewise, by [ f(6k+x)=g(k,x) ] for $k \in \mathbb N$ and $x \in [-3,3]$, and [ f(x) = 0 ] for $x < -3$.","Let $f:\mathbb R \to \mathbb R$ be a continuous function. Define $f_n:\mathbb R \to \mathbb R$ by $$ f_n(x) := f(x+1/n). $$ Suppose that $(f_n)_{n=1}^\infty$ converges uniformly to $f$. Does it follow that $f$ is uniformly continuous? Note: the answer is clearly no if we don't assume that $f$ is continuous. I suspect there is a counterexample, showing that the answer is no even if $f$ is continuous. Edit : The following observation might help. There exists a continuous, non-uniformly continuous function $f:\mathbb R \to \mathbb R$ such that $(f_n)_{n=1}^\infty$ converges uniformly to $f$, if and only if there exists a continuous, non-uniformly continuous function $g:\mathbb N \times \mathbb R \to \mathbb R$, such that $(g_n)_{n=1}^\infty$ converges uniformly to $g$, where $g_n(k,x) = g(k,x+1/n)$ and the metric on $\mathbb N \times \mathbb R$ comes from viewing it as a subspace of $\mathbb R \times \mathbb R$ (with the Euclidean metric, say). Proof : Given the function $g$, there is some $\epsilon>0$ which witnesses non-uniform continuity. That is, for each $n$, there exists $m \in \mathbb N$ and $x,y \in \mathbb R$ such that $|x-y|<1/n$ and $$|g(m,x)-g(m,y)|\geq\epsilon.$$ By moving things around, we can assume that for each $k$, there exists $y \in (0,1/k)$ such that $$ |g(k,0)-g(k,y)| \geq \epsilon. $$ Next, we can also assume that $g(k,x)=0$ for all $|x|>2$. This is achieved by multiplying $g$ by the function $h$ which is $1$ on $\mathbb N \times [-1,1]$, $0$ on $\mathbb N \times (\mathbb R \setminus (-2,2)$, and linear elsewhere. That $(gh)_n \to gh$ uniformly can be proven by the same argument that the product of uniformly continuous functions is uniformly continuous. Now, we just define $f$ piecewise, by [ f(6k+x)=g(k,x) ] for $k \in \mathbb N$ and $x \in [-3,3]$, and [ f(x) = 0 ] for $x < -3$.",,"['real-analysis', 'uniform-convergence', 'uniform-continuity']"
2,"If $\,\lim_{h\to 0}\frac{f(x+h)-f(x-h)}{2h}\,$ exists for every $x$, what does this imply for $f$?","If  exists for every , what does this imply for ?","\,\lim_{h\to 0}\frac{f(x+h)-f(x-h)}{2h}\, x f","Consider the function $$ f(x)=\left\{\begin{array}{rll} 1+x^2 & \text{if} & x \,\,\text{rational} \\ -x^2 & \text{if} & x \,\,\text{irrational}\end{array}\right. $$ Then, for $x=0$, the limit $\lim_{h\to 0}\dfrac{f(h)-f(-h)}{2h}$ exists, although $f$ nowhere continuous. Consider now the function $$ f(x)=\left\{\begin{array}{rll} 1 & \text{if} & x=0 \\ 0 & \text{if} & x\ne 0\end{array}\right. $$ Then the limit $\lim_{h\to 0}\dfrac{f(x+h)-f(x-h)}{2h}$ exists, for every $x$, although $f$ is not continuous at $x=0$. This example can be generalised, and obtain an $f$ which is discontinuous in countably many points (for example all the rationals), while the central difference converges. Suppose now that limit $\lim_{h\to 0}\dfrac{f(x+h)-f(x-h)}{2h}$ exists for every $x$ is some open interval. Does this imply that $f$ is not differentiable in at most countably many points?","Consider the function $$ f(x)=\left\{\begin{array}{rll} 1+x^2 & \text{if} & x \,\,\text{rational} \\ -x^2 & \text{if} & x \,\,\text{irrational}\end{array}\right. $$ Then, for $x=0$, the limit $\lim_{h\to 0}\dfrac{f(h)-f(-h)}{2h}$ exists, although $f$ nowhere continuous. Consider now the function $$ f(x)=\left\{\begin{array}{rll} 1 & \text{if} & x=0 \\ 0 & \text{if} & x\ne 0\end{array}\right. $$ Then the limit $\lim_{h\to 0}\dfrac{f(x+h)-f(x-h)}{2h}$ exists, for every $x$, although $f$ is not continuous at $x=0$. This example can be generalised, and obtain an $f$ which is discontinuous in countably many points (for example all the rationals), while the central difference converges. Suppose now that limit $\lim_{h\to 0}\dfrac{f(x+h)-f(x-h)}{2h}$ exists for every $x$ is some open interval. Does this imply that $f$ is not differentiable in at most countably many points?",,"['calculus', 'real-analysis', 'limits', 'derivatives']"
3,"The sequence $\,a_n=\lfloor \mathrm{e}^n\rfloor$ contains infinitely many odd and infinitely many even terms",The sequence  contains infinitely many odd and infinitely many even terms,"\,a_n=\lfloor \mathrm{e}^n\rfloor","PROBLEM. Show that the sequence $\,a_n=\lfloor \mathrm{e}^n\rfloor$ contains infinitely many odd and infinitely many even terms. It suffices to show that the terms of the sequence $$\,b_n=\mathrm{e}^n\,\mathrm{mod}\, 2,\,\,\,n\in\mathbb N,$$ are dense in $[0,2]$ . Unfortunately, Weyl's Theorem does not look helpful in this case. EDIT. As Chris Culter said, the claim that the terms of the sequence $$\,b_n=\mathrm{e}^n\,\mathrm{mod}\, 2,\,\,\,n\in\mathbb N,$$ are dense in $[0,2]$ is (or might be) an open problem. Nevertheless, this does not imply that the claim that the sequence $\,a_n=\lfloor \mathrm{e}^n\rfloor$ contains infinitely many odd and infinitely many even terms is necessarily an open problem as well. It is also noteworthy that it is relatively easy to construct an irrational $\alpha$ with the property that the sequence $\,\alpha^n\,\mathrm{mod}\, 2,\,\,n\in\mathbb N,$ is NOT dense in $[0,2]$ .","PROBLEM. Show that the sequence contains infinitely many odd and infinitely many even terms. It suffices to show that the terms of the sequence are dense in . Unfortunately, Weyl's Theorem does not look helpful in this case. EDIT. As Chris Culter said, the claim that the terms of the sequence are dense in is (or might be) an open problem. Nevertheless, this does not imply that the claim that the sequence contains infinitely many odd and infinitely many even terms is necessarily an open problem as well. It is also noteworthy that it is relatively easy to construct an irrational with the property that the sequence is NOT dense in .","\,a_n=\lfloor \mathrm{e}^n\rfloor \,b_n=\mathrm{e}^n\,\mathrm{mod}\, 2,\,\,\,n\in\mathbb N, [0,2] \,b_n=\mathrm{e}^n\,\mathrm{mod}\, 2,\,\,\,n\in\mathbb N, [0,2] \,a_n=\lfloor \mathrm{e}^n\rfloor \alpha \,\alpha^n\,\mathrm{mod}\, 2,\,\,n\in\mathbb N, [0,2]","['real-analysis', 'sequences-and-series', 'uniform-distribution']"
4,Why $\sum_{k=1}^{\infty} \frac{k}{2^k} = 2$? [duplicate],Why ? [duplicate],\sum_{k=1}^{\infty} \frac{k}{2^k} = 2,This question already has answers here : How can I evaluate $\sum_{n=0}^\infty(n+1)x^n$? (24 answers) Closed 9 years ago . Can you please explain why $$ \sum_{k=1}^{\infty} \dfrac{k}{2^k} = \dfrac{1}{2} +\dfrac{ 2}{4} + \dfrac{3}{8}+ \dfrac{4}{16} +\dfrac{5}{32} + \dots = 2 $$ I know $1 + 2 + 3 + ... + n = \dfrac{n(n+1)}{2}$,This question already has answers here : How can I evaluate $\sum_{n=0}^\infty(n+1)x^n$? (24 answers) Closed 9 years ago . Can you please explain why $$ \sum_{k=1}^{\infty} \dfrac{k}{2^k} = \dfrac{1}{2} +\dfrac{ 2}{4} + \dfrac{3}{8}+ \dfrac{4}{16} +\dfrac{5}{32} + \dots = 2 $$ I know $1 + 2 + 3 + ... + n = \dfrac{n(n+1)}{2}$,,"['calculus', 'real-analysis', 'sequences-and-series', 'summation']"
5,Intuitive idea of the Lipschitz function,Intuitive idea of the Lipschitz function,,I'm trying to understand intuitively the notion of Lipschitz function. I can't understand why bounded function doesn't imply Lipschitz function. I need a counterexample or an intuitive idea to clarify my notion of Lipschitz function. I need help Thanks a lot,I'm trying to understand intuitively the notion of Lipschitz function. I can't understand why bounded function doesn't imply Lipschitz function. I need a counterexample or an intuitive idea to clarify my notion of Lipschitz function. I need help Thanks a lot,,['real-analysis']
6,Continuous image of compact sets are compact,Continuous image of compact sets are compact,,"How to prove: Continuous function $f : M\to N$ maps compact set to compact set? The simplest case in real analysis: if $f: [a,b] \rightarrow \mathbb{R}$ is continuous, then I need to show that $f([a,b])$ is closed and bounded, by the Heine-Borel theorem. I have proved the boundedness, and I need some insight on how to prove that $f([a,b])$ is closed, i.e. $f([a,b])=[c,d]$ . From the Extreme Value Theorem, we know that $c$ and $d$ can be achieved, but how to prove that if $c < x < d$ , then $x \in f([a,b])$ ? What if $M, N$ are metric spaces? What if $M$ and $N$ are general topological spaces?","How to prove: Continuous function maps compact set to compact set? The simplest case in real analysis: if is continuous, then I need to show that is closed and bounded, by the Heine-Borel theorem. I have proved the boundedness, and I need some insight on how to prove that is closed, i.e. . From the Extreme Value Theorem, we know that and can be achieved, but how to prove that if , then ? What if are metric spaces? What if and are general topological spaces?","f : M\to N f: [a,b] \rightarrow \mathbb{R} f([a,b]) f([a,b]) f([a,b])=[c,d] c d c < x < d x \in f([a,b]) M, N M N","['real-analysis', 'general-topology', 'continuity', 'metric-spaces', 'compactness']"
7,Triangle inequality for subtraction? [duplicate],Triangle inequality for subtraction? [duplicate],,This question already has answers here : Prove that $||x|-|y||\le |x-y|$ (7 answers) Closed 3 years ago . Why is $|a - b| \geq|a| - |b|$ ?,This question already has answers here : Prove that $||x|-|y||\le |x-y|$ (7 answers) Closed 3 years ago . Why is ?,|a - b| \geq|a| - |b|,"['real-analysis', 'inequality', 'triangles']"
8,Evaluating $\lim\limits_{n\to\infty} \left(\frac{1^p+2^p+3^p + \cdots + n^p}{n^p} - \frac{n}{p+1}\right)$,Evaluating,\lim\limits_{n\to\infty} \left(\frac{1^p+2^p+3^p + \cdots + n^p}{n^p} - \frac{n}{p+1}\right),Evaluate  $$\lim_{n\to\infty} \left(\frac{1^p+2^p+3^p + \cdots + n^p}{n^p} - \frac{n}{p+1}\right)$$,Evaluate  $$\lim_{n\to\infty} \left(\frac{1^p+2^p+3^p + \cdots + n^p}{n^p} - \frac{n}{p+1}\right)$$,,"['calculus', 'real-analysis', 'limits']"
9,Proof that a perfect set is uncountable,Proof that a perfect set is uncountable,,"There is something I don't understand about the proof that perfect sets are uncountable . The same proof is present in Rudin's Principles of Mathematical Analysis. Do we assume that our construction of $U_n$ must contain all points of $S$? What if we are only collecting evenly-indexed points of $S$ ($x_{2n}$)? We would still get an infinitely countable subset of $S$, and the rest of $S$ can be used to provide points for $V$. What am I missing?","There is something I don't understand about the proof that perfect sets are uncountable . The same proof is present in Rudin's Principles of Mathematical Analysis. Do we assume that our construction of $U_n$ must contain all points of $S$? What if we are only collecting evenly-indexed points of $S$ ($x_{2n}$)? We would still get an infinitely countable subset of $S$, and the rest of $S$ can be used to provide points for $V$. What am I missing?",,"['real-analysis', 'general-topology']"
10,Is There a Natural Way to Extend Repeated Exponentiation Beyond Integers?,Is There a Natural Way to Extend Repeated Exponentiation Beyond Integers?,,"This question has been in my mind since high school. We can get multiplication of natural numbers by repeated addition; equivalently, if we define $f$ recursively by $f(1)=m$ and $f(n+1)=f(n)+m$, then $f(n) = m \times n$.  Likewise, we get exponentiation by repeated multiplication. If $g(1)=m$ and $g(n+1)=mg(n)$, then $g(n) = m^n$.  In my high school mind it was natural to imagine a new function defined by repeated exponentiation: $h(1)=m$ and $h(n+1)=m^{h(n)}$. These definitions only make sense for $n$ a natural number, but of course there are standard very mathematically satisfying ways to define multiplication and exponentiation by any real number. My question is this: Can the function $h$ defined above also be extended in a natural way to $\mathbb{R}^{>0}$? The question is in the spirit of seeking an extension of $f(n)=n!$ to $\mathbb{R}$ and arriving at $\Gamma(x)$. Let me focus the question, and attempt to make precise what I mean by ""in a natural way."" Take $h(1)=2$ and $h(n+1)=2^{h(n)}$. $h$ is now defined on $\mathbb{N}$, and $h(2)=4$, $h(3)=16$, $h(4)=2^{16}=65,536$ etc. Is it possible to extend the domain of definition of $h$ to all positive reals in such a way that a) The functional equation $h(x+1)=2^{h(x)}$ continues to be satisfied for all $x$ in the domain. b) $h$ is $C^\infty$. (Analytic would be even better but this seems maybe too much to hope for?) c) All $h$'s derivatives are monotone. These requirements are my attempt to codify what would count as ""natural."" I am open to suggestions about what would be a better list of requirements. If such a function exists, I would like to know how to construct it; if it doesn't, I would like to know why (i.e. outline of proof), and if relaxing some of the requirements (e.g. just the first derivative monotone) would make it possible. (If the function exists, I am also interested in the questions, ""is it unique?"" ""Could we add some natural requirements to make it unique?"" But my main query is about existence.)","This question has been in my mind since high school. We can get multiplication of natural numbers by repeated addition; equivalently, if we define $f$ recursively by $f(1)=m$ and $f(n+1)=f(n)+m$, then $f(n) = m \times n$.  Likewise, we get exponentiation by repeated multiplication. If $g(1)=m$ and $g(n+1)=mg(n)$, then $g(n) = m^n$.  In my high school mind it was natural to imagine a new function defined by repeated exponentiation: $h(1)=m$ and $h(n+1)=m^{h(n)}$. These definitions only make sense for $n$ a natural number, but of course there are standard very mathematically satisfying ways to define multiplication and exponentiation by any real number. My question is this: Can the function $h$ defined above also be extended in a natural way to $\mathbb{R}^{>0}$? The question is in the spirit of seeking an extension of $f(n)=n!$ to $\mathbb{R}$ and arriving at $\Gamma(x)$. Let me focus the question, and attempt to make precise what I mean by ""in a natural way."" Take $h(1)=2$ and $h(n+1)=2^{h(n)}$. $h$ is now defined on $\mathbb{N}$, and $h(2)=4$, $h(3)=16$, $h(4)=2^{16}=65,536$ etc. Is it possible to extend the domain of definition of $h$ to all positive reals in such a way that a) The functional equation $h(x+1)=2^{h(x)}$ continues to be satisfied for all $x$ in the domain. b) $h$ is $C^\infty$. (Analytic would be even better but this seems maybe too much to hope for?) c) All $h$'s derivatives are monotone. These requirements are my attempt to codify what would count as ""natural."" I am open to suggestions about what would be a better list of requirements. If such a function exists, I would like to know how to construct it; if it doesn't, I would like to know why (i.e. outline of proof), and if relaxing some of the requirements (e.g. just the first derivative monotone) would make it possible. (If the function exists, I am also interested in the questions, ""is it unique?"" ""Could we add some natural requirements to make it unique?"" But my main query is about existence.)",,"['real-analysis', 'functions', 'tetration']"
11,Subgroup of $\mathbb{R}$ either dense or has a least positive element?,Subgroup of  either dense or has a least positive element?,\mathbb{R},"Let's say $G$ is some additive subgroup of $\mathbb{R}$ that has at least two elements. From what I understand, $G$ is then either dense in $\mathbb{R}$, or has some least positive element. What is the reason for this?","Let's say $G$ is some additive subgroup of $\mathbb{R}$ that has at least two elements. From what I understand, $G$ is then either dense in $\mathbb{R}$, or has some least positive element. What is the reason for this?",,"['real-analysis', 'general-topology']"
12,Steinhaus theorem (sums version),Steinhaus theorem (sums version),,"This is a question from Stromberg related to Steinhaus' Theorem: If $A$ is a set of positive Lebesgue measure, show that $A + A$ contains an interval. I can't quite see how to modify the Steinhaus proof though.","This is a question from Stromberg related to Steinhaus' Theorem: If $A$ is a set of positive Lebesgue measure, show that $A + A$ contains an interval. I can't quite see how to modify the Steinhaus proof though.",,"['real-analysis', 'measure-theory']"
13,The $ l^{\infty} $-norm is equal to the limit of the $ l^{p} $-norms. [duplicate],The -norm is equal to the limit of the -norms. [duplicate], l^{\infty}   l^{p} ,"This question already has answers here : Limit of $L^p$ norm (4 answers) Closed 10 years ago . If we are in a sequence space, then the $ l^{p} $-norm of the sequence $ \mathbf{x} = (x_{i})_{i \in \mathbb{N}} $ is $ \displaystyle \left( \sum_{i=1}^{\infty} |x_{i}|^{p} \right)^{1/p} $. The $ l^{\infty} $-norm of $ \mathbf{x} $ is $ \displaystyle \sup_{i \in \mathbb{N}} |x_{i}| $. Prove that the limit of the $ l^{p} $-norms is the $ l^{\infty} $-norm. I saw an answer for $ L^{p} $-spaces, but I need one for $ l^{p} $-spaces. Besides, I didn’t really understand the $ L^{p} $-answer either. Thanks for your help!","This question already has answers here : Limit of $L^p$ norm (4 answers) Closed 10 years ago . If we are in a sequence space, then the $ l^{p} $-norm of the sequence $ \mathbf{x} = (x_{i})_{i \in \mathbb{N}} $ is $ \displaystyle \left( \sum_{i=1}^{\infty} |x_{i}|^{p} \right)^{1/p} $. The $ l^{\infty} $-norm of $ \mathbf{x} $ is $ \displaystyle \sup_{i \in \mathbb{N}} |x_{i}| $. Prove that the limit of the $ l^{p} $-norms is the $ l^{\infty} $-norm. I saw an answer for $ L^{p} $-spaces, but I need one for $ l^{p} $-spaces. Besides, I didn’t really understand the $ L^{p} $-answer either. Thanks for your help!",,"['real-analysis', 'sequences-and-series', 'functional-analysis', 'normed-spaces', 'lp-spaces']"
14,Closed form for the integral $\int_{0}^{\infty}\frac{\ln^{2}(x)\ln(1+x)}{(1-x)(x^{2}+1)}dx$,Closed form for the integral,\int_{0}^{\infty}\frac{\ln^{2}(x)\ln(1+x)}{(1-x)(x^{2}+1)}dx,Here is a challenging one maybe some would like a go at. Show that: $$\int_{0}^{\infty}\frac{\ln^{2}(x)\ln(1+x)}{(1-x)(x^{2}+1)}dx=\frac{-9\pi^{4}}{256}+\frac{\pi^{3}}{32}\ln2+\frac{\pi^{2}}{6}G-\frac{1}{1536}\left[\psi_{3}\left(\frac34\right)-\psi_{3}\left(\frac14\right)\right]$$,Here is a challenging one maybe some would like a go at. Show that: $$\int_{0}^{\infty}\frac{\ln^{2}(x)\ln(1+x)}{(1-x)(x^{2}+1)}dx=\frac{-9\pi^{4}}{256}+\frac{\pi^{3}}{32}\ln2+\frac{\pi^{2}}{6}G-\frac{1}{1536}\left[\psi_{3}\left(\frac34\right)-\psi_{3}\left(\frac14\right)\right]$$,,"['real-analysis', 'integration', 'definite-integrals', 'improper-integrals', 'closed-form']"
15,What's the limit of $\sqrt{2 + \sqrt{2-\sqrt{2+\sqrt{2-\sqrt{2-\sqrt{2 + ...}}}}}} $?,What's the limit of ?,\sqrt{2 + \sqrt{2-\sqrt{2+\sqrt{2-\sqrt{2-\sqrt{2 + ...}}}}}} ,"Let's look at the continued radical $ R = \sqrt{2 + \sqrt{2-\sqrt{2+\sqrt{2-\sqrt{2-\sqrt{2 + ...}}}}}} $ whose signs are defined as $ (+, -, +, -, -, + ,-, -, -,...)$, similar to the sequence $101001000100001...$, where $1 = +$ $0 = - $ This radical seems to converge to a constant approximately equal to $1.567883...$. The question is: Is it possible to find this limit $R$ in closed form? Remark: In the article ""On the periodic continued radicals of 2 and generalization for Vieta’s product"", it is proved that a periodic sequence of signs composed of nested square roots of two converges to $2\sin(q\pi)$ for some rational number $q$. I have tried with non periodic sequences of plus and minus, and they also converge to numbers between $0$ and $2$. if this radical has a closed form, It can be the sine of an irrational multiple of $\pi$, since both are transcendental numbers.","Let's look at the continued radical $ R = \sqrt{2 + \sqrt{2-\sqrt{2+\sqrt{2-\sqrt{2-\sqrt{2 + ...}}}}}} $ whose signs are defined as $ (+, -, +, -, -, + ,-, -, -,...)$, similar to the sequence $101001000100001...$, where $1 = +$ $0 = - $ This radical seems to converge to a constant approximately equal to $1.567883...$. The question is: Is it possible to find this limit $R$ in closed form? Remark: In the article ""On the periodic continued radicals of 2 and generalization for Vieta’s product"", it is proved that a periodic sequence of signs composed of nested square roots of two converges to $2\sin(q\pi)$ for some rational number $q$. I have tried with non periodic sequences of plus and minus, and they also converge to numbers between $0$ and $2$. if this radical has a closed form, It can be the sine of an irrational multiple of $\pi$, since both are transcendental numbers.",,"['calculus', 'real-analysis']"
16,Smallest dense subset of $\mathbb{R}$,Smallest dense subset of,\mathbb{R},"I am not sure if what I am looking for even makes sense (or) exists. Anyway I would be happy if someone can clear my confusion. The set of real numbers $\mathbb{R}$ is obtained as completion of $\mathbb{Q}$. However, $\mathbb{Q}$ is not the only set which is dense in $\mathbb{R}$. $\mathbb{Q} \backslash \mathbb{Z}$ is also a dense subset of $\mathbb{R}$. I am wondering if it makes sense to talk of the ""smallest"" dense subset of $\mathbb{R}$. To phrase what I am looking for precisely and what I mean by smallest, I am looking for a dense set $A$ of $\mathbb{R}$ such that if $B$ is a proper subset of $A$ then $B$ is not dense in $\mathbb{R}$. I am able to ""see"" that the set $A$, I am looking for doesn't exist since any open interval consists of infinite rationals. But I am unable to precisely argue out to myself and convince why $A$ doesn't exist. Could some one throw more light on this?","I am not sure if what I am looking for even makes sense (or) exists. Anyway I would be happy if someone can clear my confusion. The set of real numbers $\mathbb{R}$ is obtained as completion of $\mathbb{Q}$. However, $\mathbb{Q}$ is not the only set which is dense in $\mathbb{R}$. $\mathbb{Q} \backslash \mathbb{Z}$ is also a dense subset of $\mathbb{R}$. I am wondering if it makes sense to talk of the ""smallest"" dense subset of $\mathbb{R}$. To phrase what I am looking for precisely and what I mean by smallest, I am looking for a dense set $A$ of $\mathbb{R}$ such that if $B$ is a proper subset of $A$ then $B$ is not dense in $\mathbb{R}$. I am able to ""see"" that the set $A$, I am looking for doesn't exist since any open interval consists of infinite rationals. But I am unable to precisely argue out to myself and convince why $A$ doesn't exist. Could some one throw more light on this?",,['real-analysis']
17,When does pointwise convergence imply uniform convergence?,When does pointwise convergence imply uniform convergence?,,"On an exam question (Question 21H), it is claimed that if $K$ is compact and $f_n : K \to \mathbb{R}$ are continuous functions increasing pointwise to a continuous function $f : K \to \mathbb{R}$, then $f_n$ converges to $f$ uniformly. I have tried proving this claim for the better part of an hour but I keep coming short. I suspect a hypothesis on equicontinuity has been omitted — partly because the first half of the question is about the Arzelà–Ascoli theorem — but I don't have access to the errata for the exam so I can't be sure. Here is my attempted proof: let $g_n = f - f_n$, so that $(g_n)$ is a sequence of continuous functions decreasing pointwise to $0$. Clearly, $0 \le \cdots \le \| g_n \| \le \| g_{n-1} \| \le \cdots \le \| g_1 \|$, so we must have $\| g_n \| \longrightarrow L$ for some constant $L$. $K$ is compact, so for each $g_n$, there is an $x_n \in K$ such that $g_n(x_n) = \| g_n \|$, and there is a convergent subsequence with $x_{n_k} \longrightarrow x$ for some $x \in K$. By hypothesis, $g_n(x) \longrightarrow 0$, and by construction $g_{n_k} (x_{n_k}) \longrightarrow L$. I'd like to conclude that $L = 0$, but to do this I would need to know that the two sequences have the same limit. This is true if, say, $\{ g_n \}$ is an equicontinuous family, but this isn't one of the hypotheses, so I'm stuck.","On an exam question (Question 21H), it is claimed that if $K$ is compact and $f_n : K \to \mathbb{R}$ are continuous functions increasing pointwise to a continuous function $f : K \to \mathbb{R}$, then $f_n$ converges to $f$ uniformly. I have tried proving this claim for the better part of an hour but I keep coming short. I suspect a hypothesis on equicontinuity has been omitted — partly because the first half of the question is about the Arzelà–Ascoli theorem — but I don't have access to the errata for the exam so I can't be sure. Here is my attempted proof: let $g_n = f - f_n$, so that $(g_n)$ is a sequence of continuous functions decreasing pointwise to $0$. Clearly, $0 \le \cdots \le \| g_n \| \le \| g_{n-1} \| \le \cdots \le \| g_1 \|$, so we must have $\| g_n \| \longrightarrow L$ for some constant $L$. $K$ is compact, so for each $g_n$, there is an $x_n \in K$ such that $g_n(x_n) = \| g_n \|$, and there is a convergent subsequence with $x_{n_k} \longrightarrow x$ for some $x \in K$. By hypothesis, $g_n(x) \longrightarrow 0$, and by construction $g_{n_k} (x_{n_k}) \longrightarrow L$. I'd like to conclude that $L = 0$, but to do this I would need to know that the two sequences have the same limit. This is true if, say, $\{ g_n \}$ is an equicontinuous family, but this isn't one of the hypotheses, so I'm stuck.",,"['real-analysis', 'sequences-and-series', 'functional-analysis', 'uniform-convergence']"
18,Evaluating the limit: $\lim\limits_{x \to \infty} \sum\limits_{n=1}^{\infty} (-1)^{n-1}\frac{x^{2n-1}}{(2n)! \log (2n)}$,Evaluating the limit:,\lim\limits_{x \to \infty} \sum\limits_{n=1}^{\infty} (-1)^{n-1}\frac{x^{2n-1}}{(2n)! \log (2n)},"How to evaluate the limit:    $$\lim\limits_{x \to \infty} \sum\limits_{n=1}^{\infty}  (-1)^{n-1}\frac{x^{2n-1}}{(2n)!\log (2n)}$$ I have tried to break the sum into even and odd $n$ and estimate each sum to be $\sim \frac{e^{x}}{\log (4x)}$ by estimating the summations in the range $\sum\limits_{x/c \le n \le cx} \frac{x^{4n}}{(4n+s)!\log (4n+s)}$ with the trivial bounds while $e^{-x}\sum\limits_{n > cx} \frac{x^{4n}}{(4n+s)!\log (4n+s)}$ and $e^{-x}\sum\limits_{0 \le n < x/c} \frac{x^{4n}}{(4n+s)!\log (4n+s)}$ both decay exponentially for arbitrary $c > 1$ as $x \to +\infty$ ($s=2,4$). The precise estimates I have used are similar to here . However, I suppose stronger asymptotics will be required for calculating the limit of their difference. Any help/hint is appreciated. Thanks! I am not much acquainted with the Laplace method or probabilistic interpretations (I'd appreciate it some references are mentioned, should the answer involve advanced tools like them.) Machinato suggests that $f(z)=\sum_{n=2}^{\infty} \frac{(-1)^n z^{n-1}}{n! \log n}$ approaches $\log z$. This seems to be true! As evidence, here are plots of the real (red) and imaginary (blue) parts of $f(e^{2+i \theta})$, for $-\pi<\theta<\pi$. For comparison, the red and blue dashed lines are the real and imaginary parts of $\log z = 2+i \theta$. It's not clear from this image whether the limit holds for all $\theta$, but for $-\pi/2 < \theta < \pi/2$ the fit is very good. I'm offering a bounty for a proof of this bizarre behavior. I imagine the precise formulation is that, for $\theta$ fixed in $(-\pi,\pi)$, we have $\lim_{R \to \infty} f(R e^{i \theta}) - \log R = i \theta$ but I'll certain accept other asymptotic results of a similar flavor. (For $\theta = \pi$, it is easy to see that $f(-R)$ grows faster than any power of $R$, so it can't mimic $\log R$.) After some more experimentation, I suspect the limit only holds for $-\pi/2<\theta<\pi/2$. Here are plots of $\mathrm{Im}(f(r e^{i \theta}))$ for $\theta = \pi/3$ (first plot) and $2 \pi/3$ (second plot), with $0 < r < 10$. In each case, the dashed line is at $\theta$. Convergence looks great in the first one, terrible in the second.","How to evaluate the limit:    $$\lim\limits_{x \to \infty} \sum\limits_{n=1}^{\infty}  (-1)^{n-1}\frac{x^{2n-1}}{(2n)!\log (2n)}$$ I have tried to break the sum into even and odd $n$ and estimate each sum to be $\sim \frac{e^{x}}{\log (4x)}$ by estimating the summations in the range $\sum\limits_{x/c \le n \le cx} \frac{x^{4n}}{(4n+s)!\log (4n+s)}$ with the trivial bounds while $e^{-x}\sum\limits_{n > cx} \frac{x^{4n}}{(4n+s)!\log (4n+s)}$ and $e^{-x}\sum\limits_{0 \le n < x/c} \frac{x^{4n}}{(4n+s)!\log (4n+s)}$ both decay exponentially for arbitrary $c > 1$ as $x \to +\infty$ ($s=2,4$). The precise estimates I have used are similar to here . However, I suppose stronger asymptotics will be required for calculating the limit of their difference. Any help/hint is appreciated. Thanks! I am not much acquainted with the Laplace method or probabilistic interpretations (I'd appreciate it some references are mentioned, should the answer involve advanced tools like them.) Machinato suggests that $f(z)=\sum_{n=2}^{\infty} \frac{(-1)^n z^{n-1}}{n! \log n}$ approaches $\log z$. This seems to be true! As evidence, here are plots of the real (red) and imaginary (blue) parts of $f(e^{2+i \theta})$, for $-\pi<\theta<\pi$. For comparison, the red and blue dashed lines are the real and imaginary parts of $\log z = 2+i \theta$. It's not clear from this image whether the limit holds for all $\theta$, but for $-\pi/2 < \theta < \pi/2$ the fit is very good. I'm offering a bounty for a proof of this bizarre behavior. I imagine the precise formulation is that, for $\theta$ fixed in $(-\pi,\pi)$, we have $\lim_{R \to \infty} f(R e^{i \theta}) - \log R = i \theta$ but I'll certain accept other asymptotic results of a similar flavor. (For $\theta = \pi$, it is easy to see that $f(-R)$ grows faster than any power of $R$, so it can't mimic $\log R$.) After some more experimentation, I suspect the limit only holds for $-\pi/2<\theta<\pi/2$. Here are plots of $\mathrm{Im}(f(r e^{i \theta}))$ for $\theta = \pi/3$ (first plot) and $2 \pi/3$ (second plot), with $0 < r < 10$. In each case, the dashed line is at $\theta$. Convergence looks great in the first one, terrible in the second.",,"['calculus', 'real-analysis', 'complex-analysis', 'limits']"
19,Inequality $\sum\limits_{cyc}\frac{a^3}{13a^2+5b^2}\geq\frac{a+b+c}{18}$,Inequality,\sum\limits_{cyc}\frac{a^3}{13a^2+5b^2}\geq\frac{a+b+c}{18},"Let $a$ , $b$ and $c$ be positive numbers. Prove that: $$\frac{a^3}{13a^2+5b^2}+\frac{b^3}{13b^2+5c^2}+\frac{c^3}{13c^2+5a^2}\geq\frac{a+b+c}{18}$$ This inequality is strengthening of the following Vasile Cirtoaje's one, which he  created in 2005. Let $a$ , $b$ and $c$ be positive numbers. Prove that: $$\frac{a^3}{2a^2+b^2}+\frac{b^3}{2b^2+c^2}+\frac{c^3}{2c^2+a^2}\geq\frac{a+b+c}{3}.$$ My proof of this inequality you can see here: https://artofproblemsolving.com/community/c6h22937p427220 But this way does not help for the starting inequality. A big problem we have around the point $(a,b,c)=(0.785, 1.25, 1.861)$ because the difference between the LHS and the RHS in this point is $0.0000158...$ . I tried also to use Cauchy-Schwarz, but without success. Also, I think the BW (see here https://math.stackexchange.com/tags/buffalo-way/info I tryed!) does not help.","Let , and be positive numbers. Prove that: This inequality is strengthening of the following Vasile Cirtoaje's one, which he  created in 2005. Let , and be positive numbers. Prove that: My proof of this inequality you can see here: https://artofproblemsolving.com/community/c6h22937p427220 But this way does not help for the starting inequality. A big problem we have around the point because the difference between the LHS and the RHS in this point is . I tried also to use Cauchy-Schwarz, but without success. Also, I think the BW (see here https://math.stackexchange.com/tags/buffalo-way/info I tryed!) does not help.","a b c \frac{a^3}{13a^2+5b^2}+\frac{b^3}{13b^2+5c^2}+\frac{c^3}{13c^2+5a^2}\geq\frac{a+b+c}{18} a b c \frac{a^3}{2a^2+b^2}+\frac{b^3}{2b^2+c^2}+\frac{c^3}{2c^2+a^2}\geq\frac{a+b+c}{3}. (a,b,c)=(0.785, 1.25, 1.861) 0.0000158...","['real-analysis', 'inequality', 'contest-math']"
20,How to show that $\mathbb{Q}$ is not $G_\delta$,How to show that  is not,\mathbb{Q} G_\delta,"I read a section of a book and it made mention of the set of rationals not being a $G_\delta$. However, it gave no proof. I read on wikipedia about using contradiction, but it made use of the Baire category theorem, which is unfamiliar to me. I was wondering if anyone could offer me a different proof; perhaps using the fact that the complement of $G_\delta$ is $F_\sigma$. Thanks.","I read a section of a book and it made mention of the set of rationals not being a $G_\delta$. However, it gave no proof. I read on wikipedia about using contradiction, but it made use of the Baire category theorem, which is unfamiliar to me. I was wondering if anyone could offer me a different proof; perhaps using the fact that the complement of $G_\delta$ is $F_\sigma$. Thanks.",,['real-analysis']
21,Proof that every polynomial of odd degree has one real root,Proof that every polynomial of odd degree has one real root,,"I want to prove that every real polynomial of odd degree has at least one real root, using  the intermediate value theorem. Let $P(x) = x^{2n+1} + a_n x^{2n} + . . . + a_0$ for each $a_i \in \mathbb{R}$ and $n \in \mathbb{N}$. By the fundamental theorem of algebra I know that $P(x)$ has exactly $2n+1$ complex roots, so $P(x) = (x+r_1)(x+r_2) . . . (x+r_{2n+1})$ for each $r_i \in \mathbb{C}$ I do not know how to complete this but I do know that, at some point, I probably have to show that each root with imaginary part non zero has to come in conjugate pairs, and since $2n+1$ is odd there is at least $1$ root that is imaginary part $0$ and thus real.","I want to prove that every real polynomial of odd degree has at least one real root, using  the intermediate value theorem. Let $P(x) = x^{2n+1} + a_n x^{2n} + . . . + a_0$ for each $a_i \in \mathbb{R}$ and $n \in \mathbb{N}$. By the fundamental theorem of algebra I know that $P(x)$ has exactly $2n+1$ complex roots, so $P(x) = (x+r_1)(x+r_2) . . . (x+r_{2n+1})$ for each $r_i \in \mathbb{C}$ I do not know how to complete this but I do know that, at some point, I probably have to show that each root with imaginary part non zero has to come in conjugate pairs, and since $2n+1$ is odd there is at least $1$ root that is imaginary part $0$ and thus real.",,"['calculus', 'real-analysis', 'polynomials']"
22,Topologist's sine curve is connected,Topologist's sine curve is connected,,"I just came across the example of the topologist's sine curve that is connected but not path-connected. The rigorous proof of the non-path-connectedness can be found here . But how can I prove that the curve is connected? To be honest, even intuitively I am not being able to see that the curve is connected. I am thinking if it is proved that the limit point of $\sin(1/x)$ as $x \to 0=0$, then it would be proved. But, why is this true? IMO, this limit doesn't exist. Intuitively also, it seems that the graph would behave crazily and not approach a particular value as a tends to $0.$ EDIT (Brett Frankel): There are a few different working definitions of the topologist's since curve. For the sake of clarity/consistency, I have copied below the definition used in the linked post: $$ y(x) = \begin{cases} \sin\left(\frac{1}{x}\right) & \mbox{if $0\lt x \lt 1$,}\\\ 0 & \mbox{if $x=0$,}\end{cases}$$","I just came across the example of the topologist's sine curve that is connected but not path-connected. The rigorous proof of the non-path-connectedness can be found here . But how can I prove that the curve is connected? To be honest, even intuitively I am not being able to see that the curve is connected. I am thinking if it is proved that the limit point of $\sin(1/x)$ as $x \to 0=0$, then it would be proved. But, why is this true? IMO, this limit doesn't exist. Intuitively also, it seems that the graph would behave crazily and not approach a particular value as a tends to $0.$ EDIT (Brett Frankel): There are a few different working definitions of the topologist's since curve. For the sake of clarity/consistency, I have copied below the definition used in the linked post: $$ y(x) = \begin{cases} \sin\left(\frac{1}{x}\right) & \mbox{if $0\lt x \lt 1$,}\\\ 0 & \mbox{if $x=0$,}\end{cases}$$",,"['real-analysis', 'general-topology']"
23,"Closed form of $\mathscr{R}=\int_0^{\pi/2}\sin^2x\,\ln\big(\sin^2(\tan x)\big)\,\,dx$",Closed form of,"\mathscr{R}=\int_0^{\pi/2}\sin^2x\,\ln\big(\sin^2(\tan x)\big)\,\,dx","Inspired by Mr. Olivier Oloa in this question . Does the following integral admit a closed form? \begin{align} \mathscr{R}=\int_0^{\Large\frac{\pi}{2}}\sin^2x\,\ln\big(\sin^2(\tan x)\big)\,\,dx \end{align} It will be my last question before I take a long break from my activity on Mathematics StackExchange. So, please be nice. No more downvotes for no reason because this is a challenge problem . Edit : I am also interested in knowing the numerical value of $\mathscr{R}$ to the precision of at least $50$ digits. If you use Mathematica to find its numerical value, please share your method & the code.","Inspired by Mr. Olivier Oloa in this question . Does the following integral admit a closed form? \begin{align} \mathscr{R}=\int_0^{\Large\frac{\pi}{2}}\sin^2x\,\ln\big(\sin^2(\tan x)\big)\,\,dx \end{align} It will be my last question before I take a long break from my activity on Mathematics StackExchange. So, please be nice. No more downvotes for no reason because this is a challenge problem . Edit : I am also interested in knowing the numerical value of $\mathscr{R}$ to the precision of at least $50$ digits. If you use Mathematica to find its numerical value, please share your method & the code.",,"['calculus', 'real-analysis', 'integration', 'closed-form']"
24,"Bounded variation, difference of two increasing functions","Bounded variation, difference of two increasing functions",,"Prove that if $f$ is of bounded variation in $[a,b]$, it is the difference of two positive, monotonic increasing functions; and the difference of two bounded monotonic increasing functions is a function of bounded variation.","Prove that if $f$ is of bounded variation in $[a,b]$, it is the difference of two positive, monotonic increasing functions; and the difference of two bounded monotonic increasing functions is a function of bounded variation.",,"['real-analysis', 'bounded-variation']"
25,Equicontinuity on a compact metric space turns pointwise to uniform convergence,Equicontinuity on a compact metric space turns pointwise to uniform convergence,,"I know that If $\{f_n\}$ is an equicontinuous sequence,  defined on a compact metric space $K$, and for all $x$, $f_n(x)\rightarrow f(x)$, then $f_n\rightarrow f$ uniformly. I'm having trouble proving this.  I see the same problem here but am having trouble following the proof, particularly with part (3). Can someone guide me through a proof of this result?","I know that If $\{f_n\}$ is an equicontinuous sequence,  defined on a compact metric space $K$, and for all $x$, $f_n(x)\rightarrow f(x)$, then $f_n\rightarrow f$ uniformly. I'm having trouble proving this.  I see the same problem here but am having trouble following the proof, particularly with part (3). Can someone guide me through a proof of this result?",,"['real-analysis', 'general-topology', 'functional-analysis', 'convergence-divergence', 'continuity']"
26,Proof of continuity of Thomae Function at irrationals.,Proof of continuity of Thomae Function at irrationals.,,"In Thomae's  Function : $$ \begin{align} t(x) = \begin{cases} 0 & \text{if $x$ is irrational}\\ \frac{1}{n} & \text{if $x = \frac{m}{n}$ where $\gcd(m,n) = 1$} \end{cases} \end{align} $$ I can prove the discontinuity at rational $b$ by taking a sequence of irrationals $x_n$ which converge to $b$. But while going through an argument for continuity at irrationals.  I found this in a book. On the other hand if $b$ is an irrational number and $\epsilon > 0$   then there is a     natural number $n_0$ such that $1/n_0 < \epsilon$. There are only finite number of rationals with denominator less than   $n_0$ in the interval $(b-1,b+1)$. Hence we can find a $\delta > 0$   such that $\delta$ neighbourhood of $b$ contains no rational with   denominator less than $n_0$. I understand the rest of the proof. But I am unable to prove the emphasized text. Although I find it intuitive.","In Thomae's  Function : $$ \begin{align} t(x) = \begin{cases} 0 & \text{if $x$ is irrational}\\ \frac{1}{n} & \text{if $x = \frac{m}{n}$ where $\gcd(m,n) = 1$} \end{cases} \end{align} $$ I can prove the discontinuity at rational $b$ by taking a sequence of irrationals $x_n$ which converge to $b$. But while going through an argument for continuity at irrationals.  I found this in a book. On the other hand if $b$ is an irrational number and $\epsilon > 0$   then there is a     natural number $n_0$ such that $1/n_0 < \epsilon$. There are only finite number of rationals with denominator less than   $n_0$ in the interval $(b-1,b+1)$. Hence we can find a $\delta > 0$   such that $\delta$ neighbourhood of $b$ contains no rational with   denominator less than $n_0$. I understand the rest of the proof. But I am unable to prove the emphasized text. Although I find it intuitive.",,"['real-analysis', 'continuity']"
27,How do I show that all continuous periodic functions are bounded and uniform continuous?,How do I show that all continuous periodic functions are bounded and uniform continuous?,,"A function $f:\mathbb{R}\to \mathbb{R}$ is periodic if there exits $p>0$ such that $f(x+P)=f(x)$ for all $x\in \mathbb{R}$. Show that every continuous periodic function is bounded and uniformly continuous. For boundedness, I first tried to show that since the a periodic function is continuous, it is continuous for the closed interval $[x_0,x_0+P]$. I know that there is a theorem saying that if it is continuous on a closed interval, then it is bounded. However, I'm not allowed to state that theorem directly. Should I just aim for a contradiction by supposing f is not bounded on the interval stated above?","A function $f:\mathbb{R}\to \mathbb{R}$ is periodic if there exits $p>0$ such that $f(x+P)=f(x)$ for all $x\in \mathbb{R}$. Show that every continuous periodic function is bounded and uniformly continuous. For boundedness, I first tried to show that since the a periodic function is continuous, it is continuous for the closed interval $[x_0,x_0+P]$. I know that there is a theorem saying that if it is continuous on a closed interval, then it is bounded. However, I'm not allowed to state that theorem directly. Should I just aim for a contradiction by supposing f is not bounded on the interval stated above?",,"['real-analysis', 'continuity', 'uniform-continuity', 'periodic-functions']"
28,The measurability of convex sets,The measurability of convex sets,,"How to prove the measurability of convex sets in $R^n$ ? I have seen a proof, but too long and not very intuitive. If you have seen any, please post it here.","How to prove the measurability of convex sets in ? I have seen a proof, but too long and not very intuitive. If you have seen any, please post it here.",R^n,"['real-analysis', 'measure-theory', 'convex-analysis']"
29,Norm for pointwise convergence,Norm for pointwise convergence,,Does there exist a norm on the space of all real-valued functions on the real line (or on an open set? a compact set?) such that convergence in this norm is equivalent to pointwise convergence?,Does there exist a norm on the space of all real-valued functions on the real line (or on an open set? a compact set?) such that convergence in this norm is equivalent to pointwise convergence?,,"['real-analysis', 'functional-analysis', 'banach-spaces']"
30,Intuition of Gronwall lemma,Intuition of Gronwall lemma,,"The Gronwall lemma is a well known and very useful statement which is used in many situations, in particular in the theory of differential equations. I have seen it so many times and even the proof is very easy to understand. But at the end of the day it is seems to me a very technical 'thing', I never have been able to develop some intuition for it. Neither the statement seems very intuitive nor the proof of it, so that it becomes impossible for me to remember it after some time. Can anyone 'explain' (whatever this means) to me the intuition behind the statement or does anyone has some picture in mind which helps to grasp the idea of Gronwall's lemma? I have in mind the integral form of this lemma.","The Gronwall lemma is a well known and very useful statement which is used in many situations, in particular in the theory of differential equations. I have seen it so many times and even the proof is very easy to understand. But at the end of the day it is seems to me a very technical 'thing', I never have been able to develop some intuition for it. Neither the statement seems very intuitive nor the proof of it, so that it becomes impossible for me to remember it after some time. Can anyone 'explain' (whatever this means) to me the intuition behind the statement or does anyone has some picture in mind which helps to grasp the idea of Gronwall's lemma? I have in mind the integral form of this lemma.",,"['real-analysis', 'ordinary-differential-equations', 'gronwall-type-inequality']"
31,The set of real numbers and power set of the natural numbers,The set of real numbers and power set of the natural numbers,,I have learnt that the cardinality of the power set of the natural numbers is equal to the cardinality of the real numbers. What is the function that gives the one-to-one correspondence between these two sets? I have also learnt that there exists no set whose cardinality is strictly between the natural numbers and the real numbers. Is there a proof of this or at least some intuitiveness behind it?,I have learnt that the cardinality of the power set of the natural numbers is equal to the cardinality of the real numbers. What is the function that gives the one-to-one correspondence between these two sets? I have also learnt that there exists no set whose cardinality is strictly between the natural numbers and the real numbers. Is there a proof of this or at least some intuitiveness behind it?,,"['real-analysis', 'elementary-set-theory']"
32,Prove the following integral inequality: $\int_{0}^{1}f(g(x))dx\le\int_{0}^{1}f(x)dx+\int_{0}^{1}g(x)dx$,Prove the following integral inequality:,\int_{0}^{1}f(g(x))dx\le\int_{0}^{1}f(x)dx+\int_{0}^{1}g(x)dx,"Suppose $f(x)$ and $g(x)$ are continuous function from $[0,1]\rightarrow [0,1]$, and $f$ is monotone increasing, then how to prove the following inequality: $$\int_{0}^{1}f(g(x))dx\le\int_{0}^{1}f(x)dx+\int_{0}^{1}g(x)dx$$","Suppose $f(x)$ and $g(x)$ are continuous function from $[0,1]\rightarrow [0,1]$, and $f$ is monotone increasing, then how to prove the following inequality: $$\int_{0}^{1}f(g(x))dx\le\int_{0}^{1}f(x)dx+\int_{0}^{1}g(x)dx$$",,"['real-analysis', 'integration', 'integral-inequality']"
33,How to show that $f'(x)<2f(x)$ [closed],How to show that  [closed],f'(x)<2f(x),"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . The community reviewed whether to reopen this question 1 year ago and left it closed: Original close reason(s) were not resolved Improve this question This is question B4 in 1999 Putnam Exam. I would appreciate if somebody could help me with this. Let $f(x),f'(x),f''(x),f'''(x)>0$ , $f'''(x)$ is a continuous function and $f'''(x)<f(x)$ on $\mathbb{R}$. Then show that   $$f'(x)<2f(x),~ \forall x\in \mathbb{R}.$$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . The community reviewed whether to reopen this question 1 year ago and left it closed: Original close reason(s) were not resolved Improve this question This is question B4 in 1999 Putnam Exam. I would appreciate if somebody could help me with this. Let $f(x),f'(x),f''(x),f'''(x)>0$ , $f'''(x)$ is a continuous function and $f'''(x)<f(x)$ on $\mathbb{R}$. Then show that   $$f'(x)<2f(x),~ \forall x\in \mathbb{R}.$$",,"['real-analysis', 'derivatives', 'inequality']"
34,How to justify solving $f(x+1) + f(x) = g(x)$ using this spectral-like method?,How to justify solving  using this spectral-like method?,f(x+1) + f(x) = g(x),"Let's say that I want to find solutions $f\in C(\Bbb R)$ to the equation $$ f(x+1) + f(x) = g(x) $$ for some $g\in C(\Bbb R)$ . I can write $f(x+1) = (Tf)(x)$ where $T$ is the right shift operator and rewrite the equation suggestively as $$ (I+ T)f=g. $$ Formally, I can say that the solution of this equation is $$ f= (I+ T)^{-1} g. $$ Of course, I am aware that there are infinitely many solutions to the equation but please bear with me for a moment here. By the theory of operator algebra, if $f,g$ are from some nice Banach space $X$ and our linear operator $T:X\to X$ satisfies $\|T\|<1$ , then we have $$ f = \left(\sum_{n=0}^\infty (-T)^n \right)g. $$ However, it is not unreasonable to expect that we should have $\|T\|=1$ for a right-shift operator in most reasonable function spaces so let's try to solve the equation $$ f= (I+\lambda T)^{-1} g. $$ for $\lambda <1$ first then we'll take $\lambda\to 1$ . Note that all the steps until now is purely formal since $C(\Bbb R)$ is not a normed space. For a concrete example, let's say we take $g(x) = (x+2)^2$ . The previous method says that we first calculate (for $\lambda<1$ ) $$\begin{align} f(x) &= \left(I - \lambda T + \lambda^2 T^2 - \dots \right) g(x) \\ &= (x+2)^2 -\lambda (x+3)^1 + \lambda^2 (x+4)^2 + \dots \\ &= \left(1-\lambda+\lambda^2-\dots \right)x^2 + \left(2-3\lambda+4\lambda^2-\dots \right)2x + \left(2^2-3^2\lambda+4^2\lambda^2-\dots \right) \\ &=  \frac{1}{1+\lambda} x^2 + 2 \frac{2+\lambda}{(1+\lambda)^2} x +  \frac{4+3\lambda + \lambda^2}{(1+\lambda)^3}. \end{align}$$ We shall be brave here and substitute $\lambda=1$ even though the series doesn't converge there. This gives $$ f(x) = \frac 12 x^2 + \frac 32 x + 1 $$ but voilà, for some mysterious reasons unknown to me, this $f$ actually solves our original equation $f(x+1) + f(x) = (x+2)^2$ ! My question is simply: What are the hidden theories behind the miracle we observe here? How can we justify all these seemingly unjustifiable steps? I can't give you a reference to this method because I just conjured it up, thinking that it wouldn't work. To my greatest surprise, the answer actually makes sense. I am sure that similar method is probably practiced somewhere, probably by physicists. Some points worth mentioning: 1.) $C(\Bbb R)$ is probably not the right space to work with since it's not normed. However, I want my answer to be a continuous function on $\Bbb R$ so some form of continuity assumption is needed for our space $X$ . 2.) Norming $C(\Bbb R)$ with $L^\infty$ norm is not the way to go since it is possible that $f$ is unbounded, as our example shows. 3.) The solution $f$ is not unique since the kernel of $(I+T)$ consists of all $C(\Bbb R)$ functions $h$ such that $h(x+1)=-h(x)$ , e.g. $\sin(\pi x)$ . 4.) All the series expansions for $\lambda$ in my example converges when $|\lambda| <1$ but not at $\lambda=1$ but for some reason the result checks out.","Let's say that I want to find solutions to the equation for some . I can write where is the right shift operator and rewrite the equation suggestively as Formally, I can say that the solution of this equation is Of course, I am aware that there are infinitely many solutions to the equation but please bear with me for a moment here. By the theory of operator algebra, if are from some nice Banach space and our linear operator satisfies , then we have However, it is not unreasonable to expect that we should have for a right-shift operator in most reasonable function spaces so let's try to solve the equation for first then we'll take . Note that all the steps until now is purely formal since is not a normed space. For a concrete example, let's say we take . The previous method says that we first calculate (for ) We shall be brave here and substitute even though the series doesn't converge there. This gives but voilà, for some mysterious reasons unknown to me, this actually solves our original equation ! My question is simply: What are the hidden theories behind the miracle we observe here? How can we justify all these seemingly unjustifiable steps? I can't give you a reference to this method because I just conjured it up, thinking that it wouldn't work. To my greatest surprise, the answer actually makes sense. I am sure that similar method is probably practiced somewhere, probably by physicists. Some points worth mentioning: 1.) is probably not the right space to work with since it's not normed. However, I want my answer to be a continuous function on so some form of continuity assumption is needed for our space . 2.) Norming with norm is not the way to go since it is possible that is unbounded, as our example shows. 3.) The solution is not unique since the kernel of consists of all functions such that , e.g. . 4.) All the series expansions for in my example converges when but not at but for some reason the result checks out.","f\in C(\Bbb R) 
f(x+1) + f(x) = g(x)
 g\in C(\Bbb R) f(x+1) = (Tf)(x) T 
(I+ T)f=g.
 
f= (I+ T)^{-1} g.
 f,g X T:X\to X \|T\|<1 
f = \left(\sum_{n=0}^\infty (-T)^n \right)g.
 \|T\|=1 
f= (I+\lambda T)^{-1} g.
 \lambda <1 \lambda\to 1 C(\Bbb R) g(x) = (x+2)^2 \lambda<1 \begin{align}
f(x) &= \left(I - \lambda T + \lambda^2 T^2 - \dots \right) g(x) \\
&= (x+2)^2 -\lambda (x+3)^1 + \lambda^2 (x+4)^2 + \dots \\
&= \left(1-\lambda+\lambda^2-\dots \right)x^2 + \left(2-3\lambda+4\lambda^2-\dots \right)2x + \left(2^2-3^2\lambda+4^2\lambda^2-\dots \right) \\
&=  \frac{1}{1+\lambda} x^2 + 2 \frac{2+\lambda}{(1+\lambda)^2} x +  \frac{4+3\lambda + \lambda^2}{(1+\lambda)^3}.
\end{align} \lambda=1 
f(x) = \frac 12 x^2 + \frac 32 x + 1
 f f(x+1) + f(x) = (x+2)^2 C(\Bbb R) \Bbb R X C(\Bbb R) L^\infty f f (I+T) C(\Bbb R) h h(x+1)=-h(x) \sin(\pi x) \lambda |\lambda| <1 \lambda=1","['real-analysis', 'functional-analysis', 'operator-theory', 'spectral-theory', 'banach-algebras']"
35,An inequality: $1+\frac1{2^2}+\frac1{3^2}+\dotsb+\frac1{n^2}\lt\frac53$,An inequality:,1+\frac1{2^2}+\frac1{3^2}+\dotsb+\frac1{n^2}\lt\frac53,"$n$ is a positive integer, then $$1+\frac1{2^2}+\frac1{3^2}+\dotsb+\frac1{n^2}\lt\frac53.$$ please don't refer to the famous  $1+\frac1{2^2}+\frac1{3^2}+\dotsb=\frac{\pi^2}6$. I want to find a better proof. My  stupid method: $$1+\frac1{2^2}+\frac1{3^2}+\dotsb+\frac1{n^2}\lt \left(1+\frac1{2^2}+\dotsb+\frac1{10^2}\right)+\frac1{10\cdot11}+\dotsb+\frac1{n(n-1)}\\<1.549768...+\frac1{10}\lt\frac53$$","$n$ is a positive integer, then $$1+\frac1{2^2}+\frac1{3^2}+\dotsb+\frac1{n^2}\lt\frac53.$$ please don't refer to the famous  $1+\frac1{2^2}+\frac1{3^2}+\dotsb=\frac{\pi^2}6$. I want to find a better proof. My  stupid method: $$1+\frac1{2^2}+\frac1{3^2}+\dotsb+\frac1{n^2}\lt \left(1+\frac1{2^2}+\dotsb+\frac1{10^2}\right)+\frac1{10\cdot11}+\dotsb+\frac1{n(n-1)}\\<1.549768...+\frac1{10}\lt\frac53$$",,"['calculus', 'real-analysis', 'inequality', 'summation']"
36,Intuitive explanation for why $\left(1-\frac1n\right)^n \to \frac1e$,Intuitive explanation for why,\left(1-\frac1n\right)^n \to \frac1e,"I am aware that $e$, the base of natural logarithms, can be defined as: $$e = \lim_{n\to\infty}\left(1+\frac{1}{n}\right)^n$$ Recently, I found out that $$\lim_{n\to\infty}\left(1-\frac{1}{n}\right)^n = e^{-1}$$ How does that work? Surely the minus sign makes no difference, as when $n$ is large, $\frac{1}{n}$ is very small? I'm not asking for just any rigorous method of proving this. I've been told one: as $n$ goes to infinity, $\left(1+\frac{1}{n}\right)^n\left(1-\frac{1}{n}\right)^n = 1$, so the latter limit must be the reciprocal of $e$. However, I still don't understand why changing such a tiny component of the limit changes the output so drastically. Does anyone have a remotely intuitive explanation of this concept?","I am aware that $e$, the base of natural logarithms, can be defined as: $$e = \lim_{n\to\infty}\left(1+\frac{1}{n}\right)^n$$ Recently, I found out that $$\lim_{n\to\infty}\left(1-\frac{1}{n}\right)^n = e^{-1}$$ How does that work? Surely the minus sign makes no difference, as when $n$ is large, $\frac{1}{n}$ is very small? I'm not asking for just any rigorous method of proving this. I've been told one: as $n$ goes to infinity, $\left(1+\frac{1}{n}\right)^n\left(1-\frac{1}{n}\right)^n = 1$, so the latter limit must be the reciprocal of $e$. However, I still don't understand why changing such a tiny component of the limit changes the output so drastically. Does anyone have a remotely intuitive explanation of this concept?",,"['real-analysis', 'limits', 'exponential-function']"
37,Perfect set without rationals [closed],Perfect set without rationals [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Give an example of a perfect set in $\mathbb R^n$ that does not contain any of the rationals. (Or prove that it does not exist).","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Give an example of a perfect set in $\mathbb R^n$ that does not contain any of the rationals. (Or prove that it does not exist).",,['real-analysis']
38,Generalized mean value theorem,Generalized mean value theorem,,"I know and understand the mean value theorem. But at the moment I don't have the intuition to understand the generalized mean value theorem If $f$ and $g$ are continuous on the closed interval $[a,b]$ and differentiable on the open interval $(a,b)$, then there exists a point $c\in(a,b)$ where$$[f(b)-f(a)]g'(c)=[g(b)-g(a)]f'(c).$$If $g'$ is never zero on $(a,b)$, then the conclusion can be stated as$$\frac{f'(c)}{g'(c)}=\frac{f(b)-f(a)}{g(b)-g(a)}.$$ What is the intuition? And how can you prove this (generally) ?","I know and understand the mean value theorem. But at the moment I don't have the intuition to understand the generalized mean value theorem If $f$ and $g$ are continuous on the closed interval $[a,b]$ and differentiable on the open interval $(a,b)$, then there exists a point $c\in(a,b)$ where$$[f(b)-f(a)]g'(c)=[g(b)-g(a)]f'(c).$$If $g'$ is never zero on $(a,b)$, then the conclusion can be stated as$$\frac{f'(c)}{g'(c)}=\frac{f(b)-f(a)}{g(b)-g(a)}.$$ What is the intuition? And how can you prove this (generally) ?",,"['calculus', 'real-analysis']"
39,Why is the Cantor function not absolutely continuous?,Why is the Cantor function not absolutely continuous?,,Is there an easy way to see that the Cantor function is not absolutely continuous that directly uses the definition of absolutely continuous?,Is there an easy way to see that the Cantor function is not absolutely continuous that directly uses the definition of absolutely continuous?,,"['real-analysis', 'absolute-continuity']"
40,"Surely You're Joking, Mr. Feynman! $\int_0^\infty\frac{\sin^2x}{x^2(1+x^2)}\,dx$ [duplicate]","Surely You're Joking, Mr. Feynman!  [duplicate]","\int_0^\infty\frac{\sin^2x}{x^2(1+x^2)}\,dx","This question already has answers here : Differentiation wrt parameter $\int_0^\infty \sin^2(x)\cdot(x^2(x^2+1))^{-1}dx$ (4 answers) Closed 10 years ago . Prove the following \begin{equation}\int_0^\infty\frac{\sin^2x}{x^2(1+x^2)}\,dx=\frac{\pi}{4}+\frac{\pi}{4e^2}\end{equation} I would love to see how Mathematics SE users prove the integral preferably with the Feynman way (other methods are welcome). Thank you. (>‿◠)✌ Original question: And of course, for the sadist with a background in differential equations, I invite you to try your luck with the last integral of the group . \begin{equation}\int_0^\infty\frac{\sin^2x}{x^2(1+x^2)}\,dx\end{equation} Source: Integration: The Feynman Way","This question already has answers here : Differentiation wrt parameter $\int_0^\infty \sin^2(x)\cdot(x^2(x^2+1))^{-1}dx$ (4 answers) Closed 10 years ago . Prove the following I would love to see how Mathematics SE users prove the integral preferably with the Feynman way (other methods are welcome). Thank you. (>‿◠)✌ Original question: And of course, for the sadist with a background in differential equations, I invite you to try your luck with the last integral of the group . Source: Integration: The Feynman Way","\begin{equation}\int_0^\infty\frac{\sin^2x}{x^2(1+x^2)}\,dx=\frac{\pi}{4}+\frac{\pi}{4e^2}\end{equation} \begin{equation}\int_0^\infty\frac{\sin^2x}{x^2(1+x^2)}\,dx\end{equation}","['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
41,Density of irrationals,Density of irrationals,,"I came across the following problem: Show that if $x$ and $y$ are real numbers with $x <y$, then there exists an irrational number $t$ such that $x < t < y$. We know that $y-x>0$. By the Archimedean property, there exists a positive integer $n$ such that $n(y-x)>1$ or $1/n < y-x$. There exists an integer $m$ such that $m \leq nx < m+1$ or $\displaystyle \frac{m}{n} \leq x \leq \frac{m+1}{n} < y$. This is essentially the proof for the denseness of the rationals. Instead of $\large \frac{m+1}{n}$ I need something of the form $\large\frac{\text{irrational}}{n}$. How would I get the numerator?","I came across the following problem: Show that if $x$ and $y$ are real numbers with $x <y$, then there exists an irrational number $t$ such that $x < t < y$. We know that $y-x>0$. By the Archimedean property, there exists a positive integer $n$ such that $n(y-x)>1$ or $1/n < y-x$. There exists an integer $m$ such that $m \leq nx < m+1$ or $\displaystyle \frac{m}{n} \leq x \leq \frac{m+1}{n} < y$. This is essentially the proof for the denseness of the rationals. Instead of $\large \frac{m+1}{n}$ I need something of the form $\large\frac{\text{irrational}}{n}$. How would I get the numerator?",,['real-analysis']
42,Modus Operandi. Formulae for Maximum and Minimum of two numbers with a + b and $|a - b|$,Modus Operandi. Formulae for Maximum and Minimum of two numbers with a + b and,|a - b|,"I came across the following problem in my self-study of real analysis: For any real numbers $a$ and $b$, show that $$\max \{a,b \} = \frac{1}{2}(a+b+|a-b|)$$ and $$\min\{a,b \} = \frac{1}{2}(a+b-|a-b|)$$ So $a \geq b$ iff $a-b \ge0$ and $b \ge a$ iff $b-a \ge 0$. At first glance, it seems like an average of distances. For the first case, go to the point $a+b$, add $|a-b|$ and divide by $2$. Similarly with the second case. Would you just break it up in cases and verify the formulas? Or do you actually need to come up with the formulas?","I came across the following problem in my self-study of real analysis: For any real numbers $a$ and $b$, show that $$\max \{a,b \} = \frac{1}{2}(a+b+|a-b|)$$ and $$\min\{a,b \} = \frac{1}{2}(a+b-|a-b|)$$ So $a \geq b$ iff $a-b \ge0$ and $b \ge a$ iff $b-a \ge 0$. At first glance, it seems like an average of distances. For the first case, go to the point $a+b$, add $|a-b|$ and divide by $2$. Similarly with the second case. Would you just break it up in cases and verify the formulas? Or do you actually need to come up with the formulas?",,"['real-analysis', 'intuition']"
43,"Evaluate $\int_0^1\ln(1-x)\ln x\ln(1+x)\,\mathrm dx$",Evaluate,"\int_0^1\ln(1-x)\ln x\ln(1+x)\,\mathrm dx","What would you recommend me for the integral below? $$ \int_{0}^{1}\ln(1 - x)\ln(x) \ln(1 + x)\,\mathrm dx $$ For instance, for the version without the last logarithm would work to use Taylor series, but in this case things are a bit more complicated and it doesn't seem to work.","What would you recommend me for the integral below? For instance, for the version without the last logarithm would work to use Taylor series, but in this case things are a bit more complicated and it doesn't seem to work.","
\int_{0}^{1}\ln(1 - x)\ln(x)
\ln(1 + x)\,\mathrm dx
","['calculus', 'real-analysis', 'integration', 'definite-integrals']"
44,How to prove triangle inequality for $p$-norm?,How to prove triangle inequality for -norm?,p,"If $\mathcal{M}=\{M_i : i\in I_n\}$ is a collection of metric spaces, each with metric $d_i$, we can make $M=\prod_{i\in I_n}M_i$ a metric space using the $p$-norm, we simply set $d : M\times M\to \mathbb{R}$ as: $$d((p_1,\dots,p_n),(q_1,\dots,q_n))=\left\|(d(p_1,q_1),\dots,d(p_n,q_n))\right\|_p$$ What I want to prove is that the $p$-norm $$\left\|x\right\|_p=\left(\sum_{i=1}^{n}\left|x_i\right|^p\right)^{1/p}$$ is really a norm. Showing that $\left\|x\right\|_p \geq 0$ being zero if and only if $x = 0$ was easy. Showing that $\left\|kx\right\|_p = \left|k\right|\left\|x\right\|_p$ was also easy. The triangle inequality is the thing that is not being easy to show. Indeed, I want to show that: for every $x,y \in \mathbb{R}^n$ we have: $$\left(\sum_{i=1}^{n}\left|x_i+y_i\right|^p\right)^{1/p}\leq \left(\sum_{i=1}^{n}\left|x_i\right|^p\right)^{1/p}+\left(\sum_{i=1}^{n}\left|y_i\right|^p\right)^{1/p}.$$ I thought that it might not be as difficult as it seems, but after trying a little without sucess I've searched on the internet and the I found that we need measure theory to prove that. Is there any more elementary proof of this inequality?","If $\mathcal{M}=\{M_i : i\in I_n\}$ is a collection of metric spaces, each with metric $d_i$, we can make $M=\prod_{i\in I_n}M_i$ a metric space using the $p$-norm, we simply set $d : M\times M\to \mathbb{R}$ as: $$d((p_1,\dots,p_n),(q_1,\dots,q_n))=\left\|(d(p_1,q_1),\dots,d(p_n,q_n))\right\|_p$$ What I want to prove is that the $p$-norm $$\left\|x\right\|_p=\left(\sum_{i=1}^{n}\left|x_i\right|^p\right)^{1/p}$$ is really a norm. Showing that $\left\|x\right\|_p \geq 0$ being zero if and only if $x = 0$ was easy. Showing that $\left\|kx\right\|_p = \left|k\right|\left\|x\right\|_p$ was also easy. The triangle inequality is the thing that is not being easy to show. Indeed, I want to show that: for every $x,y \in \mathbb{R}^n$ we have: $$\left(\sum_{i=1}^{n}\left|x_i+y_i\right|^p\right)^{1/p}\leq \left(\sum_{i=1}^{n}\left|x_i\right|^p\right)^{1/p}+\left(\sum_{i=1}^{n}\left|y_i\right|^p\right)^{1/p}.$$ I thought that it might not be as difficult as it seems, but after trying a little without sucess I've searched on the internet and the I found that we need measure theory to prove that. Is there any more elementary proof of this inequality?",,"['real-analysis', 'metric-spaces', 'normed-spaces']"
45,"Application of Rolle's theorem? Establish existence of $c\in(a,b)$ such that $f(c)+f'(c)=f(c)f'(c)$",Application of Rolle's theorem? Establish existence of  such that,"c\in(a,b) f(c)+f'(c)=f(c)f'(c)","Following my question on Meta , I post this as a new question; it was originally asked by Gmgfg , and closed due to the lack of context, background, and shown  effort. Let $f\colon[a,b]\to\mathbb{R}$ be a function continuous on $[a,b]$ and differentiable on $(a,b)$ with $f(a)=f(b)=0$. Show that there exists $c\in(a,b)$ such that $f'(c)+f(c)=f(c)f'(c)$. Now, given the assumptions this looks like it should be a straightforward application of Rolle's theorem. However: as far as I can tell there is no simple auxiliary function $\Phi$ such that $\Phi'= f+f'-f\cdot f'$ to which one could apply Rolle's theorem. (If there is one, I failed to find it.) indeed, while the RHS could come from $\left(\frac{f^2}{2}\right)'$; it's mostly the LHS which looks difficult to handle (and there is no clear advantage I can see in introducing an antiderivative $F$ of $f$ to have it be $(F+f)'$). I verified the statement for some functions I could think of, such as $x\mapsto x(1-x)$ and $x\mapsto \sin \pi x$ on $[a,b]=[0,1]$. So, in that regard, it seems to hold at least against basic sanity checks. However , what bothers me is the lack of ""homogeneity."" Usually, in things like that I'm used to saying that ""without loss of generality, one can assume $[a,b]=[0,1]$."" It does not appear to be the case here: if, given $f$, one defines $g\colon[0,1]\to\mathbb{R}$ by $g(x) = f((b-a)x+a)$, finding $c\in(0,1)$ such that $g'(c)+g(c)=g(c)g'(c)$ does not directly yield $c'$ such that $f'(c')+f(c')=f(c')f'(c')$ (but rather would give $c'$ such that $f'(c')+(b-a)f(c')=f(c')f'(c')$, if I'm not mistaken). So, in short: is it true? And how to prove or disprove it — I'm at a loss, and embarrassed about it.","Following my question on Meta , I post this as a new question; it was originally asked by Gmgfg , and closed due to the lack of context, background, and shown  effort. Let $f\colon[a,b]\to\mathbb{R}$ be a function continuous on $[a,b]$ and differentiable on $(a,b)$ with $f(a)=f(b)=0$. Show that there exists $c\in(a,b)$ such that $f'(c)+f(c)=f(c)f'(c)$. Now, given the assumptions this looks like it should be a straightforward application of Rolle's theorem. However: as far as I can tell there is no simple auxiliary function $\Phi$ such that $\Phi'= f+f'-f\cdot f'$ to which one could apply Rolle's theorem. (If there is one, I failed to find it.) indeed, while the RHS could come from $\left(\frac{f^2}{2}\right)'$; it's mostly the LHS which looks difficult to handle (and there is no clear advantage I can see in introducing an antiderivative $F$ of $f$ to have it be $(F+f)'$). I verified the statement for some functions I could think of, such as $x\mapsto x(1-x)$ and $x\mapsto \sin \pi x$ on $[a,b]=[0,1]$. So, in that regard, it seems to hold at least against basic sanity checks. However , what bothers me is the lack of ""homogeneity."" Usually, in things like that I'm used to saying that ""without loss of generality, one can assume $[a,b]=[0,1]$."" It does not appear to be the case here: if, given $f$, one defines $g\colon[0,1]\to\mathbb{R}$ by $g(x) = f((b-a)x+a)$, finding $c\in(0,1)$ such that $g'(c)+g(c)=g(c)g'(c)$ does not directly yield $c'$ such that $f'(c')+f(c')=f(c')f'(c')$ (but rather would give $c'$ such that $f'(c')+(b-a)f(c')=f(c')f'(c')$, if I'm not mistaken). So, in short: is it true? And how to prove or disprove it — I'm at a loss, and embarrassed about it.",,"['real-analysis', 'rolles-theorem']"
46,Rudin's Principle of Mathematical Analysis Theorem 2.14 Question,Rudin's Principle of Mathematical Analysis Theorem 2.14 Question,,"$\mathbf{Theorem 2.14:}$ Let $A$ be the set of all sequences whose elements are the digits $0$ and $1$. Then A is uncountable, meaning there does not exist a one-to-one mapping of A onto $\mathbb{Z}$. For reference, elements of $A$ have this form $(0,1,0,1,0,0,0,1,1,1,1,1,\cdots)$ $\mathbf{Question:}$ Please bear with me. I know I'm wrong I don't know why. What is wrong with this logic? Let $E_1$ be the set containing all the sequences with just one $1$ in the sequence. i.e. $E_1 = \left\{(1,0,0,\cdots),(0,1,0,0,\cdots),(0,0,1,0,0,\cdots),\cdots\right\}$. $E_1$ is countable. Let $E_{2k}$ be the set containing all the sequences with only two $1$'s in the sequence where the next $1$ in the sequence is $k$ units next to the first $1$. $E_{21} = \left\{(1,1,0,\cdots),(0,1,1,0,0,\cdots),(0,0,1,1,0,0,\cdots),\cdots \right\}$ $E_{22} = \left\{(1,0,1,0,\cdots),(0,1,0,1,0,\cdots),(0,0,1,0,1,0,\cdots),\cdots \right\}$ Let the union of the sets $E_{2k}$ for ($k=1,2,3,\cdots$) be called $E_2$. $E_2$ is countable. Let $E_{3ij}$ be the set of sequences with only three $1$'s where the second $1$ is $i$ units away from the first $1$, and the third $1$ is $j$ units away from the second. For example: $E_{311} = \left\{(1,1,1,0,\cdots),(0,1,1,1,0,\cdots),(0,0,1,1,1,0,\cdots),\cdots\right\}$ $E_{312} = \left\{(1,1,0,1,\cdots),(0,1,1,0,1\cdots),(0,0,1,1,0,1,0,\cdots)\cdots\right\}$ $E_{322} = \left\{(1,0,1,0,1,\cdots),(0,1,0,1,0,1,0\cdots),(0,0,1,0,1,0,1\cdots)\cdots\right\}$ $E_{333} = \left\{(1,0,0,1,0,0,1\cdots),(0,1,0,0,1,0,0,1,\cdots),(0,0,1,0,0,1,0,0,1,\cdots)\cdots\right\}$ Let $E_3$ be the union of all the sets $E_{3ij}$ with ($i,j=1,2,3,\cdots$). Since each $E_{3ij}$ is countable the union of them is countable, so $E_3$ is countable. The zero sequence is just one sequence, $E_1$ is countable, $E_2$ is countable, $E_3$ is countable. Now if we continue defining the sets in this manner then each set $E_k$ (where $k$ is the number of $1$'s in the sequences) will be a countable set. So, the union of all these sets will be countable and it will represent all the sequences in A. Thank you in advance to anyone who reads all of this and answers!","$\mathbf{Theorem 2.14:}$ Let $A$ be the set of all sequences whose elements are the digits $0$ and $1$. Then A is uncountable, meaning there does not exist a one-to-one mapping of A onto $\mathbb{Z}$. For reference, elements of $A$ have this form $(0,1,0,1,0,0,0,1,1,1,1,1,\cdots)$ $\mathbf{Question:}$ Please bear with me. I know I'm wrong I don't know why. What is wrong with this logic? Let $E_1$ be the set containing all the sequences with just one $1$ in the sequence. i.e. $E_1 = \left\{(1,0,0,\cdots),(0,1,0,0,\cdots),(0,0,1,0,0,\cdots),\cdots\right\}$. $E_1$ is countable. Let $E_{2k}$ be the set containing all the sequences with only two $1$'s in the sequence where the next $1$ in the sequence is $k$ units next to the first $1$. $E_{21} = \left\{(1,1,0,\cdots),(0,1,1,0,0,\cdots),(0,0,1,1,0,0,\cdots),\cdots \right\}$ $E_{22} = \left\{(1,0,1,0,\cdots),(0,1,0,1,0,\cdots),(0,0,1,0,1,0,\cdots),\cdots \right\}$ Let the union of the sets $E_{2k}$ for ($k=1,2,3,\cdots$) be called $E_2$. $E_2$ is countable. Let $E_{3ij}$ be the set of sequences with only three $1$'s where the second $1$ is $i$ units away from the first $1$, and the third $1$ is $j$ units away from the second. For example: $E_{311} = \left\{(1,1,1,0,\cdots),(0,1,1,1,0,\cdots),(0,0,1,1,1,0,\cdots),\cdots\right\}$ $E_{312} = \left\{(1,1,0,1,\cdots),(0,1,1,0,1\cdots),(0,0,1,1,0,1,0,\cdots)\cdots\right\}$ $E_{322} = \left\{(1,0,1,0,1,\cdots),(0,1,0,1,0,1,0\cdots),(0,0,1,0,1,0,1\cdots)\cdots\right\}$ $E_{333} = \left\{(1,0,0,1,0,0,1\cdots),(0,1,0,0,1,0,0,1,\cdots),(0,0,1,0,0,1,0,0,1,\cdots)\cdots\right\}$ Let $E_3$ be the union of all the sets $E_{3ij}$ with ($i,j=1,2,3,\cdots$). Since each $E_{3ij}$ is countable the union of them is countable, so $E_3$ is countable. The zero sequence is just one sequence, $E_1$ is countable, $E_2$ is countable, $E_3$ is countable. Now if we continue defining the sets in this manner then each set $E_k$ (where $k$ is the number of $1$'s in the sequences) will be a countable set. So, the union of all these sets will be countable and it will represent all the sequences in A. Thank you in advance to anyone who reads all of this and answers!",,"['real-analysis', 'sequences-and-series', 'elementary-set-theory']"
47,Proof that $\inf A = -\sup(-A)$,Proof that,\inf A = -\sup(-A),"Let $A$ be a nonempty subset of real numbers which is bounded below. Let $-A$ be the set of of all numbers $-x$, where $x$ is in $A$. Prove that $\inf A = -\sup(-A)$ So far this is what I have Let $\alpha=\inf(A)$, which allows us to say that $\alpha \leq x$ for all $x \in A$. Therefore, we know that $-\alpha \geq -x$ for all $x \in -A$. Therefore we know that $-\alpha$ is an upper bound of $-A$. $\ \ \ \ $ Now let $b$ be the upper bound of $-A$. There exists $b \geq-x \implies-b \leq x$ for all $x \in A$. Hence, \begin{align} -b & \leq \alpha\\ -\alpha & \leq b\\ -\alpha & = - \inf(A) = \sup (-A) \end{align} By multiplying $-1$ on both sides, we get that $\inf(A) = -\sup (-A)$ Is my proof correct?","Let $A$ be a nonempty subset of real numbers which is bounded below. Let $-A$ be the set of of all numbers $-x$, where $x$ is in $A$. Prove that $\inf A = -\sup(-A)$ So far this is what I have Let $\alpha=\inf(A)$, which allows us to say that $\alpha \leq x$ for all $x \in A$. Therefore, we know that $-\alpha \geq -x$ for all $x \in -A$. Therefore we know that $-\alpha$ is an upper bound of $-A$. $\ \ \ \ $ Now let $b$ be the upper bound of $-A$. There exists $b \geq-x \implies-b \leq x$ for all $x \in A$. Hence, \begin{align} -b & \leq \alpha\\ -\alpha & \leq b\\ -\alpha & = - \inf(A) = \sup (-A) \end{align} By multiplying $-1$ on both sides, we get that $\inf(A) = -\sup (-A)$ Is my proof correct?",,"['real-analysis', 'proof-verification', 'supremum-and-infimum']"
48,dropping injectivity from multivariable change of variables,dropping injectivity from multivariable change of variables,,"The change of variables for multivariable integration in Euclidean space is almost always stated for a $C^1$ diffeomorphism $\phi$, giving the familiar equation (for continuous $f$, say) $$\boxed{\int_{\phi(U)}f=\int_U(f\circ\phi)\cdot|\det D\phi|}$$ Of course, this result by itself is not very useful in practice because a diffeomorphism is usually hard to come by. The better advanced calculus and multivariable analysis texts explain explicitly how the hypothesis that $\phi$ is injective with $\det D\phi\neq0$ can be relaxed to handle problems along sets of measure zero -- a result which is necessary for almost all practical applications of the theorem, starting with polar coordinates. Despite offering this slight generalization, very few of the standard texts state that the situation can be improved further still: there is an analogous theorem for arbitrary $C^1$ mappings $\phi$, not just those that are injective everywhere except on a set of measure zero. We simply account for how many times a point in the image gets hit by $\phi$, giving $$\boxed{\int_{\phi(U)}f\cdot\,\text{card}(\phi^{-1})=\int_U(f\circ\phi)\cdot|\det D\phi|}$$ where $\text{card}(\phi^{-1})$ measures the cardinality of $\phi^{-1}(x)$. I think this theorem is a lot more natural and satisfying than the first, for many reasons. For one thing, it removes a huge restriction, bringing the theorem closer to the standard one-variable change of variables for which injectivity is not required (though of course the one-variable theorem is really a theorem about differential forms). It emphasizes that a certain degree of regularity is what's important here, not injectivity. For another thing, it's not a big step from here to degree theory for smooth maps between closed manifolds or to the ""area formula"" in geometric measure theory. (Indeed, the factor $\text{card}(\phi^{-1})$ is a special case of what old references in geometric measure theory called the ""multiplicity function"" or the ""Banach indicatrix."") It's also used in multivariate probability to write down densities of non-injective transformations of random variables. And last, it's in the spirit of modern approaches to at least gesture at the most general possible result. The traditional statement is really just a special case; injectivity only becomes essential when we define the integral over a manifold (rather than a parametrized manifold), which we want to be independent of parametrization. I think teaching the more general result would greatly clarify these matters, which are a constant source of confusion to beginners. Yet many otherwise excellent multivariable analysis texts (Spivak, Rudin PMA and RCA, Folland, Loomis/Sternberg, Munkres, Duistermaat/Kolk, Burkill) don't mention this result, even in passing, as far as I can tell. I've had to hunt for discussions of it, and I've found it here: Zorich, Mathematical Analysis II (page 150, exercise 9, for the Riemann integral) Kuttler, Modern Analysis (page 258, for the Lebesgue integral) Csikós, Differential Geometry (page 72, for the Lebesgue integral) Ciarlet, Linear and Nonlinear Functional Analysis with Applications (page 34, for the Lebesgue integral) Bogachev, Measure Theory I (page 381, for the Lebesgue integral) the Planet Math page on multivariable change of variables (Theorem 2) I'm also confident I've seen it in some multivariable probability books, but I can't remember which. But none of these is a standard textbook, except perhaps for Zorich. My question : are there standard references with nice discussions of this extension of the more familiar result? Probability references are fine, but I'm especially curious whether I've missed some definitive treatment in one of the classic analysis texts. (Also feel free to speculate why so few texts mention it.)","The change of variables for multivariable integration in Euclidean space is almost always stated for a $C^1$ diffeomorphism $\phi$, giving the familiar equation (for continuous $f$, say) $$\boxed{\int_{\phi(U)}f=\int_U(f\circ\phi)\cdot|\det D\phi|}$$ Of course, this result by itself is not very useful in practice because a diffeomorphism is usually hard to come by. The better advanced calculus and multivariable analysis texts explain explicitly how the hypothesis that $\phi$ is injective with $\det D\phi\neq0$ can be relaxed to handle problems along sets of measure zero -- a result which is necessary for almost all practical applications of the theorem, starting with polar coordinates. Despite offering this slight generalization, very few of the standard texts state that the situation can be improved further still: there is an analogous theorem for arbitrary $C^1$ mappings $\phi$, not just those that are injective everywhere except on a set of measure zero. We simply account for how many times a point in the image gets hit by $\phi$, giving $$\boxed{\int_{\phi(U)}f\cdot\,\text{card}(\phi^{-1})=\int_U(f\circ\phi)\cdot|\det D\phi|}$$ where $\text{card}(\phi^{-1})$ measures the cardinality of $\phi^{-1}(x)$. I think this theorem is a lot more natural and satisfying than the first, for many reasons. For one thing, it removes a huge restriction, bringing the theorem closer to the standard one-variable change of variables for which injectivity is not required (though of course the one-variable theorem is really a theorem about differential forms). It emphasizes that a certain degree of regularity is what's important here, not injectivity. For another thing, it's not a big step from here to degree theory for smooth maps between closed manifolds or to the ""area formula"" in geometric measure theory. (Indeed, the factor $\text{card}(\phi^{-1})$ is a special case of what old references in geometric measure theory called the ""multiplicity function"" or the ""Banach indicatrix."") It's also used in multivariate probability to write down densities of non-injective transformations of random variables. And last, it's in the spirit of modern approaches to at least gesture at the most general possible result. The traditional statement is really just a special case; injectivity only becomes essential when we define the integral over a manifold (rather than a parametrized manifold), which we want to be independent of parametrization. I think teaching the more general result would greatly clarify these matters, which are a constant source of confusion to beginners. Yet many otherwise excellent multivariable analysis texts (Spivak, Rudin PMA and RCA, Folland, Loomis/Sternberg, Munkres, Duistermaat/Kolk, Burkill) don't mention this result, even in passing, as far as I can tell. I've had to hunt for discussions of it, and I've found it here: Zorich, Mathematical Analysis II (page 150, exercise 9, for the Riemann integral) Kuttler, Modern Analysis (page 258, for the Lebesgue integral) Csikós, Differential Geometry (page 72, for the Lebesgue integral) Ciarlet, Linear and Nonlinear Functional Analysis with Applications (page 34, for the Lebesgue integral) Bogachev, Measure Theory I (page 381, for the Lebesgue integral) the Planet Math page on multivariable change of variables (Theorem 2) I'm also confident I've seen it in some multivariable probability books, but I can't remember which. But none of these is a standard textbook, except perhaps for Zorich. My question : are there standard references with nice discussions of this extension of the more familiar result? Probability references are fine, but I'm especially curious whether I've missed some definitive treatment in one of the classic analysis texts. (Also feel free to speculate why so few texts mention it.)",,"['real-analysis', 'multivariable-calculus', 'reference-request']"
49,"Can $ f\colon \mathbb{R}^k \to \mathbb{R}^n$ such that $ \forall y \in \operatorname{im}(f)$, $f^{-1}(y) = \{a_y,b_y\} $ be continuous?","Can  such that ,  be continuous?"," f\colon \mathbb{R}^k \to \mathbb{R}^n  \forall y \in \operatorname{im}(f) f^{-1}(y) = \{a_y,b_y\} ","This is the problem we want to solve: Can $f\colon \mathbb{R}^k \to \mathbb{R}^n$ such that $ \forall y \in \operatorname{im}(f)$,   $ f^{-1}(y) = \{a_y,b_y\}, a_y \neq b_y $ be continuous? Originally I've seen this question on an exam but it was stated only for the case $ k = n = 1 $ and $f$ surjective, which made it really easy to show $f$ can't be continuous, by using the Weierstrass extreme value theorem. A very similar argument seem to work for any $k$, as long as $n=1$. However, for general $k$ and $n$ this seems much harder. I don't see how surjectivity affects this problem, so I've dropped this assumption for now. Edit:Slup commented below, showing the relevance of surjectivity for this question. Induction on $n$ and looking at projections of $f$ onto individual coordinates seemed tempting at first, but the composition of $f$ with a projection seems to lose any traces of the property that the inverse image of a point = exactly two points, so I don't see how this could be useful. Trying to visualise this for $k=n=2$, it intuitively seems that in order to transform the space in this way, we would have to 'tear' it along some curve. For bigger $k = n$, that becomes 'tearing' along some $n-1$ dimensional manifold, but that's obviously completely informal, sort of useless and I completely have no idea how this idea could be translated into a formal proof. Bonus question: Does the answer or the proof change in a significant way if we limit the domain to $ f:\overline{\mathbb{B}^k} \to \mathbb{R}^n $? We operate on a compact ball now, so that's fairly different from $\mathbb{R}^k$.","This is the problem we want to solve: Can $f\colon \mathbb{R}^k \to \mathbb{R}^n$ such that $ \forall y \in \operatorname{im}(f)$,   $ f^{-1}(y) = \{a_y,b_y\}, a_y \neq b_y $ be continuous? Originally I've seen this question on an exam but it was stated only for the case $ k = n = 1 $ and $f$ surjective, which made it really easy to show $f$ can't be continuous, by using the Weierstrass extreme value theorem. A very similar argument seem to work for any $k$, as long as $n=1$. However, for general $k$ and $n$ this seems much harder. I don't see how surjectivity affects this problem, so I've dropped this assumption for now. Edit:Slup commented below, showing the relevance of surjectivity for this question. Induction on $n$ and looking at projections of $f$ onto individual coordinates seemed tempting at first, but the composition of $f$ with a projection seems to lose any traces of the property that the inverse image of a point = exactly two points, so I don't see how this could be useful. Trying to visualise this for $k=n=2$, it intuitively seems that in order to transform the space in this way, we would have to 'tear' it along some curve. For bigger $k = n$, that becomes 'tearing' along some $n-1$ dimensional manifold, but that's obviously completely informal, sort of useless and I completely have no idea how this idea could be translated into a formal proof. Bonus question: Does the answer or the proof change in a significant way if we limit the domain to $ f:\overline{\mathbb{B}^k} \to \mathbb{R}^n $? We operate on a compact ball now, so that's fairly different from $\mathbb{R}^k$.",,"['real-analysis', 'general-topology', 'continuity']"
50,Does an increasing sequence of reals converge if the difference of consecutive terms approaches zero?,Does an increasing sequence of reals converge if the difference of consecutive terms approaches zero?,,"If $a_n$ is a sequence such that $$a_1 \leq a_2 \leq a_3 \leq \dotsb$$ and has the property that $a_{n+1}-a_n \to 0$ , then can we conclude that $a_n$ is convergent? I know that without the condition that the sequence is increasing, this is not true, as we could consider the sequence given in this answer to a similar question that does not require the sequence to be increasing. $$0, 1, \frac12, 0, \frac13, \frac23, 1, \frac34, \frac12, \frac14, 0, \frac15, \frac25, \frac35, \frac45, 1, \dotsc$$ This oscillates between $0$ and $1$ , while the difference of consecutive terms approaches $0$ since the difference is always of the form $\pm\frac1m$ and $m$ increases the further we go in this sequence. So how can we use the condition that $a_n$ is increasing to show that $a_n$ must converge?  Or is this still not sufficient?","If is a sequence such that and has the property that , then can we conclude that is convergent? I know that without the condition that the sequence is increasing, this is not true, as we could consider the sequence given in this answer to a similar question that does not require the sequence to be increasing. This oscillates between and , while the difference of consecutive terms approaches since the difference is always of the form and increases the further we go in this sequence. So how can we use the condition that is increasing to show that must converge?  Or is this still not sufficient?","a_n a_1 \leq a_2 \leq a_3 \leq \dotsb a_{n+1}-a_n \to 0 a_n 0, 1, \frac12, 0, \frac13, \frac23, 1, \frac34, \frac12, \frac14, 0, \frac15, \frac25, \frac35, \frac45, 1, \dotsc 0 1 0 \pm\frac1m m a_n a_n","['real-analysis', 'sequences-and-series', 'convergence-divergence', 'faq']"
51,Evaluating $\int_0^1 x \tan(\pi x) \log(\sin(\pi x))dx$,Evaluating,\int_0^1 x \tan(\pi x) \log(\sin(\pi x))dx,"What starting point would you recommend me for the one below? $$\int_0^1 x \tan(\pi x) \log(\sin(\pi x))dx $$ EDIT Thanks to Felix Marin, we know the integral evaluates to $$\displaystyle{\large{\ln^{2}\left(\, 2\,\right) \over 2\pi}}$$","What starting point would you recommend me for the one below? $$\int_0^1 x \tan(\pi x) \log(\sin(\pi x))dx $$ EDIT Thanks to Felix Marin, we know the integral evaluates to $$\displaystyle{\large{\ln^{2}\left(\, 2\,\right) \over 2\pi}}$$",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
52,"Looking for closed-forms of $\int_0^{\pi/4}\ln^2(\sin x)\,dx$ and $\int_0^{\pi/4}\ln^2(\cos x)\,dx$",Looking for closed-forms of  and,"\int_0^{\pi/4}\ln^2(\sin x)\,dx \int_0^{\pi/4}\ln^2(\cos x)\,dx","A few days ago, I posted the following problems Prove that \begin{equation} \int_0^{\pi/2}\ln^2(\cos x)\,dx=\frac{\pi}{2}\ln^2 2+\frac{\pi^3}{24}\\[20pt] -\int_0^{\pi/2}\ln^3(\cos x)\,dx=\frac{\pi}{2}\ln^3 2+\frac{\pi^3}{8}\ln 2 +\frac{3\pi}{4}\zeta(3) \end{equation} and the OP receives some good answers even I then could answer it. My next question is finding the closed-forms for \begin{align} \int_0^{\pi/4}\ln^2(\sin x)\,dx\tag1\\[20pt] \int_0^{\pi/4}\ln^2(\cos x)\,dx\tag2\\[20pt] \int_0^1\frac{\ln t~\ln\big(1+t^2\big)}{1+t^2}dt\tag3 \end{align} I have a strong feeling that the closed-forms exist because we have nice closed-forms for \begin{equation} \int_0^{\pi/4}\ln(\sin x)\ dx=-\frac12\left(C+\frac\pi2\ln2\right)\\ \text{and}\\ \int_0^{\pi/4}\ln(\cos x)\ dx=\frac12\left(C-\frac\pi2\ln2\right). \end{equation} The complete proofs can be found here . As shown by Mr. Lucian in his answer below, the three integrals are closely related , so finding the closed-form one of them will also find the other closed-forms. How to find the closed-forms of the integrals? Could anyone here please help me to find the closed-form, only one of them , preferably with elementary ways (high school methods)? If possible , please avoiding contour integration and double summation . Any help would be greatly appreciated. Thank you.","A few days ago, I posted the following problems Prove that \begin{equation} \int_0^{\pi/2}\ln^2(\cos x)\,dx=\frac{\pi}{2}\ln^2 2+\frac{\pi^3}{24}\\[20pt] -\int_0^{\pi/2}\ln^3(\cos x)\,dx=\frac{\pi}{2}\ln^3 2+\frac{\pi^3}{8}\ln 2 +\frac{3\pi}{4}\zeta(3) \end{equation} and the OP receives some good answers even I then could answer it. My next question is finding the closed-forms for \begin{align} \int_0^{\pi/4}\ln^2(\sin x)\,dx\tag1\\[20pt] \int_0^{\pi/4}\ln^2(\cos x)\,dx\tag2\\[20pt] \int_0^1\frac{\ln t~\ln\big(1+t^2\big)}{1+t^2}dt\tag3 \end{align} I have a strong feeling that the closed-forms exist because we have nice closed-forms for \begin{equation} \int_0^{\pi/4}\ln(\sin x)\ dx=-\frac12\left(C+\frac\pi2\ln2\right)\\ \text{and}\\ \int_0^{\pi/4}\ln(\cos x)\ dx=\frac12\left(C-\frac\pi2\ln2\right). \end{equation} The complete proofs can be found here . As shown by Mr. Lucian in his answer below, the three integrals are closely related , so finding the closed-form one of them will also find the other closed-forms. How to find the closed-forms of the integrals? Could anyone here please help me to find the closed-form, only one of them , preferably with elementary ways (high school methods)? If possible , please avoiding contour integration and double summation . Any help would be greatly appreciated. Thank you.",,"['real-analysis', 'calculus', 'integration', 'improper-integrals', 'harmonic-numbers']"
53,limit of integral $n\int_{0}^{1} x^n f(x) \text{d}x$ as $n\rightarrow \infty$,limit of integral  as,n\int_{0}^{1} x^n f(x) \text{d}x n\rightarrow \infty,"I am trying to solve the following problem at the level of a senior undergrad analysis level. So, the problem is as follows: We are given a function $f$ which is continuous on the interval $\left [ 0,1 \right ]$, and the question is to find the limit: $$\lim_{n\rightarrow \infty}\int_{0}^{1}x^{n}f(x)dx\;.$$ The second part of the problem is to deduce the following limit: $$\lim_{n\rightarrow \infty}n\int_{0}^{1}x^{n}f(x)dx\;.$$ For the first part: I just did the following: For every $0\leq x< 1$: $x\leq M$, where $0< M< 1$. Then: $$\int_{0}^{1}x^{n}f(x)dx\leq M^{n}\int_{0}^{1}f(x)dx\;.$$ Then: $$\lim_{n\rightarrow \infty }\int_{0}^{1}x^{n}f(x)dx\leq \lim_{n\rightarrow \infty }M^{n}\int_{0}^{1}f(x)dx= 0.\int_{0}^{1}f(x)dx=0\;,$$ so $$\lim_{n\rightarrow \infty }\int_{0}^{1}x^{n}f(x)dx=\lim_{n\rightarrow \infty }f(1)\int_{0}^{1}1dx=f(1)\;.$$ Does that make sense? If not, please show me the correct one. As for the second part, I have no idea what to do. Any help?","I am trying to solve the following problem at the level of a senior undergrad analysis level. So, the problem is as follows: We are given a function $f$ which is continuous on the interval $\left [ 0,1 \right ]$, and the question is to find the limit: $$\lim_{n\rightarrow \infty}\int_{0}^{1}x^{n}f(x)dx\;.$$ The second part of the problem is to deduce the following limit: $$\lim_{n\rightarrow \infty}n\int_{0}^{1}x^{n}f(x)dx\;.$$ For the first part: I just did the following: For every $0\leq x< 1$: $x\leq M$, where $0< M< 1$. Then: $$\int_{0}^{1}x^{n}f(x)dx\leq M^{n}\int_{0}^{1}f(x)dx\;.$$ Then: $$\lim_{n\rightarrow \infty }\int_{0}^{1}x^{n}f(x)dx\leq \lim_{n\rightarrow \infty }M^{n}\int_{0}^{1}f(x)dx= 0.\int_{0}^{1}f(x)dx=0\;,$$ so $$\lim_{n\rightarrow \infty }\int_{0}^{1}x^{n}f(x)dx=\lim_{n\rightarrow \infty }f(1)\int_{0}^{1}1dx=f(1)\;.$$ Does that make sense? If not, please show me the correct one. As for the second part, I have no idea what to do. Any help?",,"['real-analysis', 'calculus', 'integration', 'limits', 'analysis']"
54,"If a series converges, then the sequence of terms converges to $0$.","If a series converges, then the sequence of terms converges to .",0,"Following the guidelines suggested in this meta discussion , I am going to post a proposed proof as an answer to the theorem below. I believe the proof works, but would appreciate any needed corrections. Theorem If a series $\sum_{n=1}^{\infty}a_n$ of real numbers converges then $\lim_{n \to \infty}a_n = 0$","Following the guidelines suggested in this meta discussion , I am going to post a proposed proof as an answer to the theorem below. I believe the proof works, but would appreciate any needed corrections. Theorem If a series $\sum_{n=1}^{\infty}a_n$ of real numbers converges then $\lim_{n \to \infty}a_n = 0$",,"['real-analysis', 'sequences-and-series']"
55,What is the mean value of $|\sin x +\sin (\pi x)|$?,What is the mean value of ?,|\sin x +\sin (\pi x)|,"I was thinking about non-periodic trigonometric functions, and came up with this question: What is the mean value of $|\sin x +\sin (\pi x)|$ ? Here is the graph of $y=|\sin x +\sin (\pi x)|$ : The mean value should be $\lim\limits_{n\to\infty} \frac{1}{n}\int_0^n |\sin x +\sin (\pi x)|dx$ , but I don't know how to evaluate this. I tried complex numbers, to no avail. Desmos and Wolfram don't do a good job with numerical investigation of this limit, but we can consider the equivalent limit $\lim\limits_{n\to\infty} f(n)$ where $f(n)=\frac{1}{n}\sum\limits_{k=1}^n |\sin (\sqrt2 k)+\sin (\pi \sqrt2 k)|$ . (I put $\sqrt2$ in front of $k$ so that that the terms do not have integer multiples of $\pi$ .) $f(10^{5})\approx1.0000113115\left(\dfrac{8}{\pi^2}\right)$ $f(10^{6})\approx1.0000003459\left(\dfrac{8}{\pi^2}\right)$ $f(10^{7})\approx1.0000001068\left(\dfrac{8}{\pi^2}\right)$ $f(10^{8})\approx1.0000000137\left(\dfrac{8}{\pi^2}\right)$ This suggests that the answer is $\dfrac{8}{\pi^2}$ . Note: In the question, if we replace $\pi$ with any other irrational number, it seems that we always get the same mean value. EDIT: Here is my attempt to generalize this question. EDIT2: Related claims: The mean value of $|(\sin x)(\sin (\pi x))|$ is $\dfrac{4}{\pi^2}$ . The mean value of $\dfrac{|\sin x|}{|\sin (\pi x)|+1}$ is $\dfrac{4}{\pi^2}$ . The mean value of $|\sin (x+\sin (\pi x))|$ is $\dfrac{2}{\pi}$ . The mean value of $\sin^2 (x+\sin (\pi x))$ is $\dfrac{1}{2}$ . The mean value of $\dfrac{1}{|\sin x+\sin (\pi x)|+2}$ is $\dfrac{4G}{\pi^2}$ , where $G$ is Catalan's constant .","I was thinking about non-periodic trigonometric functions, and came up with this question: What is the mean value of ? Here is the graph of : The mean value should be , but I don't know how to evaluate this. I tried complex numbers, to no avail. Desmos and Wolfram don't do a good job with numerical investigation of this limit, but we can consider the equivalent limit where . (I put in front of so that that the terms do not have integer multiples of .) This suggests that the answer is . Note: In the question, if we replace with any other irrational number, it seems that we always get the same mean value. EDIT: Here is my attempt to generalize this question. EDIT2: Related claims: The mean value of is . The mean value of is . The mean value of is . The mean value of is . The mean value of is , where is Catalan's constant .",|\sin x +\sin (\pi x)| y=|\sin x +\sin (\pi x)| \lim\limits_{n\to\infty} \frac{1}{n}\int_0^n |\sin x +\sin (\pi x)|dx \lim\limits_{n\to\infty} f(n) f(n)=\frac{1}{n}\sum\limits_{k=1}^n |\sin (\sqrt2 k)+\sin (\pi \sqrt2 k)| \sqrt2 k \pi f(10^{5})\approx1.0000113115\left(\dfrac{8}{\pi^2}\right) f(10^{6})\approx1.0000003459\left(\dfrac{8}{\pi^2}\right) f(10^{7})\approx1.0000001068\left(\dfrac{8}{\pi^2}\right) f(10^{8})\approx1.0000000137\left(\dfrac{8}{\pi^2}\right) \dfrac{8}{\pi^2} \pi |(\sin x)(\sin (\pi x))| \dfrac{4}{\pi^2} \dfrac{|\sin x|}{|\sin (\pi x)|+1} \dfrac{4}{\pi^2} |\sin (x+\sin (\pi x))| \dfrac{2}{\pi} \sin^2 (x+\sin (\pi x)) \dfrac{1}{2} \dfrac{1}{|\sin x+\sin (\pi x)|+2} \dfrac{4G}{\pi^2} G,"['real-analysis', 'limits', 'definite-integrals', 'average', 'ergodic-theory']"
56,What is wrong in this proof: That $\mathbb{R}$ has measure zero,What is wrong in this proof: That  has measure zero,\mathbb{R},"Consider $\mathbb{Q}$ which is countable, we may enumerate $\mathbb{Q}=\{q_1, q_2, \dots\}$. For each rational number $q_k$, cover it by an open interval $I_k$ centered at $q_k$ with radius $\epsilon/2^k$. The total length of the intervals is a geometric progression that sums up to $\epsilon$. Each real number is arbitrarily close to a rational number since $\mathbb{Q}$ is dense in $\mathbb{R}$. Thus, each real number is in one of the open intervals. Thus the entire real line is covered by the union of the $I_k$, thus $\mathbb{R}$ is a null set with measure zero. Clearly there is something wrong in the above proof, however I am not sure where is it? Thanks for any help.","Consider $\mathbb{Q}$ which is countable, we may enumerate $\mathbb{Q}=\{q_1, q_2, \dots\}$. For each rational number $q_k$, cover it by an open interval $I_k$ centered at $q_k$ with radius $\epsilon/2^k$. The total length of the intervals is a geometric progression that sums up to $\epsilon$. Each real number is arbitrarily close to a rational number since $\mathbb{Q}$ is dense in $\mathbb{R}$. Thus, each real number is in one of the open intervals. Thus the entire real line is covered by the union of the $I_k$, thus $\mathbb{R}$ is a null set with measure zero. Clearly there is something wrong in the above proof, however I am not sure where is it? Thanks for any help.",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
57,"Prove that if $\sum{a_n}$ converges absolutely, then $\sum{a_n^2}$ converges absolutely","Prove that if  converges absolutely, then  converges absolutely",\sum{a_n} \sum{a_n^2},"I'm trying to re-learn my undergrad math, and I'm using Stephen Abbot's Understanding Analysis . In section 2.7, he has the following exercise: Exercise 2.7.5 (a) Show that if $\sum{a_n}$ converges absolutely, then $\sum{a_n^2}$ also converges absolutely. Does this proposition hold without absolute convergence? I'm posting about this here because my answer to that last question about whether the proposition holds without absolute convergence is ""Yes"", but my suspicions are raised by him merely asking the question. Usually questions like this are asked to point out that certain conditions are necessary in the statement of propositions, theorems, etc. I just want to see if I'm missing something here. Anyway, here's how I prove the absolute convergence of $\sum{a_n^2}$, and note that I never use the fact that $\sum{a_n}$ is absolutely convergent: Proof: Let $s_n = \sum_{i=1}^n{a_n^2}$. I want to show that $(s_n)$ is a Cauchy sequence. So, let $\epsilon > 0$ and $n > m$, and consider, \begin{equation}   \begin{aligned}     |s_n - s_m| &= |(a_1^2 + a_2^2 + \cdots a_n^2) - (a_1^2 + a_2^2 + \cdots a_m^2)|\\                 &= |(a_{m+1})^2 + (a_{m+2})^2 + \cdots + a_n^2|\\                 &= |a_{m+1}|^2 + |a_{m+2}|^2 + \cdots + |a_n|^2\\   \end{aligned} \end{equation} Now, since $\sum{a_m}$ converges, the sequence $(a_m)$ has limit 0. Therefore I can choose an $N$ such that $|a_m| < \sqrt{\epsilon/n}$ for all $m > N$. So for all $n > m> N$,  we have, \begin{equation}   \begin{aligned}     |s_n - s_m| &= |a_{m+1}|^2 + |a_{m+2}|^2 + \cdots + |a_n|^2\\                 &< \left(\sqrt{\frac{\epsilon}{n}}\right)^2 + \left(\sqrt{\frac{\epsilon}{n}}\right)^2 + \cdots + \left(\sqrt{\frac{\epsilon}{n}}\right)^2\\                 &< \left(\sqrt{\frac{\epsilon}{n-m}}\right)^2 + \left(\sqrt{\frac{\epsilon}{n-m}}\right)^2 + \cdots + \left(\sqrt{\frac{\epsilon}{n-m}}\right)^2\\                 &= \epsilon   \end{aligned} \end{equation} Therefore $(s_n)$ is Cauchy and $\sum{a_n^2}$ converges. And since $\sum{a_n^2} = \sum{|a_n^2|}$, the series $\sum{a_n^2}$ is absolutely convergent. ∎ Am I doing something wrong here? Am I right in thinking that $\sum{a_n}$ need not be absolutely convergent for $\sum{a_n^2}$ to be absolutely convergent?","I'm trying to re-learn my undergrad math, and I'm using Stephen Abbot's Understanding Analysis . In section 2.7, he has the following exercise: Exercise 2.7.5 (a) Show that if $\sum{a_n}$ converges absolutely, then $\sum{a_n^2}$ also converges absolutely. Does this proposition hold without absolute convergence? I'm posting about this here because my answer to that last question about whether the proposition holds without absolute convergence is ""Yes"", but my suspicions are raised by him merely asking the question. Usually questions like this are asked to point out that certain conditions are necessary in the statement of propositions, theorems, etc. I just want to see if I'm missing something here. Anyway, here's how I prove the absolute convergence of $\sum{a_n^2}$, and note that I never use the fact that $\sum{a_n}$ is absolutely convergent: Proof: Let $s_n = \sum_{i=1}^n{a_n^2}$. I want to show that $(s_n)$ is a Cauchy sequence. So, let $\epsilon > 0$ and $n > m$, and consider, \begin{equation}   \begin{aligned}     |s_n - s_m| &= |(a_1^2 + a_2^2 + \cdots a_n^2) - (a_1^2 + a_2^2 + \cdots a_m^2)|\\                 &= |(a_{m+1})^2 + (a_{m+2})^2 + \cdots + a_n^2|\\                 &= |a_{m+1}|^2 + |a_{m+2}|^2 + \cdots + |a_n|^2\\   \end{aligned} \end{equation} Now, since $\sum{a_m}$ converges, the sequence $(a_m)$ has limit 0. Therefore I can choose an $N$ such that $|a_m| < \sqrt{\epsilon/n}$ for all $m > N$. So for all $n > m> N$,  we have, \begin{equation}   \begin{aligned}     |s_n - s_m| &= |a_{m+1}|^2 + |a_{m+2}|^2 + \cdots + |a_n|^2\\                 &< \left(\sqrt{\frac{\epsilon}{n}}\right)^2 + \left(\sqrt{\frac{\epsilon}{n}}\right)^2 + \cdots + \left(\sqrt{\frac{\epsilon}{n}}\right)^2\\                 &< \left(\sqrt{\frac{\epsilon}{n-m}}\right)^2 + \left(\sqrt{\frac{\epsilon}{n-m}}\right)^2 + \cdots + \left(\sqrt{\frac{\epsilon}{n-m}}\right)^2\\                 &= \epsilon   \end{aligned} \end{equation} Therefore $(s_n)$ is Cauchy and $\sum{a_n^2}$ converges. And since $\sum{a_n^2} = \sum{|a_n^2|}$, the series $\sum{a_n^2}$ is absolutely convergent. ∎ Am I doing something wrong here? Am I right in thinking that $\sum{a_n}$ need not be absolutely convergent for $\sum{a_n^2}$ to be absolutely convergent?",,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'cauchy-sequences']"
58,thoughts about $f(f(x))=e^x$,thoughts about,f(f(x))=e^x,"I was thinking, inspired by mathlinks, precisely from this post , if there exists a continuous real function $f:\mathbb R\to\mathbb R$ such that $$f(f(x))=e^x.$$ However I have not still been able to come up with an answer. I would like to share this problem with you. I'm not aware of its level, though i wouldn't classify it as homework, so I'm not giving it the homework tag. If anybody feels that this problem is in reality easy or looks like an homework, please feel free to add that tag. EDIT for those interested in the complex case, I've found this on MathOverflow. It's the first answer. Wow! link","I was thinking, inspired by mathlinks, precisely from this post , if there exists a continuous real function $f:\mathbb R\to\mathbb R$ such that $$f(f(x))=e^x.$$ However I have not still been able to come up with an answer. I would like to share this problem with you. I'm not aware of its level, though i wouldn't classify it as homework, so I'm not giving it the homework tag. If anybody feels that this problem is in reality easy or looks like an homework, please feel free to add that tag. EDIT for those interested in the complex case, I've found this on MathOverflow. It's the first answer. Wow! link",,"['calculus', 'real-analysis', 'functional-equations', 'tetration']"
59,"Nonzero $f \in C([0, 1])$ for which $\int_0^1 f(x)x^n dx = 0$ for all $n$",Nonzero  for which  for all,"f \in C([0, 1]) \int_0^1 f(x)x^n dx = 0 n","As the title says, I'm wondering if there is a continuous function such that $f$ is nonzero on $[0, 1]$, and for which $\int_0^1 f(x)x^n dx = 0$ for all $n \geq 1$. I am trying to solve a problem proving that if (on $C([0, 1])$) $\int_0^1 f(x)x^n dx = 0$ for all $n \geq 0$, then $f$ must be identically zero. I presume then we do require the $n=0$ case to hold too, otherwise it wouldn't be part of the statement. Is there ay function which is not identically zero which satisfies $\int_0^1 f(x)x^n dx = 0$ for all $n \geq 1$? The statement I am attempting to prove is homework, but this is just idle curiosity (though I will tag it as homework anyway since it is related). Thank you!","As the title says, I'm wondering if there is a continuous function such that $f$ is nonzero on $[0, 1]$, and for which $\int_0^1 f(x)x^n dx = 0$ for all $n \geq 1$. I am trying to solve a problem proving that if (on $C([0, 1])$) $\int_0^1 f(x)x^n dx = 0$ for all $n \geq 0$, then $f$ must be identically zero. I presume then we do require the $n=0$ case to hold too, otherwise it wouldn't be part of the statement. Is there ay function which is not identically zero which satisfies $\int_0^1 f(x)x^n dx = 0$ for all $n \geq 1$? The statement I am attempting to prove is homework, but this is just idle curiosity (though I will tag it as homework anyway since it is related). Thank you!",,"['real-analysis', 'analysis', 'integration']"
60,How to show the that a set $A$ nowhere dense is equivalent to the complement of $A$ containing a dense open set?,How to show the that a set  nowhere dense is equivalent to the complement of  containing a dense open set?,A A,"I was reading a textbook and saw an alternative formulation of nowhere dense. I am not sure how to prove this alternate formulation below: The Normal Nowhere Dense Statement: Let $X$ be a metric space. A subset $A ⊆ X$ is called nowhere dense in $X$ if the interior of the closure of $A$ is empty, i.e. $(\overline{A})^{\circ} = ∅$. Otherwise put, $A$ is nowhere dense iﬀ it is contained in a closed set with empty interior. Alternate Formulation: ""Passing to complements, we can say equivalently that $A$ is nowhere dense iﬀ its complement contains a dense open set."" Does anyone know how I can prove this? It seems rather painfully straightforward but I am not sure how to show it exactly. Thank you!","I was reading a textbook and saw an alternative formulation of nowhere dense. I am not sure how to prove this alternate formulation below: The Normal Nowhere Dense Statement: Let $X$ be a metric space. A subset $A ⊆ X$ is called nowhere dense in $X$ if the interior of the closure of $A$ is empty, i.e. $(\overline{A})^{\circ} = ∅$. Otherwise put, $A$ is nowhere dense iﬀ it is contained in a closed set with empty interior. Alternate Formulation: ""Passing to complements, we can say equivalently that $A$ is nowhere dense iﬀ its complement contains a dense open set."" Does anyone know how I can prove this? It seems rather painfully straightforward but I am not sure how to show it exactly. Thank you!",,"['real-analysis', 'general-topology']"
61,Interchanging the order of differentiation and summation,Interchanging the order of differentiation and summation,,"Can the order of a differentiation and summation be interchanged, and if so, what is the basis of the justification for this? E.g. is $\frac{\mathrm{d}}{\mathrm{d}x}\sum_{n=1}^{\infty}f_n(x)$ equal to $\sum_{n=1}^{\infty}\frac{\mathrm{d}}{\mathrm{d}x}f_n(x)$ and how can it be proven? My intuition for this is that it should be the case, since in the limit, the summation becomes an integral and this can be interchanged with the differentiation operator, but I don't know how to justify this? Would it suffice to say that $\frac{\mathrm{d}}{\mathrm{d}x}f_1(x)+\frac{\mathrm{d}}{\mathrm{d}x}f_2(x) = \frac{\mathrm{d}}{\mathrm{d}x}(f_1(x)+f_2(x)$ because it is a linear operator and then somehow extend this to an infinite sum? Thanks in advance!","Can the order of a differentiation and summation be interchanged, and if so, what is the basis of the justification for this? E.g. is equal to and how can it be proven? My intuition for this is that it should be the case, since in the limit, the summation becomes an integral and this can be interchanged with the differentiation operator, but I don't know how to justify this? Would it suffice to say that because it is a linear operator and then somehow extend this to an infinite sum? Thanks in advance!",\frac{\mathrm{d}}{\mathrm{d}x}\sum_{n=1}^{\infty}f_n(x) \sum_{n=1}^{\infty}\frac{\mathrm{d}}{\mathrm{d}x}f_n(x) \frac{\mathrm{d}}{\mathrm{d}x}f_1(x)+\frac{\mathrm{d}}{\mathrm{d}x}f_2(x) = \frac{\mathrm{d}}{\mathrm{d}x}(f_1(x)+f_2(x),"['real-analysis', 'calculus', 'sequences-and-series', 'derivatives']"
62,"Let $a_1=\sqrt{6},a_{n+1}=\sqrt{6+a_n}$. Find $\lim\limits_{n \to \infty} (a_n-3)6^n$.",Let . Find .,"a_1=\sqrt{6},a_{n+1}=\sqrt{6+a_n} \lim\limits_{n \to \infty} (a_n-3)6^n","Let $a_1=\sqrt{6}$ , $a_{n+1}=\sqrt{6+a_n}$ . Find $\lim\limits_{n \to  \infty} (a_n-3)6^n$ . First, we may obtain $\lim\limits_{n\to \infty}a_n=3$ . Hence, $\lim\limits_{n \to \infty}(a_n-3)b^n$ belongs to a type of limit with the form $0 \cdot \infty$ . Moreover, we obtained a similar result here , which is related to the form $\lim\limits_{n \to \infty} (a_n-3)9^n$ . How should I proceed?","Let , . Find . First, we may obtain . Hence, belongs to a type of limit with the form . Moreover, we obtained a similar result here , which is related to the form . How should I proceed?","a_1=\sqrt{6} a_{n+1}=\sqrt{6+a_n} \lim\limits_{n \to
 \infty} (a_n-3)6^n \lim\limits_{n\to \infty}a_n=3 \lim\limits_{n \to \infty}(a_n-3)b^n 0 \cdot \infty \lim\limits_{n \to \infty} (a_n-3)9^n","['real-analysis', 'calculus', 'sequences-and-series', 'limits']"
63,use of $\sum $ for uncountable indexing set,use of  for uncountable indexing set,\sum ,I was wondering whether it makes sense to use the $\sum $ notation for uncountable indexing sets. For example it seems to me it would not make sense to say $$ \sum_{a \in A} a \quad \text{where A is some uncountable indexing set e.g. some $A \subset \mathbb{R}$ } $$ Would it be better to avoid the above notation in general for uncountable indexing sets ? Any help in making better sense of this would be very appreciated.,I was wondering whether it makes sense to use the $\sum $ notation for uncountable indexing sets. For example it seems to me it would not make sense to say $$ \sum_{a \in A} a \quad \text{where A is some uncountable indexing set e.g. some $A \subset \mathbb{R}$ } $$ Would it be better to avoid the above notation in general for uncountable indexing sets ? Any help in making better sense of this would be very appreciated.,,"['real-analysis', 'analysis', 'summation', 'notation', 'convention']"
64,$\int_{0}^{\infty} \frac{\cos x - e^{-x^2}}{x} \ dx$ Evaluate Integral,Evaluate Integral,\int_{0}^{\infty} \frac{\cos x - e^{-x^2}}{x} \ dx,Evaluate $$\int_{0}^{\infty} \frac{\cos x - e^{-x^2}}{x} \ dx$$,Evaluate $$\int_{0}^{\infty} \frac{\cos x - e^{-x^2}}{x} \ dx$$,,"['calculus', 'real-analysis', 'integration', 'improper-integrals']"
65,Improper integral diverges,Improper integral diverges,,"Let a real-valued function $f$ be continuous on $[0,1].$ Then there exists a number $a$ such that the integral $$\int_0^1\frac 1 {|f(x)-a|}\, dx $$ diverges. How to prove that statement?","Let a real-valued function $f$ be continuous on $[0,1].$ Then there exists a number $a$ such that the integral $$\int_0^1\frac 1 {|f(x)-a|}\, dx $$ diverges. How to prove that statement?",,"['calculus', 'real-analysis', 'improper-integrals']"
66,A classical problem about limit of continuous function at infinity and its connection with Baire Category Theorem,A classical problem about limit of continuous function at infinity and its connection with Baire Category Theorem,,"When I google ""baire category theorem"", I get a link to Ben Green's website. And at the end of the paper, he mentioned such a classic problem: Suppose that $f:\mathbb{R}^+\to\mathbb{R}^+$ is a continuous function with following property: for all $x\in\mathbb{R}^+$, the sequence $f(x), f(2x), f(3x),\dots$ tends to $0$. Prove that $\lim_{t\to\infty}f(t)=0$. I find the problem 1.17 on P.27 of the book "" Selected problems of real analysis "", and on P.169 gives the answer by prove the following lemma: If $G$ is a unbounded open set of $\mathbb{R}^+$, then for any closed   interval $[p,q]\  (0<p<q)$, there exist a $x_0\in [p,q]$ such that   $G$ contains infinitely many points of the form $nx_0\ (n\in\mathbb{N})$. But, on the above book, it also says: If $\lim_{n\to\infty}f(nx)$ exists only for points $x$ in a nonempty   closed set without isolated points , then $\lim_{x\to\infty}f(x)$  also   exist. I didn't find a proof for this result, I want to know whether for any nonempty closed set with no isolated points, the above lemma is true? and could someone tell me why Ben Green mentioned this problem on his paper (see the hint of his paper)?","When I google ""baire category theorem"", I get a link to Ben Green's website. And at the end of the paper, he mentioned such a classic problem: Suppose that $f:\mathbb{R}^+\to\mathbb{R}^+$ is a continuous function with following property: for all $x\in\mathbb{R}^+$, the sequence $f(x), f(2x), f(3x),\dots$ tends to $0$. Prove that $\lim_{t\to\infty}f(t)=0$. I find the problem 1.17 on P.27 of the book "" Selected problems of real analysis "", and on P.169 gives the answer by prove the following lemma: If $G$ is a unbounded open set of $\mathbb{R}^+$, then for any closed   interval $[p,q]\  (0<p<q)$, there exist a $x_0\in [p,q]$ such that   $G$ contains infinitely many points of the form $nx_0\ (n\in\mathbb{N})$. But, on the above book, it also says: If $\lim_{n\to\infty}f(nx)$ exists only for points $x$ in a nonempty   closed set without isolated points , then $\lim_{x\to\infty}f(x)$  also   exist. I didn't find a proof for this result, I want to know whether for any nonempty closed set with no isolated points, the above lemma is true? and could someone tell me why Ben Green mentioned this problem on his paper (see the hint of his paper)?",,"['real-analysis', 'baire-category']"
67,"Proving that $\int_0^1 \frac{\log \left(\frac{1}{t}\right) \log (t+2)}{t+1} \, dt=\frac{13}{24} \zeta (3)$",Proving that,"\int_0^1 \frac{\log \left(\frac{1}{t}\right) \log (t+2)}{t+1} \, dt=\frac{13}{24} \zeta (3)","Are we aware of an elementary way of proving that? $$\int_0^1 \frac{\log \left(\frac{1}{t}\right) \log (t+2)}{t+1} \, dt=\frac{13}{24} \zeta (3)$$ Of course, with the help of Mathematica it can be done, but I wonder if there exists an elementary, simple, easy way of finishing it. $$\int \frac{\log \left(\frac{1}{t}\right) \log (t+2)}{t+1} \, dt=$$ $$=\text{Li}_3\left(\frac{t+2}{t}\right)-\text{Li}_3\left(-\frac{t+2}{t}\right)+\text{Li}_3(-t)+\text{Li}_3(t+2)-\left(\text{Li}_2\left(\frac{t+2}{t}\right)-\text{Li}_2\left(-\frac{t+2}{t}\right)\right) \log \left(-\frac{t+2}{t}\right)-\text{Li}_2(-t-1) \left(\log \left(\frac{1}{t}\right)+\log (t)\right)-\text{Li}_2(-t) \left(\log (t+2)-\log \left(-\frac{t+2}{t}\right)\right)-\text{Li}_2(t+2) \left(\log \left(-\frac{t+2}{t}\right)+\log (t)\right)-\frac{1}{2} \left(-\log \left(\frac{2 (t+1)}{t}\right)+\log \left(-\frac{2}{t}\right)+\log (t+1)\right) \log ^2\left(-\frac{t+2}{t}\right)-(\log (-t-1)-\log (t+1)) \log (t+2) \log \left(-\frac{t+2}{t}\right)-\log (t) \log (t+1) \log (t+2)-\frac{1}{2} (\log (t+1)-\log (-t-1)) \log (t+2) (\log (t+2)-2 \log (t))$$ that is obtain by Mathematica. EDIT: I think once we rewrite all as $$ \frac{1}{2}\int_0^1 \left(\frac{\log ^2\left(\frac{t+2}{t}\right)}{t+1}-\frac{\log ^2(t+2)}{t+1}-\frac{\log ^2\left(\frac{1}{t}\right)}{t+1}\right) \, dt$$ and employ the proper variable change, we're almost done, the idea that just crossed my mind. The hardest part is probably the integral  $$\int_0^1 \frac{\log ^2\left(\frac{t+2}{t}\right)}{t+1} \ dt$$ where  by letting $t/(t+2)\mapsto t$, we obtain the far nicer form $$2\int_0^{1/3} \frac{\log ^2(t)}{1-t^2} \, dt$$ $$=\int_0^{1/3} \frac{\log ^2(t)}{ 1-t} \ dt+\int_0^{1/3}\frac{\log ^2(t)}{ 1+t} \, dt$$ where then using geometric series and swapping the integration and summation, we obtain that$$\int_0^1 \frac{\log ^2\left(\frac{t+2}{t}\right)}{t+1} \ dt=$$ $$2\text{Li}_3\left(\frac{1}{3}\right)-2\text{Li}_3\left(-\frac{1}{3}\right)+ \log (3) \left(4 \text{Li}_2\left(\frac{1}{3}\right)-\text{Li}_2\left(\frac{1}{9}\right)+\log (2) \log (3)\right) \tag1$$ Using geometric series again, we easily get that $$\int_0^1 \frac{\log ^2\left(\frac{1}{t}\right)}{t+1} \, dt=\frac{3}{2} \zeta (3) \tag2$$ The last integral can be reduced by variable change to a well-known integral treated by Leonard Lewin in his book about polylogarithms, that is $$\int_0^1 \frac{\log ^2(t+2)}{t+1} \, dt=\int_1^2 \frac{\log ^2(t+1)}{t} \, dt$$ So, we can use the fact that  $$\int_0^t \frac{\log ^2(1+t)}{t} \, dt$$ $$=\log(t)\log^2(1+t)-2/3\log^3(1+t)-2\log(1+t)\operatorname{Li}_2\left(\frac{1}{1+t}\right)-2\operatorname{Li}_3\left(\frac{1}{1+t}\right)+2\zeta(3)$$ So, using the foregoing result, we immediately get that  $$\int_1^2 \frac{\log ^2(t+1)}{t} \, dt$$ $$=-2 \text{Li}_3\left(\frac{1}{3}\right)-2 \text{Li}_2\left(\frac{1}{3}\right) \log (3)+\frac{7 \zeta (3)}{4}-\frac{1}{3} 2 \log ^3(3)+\log (2) \log ^2(3) \tag3$$ Combining $(1) (2) (3)$, we obtain that $$\int_0^1 \frac{\log \left(\frac{1}{t}\right) \log (t+2)}{t+1} \, dt$$ $$=2 \text{Li}_3\left(\frac{1}{3}\right)-\text{Li}_3\left(-\frac{1}{3}\right)-\frac{1}{2} \left(\text{Li}_2\left(\frac{1}{9}\right)-6 \text{Li}_2\left(\frac{1}{3}\right)\right) \log (3)-\frac{13 \zeta (3)}{8}+\frac{\log ^3(3)}{3}$$ Using $(20)$ from here http://mathworld.wolfram.com/Dilogarithm.html and then the identity and $(1)$ from this question/answer Proving $\text{Li}_3\left(-\frac{1}{3}\right)-2 \text{Li}_3\left(\frac{1}{3}\right)= -\frac{\log^33}{6}+\frac{\pi^2}{6}\log 3-\frac{13\zeta(3)}{6}$? , we get that $$\int_0^1 \frac{\log \left(\frac{1}{t}\right) \log (t+2)}{t+1} \, dt=\frac{13}{24} \zeta (3).$$ A SPECIAL ADDENDUM: The present integral can be viewed as the key core of calculating a pretty tough integral that was posted on MSE some time go,  $$\int_0^1 \frac{\text{Li}_2 \left(-\frac{1}{1-z}\right)-\text{Li}_2 \left(-\frac{1}{1+z}\right)}{z}dz$$ at this link Evaluating $\int_0^1 \frac{\text{Li}_2 \left(-\frac{1}{1-z}\right)-\text{Li}_2 \left(-\frac{1}{1+z}\right)}{z}dz$ . Simple integration by parts combined with the use of the geometric series shows that $$\int_0^1 \frac{\text{Li}_2 \left(-\frac{1}{1-z}\right)-\text{Li}_2 \left(-\frac{1}{1+z}\right)}{z}dz$$  $$=\int_0^1 \frac{\log (z+2) \log (z)}{z+1}dz+\underbrace{\int_0^1\frac{\log (2-z) \log (z)}{1-z} \ dz}_{\large \sum _{n=1}^{\infty } \frac{(-1)^n H_n}{n^2}=-5/8  \zeta (3)}-\underbrace{\int_0^1\frac{\log (1-z) \log (z)}{1-z}dz}_{\large\sum _{n=1}^{\infty } \frac{H_n}{(n+1)^2}=\zeta (3)}-\underbrace{\int_0^1\frac{\log (z+1) \log (z)}{z+1} \, dz}_{\large \sum _{n=1}^{\infty } \frac{(-1)^n H_n}{(n+1)^2}=-1/8\zeta (3)}$$ where since the integral unknown is calculated in this post, we conclude that $$\int_0^1 \frac{\text{Li}_2 \left(-\frac{1}{1-z}\right)-\text{Li}_2 \left(-\frac{1}{1+z}\right)}{z}dz$$  $$=-\frac{49}{24}\zeta(3).$$ A SPECIAL ADDENDUM $2$: By a careful approach of the previous results, we also obtain the value of the very beautiful series $$\sum _{k=1}^{\infty } \sum _{n=1}^{\infty } \frac{(-1)^k }{k2^k n 2^n (k+n)}=\frac{1}{12} \left(3 \text{Li}_3\left(\frac{1}{4}\right)+6\log (2) \text{Li}_2\left(\frac{1}{4}\right) +4 \log ^3(2)-4 \zeta (3)\right)$$ Q.E.D.","Are we aware of an elementary way of proving that? $$\int_0^1 \frac{\log \left(\frac{1}{t}\right) \log (t+2)}{t+1} \, dt=\frac{13}{24} \zeta (3)$$ Of course, with the help of Mathematica it can be done, but I wonder if there exists an elementary, simple, easy way of finishing it. $$\int \frac{\log \left(\frac{1}{t}\right) \log (t+2)}{t+1} \, dt=$$ $$=\text{Li}_3\left(\frac{t+2}{t}\right)-\text{Li}_3\left(-\frac{t+2}{t}\right)+\text{Li}_3(-t)+\text{Li}_3(t+2)-\left(\text{Li}_2\left(\frac{t+2}{t}\right)-\text{Li}_2\left(-\frac{t+2}{t}\right)\right) \log \left(-\frac{t+2}{t}\right)-\text{Li}_2(-t-1) \left(\log \left(\frac{1}{t}\right)+\log (t)\right)-\text{Li}_2(-t) \left(\log (t+2)-\log \left(-\frac{t+2}{t}\right)\right)-\text{Li}_2(t+2) \left(\log \left(-\frac{t+2}{t}\right)+\log (t)\right)-\frac{1}{2} \left(-\log \left(\frac{2 (t+1)}{t}\right)+\log \left(-\frac{2}{t}\right)+\log (t+1)\right) \log ^2\left(-\frac{t+2}{t}\right)-(\log (-t-1)-\log (t+1)) \log (t+2) \log \left(-\frac{t+2}{t}\right)-\log (t) \log (t+1) \log (t+2)-\frac{1}{2} (\log (t+1)-\log (-t-1)) \log (t+2) (\log (t+2)-2 \log (t))$$ that is obtain by Mathematica. EDIT: I think once we rewrite all as $$ \frac{1}{2}\int_0^1 \left(\frac{\log ^2\left(\frac{t+2}{t}\right)}{t+1}-\frac{\log ^2(t+2)}{t+1}-\frac{\log ^2\left(\frac{1}{t}\right)}{t+1}\right) \, dt$$ and employ the proper variable change, we're almost done, the idea that just crossed my mind. The hardest part is probably the integral  $$\int_0^1 \frac{\log ^2\left(\frac{t+2}{t}\right)}{t+1} \ dt$$ where  by letting $t/(t+2)\mapsto t$, we obtain the far nicer form $$2\int_0^{1/3} \frac{\log ^2(t)}{1-t^2} \, dt$$ $$=\int_0^{1/3} \frac{\log ^2(t)}{ 1-t} \ dt+\int_0^{1/3}\frac{\log ^2(t)}{ 1+t} \, dt$$ where then using geometric series and swapping the integration and summation, we obtain that$$\int_0^1 \frac{\log ^2\left(\frac{t+2}{t}\right)}{t+1} \ dt=$$ $$2\text{Li}_3\left(\frac{1}{3}\right)-2\text{Li}_3\left(-\frac{1}{3}\right)+ \log (3) \left(4 \text{Li}_2\left(\frac{1}{3}\right)-\text{Li}_2\left(\frac{1}{9}\right)+\log (2) \log (3)\right) \tag1$$ Using geometric series again, we easily get that $$\int_0^1 \frac{\log ^2\left(\frac{1}{t}\right)}{t+1} \, dt=\frac{3}{2} \zeta (3) \tag2$$ The last integral can be reduced by variable change to a well-known integral treated by Leonard Lewin in his book about polylogarithms, that is $$\int_0^1 \frac{\log ^2(t+2)}{t+1} \, dt=\int_1^2 \frac{\log ^2(t+1)}{t} \, dt$$ So, we can use the fact that  $$\int_0^t \frac{\log ^2(1+t)}{t} \, dt$$ $$=\log(t)\log^2(1+t)-2/3\log^3(1+t)-2\log(1+t)\operatorname{Li}_2\left(\frac{1}{1+t}\right)-2\operatorname{Li}_3\left(\frac{1}{1+t}\right)+2\zeta(3)$$ So, using the foregoing result, we immediately get that  $$\int_1^2 \frac{\log ^2(t+1)}{t} \, dt$$ $$=-2 \text{Li}_3\left(\frac{1}{3}\right)-2 \text{Li}_2\left(\frac{1}{3}\right) \log (3)+\frac{7 \zeta (3)}{4}-\frac{1}{3} 2 \log ^3(3)+\log (2) \log ^2(3) \tag3$$ Combining $(1) (2) (3)$, we obtain that $$\int_0^1 \frac{\log \left(\frac{1}{t}\right) \log (t+2)}{t+1} \, dt$$ $$=2 \text{Li}_3\left(\frac{1}{3}\right)-\text{Li}_3\left(-\frac{1}{3}\right)-\frac{1}{2} \left(\text{Li}_2\left(\frac{1}{9}\right)-6 \text{Li}_2\left(\frac{1}{3}\right)\right) \log (3)-\frac{13 \zeta (3)}{8}+\frac{\log ^3(3)}{3}$$ Using $(20)$ from here http://mathworld.wolfram.com/Dilogarithm.html and then the identity and $(1)$ from this question/answer Proving $\text{Li}_3\left(-\frac{1}{3}\right)-2 \text{Li}_3\left(\frac{1}{3}\right)= -\frac{\log^33}{6}+\frac{\pi^2}{6}\log 3-\frac{13\zeta(3)}{6}$? , we get that $$\int_0^1 \frac{\log \left(\frac{1}{t}\right) \log (t+2)}{t+1} \, dt=\frac{13}{24} \zeta (3).$$ A SPECIAL ADDENDUM: The present integral can be viewed as the key core of calculating a pretty tough integral that was posted on MSE some time go,  $$\int_0^1 \frac{\text{Li}_2 \left(-\frac{1}{1-z}\right)-\text{Li}_2 \left(-\frac{1}{1+z}\right)}{z}dz$$ at this link Evaluating $\int_0^1 \frac{\text{Li}_2 \left(-\frac{1}{1-z}\right)-\text{Li}_2 \left(-\frac{1}{1+z}\right)}{z}dz$ . Simple integration by parts combined with the use of the geometric series shows that $$\int_0^1 \frac{\text{Li}_2 \left(-\frac{1}{1-z}\right)-\text{Li}_2 \left(-\frac{1}{1+z}\right)}{z}dz$$  $$=\int_0^1 \frac{\log (z+2) \log (z)}{z+1}dz+\underbrace{\int_0^1\frac{\log (2-z) \log (z)}{1-z} \ dz}_{\large \sum _{n=1}^{\infty } \frac{(-1)^n H_n}{n^2}=-5/8  \zeta (3)}-\underbrace{\int_0^1\frac{\log (1-z) \log (z)}{1-z}dz}_{\large\sum _{n=1}^{\infty } \frac{H_n}{(n+1)^2}=\zeta (3)}-\underbrace{\int_0^1\frac{\log (z+1) \log (z)}{z+1} \, dz}_{\large \sum _{n=1}^{\infty } \frac{(-1)^n H_n}{(n+1)^2}=-1/8\zeta (3)}$$ where since the integral unknown is calculated in this post, we conclude that $$\int_0^1 \frac{\text{Li}_2 \left(-\frac{1}{1-z}\right)-\text{Li}_2 \left(-\frac{1}{1+z}\right)}{z}dz$$  $$=-\frac{49}{24}\zeta(3).$$ A SPECIAL ADDENDUM $2$: By a careful approach of the previous results, we also obtain the value of the very beautiful series $$\sum _{k=1}^{\infty } \sum _{n=1}^{\infty } \frac{(-1)^k }{k2^k n 2^n (k+n)}=\frac{1}{12} \left(3 \text{Li}_3\left(\frac{1}{4}\right)+6\log (2) \text{Li}_2\left(\frac{1}{4}\right) +4 \log ^3(2)-4 \zeta (3)\right)$$ Q.E.D.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
68,Properties of $\liminf$ and $\limsup$ of sum of sequences: $\limsup s_n + \liminf t_n \leq \limsup (s_n + t_n) \leq \limsup s_n + \limsup t_n$,Properties of  and  of sum of sequences:,\liminf \limsup \limsup s_n + \liminf t_n \leq \limsup (s_n + t_n) \leq \limsup s_n + \limsup t_n,"Let $\{s_n\}$ and $\{t_n\}$ be sequences. I've noticed this inequality in a few analysis textbooks that I have come across, so I've started to think this can't be a typo: $\limsup\limits_{n \rightarrow \infty} s_n + \liminf\limits_{n \rightarrow \infty} t_n \leq \limsup\limits_{n \rightarrow \infty} (s_n + t_n) \leq \limsup\limits_{n \rightarrow \infty} s_n + \limsup\limits_{n \rightarrow \infty} t_n$. I understand the right two-thirds of the equation, and can prove it.  I'm stuck on the far-left third.  I don't understand how that is true.","Let $\{s_n\}$ and $\{t_n\}$ be sequences. I've noticed this inequality in a few analysis textbooks that I have come across, so I've started to think this can't be a typo: $\limsup\limits_{n \rightarrow \infty} s_n + \liminf\limits_{n \rightarrow \infty} t_n \leq \limsup\limits_{n \rightarrow \infty} (s_n + t_n) \leq \limsup\limits_{n \rightarrow \infty} s_n + \limsup\limits_{n \rightarrow \infty} t_n$. I understand the right two-thirds of the equation, and can prove it.  I'm stuck on the far-left third.  I don't understand how that is true.",,"['real-analysis', 'sequences-and-series', 'inequality', 'limsup-and-liminf']"
69,"If $\sum_{n=1}^{\infty}{a_{[f(n)]}}$ converges, then $\sum_{n=1}^{\infty}{\frac{a_n}{f(n)}}$ converges.","If  converges, then  converges.",\sum_{n=1}^{\infty}{a_{[f(n)]}} \sum_{n=1}^{\infty}{\frac{a_n}{f(n)}},"I am trying to determine whether the following statement is true or false: Let $a_n$ be a decreasing positive sequence such that $\displaystyle \sum \limits _{n=1}^\infty a_n$ diverges, and let let $f$ be a function such that $\lim\limits_{n\to\infty} f(n) = \infty$ . If $\displaystyle \sum \limits _{n=1}^\infty a_{[f(n)]}$ converges, then $\displaystyle \sum \limits _{n=1}^\infty \frac{a_n}{f(n)}$ converges. I tried to use the comparison test but could not find any candidates. It does seems the second sum is smaller than the first, but finding an upper bound didn't work either. This question is from our calculus course booklet, expert difficulty level. Any hints will be appreciated.","I am trying to determine whether the following statement is true or false: Let be a decreasing positive sequence such that diverges, and let let be a function such that . If converges, then converges. I tried to use the comparison test but could not find any candidates. It does seems the second sum is smaller than the first, but finding an upper bound didn't work either. This question is from our calculus course booklet, expert difficulty level. Any hints will be appreciated.",a_n \displaystyle \sum \limits _{n=1}^\infty a_n f \lim\limits_{n\to\infty} f(n) = \infty \displaystyle \sum \limits _{n=1}^\infty a_{[f(n)]} \displaystyle \sum \limits _{n=1}^\infty \frac{a_n}{f(n)},"['real-analysis', 'calculus', 'sequences-and-series']"
70,Is $\ln(\ln(n))$ irrational for any integer $n>1$?,Is  irrational for any integer ?,\ln(\ln(n)) n>1,"Is there $n\in \mathbb{N}$ such that $\ln(\ln(n)) \in \mathbb{Q}$? If such $n$ exists, we will get $$\ln(\ln(n))  = \frac{p}{q}, \quad p, q \in \mathbb{Z}.$$  Hence we will get $n = e^{e^{p/q}},$ where the question about the nature of  $e^{e^{p/q}}$ haven't been answered yet. Is there any other direction ?  Thank you in advance.","Is there $n\in \mathbb{N}$ such that $\ln(\ln(n)) \in \mathbb{Q}$? If such $n$ exists, we will get $$\ln(\ln(n))  = \frac{p}{q}, \quad p, q \in \mathbb{Z}.$$  Hence we will get $n = e^{e^{p/q}},$ where the question about the nature of  $e^{e^{p/q}}$ haven't been answered yet. Is there any other direction ?  Thank you in advance.",,"['real-analysis', 'elementary-number-theory', 'algebraic-number-theory', 'irrational-numbers', 'rationality-testing']"
71,"How would you evaluate $\liminf\limits_{n\to\infty} \ n \,|\mathopen{}\sin n|$",How would you evaluate,"\liminf\limits_{n\to\infty} \ n \,|\mathopen{}\sin n|","How would you evaluate the limit inferior of the sequence $n\,|\mathopen{}\sin n|$? That is, $$\liminf\limits_{n\to\infty} \ n \,|\mathopen{}\sin n|$$ Edit. Let $\mu$ be the irrationality measure of $\pi$. Since $\mu$ is not known, I will split the question: Assuming $\mu > 2$, what is $\liminf\limits_{n\to\infty} \ n\,|\mathopen{}\sin n|$? Assuming $\mu = 2$, what is $\liminf\limits_{n\to\infty} \ n\,|\mathopen{}\sin n|$? I kind of like its graph...","How would you evaluate the limit inferior of the sequence $n\,|\mathopen{}\sin n|$? That is, $$\liminf\limits_{n\to\infty} \ n \,|\mathopen{}\sin n|$$ Edit. Let $\mu$ be the irrationality measure of $\pi$. Since $\mu$ is not known, I will split the question: Assuming $\mu > 2$, what is $\liminf\limits_{n\to\infty} \ n\,|\mathopen{}\sin n|$? Assuming $\mu = 2$, what is $\liminf\limits_{n\to\infty} \ n\,|\mathopen{}\sin n|$? I kind of like its graph...",,"['real-analysis', 'sequences-and-series', 'limsup-and-liminf']"
72,A function $f$ such that $f(x)$ increases from $0$ to $1$ when $x$ increases from $0$ to infinity?,A function  such that  increases from  to  when  increases from  to infinity?,f f(x) 0 1 x 0,"I am looking for a function f(x) with a value range of [0,1]. f(x) should increase from 0 to 1 while its parameter x increases from 0 to +infinity. f(x) increases very fast when x is small, and then very slow and eventually approach 1 when x is infinity. Here is a figure. The green curve is what I am looking for: Thanks. It would be great if I can adjust the slope of the increase. Although this is not a compulsory requirement.","I am looking for a function f(x) with a value range of [0,1]. f(x) should increase from 0 to 1 while its parameter x increases from 0 to +infinity. f(x) increases very fast when x is small, and then very slow and eventually approach 1 when x is infinity. Here is a figure. The green curve is what I am looking for: Thanks. It would be great if I can adjust the slope of the increase. Although this is not a compulsory requirement.",,"['real-analysis', 'algebra-precalculus', 'functions']"
73,How can we come up with the definition of natural logarithm?,How can we come up with the definition of natural logarithm?,,"I learned calculus for 2 years, but still don't understand the definition of $\ln(x)$ $$\ln(x) = \int_1^x \frac{\mathrm d t}{t}$$ I can't make sense of this definition. How can people find it? Do you have any intuition?","I learned calculus for 2 years, but still don't understand the definition of $\ln(x)$ $$\ln(x) = \int_1^x \frac{\mathrm d t}{t}$$ I can't make sense of this definition. How can people find it? Do you have any intuition?",,"['real-analysis', 'calculus', 'logarithms', 'definition', 'intuition']"
74,Does interior of closure of open set equal the set?,Does interior of closure of open set equal the set?,,"Would you help me to solve this question. Is it true that if A is open set then $A=\operatorname{int}(Cl(A))$ where Cl(A) denote the closure of A. I already prove that $A\subseteq\operatorname{int}(Cl(A)) $ only using definition of closure and interior, but have no idea about proving $\operatorname{int}(Cl(A))\subseteq A$ or give a counter example.","Would you help me to solve this question. Is it true that if A is open set then $A=\operatorname{int}(Cl(A))$ where Cl(A) denote the closure of A. I already prove that $A\subseteq\operatorname{int}(Cl(A)) $ only using definition of closure and interior, but have no idea about proving $\operatorname{int}(Cl(A))\subseteq A$ or give a counter example.",,"['real-analysis', 'general-topology']"
75,Composition of two Riemann integrable functions,Composition of two Riemann integrable functions,,"Suppose $f,g$ are two Riemann  Integrable functions .Is it true that $f\circ g$ is also Riemann  Integrable? Trying this for a long time but not getting the answer","Suppose $f,g$ are two Riemann  Integrable functions .Is it true that $f\circ g$ is also Riemann  Integrable? Trying this for a long time but not getting the answer",,['real-analysis']
76,Are the rationals on a unit circle dense?,Are the rationals on a unit circle dense?,,"It seems evident from infinitely many primitive pythagorean triples $(a,b,c)$ that there are infinitely many rational points $\left(\frac{a}{c}, \frac{b}{c}\right)$ on the unit circle. But how would one go about, and show that they are dense, in the sense that for two rational points $x$ and $y$ of angles $α$ and $β$ on the unit circle, if $α<β$ there is a third rational point $z$ of angle $γ$ on the unit circle, such that $α<γ$ and $γ<β$. Is it to expect that this conjecture holds and that it isn't an unsolved number theory problem?","It seems evident from infinitely many primitive pythagorean triples $(a,b,c)$ that there are infinitely many rational points $\left(\frac{a}{c}, \frac{b}{c}\right)$ on the unit circle. But how would one go about, and show that they are dense, in the sense that for two rational points $x$ and $y$ of angles $α$ and $β$ on the unit circle, if $α<β$ there is a third rational point $z$ of angle $γ$ on the unit circle, such that $α<γ$ and $γ<β$. Is it to expect that this conjecture holds and that it isn't an unsolved number theory problem?",,['real-analysis']
77,Space of Complex Measures is Banach (proof?),Space of Complex Measures is Banach (proof?),,How can we prove that the space of Complex Measures is Complete? with the norm of Total Variation. I have stuck on the last part of the proof where I have to prove that the limit function of a Cauchy sequence of measures has the properties of Complex measures. We are using the norm of total variation :$$\lVert \mu\rVert = \lvert \mu \rvert(X)$$,How can we prove that the space of Complex Measures is Complete? with the norm of Total Variation. I have stuck on the last part of the proof where I have to prove that the limit function of a Cauchy sequence of measures has the properties of Complex measures. We are using the norm of total variation :$$\lVert \mu\rVert = \lvert \mu \rvert(X)$$,,"['real-analysis', 'functional-analysis', 'measure-theory', 'banach-spaces']"
78,Example of a function such that $\varphi\left(\frac{x+y}{2}\right)\leq \frac{\varphi(x)+\varphi(y)}{2}$ but $\varphi$ is not convex,Example of a function such that  but  is not convex,\varphi\left(\frac{x+y}{2}\right)\leq \frac{\varphi(x)+\varphi(y)}{2} \varphi,"Rudin's Real and Complex Analysis Chapter 3 Exercise 4 is: Assume that $\varphi$ is a continuous real function on $(a,b)$ s.t. $$\varphi\left(\frac{x+y}{2}\right)\leq \frac{\varphi(x)+\varphi(y)}{2}$$ for all $x,y\in(a,b)$. Prove that $\varphi$ is convex. The conclusion does not follow if continuity is omitted from the hypotheses. My question is, is there some way to explicitly construct a counterexample such that  $\varphi\left(\frac{x+y}{2}\right)\leq \frac{\varphi(x)+\varphi(y)}{2}$ for all $x,y\in(a,b)$, but $\varphi$ is not convex?","Rudin's Real and Complex Analysis Chapter 3 Exercise 4 is: Assume that $\varphi$ is a continuous real function on $(a,b)$ s.t. $$\varphi\left(\frac{x+y}{2}\right)\leq \frac{\varphi(x)+\varphi(y)}{2}$$ for all $x,y\in(a,b)$. Prove that $\varphi$ is convex. The conclusion does not follow if continuity is omitted from the hypotheses. My question is, is there some way to explicitly construct a counterexample such that  $\varphi\left(\frac{x+y}{2}\right)\leq \frac{\varphi(x)+\varphi(y)}{2}$ for all $x,y\in(a,b)$, but $\varphi$ is not convex?",,"['real-analysis', 'convex-analysis', 'examples-counterexamples']"
79,"Closed-form of $\sum_{n=1}^\infty\frac{(-1)^{n+1}}{n}\Psi_3(n+1)=-\int_0^1\frac{\ln(1+x)\ln^3 x}{1-x}\,dx$",Closed-form of,"\sum_{n=1}^\infty\frac{(-1)^{n+1}}{n}\Psi_3(n+1)=-\int_0^1\frac{\ln(1+x)\ln^3 x}{1-x}\,dx","Does the following series or integral have a closed-form \begin{equation} \sum_{n=1}^\infty\frac{(-1)^{n+1}}{n}\Psi_3(n+1)=-\int_0^1\frac{\ln(1+x)\ln^3 x}{1-x}\,dx \end{equation} where $\Psi_3(x)$ is the polygamma function of order $3$. Here is my attempt. Using equation (11) from Mathworld Wolfram : \begin{equation} \Psi_n(z)=(-1)^{n+1} n!\left(\zeta(n+1)-H_{z-1}^{(n+1)}\right) \end{equation} I got \begin{equation} \Psi_3(n+1)=6\left(\zeta(4)-H_{n}^{(4)}\right) \end{equation} then \begin{align} \sum_{n=1}^\infty\frac{(-1)^{n+1}}{n}\Psi_3(n+1)&=6\sum_{n=1}^\infty\frac{(-1)^{n+1}}{n}\left(\zeta(4)-H_{n}^{(4)}\right)\\ &=6\zeta(4)\sum_{n=1}^\infty\frac{(-1)^{n+1}}{n}-6\sum_{n=1}^\infty\frac{(-1)^{n+1}H_{n}^{(4)}}{n}\\ &=\frac{\pi^4}{15}\ln2-6\sum_{n=1}^\infty\frac{(-1)^{n+1}H_{n}^{(4)}}{n}\\ \end{align} From the answers of this OP , the integral representation of the latter Euler sum is \begin{align} \sum_{n=1}^\infty\frac{(-1)^{n+1}H_{n}^{(4)}}{n}&=\int_0^1\int_0^1\int_0^1\int_0^1\int_0^1\frac{dx_1\,dx_2\,dx_3\,dx_4\,dx_5}{(1-x_1)(1+x_1x_2x_3x_4x_5)} \end{align} or another simpler form \begin{align} \sum_{n=1}^\infty\frac{(-1)^{n+1}H_{n}^{(4)}}{n}&=-\int_0^1\frac{\text{Li}_4(-x)}{x(1+x)}dx\\ &=-\int_0^1\frac{\text{Li}_4(-x)}{x}dx+\int_0^1\frac{\text{Li}_4(-x)}{1+x}dx\\ &=\text{Li}_5(-1)-\int_0^{-1}\frac{\text{Li}_4(x)}{1-x}dx\\ \end{align} I don't know how to continue it, I am stuck. Could anyone here please help me to find the closed-form of the series preferably with elementary ways? Any help would be greatly appreciated. Thank you. Edit : Using the integral representation of polygamma function \begin{equation} \Psi_m(z)=(-1)^m\int_0^1\frac{x^{z-1}}{1-x}\ln^m x\,dx \end{equation} then we have \begin{align} \sum_{n=1}^\infty\frac{(-1)^{n+1}}{n}\Psi_3(n+1)&=-\sum_{n=1}^\infty\frac{(-1)^{n+1}}{n}\int_0^1\frac{x^{n}}{1-x}\ln^3 x\,dx\\ &=-\int_0^1\sum_{n=1}^\infty\frac{(-1)^{n+1}x^{n}}{n}\cdot\frac{\ln^3 x}{1-x}\,dx\\ &=-\int_0^1\frac{\ln(1+x)\ln^3 x}{1-x}\,dx\\ \end{align} I am looking for an approach to evaluate the above integral without using residue method or double summation.","Does the following series or integral have a closed-form \begin{equation} \sum_{n=1}^\infty\frac{(-1)^{n+1}}{n}\Psi_3(n+1)=-\int_0^1\frac{\ln(1+x)\ln^3 x}{1-x}\,dx \end{equation} where $\Psi_3(x)$ is the polygamma function of order $3$. Here is my attempt. Using equation (11) from Mathworld Wolfram : \begin{equation} \Psi_n(z)=(-1)^{n+1} n!\left(\zeta(n+1)-H_{z-1}^{(n+1)}\right) \end{equation} I got \begin{equation} \Psi_3(n+1)=6\left(\zeta(4)-H_{n}^{(4)}\right) \end{equation} then \begin{align} \sum_{n=1}^\infty\frac{(-1)^{n+1}}{n}\Psi_3(n+1)&=6\sum_{n=1}^\infty\frac{(-1)^{n+1}}{n}\left(\zeta(4)-H_{n}^{(4)}\right)\\ &=6\zeta(4)\sum_{n=1}^\infty\frac{(-1)^{n+1}}{n}-6\sum_{n=1}^\infty\frac{(-1)^{n+1}H_{n}^{(4)}}{n}\\ &=\frac{\pi^4}{15}\ln2-6\sum_{n=1}^\infty\frac{(-1)^{n+1}H_{n}^{(4)}}{n}\\ \end{align} From the answers of this OP , the integral representation of the latter Euler sum is \begin{align} \sum_{n=1}^\infty\frac{(-1)^{n+1}H_{n}^{(4)}}{n}&=\int_0^1\int_0^1\int_0^1\int_0^1\int_0^1\frac{dx_1\,dx_2\,dx_3\,dx_4\,dx_5}{(1-x_1)(1+x_1x_2x_3x_4x_5)} \end{align} or another simpler form \begin{align} \sum_{n=1}^\infty\frac{(-1)^{n+1}H_{n}^{(4)}}{n}&=-\int_0^1\frac{\text{Li}_4(-x)}{x(1+x)}dx\\ &=-\int_0^1\frac{\text{Li}_4(-x)}{x}dx+\int_0^1\frac{\text{Li}_4(-x)}{1+x}dx\\ &=\text{Li}_5(-1)-\int_0^{-1}\frac{\text{Li}_4(x)}{1-x}dx\\ \end{align} I don't know how to continue it, I am stuck. Could anyone here please help me to find the closed-form of the series preferably with elementary ways? Any help would be greatly appreciated. Thank you. Edit : Using the integral representation of polygamma function \begin{equation} \Psi_m(z)=(-1)^m\int_0^1\frac{x^{z-1}}{1-x}\ln^m x\,dx \end{equation} then we have \begin{align} \sum_{n=1}^\infty\frac{(-1)^{n+1}}{n}\Psi_3(n+1)&=-\sum_{n=1}^\infty\frac{(-1)^{n+1}}{n}\int_0^1\frac{x^{n}}{1-x}\ln^3 x\,dx\\ &=-\int_0^1\sum_{n=1}^\infty\frac{(-1)^{n+1}x^{n}}{n}\cdot\frac{\ln^3 x}{1-x}\,dx\\ &=-\int_0^1\frac{\ln(1+x)\ln^3 x}{1-x}\,dx\\ \end{align} I am looking for an approach to evaluate the above integral without using residue method or double summation.",,"['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'harmonic-numbers']"
80,Methods to find $\lim\limits_{n\to\infty}\frac1n\sum\limits_{k=1}^nn^{1/k} $,Methods to find,\lim\limits_{n\to\infty}\frac1n\sum\limits_{k=1}^nn^{1/k} ,How would you suggest to find the following limit? $$\lim_{n\to\infty} \frac{1}{n} \sum_{k=1}^{n} n^{1/k} $$,How would you suggest to find the following limit?,\lim_{n\to\infty} \frac{1}{n} \sum_{k=1}^{n} n^{1/k} ,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
81,What's the mean of all real numbers?,What's the mean of all real numbers?,,"At first, I had thought the average must be zero, since for every positive number there's an equal magnitude negative number to cancel out the positive number's effect on the average, leaving only zero to set the average. But you can make a similar argument about any number, for example using an arbitrary choice of 9, for every number x units greater than 9, there's another number x units less than 9, which would make 9 the mean. But since I could have chosen any number here instead of 9 that would mean that any and every number is the average of all real numbers. So, what is the mean of all real numbers?","At first, I had thought the average must be zero, since for every positive number there's an equal magnitude negative number to cancel out the positive number's effect on the average, leaving only zero to set the average. But you can make a similar argument about any number, for example using an arbitrary choice of 9, for every number x units greater than 9, there's another number x units less than 9, which would make 9 the mean. But since I could have chosen any number here instead of 9 that would mean that any and every number is the average of all real numbers. So, what is the mean of all real numbers?",,"['real-analysis', 'average']"
82,A beautiful limit involving primes and composites,A beautiful limit involving primes and composites,,"I observed the following limit empirically. Let $p_n$ be the $n$-th prime and $c_n$ be the $n$-th composite number then, $$ \lim_{n \to \infty}\frac{1}{n}\sum_{i=1}^{n}\frac{p_n c_n}{p_n c_n + p_i c_i} = \frac{\pi}{4}. $$ I am looking for a proof.","I observed the following limit empirically. Let $p_n$ be the $n$-th prime and $c_n$ be the $n$-th composite number then, $$ \lim_{n \to \infty}\frac{1}{n}\sum_{i=1}^{n}\frac{p_n c_n}{p_n c_n + p_i c_i} = \frac{\pi}{4}. $$ I am looking for a proof.",,"['real-analysis', 'number-theory', 'limits', 'prime-numbers']"
83,Why does a Lipschitz function $f:\mathbb{R}^d\to\mathbb{R}^d$ map measure zero sets to measure zero sets?,Why does a Lipschitz function  map measure zero sets to measure zero sets?,f:\mathbb{R}^d\to\mathbb{R}^d,"Why does a Lipschitz function $f:\mathbb{R}^d\to\mathbb{R}^d$ map measure zero sets to measure zero sets? It is easy to prove this statement if the domain is bounded. Is there any way to extend the argument to unbounded domains? Can anyone give me some hint, or show me a simple proof sketch?","Why does a Lipschitz function $f:\mathbb{R}^d\to\mathbb{R}^d$ map measure zero sets to measure zero sets? It is easy to prove this statement if the domain is bounded. Is there any way to extend the argument to unbounded domains? Can anyone give me some hint, or show me a simple proof sketch?",,"['real-analysis', 'measure-theory', 'lipschitz-functions']"
84,Evaluating $\int_{0}^{1}\frac{1-x}{1+x}\frac{\mathrm dx}{\ln x}$,Evaluating,\int_{0}^{1}\frac{1-x}{1+x}\frac{\mathrm dx}{\ln x},Some time ago I came across  to the following integral: $$I=\int_{0}^{1}\frac{1-x}{1+x}\frac{\mathrm dx}{\ln x}$$ What are the hints on how to compute this integral?,Some time ago I came across  to the following integral: $$I=\int_{0}^{1}\frac{1-x}{1+x}\frac{\mathrm dx}{\ln x}$$ What are the hints on how to compute this integral?,,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
85,Why are every structures I study based on Real number?,Why are every structures I study based on Real number?,,"I've been studying basic concepts of inner product vector space, normed vector space and metric space. And all the inner products, norms and metrics are defined to be real-valued functions in my textbook. I doubted about it: why do I have to restrict the measurement standard to real number? Why not any ordered field? Why not… anything else? I do know that there can be a lot of different ways to define 'distance': I'm studying topology, too. I'm just wondering if there is any more general definition to the inner product space, normed space, or metric space(my definition is just the standard one being taught at undergraduate level: only real-valued ones as I've mentioned). For example, let $V$ be an $F$-vector space and $F_s$ be an ordered field embedded into $F$. Then I defined like this: $\langle v, v\rangle=0$ if and only if $v$ is the $0$ vector $\langle v, v\rangle>0$ (so the value is in $F_s$) $\langle av, u\rangle=a\langle v, u\rangle$ where $a\in F$ $\langle v+u, z\rangle=\langle v, z\rangle+\langle u, z\rangle$ $\langle v, u\rangle=\langle u, v\rangle$ We can define the normed vector space in this fashion as well. I think this is a more general definition: I couldn't find anything wrong about it. I mean, if we define $\|v\|=\langle v, v\rangle^{1/2}$ then indeed $(V, \|\,\|)$ becomes a normed vector space according to my definition, as it should in the ordinary definition (I think the Cauchy-Schwarz inequality which connects the two spaces also hold in this general definition: I've checked the proof in my textbook and it does not use any property of the real number). Is my definition not right, or not useful? If so, then why? I know that the real number is the only (up to isomorphism) ordered field with the least upper bound property and the property that every increasing bounded sequence converges. But is that enough to justify that every metric spaces use real number? I want to be more convinced, then. A little bit disorganized, but I just wanted to hear some other people's opinion about this. Thanks as always. P.S. I also know about the complex inner product vector space. The inner product there is complex-valued, so I think it was the start of my questioning.","I've been studying basic concepts of inner product vector space, normed vector space and metric space. And all the inner products, norms and metrics are defined to be real-valued functions in my textbook. I doubted about it: why do I have to restrict the measurement standard to real number? Why not any ordered field? Why not… anything else? I do know that there can be a lot of different ways to define 'distance': I'm studying topology, too. I'm just wondering if there is any more general definition to the inner product space, normed space, or metric space(my definition is just the standard one being taught at undergraduate level: only real-valued ones as I've mentioned). For example, let $V$ be an $F$-vector space and $F_s$ be an ordered field embedded into $F$. Then I defined like this: $\langle v, v\rangle=0$ if and only if $v$ is the $0$ vector $\langle v, v\rangle>0$ (so the value is in $F_s$) $\langle av, u\rangle=a\langle v, u\rangle$ where $a\in F$ $\langle v+u, z\rangle=\langle v, z\rangle+\langle u, z\rangle$ $\langle v, u\rangle=\langle u, v\rangle$ We can define the normed vector space in this fashion as well. I think this is a more general definition: I couldn't find anything wrong about it. I mean, if we define $\|v\|=\langle v, v\rangle^{1/2}$ then indeed $(V, \|\,\|)$ becomes a normed vector space according to my definition, as it should in the ordinary definition (I think the Cauchy-Schwarz inequality which connects the two spaces also hold in this general definition: I've checked the proof in my textbook and it does not use any property of the real number). Is my definition not right, or not useful? If so, then why? I know that the real number is the only (up to isomorphism) ordered field with the least upper bound property and the property that every increasing bounded sequence converges. But is that enough to justify that every metric spaces use real number? I want to be more convinced, then. A little bit disorganized, but I just wanted to hear some other people's opinion about this. Thanks as always. P.S. I also know about the complex inner product vector space. The inner product there is complex-valued, so I think it was the start of my questioning.",,"['real-analysis', 'metric-spaces', 'normed-spaces', 'inner-products']"
86,Are the iterates of the cosine linearly independent?,Are the iterates of the cosine linearly independent?,,"Consider the cosine function $f = \cos : \Bbb R \to \Bbb R$. Is it true that the set of iterates $$\left\{f_n := \cos \circ \dotsb \circ \cos,  \; n \text{  times }   \mid  n \geq 1\right\}$$ is linearly independent over $\Bbb R$ ? That is, I am wondering if,  for any $r \geq 1$ and any real numbers $a_k$, we have : $$\sum_{k=1}^r a_k f_k = 0 : \Bbb R \to \Bbb R \implies a_k=0 \;\forall k.$$ I know that this true if we consider the powers of $\cos( \cdot )$, but I don't know how to deal with compositions. What I tried is to take derivative, or induction on the minimal length of linear dependence relation.","Consider the cosine function $f = \cos : \Bbb R \to \Bbb R$. Is it true that the set of iterates $$\left\{f_n := \cos \circ \dotsb \circ \cos,  \; n \text{  times }   \mid  n \geq 1\right\}$$ is linearly independent over $\Bbb R$ ? That is, I am wondering if,  for any $r \geq 1$ and any real numbers $a_k$, we have : $$\sum_{k=1}^r a_k f_k = 0 : \Bbb R \to \Bbb R \implies a_k=0 \;\forall k.$$ I know that this true if we consider the powers of $\cos( \cdot )$, but I don't know how to deal with compositions. What I tried is to take derivative, or induction on the minimal length of linear dependence relation.",,"['real-analysis', 'linear-algebra']"
87,Prove that $|z^2+1|\le 2$ implies $|z^3+3z+2|\le 6$,Prove that  implies,|z^2+1|\le 2 |z^3+3z+2|\le 6,"Show that $$\{z \in \mathbb{C}: |z^2+1|\le 2 \} \subseteq \{z \in \mathbb{C} : |z^3+3z+2|\le 6 \} \tag{*}$$ (In other words: Let $z\in \mathbb{C}$ satisfy $|z^2+1|\le 2$ . Prove that $|z^3+3z+2|\le 6.$ ) This problem was asked by mengdie1982 roughly a day ago(relative to posting this question), but was closed due to lack of work. This has been bothering me because of how simply it has been stated, however even after hours of work I am not close to proving this mathematically. Geometrically the statement (*) is true, as seen below. $\hspace{4cm}$ Here are some estimates: From reverse triangle inequality, $|z^2+1| \le 2$ implies $|z| \le \sqrt{3}$ with equality at $z=i\sqrt{3}$ . From triangle inequality, $|z^3+3z+2| \le |z|^3+3|z|+2 \le 6\sqrt{3}+2 \approx 12.4$ . A better bound can also be achieved by factorization, $$|z^3+3z+2| = |z(z^2+1)+2(z+1)| \le |z||z^2+1|+2(|z|+1) \le 4\sqrt{3}+2 \approx 8.93. $$ Another approach I took was to show that for any $r<6$ , $$\{z \in \mathbb{C}: |z^2+1|\le 2 \} \nsubseteq \{z \in \mathbb{C} : |z^3+3z+2|\le r \}  $$ which explains the pinching behavior between the sets near the point $(1,0)$ . Define $$L=\left(\frac{2}{2-r+\sqrt{8-4r+r^{2}}}\right)^{\left(\frac{1}{3}\right)},\quad  z_0 = \frac{1}{2}\left(1+L-\frac{1}{L}\right), $$ then with much effort, it can be shown that $z_0$ satisfies $|z^2+1|\le 2$ , but it is not present in the other set. This implies that (*) can be reworded as  finding the largest $r>0$ such that $$\{z \in \mathbb{C}: |z^2+1|\le r \} \subseteq \{z \in \mathbb{C} : |z^3+3z+2|\le 6 \}$$ or it is possible that there is an easier solution which I am missing.","Show that (In other words: Let satisfy . Prove that ) This problem was asked by mengdie1982 roughly a day ago(relative to posting this question), but was closed due to lack of work. This has been bothering me because of how simply it has been stated, however even after hours of work I am not close to proving this mathematically. Geometrically the statement (*) is true, as seen below. Here are some estimates: From reverse triangle inequality, implies with equality at . From triangle inequality, . A better bound can also be achieved by factorization, Another approach I took was to show that for any , which explains the pinching behavior between the sets near the point . Define then with much effort, it can be shown that satisfies , but it is not present in the other set. This implies that (*) can be reworded as  finding the largest such that or it is possible that there is an easier solution which I am missing.","\{z \in \mathbb{C}: |z^2+1|\le 2 \} \subseteq \{z \in \mathbb{C} : |z^3+3z+2|\le 6 \} \tag{*} z\in \mathbb{C} |z^2+1|\le 2 |z^3+3z+2|\le 6. \hspace{4cm} |z^2+1| \le 2 |z| \le \sqrt{3} z=i\sqrt{3} |z^3+3z+2| \le |z|^3+3|z|+2 \le 6\sqrt{3}+2 \approx 12.4 |z^3+3z+2| = |z(z^2+1)+2(z+1)| \le |z||z^2+1|+2(|z|+1) \le 4\sqrt{3}+2 \approx 8.93.  r<6 \{z \in \mathbb{C}: |z^2+1|\le 2 \} \nsubseteq \{z \in \mathbb{C} : |z^3+3z+2|\le r \}   (1,0) L=\left(\frac{2}{2-r+\sqrt{8-4r+r^{2}}}\right)^{\left(\frac{1}{3}\right)},\quad  z_0 = \frac{1}{2}\left(1+L-\frac{1}{L}\right),  z_0 |z^2+1|\le 2 r>0 \{z \in \mathbb{C}: |z^2+1|\le r \} \subseteq \{z \in \mathbb{C} : |z^3+3z+2|\le 6 \}","['real-analysis', 'complex-analysis', 'algebra-precalculus', 'inequality', 'optimization']"
88,Is there a function having a limit at every point while being nowhere continuous?,Is there a function having a limit at every point while being nowhere continuous?,,"Is there a function $\,f:\mathbb{R}\rightarrow\mathbb{R},\,$ which has a limit at every $x\in\mathbb R$ and is everywhere discontinuous?","Is there a function $\,f:\mathbb{R}\rightarrow\mathbb{R},\,$ which has a limit at every $x\in\mathbb R$ and is everywhere discontinuous?",,"['calculus', 'real-analysis', 'analysis', 'limits', 'continuity']"
89,Comparing the Lebesgue measure of an open set and its closure,Comparing the Lebesgue measure of an open set and its closure,,"Let $E$ be an open set in $[0,1]^n$ and $m$ be the Lebesgue measure. Is it possible that $m(E)\neq m(\bar{E})$, where $\bar{E}$ stands for the closure of $E$?","Let $E$ be an open set in $[0,1]^n$ and $m$ be the Lebesgue measure. Is it possible that $m(E)\neq m(\bar{E})$, where $\bar{E}$ stands for the closure of $E$?",,"['real-analysis', 'measure-theory', 'examples-counterexamples']"
90,How slow/fast can $L^p$ norm grow?,How slow/fast can  norm grow?,L^p,"This is actually an exercise in Rudin's Real and Complex Analysis , $L^p$ spaces chapter. Could anyone help me out? Thanks in advance. Motivation: It's well known that if we have a function $f$ which belongs to $L^p(0,1)$ for all $p\ge 1$. Then $\lim_{p\rightarrow \infty}\|f\|_p=\|f\|_{\infty}$ (moreover, $\|f\|_p$ is increasing in $p$). This is true even if $\|f\|_{\infty}=\infty$. Question: How slow (fast) can $\|f\|_p$ grow when $\|f\|_{\infty}=\infty$? More precisely, given any positive increasing function $\Phi$ with $\lim_{p\rightarrow \infty}\Phi(p)=\infty$, can we always find a function $f$ which belongs to $L^p(0,1)$ for all $p\ge 1$, and $\|f\|_{\infty}=\infty$, such that $\|f\|_p\le (\ge)\Phi(p)$ for large $p$?","This is actually an exercise in Rudin's Real and Complex Analysis , $L^p$ spaces chapter. Could anyone help me out? Thanks in advance. Motivation: It's well known that if we have a function $f$ which belongs to $L^p(0,1)$ for all $p\ge 1$. Then $\lim_{p\rightarrow \infty}\|f\|_p=\|f\|_{\infty}$ (moreover, $\|f\|_p$ is increasing in $p$). This is true even if $\|f\|_{\infty}=\infty$. Question: How slow (fast) can $\|f\|_p$ grow when $\|f\|_{\infty}=\infty$? More precisely, given any positive increasing function $\Phi$ with $\lim_{p\rightarrow \infty}\Phi(p)=\infty$, can we always find a function $f$ which belongs to $L^p(0,1)$ for all $p\ge 1$, and $\|f\|_{\infty}=\infty$, such that $\|f\|_p\le (\ge)\Phi(p)$ for large $p$?",,"['real-analysis', 'banach-spaces']"
91,Two definitions of Lebesgue integration,Two definitions of Lebesgue integration,,"Normally, Lebesgue integral, for positive measures, is defined in the following way. First, one defines the integral for indicator functions, and linearly extend to simple functions. Then, for a general non-negative function $f$ , it is defined as the supremum of the integral of all simple functions less than or equal $f$. However, I find another definition in the book of Lieb and Loss, ""Analysis"". Let $f$ be the non-negative measurable function on a measure space $X$, let $\mu$ be the measure, and define for $t >0$, $$S_f(t) = \{x \in X : f(x) >t\},$$ and $$F_f(t) = \mu(S_f(t)) .$$ Note that $F_f(t)$ is now a Riemann integrable function. Now, the Lebesgue integral is defined as: $$ \int_X f \, d\mu = \int_0^\infty F_f(t) \, dt,$$ where the integral on the right-hand side is the Riemann integral. Here is the google books link to the definition. The book gives a heuristic reason why this definition agrees with the usual definition described here in the first paragraph. Now I would like to have a rigorous proof of the equivalence.","Normally, Lebesgue integral, for positive measures, is defined in the following way. First, one defines the integral for indicator functions, and linearly extend to simple functions. Then, for a general non-negative function $f$ , it is defined as the supremum of the integral of all simple functions less than or equal $f$. However, I find another definition in the book of Lieb and Loss, ""Analysis"". Let $f$ be the non-negative measurable function on a measure space $X$, let $\mu$ be the measure, and define for $t >0$, $$S_f(t) = \{x \in X : f(x) >t\},$$ and $$F_f(t) = \mu(S_f(t)) .$$ Note that $F_f(t)$ is now a Riemann integrable function. Now, the Lebesgue integral is defined as: $$ \int_X f \, d\mu = \int_0^\infty F_f(t) \, dt,$$ where the integral on the right-hand side is the Riemann integral. Here is the google books link to the definition. The book gives a heuristic reason why this definition agrees with the usual definition described here in the first paragraph. Now I would like to have a rigorous proof of the equivalence.",,"['real-analysis', 'measure-theory']"
92,Is mathematical history written by the victors?,Is mathematical history written by the victors?,,"The question is the title of a 2013 publication in the Notices of the American Mathematical Society , by twelve authors (of which I am one).  The contention is that traditional history of mathematics is based on the assumption of an inevitable evolution toward the real continuum-based framework as developed by Cantor, Dedekind, Weierstrass (referred to as the ""great triumvirate"" by Carl Boyer here ) and others. Taking some seminal remarks by Felix Klein as their starting point, the authors argue that the traditional view is lopsided and empoverishes our understanding of mathematical history.  Have the historians systematically underplayed the importance of the infinitesimal strand in the development of analysis? Editors are invited to submit reasoned responses based on factual historical knowledge, and refrain from answers based on opinion alone. To be even more explicit, we ask for additional examples from history that support either Boyer's viewpoint or the NAMS article viewpoint. That is, limit the question to facts and not opinions (based on a comment by Willie Wong at meta). Note 1.  For a closely related MO thread see this . Note 2.  A reaction to the Notices article by Craig Fraser was published here . Note 3. Another would-be victor Gray is analyzed in this MSE thread . Note 4. The Notices article originally contained a longish section on Euler, which was eventually split off into a separate article.  The article shows, using the writings of Ferraro as a case study, how an assumption of default Weierstrassian foundations deforms a scholar's vision of Euler's mathematics.  The article was recently published in 2017 in Journal for General Philosophy of Science . Note 5. A response to Craig Fraser's reaction was published in 2017 in Mat. Stud.; see this version with hyperlinks . Note 6. Further insight into the mentality of some math historians can be gleaned from a recent (2022-23) exchange in The Mathematical Intelligencer ; see the answer https://math.stackexchange.com/a/4725050/72694 below.","The question is the title of a 2013 publication in the Notices of the American Mathematical Society , by twelve authors (of which I am one).  The contention is that traditional history of mathematics is based on the assumption of an inevitable evolution toward the real continuum-based framework as developed by Cantor, Dedekind, Weierstrass (referred to as the ""great triumvirate"" by Carl Boyer here ) and others. Taking some seminal remarks by Felix Klein as their starting point, the authors argue that the traditional view is lopsided and empoverishes our understanding of mathematical history.  Have the historians systematically underplayed the importance of the infinitesimal strand in the development of analysis? Editors are invited to submit reasoned responses based on factual historical knowledge, and refrain from answers based on opinion alone. To be even more explicit, we ask for additional examples from history that support either Boyer's viewpoint or the NAMS article viewpoint. That is, limit the question to facts and not opinions (based on a comment by Willie Wong at meta). Note 1.  For a closely related MO thread see this . Note 2.  A reaction to the Notices article by Craig Fraser was published here . Note 3. Another would-be victor Gray is analyzed in this MSE thread . Note 4. The Notices article originally contained a longish section on Euler, which was eventually split off into a separate article.  The article shows, using the writings of Ferraro as a case study, how an assumption of default Weierstrassian foundations deforms a scholar's vision of Euler's mathematics.  The article was recently published in 2017 in Journal for General Philosophy of Science . Note 5. A response to Craig Fraser's reaction was published in 2017 in Mat. Stud.; see this version with hyperlinks . Note 6. Further insight into the mentality of some math historians can be gleaned from a recent (2022-23) exchange in The Mathematical Intelligencer ; see the answer https://math.stackexchange.com/a/4725050/72694 below.",,"['calculus', 'real-analysis', 'soft-question', 'math-history', 'nonstandard-analysis']"
93,Find function $f(x)$ satisfying $\int_{0}^{\infty} \frac{f(x)}{1+e^{nx}}dx=0$,Find function  satisfying,f(x) \int_{0}^{\infty} \frac{f(x)}{1+e^{nx}}dx=0,"I am looking for a non-trivial function $f(x)\in L_2(0,\infty)$ independent of the parameter $n$ (a natural number) satisfying the following integral equation: $$\displaystyle\int_{0}^{\infty} \frac{f(x)}{1+e^{nx}}dx=0$$ or prove that $f(x)=0$ is the only solution. The similar question is here but there are no parameters in the integral and the answer is based upon hit and trial method. I want to know what would be the nice approach to tackle this problem. EDIT: Such a function may exist, here is an example due to Stieltjes. A function $f(x) = \exp(-x^{1/4}) \sin x^{1/4}$ satisfies $\int_0^{\infty} f(x) x^n dx = 0$ for all integers $n \ge 0$ . We use the substitution $x=u^4$ to write $I_n = \int_0^{\infty} f(x) x^n dx = 4 \int_0^{\infty} e^{-u} \sin(u) u^{4n+3} du$ ; then integrate by parts four times (differentiating the power of $u$ , and integrating the rest) to show that $I_n$ is proportional to $I_{n-1}$ , and finally check that $I_0=0$ .(the Edit copied from here )","I am looking for a non-trivial function independent of the parameter (a natural number) satisfying the following integral equation: or prove that is the only solution. The similar question is here but there are no parameters in the integral and the answer is based upon hit and trial method. I want to know what would be the nice approach to tackle this problem. EDIT: Such a function may exist, here is an example due to Stieltjes. A function satisfies for all integers . We use the substitution to write ; then integrate by parts four times (differentiating the power of , and integrating the rest) to show that is proportional to , and finally check that .(the Edit copied from here )","f(x)\in L_2(0,\infty) n \displaystyle\int_{0}^{\infty} \frac{f(x)}{1+e^{nx}}dx=0 f(x)=0 f(x) = \exp(-x^{1/4}) \sin x^{1/4} \int_0^{\infty} f(x) x^n dx = 0 n \ge 0 x=u^4 I_n = \int_0^{\infty} f(x) x^n dx = 4 \int_0^{\infty} e^{-u} \sin(u) u^{4n+3} du u I_n I_{n-1} I_0=0","['real-analysis', 'integration', 'functional-analysis', 'analysis']"
94,New bound for Am-Gm of 2 variables,New bound for Am-Gm of 2 variables,,"Today I'm interested by the following problem : Let $x,y>0$ then we have : $$x+y-\sqrt{xy}\leq\exp\Big(\frac{x\ln(x)+y\ln(y)}{x+y}\Big)$$ The equality case comes when $x=y$ My proof uses derivative because for $x\geq y $ the function : $$f(x)=x+y-\sqrt{xy}-\exp\Big(\frac{x\ln(x)+y\ln(y)}{x+y}\Big)$$ is decreasing and for $y\geq x$ the function is increasing and the maximum occurs when $x=y$ My question is : Have you an alternative proof wich doesn't use derivative ? Thanks in advance.",Today I'm interested by the following problem : Let then we have : The equality case comes when My proof uses derivative because for the function : is decreasing and for the function is increasing and the maximum occurs when My question is : Have you an alternative proof wich doesn't use derivative ? Thanks in advance.,"x,y>0 x+y-\sqrt{xy}\leq\exp\Big(\frac{x\ln(x)+y\ln(y)}{x+y}\Big) x=y x\geq y  f(x)=x+y-\sqrt{xy}-\exp\Big(\frac{x\ln(x)+y\ln(y)}{x+y}\Big) y\geq x x=y","['real-analysis', 'inequality']"
95,When does a real function together with its derivatives form a basis?,When does a real function together with its derivatives form a basis?,,"Consider real functions defined on an interval, let's say $[-\pi,\pi]$. Let $s(x)$ be an analytic, periodic function such that all of its Fourier components are non-zero. We build a set of functions from $s(x)$ and all of its derivatives: S=$\{s(x),s'(x),s''(x),\dots\}$ What is the condition for $S$ to be a basis? By ""basis"" I mean that if I take any $f(x)\in L^2([-\pi,\pi])$ and produce a series of functions $f_N(x)$ which are the projections of $f(x)$ to the subspace spanned by the first $N$ elements of $S$, then $f_N(x)\to f(x)$ in $L^2$ norm. What I understood is that if we want that $S$ be a basis, then it is necessary that all Fourier components are not zero. But is this enough or is there some other condition for $s(x)$? I did not get far with any proofs. All I understand is that if I take the first few members of the set and I project to the space constructed from the first few Fourier components, then we will have a basis there. But I can't go further.","Consider real functions defined on an interval, let's say $[-\pi,\pi]$. Let $s(x)$ be an analytic, periodic function such that all of its Fourier components are non-zero. We build a set of functions from $s(x)$ and all of its derivatives: S=$\{s(x),s'(x),s''(x),\dots\}$ What is the condition for $S$ to be a basis? By ""basis"" I mean that if I take any $f(x)\in L^2([-\pi,\pi])$ and produce a series of functions $f_N(x)$ which are the projections of $f(x)$ to the subspace spanned by the first $N$ elements of $S$, then $f_N(x)\to f(x)$ in $L^2$ norm. What I understood is that if we want that $S$ be a basis, then it is necessary that all Fourier components are not zero. But is this enough or is there some other condition for $s(x)$? I did not get far with any proofs. All I understand is that if I take the first few members of the set and I project to the space constructed from the first few Fourier components, then we will have a basis there. But I can't go further.",,"['real-analysis', 'functional-analysis']"
96,"Elementary proof that the limit of $\sum_{i=1}^{\infty} \frac{1}{\operatorname{lcm}(1,2,...,i)}$ is irrational",Elementary proof that the limit of  is irrational,"\sum_{i=1}^{\infty} \frac{1}{\operatorname{lcm}(1,2,...,i)}","Show that the infinite sum $S$ defined by -$$S=\sum_{i=1}^\infty \frac{1}{\operatorname{lcm}(1,2,...,i)}$$ is an irrational number. I found this question while reading 'Mathematical Gems' by Ross Honsberger. After pondering over it for nearly an hour, I was able to prove it by using Bertrand's postulate which states that there is a prime between n and 2n for every natural number n>1. This question was solved by Lajos Pósa when he was just 12 years old. Is there any elementary proof that does not use Bertrand's postulate or any complicated theorem?","Show that the infinite sum $S$ defined by -$$S=\sum_{i=1}^\infty \frac{1}{\operatorname{lcm}(1,2,...,i)}$$ is an irrational number. I found this question while reading 'Mathematical Gems' by Ross Honsberger. After pondering over it for nearly an hour, I was able to prove it by using Bertrand's postulate which states that there is a prime between n and 2n for every natural number n>1. This question was solved by Lajos Pósa when he was just 12 years old. Is there any elementary proof that does not use Bertrand's postulate or any complicated theorem?",,"['real-analysis', 'sequences-and-series', 'elementary-number-theory', 'analytic-number-theory', 'irrational-numbers']"
97,Prove $\sum\limits_{n = 1}^\infty \frac{( - 1)^n}{\ln n + \sin n} $ is convergent.,Prove  is convergent.,\sum\limits_{n = 1}^\infty \frac{( - 1)^n}{\ln n + \sin n} ,"Help prove the alternating series $\sum\limits_{n = 1}^\infty \frac{(-1)^n}{\ln n + \sin n}$ is convergent. $\frac 1 {\ln n + \sin n}$ is a decreasing sequence but it is not motonically decreasing. I am not sure how to deal with this situation. My failed attempt.. For even terms, $$\sum_{n = 1}^\infty  \frac{( - 1)^{2n}}{\ln 2n + 1} \le \sum_{n = 1}^\infty  \frac{( - 1)^{2n}}{\ln 2n + \sin 2n} \leqslant \sum_{n = 1}^\infty  \frac{( - 1)^{2n}}{\ln 2n - 1} $$ where the two ""bound"" series do not converge For odd terms $$\sum_{n = 1}^\infty \frac{( - 1)^{2n + 1}}{\ln (2n + 1) - 1} \leqslant \sum_{n = 1}^\infty \frac{(-1)^{2n + 1}}{\ln (2n + 1) + \sin (2n + 1)}  \leqslant \sum_{n = 1}^\infty \frac{( - 1)^{2n + 1}}{\ln 2n + 1 + 1} $$ where the two ""bound"" series do not converge.","Help prove the alternating series is convergent. is a decreasing sequence but it is not motonically decreasing. I am not sure how to deal with this situation. My failed attempt.. For even terms, where the two ""bound"" series do not converge For odd terms where the two ""bound"" series do not converge.",\sum\limits_{n = 1}^\infty \frac{(-1)^n}{\ln n + \sin n} \frac 1 {\ln n + \sin n} \sum_{n = 1}^\infty  \frac{( - 1)^{2n}}{\ln 2n + 1} \le \sum_{n = 1}^\infty  \frac{( - 1)^{2n}}{\ln 2n + \sin 2n} \leqslant \sum_{n = 1}^\infty  \frac{( - 1)^{2n}}{\ln 2n - 1}  \sum_{n = 1}^\infty \frac{( - 1)^{2n + 1}}{\ln (2n + 1) - 1} \leqslant \sum_{n = 1}^\infty \frac{(-1)^{2n + 1}}{\ln (2n + 1) + \sin (2n + 1)}  \leqslant \sum_{n = 1}^\infty \frac{( - 1)^{2n + 1}}{\ln 2n + 1 + 1} ,"['calculus', 'real-analysis', 'sequences-and-series']"
98,Constructing an infinite chain of subfields of 'hyper' algebraic numbers?,Constructing an infinite chain of subfields of 'hyper' algebraic numbers?,,"This has now been cross posted to MO. Let $F$ be a subset of $\mathbb{R}$ and let $S_F$ denote the set of values which satisfy some generalized polynomial whose exponents and coefficients are drawn from $F$ .  That is, we let $S_F$ denote $$\bigg \{x \in \mathbb{R}: 0=\sum_{i=1}^n{a_i x^{e_i}}: e_i \in F \text{ distinct}, a_i\in F \text{ non-zero}, n\in \mathbb{N}  \bigg  \}$$ Then $S_{\mathbb{\mathbb{Q}}}$ is the set of algebraic real numbers and we start to see the beginnings of a chain: $ \mathbb{Q} \subsetneq S_\mathbb{Q} \subsetneq S_{S_\mathbb{Q}} $ Main Question Does this chain continue forever? That is, we let $A_0= \mathbb{Q}$ and let $A_{n+1}=S_{A_{n}}$ . Is it the case that $A_n \subsetneq A_{n+1}$ for all $n\in\mathbb{N}$ ? Other curiosities: Is $A_i$ always a field? Perhaps, the argument is analogous to this . Or maybe this is just the case in a more general setting: Is it the case that $F \subset \mathbb{R}$ , a field implies that $S_F$ is a field? Is it possible to see that $e\notin \cup A_i$ ? Perhaps this is just a tweaking of LW Theorem.","This has now been cross posted to MO. Let be a subset of and let denote the set of values which satisfy some generalized polynomial whose exponents and coefficients are drawn from .  That is, we let denote Then is the set of algebraic real numbers and we start to see the beginnings of a chain: Main Question Does this chain continue forever? That is, we let and let . Is it the case that for all ? Other curiosities: Is always a field? Perhaps, the argument is analogous to this . Or maybe this is just the case in a more general setting: Is it the case that , a field implies that is a field? Is it possible to see that ? Perhaps this is just a tweaking of LW Theorem.","F \mathbb{R} S_F F S_F \bigg \{x \in \mathbb{R}: 0=\sum_{i=1}^n{a_i x^{e_i}}: e_i \in F \text{ distinct}, a_i\in F \text{ non-zero}, n\in \mathbb{N}  \bigg  \} S_{\mathbb{\mathbb{Q}}} 
\mathbb{Q}
\subsetneq S_\mathbb{Q} \subsetneq
S_{S_\mathbb{Q}}  A_0= \mathbb{Q} A_{n+1}=S_{A_{n}} A_n \subsetneq A_{n+1} n\in\mathbb{N} A_i F \subset \mathbb{R} S_F e\notin \cup A_i","['real-analysis', 'field-theory', 'transcendental-numbers']"
99,"Computing $\int_{0}^{\pi}\ln\left(1-2a\cos x+a^2\right) \, dx$",Computing,"\int_{0}^{\pi}\ln\left(1-2a\cos x+a^2\right) \, dx","For $a\ge 0$ let's define $$I(a)=\int_{0}^{\pi}\ln\left(1-2a\cos x+a^2\right)dx.$$ Find explicit formula for $I(a)$. My attempt: Let $$\begin{align*} f_n(x) &= \frac{\ln\left(1-2 \left(a+\frac{1}{n}\right)\cos x+\left(a+\frac{1}{n}\right)^2\right)-\ln\left(1-2a\cos x+a^2\right)}{\frac{1}{n}}\\ &=\frac{\ln\left(\displaystyle\frac{1-2 \left(a+\frac{1}{n}\right)\cos x+\left(a+\frac{1}{n}\right)^2}{1-2a\cos x+a^2}\right)}{\frac{1}{n}}\\ &=\frac{\ln\left(1+\dfrac{1}{n}\left(\displaystyle\frac{2a-2\cos x+\frac{1}{n}}{1-2a\cos x+a^2}\right)\right)}{\frac{1}{n}}. \end{align*}$$ Now it is easy to see that $f_n(x) \to \frac{2a-2\cos x}{1-2a\cos x+a^2}$ as $n \to \infty$. $|f_n(x)|\le \frac{2a+2}{(1-a)^2}$ RHS is integrable so $\lim_{n\to\infty}\int_0^\pi f_n(x)dx = \int_0^\pi \frac{2a-2\cos x}{1-2a\cos x+a^2} dx=I'(a)$. But  $$\int_0^\pi \frac{2a-2\cos x}{1-2a\cos x+a^2}=\int_0^\pi\left(1-\frac{(1-a)^2}{1-2a\cos x+a^2}\right)dx.$$ Consider $$\int_0^\pi\frac{dx}{1-2a\cos x+a^2}=\int_0^\infty\frac{\frac{dy}{1+t^2}}{1-2a\frac{1-t^2}{1+t^2}+a^2}=\int_0^\infty\frac{dt}{1+t^2-2a(1-t^2)+a^2(1+t^2)}=\int_0^\infty\frac{dt}{(1-a)^2+\left((1+a)t\right)^2}\stackrel{(*)}{=}\frac{1}{(1-a)^2}\int_0^\infty\frac{dt}{1+\left(\frac{1+a}{1-a}t\right)^2}=\frac{1}{(1-a)(1+a)}\int_0^\infty\frac{du}{1+u^2}=\frac{1}{(1-a)(1+a)}\frac{\pi}{2}.$$ So $$I'(a)=\frac{\pi}{2}\left(2-\frac{1-a}{1+a}\right)\Rightarrow I(a)=\frac{\pi}{2}\left(3a-2\ln\left(a+1\right)\right).$$ It looks too easy, is there any crucial lack? $(*)$ — we have to check $a=1$ here by hand and actually consider $[0,1), (1,\infty)$ but result on these two intervals may differ only by constant - it may be important but in my opinion not crucial for this proof.","For $a\ge 0$ let's define $$I(a)=\int_{0}^{\pi}\ln\left(1-2a\cos x+a^2\right)dx.$$ Find explicit formula for $I(a)$. My attempt: Let $$\begin{align*} f_n(x) &= \frac{\ln\left(1-2 \left(a+\frac{1}{n}\right)\cos x+\left(a+\frac{1}{n}\right)^2\right)-\ln\left(1-2a\cos x+a^2\right)}{\frac{1}{n}}\\ &=\frac{\ln\left(\displaystyle\frac{1-2 \left(a+\frac{1}{n}\right)\cos x+\left(a+\frac{1}{n}\right)^2}{1-2a\cos x+a^2}\right)}{\frac{1}{n}}\\ &=\frac{\ln\left(1+\dfrac{1}{n}\left(\displaystyle\frac{2a-2\cos x+\frac{1}{n}}{1-2a\cos x+a^2}\right)\right)}{\frac{1}{n}}. \end{align*}$$ Now it is easy to see that $f_n(x) \to \frac{2a-2\cos x}{1-2a\cos x+a^2}$ as $n \to \infty$. $|f_n(x)|\le \frac{2a+2}{(1-a)^2}$ RHS is integrable so $\lim_{n\to\infty}\int_0^\pi f_n(x)dx = \int_0^\pi \frac{2a-2\cos x}{1-2a\cos x+a^2} dx=I'(a)$. But  $$\int_0^\pi \frac{2a-2\cos x}{1-2a\cos x+a^2}=\int_0^\pi\left(1-\frac{(1-a)^2}{1-2a\cos x+a^2}\right)dx.$$ Consider $$\int_0^\pi\frac{dx}{1-2a\cos x+a^2}=\int_0^\infty\frac{\frac{dy}{1+t^2}}{1-2a\frac{1-t^2}{1+t^2}+a^2}=\int_0^\infty\frac{dt}{1+t^2-2a(1-t^2)+a^2(1+t^2)}=\int_0^\infty\frac{dt}{(1-a)^2+\left((1+a)t\right)^2}\stackrel{(*)}{=}\frac{1}{(1-a)^2}\int_0^\infty\frac{dt}{1+\left(\frac{1+a}{1-a}t\right)^2}=\frac{1}{(1-a)(1+a)}\int_0^\infty\frac{du}{1+u^2}=\frac{1}{(1-a)(1+a)}\frac{\pi}{2}.$$ So $$I'(a)=\frac{\pi}{2}\left(2-\frac{1-a}{1+a}\right)\Rightarrow I(a)=\frac{\pi}{2}\left(3a-2\ln\left(a+1\right)\right).$$ It looks too easy, is there any crucial lack? $(*)$ — we have to check $a=1$ here by hand and actually consider $[0,1), (1,\infty)$ but result on these two intervals may differ only by constant - it may be important but in my opinion not crucial for this proof.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'solution-verification']"
