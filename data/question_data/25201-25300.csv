,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Let $A \in \Bbb M_3(\Bbb R)$ be such that $A^4= I, A \neq \pm I.$ Then is it true that $A^2 + I=0$?",Let  be such that  Then is it true that ?,"A \in \Bbb M_3(\Bbb R) A^4= I, A \neq \pm I. A^2 + I=0","Let $\lambda$ be an eigenvalue of $A$ . Then $\lambda^4$ is an eigenvalue of $A^4$ . Let $v$ be the corresponding eigenvector of $\lambda^4$ . Then $A^4 v = \lambda^4 v \Rightarrow Iv=\lambda^4 v \Rightarrow \lambda^4=1 \Rightarrow \lambda= \pm 1,\pm i$ . Since $A$ is a real matrix, if $i$ (or $-i$ ) is it's eigenvalue, then $-i$ (respectively $i$ ) is also it's eigenvalue. Also $A \neq \pm I$ . Therefore possible characteristic polynomials of $A$ are $(x-1)(x+1)^2,(x+1)(x-1)^2,(x-1)(x^2+1),(x+1)(x^2+1)$ . Now if possible, $x^2+1$ will be a minimal polynomial of $A$ for the characteristic polynomials $(x-1)(x^2+1),(x+1)(x^2+1)$ only. But any such minimal polynomial must have factors $(x-1)$ and $(x+1)$ respectively. Hence $A^2+ I=0$ is not possible. Are there any mistakes in my proof? I am learning to handle proofs containing characteristic polynomials, minimal polynomials, eigenvalues etc. Thanks.","Let be an eigenvalue of . Then is an eigenvalue of . Let be the corresponding eigenvector of . Then . Since is a real matrix, if (or ) is it's eigenvalue, then (respectively ) is also it's eigenvalue. Also . Therefore possible characteristic polynomials of are . Now if possible, will be a minimal polynomial of for the characteristic polynomials only. But any such minimal polynomial must have factors and respectively. Hence is not possible. Are there any mistakes in my proof? I am learning to handle proofs containing characteristic polynomials, minimal polynomials, eigenvalues etc. Thanks.","\lambda A \lambda^4 A^4 v \lambda^4 A^4 v = \lambda^4 v \Rightarrow Iv=\lambda^4 v \Rightarrow \lambda^4=1 \Rightarrow \lambda= \pm 1,\pm i A i -i -i i A \neq \pm I A (x-1)(x+1)^2,(x+1)(x-1)^2,(x-1)(x^2+1),(x+1)(x^2+1) x^2+1 A (x-1)(x^2+1),(x+1)(x^2+1) (x-1) (x+1) A^2+ I=0","['linear-algebra', 'matrices', 'proof-verification', 'contest-math']"
1,"""Visualizing'' rotation in even dimensions","""Visualizing'' rotation in even dimensions",,"So as we know, a linear transformation $\Bbb R^n\to\Bbb R^n$ must have an even number of non-real complex eigenvalues. One consequence of this is that, in $4$ dimensions, we cannot talk about rotation about a line — the only non-trivial rotation fixes a plane . Since we can't visualize $4$ dimensions, I was trying to think of a way to interpret these rotations. One useful way is to imagine a $3$ dimensional space where each point has a fourth coordinate, which we can interpret as something like ""temperature"" of the point. After playing around for a bit, I realized that rotations around a plane in this world look like stretching along one axis. For example, if we fix the $x$-$y$ plane, then the point $(0,0,1)$ might become $(0,0,2)$. Then the temperature of that point would have to decrease correspondingly. It is probably a coincidence, but in that case, a rotation looks like compressing/decompressing a gas. As far as I can tell, nothing about $6$ dimensions makes this any more interesting. You just have to add in some other properties (density, color) and you have the same basic picture. What other methods do you know for interpreting rotation about a plane? Without getting too much into physics, what is the connection between rotation in $4$ dimensions and our apparently $3$ dimensional world?","So as we know, a linear transformation $\Bbb R^n\to\Bbb R^n$ must have an even number of non-real complex eigenvalues. One consequence of this is that, in $4$ dimensions, we cannot talk about rotation about a line — the only non-trivial rotation fixes a plane . Since we can't visualize $4$ dimensions, I was trying to think of a way to interpret these rotations. One useful way is to imagine a $3$ dimensional space where each point has a fourth coordinate, which we can interpret as something like ""temperature"" of the point. After playing around for a bit, I realized that rotations around a plane in this world look like stretching along one axis. For example, if we fix the $x$-$y$ plane, then the point $(0,0,1)$ might become $(0,0,2)$. Then the temperature of that point would have to decrease correspondingly. It is probably a coincidence, but in that case, a rotation looks like compressing/decompressing a gas. As far as I can tell, nothing about $6$ dimensions makes this any more interesting. You just have to add in some other properties (density, color) and you have the same basic picture. What other methods do you know for interpreting rotation about a plane? Without getting too much into physics, what is the connection between rotation in $4$ dimensions and our apparently $3$ dimensional world?",,"['linear-algebra', 'rotations', 'visualization']"
2,determinant diagonal zero symmetric matrix,determinant diagonal zero symmetric matrix,,"Let $$M=\begin{pmatrix}           0 & 1    &  1   &     1\\     1 & 0    &  \alpha + \beta &  \alpha + \gamma\\     1 & \beta + \alpha & 0    &  \beta + \gamma\\     1& \gamma + \alpha  & \gamma + \beta  &   0 \end{pmatrix},$$ then it holds $$\det(M) = −4(\alpha\beta + \beta\gamma + \gamma\alpha).$$ What is the value of this when $\alpha,\beta,\gamma$ are the three roots of the equation $x^3 − 1 = 0$? Can anyone help me to do it by elementary row operation? My idea is just solving $\alpha,\beta,\gamma$ and then plug in $−4(\alpha\beta + \beta\gamma + \gamma\alpha)$ and when I try to find the determinant of the LHS I got $-4\beta\gamma$.","Let $$M=\begin{pmatrix}           0 & 1    &  1   &     1\\     1 & 0    &  \alpha + \beta &  \alpha + \gamma\\     1 & \beta + \alpha & 0    &  \beta + \gamma\\     1& \gamma + \alpha  & \gamma + \beta  &   0 \end{pmatrix},$$ then it holds $$\det(M) = −4(\alpha\beta + \beta\gamma + \gamma\alpha).$$ What is the value of this when $\alpha,\beta,\gamma$ are the three roots of the equation $x^3 − 1 = 0$? Can anyone help me to do it by elementary row operation? My idea is just solving $\alpha,\beta,\gamma$ and then plug in $−4(\alpha\beta + \beta\gamma + \gamma\alpha)$ and when I try to find the determinant of the LHS I got $-4\beta\gamma$.",,"['linear-algebra', 'matrices', 'vector-spaces', 'determinant', 'symmetric-matrices']"
3,Division algebra = Field?,Division algebra = Field?,,"Is a division algebra a field? If not, why does it differ? It is an abelian group with multiplication and division. How not a field?","Is a division algebra a field? If not, why does it differ? It is an abelian group with multiplication and division. How not a field?",,"['linear-algebra', 'field-theory', 'division-algebras']"
4,How does the least squares solution change if we add redundant rows?,How does the least squares solution change if we add redundant rows?,,"Let $\mathbf{Ax} = \mathbf{b}$ represent a set of linear equations, where, $\mathbf{A}\in\mathrm{R}^{m\times n} (m > n)$, $\mathbf{x}\in\mathrm{n\times 1}$. It is known that $\mathbf{A}$ has rank $n$. By using least squares method, one can estimate $\mathbf{x}$. Now, let us add some redundant rows to $\mathbf{A}$ so that the new rows are integer multiples of one or more of $m$ rows. Let the new matrix be called $\mathbf{A}_1$. If $\mathbf{x}_1$ is the new solution for $\mathbf{A}_1\mathbf{x}_1 = \mathbf{b}$, how does $\mathbf{x}_1$ differ from $\mathbf{x}$? In general, what is the effect on the least squares solution if we add redundant rows to $\mathbf{A}$?","Let $\mathbf{Ax} = \mathbf{b}$ represent a set of linear equations, where, $\mathbf{A}\in\mathrm{R}^{m\times n} (m > n)$, $\mathbf{x}\in\mathrm{n\times 1}$. It is known that $\mathbf{A}$ has rank $n$. By using least squares method, one can estimate $\mathbf{x}$. Now, let us add some redundant rows to $\mathbf{A}$ so that the new rows are integer multiples of one or more of $m$ rows. Let the new matrix be called $\mathbf{A}_1$. If $\mathbf{x}_1$ is the new solution for $\mathbf{A}_1\mathbf{x}_1 = \mathbf{b}$, how does $\mathbf{x}_1$ differ from $\mathbf{x}$? In general, what is the effect on the least squares solution if we add redundant rows to $\mathbf{A}$?",,"['linear-algebra', 'optimization', 'numerical-linear-algebra', 'least-squares']"
5,Prove that $R^{2}$ cannot decrease when adding a variable,Prove that  cannot decrease when adding a variable,R^{2},"I know that in general this is true because the smaller model is nested within the larger model, so the larger model must have SSE at least as low as the smaller one, but I'm having a hard time proving this mathematically. I know that $$R^{2} = \frac{\mathbf{{\hat{\boldsymbol\beta '} X ' y}} - n \bar{\mathbf{y}}^{2}}{\mathbf{y'y} - n\bar{\mathbf{y}}^{2}}$$ So my idea was to show that $\mathbf{{\hat{\boldsymbol\beta '} X ' y}}$ is increasing with $p$ (assuming $\mathbf{X}$ is $n$ x $p$) since the rest of the equation is constant wrt $\mathbf{X}$.  I've gotten as far as $$\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{p} \hat{\boldsymbol\beta}_j \ x_{ij} \ y_{i}$$That is where I'm stuck.","I know that in general this is true because the smaller model is nested within the larger model, so the larger model must have SSE at least as low as the smaller one, but I'm having a hard time proving this mathematically. I know that $$R^{2} = \frac{\mathbf{{\hat{\boldsymbol\beta '} X ' y}} - n \bar{\mathbf{y}}^{2}}{\mathbf{y'y} - n\bar{\mathbf{y}}^{2}}$$ So my idea was to show that $\mathbf{{\hat{\boldsymbol\beta '} X ' y}}$ is increasing with $p$ (assuming $\mathbf{X}$ is $n$ x $p$) since the rest of the equation is constant wrt $\mathbf{X}$.  I've gotten as far as $$\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{p} \hat{\boldsymbol\beta}_j \ x_{ij} \ y_{i}$$That is where I'm stuck.",,"['linear-algebra', 'regression', 'linear-regression']"
6,"$X$ is a matrix. Find matrix $A, B$ such that $X=AB-BA$",is a matrix. Find matrix  such that,"X A, B X=AB-BA","Let $X \in \mathbb{R}^{3\times 3}$ be a diagonal matrix whose diagonal elements (from left to right) are $1$, $y$ and $-1$. For what values of $y$ will there exist matrices $A,B \in \mathbb{R}^{3\times 3}$ such that $AB-BA=X$? So far using the properties of the trace function, I have deduced that $y$ cannot be non-zero, since $AB-BA=X$ implies $\text{tr}(AB-BA)=\text{tr}(X)$. Since $\text{tr}(AB-BA)=0$, the sum of diagonal elements of $X$ is also $0$. Therefore $1+y+(-1)=0$. How do I show that there exist or does not exist $A,B$ such that $AB-BA=X$, where $y=0$. The usual method of computing the product and difference to find the solution seems to be very tedious. I would be happy if someone can suggest a shorter method. Thanks.","Let $X \in \mathbb{R}^{3\times 3}$ be a diagonal matrix whose diagonal elements (from left to right) are $1$, $y$ and $-1$. For what values of $y$ will there exist matrices $A,B \in \mathbb{R}^{3\times 3}$ such that $AB-BA=X$? So far using the properties of the trace function, I have deduced that $y$ cannot be non-zero, since $AB-BA=X$ implies $\text{tr}(AB-BA)=\text{tr}(X)$. Since $\text{tr}(AB-BA)=0$, the sum of diagonal elements of $X$ is also $0$. Therefore $1+y+(-1)=0$. How do I show that there exist or does not exist $A,B$ such that $AB-BA=X$, where $y=0$. The usual method of computing the product and difference to find the solution seems to be very tedious. I would be happy if someone can suggest a shorter method. Thanks.",,"['linear-algebra', 'matrices', 'matrix-equations']"
7,Why not always make a linear system's matrix symmetric?,Why not always make a linear system's matrix symmetric?,,"This may be quite a naive question, but as I'm reading about different methods to solve linear systems of the type Ax=b with A a n x n matrix, I wonder why we should not always solve an equivalent system with a symmetric matrix obtained by multiplying the equation with the transpose of A, i.e. $$A^T A x = A^T b$$ (especially in the context of large, sparse matrices and matrix free methods where symmetric matrices have quite desirable properties)? Edit: Lets consider that the problem is well posed so that the square matrix A is invertible and the solution x is unique.","This may be quite a naive question, but as I'm reading about different methods to solve linear systems of the type Ax=b with A a n x n matrix, I wonder why we should not always solve an equivalent system with a symmetric matrix obtained by multiplying the equation with the transpose of A, i.e. $$A^T A x = A^T b$$ (especially in the context of large, sparse matrices and matrix free methods where symmetric matrices have quite desirable properties)? Edit: Lets consider that the problem is well posed so that the square matrix A is invertible and the solution x is unique.",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
8,A certain unique rotation matrix,A certain unique rotation matrix,,One can find that the matrix $A=\begin{bmatrix}     -\dfrac{1}{3} &     \dfrac{2}{3}      &     \dfrac{2}{3}   \\      \dfrac{2}{3}  &    -\dfrac{1}{3}   &   \dfrac{2}{3}    \\     \dfrac{2}{3} &    \dfrac{2}{3}  &    -\dfrac{1}{3}  \\         \end{bmatrix}  $ is at the same time $3D$ rotation matrix and for it the sum of  entries in  every  column (row)  is constant (here $-\dfrac{1}{3}+   \dfrac{2}{3}     +     \dfrac{2}{3} = 1)$. The same is true if we change  the order of columns in it. For example: $A_1=\begin{bmatrix}     \dfrac{2}{3} &     \dfrac{2}{3}      &     -\dfrac{1}{3}   \\      -\dfrac{1}{3}  &    \dfrac{2}{3}   &   \dfrac{2}{3}    \\     \dfrac{2}{3} &    -\dfrac{1}{3}  &    \dfrac{2}{3}  \\         \end{bmatrix}  $       $A_2=\begin{bmatrix}      \dfrac{2}{3} &     -\dfrac{1}{3}      &     \dfrac{2}{3}   \\      \dfrac{2}{3}  &     \dfrac{2}{3}   &   -\dfrac{1}{3}    \\     -\dfrac{1}{3} &    \dfrac{2}{3}  &     \dfrac{2}{3}  \\         \end{bmatrix}  $ Questions: Is it any systematic way to find other non-trivial (without $0$ and $1$) rotation matrices with this property? Especially it is interesting whether the above rotation matrices are the only ones with rational entries ?- maybe someone knows other rotation matrices exist where the sum of entries is constant.. and... Can it be proved in some way that if the sum of entries in columns for a  rotation matrix is constant then it should be equal to the length of  column vectors?,One can find that the matrix $A=\begin{bmatrix}     -\dfrac{1}{3} &     \dfrac{2}{3}      &     \dfrac{2}{3}   \\      \dfrac{2}{3}  &    -\dfrac{1}{3}   &   \dfrac{2}{3}    \\     \dfrac{2}{3} &    \dfrac{2}{3}  &    -\dfrac{1}{3}  \\         \end{bmatrix}  $ is at the same time $3D$ rotation matrix and for it the sum of  entries in  every  column (row)  is constant (here $-\dfrac{1}{3}+   \dfrac{2}{3}     +     \dfrac{2}{3} = 1)$. The same is true if we change  the order of columns in it. For example: $A_1=\begin{bmatrix}     \dfrac{2}{3} &     \dfrac{2}{3}      &     -\dfrac{1}{3}   \\      -\dfrac{1}{3}  &    \dfrac{2}{3}   &   \dfrac{2}{3}    \\     \dfrac{2}{3} &    -\dfrac{1}{3}  &    \dfrac{2}{3}  \\         \end{bmatrix}  $       $A_2=\begin{bmatrix}      \dfrac{2}{3} &     -\dfrac{1}{3}      &     \dfrac{2}{3}   \\      \dfrac{2}{3}  &     \dfrac{2}{3}   &   -\dfrac{1}{3}    \\     -\dfrac{1}{3} &    \dfrac{2}{3}  &     \dfrac{2}{3}  \\         \end{bmatrix}  $ Questions: Is it any systematic way to find other non-trivial (without $0$ and $1$) rotation matrices with this property? Especially it is interesting whether the above rotation matrices are the only ones with rational entries ?- maybe someone knows other rotation matrices exist where the sum of entries is constant.. and... Can it be proved in some way that if the sum of entries in columns for a  rotation matrix is constant then it should be equal to the length of  column vectors?,,"['linear-algebra', 'matrices', 'number-theory', 'rotations']"
9,Product rule for matrix-valued functions and Differentiability of matrix multiplication,Product rule for matrix-valued functions and Differentiability of matrix multiplication,,"Let $F, G: \mathbb{R}^{n \text{ x } n} \to \mathbb{R}^{n \text{ x } n}$ be two differentiable functions. Defining $(F \cdot\ G)(A):=F(A)G(A)$ using the usual matrix multiplication, prove that $F\cdot\ G$ is differentiable and express $(F\cdot\ G)'$ in terms of $F$ , $F'$ , $G$ , $G'$ . What I thought of doing was to write $A=[a_{ij}]_{i, j \in {1, ..., n}}$ and express $F$ as a matrix $[f_{ij}]_{i, j \in {1, ..., n}}$ of differentiable functions $f_{ij}:\mathbb{R} \to \mathbb{R}$ (same idea for $G$ ), and I imagine $(F\cdot\ G)'$ will look something like $F'\cdot\ G+F\cdot\ G'$ , but I don't know how to formalize it. [obs.: the matrix norm considered here is $||A||=\sup_{|v|=1}|A(v)|$ , where $v \in \mathbb{R^n}$ and $|\cdot\ |$ is the usual euclidean norm]","Let be two differentiable functions. Defining using the usual matrix multiplication, prove that is differentiable and express in terms of , , , . What I thought of doing was to write and express as a matrix of differentiable functions (same idea for ), and I imagine will look something like , but I don't know how to formalize it. [obs.: the matrix norm considered here is , where and is the usual euclidean norm]","F, G: \mathbb{R}^{n \text{ x } n} \to \mathbb{R}^{n \text{ x } n} (F \cdot\ G)(A):=F(A)G(A) F\cdot\ G (F\cdot\ G)' F F' G G' A=[a_{ij}]_{i, j \in {1, ..., n}} F [f_{ij}]_{i, j \in {1, ..., n}} f_{ij}:\mathbb{R} \to \mathbb{R} G (F\cdot\ G)' F'\cdot\ G+F\cdot\ G' ||A||=\sup_{|v|=1}|A(v)| v \in \mathbb{R^n} |\cdot\ |","['real-analysis', 'linear-algebra', 'matrix-calculus']"
10,"Why do we need ""basis"" in linear algebra?","Why do we need ""basis"" in linear algebra?",,"I am very curios about why in linear algebra we need different basis, why can't we just have the standard basis and work with that? And how basis is used in computer graphics? thank you in advance.","I am very curios about why in linear algebra we need different basis, why can't we just have the standard basis and work with that? And how basis is used in computer graphics? thank you in advance.",,['linear-algebra']
11,The minimum value of $\frac{a^3 + b^3 + c^3 }{\sqrt{a^4 + b^4 + c^4 }}$ . When $a^2 + b^2 + c^2 = 1 $,The minimum value of  . When,\frac{a^3 + b^3 + c^3 }{\sqrt{a^4 + b^4 + c^4 }} a^2 + b^2 + c^2 = 1 ,"Asume $a, b, c $ is non-negative real. I got above equation at this situation ; $\vec {x}= (a, b, c)$ ,  $\vec {y} = (a^2 , b^2 , c^2 ) $ $$ cos \phi = \frac{ \vec x \cdot \vec {y}} { \Vert {\vec {x} \Vert} \times\Vert {\vec {y} \Vert}} = \frac{ a^3 + b^3 + c^3 } { \sqrt{ a^4 + b^4 + c^4 }}$$ In this case I want to know the Maximum value of $\phi$ . But I can't compute.  I want your help.","Asume $a, b, c $ is non-negative real. I got above equation at this situation ; $\vec {x}= (a, b, c)$ ,  $\vec {y} = (a^2 , b^2 , c^2 ) $ $$ cos \phi = \frac{ \vec x \cdot \vec {y}} { \Vert {\vec {x} \Vert} \times\Vert {\vec {y} \Vert}} = \frac{ a^3 + b^3 + c^3 } { \sqrt{ a^4 + b^4 + c^4 }}$$ In this case I want to know the Maximum value of $\phi$ . But I can't compute.  I want your help.",,"['linear-algebra', 'geometry', 'inequality']"
12,Norm of matrix and its maximum eigenvalue,Norm of matrix and its maximum eigenvalue,,I've seen in some inequalities in the theory of ODEs that $\lVert Q \lVert \le \lambda_{max}(Q)$. What theorem from Linear Algebra is relevant here?,I've seen in some inequalities in the theory of ODEs that $\lVert Q \lVert \le \lambda_{max}(Q)$. What theorem from Linear Algebra is relevant here?,,"['linear-algebra', 'ordinary-differential-equations', 'dynamical-systems']"
13,Property of the conjugate transpose matrix with inner product,Property of the conjugate transpose matrix with inner product,,"I'm trying to prove that for a certain matrix $A$, and its conjugate transpose $A^*$, we have $⟨Ax,y⟩=⟨x,A^*y⟩$, where $⟨⟩$ represent the inner product. So here it's simply the dot product in $R^n$. Does anyone have an idea of how to prove this? Thank you!","I'm trying to prove that for a certain matrix $A$, and its conjugate transpose $A^*$, we have $⟨Ax,y⟩=⟨x,A^*y⟩$, where $⟨⟩$ represent the inner product. So here it's simply the dot product in $R^n$. Does anyone have an idea of how to prove this? Thank you!",,"['linear-algebra', 'matrices', 'inner-products', 'transpose']"
14,Planes through the origin are subspaces of $\Bbb{R}^3$,Planes through the origin are subspaces of,\Bbb{R}^3,"I'm reading the book Elementary Linear Algebra by Anton and Rorres, and the following has me a bit confused: ""If $\mathbf{u}$ and $\mathbf{v}$ are vectors in a plane $W$ through the origin of $\Bbb{R}^3$, then it is evident geometrically that $\mathbf{u + v}$ and $k\mathbf{u}$ also lie in the same plane $W$ for any scalar $k$ (Figure 4.2.3). Thus $W$ is closed under addition and scalar multiplication. It says that it is evident that $k\mathbf{u}$ also lies in the same plane $W$, but I feel like if $k$ is sufficiently large enough, the vector $k\mathbf{u}$ would extend outside of the vector space $W$. Can someone explain this to me a little more clearly please?","I'm reading the book Elementary Linear Algebra by Anton and Rorres, and the following has me a bit confused: ""If $\mathbf{u}$ and $\mathbf{v}$ are vectors in a plane $W$ through the origin of $\Bbb{R}^3$, then it is evident geometrically that $\mathbf{u + v}$ and $k\mathbf{u}$ also lie in the same plane $W$ for any scalar $k$ (Figure 4.2.3). Thus $W$ is closed under addition and scalar multiplication. It says that it is evident that $k\mathbf{u}$ also lies in the same plane $W$, but I feel like if $k$ is sufficiently large enough, the vector $k\mathbf{u}$ would extend outside of the vector space $W$. Can someone explain this to me a little more clearly please?",,"['linear-algebra', 'vector-spaces']"
15,Is there a relationship between isometry as defined on metric spaces and those on vector spaces?,Is there a relationship between isometry as defined on metric spaces and those on vector spaces?,,"I am taking a course on linear algebra and another on real analysis. In linear algebra we defined that two vector spaces are isomorphic if there existed a bijective and linear map between the two vector spaces In real analyis we defined that two metric spaces are isometric if there existed a bijective, distance preserving map $d(x,y) = d'(fx, fy)$   between the two metric spaces I looked up the definition of isometry online and many sources tell me it is a bijective ""structure preserving"" map. Is there some commonality between these so called structures? Or is it hopeless for me to guess what would be an isomorphism defined between topological spaces, Hilbert spaces or Banach spaces until I see the definitions?","I am taking a course on linear algebra and another on real analysis. In linear algebra we defined that two vector spaces are isomorphic if there existed a bijective and linear map between the two vector spaces In real analyis we defined that two metric spaces are isometric if there existed a bijective, distance preserving map $d(x,y) = d'(fx, fy)$   between the two metric spaces I looked up the definition of isometry online and many sources tell me it is a bijective ""structure preserving"" map. Is there some commonality between these so called structures? Or is it hopeless for me to guess what would be an isomorphism defined between topological spaces, Hilbert spaces or Banach spaces until I see the definitions?",,"['real-analysis', 'linear-algebra', 'category-theory', 'terminology', 'definition']"
16,Intuitive explanation of left- and right-inverse,Intuitive explanation of left- and right-inverse,,"I am reading about right-inverse and left-inverse matrices. According to theory if a matrix $A_{m\times n}(\mathbb{R})$ is full row rank, then it has a right-inverse. That is, $AC=I_{m}$. Similarly, if $A$ is full collumn rank, then it has a left-inverse. That is, $BA=I_{n}$. I have the following questions: Taking $AC=I_{m}\iff A^TAC=A^T \iff C=(A^TA)^{-1}A^T$ but this satisfies $CA=I$, contradiction. Similarly, taking $BA=I_{n}\iff BAA^T=A^T \iff B=A^T(AA^T)^{-1}$ but this satisfies $AB=I$, contradiction. How is that possible? Moreover, and most importantly what is the intuitive explanation of the left and right inverse? Is there any connection with the rows or collumns or any of the four foundamental subspaces of $A$? Thank you very match!","I am reading about right-inverse and left-inverse matrices. According to theory if a matrix $A_{m\times n}(\mathbb{R})$ is full row rank, then it has a right-inverse. That is, $AC=I_{m}$. Similarly, if $A$ is full collumn rank, then it has a left-inverse. That is, $BA=I_{n}$. I have the following questions: Taking $AC=I_{m}\iff A^TAC=A^T \iff C=(A^TA)^{-1}A^T$ but this satisfies $CA=I$, contradiction. Similarly, taking $BA=I_{n}\iff BAA^T=A^T \iff B=A^T(AA^T)^{-1}$ but this satisfies $AB=I$, contradiction. How is that possible? Moreover, and most importantly what is the intuitive explanation of the left and right inverse? Is there any connection with the rows or collumns or any of the four foundamental subspaces of $A$? Thank you very match!",,"['linear-algebra', 'inverse']"
17,Geometrical meaning of orientation on vector space,Geometrical meaning of orientation on vector space,,"Can any one explain, the geometrical meaning of orientation in a vector space?","Can any one explain, the geometrical meaning of orientation in a vector space?",,"['linear-algebra', 'vector-spaces', 'orientation']"
18,Inverse of circulant matrices,Inverse of circulant matrices,,"The following is an n x n circulant matrix where h is real but not equal to 1: $$ A=         \begin{bmatrix}         2&-h&0&0&...&...&0&-h\\         -h&2&-h&0&...&...&0&0\\         0&-h&2&-h&...&...&...&...\\         0&0&-h&2&...&...&...&...\\         ...&...&...&...&...&...&0&0\\         ...&...&...&...&...&...&-h&0\\         0&0&...&...&0&-h&2&-h\\         -h&0&...&...&0&0&-h&2         \end{bmatrix} $$ So my question is how can I find an explicit formula for the inverse of this matrix? So for $M = QDQ^T$, I found that $D = \mathrm{diag}[2(1-h*\cos(2\pi l/N))]$  from $0$ to $N-1$ somehow by reading some papers on unitary Van der Monde matrices and shift matrices. However, I don't fully understand how this works. Can someone please show me how to derive an explicit formula for the inverse of this matrix and explain why we are able to do so? Appreciate any help in advance.","The following is an n x n circulant matrix where h is real but not equal to 1: $$ A=         \begin{bmatrix}         2&-h&0&0&...&...&0&-h\\         -h&2&-h&0&...&...&0&0\\         0&-h&2&-h&...&...&...&...\\         0&0&-h&2&...&...&...&...\\         ...&...&...&...&...&...&0&0\\         ...&...&...&...&...&...&-h&0\\         0&0&...&...&0&-h&2&-h\\         -h&0&...&...&0&0&-h&2         \end{bmatrix} $$ So my question is how can I find an explicit formula for the inverse of this matrix? So for $M = QDQ^T$, I found that $D = \mathrm{diag}[2(1-h*\cos(2\pi l/N))]$  from $0$ to $N-1$ somehow by reading some papers on unitary Van der Monde matrices and shift matrices. However, I don't fully understand how this works. Can someone please show me how to derive an explicit formula for the inverse of this matrix and explain why we are able to do so? Appreciate any help in advance.",,"['linear-algebra', 'matrices', 'matrix-equations', 'diagonalization']"
19,Logical formula of definition of linearly dependent,Logical formula of definition of linearly dependent,,"A subset $S$ of a vector space $V$ is said to be linearly dependent if there exist a finite number of distinct vectors $x_1, \ldots , x_n$ in $S$ and scalars $a_1 , \ldots ,a_n$ not all zero, such that     $$ a_1 x_1 + \cdots + a_n x_n =0 $$ I want to translate this definition into logical formula and find the formula of linearly independence. Here is what I tried. $$ \exists [ x_1 ,\ldots , x_n \in S : x_i \not = x_j] \exists [a_k \in F : a_k \not = 0] : a_1 x_1+\cdots + a_n x_n = 0 $$ Now to obtain the definition of linearly independence, I deny the above. $$ \forall [x_1 ,\ldots , x_n \in S : x_i = x_j] \forall [a_k \in F : a_k = 0] : a_1 x_1+\cdots + a_n x_n \not =0 $$ This is obviously absurd. What is wrong and what is the exact translation?","A subset $S$ of a vector space $V$ is said to be linearly dependent if there exist a finite number of distinct vectors $x_1, \ldots , x_n$ in $S$ and scalars $a_1 , \ldots ,a_n$ not all zero, such that     $$ a_1 x_1 + \cdots + a_n x_n =0 $$ I want to translate this definition into logical formula and find the formula of linearly independence. Here is what I tried. $$ \exists [ x_1 ,\ldots , x_n \in S : x_i \not = x_j] \exists [a_k \in F : a_k \not = 0] : a_1 x_1+\cdots + a_n x_n = 0 $$ Now to obtain the definition of linearly independence, I deny the above. $$ \forall [x_1 ,\ldots , x_n \in S : x_i = x_j] \forall [a_k \in F : a_k = 0] : a_1 x_1+\cdots + a_n x_n \not =0 $$ This is obviously absurd. What is wrong and what is the exact translation?",,"['linear-algebra', 'predicate-logic']"
20,Determinant of Adjacency Matrix is null,Determinant of Adjacency Matrix is null,,"Let be $D=(V,E)$ . Prove the determinant of its adjacency matrix , $det(A) = 0$ $\iff$ $\exists S \subseteq V $ (nonempty) such that $|v_{ext} \cap S|$ is an even number , $\forall $ v $\in V$ . $v_{ext}$ denotes the nodes $u$ such that $vu \in E$. Also , D may have loops (i.e $\exists$ $A_{uu} =1$). All operations are made in $GF(2)$. I have tried the direct implication in this way : if $det(A) = 0$ , it means that the vectors of $A$ are not linearly independents. But what`s the next step ? Thanks!","Let be $D=(V,E)$ . Prove the determinant of its adjacency matrix , $det(A) = 0$ $\iff$ $\exists S \subseteq V $ (nonempty) such that $|v_{ext} \cap S|$ is an even number , $\forall $ v $\in V$ . $v_{ext}$ denotes the nodes $u$ such that $vu \in E$. Also , D may have loops (i.e $\exists$ $A_{uu} =1$). All operations are made in $GF(2)$. I have tried the direct implication in this way : if $det(A) = 0$ , it means that the vectors of $A$ are not linearly independents. But what`s the next step ? Thanks!",,"['graph-theory', 'matrices', 'linear-algebra']"
21,Why do we need an orthonormal basis to represent the adjoint of the operator?,Why do we need an orthonormal basis to represent the adjoint of the operator?,,"For any linear operator on a finite dimensional Inner Product Space, we can get orthonormal basis via Gram Schmidt Process. But what is the necessity of defining the adjoint of the operator using the orthonormal basis? Probably it helps with the computation. Why we happen to define like that?","For any linear operator on a finite dimensional Inner Product Space, we can get orthonormal basis via Gram Schmidt Process. But what is the necessity of defining the adjoint of the operator using the orthonormal basis? Probably it helps with the computation. Why we happen to define like that?",,"['linear-algebra', 'matrices', 'adjoint-operators']"
22,Norm of a Block Matrix,Norm of a Block Matrix,,"Let $X\in M_{m,n}(R)$ and $l=m+n$. Now consider the block matrix $$ Y=\left[ \begin{array}{cc} 0 &X \\ \ X^T &0   \end{array}\right] $$ where $Y\in M_l(R)$. I want to show that $||X||=||Y||$ where for any $A\in M_{m,n}(R)$, we define $||A||=max\{||Ax||: x\in R^n,||x||=1\}$ and $R^n$ has the standard Euclidean inner product. I have got upto $$ ||Y||^2=||Y^TY||=|| \left[ \begin{array}{cc} XX^T &0 \\ \ 0 &X^TX   \end{array}\right]|| $$ Thanks for any help.","Let $X\in M_{m,n}(R)$ and $l=m+n$. Now consider the block matrix $$ Y=\left[ \begin{array}{cc} 0 &X \\ \ X^T &0   \end{array}\right] $$ where $Y\in M_l(R)$. I want to show that $||X||=||Y||$ where for any $A\in M_{m,n}(R)$, we define $||A||=max\{||Ax||: x\in R^n,||x||=1\}$ and $R^n$ has the standard Euclidean inner product. I have got upto $$ ||Y||^2=||Y^TY||=|| \left[ \begin{array}{cc} XX^T &0 \\ \ 0 &X^TX   \end{array}\right]|| $$ Thanks for any help.",,['linear-algebra']
23,"If two matrices have the same trace and determinant, do their powers have the same trace?","If two matrices have the same trace and determinant, do their powers have the same trace?",,"Let $A,B$ be two $2 \times 2$ matrices over some finite field $\mathbb{F}_q$, such that they have the same trace and determinant. Does this imply that tr $A^k$ = tr $B^k$ for any integer $k$? I've checked the case $k=2$, which works and makes use of the fact that they have the same determinant as well as the same trace, but can't seem to generalise this to an induction proof. Do I need stronger hypotheses for this? Note that the answer is trivially true if the matrices are similar but I am more interested whether it holds even if they are not similar.","Let $A,B$ be two $2 \times 2$ matrices over some finite field $\mathbb{F}_q$, such that they have the same trace and determinant. Does this imply that tr $A^k$ = tr $B^k$ for any integer $k$? I've checked the case $k=2$, which works and makes use of the fact that they have the same determinant as well as the same trace, but can't seem to generalise this to an induction proof. Do I need stronger hypotheses for this? Note that the answer is trivially true if the matrices are similar but I am more interested whether it holds even if they are not similar.",,"['linear-algebra', 'matrices', 'trace']"
24,Proving the formula for finding the determinant of a square matrix.,Proving the formula for finding the determinant of a square matrix.,,"I have read the proof for finding the determinant of a $2 \times 2$ matrix.  It makes sense, since for a matrix \begin{pmatrix}         a & b \\         c & d  \\         \end{pmatrix} $(ad-bc)$ must be non-zero for the inverse of the matrix to exist. So it is logical that $(ad-bc)$ is the determinant. However when it comes to a $3 \times 3$ matrix, all the sources that I have read purely state that the determinant of a $3 \times 3$ matrix defined as a formula (omitted here, basically it's summing up the entry of a row/column * determinant of a $2 \times 2$ matrix). However, unlike the $2 \times 2$ matrix determinant formula, no proof is given. Similarly, the formula for the determinant of an $n \times n$ matrix is not given in my textbook. Unfortunately, I can't seem to find a proof that I could comprehend on the internet. It would be great if someone can give me a proof of the formula for finding the determinant of an $n \times n$ matrix.","I have read the proof for finding the determinant of a $2 \times 2$ matrix.  It makes sense, since for a matrix \begin{pmatrix}         a & b \\         c & d  \\         \end{pmatrix} $(ad-bc)$ must be non-zero for the inverse of the matrix to exist. So it is logical that $(ad-bc)$ is the determinant. However when it comes to a $3 \times 3$ matrix, all the sources that I have read purely state that the determinant of a $3 \times 3$ matrix defined as a formula (omitted here, basically it's summing up the entry of a row/column * determinant of a $2 \times 2$ matrix). However, unlike the $2 \times 2$ matrix determinant formula, no proof is given. Similarly, the formula for the determinant of an $n \times n$ matrix is not given in my textbook. Unfortunately, I can't seem to find a proof that I could comprehend on the internet. It would be great if someone can give me a proof of the formula for finding the determinant of an $n \times n$ matrix.",,"['linear-algebra', 'matrices']"
25,Eigenvalues of composition of functions,Eigenvalues of composition of functions,,"I am trying to do the following exercise: Let $V$ be a $K$-finite dimensional vector space and let $f,g \in Hom(V,V)$. Define $Spec(f)=\{\alpha \in K / \alpha \space \text{is an eigenvalue of f}\}$. Prove that $Spec(f \circ g)=Spec(g \circ f)$. Suppose one of the two linear transformations is an isomorphism, w.l.g., suppose $g$ is an isomorphism, then if $|f|_{\mathcal B}=A, |g|_{\mathcal B}=B$ for some basis $\mathcal B$ of $V$, then the characteristic polynomial of $AB$ is $$\mathcal X_{AB}=det(\alpha I_n - AB)$$$$=det(B(\alpha I_n - AB)B^{-1})$$$$=det(\alpha I_n - BA)=\mathcal X_{BA}$$ From here it follows $\alpha$ is an eigenvalue of $f \circ g$ if and only if it is an eigenvalue of $g \circ f$. I don't know what to do for the general case. Any help would be appreciated.","I am trying to do the following exercise: Let $V$ be a $K$-finite dimensional vector space and let $f,g \in Hom(V,V)$. Define $Spec(f)=\{\alpha \in K / \alpha \space \text{is an eigenvalue of f}\}$. Prove that $Spec(f \circ g)=Spec(g \circ f)$. Suppose one of the two linear transformations is an isomorphism, w.l.g., suppose $g$ is an isomorphism, then if $|f|_{\mathcal B}=A, |g|_{\mathcal B}=B$ for some basis $\mathcal B$ of $V$, then the characteristic polynomial of $AB$ is $$\mathcal X_{AB}=det(\alpha I_n - AB)$$$$=det(B(\alpha I_n - AB)B^{-1})$$$$=det(\alpha I_n - BA)=\mathcal X_{BA}$$ From here it follows $\alpha$ is an eigenvalue of $f \circ g$ if and only if it is an eigenvalue of $g \circ f$. I don't know what to do for the general case. Any help would be appreciated.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
26,To find jordan canonical form,To find jordan canonical form,,Which of the following matrices have Jordan canonical form of equal to the $3\times 3$ matrix $$ \begin{pmatrix} 0 & 1 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}$$ a)$ \begin{pmatrix} 0 & 0 & 1 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}$ b)$ \begin{pmatrix} 0 & 0 & 1 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{pmatrix}$ c)$ \begin{pmatrix} 0 & 1 & 1 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}$ d)$ \begin{pmatrix} 0 & 1 & 1 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{pmatrix}$ Here characteristic equation of the matrix is $x^3$.Hence the 3 eigenvalues of the matrix are zero. Do we want to find the eigenvalues of all the matrices in the options?Is there any other way?,Which of the following matrices have Jordan canonical form of equal to the $3\times 3$ matrix $$ \begin{pmatrix} 0 & 1 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}$$ a)$ \begin{pmatrix} 0 & 0 & 1 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}$ b)$ \begin{pmatrix} 0 & 0 & 1 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{pmatrix}$ c)$ \begin{pmatrix} 0 & 1 & 1 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}$ d)$ \begin{pmatrix} 0 & 1 & 1 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{pmatrix}$ Here characteristic equation of the matrix is $x^3$.Hence the 3 eigenvalues of the matrix are zero. Do we want to find the eigenvalues of all the matrices in the options?Is there any other way?,,['linear-algebra']
27,Symmetric matrix eigenvalues,Symmetric matrix eigenvalues,,"Let $A$ be an $n\times n$ matrix, with $A_{ij}=i+j$.  Find the eigenvalues of $A$.  A student that I tutored asked me this question, and beyond working out that there are 2 nonzero eigenvalues $a+\sqrt{b}$ and $a-\sqrt{b}$ and $0$ with multiplicity $n-2$, I'm at a bit of a loss.","Let $A$ be an $n\times n$ matrix, with $A_{ij}=i+j$.  Find the eigenvalues of $A$.  A student that I tutored asked me this question, and beyond working out that there are 2 nonzero eigenvalues $a+\sqrt{b}$ and $a-\sqrt{b}$ and $0$ with multiplicity $n-2$, I'm at a bit of a loss.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
28,$\dim C(AB)=\dim C(B)-\dim(\operatorname{Null}(A)\cap C(B))$,,\dim C(AB)=\dim C(B)-\dim(\operatorname{Null}(A)\cap C(B)),"Let $A \in M_{n \times m}\left(F\right)$ and $B\in M_{m \times p}\left(F\right)$ for a field $F$. Prove: $\dim C(AB)=\dim C(B)-\dim(\operatorname{Null}(A)\cap C(B))$, where $C(X)$ denotes the column span of a matrix $X$.","Let $A \in M_{n \times m}\left(F\right)$ and $B\in M_{m \times p}\left(F\right)$ for a field $F$. Prove: $\dim C(AB)=\dim C(B)-\dim(\operatorname{Null}(A)\cap C(B))$, where $C(X)$ denotes the column span of a matrix $X$.",,"['linear-algebra', 'matrices']"
29,Why is $\pi_r(L)$ a linear transformation into $\Lambda^r(V)$,Why is  a linear transformation into,\pi_r(L) \Lambda^r(V),"I'm reading Linear Algebra by Hoffman and Kunze, and there's a proof that just doesn't seem correct. He wants to prove that $\pi_r$ is a linear transformation from $M^r(V)$ into $\Lambda^r(V)$ . In the proof, he establishes that if $\tau$ is a permutation of $\{1,2,\ldots,r\}$ then $(\pi_rL)(\alpha_{\tau1},\ldots,\alpha_{\tau r})=(\text{sgn }\tau)(\pi_rL)(\alpha_1,\ldots,\alpha_r)$ . He then concludes that $\pi_rL$ is an alternating form. Now, if $L$ is an alternating form then $L_\tau=(\text{sgn }\tau)L$ . However, the converse isn't necessarily true, is it? Can someone please explain to me what I didn't understand? It's on page 170, if that would be of any help. Thanks in advance.","I'm reading Linear Algebra by Hoffman and Kunze, and there's a proof that just doesn't seem correct. He wants to prove that is a linear transformation from into . In the proof, he establishes that if is a permutation of then . He then concludes that is an alternating form. Now, if is an alternating form then . However, the converse isn't necessarily true, is it? Can someone please explain to me what I didn't understand? It's on page 170, if that would be of any help. Thanks in advance.","\pi_r M^r(V) \Lambda^r(V) \tau \{1,2,\ldots,r\} (\pi_rL)(\alpha_{\tau1},\ldots,\alpha_{\tau r})=(\text{sgn }\tau)(\pi_rL)(\alpha_1,\ldots,\alpha_r) \pi_rL L L_\tau=(\text{sgn }\tau)L","['linear-algebra', 'multilinear-algebra']"
30,Matrix multiplication by scalar is commutative,Matrix multiplication by scalar is commutative,,"Is matrix multiplication by scalar commutative, i.e. $(\alpha M)N=M(\alpha N)$? If so, can we prove it without induction?","Is matrix multiplication by scalar commutative, i.e. $(\alpha M)N=M(\alpha N)$? If so, can we prove it without induction?",,['linear-algebra']
31,Is it true that $ U \oplus W_1 = U \oplus W_2 \implies W_1 = W_2 $? [closed],Is it true that ? [closed], U \oplus W_1 = U \oplus W_2 \implies W_1 = W_2 ,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question Is it true that if $ U \oplus W_1 = U \oplus W_2 $ , then $ W_1 = W_2 $ ? I think that if $ U \oplus W_1 = U \oplus W_2 $ , then $u+w_1=u+w_2$ , so $W_1=W_2$ . But did I make any mistakes?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question Is it true that if , then ? I think that if , then , so . But did I make any mistakes?", U \oplus W_1 = U \oplus W_2   W_1 = W_2   U \oplus W_1 = U \oplus W_2  u+w_1=u+w_2 W_1=W_2,"['linear-algebra', 'direct-sum', 'vector-space-isomorphism']"
32,Solution of system of three variables,Solution of system of three variables,,"On solving  $$ 2x - 4y + z = 0 $$  $$ x + y - 4z = 0 $$  $$ x - y - z = 0 $$  I get  $$ y = 0.6 x $$ $$ z = 0.4 x$$  I thought that there was a rule of thumb that you need as many independent equations as the number of unknowns. So, why don't I get a unique solution in this case?","On solving  $$ 2x - 4y + z = 0 $$  $$ x + y - 4z = 0 $$  $$ x - y - z = 0 $$  I get  $$ y = 0.6 x $$ $$ z = 0.4 x$$  I thought that there was a rule of thumb that you need as many independent equations as the number of unknowns. So, why don't I get a unique solution in this case?",,['linear-algebra']
33,Proof of an equivalence with respect to self-adjoint operators,Proof of an equivalence with respect to self-adjoint operators,,"I am trying to solve this problem but I fail in the converse part... Show  that  the  product of two  self-adjoint operators is  self-adjoint if and  only    if the  two  operators  commute. ($\Rightarrow$) If we suppose that $T,U,V$ are self-adjoint operators such that $T=UV$ and $U^*=U,V^*=V,T^*=T$ we have by Theorem that: $$(UV)^*=V^*U^*=VU$$ but we know, by hypothesis, that: $$(UV)^*=T^*=T=UV$$.We already know that the adjoint operator is unique, so we hav that $UV=VU$ and we are done with this part. So my troubles are this the converse, can anyone help please.","I am trying to solve this problem but I fail in the converse part... Show  that  the  product of two  self-adjoint operators is  self-adjoint if and  only    if the  two  operators  commute. ($\Rightarrow$) If we suppose that $T,U,V$ are self-adjoint operators such that $T=UV$ and $U^*=U,V^*=V,T^*=T$ we have by Theorem that: $$(UV)^*=V^*U^*=VU$$ but we know, by hypothesis, that: $$(UV)^*=T^*=T=UV$$.We already know that the adjoint operator is unique, so we hav that $UV=VU$ and we are done with this part. So my troubles are this the converse, can anyone help please.",,['linear-algebra']
34,All the $k\times k$ minors determines the matrix?,All the  minors determines the matrix?,k\times k,"Suppose two $n\times n$ matrices $A$ and $B$ with the same $k\times k$ minors, that is, for each $1\leq i_1<\cdots<i_k\leq n$,  $$\det\left(\begin{matrix} a_{i_1i_1}&\cdots&a_{i_1i_k}\\ \vdots& \ddots& \vdots\\ a_{i_ki_1}&\cdots&a_{i_ki_k}\end{matrix}\right)=\det\left(\begin{matrix} b_{i_1i_1}&\cdots&b_{i_1i_k}\\ \vdots& \ddots& \vdots\\ b_{i_ki_1}&\cdots&b_{i_ki_k}\end{matrix}\right),$$ can we deduce that $A=B$? If $k=1$, it is obvious right. If $k=n\geq 2$, it is obvious wrong. However for other $k=2,\cdots,n-1$, I could not be affirmative... Another related question is that for any $$m=\left(n\atop k\right)$$ scalars $a_1,\cdots,a_m$, is there exists a $n\times n$ matrix $A$ such that the minors (in dictionary order) are $a_1,\cdots,a_m$.","Suppose two $n\times n$ matrices $A$ and $B$ with the same $k\times k$ minors, that is, for each $1\leq i_1<\cdots<i_k\leq n$,  $$\det\left(\begin{matrix} a_{i_1i_1}&\cdots&a_{i_1i_k}\\ \vdots& \ddots& \vdots\\ a_{i_ki_1}&\cdots&a_{i_ki_k}\end{matrix}\right)=\det\left(\begin{matrix} b_{i_1i_1}&\cdots&b_{i_1i_k}\\ \vdots& \ddots& \vdots\\ b_{i_ki_1}&\cdots&b_{i_ki_k}\end{matrix}\right),$$ can we deduce that $A=B$? If $k=1$, it is obvious right. If $k=n\geq 2$, it is obvious wrong. However for other $k=2,\cdots,n-1$, I could not be affirmative... Another related question is that for any $$m=\left(n\atop k\right)$$ scalars $a_1,\cdots,a_m$, is there exists a $n\times n$ matrix $A$ such that the minors (in dictionary order) are $a_1,\cdots,a_m$.",,"['linear-algebra', 'matrices']"
35,"If in an inner product space $\langle x,u\rangle=\langle x,v\rangle$ for all $x$, show that $u=v$.","If in an inner product space  for all , show that .","\langle x,u\rangle=\langle x,v\rangle x u=v","If in an inner product space $\langle x,u\rangle=\langle x,v\rangle$ for all $x$, show that $u=v$. This seems obvious to me, so how do I prove it? Proof by contradiction maybe? Any suggestions would be nice.","If in an inner product space $\langle x,u\rangle=\langle x,v\rangle$ for all $x$, show that $u=v$. This seems obvious to me, so how do I prove it? Proof by contradiction maybe? Any suggestions would be nice.",,['linear-algebra']
36,Finding the norm of this upper triangular matrix,Finding the norm of this upper triangular matrix,,"I have a matrix $A=\begin{pmatrix} a & b\\ 0 & a\end{pmatrix}\in M_2(\mathbb{C})$. Given that $|a|<1$ and $|b|\leq 1-|a|^2$, I am supposed to show that $\|A\|\leq 1$ (operator norm). I can't get a clear answer out of computing the operator norm as $\|A\|=\displaystyle \sup_{|x|=1}|Ax|$, and the same holds for trying to find eigenvalues of $A^*A$ (or the numerical range of $A^*A$. Since the matrix is not normal, I can't use the spectral theorem. At best, all my approaches do no better than just taking $\|A\|\leq \|aI\|+\left\|\begin{pmatrix} 0 & b\\ 0&0\end{pmatrix}\right\|\leq 1-|a|^2+a$. Is there anything else I can try here? I should say that this is part of a larger problem in Paulsen, which asks you to show that $\|A\|\leq 1$ iff $|b|\leq 1-|a|^2$. I have already proved that, for $f(z)=\displaystyle\frac{z-a}{1-\overline{a}z}$, and polynomials $p_n(z)=(z-a)\displaystyle\sum_{k=0}^n (\overline{a}z)^k$, which converge to $f$ in $A(\mathbb{D})$ that $p_n(A)=\begin{pmatrix} p_n(a)&bp'_n(a)\\ 0&p_n(a)\end{pmatrix}=\begin{pmatrix} 0&b\sum_{k=0}^n|a|^k\\0&0\end{pmatrix}$ converges in norm to $\begin{pmatrix} f(a)&bf'(a)\\0&f(a)\end{pmatrix}= \begin{pmatrix} 0&\frac{b}{1-|a|^2}\\0&0\end{pmatrix}$. However, this (plus Von Neumann's Inequality) is only sufficient to prove $\|A\|\leq 1\Rightarrow |b|\leq 1-|a|^2$. This is a homework problem, so I'd prefer to have just some guidance.","I have a matrix $A=\begin{pmatrix} a & b\\ 0 & a\end{pmatrix}\in M_2(\mathbb{C})$. Given that $|a|<1$ and $|b|\leq 1-|a|^2$, I am supposed to show that $\|A\|\leq 1$ (operator norm). I can't get a clear answer out of computing the operator norm as $\|A\|=\displaystyle \sup_{|x|=1}|Ax|$, and the same holds for trying to find eigenvalues of $A^*A$ (or the numerical range of $A^*A$. Since the matrix is not normal, I can't use the spectral theorem. At best, all my approaches do no better than just taking $\|A\|\leq \|aI\|+\left\|\begin{pmatrix} 0 & b\\ 0&0\end{pmatrix}\right\|\leq 1-|a|^2+a$. Is there anything else I can try here? I should say that this is part of a larger problem in Paulsen, which asks you to show that $\|A\|\leq 1$ iff $|b|\leq 1-|a|^2$. I have already proved that, for $f(z)=\displaystyle\frac{z-a}{1-\overline{a}z}$, and polynomials $p_n(z)=(z-a)\displaystyle\sum_{k=0}^n (\overline{a}z)^k$, which converge to $f$ in $A(\mathbb{D})$ that $p_n(A)=\begin{pmatrix} p_n(a)&bp'_n(a)\\ 0&p_n(a)\end{pmatrix}=\begin{pmatrix} 0&b\sum_{k=0}^n|a|^k\\0&0\end{pmatrix}$ converges in norm to $\begin{pmatrix} f(a)&bf'(a)\\0&f(a)\end{pmatrix}= \begin{pmatrix} 0&\frac{b}{1-|a|^2}\\0&0\end{pmatrix}$. However, this (plus Von Neumann's Inequality) is only sufficient to prove $\|A\|\leq 1\Rightarrow |b|\leq 1-|a|^2$. This is a homework problem, so I'd prefer to have just some guidance.",,"['linear-algebra', 'matrices', 'functional-analysis', 'operator-theory', 'operator-algebras']"
37,Which one is true? (CSIR),Which one is true? (CSIR),,"Let $a,b,c$ be a positive real number such that $b^2+c^2<a<1$. Let  $A=\begin{bmatrix} 1&b&c\\ b&a & 0\\ c & 0 & 1\end{bmatrix}$. Then (1) all eigen values of $A$ are positive (2) all eigenvalues of $A$ are negative (3) all eigenvalues of $A$ are either positive or negative (4) all eigenvalues of $A$ are nonreal complex number Since $A$ is symmetric, all eigen values are real. Hence option (4) is not true.","Let $a,b,c$ be a positive real number such that $b^2+c^2<a<1$. Let  $A=\begin{bmatrix} 1&b&c\\ b&a & 0\\ c & 0 & 1\end{bmatrix}$. Then (1) all eigen values of $A$ are positive (2) all eigenvalues of $A$ are negative (3) all eigenvalues of $A$ are either positive or negative (4) all eigenvalues of $A$ are nonreal complex number Since $A$ is symmetric, all eigen values are real. Hence option (4) is not true.",,['linear-algebra']
38,"An element of $SL(2,\mathbb{R})$",An element of,"SL(2,\mathbb{R})","An element $A$ of $SL(2,\mathbb{R})$ is called an elliptic element if $|\text{tr}(A)|<2$ . Find the relationship between an elliptic element of $SL(2,\mathbb{R})$ and rotation. As $|\text{tr}(A)|<2$ the characteristic equation of $A$ does not have real roots, so it has no real eigenvalue. But I am unable to go ahead.","An element of is called an elliptic element if . Find the relationship between an elliptic element of and rotation. As the characteristic equation of does not have real roots, so it has no real eigenvalue. But I am unable to go ahead.","A SL(2,\mathbb{R}) |\text{tr}(A)|<2 SL(2,\mathbb{R}) |\text{tr}(A)|<2 A","['linear-algebra', 'abstract-algebra', 'matrices']"
39,Unique least square solutions,Unique least square solutions,,"There is a theorem in my book that states: If $A$ is $m\times n$, then the equation $Ax = b$ has a unique least square solution for each $b$ in $\mathbb{R}^m$. But can we find a counter-example to this by providing a matrix $A$ and vector $b$ such that $A^TAx = A^Tb$ produces a general solution with a free variable?","There is a theorem in my book that states: If $A$ is $m\times n$, then the equation $Ax = b$ has a unique least square solution for each $b$ in $\mathbb{R}^m$. But can we find a counter-example to this by providing a matrix $A$ and vector $b$ such that $A^TAx = A^Tb$ produces a general solution with a free variable?",,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'least-squares']"
40,What is a bilinear form?,What is a bilinear form?,,"I'm a CS master student and I'm reading a paper that mentions the term ""bilinear form"". Actually the paper mentions ""bilinear regression model"". But I think in order to understand what a ""bilinear regression model"" is, I need to understand what does ""bilinear form"" mean. I checked the wiki page on ""bilinear form"" but couldn't understand. Can you please explain to me (in simpler ways) the idea behind 'bilinear form'? If you know what 'bilinear regression model' is, I would also be very thankful for an explanation as well :).","I'm a CS master student and I'm reading a paper that mentions the term ""bilinear form"". Actually the paper mentions ""bilinear regression model"". But I think in order to understand what a ""bilinear regression model"" is, I need to understand what does ""bilinear form"" mean. I checked the wiki page on ""bilinear form"" but couldn't understand. Can you please explain to me (in simpler ways) the idea behind 'bilinear form'? If you know what 'bilinear regression model' is, I would also be very thankful for an explanation as well :).",,"['linear-algebra', 'regression', 'bilinear-form']"
41,Sequences length for LFSR when polynomial is reducible,Sequences length for LFSR when polynomial is reducible,,"An LFSR with polynomial $1+x^4+x^5 = (1+x+x^2)(1+x+x^3)$ can generate several sequences, depending on the initial value. If I did not make any mistake enumerating them, the length of the sequences are 3, 7, and 21. For $1+x+x^4+x^6 = (1+x)(1+x+x^2)(1+x+x^3)$ , I get 1, 3, 3, 7, 7, 21, 21 For $1+x+x^6+x^8+x^9 = (1+x+x^2)(1+x+x^3)(1+x+x^4)$ , I get 3, 7, 15, 15, 15, 15, 21, 105, 105, 105, 105 My goal is to have an algorithm to compute those length without going through the enumeration of all sequences. I am a bit puzzled by the fact that in the last example $45=3\cdot 15$ is not on the list. A link to a paper or a book dealing with that would be a great help: a lot of books cover LFSRs but most of the time they cover only maximum length cases...","An LFSR with polynomial can generate several sequences, depending on the initial value. If I did not make any mistake enumerating them, the length of the sequences are 3, 7, and 21. For , I get 1, 3, 3, 7, 7, 21, 21 For , I get 3, 7, 15, 15, 15, 15, 21, 105, 105, 105, 105 My goal is to have an algorithm to compute those length without going through the enumeration of all sequences. I am a bit puzzled by the fact that in the last example is not on the list. A link to a paper or a book dealing with that would be a great help: a lot of books cover LFSRs but most of the time they cover only maximum length cases...",1+x^4+x^5 = (1+x+x^2)(1+x+x^3) 1+x+x^4+x^6 = (1+x)(1+x+x^2)(1+x+x^3) 1+x+x^6+x^8+x^9 = (1+x+x^2)(1+x+x^3)(1+x+x^4) 45=3\cdot 15,"['linear-algebra', 'finite-fields']"
42,What is physical interpretation of dot product? [duplicate],What is physical interpretation of dot product? [duplicate],,This question already has answers here : What does the dot product of two vectors represent? (12 answers) Closed 3 years ago . Consider two vectors $V_1$ and $V_2$ in $\mathbb{R}^3$. When we take their dot product we get a real number. How is that number related to the vectors? Is there any way we can visualize it?,This question already has answers here : What does the dot product of two vectors represent? (12 answers) Closed 3 years ago . Consider two vectors $V_1$ and $V_2$ in $\mathbb{R}^3$. When we take their dot product we get a real number. How is that number related to the vectors? Is there any way we can visualize it?,,"['linear-algebra', 'vector-spaces']"
43,$V = \operatorname{Im} T + \ker T $ then $ \operatorname{Im} T \cap \ker T = \{0\}$,then,V = \operatorname{Im} T + \ker T   \operatorname{Im} T \cap \ker T = \{0\},"Let $F$ be a field, let $V$ be a vector space with finite dimension over $F$ and let $T$ be a linear operator on $V$. Prove that: a) If $V = \operatorname{Im} T + \ker T $ then $\operatorname{Im} T \cap \ker T = \{0\}$; b) If $\operatorname{Im} T \cap \ker T = \{0\}$ then $V = \operatorname{Im} T \bigoplus \ker T $ I'm really stuck with this problem, some help to solve this please.","Let $F$ be a field, let $V$ be a vector space with finite dimension over $F$ and let $T$ be a linear operator on $V$. Prove that: a) If $V = \operatorname{Im} T + \ker T $ then $\operatorname{Im} T \cap \ker T = \{0\}$; b) If $\operatorname{Im} T \cap \ker T = \{0\}$ then $V = \operatorname{Im} T \bigoplus \ker T $ I'm really stuck with this problem, some help to solve this please.",,"['linear-algebra', 'transformation']"
44,"Can $A, B$ fail to commute if $e^A=e^B=e^{A+B}=id$?",Can  fail to commute if ?,"A, B e^A=e^B=e^{A+B}=id","Consider the real $n \times n$-matrices $A$ and $B$. Can $A, B$ fail to commute if $e^A=e^B=e^{A+B}=id$ ?","Consider the real $n \times n$-matrices $A$ and $B$. Can $A, B$ fail to commute if $e^A=e^B=e^{A+B}=id$ ?",,['linear-algebra']
45,How do you rearrange equations with dot products in them?,How do you rearrange equations with dot products in them?,,How can I go about rearranging an equation similar to this... $$\left(\pmatrix{-3\\0\\1}+ t \pmatrix{1\\4\\7} \right) \cdot n - a = 0$$ The issue I'm having is manipulating dot products,How can I go about rearranging an equation similar to this... $$\left(\pmatrix{-3\\0\\1}+ t \pmatrix{1\\4\\7} \right) \cdot n - a = 0$$ The issue I'm having is manipulating dot products,,"['linear-algebra', 'multivariable-calculus', 'vectors']"
46,Trace of the power matrix is null,Trace of the power matrix is null,,"Let $K$ be a field of characteristic $p \geq 0$ and let $M$ be a matrix $n \times n$ over $K$. If $p \nmid n$ and $Tr(M^i) = 0$ for all $i = 1,\dots,n$, how to prove that $M + Id_n$ is invertible? If $p=0$, we can write down a proof using the usual general formula to characteristic polynomial of a matrix (that is an application of Newton's identities).","Let $K$ be a field of characteristic $p \geq 0$ and let $M$ be a matrix $n \times n$ over $K$. If $p \nmid n$ and $Tr(M^i) = 0$ for all $i = 1,\dots,n$, how to prove that $M + Id_n$ is invertible? If $p=0$, we can write down a proof using the usual general formula to characteristic polynomial of a matrix (that is an application of Newton's identities).",,['linear-algebra']
47,Is evaluation homomorphism surjective?,Is evaluation homomorphism surjective?,,"Let $A^n$ be an affine space over $\mathbb{C}$ and let $\mathbb{C}[X_1,\cdots,X_n]$ be the polynomial ring of $n$ variables. Then $A^n\to (\mathbb{C}[X_1,\cdots,X_n])^*$ by evaluation homomorphism, where $(\mathbb{C}[X_1,\cdots,X_n])^*$ is the set of ring homomorphisms from $\mathbb{C}[X_1,\cdots,X_n]$ to $\mathbb{C}$. Is this homomorphism surjective?","Let $A^n$ be an affine space over $\mathbb{C}$ and let $\mathbb{C}[X_1,\cdots,X_n]$ be the polynomial ring of $n$ variables. Then $A^n\to (\mathbb{C}[X_1,\cdots,X_n])^*$ by evaluation homomorphism, where $(\mathbb{C}[X_1,\cdots,X_n])^*$ is the set of ring homomorphisms from $\mathbb{C}[X_1,\cdots,X_n]$ to $\mathbb{C}$. Is this homomorphism surjective?",,"['linear-algebra', 'abstract-algebra', 'algebraic-geometry', 'polynomials']"
48,Partial ordering of positive definite matrices,Partial ordering of positive definite matrices,,"Assume that $\mathbf{A}$, $\mathbf{B}$, and $\mathbf{A-B}$ are all positive definite Hermitian symmetric matrices of same dimensions. Prove that $\mathbf{B}^{-1}-\mathbf{A}^{-1}$ is positive definite.","Assume that $\mathbf{A}$, $\mathbf{B}$, and $\mathbf{A-B}$ are all positive definite Hermitian symmetric matrices of same dimensions. Prove that $\mathbf{B}^{-1}-\mathbf{A}^{-1}$ is positive definite.",,"['linear-algebra', 'matrices', 'order-theory', 'symmetric-matrices', 'positive-definite']"
49,Linearly independent functionals,Linearly independent functionals,,"Let $f_1,\ldots,f_n$ be linearly independent linear functionals on a vector space $X$ . Show that there exist $x_1,\ldots,x_n\in X,$ such that the $n\times n$ matrix $\big(f_i(x_j) \big)_{i,j=1}^n$ is non-singular.",Let be linearly independent linear functionals on a vector space . Show that there exist such that the matrix is non-singular.,"f_1,\ldots,f_n X x_1,\ldots,x_n\in X, n\times n \big(f_i(x_j) \big)_{i,j=1}^n","['linear-algebra', 'matrices', 'functional-analysis', 'vector-spaces', 'functional-calculus']"
50,Finding the Integer part of $\sum_{k=2} ^{9999}\frac{1}{\sqrt k}$,Finding the Integer part of,\sum_{k=2} ^{9999}\frac{1}{\sqrt k},"Question from Model Question Paper for B.Math/B.Stat: Page 28, Question 27 by Indian Statistical Institute Q. Assume the following inequalities for positive k: $$\frac{1}{2\sqrt{k+1}}< \sqrt{k+1}-\sqrt k<\frac{1}{2\sqrt k}$$ The Integer part of:  $$\sum_{k=2} ^{9999}\frac{1}{\sqrt k}$$ equals: A. $198$ B. $197$ C. $196$ D. $195$ My approach: $$S=\sum_{k=2} ^{9999}\frac{1}{\sqrt k}$$ $$S=\frac{1}{\sqrt 2}+\frac{1}{\sqrt 3}+ \cdots + \frac{1}{\sqrt{9999}}$$ $$S=2\left(\frac{1}{2\sqrt 2}+\frac{1}{2\sqrt 3}+ \cdots + \frac{1}{2\sqrt{9999}}\right)$$ $$S\approx 2\left((\sqrt 2- \sqrt 1) + (\sqrt3- \sqrt2)+\cdots +(\sqrt{9999}-\sqrt{9998})\right)$$ $$[S]\approx2(\sqrt{9999}-\sqrt1)$$ $$[S]\approx 2(98)$$ $$[S]=196$$ I know my method is not an efficient one. Also I don't know if the answer is correct. Any help will be appreciated.","Question from Model Question Paper for B.Math/B.Stat: Page 28, Question 27 by Indian Statistical Institute Q. Assume the following inequalities for positive k: $$\frac{1}{2\sqrt{k+1}}< \sqrt{k+1}-\sqrt k<\frac{1}{2\sqrt k}$$ The Integer part of:  $$\sum_{k=2} ^{9999}\frac{1}{\sqrt k}$$ equals: A. $198$ B. $197$ C. $196$ D. $195$ My approach: $$S=\sum_{k=2} ^{9999}\frac{1}{\sqrt k}$$ $$S=\frac{1}{\sqrt 2}+\frac{1}{\sqrt 3}+ \cdots + \frac{1}{\sqrt{9999}}$$ $$S=2\left(\frac{1}{2\sqrt 2}+\frac{1}{2\sqrt 3}+ \cdots + \frac{1}{2\sqrt{9999}}\right)$$ $$S\approx 2\left((\sqrt 2- \sqrt 1) + (\sqrt3- \sqrt2)+\cdots +(\sqrt{9999}-\sqrt{9998})\right)$$ $$[S]\approx2(\sqrt{9999}-\sqrt1)$$ $$[S]\approx 2(98)$$ $$[S]=196$$ I know my method is not an efficient one. Also I don't know if the answer is correct. Any help will be appreciated.",,"['real-analysis', 'linear-algebra', 'limits']"
51,Necessary condition for have same rank,Necessary condition for have same rank,,"Let $P,Q$ real $n\times n$ matrices  such that $P^2=P$ , $Q^2=Q$ and $I-P-Q$ is an invertible matrix. Prove that $P$ and $Q$ have the same rank. Some help with this please , happy year and thanks.","Let $P,Q$ real $n\times n$ matrices  such that $P^2=P$ , $Q^2=Q$ and $I-P-Q$ is an invertible matrix. Prove that $P$ and $Q$ have the same rank. Some help with this please , happy year and thanks.",,"['linear-algebra', 'algebra-precalculus', 'matrices', 'vector-spaces']"
52,Is there an invertible matrix that transposes?,Is there an invertible matrix that transposes?,,"Quick question: I was asked if there exists an invertible matrix $P$ over the complex numbers such that for any matrix $A$: $PAP^{-1} = A^{T}$ I don't know how to prove it, but I don't think this is true. I know every matrix is similair to its transpose, but it can't be the same matrix $P$ for all matrices...So my gut feeling tells me no, but how do I show it?","Quick question: I was asked if there exists an invertible matrix $P$ over the complex numbers such that for any matrix $A$: $PAP^{-1} = A^{T}$ I don't know how to prove it, but I don't think this is true. I know every matrix is similair to its transpose, but it can't be the same matrix $P$ for all matrices...So my gut feeling tells me no, but how do I show it?",,"['linear-algebra', 'matrices']"
53,Geometric understanding of the Cross Product,Geometric understanding of the Cross Product,,Say you have vectors $v$ and $w$. Let there cross product be denoted by $x$ so that: $$v \times w = x$$ According to Wikipedia: $$x_x = v_yw_z - v_zw_y$$ $$x_y = v_zw_x - v_xw_z$$ $$x_z = v_xw_y - v_yw_x$$ This is equivalent to saying: $$x_x = \left|\begin{matrix}v_y&v_z\\w_y&w_z\end{matrix}\right|$$ $$x_y = \left|\begin{matrix}v_z&v_x\\w_z&w_x\end{matrix}\right|$$ $$x_z = \left|\begin{matrix}v_x&v_y\\w_x&w_y\end{matrix}\right|$$ The determinant can be interpreted as the area spanned by the column vectors. Could you give me a geometric explanation of why the area of those above vectors give the coordinates of the cross product? Thanks! EDIT: An interpretation for a 2x2 matrix is fine too,Say you have vectors $v$ and $w$. Let there cross product be denoted by $x$ so that: $$v \times w = x$$ According to Wikipedia: $$x_x = v_yw_z - v_zw_y$$ $$x_y = v_zw_x - v_xw_z$$ $$x_z = v_xw_y - v_yw_x$$ This is equivalent to saying: $$x_x = \left|\begin{matrix}v_y&v_z\\w_y&w_z\end{matrix}\right|$$ $$x_y = \left|\begin{matrix}v_z&v_x\\w_z&w_x\end{matrix}\right|$$ $$x_z = \left|\begin{matrix}v_x&v_y\\w_x&w_y\end{matrix}\right|$$ The determinant can be interpreted as the area spanned by the column vectors. Could you give me a geometric explanation of why the area of those above vectors give the coordinates of the cross product? Thanks! EDIT: An interpretation for a 2x2 matrix is fine too,,"['linear-algebra', 'matrices', 'vector-spaces', 'determinant', 'cross-product']"
54,How to prove that $A$ and $B$ are simultaneously diagonalizable,How to prove that  and  are simultaneously diagonalizable,A B,"Question: Let $A,B$ be $n\times n$ complex matrices, and let $a$ and $b$ be complex numbers such that $$AB-BA=aA+bB.$$ Show that $A$ and $B$ are simultaneously diagonalizable. My Try: Case 1: If $a=b=0$ , then $AB=BA$ . This is true. [Editor's note: In this case, the result is well known.] Case 2: Without loss of generality, we let $a\neq 0$ , so we may assume $a=1$ (let $\dfrac{1}{a}B$ replace $B$ ). Let $C=AB-BA=A+bB$ , then $$CB-BC=C$$ and I can't continue. Thank you very much!","Question: Let be complex matrices, and let and be complex numbers such that Show that and are simultaneously diagonalizable. My Try: Case 1: If , then . This is true. [Editor's note: In this case, the result is well known.] Case 2: Without loss of generality, we let , so we may assume (let replace ). Let , then and I can't continue. Thank you very much!","A,B n\times n a b AB-BA=aA+bB. A B a=b=0 AB=BA a\neq 0 a=1 \dfrac{1}{a}B B C=AB-BA=A+bB CB-BC=C","['linear-algebra', 'matrices', 'diagonalization']"
55,Action of the linear group over a vector space,Action of the linear group over a vector space,,"What is a simple proof that the action of $GL_n(\mathbb{K})$ is transitive on  $K-{0_\mathbb{K}}$ ? I don't understand the one in my book... I have an idea but it does not work the thing out completely. Let $E$ be a finite-dimensional vector space, therefore isomorphic to $\mathbb{R}^n$. We want to prove that for any $x,y\in E-{0_E}$, there exists $f\in GL_n(\mathbb{K})$ such that $f(x)=y$. If all coordinates of $x$ are different from $0$ in a certain base $(e_1,...,e_n)$, then $x=\sum_{i=1}^{n}x_ie_i$, $y=\sum_{i=1}^{n}y_ie_i$, and $f$ such that$$\forall i\in [1,n], f(e_i)=\frac{y_i}{x_i}e_i$$ does the job : $$f(x)=f(\sum_{i=1}^{n}x_ie_i)=\sum_{i=1}^{n}x_if(e_i)=y$$ The matrix of $f$ in the said base is diagonal, with diagonal elements equal to $\frac {y_i}{x_i}$ therefore $f\in GL_n(\mathbb{K})$. Now what if some of the coordinates of $x$ are equal to zero ? I mean, is it obvious that there is a base in which the coordinates of $x$ are all different from $0$ provided $x$ is different from $\vec0$ ? Thanks in advance,","What is a simple proof that the action of $GL_n(\mathbb{K})$ is transitive on  $K-{0_\mathbb{K}}$ ? I don't understand the one in my book... I have an idea but it does not work the thing out completely. Let $E$ be a finite-dimensional vector space, therefore isomorphic to $\mathbb{R}^n$. We want to prove that for any $x,y\in E-{0_E}$, there exists $f\in GL_n(\mathbb{K})$ such that $f(x)=y$. If all coordinates of $x$ are different from $0$ in a certain base $(e_1,...,e_n)$, then $x=\sum_{i=1}^{n}x_ie_i$, $y=\sum_{i=1}^{n}y_ie_i$, and $f$ such that$$\forall i\in [1,n], f(e_i)=\frac{y_i}{x_i}e_i$$ does the job : $$f(x)=f(\sum_{i=1}^{n}x_ie_i)=\sum_{i=1}^{n}x_if(e_i)=y$$ The matrix of $f$ in the said base is diagonal, with diagonal elements equal to $\frac {y_i}{x_i}$ therefore $f\in GL_n(\mathbb{K})$. Now what if some of the coordinates of $x$ are equal to zero ? I mean, is it obvious that there is a base in which the coordinates of $x$ are all different from $0$ provided $x$ is different from $\vec0$ ? Thanks in advance,",,"['linear-algebra', 'group-theory']"
56,Condition number of a block matrix,Condition number of a block matrix,,"Let $\mbox{cond} (M) := \frac{\sigma_1 (M)}{\sigma_n (M)}$ be the condition number of matrix $M$. Is $$\mbox{cond} ([A,B]) \leq \mbox{cond}(A) + \mbox{cond}(B)$$ true? And is this true for $n \times m$ rectangular matrices? Let's  consider $3$ different cases: $n<m$ $n=m$ $n>m$","Let $\mbox{cond} (M) := \frac{\sigma_1 (M)}{\sigma_n (M)}$ be the condition number of matrix $M$. Is $$\mbox{cond} ([A,B]) \leq \mbox{cond}(A) + \mbox{cond}(B)$$ true? And is this true for $n \times m$ rectangular matrices? Let's  consider $3$ different cases: $n<m$ $n=m$ $n>m$",,"['linear-algebra', 'matrices', 'normed-spaces', 'svd', 'condition-number']"
57,Hyperplanes in finite and infinite dimension vector spaces.,Hyperplanes in finite and infinite dimension vector spaces.,,"I know that hyperplanes of a n-dimensional vector space are sub-spaces of dimension n-1, This is in finite dimension spaces. BUT what about infinite dimension spaces what are hyperplanes? are they the same?","I know that hyperplanes of a n-dimensional vector space are sub-spaces of dimension n-1, This is in finite dimension spaces. BUT what about infinite dimension spaces what are hyperplanes? are they the same?",,"['linear-algebra', 'vector-spaces']"
58,How to show that a given set is a vector space?,How to show that a given set is a vector space?,,"I am having some issues with this problem in my Linear Algebra textbook. The goal is to either show that the given set, $W$ , is a vector space, or to find a specific example to the contrary: \begin{Bmatrix} \begin{bmatrix} a\\  b\\  c\\  d \end{bmatrix} : \begin{matrix} 3a + b = c\\  a + b + 2c = 2d \end{matrix} \end{Bmatrix} I understand the basic properties of Vector Spaces - such as having to contain the zero vector, being closed under addition, and being closed under scalar multiplication. I have no problem proving when these sets are not vector spaces, for example if they do not contain the zero vector. This set appears to contain the zero vector (if you plug in $0$ for $a, b, c, d$ the equations are consistent). But I'm not quite sure how to prove that this set is a vector space, or how to prove that it is closed under addition and scalar multiplication. Thanks for your help.","I am having some issues with this problem in my Linear Algebra textbook. The goal is to either show that the given set, , is a vector space, or to find a specific example to the contrary: I understand the basic properties of Vector Spaces - such as having to contain the zero vector, being closed under addition, and being closed under scalar multiplication. I have no problem proving when these sets are not vector spaces, for example if they do not contain the zero vector. This set appears to contain the zero vector (if you plug in for the equations are consistent). But I'm not quite sure how to prove that this set is a vector space, or how to prove that it is closed under addition and scalar multiplication. Thanks for your help.","W \begin{Bmatrix}
\begin{bmatrix}
a\\ 
b\\ 
c\\ 
d
\end{bmatrix} :
\begin{matrix}
3a + b = c\\ 
a + b + 2c = 2d
\end{matrix}
\end{Bmatrix} 0 a, b, c, d","['linear-algebra', 'vector-spaces']"
59,Solutions to the Anticommutator Matrix Equation,Solutions to the Anticommutator Matrix Equation,,"I'm investigating matrices with regards to antisymmetric properties, and I'm curious if there is a general solution to $$[A,X]=B$$ in terms of a (constant) matrix $X$, where $[A,X]$ denotes for shorthand the anticommutator $AX+XA$ (note: not $AX-XA$) and $A$ and $B$ are given matrices. For simplicity, let $A$ and $B$ be real symmetric pos. def. matrices. Is there a reference for this if it's true, or is it possibly a deep result requiring much theory and/or more conditions?","I'm investigating matrices with regards to antisymmetric properties, and I'm curious if there is a general solution to $$[A,X]=B$$ in terms of a (constant) matrix $X$, where $[A,X]$ denotes for shorthand the anticommutator $AX+XA$ (note: not $AX-XA$) and $A$ and $B$ are given matrices. For simplicity, let $A$ and $B$ be real symmetric pos. def. matrices. Is there a reference for this if it's true, or is it possibly a deep result requiring much theory and/or more conditions?",,"['linear-algebra', 'matrices']"
60,A better proof for $\det(P) = \pm1$ if $P$ is an orthogonal matrix,A better proof for  if  is an orthogonal matrix,\det(P) = \pm1 P,"Looking for elementary proof for $\rm~det(\textbf{P})$ = $\pm 1$ if $P$ is an orthogonal matrix.  I prefer a proof without using determinant of transpose matrix. My First Proof, with $\det(\textbf{P}^{t}) = \det(\textbf{P})$ If $P$ is orthogonal matrix, $\textbf{P}^{t}=\textbf{P}^{-1}$ . So, $$\det(\textbf{P}^{t}\textbf{P})=\det(\textbf{I}) \implies \det(\textbf{P}^{t}\textbf{P}) = 1 \implies \det(\textbf{P}^{t}) \det(\textbf{P}) = 1$$ because $ \det(\textbf{P}^{t}) = \det(\textbf{P})$ . Therefore, $\det(\textbf{P}^{t})=\det(\textbf{P}) = \pm 1$ . My Second Proof,  without $\det(\textbf{P}^{t}) = \det(\textbf{P})$ Let $\lambda$ be an eigenvalue for orthogonal matrix $\textbf{P}$ . $$\textbf{P}\vec{v}=\lambda\vec{v},\quad\vec{v} \neq \vec{0}$$ $$\implies \rVert \textbf{P}\vec{v}\lVert = \lVert \lambda\vec{v} \lVert \implies \rVert \textbf{P}\vec{v} \lVert = \rvert \lambda \ \lvert \lVert \vec{v} \rVert = 1$$ because $\lVert \vec{v} \rVert = 1$ Therefore $\lvert \lambda \rvert = 1$ because $\textbf{P}$ is orthonormal matrix. $$\textbf{P} = \textbf{P}^{t}$$ Therefore $\textbf{P} = \textbf{U}^{t}\Lambda\textbf{U} $ $$\det(\textbf{U}^{t}\Lambda\textbf{U}) = \det(\textbf{U}^{t})\det(\Lambda)\det(\textbf{U})$$ $$\det(\textbf{U}^{t}\Lambda\textbf{U}) = \det(\textbf{U}^{t})\det(\textbf{U})\det(\Lambda)$$ $$\det(\textbf{U}^{t})\det(\textbf{U}) = 1 $$ (because $\textbf{U}$ is orthogonal matrix}) $$\det(\textbf{P}) = \det(\textbf{U}^{t}\Lambda\textbf{U}) = \det(\Lambda) = \prod_{i=1}^{n}\lambda_i = \pm 1 $$ $$\implies \det(\textbf{P}) = \pm 1$$","Looking for elementary proof for = if is an orthogonal matrix.  I prefer a proof without using determinant of transpose matrix. My First Proof, with If is orthogonal matrix, . So, because . Therefore, . My Second Proof,  without Let be an eigenvalue for orthogonal matrix . because Therefore because is orthonormal matrix. Therefore (because is orthogonal matrix})","\rm~det(\textbf{P}) \pm 1 P \det(\textbf{P}^{t}) = \det(\textbf{P}) P \textbf{P}^{t}=\textbf{P}^{-1} \det(\textbf{P}^{t}\textbf{P})=\det(\textbf{I}) \implies \det(\textbf{P}^{t}\textbf{P}) = 1 \implies \det(\textbf{P}^{t}) \det(\textbf{P}) = 1  \det(\textbf{P}^{t}) = \det(\textbf{P}) \det(\textbf{P}^{t})=\det(\textbf{P}) = \pm 1 \det(\textbf{P}^{t}) = \det(\textbf{P}) \lambda \textbf{P} \textbf{P}\vec{v}=\lambda\vec{v},\quad\vec{v} \neq \vec{0} \implies \rVert \textbf{P}\vec{v}\lVert = \lVert \lambda\vec{v} \lVert \implies \rVert \textbf{P}\vec{v} \lVert = \rvert \lambda \ \lvert \lVert \vec{v} \rVert = 1 \lVert \vec{v} \rVert = 1 \lvert \lambda \rvert = 1 \textbf{P} \textbf{P} = \textbf{P}^{t} \textbf{P} = \textbf{U}^{t}\Lambda\textbf{U}  \det(\textbf{U}^{t}\Lambda\textbf{U}) = \det(\textbf{U}^{t})\det(\Lambda)\det(\textbf{U}) \det(\textbf{U}^{t}\Lambda\textbf{U}) = \det(\textbf{U}^{t})\det(\textbf{U})\det(\Lambda) \det(\textbf{U}^{t})\det(\textbf{U}) = 1  \textbf{U} \det(\textbf{P}) = \det(\textbf{U}^{t}\Lambda\textbf{U}) = \det(\Lambda) = \prod_{i=1}^{n}\lambda_i = \pm 1  \implies \det(\textbf{P}) = \pm 1","['linear-algebra', 'matrices', 'determinant']"
61,Angle between two 3D lines,Angle between two 3D lines,,"I know  for given 2 vector $\vec{u},\vec{v}$ the angle between them achieved by -  $$\cos{\theta} = \frac{\vec{u} \cdot \vec{v}}{\|\mathbf{u}\|\|\mathbf{v}\|}$$ but what if I want to calculate the $\theta$ between two 3D line   ? For example given 2 lines which each of them represented by two 3D points  -  $$line1: (3,2,-5)\hspace{5 mm }, (1,1,1) \\ line2: (1,-4,6)\hspace{5 mm }, (1,1,1)$$ How should I caclculate the angle $\theta$ between those 2 lines ?","I know  for given 2 vector $\vec{u},\vec{v}$ the angle between them achieved by -  $$\cos{\theta} = \frac{\vec{u} \cdot \vec{v}}{\|\mathbf{u}\|\|\mathbf{v}\|}$$ but what if I want to calculate the $\theta$ between two 3D line   ? For example given 2 lines which each of them represented by two 3D points  -  $$line1: (3,2,-5)\hspace{5 mm }, (1,1,1) \\ line2: (1,-4,6)\hspace{5 mm }, (1,1,1)$$ How should I caclculate the angle $\theta$ between those 2 lines ?",,"['linear-algebra', '3d']"
62,"If $T$ is an operator on a complex inner-product space, each eigenvalue $|\lambda|=1$ and $\|Tv\|\le\|v\|$, show that $T$ is unitary.","If  is an operator on a complex inner-product space, each eigenvalue  and , show that  is unitary.",T |\lambda|=1 \|Tv\|\le\|v\| T,"If $T$ is an operator on a finite-dimensional complex inner product space, each eigenvalue $|\lambda|=1$ and $\|Tv\|\le\|v\|$, show that $T$ is unitary. Here's what I had in mind and where I was stuck: $$\|Tv\|\le\|v\| \to \langle v,(I-T^*T)v\rangle \ge0$$ Therefore $I-T^*T$ is self-adjoint hence there exists an orthonormal basis of eigenvectors of $I-T^*T$. I'm able to deduce that each one of its eigenvalues is real and satisfies $\lambda\ge 0$. A possible approach I thought I might take is calculate $\operatorname{trace}(I-T^*T)$ and maybe show that it's $0$. That would complete the proof since then we would get $\lambda=0$ for all of its eigenvalues which would imply $I-T^*T=0 \to T^*T=I$ and hence $T$ is unitary. However: $$\operatorname{trace}(I-T^*T)=\operatorname{trace}(I)-\operatorname{trace}(T^*T)=n -\operatorname{trace}(T^*T)$$ I don't know how to evaluate the second term. I realize that $T^*T$ is positive and all of its eigenvalues are nonnegative, but I don't know how to go on from here. Any help would be greatly appreciated (or otherwise, I'd love to see other ways to go about the problem).","If $T$ is an operator on a finite-dimensional complex inner product space, each eigenvalue $|\lambda|=1$ and $\|Tv\|\le\|v\|$, show that $T$ is unitary. Here's what I had in mind and where I was stuck: $$\|Tv\|\le\|v\| \to \langle v,(I-T^*T)v\rangle \ge0$$ Therefore $I-T^*T$ is self-adjoint hence there exists an orthonormal basis of eigenvectors of $I-T^*T$. I'm able to deduce that each one of its eigenvalues is real and satisfies $\lambda\ge 0$. A possible approach I thought I might take is calculate $\operatorname{trace}(I-T^*T)$ and maybe show that it's $0$. That would complete the proof since then we would get $\lambda=0$ for all of its eigenvalues which would imply $I-T^*T=0 \to T^*T=I$ and hence $T$ is unitary. However: $$\operatorname{trace}(I-T^*T)=\operatorname{trace}(I)-\operatorname{trace}(T^*T)=n -\operatorname{trace}(T^*T)$$ I don't know how to evaluate the second term. I realize that $T^*T$ is positive and all of its eigenvalues are nonnegative, but I don't know how to go on from here. Any help would be greatly appreciated (or otherwise, I'd love to see other ways to go about the problem).",,"['linear-algebra', 'operator-theory', 'inner-products']"
63,Proving an inequality in Eigenvalues of a matrix,Proving an inequality in Eigenvalues of a matrix,,"If $\lambda_1$, $\lambda_2$, $\lambda_3$ are the eigenvalues of the matrix : $$         \begin{pmatrix}         26 & -2 & 2 \\         2 & 21 & 4 \\         4 & 2 & 28 \\         \end{pmatrix} $$ Show that : $$\sqrt{\lambda_1^2 + \lambda_2^2 + \lambda_3^2} \le \sqrt{1949}$$ I found the characterstic equation as $$\lambda^3 - 75\lambda^2 + 1850\lambda - 15000 = 0$$ This gave $$\lambda_1 + \lambda_2 + \lambda_3 = 75$$ and $$\lambda_1\lambda_2 + \lambda_2\lambda_3 + \lambda_1\lambda_3 = 1850$$ Also $${(\lambda_1 + \lambda_2 + \lambda_3)}^2 = \lambda_1^2 + \lambda_2^2 + \lambda_3^2 + 2{(\lambda_1\lambda_2 + \lambda_2\lambda_3 + \lambda_1\lambda_3)}$$ From this I found out $$\sqrt{\lambda_1^2 + \lambda_2^2 + \lambda_3^2} = \sqrt{1925}$$ But am not satisfied with the way I have approached the problem. Does anyone know of an easier way of doing it?","If $\lambda_1$, $\lambda_2$, $\lambda_3$ are the eigenvalues of the matrix : $$         \begin{pmatrix}         26 & -2 & 2 \\         2 & 21 & 4 \\         4 & 2 & 28 \\         \end{pmatrix} $$ Show that : $$\sqrt{\lambda_1^2 + \lambda_2^2 + \lambda_3^2} \le \sqrt{1949}$$ I found the characterstic equation as $$\lambda^3 - 75\lambda^2 + 1850\lambda - 15000 = 0$$ This gave $$\lambda_1 + \lambda_2 + \lambda_3 = 75$$ and $$\lambda_1\lambda_2 + \lambda_2\lambda_3 + \lambda_1\lambda_3 = 1850$$ Also $${(\lambda_1 + \lambda_2 + \lambda_3)}^2 = \lambda_1^2 + \lambda_2^2 + \lambda_3^2 + 2{(\lambda_1\lambda_2 + \lambda_2\lambda_3 + \lambda_1\lambda_3)}$$ From this I found out $$\sqrt{\lambda_1^2 + \lambda_2^2 + \lambda_3^2} = \sqrt{1925}$$ But am not satisfied with the way I have approached the problem. Does anyone know of an easier way of doing it?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
64,"What ""is"" a matrix in the context of a vector space?","What ""is"" a matrix in the context of a vector space?",,"I'm familiar with the definition of a vector space $V$ over a field $F$ I'm also comfortable with the notion that a matrix ""represents"" a linear map from one vector space $V$ to another vector space $W$. The Wiki article on Vector Spaces says this: Matrices are a useful notion to encode linear maps. But this conflicts with what I think a matrix is. Is a matrix a ""thing"" or ""element"" that we can write down and multiply by other matricies/vectors? Or is it simply a ""representation"" of a linear map between spaces? Does it depend on the context? If it is a ""thing"" - then how do we define the multiplication of objects from the vector space with objects of a different kind that do not belong to the vector space? We can write a meaningful equation such as: $M \cdot \textbf{x} = \textbf{y}$ Where x and y are vectors, and M is a matrix. I know how to perform the multiplication, but it seems wrong to me that $M$ is of a different ""species"" (apologies - I'm not familiar enough with abstract algebra to know the correct classification - set/group/etc.) than the thing we are multiplying it with. Sorry if this is vague, I'm just hoping someone might be able to clear up this slight confusion. Thanks!","I'm familiar with the definition of a vector space $V$ over a field $F$ I'm also comfortable with the notion that a matrix ""represents"" a linear map from one vector space $V$ to another vector space $W$. The Wiki article on Vector Spaces says this: Matrices are a useful notion to encode linear maps. But this conflicts with what I think a matrix is. Is a matrix a ""thing"" or ""element"" that we can write down and multiply by other matricies/vectors? Or is it simply a ""representation"" of a linear map between spaces? Does it depend on the context? If it is a ""thing"" - then how do we define the multiplication of objects from the vector space with objects of a different kind that do not belong to the vector space? We can write a meaningful equation such as: $M \cdot \textbf{x} = \textbf{y}$ Where x and y are vectors, and M is a matrix. I know how to perform the multiplication, but it seems wrong to me that $M$ is of a different ""species"" (apologies - I'm not familiar enough with abstract algebra to know the correct classification - set/group/etc.) than the thing we are multiplying it with. Sorry if this is vague, I'm just hoping someone might be able to clear up this slight confusion. Thanks!",,"['linear-algebra', 'abstract-algebra', 'matrices', 'vector-spaces']"
65,Minpoly and Charpoly of block diagonal matrix,Minpoly and Charpoly of block diagonal matrix,,"I am currently struggling with an exercise where I have to treat a Block diagonal matrix (so it is a square matrix, where square block matrices are down the diagonal). Now I was wondering whether we can say something about the characteristic or minimal polynomial of the whole matrix, if we know the characteristic and minimal polynomial of the blocks? Anything you know could be helpful!","I am currently struggling with an exercise where I have to treat a Block diagonal matrix (so it is a square matrix, where square block matrices are down the diagonal). Now I was wondering whether we can say something about the characteristic or minimal polynomial of the whole matrix, if we know the characteristic and minimal polynomial of the blocks? Anything you know could be helpful!",,"['linear-algebra', 'matrices']"
66,Nilpotent matrices with same minimal polynomial and nullity,Nilpotent matrices with same minimal polynomial and nullity,,From Hoffman and Kunze. Let $N_1$ and $N_2$ be 6 X 6 nilpotent matrices over the field F. Suppose that   $N_1$ and $N_2$ have the same minimal polynomial and the same nullity. Prove that   $N_1$ and $N_2$ are similar. Show that this is not true for 7 X 7 nilpotent matrices. Right so nilpotent matrices have $N^k= 0$ for some k. Since these have the same nullity and minimal polynomial $N_1^k = N_2^k = 0$ from some k right? The same minimal polynomial implies the same characteristic values but thats not enough to say they have the same jordan form and are thus similar right? and how would I get started in proving this breaks for 7x7 nilpotent matrices? Thanks in advance for any assistance.,From Hoffman and Kunze. Let $N_1$ and $N_2$ be 6 X 6 nilpotent matrices over the field F. Suppose that   $N_1$ and $N_2$ have the same minimal polynomial and the same nullity. Prove that   $N_1$ and $N_2$ are similar. Show that this is not true for 7 X 7 nilpotent matrices. Right so nilpotent matrices have $N^k= 0$ for some k. Since these have the same nullity and minimal polynomial $N_1^k = N_2^k = 0$ from some k right? The same minimal polynomial implies the same characteristic values but thats not enough to say they have the same jordan form and are thus similar right? and how would I get started in proving this breaks for 7x7 nilpotent matrices? Thanks in advance for any assistance.,,"['linear-algebra', 'matrices']"
67,correcting a mistake in Spivak [duplicate],correcting a mistake in Spivak [duplicate],,"This question already has answers here : Question about Angle-Preserving Operators (3 answers) Closed 4 years ago . Spivak's Calculus on Manifolds asks the reader to prove this (problem 1-8, pp.4-5): If there is a basis $x_1, x_2, ..., x_n$ of $\mathbb{R}^n$ and numbers $\lambda_1, \lambda_2, ..., \lambda_n$ such that $T(x_i) = \lambda_i x_i$, $1 \leq i \leq n$, prove that $T$ is angle-preserving iff $\left| \lambda_i \right| = c, 1 \leq i \leq n$. Here ""angle-preserving"" means that the linear map $T$ satisfies $$\frac{ \langle x, y \rangle}{\|x\| \|y\|} = \frac{ \langle T(x), T(y) \rangle}{\|T(x)\| \|T(y)\|},$$ and that $T$ is injective. My first problem with this question is that the claim is false. Taking $n = 2$, $x_1 = (1, 0)$, $x_2 = (1,1)$, $T(x_1) = -x_1$, $T(x_2) = x_2$, and setting $x = x_1$, $y = x_1 + x_2$, the expression in the RHS above evaluates to $0$, while the expression in the LHS evaluates to $\frac{1}{\sqrt{2}}$. My second, bigger problem is that I'm not really understanding what's going on. An earlier part of the problem had me show that norm-preserving matrices are angle-preserving; this I'm not sure I get. Thus, I'm not sure what true ""version"" of this statement the author had in mind (was he trying to get a converse?) and I don't know what to do. Here's my guess: Looking at some transformations in $\mathbb{R}^2$ (just drawing them), it looks like some of them could be ""flip the sign of a basis vector"" IF it's an orthogonal basis. However, I can't seem to recover the usual ""rotation through an angle $\theta$"" transformation this way, so I'm not sure that requiring the basis be orthogonal makes the statement true. Also, I'm not sure how to take the inner product of vectors which aren't in standard coordinates. Or am I missing something here?","This question already has answers here : Question about Angle-Preserving Operators (3 answers) Closed 4 years ago . Spivak's Calculus on Manifolds asks the reader to prove this (problem 1-8, pp.4-5): If there is a basis $x_1, x_2, ..., x_n$ of $\mathbb{R}^n$ and numbers $\lambda_1, \lambda_2, ..., \lambda_n$ such that $T(x_i) = \lambda_i x_i$, $1 \leq i \leq n$, prove that $T$ is angle-preserving iff $\left| \lambda_i \right| = c, 1 \leq i \leq n$. Here ""angle-preserving"" means that the linear map $T$ satisfies $$\frac{ \langle x, y \rangle}{\|x\| \|y\|} = \frac{ \langle T(x), T(y) \rangle}{\|T(x)\| \|T(y)\|},$$ and that $T$ is injective. My first problem with this question is that the claim is false. Taking $n = 2$, $x_1 = (1, 0)$, $x_2 = (1,1)$, $T(x_1) = -x_1$, $T(x_2) = x_2$, and setting $x = x_1$, $y = x_1 + x_2$, the expression in the RHS above evaluates to $0$, while the expression in the LHS evaluates to $\frac{1}{\sqrt{2}}$. My second, bigger problem is that I'm not really understanding what's going on. An earlier part of the problem had me show that norm-preserving matrices are angle-preserving; this I'm not sure I get. Thus, I'm not sure what true ""version"" of this statement the author had in mind (was he trying to get a converse?) and I don't know what to do. Here's my guess: Looking at some transformations in $\mathbb{R}^2$ (just drawing them), it looks like some of them could be ""flip the sign of a basis vector"" IF it's an orthogonal basis. However, I can't seem to recover the usual ""rotation through an angle $\theta$"" transformation this way, so I'm not sure that requiring the basis be orthogonal makes the statement true. Also, I'm not sure how to take the inner product of vectors which aren't in standard coordinates. Or am I missing something here?",,"['linear-algebra', 'multivariable-calculus']"
68,Involution $\Rightarrow$ Hermitian & Unitary,Involution  Hermitian & Unitary,\Rightarrow,"I've been learning about unitary diagonalization and this question has been on my mind for a while, unanswered by the text I'm using. Would it be true to say that if a square matrix represents an involution, i.e. $A^2=I$ (or $A=A^{-1}$), then it is Hermitian ($A=A^*$) and Unitary ($A^{-1}=A^*$). The reverse implication is definitely true, but is it an equivalence? If so, then why? And if not, could it be true for real matrices? (Involution $\Rightarrow$ Symmetric & Orthogonal) Thanks for help!","I've been learning about unitary diagonalization and this question has been on my mind for a while, unanswered by the text I'm using. Would it be true to say that if a square matrix represents an involution, i.e. $A^2=I$ (or $A=A^{-1}$), then it is Hermitian ($A=A^*$) and Unitary ($A^{-1}=A^*$). The reverse implication is definitely true, but is it an equivalence? If so, then why? And if not, could it be true for real matrices? (Involution $\Rightarrow$ Symmetric & Orthogonal) Thanks for help!",,['linear-algebra']
69,Finding a vector orthogonal to a subspace,Finding a vector orthogonal to a subspace,,"Suppose you were given vectors $a_1,\dots,a_n \in \mathbb{R}^m$ then how would you compute some vector orthogonal to the given list of vectors? Note that you are allowed to return the zero vector only if the vectors span $\mathbb{R}^m$. I thought about it for a while and the best I could do was to form the matrix with these vectors as rows and pick a vector from the nullspace. Is there an easier/faster way? Perhaps something geometric like Gram-Schmidt could be possible. Thanks!","Suppose you were given vectors $a_1,\dots,a_n \in \mathbb{R}^m$ then how would you compute some vector orthogonal to the given list of vectors? Note that you are allowed to return the zero vector only if the vectors span $\mathbb{R}^m$. I thought about it for a while and the best I could do was to form the matrix with these vectors as rows and pick a vector from the nullspace. Is there an easier/faster way? Perhaps something geometric like Gram-Schmidt could be possible. Thanks!",,['linear-algebra']
70,let $A\in M_n\mathbb R$ .how prove these statements with following condition?,let  .how prove these statements with following condition?,A\in M_n\mathbb R,"Assume $A\in M_n(\mathbb{R})$, $A\neq 0$ such that: \begin{align*} A=(a_{ij}),\ 1\le i,j\le n,\\ a_{ik}a_{jk}=a_{kk}a_{ij},\ \forall\, i,j \end{align*} How to prove that: $\operatorname{trace}(A)\neq0$, $A$ is symmetric, $x^{n-1}(x-\operatorname{trace}(A))$ is the characteristics polynomial of $A$. Thanks in advance.","Assume $A\in M_n(\mathbb{R})$, $A\neq 0$ such that: \begin{align*} A=(a_{ij}),\ 1\le i,j\le n,\\ a_{ik}a_{jk}=a_{kk}a_{ij},\ \forall\, i,j \end{align*} How to prove that: $\operatorname{trace}(A)\neq0$, $A$ is symmetric, $x^{n-1}(x-\operatorname{trace}(A))$ is the characteristics polynomial of $A$. Thanks in advance.",,"['linear-algebra', 'matrices', 'contest-math']"
71,A question about a method that shows $\mathbb{R} $ not finite dimensional.,A question about a method that shows  not finite dimensional.,\mathbb{R} ,"Upon looking at methods that show $\mathbb{R}$ is not finite dimensional over $\mathbb{Q}$ I came across a method mentioned here by the user Bill Dubuque, he took a set of vectors of the form $\log(p)$ where $p$ is prime and showed that the set is independent, but in his proof he only takes $n$-primes. So my questions are: The set of these logarithms is infinite , why did he only use $n$ primes? Why does this show that $\mathbb{R}$ is not finite dimensional? For the second question I'm not sure but I think it is because no matter what $n$ is the set is independent, but I'm not sure this can be extended to $n= \infty$.","Upon looking at methods that show $\mathbb{R}$ is not finite dimensional over $\mathbb{Q}$ I came across a method mentioned here by the user Bill Dubuque, he took a set of vectors of the form $\log(p)$ where $p$ is prime and showed that the set is independent, but in his proof he only takes $n$-primes. So my questions are: The set of these logarithms is infinite , why did he only use $n$ primes? Why does this show that $\mathbb{R}$ is not finite dimensional? For the second question I'm not sure but I think it is because no matter what $n$ is the set is independent, but I'm not sure this can be extended to $n= \infty$.",,['linear-algebra']
72,Numerical Linear Algebra,Numerical Linear Algebra,,If $X$ is an appropriate inverse of the nonsingular matrix $A\in \mathbb C^{n\times n}$ then two different measures of the quality of $X$ are $\|AX - I\|$ and $\|XA - I\|$. What is the largest factor by which these two quantities can differ ? Thanks,If $X$ is an appropriate inverse of the nonsingular matrix $A\in \mathbb C^{n\times n}$ then two different measures of the quality of $X$ are $\|AX - I\|$ and $\|XA - I\|$. What is the largest factor by which these two quantities can differ ? Thanks,,['linear-algebra']
73,Uncountability of basis of $\mathbb R^{\mathbb N}$,Uncountability of basis of,\mathbb R^{\mathbb N},Given vector space $V$ over $\mathbb R$ such that the elements of $V$ are infinite-tuples. How to show that any basis of it is uncountable?,Given vector space $V$ over $\mathbb R$ such that the elements of $V$ are infinite-tuples. How to show that any basis of it is uncountable?,,"['linear-algebra', 'hamel-basis']"
74,Resultant of two polynomials,Resultant of two polynomials,,"Let $Res(f,g)$ be a resultant of two polynomials $f(x)$ and $g(x)$. Is it true that resultant does not change under a linear change of coordinates $x\mapsto x+\alpha$? Thanks a lot!","Let $Res(f,g)$ be a resultant of two polynomials $f(x)$ and $g(x)$. Is it true that resultant does not change under a linear change of coordinates $x\mapsto x+\alpha$? Thanks a lot!",,['linear-algebra']
75,What is an application of the dual space?,What is an application of the dual space?,,"Does somebody know any application of the dual space in physics, chemistry, biology, computer science or economics? (I would like to add that to the german wikipedia article about the dual space.)","Does somebody know any application of the dual space in physics, chemistry, biology, computer science or economics? (I would like to add that to the german wikipedia article about the dual space.)",,"['linear-algebra', 'applications']"
76,Showing that a set of trigonometric functions is linearly independent over $\mathbb{R}$,Showing that a set of trigonometric functions is linearly independent over,\mathbb{R},"I would like to determine under what conditions on $k$ the set $$ \begin{align} A = &\{1,\cos(t),\sin(t), \\ &\quad \cos(t(1+k)),\sin(t(1+k)),\cos(t(1−k)),\sin(t(1−k)), \\ &\quad \cos(t(1+2k)),\sin(t(1+2k)),\cos(t(1−2k)),\sin(t(1−2k))\}, \end{align}$$ is linearly independent, where $k$ is some arbitrary real number. As motivation, I know that the set defined by $$ \{1, \cos wt, \sin wt\}, \quad w = 1, \dots, n $$ is linearly independent on $\mathbb{R}$, which one generally proves by computing the Wronskian.  I thought that I could extend this result to the set in question, but I haven't found a proper way to do so.  My intuition tells me that $A$ will be linearly dependent when the arguments of the trig functions coincide, which will depend on the value of $k$. Though, I'm at a loss for proving this is true.  Computing the Wronskian for this set required an inordinate amount of time-- I stopped running the calculation after a day.  Is there perhaps a way to reduce the set in question so that the Wronskian becomes manageable? I'm interested in any suggestions/alternative methods for proving linear independence that could help my situation.  Note that I'd like to have a result that holds for any $m = 0, \dots, n,$ where $n \in \mathbb{Z}$ if possible. Thanks for your time. EDIT :  The set originally defined in the first instance of this post was incorrectly cited.  My sincere apologies.","I would like to determine under what conditions on $k$ the set $$ \begin{align} A = &\{1,\cos(t),\sin(t), \\ &\quad \cos(t(1+k)),\sin(t(1+k)),\cos(t(1−k)),\sin(t(1−k)), \\ &\quad \cos(t(1+2k)),\sin(t(1+2k)),\cos(t(1−2k)),\sin(t(1−2k))\}, \end{align}$$ is linearly independent, where $k$ is some arbitrary real number. As motivation, I know that the set defined by $$ \{1, \cos wt, \sin wt\}, \quad w = 1, \dots, n $$ is linearly independent on $\mathbb{R}$, which one generally proves by computing the Wronskian.  I thought that I could extend this result to the set in question, but I haven't found a proper way to do so.  My intuition tells me that $A$ will be linearly dependent when the arguments of the trig functions coincide, which will depend on the value of $k$. Though, I'm at a loss for proving this is true.  Computing the Wronskian for this set required an inordinate amount of time-- I stopped running the calculation after a day.  Is there perhaps a way to reduce the set in question so that the Wronskian becomes manageable? I'm interested in any suggestions/alternative methods for proving linear independence that could help my situation.  Note that I'd like to have a result that holds for any $m = 0, \dots, n,$ where $n \in \mathbb{Z}$ if possible. Thanks for your time. EDIT :  The set originally defined in the first instance of this post was incorrectly cited.  My sincere apologies.",,"['linear-algebra', 'vector-spaces']"
77,The Duality Functor in Linear Algebra,The Duality Functor in Linear Algebra,,"I'm trying to gain an intuitive understanding of the following construction: For any vector space $M$ over a field $R$, one can define the algebraic dual of $M$ as $M^* := \mathsf{Hom}(M, R)$ and given another vector space $N$ one can define the algebraic dual of a linear map $A \in \mathsf{Hom}(M,N)$ according to $A^*(\omega) = \omega \circ A$. This establishes a linear mapping from $N^*$ to $M^*$ and since for suitable linear maps $A$ and $B$, $(A \circ B)^* = B^* \circ A^*$ and $(1_A)^* = 1_{A^*}$ the ""duality"" operation $*$ sets up an endomorphic (contravariant) functor on the category of vector spaces. I understand the basics of this construction, but what I would like to see are some good concrete examples that would illustrate the abstractions in a meaningful way. It's easy to come up with specific examples of linear maps and dual spaces where one applies this to specific linear forms, but are there some examples that would shed light on the motivation for these definitions and relations? Ironically, I think I have a better understanding of this in purely categorical terms since the duality construct shows up repeatedly in different contexts and when I think of ""duality"" I think of precisely this construct...","I'm trying to gain an intuitive understanding of the following construction: For any vector space $M$ over a field $R$, one can define the algebraic dual of $M$ as $M^* := \mathsf{Hom}(M, R)$ and given another vector space $N$ one can define the algebraic dual of a linear map $A \in \mathsf{Hom}(M,N)$ according to $A^*(\omega) = \omega \circ A$. This establishes a linear mapping from $N^*$ to $M^*$ and since for suitable linear maps $A$ and $B$, $(A \circ B)^* = B^* \circ A^*$ and $(1_A)^* = 1_{A^*}$ the ""duality"" operation $*$ sets up an endomorphic (contravariant) functor on the category of vector spaces. I understand the basics of this construction, but what I would like to see are some good concrete examples that would illustrate the abstractions in a meaningful way. It's easy to come up with specific examples of linear maps and dual spaces where one applies this to specific linear forms, but are there some examples that would shed light on the motivation for these definitions and relations? Ironically, I think I have a better understanding of this in purely categorical terms since the duality construct shows up repeatedly in different contexts and when I think of ""duality"" I think of precisely this construct...",,"['linear-algebra', 'category-theory', 'examples-counterexamples', 'intuition']"
78,"Prove that if $A$ is Hermitian and $A^m=I$, then $A^2=I$ (and $A=I$ if $m$ is odd)","Prove that if  is Hermitian and , then  (and  if  is odd)",A A^m=I A^2=I A=I m,"Let $A$ be a Hermitian $n\times n$ matrix over ${\Bbb C}$ such that $A^m = I$ for some natural number $m$ . Prove: $A^2=I$ . If $m$ is odd, then $A=I$ Well, for the first question I did this: Since $A$ is Hermitian then $A$ is diagonalizable. $A$ 's eigenvalues are $\{-1,1\}$ .  The minimal polynomial that reset $A$ and $$M(x) = (x+1)^z (x-1)^k $$ for $k,z$ natural numbers. So the options for the minimal polynomial are: $$(x-1)$$ $$(x+1)$$ $$(x-1)(x+1)$$ $$(x-1)^2 $$ Eventually I got $A$ can be: $$ A=I   \quad \mathrm{or}\quad    A=-I $$ Is that even right? For the second part of the question I have no clue.","Let be a Hermitian matrix over such that for some natural number . Prove: . If is odd, then Well, for the first question I did this: Since is Hermitian then is diagonalizable. 's eigenvalues are .  The minimal polynomial that reset and for natural numbers. So the options for the minimal polynomial are: Eventually I got can be: Is that even right? For the second part of the question I have no clue.","A n\times n {\Bbb C} A^m = I m A^2=I m A=I A A A \{-1,1\} A M(x) = (x+1)^z (x-1)^k  k,z (x-1) (x+1) (x-1)(x+1) (x-1)^2  A  A=I   \quad \mathrm{or}\quad    A=-I ","['linear-algebra', 'matrices', 'nilpotence']"
79,Linear Algebra Journal,Linear Algebra Journal,,Is there any journal which has significant material on the teaching of linear algebra. I am investigating the most effective way to teach a course on Linear Algebra. What are the most important things students should learn on a first course? What are the main questions in this field?,Is there any journal which has significant material on the teaching of linear algebra. I am investigating the most effective way to teach a course on Linear Algebra. What are the most important things students should learn on a first course? What are the main questions in this field?,,"['linear-algebra', 'reference-request', 'education', 'online-resources']"
80,uniform continuity of linear functions,uniform continuity of linear functions,,"I've just proved the fact that every linear function on a finite dimensional normed vector space is uniformly continuous. Because, let $T:U\to V$ be a linear function on $U$ with basis : $(u_1,u_2,\dots,u_n)$ and suppose that $\|\cdot\|_u$, $\|\cdot\|_v$ be the respective norms associated with $U$ and $V$. Setting $M= \max(\|Tu_1\|_v,\dots,\|Tu_n\|_v)$, we have $$\|Tu\|_v=\|a_1Tu_1 + a_2Tu_2 +\dots+ a_nTu_n\|_v \le |a_1|\cdot\|Tu_1\|_v+\dots+|a_n|\cdot\|Tu_n\|_v \le M\|u\|_1,$$ where $\|u\|_1=|a_1|+\dots+|a_n|$. Since all norms on $U$ are equivalent, we have $\|Tu\|_v\le C\|u\|_u$ for some $C>0$. Thus, $\|Tx-Ty\| \le C\|x-y\|$ for all $x$, $y$ in $U$, thereby satisfying Lipschitz' condition. My questions are: Is it true that a linear function from one vector space to another is always continuous? (finite dimension not assumed) If so, or else, is a continuous linear map always uniformly continuous?","I've just proved the fact that every linear function on a finite dimensional normed vector space is uniformly continuous. Because, let $T:U\to V$ be a linear function on $U$ with basis : $(u_1,u_2,\dots,u_n)$ and suppose that $\|\cdot\|_u$, $\|\cdot\|_v$ be the respective norms associated with $U$ and $V$. Setting $M= \max(\|Tu_1\|_v,\dots,\|Tu_n\|_v)$, we have $$\|Tu\|_v=\|a_1Tu_1 + a_2Tu_2 +\dots+ a_nTu_n\|_v \le |a_1|\cdot\|Tu_1\|_v+\dots+|a_n|\cdot\|Tu_n\|_v \le M\|u\|_1,$$ where $\|u\|_1=|a_1|+\dots+|a_n|$. Since all norms on $U$ are equivalent, we have $\|Tu\|_v\le C\|u\|_u$ for some $C>0$. Thus, $\|Tx-Ty\| \le C\|x-y\|$ for all $x$, $y$ in $U$, thereby satisfying Lipschitz' condition. My questions are: Is it true that a linear function from one vector space to another is always continuous? (finite dimension not assumed) If so, or else, is a continuous linear map always uniformly continuous?",,"['linear-algebra', 'functional-analysis']"
81,Open Dense Subset of $M_n(\mathbb{R})$,Open Dense Subset of,M_n(\mathbb{R}),"Well, I know the fact that $GL_n(\mathbb{R})$ is open set in $M_n(\mathbb{R})$, how to show that it is dense also? Well I thought like this: If $A\in M_n(\mathbb{R})$ and If $\lambda_1,\dots,\lambda_n$ are the eigenvalues of $A$ and if I take $\lambda<\min\{|\lambda_1|,\dots,|\lambda_n|\}$ then the sequence $$B_n= A + \frac{\lambda}{n}I \rightarrow A$$ Where each $A + \frac{\lambda}{n}I$ is invertible matrix. Is my approach is correct? Is the set of all diagonalizable matrices also dense?","Well, I know the fact that $GL_n(\mathbb{R})$ is open set in $M_n(\mathbb{R})$, how to show that it is dense also? Well I thought like this: If $A\in M_n(\mathbb{R})$ and If $\lambda_1,\dots,\lambda_n$ are the eigenvalues of $A$ and if I take $\lambda<\min\{|\lambda_1|,\dots,|\lambda_n|\}$ then the sequence $$B_n= A + \frac{\lambda}{n}I \rightarrow A$$ Where each $A + \frac{\lambda}{n}I$ is invertible matrix. Is my approach is correct? Is the set of all diagonalizable matrices also dense?",,"['linear-algebra', 'real-analysis', 'general-topology', 'matrices', 'metric-spaces']"
82,Proving that an $n\times n$ matrix has at most $n$ distinct eigenvalues,Proving that an  matrix has at most  distinct eigenvalues,n\times n n,$A$ is a $n\times n$ matrix over the field $F$. How can I prove that there are at most $n$ distinct scalars $c$ in $F$ such that $\det(cI - A) = 0$? Thank you!,$A$ is a $n\times n$ matrix over the field $F$. How can I prove that there are at most $n$ distinct scalars $c$ in $F$ such that $\det(cI - A) = 0$? Thank you!,,"['linear-algebra', 'eigenvalues-eigenvectors', 'determinant']"
83,What is the relation of basis in linear algebra and basis in topology?,What is the relation of basis in linear algebra and basis in topology?,,"In linear algebra and topology ,it all has the concept basis,but I can not construct the relation of them,could you explain the relation of two basis,such as the basis in linear algebra is special case of basis in topology,and some details about it,thanks a lot.","In linear algebra and topology ,it all has the concept basis,but I can not construct the relation of them,could you explain the relation of two basis,such as the basis in linear algebra is special case of basis in topology,and some details about it,thanks a lot.",,"['linear-algebra', 'general-topology']"
84,Derivation of the Riccati Differential Equation,Derivation of the Riccati Differential Equation,,"I am attempting to derive the Riccati Equation for linear-quadratic control. The original equation is: $-\partial V/\partial t = \min_{u(t)} \{x^TQx + u^TRu + \partial V^T/\partial x(Ax + Bu) \}$ $x \in \Re^n$, $u \in \Re^m$, $Q \in \Re^{n\times n}$, $R \in \Re^{m\times m}$, $A \in \Re^{n\times n}$, $B \in \Re^{n\times m}$. It can be shown that the minimal $u$ is $u^*=-\frac{1}{2}R^{-1}B^T\partial V/\partial x$; also, $V(x,t)$ can be shown to be quadratic in $x$, so it is of the form $V(x(t),t) = x(t)^{T}P(t)x(t)$, so $\partial V/\partial x = 2P(t)x(t)$. Thus $u^*(t) = -R^{-1}B^TP(t)x(t)$. We'd like to solve for $P$, which is symmetrical. Plugging into the original equation, I obtain $-\partial V/\partial t = -x^T\dot{P}x \equiv x^TQx + (-R^{-1}B^TPx)^TR(-R^{-1}B^TPx)+2x^TP(Ax+B[-R^{-1}B^TPx])$ Somehow this gets reduced to $-x^T\dot{P}x = x^T\{A^TP + PA + Q - PBR^{-1}B^TP\}x$ I cannot figure out the manipulation to get to the final equation. In particular, how is there both an $A^TP$ and $PA$ term in the final expression, when I distribute $2x^TP$ into $Ax+Bu^*$? Any insight would be helpful. Thanks.","I am attempting to derive the Riccati Equation for linear-quadratic control. The original equation is: $-\partial V/\partial t = \min_{u(t)} \{x^TQx + u^TRu + \partial V^T/\partial x(Ax + Bu) \}$ $x \in \Re^n$, $u \in \Re^m$, $Q \in \Re^{n\times n}$, $R \in \Re^{m\times m}$, $A \in \Re^{n\times n}$, $B \in \Re^{n\times m}$. It can be shown that the minimal $u$ is $u^*=-\frac{1}{2}R^{-1}B^T\partial V/\partial x$; also, $V(x,t)$ can be shown to be quadratic in $x$, so it is of the form $V(x(t),t) = x(t)^{T}P(t)x(t)$, so $\partial V/\partial x = 2P(t)x(t)$. Thus $u^*(t) = -R^{-1}B^TP(t)x(t)$. We'd like to solve for $P$, which is symmetrical. Plugging into the original equation, I obtain $-\partial V/\partial t = -x^T\dot{P}x \equiv x^TQx + (-R^{-1}B^TPx)^TR(-R^{-1}B^TPx)+2x^TP(Ax+B[-R^{-1}B^TPx])$ Somehow this gets reduced to $-x^T\dot{P}x = x^T\{A^TP + PA + Q - PBR^{-1}B^TP\}x$ I cannot figure out the manipulation to get to the final equation. In particular, how is there both an $A^TP$ and $PA$ term in the final expression, when I distribute $2x^TP$ into $Ax+Bu^*$? Any insight would be helpful. Thanks.",,"['linear-algebra', 'ordinary-differential-equations', 'control-theory']"
85,Similar matrices and rref,Similar matrices and rref,,I have a hunch that all conjugate/similar matrices have the same reduced row echelon form . Am I right?,I have a hunch that all conjugate/similar matrices have the same reduced row echelon form . Am I right?,,"['linear-algebra', 'matrices']"
86,matrix multiplication convergence problem,matrix multiplication convergence problem,,Suppose A is a square matrix. Does $A^n$(matrix multiplication) converge when n is an infinite big number？ Is it always true or under certain circumstances?,Suppose A is a square matrix. Does $A^n$(matrix multiplication) converge when n is an infinite big number？ Is it always true or under certain circumstances?,,"['linear-algebra', 'convergence-divergence']"
87,How is diagonal matrices a subspace of upper triangular matrices?,How is diagonal matrices a subspace of upper triangular matrices?,,"I'm kind of confused that the diagonal matrices is a subspace of the upper triangular matrices. Suppose I have the following matrices: $$U=\begin{bmatrix} a & b\\  0 & d \end{bmatrix} ; \; D=\begin{bmatrix} a & 0\\  0 & d \end{bmatrix}$$ I read in the book that the matrix $D$ is a subspace of $U$. But how's it so? At first, I thought the reason why the diagonal matrices are subspaces of the upper triangular matrices was because the the matrix $D$ is just a case of matrix $U$ with $b=0$ and that's why it is a subspace of it. But on a second thought, if my assumption was true, then everything, even the identity matrix is a subspace of the upper triangular matrices and the upper triangular matrices would be subspace of any 2 by 2 matrices. This doesn't make sense at all. So how is this being looked at that $D$ is a subspace of $U$?","I'm kind of confused that the diagonal matrices is a subspace of the upper triangular matrices. Suppose I have the following matrices: $$U=\begin{bmatrix} a & b\\  0 & d \end{bmatrix} ; \; D=\begin{bmatrix} a & 0\\  0 & d \end{bmatrix}$$ I read in the book that the matrix $D$ is a subspace of $U$. But how's it so? At first, I thought the reason why the diagonal matrices are subspaces of the upper triangular matrices was because the the matrix $D$ is just a case of matrix $U$ with $b=0$ and that's why it is a subspace of it. But on a second thought, if my assumption was true, then everything, even the identity matrix is a subspace of the upper triangular matrices and the upper triangular matrices would be subspace of any 2 by 2 matrices. This doesn't make sense at all. So how is this being looked at that $D$ is a subspace of $U$?",,"['linear-algebra', 'matrices']"
88,Simultaneous diagonalizability of commuting unitary operators,Simultaneous diagonalizability of commuting unitary operators,,"I'm trying to prove the following: If $S\colon V\to V$ and $T\colon V\to V$ are unitary linear transformations on unitary space $V$ ($\dim V=n$, $n$ is finite), such that $ST=TS$, then they have a joint eigenvector basis (aka there is a basis of $V$ composed of eigenvectors of both $S$ and $T$ - not necessarily of the same eigenvalue per each). Can anyone help me out? I've tried rephrasing the 'matrix equivalent' of the theorem, but I didn't get much further. Thanks!","I'm trying to prove the following: If $S\colon V\to V$ and $T\colon V\to V$ are unitary linear transformations on unitary space $V$ ($\dim V=n$, $n$ is finite), such that $ST=TS$, then they have a joint eigenvector basis (aka there is a basis of $V$ composed of eigenvectors of both $S$ and $T$ - not necessarily of the same eigenvalue per each). Can anyone help me out? I've tried rephrasing the 'matrix equivalent' of the theorem, but I didn't get much further. Thanks!",,['linear-algebra']
89,"Prove that if $g(t)$ is relatively prime to the characteristic polynomial of $A$, then $g(A)$ is invertible","Prove that if  is relatively prime to the characteristic polynomial of , then  is invertible",g(t) A g(A),"I'd like to write to you two problems that I tried to solve, I'm not sure of the solution of the first. Let $A\in M_n (F)$ be a matrix and $g(t)\in F(t)$ a polynomial, $P_A(t)$- the characteristic poly of $A$. I need to prove that if $\gcd(P_A(t),g(t)=1$ so $g(A)$ is invertible. Things I know: *If $\gcd(P_{A}(t),g(t)=1$ so $P_A(t)$ and $g(t)$ have no factor in common *for every eigenvalue $\lambda$ of A $\det(A-\lambda)=0$ *The minimal poly has all the factors that the cha. has and it is the poly with the minimal degree that if we plug $A$ into it it will equal 0  So I know that for sure $g(A) \neq0$ , but it's not enough to say that $\det g(A) \neq 0$ and to conclude what I need. Is it correct to say that  if $\det g(A) = 0$ than $g(x)$ was part of $P_A(t)$? And if yes, why? The second question is for this bilinear form: $q(v)=q(x,y,z)=2x^2-3y^2+xy-5yz$ I need to find base $B$ so that $[q]^B$ will be diagonalizable matrix. So, I tried to look for eigenvalues but it's impossible mission, it's really messy. Is there any other method which with I can find it? maybe something with Jacoby method? Thanks alot!","I'd like to write to you two problems that I tried to solve, I'm not sure of the solution of the first. Let $A\in M_n (F)$ be a matrix and $g(t)\in F(t)$ a polynomial, $P_A(t)$- the characteristic poly of $A$. I need to prove that if $\gcd(P_A(t),g(t)=1$ so $g(A)$ is invertible. Things I know: *If $\gcd(P_{A}(t),g(t)=1$ so $P_A(t)$ and $g(t)$ have no factor in common *for every eigenvalue $\lambda$ of A $\det(A-\lambda)=0$ *The minimal poly has all the factors that the cha. has and it is the poly with the minimal degree that if we plug $A$ into it it will equal 0  So I know that for sure $g(A) \neq0$ , but it's not enough to say that $\det g(A) \neq 0$ and to conclude what I need. Is it correct to say that  if $\det g(A) = 0$ than $g(x)$ was part of $P_A(t)$? And if yes, why? The second question is for this bilinear form: $q(v)=q(x,y,z)=2x^2-3y^2+xy-5yz$ I need to find base $B$ so that $[q]^B$ will be diagonalizable matrix. So, I tried to look for eigenvalues but it's impossible mission, it's really messy. Is there any other method which with I can find it? maybe something with Jacoby method? Thanks alot!",,[]
90,Programming Logic: - Splitting up Tasks Between Threads,Programming Logic: - Splitting up Tasks Between Threads,,"I asked this question at stackoverflow and instead of addressing the math required in the problem, they wanted to talk about why setting up 5 threads is no good, or question my intentions. I just want the math solved. Lets say you want 5 threads to process data simultaneous. Also assume, you have 89 tasks to process. Off the bat you know 89 / 5 = 17 with a remainder of 4. The best way to split up tasks would be to have  4 (the remainder) threads process 18 (17+1) tasks each and then have 1 (# threads - remainder) thread to process 17. This will eliminate the remainder. Just to verify: Thread 1: Tasks  1-18  (18 tasks) Thread 2: Tasks 19-36  (18 tasks) Thread 3: Tasks 37-54  (18 tasks) Thread 4: Tasks 55-72  (18 tasks) Thread 5: Tasks 73-89  (17 tasks) Giving you a total of 89 tasks completed. I need a way of getting the start and ending range of each thread mathematically/programmability; where the following should print the exact thing I have listed above: $NumTasks = 89 $NumThreads = 5 $Remainder = $NumTasks % $NumThreads  $DefaultNumTasksAssigned = floor($NumTasks / $NumThreads)  For $i = 1 To $NumThreads     if $i <= $Remainder Then         $NumTasksAssigned = $DefaultNumTasksAssigned + 1     else         $NumTasksAssigned = $DefaultNumTasksAssigned     endif     $Start = ??????????     $End = ??????????     print Thread $i: Tasks $Start-$End ($NumTasksAssigned tasks) Next This should also work for any number of $NumTasks . Note: Please stick to answering the math at hand and avoid suggesting or assuming the situation.","I asked this question at stackoverflow and instead of addressing the math required in the problem, they wanted to talk about why setting up 5 threads is no good, or question my intentions. I just want the math solved. Lets say you want 5 threads to process data simultaneous. Also assume, you have 89 tasks to process. Off the bat you know 89 / 5 = 17 with a remainder of 4. The best way to split up tasks would be to have  4 (the remainder) threads process 18 (17+1) tasks each and then have 1 (# threads - remainder) thread to process 17. This will eliminate the remainder. Just to verify: Thread 1: Tasks  1-18  (18 tasks) Thread 2: Tasks 19-36  (18 tasks) Thread 3: Tasks 37-54  (18 tasks) Thread 4: Tasks 55-72  (18 tasks) Thread 5: Tasks 73-89  (17 tasks) Giving you a total of 89 tasks completed. I need a way of getting the start and ending range of each thread mathematically/programmability; where the following should print the exact thing I have listed above: $NumTasks = 89 $NumThreads = 5 $Remainder = $NumTasks % $NumThreads  $DefaultNumTasksAssigned = floor($NumTasks / $NumThreads)  For $i = 1 To $NumThreads     if $i <= $Remainder Then         $NumTasksAssigned = $DefaultNumTasksAssigned + 1     else         $NumTasksAssigned = $DefaultNumTasksAssigned     endif     $Start = ??????????     $End = ??????????     print Thread $i: Tasks $Start-$End ($NumTasksAssigned tasks) Next This should also work for any number of $NumTasks . Note: Please stick to answering the math at hand and avoid suggesting or assuming the situation.",,"['linear-algebra', 'algebra-precalculus', 'logic']"
91,Linear Algebra: Finding a steady state matrix,Linear Algebra: Finding a steady state matrix,,"Here is the problem: And here is what I tried to do: I tried doing it in my calculator and got that the answer is 40 and 160 as you keep multiplying PX. Can anyone point out what I'm doing wrong or help me? Thanks. Edit for readability: Problem Statement: A college dormitory houses 200 students. Those who watch an hour or more of TV on any day always watch for less than an hour the next day. One-fourth of those who watch TV for less than an hour one day will watch an hour or more the next day. Half of the students watch TV for an hour or more today. a) How many will watch TV for an hour or more tomorrow? b) In 2 days? c) Find the steady state matrix for populations described in this problem. Work: $ P = \stackrel{>1 \hphantom{XX} <1}{\begin{bmatrix}  0 & 0.25\\  1 & 0.75  \end{bmatrix}} {>1 \atop <1} \quad x = \begin{bmatrix} 100\\ 100 \end{bmatrix} {>1 \atop <1} $ $ Px = \begin{bmatrix} 0 & 0.25\\ 1 & 0.75 \end{bmatrix} \begin{bmatrix} 100\\ 100 \end{bmatrix} = \begin{bmatrix} 25\\ 175 \end{bmatrix} {>1 \atop } $ a) 25. $ P^{\,2}x = \begin{bmatrix} 0 & 0.25\\ 1 & 0.75 \end{bmatrix} \begin{bmatrix} 25\\ 175 \end{bmatrix} = \begin{bmatrix} 43.75\\ 156.25 \end{bmatrix} {>1 \atop <1} $ b) $ 43.75 \sim 44 $ . Steady state $P\,\overline{x} = \overline{x}$ \begin{bmatrix} 0 & 0.25\\ 1 & 0.75 \end{bmatrix} \begin{bmatrix} x_{1}\\ x_{2} \end{bmatrix} \begin{bmatrix} x_{1}\\ x_{2} \end{bmatrix} $ \Rightarrow $ \begin{bmatrix} 0.25 x_{2}\\ x_{1}+0.75 x_{2} \end{bmatrix} $ \Rightarrow $ \begin{bmatrix} 25 \\ 175 \end{bmatrix} $ \boxed{\begin{align*} 0.25 x_{2} &= x_{1}\\ x_{1} + 0.75 x_{2} &= x_{2} \end{align*}} $",Here is the problem: And here is what I tried to do: I tried doing it in my calculator and got that the answer is 40 and 160 as you keep multiplying PX. Can anyone point out what I'm doing wrong or help me? Thanks. Edit for readability: Problem Statement: A college dormitory houses 200 students. Those who watch an hour or more of TV on any day always watch for less than an hour the next day. One-fourth of those who watch TV for less than an hour one day will watch an hour or more the next day. Half of the students watch TV for an hour or more today. a) How many will watch TV for an hour or more tomorrow? b) In 2 days? c) Find the steady state matrix for populations described in this problem. Work: a) 25. b) . Steady state,"
P = \stackrel{>1 \hphantom{XX} <1}{\begin{bmatrix} 
0 & 0.25\\ 
1 & 0.75 
\end{bmatrix}} {>1 \atop <1}
\quad
x = \begin{bmatrix}
100\\
100
\end{bmatrix}
{>1 \atop <1}
 
Px = \begin{bmatrix}
0 & 0.25\\
1 & 0.75
\end{bmatrix}
\begin{bmatrix}
100\\
100
\end{bmatrix}
=
\begin{bmatrix}
25\\
175
\end{bmatrix}
{>1 \atop }
 
P^{\,2}x = \begin{bmatrix}
0 & 0.25\\
1 & 0.75
\end{bmatrix}
\begin{bmatrix}
25\\
175
\end{bmatrix}
=
\begin{bmatrix}
43.75\\
156.25
\end{bmatrix}
{>1 \atop <1}
  43.75 \sim 44  P\,\overline{x} = \overline{x} \begin{bmatrix}
0 & 0.25\\
1 & 0.75
\end{bmatrix} \begin{bmatrix}
x_{1}\\
x_{2}
\end{bmatrix} \begin{bmatrix}
x_{1}\\
x_{2}
\end{bmatrix} 
\Rightarrow
 \begin{bmatrix}
0.25 x_{2}\\
x_{1}+0.75 x_{2}
\end{bmatrix} 
\Rightarrow
 \begin{bmatrix}
25 \\
175
\end{bmatrix} 
\boxed{\begin{align*}
0.25 x_{2} &= x_{1}\\
x_{1} + 0.75 x_{2} &= x_{2}
\end{align*}}
",['linear-algebra']
92,Finding the determinant of a matrix,Finding the determinant of a matrix,,"I am given $A = \begin{pmatrix} a & b\\  c & d \end{pmatrix} $ and B = $ \begin{pmatrix} e & f\\  g & h \end{pmatrix}$ whose elements are non-zero reals. If $BA = I$, where $I$ is the $2 \times 2$ identity matrix and $D$ is the value of the determinant of $B$, then find the value of $D$ Assume that four options are given for the correct answer (which is $\frac{d}{e}$) and only one is correct. How can I find the correct answer quickly? ADDED: The answer suggested in my module is $\frac{d}{e}$, so I am suppose to derive to that point.","I am given $A = \begin{pmatrix} a & b\\  c & d \end{pmatrix} $ and B = $ \begin{pmatrix} e & f\\  g & h \end{pmatrix}$ whose elements are non-zero reals. If $BA = I$, where $I$ is the $2 \times 2$ identity matrix and $D$ is the value of the determinant of $B$, then find the value of $D$ Assume that four options are given for the correct answer (which is $\frac{d}{e}$) and only one is correct. How can I find the correct answer quickly? ADDED: The answer suggested in my module is $\frac{d}{e}$, so I am suppose to derive to that point.",,"['linear-algebra', 'matrices', 'determinant']"
93,Question regarding positive-definite matrices,Question regarding positive-definite matrices,,"Let $A, B$ be positive-definite matrices and $Q$ a unitary matrix, furthermore suppose $A=BQ$ . Prove or disprove: $A=B$ . I'm having a hard time figuring out where to begin. Thanks.","Let be positive-definite matrices and a unitary matrix, furthermore suppose . Prove or disprove: . I'm having a hard time figuring out where to begin. Thanks.","A, B Q A=BQ A=B",[]
94,Convert Frobenius canonical form to Jordan canonical form,Convert Frobenius canonical form to Jordan canonical form,,recently I've come across a few cases where I had to evaluate $e^{At}$ (differential equations) where A is in Frobenius canonical form. The algorithm looks like: $$e^{At} = Pe^{Jt}P^{-1}$$ so I need to do Jordan decomposition. From Frobenius canonical form I can easily write the characteristic equation => compute the roots which are eigenvalues and write the Jordan canonical form. This was the easy part. Now the other part: compute matrices $P$ and $P^{-1}$. Is there any standard form of those matrices if matrix A is in a Frobenius canonical form?,recently I've come across a few cases where I had to evaluate $e^{At}$ (differential equations) where A is in Frobenius canonical form. The algorithm looks like: $$e^{At} = Pe^{Jt}P^{-1}$$ so I need to do Jordan decomposition. From Frobenius canonical form I can easily write the characteristic equation => compute the roots which are eigenvalues and write the Jordan canonical form. This was the easy part. Now the other part: compute matrices $P$ and $P^{-1}$. Is there any standard form of those matrices if matrix A is in a Frobenius canonical form?,,"['linear-algebra', 'ordinary-differential-equations']"
95,proof that $\phi\circ\phi=id$ implies the existence of a diagonal matrix,proof that  implies the existence of a diagonal matrix,\phi\circ\phi=id,"As exam preparation we were trying to proof the following task: Let $V=\mathbb{R}^2$ and let $\phi$ be an endomorphism of $V$ with $\phi \circ \phi = id$ and $\phi \neq id$ and $\phi \neq -id$. Proof that this implies the existence of a basis $B=(b_1,b_2)$ of $V$ with $\phi(b_1) = b_1$ and $\phi(b_2)=-b_2$ Unfortunately we aren't able to solve that task and would very much appreciate some proofs and and a ""how to"" of how to approach such problems. Thanks for your help.","As exam preparation we were trying to proof the following task: Let $V=\mathbb{R}^2$ and let $\phi$ be an endomorphism of $V$ with $\phi \circ \phi = id$ and $\phi \neq id$ and $\phi \neq -id$. Proof that this implies the existence of a basis $B=(b_1,b_2)$ of $V$ with $\phi(b_1) = b_1$ and $\phi(b_2)=-b_2$ Unfortunately we aren't able to solve that task and would very much appreciate some proofs and and a ""how to"" of how to approach such problems. Thanks for your help.",,['linear-algebra']
96,Help with proof about maximum number of eigenvalues,Help with proof about maximum number of eigenvalues,,"I'm working my way through Linear Algebra Done Right . To help with one proof, I want to prove the following: Given $\mathbf{V}$, a vector space and $T$, a linear operator on it, then: If $\mathbf{W}_1$ and $\mathbf{W}_2$ are subspaces of $\mathbf{V}$ such that: $\mathbf{V}$ is a direct sum of $\mathbf{W}_1$ and $\mathbf{W}_2$. $\mathbf{W}_1$ and $\mathbf{W}_2$ are invariant under $T$. The restriction of $T$ to $\mathbf{W}_1$ has at most $k$ eigenvalues. The restriction of $T$ to $\mathbf{W}_2$ has at most $p$ eigenvalues. Then $T$ has at most $k+p$ eigenvalues. I've done a sketch of a proof using determinants, but it was based on old knowledge about the properties of determinants with regards to eigenvalues, so it may not be correct. The book doesn't emphasize using them though, and maybe there's a proof of this without using determinants. I've tried a proof by contradiction, trying to find something weird by assuming that T can have more than $k+p$ eigenvalues, but I haven't been able to find anything. Any help would be appreciated.","I'm working my way through Linear Algebra Done Right . To help with one proof, I want to prove the following: Given $\mathbf{V}$, a vector space and $T$, a linear operator on it, then: If $\mathbf{W}_1$ and $\mathbf{W}_2$ are subspaces of $\mathbf{V}$ such that: $\mathbf{V}$ is a direct sum of $\mathbf{W}_1$ and $\mathbf{W}_2$. $\mathbf{W}_1$ and $\mathbf{W}_2$ are invariant under $T$. The restriction of $T$ to $\mathbf{W}_1$ has at most $k$ eigenvalues. The restriction of $T$ to $\mathbf{W}_2$ has at most $p$ eigenvalues. Then $T$ has at most $k+p$ eigenvalues. I've done a sketch of a proof using determinants, but it was based on old knowledge about the properties of determinants with regards to eigenvalues, so it may not be correct. The book doesn't emphasize using them though, and maybe there's a proof of this without using determinants. I've tried a proof by contradiction, trying to find something weird by assuming that T can have more than $k+p$ eigenvalues, but I haven't been able to find anything. Any help would be appreciated.",,[]
97,Proving that $||A-B||=||A+B||\Leftrightarrow AB=0$,Proving that,||A-B||=||A+B||\Leftrightarrow AB=0,"I have to prove that \begin{equation*} ||A-B||=||A+B||\Leftrightarrow AB=0 \end{equation*} and I was wondering if this approach is correct, or if there's a better/more elegant way to prove this. Given n-dimensional vectors A and B, we can write $||A-B||=||A+B||$ as: \begin{equation*} \sqrt{\sum\limits_{j=1}^{n}(a_j-b_j)^2}=\sqrt{\sum\limits_{j=1}^{n}(a_j+b_j)^2}. \end{equation*} Squaring both sides and expanding the binomials: \begin{equation*} \sum\limits_{j=1}^{n}a^2_j-2a_jb_j+b_j^2=\sum\limits_{j=1}^{n}a^2_j+2a_jb_j+b_j^2. \end{equation*} Simplifying: \begin{equation*} -\sum\limits_{j=1}^{n}a_jb_j=\sum\limits_{j=1}^{n}a_jb_j,~\text{which holds true if and only if}~\sum\limits_{j=1}^{n}a_jb_j=0.  \end{equation*} Since $AB$ is equivalent to $\sum\limits_{j=1}^{n}a_jb_j$, then $||A-B||=||A+B||\Leftrightarrow AB=0$ Thanks in advance.","I have to prove that \begin{equation*} ||A-B||=||A+B||\Leftrightarrow AB=0 \end{equation*} and I was wondering if this approach is correct, or if there's a better/more elegant way to prove this. Given n-dimensional vectors A and B, we can write $||A-B||=||A+B||$ as: \begin{equation*} \sqrt{\sum\limits_{j=1}^{n}(a_j-b_j)^2}=\sqrt{\sum\limits_{j=1}^{n}(a_j+b_j)^2}. \end{equation*} Squaring both sides and expanding the binomials: \begin{equation*} \sum\limits_{j=1}^{n}a^2_j-2a_jb_j+b_j^2=\sum\limits_{j=1}^{n}a^2_j+2a_jb_j+b_j^2. \end{equation*} Simplifying: \begin{equation*} -\sum\limits_{j=1}^{n}a_jb_j=\sum\limits_{j=1}^{n}a_jb_j,~\text{which holds true if and only if}~\sum\limits_{j=1}^{n}a_jb_j=0.  \end{equation*} Since $AB$ is equivalent to $\sum\limits_{j=1}^{n}a_jb_j$, then $||A-B||=||A+B||\Leftrightarrow AB=0$ Thanks in advance.",,['linear-algebra']
98,The orthogonal complement of the space of row-null and column-null matrices,The orthogonal complement of the space of row-null and column-null matrices,,"I propose the following lemma and its proof. It is related to row-null and column-null matrices - i.e. matrices whose rows and columns both sum to zero. Could you please give your opinion on the plausibility of the lemma, and the validity of the proof? Lemma: Let $Z\in\text{GL}(n,\mathbb{R})$ be a general $n\times n$ real matrix, and let $Y\in\mathcal{S}(n,\mathbb{R})$, where $\mathcal{S}(n,\mathbb{R})$ is the space of row-null column-null $n\times n$ real matrices. Then $\text{Tr}(ZY)=0$ for all $Y$ in $\mathcal{S}(n,\mathbb{R})$ if and only if $Z$ has the form $$Z_{ij}=\left(p_{j}-p_{i}\right)+\left(q_{j}+q_{i}\right)$$. Proof: Consider the space of row-null and column-null matrices $$\mathcal{S}(n,\mathbb{R})= \left\{ Y_{ij}\in GL(n,\mathbb{R}):\sum_{i}Y_{ij}=0,\sum_{j}Y_{ij}=0 \right\} $$ Its dimension is $$\text{dim}(S(n,\mathbb{R}))=N^{2}-2N+1$$ since the row-nullness and column-nullness are defined by $2N$ equations, only $2N-1$ of which are linearly independent. Consider the following space $$\mathcal{G}(n,\mathbb{R})=\left\{ Z_{ij}\in GL(n,\mathbb{R}):Z_{ij}=\left(p_{j}-p_{i}\right)+\left(q_{j}+q_{i}\right)\right\}$$ Its dimension is $$\text{dim}(\mathcal{G}(n,\mathbb{R}))=2N-1$$ where $N-1$ is the contribution from the antisymmetric part and $N$ is from the symmetric part. Assume $Y\in\mathcal{S}$ and $Z\in\mathcal{G}$, then the Frobenius inner product of two such elements is $$ \text{Tr}(ZY) =\sum_{ij}\left[\left(p_{j}-p_{i}\right)Y_{ji}+\left(q_{j}+q_{i}\right)Y_{ji}\right] $$ $$ =\sum_{j}(q_{j}+p_{j})\sum_{i}Y_{ji}+\sum_{i}(q_{i}-p_{i})\sum_{j}Y_{ji}=0 $$ Since $\text{dim}(\mathcal{G})+\text{dim}(\mathcal{S})=\text{dim}(GL)$ and $\mathcal{G}\perp\mathcal{S}$, then $\mathcal{G}$ and $\mathcal{S}$ must be complementary in $GL$. Therfore, if $Y$ is orthogonal to all the matrices in $\mathcal{S}$, it must lie in $\mathcal{G}$. PS: How can I get the curly brackets {} to render in latex mode?","I propose the following lemma and its proof. It is related to row-null and column-null matrices - i.e. matrices whose rows and columns both sum to zero. Could you please give your opinion on the plausibility of the lemma, and the validity of the proof? Lemma: Let $Z\in\text{GL}(n,\mathbb{R})$ be a general $n\times n$ real matrix, and let $Y\in\mathcal{S}(n,\mathbb{R})$, where $\mathcal{S}(n,\mathbb{R})$ is the space of row-null column-null $n\times n$ real matrices. Then $\text{Tr}(ZY)=0$ for all $Y$ in $\mathcal{S}(n,\mathbb{R})$ if and only if $Z$ has the form $$Z_{ij}=\left(p_{j}-p_{i}\right)+\left(q_{j}+q_{i}\right)$$. Proof: Consider the space of row-null and column-null matrices $$\mathcal{S}(n,\mathbb{R})= \left\{ Y_{ij}\in GL(n,\mathbb{R}):\sum_{i}Y_{ij}=0,\sum_{j}Y_{ij}=0 \right\} $$ Its dimension is $$\text{dim}(S(n,\mathbb{R}))=N^{2}-2N+1$$ since the row-nullness and column-nullness are defined by $2N$ equations, only $2N-1$ of which are linearly independent. Consider the following space $$\mathcal{G}(n,\mathbb{R})=\left\{ Z_{ij}\in GL(n,\mathbb{R}):Z_{ij}=\left(p_{j}-p_{i}\right)+\left(q_{j}+q_{i}\right)\right\}$$ Its dimension is $$\text{dim}(\mathcal{G}(n,\mathbb{R}))=2N-1$$ where $N-1$ is the contribution from the antisymmetric part and $N$ is from the symmetric part. Assume $Y\in\mathcal{S}$ and $Z\in\mathcal{G}$, then the Frobenius inner product of two such elements is $$ \text{Tr}(ZY) =\sum_{ij}\left[\left(p_{j}-p_{i}\right)Y_{ji}+\left(q_{j}+q_{i}\right)Y_{ji}\right] $$ $$ =\sum_{j}(q_{j}+p_{j})\sum_{i}Y_{ji}+\sum_{i}(q_{i}-p_{i})\sum_{j}Y_{ji}=0 $$ Since $\text{dim}(\mathcal{G})+\text{dim}(\mathcal{S})=\text{dim}(GL)$ and $\mathcal{G}\perp\mathcal{S}$, then $\mathcal{G}$ and $\mathcal{S}$ must be complementary in $GL$. Therfore, if $Y$ is orthogonal to all the matrices in $\mathcal{S}$, it must lie in $\mathcal{G}$. PS: How can I get the curly brackets {} to render in latex mode?",,['linear-algebra']
99,Sums of a set of symmetric matrices,Sums of a set of symmetric matrices,,"Say we have a set of symmetric $n \times n$ matrices $M_i$ for $1 \leq i \leq k$, elements in $\mathbb{R}$.  Suppose that for every $\boldsymbol{\lambda} = (\lambda_1, \dots , \lambda_k) \in \mathbb{R}^k$ we have that the kernel of \begin{equation*} M_{\boldsymbol{\lambda}} = \sum_i \lambda_i M_i  \end{equation*} is nontrivial.  Does it follow that there exists some nonzero $n$ vector $\textbf{v}$ with $M_i \textbf{v} = 0$ for all $i$?","Say we have a set of symmetric $n \times n$ matrices $M_i$ for $1 \leq i \leq k$, elements in $\mathbb{R}$.  Suppose that for every $\boldsymbol{\lambda} = (\lambda_1, \dots , \lambda_k) \in \mathbb{R}^k$ we have that the kernel of \begin{equation*} M_{\boldsymbol{\lambda}} = \sum_i \lambda_i M_i  \end{equation*} is nontrivial.  Does it follow that there exists some nonzero $n$ vector $\textbf{v}$ with $M_i \textbf{v} = 0$ for all $i$?",,['linear-algebra']
