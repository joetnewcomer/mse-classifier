,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Is the pointwise maximum of two Riemann integrable functions Riemann integrable? [closed],Is the pointwise maximum of two Riemann integrable functions Riemann integrable? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Improve this question Let $f,g$ be Riemann integrable functions, prove that the function $ h(x) $ defined by $$ h\left( x \right) = \max \left\{ {f\left( x \right),g\left( x \right)} \right\} $$ is also Riemann integrable.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Improve this question Let $f,g$ be Riemann integrable functions, prove that the function $ h(x) $ defined by $$ h\left( x \right) = \max \left\{ {f\left( x \right),g\left( x \right)} \right\} $$ is also Riemann integrable.",,"['real-analysis', 'integration']"
1,Classifying Functions of the form $f(x+y)=f(x)f(y)$ [duplicate],Classifying Functions of the form  [duplicate],f(x+y)=f(x)f(y),"This question already has answers here : Closed 13 years ago . Possible Duplicate: Is there a name for such kind of function? The question is:  is there a nice characterization of all nonnegative functions $f:\mathbb{R}\rightarrow \mathbb{R}$ such that $f(x+y)=f(x)f(y)$. If $f$ is continuously differentiable, then I can prove that $f$ is exponential, but I don't know this in general.  For what it's worth, in my particular case, I can assume that $f$ is right continuous, i.e. $\lim _{x\to c^+}f(x)=f(c)$, but that is all.  From this, what can I deduce about the form of $f$? Thanks much!","This question already has answers here : Closed 13 years ago . Possible Duplicate: Is there a name for such kind of function? The question is:  is there a nice characterization of all nonnegative functions $f:\mathbb{R}\rightarrow \mathbb{R}$ such that $f(x+y)=f(x)f(y)$. If $f$ is continuously differentiable, then I can prove that $f$ is exponential, but I don't know this in general.  For what it's worth, in my particular case, I can assume that $f$ is right continuous, i.e. $\lim _{x\to c^+}f(x)=f(c)$, but that is all.  From this, what can I deduce about the form of $f$? Thanks much!",,"['real-analysis', 'functional-equations']"
2,Can an open and closed function be neither injective or surjective.,Can an open and closed function be neither injective or surjective.,,"I know if a function $f: X \to Y$ is bijective, it is open iff it is closed as both are equivalent to $f^{-1}$ being continuous. Now I wonder if a function $f$ on two topological spaces is open and closed, must it be bijective? Furthermore, can it be neither injective nor surjective? I suspect that there is some easy counterexample, but I cannot think of any.","I know if a function is bijective, it is open iff it is closed as both are equivalent to being continuous. Now I wonder if a function on two topological spaces is open and closed, must it be bijective? Furthermore, can it be neither injective nor surjective? I suspect that there is some easy counterexample, but I cannot think of any.",f: X \to Y f^{-1} f,"['real-analysis', 'general-topology', 'functions', 'metric-spaces']"
3,High school contest math problem,High school contest math problem,,"This is a calculus problem from a high school math contest in Greece,from 2012. I wish to know some solutions for this. I attempted to solve it. Let $f:\Bbb{R} \to \Bbb{R}$ differentiable such that $\lim_{x \to +\infty}f(x)=+\infty$ and $\lim_{x \to +\infty}\frac{f'(x)}{f(x)}=2$ .Show that $$\lim_{x \to +\infty}\frac{f(x)}{x^{2012}}=+\infty$$ Here is my attempt: $\frac{f(x)}{x^{2012}}=e^{\ln{\frac{f(x)}{x^{2012}}}}$ Now $\ln{\frac{f(x)}{x^{2012}}}=\ln{f(x)}-2012\ln x=\ln{x}\left( \frac{\ln{f(x)}}{\ln{x}}-2012\right)$ Now from hypothesis we see that $\lim_{x \to +\infty}\ln{f(x)}=+\infty$ By L'Hospital's rule we have that $$\lim_{x \to +\infty}\frac{\ln{f(x)}}{\ln{x}}=\lim_{x \to +\infty}x \frac{f'(x)}{f(x)}=2(+\infty)=+\infty$$ Thus $$\lim_{x \to +\infty}\ln{x}\left( \frac{\ln{f(x)}}{\ln{x}}-2012\right)=+\infty$$ Finally $\lim_{x \to +\infty}\frac{f(x)}{x^{2012}}=+\infty$ Is this solution correct? If it is,then are there also  better and quicker ways  to solve this? Thank you in advance.","This is a calculus problem from a high school math contest in Greece,from 2012. I wish to know some solutions for this. I attempted to solve it. Let differentiable such that and .Show that Here is my attempt: Now Now from hypothesis we see that By L'Hospital's rule we have that Thus Finally Is this solution correct? If it is,then are there also  better and quicker ways  to solve this? Thank you in advance.",f:\Bbb{R} \to \Bbb{R} \lim_{x \to +\infty}f(x)=+\infty \lim_{x \to +\infty}\frac{f'(x)}{f(x)}=2 \lim_{x \to +\infty}\frac{f(x)}{x^{2012}}=+\infty \frac{f(x)}{x^{2012}}=e^{\ln{\frac{f(x)}{x^{2012}}}} \ln{\frac{f(x)}{x^{2012}}}=\ln{f(x)}-2012\ln x=\ln{x}\left( \frac{\ln{f(x)}}{\ln{x}}-2012\right) \lim_{x \to +\infty}\ln{f(x)}=+\infty \lim_{x \to +\infty}\frac{\ln{f(x)}}{\ln{x}}=\lim_{x \to +\infty}x \frac{f'(x)}{f(x)}=2(+\infty)=+\infty \lim_{x \to +\infty}\ln{x}\left( \frac{\ln{f(x)}}{\ln{x}}-2012\right)=+\infty \lim_{x \to +\infty}\frac{f(x)}{x^{2012}}=+\infty,"['real-analysis', 'calculus', 'limits', 'contest-math', 'solution-verification']"
4,"Is 1/x, integrated from 1 to infinity, considered ""integrable""?","Is 1/x, integrated from 1 to infinity, considered ""integrable""?",,"$\large \int\limits_1^\infty \frac{dx}{x} $ diverges by comparison with the infinite sum $\sum\limits^\infty\frac{1}{n}$. But $\frac{1}{x}$ is continuous at all non-zero values of $x$. So is $\frac{1}{x}$ ""integrable"" from 1 to infinity, or does the fact that the integral diverges mean it is not integrable? I'm just trying to make sure I am using the dominated convergence theorem correctly.  DCT states that if the integrand is bounded by some ""integrable"" function, then the limit of the integrals, is the integral of the limit, roughly speaking.","$\large \int\limits_1^\infty \frac{dx}{x} $ diverges by comparison with the infinite sum $\sum\limits^\infty\frac{1}{n}$. But $\frac{1}{x}$ is continuous at all non-zero values of $x$. So is $\frac{1}{x}$ ""integrable"" from 1 to infinity, or does the fact that the integral diverges mean it is not integrable? I'm just trying to make sure I am using the dominated convergence theorem correctly.  DCT states that if the integrand is bounded by some ""integrable"" function, then the limit of the integrals, is the integral of the limit, roughly speaking.",,"['calculus', 'real-analysis']"
5,Almost everywhere differentiable definition,Almost everywhere differentiable definition,,"I dont understand what is the almost everwhere differentiable, can you give an example and definition please?","I dont understand what is the almost everwhere differentiable, can you give an example and definition please?",,"['calculus', 'real-analysis']"
6,Does an uncountable Golomb ruler exist?,Does an uncountable Golomb ruler exist?,,"Does there exist an uncountable set $G\subset \mathbb{R}$ such that, for $a,b,c,d \in G$, if $a-b=c-d$ then $a=c$ and $b=d$?","Does there exist an uncountable set $G\subset \mathbb{R}$ such that, for $a,b,c,d \in G$, if $a-b=c-d$ then $a=c$ and $b=d$?",,"['real-analysis', 'general-topology']"
7,Infinite Series $1+\frac12-\frac23+\frac14+\frac15-\frac26+\cdots$,Infinite Series,1+\frac12-\frac23+\frac14+\frac15-\frac26+\cdots,"Was given the following infinite sum in class as a question, while we were talking about Taylor series expansions of $\ln(1+x)$ and $\arctan(x)$: $$1+\frac12-\frac23+\frac14+\frac15-\frac26+\cdots$$ The question isn't homework or anything, just a thought tease. I tried for a long while but couldn't find anything remotely close. Thanks in advance for the help.","Was given the following infinite sum in class as a question, while we were talking about Taylor series expansions of $\ln(1+x)$ and $\arctan(x)$: $$1+\frac12-\frac23+\frac14+\frac15-\frac26+\cdots$$ The question isn't homework or anything, just a thought tease. I tried for a long while but couldn't find anything remotely close. Thanks in advance for the help.",,"['calculus', 'real-analysis', 'sequences-and-series', 'summation', 'closed-form']"
8,"Show that $\int_0^{2\pi}\frac{R^2-r^2}{R^2 - 2Rr\cos (\varphi-\vartheta) + r^2}d\vartheta$ is independent of $R>r>0$, using only real numbers.","Show that  is independent of , using only real numbers.",\int_0^{2\pi}\frac{R^2-r^2}{R^2 - 2Rr\cos (\varphi-\vartheta) + r^2}d\vartheta R>r>0,"The poisson kernel is sometimes written as $$ \frac{1}{2\pi}\int_0^{2\pi} \frac{R^2-r^2}{R^2 - 2Rr\cos(\varphi-\vartheta) + r^2} \mathrm{d}\vartheta = 1 \ , \ \ R>r>0 $$ Where $\varphi$ is some arbitary angle.  This much used in complex analysis amongst other fields. Is there some basic, elementary way of showing that the integral is independent on $R$ and $r$? Splitting the integral at $\int_0^\pi + \int_\pi^{2\pi}$ and using the Weierstrass substitution seems like somewhat ugly and a  messy way to approach the problem.","The poisson kernel is sometimes written as $$ \frac{1}{2\pi}\int_0^{2\pi} \frac{R^2-r^2}{R^2 - 2Rr\cos(\varphi-\vartheta) + r^2} \mathrm{d}\vartheta = 1 \ , \ \ R>r>0 $$ Where $\varphi$ is some arbitary angle.  This much used in complex analysis amongst other fields. Is there some basic, elementary way of showing that the integral is independent on $R$ and $r$? Splitting the integral at $\int_0^\pi + \int_\pi^{2\pi}$ and using the Weierstrass substitution seems like somewhat ugly and a  messy way to approach the problem.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'contour-integration']"
9,Prove that a continuous function on a closed interval attains a maximum,Prove that a continuous function on a closed interval attains a maximum,,"As the title indicates, I'd like to prove the following: If $f:\mathbb R\to\mathbb R$ is a continuous function on $[a,b]$, then $f$ attains its maximum. Now, I do have a working proof: $[a,b]$ is a connected, compact space, which means that because $f$ is continuous, $f([a,b])$ is compact and connected as well.  Therefore, $f([a,b])$ is a closed interval, which means it has both a minimum and, as desired, a maximum. What I would like, however, is a proof that doesn't require such general or sophisticated framework.  In particular, I'd like to know if there's a proof that is understandable to somebody beginning calculus, one that (at the very least) doesn't invoke compactness.  Any comments, hints, or solutions are welcome and apreciated.","As the title indicates, I'd like to prove the following: If $f:\mathbb R\to\mathbb R$ is a continuous function on $[a,b]$, then $f$ attains its maximum. Now, I do have a working proof: $[a,b]$ is a connected, compact space, which means that because $f$ is continuous, $f([a,b])$ is compact and connected as well.  Therefore, $f([a,b])$ is a closed interval, which means it has both a minimum and, as desired, a maximum. What I would like, however, is a proof that doesn't require such general or sophisticated framework.  In particular, I'd like to know if there's a proof that is understandable to somebody beginning calculus, one that (at the very least) doesn't invoke compactness.  Any comments, hints, or solutions are welcome and apreciated.",,"['calculus', 'real-analysis', 'alternative-proof']"
10,Is second derivative of a convex function convex?,Is second derivative of a convex function convex?,,"If $f$ is twice differentiable and convex, is it true that $f''$ is a convex function ?","If $f$ is twice differentiable and convex, is it true that $f''$ is a convex function ?",,"['real-analysis', 'convex-analysis']"
11,"How do I prove the sequence $x_{n+1}=\sqrt{\alpha +x_n}$, with $x_0=\sqrt \alpha$ converges? ( boundedness?)","How do I prove the sequence , with  converges? ( boundedness?)",x_{n+1}=\sqrt{\alpha +x_n} x_0=\sqrt \alpha,"I need to ""study the limit behavior of the following sequence"" and then compute the limit. I can compute the limit and prove the monotonicity, but I am having trouble proving boundedness. I tried to understand from other similar posts how this could be done, but since I didn't understand I'm asking it as a new question. The sequence is $x_{n+1}=\sqrt{\alpha +x_n}, x_0=\sqrt \alpha, \alpha>0$. It seems to be monotonically increasing. Proof: It can be shown by induction that the sequence is monotonically increasing. Claim that $x_{n+1}\geq x_n$  $$x_1=\sqrt{\alpha+\sqrt\alpha}>\sqrt\alpha=x_0$$ $x_{n+1}\implies x_{n+2}$ $$x_{n+1}=\sqrt{\alpha+x_n}<\sqrt{\alpha+x_{n+1}}=\sqrt{\alpha+\sqrt{\alpha+\sqrt\alpha}}=x_{n+2}$$ Boundedness: Because the sequence is monotonically increasing and $x_o=\sqrt\alpha$, it is bounded below by $\sqrt\alpha$. Now comes the part where I have to prove that the sequence is bounded above. The problem is that I don't know how to start, so if anyone could give me a bit of a push I'd be grateful. The limit: I know that $\lim\ x_n=\lim \ x_{n+1}$, so letting $\lim\ x_n=x$ then $$x=\sqrt{\alpha+x}$$ and solving would give $x=\frac{1+\sqrt{1+4\alpha}}{2}$. May somebody please confirm my proof thus far, and help me prove the upper bound? Thanks!","I need to ""study the limit behavior of the following sequence"" and then compute the limit. I can compute the limit and prove the monotonicity, but I am having trouble proving boundedness. I tried to understand from other similar posts how this could be done, but since I didn't understand I'm asking it as a new question. The sequence is $x_{n+1}=\sqrt{\alpha +x_n}, x_0=\sqrt \alpha, \alpha>0$. It seems to be monotonically increasing. Proof: It can be shown by induction that the sequence is monotonically increasing. Claim that $x_{n+1}\geq x_n$  $$x_1=\sqrt{\alpha+\sqrt\alpha}>\sqrt\alpha=x_0$$ $x_{n+1}\implies x_{n+2}$ $$x_{n+1}=\sqrt{\alpha+x_n}<\sqrt{\alpha+x_{n+1}}=\sqrt{\alpha+\sqrt{\alpha+\sqrt\alpha}}=x_{n+2}$$ Boundedness: Because the sequence is monotonically increasing and $x_o=\sqrt\alpha$, it is bounded below by $\sqrt\alpha$. Now comes the part where I have to prove that the sequence is bounded above. The problem is that I don't know how to start, so if anyone could give me a bit of a push I'd be grateful. The limit: I know that $\lim\ x_n=\lim \ x_{n+1}$, so letting $\lim\ x_n=x$ then $$x=\sqrt{\alpha+x}$$ and solving would give $x=\frac{1+\sqrt{1+4\alpha}}{2}$. May somebody please confirm my proof thus far, and help me prove the upper bound? Thanks!",,"['real-analysis', 'sequences-and-series', 'limits']"
12,Calculate $\lim_{n\to{+}\infty}{(\sqrt{n^{2}+n}-n)}$ [duplicate],Calculate  [duplicate],\lim_{n\to{+}\infty}{(\sqrt{n^{2}+n}-n)},This question already has answers here : Closed 12 years ago . Possible Duplicate: Limits: How to evaluate $\lim\limits_{x\rightarrow \infty}\sqrt[n]{x^{n}+a_{n-1}x^{n-1}+\cdots+a_{0}}-x$ Could someone help me through this problem?  Calculate $\displaystyle\lim_{n \to{+}\infty}{(\sqrt{n^{2}+n}-n)}$,This question already has answers here : Closed 12 years ago . Possible Duplicate: Limits: How to evaluate $\lim\limits_{x\rightarrow \infty}\sqrt[n]{x^{n}+a_{n-1}x^{n-1}+\cdots+a_{0}}-x$ Could someone help me through this problem?  Calculate $\displaystyle\lim_{n \to{+}\infty}{(\sqrt{n^{2}+n}-n)}$,,"['real-analysis', 'sequences-and-series', 'limits', 'radicals']"
13,Does convergence on finite intervals imply uniform convergence?,Does convergence on finite intervals imply uniform convergence?,,"Let $f_n(x) \rightarrow 0$, $n\rightarrow \infty$ for all $x \in \mathbb{R}$. Does this imply $f_n(x) \rightarrow 0$ uniformly on finite intervals? I could think it could be proofen like this maybe: Let the interval $I$ be compact without loss of generalization Choose a finite subcover for $I$ take $N := \sup N_j$ for the $\epsilon$-$\delta$-proof But I am not sure if it is okay this way, or if it can be done more easy. Is it maybe a known lemma?","Let $f_n(x) \rightarrow 0$, $n\rightarrow \infty$ for all $x \in \mathbb{R}$. Does this imply $f_n(x) \rightarrow 0$ uniformly on finite intervals? I could think it could be proofen like this maybe: Let the interval $I$ be compact without loss of generalization Choose a finite subcover for $I$ take $N := \sup N_j$ for the $\epsilon$-$\delta$-proof But I am not sure if it is okay this way, or if it can be done more easy. Is it maybe a known lemma?",,['real-analysis']
14,the approximation of $\log(266)$?,the approximation of ?,\log(266),"Consider the following exercise: Of the following, which is the best   approximation of   $\sqrt{1.5}(266)^{1.5}$? A 1,000 B 2,700 C 3,200 D 4,100 E   5,300 The direct idea is using the ""differential approximation"": $$f(x)\approx f(x_0)+f'(x_0)(x-x_0)$$ where $f(x)=\sqrt{x}266^x$ and $x_0=1$, $x=1.5$. Finally, one may have to approximate $\log 266$. So here are my questions: How to approximate $\log 266$? Is there any other methods to answer this question?","Consider the following exercise: Of the following, which is the best   approximation of   $\sqrt{1.5}(266)^{1.5}$? A 1,000 B 2,700 C 3,200 D 4,100 E   5,300 The direct idea is using the ""differential approximation"": $$f(x)\approx f(x_0)+f'(x_0)(x-x_0)$$ where $f(x)=\sqrt{x}266^x$ and $x_0=1$, $x=1.5$. Finally, one may have to approximate $\log 266$. So here are my questions: How to approximate $\log 266$? Is there any other methods to answer this question?",,['real-analysis']
15,Non-Borel set in arbitrary metric space,Non-Borel set in arbitrary metric space,,"Most sources give non-Borel set in Euclidean space. I wonder if there is a way to construct such sets in arbitrary metric space. In particular, is there a non-borel set in $C[0,1]$ all continuous functions on $[0,1]$ where metrics is supremum.","Most sources give non-Borel set in Euclidean space. I wonder if there is a way to construct such sets in arbitrary metric space. In particular, is there a non-borel set in all continuous functions on where metrics is supremum.","C[0,1] [0,1]","['real-analysis', 'general-topology', 'functional-analysis', 'measure-theory']"
16,Cardinality of the set of multiple-representation decimals,Cardinality of the set of multiple-representation decimals,,"The set of real numbers in the open interval $(0,1)$ which have more   than one decimal expansion is (A) Empty (B)non-empty but finite (C)Countably infinite (D)uncountable I know that, $$\frac{1}{10}=.10000...=.0999999...$$               $$\frac{1}{10}=.010000...=.099999...$$               $$ \frac{1}{100}=.0010000...=.0099999...$$ $$...$$ $$\frac{1}{10^n}$$ has also the two binary representation. I think answer is (C). Am I correct? How do I prove it rigorously. Please help me. When I was doing the above question, this question came in to mind. I have question that The set of real numbers in the open interval   $(0,1)$ which have more than one binary expansion is (A) Empty (B)non-empty but finite (C)Countably infinite (D)uncountable I think the elements of the form $\frac{1}{2^n}$ has two binary expansion in $(0,1)$ . but I couldn't give the rigorous proof. Please help me.","The set of real numbers in the open interval $(0,1)$ which have more   than one decimal expansion is (A) Empty (B)non-empty but finite (C)Countably infinite (D)uncountable I know that, $$\frac{1}{10}=.10000...=.0999999...$$               $$\frac{1}{10}=.010000...=.099999...$$               $$ \frac{1}{100}=.0010000...=.0099999...$$ $$...$$ $$\frac{1}{10^n}$$ has also the two binary representation. I think answer is (C). Am I correct? How do I prove it rigorously. Please help me. When I was doing the above question, this question came in to mind. I have question that The set of real numbers in the open interval   $(0,1)$ which have more than one binary expansion is (A) Empty (B)non-empty but finite (C)Countably infinite (D)uncountable I think the elements of the form $\frac{1}{2^n}$ has two binary expansion in $(0,1)$ . but I couldn't give the rigorous proof. Please help me.",,"['real-analysis', 'elementary-set-theory']"
17,"Does $\varphi(x)\le\int_0^x\varphi(t)dt$ for all $x\in[0,\infty)$ imply $\varphi\equiv0$.",Does  for all  imply .,"\varphi(x)\le\int_0^x\varphi(t)dt x\in[0,\infty) \varphi\equiv0","Let $\varphi$ be a nonnegative and continuous function on $[0,\infty)$ and such that $$\varphi(x)\le\int_0^x\varphi(t)dt$$ for all $x\in[0,\infty)$. Can we infer from here that $\varphi\equiv0$. By taking limit on both sides, I got $\varphi(0)\le 0$. As $\varphi$ is nonnegative, I can say $\varphi(0)=0$. But what then. I could have progressed if I could show that $\varphi(x)\le \varphi'(x)$, but it is not quite evident from the given condition.","Let $\varphi$ be a nonnegative and continuous function on $[0,\infty)$ and such that $$\varphi(x)\le\int_0^x\varphi(t)dt$$ for all $x\in[0,\infty)$. Can we infer from here that $\varphi\equiv0$. By taking limit on both sides, I got $\varphi(0)\le 0$. As $\varphi$ is nonnegative, I can say $\varphi(0)=0$. But what then. I could have progressed if I could show that $\varphi(x)\le \varphi'(x)$, but it is not quite evident from the given condition.",,"['calculus', 'real-analysis', 'integration', 'derivatives']"
18,"Find $x\in \Bbb R,$ solving $x=\sqrt{1+\sqrt{1+\sqrt{1+x}}}$",Find  solving,"x\in \Bbb R, x=\sqrt{1+\sqrt{1+\sqrt{1+x}}}","Given: $x\in \mathbb R$ , $x=\sqrt{1+\sqrt{1+\sqrt{1+x}}}$ Find: numeric value of $x$ Problem from a math contest. Sorry if it is a duplicate, but could not find anything similar using the search tool. My attempt: I squared both sides, and developed the resulting expression, but I'm getting to nowhere. Hints and/or answers please. Is there any standard way to approach problems like these?","Given: , Find: numeric value of Problem from a math contest. Sorry if it is a duplicate, but could not find anything similar using the search tool. My attempt: I squared both sides, and developed the resulting expression, but I'm getting to nowhere. Hints and/or answers please. Is there any standard way to approach problems like these?",x\in \mathbb R x=\sqrt{1+\sqrt{1+\sqrt{1+x}}} x,"['real-analysis', 'algebra-precalculus', 'contest-math', 'real-numbers', 'nested-radicals']"
19,Why is a strictly monotonic mapping between intervals continuous?,Why is a strictly monotonic mapping between intervals continuous?,,"Consider the following problem: Say $a,b\in\mathbb R$, $a <b$ and $f:[a,b]\rightarrow\mathbb R $ strictly monotonic, let's say increasing for the sake of simplicity. Let $f ([a,b])=[c,d]$ for some $c,d\in\mathbb R$. Show that $f$ is continuous. This question is part of an old exam in introductory real analysis. Intuitively, I understand that a discontinuity in $f$ would imply a ""breach"" in the interval of the image. I'm having trouble with a rigorous proof, however. Edit: One of the comments on this question suggests a proof that involves upper and lower limits. As I am not very comfortable with those, I was wondering if there is a (simple) proof that abstains from their usage.","Consider the following problem: Say $a,b\in\mathbb R$, $a <b$ and $f:[a,b]\rightarrow\mathbb R $ strictly monotonic, let's say increasing for the sake of simplicity. Let $f ([a,b])=[c,d]$ for some $c,d\in\mathbb R$. Show that $f$ is continuous. This question is part of an old exam in introductory real analysis. Intuitively, I understand that a discontinuity in $f$ would imply a ""breach"" in the interval of the image. I'm having trouble with a rigorous proof, however. Edit: One of the comments on this question suggests a proof that involves upper and lower limits. As I am not very comfortable with those, I was wondering if there is a (simple) proof that abstains from their usage.",,"['real-analysis', 'continuity', 'monotone-functions']"
20,Distance between compact sets,Distance between compact sets,,"Let $K$ and $L$ be nonempty compact sets, and define  $$d = inf\{\lvert x-y \rvert : x \in K, y \in L\}$$ If $K$ and $L$ are disjoint, show $d \gt 0$ and that $d = \lvert x_{0}-y_{0} \rvert $ for some $x_{0}$ in $K$ and $y_{0}$ in $L$.","Let $K$ and $L$ be nonempty compact sets, and define  $$d = inf\{\lvert x-y \rvert : x \in K, y \in L\}$$ If $K$ and $L$ are disjoint, show $d \gt 0$ and that $d = \lvert x_{0}-y_{0} \rvert $ for some $x_{0}$ in $K$ and $y_{0}$ in $L$.",,"['real-analysis', 'general-topology']"
21,"Where does this sequence $\sqrt{7}$,$\sqrt{7+ \sqrt{7}}$,$\sqrt{7+\sqrt{7+\sqrt{7}}}$,.... converge? [duplicate]","Where does this sequence ,,,.... converge? [duplicate]",\sqrt{7} \sqrt{7+ \sqrt{7}} \sqrt{7+\sqrt{7+\sqrt{7}}},"This question already has answers here : Limit of the nested radical $x_{n+1} = \sqrt{c+x_n}$ (5 answers) Closed 4 years ago . The  given  sequence  is $\sqrt{7}$,$\sqrt{7+ \sqrt{7}}$,$\sqrt{7+\sqrt{7+\sqrt{7}}}$,.....and  so  on. the  sequence  is  increasing  so  to  converge  must  be  bounded  above.Now  looks  like  they  would not  exceed  7. The  given  options  are ${1+\sqrt{33}}\over{2}$ ${1+\sqrt{32}}\over{2}$ ${1+\sqrt{30}}\over{2}$ ${1+\sqrt{29}}\over{2}$ How  to  proceed now.  Thanks  for  any  help.","This question already has answers here : Limit of the nested radical $x_{n+1} = \sqrt{c+x_n}$ (5 answers) Closed 4 years ago . The  given  sequence  is $\sqrt{7}$,$\sqrt{7+ \sqrt{7}}$,$\sqrt{7+\sqrt{7+\sqrt{7}}}$,.....and  so  on. the  sequence  is  increasing  so  to  converge  must  be  bounded  above.Now  looks  like  they  would not  exceed  7. The  given  options  are ${1+\sqrt{33}}\over{2}$ ${1+\sqrt{32}}\over{2}$ ${1+\sqrt{30}}\over{2}$ ${1+\sqrt{29}}\over{2}$ How  to  proceed now.  Thanks  for  any  help.",,"['real-analysis', 'sequences-and-series']"
22,"Is it true that $\sin x > \frac x{\sqrt {x^2+1}} , \forall x \in (0, \frac {\pi}2)$?",Is it true that ?,"\sin x > \frac x{\sqrt {x^2+1}} , \forall x \in (0, \frac {\pi}2)","Is it true that $$\sin x > \dfrac x{\sqrt {x^2+1}} , \forall x \in \left(0, \dfrac {\pi}2\right)$$  (I tried differentiating , but it's not coming , please help)","Is it true that $$\sin x > \dfrac x{\sqrt {x^2+1}} , \forall x \in \left(0, \dfrac {\pi}2\right)$$  (I tried differentiating , but it's not coming , please help)",,['real-analysis']
23,Trying to understand the concept of limsup and liminf of sets,Trying to understand the concept of limsup and liminf of sets,,"Let $(E_n)$ be a sequence of sets. I was giving the following definitions: $$ \limsup_{n \to \infty} E_n = \bigcap_{k=1}^{\infty} \bigcup_{n \geq k} E_n $$ $$ \liminf_{n \to \infty} E_n = \bigcup_{k=1}^{\infty} \bigcap_{n \geq k} E_n $$ I am having hard time trying to understand this definitions. I was thinking on a concrete example and see how it works. For instance, let $(E_n) = \left( \dfrac{1}{n} \right) $ be sequence of sets. Then, $$ \limsup E_n = \bigcap_{k=1}^{\infty} \bigcup_{n \geq k} \left( \frac{1}{n}\right) = \bigcap_{k} \left( \frac{1}{k}\right) = \{1 \}$$ Is this correct? I am still kind of puzzled with this definition.","Let $(E_n)$ be a sequence of sets. I was giving the following definitions: $$ \limsup_{n \to \infty} E_n = \bigcap_{k=1}^{\infty} \bigcup_{n \geq k} E_n $$ $$ \liminf_{n \to \infty} E_n = \bigcup_{k=1}^{\infty} \bigcap_{n \geq k} E_n $$ I am having hard time trying to understand this definitions. I was thinking on a concrete example and see how it works. For instance, let $(E_n) = \left( \dfrac{1}{n} \right) $ be sequence of sets. Then, $$ \limsup E_n = \bigcap_{k=1}^{\infty} \bigcup_{n \geq k} \left( \frac{1}{n}\right) = \bigcap_{k} \left( \frac{1}{k}\right) = \{1 \}$$ Is this correct? I am still kind of puzzled with this definition.",,[]
24,"Given two real sequences that go to infinity, is it possible to select two subsequences that grow at the same rate asympotically?","Given two real sequences that go to infinity, is it possible to select two subsequences that grow at the same rate asympotically?",,"Given two positive real sequences $a_n$ and $b_n$ that both diverge to infinity, is it possible to choose two subsequences $a_{s_n}$ and $b_{t_n}$ such that $a_{s_n}/b_{t_n}\rightarrow1$?","Given two positive real sequences $a_n$ and $b_n$ that both diverge to infinity, is it possible to choose two subsequences $a_{s_n}$ and $b_{t_n}$ such that $a_{s_n}/b_{t_n}\rightarrow1$?",,"['real-analysis', 'sequences-and-series', 'asymptotics']"
25,Is there a proof of the irrationality of $\sqrt{2}$ that involves modular arithmetic?,Is there a proof of the irrationality of  that involves modular arithmetic?,\sqrt{2},"I was reading Ian Stewart's Concepts of Modern Mathematics . Using congruences, It's possible to explain why all perfect squares end in $0,1,4,5,6,9$ but not in $2,3,7,8$. With this I had the idea of exploring the congruences for both sides of $n^2=2m^2$ in Mathematica: Table[Mod[n^2, 9], {n, 0, 20}] Table[Mod[2 m^2, 9], {n, 0, 20}] And had the results: {0, 1, 4, 0, 7, 7, 0, 4, 1, 0, 1, 4, 0, 7, 7, 0, 4, 1, 0, 1, 4} {0, 2, 8, 0, 5, 5, 0, 8, 2, 0, 2, 8, 0, 5, 5, 0, 8, 2, 0, 2, 8} But I'm still not sure if the outputs really show what I'm looking for, I have also tried $mod \;10$. The idea is still pretty loose in my mind, I'm stuck on deciding if this proves something or what directions I could take in this enterprise.","I was reading Ian Stewart's Concepts of Modern Mathematics . Using congruences, It's possible to explain why all perfect squares end in $0,1,4,5,6,9$ but not in $2,3,7,8$. With this I had the idea of exploring the congruences for both sides of $n^2=2m^2$ in Mathematica: Table[Mod[n^2, 9], {n, 0, 20}] Table[Mod[2 m^2, 9], {n, 0, 20}] And had the results: {0, 1, 4, 0, 7, 7, 0, 4, 1, 0, 1, 4, 0, 7, 7, 0, 4, 1, 0, 1, 4} {0, 2, 8, 0, 5, 5, 0, 8, 2, 0, 2, 8, 0, 5, 5, 0, 8, 2, 0, 2, 8} But I'm still not sure if the outputs really show what I'm looking for, I have also tried $mod \;10$. The idea is still pretty loose in my mind, I'm stuck on deciding if this proves something or what directions I could take in this enterprise.",,"['real-analysis', 'abstract-algebra', 'alternative-proof']"
26,Prove that $f$ is a constant function,Prove that  is a constant function,f,"Let $f:\mathbb{R}\to\mathbb{R}$ be a function. Suppose: $$\left|\sum_{k=1}^{n}3^k(f(x+ky)-f(x-ky))\right|\leqslant 1\quad\forall n\in\mathbb{N}\quad\forall x,y\in\mathbb{R}$$ Show that $f$ is a constant function. I don't even know where to start and what is the possible approach. Any hints?","Let $f:\mathbb{R}\to\mathbb{R}$ be a function. Suppose: $$\left|\sum_{k=1}^{n}3^k(f(x+ky)-f(x-ky))\right|\leqslant 1\quad\forall n\in\mathbb{N}\quad\forall x,y\in\mathbb{R}$$ Show that $f$ is a constant function. I don't even know where to start and what is the possible approach. Any hints?",,"['calculus', 'real-analysis', 'functions']"
27,Is the set of polynomial with coefficients on $\mathbb{Q}$ enumerable?,Is the set of polynomial with coefficients on  enumerable?,\mathbb{Q},"Using the definition of enumerability of sets: A non-empty set B is enumerable iff there is a bijection $f:\mathbb{N}\supset L \rightarrow B$. So, I have to prove that the set of polynomial of one variable with coefficients on $\mathbb{Q}$ is enumerable. What I thought is that I can write every polynomial $p_0=A_0^0+A_1^0X+\dots +A_n^0X^n$ as $$p_0=(A_0^0,A_1^0,\dots ,A_n^0,\dots ),$$ where the $A_i's$ are 0 except for a finite number of them. To say that this set is enumerable is the same that to say (denoting this set by B): $$B=\{p_1,p_2,\dots\}$$ Now define the polynomial $$ p=(x_0 \neq A_0^0, x_1 \neq A_1^1, \dots), $$ where every $x_i \neq A_i^i$. This polynomial is obviously not in the list B. My question is: my approach is right to conclude that the set B is non-enumerable?","Using the definition of enumerability of sets: A non-empty set B is enumerable iff there is a bijection $f:\mathbb{N}\supset L \rightarrow B$. So, I have to prove that the set of polynomial of one variable with coefficients on $\mathbb{Q}$ is enumerable. What I thought is that I can write every polynomial $p_0=A_0^0+A_1^0X+\dots +A_n^0X^n$ as $$p_0=(A_0^0,A_1^0,\dots ,A_n^0,\dots ),$$ where the $A_i's$ are 0 except for a finite number of them. To say that this set is enumerable is the same that to say (denoting this set by B): $$B=\{p_1,p_2,\dots\}$$ Now define the polynomial $$ p=(x_0 \neq A_0^0, x_1 \neq A_1^1, \dots), $$ where every $x_i \neq A_i^i$. This polynomial is obviously not in the list B. My question is: my approach is right to conclude that the set B is non-enumerable?",,"['real-analysis', 'elementary-set-theory']"
28,Find $\lim_{n\to \infty}\int _0^{\frac{\pi}{2}} \sqrt{1+\sin^nx}$,Find,\lim_{n\to \infty}\int _0^{\frac{\pi}{2}} \sqrt{1+\sin^nx},"Define $$I_n=\int _0^{\frac{\pi}{2}} \sqrt{1+\sin^nx}\, dx$$ I have to show this sequence is convergent and find its limit. I proved it is decreasing: $\sin^{n+1} x \le \sin^n x \implies I_{n+1} \le I_n$ . Also, it is bounded because: $$0 \le \sin^n x \le 1 \implies \frac\pi{2} \le \int _0^{\frac{\pi}{2}} \sqrt{1+\sin^n x}\, dx\le \frac{\pi\sqrt{2}}{2}$$ so it is convergent. I'm stuck at finding the limit. I think it should $\frac{\pi}{2}$ but I'm not sure.","Define I have to show this sequence is convergent and find its limit. I proved it is decreasing: . Also, it is bounded because: so it is convergent. I'm stuck at finding the limit. I think it should but I'm not sure.","I_n=\int _0^{\frac{\pi}{2}} \sqrt{1+\sin^nx}\, dx \sin^{n+1} x \le \sin^n x \implies I_{n+1} \le I_n 0 \le \sin^n x \le 1 \implies \frac\pi{2} \le \int _0^{\frac{\pi}{2}} \sqrt{1+\sin^n x}\, dx\le \frac{\pi\sqrt{2}}{2} \frac{\pi}{2}","['real-analysis', 'calculus']"
29,"Evaluating integrals involving products of exponential and Bessel functions over the interval $(0,\infty)$",Evaluating integrals involving products of exponential and Bessel functions over the interval,"(0,\infty)","While trying to solve a challenging system of dual integral equations resulting from a mixed boundary value problem arising in a fluid mechanical problem, the four following non-trivial convergent improper integrals emerge: \begin{align} I_1 (r,t) &= \int_0^\infty \lambda^{\frac{1}{2}} e^{-\lambda} J_1 (\lambda r) J_{\frac{1}{2}} (\lambda t) \, \mathrm{d}\lambda \, , \\ I_2 (r,t) &= \int_0^\infty \lambda^{\frac{1}{2}} e^{-\lambda} J_0 (\lambda r) J_{\frac{3}{2}} (\lambda t) \, \mathrm{d} \lambda\, , \\ I_3 (r,t) &= \int_0^\infty \left( \lambda^{\frac{1}{2}}-\lambda^{-\frac{1}{2}} \right) e^{-\lambda} J_1 (\lambda r) J_{\frac{3}{2}} (\lambda t) \, \mathrm{d}\lambda \, , \\ I_4 (r,t) &= \int_0^\infty \left( \lambda^{\frac{1}{2}}+\lambda^{-\frac{1}{2}} \right) e^{-\lambda} J_0 (\lambda r) J_{\frac{1}{2}} (\lambda t) \, \mathrm{d}\lambda \, ,  \end{align} wherein $t$ and $r$ are positive real numbers. It can be checked that, thanks to the exponential function, these integrals are convergent. If the integrands do not contain the exponential function, then the evaluation of these integrals is easy and straightforward. Is there probably a way to evaluate these integrals analytically even as infinite convergent series functions? Any help or hint is highly appreciated!","While trying to solve a challenging system of dual integral equations resulting from a mixed boundary value problem arising in a fluid mechanical problem, the four following non-trivial convergent improper integrals emerge: wherein and are positive real numbers. It can be checked that, thanks to the exponential function, these integrals are convergent. If the integrands do not contain the exponential function, then the evaluation of these integrals is easy and straightforward. Is there probably a way to evaluate these integrals analytically even as infinite convergent series functions? Any help or hint is highly appreciated!","\begin{align}
I_1 (r,t) &= \int_0^\infty \lambda^{\frac{1}{2}}
e^{-\lambda} J_1 (\lambda r) J_{\frac{1}{2}} (\lambda t) \, \mathrm{d}\lambda \, , \\
I_2 (r,t) &= \int_0^\infty \lambda^{\frac{1}{2}}
e^{-\lambda} J_0 (\lambda r) J_{\frac{3}{2}} (\lambda t) \, \mathrm{d} \lambda\, , \\
I_3 (r,t) &= \int_0^\infty \left( \lambda^{\frac{1}{2}}-\lambda^{-\frac{1}{2}} \right)
e^{-\lambda} J_1 (\lambda r) J_{\frac{3}{2}} (\lambda t) \, \mathrm{d}\lambda \, , \\
I_4 (r,t) &= \int_0^\infty \left( \lambda^{\frac{1}{2}}+\lambda^{-\frac{1}{2}} \right)
e^{-\lambda} J_0 (\lambda r) J_{\frac{1}{2}} (\lambda t) \, \mathrm{d}\lambda \, , 
\end{align} t r","['real-analysis', 'calculus', 'integration', 'improper-integrals', 'indefinite-integrals']"
30,"Quick but not simple question. $2^\sqrt2$ or e, which is greater?","Quick but not simple question.  or e, which is greater?",2^\sqrt2,"$2^\sqrt2$ vs $e$ , which is greater? $(2^\sqrt2)^\sqrt2 = 4\quad $ & $\quad e^\sqrt2$ = ? $\log(2^\sqrt2) = \sqrt2\log(2)\quad$ & $\quad \log(e) = 1$ I tried but can't induce comparable form. Is anybody know how to prove it?","vs , which is greater? & = ? & I tried but can't induce comparable form. Is anybody know how to prove it?",2^\sqrt2 e (2^\sqrt2)^\sqrt2 = 4\quad  \quad e^\sqrt2 \log(2^\sqrt2) = \sqrt2\log(2)\quad \quad \log(e) = 1,"['real-analysis', 'analysis', 'inequality']"
31,Proof that the limit of the normal distribution for a standard deviation approximating 0 is the dirac delta function.,Proof that the limit of the normal distribution for a standard deviation approximating 0 is the dirac delta function.,,"so basically I want a proof for $\lim_{\epsilon\rightarrow 0}\frac{1}{\sqrt{2\pi\epsilon}}e^{\frac{-x^2}{2\epsilon}}=\delta(x)$ I don't yet care about proving $\int_{-\infty}^{\infty}dx\cdot\delta(x)=1$ I just want to prove that $\delta(x)=\lim_{\epsilon\rightarrow 0}\frac{1}{\sqrt{2\pi\epsilon}}e^{\frac{-x^2}{2\epsilon}}=0$ $\forall x\neq 0$ I don't have a clue how to solve the limit. I tried using L'Hospitals rule but it doesn't work and I'm completely clueless how I should be going about this. I mean, it makes sense that the limit is zero when I look at a graph of the normal distribution, but how can I prove that with equations? Thanks in advance!","so basically I want a proof for $\lim_{\epsilon\rightarrow 0}\frac{1}{\sqrt{2\pi\epsilon}}e^{\frac{-x^2}{2\epsilon}}=\delta(x)$ I don't yet care about proving $\int_{-\infty}^{\infty}dx\cdot\delta(x)=1$ I just want to prove that $\delta(x)=\lim_{\epsilon\rightarrow 0}\frac{1}{\sqrt{2\pi\epsilon}}e^{\frac{-x^2}{2\epsilon}}=0$ $\forall x\neq 0$ I don't have a clue how to solve the limit. I tried using L'Hospitals rule but it doesn't work and I'm completely clueless how I should be going about this. I mean, it makes sense that the limit is zero when I look at a graph of the normal distribution, but how can I prove that with equations? Thanks in advance!",,"['real-analysis', 'limits', 'distribution-theory']"
32,"Prove that $[0,1]$ isn't homeomorphic to $\mathbb{R}$",Prove that  isn't homeomorphic to,"[0,1] \mathbb{R}","Prove that $[0,1]$ isn't homeomorphic to $\mathbb{R}$ My first thought is that there can not be a continuous bijection, $f$, from $[0,1]$ to $\mathbb{R}$ because a continuous function that maps $[0,1]\rightarrow \mathbb{R}$ must be bounded so there can't be a surjection. So that would then be the proof. Though I believe I am incorrect in my thinking because a hint for the assignment says to use the intermediate value theorem. That is ""Suppose $f:[a,b]\rightarrow\mathbb{R}$ is continuous. If $f(a)<\delta<f(b)$ or $f(b)<\delta<f(a)$ then $\delta=f(c)$ for some $c\in[a,b]$"". Why is my first thought wrong and how is the IVT useful?","Prove that $[0,1]$ isn't homeomorphic to $\mathbb{R}$ My first thought is that there can not be a continuous bijection, $f$, from $[0,1]$ to $\mathbb{R}$ because a continuous function that maps $[0,1]\rightarrow \mathbb{R}$ must be bounded so there can't be a surjection. So that would then be the proof. Though I believe I am incorrect in my thinking because a hint for the assignment says to use the intermediate value theorem. That is ""Suppose $f:[a,b]\rightarrow\mathbb{R}$ is continuous. If $f(a)<\delta<f(b)$ or $f(b)<\delta<f(a)$ then $\delta=f(c)$ for some $c\in[a,b]$"". Why is my first thought wrong and how is the IVT useful?",,"['real-analysis', 'general-topology']"
33,My proof of uniqueness of limit (of sequence),My proof of uniqueness of limit (of sequence),,"Should I try to write a direct (i.e. non$-$by-contradiction) proof   instead of the below proof?   (I was told that mathematicians prefer direct proofs.) We consider a convergent sequence   which we denote by   $(x_n)_{n \in \mathbb{N}}$.   By definition, there is a limit (of the sequence). $\textbf{Theorem.}$   There are no two limits. $\textit{Proof.}$   We prove by contradiction.   To that end,   we assume that there are two limits.   Now, our mission is to deduce a contradiction.   Let $x,x'$ be limits such that $x \ne x'$.   By definition ($\textit{limit}$), we have   \begin{equation*}     \begin{split}      &\forall \varepsilon \in \mathbb{R}, \varepsilon > 0 :       \exists N           \in \mathbb{N}                  :       \forall n           \in \mathbb{N}, n > N           :       |x_n - x| < \varepsilon && \text{ and} \\      &\forall \varepsilon \in \mathbb{R}, \varepsilon > 0 :       \exists N           \in \mathbb{N}                  :       \forall n           \in \mathbb{N}, n > N           :       |x_n - x'| < \varepsilon.     \end{split}   \end{equation*} Since $x \ne x'$, we have $0 < \frac{1}{2} |x - x'|$.   We choose $\varepsilon := \frac{1}{2} |x - x'|$. By assumption, there are $N,N' \in \mathbb{N}$ such that   \begin{equation*}     \begin{split}      &\forall n           \in \mathbb{N}, n > N           :       |x_n - x| < \varepsilon && \text{ and} \\      &\forall n           \in \mathbb{N}, n > N'          :       |x_n - x'| < \varepsilon.     \end{split}   \end{equation*}   We choose $n := \max\{N, N'\} + 1$.   Obviously, both $n > N$ and $n > N'$.   Therefore, we have both $|x_n - x| < \varepsilon$ and $|x_n - x'| < \varepsilon$.   Thus, by adding inequalities,   \begin{equation*}     |x_n - x| + |x_n - x'| < 2 \varepsilon .   \end{equation*}   Moreover,   \begin{equation*}     \begin{split}       2 \varepsilon & =   |x - x'|                       && | \text{ by choice of } \varepsilon \\                     & =   |x + 0 - x'| \\                     & =   |x + ( - x_n + x_n) - x'| \\                     & =   |(x - x_n) + (x_n - x')| & \qquad & \\                     & \le |x - x_n| + |x_n - x'|                       && | \text{ by subadditivity of abs. val.} \\                     & =   |x_n - x| + |x_n - x'|                       && | \text{ by evenness of abs. val.} \\     \end{split}   \end{equation*}   Hence, by transitivity, we have $2 \varepsilon < 2 \varepsilon$.   Obviously, we deduced a contradiction. QED","Should I try to write a direct (i.e. non$-$by-contradiction) proof   instead of the below proof?   (I was told that mathematicians prefer direct proofs.) We consider a convergent sequence   which we denote by   $(x_n)_{n \in \mathbb{N}}$.   By definition, there is a limit (of the sequence). $\textbf{Theorem.}$   There are no two limits. $\textit{Proof.}$   We prove by contradiction.   To that end,   we assume that there are two limits.   Now, our mission is to deduce a contradiction.   Let $x,x'$ be limits such that $x \ne x'$.   By definition ($\textit{limit}$), we have   \begin{equation*}     \begin{split}      &\forall \varepsilon \in \mathbb{R}, \varepsilon > 0 :       \exists N           \in \mathbb{N}                  :       \forall n           \in \mathbb{N}, n > N           :       |x_n - x| < \varepsilon && \text{ and} \\      &\forall \varepsilon \in \mathbb{R}, \varepsilon > 0 :       \exists N           \in \mathbb{N}                  :       \forall n           \in \mathbb{N}, n > N           :       |x_n - x'| < \varepsilon.     \end{split}   \end{equation*} Since $x \ne x'$, we have $0 < \frac{1}{2} |x - x'|$.   We choose $\varepsilon := \frac{1}{2} |x - x'|$. By assumption, there are $N,N' \in \mathbb{N}$ such that   \begin{equation*}     \begin{split}      &\forall n           \in \mathbb{N}, n > N           :       |x_n - x| < \varepsilon && \text{ and} \\      &\forall n           \in \mathbb{N}, n > N'          :       |x_n - x'| < \varepsilon.     \end{split}   \end{equation*}   We choose $n := \max\{N, N'\} + 1$.   Obviously, both $n > N$ and $n > N'$.   Therefore, we have both $|x_n - x| < \varepsilon$ and $|x_n - x'| < \varepsilon$.   Thus, by adding inequalities,   \begin{equation*}     |x_n - x| + |x_n - x'| < 2 \varepsilon .   \end{equation*}   Moreover,   \begin{equation*}     \begin{split}       2 \varepsilon & =   |x - x'|                       && | \text{ by choice of } \varepsilon \\                     & =   |x + 0 - x'| \\                     & =   |x + ( - x_n + x_n) - x'| \\                     & =   |(x - x_n) + (x_n - x')| & \qquad & \\                     & \le |x - x_n| + |x_n - x'|                       && | \text{ by subadditivity of abs. val.} \\                     & =   |x_n - x| + |x_n - x'|                       && | \text{ by evenness of abs. val.} \\     \end{split}   \end{equation*}   Hence, by transitivity, we have $2 \varepsilon < 2 \varepsilon$.   Obviously, we deduced a contradiction. QED",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'proof-verification']"
34,"Every invertible linear transformation can be perturbed a bit without destroying invertbility, Neumann series","Every invertible linear transformation can be perturbed a bit without destroying invertbility, Neumann series",,Let $T: V \to V$ be any linear transformation on a real or complex vector space $V$. Show that there exists $\epsilon_0 > 0$ $($depending on $T$$)$ so that $I + \epsilon T$ is invertible for any $|\epsilon| < \epsilon_0$. Now suppose $S: V\to V$ is an invertible linear transformation and let $T$ be as in $(1)$. Show that there exists $\epsilon_0 > 0$ $($depending on $S$ and $T)$ so that $S + \epsilon T$ is invertible for any $|\epsilon| < \epsilon_0$. For such a fundamental fact in the study of linear algebra I cannot find a proof of this anywhere... can anyone supply a proof or refer me to a place where I can find one?,Let $T: V \to V$ be any linear transformation on a real or complex vector space $V$. Show that there exists $\epsilon_0 > 0$ $($depending on $T$$)$ so that $I + \epsilon T$ is invertible for any $|\epsilon| < \epsilon_0$. Now suppose $S: V\to V$ is an invertible linear transformation and let $T$ be as in $(1)$. Show that there exists $\epsilon_0 > 0$ $($depending on $S$ and $T)$ so that $S + \epsilon T$ is invertible for any $|\epsilon| < \epsilon_0$. For such a fundamental fact in the study of linear algebra I cannot find a proof of this anywhere... can anyone supply a proof or refer me to a place where I can find one?,,"['real-analysis', 'linear-algebra']"
35,Why do we need min to choose $\delta$?,Why do we need min to choose ?,\delta,"On this thread: Problem: The person uses $\delta = \text {min} (\frac{\epsilon}{2}, \frac{1}{2})$ why do we need the min function to determine $\delta$? Thanks!","On this thread: Problem: The person uses $\delta = \text {min} (\frac{\epsilon}{2}, \frac{1}{2})$ why do we need the min function to determine $\delta$? Thanks!",,"['calculus', 'real-analysis', 'analysis', 'limits', 'epsilon-delta']"
36,How can we prove $\int_1^\pi x \cos(\frac1{x}) dx<4$ by hand?,How can we prove  by hand?,\int_1^\pi x \cos(\frac1{x}) dx<4,"Is there any way we can prove this definite integral inequality by hand: $$ \int_{1}^{\pi}x\cos\left(1 \over x\right)\,{\rm d}x < 4 $$  I don't where to start even, please help. That $\displaystyle\cos\left(1 \over x\right)\ \leq\ 1$ doesn't seem to help because $\displaystyle\int_{1}^{\pi}x\,{\rm d}x\ >\ 4$.","Is there any way we can prove this definite integral inequality by hand: $$ \int_{1}^{\pi}x\cos\left(1 \over x\right)\,{\rm d}x < 4 $$  I don't where to start even, please help. That $\displaystyle\cos\left(1 \over x\right)\ \leq\ 1$ doesn't seem to help because $\displaystyle\int_{1}^{\pi}x\,{\rm d}x\ >\ 4$.",,"['calculus', 'real-analysis', 'integration', 'inequality', 'definite-integrals']"
37,Integral $\int_0^\infty \frac{\sin^2 ax}{x(1-e^x)}dx=\frac{1}{4}\log\left( \frac{2a\pi}{\sinh 2a\pi}\right)$,Integral,\int_0^\infty \frac{\sin^2 ax}{x(1-e^x)}dx=\frac{1}{4}\log\left( \frac{2a\pi}{\sinh 2a\pi}\right),How can we prove this ${\it{interesting}}$ integral $$ I:=\int_0^\infty \frac{\sin^2 ax}{x(1-e^x)}dx=\frac{1}{4}\log\left( \frac{2a\pi}{\sinh 2a\pi}\right) $$ I to write $$ I=\frac{1}{2}\int_0^\infty \frac{(1-\cos a x)}{x}dx \sum_{n=0}^\infty e^{nx} $$ simplifying $$ I=\frac{1}{2} \int_0^\infty \sum_{n=0}^\infty e^{nx}\frac{dx}{x} -\frac{1}{2}\int_0^\infty \frac{\cos a x}{x}dx \sum_{n=0}^\infty e^{nx} $$ but didn't help much.  Thank you,How can we prove this ${\it{interesting}}$ integral $$ I:=\int_0^\infty \frac{\sin^2 ax}{x(1-e^x)}dx=\frac{1}{4}\log\left( \frac{2a\pi}{\sinh 2a\pi}\right) $$ I to write $$ I=\frac{1}{2}\int_0^\infty \frac{(1-\cos a x)}{x}dx \sum_{n=0}^\infty e^{nx} $$ simplifying $$ I=\frac{1}{2} \int_0^\infty \sum_{n=0}^\infty e^{nx}\frac{dx}{x} -\frac{1}{2}\int_0^\infty \frac{\cos a x}{x}dx \sum_{n=0}^\infty e^{nx} $$ but didn't help much.  Thank you,,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
38,Simple Integral $\int_0^\infty (1-x\cot^{-1} x)dx=\frac{\pi}{4}$.,Simple Integral .,\int_0^\infty (1-x\cot^{-1} x)dx=\frac{\pi}{4},"Hellow I am trying to prove this result. $$ I:=\int_0^\infty (1-x\cot^{-1} x)dx=\frac{\pi}{4}. $$ The indefinite integral exists for this integral. The function $\cot^{-1} x$ is the arc-cotangent function, not the multiplicative inverse.  Note, we can not just break the integral up into two pieces because we will have problems with divergence. Using the relation  $$ x\cot^{-1} x=x \tan^{-1} \frac{1}{x},\to \quad  I=\int_0^\infty \left(1-x\tan^{-1} \frac{1}{x}\right)dx $$ may be of help to some but didn't help me.  I am not sure if I am missing a clever substitution, perhaps integration by parts will work, I obtained using this method $$ I=(x-x^2\cot^{-1} x)\big|^\infty_0-\int_0^\infty \frac{x^2}{x^2+1}dx+\int_0^\infty x\cot^{-1} x \, dx $$ but this is clearly a problem since we have divergence issue now. The indefinite integral is given by $$ \int (1-x\cot^{-1} x ) dx =\frac{1}{2}\left(x+\tan^{-1} x-x^2 \cot^{-1} x\right) $$ but I am unable to prove this result.  Thanks.","Hellow I am trying to prove this result. $$ I:=\int_0^\infty (1-x\cot^{-1} x)dx=\frac{\pi}{4}. $$ The indefinite integral exists for this integral. The function $\cot^{-1} x$ is the arc-cotangent function, not the multiplicative inverse.  Note, we can not just break the integral up into two pieces because we will have problems with divergence. Using the relation  $$ x\cot^{-1} x=x \tan^{-1} \frac{1}{x},\to \quad  I=\int_0^\infty \left(1-x\tan^{-1} \frac{1}{x}\right)dx $$ may be of help to some but didn't help me.  I am not sure if I am missing a clever substitution, perhaps integration by parts will work, I obtained using this method $$ I=(x-x^2\cot^{-1} x)\big|^\infty_0-\int_0^\infty \frac{x^2}{x^2+1}dx+\int_0^\infty x\cot^{-1} x \, dx $$ but this is clearly a problem since we have divergence issue now. The indefinite integral is given by $$ \int (1-x\cot^{-1} x ) dx =\frac{1}{2}\left(x+\tan^{-1} x-x^2 \cot^{-1} x\right) $$ but I am unable to prove this result.  Thanks.",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
39,Prove that function is not Lebesgue integrable,Prove that function is not Lebesgue integrable,,"Prove that function $f(x,y)=\dfrac{1}{x^2+y^2}$ is not Lebesgue integrable on $A=(0,1]\times(0,1]$. To my knowledge the fastest way to do it is to use Fubini's theorem. From what I would get: $$\int_0^1 \frac{1}{x^2+y^2}dy=\frac{1}{x^3}\operatorname{arctg}\left(\frac{y}{x}\right)\Big|^1_0=\frac{1}{x^3}\operatorname{arctg}\left(\frac{1}{x}\right)$$ $$\int_0^1 \frac{1}{x^3}\operatorname{arctg}\left(\frac{1}{x}\right)\,dx=\cdots$$ Which is quite a lot of computing and I have a hard time believing that's the point of this task. So my question is: How do I prove this function is not integrable using Fubini's theorem the fastest way possible? EDIT. There are already two solutions posted to this question, both of which use  substitution of variables but I'd like to avoid if possible, because at the point of doing this exercise we didn't know that theorem. Can anyone think any other solution (also other than just computing that complicated integral).","Prove that function $f(x,y)=\dfrac{1}{x^2+y^2}$ is not Lebesgue integrable on $A=(0,1]\times(0,1]$. To my knowledge the fastest way to do it is to use Fubini's theorem. From what I would get: $$\int_0^1 \frac{1}{x^2+y^2}dy=\frac{1}{x^3}\operatorname{arctg}\left(\frac{y}{x}\right)\Big|^1_0=\frac{1}{x^3}\operatorname{arctg}\left(\frac{1}{x}\right)$$ $$\int_0^1 \frac{1}{x^3}\operatorname{arctg}\left(\frac{1}{x}\right)\,dx=\cdots$$ Which is quite a lot of computing and I have a hard time believing that's the point of this task. So my question is: How do I prove this function is not integrable using Fubini's theorem the fastest way possible? EDIT. There are already two solutions posted to this question, both of which use  substitution of variables but I'd like to avoid if possible, because at the point of doing this exercise we didn't know that theorem. Can anyone think any other solution (also other than just computing that complicated integral).",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus', 'lebesgue-integral']"
40,Did I take the derivative correctly? $x^y=y^x$,Did I take the derivative correctly?,x^y=y^x,Need to differentiate following equation: $$x^y=y^x$$ My attempt: $$x^y\log(x) \cdot y' = y^x\log(y)\cdot1;  $$ $$y'=\frac{y^x\log(y)}{x^y\log(x)}$$ Please tell me if I've made a mistake.,Need to differentiate following equation: $$x^y=y^x$$ My attempt: $$x^y\log(x) \cdot y' = y^x\log(y)\cdot1;  $$ $$y'=\frac{y^x\log(y)}{x^y\log(x)}$$ Please tell me if I've made a mistake.,,"['real-analysis', 'derivatives', 'solution-verification']"
41,Determining whether or not spaces are separable,Determining whether or not spaces are separable,,"I've been going over practice problems, and I ran into this one. I was wondering if anyone could help me out with the following problem. Let $X$ be a metric space of all bounded sequences $(a_n) \subset \mathbb{R}$ with the metric defined by    $$d( (a_n), (b_n)) = \sup\{ |a_n - b_n| : n = 1, 2, \ldots \}.$$   Let $Y \subset X$ be the subspace of all sequences converging to zero.   Determine whether or not $X$ and $Y$ are separable. Thanks in advance!","I've been going over practice problems, and I ran into this one. I was wondering if anyone could help me out with the following problem. Let $X$ be a metric space of all bounded sequences $(a_n) \subset \mathbb{R}$ with the metric defined by    $$d( (a_n), (b_n)) = \sup\{ |a_n - b_n| : n = 1, 2, \ldots \}.$$   Let $Y \subset X$ be the subspace of all sequences converging to zero.   Determine whether or not $X$ and $Y$ are separable. Thanks in advance!",,"['real-analysis', 'general-topology', 'metric-spaces']"
42,"to show $f(t)=g(t)$ for some $t\in [0,1]$",to show  for some,"f(t)=g(t) t\in [0,1]","Let $f,g:[0,1] \rightarrow \mathbb{R}$ be non-negative, continuous functions so that $$\sup_{x \in [0,1]} f(x)= \sup_{x \in [0,1]} g(x).$$ We need To show $f(t)=g(t)$ for some $t\in [0,1].$ Thank you for help.","Let $f,g:[0,1] \rightarrow \mathbb{R}$ be non-negative, continuous functions so that $$\sup_{x \in [0,1]} f(x)= \sup_{x \in [0,1]} g(x).$$ We need To show $f(t)=g(t)$ for some $t\in [0,1].$ Thank you for help.",,['real-analysis']
43,Proof of exponent rule $a^{n/m}$ when $n$ and $m$ belongs in the reals,Proof of exponent rule  when  and  belongs in the reals,a^{n/m} n m,"Now I have not studied math for very long. I have just completed Calculus1, although my knowledge extends a bit outside of this. My question is, how can the transforms below be justified $$ a^{n/m} \, = \, \left( a^{1/m} \right)^{n}  \, = \, \left( a^{n} \right)^{1/m} $$ when $n$ and $m$ can be any real number? Now the statement above is easily proven for the natural numbers. For an example we can can see that these rules makes sense when dealing with real numbers. $$ 8^{2/3} \, = \, \left( 8^{1/3} \right)^{2}  \, = \, \left( 8^{2} \right)^{1/3} $$ But I just can not wrap my head around how we can justify the below algebraic manipulations $$  2^{\pi/e} \, = \, \left( 2^{1/e} \right)^{\pi}  \, = \, \left( 2^{\pi} \right)^{1/e} $$ As I have problems understanding what $2^{\pi} $ means. How would one go about multiplying a number by itself $\pi$ times? Someone said that this exponent rule, could be explained by something called dedikins cuts, or something like that. Any insight to answer my silly questions is greatly appreciated. Is there a general proof of exponent rule $a^{n/m}$ when $n$ and $m$ belongs in the reals?","Now I have not studied math for very long. I have just completed Calculus1, although my knowledge extends a bit outside of this. My question is, how can the transforms below be justified $$ a^{n/m} \, = \, \left( a^{1/m} \right)^{n}  \, = \, \left( a^{n} \right)^{1/m} $$ when $n$ and $m$ can be any real number? Now the statement above is easily proven for the natural numbers. For an example we can can see that these rules makes sense when dealing with real numbers. $$ 8^{2/3} \, = \, \left( 8^{1/3} \right)^{2}  \, = \, \left( 8^{2} \right)^{1/3} $$ But I just can not wrap my head around how we can justify the below algebraic manipulations $$  2^{\pi/e} \, = \, \left( 2^{1/e} \right)^{\pi}  \, = \, \left( 2^{\pi} \right)^{1/e} $$ As I have problems understanding what $2^{\pi} $ means. How would one go about multiplying a number by itself $\pi$ times? Someone said that this exponent rule, could be explained by something called dedikins cuts, or something like that. Any insight to answer my silly questions is greatly appreciated. Is there a general proof of exponent rule $a^{n/m}$ when $n$ and $m$ belongs in the reals?",,['real-analysis']
44,Is $\sum \sin{\frac{\pi}{n}}$ convergent?,Is  convergent?,\sum \sin{\frac{\pi}{n}},I have to test for convergence of the series: $\displaystyle \sum\limits_{n=1}^{\infty} \sin\Bigl(\frac{\pi}{n}\Bigr)$ What i did was \begin{align*} \sin\Bigl(\frac{\pi}{n}\Bigr)+ \sin\Bigl(\frac{\pi}{n+1}\Bigr) + \cdots  & < \pi \biggl( \frac{1}{n+1} + \frac{1}{n+2} + \cdots \biggr) \\ &= \pi \biggl( \sum\limits_{r=1}^{\infty} \frac{1}{n+r}\biggr) =\int\limits_{0}^{1} \frac{1}{1+x} \ dx \\ &= \pi\log{2} \end{align*} I think this proves the convergence of the series. I am interesting in knowing some more methods which can be used to prove the convergence so that i can apply them. ADDED: Note that $$\lim_{n \to \infty} \sum\limits_{r=1}^{n} \frac{1}{n} \cdot f\Bigl(\frac{r}{n}\Bigr) = \int\limits_{0}^{1} f(x) \ \textrm{dx}$$,I have to test for convergence of the series: $\displaystyle \sum\limits_{n=1}^{\infty} \sin\Bigl(\frac{\pi}{n}\Bigr)$ What i did was \begin{align*} \sin\Bigl(\frac{\pi}{n}\Bigr)+ \sin\Bigl(\frac{\pi}{n+1}\Bigr) + \cdots  & < \pi \biggl( \frac{1}{n+1} + \frac{1}{n+2} + \cdots \biggr) \\ &= \pi \biggl( \sum\limits_{r=1}^{\infty} \frac{1}{n+r}\biggr) =\int\limits_{0}^{1} \frac{1}{1+x} \ dx \\ &= \pi\log{2} \end{align*} I think this proves the convergence of the series. I am interesting in knowing some more methods which can be used to prove the convergence so that i can apply them. ADDED: Note that $$\lim_{n \to \infty} \sum\limits_{r=1}^{n} \frac{1}{n} \cdot f\Bigl(\frac{r}{n}\Bigr) = \int\limits_{0}^{1} f(x) \ \textrm{dx}$$,,['real-analysis']
45,Closed form for $\int_0^1\frac{(x^3-3x^2+x)\log(x-x^2)}{(x^2-x+1)^3}\mathrm dx$,Closed form for,\int_0^1\frac{(x^3-3x^2+x)\log(x-x^2)}{(x^2-x+1)^3}\mathrm dx,"I am looking for a closed form for $$I=\int_0^1\frac{(x^3-3x^2+x)\log(x-x^2)}{(x^2-x+1)^3}\mathrm dx\approx 0.851035604949$$ Wolfram does not evaluate $I$ . I suspect $I$ has a closed form, because if $I$ has a closed form then the answer to my question In Pascal's triangle without the $1$ s, what is the sum of squares of reciprocals? has a closed form. In that question, I give reasons for why I suspect a closed form answer. But of course I could be wrong. My attempt Here is the graph of $y=\frac{(x^3-3x^2+x)\log(x-x^2)}{(x^2-x+1)^3}$ . I thought about translating the function to an odd function in order to take advantage of rotational symmetry, but the curve clearly does not have rotational symmetry. I tried substitution and Maclaurin series, but got nowhere.","I am looking for a closed form for Wolfram does not evaluate . I suspect has a closed form, because if has a closed form then the answer to my question In Pascal's triangle without the s, what is the sum of squares of reciprocals? has a closed form. In that question, I give reasons for why I suspect a closed form answer. But of course I could be wrong. My attempt Here is the graph of . I thought about translating the function to an odd function in order to take advantage of rotational symmetry, but the curve clearly does not have rotational symmetry. I tried substitution and Maclaurin series, but got nowhere.",I=\int_0^1\frac{(x^3-3x^2+x)\log(x-x^2)}{(x^2-x+1)^3}\mathrm dx\approx 0.851035604949 I I I 1 y=\frac{(x^3-3x^2+x)\log(x-x^2)}{(x^2-x+1)^3},"['real-analysis', 'calculus', 'integration', 'definite-integrals', 'closed-form']"
46,Why does ${\lim_{x\to2}}\;(x-4)^x$ not exist?,Why does  not exist?,{\lim_{x\to2}}\;(x-4)^x,"Here's a question from the calculus section of AEEE 1997: $${\lim_{x\to2}}\;(x-4)^x$$ I saw this question in a very old paper of the AEEE exam ( $90's$ edition of the JEE exam). So when I saw this question, I obviously, wrote $4$ as the answer. Now the answerkey said Limit does not exist. I believed that maybe it has something to do with left hand and right hand limit and I tried referring some books on limits and understood how RHL and LHL should exist to say limit should exist but all those books have demonstrated this concept with problems involving greatest integer function or cases where limit tends to 0 which is pretty simple and understandable. But I cant seem to apply that concept here as x approaches 2 from Right hand and Left hand side which is all positive. So could anybody explain me the concept behind this. (And I would appreciate graphical representations!!! although not necessary) Also not I'm a high school student and starting calculus so please try to make the answer in simple language. EDIT 1 : After reading some links from mathematics SE, I understood a possible reason for this could be ' If exponentiation $x^y$ of real numbers $x$ and $y$ is defined as $e^{y \log x}$ , then one cannot define things like $(-2)^4$ ' So that creates whole new level of confusion now for me. In a cubic equation say $x^3+....$ , if one of the roots of the solution is -2, then...how can $x=-2$ if $(-2)^3$ isn't possible using the above logic? EDIT 2 (Very important) For anyone answering, please also illustrate if or why $(x+4)^x$ should exist.","Here's a question from the calculus section of AEEE 1997: I saw this question in a very old paper of the AEEE exam ( edition of the JEE exam). So when I saw this question, I obviously, wrote as the answer. Now the answerkey said Limit does not exist. I believed that maybe it has something to do with left hand and right hand limit and I tried referring some books on limits and understood how RHL and LHL should exist to say limit should exist but all those books have demonstrated this concept with problems involving greatest integer function or cases where limit tends to 0 which is pretty simple and understandable. But I cant seem to apply that concept here as x approaches 2 from Right hand and Left hand side which is all positive. So could anybody explain me the concept behind this. (And I would appreciate graphical representations!!! although not necessary) Also not I'm a high school student and starting calculus so please try to make the answer in simple language. EDIT 1 : After reading some links from mathematics SE, I understood a possible reason for this could be ' If exponentiation of real numbers and is defined as , then one cannot define things like ' So that creates whole new level of confusion now for me. In a cubic equation say , if one of the roots of the solution is -2, then...how can if isn't possible using the above logic? EDIT 2 (Very important) For anyone answering, please also illustrate if or why should exist.",{\lim_{x\to2}}\;(x-4)^x 90's 4 x^y x y e^{y \log x} (-2)^4 x^3+.... x=-2 (-2)^3 (x+4)^x,"['real-analysis', 'calculus', 'limits', 'graphing-functions', 'limits-without-lhopital']"
47,Limit of $\frac{1}{\sqrt[n] n}$,Limit of,\frac{1}{\sqrt[n] n},"I would like to show that the limit of the sequence $(\frac{1}{\sqrt[n] n})_{n=1}^{\infty}$ is equal to 1 using the definition of the limit of a sequence. (Another way to say this is that it converges to 1). Definition: A sequence $(a_n)_{n=1}^{\infty}$ converges to a real number A iff for each $\epsilon > 0$ there is a positive integer N such that for all $n \geq N$ we have $|a_n - A| < \epsilon$ . Edit: Here is my work. Let $\epsilon > 0$ . There exists $N$ such that for $n \geq N$ , $$|\frac{1}{\sqrt[n] n}-1| = |\frac{n^{\frac{n-1}{n}}}{n} - 1| = |\frac{n^{\frac{n-1}{n}} - n}{n}| = \frac{n-n^{\frac{n-1}{n}}}{n}$$ Here is where I am stuck.","I would like to show that the limit of the sequence is equal to 1 using the definition of the limit of a sequence. (Another way to say this is that it converges to 1). Definition: A sequence converges to a real number A iff for each there is a positive integer N such that for all we have . Edit: Here is my work. Let . There exists such that for , Here is where I am stuck.",(\frac{1}{\sqrt[n] n})_{n=1}^{\infty} (a_n)_{n=1}^{\infty} \epsilon > 0 n \geq N |a_n - A| < \epsilon \epsilon > 0 N n \geq N |\frac{1}{\sqrt[n] n}-1| = |\frac{n^{\frac{n-1}{n}}}{n} - 1| = |\frac{n^{\frac{n-1}{n}} - n}{n}| = \frac{n-n^{\frac{n-1}{n}}}{n},"['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'epsilon-delta']"
48,Difference between a number and a set with one number,Difference between a number and a set with one number,,"I am studying Analysis in Tao's textbook and he mentions the following: The set $\{3, \{3, 4\}, 4\}$ is a set of three distinct elements, one of which happens to itself be a set of two elements. However, not all objects are sets; for instance, we typically do not consider a natural number such as $3$ to be a set. (The more accurate statement is that natural numbers can be the cardinalities of sets, rather than necessarily being sets themselves) but my question is wouldn't $\{3\}$ be a set of cardinality one? Is this just a matter of notation (if we include the curly braces then we consider it a set and if not then it's just a natural number)?","I am studying Analysis in Tao's textbook and he mentions the following: The set is a set of three distinct elements, one of which happens to itself be a set of two elements. However, not all objects are sets; for instance, we typically do not consider a natural number such as to be a set. (The more accurate statement is that natural numbers can be the cardinalities of sets, rather than necessarily being sets themselves) but my question is wouldn't be a set of cardinality one? Is this just a matter of notation (if we include the curly braces then we consider it a set and if not then it's just a natural number)?","\{3, \{3, 4\}, 4\} 3 \{3\}","['real-analysis', 'elementary-set-theory']"
49,"Show that $f'(c) +9 \int_{0}^{1/3} f(x) \, dx = 0$ for some $c$ if $\int_{0}^{1} f(x) \, dx = 0$",Show that  for some  if,"f'(c) +9 \int_{0}^{1/3} f(x) \, dx = 0 c \int_{0}^{1} f(x) \, dx = 0","Let $f:[0, 1] \rightarrow \mathbb{R} $ a differentiable function such that $$ \int_{0}^{1} f(x) \, dx = 0 \, . $$ Prove that there exists a $c \in (0, 1)$ such that $$ f'(c) +9 \int_{0}^{1/3} f(x) \, dx = 0 \, . $$ I think that the solution of this problem can be obtained by applying the mean value theorem for integrals. Since the integral of $f$ on $[0, 1]$ is $0$ , we obtain that $$ 0 = \int_{0}^{1} f(x) \, dx=f(a) (1-0) = f(a)  $$ for some $a\in (0, 1)$ . Next $$ 0 = \int_{0}^{1} f(x) \,  dx=\int_{0}^{1/3} f(x) \, dx + \int_{1/3}^{1}f(x) \, dx \, , $$ so we have to prove that there exists a point $c \in (0, 1) $ such that $$ f'(c) = 9 \int_{1/3}^{1}f(x) \, dx \, . $$ From now on I don't have any idea. I just verified that for the function $f(x)  =2x-1$ , the assertion is true.","Let a differentiable function such that Prove that there exists a such that I think that the solution of this problem can be obtained by applying the mean value theorem for integrals. Since the integral of on is , we obtain that for some . Next so we have to prove that there exists a point such that From now on I don't have any idea. I just verified that for the function , the assertion is true.","f:[0, 1] \rightarrow \mathbb{R}   \int_{0}^{1} f(x) \, dx = 0 \, .  c \in (0, 1)  f'(c) +9 \int_{0}^{1/3} f(x) \, dx = 0 \, .  f [0, 1] 0 
0 = \int_{0}^{1} f(x) \, dx=f(a) (1-0) = f(a) 
 a\in (0, 1) 
0 = \int_{0}^{1} f(x) \,  dx=\int_{0}^{1/3} f(x) \, dx + \int_{1/3}^{1}f(x) \, dx \, ,
 c \in (0, 1)  
f'(c) = 9 \int_{1/3}^{1}f(x) \, dx \, .
 f(x)  =2x-1","['real-analysis', 'calculus', 'integration', 'means']"
50,Integrate $\int \frac{1}{1+x+x^4}dx$,Integrate,\int \frac{1}{1+x+x^4}dx,"Find $$\int \frac{{\rm d}x}{1+x+x^4}.$$ WA gives a result, which introduces complex numbers. But according to the algebraic fundamental theorem, any rational fraction can be integrated over the real number field. How to do this?","Find WA gives a result, which introduces complex numbers. But according to the algebraic fundamental theorem, any rational fraction can be integrated over the real number field. How to do this?",\int \frac{{\rm d}x}{1+x+x^4}.,"['real-analysis', 'calculus', 'integration', 'indefinite-integrals', 'quartics']"
51,Can I get a sequence of bounded functions converging pointwise to $f(x)=1/x$ for $x$ non zero and $0$ for $x$ zero?,Can I get a sequence of bounded functions converging pointwise to  for  non zero and  for  zero?,f(x)=1/x x 0 x,How do I construct an explicit sequence of bounded functions converging pointwise to $f(x)=1/x$ for non zero $x$ and $f(0)=0$ . It would be better if someone may find a continuous and even better if someone gives a differentiable sequence of functions on $\Bbb R$ converging to $f$ pointwise.,How do I construct an explicit sequence of bounded functions converging pointwise to for non zero and . It would be better if someone may find a continuous and even better if someone gives a differentiable sequence of functions on converging to pointwise.,f(x)=1/x x f(0)=0 \Bbb R f,"['real-analysis', 'sequences-and-series']"
52,Why iterated limits are different from simultaneous limits?,Why iterated limits are different from simultaneous limits?,,"There is sequence $a_{m,n}=\frac{m}{m+n}$ we calculate the following limits $$\lim_{n\rightarrow\infty}\lim_{m\rightarrow\infty}a_{m,n} \qquad \lim_{m\rightarrow\infty}\lim_{n\rightarrow\infty}a_{m,n}$$ I find both of these limits to be $1,0$ respectively. But the limit of $$\lim_{m,n\rightarrow\infty}a_{m,n}=0.5$$ should be $0.5$ because we have a denominator that will be twice the numerator for very large but comparable values of $m,n$. What is the notion of limits in this situation? Edit: Can the simultaneous limit be written like this Since $m,n\rightarrow \infty \implies m \approx n\implies \lim_{m,n\rightarrow \infty}a_{m,n}=\lim_{n\rightarrow \infty}\frac{n}{n+n}=0.5$","There is sequence $a_{m,n}=\frac{m}{m+n}$ we calculate the following limits $$\lim_{n\rightarrow\infty}\lim_{m\rightarrow\infty}a_{m,n} \qquad \lim_{m\rightarrow\infty}\lim_{n\rightarrow\infty}a_{m,n}$$ I find both of these limits to be $1,0$ respectively. But the limit of $$\lim_{m,n\rightarrow\infty}a_{m,n}=0.5$$ should be $0.5$ because we have a denominator that will be twice the numerator for very large but comparable values of $m,n$. What is the notion of limits in this situation? Edit: Can the simultaneous limit be written like this Since $m,n\rightarrow \infty \implies m \approx n\implies \lim_{m,n\rightarrow \infty}a_{m,n}=\lim_{n\rightarrow \infty}\frac{n}{n+n}=0.5$",,"['real-analysis', 'limits']"
53,Integrating :$\int_{0}^{1} \frac{x^{b}-x^{a}}{\log(x)}\sin(\log(x))dx$,Integrating :,\int_{0}^{1} \frac{x^{b}-x^{a}}{\log(x)}\sin(\log(x))dx,Consider $$\int_{0}^{1} \frac{x^{b}-x^{a}}{\log(x)}\sin(\log(x))dx$$ I thought about making some substitution like : $x^{y}$(it's easy to calculate $$\int_{0}^{1}\frac{x^{b}-x^{a}}{\log(x)}dx = \int_{a}^{b}dy\int_{0}^{1}x^{y}dx=\log(b-1)-\log(a-1)$$). But I couldn't find substitution like that. Any hints?,Consider $$\int_{0}^{1} \frac{x^{b}-x^{a}}{\log(x)}\sin(\log(x))dx$$ I thought about making some substitution like : $x^{y}$(it's easy to calculate $$\int_{0}^{1}\frac{x^{b}-x^{a}}{\log(x)}dx = \int_{a}^{b}dy\int_{0}^{1}x^{y}dx=\log(b-1)-\log(a-1)$$). But I couldn't find substitution like that. Any hints?,,"['calculus', 'real-analysis', 'integration', 'sequences-and-series', 'improper-integrals']"
54,Show that $a_{n+1}= 1 + \frac{1}{a_n}$ converges. [duplicate],Show that  converges. [duplicate],a_{n+1}= 1 + \frac{1}{a_n},"This question already has answers here : Prove that the sequence $(a_n)$ defined by $a_0 = 1$, $a_{n+1} = 1 + \frac 1{a_n}$ is convergent in $\mathbb{R}$ (4 answers) Closed 6 years ago . Let $\{a_n\}$ be defined by $a_1 =1  $ and $a_{n+1} = 1 + {\dfrac{1}{a_n}}$ with $n \in N$ . Show that $a_{n+1}= 1 + {\dfrac{1}{a_n}}$ converges. I know the limit but how can I show that is a Cauchy sequence or that this sequence converges.","This question already has answers here : Prove that the sequence $(a_n)$ defined by $a_0 = 1$, $a_{n+1} = 1 + \frac 1{a_n}$ is convergent in $\mathbb{R}$ (4 answers) Closed 6 years ago . Let be defined by and with . Show that converges. I know the limit but how can I show that is a Cauchy sequence or that this sequence converges.",\{a_n\} a_1 =1   a_{n+1} = 1 + {\dfrac{1}{a_n}} n \in N a_{n+1}= 1 + {\dfrac{1}{a_n}},"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'recurrence-relations']"
55,General structure of the proof that every compact metric space is the continuous image of the Cantor set,General structure of the proof that every compact metric space is the continuous image of the Cantor set,,I am reading the book General Topology by Stephen Willard and I have seen the theorem Every compact metric space is a continuous image of the Cantor set. The book also presents its proof. The proof is very lengthy and I am struggling to understand it. Could someone provide an outline of the general structure of the proof?,I am reading the book General Topology by Stephen Willard and I have seen the theorem Every compact metric space is a continuous image of the Cantor set. The book also presents its proof. The proof is very lengthy and I am struggling to understand it. Could someone provide an outline of the general structure of the proof?,,"['real-analysis', 'general-topology', 'metric-spaces', 'compactness', 'cantor-set']"
56,"f is continuous, show that f(closure) is a subset of closure of f","f is continuous, show that f(closure) is a subset of closure of f",,"If $f:X\rightarrow Y$ is continuous and $E\subset X$, prove that $f(\overline{E})\subset \overline{f(E)}$. Provide an example to show that the inclusion does not have to be equality. So far what I have is that the preimage of a closed set in $Y$ is closed in $X$. So $f^{-1}(\overline{f(E)})$ is closed and contains $E$.","If $f:X\rightarrow Y$ is continuous and $E\subset X$, prove that $f(\overline{E})\subset \overline{f(E)}$. Provide an example to show that the inclusion does not have to be equality. So far what I have is that the preimage of a closed set in $Y$ is closed in $X$. So $f^{-1}(\overline{f(E)})$ is closed and contains $E$.",,['real-analysis']
57,Continuous function on $\mathbb{Q}$,Continuous function on,\mathbb{Q},Let $f:\mathbb{Q}\to\mathbb{R}$ be a function defined as: $$f(x) =  \begin{cases}     0 &  x^2 < 2\\     1 &  x^2 \geq 2 \end{cases} $$ Is this function continuous? How can we check the continuity around $\sqrt{2}$ since it's not in $\mathbb{Q}$?,Let $f:\mathbb{Q}\to\mathbb{R}$ be a function defined as: $$f(x) =  \begin{cases}     0 &  x^2 < 2\\     1 &  x^2 \geq 2 \end{cases} $$ Is this function continuous? How can we check the continuity around $\sqrt{2}$ since it's not in $\mathbb{Q}$?,,"['calculus', 'real-analysis', 'continuity']"
58,Proof that the derivative of an extremum is $0$,Proof that the derivative of an extremum is,0,"My notes say that if $f(x_0)$ is an extremum then $f'(x_0)=0$, but i'm having trouble proving it. I've shown that if  $f'(x_0)>0$ then $\exists h>0$ such that $\forall x_1 ,x_2 \in (x_0-h,x_0+h),\; x_1<x_0<x_2 \implies f(x_1)<f(x_0)<f(x_2)$ The result apparently follows trivially from this, but I can't do it. My first thought was to say that if $f(x_0)$ is a maximum then the right inequality doesn't hold and if $f(x_0)$ is a minimum then the left inequality doesn't hold, but I don't see how this implies that $f'(x_0) =0$. Can anyone help?","My notes say that if $f(x_0)$ is an extremum then $f'(x_0)=0$, but i'm having trouble proving it. I've shown that if  $f'(x_0)>0$ then $\exists h>0$ such that $\forall x_1 ,x_2 \in (x_0-h,x_0+h),\; x_1<x_0<x_2 \implies f(x_1)<f(x_0)<f(x_2)$ The result apparently follows trivially from this, but I can't do it. My first thought was to say that if $f(x_0)$ is a maximum then the right inequality doesn't hold and if $f(x_0)$ is a minimum then the left inequality doesn't hold, but I don't see how this implies that $f'(x_0) =0$. Can anyone help?",,"['real-analysis', 'derivatives']"
59,"If $ \lim_{n \to \infty} (2 x_{n + 1} - x_{n}) = x $, then is it true that $ \lim_{n \to \infty} x_{n} = x $?","If , then is it true that ?", \lim_{n \to \infty} (2 x_{n + 1} - x_{n}) = x   \lim_{n \to \infty} x_{n} = x ,"If $ (x_{n})_{n \in \mathbb{N}} $ is a sequence in $ \mathbb{R} $ and $ \displaystyle \lim_{n \to \infty} (2 x_{n + 1} - x_{n}) = x $, then is it necessarily true that $$ \lim_{n \to \infty} x_{n} = x? $$ Could anyone kindly offer suggestions on how to approach this problem? Thanks!","If $ (x_{n})_{n \in \mathbb{N}} $ is a sequence in $ \mathbb{R} $ and $ \displaystyle \lim_{n \to \infty} (2 x_{n + 1} - x_{n}) = x $, then is it necessarily true that $$ \lim_{n \to \infty} x_{n} = x? $$ Could anyone kindly offer suggestions on how to approach this problem? Thanks!",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
60,$\displaystyle\sum_{n=2}^{\infty}\frac{2}{(n^3-n)3^n} = -\frac{1}{2}+\frac{4}{3}\sum_{n=1}^{\infty}\frac{1}{n\cdot 3^n}$,,\displaystyle\sum_{n=2}^{\infty}\frac{2}{(n^3-n)3^n} = -\frac{1}{2}+\frac{4}{3}\sum_{n=1}^{\infty}\frac{1}{n\cdot 3^n},"Please help me, to prove that $$ \sum_{n=2}^{\infty}\frac{2}{(n^3-n)3^n} =   -\frac{1}{2}+\frac{4}{3}\sum_{n=1}^{\infty}\frac{1}{n\cdot 3^n}. $$","Please help me, to prove that $$ \sum_{n=2}^{\infty}\frac{2}{(n^3-n)3^n} =   -\frac{1}{2}+\frac{4}{3}\sum_{n=1}^{\infty}\frac{1}{n\cdot 3^n}. $$",,"['real-analysis', 'sequences-and-series']"
61,Prove that $\sum_{n=1}^{\infty}\ a_n^2$ is convergent if $\sum_{n=1}^{\infty}\ a_n$ is absolutely convergent [duplicate],Prove that  is convergent if  is absolutely convergent [duplicate],\sum_{n=1}^{\infty}\ a_n^2 \sum_{n=1}^{\infty}\ a_n,"This question already has answers here : If $\sum_{n=1}^{\infty} a_n$ is absolutely convergent, then $\sum_{n=1}^{\infty} (a_n)^2$ is convergent [closed] (2 answers) Closed 10 years ago . Suppose that $\displaystyle\sum_{n=1}^{\infty}\ a_n$ is absolutely convergent. How can we prove that $\displaystyle\sum_{n=1}^{\infty}\ a_n^2$ is convergent?","This question already has answers here : If $\sum_{n=1}^{\infty} a_n$ is absolutely convergent, then $\sum_{n=1}^{\infty} (a_n)^2$ is convergent [closed] (2 answers) Closed 10 years ago . Suppose that $\displaystyle\sum_{n=1}^{\infty}\ a_n$ is absolutely convergent. How can we prove that $\displaystyle\sum_{n=1}^{\infty}\ a_n^2$ is convergent?",,"['real-analysis', 'sequences-and-series']"
62,Show uncountable set of real numbers has a point of accumulation [closed],Show uncountable set of real numbers has a point of accumulation [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Show that every uncountable set of real numbers has a point of accumulation.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Show that every uncountable set of real numbers has a point of accumulation.",,"['real-analysis', 'general-topology', 'analysis']"
63,Limit exercise from Rudin: $\lim\limits_{n \to \infty} \sqrt{n^2+n} -n$ [duplicate],Limit exercise from Rudin:  [duplicate],\lim\limits_{n \to \infty} \sqrt{n^2+n} -n,"This question already has answers here : Calculate $\lim_{n\to{+}\infty}{(\sqrt{n^{2}+n}-n)}$ [duplicate] (3 answers) Closed 4 years ago . This is Chapter 3, Exercise 2 of Rudin's Principles. Calculate $\lim\limits_{n \to \infty} \sqrt{n^2+n} -n$. Hints will be appreciated.","This question already has answers here : Calculate $\lim_{n\to{+}\infty}{(\sqrt{n^{2}+n}-n)}$ [duplicate] (3 answers) Closed 4 years ago . This is Chapter 3, Exercise 2 of Rudin's Principles. Calculate $\lim\limits_{n \to \infty} \sqrt{n^2+n} -n$. Hints will be appreciated.",,"['real-analysis', 'limits', 'radicals']"
64,Is this sum related to the Gregory's limit?,Is this sum related to the Gregory's limit?,,Compute the series $$\sum_{k=0}^{\infty}\left(\frac{1}{4k+1} - \frac{1}{4k+2}\right)$$,Compute the series $$\sum_{k=0}^{\infty}\left(\frac{1}{4k+1} - \frac{1}{4k+2}\right)$$,,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
65,Computing $ \sum_{n=1}^{\infty} \frac{\lfloor{\sqrt{n+1}}\rfloor-\lfloor{\sqrt{n}}\rfloor}{n}$,Computing, \sum_{n=1}^{\infty} \frac{\lfloor{\sqrt{n+1}}\rfloor-\lfloor{\sqrt{n}}\rfloor}{n},I would like to prove the existence and the exact value of the following series: $$ \sum_{n=1}^{\infty} \frac{\lfloor{\sqrt{n+1}}\rfloor-\lfloor{\sqrt{n}}\rfloor}{n}$$,I would like to prove the existence and the exact value of the following series: $$ \sum_{n=1}^{\infty} \frac{\lfloor{\sqrt{n+1}}\rfloor-\lfloor{\sqrt{n}}\rfloor}{n}$$,,"['real-analysis', 'sequences-and-series']"
66,Evaluate $\sum\limits_{k=1}^{\infty}\frac{(18)[(k-1)!]^2}{(2k)!}$,Evaluate,\sum\limits_{k=1}^{\infty}\frac{(18)[(k-1)!]^2}{(2k)!},Just as the topic ask how to evaluate $$\sum_{k=1}^{\infty}\frac{(18)[(k-1)!]^2}{(2k)!}.$$,Just as the topic ask how to evaluate $$\sum_{k=1}^{\infty}\frac{(18)[(k-1)!]^2}{(2k)!}.$$,,"['real-analysis', 'sequences-and-series']"
67,Looking for a counterexample for this statement regarding divergent series,Looking for a counterexample for this statement regarding divergent series,,"I need counter for this statement : If the series $ \sum a_n$ is divergent then the series $b_n$ = $\sum \text{min}(a_n,\frac{1}{n})$ is also divergent. I closest I reached to a counter is , Define $a_n$ as $$a_n =  \begin{cases} \frac{1}{n²} \text{for n even} \\ \frac{1}{n} \text{(any divegrent series) for n odd} \end{cases} $$ Here, $\sum a_n$ is divergent. Because $\sum 1/n²$ is convergent but $\sum 1/n$ is not. So their sum is divergent. Now, in $b_n$ At times when $n$ is even $a_n$ (is $\sum 1/n²$ will converge )  will be less than $1/n$ and could be a counter but at odd it will cause problem when because when $n$ is odd there will be $1/n$ equal to it and is divergent but i need it to converge. If this statement is true then a proof is needed. Kindly Help. Thank You.","I need counter for this statement : If the series is divergent then the series = is also divergent. I closest I reached to a counter is , Define as Here, is divergent. Because is convergent but is not. So their sum is divergent. Now, in At times when is even (is will converge )  will be less than and could be a counter but at odd it will cause problem when because when is odd there will be equal to it and is divergent but i need it to converge. If this statement is true then a proof is needed. Kindly Help. Thank You."," \sum a_n b_n \sum \text{min}(a_n,\frac{1}{n}) a_n a_n =  \begin{cases} \frac{1}{n²} \text{for n even} \\ \frac{1}{n} \text{(any divegrent series) for n odd} \end{cases}  \sum a_n \sum 1/n² \sum 1/n b_n n a_n \sum 1/n² 1/n n 1/n","['real-analysis', 'sequences-and-series', 'convergence-divergence', 'examples-counterexamples']"
68,$\int_0^\infty \frac{x}{(e^{2\pi x}-1)(x^2+1)^2}dx$?,?,\int_0^\infty \frac{x}{(e^{2\pi x}-1)(x^2+1)^2}dx,"How to calculate integral $\int_0^\infty \frac{x}{(e^{2\pi x}-1)(x^2+1)^2}dx$ ? I got this integral by using Abel-Plana formula on series $\sum_{n=0}^\infty \frac{1}{(n+1)^2}$ . This integral can be splitted into two integrals with bounds from 0 to 1 and from 1 to infinity and the both integrals converge, so does the sum. I checked with WolframAlpha and the value of the integral is $\frac{-9 + \pi^2}{24}$ , but I don't know how to compute it. Also, I tried to write $\frac{2xdx}{(1+x^2)^2}=d\frac{1}{x^2+1}$ and then tried to use partial integration, but didn't succeded. Any help is welcome. Thanks in advance.","How to calculate integral ? I got this integral by using Abel-Plana formula on series . This integral can be splitted into two integrals with bounds from 0 to 1 and from 1 to infinity and the both integrals converge, so does the sum. I checked with WolframAlpha and the value of the integral is , but I don't know how to compute it. Also, I tried to write and then tried to use partial integration, but didn't succeded. Any help is welcome. Thanks in advance.",\int_0^\infty \frac{x}{(e^{2\pi x}-1)(x^2+1)^2}dx \sum_{n=0}^\infty \frac{1}{(n+1)^2} \frac{-9 + \pi^2}{24} \frac{2xdx}{(1+x^2)^2}=d\frac{1}{x^2+1},"['real-analysis', 'calculus', 'integration', 'complex-analysis', 'definite-integrals']"
69,Assume that $\int_{a}^{ab} f(x) dx$ is independent of $a$. Prove $f(x)=\frac{c}{x}$,Assume that  is independent of . Prove,\int_{a}^{ab} f(x) dx a f(x)=\frac{c}{x},"Assume $f$ is integrable on $[0,\infty)$ and assume that for $a,b>0$ , the value of $\int_{a}^{ab} f(x) dx$ is independent of $a$ . Prove that $f(x)=\frac{c}{x}$ , where $c$ is a constant. I have tried several things, like showing $g(x)=xf(x)$ must have derivative zero, but I am unsure how to use the integral assumption. Thanks!","Assume is integrable on and assume that for , the value of is independent of . Prove that , where is a constant. I have tried several things, like showing must have derivative zero, but I am unsure how to use the integral assumption. Thanks!","f [0,\infty) a,b>0 \int_{a}^{ab} f(x) dx a f(x)=\frac{c}{x} c g(x)=xf(x)","['real-analysis', 'calculus']"
70,$\int_{0}^{\infty }{{{x}^{n}}\sin \left( {{x}^{1/4}} \right)\exp \left( -{{x}^{1/4}} \right)dx}=0$,,\int_{0}^{\infty }{{{x}^{n}}\sin \left( {{x}^{1/4}} \right)\exp \left( -{{x}^{1/4}} \right)dx}=0,"I'd like to show that for all positive integers $n$ we have $$I\left( n \right)=\int_{0}^{\infty }{{{x}^{n}}\sin \left( {{x}^{1/4}} \right)\exp \left( -{{x}^{1/4}} \right)dx}=0.$$ This is true after some computer experiments, besides after setting $x={{u}^{4}}$ we get $$I\left( n \right)=4\int_{0}^{\infty }{{{u}^{4n+3}}\sin \left( u \right)\exp \left( -u \right)du}.$$ But how to proceed? Integration by parts maybe?","I'd like to show that for all positive integers we have This is true after some computer experiments, besides after setting we get But how to proceed? Integration by parts maybe?",n I\left( n \right)=\int_{0}^{\infty }{{{x}^{n}}\sin \left( {{x}^{1/4}} \right)\exp \left( -{{x}^{1/4}} \right)dx}=0. x={{u}^{4}} I\left( n \right)=4\int_{0}^{\infty }{{{u}^{4n+3}}\sin \left( u \right)\exp \left( -u \right)du}.,"['real-analysis', 'definite-integrals', 'riemann-integration']"
71,Finding $\lim_{n\rightarrow\infty }\frac{\frac{1}{2}+\frac{\sqrt 2}{3}+\dots+\frac{\sqrt n}{n+1}}{\sqrt n}$,Finding,\lim_{n\rightarrow\infty }\frac{\frac{1}{2}+\frac{\sqrt 2}{3}+\dots+\frac{\sqrt n}{n+1}}{\sqrt n},Find $$\lim_{n\rightarrow\infty }\frac{\frac{1}{2}+\frac{\sqrt 2}{3}+\dots+\frac{\sqrt n}{n+1}}{\sqrt n}$$ My work $$\lim_{n\rightarrow\infty }\frac{\frac{1}{2}+\frac{\sqrt 2}{3}+\dots+\frac{\sqrt n}{n+1}}{\sqrt n}=\frac{\sum_{m=1}^n\frac{\sqrt m}{m+1}}{\sqrt n}$$ The series $$\sum_{m=1}^\infty \frac{\sqrt m}{m+1}$$ does not converge so can I say $\lim_{n\rightarrow\infty }\frac{\frac{1}{2}+\frac{\sqrt 2}{3}+\dots+\frac{\sqrt n}{n+1}}{\sqrt n}$ does not exist?,Find My work The series does not converge so can I say does not exist?,\lim_{n\rightarrow\infty }\frac{\frac{1}{2}+\frac{\sqrt 2}{3}+\dots+\frac{\sqrt n}{n+1}}{\sqrt n} \lim_{n\rightarrow\infty }\frac{\frac{1}{2}+\frac{\sqrt 2}{3}+\dots+\frac{\sqrt n}{n+1}}{\sqrt n}=\frac{\sum_{m=1}^n\frac{\sqrt m}{m+1}}{\sqrt n} \sum_{m=1}^\infty \frac{\sqrt m}{m+1} \lim_{n\rightarrow\infty }\frac{\frac{1}{2}+\frac{\sqrt 2}{3}+\dots+\frac{\sqrt n}{n+1}}{\sqrt n},"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
72,"Problem involving Fundamental Theorem of Calculus: simplifying $\frac{\mathrm d}{\mathrm dx}\int_x^{x^2}\frac{t}{\log t}\,\mathrm dt$",Problem involving Fundamental Theorem of Calculus: simplifying,"\frac{\mathrm d}{\mathrm dx}\int_x^{x^2}\frac{t}{\log t}\,\mathrm dt","I'm working on the following question: Simplify the following: $$\frac{\mathrm d}{\mathrm dx}\int_x^{x^2}\frac{t}{\log t}\,\mathrm dt$$ The solution key says this simplifies to: $$2x\frac{x^2}{\log(x^2)}-\frac{x}{\log(x)}$$ I think this wrong though. For two reasons: We can't just use fundamental theorem of calculus (FTC) because the integrand needs to be continuous on the interval of interest. The quotient of continuous functions is continuous, provided the denominator is non-zero. There's a problem though at $t=1$ . FTC is stated with one end fixed -- can we just assume both ends are functions? Given these caveats, I'm not sure how to proceed -- thoughts?","I'm working on the following question: Simplify the following: The solution key says this simplifies to: I think this wrong though. For two reasons: We can't just use fundamental theorem of calculus (FTC) because the integrand needs to be continuous on the interval of interest. The quotient of continuous functions is continuous, provided the denominator is non-zero. There's a problem though at . FTC is stated with one end fixed -- can we just assume both ends are functions? Given these caveats, I'm not sure how to proceed -- thoughts?","\frac{\mathrm d}{\mathrm dx}\int_x^{x^2}\frac{t}{\log t}\,\mathrm dt 2x\frac{x^2}{\log(x^2)}-\frac{x}{\log(x)} t=1","['calculus', 'real-analysis']"
73,$\lim\limits_{n\to \infty} \frac{1}{n}\cdot \big((m+1)(m+2) \ldots(m+n)\big)^{\frac{1}{n}}$ where $m$ is a fixed positive integer is?,where  is a fixed positive integer is?,\lim\limits_{n\to \infty} \frac{1}{n}\cdot \big((m+1)(m+2) \ldots(m+n)\big)^{\frac{1}{n}} m,$$\lim_{n\to \infty} \frac{1}{n}\cdot \big((m+1)(m+2) \ldots(m+n)\big)^{\frac{1}{n}}$$ where $m$ is a fixed positive integer. Here is my attempt: According to Cauchy's theorem of limit if $\lim\limits_{n\to \infty}a_n=l$ then $\lim{(a_1a_2 \ldots a_n)}^{\frac{1}{n}}=l$ hence $\lim\limits_{n\to \infty}\frac {m+n}{n}$ $\Rightarrow \lim\limits_{n\to\infty}\left(1+\frac{m}{n}\right)=1$ I'm 90 percent clear that my solution is correct. If not then please give me the right solution.,where is a fixed positive integer. Here is my attempt: According to Cauchy's theorem of limit if then hence I'm 90 percent clear that my solution is correct. If not then please give me the right solution.,\lim_{n\to \infty} \frac{1}{n}\cdot \big((m+1)(m+2) \ldots(m+n)\big)^{\frac{1}{n}} m \lim\limits_{n\to \infty}a_n=l \lim{(a_1a_2 \ldots a_n)}^{\frac{1}{n}}=l \lim\limits_{n\to \infty}\frac {m+n}{n} \Rightarrow \lim\limits_{n\to\infty}\left(1+\frac{m}{n}\right)=1,"['real-analysis', 'sequences-and-series', 'limits']"
74,Finding $\lim x_n$ when $\left( 1+\frac{1}{n}\right)^{n+x_n}=1+\frac{1}{1!}+\frac{1}{2!}+\dots+\frac{1}{n!}$,Finding  when,\lim x_n \left( 1+\frac{1}{n}\right)^{n+x_n}=1+\frac{1}{1!}+\frac{1}{2!}+\dots+\frac{1}{n!},"Let $x_n$ be the unique solution of the equation $$\left( 1+\frac{1}{n}\right)^{n+x_n}=1+\frac{1}{1!}+\frac{1}{2!}+\dots+\frac{1}{n!}$$   Find $\lim_{n \to \infty} x_n$ I think that the limit must be $\frac{1}{2}$, because $\left(1+\frac{1}{n}\right)^{n+\frac{1}{2}}$ is decreasing and convergent to $e$, while $1+\frac{1}{1!}+\dots+\frac{1}{n!}$ is increasing and convergent to $e$, so $$\left(1+\frac{1}{n}\right)^{n+\frac{1}{2}}>1+\frac{1}{1!}+\dots+\frac{1}{n!}$$ which means  that $\frac{1}{2}>x_n$. I also know that for $a \in [0,\frac{1}{2}), \left(1+\frac{1}{n}\right)^{n+a}$ is eventually increasing, but I don't know how to get a lower bound for $x_n$ which goes to $\frac{1}{2}$","Let $x_n$ be the unique solution of the equation $$\left( 1+\frac{1}{n}\right)^{n+x_n}=1+\frac{1}{1!}+\frac{1}{2!}+\dots+\frac{1}{n!}$$   Find $\lim_{n \to \infty} x_n$ I think that the limit must be $\frac{1}{2}$, because $\left(1+\frac{1}{n}\right)^{n+\frac{1}{2}}$ is decreasing and convergent to $e$, while $1+\frac{1}{1!}+\dots+\frac{1}{n!}$ is increasing and convergent to $e$, so $$\left(1+\frac{1}{n}\right)^{n+\frac{1}{2}}>1+\frac{1}{1!}+\dots+\frac{1}{n!}$$ which means  that $\frac{1}{2}>x_n$. I also know that for $a \in [0,\frac{1}{2}), \left(1+\frac{1}{n}\right)^{n+a}$ is eventually increasing, but I don't know how to get a lower bound for $x_n$ which goes to $\frac{1}{2}$",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
75,Prove that these functions are linearly independent,Prove that these functions are linearly independent,,"Given $ \alpha_1 > \alpha_2 > \cdots > \alpha_n \geq 0$, $f_1(x)= e^{-\alpha_1x},..., f_n(x)=e^{-\alpha_nx}$, prove that they are linearly independent . Hint: don’t forget the limit $ x \rightarrow \infty $. I have already seen proofs about similar questions. What you have to do is to set the combination of those functions (or polynomials) equal to 0 and show that all coefficients have to be 0. But here, the coefficients are in the exponent, they can’t be equal to each other, and the negative exponent disturbs me a bit. Thanks for your help.","Given $ \alpha_1 > \alpha_2 > \cdots > \alpha_n \geq 0$, $f_1(x)= e^{-\alpha_1x},..., f_n(x)=e^{-\alpha_nx}$, prove that they are linearly independent . Hint: don’t forget the limit $ x \rightarrow \infty $. I have already seen proofs about similar questions. What you have to do is to set the combination of those functions (or polynomials) equal to 0 and show that all coefficients have to be 0. But here, the coefficients are in the exponent, they can’t be equal to each other, and the negative exponent disturbs me a bit. Thanks for your help.",,['real-analysis']
76,Number of Real solutions of $e^{x^2}=ex$,Number of Real solutions of,e^{x^2}=ex,Find a number of real  solutions of the following equation.    $$e^{x^2}=ex$$ Need a pure calculus approach..,Find a number of real  solutions of the following equation.    $$e^{x^2}=ex$$ Need a pure calculus approach..,,"['calculus', 'real-analysis', 'algebra-precalculus', 'derivatives', 'exponential-function']"
77,Proving that a sequence of functions converge uniformly,Proving that a sequence of functions converge uniformly,,"I have a sequence of functions: $$S_n(x) = x\frac{1-x^n}{1-x}, \text{ where } x \in (-1, 1)$$ It is known that: $$\lim_{n \to \infty}S_n(x) = \frac{x}{1-x}$$ thus $S_n(x)$ converges pointwise. I am to prove that is does not converge uniformly. I tried to estimate: $$\left|S_n(x) - \frac{x}{1-x}\right|$$ using an $x \in (-1, 1)$ but it didn't work. I would appreciate any help.","I have a sequence of functions: $$S_n(x) = x\frac{1-x^n}{1-x}, \text{ where } x \in (-1, 1)$$ It is known that: $$\lim_{n \to \infty}S_n(x) = \frac{x}{1-x}$$ thus $S_n(x)$ converges pointwise. I am to prove that is does not converge uniformly. I tried to estimate: $$\left|S_n(x) - \frac{x}{1-x}\right|$$ using an $x \in (-1, 1)$ but it didn't work. I would appreciate any help.",,"['real-analysis', 'sequences-and-series', 'uniform-convergence']"
78,Proper clopen subset of disconnected set,Proper clopen subset of disconnected set,,"In Chapman Pugh's Real Analysis the definition of a disconnected set is that it has a proper clopen subset. I was trying to apply that to a simple example but got the following inconsistency: The disconnected set $U=[a,b]\cup[c,d]\subset\mathbb R$ where $a<b<c<d$ should have a proper clopen subset. However, $\mathbb R$ has no clopen proper subsets and any proper subset of $U$ is also a proper subset of $\mathbb R$ which is a contradiction. Where am I wrong in my reasoning? Thanks in advance!","In Chapman Pugh's Real Analysis the definition of a disconnected set is that it has a proper clopen subset. I was trying to apply that to a simple example but got the following inconsistency: The disconnected set $U=[a,b]\cup[c,d]\subset\mathbb R$ where $a<b<c<d$ should have a proper clopen subset. However, $\mathbb R$ has no clopen proper subsets and any proper subset of $U$ is also a proper subset of $\mathbb R$ which is a contradiction. Where am I wrong in my reasoning? Thanks in advance!",,"['real-analysis', 'general-topology', 'connectedness']"
79,Evaluating the integral $\int \frac{x^2+x}{(e^x+x+1)^2}dx$,Evaluating the integral,\int \frac{x^2+x}{(e^x+x+1)^2}dx,"Evaluate $$\int \frac{x^2+x}{(e^x+x+1)^2}dx$$ I tried converting in the form of Quotient rule(seeing the square in the denominator), neither am I able to make the denominators' derivative in the numerator. Some hints would be great. Thanks.","Evaluate $$\int \frac{x^2+x}{(e^x+x+1)^2}dx$$ I tried converting in the form of Quotient rule(seeing the square in the denominator), neither am I able to make the denominators' derivative in the numerator. Some hints would be great. Thanks.",,"['calculus', 'real-analysis']"
80,Why exactly does a function need to be continuous on a closed interval for the intermediate value theorem to apply?,Why exactly does a function need to be continuous on a closed interval for the intermediate value theorem to apply?,,"I apologize if this question is too basic. The intermediate value theorem states that if $f$ is continuous on a closed interval $[a,b]$, then for every value $c$ between $f(a)$ and $f(b)$ there exists some $x \in (a,b)$ such that $f(x) = c$. This, or some very similar variant thereof, is how the intermediate value is usually presented in textbooks. What bugs me, however, is the condition that $f$ need be continuous on the closed interval $[a,b]$ rather than the less strict condition of only being continuous on the open interval $(a,b)$. To illustrate this point, consider $f:[-1,1] \rightarrow \mathbb{R}$ where $f(x)= e^x$. This is only continuous on the open interval $(-1,1)$ but surely the IVT applies to it. A less artificial example would be the inverse sine function. Would this reasoning not apply to all such (continuous/well-behaved) functions? As an alternative, wouldn't the following definition from Proofwiki be superior (in that it is slightly more general)? Let $I$ be a real interval. Let $a,b \in I$ such that $(a,b)$ is an open interval. Let $f:I \rightarrow \mathbb{R}$ be a real function continuous in $(a,b)$. Then for every value $c \in \mathbb{R}$ between $f(a)$ and $f(b)$ there exists some $x \in (a,b)$ such that $f(x) = c$","I apologize if this question is too basic. The intermediate value theorem states that if $f$ is continuous on a closed interval $[a,b]$, then for every value $c$ between $f(a)$ and $f(b)$ there exists some $x \in (a,b)$ such that $f(x) = c$. This, or some very similar variant thereof, is how the intermediate value is usually presented in textbooks. What bugs me, however, is the condition that $f$ need be continuous on the closed interval $[a,b]$ rather than the less strict condition of only being continuous on the open interval $(a,b)$. To illustrate this point, consider $f:[-1,1] \rightarrow \mathbb{R}$ where $f(x)= e^x$. This is only continuous on the open interval $(-1,1)$ but surely the IVT applies to it. A less artificial example would be the inverse sine function. Would this reasoning not apply to all such (continuous/well-behaved) functions? As an alternative, wouldn't the following definition from Proofwiki be superior (in that it is slightly more general)? Let $I$ be a real interval. Let $a,b \in I$ such that $(a,b)$ is an open interval. Let $f:I \rightarrow \mathbb{R}$ be a real function continuous in $(a,b)$. Then for every value $c \in \mathbb{R}$ between $f(a)$ and $f(b)$ there exists some $x \in (a,b)$ such that $f(x) = c$",,"['calculus', 'real-analysis']"
81,Why does the domain and range of $\sqrt x$ contain only positive real numbers?,Why does the domain and range of  contain only positive real numbers?,\sqrt x,"Consider the function $f(x)=\sqrt x$. Pretty much every resource I can find gives its domain as $\{x\in \mathbb{R} \mid x \ge 0\}$ and its range as $\{y\in \mathbb{R} \mid y \ge 0\}$ I don't see why the domain needs to be restricted to positive numbers. Isn't the square root of a negative number defined? That is, the square root of a negative number is a complex number of the form $a + bi$. So why isn't the domain of $f(x)=\sqrt x$ instead $\{x\in \mathbb{R}\}$ and its range $\{y\in \mathbb{C} \mid y \ge 0\}$? What am I missing?","Consider the function $f(x)=\sqrt x$. Pretty much every resource I can find gives its domain as $\{x\in \mathbb{R} \mid x \ge 0\}$ and its range as $\{y\in \mathbb{R} \mid y \ge 0\}$ I don't see why the domain needs to be restricted to positive numbers. Isn't the square root of a negative number defined? That is, the square root of a negative number is a complex number of the form $a + bi$. So why isn't the domain of $f(x)=\sqrt x$ instead $\{x\in \mathbb{R}\}$ and its range $\{y\in \mathbb{C} \mid y \ge 0\}$? What am I missing?",,"['real-analysis', 'functions', 'elementary-set-theory']"
82,"Prove $xe^x =2$ for some $x \in (0,1)$",Prove  for some,"xe^x =2 x \in (0,1)","We are trying to prove $xe^x =2$ for some $x \in (0,1)$. I know for certain that if this question were asking to prove the equality for some $x$ in the closed interval $[0,1]$ then I could apply the intermediate value theorem: Let $f(x) = xe^x$ implying: $f(0) = 0 \lt 2$ and $f(1) = 1\cdot e^1=e \gt 2$ and by the intermediate value theorem there must exist some $x \in [0,1]$ such that $f(x)=2$. My question is if we can use the Intermediate Value Theorem in my original question - where we have $x \in (0,1)$ the open interval. I know the discrepancy arises since the interval is open and there is no explicit definition of the function at the end-points $0$ and $1$. Thank you guys!","We are trying to prove $xe^x =2$ for some $x \in (0,1)$. I know for certain that if this question were asking to prove the equality for some $x$ in the closed interval $[0,1]$ then I could apply the intermediate value theorem: Let $f(x) = xe^x$ implying: $f(0) = 0 \lt 2$ and $f(1) = 1\cdot e^1=e \gt 2$ and by the intermediate value theorem there must exist some $x \in [0,1]$ such that $f(x)=2$. My question is if we can use the Intermediate Value Theorem in my original question - where we have $x \in (0,1)$ the open interval. I know the discrepancy arises since the interval is open and there is no explicit definition of the function at the end-points $0$ and $1$. Thank you guys!",,"['real-analysis', 'continuity']"
83,"Which of the following sets are dense in $C[0,1]$",Which of the following sets are dense in,"C[0,1]","Which of the following sets are dense in $C[0,1]$ with respect to sup-norm topology? $1$. {$f$$\in$ $C[0,1]$ : $f$ is a polynomial } $2$. {$f$$\in$ $C[0,1]$ :$f(0)$=$0$} $3$. {$f$$\in$ $C[0,1]$ :$f(0)$$\neq$$0$} $4$. {$f$$\in$$C[0,1]$ :$\int_{0}^{1}f(x)dx$$=$5} I am thinking to apply Stone-weierstrass theorem but I don't know how to use it properly. Please help!","Which of the following sets are dense in $C[0,1]$ with respect to sup-norm topology? $1$. {$f$$\in$ $C[0,1]$ : $f$ is a polynomial } $2$. {$f$$\in$ $C[0,1]$ :$f(0)$=$0$} $3$. {$f$$\in$ $C[0,1]$ :$f(0)$$\neq$$0$} $4$. {$f$$\in$$C[0,1]$ :$\int_{0}^{1}f(x)dx$$=$5} I am thinking to apply Stone-weierstrass theorem but I don't know how to use it properly. Please help!",,"['real-analysis', 'general-topology', 'metric-spaces']"
84,Is $\int_x^{\infty}e^{-\frac{t^2}{2}} < \frac{1}{x}e^{-\frac{x^2}{2}}$?,Is ?,\int_x^{\infty}e^{-\frac{t^2}{2}} < \frac{1}{x}e^{-\frac{x^2}{2}},"While solving a problem in real analysis, I got stuck. I need to prove $$\int_x^{\infty}e^{-\frac{t^2}{2}}dt <  \frac{1}{x}e^{-\frac{x^2}{2}} $$ Clearly I have to use some kind of inequality, but cant figure out how to proceed further. Thanks for the help.","While solving a problem in real analysis, I got stuck. I need to prove $$\int_x^{\infty}e^{-\frac{t^2}{2}}dt <  \frac{1}{x}e^{-\frac{x^2}{2}} $$ Clearly I have to use some kind of inequality, but cant figure out how to proceed further. Thanks for the help.",,"['real-analysis', 'integration', 'inequality', 'exponential-function']"
85,Integral $\int_0^{\infty}\frac{e^{-x}-e^{-2x}}{x}dx$,Integral,\int_0^{\infty}\frac{e^{-x}-e^{-2x}}{x}dx,"I am working on the improper integral: $$\int_0^{\infty}\frac{e^{-x}-e^{-2x}}{x}dx$$ This function does not have an elementary anti-derivative, so here is what I did: define: $$f(t):=\int_0^{\infty}\frac{e^{-xt}-e^{-2xt}}{x}dx,\quad t>0$$ Then differentiation gives: $$f'(t)=\int_0^{\infty}\frac{-xe^{-xt}+2xe^{-2xt}}{x}dx=\int_0^{\infty}-e^{-xt}+2e^{-2xt}dx=0$$ this means $f$ is constant. I feel something is wrong here because $f$ should depend on $t$. Where am I wrong and what is the right way to do this?","I am working on the improper integral: $$\int_0^{\infty}\frac{e^{-x}-e^{-2x}}{x}dx$$ This function does not have an elementary anti-derivative, so here is what I did: define: $$f(t):=\int_0^{\infty}\frac{e^{-xt}-e^{-2xt}}{x}dx,\quad t>0$$ Then differentiation gives: $$f'(t)=\int_0^{\infty}\frac{-xe^{-xt}+2xe^{-2xt}}{x}dx=\int_0^{\infty}-e^{-xt}+2e^{-2xt}dx=0$$ this means $f$ is constant. I feel something is wrong here because $f$ should depend on $t$. Where am I wrong and what is the right way to do this?",,"['calculus', 'real-analysis', 'integration', 'improper-integrals']"
86,Can an irrational always be found by multiplying irrationals?,Can an irrational always be found by multiplying irrationals?,,"I was thinking about the function $\ f(a,b) = a/b $ where $a$ and $b$ where both irrational. It quickly stood out to me that the codomain of that function would include every rational number. But, does it include every irrational number as well (in other words, is the codomain of the function $\mathbb{R}$)? Then I thought that if we establish that $a = n \cdot m$  and $b = m$, then if for every irrational number $n$ there exists at least another irrational number $m$ (which could be itself) such that $n \cdot m$ is also irrational, every irrational could be represented by $a/b$ (as $m$ cancels out), and so the function would eventually ""spit out"" all the reals given only irrational input. So my questions are: For every irrational $n$, does there always exist another irrational $m$ such that $n \cdot m$ is also irrational? If this is true as I suspect, what is the simplest proof for it?","I was thinking about the function $\ f(a,b) = a/b $ where $a$ and $b$ where both irrational. It quickly stood out to me that the codomain of that function would include every rational number. But, does it include every irrational number as well (in other words, is the codomain of the function $\mathbb{R}$)? Then I thought that if we establish that $a = n \cdot m$  and $b = m$, then if for every irrational number $n$ there exists at least another irrational number $m$ (which could be itself) such that $n \cdot m$ is also irrational, every irrational could be represented by $a/b$ (as $m$ cancels out), and so the function would eventually ""spit out"" all the reals given only irrational input. So my questions are: For every irrational $n$, does there always exist another irrational $m$ such that $n \cdot m$ is also irrational? If this is true as I suspect, what is the simplest proof for it?",,"['real-analysis', 'functions', 'irrational-numbers']"
87,Real function continuous on closed interval implies it is bounded - over-simple proof??,Real function continuous on closed interval implies it is bounded - over-simple proof??,,"So here is my proof, which after looking up others seems to be too simple, or not rigourous enough, though I don't see why (hence I am asking!): We take the contrapositive, and so prove that if $f$ is unbounded on $[a,b]$, then it is not continuous on this interval. By hypothesis, $\exists c \in [a,b] : \displaystyle\lim_{x\to c} f(x) = \infty \implies f(c)$ is undefined. (or, $ \displaystyle\lim_{x\to c} f(x) \not= f(c)$ so we get discontinuity instantly) Since $f(c)$ doesn't exist, the definition of continuity cannot be applied so $f$ must be discontinuous at at least $x=c$, as required.","So here is my proof, which after looking up others seems to be too simple, or not rigourous enough, though I don't see why (hence I am asking!): We take the contrapositive, and so prove that if $f$ is unbounded on $[a,b]$, then it is not continuous on this interval. By hypothesis, $\exists c \in [a,b] : \displaystyle\lim_{x\to c} f(x) = \infty \implies f(c)$ is undefined. (or, $ \displaystyle\lim_{x\to c} f(x) \not= f(c)$ so we get discontinuity instantly) Since $f(c)$ doesn't exist, the definition of continuity cannot be applied so $f$ must be discontinuous at at least $x=c$, as required.",,"['real-analysis', 'proof-writing']"
88,Construction of $\Bbb R$ from $\Bbb Q$,Construction of  from,\Bbb R \Bbb Q,"As it is true that we can construct all rational numbers $\Bbb Q$ from the set of integers $\Bbb Z$, is it possible to construct the set of real numbers $\Bbb R$ from $\Bbb Q$? If yes, how? Is there any procedure? And if no, is there any proof that we can't ? Thanks!","As it is true that we can construct all rational numbers $\Bbb Q$ from the set of integers $\Bbb Z$, is it possible to construct the set of real numbers $\Bbb R$ from $\Bbb Q$? If yes, how? Is there any procedure? And if no, is there any proof that we can't ? Thanks!",,['real-analysis']
89,Extension of a continuous function so that the integral does not change,Extension of a continuous function so that the integral does not change,,"Suppose $f : [a,b] \to \mathbb{R}$ is a continuous function. Can another continuous function $g : [a,c] \to \mathbb{R}$ be defined such that $\int_a^b f(x)dx =\int_a^c g(x)dx$ so that $f(x) = g(x), x\in[a,b]$ and $c>b$ (Note: if $g \geq 0$ on $[b,c]$ this will not be possible but this restriction is not imposed on the above.)","Suppose $f : [a,b] \to \mathbb{R}$ is a continuous function. Can another continuous function $g : [a,c] \to \mathbb{R}$ be defined such that $\int_a^b f(x)dx =\int_a^c g(x)dx$ so that $f(x) = g(x), x\in[a,b]$ and $c>b$ (Note: if $g \geq 0$ on $[b,c]$ this will not be possible but this restriction is not imposed on the above.)",,['real-analysis']
90,Proving $f$ is not surjective,Proving  is not surjective,f,"Given the function $f: \mathbb R \mapsto \mathbb R, f(x) = 2e^x + 3x^2 - 2x + 5,$ I am asked to show that it is not surjective. My book goes about it like this: $f(x) = 2e^x + x^2 - 2x + 1 + 2x^2 + 4 = f(x) = 2e^x + (x-1)^2  + 2x^2 + 4$ $f(x) > 0 \implies$ not surjective Prior to seeing how the book authors solved this, I tried my own solution by using limits: $\lim_{x\to\infty}f(x) = \infty$ and $\lim_{x\to-\infty}f(x) = \infty \implies f$ not surjective Is my solution also correct?","Given the function $f: \mathbb R \mapsto \mathbb R, f(x) = 2e^x + 3x^2 - 2x + 5,$ I am asked to show that it is not surjective. My book goes about it like this: $f(x) = 2e^x + x^2 - 2x + 1 + 2x^2 + 4 = f(x) = 2e^x + (x-1)^2  + 2x^2 + 4$ $f(x) > 0 \implies$ not surjective Prior to seeing how the book authors solved this, I tried my own solution by using limits: $\lim_{x\to\infty}f(x) = \infty$ and $\lim_{x\to-\infty}f(x) = \infty \implies f$ not surjective Is my solution also correct?",,['real-analysis']
91,Proper Measurable subgroups of $\mathbb R$,Proper Measurable subgroups of,\mathbb R,"If $(\mathbb{R},+)$ is a group and $H$ is a proper subgroup of $\mathbb{R}$ then prove that $H$ is of measure zero.","If $(\mathbb{R},+)$ is a group and $H$ is a proper subgroup of $\mathbb{R}$ then prove that $H$ is of measure zero.",,[]
92,Why is the range a larger set than the domain?,Why is the range a larger set than the domain?,,"When we have a function $f: \mathbb{R} \to \mathbb{R}$ , I can intuitively picture that and think that for every $x \in \mathbb{R}$ , we can find a $y \in \mathbb{R}$ such that our function $f$ maps $x$ onto $y$ . I'm confused, however, when we have something like: $g: D \to \mathbb{R}$ , where $D$ is the domain of our function such that $D \subset \mathbb{R}$ . How can our function output every element in $\mathbb{R}$ , when our input was specifically less than the whole set of reals?","When we have a function , I can intuitively picture that and think that for every , we can find a such that our function maps onto . I'm confused, however, when we have something like: , where is the domain of our function such that . How can our function output every element in , when our input was specifically less than the whole set of reals?",f: \mathbb{R} \to \mathbb{R} x \in \mathbb{R} y \in \mathbb{R} f x y g: D \to \mathbb{R} D D \subset \mathbb{R} \mathbb{R},"['real-analysis', 'functions', 'real-numbers']"
93,"Proof of the identity $\int_0^{+\infty}\frac{\sin(x)}{x^\alpha}dx=\frac{\Gamma(\alpha/2)\Gamma(1-\alpha/2)}{2\Gamma(\alpha)}$ for $\alpha\in (0,2)$.",Proof of the identity  for .,"\int_0^{+\infty}\frac{\sin(x)}{x^\alpha}dx=\frac{\Gamma(\alpha/2)\Gamma(1-\alpha/2)}{2\Gamma(\alpha)} \alpha\in (0,2)",Let $0<\alpha<2.$ Looking for a proof for the following: $$\int_0^{+\infty}\frac{\sin(x)}{x^\alpha}dx=\frac{\Gamma(\alpha/2)\Gamma(1-\alpha/2)}{2\Gamma(\alpha)}.$$ Any ideas?,Let Looking for a proof for the following: Any ideas?,0<\alpha<2. \int_0^{+\infty}\frac{\sin(x)}{x^\alpha}dx=\frac{\Gamma(\alpha/2)\Gamma(1-\alpha/2)}{2\Gamma(\alpha)}.,"['real-analysis', 'integration', 'measure-theory', 'improper-integrals', 'lebesgue-integral']"
94,Spivak Calculus. Why is the books proof valid? Is my attempt at a proof valid?,Spivak Calculus. Why is the books proof valid? Is my attempt at a proof valid?,,"Here is the problem. If f and g are continuous and $f(x)\ge g(x)$ for all x in a dense set A, prove that $f(x)\ge g(x)$ for all x. Here is the proof I gave on my own attempt. I will prove $f(x)\ge 0$ for all x using $f(x)\ge0$ for all x in A. Now in an earlier problem I already proved that if $f(x)=0$ for all x in A, then $f(x)=0$ for all x. So I only need to prove it for the case $f(x)\gt0$ . Since f is continuous, for all $\epsilon\gt0$ there is some $\delta\gt0$ such that for all $x$ if $$\vert x-a \vert \lt \delta$$ then, $$\vert f(x)- f(a) \vert \lt \epsilon.$$ So there is some $a\in A$ in the interval $(x-\delta, x+\delta)$ since A is dense. Choosing $\epsilon=f(a)$ we obtain $$-f(a)\lt f(x)-f(a).$$ So $f(x)\ge0$ for all x. Applying this to the funtion $f(x)-g(x)$ we get the desired solution. Now here is the books solution. It suffices to show that if $f$ is continuous and $f(x)\ge0$ for all x in A, then $f(x)\ge0$ for all x. Now there is a $\delta\gt0$ such that, for all x, if $$0\lt \vert x-a\vert \lt\delta,$$ then $$\vert f(x)-l \vert \lt \frac{\vert l \vert}{2}.$$ This implies that $$f(x)\lt l+ \frac{\vert l \vert}{2}.$$ Now if $l\lt0$ , it would follow that $f(x)\lt0$ , which would be false for those x in A which satisfy $0\lt \vert x-a \vert \lt \delta$ . So this brings me to my questions. Why is his proof correct? Couldn't $\frac{\vert l \vert}{2}=0$ ? Then it would not be an $\epsilon \gt0$ . And also is my proof correct? I am guessing I did something wrong since it seems much cleaner than the one he gave and I would assume he would have used it if it is correct.","Here is the problem. If f and g are continuous and for all x in a dense set A, prove that for all x. Here is the proof I gave on my own attempt. I will prove for all x using for all x in A. Now in an earlier problem I already proved that if for all x in A, then for all x. So I only need to prove it for the case . Since f is continuous, for all there is some such that for all if then, So there is some in the interval since A is dense. Choosing we obtain So for all x. Applying this to the funtion we get the desired solution. Now here is the books solution. It suffices to show that if is continuous and for all x in A, then for all x. Now there is a such that, for all x, if then This implies that Now if , it would follow that , which would be false for those x in A which satisfy . So this brings me to my questions. Why is his proof correct? Couldn't ? Then it would not be an . And also is my proof correct? I am guessing I did something wrong since it seems much cleaner than the one he gave and I would assume he would have used it if it is correct.","f(x)\ge g(x) f(x)\ge g(x) f(x)\ge 0 f(x)\ge0 f(x)=0 f(x)=0 f(x)\gt0 \epsilon\gt0 \delta\gt0 x \vert x-a \vert \lt \delta \vert f(x)- f(a) \vert \lt \epsilon. a\in A (x-\delta, x+\delta) \epsilon=f(a) -f(a)\lt f(x)-f(a). f(x)\ge0 f(x)-g(x) f f(x)\ge0 f(x)\ge0 \delta\gt0 0\lt \vert x-a\vert \lt\delta, \vert f(x)-l \vert \lt \frac{\vert l \vert}{2}. f(x)\lt l+ \frac{\vert l \vert}{2}. l\lt0 f(x)\lt0 0\lt \vert x-a \vert \lt \delta \frac{\vert l \vert}{2}=0 \epsilon \gt0","['real-analysis', 'calculus']"
95,Is the gradient of the self-intersections of a curve zero?,Is the gradient of the self-intersections of a curve zero?,,"Suppose a curve with self-intersections can be described by $\phi(x,y)=0$ . Suppose the intersections are $T_i$ , $i=1,2,...$ and the gradient $\nabla \phi$ at those intersections are well defined. Then is it true that $\nabla\phi(T_i)=0$ for all $i$ ? In other words, are the gradients at those intersections all zero?","Suppose a curve with self-intersections can be described by . Suppose the intersections are , and the gradient at those intersections are well defined. Then is it true that for all ? In other words, are the gradients at those intersections all zero?","\phi(x,y)=0 T_i i=1,2,... \nabla \phi \nabla\phi(T_i)=0 i","['real-analysis', 'calculus', 'geometry', 'differential-geometry']"
96,"Prove that $0!+1! + 2! + 3! + ..... + n!$ $\neq$ $p^\text{r}$, where $n \geqslant 3$ and $n$, $p$ and $r$ are three integers","Prove that   , where  and ,  and  are three integers",0!+1! + 2! + 3! + ..... + n! \neq p^\text{r} n \geqslant 3 n p r,"Let $n$ , $p$ and $r$ be three positive integers. Prove that for $n \geqslant 3, r>1$ , $$\sum_{k = 0}^{n} k! \neq p^\text{r}$$ SOURCE: BANGLADESH MATH OLYMPIAD (Preaparatory Question) I am not so familiar with such a this kind of problem. Seeing that problem, I became little bit curious about what the text states and what will be its conception? I couldn't solve the problem. Moreover, I don't know about the formula of $\sum k!$ . Is there any? And I couldn't realize the essence of $p^\text{r}$ and what the reason is behind the fact the summation can't be equal to $p^r$ for some integer $p$ . Any kind of reference or conception will massively help me start with some approach to solve the above problem. Thanks for your support and effort in advance.","Let , and be three positive integers. Prove that for , SOURCE: BANGLADESH MATH OLYMPIAD (Preaparatory Question) I am not so familiar with such a this kind of problem. Seeing that problem, I became little bit curious about what the text states and what will be its conception? I couldn't solve the problem. Moreover, I don't know about the formula of . Is there any? And I couldn't realize the essence of and what the reason is behind the fact the summation can't be equal to for some integer . Any kind of reference or conception will massively help me start with some approach to solve the above problem. Thanks for your support and effort in advance.","n p r n \geqslant 3, r>1 \sum_{k = 0}^{n} k! \neq p^\text{r} \sum k! p^\text{r} p^r p","['real-analysis', 'sequences-and-series', 'number-theory', 'summation', 'factorial']"
97,"What is a ""linear function"" in the context of multivariable calculus?","What is a ""linear function"" in the context of multivariable calculus?",,"On Wikipedia , it says When $f$ is a function from an open subset of $\mathbb{R}^n$ to $\mathbb{R}^m$ , then the directional derivative of $f$ in a chosen direction is the best linear approximation to f at that point and in that direction. I just want to check that linear functions from $\mathbb{R}^n$ to $\mathbb{R}^m$ , are defined as functions of the form $f(x) = ax+b$ where $a$ is a scalar and $b$ is a vector? Also, it seems like functions of the form above just enlarge/shrink and shift. Is this correct? I thought that if anything was going to be a counterexample, it was going to be an off center circle; under the transformation x $\mapsto$ 2x , I thought an off-center circle might map to an ellipse; but this doesn't seem to be the case. For example, if $(x, y)$ satisfies $(x-2)^2 + (y-2)^2 = 1$ , then multiplying both sides by $2^2$ gives $(2x-4)^2 + (2y-4)^2 = 4$ ; so $(2x, 2y)$ satisfies $(X^2-4)^2 + (Y-4)^2 = 4$ , which is still a circle with center at $(4, 4)$ , as expected.","On Wikipedia , it says When is a function from an open subset of to , then the directional derivative of in a chosen direction is the best linear approximation to f at that point and in that direction. I just want to check that linear functions from to , are defined as functions of the form where is a scalar and is a vector? Also, it seems like functions of the form above just enlarge/shrink and shift. Is this correct? I thought that if anything was going to be a counterexample, it was going to be an off center circle; under the transformation x 2x , I thought an off-center circle might map to an ellipse; but this doesn't seem to be the case. For example, if satisfies , then multiplying both sides by gives ; so satisfies , which is still a circle with center at , as expected.","f \mathbb{R}^n \mathbb{R}^m f \mathbb{R}^n \mathbb{R}^m f(x) = ax+b a b \mapsto (x, y) (x-2)^2 + (y-2)^2 = 1 2^2 (2x-4)^2 + (2y-4)^2 = 4 (2x, 2y) (X^2-4)^2 + (Y-4)^2 = 4 (4, 4)","['real-analysis', 'analysis', 'multivariable-calculus']"
98,Show that $n^{\frac{n+1}{n}}$ is increasing,Show that  is increasing,n^{\frac{n+1}{n}},"How can one show that $n^{\frac{n+1}{n}}$ is increasing, possibly using logarithms but not derivatives?  I obtain $(n^2+2n)log(n+1)\geq (n^2+2n+1)log (n)$ but cannot proceed further.","How can one show that $n^{\frac{n+1}{n}}$ is increasing, possibly using logarithms but not derivatives?  I obtain $(n^2+2n)log(n+1)\geq (n^2+2n+1)log (n)$ but cannot proceed further.",,"['real-analysis', 'sequences-and-series', 'limits', 'induction']"
99,Justification for indefinite integration by substitution,Justification for indefinite integration by substitution,,"I know that if we want to compute a definite integral using substitution, we use $$\int_{a}^b f(\phi(t))\phi '(t)~dt = \int_{\phi(a)}^{\phi(b)}f(x)~dx\tag1$$ I know that this method can go from left to right or right to left. If we want to compute an indefinite integral using substitution, we use $$\int f(t)~dt = \int g(\phi(t))\phi '(t)~dt = G(\phi(t)) + C, \tag{2}$$ where $G$ is an antiderivative of $g$ and $C$ is the constant of integration. For example, for evaluating functions like $\dfrac{1}{\sqrt{4 - x^2}}$ we can make the substitution $x = 2\sin \theta$ and find $dx$ in terms of $d\theta$ . Substituting its value in the integral, it becomes computable. However, pretending that $\dfrac{dx}{d\theta}$ is a fraction and substituting the value of $dx$ in the integral just because it ""works"" is not very rigorous. Is there a rigorous statement for computing indefinite integrals that can't be expressed in the aforementioned form using substitution, just like we have for definite integrals, or can they be computed using $(2)$ in a way that I can't figure out?","I know that if we want to compute a definite integral using substitution, we use I know that this method can go from left to right or right to left. If we want to compute an indefinite integral using substitution, we use where is an antiderivative of and is the constant of integration. For example, for evaluating functions like we can make the substitution and find in terms of . Substituting its value in the integral, it becomes computable. However, pretending that is a fraction and substituting the value of in the integral just because it ""works"" is not very rigorous. Is there a rigorous statement for computing indefinite integrals that can't be expressed in the aforementioned form using substitution, just like we have for definite integrals, or can they be computed using in a way that I can't figure out?","\int_{a}^b f(\phi(t))\phi '(t)~dt = \int_{\phi(a)}^{\phi(b)}f(x)~dx\tag1 \int f(t)~dt = \int g(\phi(t))\phi '(t)~dt = G(\phi(t)) + C, \tag{2} G g C \dfrac{1}{\sqrt{4 - x^2}} x = 2\sin \theta dx d\theta \dfrac{dx}{d\theta} dx (2)","['real-analysis', 'calculus', 'integration', 'indefinite-integrals', 'substitution']"
