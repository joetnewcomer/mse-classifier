,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"How to show, that a Hermitian matrix is positive definite, if all eigenvalues are positive.","How to show, that a Hermitian matrix is positive definite, if all eigenvalues are positive.",,"I want to show, that a Hermitian matrix is positive definite, if all eigenvalues of the matrix are positive. And the other way round. Also I wonder, if every Hermitian, strict diagonally dominant matrix is positive definite. Thank you guys.","I want to show, that a Hermitian matrix is positive definite, if all eigenvalues of the matrix are positive. And the other way round. Also I wonder, if every Hermitian, strict diagonally dominant matrix is positive definite. Thank you guys.",,"['linear-algebra', 'matrices']"
1,Is there a shorter way to prove this?,Is there a shorter way to prove this?,,"Let $n$ a natural number and $A=(a_{ij})$, where $a_{ij}=\left(\begin{array}{c}i+j \\i\end{array} \right)$, for $0≤i,j<n$. Prove that A has an inverse matrix and that all the entries of $A^{−1}$ are integers. I tried to prove this like this: (but I'm not sure if it's correct or formal enough) Also if you could give me a hint to prove the second aasertion it would be great.  Thank you.","Let $n$ a natural number and $A=(a_{ij})$, where $a_{ij}=\left(\begin{array}{c}i+j \\i\end{array} \right)$, for $0≤i,j<n$. Prove that A has an inverse matrix and that all the entries of $A^{−1}$ are integers. I tried to prove this like this: (but I'm not sure if it's correct or formal enough) Also if you could give me a hint to prove the second aasertion it would be great.  Thank you.",,"['linear-algebra', 'combinatorics']"
2,"Show that if $A = \begin{bmatrix} 0 & 1 \\ 1 & 1 \end{bmatrix} $, $\mathrm{tr}(A^{k}) = \mathrm{tr}(A^{k-1}) + \mathrm{tr}(A^{k-2})$","Show that if ,",A = \begin{bmatrix} 0 & 1 \\ 1 & 1 \end{bmatrix}  \mathrm{tr}(A^{k}) = \mathrm{tr}(A^{k-1}) + \mathrm{tr}(A^{k-2}),"$\newcommand{\tr}{\operatorname{tr}}$ If $A =   \begin{bmatrix}      0 & 1  \\      1 & 1    \end{bmatrix} $ , then $\tr(A^{k}) = \tr(A^{k-1}) + \tr(A^{k-2})$ . Hint: If $AB=0$ , then $\tr[(A+B)^k]=\tr(A^k)+\tr(B^k)$ . I tried to decompose \begin{bmatrix}     0 & 1  \\     1 & 1    \end{bmatrix} to $P$ and $Q$ such that $P+Q=\begin{bmatrix}     0 & 1  \\     1 & 1    \end{bmatrix}$ and $PQ=0$ , but it seems that this does not work.","If , then . Hint: If , then . I tried to decompose to and such that and , but it seems that this does not work.","\newcommand{\tr}{\operatorname{tr}} A =   \begin{bmatrix}
     0 & 1  \\
     1 & 1    \end{bmatrix}  \tr(A^{k}) = \tr(A^{k-1}) + \tr(A^{k-2}) AB=0 \tr[(A+B)^k]=\tr(A^k)+\tr(B^k) \begin{bmatrix}
    0 & 1  \\
    1 & 1 
  \end{bmatrix} P Q P+Q=\begin{bmatrix}
    0 & 1  \\
    1 & 1 
  \end{bmatrix} PQ=0","['linear-algebra', 'matrices']"
3,Are there uncountably many $A\in M_3 (\mathbb {R})$ such that $A^8=I $?,Are there uncountably many  such that ?,A\in M_3 (\mathbb {R}) A^8=I ,I'm working on the following problem: Let $A \in M_3 (\mathbb {R})$ be such that $A^8=I$. Then the minimal polynomial of $A$ can only be of degree $2$. the minimal polynomial of $A$ can only be of degree $3$. either $A = I$ or $ A = -I$. there are uncountably many such $A$. By taking $A=I $ we can eliminate options (1) & (2). For the minimal polynomial of $A$ in that case is of degree $1$. Now take $$A = \begin{bmatrix}      1 & 0 & 0 \\      0 & -1 & 0 \\      0 & 0 & -1 \end{bmatrix}$$ Then $A^8=I $ but $A$ is neither $I$ nor $-I$. So option (3) is eliminated. Now I don't know how to proceed about with option  (4). Can someone help me please? Thanks in advance.,I'm working on the following problem: Let $A \in M_3 (\mathbb {R})$ be such that $A^8=I$. Then the minimal polynomial of $A$ can only be of degree $2$. the minimal polynomial of $A$ can only be of degree $3$. either $A = I$ or $ A = -I$. there are uncountably many such $A$. By taking $A=I $ we can eliminate options (1) & (2). For the minimal polynomial of $A$ in that case is of degree $1$. Now take $$A = \begin{bmatrix}      1 & 0 & 0 \\      0 & -1 & 0 \\      0 & 0 & -1 \end{bmatrix}$$ Then $A^8=I $ but $A$ is neither $I$ nor $-I$. So option (3) is eliminated. Now I don't know how to proceed about with option  (4). Can someone help me please? Thanks in advance.,,"['linear-algebra', 'matrices', 'matrix-equations', 'minimal-polynomials']"
4,What makes Krylov subspaces so special?,What makes Krylov subspaces so special?,,"I am reading through ""Numerical Linear Algebra"", by Lloyd N. Trefethen and David Bau. I reached the final chapter about iterative methods. They all share the fact, that they are using Krylov subspaces. What I understood so far: We are using those subspaces $K_{n}=<b, Ab, AAb, ... A^{n-1}b>$ for different problems, e.g. solving systems, that require dimensions much higher than $n$. Usually we first orthonormalize $K_n$, e.g. by using the Arnoldi iteration, so it is more stable. This way, we can save much time and computing-cost, and the solution can converge nicely, even for $n<<m$. My question is: What makes Krylov spaces in particular so special? Why doesn't it work  well (or does it?) for arbitrary other orthonormal bases, that I can expand dimension by dimension, too?","I am reading through ""Numerical Linear Algebra"", by Lloyd N. Trefethen and David Bau. I reached the final chapter about iterative methods. They all share the fact, that they are using Krylov subspaces. What I understood so far: We are using those subspaces $K_{n}=<b, Ab, AAb, ... A^{n-1}b>$ for different problems, e.g. solving systems, that require dimensions much higher than $n$. Usually we first orthonormalize $K_n$, e.g. by using the Arnoldi iteration, so it is more stable. This way, we can save much time and computing-cost, and the solution can converge nicely, even for $n<<m$. My question is: What makes Krylov spaces in particular so special? Why doesn't it work  well (or does it?) for arbitrary other orthonormal bases, that I can expand dimension by dimension, too?",,"['linear-algebra', 'numerical-linear-algebra']"
5,"Prove that ""Every subspaces of a finite-dimensional vector space is finite-dimensional""","Prove that ""Every subspaces of a finite-dimensional vector space is finite-dimensional""",,"In Sheldon Axler's ""Linear Algebra Done Right"" 3rd edtion Page 36 he worte: Proof of every subspaces of a finite-dimensional vector space is finite-dimensional The question is: I do not understand the last sentence""Thus the process eventually terminates, which means that U is finite-dimensional"". So far I can understand that the subspace $U$ has limited length of independent vector list, but how to reach from here to the conclusion that subspace $U$ is finite dimensional? By 2.23 we see that the length of spanning list of vectors can be more than(rather than less than) the finite list vetors' length. Thus finite length of  independent vectors list does not imply finite length of spanning list! My proof : By the definition of subspace, we see that $U$ is a subset of $V$ (with some other restrictions). Then, for every $u\in U$ , $u\in V$ , implying $u\in span(v_1,v_2,...,v_m)$ for $V= span (v_1,v_2,...,v_m)$ (Here I used the difinition 2.10). QED. 2.23 Length of linearly independent list length of spanning list In a ﬁnite-dimensional vector space, the length of every linearly independent list of vectors is less than or equal to the length of every spanning list of vectors. 2.10 Deﬁnition ﬁnite-dimensional vectors pace A vector space is called ﬁnite-dimensional if some list of vectors in it spans the space. Please verify why Professor Axler's proof is valid and point out any mistakes I've made in my understanding or proof if there are some, thanks in advance!","In Sheldon Axler's ""Linear Algebra Done Right"" 3rd edtion Page 36 he worte: Proof of every subspaces of a finite-dimensional vector space is finite-dimensional The question is: I do not understand the last sentence""Thus the process eventually terminates, which means that U is finite-dimensional"". So far I can understand that the subspace has limited length of independent vector list, but how to reach from here to the conclusion that subspace is finite dimensional? By 2.23 we see that the length of spanning list of vectors can be more than(rather than less than) the finite list vetors' length. Thus finite length of  independent vectors list does not imply finite length of spanning list! My proof : By the definition of subspace, we see that is a subset of (with some other restrictions). Then, for every , , implying for (Here I used the difinition 2.10). QED. 2.23 Length of linearly independent list length of spanning list In a ﬁnite-dimensional vector space, the length of every linearly independent list of vectors is less than or equal to the length of every spanning list of vectors. 2.10 Deﬁnition ﬁnite-dimensional vectors pace A vector space is called ﬁnite-dimensional if some list of vectors in it spans the space. Please verify why Professor Axler's proof is valid and point out any mistakes I've made in my understanding or proof if there are some, thanks in advance!","U U U V u\in U u\in V u\in span(v_1,v_2,...,v_m) V= span (v_1,v_2,...,v_m)","['linear-algebra', 'proof-verification', 'vector-spaces']"
6,Unbounded linear operator,Unbounded linear operator,,"Let $(A, \|\cdot\|_A), (B, \|\cdot\|_B)$ be normed linear spaces. Consider $T \in L(A,B)$ The operator norm of $T$ is defined to be $$\|T\| = \sup\{\|Tx\|_B: \|x\|_A \leq 1\}$$ $T$ is bounded if $\|T\| < \infty$ otherwise it is unbounded. So can someone give me an example of an unbounded linear operator? This seems very counterintuitive to me because, that means  $$\exists \space x \in A, \|Tx\|_B = \infty$$ but then any scalar multiples of $Tx$ would have an infinite norm. Then what would $T(0)$ be?","Let $(A, \|\cdot\|_A), (B, \|\cdot\|_B)$ be normed linear spaces. Consider $T \in L(A,B)$ The operator norm of $T$ is defined to be $$\|T\| = \sup\{\|Tx\|_B: \|x\|_A \leq 1\}$$ $T$ is bounded if $\|T\| < \infty$ otherwise it is unbounded. So can someone give me an example of an unbounded linear operator? This seems very counterintuitive to me because, that means  $$\exists \space x \in A, \|Tx\|_B = \infty$$ but then any scalar multiples of $Tx$ would have an infinite norm. Then what would $T(0)$ be?",,"['linear-algebra', 'operator-theory', 'linear-transformations']"
7,Does $A^2 \geq B^2 > 0$ imply $ACA \geq BCB$ for square positive definite matrices?,Does  imply  for square positive definite matrices?,A^2 \geq B^2 > 0 ACA \geq BCB,"Assume we have two $n \times n$ real nondegenerate matrices $ A^2 $ and $B^2$, such that $$ A^2 \geq B^2 > 0, $$ where ""$\geq$"" means positive semidefinite (Loewner) ordering. Does the following inequality holds for any real matrix $C$  $$ ACA \geq BCB \ ? $$ If not, under which conditions on $C$ (or additional conditions on $A$ and $B$) does it holds? I would appreciate any ideas, suggestions, counterexamples. Thanks!","Assume we have two $n \times n$ real nondegenerate matrices $ A^2 $ and $B^2$, such that $$ A^2 \geq B^2 > 0, $$ where ""$\geq$"" means positive semidefinite (Loewner) ordering. Does the following inequality holds for any real matrix $C$  $$ ACA \geq BCB \ ? $$ If not, under which conditions on $C$ (or additional conditions on $A$ and $B$) does it holds? I would appreciate any ideas, suggestions, counterexamples. Thanks!",,"['linear-algebra', 'matrices', 'positive-definite']"
8,Vector subspace of $M_n(\mathbb{R})$ with invertible matrices,Vector subspace of  with invertible matrices,M_n(\mathbb{R}),"I recall this claim that I have read in some book a long time ago, but now I do not remember and unfortunately I could not find anything on google about it. I was wondering if someone could help me with some reference about this. For $n>8$ there is no a $n$-dimensional vector subspace of $M_n(\mathbb{R})$ which all non zero elements are invertible matrix. I was also wondering if we can say something for $n \leq 8$. Thank you. Remark: I think this should be related to Hurwitz's Theorem ( https://en.wikipedia.org/wiki/Hurwitz%27s_theorem_(composition_algebras) ). For example, for $n=1,2,4,8$ these $n$-dimensional vector subspaces are the ones isomorphic to $\mathbb{R},\mathbb{C},\mathbb{H}$ (quaternions) and $\mathbb{O}$ (octonions) respectively. Remark 2: I think that the fact these matrices are real it is very important, but I don't know why.","I recall this claim that I have read in some book a long time ago, but now I do not remember and unfortunately I could not find anything on google about it. I was wondering if someone could help me with some reference about this. For $n>8$ there is no a $n$-dimensional vector subspace of $M_n(\mathbb{R})$ which all non zero elements are invertible matrix. I was also wondering if we can say something for $n \leq 8$. Thank you. Remark: I think this should be related to Hurwitz's Theorem ( https://en.wikipedia.org/wiki/Hurwitz%27s_theorem_(composition_algebras) ). For example, for $n=1,2,4,8$ these $n$-dimensional vector subspaces are the ones isomorphic to $\mathbb{R},\mathbb{C},\mathbb{H}$ (quaternions) and $\mathbb{O}$ (octonions) respectively. Remark 2: I think that the fact these matrices are real it is very important, but I don't know why.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'reference-request', 'vector-spaces']"
9,Definition of complex conjugate in complex vector space,Definition of complex conjugate in complex vector space,,"I am starting reading about Hodge theory and while reading the definition of abstract Hodge structure a very basic question came to my mind... What is the definition of the conjugate of a subspace of a complex vector space? I found online the definition of conjugate of an entire vector space, but when we do the Hodge decompostition we want $V_{\mathbb{C}}=\oplus_{p,q \in \mathbb{Z}}V^{p,q}$ with the requirement $\overline{V^{q,p}}=V^{p,q}$. So I want to take a conjugate inside the space $V_{\mathbb{C}}$. I can see it if my space is of differential forms, but what is the definition for an abstract one? Also, if I take the tensor product of two vector spaces, is this conjugation induced componentwise? I know the question is pretty basic, but I couldn't find any reference for such definitions.","I am starting reading about Hodge theory and while reading the definition of abstract Hodge structure a very basic question came to my mind... What is the definition of the conjugate of a subspace of a complex vector space? I found online the definition of conjugate of an entire vector space, but when we do the Hodge decompostition we want $V_{\mathbb{C}}=\oplus_{p,q \in \mathbb{Z}}V^{p,q}$ with the requirement $\overline{V^{q,p}}=V^{p,q}$. So I want to take a conjugate inside the space $V_{\mathbb{C}}$. I can see it if my space is of differential forms, but what is the definition for an abstract one? Also, if I take the tensor product of two vector spaces, is this conjugation induced componentwise? I know the question is pretty basic, but I couldn't find any reference for such definitions.",,"['linear-algebra', 'definition', 'complex-geometry', 'hodge-theory']"
10,eigenvalues of integral matrices,eigenvalues of integral matrices,,"Is it possible that a $3$-by-$3$ matrix with integer values and determinant 1 has a real eigenvalue with algebraic multiplicity 2, that is not equal to $\pm 1$? Doing some elementary computations one can rephrase the question as follows. Do there exist integers $k$ and $m$ such that $a=\frac{1}{3}(k\pm \sqrt{k^2-3m})$ and  $b=\frac{1}{3}(k\mp 2\sqrt{k^2-3m})$ are real numbers and $a^2b=1$ but $a,b \neq \pm 1$?","Is it possible that a $3$-by-$3$ matrix with integer values and determinant 1 has a real eigenvalue with algebraic multiplicity 2, that is not equal to $\pm 1$? Doing some elementary computations one can rephrase the question as follows. Do there exist integers $k$ and $m$ such that $a=\frac{1}{3}(k\pm \sqrt{k^2-3m})$ and  $b=\frac{1}{3}(k\mp 2\sqrt{k^2-3m})$ are real numbers and $a^2b=1$ but $a,b \neq \pm 1$?",,"['linear-algebra', 'number-theory']"
11,Prove that $\det(A)\neq 0$.,Prove that .,\det(A)\neq 0,"Let $A$ be a $n \times n$ matrix, $n$ even, with even diagonal elements and all other elements odd integers. Prove that  $\det(A)\neq 0$.  Can anyone give me a hint? Thank you.","Let $A$ be a $n \times n$ matrix, $n$ even, with even diagonal elements and all other elements odd integers. Prove that  $\det(A)\neq 0$.  Can anyone give me a hint? Thank you.",,['linear-algebra']
12,Example of a Markov chain transition matrix that is not diagonalizable?,Example of a Markov chain transition matrix that is not diagonalizable?,,"It is well-known that every detailed-balance Markov chain has a diagonalizable transition matrix. I am looking for an example of a Markov chain whose transition matrix is not diagonalizable. That is: Give a transition matrix $M$ such that there exists no invertible matrix $U$ with $U^{-1} M U$ a diagonal matrix. Is there a combinatorial interpretation for the Jordan blocks that I can see directly from the graph? Edit: I found this example here: http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-262-discrete-stochastic-processes-spring-2011/video-lectures/lecture-8-markov-eigenvalues-and-eigenvectors/MIT6_262S11_lec08.pdf on page 21. \begin{array}{ccc} 1/2 & 1/2 & 0\\ 0 & 1/2 & 1/2\\ 0 & 0 & 1 \end{array} This particular example has two non-recurrent states, which is not really what I want. So I am modifying my question to ask for an example of a Markov chain for which every state is recurrent.","It is well-known that every detailed-balance Markov chain has a diagonalizable transition matrix. I am looking for an example of a Markov chain whose transition matrix is not diagonalizable. That is: Give a transition matrix $M$ such that there exists no invertible matrix $U$ with $U^{-1} M U$ a diagonal matrix. Is there a combinatorial interpretation for the Jordan blocks that I can see directly from the graph? Edit: I found this example here: http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-262-discrete-stochastic-processes-spring-2011/video-lectures/lecture-8-markov-eigenvalues-and-eigenvectors/MIT6_262S11_lec08.pdf on page 21. \begin{array}{ccc} 1/2 & 1/2 & 0\\ 0 & 1/2 & 1/2\\ 0 & 0 & 1 \end{array} This particular example has two non-recurrent states, which is not really what I want. So I am modifying my question to ask for an example of a Markov chain for which every state is recurrent.",,"['linear-algebra', 'examples-counterexamples', 'markov-chains']"
13,Show that $B$ is diagonalizable if If $AB=BA$ and $A$ has distinct real eigenvalues,Show that  is diagonalizable if If  and  has distinct real eigenvalues,B AB=BA A,"We were asked to prove the following: Let $ A $ be an $n \times n$ matrix with $n$ distinct real eigenvalues. If $AB=BA$, show that $B$ is diagonalizable. It was suggested I show that an eigenvector of $A$ is also an eigenvector of $B$. I am both having trouble doing this and failing to see how I would complete the proof after. Any help would be appreciated. Thanks","We were asked to prove the following: Let $ A $ be an $n \times n$ matrix with $n$ distinct real eigenvalues. If $AB=BA$, show that $B$ is diagonalizable. It was suggested I show that an eigenvector of $A$ is also an eigenvector of $B$. I am both having trouble doing this and failing to see how I would complete the proof after. Any help would be appreciated. Thanks",,"['linear-algebra', 'eigenvalues-eigenvectors', 'diagonalization']"
14,Representation of a linear functional in vector space,Representation of a linear functional in vector space,,"In the book Functional Analysis, Sobolev Spaces and Partial Differential Equations of Haim Brezis we have the following lemma: Lemma. Let $X$ be a vector space and let $\varphi, \varphi_1, \varphi_2, \ldots, \varphi_k$ be $(k + 1)$ linear functionals on $X$ such that $$ [\varphi_i(v) = 0 \quad \forall\; i \in \{1, 2, \ldots , k\}] \Rightarrow [\varphi(v) = 0]. $$ Then there exist constants $\lambda_1, \lambda_2, \ldots, \lambda_k\in\mathbb{R}$ such that $\varphi=\lambda_1\varphi_1+\lambda_2\varphi_2+\ldots+\lambda_k\varphi_k$ . In this book, the author used separation theorem to prove this lemma. I would like ask whether we can use only knowledge of linear algebra to prove this lemma. Thank you for all helping.","In the book Functional Analysis, Sobolev Spaces and Partial Differential Equations of Haim Brezis we have the following lemma: Lemma. Let be a vector space and let be linear functionals on such that Then there exist constants such that . In this book, the author used separation theorem to prove this lemma. I would like ask whether we can use only knowledge of linear algebra to prove this lemma. Thank you for all helping.","X \varphi, \varphi_1, \varphi_2, \ldots, \varphi_k (k + 1) X 
[\varphi_i(v) = 0 \quad \forall\; i \in \{1, 2, \ldots , k\}] \Rightarrow [\varphi(v) = 0].
 \lambda_1, \lambda_2, \ldots, \lambda_k\in\mathbb{R} \varphi=\lambda_1\varphi_1+\lambda_2\varphi_2+\ldots+\lambda_k\varphi_k","['linear-algebra', 'functional-analysis']"
15,Determining the rank of a matrix based on its minors,Determining the rank of a matrix based on its minors,,"I was wondering if the following lemma was easy to prove.  I got a little tripped up when I saw the explicit condition that the matrix could have elements in any field not necessarily finite, so I didn't know if a standard proof from linear algebra would still apply. Let $M$ be a matrix with entries  in a (possibly infinite) field $F$ . Suppose that there exists a minor $m_n$ of order $n$ for $M$ that is nonzero and such that all minors of order $n+1$ which contain $m_n$ are zero. How do you show the rank of the matrix is $n$ ?","I was wondering if the following lemma was easy to prove.  I got a little tripped up when I saw the explicit condition that the matrix could have elements in any field not necessarily finite, so I didn't know if a standard proof from linear algebra would still apply. Let be a matrix with entries  in a (possibly infinite) field . Suppose that there exists a minor of order for that is nonzero and such that all minors of order which contain are zero. How do you show the rank of the matrix is ?",M F m_n n M n+1 m_n n,"['linear-algebra', 'matrices']"
16,Prove that three $2\times2$ matrices that commute are linearly dependent,Prove that three  matrices that commute are linearly dependent,2\times2,"Statement: Suppose that $A$ , $B$ and $C$ are complex $2\times2$ matrices, any two of which commute under matrix multiplication.  Show that $A$ , $B$ and $C$ are linearly dependent. I think one method is to show the existence of $a,b,c\in\mathbb C$ , such that $aA+bB+cC=0$ while $a$ , $b$ , $c$ are not all zero.  I'm not sure how to proceed with this. I observed that if we add an assumption that $A$ , $B$ and $C$ are diagonalizable, then they are simultaneously diagonalizable since they all commute.  I think this implies that there exists a common $P$ such that $A=PD_1P^{-1}$ , $B=PD_2P^{-1}$ , $C=PD_3P^{-1}$ , where the $D_i$ are diagonal matrices.  Any three $2\times2$ diagonal matrices must be linearly dependent because they each have two non-zero entries only.  As a consequence, $A$ , $B$ and $C$ are linearly dependent. Unfortunately, not all matrices are diagonalizable.  I also tried to use Jordan canonical forms, but all I can see is that three $2\times2$ upper-triangular matrices may not be linearly dependent and that this line of reasoning might lead to a dead end. Therefore, how to prove the original statement?","Statement: Suppose that , and are complex matrices, any two of which commute under matrix multiplication.  Show that , and are linearly dependent. I think one method is to show the existence of , such that while , , are not all zero.  I'm not sure how to proceed with this. I observed that if we add an assumption that , and are diagonalizable, then they are simultaneously diagonalizable since they all commute.  I think this implies that there exists a common such that , , , where the are diagonal matrices.  Any three diagonal matrices must be linearly dependent because they each have two non-zero entries only.  As a consequence, , and are linearly dependent. Unfortunately, not all matrices are diagonalizable.  I also tried to use Jordan canonical forms, but all I can see is that three upper-triangular matrices may not be linearly dependent and that this line of reasoning might lead to a dead end. Therefore, how to prove the original statement?","A B C 2\times2 A B C a,b,c\in\mathbb C aA+bB+cC=0 a b c A B C P A=PD_1P^{-1} B=PD_2P^{-1} C=PD_3P^{-1} D_i 2\times2 A B C 2\times2",['linear-algebra']
17,"If the entries of a positive semidefinite matrix shrink individually, will the operator norm always decrease?","If the entries of a positive semidefinite matrix shrink individually, will the operator norm always decrease?",,"Given a positive semidefinite matrix $P$, if we scale down its entries individually , will its operator norm always decrease? Put it another way: Suppose $P\in M_n(\mathbb R)$ is positive semidefinite and $B\in M_n(\mathbb R)$ is a $[0,1]$-matrix, i.e. $B$ has all entries between $0$ and $1$ (note: $B$ is not necessarily symmetric). Let $\|\cdot\|_2$ denotes the operator norm (i.e. the largest singular value). Is it always true that    $$\|P\|_2\ge\|P\circ B\|_2?\tag{$\ast$}$$ Background. I ran into this inequality in another question . Having done a numerical experiment, I believed the inequality is true, but I hadn't been able to prove it. If $(\ast)$ turns out to be true, we immediately obtain the analogous inequality $\rho(P)\ge\rho(P\circ B)$ for the spectral radii because $\rho(P)=\|P\|_2\ge\|P\circ B\|_2\ge\rho(P\circ B)$. Remarks. There is much research on inequalities about spectral radii or operator norms of Hadamard products. Often, either all multiplicands in each product are semidefinite or all of them are nonnegative. Inequalities like those two here, which involve mixtures of  semidefinite matrices with nonnegative matrices, are rarely seen. I have tested the inequality for $n=2,3,4,5$ with 100,000 random examples for each $n$. No counterexamples were found. The semidefiniteness condition is essential. If it is removed, counterexamples with symmetric $P$s can be easily obtained. The inequality is known to be true if $P$ is also entrywise nonnegative. So, if you want to carry out a numerical experiment to verify $(\ast)$, make sure that the $P$s you generate have both positive and negative entries. One difficulty I met in constructing a proof is that I couldn't make use of the submultiplicativity of the operator norm. Note that tie occurs if $B$ is the all-one matrix, which has spectral norm $n\,(>1)$. If you somehow manage to extract a factor like $\|B\|_2$ from $\|P\circ B\|_2$, that factor may be too large. For a similar reason, the triangle inequality also looks useless.","Given a positive semidefinite matrix $P$, if we scale down its entries individually , will its operator norm always decrease? Put it another way: Suppose $P\in M_n(\mathbb R)$ is positive semidefinite and $B\in M_n(\mathbb R)$ is a $[0,1]$-matrix, i.e. $B$ has all entries between $0$ and $1$ (note: $B$ is not necessarily symmetric). Let $\|\cdot\|_2$ denotes the operator norm (i.e. the largest singular value). Is it always true that    $$\|P\|_2\ge\|P\circ B\|_2?\tag{$\ast$}$$ Background. I ran into this inequality in another question . Having done a numerical experiment, I believed the inequality is true, but I hadn't been able to prove it. If $(\ast)$ turns out to be true, we immediately obtain the analogous inequality $\rho(P)\ge\rho(P\circ B)$ for the spectral radii because $\rho(P)=\|P\|_2\ge\|P\circ B\|_2\ge\rho(P\circ B)$. Remarks. There is much research on inequalities about spectral radii or operator norms of Hadamard products. Often, either all multiplicands in each product are semidefinite or all of them are nonnegative. Inequalities like those two here, which involve mixtures of  semidefinite matrices with nonnegative matrices, are rarely seen. I have tested the inequality for $n=2,3,4,5$ with 100,000 random examples for each $n$. No counterexamples were found. The semidefiniteness condition is essential. If it is removed, counterexamples with symmetric $P$s can be easily obtained. The inequality is known to be true if $P$ is also entrywise nonnegative. So, if you want to carry out a numerical experiment to verify $(\ast)$, make sure that the $P$s you generate have both positive and negative entries. One difficulty I met in constructing a proof is that I couldn't make use of the submultiplicativity of the operator norm. Note that tie occurs if $B$ is the all-one matrix, which has spectral norm $n\,(>1)$. If you somehow manage to extract a factor like $\|B\|_2$ from $\|P\circ B\|_2$, that factor may be too large. For a similar reason, the triangle inequality also looks useless.",,"['linear-algebra', 'matrices', 'inequality', 'normed-spaces', 'hadamard-product']"
18,Identity matrix and its relation to eigenvalues and eigenvectors,Identity matrix and its relation to eigenvalues and eigenvectors,,"Kindly help me understand this statement made by my prof. The identity matrix I has the property that any non zero vector $V$ is an eigenvector of eigenvalue $1$ . My assumption of this statement is that the column vector (1,1) multiplied by the identity matrix is equal to the identity matrix. But the confusing part is when he says ""...any non zero.."". This is implying we can use other values that don't equal one. I believe the eigenvalue would change in light of the different non- $1$ values.","Kindly help me understand this statement made by my prof. The identity matrix I has the property that any non zero vector is an eigenvector of eigenvalue . My assumption of this statement is that the column vector (1,1) multiplied by the identity matrix is equal to the identity matrix. But the confusing part is when he says ""...any non zero.."". This is implying we can use other values that don't equal one. I believe the eigenvalue would change in light of the different non- values.",V 1 1,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
19,Matrices that commute with all matrices [duplicate],Matrices that commute with all matrices [duplicate],,"This question already has answers here : A linear operator commuting with all such operators is a scalar multiple of the identity. (10 answers) Closed 9 years ago . Let $Z_n$ be the set of all $n \times n$ matrices that commute with all $n \times n $ matrices. Show that $$Z_n = \{\lambda I_n \ | \  \lambda \in \mathbb R\}$$ ($I_n$ is the $n \times n$ identity matrix) I don't know how to use $E_{ij}$ (matrix with $1$ in $(i,j)$ and $0$ elsewhere) and the elementary matrix $P_{ij}$ to prove this question. Can anyone explain it please?","This question already has answers here : A linear operator commuting with all such operators is a scalar multiple of the identity. (10 answers) Closed 9 years ago . Let $Z_n$ be the set of all $n \times n$ matrices that commute with all $n \times n $ matrices. Show that $$Z_n = \{\lambda I_n \ | \  \lambda \in \mathbb R\}$$ ($I_n$ is the $n \times n$ identity matrix) I don't know how to use $E_{ij}$ (matrix with $1$ in $(i,j)$ and $0$ elsewhere) and the elementary matrix $P_{ij}$ to prove this question. Can anyone explain it please?",,"['linear-algebra', 'matrices']"
20,Norm with symmetric positive definite matrix,Norm with symmetric positive definite matrix,,"If B is $n \times n$ real symmetric positive definite matrix, then $(x,y) = x^T By$ defines an inner product on $R^n$ . How do you prove that $\|x\|=(x^T B x)^{1/2}$ is a norm on $R^n$ ?","If B is real symmetric positive definite matrix, then defines an inner product on . How do you prove that is a norm on ?","n \times n (x,y) = x^T By R^n \|x\|=(x^T B x)^{1/2} R^n","['linear-algebra', 'normed-spaces', 'inner-products']"
21,Prove or disprove : $\det(A^k + B^k) \geq 0$,Prove or disprove :,\det(A^k + B^k) \geq 0,"This question came from here . As the OP hasn't edited his question and I really want the answer, I'm adding my thoughts. Let $A, B$ be two real $n\times n$ matrices that commute and $\det(A + B)\ge 0$. Prove or disprove : $\forall k\in \mathbb{N}^*$   $$\det(A^k + B^k) \geq 0$$ What I think about it I think we can consider two cases : k even and k odd. For k even, the result is pretty easy considering $$\det(A^{2k}+B^{2k})=\det(A^{k}+iB^{k}){\det(A^{k}-iB^{k})}=\det(A^{k}+iB^{k})\overline {\det(A^{k}+iB^{k})}\ge 0$$ For k odd, I thought about this equality but I don't know if it can help $$A^{2k+1}+B^{2k+1}=(A+B)\left(\sum_{i=0}^{2k} (-1)^i A^{2k-i}B^{i}\right)$$ so $$\det(A^{2k+1}+B^{2k+1})\ge 0 \iff \det\left(\sum_{i=0}^{2k} (-1)^i A^{2k-i}B^{i}\right)\ge 0$$","This question came from here . As the OP hasn't edited his question and I really want the answer, I'm adding my thoughts. Let $A, B$ be two real $n\times n$ matrices that commute and $\det(A + B)\ge 0$. Prove or disprove : $\forall k\in \mathbb{N}^*$   $$\det(A^k + B^k) \geq 0$$ What I think about it I think we can consider two cases : k even and k odd. For k even, the result is pretty easy considering $$\det(A^{2k}+B^{2k})=\det(A^{k}+iB^{k}){\det(A^{k}-iB^{k})}=\det(A^{k}+iB^{k})\overline {\det(A^{k}+iB^{k})}\ge 0$$ For k odd, I thought about this equality but I don't know if it can help $$A^{2k+1}+B^{2k+1}=(A+B)\left(\sum_{i=0}^{2k} (-1)^i A^{2k-i}B^{i}\right)$$ so $$\det(A^{2k+1}+B^{2k+1})\ge 0 \iff \det\left(\sum_{i=0}^{2k} (-1)^i A^{2k-i}B^{i}\right)\ge 0$$",,"['linear-algebra', 'matrices', 'determinant']"
22,Determinant of anti-circulant matrix,Determinant of anti-circulant matrix,,"Find the determinant of the following matrix in the terms of $a_1,a_2,\dots,a_n$ explicitly. $$\begin{bmatrix} a_1 & a_2 & a_3 & \cdots & a_n\\ a_2 & a_3 & a_4 & \cdots & a_1\\ a_3 & a_4 & a_5 & \cdots & a_2\\ \vdots & \vdots & \vdots & \ddots & \vdots\\ a_n & a_1 & a_2 & \cdots & a_{n-1}\\ \end{bmatrix}$$ When is the determinant zero?",Find the determinant of the following matrix in the terms of explicitly. When is the determinant zero?,"a_1,a_2,\dots,a_n \begin{bmatrix}
a_1 & a_2 & a_3 & \cdots & a_n\\
a_2 & a_3 & a_4 & \cdots & a_1\\
a_3 & a_4 & a_5 & \cdots & a_2\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
a_n & a_1 & a_2 & \cdots & a_{n-1}\\
\end{bmatrix}","['linear-algebra', 'matrices', 'determinant', 'numerical-linear-algebra', 'circulant-matrices']"
23,Question on cofactors,Question on cofactors,,"If the row sums of a symmetric matrix of size 4 by 4 are all $0$, then why are all the cofactors of the matrix equal? Thanks in advance for any helpful answers.","If the row sums of a symmetric matrix of size 4 by 4 are all $0$, then why are all the cofactors of the matrix equal? Thanks in advance for any helpful answers.",,"['linear-algebra', 'abstract-algebra', 'matrices']"
24,"Given two subspaces $U,W$ of vector space $V$, how to show that $\dim(U)+\dim(W)=\dim(U+W)+\dim(U\cap W)$","Given two subspaces  of vector space , how to show that","U,W V \dim(U)+\dim(W)=\dim(U+W)+\dim(U\cap W)","Let $U,W$ be subspaces of a vector space $V$. Show that   $$\dim(U)+\dim(W)=\dim(U+W)+\dim(U\cap W)$$ Hint : Show that the map given by $L:U×W\to V$ given by $L(u,w)=u-w$ is linear. I can show that $L:U×W\to V$ given by $L(u,w)=u-w$ is a linear map. I also know that the dimension of $U×W$ is $\dim(U)+\dim(W)$. What do I do next? Any hints?","Let $U,W$ be subspaces of a vector space $V$. Show that   $$\dim(U)+\dim(W)=\dim(U+W)+\dim(U\cap W)$$ Hint : Show that the map given by $L:U×W\to V$ given by $L(u,w)=u-w$ is linear. I can show that $L:U×W\to V$ given by $L(u,w)=u-w$ is a linear map. I also know that the dimension of $U×W$ is $\dim(U)+\dim(W)$. What do I do next? Any hints?",,['linear-algebra']
25,How do I get the parametric form solution of a linear system from reduced row-echelon form?,How do I get the parametric form solution of a linear system from reduced row-echelon form?,,"I have the following system of equations: x 1 + 6x 2 + 2x 3 - 5x 4 = 0 -x 1 - 6x 2 - x 3 - 3x 4 = 0 2x 1 + 612x 2 + 5x 3 - 18x 4 = 0 and I understand that it translated into the following matrix: 1       6     2     -5     0 -1     -6    -1     -3     0 2      12     5     -18    0 Finally, I understand how to use Gauss-Jordan elimination to change this to reduced row-echelon form: 1    6    0   11    0 0    0    1   -8    0 0    0    0    0    0 However, in an example solution that my instructor has prepared, this is then used to find the general solution in parametric form: x1 = -6s - 11t x2 = s x3 = 8t x4 = t No intermediate steps are given. I can see that a similarity in the numbers, but I'm not sure exactly what to do. It looks like arbitrary letter variables have been assigned to those columns which don't start any row with a one and then these variables are used to complete equations for the columns which do start rows. Is that all this is? Or is there something that I'm missing?","I have the following system of equations: x 1 + 6x 2 + 2x 3 - 5x 4 = 0 -x 1 - 6x 2 - x 3 - 3x 4 = 0 2x 1 + 612x 2 + 5x 3 - 18x 4 = 0 and I understand that it translated into the following matrix: 1       6     2     -5     0 -1     -6    -1     -3     0 2      12     5     -18    0 Finally, I understand how to use Gauss-Jordan elimination to change this to reduced row-echelon form: 1    6    0   11    0 0    0    1   -8    0 0    0    0    0    0 However, in an example solution that my instructor has prepared, this is then used to find the general solution in parametric form: x1 = -6s - 11t x2 = s x3 = 8t x4 = t No intermediate steps are given. I can see that a similarity in the numbers, but I'm not sure exactly what to do. It looks like arbitrary letter variables have been assigned to those columns which don't start any row with a one and then these variables are used to complete equations for the columns which do start rows. Is that all this is? Or is there something that I'm missing?",,['linear-algebra']
26,Are a square matrix's columns and rows either both(separately) linearly independent or both(separately) linearly dependent?,Are a square matrix's columns and rows either both(separately) linearly independent or both(separately) linearly dependent?,,"Prove or disprove: Given a square matrix $A$,the columns of $A$ are linearly independent iff. the rows of $A$ are linearly independent.","Prove or disprove: Given a square matrix $A$,the columns of $A$ are linearly independent iff. the rows of $A$ are linearly independent.",,"['linear-algebra', 'matrices']"
27,Eigenvalues of some peculiar matrices,Eigenvalues of some peculiar matrices,,"While I was toying around with matrices, I chanced upon a family of tridiagonal matrices $M_n$ that take the following form: the superdiagonal entries are all $1$'s, the diagonal entries take the form $m_{j,j}=4 j (2 n + 3) - 8 j^2 - 6 n - 5$, and the subdiagonal entries take the form $m_{j+1,j}=4 j (2 j - 1) (2 n - 2 j + 1) (n - j)$. For example, the $4\times 4$ member of this family looks like this: $$M_4=\begin{pmatrix} 7 & 1 & 0 & 0 \\ 84 & 27 & 1 & 0 \\ 0 & 240 & 31 & 1 \\ 0 & 0 & 180 & 19\end{pmatrix}$$ I checked the eigenvalues of members of this family and I found that each member has the squares of the first few odd integers as eigenvalues. (For example, the eigenvalues of $M_4$ are $1,9,25,49$.) I couldn't find a way to prove this though. I wish someone would help me! Thanks!","While I was toying around with matrices, I chanced upon a family of tridiagonal matrices $M_n$ that take the following form: the superdiagonal entries are all $1$'s, the diagonal entries take the form $m_{j,j}=4 j (2 n + 3) - 8 j^2 - 6 n - 5$, and the subdiagonal entries take the form $m_{j+1,j}=4 j (2 j - 1) (2 n - 2 j + 1) (n - j)$. For example, the $4\times 4$ member of this family looks like this: $$M_4=\begin{pmatrix} 7 & 1 & 0 & 0 \\ 84 & 27 & 1 & 0 \\ 0 & 240 & 31 & 1 \\ 0 & 0 & 180 & 19\end{pmatrix}$$ I checked the eigenvalues of members of this family and I found that each member has the squares of the first few odd integers as eigenvalues. (For example, the eigenvalues of $M_4$ are $1,9,25,49$.) I couldn't find a way to prove this though. I wish someone would help me! Thanks!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
28,Calculating an Angle from $2$ points in space,Calculating an Angle from  points in space,2,"Given two points $p_1$ , $p_2$ around the origin $(0,0)$ in $2D$ space, how would you calculate the angle from $p_1$ to $p_2$ ? How would this change in $3D$ space?","Given two points , around the origin in space, how would you calculate the angle from to ? How would this change in space?","p_1 p_2 (0,0) 2D p_1 p_2 3D","['linear-algebra', 'geometry']"
29,"Intuitively, $why$ do defective matrices have extra eigenvalues?","Intuitively,  do defective matrices have extra eigenvalues?",why,"I've recently seen the example on how a shear $$\begin{bmatrix} 1 & 1\\ 0 & 1\end{bmatrix}$$ is an example of a defective matrix, since it has eigenvalues $1,1$ but only one independent eigenvector $\mathbf{v}_1 = (1,0)$ . So in this case, I can see that the algebraic multiplicity is greater than the geometric multiplicity, But what I am wondering is why the system created ""two"" eigenvalues when only one of them was ""actually an eigenvalue"" - (It's not even that there were two eigenvectors where each one corresponded $1$ , $1$ ). Is there any underlying reason why the system came up with two eigenvalues? (If I was to intuitively guess what would happen purely from a geometric perspective, I would have guessed that the characteristic polynomial would just be linear: $\lambda - 1$ . Although this isn't possible for a $2 \times 2$ , I was wondering whether there was any other meaning for the second eigenvalue?)","I've recently seen the example on how a shear is an example of a defective matrix, since it has eigenvalues but only one independent eigenvector . So in this case, I can see that the algebraic multiplicity is greater than the geometric multiplicity, But what I am wondering is why the system created ""two"" eigenvalues when only one of them was ""actually an eigenvalue"" - (It's not even that there were two eigenvectors where each one corresponded , ). Is there any underlying reason why the system came up with two eigenvalues? (If I was to intuitively guess what would happen purely from a geometric perspective, I would have guessed that the characteristic polynomial would just be linear: . Although this isn't possible for a , I was wondering whether there was any other meaning for the second eigenvalue?)","\begin{bmatrix} 1 & 1\\ 0 & 1\end{bmatrix} 1,1 \mathbf{v}_1 = (1,0) 1 1 \lambda - 1 2 \times 2","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'linear-transformations', 'intuition']"
30,Orthogonal complement in a finite field ${\mathbb Z}^{n}_{q}$,Orthogonal complement in a finite field,{\mathbb Z}^{n}_{q},"When $V=\mathbb{Z}^n_q$ is a vector space, where $\mathbb Z_q$ is the set of integers modulo prime $q>2$, are the following statements true? If $U ⊂ V$ is a $k$-dimensional subspace, then   $$ U^⊥= \{ x∈V∣x^Tu=0 \ \mathrm {for\ all}\ u∈U \} $$   is of dimension $n-k$. (If statement 1 is true) $U^⊥$ can be written by $n-k$ orthogonal vectors. For an example of statement 1, if $V=\mathbb{Z}^3_3$, and $U=\{ a_1(1,2,1)^T+a_2(0,1,1)^T\ |\ \forall a_1,a_2\in\mathbb Z_3\} $  then $U^⊥$ can be expressed with basis $(1,2,1)^T$. I'm not familiar with using linear algebra in finite fields, so I don't know how I should handle ""orthogonal"" vectors. I wrote down some examples and am pretty sure that statement 1 holds in finite fields, as it does in real case, but I'm not so certain of statement 2. Furthermore, I don't have a clue on how to prove these statements. I'm not used to using English, so if there's anything wrong with my explanation, don't hesitate to ask.  Thanks for your help!","When $V=\mathbb{Z}^n_q$ is a vector space, where $\mathbb Z_q$ is the set of integers modulo prime $q>2$, are the following statements true? If $U ⊂ V$ is a $k$-dimensional subspace, then   $$ U^⊥= \{ x∈V∣x^Tu=0 \ \mathrm {for\ all}\ u∈U \} $$   is of dimension $n-k$. (If statement 1 is true) $U^⊥$ can be written by $n-k$ orthogonal vectors. For an example of statement 1, if $V=\mathbb{Z}^3_3$, and $U=\{ a_1(1,2,1)^T+a_2(0,1,1)^T\ |\ \forall a_1,a_2\in\mathbb Z_3\} $  then $U^⊥$ can be expressed with basis $(1,2,1)^T$. I'm not familiar with using linear algebra in finite fields, so I don't know how I should handle ""orthogonal"" vectors. I wrote down some examples and am pretty sure that statement 1 holds in finite fields, as it does in real case, but I'm not so certain of statement 2. Furthermore, I don't have a clue on how to prove these statements. I'm not used to using English, so if there's anything wrong with my explanation, don't hesitate to ask.  Thanks for your help!",,"['linear-algebra', 'abstract-algebra', 'vector-spaces']"
31,How to choose the starting row when computing the reduced row echelon form?,How to choose the starting row when computing the reduced row echelon form?,,"I'm having hell of a time going around solving matrices to reduced row echelon form. My main issue is which row to start simplifying values and based on what? I have this example so again, the questions are: 1.Which row to start simplifying values? 2.Based on what criteria? Our professor solved it in the class with no fractions but I could not do it. Even though  I know the 3 operations performed on matrices","I'm having hell of a time going around solving matrices to reduced row echelon form. My main issue is which row to start simplifying values and based on what? I have this example so again, the questions are: 1.Which row to start simplifying values? 2.Based on what criteria? Our professor solved it in the class with no fractions but I could not do it. Even though  I know the 3 operations performed on matrices",,"['linear-algebra', 'matrices']"
32,Why are there $736$ matrices $M\in \mathcal M_2(\mathbb{Z}_{26})$ for which it holds that $M=M^{-1}$?,Why are there  matrices  for which it holds that ?,736 M\in \mathcal M_2(\mathbb{Z}_{26}) M=M^{-1},"I'm currently trying to introduce myself to cryptography. I'm reading about the Hill Cipher currently in the book Applied Abstract Algebra. The Hill Cipher uses an invertible matrix $M$ for encryption and the inverse matrix $M^{-1}$ for decryption. The book recommends to use a matrix $M$ for which it holds that $M=M^{-1}$, since we then have the same key for encryption and decryption. In the book, they use a $2 \times 2$ matrix $M$ over $\mathbb{Z}_{26}$ as example, and state that for there are 736 $2 \times 2$ matrices for which it hold that $M=M^{-1}$. I'm trying to pick up on as much as possible when reading things, since I find it counter-productive for learning to skip something, when you don't get the theory behind it. Can someone enlighten to me, as to why it is that there are 736 possible $2 \times 2$ $M=M^{-1}$ matrices and how to find them?","I'm currently trying to introduce myself to cryptography. I'm reading about the Hill Cipher currently in the book Applied Abstract Algebra. The Hill Cipher uses an invertible matrix $M$ for encryption and the inverse matrix $M^{-1}$ for decryption. The book recommends to use a matrix $M$ for which it holds that $M=M^{-1}$, since we then have the same key for encryption and decryption. In the book, they use a $2 \times 2$ matrix $M$ over $\mathbb{Z}_{26}$ as example, and state that for there are 736 $2 \times 2$ matrices for which it hold that $M=M^{-1}$. I'm trying to pick up on as much as possible when reading things, since I find it counter-productive for learning to skip something, when you don't get the theory behind it. Can someone enlighten to me, as to why it is that there are 736 possible $2 \times 2$ $M=M^{-1}$ matrices and how to find them?",,"['linear-algebra', 'matrices', 'modular-arithmetic', 'cryptography']"
33,What are mandatory conditions for a family of matrices to commute?,What are mandatory conditions for a family of matrices to commute?,,Suppose that there are some matrices. Each matrix in the set must commute with another in the set. What are the mandatory conditions for this?,Suppose that there are some matrices. Each matrix in the set must commute with another in the set. What are the mandatory conditions for this?,,"['linear-algebra', 'matrices']"
34,Nilpotent matrices over field of characteristic zero,Nilpotent matrices over field of characteristic zero,,"From this Wikipedia link . Let $K$ be a field of characteristic zero and let $A$ be an $n \times n$ matrix over $K$ . Prove the following: (a) $N$ is nilpotent iff $\mathrm{tr}(N^m)=0$ for all $0<m \leq n$ iff $\mathrm{tr}(N^m)=0$ for all $m \in \mathbb{N_{+}}$ . (b) If $N$ is nilpotent, then $N$ is similar to a strictly upper triangular matrix. Additional Query: Is above true if $K$ is not assumed to be algebraically closed? (Then one can't apply Jordan normal form anymore.)","From this Wikipedia link . Let be a field of characteristic zero and let be an matrix over . Prove the following: (a) is nilpotent iff for all iff for all . (b) If is nilpotent, then is similar to a strictly upper triangular matrix. Additional Query: Is above true if is not assumed to be algebraically closed? (Then one can't apply Jordan normal form anymore.)",K A n \times n K N \mathrm{tr}(N^m)=0 0<m \leq n \mathrm{tr}(N^m)=0 m \in \mathbb{N_{+}} N N K,"['linear-algebra', 'matrices']"
35,Solving non-square linear matrix equations,Solving non-square linear matrix equations,,"Let's say we have $${\bf A} = {\bf B} {\bf X}$$ where $\bf A$ and $\bf B$ are known matrices, $\bf X$ is unknown. In case $\bf B$ was square, a solution can be found by ${\bf B}^{-1} {\bf A} = {\bf X}$ . But how do you attempt to solve for $\bf X$ when $\bf B$ is not square, i.e., $n \neq m$ ?","Let's say we have where and are known matrices, is unknown. In case was square, a solution can be found by . But how do you attempt to solve for when is not square, i.e., ?",{\bf A} = {\bf B} {\bf X} \bf A \bf B \bf X \bf B {\bf B}^{-1} {\bf A} = {\bf X} \bf X \bf B n \neq m,"['linear-algebra', 'matrices', 'matrix-equations']"
36,Eigenvalues of product of a matrix and a diagonal matrix,Eigenvalues of product of a matrix and a diagonal matrix,,"My situation is as follows: I have a symmetric positive semi-definite matrix L (the Laplacian matrix of a graph) and a diagonal matrix S with positive entries $s_i$. There's plenty of literature on the spectrum of $L$, and I'm most interested in bounds on the second-lowest eigenvalue, $\lambda_2$. Now the thing is that I'm not using the Laplacian $L$ itself, but rather the 'generalized' Laplacian $L S^{-1}$. I still need results on its second lowest eigenvalue $\lambda_2$ (note that the lowest eigenvalue of the Laplacian, both the normal and the generalized, is 0). My question is: Are there some readily available theorems/lemmata that allow me to relate the spectra of $L$ and $L S^{-1}$? EDIT: Of course, $LS^{-1}$ is not a symmetric matrix any more, so I'm talking about its right-eigenvectors. The eigenvalues of $LS^{-1}$ are the same as those of  $S^{-1/2} L S^{-1/2}$ which again is a symmetric positive semi-definite matrix, so I know an eigenbasis actually exists.","My situation is as follows: I have a symmetric positive semi-definite matrix L (the Laplacian matrix of a graph) and a diagonal matrix S with positive entries $s_i$. There's plenty of literature on the spectrum of $L$, and I'm most interested in bounds on the second-lowest eigenvalue, $\lambda_2$. Now the thing is that I'm not using the Laplacian $L$ itself, but rather the 'generalized' Laplacian $L S^{-1}$. I still need results on its second lowest eigenvalue $\lambda_2$ (note that the lowest eigenvalue of the Laplacian, both the normal and the generalized, is 0). My question is: Are there some readily available theorems/lemmata that allow me to relate the spectra of $L$ and $L S^{-1}$? EDIT: Of course, $LS^{-1}$ is not a symmetric matrix any more, so I'm talking about its right-eigenvectors. The eigenvalues of $LS^{-1}$ are the same as those of  $S^{-1/2} L S^{-1/2}$ which again is a symmetric positive semi-definite matrix, so I know an eigenbasis actually exists.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
37,Why non-trivial solution only if determinant is zero,Why non-trivial solution only if determinant is zero,,The equation $(A−λI)x=0$ has a nontrivial solution (a solution where $x≠0$) if and only if  $det(A−λI)=0$. Why is that? How can this be proven?,The equation $(A−λI)x=0$ has a nontrivial solution (a solution where $x≠0$) if and only if  $det(A−λI)=0$. Why is that? How can this be proven?,,"['linear-algebra', 'determinant']"
38,Does $AB+BA=0$ imply $AB=BA=0$ when $A$ is real semidefinite matrix?,Does  imply  when  is real semidefinite matrix?,AB+BA=0 AB=BA=0 A,This is a general question that came to my mind while listening to a lecture(although its framing may make it look like a textbook question). Suppose that $A$ and $B$ be real matrices. $A$ is symmetric and positive semi-definite $(x^tAx\ge0\ \ \forall x\in \mathbb{R}^n)$. Let $AB+BA=0$. Does this imply $AB=BA=0$?  If yes can you give me some example?,This is a general question that came to my mind while listening to a lecture(although its framing may make it look like a textbook question). Suppose that $A$ and $B$ be real matrices. $A$ is symmetric and positive semi-definite $(x^tAx\ge0\ \ \forall x\in \mathbb{R}^n)$. Let $AB+BA=0$. Does this imply $AB=BA=0$?  If yes can you give me some example?,,"['linear-algebra', 'matrices', 'positive-definite']"
39,Why do the columns of a unitary matrix form an orthonormal basis?,Why do the columns of a unitary matrix form an orthonormal basis?,,"So I'm trying to understand why the columns of a unitary matrix form an orthonormal basis. I know it has something to do with the inner product, but I don't fully understand that either (we learned all of this together this past week). I've searched here and done a google search, and everything I found seems to assume I would understand the connection between the inner product and why it would be important, or they rely on eigenvalues/vectors, which we haven't explicitly learned about yet. If anyone is able to help with this, I would appreciate it!","So I'm trying to understand why the columns of a unitary matrix form an orthonormal basis. I know it has something to do with the inner product, but I don't fully understand that either (we learned all of this together this past week). I've searched here and done a google search, and everything I found seems to assume I would understand the connection between the inner product and why it would be important, or they rely on eigenvalues/vectors, which we haven't explicitly learned about yet. If anyone is able to help with this, I would appreciate it!",,"['linear-algebra', 'matrices', 'orthonormal']"
40,"If a real $2\times2$ matrix satisfies $A^4=I$, does it follow that $A^2=\pm I$?","If a real  matrix satisfies , does it follow that ?",2\times2 A^4=I A^2=\pm I,"I would like to know if a  $2 \times 2$ matrix $A$ satisfies $A^4=I$, also satisfies $A^2=I$ or $A^2=-I$ if all elements of $A$ are real.  Also, I would really appreciate your help on further generalizations.  i.e) Properties of $A$ such that $A^n=I$.","I would like to know if a  $2 \times 2$ matrix $A$ satisfies $A^4=I$, also satisfies $A^2=I$ or $A^2=-I$ if all elements of $A$ are real.  Also, I would really appreciate your help on further generalizations.  i.e) Properties of $A$ such that $A^n=I$.",,"['linear-algebra', 'matrices']"
41,An element of $GL_n(\mathbb F_p)$ cannot have order $p^2$ if $n < p$,An element of  cannot have order  if,GL_n(\mathbb F_p) p^2 n < p,"I'm preparing for my graduate program's entrance exams, and I came across this problem when studying. Our study group came up with a solution, but I wanted to ask if it was actually correct, since after reflection I think there is a big hole. I'm also curious if there is a solution that doesn't involve linear algebra. The Problem Let $G$ be the direct product of $n$ copies of $\mathbb Z_p$, where $n < p$. Show that no automorphism of $G$ has order $p^2$. Solution It is easy to see that the automorphism group of $G$ is isomorphic to $GL_n(\mathbb F_p)$, so it suffices to show that no matrix in $GL_n(\mathbb F_p)$ has order $p^2$. Let $M \in GL_n(\mathbb F_p)$ satisfy $M^{p^2} = I$. Then $M$ satisfies the polynomial $x^{p^2} - 1$, which over a field of characteristic $p$ is equal to $(x-1)^{p^2}$, and it follows that the eigenvalues of $M$ are all $1$. (This is the potentially problematic step: it might follow from the Cayley-Hamilton theorem and the fact that the characteristic polynomial of $M$ divides $x^{p^2} - 1$, but I'm not clear that the second thing is true...) Now that we know that $M$ has only $1$ as an eigenvalue, consider the Jordan decomposition of $M$ over the algebraic closure $k$ of $\mathbb F_p$. It consists of Jordan blocks with (generalized) eigenvalue $1$ of size at most $n < p$, which means that the $p$th power of each block is the identity. Thus $M^p = I$ and $M$ cannot have order $p^2$.","I'm preparing for my graduate program's entrance exams, and I came across this problem when studying. Our study group came up with a solution, but I wanted to ask if it was actually correct, since after reflection I think there is a big hole. I'm also curious if there is a solution that doesn't involve linear algebra. The Problem Let $G$ be the direct product of $n$ copies of $\mathbb Z_p$, where $n < p$. Show that no automorphism of $G$ has order $p^2$. Solution It is easy to see that the automorphism group of $G$ is isomorphic to $GL_n(\mathbb F_p)$, so it suffices to show that no matrix in $GL_n(\mathbb F_p)$ has order $p^2$. Let $M \in GL_n(\mathbb F_p)$ satisfy $M^{p^2} = I$. Then $M$ satisfies the polynomial $x^{p^2} - 1$, which over a field of characteristic $p$ is equal to $(x-1)^{p^2}$, and it follows that the eigenvalues of $M$ are all $1$. (This is the potentially problematic step: it might follow from the Cayley-Hamilton theorem and the fact that the characteristic polynomial of $M$ divides $x^{p^2} - 1$, but I'm not clear that the second thing is true...) Now that we know that $M$ has only $1$ as an eigenvalue, consider the Jordan decomposition of $M$ over the algebraic closure $k$ of $\mathbb F_p$. It consists of Jordan blocks with (generalized) eigenvalue $1$ of size at most $n < p$, which means that the $p$th power of each block is the identity. Thus $M^p = I$ and $M$ cannot have order $p^2$.",,"['linear-algebra', 'group-theory', 'finite-groups']"
42,The dimension of centralizer of a Matrix.,The dimension of centralizer of a Matrix.,,"Let $A$ be a $n\times n$ matrix with characteristic polynomial   $$(x-c_{1})^{{d}_{1}}(x-c_{2})^{{d}_{2}}...(x-c_{k})^{{d}_{k}}$$ where $c_{1},c_{2},...,c_{k}$ are distinct. Let $V$ be the space of $n\times n$ matrics $B$ such that $AB=BA$. How to find dimension of this vector space? Clearly it is easy to find dimension if the matrix $A$ is given diagonalizable  but how to find dimension if matrix $A$ is not diagonalizable. I tried it by using Jordan canonical form but its very lengthy and only gives possible dimensions. Can some one suggest how to find by giving a particular  matrix.","Let $A$ be a $n\times n$ matrix with characteristic polynomial   $$(x-c_{1})^{{d}_{1}}(x-c_{2})^{{d}_{2}}...(x-c_{k})^{{d}_{k}}$$ where $c_{1},c_{2},...,c_{k}$ are distinct. Let $V$ be the space of $n\times n$ matrics $B$ such that $AB=BA$. How to find dimension of this vector space? Clearly it is easy to find dimension if the matrix $A$ is given diagonalizable  but how to find dimension if matrix $A$ is not diagonalizable. I tried it by using Jordan canonical form but its very lengthy and only gives possible dimensions. Can some one suggest how to find by giving a particular  matrix.",,['linear-algebra']
43,$\sin$ and $\cos$ are the basis of what space?,and  are the basis of what space?,\sin \cos,"When learning Fourier expansions, we learn that $\{\sin(mx), \cos(mx)\}_{m \in \Bbb N}$ is an orthonormal basis for our space and thus we can expand functions in it.  My question is what space is this exactly? Is it the space of all functions $f: \Bbb R \to \Bbb R$?  I doubt it.  There are some really weird functions.  Or is it continuous functions?  Or periodic functions?  Or analytic functions?  Or just some set of functions that can only be described as the span of the above basis functions?","When learning Fourier expansions, we learn that $\{\sin(mx), \cos(mx)\}_{m \in \Bbb N}$ is an orthonormal basis for our space and thus we can expand functions in it.  My question is what space is this exactly? Is it the space of all functions $f: \Bbb R \to \Bbb R$?  I doubt it.  There are some really weird functions.  Or is it continuous functions?  Or periodic functions?  Or analytic functions?  Or just some set of functions that can only be described as the span of the above basis functions?",,"['linear-algebra', 'functional-analysis', 'fourier-analysis', 'hilbert-spaces']"
44,"generic rule matrix differentiation (Hadamard Product, element-wise)","generic rule matrix differentiation (Hadamard Product, element-wise)",,"I struggle with taking the derivative of the Hadamard-Product? Let us consider $f(x)=x^TAx=x^T(Ax)$. We know $$\frac{\partial}{\partial x} x^TAx = (A+A^T)x.$$ The Matrix-Cookbook claimed $d(XY)=d(X)Y+Xd(Y)$ and $$\frac{\partial}{\partial x} x^Ta = \frac{\partial}{\partial x}a^Tx = a.$$ Setting $X:=x^T$ and $Y:=Ax$ we have \begin{align*} X &= x^TE & d_x(X) = E\\ Y &= Ax & d_x(Y) = A\\ \end{align*} This gives $$d(XY)=d(X)Y+Xd(Y)= 1^TAx + x^TA= Ax + x^TA$$ This format (dimension) is incorrect. A generic rule seems to be $d(XY)=d(X)Y+(Xd(Y))^T$ instead?  What does a generic rule look like for the Hadamard product $[a\odot b]_i=[a]_i\cdot [b]_i$? The Matrix-Cookbook states: $$d(X\odot Y)=d(X)\odot Y+X\odot d(Y).$$ For example, the derivative of $$ (x\odot y)^TA(x\odot y)$$ we have \begin{align*} X &= (x\odot y)^TE & d_x(X) &= \left(d(x\odot y)\right)^TE\\ & &  &= \left(1\odot y + x\odot 0\right)^TE\\ & &  &= y^TE\\ Y &= A(x\odot y) & d_x(Y) &= A\left(d(x\odot y)\right)\\  &  &  &=A\left(1\odot y + x\odot 0\right)\\  &  &  &=Ay\\ \end{align*} Which implies  \begin{align*} d_x((x\odot y)^TA(x\odot y))&=y^T\odot A(x\odot y) + \left( (x\odot y)^T Ay \right)^T\\ &=y^T\odot A(x\odot y) + y^TA^T(x\odot y) \end{align*} as a derivative. But the correct derivative should be $$2y\odot  A(x\odot y)$$ What is missing?","I struggle with taking the derivative of the Hadamard-Product? Let us consider $f(x)=x^TAx=x^T(Ax)$. We know $$\frac{\partial}{\partial x} x^TAx = (A+A^T)x.$$ The Matrix-Cookbook claimed $d(XY)=d(X)Y+Xd(Y)$ and $$\frac{\partial}{\partial x} x^Ta = \frac{\partial}{\partial x}a^Tx = a.$$ Setting $X:=x^T$ and $Y:=Ax$ we have \begin{align*} X &= x^TE & d_x(X) = E\\ Y &= Ax & d_x(Y) = A\\ \end{align*} This gives $$d(XY)=d(X)Y+Xd(Y)= 1^TAx + x^TA= Ax + x^TA$$ This format (dimension) is incorrect. A generic rule seems to be $d(XY)=d(X)Y+(Xd(Y))^T$ instead?  What does a generic rule look like for the Hadamard product $[a\odot b]_i=[a]_i\cdot [b]_i$? The Matrix-Cookbook states: $$d(X\odot Y)=d(X)\odot Y+X\odot d(Y).$$ For example, the derivative of $$ (x\odot y)^TA(x\odot y)$$ we have \begin{align*} X &= (x\odot y)^TE & d_x(X) &= \left(d(x\odot y)\right)^TE\\ & &  &= \left(1\odot y + x\odot 0\right)^TE\\ & &  &= y^TE\\ Y &= A(x\odot y) & d_x(Y) &= A\left(d(x\odot y)\right)\\  &  &  &=A\left(1\odot y + x\odot 0\right)\\  &  &  &=Ay\\ \end{align*} Which implies  \begin{align*} d_x((x\odot y)^TA(x\odot y))&=y^T\odot A(x\odot y) + \left( (x\odot y)^T Ay \right)^T\\ &=y^T\odot A(x\odot y) + y^TA^T(x\odot y) \end{align*} as a derivative. But the correct derivative should be $$2y\odot  A(x\odot y)$$ What is missing?",,"['linear-algebra', 'derivatives', 'matrix-calculus', 'hadamard-product']"
45,Geometric interpretation of Laplace's formula for determinants,Geometric interpretation of Laplace's formula for determinants,,"Coming from the geometric point of view, the determinant of an $n \times n$-Matrix computes the volume of an parallelepiped spanned by the columns of the matrix. In context of this question, let the determinant be defined by Laplace's formula and let us assume that we use the Laplace expansion along the first column of the matrix. In case of a $2\times 2$-matrix of the form $M=\begin{pmatrix} a & c\\ b & d\end{pmatrix}$, the expansion is quite intuitive: One takes $a$ - which can be seen as the projection of the vektor $\begin{pmatrix} a \\ b \end{pmatrix}$ onto the $e_1$-axis - and multiplies it with $d$, which can be seen as the projection of the vector $\begin{pmatrix} c \\ d \end{pmatrix}$ onto the $e_2$-axis. So it's the base $\times$ height-formula which would be correct if $b=0$. If $b \neq 0$, the error which is made is exactly $c \cdot d$, again by a base $\times$ height-formula, which one can check with the help of a figure by Salomon Golomb, which I found here: Why determinant of a 2 by 2 matrix is the area of a parallelogram? I hoped to transfer this principle to $3 \times 3$-matrices. I considered the parallelogram spanned by the 2nd and the 3rd column of a $3 \times 3$-matrix. Keeping Laplace's formula in mind, the first entry of the first column is multiplied with the area of the projection of the parallelogram onto the $e_2$-$e_3$-plane, the second entry is multiplied with the area of the projection onto the $e_1$-$e_3$-plane and the third entry is multiplied with the area of the projection onto the $e_1$-$e_2$-plane. (That is, $\det(a,b,c)= c\cdot (a\times b)$). I tried to give an geometric sense to this calculations mimicking what was done in Golomb's figure but I failed. Sure, there is a way to rearrange the three sets to conclude that actually the volume of the parallelepiped was computed but again I would not understand why. Is the anyone who knows a way to interprete Laplace's formula geometrically? I think there might be a way which uses the projections of the parallelogram which I mentioned above... I would be very thankful for all other (geometric) interpretations, too! Ole","Coming from the geometric point of view, the determinant of an $n \times n$-Matrix computes the volume of an parallelepiped spanned by the columns of the matrix. In context of this question, let the determinant be defined by Laplace's formula and let us assume that we use the Laplace expansion along the first column of the matrix. In case of a $2\times 2$-matrix of the form $M=\begin{pmatrix} a & c\\ b & d\end{pmatrix}$, the expansion is quite intuitive: One takes $a$ - which can be seen as the projection of the vektor $\begin{pmatrix} a \\ b \end{pmatrix}$ onto the $e_1$-axis - and multiplies it with $d$, which can be seen as the projection of the vector $\begin{pmatrix} c \\ d \end{pmatrix}$ onto the $e_2$-axis. So it's the base $\times$ height-formula which would be correct if $b=0$. If $b \neq 0$, the error which is made is exactly $c \cdot d$, again by a base $\times$ height-formula, which one can check with the help of a figure by Salomon Golomb, which I found here: Why determinant of a 2 by 2 matrix is the area of a parallelogram? I hoped to transfer this principle to $3 \times 3$-matrices. I considered the parallelogram spanned by the 2nd and the 3rd column of a $3 \times 3$-matrix. Keeping Laplace's formula in mind, the first entry of the first column is multiplied with the area of the projection of the parallelogram onto the $e_2$-$e_3$-plane, the second entry is multiplied with the area of the projection onto the $e_1$-$e_3$-plane and the third entry is multiplied with the area of the projection onto the $e_1$-$e_2$-plane. (That is, $\det(a,b,c)= c\cdot (a\times b)$). I tried to give an geometric sense to this calculations mimicking what was done in Golomb's figure but I failed. Sure, there is a way to rearrange the three sets to conclude that actually the volume of the parallelepiped was computed but again I would not understand why. Is the anyone who knows a way to interprete Laplace's formula geometrically? I think there might be a way which uses the projections of the parallelogram which I mentioned above... I would be very thankful for all other (geometric) interpretations, too! Ole",,"['linear-algebra', 'geometry', 'differential-geometry']"
46,Example of similar matrices $A$ and $B$ such that products $AB$ and $BA$ are not similar,Example of similar matrices  and  such that products  and  are not similar,A B AB BA,"I'm looking for the simplest possible example of square matrices $A$ and $B$ such that $A$ is similar to $B$, $AB$ is not similar to $BA$. Such an example should exist, but I would like to find the ""smallest"" one. If either $A$ or $B$ is invertible, then $AB$ will be similar to $BA$, so one needs to look at singular matrices.","I'm looking for the simplest possible example of square matrices $A$ and $B$ such that $A$ is similar to $B$, $AB$ is not similar to $BA$. Such an example should exist, but I would like to find the ""smallest"" one. If either $A$ or $B$ is invertible, then $AB$ will be similar to $BA$, so one needs to look at singular matrices.",,"['linear-algebra', 'matrices', 'examples-counterexamples']"
47,Calculating the trace of the product of two matrices,Calculating the trace of the product of two matrices,,"I have to calculated $\mbox{trace}(A^{-1}B)$ where $A$ is a symmetric positive definite matrix and $B$ is a symmetric matrix, very sparse with only two elements non zero. I want to find a way that I could calculate the above expression efficiently specially when A and B are high dimensional like $10000\times 10000. $  What is the best way to do it. I have a bunch of Bs each very sparse with only two non zero values. I cannot store $A^{-1}$ since it is dense and I won't have enough memory. Any efficient ways/tricks to do it efficiently, like trace properties or something?","I have to calculated $\mbox{trace}(A^{-1}B)$ where $A$ is a symmetric positive definite matrix and $B$ is a symmetric matrix, very sparse with only two elements non zero. I want to find a way that I could calculate the above expression efficiently specially when A and B are high dimensional like $10000\times 10000. $  What is the best way to do it. I have a bunch of Bs each very sparse with only two non zero values. I cannot store $A^{-1}$ since it is dense and I won't have enough memory. Any efficient ways/tricks to do it efficiently, like trace properties or something?",,"['linear-algebra', 'inverse', 'trace']"
48,$GL_n(\mathbb F_q)$ has an element of order $q^n-1$,has an element of order,GL_n(\mathbb F_q) q^n-1,For fixed prime power $q$ show that the general linear group $GL_n(\mathbb F_q)$ of invertible matrices with entries in the finite field $\mathbb F_q$ has an element of order $q^n-1$. I tried to show this question with showing diagonal matrix but i can't find element directly  competible with order i think i am on wrong way please give me clue ?,For fixed prime power $q$ show that the general linear group $GL_n(\mathbb F_q)$ of invertible matrices with entries in the finite field $\mathbb F_q$ has an element of order $q^n-1$. I tried to show this question with showing diagonal matrix but i can't find element directly  competible with order i think i am on wrong way please give me clue ?,,"['linear-algebra', 'abstract-algebra', 'finite-fields', 'linear-groups']"
49,Equation of plane containing two vectors,Equation of plane containing two vectors,,"I am struggling with the interpretation of this question: Vectors: $u = \left(1,\ 0,\ \sqrt3 \right)$ and $v = (1,\ \sqrt3,\ 0)$ in standard position. Find an equation of the plane containing $u$ and $v$. Am I correct in interpreting this question that the plane is parallel to both vectors? Does this mean that I can form the cross product of $u$ and $v$ to find the normal and use one of the vectors as a point on the plane? Thank you in advance","I am struggling with the interpretation of this question: Vectors: $u = \left(1,\ 0,\ \sqrt3 \right)$ and $v = (1,\ \sqrt3,\ 0)$ in standard position. Find an equation of the plane containing $u$ and $v$. Am I correct in interpreting this question that the plane is parallel to both vectors? Does this mean that I can form the cross product of $u$ and $v$ to find the normal and use one of the vectors as a point on the plane? Thank you in advance",,['linear-algebra']
50,Proving that $\|A\|_{\infty}$ the largest row sum of absolute value of matrix $A$,Proving that  the largest row sum of absolute value of matrix,\|A\|_{\infty} A,"I am studying matrix norms. I have read that $\|A\|_{\infty}$ is  the largest row sum of absolute value and $\|A\|_{1}$ is the highest column sum of absolute values of the matrix $A$. However, I am not able to prove this. Are there any proof of these statements? Please help  and thanks for your time. Edit: Where $\|A\|_{\infty}$ is the matrix norm induced by the vector norm $\|x\|_{\infty}$.","I am studying matrix norms. I have read that $\|A\|_{\infty}$ is  the largest row sum of absolute value and $\|A\|_{1}$ is the highest column sum of absolute values of the matrix $A$. However, I am not able to prove this. Are there any proof of these statements? Please help  and thanks for your time. Edit: Where $\|A\|_{\infty}$ is the matrix norm induced by the vector norm $\|x\|_{\infty}$.",,"['linear-algebra', 'matrices', 'normed-spaces']"
51,Can you determine from the minors if the presented module is free?,Can you determine from the minors if the presented module is free?,,"Motivation (you can ignore this part): A problem in Hartshorne (II.5.8c) asks to show that if we have a coherent sheaf $\mathscr{F}$ on a reduced noetherian scheme $X$, and the function $$\varphi(x)=\dim_{k(x)} \mathscr{F}_x \otimes_{\mathcal{O}_x} k(x)$$ is constant on $X$ (where $k(x)$ is the residue field), then in fact $\mathscr{F}$ is locally free. (This is a homework problem for me.) I think that if $X$ is affine (say $=\operatorname{Spec}A$), then $\mathscr{F}$ will actually be free, not just locally. It will certainly be true that $\mathscr{F}$ will be the sheaf associated with a finitely generated $A$-module $M$, so there will be an exact sequence $$0\rightarrow K\rightarrow A^m \rightarrow M\rightarrow 0$$ for some $m\in\mathbb{N}$, and because $A$ is noetherian, $K$ will be finitely generated. Thus $M$ is a finitely presented module, say by the matrix $$\begin{pmatrix} g_{11}&\dots& g_{1r}\\ \vdots &\ddots &\vdots \\ g_{m1}&\dots& g_{mr} \end{pmatrix}$$ Then if $x=\mathfrak{p}\triangleleft A$, $\mathscr{F}_x\otimes_{\mathcal{O}_x}k(x) = M_\mathfrak{p}\otimes_{A_\mathfrak{p}}k(\mathfrak{p}) = M\otimes_A k(\mathfrak{p})$ is precisely the module presented by this matrix except with the entries interpreted as elements of $k(\mathfrak{p})$. The condition that $\varphi(x)$ is constant means that the rank of this matrix (with entries interpreted in $k(\mathfrak{p})$) doesn't depend on $\mathfrak{p}$. (Say it always $=s$.) Then the original matrix (entries interpreted in $A$) has the property that no $\mathfrak{p}\triangleleft A$ contains every $s\times s$ minor; however, every $\mathfrak{p}$ contains every $(s+1)\times (s+1)$ minor. The former condition means that the $s\times s$ minors generate the unit ideal in $A$. Since $A$ is presumed to be a reduced ring, the latter condition means that every $(s+1)\times(s+1)$ minor is zero. Now $\mathscr{F}$ is free iff $M$ is free. So if it's true that $X$ affine $\Rightarrow$ $\mathscr{F}$ is free (under the conditions of the question), then what I want to show is the plausible-to-me-seeming claim that if a matrix fulfills the conditions just described, then it presents a free module. It seems to me that if this is true it will not depend on the noetherian hypothesis on $A$, since all the relevant ideals and modules are already finitely generated. So - My question: Let $A$ be a commutative ring with unity. Let $M$ be a finitely presented module over $A$. Say that $M$ is presented by an $m\times r$ matrix with the property that the $s\times s$ minors generate the unit ideal in $A$, but all the $(s+1)\times (s+1)$ minors are zero. Is it true that in this case $M$ is free, of rank $m-s$? If so, can you give me a hint toward a proof? (I have been having fun with this problem so far so I'd prefer less than a full solution.) My work so far: I checked this ""by hand"" in the simplest nontrivial case, that the matrix presenting $M$ is $2\times 1$ and $s=1$: suppose $a,b\in A$ and $M$ is presented by $\begin{pmatrix} a\\b\end{pmatrix}$. The condition on the $s\times s$ minors means $(a,b)=1$, so $\exists f,g\in A$ with $fa+gb=1$. (The condition on the $(s+1)\times(s+1)$ minors doesn't tell us anything because it's already forced by the shape of the matrix.) Then the $A$-linear map $$ r\mapsto \begin{pmatrix}gr\\-fr\end{pmatrix}$$ is an isomorphism of $A^1\rightarrow M$. It is injective because $gr=-fr=0$ implies that $r=r(fa+gb)=0+0=0$, and it is surjective because for arbitrary $\begin{pmatrix}x\\y\end{pmatrix}$ in $A^2$, take $r=bx-ay$, and then since $fa=1-gb$ and $gb=1-fa$, we have $$\begin{pmatrix}x\\y\end{pmatrix} - \begin{pmatrix}gr\\-fr\end{pmatrix} = \begin{pmatrix} x-g(bx-ay)\\ y+f(bx-ay)\end{pmatrix}=\begin{pmatrix}(1-gb)x+gay\\ (1-fa)y+fbx\end{pmatrix}=(fx+gy)\begin{pmatrix}a\\b\end{pmatrix}$$ which represents zero in $M$. I started to look at the next simplest case I could think of: I took $A=k[a,b,c,d]/(a+b+c+d-1,ad-bc)$ for $k$ some field, and was thinking about the module presented by the matrix $\begin{pmatrix}a&b\\c&d\end{pmatrix}$. I would like to show this matrix is isomorphic to $A^1$. I still haven't thought about minors bigger than $1\times 1$ generating the unit ideal, so I realized in the interest of time I should ask for help. A last thought: Suddenly while writing this I realized that the result I want is strongly reminiscent of the Quillen-Suslin theorem, as described in Michael Artin's Algebra . This makes me suspicious that the result I want won't be true in full generality. Any thoughts on the relation of my question to the Quillen-Suslin theorem would be appreciated as well.","Motivation (you can ignore this part): A problem in Hartshorne (II.5.8c) asks to show that if we have a coherent sheaf $\mathscr{F}$ on a reduced noetherian scheme $X$, and the function $$\varphi(x)=\dim_{k(x)} \mathscr{F}_x \otimes_{\mathcal{O}_x} k(x)$$ is constant on $X$ (where $k(x)$ is the residue field), then in fact $\mathscr{F}$ is locally free. (This is a homework problem for me.) I think that if $X$ is affine (say $=\operatorname{Spec}A$), then $\mathscr{F}$ will actually be free, not just locally. It will certainly be true that $\mathscr{F}$ will be the sheaf associated with a finitely generated $A$-module $M$, so there will be an exact sequence $$0\rightarrow K\rightarrow A^m \rightarrow M\rightarrow 0$$ for some $m\in\mathbb{N}$, and because $A$ is noetherian, $K$ will be finitely generated. Thus $M$ is a finitely presented module, say by the matrix $$\begin{pmatrix} g_{11}&\dots& g_{1r}\\ \vdots &\ddots &\vdots \\ g_{m1}&\dots& g_{mr} \end{pmatrix}$$ Then if $x=\mathfrak{p}\triangleleft A$, $\mathscr{F}_x\otimes_{\mathcal{O}_x}k(x) = M_\mathfrak{p}\otimes_{A_\mathfrak{p}}k(\mathfrak{p}) = M\otimes_A k(\mathfrak{p})$ is precisely the module presented by this matrix except with the entries interpreted as elements of $k(\mathfrak{p})$. The condition that $\varphi(x)$ is constant means that the rank of this matrix (with entries interpreted in $k(\mathfrak{p})$) doesn't depend on $\mathfrak{p}$. (Say it always $=s$.) Then the original matrix (entries interpreted in $A$) has the property that no $\mathfrak{p}\triangleleft A$ contains every $s\times s$ minor; however, every $\mathfrak{p}$ contains every $(s+1)\times (s+1)$ minor. The former condition means that the $s\times s$ minors generate the unit ideal in $A$. Since $A$ is presumed to be a reduced ring, the latter condition means that every $(s+1)\times(s+1)$ minor is zero. Now $\mathscr{F}$ is free iff $M$ is free. So if it's true that $X$ affine $\Rightarrow$ $\mathscr{F}$ is free (under the conditions of the question), then what I want to show is the plausible-to-me-seeming claim that if a matrix fulfills the conditions just described, then it presents a free module. It seems to me that if this is true it will not depend on the noetherian hypothesis on $A$, since all the relevant ideals and modules are already finitely generated. So - My question: Let $A$ be a commutative ring with unity. Let $M$ be a finitely presented module over $A$. Say that $M$ is presented by an $m\times r$ matrix with the property that the $s\times s$ minors generate the unit ideal in $A$, but all the $(s+1)\times (s+1)$ minors are zero. Is it true that in this case $M$ is free, of rank $m-s$? If so, can you give me a hint toward a proof? (I have been having fun with this problem so far so I'd prefer less than a full solution.) My work so far: I checked this ""by hand"" in the simplest nontrivial case, that the matrix presenting $M$ is $2\times 1$ and $s=1$: suppose $a,b\in A$ and $M$ is presented by $\begin{pmatrix} a\\b\end{pmatrix}$. The condition on the $s\times s$ minors means $(a,b)=1$, so $\exists f,g\in A$ with $fa+gb=1$. (The condition on the $(s+1)\times(s+1)$ minors doesn't tell us anything because it's already forced by the shape of the matrix.) Then the $A$-linear map $$ r\mapsto \begin{pmatrix}gr\\-fr\end{pmatrix}$$ is an isomorphism of $A^1\rightarrow M$. It is injective because $gr=-fr=0$ implies that $r=r(fa+gb)=0+0=0$, and it is surjective because for arbitrary $\begin{pmatrix}x\\y\end{pmatrix}$ in $A^2$, take $r=bx-ay$, and then since $fa=1-gb$ and $gb=1-fa$, we have $$\begin{pmatrix}x\\y\end{pmatrix} - \begin{pmatrix}gr\\-fr\end{pmatrix} = \begin{pmatrix} x-g(bx-ay)\\ y+f(bx-ay)\end{pmatrix}=\begin{pmatrix}(1-gb)x+gay\\ (1-fa)y+fbx\end{pmatrix}=(fx+gy)\begin{pmatrix}a\\b\end{pmatrix}$$ which represents zero in $M$. I started to look at the next simplest case I could think of: I took $A=k[a,b,c,d]/(a+b+c+d-1,ad-bc)$ for $k$ some field, and was thinking about the module presented by the matrix $\begin{pmatrix}a&b\\c&d\end{pmatrix}$. I would like to show this matrix is isomorphic to $A^1$. I still haven't thought about minors bigger than $1\times 1$ generating the unit ideal, so I realized in the interest of time I should ask for help. A last thought: Suddenly while writing this I realized that the result I want is strongly reminiscent of the Quillen-Suslin theorem, as described in Michael Artin's Algebra . This makes me suspicious that the result I want won't be true in full generality. Any thoughts on the relation of my question to the Quillen-Suslin theorem would be appreciated as well.",,"['linear-algebra', 'matrices', 'commutative-algebra']"
52,Orbit-stabilizer theorem for Lie groups?,Orbit-stabilizer theorem for Lie groups?,,"Let $G$ be a finite-dimensional lie group, with a transitive action on the points of a smooth finite-dimensional manifold $S$. Let $p$ be some point of $S$ and let $T$ be the stabilizer of $p$ in $G$. Suppose that the action is compatible with the smooth structure of $S$, in the sense that for fixed $g\in G$, $g:S\rightarrow S$ defined by $x\mapsto gx$ is a smooth map, and for fixed $x\in S$, $x:G\rightarrow S$ defined by $g\mapsto gx$ is a smooth map. (I assume this is a standard concept but I do not know its name.) I have very little background in Lie groups, but it seems clear to me that the above forces $T$ to be a submanifold of $G$. (Initial question: is this correct?) Assuming this, Primary question: Is it true that $\dim S + \dim T = \dim G$? Secondary questions , if the answer is ""yes"": What is the argument? Can the assumptions be relaxed? Is there an analogous theorem in other settings? For example if $G$ and $S$ have the structure of algebraic varieties over some algebraically closed field $k$, can we replace all references to ""smooth map"" with ""morphism"" and get the same result? Motivation: Feel free to ignore this part but comments on it are welcome.  The question occurred to me when I was working on a problem (in Miles Reid's introductory algebraic geometry text) about putting an arbitrary cubic curve in $\mathbb{P}^2$ possessed of an inflection point into normal form $y^2z=x^3+ax^2z+bxz^2+cz^3$. It was necessary to make use of the fact that the group of projective transformations of $\mathbb{P}^2$, which I believe is an 8-dimensional quasi-projective variety, acts transitively on the set of pairs (line, point on that line), which set forms a 3-dimensional variety. I grew curious about what the stabilizer of a particular pair (line, point on that line) looked like. Based on intuition from basic group theory and basic linear algebra, I expected it to be a 5-dimensional subvariety of the group of projective transformations (because $3+5=8$), and this was true: for example if the point is $(1:0:0)$ and the line is spanned by this and $(0:1:0)$ then the stabilizer in question is the quotient group of the (6-dimensional) subgroup of upper triangular $3\times 3$ matrices by its (1-dimensional) center. I got curious if this relationship was true in some general settings. It seemed to me it should be at least in the lie group context, because of something like this: perhaps the derivative of the map $g\mapsto gx$ mentioned in the first paragraph has as kernel the tangent space of $T$? And then the desired result becomes the rank-nullity theorem? But I do not know enough to convince myself that this works.","Let $G$ be a finite-dimensional lie group, with a transitive action on the points of a smooth finite-dimensional manifold $S$. Let $p$ be some point of $S$ and let $T$ be the stabilizer of $p$ in $G$. Suppose that the action is compatible with the smooth structure of $S$, in the sense that for fixed $g\in G$, $g:S\rightarrow S$ defined by $x\mapsto gx$ is a smooth map, and for fixed $x\in S$, $x:G\rightarrow S$ defined by $g\mapsto gx$ is a smooth map. (I assume this is a standard concept but I do not know its name.) I have very little background in Lie groups, but it seems clear to me that the above forces $T$ to be a submanifold of $G$. (Initial question: is this correct?) Assuming this, Primary question: Is it true that $\dim S + \dim T = \dim G$? Secondary questions , if the answer is ""yes"": What is the argument? Can the assumptions be relaxed? Is there an analogous theorem in other settings? For example if $G$ and $S$ have the structure of algebraic varieties over some algebraically closed field $k$, can we replace all references to ""smooth map"" with ""morphism"" and get the same result? Motivation: Feel free to ignore this part but comments on it are welcome.  The question occurred to me when I was working on a problem (in Miles Reid's introductory algebraic geometry text) about putting an arbitrary cubic curve in $\mathbb{P}^2$ possessed of an inflection point into normal form $y^2z=x^3+ax^2z+bxz^2+cz^3$. It was necessary to make use of the fact that the group of projective transformations of $\mathbb{P}^2$, which I believe is an 8-dimensional quasi-projective variety, acts transitively on the set of pairs (line, point on that line), which set forms a 3-dimensional variety. I grew curious about what the stabilizer of a particular pair (line, point on that line) looked like. Based on intuition from basic group theory and basic linear algebra, I expected it to be a 5-dimensional subvariety of the group of projective transformations (because $3+5=8$), and this was true: for example if the point is $(1:0:0)$ and the line is spanned by this and $(0:1:0)$ then the stabilizer in question is the quotient group of the (6-dimensional) subgroup of upper triangular $3\times 3$ matrices by its (1-dimensional) center. I got curious if this relationship was true in some general settings. It seemed to me it should be at least in the lie group context, because of something like this: perhaps the derivative of the map $g\mapsto gx$ mentioned in the first paragraph has as kernel the tangent space of $T$? And then the desired result becomes the rank-nullity theorem? But I do not know enough to convince myself that this works.",,"['linear-algebra', 'group-theory', 'algebraic-geometry', 'lie-groups']"
53,A question concerning linear algebra,A question concerning linear algebra,,"(Multiple options could be correct!) Q. Let $A$ be a $4\times 4$ matrix over $\mathbb R$ such that $\rho(A)=2$ , and $A^3=A^2\ne O$ . Suppose that $A$ is not diagonalizable. Then (a) One of the Jordan blocks of the Jordan Canonical form of $A$ is $\pmatrix{ 0 & 1 \\ 0 & 0}.$ (b) $A^2=A\ne O.$ (c) $\exists$ a vector $v$ such that $A^2v=O.$ (d) The characteristic polynomial of $A$ is $x^4-x^3.$ Here, $\rho(A)$ means the rank of $A,$ and $O$ denotes the zero matrix of order $4$ . My attempt: If $f(x)=x^3-x^2=x^2(x-1),$ then $f(A)=O.$ So, the minimal polynomial of $A, m(x)$ (say), must divide $f(x).$ Since $A$ is not diagonalizable, the only possible choice for $m(x)$ is $x^2(x-1).$ This imples that there is a Jordan block of size $2$ in the Jordan Canonical form of $A$ corresponding to the eigen value $0$ of $A$ . Therefore, option (a) is correct. If $A^2=A\ne O$ were true, that would mean that the matrix $A$ is idempotent, hence diagonalizable. However, it is not so. Thus, we see that option (b) is false. (right?) $A^2v=O$ represents a homogeneous system of linear equations. As such, it is always consistent. So, option (c) is correct! (right?) Since $m(x)\mid c(x),$ the characteristic polynomial of $A.$ There are two possible choices for $c(x)$ , viz., $x^3(x-1)$ and $x^2(x-1)^2.$ In addition, the geometric multiplicity of the eigen value $0, \gamma(0)$ (say), equals the nullity of $A-0I_4,$ that is: $\gamma(0)=\eta(A-0I_4)=\eta(A)=4-\rho(A)=4-2=2.$ But, $A$ is not diagonalizable. Therefore, the algebraic multiplicity of the eigen value $0$ , $\alpha(0)$ (say), must not equal $\gamma(0).$ Now, we find that among the two feasible choices for $c(x),$ only the former one, i.e, $c(x)=x^3(x-1)=x^4-x^3,$ satisfies this condition. Because $\alpha(0)=3\ne 2=\gamma(0)$ in this case. Hence, option (d) is correct as well. (right?)","(Multiple options could be correct!) Q. Let be a matrix over such that , and . Suppose that is not diagonalizable. Then (a) One of the Jordan blocks of the Jordan Canonical form of is (b) (c) a vector such that (d) The characteristic polynomial of is Here, means the rank of and denotes the zero matrix of order . My attempt: If then So, the minimal polynomial of (say), must divide Since is not diagonalizable, the only possible choice for is This imples that there is a Jordan block of size in the Jordan Canonical form of corresponding to the eigen value of . Therefore, option (a) is correct. If were true, that would mean that the matrix is idempotent, hence diagonalizable. However, it is not so. Thus, we see that option (b) is false. (right?) represents a homogeneous system of linear equations. As such, it is always consistent. So, option (c) is correct! (right?) Since the characteristic polynomial of There are two possible choices for , viz., and In addition, the geometric multiplicity of the eigen value (say), equals the nullity of that is: But, is not diagonalizable. Therefore, the algebraic multiplicity of the eigen value , (say), must not equal Now, we find that among the two feasible choices for only the former one, i.e, satisfies this condition. Because in this case. Hence, option (d) is correct as well. (right?)","A 4\times 4 \mathbb R \rho(A)=2 A^3=A^2\ne O A A \pmatrix{ 0 & 1 \\ 0 & 0}. A^2=A\ne O. \exists v A^2v=O. A x^4-x^3. \rho(A) A, O 4 f(x)=x^3-x^2=x^2(x-1), f(A)=O. A, m(x) f(x). A m(x) x^2(x-1). 2 A 0 A A^2=A\ne O A A^2v=O m(x)\mid c(x), A. c(x) x^3(x-1) x^2(x-1)^2. 0, \gamma(0) A-0I_4, \gamma(0)=\eta(A-0I_4)=\eta(A)=4-\rho(A)=4-2=2. A 0 \alpha(0) \gamma(0). c(x), c(x)=x^3(x-1)=x^4-x^3, \alpha(0)=3\ne 2=\gamma(0)","['linear-algebra', 'matrices', 'linear-transformations']"
54,A characterization of 'orthogonal' matrices,A characterization of 'orthogonal' matrices,,"(All matrices in this post are assumed to be real square matrices). Recently I answered a question on this site where one was asked to show that if $A$ is a symmetric matrix, then $U^{-1}AU$ is symmetric if $U$ is orthogonal. I wondered whether the converse was true as well. Concretely: Suppose that $U$ is an invertible matrix such that for any symmetric matrix $A$, we have that $U^{-1}AU$ is symmetric. Can we conclude that $U$ is orthogonal? So I tried to prove this and quickly realized this is almost true, indeed one can conclude that $UU^t=\lambda Id$ for some real number $\lambda$. (This is what I would call an orthogonal matrix, an orthonormal matrix is one such that $UU^t=Id$). I'll sketch my proof below: Let $A$ be symmetric and assume $U^{-1}AU$ is symmetric. Then $(UU^t)A=A(UU^t)$. For any symmetric matrix, the equality $(UU^t)A=A(UU^t)$ allows us to find some conditions on $(UU^t)$. So I choose the easiest basis of the symmetric matrices I could think of and plugged it in. After some calculations you're able to conclude $(UU^t)_{i,j}=0$ if $i\neq j$ and $(UU^t)_{i,i}=(UU^t)_{j,j}$ for all $i,j$. This is what we needed to show. There is nothing wrong with the above proof, it's just a brute force method. I was wondering whether anyone has a fun conceptual way of proving this. I'd very much like to see such a proof. EDIT: In particular, I'm looking for an argument that a student could find at the beginning of a linear algebra course. (Diagonalization and spectral theorem are a bit too advanced for now).","(All matrices in this post are assumed to be real square matrices). Recently I answered a question on this site where one was asked to show that if $A$ is a symmetric matrix, then $U^{-1}AU$ is symmetric if $U$ is orthogonal. I wondered whether the converse was true as well. Concretely: Suppose that $U$ is an invertible matrix such that for any symmetric matrix $A$, we have that $U^{-1}AU$ is symmetric. Can we conclude that $U$ is orthogonal? So I tried to prove this and quickly realized this is almost true, indeed one can conclude that $UU^t=\lambda Id$ for some real number $\lambda$. (This is what I would call an orthogonal matrix, an orthonormal matrix is one such that $UU^t=Id$). I'll sketch my proof below: Let $A$ be symmetric and assume $U^{-1}AU$ is symmetric. Then $(UU^t)A=A(UU^t)$. For any symmetric matrix, the equality $(UU^t)A=A(UU^t)$ allows us to find some conditions on $(UU^t)$. So I choose the easiest basis of the symmetric matrices I could think of and plugged it in. After some calculations you're able to conclude $(UU^t)_{i,j}=0$ if $i\neq j$ and $(UU^t)_{i,i}=(UU^t)_{j,j}$ for all $i,j$. This is what we needed to show. There is nothing wrong with the above proof, it's just a brute force method. I was wondering whether anyone has a fun conceptual way of proving this. I'd very much like to see such a proof. EDIT: In particular, I'm looking for an argument that a student could find at the beginning of a linear algebra course. (Diagonalization and spectral theorem are a bit too advanced for now).",,"['linear-algebra', 'matrices', 'alternative-proof', 'orthogonality']"
55,Proof of Cayley-Hamilton Theorem in infinite fields only?,Proof of Cayley-Hamilton Theorem in infinite fields only?,,"While trying to prove the Cayley-Hamilton theorem, I came up with the following proof: If $A$ is a diagonalizable matrix, so $A=SDS^{-1}$ with $D$ diagonal, then, letting $$P(\lambda)=\det(A-\lambda I)=\sum_{i=0}^n c_i\lambda^i,$$ $$``P(A)"" = \sum_{i=0}^n c_i A^i = S\left(\sum_{i=0}^n c_i D^i \right)S^{-1},$$ which is clearly the zero matrix as, if $D$ contains $\lambda_1,\cdots,\lambda_n$ on its diagonal, $D^i$ contains $\lambda_1^i,\cdots,\lambda_n^i$, and each of the eigenvalues satisfy the characteristic polynomial. So, the Cayley-Hamilton theorem is true for diagonalizable matrices. Specifically, it is true for all matrices with $n$ distinct eigenvalues, or equivalently all matrices whose characteristic polynomials have no repeated roots. However, the coefficients of the characteristic polynomial are polynomials in the elements $a_1,\cdots,a_{n^2}$ of the matrix $A$. Also, there is a polynomial expression of the coefficients of a polynomial that is $0$ iff the polynomial has a repeated root (the resultant ). So, there exists some polynomial $Q$ such that either the matrix $A$ satisfies its characteristic polynomial or $$Q(a_1,\cdots,a_{n^2}) = 0.$$ However, a matrix inputted into its characteristic polynomial is itself a matrix whose elements are polynomial functions $R_1,\cdots,R_{n^2}$ of the elements of the matrix. Thus, for each $1\leq i\leq n^2$ and any values $a_1,\cdots,a_{n^2}$, we have either that $$Q(a_1,\cdots,a_{n^2}) = 0\ \mathrm{or}\ R_i(a_1,\cdots,a_{n^2}) = 0.$$ Thus, for all $(a_1,\cdots,a_{n^2}) \in \mathbb{R}^{n^2}$, and each $1\leq i\leq n^2$, we have $$Q(a_1,\cdots,a_{n^2})\cdot R_i(a_1,\cdots,a_{n^2}) = 0.$$ However, a polynomial (here $QR_i$) that equals zero everywhere must be the zero polynomial, and if a product of two polynomials is the zero polynomial, one of the two must be the zero polynomial. It's clearly not $Q$, since there exist matrices with distinct eigenvalues. So, each $R_i$ must be identically $0$, and the Cayley-Hamilton theorem is proven. However, this only works in infinite fields. In finite fields, the two statements: If a polynomial is $0$ everywhere, it is the zero polynomial. If two polynomials multiply to make the zero polynomial, one of them must be the zero polynomial. are not both true (I'm sure the first one isn't; I'm not sure about the second). Is there a way to rigorize the notion that ""It's an algebraic statement that's true in, say, $\mathbb{R}$, so it must be true in any field,"" or is there some other way to extend this proof to finite fields? Or does this proof just only work (or make sense) in infinite fields?","While trying to prove the Cayley-Hamilton theorem, I came up with the following proof: If $A$ is a diagonalizable matrix, so $A=SDS^{-1}$ with $D$ diagonal, then, letting $$P(\lambda)=\det(A-\lambda I)=\sum_{i=0}^n c_i\lambda^i,$$ $$``P(A)"" = \sum_{i=0}^n c_i A^i = S\left(\sum_{i=0}^n c_i D^i \right)S^{-1},$$ which is clearly the zero matrix as, if $D$ contains $\lambda_1,\cdots,\lambda_n$ on its diagonal, $D^i$ contains $\lambda_1^i,\cdots,\lambda_n^i$, and each of the eigenvalues satisfy the characteristic polynomial. So, the Cayley-Hamilton theorem is true for diagonalizable matrices. Specifically, it is true for all matrices with $n$ distinct eigenvalues, or equivalently all matrices whose characteristic polynomials have no repeated roots. However, the coefficients of the characteristic polynomial are polynomials in the elements $a_1,\cdots,a_{n^2}$ of the matrix $A$. Also, there is a polynomial expression of the coefficients of a polynomial that is $0$ iff the polynomial has a repeated root (the resultant ). So, there exists some polynomial $Q$ such that either the matrix $A$ satisfies its characteristic polynomial or $$Q(a_1,\cdots,a_{n^2}) = 0.$$ However, a matrix inputted into its characteristic polynomial is itself a matrix whose elements are polynomial functions $R_1,\cdots,R_{n^2}$ of the elements of the matrix. Thus, for each $1\leq i\leq n^2$ and any values $a_1,\cdots,a_{n^2}$, we have either that $$Q(a_1,\cdots,a_{n^2}) = 0\ \mathrm{or}\ R_i(a_1,\cdots,a_{n^2}) = 0.$$ Thus, for all $(a_1,\cdots,a_{n^2}) \in \mathbb{R}^{n^2}$, and each $1\leq i\leq n^2$, we have $$Q(a_1,\cdots,a_{n^2})\cdot R_i(a_1,\cdots,a_{n^2}) = 0.$$ However, a polynomial (here $QR_i$) that equals zero everywhere must be the zero polynomial, and if a product of two polynomials is the zero polynomial, one of the two must be the zero polynomial. It's clearly not $Q$, since there exist matrices with distinct eigenvalues. So, each $R_i$ must be identically $0$, and the Cayley-Hamilton theorem is proven. However, this only works in infinite fields. In finite fields, the two statements: If a polynomial is $0$ everywhere, it is the zero polynomial. If two polynomials multiply to make the zero polynomial, one of them must be the zero polynomial. are not both true (I'm sure the first one isn't; I'm not sure about the second). Is there a way to rigorize the notion that ""It's an algebraic statement that's true in, say, $\mathbb{R}$, so it must be true in any field,"" or is there some other way to extend this proof to finite fields? Or does this proof just only work (or make sense) in infinite fields?",,"['linear-algebra', 'finite-fields', 'cayley-hamilton']"
56,Every subspace of the dual of a finite-dimensional vector space is an annihilator,Every subspace of the dual of a finite-dimensional vector space is an annihilator,,"Exercise 26 page 115 of Linear Algebra Done Right by Sheldon Axler is the following: Suppose $V$ is finite-dimensional and $\Gamma$ is a subspace of $V'$. Show that $$\Gamma=\{v\in V:\varphi(v)=0\text{ for every }\varphi\in\Gamma\}^0$$ where $V'$ is the dual space of $V$ and, for any $S\subset V$, $S^0$ is the annihilator of $S$. Attempt: Let $S=\{v\in V:\varphi(v)=0\text{ for every }\varphi\in\Gamma\}$. Clearly, $\Gamma\subset S^0$.I tried to show that $S^0\subset\Gamma$ using some bases of $V$ and $V'$, but I failed. I also tried to show that $\dim{\Gamma}=\dim{V}-\dim{S}=\dim{S^°}$. If $s_1,\dots,s_n$ is a basis of $S$ and $s_1',\dots,s_n'$ its dual, and if $\psi_1,\dots,\psi_p$ is a basis of $\Gamma$, it's easy to see that $s_1',\dots,s_n',\psi_1,\dots,\psi_p$ is a linearly independent list of $V'$. Also, if you extend $s_1,\dots,s_n$ to a basis $s_1,\dots,s_n,v_1,\dots,v_m$ of $V$, then its dual $s_1',\dots,s_n',v_1',\dots,v_m'$ is a basis of $V'$; in fact $S^0=\text{span}\{v_1',\dots,v_m'\}$. Two remarkable facts:$$\forall i\in[1,m],\,\exists j\in[1,p],\,\psi_j(v_i)\neq0$$ because $v_j\notin S$, and $$\forall i\in[1,p],\,\exists j\in[1,m],\,\psi_i(v_j)\neq 0$$ because $\psi_i\neq 0$; in other words the matrix of the inclusion map from $\Gamma$ to $V'$ with respect to the basis of $\Gamma$ and the dual base of the chosen basis of $V$ has no $0$ row nor $0$ column. But this doesn't seem to provide a way to prove that any linear comination of the $(v_i')_{1\le i\le m}$ is a linear combination of the elements of the basis of $\Gamma$. I believe that $v_1,\dots, v_m$ should be chosen more carefuly but I fail to. Could you please help me? Thank you in advance!","Exercise 26 page 115 of Linear Algebra Done Right by Sheldon Axler is the following: Suppose $V$ is finite-dimensional and $\Gamma$ is a subspace of $V'$. Show that $$\Gamma=\{v\in V:\varphi(v)=0\text{ for every }\varphi\in\Gamma\}^0$$ where $V'$ is the dual space of $V$ and, for any $S\subset V$, $S^0$ is the annihilator of $S$. Attempt: Let $S=\{v\in V:\varphi(v)=0\text{ for every }\varphi\in\Gamma\}$. Clearly, $\Gamma\subset S^0$.I tried to show that $S^0\subset\Gamma$ using some bases of $V$ and $V'$, but I failed. I also tried to show that $\dim{\Gamma}=\dim{V}-\dim{S}=\dim{S^°}$. If $s_1,\dots,s_n$ is a basis of $S$ and $s_1',\dots,s_n'$ its dual, and if $\psi_1,\dots,\psi_p$ is a basis of $\Gamma$, it's easy to see that $s_1',\dots,s_n',\psi_1,\dots,\psi_p$ is a linearly independent list of $V'$. Also, if you extend $s_1,\dots,s_n$ to a basis $s_1,\dots,s_n,v_1,\dots,v_m$ of $V$, then its dual $s_1',\dots,s_n',v_1',\dots,v_m'$ is a basis of $V'$; in fact $S^0=\text{span}\{v_1',\dots,v_m'\}$. Two remarkable facts:$$\forall i\in[1,m],\,\exists j\in[1,p],\,\psi_j(v_i)\neq0$$ because $v_j\notin S$, and $$\forall i\in[1,p],\,\exists j\in[1,m],\,\psi_i(v_j)\neq 0$$ because $\psi_i\neq 0$; in other words the matrix of the inclusion map from $\Gamma$ to $V'$ with respect to the basis of $\Gamma$ and the dual base of the chosen basis of $V$ has no $0$ row nor $0$ column. But this doesn't seem to provide a way to prove that any linear comination of the $(v_i')_{1\le i\le m}$ is a linear combination of the elements of the basis of $\Gamma$. I believe that $v_1,\dots, v_m$ should be chosen more carefuly but I fail to. Could you please help me? Thank you in advance!",,"['linear-algebra', 'vector-spaces', 'linear-transformations', 'duality-theorems']"
57,The image of the transpose of $A^\text{T}$ is the orthogonal complement of its kernel,The image of the transpose of  is the orthogonal complement of its kernel,A^\text{T},Suppose $V$ is a finite dimensional vector space over $\mathbb{K}$ . Let $A$ be a linear map. I am trying to prove that $$\operatorname{Im}A^\text{T}=(\operatorname{Ker}A)^{\perp}$$ I know one direction: $\operatorname{Im}A^\text{T} \subset (\operatorname{Ker}A)^{\perp}$ but I don’t know how to show the other direction. Can anyone help me?,Suppose is a finite dimensional vector space over . Let be a linear map. I am trying to prove that I know one direction: but I don’t know how to show the other direction. Can anyone help me?,V \mathbb{K} A \operatorname{Im}A^\text{T}=(\operatorname{Ker}A)^{\perp} \operatorname{Im}A^\text{T} \subset (\operatorname{Ker}A)^{\perp},['linear-algebra']
58,Techniques for showing that a subgroup is not normal,Techniques for showing that a subgroup is not normal,,"To show that a subgroup of a group is normal, I typically construct a homomorphism whose kernel is that subgroup. Are there any general principles or tests that I can use to determine that a subgroup is not normal? Or are such claims usually proved by ad hoc methods? In particular, I would like to show that $\textbf{SL}_n(\mathbb{Z})$ is not a normal subgroup of $\textbf{SL}_n(\mathbb{R})$. Any help is appreciated!","To show that a subgroup of a group is normal, I typically construct a homomorphism whose kernel is that subgroup. Are there any general principles or tests that I can use to determine that a subgroup is not normal? Or are such claims usually proved by ad hoc methods? In particular, I would like to show that $\textbf{SL}_n(\mathbb{Z})$ is not a normal subgroup of $\textbf{SL}_n(\mathbb{R})$. Any help is appreciated!",,"['linear-algebra', 'abstract-algebra', 'matrices', 'group-theory', 'linear-groups']"
59,Real Linear vs. Complex Linear,Real Linear vs. Complex Linear,,"I recently started a new math course and got hung up on a particular problem from the book ""Linear Algebra Done Wrong"". Specifically, problem 1.3.6 (c). I am an engineer, and I believe I simply lack terminology/definition to solve the problem. Again, it is part (c) of the problem: The set $\mathbb C$ of complex numbers can be canonically identied with the space $\mathbb R^2$ by treating each ($z = x + iy$) of $\mathbb C$ as a column $(x , y)^T$ of $\mathbb R^2$. Define $T(x+iy) = 2x-y+i(x-3y)$. Show that this transformation is not a linear transformation in the complex vectors space $\mathbb C$, but if we treat $\mathbb C$ as the real vector space $\mathbb R^2$ then it is a linear transformation there (i.e. that $T$ is a real linear but not a complex linear transformation). Find the matrix of the real liner transformation $T$. It appears to me that: $T(au+bv) = aT(u)+bT(v)$ I can't see why it is not linear on $\mathbb C$ but it is on $\mathbb R^2$. However, I recognize that I don't understand how this changes the outcome. Any ideas? Thanks","I recently started a new math course and got hung up on a particular problem from the book ""Linear Algebra Done Wrong"". Specifically, problem 1.3.6 (c). I am an engineer, and I believe I simply lack terminology/definition to solve the problem. Again, it is part (c) of the problem: The set $\mathbb C$ of complex numbers can be canonically identied with the space $\mathbb R^2$ by treating each ($z = x + iy$) of $\mathbb C$ as a column $(x , y)^T$ of $\mathbb R^2$. Define $T(x+iy) = 2x-y+i(x-3y)$. Show that this transformation is not a linear transformation in the complex vectors space $\mathbb C$, but if we treat $\mathbb C$ as the real vector space $\mathbb R^2$ then it is a linear transformation there (i.e. that $T$ is a real linear but not a complex linear transformation). Find the matrix of the real liner transformation $T$. It appears to me that: $T(au+bv) = aT(u)+bT(v)$ I can't see why it is not linear on $\mathbb C$ but it is on $\mathbb R^2$. However, I recognize that I don't understand how this changes the outcome. Any ideas? Thanks",,"['linear-algebra', 'transformation', 'linear-transformations']"
60,Shared eigenvectors between $A$ and $A^k$,Shared eigenvectors between  and,A A^k,"$\newcommand\la{\lambda}$ Thanks to the spectral mapping theorem, we know that if $\la_1,\ldots,\la_n$ are the eigenvalues of a $n\times n$ complex matrix $A$, then $\la_1^k,\ldots,\la_n^k$ are the eigenvalues of $A^k$. Furthermore, if $v_i$ is an eigenvector of $A$ associated to $\lambda_i$, then $v_i$ is also an eigenvector of $A^k$ associated to $\la_i^k$. Indeed, $A^kv=A^{k-1}\la_i v_i=\cdots=\la_i^kv_i$. So my question is the following: If $v_1,\ldots,v_n$ are the eigenvectors of $A$, are $v_1,\ldots,v_n$ also all the eigenvectors of $A^k$? In other words, is it possible that there is some vector $v$ that is an eigenvector of $A^k$ but not an eigenvector of $A$? It seems intuitive that this should not happen, but I am unable to find an argument to show it.","$\newcommand\la{\lambda}$ Thanks to the spectral mapping theorem, we know that if $\la_1,\ldots,\la_n$ are the eigenvalues of a $n\times n$ complex matrix $A$, then $\la_1^k,\ldots,\la_n^k$ are the eigenvalues of $A^k$. Furthermore, if $v_i$ is an eigenvector of $A$ associated to $\lambda_i$, then $v_i$ is also an eigenvector of $A^k$ associated to $\la_i^k$. Indeed, $A^kv=A^{k-1}\la_i v_i=\cdots=\la_i^kv_i$. So my question is the following: If $v_1,\ldots,v_n$ are the eigenvectors of $A$, are $v_1,\ldots,v_n$ also all the eigenvectors of $A^k$? In other words, is it possible that there is some vector $v$ that is an eigenvector of $A^k$ but not an eigenvector of $A$? It seems intuitive that this should not happen, but I am unable to find an argument to show it.",,['linear-algebra']
61,How to rotate a matrix by 45 degrees?,How to rotate a matrix by 45 degrees?,,"Assume you have a 2D matrix. Ignore the blue squares. The first image represents the initial matrix and the second represents the matrix rotated by 45 degrees. For example, let's consider a random cell from the first image. a32 (row 3, column 2) Let x be the row and y be the column, so x = 3 and y = 2. The formula of this rotation is : RM[x + y - 1][n - x + y] = M[x][y], where RM means rotated matrix, M the initial matrix, and n the dimension of the initial matrix (which is n x n). So, a32, from the third row and second column will get to the fourth row and the fourth column. The problem is, I don't really understand this forumla. Can you help me? And how do you rotate by 45 degrees a matrix that is not square shaped? (a rectangle shaped one) Any help is much appreciated. Thank you very much!","Assume you have a 2D matrix. Ignore the blue squares. The first image represents the initial matrix and the second represents the matrix rotated by 45 degrees. For example, let's consider a random cell from the first image. a32 (row 3, column 2) Let x be the row and y be the column, so x = 3 and y = 2. The formula of this rotation is : RM[x + y - 1][n - x + y] = M[x][y], where RM means rotated matrix, M the initial matrix, and n the dimension of the initial matrix (which is n x n). So, a32, from the third row and second column will get to the fourth row and the fourth column. The problem is, I don't really understand this forumla. Can you help me? And how do you rotate by 45 degrees a matrix that is not square shaped? (a rectangle shaped one) Any help is much appreciated. Thank you very much!",,"['linear-algebra', 'matrices', 'rotations']"
62,Spectrum of the cycle graph $C_n$,Spectrum of the cycle graph,C_n,"I am trying to find out the spectrum (the collection of eigenvalues) with their multiplicities of the cycle graph $C_n$. Assuming that $X=\pmatrix{x_1\\x_2\\\vdots\\x_n}$ is the eigenvector, by considering $AX=\lambda X$, where $A$ is the adjacency matrix, I get the system of (non linear) equations: $x_n+x_2=\lambda x_1\\x_1+x_3=\lambda x_2\\\cdots\\x_{n-1}+x_1=\lambda x_n$. There seem to be no obvious way to solve this except summing which yields $\lambda=2$, provided $\sum x_i\ne 0$. However I already knew that $2$ is an eigenvalue of multiplicity $1$ as $C_n$ is a connected $2$-regular graph. How do I go about finding the other eigenvalues? Also in my book the solution provided magically says let $x_i=\epsilon^i$ where $\epsilon$ is a $n$-th root of unity, and proceeds from this point on. I do not understand the basis of this assumption, and if someone can explain just that I would be most obliged.","I am trying to find out the spectrum (the collection of eigenvalues) with their multiplicities of the cycle graph $C_n$. Assuming that $X=\pmatrix{x_1\\x_2\\\vdots\\x_n}$ is the eigenvector, by considering $AX=\lambda X$, where $A$ is the adjacency matrix, I get the system of (non linear) equations: $x_n+x_2=\lambda x_1\\x_1+x_3=\lambda x_2\\\cdots\\x_{n-1}+x_1=\lambda x_n$. There seem to be no obvious way to solve this except summing which yields $\lambda=2$, provided $\sum x_i\ne 0$. However I already knew that $2$ is an eigenvalue of multiplicity $1$ as $C_n$ is a connected $2$-regular graph. How do I go about finding the other eigenvalues? Also in my book the solution provided magically says let $x_i=\epsilon^i$ where $\epsilon$ is a $n$-th root of unity, and proceeds from this point on. I do not understand the basis of this assumption, and if someone can explain just that I would be most obliged.",,"['linear-algebra', 'graph-theory']"
63,Spectrum of a linear operator on a vector space of countable dim,Spectrum of a linear operator on a vector space of countable dim,,"How would you prove that if V is a vector space over $\mathbb{C}$ of countably infinite dimension, and $T$ is a linear operator on V, then Spectrum($T$) is non-empty?","How would you prove that if V is a vector space over $\mathbb{C}$ of countably infinite dimension, and $T$ is a linear operator on V, then Spectrum($T$) is non-empty?",,['linear-algebra']
64,"Show that $PSL(3,4)$ has no element of order $15$.",Show that  has no element of order .,"PSL(3,4) 15","$PSL(3,4)$ has no element of order $15$. Thus it is no isomorphic to $A_8$. Here, $PSL(3,4)$ denotes the $3 \times 3$ projective special linear group on the field with $4$ elements. As listing all the elements takes too much work, is there any better way to prove there is no element of order $15$ in $PSL(3,4)$? Thank you very much.","$PSL(3,4)$ has no element of order $15$. Thus it is no isomorphic to $A_8$. Here, $PSL(3,4)$ denotes the $3 \times 3$ projective special linear group on the field with $4$ elements. As listing all the elements takes too much work, is there any better way to prove there is no element of order $15$ in $PSL(3,4)$? Thank you very much.",,"['linear-algebra', 'group-theory', 'finite-groups']"
65,Solving systems of linear equations over a finite ring,Solving systems of linear equations over a finite ring,,"I want to solve equations like this (mod $2^n$): $$\begin{array}{rcrcrcr} 3x&+&4y&+&13z&=&3&\pmod{16} \\ x&+&5y&+&3z&=&5&\pmod{16} \\ 4x&+&7y&+&11z&=&12&\pmod{16}\end{array}$$ Since we are working over a ring and not a field, Gaussian elimination doesn't work. So how can I still solve these types of equations?","I want to solve equations like this (mod $2^n$): $$\begin{array}{rcrcrcr} 3x&+&4y&+&13z&=&3&\pmod{16} \\ x&+&5y&+&3z&=&5&\pmod{16} \\ 4x&+&7y&+&11z&=&12&\pmod{16}\end{array}$$ Since we are working over a ring and not a field, Gaussian elimination doesn't work. So how can I still solve these types of equations?",,"['linear-algebra', 'number-theory', 'finite-rings', 'gaussian-elimination']"
66,Prove that $\det(A^{2019} +B^{2019} )$ and $\det(A^{2019} -B^{2019} )$ are divisible by $4$,Prove that  and  are divisible by,\det(A^{2019} +B^{2019} ) \det(A^{2019} -B^{2019} ) 4,"Let $A, B \in M_2(\mathbb{Z})$ so that $$\det A=\det B=\frac{1}{4} \det(A^2+B^2)=1$$ Prove that $\det(A^{2019} +B^{2019} )$ and $\det(A^{2019} -B^{2019} )$ are divisible by $4$ . The only observation I have made is that $\det(A^{2019} +B^{2019}) +\det(A^{2019} - B^{2019} )=4$ so suffice it to prove that one of them is divisible by $4$ . EDIT : This was featured on the regional stage of the maths olympiad in Romania on Saturday, so it is an actual problem, not something I came up with. It is relevant to me because I am preparing for the next stage. EDIT : $AB=BA$ indeed, I forgot to include it when I posted the problem and I am sorry for this. Since there are people in the comments eager to see the official paper, here it is : https://i.sstatic.net/JTOYy.jpg As you can see, this contest took place on the 24th of February(I know we have a pretty strange date format, but this is what 24.02. 2019 means) and it wasn't an online competition, so I really was honest when I said I was not cheating.","Let so that Prove that and are divisible by . The only observation I have made is that so suffice it to prove that one of them is divisible by . EDIT : This was featured on the regional stage of the maths olympiad in Romania on Saturday, so it is an actual problem, not something I came up with. It is relevant to me because I am preparing for the next stage. EDIT : indeed, I forgot to include it when I posted the problem and I am sorry for this. Since there are people in the comments eager to see the official paper, here it is : https://i.sstatic.net/JTOYy.jpg As you can see, this contest took place on the 24th of February(I know we have a pretty strange date format, but this is what 24.02. 2019 means) and it wasn't an online competition, so I really was honest when I said I was not cheating.","A, B \in M_2(\mathbb{Z}) \det A=\det B=\frac{1}{4} \det(A^2+B^2)=1 \det(A^{2019} +B^{2019} ) \det(A^{2019} -B^{2019} ) 4 \det(A^{2019} +B^{2019}) +\det(A^{2019} - B^{2019} )=4 4 AB=BA","['linear-algebra', 'matrices', 'contest-math', 'determinant']"
67,Why are anti-diagonal / persymmetric matrices not as important as diagonal / symmetric matrices?,Why are anti-diagonal / persymmetric matrices not as important as diagonal / symmetric matrices?,,"Diagonal matrices and diagonalizability are key topics in linear algebra as well as numerical linear algebra. Likewise, symmetric matrices have lots of nice properties that make them widely studied and important both theoretically and computationally. However, anti-diagonal matrices seem to be no more than a curiosity in matrix algebra. While symmetry along the main diagonal seems to count for so much, persymmetry does not seem to count for very much at all. Is there a reason for this? After all (and this might sound naive) why should one diagonal (left to right) matter so much more than the other one? Is this an artifact / convention arising from the development of matrix algebra or does it reflect something deeper. Or, are anti-diagonal and per-symmetric matrices of far greater importance than I think? I was thinking about this and was not really able to come up with anything close to a satisfactory answer.","Diagonal matrices and diagonalizability are key topics in linear algebra as well as numerical linear algebra. Likewise, symmetric matrices have lots of nice properties that make them widely studied and important both theoretically and computationally. However, anti-diagonal matrices seem to be no more than a curiosity in matrix algebra. While symmetry along the main diagonal seems to count for so much, persymmetry does not seem to count for very much at all. Is there a reason for this? After all (and this might sound naive) why should one diagonal (left to right) matter so much more than the other one? Is this an artifact / convention arising from the development of matrix algebra or does it reflect something deeper. Or, are anti-diagonal and per-symmetric matrices of far greater importance than I think? I was thinking about this and was not really able to come up with anything close to a satisfactory answer.",,"['linear-algebra', 'matrices']"
68,What is the mathematical notation for Convex Hull?,What is the mathematical notation for Convex Hull?,,"I've been scanning through scientific papers, this site and just googling for it, but I can't find a commonly accepted notation for the convex hull. So my question is; if there is, what is the standard notation for convex hull? Seems to me people just use their favorite out of a large collection of notations (or invent their own notation) to denote the convex hull of, say $S$, including \begin{align} \mathrm{conv}(S),\ \mathrm{CH}(S),\ \mathrm{conv.hull}(S),\ \mathrm{Co}(S),\ \mathrm{C}(S), \end{align} which I find frustrating. :)","I've been scanning through scientific papers, this site and just googling for it, but I can't find a commonly accepted notation for the convex hull. So my question is; if there is, what is the standard notation for convex hull? Seems to me people just use their favorite out of a large collection of notations (or invent their own notation) to denote the convex hull of, say $S$, including \begin{align} \mathrm{conv}(S),\ \mathrm{CH}(S),\ \mathrm{conv.hull}(S),\ \mathrm{Co}(S),\ \mathrm{C}(S), \end{align} which I find frustrating. :)",,"['linear-algebra', 'notation', 'convex-analysis']"
69,Show that there is always a way to achieve det(A) > 0,Show that there is always a way to achieve det(A) > 0,,"a) Assume that $(a_1, ..., a_9)$ are different positive numbers. Let us make a $3 \times 3$ matrix $A_s$ by placing them arbitrarily into $9$ positions available. Show that there is always a way to assemble them so that $\det(A_s) > 0$. b) Assume now that some of $a_i, i =1,\dots,9$ are equal and the total number of different values taken by the $a_i$ is $N \in \{1,2,...,9\}$.  What is the minimal $N$ which guarantees the existence of $A_s$ as above with $\det(A_s) >0$? Give a proof. For part (a), I tried to show that, with row operations, $\det(A_s)$ can never equal $0$, so that the matrix is invertible, with determinant either negative or positive. Then if the determinant is positive, the proof is complete; if not, then simply interchange any two rows, which negates the determinant, giving a positive determinant as required. However, this proof doesn't work. Using just the numbers $1 \dots 9$, then $(7,8,9)$ is in the span of $(1,2,3)$ and $(4,5,6)$. Since the question is asking to show that there is always a way, i.e., there exists  a way to achieve $\det(A_s)>0$, I feel that I should work on a contradiction, and assume first that there is no way to achieve it. Perhaps I can use the fact that $\rm{trace} \, A_s$ must be positive, but I don't see it right now. Any hints would be greatly appreciated. Thanks.","a) Assume that $(a_1, ..., a_9)$ are different positive numbers. Let us make a $3 \times 3$ matrix $A_s$ by placing them arbitrarily into $9$ positions available. Show that there is always a way to assemble them so that $\det(A_s) > 0$. b) Assume now that some of $a_i, i =1,\dots,9$ are equal and the total number of different values taken by the $a_i$ is $N \in \{1,2,...,9\}$.  What is the minimal $N$ which guarantees the existence of $A_s$ as above with $\det(A_s) >0$? Give a proof. For part (a), I tried to show that, with row operations, $\det(A_s)$ can never equal $0$, so that the matrix is invertible, with determinant either negative or positive. Then if the determinant is positive, the proof is complete; if not, then simply interchange any two rows, which negates the determinant, giving a positive determinant as required. However, this proof doesn't work. Using just the numbers $1 \dots 9$, then $(7,8,9)$ is in the span of $(1,2,3)$ and $(4,5,6)$. Since the question is asking to show that there is always a way, i.e., there exists  a way to achieve $\det(A_s)>0$, I feel that I should work on a contradiction, and assume first that there is no way to achieve it. Perhaps I can use the fact that $\rm{trace} \, A_s$ must be positive, but I don't see it right now. Any hints would be greatly appreciated. Thanks.",,"['linear-algebra', 'permutations', 'determinant', 'trace']"
70,The colimit of all finite-dimensional vector spaces,The colimit of all finite-dimensional vector spaces,,"Let $\mathsf{iFinVect}_K$ be the category of finite-dimensional vector spaces with injective linear maps and $X : \mathsf{iFinVect}_K \to \mathsf{Vect}_K$ be the inclusion functor. Then $\mathrm{colim}(X)$ exists. This is because $\mathsf{Vect}_K$ is cocomplete and $\mathsf{iFinVect}_K$ is essentially small (although it is not small). This colimit seems to be a bit strange to me, though. We merge all finite-dimensional vector spaces into a single large vector space. The embeddings are natural with respect to all injective linear maps between finite-dimensional vector spaces. Can we make this vector space more explicit? Is it, perhaps, even a well-known object? Can we find a basis? Every element of $\mathrm{colim}(X)$ should have the form $\iota_V(v)$ for some finite-dimensional vector space $V$ and some vector $v \in V$, where $\iota_V : V \to \mathrm{colim}(X)$ is the colimit inclusion. This is because for every $V,W \in \mathsf{iFinVect}_K$ there is some $U \in \mathsf{iFinVect}_K$ with morphisms $V \xrightarrow{f} U \xleftarrow{g} W$, namely the coproduct. This implies $\iota_V(v)+\iota_W(w) = \iota_U(f(v)+g(w))$. Of course we have $\lambda \cdot \iota_V(v)=\iota_V(\lambda \cdot v)$ for $\lambda \in K$. This shows how to calculate with elements of $\mathrm{colim}(X)$. Notice, however, that $\mathsf{iFinVect}_K$ is not filtered (because the only parallel morphisms which may be coequalized by some morphism are already equal). For this reason I think that a priori it is not so easy to decide when two elements of a colimit, say $\iota_V(v)$ and $\iota_W(w)$, are equal. It suffices to find a criterion when some element $\iota_V(v)$ is zero. This can happen when $v \neq 0$! For example, we have $0=\iota_V(v)+\iota_V(-v) = \iota_{V \oplus V}((v,-v))$. So probably we should first answer: Do we have $\mathrm{colim}(X) \neq 0$?","Let $\mathsf{iFinVect}_K$ be the category of finite-dimensional vector spaces with injective linear maps and $X : \mathsf{iFinVect}_K \to \mathsf{Vect}_K$ be the inclusion functor. Then $\mathrm{colim}(X)$ exists. This is because $\mathsf{Vect}_K$ is cocomplete and $\mathsf{iFinVect}_K$ is essentially small (although it is not small). This colimit seems to be a bit strange to me, though. We merge all finite-dimensional vector spaces into a single large vector space. The embeddings are natural with respect to all injective linear maps between finite-dimensional vector spaces. Can we make this vector space more explicit? Is it, perhaps, even a well-known object? Can we find a basis? Every element of $\mathrm{colim}(X)$ should have the form $\iota_V(v)$ for some finite-dimensional vector space $V$ and some vector $v \in V$, where $\iota_V : V \to \mathrm{colim}(X)$ is the colimit inclusion. This is because for every $V,W \in \mathsf{iFinVect}_K$ there is some $U \in \mathsf{iFinVect}_K$ with morphisms $V \xrightarrow{f} U \xleftarrow{g} W$, namely the coproduct. This implies $\iota_V(v)+\iota_W(w) = \iota_U(f(v)+g(w))$. Of course we have $\lambda \cdot \iota_V(v)=\iota_V(\lambda \cdot v)$ for $\lambda \in K$. This shows how to calculate with elements of $\mathrm{colim}(X)$. Notice, however, that $\mathsf{iFinVect}_K$ is not filtered (because the only parallel morphisms which may be coequalized by some morphism are already equal). For this reason I think that a priori it is not so easy to decide when two elements of a colimit, say $\iota_V(v)$ and $\iota_W(w)$, are equal. It suffices to find a criterion when some element $\iota_V(v)$ is zero. This can happen when $v \neq 0$! For example, we have $0=\iota_V(v)+\iota_V(-v) = \iota_{V \oplus V}((v,-v))$. So probably we should first answer: Do we have $\mathrm{colim}(X) \neq 0$?",,"['linear-algebra', 'category-theory']"
71,Difference between dimension and rank of matrix,Difference between dimension and rank of matrix,,"Let $A = \left[ {\begin{array}{cc} 1 & 1 & 1 \\ 3 & -1 & 1 \\1 & 5 & 3\\ \end{array} } \right]$ and $V$ be the vector space of all $X\in \mathbb{R^3}$ such that $AX = 0$. Then what is $dim(V)$ ? Here when I change the matrix into echelon form I get $\left[ {\begin{array}{cc} 1 & 1 & 1 \\ 0 & -4 & -2 \\0 & 0 & 0\\ \end{array} } \right]$, which means that $rank(A) = 2$; we have one free variable and two basic variables; and $dim(null space) = 1$. So, is the $dim(V) = rank(A) = 2$? or $dim(V)$ = number of elements in basis of null space = $1$?or we apply sylvester's law $(dim = rank + nullity)$ to get $3$?? Which will be correct and why?? Very confused... Thanks..","Let $A = \left[ {\begin{array}{cc} 1 & 1 & 1 \\ 3 & -1 & 1 \\1 & 5 & 3\\ \end{array} } \right]$ and $V$ be the vector space of all $X\in \mathbb{R^3}$ such that $AX = 0$. Then what is $dim(V)$ ? Here when I change the matrix into echelon form I get $\left[ {\begin{array}{cc} 1 & 1 & 1 \\ 0 & -4 & -2 \\0 & 0 & 0\\ \end{array} } \right]$, which means that $rank(A) = 2$; we have one free variable and two basic variables; and $dim(null space) = 1$. So, is the $dim(V) = rank(A) = 2$? or $dim(V)$ = number of elements in basis of null space = $1$?or we apply sylvester's law $(dim = rank + nullity)$ to get $3$?? Which will be correct and why?? Very confused... Thanks..",,"['linear-algebra', 'matrices']"
72,"Given $BA$, find $AB$.","Given , find .",BA AB,"If $A, B$ are $4\times 3, 3\times 4$ real matrices, respectively, $$BA=\begin{bmatrix} -9 & -20 & -35 \\ 2 & 5 & 7 \\ 2 & 4 &8 \end{bmatrix}$$ $$AB=\begin{bmatrix} -14 & 0 & -15&-32 \\ 2x-9 & 1 & 3x-9&4x-19\\ 2 & 0 & 3&4\\6&0&6&14 \end{bmatrix}$$ What is $x$ ? I try some ways, for example, $\det AB=0$, but it didn't work","If $A, B$ are $4\times 3, 3\times 4$ real matrices, respectively, $$BA=\begin{bmatrix} -9 & -20 & -35 \\ 2 & 5 & 7 \\ 2 & 4 &8 \end{bmatrix}$$ $$AB=\begin{bmatrix} -14 & 0 & -15&-32 \\ 2x-9 & 1 & 3x-9&4x-19\\ 2 & 0 & 3&4\\6&0&6&14 \end{bmatrix}$$ What is $x$ ? I try some ways, for example, $\det AB=0$, but it didn't work",,"['linear-algebra', 'matrices']"
73,Are matrix $p$-norms unitary invariant?,Are matrix -norms unitary invariant?,p,"Consider a matrix $X \in \mathbb{R}^{N \times N}$. Let $\| X \|_p$ be its $p$-norm $$\| X\|_p = \left( \sum_{ij} |X_{ij}|^p \right)^{\frac 1p}$$ Is $\|X\|_p$ unitary invariant? That is, given any unitary matrix $U$, $V$, is $\|X\|_p$ = $\|UXV\|_p$? My strategy is to formulate the $p$-norm into some kind of singular value or trace, but I failed to do this. If it is not unitary invariant, how can I prove it?","Consider a matrix $X \in \mathbb{R}^{N \times N}$. Let $\| X \|_p$ be its $p$-norm $$\| X\|_p = \left( \sum_{ij} |X_{ij}|^p \right)^{\frac 1p}$$ Is $\|X\|_p$ unitary invariant? That is, given any unitary matrix $U$, $V$, is $\|X\|_p$ = $\|UXV\|_p$? My strategy is to formulate the $p$-norm into some kind of singular value or trace, but I failed to do this. If it is not unitary invariant, how can I prove it?",,"['linear-algebra', 'matrices', 'matrix-norms']"
74,How to compute the image of a polyhedron under a linear transformation,How to compute the image of a polyhedron under a linear transformation,,"Suppose $P$ is a polyhedron whose representation as a system of linear inequalities is given to us: $$ P = \{ x ~|~ Ax \leq b\}$$ Define $P'$ be the image of $P$ under the linear transformation which maps $x$ to $Cx$: $$ P' = \{ Cx ~|~ x \in P \}.$$ Given $A,b,C$, our goal is to compute a representation of $P'$ as a system of linear inequalities, i.e., to compute $D,e$ satisfying $$ P' = \{ x ~|~ D x \leq e \}.$$ What is the complexity of this problem (i.e., how many arithmetic operations or bit operations are required)? Let us adopt the convention that $A \in R^{m \times n}$ while $C \in R^{k \times n}$ so the answer should be in terms of $m,n,k$. Further, one may suppose that entries of $P$ are rational numbers whose numerators and denominators take $B$ bits to specify, so that in the bit-model answers should be in terms of $B$ as well.","Suppose $P$ is a polyhedron whose representation as a system of linear inequalities is given to us: $$ P = \{ x ~|~ Ax \leq b\}$$ Define $P'$ be the image of $P$ under the linear transformation which maps $x$ to $Cx$: $$ P' = \{ Cx ~|~ x \in P \}.$$ Given $A,b,C$, our goal is to compute a representation of $P'$ as a system of linear inequalities, i.e., to compute $D,e$ satisfying $$ P' = \{ x ~|~ D x \leq e \}.$$ What is the complexity of this problem (i.e., how many arithmetic operations or bit operations are required)? Let us adopt the convention that $A \in R^{m \times n}$ while $C \in R^{k \times n}$ so the answer should be in terms of $m,n,k$. Further, one may suppose that entries of $P$ are rational numbers whose numerators and denominators take $B$ bits to specify, so that in the bit-model answers should be in terms of $B$ as well.",,"['linear-algebra', 'convex-analysis', 'polyhedra']"
75,What is the Lie algebra of the ``indefinite orthogonal group''?,What is the Lie algebra of the ``indefinite orthogonal group''?,,"For $p,q \geq 0$ and $n=p+q\geq 1$, give $\mathbb{R}^n$ the indefinite inner product (written as a matrix) $$ \begin{pmatrix} I_p & \\ & -I_q \end{pmatrix}, $$ where $I_m$ is the $m \times m$ identity matrix.  For example, if $\{e_i\}$ is a basis of $\mathbb{R}^n$ and $X = X^i e_i,$ then $$ |X|^2 = (X^1)^2 + \cdots + (X^p)^2 - (X^{p+1})^2 - \cdots - (X^{p+q})^2.$$ Let $\mathrm{O}(p,q,\mathbb{R})$ be the Lie group of all linear transformations $T : \mathbb{R}^n \rightarrow \mathbb{R}^n$ that preserve this indefinite inner product. What is the Lie algebra of $\mathrm{O}(p,q,\mathbb{R})$?  Does it admit a ``nice'' description when $p$ and $q$ are both positive?","For $p,q \geq 0$ and $n=p+q\geq 1$, give $\mathbb{R}^n$ the indefinite inner product (written as a matrix) $$ \begin{pmatrix} I_p & \\ & -I_q \end{pmatrix}, $$ where $I_m$ is the $m \times m$ identity matrix.  For example, if $\{e_i\}$ is a basis of $\mathbb{R}^n$ and $X = X^i e_i,$ then $$ |X|^2 = (X^1)^2 + \cdots + (X^p)^2 - (X^{p+1})^2 - \cdots - (X^{p+q})^2.$$ Let $\mathrm{O}(p,q,\mathbb{R})$ be the Lie group of all linear transformations $T : \mathbb{R}^n \rightarrow \mathbb{R}^n$ that preserve this indefinite inner product. What is the Lie algebra of $\mathrm{O}(p,q,\mathbb{R})$?  Does it admit a ``nice'' description when $p$ and $q$ are both positive?",,"['linear-algebra', 'matrices', 'lie-algebras', 'lie-groups']"
76,"invertible $\{0, 1\}$ matrix",invertible  matrix,"\{0, 1\}","For an $n\times n$ matrix $A$ with entries from $\{0,1\}$. What is the maximum number of 1's such that $A$ is invertible? If $n=2$, the answer is $3$. If $n=3$, the answer is $7$. Is there a formula for general $n$?","For an $n\times n$ matrix $A$ with entries from $\{0,1\}$. What is the maximum number of 1's such that $A$ is invertible? If $n=2$, the answer is $3$. If $n=3$, the answer is $7$. Is there a formula for general $n$?",,['linear-algebra']
77,Is there a non-trivial example of a $\mathbb Q-$endomorphism of $\mathbb R$?,Is there a non-trivial example of a endomorphism of ?,\mathbb Q- \mathbb R,"$\mathbb R$ is an uncountably dimensional vector space over $\mathbb Q.$ We can define as many endomorphisms of this vector space as we want by picking their values on the elements of the basis. However, to have a basis we need to use the axiom of choice, so this way is non-constructive. Any $\mathbb R-$ endomorphism of $\mathbb R$ is also a $\mathbb Q-$endomorphism. But can we give a concrete example of a $\mathbb Q-$endomorphism of $\mathbb R$ that is not an $\mathbb R-$ endomorphism?","$\mathbb R$ is an uncountably dimensional vector space over $\mathbb Q.$ We can define as many endomorphisms of this vector space as we want by picking their values on the elements of the basis. However, to have a basis we need to use the axiom of choice, so this way is non-constructive. Any $\mathbb R-$ endomorphism of $\mathbb R$ is also a $\mathbb Q-$endomorphism. But can we give a concrete example of a $\mathbb Q-$endomorphism of $\mathbb R$ that is not an $\mathbb R-$ endomorphism?",,['linear-algebra']
78,Dimension of the sum of subspaces,Dimension of the sum of subspaces,,"Let $V_1, \ldots, V_n$ be $n$ subspaces of a vector space $V$. Is there a formula for $\dim(V_1 + \cdots + V_n)$ similar to  $\dim(V_1 + V_2)=\dim(V_1) + \dim(V_1) - \dim(V_1 \cap V_2)$?","Let $V_1, \ldots, V_n$ be $n$ subspaces of a vector space $V$. Is there a formula for $\dim(V_1 + \cdots + V_n)$ similar to  $\dim(V_1 + V_2)=\dim(V_1) + \dim(V_1) - \dim(V_1 \cap V_2)$?",,['linear-algebra']
79,Proving that $\mathbf{W}$+$\mathbf{W^{\perp}}$=$\mathbb{R^{n}}$,Proving that +=,\mathbf{W} \mathbf{W^{\perp}} \mathbb{R^{n}},"I am trying to prove that given a subspace $\mathbf{W}$ in $\mathbb{R^{n}}$, the subspace and its orthogonal complement 'cover' whole of $\mathbb{R^{n}}$ through '+' where we define $\mathbf{W}$+$\mathbf{W^{\perp}}$ as linear combinations of vectors both in the subspace and in its orthogonal complement. It seems intuitively right, and I can prove that the sum of their dimensions adds up to n, but I am not sure how to prove the question I am looking at. Thanks!","I am trying to prove that given a subspace $\mathbf{W}$ in $\mathbb{R^{n}}$, the subspace and its orthogonal complement 'cover' whole of $\mathbb{R^{n}}$ through '+' where we define $\mathbf{W}$+$\mathbf{W^{\perp}}$ as linear combinations of vectors both in the subspace and in its orthogonal complement. It seems intuitively right, and I can prove that the sum of their dimensions adds up to n, but I am not sure how to prove the question I am looking at. Thanks!",,['linear-algebra']
80,Symmetrize eigenvectors of degenerate (repeated) eigenvalue,Symmetrize eigenvectors of degenerate (repeated) eigenvalue,,"I have a Hermitian matrix $A$ that satisfies some symmetries which I can express via $AS = SA$ for a unitary matrix $S$ . Now I am interested in the eigenvectors of $A$ , but I want that these eigenvectors also respect my symmetries ( compare to Bloch waves in physics where the eigenvectors are chosen to reflect the translational invariance of the lattice). Since $A$ has degenerate (repeated) eigenvalues, the standard numerical techniques will return some arbitrary (yet orthonormal) eigenvectors spanning the eigenspace. I, however, want to obtain unique results and thus want to make use of the symmetries. How can I do this numerically? I know that commuting matrices can be diagonalized simultaneously - in theory. But I don't know how to do it practically. Would I have to diagonalize one of them, apply the unitary transform thus obtained to the other one, arriving at a block diagonal form where I then have to diagonalize each block separately? Or is there something more elegant I can do? EDIT: The matrix is dense, but quite small (12x12 to 18x18). The symmetry would be something like translation symmetry: $$\begin{pmatrix} 0 & 0 & 0 & -1 \\ 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0  & 0 & 1 & 0 \end{pmatrix}$$ where each entry is a 3x3-block in the case of a $12 \times 12$ matrix, which looks something like $$\begin{pmatrix} a & b & 0 & -b\\ b&a&b&0\\ 0&b&a&b\\ -b&0&b&a\end{pmatrix}$$","I have a Hermitian matrix that satisfies some symmetries which I can express via for a unitary matrix . Now I am interested in the eigenvectors of , but I want that these eigenvectors also respect my symmetries ( compare to Bloch waves in physics where the eigenvectors are chosen to reflect the translational invariance of the lattice). Since has degenerate (repeated) eigenvalues, the standard numerical techniques will return some arbitrary (yet orthonormal) eigenvectors spanning the eigenspace. I, however, want to obtain unique results and thus want to make use of the symmetries. How can I do this numerically? I know that commuting matrices can be diagonalized simultaneously - in theory. But I don't know how to do it practically. Would I have to diagonalize one of them, apply the unitary transform thus obtained to the other one, arriving at a block diagonal form where I then have to diagonalize each block separately? Or is there something more elegant I can do? EDIT: The matrix is dense, but quite small (12x12 to 18x18). The symmetry would be something like translation symmetry: where each entry is a 3x3-block in the case of a matrix, which looks something like",A AS = SA S A A \begin{pmatrix} 0 & 0 & 0 & -1 \\ 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0  & 0 & 1 & 0 \end{pmatrix} 12 \times 12 \begin{pmatrix} a & b & 0 & -b\\ b&a&b&0\\ 0&b&a&b\\ -b&0&b&a\end{pmatrix},"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
81,Upper-triangular matrix is invertible iff its diagonal is invertible: C*-algebra case,Upper-triangular matrix is invertible iff its diagonal is invertible: C*-algebra case,,"Exercise 1.14 of the book Rordam, Larsen and Laustsen ""An introduction to K-theory for C*-algebras"" asks to prove that an upper triangular matrix with elements from some C*-algebra $A$ is invertible in $M_n(A)$ iff all diagonal entries are invertible in $A$ . Trying to solve this I've found that if $a$ is invertible and $\delta$ is such that $(a^{-1}\delta)^n=0$ then $a+\delta$ is invertible too and its inverse is given by $(a+\delta)^{-1}=\sum_{k=0}^{n} (-a^{-1}\delta)^ka^{-1}$ . Using this fact I can show that if diagonal is invertible, then upper-triangular matrix with this diagonal is invertible too, and also that if upper-triangular matrix has upper-triangular inverse, then its diagonal is invertible. So all I need to prove is that if upper-triangular matrix invertible, then its inverse is upper-triangular. I've failed to prove this. Also there is a hint for this exercise: ""Solve the equation $ab=1$ where $a$ is as above [i.e. upper-triangular matrix] and where $b$ is unknown upper triangular matrix"". Solution of this equation follows from my reasoning above, but this doesn't help. Update (counterexample attempt): I've made one more attempt and it looks for me like I have found a counterexample. However I think there is a mistake in it (because otherwise there is a mistake in the book ). Here it is. Let $A=B(l^2(\mathbb{N}))$ --- algebra of bounded operators on sequences $x=\{x_i\}_ {i=1}^ \infty:\|x\|^2=\sum_{x=1}^{\infty}|x_i|^2<\infty$ . Let $z\in A$ be defined by $(zx)_ {2n-1}=0$ , $(zx)_{2n}=x_n$ , and $t\in A$ be defined by $(tx)_{2n-1}=x_n$ , $(tx)_ {2n}=0$ . Then we have $t^*t=z^ * z=tt^* +zz^* =1$ , $t^* z=z^* t=0$ . From these we have that $$\begin{pmatrix}z&tz^* \\\ 0&t^* \end{pmatrix}\begin{pmatrix}z^* &0\\\ zt^* &t\end{pmatrix}=\begin{pmatrix}1&0\\\ 0&1\end{pmatrix}$$ and $$\begin{pmatrix}z^* &0\\\ zt^* &t\end{pmatrix}\begin{pmatrix}z&tz^* \\\ 0&t^* \end{pmatrix}=\begin{pmatrix}1&0\\\ 0&1\end{pmatrix}.$$ So now my question should say ""Where am I wrong?"".","Exercise 1.14 of the book Rordam, Larsen and Laustsen ""An introduction to K-theory for C*-algebras"" asks to prove that an upper triangular matrix with elements from some C*-algebra is invertible in iff all diagonal entries are invertible in . Trying to solve this I've found that if is invertible and is such that then is invertible too and its inverse is given by . Using this fact I can show that if diagonal is invertible, then upper-triangular matrix with this diagonal is invertible too, and also that if upper-triangular matrix has upper-triangular inverse, then its diagonal is invertible. So all I need to prove is that if upper-triangular matrix invertible, then its inverse is upper-triangular. I've failed to prove this. Also there is a hint for this exercise: ""Solve the equation where is as above [i.e. upper-triangular matrix] and where is unknown upper triangular matrix"". Solution of this equation follows from my reasoning above, but this doesn't help. Update (counterexample attempt): I've made one more attempt and it looks for me like I have found a counterexample. However I think there is a mistake in it (because otherwise there is a mistake in the book ). Here it is. Let --- algebra of bounded operators on sequences . Let be defined by , , and be defined by , . Then we have , . From these we have that and So now my question should say ""Where am I wrong?"".",A M_n(A) A a \delta (a^{-1}\delta)^n=0 a+\delta (a+\delta)^{-1}=\sum_{k=0}^{n} (-a^{-1}\delta)^ka^{-1} ab=1 a b A=B(l^2(\mathbb{N})) x=\{x_i\}_ {i=1}^ \infty:\|x\|^2=\sum_{x=1}^{\infty}|x_i|^2<\infty z\in A (zx)_ {2n-1}=0 (zx)_{2n}=x_n t\in A (tx)_{2n-1}=x_n (tx)_ {2n}=0 t^*t=z^ * z=tt^* +zz^* =1 t^* z=z^* t=0 \begin{pmatrix}z&tz^* \\\ 0&t^* \end{pmatrix}\begin{pmatrix}z^* &0\\\ zt^* &t\end{pmatrix}=\begin{pmatrix}1&0\\\ 0&1\end{pmatrix} \begin{pmatrix}z^* &0\\\ zt^* &t\end{pmatrix}\begin{pmatrix}z&tz^* \\\ 0&t^* \end{pmatrix}=\begin{pmatrix}1&0\\\ 0&1\end{pmatrix}.,"['linear-algebra', 'abstract-algebra', 'matrices', 'c-star-algebras']"
82,Dual statement to the fundamental theorem of group homomorphism?,Dual statement to the fundamental theorem of group homomorphism?,,"Let $G$ and $H$ be groups. The fundamental theorem of group homomophism states that, for any surjection $f : G \to H$ , there exists an isomorphism $\eta : G/\text{ker}(f) \to H$ . I am wondering if there exists a dual statement to this theorem: starting with an injection $f : G \to H$ . It would be interesting to see $\text{im}(f)$ (or the cokernel of some map) appearing instead of $\text{ker}(f)$ , in some way. I've read that surjections and injections exist in opposite categories; I've thus tagged 'category theory', for potential reasoning in this area.","Let and be groups. The fundamental theorem of group homomophism states that, for any surjection , there exists an isomorphism . I am wondering if there exists a dual statement to this theorem: starting with an injection . It would be interesting to see (or the cokernel of some map) appearing instead of , in some way. I've read that surjections and injections exist in opposite categories; I've thus tagged 'category theory', for potential reasoning in this area.",G H f : G \to H \eta : G/\text{ker}(f) \to H f : G \to H \text{im}(f) \text{ker}(f),"['linear-algebra', 'group-theory', 'commutative-algebra', 'category-theory', 'group-homomorphism']"
83,Volume of the intersection of the unit ball with a polyhedral cone,Volume of the intersection of the unit ball with a polyhedral cone,,"Given vectors $x_1,...,x_n\in\Bbb R^d$ . The conic span of these vectors is $$\mathrm{cone}\{x_1,...,x_n\}:=\{\alpha_1 x_1+\cdots +\alpha_n x_n\mid \alpha_1,...,\alpha_n\ge 0\}.$$ Question: Is there a ""simple"" explicit formula for computing the volume of $\mathrm{cone}\{x_1,...,x_n\}\cap \Bbb B^n$ , where $\Bbb B^n$ is the unit ball centered at the origin? $\mathrm{Vol}$ indicates the volume that I am interested in.","Given vectors . The conic span of these vectors is Question: Is there a ""simple"" explicit formula for computing the volume of , where is the unit ball centered at the origin? indicates the volume that I am interested in.","x_1,...,x_n\in\Bbb R^d \mathrm{cone}\{x_1,...,x_n\}:=\{\alpha_1 x_1+\cdots +\alpha_n x_n\mid \alpha_1,...,\alpha_n\ge 0\}. \mathrm{cone}\{x_1,...,x_n\}\cap \Bbb B^n \Bbb B^n \mathrm{Vol}","['linear-algebra', 'lebesgue-measure', 'volume', 'convex-geometry', 'convex-cone']"
84,"When solving for eigenvector, when do you have to check every equation?","When solving for eigenvector, when do you have to check every equation?",,"For example, suppose we want to solve for the eigenvectors of: $$A = \begin{bmatrix} 0 & 1 \\ 2 & -1 \end{bmatrix} $$ We quickly find the eigenvalues are $1, -2$ i.e. $\sigma(A) = \{1, -2\}$ Then $Av_1 = \lambda_1 v_1$ $$ \begin{bmatrix} 0 & 1 \\ 2 & -1 \end{bmatrix} \begin{bmatrix}v_{11} \\ v_{12} \end{bmatrix} =  \begin{bmatrix} -2v_{11} \\ -2v_{12} \end{bmatrix}$$ We get two equations: $v_{12} = -2 v_{11}$ $2v_{11} - v_{12} = -2 v_{12}$ Solving either equation will yield equivalent eigenvectors i.e. $v_1 = [1 -2]^T$. What account for this property? Why is it that we only need to check for one equation in this case? Are there conditions or criteria for when we should check all equations instead just a single one?","For example, suppose we want to solve for the eigenvectors of: $$A = \begin{bmatrix} 0 & 1 \\ 2 & -1 \end{bmatrix} $$ We quickly find the eigenvalues are $1, -2$ i.e. $\sigma(A) = \{1, -2\}$ Then $Av_1 = \lambda_1 v_1$ $$ \begin{bmatrix} 0 & 1 \\ 2 & -1 \end{bmatrix} \begin{bmatrix}v_{11} \\ v_{12} \end{bmatrix} =  \begin{bmatrix} -2v_{11} \\ -2v_{12} \end{bmatrix}$$ We get two equations: $v_{12} = -2 v_{11}$ $2v_{11} - v_{12} = -2 v_{12}$ Solving either equation will yield equivalent eigenvectors i.e. $v_1 = [1 -2]^T$. What account for this property? Why is it that we only need to check for one equation in this case? Are there conditions or criteria for when we should check all equations instead just a single one?",,"['linear-algebra', 'matrices', 'vector-spaces', 'eigenvalues-eigenvectors', 'matrix-rank']"
85,"Subgroups of $\mathrm{GL}(n,\mathbb{Z})$ which are not finitely generated",Subgroups of  which are not finitely generated,"\mathrm{GL}(n,\mathbb{Z})","The group $\mathrm{GL}(n,\mathbb{Z})$ is finitely generated: take for example diagonal matrices, permutations and one elementary matrix (upper triangular). Are there some simple / nice examples of non-finitely generated subgroups? If possible, for $n$ not too big.","The group $\mathrm{GL}(n,\mathbb{Z})$ is finitely generated: take for example diagonal matrices, permutations and one elementary matrix (upper triangular). Are there some simple / nice examples of non-finitely generated subgroups? If possible, for $n$ not too big.",,"['linear-algebra', 'group-theory', 'matrices', 'finitely-generated']"
86,"$\hom(V,W)$ is canonic isomorph to $\hom(W^*, V^*)$",is canonic isomorph to,"\hom(V,W) \hom(W^*, V^*)","Introduction My Semester just started and we have a new Professor for Linear Algebra II (replacing our former Professor). Apparently we are behind our schedule and thus we only had a brief introduction (5 Minutes on the Blackboard) about the Dual Space and canonical isomorphisms . In the current problem set there is this optional (not mandatory and not accredited) exercise which I understand as good as nothing about: Problem : Let $V$ and $W$ be finite dimensional $\mathbb{K}$-Vectorspaces$^{1)}$. Show that $\hom(V,W)$ is canonical isomorph to $\hom(W^*,V^*)$ and find the canonical isomorphism. $^{1)}$ ($\mathbb{K}$ denotes a field here, I presume in english literature they write $\mathbb{F}$) Hint (given by my tutor): $$ \Phi: \hom(V,W) \longrightarrow  \hom(W^*,V^*) \\ \Psi \longmapsto (\varphi \longmapsto \varphi\circ \Psi)$$ My problems are vast now: The literature I am reading does a bad job at explaining this topic The online literature I find seems to be way beyond my level, they usually include tensor algebra to solve this. So I fall down the math rabbit hole see, On ""familiarity"" (or How to avoid ""going down the Math Rabbit Hole""?) I don't understand the Hint , I did not encounter such a function before, not even in Analysis and I don't know what's happening. I can only guess that it is a function that somehow completes a circle. On the bright side : I believe to understand what a canonical isomorphism is. My Professor said it is an isomorphism that does not involve making a choice for a basis. In the above function we do not make such a decision and therefore it would be a candidate for a canonical isomorphism. If I would understand the above function, I'd merely have to show that it is injective (trivial Kern) and surjective to complete the task. Unfortunately, as far, we've only done such things using an explicit Matrix and not a function. I do know the definition of the Dual Space, also using the Kronecker Delta.  (although I know nothing about it's meaning and geometric intrepreation, if there is any) The way my tutors sell this exercise it's supposed to be very easy, once the definitions are clear. To-Do-List : Understand the function, know what's happening. Show that the function is surjective Show that the function is injective (trivial Kern)","Introduction My Semester just started and we have a new Professor for Linear Algebra II (replacing our former Professor). Apparently we are behind our schedule and thus we only had a brief introduction (5 Minutes on the Blackboard) about the Dual Space and canonical isomorphisms . In the current problem set there is this optional (not mandatory and not accredited) exercise which I understand as good as nothing about: Problem : Let $V$ and $W$ be finite dimensional $\mathbb{K}$-Vectorspaces$^{1)}$. Show that $\hom(V,W)$ is canonical isomorph to $\hom(W^*,V^*)$ and find the canonical isomorphism. $^{1)}$ ($\mathbb{K}$ denotes a field here, I presume in english literature they write $\mathbb{F}$) Hint (given by my tutor): $$ \Phi: \hom(V,W) \longrightarrow  \hom(W^*,V^*) \\ \Psi \longmapsto (\varphi \longmapsto \varphi\circ \Psi)$$ My problems are vast now: The literature I am reading does a bad job at explaining this topic The online literature I find seems to be way beyond my level, they usually include tensor algebra to solve this. So I fall down the math rabbit hole see, On ""familiarity"" (or How to avoid ""going down the Math Rabbit Hole""?) I don't understand the Hint , I did not encounter such a function before, not even in Analysis and I don't know what's happening. I can only guess that it is a function that somehow completes a circle. On the bright side : I believe to understand what a canonical isomorphism is. My Professor said it is an isomorphism that does not involve making a choice for a basis. In the above function we do not make such a decision and therefore it would be a candidate for a canonical isomorphism. If I would understand the above function, I'd merely have to show that it is injective (trivial Kern) and surjective to complete the task. Unfortunately, as far, we've only done such things using an explicit Matrix and not a function. I do know the definition of the Dual Space, also using the Kronecker Delta.  (although I know nothing about it's meaning and geometric intrepreation, if there is any) The way my tutors sell this exercise it's supposed to be very easy, once the definitions are clear. To-Do-List : Understand the function, know what's happening. Show that the function is surjective Show that the function is injective (trivial Kern)",,"['linear-algebra', 'definition', 'self-learning', 'vector-space-isomorphism']"
87,Proof that multiplying by the scalar 1 does not change the vector in a normed vector space.,Proof that multiplying by the scalar 1 does not change the vector in a normed vector space.,,"I'm beginning a self-study of functional analysis, and I seem to have come to a halt trying to solve the first problem in the first problem set, and was wondering if someone could give me a pointer. The problem asks to show that, while the property $1\mathbf{a}=\mathbf{a}$ must be included in the axioms of a vector space, it is a property that can be derived from the axioms of a normed vector space. The axioms for a vector space $V$ over the field $\mathbf{R}$ given in the book are as follow: $V$ is a group. For each $\alpha\in\mathbf{R}$ and $\mathbf{a}\in V$, $\alpha\mathbf{a}\in V$ $\alpha(\mathbf{a}+\mathbf{b})=\alpha\mathbf{a}+\alpha\mathbf{b}$ $(\alpha+\beta)\mathbf{a}=\alpha\mathbf{a}+\beta\mathbf{a}$ $\alpha(\beta\mathbf{a})=(\alpha\beta)\mathbf{a}$ $1\mathbf{a}=\mathbf{a}$ The axioms for a normed vector space are 1--5 plus the additional axiom that there is a map $\Vert\cdot\Vert:V\to\mathbf{R}$ that satisfies all the properties of a norm. The problem formally stated now is to show that 1--5 do not imply 6, but 1--5 together with the norm do imply 6. This problem seems similar to the question posed here , and from the discussion of that question it seems an easy example of a vector space satisfying 1--5 but not 6 would be one where scalar multiplication always yields the zero vector. This cannot be the case in a normed vector space, because for $\mathbf{a}\neq0$ and $\alpha\neq0$ we know $\Vert\mathbf{a}\Vert\neq0$, and a norm satisfies $\Vert\alpha\mathbf{a}\Vert=\vert\alpha\vert\Vert\mathbf{a}\Vert\neq0$, but we also have $\Vert\alpha\mathbf{a}\Vert=\Vert\mathbf{0}\Vert=0$ from our definition of scalar multiplication, which is a contradiction. It is trivial that $1\mathbf{a}=\mathbf{a}$ if every vector in $V$ can be written as a scalar multiple of a vector in $V$, since then $\mathbf{a}=\alpha\mathbf{b}=(1\alpha)\mathbf{b}=1(\alpha\mathbf{b})=1\mathbf{a}$, but while the norm has ruled out the strange case of scalar multiplication that means only $\mathbf{0}$ can be written as a scalar multiple of a vector, I haven't been able to convince myself that other cases where there exist vectors that are not scalar multiples of a vector have been ruled out. Can anyone offer a pointer for how to show every vector in a normed vector space can be written as a scalar multiple of a vector in the space, or perhaps point me to a more natural way of approaching this problem? Thanks.","I'm beginning a self-study of functional analysis, and I seem to have come to a halt trying to solve the first problem in the first problem set, and was wondering if someone could give me a pointer. The problem asks to show that, while the property $1\mathbf{a}=\mathbf{a}$ must be included in the axioms of a vector space, it is a property that can be derived from the axioms of a normed vector space. The axioms for a vector space $V$ over the field $\mathbf{R}$ given in the book are as follow: $V$ is a group. For each $\alpha\in\mathbf{R}$ and $\mathbf{a}\in V$, $\alpha\mathbf{a}\in V$ $\alpha(\mathbf{a}+\mathbf{b})=\alpha\mathbf{a}+\alpha\mathbf{b}$ $(\alpha+\beta)\mathbf{a}=\alpha\mathbf{a}+\beta\mathbf{a}$ $\alpha(\beta\mathbf{a})=(\alpha\beta)\mathbf{a}$ $1\mathbf{a}=\mathbf{a}$ The axioms for a normed vector space are 1--5 plus the additional axiom that there is a map $\Vert\cdot\Vert:V\to\mathbf{R}$ that satisfies all the properties of a norm. The problem formally stated now is to show that 1--5 do not imply 6, but 1--5 together with the norm do imply 6. This problem seems similar to the question posed here , and from the discussion of that question it seems an easy example of a vector space satisfying 1--5 but not 6 would be one where scalar multiplication always yields the zero vector. This cannot be the case in a normed vector space, because for $\mathbf{a}\neq0$ and $\alpha\neq0$ we know $\Vert\mathbf{a}\Vert\neq0$, and a norm satisfies $\Vert\alpha\mathbf{a}\Vert=\vert\alpha\vert\Vert\mathbf{a}\Vert\neq0$, but we also have $\Vert\alpha\mathbf{a}\Vert=\Vert\mathbf{0}\Vert=0$ from our definition of scalar multiplication, which is a contradiction. It is trivial that $1\mathbf{a}=\mathbf{a}$ if every vector in $V$ can be written as a scalar multiple of a vector in $V$, since then $\mathbf{a}=\alpha\mathbf{b}=(1\alpha)\mathbf{b}=1(\alpha\mathbf{b})=1\mathbf{a}$, but while the norm has ruled out the strange case of scalar multiplication that means only $\mathbf{0}$ can be written as a scalar multiple of a vector, I haven't been able to convince myself that other cases where there exist vectors that are not scalar multiples of a vector have been ruled out. Can anyone offer a pointer for how to show every vector in a normed vector space can be written as a scalar multiple of a vector in the space, or perhaps point me to a more natural way of approaching this problem? Thanks.",,"['linear-algebra', 'vector-spaces', 'normed-spaces']"
88,Rank of the difference of matrices [duplicate],Rank of the difference of matrices [duplicate],,This question already has an answer here : Prove that $\operatorname{rank}(A) + \operatorname{rank}(B) \ge \operatorname{rank}(A + B)$ (1 answer) Closed 11 years ago . Let $A$ and $B$ be to $n \times n$ matrices. My question is: Is $\operatorname{rank}(A-B) \geq \operatorname{rank}(A) - \operatorname{rank}(B)$ true in general? Or maybe under certain assumptions?,This question already has an answer here : Prove that $\operatorname{rank}(A) + \operatorname{rank}(B) \ge \operatorname{rank}(A + B)$ (1 answer) Closed 11 years ago . Let $A$ and $B$ be to $n \times n$ matrices. My question is: Is $\operatorname{rank}(A-B) \geq \operatorname{rank}(A) - \operatorname{rank}(B)$ true in general? Or maybe under certain assumptions?,,"['linear-algebra', 'matrices']"
89,Adjoint Operators and Inner Product Spaces,Adjoint Operators and Inner Product Spaces,,"My linear algebra textbook gives the definition of the Adjoint Operator and then says, You should verify the following properties: Additivity: $(S + T)^* = S^* + T^*$ Conjugate homogeneity: $(aT)^* = \overline{a}\,T^*$ Adjoint of adjoint: $(T^*)^* = T$ Identity: $I^* = I$, where $I$ is the identity operator on $V$. I've stared at the pages for a couple hours now. How do you verify this? Here's my attempt at a proof for Adjoint of Adjoint: (T*)* = (T*v, w)* = (v, Tw)* = (Tv, w) = T Is that correct reasoning? BTW, this is NOT homework. Just reading for pleasure. Thanks!","My linear algebra textbook gives the definition of the Adjoint Operator and then says, You should verify the following properties: Additivity: $(S + T)^* = S^* + T^*$ Conjugate homogeneity: $(aT)^* = \overline{a}\,T^*$ Adjoint of adjoint: $(T^*)^* = T$ Identity: $I^* = I$, where $I$ is the identity operator on $V$. I've stared at the pages for a couple hours now. How do you verify this? Here's my attempt at a proof for Adjoint of Adjoint: (T*)* = (T*v, w)* = (v, Tw)* = (Tv, w) = T Is that correct reasoning? BTW, this is NOT homework. Just reading for pleasure. Thanks!",,['linear-algebra']
90,$A = B^2$ for which matrix $A$?,for which matrix ?,A = B^2 A,"Is it true that for any $A\in M(n,\mathbb{C})$there exist a $B\in M(n,\mathbb{C})$ such that $A = B^2$? I think this is not true (but I don't know nay example), and then is it possible to characterize such $A$?","Is it true that for any $A\in M(n,\mathbb{C})$there exist a $B\in M(n,\mathbb{C})$ such that $A = B^2$? I think this is not true (but I don't know nay example), and then is it possible to characterize such $A$?",,"['linear-algebra', 'matrices', 'radicals']"
91,Linear independence of $n$th roots over $\mathbb{Q}$,Linear independence of th roots over,n \mathbb{Q},"I know that the set of square roots of distinct square-free integers is linearly independent over $\mathbb{Q}$. To generalize this fact, define $R_n = \{ \sqrt[n]{s} \mid s\text{ integer with prime factorization }s = p_1^{a_1} \ldots p_k^{a_k}, \text{ where } 0 \leq a_i < n \}$ For example, $R_2$ is the set of square roots of square-free integers. Question: Is $R_n$ linearly independent over $\mathbb{Q}$ for all $n \geq 2$? Harder (?) question: Is $\cup_{n\geq2}R_n$ linearly independent over $\mathbb{Q}$?","I know that the set of square roots of distinct square-free integers is linearly independent over $\mathbb{Q}$. To generalize this fact, define $R_n = \{ \sqrt[n]{s} \mid s\text{ integer with prime factorization }s = p_1^{a_1} \ldots p_k^{a_k}, \text{ where } 0 \leq a_i < n \}$ For example, $R_2$ is the set of square roots of square-free integers. Question: Is $R_n$ linearly independent over $\mathbb{Q}$ for all $n \geq 2$? Harder (?) question: Is $\cup_{n\geq2}R_n$ linearly independent over $\mathbb{Q}$?",,"['linear-algebra', 'abstract-algebra', 'elementary-number-theory']"
92,4th order tensors double dot product and inverse computation,4th order tensors double dot product and inverse computation,,"I am currently working on a subject that involves a lot of 4th order tensors computations including double dot product and inverse of fourth order tensors. First the definitions so that we are on the same page. What I call the double dot product is : $$ (A:B)_{ijkl} = A_{ijmn}B_{mnkl} $$ and for the double dot product between a fourth order tensor and a second order tensor : $$ (A:s)_{ij} = A_{ijkl}s_{kl}$$ Using the convention of sommation over repeating indices. What I call the identity of the fourth order tensors is the only tensor such that : $$ A:I = I:A = A $$ it is defined by $ I = \delta_{ik}\delta_{jl} e_{i} \otimes e_{j} \otimes e_{k} \otimes e_{l} $ . What I call the inverse of a fourth order tensor is the inverse with respect to the double dot product, that is, the inverse of $A$ is the only tensor $B$ such that $AB = BA = I$ . The double dot product is easy to compute if you don't think about the efficiency of the code, just create an array and loop over the four indices. Computing the inverse is something else. Every tensor I use has the minor symmetries $A_{ijkl} = A_{jikl} = A_{ijlk}$ so I thought I would use the Mandel representation for second order and fourth order tensors mentioned on Wikipedia. The fourth order tensor can be put into a $6 \times6$ matrix with the following components : $$ [C] = \begin{bmatrix}     C_{1111}  & C_{1122} & C_{1133} & \sqrt{2}C_{1123} & \sqrt{2}C_{1131} & \sqrt{2}C_{1112}\\     C_{2211} & C_{2222} & C_{2233} & \sqrt{2}C_{2223} & \sqrt{2}C_{2231} & \sqrt{2}C_{2212}\\     C_{3311} & C_{3322} & C_{3333} & \sqrt{2}C_{3323} & \sqrt{2}C_{3331} & \sqrt{2}C_{3312}\\     \sqrt{2}C_{2311} & \sqrt{2}C_{2322} & \sqrt{2}C_{2333} & 2C_{2323} & 2C_{2331} & 2C_{2312}\\     \sqrt{2}C_{3111} & \sqrt{2}C_{3122} & \sqrt{2}C_{3133} & 2C_{3123} & 2C_{3131} & 2C_{3112}\\     \sqrt{2}C_{1211} & \sqrt{2}C_{1222} & \sqrt{2}C_{1233} & 2C_{1223} &2C_{1231} & 2C_{1212} \end{bmatrix} $$ $C$ is a fourth order tensor with minor symmetries and $[C]$ is its Mandel representation. The reason why Mandel's representation exists according to different sources is such that the matrix-matrix and matrix-vector usual products coincide with the fourth order tensors double dot product and the inverse in each respective space (fourth order tensors and $6\times 6$ matrices) coincides as well, that is $$ [A:B] = [A] \cdot [B] \qquad \qquad (1) $$ and $$ [A^{-1}] = [A]^{-1} \qquad \qquad (2) $$ where $ \cdot $ is the usual matrix-matrix product. But it doesn't work or at least there must be something I don't understand. If I put the identity 4th order tensor defined above into Mandel's notation, I get the following matrix : $$ [I] = \begin{bmatrix}     1&0&0&0&0&0\\     0&1&0&0&0&0\\     0&0&1&0&0&0\\     0&0&0&2&0&0\\     0&0&0&0&2&0\\     0&0&0&0&0&2 \end{bmatrix} $$ which is obviously different from the identity of $6 \times 6$ matrices so if I compute $[C] \cdot [I]$ using the usual matrix-matrix product I won't get the same $[C]$ . I also wrote a little script to check relations (1) and (2) but wasn't able to find this result with random $4^{th}$ order tensors possessing minor symmetries. What am I missing here ? Thanks a lot for your help and the discussions to come :)","I am currently working on a subject that involves a lot of 4th order tensors computations including double dot product and inverse of fourth order tensors. First the definitions so that we are on the same page. What I call the double dot product is : and for the double dot product between a fourth order tensor and a second order tensor : Using the convention of sommation over repeating indices. What I call the identity of the fourth order tensors is the only tensor such that : it is defined by . What I call the inverse of a fourth order tensor is the inverse with respect to the double dot product, that is, the inverse of is the only tensor such that . The double dot product is easy to compute if you don't think about the efficiency of the code, just create an array and loop over the four indices. Computing the inverse is something else. Every tensor I use has the minor symmetries so I thought I would use the Mandel representation for second order and fourth order tensors mentioned on Wikipedia. The fourth order tensor can be put into a matrix with the following components : is a fourth order tensor with minor symmetries and is its Mandel representation. The reason why Mandel's representation exists according to different sources is such that the matrix-matrix and matrix-vector usual products coincide with the fourth order tensors double dot product and the inverse in each respective space (fourth order tensors and matrices) coincides as well, that is and where is the usual matrix-matrix product. But it doesn't work or at least there must be something I don't understand. If I put the identity 4th order tensor defined above into Mandel's notation, I get the following matrix : which is obviously different from the identity of matrices so if I compute using the usual matrix-matrix product I won't get the same . I also wrote a little script to check relations (1) and (2) but wasn't able to find this result with random order tensors possessing minor symmetries. What am I missing here ? Thanks a lot for your help and the discussions to come :)"," (A:B)_{ijkl} = A_{ijmn}B_{mnkl}   (A:s)_{ij} = A_{ijkl}s_{kl}  A:I = I:A = A   I = \delta_{ik}\delta_{jl} e_{i} \otimes e_{j} \otimes e_{k} \otimes e_{l}  A B AB = BA = I A_{ijkl} = A_{jikl} = A_{ijlk} 6 \times6  [C] =
\begin{bmatrix}
    C_{1111}  & C_{1122} & C_{1133} & \sqrt{2}C_{1123} & \sqrt{2}C_{1131} & \sqrt{2}C_{1112}\\
    C_{2211} & C_{2222} & C_{2233} & \sqrt{2}C_{2223} & \sqrt{2}C_{2231} & \sqrt{2}C_{2212}\\
    C_{3311} & C_{3322} & C_{3333} & \sqrt{2}C_{3323} & \sqrt{2}C_{3331} & \sqrt{2}C_{3312}\\
    \sqrt{2}C_{2311} & \sqrt{2}C_{2322} & \sqrt{2}C_{2333} & 2C_{2323} & 2C_{2331} & 2C_{2312}\\
    \sqrt{2}C_{3111} & \sqrt{2}C_{3122} & \sqrt{2}C_{3133} & 2C_{3123} & 2C_{3131} & 2C_{3112}\\
    \sqrt{2}C_{1211} & \sqrt{2}C_{1222} & \sqrt{2}C_{1233} & 2C_{1223} &2C_{1231} & 2C_{1212}
\end{bmatrix}
 C [C] 6\times 6 
[A:B] = [A] \cdot [B] \qquad \qquad (1)
 
[A^{-1}] = [A]^{-1} \qquad \qquad (2)
  \cdot   [I] =
\begin{bmatrix}
    1&0&0&0&0&0\\
    0&1&0&0&0&0\\
    0&0&1&0&0&0\\
    0&0&0&2&0&0\\
    0&0&0&0&2&0\\
    0&0&0&0&0&2
\end{bmatrix}
 6 \times 6 [C] \cdot [I] [C] 4^{th}","['linear-algebra', 'abstract-algebra', 'matrices', 'tensor-products', 'tensors']"
93,Trace of an inverse inequality $\text{Tr}(A^{-1}) \ge n^2 \text{Tr}(A)^{-1}$,Trace of an inverse inequality,\text{Tr}(A^{-1}) \ge n^2 \text{Tr}(A)^{-1},"Let $A \in \mathbb{R}^n$ be a positive definite matrix. Then, it is well known that $$ \mbox{Tr} \left( A^{-1} \right) \ge n^2 \, \mbox{Tr}(A)^{-1} $$ The proof follows by using the fact that trace is and a sum of eigenvalues and using AM-GM inequality. My question: Does this inequality hold with equality iff and only if $A$ is a diagonal matrix? I know also that this inequality holds with equality iff eigenvalues of $A$ are identical.   But not sure of this implies that $A$ is a diagonal matrix.","Let be a positive definite matrix. Then, it is well known that The proof follows by using the fact that trace is and a sum of eigenvalues and using AM-GM inequality. My question: Does this inequality hold with equality iff and only if is a diagonal matrix? I know also that this inequality holds with equality iff eigenvalues of are identical.   But not sure of this implies that is a diagonal matrix.","A \in \mathbb{R}^n  \mbox{Tr} \left( A^{-1} \right) \ge n^2 \, \mbox{Tr}(A)^{-1}  A A A","['linear-algebra', 'matrices', 'trace', 'positive-definite']"
94,Axioms of Affine Space,Axioms of Affine Space,,"In every definition of an affine space I see, the affine space is defined as a set $A$ with an associated vector space $V$ with a group action of $V$ on $A$. But I also see that vector spaces are often identified as ""affine spaces with an origin"". This makes me think (/hope) that we there should be some equivalent definition of an affine space that doesn't rely on the concept of a vector space.  Are there some axioms of an $n$-dimensional affine space (analogous to the ones for a vector space) that make no reference to vector spaces?  If so, would we then be able to show that an affine space equipped with an origin satisfies all of the vector space axioms?","In every definition of an affine space I see, the affine space is defined as a set $A$ with an associated vector space $V$ with a group action of $V$ on $A$. But I also see that vector spaces are often identified as ""affine spaces with an origin"". This makes me think (/hope) that we there should be some equivalent definition of an affine space that doesn't rely on the concept of a vector space.  Are there some axioms of an $n$-dimensional affine space (analogous to the ones for a vector space) that make no reference to vector spaces?  If so, would we then be able to show that an affine space equipped with an origin satisfies all of the vector space axioms?",,"['linear-algebra', 'axioms', 'affine-geometry']"
95,How to take the derivative of a matrix with respect to itself?,How to take the derivative of a matrix with respect to itself?,,Could someone please explain how to take the derivative of matrix with respect to itself? $$\frac{\partial \textbf{X}}{\partial \textbf{X}}$$ where $\textbf{X}$ is an M x N matrix,Could someone please explain how to take the derivative of matrix with respect to itself? $$\frac{\partial \textbf{X}}{\partial \textbf{X}}$$ where $\textbf{X}$ is an M x N matrix,,"['linear-algebra', 'derivatives', 'matrix-calculus']"
96,Why is a graph Laplacian matrix positive semidefinite?,Why is a graph Laplacian matrix positive semidefinite?,,Why is a graph's Laplacian matrix positive semidefinite? Can anyone provide an intuitive explanation and a proof?,Why is a graph's Laplacian matrix positive semidefinite? Can anyone provide an intuitive explanation and a proof?,,"['linear-algebra', 'matrices', 'graph-theory', 'algebraic-graph-theory', 'graph-laplacian']"
97,Is the derivative of the characteristic polynomial equal to the sum of characteristic polynomial of principal submatrices?,Is the derivative of the characteristic polynomial equal to the sum of characteristic polynomial of principal submatrices?,,"Let $A$ by an $n \times n$ matrix over the complex numbers and let $\phi(A,x) = \det(xI-A)$ be the characteristic polynomial of $A$.  Let $B_i$ be the principal submatrix of $A$ formed by deleting the $i$-th row and column of $A$. Let $\phi(B_i,x)$ be the characteristic polynomial of $B_i$. Is it always true that the derivative of $\phi(A,x)$ is the sum of $\phi(B_i,x)$ as $i$ ranges across the rows/columns of $A$? That is, is the following an identity ? $$ \frac{d\ \phi(A,x)}{dx} = \sum_i \phi(B_i,x) $$ If not, what is a small counterexample?  (The equation works for all $2 \times 2$ matrices and all adjacency matrices of graphs....)","Let $A$ by an $n \times n$ matrix over the complex numbers and let $\phi(A,x) = \det(xI-A)$ be the characteristic polynomial of $A$.  Let $B_i$ be the principal submatrix of $A$ formed by deleting the $i$-th row and column of $A$. Let $\phi(B_i,x)$ be the characteristic polynomial of $B_i$. Is it always true that the derivative of $\phi(A,x)$ is the sum of $\phi(B_i,x)$ as $i$ ranges across the rows/columns of $A$? That is, is the following an identity ? $$ \frac{d\ \phi(A,x)}{dx} = \sum_i \phi(B_i,x) $$ If not, what is a small counterexample?  (The equation works for all $2 \times 2$ matrices and all adjacency matrices of graphs....)",,"['linear-algebra', 'matrices', 'graph-theory']"
98,the difference between statistically independent and linearly independent?,the difference between statistically independent and linearly independent?,,what is the difference between statistically independent and linearly independent concepts?,what is the difference between statistically independent and linearly independent concepts?,,"['linear-algebra', 'statistics']"
99,Probability every hyperplane contains at most $m/2$ vectors,Probability every hyperplane contains at most  vectors,m/2,"I pick $m \geq n$ vectors drawn uniformly from  $\{-1,1\}^n$, and call the set of vectors  $X$.  What is the probability that for every non-zero $v \in \mathbb{R}^n$ there exist at least $m/2$ vectors in $X$ which are not orthogonal to $v$? If an exact probability is not possible, can bounds be found?","I pick $m \geq n$ vectors drawn uniformly from  $\{-1,1\}^n$, and call the set of vectors  $X$.  What is the probability that for every non-zero $v \in \mathbb{R}^n$ there exist at least $m/2$ vectors in $X$ which are not orthogonal to $v$? If an exact probability is not possible, can bounds be found?",,"['linear-algebra', 'probability']"
