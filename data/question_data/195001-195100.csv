,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Second derivative of $\arctan(x^2)$,Second derivative of,\arctan(x^2),"Given that $y=\arctan(x^2)$ find $\ \dfrac{d^2y}{dx^2}$. I got $$\frac{dy}{dx}=\frac{2x}{1+x^4}.$$ Using low d high minus high d low over low squared , I got $$\frac{d^2y}{dx^2}=\frac{(1+x)^4 \cdot 2 - 2x \cdot 4(1+x)^3}{(1+x^4)^2}.$$ I tried to simplify this but didn't get the answer which is $$\frac{d^2y}{dx^2}=\frac{2(1-3x^4)}{(1+x^4)^2}.$$ Where am I going wrong?","Given that $y=\arctan(x^2)$ find $\ \dfrac{d^2y}{dx^2}$. I got $$\frac{dy}{dx}=\frac{2x}{1+x^4}.$$ Using low d high minus high d low over low squared , I got $$\frac{d^2y}{dx^2}=\frac{(1+x)^4 \cdot 2 - 2x \cdot 4(1+x)^3}{(1+x^4)^2}.$$ I tried to simplify this but didn't get the answer which is $$\frac{d^2y}{dx^2}=\frac{2(1-3x^4)}{(1+x^4)^2}.$$ Where am I going wrong?",,['derivatives']
1,Polynomial interpolation using derivatives at some points,Polynomial interpolation using derivatives at some points,,"Given $(x_1, y_1), (x_2, y_2), (x_3, y_3), (x_4, y_4), (x_5, y_5)$, we can interpolate a polynomial of degree 4 using Lagrange method. But, when we are given $(x_1, y_1), (x_2, y_2), (x_3, y_3), (x_4, y_4), (x_5, y'''_5)$, how can we interpolate the same degree-4 polynomial? Will Birkhoff interpolation be a good method to solve this or some modified version of Lagrange can be used?","Given $(x_1, y_1), (x_2, y_2), (x_3, y_3), (x_4, y_4), (x_5, y_5)$, we can interpolate a polynomial of degree 4 using Lagrange method. But, when we are given $(x_1, y_1), (x_2, y_2), (x_3, y_3), (x_4, y_4), (x_5, y'''_5)$, how can we interpolate the same degree-4 polynomial? Will Birkhoff interpolation be a good method to solve this or some modified version of Lagrange can be used?",,"['polynomials', 'derivatives', 'interpolation']"
2,Functions that cannot be differentiated in terms of elementary functions,Functions that cannot be differentiated in terms of elementary functions,,"A while ago, I learned how to take the derivative of $y=x^x$ using implicit differentiation, and I wondered if the same trick would work on every function of this type. I tried to differentiate $y=x^{x^x}$ the same way: $\ln y=x^x\ln x$ $\ln (\ln y)=x \ln x+\ln (\ln x)$ $\frac{\frac{dy}{dx}}{y\ln y}=1+\ln x+\frac{1}{x\ln x}$ $\frac{dy}{dx}=x^{x^x+x}\ln x(1+\ln x+\frac{1}{x\ln x})$ This result seems to imply that all functions of this type, no matter how complicated, could be differentiated in this manner. My question is: Are there any functions of this sort that are so complicated that they are impossible to differentiate? If not, can this be proven?","A while ago, I learned how to take the derivative of $y=x^x$ using implicit differentiation, and I wondered if the same trick would work on every function of this type. I tried to differentiate $y=x^{x^x}$ the same way: $\ln y=x^x\ln x$ $\ln (\ln y)=x \ln x+\ln (\ln x)$ $\frac{\frac{dy}{dx}}{y\ln y}=1+\ln x+\frac{1}{x\ln x}$ $\frac{dy}{dx}=x^{x^x+x}\ln x(1+\ln x+\frac{1}{x\ln x})$ This result seems to imply that all functions of this type, no matter how complicated, could be differentiated in this manner. My question is: Are there any functions of this sort that are so complicated that they are impossible to differentiate? If not, can this be proven?",,"['calculus', 'derivatives', 'implicit-differentiation']"
3,Differentiating $\left[\frac{\mathrm{d}^{n}}{\mathrm{d}x^{n}}(x^{2}-1)^{n}\right]^{2}$ [closed],Differentiating  [closed],\left[\frac{\mathrm{d}^{n}}{\mathrm{d}x^{n}}(x^{2}-1)^{n}\right]^{2},Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 10 years ago . Improve this question Given that $$\frac{\mathrm{d}^{2n}}{\mathrm{d}x^{2n}}(x^{2}-1)^{n} = (2n)!$$ How can we find $$\left[\frac{\mathrm{d}^{n}}{\mathrm{d}x^{n}}(x^{2}-1)^{n}\right]^{2}\quad ?$$,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 10 years ago . Improve this question Given that $$\frac{\mathrm{d}^{2n}}{\mathrm{d}x^{2n}}(x^{2}-1)^{n} = (2n)!$$ How can we find $$\left[\frac{\mathrm{d}^{n}}{\mathrm{d}x^{n}}(x^{2}-1)^{n}\right]^{2}\quad ?$$,,['derivatives']
4,Difficulty in proving this inequality,Difficulty in proving this inequality,,"Let $f \in C^{(n)}(-1,1)$ and $\sup_{-1 <x< 1}|f(x)|\leq 1$. Let  $m_k(I) = \inf_{x \in I} |f^{(k)}(x)|$, where $I$ is an interval contained in $(-1,1)$. If $I$ is partitioned into three successive intervals $I_1,I_2,$ and $I_3$ and $\mu$ is the length of $I_2$, then $$m_k(I) \leq \frac 1\mu\left(m_{k-1}(I_1)+m_{k-1}(I_3)\right)$$ Things I have tried do not seem to work and I am stuck. I would be grateful for some help. Edit:  Second Part: if $I$ has length $\lambda$, then $$m_k(I) \leq \frac{2^{k(k+1)/2}k^k}{\lambda^k}.$$ It is suggested that induction and part a) be used. For $k=1$ the assertion holds because of the mean value theorem and the supremem on the  values of the $f$ on  $(-1,1)$.","Let $f \in C^{(n)}(-1,1)$ and $\sup_{-1 <x< 1}|f(x)|\leq 1$. Let  $m_k(I) = \inf_{x \in I} |f^{(k)}(x)|$, where $I$ is an interval contained in $(-1,1)$. If $I$ is partitioned into three successive intervals $I_1,I_2,$ and $I_3$ and $\mu$ is the length of $I_2$, then $$m_k(I) \leq \frac 1\mu\left(m_{k-1}(I_1)+m_{k-1}(I_3)\right)$$ Things I have tried do not seem to work and I am stuck. I would be grateful for some help. Edit:  Second Part: if $I$ has length $\lambda$, then $$m_k(I) \leq \frac{2^{k(k+1)/2}k^k}{\lambda^k}.$$ It is suggested that induction and part a) be used. For $k=1$ the assertion holds because of the mean value theorem and the supremem on the  values of the $f$ on  $(-1,1)$.",,"['analysis', 'derivatives']"
5,How to find the second derivative?,How to find the second derivative?,,"I use this article from Wikipedia to build it in my program. How to find the second derivative in $(x_i, y_i)$ point of this cubic interpolation, if I know other $(x_j, y_j)$ points?","I use this article from Wikipedia to build it in my program. How to find the second derivative in $(x_i, y_i)$ point of this cubic interpolation, if I know other $(x_j, y_j)$ points?",,"['derivatives', 'interpolation', 'cubics']"
6,"Showing that if derivative is 0, function is constant ($f: U \rightarrow \mathbb{R}$ where $U \subset \mathbb{R}^n$)","Showing that if derivative is 0, function is constant ( where )",f: U \rightarrow \mathbb{R} U \subset \mathbb{R}^n,"Here's the question: Suppose that $f: U \rightarrow \mathbb{R}$ is differentiable on the open subset $U\subset \mathbb{R}^n$, and $Df(x) =0$ for all $x\in U$. Show that $f$ is constant on $U$. My thoughts:  First, I think it requires that the region is convex. Or path-connected. But anyways, beyond that I'm a bit stuck. MVT can't be applied to this case for obvious reasons.","Here's the question: Suppose that $f: U \rightarrow \mathbb{R}$ is differentiable on the open subset $U\subset \mathbb{R}^n$, and $Df(x) =0$ for all $x\in U$. Show that $f$ is constant on $U$. My thoughts:  First, I think it requires that the region is convex. Or path-connected. But anyways, beyond that I'm a bit stuck. MVT can't be applied to this case for obvious reasons.",,"['calculus', 'real-analysis', 'derivatives', 'proof-writing']"
7,Derivative of $\; y={(1+e^x)}^{0.5}\; $ using the definition of the derivative,Derivative of  using the definition of the derivative,\; y={(1+e^x)}^{0.5}\; ,$$y={(1+e^x)}^{0.5} =f(x)$$ $$\frac{dy}{dx}= \lim_{h\to0}\frac {f(x+h)-f(x)}{h}$$ My attempt I got down to  $$\lim_{h\to0}\frac{(1+e^xe^h)^{0.5}-(1+e^x)^{0.5}}{h}$$ I can't see where to go from here,$$y={(1+e^x)}^{0.5} =f(x)$$ $$\frac{dy}{dx}= \lim_{h\to0}\frac {f(x+h)-f(x)}{h}$$ My attempt I got down to  $$\lim_{h\to0}\frac{(1+e^xe^h)^{0.5}-(1+e^x)^{0.5}}{h}$$ I can't see where to go from here,,"['limits', 'derivatives']"
8,Isn't partial differentiation implied by function context?,Isn't partial differentiation implied by function context?,,"This has been bothering me for some time, so I thought I'd finally ask it here. If we are given a function, say, $$f(x,y)=x^2+y^2,$$ and are asked to differentiate it w.r.t. $x$, i.e. $$\frac{\partial}{\partial x} f(x,y),$$ then surely this is the same as saying $$\frac{d}{dx} f(x,y),$$ since we will know by the context that obviously $$\frac{d}{dx} y^2 = 0,$$ and therefore $$\frac{\partial}{\partial x} f(x,y)=\frac{d}{dx} f(x,y)=2x.$$ Or am I missing something?","This has been bothering me for some time, so I thought I'd finally ask it here. If we are given a function, say, $$f(x,y)=x^2+y^2,$$ and are asked to differentiate it w.r.t. $x$, i.e. $$\frac{\partial}{\partial x} f(x,y),$$ then surely this is the same as saying $$\frac{d}{dx} f(x,y),$$ since we will know by the context that obviously $$\frac{d}{dx} y^2 = 0,$$ and therefore $$\frac{\partial}{\partial x} f(x,y)=\frac{d}{dx} f(x,y)=2x.$$ Or am I missing something?",,"['derivatives', 'partial-derivative']"
9,How exactly do I 'see' the function I need to make for optimization?,How exactly do I 'see' the function I need to make for optimization?,,"Optimization problems in Calculus seems to be my white whale. I always seem to struggle with it. I know that once I find the function I need to manipulate with it's pretty much smooth sailing from there. find f'(x), Set to zero, solve, plug back into f(x). But I can't seem to know how to come up with the original function to create. I can write about an optimization problem concerning a bridge I'm currently working on, but I fear having answers simply given to me. (Given answers are no help at all.) But how can you take information given from the problem to make your function? The more simplified optimization problems dealing with cut boxes or the relationship of two numbers are easy enough but when I get to higher forms of thought like sales or said bridge problem I get lost.","Optimization problems in Calculus seems to be my white whale. I always seem to struggle with it. I know that once I find the function I need to manipulate with it's pretty much smooth sailing from there. find f'(x), Set to zero, solve, plug back into f(x). But I can't seem to know how to come up with the original function to create. I can write about an optimization problem concerning a bridge I'm currently working on, but I fear having answers simply given to me. (Given answers are no help at all.) But how can you take information given from the problem to make your function? The more simplified optimization problems dealing with cut boxes or the relationship of two numbers are easy enough but when I get to higher forms of thought like sales or said bridge problem I get lost.",,"['calculus', 'derivatives', 'optimization']"
10,Strictly decreasing function with a horizontal asymptote is convex?,Strictly decreasing function with a horizontal asymptote is convex?,,"Suppose $f$ is a strictly decreasing function with a horizontal asymptote at $t \rightarrow + \infty$. Hence, there exists a $t_{0}$ such that $\forall t>t_{0}, ~f(t)$ is a convex fuction. Is this the case?","Suppose $f$ is a strictly decreasing function with a horizontal asymptote at $t \rightarrow + \infty$. Hence, there exists a $t_{0}$ such that $\forall t>t_{0}, ~f(t)$ is a convex fuction. Is this the case?",,"['real-analysis', 'derivatives', 'convex-analysis']"
11,Prove that $\lim_{n\to \infty} {f(x_n)-f(x_0)\over x_n - x_0}= f´(x_0)$,Prove that,\lim_{n\to \infty} {f(x_n)-f(x_0)\over x_n - x_0}= f´(x_0),"Problem: Prove that if $f$ is continuous at $x_0$ and$$\lim_{n\to \infty} {f(x_n)-f(x_0)\over x_n - x_0}$$ exist for any sequence ${x_n} \to x_0$ and $x_n\neq x_0$   $\forall n\in \mathbb N$, then  $f´(x_0)$ exists and $$\lim_{n\to \infty} {f(x_n)-f(x_0)\over x_n - x_0}= f´(x_0).$$ I started the proof like this: Let $\epsilon\gt 0$. Then there exists $N$ such that $\forall n\gt N$, $$\left|{f(x_n)-f(x_0)\over x_n - x_0}-L\right|\lt\epsilon.$$ By hypothesis $f$ is continuous at $x_0$, so there exist $\delta\gt 0$ such that $$\left|x-x_0\right|\lt\delta\Rightarrow \left|f(x)-f(x_0)\right|\lt\epsilon.$$ The elements in the sequence ${x_n}$ are in the neighborhood of radius $\delta$ for some $n\gt N$ but this was an arbitrary  sequence so I don´t know if this implies that $f´(x_0)$ exist. I would really appreciate your help with this problem thank you.","Problem: Prove that if $f$ is continuous at $x_0$ and$$\lim_{n\to \infty} {f(x_n)-f(x_0)\over x_n - x_0}$$ exist for any sequence ${x_n} \to x_0$ and $x_n\neq x_0$   $\forall n\in \mathbb N$, then  $f´(x_0)$ exists and $$\lim_{n\to \infty} {f(x_n)-f(x_0)\over x_n - x_0}= f´(x_0).$$ I started the proof like this: Let $\epsilon\gt 0$. Then there exists $N$ such that $\forall n\gt N$, $$\left|{f(x_n)-f(x_0)\over x_n - x_0}-L\right|\lt\epsilon.$$ By hypothesis $f$ is continuous at $x_0$, so there exist $\delta\gt 0$ such that $$\left|x-x_0\right|\lt\delta\Rightarrow \left|f(x)-f(x_0)\right|\lt\epsilon.$$ The elements in the sequence ${x_n}$ are in the neighborhood of radius $\delta$ for some $n\gt N$ but this was an arbitrary  sequence so I don´t know if this implies that $f´(x_0)$ exist. I would really appreciate your help with this problem thank you.",,"['calculus', 'sequences-and-series', 'derivatives']"
12,Does the little-oh relation remain if $f(x)$ and $g(x)$ both integrate or differentiate?,Does the little-oh relation remain if  and  both integrate or differentiate?,f(x) g(x),"Give two functions $f$ and $g$ with derivatives in some interval containing 0,where $g$ is positive.Assume also $f(x)=o(g(x))$ as $x \to 0$. Prove or disprove each of the following statements: (a) $\int^x_0 f(t)dt = o(\int^x_0g(t)dt)$ as $x \to 0$ (b) $f'(x)=o(g'(x))$ as $x \to 0$ I use the definition to prove (a)  $$\begin{align}  \lim_{x \to 0} \frac{\int^x_0 f(t)dt}{\int^x_0 g(t)dt} &= \lim_{x \to 0} \frac{\int^x_0 \frac{f(t)}{g(t)}g(t)dt}{\int^x_0 g(t)dt} \\ &=  \lim_{x \to 0}   \frac {f(c)}{g(c)} \frac{\int^x_0 g(t)dt}{\int^x_0 g(t)dt} \;\;\;\text{(c is between 0 and x)}\\ &= \lim_{x \to 0} \frac {f(c)}{g(c)}  \end{align}$$ $c \to 0$ as $x \to 0$ ,so the limit is $0$. I tried to prove (b) by using $k(x)=\frac{f(x)}{g(x)}$ and $f'(x)=g'(x)k(x)+k'(x)g(x)$ then$$\begin{align} \lim_{x \to 0} \frac{f'(x)}{g'(x)} &= \lim_{x \to 0} \frac{g'(x)k(x)+k'(x)g(x)}{g'(x)} \\ &= \lim_{x \to 0} (k(x) + \frac{ k'(x)g(x)}{g'(x)})\end{align}$$  I am not able to deal with second term.It seems I'm not doing in a right way. Any help is appreciated.","Give two functions $f$ and $g$ with derivatives in some interval containing 0,where $g$ is positive.Assume also $f(x)=o(g(x))$ as $x \to 0$. Prove or disprove each of the following statements: (a) $\int^x_0 f(t)dt = o(\int^x_0g(t)dt)$ as $x \to 0$ (b) $f'(x)=o(g'(x))$ as $x \to 0$ I use the definition to prove (a)  $$\begin{align}  \lim_{x \to 0} \frac{\int^x_0 f(t)dt}{\int^x_0 g(t)dt} &= \lim_{x \to 0} \frac{\int^x_0 \frac{f(t)}{g(t)}g(t)dt}{\int^x_0 g(t)dt} \\ &=  \lim_{x \to 0}   \frac {f(c)}{g(c)} \frac{\int^x_0 g(t)dt}{\int^x_0 g(t)dt} \;\;\;\text{(c is between 0 and x)}\\ &= \lim_{x \to 0} \frac {f(c)}{g(c)}  \end{align}$$ $c \to 0$ as $x \to 0$ ,so the limit is $0$. I tried to prove (b) by using $k(x)=\frac{f(x)}{g(x)}$ and $f'(x)=g'(x)k(x)+k'(x)g(x)$ then$$\begin{align} \lim_{x \to 0} \frac{f'(x)}{g'(x)} &= \lim_{x \to 0} \frac{g'(x)k(x)+k'(x)g(x)}{g'(x)} \\ &= \lim_{x \to 0} (k(x) + \frac{ k'(x)g(x)}{g'(x)})\end{align}$$  I am not able to deal with second term.It seems I'm not doing in a right way. Any help is appreciated.",,"['calculus', 'integration', 'limits', 'derivatives']"
13,Implicitly differentiate $e^y \cos(x) = 1 + \sin(xy)$,Implicitly differentiate,e^y \cos(x) = 1 + \sin(xy),"I can differentiate one side of the equation, but I dont know how to deal with sin(xy)","I can differentiate one side of the equation, but I dont know how to deal with sin(xy)",,"['calculus', 'derivatives', 'implicit-differentiation']"
14,Max perimeter of triangle inscribed in a circle,Max perimeter of triangle inscribed in a circle,,What is the maximum perimeter of a triangle inscibed in a circle of radius $1$? I can't seem to find a proper equation to calculate the derivative.,What is the maximum perimeter of a triangle inscibed in a circle of radius $1$? I can't seem to find a proper equation to calculate the derivative.,,"['geometry', 'derivatives', 'optimization']"
15,Getting stonewalled on computation of $2\times 2$ Hessian matrix,Getting stonewalled on computation of  Hessian matrix,2\times 2,"The question: Let $z \in R^N$, and let $f(z) = \log[1^T z] \in R$. I am told that the Hessian matrix of this function is the following: $$ H = \frac{1}{1^Tz}\Big[ 1^Tz \mathrm{diag}(z) - zz^T  \Big] $$ I am not sure how this was attained. EDIT: This is the clip from my book. Here is the function: ...and here is the Hessian my books claims: What I tried: To keep things simple, I simply decided to do this 'by hand' for a 2x2 case. So first I write the function as $f(z_1,z_2) = \log(\frac{1}{z_1 + z_2})$. When I take the first and second partial derivatives, I will get $-\frac{1}{(z_1 + z_2)^2}$ for all (second order) partial combinations. So the Hessian will just be a matrix of that for all 4 of its entries. I am not seeing how what I did for a simple $2 \times 2$ case corresponds to what the answer should be. Furthermore I would like to do the derivation using vector/matrix derivatives but not sure how. Thanks.","The question: Let $z \in R^N$, and let $f(z) = \log[1^T z] \in R$. I am told that the Hessian matrix of this function is the following: $$ H = \frac{1}{1^Tz}\Big[ 1^Tz \mathrm{diag}(z) - zz^T  \Big] $$ I am not sure how this was attained. EDIT: This is the clip from my book. Here is the function: ...and here is the Hessian my books claims: What I tried: To keep things simple, I simply decided to do this 'by hand' for a 2x2 case. So first I write the function as $f(z_1,z_2) = \log(\frac{1}{z_1 + z_2})$. When I take the first and second partial derivatives, I will get $-\frac{1}{(z_1 + z_2)^2}$ for all (second order) partial combinations. So the Hessian will just be a matrix of that for all 4 of its entries. I am not seeing how what I did for a simple $2 \times 2$ case corresponds to what the answer should be. Furthermore I would like to do the derivation using vector/matrix derivatives but not sure how. Thanks.",,"['matrices', 'derivatives', 'vector-spaces', 'partial-derivative']"
16,Derivative of remainder function.,Derivative of remainder function.,,"I cannot find a derivative of remainder function (i.e. derivative of a(x) mod b(x) with respect to x , and x is a real number and a() , b() is also real-valued functions) in tables of derivatives. Without the loss of generality, we may assume (and it desired at all), that continuous approximation is acceptable. I note, that (a(x) mod const)' ~ a'(x) , (c(x) mod c(x))' = 0 , but still I can't conclude form of desired right hand side (a(x) mod b(x))' -> ? function from these borderline cases (maybe dimensional method or method of indefinite coefficients in some form is applicable, but I have not an intuitions of how). What is the generalized (in sense of continuity) form of remainder derivative? I mean a remainder function as presented on x86/x86-64 architectures ( FPREM and FPREM1 ).","I cannot find a derivative of remainder function (i.e. derivative of a(x) mod b(x) with respect to x , and x is a real number and a() , b() is also real-valued functions) in tables of derivatives. Without the loss of generality, we may assume (and it desired at all), that continuous approximation is acceptable. I note, that (a(x) mod const)' ~ a'(x) , (c(x) mod c(x))' = 0 , but still I can't conclude form of desired right hand side (a(x) mod b(x))' -> ? function from these borderline cases (maybe dimensional method or method of indefinite coefficients in some form is applicable, but I have not an intuitions of how). What is the generalized (in sense of continuity) form of remainder derivative? I mean a remainder function as presented on x86/x86-64 architectures ( FPREM and FPREM1 ).",,['derivatives']
17,"Find a function f(x,y) such that the gradient is","Find a function f(x,y) such that the gradient is",,"Problem: Find a function $f(x,y)$ such that $ \nabla f = <y,x>$ My work: $\dfrac {\partial f}{\partial x} = y$ $\dfrac {\partial f}{\partial y} = x$ $f(x,y) =  \displaystyle\int_{ }^{ } \dfrac{\partial f}{\partial x} dx +  \int_{ }^{ } \dfrac {\partial f}{\partial y} dy = yx + xy = 2xy$ $\dfrac {\partial (2xy)}{\partial x}  = 2x$ $\dfrac {\partial  (2xy)}{\partial y}  = 2y$ Then shouldn't the function be $xy$?","Problem: Find a function $f(x,y)$ such that $ \nabla f = <y,x>$ My work: $\dfrac {\partial f}{\partial x} = y$ $\dfrac {\partial f}{\partial y} = x$ $f(x,y) =  \displaystyle\int_{ }^{ } \dfrac{\partial f}{\partial x} dx +  \int_{ }^{ } \dfrac {\partial f}{\partial y} dy = yx + xy = 2xy$ $\dfrac {\partial (2xy)}{\partial x}  = 2x$ $\dfrac {\partial  (2xy)}{\partial y}  = 2y$ Then shouldn't the function be $xy$?",,['derivatives']
18,Finding the derivative of a definite integral,Finding the derivative of a definite integral,,"$$ G(x)=\int_1^{x^2}(x-t)\sin^2(t)dt  $$ Find $ G'(x) $ given $G(x)$. Normally I can solve these types of problems, but I'm thrown off by the two variables present, both $x$ and $t$ under the integral.","$$ G(x)=\int_1^{x^2}(x-t)\sin^2(t)dt  $$ Find $ G'(x) $ given $G(x)$. Normally I can solve these types of problems, but I'm thrown off by the two variables present, both $x$ and $t$ under the integral.",,"['calculus', 'integration', 'derivatives']"
19,general formulation for 1/g(x) derivative,general formulation for 1/g(x) derivative,,Is there a general formulation for $\frac {d^n(g(x)^{-1})}{dx^n}$ ? Something like $$\frac {d^n(g(x)^{-1})}{dx^n} = \sum_{i=1}^{f(n)}\prod_{k=1}^{h(n)} ...$$,Is there a general formulation for $\frac {d^n(g(x)^{-1})}{dx^n}$ ? Something like $$\frac {d^n(g(x)^{-1})}{dx^n} = \sum_{i=1}^{f(n)}\prod_{k=1}^{h(n)} ...$$,,['derivatives']
20,Showing that this function is infinitely differentiable,Showing that this function is infinitely differentiable,,"Show, that the function $$ \mathcal E: \mathbb R \to \mathbb R:  x \mapsto \begin{cases}  \exp(-\frac{1}{x^2}), & \text{if x $\neq$ 0}, \\  0, & \text{otherwise},  \end{cases} $$ is infinitely differentiable and that $\frac{d^k\mathcal E}{dx}(0) = 0$ for all $k \in \mathbb N$. We just introduced differentiation, so the solution should not contain very advanced techniques to solve this. We also got the tip it should be done by induction. @fgp I found that it is $$f^{(n)}(x) = P_n \left(\frac1x\right)e^{-\frac1{x^2}}$$ where $P_n$ is a polynomial with integer coefficients. I could also do the initial step for the induction: For $n=1:$ $$f'(x) = \frac2{x^2}e^{-\frac1{x^2}} = P_1 \left(\frac1t\right) e^{-\frac1{x^2}}$$ where $P_1(x)=2x^2.$ I am stuck at the induction step: $f^{(n+1)}(x) = $.....","Show, that the function $$ \mathcal E: \mathbb R \to \mathbb R:  x \mapsto \begin{cases}  \exp(-\frac{1}{x^2}), & \text{if x $\neq$ 0}, \\  0, & \text{otherwise},  \end{cases} $$ is infinitely differentiable and that $\frac{d^k\mathcal E}{dx}(0) = 0$ for all $k \in \mathbb N$. We just introduced differentiation, so the solution should not contain very advanced techniques to solve this. We also got the tip it should be done by induction. @fgp I found that it is $$f^{(n)}(x) = P_n \left(\frac1x\right)e^{-\frac1{x^2}}$$ where $P_n$ is a polynomial with integer coefficients. I could also do the initial step for the induction: For $n=1:$ $$f'(x) = \frac2{x^2}e^{-\frac1{x^2}} = P_1 \left(\frac1t\right) e^{-\frac1{x^2}}$$ where $P_1(x)=2x^2.$ I am stuck at the induction step: $f^{(n+1)}(x) = $.....",,"['analysis', 'derivatives']"
21,Intuition behind power rule?,Intuition behind power rule?,,"I've been using it for a while but still don't really understand why it works. For integer exponents greater or equal to 2, its easy to intuitively understand it using the geometric interpretation of the product rule: http://web.mit.edu/wwmath/calculus/differentiation/products.html But could someone give me an intuition about why it works ( not a proof )? An intuition that extends to non-integer and negative powers?","I've been using it for a while but still don't really understand why it works. For integer exponents greater or equal to 2, its easy to intuitively understand it using the geometric interpretation of the product rule: http://web.mit.edu/wwmath/calculus/differentiation/products.html But could someone give me an intuition about why it works ( not a proof )? An intuition that extends to non-integer and negative powers?",,"['calculus', 'derivatives', 'intuition']"
22,Logaritmic derivation!,Logaritmic derivation!,,"Why can't I derive this function using normal methods?Text book says that i have to use something called ""logarithmic derivation"". I don't know if this term exists in English, but that is the direct translation from my language. Apparently you use this method when there is a variable in both the base, and the exponent, but i can't seem to spot this situation in the function that is given to me: $$y=xe^x\arcsin{x}$$","Why can't I derive this function using normal methods?Text book says that i have to use something called ""logarithmic derivation"". I don't know if this term exists in English, but that is the direct translation from my language. Apparently you use this method when there is a variable in both the base, and the exponent, but i can't seem to spot this situation in the function that is given to me: $$y=xe^x\arcsin{x}$$",,['derivatives']
23,Differentiating $ y = e^x \sqrt{x} $,Differentiating, y = e^x \sqrt{x} ,"I need to take the derivative of: $$ y = e^x \sqrt{x} $$ I tried completing this question, and my result was: $$\frac{e^x (2x+1)}{2\sqrt{x}}$$ Let $u = e^x$ and $v = x^{1/2}$, I followed through the rule and got my answer, problem is it's wrong.. Can someone please complete this question and show me how it's done? Thanks in advance. The answer should be in the form: $$ \frac{dy}{dx} = \frac{\square}{\square} + \square $$","I need to take the derivative of: $$ y = e^x \sqrt{x} $$ I tried completing this question, and my result was: $$\frac{e^x (2x+1)}{2\sqrt{x}}$$ Let $u = e^x$ and $v = x^{1/2}$, I followed through the rule and got my answer, problem is it's wrong.. Can someone please complete this question and show me how it's done? Thanks in advance. The answer should be in the form: $$ \frac{dy}{dx} = \frac{\square}{\square} + \square $$",,"['calculus', 'derivatives']"
24,Derivatives of second order,Derivatives of second order,,"Consider a real function $f$ of one variable. Suppose the second order derivative exists. To  find the second order derivative of $f$, I usually derivate $f$ two times. I start with $f$, and derivate to get the function $\frac{d}{dx} f(x)$ of one variable, which I then derivate again. I would succinctly describe this by writing $$\frac{d}{dx} \frac{df}{dx}.$$ But when I consider a real multivariable function which maps elements into $\mathbb{R}^k$, with $k \geq 1$, I do not always know how to find the second order derivative. In the case $k=1$ we can consider the derivative of some function $g$ as the gradient $\nabla g$ of $g$. Then the second order derivative of $g$ would be the hessian matrix $H(g)$ of $g$. But when $k \geq 2$, for example $h(x, y)=(x^2, y^2)$, I do not find it easy to compute the second order derivative, and our coursebook does not discuss this topic. I know that $$[h'(x, y)]=\begin{pmatrix} 2x & 0 \\ 0 & 2y \end{pmatrix}.$$ The difference between finding the derivative of $f$ and $g$ or $h$ is that when derivating $f$ we get a function of the same structure, we still map elements from $\mathbb{R}$ into $\mathbb{R}$,  but when derivating $g$ or $h$ the structure changes and instead of, for example, mapping elements from $\mathbb{R}^2$ into  $\mathbb{R}^2$, we map elements from  $\mathbb{R}^2$ into  $\mathbb{R}^{2 \times 2}$ (the set of 2 by 2 matrices with real elements). To me, it seems that the problem of finding the second order derivative of $g$ is to derivate a matrix in which all elements are real functions. I would be grateful if you could, for example, direct me to any material that would help me solve this general problem. Thanks.","Consider a real function $f$ of one variable. Suppose the second order derivative exists. To  find the second order derivative of $f$, I usually derivate $f$ two times. I start with $f$, and derivate to get the function $\frac{d}{dx} f(x)$ of one variable, which I then derivate again. I would succinctly describe this by writing $$\frac{d}{dx} \frac{df}{dx}.$$ But when I consider a real multivariable function which maps elements into $\mathbb{R}^k$, with $k \geq 1$, I do not always know how to find the second order derivative. In the case $k=1$ we can consider the derivative of some function $g$ as the gradient $\nabla g$ of $g$. Then the second order derivative of $g$ would be the hessian matrix $H(g)$ of $g$. But when $k \geq 2$, for example $h(x, y)=(x^2, y^2)$, I do not find it easy to compute the second order derivative, and our coursebook does not discuss this topic. I know that $$[h'(x, y)]=\begin{pmatrix} 2x & 0 \\ 0 & 2y \end{pmatrix}.$$ The difference between finding the derivative of $f$ and $g$ or $h$ is that when derivating $f$ we get a function of the same structure, we still map elements from $\mathbb{R}$ into $\mathbb{R}$,  but when derivating $g$ or $h$ the structure changes and instead of, for example, mapping elements from $\mathbb{R}^2$ into  $\mathbb{R}^2$, we map elements from  $\mathbb{R}^2$ into  $\mathbb{R}^{2 \times 2}$ (the set of 2 by 2 matrices with real elements). To me, it seems that the problem of finding the second order derivative of $g$ is to derivate a matrix in which all elements are real functions. I would be grateful if you could, for example, direct me to any material that would help me solve this general problem. Thanks.",,['derivatives']
25,Smooth Hinge Loss Lipschitz Constant,Smooth Hinge Loss Lipschitz Constant,,"Given the smooth hinge loss $L_\epsilon$ as follows $L_\epsilon(y_i (w^T x_i + b)) = \begin{cases} 0 & y_i (w^T x_i + b) \\ \frac{(1-y_i (w^T x_i + b))^2}{2 \delta} & 1 - \delta < y_i (w^T x_i + b) \le 1 \\ 1 - y_i (w^T x_i + b) & y_i (w^T x_i + b) \le 1 - \delta \end{cases}$ and the objective function for a SVM, where I want to minimize with respect to $w, b$ and $x_i$ is the $i$th training example with $y_i \in {-1, 1}$, as $f = \frac{C}{N} \sum_{i = 1}^N L_\epsilon(y_i (w^T x_i + b)) + \frac{1}{2} ||w||^2$ I want to compute the Lipschitz constant and the strongly convexity parameter of the above function so I can use the minimization algorithm by Nesterov to compute the optimal solution. These values are given by the largest and smallest eigenvalues of the Hessian, respectively. I have computed the Hessian as follos $\nabla^2 L_\epsilon = \begin{pmatrix} \frac{\partial^2L_\epsilon}{\partial w^2} & \frac{\partial^2L_\epsilon}{\partial w \partial b} \\ \frac{\partial^2L_\epsilon}{\partial w \partial b} & \frac{\partial^2L_\epsilon}{\partial b^2}\end{pmatrix}$ with $\frac{\partial^2L_\epsilon}{\partial w^2} = \begin{cases} 0 & y_i (w^T x_i + b) \\ \frac{y_i^2 x_i x_i^T}{\delta} & 1 - \delta < y_i (w^T x_i + b) \le 1 \\ 0 & y_i (w^T x_i + b) \le 1 - \delta \end{cases}$ $\frac{\partial^2L_\epsilon}{\partial b^2} = \begin{cases} 0 & y_i (w^T x_i + b) \\ \frac{y_i^2}{\delta} & 1 - \delta < y_i (w^T x_i + b) \le 1 \\ 0 & y_i (w^T x_i + b) \le 1 - \delta \end{cases}$ $\frac{\partial^2L_\epsilon}{\partial w \partial b} = \begin{cases} 0 & y_i (w^T x_i + b) \\ \frac{y_i^2 x_i}{\delta} & 1 - \delta < y_i (w^T x_i + b) \le 1 \\ 0 & y_i (w^T x_i + b) \le 1 - \delta \end{cases}$ So the Hessian of $f$ becomes $\nabla^2 f = \frac{C}{N} \nabla^2 L_\epsilon + 1$ But now I'm stuck. Can I, for given training examples $\{x_i\}_{i = 1}^N$ compute the Hessian taking the second case, or do I have something wrong in the Hessian?","Given the smooth hinge loss $L_\epsilon$ as follows $L_\epsilon(y_i (w^T x_i + b)) = \begin{cases} 0 & y_i (w^T x_i + b) \\ \frac{(1-y_i (w^T x_i + b))^2}{2 \delta} & 1 - \delta < y_i (w^T x_i + b) \le 1 \\ 1 - y_i (w^T x_i + b) & y_i (w^T x_i + b) \le 1 - \delta \end{cases}$ and the objective function for a SVM, where I want to minimize with respect to $w, b$ and $x_i$ is the $i$th training example with $y_i \in {-1, 1}$, as $f = \frac{C}{N} \sum_{i = 1}^N L_\epsilon(y_i (w^T x_i + b)) + \frac{1}{2} ||w||^2$ I want to compute the Lipschitz constant and the strongly convexity parameter of the above function so I can use the minimization algorithm by Nesterov to compute the optimal solution. These values are given by the largest and smallest eigenvalues of the Hessian, respectively. I have computed the Hessian as follos $\nabla^2 L_\epsilon = \begin{pmatrix} \frac{\partial^2L_\epsilon}{\partial w^2} & \frac{\partial^2L_\epsilon}{\partial w \partial b} \\ \frac{\partial^2L_\epsilon}{\partial w \partial b} & \frac{\partial^2L_\epsilon}{\partial b^2}\end{pmatrix}$ with $\frac{\partial^2L_\epsilon}{\partial w^2} = \begin{cases} 0 & y_i (w^T x_i + b) \\ \frac{y_i^2 x_i x_i^T}{\delta} & 1 - \delta < y_i (w^T x_i + b) \le 1 \\ 0 & y_i (w^T x_i + b) \le 1 - \delta \end{cases}$ $\frac{\partial^2L_\epsilon}{\partial b^2} = \begin{cases} 0 & y_i (w^T x_i + b) \\ \frac{y_i^2}{\delta} & 1 - \delta < y_i (w^T x_i + b) \le 1 \\ 0 & y_i (w^T x_i + b) \le 1 - \delta \end{cases}$ $\frac{\partial^2L_\epsilon}{\partial w \partial b} = \begin{cases} 0 & y_i (w^T x_i + b) \\ \frac{y_i^2 x_i}{\delta} & 1 - \delta < y_i (w^T x_i + b) \le 1 \\ 0 & y_i (w^T x_i + b) \le 1 - \delta \end{cases}$ So the Hessian of $f$ becomes $\nabla^2 f = \frac{C}{N} \nabla^2 L_\epsilon + 1$ But now I'm stuck. Can I, for given training examples $\{x_i\}_{i = 1}^N$ compute the Hessian taking the second case, or do I have something wrong in the Hessian?",,"['derivatives', 'convex-optimization', 'partial-derivative', 'machine-learning']"
26,"""The first derivative is monotonic increasing""","""The first derivative is monotonic increasing""",,"My professor posted this question as a supplement to our exercises in Rudin. As a disclaimer, this class has been difficult for me in the past, so forgive me if I've missed any really simple steps... Here is the question: ""Let $f:[0, \infty)\to R$ be given, such that $f$ is continuous on $[0,\infty)$. Suppose $f(0)=0$, where $f$ is differentiable on $(0,\infty)$, and $f'$ is monotonic increasing on $(0,\infty)$. Prove that f is supperadditive on $[0,\infty)$ meaning that $f(x+y)\ge f(x)+f(y)$ for all $x, y \in [0,\infty)$."" What does it mean when the first derivative is monotonic increasing? Does that tell me $f''\ge0$ or is that even useful information? Where does one begin on problems like this? If not a solution to the problem, general problem solving advice for real analysis proofs would be much appreciated. - Thanks.","My professor posted this question as a supplement to our exercises in Rudin. As a disclaimer, this class has been difficult for me in the past, so forgive me if I've missed any really simple steps... Here is the question: ""Let $f:[0, \infty)\to R$ be given, such that $f$ is continuous on $[0,\infty)$. Suppose $f(0)=0$, where $f$ is differentiable on $(0,\infty)$, and $f'$ is monotonic increasing on $(0,\infty)$. Prove that f is supperadditive on $[0,\infty)$ meaning that $f(x+y)\ge f(x)+f(y)$ for all $x, y \in [0,\infty)$."" What does it mean when the first derivative is monotonic increasing? Does that tell me $f''\ge0$ or is that even useful information? Where does one begin on problems like this? If not a solution to the problem, general problem solving advice for real analysis proofs would be much appreciated. - Thanks.",,"['real-analysis', 'derivatives']"
27,"$\frac{\frac{dx}{dt}}{\frac{dy}{dt}}$, for $x(t),y(t)$",", for","\frac{\frac{dx}{dt}}{\frac{dy}{dt}} x(t),y(t)","I have a hopefully simple question. I am guessing that this has been asked before but I wasn't sure how to search for it. Suppose we have two functions: $x(t)$, $y(t)$. What is $\frac{dx}{dt}/\frac{dy}{dt}$? And why? Note that this is not the setup for the standard chain rule (or at least its not obvious to me that it is). I talked to some friends and some of them say ""just cancel out the $dt$s."" Obviously, $\frac{dx}{dt}$, for example, is just the notation for a limit and not a ratio of numbers and so there is no reason as such that we should be able to ""just cancel out."" Working with the limits directly, I can get up to: $$ \lim_{\triangle t \to 0} \frac{x(t+ \triangle t)-x(t)}{y(t+\triangle t)-y(t)} $$ Thinking in terms of infinitesimals, this ""feels"" a lot like $dx/dy$ but I don't see any formal reason for why it is. Thanks for your time.","I have a hopefully simple question. I am guessing that this has been asked before but I wasn't sure how to search for it. Suppose we have two functions: $x(t)$, $y(t)$. What is $\frac{dx}{dt}/\frac{dy}{dt}$? And why? Note that this is not the setup for the standard chain rule (or at least its not obvious to me that it is). I talked to some friends and some of them say ""just cancel out the $dt$s."" Obviously, $\frac{dx}{dt}$, for example, is just the notation for a limit and not a ratio of numbers and so there is no reason as such that we should be able to ""just cancel out."" Working with the limits directly, I can get up to: $$ \lim_{\triangle t \to 0} \frac{x(t+ \triangle t)-x(t)}{y(t+\triangle t)-y(t)} $$ Thinking in terms of infinitesimals, this ""feels"" a lot like $dx/dy$ but I don't see any formal reason for why it is. Thanks for your time.",,"['calculus', 'derivatives']"
28,Differentiation of $2\arccos \left(\sqrt{\frac{a-x}{a-b}}\right)$,Differentiation of,2\arccos \left(\sqrt{\frac{a-x}{a-b}}\right),"Okay so the question is: Show that the function $$2\arccos \left(\sqrt{\dfrac{a-x}{a-b}}\right)$$ is equal to $$\frac{1}{\sqrt{(a-x)(x-b)}} .$$ I started by changing the arccosine into inverse cosine, then attempted to apply chain rule but I didn't get very far. Then I tried substituting the derivative for arccosine in and then applying chain rule. Is there another method besides chain rule I should use? Any help is appreciated.","Okay so the question is: Show that the function is equal to I started by changing the arccosine into inverse cosine, then attempted to apply chain rule but I didn't get very far. Then I tried substituting the derivative for arccosine in and then applying chain rule. Is there another method besides chain rule I should use? Any help is appreciated.",2\arccos \left(\sqrt{\dfrac{a-x}{a-b}}\right) \frac{1}{\sqrt{(a-x)(x-b)}} .,"['calculus', 'derivatives']"
29,How to show that $\frac{\partial}{\partial y}\left(\int_{0}^{y}\frac{1}{x+it-2}dt\right)=\frac{1}{x+iy-2}$,How to show that,\frac{\partial}{\partial y}\left(\int_{0}^{y}\frac{1}{x+it-2}dt\right)=\frac{1}{x+iy-2},I'm trying to show that $$\frac{\partial}{\partial y}\left(\int_{0}^{y}\frac{1}{x+it-2}dt\right)=\frac{1}{x+iy-2}$$ In an area that doesn't contain the point $2+0i$. If the function under the integral was a real function then it would have been immediate from the Fundamental Theorem but I haven't studied of an equivalent result for a complex function. I need to somehow show this result through a direct computation of some sort. Edit: I would prefer not to use complex logarithms since I'm not really familiar with the subject but I do know it differs quite a bit from the real logarithm. Help would be appreciated.,I'm trying to show that $$\frac{\partial}{\partial y}\left(\int_{0}^{y}\frac{1}{x+it-2}dt\right)=\frac{1}{x+iy-2}$$ In an area that doesn't contain the point $2+0i$. If the function under the integral was a real function then it would have been immediate from the Fundamental Theorem but I haven't studied of an equivalent result for a complex function. I need to somehow show this result through a direct computation of some sort. Edit: I would prefer not to use complex logarithms since I'm not really familiar with the subject but I do know it differs quite a bit from the real logarithm. Help would be appreciated.,,"['complex-analysis', 'derivatives', 'partial-derivative']"
30,How can rate of change be with respect to time if you're not differentiating with respect to time?,How can rate of change be with respect to time if you're not differentiating with respect to time?,,"A high school Calculus textbook asks: Determine the instantaneous rate of change in the surface area of a spherical balloon (as it is inflated) at the point in time when the radius reaches 10 cm. My solution: The function relating surface area of a sphere to its radius is $A=4\pi r^2$. So, instantaneous rate of change of surface area with respect to radius is $A'=8\pi r$. When $r=10$, $A'=80\pi$. This answer is in agreement with the text, but what's confusing me are the units. The textbook gives the units as $\mathrm{cm}^2$/unit of time. However, I would think that the units would be $\mathrm{cm}^2/\mathrm{cm}$ because I'm finding rate of change of surface area with respect to the radius, not time. On the other hand, it does seem plausible that the rate of change of surface area would depend on time (as in, how quickly the balloon is being blown up). Also, the question uses the phrase ""at the point in time"" This is really confusing me. Is the textbook mistaken, or am I? Any help is appreciated.","A high school Calculus textbook asks: Determine the instantaneous rate of change in the surface area of a spherical balloon (as it is inflated) at the point in time when the radius reaches 10 cm. My solution: The function relating surface area of a sphere to its radius is $A=4\pi r^2$. So, instantaneous rate of change of surface area with respect to radius is $A'=8\pi r$. When $r=10$, $A'=80\pi$. This answer is in agreement with the text, but what's confusing me are the units. The textbook gives the units as $\mathrm{cm}^2$/unit of time. However, I would think that the units would be $\mathrm{cm}^2/\mathrm{cm}$ because I'm finding rate of change of surface area with respect to the radius, not time. On the other hand, it does seem plausible that the rate of change of surface area would depend on time (as in, how quickly the balloon is being blown up). Also, the question uses the phrase ""at the point in time"" This is really confusing me. Is the textbook mistaken, or am I? Any help is appreciated.",,"['calculus', 'derivatives']"
31,"Questions on ""painless conjugate gradient"": take gradient of a quadratic form","Questions on ""painless conjugate gradient"": take gradient of a quadratic form",,"I am reading this paper: http://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf I have difficulties on the derivation of equation (6) on page 4. It is to take gradient of a quadratic form. I searched around and found this: How to take the gradient of the quadratic form? I can understand most of the answer in above link, but: Why the $y$ in the second part of chain rule needs to be transposed? In neither original paper or above Q/A it tells me how to take derivative of a vector valued function($R^n \rightarrow R^n$). I think that was used implicitly in the derivation of $\dfrac{\partial (x^TA^T)}{\partial x}$. And that may be not rigorous to apply  $$\dfrac{\partial (b^Tx)}{\partial x} = \dfrac{\partial (x^Tb)}{\partial x} = b$$ directly on $\dfrac{\partial (x^TA^T)}{\partial x}$ to get $A^T$.","I am reading this paper: http://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf I have difficulties on the derivation of equation (6) on page 4. It is to take gradient of a quadratic form. I searched around and found this: How to take the gradient of the quadratic form? I can understand most of the answer in above link, but: Why the $y$ in the second part of chain rule needs to be transposed? In neither original paper or above Q/A it tells me how to take derivative of a vector valued function($R^n \rightarrow R^n$). I think that was used implicitly in the derivation of $\dfrac{\partial (x^TA^T)}{\partial x}$. And that may be not rigorous to apply  $$\dfrac{\partial (b^Tx)}{\partial x} = \dfrac{\partial (x^Tb)}{\partial x} = b$$ directly on $\dfrac{\partial (x^TA^T)}{\partial x}$ to get $A^T$.",,"['linear-algebra', 'derivatives']"
32,"Given that $f'(0) = f''(0) = 1$, $f^{(12)}$ exists, and $g(x)= f(x^{10})$. Find $g^{(11)}(0)$.","Given that ,  exists, and . Find .",f'(0) = f''(0) = 1 f^{(12)} g(x)= f(x^{10}) g^{(11)}(0),"Given $f'(0) = f''(0) = 1$, $f^{(12)}$ exists, and $g\colon x \mapsto f(x^{10})$. Find $g^{(11)}(0)$.","Given $f'(0) = f''(0) = 1$, $f^{(12)}$ exists, and $g\colon x \mapsto f(x^{10})$. Find $g^{(11)}(0)$.",,"['calculus', 'derivatives']"
33,"Find an equation of the tangent line to the curve $y=x^3-3x+1$ at the given point $(2,3)$",Find an equation of the tangent line to the curve  at the given point,"y=x^3-3x+1 (2,3)","The only thing I know is that you must use the formula to find the slope of the tangent line, but I'm not quite sure on the steps to doing so.","The only thing I know is that you must use the formula to find the slope of the tangent line, but I'm not quite sure on the steps to doing so.",,"['calculus', 'derivatives']"
34,How does this differentiation come about ?,How does this differentiation come about ?,,"The question is that: If $f(z)$ is analytic, show that $\frac{\partial f}{\partial \bar z} = 0$ Now, assuming $f(z) = u + iv$ $\frac{\partial f}{\partial \bar z} = \frac{\partial}{\partial \bar z}(u + iv)$ What the book does is this: $$ (\frac{\partial u}{\partial x}.\frac{\partial x}{\partial \bar z} + \frac{\partial u}{\partial y}.\frac{\partial y} {\partial \bar z}) $$ for the $u$ part. Similarly for the $v$ part Can someone please explain how that is done ?","The question is that: If $f(z)$ is analytic, show that $\frac{\partial f}{\partial \bar z} = 0$ Now, assuming $f(z) = u + iv$ $\frac{\partial f}{\partial \bar z} = \frac{\partial}{\partial \bar z}(u + iv)$ What the book does is this: $$ (\frac{\partial u}{\partial x}.\frac{\partial x}{\partial \bar z} + \frac{\partial u}{\partial y}.\frac{\partial y} {\partial \bar z}) $$ for the $u$ part. Similarly for the $v$ part Can someone please explain how that is done ?",,"['complex-analysis', 'derivatives', 'complex-numbers']"
35,Why does $\frac{d}{d\theta} \left(\theta \ln\prod\limits_{i=1}^nx_i\right) = \sum\limits_{i=1}^n\ln x_i$,Why does,\frac{d}{d\theta} \left(\theta \ln\prod\limits_{i=1}^nx_i\right) = \sum\limits_{i=1}^n\ln x_i,"Is this just the product rule? I have this in my notes but I didn't think anything of it and now I'm wondering how this happens? Edit: Im working with maximum likelihood estimation and in my notes I have that the likelihood funciton $=L(x;\theta)=\prod_{i=1}^nf(x;\theta)$ where $x$ is the variable and $\theta$ is the parameter of a probability distribution. To estimate I was told that we take the log of the likelihood function, i.e. $\ln(L)$, then take its derivative to estimate the parameter. The function I'm working with is $f(x;\theta)=(\theta +1)x^{\theta}$. So  $$   L(x;\theta)=\prod_{i=1}^n(\theta +1)x_i^{\theta}=(\theta+1)^n\prod_{i=1}^nx_i^{\theta}. $$  Now  $$   \ln(L(x;\theta))=n*\ln(\theta+1)+\theta \ln\left(\prod_{i=1}^nx_i\right). $$ Here's where I'm confused, I have in my notes that  $$   \frac{d(\ln L)}{d\theta}=\frac{n}{\theta+1}+\sum_{i=1}^n\ln(x_i). $$  Why does the product of $x_i$ become the summation of $x_i$?","Is this just the product rule? I have this in my notes but I didn't think anything of it and now I'm wondering how this happens? Edit: Im working with maximum likelihood estimation and in my notes I have that the likelihood funciton $=L(x;\theta)=\prod_{i=1}^nf(x;\theta)$ where $x$ is the variable and $\theta$ is the parameter of a probability distribution. To estimate I was told that we take the log of the likelihood function, i.e. $\ln(L)$, then take its derivative to estimate the parameter. The function I'm working with is $f(x;\theta)=(\theta +1)x^{\theta}$. So  $$   L(x;\theta)=\prod_{i=1}^n(\theta +1)x_i^{\theta}=(\theta+1)^n\prod_{i=1}^nx_i^{\theta}. $$  Now  $$   \ln(L(x;\theta))=n*\ln(\theta+1)+\theta \ln\left(\prod_{i=1}^nx_i\right). $$ Here's where I'm confused, I have in my notes that  $$   \frac{d(\ln L)}{d\theta}=\frac{n}{\theta+1}+\sum_{i=1}^n\ln(x_i). $$  Why does the product of $x_i$ become the summation of $x_i$?",,"['derivatives', 'summation', 'products']"
36,Magnitude of 3D vector functions in relation to its derivative,Magnitude of 3D vector functions in relation to its derivative,,"If the magnitude of a 3D vector function is always 1, then is the derivative of that function always perpendicular to the original function?","If the magnitude of a 3D vector function is always 1, then is the derivative of that function always perpendicular to the original function?",,"['calculus', 'derivatives']"
37,Equations of a moving particle on a plane,Equations of a moving particle on a plane,,"Equations of a moving particle on a plane: $$\mathbf{r}(t)=x(t)\mathbf{i} + y(t)\mathbf{j}$$ $$\mathbf{v}(t)=\dot{\mathbf{r}}=\dot{x}(t)\mathbf{i} + \dot{y}(t)\mathbf{j}$$ $$\mathbf{a}(t)=\dot{\mathbf{v}}=\ddot{\mathbf{r}}=\ddot{x}(t)\mathbf{i} + \ddot{y}(t)\mathbf{j}$$ $$\mathbf{T}(t)=\frac{\mathbf{v}}{|{\mathbf{v}}|}=(cos(\phi(t)), sin(\phi(t)))$$ If $s(t)$ denotes the arc length along the curve then $$\dot{s}=|\mathbf{v}|=\frac{ds}{dt}$$ $$\mathbf{v}=|\mathbf{v}|\mathbf{T}=\dot{s}\mathbf{T}$$ $$\mathbf{a}=\dot{\mathbf{v}}=\frac{d}{dt}(\dot{s}\mathbf{T})=\ddot{s}\mathbf{T}+\dot{s}\frac{d\mathbf{T}}{dt}=\ddot{s}\mathbf{T}+\dot{s}^2\frac{d\mathbf{T}}{ds}$$ I don't get how in the last transformation we got $\dot{s}^2\frac{d\mathbf{T}}{ds}$ out of $\dot{s}\frac{d\mathbf{T}}{dt}$ Please help","Equations of a moving particle on a plane: $$\mathbf{r}(t)=x(t)\mathbf{i} + y(t)\mathbf{j}$$ $$\mathbf{v}(t)=\dot{\mathbf{r}}=\dot{x}(t)\mathbf{i} + \dot{y}(t)\mathbf{j}$$ $$\mathbf{a}(t)=\dot{\mathbf{v}}=\ddot{\mathbf{r}}=\ddot{x}(t)\mathbf{i} + \ddot{y}(t)\mathbf{j}$$ $$\mathbf{T}(t)=\frac{\mathbf{v}}{|{\mathbf{v}}|}=(cos(\phi(t)), sin(\phi(t)))$$ If $s(t)$ denotes the arc length along the curve then $$\dot{s}=|\mathbf{v}|=\frac{ds}{dt}$$ $$\mathbf{v}=|\mathbf{v}|\mathbf{T}=\dot{s}\mathbf{T}$$ $$\mathbf{a}=\dot{\mathbf{v}}=\frac{d}{dt}(\dot{s}\mathbf{T})=\ddot{s}\mathbf{T}+\dot{s}\frac{d\mathbf{T}}{dt}=\ddot{s}\mathbf{T}+\dot{s}^2\frac{d\mathbf{T}}{ds}$$ I don't get how in the last transformation we got $\dot{s}^2\frac{d\mathbf{T}}{ds}$ out of $\dot{s}\frac{d\mathbf{T}}{dt}$ Please help",,"['calculus', 'derivatives']"
38,How do I solve this partial derivative?,How do I solve this partial derivative?,,"Find the partial derivatives of the function $f(x,y,z)=(x+2y)^2 \sin(xy).$ I know the answers are: $\frac{\partial f}{\partial x} = y(x + 2y)^2\cos(xy) + 2(x + 2y)\sin(xy)$ $\frac{\partial f}{\partial y} = x(x + 2y)^2\cos(xy) + 4(x + 2y)\sin(xy)$ $\frac{\partial f}{\partial z} = 0$ But I can't figure out why. I would really like to see the working for these and to know what background areas I would need to study to be able to do them myself.  Thanks.","Find the partial derivatives of the function $f(x,y,z)=(x+2y)^2 \sin(xy).$ I know the answers are: $\frac{\partial f}{\partial x} = y(x + 2y)^2\cos(xy) + 2(x + 2y)\sin(xy)$ $\frac{\partial f}{\partial y} = x(x + 2y)^2\cos(xy) + 4(x + 2y)\sin(xy)$ $\frac{\partial f}{\partial z} = 0$ But I can't figure out why. I would really like to see the working for these and to know what background areas I would need to study to be able to do them myself.  Thanks.",,"['calculus', 'derivatives', 'partial-derivative']"
39,question about MIT calc 1 implicit diff video,question about MIT calc 1 implicit diff video,,"The implicit solution to $y^4+xy^2-2=0$ is written on the board as $$ y^{\prime} = \frac{-y^2}{4y^3+2xy}$$ did I miss something, but can't you factor out a y? so it should be $$y^{\prime} = \frac{-y}{4y^2+2x}$$","The implicit solution to $y^4+xy^2-2=0$ is written on the board as $$ y^{\prime} = \frac{-y^2}{4y^3+2xy}$$ did I miss something, but can't you factor out a y? so it should be $$y^{\prime} = \frac{-y}{4y^2+2x}$$",,"['calculus', 'derivatives', 'implicit-differentiation']"
40,Find an equation for the straight line tangent to this curve at the given point.,Find an equation for the straight line tangent to this curve at the given point.,,"As you can check, the point $(1,\ln(2))$ lies on the curve $xe^y+7y−2x=7\ln(2).$ Find an equation for the straight line tangent to this curve at the given point. Your answer should be an equation involving x and/or y. Use ln() for the natural logarithm. Tried to fin a function y=... but not sure how to do it.","As you can check, the point $(1,\ln(2))$ lies on the curve $xe^y+7y−2x=7\ln(2).$ Find an equation for the straight line tangent to this curve at the given point. Your answer should be an equation involving x and/or y. Use ln() for the natural logarithm. Tried to fin a function y=... but not sure how to do it.",,"['calculus', 'derivatives']"
41,Basic Differentiation Rules : Derivatives [closed],Basic Differentiation Rules : Derivatives [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I don't know how to obtain the derivative of these equations. Can you give me some hints, or some suggestions as to how to proceed? $$(1)\;\;f(x) = \frac{x^2(2x+8)}{x+4}$$ $$ $$ $$(2)\;\;f(x) = (1/2+x) (1/3-x)$$","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I don't know how to obtain the derivative of these equations. Can you give me some hints, or some suggestions as to how to proceed? $$(1)\;\;f(x) = \frac{x^2(2x+8)}{x+4}$$ $$ $$ $$(2)\;\;f(x) = (1/2+x) (1/3-x)$$",,"['calculus', 'derivatives']"
42,Find the second derivative of the given function,Find the second derivative of the given function,,If $$x=a(\cos \theta + \theta \sin \theta) $$$$ y=a(\sin \theta- \theta \cos \theta) $$ prove that $$\frac{d^2y}{dx^2}= \frac{\sec^3 \theta}{a \theta}$$ Can you solve this for me? I tried finding $\frac{dy}{dx} $ by dividing $\frac{dy}{dt} $ by $\frac{dx}{dt} $ but failed to get the required answer $$\frac{\frac{d}{d \theta} \frac{ \cos \theta + \theta \sin \theta}{\theta \cos \theta - \sin \theta}}{\frac{dx}{d \theta}}=\frac{1+\theta^2}{a(\theta \cos \theta- \sin \theta)}$$ I am stuck here Please offer your assistance. :),If $$x=a(\cos \theta + \theta \sin \theta) $$$$ y=a(\sin \theta- \theta \cos \theta) $$ prove that $$\frac{d^2y}{dx^2}= \frac{\sec^3 \theta}{a \theta}$$ Can you solve this for me? I tried finding $\frac{dy}{dx} $ by dividing $\frac{dy}{dt} $ by $\frac{dx}{dt} $ but failed to get the required answer $$\frac{\frac{d}{d \theta} \frac{ \cos \theta + \theta \sin \theta}{\theta \cos \theta - \sin \theta}}{\frac{dx}{d \theta}}=\frac{1+\theta^2}{a(\theta \cos \theta- \sin \theta)}$$ I am stuck here Please offer your assistance. :),,['derivatives']
43,Solving a differential equation with composite functions,Solving a differential equation with composite functions,,"If $c=f(a+e^b)+g(a-e^b)$ where $f$ and $g$ are functions of $a+b^2$ and $a-b^2$ respectively, find $c$ such that when $b=0$, you find that $c=0$ and $\frac{\partial c}{\partial b}=1+a$.","If $c=f(a+e^b)+g(a-e^b)$ where $f$ and $g$ are functions of $a+b^2$ and $a-b^2$ respectively, find $c$ such that when $b=0$, you find that $c=0$ and $\frac{\partial c}{\partial b}=1+a$.",,['derivatives']
44,Implicit differentiation question,Implicit differentiation question,,"Given that $x^n + y^n = 1$, show that $$\frac{d^2y}{dx^2} = -\frac{(n-1)x^{n-2}}{y^{2n-1}}.$$ I found that $\displaystyle nx^{n-1}+ny^{n-1}\frac{dy}{dx} = 0$ so that $\displaystyle y'=\frac{-x^{n-1}}{y^{n-1}}$. Then $$n(n-1)x^{n-2}+n(n-1)y^{n-2}\left(\frac{dy}{dx}\right)^2 + \frac{d^2y}{dx^2}ny^{n-1} = 0.$$ Therefore $$y'' = \displaystyle \frac{-n(n-1)x^{n-2}-n(n-1)y^{n-2}(y')^2}{ny^{n-1}}.$$ Substituting the first derivative: $$y'' = \displaystyle\frac{-n(n-1)x^{n-2}-n(n-1)y^{n-2}\left(\dfrac{-x^{n-1}}{y^{n-1}}\right)^2}{ny^{n-1}}.$$ I've been trying tons of different steps and can't establish which way to eliminate the $y$ terms from the numerator. Could someone offer a hint on how to proceed. Thanks.","Given that $x^n + y^n = 1$, show that $$\frac{d^2y}{dx^2} = -\frac{(n-1)x^{n-2}}{y^{2n-1}}.$$ I found that $\displaystyle nx^{n-1}+ny^{n-1}\frac{dy}{dx} = 0$ so that $\displaystyle y'=\frac{-x^{n-1}}{y^{n-1}}$. Then $$n(n-1)x^{n-2}+n(n-1)y^{n-2}\left(\frac{dy}{dx}\right)^2 + \frac{d^2y}{dx^2}ny^{n-1} = 0.$$ Therefore $$y'' = \displaystyle \frac{-n(n-1)x^{n-2}-n(n-1)y^{n-2}(y')^2}{ny^{n-1}}.$$ Substituting the first derivative: $$y'' = \displaystyle\frac{-n(n-1)x^{n-2}-n(n-1)y^{n-2}\left(\dfrac{-x^{n-1}}{y^{n-1}}\right)^2}{ny^{n-1}}.$$ I've been trying tons of different steps and can't establish which way to eliminate the $y$ terms from the numerator. Could someone offer a hint on how to proceed. Thanks.",,"['calculus', 'derivatives', 'implicit-differentiation']"
45,Differentiate $x \sqrt{1+y}+y \sqrt{1+x}=0$ [duplicate],Differentiate  [duplicate],x \sqrt{1+y}+y \sqrt{1+x}=0,"This question already has answers here : If $x\sqrt{1+y}+y\sqrt{1+x}=0$ find $y'$ (3 answers) Closed 6 years ago . If $x \sqrt{1+y}+y \sqrt{1+x}=0$, prove that $(1+x^2)\frac{dy}{dx}+1=0.$ The answer I got is $$\frac{dy}{dx}= -\frac{2 \sqrt{1+x} \sqrt{1+y}+y}{x+2 \sqrt{1+x}\sqrt{1+y}}$$ but I cannot simplify it further. Please provide your assistance.","This question already has answers here : If $x\sqrt{1+y}+y\sqrt{1+x}=0$ find $y'$ (3 answers) Closed 6 years ago . If $x \sqrt{1+y}+y \sqrt{1+x}=0$, prove that $(1+x^2)\frac{dy}{dx}+1=0.$ The answer I got is $$\frac{dy}{dx}= -\frac{2 \sqrt{1+x} \sqrt{1+y}+y}{x+2 \sqrt{1+x}\sqrt{1+y}}$$ but I cannot simplify it further. Please provide your assistance.",,['derivatives']
46,Continuity everywhere+partial derivatives in one point$\implies$Total differentiability in this point?,Continuity everywhere+partial derivatives in one pointTotal differentiability in this point?,\implies,"Let $f\colon\mathbb{R}^{n}\to\mathbb{R}$ be continuous everywhere. If we know that in one single point $x_0$ all the partial derivatives exist and are linear, $\partial_vf(x_0)=L(v)$, does this imply that $f$ is totally differentiable in $x_0$? In a previous question, user Etienne gave an example that showed that the continuity of $f$ everywhere rather than just in $x_0$ is necessary.","Let $f\colon\mathbb{R}^{n}\to\mathbb{R}$ be continuous everywhere. If we know that in one single point $x_0$ all the partial derivatives exist and are linear, $\partial_vf(x_0)=L(v)$, does this imply that $f$ is totally differentiable in $x_0$? In a previous question, user Etienne gave an example that showed that the continuity of $f$ everywhere rather than just in $x_0$ is necessary.",,"['calculus', 'derivatives', 'partial-derivative']"
47,"When a limit does not exist, can its derivative be found?","When a limit does not exist, can its derivative be found?",,"I am learning derivatives of complex numbers (functions, actually) and what a learned community member pointed to me was that there is a subtle difference between finding derivatives of real numbers. He said that the derivative of a complex function can be calculated iff it satisfies the Cauchy Riemann equations. Which means, limit at the point $z$ exists. Back to reals . Have a look at the diagram above. I can calculate derivatives in both the cases. The 'mechanical' derivative using chain rule, quotient rule, product rule, etc. The derivative will fail if I plug in x = 13 (in both the cases) but for all other values of $x$, I can calculate the derivative (slope). In other words, $f(x)$ is not differentiable at $x = 13$. The same concept applies in case of complex functions, right ? You use Cauchy Riemann equations to see if $f(z)$ is differentiable at a given $z$. Even if it is not differentiable, I can still calculate the derivative mechanically, right ? And the derivative will fail if I plug in the given $z$ at which it was not differentiable Please clarify.","I am learning derivatives of complex numbers (functions, actually) and what a learned community member pointed to me was that there is a subtle difference between finding derivatives of real numbers. He said that the derivative of a complex function can be calculated iff it satisfies the Cauchy Riemann equations. Which means, limit at the point $z$ exists. Back to reals . Have a look at the diagram above. I can calculate derivatives in both the cases. The 'mechanical' derivative using chain rule, quotient rule, product rule, etc. The derivative will fail if I plug in x = 13 (in both the cases) but for all other values of $x$, I can calculate the derivative (slope). In other words, $f(x)$ is not differentiable at $x = 13$. The same concept applies in case of complex functions, right ? You use Cauchy Riemann equations to see if $f(z)$ is differentiable at a given $z$. Even if it is not differentiable, I can still calculate the derivative mechanically, right ? And the derivative will fail if I plug in the given $z$ at which it was not differentiable Please clarify.",,"['complex-analysis', 'limits', 'derivatives']"
48,Evaluating the derivative of a Cantor-Vitali function,Evaluating the derivative of a Cantor-Vitali function,,"Let $\varphi \colon [0,1] \to \mathbb R$ a ""Cantor-Vitali function"", viz. take $x \in [0,1)$ and write it as $$ x = \sum_{j=1}^{\infty} \frac{a_j}{3^j}, \quad a_j \in \{0,1,2\} $$ with $a_j$ non definitely equal to 2. Then  $$ \varphi (x) := \begin{cases} \sum_{j=1}^{k-1}\frac{a_j}{2^{j+1}} & a_k=1 \text{ and } a_j \ne 1 \quad \forall j \in \{1,2, \ldots , k-1\} \\ \sum_{j=1}^{\infty}\frac{a_j}{2^{j+1}} & a_j \ne 1\quad \forall j \in \mathbb N  \end{cases} $$ Moreover, put $\varphi(1):=1$. Question . Evaluate $\varphi^{\prime}(x)$ for every $x \in [0,1]$. Well, I think it is not difficult to prove the existence (at least a.e.) of $\varphi^{\prime}$, since the function should be monotone (increasing?), hence differentiable a.e. by Lebesgue's theorem. The problem is that I cannot understand how to calculate $\varphi^{\prime}(x)$. Is it zero a.e.? How to prove this? How can I do? Thanks in advance.","Let $\varphi \colon [0,1] \to \mathbb R$ a ""Cantor-Vitali function"", viz. take $x \in [0,1)$ and write it as $$ x = \sum_{j=1}^{\infty} \frac{a_j}{3^j}, \quad a_j \in \{0,1,2\} $$ with $a_j$ non definitely equal to 2. Then  $$ \varphi (x) := \begin{cases} \sum_{j=1}^{k-1}\frac{a_j}{2^{j+1}} & a_k=1 \text{ and } a_j \ne 1 \quad \forall j \in \{1,2, \ldots , k-1\} \\ \sum_{j=1}^{\infty}\frac{a_j}{2^{j+1}} & a_j \ne 1\quad \forall j \in \mathbb N  \end{cases} $$ Moreover, put $\varphi(1):=1$. Question . Evaluate $\varphi^{\prime}(x)$ for every $x \in [0,1]$. Well, I think it is not difficult to prove the existence (at least a.e.) of $\varphi^{\prime}$, since the function should be monotone (increasing?), hence differentiable a.e. by Lebesgue's theorem. The problem is that I cannot understand how to calculate $\varphi^{\prime}(x)$. Is it zero a.e.? How to prove this? How can I do? Thanks in advance.",,"['real-analysis', 'derivatives']"
49,Solve using L'Hôpital rule,Solve using L'Hôpital rule,,"Solve using L'Hôpital rule $\lim_{x\to 1} (1-x)^{\cos[(\Pi/2) x)]}$ So (...) $$L=\lim_{x\to 1} (1-x)^{\cos[(\Pi/2) x)]} \rightarrow0^0$$ $$\ln L = \lim_{x\to 1} \ln (1-x)^{\cos[(\Pi/2) x)]}$$ $$\ln L = \lim_{x\to 1}\cos[(\Pi/2)] \ln(1-x)$$ $$\ln L=\lim_{x\to 1} \cfrac{\ln(1-x)}{\cfrac{1}{\cos[(\Pi/2)x]}}={\infty\over\infty} \rightarrow \text{Using L'Hôpital rule}$$ $$\ln L=\lim_{x\to 1}  { \cfrac{ \left({-1\over{1-x}}\right) } { \cfrac {-[{\Pi\over2}\cos x.-\sin x({\Pi\over2}x)] } {[\cos x({\Pi\over2}x)]^2} } } = \cfrac{\infty}{\infty} \rightarrow \text{Using L'Hôpital rule}$$ $$\ln L=\lim_{x\to 1} { \cfrac { 1 } { \cfrac { -[{\Pi\over2}(-\sin x)(-\sin x({\Pi\over2}x))-({\Pi\over2}(- \cos({\Pi\over2}x)))] } { [\cos({\Pi\over2}x)]^4 } } } $$ $$\ln L=\lim_{x\to 1} { \cfrac { [\cos({\Pi\over2}x)]^4 } { -[{\Pi\over2}(-\sin x)(- \sin({\Pi\over2}x))-({\Pi\over2}(- \cos({\Pi\over2}x)))] } } = 0$$ $$\ln L = 0 \rightarrow e^{\ln L} = e^0\rightarrow L = 1$$ I have an exam in two days, and I need to know: is this well done? If not, could someone point out the error to me?","Solve using L'Hôpital rule $\lim_{x\to 1} (1-x)^{\cos[(\Pi/2) x)]}$ So (...) $$L=\lim_{x\to 1} (1-x)^{\cos[(\Pi/2) x)]} \rightarrow0^0$$ $$\ln L = \lim_{x\to 1} \ln (1-x)^{\cos[(\Pi/2) x)]}$$ $$\ln L = \lim_{x\to 1}\cos[(\Pi/2)] \ln(1-x)$$ $$\ln L=\lim_{x\to 1} \cfrac{\ln(1-x)}{\cfrac{1}{\cos[(\Pi/2)x]}}={\infty\over\infty} \rightarrow \text{Using L'Hôpital rule}$$ $$\ln L=\lim_{x\to 1}  { \cfrac{ \left({-1\over{1-x}}\right) } { \cfrac {-[{\Pi\over2}\cos x.-\sin x({\Pi\over2}x)] } {[\cos x({\Pi\over2}x)]^2} } } = \cfrac{\infty}{\infty} \rightarrow \text{Using L'Hôpital rule}$$ $$\ln L=\lim_{x\to 1} { \cfrac { 1 } { \cfrac { -[{\Pi\over2}(-\sin x)(-\sin x({\Pi\over2}x))-({\Pi\over2}(- \cos({\Pi\over2}x)))] } { [\cos({\Pi\over2}x)]^4 } } } $$ $$\ln L=\lim_{x\to 1} { \cfrac { [\cos({\Pi\over2}x)]^4 } { -[{\Pi\over2}(-\sin x)(- \sin({\Pi\over2}x))-({\Pi\over2}(- \cos({\Pi\over2}x)))] } } = 0$$ $$\ln L = 0 \rightarrow e^{\ln L} = e^0\rightarrow L = 1$$ I have an exam in two days, and I need to know: is this well done? If not, could someone point out the error to me?",,"['calculus', 'limits', 'derivatives']"
50,Second Derivative of basic fraction using quotient rule,Second Derivative of basic fraction using quotient rule,,"I know this is a very basic question but I need some help. I have to find the second derivative of: $$\frac{1}{3x^2 + 4}$$ I start by using the Quotient Rule and get the first derivative to be: $$\frac{-6x}{(3x^2 + 4)^2}$$ This I believe to be correct. Following that I proceed to find the second derivative in the same manner but I get this as my answer: $$\frac{(54x^4 + 144x^2 +96) - (-36x^3 + 48x)}{(9x^4 +24x^2 +16)^2}$$ This I believe to be correct just not simplified.  However the answer I need to get is: $$- \frac{6(4 - 9x^2)}{(3x^2 + 4)^3}$$ I do not know what the best way to approach this would be, should I multiply out the denominator and try to cancel?  Could someone point me in the right direction, I want to solve it myself but I need some guidance. Thanks","I know this is a very basic question but I need some help. I have to find the second derivative of: $$\frac{1}{3x^2 + 4}$$ I start by using the Quotient Rule and get the first derivative to be: $$\frac{-6x}{(3x^2 + 4)^2}$$ This I believe to be correct. Following that I proceed to find the second derivative in the same manner but I get this as my answer: $$\frac{(54x^4 + 144x^2 +96) - (-36x^3 + 48x)}{(9x^4 +24x^2 +16)^2}$$ This I believe to be correct just not simplified.  However the answer I need to get is: $$- \frac{6(4 - 9x^2)}{(3x^2 + 4)^3}$$ I do not know what the best way to approach this would be, should I multiply out the denominator and try to cancel?  Could someone point me in the right direction, I want to solve it myself but I need some guidance. Thanks",,"['calculus', 'derivatives']"
51,Finding the value of t where tangent line is perpendicular to x axis,Finding the value of t where tangent line is perpendicular to x axis,,"For the curve x = t$^2 - 1, y = t^2 - t$, the tangent line is perpendicular to x-axis, where Options are : a ) t = 0 b)  $t \to \infty$ c)  $t = \frac{1}{\sqrt{3}}$ d)  $t = \frac{-1}{\sqrt{3}}$ Here we have $\frac{dx}{dt} = 2t$   $\& $  $ \frac{dy}{dt} = 2t-1$ $\therefore, \frac{dy}{dx} = \frac{2t-1}{2t}$ If this tangent is parallel to x axis then $\frac{dy}{dx}=0$ and if this is perpendicular then $\frac{-dx}{dy} =0$ $\Rightarrow  \frac{2t}{2t-1}=0 \Rightarrow 2t = 0 \Rightarrow t =0$ I hope this is the correct approach please suggest.. thanks..","For the curve x = t$^2 - 1, y = t^2 - t$, the tangent line is perpendicular to x-axis, where Options are : a ) t = 0 b)  $t \to \infty$ c)  $t = \frac{1}{\sqrt{3}}$ d)  $t = \frac{-1}{\sqrt{3}}$ Here we have $\frac{dx}{dt} = 2t$   $\& $  $ \frac{dy}{dt} = 2t-1$ $\therefore, \frac{dy}{dx} = \frac{2t-1}{2t}$ If this tangent is parallel to x axis then $\frac{dy}{dx}=0$ and if this is perpendicular then $\frac{-dx}{dy} =0$ $\Rightarrow  \frac{2t}{2t-1}=0 \Rightarrow 2t = 0 \Rightarrow t =0$ I hope this is the correct approach please suggest.. thanks..",,"['calculus', 'derivatives']"
52,Unique continuos linear function given a continuous function from a dense space in X to Y (Y is a Banach Space).,Unique continuos linear function given a continuous function from a dense space in X to Y (Y is a Banach Space).,,"Let $X$ be a normed space, let $Y$ be a Banach Space, let $D\subseteq X$ be a dense linear subspace of $X$ and let $L:D\rightarrow Y$ be a continuous linear function. Then there is a unique continuous linear function $M:X\rightarrow Y$ such that $M|_D=L$. The task is to prove this theorem. I have no idea where to even start, any direction you can give me would be greatly appreciated. edit: "" Banach Space, let $D\in X$ "" [;\rightarrow ;] """" Banach Space, let $D\subseteq X$ """"","Let $X$ be a normed space, let $Y$ be a Banach Space, let $D\subseteq X$ be a dense linear subspace of $X$ and let $L:D\rightarrow Y$ be a continuous linear function. Then there is a unique continuous linear function $M:X\rightarrow Y$ such that $M|_D=L$. The task is to prove this theorem. I have no idea where to even start, any direction you can give me would be greatly appreciated. edit: "" Banach Space, let $D\in X$ "" [;\rightarrow ;] """" Banach Space, let $D\subseteq X$ """"",,"['derivatives', 'continuity', 'normed-spaces']"
53,Can we use (higher order) derivatives to help integration?,Can we use (higher order) derivatives to help integration?,,"I'm wondering if it's possible to use derivatives to ease the evaluation of an integral.  For instance, I know that to evaluate an integral with enough precision I need to evaluate it at $n$ points.  I also have a method to get higher-order derivatives, like the 2nd derivative, 3rd derivative, etc. with relative ease.  So I'm wondering, and hopefully this isn't to vague, if I can make use of these derivatives so that I can evaluate the function at less than $n$ points. In other words, can I use derivatives so that I can combine them with an integral somehow so that I can evaluate the integral at fewer points?  I know this may be vague, but I'm just trying to get a general feel of whether derivatives can with the evaluation of an integral.","I'm wondering if it's possible to use derivatives to ease the evaluation of an integral.  For instance, I know that to evaluate an integral with enough precision I need to evaluate it at $n$ points.  I also have a method to get higher-order derivatives, like the 2nd derivative, 3rd derivative, etc. with relative ease.  So I'm wondering, and hopefully this isn't to vague, if I can make use of these derivatives so that I can evaluate the function at less than $n$ points. In other words, can I use derivatives so that I can combine them with an integral somehow so that I can evaluate the integral at fewer points?  I know this may be vague, but I'm just trying to get a general feel of whether derivatives can with the evaluation of an integral.",,"['integration', 'numerical-methods', 'derivatives']"
54,proof of $p'(x)^2 \geq p(x)p''(x) \text { for all x } \in \Bbb{R}$,proof of,p'(x)^2 \geq p(x)p''(x) \text { for all x } \in \Bbb{R},"$p(x)$ is non-constant polynomial with only real roots. If $x = a_i$ is a root of $p(x)$,  we are done. Assume then $x$ is not a root. Product of differentiation: $$p\prime(x) = \sum\limits_{k=1}^n\frac{p(x)}{x-a_k}$$ that is $$\frac{p\prime(x)}{p(x)} = \sum\limits_{k=1}^n\frac{1}{x-a_k}$$ now by differentiating it again, we get: $$\frac{p\prime\prime(x)p(x)-p\prime(x)^2}{p(x)^2} = -\sum\limits_{k=1}^n\frac{1}{(x-a_k)^2} $$ I understand, why after second differentiation we get left side, but I can't figure out, how we got the right side ($-\sum\limits_{k=1}^n\frac{1}{(x-a_k)^2} $). Can you please provide me some tips? Thanks ... and to complete the proof, by second differentiation we see, that $-\sum\limits_{k=1}^n\frac{1}{(x-a_k)^2} < 0$, so also: $$\frac{p\prime\prime(x)p(x)-p\prime(x)^2}{p(x)^2} < 0$$ $$p\prime\prime(x)p(x)-p\prime(x)^2 < 0$$ $$p\prime\prime(x)p(x) < p\prime(x)^2 $$ ...done :)","$p(x)$ is non-constant polynomial with only real roots. If $x = a_i$ is a root of $p(x)$,  we are done. Assume then $x$ is not a root. Product of differentiation: $$p\prime(x) = \sum\limits_{k=1}^n\frac{p(x)}{x-a_k}$$ that is $$\frac{p\prime(x)}{p(x)} = \sum\limits_{k=1}^n\frac{1}{x-a_k}$$ now by differentiating it again, we get: $$\frac{p\prime\prime(x)p(x)-p\prime(x)^2}{p(x)^2} = -\sum\limits_{k=1}^n\frac{1}{(x-a_k)^2} $$ I understand, why after second differentiation we get left side, but I can't figure out, how we got the right side ($-\sum\limits_{k=1}^n\frac{1}{(x-a_k)^2} $). Can you please provide me some tips? Thanks ... and to complete the proof, by second differentiation we see, that $-\sum\limits_{k=1}^n\frac{1}{(x-a_k)^2} < 0$, so also: $$\frac{p\prime\prime(x)p(x)-p\prime(x)^2}{p(x)^2} < 0$$ $$p\prime\prime(x)p(x)-p\prime(x)^2 < 0$$ $$p\prime\prime(x)p(x) < p\prime(x)^2 $$ ...done :)",,"['polynomials', 'derivatives', 'proof-writing']"
55,Math question partial derivatives help?,Math question partial derivatives help?,,"I have the functions : $u=\arctan(xyz)$ where $x=\cos(t)\quad  y=e^t$ and $z=1/t$. I have to find $\dfrac{du}{dt}$. My attempt to a solution : $\dfrac{du}{dt}=\dfrac{\partial u}{\partial x}\dfrac{dx}{dt} + \dfrac{\partial u}{\partial y}\dfrac{dy}{dt} + \dfrac{\partial u}{\partial z}\dfrac{dz}{dt}$ $\dfrac{\partial u}{\partial x}=\dfrac{yz}{1+x^2\cdot y^2\cdot z^2}$ ;$\dfrac{dx}{dt}=\dfrac{-\sin t \partial u}{\partial y}=\dfrac{xz}{1+x^2\cdot y^2\cdot z^2}$ ,$\dfrac{dy}{dt}=e^t$ , $\dfrac{\partial u}{\partial z}=\dfrac{xy}{1+x^2\cdot y^2 \cdot z^2}$ $\dfrac{dz}{dt}=\dfrac{-1}{t^2}$. The problem is,how do I replace these back into the formula so I could get a result :  $\dfrac{e^t(t\cos t-t\sin t+\cos t)}{t^2+e^{(2t)} \cdot \cos^2(t)}$ I SOLVED THIS,THANK YOU ALREADY.","I have the functions : $u=\arctan(xyz)$ where $x=\cos(t)\quad  y=e^t$ and $z=1/t$. I have to find $\dfrac{du}{dt}$. My attempt to a solution : $\dfrac{du}{dt}=\dfrac{\partial u}{\partial x}\dfrac{dx}{dt} + \dfrac{\partial u}{\partial y}\dfrac{dy}{dt} + \dfrac{\partial u}{\partial z}\dfrac{dz}{dt}$ $\dfrac{\partial u}{\partial x}=\dfrac{yz}{1+x^2\cdot y^2\cdot z^2}$ ;$\dfrac{dx}{dt}=\dfrac{-\sin t \partial u}{\partial y}=\dfrac{xz}{1+x^2\cdot y^2\cdot z^2}$ ,$\dfrac{dy}{dt}=e^t$ , $\dfrac{\partial u}{\partial z}=\dfrac{xy}{1+x^2\cdot y^2 \cdot z^2}$ $\dfrac{dz}{dt}=\dfrac{-1}{t^2}$. The problem is,how do I replace these back into the formula so I could get a result :  $\dfrac{e^t(t\cos t-t\sin t+\cos t)}{t^2+e^{(2t)} \cdot \cos^2(t)}$ I SOLVED THIS,THANK YOU ALREADY.",,['derivatives']
56,Lagrange polynomial and derivative problem,Lagrange polynomial and derivative problem,,"I need to code this function in matlab $$L'(x) = \sum_{k = 0}^{n} f_k l_k(x)  $$ Where's $l_k$ looks like this $$l_k(x) = \sum^{n}_{j=0, j \neq k} \frac{\prod^{n}_{i=0}(x - x_i)}{(x - x_k)(x-x_j)\prod^{n}_{i=0, i \neq k} (x_k - x_i)} $$ Well I got an array of $f_k, k = 1,\dots,n$ (there are results of error function $erf(x)$ in some points). So I wrote two functions. First - to find $ l_k$ in some point (in arguments: res_arr array with $f_k$, $x$ - some point, k it's step in $\sum_{k}$) function lDiffout = lDiff(res_arr, x, k)  n = size(res_arr,2); result = 0; p1 =1; p2 =1;  %   П(x - x_i)     for i = 1:n          p1 = p1*(x - res_arr(i));     end; %    П(x_k - x_i)        for i = 1:n          if(k ~= i)           p2 = p2*(res_arr(k) - res_arr(i));         end     end;   for j=1:n      p_res = p1/p2;  %П(x - x_i)/П(x_k - x_i)      if(j ~= k)        p_rd = (x - res_arr(k))*(x - res_arr(j));% (x - x_k)(x - x_j)        result = result + p_res/p_rd; % += П(x - x_i)/ (x - x_k)(x - x_j)П(x_k - x_i)     end end;  lDiffout = result; end Second to find result (here  is start, wstep, final in arguments it's because of x it's vector (in my situation from 0 to 2 with setp 0.2)) function erdifOut = erfDiff(start, wstep, final, arr_res);     clc      cnt = 1;       for xloop = start:wstep:final            erdifOut(cnt) = arr_res(cnt)*lDiff(arr_res, xloop, cnt);           disp(['in x=', num2str(xloop), ' result is L = ',  num2str(erdifOut(cnt))]);           cnt = cnt + 1;       end;     end But it does not works correctly for some reason. My teacher said that i need to do something with $l_k$ function, make it simpler or something. Please how I can do it? Or maybe something wrong with the code?","I need to code this function in matlab $$L'(x) = \sum_{k = 0}^{n} f_k l_k(x)  $$ Where's $l_k$ looks like this $$l_k(x) = \sum^{n}_{j=0, j \neq k} \frac{\prod^{n}_{i=0}(x - x_i)}{(x - x_k)(x-x_j)\prod^{n}_{i=0, i \neq k} (x_k - x_i)} $$ Well I got an array of $f_k, k = 1,\dots,n$ (there are results of error function $erf(x)$ in some points). So I wrote two functions. First - to find $ l_k$ in some point (in arguments: res_arr array with $f_k$, $x$ - some point, k it's step in $\sum_{k}$) function lDiffout = lDiff(res_arr, x, k)  n = size(res_arr,2); result = 0; p1 =1; p2 =1;  %   П(x - x_i)     for i = 1:n          p1 = p1*(x - res_arr(i));     end; %    П(x_k - x_i)        for i = 1:n          if(k ~= i)           p2 = p2*(res_arr(k) - res_arr(i));         end     end;   for j=1:n      p_res = p1/p2;  %П(x - x_i)/П(x_k - x_i)      if(j ~= k)        p_rd = (x - res_arr(k))*(x - res_arr(j));% (x - x_k)(x - x_j)        result = result + p_res/p_rd; % += П(x - x_i)/ (x - x_k)(x - x_j)П(x_k - x_i)     end end;  lDiffout = result; end Second to find result (here  is start, wstep, final in arguments it's because of x it's vector (in my situation from 0 to 2 with setp 0.2)) function erdifOut = erfDiff(start, wstep, final, arr_res);     clc      cnt = 1;       for xloop = start:wstep:final            erdifOut(cnt) = arr_res(cnt)*lDiff(arr_res, xloop, cnt);           disp(['in x=', num2str(xloop), ' result is L = ',  num2str(erdifOut(cnt))]);           cnt = cnt + 1;       end;     end But it does not works correctly for some reason. My teacher said that i need to do something with $l_k$ function, make it simpler or something. Please how I can do it? Or maybe something wrong with the code?",,"['polynomials', 'derivatives', 'matlab', 'error-function']"
57,Derivative of $\sin^2 (\sqrt{t})$,Derivative of,\sin^2 (\sqrt{t}),I need to find the derivative of $\sin^2 (\sqrt{t})$ which I believe have done but the answer seems to be more simplified and I don't know how to arrive to it. Here are my steps $$\begin{align} & \frac{d}{dt}(\sin \sqrt{t})^2 \\ & = 2(\sin \sqrt{t}) \cdot \frac{\cos \sqrt{t}}{2\sqrt{t}} \\ & = \frac{\sin \sqrt{t} \cdot \cos \sqrt{t}}{\sqrt{t}}  \end{align}$$ The question is how does this simplify to $\frac{\sin 2\sqrt{t}}{2\sqrt{t}}$? EDIT: I forgot to add $2$ but I don't understand why top and bottom don't just cancel?,I need to find the derivative of $\sin^2 (\sqrt{t})$ which I believe have done but the answer seems to be more simplified and I don't know how to arrive to it. Here are my steps $$\begin{align} & \frac{d}{dt}(\sin \sqrt{t})^2 \\ & = 2(\sin \sqrt{t}) \cdot \frac{\cos \sqrt{t}}{2\sqrt{t}} \\ & = \frac{\sin \sqrt{t} \cdot \cos \sqrt{t}}{\sqrt{t}}  \end{align}$$ The question is how does this simplify to $\frac{\sin 2\sqrt{t}}{2\sqrt{t}}$? EDIT: I forgot to add $2$ but I don't understand why top and bottom don't just cancel?,,"['calculus', 'derivatives']"
58,Analyzing the lower bound of a logarithm of factorials using Stirling's Approximation,Analyzing the lower bound of a logarithm of factorials using Stirling's Approximation,,"I am trying to get the lower bound for: $f(x) = \ln(\lfloor\frac{x}{4}\rfloor!) - \ln(\lfloor\frac{x}{5}\rfloor!) -\ln(\lfloor\frac{x}{20}\rfloor!) - 2(1.03883)(\sqrt{\frac{x}{4}}) - (1.03883)(\frac{x}{8}) + \ln(\lfloor\frac{x}{10}\rfloor!) - \ln(\lfloor\frac{x}{12}\rfloor!) - \ln(\lfloor(\frac{x}{60}\rfloor!)$ using Stirling's approximation, $n! = \sqrt{2\pi{n}}(\frac{n}{e})^n + r_1(n)$, to show that $f(x)$ is greater than $0$ for all $x \ge 928$? Let's define $g(x)$ as: $g(x) = \ln(\sqrt{2\pi\frac{x}{4}}(\frac{x}{4e})^{\frac{x}{4}}) - \ln(\sqrt{2w\pi\frac{x}{5}}(\frac{x}{5e})^{\frac{x}{5}}) -\ln(\sqrt{2w\pi\frac{x}{20}}(\frac{x}{20e})^{\frac{x}{20}}) - 2(1.03883)(\sqrt{\frac{x}{4}}) - (1.03883)(\frac{x}{8}) + \ln(\sqrt{2\pi\frac{x}{10}}(\frac{x}{10e})^{\frac{x}{10}}) - \ln(\sqrt{2w\pi\frac{x}{12}}(\frac{x}{12e})^{\frac{x}{12}}) -\ln(\sqrt{2w\pi\frac{x}{60}}(\frac{x}{60e})^{\frac{x}{60}})$ where $w$ is a very large number. Would we have established that $f(x) > 0$ for $x \ge 928$ if we show: (a)  $f(928) > 0$ (b)  for $x \ge 928$, $g'(x) > 0$? Attempted Answer: I believe that the answer to my question is no.  Derivatives cannot be applied to $f(x)$ because it is not continuous.  The floor function is what causes the problem.  A better approach is either to analyze the size of the error term based on $x$ or define $f(x)$ in terms of the gamma function which is continuous. Is this correct? edit: Changed my question to make my point clearer in light of Marty's point that you need an upper bound of $\ln y$ to establish the lower bound of $\ln x - \ln y$.","I am trying to get the lower bound for: $f(x) = \ln(\lfloor\frac{x}{4}\rfloor!) - \ln(\lfloor\frac{x}{5}\rfloor!) -\ln(\lfloor\frac{x}{20}\rfloor!) - 2(1.03883)(\sqrt{\frac{x}{4}}) - (1.03883)(\frac{x}{8}) + \ln(\lfloor\frac{x}{10}\rfloor!) - \ln(\lfloor\frac{x}{12}\rfloor!) - \ln(\lfloor(\frac{x}{60}\rfloor!)$ using Stirling's approximation, $n! = \sqrt{2\pi{n}}(\frac{n}{e})^n + r_1(n)$, to show that $f(x)$ is greater than $0$ for all $x \ge 928$? Let's define $g(x)$ as: $g(x) = \ln(\sqrt{2\pi\frac{x}{4}}(\frac{x}{4e})^{\frac{x}{4}}) - \ln(\sqrt{2w\pi\frac{x}{5}}(\frac{x}{5e})^{\frac{x}{5}}) -\ln(\sqrt{2w\pi\frac{x}{20}}(\frac{x}{20e})^{\frac{x}{20}}) - 2(1.03883)(\sqrt{\frac{x}{4}}) - (1.03883)(\frac{x}{8}) + \ln(\sqrt{2\pi\frac{x}{10}}(\frac{x}{10e})^{\frac{x}{10}}) - \ln(\sqrt{2w\pi\frac{x}{12}}(\frac{x}{12e})^{\frac{x}{12}}) -\ln(\sqrt{2w\pi\frac{x}{60}}(\frac{x}{60e})^{\frac{x}{60}})$ where $w$ is a very large number. Would we have established that $f(x) > 0$ for $x \ge 928$ if we show: (a)  $f(928) > 0$ (b)  for $x \ge 928$, $g'(x) > 0$? Attempted Answer: I believe that the answer to my question is no.  Derivatives cannot be applied to $f(x)$ because it is not continuous.  The floor function is what causes the problem.  A better approach is either to analyze the size of the error term based on $x$ or define $f(x)$ in terms of the gamma function which is continuous. Is this correct? edit: Changed my question to make my point clearer in light of Marty's point that you need an upper bound of $\ln y$ to establish the lower bound of $\ln x - \ln y$.",,"['elementary-number-theory', 'derivatives', 'logarithms', 'factorial']"
59,estimating the derivatives of $\sin z$ at $z=0$,estimating the derivatives of  at,\sin z z=0,"(a) Use Cauchy inequality to obtain estimate for the derivatives of $\sin z$ at $z=0$ and (b) determine how good these estimates are No examples are given except the proof on Cauchy Inequality . How do I work this out? If I take $|z|=1$ as my circle, would $\displaystyle \max \left \{ \sqrt{\sin^2 x + \sinh^2 \left( \sqrt{1-x^2} \right) }\right \} n!$  be an approximation for my n-th derivative of $\sin z$ inside circle $|z| = 1 $?","(a) Use Cauchy inequality to obtain estimate for the derivatives of $\sin z$ at $z=0$ and (b) determine how good these estimates are No examples are given except the proof on Cauchy Inequality . How do I work this out? If I take $|z|=1$ as my circle, would $\displaystyle \max \left \{ \sqrt{\sin^2 x + \sinh^2 \left( \sqrt{1-x^2} \right) }\right \} n!$  be an approximation for my n-th derivative of $\sin z$ inside circle $|z| = 1 $?",,"['complex-analysis', 'derivatives']"
60,"Prove the mapping $(x,y,z)\mapsto (x+e^y,y+e^z,z+e^x)$ is locally invertible.",Prove the mapping  is locally invertible.,"(x,y,z)\mapsto (x+e^y,y+e^z,z+e^x)","Show that the mapping $\mathbb{R}^3\to \mathbb{R}^3$, $(x,y,z)\mapsto (u,v,w)$ which is defined by $$\begin{align*} u&=x+e^y\\ v&= y+e^z\\  w &=z+e^x \end{align*}$$is locally invertible  everywhere. Any assistance will be appreciated. Thanks","Show that the mapping $\mathbb{R}^3\to \mathbb{R}^3$, $(x,y,z)\mapsto (u,v,w)$ which is defined by $$\begin{align*} u&=x+e^y\\ v&= y+e^z\\  w &=z+e^x \end{align*}$$is locally invertible  everywhere. Any assistance will be appreciated. Thanks",,"['calculus', 'matrices', 'derivatives']"
61,"How do I calculate the sum of $\sum_{k=1}^{\infty}\frac{(2-x)^k}{2^k\cdot k}$ in every x in (0, 4)?","How do I calculate the sum of  in every x in (0, 4)?",\sum_{k=1}^{\infty}\frac{(2-x)^k}{2^k\cdot k},Well I've been trying to search for the appropriate derivative but I couldn't find it Thanks,Well I've been trying to search for the appropriate derivative but I couldn't find it Thanks,,"['convergence-divergence', 'derivatives', 'summation']"
62,Uniqueness of derivative,Uniqueness of derivative,,"I don't know how to phrase the title better. We say that a function is ""growing"" (don't know the English term) if $f'(x)\ge0, \forall x$ However, if we want to say that the function is ""strictly growing"", we also need an extra statement saying that $S=\{x | f'(x)=0\}$ contains no intervals. This means that if the derivative of a function is 0 in one isolated point, the function can still described as ""strictly growing"". That got me thinking. If we had a function like the following: $f(x)=x$ It's derivative is obviously $f'(x)=1$ But why wouldn't the function $g(x)=\begin{cases} 1 &\mbox{if } x \ne 2 \\ 0 & \mbox{if } x = 2 \end{cases} $ Also be it's derivative? I've chosen the number 2 at random but the point is that only one of the points differs which I don't think is enough for the function to ""change it's angle"". So, what gives? Furthermore, what does the function look like if it's derivative is something like $g(x)=\begin{cases} 1 &\mbox{if } x \in \mathbb{Q} \\ 0 & \mbox{if } x \not\in \mathbb{Q} \end{cases} $","I don't know how to phrase the title better. We say that a function is ""growing"" (don't know the English term) if $f'(x)\ge0, \forall x$ However, if we want to say that the function is ""strictly growing"", we also need an extra statement saying that $S=\{x | f'(x)=0\}$ contains no intervals. This means that if the derivative of a function is 0 in one isolated point, the function can still described as ""strictly growing"". That got me thinking. If we had a function like the following: $f(x)=x$ It's derivative is obviously $f'(x)=1$ But why wouldn't the function $g(x)=\begin{cases} 1 &\mbox{if } x \ne 2 \\ 0 & \mbox{if } x = 2 \end{cases} $ Also be it's derivative? I've chosen the number 2 at random but the point is that only one of the points differs which I don't think is enough for the function to ""change it's angle"". So, what gives? Furthermore, what does the function look like if it's derivative is something like $g(x)=\begin{cases} 1 &\mbox{if } x \in \mathbb{Q} \\ 0 & \mbox{if } x \not\in \mathbb{Q} \end{cases} $",,"['integration', 'derivatives']"
63,Check monotonicity using derivatives,Check monotonicity using derivatives,,"Depending on the $ c \in \mathbb{R}, c \not= 0$ Check monotonicity (increasing or non-decreasing) for $ x_{n}=(1+\frac{c}{n})^n$ I tired intruduce function $f(1/n)=(1+cn)^{\frac{1}{n}}$ and sign the derivative depends on $$\frac{\large{\frac{c n}{1+c n}}-\log(1+c n)}{n^2}$$ but dont know what to do next.","Depending on the $ c \in \mathbb{R}, c \not= 0$ Check monotonicity (increasing or non-decreasing) for $ x_{n}=(1+\frac{c}{n})^n$ I tired intruduce function $f(1/n)=(1+cn)^{\frac{1}{n}}$ and sign the derivative depends on $$\frac{\large{\frac{c n}{1+c n}}-\log(1+c n)}{n^2}$$ but dont know what to do next.",,"['real-analysis', 'derivatives']"
64,Bezier Curves and Acceleration,Bezier Curves and Acceleration,,"So I'm working on a program that graphs a bezier curve by manipulating the control points.  This curve represents the velocity of something over time; I also want the option manipulate it all in terms of acceleration in respect to that velocity.  My initial thought was to take the derivative of the bezier curve function and plot that with the same control points. In my program it works, however I think the math doesn't really do what I want. This nice article has helped me on the math so far, but the problem I'm facing is that the derivative curve is simply ""off."" The graphs below are velocity on the Y axis and time on the X axis. Here's what the graph looks like with my bezier function: Now, the derivative of that looks like this... I'm not sure if my math is off, or if that's simply what the derivative of the bezier function actually does.  But, what I really want to do is take my bezier curve and draw the rate of change over the same domain.  However, the derivative seems to take it out of the domain (when you see my graphs below you'll see how the derivative changes the scope). What can I do to extrapolate the data I'm looking for from this curve (turning velocity over time with a bezier function into acceleration over time)? Here's the code where I'm drawing the point of the graph (it is calculating the curve from 0 to 1 and drawing a point there).  I subract an extra 1 with n (the number of control points) as I calculate in an extra 1 at some point in the code above to be more efficient elsewhere: Bezier function: position $+=$ linearCombination$(n - 1, z) \cdot$ Mathf.Pow$((1.0f - t), n - 1 - z) \cdot$ Mathf.Pow$(t, z) \cdot$ controls[$z$].position; Derivative: position $+=$ linearCombination$( n - 2, z ) \cdot$ Mathf.Pow$(1.0f - t, n - 2 - z) \cdot$ Mathf.Pow $(t, z) \cdot (n - 1) \cdot$ (controls[$z + 1$].position - controls[$z$].position );","So I'm working on a program that graphs a bezier curve by manipulating the control points.  This curve represents the velocity of something over time; I also want the option manipulate it all in terms of acceleration in respect to that velocity.  My initial thought was to take the derivative of the bezier curve function and plot that with the same control points. In my program it works, however I think the math doesn't really do what I want. This nice article has helped me on the math so far, but the problem I'm facing is that the derivative curve is simply ""off."" The graphs below are velocity on the Y axis and time on the X axis. Here's what the graph looks like with my bezier function: Now, the derivative of that looks like this... I'm not sure if my math is off, or if that's simply what the derivative of the bezier function actually does.  But, what I really want to do is take my bezier curve and draw the rate of change over the same domain.  However, the derivative seems to take it out of the domain (when you see my graphs below you'll see how the derivative changes the scope). What can I do to extrapolate the data I'm looking for from this curve (turning velocity over time with a bezier function into acceleration over time)? Here's the code where I'm drawing the point of the graph (it is calculating the curve from 0 to 1 and drawing a point there).  I subract an extra 1 with n (the number of control points) as I calculate in an extra 1 at some point in the code above to be more efficient elsewhere: Bezier function: position $+=$ linearCombination$(n - 1, z) \cdot$ Mathf.Pow$((1.0f - t), n - 1 - z) \cdot$ Mathf.Pow$(t, z) \cdot$ controls[$z$].position; Derivative: position $+=$ linearCombination$( n - 2, z ) \cdot$ Mathf.Pow$(1.0f - t, n - 2 - z) \cdot$ Mathf.Pow $(t, z) \cdot (n - 1) \cdot$ (controls[$z + 1$].position - controls[$z$].position );",,"['derivatives', 'graphing-functions', 'bezier-curve']"
65,trouble with $f'$,trouble with,f',"I'm having a lot of trouble understanding the core concept of finding $f\,'(a)$ or any number in place of $a$. I do not understand what finding $f\,'$ means to begin with, or how to solve them. If somebody could give any information i would be appreciative.","I'm having a lot of trouble understanding the core concept of finding $f\,'(a)$ or any number in place of $a$. I do not understand what finding $f\,'$ means to begin with, or how to solve them. If somebody could give any information i would be appreciative.",,"['calculus', 'limits', 'derivatives']"
66,Justification behind changing coordinates of a differential operator,Justification behind changing coordinates of a differential operator,,"On many websites focused on physics, (say http://skisickness.com/2009/11/20/ ) they like to represent differential operators in different coordinates. I.e. going from the standard basis to polar coordinates they would write: $$\frac{\partial }{\partial x} = \frac{\partial r}{\partial x} \frac{\partial }{\partial r} + \frac{\partial \theta}{\partial x} \frac{\partial }{\partial \theta}.$$ Here is my understanding and I would like some validation or corroboration:  If our function $f$ is assumed to be independent of coordinates (and so it should be in a real life application such as in physics), then the derivatives of $f$ in different basis relate to each other. We know that $x=r\cos\theta$ and $y= r\sin\theta$ and so in an abuse of notation we may write $$f(x,y) = f(r\cos\theta,r\sin\theta) :=g(r,\theta)$$ and rename the $g$ to $f$ in an abuse of notation because we are identifying them as the same output (but with a different basis representing their domains). If the maps between the coordinates are smooth enough (and in this case away from 0), we may use the chain rule to compute $$ \frac{\partial f}{\partial x} = \frac{\partial f}{\partial r}\frac{\partial r}{\partial x}+\frac{\partial f}{\partial \theta}\frac{\partial \theta}{\partial x}$$ By plugging in for $r_x$ and $\theta_x$ and ""erasing""  the $f$ from both sides, we obtain the ""change of variables"" for the differential operator. Now because we know by definition of applying the operator, $$(\frac{\partial r}{\partial x} \frac{\partial }{\partial r} + \frac{\partial \theta}{\partial x} \frac{\partial }{\partial \theta})f =  \frac{\partial f}{\partial r}\frac{\partial r}{\partial x}+\frac{\partial f}{\partial \theta}\frac{\partial \theta}{\partial x},$$ does this serve as sufficient justification for this notation? Further why may we them use such methods algebraically such  $$\frac{\partial^2}{\partial x^2}=(\frac{\partial r}{\partial x} \frac{\partial }{\partial r} + \frac{\partial \theta}{\partial x} \frac{\partial }{\partial \theta})(\frac{\partial r}{\partial x} \frac{\partial }{\partial r} + \frac{\partial \theta}{\partial x} \frac{\partial }{\partial \theta})$$ and expanding keeping in mind left and right multiplication (composition!) may not be commutative?","On many websites focused on physics, (say http://skisickness.com/2009/11/20/ ) they like to represent differential operators in different coordinates. I.e. going from the standard basis to polar coordinates they would write: $$\frac{\partial }{\partial x} = \frac{\partial r}{\partial x} \frac{\partial }{\partial r} + \frac{\partial \theta}{\partial x} \frac{\partial }{\partial \theta}.$$ Here is my understanding and I would like some validation or corroboration:  If our function $f$ is assumed to be independent of coordinates (and so it should be in a real life application such as in physics), then the derivatives of $f$ in different basis relate to each other. We know that $x=r\cos\theta$ and $y= r\sin\theta$ and so in an abuse of notation we may write $$f(x,y) = f(r\cos\theta,r\sin\theta) :=g(r,\theta)$$ and rename the $g$ to $f$ in an abuse of notation because we are identifying them as the same output (but with a different basis representing their domains). If the maps between the coordinates are smooth enough (and in this case away from 0), we may use the chain rule to compute $$ \frac{\partial f}{\partial x} = \frac{\partial f}{\partial r}\frac{\partial r}{\partial x}+\frac{\partial f}{\partial \theta}\frac{\partial \theta}{\partial x}$$ By plugging in for $r_x$ and $\theta_x$ and ""erasing""  the $f$ from both sides, we obtain the ""change of variables"" for the differential operator. Now because we know by definition of applying the operator, $$(\frac{\partial r}{\partial x} \frac{\partial }{\partial r} + \frac{\partial \theta}{\partial x} \frac{\partial }{\partial \theta})f =  \frac{\partial f}{\partial r}\frac{\partial r}{\partial x}+\frac{\partial f}{\partial \theta}\frac{\partial \theta}{\partial x},$$ does this serve as sufficient justification for this notation? Further why may we them use such methods algebraically such  $$\frac{\partial^2}{\partial x^2}=(\frac{\partial r}{\partial x} \frac{\partial }{\partial r} + \frac{\partial \theta}{\partial x} \frac{\partial }{\partial \theta})(\frac{\partial r}{\partial x} \frac{\partial }{\partial r} + \frac{\partial \theta}{\partial x} \frac{\partial }{\partial \theta})$$ and expanding keeping in mind left and right multiplication (composition!) may not be commutative?",,"['analysis', 'derivatives', 'differential-operators']"
67,"Integral of $f(x) \exp(ikx)$ with finite bounds calculated using Fourier transform, and its derivative","Integral of  with finite bounds calculated using Fourier transform, and its derivative",f(x) \exp(ikx),"I have an integral which I need to calculate numerically along the lines of $$ I(k)=\int_0^{L} \exp(i k x)f(x) dx $$ where $x$ and $L$ are real. $f(x)$ is not necessarily periodic and differentiable but not easy to differentiate. It looks remarkably like the Fourier transform of $f(x)$, but with finite bounds, so I'd like to be able to calculate this using a Fast Fourier Transform (FFT), though I suspect that FFT [$f(x)$] will give me $\int^{\infty}_{-\infty} \neq \int^L_0$ . Is there a way around this? I'd also like to be able to calculate $dI/dk$. $f(x)$ is not easy to differentiate. Were $I(k)$ a simple FT, I would say that  $dI/dk =$ FT[$i x f(x)$]. Is this still valid?","I have an integral which I need to calculate numerically along the lines of $$ I(k)=\int_0^{L} \exp(i k x)f(x) dx $$ where $x$ and $L$ are real. $f(x)$ is not necessarily periodic and differentiable but not easy to differentiate. It looks remarkably like the Fourier transform of $f(x)$, but with finite bounds, so I'd like to be able to calculate this using a Fast Fourier Transform (FFT), though I suspect that FFT [$f(x)$] will give me $\int^{\infty}_{-\infty} \neq \int^L_0$ . Is there a way around this? I'd also like to be able to calculate $dI/dk$. $f(x)$ is not easy to differentiate. Were $I(k)$ a simple FT, I would say that  $dI/dk =$ FT[$i x f(x)$]. Is this still valid?",,"['integration', 'numerical-methods', 'fourier-analysis', 'derivatives']"
68,Find derivative of convolution with gaussian,Find derivative of convolution with gaussian,,"Let $A(\sigma)$, $\sigma > 0$ be an operator that acts on bounded continuous functions $f$ on $\mathbb{R}$ by the rule $$    (A(t)f)(x) = \int\limits_{\mathbb{R}} f(y)\frac{1}{\sqrt{2 \pi t}}\exp\left( -\frac{(x-y)^2}{2t} \right)dy, $$ and let $(A(0)f)(x) = f(x)$. Then $(A(t)f)(x) \to f(x)$ when $t \to +0$. But is it true in general that $$    \left(\frac{ A(t) - A(0)}{t}f\right)(x) \to \frac{1}{2}\frac{d^2}{dx^2}f(x), \;\;\; t \to +0 $$ for any $C^2(\mathbb{R})$ bounded function $f$? If $f(x)$ is bounded and continuous on $\mathbb{R}$ then $u(t,x)=(A(t)f)(x)$ is a solution of Cauchy problem for the heat equation: $$    \left\{ \begin{array}{l} u_{t}   =  \frac{1}{2} u_{xx}, \;\;\; t>0, \; x \in \mathbb{R} \\    u(0,x)  =  f(x), \;\;\; x \in \mathbb{R} \end{array} \right. $$ Then $\frac{1}{t}(A(t)-A(0))f(x) = \frac{1}{t}(u(t,x)-u(0,x))$. So the question is equivalent to a question: is it true that for bounded $f(x) \in C^2(\mathbb{R})$ there exists a right derivative $$    \lim\limits_{t\to +0}\frac{u(t,x)-u(0,x)}{t} = u_{t}(+0,x) = \frac{1}{2}u_{xx}(0,x) $$ so that the equation $u_t = \frac{1}{2}u_{xx}$ is valid for limit value of $t$.","Let $A(\sigma)$, $\sigma > 0$ be an operator that acts on bounded continuous functions $f$ on $\mathbb{R}$ by the rule $$    (A(t)f)(x) = \int\limits_{\mathbb{R}} f(y)\frac{1}{\sqrt{2 \pi t}}\exp\left( -\frac{(x-y)^2}{2t} \right)dy, $$ and let $(A(0)f)(x) = f(x)$. Then $(A(t)f)(x) \to f(x)$ when $t \to +0$. But is it true in general that $$    \left(\frac{ A(t) - A(0)}{t}f\right)(x) \to \frac{1}{2}\frac{d^2}{dx^2}f(x), \;\;\; t \to +0 $$ for any $C^2(\mathbb{R})$ bounded function $f$? If $f(x)$ is bounded and continuous on $\mathbb{R}$ then $u(t,x)=(A(t)f)(x)$ is a solution of Cauchy problem for the heat equation: $$    \left\{ \begin{array}{l} u_{t}   =  \frac{1}{2} u_{xx}, \;\;\; t>0, \; x \in \mathbb{R} \\    u(0,x)  =  f(x), \;\;\; x \in \mathbb{R} \end{array} \right. $$ Then $\frac{1}{t}(A(t)-A(0))f(x) = \frac{1}{t}(u(t,x)-u(0,x))$. So the question is equivalent to a question: is it true that for bounded $f(x) \in C^2(\mathbb{R})$ there exists a right derivative $$    \lim\limits_{t\to +0}\frac{u(t,x)-u(0,x)}{t} = u_{t}(+0,x) = \frac{1}{2}u_{xx}(0,x) $$ so that the equation $u_t = \frac{1}{2}u_{xx}$ is valid for limit value of $t$.",,"['partial-differential-equations', 'derivatives', 'convolution']"
69,How to derive function for least square?,How to derive function for least square?,,"i want to use least square to find x and y that minimize the result of the following function for a series of points (xi,yi) -> (x1,y1), (x2,y2),...: note: y = f(x) E(x,y) = SUM  (y - ((mi*x) - (mi*xi)))^2           i    --------------------                     1 + |mi|^2 where ""mi"" is the slope of f(xi, yi) now my math is very rusty, and i think to use least square to find x and y that minimize the function above i need to find the derivative of the function above in respect to x and y independently where the derivative of each is equal to 0. can someone please show me how to perform derivation on the above function in respect to x and y please? thanks! edit: I did check a few examples on the web, such as the one given below by potato, but here is where i have difficulty: these examples always look for m and b , thing is in my case, i already have m and b, what i need is the x that will give the best curve. in short, the curve im looking for has to be a point on the function f(x) with the optimized slope. This is due to my own shortcomings in calculus, but all examples give are only numerators, but the example i have above contains a denominator. Can someone help me understand how to derive with a denominator please? thanks!","i want to use least square to find x and y that minimize the result of the following function for a series of points (xi,yi) -> (x1,y1), (x2,y2),...: note: y = f(x) E(x,y) = SUM  (y - ((mi*x) - (mi*xi)))^2           i    --------------------                     1 + |mi|^2 where ""mi"" is the slope of f(xi, yi) now my math is very rusty, and i think to use least square to find x and y that minimize the function above i need to find the derivative of the function above in respect to x and y independently where the derivative of each is equal to 0. can someone please show me how to perform derivation on the above function in respect to x and y please? thanks! edit: I did check a few examples on the web, such as the one given below by potato, but here is where i have difficulty: these examples always look for m and b , thing is in my case, i already have m and b, what i need is the x that will give the best curve. in short, the curve im looking for has to be a point on the function f(x) with the optimized slope. This is due to my own shortcomings in calculus, but all examples give are only numerators, but the example i have above contains a denominator. Can someone help me understand how to derive with a denominator please? thanks!",,"['optimization', 'derivatives']"
70,Discuss the differentiability of $y=\sin(\pi(x-[x]))$,Discuss the differentiability of,y=\sin(\pi(x-[x])),"Discuss the differentiability of $y=\sin(\pi(x-[x]))$ in $(-\frac{\pi}{2},\frac{\pi}{2})$, where $[x]$ is the largest integer in x which is $\leq x$. $$ y = \left\{         \begin{array}{ll}             \sin(\pi(x+2)), & \quad -\frac{\pi}{2}<x<-1 \\             \sin(\pi(x+1)), & \quad -1\leq x<0 \\             \sin(\pi x), &\;\;\; \quad 0\leq x<1 \\ \sin(\pi(x-1)), & \:\:\:\:\quad 1\leq x<\frac{\pi}{2}         \end{array}     \right. $$","Discuss the differentiability of $y=\sin(\pi(x-[x]))$ in $(-\frac{\pi}{2},\frac{\pi}{2})$, where $[x]$ is the largest integer in x which is $\leq x$. $$ y = \left\{         \begin{array}{ll}             \sin(\pi(x+2)), & \quad -\frac{\pi}{2}<x<-1 \\             \sin(\pi(x+1)), & \quad -1\leq x<0 \\             \sin(\pi x), &\;\;\; \quad 0\leq x<1 \\ \sin(\pi(x-1)), & \:\:\:\:\quad 1\leq x<\frac{\pi}{2}         \end{array}     \right. $$",,"['real-analysis', 'derivatives']"
71,Suppose the function $f:\mathbb R \rightarrow \mathbb R$ has left and right derivatives at $0$.,Suppose the function  has left and right derivatives at .,f:\mathbb R \rightarrow \mathbb R 0,"I have been trying to solve the following problem: Suppose the function $f:\mathbb R \rightarrow \mathbb R$ has left and right derivatives at $0$.Then at $x=0$, which of the following options is correct? (a)$f$ must be continuous but may not be differentiable, (b)$f$ need not be continuous but must be left continuous  or right continuous, (c)$f$ must be differentiable, (d)if $f$ is continuous then $f$ must be differentiable. Could someone point me in the right direction.Thanks in advance for your time.","I have been trying to solve the following problem: Suppose the function $f:\mathbb R \rightarrow \mathbb R$ has left and right derivatives at $0$.Then at $x=0$, which of the following options is correct? (a)$f$ must be continuous but may not be differentiable, (b)$f$ need not be continuous but must be left continuous  or right continuous, (c)$f$ must be differentiable, (d)if $f$ is continuous then $f$ must be differentiable. Could someone point me in the right direction.Thanks in advance for your time.",,"['real-analysis', 'analysis']"
72,How to find $f'$ from the definition of derivative?,How to find  from the definition of derivative?,f',How to find the derivative of this function: $f(x) = e^{2x}$ - using definition of derivative: \begin{equation}     f'(x) = \lim_{h\to0}\dfrac{f(x + h) - f(x)}{h} \end{equation},How to find the derivative of this function: $f(x) = e^{2x}$ - using definition of derivative: \begin{equation}     f'(x) = \lim_{h\to0}\dfrac{f(x + h) - f(x)}{h} \end{equation},,['derivatives']
73,Differentiation; trouble finding $\frac{dy}{dx}$ in terms of $y$,Differentiation; trouble finding  in terms of,\frac{dy}{dx} y,"I'm self-studying through some exercises on differentiation, and have found a section which I've gotten almost every question wrong.  Can anyone help me find out where I'm going wrong? An example of one of the questions and my attempt to answer it is Q: Find $\frac{dy}{dx}$ in terms of $y$ when $x = 4y^5-8\sqrt{y}$ My answer: $$x = 4y^5-8\sqrt{y} = 4y^5 - 8y^\frac{1}{2}$$ $$\frac{dx}{dy} = 20y^4 - 4y^{-\frac{1}{2}}$$ $$ = 20y^4-\frac{4}{\sqrt{y}}$$ since:  $$ \frac{dy}{dx} = \frac{1}{\frac{dx}{dy}} \\ \therefore \frac{dy}{dx} = \frac{1}{20y^4}-\frac {y^{-\frac{1}{2}}}{4} \\ = \frac {4 - 20(y^{-\frac{1}{2}})}{80y^4} \\ = \frac {1 - 5(y^{-\frac{1}{2}})}{20y^4} $$ But the listed answer is: $$\frac{y^\frac{1}{2}}{4(5y^\frac{9}{2}-1)}$$ I suspect that due to the fact I've gotten only 2 out of the 6 questions right I'm doing something fundamentally wrong!  Can anyone help me figure out either where I'm going wrong, or what I'm missing?","I'm self-studying through some exercises on differentiation, and have found a section which I've gotten almost every question wrong.  Can anyone help me find out where I'm going wrong? An example of one of the questions and my attempt to answer it is Q: Find $\frac{dy}{dx}$ in terms of $y$ when $x = 4y^5-8\sqrt{y}$ My answer: $$x = 4y^5-8\sqrt{y} = 4y^5 - 8y^\frac{1}{2}$$ $$\frac{dx}{dy} = 20y^4 - 4y^{-\frac{1}{2}}$$ $$ = 20y^4-\frac{4}{\sqrt{y}}$$ since:  $$ \frac{dy}{dx} = \frac{1}{\frac{dx}{dy}} \\ \therefore \frac{dy}{dx} = \frac{1}{20y^4}-\frac {y^{-\frac{1}{2}}}{4} \\ = \frac {4 - 20(y^{-\frac{1}{2}})}{80y^4} \\ = \frac {1 - 5(y^{-\frac{1}{2}})}{20y^4} $$ But the listed answer is: $$\frac{y^\frac{1}{2}}{4(5y^\frac{9}{2}-1)}$$ I suspect that due to the fact I've gotten only 2 out of the 6 questions right I'm doing something fundamentally wrong!  Can anyone help me figure out either where I'm going wrong, or what I'm missing?",,"['derivatives', 'self-learning']"
74,Differentiability and extrema - counterexamples for a few statements,Differentiability and extrema - counterexamples for a few statements,,"All the following statement are wrong. Give a counterexample $f:(-1,1)\to\mathbb{R}$ for each statement and explain why your function $f$ contradicts with the statement. If $f'(x)$ exists for all $x\neq 0$ and $\lim\limits_{x\to 0}f'(x)$ exists too, then $f$ is differentiable at $x=0$. If $f$ is not differentiable at $x=0$ and therefore $f'(0)= 0$ is false then $f$ has no local extremum at $x=0$. If $f$ is continuously differentiable in $(-1,1)$ and $f'(0)=0$ then $x=0$ is a local extremum of $f$. If $f$ is continuously differentiable in $(-1,1)$ and $x=0$ is a local maximum then $f''(0)$ exists and $f''(0)<0$. If $f$ is differentiable and strictly monotonically increasing then $f'(x)>0$. $f(x)=|x|$ was my first thought, but $f'(x)=1$ for $x>0$ and $f'(x)=-1$ for $x<0$ and i am unsure whether the limit for $x\to 0$ actually exists. Let $f(x)=|x|$ then $f$ is not differentiable at $x=0$ however $x=0$ is a local minimum. Let $f(x)=x^3$ then $f''(0)=0$ and $f$ has therefore no local extremum at $x=0$. Let $f(x)=-x^4$ then $f''(x)=-12x^2$ and $f''(0)=0$. Let $f(x)=x^3$ yields $f'(x)=3x^2$ and with $x=0$ we get $f'(0)=0$. I would like to know which function I could use for 1. and whether my other examples are right.","All the following statement are wrong. Give a counterexample $f:(-1,1)\to\mathbb{R}$ for each statement and explain why your function $f$ contradicts with the statement. If $f'(x)$ exists for all $x\neq 0$ and $\lim\limits_{x\to 0}f'(x)$ exists too, then $f$ is differentiable at $x=0$. If $f$ is not differentiable at $x=0$ and therefore $f'(0)= 0$ is false then $f$ has no local extremum at $x=0$. If $f$ is continuously differentiable in $(-1,1)$ and $f'(0)=0$ then $x=0$ is a local extremum of $f$. If $f$ is continuously differentiable in $(-1,1)$ and $x=0$ is a local maximum then $f''(0)$ exists and $f''(0)<0$. If $f$ is differentiable and strictly monotonically increasing then $f'(x)>0$. $f(x)=|x|$ was my first thought, but $f'(x)=1$ for $x>0$ and $f'(x)=-1$ for $x<0$ and i am unsure whether the limit for $x\to 0$ actually exists. Let $f(x)=|x|$ then $f$ is not differentiable at $x=0$ however $x=0$ is a local minimum. Let $f(x)=x^3$ then $f''(0)=0$ and $f$ has therefore no local extremum at $x=0$. Let $f(x)=-x^4$ then $f''(x)=-12x^2$ and $f''(0)=0$. Let $f(x)=x^3$ yields $f'(x)=3x^2$ and with $x=0$ we get $f'(0)=0$. I would like to know which function I could use for 1. and whether my other examples are right.",,"['real-analysis', 'derivatives']"
75,Derivative of the composition of two differentiable functions,Derivative of the composition of two differentiable functions,,"Calculate  $$ \frac{\mathrm d}{\mathrm dt} f (g(t^2),g(t^4)), $$ where $f$ is a differentiable function of two variables and $g$ is a differentiable function of one variable. Your answer should be expressed in terms of $f, g$ and their derivatives and/or partial derivatives. I am assuming it is a partial derivatives question. I have never encountered one like this before. Any help would be much appreciated.","Calculate  $$ \frac{\mathrm d}{\mathrm dt} f (g(t^2),g(t^4)), $$ where $f$ is a differentiable function of two variables and $g$ is a differentiable function of one variable. Your answer should be expressed in terms of $f, g$ and their derivatives and/or partial derivatives. I am assuming it is a partial derivatives question. I have never encountered one like this before. Any help would be much appreciated.",,['derivatives']
76,Find the maximum rate of change of f at the origin,Find the maximum rate of change of f at the origin,,"The directional derivatives of $f$ at the origin in the directions of  $\overrightarrow v =\langle1,-1\rangle$ and $\overrightarrow w =\langle\sqrt{3}, 1\rangle$ are $-\sqrt{2}$ and $4 + 3\sqrt{3}$ respectively. Find the maximum rate of change of $f$ at the origin. Can someone help me answer? Or give me an idea or the steps I should do to answer this? Thank you!","The directional derivatives of $f$ at the origin in the directions of  $\overrightarrow v =\langle1,-1\rangle$ and $\overrightarrow w =\langle\sqrt{3}, 1\rangle$ are $-\sqrt{2}$ and $4 + 3\sqrt{3}$ respectively. Find the maximum rate of change of $f$ at the origin. Can someone help me answer? Or give me an idea or the steps I should do to answer this? Thank you!",,"['calculus', 'derivatives']"
77,Symbol for derivative in MAPLE,Symbol for derivative in MAPLE,,"I am trying to use MAPLE to do some computations involving system of equations which terms are derivatives of functions. When I type diff(alpha(x),x) it shows $\frac{d}{dx}\alpha(x)$. Is it possible to change this notation to obtain something like this: $\alpha'(x)$?","I am trying to use MAPLE to do some computations involving system of equations which terms are derivatives of functions. When I type diff(alpha(x),x) it shows $\frac{d}{dx}\alpha(x)$. Is it possible to change this notation to obtain something like this: $\alpha'(x)$?",,"['derivatives', 'maple', 'symbolic-computation']"
78,Calculus 1- Find directly the derivative of a function f.,Calculus 1- Find directly the derivative of a function f.,,The following limit represents the derivative of a function $f$ at a point $a$. Evaluate the limit. $$\lim\limits_{h \to 0 } \frac{\sin^2\left(\frac\pi 4+h \right)-\frac 1 2} h$$,The following limit represents the derivative of a function $f$ at a point $a$. Evaluate the limit. $$\lim\limits_{h \to 0 } \frac{\sin^2\left(\frac\pi 4+h \right)-\frac 1 2} h$$,,"['calculus', 'derivatives']"
79,Derivative of $\sqrt {xy}$?,Derivative of ?,\sqrt {xy},"$\sqrt{|xy|} = 1$ Attempting to find the derivative gives me $$\frac12(xy)^{-1/2}\left(x\frac{dy}{dx} + y\right) = 0$$ But I haven't figured out how to simplify this further. My teacher says that that's all I'll need to know, but I want to understand how the derivative of $\sqrt {xy} = 1$ is $-\frac{y}x$. Edited to explain that I know the whole thing equals zero, but how do I solve for (dy/dx)? Attempts to solve get me this far:","$\sqrt{|xy|} = 1$ Attempting to find the derivative gives me $$\frac12(xy)^{-1/2}\left(x\frac{dy}{dx} + y\right) = 0$$ But I haven't figured out how to simplify this further. My teacher says that that's all I'll need to know, but I want to understand how the derivative of $\sqrt {xy} = 1$ is $-\frac{y}x$. Edited to explain that I know the whole thing equals zero, but how do I solve for (dy/dx)? Attempts to solve get me this far:",,"['calculus', 'derivatives']"
80,"$f (x, y)$ is given by $f (x, y) = (x^2 - 5x\cdot y)\cdot e^y$",is given by,"f (x, y) f (x, y) = (x^2 - 5x\cdot y)\cdot e^y","These are the question to that function that I'm struggling with: Find the partial derivatives of first and second order of $f(x, y)$. Find the stationary points of $f(x, y)$ and determines for each point on  it/they are a local maximum point, the local minimum point or saddle point. Is it possible to say something about the function has maximum and minimum values ​​based on the information you have found? I've tried over and over and I'm getting real frustrated. It's a bonus problem that I really don't have to do, but I'd like to anyway. What I got on first problem: First order: $f'_x(x,y) = (2x-5y)\cdot e^y$ and $f'_y(x,y)= -5x\cdot e^y$. Correct?","These are the question to that function that I'm struggling with: Find the partial derivatives of first and second order of $f(x, y)$. Find the stationary points of $f(x, y)$ and determines for each point on  it/they are a local maximum point, the local minimum point or saddle point. Is it possible to say something about the function has maximum and minimum values ​​based on the information you have found? I've tried over and over and I'm getting real frustrated. It's a bonus problem that I really don't have to do, but I'd like to anyway. What I got on first problem: First order: $f'_x(x,y) = (2x-5y)\cdot e^y$ and $f'_y(x,y)= -5x\cdot e^y$. Correct?",,['derivatives']
81,Functional Derivative,Functional Derivative,,"I am completely confused about calculating the (infinitesimal) change of an expression that depends on a function, when I vary the function. Consider the following: $$ U(x;\rho)=\int_{0}^{x}\left[1-\int_{s}^{1}F(\rho(\xi))f(\xi)d\xi\right]^{n-1}ds $$ where $F:[0,1]\rightarrow [0,1]$ is continuous and increasing; $f(x)=\frac{dF(x)}{dx}$, $x\in[0,1]$, and $n>2$. Fix the value of $x$. Then, the above expression becomes a functional (?). Now, I want to compute the change in $U$ for a fix $x$ when the function $\rho$ varies. According to me, this is what the resulting expression should look like: $$ \frac{dU(\rho)}{d\rho}=(n-1)\int_{0}^{x}\left[1-\int_{s}^{1}F(\rho(\xi))f(\xi)d\xi\right]^{n-2}\left(\int_{s}^{1}f(\rho(\xi))f(\xi)d\xi\right)ds $$ Can someone please tell me if what I'm doing is correct? My confusion stems from the fact that for a given value of $x$, the above expression becomes a functional and I am not really confident about differentiating functionals. Any help is really, really appreciated!!!","I am completely confused about calculating the (infinitesimal) change of an expression that depends on a function, when I vary the function. Consider the following: $$ U(x;\rho)=\int_{0}^{x}\left[1-\int_{s}^{1}F(\rho(\xi))f(\xi)d\xi\right]^{n-1}ds $$ where $F:[0,1]\rightarrow [0,1]$ is continuous and increasing; $f(x)=\frac{dF(x)}{dx}$, $x\in[0,1]$, and $n>2$. Fix the value of $x$. Then, the above expression becomes a functional (?). Now, I want to compute the change in $U$ for a fix $x$ when the function $\rho$ varies. According to me, this is what the resulting expression should look like: $$ \frac{dU(\rho)}{d\rho}=(n-1)\int_{0}^{x}\left[1-\int_{s}^{1}F(\rho(\xi))f(\xi)d\xi\right]^{n-2}\left(\int_{s}^{1}f(\rho(\xi))f(\xi)d\xi\right)ds $$ Can someone please tell me if what I'm doing is correct? My confusion stems from the fact that for a given value of $x$, the above expression becomes a functional and I am not really confident about differentiating functionals. Any help is really, really appreciated!!!",,"['functional-analysis', 'derivatives']"
82,Determining values where a function is not differentiable,Determining values where a function is not differentiable,,"Given $$g(x) =  \begin{cases} -1-2x & \text{if }x< -1,\\ x^2 & \text{if }-1\leq x\leq1,\\ x & \text{if }x>1, \end{cases} $$ determine at which values $g(x)$ is differentiable. The approach I have taken with this question is to determine the values at which it is not differentiable, which will tell me all other values will be. I know that the function will not be differentiable where the limit at a given value does not exist. If I differentiate this function I get: $$ g'(x) =  \begin{cases} -2 & \text{if }x< -1,\\ 2x & \text{if }-1\leq x\leq1,\\ 0 & \text{if }x>1. \end{cases} $$ I am a little bit lost as to how to proceed with this question - if I can show that the left hand and right hand limits disagree, then I can determine where the function is not differentiable, and therefore where it is differentiable. Am I heading in the right direction here?","Given $$g(x) =  \begin{cases} -1-2x & \text{if }x< -1,\\ x^2 & \text{if }-1\leq x\leq1,\\ x & \text{if }x>1, \end{cases} $$ determine at which values $g(x)$ is differentiable. The approach I have taken with this question is to determine the values at which it is not differentiable, which will tell me all other values will be. I know that the function will not be differentiable where the limit at a given value does not exist. If I differentiate this function I get: $$ g'(x) =  \begin{cases} -2 & \text{if }x< -1,\\ 2x & \text{if }-1\leq x\leq1,\\ 0 & \text{if }x>1. \end{cases} $$ I am a little bit lost as to how to proceed with this question - if I can show that the left hand and right hand limits disagree, then I can determine where the function is not differentiable, and therefore where it is differentiable. Am I heading in the right direction here?",,"['calculus', 'derivatives']"
83,The definition of a directional derivative,The definition of a directional derivative,,"We're given that for $e \in \mathbb{R}^2$ the directional derivative of $u$ in the direction of $e$ is,  $$\frac{\partial u}{\partial e}(x,t):= \lim_{h \to 0}\frac{u((x,t) + he) - u(x,t)}{h} = \frac{d}{dh}u((x,t) + he)|_{h=0}$$ and don't understand how they managed to jump from the 'middle' to the 'last' equation in this directional derivative definition. Could someone break this down for me? Cheers!","We're given that for $e \in \mathbb{R}^2$ the directional derivative of $u$ in the direction of $e$ is,  $$\frac{\partial u}{\partial e}(x,t):= \lim_{h \to 0}\frac{u((x,t) + he) - u(x,t)}{h} = \frac{d}{dh}u((x,t) + he)|_{h=0}$$ and don't understand how they managed to jump from the 'middle' to the 'last' equation in this directional derivative definition. Could someone break this down for me? Cheers!",,['partial-differential-equations']
84,Differentation under an integral,Differentation under an integral,,"Let $f(x)$ be an integrable (not necessarily differentiable) function on $[a,\infty)$. Let $g(x,y)$ be differentiable on $[a,\infty)\times [c,d)$. I can also assume that the partial derivative of $g(x,y)$ with respect to $y$ is continuous. Define $$h(y) = \int_a^\infty g(x,y)\,f(x)\,dx.$$ When calculating the differentiation $h'(y)$ for $y\in[c,d)$, can I always put the differentiation under the integral sign? (If not, let me know if it will work with any additional ""reasonable"" assumptions.)","Let $f(x)$ be an integrable (not necessarily differentiable) function on $[a,\infty)$. Let $g(x,y)$ be differentiable on $[a,\infty)\times [c,d)$. I can also assume that the partial derivative of $g(x,y)$ with respect to $y$ is continuous. Define $$h(y) = \int_a^\infty g(x,y)\,f(x)\,dx.$$ When calculating the differentiation $h'(y)$ for $y\in[c,d)$, can I always put the differentiation under the integral sign? (If not, let me know if it will work with any additional ""reasonable"" assumptions.)",,"['calculus', 'integration']"
85,Partial derivatives and orthogonality with polar-coordinates,Partial derivatives and orthogonality with polar-coordinates,,"We are stuck with this question here because I cannot understand the following results. I find it hard to visualize this, let alone deduce from that. How to do it? Objective to Attack The closely Related Problems with Orthogonal Basis and Dot Products In Polar-coordinates $\left(\hat{e}_{r}\partial_{r}\right) \cdot \left(\frac{1}{r}\hat{e}_{\theta}\partial_{\theta}\right)= 0$ $\left(\frac{1}{r}\hat{e}_{\theta}\partial_{\theta}\right)       \cdot \left(\hat{e}_{r}\partial_{r}\right)           = \frac{1}{r} \partial_r$ $\partial_\theta \hat e_r = \hat e_\theta$ $\partial_\theta \hat e_\theta = -\hat e_r$ Trials I have some errors there, related to 3-4 apparently. $$ \left(\hat{e} {r}\partial {r}\right) \cdot    \left(\frac{1}{r}\hat{e} {\theta}\partial {\theta}\right)     = \left(\hat{e} {r}\partial {r}\right) \cdot \frac{1}{r}+ \left(\hat{e} {r}\partial {r}\right) \cdot \left( \hat{e} {\theta}\partial {\theta} \right) \not \frac{-\hat{e}_r}{r^2}+ \left(\hat{e} {r}\cdot\hat{e} \theta \right) \partial_{r} \partial_{\theta} $$ Perhaps Related Explain Dot product with Partial derivatives in Polar-coordinates Orthonormal vectors in Polar coordinates, show $\hat{e}_R=\frac{(x,y,z)}{r}$ Visual Ways to Remember Cross products of Unit vectors? Cross-product in $\mathbb F^3$?","We are stuck with this question here because I cannot understand the following results. I find it hard to visualize this, let alone deduce from that. How to do it? Objective to Attack The closely Related Problems with Orthogonal Basis and Dot Products In Polar-coordinates $\left(\hat{e}_{r}\partial_{r}\right) \cdot \left(\frac{1}{r}\hat{e}_{\theta}\partial_{\theta}\right)= 0$ $\left(\frac{1}{r}\hat{e}_{\theta}\partial_{\theta}\right)       \cdot \left(\hat{e}_{r}\partial_{r}\right)           = \frac{1}{r} \partial_r$ $\partial_\theta \hat e_r = \hat e_\theta$ $\partial_\theta \hat e_\theta = -\hat e_r$ Trials I have some errors there, related to 3-4 apparently. $$ \left(\hat{e} {r}\partial {r}\right) \cdot    \left(\frac{1}{r}\hat{e} {\theta}\partial {\theta}\right)     = \left(\hat{e} {r}\partial {r}\right) \cdot \frac{1}{r}+ \left(\hat{e} {r}\partial {r}\right) \cdot \left( \hat{e} {\theta}\partial {\theta} \right) \not \frac{-\hat{e}_r}{r^2}+ \left(\hat{e} {r}\cdot\hat{e} \theta \right) \partial_{r} \partial_{\theta} $$ Perhaps Related Explain Dot product with Partial derivatives in Polar-coordinates Orthonormal vectors in Polar coordinates, show $\hat{e}_R=\frac{(x,y,z)}{r}$ Visual Ways to Remember Cross products of Unit vectors? Cross-product in $\mathbb F^3$?",,"['derivatives', 'polar-coordinates']"
86,how to calculate derivative of combined functions?,how to calculate derivative of combined functions?,,"Well, I know there is chain rule to calculate derivations like $$\frac{d f(g(x))}{dx}=g'(x)*f'(g(x))$$But I'm wondering how did they get to this formula, and if you can expand it to derivation of functions with more than one parameter. I mean something like $$\frac {f(g(x),h(x))}{dx}$$","Well, I know there is chain rule to calculate derivations like $$\frac{d f(g(x))}{dx}=g'(x)*f'(g(x))$$But I'm wondering how did they get to this formula, and if you can expand it to derivation of functions with more than one parameter. I mean something like $$\frac {f(g(x),h(x))}{dx}$$",,['derivatives']
87,Changing the argument for a higher order derivative,Changing the argument for a higher order derivative,,"I start with the following: $$\frac{d^n}{dx^n} \left[(1-x^2)^{n+\alpha-1/2}\right]$$ Which is part of the Rodrigues definition of a Gegenbauer polynomial.  Gegenbauer polynomials are also useful in terms of trigonometric functions so I want to use the substitution $x = \cos\theta$, which is the usual way of doing it.  However, I'm stuck as to how this works for the Rodrigues definition, because it gives me a derivative with respect to $\cos\theta$ instead of a derivative with respect to $\theta$: $$\frac{d^n}{d(\cos\theta)^n} \left[(\sin^2\theta)^{n+\alpha-1/2}\right]$$ QUESTION: Is there a way to write this as $\dfrac{d^n}{d\theta^n}[\text{something}]$? I have read some about Faa di Bruno's formula for the $n$-th order derivative of a composition of functions but it doesn't seem to do what I want to do. Also, for n=1 there is the identity, from the chain rule, $\dfrac{d}{d(\cos\theta)} \left[(\sin^2\theta)^{n+\alpha-1/2}\right]=\frac{\frac{d}{d\theta} \left[(\sin^2\theta)^{n+\alpha-1/2}\right]}{\frac{d}{d\theta} \left[\cos\theta\right]}$, but this doesn't hold for higher order derivatives.  Any ideas?","I start with the following: $$\frac{d^n}{dx^n} \left[(1-x^2)^{n+\alpha-1/2}\right]$$ Which is part of the Rodrigues definition of a Gegenbauer polynomial.  Gegenbauer polynomials are also useful in terms of trigonometric functions so I want to use the substitution $x = \cos\theta$, which is the usual way of doing it.  However, I'm stuck as to how this works for the Rodrigues definition, because it gives me a derivative with respect to $\cos\theta$ instead of a derivative with respect to $\theta$: $$\frac{d^n}{d(\cos\theta)^n} \left[(\sin^2\theta)^{n+\alpha-1/2}\right]$$ QUESTION: Is there a way to write this as $\dfrac{d^n}{d\theta^n}[\text{something}]$? I have read some about Faa di Bruno's formula for the $n$-th order derivative of a composition of functions but it doesn't seem to do what I want to do. Also, for n=1 there is the identity, from the chain rule, $\dfrac{d}{d(\cos\theta)} \left[(\sin^2\theta)^{n+\alpha-1/2}\right]=\frac{\frac{d}{d\theta} \left[(\sin^2\theta)^{n+\alpha-1/2}\right]}{\frac{d}{d\theta} \left[\cos\theta\right]}$, but this doesn't hold for higher order derivatives.  Any ideas?",,"['calculus', 'trigonometry', 'polynomials', 'derivatives']"
88,Why $\nabla f$ do not exactly coincide with $D f$ (it's its transpose),Why  do not exactly coincide with  (it's its transpose),\nabla f D f,"Is there any reason (historical, or of any other kind) to why $$\nabla f= \begin{bmatrix}\frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \\ \frac{\partial f}{\partial z} \\ \end{bmatrix}$$ for a three variables, scalar-valued function $f$ is the transpose of the total derivative of $f$ (and is not exactly equal to the total derivative of $f$ ) ? Also, as an auxiliary: is my following reasoning right, in finding the Hessian of $f$ ? I perform the matrix product between the $3\times 1$ nabla operator matrix: $$\nabla = \begin{bmatrix}\frac{\partial}{\partial x} \\ \frac{\partial}{\partial y} \\ \frac{\partial}{\partial z} \end{bmatrix}$$ and the $1\times 3$ total derivative matrix. I came up with this ""out of luck"", but I have to say that I never know when to use $\nabla$ or instead its transpose. I am also further confused by the fact that $H$ is informally described as $\nabla ^2 $ in its Wikipedia article. Is it supposed to mean $\nabla$ applied to $\nabla f$ ? PS: $f$ is assumed to be twice differentiable.","Is there any reason (historical, or of any other kind) to why for a three variables, scalar-valued function is the transpose of the total derivative of (and is not exactly equal to the total derivative of ) ? Also, as an auxiliary: is my following reasoning right, in finding the Hessian of ? I perform the matrix product between the nabla operator matrix: and the total derivative matrix. I came up with this ""out of luck"", but I have to say that I never know when to use or instead its transpose. I am also further confused by the fact that is informally described as in its Wikipedia article. Is it supposed to mean applied to ? PS: is assumed to be twice differentiable.",\nabla f= \begin{bmatrix}\frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \\ \frac{\partial f}{\partial z} \\ \end{bmatrix} f f f f 3\times 1 \nabla = \begin{bmatrix}\frac{\partial}{\partial x} \\ \frac{\partial}{\partial y} \\ \frac{\partial}{\partial z} \end{bmatrix} 1\times 3 \nabla H \nabla ^2  \nabla \nabla f f,"['derivatives', 'hessian-matrix', 'transpose']"
89,Justify if there exist a differentiable function f such that $|f(x)|<2$ and $f(x)f'(x)\geq \sin(x)$,Justify if there exist a differentiable function f such that  and,|f(x)|<2 f(x)f'(x)\geq \sin(x),"The problem states: Justify if there exist a differentiable function f such that $|f(x)|<2$ and $f(x)f'(x)\geq \sin(x)$ for every $x\in \mathbb{R}$ . What I got by another question I made about a related result: $f(x)f'(x)=(\frac 12f(x)^2)'$ so $(\frac 12f(x)^2)'=\sin(x)+c(x)\implies \frac 12f(x)^2=C(x)-\cos(x)$ now, $|f|<2$ so $\sqrt{f(x)^2}<2$ or equivalently $\frac{f(x)^2}{2}<2$ . Putting this result on the above equations gives $C(x)<2+\cos(x)$ . In other words, we got a function that has to be lower than 1 in some points like $x=\pi$ , but, at the same time $C(x)-\cos(x)\geq 0$ so $C(x)\geq \cos(x)$ . Finishing my reasoning, we are looking for an increasing function $C(x)$ such that $C(0)\geq1$ and $C(\pi)<1$ which is impossible. Is this right?","The problem states: Justify if there exist a differentiable function f such that and for every . What I got by another question I made about a related result: so now, so or equivalently . Putting this result on the above equations gives . In other words, we got a function that has to be lower than 1 in some points like , but, at the same time so . Finishing my reasoning, we are looking for an increasing function such that and which is impossible. Is this right?",|f(x)|<2 f(x)f'(x)\geq \sin(x) x\in \mathbb{R} f(x)f'(x)=(\frac 12f(x)^2)' (\frac 12f(x)^2)'=\sin(x)+c(x)\implies \frac 12f(x)^2=C(x)-\cos(x) |f|<2 \sqrt{f(x)^2}<2 \frac{f(x)^2}{2}<2 C(x)<2+\cos(x) x=\pi C(x)-\cos(x)\geq 0 C(x)\geq \cos(x) C(x) C(0)\geq1 C(\pi)<1,['derivatives']
90,"Formula for higher-order derivatives of the logarithm of a function, $\frac {d^m}{dx^m} \log h(x)$","Formula for higher-order derivatives of the logarithm of a function,",\frac {d^m}{dx^m} \log h(x),"I am trying to see if I can get a general formula for the $m$ th derivative of the logarithm of a function. Specifically, I am trying to find the $m$ th derivative of $g(x) = \log h(x)$ . Let $h^{(m)}(x) = \frac {d^m}{dx^m} h(x)$ denote the $m$ th derivative of $h(x)$ . Then, we see that the first derivative is \begin{equation} \frac d{dx} g(x) = \frac{h^{(1)}(x)}{h(x)} \end{equation} while the second derivative is \begin{equation} \frac {d^2}{dx^2}g(x) = \frac{h^{(2)}(x)}{h(x)} - \left[\frac{h^{(1)}(x)}{h(x)}\right]^2, \end{equation} and the third derivative is \begin{equation} \frac {d^3}{dx^3}g(x) = \frac{h^{(3)}(x)}{h(x)} - \frac{3h^{(1)}(x)h^{(2)}(x)}{h^2(x)} +  2\left[\frac{h^{(1)}(x)}{h(x)}\right]^3, \end{equation} and the fourth derivative is \begin{equation} \frac {d^4}{dx^4}g(x) = \frac{h^{(4)}(x)}{h(x)} - \frac{h^{(3)}(x)h^{(1)}(x)}{h^2(x)} - 3\left[\frac{h^{(2)}(x)}{h(x)}\right]^2 - 3\frac{h^{(1)}(x)h^{(3)}(x)}{h^2(x)} +   \frac{12[h^{(1)}(x)]^2h^{(2)}(x)}{h^3(x)} - 6\left[\frac{h^{(1)}(x)}{h(x)}\right]^4. \end{equation} But I can not see a way to specify a general formula for the $m$ th order derivative of $g(x)$ . Is there such a general formula possible? Many thanks for any pointers and suggestions!","I am trying to see if I can get a general formula for the th derivative of the logarithm of a function. Specifically, I am trying to find the th derivative of . Let denote the th derivative of . Then, we see that the first derivative is while the second derivative is and the third derivative is and the fourth derivative is But I can not see a way to specify a general formula for the th order derivative of . Is there such a general formula possible? Many thanks for any pointers and suggestions!","m m g(x) = \log h(x) h^{(m)}(x) = \frac {d^m}{dx^m} h(x) m h(x) \begin{equation}
\frac d{dx} g(x) = \frac{h^{(1)}(x)}{h(x)}
\end{equation} \begin{equation}
\frac {d^2}{dx^2}g(x) = \frac{h^{(2)}(x)}{h(x)} - \left[\frac{h^{(1)}(x)}{h(x)}\right]^2,
\end{equation} \begin{equation}
\frac {d^3}{dx^3}g(x) = \frac{h^{(3)}(x)}{h(x)} - \frac{3h^{(1)}(x)h^{(2)}(x)}{h^2(x)} +  2\left[\frac{h^{(1)}(x)}{h(x)}\right]^3,
\end{equation} \begin{equation}
\frac {d^4}{dx^4}g(x) = \frac{h^{(4)}(x)}{h(x)} - \frac{h^{(3)}(x)h^{(1)}(x)}{h^2(x)} - 3\left[\frac{h^{(2)}(x)}{h(x)}\right]^2 - 3\frac{h^{(1)}(x)h^{(3)}(x)}{h^2(x)} +   \frac{12[h^{(1)}(x)]^2h^{(2)}(x)}{h^3(x)} - 6\left[\frac{h^{(1)}(x)}{h(x)}\right]^4.
\end{equation} m g(x)","['derivatives', 'logarithms']"
91,How to find the equation for $x_{\theta}$ in this problem?,How to find the equation for  in this problem?,x_{\theta},"I have the below problem from a research paper that I would like to solve $$\frac{1}{2}(y^2 - \bar{y}^2) = -\gamma\sin\theta-\gamma_{\theta}\cos\theta + a_1 \tag1$$ where, $$a_1 = \gamma\sin\theta +\gamma_{\theta}\cos\theta$$ at $\theta=\bar{\theta}$ . This gives me the following $$y=[2(-\gamma\sin\theta-\gamma_{\theta}\cos\theta + \gamma\sin\bar\theta +\gamma_{\bar\theta}\cos\bar\theta) + \bar{y}^2]^{1/2}$$ Here, $\bar{y}$ is the value of $y$ at some angle $\theta = \bar{\theta} \tag2$ . Now, there are further couple of equations, $$yy_{\theta} = - (\gamma + \gamma_{\theta\theta})\cos\theta \tag3$$ and $$yx_{\theta} = (\gamma + \gamma_{\theta\theta})\sin\theta \tag4$$ Where, $$\gamma(\theta) = 1+\gamma_{d}\cos[4(\theta+\theta_{0})]$$ Here, $\gamma_{d}$ is a constant, and $\theta_{0}$ is some phase angle. I have to substitute the above equation and it gives me: $$yx_{\theta} = \gamma\sin\theta-16\gamma_{d}\cos[4(\theta + \theta_0)]$$ Now, after substituting $y$ from equation(2) into equation(4) I need to find an equation for $x_{\theta}$ such that can be integrated numerically. Here, subscripts denote differentiation, so $\gamma_{\theta\theta}$ is double derivate of $\gamma(\theta)$ w.r.t $\theta$ , and similar for $x_{\theta}$ . Can anyone suggest how to proceed forward and then obtain the equation for $x_{\theta}$ ? I'm stuck at what to do with $\bar{y}$ , how should I eliminate it. The paper doesn't state the final equation for $x_\theta$ , and hence I'm here for a solution. Any suggestions or help appreciated. Source for the paper: here , the problem stated is from equations 2.7, 2.6 and 1.5 in the article.","I have the below problem from a research paper that I would like to solve where, at . This gives me the following Here, is the value of at some angle . Now, there are further couple of equations, and Where, Here, is a constant, and is some phase angle. I have to substitute the above equation and it gives me: Now, after substituting from equation(2) into equation(4) I need to find an equation for such that can be integrated numerically. Here, subscripts denote differentiation, so is double derivate of w.r.t , and similar for . Can anyone suggest how to proceed forward and then obtain the equation for ? I'm stuck at what to do with , how should I eliminate it. The paper doesn't state the final equation for , and hence I'm here for a solution. Any suggestions or help appreciated. Source for the paper: here , the problem stated is from equations 2.7, 2.6 and 1.5 in the article.",\frac{1}{2}(y^2 - \bar{y}^2) = -\gamma\sin\theta-\gamma_{\theta}\cos\theta + a_1 \tag1 a_1 = \gamma\sin\theta +\gamma_{\theta}\cos\theta \theta=\bar{\theta} y=[2(-\gamma\sin\theta-\gamma_{\theta}\cos\theta + \gamma\sin\bar\theta +\gamma_{\bar\theta}\cos\bar\theta) + \bar{y}^2]^{1/2} \bar{y} y \theta = \bar{\theta} \tag2 yy_{\theta} = - (\gamma + \gamma_{\theta\theta})\cos\theta \tag3 yx_{\theta} = (\gamma + \gamma_{\theta\theta})\sin\theta \tag4 \gamma(\theta) = 1+\gamma_{d}\cos[4(\theta+\theta_{0})] \gamma_{d} \theta_{0} yx_{\theta} = \gamma\sin\theta-16\gamma_{d}\cos[4(\theta + \theta_0)] y x_{\theta} \gamma_{\theta\theta} \gamma(\theta) \theta x_{\theta} x_{\theta} \bar{y} x_\theta,"['derivatives', 'partial-derivative', 'mathematical-modeling']"
92,"Can you have a function that is differentiable, yet the jacobian matrix is not continuous?","Can you have a function that is differentiable, yet the jacobian matrix is not continuous?",,"Suppose we have a function $f:\mathbb{R}^n\to\mathbb{R}^m$ that is differentiable at $x\in \mathbb{R}^n$ . We all know that this means that there exists a linear map $D_{f(x)} \in L(\mathbb{R}^n,\mathbb{R}^m)$ such that $f(x+h)$ can be approximated by the affine linear map $f(x) + D_{f(x)}h$ for small values of h. We also know that $D_{f(x)}$ (if it exists) has matrix representation $\partial f(x)$ . I also know that if $\partial f(x)$ is continuous on some open ball around $x$ , then we know that $\partial f(x) = D_{f(x)}$ . My question is, is it possible to have a function $f:\mathbb{R}^n\to\mathbb{R}^m$ Such that $f$ is differentiable at $x \in \mathbb{R}^n$ , and $\partial f(x) = D_{f(x)}$ but $\partial f(x)$ is not continuous at $x$ ? If possible can someone give an example? If not can someone prove it? My guess is that it is possible, it would just be some function whose derivative is not continuous but excited to see what everyone has to say.","Suppose we have a function that is differentiable at . We all know that this means that there exists a linear map such that can be approximated by the affine linear map for small values of h. We also know that (if it exists) has matrix representation . I also know that if is continuous on some open ball around , then we know that . My question is, is it possible to have a function Such that is differentiable at , and but is not continuous at ? If possible can someone give an example? If not can someone prove it? My guess is that it is possible, it would just be some function whose derivative is not continuous but excited to see what everyone has to say.","f:\mathbb{R}^n\to\mathbb{R}^m x\in \mathbb{R}^n D_{f(x)} \in L(\mathbb{R}^n,\mathbb{R}^m) f(x+h) f(x) + D_{f(x)}h D_{f(x)} \partial f(x) \partial f(x) x \partial f(x) = D_{f(x)} f:\mathbb{R}^n\to\mathbb{R}^m f x \in \mathbb{R}^n \partial f(x) = D_{f(x)} \partial f(x) x","['calculus', 'analysis', 'derivatives']"
93,Question regarding the nature of derivative of a function.,Question regarding the nature of derivative of a function.,,"This question is in regard to the following problem The complete range of values of $a$ such that $(\frac{1}{2})^{|x|} = x^2 - a$ is satisfied for maximum of values $x$ is: How i approached this problem is as follows, plugging in 0 in the place of $x$ gives the value of $a = -1$ rearranging the equation. $$a = x^2 - (\frac{1}{2})^{|x|}$$ If we can prove that this function is strictly increasing after $x = 0$ then we can prove that the answer to this question is one of the options. The derivative of the above function is, $$2x - \frac{ln(2)\cdot x}{2^{|x|}\cdot |x|}$$ By plugging in some arbitrary values and some observation it is quite intuitive that this derivative will always be positive from $x = 0$ to $x = \infty$ . However i wanted a concrete proof for this, could someone point me in the right direction, any help would be appreciated.","This question is in regard to the following problem The complete range of values of such that is satisfied for maximum of values is: How i approached this problem is as follows, plugging in 0 in the place of gives the value of rearranging the equation. If we can prove that this function is strictly increasing after then we can prove that the answer to this question is one of the options. The derivative of the above function is, By plugging in some arbitrary values and some observation it is quite intuitive that this derivative will always be positive from to . However i wanted a concrete proof for this, could someone point me in the right direction, any help would be appreciated.",a (\frac{1}{2})^{|x|} = x^2 - a x x a = -1 a = x^2 - (\frac{1}{2})^{|x|} x = 0 2x - \frac{ln(2)\cdot x}{2^{|x|}\cdot |x|} x = 0 x = \infty,['derivatives']
94,What's the product rule for the exponential differential operator?,What's the product rule for the exponential differential operator?,,"So I was thinking say you have a linear differential operator such as the exponential differential one which is renown in some fields in physics: $$e^{\mathrm D_x}\equiv\sum_{n=0}^\infty\frac{\mathrm D_x^n}{n!},\text{ where } \mathrm D_x^n\equiv\frac{\mathrm d^n}{\mathrm dx^n},$$ and you applied it to a product of functions $u(x)\cdot v(x)$ . Then what would the ""product rule"" be for this operator? I.e., $$e^{\mathrm D_x}[u\cdot v]=?$$ APPROACH: Is this it? $$\sum_{n=0}^\infty\frac{1}{n!}\frac{\mathrm d^n}{\mathrm dx^n}(u\cdot v)=\sum_{n=0}^\infty\frac{1}{n!}\sum_{i=0}^n{n\choose i}u^{(n-i)}v^{(i)}=\sum_{n=0}^\infty\sum_{i=0}^n\frac{u^{(n-i)}}{(n-i)!}\frac{v^{(i)}}{i!}\overset{j=n-i}{=}\sum_{i=0}^\infty\sum_{j=0}^\infty\frac{u^{(j)}}{j!}\frac{v^{(i)}}{i!}$$","So I was thinking say you have a linear differential operator such as the exponential differential one which is renown in some fields in physics: and you applied it to a product of functions . Then what would the ""product rule"" be for this operator? I.e., APPROACH: Is this it?","e^{\mathrm D_x}\equiv\sum_{n=0}^\infty\frac{\mathrm D_x^n}{n!},\text{ where } \mathrm D_x^n\equiv\frac{\mathrm d^n}{\mathrm dx^n}, u(x)\cdot v(x) e^{\mathrm D_x}[u\cdot v]=? \sum_{n=0}^\infty\frac{1}{n!}\frac{\mathrm d^n}{\mathrm dx^n}(u\cdot v)=\sum_{n=0}^\infty\frac{1}{n!}\sum_{i=0}^n{n\choose i}u^{(n-i)}v^{(i)}=\sum_{n=0}^\infty\sum_{i=0}^n\frac{u^{(n-i)}}{(n-i)!}\frac{v^{(i)}}{i!}\overset{j=n-i}{=}\sum_{i=0}^\infty\sum_{j=0}^\infty\frac{u^{(j)}}{j!}\frac{v^{(i)}}{i!}","['calculus', 'derivatives', 'taylor-expansion', 'differential-operators']"
95,"Reduce $\frac{d^{n-1}}{dw^{n-1}}\frac{4^{-n/{\sqrt w}}}{\sqrt w}\Big|_1=\frac1{\sqrt\pi}G^{3,0}_{1,3}\left(^{3/2-n}_{0,1/2,1/2};(\ln(2)n)^2\right)$",Reduce,"\frac{d^{n-1}}{dw^{n-1}}\frac{4^{-n/{\sqrt w}}}{\sqrt w}\Big|_1=\frac1{\sqrt\pi}G^{3,0}_{1,3}\left(^{3/2-n}_{0,1/2,1/2};(\ln(2)n)^2\right)","In this answer to Is there any valid complex or just real solution to $\sin(x)^{\cos(x)} = 2$ ? , one must calculate $$\frac{d^{n-1}}{dw^{n-1}}\left.\frac{4^{-\frac{n}{\sqrt w}}}{\sqrt w}\right|_1=\sum_{m=0}^\infty\frac{\Gamma\left(\frac12-\frac m2\right)(-\ln(4)n)^m}{\Gamma\left(\frac32-n-\frac m2\right)m!}=\text H^{1,1}_{1,2}\left(^{\left(\frac12,-\frac12\right)}_{(0,1),\left(n-\frac12,-\frac12\right)};\ln(4)n\right)=\frac1{\sqrt\pi}\text G^{3,0}_{1,3}\left(^{\frac32-n}_{0,\frac12,\frac12};(\ln(2)n)^2\right)\tag1$$ where one can convert Fox H into Meijer G functions using the Wolfram repository function FoxHToMeijerG . @Mariusz Iwaniuk simplified it further using Maple: $$\frac{d^{n-1}}{dw^{n-1}}\left.\frac{4^{-\frac{n}{\sqrt w}}}{\sqrt w}\right|_1 = \frac1{\sqrt\pi}\text G^{3,0}_{1,3}\left(^{\frac32-n}_{0,\frac12,\frac12};(\ln(2)n)^2\right) \\ =\frac{\sqrt\pi}{\Gamma\left(\frac32-n\right)}\,_1\text F_2\left(n-\frac12;\frac12,\frac12;(\ln(2)n)^2\right)\\ +\ln(4)(-1)^n n!\,_1\text F_2\left(n;1,\frac32;(\ln(2)n)^2\right)\tag2$$ which was tested and it matches the derivatives. However, there is seemingly no other way to find this result. MeijerGToHypergeometricPFQ did not work. Also, there is a formula for converting $\text G^{3,0}_{1,3}\left(^{\ \ \ \ \ a_1}_{b_1,b_2,b_3};z\right)$ into a sum of $_1\text F_2$ functions, but part of it involves $\csc(\pi (b_2-b_3))$ which is undefined if $b_3=b_2$ , like in $(1)$ , so $\lim\limits_{b_2\to\frac12}$ must be taken. This problem occurs in other cases where $b_{m+1}=b_{m+j}$ , so understanding how to reduce Meijer G in these cases helps. Is there any way to find $(2)$ without using Maple, like maybe with a Wolfram function or a reduction formula?","In this answer to Is there any valid complex or just real solution to ? , one must calculate where one can convert Fox H into Meijer G functions using the Wolfram repository function FoxHToMeijerG . @Mariusz Iwaniuk simplified it further using Maple: which was tested and it matches the derivatives. However, there is seemingly no other way to find this result. MeijerGToHypergeometricPFQ did not work. Also, there is a formula for converting into a sum of functions, but part of it involves which is undefined if , like in , so must be taken. This problem occurs in other cases where , so understanding how to reduce Meijer G in these cases helps. Is there any way to find without using Maple, like maybe with a Wolfram function or a reduction formula?","\sin(x)^{\cos(x)} = 2 \frac{d^{n-1}}{dw^{n-1}}\left.\frac{4^{-\frac{n}{\sqrt w}}}{\sqrt w}\right|_1=\sum_{m=0}^\infty\frac{\Gamma\left(\frac12-\frac m2\right)(-\ln(4)n)^m}{\Gamma\left(\frac32-n-\frac m2\right)m!}=\text H^{1,1}_{1,2}\left(^{\left(\frac12,-\frac12\right)}_{(0,1),\left(n-\frac12,-\frac12\right)};\ln(4)n\right)=\frac1{\sqrt\pi}\text G^{3,0}_{1,3}\left(^{\frac32-n}_{0,\frac12,\frac12};(\ln(2)n)^2\right)\tag1 \frac{d^{n-1}}{dw^{n-1}}\left.\frac{4^{-\frac{n}{\sqrt w}}}{\sqrt w}\right|_1 =
\frac1{\sqrt\pi}\text G^{3,0}_{1,3}\left(^{\frac32-n}_{0,\frac12,\frac12};(\ln(2)n)^2\right) \\
=\frac{\sqrt\pi}{\Gamma\left(\frac32-n\right)}\,_1\text F_2\left(n-\frac12;\frac12,\frac12;(\ln(2)n)^2\right)\\
+\ln(4)(-1)^n n!\,_1\text F_2\left(n;1,\frac32;(\ln(2)n)^2\right)\tag2 \text G^{3,0}_{1,3}\left(^{\ \ \ \ \ a_1}_{b_1,b_2,b_3};z\right) _1\text F_2 \csc(\pi (b_2-b_3)) b_3=b_2 (1) \lim\limits_{b_2\to\frac12} b_{m+1}=b_{m+j} (2)","['limits', 'derivatives', 'special-functions', 'hypergeometric-function', 'mathematica']"
96,How to take derivative of an arithmetic function?,How to take derivative of an arithmetic function?,,"Arithmetic functions are defined from natural numbers to complex numbers. Therefore, they are not continuous in the analytic sense and consequently cannot be differentiated analytically. However, we do know that some arithmetic functions are asymptotic to analytic functions. That is, when we connect the points of an arithmetic function on the coordinate plane, it looks as though we could take a derivative, as if it were a curve. Is there a definition in the literature for such a derivative? For example, it is accepted that $\frac{d\pi(n)}{dn} \sim \frac{1}{\log n}$ which is a consequence of the Prime Number Theorem. But how can this derivative $\frac{d\pi(n)}{dn}$ be formally expressed? $\pi(n)$ function looks like below: Here you can see it can not be differentiable. But as you scale down the graph, you can see it will look like a curve.","Arithmetic functions are defined from natural numbers to complex numbers. Therefore, they are not continuous in the analytic sense and consequently cannot be differentiated analytically. However, we do know that some arithmetic functions are asymptotic to analytic functions. That is, when we connect the points of an arithmetic function on the coordinate plane, it looks as though we could take a derivative, as if it were a curve. Is there a definition in the literature for such a derivative? For example, it is accepted that which is a consequence of the Prime Number Theorem. But how can this derivative be formally expressed? function looks like below: Here you can see it can not be differentiable. But as you scale down the graph, you can see it will look like a curve.",\frac{d\pi(n)}{dn} \sim \frac{1}{\log n} \frac{d\pi(n)}{dn} \pi(n),"['real-analysis', 'number-theory', 'derivatives', 'arithmetic-functions']"
97,Obtaining a bound on a function given a bound on its derivative,Obtaining a bound on a function given a bound on its derivative,,"Suppose $f\in C^1([0,1])$ satisfies $f(0)=0$ , $|f'(0)| <\infty$ and $|f'(t)| \leq \alpha t^{-1}$ on $(0,1]$ for some fixed positive constant $\alpha$ . I would like to show $f$ satisfies a lower bound on $[0,1]$ depending on $t$ and $\alpha$ , but independent of the value of $f'(0)$ . My guess is that $f(t) \geq c\ln(d)[1-\ln(t+d)]$ for some constants $c\in\mathbb{R}$ and $d>0$ depending only on $\alpha$ , since this satisfies $f(0)=0$ and $f'(t) = -\frac{c\ln(d)}{t+d}$ . I tried using the fundamental theorem of calculus: if $x\in(0,1]$ and $\epsilon<x$ , then $$f(x) = f(\epsilon) + \int_\epsilon^x f'(t)dt \geq f(\epsilon) - \alpha\int_\epsilon^x t^{-1}dt = f(\epsilon) - \alpha\ln(x) + \alpha \ln(\epsilon).$$ As $\epsilon\rightarrow 0$ , we have $f(\epsilon)\rightarrow 0$ , but the $\ln(\epsilon)$ term blows up. So I'm not really sure how to proceed, or if even such a bound is possible. Any help would be appreciated!","Suppose satisfies , and on for some fixed positive constant . I would like to show satisfies a lower bound on depending on and , but independent of the value of . My guess is that for some constants and depending only on , since this satisfies and . I tried using the fundamental theorem of calculus: if and , then As , we have , but the term blows up. So I'm not really sure how to proceed, or if even such a bound is possible. Any help would be appreciated!","f\in C^1([0,1]) f(0)=0 |f'(0)| <\infty |f'(t)| \leq \alpha t^{-1} (0,1] \alpha f [0,1] t \alpha f'(0) f(t) \geq c\ln(d)[1-\ln(t+d)] c\in\mathbb{R} d>0 \alpha f(0)=0 f'(t) = -\frac{c\ln(d)}{t+d} x\in(0,1] \epsilon<x f(x) = f(\epsilon) + \int_\epsilon^x f'(t)dt \geq f(\epsilon) - \alpha\int_\epsilon^x t^{-1}dt = f(\epsilon) - \alpha\ln(x) + \alpha \ln(\epsilon). \epsilon\rightarrow 0 f(\epsilon)\rightarrow 0 \ln(\epsilon)","['real-analysis', 'calculus', 'integration', 'derivatives']"
98,Quotient Rule: Why does Tao assume $g$ is non-zero on $X$?,Quotient Rule: Why does Tao assume  is non-zero on ?,g X,"Tao ( Analysis I , 2022, p. 220): Why can't Tao just assume $g(x_0)\neq 0$ ? Wouldn't the conclusion still hold if we changed the assumption "" $g$ is non-zero on $X$ "" to "" $g(x_0)\neq 0$ ""? In contrast, Bartle & Sherbert (2011, Introduction to Real Analysis , p. 163f): Why do Bartle & Sherbert use the assumption $g(c)\neq0$ (as I'd expect) but Tao doesn't?","Tao ( Analysis I , 2022, p. 220): Why can't Tao just assume ? Wouldn't the conclusion still hold if we changed the assumption "" is non-zero on "" to "" ""? In contrast, Bartle & Sherbert (2011, Introduction to Real Analysis , p. 163f): Why do Bartle & Sherbert use the assumption (as I'd expect) but Tao doesn't?",g(x_0)\neq 0 g X g(x_0)\neq 0 g(c)\neq0,"['calculus', 'analysis', 'derivatives', 'intuition']"
99,"Prove that in $\mathbb R$, the integral of the derivative of an integrable function is $0$","Prove that in , the integral of the derivative of an integrable function is",\mathbb R 0,"$\quad$ Let $f$ be a differentiable function in $\mathbb R$ , and $f$ is also a Lebesgue integrable function in $\mathbb R$ . Let $f'$ be a Lebesgue integrable function in $\mathbb R$ , too. How to prove that: $$\int_{\mathbb R}f'(x)\mathrm dx=0$$ $\quad$ My intuition tells me that when a function is integrable over the whole space, that is, when its integral is finite, the value of the function always starts at $0$ and ends at $0$ . The way up is the way down, so the derivative's integration is $0$ . But I know that even if the function is integrable on the whole space, it doesn't have to be $$\lim_{x\to\infty}f(x)=0$$ This function is not necessarily an absolutely continuous function, so there seems to be no direct relationship between the integral of its derivative and the original function in my mind. I can't think any more. Is there any way to deal with this problem? Any discussion and suggestions are welcome! Edit: According to PhoemueX, I learned that $f$ is absolutely continuous over any interval $[a,b]$ . Based on Kurt G's answers and Stefan's hints, I understand the significance of limits there. So it seems that we can get the result without doing an odd-even decomposition. My idea is that already known $$\int_a^bf'(x)\mathrm dx=f(b)- f(a)$$ and $f'(x)$ is integrable, so $$\lim_{a\to+\infty}\int_{-a}^0f'(x)\mathrm dx,\quad\lim_{b\to+\infty}\int_0^bf'(x)\mathrm dx$$ both exist, so $$\lim_{a\to+\infty}(f(0)- f(-a)),\quad\lim_{b\to+\infty}(f(b)- f(0))$$ both exist, therefore $f(\pm\infty)$ both exist. Then according to $f$ being integrable, $f(\pm\infty)$ can only be $0$ , so we get the final conclusion.","Let be a differentiable function in , and is also a Lebesgue integrable function in . Let be a Lebesgue integrable function in , too. How to prove that: My intuition tells me that when a function is integrable over the whole space, that is, when its integral is finite, the value of the function always starts at and ends at . The way up is the way down, so the derivative's integration is . But I know that even if the function is integrable on the whole space, it doesn't have to be This function is not necessarily an absolutely continuous function, so there seems to be no direct relationship between the integral of its derivative and the original function in my mind. I can't think any more. Is there any way to deal with this problem? Any discussion and suggestions are welcome! Edit: According to PhoemueX, I learned that is absolutely continuous over any interval . Based on Kurt G's answers and Stefan's hints, I understand the significance of limits there. So it seems that we can get the result without doing an odd-even decomposition. My idea is that already known and is integrable, so both exist, so both exist, therefore both exist. Then according to being integrable, can only be , so we get the final conclusion.","\quad f \mathbb R f \mathbb R f' \mathbb R \int_{\mathbb R}f'(x)\mathrm dx=0 \quad 0 0 0 \lim_{x\to\infty}f(x)=0 f [a,b] \int_a^bf'(x)\mathrm dx=f(b)- f(a) f'(x) \lim_{a\to+\infty}\int_{-a}^0f'(x)\mathrm dx,\quad\lim_{b\to+\infty}\int_0^bf'(x)\mathrm dx \lim_{a\to+\infty}(f(0)- f(-a)),\quad\lim_{b\to+\infty}(f(b)- f(0)) f(\pm\infty) f f(\pm\infty) 0","['real-analysis', 'integration', 'derivatives', 'lebesgue-integral', 'lebesgue-measure']"
