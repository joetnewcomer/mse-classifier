,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Why does $\frac{|f(z)-f(z_0)|}{|f(z)-\overline{f(z_0)}|}\leq\frac{|z-z_0|}{|z-\bar{z}_0|}$ when $\mathrm{Im}z>0\implies\mathrm{Im}f(z)\geq 0$?,Why does  when ?,\frac{|f(z)-f(z_0)|}{|f(z)-\overline{f(z_0)}|}\leq\frac{|z-z_0|}{|z-\bar{z}_0|} \mathrm{Im}z>0\implies\mathrm{Im}f(z)\geq 0,"I'm trying to understand the following inequality. Let $f$ be holomorphic, such that $\mathrm{Im}f(z)\geq 0$ when $\mathrm{Im}(z)>0$. Why is it that $$ \displaystyle\frac{|f(z)-f(z_0)|}{|f(z)-\overline{f(z_0)}|}\leq\frac{|z-z_0|}{|z-\bar{z}_0|}? $$ Since $f$ maps the upper half plane to itself, I was thinking of mapping the plane to the unit disk by some linear fractional, and then attempt to use Schwarz' lemma somehow. I haven't been able to execute a good plan. Does anyone have any hints and/or solutions to show this inequality? Thank you.","I'm trying to understand the following inequality. Let $f$ be holomorphic, such that $\mathrm{Im}f(z)\geq 0$ when $\mathrm{Im}(z)>0$. Why is it that $$ \displaystyle\frac{|f(z)-f(z_0)|}{|f(z)-\overline{f(z_0)}|}\leq\frac{|z-z_0|}{|z-\bar{z}_0|}? $$ Since $f$ maps the upper half plane to itself, I was thinking of mapping the plane to the unit disk by some linear fractional, and then attempt to use Schwarz' lemma somehow. I haven't been able to execute a good plan. Does anyone have any hints and/or solutions to show this inequality? Thank you.",,"['complex-analysis', 'inequality']"
1,help with a bizarre integral,help with a bizarre integral,,"i've been trying to do this integral , but with no luck . $$\int_1^\infty \frac{\left \{x \right \}}{x}\left(\frac{1}{x^{s}-1}\right) \; dx$$ $\Re(s)>1 $ , $\left \{x \right \} $ is the fractional , sawtooth function. i have tried the Fourier expansion of the sawtooth function : $$ \int_{1}^\infty \frac{\left \{x \right \}}{x}\left(\frac{1}{x^s - 1}\right)\; dx = \int_1^\infty \frac{1}{x(x^{s}-1)}\left(\frac{1}{2}-\frac{1}{\pi}\sum_{n=1}^\infty \frac{\sin(2\pi i nx)}{n} \right) \; dx$$ $$=\int_1^\infty\frac{1}{x(x^s-1)}\left(\frac{1}{2}+\frac{1}{2\pi i}\ln \left(\frac{1-q^2}{1-q^{-2}} \right)\right ) \; dx$$ where $ q $ is the nome : $ q=e^{i \pi x}$ after some manipulation , the integral reduces to : $$\frac{1}{2\pi i }\int_1^\infty \frac{\left(\pi i +\ln(-e^{2\pi i x}) \right )}{x(x^s - 1)} \; dx$$ but that brought me no where near a solution !! any suggestions on how to do the integral ??","i've been trying to do this integral , but with no luck . $$\int_1^\infty \frac{\left \{x \right \}}{x}\left(\frac{1}{x^{s}-1}\right) \; dx$$ $\Re(s)>1 $ , $\left \{x \right \} $ is the fractional , sawtooth function. i have tried the Fourier expansion of the sawtooth function : $$ \int_{1}^\infty \frac{\left \{x \right \}}{x}\left(\frac{1}{x^s - 1}\right)\; dx = \int_1^\infty \frac{1}{x(x^{s}-1)}\left(\frac{1}{2}-\frac{1}{\pi}\sum_{n=1}^\infty \frac{\sin(2\pi i nx)}{n} \right) \; dx$$ $$=\int_1^\infty\frac{1}{x(x^s-1)}\left(\frac{1}{2}+\frac{1}{2\pi i}\ln \left(\frac{1-q^2}{1-q^{-2}} \right)\right ) \; dx$$ where $ q $ is the nome : $ q=e^{i \pi x}$ after some manipulation , the integral reduces to : $$\frac{1}{2\pi i }\int_1^\infty \frac{\left(\pi i +\ln(-e^{2\pi i x}) \right )}{x(x^s - 1)} \; dx$$ but that brought me no where near a solution !! any suggestions on how to do the integral ??",,"['calculus', 'complex-analysis']"
2,time-frequency domain,time-frequency domain,,"im confused on how these folks seems to like convert a frequency into a time function, and a time function into a frequency function. i know that time function uses amplitude that varies over time, but i dont understand the mystery of frequency domain. if time has amplitude then frequency has what? here is a sample image i still cant get the relevance or the gist between transitioning between time and frequency. thanks","im confused on how these folks seems to like convert a frequency into a time function, and a time function into a frequency function. i know that time function uses amplitude that varies over time, but i dont understand the mystery of frequency domain. if time has amplitude then frequency has what? here is a sample image i still cant get the relevance or the gist between transitioning between time and frequency. thanks",,"['complex-analysis', 'fourier-analysis', 'fourier-series']"
3,Entire map taking disc to annulus,Entire map taking disc to annulus,,"Does there exist an entire function on $\mathbb{C}$ mapping an open disc to an annulus? The reason I ask this is because I want to to answer this question: Suppose $f$ is entire, and suppose there exists an open disc $U$ and $\delta > 0$ such that for all $z\in U$, $|f(z)|>\delta > 0$. Does there exist a branch cut for $\log$ such that $\log f(z)$ is analytic on all of $U$? (Note that $f$ is not necessarily one-to-one). A problem arises if $U$ is mapped to an annulus. EDIT: I guess the answer to the first question should be no, since if $f$ is entire and nonconstant, it is an open map. Therefore, all interior points of $U$ remain interior for $f(U)$. Thus $\partial f(U) = f(\partial U)$. On the other hand, since $\partial U$ is connected, $f(\partial U)$ cannot be the boundary of an annulus (which consists of two connected components). What do you think? PS. For a set $S$, by $\partial S$ I of course mean the boundary of $S$.","Does there exist an entire function on $\mathbb{C}$ mapping an open disc to an annulus? The reason I ask this is because I want to to answer this question: Suppose $f$ is entire, and suppose there exists an open disc $U$ and $\delta > 0$ such that for all $z\in U$, $|f(z)|>\delta > 0$. Does there exist a branch cut for $\log$ such that $\log f(z)$ is analytic on all of $U$? (Note that $f$ is not necessarily one-to-one). A problem arises if $U$ is mapped to an annulus. EDIT: I guess the answer to the first question should be no, since if $f$ is entire and nonconstant, it is an open map. Therefore, all interior points of $U$ remain interior for $f(U)$. Thus $\partial f(U) = f(\partial U)$. On the other hand, since $\partial U$ is connected, $f(\partial U)$ cannot be the boundary of an annulus (which consists of two connected components). What do you think? PS. For a set $S$, by $\partial S$ I of course mean the boundary of $S$.",,['complex-analysis']
4,$\sum \frac{z^{2n}}{1-z^{n}}$ normally convergent in $\mathbb{E}$,normally convergent in,\sum \frac{z^{2n}}{1-z^{n}} \mathbb{E},"I tried to solve this exercise (Remmert Theory of Complex Functions, p. 107, exercise 1 ), but I didn't get very far: Proposition: $$\sum \frac{z^{2n}}{1-z^{n}}$$ is normally convergent in $\mathbb{E}$ What does $\mathbb{E}=\{z\in \mathbb{C} | |z|<1 \}$ stand for? For normal convergence, it suffices if one finds a majorant series whose absolute value is less than infinity? so (most likely it is $|z|<1$ for all $z\in \mathbb{C})$  : $$\left|\sum \frac{z^{2n}}{1-z^{n}} \right| \le \sum \left| \frac{r^{2n}}{1-r^{n}}\right| \le \sum |r^{n}| < \infty$$ Does anybody see if this is right?","I tried to solve this exercise (Remmert Theory of Complex Functions, p. 107, exercise 1 ), but I didn't get very far: Proposition: $$\sum \frac{z^{2n}}{1-z^{n}}$$ is normally convergent in $\mathbb{E}$ What does $\mathbb{E}=\{z\in \mathbb{C} | |z|<1 \}$ stand for? For normal convergence, it suffices if one finds a majorant series whose absolute value is less than infinity? so (most likely it is $|z|<1$ for all $z\in \mathbb{C})$  : $$\left|\sum \frac{z^{2n}}{1-z^{n}} \right| \le \sum \left| \frac{r^{2n}}{1-r^{n}}\right| \le \sum |r^{n}| < \infty$$ Does anybody see if this is right?",,"['analysis', 'complex-analysis']"
5,How can one use the logarithm function to define angles?,How can one use the logarithm function to define angles?,,"In dealing with the complex logarithm function, I read that the imaginary part of $\log w$, is also called the argument of $w$, $\operatorname{arg }w$, and it is interpreted geometrically as the angle between the positive real axis and the half line from the origin through the point $w$. Using the interpretation, is there a standard way to define angles in triangles in the plane? I tried to draw a crude little picture. So something like $\Im(\log b)-\Im(\log a)$ would give the angle between the two dashed lines emanating out to $b$ and $a$, but how would one define the angle between the legs $ab$ and $ac$? I would like to avoid angles greater than $\pi$, so I'm hoping there's a definition that returns angles in $[0,\pi]$. Thanks.","In dealing with the complex logarithm function, I read that the imaginary part of $\log w$, is also called the argument of $w$, $\operatorname{arg }w$, and it is interpreted geometrically as the angle between the positive real axis and the half line from the origin through the point $w$. Using the interpretation, is there a standard way to define angles in triangles in the plane? I tried to draw a crude little picture. So something like $\Im(\log b)-\Im(\log a)$ would give the angle between the two dashed lines emanating out to $b$ and $a$, but how would one define the angle between the legs $ab$ and $ac$? I would like to avoid angles greater than $\pi$, so I'm hoping there's a definition that returns angles in $[0,\pi]$. Thanks.",,"['geometry', 'complex-analysis', 'analytic-geometry']"
6,Why are $\limsup \sqrt[n]{1/n!}=0$ and $\limsup\sqrt[n]{n!}=\infty$?,Why are  and ?,\limsup \sqrt[n]{1/n!}=0 \limsup\sqrt[n]{n!}=\infty,"I was looking at the power series $\sum\frac{z^n}{n!}$ and $\sum n!z^n$, and wanted to compute their radii of convergence. For the first, $\limsup \sqrt[n]{1/n!})=0$, and for the second $\limsup \sqrt[n]{n!}=\infty$, so the radii are $\infty$ and $0$ respectively. I'm curious, without resorting to Wolfram or Mathematica or some similar program, how could one justify by hand these limits? Intuitively they make sense that they are what they are, since the factorial should grow more quickly than the power of $1/n$ can handle. I suppose would explain the other since the terms are just reciprocals. Thanks!","I was looking at the power series $\sum\frac{z^n}{n!}$ and $\sum n!z^n$, and wanted to compute their radii of convergence. For the first, $\limsup \sqrt[n]{1/n!})=0$, and for the second $\limsup \sqrt[n]{n!}=\infty$, so the radii are $\infty$ and $0$ respectively. I'm curious, without resorting to Wolfram or Mathematica or some similar program, how could one justify by hand these limits? Intuitively they make sense that they are what they are, since the factorial should grow more quickly than the power of $1/n$ can handle. I suppose would explain the other since the terms are just reciprocals. Thanks!",,"['sequences-and-series', 'complex-analysis', 'limits', 'power-series', 'factorial']"
7,Confusion about the Proof of Dieudonné's Criterion - Why doesn't it hold for analytic functions?,Confusion about the Proof of Dieudonné's Criterion - Why doesn't it hold for analytic functions?,,"The following theorem is due to Dieudonné: Let $p(z) = a_0 + a_1 z + \cdots + a_n z^n$, a complex polynomial. $p$ is univalent on $D$, the unit disk, if and only if $f_\theta(z) = \sum_{k=1}^n a_k \dfrac{\sin(k\theta)}{\sin(\theta)} z^{k-1}$ has no zeros in $D$. ($\theta \in (0, \dfrac{\pi}{2}]$). The idea of the proof is as follows: 1) $p$ is univalent on $D$ if and only if $t\mapsto p(re^{it})$ is injective on $t\in [0,2\pi)$ for each $0< r < 1$. 2) $f_\theta$ has no zero in $D$ for $\theta \in (0,\dfrac{\pi}{2}]$ if and only if $p(re^{i(t+\theta)}) \ne p(re^{i(t-\theta)})$ for $t \in [0,2\pi)$, $0<r<1$ and $\theta \in (0,\dfrac{\pi}{2}]$. The second pars of 1) and 2) are saying the exact same thing, so this would prove the theorem. What (I think) I don't really get is is the statement 1). It seems like the proof for $p$ polynomial should work for any analytic function. So it seems like the theorem should be true for a general analytic function, where the sum becomes an infinite sum. What am I missing? EDIT: Basically, it seems like nowhere in the proof of Dieudonné the fact that $p$ is a polynomial is used.","The following theorem is due to Dieudonné: Let $p(z) = a_0 + a_1 z + \cdots + a_n z^n$, a complex polynomial. $p$ is univalent on $D$, the unit disk, if and only if $f_\theta(z) = \sum_{k=1}^n a_k \dfrac{\sin(k\theta)}{\sin(\theta)} z^{k-1}$ has no zeros in $D$. ($\theta \in (0, \dfrac{\pi}{2}]$). The idea of the proof is as follows: 1) $p$ is univalent on $D$ if and only if $t\mapsto p(re^{it})$ is injective on $t\in [0,2\pi)$ for each $0< r < 1$. 2) $f_\theta$ has no zero in $D$ for $\theta \in (0,\dfrac{\pi}{2}]$ if and only if $p(re^{i(t+\theta)}) \ne p(re^{i(t-\theta)})$ for $t \in [0,2\pi)$, $0<r<1$ and $\theta \in (0,\dfrac{\pi}{2}]$. The second pars of 1) and 2) are saying the exact same thing, so this would prove the theorem. What (I think) I don't really get is is the statement 1). It seems like the proof for $p$ polynomial should work for any analytic function. So it seems like the theorem should be true for a general analytic function, where the sum becomes an infinite sum. What am I missing? EDIT: Basically, it seems like nowhere in the proof of Dieudonné the fact that $p$ is a polynomial is used.",,['complex-analysis']
8,Complex Analysis: Continuity of a Function,Complex Analysis: Continuity of a Function,,"Problem: Let $f$ be defined as $f(z)=\frac{z}{1+|z|}$ . Is $f$ continuous from $\mathbb{C} \to \mathbb{C}$ ? Progress: $f$ is clearly well-defined on $\mathbb{C}$ , but is not holomorphic (Cauchy-Riemann equations are not satisfied as a result of the ' $|z|$ ' term). I think we may need to make use of the ' $\epsilon$ - $\delta$ ' definition of continuity but I'm really not sure how to apply this to complex-valued functions. Any assistance would be very appreciated.","Problem: Let be defined as . Is continuous from ? Progress: is clearly well-defined on , but is not holomorphic (Cauchy-Riemann equations are not satisfied as a result of the ' ' term). I think we may need to make use of the ' - ' definition of continuity but I'm really not sure how to apply this to complex-valued functions. Any assistance would be very appreciated.",f f(z)=\frac{z}{1+|z|} f \mathbb{C} \to \mathbb{C} f \mathbb{C} |z| \epsilon \delta,['complex-analysis']
9,Need help proving/understanding an inequality,Need help proving/understanding an inequality,,"I have been asked to prove the following inequality, given that $f:\mathbb{D}\to\mathbb{C}$ is an analytic function on the open unit disc $\mathbb{D}\subset\mathbb{C}$. $|f(w)-f(0)-wf~^\prime(0)|\leq|w|^2\sup|f(z)-f(0)-zf~^\prime(0)|$ where the supremum is taken over all $z\in\mathbb{D}$ and $w$ is any element of $\mathbb{D}$. Since $f$ is analytic on $\mathbb{D}$, it is equal to its power series representation at $z=0$ there.  So $f(z)=\displaystyle\sum\limits_{n=0}^\infty a_nz^n$ where $a_n=\frac{f^{(n)}(0)}{n!}$. We see that the expression $f(z)-f(0)-zf~^\prime(0)$ is just the power series expansion for $f$ without the first two terms.  This is another analytic function $g(z)=\displaystyle\sum\limits_{n=2}^\infty a_nz^n$ which satisfies $g(0)=g~^\prime(0)=0$.  So equivalently we want to show $|g(w)|\leq|w|^2\sup|g(z)|$ or $|\frac{g(w)}{w^2}|\leq\sup|g(z)|$ (the supremum still over $z\in\mathbb{D}$). Here is where I am getting hung up. I have $\frac{g(w)}{w^2}=\displaystyle\sum\limits_{n=2}^\infty a_nw^{n-2}$ is still an analytic function (right?). But I am not sure what I need to get the inequality. $\bf{UPDATE:}$  We can use Schwarz's Lemma on $h(w)=\frac{g(w)}{w}$ to obtain $|\frac{g(w)}{w}|\leq|w|$ since $h(0)=0$.  Can we show that the supremum is at least $1$?","I have been asked to prove the following inequality, given that $f:\mathbb{D}\to\mathbb{C}$ is an analytic function on the open unit disc $\mathbb{D}\subset\mathbb{C}$. $|f(w)-f(0)-wf~^\prime(0)|\leq|w|^2\sup|f(z)-f(0)-zf~^\prime(0)|$ where the supremum is taken over all $z\in\mathbb{D}$ and $w$ is any element of $\mathbb{D}$. Since $f$ is analytic on $\mathbb{D}$, it is equal to its power series representation at $z=0$ there.  So $f(z)=\displaystyle\sum\limits_{n=0}^\infty a_nz^n$ where $a_n=\frac{f^{(n)}(0)}{n!}$. We see that the expression $f(z)-f(0)-zf~^\prime(0)$ is just the power series expansion for $f$ without the first two terms.  This is another analytic function $g(z)=\displaystyle\sum\limits_{n=2}^\infty a_nz^n$ which satisfies $g(0)=g~^\prime(0)=0$.  So equivalently we want to show $|g(w)|\leq|w|^2\sup|g(z)|$ or $|\frac{g(w)}{w^2}|\leq\sup|g(z)|$ (the supremum still over $z\in\mathbb{D}$). Here is where I am getting hung up. I have $\frac{g(w)}{w^2}=\displaystyle\sum\limits_{n=2}^\infty a_nw^{n-2}$ is still an analytic function (right?). But I am not sure what I need to get the inequality. $\bf{UPDATE:}$  We can use Schwarz's Lemma on $h(w)=\frac{g(w)}{w}$ to obtain $|\frac{g(w)}{w}|\leq|w|$ since $h(0)=0$.  Can we show that the supremum is at least $1$?",,['complex-analysis']
10,Circle preserving homeomorphisms in the closure of $\mathbb{C}$ and Möbius Transformations,Circle preserving homeomorphisms in the closure of  and Möbius Transformations,\mathbb{C},"I am presently a learner of Hyperbolic Geometry and am using J. W. Anderson's book $Hyperbolic$ $Geometry$.  Now the author presents a sketch proof of why every circle preserving homeomorphism in $\overline{\mathbb{C}}$ is an element of the general Möbius group, which is what I am struggling to understand. First, a brief outline. Let $f$ be an element of the set of all circle preserving homeomorphisms which we denote Homeo$^{C}(\overline{\mathbb{C}})$ and let $p$ be a Möbius transormations that maps the triples $(f(0),f(1),f(\infty))$ to $(0,1,\infty)$ Then we see that $p\circ f(0) = 0, p\circ f(1) = 1$ and $p \circ f(\infty)=\infty$, and  since $p \circ f(\mathbb{R}) = \mathbb{R}$, either such a composition maps the upper half of the complex plane, $\mathbb{H}$ to itself or the lower half of the complex plane. If $p \circ f(\mathbb{H}) = \mathbb{H}$, we take $ m =p$, while if $m \circ (\mathbb{H})$ goes to the lower half, we just take $m = W \circ p$, where $W(z) = \overline{z}$ Now here is the thing I don't understand: Let $A$ be an euclidean circle in $\mathbb{C}$ with euclidean centre $\frac{1}{2}$ and radius $\frac{1}{2}$. Let $V(0), V(1)$ be the vertical lines through the points $x=0$ and $x=1$. Can anybody explain why as $V(0)$ and $V(1)$ are vertical tangents to the circle, then these lines under the map $m \circ f(z)$, namely  $m \circ f\Big(V(0)\Big)$ and  $m \circ f\Big(V(1)\Big)$ are again vertical tangents to the circle $m \circ f(A)$ at $m \circ f(0) = 0$ and $m \circ f(1) = 1$? I am trying to conclude from here that $m \circ f = Id_z$ , the identity transformation. Thanks, Ben","I am presently a learner of Hyperbolic Geometry and am using J. W. Anderson's book $Hyperbolic$ $Geometry$.  Now the author presents a sketch proof of why every circle preserving homeomorphism in $\overline{\mathbb{C}}$ is an element of the general Möbius group, which is what I am struggling to understand. First, a brief outline. Let $f$ be an element of the set of all circle preserving homeomorphisms which we denote Homeo$^{C}(\overline{\mathbb{C}})$ and let $p$ be a Möbius transormations that maps the triples $(f(0),f(1),f(\infty))$ to $(0,1,\infty)$ Then we see that $p\circ f(0) = 0, p\circ f(1) = 1$ and $p \circ f(\infty)=\infty$, and  since $p \circ f(\mathbb{R}) = \mathbb{R}$, either such a composition maps the upper half of the complex plane, $\mathbb{H}$ to itself or the lower half of the complex plane. If $p \circ f(\mathbb{H}) = \mathbb{H}$, we take $ m =p$, while if $m \circ (\mathbb{H})$ goes to the lower half, we just take $m = W \circ p$, where $W(z) = \overline{z}$ Now here is the thing I don't understand: Let $A$ be an euclidean circle in $\mathbb{C}$ with euclidean centre $\frac{1}{2}$ and radius $\frac{1}{2}$. Let $V(0), V(1)$ be the vertical lines through the points $x=0$ and $x=1$. Can anybody explain why as $V(0)$ and $V(1)$ are vertical tangents to the circle, then these lines under the map $m \circ f(z)$, namely  $m \circ f\Big(V(0)\Big)$ and  $m \circ f\Big(V(1)\Big)$ are again vertical tangents to the circle $m \circ f(A)$ at $m \circ f(0) = 0$ and $m \circ f(1) = 1$? I am trying to conclude from here that $m \circ f = Id_z$ , the identity transformation. Thanks, Ben",,['complex-analysis']
11,fractional linear maps,fractional linear maps,,what is the fractional linear map maps the circles $|z-5|=3$ and $|z+5|=3$ to concentric circles? What is the general method to find such maps? What is the image of $|z+2|=2$ under the map: $z \to 1/\bar{z}$? Thank you very much.,what is the fractional linear map maps the circles $|z-5|=3$ and $|z+5|=3$ to concentric circles? What is the general method to find such maps? What is the image of $|z+2|=2$ under the map: $z \to 1/\bar{z}$? Thank you very much.,,[]
12,is this a linear fractional transformation(LFT)?,is this a linear fractional transformation(LFT)?,,"Now, suppose the transformation(in 2d) I am working with has two separate functions for $x$ and $y$. That is, the transformation for $x$ is of the form $$ x'=\frac{x}{x+y} $$ and the transformation of $y$ is $$ y'=\frac{y}{x+y} $$ Each is an LFT (The schwarzian derivatives are $0$) but is the transform as a whole still considered an LFT?","Now, suppose the transformation(in 2d) I am working with has two separate functions for $x$ and $y$. That is, the transformation for $x$ is of the form $$ x'=\frac{x}{x+y} $$ and the transformation of $y$ is $$ y'=\frac{y}{x+y} $$ Each is an LFT (The schwarzian derivatives are $0$) but is the transform as a whole still considered an LFT?",,['complex-analysis']
13,"Polynomials, Rouche's theorem and index of vector fields","Polynomials, Rouche's theorem and index of vector fields",,"In the proof of Rouche's theorem I saw in a book, there are two points I failed to understand, or failed to prove myself. (if you aren't familiar with the theorem, please try to look at the two statements here below and explain them regardless of the theorem): ""Consider the complex plane. It's not difficult to show that the index of a curve $C$ with respect to a vector field $v$ is equal to the sum of indices of singular points, i.e., those at which $v(z) = 0$ ."" ""If $f(z)$ is a polynomial, and let $v(z) = f(z)$ , where $v$ is a vector field, then the index of the singular point $z_0$ is equal to the multiplicity of the root $z_0$ of $f$ ."" I am having trouble understanding this; can anyone please help?","In the proof of Rouche's theorem I saw in a book, there are two points I failed to understand, or failed to prove myself. (if you aren't familiar with the theorem, please try to look at the two statements here below and explain them regardless of the theorem): ""Consider the complex plane. It's not difficult to show that the index of a curve with respect to a vector field is equal to the sum of indices of singular points, i.e., those at which ."" ""If is a polynomial, and let , where is a vector field, then the index of the singular point is equal to the multiplicity of the root of ."" I am having trouble understanding this; can anyone please help?",C v v(z) = 0 f(z) v(z) = f(z) v z_0 z_0 f,"['abstract-algebra', 'complex-analysis', 'polynomials', 'plane-curves', 'roots']"
14,Doubt about a proof in complex analysis,Doubt about a proof in complex analysis,,"In proving that a continuous function $$f:S(0,1)\to \mathbb{R}$$ Has a continuous extension to $$h:\overline{B(0,1)}\to \mathbb{R}$$ Such that $h|_{B(0,1)}$ is harmonic, one needs to show that the following integral vanishes $$\int_{0}^{2\pi}\frac{1-|z|^2}{|e^{i\theta}-z|^2}|f(w)-f(e^{i\theta})|d\theta$$ As $z\to w$ where $|w| = 1$ . In this (pdf download) set of notes, the proof of this is given as follows: Now, we split the integral above in two. One over the range of values of $\theta$ for which $|e^{i\theta} - w| < \delta$ and the other over the complementary range. For any $\epsilon > 0$ the former can be smaller than $\epsilon$ by the choice of $\delta$ . This is because $f$ is continuous. And for fixed $\delta$ the latter tends to zero as $z$ approaches $w$ in view of property (c) of the Poisson Integral. Since $\epsilon$ is arbitrary, the limit in Part (b) follows. The issue with this proof is that it seems like we fix $\delta$ first, and then choose $z$ close enough to the unit circle, however, our choice of $\delta$ is such that the integral is less than $\epsilon$ , but this integral necessarily depends on $z$ , and so it is not possible to choose $\delta$ without first having chosen $z$ , and this is not a trivial issue, since the term $$ \frac{1-|z|^2}{|e^{i\theta}-z|^2} $$ Diverges as $z\to e^{i\theta}$ . Perhaps this argument can be fixed, but I cannot see a simple way to justify it. Am I correct in finding this reasoning suspicious? What I've tried: Some simplifying assumptions can be made. Firstly, we may assume w.l.o.g. that $w = 1$ i.e. that it lies on the positive real axis, we can make the same assumption about $z$ , calling it now instead $s \in (0,1)$ . If we are able to bound the integral by choosing $|s-1|<\delta$ where $\delta$ does not depend on our choice of $w$ to be on the real line, so that the same $\delta$ would work along any ""ray"", then through some uniform continuity arguments we get that this integral vanishes. So the new limit is taken over the real numbers, i.e. we need to show that $$\lim_{s\to 1}\int_{0}^{2\pi}\frac{1-s^2}{|e^{i\theta}-s|^2}|f(1)-f(e^{i\theta})|d\theta = 0$$","In proving that a continuous function Has a continuous extension to Such that is harmonic, one needs to show that the following integral vanishes As where . In this (pdf download) set of notes, the proof of this is given as follows: Now, we split the integral above in two. One over the range of values of for which and the other over the complementary range. For any the former can be smaller than by the choice of . This is because is continuous. And for fixed the latter tends to zero as approaches in view of property (c) of the Poisson Integral. Since is arbitrary, the limit in Part (b) follows. The issue with this proof is that it seems like we fix first, and then choose close enough to the unit circle, however, our choice of is such that the integral is less than , but this integral necessarily depends on , and so it is not possible to choose without first having chosen , and this is not a trivial issue, since the term Diverges as . Perhaps this argument can be fixed, but I cannot see a simple way to justify it. Am I correct in finding this reasoning suspicious? What I've tried: Some simplifying assumptions can be made. Firstly, we may assume w.l.o.g. that i.e. that it lies on the positive real axis, we can make the same assumption about , calling it now instead . If we are able to bound the integral by choosing where does not depend on our choice of to be on the real line, so that the same would work along any ""ray"", then through some uniform continuity arguments we get that this integral vanishes. So the new limit is taken over the real numbers, i.e. we need to show that","f:S(0,1)\to \mathbb{R} h:\overline{B(0,1)}\to \mathbb{R} h|_{B(0,1)} \int_{0}^{2\pi}\frac{1-|z|^2}{|e^{i\theta}-z|^2}|f(w)-f(e^{i\theta})|d\theta z\to w |w| = 1 \theta |e^{i\theta} - w| < \delta \epsilon > 0 \epsilon \delta f \delta z w \epsilon \delta z \delta \epsilon z \delta z  \frac{1-|z|^2}{|e^{i\theta}-z|^2}  z\to e^{i\theta} w = 1 z s \in (0,1) |s-1|<\delta \delta w \delta \lim_{s\to 1}\int_{0}^{2\pi}\frac{1-s^2}{|e^{i\theta}-s|^2}|f(1)-f(e^{i\theta})|d\theta = 0","['real-analysis', 'complex-analysis', 'limits', 'definite-integrals', 'harmonic-functions']"
15,"Finding $\int_\Gamma \frac{z f'(z)}{f(z)} \, dz$ over a given contour [duplicate]",Finding  over a given contour [duplicate],"\int_\Gamma \frac{z f'(z)}{f(z)} \, dz","This question already has answers here : Using argument principle to compute an integral (1 answer) Calculate $\int_\Gamma \frac{f'(z)z}{f(z)}\, \operatorname dz$ (2 answers) Closed 29 days ago . Let $f(z)=z^4-2z^3+2z^2-3z+60$ and let $\Gamma$ be the circle $|z|=5$ . I want to find $$\int_\Gamma \frac{z f'(z)}{f(z)} \, dz$$ Supposing we had $f'(z)$ in the numerator instead of $z f'(z)$ , this would very clearly be solved with the Argument Principle. Here, we cannot use it. So, we investigate using Rouche's theorem. Note that if we let $g(z) = z^4$ , then for $z$ on $\Gamma$ , we have $|f(z)-g(z)| < 2(5)^3+2(5)^2+3(5)+60 < 5^4 = |g(z)|$ and so since $g$ has four zeros interior to $\Gamma$ , then so does $f$ . Going back to the integral, we have \begin{align} \underbrace{\int_\Gamma \frac{z f'(z)}{f(z)}}_{=I_5} \, dz = \int_\Gamma \frac{z (4z^3-6z^2+4z-3)}{z^4-2z^3+2z^2-3z+60} = \int_\Gamma \frac{4z^4-6z^3+4z^2-3z}{z^4-2z^3+2z^2-3z+60} \tag{1} \end{align} Denote the above integral by $I_5$ $($ because $\Gamma$ is the curve $|z|=5)$ . Now, because $\Gamma$ already contains all the zeros of $f(z)$ , then if $I_R$ is the same integral as in $(1)$ except with $\Gamma$ being the curve $|z|=R$ , we must have $$|I_5| = |I_R| \hspace{1cm} \forall \, R \geq 5 \tag{2}$$ So, we get \begin{align} |I_5| = |I_R| &= \left| \int_{|z|=R}\frac{z f'(z)}{f(z)} \, dz \right| \\ &\leq \int_{|z|=5} \left| \frac{z f'(z)}{f(z)} \, dz \right| \\ &= \int_{|z|=5} \frac{|z f'(z)|}{|f(z)|} \, |dz| \\  &= \frac{4R^4-6R^3+4R^2-3R}{R^4-2R^4+2R^2-3R+60} \cdot 2\pi R \end{align} But now we have a problem because $|I_R| \to \infty$ as $R \to \infty$ . In fact, I suspect that the method I've contrived is problematic because it requires the degree of the numerator to be at least two less than the denominator if you want the integral to vanish if to be at least one less if you want the integral to converge to a nonzero magnitude. In our case, $\deg(z \cdot f'(z)) = \deg(f(z))$ and so the extra $|dz|$ is the final culprit in all this which makes the magnitude of the integral diverge. Perhaps there is a better way to approach this?","This question already has answers here : Using argument principle to compute an integral (1 answer) Calculate $\int_\Gamma \frac{f'(z)z}{f(z)}\, \operatorname dz$ (2 answers) Closed 29 days ago . Let and let be the circle . I want to find Supposing we had in the numerator instead of , this would very clearly be solved with the Argument Principle. Here, we cannot use it. So, we investigate using Rouche's theorem. Note that if we let , then for on , we have and so since has four zeros interior to , then so does . Going back to the integral, we have Denote the above integral by because is the curve . Now, because already contains all the zeros of , then if is the same integral as in except with being the curve , we must have So, we get But now we have a problem because as . In fact, I suspect that the method I've contrived is problematic because it requires the degree of the numerator to be at least two less than the denominator if you want the integral to vanish if to be at least one less if you want the integral to converge to a nonzero magnitude. In our case, and so the extra is the final culprit in all this which makes the magnitude of the integral diverge. Perhaps there is a better way to approach this?","f(z)=z^4-2z^3+2z^2-3z+60 \Gamma |z|=5 \int_\Gamma \frac{z f'(z)}{f(z)} \, dz f'(z) z f'(z) g(z) = z^4 z \Gamma |f(z)-g(z)| < 2(5)^3+2(5)^2+3(5)+60 < 5^4 = |g(z)| g \Gamma f \begin{align}
\underbrace{\int_\Gamma \frac{z f'(z)}{f(z)}}_{=I_5} \, dz = \int_\Gamma \frac{z (4z^3-6z^2+4z-3)}{z^4-2z^3+2z^2-3z+60} = \int_\Gamma \frac{4z^4-6z^3+4z^2-3z}{z^4-2z^3+2z^2-3z+60} \tag{1}
\end{align} I_5 ( \Gamma |z|=5) \Gamma f(z) I_R (1) \Gamma |z|=R |I_5| = |I_R| \hspace{1cm} \forall \, R \geq 5 \tag{2} \begin{align}
|I_5| = |I_R| &= \left| \int_{|z|=R}\frac{z f'(z)}{f(z)} \, dz \right| \\
&\leq \int_{|z|=5} \left| \frac{z f'(z)}{f(z)} \, dz \right| \\
&= \int_{|z|=5} \frac{|z f'(z)|}{|f(z)|} \, |dz| \\ 
&= \frac{4R^4-6R^3+4R^2-3R}{R^4-2R^4+2R^2-3R+60} \cdot 2\pi R
\end{align} |I_R| \to \infty R \to \infty \deg(z \cdot f'(z)) = \deg(f(z)) |dz|","['complex-analysis', 'rouches-theorem']"
16,Calculating $\int_{0}^{2\pi} \frac{\sin(\theta) + \cos(2\theta)}{2 + \sin(2\theta)}d\theta$ using Cauchy's Residue Theorem,Calculating  using Cauchy's Residue Theorem,\int_{0}^{2\pi} \frac{\sin(\theta) + \cos(2\theta)}{2 + \sin(2\theta)}d\theta,"Calculate the following integral using the residue theorem. $$\int_{0}^{2\pi} \frac{\sin(\theta) + \cos(2\theta)}{2 + \sin(2\theta)}d\theta$$ This was my attempted method: Let $z=e^{i\theta}$ , $dz=izd\theta$ $\sin(\theta)=\frac{1}{2i}(z-\frac{1}{z})$ , $\sin(2\theta)=\frac{1}{2i}(z^2-\frac{1}{z^2})$ , $\cos(2\theta)=\frac{1}{2}(z^2+\frac{1}{z^2})$ By substitution, $$\oint_{|z|=1}\frac{\frac{1}{2i}(z-\frac{1}{z})+\frac{1}{2}(z^2+\frac{1}{z^2})}{2+\frac{1}{2i}(z^2-\frac{1}{z^2})}\frac{dz}{iz}= \oint_{|z|=1}\frac{z^4-iz^3+iz+1}{z(z^4+4iz^2-1)}dz $$ From the denominator, we have poles at $z=0, \sqrt{\left( 2-\sqrt3 \right)}\angle-\frac{\pi}{4}, \sqrt{\left( 2-\sqrt3 \right)}\angle\frac{3\pi}{4}, \sqrt{\left( 2+\sqrt3 \right)}\angle-\frac{\pi}{4}, \sqrt{\left( 2+\sqrt3 \right)}\angle\frac{3\pi}{4}$ Only $z=0,  \sqrt{\left( 2-\sqrt3 \right)}\angle-\frac{\pi}{4},  \sqrt{\left( 2-\sqrt3 \right)}\angle\frac{3\pi}{4}$ are located within the contour path $|z|=1$ . Rewriting the denominator, $$z(z^4+4iz^2-1)=z(z-\sqrt{\left( 2-\sqrt3 \right)}\angle-\frac{\pi}{4})(z-\sqrt{\left( 2-\sqrt3 \right)}\angle\frac{3\pi}{4})(z^2+(2+\sqrt3)i)$$ Around $z=0$ , $$\oint_{|z|=1}\frac{1}{z}\frac{z^4-iz^3+iz+1}{z^4+4iz^2-1}=2\pi i(-1)$$ Around $z=\sqrt{\left( 2-\sqrt3 \right)}\angle-\frac{\pi}{4}$ , $$\oint_{|z|=1}\frac{1}{(z-\sqrt{\left( 2-\sqrt3 \right)}\angle-\frac{\pi}{4})}\frac{z^4-iz^3+iz+1}{z(z-\sqrt{\left( 2-\sqrt3 \right)}\angle\frac{3\pi}{4})(z^2+(2+\sqrt3)i)}\approx2\pi i(0.59386\angle5.1039^\circ)$$ Around $z=\sqrt{\left( 2-\sqrt3 \right)}\angle\frac{3\pi}{4}$ , $$\oint_{|z|=1}\frac{1}{(z-\sqrt{\left( 2-\sqrt3 \right)}\angle\frac{3\pi}{4})}\frac{z^4-iz^3+iz+1}{z(z-\sqrt{\left( 2-\sqrt3 \right)}\angle-\frac{\pi}{4})(z^2+(2+\sqrt3)i)}\approx2\pi i(0.699\angle-4.3336^\circ)$$ Adding them up, the resulting sum is $\approx2\pi i(0.288675)$ , which does not make sense since this is a real integral to begin with. At which step did I make a mistake?","Calculate the following integral using the residue theorem. This was my attempted method: Let , , , By substitution, From the denominator, we have poles at Only are located within the contour path . Rewriting the denominator, Around , Around , Around , Adding them up, the resulting sum is , which does not make sense since this is a real integral to begin with. At which step did I make a mistake?","\int_{0}^{2\pi} \frac{\sin(\theta) + \cos(2\theta)}{2 + \sin(2\theta)}d\theta z=e^{i\theta} dz=izd\theta \sin(\theta)=\frac{1}{2i}(z-\frac{1}{z}) \sin(2\theta)=\frac{1}{2i}(z^2-\frac{1}{z^2}) \cos(2\theta)=\frac{1}{2}(z^2+\frac{1}{z^2}) \oint_{|z|=1}\frac{\frac{1}{2i}(z-\frac{1}{z})+\frac{1}{2}(z^2+\frac{1}{z^2})}{2+\frac{1}{2i}(z^2-\frac{1}{z^2})}\frac{dz}{iz}=
\oint_{|z|=1}\frac{z^4-iz^3+iz+1}{z(z^4+4iz^2-1)}dz
 z=0, \sqrt{\left( 2-\sqrt3 \right)}\angle-\frac{\pi}{4}, \sqrt{\left( 2-\sqrt3 \right)}\angle\frac{3\pi}{4}, \sqrt{\left( 2+\sqrt3 \right)}\angle-\frac{\pi}{4}, \sqrt{\left( 2+\sqrt3 \right)}\angle\frac{3\pi}{4} z=0, 
\sqrt{\left( 2-\sqrt3 \right)}\angle-\frac{\pi}{4}, 
\sqrt{\left( 2-\sqrt3 \right)}\angle\frac{3\pi}{4} |z|=1 z(z^4+4iz^2-1)=z(z-\sqrt{\left( 2-\sqrt3 \right)}\angle-\frac{\pi}{4})(z-\sqrt{\left( 2-\sqrt3 \right)}\angle\frac{3\pi}{4})(z^2+(2+\sqrt3)i) z=0 \oint_{|z|=1}\frac{1}{z}\frac{z^4-iz^3+iz+1}{z^4+4iz^2-1}=2\pi i(-1) z=\sqrt{\left( 2-\sqrt3 \right)}\angle-\frac{\pi}{4} \oint_{|z|=1}\frac{1}{(z-\sqrt{\left( 2-\sqrt3 \right)}\angle-\frac{\pi}{4})}\frac{z^4-iz^3+iz+1}{z(z-\sqrt{\left( 2-\sqrt3 \right)}\angle\frac{3\pi}{4})(z^2+(2+\sqrt3)i)}\approx2\pi i(0.59386\angle5.1039^\circ) z=\sqrt{\left( 2-\sqrt3 \right)}\angle\frac{3\pi}{4} \oint_{|z|=1}\frac{1}{(z-\sqrt{\left( 2-\sqrt3 \right)}\angle\frac{3\pi}{4})}\frac{z^4-iz^3+iz+1}{z(z-\sqrt{\left( 2-\sqrt3 \right)}\angle-\frac{\pi}{4})(z^2+(2+\sqrt3)i)}\approx2\pi i(0.699\angle-4.3336^\circ) \approx2\pi i(0.288675)","['integration', 'complex-analysis', 'residue-calculus', 'trigonometric-integrals']"
17,Convergence of series from inverse of Cauchy product,Convergence of series from inverse of Cauchy product,,"The Cauchy product of two real or complex infinite series $\sum_{n\in\mathbb{N}} a_n$ and $\sum_{n\in\mathbb{N}} b_n$ is defined as: $$ \forall n\in\mathbb{N}, c_n = \sum_{k=0}^n a_k b_{n-k} $$ Mertens' theorem states that if one of the two series $\sum_{n\in\mathbb{N}} a_n$ and $\sum_{n\in\mathbb{N}} b_n$ is absolutely convergent then the series $\sum_{n\in\mathbb{N}} c_n$ also converge. If both $\sum_{n\in\mathbb{N}} a_n$ and $\sum_{n\in\mathbb{N}} b_n$ are absolutely convergent then $\sum_{n\in\mathbb{N}} c_n$ is also absolutely convergent. Now that we have two convergent series $\sum_{n\in\mathbb{N}} a_n$ and $\sum_{n\in\mathbb{N}} b_n$ , when $b_0\neq 0$ , by recursively defining $c_n$ as: $$ c_n = \frac{a_n - \sum_{k=0}^{n-1} c_k b_{n-k}}{b_0}, $$ we can make a series that satisfies: $$ \forall n\in\mathbb{N}, a_n = \sum_{k=0}^n c_k b_{n-k}. $$ which resembles the inverse of a Cauchy product. My question is: what's the condition on the series $\sum_{n\in\mathbb{N}} a_n$ and $\sum_{n\in\mathbb{N}} b_n$ for thus defined series $\sum_{n\in\mathbb{N}} c_n$ to be convergent? Absolutely convergent?","The Cauchy product of two real or complex infinite series and is defined as: Mertens' theorem states that if one of the two series and is absolutely convergent then the series also converge. If both and are absolutely convergent then is also absolutely convergent. Now that we have two convergent series and , when , by recursively defining as: we can make a series that satisfies: which resembles the inverse of a Cauchy product. My question is: what's the condition on the series and for thus defined series to be convergent? Absolutely convergent?","\sum_{n\in\mathbb{N}} a_n \sum_{n\in\mathbb{N}} b_n  \forall n\in\mathbb{N}, c_n = \sum_{k=0}^n a_k b_{n-k}  \sum_{n\in\mathbb{N}} a_n \sum_{n\in\mathbb{N}} b_n \sum_{n\in\mathbb{N}} c_n \sum_{n\in\mathbb{N}} a_n \sum_{n\in\mathbb{N}} b_n \sum_{n\in\mathbb{N}} c_n \sum_{n\in\mathbb{N}} a_n \sum_{n\in\mathbb{N}} b_n b_0\neq 0 c_n  c_n = \frac{a_n - \sum_{k=0}^{n-1} c_k b_{n-k}}{b_0},   \forall n\in\mathbb{N}, a_n = \sum_{k=0}^n c_k b_{n-k}.  \sum_{n\in\mathbb{N}} a_n \sum_{n\in\mathbb{N}} b_n \sum_{n\in\mathbb{N}} c_n","['real-analysis', 'sequences-and-series', 'complex-analysis', 'convergence-divergence', 'absolute-convergence']"
18,Application of the residue theorem: how to use angular sectors,Application of the residue theorem: how to use angular sectors,,"I am studying the residue theorem applications to physics and in particular the angular sector method. I understand the concept of finding an angle such that the infinite half-segment along it acts as the real axis, but I found an integral that I do not know how to solve with this method: $$\int{\frac{1-x}{1+x^3}}$$ I tried the following: $$ \oint{\frac{1-z}{1-z^3} dz}=\int_{0}^{\infty}{\frac{1-x}{1-x^3}dx} + \int_{\infty}^0{\frac{1-z}{1+z^3}dz}=2\pi i \Sigma_jRes(f, z_j)$$ And taking $z=re^{2\pi i/3}$ in the last integral, I get $$ \oint{\frac{1-z}{1-z^3} dz}=\int_{0}^{\infty}{\frac{1-x}{1-x^3}dx} - \int^{\infty}_0{\frac{1-re^{2 \pi i /3}}{1+r^3}dr}=2\pi i \Sigma_jRes(f, z_j)$$ What I wanted to get is something of the form $(1-a)I=2\pi i \Sigma_j Res(f,z_j)$ , but I dont see anyway of factorizing. What should I do ? By the way, the integral should vanish, maybe it helps...","I am studying the residue theorem applications to physics and in particular the angular sector method. I understand the concept of finding an angle such that the infinite half-segment along it acts as the real axis, but I found an integral that I do not know how to solve with this method: I tried the following: And taking in the last integral, I get What I wanted to get is something of the form , but I dont see anyway of factorizing. What should I do ? By the way, the integral should vanish, maybe it helps...","\int{\frac{1-x}{1+x^3}}  \oint{\frac{1-z}{1-z^3} dz}=\int_{0}^{\infty}{\frac{1-x}{1-x^3}dx} + \int_{\infty}^0{\frac{1-z}{1+z^3}dz}=2\pi i \Sigma_jRes(f, z_j) z=re^{2\pi i/3}  \oint{\frac{1-z}{1-z^3} dz}=\int_{0}^{\infty}{\frac{1-x}{1-x^3}dx} - \int^{\infty}_0{\frac{1-re^{2 \pi i /3}}{1+r^3}dr}=2\pi i \Sigma_jRes(f, z_j) (1-a)I=2\pi i \Sigma_j Res(f,z_j)","['complex-analysis', 'residue-calculus']"
19,Show that $F(z)=\int_{-\infty}^{\infty} e^{-t^2+4 z t} d t$ is holomorphic in $\mathbb{C}$,Show that  is holomorphic in,F(z)=\int_{-\infty}^{\infty} e^{-t^2+4 z t} d t \mathbb{C},"I am trying to show that $$F(z)=\int_{-\infty}^{\infty} e^{-t^2+4 z t} d t$$ is a holomorphic function in $\mathbb{C}$ . The idea is to use the following theorem for parametric integrals: Suppose $I$ is an interval, possibly unbounded, $A \subset \mathbb{C}$ is an open subset and suppose $\phi(t,z)$ is defined on $(I\backslash P) \times A$ , where $P=\{t_1,\dots,t_N\}$ . Suppose that for each $t \in (I\backslash P)$ , $\phi(t,z) \in H(A)$ and $\phi, D_z \phi \in C((I\backslash P) \times A)$ . Suppose further that for each $K \subset A$ a compact subset, there exists $F_K$ such that $$|\phi(t,z)| \leq F_K(t), \quad \forall t \in I \backslash P, z \in K,$$ where $$\int_{I} F_K(t) dt < \infty.$$ Then if $f(z)= \int_{I} \phi(t,z) dt$ then $f \in H(A)$ and $f^\prime (z)= \int_{I} D_z \phi(t,z) dt$ . ATTEMPT Let $\phi(t,z)=e^{-t^2+4zt}$ . Obviously $\phi(t,\cdot) \in H(\mathbb{C})$ for $t$ fixed, and $\phi, D_z \phi$ are continuous in $\mathbb{R} \times \mathbb{C}$ . Let $K \subset \mathbb{C}$ be a compact subset. We know that $$|\phi(t,z)|=e^{-t^2+4 Re(z)t}.$$ Now we can suppose that $K \subset \{z: \alpha < Re(z) < \beta\}$ . The problem in trying to bound $|\phi(t,z)|$ is that $t$ can be both negative and positive, and so can $\alpha$ and $\beta$ , depending on where the compact is located in the complex plane. For example: If $K$ is in the half-plane where $Re(z) >0$ , then $\alpha,\beta >0$ , and if $t <0$ , then $4Re(z)t <0$ , and thus $$|\phi(t,z)| \leq e^{-t^2},$$ and it is well known that $\int_{0}^{\infty} e^{-t^2} dt< \infty$ . So we want to define $F_K$ as $$F_K=\left\{ \begin{array}{cl} e^{-t^2} & : \ t < 0 \\ \text{something} & : \ t \geq 0 \end{array} \right.$$ The problem is that, for $t \geq 0$ I can't seem to find a good bound to get a convergent integral, and by working like this it seems that I will have to do many cases by exhaustion. Is there a simpler way? I know of the way using Morera and Fubini, but that is not the objective here.","I am trying to show that is a holomorphic function in . The idea is to use the following theorem for parametric integrals: Suppose is an interval, possibly unbounded, is an open subset and suppose is defined on , where . Suppose that for each , and . Suppose further that for each a compact subset, there exists such that where Then if then and . ATTEMPT Let . Obviously for fixed, and are continuous in . Let be a compact subset. We know that Now we can suppose that . The problem in trying to bound is that can be both negative and positive, and so can and , depending on where the compact is located in the complex plane. For example: If is in the half-plane where , then , and if , then , and thus and it is well known that . So we want to define as The problem is that, for I can't seem to find a good bound to get a convergent integral, and by working like this it seems that I will have to do many cases by exhaustion. Is there a simpler way? I know of the way using Morera and Fubini, but that is not the objective here.","F(z)=\int_{-\infty}^{\infty} e^{-t^2+4 z t} d t \mathbb{C} I A \subset \mathbb{C} \phi(t,z) (I\backslash P) \times A P=\{t_1,\dots,t_N\} t \in (I\backslash P) \phi(t,z) \in H(A) \phi, D_z \phi \in C((I\backslash P) \times A) K \subset A F_K |\phi(t,z)| \leq F_K(t), \quad \forall t \in I \backslash P, z \in K, \int_{I} F_K(t) dt < \infty. f(z)= \int_{I} \phi(t,z) dt f \in H(A) f^\prime (z)= \int_{I} D_z \phi(t,z) dt \phi(t,z)=e^{-t^2+4zt} \phi(t,\cdot) \in H(\mathbb{C}) t \phi, D_z \phi \mathbb{R} \times \mathbb{C} K \subset \mathbb{C} |\phi(t,z)|=e^{-t^2+4 Re(z)t}. K \subset \{z: \alpha < Re(z) < \beta\} |\phi(t,z)| t \alpha \beta K Re(z) >0 \alpha,\beta >0 t <0 4Re(z)t <0 |\phi(t,z)| \leq e^{-t^2}, \int_{0}^{\infty} e^{-t^2} dt< \infty F_K F_K=\left\{ \begin{array}{cl}
e^{-t^2} & : \ t < 0 \\
\text{something} & : \ t \geq 0
\end{array} \right. t \geq 0","['integration', 'complex-analysis', 'convergence-divergence', 'parametric']"
20,Show that $\dfrac{1}{2 \pi i} \int_{c- i \infty}^{c+ i \infty} F(s) G(w-s) ds = \int_0^{\infty} f(x) g(x) x^{w-1} dx.$,Show that,\dfrac{1}{2 \pi i} \int_{c- i \infty}^{c+ i \infty} F(s) G(w-s) ds = \int_0^{\infty} f(x) g(x) x^{w-1} dx.,"Eq. 2.15.10 in Titchmarsh's book The Theory of the Riemann Zeta-Function comes with no proof: Let f(x) and $F(s)$ be ralated by $$F(s) = \int_0^{\infty} f(x) x^{s-1} dx, \ \ \ f(x) = \dfrac{1}{2 \pi i} \int_{\sigma- i \infty}^{\sigma+ i \infty} F(s) x^{-s} ds.$$ Then we have, subject to appropriate conditions, $$\dfrac{1}{2 \pi i} \int_{c- i \infty}^{c+ i \infty} F(s) G(w-s) ds = \int_0^{\infty} f(x) g(x) x^{w-1} dx.$$ First thing to do is to compute the integral of $f(x)g(x)x^{w-1}$ and see what happens but I failed to reach a result, especially that I very much am hoping for a rigorous proof. Then searching on web and some books that I had lead me nowhere. The two questions I have: How the equality is derived and what are the conditions are considered ""appropriate""? (perhaps the author is thinking of determining the $c$ based on $\sigma_f$ and $\sigma_g$ (?)) Note for Bounty: The post below from ""Daigaku no Baku"" answers the question partially - to get a complete answer I need to justify the interchange of integrals used in ""Daigaku no Baku"" 's answer and validity of the real interval for $\max (\alpha_1, 1- \beta_2 ) < c < \max ( \beta_1, 1- \alpha_2)$ mentioned in  en.wikipedia.org/wiki/Mellin_transform in the section concerning Parseval's theorem (which is my second question in OP).","Eq. 2.15.10 in Titchmarsh's book The Theory of the Riemann Zeta-Function comes with no proof: Let f(x) and be ralated by Then we have, subject to appropriate conditions, First thing to do is to compute the integral of and see what happens but I failed to reach a result, especially that I very much am hoping for a rigorous proof. Then searching on web and some books that I had lead me nowhere. The two questions I have: How the equality is derived and what are the conditions are considered ""appropriate""? (perhaps the author is thinking of determining the based on and (?)) Note for Bounty: The post below from ""Daigaku no Baku"" answers the question partially - to get a complete answer I need to justify the interchange of integrals used in ""Daigaku no Baku"" 's answer and validity of the real interval for mentioned in  en.wikipedia.org/wiki/Mellin_transform in the section concerning Parseval's theorem (which is my second question in OP).","F(s) F(s) = \int_0^{\infty} f(x) x^{s-1} dx, \ \ \ f(x) = \dfrac{1}{2 \pi i} \int_{\sigma- i \infty}^{\sigma+ i \infty} F(s) x^{-s} ds. \dfrac{1}{2 \pi i} \int_{c- i \infty}^{c+ i \infty} F(s) G(w-s) ds = \int_0^{\infty} f(x) g(x) x^{w-1} dx. f(x)g(x)x^{w-1} c \sigma_f \sigma_g \max (\alpha_1, 1- \beta_2 ) < c < \max ( \beta_1, 1- \alpha_2)","['complex-analysis', 'proof-explanation', 'improper-integrals', 'mellin-transform']"
21,Finding the convergent interval of $z^{{2z}^{{3z}\ldots^{{nz}\ldots}}}$,Finding the convergent interval of,z^{{2z}^{{3z}\ldots^{{nz}\ldots}}},"I was trying to find an exact solution to $x=a^x$ , and naturally derived a solution defined by infinite tetrations of $a$ . I first defined a recursion equivalent to the definition of tetrations and then used invariant points to show that $$\lim_{n \to \infty} (^nx)$$ converges for $x \in [e^{-e}, e^{\frac{1}{e}}]$ . Now, I am curious if I could extend the definition of tetrations and find the convergence interval of the following function. $$f(x)=x^{{2x}^{{3x}^{{...}}}}$$ I find this to be hard because I cannot, as in the case of normal tetrations, define a recursion that can be applied to define $f(x)$ . Any tips on how to proceed will be appreciated. Thanks a lot!","I was trying to find an exact solution to , and naturally derived a solution defined by infinite tetrations of . I first defined a recursion equivalent to the definition of tetrations and then used invariant points to show that converges for . Now, I am curious if I could extend the definition of tetrations and find the convergence interval of the following function. I find this to be hard because I cannot, as in the case of normal tetrations, define a recursion that can be applied to define . Any tips on how to proceed will be appreciated. Thanks a lot!","x=a^x a \lim_{n \to \infty} (^nx) x \in [e^{-e}, e^{\frac{1}{e}}] f(x)=x^{{2x}^{{3x}^{{...}}}} f(x)","['real-analysis', 'sequences-and-series', 'complex-analysis', 'convergence-divergence', 'tetration']"
22,Why does this trick make the oscillating exponential integral converge?,Why does this trick make the oscillating exponential integral converge?,,"I have the following integral: $$\int_0^{+\infty} e^{iEt} dt,$$ where $E$ is a real constant. I know this integral does not converge. However, I have seen the following trick which makes it converge: \begin{align} \int_0^{+\infty} e^{iEt} dt &= \lim_{\epsilon \rightarrow 0} \int_0^{+\infty} e^{i(E+i\epsilon) t} dt\\ &= \lim_{\epsilon \rightarrow 0}\left[ \frac{1}{i(E+i\epsilon)} e^{i(E+i\epsilon) t} \right] _0^{+\infty}\\ &=\frac{1}{iE} \end{align} If it was just for this, I would have thought that this is completely nonsense, and that it is like thinking that the limit of a discontinuous function at a point actually equals the function at that point. However, if one uses this trick when calculating the solution of the non-homogeneous Klein-Gordon equation $$(\square-m^2)\phi(\vec{x})=g\delta^3(\vec{x})$$ where $g$ is a real constant, using the Green function $G(x)$ , where $x$ is a 4d vector, such that $$-(\square-m^2)G(x)=\delta^4(x)$$ the result is actually the Yukawa potential, which can obviously be calculated using other methods. I suspect that the delta is playing some role here, but I do not know distribution theory so I am asking here. What is really going on mathematically?","I have the following integral: where is a real constant. I know this integral does not converge. However, I have seen the following trick which makes it converge: If it was just for this, I would have thought that this is completely nonsense, and that it is like thinking that the limit of a discontinuous function at a point actually equals the function at that point. However, if one uses this trick when calculating the solution of the non-homogeneous Klein-Gordon equation where is a real constant, using the Green function , where is a 4d vector, such that the result is actually the Yukawa potential, which can obviously be calculated using other methods. I suspect that the delta is playing some role here, but I do not know distribution theory so I am asking here. What is really going on mathematically?","\int_0^{+\infty} e^{iEt} dt, E \begin{align}
\int_0^{+\infty} e^{iEt} dt &= \lim_{\epsilon \rightarrow 0} \int_0^{+\infty} e^{i(E+i\epsilon) t} dt\\
&= \lim_{\epsilon \rightarrow 0}\left[ \frac{1}{i(E+i\epsilon)} e^{i(E+i\epsilon) t} \right] _0^{+\infty}\\
&=\frac{1}{iE}
\end{align} (\square-m^2)\phi(\vec{x})=g\delta^3(\vec{x}) g G(x) x -(\square-m^2)G(x)=\delta^4(x)","['complex-analysis', 'improper-integrals', 'mathematical-physics', 'distribution-theory']"
23,Choice of deformed contour in steepest descent method,Choice of deformed contour in steepest descent method,,"I'm trying to learn about the method of steepest descent (from https://people.math.osu.edu/tanveer.1/m805/supplement.steepest.pdf ). An example there is $$\int_{-\infty}^{+\infty}e^{i\lambda(t+t^3/3)}dt$$ where $\lambda\rightarrow \infty$ . We have two saddle points, $t_0=\pm i$ at which the first derivative of $p=-i(t+t^3/3)$ is zero. Now, I'm trying to deform the contour to pass from either or both these points. Due to arguments that can be found in the link, the two deformations are (see p.7): $C_1$ : connects $\infty e^{5i\pi/6}$ to $\infty e^{i\pi/6}$ , passing through $+i$ . At the latter, it's parallel with the real axis. $C_2$ : connects $\infty e^{-5i\pi/6}$ to $\infty e^{-i\pi/6}$ , passing through $-i$ . At the latter, it's parallel with the real axis. The deformed contours cannot cross the real axis, so we can't deform either in order to get to both points with one contour. What I don't understand is how we choose one over the other. The author chooses $C_1$ and simply states: ""It is to be noted that even though $Re (p)$ is smaller at the other saddle $t = − i$ , there is no contribution from this since the steepest descent path equivalent to the original contour integral does not pass through $t = − i$ "". So, not only I do not understand why $C_2$ is equivalent to the original contour while $C_1$ is not, I don't get why the author even studied the $C_2$ case. Any help will be appreciated. EDIT : Since physicists tend to use a lot of saddle point approximations, maybe the question should transition to physics stackexchange?","I'm trying to learn about the method of steepest descent (from https://people.math.osu.edu/tanveer.1/m805/supplement.steepest.pdf ). An example there is where . We have two saddle points, at which the first derivative of is zero. Now, I'm trying to deform the contour to pass from either or both these points. Due to arguments that can be found in the link, the two deformations are (see p.7): : connects to , passing through . At the latter, it's parallel with the real axis. : connects to , passing through . At the latter, it's parallel with the real axis. The deformed contours cannot cross the real axis, so we can't deform either in order to get to both points with one contour. What I don't understand is how we choose one over the other. The author chooses and simply states: ""It is to be noted that even though is smaller at the other saddle , there is no contribution from this since the steepest descent path equivalent to the original contour integral does not pass through "". So, not only I do not understand why is equivalent to the original contour while is not, I don't get why the author even studied the case. Any help will be appreciated. EDIT : Since physicists tend to use a lot of saddle point approximations, maybe the question should transition to physics stackexchange?",\int_{-\infty}^{+\infty}e^{i\lambda(t+t^3/3)}dt \lambda\rightarrow \infty t_0=\pm i p=-i(t+t^3/3) C_1 \infty e^{5i\pi/6} \infty e^{i\pi/6} +i C_2 \infty e^{-5i\pi/6} \infty e^{-i\pi/6} -i C_1 Re (p) t = − i t = − i C_2 C_1 C_2,"['integration', 'complex-analysis', 'asymptotics', 'approximation']"
24,Generalisation of the idea of decomposition into even and odd functions,Generalisation of the idea of decomposition into even and odd functions,,"We know that a function $f(z)$ can be decomposed into $\frac{f(z)+f(-z)}{2}$ and $\frac{f(z)-f(-z)}{2}$ . These are called the even and odd components. I have made a generalisaiton of this. Suppose $f(z)$ is our function on the complex plane, and $e^{\frac{i2\pi q}{n}}$ are the $nth$ roots of unity, $q=0,1,2,3...n-1$ . We can decompose the function into $n$ functions $g_k(z)$ , $k=0,1,2,3....n-1$ given by: $g_k(z)= \frac{1}{n} \sum _{q=0} ^{n-1} e^{\frac{2\pi qk}{n}} f(e^{2\pi i \frac{q}{n}} z)$ Adding these back together: $$\sum _{k=0}^{n-1} g_k (z)$$ $$= \sum _{k=0}^{n-1} \frac{1}{n} \sum _{q=0} ^{n-1} e^{\frac{2\pi qk}{n}} f(e^{2\pi i \frac{q}{n}} z)$$ $$= \sum _{q=0}^{n-1} \frac{1}{n}( \sum _{k=0} ^{n-1} e^{\frac{2\pi qk}{n}} f(e^{2\pi i \frac{q}{n}} z))$$ If we look at the first term of this sum corresponding to $q=0$ , we get $\frac{1}{n} nf(z)=f(z)$ . The terms for other values of $q$ are 0. This means that the decomposition $g_k (z)$ adds upto $f(z)$ . For $n=2$ , this is the same as the decomposition of $f(z)$ into even and odd functions. This is a generalisation of the decomposition into even and odd functions. Each of the functions $g_k(z)$ has a rotational symmetry of rotation of the argument $z$ by $e^{\frac{i2\pi k}{n}}$ . Is there a name for this generalisation?","We know that a function can be decomposed into and . These are called the even and odd components. I have made a generalisaiton of this. Suppose is our function on the complex plane, and are the roots of unity, . We can decompose the function into functions , given by: Adding these back together: If we look at the first term of this sum corresponding to , we get . The terms for other values of are 0. This means that the decomposition adds upto . For , this is the same as the decomposition of into even and odd functions. This is a generalisation of the decomposition into even and odd functions. Each of the functions has a rotational symmetry of rotation of the argument by . Is there a name for this generalisation?","f(z) \frac{f(z)+f(-z)}{2} \frac{f(z)-f(-z)}{2} f(z) e^{\frac{i2\pi q}{n}} nth q=0,1,2,3...n-1 n g_k(z) k=0,1,2,3....n-1 g_k(z)= \frac{1}{n} \sum _{q=0} ^{n-1} e^{\frac{2\pi qk}{n}} f(e^{2\pi i \frac{q}{n}} z) \sum _{k=0}^{n-1} g_k (z) = \sum _{k=0}^{n-1} \frac{1}{n} \sum _{q=0} ^{n-1} e^{\frac{2\pi qk}{n}} f(e^{2\pi i \frac{q}{n}} z) = \sum _{q=0}^{n-1} \frac{1}{n}( \sum _{k=0} ^{n-1} e^{\frac{2\pi qk}{n}} f(e^{2\pi i \frac{q}{n}} z)) q=0 \frac{1}{n} nf(z)=f(z) q g_k (z) f(z) n=2 f(z) g_k(z) z e^{\frac{i2\pi k}{n}}","['complex-analysis', 'functions', 'even-and-odd-functions']"
25,"Let $f : D → D$ be analytic with $f(0) = f''(0) = 0, f'''(0) = 1$. Prove $\exists r$ independent of $f$ where $B_r(0)\subset f(D)$.",Let  be analytic with . Prove  independent of  where .,"f : D → D f(0) = f''(0) = 0, f'''(0) = 1 \exists r f B_r(0)\subset f(D)","Let $D$ be the unit disc centered at the origin. Assume $f : D → D$ is analytic with $f(0) = f''(0) = 0,$ and $f'''(0) = 1$ . Prove $\exists r$ independent of $f$ where $B_r(0)\subset f(D)$ . I'm completely lost on what to do here - the idea I initially started with was to use Schwarz's Lemma, which led me to the conclusion that $f(z) = \alpha z^3/6$ for some $|\alpha|<1$ . I don't feel particularly confident about this, though. Any ideas would be greatly appreciated. Edit: An attempted solution by another member of this site, which has since been deleted, tried using Schwarz Lemma on $g(z)=f(z)/z^2$ , which is only analytic if $f'(0)=0$ as well.  However, this is unfortunately not an assumption we are allowed to make. So, this either needs to be proven or a different method is needed.","Let be the unit disc centered at the origin. Assume is analytic with and . Prove independent of where . I'm completely lost on what to do here - the idea I initially started with was to use Schwarz's Lemma, which led me to the conclusion that for some . I don't feel particularly confident about this, though. Any ideas would be greatly appreciated. Edit: An attempted solution by another member of this site, which has since been deleted, tried using Schwarz Lemma on , which is only analytic if as well.  However, this is unfortunately not an assumption we are allowed to make. So, this either needs to be proven or a different method is needed.","D f : D → D f(0) = f''(0) = 0, f'''(0) = 1 \exists r f B_r(0)\subset f(D) f(z) = \alpha z^3/6 |\alpha|<1 g(z)=f(z)/z^2 f'(0)=0",['complex-analysis']
26,Riemann Zeta function's behavior near the pole and Euler-Mascheroni constant,Riemann Zeta function's behavior near the pole and Euler-Mascheroni constant,,"I want to show that $$\zeta(z) = \frac{1}{z-1} + \gamma   + O(z-1)$$ for $z \rightarrow 1 $ . Why is it enough to show this statement for real $z = s \in (1,2)$ ? We know that $\zeta$ is meromorphic and has only one simple pole at $z=1$ but how does this help? Now, for $s \in (1,2)$ we can write $$\zeta(z) - \frac{1}{z-1} = \sum_{n=1}^{\infty} k^{-s} - \int_{1}^{\infty}t^{-s}dt =\sum_{k=1}^{\infty}\int_k^{k+1}(k^{-s} - t^{-s}) dt$$ and since $|k^{-s}-t^{-s}| = |\int_{k}^{t}su^{-s-1}du| \leq |s|k^{-s-1}$ we know that there exists a constant M such that $$\sum_{k=1}^{\infty}\int_k^{k+1}(k^{-s} - t^{-s}) dt< M$$ for all $s \in (1,2)$ . This implies the whole thing is uniform there and that we are allowed pull in limit, in the end we have: $$\lim_{s \rightarrow 1}(\zeta(z) - \frac{1}{z-1}) = \sum_{k=1}^{\infty} \int_k^{k+1}(k^{-1}-t^{-1}) dt = \sum_{k=1}^{\infty}(k^{-1}-ln(k+1)+ln(k)) = \gamma$$ . I'm still a little confused with big O-notation, how does this imply our statement for general $z$ ?","I want to show that for . Why is it enough to show this statement for real ? We know that is meromorphic and has only one simple pole at but how does this help? Now, for we can write and since we know that there exists a constant M such that for all . This implies the whole thing is uniform there and that we are allowed pull in limit, in the end we have: . I'm still a little confused with big O-notation, how does this imply our statement for general ?","\zeta(z) = \frac{1}{z-1} + \gamma   + O(z-1) z \rightarrow 1  z = s \in (1,2) \zeta z=1 s \in (1,2) \zeta(z) - \frac{1}{z-1} = \sum_{n=1}^{\infty} k^{-s} - \int_{1}^{\infty}t^{-s}dt
=\sum_{k=1}^{\infty}\int_k^{k+1}(k^{-s} - t^{-s}) dt |k^{-s}-t^{-s}| = |\int_{k}^{t}su^{-s-1}du| \leq |s|k^{-s-1} \sum_{k=1}^{\infty}\int_k^{k+1}(k^{-s} - t^{-s}) dt< M s \in (1,2) \lim_{s \rightarrow 1}(\zeta(z) - \frac{1}{z-1}) = \sum_{k=1}^{\infty} \int_k^{k+1}(k^{-1}-t^{-1}) dt = \sum_{k=1}^{\infty}(k^{-1}-ln(k+1)+ln(k)) = \gamma z","['complex-analysis', 'riemann-zeta', 'euler-mascheroni-constant']"
27,Bessel function integral representation using Hankel's contour,Bessel function integral representation using Hankel's contour,,"This question appears in a set of  old qualifying exams I am using to practice. The Hankel path $H(c,R)$ is an infinite path that starts at a point at infinity $-\infty - ic$ to a point on a circle of $C_R$ radius $R$ ( $0<c<R$ ) centered at the origin along the line $y=-c$ , continuing along the an arc on $C_R$ to a point on the line $y=c$ , and then to the point at infinity $\infty+ic$ along the line $y=c$ . The problem asks us to show that the Bessel function $J_\nu$ of order $\nu\in\mathbb{C}$ can be expressed as $$ J_\nu(z)=\frac{1}{2\pi i}\oint_{H(c,R)}\lambda^{-\nu-1}e^{\frac12 z\left(\lambda-\frac1\lambda\right)}\ d\lambda,\qquad \mathfrak{R}(z)>0$$ In the first part of the problem, we prove that the Gamma function satisfies $$ \Gamma(z)=\frac{1}{2 i\sin(\pi z)}\oint_{H(c,R)}\lambda^{z-1} e^\lambda\ d\lambda, \qquad z\in\mathbb{C}\setminus\mathbb{Z}.$$ Using this in combination with Euler's reflection formula $\Gamma(z)\Gamma(1-z)=\frac{\pi}{\sin \pi z}$ gives $$\frac{1}{\Gamma(z)}=\frac{1}{2\pi i}\oint_{H(c,R)}\lambda^{-z}e^\lambda \ d\lambda \qquad z\in\mathbb{C}.$$ Using Cauchy's integration theorem, we also have that the formulas above are independent of $c$ and $R$ , i.e. we can deform $H(c,R)$ to any other Hankel path $H(c',R')$ . This allows us to  rewrite $J_\nu$ as $$J_\nu(z)=\sum^\infty_{n=0}\frac{(-1)^n}{n!\Gamma(n+\nu+1)}\Big(\frac{z}{2}\Big)^{\nu+2n}= \frac{1}{2\pi i}\sum^\infty_{n=0}\frac{(-1)^n}{n!}\Big(\frac{z}{2}\Big)^{2n+\nu}\oint_{H(c,R)}\lambda^{-\nu-n-1}e^\lambda \ d\lambda.$$ By analytic continuation, it suffices to assume $z>0$ . A change of variables $\lambda=\frac12 zw$ deforms $H(c,R)$ to $H(2z^{-1}c, 2z^{-1}R)$ . This change of variables, provided that order of summation and integration can be inverted, gives $$J_\nu(z)= \frac{1}{2\pi i}\oint_{H(2z^{-1}c,2z^{-1}R)}\sum^\infty_{n=0}\frac{(-1)^n}{n!} \Big( \frac{zw^{-1}}{2}\Big)^n w^{-\nu-1}e^{\frac12 zw} \ dw.$$ The desired formula follows easily from this (Cauchy integration type arguments allow us to deform the path $H(2z^{-1}c,2z^{-1}R)$ back to $H(c,R)$ ). Problem: I am having trouble justifying the change in the order of summation and integration. I am trying to use dominated convergence type of arguments but I am not getting very far. Any hints are appreciated. Thanks!","This question appears in a set of  old qualifying exams I am using to practice. The Hankel path is an infinite path that starts at a point at infinity to a point on a circle of radius ( ) centered at the origin along the line , continuing along the an arc on to a point on the line , and then to the point at infinity along the line . The problem asks us to show that the Bessel function of order can be expressed as In the first part of the problem, we prove that the Gamma function satisfies Using this in combination with Euler's reflection formula gives Using Cauchy's integration theorem, we also have that the formulas above are independent of and , i.e. we can deform to any other Hankel path . This allows us to  rewrite as By analytic continuation, it suffices to assume . A change of variables deforms to . This change of variables, provided that order of summation and integration can be inverted, gives The desired formula follows easily from this (Cauchy integration type arguments allow us to deform the path back to ). Problem: I am having trouble justifying the change in the order of summation and integration. I am trying to use dominated convergence type of arguments but I am not getting very far. Any hints are appreciated. Thanks!","H(c,R) -\infty - ic C_R R 0<c<R y=-c C_R y=c \infty+ic y=c J_\nu \nu\in\mathbb{C}  J_\nu(z)=\frac{1}{2\pi i}\oint_{H(c,R)}\lambda^{-\nu-1}e^{\frac12 z\left(\lambda-\frac1\lambda\right)}\ d\lambda,\qquad \mathfrak{R}(z)>0  \Gamma(z)=\frac{1}{2 i\sin(\pi z)}\oint_{H(c,R)}\lambda^{z-1} e^\lambda\ d\lambda, \qquad z\in\mathbb{C}\setminus\mathbb{Z}. \Gamma(z)\Gamma(1-z)=\frac{\pi}{\sin \pi z} \frac{1}{\Gamma(z)}=\frac{1}{2\pi i}\oint_{H(c,R)}\lambda^{-z}e^\lambda \ d\lambda \qquad z\in\mathbb{C}. c R H(c,R) H(c',R') J_\nu J_\nu(z)=\sum^\infty_{n=0}\frac{(-1)^n}{n!\Gamma(n+\nu+1)}\Big(\frac{z}{2}\Big)^{\nu+2n}=
\frac{1}{2\pi i}\sum^\infty_{n=0}\frac{(-1)^n}{n!}\Big(\frac{z}{2}\Big)^{2n+\nu}\oint_{H(c,R)}\lambda^{-\nu-n-1}e^\lambda \ d\lambda. z>0 \lambda=\frac12 zw H(c,R) H(2z^{-1}c, 2z^{-1}R) J_\nu(z)=
\frac{1}{2\pi i}\oint_{H(2z^{-1}c,2z^{-1}R)}\sum^\infty_{n=0}\frac{(-1)^n}{n!} \Big( \frac{zw^{-1}}{2}\Big)^n w^{-\nu-1}e^{\frac12 zw} \ dw. H(2z^{-1}c,2z^{-1}R) H(c,R)","['real-analysis', 'integration', 'complex-analysis', 'analysis', 'bessel-functions']"
28,"What does the equation $x^2+y^2=r^2$ represent when $x, y, r$ are complex numbers?",What does the equation  represent when  are complex numbers?,"x^2+y^2=r^2 x, y, r","I know this question is vague or maybe broad and subjective. But, I am interested in studying the equation $x^2+y^2=r^2$ when $x,y,r$ are complex numbers. What are a few directions that I can follow to study this equation? Is it even interesting (subjective, I know)? Are there already results for this equation?","I know this question is vague or maybe broad and subjective. But, I am interested in studying the equation when are complex numbers. What are a few directions that I can follow to study this equation? Is it even interesting (subjective, I know)? Are there already results for this equation?","x^2+y^2=r^2 x,y,r","['complex-analysis', 'functions', 'complex-numbers', 'systems-of-equations', 'research']"
29,Functional equation and elliptic functions,Functional equation and elliptic functions,,"I'm sorry if this question is similar to a one already asked, I'm not aware of many basic facts about elliptic curves. Let $g$ be a meromorphic function on $\mathbb{C}$ , and $\Lambda$ be a lattice in $\mathbb{C}$ . Suppose that  for any $\lambda\in \Lambda$ , there are constants $c_{\lambda}$ such that for $z\in \mathbb{C}$ (out of poles) the following holds: $$ g(z+k\lambda)=g(z)+kc_{\lambda}$$ Is it true that $g(z)=\alpha z+R(\mathcal{S})(z)$ for some elliptic function $\mathcal{S}$ , polynomial $R$ and constant $\alpha$ ? My ideas are as follow: The field of meromorphic functions on $\mathbb{C}/\Lambda$ is $\mathbb{C}(\mathcal{S},\mathcal{S}')$ for some elliptic function $\mathcal{S}$ (for example the Weirestrass function) and $(\mathcal{S}')^2 = Q(\mathcal{S})$ for some polynomial $Q$ The assumption on $g$ implies that $h(z):=g'(z)$ is elliptic, that is $h(z)=P(\mathcal{S},\mathcal{S}')(z)$ by 1. ""Locally"" (that is for $z$ near enough from a zero $z_0$ of $\mathcal{S}$ , in order to find a determination of $\sqrt{Q(\mathcal{S})}$ ) : $$g(z)-g(z_0)=\int_{[z_0,z]} h(u) du = \int_{[z_0,z]} \mathcal{S}'(u) \frac{P(\mathcal{S},\mathcal{S}')(u)}{\sqrt{Q(\mathcal{S}(u))}} du$$ We may decompose the last integral into an integral with residu, giving $\alpha z$ , and the remainder would be a function of $\mathcal{S}$ by an apporpriate change of variable. I'm not sure because I have difficulties to write formally the statement 4. Is there a counterexample or do you know a classical proof otherwise ? Thank you very much, best regards","I'm sorry if this question is similar to a one already asked, I'm not aware of many basic facts about elliptic curves. Let be a meromorphic function on , and be a lattice in . Suppose that  for any , there are constants such that for (out of poles) the following holds: Is it true that for some elliptic function , polynomial and constant ? My ideas are as follow: The field of meromorphic functions on is for some elliptic function (for example the Weirestrass function) and for some polynomial The assumption on implies that is elliptic, that is by 1. ""Locally"" (that is for near enough from a zero of , in order to find a determination of ) : We may decompose the last integral into an integral with residu, giving , and the remainder would be a function of by an apporpriate change of variable. I'm not sure because I have difficulties to write formally the statement 4. Is there a counterexample or do you know a classical proof otherwise ? Thank you very much, best regards","g \mathbb{C} \Lambda \mathbb{C} \lambda\in \Lambda c_{\lambda} z\in \mathbb{C}  g(z+k\lambda)=g(z)+kc_{\lambda} g(z)=\alpha z+R(\mathcal{S})(z) \mathcal{S} R \alpha \mathbb{C}/\Lambda \mathbb{C}(\mathcal{S},\mathcal{S}') \mathcal{S} (\mathcal{S}')^2 = Q(\mathcal{S}) Q g h(z):=g'(z) h(z)=P(\mathcal{S},\mathcal{S}')(z) z z_0 \mathcal{S} \sqrt{Q(\mathcal{S})} g(z)-g(z_0)=\int_{[z_0,z]} h(u) du = \int_{[z_0,z]} \mathcal{S}'(u) \frac{P(\mathcal{S},\mathcal{S}')(u)}{\sqrt{Q(\mathcal{S}(u))}} du \alpha z \mathcal{S}","['complex-analysis', 'complex-geometry', 'elliptic-curves', 'elliptic-functions']"
30,"$f$ non-constant and holomorphic, then for each $z_0$ there is a neighborhood where $f$ must take a different value from $f(z_0)$","non-constant and holomorphic, then for each  there is a neighborhood where  must take a different value from",f z_0 f f(z_0),"Let $f:\Omega \to \mathbb{C}$ be non-constant and holomorphic function and $\Omega\subseteq\mathbb{C}$ be an open region. We will show that for any $z_0\in\Omega$ , there is a neighborhood of $z_0$ such that for all $z\not=z_0$ in this neighborhood, $f(z)\not=f(z_0)$ . To do this, we need the result that If $f,g$ holomorphic on region $\Omega$ agree on a sequence of points with limit point in $\Omega$ , they are identical on $\Omega$ . Proof : We prove the claim in the title by contradiction. Suppose that for each neighborhood of $z_0$ in $\Omega$ , there is a $z$ in that neighborhood with $f(z)=f(z_0)$ . Since $\Omega$ is open, there is a $\varepsilon>0$ such that $D_\varepsilon(z_0)\subseteq\Omega$ . Consider the monotonically decreasing sequence $(r_n=\varepsilon/n)_n$ with $r_n\to 0$ . By assumption, for each $n\in\mathbb{N}_0$ there is a $z_n\in D_{r_n}(z_0)\setminus\{z_0\}$ such that $f(z_n)=f(z_0)$ . Then $z_n\to z_0$ . By the result above this implies that $f=f(z_0)$ on all of $\Omega$ , which is a contradiction. QED Question : I've only now stumbled on this fact about holomorphic functions, even though I've been studying the complex analysis course for an entire semester. It's not mentioned in the book. Is it an obvious property that is much more easily seen than my proof above, perhaps?","Let be non-constant and holomorphic function and be an open region. We will show that for any , there is a neighborhood of such that for all in this neighborhood, . To do this, we need the result that If holomorphic on region agree on a sequence of points with limit point in , they are identical on . Proof : We prove the claim in the title by contradiction. Suppose that for each neighborhood of in , there is a in that neighborhood with . Since is open, there is a such that . Consider the monotonically decreasing sequence with . By assumption, for each there is a such that . Then . By the result above this implies that on all of , which is a contradiction. QED Question : I've only now stumbled on this fact about holomorphic functions, even though I've been studying the complex analysis course for an entire semester. It's not mentioned in the book. Is it an obvious property that is much more easily seen than my proof above, perhaps?","f:\Omega \to \mathbb{C} \Omega\subseteq\mathbb{C} z_0\in\Omega z_0 z\not=z_0 f(z)\not=f(z_0) f,g \Omega \Omega \Omega z_0 \Omega z f(z)=f(z_0) \Omega \varepsilon>0 D_\varepsilon(z_0)\subseteq\Omega (r_n=\varepsilon/n)_n r_n\to 0 n\in\mathbb{N}_0 z_n\in D_{r_n}(z_0)\setminus\{z_0\} f(z_n)=f(z_0) z_n\to z_0 f=f(z_0) \Omega",['complex-analysis']
31,"Does there exist two linearly independent functions $u$ in $\mathbb{R}^{2}$ satsfying $\bar{\partial}^{2}u(x,y) + A(x,y)u(x,y) = 0$?",Does there exist two linearly independent functions  in  satsfying ?,"u \mathbb{R}^{2} \bar{\partial}^{2}u(x,y) + A(x,y)u(x,y) = 0","Does there exist two linearly independent functions $u$ in $\mathbb{R}^{2}$ satsfying $$\frac{\partial^{2}}{\partial \bar{z}^{2}}u(x,y) + A(x,y)u(x,y) = 0$$ where $A\in C^{\infty}(\mathbb{R}^{2})$ ? We define $$\frac{\partial}{\partial\bar{z}}u(x,y) ={\frac {1}{2}}\left({\frac {\partial }{\partial x}u(x,y)} + i{\frac {\partial }{\partial y}}u(x,y)\right).$$ We know that if $A$ is a antiholomorphic function, then our equation has two independent antiholomorphic solutions $u$ .","Does there exist two linearly independent functions in satsfying where ? We define We know that if is a antiholomorphic function, then our equation has two independent antiholomorphic solutions .","u \mathbb{R}^{2} \frac{\partial^{2}}{\partial \bar{z}^{2}}u(x,y) + A(x,y)u(x,y) = 0 A\in C^{\infty}(\mathbb{R}^{2}) \frac{\partial}{\partial\bar{z}}u(x,y) ={\frac {1}{2}}\left({\frac {\partial }{\partial x}u(x,y)} + i{\frac {\partial }{\partial y}}u(x,y)\right). A u","['complex-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'complex-geometry', 'riemann-surfaces']"
32,Computing a residue (complex analysis),Computing a residue (complex analysis),,"Compute the following residue: \begin{equation} \textrm{res}_{z=0}\frac{(\sin(z))^2}{(\sinh(z))^5} \end{equation} I thought of using the power series expansions, and trying to find the $\frac{1}{z}$ coefficient as follows: \begin{equation} \frac{\sin^2(z)}{\sinh^5(z)}=\frac{z^2(1-\frac{z^2}{6}+O(z^4))^2}{z^5(1+\frac{z^2}{6}+O(z^4))^5} \end{equation} But it seems very hard to find the $\frac{1}{z}$ term from this - any ideas?","Compute the following residue: I thought of using the power series expansions, and trying to find the coefficient as follows: But it seems very hard to find the term from this - any ideas?","\begin{equation}
\textrm{res}_{z=0}\frac{(\sin(z))^2}{(\sinh(z))^5}
\end{equation} \frac{1}{z} \begin{equation}
\frac{\sin^2(z)}{\sinh^5(z)}=\frac{z^2(1-\frac{z^2}{6}+O(z^4))^2}{z^5(1+\frac{z^2}{6}+O(z^4))^5}
\end{equation} \frac{1}{z}","['calculus', 'complex-analysis', 'analysis', 'complex-integration', 'residue-calculus']"
33,Question on meromorphic functions,Question on meromorphic functions,,"I'm trying to do this optative exercise from my complex analysis course: Check if series $$\sum_{k=-\infty}^{\infty}\left(\frac{1}{z-\pi k}\right)^2$$ converges for all $z\in \mathbb{C}$ to a meromorphic function $f_1(z)$ , so that if we consider the function $$f(z)=\frac{1}{\sin^2 z}$$ then $g(z)=f(z)-f_1(z)$ is an entire function. This exercise is way harder than the previous ones I've done and don't know how to approach it. I don't know if I should find $f_1(z)$ first or there's a way to prove the result without finding it. Thanks for the time and suggestions.","I'm trying to do this optative exercise from my complex analysis course: Check if series converges for all to a meromorphic function , so that if we consider the function then is an entire function. This exercise is way harder than the previous ones I've done and don't know how to approach it. I don't know if I should find first or there's a way to prove the result without finding it. Thanks for the time and suggestions.",\sum_{k=-\infty}^{\infty}\left(\frac{1}{z-\pi k}\right)^2 z\in \mathbb{C} f_1(z) f(z)=\frac{1}{\sin^2 z} g(z)=f(z)-f_1(z) f_1(z),"['sequences-and-series', 'complex-analysis', 'complex-numbers']"
34,Calculation of $\int_{-\infty}^\infty {\cos x\over a^2-x^2}\ dx$ using residue theorem,Calculation of  using residue theorem,\int_{-\infty}^\infty {\cos x\over a^2-x^2}\ dx,"Compute the integral using Residue theorem: $$\int_{-\infty}^\infty {\cos x\over a^2-x^2}\ dx\quad a>0.$$ My attempt: Choose the contour $$\Gamma = \{Re^{i\theta}:0\leq\theta\leq\pi\}\cup [-R,-a-\epsilon]\cup\{-a+\epsilon e^{i\theta}:0\leq\theta\leq\pi\}\cup[-a+\epsilon,a-\epsilon]\cup\{a+\epsilon e^{i\theta}:0\leq\theta\leq\pi\}\cup[a+\epsilon, R]$$ Denote the three upper half circle $C_R$ , $C_{\epsilon_1}$ and $C_{\epsilon_2}$ . The function ${e^{iz}\over a^2-z^2}$ is analytic on and inside of $\Gamma$ so by Cauchy theorem, the integral over $\Gamma$ is zero. \begin{align*} \int_{C_R}{e^{iz}\over a^2-z^2}\ dz & = \int_0^{\pi}{e^{i(Re^{i\theta})}\over a^2-R^2e^{2i\theta}}iRe^{i\theta}\ d\theta\\ & \leq\int_0^\pi{R\over R^2-a^2}e^{-R\sin\theta}\ d\theta\\ & = 2\int_0^{\pi/2}{R\over R^2-a^2}e^{-R\sin\theta}\ d\theta\\ &\leq 2\int_0^{\pi/2}{R\over R^2- a^2} e^{-R(2\theta/\pi)}\ d\theta\\ & = {2R\over R^2-a^2}\left(-{\pi\over 2R}\right)(e^{-R}-1)\to 0\quad R\to\infty.\\ \int_{C_{\epsilon_2}}{e^{iz}\over a^2-z^2}\ dz & = \int_0^{\pi}{e^{i(a+\epsilon e^{i\theta})}\over a^2 - (a+\epsilon e^{i\theta})^2} i \epsilon e^{i\theta}\ d\theta\\ & = \int_0^{\pi}{e^{ia+i\epsilon\cos\theta}e^{-\epsilon\sin\theta}\over -2a\epsilon e^{i\theta} - \epsilon^2 e^{2i\theta}}i\epsilon e^{i\theta}\ d\theta\\ & = \int_{0}^\pi - i{e^{ia+i\epsilon\cos\theta}e^{-\epsilon\sin\theta} -1\over 2a+\epsilon e^{i\theta}} - i{1\over 2a+\epsilon e^{i\theta}}\ d\theta\\ \end{align*} Note that \begin{align*} \left|{e^{ia+i\epsilon\cos\theta}e^{-\epsilon\sin\theta} -1\over 2a+\epsilon e^{i\theta}}\right|& \leq {1-e^{-\epsilon\sin\theta}\over 2a-\epsilon}\\ \int_0^{\pi}{1-e^{-\epsilon\sin\theta}\over 2a-\epsilon}\ d\theta & = {2\over 2a-\epsilon}\int_0^{\pi/2}1-e^{-\epsilon\sin\theta}\ d\theta\\ &\leq{2\over 2a-\epsilon}\int_0^{\pi/2}1-e^{-\epsilon}\ d\theta\to 0\quad\epsilon\to 0\\ \end{align*} Hence, $$ \int_{0}^\pi - i{e^{ia+i\epsilon\cos\theta}e^{-\epsilon\sin\theta} -1\over 2a+\epsilon e^{i\theta}} - i{1\over 2a+\epsilon e^{i\theta}}\ d\theta = -i\int_0^{\pi}{1\over 2a}\ d\theta = -{i\pi\over 2a}\quad\epsilon\to 0$$ Similarly, we can compute the integral over $C_{\epsilon_1}$ \begin{align*} \int_{\epsilon_1}{e^{iz}\over a^2-z^2}\ dz & = \int_0^{\pi}{e^{i(-a+\epsilon e^{i\theta})}\over a^2- (-a+\epsilon e^{i\theta})^2} i\epsilon e^{i\theta}\ d\theta\\ & = \int_0^\pi i{e^{i(-a+\epsilon\cos\theta)}e^{-\epsilon\sin\theta}-1\over 2a-\epsilon e^{i\theta}}\ d\theta + \int_0^{\pi}i{1\over 2a-\epsilon e^{i\theta}}\ d\theta\to {i\pi\over 2a}\quad \epsilon\to 0\\ \end{align*} So the integral is $0$ ? I can't find the error here. Please help. By the way, the contour containing the poles by reflecting $C_{\epsilon_1}$ and $C_{\epsilon_2}$ will give the same calculation except that residue appears now. But they cancel each other $$\int_{-\infty}^\infty{\cos x\over a^2-x^2}\ dx = \operatorname{Re}\left[2\pi i\operatorname{Res}\left({e^{iz}\over a^2-z^2}, z = \pm a\right)\right] = 0.$$ N.B. The integration is interpreted as $$\int_{-\infty}^\infty = \lim_{R\to\infty,\epsilon\to 0+}\left(\int_{-R}^{-a-\epsilon}+\int_{-a+\epsilon}^{a-\epsilon}+\int_{a+\epsilon}^R\right).$$","Compute the integral using Residue theorem: My attempt: Choose the contour Denote the three upper half circle , and . The function is analytic on and inside of so by Cauchy theorem, the integral over is zero. Note that Hence, Similarly, we can compute the integral over So the integral is ? I can't find the error here. Please help. By the way, the contour containing the poles by reflecting and will give the same calculation except that residue appears now. But they cancel each other N.B. The integration is interpreted as","\int_{-\infty}^\infty {\cos x\over a^2-x^2}\ dx\quad a>0. \Gamma = \{Re^{i\theta}:0\leq\theta\leq\pi\}\cup [-R,-a-\epsilon]\cup\{-a+\epsilon e^{i\theta}:0\leq\theta\leq\pi\}\cup[-a+\epsilon,a-\epsilon]\cup\{a+\epsilon e^{i\theta}:0\leq\theta\leq\pi\}\cup[a+\epsilon, R] C_R C_{\epsilon_1} C_{\epsilon_2} {e^{iz}\over a^2-z^2} \Gamma \Gamma \begin{align*}
\int_{C_R}{e^{iz}\over a^2-z^2}\ dz & = \int_0^{\pi}{e^{i(Re^{i\theta})}\over a^2-R^2e^{2i\theta}}iRe^{i\theta}\ d\theta\\
& \leq\int_0^\pi{R\over R^2-a^2}e^{-R\sin\theta}\ d\theta\\
& = 2\int_0^{\pi/2}{R\over R^2-a^2}e^{-R\sin\theta}\ d\theta\\
&\leq 2\int_0^{\pi/2}{R\over R^2- a^2} e^{-R(2\theta/\pi)}\ d\theta\\
& = {2R\over R^2-a^2}\left(-{\pi\over 2R}\right)(e^{-R}-1)\to 0\quad R\to\infty.\\
\int_{C_{\epsilon_2}}{e^{iz}\over a^2-z^2}\ dz & = \int_0^{\pi}{e^{i(a+\epsilon e^{i\theta})}\over a^2 - (a+\epsilon e^{i\theta})^2} i \epsilon e^{i\theta}\ d\theta\\
& = \int_0^{\pi}{e^{ia+i\epsilon\cos\theta}e^{-\epsilon\sin\theta}\over -2a\epsilon e^{i\theta} - \epsilon^2 e^{2i\theta}}i\epsilon e^{i\theta}\ d\theta\\
& = \int_{0}^\pi - i{e^{ia+i\epsilon\cos\theta}e^{-\epsilon\sin\theta} -1\over 2a+\epsilon e^{i\theta}} - i{1\over 2a+\epsilon e^{i\theta}}\ d\theta\\
\end{align*} \begin{align*}
\left|{e^{ia+i\epsilon\cos\theta}e^{-\epsilon\sin\theta} -1\over 2a+\epsilon e^{i\theta}}\right|& \leq {1-e^{-\epsilon\sin\theta}\over 2a-\epsilon}\\
\int_0^{\pi}{1-e^{-\epsilon\sin\theta}\over 2a-\epsilon}\ d\theta & = {2\over 2a-\epsilon}\int_0^{\pi/2}1-e^{-\epsilon\sin\theta}\ d\theta\\
&\leq{2\over 2a-\epsilon}\int_0^{\pi/2}1-e^{-\epsilon}\ d\theta\to 0\quad\epsilon\to 0\\
\end{align*}  \int_{0}^\pi - i{e^{ia+i\epsilon\cos\theta}e^{-\epsilon\sin\theta} -1\over 2a+\epsilon e^{i\theta}} - i{1\over 2a+\epsilon e^{i\theta}}\ d\theta = -i\int_0^{\pi}{1\over 2a}\ d\theta = -{i\pi\over 2a}\quad\epsilon\to 0 C_{\epsilon_1} \begin{align*}
\int_{\epsilon_1}{e^{iz}\over a^2-z^2}\ dz & = \int_0^{\pi}{e^{i(-a+\epsilon e^{i\theta})}\over a^2- (-a+\epsilon e^{i\theta})^2} i\epsilon e^{i\theta}\ d\theta\\
& = \int_0^\pi i{e^{i(-a+\epsilon\cos\theta)}e^{-\epsilon\sin\theta}-1\over 2a-\epsilon e^{i\theta}}\ d\theta + \int_0^{\pi}i{1\over 2a-\epsilon e^{i\theta}}\ d\theta\to {i\pi\over 2a}\quad \epsilon\to 0\\
\end{align*} 0 C_{\epsilon_1} C_{\epsilon_2} \int_{-\infty}^\infty{\cos x\over a^2-x^2}\ dx = \operatorname{Re}\left[2\pi i\operatorname{Res}\left({e^{iz}\over a^2-z^2}, z = \pm a\right)\right] = 0. \int_{-\infty}^\infty = \lim_{R\to\infty,\epsilon\to 0+}\left(\int_{-R}^{-a-\epsilon}+\int_{-a+\epsilon}^{a-\epsilon}+\int_{a+\epsilon}^R\right).","['complex-analysis', 'residue-calculus']"
35,Stein complex analysis 6.2,Stein complex analysis 6.2,,"Prove that $$\prod_{n=1}^\infty{n(n+a+b)\over(n+a)(n+b)} = {\Gamma(a+1)\Gamma(b+1)\over\Gamma(a+b+1)}$$ whenever $a$ and $b$ are positive. Using the product formula for $\sin\pi s$ , give another proof that $\Gamma(s)\Gamma(1-s) =\pi/\sin\pi s$ . My attempt: First of all, I guess the assumption $a,b$ are positive can be dropped. I think it should be dropped because we need to use that product formula to show $\Gamma(s)\Gamma(1-s) = \pi/\sin\pi s$ . Recall the product formula for entire function $1/\Gamma(s)$ : $${1\over\Gamma(s)}= e^{\gamma s}s\prod_{n=1}^\infty\left(1+{s\over n}\right)e^{-{s\over n}}$$ for all $s\in\Bbb C$ . In particular, term-by-term multiplication of infinite products is allowed. \begin{align*} {\Gamma(a+1)\Gamma(b+1)\over\Gamma(a+b+1)} & = {e^{\gamma(a+b+1)}(a+b+1)\prod_{n=1}^\infty\left(1+{a+b+1\over n}\right)e^{-{a+b+1\over n}}\over e^{\gamma(a+1)}(a+1)\prod_{n=1}^\infty\left(1+{a+1\over n}e^{-{a+1\over n}}\right)e^{\gamma(b+1)}(b+1)\prod_{n=1}^\infty\left(1+{b+1\over n}\right)e^{-{b+1\over n}}}\\ & = {e^{\gamma(a+b+1)}(a+b+1)\over e^{\gamma(a+b+2)}(a+1)(b+1)}\prod_{n=1}^\infty{\left(1+{a+b+1\over n}\right)e^{-{a+b+1\over n}}\over \left(1+{a+1\over n}\right)\left(1+{b+1\over n}\right)e^{-{a+b+2\over n}}}\\ & = {e^{\gamma(a+b+1)(a+b+1)}\over e^{\gamma(a+b+2)}(a+1)(b+1)}\prod_{n=1}^\infty{(a+b+n+1)n\over (a+n+1)(b+n+1)}e^{1\over n}\\ & = e^{-\gamma}\prod_{n=1}^\infty{n(a+b+n)\over(a+n)(b+n)}e^{1\over n}\\ \end{align*} Now observe that using $\Gamma(1) =1$ , \begin{align*} 1 & = e^{\gamma}\prod_{n=1}^\infty\left(1+{1\over n}\right)e^{-{1\over n}}\\ e^{-\gamma} & = \prod_{n=1}^\infty{n+1\over n}e^{-{1\over n}}.\\ \end{align*} Hence, \begin{align*} e^{-\gamma}\prod_{n=1}^\infty{n(a+b+n)\over(a+n)(b+n)}e^{1\over n} & = \prod_{n=1}^\infty{(n+1)(a+b+n)\over(a+n)(b+n)}\\ & = \prod_{n=1}^\infty{n(a+b+n)\over (a+n)(b+n)}\\ \end{align*} Does it make sense?","Prove that whenever and are positive. Using the product formula for , give another proof that . My attempt: First of all, I guess the assumption are positive can be dropped. I think it should be dropped because we need to use that product formula to show . Recall the product formula for entire function : for all . In particular, term-by-term multiplication of infinite products is allowed. Now observe that using , Hence, Does it make sense?","\prod_{n=1}^\infty{n(n+a+b)\over(n+a)(n+b)} = {\Gamma(a+1)\Gamma(b+1)\over\Gamma(a+b+1)} a b \sin\pi s \Gamma(s)\Gamma(1-s) =\pi/\sin\pi s a,b \Gamma(s)\Gamma(1-s) = \pi/\sin\pi s 1/\Gamma(s) {1\over\Gamma(s)}= e^{\gamma s}s\prod_{n=1}^\infty\left(1+{s\over n}\right)e^{-{s\over n}} s\in\Bbb C \begin{align*}
{\Gamma(a+1)\Gamma(b+1)\over\Gamma(a+b+1)} & = {e^{\gamma(a+b+1)}(a+b+1)\prod_{n=1}^\infty\left(1+{a+b+1\over n}\right)e^{-{a+b+1\over n}}\over e^{\gamma(a+1)}(a+1)\prod_{n=1}^\infty\left(1+{a+1\over n}e^{-{a+1\over n}}\right)e^{\gamma(b+1)}(b+1)\prod_{n=1}^\infty\left(1+{b+1\over n}\right)e^{-{b+1\over n}}}\\
& = {e^{\gamma(a+b+1)}(a+b+1)\over e^{\gamma(a+b+2)}(a+1)(b+1)}\prod_{n=1}^\infty{\left(1+{a+b+1\over n}\right)e^{-{a+b+1\over n}}\over \left(1+{a+1\over n}\right)\left(1+{b+1\over n}\right)e^{-{a+b+2\over n}}}\\
& = {e^{\gamma(a+b+1)(a+b+1)}\over e^{\gamma(a+b+2)}(a+1)(b+1)}\prod_{n=1}^\infty{(a+b+n+1)n\over (a+n+1)(b+n+1)}e^{1\over n}\\
& = e^{-\gamma}\prod_{n=1}^\infty{n(a+b+n)\over(a+n)(b+n)}e^{1\over n}\\
\end{align*} \Gamma(1) =1 \begin{align*}
1 & = e^{\gamma}\prod_{n=1}^\infty\left(1+{1\over n}\right)e^{-{1\over n}}\\
e^{-\gamma} & = \prod_{n=1}^\infty{n+1\over n}e^{-{1\over n}}.\\
\end{align*} \begin{align*}
e^{-\gamma}\prod_{n=1}^\infty{n(a+b+n)\over(a+n)(b+n)}e^{1\over n} & = \prod_{n=1}^\infty{(n+1)(a+b+n)\over(a+n)(b+n)}\\
& = \prod_{n=1}^\infty{n(a+b+n)\over (a+n)(b+n)}\\
\end{align*}","['complex-analysis', 'solution-verification', 'gamma-function', 'infinite-product']"
36,Prove for every $R>0$ there's an integer $n_0>0$ such that if $n\geq n_0$ then $f_n$ has no zeroes. Using Hurwitz Theorem.,Prove for every  there's an integer  such that if  then  has no zeroes. Using Hurwitz Theorem.,R>0 n_0>0 n\geq n_0 f_n,"Let $$f_n(z)=\sum_{k=0}^n \frac{z^k}{k!}$$ and let $$f(z)=\sum_{k=0}^\infty \frac{z^k}{k!}=e^z.$$ As a polynomial $f_n$ has $n$ roots in $\Bbb{C}$ . Prove for every $R>0$ there's an integer $n_0>0$ such that if $n\geq n_0$ then $f_n$ has no zeroes in $B_R(0)$ . So I want to appeal to Hurwitz's theorem that States if $f_n \rightarrow f$ uniformly on every compact subset of $\Bbb{C}$ which the $f_n$ do converge uniformly to $e^z$ on every compact subset, then if $f$ has a zero at $z_0$ of order $m$ so do the $f_n$ . Then since $f(z)$ has no zeroes, can I conclude the $f_n$ 's have no zeroes? Or do I need to find such an $n_0$ .","Let and let As a polynomial has roots in . Prove for every there's an integer such that if then has no zeroes in . So I want to appeal to Hurwitz's theorem that States if uniformly on every compact subset of which the do converge uniformly to on every compact subset, then if has a zero at of order so do the . Then since has no zeroes, can I conclude the 's have no zeroes? Or do I need to find such an .",f_n(z)=\sum_{k=0}^n \frac{z^k}{k!} f(z)=\sum_{k=0}^\infty \frac{z^k}{k!}=e^z. f_n n \Bbb{C} R>0 n_0>0 n\geq n_0 f_n B_R(0) f_n \rightarrow f \Bbb{C} f_n e^z f z_0 m f_n f(z) f_n n_0,['complex-analysis']
37,why Hard Lefschetz is stated in the real coefficient in Huybrechts' complex geometry book?,why Hard Lefschetz is stated in the real coefficient in Huybrechts' complex geometry book?,,"In  Huybrechts' complex geometry book , the Hard Lefschetz theorem is stated as follows: Proposition 3.3.13 (Hard Lefschetz theorem) Let $(X, g)$ be a compact Kähler manifold of dimension $n$ . Then for $k \leq n$ $$ L^{n-k}: H^k(X, \mathbb{R}) \cong H^{2 n-k}(X, \mathbb{R}) $$ and for any $k$ $$ H^k(X, \mathbb{R})=\bigoplus_{i \geq 0} L^i H^{k-2 i}(X, \mathbb{R})_{\mathrm{p}} $$ Moreover, both assertions respect the bidegree decomposition. In particular, $H^k(X, \mathbb{R})_{\mathrm{p}} \otimes \mathbb{C}=\bigoplus_{p+q=k} H^{p, q}(X)_{\mathrm{p}}$ Proof. The first assertion is iii) of Remark 3.2.7. (which used the isomorphism of harmonic forms). There seem to have many things to talk about the coefficient of the cohomology. In the book, we use that Lefschetz operator induced isomorphism on the harmonic form to prove the statement. However, the harmonic form is defined using the complex coefficient. I was confused why we can use it for the real coefficient case?","In  Huybrechts' complex geometry book , the Hard Lefschetz theorem is stated as follows: Proposition 3.3.13 (Hard Lefschetz theorem) Let be a compact Kähler manifold of dimension . Then for and for any Moreover, both assertions respect the bidegree decomposition. In particular, Proof. The first assertion is iii) of Remark 3.2.7. (which used the isomorphism of harmonic forms). There seem to have many things to talk about the coefficient of the cohomology. In the book, we use that Lefschetz operator induced isomorphism on the harmonic form to prove the statement. However, the harmonic form is defined using the complex coefficient. I was confused why we can use it for the real coefficient case?","(X, g) n k \leq n 
L^{n-k}: H^k(X, \mathbb{R}) \cong H^{2 n-k}(X, \mathbb{R})
 k 
H^k(X, \mathbb{R})=\bigoplus_{i \geq 0} L^i H^{k-2 i}(X, \mathbb{R})_{\mathrm{p}}
 H^k(X, \mathbb{R})_{\mathrm{p}} \otimes \mathbb{C}=\bigoplus_{p+q=k} H^{p, q}(X)_{\mathrm{p}}","['complex-analysis', 'differential-geometry', 'algebraic-geometry', 'manifolds', 'complex-geometry']"
38,How to actually plot Riemann Surfaces?,How to actually plot Riemann Surfaces?,,"I took a course in complex variables and I remember reading something about Riemann surfaces on Wikipedia . There are even some examples in this page: I am curious about the following: How do we actually plot them? The definition in there doesn't look like anything I could translate into points $(x,y,z)$ . I've been trying to guess for a while how to plot the Riemann surface for $f(z)=z^{\frac{1}{2}}$ but with no success. I think we need to compute the branches for $f(z)=z^{\frac{1}{2}}$ and we can do that with the following formula: $$f_k(z)=|z|^{\frac{1}{n} } \left( \cos\left(\frac{\arg z + 2 k \pi}{n}\right) + i\sin\left(\frac{\arg z + 2 k \pi}{n}\right) \right) \quad k=0,1,2,..,n-1 $$ So in our case, we have $f_0$ and $f_1$ with $n=2$ . Now I tried to plot the following sets of points: $$(a,b,\arg f_0(a+bi))\qquad (a,b,\arg f_1(a+bi))$$ And I got this: $\quad \quad \quad \quad \quad \quad \quad $ Which kinda looks like it can be cut and glued like the figure I gave previously. Is it correct? EDIT: I had some progress on it, I read on Agarwal's Introduction to Complex Analysis: I tried to do as he describes and am plotting the function $z=w^2$ , with the restrictions he gave in the text. By plotting the pairs $(\Re(z),\Im(z),\arg(z))$ I obtained this: I don't know if this is the correct result but it still doesn't look like the one I found on Wikipedia:","I took a course in complex variables and I remember reading something about Riemann surfaces on Wikipedia . There are even some examples in this page: I am curious about the following: How do we actually plot them? The definition in there doesn't look like anything I could translate into points . I've been trying to guess for a while how to plot the Riemann surface for but with no success. I think we need to compute the branches for and we can do that with the following formula: So in our case, we have and with . Now I tried to plot the following sets of points: And I got this: Which kinda looks like it can be cut and glued like the figure I gave previously. Is it correct? EDIT: I had some progress on it, I read on Agarwal's Introduction to Complex Analysis: I tried to do as he describes and am plotting the function , with the restrictions he gave in the text. By plotting the pairs I obtained this: I don't know if this is the correct result but it still doesn't look like the one I found on Wikipedia:","(x,y,z) f(z)=z^{\frac{1}{2}} f(z)=z^{\frac{1}{2}} f_k(z)=|z|^{\frac{1}{n} } \left( \cos\left(\frac{\arg z + 2 k \pi}{n}\right) + i\sin\left(\frac{\arg z + 2 k \pi}{n}\right) \right) \quad k=0,1,2,..,n-1  f_0 f_1 n=2 (a,b,\arg f_0(a+bi))\qquad (a,b,\arg f_1(a+bi)) \quad \quad \quad \quad \quad \quad \quad  z=w^2 (\Re(z),\Im(z),\arg(z))",['complex-analysis']
39,Complex functions with similar magnitude,Complex functions with similar magnitude,,"Let $f$ and $g$ be complex-analytic functions on the unit disk $D_1$ , and suppose that $$ \sup_{|z|\leq 1} \big||f(z)|-|g(z)|\big| \leq \epsilon. $$ I am curious whether there exists some $\theta\in \mathbb{R}$ such that on the half-disk $$ \sup_{|z|\leq 1/2} |f(z)- e^{i\theta} g(z)| \leq C \epsilon. $$ I can show that this holds when $g=1$ is the constant function.  In this case I can use the fact that $|f|+\epsilon -1$ is subharmonic and positive in order to apply Cacciopoli's inequality to get a bound on $\int_{D_{3/4}} \big|\nabla |f|\big|^2$ .  Then I can use the fact that $\big|\nabla |f|\big| = |f'|$ to show that on $D_{1/2}$ , $$ |f'|\leq C\delta. $$ This shows that $f$ is close to a constant, as desired. This argument does not work when $g$ is not constant because $|f|-|g|+\epsilon$ is not subharmonic.  I am curious if there is another way to proceed. One barrier that makes the argument difficult to find is the example $f(z)=z$ and $g(z)=\overline{z}$ .  In this case $g$ is not analytic so the hypotheses do not hold, but it does show that the desired bound will not follow from only using bounds such as the maximum principle on $f$ and $g$ in isolation (perhaps one can use that $fg$ is also analytic, for example).","Let and be complex-analytic functions on the unit disk , and suppose that I am curious whether there exists some such that on the half-disk I can show that this holds when is the constant function.  In this case I can use the fact that is subharmonic and positive in order to apply Cacciopoli's inequality to get a bound on .  Then I can use the fact that to show that on , This shows that is close to a constant, as desired. This argument does not work when is not constant because is not subharmonic.  I am curious if there is another way to proceed. One barrier that makes the argument difficult to find is the example and .  In this case is not analytic so the hypotheses do not hold, but it does show that the desired bound will not follow from only using bounds such as the maximum principle on and in isolation (perhaps one can use that is also analytic, for example).","f g D_1 
\sup_{|z|\leq 1} \big||f(z)|-|g(z)|\big| \leq \epsilon.
 \theta\in \mathbb{R} 
\sup_{|z|\leq 1/2} |f(z)- e^{i\theta} g(z)| \leq C \epsilon.
 g=1 |f|+\epsilon -1 \int_{D_{3/4}} \big|\nabla |f|\big|^2 \big|\nabla |f|\big| = |f'| D_{1/2} 
|f'|\leq C\delta.
 f g |f|-|g|+\epsilon f(z)=z g(z)=\overline{z} g f g fg",['complex-analysis']
40,First Fundamental Form and the Metric,First Fundamental Form and the Metric,,"On the upper half-plane with standard $x,y$ coordinates we have the hyperbolic metric given by $g_{11}=g_{22}=1/y^2$ and $g_{12}=0$ . In terms of the first fundamental form, we write $$ds^2=\frac{dx^2+dy^2}{y^2}= \frac{-4\,dz\,d\bar{z}}{(z-\bar{z})^2}.$$ I can interpret $dz$ and $d\bar{z}$ as (complex-valued) $1$ -forms on the upper half plane. Suppose I now want to show that a fractional linear transformation $z\mapsto z'$ where $z'=\frac{az+b}{cz+d}$ and $ad-bc=1$ gives an isometry. By definition of an isometry, I would want to show that the pushforward of two tangent vectors in the domain yield the same result evaluated under the metric. But when I see other people approach the problem, what they do is calculate $dz'$ and $d\bar{z'}$ , and then prove that $$\frac{-4\,dz'\,d\bar{z'}}{(z'-\bar{z'})^2}=\frac{-4\,dz\,d\bar{z}}{(z-\bar{z})^2}.$$ I'm a little confused by what this calculation is actually doing. I would think that what I need to do is evaluate the same fundamental form (without the 's in the numerator) at the point $z'$ and $\bar{z'}$ on the pushforward of some tangent vectors in the domain.  I am not sure why it is valid to simply substitute in $dz'$ and $\bar{dz'}$ and compute ""the same thing"", and along the way I seem to have lost any intuition of what $dz'$ and $d\bar{z'}$ really mean as $1$ -forms.","On the upper half-plane with standard coordinates we have the hyperbolic metric given by and . In terms of the first fundamental form, we write I can interpret and as (complex-valued) -forms on the upper half plane. Suppose I now want to show that a fractional linear transformation where and gives an isometry. By definition of an isometry, I would want to show that the pushforward of two tangent vectors in the domain yield the same result evaluated under the metric. But when I see other people approach the problem, what they do is calculate and , and then prove that I'm a little confused by what this calculation is actually doing. I would think that what I need to do is evaluate the same fundamental form (without the 's in the numerator) at the point and on the pushforward of some tangent vectors in the domain.  I am not sure why it is valid to simply substitute in and and compute ""the same thing"", and along the way I seem to have lost any intuition of what and really mean as -forms.","x,y g_{11}=g_{22}=1/y^2 g_{12}=0 ds^2=\frac{dx^2+dy^2}{y^2}=
\frac{-4\,dz\,d\bar{z}}{(z-\bar{z})^2}. dz d\bar{z} 1 z\mapsto z' z'=\frac{az+b}{cz+d} ad-bc=1 dz' d\bar{z'} \frac{-4\,dz'\,d\bar{z'}}{(z'-\bar{z'})^2}=\frac{-4\,dz\,d\bar{z}}{(z-\bar{z})^2}. z' \bar{z'} dz' \bar{dz'} dz' d\bar{z'} 1","['complex-analysis', 'differential-geometry', 'riemannian-geometry']"
41,Convergence of $\sum_{\alpha\in \Bbb N^n} z^\alpha$ in $\{z\in \Bbb C^n: |z_i| < 1 \text{ for all } 1\le i\le n\}$,Convergence of  in,\sum_{\alpha\in \Bbb N^n} z^\alpha \{z\in \Bbb C^n: |z_i| < 1 \text{ for all } 1\le i\le n\},"I am reading Introduction to Complex Analysis in Several Variables by Volker Scheidemann. The convergence of power series in several complex variables is defined in the following way: Let $\{c_\alpha\}_{\alpha\in\Bbb N^n} \subset\Bbb C$ . The power series $\sum_{\alpha\in \Bbb N^n}c_\alpha (z-a)^\alpha = \sum_{\alpha\in \Bbb N^n} c_\alpha (z_1-a_1)^{\alpha_1} \ldots (z_n-a_n)^{\alpha_n}$ in $n$ variables $z:= (z_1,z_2,\ldots,z_n)$ centered at $a\in \Bbb C^n$ converges at $w\in \Bbb C^n$ if there exists $C> 0$ such that $$\sum_{\alpha\in F} |c_\alpha| |(w-a)^\alpha| \le C$$ for all finite subsets $F\subset\Bbb N^n$ . I am trying to understand the convergence behavior of $\sum_{\alpha\in \Bbb N^n} z^\alpha$ in the unit polycylinder $P^n_1(\mathbf 0) := \{z\in \Bbb C^n: |z_i| < 1 \text{ for all } 1\le i\le n\}$ . This is Example $1.5.7$ of the book. Let $z\in P^n_1(0)$ and $F\subset \Bbb N^n$ be finite. There exists $q\in [0,1)$ such that $|z_j| \le q$ for all $1\le j\le n$ . Hence, $$\sum_{\alpha\in F} |z^\alpha| = \sum_{\alpha\in F} |z_1|^{\alpha_1} \ldots |z_n|^{\alpha_n} \le \color{blue}{\sum_{\alpha\in F} q^{\alpha_1 + \ldots + \alpha_n} \le \sum_{j=0}^\infty q^j} = \frac{1}{1-q}$$ Why is the inequality in blue true? Certainly, there may exist distinct $\beta,\gamma\in F$ such that $\sum_{i=1}^n \beta_i = \sum_{i=1}^n \gamma_i$ . In such a case, I don't see how to get the above inequality. Perhaps the homogeneous expansion may be useful at some point: $$\sum_{\alpha\in \Bbb N^n} c_\alpha (w-a)^\alpha = \sum_{j=0}^\infty \left(\sum_{|\alpha| = j} c_\alpha (w-a)^\alpha \right)$$ In fact, using the above homogeneous expansion (once convergence issues are settled) the author goes on to say that $$\color{blue}{\sum_{k=0}^\infty \left(\sum_{|\alpha| = k} z^{\alpha} \right) = \sum_{k_1 = 0}^\infty \ldots \sum_{k_n = 0}^\infty z_1^{\alpha_1} \ldots z_n^{\alpha_n}}$$ Why is this true? Notation . For $\alpha\in \Bbb N^n$ , $|\alpha|$ denotes $\alpha_1 + \ldots + \alpha_n$ .","I am reading Introduction to Complex Analysis in Several Variables by Volker Scheidemann. The convergence of power series in several complex variables is defined in the following way: Let . The power series in variables centered at converges at if there exists such that for all finite subsets . I am trying to understand the convergence behavior of in the unit polycylinder . This is Example of the book. Let and be finite. There exists such that for all . Hence, Why is the inequality in blue true? Certainly, there may exist distinct such that . In such a case, I don't see how to get the above inequality. Perhaps the homogeneous expansion may be useful at some point: In fact, using the above homogeneous expansion (once convergence issues are settled) the author goes on to say that Why is this true? Notation . For , denotes .","\{c_\alpha\}_{\alpha\in\Bbb N^n} \subset\Bbb C \sum_{\alpha\in \Bbb N^n}c_\alpha (z-a)^\alpha = \sum_{\alpha\in \Bbb N^n} c_\alpha (z_1-a_1)^{\alpha_1} \ldots (z_n-a_n)^{\alpha_n} n z:= (z_1,z_2,\ldots,z_n) a\in \Bbb C^n w\in \Bbb C^n C> 0 \sum_{\alpha\in F} |c_\alpha| |(w-a)^\alpha| \le C F\subset\Bbb N^n \sum_{\alpha\in \Bbb N^n} z^\alpha P^n_1(\mathbf 0) := \{z\in \Bbb C^n: |z_i| < 1 \text{ for all } 1\le i\le n\} 1.5.7 z\in P^n_1(0) F\subset \Bbb N^n q\in [0,1) |z_j| \le q 1\le j\le n \sum_{\alpha\in F} |z^\alpha| = \sum_{\alpha\in F} |z_1|^{\alpha_1} \ldots |z_n|^{\alpha_n} \le \color{blue}{\sum_{\alpha\in F} q^{\alpha_1 + \ldots + \alpha_n} \le \sum_{j=0}^\infty q^j} = \frac{1}{1-q} \beta,\gamma\in F \sum_{i=1}^n \beta_i = \sum_{i=1}^n \gamma_i \sum_{\alpha\in \Bbb N^n} c_\alpha (w-a)^\alpha = \sum_{j=0}^\infty \left(\sum_{|\alpha| = j} c_\alpha (w-a)^\alpha \right) \color{blue}{\sum_{k=0}^\infty \left(\sum_{|\alpha| = k} z^{\alpha} \right) = \sum_{k_1 = 0}^\infty \ldots \sum_{k_n = 0}^\infty z_1^{\alpha_1} \ldots z_n^{\alpha_n}} \alpha\in \Bbb N^n |\alpha| \alpha_1 + \ldots + \alpha_n","['complex-analysis', 'analysis', 'convergence-divergence', 'power-series', 'several-complex-variables']"
42,Expressing $\int_0^{+ \infty}\frac{\mathrm{e}^{- t^2} \mathrm{d} t}{z^2 - t^2}$ with the error function (complex analysis or Fubini theorem ?),Expressing  with the error function (complex analysis or Fubini theorem ?),\int_0^{+ \infty}\frac{\mathrm{e}^{- t^2} \mathrm{d} t}{z^2 - t^2},"I would like to show the following (Abrahomitz et Stegun) for any $z\in\mathbb{C}, \Im(z)>0$ , $$      \mathrm{e}^{- z^2} \left( 1 + \frac{2 i}{\sqrt{\pi}} \int_0^z      \mathrm{e}^{t^2} \mathrm{d} t \right) = \frac{2 iz}{\pi} \int_0^{+ \infty}      \frac{\mathrm{e}^{- t^2} \mathrm{d} t}{z^2 - t^2} \quad, \Im (z) > 0\\ $$ I tried two methods so far: Evaluating the integral on the right with the help of complex analysis, I didn't manage to make this work as I couldn't use Jordan Lemma because of divergence of $\mathrm{e}^{-R^2(\cos(2\theta)+i\sin(2\theta))}$ for large $R$ Expressing the integrand in RHS $\times \mathrm{e}^{z^2}$ as an integral and applying Fubuni theorem: \begin{align} \int_0^{+ \infty} \int_{- \infty}^1 \mathrm{e}^{- a (t^2 - z^2)} \mathrm{d} a      \mathrm{d} t = \int_{- \infty}^1 \mathrm{e}^{az^2} \int_0^{+ \infty} \mathrm{e}^{-      at^2} \mathrm{d} t \mathrm{d} a \end{align} Then using expression for Gaussian integral and splitting the $]-\infty;1]$ into $]-\infty;0]$ and $]0;1]$ : \begin{align} \frac{1}{2} \int_{- \infty}^1 \mathrm{e}^{az^2} \sqrt{\frac{\pi}{a}} \mathrm{d} a      = \frac{\sqrt{\pi}}{2} \left( \int_{- \infty}^0      \frac{\mathrm{e}^{az^2}}{\sqrt{a}} \mathrm{d} a + \int_0^1      \frac{\mathrm{e}^{az^2}}{\sqrt{a}} \mathrm{d} a \right) \end{align} With substitution and using Gaussian formula again I get the good result, but then I realized ... there is a problem in writing: $$   \frac{\mathrm{e}^{- (t^2 - z^2)}}{z^2 - t^2} = \int_{- \infty}^1 \mathrm{e}^{- a      (t^2 - z^2)} \mathrm{d} a $$ as we don't have: $$   \lim_{a \rightarrow - \infty} \mathrm{e}^{- a (t^2 - z^2)} = 0 $$ How can I conclude ? Thanks.","I would like to show the following (Abrahomitz et Stegun) for any , I tried two methods so far: Evaluating the integral on the right with the help of complex analysis, I didn't manage to make this work as I couldn't use Jordan Lemma because of divergence of for large Expressing the integrand in RHS as an integral and applying Fubuni theorem: Then using expression for Gaussian integral and splitting the into and : With substitution and using Gaussian formula again I get the good result, but then I realized ... there is a problem in writing: as we don't have: How can I conclude ? Thanks.","z\in\mathbb{C}, \Im(z)>0 
     \mathrm{e}^{- z^2} \left( 1 + \frac{2 i}{\sqrt{\pi}} \int_0^z
     \mathrm{e}^{t^2} \mathrm{d} t \right) = \frac{2 iz}{\pi} \int_0^{+ \infty}
     \frac{\mathrm{e}^{- t^2} \mathrm{d} t}{z^2 - t^2} \quad, \Im (z) > 0\\
 \mathrm{e}^{-R^2(\cos(2\theta)+i\sin(2\theta))} R \times \mathrm{e}^{z^2} \begin{align} \int_0^{+ \infty} \int_{- \infty}^1 \mathrm{e}^{- a (t^2 - z^2)} \mathrm{d} a
     \mathrm{d} t = \int_{- \infty}^1 \mathrm{e}^{az^2} \int_0^{+ \infty} \mathrm{e}^{-
     at^2} \mathrm{d} t \mathrm{d} a \end{align} ]-\infty;1] ]-\infty;0] ]0;1] \begin{align} \frac{1}{2} \int_{- \infty}^1 \mathrm{e}^{az^2} \sqrt{\frac{\pi}{a}} \mathrm{d} a
     = \frac{\sqrt{\pi}}{2} \left( \int_{- \infty}^0
     \frac{\mathrm{e}^{az^2}}{\sqrt{a}} \mathrm{d} a + \int_0^1
     \frac{\mathrm{e}^{az^2}}{\sqrt{a}} \mathrm{d} a \right) \end{align}    \frac{\mathrm{e}^{- (t^2 - z^2)}}{z^2 - t^2} = \int_{- \infty}^1 \mathrm{e}^{- a
     (t^2 - z^2)} \mathrm{d} a     \lim_{a \rightarrow - \infty} \mathrm{e}^{- a (t^2 - z^2)} = 0 ","['complex-analysis', 'error-function']"
43,"What is the principal branch of the complex arcsine, in simple terms?","What is the principal branch of the complex arcsine, in simple terms?",,"By simple, I mean something akin to $\{z: -\pi < \mathrm{Im}\, z \le  \pi\}$ for the logarithm or $\{z: -\pi/2 < \mathrm{Arg}\, z \le \pi/2\}$ for the square root. I am aware of the answer, ""the range of $z \mapsto -i\,\mathrm{Log}(iz + \sqrt{1-z^2})$ "", where one uses the principal branch of the square root, but I have having trouble sketching that region.  Clearly, it is not the $-i$ times the principal branch of the logarithm ( i.e. , $\{z: -\pi < \mathrm{Re}\, z \le \pi\}$ ), as sine is not one-to-one on that region.  The trouble lies in figuring out what the principal branch of $z \mapsto iz + \sqrt{1-z^2}$ is. Before looking at other sources, I tried determining a branch of the arcsine from the relation, $\sin(x + iy) = \sin x \cosh y + i \cos x \sinh y$ .  From this, I got $\{z: |\mathrm{Re} \, z| < \pi/2\} \cup \{z: \mathrm{Re}\, z=\pm \pi/2, \mathrm{Im}\,z \ge 0\}$ as a natural candidate.  (I think that's a branch, isn't it?  That is, I've convinced myself that sine provides a bijection from this region to the entire complex plane, but I still doubt myself a little.)  But this turns out not to be the principal branch, for $\mathrm{Arcsin}(2) = \pi/2 - i\ln(2+\sqrt{3})$ , with a negative imaginary part.  ( $\sin[\pi/2 + i\ln(2+\sqrt{3})]$ is indeed 2.) It turns out that $\mathrm{sgn \, Im\, Arcsin}\, a = -\mathrm{sgn}\,a$ when $a$ is a real number whose magnitude exceeds unity.  This maintains $\mathrm{Arcsin}(-z)= -\mathrm{Arcsin}\,z$ .  So now I hypothesize that the principal branch is almost the one I suggested as my natural candidate above.  Just replace the positive imaginary parts with their negatives when the real part is $\pi/2$ .  Haven't proved this, though.  (Somehow, it would feel more natural to me if the signs of the real and imaginary parts matched on these boundaries.) This comes up because, as a teacher and tutor of precalculus, I often remark in passing that arcsin(2) is undefined until you get to complex analysis (which is never for almost all of my students), where things gets really hairy. I haven't done complex analysis in years, though I was assigned to teach it to undergraduates once. Just got me wondering.  I've tried very hard, but, surprisingly, I have been unable to find an answer to my question on the 'net.","By simple, I mean something akin to for the logarithm or for the square root. I am aware of the answer, ""the range of "", where one uses the principal branch of the square root, but I have having trouble sketching that region.  Clearly, it is not the times the principal branch of the logarithm ( i.e. , ), as sine is not one-to-one on that region.  The trouble lies in figuring out what the principal branch of is. Before looking at other sources, I tried determining a branch of the arcsine from the relation, .  From this, I got as a natural candidate.  (I think that's a branch, isn't it?  That is, I've convinced myself that sine provides a bijection from this region to the entire complex plane, but I still doubt myself a little.)  But this turns out not to be the principal branch, for , with a negative imaginary part.  ( is indeed 2.) It turns out that when is a real number whose magnitude exceeds unity.  This maintains .  So now I hypothesize that the principal branch is almost the one I suggested as my natural candidate above.  Just replace the positive imaginary parts with their negatives when the real part is .  Haven't proved this, though.  (Somehow, it would feel more natural to me if the signs of the real and imaginary parts matched on these boundaries.) This comes up because, as a teacher and tutor of precalculus, I often remark in passing that arcsin(2) is undefined until you get to complex analysis (which is never for almost all of my students), where things gets really hairy. I haven't done complex analysis in years, though I was assigned to teach it to undergraduates once. Just got me wondering.  I've tried very hard, but, surprisingly, I have been unable to find an answer to my question on the 'net.","\{z: -\pi < \mathrm{Im}\, z \le  \pi\} \{z: -\pi/2 < \mathrm{Arg}\, z \le \pi/2\} z \mapsto -i\,\mathrm{Log}(iz + \sqrt{1-z^2}) -i \{z: -\pi < \mathrm{Re}\, z \le \pi\} z \mapsto iz + \sqrt{1-z^2} \sin(x + iy) = \sin x \cosh y + i \cos x \sinh y \{z: |\mathrm{Re} \, z| < \pi/2\} \cup \{z: \mathrm{Re}\, z=\pm \pi/2, \mathrm{Im}\,z \ge 0\} \mathrm{Arcsin}(2) = \pi/2 - i\ln(2+\sqrt{3}) \sin[\pi/2 + i\ln(2+\sqrt{3})] \mathrm{sgn \, Im\, Arcsin}\, a = -\mathrm{sgn}\,a a \mathrm{Arcsin}(-z)= -\mathrm{Arcsin}\,z \pi/2","['complex-analysis', 'trigonometry', 'inverse-function']"
44,Calculating the contour integral: $\int_{|z-z_0|=r}\frac{1}{\bar{z}}dz$,Calculating the contour integral:,\int_{|z-z_0|=r}\frac{1}{\bar{z}}dz,"I'm trying to find out what the following contour integral equals to: $$\int_{|z-z_0|=r}\frac{1}{\bar{z}}dz$$ given that $z_0$ is a point in the complex plane, where $|z_0|\neq r >0$ . Due to the fact that the function is not holomorphic, I'm not able to apply any theorems that require this condition. If I parametrize the contour, I get: $$i\int_0^{2\pi}\frac{re^{2it}}{\bar{z_0}e^{it}+r}dt$$ When $z_0=0$ , the answer is trivial. However, I have difficulty calculating this integral when $z_0$ is any other point. I would appreciate some help.","I'm trying to find out what the following contour integral equals to: given that is a point in the complex plane, where . Due to the fact that the function is not holomorphic, I'm not able to apply any theorems that require this condition. If I parametrize the contour, I get: When , the answer is trivial. However, I have difficulty calculating this integral when is any other point. I would appreciate some help.",\int_{|z-z_0|=r}\frac{1}{\bar{z}}dz z_0 |z_0|\neq r >0 i\int_0^{2\pi}\frac{re^{2it}}{\bar{z_0}e^{it}+r}dt z_0=0 z_0,"['complex-analysis', 'complex-integration']"
45,Is there a simply connected region with only two (or 1<N<$+\infty$) boundary points?,Is there a simply connected region with only two (or 1<N<) boundary points?,+\infty,"My textbook states the Riemann mapping theorem as follows: If D is a simply-connected domain on the extended complex plane that has at least two boundary points ... (translated) I'm wondering what a simply-connected region with only two boundary points would be. Definition of simply-connected domain: For every simple closed curve C in domain D, all points in the interior of C are also in D, where the ""interior"" means: a simple closed curve in the plane divides the plane into two regions, one exterior, one interior. What I know: There are both points belonging and not belonging to the set in any neighborhood of the boundary point. The complement of a simply-connected region is a connected region. PS. I major in physics. I don't know much about this problem and the translation maybe not very clear. The textbook is specially written for physics students as well. Thanks a lot.","My textbook states the Riemann mapping theorem as follows: If D is a simply-connected domain on the extended complex plane that has at least two boundary points ... (translated) I'm wondering what a simply-connected region with only two boundary points would be. Definition of simply-connected domain: For every simple closed curve C in domain D, all points in the interior of C are also in D, where the ""interior"" means: a simple closed curve in the plane divides the plane into two regions, one exterior, one interior. What I know: There are both points belonging and not belonging to the set in any neighborhood of the boundary point. The complement of a simply-connected region is a connected region. PS. I major in physics. I don't know much about this problem and the translation maybe not very clear. The textbook is specially written for physics students as well. Thanks a lot.",,"['complex-analysis', 'connectedness']"
46,conformal map from disc to funny domain,conformal map from disc to funny domain,,"I am working on numerical experiment about the Riemann mapping theorem. In order to test my program, I would like to have some test function on highly non-symmetric and non convex shape, funnier than this one : Ideally, something like the standard test domain, like the rabbit: But I need an exact conformal map like a rational function: $$\left(\sum_{0}^n a_k z^k\right)/\left(\sum_{0}^m b_k z^k\right), $$ or at least something I can give to python. Of course for the rabbit like domain, I am not expected to easily get something  perfectly resembling with a rational function, but at least something that is roughly comparable. Does anyone knows where I can find this can kind of conformal parametrization? I have google many key words without any good result. Thank in advance! Edit: Thanks to Matt E., I get the desired shape, an I am also able to interpolate, but the code for the interpolation using Bezier curves, so the coefficients correspond to splines functions. How can get some trigonometric ones with python? Then I have to hope that the harmonic extension is bijective in the interior of the disc.","I am working on numerical experiment about the Riemann mapping theorem. In order to test my program, I would like to have some test function on highly non-symmetric and non convex shape, funnier than this one : Ideally, something like the standard test domain, like the rabbit: But I need an exact conformal map like a rational function: or at least something I can give to python. Of course for the rabbit like domain, I am not expected to easily get something  perfectly resembling with a rational function, but at least something that is roughly comparable. Does anyone knows where I can find this can kind of conformal parametrization? I have google many key words without any good result. Thank in advance! Edit: Thanks to Matt E., I get the desired shape, an I am also able to interpolate, but the code for the interpolation using Bezier curves, so the coefficients correspond to splines functions. How can get some trigonometric ones with python? Then I have to hope that the harmonic extension is bijective in the interior of the disc.","\left(\sum_{0}^n a_k z^k\right)/\left(\sum_{0}^m b_k z^k\right), ","['complex-analysis', 'conformal-geometry']"
47,Component wise convergence,Component wise convergence,,"Attempt: Comparing real and imaginary parts we get: $r_n \cos \theta_n \to r \cos \theta$ $r_n \sin \theta_n \to r \sin \theta$ $\,\,$ Thus, we get: $r_n \cos \theta_n \, . \,r_n \cos \theta_n \to r \cos \theta \, . \, r \cos \theta$ $r_n \sin \theta_n \, . \,r_n \sin \theta_n \to r \sin \theta \, . \, r \sin \theta$ $\implies r_n^2 \sin^2 \theta_n \, + \,r_n^2 \cos^2 \theta_n \to r^2 \sin^2 \theta \,+ \, r^2\cos^2 \theta$ $\implies r_n^2  \to r^2$ How do I conclude from here that $r_n \to r$ ? Should I approach this differently? Edit: I just realized I could just compose with square root function. $\lim_{n \to \infty} r_n=\lim_{n \to \infty} \sqrt{{r_n}^2}= \sqrt{\lim_{n \to \infty} {r_n}^2} = \sqrt{r^2}=r$","Attempt: Comparing real and imaginary parts we get: Thus, we get: How do I conclude from here that ? Should I approach this differently? Edit: I just realized I could just compose with square root function.","r_n \cos \theta_n \to r \cos \theta r_n \sin \theta_n \to r \sin \theta \,\, r_n \cos \theta_n \, . \,r_n \cos \theta_n \to r \cos \theta \, . \, r \cos \theta r_n \sin \theta_n \, . \,r_n \sin \theta_n \to r \sin \theta \, . \, r \sin \theta \implies r_n^2 \sin^2 \theta_n \, + \,r_n^2 \cos^2 \theta_n \to r^2 \sin^2 \theta \,+ \, r^2\cos^2 \theta \implies r_n^2  \to r^2 r_n \to r \lim_{n \to \infty} r_n=\lim_{n \to \infty} \sqrt{{r_n}^2}= \sqrt{\lim_{n \to \infty} {r_n}^2} = \sqrt{r^2}=r","['sequences-and-series', 'complex-analysis']"
48,Given product and convolution of pair of functions can you find original pair?,Given product and convolution of pair of functions can you find original pair?,,"Suppose you have two functions $f,g: \mathbb{R} \rightarrow \mathbb{C}$ and you have their product and convolution $$ h_1(t) = fg$$ $$ h_2(t) = \int_{-\infty}^{\infty} f(t-\tau)g(\tau) d\tau $$ Is it possible to express $f,g$ solely in terms of $h_1$ , $h_2$ ? I have reason to believe no, but it should be possible up to a constant factor that is on the unit circle. I would like to ask: how to go about doing so? Why I think it is possible: I believe if you know the entire momentum probability distribution $|\phi(p,t)|^2$ and position probability distribution $|\psi(x,t)|^2$ of an object you should be able to recover the wave function of the object up to a multiplicative factor of a root of unity. If we define the physicists ""fourier transform"" as $$ \mathcal{F}_x[f] = \frac{1}{\sqrt{2\pi\hbar}} \int_{-\infty}^{\infty}f(x) e^{\frac{ipx}{\hbar}} dx$$ Then we can state the famous momentum position relationship as $$ \psi(x,t) = F_x[\phi(p,t)]$$ $$ \phi(p,t) = F_p[\psi(x,t)]$$ It then follows that: $$ \sqrt{2\pi\hbar} \mathcal{F}_x^{-1} [|\phi(p,t)|^2] = \psi(x,t) \star \psi^*(x,t)$$ $$ |\psi(x,t)|^2 = \psi(x,t)\psi^*(x,t) $$ Where $\star$ indicates convolution.  I'm taking a bet that it should be possible to solve for $\psi, \psi^*$ here up to a multiplicative root of unity, although I am not clear how to do so.","Suppose you have two functions and you have their product and convolution Is it possible to express solely in terms of , ? I have reason to believe no, but it should be possible up to a constant factor that is on the unit circle. I would like to ask: how to go about doing so? Why I think it is possible: I believe if you know the entire momentum probability distribution and position probability distribution of an object you should be able to recover the wave function of the object up to a multiplicative factor of a root of unity. If we define the physicists ""fourier transform"" as Then we can state the famous momentum position relationship as It then follows that: Where indicates convolution.  I'm taking a bet that it should be possible to solve for here up to a multiplicative root of unity, although I am not clear how to do so.","f,g: \mathbb{R} \rightarrow \mathbb{C}  h_1(t) = fg  h_2(t) = \int_{-\infty}^{\infty} f(t-\tau)g(\tau) d\tau  f,g h_1 h_2 |\phi(p,t)|^2 |\psi(x,t)|^2  \mathcal{F}_x[f] = \frac{1}{\sqrt{2\pi\hbar}} \int_{-\infty}^{\infty}f(x) e^{\frac{ipx}{\hbar}} dx  \psi(x,t) = F_x[\phi(p,t)]  \phi(p,t) = F_p[\psi(x,t)]  \sqrt{2\pi\hbar} \mathcal{F}_x^{-1} [|\phi(p,t)|^2] = \psi(x,t) \star \psi^*(x,t)  |\psi(x,t)|^2 = \psi(x,t)\psi^*(x,t)  \star \psi, \psi^*","['complex-analysis', 'probability-distributions', 'fourier-analysis', 'convolution', 'quantum-mechanics']"
49,Computing $\int_0^\infty \frac{dx}{(x^2 + 1)^2} $ with the residue theorem,Computing  with the residue theorem,\int_0^\infty \frac{dx}{(x^2 + 1)^2} ,"I am solving this integral and am stuck on proving an inequality, I believe I have the rest worked out. My work: First observe that the integrand is even, hence $$\int_0^\infty \frac{dx}{(x^2+1)^2} = \frac{1}{2}\int_{-\infty}^\infty \frac{dx}{(x^2+1)^2}$$ We now consider the integral on the r.h.s. above and integrate over the upper half circle with radius $R$ and the segment $[-R, R]$ on the real axis. We let $R$ tend towards infinity. Denoting this region by $D$ , and the upper half circle by $C$ , by Cauchy's Residue Theorem we have, $$\int_{D} \frac{dz}{(z^2+1)^2} = 2\pi i \sum \text{Res} = \int_{-\infty}^\infty \frac{dx}{(x^2+1)^2} + \int_C \frac{dz}{(z^2+1)^2}$$ I first calculate the residues of the integrand in $D$ . Notice there is a pole of order 2 at $z = i$ . Hence, $$\text{Res}_{z = i} = \lim_{z \rightarrow i} \Bigl( \frac{(z-i)^2}{(z^2+1)^2} \Bigr)' = \frac{1}{4i}$$ Thus, $$\frac{\pi}{2} = \int_{-\infty}^\infty \frac{dx}{(x^2+1)^2} + \int_C \frac{dz}{(z^2+1)^2}$$ Now I would like to show that the complex integral on the r.h.s. tends to 0 as $R$ tends to infinity: $$\int_C \frac{dz}{(z^2+1)^2} \leq \Biggl \vert \int_C \frac{dz}{(z^2+1)^2} \Biggr \vert \leq \sup f(X) \cdot L = \sup f(X) \cdot \pi R$$ This is as far as I have gotten. I know I have to play with the triangle inequality somehow to bound the integrand from above and determine $\sup f(x)$ , but I cannot figure out how to do this. One thought I had was to use the reverse triangle inequality since: $$|(z^2+1)^2| = |z^4 +2z^2 + 1| \geq \bigl| |z^4| - |2z|^2 - 1\ \bigr|$$ so that, $$\frac{1}{(z^2+1)^2} \leq \frac{1}{\bigl| |z^4| - |2z|^2 - 1\ \bigr|} = \frac{1}{\bigl| R^4 - 2R^2 - 1\ \bigr|}$$ This will wrap things up as $$\sup f(x) \cdot R = \frac{\pi R}{\bigl| R^4 - 2R^2 - 1\ \bigr|}$$ which will tend to 0 as $R$ tends to infinity. Putting it all together and diving by two, $$\int_0^\infty \frac{dx}{(x^2 + 1)^2} = \frac{\pi}{4}$$ This is the correct numerical answer, but I am not sure if my use of the reverse triangle inequality is correct, can anyone confirm? Thanks!","I am solving this integral and am stuck on proving an inequality, I believe I have the rest worked out. My work: First observe that the integrand is even, hence We now consider the integral on the r.h.s. above and integrate over the upper half circle with radius and the segment on the real axis. We let tend towards infinity. Denoting this region by , and the upper half circle by , by Cauchy's Residue Theorem we have, I first calculate the residues of the integrand in . Notice there is a pole of order 2 at . Hence, Thus, Now I would like to show that the complex integral on the r.h.s. tends to 0 as tends to infinity: This is as far as I have gotten. I know I have to play with the triangle inequality somehow to bound the integrand from above and determine , but I cannot figure out how to do this. One thought I had was to use the reverse triangle inequality since: so that, This will wrap things up as which will tend to 0 as tends to infinity. Putting it all together and diving by two, This is the correct numerical answer, but I am not sure if my use of the reverse triangle inequality is correct, can anyone confirm? Thanks!","\int_0^\infty \frac{dx}{(x^2+1)^2} = \frac{1}{2}\int_{-\infty}^\infty \frac{dx}{(x^2+1)^2} R [-R, R] R D C \int_{D} \frac{dz}{(z^2+1)^2} = 2\pi i \sum \text{Res} = \int_{-\infty}^\infty \frac{dx}{(x^2+1)^2} + \int_C \frac{dz}{(z^2+1)^2} D z = i \text{Res}_{z = i} = \lim_{z \rightarrow i} \Bigl( \frac{(z-i)^2}{(z^2+1)^2} \Bigr)' = \frac{1}{4i} \frac{\pi}{2} = \int_{-\infty}^\infty \frac{dx}{(x^2+1)^2} + \int_C \frac{dz}{(z^2+1)^2} R \int_C \frac{dz}{(z^2+1)^2} \leq \Biggl \vert \int_C \frac{dz}{(z^2+1)^2} \Biggr \vert \leq \sup f(X) \cdot L = \sup f(X) \cdot \pi R \sup f(x) |(z^2+1)^2| = |z^4 +2z^2 + 1| \geq \bigl| |z^4| - |2z|^2 - 1\ \bigr| \frac{1}{(z^2+1)^2} \leq \frac{1}{\bigl| |z^4| - |2z|^2 - 1\ \bigr|} = \frac{1}{\bigl| R^4 - 2R^2 - 1\ \bigr|} \sup f(x) \cdot R = \frac{\pi R}{\bigl| R^4 - 2R^2 - 1\ \bigr|} R \int_0^\infty \frac{dx}{(x^2 + 1)^2} = \frac{\pi}{4}","['calculus', 'integration', 'complex-analysis', 'improper-integrals', 'contour-integration']"
50,power series expansion of holomorphic function self-composes to the identity,power series expansion of holomorphic function self-composes to the identity,,"I struggle with the following problem that I came across dealing with discontinuous group actions on Riemann surfaces. Let $D\subseteq\mathbb{C}$ be a domain that contains $0$ and $f:D\to\mathbb{C}$ a holomorphic function with the power series expansion $f(z)=\sum_{i=0}^\infty a_iz^i$ at $z=0$ . The function satifies $f(0)=0$ , i.e. $a_0 = 0$ , and there exists a $n\in\mathbb{N}$ such that $f_k:=\underbrace{f\circ...\circ f}_{k\text{-times}}\neq\operatorname{Id}$ for $k=1,...,n-1$ and $f_n=\operatorname{Id}$ . Prove or disprove the following : $a_k=0$ for all $k>1$ and $a_1$ is a $n$ -th root of unity. Surely $a_1$ is a primitive root of unity. If $f$ was a polynomial, the rest would be easy too (although I have a feeling that it is easy either way and I'm just blind): if $f(z)=\sum_{i=0}^m a_iz^i$ , then the leading coefficient of $f_n$ would be a power of $a_m$ and equal to zero, so $a_m=0$ , and by induction the same follows for all coefficients. With $f$ not being a polynomial, I thought I could start computing the second coefficient of $f_m$ and show that $a_2=0$ and so on. However, the second coefficients is, if I'm correct, $a_2\sum_{j=m-1}^{2m-2}a^j$ . Since $a_1$ must be a primitve $n$ -th root of unity, the sum is equal to the sum of all $n$ -th roots of unity and hence 0. Thus the second coefficient of $f_n$ is zero independent of what $a_2$ is. Same for the third coefficient. I feel like computing coefficients won't help here and some other (maybe geometric) tool is needed, but I can't think of one that helps.","I struggle with the following problem that I came across dealing with discontinuous group actions on Riemann surfaces. Let be a domain that contains and a holomorphic function with the power series expansion at . The function satifies , i.e. , and there exists a such that for and . Prove or disprove the following : for all and is a -th root of unity. Surely is a primitive root of unity. If was a polynomial, the rest would be easy too (although I have a feeling that it is easy either way and I'm just blind): if , then the leading coefficient of would be a power of and equal to zero, so , and by induction the same follows for all coefficients. With not being a polynomial, I thought I could start computing the second coefficient of and show that and so on. However, the second coefficients is, if I'm correct, . Since must be a primitve -th root of unity, the sum is equal to the sum of all -th roots of unity and hence 0. Thus the second coefficient of is zero independent of what is. Same for the third coefficient. I feel like computing coefficients won't help here and some other (maybe geometric) tool is needed, but I can't think of one that helps.","D\subseteq\mathbb{C} 0 f:D\to\mathbb{C} f(z)=\sum_{i=0}^\infty a_iz^i z=0 f(0)=0 a_0 = 0 n\in\mathbb{N} f_k:=\underbrace{f\circ...\circ f}_{k\text{-times}}\neq\operatorname{Id} k=1,...,n-1 f_n=\operatorname{Id} a_k=0 k>1 a_1 n a_1 f f(z)=\sum_{i=0}^m a_iz^i f_n a_m a_m=0 f f_m a_2=0 a_2\sum_{j=m-1}^{2m-2}a^j a_1 n n f_n a_2","['complex-analysis', 'analysis', 'power-series']"
51,How to show Riemann zeta function $\zeta(s)$ is holomorphic (except at s=1)?,How to show Riemann zeta function  is holomorphic (except at s=1)?,\zeta(s),"The Riemann zeta function $\zeta(s)$ is known to be holomorphic, except at $s=1$ . I am not trained to university maths, but I interpret this to mean: it is complex differentiable at every point (smoothly changing values, no discontinuities) and is equal to Taylor series developed around any point in its domain Question: How do we show $\zeta(s)$ is holomorphic? Is it enough to show that the series which represent it, $\zeta(s)=\sum 1/n^s$ valid for $\Re(s)>1$ for example, are infinitely differentiable , except at the known pole at $s=1$ ? Is it also correct to say that by the principle of analytic continuation, any other series, such as $\zeta(s)=(1-s^{1-s})^{-1}\eta(s)$ valid over larger domains, are also holomorphic, except at any new poles. By separate analysis, there are no new poles in $0<\Re(s)\leq 1$ . Apologies if this question seems naive, I am self-teaching.","The Riemann zeta function is known to be holomorphic, except at . I am not trained to university maths, but I interpret this to mean: it is complex differentiable at every point (smoothly changing values, no discontinuities) and is equal to Taylor series developed around any point in its domain Question: How do we show is holomorphic? Is it enough to show that the series which represent it, valid for for example, are infinitely differentiable , except at the known pole at ? Is it also correct to say that by the principle of analytic continuation, any other series, such as valid over larger domains, are also holomorphic, except at any new poles. By separate analysis, there are no new poles in . Apologies if this question seems naive, I am self-teaching.",\zeta(s) s=1 \zeta(s) \zeta(s)=\sum 1/n^s \Re(s)>1 s=1 \zeta(s)=(1-s^{1-s})^{-1}\eta(s) 0<\Re(s)\leq 1,"['complex-analysis', 'riemann-zeta']"
52,Laplace's equation in the first quadrant,Laplace's equation in the first quadrant,,"I am trying to find a non-constant harmonic function $\phi$ on $Q = \{z \mid \mathfrak{R}(z), \mathfrak{I}(z) > 0\}$ , (i.e. the first quadrant) such that $\phi$ extends continuously to $\overline{Q} \setminus \{0\}$ with constant values on each the components of $\partial Q - \{0\}$ ( $\phi$ need not be continuous at the origin). My method for doing this is to solve Laplace's equation in the strip $S = \{0 < \mathfrak{I}(z) < 1\}$ subject to appropriate boundary conditions, then carrying the solution forward under the conformal map $f(z) = e^{\pi z/2}$ which is a conformal equivalence between $S$ and $Q$ (I'm doing it this way because in an earlier part of the problem they ask us to find $f$ ). Solving $\Delta u = 0$ on $S$ in general seems straightforward enough. Assume $u(x,y) = \alpha(x)\beta(y)$ then we get $\alpha''(x) = \lambda \alpha(x)$ and $\beta''(y) = -\lambda \beta(y)$ by simple algebra. This gives $\alpha(x) = Ae^{\sqrt \lambda x} + Be^{-\sqrt \lambda x}$ and $\beta(y) = Ce^{-\sqrt \lambda y} + De^{-i\sqrt \lambda y}$ . Then we allow $u$ in general to be any linear combination of these solutions. My issue here is that I'm not sure how to carry the boundary conditions in $Q$ to boundary conditions on $S$ . Intuitively it seems that it should just that $u$ must be constant on $\mathfrak{I}(z) = 0$ and $\mathfrak{I}(z) = 1$ , but this forces $\alpha(x) = 0$ which gives $u = 0$ , clearly not the desired result.My question is, how can I carry the boundary conditions over carefully (in general or in this case - I can probably figure out the general case if someone can work out this example)?","I am trying to find a non-constant harmonic function on , (i.e. the first quadrant) such that extends continuously to with constant values on each the components of ( need not be continuous at the origin). My method for doing this is to solve Laplace's equation in the strip subject to appropriate boundary conditions, then carrying the solution forward under the conformal map which is a conformal equivalence between and (I'm doing it this way because in an earlier part of the problem they ask us to find ). Solving on in general seems straightforward enough. Assume then we get and by simple algebra. This gives and . Then we allow in general to be any linear combination of these solutions. My issue here is that I'm not sure how to carry the boundary conditions in to boundary conditions on . Intuitively it seems that it should just that must be constant on and , but this forces which gives , clearly not the desired result.My question is, how can I carry the boundary conditions over carefully (in general or in this case - I can probably figure out the general case if someone can work out this example)?","\phi Q = \{z \mid \mathfrak{R}(z), \mathfrak{I}(z) > 0\} \phi \overline{Q} \setminus \{0\} \partial Q - \{0\} \phi S = \{0 < \mathfrak{I}(z) < 1\} f(z) = e^{\pi z/2} S Q f \Delta u = 0 S u(x,y) = \alpha(x)\beta(y) \alpha''(x) = \lambda \alpha(x) \beta''(y) = -\lambda \beta(y) \alpha(x) = Ae^{\sqrt \lambda x} + Be^{-\sqrt \lambda x} \beta(y) = Ce^{-\sqrt \lambda y} + De^{-i\sqrt \lambda y} u Q S u \mathfrak{I}(z) = 0 \mathfrak{I}(z) = 1 \alpha(x) = 0 u = 0",['complex-analysis']
53,Show that $ \sum_{z\in\mathbb{P}^{1}\left(\mathbb{C}\right)}\text{ord}_{z}\left(f\right)=0$,Show that, \sum_{z\in\mathbb{P}^{1}\left(\mathbb{C}\right)}\text{ord}_{z}\left(f\right)=0,"$ \mathbb{P}^{1}\left(\mathbb{C}\right) :=\mathbb{C}\cup{\infty} $ (There are  some different definitions, the one that I know is the stereographic projection and defining the image of the north pole under the projection as $\infty $ in $ \mathbb{C} $ . Define also $ \text{ord}_{z=a}\left(f\right) $ as the following: if $ f $ has a zero of multiplicity $m\geq1 $ at $ a $ , then $ \text{ord}_{z=a}\left(f\right)= m $ . if $ a $ is a pole of multiplicity $m\geq 1 $ for $ f $ , then $ \text{ord}_{z=a}\left(f\right) = -m $ if $ f $ defined well\has a removable singularity at $ a $ and $f(a) \neq 0 $ then $ \text{ord}_{z=a}\left(f\right) =0 $ . Prove that the following sum has finitely many nonzero terms and that $ \sum_{z\in\mathbb{P}^{1}\left(\mathbb{C}\right)}\text{ord}_{z}\left(f\right)=0 $ Where we define $ \text{ord}_{z=\infty}\left(f\right)=\text{ord}_{\omega=0}\left(g\right) $ Where $ g $ is the nonzero meromorphic function defined by $ g\left(\omega\right):=f\left(\frac{1}{\omega}\right) $ . Im have absolutely no intuition for the proof, I cant even understand why it is correct. For example what about the function $ \frac{1}{z}+\frac{1}{z-1} $ ? It has a pole of order $ 1 $ in $0 $ and in $ 1 $ so that together it summed to $-2 $ , and the order at $\infty $ is the order of $ z+\frac{z}{1-z} $ at $0 $ , which is $1$ . So it seems like the sum is not $0 $ . Any help would be appreciated.","(There are  some different definitions, the one that I know is the stereographic projection and defining the image of the north pole under the projection as in . Define also as the following: if has a zero of multiplicity at , then . if is a pole of multiplicity for , then if defined well\has a removable singularity at and then . Prove that the following sum has finitely many nonzero terms and that Where we define Where is the nonzero meromorphic function defined by . Im have absolutely no intuition for the proof, I cant even understand why it is correct. For example what about the function ? It has a pole of order in and in so that together it summed to , and the order at is the order of at , which is . So it seems like the sum is not . Any help would be appreciated.", \mathbb{P}^{1}\left(\mathbb{C}\right) :=\mathbb{C}\cup{\infty}  \infty   \mathbb{C}   \text{ord}_{z=a}\left(f\right)   f  m\geq1   a   \text{ord}_{z=a}\left(f\right)= m   a  m\geq 1   f   \text{ord}_{z=a}\left(f\right) = -m   f   a  f(a) \neq 0   \text{ord}_{z=a}\left(f\right) =0   \sum_{z\in\mathbb{P}^{1}\left(\mathbb{C}\right)}\text{ord}_{z}\left(f\right)=0   \text{ord}_{z=\infty}\left(f\right)=\text{ord}_{\omega=0}\left(g\right)   g   g\left(\omega\right):=f\left(\frac{1}{\omega}\right)   \frac{1}{z}+\frac{1}{z-1}   1  0   1  -2  \infty   z+\frac{z}{1-z}  0  1 0 ,['complex-analysis']
54,Complicated Integration involving Bessel function and exponential function,Complicated Integration involving Bessel function and exponential function,,"Sir, while studying the quantum scattering for various cases, I have got the following integrals: $$\int_{t_0=-\infty}^{t-R/c}\Big[\frac{\exp{(-i\omega t_0)}}{(t-t_0)}\Big]J_0\big(\sqrt{(t-t_0)^2-(R/c)^2}\big)\,dt_0$$ and similarly, $$\int_{t_0=-\infty}^{t-R/c}\Big[-\frac{\exp{(-i\omega t_0)}}{(t-t_0)^2}\Big]J_0\big(\sqrt{(t-t_0)^2-(R/c)^2}\big)\,dt_0$$ where, $t$ is observation time, $t_0$ is the retarded time, $J_0$ is the Bessel function of first kind and $R$ is the distance between the observation point and the point on scatterer such that $R=\sqrt{(x-x_0)^2+(y-y_0)^2+(z-z_0)^2}$ , where, $(x,y,z)$ is the point of observation. $c$ is the speed of light and taken as constant. Further, the causality condition is followed such that $t-t_0\geq R/c$ or $t_0\leq t- R/c$ . According to the physical resctriction imposed on the problem, it is also true that in the limit $t_0\rightarrow -\infty$ , the integrand should be zero. I am trying to find out the closed form answer of the integrals (if one can be solved, the other can be solved by the same technique).I searched through  the ""Table of integrals"" by Gradshteyn but somehow, I could not find out the exact match. Would you kindly suggest me any method or any relevant texts from where I can get some help.","Sir, while studying the quantum scattering for various cases, I have got the following integrals: and similarly, where, is observation time, is the retarded time, is the Bessel function of first kind and is the distance between the observation point and the point on scatterer such that , where, is the point of observation. is the speed of light and taken as constant. Further, the causality condition is followed such that or . According to the physical resctriction imposed on the problem, it is also true that in the limit , the integrand should be zero. I am trying to find out the closed form answer of the integrals (if one can be solved, the other can be solved by the same technique).I searched through  the ""Table of integrals"" by Gradshteyn but somehow, I could not find out the exact match. Would you kindly suggest me any method or any relevant texts from where I can get some help.","\int_{t_0=-\infty}^{t-R/c}\Big[\frac{\exp{(-i\omega t_0)}}{(t-t_0)}\Big]J_0\big(\sqrt{(t-t_0)^2-(R/c)^2}\big)\,dt_0 \int_{t_0=-\infty}^{t-R/c}\Big[-\frac{\exp{(-i\omega t_0)}}{(t-t_0)^2}\Big]J_0\big(\sqrt{(t-t_0)^2-(R/c)^2}\big)\,dt_0 t t_0 J_0 R R=\sqrt{(x-x_0)^2+(y-y_0)^2+(z-z_0)^2} (x,y,z) c t-t_0\geq R/c t_0\leq t- R/c t_0\rightarrow -\infty","['integration', 'complex-analysis', 'definite-integrals', 'special-functions', 'bessel-functions']"
55,Regarding radius of convergence of power series.,Regarding radius of convergence of power series.,,"This is a question from a past exam. Let $f(z) = \frac{1}{1+z^2}$ . Find the power series representation of $f$ centered at $1$ and its radius of convergence. My solution: Notice that (it was given as a hint) $$ f(z)=\frac{i}{2}\left[\frac{1}{1+i} \cdot \frac{1}{\left(1+\frac{z-1}{1+i}\right)}-\frac{1}{1-i}\cdot \frac{1}{\left(1+\frac{z-1}{1-i}\right)}\right] $$ Then I expanded the fractions using the geometric series. To find the radius of convergence in this particular case is easy. First, notice that for $|z-1| < |1+i|=\sqrt{2}$ the series converges to $f(z)$ so the radius $R$ must be greater than or equal to $\sqrt{2}$ . Next notice that the distance of $1$ from the singularities at $\pm i$ should be greater than or equal to $R$ so $R \leq \sqrt{2}$ . Putting it all together we get $R = \sqrt{2}$ . First of all, is my solution correct? If yes then I wonder how to solve such problems if we can't get both inequalities. Calculating the $\limsup$ of the coefficients can be quite difficult so what is a good way to approach them? Is it to try and write $f$ as the sum of terms of the form $$\frac{1}{1 \pm \frac{z-a}{a-z_k}}$$ where $z_k$ are the singularities of $f$ (where $a$ denotes the center)? Thanks in advance.","This is a question from a past exam. Let . Find the power series representation of centered at and its radius of convergence. My solution: Notice that (it was given as a hint) Then I expanded the fractions using the geometric series. To find the radius of convergence in this particular case is easy. First, notice that for the series converges to so the radius must be greater than or equal to . Next notice that the distance of from the singularities at should be greater than or equal to so . Putting it all together we get . First of all, is my solution correct? If yes then I wonder how to solve such problems if we can't get both inequalities. Calculating the of the coefficients can be quite difficult so what is a good way to approach them? Is it to try and write as the sum of terms of the form where are the singularities of (where denotes the center)? Thanks in advance.","f(z) = \frac{1}{1+z^2} f 1 
f(z)=\frac{i}{2}\left[\frac{1}{1+i} \cdot \frac{1}{\left(1+\frac{z-1}{1+i}\right)}-\frac{1}{1-i}\cdot \frac{1}{\left(1+\frac{z-1}{1-i}\right)}\right]
 |z-1| < |1+i|=\sqrt{2} f(z) R \sqrt{2} 1 \pm i R R \leq \sqrt{2} R = \sqrt{2} \limsup f \frac{1}{1 \pm \frac{z-a}{a-z_k}} z_k f a","['complex-analysis', 'power-series']"
56,Find all the possible laurent expansions of $f(z)=\frac{1}{\sqrt{1-z^2}}$,Find all the possible laurent expansions of,f(z)=\frac{1}{\sqrt{1-z^2}},"I'm trying to find all the possible laurent expansions of $f(z)=\frac{1}{\sqrt{1-z^2}}$ . In this case, there're two poles of the function at $\pm1$ , but I'm not pretty sure how many cases I need to consider here? Is there only one case? Also, I'm told that I may use that the nth derivative of $\frac{1}{\sqrt{1-x^2}}$ at $x=0$ is $\frac{(2n-1)!!}{2^n}$ , how can I use this identity? Thanks a lot for the help:)","I'm trying to find all the possible laurent expansions of . In this case, there're two poles of the function at , but I'm not pretty sure how many cases I need to consider here? Is there only one case? Also, I'm told that I may use that the nth derivative of at is , how can I use this identity? Thanks a lot for the help:)",f(z)=\frac{1}{\sqrt{1-z^2}} \pm1 \frac{1}{\sqrt{1-x^2}} x=0 \frac{(2n-1)!!}{2^n},"['complex-analysis', 'laurent-series']"
57,Synthetic geometry: stereographic projections of $\mathbb{C}$ on Riemann sphere $\Sigma$ are inversions in sphere $K$ centered on $\infty$ of $\Sigma$,Synthetic geometry: stereographic projections of  on Riemann sphere  are inversions in sphere  centered on  of,\mathbb{C} \Sigma K \infty \Sigma,"The complete statement is the following: Show that if $K$ is the sphere of radius $\sqrt{2}$ centered at the north pole ( $N=\infty$ ) of the Riemann sphere $\Sigma$ s.t. $K$ intersects $\Sigma$ about the unit circle $C$ of $\mathbb{C}$ : Then, for a point $a$ on $\mathbb{C}$ and its stereographic projection $\hat{a}$ on $\Sigma$ , $\hat{a}$ is the inversion in $K$ of $a$ , and $a$ is the inversion in $K$ of $\hat{a}$ . Notice that, on the image, the unit circle $C$ is represented by the line from $-1$ to $+1$ . I'm looking for a proof on this 2D graph with circles, using no algebraic property, i.e. pure synthetic geometry. Naturally this will trivially generalize to spheres. Here's what I've done up to now. First, remember the geometrical definition of circle inversion of $a$ in $K$ : Where $qI1\hat{a}$ and $qI2\hat{a}$ are right angles, and $I1\hat{a}, I2\hat{a}$ are tangents to $K$ . The theorem is very obvious if one constructs the two following machines where either $a$ or $\hat{a}$ is fixed on the line $\mathbb{C}$ : CASE 1. $a$ on the complex plane and within $C$ : CASE 2. $a$ off the complex plane: The problem is, of course, to produce the geometrical proof that these constructions do hold the property of necessarily making $\hat{a}$ the inversion in $K$ of $a$ , and $a$ the inversion in $K$ of $\hat{a}$ , no matter what point on $\mathbb{C}$ we might choose. This problem is from Needham's Visual Complex Analysis, p. 142-143; the author offers an algebraic solution while I'm looking for a geometrical one.","The complete statement is the following: Show that if is the sphere of radius centered at the north pole ( ) of the Riemann sphere s.t. intersects about the unit circle of : Then, for a point on and its stereographic projection on , is the inversion in of , and is the inversion in of . Notice that, on the image, the unit circle is represented by the line from to . I'm looking for a proof on this 2D graph with circles, using no algebraic property, i.e. pure synthetic geometry. Naturally this will trivially generalize to spheres. Here's what I've done up to now. First, remember the geometrical definition of circle inversion of in : Where and are right angles, and are tangents to . The theorem is very obvious if one constructs the two following machines where either or is fixed on the line : CASE 1. on the complex plane and within : CASE 2. off the complex plane: The problem is, of course, to produce the geometrical proof that these constructions do hold the property of necessarily making the inversion in of , and the inversion in of , no matter what point on we might choose. This problem is from Needham's Visual Complex Analysis, p. 142-143; the author offers an algebraic solution while I'm looking for a geometrical one.","K \sqrt{2} N=\infty \Sigma K \Sigma C \mathbb{C} a \mathbb{C} \hat{a} \Sigma \hat{a} K a a K \hat{a} C -1 +1 a K qI1\hat{a} qI2\hat{a} I1\hat{a}, I2\hat{a} K a \hat{a} \mathbb{C} a C a \hat{a} K a a K \hat{a} \mathbb{C}","['complex-analysis', 'geometry', 'euclidean-geometry', 'stereographic-projections']"
58,Kernel of certain differential operators,Kernel of certain differential operators,,"Let $U \subset \mathbb C^2$ be a domain and $f : U \to \mathbb C$ smooth (not necessarily holomorphic). Let $f$ further be in the kernel of the following differential operators. $$\frac{\partial^2}{\partial x_i \partial x_j} + \frac{\partial^2}{\partial y_i \partial y_j}$$ and $$\frac{\partial^2}{\partial x_i \partial y_j} - \frac{\partial^2}{\partial x_j \partial y_i}$$ for $i,j \in \{1,2\}$ . What is the solution space of $f$ ? Is it only the polynomials of degree less than 2?. Motivation: This should be just the kernel of $\partial \overline{\partial}$ sending $0$ -forms to $(1,1)$ -forms.",Let be a domain and smooth (not necessarily holomorphic). Let further be in the kernel of the following differential operators. and for . What is the solution space of ? Is it only the polynomials of degree less than 2?. Motivation: This should be just the kernel of sending -forms to -forms.,"U \subset \mathbb C^2 f : U \to \mathbb C f \frac{\partial^2}{\partial x_i \partial x_j} + \frac{\partial^2}{\partial y_i \partial y_j} \frac{\partial^2}{\partial x_i \partial y_j} - \frac{\partial^2}{\partial x_j \partial y_i} i,j \in \{1,2\} f \partial \overline{\partial} 0 (1,1)","['real-analysis', 'complex-analysis', 'ordinary-differential-equations', 'derivatives', 'differential-forms']"
59,On $\lim_{n\to\infty}b_n/a_n$ where $\exp(\sum_{n=1}^\infty a_n z^n)=1+\sum_{n=1}^\infty b_n z^n$,On  where,\lim_{n\to\infty}b_n/a_n \exp(\sum_{n=1}^\infty a_n z^n)=1+\sum_{n=1}^\infty b_n z^n,"Suppose that $(a_n)_{n>0}$ is a decreasing sequence of positive real numbers, the radius of convergence of $f(z)=\sum_{n=1}^\infty a_n z^n$ is equal to $1$ , and $f(1)=\sum_{n=1}^\infty a_n$ converges. Let $e^{f(z)}=1+\sum_{n=1}^\infty b_n z^n$ (it's easy to see that $b_n$ are positive, and the radius of convergence of this series is also equal to $1$ ). Does $\lim_{n\to\infty}b_n/a_n$ necessarily exist under these conditions? At first sight this looks too good to be true, but I can't find a counterexample, no matter what I try. The motivation is this answer , where I'm working with $f(z)=\operatorname{Li}_2(z)$ (i.e. $a_n=1/n^2$ ). The computation of $\lim_{n\to\infty}n^2 b_n$ , as seen there, would be much easier if the existence is established; without it, I (seem to) have to go a very tedious way. Update. The answer by @reuns indeed gives a counterexample (even a series of). Simplified a bit, let $a_n=4^{-k}$ for $2^k\leqslant n<2^{k+1}$ for each $k\geqslant 0$ ; then $$f(z)=\frac{w(z)}{1-z},\quad w(z)=z-3\sum_{k=1}^\infty 4^{-k}z^{2^k}$$ and then $g(z)=e^{f(z)}$ satisfies $$(1-z)^2 g'(z)=\big(w(z)+(1-z)w'(z)\big)g(z),$$ which, after substituting $g(z)=\sum_{n=0}^\infty b_n z^n$ , gives (for $n>1$ ) $$(n+1)b_{n+1}+(n-1)b_{n-1}-2nb_n=b_n+3\sum_{\substack{k>0\\2^k\leqslant n}}(2^{-k}-4^{-k})b_{n-2^k}-3\sum_{\substack{k>0\\2^k\leqslant n+1}}2^{-k}b_{n-2^k+1}.$$ Now, if $L:=\lim_{n\to\infty}(b_n/a_n)$ exists, then $L=L_d:=\lim_{m\to\infty}(b_{2^m+d}/a_{2^m+d})$ for each $d\in\mathbb{Z}$ . But the preceding equality (at $n=2^m+d$ , multiplied by $2^m$ , with $m\to\infty$ taken) implies $$L_0+4L_{-2}-8L_{-1}=-3,\quad L_1+4L_{-1}-2L_0=0,\quad\text{etc.}$$ The same way, the existence of $L_{-1}$ implies $L_d=L_{-1}$ for $d<0$ , and the values of $L_d$ for $d\geqslant 0$ can be computed just like above. Numerical experiments show that actually $L_{-1}=g(1)=e^2$ . I didn't prove the latter yet, and didn't consider generalisations ( $2^k$ replaced by $c_k$ with increasing $c_{k+1}-c_k$ , etc.) as well; I think these questions deserve to be dedicated follow-ups.","Suppose that is a decreasing sequence of positive real numbers, the radius of convergence of is equal to , and converges. Let (it's easy to see that are positive, and the radius of convergence of this series is also equal to ). Does necessarily exist under these conditions? At first sight this looks too good to be true, but I can't find a counterexample, no matter what I try. The motivation is this answer , where I'm working with (i.e. ). The computation of , as seen there, would be much easier if the existence is established; without it, I (seem to) have to go a very tedious way. Update. The answer by @reuns indeed gives a counterexample (even a series of). Simplified a bit, let for for each ; then and then satisfies which, after substituting , gives (for ) Now, if exists, then for each . But the preceding equality (at , multiplied by , with taken) implies The same way, the existence of implies for , and the values of for can be computed just like above. Numerical experiments show that actually . I didn't prove the latter yet, and didn't consider generalisations ( replaced by with increasing , etc.) as well; I think these questions deserve to be dedicated follow-ups.","(a_n)_{n>0} f(z)=\sum_{n=1}^\infty a_n z^n 1 f(1)=\sum_{n=1}^\infty a_n e^{f(z)}=1+\sum_{n=1}^\infty b_n z^n b_n 1 \lim_{n\to\infty}b_n/a_n f(z)=\operatorname{Li}_2(z) a_n=1/n^2 \lim_{n\to\infty}n^2 b_n a_n=4^{-k} 2^k\leqslant n<2^{k+1} k\geqslant 0 f(z)=\frac{w(z)}{1-z},\quad w(z)=z-3\sum_{k=1}^\infty 4^{-k}z^{2^k} g(z)=e^{f(z)} (1-z)^2 g'(z)=\big(w(z)+(1-z)w'(z)\big)g(z), g(z)=\sum_{n=0}^\infty b_n z^n n>1 (n+1)b_{n+1}+(n-1)b_{n-1}-2nb_n=b_n+3\sum_{\substack{k>0\\2^k\leqslant n}}(2^{-k}-4^{-k})b_{n-2^k}-3\sum_{\substack{k>0\\2^k\leqslant n+1}}2^{-k}b_{n-2^k+1}. L:=\lim_{n\to\infty}(b_n/a_n) L=L_d:=\lim_{m\to\infty}(b_{2^m+d}/a_{2^m+d}) d\in\mathbb{Z} n=2^m+d 2^m m\to\infty L_0+4L_{-2}-8L_{-1}=-3,\quad L_1+4L_{-1}-2L_0=0,\quad\text{etc.} L_{-1} L_d=L_{-1} d<0 L_d d\geqslant 0 L_{-1}=g(1)=e^2 2^k c_k c_{k+1}-c_k","['real-analysis', 'sequences-and-series', 'complex-analysis', 'limits', 'power-series']"
60,Does analytic imply mixed paritials exist?,Does analytic imply mixed paritials exist?,,"Let $f(z,w)$ be defined on $\Bbb{C}\times\Bbb{C}$ . Assume for each fixed $z$ , $f(z,w)$ is analytic for all $w$ . Likewise, assume for each fixed $w$ , $f(z,w)$ is analytic for all $z$ . Is it the case that the mixed partial derivatives must exist? That is, WLOG, for a fixed $w_0$ , will $$\left.\frac{\partial}{\partial w}f(z,w)\right|_{w=w_0}$$ be analytic for all $z$ ? If not, what is the counterexample?","Let be defined on . Assume for each fixed , is analytic for all . Likewise, assume for each fixed , is analytic for all . Is it the case that the mixed partial derivatives must exist? That is, WLOG, for a fixed , will be analytic for all ? If not, what is the counterexample?","f(z,w) \Bbb{C}\times\Bbb{C} z f(z,w) w w f(z,w) z w_0 \left.\frac{\partial}{\partial w}f(z,w)\right|_{w=w_0} z","['complex-analysis', 'multivariable-calculus', 'partial-derivative', 'several-complex-variables']"
61,Singularity at 0 is at most a pole of order 7 if the complex function is 1/4 integrable.,Singularity at 0 is at most a pole of order 7 if the complex function is 1/4 integrable.,,"So the real question is, given f analytic in the punctured disk $(D_1 \backslash \{ 0\})$ and $\int_{D_1} |f(z)|^{1/4} < \infty$ , characterize the smoothness of f at 0. It is easy to show that if $z=0$ is a pole, then it is at most of order 7. However, I cannot show that it cannot be an essential singularity. (Or find a counter example) Remark, if instead we had that $\int_{D_1} |f(z)|^p < \infty$ , for $p >=1$ , it is relatively easy to show that it cannot be an essential singularity, however it heavily depends on Holder inequality.","So the real question is, given f analytic in the punctured disk and , characterize the smoothness of f at 0. It is easy to show that if is a pole, then it is at most of order 7. However, I cannot show that it cannot be an essential singularity. (Or find a counter example) Remark, if instead we had that , for , it is relatively easy to show that it cannot be an essential singularity, however it heavily depends on Holder inequality.",(D_1 \backslash \{ 0\}) \int_{D_1} |f(z)|^{1/4} < \infty z=0 \int_{D_1} |f(z)|^p < \infty p >=1,"['complex-analysis', 'lp-spaces']"
62,"Why does $\operatorname{Im}\psi\left(\frac{1}{2}+i x\right)=\frac{\pi}{2}\tanh(\pi x)$, and what can be said of the real part?","Why does , and what can be said of the real part?",\operatorname{Im}\psi\left(\frac{1}{2}+i x\right)=\frac{\pi}{2}\tanh(\pi x),"Wikipedia claims that $$\operatorname{Im}\psi\left(\frac{1}{2}+i x\right)=\frac{\pi}{2}\tanh(\pi x),$$ where $\psi$ is the digamma function. How can we prove this identity? Does a similar identity hold for $\operatorname{Re}\psi\left(\frac{1}{2}+i x\right)$ ? Wikipedia claims the above identity can be derived by ""taking the logarithmic derivative of $\left|\Gamma\left(\frac{1}{2}+i x\right)\right|^2$ , which I can almost see*, but I'm not sure how to handle taking derivatives of complex absolute values. *based on the identity $\left|\Gamma\left(\frac{1}{2}+i x\right)\right|^2=\frac{\pi}{\cosh(\pi x)},$ found here .","Wikipedia claims that where is the digamma function. How can we prove this identity? Does a similar identity hold for ? Wikipedia claims the above identity can be derived by ""taking the logarithmic derivative of , which I can almost see*, but I'm not sure how to handle taking derivatives of complex absolute values. *based on the identity found here .","\operatorname{Im}\psi\left(\frac{1}{2}+i x\right)=\frac{\pi}{2}\tanh(\pi x), \psi \operatorname{Re}\psi\left(\frac{1}{2}+i x\right) \left|\Gamma\left(\frac{1}{2}+i x\right)\right|^2 \left|\Gamma\left(\frac{1}{2}+i x\right)\right|^2=\frac{\pi}{\cosh(\pi x)},","['complex-analysis', 'special-functions', 'gamma-function', 'digamma-function']"
63,"Proof that $\int_0^\infty\frac{\ln x}{x^3 - 1} \, dx = \frac{4 \pi^2}{27}$",Proof that,"\int_0^\infty\frac{\ln x}{x^3 - 1} \, dx = \frac{4 \pi^2}{27}","I realise this question was asked here , but I'm not able to work with any of the answers. The hint given by my professor is Integrate around the boundary of an indented sector of aperture $\frac{2 \pi}{3}$ but when I try that I can't figure out how to deal with the (divergent) integral along the radial line at angle $2 \pi / 3$ . My issue with the accepted answer is that it uses the residue theorem where it doesn't apply, at least as we've learned it, since $$z \mapsto \frac{\log^2z}{z^3 - 1}$$ has non-isolated singularities on the closed region bounded by the proposed contour (due to branch cuts), and I am not sure how to relate the integral along the real axis to one over a contour modified to avoid the branch cut. For a fixed $\varepsilon > 0$ , and for any $\delta \in (0, 1 - \varepsilon)$ , we could let $\log_{-\delta / 2}$ be the branch of the logarithmic function with a cut along the ray $\operatorname{arg}z = -\delta / 2$ and define a contour which goes along the positive real axis from $\varepsilon$ to $1 - \delta$ , a semicircle in the upper half plane of radius $\delta$ around $1$ , the positive real axis from $1 + \delta$ to $2$ , an arc of radius $2$ around $0$ with central angle $2 \pi - \delta$ , the ray $\operatorname{arg}z = 2 \pi - \delta$ from $r = 2$ to $r = \varepsilon$ , and finally an arc of radius $\varepsilon$ around $0$ back to $\varepsilon$ . But then, for example, I don't know how to calculate the limit of integral along the arc of radius $\varepsilon$ $$\lim_{\delta \to 0}\int_0^{2 \pi - \delta}\frac{\log_{-\delta / 2}^2(\varepsilon e^{i \theta})}{\varepsilon^3 e^{3 i \theta} - 1} \varepsilon i e^{i \theta} \, d\theta.$$ If I instead try to first use the substitution $x = e^u$ on the real integral and then compute a contour integral, I still get a divergent integral that I don't know how to handle, this time along the top of an indented rectangle.","I realise this question was asked here , but I'm not able to work with any of the answers. The hint given by my professor is Integrate around the boundary of an indented sector of aperture but when I try that I can't figure out how to deal with the (divergent) integral along the radial line at angle . My issue with the accepted answer is that it uses the residue theorem where it doesn't apply, at least as we've learned it, since has non-isolated singularities on the closed region bounded by the proposed contour (due to branch cuts), and I am not sure how to relate the integral along the real axis to one over a contour modified to avoid the branch cut. For a fixed , and for any , we could let be the branch of the logarithmic function with a cut along the ray and define a contour which goes along the positive real axis from to , a semicircle in the upper half plane of radius around , the positive real axis from to , an arc of radius around with central angle , the ray from to , and finally an arc of radius around back to . But then, for example, I don't know how to calculate the limit of integral along the arc of radius If I instead try to first use the substitution on the real integral and then compute a contour integral, I still get a divergent integral that I don't know how to handle, this time along the top of an indented rectangle.","\frac{2 \pi}{3} 2 \pi / 3 z \mapsto \frac{\log^2z}{z^3 - 1} \varepsilon > 0 \delta \in (0, 1 - \varepsilon) \log_{-\delta / 2} \operatorname{arg}z = -\delta / 2 \varepsilon 1 - \delta \delta 1 1 + \delta 2 2 0 2 \pi - \delta \operatorname{arg}z = 2 \pi - \delta r = 2 r = \varepsilon \varepsilon 0 \varepsilon \varepsilon \lim_{\delta \to 0}\int_0^{2 \pi - \delta}\frac{\log_{-\delta / 2}^2(\varepsilon e^{i \theta})}{\varepsilon^3 e^{3 i \theta} - 1} \varepsilon i e^{i \theta} \, d\theta. x = e^u","['complex-analysis', 'definite-integrals', 'improper-integrals', 'contour-integration', 'multivalued-functions']"
64,Where $Arg(z)$ is differentiable?,Where  is differentiable?,Arg(z),"Let $f(z)=Arg(z)$ denotes the principal argument of $z$ . If $f(z)=u(x,y)+iv(x,y)$ , then for $x\neq0$ , we obtain that $u(x,y)=\arctan(y/x)+\{-\pi,0,\pi\}$ and $v(x,y)=0$ , so $$ u_x(x,y)=-\frac{y}{x^2+y^2}\quad u_y(x,y)=\frac{x}{x^2+y^2} $$ and $v_x(x,y)=v_y(x,y)=0$ . Hence, the Cauchy Riemann equations does not hold unless $(x,y)=(0,0)$ . Therefore $f$ is not differentiable at every point with $x\neq0$ . Comment: we say that $f$ is differentiable at $z_0$ if the following limit exists $$ \lim_{z\to z_0}\frac{f(z)-f(z_0)}{z-z_0} $$ (a) What about differentiable in points with $x=0$ ? (b) How to prove that $f(z)=Arg(z)$ is not differentiable without using the Cauchy Riemann equations?","Let denotes the principal argument of . If , then for , we obtain that and , so and . Hence, the Cauchy Riemann equations does not hold unless . Therefore is not differentiable at every point with . Comment: we say that is differentiable at if the following limit exists (a) What about differentiable in points with ? (b) How to prove that is not differentiable without using the Cauchy Riemann equations?","f(z)=Arg(z) z f(z)=u(x,y)+iv(x,y) x\neq0 u(x,y)=\arctan(y/x)+\{-\pi,0,\pi\} v(x,y)=0 
u_x(x,y)=-\frac{y}{x^2+y^2}\quad
u_y(x,y)=\frac{x}{x^2+y^2}
 v_x(x,y)=v_y(x,y)=0 (x,y)=(0,0) f x\neq0 f z_0 
\lim_{z\to z_0}\frac{f(z)-f(z_0)}{z-z_0}
 x=0 f(z)=Arg(z)",['complex-analysis']
65,Regarding the roots of a quadratic polynomial,Regarding the roots of a quadratic polynomial,,"Let $z^2-\alpha z+\beta$ be a complex quadratic polynomial where $|\alpha|<1$ and $|\beta|<1$ . Then can we say that that if $z_1$ and $z_2$ are roots of this polynomial, then $|z_1|\leq 1$ and $|z_1|\leq 1$ ? Is there any condition on $\alpha$ , $\beta$ that I can put to get the desired claim? I think $|\alpha|+|\beta|\leq 1$ would be such condition because of this post . Could there be any other?","Let be a complex quadratic polynomial where and . Then can we say that that if and are roots of this polynomial, then and ? Is there any condition on , that I can put to get the desired claim? I think would be such condition because of this post . Could there be any other?",z^2-\alpha z+\beta |\alpha|<1 |\beta|<1 z_1 z_2 |z_1|\leq 1 |z_1|\leq 1 \alpha \beta |\alpha|+|\beta|\leq 1,"['complex-analysis', 'polynomials']"
66,Doubly periodic meromorphic function with prescribed poles and zeros,Doubly periodic meromorphic function with prescribed poles and zeros,,"The field of the meromorphic functions on a complex torus $\mathbb{C} \mathbin{/} \Lambda$ is $\mathbb{C}(\wp, \wp')$ , where $\wp$ is the weierstrass p-function to the lattice $\Lambda$ . Furthermore, for such a function $f$ and its finite set $U$ of poles and zeros holds: $\sum_{ u \in U } \operatorname{ord}_u(f) = 0$ and $\sum_{ u \in U } u \cdot \operatorname{ord}_u(f) \in \Lambda$ , where $\operatorname{ord}_u(f)$ is the order of the pole (if negative) resp. the zero (if positive) of $f$ at $u$ . If now some points $U$ and their orders are given, and fulfill constraints above, I believe (because of the Riemann–Roch theorem) that a corresponding meromorphic function exists and is unique (up to a multiplicative constant), but I cannot figure out how to construct it from $\wp$ and $\wp'$ . Are my claims correct? And if yes, how to construct the meromorphic function in question (with closed-form formula, or recursively)?","The field of the meromorphic functions on a complex torus is , where is the weierstrass p-function to the lattice . Furthermore, for such a function and its finite set of poles and zeros holds: and , where is the order of the pole (if negative) resp. the zero (if positive) of at . If now some points and their orders are given, and fulfill constraints above, I believe (because of the Riemann–Roch theorem) that a corresponding meromorphic function exists and is unique (up to a multiplicative constant), but I cannot figure out how to construct it from and . Are my claims correct? And if yes, how to construct the meromorphic function in question (with closed-form formula, or recursively)?","\mathbb{C} \mathbin{/} \Lambda \mathbb{C}(\wp, \wp') \wp \Lambda f U \sum_{ u \in U } \operatorname{ord}_u(f) = 0 \sum_{ u \in U } u \cdot \operatorname{ord}_u(f) \in \Lambda \operatorname{ord}_u(f) f u U \wp \wp'","['complex-analysis', 'elliptic-curves', 'elliptic-functions', 'divisors-algebraic-geometry', 'meromorphic-functions']"
67,Removable singularity and Liouville's Theorem,Removable singularity and Liouville's Theorem,,"Please verify if I got either correct. 1a. Let $f$ be entire function such that $\sup_{\mathbb C} \left |\frac{f(z)}{z} \right | < \infty$ . Show $z = 0$ is a removable singularity of $g(z) = \frac{f(z)}{z}$ . 1b. Suppose $f$ and $g$ are entire functions such that $|f| \leq K|g|$ , show that $f = cg$ for all $z \in \mathbb{C}$ . I wrote that 1a. Since $\sup |f(z)/z| < \infty$ then $\left |\frac{f(z)}{z} \right | < M$ . Hence $\lim_{z \to 0}|zg(z)| = \lim_{z \to 0}|f(z)| \leq \lim_{z \to 0} M|z| =0.$ So passing the limit to both sides yields the result. 1b. I think this is just applying Liouville Theorem to $(f/g)$ never mind I found the answer. My answer is incomplete for 1b. Only need verification for 1a. Thanks for reading.","Please verify if I got either correct. 1a. Let be entire function such that . Show is a removable singularity of . 1b. Suppose and are entire functions such that , show that for all . I wrote that 1a. Since then . Hence So passing the limit to both sides yields the result. 1b. I think this is just applying Liouville Theorem to never mind I found the answer. My answer is incomplete for 1b. Only need verification for 1a. Thanks for reading.",f \sup_{\mathbb C} \left |\frac{f(z)}{z} \right | < \infty z = 0 g(z) = \frac{f(z)}{z} f g |f| \leq K|g| f = cg z \in \mathbb{C} \sup |f(z)/z| < \infty \left |\frac{f(z)}{z} \right | < M \lim_{z \to 0}|zg(z)| = \lim_{z \to 0}|f(z)| \leq \lim_{z \to 0} M|z| =0. (f/g),"['complex-analysis', 'solution-verification']"
68,When does equality hold in $\Bigr\lvert\sum_{k=1}^n a_kb_k\Bigr\rvert^2 \le \left(\sum_{k=1}^n |a_k|^2\right)\left(\sum_{k=1}^n |b_k|^2\right)$?,When does equality hold in ?,\Bigr\lvert\sum_{k=1}^n a_kb_k\Bigr\rvert^2 \le \left(\sum_{k=1}^n |a_k|^2\right)\left(\sum_{k=1}^n |b_k|^2\right),"I'm reading Ahlfors' complex analysis. During the proof of Cauchy's inequality, the author uses the following equation: $$ \sum_{k=1}^n \bigr\lvert a_k - \lambda \overline{b_k}\bigr\rvert^2 = \sum_{k=1}^n |a_k|^2 + |\lambda|^2\sum_{k=1}^n |b_k|^2 - 2  \Re\left(\overline{\lambda}\sum_{k=1}^n a_kb_k\right) \tag{1} $$ where the $a_k$ 's, $b_k$ 's and $\lambda=\frac{\sum_{j=1}^n a_jb_j}{\sum_{j=1}^n |b_j|^2}$ are all some complex numbers. After the proof is concluded, the author states the following: From $(1)$ we conclude further that the sign of equality holds in $\Bigr\lvert\sum_{k=1}^n a_kb_k\Bigr\rvert^2 \le \left(\sum_{k=1}^n |a_k|^2\right)\left(\sum_{k=1}^n |b_k|^2\right)$ if and only if the $a_k$ are proportional to the $\overline{b_k}$ . I understand that the idea here is that, under the hypothesis that $a_k$ and $\overline{b_k}$ are linearly dependent, then in equation $(1)$ both sides should be equal to $0$ (instead of greater or equal to $0$ as in the normal proof). However, I don't see how this is the case. If I take $a_k = \gamma_k \overline{b_k}$ for some scalars $\gamma_k$ 's then $(1)$ reduces to \begin{align} \sum_{k=1}^n \bigr\lvert \gamma_k \overline{b_k} - \lambda \overline{b_k}\bigr\rvert &= \sum_{k=1}^n |\gamma_k \overline{b_k}|^2 + |\lambda|^2\sum_{k=1}^n |b_k|^2 - 2  \Re\left(\overline{\lambda}\sum_{k=1}^n \gamma_k \overline{b_k}b_k\right) \notag \\ &= \sum_{k=1}^n |\gamma_k|^2 |b_k|^2 + |\lambda|^2\sum_{k=1}^n |b_k|^2 - 2  \Re\left(\overline{\lambda}\sum_{k=1}^n \gamma_k |b_k|^2\right) \notag  \end{align} but from here, I don't see how I could simplify the result further. Could somebody explain to me what the author meant and why it is that equality holds in this case? Thank you!","I'm reading Ahlfors' complex analysis. During the proof of Cauchy's inequality, the author uses the following equation: where the 's, 's and are all some complex numbers. After the proof is concluded, the author states the following: From we conclude further that the sign of equality holds in if and only if the are proportional to the . I understand that the idea here is that, under the hypothesis that and are linearly dependent, then in equation both sides should be equal to (instead of greater or equal to as in the normal proof). However, I don't see how this is the case. If I take for some scalars 's then reduces to but from here, I don't see how I could simplify the result further. Could somebody explain to me what the author meant and why it is that equality holds in this case? Thank you!","
\sum_{k=1}^n \bigr\lvert a_k - \lambda \overline{b_k}\bigr\rvert^2 = \sum_{k=1}^n |a_k|^2 + |\lambda|^2\sum_{k=1}^n |b_k|^2 - 2  \Re\left(\overline{\lambda}\sum_{k=1}^n a_kb_k\right) \tag{1}
 a_k b_k \lambda=\frac{\sum_{j=1}^n a_jb_j}{\sum_{j=1}^n |b_j|^2} (1) \Bigr\lvert\sum_{k=1}^n a_kb_k\Bigr\rvert^2 \le \left(\sum_{k=1}^n |a_k|^2\right)\left(\sum_{k=1}^n |b_k|^2\right) a_k \overline{b_k} a_k \overline{b_k} (1) 0 0 a_k = \gamma_k \overline{b_k} \gamma_k (1) \begin{align}
\sum_{k=1}^n \bigr\lvert \gamma_k \overline{b_k} - \lambda \overline{b_k}\bigr\rvert &= \sum_{k=1}^n |\gamma_k \overline{b_k}|^2 + |\lambda|^2\sum_{k=1}^n |b_k|^2 - 2  \Re\left(\overline{\lambda}\sum_{k=1}^n \gamma_k \overline{b_k}b_k\right) \notag \\
&= \sum_{k=1}^n |\gamma_k|^2 |b_k|^2 + |\lambda|^2\sum_{k=1}^n |b_k|^2 - 2  \Re\left(\overline{\lambda}\sum_{k=1}^n \gamma_k |b_k|^2\right) \notag 
\end{align}","['complex-analysis', 'complex-numbers', 'proof-explanation', 'cauchy-schwarz-inequality']"
69,Power Series and Analyticity of a complex function,Power Series and Analyticity of a complex function,,"I was studying about the connection of analytic function and their power series representation. Finally, I came to an understanding that, if I am given with an function, analytic at some point 'a', then I will be able to write a power series representation of that function, where that power series representation is convergent in some circle centered around that 'a'. Now, what about the behavior points outside this circle of convergence? Can the function remain analytic at those points? In short, is it true if a function having a power series representation about a point is not convergent at a point outside the radius of convergence, then we cannot say about the analyticity of that function at that point. Is my understanding correct? Or Am I still missing the essence of the power series expansion?","I was studying about the connection of analytic function and their power series representation. Finally, I came to an understanding that, if I am given with an function, analytic at some point 'a', then I will be able to write a power series representation of that function, where that power series representation is convergent in some circle centered around that 'a'. Now, what about the behavior points outside this circle of convergence? Can the function remain analytic at those points? In short, is it true if a function having a power series representation about a point is not convergent at a point outside the radius of convergence, then we cannot say about the analyticity of that function at that point. Is my understanding correct? Or Am I still missing the essence of the power series expansion?",,"['complex-analysis', 'power-series', 'analytic-functions']"
70,Lower bound on the roots of polynomial,Lower bound on the roots of polynomial,,"To solve a problem almost identical to the one in this question I would like to locate the zeros of the function $$ Q(z) = \sum_{k=0}^{m-1}(m-k)z^k = \frac{m - (m+1)z + z^{m+1}}{{(z-1)}^2} $$ where $m$ is a positive integer. In particular, I want to show that $Q(z)=0 \implies |z|>R$ for some $R \geq 1$ . Based on computer solutions it seems that the statement holds at least for $R=\frac{m+1}{m}$ , perhaps even $R=\frac{m+2}{m}$ , so I have mostly worked with discs of such radii while trying to prove the statement. I have tried using Rouché's theorem, but so far it has only given me bounds with $R<1$ . Inspired by plots of the roots I considered the equation $Q(z-c)=0$ shifting the roots by some real number $c$ but it didn't seem to help. A possibly related observation that I have not proven is that the roots of $Q(z)$ sum up to $-2$ .","To solve a problem almost identical to the one in this question I would like to locate the zeros of the function where is a positive integer. In particular, I want to show that for some . Based on computer solutions it seems that the statement holds at least for , perhaps even , so I have mostly worked with discs of such radii while trying to prove the statement. I have tried using Rouché's theorem, but so far it has only given me bounds with . Inspired by plots of the roots I considered the equation shifting the roots by some real number but it didn't seem to help. A possibly related observation that I have not proven is that the roots of sum up to .","
Q(z) = \sum_{k=0}^{m-1}(m-k)z^k = \frac{m - (m+1)z + z^{m+1}}{{(z-1)}^2}
 m Q(z)=0 \implies |z|>R R \geq 1 R=\frac{m+1}{m} R=\frac{m+2}{m} R<1 Q(z-c)=0 c Q(z) -2","['complex-analysis', 'polynomials', 'roots']"
71,Evaluation of Singular Integrals by Complex Contour Integration,Evaluation of Singular Integrals by Complex Contour Integration,,"I am learning to compute singular integrals using the ' $i\epsilon$ -prescription' of complex contour integration from the book Mathematical Physics by V. Balakrishnan (Chapter 23, Article 23.3.4). Following the book, the integral I want to compute is, $$f(x_0)=\displaystyle{\int_a^b dx\frac{\phi (x)}{x-x_0}}\tag{1}$$ where $a < x_0 < b$ are real numbers, $\phi(x)$ is a sufficiently smooth function, and $\phi(x_0)\neq 0$ . The integrand obviously diverges at $x = x_0$ because of the factor $(x − x_0)$ in the denominator. As it stands, the Riemann integral in Eq.(1) does not exist, because of this nonintegrable singularity of the integrand. Suppose, however, we move the singularity away from the path of integration by giving it either a positive   imaginary part $+i\epsilon$ (where $\epsilon > 0$ ), or a negative imaginary part $−i\epsilon$ . This step, called an $i\epsilon$ -prescription, makes the original integral well-defined. The integrals in the two cases are given, respectively, by $$f(x_0 ± i\epsilon) = \int_a^b dx \frac{\phi(x)}{x−(x_0±i\epsilon)} = \int_a^b dx\frac{\phi(x)}{x−x_0∓i\epsilon}\tag{2}$$ The question is: what happens as $\epsilon \to 0$ ? We can continue to keep the integral well-defined by distorting the path of integration away from the approaching singularity, to form a small semicircle of radius $\epsilon$ . This semicircle lies in the lower half-plane in the case of $\boldsymbol{f(x_0+i\epsilon)}$ , and in the upper half-plane in the case of $\boldsymbol{f(x_0 − i\epsilon)}$ . This is shown in figures (a) and (b) : On the small semicircles, the variable of integration is $z = x_0 + \epsilon e^{i\theta}$ , so that $dz =\epsilon e^{i\theta}id\theta$ . The argument $\theta$ runs from $\pi$ to $2\pi$ in the case of $f (x + i\epsilon)$ , and from $\pi$ to $0$ in the case of $f (x − i\epsilon)$ . Taking the limit $\epsilon \to 0$ then yields the Cauchy principal value integral from $a$ to $b$ , plus the contribution from the semicircle: $$\lim_{\epsilon \to 0}\int_a^b dx\frac{\phi(x)}{x−x_0∓i\epsilon}= P\int_a^b dx\frac{\phi(x)}{x-x_0}± i\pi\phi(x_0)\tag{3}$$ I have two questions : Firstly, I do not understand the line in bold. Why the semicircle lies in the lower half-plane in the case of $f(x_0+i\epsilon)$ , and in the upper half-plane in the case of $f(x_0 − i\epsilon)$ ? Secondly, when I am computing the contribution from the semi-circle, I do not get the value $i\pi\phi(x_0)$ . This is how I am doing it for $f(x_0+i\epsilon)$ : $$\begin{align} \lim_{\epsilon \to 0}\int_{-\epsilon}^{+\epsilon}dz\frac{\phi(z)}{z-x_0-i\epsilon}&=\lim_{\epsilon \to 0}\int_{\pi}^{2\pi} d\theta \,\,\epsilon e^{i\theta}i\frac{\phi(x_0+\epsilon e^{i\theta})}{\epsilon (e^{i\theta}-i)}=i\lim_{\epsilon \to 0}\int_{\pi}^{2\pi} d\theta \,\, e^{i\theta}\frac{\phi(x_0+\epsilon e^{i\theta})}{(e^{i\theta}-i)}\\ &=i\lim_{\epsilon \to 0}\left(\left[\phi(x_0+\epsilon e^{i\theta})\int d\theta\frac{e^{i\theta}}{(e^{i\theta}-i)}\right]_{\pi}^{2\pi}-\epsilon\int_{\pi}^{2\pi} d\theta \,\left\{\frac{d\phi(x_0+\epsilon e^{i\theta})}{d\theta}\int d\theta \frac{e^{i\theta}}{(e^{i\theta}-i)}\right\}\right) \end{align} $$ The second term vanishes by the application of the limit. Then, $$\begin{align}\lim_{\epsilon \to 0}\int_{-\epsilon}^{+\epsilon}dz\frac{\phi(z)}{z-x_0-i\epsilon}&=\lim_{\epsilon \to 0}\left(\left[\phi(x_0+\epsilon e^{i\theta})\ln{(e^{i\theta}-i)}\right]_{\pi}^{2\pi}\right)\\ &=\lim_{\epsilon \to 0}\left(\phi(x_0+\epsilon e^{i\theta})\ln{\left(\frac{1-i}{-1-i}\right)}\right)\\ &=\phi(x_0)\ln{\left(\frac{i-1}{i+1}\right)}\\ &=\phi(x_0)\ln{\left(\frac{{(i-1)}^2}{-2}\right)}\\ &=\phi(x_0)\ln{i}\\ &=\phi(x_0)\ln{e^{i\pi/2}}\\ &=i\frac{\pi}{2}\phi(x_0) \end{align}$$ So, a factor of 2 is coming in the denominator which should not be there. Where am I getting wrong? Please help.","I am learning to compute singular integrals using the ' -prescription' of complex contour integration from the book Mathematical Physics by V. Balakrishnan (Chapter 23, Article 23.3.4). Following the book, the integral I want to compute is, where are real numbers, is a sufficiently smooth function, and . The integrand obviously diverges at because of the factor in the denominator. As it stands, the Riemann integral in Eq.(1) does not exist, because of this nonintegrable singularity of the integrand. Suppose, however, we move the singularity away from the path of integration by giving it either a positive   imaginary part (where ), or a negative imaginary part . This step, called an -prescription, makes the original integral well-defined. The integrals in the two cases are given, respectively, by The question is: what happens as ? We can continue to keep the integral well-defined by distorting the path of integration away from the approaching singularity, to form a small semicircle of radius . This semicircle lies in the lower half-plane in the case of , and in the upper half-plane in the case of . This is shown in figures (a) and (b) : On the small semicircles, the variable of integration is , so that . The argument runs from to in the case of , and from to in the case of . Taking the limit then yields the Cauchy principal value integral from to , plus the contribution from the semicircle: I have two questions : Firstly, I do not understand the line in bold. Why the semicircle lies in the lower half-plane in the case of , and in the upper half-plane in the case of ? Secondly, when I am computing the contribution from the semi-circle, I do not get the value . This is how I am doing it for : The second term vanishes by the application of the limit. Then, So, a factor of 2 is coming in the denominator which should not be there. Where am I getting wrong? Please help.","i\epsilon f(x_0)=\displaystyle{\int_a^b dx\frac{\phi (x)}{x-x_0}}\tag{1} a < x_0 < b \phi(x) \phi(x_0)\neq 0 x = x_0 (x − x_0) +i\epsilon \epsilon > 0 −i\epsilon i\epsilon f(x_0 ± i\epsilon) = \int_a^b dx \frac{\phi(x)}{x−(x_0±i\epsilon)} = \int_a^b dx\frac{\phi(x)}{x−x_0∓i\epsilon}\tag{2} \epsilon \to 0 \epsilon \boldsymbol{f(x_0+i\epsilon)} \boldsymbol{f(x_0 − i\epsilon)} z = x_0 + \epsilon e^{i\theta} dz =\epsilon e^{i\theta}id\theta \theta \pi 2\pi f (x + i\epsilon) \pi 0 f (x − i\epsilon) \epsilon \to 0 a b \lim_{\epsilon \to 0}\int_a^b dx\frac{\phi(x)}{x−x_0∓i\epsilon}= P\int_a^b dx\frac{\phi(x)}{x-x_0}± i\pi\phi(x_0)\tag{3} f(x_0+i\epsilon) f(x_0 − i\epsilon) i\pi\phi(x_0) f(x_0+i\epsilon) \begin{align}
\lim_{\epsilon \to 0}\int_{-\epsilon}^{+\epsilon}dz\frac{\phi(z)}{z-x_0-i\epsilon}&=\lim_{\epsilon \to 0}\int_{\pi}^{2\pi} d\theta \,\,\epsilon e^{i\theta}i\frac{\phi(x_0+\epsilon e^{i\theta})}{\epsilon (e^{i\theta}-i)}=i\lim_{\epsilon \to 0}\int_{\pi}^{2\pi} d\theta \,\, e^{i\theta}\frac{\phi(x_0+\epsilon e^{i\theta})}{(e^{i\theta}-i)}\\
&=i\lim_{\epsilon \to 0}\left(\left[\phi(x_0+\epsilon e^{i\theta})\int d\theta\frac{e^{i\theta}}{(e^{i\theta}-i)}\right]_{\pi}^{2\pi}-\epsilon\int_{\pi}^{2\pi} d\theta \,\left\{\frac{d\phi(x_0+\epsilon e^{i\theta})}{d\theta}\int d\theta \frac{e^{i\theta}}{(e^{i\theta}-i)}\right\}\right)
\end{align}  \begin{align}\lim_{\epsilon \to 0}\int_{-\epsilon}^{+\epsilon}dz\frac{\phi(z)}{z-x_0-i\epsilon}&=\lim_{\epsilon \to 0}\left(\left[\phi(x_0+\epsilon e^{i\theta})\ln{(e^{i\theta}-i)}\right]_{\pi}^{2\pi}\right)\\
&=\lim_{\epsilon \to 0}\left(\phi(x_0+\epsilon e^{i\theta})\ln{\left(\frac{1-i}{-1-i}\right)}\right)\\
&=\phi(x_0)\ln{\left(\frac{i-1}{i+1}\right)}\\
&=\phi(x_0)\ln{\left(\frac{{(i-1)}^2}{-2}\right)}\\
&=\phi(x_0)\ln{i}\\
&=\phi(x_0)\ln{e^{i\pi/2}}\\
&=i\frac{\pi}{2}\phi(x_0)
\end{align}","['complex-analysis', 'contour-integration', 'singular-integrals']"
72,Möbius transformation that maps the unit circle to itself,Möbius transformation that maps the unit circle to itself,,"I need to find necessary and sufficient conditions on the coefficients of a Möbius transform $T(z)=\frac{\tilde a z+\tilde b}{\tilde c z+ \tilde d}$ so that it maps the unit circle $\{z: |z|=1\}$ into itself. I initially thought that, since any Möbius transformation can be written as a finite composition of simple transformations (translations ( $z+a$ ), rotations ( $e^{i\theta}z$ ), dilations ( $az$ ) and inversions ( $\frac1z$ )) and since we do not want to dilate or move the unit circle, we then can write the required transformation as $T(z)=e^{i\alpha}z$ or $T(z)=\frac{e^{i\alpha}}{z}$ for some $\alpha\in (-\pi,\pi]$ . However, this does not look like the result I'm supposed to get. Where is my mistake? I then read the exercise hint, which says I should first write a transformation $R$ that maps the unit circle to $\mathbb{R}_\infty$ , and use transformations $S(z)$ that map $\mathbb{R}_\infty$ to $\mathbb{R}_\infty$ , which I believe are of the form $S(z)=\frac{az+b}{cz+d}$ where $a,b,c,d\in\mathbb{R}$ . I then chose $R(z)=\frac{z+1}{z-1}i$ and tried composing $T=R^{-1}\circ S\circ R$ to find the answer. However, I'm getting an ugly expression that does not seem to be correct either: $T(z)=\frac{(A+Bi)z-\overline{(A-Bi)}}{(A-Bi)z-\overline{(A+Bi)}}$ , where $A=b+ai$ and $B=d+ci$ . Could you help me see how to use the hint? I know there are other solutions for this problem on this site, but they solve it in different ways. Thank you!","I need to find necessary and sufficient conditions on the coefficients of a Möbius transform so that it maps the unit circle into itself. I initially thought that, since any Möbius transformation can be written as a finite composition of simple transformations (translations ( ), rotations ( ), dilations ( ) and inversions ( )) and since we do not want to dilate or move the unit circle, we then can write the required transformation as or for some . However, this does not look like the result I'm supposed to get. Where is my mistake? I then read the exercise hint, which says I should first write a transformation that maps the unit circle to , and use transformations that map to , which I believe are of the form where . I then chose and tried composing to find the answer. However, I'm getting an ugly expression that does not seem to be correct either: , where and . Could you help me see how to use the hint? I know there are other solutions for this problem on this site, but they solve it in different ways. Thank you!","T(z)=\frac{\tilde a z+\tilde b}{\tilde c z+ \tilde d} \{z: |z|=1\} z+a e^{i\theta}z az \frac1z T(z)=e^{i\alpha}z T(z)=\frac{e^{i\alpha}}{z} \alpha\in (-\pi,\pi] R \mathbb{R}_\infty S(z) \mathbb{R}_\infty \mathbb{R}_\infty S(z)=\frac{az+b}{cz+d} a,b,c,d\in\mathbb{R} R(z)=\frac{z+1}{z-1}i T=R^{-1}\circ S\circ R T(z)=\frac{(A+Bi)z-\overline{(A-Bi)}}{(A-Bi)z-\overline{(A+Bi)}} A=b+ai B=d+ci","['complex-analysis', 'mobius-transformation']"
73,How do you compute this integral?,How do you compute this integral?,,I obtain this using mathematica: $$\int_{-\pi}^\pi(2+2\cos t)^a\cos(b t)dt=\frac{2\pi\Gamma(1+2a)}{\Gamma(1+a+b)\Gamma(1+a-b)}.$$ This should hold for $\Re(a)>-1/2$ .,I obtain this using mathematica: This should hold for .,\int_{-\pi}^\pi(2+2\cos t)^a\cos(b t)dt=\frac{2\pi\Gamma(1+2a)}{\Gamma(1+a+b)\Gamma(1+a-b)}. \Re(a)>-1/2,"['integration', 'complex-analysis', 'complex-integration']"
74,Understanding notation for holomorphic quadratic differentials,Understanding notation for holomorphic quadratic differentials,,"I've found the notation in books to be a bit confusing (perhaps my background is inadequate). For example, in Farb and Margalit's book A Primer on Mapping Class Groups , they give the following definition: Definition ""Let $\{ z_\alpha: U_\alpha \to \mathbb{C} \}$ be an atlas for $X$ [Here $X$ is a Riemann surface]. A holomorphic quadratic differential $q$ on $X$ is specified by a collection of expressions $\{ \phi_\alpha (z_\alpha) dz_\alpha^2 \}$ with the following properties: 1. Each $\phi_\alpha : z_\alpha(U_\alpha) \to \mathbb{C}$ is a holomorphic function with a finite number of zeros. 2. For any two coordinate charts $z_\alpha$ and $z_\beta$ , we have $$ \phi_\beta(z_\beta)( \frac{dz_\beta}{dz_\alpha})^2 = \phi_\alpha(z_\alpha) $$ "" Here $\frac{dz_\beta}{dz_\alpha}$ is the derivative of the change of coordinates $z_\beta \circ z_\alpha^{-1}$ . So, my problem comes when I try to check that this definition is independent of the coordinate system that you choose. Say that you have $z_\alpha: U_\alpha \to \mathbb{C}, z_\beta: U_\beta \to \mathbb{C}$ , $w \in U_\alpha \cap U_{\beta}$ and a vector $v \in T_wX$ . Then, I would like to use property number 2 to check that $q(v)$ is well defined, i.e. that $$ \phi_\alpha \circ z_\alpha(w)  dz_\alpha^2(v) = \phi_\beta \circ z_\beta(w)  dz_\beta^2(v) $$ I'm confused about what exactly $dz_\alpha(v)$ and $dz_\beta(v)$ are. Is it the case that $dz_\alpha = z_\alpha^*(dz)$ ? I thought that could be it, but then I get $dz_\alpha(v) = dz(d_w z_\alpha(v)) $ , and I don't really get anywhere. I would like to do this in a rigorous way, not just formally manipulating $dz_\alpha$ and $dz_\beta$ (""multiplying"" by $dz_\alpha^2$ in property number 2 would give me what I want, but I would like to understand what I'm doing).","I've found the notation in books to be a bit confusing (perhaps my background is inadequate). For example, in Farb and Margalit's book A Primer on Mapping Class Groups , they give the following definition: Definition ""Let be an atlas for [Here is a Riemann surface]. A holomorphic quadratic differential on is specified by a collection of expressions with the following properties: 1. Each is a holomorphic function with a finite number of zeros. 2. For any two coordinate charts and , we have "" Here is the derivative of the change of coordinates . So, my problem comes when I try to check that this definition is independent of the coordinate system that you choose. Say that you have , and a vector . Then, I would like to use property number 2 to check that is well defined, i.e. that I'm confused about what exactly and are. Is it the case that ? I thought that could be it, but then I get , and I don't really get anywhere. I would like to do this in a rigorous way, not just formally manipulating and (""multiplying"" by in property number 2 would give me what I want, but I would like to understand what I'm doing).","\{ z_\alpha: U_\alpha \to \mathbb{C} \} X X q X \{ \phi_\alpha (z_\alpha) dz_\alpha^2 \} \phi_\alpha : z_\alpha(U_\alpha) \to \mathbb{C} z_\alpha z_\beta  \phi_\beta(z_\beta)( \frac{dz_\beta}{dz_\alpha})^2 = \phi_\alpha(z_\alpha)  \frac{dz_\beta}{dz_\alpha} z_\beta \circ z_\alpha^{-1} z_\alpha: U_\alpha \to \mathbb{C}, z_\beta: U_\beta \to \mathbb{C} w \in U_\alpha \cap U_{\beta} v \in T_wX q(v)  \phi_\alpha \circ z_\alpha(w)  dz_\alpha^2(v) = \phi_\beta \circ z_\beta(w)  dz_\beta^2(v)  dz_\alpha(v) dz_\beta(v) dz_\alpha = z_\alpha^*(dz) dz_\alpha(v) = dz(d_w z_\alpha(v))  dz_\alpha dz_\beta dz_\alpha^2","['complex-analysis', 'differential-forms', 'riemann-surfaces']"
75,Riemann surfaces exercise,Riemann surfaces exercise,,"I am trying to do an exercise from Rick Miranda's Book Let $U$ be the affine plane curve defined by $x^2=3+10t^4+3t^8$ and $V$ defined by $w^2=z^6-1$ , show that the function $F :U \rightarrow V$ , $$F(x,t)=\left(\frac{1+t^2}{1-t^2},\frac{2tx}{(1-t^2)^3}\right)$$ is holomorphic and nowhere ramified when $t \neq  \pm 1$ . Well to prove that this is holomorphic i simply separated in $4$ cases for the coordinate charts that we could have and checked that the local representation of these functions were holomorphic. To prove that part about the unramified points and i was trying to check the derivative of these representations and see where they would be zero, hopefully nowhere in the surface would be the result we wanted, however using this and the fact that we know the derivative of the implicit function i was not able to prove this, so my question is if this is the right way to think abou this problem, or should i be taking another approach, should i try to explicit right with are the implicit functis envolved in this problem? Thanks in advance. New edit: I was able to prove this was true for two of  the representations , now im trying to prove it for the other two, where in one we suppose that $t$ is given by $h(x)$ and $z$ is given by $k(w)$ , and the other one where $x$ is given by $j(t)$ and $z$ is given by $k(w)$ .  Calculating the maps and their derivatives i cant seem to get a contradiction for why the derivative cannot be zero. At this point what i cant seem to do is this So suppose now the charts is $t=h(x)$ , so $\frac{df}{dt} \neq 0$ and $z$ depends on $w$ by some holomorphic function k,so $\frac{dk}{dw}\neq 0$ , we have  in local coordinates $\psi_2 \circ f \circ \phi_1(x) = \frac{2h(x)x}{(1-h(x)^2)^3}$ with derivative $\frac{2(5xh(x)^2h'(x)+xh'(x)-h(x)^3+h(x))}{(1-h(x)^2)^4}$ and i cant seem to find a contradition for why this derivative cannot be zero using the fact that those derivatives cannot be zero, so any tips on something i may be forgetting would be helpfull.","I am trying to do an exercise from Rick Miranda's Book Let be the affine plane curve defined by and defined by , show that the function , is holomorphic and nowhere ramified when . Well to prove that this is holomorphic i simply separated in cases for the coordinate charts that we could have and checked that the local representation of these functions were holomorphic. To prove that part about the unramified points and i was trying to check the derivative of these representations and see where they would be zero, hopefully nowhere in the surface would be the result we wanted, however using this and the fact that we know the derivative of the implicit function i was not able to prove this, so my question is if this is the right way to think abou this problem, or should i be taking another approach, should i try to explicit right with are the implicit functis envolved in this problem? Thanks in advance. New edit: I was able to prove this was true for two of  the representations , now im trying to prove it for the other two, where in one we suppose that is given by and is given by , and the other one where is given by and is given by .  Calculating the maps and their derivatives i cant seem to get a contradiction for why the derivative cannot be zero. At this point what i cant seem to do is this So suppose now the charts is , so and depends on by some holomorphic function k,so , we have  in local coordinates with derivative and i cant seem to find a contradition for why this derivative cannot be zero using the fact that those derivatives cannot be zero, so any tips on something i may be forgetting would be helpfull.","U x^2=3+10t^4+3t^8 V w^2=z^6-1 F :U \rightarrow V F(x,t)=\left(\frac{1+t^2}{1-t^2},\frac{2tx}{(1-t^2)^3}\right) t \neq  \pm 1 4 t h(x) z k(w) x j(t) z k(w) t=h(x) \frac{df}{dt} \neq 0 z w \frac{dk}{dw}\neq 0 \psi_2 \circ f \circ \phi_1(x) = \frac{2h(x)x}{(1-h(x)^2)^3} \frac{2(5xh(x)^2h'(x)+xh'(x)-h(x)^3+h(x))}{(1-h(x)^2)^4}","['complex-analysis', 'riemann-surfaces']"
76,Doing a standard integral with complex numbers instead of using a trigonometric substitution,Doing a standard integral with complex numbers instead of using a trigonometric substitution,,"I was looking at some integrals to do with trigonometric substitutions and I stumbled across this one $$\int\frac{1}{\sqrt{x^2-1}}dx$$ I know you can do it with a regular trigonometric substitution or just use a hyperbolic substitution but I was wondering if you can do it the following way. $$ \int \frac{1}{\sqrt{x^2-1}} dx  = \int \frac{\cos \theta}{\sqrt{-\cos^2\theta}}d\theta  = \int \frac{1}{i}d\theta  = \frac{1}{i}\arcsin x, $$ where the $x=\sin \theta$ substitution was used. Could anybody please explain to me why I don't get the same result as one would get if a hyperbolic or other trigonometric substitution was used? Thanks in advance.",I was looking at some integrals to do with trigonometric substitutions and I stumbled across this one I know you can do it with a regular trigonometric substitution or just use a hyperbolic substitution but I was wondering if you can do it the following way. where the substitution was used. Could anybody please explain to me why I don't get the same result as one would get if a hyperbolic or other trigonometric substitution was used? Thanks in advance.,"\int\frac{1}{\sqrt{x^2-1}}dx 
\int \frac{1}{\sqrt{x^2-1}} dx
 = \int \frac{\cos \theta}{\sqrt{-\cos^2\theta}}d\theta
 = \int \frac{1}{i}d\theta
 = \frac{1}{i}\arcsin x,
 x=\sin \theta","['calculus', 'integration', 'complex-analysis', 'complex-numbers', 'indefinite-integrals']"
77,Finding the leading order contribution to a certain integral.,Finding the leading order contribution to a certain integral.,,"I am trying to compute the leading order term of the following expression in the small $\epsilon$ limit; $$ I = \frac{d}{ds}\biggr|_{s\rightarrow 0}\frac{1}{\Gamma(s)} \int_{0}^{\infty} dt  \frac{t^{s-1}e^{itx}}{(1-e^{i\epsilon_{1}t}) (1-e^{i\epsilon_{2} t})} $$ First of all I tried expanding the exponentials with $\epsilon$ 's in them, leading to $$ -\frac{1}{\epsilon_{1}\epsilon_{2}} \frac{d}{ds}\biggr|_{s\rightarrow 0}\frac{1}{\Gamma(s)} \int_{0}^{\infty}  t^{s-3}e^{itx} dt $$ I'm not sure how valid this is given that $\epsilon t=\mathcal{O}(1)$ in the large $t$ region, but it's all I could think of doing for now. From here I noticed that the integral looked very similar to the gamma function. I tried changing variables to convert it to something involving the gamma function, but the integration limits were giving me issues. Next I tried taking the derivative inside the integral. I found that $\left(\frac{t^{s}}{\Gamma(s)}\right)'\biggr|_{s=0}=-1$ , resulting in the following expression: $$ \frac{1}{\epsilon_{1}\epsilon_{2}} \int_{0}^{\infty}  t^{-3}e^{itx} dt $$ This looks relatively simple, but evaluating the antideriavtive gave an expression involving triginometric integrals which are divergent at zero. This makes me think that maybe one of my approximations is invalid. I am quite certain that the resulting expression should be $\frac{1}{2\epsilon_{1}\epsilon_{2}}x^{2}(\log(x)-\frac{3}{2})$ , and would really appreciate some help with proving it.","I am trying to compute the leading order term of the following expression in the small limit; First of all I tried expanding the exponentials with 's in them, leading to I'm not sure how valid this is given that in the large region, but it's all I could think of doing for now. From here I noticed that the integral looked very similar to the gamma function. I tried changing variables to convert it to something involving the gamma function, but the integration limits were giving me issues. Next I tried taking the derivative inside the integral. I found that , resulting in the following expression: This looks relatively simple, but evaluating the antideriavtive gave an expression involving triginometric integrals which are divergent at zero. This makes me think that maybe one of my approximations is invalid. I am quite certain that the resulting expression should be , and would really appreciate some help with proving it.","\epsilon 
I
=
\frac{d}{ds}\biggr|_{s\rightarrow 0}\frac{1}{\Gamma(s)}
\int_{0}^{\infty} dt 
\frac{t^{s-1}e^{itx}}{(1-e^{i\epsilon_{1}t}) (1-e^{i\epsilon_{2} t})}
 \epsilon 
-\frac{1}{\epsilon_{1}\epsilon_{2}}
\frac{d}{ds}\biggr|_{s\rightarrow 0}\frac{1}{\Gamma(s)}
\int_{0}^{\infty} 
t^{s-3}e^{itx}
dt
 \epsilon t=\mathcal{O}(1) t \left(\frac{t^{s}}{\Gamma(s)}\right)'\biggr|_{s=0}=-1 
\frac{1}{\epsilon_{1}\epsilon_{2}}
\int_{0}^{\infty} 
t^{-3}e^{itx}
dt
 \frac{1}{2\epsilon_{1}\epsilon_{2}}x^{2}(\log(x)-\frac{3}{2})","['calculus', 'integration', 'complex-analysis', 'asymptotics', 'improper-integrals']"
78,Let $f$ be an entire function,Let  be an entire function,f,"I'm working on this problem: Let $f$ be an entire function. Suppose $|f(z)|=1$ if $|z|=1$ and $f$ has only one zero in the unit disk $D_1(0)$ . Prove that $f(z)=cz$ for some constant $c$ . proof: I write down what I want to prove: $(1)$ I would like to prove that $\frac{f(z)}{z}$ is entire. $(2)$ I would like to prove $\left|\frac{f(z)}{z} \right|$ is bounded. For $(1)$ I think I must apply Riemann's removable theorem to extend analytically $\frac{f(z)}{z}$ to $\mathbb{C}$ . It happens if it is bounded at that singularity. I'm not sure if this holds by our second assumption. For $(2)$ I observe $ \left| \frac{f(z)}{z} \right|\leq 1\quad \ \forall z \in \partial D_1(0). $ So, finally I could apply Liouville's  theorem. I'll appreciate if someone could help me out. Thanks.","I'm working on this problem: Let be an entire function. Suppose if and has only one zero in the unit disk . Prove that for some constant . proof: I write down what I want to prove: I would like to prove that is entire. I would like to prove is bounded. For I think I must apply Riemann's removable theorem to extend analytically to . It happens if it is bounded at that singularity. I'm not sure if this holds by our second assumption. For I observe So, finally I could apply Liouville's  theorem. I'll appreciate if someone could help me out. Thanks.","f |f(z)|=1 |z|=1 f D_1(0) f(z)=cz c (1) \frac{f(z)}{z} (2) \left|\frac{f(z)}{z} \right| (1) \frac{f(z)}{z} \mathbb{C} (2) 
\left| \frac{f(z)}{z} \right|\leq 1\quad \ \forall z \in \partial D_1(0).
",['complex-analysis']
79,Homeomorphism Between Closed Riemann Surfaces Homotopic to Quasiconformal Mapping,Homeomorphism Between Closed Riemann Surfaces Homotopic to Quasiconformal Mapping,,"I'm re-reading a paper of Bers and for the second time, and I am yet again confused about the claim in the title, which Bers declares to be easy to prove. For context, I'll lay out some terminology. We call a pair $(S, \alpha)$ a marked Riemann surface when $S$ is a Riemann surface and $\alpha$ is an equivalence class of generators of $\pi_1S$ , each of which $\{a_i\}_{i=1}^{2g}$ satisfies the standard one relation $1= \prod_{j=1}^{2g} a_{2j-1} a_{2j} a_{2j-1}^{-1}a_{2j}^{-1}$ . A map between marked Riemann surfaces $f:(S, \alpha) \rightarrow (S',\alpha')$ is a homeomorphism that respects the markings. Finally, call a pair of marked Riemann surfaces $(S, \alpha), (S', \alpha')$ to be similarly oriented if there is an orientation-preserving homeomorphism $f: S \rightarrow S'$ . Now, Bers goes on to make the following claim: If $(S, \alpha)$ and $(S', \alpha')$ are similarly oriented marked Riemann surfaces, then there is a quasiconformal mapping $f: (S, \alpha) \rightarrow (S', \alpha')$ . Ok, this sounds plausible enough to me. On the other hand, it is by no means obvious. Here, I should mention Bers has earlier stated the equivalence of a few (standard) definitions of quasiconformality for a mapping $f: S \rightarrow S'$ , including the following: (i) $\hat{f}_\bar{z} = \mu \hat{f}_{z}$ , in terms of weak derivatives, for any coordinate representation $\hat{f}$ , where $\mu \in L^{\infty}$ (ii) any coordinate representation $\hat{f}$ has bounded dilatation across all quadrilaterals. Specifically, Bers goes on to say Had we demanded that the homeomorphism $f$ be continuously differentiable everywhere the proof would be somewhat laborious. Since we use a very general definition of quasiconformality, the proof presents no difficulties and may be omitted. My question is thus, what is the idea that Bers' has in mind for the proof? By hypothesis, we've got our hands on a homeomorphism $f$ that respects the desired marking, but a priori $f$ might have unbounded dilatation. So, we need some clever way to homotope $f$ so that its dilatation becomes bounded across the whole surface, at which point we are done. However, I see no intuitive way to make this work. Technical details aside, I am very interested to see what ideas anyone has, or better yet, what idea they imagine Bers thought would easily yield a proof. Bers, Lipman , Quasiconformal mappings and Teichmüller’s theorem, Princeton Math. Ser. 24, 89-119 (1960). ZBL0100.28904 .","I'm re-reading a paper of Bers and for the second time, and I am yet again confused about the claim in the title, which Bers declares to be easy to prove. For context, I'll lay out some terminology. We call a pair a marked Riemann surface when is a Riemann surface and is an equivalence class of generators of , each of which satisfies the standard one relation . A map between marked Riemann surfaces is a homeomorphism that respects the markings. Finally, call a pair of marked Riemann surfaces to be similarly oriented if there is an orientation-preserving homeomorphism . Now, Bers goes on to make the following claim: If and are similarly oriented marked Riemann surfaces, then there is a quasiconformal mapping . Ok, this sounds plausible enough to me. On the other hand, it is by no means obvious. Here, I should mention Bers has earlier stated the equivalence of a few (standard) definitions of quasiconformality for a mapping , including the following: (i) , in terms of weak derivatives, for any coordinate representation , where (ii) any coordinate representation has bounded dilatation across all quadrilaterals. Specifically, Bers goes on to say Had we demanded that the homeomorphism be continuously differentiable everywhere the proof would be somewhat laborious. Since we use a very general definition of quasiconformality, the proof presents no difficulties and may be omitted. My question is thus, what is the idea that Bers' has in mind for the proof? By hypothesis, we've got our hands on a homeomorphism that respects the desired marking, but a priori might have unbounded dilatation. So, we need some clever way to homotope so that its dilatation becomes bounded across the whole surface, at which point we are done. However, I see no intuitive way to make this work. Technical details aside, I am very interested to see what ideas anyone has, or better yet, what idea they imagine Bers thought would easily yield a proof. Bers, Lipman , Quasiconformal mappings and Teichmüller’s theorem, Princeton Math. Ser. 24, 89-119 (1960). ZBL0100.28904 .","(S, \alpha) S \alpha \pi_1S \{a_i\}_{i=1}^{2g} 1= \prod_{j=1}^{2g} a_{2j-1} a_{2j} a_{2j-1}^{-1}a_{2j}^{-1} f:(S, \alpha) \rightarrow (S',\alpha') (S, \alpha), (S', \alpha') f: S \rightarrow S' (S, \alpha) (S', \alpha') f: (S, \alpha) \rightarrow (S', \alpha') f: S \rightarrow S' \hat{f}_\bar{z} = \mu \hat{f}_{z} \hat{f} \mu \in L^{\infty} \hat{f} f f f f","['complex-analysis', 'surfaces', 'riemann-surfaces', 'teichmueller-theory', 'quasiconformal-maps']"
80,Complex series - convergence criteria,Complex series - convergence criteria,,"I am doing Apostol sections concerning series/sequences. In all his statements of tests, like ratio and root tests, it is mentioned that the tested series are non-negative, which automatically precludes the use of these tests for the complex series, since complex numbers cannot be non-negative. However, in Wiki, it is mentioned that the root and ratio tests are actually used with the complex series too. Moreover, some of Apostol exercises are not easily solvable without using these tests with the complex series. But how does it work? Both the root and ratio tests are based on the comparison with the geometric series. In order to use the root test, we first need to take the absolute value of the complex sequence, because the limit has to be compared to 1, which is a real number. Thus, even if the root test shows that the convergence, it only shows the absolute convergence! It says nothing about conditional convergence. An example from Apostol: $\sum\limits_{n = 1}^\infty (1 + \frac{1}{n})^{n^2} z^n, z \in \mathcal{C}$ By the root test we get: $|a_n|^{\frac{1}{n}} = (1 + \frac{1}{n})^n |z| = e|z|$ as $n \rightarrow \infty$ Now, we can see for which values of $z$ the series converges. If $|z| < 1/e$ , then the series converges absolutely . If $|z| > 1/e$ , the series diverges, if $|z| = 1$ , the result is inconclusive. By saying the series diverges in the case $|z| > 1/e$ , why? To me, it says that the series does not converge absolutely ( absolutely diverges ?), however, it does not say if the series can still converge conditionally if $z > 1/e$ . Moreover, is it valid to test the above series without the modulus operation? So that $a^{\frac{1}{n}} = ez, n \rightarrow \infty$ . What does it tell us then, and how is it compared to 1, since complex numbers are unordered? Can someone clarify?","I am doing Apostol sections concerning series/sequences. In all his statements of tests, like ratio and root tests, it is mentioned that the tested series are non-negative, which automatically precludes the use of these tests for the complex series, since complex numbers cannot be non-negative. However, in Wiki, it is mentioned that the root and ratio tests are actually used with the complex series too. Moreover, some of Apostol exercises are not easily solvable without using these tests with the complex series. But how does it work? Both the root and ratio tests are based on the comparison with the geometric series. In order to use the root test, we first need to take the absolute value of the complex sequence, because the limit has to be compared to 1, which is a real number. Thus, even if the root test shows that the convergence, it only shows the absolute convergence! It says nothing about conditional convergence. An example from Apostol: By the root test we get: as Now, we can see for which values of the series converges. If , then the series converges absolutely . If , the series diverges, if , the result is inconclusive. By saying the series diverges in the case , why? To me, it says that the series does not converge absolutely ( absolutely diverges ?), however, it does not say if the series can still converge conditionally if . Moreover, is it valid to test the above series without the modulus operation? So that . What does it tell us then, and how is it compared to 1, since complex numbers are unordered? Can someone clarify?","\sum\limits_{n = 1}^\infty (1 + \frac{1}{n})^{n^2} z^n, z \in \mathcal{C} |a_n|^{\frac{1}{n}} = (1 + \frac{1}{n})^n |z| = e|z| n \rightarrow \infty z |z| < 1/e |z| > 1/e |z| = 1 |z| > 1/e z > 1/e a^{\frac{1}{n}} = ez, n \rightarrow \infty","['sequences-and-series', 'complex-analysis', 'convergence-divergence']"
81,argument principle with a polynomial,argument principle with a polynomial,,"The problem is: How many zeros of the polynomial $$ f(z)=z^4+3z^2+z+1 $$ lie in the right half-plane? To solve this, we use the argument principle, $$ \text{number of zeros}=\frac{1}{2\pi i}\int_{\partial\Omega}\frac{f'(z)}{f(z)}dz. $$ Here $$ \partial\Omega=\{z=iy, -R\leq y\leq R\}\cup\{ z=Re^{i\theta},-\frac{\pi}{2}\leq \theta\leq\frac{\pi}{2}\}. $$ It claims that $$ \int_{-iR}^{iR}\frac{f'(z)}{f(z)}dz=0. $$ But I don't know why above equality holds.","The problem is: How many zeros of the polynomial lie in the right half-plane? To solve this, we use the argument principle, Here It claims that But I don't know why above equality holds.","
f(z)=z^4+3z^2+z+1
 
\text{number of zeros}=\frac{1}{2\pi i}\int_{\partial\Omega}\frac{f'(z)}{f(z)}dz.
 
\partial\Omega=\{z=iy, -R\leq y\leq R\}\cup\{ z=Re^{i\theta},-\frac{\pi}{2}\leq \theta\leq\frac{\pi}{2}\}.
 
\int_{-iR}^{iR}\frac{f'(z)}{f(z)}dz=0.
",['complex-analysis']
82,"How to find the analytic function $f=u(x,y)+iv(x,y)$",How to find the analytic function,"f=u(x,y)+iv(x,y)","How to find $u$ and $v$ such that $f=u(x,y)+iv(x,y)$ is analytic, knowing that $\cfrac{v}{u}=\phi(y)$ . I tried writing $v(x,y)=u(x,y)\phi(y)$ and then calculating the partial derivatives with respect to $x$ and $y$ : a) $\cfrac{\partial{v}}{\partial{x}}=\cfrac{\partial{u}}{\partial{x}}\phi(y)$ ; b) $\cfrac{\partial{v}}{\partial{y}}=\cfrac{\partial{u}}{\partial{y}}\phi(y)+u(x,y)\phi'(y)$ From the first equation, after replacing $\frac{\partial{v}}{\partial{x}}$ by $-\frac{\partial{u}}{\partial{y}}$ from the Cauchy-Riemann equations, I find that $$u(x,y)=-\int{\phi(y)\frac{\partial{u}}{\partial{x}}dy}$$ Now how should I calculate this integral? Is there another way to find $u$ and $v$ ?","How to find and such that is analytic, knowing that . I tried writing and then calculating the partial derivatives with respect to and : a) ; b) From the first equation, after replacing by from the Cauchy-Riemann equations, I find that Now how should I calculate this integral? Is there another way to find and ?","u v f=u(x,y)+iv(x,y) \cfrac{v}{u}=\phi(y) v(x,y)=u(x,y)\phi(y) x y \cfrac{\partial{v}}{\partial{x}}=\cfrac{\partial{u}}{\partial{x}}\phi(y) \cfrac{\partial{v}}{\partial{y}}=\cfrac{\partial{u}}{\partial{y}}\phi(y)+u(x,y)\phi'(y) \frac{\partial{v}}{\partial{x}} -\frac{\partial{u}}{\partial{y}} u(x,y)=-\int{\phi(y)\frac{\partial{u}}{\partial{x}}dy} u v","['integration', 'complex-analysis']"
83,hyperbolic distance between two reals,hyperbolic distance between two reals,,"The hyperbolic distance between two points $z_1, z_2 \in D_1(0)$ is defined as \begin{equation} d(z_1, z_2)= \inf_\gamma \int_0^1 \frac{|\gamma'(t)|} {1- |\gamma(t)|^2} dt \, , \end{equation} where the infimum is taken over all smooth curves $\gamma:[0,1] \to D_1(0)$ joining $z_1$ and $z_2$ . Now I want to prove that \begin{equation} d(0,s) = \frac{1}{2} \log \left( \frac{1+s}{1-s}\right) \end{equation} for $s \in [0,1)$ . I think I know where I have to go, since \begin{equation} \int_0^s \frac{1}{1-x^2} dx = \frac{1}{2} \log \left( \frac{1+s}{1-s}\right) \,, \end{equation} but I don't know how to get there. Any ideas?","The hyperbolic distance between two points is defined as where the infimum is taken over all smooth curves joining and . Now I want to prove that for . I think I know where I have to go, since but I don't know how to get there. Any ideas?","z_1, z_2 \in D_1(0) \begin{equation}
d(z_1, z_2)= \inf_\gamma \int_0^1 \frac{|\gamma'(t)|} {1- |\gamma(t)|^2} dt \, ,
\end{equation} \gamma:[0,1] \to D_1(0) z_1 z_2 \begin{equation}
d(0,s) = \frac{1}{2} \log \left( \frac{1+s}{1-s}\right)
\end{equation} s \in [0,1) \begin{equation}
\int_0^s \frac{1}{1-x^2} dx = \frac{1}{2} \log \left( \frac{1+s}{1-s}\right) \,,
\end{equation}","['complex-analysis', 'complex-integration', 'hyperbolic-geometry']"
84,Learning roadmap for Complex Geometry,Learning roadmap for Complex Geometry,,"I am still an undergraduate and I have taken courses like complex analysis and differential geometry. Also, I learnt myself manifold theory. (the book by Loring Tu). Currently, I am quite interested in complex geometry, where I guess it is the intersection between complex analysis and geometry. But what is the roadmap for me to reach that research field? What particular mathematics is required? Do I need to learn PDE?","I am still an undergraduate and I have taken courses like complex analysis and differential geometry. Also, I learnt myself manifold theory. (the book by Loring Tu). Currently, I am quite interested in complex geometry, where I guess it is the intersection between complex analysis and geometry. But what is the roadmap for me to reach that research field? What particular mathematics is required? Do I need to learn PDE?",,"['complex-analysis', 'soft-question', 'manifolds', 'complex-geometry', 'several-complex-variables']"
85,Is there a simple proof that $(\Sigma a_n)(\Sigma b_n)=\Sigma (a_0b_n + \ldots + a_nb_0)$ whenever the series converge?,Is there a simple proof that  whenever the series converge?,(\Sigma a_n)(\Sigma b_n)=\Sigma (a_0b_n + \ldots + a_nb_0),"That is, if the power series $\Sigma a_n$ , $\Sigma b_n$ and $\Sigma c_n$ converge to $A, B$ and $C$ respectively, where $c_n = a_0b_n + \ldots + a_nb_0$ , then, $$AB=C$$ Rudin's proof, for instance, makes use of Merten's Theorem and the continuity of power series (in fact, he mentions the theorem in chapter 3, but doesn't prove it until chapter 8 due to lack of tools to establish the result). I was just wondering if there was a more direct proof.","That is, if the power series , and converge to and respectively, where , then, Rudin's proof, for instance, makes use of Merten's Theorem and the continuity of power series (in fact, he mentions the theorem in chapter 3, but doesn't prove it until chapter 8 due to lack of tools to establish the result). I was just wondering if there was a more direct proof.","\Sigma a_n \Sigma b_n \Sigma c_n A, B C c_n = a_0b_n + \ldots + a_nb_0 AB=C","['sequences-and-series', 'complex-analysis', 'analysis', 'proof-writing']"
86,Pole expansion using Mittag-Leffler's theorem,Pole expansion using Mittag-Leffler's theorem,,"In Arfken's Mathematical Methods for Physicists, we have the following statement of a result due to Mittag-Leffler. We assume there are poles at $0<|a_1|<|a_2|<...$ with residues $b_n$ . ""Let us consider a series of concentric circles $C_n$ about the origin so that $C_n$ includes $a_1,a_2,...a_n$ but no other poles, its radius $R_n \rightarrow \infty$ as $n\rightarrow\infty$ . To guarantee convergence we assume that $|f(z)|<\epsilon R_n$ for any small positive constant $\epsilon$ and all z on $C_n$ . Then the series $$f(z)=f(0)+\sum_{n=1}^{\infty} b_n(\left(z-a_n\right)^{-1}+a_n^{-1})$$ converges to $f(z)$ ."" I'm confused by the $\epsilon$ . For which $n$ does the function satisfy this bound? If the function satisfies that bound for each $n$ and any $\epsilon$ , then the requirement sounds much too specific for the theorem to be of any use - $f$ would need to be zero on each $C_n$ to satisfy that bound, and that's not realistic to expect for a reasonable function. I'm sure there's a communication breakdown between Arfken and I. Can someone help resolve this?","In Arfken's Mathematical Methods for Physicists, we have the following statement of a result due to Mittag-Leffler. We assume there are poles at with residues . ""Let us consider a series of concentric circles about the origin so that includes but no other poles, its radius as . To guarantee convergence we assume that for any small positive constant and all z on . Then the series converges to ."" I'm confused by the . For which does the function satisfy this bound? If the function satisfies that bound for each and any , then the requirement sounds much too specific for the theorem to be of any use - would need to be zero on each to satisfy that bound, and that's not realistic to expect for a reasonable function. I'm sure there's a communication breakdown between Arfken and I. Can someone help resolve this?","0<|a_1|<|a_2|<... b_n C_n C_n a_1,a_2,...a_n R_n \rightarrow \infty n\rightarrow\infty |f(z)|<\epsilon R_n \epsilon C_n f(z)=f(0)+\sum_{n=1}^{\infty} b_n(\left(z-a_n\right)^{-1}+a_n^{-1}) f(z) \epsilon n n \epsilon f C_n",['complex-analysis']
87,Formula for $\prod\limits_{k=-\infty}^\infty \frac{1}{e}\left(1+\frac{1}{k+z}\right)^{k+z+\frac{1}{2}}$,Formula for,\prod\limits_{k=-\infty}^\infty \frac{1}{e}\left(1+\frac{1}{k+z}\right)^{k+z+\frac{1}{2}},"Let $\displaystyle ~(-1)^z:=e^{i\pi z}~$ for $~z\in\mathbb{C} ~\land~\Re(z)\geq 0~$ . $\displaystyle A(z):=\prod\limits_{k=0}^\infty \frac{1}{e}\left(1+\frac{1}{k+z}\right)^{k+z+\frac{1}{2}}~$ for $~z\in\mathbb{C}\setminus\mathbb{Z}~$ . From that follows immediately $\displaystyle ~A(-z)\,A(z)=\prod\limits_{k=-\infty}^\infty \frac{1}{e}\left(1+\frac{1}{k+z}\right)^{k+z+\frac{1}{2}}~$ . Question: $~$ How can we proof $$A(-z)\,A(z)=\frac{1}{1-e^{-i2\pi z}} ~~, \hspace{5mm}\Re(z)\geq 0 \hspace{1cm} ?$$ Background: Proof of the well-known Euler’s Infinite Product for the Sine using the Stirling formula. $\displaystyle F(z) := \Gamma(z+1) = \left(\frac{z}{e}\right)^z\sqrt{2\pi z}\,A(z)$ Formal extension: $\enspace\displaystyle F(-z):=\left(-\frac{z}{e}\right)^{-z}\sqrt{-2\pi z}\,A(-z)$ We get $\enspace\displaystyle F(-z) = e^{i\pi\left(-z+\frac{1}{2}\right)}\left(\frac{z}{e}\right)^{-z}\sqrt{2\pi z}\,A(-z) = \frac{e^{i\pi\left(-z+\frac{1}{2}\right)}}{1-e^{-i2\pi z}}\left(\frac{e}{z}\right)^z\frac{\sqrt{2\pi z}}{A(z)} =  \frac{\sqrt{2\pi z}}{2A(z)\sin(\pi z)}\left(\frac{e}{z}\right)^z$ and therefore $\enspace\displaystyle F(-z)\,F(z) = \frac{\sqrt{2\pi z}}{2\,A(z)\sin(\pi z)}\left(\frac{e}{z}\right)^z \cdot \left(\frac{z}{e}\right)^z\sqrt{2\pi z}\,A(z) = \frac{\pi z}{\sin(\pi z)}~$ . EDIT: After a discussion with Diger , I think it's a problem to use the definition of $A(z)$ for $A(-z)$ . It comes from the interpretation of $(-1)^{-z}$ and $(\frac{1}{-1})^z$ . It makes sense to define: $$A(z):=\prod\limits_{k=0}^\infty \frac{1}{e}\frac{(k+z+1)^{k+z+\frac{1}{2}}}{(k+z)^{k+z+\frac{1}{2}}}$$ EDIT 2: $~~$ Solution J. R. Quine, S. H. Heydari and R. Y. Song are discussing Zeta Regularized Products here . On page 226, example $10$ , is shown how my question could be answered.","Let for . for . From that follows immediately . Question: How can we proof Background: Proof of the well-known Euler’s Infinite Product for the Sine using the Stirling formula. Formal extension: We get and therefore . EDIT: After a discussion with Diger , I think it's a problem to use the definition of for . It comes from the interpretation of and . It makes sense to define: EDIT 2: Solution J. R. Quine, S. H. Heydari and R. Y. Song are discussing Zeta Regularized Products here . On page 226, example , is shown how my question could be answered.","\displaystyle ~(-1)^z:=e^{i\pi z}~ ~z\in\mathbb{C} ~\land~\Re(z)\geq 0~ \displaystyle A(z):=\prod\limits_{k=0}^\infty \frac{1}{e}\left(1+\frac{1}{k+z}\right)^{k+z+\frac{1}{2}}~ ~z\in\mathbb{C}\setminus\mathbb{Z}~ \displaystyle ~A(-z)\,A(z)=\prod\limits_{k=-\infty}^\infty \frac{1}{e}\left(1+\frac{1}{k+z}\right)^{k+z+\frac{1}{2}}~ ~ A(-z)\,A(z)=\frac{1}{1-e^{-i2\pi z}} ~~, \hspace{5mm}\Re(z)\geq 0 \hspace{1cm} ? \displaystyle F(z) := \Gamma(z+1) = \left(\frac{z}{e}\right)^z\sqrt{2\pi z}\,A(z) \enspace\displaystyle F(-z):=\left(-\frac{z}{e}\right)^{-z}\sqrt{-2\pi z}\,A(-z) \enspace\displaystyle F(-z) = e^{i\pi\left(-z+\frac{1}{2}\right)}\left(\frac{z}{e}\right)^{-z}\sqrt{2\pi z}\,A(-z) = \frac{e^{i\pi\left(-z+\frac{1}{2}\right)}}{1-e^{-i2\pi z}}\left(\frac{e}{z}\right)^z\frac{\sqrt{2\pi z}}{A(z)} = 
\frac{\sqrt{2\pi z}}{2A(z)\sin(\pi z)}\left(\frac{e}{z}\right)^z \enspace\displaystyle F(-z)\,F(z) = \frac{\sqrt{2\pi z}}{2\,A(z)\sin(\pi z)}\left(\frac{e}{z}\right)^z \cdot \left(\frac{z}{e}\right)^z\sqrt{2\pi z}\,A(z) = \frac{\pi z}{\sin(\pi z)}~ A(z) A(-z) (-1)^{-z} (\frac{1}{-1})^z A(z):=\prod\limits_{k=0}^\infty \frac{1}{e}\frac{(k+z+1)^{k+z+\frac{1}{2}}}{(k+z)^{k+z+\frac{1}{2}}} ~~ 10","['complex-analysis', 'infinite-product']"
88,find all entire functions $f$ such that $f(\frac{1}{n})=f(\frac{1}{n²})$,find all entire functions  such that,f f(\frac{1}{n})=f(\frac{1}{n²}),"I've got determine all entire functions $f$ such that $f(\frac{1}{n})=f(\frac{1}{n²})$ is fullfilled for all $ n\in \mathbb{N}$ . Knowing, that they are entire we can write down both terms as a taylor series around $0$ : $f(1/n)=\sum_{k=0}^\infty a_k(\frac{1}{n})^k=a_0+a_1\frac{1}{n}+a_2\frac{1}{n}^2+a_3\frac{1}{n}^3+a_4\frac{1}{n}^4...$ $f(1/n^2)=\sum_{k=0}^\infty a_k(\frac{1}{n^2})^k=a_0+a_1\frac{1}{n^2}+a_2\frac{1}{n^2}^2+a_3\frac{1}{n^2}^3+a_4\frac{1}{n^2}^4...$ When they are equal, then the coefficients are $0$ for all odd numbers, that is $a_k=0$ for $k=2n+1$ , That means $a_1=a_3=a_5=...=0$ . Then because of the equality of both series we get $a_2=a_1=0$ which then implies $a_4=a_2=0$ and so on.   So there is no entire function other then a constant function, right? Is there maybe a more simple proof for this? Edit: My argumentation for $f(0)=0$ is wrong. So $a_0$ is the only coefficent I have.","I've got determine all entire functions such that is fullfilled for all . Knowing, that they are entire we can write down both terms as a taylor series around : When they are equal, then the coefficients are for all odd numbers, that is for , That means . Then because of the equality of both series we get which then implies and so on.   So there is no entire function other then a constant function, right? Is there maybe a more simple proof for this? Edit: My argumentation for is wrong. So is the only coefficent I have.",f f(\frac{1}{n})=f(\frac{1}{n²})  n\in \mathbb{N} 0 f(1/n)=\sum_{k=0}^\infty a_k(\frac{1}{n})^k=a_0+a_1\frac{1}{n}+a_2\frac{1}{n}^2+a_3\frac{1}{n}^3+a_4\frac{1}{n}^4... f(1/n^2)=\sum_{k=0}^\infty a_k(\frac{1}{n^2})^k=a_0+a_1\frac{1}{n^2}+a_2\frac{1}{n^2}^2+a_3\frac{1}{n^2}^3+a_4\frac{1}{n^2}^4... 0 a_k=0 k=2n+1 a_1=a_3=a_5=...=0 a_2=a_1=0 a_4=a_2=0 f(0)=0 a_0,"['complex-analysis', 'alternative-proof', 'entire-functions']"
89,For polynomial $f$ : $\forall z \in \mathbb{C} : |zf(z)| \leq |z|^k \implies |f(z)| \leq |z|^{k-1}$?,For polynomial  : ?,f \forall z \in \mathbb{C} : |zf(z)| \leq |z|^k \implies |f(z)| \leq |z|^{k-1},"Suppose $f : \mathbb{C} \to \mathbb{C}$ is a polynomial such that $\forall z \in \mathbb{C}: |zf(z)| \leq |z|^k$ . Does it follow that $ \forall z \in \mathbb{C}: |f(z)| \leq |z|^{k-1}$ ? My thoughts: Certainly this holds $\forall z \neq 0$ . For $z = 0 $ , I believe this should also be true by continuity of $f$ . So I would say the assertion is valid. However I am unsure. I would be grateful if someone could either verify this for me or point out where I am mistaken!","Suppose is a polynomial such that . Does it follow that ? My thoughts: Certainly this holds . For , I believe this should also be true by continuity of . So I would say the assertion is valid. However I am unsure. I would be grateful if someone could either verify this for me or point out where I am mistaken!",f : \mathbb{C} \to \mathbb{C} \forall z \in \mathbb{C}: |zf(z)| \leq |z|^k  \forall z \in \mathbb{C}: |f(z)| \leq |z|^{k-1} \forall z \neq 0 z = 0  f,"['complex-analysis', 'analysis']"
90,Equivalence of power series,Equivalence of power series,,"Ok, this is probably a very silly question, and I'm probably overcomplicating the issue, but here it goes: I'm making questions on power series, and it asks me to find the convergence radius of the series $$\sum_{n=1}^\infty z^{n!}$$ Of course, usually a power series is given in the form $$\sum_{n=1}^\infty a_n z^n$$ and we see that we here have to take $$a_n = \begin{cases}1 \quad n = k!   \quad \mathrm{for \ some \ k}\\ 0 \quad \mathrm{else}\end{cases}$$ From this, it is easy to see that $\limsup_{n\to \infty} |a_n|^{1/n} = 1$ as we have a subsequence converging to $1$ and the limsup can't become greater than that, so $1$ is the largest limit of a subsequence. Hence, the convergence radius is $1$ . My concern : Shouldn't we formally show that $$\sum_{n=1}^\infty z^{n!}= \sum_{n=1}^\infty a_n z^n$$ for my choice of $(a_n)_n$ ? In the left sum, we sum over 'less' terms that the right one, while in the right sum we sum over 'more' terms, but these are zero so intuitively the terms are not affected. Write $(c_n)_n$ for the left partial sums and $(d_n)_n$ for the right partial sums. Then we see that $(d_n)_n$ has the form $(c_1,\underbrace{c_1, \dots, c_1}_{k_1 terms}, c_2, \underbrace{c_2, \dots, c_2}_{k_2 terms}, \dots)$ and it suffices to show that $(c_n)_n$ converges if and only if $(d_n)_n$ converges and in this   case the limits coincide If $(d_n)_n$ converges, then $(c_n)_n$ converges to the same limit because it contains $(c_n)_n$ as a subsequence. Conversely, assume $(c_n)_n$ converges to $c$ . Let $\epsilon > 0$ and choose $n_0$ such that $d(c_n, c) < \epsilon$ whenever $n \geq n_0$ . If $n \geq n_0 + k_1 + \dots + k_{n_0}$ , then $d_n\in \{c_{n_0}, c_{n_0+1}, \dots\}$ and it follows that $d(d_n, c) < \epsilon$ . Hence $d_n \to c$ , proving the statement. $\quad \square$ . Any thoughts about it? Am I overthinking this?","Ok, this is probably a very silly question, and I'm probably overcomplicating the issue, but here it goes: I'm making questions on power series, and it asks me to find the convergence radius of the series Of course, usually a power series is given in the form and we see that we here have to take From this, it is easy to see that as we have a subsequence converging to and the limsup can't become greater than that, so is the largest limit of a subsequence. Hence, the convergence radius is . My concern : Shouldn't we formally show that for my choice of ? In the left sum, we sum over 'less' terms that the right one, while in the right sum we sum over 'more' terms, but these are zero so intuitively the terms are not affected. Write for the left partial sums and for the right partial sums. Then we see that has the form and it suffices to show that converges if and only if converges and in this   case the limits coincide If converges, then converges to the same limit because it contains as a subsequence. Conversely, assume converges to . Let and choose such that whenever . If , then and it follows that . Hence , proving the statement. . Any thoughts about it? Am I overthinking this?","\sum_{n=1}^\infty z^{n!} \sum_{n=1}^\infty a_n z^n a_n = \begin{cases}1 \quad n = k! 
 \quad \mathrm{for \ some \ k}\\ 0 \quad \mathrm{else}\end{cases} \limsup_{n\to \infty} |a_n|^{1/n} = 1 1 1 1 \sum_{n=1}^\infty z^{n!}= \sum_{n=1}^\infty a_n z^n (a_n)_n (c_n)_n (d_n)_n (d_n)_n (c_1,\underbrace{c_1, \dots, c_1}_{k_1 terms}, c_2, \underbrace{c_2, \dots, c_2}_{k_2 terms}, \dots) (c_n)_n (d_n)_n (d_n)_n (c_n)_n (c_n)_n (c_n)_n c \epsilon > 0 n_0 d(c_n, c) < \epsilon n \geq n_0 n \geq n_0 + k_1 + \dots + k_{n_0} d_n\in \{c_{n_0}, c_{n_0+1}, \dots\} d(d_n, c) < \epsilon d_n \to c \quad \square","['real-analysis', 'complex-analysis']"
91,Fundamental Theorem of Calculus in complex analysis?,Fundamental Theorem of Calculus in complex analysis?,,"This following fact came up in a course of complex analysis I was studying, and I was wondering how to prove it. Suppose that $f:D \rightarrow \mathbb{C}$ is continuous, and that $\oint f(z) dz=0$ . $D$ is a domain, not necessarily simply connected. Let $\Gamma$ be a curve connecting $z_0,z \in \mathbb{C}$ , define $F(z)=\int_{\Gamma} f(w) dw$ . Then, $F$ is an analytic function. Comments: It is easy to show that $F$ is well defined, and I was able to do that. I know that there is a real analysis version that is the fundamental theorem of calculus, but to prove analycity I need to show that the C-R eq's hold which is what I am struggling with.","This following fact came up in a course of complex analysis I was studying, and I was wondering how to prove it. Suppose that is continuous, and that . is a domain, not necessarily simply connected. Let be a curve connecting , define . Then, is an analytic function. Comments: It is easy to show that is well defined, and I was able to do that. I know that there is a real analysis version that is the fundamental theorem of calculus, but to prove analycity I need to show that the C-R eq's hold which is what I am struggling with.","f:D \rightarrow \mathbb{C} \oint f(z) dz=0 D \Gamma z_0,z \in \mathbb{C} F(z)=\int_{\Gamma} f(w) dw F F","['calculus', 'complex-analysis', 'cauchy-integral-formula']"
92,Find the residue of $\frac1{z-\sin z} $at its pole. [duplicate],Find the residue of at its pole. [duplicate],\frac1{z-\sin z} ,This question already has answers here : Find the poles and residue of the function $\frac{1}{z-\sin z }$ at $z=0$? (2 answers) Closed 4 years ago . $f(z)=\frac{1}{z-\sin z}$ Now $z-\sin z=z-(z-\frac {z^3}{3!}+\frac{z^5}{5!}\cdots)$ After solving $z=0$ is a pole of order $3$ for $f(z)$ and $$f(z)=\frac1{z^3(\frac1{3!}-\frac{z^2}{5!}+ \cdots  )}$$ What is next?,This question already has answers here : Find the poles and residue of the function $\frac{1}{z-\sin z }$ at $z=0$? (2 answers) Closed 4 years ago . Now After solving is a pole of order for and What is next?,f(z)=\frac{1}{z-\sin z} z-\sin z=z-(z-\frac {z^3}{3!}+\frac{z^5}{5!}\cdots) z=0 3 f(z) f(z)=\frac1{z^3(\frac1{3!}-\frac{z^2}{5!}+ \cdots  )},['complex-analysis']
93,Show that if $f(z)$ is entire and if $f(z)/z^n$ is bounded when $z$ is large then $f$ must be a polynomial. [duplicate],Show that if  is entire and if  is bounded when  is large then  must be a polynomial. [duplicate],f(z) f(z)/z^n z f,"This question already has answers here : Entire function bounded by a polynomial is a polynomial (5 answers) Closed 2 years ago . Suppose that if $f(z)$ is an entire function such that $\dfrac{f(z)}{z^n}$ is bounded for $|z|\ge R$ then $f(z)$ must be a polynomial of degree at most $n$ This same question has already been asked and solved on this website but the solution relies on power series and this question is asked in my text before power series has been introduced. My book gives the hint that I should use Cauchy's estimates for $f^{(m+1)}(z)$ on a disk $|z-z_0|<R$ and then let $R \to \infty$ to obtain that $f^{(m+1)}(z_0)=0$ I proved the hint as follows. $f$ is analytic on $|z|<R$ and also $|f(z)|<k|z|^n$ when $|z|\ge R$ so by Cauchy's estimate on $|z|<R$ , $f^{(m)}(0)\le \dfrac{m!}{R^m}k|z|^n=m!kR^{n-m}\to0$ when $m>n$ and so $f^{(m)}(0)=0 \; \forall \;m>n$ . Now let $z_0\in \mathbb C$ and choose $R'$ big enough such that $B_R(0) \subset B_{R'}(z_0)$ , hence $|z|>R$ when $z \in \partial B_{R'}(z_0).$ $f$ being entire is then analytic on $B_{R'}(z_0)$ and so again by Cauchy's estimates we see that $f^{(m)}(z_0)\le \dfrac{m!}{(R')^m}k|\hat z|^n$ for $\hat z\in \partial B_{R'}(z_0)$ such that $|\hat z| \ge |z|$ for any $z \in \partial B_{R'}(z_0)$ . Note that $|\hat z|=|z_0|+R'$ since for any $z \in \partial B_{R'}(z_0)$ we have $|z|\le |z-z_0|+|z_0|= R' +|z_0|$ . So $|\hat z| \le R' + |z_0|$ but we see that $z_0 + R'\dfrac{z_0}{|z_0|}$ is on $\partial B_{R'}(z_0)$ and $|z_0 + R'\dfrac{z_0}{|z_0|}|=|z_0|+R'$ and this means that $|\hat z| = R' +|z_0|$ . Using this we now see that $f^{(m)}(z_0)\le \dfrac{m!}{(R')^m}k(|z_0|+R')^n$ and if we let $m>n$ then we see that $f^{(m)}(z_0) \to 0$ and $R' \to \infty$ . Now we can say that $f^{(m)}(z)=0$ whenever $m>n$ . Now I do not know how to continue? Why does this mean that $f$ must be a polynomial? I see how it is true if we can use power series but thats not available to me now. Can I use induction as follows? I want to show that if $f$ is entire and $f^{(m)}(z)=0$ for $m>n$ means that $f$ is a polynomial with degree at most $n$ . For the base case suppose $f^{m}(z)=0$ whenever $m>0$ . Hence $f'(z)=0$ which means $f$ is a constant function, or in other words a polynomial with degree $0$ . So the base case is true. Now suppose that $f^{m}(z)=0$ whenever $m>k$ means that $f$ is a polynomial if at most degree $k$ . Suppose that $f^{m}(z)=0$ whenever $m>k+1$ . Let $g(z)=f'(z)$ which is also entire since $f$ is entire. Then we see that $g^{m}(z)=0$ whenever $m>k$ which implys that $g$ is a polynomial of degree at most $k$ and therfore $f'(z)$ is a polynomial of degree at most $k$ which means that $f(z)$ is a polynomial of  degree at most $k+1$ which proves the result. Is this all correct?","This question already has answers here : Entire function bounded by a polynomial is a polynomial (5 answers) Closed 2 years ago . Suppose that if is an entire function such that is bounded for then must be a polynomial of degree at most This same question has already been asked and solved on this website but the solution relies on power series and this question is asked in my text before power series has been introduced. My book gives the hint that I should use Cauchy's estimates for on a disk and then let to obtain that I proved the hint as follows. is analytic on and also when so by Cauchy's estimate on , when and so . Now let and choose big enough such that , hence when being entire is then analytic on and so again by Cauchy's estimates we see that for such that for any . Note that since for any we have . So but we see that is on and and this means that . Using this we now see that and if we let then we see that and . Now we can say that whenever . Now I do not know how to continue? Why does this mean that must be a polynomial? I see how it is true if we can use power series but thats not available to me now. Can I use induction as follows? I want to show that if is entire and for means that is a polynomial with degree at most . For the base case suppose whenever . Hence which means is a constant function, or in other words a polynomial with degree . So the base case is true. Now suppose that whenever means that is a polynomial if at most degree . Suppose that whenever . Let which is also entire since is entire. Then we see that whenever which implys that is a polynomial of degree at most and therfore is a polynomial of degree at most which means that is a polynomial of  degree at most which proves the result. Is this all correct?",f(z) \dfrac{f(z)}{z^n} |z|\ge R f(z) n f^{(m+1)}(z) |z-z_0|<R R \to \infty f^{(m+1)}(z_0)=0 f |z|<R |f(z)|<k|z|^n |z|\ge R |z|<R f^{(m)}(0)\le \dfrac{m!}{R^m}k|z|^n=m!kR^{n-m}\to0 m>n f^{(m)}(0)=0 \; \forall \;m>n z_0\in \mathbb C R' B_R(0) \subset B_{R'}(z_0) |z|>R z \in \partial B_{R'}(z_0). f B_{R'}(z_0) f^{(m)}(z_0)\le \dfrac{m!}{(R')^m}k|\hat z|^n \hat z\in \partial B_{R'}(z_0) |\hat z| \ge |z| z \in \partial B_{R'}(z_0) |\hat z|=|z_0|+R' z \in \partial B_{R'}(z_0) |z|\le |z-z_0|+|z_0|= R' +|z_0| |\hat z| \le R' + |z_0| z_0 + R'\dfrac{z_0}{|z_0|} \partial B_{R'}(z_0) |z_0 + R'\dfrac{z_0}{|z_0|}|=|z_0|+R' |\hat z| = R' +|z_0| f^{(m)}(z_0)\le \dfrac{m!}{(R')^m}k(|z_0|+R')^n m>n f^{(m)}(z_0) \to 0 R' \to \infty f^{(m)}(z)=0 m>n f f f^{(m)}(z)=0 m>n f n f^{m}(z)=0 m>0 f'(z)=0 f 0 f^{m}(z)=0 m>k f k f^{m}(z)=0 m>k+1 g(z)=f'(z) f g^{m}(z)=0 m>k g k f'(z) k f(z) k+1,['complex-analysis']
94,Holomorphic Map of Open Unit Disk into Itself,Holomorphic Map of Open Unit Disk into Itself,,"I'm currently trying to work through the following problem: Let $\mathbb{D}=\{z\in\mathbb{C}~:~|z|<1\}$ denote the open unit disk. Suppose $f:\mathbb{D}\to\mathbb{D}$ is a holomorphic function whose range is contained in a compact subset of $\mathbb{D}$ . Show that $f$ has a unique fixed point. So, we must show that there exists a point $z^{*}\in\mathbb{D}$ with $f(z^{*})=z^{*}$ , and that $f(z)\neq z$ for all $z\in\mathbb{D}$ with $z\neq z^{*}$ . I'm not even sure where to start on this problem. I'm not sure if I should expand $f$ in a power series near the origin (or if that does nothing). Seeing the fact that $f(\mathbb{D})$ lies in a compact subset of $\mathbb{D}$ suggests to me that I should be using Rouche's Theorem somehow, but this could also be completely useless, I'm not sure. I'm also not sure how I would apply it, since it deals with zeros of functions and not fixed points. Thanks in advance for any suggestions.","I'm currently trying to work through the following problem: Let denote the open unit disk. Suppose is a holomorphic function whose range is contained in a compact subset of . Show that has a unique fixed point. So, we must show that there exists a point with , and that for all with . I'm not even sure where to start on this problem. I'm not sure if I should expand in a power series near the origin (or if that does nothing). Seeing the fact that lies in a compact subset of suggests to me that I should be using Rouche's Theorem somehow, but this could also be completely useless, I'm not sure. I'm also not sure how I would apply it, since it deals with zeros of functions and not fixed points. Thanks in advance for any suggestions.",\mathbb{D}=\{z\in\mathbb{C}~:~|z|<1\} f:\mathbb{D}\to\mathbb{D} \mathbb{D} f z^{*}\in\mathbb{D} f(z^{*})=z^{*} f(z)\neq z z\in\mathbb{D} z\neq z^{*} f f(\mathbb{D}) \mathbb{D},"['complex-analysis', 'analysis', 'fixed-points']"
95,Could this problem be similar to the Riemann Hypothesis?,Could this problem be similar to the Riemann Hypothesis?,,"I've found the below equivalence. For $a,b,x\in\mathbb{C}$ , provided that there are no singularities on the right-hand side: \begin{multline}\sum _{k=2}^{\infty}\sum _{j=1}^{\infty}\frac{x^k}{(a j+b)^k}=-\frac{x^2}{2b(b-x)}+\frac{\pi x}{2a}\csc{\frac{b\pi }{a}}\csc{\frac{\pi (b-x)}{a}}\sin{\frac{\pi x}{a}}\\-\frac{x \pi}{a}\int _0^1\left(\csc{\frac{2 \pi (b-x)}{a}}\sin{\frac{2 \pi (b-x)u}{a}}-\csc{\frac{2 \pi b}{a}}\sin{\frac{2 \pi  b u}{a}}\right)\cot{\pi u}\,du\end{multline} Even when the left-hand side may not converge, the right-hand side may still converge. I've notice that $b=1/2$ causes the integral (the imaginary part for real $a,b$ ) to vanish for $x=1$ and $a=-i$ (though the real part doesn't vanish). The integral will always vanish for $x=2b$ , so it's a matter of finding a triple $(a,b,2b)$ that zero out the other part. What are the zeros of this equation?","I've found the below equivalence. For , provided that there are no singularities on the right-hand side: Even when the left-hand side may not converge, the right-hand side may still converge. I've notice that causes the integral (the imaginary part for real ) to vanish for and (though the real part doesn't vanish). The integral will always vanish for , so it's a matter of finding a triple that zero out the other part. What are the zeros of this equation?","a,b,x\in\mathbb{C} \begin{multline}\sum _{k=2}^{\infty}\sum _{j=1}^{\infty}\frac{x^k}{(a j+b)^k}=-\frac{x^2}{2b(b-x)}+\frac{\pi x}{2a}\csc{\frac{b\pi
}{a}}\csc{\frac{\pi (b-x)}{a}}\sin{\frac{\pi x}{a}}\\-\frac{x \pi}{a}\int _0^1\left(\csc{\frac{2 \pi (b-x)}{a}}\sin{\frac{2 \pi (b-x)u}{a}}-\csc{\frac{2
\pi b}{a}}\sin{\frac{2 \pi  b u}{a}}\right)\cot{\pi u}\,du\end{multline} b=1/2 a,b x=1 a=-i x=2b (a,b,2b)",['complex-analysis']
96,A line integral involving $\log \zeta(s)$,A line integral involving,\log \zeta(s),"Let $\zeta$ denote the Riemann zeta function. Using the Cauchy integral theorem, can you evaluate $$I=\int_{\Re(s)=\frac{1}{2}} \frac{(2s-1)}{s^{2}(1-s)^2}\Bigg[\int \log((s-1) \zeta(s)) \mathrm{d}s\Bigg] \mathrm{d}s?$$ Note that $I$ converges since $\zeta(s)=O(|s|)$ . I have provided an answer below as an attempt.","Let denote the Riemann zeta function. Using the Cauchy integral theorem, can you evaluate Note that converges since . I have provided an answer below as an attempt.",\zeta I=\int_{\Re(s)=\frac{1}{2}} \frac{(2s-1)}{s^{2}(1-s)^2}\Bigg[\int \log((s-1) \zeta(s)) \mathrm{d}s\Bigg] \mathrm{d}s? I \zeta(s)=O(|s|),"['complex-analysis', 'number-theory']"
97,Compute the integral $\int_{|z|=\rho}|z-a|^{-4}|dz|$ with $|a|\neq \rho$,Compute the integral  with,\int_{|z|=\rho}|z-a|^{-4}|dz| |a|\neq \rho,I need help in computing the integral indicated above. What I've tried so far: Parametrize the curve indicated by $|z|=\rho$ with $\gamma = z(t) = \rho \cos t + i\sin t$ . Then by definition $$ \int_\gamma f(z)|dz|=\int_\gamma f(z(t))|z'(t)| dt $$ gives the following \begin{align} \int_\gamma |z-a|^{-4} |dz| & = \int_0^{2\pi} |\rho \cos t+i\rho \sin t-a_1-ia_2|^{-4}\rho dt\\ & = \rho\int_0^{2\pi}\frac{1}{(\rho^2-2a_1\cos t-2a_2\sin t + a_1^2+a_2^2)^2} dt \end{align} Where $a=a_1+ia_2$ . It's not hard to see how this becomes complicated very easily. I want to know if there is some sort of 'trick' I'm not aware of or something I might be missing.,I need help in computing the integral indicated above. What I've tried so far: Parametrize the curve indicated by with . Then by definition gives the following Where . It's not hard to see how this becomes complicated very easily. I want to know if there is some sort of 'trick' I'm not aware of or something I might be missing.,"|z|=\rho \gamma = z(t) = \rho \cos t + i\sin t 
\int_\gamma f(z)|dz|=\int_\gamma f(z(t))|z'(t)| dt
 \begin{align}
\int_\gamma |z-a|^{-4} |dz| & = \int_0^{2\pi} |\rho \cos t+i\rho \sin t-a_1-ia_2|^{-4}\rho dt\\
& = \rho\int_0^{2\pi}\frac{1}{(\rho^2-2a_1\cos t-2a_2\sin t + a_1^2+a_2^2)^2} dt
\end{align} a=a_1+ia_2","['integration', 'complex-analysis', 'line-integrals']"
98,How many solutions of $3z^5 + z^2 + 1=0$ have in $1<|z|<2$.,How many solutions of  have in .,3z^5 + z^2 + 1=0 1<|z|<2,I used Rouche's theorem. I got $5$ solutions in $1<|z|<2$ . Is my approach correct?,I used Rouche's theorem. I got solutions in . Is my approach correct?,5 1<|z|<2,['complex-analysis']
99,Extending Lacunary Series beyond their disks,Extending Lacunary Series beyond their disks,,"I've been for the past year and half fascinated by the lacunary series $f(z) = \sum_{n=0}^{\infty} z^{2^n}$ . This function obeys the following equation inside the unit disk. $$f(z^2) = f(z)-z$$ And there is no known ""natural"" way to extend the function outside the disk, so I attempted to create a way. Consider now the following formula. $$ G(x)= - \frac{1}{\ln(2)} \ln\ln x + \left(1 - \sqrt{x}\right) + \left( 1 - \sqrt{\sqrt{x}} \right)+ \left(1- \sqrt{\sqrt{\sqrt{x}}} \right)  ... $$ It's clear that $G(x^2) = G(x) - x$ , and moreover it's well defined over $\mathbb{R}^+ - \lbrace 0,1\rbrace$ . If we consider $0 < x < 1$ $$G(x) = - \frac{i \pi}{\ln(2)}  - \frac{1}{\ln(2)} \ln\ln \frac{1}{x} + \left(1 - \sqrt{x}\right) + \left( 1 - \sqrt{\sqrt{x}} \right)+ \left(1- \sqrt{\sqrt{\sqrt{x}}} \right)  ... $$ So you can graph $G(x) +  \frac{i \pi}{\ln(2)} $ , and observe it as  a function, compared to the lacunary series above Clearly the graphs are extremely similar. And it's not hard to see that $w(x) = g(x)-f(x)$ obeys the functional equation $w(x^2) = w(x)$ . After taking the difference of the two graphs, it looks nearly constant but some careful inspection reveals that $w$ has non trivial structure and its first terms are $$ \frac{i \pi}{\ln(2)} + w(x) \approx 0.333 + \frac{0.255}{1000} + c_0 \sin \frac{2\pi}{\ln 2} \ln \ln \frac{1}{x}...  $$ Where $c_0 \approx 10^{-4}$ see below for visual evidence So naturally one can use a variant of fourier analysis to come up with sequences of complex numbers $a_k, b_l$ such that: $$w(x) +  \frac{i \pi}{\ln(2)} = c + \sum_{k=0}^{\infty} a_k \sin \frac{2\pi}{\ln 2} \ln \ln \frac{1}{x} + \sum_{l=0}^{\infty} b_l \sin \frac{2\pi}{\ln 2} \ln \ln \frac{1}{x} $$ And therefore: $$ -\frac{\ln \ln x }{\ln(2)} + \sum_{n=1}^{\infty} \left( 1 - x^{\frac{1}{2^n}} \right)  - \frac{i \pi}{\ln(2)} -  c - \sum_{k=0}^{\infty} \left( a_k \sin \frac{2\pi}{\ln 2} \ln \ln \frac{1}{x} \right) - \sum_{l=0}^{\infty} \left(  b_l \sin \frac{2\pi}{\ln 2} \ln \ln \frac{1}{x} \right)  = \sum_{n=0}^{\infty} x^{2^n} $$ Over $0 < x < 1$ (call the entire completed left hand side $L$ noting that $L = G(x) - w(x)$ ). Now it's interesting observe that $L$ is a complex  function defined over all real positive numbers, and $L(x^2) = L(x) -x $ over the entire line. We CAN extend $L$ to the entirety of the complex plane using that functional equation and using the values of $L$ on the real line, leading us to create $L(z)$ defined globally on the plane minus the unit disc. Some natural questions arise then: Can we prove that as $x \rightarrow 0$ our definition of $L$ approaches 0? It's clear with the many nested-log-terms that $L$ is not defined at $x=0$ but if a limit can nevertheless be assigned it would be nice since we can then have $L$ agree with $f(z)$ over the entire open unit disk. ""What nice properties"" does $L$ extended to the complex plane have? Is it continuous, is it locally analytic, etc...? We define $L$ through its functional equation for complex arguments, but the series form of $L$ has 3 countably long sequences of functions. Is it possible to select branch cuts for each function in such as way, as to allow $L$ to be directly evaluated from its formula over complex values? (Instead of resorting to the functional equation). There's a nice result from the 1980s that lacunary series such as the one above don't have unique continuous or even unique locally analytic extensions. Nevertheless, if (3) is successful, then if one looks at $L$ it has a ""nice"" decomposition, meaning it is built using rather elementary or simply behaved functions. This motivates the following question, is there a subset of locally-analytic functions that can be defined (Call them R-functions) so that all ""analytic continuations"" are also unique R continuations, and moreover the series I have given above is the unique R continuation of the lacunary series $f(z)$ ? I've been ruminating on this result for a while and I needed a place to put my questions out in the public domain, to collect feedback. My blog has a total viewership of maybe 3 people, so posting this on stackexchange seemed like a more appropriate venue. Updates: The fourier series was explored alongside Bill Gosper and we found a few terms: https://www.wolframcloud.com/objects/637b5c6b-f016-43e1-8d1e-935506bb4d24 , there are more written up if anyone wants I can provide. Discussions with Noam Elkies and Warren Smith revealed that these series are known in the past, but its no longer clear if the coefficents decay fast enough to enable convergence outside the disc. This exercise might just reduce to pretty new identity.","I've been for the past year and half fascinated by the lacunary series . This function obeys the following equation inside the unit disk. And there is no known ""natural"" way to extend the function outside the disk, so I attempted to create a way. Consider now the following formula. It's clear that , and moreover it's well defined over . If we consider So you can graph , and observe it as  a function, compared to the lacunary series above Clearly the graphs are extremely similar. And it's not hard to see that obeys the functional equation . After taking the difference of the two graphs, it looks nearly constant but some careful inspection reveals that has non trivial structure and its first terms are Where see below for visual evidence So naturally one can use a variant of fourier analysis to come up with sequences of complex numbers such that: And therefore: Over (call the entire completed left hand side noting that ). Now it's interesting observe that is a complex  function defined over all real positive numbers, and over the entire line. We CAN extend to the entirety of the complex plane using that functional equation and using the values of on the real line, leading us to create defined globally on the plane minus the unit disc. Some natural questions arise then: Can we prove that as our definition of approaches 0? It's clear with the many nested-log-terms that is not defined at but if a limit can nevertheless be assigned it would be nice since we can then have agree with over the entire open unit disk. ""What nice properties"" does extended to the complex plane have? Is it continuous, is it locally analytic, etc...? We define through its functional equation for complex arguments, but the series form of has 3 countably long sequences of functions. Is it possible to select branch cuts for each function in such as way, as to allow to be directly evaluated from its formula over complex values? (Instead of resorting to the functional equation). There's a nice result from the 1980s that lacunary series such as the one above don't have unique continuous or even unique locally analytic extensions. Nevertheless, if (3) is successful, then if one looks at it has a ""nice"" decomposition, meaning it is built using rather elementary or simply behaved functions. This motivates the following question, is there a subset of locally-analytic functions that can be defined (Call them R-functions) so that all ""analytic continuations"" are also unique R continuations, and moreover the series I have given above is the unique R continuation of the lacunary series ? I've been ruminating on this result for a while and I needed a place to put my questions out in the public domain, to collect feedback. My blog has a total viewership of maybe 3 people, so posting this on stackexchange seemed like a more appropriate venue. Updates: The fourier series was explored alongside Bill Gosper and we found a few terms: https://www.wolframcloud.com/objects/637b5c6b-f016-43e1-8d1e-935506bb4d24 , there are more written up if anyone wants I can provide. Discussions with Noam Elkies and Warren Smith revealed that these series are known in the past, but its no longer clear if the coefficents decay fast enough to enable convergence outside the disc. This exercise might just reduce to pretty new identity.","f(z) = \sum_{n=0}^{\infty} z^{2^n} f(z^2) = f(z)-z  G(x)= - \frac{1}{\ln(2)} \ln\ln x + \left(1 - \sqrt{x}\right) + \left( 1 - \sqrt{\sqrt{x}} \right)+ \left(1- \sqrt{\sqrt{\sqrt{x}}} \right)  ...  G(x^2) = G(x) - x \mathbb{R}^+ - \lbrace 0,1\rbrace 0 < x < 1 G(x) = - \frac{i \pi}{\ln(2)}  - \frac{1}{\ln(2)} \ln\ln \frac{1}{x} + \left(1 - \sqrt{x}\right) + \left( 1 - \sqrt{\sqrt{x}} \right)+ \left(1- \sqrt{\sqrt{\sqrt{x}}} \right)  ...  G(x) +  \frac{i \pi}{\ln(2)}  w(x) = g(x)-f(x) w(x^2) = w(x) w  \frac{i \pi}{\ln(2)} + w(x) \approx 0.333 + \frac{0.255}{1000} + c_0 \sin \frac{2\pi}{\ln 2} \ln \ln \frac{1}{x}...   c_0 \approx 10^{-4} a_k, b_l w(x) +  \frac{i \pi}{\ln(2)} = c + \sum_{k=0}^{\infty} a_k \sin \frac{2\pi}{\ln 2} \ln \ln \frac{1}{x} + \sum_{l=0}^{\infty} b_l \sin \frac{2\pi}{\ln 2} \ln \ln \frac{1}{x}   -\frac{\ln \ln x }{\ln(2)} + \sum_{n=1}^{\infty} \left( 1 - x^{\frac{1}{2^n}} \right)  - \frac{i \pi}{\ln(2)} -  c - \sum_{k=0}^{\infty} \left( a_k \sin \frac{2\pi}{\ln 2} \ln \ln \frac{1}{x} \right) - \sum_{l=0}^{\infty} \left(  b_l \sin \frac{2\pi}{\ln 2} \ln \ln \frac{1}{x} \right)  = \sum_{n=0}^{\infty} x^{2^n}  0 < x < 1 L L = G(x) - w(x) L L(x^2) = L(x) -x  L L L(z) x \rightarrow 0 L L x=0 L f(z) L L L L L f(z)","['sequences-and-series', 'complex-analysis', 'analytic-continuation', 'lacunary-series']"
