,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Derivative of $F^TF$ with respect to $F$,Derivative of  with respect to,F^TF F,I stacked with evaluating the derivatives $$ \frac{\partial\left(F^TF\right)}{\partial F}~~ {\rm and}~~ \frac{\partial^2\left(F^TF\right)}{\partial F^2} $$ in terms of $F\in\mathbb{R}^{3\times3}$. Any hint or any idea would be acknowledged.,I stacked with evaluating the derivatives $$ \frac{\partial\left(F^TF\right)}{\partial F}~~ {\rm and}~~ \frac{\partial^2\left(F^TF\right)}{\partial F^2} $$ in terms of $F\in\mathbb{R}^{3\times3}$. Any hint or any idea would be acknowledged.,,"['matrices', 'derivatives', 'matrix-equations', 'matrix-calculus']"
1,Exact Matrix Inversion,Exact Matrix Inversion,,"I have the following matrix: $$ \left( \begin{array}{ccc} 13 & 9.1 & 8.19 & 8.281 & 8.9271\\ 9.1 & 8.19 & 8.281 & 8.9271 & 10.02001\\ 8.19 & 8.281 & 8.9271 & 10.02001 & 11.562759\\ 8.281 & 8.9271 & 10.02001 & 11.562759 & 13.6147921\\ 8.9271 & 10.02001 & 11.562759 & 13.6147921 & 16.27802631\end{array} \right)  $$ When I find the inverse of this matrix on Excel, I get: $$ \left( \begin{array}{ccc} 5.657342657 & -48.60139861 & 124.4172494 & -122.3776224 & 40.79254079\\ -48.6013986 & 467.9916897 &-1266.958328 & 1288.221582 & -438.9197404\\ 124.4172494 & -1266.958328 & 3565.198078 & -3723.319165 & 1293.334933\\ -122.3776224 & 1288.221582 & -3723.319165 & 3967.84588 & -1399.744047\\ 40.79254079 & -438.9197404 & 1293.334933 & -1399.744047 & 499.9085881\end{array} \right)  $$ These values are not exact values. I can indefinitely keep adding more and more decimal places on Excel - However,  I need exact values (in the form of a fraction). I tried converting the values in the initial matrix into fractions before inverting it (the following matrix), but had no luck: $$ \left( \begin{array}{ccc} 130/10 & 91/10 & 819/10^2 & 8281/10^3 & 89271/10^4\\ 91/10 & 819/10^2 & 8281/10^3 & 89271/10^4 & 1002001/10^5\\ 819/10^2 & 8281/10^3 & 89271/10^4 & 1002001/10^5 & 11562759/10^6\\ 8281/10^3 & 89271/10^4 & 1002001/10^5 & 11562759/10^6 & 136147921/10^7\\ 89271/10^4 & 1002001/10^5 & 11562759/10^6 & 136147921/10^7 & 1627802631/10^8\end{array}\right)  $$ How can I get an exact inverse? I'd like to do some matrix multiplication with the exact inverse, and so a method to  do that would also be much appreciated.","I have the following matrix: $$ \left( \begin{array}{ccc} 13 & 9.1 & 8.19 & 8.281 & 8.9271\\ 9.1 & 8.19 & 8.281 & 8.9271 & 10.02001\\ 8.19 & 8.281 & 8.9271 & 10.02001 & 11.562759\\ 8.281 & 8.9271 & 10.02001 & 11.562759 & 13.6147921\\ 8.9271 & 10.02001 & 11.562759 & 13.6147921 & 16.27802631\end{array} \right)  $$ When I find the inverse of this matrix on Excel, I get: $$ \left( \begin{array}{ccc} 5.657342657 & -48.60139861 & 124.4172494 & -122.3776224 & 40.79254079\\ -48.6013986 & 467.9916897 &-1266.958328 & 1288.221582 & -438.9197404\\ 124.4172494 & -1266.958328 & 3565.198078 & -3723.319165 & 1293.334933\\ -122.3776224 & 1288.221582 & -3723.319165 & 3967.84588 & -1399.744047\\ 40.79254079 & -438.9197404 & 1293.334933 & -1399.744047 & 499.9085881\end{array} \right)  $$ These values are not exact values. I can indefinitely keep adding more and more decimal places on Excel - However,  I need exact values (in the form of a fraction). I tried converting the values in the initial matrix into fractions before inverting it (the following matrix), but had no luck: $$ \left( \begin{array}{ccc} 130/10 & 91/10 & 819/10^2 & 8281/10^3 & 89271/10^4\\ 91/10 & 819/10^2 & 8281/10^3 & 89271/10^4 & 1002001/10^5\\ 819/10^2 & 8281/10^3 & 89271/10^4 & 1002001/10^5 & 11562759/10^6\\ 8281/10^3 & 89271/10^4 & 1002001/10^5 & 11562759/10^6 & 136147921/10^7\\ 89271/10^4 & 1002001/10^5 & 11562759/10^6 & 136147921/10^7 & 1627802631/10^8\end{array}\right)  $$ How can I get an exact inverse? I'd like to do some matrix multiplication with the exact inverse, and so a method to  do that would also be much appreciated.",,['matrices']
2,General Position,General Position,,"What does it mean when the columns of a matrix are in general position? I do not know if this is relevant or not, but the matrix in question is under-determined.","What does it mean when the columns of a matrix are in general position? I do not know if this is relevant or not, but the matrix in question is under-determined.",,"['linear-algebra', 'matrices']"
3,Showing that $I+tA$ is invertible using $f(t)=\det(I+tA)$ [closed],Showing that  is invertible using  [closed],I+tA f(t)=\det(I+tA),"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $A \in \mathbb{R}^{n\times n}$ a square matrix with $\text{trace}(A) \neq 0$. I would like to show that $f : \mathbb{R} \to \mathbb{R}$, $$f(t)=\det(I+tA)$$ local while $t=0$ is invertible.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $A \in \mathbb{R}^{n\times n}$ a square matrix with $\text{trace}(A) \neq 0$. I would like to show that $f : \mathbb{R} \to \mathbb{R}$, $$f(t)=\det(I+tA)$$ local while $t=0$ is invertible.",,"['matrices', 'analysis']"
4,matrix multiplied by rotation matrix on right side and transpose(rotation) on left side,matrix multiplied by rotation matrix on right side and transpose(rotation) on left side,,Would a matrix remain un-rotated if it is multiplied by an orthonormal rotation matrix on right side and transpose of same rotation matrix on the left side?,Would a matrix remain un-rotated if it is multiplied by an orthonormal rotation matrix on right side and transpose of same rotation matrix on the left side?,,"['linear-algebra', 'matrices', 'rotations']"
5,Geometric interpretation of a hollow symmetrical 3D matrix,Geometric interpretation of a hollow symmetrical 3D matrix,,"Any matrix $A$ can be presented as a sum of its symmetrical and skew-symmetrical part: $A=sym(A)+skew(A)$. Decomposition can go further and we can present symmetrical part as a sum of some diagonal matrix and hollow (zeros on diagonal) symmetrical matrix. Now we have (used below notation is only for the needs of this question) $A= diag(A)+hols(A)+ skew(A)$. Let's assume the dimension of $A$ is $3$. In this case we have the sum of   $3$ components where every component has $3$ DOF (decoded in three 3D vectors). We can also present these components in the form $A= k_d{diag_n}(A )+k_h{hols_n}(A )+ k_s{skew_n}(A )$, where coefficients $k_d, k_h, k_s$ are  calculated so to assure that vectors which represent DOF of components are unit vectors , index $n$ denotes here this kind of ""normalization""  (so in these unit vectors we have decoded 2 degrees of freedom and additional DOF is in the appropriate scaling coefficient $k_{\{d,h,s)\}}$) Example of such decomposition: $\begin{bmatrix}   1 & 3 & 5 \\ 1 & 4 & 4 \\ 1 & 8 & 8 \end{bmatrix}$ =  9$\begin{bmatrix}   \dfrac{1}{9} & 0 & 0 \\ 0 & \dfrac{4}{9} & 0 \\ 0 &0 & \dfrac{8}{9} \end{bmatrix}$+ 7$\begin{bmatrix}   0 & \dfrac{2}{7} & \dfrac{3}{7} \\ \dfrac{2}{7} & 0 & \dfrac{6}{7} \\ \dfrac{3}{7} & \dfrac{6}{7}  & 0  \end{bmatrix}$+ 3$\begin{bmatrix}   0 & \dfrac{1}{3} & \dfrac{2}{3} \\ -\dfrac{1}{3} & 0 & -\dfrac{2}{3} \\ -\dfrac{2}{3} & \dfrac{2}{3} & 0 \end{bmatrix}$ Two of components have distinct geometric interpretation: $diag(A)$ is simply a scaling matrix and $skew(A)$ is a scaled composition of a projection and  rotation by $\pi/2$ (See question*) . However the geometric interpretation for the $hols_n(A)$ is unknown to me.. In the general case matrix  $hols(A)$ has a form $\begin{bmatrix}   0 & a & b \\ a & 0 & c \\ b & c & 0 \end{bmatrix}$ Some properties of this matrix    maybe  provide some information about its nature: it has real eigenvalues (as a special case of symmetric matrix) so rotation probably can't be engaged for this interpretation (unless it is by $\pi$) determinant of this matrix  $det(hols(A))= 2abc$ so its rank $=3$      if $ a,b,c \neq 0$. If one of $a,b,c=$ $0$ its rank decreases ( unlike $skew_n(A)$ where zeroing a single entry (and its skew-symmetrical one) doesn't change geometric interpretation - only axis of rotation which determines at the same time the direction of projection ). few others  are in Wikipedia . So my question is: Could   hollow symmetrical matrix $hols_n(A)$ be decomposed in such a way that geometric    interpretation of $hols_n(A)$ would be explicit ? If geometric interpretation of the hollow matrix is too difficult one can propose other method for decomposition of symmetrical part $sym(A)$ only on condition that both parts should have 3 DOF and each part should have explicit geometric interpretation..","Any matrix $A$ can be presented as a sum of its symmetrical and skew-symmetrical part: $A=sym(A)+skew(A)$. Decomposition can go further and we can present symmetrical part as a sum of some diagonal matrix and hollow (zeros on diagonal) symmetrical matrix. Now we have (used below notation is only for the needs of this question) $A= diag(A)+hols(A)+ skew(A)$. Let's assume the dimension of $A$ is $3$. In this case we have the sum of   $3$ components where every component has $3$ DOF (decoded in three 3D vectors). We can also present these components in the form $A= k_d{diag_n}(A )+k_h{hols_n}(A )+ k_s{skew_n}(A )$, where coefficients $k_d, k_h, k_s$ are  calculated so to assure that vectors which represent DOF of components are unit vectors , index $n$ denotes here this kind of ""normalization""  (so in these unit vectors we have decoded 2 degrees of freedom and additional DOF is in the appropriate scaling coefficient $k_{\{d,h,s)\}}$) Example of such decomposition: $\begin{bmatrix}   1 & 3 & 5 \\ 1 & 4 & 4 \\ 1 & 8 & 8 \end{bmatrix}$ =  9$\begin{bmatrix}   \dfrac{1}{9} & 0 & 0 \\ 0 & \dfrac{4}{9} & 0 \\ 0 &0 & \dfrac{8}{9} \end{bmatrix}$+ 7$\begin{bmatrix}   0 & \dfrac{2}{7} & \dfrac{3}{7} \\ \dfrac{2}{7} & 0 & \dfrac{6}{7} \\ \dfrac{3}{7} & \dfrac{6}{7}  & 0  \end{bmatrix}$+ 3$\begin{bmatrix}   0 & \dfrac{1}{3} & \dfrac{2}{3} \\ -\dfrac{1}{3} & 0 & -\dfrac{2}{3} \\ -\dfrac{2}{3} & \dfrac{2}{3} & 0 \end{bmatrix}$ Two of components have distinct geometric interpretation: $diag(A)$ is simply a scaling matrix and $skew(A)$ is a scaled composition of a projection and  rotation by $\pi/2$ (See question*) . However the geometric interpretation for the $hols_n(A)$ is unknown to me.. In the general case matrix  $hols(A)$ has a form $\begin{bmatrix}   0 & a & b \\ a & 0 & c \\ b & c & 0 \end{bmatrix}$ Some properties of this matrix    maybe  provide some information about its nature: it has real eigenvalues (as a special case of symmetric matrix) so rotation probably can't be engaged for this interpretation (unless it is by $\pi$) determinant of this matrix  $det(hols(A))= 2abc$ so its rank $=3$      if $ a,b,c \neq 0$. If one of $a,b,c=$ $0$ its rank decreases ( unlike $skew_n(A)$ where zeroing a single entry (and its skew-symmetrical one) doesn't change geometric interpretation - only axis of rotation which determines at the same time the direction of projection ). few others  are in Wikipedia . So my question is: Could   hollow symmetrical matrix $hols_n(A)$ be decomposed in such a way that geometric    interpretation of $hols_n(A)$ would be explicit ? If geometric interpretation of the hollow matrix is too difficult one can propose other method for decomposition of symmetrical part $sym(A)$ only on condition that both parts should have 3 DOF and each part should have explicit geometric interpretation..",,"['linear-algebra', 'matrices']"
6,$R[X]/(f)$ separable $\iff \Delta(f) \in R^*$?,separable ?,R[X]/(f) \iff \Delta(f) \in R^*,"Let $R$ be a commutative ring with $1$. Let $f \in R[X]$ be monic. I have to prove the following: $$ \text{The discriminant of } f \text{ is invertible} \quad \quad \iff \quad \quad R[X] / (f) \text{ a is separable algebra over R} $$ Could you please help me to do so? The definitions I work with For me the discriminant of a polynomial $f$ is understood to be $$ \Delta(f) \ := \ \prod_{1 \leq i < j \leq n} (\alpha_i - \alpha_j)^2.   $$ We call an algebra $B$ separable iff the following map is bijective: $$ \Phi \ : \ B \rightarrow Hom(B,R) \ : \  x \ \longmapsto \ \left(y \mapsto Tr(xy)\right) $$ where the trace of an element $b \in B$ is defined as the trace of the map $ x \mapsto bx$. My own thoughts We tried to use another fact about separability of algebras from this source : Let $R$ be a commutative ring with $1$, B an $R$-algebra with basis    $\{w_1, w_2, \dots, w_n \}$, then    $$ \det(Tr(w_iw_j):1 \leq i,j \leq n ) \text{ is invertible} \quad \quad \iff \quad \quad \text{ B is a separable algebra over $R$} $$   Where $Tr(a)$ is a notation for the trace of the map   $$ B \ \longrightarrow \ B \ : \ x \ \longmapsto \ ax $$ Thus it would make sense to try to prove that $\Delta(f) = (Tr(w_iw_j):i,j)$, but we only managed to prove this for polynomials of degree two. In the general case both sides are very hard to calculate Some computational effort Lets write $f(X) = a_0 + a_1X + \dots + a_nX^n$.  In our case $\{1,X,X^2,\dots, X^{n-1}\}$ is a basis for $A[X]/(f)$. The determinant we have to calculate looks like if $n=3$: $$  \left| \begin{array}         Tr(1) & Tr(X) & Tr(X^2) \\         Tr(X) & Tr(X^2) & Tr(X^3) \\         Tr(X^2) & Tr(X^3) & Tr(X^4) \\         \end{array} \right| $$ and is similar for other $n \in \mathbb{N}$. Below I ""sketched"" what the matrix of a map $f(X) \mapsto X^j f(X)$ looks like. $$ \left[     \begin{array}{cccc|cccc}       0& 0& \dots &0 \\       \vdots& \vdots & & \vdots \\       0& 0& \dots &0 \\       \hline       1 &0 &\dots &0 & \\       0 &1 &\dots &0 && \\       \vdots & \vdots &  \\       0 &0 & \dots &1  &&&&  \\     \end{array} \right] $$ The empty parts are hard to spell out. At least we can see this way that for taking the trace we only need te consider the elements on the right. This is still hard, though. The discriminant isn't easy to calculate either. Could you please help me to solve this? Feel free to use any method you like.","Let $R$ be a commutative ring with $1$. Let $f \in R[X]$ be monic. I have to prove the following: $$ \text{The discriminant of } f \text{ is invertible} \quad \quad \iff \quad \quad R[X] / (f) \text{ a is separable algebra over R} $$ Could you please help me to do so? The definitions I work with For me the discriminant of a polynomial $f$ is understood to be $$ \Delta(f) \ := \ \prod_{1 \leq i < j \leq n} (\alpha_i - \alpha_j)^2.   $$ We call an algebra $B$ separable iff the following map is bijective: $$ \Phi \ : \ B \rightarrow Hom(B,R) \ : \  x \ \longmapsto \ \left(y \mapsto Tr(xy)\right) $$ where the trace of an element $b \in B$ is defined as the trace of the map $ x \mapsto bx$. My own thoughts We tried to use another fact about separability of algebras from this source : Let $R$ be a commutative ring with $1$, B an $R$-algebra with basis    $\{w_1, w_2, \dots, w_n \}$, then    $$ \det(Tr(w_iw_j):1 \leq i,j \leq n ) \text{ is invertible} \quad \quad \iff \quad \quad \text{ B is a separable algebra over $R$} $$   Where $Tr(a)$ is a notation for the trace of the map   $$ B \ \longrightarrow \ B \ : \ x \ \longmapsto \ ax $$ Thus it would make sense to try to prove that $\Delta(f) = (Tr(w_iw_j):i,j)$, but we only managed to prove this for polynomials of degree two. In the general case both sides are very hard to calculate Some computational effort Lets write $f(X) = a_0 + a_1X + \dots + a_nX^n$.  In our case $\{1,X,X^2,\dots, X^{n-1}\}$ is a basis for $A[X]/(f)$. The determinant we have to calculate looks like if $n=3$: $$  \left| \begin{array}         Tr(1) & Tr(X) & Tr(X^2) \\         Tr(X) & Tr(X^2) & Tr(X^3) \\         Tr(X^2) & Tr(X^3) & Tr(X^4) \\         \end{array} \right| $$ and is similar for other $n \in \mathbb{N}$. Below I ""sketched"" what the matrix of a map $f(X) \mapsto X^j f(X)$ looks like. $$ \left[     \begin{array}{cccc|cccc}       0& 0& \dots &0 \\       \vdots& \vdots & & \vdots \\       0& 0& \dots &0 \\       \hline       1 &0 &\dots &0 & \\       0 &1 &\dots &0 && \\       \vdots & \vdots &  \\       0 &0 & \dots &1  &&&&  \\     \end{array} \right] $$ The empty parts are hard to spell out. At least we can see this way that for taking the trace we only need te consider the elements on the right. This is still hard, though. The discriminant isn't easy to calculate either. Could you please help me to solve this? Feel free to use any method you like.",,"['matrices', 'polynomials', 'algebras']"
7,derivative transpose,derivative transpose,,"I'm reading the book ""The Elements of Statistical Learning - Data Mining, Inference, and Prediction"" chapter 3 and there comes a simple derivation that I don't understand: We have:                   $$(1): RSS(\beta) = (y-X\beta)^T(y-X\beta)$$ with input X is N × (p + 1) matrix, (p+1) column is 1, $X_1$,$X_2$, ... $X_p$; y is N-vector output. The question is when differentiating with  respect to $\beta$ why we obtain this ? $$(2):  \frac{\partial RSS}{\partial \beta} = -2X^T(y-X\beta)  $$  An answer that explain how to differentiate RSS with respect to $\beta$ will be highly appreciated","I'm reading the book ""The Elements of Statistical Learning - Data Mining, Inference, and Prediction"" chapter 3 and there comes a simple derivation that I don't understand: We have:                   $$(1): RSS(\beta) = (y-X\beta)^T(y-X\beta)$$ with input X is N × (p + 1) matrix, (p+1) column is 1, $X_1$,$X_2$, ... $X_p$; y is N-vector output. The question is when differentiating with  respect to $\beta$ why we obtain this ? $$(2):  \frac{\partial RSS}{\partial \beta} = -2X^T(y-X\beta)  $$  An answer that explain how to differentiate RSS with respect to $\beta$ will be highly appreciated",,"['matrices', 'derivatives', 'linear-regression', 'transpose']"
8,What kind of $n^{th}$ order polynomials are solvable by a square matrix with integer entries?,What kind of  order polynomials are solvable by a square matrix with integer entries?,n^{th},"Consider a polynomial (monic for simplicity): $$x^n+a_1x^{n-1}+\dots+a_{n-1}x+a_n=0$$ Here we assume the roots are complex numbers. $a_k$ are integers. Now consider the corresponding matrix polynomial: $$X^n+a_1X^{n-1}+\dots+a_{n-1}X+a_nI_m=0$$ Here $X$ is an $m \times m$ matrix, and $I_m$ - $m \times m$ identity matrix. $a_k$ are still integers. What kind of polynomials can be solved in matrices with integer entries? Obviously, any $X$ should satisfy at least its characteristic polynomial equation by Cayley–Hamilton theorem, as well as its minimal polynomial. But I don't know if every polynomial with integer coefficients is also a characteristic or minimal polynomial of some matrix with integer entries.","Consider a polynomial (monic for simplicity): $$x^n+a_1x^{n-1}+\dots+a_{n-1}x+a_n=0$$ Here we assume the roots are complex numbers. $a_k$ are integers. Now consider the corresponding matrix polynomial: $$X^n+a_1X^{n-1}+\dots+a_{n-1}X+a_nI_m=0$$ Here $X$ is an $m \times m$ matrix, and $I_m$ - $m \times m$ identity matrix. $a_k$ are still integers. What kind of polynomials can be solved in matrices with integer entries? Obviously, any $X$ should satisfy at least its characteristic polynomial equation by Cayley–Hamilton theorem, as well as its minimal polynomial. But I don't know if every polynomial with integer coefficients is also a characteristic or minimal polynomial of some matrix with integer entries.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'polynomials']"
9,A matrix being symmetric/orthogonal/projection matrix/stochastic matrix,A matrix being symmetric/orthogonal/projection matrix/stochastic matrix,,"I am trying to do some practice questions and wanted to check the following properties and confirm my definition of projection matrix: Let $$A = \left[\begin{matrix} 1/2 & 0 & 1/2 \\ 0 & 1 & 0 \\ 1/2 & 0 & 1/2 \end{matrix}\right]$$ Is the following true or false ? i) $A$ is orthogonal ii) $A$ is a projection matrix I think that $A$ is not orthogonal since $AA^T \neq I$ but is a projection matrix given that a projection matrix is defined as $P^2 = P$ . Also accordingly, a projection matrix is orthogonal iff $P^T = P$ and in that case, is an orthogonal projection matrix different from the idea of an orthogonal matrix itself ? It appears that $A^T = A$ but yet $AA^T \neq A$ . Would someone mind helping me fill up the gap for this misconception, any advice is appreciated. Thank you","I am trying to do some practice questions and wanted to check the following properties and confirm my definition of projection matrix: Let Is the following true or false ? i) is orthogonal ii) is a projection matrix I think that is not orthogonal since but is a projection matrix given that a projection matrix is defined as . Also accordingly, a projection matrix is orthogonal iff and in that case, is an orthogonal projection matrix different from the idea of an orthogonal matrix itself ? It appears that but yet . Would someone mind helping me fill up the gap for this misconception, any advice is appreciated. Thank you",A = \left[\begin{matrix} 1/2 & 0 & 1/2 \\ 0 & 1 & 0 \\ 1/2 & 0 & 1/2 \end{matrix}\right] A A A AA^T \neq I P^2 = P P^T = P A^T = A AA^T \neq A,"['linear-algebra', 'matrices']"
10,Positive definite if and only if determinants are positive,Positive definite if and only if determinants are positive,,"Suppose $\mathbf{A} = [a_{ij}]$ is a $n \times n$ matrix. I have read that $\mathbf{A}$ is positive-definite if and only if  $$\begin{vmatrix} a_{11} \end{vmatrix} > 0\text{, }\begin{vmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{vmatrix} > 0\text{, }\begin{vmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33} \end{vmatrix} > 0\text{, } \dots\text{,  etc}.$$ I'd like to prove this statement. Let $\mathbf{x} \in \mathbb{R}^n \setminus \{\mathbf{0}\}$. Perform induction on $n$. If $n = 1$, this is trivial: $a_{11}\|\mathbf{x}\|^2 > 0 \Longleftrightarrow a_{11} > 0$. Suppose this is true for $n = k$. This is where it gets particularly difficult: how do you consider every upper left submatrix (as above) of $\mathbf{A}$? Please avoid using the singular value decomposition or eigendecomposition.","Suppose $\mathbf{A} = [a_{ij}]$ is a $n \times n$ matrix. I have read that $\mathbf{A}$ is positive-definite if and only if  $$\begin{vmatrix} a_{11} \end{vmatrix} > 0\text{, }\begin{vmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{vmatrix} > 0\text{, }\begin{vmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33} \end{vmatrix} > 0\text{, } \dots\text{,  etc}.$$ I'd like to prove this statement. Let $\mathbf{x} \in \mathbb{R}^n \setminus \{\mathbf{0}\}$. Perform induction on $n$. If $n = 1$, this is trivial: $a_{11}\|\mathbf{x}\|^2 > 0 \Longleftrightarrow a_{11} > 0$. Suppose this is true for $n = k$. This is where it gets particularly difficult: how do you consider every upper left submatrix (as above) of $\mathbf{A}$? Please avoid using the singular value decomposition or eigendecomposition.",,"['linear-algebra', 'matrices']"
11,"Is $\operatorname{Mat}(2,\mathbb{R})$ isomorphic to $\operatorname{Mat}(3,\mathbb{R})$?",Is  isomorphic to ?,"\operatorname{Mat}(2,\mathbb{R}) \operatorname{Mat}(3,\mathbb{R})","Let $\operatorname{Mat}(n,\mathbb{K})$ be the ring of $n\times n$ matrices with entries in a field $\mathbb{K}$ .   Is $\operatorname{Mat}(2,\mathbb{R})$ isomorphic to $\operatorname{Mat}(3,\mathbb{R})$ ? I want to know whether there is ring isomorphism between the two. I cannot seem to find any bijection that will do. Thanks. An Idea. Let $f:\text{Mat}(2,\mathbb{R})\to\text{Mat}(3,\mathbb{R})$ be a ring homomorphism.  Then, $\ker f$ is a two-sided ideal of $\text{Mat}(2,\mathbb{R})$ .  It is known that $\text{Mat}(2,\mathbb{R})$ is a simple ring.  Therefore, either $\ker f=0$ or $\ker f=\text{Mat}(2,\mathbb{R})$ .  In the case $\ker f=\text{Mat}(2,\mathbb{R})$ , it follows that $f=0$ , so it is not an isomorphism.  If $\ker f=0$ , then $f$ is an embedding.  This is where the argument stops.  There are many ways to embed $\text{Mat}(2,\mathbb{R})$ into $\text{Mat}(3,\mathbb{R})$ .  But is there an embedding that is a surjection, i.e., it is an isomorphism?","Let be the ring of matrices with entries in a field .   Is isomorphic to ? I want to know whether there is ring isomorphism between the two. I cannot seem to find any bijection that will do. Thanks. An Idea. Let be a ring homomorphism.  Then, is a two-sided ideal of .  It is known that is a simple ring.  Therefore, either or .  In the case , it follows that , so it is not an isomorphism.  If , then is an embedding.  This is where the argument stops.  There are many ways to embed into .  But is there an embedding that is a surjection, i.e., it is an isomorphism?","\operatorname{Mat}(n,\mathbb{K}) n\times n \mathbb{K} \operatorname{Mat}(2,\mathbb{R}) \operatorname{Mat}(3,\mathbb{R}) f:\text{Mat}(2,\mathbb{R})\to\text{Mat}(3,\mathbb{R}) \ker f \text{Mat}(2,\mathbb{R}) \text{Mat}(2,\mathbb{R}) \ker f=0 \ker f=\text{Mat}(2,\mathbb{R}) \ker f=\text{Mat}(2,\mathbb{R}) f=0 \ker f=0 f \text{Mat}(2,\mathbb{R}) \text{Mat}(3,\mathbb{R})","['linear-algebra', 'abstract-algebra']"
12,Idempotent and nilpotent matrices are defined differently. Why?,Idempotent and nilpotent matrices are defined differently. Why?,,We call $A$ idempotent if $A^2$ is $A$.  But we call A nilpotent if $A^k$ is $0$ for some integer $k$.  Why are not they defined uniformly like both with power 2 or both with power some integer $k$.,We call $A$ idempotent if $A^2$ is $A$.  But we call A nilpotent if $A^k$ is $0$ for some integer $k$.  Why are not they defined uniformly like both with power 2 or both with power some integer $k$.,,['matrices']
13,Question about idempotent matrices. [closed],Question about idempotent matrices. [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Let $E$ be the $m \times m$ matrix that extracts the ""even part"" of an $m$-vector $Ex = (x+Fx)/2$, where $F$ is the $m\times m$ matrix that flips $[x_1,\dotsc ,x_m]^{T}$ to $[x_m,\dotsc ,x_1]^T$. Is $E$ idempotent?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Let $E$ be the $m \times m$ matrix that extracts the ""even part"" of an $m$-vector $Ex = (x+Fx)/2$, where $F$ is the $m\times m$ matrix that flips $[x_1,\dotsc ,x_m]^{T}$ to $[x_m,\dotsc ,x_1]^T$. Is $E$ idempotent?",,"['matrices', 'idempotents']"
14,"SCALAR MATRIX, Matrices","SCALAR MATRIX, Matrices",,"I have read two definitions of scalar matrix. The first one is that a square  matrix whose principal diagonal elements are some nonzero scalar is called scalar matrix. But the second is slightly different which says a square matrix whose  diagonal elements all contain the same scalar. In the second definition, it has not been mentioned that the scalar is nonzero. Which one is correct? Please clear my confusion.","I have read two definitions of scalar matrix. The first one is that a square  matrix whose principal diagonal elements are some nonzero scalar is called scalar matrix. But the second is slightly different which says a square matrix whose  diagonal elements all contain the same scalar. In the second definition, it has not been mentioned that the scalar is nonzero. Which one is correct? Please clear my confusion.",,['matrices']
15,(Easier) way to determine determinant of a almost-circulant matrix,(Easier) way to determine determinant of a almost-circulant matrix,,"Consider the matrix: $$\begin{pmatrix} \color{red}{-x+y} &-1&0&-1 \\ -1&\color{blue}{x+y}&-1&0  \\ 0&-1&\color{red}{-x+y}&-1 \\ -1&0&-1&\color{blue}{x+y} \end{pmatrix}$$ Is there any easy way to calculate the determinant of this matrix? I have read up on circulant matrices and there is some nice theory on them, however this is not quite circulant, the only difference is that the main diagonal contains 2 different values. So I was wondering if there is a 'short cut' to find the determinant of these kind of almost -circulant matrices.","Consider the matrix: Is there any easy way to calculate the determinant of this matrix? I have read up on circulant matrices and there is some nice theory on them, however this is not quite circulant, the only difference is that the main diagonal contains 2 different values. So I was wondering if there is a 'short cut' to find the determinant of these kind of almost -circulant matrices.","\begin{pmatrix} \color{red}{-x+y} &-1&0&-1 \\ -1&\color{blue}{x+y}&-1&0 
\\ 0&-1&\color{red}{-x+y}&-1 \\ -1&0&-1&\color{blue}{x+y} \end{pmatrix}","['linear-algebra', 'matrices', 'determinant']"
16,Inverse of $A$ for valid $a$,Inverse of  for valid,A a,"$$\begin{pmatrix} 1 &0 &1 \\ a& 0 &1 \\ 1& a &0 \end{pmatrix}$$ I am asked to find the values of a where my matrix A is invertible. It is invertible when $\det(A) \neq 0$, and that is when $a^{2} - a \neq 0$ or in other words when $a(a-1) \neq 0$ I have now found the valid values of a ($a \neq 0$ and $a \neq 1$), but now I am asked to show $A^{-1}$ for all valid $a$. The key says it will be represented as: $$A^{-1} = \frac{1}{a(a-1)}\begin{pmatrix} -a & a &0 \\ 1& -1 &a-1 \\ a^2 &-a &0 \end{pmatrix}$$ But show no steps on how to reach that conclusion. If anyone could shed any light on how I do to represent $A^{-1}$ when after I found the valid values for $a$.","$$\begin{pmatrix} 1 &0 &1 \\ a& 0 &1 \\ 1& a &0 \end{pmatrix}$$ I am asked to find the values of a where my matrix A is invertible. It is invertible when $\det(A) \neq 0$, and that is when $a^{2} - a \neq 0$ or in other words when $a(a-1) \neq 0$ I have now found the valid values of a ($a \neq 0$ and $a \neq 1$), but now I am asked to show $A^{-1}$ for all valid $a$. The key says it will be represented as: $$A^{-1} = \frac{1}{a(a-1)}\begin{pmatrix} -a & a &0 \\ 1& -1 &a-1 \\ a^2 &-a &0 \end{pmatrix}$$ But show no steps on how to reach that conclusion. If anyone could shed any light on how I do to represent $A^{-1}$ when after I found the valid values for $a$.",,"['linear-algebra', 'matrices']"
17,Finding powers of matrices that cannot be diagonalised,Finding powers of matrices that cannot be diagonalised,,"In lessons, we have been taught to find a general solution finding a matrix to the power n by diagonalising it but for an assignment have been asked to find a general formula for: $$\begin{bmatrix}2 & 1\\0 & 2\end{bmatrix} ^ n$$ By expending it by hand for the first few terms, I found this to be: $$\begin{bmatrix}2^n & u_n\\0 & 2^n\end{bmatrix}$$ where $u_n = 2u_{n-1} + 2^{n-1}$ How would I convert this to a general formula and is there a better way to solve this kind of question as several similar ones were set.","In lessons, we have been taught to find a general solution finding a matrix to the power n by diagonalising it but for an assignment have been asked to find a general formula for: $$\begin{bmatrix}2 & 1\\0 & 2\end{bmatrix} ^ n$$ By expending it by hand for the first few terms, I found this to be: $$\begin{bmatrix}2^n & u_n\\0 & 2^n\end{bmatrix}$$ where $u_n = 2u_{n-1} + 2^{n-1}$ How would I convert this to a general formula and is there a better way to solve this kind of question as several similar ones were set.",,['matrices']
18,"Given a $n \times n$ matrix $P$, where $P^2=P$... What is wrong with my logic here?","Given a  matrix , where ... What is wrong with my logic here?",n \times n P P^2=P,"I have managed to 'prove' something incorrect and am trying to figure out where my logic is going wrong.  I'm pretty new to matrices so still getting to understand them. Given an $n \times n$ matrix $P$ where $P^2=P$ and $I$ is the identity matrix: $$P^2=P$$ $$PP=P$$ $$P^{-1}PP=P^{-1}P$$ $$IP=I$$ $$P=I$$ This seems to show that if $P^2=P$ then $P$ must be the identity.  While this case is true, other forms of $P$ also work.  For example: $\begin{bmatrix} 1&0\\ 1&0\\ \end{bmatrix}$ I feel like I am breaking a fundamental rule (or just making a really silly mistake). Any help is appreciated.","I have managed to 'prove' something incorrect and am trying to figure out where my logic is going wrong.  I'm pretty new to matrices so still getting to understand them. Given an $n \times n$ matrix $P$ where $P^2=P$ and $I$ is the identity matrix: $$P^2=P$$ $$PP=P$$ $$P^{-1}PP=P^{-1}P$$ $$IP=I$$ $$P=I$$ This seems to show that if $P^2=P$ then $P$ must be the identity.  While this case is true, other forms of $P$ also work.  For example: $\begin{bmatrix} 1&0\\ 1&0\\ \end{bmatrix}$ I feel like I am breaking a fundamental rule (or just making a really silly mistake). Any help is appreciated.",,"['linear-algebra', 'matrices']"
19,Rewriting weighted least squares equation in matrix form,Rewriting weighted least squares equation in matrix form,,"I'm trying to rewrite a weighted least squares problem in matrix form for my research. Let $X$ be an $n \times d$ data matrix, let $Y$ be the corresponding $n \times 1$ target vector, and let $\Lambda$ be a diagonal matrix with weights $\lambda_1, \dots, \lambda_n$ on its diagonal. Let $w$ be the $d \times 1$ weights vector that we wish to find. The risk function is given by $$ \sum_{i=1}^n \lambda_i (w^Tx_i - y_i)^2 $$ where $x_i$ is the $i^{th}$ row of $X$. I'm trying to rewrite this more compactly using matrices (i.e. just in terms of the matrices $\Lambda, X, Y$ and the weight vector $w$. I'm having some issues doing this because I can't figure out how to deal with the squared term. Could someone show me how to do this?","I'm trying to rewrite a weighted least squares problem in matrix form for my research. Let $X$ be an $n \times d$ data matrix, let $Y$ be the corresponding $n \times 1$ target vector, and let $\Lambda$ be a diagonal matrix with weights $\lambda_1, \dots, \lambda_n$ on its diagonal. Let $w$ be the $d \times 1$ weights vector that we wish to find. The risk function is given by $$ \sum_{i=1}^n \lambda_i (w^Tx_i - y_i)^2 $$ where $x_i$ is the $i^{th}$ row of $X$. I'm trying to rewrite this more compactly using matrices (i.e. just in terms of the matrices $\Lambda, X, Y$ and the weight vector $w$. I'm having some issues doing this because I can't figure out how to deal with the squared term. Could someone show me how to do this?",,"['linear-algebra', 'matrices', 'least-squares']"
20,"$A \in SO(3,\mathbb R)\setminus\{I\}$ , then there are exactly two points in $S^2:=\{(x,y,z)\in \mathbb R^3:x^2+y^2+z^2=1\}$ which are fixed by $A$?",", then there are exactly two points in  which are fixed by ?","A \in SO(3,\mathbb R)\setminus\{I\} S^2:=\{(x,y,z)\in \mathbb R^3:x^2+y^2+z^2=1\} A","Let $A \in SO(3,\mathbb R)\setminus\{I\}$ , then is it true that there exist exactly two points in $$S^2:=\{(x,y,z)\in \mathbb R^3:x^2+y^2+z^2=1\}$$ which are fixed by $A$? Or equivalently we have to show that $1$ is an eigen-value of $A$ and there are exactly two distinct eigenvectors corresponding to it. Now I can show that $\det (A-I)=0 $ and $Ax=x$ implies $A\left(\dfrac x{||x||}\right)=\dfrac x{||x||} $ and $A \left(-\dfrac{x}{||x||}\right)=-\dfrac {x}{||x||}$ also; but how can I show that these are only two fixed points of $A$ on the unit sphere? Please help. Thanks in advance","Let $A \in SO(3,\mathbb R)\setminus\{I\}$ , then is it true that there exist exactly two points in $$S^2:=\{(x,y,z)\in \mathbb R^3:x^2+y^2+z^2=1\}$$ which are fixed by $A$? Or equivalently we have to show that $1$ is an eigen-value of $A$ and there are exactly two distinct eigenvectors corresponding to it. Now I can show that $\det (A-I)=0 $ and $Ax=x$ implies $A\left(\dfrac x{||x||}\right)=\dfrac x{||x||} $ and $A \left(-\dfrac{x}{||x||}\right)=-\dfrac {x}{||x||}$ also; but how can I show that these are only two fixed points of $A$ on the unit sphere? Please help. Thanks in advance",,"['linear-algebra', 'matrices']"
21,"Characteristic polynomial of $A^2$, given the characteristic polynomial of $A$","Characteristic polynomial of , given the characteristic polynomial of",A^2 A,"Let $A \in M_{3\times 3}(\mathbb{R})$ . Its characteristic polynomial is $P_A(t) = t^3+t^2+t-3$ . Find the coefficient of the characteristic polynomial of $A^2$ . I tried to solve it by finding the factors of A, but it looks like it doesn't have factors in the real numbers $\mathbb{R}$ only above $\mathbb{C}$ .","Let . Its characteristic polynomial is . Find the coefficient of the characteristic polynomial of . I tried to solve it by finding the factors of A, but it looks like it doesn't have factors in the real numbers only above .",A \in M_{3\times 3}(\mathbb{R}) P_A(t) = t^3+t^2+t-3 A^2 \mathbb{R} \mathbb{C},"['linear-algebra', 'matrices']"
22,Prove a matrix is a generalized permutation matrix,Prove a matrix is a generalized permutation matrix,,"A generalized permutation matrix is a matrix in which each row and each column contains exactly one nonzero entry. I am trying to prove the following: If a non-singular matrix and its inverse are both non-negative matrices (i.e. matrices with non-negative entries), then the matrix is a generalized permutation matrix. So I need to prove that there exists a diagonal matrix $D$ with positive diagonal entries and a permutation matrix $P$ such that $A = DP$. I don't know how to start proving this, and I don't get how to use the non-negativity of $A^{-1}$. Please help me.","A generalized permutation matrix is a matrix in which each row and each column contains exactly one nonzero entry. I am trying to prove the following: If a non-singular matrix and its inverse are both non-negative matrices (i.e. matrices with non-negative entries), then the matrix is a generalized permutation matrix. So I need to prove that there exists a diagonal matrix $D$ with positive diagonal entries and a permutation matrix $P$ such that $A = DP$. I don't know how to start proving this, and I don't get how to use the non-negativity of $A^{-1}$. Please help me.",,"['linear-algebra', 'matrices']"
23,How to check the correctness of calculated eigenvalues?,How to check the correctness of calculated eigenvalues?,,Let's say you are given the following easy matrix: $$\begin{bmatrix}{-1}&{0}\\ {1}&{1}\end{bmatrix}$$ and you've calculated the following two eigenvalues: $\lambda_{1} = -1$ $ \lambda_{2} = 1$ Is there a way to check whether the calculated eigenvalues are correct? I know that with the eigenvectors you can just check everything all at once by checking it with this formula: $$A. \vec{x} = \lambda \vec{x}$$ But how can you check the correctness of the computed results without having to calculate all the eigenvectors and fill in the formula?,Let's say you are given the following easy matrix: $$\begin{bmatrix}{-1}&{0}\\ {1}&{1}\end{bmatrix}$$ and you've calculated the following two eigenvalues: $\lambda_{1} = -1$ $ \lambda_{2} = 1$ Is there a way to check whether the calculated eigenvalues are correct? I know that with the eigenvectors you can just check everything all at once by checking it with this formula: $$A. \vec{x} = \lambda \vec{x}$$ But how can you check the correctness of the computed results without having to calculate all the eigenvectors and fill in the formula?,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
24,"Hoffman Exercise, Linear Algebra","Hoffman Exercise, Linear Algebra",,How to prove this exercise? Let $A$ be a $n \times n$ diagonal matrix with characterist polynominal $$(x-c_1)^{d_1}...(x-c_k)^{d_k}$$ where $c_i$ are distinct. Let $V$ be the space of $n \times n$  matrixes $B$ such that $AB=BA$. Prove that dimension of $V$ is $d_{1}^{2}+...+d_{k}^{2}$.,How to prove this exercise? Let $A$ be a $n \times n$ diagonal matrix with characterist polynominal $$(x-c_1)^{d_1}...(x-c_k)^{d_k}$$ where $c_i$ are distinct. Let $V$ be the space of $n \times n$  matrixes $B$ such that $AB=BA$. Prove that dimension of $V$ is $d_{1}^{2}+...+d_{k}^{2}$.,,"['linear-algebra', 'matrices', 'linear-transformations']"
25,Prove that the determinant is $0$ by expressing as a product,Prove that the determinant is  by expressing as a product,0,I need to prove that the determinant $$\begin{vmatrix} my+nz & mq-nr &  mb+nc \\ kz-mx & kr-mp & kb-ma \\ nx+ky & np+kq & na+kb  \end{vmatrix}=0$$ In my book it is given as hint that the determinant can be expressed as a product of two other determinants whose value will evaluate to $0$.But I'm not being able to express the given determinant as a product of two other determinants.How should I do it?Please guide me through the procedure.,I need to prove that the determinant $$\begin{vmatrix} my+nz & mq-nr &  mb+nc \\ kz-mx & kr-mp & kb-ma \\ nx+ky & np+kq & na+kb  \end{vmatrix}=0$$ In my book it is given as hint that the determinant can be expressed as a product of two other determinants whose value will evaluate to $0$.But I'm not being able to express the given determinant as a product of two other determinants.How should I do it?Please guide me through the procedure.,,['matrices']
26,Are the two matrices similar?,Are the two matrices similar?,,"Consider the matrices  $$A= \begin{bmatrix} 2 & 2 & 1 \\0 & 2  & -1\\ 0 & 0 & 3\end{bmatrix}$$ and $$B= \begin{bmatrix} 2 & 1 & 0 \\0 & 2  & 0\\ 0 & 0 & 3\end{bmatrix}.$$ Which are true: $A$ and $B$ are similar over $\Bbb Q$. $A$ is diagonalizable over $\Bbb Q$. $B$ is Jordan Canonical Form of $A$. The minimal and characteristic polynomial of $A$ are same. The characteristic polynomial of $A$ is coming as $(x-2)^2(x-3)$. Since minimal polynomial and characteristic polynomial have the same roots so minimal polynomial must be $(x-2)(x-3)$ or $(x-2)^2(x-3)$.$A$ does not satisfy $(x-2)(x-3)$ so it's minimal polynomial must be $(x-2)^2(x-3)$.Obviously $B$ is JCF of $A$. And since roots of minimal polynomial are not distinct so $A$ is not diagonalisable.Hence $3,4$ are true; $1$ is false. How to prove/disprove $1$","Consider the matrices  $$A= \begin{bmatrix} 2 & 2 & 1 \\0 & 2  & -1\\ 0 & 0 & 3\end{bmatrix}$$ and $$B= \begin{bmatrix} 2 & 1 & 0 \\0 & 2  & 0\\ 0 & 0 & 3\end{bmatrix}.$$ Which are true: $A$ and $B$ are similar over $\Bbb Q$. $A$ is diagonalizable over $\Bbb Q$. $B$ is Jordan Canonical Form of $A$. The minimal and characteristic polynomial of $A$ are same. The characteristic polynomial of $A$ is coming as $(x-2)^2(x-3)$. Since minimal polynomial and characteristic polynomial have the same roots so minimal polynomial must be $(x-2)(x-3)$ or $(x-2)^2(x-3)$.$A$ does not satisfy $(x-2)(x-3)$ so it's minimal polynomial must be $(x-2)^2(x-3)$.Obviously $B$ is JCF of $A$. And since roots of minimal polynomial are not distinct so $A$ is not diagonalisable.Hence $3,4$ are true; $1$ is false. How to prove/disprove $1$",,"['linear-algebra', 'matrices']"
27,Simultaneous diagonalization,Simultaneous diagonalization,,"Given two symmetric matrices $A,B\in\Bbb R^n$ how can we find if they are simultaneously diagonalizable? If they have such property how can we find $U$ such that $UAU'$ and $UBU'$ are simultaneously diagonalized efficiently?","Given two symmetric matrices $A,B\in\Bbb R^n$ how can we find if they are simultaneously diagonalizable? If they have such property how can we find $U$ such that $UAU'$ and $UBU'$ are simultaneously diagonalized efficiently?",,"['linear-algebra', 'matrices', 'algorithms', 'diagonalization', 'orthogonality']"
28,proving subspace of matrix,proving subspace of matrix,,"Let $W = \lbrace A \in M_n(\mathbb{R}) \mid \operatorname{Tr}(A) = 0\rbrace$ where $\operatorname{Tr}(A)$ is the trace of $A$ (i.e. equal to sum of diagonal elements of $A$). Show that $W$ is a subspace of $M_n(\mathbb{R})$. What I tried I tried to construct a matrix $M_3$ e.g. $$\begin{bmatrix} 1 & 0 & 5 \\ 0 & -4 & 1 \\ 0 & 0 & 3 \end{bmatrix}$$ And try to prove that the sum of any two elements of $W$ is also in $W$ but I can't see how that is true. Then I was going to prove that an element of $W$ under scalar multiplication also produces an element in $W$, but I encountered a similar problem. And obviously I know $W$ is a subset of $M_n$. Help?","Let $W = \lbrace A \in M_n(\mathbb{R}) \mid \operatorname{Tr}(A) = 0\rbrace$ where $\operatorname{Tr}(A)$ is the trace of $A$ (i.e. equal to sum of diagonal elements of $A$). Show that $W$ is a subspace of $M_n(\mathbb{R})$. What I tried I tried to construct a matrix $M_3$ e.g. $$\begin{bmatrix} 1 & 0 & 5 \\ 0 & -4 & 1 \\ 0 & 0 & 3 \end{bmatrix}$$ And try to prove that the sum of any two elements of $W$ is also in $W$ but I can't see how that is true. Then I was going to prove that an element of $W$ under scalar multiplication also produces an element in $W$, but I encountered a similar problem. And obviously I know $W$ is a subset of $M_n$. Help?",,"['linear-algebra', 'matrices', 'vector-spaces', 'vectors']"
29,A question on a certain block decomposition of semi-definite matrices.,A question on a certain block decomposition of semi-definite matrices.,,"Let $m,n\in\mathbb{N}$, with $m,n>1$. Suppose $K\in \mathbb{M}_{mn\times mn}(\mathbb{C})$ is positive semidefinite. We can always write $$K=\sum_{i,j=1}^m E_{i,j}\otimes K_{i,j},$$ for some collection of matrices $K_{i,j}\in \mathbb{M}_{n\times n}(\mathbb{C}) $, where $E_{i,j}\in \mathbb{M}_{m\times m}(\mathbb{C})$ is the matrix with 1 in the  entry $(i,j)$ and zeros everywhere else. This amounts to writing $K$ in the block diagonal form $$\begin{pmatrix} K_{1,1} &\dots  &K_{1,m} \\   \vdots &\ddots  &\vdots \\   K_{m,1}& \dots & K_{m,m} \end{pmatrix}$$. Under what conditions can we find a collection of non-square matrices $A_k\in \mathbb{M}_{mn\times n}(\mathbb{C})$ such that $K_{i,j}=A_i^*A_j$ for all $i,j\in \{1,\dots,m \}$? In other words, when can we write $$K=\begin{pmatrix} A_1^*A_1 &\dots  &A_1^*A_m \\   \vdots &\ddots  &\vdots \\   A_m^*A_1& \dots & A_m^*A_m \end{pmatrix}=\begin{pmatrix} A^*_{1}\\  \vdots\\ A^*_{m}\end{pmatrix} \begin{pmatrix} A_{1} &\dots  &A_{m}  \end{pmatrix}$$ for some collection of matrices $A_k\in \mathbb{M}_{mn\times n}(\mathbb{C})$?","Let $m,n\in\mathbb{N}$, with $m,n>1$. Suppose $K\in \mathbb{M}_{mn\times mn}(\mathbb{C})$ is positive semidefinite. We can always write $$K=\sum_{i,j=1}^m E_{i,j}\otimes K_{i,j},$$ for some collection of matrices $K_{i,j}\in \mathbb{M}_{n\times n}(\mathbb{C}) $, where $E_{i,j}\in \mathbb{M}_{m\times m}(\mathbb{C})$ is the matrix with 1 in the  entry $(i,j)$ and zeros everywhere else. This amounts to writing $K$ in the block diagonal form $$\begin{pmatrix} K_{1,1} &\dots  &K_{1,m} \\   \vdots &\ddots  &\vdots \\   K_{m,1}& \dots & K_{m,m} \end{pmatrix}$$. Under what conditions can we find a collection of non-square matrices $A_k\in \mathbb{M}_{mn\times n}(\mathbb{C})$ such that $K_{i,j}=A_i^*A_j$ for all $i,j\in \{1,\dots,m \}$? In other words, when can we write $$K=\begin{pmatrix} A_1^*A_1 &\dots  &A_1^*A_m \\   \vdots &\ddots  &\vdots \\   A_m^*A_1& \dots & A_m^*A_m \end{pmatrix}=\begin{pmatrix} A^*_{1}\\  \vdots\\ A^*_{m}\end{pmatrix} \begin{pmatrix} A_{1} &\dots  &A_{m}  \end{pmatrix}$$ for some collection of matrices $A_k\in \mathbb{M}_{mn\times n}(\mathbb{C})$?",,['matrices']
30,Meaning of adding rows matrix,Meaning of adding rows matrix,,"English is not my mother tongue and I'm studying Algebra using a book in English. This sentence came up to me in an exercise ""every row of matrix $A$ adds to zero"". What does that mean, in concrete? EDIT: Full exercise: If every row of $A$ adds to zero, prove that $\det A = 0$. If every row adds to $1$, prove that $\det (A-I) = 0$. Show by example that this does not imply $\det A = 1$.","English is not my mother tongue and I'm studying Algebra using a book in English. This sentence came up to me in an exercise ""every row of matrix $A$ adds to zero"". What does that mean, in concrete? EDIT: Full exercise: If every row of $A$ adds to zero, prove that $\det A = 0$. If every row adds to $1$, prove that $\det (A-I) = 0$. Show by example that this does not imply $\det A = 1$.",,"['linear-algebra', 'matrices', 'terminology']"
31,Show that $\hat x$ is a least squares solution of the system $Ax=b$,Show that  is a least squares solution of the system,\hat x Ax=b,"Show that if $$\begin{bmatrix}A & I \\ O & A^T\end{bmatrix} \begin{bmatrix}\hat x \\ r\end{bmatrix} = \begin{bmatrix}b \\ 0\end{bmatrix}$$ then $\hat x$ is a least squares solution of the system $Ax = b$ and $r$ is the residual vector. The wording of the problem is confusing to me, because I can usually find the least squares solution by finding the solution of $\hat x$ to $A^TA\hat x = A^Tb$. Except, this wants me to prove the least squares solution is $\hat x$, which doesn't make much sense to me.","Show that if $$\begin{bmatrix}A & I \\ O & A^T\end{bmatrix} \begin{bmatrix}\hat x \\ r\end{bmatrix} = \begin{bmatrix}b \\ 0\end{bmatrix}$$ then $\hat x$ is a least squares solution of the system $Ax = b$ and $r$ is the residual vector. The wording of the problem is confusing to me, because I can usually find the least squares solution by finding the solution of $\hat x$ to $A^TA\hat x = A^Tb$. Except, this wants me to prove the least squares solution is $\hat x$, which doesn't make much sense to me.",,"['linear-algebra', 'matrices', 'least-squares', 'transpose']"
32,"Does this type of ""cyclic"" matrix have a name?","Does this type of ""cyclic"" matrix have a name?",,"Let $\{a_1, a_2, ..., a_n\} \subset \mathbb{C}$ and consider the matrix of the form $$ \begin{bmatrix}   a_1 & a_2 & ... & a_{n-1} & a_n\\   a_2 & a_3 & ... & a_n & a_1\\   .\\   .\\   .\\   a_n & a_1 & ... & a_{n-2} & a_{n-1} \end{bmatrix} $$ Does this type of matrix have a specific name?","Let $\{a_1, a_2, ..., a_n\} \subset \mathbb{C}$ and consider the matrix of the form $$ \begin{bmatrix}   a_1 & a_2 & ... & a_{n-1} & a_n\\   a_2 & a_3 & ... & a_n & a_1\\   .\\   .\\   .\\   a_n & a_1 & ... & a_{n-2} & a_{n-1} \end{bmatrix} $$ Does this type of matrix have a specific name?",,"['linear-algebra', 'matrices']"
33,how can i find the matrix $P$ that diagonalizes the matrix $A$?,how can i find the matrix  that diagonalizes the matrix ?,P A,I want to find matrix $P$ that diagonalizes the matrix $A$: $$         \begin{bmatrix}         4 & 0 & 1 \\         2 & 3 & 2 \\         1 & 0 & 4 \\         \end{bmatrix} $$,I want to find matrix $P$ that diagonalizes the matrix $A$: $$         \begin{bmatrix}         4 & 0 & 1 \\         2 & 3 & 2 \\         1 & 0 & 4 \\         \end{bmatrix} $$,,"['linear-algebra', 'matrices', 'diagonalization']"
34,Why does $(\det A)(\det C) \ge {\left| {\det B} \right|^2}$? [closed],Why does ? [closed],(\det A)(\det C) \ge {\left| {\det B} \right|^2},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Let $H = \left[ {\begin{array}{*{20}{c}}    A & B  \\    {{B^*}} & C  \\ \end{array}} \right]$ be positive semidefinite and $A,C\in M_p$. Why does $(\det A)(\det C) \ge {\left| {\det B} \right|^2}$?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Let $H = \left[ {\begin{array}{*{20}{c}}    A & B  \\    {{B^*}} & C  \\ \end{array}} \right]$ be positive semidefinite and $A,C\in M_p$. Why does $(\det A)(\det C) \ge {\left| {\det B} \right|^2}$?",,"['linear-algebra', 'matrices']"
35,"Relationship between BIBO, Marginal and Asymptotic stability: statements","Relationship between BIBO, Marginal and Asymptotic stability: statements",,"I know that asymptotic stability implies BIBO stability. But then, which of the following can be held true: 1. If a system is BIBO stable would it definitely be asymptotic stable?    2. If a system is marginally stable, it will definitely be    asymptotically stable?    3. If a system is asymptotic stable, it will    definitely be marginally stable? Kepeing in view the poles real part values in mind, what could be held true?","I know that asymptotic stability implies BIBO stability. But then, which of the following can be held true: 1. If a system is BIBO stable would it definitely be asymptotic stable?    2. If a system is marginally stable, it will definitely be    asymptotically stable?    3. If a system is asymptotic stable, it will    definitely be marginally stable? Kepeing in view the poles real part values in mind, what could be held true?",,"['linear-algebra', 'matrices', 'vector-spaces', 'matrix-equations', 'nonlinear-system']"
36,Necessar and sufficient condition of similitude,Necessar and sufficient condition of similitude,,Here is the problem : Give a necessar and sufficient over $M\in \mathcal{M}_n(\mathbb{C})$ (a complex matrix) such that there exists $P \in GL_n(\mathbb{C})$ and $N \in \mathcal{M}_n(\mathbb{R})$ satisfying : $$M=P^{-1}NP$$ I have barely no ideas how to start. The problem is difficult and has been proposed to a french oral examination (ENS) If you have the solution to this problem or hints please give them. Thank you.,Here is the problem : Give a necessar and sufficient over $M\in \mathcal{M}_n(\mathbb{C})$ (a complex matrix) such that there exists $P \in GL_n(\mathbb{C})$ and $N \in \mathcal{M}_n(\mathbb{R})$ satisfying : $$M=P^{-1}NP$$ I have barely no ideas how to start. The problem is difficult and has been proposed to a french oral examination (ENS) If you have the solution to this problem or hints please give them. Thank you.,,"['linear-algebra', 'matrices']"
37,"If $A=\left(\begin{smallmatrix} 1 & \tan x\\ -\tan x & 1 \end{smallmatrix}\right)$ and $f(x)=\det(A^TA^{-1})$, then what is $f(f(f(f \cdots f(x))))$?","If  and , then what is ?",A=\left(\begin{smallmatrix} 1 & \tan x\\ -\tan x & 1 \end{smallmatrix}\right) f(x)=\det(A^TA^{-1}) f(f(f(f \cdots f(x)))),Let $$A := \begin{bmatrix} 1 & \tan x\\ -\tan x & 1\end{bmatrix}$$ and $$f(x) := \det(A^TA^{-1})$$ Which of the following can not be the value of $f(f(f(f \cdots f(x))))$ ? $f^n(x)$ $1$ $f^{n-1}(x)$ $nf(x)$ where $n \geq 2$ . I found $$f(x)=\frac{\det(A^T)}{\det(A)}=\frac{\det(A)}{\det(A)}=1.$$ But I could not figure out the answer. Please help me.,Let and Which of the following can not be the value of ? where . I found But I could not figure out the answer. Please help me.,A := \begin{bmatrix} 1 & \tan x\\ -\tan x & 1\end{bmatrix} f(x) := \det(A^TA^{-1}) f(f(f(f \cdots f(x)))) f^n(x) 1 f^{n-1}(x) nf(x) n \geq 2 f(x)=\frac{\det(A^T)}{\det(A)}=\frac{\det(A)}{\det(A)}=1.,"['matrices', 'functions']"
38,Equation of a circle in matrix form,Equation of a circle in matrix form,,"I have an equation $ \left( x-3 \right)^{2}+\left( y-3 \right)^{2}=9 $, and am trying to apply a matrix rotation of 180 degrees to it, however, I am having difficulty transferring the equation of the circle into matrix form so to complete the transformation. Thanks","I have an equation $ \left( x-3 \right)^{2}+\left( y-3 \right)^{2}=9 $, and am trying to apply a matrix rotation of 180 degrees to it, however, I am having difficulty transferring the equation of the circle into matrix form so to complete the transformation. Thanks",,"['linear-algebra', 'matrices', 'circles', 'matrix-equations']"
39,Why does using orthogonal matrices as change of basis produce decoupled system of equation?,Why does using orthogonal matrices as change of basis produce decoupled system of equation?,,"Suppose I have the following set of differential equations $\dot x_1 = -x_1 - 3x_2 \quad \dot x_2 = 2x_2$ This is the phase portrait of our system But let's take a change of coordinate using $P = \begin{bmatrix} 1 & 0 \\ -1 & 1 \end{bmatrix}$ where each of the columns corresponds to an eigenvector Then let $y = P^{-1}x$ Couple things: The phase portrait has been ""straightened"" The system of equation becomes decoupled (i.e. only diagonal elements exist) What accounts for this amazing behavior? Why is it this particular change of coordinate matrix is able to yield a new set of equations with these properties?","Suppose I have the following set of differential equations This is the phase portrait of our system But let's take a change of coordinate using where each of the columns corresponds to an eigenvector Then let Couple things: The phase portrait has been ""straightened"" The system of equation becomes decoupled (i.e. only diagonal elements exist) What accounts for this amazing behavior? Why is it this particular change of coordinate matrix is able to yield a new set of equations with these properties?",\dot x_1 = -x_1 - 3x_2 \quad \dot x_2 = 2x_2 P = \begin{bmatrix} 1 & 0 \\ -1 & 1 \end{bmatrix} y = P^{-1}x,"['linear-algebra', 'matrices', 'ordinary-differential-equations', 'vector-spaces', 'dynamical-systems']"
40,How can I minimize $\|Axx^T-A\|$?,How can I minimize ?,\|Axx^T-A\|,How can the following equation be minimized over $x$? $$f(x)=\|Axx^T-A\|_2^2$$ where $A$ is a $n\times n$ matrix of full rank and $x$ is an $n\times 1$ column vector. The norm is the Frobenius norm. I tried to substitute $Ax$ but cannot find an appropriate solution. Can we somehow use the Cauchy Schwarz Inequality here?,How can the following equation be minimized over $x$? $$f(x)=\|Axx^T-A\|_2^2$$ where $A$ is a $n\times n$ matrix of full rank and $x$ is an $n\times 1$ column vector. The norm is the Frobenius norm. I tried to substitute $Ax$ but cannot find an appropriate solution. Can we somehow use the Cauchy Schwarz Inequality here?,,"['matrices', 'matrix-equations']"
41,"given matrix $A$, decide if $P^{-1}AP$ is diagonal","given matrix , decide if  is diagonal",A P^{-1}AP,"$$A=         \begin{pmatrix}         1 & 0 & 0 & 0\\         2 & 3 & 2 & 2\\         2 & 2 & 3 & 2\\         2 & 2 & 2 & 3\\         \end{pmatrix} $$ I know that the eigenvalues are 1 of geometric multiplicity = algebric multiplicity = 3, and 7 of geometric multiplicity = algebric multiplicity = 1. The eigenvectors of 1 are $(1, 0, 0, -1),(1, 0, -1, 0),(1, -1, 0, 0)$ and of 7 is $(0, 1, 1, 1)$. I given some matrices (each called $P$) and I need to decide if $P^{-1}AP$ is diagonal. For example: $$P=         \begin{pmatrix}         0 & 0 & 0 & 6\\         0 & -1 & 3 & -3\\         2 & 0 & 3 & -3\\         -2 & 1 & 3 & 0\\         \end{pmatrix} $$ How do I check it?","$$A=         \begin{pmatrix}         1 & 0 & 0 & 0\\         2 & 3 & 2 & 2\\         2 & 2 & 3 & 2\\         2 & 2 & 2 & 3\\         \end{pmatrix} $$ I know that the eigenvalues are 1 of geometric multiplicity = algebric multiplicity = 3, and 7 of geometric multiplicity = algebric multiplicity = 1. The eigenvectors of 1 are $(1, 0, 0, -1),(1, 0, -1, 0),(1, -1, 0, 0)$ and of 7 is $(0, 1, 1, 1)$. I given some matrices (each called $P$) and I need to decide if $P^{-1}AP$ is diagonal. For example: $$P=         \begin{pmatrix}         0 & 0 & 0 & 6\\         0 & -1 & 3 & -3\\         2 & 0 & 3 & -3\\         -2 & 1 & 3 & 0\\         \end{pmatrix} $$ How do I check it?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
42,find the eigenvalues of $5×5$ matrix,find the eigenvalues of  matrix,5×5,"The product of the non-zero eigenvalues of the matrix is ____. using characteristic equation , it is too lengthy to find eigenvalues of $5×5$ matrix , I'm looking for any short trick to solve this question .","The product of the non-zero eigenvalues of the matrix is ____. using characteristic equation , it is too lengthy to find eigenvalues of $5×5$ matrix , I'm looking for any short trick to solve this question .",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
43,Decomposition into diagonal and orthogonal matrixes,Decomposition into diagonal and orthogonal matrixes,,"I'd like to decompose a matrix M into a diagonal matrix D and an orthogonal matrix Q, such that $$         \begin{bmatrix}         d_1 & 0 & 0 \\         0& d_2  & 0\\         0 & 0 & d_3 \\         \end{bmatrix} \cdot  \begin{bmatrix}         q_1 & q_2 & q_3 \\         q_4 & q_5  & q_6\\         q_7 & q_8 & q_9 \\         \end{bmatrix} =  \begin{bmatrix}         m_1 & m_2 & m_3 \\         m_4 & m_5  & m_6\\         m_7 & m_8 & m_9 \\         \end{bmatrix} $$ Does somebody know, if there exists such a decomposition? I'm using this to split a matrix into a projection matrix and a rotation matrix. This is usually done with the RQ-Decomposition (Upper-Triangle * Orthogonal). But in my case, I want the projection matrix to be a 'trivial' diagonal matrix. Thanks for your help.","I'd like to decompose a matrix M into a diagonal matrix D and an orthogonal matrix Q, such that $$         \begin{bmatrix}         d_1 & 0 & 0 \\         0& d_2  & 0\\         0 & 0 & d_3 \\         \end{bmatrix} \cdot  \begin{bmatrix}         q_1 & q_2 & q_3 \\         q_4 & q_5  & q_6\\         q_7 & q_8 & q_9 \\         \end{bmatrix} =  \begin{bmatrix}         m_1 & m_2 & m_3 \\         m_4 & m_5  & m_6\\         m_7 & m_8 & m_9 \\         \end{bmatrix} $$ Does somebody know, if there exists such a decomposition? I'm using this to split a matrix into a projection matrix and a rotation matrix. This is usually done with the RQ-Decomposition (Upper-Triangle * Orthogonal). But in my case, I want the projection matrix to be a 'trivial' diagonal matrix. Thanks for your help.",,"['linear-algebra', 'matrices', 'projective-geometry']"
44,The maximum value of determinant,The maximum value of determinant,,"Suppose we have a matrix $A_{3\times3} = (a_{ij})$, where $a_{ij}\in \mathbb{Z}$ and $|a_{ij}|\le 9$ for $1\le i,j\le 3$. What is the maximum value may take the value $\det (A)$? I'm looking for a reasonable solution without the use of brute-force.","Suppose we have a matrix $A_{3\times3} = (a_{ij})$, where $a_{ij}\in \mathbb{Z}$ and $|a_{ij}|\le 9$ for $1\le i,j\le 3$. What is the maximum value may take the value $\det (A)$? I'm looking for a reasonable solution without the use of brute-force.",,"['linear-algebra', 'matrices', 'determinant']"
45,Commutativity in terms of the Jordan Normal Form.,Commutativity in terms of the Jordan Normal Form.,,"Let us consider requirements for commutativity of matrices in terms of the Jordan Normal Form, Say we have two matrices $\bf A$ and $\bf B$. Then ${\bf A} = {\bf S}^{-1}{\bf JS}$, where $\bf J$ can be written in block matrix form: $${\bf J} = \left[ \begin{array}{ccccc} {\bf \Lambda_1} & \bf 0 & \cdots & \bf 0  & \bf 0 \\ \bf 0 & \bf \Lambda_2 & \cdots &\bf 0 & \bf 0\\ \bf 0&\bf 0 &\bf \ddots & \bf 0 & \bf 0 \\ \bf 0&\bf 0&\cdots&\bf \Lambda_{n-1}& \bf0\\ \bf 0 &\bf 0&\cdots& \bf 0 & \bf\Lambda_n \end{array}\right] $$ where $$ {\bf \Lambda_k} = \left[ \begin{array}{ccccc} {\bf \Lambda_{k,1}} & \bf 0 & \cdots & \bf 0  & \bf 0 \\ \bf 0 & \bf \Lambda_{k,2} & \cdots &\bf 0 & \bf 0\\ \bf 0&\bf 0 &\bf \ddots & \bf 0 & \bf 0 \\ \bf 0&\bf 0&\cdots&\bf \Lambda_{k,m-1}& \bf0\\ \bf 0 &\bf 0&\cdots& \bf 0 & \bf\Lambda_{k,m} \end{array}\right] $$ where each $$ {\bf \Lambda_{k,l}} = \left[ \begin{array}{cccc} {\lambda_{k}} & 1 & 0 & 0  \\  0 & \lambda_{k} & \ddots & 0 \\  0& 0& \ddots &  1  \\ 0& 0&0& \lambda_k \end{array}\right]$$ where the matrix size may be individual for each $l$. Let us now assume that we can write $\bf B = S^{-1}J_BS$. For which $\bf J_B$ will this commute? Any necessary or sufficient conditions? Clarification: I did not intend $\bf J_B$ to be a Jordan matrix like $\bf J$ above. Are there any other types of matrices for which it could work? What could we say about such a matrix? Does it need to have any specific type of block structure?","Let us consider requirements for commutativity of matrices in terms of the Jordan Normal Form, Say we have two matrices $\bf A$ and $\bf B$. Then ${\bf A} = {\bf S}^{-1}{\bf JS}$, where $\bf J$ can be written in block matrix form: $${\bf J} = \left[ \begin{array}{ccccc} {\bf \Lambda_1} & \bf 0 & \cdots & \bf 0  & \bf 0 \\ \bf 0 & \bf \Lambda_2 & \cdots &\bf 0 & \bf 0\\ \bf 0&\bf 0 &\bf \ddots & \bf 0 & \bf 0 \\ \bf 0&\bf 0&\cdots&\bf \Lambda_{n-1}& \bf0\\ \bf 0 &\bf 0&\cdots& \bf 0 & \bf\Lambda_n \end{array}\right] $$ where $$ {\bf \Lambda_k} = \left[ \begin{array}{ccccc} {\bf \Lambda_{k,1}} & \bf 0 & \cdots & \bf 0  & \bf 0 \\ \bf 0 & \bf \Lambda_{k,2} & \cdots &\bf 0 & \bf 0\\ \bf 0&\bf 0 &\bf \ddots & \bf 0 & \bf 0 \\ \bf 0&\bf 0&\cdots&\bf \Lambda_{k,m-1}& \bf0\\ \bf 0 &\bf 0&\cdots& \bf 0 & \bf\Lambda_{k,m} \end{array}\right] $$ where each $$ {\bf \Lambda_{k,l}} = \left[ \begin{array}{cccc} {\lambda_{k}} & 1 & 0 & 0  \\  0 & \lambda_{k} & \ddots & 0 \\  0& 0& \ddots &  1  \\ 0& 0&0& \lambda_k \end{array}\right]$$ where the matrix size may be individual for each $l$. Let us now assume that we can write $\bf B = S^{-1}J_BS$. For which $\bf J_B$ will this commute? Any necessary or sufficient conditions? Clarification: I did not intend $\bf J_B$ to be a Jordan matrix like $\bf J$ above. Are there any other types of matrices for which it could work? What could we say about such a matrix? Does it need to have any specific type of block structure?",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
46,Determine if matrices A and B are similar,Determine if matrices A and B are similar,,Determine if matrices $A$ and $B$ are similar. $A=\begin{pmatrix}1 & 1 & 0 & 0 \\0 & 1 & 0 & 0 \\0 & 0 & 3 & 0 \\0 & 0 & 0 & 3\end{pmatrix}\: $ and $B=\begin{pmatrix}1 & 1 & 0 & 0 \\0 & 1 & 0 & 0 \\0 & 0 & 3 & 1 \\0 & 0 & 0 & 3\end{pmatrix}$ I found the characteristic polynomials to be the same for both matrices $(1-λ)(3-λ)(3-λ)(1-λ)$ so I went to plug in the values to find the eigenvectors for matrix $A$ and I think I may have screwed up because $\begin{pmatrix}0 & 1 & 0 & 0 \\0 & 0 & 0 & 0 \\0 & 0 & 2 & 0 \\0 & 0 & 0 & 2\end{pmatrix}$ I got an eigenvector for $λ=1$ to be $\begin{pmatrix}0 \\0 \\0 \\0\end{pmatrix}$ which I'm not even sure if I did it correctly or not because I don't think this is possible. for $λ=3$ I got $\:\begin{pmatrix}1 \\\frac{1}{2} \\0 \\0\end{pmatrix}$ If I did do this correctly then is the geometric multiplicity for both = 1?,Determine if matrices $A$ and $B$ are similar. $A=\begin{pmatrix}1 & 1 & 0 & 0 \\0 & 1 & 0 & 0 \\0 & 0 & 3 & 0 \\0 & 0 & 0 & 3\end{pmatrix}\: $ and $B=\begin{pmatrix}1 & 1 & 0 & 0 \\0 & 1 & 0 & 0 \\0 & 0 & 3 & 1 \\0 & 0 & 0 & 3\end{pmatrix}$ I found the characteristic polynomials to be the same for both matrices $(1-λ)(3-λ)(3-λ)(1-λ)$ so I went to plug in the values to find the eigenvectors for matrix $A$ and I think I may have screwed up because $\begin{pmatrix}0 & 1 & 0 & 0 \\0 & 0 & 0 & 0 \\0 & 0 & 2 & 0 \\0 & 0 & 0 & 2\end{pmatrix}$ I got an eigenvector for $λ=1$ to be $\begin{pmatrix}0 \\0 \\0 \\0\end{pmatrix}$ which I'm not even sure if I did it correctly or not because I don't think this is possible. for $λ=3$ I got $\:\begin{pmatrix}1 \\\frac{1}{2} \\0 \\0\end{pmatrix}$ If I did do this correctly then is the geometric multiplicity for both = 1?,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
47,Find the matrix $\mathbf{A}$ if $A\binom{7}{-1} = \binom{6}{2}.$,Find the matrix  if,\mathbf{A} A\binom{7}{-1} = \binom{6}{2}.,Find the $2\times2$ matrix $A$ where $A^2=A$ and $$A\begin{pmatrix} 7 \\ -1 \end{pmatrix} = \begin{pmatrix} 6 \\ 2 \end{pmatrix}.$$ I tried plugging in: $A= \begin{pmatrix}a&b\\c&d\end{pmatrix}$ but that became messy very quickly. I got the equations: $7a-b = 6$ $7c-d = 2$ $a^2+bc = a$ $ab+bd = b$ $ac + cd = c$ $bc + d^2 = d$ from trying that method. What should I do?,Find the $2\times2$ matrix $A$ where $A^2=A$ and $$A\begin{pmatrix} 7 \\ -1 \end{pmatrix} = \begin{pmatrix} 6 \\ 2 \end{pmatrix}.$$ I tried plugging in: $A= \begin{pmatrix}a&b\\c&d\end{pmatrix}$ but that became messy very quickly. I got the equations: $7a-b = 6$ $7c-d = 2$ $a^2+bc = a$ $ab+bd = b$ $ac + cd = c$ $bc + d^2 = d$ from trying that method. What should I do?,,"['calculus', 'matrices', 'algebra-precalculus', 'trigonometry']"
48,Why is the determinant of $\sum_{i=1}^n A_i^2$ non-negative?,Why is the determinant of  non-negative?,\sum_{i=1}^n A_i^2,"Let $A_i$ be an $n\times n$ matrix in $\mathbb{R}$ and $\{A_i\}_{i=1}^k$ are pairwise commutative: $A_iA_j = A_jA_i$. How to show $det(\sum_{i=1}^k A_i^2)\geq 0$? We may consider this question in the complex field $\mathbb{C}$. Each $A_i$ is triangulable. Since they are commutative , there exists $P\in GL_n(\mathbb{C})$ such that $\{A_i\}_{i=1}^k$ are simultaneously triangluated. Say $B_i = P^{-1}A_iP$ is upper triangular. Denote the $(j,j)$-th element in $B_i$ by $b_{i,j}$. Thus, $det(\sum_{i=1}^k A_i^2) = det(\sum_{i=1}^k B_i^2) = \prod_{j=1}^n\sum_{i=1}^k b_{i,j}^2$. But how to show this value is always non-negative? Indeed, I don't know how the ""pairwise commutative"" condition helps here. Many thanks.","Let $A_i$ be an $n\times n$ matrix in $\mathbb{R}$ and $\{A_i\}_{i=1}^k$ are pairwise commutative: $A_iA_j = A_jA_i$. How to show $det(\sum_{i=1}^k A_i^2)\geq 0$? We may consider this question in the complex field $\mathbb{C}$. Each $A_i$ is triangulable. Since they are commutative , there exists $P\in GL_n(\mathbb{C})$ such that $\{A_i\}_{i=1}^k$ are simultaneously triangluated. Say $B_i = P^{-1}A_iP$ is upper triangular. Denote the $(j,j)$-th element in $B_i$ by $b_{i,j}$. Thus, $det(\sum_{i=1}^k A_i^2) = det(\sum_{i=1}^k B_i^2) = \prod_{j=1}^n\sum_{i=1}^k b_{i,j}^2$. But how to show this value is always non-negative? Indeed, I don't know how the ""pairwise commutative"" condition helps here. Many thanks.",,"['linear-algebra', 'matrices']"
49,Least-squares solution to a matrix equation?,Least-squares solution to a matrix equation?,,"Suppose I have $n$ observations of $m$ dependent variables $y_1,\dots,y_m$, and I believe they follow some model wherein they can all be written as linear combinations of some underlying variables $x_1,\dots,x_k$ (with $k<m<n$). In other words, I have the model $$Y=X\beta$$ for a (known) $n\times m$ matrix of observations $Y$, an (unknown) $n\times k$ matrix of underlying variables $X$, and an (unknown) $k\times m$ matrix of coefficients $\beta$. If $n$ is sufficiently large, then this system is over-determined and I should be able to solve for $X$ and $\beta$ that give the least-squares solution to this equation, right? It seems that this should be solvable with something like linear regression but I'm not sure how.","Suppose I have $n$ observations of $m$ dependent variables $y_1,\dots,y_m$, and I believe they follow some model wherein they can all be written as linear combinations of some underlying variables $x_1,\dots,x_k$ (with $k<m<n$). In other words, I have the model $$Y=X\beta$$ for a (known) $n\times m$ matrix of observations $Y$, an (unknown) $n\times k$ matrix of underlying variables $X$, and an (unknown) $k\times m$ matrix of coefficients $\beta$. If $n$ is sufficiently large, then this system is over-determined and I should be able to solve for $X$ and $\beta$ that give the least-squares solution to this equation, right? It seems that this should be solvable with something like linear regression but I'm not sure how.",,"['linear-algebra', 'matrices', 'statistics', 'regression', 'least-squares']"
50,Is $A+uv^T$ invertible?,Is  invertible?,A+uv^T,"Let $u, v \in \mathbb{R}^n$ and $A \in \mathbb{R}^{n\times n}$. For which condition(s) this matrix is invertible? $$A + uv^T$$ and find the inverse of this matrix. I tried to take elementary matrices, $I - \alpha xy^T$, to solve this problem, where $\alpha$ is a constant. $(I-\alpha xy^T)^{-1} = I - \beta xy^T$ iff $\beta = \frac{\alpha}{\alpha y^T x - 1}$ where $\alpha y^T x -1 \neq 0$. If we take $x=(0,0,\ldots, l_{i+1,i},\ldots,l_{n,i})^T$ and $y=e_i=(0,\ldots, 1,\ldots,0)^T$ then we can find a lower triangular matrix like below: $$L = \left[ \begin{array}{ccccc}  1 & 0 & \cdots & 0 & 0 \\  \ell _{2,1} & 1 & \cdots & 0 & 0 \\  \vdots & \vdots & \ddots & \vdots & \vdots \\  \ell _{n-1,1} & \ell _{n-1,2} &  & 1 & 0 \\  \ell _{n,1} & \ell _{n,2} & \cdots & \ell _{n,n-1} & 1 \\ \end{array} \right]=I+\ell_1e_1^T + \ldots + \ell_{n-1}e_{n-1}^T$$ But I don't know how to complete ?","Let $u, v \in \mathbb{R}^n$ and $A \in \mathbb{R}^{n\times n}$. For which condition(s) this matrix is invertible? $$A + uv^T$$ and find the inverse of this matrix. I tried to take elementary matrices, $I - \alpha xy^T$, to solve this problem, where $\alpha$ is a constant. $(I-\alpha xy^T)^{-1} = I - \beta xy^T$ iff $\beta = \frac{\alpha}{\alpha y^T x - 1}$ where $\alpha y^T x -1 \neq 0$. If we take $x=(0,0,\ldots, l_{i+1,i},\ldots,l_{n,i})^T$ and $y=e_i=(0,\ldots, 1,\ldots,0)^T$ then we can find a lower triangular matrix like below: $$L = \left[ \begin{array}{ccccc}  1 & 0 & \cdots & 0 & 0 \\  \ell _{2,1} & 1 & \cdots & 0 & 0 \\  \vdots & \vdots & \ddots & \vdots & \vdots \\  \ell _{n-1,1} & \ell _{n-1,2} &  & 1 & 0 \\  \ell _{n,1} & \ell _{n,2} & \cdots & \ell _{n,n-1} & 1 \\ \end{array} \right]=I+\ell_1e_1^T + \ldots + \ell_{n-1}e_{n-1}^T$$ But I don't know how to complete ?",,"['linear-algebra', 'matrices']"
51,Showing that a bilinear form is non degenerate,Showing that a bilinear form is non degenerate,,"Given a finite dimensional vector space $V$ over $F$ and a fixed matrix $(\alpha_{ij})=A \in M_n(F)$ and the bilinear form on $V \times V$ by $B(u,v)=\sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{ij} \zeta_i \eta_j$, where $\zeta_i$ and $\eta_j$ are the coordinates of $u,v$, I want to show that if the rank of $A$ is $n$, then $B$ is non-degenerate. I am absolutely stuck. :( I cannot seem to get a hold on this problem. I would really appreciate a hint in the right direction. The only thing that I think is in the right direction is that if $rank(A)=n$ then $n(A)=\lbrace 0 \rbrace$ and the columns form a basis of $V$....  Also, that $B(u,v) \equiv u^TAv$.","Given a finite dimensional vector space $V$ over $F$ and a fixed matrix $(\alpha_{ij})=A \in M_n(F)$ and the bilinear form on $V \times V$ by $B(u,v)=\sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{ij} \zeta_i \eta_j$, where $\zeta_i$ and $\eta_j$ are the coordinates of $u,v$, I want to show that if the rank of $A$ is $n$, then $B$ is non-degenerate. I am absolutely stuck. :( I cannot seem to get a hold on this problem. I would really appreciate a hint in the right direction. The only thing that I think is in the right direction is that if $rank(A)=n$ then $n(A)=\lbrace 0 \rbrace$ and the columns form a basis of $V$....  Also, that $B(u,v) \equiv u^TAv$.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'bilinear-form']"
52,Matrix multiplication: What is $\mathbf A^3$ and $\mathbf A^n$?,Matrix multiplication: What is  and ?,\mathbf A^3 \mathbf A^n,Suppose there is matrix A . I know that A 2 = A $\cdot $A But what if it is A 3 ? Is it A $\cdot $A $\cdot$A OR A 2 $\cdot$ A OR A  $\cdot$ A 2 ? So basically my question is what is A n ?,Suppose there is matrix A . I know that A 2 = A $\cdot $A But what if it is A 3 ? Is it A $\cdot $A $\cdot$A OR A 2 $\cdot$ A OR A  $\cdot$ A 2 ? So basically my question is what is A n ?,,"['matrices', 'exponentiation']"
53,Reflect on y axis in 3D Matrix?,Reflect on y axis in 3D Matrix?,,"I have a question saying ""Define a 3D Matrix that performs a reflection in the y axis"" but I don't know how to solve it. So if we have a 2D matrix and we say 'reflection on the y axis' we mean that x becomes -x. So a point (x,y) will be (-x, y). But what about in 3D? If we are reflecting on y, are both x and z negated? I tried to look for solutions and found this website http://www.idomaths.com/linear_transformation_3d.php on which I can see it's talking about xy, xz and yz planes. Does 'reflect on the y axis' mean the same thing as 'reflect against the xz plane'? Thanks","I have a question saying ""Define a 3D Matrix that performs a reflection in the y axis"" but I don't know how to solve it. So if we have a 2D matrix and we say 'reflection on the y axis' we mean that x becomes -x. So a point (x,y) will be (-x, y). But what about in 3D? If we are reflecting on y, are both x and z negated? I tried to look for solutions and found this website http://www.idomaths.com/linear_transformation_3d.php on which I can see it's talking about xy, xz and yz planes. Does 'reflect on the y axis' mean the same thing as 'reflect against the xz plane'? Thanks",,"['linear-algebra', 'matrices', 'linear-transformations']"
54,How to find general inverse of a matrix,How to find general inverse of a matrix,,Find the general inverse (G) of the matrix    $$A=\begin{bmatrix}1 & 2 & 3 \\4 & 5 & 6\end{bmatrix}$$   Also check that $AGA=A$ I am new in G- inverse calculation. I understand that G will be a $3 \times 2$ matrix. But unable to find it. Please help. Thanks in advance.,Find the general inverse (G) of the matrix    $$A=\begin{bmatrix}1 & 2 & 3 \\4 & 5 & 6\end{bmatrix}$$   Also check that $AGA=A$ I am new in G- inverse calculation. I understand that G will be a $3 \times 2$ matrix. But unable to find it. Please help. Thanks in advance.,,['matrices']
55,Show a matrix satisfying $A^2 − 8A + 15I = 0$ is diagonalisable.,Show a matrix satisfying  is diagonalisable.,A^2 − 8A + 15I = 0,"A square matrix $A$ (of some size $n × n$) satisfies the condition $A^2 − 8A + 15I = 0$. Show that this matrix is similar to a diagonal matrix. I know that we must show that 5 and 3 are the eigenvalues of this matrix, and that they yield n linearly independent eigenvectors, but I have no idea how. Furthermore: Show that for every positive integer $k ≥ 8$ there exists a matrix $A$ satisfying the above condition with $tr(A) = k$. I think there is some formula that the sum of the eigenvalues is equal to the trace, but I'm not entirely sure.","A square matrix $A$ (of some size $n × n$) satisfies the condition $A^2 − 8A + 15I = 0$. Show that this matrix is similar to a diagonal matrix. I know that we must show that 5 and 3 are the eigenvalues of this matrix, and that they yield n linearly independent eigenvectors, but I have no idea how. Furthermore: Show that for every positive integer $k ≥ 8$ there exists a matrix $A$ satisfying the above condition with $tr(A) = k$. I think there is some formula that the sum of the eigenvalues is equal to the trace, but I'm not entirely sure.",,"['linear-algebra', 'matrices']"
56,What seems to be the minors of the Adjugate matrix $\text{adj}(A)$ of a square matrix $A$?,What seems to be the minors of the Adjugate matrix  of a square matrix ?,\text{adj}(A) A,"It is by definition that entries of the adjugate matrix $\text{adj}(A)$ are the corresponding $(n-1)$-minors of $A$ (up to a sign). What can we say about the $k$-minor of $\text{adj}(A)$ in relation to minors of $A$? I have tried some cases starting from the definition of determinant (just like the proof of the Laplace expansion), but so far no luck. But I guess it is some kind of complementary minor in $A$.","It is by definition that entries of the adjugate matrix $\text{adj}(A)$ are the corresponding $(n-1)$-minors of $A$ (up to a sign). What can we say about the $k$-minor of $\text{adj}(A)$ in relation to minors of $A$? I have tried some cases starting from the definition of determinant (just like the proof of the Laplace expansion), but so far no luck. But I guess it is some kind of complementary minor in $A$.",,"['linear-algebra', 'matrices', 'laplace-expansion']"
57,How to reconstruct a symmetric matrix given the eigenvalues and eigenvectors?,How to reconstruct a symmetric matrix given the eigenvalues and eigenvectors?,,"I am trying to reconstruct a symmetric 3 x 3 matrix from just its eigenvalues and eigenvectors. I think the solution involves orthogonalizing two of the eigenvectors using the Gram-Schmidt procedure, but I am not sure why or how. Thanks.","I am trying to reconstruct a symmetric 3 x 3 matrix from just its eigenvalues and eigenvectors. I think the solution involves orthogonalizing two of the eigenvectors using the Gram-Schmidt procedure, but I am not sure why or how. Thanks.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
58,Why are these matrix row operations even allowed simultanously on more than one matrix?,Why are these matrix row operations even allowed simultanously on more than one matrix?,,"My Differential Equations book is going over finding the inverse of Matrices, and clearly I've forgotten my college algebra.  I have no idea why this works. The first example gives this: $\begin{bmatrix}1 & -1 & -1\\3 & -1 & 2\\2 & 2 & 3\end{bmatrix}$ to give the identity matrix, which is of course  $\begin{bmatrix}1 & 0 & 0\\0 & 1 & 0\\0 & 0 & 1\end{bmatrix}$ The book says: Obtain zeros in the off diagonal position in the first (-3) times the first row to the second row and adding -2 times the first row to the third row. This generates the Matrices: $\begin{bmatrix}1 & -1 & -1\\0 & 2 & 5\\0 & 4 & 5\end{bmatrix} \begin{bmatrix}1 & 0 & 0\\-3 & 1 & 0\\-2 & 0 & 1\end{bmatrix}$ It Continues: Obtain a one in the diagonal position of the second column by multiplying the second row by $\frac{1}{2}$ . $\begin{bmatrix}1 & -1 & -1\\0 & 1 & \frac{5}{2}\\0 & 4 & 5\end{bmatrix} \begin{bmatrix}1 & 0 & 0\\-\frac{3}{2} & \frac{1}{2} & 0\\-2 & 0 & 1\end{bmatrix}$ And it proceeds: Obtain zeros in the off-diagonal positions inthe second column by adding the second row to the first row and adding (-4) times the second row to the third row. $\begin{bmatrix}1 & 0 & \frac{3}{2}\\0 & 1 & \frac{5}{2}\\0 & 0 & -5\end{bmatrix} \begin{bmatrix}-\frac{1}{2} & \frac{1}{2} & 0\\-\frac{3}{2} & \frac{1}{2} & 0\\4 & -2 & 1\end{bmatrix}$. This is where the set of rules I was constructing to repeat this breaks down completely.  However, in the first step, why are we allowed to add a number to the first column of a row being operated on in the left matrix to the second matrix, then multiply it by an arbitrary row? Then How is simply multiplying across rows simultaneously by a constant multiplicatively consistent? and worst of all in the final operation shown, why is the 4 not negative?  Why is anything other than the first column of the row being operated on being modified, unlike the other two additive operations that used a coefficient? I tried to show this for myself by constructing a matrix to represent $\bf{AB}$.  One matrix was constructed of elements $a_1,b_1...$ proceeding first down rows, not columns, and the second of such elements $a_2,b_2...$ It looked like thus: \begin{bmatrix}a_1a_2+b_2d_2+c_2g_2 & a_1b_2+b_1e_2 + c_1h_1 & a_1c_2+b_1f_2+c_1i_2\\... & ... & ...\\... & ... & ...\end{bmatrix} I would like to learn how to do this in row operations, because, well, it's faster, and several of these may be on the test.  However, in essence, all of the operations in the procedure seem arbitrary and I can't construct rules for repeating it.  Thank you for any help. By the way, the book is: Elementary Differential Equations , Ninth Edition, By Boyce and DiPrima.  The book is currently in it's tenth edition.","My Differential Equations book is going over finding the inverse of Matrices, and clearly I've forgotten my college algebra.  I have no idea why this works. The first example gives this: $\begin{bmatrix}1 & -1 & -1\\3 & -1 & 2\\2 & 2 & 3\end{bmatrix}$ to give the identity matrix, which is of course  $\begin{bmatrix}1 & 0 & 0\\0 & 1 & 0\\0 & 0 & 1\end{bmatrix}$ The book says: Obtain zeros in the off diagonal position in the first (-3) times the first row to the second row and adding -2 times the first row to the third row. This generates the Matrices: $\begin{bmatrix}1 & -1 & -1\\0 & 2 & 5\\0 & 4 & 5\end{bmatrix} \begin{bmatrix}1 & 0 & 0\\-3 & 1 & 0\\-2 & 0 & 1\end{bmatrix}$ It Continues: Obtain a one in the diagonal position of the second column by multiplying the second row by $\frac{1}{2}$ . $\begin{bmatrix}1 & -1 & -1\\0 & 1 & \frac{5}{2}\\0 & 4 & 5\end{bmatrix} \begin{bmatrix}1 & 0 & 0\\-\frac{3}{2} & \frac{1}{2} & 0\\-2 & 0 & 1\end{bmatrix}$ And it proceeds: Obtain zeros in the off-diagonal positions inthe second column by adding the second row to the first row and adding (-4) times the second row to the third row. $\begin{bmatrix}1 & 0 & \frac{3}{2}\\0 & 1 & \frac{5}{2}\\0 & 0 & -5\end{bmatrix} \begin{bmatrix}-\frac{1}{2} & \frac{1}{2} & 0\\-\frac{3}{2} & \frac{1}{2} & 0\\4 & -2 & 1\end{bmatrix}$. This is where the set of rules I was constructing to repeat this breaks down completely.  However, in the first step, why are we allowed to add a number to the first column of a row being operated on in the left matrix to the second matrix, then multiply it by an arbitrary row? Then How is simply multiplying across rows simultaneously by a constant multiplicatively consistent? and worst of all in the final operation shown, why is the 4 not negative?  Why is anything other than the first column of the row being operated on being modified, unlike the other two additive operations that used a coefficient? I tried to show this for myself by constructing a matrix to represent $\bf{AB}$.  One matrix was constructed of elements $a_1,b_1...$ proceeding first down rows, not columns, and the second of such elements $a_2,b_2...$ It looked like thus: \begin{bmatrix}a_1a_2+b_2d_2+c_2g_2 & a_1b_2+b_1e_2 + c_1h_1 & a_1c_2+b_1f_2+c_1i_2\\... & ... & ...\\... & ... & ...\end{bmatrix} I would like to learn how to do this in row operations, because, well, it's faster, and several of these may be on the test.  However, in essence, all of the operations in the procedure seem arbitrary and I can't construct rules for repeating it.  Thank you for any help. By the way, the book is: Elementary Differential Equations , Ninth Edition, By Boyce and DiPrima.  The book is currently in it's tenth edition.",,['matrices']
59,If $A$ is positive definite then so is $A^k$,If  is positive definite then so is,A A^k,"I know how to show the inverse of positive definite is positive definite but I don't know how to expand that. Suppose $A$ is positive definite then $A$ is invertible, so define $y=Ax$ for $x\neq 0$. Then $y^TA^{-1}y=x^TA^TA^{-1}Ax=x^TAx>0$, so the inverse of $A$ is positive definite. How can I show that for other powers of $A$?","I know how to show the inverse of positive definite is positive definite but I don't know how to expand that. Suppose $A$ is positive definite then $A$ is invertible, so define $y=Ax$ for $x\neq 0$. Then $y^TA^{-1}y=x^TA^TA^{-1}Ax=x^TAx>0$, so the inverse of $A$ is positive definite. How can I show that for other powers of $A$?",,"['linear-algebra', 'matrices']"
60,Proving $AD_1A^{-1}=D_2$,Proving,AD_1A^{-1}=D_2,"I want to prove that if $A$ is a permutation matrix, and $D_1$ is diagonal, than $AD_1A^{-1}=D_2$ where $D_2$ is also a diagonal matrix. I have worked out that $A^{-1}=A^T$ and I can see that the positions of $A$ are the same positions $AD_1$ has filled, but in $A$ they are $1$'s obviously and in $AD_1$ they are the elements of $D_1$ rearranged(by row I believe). Then $A^T$ is going to rearrange columns to the diagonal form, but I don't know how to prove this. Thanks.","I want to prove that if $A$ is a permutation matrix, and $D_1$ is diagonal, than $AD_1A^{-1}=D_2$ where $D_2$ is also a diagonal matrix. I have worked out that $A^{-1}=A^T$ and I can see that the positions of $A$ are the same positions $AD_1$ has filled, but in $A$ they are $1$'s obviously and in $AD_1$ they are the elements of $D_1$ rearranged(by row I believe). Then $A^T$ is going to rearrange columns to the diagonal form, but I don't know how to prove this. Thanks.",,"['linear-algebra', 'matrices', 'permutations']"
61,"""Simpler"" geometrical description","""Simpler"" geometrical description",,"So i was asked to find: Find the matrix that represents the linear transformation of the plane obtained by: reflecting in the line y = x, $\begin{bmatrix} 0&1 \\ 1&0 \end{bmatrix}$ then rotating anticlockwise through an angle of 45 degrees, $\begin{bmatrix} \frac{1}{\sqrt{2}}&-\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}} \end{bmatrix}$ and finally reflecting in the y axis.  $\begin{bmatrix} -1&0 \\ 0&1 \end{bmatrix}$ Give a simpler geometrical description of what this transformation does. Which i guess is just multiplying all the steps $$\begin{bmatrix} 0&1 \\ 1&0 \end{bmatrix}  \begin{bmatrix} \frac{1}{\sqrt{2}}&-\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}} \end{bmatrix}  \begin{bmatrix} -1&0 \\ 0&1 \end{bmatrix} = \begin{bmatrix} -\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}} \\ \frac{1}{-\sqrt{2}}&-\frac{1}{\sqrt{2}} \end{bmatrix}  $$ How do i give a simpler geometrical description of this other than what's already stated in the question: reflect in the line y=x, rotate through angle of 45 degree and reflect by the y axis. Is this some kind of inverse project it feels a lot like the geometric aspect of an inverse to its main function.","So i was asked to find: Find the matrix that represents the linear transformation of the plane obtained by: reflecting in the line y = x, $\begin{bmatrix} 0&1 \\ 1&0 \end{bmatrix}$ then rotating anticlockwise through an angle of 45 degrees, $\begin{bmatrix} \frac{1}{\sqrt{2}}&-\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}} \end{bmatrix}$ and finally reflecting in the y axis.  $\begin{bmatrix} -1&0 \\ 0&1 \end{bmatrix}$ Give a simpler geometrical description of what this transformation does. Which i guess is just multiplying all the steps $$\begin{bmatrix} 0&1 \\ 1&0 \end{bmatrix}  \begin{bmatrix} \frac{1}{\sqrt{2}}&-\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}} \end{bmatrix}  \begin{bmatrix} -1&0 \\ 0&1 \end{bmatrix} = \begin{bmatrix} -\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}} \\ \frac{1}{-\sqrt{2}}&-\frac{1}{\sqrt{2}} \end{bmatrix}  $$ How do i give a simpler geometrical description of this other than what's already stated in the question: reflect in the line y=x, rotate through angle of 45 degree and reflect by the y axis. Is this some kind of inverse project it feels a lot like the geometric aspect of an inverse to its main function.",,"['linear-algebra', 'matrices']"
62,How to use Cayley-Hamiltonian theorem in proving upper bound on linear space $W$?,How to use Cayley-Hamiltonian theorem in proving upper bound on linear space ?,W,"If $W = span(I,A,A^1,A^2, \dots)$. What is the upper bound on dimension of $W$? All matrices are $n \times n$. I know that the dim($W$) $\leq n$, by the Cayley-Hamiltonian theorem. However, I don't see how the Cayley-Hamiltonian (C-H) theorem is used to show this. From what I understand, the C-H theorem says once you get the characteristic polynomial equation for matrix $M$, $p(\lambda)$, then $p(M) =$ the zero matrix. I'm not sure how C-H can be used in showing that $dim(W) \leq n$. Second question: If $A=$ zero matrix, then would the dimension of $W = 1$ or $2$?","If $W = span(I,A,A^1,A^2, \dots)$. What is the upper bound on dimension of $W$? All matrices are $n \times n$. I know that the dim($W$) $\leq n$, by the Cayley-Hamiltonian theorem. However, I don't see how the Cayley-Hamiltonian (C-H) theorem is used to show this. From what I understand, the C-H theorem says once you get the characteristic polynomial equation for matrix $M$, $p(\lambda)$, then $p(M) =$ the zero matrix. I'm not sure how C-H can be used in showing that $dim(W) \leq n$. Second question: If $A=$ zero matrix, then would the dimension of $W = 1$ or $2$?",,"['linear-algebra', 'matrices', 'characteristic-functions']"
63,Finding a kernel and an image of $T^2$,Finding a kernel and an image of,T^2,"Let $T$ be a linear transformation $T: \mathbb{R}^4 \to \mathbb{R}^4$ that is defined by: $$T\begin{pmatrix}x\\y\\z\\u\end{pmatrix}=\begin{pmatrix}0\\z\\y\\x\end{pmatrix}$$ Find the kernel and image of $T^2$. I will tell you exactly where i'm stuck, after applying $T$ on $(0,z,y,x)$ i'm pretty sure i'll be getting $(0,y,z,0)$, to find the kernel i will compare it to $(0,0,0,0)$ which will tell me that $z = 0$ and $y = 0$. Now, this is what i am unsure of, should i write the $z = 0$ and $y = 0$ in the original $T$? should i write it on $(x,y,z,u)$ thus getting $(x,0,0,u)$ or should i forget the original and apply it on $(0,z,y,x)$ thus getting $(0,0,0,x)$. Kind of confused about that little part. Thanks in advance !","Let $T$ be a linear transformation $T: \mathbb{R}^4 \to \mathbb{R}^4$ that is defined by: $$T\begin{pmatrix}x\\y\\z\\u\end{pmatrix}=\begin{pmatrix}0\\z\\y\\x\end{pmatrix}$$ Find the kernel and image of $T^2$. I will tell you exactly where i'm stuck, after applying $T$ on $(0,z,y,x)$ i'm pretty sure i'll be getting $(0,y,z,0)$, to find the kernel i will compare it to $(0,0,0,0)$ which will tell me that $z = 0$ and $y = 0$. Now, this is what i am unsure of, should i write the $z = 0$ and $y = 0$ in the original $T$? should i write it on $(x,y,z,u)$ thus getting $(x,0,0,u)$ or should i forget the original and apply it on $(0,z,y,x)$ thus getting $(0,0,0,x)$. Kind of confused about that little part. Thanks in advance !",,"['linear-algebra', 'matrices', 'linear-transformations']"
64,Inequality $|A+B|_m\leq|A|_m+|B|_m$ on square matrices,Inequality  on square matrices,|A+B|_m\leq|A|_m+|B|_m,"Consider $n\times n$ real matrices $A$ and $B$. If $|A|_m$ denotes the modulus matrix of $A=[a_{i,j}]_{n\times n}$, and is defined as $|A|_m := [|a_{i,j}|]_{n\times n}$, prove that $|A+B|_m\leq|A|_m+|B|_m$. (We mean by $N\leq M$ that matrix $(N-M) $ is a negative semi-definite matix) I have seen this inequality in a paper, where no proof is provided.","Consider $n\times n$ real matrices $A$ and $B$. If $|A|_m$ denotes the modulus matrix of $A=[a_{i,j}]_{n\times n}$, and is defined as $|A|_m := [|a_{i,j}|]_{n\times n}$, prove that $|A+B|_m\leq|A|_m+|B|_m$. (We mean by $N\leq M$ that matrix $(N-M) $ is a negative semi-definite matix) I have seen this inequality in a paper, where no proof is provided.",,"['linear-algebra', 'matrices', 'matrix-equations']"
65,Question regarding matrices and determinants.,Question regarding matrices and determinants.,,"The question states : If $A$ is a non-singular square matrix satisfying $AB-BA=A$, then prove that $|B+I|=|B-I|$. Note : $1.$ Here $I$ is the Identity matrix. $2.$ Modulus sign means determinant. $|A|=\det{A}$. I have never encountered questions where there was addition or subtraction inside a determinant. My first thought was to use the property $|A|.|B|=|A.B|$. So, I wrote $$|B+I| = \frac{1}{|A|}.|BA+A| $$ (should it be $|BA+A|$ or $|AB+A|$ or am I wrong altogether?) Anyway, that leads to $$|B+I|=\frac{1}{|A|}.|AB-A+A|$$ or $$|B+I|=|B|.$$ Obviously, this is not what the question asked. So I must have made a mistake. Where did I make a mistake and what is a good way to approach this question?","The question states : If $A$ is a non-singular square matrix satisfying $AB-BA=A$, then prove that $|B+I|=|B-I|$. Note : $1.$ Here $I$ is the Identity matrix. $2.$ Modulus sign means determinant. $|A|=\det{A}$. I have never encountered questions where there was addition or subtraction inside a determinant. My first thought was to use the property $|A|.|B|=|A.B|$. So, I wrote $$|B+I| = \frac{1}{|A|}.|BA+A| $$ (should it be $|BA+A|$ or $|AB+A|$ or am I wrong altogether?) Anyway, that leads to $$|B+I|=\frac{1}{|A|}.|AB-A+A|$$ or $$|B+I|=|B|.$$ Obviously, this is not what the question asked. So I must have made a mistake. Where did I make a mistake and what is a good way to approach this question?",,"['matrices', 'determinant']"
66,Proof of relation between maximum element and induced $p$-norm of a matrix,Proof of relation between maximum element and induced -norm of a matrix,p,"If true, prove the identity: $$ ||A|| \ge \max\limits_{i,j}|a_{ij}| $$ $||.||$ is any induced/operator norm. Edit: The identity is true only for operator norm induced by $p$-norm for vectors. I found this property in a presentation of singular values and matrix norms. This property could be useful for me in my work but I am not able to prove it.","If true, prove the identity: $$ ||A|| \ge \max\limits_{i,j}|a_{ij}| $$ $||.||$ is any induced/operator norm. Edit: The identity is true only for operator norm induced by $p$-norm for vectors. I found this property in a presentation of singular values and matrix norms. This property could be useful for me in my work but I am not able to prove it.",,"['matrices', 'normed-spaces', 'vectors']"
67,"Using the associativity of matrix multiplication to prove that if A^2015 is invertible, then A is also invertible","Using the associativity of matrix multiplication to prove that if A^2015 is invertible, then A is also invertible",,"I have the matrix $A^2$$^0$$^1$$^5$ and have to use the associativity of matrix multiplication to prove that if that matrix is invertible, then so is A. I know how to find the inverse of a matrix (check the determinant isn't 0, and then use elementary row operations) but any hints on this proof as I have no idea how to start or where to go. For part 2, the question says ""Consider an m × n matrix A. Show that A has a left inverse if and only if $A^T$ has a right inverse"" Again no idea how to start so any tips/full proofs would be great","I have the matrix $A^2$$^0$$^1$$^5$ and have to use the associativity of matrix multiplication to prove that if that matrix is invertible, then so is A. I know how to find the inverse of a matrix (check the determinant isn't 0, and then use elementary row operations) but any hints on this proof as I have no idea how to start or where to go. For part 2, the question says ""Consider an m × n matrix A. Show that A has a left inverse if and only if $A^T$ has a right inverse"" Again no idea how to start so any tips/full proofs would be great",,"['linear-algebra', 'abstract-algebra', 'matrices']"
68,A matrix is an orthogonal projection if idempotent and symmetric.,A matrix is an orthogonal projection if idempotent and symmetric.,,I have a matrix $A=\mathbf{v}\mathbf{v}^t$ where v is a vector in $\mathbb{R}^n$ with magnitude $1$.  I have to prove that $A$ represents an orthogonal projection onto span$\{\mathbf{v}\}$. I have shown that $A$ is symmetric and idempotent with rank $1$ but I am not sure how to go from those properties to it being an orthogonal projection. Any help would be much appreciated! Thank you.,I have a matrix $A=\mathbf{v}\mathbf{v}^t$ where v is a vector in $\mathbb{R}^n$ with magnitude $1$.  I have to prove that $A$ represents an orthogonal projection onto span$\{\mathbf{v}\}$. I have shown that $A$ is symmetric and idempotent with rank $1$ but I am not sure how to go from those properties to it being an orthogonal projection. Any help would be much appreciated! Thank you.,,['matrices']
69,Let A be a symmetric positive definite matrix. Find a matrix B such that $B^2=A$,Let A be a symmetric positive definite matrix. Find a matrix B such that,B^2=A,"I believe this question is the same as asking find matrix B to be the square root of the matrix A. $B=\sqrt A$. Since the problem is not specific I am thinking to solve it in the general case by diagonalizing the matrix and find the eigenvalues and then take the square. Or, should I just take an example of a matrix A that is symmetric positive semidefinite and find $\sqrt A$ ? Any thoughts?","I believe this question is the same as asking find matrix B to be the square root of the matrix A. $B=\sqrt A$. Since the problem is not specific I am thinking to solve it in the general case by diagonalizing the matrix and find the eigenvalues and then take the square. Or, should I just take an example of a matrix A that is symmetric positive semidefinite and find $\sqrt A$ ? Any thoughts?",,"['matrices', 'eigenvalues-eigenvectors']"
70,Help with this matrix identity,Help with this matrix identity,,"I am looking at a paper which has the following manipulation: $$ \exp \big(\sum_i(y_i - Ax_i)^T V^{-1}(y_i - Ax_i)\big) $$ Here $A$ is a matrix. $V$ is a symmetric, positive definite matrix (inverse of a covariance i.e. a precision matrix). The paper goes on to write this as: $$ \exp\big(\operatorname{tr}(V^{-1}(Y-Ax)(Y-Ax)^T)\big) $$ However, it is not clear to me how this comes about! Can someone point me to what matrix identity has been used or guide to some proof of this?","I am looking at a paper which has the following manipulation: $$ \exp \big(\sum_i(y_i - Ax_i)^T V^{-1}(y_i - Ax_i)\big) $$ Here $A$ is a matrix. $V$ is a symmetric, positive definite matrix (inverse of a covariance i.e. a precision matrix). The paper goes on to write this as: $$ \exp\big(\operatorname{tr}(V^{-1}(Y-Ax)(Y-Ax)^T)\big) $$ However, it is not clear to me how this comes about! Can someone point me to what matrix identity has been used or guide to some proof of this?",,"['linear-algebra', 'matrices']"
71,Projection onto subspaces - point to line projection,Projection onto subspaces - point to line projection,,"In the following document about projection onto subspaces, the author is computing the transformation matrix to project a vector $b$ onto a line formed by vector $a$. Since the projected vector $p$ is on the $a$ line, therefore it can be expressed as $p = \bar{x}a$. The projection ""line"" which is the vector $b - p$ is orthogonal to $a$. This means the dot-product of these two is zero: $$a \dot{} (b - p) = a^T(b - \bar{x}a) = 0$$ It comes to the conclusion that $\bar{x} = \frac{a^T b}{a^T a}$ so $$p = ax = a \frac{a^T b}{a^T a}$$ (Which I fully understand) Then the objective is to find the projection matrix P such as $p = P b $. The author wrote that $p = \bar{x}a = \frac{aa^Ta}{a^Ta}$ and so $P = \frac{aa^T}{a^Ta}$ How does he deduced this last part? I cannot see how he goes from  $$p = \bar{x} a = \frac{a^T b}{a^T a} a$$ to $$p = \frac{aa^Ta}{a^Ta}$$ And even then, why would $P$ be $\frac{aa^T}{a^Ta}$ since there are absolutely no $b$ in this last expression to identify a $p = Pb$ ?","In the following document about projection onto subspaces, the author is computing the transformation matrix to project a vector $b$ onto a line formed by vector $a$. Since the projected vector $p$ is on the $a$ line, therefore it can be expressed as $p = \bar{x}a$. The projection ""line"" which is the vector $b - p$ is orthogonal to $a$. This means the dot-product of these two is zero: $$a \dot{} (b - p) = a^T(b - \bar{x}a) = 0$$ It comes to the conclusion that $\bar{x} = \frac{a^T b}{a^T a}$ so $$p = ax = a \frac{a^T b}{a^T a}$$ (Which I fully understand) Then the objective is to find the projection matrix P such as $p = P b $. The author wrote that $p = \bar{x}a = \frac{aa^Ta}{a^Ta}$ and so $P = \frac{aa^T}{a^Ta}$ How does he deduced this last part? I cannot see how he goes from  $$p = \bar{x} a = \frac{a^T b}{a^T a} a$$ to $$p = \frac{aa^Ta}{a^Ta}$$ And even then, why would $P$ be $\frac{aa^T}{a^Ta}$ since there are absolutely no $b$ in this last expression to identify a $p = Pb$ ?",,"['linear-algebra', 'matrices', 'projective-space']"
72,What is this norm $\|A\|_*$ called and what is it?,What is this norm  called and what is it?,\|A\|_*,"In my lecture notes, I have found the notation $\|A\|_*$ for a matrix norm. Do you know the name of this norm (such that I can read the definition of it), or do you even know the definition of it? Thank you very much.","In my lecture notes, I have found the notation for a matrix norm. Do you know the name of this norm (such that I can read the definition of it), or do you even know the definition of it? Thank you very much.",\|A\|_*,"['matrices', 'notation', 'normed-spaces', 'matrix-norms', 'nuclear-norm']"
73,"If $A$ is a square matrix that is linearly independent, is $AA$?","If  is a square matrix that is linearly independent, is ?",A AA,"I'm just not sure how to start this problem from Linear Algebra Done Wrong. The problem is to prove that if the columns of $A$, square matrix, are linearly independent, then the columns of $A^2$ = $AA$ are also linearly independent. I'm mostly just not sure how to start this proof.","I'm just not sure how to start this problem from Linear Algebra Done Wrong. The problem is to prove that if the columns of $A$, square matrix, are linearly independent, then the columns of $A^2$ = $AA$ are also linearly independent. I'm mostly just not sure how to start this proof.",,"['linear-algebra', 'matrices']"
74,"What is the ratio between $|\langle Av,v \rangle |$ and $\|v\|^2$ where $A$ is an $n\times n$ unitary matrix and $v\in \mathbb C^n$?",What is the ratio between  and  where  is an  unitary matrix and ?,"|\langle Av,v \rangle | \|v\|^2 A n\times n v\in \mathbb C^n","I'm trying to determine the ratio between $|\langle Av,v \rangle |$ and $\|v\|^2$ where $A$ is an $n\times n$ unitary matrix and $v\in \mathbb C^n$. In particular I'm trying to determine whether $|\langle Av,v \rangle | \le \|v\|^2$ or $|\langle Av,v \rangle | \gt \|v\|^2$. This is what I have so far: Let $w,u\in \mathbb C^n$ such that $w+u=v$ then: Computing $|\langle Av,v \rangle |$ : $$|\langle Av,v \rangle | =|\langle A(w+u),w+u \rangle |=|\langle Aw+Au,w+u \rangle | = |\langle Aw,w \rangle + \langle Aw,u \rangle + \langle Au,w \rangle + \langle Au,u \rangle |$$ Computing $\|v\|^2$ : $$\|v\|^2 = \langle v,v \rangle = \langle w+u,w+u \rangle = \langle w,w \rangle + \langle w,u \rangle + \langle u,w \rangle + \langle u,u \rangle$$ I'm not quite sure how to continue. I can use the exchange lemma and the fact that $A^{-1} = A^*$ because $A$ is an unitary matrix but I can't see how it helps me to simply the first equation. Note : $\langle \cdot \rangle$ denotes the dot product.","I'm trying to determine the ratio between $|\langle Av,v \rangle |$ and $\|v\|^2$ where $A$ is an $n\times n$ unitary matrix and $v\in \mathbb C^n$. In particular I'm trying to determine whether $|\langle Av,v \rangle | \le \|v\|^2$ or $|\langle Av,v \rangle | \gt \|v\|^2$. This is what I have so far: Let $w,u\in \mathbb C^n$ such that $w+u=v$ then: Computing $|\langle Av,v \rangle |$ : $$|\langle Av,v \rangle | =|\langle A(w+u),w+u \rangle |=|\langle Aw+Au,w+u \rangle | = |\langle Aw,w \rangle + \langle Aw,u \rangle + \langle Au,w \rangle + \langle Au,u \rangle |$$ Computing $\|v\|^2$ : $$\|v\|^2 = \langle v,v \rangle = \langle w+u,w+u \rangle = \langle w,w \rangle + \langle w,u \rangle + \langle u,w \rangle + \langle u,u \rangle$$ I'm not quite sure how to continue. I can use the exchange lemma and the fact that $A^{-1} = A^*$ because $A$ is an unitary matrix but I can't see how it helps me to simply the first equation. Note : $\langle \cdot \rangle$ denotes the dot product.",,"['linear-algebra', 'matrices']"
75,Remove the Kronecker operator in $\mathrm{trace((\Sigma^{-1}\otimes S^{-1})A})$,Remove the Kronecker operator in,\mathrm{trace((\Sigma^{-1}\otimes S^{-1})A}),"I am not sure if I can remove the Kronecker operator in the following formula $$\mathrm{trace((\Sigma^{-1}\otimes S^{-1})A}),$$ where $\Sigma,S$ are positive-semidefinite and symmetric, and $A$ is symmetric. Any help would be appreciated. Update: @greg claimed in the answer below that we can "" exactly represent $A$ as a finite sum of Kronecker products ."" I am wondering how to obtain this exact representation. To be more specific, I have $$A=\left(\begin{array}{cc} 0 & 0\\ 0 & R \end{array}\right),  $$ where $R$ is a symmetric and positive semi-definite matrix, and its dimension is $k \times k$, $k=1,\dots,\mathrm{ncol}(A)$. The ultimate goal is to calculate the derivative $$\frac{\partial \mathrm{trace((\Sigma^{-1}\otimes S^{-1})A})}{\partial \Sigma}.$$ This is related to question: Derivative involving the trace of a Kronecker product . I did read Van Loan and Pitsianis (1993) , but did not find a solution.","I am not sure if I can remove the Kronecker operator in the following formula $$\mathrm{trace((\Sigma^{-1}\otimes S^{-1})A}),$$ where $\Sigma,S$ are positive-semidefinite and symmetric, and $A$ is symmetric. Any help would be appreciated. Update: @greg claimed in the answer below that we can "" exactly represent $A$ as a finite sum of Kronecker products ."" I am wondering how to obtain this exact representation. To be more specific, I have $$A=\left(\begin{array}{cc} 0 & 0\\ 0 & R \end{array}\right),  $$ where $R$ is a symmetric and positive semi-definite matrix, and its dimension is $k \times k$, $k=1,\dots,\mathrm{ncol}(A)$. The ultimate goal is to calculate the derivative $$\frac{\partial \mathrm{trace((\Sigma^{-1}\otimes S^{-1})A})}{\partial \Sigma}.$$ This is related to question: Derivative involving the trace of a Kronecker product . I did read Van Loan and Pitsianis (1993) , but did not find a solution.",,"['linear-algebra', 'matrices', 'derivatives', 'trace', 'kronecker-product']"
76,How to find all orthogonal matrices which commute with a given symmetric matrix?,How to find all orthogonal matrices which commute with a given symmetric matrix?,,"Suppose we have a symmetric matrix $H$. I'd like to find all the orthogonal matrices $S_i$, which commute with it. Particularly I'm interested in the set of $S_i$, which are linearly independent from the others in the set. Currently to find the first such matrix I just minimize the difference $SH-HS$ with the constraint that $SS^T=I$. But even for not too large matrices like $16\times16$ this takes quite a lot of time to minimize it. Are there any efficient numerical procedures to find these matrices?","Suppose we have a symmetric matrix $H$. I'd like to find all the orthogonal matrices $S_i$, which commute with it. Particularly I'm interested in the set of $S_i$, which are linearly independent from the others in the set. Currently to find the first such matrix I just minimize the difference $SH-HS$ with the constraint that $SS^T=I$. But even for not too large matrices like $16\times16$ this takes quite a lot of time to minimize it. Are there any efficient numerical procedures to find these matrices?",,"['matrices', 'numerical-linear-algebra']"
77,The determinant of a matrix exponential?,The determinant of a matrix exponential?,,"So I know that if a matrix $A = e^{B}$, then $det(A) = e^{tr(B)}$. I'm wondering, is the converse true? Right now I have a matrix and I know its determinant is $e^{tr(B)}$. So can I conclude that my matrix is $e^{B}$?","So I know that if a matrix $A = e^{B}$, then $det(A) = e^{tr(B)}$. I'm wondering, is the converse true? Right now I have a matrix and I know its determinant is $e^{tr(B)}$. So can I conclude that my matrix is $e^{B}$?",,"['real-analysis', 'linear-algebra', 'matrices']"
78,Finding the corresponding Perron eigenvalue,Finding the corresponding Perron eigenvalue,,"Find the Perron root and the corresponding Perron eigenvector of A. $\begin{bmatrix} 0 &1 &1 \\ 1&0&1 \\ 1&1&0 \end{bmatrix}$ I figured out the Perron root which happens to be $ \lambda = 2 $. And I tried to figure out the eigenvector and got $\begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$, but that turned out to be incorrect. Can someone show me how to find the Perron eigenvector for the Perron root 2?","Find the Perron root and the corresponding Perron eigenvector of A. $\begin{bmatrix} 0 &1 &1 \\ 1&0&1 \\ 1&1&0 \end{bmatrix}$ I figured out the Perron root which happens to be $ \lambda = 2 $. And I tried to figure out the eigenvector and got $\begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$, but that turned out to be incorrect. Can someone show me how to find the Perron eigenvector for the Perron root 2?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
79,Example of a non singular square matrix such that $A+A^{-1} = 0$,Example of a non singular square matrix such that,A+A^{-1} = 0,"Is there any example of a non singular square matrix $A$ such that $A+A^{-1} = 0$? Are they any specific type of matrices or can these be found under any category of matrices (such as symmetric, hermitian, unitary etc.)? Thanks...","Is there any example of a non singular square matrix $A$ such that $A+A^{-1} = 0$? Are they any specific type of matrices or can these be found under any category of matrices (such as symmetric, hermitian, unitary etc.)? Thanks...",,"['linear-algebra', 'matrices']"
80,Can someone direct me to prove the following three claims about a rotation group $SO(3)$ and its Lie Algebra?,Can someone direct me to prove the following three claims about a rotation group  and its Lie Algebra?,SO(3),"In class my prof made three claims about a group and its Lie algebra. I cannot find direct reference to these claims because they are delivered in verbatim (im not even sure if I have them jogged down 100% correctly). Can someone please help me identify these three properties? Claim 1: $R\in SO(3)$, so any rotation matrix $R$ is the matrix exponential skew symmetric matrix $M$ Claim 2: Any skew symmetric 3x3 matrix is determined by 3 distinct numbers i.e. a vector Claim 3: $\omega$ be a vector in $R^3$. Let $v$ = normalized $\omega$ (?) and $\theta$ = length of $\omega$. Then, $\omega$ = $v$ $\theta$. Now, it can be shown that if you take the identity matrix and rotate it by the angle $\theta$ around the axis span{$v$} using the right-hand rule, the result is precisely the matrix $R$ Please direct me to appropriate reference or if these are in fact simple proofs, please show me how these are true. Thank you for your time!","In class my prof made three claims about a group and its Lie algebra. I cannot find direct reference to these claims because they are delivered in verbatim (im not even sure if I have them jogged down 100% correctly). Can someone please help me identify these three properties? Claim 1: $R\in SO(3)$, so any rotation matrix $R$ is the matrix exponential skew symmetric matrix $M$ Claim 2: Any skew symmetric 3x3 matrix is determined by 3 distinct numbers i.e. a vector Claim 3: $\omega$ be a vector in $R^3$. Let $v$ = normalized $\omega$ (?) and $\theta$ = length of $\omega$. Then, $\omega$ = $v$ $\theta$. Now, it can be shown that if you take the identity matrix and rotate it by the angle $\theta$ around the axis span{$v$} using the right-hand rule, the result is precisely the matrix $R$ Please direct me to appropriate reference or if these are in fact simple proofs, please show me how these are true. Thank you for your time!",,"['matrices', 'lie-groups', 'rotations']"
81,Find operation on two matrices from given scalar [closed],Find operation on two matrices from given scalar [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question If $A = \begin{bmatrix} a_{11}&a_{12}\\a_{21}&a_{22} \end{bmatrix}$ and $B = \begin{bmatrix} b_{11}&b_{12}\\b_{21}&b_{22} \end{bmatrix}$, what operation on matrices will result in the scalar $$-a_{11} b_{22} - a_{22} b_{11} + a_{12} b_{12} + a_{21} b_{21}$$ ? Note: I stumbled upon this question while trying to find eigenvalues of a transfer matrix in physics. I really wasn't sure what the answer should be but was mainly looking for a way to compactify notation.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question If $A = \begin{bmatrix} a_{11}&a_{12}\\a_{21}&a_{22} \end{bmatrix}$ and $B = \begin{bmatrix} b_{11}&b_{12}\\b_{21}&b_{22} \end{bmatrix}$, what operation on matrices will result in the scalar $$-a_{11} b_{22} - a_{22} b_{11} + a_{12} b_{12} + a_{21} b_{21}$$ ? Note: I stumbled upon this question while trying to find eigenvalues of a transfer matrix in physics. I really wasn't sure what the answer should be but was mainly looking for a way to compactify notation.",,"['linear-algebra', 'matrices']"
82,Converting a matrix to the nearest positive definite matrix,Converting a matrix to the nearest positive definite matrix,,I have a matrix  $A = \begin{bmatrix}  634.156 & 0 & 755912.06 \\  0 & 1426.8604 & 598151.25\\  755912.06 & 598151.25 & 1.1517e9\\  \end{bmatrix} $ with eigenvalues  $\begin{bmatrix}  1.15179e9\\  1254.2858\\  -1.52588e-5\\ \end{bmatrix}$. I want to calculate the Cholesky decomposition of the matrix A but A is not positive definite (the last eigenvalue of A is negative). How can I transform A into a nearest positive definite matrix inorder to calculate the Cholesky decomposition?,I have a matrix  $A = \begin{bmatrix}  634.156 & 0 & 755912.06 \\  0 & 1426.8604 & 598151.25\\  755912.06 & 598151.25 & 1.1517e9\\  \end{bmatrix} $ with eigenvalues  $\begin{bmatrix}  1.15179e9\\  1254.2858\\  -1.52588e-5\\ \end{bmatrix}$. I want to calculate the Cholesky decomposition of the matrix A but A is not positive definite (the last eigenvalue of A is negative). How can I transform A into a nearest positive definite matrix inorder to calculate the Cholesky decomposition?,,"['matrices', 'matrix-calculus', 'matrix-decomposition']"
83,Transforming a matrix A into a zero matrix using finitely many steps.,Transforming a matrix A into a zero matrix using finitely many steps.,,Let $A$ be a $m\times n$ matrix whose entries are positive integers. A step consist of transforming the matrix either by multiplying every entry of a row by $2$ or subtracting $1$ from every entry of a column. My question is can we transform $A$ into the zero matrix in finitely many steps??,Let $A$ be a $m\times n$ matrix whose entries are positive integers. A step consist of transforming the matrix either by multiplying every entry of a row by $2$ or subtracting $1$ from every entry of a column. My question is can we transform $A$ into the zero matrix in finitely many steps??,,"['matrices', 'recreational-mathematics']"
84,XA+B=X will have a unique solution...,XA+B=X will have a unique solution...,,"I am currently stuck on this question: Let A and B be n x n matrices. Show that if none of the eigenvalues of A are equal to 1, then the matrix equation XA+B=X will have a unique solution. I've heard that if the eigenvalues of a matrix are not equal to 1, then it implies a unique solution. However, I'm confused how to take this one step further and prove that XA+B=X will have a unique solution.","I am currently stuck on this question: Let A and B be n x n matrices. Show that if none of the eigenvalues of A are equal to 1, then the matrix equation XA+B=X will have a unique solution. I've heard that if the eigenvalues of a matrix are not equal to 1, then it implies a unique solution. However, I'm confused how to take this one step further and prove that XA+B=X will have a unique solution.",,"['linear-algebra', 'matrices']"
85,representation of a matrix norm of the inverse of a matrix,representation of a matrix norm of the inverse of a matrix,,"Let $A$ be an $n$ by $n$ nonsingular matrix and suppose that a matrix norm $|||\cdot|||$ is induced by the vector norm $\lVert \cdot \rVert$ on $\mathbb{C}^n$. Show that   $$|||A^{-1}||| = \frac1{\min_{\lVert x \rVert=1}\|Ax\|}.$$ As in below picture I've shown one direction. However, I cannot show the converse direction. How can I do this? My try for $\le$: Let $x$ such that $\|x\| = 1$ then $$1 = \|x\| = \|A^{-1}Ax\| \le ||| A^{-1} ||| \| Ax \| \\  \Rightarrow \frac1{\|Ax\|} \le ||| A^{-1} ||| \\ \Rightarrow \frac1{\min_{\|x\|=1} \|Ax\|} \le ||| A^{-1} |||$$","Let $A$ be an $n$ by $n$ nonsingular matrix and suppose that a matrix norm $|||\cdot|||$ is induced by the vector norm $\lVert \cdot \rVert$ on $\mathbb{C}^n$. Show that   $$|||A^{-1}||| = \frac1{\min_{\lVert x \rVert=1}\|Ax\|}.$$ As in below picture I've shown one direction. However, I cannot show the converse direction. How can I do this? My try for $\le$: Let $x$ such that $\|x\| = 1$ then $$1 = \|x\| = \|A^{-1}Ax\| \le ||| A^{-1} ||| \| Ax \| \\  \Rightarrow \frac1{\|Ax\|} \le ||| A^{-1} ||| \\ \Rightarrow \frac1{\min_{\|x\|=1} \|Ax\|} \le ||| A^{-1} |||$$",,"['linear-algebra', 'matrices']"
86,Jacobson radical of a certain ring of matrices,Jacobson radical of a certain ring of matrices,,"Let $A$ be the $\mathbb{C}$-subalgebra of $M_4(\mathbb C)$ consisting of matrices of the form   \begin{pmatrix} * & * & * & *\\ * & * & * &*\\ 0 & 0 & *&0 \\ 0 & 0&* &* \end{pmatrix}   Then how would I find the Jacobson radical $J(A)$ of $A$? I don't really understand finding the Jacobson radical of $\mathbb{C}$-subalgebras. Any help on this would be greatly appreciated, thank you!","Let $A$ be the $\mathbb{C}$-subalgebra of $M_4(\mathbb C)$ consisting of matrices of the form   \begin{pmatrix} * & * & * & *\\ * & * & * &*\\ 0 & 0 & *&0 \\ 0 & 0&* &* \end{pmatrix}   Then how would I find the Jacobson radical $J(A)$ of $A$? I don't really understand finding the Jacobson radical of $\mathbb{C}$-subalgebras. Any help on this would be greatly appreciated, thank you!",,"['abstract-algebra', 'matrices', 'ring-theory']"
87,"What is the meaning of ""Hermitian""?","What is the meaning of ""Hermitian""?",,"Google search-bar gives the definition of Hermitian as: Hermitian: denoting or relating to a matrix in which those pairs of elements that are symmetrically placed with respect to the principal diagonal are complex conjugates I have thought that Hermitian was synonymous with ""real"", meaning, if the matrix ( A , for example) is Hermitian then that means there are no complex values in the matrix. I also believe it means the complex conjugate of the matrix is equal to the matrix like so: $$A = A^\dagger.$$ However, there also exist Hermitian functions (which are complex?!) and the Hermitian operator (does not have to be real). Could  someone please tell me what does the word ""Hermitian"" mean and what are the differences between the three: Hermitian matrix, Hermitian function, and Hermitian Operator? I am confused. (PS: Please feel free to correct me if I have tagged this question incorrectly.)","Google search-bar gives the definition of Hermitian as: Hermitian: denoting or relating to a matrix in which those pairs of elements that are symmetrically placed with respect to the principal diagonal are complex conjugates I have thought that Hermitian was synonymous with ""real"", meaning, if the matrix ( A , for example) is Hermitian then that means there are no complex values in the matrix. I also believe it means the complex conjugate of the matrix is equal to the matrix like so: $$A = A^\dagger.$$ However, there also exist Hermitian functions (which are complex?!) and the Hermitian operator (does not have to be real). Could  someone please tell me what does the word ""Hermitian"" mean and what are the differences between the three: Hermitian matrix, Hermitian function, and Hermitian Operator? I am confused. (PS: Please feel free to correct me if I have tagged this question incorrectly.)",,"['matrices', 'quantum-mechanics']"
88,How adjacency matrix shows that the graph have no cycles?,How adjacency matrix shows that the graph have no cycles?,,"Let $G$ a directed graph and $A$ the corresponding adjacency matrix . Let denote the identity matrix with $I$. I've read in a wikipedia article , that the following statement is true. Question. Is it true, that $I-A$ matrix is invertible if and only if there is no directed cycle in $G$?","Let $G$ a directed graph and $A$ the corresponding adjacency matrix . Let denote the identity matrix with $I$. I've read in a wikipedia article , that the following statement is true. Question. Is it true, that $I-A$ matrix is invertible if and only if there is no directed cycle in $G$?",,"['linear-algebra', 'matrices', 'graph-theory', 'inverse', 'spectral-graph-theory']"
89,Positive definiteness of the matrix $A+B$,Positive definiteness of the matrix,A+B,"Let, $A$ & $B$ are $n\times n$ positive definite matrices & $I$ be the $n\times n$ identity matrix. Then which of the followings are positive definite? (a) $A+B$ (b) $ABA$ (c) $A^{2}+I$ (d) $AB$ I know that, $A^{2}+I$ is positive definite, as if $\lambda$ is an eigen value of $A$ then $(1+\lambda^2)$ is an eigen value of $A^2+I$. I think (d) is true.Suppose, $\lambda_{1}$ & $\lambda_{2}$ be two eigen values of $A_{2\times 2}$ matrix & $\beta_{1}$, $\beta_{2}$ be two eigen values of $B_{2\times 2}$ matrix. Now, $det(AB)=det(A)det(B)=\lambda_{1}.\lambda_{2}.\beta_{1}.\beta_{2}.$ As, $\lambda_{1},\lambda_{2},\beta_{1},\beta_{2}$ are all positive so eigen values of $AB$ are all positive, so $AB$ is positive definite. Similarly, $ABA$ is positive definite.But I am not sure & I have no idea about $A+B$.","Let, $A$ & $B$ are $n\times n$ positive definite matrices & $I$ be the $n\times n$ identity matrix. Then which of the followings are positive definite? (a) $A+B$ (b) $ABA$ (c) $A^{2}+I$ (d) $AB$ I know that, $A^{2}+I$ is positive definite, as if $\lambda$ is an eigen value of $A$ then $(1+\lambda^2)$ is an eigen value of $A^2+I$. I think (d) is true.Suppose, $\lambda_{1}$ & $\lambda_{2}$ be two eigen values of $A_{2\times 2}$ matrix & $\beta_{1}$, $\beta_{2}$ be two eigen values of $B_{2\times 2}$ matrix. Now, $det(AB)=det(A)det(B)=\lambda_{1}.\lambda_{2}.\beta_{1}.\beta_{2}.$ As, $\lambda_{1},\lambda_{2},\beta_{1},\beta_{2}$ are all positive so eigen values of $AB$ are all positive, so $AB$ is positive definite. Similarly, $ABA$ is positive definite.But I am not sure & I have no idea about $A+B$.",,"['linear-algebra', 'matrices']"
90,A limit-determinant question,A limit-determinant question,,"Let $d_n$ be the determinant of the $n\times n$ matrix whose entries, from left to right and then from top to bottom, are $\cos1,\cos2,\ldots,\cos n^2$. (For example, $$d_3= \begin{vmatrix} \cos1&\cos2&\cos3\\ \cos4&\cos5&\cos6\\ \cos7&\cos8&\cos9\\ \end{vmatrix}$$ The argument of $\cos$ is always in radians, not degrees.) Evaluate $\lim_{n\to\infty}d_n$. Interesting question, I don't know where to start.","Let $d_n$ be the determinant of the $n\times n$ matrix whose entries, from left to right and then from top to bottom, are $\cos1,\cos2,\ldots,\cos n^2$. (For example, $$d_3= \begin{vmatrix} \cos1&\cos2&\cos3\\ \cos4&\cos5&\cos6\\ \cos7&\cos8&\cos9\\ \end{vmatrix}$$ The argument of $\cos$ is always in radians, not degrees.) Evaluate $\lim_{n\to\infty}d_n$. Interesting question, I don't know where to start.",,"['matrices', 'limits', 'determinant']"
91,Is the set of matrices with rank at most $r$ closed? [duplicate],Is the set of matrices with rank at most  closed? [duplicate],r,"This question already has answers here : How to prove that the collection of rank-k matrices forms a closed subset of the space of matrices? (3 answers) Closed 9 years ago . The question is as follows: $\DeclareMathOperator{\rank}{rank}$ Is the set $S_r = \{A \in \Bbb R^{n \times n}: \rank(A) \leq r\}$ closed in $\Bbb R^{n \times n}$ in the Euclidean topology? I have a feeling that this is true, but I got stuck looking for a convincing argument.  Certainly this is true for $r = n-1$.  Perhaps it can be shown that $S_{r-1}$ is a closed subset of $S_r$? No neat tricks are coming to mind, but I would think that there must be one. This seems like the kind of thing that has a canonical answer, so links are welcome.","This question already has answers here : How to prove that the collection of rank-k matrices forms a closed subset of the space of matrices? (3 answers) Closed 9 years ago . The question is as follows: $\DeclareMathOperator{\rank}{rank}$ Is the set $S_r = \{A \in \Bbb R^{n \times n}: \rank(A) \leq r\}$ closed in $\Bbb R^{n \times n}$ in the Euclidean topology? I have a feeling that this is true, but I got stuck looking for a convincing argument.  Certainly this is true for $r = n-1$.  Perhaps it can be shown that $S_{r-1}$ is a closed subset of $S_r$? No neat tricks are coming to mind, but I would think that there must be one. This seems like the kind of thing that has a canonical answer, so links are welcome.",,"['linear-algebra', 'general-topology', 'matrices']"
92,Is there a 3D equivalent of a 2D matrix?,Is there a 3D equivalent of a 2D matrix?,,"Just thinking, is there a 3D 'equivalent' of a matrix. I know it's possible to get matrices that only have one row or column (i.e. vectors) thus making there a sort of 1D equivalent, but is there a 3D equivalent of a matrix?","Just thinking, is there a 3D 'equivalent' of a matrix. I know it's possible to get matrices that only have one row or column (i.e. vectors) thus making there a sort of 1D equivalent, but is there a 3D equivalent of a matrix?",,['matrices']
93,"Proof that $A^2 = A$ where A and B are square matrices , if $BA = B$ and $AB = A$. What did I do wrong?","Proof that  where A and B are square matrices , if  and . What did I do wrong?",A^2 = A BA = B AB = A,"The problem Given that for square matrices $A$ and $B$ of the same order, $AB = A$ $BA = B$ Prove that $A^{2} = A$. My proof $$ \text{Starting with the given condition ,}\\ BA = B\\ \text{Premultiplying } B^{-1} \text{ to both sides , we get ,}\\ (B^{-1} \cdot B )\cdot A = (B^{-1} \cdot B )\\ \text{so , }\ \ \ I \cdot A = I\\ \text{or , }\ \ \ A = I\\ \text{Therefore }\ \ \ A^2 = I^2 = I = A $$ According to my teacher , this is wrong and not how it should have been proved. I couldn't yet see what I did wrong , could some one please tell me where I made a mistake .","The problem Given that for square matrices $A$ and $B$ of the same order, $AB = A$ $BA = B$ Prove that $A^{2} = A$. My proof $$ \text{Starting with the given condition ,}\\ BA = B\\ \text{Premultiplying } B^{-1} \text{ to both sides , we get ,}\\ (B^{-1} \cdot B )\cdot A = (B^{-1} \cdot B )\\ \text{so , }\ \ \ I \cdot A = I\\ \text{or , }\ \ \ A = I\\ \text{Therefore }\ \ \ A^2 = I^2 = I = A $$ According to my teacher , this is wrong and not how it should have been proved. I couldn't yet see what I did wrong , could some one please tell me where I made a mistake .",,"['matrices', 'proof-verification']"
94,True or False: Matrices with linearly independent row and column vectors are square.,True or False: Matrices with linearly independent row and column vectors are square.,,"True or False: Matrices with linearly independent row and column vectors are square. Here is the answer of my textbook: True; if the row vectors are linearly independent then $\text{nullity}(A)=0$ and $\text{rank}(A)=n=\text{the number of rows}$. But since $\text{rank}(A)+\text{nullity}(A)=\text{the number of columns}$, $A$ must be square. Why must a matrice with linearly independent vectors have $\text{nullity}(A)=0$? That is where I lose track of the question. Are zero rows considered to be linearly dependent?","True or False: Matrices with linearly independent row and column vectors are square. Here is the answer of my textbook: True; if the row vectors are linearly independent then $\text{nullity}(A)=0$ and $\text{rank}(A)=n=\text{the number of rows}$. But since $\text{rank}(A)+\text{nullity}(A)=\text{the number of columns}$, $A$ must be square. Why must a matrice with linearly independent vectors have $\text{nullity}(A)=0$? That is where I lose track of the question. Are zero rows considered to be linearly dependent?",,"['linear-algebra', 'matrices', 'vector-spaces']"
95,Prove that Frobenius matrix norm is compatible with the vector norm,Prove that Frobenius matrix norm is compatible with the vector norm,,"Show that, the Frobenius matrix norm $||.||_F$ is compatible or consistent with a vector norm $||.||_2$ , that is, $||Ax||_2 \leq ||A||_F ||x||_2, \forall x \in \mathbb{R}^n$. Where $||A||_F = \sqrt{ \sum_{i,j=1}^N |a_{ij}|^2} $","Show that, the Frobenius matrix norm $||.||_F$ is compatible or consistent with a vector norm $||.||_2$ , that is, $||Ax||_2 \leq ||A||_F ||x||_2, \forall x \in \mathbb{R}^n$. Where $||A||_F = \sqrt{ \sum_{i,j=1}^N |a_{ij}|^2} $",,"['linear-algebra', 'matrices', 'normed-spaces', 'numerical-linear-algebra']"
96,Inverting the infinite matrix $+\mathbf{I}$ with entries $\mathbf{P}_{ij}={i-1\choose j-1}$ [closed],Inverting the infinite matrix  with entries  [closed],+\mathbf{I} \mathbf{P}_{ij}={i-1\choose j-1},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Let $ \mathbf{P}$ denote the ""infinite matrix"" $$ \left[ \begin{array}{ccccc}  1 & 0 & 0 & 0 & \dots \\ 1 & 1 & 0 & 0 & \dots \\ 1 & 2 & 1 & 0 & \dots \\ 1 & 3 & 3 & 1 & \dots \\ \vdots & \vdots & \vdots & \vdots & \ddots \\ \end{array} \right]$$ with entries $ \mathbf{P}_{ij} = \dbinom{i-1}{j-1}$ and let $ \mathbf{I}$ denote the ""infinite identity matrix.""  Compute the inverse of $ \mathbf{P} + \mathbf{I}$. This was not the initial attempt. I couldn't think of anything at first. But after some nudges, I tried to compute the $n\times n$ matrix $\mathbf{P}_n$. Now my first observation was $\det \mathbf{P}_n=1$. Now if I could show that the invertibility of $\mathbf{P}_n$ would be efficient. So we expand $\mathbf{P}_{n+1}$ by the last row, then it is obvious  that $\det \mathbf{P}_{n+1}=\det \mathbf{P}_n=1$. So invertibility is meaningful. But when I inverted for small values, I couldn't find any pattern. I can't think of a method to cook up the solution. Can someone help me? I see my method of thinking should have been presented and I apologise. I will add them later on. How to compute an inverse of an infinite matrix? And even if I can, what to do with it? Thanks for any help.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Let $ \mathbf{P}$ denote the ""infinite matrix"" $$ \left[ \begin{array}{ccccc}  1 & 0 & 0 & 0 & \dots \\ 1 & 1 & 0 & 0 & \dots \\ 1 & 2 & 1 & 0 & \dots \\ 1 & 3 & 3 & 1 & \dots \\ \vdots & \vdots & \vdots & \vdots & \ddots \\ \end{array} \right]$$ with entries $ \mathbf{P}_{ij} = \dbinom{i-1}{j-1}$ and let $ \mathbf{I}$ denote the ""infinite identity matrix.""  Compute the inverse of $ \mathbf{P} + \mathbf{I}$. This was not the initial attempt. I couldn't think of anything at first. But after some nudges, I tried to compute the $n\times n$ matrix $\mathbf{P}_n$. Now my first observation was $\det \mathbf{P}_n=1$. Now if I could show that the invertibility of $\mathbf{P}_n$ would be efficient. So we expand $\mathbf{P}_{n+1}$ by the last row, then it is obvious  that $\det \mathbf{P}_{n+1}=\det \mathbf{P}_n=1$. So invertibility is meaningful. But when I inverted for small values, I couldn't find any pattern. I can't think of a method to cook up the solution. Can someone help me? I see my method of thinking should have been presented and I apologise. I will add them later on. How to compute an inverse of an infinite matrix? And even if I can, what to do with it? Thanks for any help.",,"['linear-algebra', 'matrices', 'inverse', 'infinite-matrices']"
97,Lower bounds on eigenvalues of a symmetric matrix based on the diagonals,Lower bounds on eigenvalues of a symmetric matrix based on the diagonals,,"A symmetric matrix $A$ always has real eigenvalues. If I know the elements on the diagonals, is it possible to have a lower bound on the smallest eigenvalue? How sharp would this bound be? For now I only found a paper of Hemy WoIkowicz and George P. H. Styan titled as ""Bounds for Elgenvalues Using Traces"", however, their bounds require the trace of $A^2$ which needs the other entries. Is there any other bounds or references on this topic? p.s. I cannot assume that $A$ is positive-definite coz I know that the smallest eigenvalue is $0$ indeed.","A symmetric matrix $A$ always has real eigenvalues. If I know the elements on the diagonals, is it possible to have a lower bound on the smallest eigenvalue? How sharp would this bound be? For now I only found a paper of Hemy WoIkowicz and George P. H. Styan titled as ""Bounds for Elgenvalues Using Traces"", however, their bounds require the trace of $A^2$ which needs the other entries. Is there any other bounds or references on this topic? p.s. I cannot assume that $A$ is positive-definite coz I know that the smallest eigenvalue is $0$ indeed.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
98,Matrix Theory: Orthogonal Matrix,Matrix Theory: Orthogonal Matrix,,"Consider a vector    $$   \begin{array}{l}     a=(1,-3,0,1), \\     b=(1,2,0,-1), \\      c=(0,0,1,0)   \end{array} $$ Question: (a) A non-zero vector $u$ orthogonal to all three of $a, b, c$ (there are many, just find one). (b) Describe the set of all vectors with these properties using a parameter or parameters. Can anyone tell me how to solve this?","Consider a vector    $$   \begin{array}{l}     a=(1,-3,0,1), \\     b=(1,2,0,-1), \\      c=(0,0,1,0)   \end{array} $$ Question: (a) A non-zero vector $u$ orthogonal to all three of $a, b, c$ (there are many, just find one). (b) Describe the set of all vectors with these properties using a parameter or parameters. Can anyone tell me how to solve this?",,"['matrices', 'orthogonality']"
99,Trace minimization subject to constraints,Trace minimization subject to constraints,,"I have seen in an article that $ \min_{\mathbf{K}} \hspace{0.2cm} tr[\mathbf{K} \Sigma \mathbf{K}^T]$ s.t. $ \mathbf{KH} = \mathbf{I} $ where $\mathbf{H}$ is of full column rank yields, $\tilde{\mathbf{K} } = (\mathbf{H}^T\Sigma^{-1}\mathbf{H})^{-1}\mathbf{H}^T\Sigma^{-1}$. Does anyone aware of some theorem related to this result.","I have seen in an article that $ \min_{\mathbf{K}} \hspace{0.2cm} tr[\mathbf{K} \Sigma \mathbf{K}^T]$ s.t. $ \mathbf{KH} = \mathbf{I} $ where $\mathbf{H}$ is of full column rank yields, $\tilde{\mathbf{K} } = (\mathbf{H}^T\Sigma^{-1}\mathbf{H})^{-1}\mathbf{H}^T\Sigma^{-1}$. Does anyone aware of some theorem related to this result.",,"['matrices', 'optimization', 'matrix-calculus', 'constraint-programming', 'constraints']"
