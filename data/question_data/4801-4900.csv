,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Is there any way to save this ""proof"" that $\aleph_0=\aleph$? [closed]","Is there any way to save this ""proof"" that ? [closed]",\aleph_0=\aleph,"It's difficult to tell what is being asked here. This question is ambiguous, vague, incomplete, overly broad, or rhetorical and cannot be reasonably answered in its current form. For help clarifying this question so that it can be reopened, visit the help center . Closed 10 years ago . I came up with this idea of proving that $\aleph_0=\aleph$. I know this is not true at all, but maybe there is more to it than I can see. we start with the inequality $\aleph_0 \leq 2^{\aleph_0}=\aleph$. We should have $2^{\aleph_0} \leq e^{\aleph_0}$, where $e^{\aleph_0}$ is defined by the following power series: $$1+\frac{\aleph_0}{1}+\frac{\aleph_0}{2}+\frac{\aleph_0}{6}+\dots $$ The sequence of partial sums of this series is $1,\aleph_0,\aleph_0,\aleph_0,\dots$, therefore its limit should be $\aleph_0$ as well, no? And if $e^{\aleph_0}$ is really $\aleph_0$, we have ""proven"" that only one infinity exists. Is this complete BS, or maybe some hope exists for this idea? NB another possible definition of $e^{\aleph_0}$ as $\lim_{n \to \infty} \left(1+\frac{\aleph_0}{n} \right)^n$ yields the same result.","It's difficult to tell what is being asked here. This question is ambiguous, vague, incomplete, overly broad, or rhetorical and cannot be reasonably answered in its current form. For help clarifying this question so that it can be reopened, visit the help center . Closed 10 years ago . I came up with this idea of proving that $\aleph_0=\aleph$. I know this is not true at all, but maybe there is more to it than I can see. we start with the inequality $\aleph_0 \leq 2^{\aleph_0}=\aleph$. We should have $2^{\aleph_0} \leq e^{\aleph_0}$, where $e^{\aleph_0}$ is defined by the following power series: $$1+\frac{\aleph_0}{1}+\frac{\aleph_0}{2}+\frac{\aleph_0}{6}+\dots $$ The sequence of partial sums of this series is $1,\aleph_0,\aleph_0,\aleph_0,\dots$, therefore its limit should be $\aleph_0$ as well, no? And if $e^{\aleph_0}$ is really $\aleph_0$, we have ""proven"" that only one infinity exists. Is this complete BS, or maybe some hope exists for this idea? NB another possible definition of $e^{\aleph_0}$ as $\lim_{n \to \infty} \left(1+\frac{\aleph_0}{n} \right)^n$ yields the same result.",,"['real-analysis', 'elementary-set-theory', 'fake-proofs']"
1,Trig Fresnel Integral,Trig Fresnel Integral,,"$$\int_{0}^{\infty }\sin(x^{2})dx$$ I'm confused with this integral because the square is on the x, not the whole function. How can I integrate it? Thank you. I have not done complex analysis (only real analysis as I am a high school student) so how can I evaluate it using elementary functions ( without complex analysis)?","$$\int_{0}^{\infty }\sin(x^{2})dx$$ I'm confused with this integral because the square is on the x, not the whole function. How can I integrate it? Thank you. I have not done complex analysis (only real analysis as I am a high school student) so how can I evaluate it using elementary functions ( without complex analysis)?",,"['real-analysis', 'integration', 'definite-integrals', 'fresnel-integrals']"
2,Integral $\int_0^1\frac{\arcsin^3 x}{x^2}\text{d}x=6\pi G-\frac{\pi^3}{8}-\frac{21}{2}\zeta(3)$,Integral,\int_0^1\frac{\arcsin^3 x}{x^2}\text{d}x=6\pi G-\frac{\pi^3}{8}-\frac{21}{2}\zeta(3),Show that:  $$\int_0^1\frac{\arcsin^3 x}{x^2}\text{d}x=6\pi G-\frac{\pi^3}{8}-\frac{21}{2}\zeta(3)$$ I evaluated this by some Fourier series. Is there any other method? Start with substitution of $$u=\arcsin x$$ Then we have to integrate $$\int_0^{\frac{\pi}{2}}\frac{u^3\cos u}{\sin^2 u}\text{d}u=-\int_0^{\frac{\pi}{2}}u^3\csc u\text{d}u$$ Since $$\int\csc u\text{d}u=\ln (\csc u-\cot u)=\ln \left(\frac{1-\cos x}{\sin x}\right)=\ln 2+2\ln \left(\sin \frac{x}{2}\right)-\ln \sin x$$ Thus $$\int_0^{\frac{\pi}{2}}u^2\csc u\text{d}u=\int_0^{\frac{\pi}{2}}u^2\text{d}\left(2\ln \frac{\sin u}{2}-\ln \sin u\right)$$ $$=-\frac{\pi^2}{4}\ln 2-2\int_0^{\frac{\pi}{2}}u\left(2\ln \sin \frac{u}{2}-\ln \sin u\right)$$ $$=-\frac{\pi^2}{4}\ln 2-4\int_0^{\frac{\pi}{2}}u\ln \sin \frac{u}{2}\text{d}u+2\int_0^{\frac{\pi}{2}}u\ln \sin u\text{d}u$$ $$=-\frac{\pi^2}{4}\ln 2+4\int_0^{\frac{\pi}{2}}u\left[\ln 2+\sum_{n=1}^{\infty}\frac{\cos nu}{n}\right]\text{d}u-\int_0^{\frac{\pi}{2}}u^2\cot u\text{d}u$$ $$=\frac{\pi^2}{4}\ln 2+4\sum_{n=1}^{\infty}\frac{1}{n}\int_0^{\frac{\pi}{2}}u\cos nu\text{d}u-\frac{\pi^2}{4}\ln 2+\frac78\zeta(3)$$ $$=\frac78\zeta(3)+2\sum_{n=1}^{\infty}\frac{-2+2\cos \frac{n\pi}{2}+n\pi \sin \frac{n\pi}{2}}{n^3}$$ $$=\frac78\zeta(3)-4\zeta(3)-\frac38\zeta(3)+2\pi G=2\pi G-\frac72\zeta(3)$$ Combine these gives $$\int_0^1\frac{\arcsin ^3x}{x^2}\text{d}x=6\pi G-\frac{\pi^3}{8}-\frac{21}{2}\zeta(3)$$,Show that:  $$\int_0^1\frac{\arcsin^3 x}{x^2}\text{d}x=6\pi G-\frac{\pi^3}{8}-\frac{21}{2}\zeta(3)$$ I evaluated this by some Fourier series. Is there any other method? Start with substitution of $$u=\arcsin x$$ Then we have to integrate $$\int_0^{\frac{\pi}{2}}\frac{u^3\cos u}{\sin^2 u}\text{d}u=-\int_0^{\frac{\pi}{2}}u^3\csc u\text{d}u$$ Since $$\int\csc u\text{d}u=\ln (\csc u-\cot u)=\ln \left(\frac{1-\cos x}{\sin x}\right)=\ln 2+2\ln \left(\sin \frac{x}{2}\right)-\ln \sin x$$ Thus $$\int_0^{\frac{\pi}{2}}u^2\csc u\text{d}u=\int_0^{\frac{\pi}{2}}u^2\text{d}\left(2\ln \frac{\sin u}{2}-\ln \sin u\right)$$ $$=-\frac{\pi^2}{4}\ln 2-2\int_0^{\frac{\pi}{2}}u\left(2\ln \sin \frac{u}{2}-\ln \sin u\right)$$ $$=-\frac{\pi^2}{4}\ln 2-4\int_0^{\frac{\pi}{2}}u\ln \sin \frac{u}{2}\text{d}u+2\int_0^{\frac{\pi}{2}}u\ln \sin u\text{d}u$$ $$=-\frac{\pi^2}{4}\ln 2+4\int_0^{\frac{\pi}{2}}u\left[\ln 2+\sum_{n=1}^{\infty}\frac{\cos nu}{n}\right]\text{d}u-\int_0^{\frac{\pi}{2}}u^2\cot u\text{d}u$$ $$=\frac{\pi^2}{4}\ln 2+4\sum_{n=1}^{\infty}\frac{1}{n}\int_0^{\frac{\pi}{2}}u\cos nu\text{d}u-\frac{\pi^2}{4}\ln 2+\frac78\zeta(3)$$ $$=\frac78\zeta(3)+2\sum_{n=1}^{\infty}\frac{-2+2\cos \frac{n\pi}{2}+n\pi \sin \frac{n\pi}{2}}{n^3}$$ $$=\frac78\zeta(3)-4\zeta(3)-\frac38\zeta(3)+2\pi G=2\pi G-\frac72\zeta(3)$$ Combine these gives $$\int_0^1\frac{\arcsin ^3x}{x^2}\text{d}x=6\pi G-\frac{\pi^3}{8}-\frac{21}{2}\zeta(3)$$,,"['real-analysis', 'calculus', 'integration', 'fourier-analysis', 'trigonometric-integrals']"
3,proving convergence for a sequence defined recursively,proving convergence for a sequence defined recursively,,"The sequence $\left \{ a_{n} \right \}$ is defined by the following recurrence relation: $$a_{0}=1$$ and $$a_{n}=1+\frac{1}{1+a_{n-1}}$$ for all $n\geq 1$ Part 1)- Prove that $a_{n}\geq 1$ for all $n\geq 0$ Part2)- Prove that the sequence $\left \{ a_{n} \right \}$ converges to some real number $x$, and then calculate $x$ For the first part, I could prove it using induction. For the second part: The problem is how to prove that this sequence is convergent. Once the convergence is proved, then from the recurrence relation we can deduce that $x=\sqrt{2}$. In order to prove it is convergent, I tried to see how this sequence converges to $x$. I calculated the terms $a_{0}$, $a_{1}$, $a_{2}$, $a_{3}$, $a_{4}$. I can see that the sequence is neither decreasing nor increasing, so the monotone convergence theorem cannot be applied. I can see that the distance between two consecutive terms is getting smaller and smaller, so I tried to prove that this sequence is contractive. $\left | a_{n+1} -a_{n}\right |=\frac{1}{\left | 1-a_{n} \right |\left | 1+a_{n} \right |}\left | a_{n}-a_{n-1} \right |$, and obviously, $\frac{1}{\left | 1+a_{n} \right |}\leq \frac{1}{2}$. I need to prove that $\frac{1}{\left | 1-a_{n} \right |}\leq \alpha $ where $0< \frac{\alpha }{2}< 1$, and hence the sequence is contractive and therefore it is convergent. If you have any idea how to prove $\frac{1}{\left | 1-a_{n} \right |}\leq \alpha $ or any other idea please share...","The sequence $\left \{ a_{n} \right \}$ is defined by the following recurrence relation: $$a_{0}=1$$ and $$a_{n}=1+\frac{1}{1+a_{n-1}}$$ for all $n\geq 1$ Part 1)- Prove that $a_{n}\geq 1$ for all $n\geq 0$ Part2)- Prove that the sequence $\left \{ a_{n} \right \}$ converges to some real number $x$, and then calculate $x$ For the first part, I could prove it using induction. For the second part: The problem is how to prove that this sequence is convergent. Once the convergence is proved, then from the recurrence relation we can deduce that $x=\sqrt{2}$. In order to prove it is convergent, I tried to see how this sequence converges to $x$. I calculated the terms $a_{0}$, $a_{1}$, $a_{2}$, $a_{3}$, $a_{4}$. I can see that the sequence is neither decreasing nor increasing, so the monotone convergence theorem cannot be applied. I can see that the distance between two consecutive terms is getting smaller and smaller, so I tried to prove that this sequence is contractive. $\left | a_{n+1} -a_{n}\right |=\frac{1}{\left | 1-a_{n} \right |\left | 1+a_{n} \right |}\left | a_{n}-a_{n-1} \right |$, and obviously, $\frac{1}{\left | 1+a_{n} \right |}\leq \frac{1}{2}$. I need to prove that $\frac{1}{\left | 1-a_{n} \right |}\leq \alpha $ where $0< \frac{\alpha }{2}< 1$, and hence the sequence is contractive and therefore it is convergent. If you have any idea how to prove $\frac{1}{\left | 1-a_{n} \right |}\leq \alpha $ or any other idea please share...",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
4,About the notion of limsup and liminf,About the notion of limsup and liminf,,"Recently, I'm reading something about viscosity solution of a PDE, and the notions of limsup and liminf haunt me all the time. The following are some examples. The upper semi-continuous envelope of a function $z: \mathbb{R}^n \rightarrow \mathbb{R}$ is defined as $$ z^*(x) := \limsup_{x' \rightarrow x}\ z(x') $$ Suppose for every partition $P$ of the interval $[0,T]$, we are given a (continuous) function $V^P: [0,T]\times \mathbb{R}^2 \rightarrow \mathbb{R}$. Then, we define $$ \bar{V}(t,x,y) := \limsup_{mesh(P)\rightarrow 0, (t',x',y')\rightarrow (t,x,y)} V^P(t',x',y') $$ where $mesh(P)$ is the mesh size of the partition. Suppose we have a sequence of functions $f_n: \mathbb{R}^n \rightarrow \mathbb{R}$, then we can define  $$ g(x) := \limsup_{n\rightarrow \infty}\ \sup_{x'\in\mathbb{R}^n} f_n(x') $$ I have formally learned limsup and liminf only in terms of sequences, so I wonder How to interpret the above definitions of functions in precise mathematical language? ($\epsilon$-$\delta$ description will be great.) If there are two limiting processes like in the second example, is there any kind of order of computation? How to generalize the notions of limsup and liminf in more general settings?","Recently, I'm reading something about viscosity solution of a PDE, and the notions of limsup and liminf haunt me all the time. The following are some examples. The upper semi-continuous envelope of a function $z: \mathbb{R}^n \rightarrow \mathbb{R}$ is defined as $$ z^*(x) := \limsup_{x' \rightarrow x}\ z(x') $$ Suppose for every partition $P$ of the interval $[0,T]$, we are given a (continuous) function $V^P: [0,T]\times \mathbb{R}^2 \rightarrow \mathbb{R}$. Then, we define $$ \bar{V}(t,x,y) := \limsup_{mesh(P)\rightarrow 0, (t',x',y')\rightarrow (t,x,y)} V^P(t',x',y') $$ where $mesh(P)$ is the mesh size of the partition. Suppose we have a sequence of functions $f_n: \mathbb{R}^n \rightarrow \mathbb{R}$, then we can define  $$ g(x) := \limsup_{n\rightarrow \infty}\ \sup_{x'\in\mathbb{R}^n} f_n(x') $$ I have formally learned limsup and liminf only in terms of sequences, so I wonder How to interpret the above definitions of functions in precise mathematical language? ($\epsilon$-$\delta$ description will be great.) If there are two limiting processes like in the second example, is there any kind of order of computation? How to generalize the notions of limsup and liminf in more general settings?",,"['real-analysis', 'limsup-and-liminf']"
5,"Are rotations of $(0,1)$ by $n \arccos(\frac{1}{3})$ dense in the unit circle?",Are rotations of  by  dense in the unit circle?,"(0,1) n \arccos(\frac{1}{3})","Under which conditions will successive rotations of $(0, 1)$ by an angle $\theta$ guarantee that given $\delta > 0$ and some point $p$ on the unit circle, there exists some $n$ such that rotating $(0, 1)$ by $n\theta$ yields a point within $\delta$ radians of $p$?  That is, are there certain angles that will guarantee that the set generated by rotating $(0, 1)$ by integral multiples of $\theta$ is dense in the unit circle? More specifically, I am working with the angle $\cos^{-1}\left(\frac{1}{3}\right)$ and wish to show that this property is true, but I'm not sure how to go about proving this.  It seems likely I'm missing something simple here...  Does it hold for any angle that is an irrational multiple of $\pi$?  If so, why?","Under which conditions will successive rotations of $(0, 1)$ by an angle $\theta$ guarantee that given $\delta > 0$ and some point $p$ on the unit circle, there exists some $n$ such that rotating $(0, 1)$ by $n\theta$ yields a point within $\delta$ radians of $p$?  That is, are there certain angles that will guarantee that the set generated by rotating $(0, 1)$ by integral multiples of $\theta$ is dense in the unit circle? More specifically, I am working with the angle $\cos^{-1}\left(\frac{1}{3}\right)$ and wish to show that this property is true, but I'm not sure how to go about proving this.  It seems likely I'm missing something simple here...  Does it hold for any angle that is an irrational multiple of $\pi$?  If so, why?",,"['real-analysis', 'geometry', 'analysis']"
6,Summing the series $ \frac{1}{2n+1} + \frac{1}{2} \cdot \frac{1}{2n+3} + \cdots \ \text{ad inf}$,Summing the series, \frac{1}{2n+1} + \frac{1}{2} \cdot \frac{1}{2n+3} + \cdots \ \text{ad inf},"How does one sum the given series: $$ \frac{1}{2n+1} + \frac{1}{2} \cdot \frac{1}{2n+3} + \frac{1 \cdot 3}{2 \cdot 4} \frac{1}{2n+5} + \frac{ 1 \cdot 3 \cdot 5}{2 \cdot 4 \cdot 6} \frac{1}{2n+7} + \cdots \ \text{ad inf}$$ Given, such a series, how does one go about solving it. Getting an Integral Representation seems tough for me. I thought of going along these lines, Summing the series $(-1)^k \frac{(2k)!!}{(2k+1)!!} a^{2k+1}$ but couldn't succeed.","How does one sum the given series: $$ \frac{1}{2n+1} + \frac{1}{2} \cdot \frac{1}{2n+3} + \frac{1 \cdot 3}{2 \cdot 4} \frac{1}{2n+5} + \frac{ 1 \cdot 3 \cdot 5}{2 \cdot 4 \cdot 6} \frac{1}{2n+7} + \cdots \ \text{ad inf}$$ Given, such a series, how does one go about solving it. Getting an Integral Representation seems tough for me. I thought of going along these lines, Summing the series $(-1)^k \frac{(2k)!!}{(2k+1)!!} a^{2k+1}$ but couldn't succeed.",,"['calculus', 'real-analysis']"
7,How to prove this integral inequality $\int_{0}^{1}[f''(x)]^2dx\ge 12$?,How to prove this integral inequality ?,\int_{0}^{1}[f''(x)]^2dx\ge 12,"Let $f\in C^{2}[0,1]$ such that $f(0)=0,f(1/2)=f(1)=1$ . Show that $$\int_{0}^{1}[f''(x)]^2dx\ge 12.$$ My idea: try to find a function $f(x)$ such that the equality holds and use Cauchy-Schwarz. So I tried some ""good"" functions to achieve the minimum value. Assume $f(x)=ax^3+bx^2+cx$ is a polynomial. Then I find that when $f(x)=-2x^2+3x$ , the integral $\int_{0}^{1}[f''(x)]^2dx$ has minimum value $16$ . It seems it doesn't help to solve the problem. Since $f''(x)=(f(x)-ax-b)''$ and replace $f(x)$ by $f(x)-x$ , we can change the condition to $f(0)=f(1)=0, f(1/2)=1/2$ . I find that $f(x)=\frac{1}{2}\sin(\pi x)$ satisfies the condition and $\int_{0}^{1}[f''(x)]^2dx=\frac{\pi^2}{8}=12.1761...$ which is very closed to $12$ . But it still doesn't help. It seems that it isn't easy to find such $f(x)$ such that the equality holds. So how to prove this inequality? And what is the minumum value of this integral? May be not 12? Thank you!","Let such that . Show that My idea: try to find a function such that the equality holds and use Cauchy-Schwarz. So I tried some ""good"" functions to achieve the minimum value. Assume is a polynomial. Then I find that when , the integral has minimum value . It seems it doesn't help to solve the problem. Since and replace by , we can change the condition to . I find that satisfies the condition and which is very closed to . But it still doesn't help. It seems that it isn't easy to find such such that the equality holds. So how to prove this inequality? And what is the minumum value of this integral? May be not 12? Thank you!","f\in C^{2}[0,1] f(0)=0,f(1/2)=f(1)=1 \int_{0}^{1}[f''(x)]^2dx\ge 12. f(x) f(x)=ax^3+bx^2+cx f(x)=-2x^2+3x \int_{0}^{1}[f''(x)]^2dx 16 f''(x)=(f(x)-ax-b)'' f(x) f(x)-x f(0)=f(1)=0, f(1/2)=1/2 f(x)=\frac{1}{2}\sin(\pi x) \int_{0}^{1}[f''(x)]^2dx=\frac{\pi^2}{8}=12.1761... 12 f(x)","['real-analysis', 'integration', 'inequality', 'integral-inequality']"
8,Example of a function that is continuous at $c$ whose inverse is discontinuous at $f(c)$,Example of a function that is continuous at  whose inverse is discontinuous at,c f(c),"I'd like an example of a function $f:(a,b)\to\mathbb R$ and a point $c\in(a,b)$ such that: $f$ is invertible. $f$ is continuous at $c$ . $f^{-1}$ is discontinuous at $f(c)$ . Motivation: There is a calculus book that states the following. Let $f$ be an invertible function defined on an interval $I$ . If $f$ is differentiable at $c\in I$ and $f'(c)\neq 0$ , then $f^{-1}$ is differentiable at $f(c)$ . In the proof, the continuity of $f^{-1}$ at $f(c)$ is essential. Usually, the said essential fact is an hypothesis (if the domain is not an interval) or it is implied by the hypothesis that $f$ is continuous in a neighborhood of $c$ (if the domain is an interval). But in the said book, both hipothesis are missing and the fact is justified as follows: As $f$ is differentiable at $c$ , $f$ is continuous at $c$ . Therefore, $f^{-1}$ is continuous at $f(c)$ . I suspect continuity at $c$ does not imply continuity of the inverse at $f(c)$ due to the following facts: It seems it is not a common result in analysis books. In the usual proofs that the inverse of a continuos map (on an interval or on a compact set) is continous, in order to prove that the inverse is continuous at a given point, we need the continuity of $f$ in the whole domain. In more recent editions of the said book, the statement was modified (now, it is supposed that $f$ is differentiable in a neighborhood of $c$ , which implies what is needed). However, I do not have a counterexample.","I'd like an example of a function and a point such that: is invertible. is continuous at . is discontinuous at . Motivation: There is a calculus book that states the following. Let be an invertible function defined on an interval . If is differentiable at and , then is differentiable at . In the proof, the continuity of at is essential. Usually, the said essential fact is an hypothesis (if the domain is not an interval) or it is implied by the hypothesis that is continuous in a neighborhood of (if the domain is an interval). But in the said book, both hipothesis are missing and the fact is justified as follows: As is differentiable at , is continuous at . Therefore, is continuous at . I suspect continuity at does not imply continuity of the inverse at due to the following facts: It seems it is not a common result in analysis books. In the usual proofs that the inverse of a continuos map (on an interval or on a compact set) is continous, in order to prove that the inverse is continuous at a given point, we need the continuity of in the whole domain. In more recent editions of the said book, the statement was modified (now, it is supposed that is differentiable in a neighborhood of , which implies what is needed). However, I do not have a counterexample.","f:(a,b)\to\mathbb R c\in(a,b) f f c f^{-1} f(c) f I f c\in I f'(c)\neq 0 f^{-1} f(c) f^{-1} f(c) f c f c f c f^{-1} f(c) c f(c) f f c","['real-analysis', 'calculus', 'continuity', 'examples-counterexamples']"
9,"Prove that $f(x,y)+f(y,z)\ge f(x,z)$ where $f(x,y)=\sqrt{x\ln x+y\ln y-(x+y)\ln(\frac{x+y}2)}$",Prove that  where,"f(x,y)+f(y,z)\ge f(x,z) f(x,y)=\sqrt{x\ln x+y\ln y-(x+y)\ln(\frac{x+y}2)}","Denote $f(x,y)=\sqrt{x\ln x+y\ln y-(x+y)\ln(\frac{x+y}2)}$ . Show that $f(x,y)+f(y,z)\ge f(x,z)$ for $x,y,z> 0$ . This is a question from a friend, which is a deep learning homework. It looks like some square roots of KL divergence, but it seems no help. Some other friends have tried to square it but dealing with the crossing terms like $xy\ln x \ln y$ makes it tough. Some friends and the asker himself try to take the derivative of $y$ and calculate the minimum but to no avail... Some students suggested that it can be written as an integral. I don't have any idea, so I ask here.","Denote . Show that for . This is a question from a friend, which is a deep learning homework. It looks like some square roots of KL divergence, but it seems no help. Some other friends have tried to square it but dealing with the crossing terms like makes it tough. Some friends and the asker himself try to take the derivative of and calculate the minimum but to no avail... Some students suggested that it can be written as an integral. I don't have any idea, so I ask here.","f(x,y)=\sqrt{x\ln x+y\ln y-(x+y)\ln(\frac{x+y}2)} f(x,y)+f(y,z)\ge f(x,z) x,y,z> 0 xy\ln x \ln y y","['real-analysis', 'inequality', 'entropy']"
10,Find the value of $\displaystyle \int \limits _{0}^{\infty} \dfrac{\mathrm dx}{\sqrt{x^n+a} + \sqrt{x^n+b}}$,Find the value of,\displaystyle \int \limits _{0}^{\infty} \dfrac{\mathrm dx}{\sqrt{x^n+a} + \sqrt{x^n+b}},"I have a question which asks to find the value of: $\displaystyle \tag*{} \int \limits _{0}^{\infty} \dfrac{\mathrm dx}{\sqrt{x^n+a} + \sqrt{x^n+b}}$ Where, $a,b >0$ and $n >2$ I tried to rationalize the denominator and arrived: $\displaystyle \tag*{} \dfrac{1}{a-b}\int \limits _{0}^{\infty}\sqrt{x^n+a} - \sqrt{x^n+b} \ \ \mathrm dx $ Since individual integrals do not converge. But couldn't proceed any further. Any help would be greatly appreciated. Thank you!","I have a question which asks to find the value of: Where, and I tried to rationalize the denominator and arrived: Since individual integrals do not converge. But couldn't proceed any further. Any help would be greatly appreciated. Thank you!","\displaystyle \tag*{} \int \limits _{0}^{\infty} \dfrac{\mathrm dx}{\sqrt{x^n+a} + \sqrt{x^n+b}} a,b >0 n >2 \displaystyle \tag*{} \dfrac{1}{a-b}\int \limits _{0}^{\infty}\sqrt{x^n+a} - \sqrt{x^n+b} \ \ \mathrm dx ","['real-analysis', 'calculus']"
11,$\int_{-1}^{1} \log\left(\frac{1+x}{1-x} \right) \frac{1}{1-ax} dx.$,,\int_{-1}^{1} \log\left(\frac{1+x}{1-x} \right) \frac{1}{1-ax} dx.,"Definite Integral: $$\int_{-1}^{1} \log\left(\frac{1+x}{1-x} \right) \frac{1}{1-ax} dx,$$ where $0 < a <1$ . I tried with integration by parts taking $\log\left(\frac{1+x}{1-x} \right) $ as the first function and $\frac{1}{1-ax} $ as the second function, but it did not work. Need some help to compute it.","Definite Integral: where . I tried with integration by parts taking as the first function and as the second function, but it did not work. Need some help to compute it.","\int_{-1}^{1} \log\left(\frac{1+x}{1-x} \right) \frac{1}{1-ax} dx, 0 < a <1 \log\left(\frac{1+x}{1-x} \right)  \frac{1}{1-ax} ","['real-analysis', 'calculus', 'integration']"
12,A double sum for the square of the natural logarithm of $2$.,A double sum for the square of the natural logarithm of .,2,"I am trying to show \begin{eqnarray*} \sum_{n=1}^{\infty} \sum_{m=1}^{\infty} \frac{1}{(n+m)^2 2^{n}} =(\ln(2))^2.  \end{eqnarray*} Motivation : I want to use this to calculate $ \operatorname{Li}_2(1/2)$ . So I want a solution to the above that does not use any reference to dilogarithms and please avoid rational multiples of $\pi^2$ (if possible). Right, lets turn it into a double integral. ( I know lots of you folks prefer integrals to plums.) Show \begin{eqnarray*} \int_0^1 \int_0^1 \frac{xy \ dx  \ dy}{(1-xy)(2-xy)} =(\ln(2))^2.  \end{eqnarray*} Reassuringly Wolfy agrees My try: Let $u=xy$ , & the double integral becomes \begin{eqnarray*} \int_0^1 \frac{dy}{y}  \int_0^y \frac{u \ du }{(1-u)(2-u)} .  \end{eqnarray*} Partial fractions \begin{eqnarray*}  \frac{u}{(1-u)(2-u)} =\frac{1}{1-u} - \frac{2}{2-u}. \end{eqnarray*} Do the $u$ integrations to leave the $y$ integrals \begin{eqnarray*} -\int_0^1 \frac{\ln(1-y) dy}{y}  +2 \int_0^1 \frac{\ln(2-y) dy}{y}. \end{eqnarray*} The first integral is \begin{eqnarray*} -\int_0^1 \frac{\ln(1-y) dy}{y}  = \frac{ \pi^2}{6}. \end{eqnarray*} which I was hoping to avoid and even worse Wolfy says the second integral is divergent So you have a choice of questions, where did I go wrong in the above ? OR how can we show the initially stated result?","I am trying to show Motivation : I want to use this to calculate . So I want a solution to the above that does not use any reference to dilogarithms and please avoid rational multiples of (if possible). Right, lets turn it into a double integral. ( I know lots of you folks prefer integrals to plums.) Show Reassuringly Wolfy agrees My try: Let , & the double integral becomes Partial fractions Do the integrations to leave the integrals The first integral is which I was hoping to avoid and even worse Wolfy says the second integral is divergent So you have a choice of questions, where did I go wrong in the above ? OR how can we show the initially stated result?","\begin{eqnarray*}
\sum_{n=1}^{\infty} \sum_{m=1}^{\infty} \frac{1}{(n+m)^2 2^{n}} =(\ln(2))^2. 
\end{eqnarray*}  \operatorname{Li}_2(1/2) \pi^2 \begin{eqnarray*}
\int_0^1 \int_0^1 \frac{xy \ dx  \ dy}{(1-xy)(2-xy)} =(\ln(2))^2. 
\end{eqnarray*} u=xy \begin{eqnarray*}
\int_0^1 \frac{dy}{y}  \int_0^y \frac{u \ du }{(1-u)(2-u)} . 
\end{eqnarray*} \begin{eqnarray*}
 \frac{u}{(1-u)(2-u)} =\frac{1}{1-u} - \frac{2}{2-u}.
\end{eqnarray*} u y \begin{eqnarray*}
-\int_0^1 \frac{\ln(1-y) dy}{y}  +2 \int_0^1 \frac{\ln(2-y) dy}{y}.
\end{eqnarray*} \begin{eqnarray*}
-\int_0^1 \frac{\ln(1-y) dy}{y}  = \frac{ \pi^2}{6}.
\end{eqnarray*}","['real-analysis', 'integration', 'summation', 'harmonic-numbers']"
13,Proving roots of a function cannot all be real,Proving roots of a function cannot all be real,,"The question is Let $a$$,b$$,c$$,d$ be any four real number not all equal to zero. Prove that the roots of the polynomial $f(x)=x^6+ax^3+bx^2+cx+d$ cannot all be real. I know that the first derivative of a functions gives its turning points and the second derivative give its concavity but how does it help to solve the question. Or if you have any other methods it would really help.Any hint would also be appreciated . Thanks in advance.",The question is Let be any four real number not all equal to zero. Prove that the roots of the polynomial cannot all be real. I know that the first derivative of a functions gives its turning points and the second derivative give its concavity but how does it help to solve the question. Or if you have any other methods it would really help.Any hint would also be appreciated . Thanks in advance.,"a,b,c,d f(x)=x^6+ax^3+bx^2+cx+d","['real-analysis', 'derivatives', 'polynomials', 'graphing-functions']"
14,Are there any functions that are differentiable but not continuously-differentiable?,Are there any functions that are differentiable but not continuously-differentiable?,,"Let $U$ be an open set on ${\mathbb R}^{n}$ (but $U$ is not an empty set), $\textbf{p}\in{U}$ , and $f:U\to \mathbb R$ is continuously-differentiable on $U$ . Then, it is known that, ""the function $f$ can be differentiable for all $\textbf{q}\in U$ ."" (See Spivac ) And I know that, there is a function $f$ such that it is differentiable at $\textbf{p}$ but, for any $r> 0$ , $f$ is not differentiable  (and continuously differentiable) on $U_{\textbf{p}} (r)$ . Here, $U_{\textbf{p}} (r)$ is an open ball of radius $r$ centered on $\textbf{p}$ . For example, if $f:{\mathbb R}^{2}\to \mathbb R$ is defined as follows, $f$ is differentiable at $\textbf{0}$ ,, but is not differentiable (and not continuous) at any other point. Here $\mathbb Q$ is the set of all rational numbers, and $U_{\textbf{p}} (r)$ is an open ball of radius $r$ centered on $\textbf{p}$ . $f(x,y):=\left\{ \begin{array}{rr} 0, &  (x,y)\in \mathbb Q^{2} \\ x^2 + y^2, &  (x,y)\notin \mathbb Q^{2} \\ \end{array} \right.$ Therefore, there is at least one function that does not have a continuously-differentiable region, even if it can be differentiable at one point.  But I cannot imagine whether are there any functions that are differentiable on $U$ but not continuously-differentiable . My question Let $U$ be an open set of $\mathbb R^n$ (but is not an empty set), and $\  \textbf{p}\in U $ . Then, are there any functions $f:U\to \mathbb R$ such that, $f$ is differentiable on $U$ , but for any $r> 0$ , $f$ is not continuously-differentiable on $U_{\textbf{p}} (r)$ ? If so, give an example. If not, please explain why. Here, $U_{\textbf{p}} (r)$ is an open ball of radius $r$ centered on $\textbf{p}$ . Here, the definitions of differentiable and continuously differentiable are as follows. Def1 (Differentiable at $\textbf{p}$ ) Let $U$ be an open set (but not empty set) of ${\mathbb R}^{n}$ , $\textbf {p} \in \mathbb R^n$ , and $f$ is a function whose domain is $U$ .   At this time, $f$ is differentiable at $\textbf{p}$ iff the following is satisfied. ${\exists} A:{\mathbb R}^{n}\to \mathbb R$ : a linear map such that $${\lim}_{\textbf{x}\to\textbf{p}}\frac{|f(\textbf{x}) - A(\textbf{x}-\textbf{p}) - f(\textbf{p})|}{|\textbf{x}-\textbf{p}|} = 0$$ Def2 (Differentiable on $\textbf{U}$ ) Let $U$ be an open set (but not empty sets) of ${\mathbb R}^{n}$ , and $f$ is a function which domain is $U$ . At this time, $f$ is differentiable at $U$ iff ""for all $\textbf{q}\in{\mathbb R}^{n}$ , $f$ is differentiable at $\textbf{q}$ "". Def3 (Continuously-differentiable on $U$ ) Let $U$ be an open set (but is not empty sets) of ${\mathbb R}^{n}$ , and $f$ is a function which domain is $U$ .   At this time, $f:U\to \mathbb R$ is continuously-differentiable on $U$ iff $f$ is  partially differentiable for all direction, ${x}_{1},    {x}_{2}, ..., {x}_{n}$ (that mean, we can define $\frac{\partial f}{\partial{x}_{1}}, \cdots\frac{\partial f} {\partial{x}_{n}} $ on $U$ ).  and, $\frac{\partial f} {\partial{x}_{1}}, \cdots\frac{\partial f}    {\partial{x}_{n}} $ are continuous on $U$ . P.S. I'm not very good at English, so I'm sorry if I have some impolite or unclear expressions. Post-hoc Note: 【Verification of the function taught by Thomas Shelby】 The following are the confirmation that the following function $f$ meets my requirement (Is it correct as proof?): $f(x,y)=\begin{cases}(x^2+y^2)\sin\left(\frac1 {\sqrt{x^2+y^2}}\right),&(x,y)\neq 0\\0,&(x,y)=0\end{cases}.$ My proof: ${\lim}_{\|\textbf{x}\|\to 0} \frac{f(\textbf{x}) - f(\textbf{0})}{\|\textbf{x}\|}= {\lim}_{\|\textbf{x}\|\to 0} \frac{{\|\textbf{x}\|}^{2}\sin(1/ \|\textbf{x}\| - 0)}{\|\textbf{x}\|}= $ ${\lim}_{\|\textbf{x}\|\to 0} \|\textbf{x}\|\sin(1/ \|\textbf{x}\|) = 0$ Therefore, the $f$ is differntiable at $(0,0)$ and $Jf(0,0)=(0,0)$ . On the other hand, for $\textbf{x}\neq\textbf{0}$ , Let $g$ and $h$ be $g(x,y):=\sqrt{{x}^2 + {y}^2}\ $ and $\ h(t):={t}^{2}\sin(1/t)$ (for $t\neq 0$ ) respectively, then $$\frac{d\sqrt{t}}{dt} = \frac{1}{2\sqrt{t}} $$ and, $$(J\|\textbf{x}\|^2)(x,y) = (2x,2y) ,$$ Therefore, $$(Jg)(x,y) = \left(\frac{x}{\|\textbf{x}\|} , \frac{y}{\|\textbf{x}\|}\right)\quad (\textrm{for all $\textbf{x}\neq\textbf{0}\ $}),$$ and $$\ \frac{d\sin(1/t)}{dt} = -\frac{\cos(1/t)}{t^2}\ \ \ (\textrm{at $t\neq 0$}).$$ Therefore, $$\frac{dh}{dt} ={t}^{2}\frac{d\sin(1/t)}{dt} + 2t\sin(1/t) = -\cos(1/t) + 2t\sin(1/t).$$ Therefore, at $\textbf{x}\neq \textbf{0}$ , $$Jf(x,y) = \left(\left.\frac{dh}{dt}\right|_{t=||\textbf{x}||}\right)(Jg)(x,y) = (-\cos(1/||\textbf{x}||) + 2t\sin(1/||\textbf{x}||))\left(\frac{x}{||\textbf{x}||} , \frac{y}{||\textbf{x}||}\right).$$ Therefore, $$\frac{\partial f}{\partial x} =  -\frac{x\cos(1/||\textbf{x}||)}{||\textbf{x}||} + 2x\sin(1/||\textbf{x}||),\,\textbf{x}\neq\textbf{0}$$ and $$\frac{\partial f}{\partial y} =  -\frac{y\cos(1/||\textbf{x}||)}{||\textbf{x}||} + 2y\sin(1/||\textbf{x}||),\,\textbf{x}\neq\textbf{0}.$$ However, both $\dfrac{x\cos(1/||\textbf{x}||)}{||\textbf{x}||} $ and $\dfrac{y\cos(1/||\textbf{x}||)}{||\textbf{x}||} $ do not get converted at $(0,0)$ . So, both $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ are not continuous at $(0,0)$ . ■","Let be an open set on (but is not an empty set), , and is continuously-differentiable on . Then, it is known that, ""the function can be differentiable for all ."" (See Spivac ) And I know that, there is a function such that it is differentiable at but, for any , is not differentiable  (and continuously differentiable) on . Here, is an open ball of radius centered on . For example, if is defined as follows, is differentiable at ,, but is not differentiable (and not continuous) at any other point. Here is the set of all rational numbers, and is an open ball of radius centered on . Therefore, there is at least one function that does not have a continuously-differentiable region, even if it can be differentiable at one point.  But I cannot imagine whether are there any functions that are differentiable on but not continuously-differentiable . My question Let be an open set of (but is not an empty set), and . Then, are there any functions such that, is differentiable on , but for any , is not continuously-differentiable on ? If so, give an example. If not, please explain why. Here, is an open ball of radius centered on . Here, the definitions of differentiable and continuously differentiable are as follows. Def1 (Differentiable at ) Let be an open set (but not empty set) of , , and is a function whose domain is .   At this time, is differentiable at iff the following is satisfied. : a linear map such that Def2 (Differentiable on ) Let be an open set (but not empty sets) of , and is a function which domain is . At this time, is differentiable at iff ""for all , is differentiable at "". Def3 (Continuously-differentiable on ) Let be an open set (but is not empty sets) of , and is a function which domain is .   At this time, is continuously-differentiable on iff is  partially differentiable for all direction, (that mean, we can define on ).  and, are continuous on . P.S. I'm not very good at English, so I'm sorry if I have some impolite or unclear expressions. Post-hoc Note: 【Verification of the function taught by Thomas Shelby】 The following are the confirmation that the following function meets my requirement (Is it correct as proof?): My proof: Therefore, the is differntiable at and . On the other hand, for , Let and be and (for ) respectively, then and, Therefore, and Therefore, Therefore, at , Therefore, and However, both and do not get converted at . So, both and are not continuous at . ■","U {\mathbb R}^{n} U \textbf{p}\in{U} f:U\to \mathbb R U f \textbf{q}\in U f \textbf{p} r> 0 f U_{\textbf{p}} (r) U_{\textbf{p}} (r) r \textbf{p} f:{\mathbb R}^{2}\to \mathbb R f \textbf{0} \mathbb Q U_{\textbf{p}} (r) r \textbf{p} f(x,y):=\left\{
\begin{array}{rr}
0, &  (x,y)\in \mathbb Q^{2} \\
x^2 + y^2, &  (x,y)\notin \mathbb Q^{2} \\
\end{array}
\right. U U \mathbb R^n \  \textbf{p}\in U  f:U\to \mathbb R f U r> 0 f U_{\textbf{p}} (r) U_{\textbf{p}} (r) r \textbf{p} \textbf{p} U {\mathbb R}^{n} \textbf {p} \in \mathbb R^n f U f \textbf{p} {\exists} A:{\mathbb R}^{n}\to \mathbb R {\lim}_{\textbf{x}\to\textbf{p}}\frac{|f(\textbf{x}) - A(\textbf{x}-\textbf{p}) - f(\textbf{p})|}{|\textbf{x}-\textbf{p}|} = 0 \textbf{U} U {\mathbb R}^{n} f U f U \textbf{q}\in{\mathbb R}^{n} f \textbf{q} U U {\mathbb R}^{n} f U f:U\to \mathbb R U f {x}_{1},
   {x}_{2}, ..., {x}_{n} \frac{\partial f}{\partial{x}_{1}}, \cdots\frac{\partial f} {\partial{x}_{n}}  U \frac{\partial f} {\partial{x}_{1}}, \cdots\frac{\partial f}
   {\partial{x}_{n}}  U f f(x,y)=\begin{cases}(x^2+y^2)\sin\left(\frac1 {\sqrt{x^2+y^2}}\right),&(x,y)\neq 0\\0,&(x,y)=0\end{cases}. {\lim}_{\|\textbf{x}\|\to 0} \frac{f(\textbf{x}) - f(\textbf{0})}{\|\textbf{x}\|}=
{\lim}_{\|\textbf{x}\|\to 0} \frac{{\|\textbf{x}\|}^{2}\sin(1/ \|\textbf{x}\| - 0)}{\|\textbf{x}\|}=
 {\lim}_{\|\textbf{x}\|\to 0} \|\textbf{x}\|\sin(1/ \|\textbf{x}\|)
= 0 f (0,0) Jf(0,0)=(0,0) \textbf{x}\neq\textbf{0} g h g(x,y):=\sqrt{{x}^2 + {y}^2}\  \ h(t):={t}^{2}\sin(1/t) t\neq 0 \frac{d\sqrt{t}}{dt} = \frac{1}{2\sqrt{t}}  (J\|\textbf{x}\|^2)(x,y) = (2x,2y) , (Jg)(x,y) = \left(\frac{x}{\|\textbf{x}\|} , \frac{y}{\|\textbf{x}\|}\right)\quad (\textrm{for all \textbf{x}\neq\textbf{0}\ }), \ \frac{d\sin(1/t)}{dt} = -\frac{\cos(1/t)}{t^2}\ \ \ (\textrm{at t\neq 0}). \frac{dh}{dt} ={t}^{2}\frac{d\sin(1/t)}{dt} + 2t\sin(1/t) = -\cos(1/t) + 2t\sin(1/t). \textbf{x}\neq \textbf{0} Jf(x,y) = \left(\left.\frac{dh}{dt}\right|_{t=||\textbf{x}||}\right)(Jg)(x,y) = (-\cos(1/||\textbf{x}||) + 2t\sin(1/||\textbf{x}||))\left(\frac{x}{||\textbf{x}||} , \frac{y}{||\textbf{x}||}\right). \frac{\partial f}{\partial x} =  -\frac{x\cos(1/||\textbf{x}||)}{||\textbf{x}||} + 2x\sin(1/||\textbf{x}||),\,\textbf{x}\neq\textbf{0} \frac{\partial f}{\partial y} =  -\frac{y\cos(1/||\textbf{x}||)}{||\textbf{x}||} + 2y\sin(1/||\textbf{x}||),\,\textbf{x}\neq\textbf{0}. \dfrac{x\cos(1/||\textbf{x}||)}{||\textbf{x}||}  \dfrac{y\cos(1/||\textbf{x}||)}{||\textbf{x}||}  (0,0) \frac{\partial f}{\partial x} \frac{\partial f}{\partial y} (0,0)","['real-analysis', 'calculus', 'derivatives']"
15,What exactly is a 'dummy variable'?,What exactly is a 'dummy variable'?,,"I was watching an online course ( The Calculus You Need - MIT OpenCourseWare ), when (around 03:08), the lecturer (Gilbert Strang) says that he doesn't ""care what that dummy variable is"" (the variable x associated with the function y ). He made the following change in the video: $$\frac{d}{dx}\int_{0}^{x}y(x) = y(x)\Rightarrow \text{ change } \Rightarrow\frac{d}{dx}\int_{0}^{x}y(t)dt = y(x)$$ I don't know why the notation of that variable doesn't cause any ambiguity here (I'm assuming Strang wanted to mean that), once x is clearly creating a dependency relation with the bound and the variable associated with the function y . Ultimately... why it doesn't make a difference to call the variable associated with y by x (in this case)? What exactly is a 'dummy variable'?","I was watching an online course ( The Calculus You Need - MIT OpenCourseWare ), when (around 03:08), the lecturer (Gilbert Strang) says that he doesn't ""care what that dummy variable is"" (the variable x associated with the function y ). He made the following change in the video: I don't know why the notation of that variable doesn't cause any ambiguity here (I'm assuming Strang wanted to mean that), once x is clearly creating a dependency relation with the bound and the variable associated with the function y . Ultimately... why it doesn't make a difference to call the variable associated with y by x (in this case)? What exactly is a 'dummy variable'?",\frac{d}{dx}\int_{0}^{x}y(x) = y(x)\Rightarrow \text{ change } \Rightarrow\frac{d}{dx}\int_{0}^{x}y(t)dt = y(x),"['real-analysis', 'calculus', 'integration', 'riemann-integration']"
16,"Epsilon-delta definitions, inequality strict / non-strict?","Epsilon-delta definitions, inequality strict / non-strict?",,"Reading some of the analysis related posts, I have a question regarding the epsilon-delta language. What we are taught is the inequality in the definition is strict. E.g $$\forall\varepsilon >0\ \exists\delta >0: \forall x\in D\left ( |x-a|<\delta \Longrightarrow |f(x)-f(a)|<\varepsilon \right )$$ ( definition of continuity at $a\in D$ ). If this is satisfied, we conclude for suitable choice of $x$ the difference between $f(x)$ and $f(a)$ is strictly less than any positive number hence it must be zero. Intuitively, it also makes sense that it's sufficient if the inequality involving $\varepsilon$ is not strict. But how does one justify that? Let $R(\varepsilon)$ represent a definition with strict $\varepsilon$ -inequality. Let $M(\varepsilon)$ be the same definition, but let $\varepsilon$ -inequality be non-strict.  Then $R(\varepsilon)$ is satisfied iff $M(\varepsilon)$ is satisfied? The question really is if $M(\varepsilon)$ is satisfied, is then $R(\varepsilon)$ satisfied? Does one simply say that since $M\left (\frac{\varepsilon}{2}\right )$ , then $R(\varepsilon)$ ? I can almost be sure that we can't allow the inequality involving $\delta$ to be non-strict, otherwise we could potentially permit points where $f$ tends to infinity? [On second thought, just make $\delta$ smaller]","Reading some of the analysis related posts, I have a question regarding the epsilon-delta language. What we are taught is the inequality in the definition is strict. E.g ( definition of continuity at ). If this is satisfied, we conclude for suitable choice of the difference between and is strictly less than any positive number hence it must be zero. Intuitively, it also makes sense that it's sufficient if the inequality involving is not strict. But how does one justify that? Let represent a definition with strict -inequality. Let be the same definition, but let -inequality be non-strict.  Then is satisfied iff is satisfied? The question really is if is satisfied, is then satisfied? Does one simply say that since , then ? I can almost be sure that we can't allow the inequality involving to be non-strict, otherwise we could potentially permit points where tends to infinity? [On second thought, just make smaller]",\forall\varepsilon >0\ \exists\delta >0: \forall x\in D\left ( |x-a|<\delta \Longrightarrow |f(x)-f(a)|<\varepsilon \right ) a\in D x f(x) f(a) \varepsilon R(\varepsilon) \varepsilon M(\varepsilon) \varepsilon R(\varepsilon) M(\varepsilon) M(\varepsilon) R(\varepsilon) M\left (\frac{\varepsilon}{2}\right ) R(\varepsilon) \delta f \delta,"['real-analysis', 'functional-analysis', 'continuity', 'epsilon-delta']"
17,Use integration to estimate $\sum\limits_{n=0}^{+\infty}(-1)^n\frac 1 {n+1}\sum\limits_{k=0}^n \frac 1 {k+1}$,Use integration to estimate,\sum\limits_{n=0}^{+\infty}(-1)^n\frac 1 {n+1}\sum\limits_{k=0}^n \frac 1 {k+1},"Let $$v_n=\dfrac 1 {n+1}\sum_{k=0}^n \dfrac 1 {k+1}$$ We wanna study the sum $$S=\sum_{n=0}^{+\infty}(-1)^n v_n$$ The problem says we should first find $\omega(x)$ s.t.  $$v_n=\int_0^1 x^n\omega(x)dx$$ Then we'll have $S=\int_0^1\dfrac {\omega(x)} {1+x}dx$, but I can't find such $\omega(x)$. What's the idea of constructing such integral?","Let $$v_n=\dfrac 1 {n+1}\sum_{k=0}^n \dfrac 1 {k+1}$$ We wanna study the sum $$S=\sum_{n=0}^{+\infty}(-1)^n v_n$$ The problem says we should first find $\omega(x)$ s.t.  $$v_n=\int_0^1 x^n\omega(x)dx$$ Then we'll have $S=\int_0^1\dfrac {\omega(x)} {1+x}dx$, but I can't find such $\omega(x)$. What's the idea of constructing such integral?",,"['real-analysis', 'sequences-and-series']"
18,How to calculate the series $\sum\limits_{n=1}^{\infty} \arctan(\frac{2}{n^{2}})$?,How to calculate the series ?,\sum\limits_{n=1}^{\infty} \arctan(\frac{2}{n^{2}}),"I encountered the series  $$ \sum_{n=1}^{\infty} \arctan\frac{2}{n^{2}}. $$ I know it converges (by ratio test), but if I need to calculate its limit explicitly, how do I do that? Any hint would be helpful..","I encountered the series  $$ \sum_{n=1}^{\infty} \arctan\frac{2}{n^{2}}. $$ I know it converges (by ratio test), but if I need to calculate its limit explicitly, how do I do that? Any hint would be helpful..",,"['real-analysis', 'sequences-and-series']"
19,Proof that the Cardinality of Borel Sets on $\mathbb R$ is $c$ without using the ordinals .,Proof that the Cardinality of Borel Sets on  is  without using the ordinals .,\mathbb R c,"I'm trying to prove that cardinality of Borel sets is $c$ without using the concept of ordinal number. I know that the Cardinal of Borel sets are greater than $c$ because of every point in $\mathbb R$ is Borel set, therefore it's enough to show that the cardinal of borel sets is at most $c$ . My idea about this question is as follow : $$B_{(I,J)}=\left(\bigcup_{i\in I}A_i\right)\cup \left(\bigcap_{j\in J}A_j\right)$$ here $I,J$ are arbitrary subset of $\mathbb N$ and since $\mathbb R$ is second countable so has countable basis $\{{A_i}\}_{i \in \mathbb N}$ . I seek to construct a sigma-algebra on $\{B_{(I,J)}\}$ Can somebody improve my idea or give me another idea? Thanks","I'm trying to prove that cardinality of Borel sets is $c$ without using the concept of ordinal number. I know that the Cardinal of Borel sets are greater than $c$ because of every point in $\mathbb R$ is Borel set, therefore it's enough to show that the cardinal of borel sets is at most $c$ . My idea about this question is as follow : $$B_{(I,J)}=\left(\bigcup_{i\in I}A_i\right)\cup \left(\bigcap_{j\in J}A_j\right)$$ here $I,J$ are arbitrary subset of $\mathbb N$ and since $\mathbb R$ is second countable so has countable basis $\{{A_i}\}_{i \in \mathbb N}$ . I seek to construct a sigma-algebra on $\{B_{(I,J)}\}$ Can somebody improve my idea or give me another idea? Thanks",,"['real-analysis', 'measure-theory', 'descriptive-set-theory', 'borel-sets']"
20,Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a function such that $f'(x)$ is continuous and $|f'(x)|\le|f(x)|$ for all $x\in\mathbb{R}$,Let  be a function such that  is continuous and  for all,f:\mathbb{R}\rightarrow\mathbb{R} f'(x) |f'(x)|\le|f(x)| x\in\mathbb{R},"Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a function such that $f'(x)$ is continuous and $|f'(x)|\le|f(x)|$ for all $x\in\mathbb{R}$. If $f(0)=0$, find the maximum value of $f(5)$. $f'(x)=f(x)$ is true when $f(x)=ke^x$. $f(x)=0$ satisfies the condition. So $f(5)=0$ which is also the correct answer. But is there any method other than substitution?","Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a function such that $f'(x)$ is continuous and $|f'(x)|\le|f(x)|$ for all $x\in\mathbb{R}$. If $f(0)=0$, find the maximum value of $f(5)$. $f'(x)=f(x)$ is true when $f(x)=ke^x$. $f(x)=0$ satisfies the condition. So $f(5)=0$ which is also the correct answer. But is there any method other than substitution?",,"['calculus', 'real-analysis']"
21,I want to calculate the limit of: $\lim_{x \to 0} \left(\frac{2^x+8^x}{2} \right)^\frac{1}{x} $,I want to calculate the limit of:,\lim_{x \to 0} \left(\frac{2^x+8^x}{2} \right)^\frac{1}{x} ,"I want to calculate the limit of: $$\lim_{x \to 0} \left(\frac{2^x+8^x}{2} \right)^\frac{1}{x} $$ or prove that it does not exist. Now I know the result is $4$, but I am having trouble getting to it. Any ideas would be greatly appreciated.","I want to calculate the limit of: $$\lim_{x \to 0} \left(\frac{2^x+8^x}{2} \right)^\frac{1}{x} $$ or prove that it does not exist. Now I know the result is $4$, but I am having trouble getting to it. Any ideas would be greatly appreciated.",,"['real-analysis', 'limits']"
22,"If the limit of a derivative is zero as $x \to \infty$, what can we say about $f(x+1)-f(x)$? [duplicate]","If the limit of a derivative is zero as , what can we say about ? [duplicate]",x \to \infty f(x+1)-f(x),"This question already has an answer here : If $f:\mathbb{R}\rightarrow \mathbb{R}$ is differentiable and $\lim_{x\to \infty } f^\prime(x)=0$ show that $\lim _{x\to \infty } (f(x+1)-f(x))=0$. (1 answer) Closed 5 years ago . Given a differentiable function $f$ such that  $$ \lim_{x \to +\infty}f'(x) = 0 $$ what can we say about $$ \lim_{x\to\infty}(f(x+1)-f(x)) \text{ ?} $$ My first thought was to use mean value theorem on $[x,x+1]$, and I will get that the limit is $0$, is this true? Thank you for your help.","This question already has an answer here : If $f:\mathbb{R}\rightarrow \mathbb{R}$ is differentiable and $\lim_{x\to \infty } f^\prime(x)=0$ show that $\lim _{x\to \infty } (f(x+1)-f(x))=0$. (1 answer) Closed 5 years ago . Given a differentiable function $f$ such that  $$ \lim_{x \to +\infty}f'(x) = 0 $$ what can we say about $$ \lim_{x\to\infty}(f(x+1)-f(x)) \text{ ?} $$ My first thought was to use mean value theorem on $[x,x+1]$, and I will get that the limit is $0$, is this true? Thank you for your help.",,['real-analysis']
23,Any elementary proof for Euler's product formula for sine [duplicate],Any elementary proof for Euler's product formula for sine [duplicate],,"This question already has answers here : Infinite Product $\prod\limits_{k=1}^\infty\left({1-\frac{x^2}{k^2\pi^2}}\right)$ (5 answers) Closed 8 years ago . Is there a proof for the following fomula of Euler which does not use complex analysis or fourier analysis? $${\sin (\pi x)} = \pi x\prod_{n=1}^\infty\left(1 - \frac{x^2}{n^2}\right).$$ Suppose that the student studied ""Introduction to Real Analysis"" by Bartle & Sherbert and they just learned what the convergence of an infinite product means.  Then is there a proof that is suitable for them?","This question already has answers here : Infinite Product $\prod\limits_{k=1}^\infty\left({1-\frac{x^2}{k^2\pi^2}}\right)$ (5 answers) Closed 8 years ago . Is there a proof for the following fomula of Euler which does not use complex analysis or fourier analysis? $${\sin (\pi x)} = \pi x\prod_{n=1}^\infty\left(1 - \frac{x^2}{n^2}\right).$$ Suppose that the student studied ""Introduction to Real Analysis"" by Bartle & Sherbert and they just learned what the convergence of an infinite product means.  Then is there a proof that is suitable for them?",,"['real-analysis', 'education']"
24,"Showing that $C^1[0,1]$ is a Banach space with the $||f||=||f||_\infty + ||f^\prime||_\infty$ norm.",Showing that  is a Banach space with the  norm.,"C^1[0,1] ||f||=||f||_\infty + ||f^\prime||_\infty","So I am a bit stuck on where to begin with this one... Show that $C^1[0,1]$ with the norm defined as $\|f\|=\|f\|_\infty + \|f^\prime\|_\infty$ is a Banach space. I started with an arbitrary Cauchy sequence, $f_n \to f$ , so we know that for every $\epsilon > 0, \exists M \in \mathbb{N}$ such that $\|f_n - f\| \leq \epsilon , \forall n \geq M.$ Where I am getting stuck, is how do we then show that $f \in C^1[0,1]$ , and that therefore $C^1[0,1]$ with the norm defined, is complete (and therefore a Banach space). So here is my attempted solution, based on the comments below: Assume $f_n$ is a Cauchy sequence in $C^1[0,1]$ , then for every $\epsilon > 0, \exists M \in \mathbb{N}$ such that $$\|f_n - f_m\|=\|f_n - f_m\|_\infty + \|f_n^\prime - f_m^\prime \|_\infty \leq \epsilon, \forall m,n\geq M.$$ So for any fixed $x \in C^1[0,1], \|f_n - f_m\|_\infty \leq \epsilon$ and $\|f_n^\prime - f_m^\prime \|_\infty \leq \epsilon$ and thus for this fixed $x$ $f_n$ and $f_n^\prime$ are Cauchy sequences, and therefore converge: $f_n \to f$ and $f_n^\prime \to f^\prime.$ So let $m \to \infty$ then $\|f_n - f\|\leq \epsilon, \forall n\geq M, \forall x\in [0,1]$ . Then: \begin{align}\|f_n - f\|&= \|f_n - f\|_\infty + \|f_n^\prime - f^\prime \|_\infty\\ &=\sup_{x\in [0,1]}|f_n(x)-f(x)| + \sup_{x\in [0,1]}|f_n^\prime (x)-f^\prime (x)|\\&\leq \epsilon\quad \forall n\geq M. \end{align} Therefore $\lim_{n\to \infty} \|f_n - f\| = 0 \implies$ $C^1[0,1]$ is complete. How is this looking?","So I am a bit stuck on where to begin with this one... Show that with the norm defined as is a Banach space. I started with an arbitrary Cauchy sequence, , so we know that for every such that Where I am getting stuck, is how do we then show that , and that therefore with the norm defined, is complete (and therefore a Banach space). So here is my attempted solution, based on the comments below: Assume is a Cauchy sequence in , then for every such that So for any fixed and and thus for this fixed and are Cauchy sequences, and therefore converge: and So let then . Then: Therefore is complete. How is this looking?","C^1[0,1] \|f\|=\|f\|_\infty + \|f^\prime\|_\infty f_n \to f \epsilon > 0, \exists M \in \mathbb{N} \|f_n - f\| \leq \epsilon , \forall n \geq M. f \in C^1[0,1] C^1[0,1] f_n C^1[0,1] \epsilon > 0, \exists M \in \mathbb{N} \|f_n - f_m\|=\|f_n - f_m\|_\infty + \|f_n^\prime - f_m^\prime \|_\infty \leq \epsilon, \forall m,n\geq M. x \in C^1[0,1], \|f_n - f_m\|_\infty \leq \epsilon \|f_n^\prime - f_m^\prime \|_\infty \leq \epsilon x f_n f_n^\prime f_n \to f f_n^\prime \to f^\prime. m \to \infty \|f_n - f\|\leq \epsilon, \forall n\geq M, \forall x\in [0,1] \begin{align}\|f_n - f\|&= \|f_n - f\|_\infty + \|f_n^\prime - f^\prime \|_\infty\\
&=\sup_{x\in [0,1]}|f_n(x)-f(x)| + \sup_{x\in [0,1]}|f_n^\prime (x)-f^\prime (x)|\\&\leq \epsilon\quad \forall n\geq M.
\end{align} \lim_{n\to \infty} \|f_n - f\| = 0 \implies C^1[0,1]","['real-analysis', 'functional-analysis', 'solution-verification']"
25,$\Gamma \subset \mathbb{R}^{+}$ is uncountable. Can we choose a sequence from $\Gamma$ of which the sum is $\infty$,is uncountable. Can we choose a sequence from  of which the sum is,\Gamma \subset \mathbb{R}^{+} \Gamma \infty,"If $\Gamma$ is a set of uncountably many different positive real numbers, can we choose a sequence of pairwise different positive numbers from $\Gamma$, say $\{a_n\}$, such that $\sum a_n = \infty$ ? I am considering such problem and I think the answer should be yes . But what's the rigorous way to prove it? Please give me some hints and I will appreciate any help :-)","If $\Gamma$ is a set of uncountably many different positive real numbers, can we choose a sequence of pairwise different positive numbers from $\Gamma$, say $\{a_n\}$, such that $\sum a_n = \infty$ ? I am considering such problem and I think the answer should be yes . But what's the rigorous way to prove it? Please give me some hints and I will appreciate any help :-)",,"['real-analysis', 'elementary-set-theory']"
26,If $(x_n) \to x$ then $(\sqrt[n]{x_1x_2\cdots x_n}) \to x$,If  then,(x_n) \to x (\sqrt[n]{x_1x_2\cdots x_n}) \to x,"This is not a duplicate of this question . The linked question says that it suffices to show that if $(x_n)\to x$ then $(\frac{x_1+\cdots+x_n}{n})\to x$ to prove my question, but how so? I tried using the same strategy as how one proves that if $(x_n)\to x$ then $(\frac{x_1+\cdots+x_n}{n})\to x$, by ""splitting"" the product in th $N$th term: $$\sqrt[n]{x_1x_2\cdots x_n}=\sqrt[n]{x_1x_2\cdots x_Nx_{N+1}\cdots x_n}=\sqrt[n]{x_1x_2\cdots x_N} \sqrt[n]{x_{N+1}\cdots x_n}$$ but it seems I can't use for now the definition of convergence of $(x_n)$ because of the $n$th root. I also tried to use a result: if $(x_n)\to x$ then $(\frac{x_n}{n})\to 1$ but don't know if this is true. Sadly I've had no real progress. Any help will be greatly appreciated, thanks in advance!","This is not a duplicate of this question . The linked question says that it suffices to show that if $(x_n)\to x$ then $(\frac{x_1+\cdots+x_n}{n})\to x$ to prove my question, but how so? I tried using the same strategy as how one proves that if $(x_n)\to x$ then $(\frac{x_1+\cdots+x_n}{n})\to x$, by ""splitting"" the product in th $N$th term: $$\sqrt[n]{x_1x_2\cdots x_n}=\sqrt[n]{x_1x_2\cdots x_Nx_{N+1}\cdots x_n}=\sqrt[n]{x_1x_2\cdots x_N} \sqrt[n]{x_{N+1}\cdots x_n}$$ but it seems I can't use for now the definition of convergence of $(x_n)$ because of the $n$th root. I also tried to use a result: if $(x_n)\to x$ then $(\frac{x_n}{n})\to 1$ but don't know if this is true. Sadly I've had no real progress. Any help will be greatly appreciated, thanks in advance!",,"['real-analysis', 'sequences-and-series', 'limits', 'radicals']"
27,"Sigma-algebra requirement 3, closed under countable unions.","Sigma-algebra requirement 3, closed under countable unions.",,"The requirement for sigma-algebra is that. It contains the empty set. If A is in the sigma-algebra, then the complement of A is there. 3. It is closed under countable unions. My question relates to 3. When we define the measure-function, it has to be disjoint countable unions, but not when we define a sigma-algebra. I have tried to prove that if we require instead that the sigma algebra is closed under countable unions of disjoint sets, then it is closed under sets that are not disjoint. But this must be false, I think. Is there a way to prove this? That is I want to show that if I define a sigma algebra to be: It contains the empty set. If A is in the sigma-algebra, then the complement of A is there. It is closed under countable unions of disjoint sets. Then it is not nececarrily closed under countable unions if the sets are not disjoint. I guess this can be shown by creating a a set that satisfies the new definition, but is not closed under countable unions where what we take union over is not disjoint. But is it difficult to create a counterexample like this? If they don't exist then the definitions are equal, but I don't think they are?","The requirement for sigma-algebra is that. It contains the empty set. If A is in the sigma-algebra, then the complement of A is there. 3. It is closed under countable unions. My question relates to 3. When we define the measure-function, it has to be disjoint countable unions, but not when we define a sigma-algebra. I have tried to prove that if we require instead that the sigma algebra is closed under countable unions of disjoint sets, then it is closed under sets that are not disjoint. But this must be false, I think. Is there a way to prove this? That is I want to show that if I define a sigma algebra to be: It contains the empty set. If A is in the sigma-algebra, then the complement of A is there. It is closed under countable unions of disjoint sets. Then it is not nececarrily closed under countable unions if the sets are not disjoint. I guess this can be shown by creating a a set that satisfies the new definition, but is not closed under countable unions where what we take union over is not disjoint. But is it difficult to create a counterexample like this? If they don't exist then the definitions are equal, but I don't think they are?",,"['real-analysis', 'measure-theory', 'examples-counterexamples']"
28,Open dense subset with small outer measure,Open dense subset with small outer measure,,"Show that for any $\delta>0$ there exists an open dense subset $U$ of $\mathbb{R}$ with $\mu(U)<\delta$, where the measure is the outer measure. I'm not sure how to go here. The set $U$ must be dense (so for any point $x\in\mathbb{R}$ there exists a point in $U$ arbitrarily close to $x$), open (so any point in $U$ has a neighborhood entirely contained in $U$), and yet can be covered by a countable union of intervals with really small length.","Show that for any $\delta>0$ there exists an open dense subset $U$ of $\mathbb{R}$ with $\mu(U)<\delta$, where the measure is the outer measure. I'm not sure how to go here. The set $U$ must be dense (so for any point $x\in\mathbb{R}$ there exists a point in $U$ arbitrarily close to $x$), open (so any point in $U$ has a neighborhood entirely contained in $U$), and yet can be covered by a countable union of intervals with really small length.",,"['real-analysis', 'measure-theory']"
29,$\lim_{x\to \infty}\left(\frac{2\arctan(x)}{\pi}\right)^x=? $,,\lim_{x\to \infty}\left(\frac{2\arctan(x)}{\pi}\right)^x=? ,I`m trying to evaluate this limit and I need some advice how to do that. $$\displaystyle\lim_{x\to \infty}\left(\frac{2\arctan(x)}{\pi}\right)^x $$ I have a feeling it has to do with a solution of form $1^\infty$ but do not know how to proceed. Any hints/solutions/links will be appreciated,I`m trying to evaluate this limit and I need some advice how to do that. I have a feeling it has to do with a solution of form but do not know how to proceed. Any hints/solutions/links will be appreciated,\displaystyle\lim_{x\to \infty}\left(\frac{2\arctan(x)}{\pi}\right)^x  1^\infty,"['real-analysis', 'calculus', 'limits']"
30,"If the graph $G(f)$ of $f : [a, b] \rightarrow \mathbb{R}$ is path-connected, then $f$ is continuous.","If the graph  of  is path-connected, then  is continuous.","G(f) f : [a, b] \rightarrow \mathbb{R} f","Yesterday I woke up thinking about this question, and I believe I have a proof, but I'm not sure of its validity. Let $\gamma : [a, b] \rightarrow \mathbb{R}^{2}$ be a path from $(a, f(a))$ to $(b, f(b))$. First of all, I will show we may take WLOG $\gamma$ to be injective. Consider a point $\alpha = (p, q) \in \mathbb{R}^{2}$. Consider the set $S = \{x \in [a, b]; \gamma(x) = \alpha\}$, which we suppose non-empty. Let $c = \inf S$, $c' = \sup S$. By the continuity of $\gamma$, we have $\gamma(c) = \gamma(c') = \alpha$. We may now consider the equivalence relation $\sim$ on $[a, b]$ which sets $x \sim x$ and $z \sim y$ for $z, y \in [c, c']$. The quotient space $[a, b] / \sim$ is homeomorphic to an interval (since $a < c \leq c' < b$, by the continuity of $\gamma$), and the induced map $\bar{\gamma} : [a, b] / \sim \rightarrow \mathbb{R}^{2}$ may be taken as a path which passes through $\alpha$ only once. Since $[a, b]$ is compact and $\mathbb{R}^{2}$ is Hausdorff, $\gamma$ is a homeomorphism onto its image. Clearly $\gamma([a, b]) \subset G(f)$. We must now show this is an equality, which is done via the intermediate value theorem. Consider $\pi_{1} : G(f) \rightarrow \mathbb{R}$ to be the projection onto the first coordinate, which is continuous. Consider $\phi = \pi_{1} \circ \gamma : [a, b] \rightarrow [a, b]$, which also is continuous. Since $\phi(a) = a, \phi(b) = b$, $\phi([a, b]) = [a, b]$, and therefore $\gamma([a, b]) = G(f)$. Thus $f$ is a function whose graph is homeomorphic to its domain. We will show in the next paragraph that this implies the continuity of $f$. Indeed, suppose $g : [a, b] \rightarrow \mathbb{R}$ has a homeomorphism $H : [a, b] \rightarrow G(g)$. This induces a map $T : [a, b] \rightarrow [a, b]$ such that $H(x) = (T(x), g(T(x))$. We note $T$ is continuous since $T = \pi_{1} \circ H$. It is clearly seen $T$ is bijective, and thus a homeomorphism since [a, b] is compact and Hausdorff. Since $g \circ T$ is continuous, $g \circ T \circ T^{-1} = g$ also is.","Yesterday I woke up thinking about this question, and I believe I have a proof, but I'm not sure of its validity. Let $\gamma : [a, b] \rightarrow \mathbb{R}^{2}$ be a path from $(a, f(a))$ to $(b, f(b))$. First of all, I will show we may take WLOG $\gamma$ to be injective. Consider a point $\alpha = (p, q) \in \mathbb{R}^{2}$. Consider the set $S = \{x \in [a, b]; \gamma(x) = \alpha\}$, which we suppose non-empty. Let $c = \inf S$, $c' = \sup S$. By the continuity of $\gamma$, we have $\gamma(c) = \gamma(c') = \alpha$. We may now consider the equivalence relation $\sim$ on $[a, b]$ which sets $x \sim x$ and $z \sim y$ for $z, y \in [c, c']$. The quotient space $[a, b] / \sim$ is homeomorphic to an interval (since $a < c \leq c' < b$, by the continuity of $\gamma$), and the induced map $\bar{\gamma} : [a, b] / \sim \rightarrow \mathbb{R}^{2}$ may be taken as a path which passes through $\alpha$ only once. Since $[a, b]$ is compact and $\mathbb{R}^{2}$ is Hausdorff, $\gamma$ is a homeomorphism onto its image. Clearly $\gamma([a, b]) \subset G(f)$. We must now show this is an equality, which is done via the intermediate value theorem. Consider $\pi_{1} : G(f) \rightarrow \mathbb{R}$ to be the projection onto the first coordinate, which is continuous. Consider $\phi = \pi_{1} \circ \gamma : [a, b] \rightarrow [a, b]$, which also is continuous. Since $\phi(a) = a, \phi(b) = b$, $\phi([a, b]) = [a, b]$, and therefore $\gamma([a, b]) = G(f)$. Thus $f$ is a function whose graph is homeomorphic to its domain. We will show in the next paragraph that this implies the continuity of $f$. Indeed, suppose $g : [a, b] \rightarrow \mathbb{R}$ has a homeomorphism $H : [a, b] \rightarrow G(g)$. This induces a map $T : [a, b] \rightarrow [a, b]$ such that $H(x) = (T(x), g(T(x))$. We note $T$ is continuous since $T = \pi_{1} \circ H$. It is clearly seen $T$ is bijective, and thus a homeomorphism since [a, b] is compact and Hausdorff. Since $g \circ T$ is continuous, $g \circ T \circ T^{-1} = g$ also is.",,"['real-analysis', 'general-topology']"
31,How do I calculate $\lim_{x\rightarrow 0} x\ln x$,How do I calculate,\lim_{x\rightarrow 0} x\ln x,"I was thinking about the reasons behind $0^0=1$ and I remember one of my friends studying math arguing about the continuity of the function $x^x$ in $0$. But when I write as $$x^x=e^{x\ln x}$$ I am now looking at $$\lim_{x\rightarrow 0} x\ln x$$ Graphically I can see in Mathematica that it goes to $0.$ But I can't calculate by using a Taylor expansion, because I can't expand log around $0$. How do you prove that?","I was thinking about the reasons behind $0^0=1$ and I remember one of my friends studying math arguing about the continuity of the function $x^x$ in $0$. But when I write as $$x^x=e^{x\ln x}$$ I am now looking at $$\lim_{x\rightarrow 0} x\ln x$$ Graphically I can see in Mathematica that it goes to $0.$ But I can't calculate by using a Taylor expansion, because I can't expand log around $0$. How do you prove that?",,"['real-analysis', 'limits', 'logarithms']"
32,Closure of a connected subset of $\mathbb{R}$ is connected?,Closure of a connected subset of  is connected?,\mathbb{R},"Prove or disprove: The closure of a connected set in $\mathbb{R}$ is always connected. Response: I don't really have a grasp on this conceptually, but here's an attempt. Proof: Let $X$ be a connected subset of $R$, and let $Z$ be the closure of $X$. Suppose (to get a contradiction) that $Z$ is disconnected. That means there are two nonempty open subsets of $R$ covering $Z$, call them $U$ and $V$, such that $U\cap Z$ and $V\cap Z$ are nonempty while $U\cap V\cap Z$ is empty. Then $U$ and $V$ cover $X$ also. And since $U\cap V\cap X \subseteq U\cap V\cap Z$, then $U\cap V\cap X$ is empty. Pick points $u\in U\cap Z$ and $v\in V\cap Z$ (which exist since those sets are nonempty). That means $u$ is in the closure of $X$ and $U$ is an open neighborhood of $u$, so $U\cap X$ is nonempty. Similarly, $V\cap X$ is nonempty. But then the previous paragraph shows that $X$ is disconnected, which is a contradiction, so $Z$ must have actually been connected.","Prove or disprove: The closure of a connected set in $\mathbb{R}$ is always connected. Response: I don't really have a grasp on this conceptually, but here's an attempt. Proof: Let $X$ be a connected subset of $R$, and let $Z$ be the closure of $X$. Suppose (to get a contradiction) that $Z$ is disconnected. That means there are two nonempty open subsets of $R$ covering $Z$, call them $U$ and $V$, such that $U\cap Z$ and $V\cap Z$ are nonempty while $U\cap V\cap Z$ is empty. Then $U$ and $V$ cover $X$ also. And since $U\cap V\cap X \subseteq U\cap V\cap Z$, then $U\cap V\cap X$ is empty. Pick points $u\in U\cap Z$ and $v\in V\cap Z$ (which exist since those sets are nonempty). That means $u$ is in the closure of $X$ and $U$ is an open neighborhood of $u$, so $U\cap X$ is nonempty. Similarly, $V\cap X$ is nonempty. But then the previous paragraph shows that $X$ is disconnected, which is a contradiction, so $Z$ must have actually been connected.",,"['real-analysis', 'connectedness']"
33,What does a well ordering of $\mathbb{R}$ look like? [duplicate],What does a well ordering of  look like? [duplicate],\mathbb{R},"This question already has answers here : Closed 11 years ago . Possible Duplicate: Is there a known well ordering of the reals? I am having a hard time wrapping my head around what a well-ordering of $\mathbb{R}$ looks like. I have seen the presentation of a well-ordering of $\mathbb{Z}$ ie $0, -1, 1, -2, 2, \ldots$ but how could you do this type of ordering with $\mathbb{R}$ where numbers are not countable in that way? Edit: I'm guessing what I'm asking is if $0, -\epsilon , \epsilon, -2\epsilon, 2\epsilon,\ldots$ could be thought of as  a well-ordering of $\mathbb{R}$ in a similar fashion.","This question already has answers here : Closed 11 years ago . Possible Duplicate: Is there a known well ordering of the reals? I am having a hard time wrapping my head around what a well-ordering of $\mathbb{R}$ looks like. I have seen the presentation of a well-ordering of $\mathbb{Z}$ ie $0, -1, 1, -2, 2, \ldots$ but how could you do this type of ordering with $\mathbb{R}$ where numbers are not countable in that way? Edit: I'm guessing what I'm asking is if $0, -\epsilon , \epsilon, -2\epsilon, 2\epsilon,\ldots$ could be thought of as  a well-ordering of $\mathbb{R}$ in a similar fashion.",,"['real-analysis', 'order-theory', 'axiom-of-choice']"
34,Open Measurable Sets Containing All Rational Numbers,Open Measurable Sets Containing All Rational Numbers,,"So I am trying to figure out a proof for the following statement, but I'm not really sure how to go about it.  The statement is: ""Show that for every $\epsilon>0$, there exists an open set G in $\mathbb{R}$ which contains all of the rational numbers but $m(G)<\epsilon$.""  How can it be true that the open set G contains all of the rational numbers but has an arbitrarily small measure?","So I am trying to figure out a proof for the following statement, but I'm not really sure how to go about it.  The statement is: ""Show that for every $\epsilon>0$, there exists an open set G in $\mathbb{R}$ which contains all of the rational numbers but $m(G)<\epsilon$.""  How can it be true that the open set G contains all of the rational numbers but has an arbitrarily small measure?",,"['real-analysis', 'measure-theory']"
35,regularization of a divergent integral,regularization of a divergent integral,,"is there any way to regularize the following divergent  integral : $$\int_{0}^{\infty}\frac{dx}{xe^{x}(e^{x}-1)}$$ the integral comes from trying to find an analytic continuation of  $$I(s)=s\int_{0}^{\infty} \frac{dx}{2x}\left(E_{s/2}((\pi x)^{s/2})-1\right)\omega(x)-\left(E_{s/2}(( x)^{s/2})-1\right)e^{-x}$$ $E_{s}(x)$ is the mittag-leffler function . and admits the beautiful continuation : $E_{s}(x^{-1})=1-E_{-s}(x)$  and $$\omega(x)=\sum_{n=1}^{\infty}e^{-n^{2}\pi x}$$ which is the jacobi theta function in disguise another representation of the integral above is - for a minute assume i'm correct about this equivalence -  : $$I(s)=s\int_{0}^{\infty}\frac{E_{s}(x^{s})-1}{xe^{x}(e^{x}-1)}dx$$ i was wondering if we can apply Riemann's trick, and replace this integral with a contour integral to obtain a meromorphic integral !? following Riemann's trick, here is what i did : start with contour integral : $$I(s)=-s\oint_{c}\frac{E_{s}((-x)^{s})-1}{xe^{x}(e^{x}-1)}dx$$ the contour is the usual Hankel contour. consider $I(-s)$ : $$I(-s)=s\oint_{c}\frac{E_{-s}((-x)^{-s})-1}{xe^{x}(e^{x}-1)}dx=-s\oint_{c}\frac{E_{s}((-x)^{s})}{xe^{x}(e^{x}-1)}dx$$ or $$I(s)-I(-s)=s\oint_{c}\frac{dx}{xe^{x}(e^{x}-1)}=s\oint_{c}(-x)^{-1}e^{-x}dx-s\oint_{c}\frac{(-x)^{-1}dx}{e^{x}-1}$$ now :$$\oint_{c}(-x)^{-1}e^{-x}dx=\frac{-2\pi i}{\Gamma(1)}=-2\pi i$$ and the second integral could be thought of as: $$\oint_{c}\frac{(-x)^{-1}dx}{e^{x}-1}=\lim_{z\rightarrow 0}\oint_{c}\frac{(-x)^{z-1}dx}{e^{x}-1}=-2i\lim_{z\rightarrow 0}\sin(\pi z)\Gamma(z)\zeta(z)=i\pi$$ or : $$I(s)-I(-s)=-3\pi is$$ lets go back to the 1st integral, and expand the Mittag-leffler function : $$I(s)=-s\oint_{c}\frac{E_{s}((-x)^{s})-1}{xe^{x}(e^{x}-1)}dx=-s\sum_{n=1}^{\infty}\frac{1}{\Gamma(1+ns)}\oint_{c}\frac{(-x)^{sk-1}dx}{e^{x}(e^{x}-1)}$$ $$=s\sum_{n=1}^{\infty}\frac{2i \sin(k\pi s)\Gamma(ks)}{\Gamma(1+ns)}\left(\zeta(ks)-1\right)=2i\sum_{n=1}^{\infty}\sin(k\pi s)\frac{\zeta(ks)-1}{k}$$ now the problem becomes finding a function of the variable s -lets call it $A(s)$- such that: $$\sum_{n=1}^{\infty}\sin(k\pi s)\frac{\zeta(ks)-1}{k}=A(s)\sum_{n=1}^{\infty}\frac{\zeta(ks)-1}{k}$$ if we define : $$k(s)=\sum_{n=1}^{\infty}\frac{\zeta(ks)-1}{k}$$ then : $$A(s)k(s)-A(-s)k(-s)=-\frac{3}{2}\pi  s$$ and the problem becomes proving the existence of $A(s)$ for all s, and of course, finding it !!","is there any way to regularize the following divergent  integral : $$\int_{0}^{\infty}\frac{dx}{xe^{x}(e^{x}-1)}$$ the integral comes from trying to find an analytic continuation of  $$I(s)=s\int_{0}^{\infty} \frac{dx}{2x}\left(E_{s/2}((\pi x)^{s/2})-1\right)\omega(x)-\left(E_{s/2}(( x)^{s/2})-1\right)e^{-x}$$ $E_{s}(x)$ is the mittag-leffler function . and admits the beautiful continuation : $E_{s}(x^{-1})=1-E_{-s}(x)$  and $$\omega(x)=\sum_{n=1}^{\infty}e^{-n^{2}\pi x}$$ which is the jacobi theta function in disguise another representation of the integral above is - for a minute assume i'm correct about this equivalence -  : $$I(s)=s\int_{0}^{\infty}\frac{E_{s}(x^{s})-1}{xe^{x}(e^{x}-1)}dx$$ i was wondering if we can apply Riemann's trick, and replace this integral with a contour integral to obtain a meromorphic integral !? following Riemann's trick, here is what i did : start with contour integral : $$I(s)=-s\oint_{c}\frac{E_{s}((-x)^{s})-1}{xe^{x}(e^{x}-1)}dx$$ the contour is the usual Hankel contour. consider $I(-s)$ : $$I(-s)=s\oint_{c}\frac{E_{-s}((-x)^{-s})-1}{xe^{x}(e^{x}-1)}dx=-s\oint_{c}\frac{E_{s}((-x)^{s})}{xe^{x}(e^{x}-1)}dx$$ or $$I(s)-I(-s)=s\oint_{c}\frac{dx}{xe^{x}(e^{x}-1)}=s\oint_{c}(-x)^{-1}e^{-x}dx-s\oint_{c}\frac{(-x)^{-1}dx}{e^{x}-1}$$ now :$$\oint_{c}(-x)^{-1}e^{-x}dx=\frac{-2\pi i}{\Gamma(1)}=-2\pi i$$ and the second integral could be thought of as: $$\oint_{c}\frac{(-x)^{-1}dx}{e^{x}-1}=\lim_{z\rightarrow 0}\oint_{c}\frac{(-x)^{z-1}dx}{e^{x}-1}=-2i\lim_{z\rightarrow 0}\sin(\pi z)\Gamma(z)\zeta(z)=i\pi$$ or : $$I(s)-I(-s)=-3\pi is$$ lets go back to the 1st integral, and expand the Mittag-leffler function : $$I(s)=-s\oint_{c}\frac{E_{s}((-x)^{s})-1}{xe^{x}(e^{x}-1)}dx=-s\sum_{n=1}^{\infty}\frac{1}{\Gamma(1+ns)}\oint_{c}\frac{(-x)^{sk-1}dx}{e^{x}(e^{x}-1)}$$ $$=s\sum_{n=1}^{\infty}\frac{2i \sin(k\pi s)\Gamma(ks)}{\Gamma(1+ns)}\left(\zeta(ks)-1\right)=2i\sum_{n=1}^{\infty}\sin(k\pi s)\frac{\zeta(ks)-1}{k}$$ now the problem becomes finding a function of the variable s -lets call it $A(s)$- such that: $$\sum_{n=1}^{\infty}\sin(k\pi s)\frac{\zeta(ks)-1}{k}=A(s)\sum_{n=1}^{\infty}\frac{\zeta(ks)-1}{k}$$ if we define : $$k(s)=\sum_{n=1}^{\infty}\frac{\zeta(ks)-1}{k}$$ then : $$A(s)k(s)-A(-s)k(-s)=-\frac{3}{2}\pi  s$$ and the problem becomes proving the existence of $A(s)$ for all s, and of course, finding it !!",,"['real-analysis', 'integration', 'special-functions', 'regularization', 'divergent-integrals']"
36,$ f: \mathbb{R}^n \to \mathbb{R}^m $ preserving distances,preserving distances, f: \mathbb{R}^n \to \mathbb{R}^m ,"Let $ f: \mathbb{R}^n \to \mathbb{R}^m $ be a function, that preserves distances. Prove that there exist a linear transformation $T$, and a vector $\mathbf{j} \in \mathbb{R}^m $ such that $ f(\mathbf{x}) = T\mathbf{x}+\mathbf{j}$ for every $\mathbf{x} \in \mathbb{R}^n $. First I suppose that $f(\mathbf{0})=\mathbf{0}$, since I can translate, without losing the property of preserving distances. So I need to prove that $f$ is linear. If I simply prove that $ f(\mathbf{x}+\mathbf{y}) = f(\mathbf{x})+f(\mathbf{y})$  then it´s done, because obviously I can deduce that this implies $ f(r\mathbf{x}) = rf(\mathbf{x}) $ with $r$ rational. But since I know that $f$ preserves distances, then in particular $f$ is continuous, and it´s easy to prove that this implies that $ f(c\mathbf{x}) = cf(\mathbf{x})$ for every real number $c$. But I don´t know How can I prove that $f$ respects the sum.","Let $ f: \mathbb{R}^n \to \mathbb{R}^m $ be a function, that preserves distances. Prove that there exist a linear transformation $T$, and a vector $\mathbf{j} \in \mathbb{R}^m $ such that $ f(\mathbf{x}) = T\mathbf{x}+\mathbf{j}$ for every $\mathbf{x} \in \mathbb{R}^n $. First I suppose that $f(\mathbf{0})=\mathbf{0}$, since I can translate, without losing the property of preserving distances. So I need to prove that $f$ is linear. If I simply prove that $ f(\mathbf{x}+\mathbf{y}) = f(\mathbf{x})+f(\mathbf{y})$  then it´s done, because obviously I can deduce that this implies $ f(r\mathbf{x}) = rf(\mathbf{x}) $ with $r$ rational. But since I know that $f$ preserves distances, then in particular $f$ is continuous, and it´s easy to prove that this implies that $ f(c\mathbf{x}) = cf(\mathbf{x})$ for every real number $c$. But I don´t know How can I prove that $f$ respects the sum.",,['real-analysis']
37,"Does $f$ monotone and $f\in L_{1}([a,\infty))$ imply $\lim_{t\to\infty} t f(t)=0$?",Does  monotone and  imply ?,"f f\in L_{1}([a,\infty)) \lim_{t\to\infty} t f(t)=0","I want to show that if $f$ is non-increasing and $f\in L_{1}([a,\infty),m)$ where $m$ is Lebesgue measure then $\lim_{t\to\infty} t f(t)=0$. So far I've been able to show that $f\geq 0$ and that $\lim_{t\to\infty} f(t)=0$. Since monotone functions are differentiable a.e. I thought about using integration by parts but couldn't get anywhere with that. Any hints or suggestions would be greatly appreciated.","I want to show that if $f$ is non-increasing and $f\in L_{1}([a,\infty),m)$ where $m$ is Lebesgue measure then $\lim_{t\to\infty} t f(t)=0$. So far I've been able to show that $f\geq 0$ and that $\lim_{t\to\infty} f(t)=0$. Since monotone functions are differentiable a.e. I thought about using integration by parts but couldn't get anywhere with that. Any hints or suggestions would be greatly appreciated.",,"['real-analysis', 'measure-theory']"
38,Characterizing continuous functions based on the graph of the function,Characterizing continuous functions based on the graph of the function,,"I had asked this question: Characterising Continuous functions some time back, and this question is more or less related to that question. Suppose we have a function $f: \mathbb{R} \to \mathbb{R}$ and suppose the set  $G = \\{ (x,f(x) : x \in \mathbb{R}\\}$ is connected and closed in $\mathbb{R}^{2}$, then does it imply $f$ is continuous?","I had asked this question: Characterising Continuous functions some time back, and this question is more or less related to that question. Suppose we have a function $f: \mathbb{R} \to \mathbb{R}$ and suppose the set  $G = \\{ (x,f(x) : x \in \mathbb{R}\\}$ is connected and closed in $\mathbb{R}^{2}$, then does it imply $f$ is continuous?",,['real-analysis']
39,"What purely real analytic techniques are there to evaluate $\int_{-\pi/2}^{\pi/2}\frac{1}{1+\sin^4(x)}\,\mathrm{d}x$?",What purely real analytic techniques are there to evaluate ?,"\int_{-\pi/2}^{\pi/2}\frac{1}{1+\sin^4(x)}\,\mathrm{d}x","$\newcommand{\d}{\,\mathrm{d}}$ Last night, I evaluated the following integral: $$\begin{align}I:&=\int_{-\pi/2}^{\pi/2}\frac{1}{1+\sin^4(x)}\d x\\&=\int_{-1}^1\frac{1}{(1+x^4)\sqrt{1-x^2}}\d x\\&=\frac{\pi}{2^{3/4}} (\sin(\pi/8)+\cos(\pi/8))\\&=\frac{\pi}{2}\sqrt{1+\sqrt{2}}\end{align}$$ Using a ""double keyhole"" (as I phrase it) contour method involving a management of branch cuts and residues at infinity, here . Although I was happy to have succeeded in this, I wondered afterwards if I would have had any hope of evaluating $I$ with real analytic technique only. The challenge: Evaluate $I$ without use of complex analysis or even of complex arithmetic (e.g. for partial fraction decompositions involving $i$ ) I posed this to some friends and they came up with the following method which I wanted to share with MSE: $$\begin{align}I&\overset{x\mapsto\tan x}{=}\int_{-\infty}^\infty\frac{1+x^2}{(1+x^2)^2+x^4}\d x\\&\overset{x\mapsto1/x}{=}2\int_0^\infty\frac{1+x^2}{(1+x^2)^2+1}\d x\\&=2\int_0^\infty\int_0^\infty e^{-t(1+x^2)}\cos(t)\d t\d x\quad\text{Repr. with IBP}\\&=\sqrt{\pi}\int_0^\infty\frac{e^{-t}\cos(t)}{\sqrt{t}}\d t\end{align}$$ $$\begin{align}J:&=\int_0^\infty\frac{e^{-t}\cos(t)}{\sqrt{t}}\d t\\J^2&=\int_0^\infty\int_0^\infty\frac{e^{-(t+x)}\cos(t)\cos(x)}{\sqrt{tx}}\d t\d x\\&\overset{x\mapsto tx}{=}\int_0^\infty\int_0^\infty\frac{e^{-t(1+x)}\cos(t)\cos(tx)}{\sqrt{x}}\d x\d t\\&=\frac{1}{2}\int_0^\infty\frac{1}{\sqrt{x}}\cdot\frac{1+x+x^2}{(1+x)(1+x^2)}\d x\\&\overset{x\mapsto x^2}{=}\frac{1}{2}\int_0^\infty\left(\frac{1+x^2}{1+x^4}+\frac{1}{1+x^2}\right)\d x\\&=\frac{1}{2}\left[\frac{\pi}{4}\csc\left(\frac{\pi}{4}\right)+\frac{\pi}{4}\csc\left(\frac{3\pi}{4}\right)+\frac{\pi}{2}\right]\\&=\frac{\pi}{4}(1+\sqrt{2})\end{align}$$ Referencing this answer by Sangchul. We conclude: $$\begin{align}I&=\sqrt{\pi}\cdot\sqrt{J^2}\\&=\sqrt{\pi}\cdot\sqrt{\frac{\pi}{4}(1+\sqrt{2})}\\&=\frac{\pi}{2}\sqrt{1+\sqrt{2}}\end{align}$$ Among those who helped me, who use MSE, I credit @TheSimpliFire and @KStarGamer who are much better at real integration than I am! My question is less of a question and more of a request for a list - a list of other, purely real, methods to attack this integral. I hope the outcome of this will be an interesting selection of advanced integration techniques that I and others can learn from. Note 1: I am aware of this posting by Quanto but it uses complex numbers. Note 2: You must expand the cosine product as a sum of cosines and use the same integral representation (which is classically gotten from complex arithmetic but can be done with integration by parts): $$\int_0^\infty e^{-tx}\cos(t)\d t=\frac{x}{x^2+1},\,x\gt0$$","Last night, I evaluated the following integral: Using a ""double keyhole"" (as I phrase it) contour method involving a management of branch cuts and residues at infinity, here . Although I was happy to have succeeded in this, I wondered afterwards if I would have had any hope of evaluating with real analytic technique only. The challenge: Evaluate without use of complex analysis or even of complex arithmetic (e.g. for partial fraction decompositions involving ) I posed this to some friends and they came up with the following method which I wanted to share with MSE: Referencing this answer by Sangchul. We conclude: Among those who helped me, who use MSE, I credit @TheSimpliFire and @KStarGamer who are much better at real integration than I am! My question is less of a question and more of a request for a list - a list of other, purely real, methods to attack this integral. I hope the outcome of this will be an interesting selection of advanced integration techniques that I and others can learn from. Note 1: I am aware of this posting by Quanto but it uses complex numbers. Note 2: You must expand the cosine product as a sum of cosines and use the same integral representation (which is classically gotten from complex arithmetic but can be done with integration by parts):","\newcommand{\d}{\,\mathrm{d}} \begin{align}I:&=\int_{-\pi/2}^{\pi/2}\frac{1}{1+\sin^4(x)}\d x\\&=\int_{-1}^1\frac{1}{(1+x^4)\sqrt{1-x^2}}\d x\\&=\frac{\pi}{2^{3/4}}
(\sin(\pi/8)+\cos(\pi/8))\\&=\frac{\pi}{2}\sqrt{1+\sqrt{2}}\end{align} I I i \begin{align}I&\overset{x\mapsto\tan x}{=}\int_{-\infty}^\infty\frac{1+x^2}{(1+x^2)^2+x^4}\d x\\&\overset{x\mapsto1/x}{=}2\int_0^\infty\frac{1+x^2}{(1+x^2)^2+1}\d x\\&=2\int_0^\infty\int_0^\infty e^{-t(1+x^2)}\cos(t)\d t\d x\quad\text{Repr. with IBP}\\&=\sqrt{\pi}\int_0^\infty\frac{e^{-t}\cos(t)}{\sqrt{t}}\d t\end{align} \begin{align}J:&=\int_0^\infty\frac{e^{-t}\cos(t)}{\sqrt{t}}\d t\\J^2&=\int_0^\infty\int_0^\infty\frac{e^{-(t+x)}\cos(t)\cos(x)}{\sqrt{tx}}\d t\d x\\&\overset{x\mapsto tx}{=}\int_0^\infty\int_0^\infty\frac{e^{-t(1+x)}\cos(t)\cos(tx)}{\sqrt{x}}\d x\d t\\&=\frac{1}{2}\int_0^\infty\frac{1}{\sqrt{x}}\cdot\frac{1+x+x^2}{(1+x)(1+x^2)}\d x\\&\overset{x\mapsto x^2}{=}\frac{1}{2}\int_0^\infty\left(\frac{1+x^2}{1+x^4}+\frac{1}{1+x^2}\right)\d x\\&=\frac{1}{2}\left[\frac{\pi}{4}\csc\left(\frac{\pi}{4}\right)+\frac{\pi}{4}\csc\left(\frac{3\pi}{4}\right)+\frac{\pi}{2}\right]\\&=\frac{\pi}{4}(1+\sqrt{2})\end{align} \begin{align}I&=\sqrt{\pi}\cdot\sqrt{J^2}\\&=\sqrt{\pi}\cdot\sqrt{\frac{\pi}{4}(1+\sqrt{2})}\\&=\frac{\pi}{2}\sqrt{1+\sqrt{2}}\end{align} \int_0^\infty e^{-tx}\cos(t)\d t=\frac{x}{x^2+1},\,x\gt0","['real-analysis', 'integration', 'big-list']"
40,"Prove $\sum_{i,j:i<j} μ(E_{i} \cap E_{j} )=\infty$.",Prove .,"\sum_{i,j:i<j} μ(E_{i} \cap E_{j} )=\infty","The problem is: Assume that $(X,\mathcal{A} , μ)$ is a finite measure space (i.e., $μ(X) < \infty $ ) and the sequence of sets $E_{j}\in \mathcal{A}$ satisfies $\sum^\infty_{j=1} μ(E_{j} )=\infty$ . Show that then $\sum_{i,j:i<j} μ(E_{i} \cap E_{j} )=\infty$ . I tried to assume that $\sum_{i,j:i<j} μ(E_{i} \cap E_{j} )<\infty$ to get a contradiction, and I know that we can subtract since the measure is finite. I need help! I have no idea how to solve it. Thanks in advance.","The problem is: Assume that is a finite measure space (i.e., ) and the sequence of sets satisfies . Show that then . I tried to assume that to get a contradiction, and I know that we can subtract since the measure is finite. I need help! I have no idea how to solve it. Thanks in advance.","(X,\mathcal{A} , μ) μ(X) < \infty  E_{j}\in \mathcal{A} \sum^\infty_{j=1} μ(E_{j} )=\infty \sum_{i,j:i<j} μ(E_{i} \cap E_{j} )=\infty \sum_{i,j:i<j} μ(E_{i} \cap E_{j} )<\infty","['real-analysis', 'measure-theory']"
41,Can you prove that these two series are equal?,Can you prove that these two series are equal?,,"Let for all $x>0$ $$ f(x)=\sum_{n=0}^{+\infty}\frac{1}{x(x+1)\dots(x+n)}$$ Can you prove that for all $x>0$ $$f(x)= e \sum_{n=0}^{+\infty}\frac{(-1)^n}{(x+n)n!} $$ this is a question in a test for undergraduate students. I checked that the series that defines $f$ converges. Moreover i proved that it uniformly converges in every interval of the form $[a,+\infty[$ with $a>0$ . One of my attempts to solve the exercise was to trying to differentiate both series and see if the expression of the derivatives was easier to handle. but I didn't get anywhere. Any suggestions?",Let for all Can you prove that for all this is a question in a test for undergraduate students. I checked that the series that defines converges. Moreover i proved that it uniformly converges in every interval of the form with . One of my attempts to solve the exercise was to trying to differentiate both series and see if the expression of the derivatives was easier to handle. but I didn't get anywhere. Any suggestions?,"x>0  f(x)=\sum_{n=0}^{+\infty}\frac{1}{x(x+1)\dots(x+n)} x>0 f(x)= e \sum_{n=0}^{+\infty}\frac{(-1)^n}{(x+n)n!}  f [a,+\infty[ a>0","['real-analysis', 'sequences-and-series', 'analysis']"
42,Is there a closed form for $\sum_{n=1}^\infty\frac{2^{2n}H_n}{n^3{2n\choose n}}?$,Is there a closed form for,\sum_{n=1}^\infty\frac{2^{2n}H_n}{n^3{2n\choose n}}?,"I found $$\sum_{n=1}^\infty\frac{2^{2n}H_n}{n^3{2n\choose n}}=-8\int_0^{\pi/2}x^2\cot x\ln(\cos x)\ dx=I\tag1.$$ Mathematica failed to find $I$ , so I am not sure if there is closed form for it. I am just giving it a try here. First idea came to my mind is to use the Fourier series of $-\ln(\cos x)=\ln(2)+\sum_{n=1}^\infty\frac{(-1)^n\cos(2nx)}{n}$ and we have $$I=8\ln(2)\underbrace{\int_0^{\pi/2}x^2\cot x\ dx}_{\frac32\ln(2)\zeta(2)-\frac78\zeta(3)}+8\sum_{n=1}^\infty\frac{(-1)^n}{n}\int_0^{\pi/2}x^2 \cot x\cos(2nx)\ dx.$$ I got stuck here. Any help would be much appreciated. Proof of $(1)$ from here we have $$\arcsin^2(x)=\frac12\sum_{n=1}^\infty\frac{(2x)^{2n}}{n^2{2n\choose n}}$$ replace $x$ by $\sqrt{x}$ we get $$\sum_{n=1}^\infty\frac{2^{2n}x^n}{n^2{2n\choose n}}=2\arcsin^2(\sqrt{x})$$ multiply both sides by $-\frac{\ln(1-x)}{x}$ then $\int_0^1$ and use $-\int_0^1 x^{n-1}\ln(1-x)dx=\frac{H_n}{n}$ we get $$\sum_{n=1}^\infty\frac{2^{2n}H_n}{n^3{2n\choose n}}=2\int_0^1\frac{\arcsin^2(\sqrt{x})\ln(1-x)}{x}dx\overset{\sqrt{x}=\sin\theta}{=}-8\int_0^{\pi/2}x^2\cot x\ln(\cos x)\ dx$$","I found Mathematica failed to find , so I am not sure if there is closed form for it. I am just giving it a try here. First idea came to my mind is to use the Fourier series of and we have I got stuck here. Any help would be much appreciated. Proof of from here we have replace by we get multiply both sides by then and use we get",\sum_{n=1}^\infty\frac{2^{2n}H_n}{n^3{2n\choose n}}=-8\int_0^{\pi/2}x^2\cot x\ln(\cos x)\ dx=I\tag1. I -\ln(\cos x)=\ln(2)+\sum_{n=1}^\infty\frac{(-1)^n\cos(2nx)}{n} I=8\ln(2)\underbrace{\int_0^{\pi/2}x^2\cot x\ dx}_{\frac32\ln(2)\zeta(2)-\frac78\zeta(3)}+8\sum_{n=1}^\infty\frac{(-1)^n}{n}\int_0^{\pi/2}x^2 \cot x\cos(2nx)\ dx. (1) \arcsin^2(x)=\frac12\sum_{n=1}^\infty\frac{(2x)^{2n}}{n^2{2n\choose n}} x \sqrt{x} \sum_{n=1}^\infty\frac{2^{2n}x^n}{n^2{2n\choose n}}=2\arcsin^2(\sqrt{x}) -\frac{\ln(1-x)}{x} \int_0^1 -\int_0^1 x^{n-1}\ln(1-x)dx=\frac{H_n}{n} \sum_{n=1}^\infty\frac{2^{2n}H_n}{n^3{2n\choose n}}=2\int_0^1\frac{\arcsin^2(\sqrt{x})\ln(1-x)}{x}dx\overset{\sqrt{x}=\sin\theta}{=}-8\int_0^{\pi/2}x^2\cot x\ln(\cos x)\ dx,"['real-analysis', 'integration', 'sequences-and-series', 'binomial-coefficients', 'harmonic-numbers']"
43,$\int_0^\pi\left|\frac{\sin {nx}}{x}\right|dx\ge \frac{2}{\pi}\left(1+\frac{1}{2}+\cdots+\frac{1}{n}\right)$,,\int_0^\pi\left|\frac{\sin {nx}}{x}\right|dx\ge \frac{2}{\pi}\left(1+\frac{1}{2}+\cdots+\frac{1}{n}\right),"Question: Show that $$\int_0^\pi\left|\frac{\sin {nx}}{x}\right|dx\ge \frac{2}{\pi}\left(1+\frac{1}{2}+\cdots+\frac{1}{n}\right).$$ My approach: We know that $$1+\frac{1}{2}+\cdots+\frac{1}{n}=\int_0^1\left(1+x+x^2+\cdots+x^{n-1}\right)dx=\int_0^1\frac{x^n-1}{x-1}dx.$$ Therefore we have $$\int_0^\pi\left|\frac{\sin {nx}}{x}\right|dx\ge \frac{2}{\pi}\left(1+\frac{1}{2}+\cdots+\frac{1}{n}\right) \\ \Leftrightarrow \int_0^\pi\left|\frac{\sin {nx}}{x}\right|dx\ge \frac{2}{\pi}\int_0^1\frac{x^n-1}{x-1}dx.$$ Now substituting $x=\pi t$ , we have $$\frac{2}{\pi}\int_0^1\frac{x^n-1}{x-1}dx=\frac{2}{\pi^{n+1}}\int_0^\pi \frac{t^n-\pi^n}{t-\pi}dt.$$ Thus we have $$\int_0^\pi\left|\frac{\sin {nx}}{x}\right|dx\ge \frac{2}{\pi}\int_0^1\frac{x^n-1}{x-1}dx \\ \Leftrightarrow \int_0^\pi\left|\frac{\sin {nx}}{x}\right|dx\ge \frac{2}{\pi^{n+1}}\int_0^\pi \frac{x^n-\pi^n}{x-\pi}dx\hspace{0.5 cm}...(1)$$ Therefore, if we show that $(1)$ is true, then we are done. Can someone provide me a hint?","Question: Show that My approach: We know that Therefore we have Now substituting , we have Thus we have Therefore, if we show that is true, then we are done. Can someone provide me a hint?",\int_0^\pi\left|\frac{\sin {nx}}{x}\right|dx\ge \frac{2}{\pi}\left(1+\frac{1}{2}+\cdots+\frac{1}{n}\right). 1+\frac{1}{2}+\cdots+\frac{1}{n}=\int_0^1\left(1+x+x^2+\cdots+x^{n-1}\right)dx=\int_0^1\frac{x^n-1}{x-1}dx. \int_0^\pi\left|\frac{\sin {nx}}{x}\right|dx\ge \frac{2}{\pi}\left(1+\frac{1}{2}+\cdots+\frac{1}{n}\right) \\ \Leftrightarrow \int_0^\pi\left|\frac{\sin {nx}}{x}\right|dx\ge \frac{2}{\pi}\int_0^1\frac{x^n-1}{x-1}dx. x=\pi t \frac{2}{\pi}\int_0^1\frac{x^n-1}{x-1}dx=\frac{2}{\pi^{n+1}}\int_0^\pi \frac{t^n-\pi^n}{t-\pi}dt. \int_0^\pi\left|\frac{\sin {nx}}{x}\right|dx\ge \frac{2}{\pi}\int_0^1\frac{x^n-1}{x-1}dx \\ \Leftrightarrow \int_0^\pi\left|\frac{\sin {nx}}{x}\right|dx\ge \frac{2}{\pi^{n+1}}\int_0^\pi \frac{x^n-\pi^n}{x-\pi}dx\hspace{0.5 cm}...(1) (1),"['real-analysis', 'calculus', 'integration', 'definite-integrals', 'integral-inequality']"
44,Can every positive real number $\leqslant\frac{\pi^2}6$ be expressed in this form?,Can every positive real number  be expressed in this form?,\leqslant\frac{\pi^2}6,"For an arbitrary $0\leqslant x \leq\frac{\pi^2}6$ , can we write $x$ in the form $$ x = x_0+\sum_{j\in S\subset\mathbb N\setminus\{0\}} \frac1{j^2}, \tag 1  $$ where $x_0\in\{0,1\}$ . My motivation is this: Let $X_n\stackrel{\mathrm{i.i.d.}}\sim\mathrm{Ber}(p)$ , $Y_n = \frac{X_n}{n^2}$ , and $S_n = \sum_{k=0}^n Y_k$ . Then $S_n$ converges weakly to some random variable $S$ . I would like to know whether $S$ is continuous, i.e. takes all values over $\left[0,\frac{\pi^2}6\right)$ , or if $S$ is discrete (takes values in some countable subset of $\left[0,\frac{\pi^2}6\right)$ ). The former is true if the representation of elements of $\mathbb R$ described in (1) is correct, and the latter is true if not. Note that $\mathbb P(Y\leqslant \frac{\pi^2}6)=1$ because $Y\leqslant\sum_{j=1}^\infty \frac1{j^2}=\frac{\pi^2}6$ a.s. My gut feeling is that $S$ is discrete, as there will be values $\frac jk$ which cannot be obtained by finite sums of elements of $\{\frac1{m^2}:m=1,2,\ldots n\}$ no matter how large $n$ is. But I do not know how to show this rigorously. Advice on how to show this, and hints what the distribution of $S$ looks like would be appreciated.","For an arbitrary , can we write in the form where . My motivation is this: Let , , and . Then converges weakly to some random variable . I would like to know whether is continuous, i.e. takes all values over , or if is discrete (takes values in some countable subset of ). The former is true if the representation of elements of described in (1) is correct, and the latter is true if not. Note that because a.s. My gut feeling is that is discrete, as there will be values which cannot be obtained by finite sums of elements of no matter how large is. But I do not know how to show this rigorously. Advice on how to show this, and hints what the distribution of looks like would be appreciated.","0\leqslant x \leq\frac{\pi^2}6 x 
x = x_0+\sum_{j\in S\subset\mathbb N\setminus\{0\}} \frac1{j^2}, \tag 1 
 x_0\in\{0,1\} X_n\stackrel{\mathrm{i.i.d.}}\sim\mathrm{Ber}(p) Y_n = \frac{X_n}{n^2} S_n = \sum_{k=0}^n Y_k S_n S S \left[0,\frac{\pi^2}6\right) S \left[0,\frac{\pi^2}6\right) \mathbb R \mathbb P(Y\leqslant \frac{\pi^2}6)=1 Y\leqslant\sum_{j=1}^\infty \frac1{j^2}=\frac{\pi^2}6 S \frac jk \{\frac1{m^2}:m=1,2,\ldots n\} n S","['real-analysis', 'probability-theory']"
45,Does the sequence $(x_n)$ given by $x_{n+1} = -16+6x_n+\frac{12}{x_n}$ converge?,Does the sequence  given by  converge?,(x_n) x_{n+1} = -16+6x_n+\frac{12}{x_n},"Question. If $x_0$ is sufficiently close to $2$ , then will the sequence obtained as $$x_{n+1} = -16+6x_n+\frac{12}{x_n}$$ converge to 2 ? My attempt : I have shown that if $x_0$ is close to $2$ , then $x_n > 0$ for all $n \in \mathbb{N}$ . Also , if $x_n > 2 $ for some $n \in \mathbb{N} $ then , $x_n \rightarrow \infty $ as $ n \rightarrow \infty$ . I strongly suspect that this sequence will diverge , but I am not able to proceed further .","Question. If is sufficiently close to , then will the sequence obtained as converge to 2 ? My attempt : I have shown that if is close to , then for all . Also , if for some then , as . I strongly suspect that this sequence will diverge , but I am not able to proceed further .",x_0 2 x_{n+1} = -16+6x_n+\frac{12}{x_n} x_0 2 x_n > 0 n \in \mathbb{N} x_n > 2  n \in \mathbb{N}  x_n \rightarrow \infty   n \rightarrow \infty,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'fixed-point-theorems']"
46,What exactly ARE $\pi$ and $e$? [closed],What exactly ARE  and ? [closed],\pi e,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 5 years ago . Improve this question First of all, apologies if this is a bad question. I don't really know how to phrase it. I first got introduced to $\pi$ in elementary school, where it was presented as a ratio for a circle's area. I thought it was some special number that had to do with circles. You can imagine my confusion when I found it popping up in stuff that seemingly had nothing to do with circles, like in the Wallis formula or the Basel Problem. Same with $e$ . I thought it was this financial growth thing, so I got pretty confused when it showed up in stuff like the Probability Distribution, and Euler's identity. Right now, they seem like two magic numbers with magical properties - same thing with $\cos$ and $\sin$ - magical equations that give you ratios of lengths. Could someone explain what the heck exactly is the significance of $e$ and $\pi$ ? Beyond circles, beyond geometry. Because they certainly seem to be more than just that.","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 5 years ago . Improve this question First of all, apologies if this is a bad question. I don't really know how to phrase it. I first got introduced to in elementary school, where it was presented as a ratio for a circle's area. I thought it was some special number that had to do with circles. You can imagine my confusion when I found it popping up in stuff that seemingly had nothing to do with circles, like in the Wallis formula or the Basel Problem. Same with . I thought it was this financial growth thing, so I got pretty confused when it showed up in stuff like the Probability Distribution, and Euler's identity. Right now, they seem like two magic numbers with magical properties - same thing with and - magical equations that give you ratios of lengths. Could someone explain what the heck exactly is the significance of and ? Beyond circles, beyond geometry. Because they certainly seem to be more than just that.",\pi e \cos \sin e \pi,"['real-analysis', 'complex-analysis', 'number-theory']"
47,Showing that $-\frac{1}{2}=\sum^{\infty}_{n=1}\frac{(-1)^{n}\sin(n)}{n}$,Showing that,-\frac{1}{2}=\sum^{\infty}_{n=1}\frac{(-1)^{n}\sin(n)}{n},"So I've been trying to prove that $$\sum^{\infty}_{n=1}\frac{(-1)^{n}\sin(n)}{n}=-\frac{1}{2}$$ I've tried putting various bounds on it to see if I can ""squeeze"" out the result. Say something like (one of many tried examples): $$ -\frac{1}{n}-\frac{1}{2}\leq \sum^{n}_{k=1}\frac{(-1)^{k}\sin(k)}{k}\leq \frac{1}{n}-\frac{1}{2}$$ I've tried too see if I could find some periodic continuous function in order to use Parseval's theorem, but I can't come up with any that work. May I please get a hint or some piece of the puzzle for this problem?","So I've been trying to prove that I've tried putting various bounds on it to see if I can ""squeeze"" out the result. Say something like (one of many tried examples): I've tried too see if I could find some periodic continuous function in order to use Parseval's theorem, but I can't come up with any that work. May I please get a hint or some piece of the puzzle for this problem?",\sum^{\infty}_{n=1}\frac{(-1)^{n}\sin(n)}{n}=-\frac{1}{2}  -\frac{1}{n}-\frac{1}{2}\leq \sum^{n}_{k=1}\frac{(-1)^{k}\sin(k)}{k}\leq \frac{1}{n}-\frac{1}{2},"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'fourier-series']"
48,"Let $f\in L^1(\mathbb R)$ and let $F,G:\mathbb R\to\mathbb R$ be the functions defined by: ...",Let  and let  be the functions defined by: ...,"f\in L^1(\mathbb R) F,G:\mathbb R\to\mathbb R","I have applied for a Ph.D. in Trieste and am preparing for the exams. I am having a problem with Problem 8 here . Here is the text. Let $f\in L^1(\mathbb R)$ and let $F,G:\mathbb R\to\mathbb R$ be the functions defined by: $$F(x)=\int_x^{x+1}f(t)dt,\qquad\text{and}\qquad G(x)=\left|\int_x^{x+1}f(t)dt\right|.$$ (a) Prove that $G$ has a maximum point on $\mathbb R$ . (b) Give an example of $f\in L^1(\mathbb R)$ such that $F$ has no maximum point on $\mathbb R$ . Now, unless I'm much mistaken (proof at question end), we have: $F,G$ continuous on $\mathbb R$ ; $F,G$ tend to 0 as $|x|\to\infty$ . With that, by 2., both $F$ and $G$ are less than their sup-norms on $\mathbb R$ whenever $|x|>M$ for $M$ big enough, and by 1. and the compactness of $[-M,M]$ they must have a maximum on $[-M,M]$ , which is then a global maximum on $\mathbb R$ . So (a) is done, and (b)… is asking me to disprove the maximum of $F$ which I just proved, so it is impossible! Is my reasoning above correct? Are the proofs below correct? Or is there anything I am missing that is wrong in them? Proofs $F,G\to0$ as $|x|\to\infty$ I write $|x|\to\infty$ to say $x\to\infty$ or $x\to-\infty$ , so let's do $x\to\infty$ , and $x\to-\infty$ is proved the same way, more or less. Now: $$|F(x)|\leq\int_x^{x+1}|f(t)|dt\leq\int_x^{+\infty}|f(t)|dt,$$ which tends to zero for $x\to+\infty$ since $f\in L^1(\mathbb R)$ . And of course $G$ is already handled this way. For $x\to-\infty$ : $$|F(x)|\leq\int_{-\infty}^{x+1}|f(t)|dt.$$ Continuity We rewrite: $$F(x)=\int_{-\infty}^{+\infty}f(t)1_{[x,x+1]}(t)dt,$$ $1_A$ being the indicator of $A$ . Suppose $x\to x_0$ . If $t<x_0$ , then $t<x$ eventually, so that $f(t)1_{[x,x+1]}(t)=0$ eventually, hence $f(t)1_{[x,x+1]}\to0$ . The same occurs if $t>x_0+1$ , whereas of $t\in[x_0,x_0+1]$ then $t\in[x,x+1]$ eventually so that $f(t)1_{[x,x+1]}(t)=f(t)$ eventually. So for $t\neq x_0$ we have $f(t)1_{[x,x+1]}(t)\to f(t)1_{[x_0,x_0+1]}(t)$ . Since this leaves out only two points, $x_0$ and $x_0+1$ , the convergence is pointwise almost everywhere. All of these functions have absolute values that is at most $|f(t)|$ , so by dominated convergence we have: $$F(x)=\int_x^{x+1}f(t)dt=\int_{-\infty}^{+\infty}f(t)1_{[x,x+1]}(t)dt\to\int_{-\infty}^{+\infty}f(t)1_{[x_0,x_0+1]}(t)dt=\int_{x_0}^{x_0+1}f(t)dt=F(x_0),$$ as $x\to x_0$ , proving $F$ is continuous. $G=|F|$ is the composition of $h(x)=|x|$ and $F$ , and since $h,F$ are both continuous we have $G=h\circ F$ is also continuous.","I have applied for a Ph.D. in Trieste and am preparing for the exams. I am having a problem with Problem 8 here . Here is the text. Let and let be the functions defined by: (a) Prove that has a maximum point on . (b) Give an example of such that has no maximum point on . Now, unless I'm much mistaken (proof at question end), we have: continuous on ; tend to 0 as . With that, by 2., both and are less than their sup-norms on whenever for big enough, and by 1. and the compactness of they must have a maximum on , which is then a global maximum on . So (a) is done, and (b)… is asking me to disprove the maximum of which I just proved, so it is impossible! Is my reasoning above correct? Are the proofs below correct? Or is there anything I am missing that is wrong in them? Proofs as I write to say or , so let's do , and is proved the same way, more or less. Now: which tends to zero for since . And of course is already handled this way. For : Continuity We rewrite: being the indicator of . Suppose . If , then eventually, so that eventually, hence . The same occurs if , whereas of then eventually so that eventually. So for we have . Since this leaves out only two points, and , the convergence is pointwise almost everywhere. All of these functions have absolute values that is at most , so by dominated convergence we have: as , proving is continuous. is the composition of and , and since are both continuous we have is also continuous.","f\in L^1(\mathbb R) F,G:\mathbb R\to\mathbb R F(x)=\int_x^{x+1}f(t)dt,\qquad\text{and}\qquad G(x)=\left|\int_x^{x+1}f(t)dt\right|. G \mathbb R f\in L^1(\mathbb R) F \mathbb R F,G \mathbb R F,G |x|\to\infty F G \mathbb R |x|>M M [-M,M] [-M,M] \mathbb R F F,G\to0 |x|\to\infty |x|\to\infty x\to\infty x\to-\infty x\to\infty x\to-\infty |F(x)|\leq\int_x^{x+1}|f(t)|dt\leq\int_x^{+\infty}|f(t)|dt, x\to+\infty f\in L^1(\mathbb R) G x\to-\infty |F(x)|\leq\int_{-\infty}^{x+1}|f(t)|dt. F(x)=\int_{-\infty}^{+\infty}f(t)1_{[x,x+1]}(t)dt, 1_A A x\to x_0 t<x_0 t<x f(t)1_{[x,x+1]}(t)=0 f(t)1_{[x,x+1]}\to0 t>x_0+1 t\in[x_0,x_0+1] t\in[x,x+1] f(t)1_{[x,x+1]}(t)=f(t) t\neq x_0 f(t)1_{[x,x+1]}(t)\to f(t)1_{[x_0,x_0+1]}(t) x_0 x_0+1 |f(t)| F(x)=\int_x^{x+1}f(t)dt=\int_{-\infty}^{+\infty}f(t)1_{[x,x+1]}(t)dt\to\int_{-\infty}^{+\infty}f(t)1_{[x_0,x_0+1]}(t)dt=\int_{x_0}^{x_0+1}f(t)dt=F(x_0), x\to x_0 F G=|F| h(x)=|x| F h,F G=h\circ F",['real-analysis']
49,Proving that a recursive sequence will eventually reach an even number.,Proving that a recursive sequence will eventually reach an even number.,,"Take an arbitrary odd natural number $x$, take the recursive sequence given by $$a_0 = x, a_n= \left \lfloor{ \frac{3 a_{n-1}}{2}} \right \rfloor$$ where $\left \lfloor{ .} \right \rfloor : N \rightarrow N$ is the floor function. I would like to prove that for any $x$ chosen this sequence eventually has an even number in it, I thought this would be doable but I am stuck. How could one prove this?","Take an arbitrary odd natural number $x$, take the recursive sequence given by $$a_0 = x, a_n= \left \lfloor{ \frac{3 a_{n-1}}{2}} \right \rfloor$$ where $\left \lfloor{ .} \right \rfloor : N \rightarrow N$ is the floor function. I would like to prove that for any $x$ chosen this sequence eventually has an even number in it, I thought this would be doable but I am stuck. How could one prove this?",,"['real-analysis', 'sequences-and-series']"
50,"Are the limits $\lim\limits_{n\to \infty }\left|\frac{a_{n+1}}{a_n}\right|\,$ and $\lim\limits_{n\to \infty }\sqrt[n]{|a_n|}\,$ equal?",Are the limits  and  equal?,"\lim\limits_{n\to \infty }\left|\frac{a_{n+1}}{a_n}\right|\, \lim\limits_{n\to \infty }\sqrt[n]{|a_n|}\,","Let consider the power series $$ \sum_{k=1}^\infty a_kx^k. $$ We know that it converges when  $$\lim_{n\to \infty }\sqrt[n]{|a_n|}|x|< 1\quad\text{or}\quad |x|< \frac{1}{\lim_{n\to \infty }\sqrt[n]{|a_n|}}$$ But D'Alembert test tells us that it converges if $$|x|< \frac{1}{\lim_{n\to \infty }\left|\frac{a_{n+1}}{a_n}\right|},$$ so since the radius of convergence is unique, we should have that $$\lim_{n\to \infty }\sqrt[n]{|a_n|}=\lim_{n\to \infty }\left|\frac{a_{n+1}}{a_n}\right|.$$ I tried to prove it, but this result looks strange to me. So if it doesn't work always, how can we have those to limit as radius of convergence? I guess the equality of those two limit should be true most of the time.  So under what conditions is it true?","Let consider the power series $$ \sum_{k=1}^\infty a_kx^k. $$ We know that it converges when  $$\lim_{n\to \infty }\sqrt[n]{|a_n|}|x|< 1\quad\text{or}\quad |x|< \frac{1}{\lim_{n\to \infty }\sqrt[n]{|a_n|}}$$ But D'Alembert test tells us that it converges if $$|x|< \frac{1}{\lim_{n\to \infty }\left|\frac{a_{n+1}}{a_n}\right|},$$ so since the radius of convergence is unique, we should have that $$\lim_{n\to \infty }\sqrt[n]{|a_n|}=\lim_{n\to \infty }\left|\frac{a_{n+1}}{a_n}\right|.$$ I tried to prove it, but this result looks strange to me. So if it doesn't work always, how can we have those to limit as radius of convergence? I guess the equality of those two limit should be true most of the time.  So under what conditions is it true?",,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'power-series']"
51,book recommendation for real analysis,book recommendation for real analysis,,"During the next quarter at uni, I'll be taking a course in real analysis and since I prefer studying with an additional text I thought I'd come here to look for some book recommendations. My background: I'm comfortable with linear algebra and single variable calculus, but shakey on multivariable calculus. I did have a slight introduction in to geometry. For linear algebra I found the common recommendation of Hoffman & Kunze to be very helpful (although the abstractness came as a bit of a kicker at first). For Single variable calculus I used the book Calculus by Adams and Essex, which I found frustrating to work with, because it often left me guessing at what they were trying to do or why they were doing it. I also felt that they exercised a certain lack of rigour. (Which may be due to the subject which would make it hard to derive from axioms.) Details: Though it would be a complementary text, I'd prefer one which will still hold value as a work of reference at a later point in my studies. I've heard that Rudin's Principles of mathematical analysis is one of the better textst out there, but also read several comments discouraging Rudin's book as a first introduction to real analysis, hence my quest to gather more information and maybe get some more personalised recommendations. The course: The mandatory course literature will consist of lecture notes, but 4 texts are suggested as recommended reading. T.M. Apostol, Mathematical analysis. Addison-Wesley (1974) J. Dieudonné, Foundations of Modern Analysis. Academic Press (1960) A. van Rooij, Analyse voor Beginners.Epsilon Uitgaven, no. 6 (2003) R.S. Strichartz, The way of analysis (1995) Thanks in advance!","During the next quarter at uni, I'll be taking a course in real analysis and since I prefer studying with an additional text I thought I'd come here to look for some book recommendations. My background: I'm comfortable with linear algebra and single variable calculus, but shakey on multivariable calculus. I did have a slight introduction in to geometry. For linear algebra I found the common recommendation of Hoffman & Kunze to be very helpful (although the abstractness came as a bit of a kicker at first). For Single variable calculus I used the book Calculus by Adams and Essex, which I found frustrating to work with, because it often left me guessing at what they were trying to do or why they were doing it. I also felt that they exercised a certain lack of rigour. (Which may be due to the subject which would make it hard to derive from axioms.) Details: Though it would be a complementary text, I'd prefer one which will still hold value as a work of reference at a later point in my studies. I've heard that Rudin's Principles of mathematical analysis is one of the better textst out there, but also read several comments discouraging Rudin's book as a first introduction to real analysis, hence my quest to gather more information and maybe get some more personalised recommendations. The course: The mandatory course literature will consist of lecture notes, but 4 texts are suggested as recommended reading. T.M. Apostol, Mathematical analysis. Addison-Wesley (1974) J. Dieudonné, Foundations of Modern Analysis. Academic Press (1960) A. van Rooij, Analyse voor Beginners.Epsilon Uitgaven, no. 6 (2003) R.S. Strichartz, The way of analysis (1995) Thanks in advance!",,"['real-analysis', 'reference-request', 'book-recommendation']"
52,Show that the dual space of $\ell^1$ is isomorphic to $\ell^{\infty}$,Show that the dual space of  is isomorphic to,\ell^1 \ell^{\infty},I've started taking my first course in multivariable analysis and in our notes there is an exercise about the dual space of the sequence space $\ell_1$ and I'm unsure how to prove that it is isomorphic to $\ell_{\infty}$ as in class we haven't gone over the dual space in detail. If somebody could also give me some extra intuition about dual spaces that would be much appreciated :),I've started taking my first course in multivariable analysis and in our notes there is an exercise about the dual space of the sequence space $\ell_1$ and I'm unsure how to prove that it is isomorphic to $\ell_{\infty}$ as in class we haven't gone over the dual space in detail. If somebody could also give me some extra intuition about dual spaces that would be much appreciated :),,"['real-analysis', 'functional-analysis', 'lp-spaces', 'dual-spaces']"
53,Spivak's Calculus?,Spivak's Calculus?,,"I have seen many users here asking questions about problems in what they call ""Spivak's Calculus Book"". I have never seen the book, and information online is scarce. From what I've gathered, it is just a more rigorous calculus 1-3 book with harder problems. I have already taken calculus, and I am about to take analysis. Is there any point for me to buy Spivak's book, or is reading analysis books better at this point? Is this calculus book better than a standard analysis book? Why don't students who are gifted enough just start off with an analysis book instead of Spivak's calculus book?","I have seen many users here asking questions about problems in what they call ""Spivak's Calculus Book"". I have never seen the book, and information online is scarce. From what I've gathered, it is just a more rigorous calculus 1-3 book with harder problems. I have already taken calculus, and I am about to take analysis. Is there any point for me to buy Spivak's book, or is reading analysis books better at this point? Is this calculus book better than a standard analysis book? Why don't students who are gifted enough just start off with an analysis book instead of Spivak's calculus book?",,"['calculus', 'real-analysis', 'reference-request', 'soft-question']"
54,Why do we use the Borel sigma algebra for the codomain of a measurable function?,Why do we use the Borel sigma algebra for the codomain of a measurable function?,,"In several measure theory books, I see that a measurable function $f:\mathbb{R} \rightarrow \mathbb{R}$ often equips the domain with the Lebesgue $\sigma$-algebra and the codomain with the Borel  $\sigma$-algebra. However, if $g$ is another such function, then the composition of $f$ and $g$ may fail to be measurable. Why do we not use the Lebesgue $\sigma$-algebra for both the domain and the codomain?","In several measure theory books, I see that a measurable function $f:\mathbb{R} \rightarrow \mathbb{R}$ often equips the domain with the Lebesgue $\sigma$-algebra and the codomain with the Borel  $\sigma$-algebra. However, if $g$ is another such function, then the composition of $f$ and $g$ may fail to be measurable. Why do we not use the Lebesgue $\sigma$-algebra for both the domain and the codomain?",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
55,$f \in L^p \cap L^q$ implies $f \in L^r$ for $p\leq r \leq q$.,implies  for .,f \in L^p \cap L^q f \in L^r p\leq r \leq q,"Let $(X, \mathfrak{M}, \mu)$ is a measure space. Let $f:X \rightarrow \mathbb{C}$ be a measurable function. Prove that the set $\{1 \le p \le \infty \, | \, f \in L^p(X)\}$ is connected. In other words prove that if $f \in L^p(X) \cap L^q(X)$ with$ 1\le p < q \le \infty$ and if $p\le r \le q$, then $f \in L^r(X)$. I've tried to prove this using Holder's Inequality, here's my attempt: Since $r \in [p,q]$, let $\lambda$ be such that $(1-\lambda)p+\lambda q = r.$ We use the fact that $f \in L^r(X)$ iff $|f|^r \in L^1(X)$. Let $m = \frac{1}{1-\lambda}$ and $n=\frac{1}{\lambda}$ so that $m$ and $n$ are conjugates. Then, \begin{eqnarray*} \int_{X} |f|^r \, d\mu &=& \int_{X} |f|^{(1-\lambda)p+\lambda q} \, d\mu \\ &=& \int_X |f|^{(1-\lambda)p}|f|^{\lambda q} d\mu \\ &\stackrel{\text{H$\ddot{o}$lder}}{\leq}& \left(\int_X |f|^p \, d\mu\right)^{(1-\lambda)}\left(\int_X |f|^q \, d\mu \right)^\lambda \, \\ &<& \infty \end{eqnarray*} Since surely the RHS is finite since $f \in L^p(X) \cap L^q(X)$. Is this correct? Is this the intended method of proof?","Let $(X, \mathfrak{M}, \mu)$ is a measure space. Let $f:X \rightarrow \mathbb{C}$ be a measurable function. Prove that the set $\{1 \le p \le \infty \, | \, f \in L^p(X)\}$ is connected. In other words prove that if $f \in L^p(X) \cap L^q(X)$ with$ 1\le p < q \le \infty$ and if $p\le r \le q$, then $f \in L^r(X)$. I've tried to prove this using Holder's Inequality, here's my attempt: Since $r \in [p,q]$, let $\lambda$ be such that $(1-\lambda)p+\lambda q = r.$ We use the fact that $f \in L^r(X)$ iff $|f|^r \in L^1(X)$. Let $m = \frac{1}{1-\lambda}$ and $n=\frac{1}{\lambda}$ so that $m$ and $n$ are conjugates. Then, \begin{eqnarray*} \int_{X} |f|^r \, d\mu &=& \int_{X} |f|^{(1-\lambda)p+\lambda q} \, d\mu \\ &=& \int_X |f|^{(1-\lambda)p}|f|^{\lambda q} d\mu \\ &\stackrel{\text{H$\ddot{o}$lder}}{\leq}& \left(\int_X |f|^p \, d\mu\right)^{(1-\lambda)}\left(\int_X |f|^q \, d\mu \right)^\lambda \, \\ &<& \infty \end{eqnarray*} Since surely the RHS is finite since $f \in L^p(X) \cap L^q(X)$. Is this correct? Is this the intended method of proof?",,"['real-analysis', 'integration', 'analysis', 'proof-verification', 'lp-spaces']"
56,"Let $C \subseteq [0,1]$ be uncountable, show there exists $a \in (0,1)$ such that $C \cap [a,1] $ is uncountable","Let  be uncountable, show there exists  such that  is uncountable","C \subseteq [0,1] a \in (0,1) C \cap [a,1] ","Let $C \subseteq [0,1]$ be uncountable, show there exists $a \in (0,1)$ such that $C \cap [a,1] $ is uncountable From what I know so far if something is countable then it has the same cardinality as $\mathbb{N}$, so to show there exists an $a \in (0,1)$ that works then I need to show that $\mid\,\mathbb{N}\mid \, <  \,\mid C \cap [a,1] \,\mid$ If I assume $C$ is uncountable but $\neg\exists a \in (0,1)$ s.t. $C \cap [a,1]$ is uncountable. Then there is a surjection from $\mathbb{N}$ to $C \cap [a,1]$ and an injection from $C \cap [a,1]$ to $\mathbb{N}, \forall a \in (0,1)$ The union of countable sets are countable, so $\bigcup_{a=1}^{\infty} \bigg( C \cap [a,1] \bigg) $ is countable thus surjective from $\mathbb{N}$ and injective to $\mathbb{N}$. The intuition makes perfect sense but I'm getting hung up on how I can get to my contradiction.","Let $C \subseteq [0,1]$ be uncountable, show there exists $a \in (0,1)$ such that $C \cap [a,1] $ is uncountable From what I know so far if something is countable then it has the same cardinality as $\mathbb{N}$, so to show there exists an $a \in (0,1)$ that works then I need to show that $\mid\,\mathbb{N}\mid \, <  \,\mid C \cap [a,1] \,\mid$ If I assume $C$ is uncountable but $\neg\exists a \in (0,1)$ s.t. $C \cap [a,1]$ is uncountable. Then there is a surjection from $\mathbb{N}$ to $C \cap [a,1]$ and an injection from $C \cap [a,1]$ to $\mathbb{N}, \forall a \in (0,1)$ The union of countable sets are countable, so $\bigcup_{a=1}^{\infty} \bigg( C \cap [a,1] \bigg) $ is countable thus surjective from $\mathbb{N}$ and injective to $\mathbb{N}$. The intuition makes perfect sense but I'm getting hung up on how I can get to my contradiction.",,['real-analysis']
57,"Computing $\lim_{A\to\infty} \frac{1}{A} \int\limits_1^A \! A^{\frac{1}{x}} \, \mathrm{d}x.$",Computing,"\lim_{A\to\infty} \frac{1}{A} \int\limits_1^A \! A^{\frac{1}{x}} \, \mathrm{d}x.","On this year's IMC there was this problem: Compute $$ \lim_{A\to\infty} \frac{1}{A} \int\limits_1^A \! A^{\frac{1}{x}} \, \mathrm{d}x. $$ In addition to the two official solutions, I am curious as to if there exist other solutions. I heard that someone wrote that this is actually a probability density function (just for this one line, that person got 6 out of 10 points), so it would be great to see an answer involving this claim. Also, I thought of turning this integral (using substitutions) to something I can evaluate using the Gamma function. Could someone hint a substitution which could lead to that? EDIT: The official solutions can be found here , Problem 7.","On this year's IMC there was this problem: Compute $$ \lim_{A\to\infty} \frac{1}{A} \int\limits_1^A \! A^{\frac{1}{x}} \, \mathrm{d}x. $$ In addition to the two official solutions, I am curious as to if there exist other solutions. I heard that someone wrote that this is actually a probability density function (just for this one line, that person got 6 out of 10 points), so it would be great to see an answer involving this claim. Also, I thought of turning this integral (using substitutions) to something I can evaluate using the Gamma function. Could someone hint a substitution which could lead to that? EDIT: The official solutions can be found here , Problem 7.",,"['real-analysis', 'analysis', 'contest-math']"
58,Munkres' Analysis on Manifolds and Differential Geometry,Munkres' Analysis on Manifolds and Differential Geometry,,Will Munkres' Analysis on Manifolds prepare me for a text like John Lee's Introduction to Topological Manifolds and his Introduction to Smooth Manifolds text? Would one be able to successfully tackle Spivak's Differential Geometry series after Munkres'?,Will Munkres' Analysis on Manifolds prepare me for a text like John Lee's Introduction to Topological Manifolds and his Introduction to Smooth Manifolds text? Would one be able to successfully tackle Spivak's Differential Geometry series after Munkres'?,,"['real-analysis', 'soft-question', 'manifolds']"
59,Showing Lipschitz continuity for a particular distance function,Showing Lipschitz continuity for a particular distance function,,"My friend and I have been working on trying to prove this inequality for a while. However, I think there is some trick we are just not seeing. Suppose $F$ is a closed set in $\mathbb{R}$ , whose complement has finite measure, and let $$ \delta(x)=d(x,F)=\inf \{ |x - y| : y \in F \}.$$ Prove $$ \mid \delta(x) - \delta(y)| \le |x-y|.$$","My friend and I have been working on trying to prove this inequality for a while. However, I think there is some trick we are just not seeing. Suppose is a closed set in , whose complement has finite measure, and let Prove","F \mathbb{R}  \delta(x)=d(x,F)=\inf \{ |x - y| : y \in F \}.  \mid \delta(x) - \delta(y)| \le |x-y|.","['real-analysis', 'metric-spaces', 'lipschitz-functions']"
60,Distance between a point and a closed set in metric space,Distance between a point and a closed set in metric space,,"Here is what I am thinking. Let $(X,d)$ be a metric space and let $C$ be a closed subset of $X$. Fix any point $p$ in $X$. Then, there exists a point $q$ in $C$ such that $$d(p,q) = \mathrm {distance}(p,C)$$. I think this statement is true, so I tried to the following proof. For any natural number $n$, let $a_n$ be a point in $C$ such that $$d(p,a_n) < \mathrm{distance}(p,C)+{1\over n}$$ But,then I am lost as to what to do next. I want to use the fact that any convergent sequence in $C$ converges to a point in $C$, but I am not sure how to proceed. Or, is what I am trying to prove even true??","Here is what I am thinking. Let $(X,d)$ be a metric space and let $C$ be a closed subset of $X$. Fix any point $p$ in $X$. Then, there exists a point $q$ in $C$ such that $$d(p,q) = \mathrm {distance}(p,C)$$. I think this statement is true, so I tried to the following proof. For any natural number $n$, let $a_n$ be a point in $C$ such that $$d(p,a_n) < \mathrm{distance}(p,C)+{1\over n}$$ But,then I am lost as to what to do next. I want to use the fact that any convergent sequence in $C$ converges to a point in $C$, but I am not sure how to proceed. Or, is what I am trying to prove even true??",,"['real-analysis', 'general-topology', 'analysis', 'metric-spaces']"
61,"Let $f:(\mathbb{R}\setminus\mathbb{Q})\cap [0,1]\to \mathbb{Q}\cap [0,1]$. Prove there exists a continuous$f$.",Let . Prove there exists a continuous.,"f:(\mathbb{R}\setminus\mathbb{Q})\cap [0,1]\to \mathbb{Q}\cap [0,1] f","I'm working on the following problem from N.L. Carother's Real Analysis: Let $I=(\mathbb{R}\setminus\mathbb{Q})\cap [0,1]$ with its usual metric. Prove that there is a continuous function $g$ mapping $I$ onto $\mathbb{Q}\cap[0,1]$. My thoughts: I feel the preimage of open sets definition of continuity will be the easiest way to prove this. If I could show $V\subset \mathbb{Q}\cap [0,1]$ is open for all open sets $V$, and I could show that $f^{-1}(V)$ is open as well, then that would mean $f$ is continuous. I've considered trying to prove that $(\mathbb{Q}\cap [0,1])^c$ is closed, but that doesn't seem much easier. I know $\mathbb{Q}$ is dense in $\mathbb{R}$, and so maybe I can use that to say that $B_{\epsilon}(x)\setminus\{x\}\cap(\mathbb{Q}\cap[0,1])\neq\emptyset$, which would mean every $x\in\mathbb{Q}\cap[0,1]$ is a limit point of $\mathbb{Q}\cap[0,1]$, but I still don't see how this could be helpful. Any hints on how to proceed would be appreciated. Thanks.","I'm working on the following problem from N.L. Carother's Real Analysis: Let $I=(\mathbb{R}\setminus\mathbb{Q})\cap [0,1]$ with its usual metric. Prove that there is a continuous function $g$ mapping $I$ onto $\mathbb{Q}\cap[0,1]$. My thoughts: I feel the preimage of open sets definition of continuity will be the easiest way to prove this. If I could show $V\subset \mathbb{Q}\cap [0,1]$ is open for all open sets $V$, and I could show that $f^{-1}(V)$ is open as well, then that would mean $f$ is continuous. I've considered trying to prove that $(\mathbb{Q}\cap [0,1])^c$ is closed, but that doesn't seem much easier. I know $\mathbb{Q}$ is dense in $\mathbb{R}$, and so maybe I can use that to say that $B_{\epsilon}(x)\setminus\{x\}\cap(\mathbb{Q}\cap[0,1])\neq\emptyset$, which would mean every $x\in\mathbb{Q}\cap[0,1]$ is a limit point of $\mathbb{Q}\cap[0,1]$, but I still don't see how this could be helpful. Any hints on how to proceed would be appreciated. Thanks.",,"['real-analysis', 'continuity']"
62,Proving $fg$ and $f+g$ is Riemann integrable through the easy and hard way.,Proving  and  is Riemann integrable through the easy and hard way.,fg f+g,"Problem : Suppose $f,g$ are Riemann integrable functions, show that $f+g$ and $fg$ are also Riemann integrable. I know there is really easy to do this with measure theory, but I want to see if this method works as well. I will write an answer for $f+g$ using measures. Write up 1 : Denote $D_f, D_g$ to be the set of all discontinuities of $f$ and $g$. $f+g$ can only be integrable if and only if $D_f \cap D_g$ has measure zero, but without any loss of generality, we also have $D_f \cap D_g \subset D_f$, the set on the left has measure zero. So $f+g$ is Riemann integrable. Okay this one is the longer one. Write up 2 : As $f$ and $g$ are integrable, there are partitions $P_f$ and $P_g$ such that $$U(f,P_f) - L(f,P_f) < \epsilon/2,$$   $$U(g,P_g) - L(g,P_g) < \epsilon/2.$$   Now we begin the estimation starting with $$L(f,P)L(g,P) \leq L(fg,P),$$ and $$U(f,P)U(g,P) \geq U(fg, P).$$   Therefore if we let $P \supset P_f \cup P_g$ then we get, \begin{align} U(fg,P) - L(fg,P) &\leq U(f,P)U(g,P) - L(f,P)U(g,P) + L(f,P)U(g,P) - L(f,P)L(g,P) \\ &=U(g,P)[U(f,P) - L(f,P)] + L(f,P)[U(g,P) - L(g,P)]\\ &\leq \epsilon/2[U(g,P) + L(f,P)]\\ &\leq \epsilon/2[ U(g,P)  +  \sup \{L(f,P) \}] \end{align} I am stuck with the last step, I am not sure how to bound $U(g,P)$. May I get some pointers? My futile idea is that I can do the following bound $U(g,P) < \epsilon/2 + L(g,P) < \epsilon/2 + \sup \{L(g,P) \}$ note : I know for $fg$, there is another short proof with $4fg = (f+g)^2 - (f - g)^2.$ I am not seeking that one either.","Problem : Suppose $f,g$ are Riemann integrable functions, show that $f+g$ and $fg$ are also Riemann integrable. I know there is really easy to do this with measure theory, but I want to see if this method works as well. I will write an answer for $f+g$ using measures. Write up 1 : Denote $D_f, D_g$ to be the set of all discontinuities of $f$ and $g$. $f+g$ can only be integrable if and only if $D_f \cap D_g$ has measure zero, but without any loss of generality, we also have $D_f \cap D_g \subset D_f$, the set on the left has measure zero. So $f+g$ is Riemann integrable. Okay this one is the longer one. Write up 2 : As $f$ and $g$ are integrable, there are partitions $P_f$ and $P_g$ such that $$U(f,P_f) - L(f,P_f) < \epsilon/2,$$   $$U(g,P_g) - L(g,P_g) < \epsilon/2.$$   Now we begin the estimation starting with $$L(f,P)L(g,P) \leq L(fg,P),$$ and $$U(f,P)U(g,P) \geq U(fg, P).$$   Therefore if we let $P \supset P_f \cup P_g$ then we get, \begin{align} U(fg,P) - L(fg,P) &\leq U(f,P)U(g,P) - L(f,P)U(g,P) + L(f,P)U(g,P) - L(f,P)L(g,P) \\ &=U(g,P)[U(f,P) - L(f,P)] + L(f,P)[U(g,P) - L(g,P)]\\ &\leq \epsilon/2[U(g,P) + L(f,P)]\\ &\leq \epsilon/2[ U(g,P)  +  \sup \{L(f,P) \}] \end{align} I am stuck with the last step, I am not sure how to bound $U(g,P)$. May I get some pointers? My futile idea is that I can do the following bound $U(g,P) < \epsilon/2 + L(g,P) < \epsilon/2 + \sup \{L(g,P) \}$ note : I know for $fg$, there is another short proof with $4fg = (f+g)^2 - (f - g)^2.$ I am not seeking that one either.",,"['real-analysis', 'proof-verification', 'alternative-proof']"
63,"If a function is positive on a set of measure greater than zero, is the Lebesgue integral of that function greater than zero?","If a function is positive on a set of measure greater than zero, is the Lebesgue integral of that function greater than zero?",,"Suppose we have a set $A \subset \mathbb{R}^n$ such that $f(x) > 0$ for $x \in A$ and $m(A) > 0$. Does it follow that $\int_A f > 0$? Obviously if there is some kind of lower bound on $f(x)$ on some non-trivial subset of $A$, then we're done, but is it possible for there not to be any such lower bound? I'm thinking like $f(a)$ is some number and then the value of $f$ decreases very rapidly everywhere on $A$.","Suppose we have a set $A \subset \mathbb{R}^n$ such that $f(x) > 0$ for $x \in A$ and $m(A) > 0$. Does it follow that $\int_A f > 0$? Obviously if there is some kind of lower bound on $f(x)$ on some non-trivial subset of $A$, then we're done, but is it possible for there not to be any such lower bound? I'm thinking like $f(a)$ is some number and then the value of $f$ decreases very rapidly everywhere on $A$.",,"['real-analysis', 'measure-theory']"
64,Prove there is a unique continuous function satisfying this integral equation,Prove there is a unique continuous function satisfying this integral equation,,"This is a question from an old real analysis qual: Prove that there is a unique continuous function $f:[0,1] \to \mathbb{R}$ such that $$f(x) = \cos x + \int_0^x f(y)e^{-y}dy$$ for $x \in [0,1]$ I haven't seen any problems like this before and I'm not really sure where to start.","This is a question from an old real analysis qual: Prove that there is a unique continuous function $f:[0,1] \to \mathbb{R}$ such that $$f(x) = \cos x + \int_0^x f(y)e^{-y}dy$$ for $x \in [0,1]$ I haven't seen any problems like this before and I'm not really sure where to start.",,"['real-analysis', 'integral-equations']"
65,Number of real roots of $\sum_{k=0}^{n}\frac{x^{k}}{k!}=0$,Number of real roots of,\sum_{k=0}^{n}\frac{x^{k}}{k!}=0,"Prove the following, without induction . Is it possible? The equation $\sum_{k=0}^{n}\frac{x^{k}}{k!}=0$ has no real root if $n$ is even. And if $n$ is odd, it has only one real root. I also tried searching the proofs many times with search key words ""number of real root"" or ""exponential function"" or ""$\sum_{k=0}^{n}\frac{x^{k}}{k!}$"". But failed. What is the way to search the related topics?","Prove the following, without induction . Is it possible? The equation $\sum_{k=0}^{n}\frac{x^{k}}{k!}=0$ has no real root if $n$ is even. And if $n$ is odd, it has only one real root. I also tried searching the proofs many times with search key words ""number of real root"" or ""exponential function"" or ""$\sum_{k=0}^{n}\frac{x^{k}}{k!}$"". But failed. What is the way to search the related topics?",,['real-analysis']
66,$\{x_n\}$ be a bounded sequence of real number we need to show,be a bounded sequence of real number we need to show,\{x_n\},"Let $\{x_n\}$ be a bounded sequence of real numbers. We need to show that there exist a  real number $\alpha$ and positive integers $n_1,n_2,\dots$ such that $n_1<n_2<\dots$ and $\sum_{k}|x_{n_k}-\alpha|<\infty$, please hint!","Let $\{x_n\}$ be a bounded sequence of real numbers. We need to show that there exist a  real number $\alpha$ and positive integers $n_1,n_2,\dots$ such that $n_1<n_2<\dots$ and $\sum_{k}|x_{n_k}-\alpha|<\infty$, please hint!",,"['real-analysis', 'sequences-and-series']"
67,"Limits are additive, but suprema and infima aren't?","Limits are additive, but suprema and infima aren't?",,"I find it curious that limits have the additive property, but suprema are only subadditive and infima are only superadditive. Since extrema are limits, why is this so? I was thinking if, for uniformly bounded functions all defined over the same set of finite measure, the supremum over such functions is additive, not just subadditive. Is this true? Are there more general conditions for functions where suprema and infima are additive over?","I find it curious that limits have the additive property, but suprema are only subadditive and infima are only superadditive. Since extrema are limits, why is this so? I was thinking if, for uniformly bounded functions all defined over the same set of finite measure, the supremum over such functions is additive, not just subadditive. Is this true? Are there more general conditions for functions where suprema and infima are additive over?",,"['real-analysis', 'sequences-and-series', 'limits']"
68,Real analysis question $e^{-1/x^2}$,Real analysis question,e^{-1/x^2},"Let $f$ be defined on $\mathbb{R}$ by $f(x) = e^{-1/x^2}$ for $x$ not equal to $0$. and $f(0)= 0$. Prove that $f^{(n)}(0)=0$ for all $n = 1, 2,3$ ... Do I need to use Taylor expansion from calculus class? Any hint would be appreciated.","Let $f$ be defined on $\mathbb{R}$ by $f(x) = e^{-1/x^2}$ for $x$ not equal to $0$. and $f(0)= 0$. Prove that $f^{(n)}(0)=0$ for all $n = 1, 2,3$ ... Do I need to use Taylor expansion from calculus class? Any hint would be appreciated.",,['real-analysis']
69,Meaning of functions that vanish at a point,Meaning of functions that vanish at a point,,"I realize this may be a very thick question, but I have been wondering for some time. Sometimes I am asked to prove or read proofs involving ""functions that vanish at a point"" or ""every point"" or something along these lines. The problem is I do not know what it means for a function to vanish at a point. It sounds like it means the function goes to 0 as a sequence of points arrives at the point, but if the function is continuous, doesn't that just mean the function is equivalently 0 at the point? I am basically confused what ""vanish"" means.","I realize this may be a very thick question, but I have been wondering for some time. Sometimes I am asked to prove or read proofs involving ""functions that vanish at a point"" or ""every point"" or something along these lines. The problem is I do not know what it means for a function to vanish at a point. It sounds like it means the function goes to 0 as a sequence of points arrives at the point, but if the function is continuous, doesn't that just mean the function is equivalently 0 at the point? I am basically confused what ""vanish"" means.",,"['real-analysis', 'functions', 'convergence-divergence']"
70,Could someone remind me why is incorrect to switch an infinite sum and an integral?,Could someone remind me why is incorrect to switch an infinite sum and an integral?,,"Could someone jog my memory on this? The order of operation between an $\int$ and $\sum_{n\in \mathbb{N}}$ is not always interchangable? Note that the sum is an INFINITE sum Why is it that $\int \sum_{n \in \mathbb{N}} \neq \sum_{n \in \mathbb{N}} \int$ Is the reason because the integral itself is a sum and the order of ""summing"" actually matters? (I think it's Multivariable calculus related stuff now)","Could someone jog my memory on this? The order of operation between an $\int$ and $\sum_{n\in \mathbb{N}}$ is not always interchangable? Note that the sum is an INFINITE sum Why is it that $\int \sum_{n \in \mathbb{N}} \neq \sum_{n \in \mathbb{N}} \int$ Is the reason because the integral itself is a sum and the order of ""summing"" actually matters? (I think it's Multivariable calculus related stuff now)",,"['calculus', 'real-analysis', 'measure-theory']"
71,A function is continuous if its graph is closed,A function is continuous if its graph is closed,,"Any help with the following problem: Prove that if a function $f: \mathbb{R}\rightarrow \mathbb{R}$ is bounded and its graph is a closed subset of $\mathbb{R}^{2}$, then $f$ is continuous.","Any help with the following problem: Prove that if a function $f: \mathbb{R}\rightarrow \mathbb{R}$ is bounded and its graph is a closed subset of $\mathbb{R}^{2}$, then $f$ is continuous.",,"['real-analysis', 'general-topology', 'analysis', 'continuity']"
72,Prove $\liminf_{n\to\infty} \left(n^2(4a_n(1-a_{n-1})-1)\right)\leq\frac14$ for any non-negative seqence $\{a_n\}$.,Prove  for any non-negative seqence .,\liminf_{n\to\infty} \left(n^2(4a_n(1-a_{n-1})-1)\right)\leq\frac14 \{a_n\},"Let $\{a_n\}_{n=1}^\infty$ be a sequence of non-negative numbers. Prove that $$\liminf_{n\to\infty} \left(n^2\left(4a_n(1-a_{n-1})-1\right)\right)\leq\frac14.$$ I found this problem on one of my old notebooks, and clearly I forgot where it came from. I searched using Approach0 and it led me to a post on AOPS without solutions. I have to admit that I don't know how to start. First of all, it is easy to notice that if we have a subsequence $\{a_{n_k}\}$ with $a_{n_k}\geq 1$ for all $k$ , then we must have $\liminf_{n\to\infty} \left(n^2(4a_n(1-a_{n-1})-1)\right)\leq0\leq \frac14$ ; also, the case where there are infinitely many $0$ 's is easy to handle. So, WLOG we can assume that $a_n\in(0,1)$ for all $n$ . Secondly, notice that $4x(1-x)-1=-(2x-1)^2\leq 0$ , with equality holds iff $x=\frac12$ . It may suggest that $\frac12$ is an important number here. Now it seems natural to write $a_n=\frac12+b_n$ , so we have $b_n\in(-1/2,1/2)$ and we want to prove that $$\liminf_{n\to\infty}\left(n^2(b_n-b_{n-1}-2b_nb_{n-1})\right)\leq\frac18.$$ But this is not obvious, either. Remark. The bound $1/4$ (or $1/8$ in terms of $b_n$ ) is sharp. If we choose $b_n=-\frac1{4n}$ , or equivalently $a_n=\frac12-\frac1{4n}$ for $n\geq1$ , then $$n^2(b_n-b_{n-1}-2b_nb_{n-1})=\frac{n^2}{8n(n-1)}\to\frac18,\qquad n\to\infty.$$ The above are simplest (kind of silly) observations.  Any help would be appreciated!","Let be a sequence of non-negative numbers. Prove that I found this problem on one of my old notebooks, and clearly I forgot where it came from. I searched using Approach0 and it led me to a post on AOPS without solutions. I have to admit that I don't know how to start. First of all, it is easy to notice that if we have a subsequence with for all , then we must have ; also, the case where there are infinitely many 's is easy to handle. So, WLOG we can assume that for all . Secondly, notice that , with equality holds iff . It may suggest that is an important number here. Now it seems natural to write , so we have and we want to prove that But this is not obvious, either. Remark. The bound (or in terms of ) is sharp. If we choose , or equivalently for , then The above are simplest (kind of silly) observations.  Any help would be appreciated!","\{a_n\}_{n=1}^\infty \liminf_{n\to\infty} \left(n^2\left(4a_n(1-a_{n-1})-1\right)\right)\leq\frac14. \{a_{n_k}\} a_{n_k}\geq 1 k \liminf_{n\to\infty} \left(n^2(4a_n(1-a_{n-1})-1)\right)\leq0\leq \frac14 0 a_n\in(0,1) n 4x(1-x)-1=-(2x-1)^2\leq 0 x=\frac12 \frac12 a_n=\frac12+b_n b_n\in(-1/2,1/2) \liminf_{n\to\infty}\left(n^2(b_n-b_{n-1}-2b_nb_{n-1})\right)\leq\frac18. 1/4 1/8 b_n b_n=-\frac1{4n} a_n=\frac12-\frac1{4n} n\geq1 n^2(b_n-b_{n-1}-2b_nb_{n-1})=\frac{n^2}{8n(n-1)}\to\frac18,\qquad n\to\infty.","['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'limsup-and-liminf']"
73,Fibonacci sum: $\sum\limits_{k\ge0}\frac{F_{2k+1}}{2k+1}\left(\frac{2+2\sqrt{2}}{1+\sqrt{\frac{17+8\sqrt{2}}{5}}}\right)^{2k+1}(-\frac{1}{5})^k$,Fibonacci sum:,\sum\limits_{k\ge0}\frac{F_{2k+1}}{2k+1}\left(\frac{2+2\sqrt{2}}{1+\sqrt{\frac{17+8\sqrt{2}}{5}}}\right)^{2k+1}(-\frac{1}{5})^k,"Prove $$\frac{3\pi}{8}=\sum\limits_{k\ge0}\left(-\frac{1}{5}\right)^k\frac{F_{2k+1}}{2k+1}q^{2k+1}$$ where $q=\frac{2+2\sqrt{2}}{1+\sqrt{\frac{17+8\sqrt{2}}{5}}}$ , and $F_n$ are the Fibonacci numbers. This result is from Wolfram . My ideas as to a proof are limited. I know that this is some sort of series for the inverse tangent because $3\pi/8=\arctan(1+\sqrt{2})$ , but I have never seen any series for $\arctan$ involving the Fibonacci numbers. Essentially, it comes to the explicit evaluation of the function $$f(x)=\sum_{k\ge0}\frac{F_{2k+1}}{2k+1}x^{k}.$$ Indeed, the sum in question is the value $qf(-q^2/5)$ . Potentially connected is the closed form $$\sum_{k\ge0}F_kx^k=\frac{x}{1-x-x^2},$$ perhaps we could get some sort of $\arctan$ -related integral out of this. Could I have some help evaluating $f$ ? Thanks.","Prove where , and are the Fibonacci numbers. This result is from Wolfram . My ideas as to a proof are limited. I know that this is some sort of series for the inverse tangent because , but I have never seen any series for involving the Fibonacci numbers. Essentially, it comes to the explicit evaluation of the function Indeed, the sum in question is the value . Potentially connected is the closed form perhaps we could get some sort of -related integral out of this. Could I have some help evaluating ? Thanks.","\frac{3\pi}{8}=\sum\limits_{k\ge0}\left(-\frac{1}{5}\right)^k\frac{F_{2k+1}}{2k+1}q^{2k+1} q=\frac{2+2\sqrt{2}}{1+\sqrt{\frac{17+8\sqrt{2}}{5}}} F_n 3\pi/8=\arctan(1+\sqrt{2}) \arctan f(x)=\sum_{k\ge0}\frac{F_{2k+1}}{2k+1}x^{k}. qf(-q^2/5) \sum_{k\ge0}F_kx^k=\frac{x}{1-x-x^2}, \arctan f","['real-analysis', 'sequences-and-series', 'closed-form', 'fibonacci-numbers']"
74,Show that $(\ln x)^n$ are linearly independent over polynomial.,Show that  are linearly independent over polynomial.,(\ln x)^n,"Question : If $f_n(x)(\ln x)^n + f_{n-1}(x)(\ln x)^{n-1} + ... +f_0(x) = 0$ where $ f_i (x) $ are polynomial with real coefficients , then show that all $f_i (x)$ are identically zero. I already know that if we replace $ \ln x $ as $ e^x$ , then this is true. Let $f_n(x)e^{nx}+ f_{n-1}(x)e^{(n-1)x} + ... +f_0(x) = 0$ . By divide $e^{nx}$ , $$f_n(x)+ \frac{f_{n-1}(x)}{e^{x}}+ ... +\frac{f_0(x)}{e^{nx}}= 0$$ . Take limit infinity, $\lim f_n(x)=0$ and this mean $f_n(x)$ is identically zero. By induction, we're done. However, $(\ln x)$ is not accepted in this process because $\frac{P(x)}{\ln x}$ does not approach to zero as $ n \rightarrow \infty$ where $P(x)$ is polynomial. How to prove this?","Question : If where are polynomial with real coefficients , then show that all are identically zero. I already know that if we replace as , then this is true. Let . By divide , . Take limit infinity, and this mean is identically zero. By induction, we're done. However, is not accepted in this process because does not approach to zero as where is polynomial. How to prove this?",f_n(x)(\ln x)^n + f_{n-1}(x)(\ln x)^{n-1} + ... +f_0(x) = 0  f_i (x)  f_i (x)  \ln x   e^x f_n(x)e^{nx}+ f_{n-1}(x)e^{(n-1)x} + ... +f_0(x) = 0 e^{nx} f_n(x)+ \frac{f_{n-1}(x)}{e^{x}}+ ... +\frac{f_0(x)}{e^{nx}}= 0 \lim f_n(x)=0 f_n(x) (\ln x) \frac{P(x)}{\ln x}  n \rightarrow \infty P(x),"['real-analysis', 'calculus']"
75,"$f:[0,1]\to[0,1]$ be a continuous function. Let $x_1\in[0,1]$ and define $x_{n+1}={\sum_{i=1}^n f(x_i)\over n}$.Prove, $\{x_n\}$ is convergent","be a continuous function. Let  and define .Prove,  is convergent","f:[0,1]\to[0,1] x_1\in[0,1] x_{n+1}={\sum_{i=1}^n f(x_i)\over n} \{x_n\}","I have tried a little bit which as follows- Since $f(x_n)\in[0,1]$ , $\{f(x_n)\}$ has a convergent subsequence say $y_n=f(x_{r_n})\ \forall n\in\Bbb{N}$ Let, $\lim y_n=l\implies \lim \frac{y_1+y_2+\cdots+y_n}{n}=l\implies \lim \frac{f(x_{r_1})+f(x_{r_2})+\cdots+f(x_{r_n})}{n}=l$ But I am getting no idea to proceed and prove the convergence of $\{x_n\}$ . I have also tried to prove the sequence to be cauchy which goes- $x_{m+1}-x_{n+1}={\sum_{i=1}^m f(x_i)\over m}-{\sum_{i=1}^n f(x_i)\over n}\le {\sum_{i=1}^m f(x_i)\over n}-{\sum_{i=1}^n f(x_i)\over n}$ (since $m\ge n)$ $\implies |x_{m+1}-x_{n+1}|\le \frac{|f(x_m)|+|f(x_{m-1})+\cdots+|f(x_{n+1})|}{n}\le\frac{m-n}{n}$ (since $f([0,1])\subseteq [0,1])$ Now, what will I get if I tend $m,n\to\infty$ ? But I need to do something more in the 2nd case since I have nowhere used the continuity of $f$ . Can anybody give an idea to prove it? Thanks for assistance in advance.","I have tried a little bit which as follows- Since , has a convergent subsequence say Let, But I am getting no idea to proceed and prove the convergence of . I have also tried to prove the sequence to be cauchy which goes- (since (since Now, what will I get if I tend ? But I need to do something more in the 2nd case since I have nowhere used the continuity of . Can anybody give an idea to prove it? Thanks for assistance in advance.","f(x_n)\in[0,1] \{f(x_n)\} y_n=f(x_{r_n})\ \forall n\in\Bbb{N} \lim y_n=l\implies \lim \frac{y_1+y_2+\cdots+y_n}{n}=l\implies \lim \frac{f(x_{r_1})+f(x_{r_2})+\cdots+f(x_{r_n})}{n}=l \{x_n\} x_{m+1}-x_{n+1}={\sum_{i=1}^m f(x_i)\over m}-{\sum_{i=1}^n f(x_i)\over n}\le {\sum_{i=1}^m f(x_i)\over n}-{\sum_{i=1}^n f(x_i)\over n} m\ge n) \implies |x_{m+1}-x_{n+1}|\le \frac{|f(x_m)|+|f(x_{m-1})+\cdots+|f(x_{n+1})|}{n}\le\frac{m-n}{n} f([0,1])\subseteq [0,1]) m,n\to\infty f","['real-analysis', 'sequences-and-series', 'convergence-divergence', 'continuity']"
76,"Proving that $\frac{f(b)-f(a)}{b-a}+ \left(\frac{g(b)-g(a)}{b-a}\right)^2\le \max_{t\in [a,b]}\{f'(t)+(g'(t))^2\}$",Proving that,"\frac{f(b)-f(a)}{b-a}+ \left(\frac{g(b)-g(a)}{b-a}\right)^2\le \max_{t\in [a,b]}\{f'(t)+(g'(t))^2\}","Let $f,g\in C^1([a,b])$ with $a<b$ then prove that $$\frac{f(b)-f(a)}{b-a}+ \left(\frac{g(b)-g(a)}{b-a}\right)^2\le \max_{t\in [a,b]}\{f'(t)+(g'(t))^2\}$$ It smells  like there is some mean value theorem going around. But I tried it as follows: Indeed, it springs from mean value theorem that There exists  $c_1,c_2\in (a,b)$ such that $$\frac{f(b)-f(a)}{b-a} = f'(c_1)~~~ and  ~~~~\frac{g(b)-g(a)}{b-a} = g'(c_2)$$ Then I have  $$\frac{f(b)-f(a)}{b-a}+ \left(\frac{g(b)-g(a)}{b-a}\right)^2= f'(c_1)+(g'(c_2))^2\le \max_{t\in [a,b]}\{f'(t)\}+\max_{t\in [a,b]}\{(g'(t)^2)\}$$ Which is however not the required inequality. Can anyone help? how can I improve this ?","Let $f,g\in C^1([a,b])$ with $a<b$ then prove that $$\frac{f(b)-f(a)}{b-a}+ \left(\frac{g(b)-g(a)}{b-a}\right)^2\le \max_{t\in [a,b]}\{f'(t)+(g'(t))^2\}$$ It smells  like there is some mean value theorem going around. But I tried it as follows: Indeed, it springs from mean value theorem that There exists  $c_1,c_2\in (a,b)$ such that $$\frac{f(b)-f(a)}{b-a} = f'(c_1)~~~ and  ~~~~\frac{g(b)-g(a)}{b-a} = g'(c_2)$$ Then I have  $$\frac{f(b)-f(a)}{b-a}+ \left(\frac{g(b)-g(a)}{b-a}\right)^2= f'(c_1)+(g'(c_2))^2\le \max_{t\in [a,b]}\{f'(t)\}+\max_{t\in [a,b]}\{(g'(t)^2)\}$$ Which is however not the required inequality. Can anyone help? how can I improve this ?",,"['calculus', 'real-analysis', 'analysis', 'functions', 'inequality']"
77,Proof every convex function is continuous (Problem 10 Convex Functions Spivak),Proof every convex function is continuous (Problem 10 Convex Functions Spivak),,"I am working on exercise 10 of the appendix between chapters 11 and 12 of Spivak's Calculus. The problem is to show that a convex function must be continuous.  I would like to check my proof as it is different from the ones I have found so far. Let $f$ be a function that is convex in $ (a,b)$.  Let us assume that $f$ is not continuous in the point $a$. By the definition of convexity, we have:  $ \dfrac{f(x)-f(a)}{x-a}<\dfrac{f(b)-f(a)}{b-a}$ To remove the inequality let $ \dfrac{f(x)-f(a)}{x-a} +h(x) = \dfrac{f(b)-f(a)}{b-a} ~(1)$, where $h(b)=0$ and $h(a)=\dfrac{f(b)-f(a)}{b-a}$ Now let's rearrange equation 1: $ f(x)-f(a) = (x-a)\left(\dfrac{f(b)-f(a)}{b-a} - h(x)\right)$ Taking $lim_{x\to a^+}$ on both sides: $ lim_{x\to a^+} (f(x)-f(a)) = lim_{x\to a^+} (x-a)\left(\dfrac{f(b)-f(a)}{b-a} - h(x)\right)= 0$ So  $~lim_{x\to a^+} f(x)= f(a)$ Thus $f(x)$ is right continuous on $a$ I can use a similar argument to prove that $f(x)$ is left continuous on $b$ Since $f$ is convex in $(a,b)$, it is also convex in $(a+h,b)$ with $h< b-a$. And by letting $h \to b-a$, I prove right continuity over the whole interval. Similarly, as $f$ in convex in $(a,b)$, it is also convex in $(a,b-k)$ with $k > b-a$. And by letting $k \to b-a$, the whole interval is left continuous. As any $x_0 \in (a,b)$ can be uniquely expressed as $x_0= a+h = b-k$ and $f$ is right continuous for $a+h$ and left continuous for $b-k$ then $f$ is continuous in $x_0 \in (a,b)$ While writing the question, I have cleaned the logic from what I had initially drafted, so I am more confident about it. Still I am not sure if this logic is correct, as it is longer than any other answer I have found.","I am working on exercise 10 of the appendix between chapters 11 and 12 of Spivak's Calculus. The problem is to show that a convex function must be continuous.  I would like to check my proof as it is different from the ones I have found so far. Let $f$ be a function that is convex in $ (a,b)$.  Let us assume that $f$ is not continuous in the point $a$. By the definition of convexity, we have:  $ \dfrac{f(x)-f(a)}{x-a}<\dfrac{f(b)-f(a)}{b-a}$ To remove the inequality let $ \dfrac{f(x)-f(a)}{x-a} +h(x) = \dfrac{f(b)-f(a)}{b-a} ~(1)$, where $h(b)=0$ and $h(a)=\dfrac{f(b)-f(a)}{b-a}$ Now let's rearrange equation 1: $ f(x)-f(a) = (x-a)\left(\dfrac{f(b)-f(a)}{b-a} - h(x)\right)$ Taking $lim_{x\to a^+}$ on both sides: $ lim_{x\to a^+} (f(x)-f(a)) = lim_{x\to a^+} (x-a)\left(\dfrac{f(b)-f(a)}{b-a} - h(x)\right)= 0$ So  $~lim_{x\to a^+} f(x)= f(a)$ Thus $f(x)$ is right continuous on $a$ I can use a similar argument to prove that $f(x)$ is left continuous on $b$ Since $f$ is convex in $(a,b)$, it is also convex in $(a+h,b)$ with $h< b-a$. And by letting $h \to b-a$, I prove right continuity over the whole interval. Similarly, as $f$ in convex in $(a,b)$, it is also convex in $(a,b-k)$ with $k > b-a$. And by letting $k \to b-a$, the whole interval is left continuous. As any $x_0 \in (a,b)$ can be uniquely expressed as $x_0= a+h = b-k$ and $f$ is right continuous for $a+h$ and left continuous for $b-k$ then $f$ is continuous in $x_0 \in (a,b)$ While writing the question, I have cleaned the logic from what I had initially drafted, so I am more confident about it. Still I am not sure if this logic is correct, as it is longer than any other answer I have found.",,"['real-analysis', 'proof-verification', 'convex-analysis']"
78,If $\sum\limits_{n=1}^\infty a_n $converges does it imply that $\sum\limits_{n=1}^\infty \dfrac {a_n^{1/4}}{n^{4/5}}$ is convergent?,If converges does it imply that  is convergent?,\sum\limits_{n=1}^\infty a_n  \sum\limits_{n=1}^\infty \dfrac {a_n^{1/4}}{n^{4/5}},"Let $\sum_{n=1}^\infty a_n$ be a convergent series of positive terms, then is it true that $$\sum_{n=1}^\infty \dfrac {a_n^{1/4}}{n^{4/5}}$$ is convergent ? I tried comparing with $\sum a_n$ , but without any progress. Please help with any hint.","Let $\sum_{n=1}^\infty a_n$ be a convergent series of positive terms, then is it true that $$\sum_{n=1}^\infty \dfrac {a_n^{1/4}}{n^{4/5}}$$ is convergent ? I tried comparing with $\sum a_n$ , but without any progress. Please help with any hint.",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
79,"Radon-Nikodym Theorem for (positive) measures, chain rule","Radon-Nikodym Theorem for (positive) measures, chain rule",,"Let us take some time to recall the Radon-Nikodym Theorem for (positive) measures: If $\nu$ and $\mu$ are $\sigma$-finite measures on $(X, \mathcal{M})$ then $\nu = \nu_a + \nu_s$ with $\nu_a \ll \mu$ and $\nu_s \perp \mu$, and the decomposition is unique. Moreover, $d\nu_a = f\,d\mu$ for a nonnegative measurable function $f$ (note: $d\nu_a = f\,d\mu$ means that for every measurable $E$, $\int_E d\nu_a = \int_E f\,d\mu$). If $\nu \ll \mu$ then $\nu_s = 0$ and the function $f$ is called the Radon-Nikodym derivative and is denoted ${{d\nu}\over{d\mu}}$. Say that $\mu$, $\nu$ and $\sigma$ are finite measures on $(X, \mathcal{M})$ and suppose that $\mu \ll\nu$ and $\nu \ll \lambda$. How do I see that $\mu \ll \lambda$ and$${{d\mu}\over{d\lambda}} = {{d\mu}\over{d\nu}}{{d\nu}\over{d\lambda}}$$for $\lambda$ almost every point?","Let us take some time to recall the Radon-Nikodym Theorem for (positive) measures: If $\nu$ and $\mu$ are $\sigma$-finite measures on $(X, \mathcal{M})$ then $\nu = \nu_a + \nu_s$ with $\nu_a \ll \mu$ and $\nu_s \perp \mu$, and the decomposition is unique. Moreover, $d\nu_a = f\,d\mu$ for a nonnegative measurable function $f$ (note: $d\nu_a = f\,d\mu$ means that for every measurable $E$, $\int_E d\nu_a = \int_E f\,d\mu$). If $\nu \ll \mu$ then $\nu_s = 0$ and the function $f$ is called the Radon-Nikodym derivative and is denoted ${{d\nu}\over{d\mu}}$. Say that $\mu$, $\nu$ and $\sigma$ are finite measures on $(X, \mathcal{M})$ and suppose that $\mu \ll\nu$ and $\nu \ll \lambda$. How do I see that $\mu \ll \lambda$ and$${{d\mu}\over{d\lambda}} = {{d\mu}\over{d\nu}}{{d\nu}\over{d\lambda}}$$for $\lambda$ almost every point?",,"['real-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
80,Let $p(x)$ be a real $7$ degree polynomial with $p(\pi)=\sqrt 3$ and $\int_{-\pi}^{\pi}x^k p(x)=0$ for $0\le k\le 6$. Find $p(0)$ and $p(-\pi)$.,Let  be a real  degree polynomial with  and  for . Find  and .,p(x) 7 p(\pi)=\sqrt 3 \int_{-\pi}^{\pi}x^k p(x)=0 0\le k\le 6 p(0) p(-\pi),"Q.Let $p(x)$ be a real $7$ degree polynomial with $p(\pi)=\sqrt 3$ and $\int_{-\pi}^{\pi}x^k p(x)=0$ for $0\le k\le 6$. Find $p(0)$ and $p(-\pi)$. Let $p(x)=a_0 + a_1 x+ a_2 x^2+ a_3 x^3+...+a_7 x^7.$ Then we can approach this problem by evaluating the definite integral given in question for each $k$, therefore giving us $7$ equations in $8$ variables $a_0, a_1,...,a_7$. We get eighth equation by using given condition $p(\pi)=\sqrt 3$. This way we have $8$ equations with $8$ unknowns. We form following system of equations, $$     \left(\begin{matrix}     1 & 0 & \frac {{\pi}^2}3 & 0 & \frac {{\pi}^4}5 & 0 & \frac {{\pi}^6}7 & 0 \\     0 & \frac 13 & 0 & \frac {{\pi}^2}5 & 0 & \frac {{\pi}^4}7 & 0 & \frac {{\pi}^6}9 \\     \frac 13 & 0 & \frac {{\pi}^2}5 & 0 & \frac {{\pi}^4}7 & 0 & \frac {{\pi}^6}9 & 0 \\     0 & \frac 15 & 0 & \frac {{\pi}^2}7 & 0 & \frac {{\pi}^4}9 & 0 & \frac {{\pi}^6}{11} \\     \frac 15 & 0 & \frac {{\pi}^2}7 & 0 & \frac {{\pi}^4}9 & 0 & \frac {{\pi}^6}{11} & 0 \\     0 & \frac 17 & 0 & \frac {{\pi}^2}9 & 0 & \frac {{\pi}^4}{11} & 0 & \frac {{\pi}^6}{13} \\     \frac 17 & 0 & \frac {{\pi}^2}9 & 0 & \frac {{\pi}^4}{11} & 0 & \frac {{\pi}^6}{13} & 0 \\     1 & \pi & \pi^2 & \pi^3 & \pi^4 & \pi^5 & \pi^6 & \pi^7      \end{matrix}\right) \cdot      \left(\begin{matrix}     a_0  \\     a_1  \\     a_2  \\     a_3 \\     a_4 \\     a_5 \\     a_6 \\     a_7  \\     \end{matrix}\right)=     \left(\begin{matrix}     0  \\     0 \\     0  \\     0 \\     0   \\     0 \\     0 \\     \sqrt 3 \\     \end{matrix}\right)$$ If we solve this system of equations, then we get the values of $a_i$s. This way we have identified the polynomial $p(x)$. Once we have identified the polynomial it is easy to calculate $p(0)$ and $p(-\pi)$. Note that $p(0)=a_0$. But this approach is very much lengthy. Is there a faster way?","Q.Let $p(x)$ be a real $7$ degree polynomial with $p(\pi)=\sqrt 3$ and $\int_{-\pi}^{\pi}x^k p(x)=0$ for $0\le k\le 6$. Find $p(0)$ and $p(-\pi)$. Let $p(x)=a_0 + a_1 x+ a_2 x^2+ a_3 x^3+...+a_7 x^7.$ Then we can approach this problem by evaluating the definite integral given in question for each $k$, therefore giving us $7$ equations in $8$ variables $a_0, a_1,...,a_7$. We get eighth equation by using given condition $p(\pi)=\sqrt 3$. This way we have $8$ equations with $8$ unknowns. We form following system of equations, $$     \left(\begin{matrix}     1 & 0 & \frac {{\pi}^2}3 & 0 & \frac {{\pi}^4}5 & 0 & \frac {{\pi}^6}7 & 0 \\     0 & \frac 13 & 0 & \frac {{\pi}^2}5 & 0 & \frac {{\pi}^4}7 & 0 & \frac {{\pi}^6}9 \\     \frac 13 & 0 & \frac {{\pi}^2}5 & 0 & \frac {{\pi}^4}7 & 0 & \frac {{\pi}^6}9 & 0 \\     0 & \frac 15 & 0 & \frac {{\pi}^2}7 & 0 & \frac {{\pi}^4}9 & 0 & \frac {{\pi}^6}{11} \\     \frac 15 & 0 & \frac {{\pi}^2}7 & 0 & \frac {{\pi}^4}9 & 0 & \frac {{\pi}^6}{11} & 0 \\     0 & \frac 17 & 0 & \frac {{\pi}^2}9 & 0 & \frac {{\pi}^4}{11} & 0 & \frac {{\pi}^6}{13} \\     \frac 17 & 0 & \frac {{\pi}^2}9 & 0 & \frac {{\pi}^4}{11} & 0 & \frac {{\pi}^6}{13} & 0 \\     1 & \pi & \pi^2 & \pi^3 & \pi^4 & \pi^5 & \pi^6 & \pi^7      \end{matrix}\right) \cdot      \left(\begin{matrix}     a_0  \\     a_1  \\     a_2  \\     a_3 \\     a_4 \\     a_5 \\     a_6 \\     a_7  \\     \end{matrix}\right)=     \left(\begin{matrix}     0  \\     0 \\     0  \\     0 \\     0   \\     0 \\     0 \\     \sqrt 3 \\     \end{matrix}\right)$$ If we solve this system of equations, then we get the values of $a_i$s. This way we have identified the polynomial $p(x)$. Once we have identified the polynomial it is easy to calculate $p(0)$ and $p(-\pi)$. Note that $p(0)=a_0$. But this approach is very much lengthy. Is there a faster way?",,"['real-analysis', 'linear-algebra', 'contest-math', 'alternative-proof']"
81,Specific Integral Question: $\int_0^1(f'(t))^2dt \geq 3(\int_0^1f(t)dt)^2$,Specific Integral Question:,\int_0^1(f'(t))^2dt \geq 3(\int_0^1f(t)dt)^2,"I'm in the midst of studying for my real analysis final tomorrow and I came across this question in a practice final and am stumped on how to even start it. Let $f: [0,1] \rightarrow \mathbb{R}$ be a differentiable function such that $f'$ is continuous and $f(1)=0$. Prove that the following inequality holds: $\int_{0}^{1}(f'(t))^2dt \geq 3(\int_{0}^{1}f(t)dt)^2$. Any guidance on how to start would be of help..","I'm in the midst of studying for my real analysis final tomorrow and I came across this question in a practice final and am stumped on how to even start it. Let $f: [0,1] \rightarrow \mathbb{R}$ be a differentiable function such that $f'$ is continuous and $f(1)=0$. Prove that the following inequality holds: $\int_{0}^{1}(f'(t))^2dt \geq 3(\int_{0}^{1}f(t)dt)^2$. Any guidance on how to start would be of help..",,"['real-analysis', 'integration']"
82,Example of continuous mapping of open(closed) set to not open(closed) set,Example of continuous mapping of open(closed) set to not open(closed) set,,"I want to find a continuous function: $f:\textbf{R}^n \rightarrow \textbf{R}^m$ s.t. for some open subset $A$, $f(A)$ is not open, and for some closed $B$, $f(B)$ is not closed. I am able to find some mappings that satisfy one condition, e.g. $f(x)=exp(-x)$ maps closed $[0,\infty)$ to not closed $(0,1]$, but cannot find an example which satisfies both conditions.","I want to find a continuous function: $f:\textbf{R}^n \rightarrow \textbf{R}^m$ s.t. for some open subset $A$, $f(A)$ is not open, and for some closed $B$, $f(B)$ is not closed. I am able to find some mappings that satisfy one condition, e.g. $f(x)=exp(-x)$ maps closed $[0,\infty)$ to not closed $(0,1]$, but cannot find an example which satisfies both conditions.",,"['real-analysis', 'general-topology', 'metric-spaces']"
83,Example of dynamical system where: $NW(f) \not\subset \overline{R(f)}$,Example of dynamical system where:,NW(f) \not\subset \overline{R(f)},"Can you provide me an example of a Dynamical System continuous or discrete in which: $$NW(f)  \not\subset \overline{R(f)}$$ $NW(f)$ is the set of non wandering points, i.e. all $x$ such that $\forall$ U open containing $x$ and $\forall$ $N>0$ there exists some $n>N$ such that $f^n(U) \cap U \ne \emptyset$. $R(f)$ is the set of all recurrent points.  A point $x$ is recurrent if it belongs to its own limit set $\omega(x)$ i.e. $\forall$ U neighborhood of $x$, $\exists$ $ n>0$ such that $f^n(x) \in U$. The relation between the two sets is: $\overline{R(f)} \subseteq NW(f)$.","Can you provide me an example of a Dynamical System continuous or discrete in which: $$NW(f)  \not\subset \overline{R(f)}$$ $NW(f)$ is the set of non wandering points, i.e. all $x$ such that $\forall$ U open containing $x$ and $\forall$ $N>0$ there exists some $n>N$ such that $f^n(U) \cap U \ne \emptyset$. $R(f)$ is the set of all recurrent points.  A point $x$ is recurrent if it belongs to its own limit set $\omega(x)$ i.e. $\forall$ U neighborhood of $x$, $\exists$ $ n>0$ such that $f^n(x) \in U$. The relation between the two sets is: $\overline{R(f)} \subseteq NW(f)$.",,"['real-analysis', 'dynamical-systems', 'examples-counterexamples']"
84,Understanding power series and their representation of functions,Understanding power series and their representation of functions,,"Below is my current level of understanding about Power Series (my understanding could be completely wrong, in which case please correct me), and I want to know if it is correct. I feel that Power Series is something that it treated very poorly in most (introductory) textbooks. It seems as though authors keep on dodging the central ideas of Power Series, and their relation to functions, for seemingly unknown reasons. My Understanding of Power Series Let's say we have a power series, call it $p(x)$ (as power  series are functions themselves) and it converges to a finite set of values, $S$ over an interval of convergence $R$ then it can be used to represent $f(x)$ within that interval of convergence $R$ $$\underbrace{f(x)}_\text{Some analytic function} = \underbrace{p(x)}_\text{A power series representation} \ \ \underbrace{\forall\  |x| < R}_\text{within the power series' radius of convergence}$$ An analytic function is equal to its power series representation within the power series' radius of convergence An Example: The Geometric Series Take the famous geometric series (note the LHS is $f(x)$ and the RHS is $p(x))$ $$\frac{1}{1-x} = 1 + x + x^2 + x^3 + \ ...$$ it has a radius of convergence of $|x| <1$, now what this means is that only for $x \in (-1, 1)$, can it actually be used as a representation of $f(x) = \frac{1}{1-x}$. Outside of this interval ( of convergence ), equality is broken and we can't really use it anymore as a representation of $f$, therefore if we let $f(x) = \frac{1}{1-x}$ and $p(x) = 1+ x + x^2 + x^3 + \ ...$, the the following two statements are true: $$f(x) = p(x) \ \ \ \forall \ |x| <1$$ $$f(x) \neq p(x) \ \ \ \forall\ |x| >1$$ This is the reason why we talk about convergence of Power Series, and why we need Power Series to converge because if it didn’t converge, our power series representation $p(x)$, would never equal $f(x)$ and we could never use it as a way to evaluate $f(x)$. Another example: $e^x$ In the case of $e^x$, the Power Series representation of it can actually be used as one of the definitions of it, because the Power Series representation of $e^x$ is valid for all $x \in \mathbb{C}$ $$e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \frac{x^4}{4!}  \ ... \ \ \forall x \in \mathbb{C}$$ Why are Power Series important? Take the example of $e^x$ I've given above. So if we have a power series representation , $p(x)$ of some function $f$, then we can use $p(x)$ to define $f(x)$ for all $x$ inside the radius of convergence of the power series! Power Series/Taylor Series and Polynomial Approximations (The Big Picture) A polynomial approximation (a Power Series with finite terms) of any analytic function approaches the actual function as the number of terms in the polynomial approximation (the Power Series with finite terms, or the partial sums of a Power Series) tends to infinity, at which point it is equivalent to the analytic function. This is the reason why transcendental functions, like $e^x$, $\sin(x)$ etc. are transcendental, because they need a power series to represent/define them. They can't be defined by a finite sequence of terms. Thus the only way to define transcendental functions is via Power Series. Furthermore Power Series, provide us with a deep way to express non-polynomial functions, such as trigonometric functions, as polynomial functions (via a Power Series). It's one of the neat shortcuts that an infinite amount of terms provides us with, the ability to represent non-polynomial functions as polynomial functions. Questions: Is my understanding correct? Is there anything that you can add to what I've written above that would make Power Series clearer to those learning about them? Furthermore are their higher levels of understandings of Power Series from Complex Analysis, Real Analysis, etc?","Below is my current level of understanding about Power Series (my understanding could be completely wrong, in which case please correct me), and I want to know if it is correct. I feel that Power Series is something that it treated very poorly in most (introductory) textbooks. It seems as though authors keep on dodging the central ideas of Power Series, and their relation to functions, for seemingly unknown reasons. My Understanding of Power Series Let's say we have a power series, call it $p(x)$ (as power  series are functions themselves) and it converges to a finite set of values, $S$ over an interval of convergence $R$ then it can be used to represent $f(x)$ within that interval of convergence $R$ $$\underbrace{f(x)}_\text{Some analytic function} = \underbrace{p(x)}_\text{A power series representation} \ \ \underbrace{\forall\  |x| < R}_\text{within the power series' radius of convergence}$$ An analytic function is equal to its power series representation within the power series' radius of convergence An Example: The Geometric Series Take the famous geometric series (note the LHS is $f(x)$ and the RHS is $p(x))$ $$\frac{1}{1-x} = 1 + x + x^2 + x^3 + \ ...$$ it has a radius of convergence of $|x| <1$, now what this means is that only for $x \in (-1, 1)$, can it actually be used as a representation of $f(x) = \frac{1}{1-x}$. Outside of this interval ( of convergence ), equality is broken and we can't really use it anymore as a representation of $f$, therefore if we let $f(x) = \frac{1}{1-x}$ and $p(x) = 1+ x + x^2 + x^3 + \ ...$, the the following two statements are true: $$f(x) = p(x) \ \ \ \forall \ |x| <1$$ $$f(x) \neq p(x) \ \ \ \forall\ |x| >1$$ This is the reason why we talk about convergence of Power Series, and why we need Power Series to converge because if it didn’t converge, our power series representation $p(x)$, would never equal $f(x)$ and we could never use it as a way to evaluate $f(x)$. Another example: $e^x$ In the case of $e^x$, the Power Series representation of it can actually be used as one of the definitions of it, because the Power Series representation of $e^x$ is valid for all $x \in \mathbb{C}$ $$e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \frac{x^4}{4!}  \ ... \ \ \forall x \in \mathbb{C}$$ Why are Power Series important? Take the example of $e^x$ I've given above. So if we have a power series representation , $p(x)$ of some function $f$, then we can use $p(x)$ to define $f(x)$ for all $x$ inside the radius of convergence of the power series! Power Series/Taylor Series and Polynomial Approximations (The Big Picture) A polynomial approximation (a Power Series with finite terms) of any analytic function approaches the actual function as the number of terms in the polynomial approximation (the Power Series with finite terms, or the partial sums of a Power Series) tends to infinity, at which point it is equivalent to the analytic function. This is the reason why transcendental functions, like $e^x$, $\sin(x)$ etc. are transcendental, because they need a power series to represent/define them. They can't be defined by a finite sequence of terms. Thus the only way to define transcendental functions is via Power Series. Furthermore Power Series, provide us with a deep way to express non-polynomial functions, such as trigonometric functions, as polynomial functions (via a Power Series). It's one of the neat shortcuts that an infinite amount of terms provides us with, the ability to represent non-polynomial functions as polynomial functions. Questions: Is my understanding correct? Is there anything that you can add to what I've written above that would make Power Series clearer to those learning about them? Furthermore are their higher levels of understandings of Power Series from Complex Analysis, Real Analysis, etc?",,"['calculus', 'real-analysis', 'sequences-and-series', 'complex-analysis', 'power-series']"
85,Limit of $\sqrt{\frac{\pi}{1-x}}-\sum\limits_{k=1}^\infty\frac{x^k}{\sqrt{k}}$ when $x\to 1^-$?,Limit of  when ?,\sqrt{\frac{\pi}{1-x}}-\sum\limits_{k=1}^\infty\frac{x^k}{\sqrt{k}} x\to 1^-,"I am trying to understand if $$\sqrt{\frac{2\pi}{1-x}}-\sum\limits_{k=1}^\infty\frac{x^k}{\sqrt{k}}$$ is convergent for $x\to 1^-$. Any help? Update: Given the insightful comments below, it is clear it is not converging, hence the actual question now is to find $$ \lim_{x\to 1^-}\left(\sqrt{\frac{\pi}{1-x}}-\sum_{n\geq 1}\frac{x^n}{\sqrt{n}}\right)$$","I am trying to understand if $$\sqrt{\frac{2\pi}{1-x}}-\sum\limits_{k=1}^\infty\frac{x^k}{\sqrt{k}}$$ is convergent for $x\to 1^-$. Any help? Update: Given the insightful comments below, it is clear it is not converging, hence the actual question now is to find $$ \lim_{x\to 1^-}\left(\sqrt{\frac{\pi}{1-x}}-\sum_{n\geq 1}\frac{x^n}{\sqrt{n}}\right)$$",,"['real-analysis', 'sequences-and-series', 'asymptotics', 'riemann-zeta']"
86,$f$ continuous iff $f(K)$ is compact for all compact $K$.,continuous iff  is compact for all compact .,f f(K) K,"In topology, I have seen that $f$ is continuous if and only if $f^{-1}(U)$ is open when $U$ is open. But does it hold that $f$ continuous iff $f(K)$ is compact when $K$ is compact ? I know that if $f$ is continuous, then $f(K)$ is compact when $K$ is compact. But does the converse hold ? I'm talking especially for a function $f:\mathbb R\longrightarrow \mathbb R$, but a general answer would be good too.","In topology, I have seen that $f$ is continuous if and only if $f^{-1}(U)$ is open when $U$ is open. But does it hold that $f$ continuous iff $f(K)$ is compact when $K$ is compact ? I know that if $f$ is continuous, then $f(K)$ is compact when $K$ is compact. But does the converse hold ? I'm talking especially for a function $f:\mathbb R\longrightarrow \mathbb R$, but a general answer would be good too.",,"['real-analysis', 'general-topology']"
87,"Prove that if $\sum a_n$ converges, then $na_n \to 0$. [duplicate]","Prove that if  converges, then . [duplicate]",\sum a_n na_n \to 0,"This question already has answers here : If $(a_n)$ is a decreasing sequence of strictly positive numbers and if $\sum{a_n}$ is convergent, show that $\lim{na_n}=0$ [duplicate] (3 answers) Closed 8 years ago . Let $a_n$ be a decreasing sequence of nonnegative real numbers. Prove that if $\sum a_n$ converges, then $na_n \to 0$ . Hint: use that $n\, a_{2n} \le a_{n+1}+\cdots + a_{2n}$ I couldn't prove this using the given hint, could someone give me a few tips? I also have two more questions: Suppose I have $2na_{2n},(2n+1)a_{2n+1}\to 0$ is that enough to say that $na_n\to 0$ ? Is there any easy way to show $n \, a_{2n}\le a_{n+1}+\cdots + a_{2n}$ , there's probably a simple inductive proof I couldn't get.","This question already has answers here : If $(a_n)$ is a decreasing sequence of strictly positive numbers and if $\sum{a_n}$ is convergent, show that $\lim{na_n}=0$ [duplicate] (3 answers) Closed 8 years ago . Let be a decreasing sequence of nonnegative real numbers. Prove that if converges, then . Hint: use that I couldn't prove this using the given hint, could someone give me a few tips? I also have two more questions: Suppose I have is that enough to say that ? Is there any easy way to show , there's probably a simple inductive proof I couldn't get.","a_n \sum a_n na_n \to 0 n\, a_{2n} \le a_{n+1}+\cdots + a_{2n} 2na_{2n},(2n+1)a_{2n+1}\to 0 na_n\to 0 n \, a_{2n}\le a_{n+1}+\cdots + a_{2n}","['real-analysis', 'sequences-and-series']"
88,Characteristic function and moment generating function: differentiating under the integral,Characteristic function and moment generating function: differentiating under the integral,,"In order to justify the interchange of the derivative and integral when differentiating a characteristic function, one can use the dominated convergence theorem: $$\frac{d}{dt} \int e^{itx} P(dx) = \lim_{h \to 0} \frac{1}{h} \int (e^{ihx}-1) e^{itx} P(dx).$$ Since $|e^{ihx}-1| \le |hx|$, we have $$\frac{1}{h} \int |e^{ihx}-1| P(dx) \le \int |x| P(dx),$$   so if we assume the random variable is in $L^1$, we may push the derivative under the integral. Similarly, if the random variable is in $L^k$, then we can push the $k$th derivative under the integral. I am trying to find an analogous statement for moment generating functions, but I am having trouble generalizing the above argument. Under what conditions can we do this for MGFs? Any hints would be appreciated, but I would prefer an argument that uses dominated convergence rather than Leibniz's integral rule.","In order to justify the interchange of the derivative and integral when differentiating a characteristic function, one can use the dominated convergence theorem: $$\frac{d}{dt} \int e^{itx} P(dx) = \lim_{h \to 0} \frac{1}{h} \int (e^{ihx}-1) e^{itx} P(dx).$$ Since $|e^{ihx}-1| \le |hx|$, we have $$\frac{1}{h} \int |e^{ihx}-1| P(dx) \le \int |x| P(dx),$$   so if we assume the random variable is in $L^1$, we may push the derivative under the integral. Similarly, if the random variable is in $L^k$, then we can push the $k$th derivative under the integral. I am trying to find an analogous statement for moment generating functions, but I am having trouble generalizing the above argument. Under what conditions can we do this for MGFs? Any hints would be appreciated, but I would prefer an argument that uses dominated convergence rather than Leibniz's integral rule.",,"['real-analysis', 'probability-theory', 'characteristic-functions', 'moment-generating-functions']"
89,Is a single point a closed interval?,Is a single point a closed interval?,,"For example, is $\{0\}$ considered a closed interval? Why or why not? Doesn't it contain all (it's only) limit point of $0$ ?","For example, is considered a closed interval? Why or why not? Doesn't it contain all (it's only) limit point of ?",\{0\} 0,"['real-analysis', 'terminology']"
90,Exact result of a series using Euler-Maclaurin expansion.,Exact result of a series using Euler-Maclaurin expansion.,,"This is a variant of Exercise 64 in Chapter 9 of concrete mathematics. Prove the following identity \begin{equation} \sum_{n = -\infty}^{\infty}' \frac{1 - \cos( 2\pi n k )}{n^2 }  = 2 \pi^2 ( k - k^2 ) \qquad k \in [0,1] \end{equation} I came across it in a different context and was surprised that it is exact. Let's use Euler-Maclaurin expansion to convert the sum into an integral. Take $f(x) = \frac{1 - \cos( 2\pi x k )}{x^2 } $ \begin{equation} \sum_{n = -N+1}^{N}' f(n) = \int_{-N}^{N} f(x) dx - f(0) + \sum_{k=1}^{p} \frac{B_k}{k!}f^{(k-1)}(x)\Big|^N_{-N} + R_p  \end{equation} The first two terms are already the exact result,  \begin{equation} \int_{-\infty}^{\infty} f(x) dx = 2\pi k \int_{-\infty}^{\infty} \frac{1- \cos x}{x^2}dx = 2\pi^2 k \qquad f(0) = 2\pi^2 k^2  \end{equation} so one needs to prove the end point corrections(terms with $B_k$ in it) as well as the reminder terms are zero. At any order $p$, there are only finite number of end point correction terms which goes at most like $\mathcal{O}(\frac{1}{N^2})$, so they vanish when taking the $N\rightarrow \infty$ limit. But I don't know how to show the reminder term is zero(as $N\rightarrow \infty$), \begin{equation} R_p(N)  = (-1)^p \int_{-N}^{N} \frac{1}{p!} B_p( x - \lfloor x \rfloor ) f^{(p)}(x) dx  \end{equation} Maybe there are other methods to prove the identity, but I personally would appreciate the proof using Euler Maclaurin, since I'm going to use it for another series, and the reminder term has \begin{equation} f(x) = \frac{g(x) - g(0) }{ x^2} - \frac{g'(0)}{x} \end{equation} where $g(x ) = g(x + N)$ is a periodic function. So you can also go ahead and prove the general $g(x)$ case. Thanks. Edit : The general case can also be worked out by robjohn 's method by summing over each individual the Fourier components. Though a direct proof of the vanishing Euler-Maclaurin reminder term is still absent, robjohn's method solved my problem.","This is a variant of Exercise 64 in Chapter 9 of concrete mathematics. Prove the following identity \begin{equation} \sum_{n = -\infty}^{\infty}' \frac{1 - \cos( 2\pi n k )}{n^2 }  = 2 \pi^2 ( k - k^2 ) \qquad k \in [0,1] \end{equation} I came across it in a different context and was surprised that it is exact. Let's use Euler-Maclaurin expansion to convert the sum into an integral. Take $f(x) = \frac{1 - \cos( 2\pi x k )}{x^2 } $ \begin{equation} \sum_{n = -N+1}^{N}' f(n) = \int_{-N}^{N} f(x) dx - f(0) + \sum_{k=1}^{p} \frac{B_k}{k!}f^{(k-1)}(x)\Big|^N_{-N} + R_p  \end{equation} The first two terms are already the exact result,  \begin{equation} \int_{-\infty}^{\infty} f(x) dx = 2\pi k \int_{-\infty}^{\infty} \frac{1- \cos x}{x^2}dx = 2\pi^2 k \qquad f(0) = 2\pi^2 k^2  \end{equation} so one needs to prove the end point corrections(terms with $B_k$ in it) as well as the reminder terms are zero. At any order $p$, there are only finite number of end point correction terms which goes at most like $\mathcal{O}(\frac{1}{N^2})$, so they vanish when taking the $N\rightarrow \infty$ limit. But I don't know how to show the reminder term is zero(as $N\rightarrow \infty$), \begin{equation} R_p(N)  = (-1)^p \int_{-N}^{N} \frac{1}{p!} B_p( x - \lfloor x \rfloor ) f^{(p)}(x) dx  \end{equation} Maybe there are other methods to prove the identity, but I personally would appreciate the proof using Euler Maclaurin, since I'm going to use it for another series, and the reminder term has \begin{equation} f(x) = \frac{g(x) - g(0) }{ x^2} - \frac{g'(0)}{x} \end{equation} where $g(x ) = g(x + N)$ is a periodic function. So you can also go ahead and prove the general $g(x)$ case. Thanks. Edit : The general case can also be worked out by robjohn 's method by summing over each individual the Fourier components. Though a direct proof of the vanishing Euler-Maclaurin reminder term is still absent, robjohn's method solved my problem.",,"['calculus', 'real-analysis', 'numerical-methods', 'bernoulli-numbers', 'euler-maclaurin']"
91,Does intermediate value theorem apply to continuity on open intervals?,Does intermediate value theorem apply to continuity on open intervals?,,"Does the intermediate value theorem apply to functions that are continuous on the open intervals $(a,b)\,$ ? I know it's pretty vital for the theorem to be able to show the values of $f(a)$ and $f(b)$ , but what if I have calculated the limits as $x \to a$ (from the right-hand side) and $x\to b$ (from the left-hand side)? If these limits are of opposite signs, would this be sufficient to say that according to the intermediate value theorem, the function has at least one root? Would it be a problem if the limits were $+$ or $-$ infinity?","Does the intermediate value theorem apply to functions that are continuous on the open intervals ? I know it's pretty vital for the theorem to be able to show the values of and , but what if I have calculated the limits as (from the right-hand side) and (from the left-hand side)? If these limits are of opposite signs, would this be sufficient to say that according to the intermediate value theorem, the function has at least one root? Would it be a problem if the limits were or infinity?","(a,b)\, f(a) f(b) x \to a x\to b + -","['real-analysis', 'limits', 'continuity']"
92,From finitely additive to countably additive,From finitely additive to countably additive,,"Given a finitely additive measure $\mu : \mathcal{B} \rightarrow [0,\infty]$. If we want to show that it is also countably additive, finite additivity implies  $$\mu(E) \geq \sum_{n=1}^\infty \mu(E_n)$$  where $\cup E_n = E$ with $E_n$ disjoint automatically. For the inequality in the other direction, clearly countably subadditive is sufficient. I was wondering if there are weaker conditions that would also be sufficient. (Just take $\mathcal{B}$ to be the Borel $\sigma$-algebra on $\mathbb{R}$) I know for fact that if $\mu$ is a Radon measure would also be enough (maybe $\sigma$-finite with some regularity properties).","Given a finitely additive measure $\mu : \mathcal{B} \rightarrow [0,\infty]$. If we want to show that it is also countably additive, finite additivity implies  $$\mu(E) \geq \sum_{n=1}^\infty \mu(E_n)$$  where $\cup E_n = E$ with $E_n$ disjoint automatically. For the inequality in the other direction, clearly countably subadditive is sufficient. I was wondering if there are weaker conditions that would also be sufficient. (Just take $\mathcal{B}$ to be the Borel $\sigma$-algebra on $\mathbb{R}$) I know for fact that if $\mu$ is a Radon measure would also be enough (maybe $\sigma$-finite with some regularity properties).",,"['real-analysis', 'measure-theory']"
93,Measure Theory - Absolute Continuity,Measure Theory - Absolute Continuity,,"A question from my homework: Suppose $\mu$ is a measure on the real line w.r.t to the Borel $\sigma$-algebra such that $\forall x \in \mathbb{R}$ $\mu(\{x\})=0 $. is $\mu$ necessarily absolutely continuous w.r.t. to the Lebesgue measure? We say $\mu$ is absolutely continuous w.r.t. to $m$ if for every measurable set $E$ such that $m(E)=0$ than $\mu(E)=0$ also. I can't see a reason why this should be true, but can't find a good counter example. I tired some variations on the counting measure. Also tried to construct some measures that give positive measure on uncountable sets and zero measure on countable, be these come out non-additive (i.e. not measures). Would greatly appreciate the help. Thanks!","A question from my homework: Suppose $\mu$ is a measure on the real line w.r.t to the Borel $\sigma$-algebra such that $\forall x \in \mathbb{R}$ $\mu(\{x\})=0 $. is $\mu$ necessarily absolutely continuous w.r.t. to the Lebesgue measure? We say $\mu$ is absolutely continuous w.r.t. to $m$ if for every measurable set $E$ such that $m(E)=0$ than $\mu(E)=0$ also. I can't see a reason why this should be true, but can't find a good counter example. I tired some variations on the counting measure. Also tried to construct some measures that give positive measure on uncountable sets and zero measure on countable, be these come out non-additive (i.e. not measures). Would greatly appreciate the help. Thanks!",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
94,About the order of the $L^1$ norm of the Dirichlet kernel.,About the order of the  norm of the Dirichlet kernel.,L^1,"Reading this text from Wikipedia , I found the following statement about the Dirichlet kernel: $$\| D_n \|_{L^1} \approx \log n, $$ where $\approx$ denotes ""is of the order"". I think that this mean that $$\displaystyle{\lim_{n\to\infty} \frac{\|D_n\|_{L^1}}{\log n} = 1},$$ as usual. I have trying to prove this, but without success. Someone can to say me how to show this? Or maybe, indicate to me a text where I can find a proof of this? Thanks! Note 1: $D_n$ is the Dirichlet kernel, i.e., $$ D_n(t) := \sum_{k=-n}^{n} e^{i kt} = 1 + 2\sum_{k=1}^{n} \cos kt. $$ Note 2: $\|D_n\|_{L^1}$ is the $L^1$ norm of $D_n$, i.e., $$\displaystyle{\|D_n\|_{L^1} := \int_{-\pi}^{\pi} |D_n(t)|dt }.$$","Reading this text from Wikipedia , I found the following statement about the Dirichlet kernel: $$\| D_n \|_{L^1} \approx \log n, $$ where $\approx$ denotes ""is of the order"". I think that this mean that $$\displaystyle{\lim_{n\to\infty} \frac{\|D_n\|_{L^1}}{\log n} = 1},$$ as usual. I have trying to prove this, but without success. Someone can to say me how to show this? Or maybe, indicate to me a text where I can find a proof of this? Thanks! Note 1: $D_n$ is the Dirichlet kernel, i.e., $$ D_n(t) := \sum_{k=-n}^{n} e^{i kt} = 1 + 2\sum_{k=1}^{n} \cos kt. $$ Note 2: $\|D_n\|_{L^1}$ is the $L^1$ norm of $D_n$, i.e., $$\displaystyle{\|D_n\|_{L^1} := \int_{-\pi}^{\pi} |D_n(t)|dt }.$$",,"['real-analysis', 'fourier-analysis', 'fourier-series']"
95,Show that $f(x)=x^{1/3}$ is not differentiable at $x=0$.,Show that  is not differentiable at .,f(x)=x^{1/3} x=0,Show that $$f(x)=x^{1/3}$$ is not differentiable at $x=0$. $$\text{LHD at} \ x=0$$  $$= \lim_{h\to 0}\frac{f(0-h)-f(h)}{0-h-0}$$$$=\lim_{h\to 0}\frac{-h^{1/3}}{-h}=\lim_{h\to 0}-h^{-2/3}$$ and similarly $$\text{RHD at} \ x=0=\lim_{h\to 0}h^{-2/3}$$ If I directly substitute $h=0$ both will be $0$ or should I take $0$ to the denominator.  How do I solve this? and I have another doubt: How is $$\lim_{x\to 0}e^{-2/x}=0???$$ Thankyou,Show that $$f(x)=x^{1/3}$$ is not differentiable at $x=0$. $$\text{LHD at} \ x=0$$  $$= \lim_{h\to 0}\frac{f(0-h)-f(h)}{0-h-0}$$$$=\lim_{h\to 0}\frac{-h^{1/3}}{-h}=\lim_{h\to 0}-h^{-2/3}$$ and similarly $$\text{RHD at} \ x=0=\lim_{h\to 0}h^{-2/3}$$ If I directly substitute $h=0$ both will be $0$ or should I take $0$ to the denominator.  How do I solve this? and I have another doubt: How is $$\lim_{x\to 0}e^{-2/x}=0???$$ Thankyou,,"['calculus', 'real-analysis', 'derivatives']"
96,"Show that $f$ cannot have infinitely many zeroes in $[0, 1]$. [duplicate]",Show that  cannot have infinitely many zeroes in . [duplicate],"f [0, 1]","This question already has an answer here : on $\mathbb{R}$ such that $f$ and $f'$ has no common $0$ in $[0,1]$, (1 answer) Closed 11 years ago . Let $f : \mathbb{R}\to \mathbb{R}$ be a differentiable function such that $f$ and its derivative have no common zero in the closed interval $[0, 1]$. Show that f cannot have infinitely many zeroes in $[0, 1]$.","This question already has an answer here : on $\mathbb{R}$ such that $f$ and $f'$ has no common $0$ in $[0,1]$, (1 answer) Closed 11 years ago . Let $f : \mathbb{R}\to \mathbb{R}$ be a differentiable function such that $f$ and its derivative have no common zero in the closed interval $[0, 1]$. Show that f cannot have infinitely many zeroes in $[0, 1]$.",,['real-analysis']
97,Why does the Jacobian show a transformation is one to one?,Why does the Jacobian show a transformation is one to one?,,"Consider $f: \mathbb{R}^2 \to \mathbb{R}^2$, where $f$ is given by $$f =\begin{bmatrix} f_1(x,y)\\  f_2(x,y) \end{bmatrix}$$ Where we may assume $f$ is of class $C^1$ (continuously differentiable), if $J$ is the associated Jacobian of $f$ and if $\det J \neq 0$ for every $x,y \in\mathbb{R}^2$, why does that imply $f$ is one to one? I understand this idea for single variable functions, but not so convinced in multivariables. EDIT , Thank you for the counterexample, but let me add one more restriction. Let $f$ be one to one  defined on an open set $A$, so $f: A \to \mathbb{R}^2$","Consider $f: \mathbb{R}^2 \to \mathbb{R}^2$, where $f$ is given by $$f =\begin{bmatrix} f_1(x,y)\\  f_2(x,y) \end{bmatrix}$$ Where we may assume $f$ is of class $C^1$ (continuously differentiable), if $J$ is the associated Jacobian of $f$ and if $\det J \neq 0$ for every $x,y \in\mathbb{R}^2$, why does that imply $f$ is one to one? I understand this idea for single variable functions, but not so convinced in multivariables. EDIT , Thank you for the counterexample, but let me add one more restriction. Let $f$ be one to one  defined on an open set $A$, so $f: A \to \mathbb{R}^2$",,"['real-analysis', 'linear-algebra', 'multivariable-calculus']"
98,Uniform convergence of $n^2x^2e^{-nx}$,Uniform convergence of,n^2x^2e^{-nx},"The problem: Show that if $a>0$, then the sequence $f_n(x) = (n^2x^2e^{-nx})$ converges uniformly on the interval $[a,\infty)$ but not on $[0,\infty)$ My Solution: On the interval of $[0,\infty)$ we notice that $f_n(x)$ attains a maximum at $2/n$ by taking the derivative and setting it equal to zero. Since $f_n(2/n) = 4e^{-2}$, we know that our function cannot converge uniformly on $[0,\infty)$ since $||f_n||\not\rightarrow 0$. On the interval of $[a,\infty)$ $f_n$ does converge uniformly, because for large $n$, $2/n \rightarrow 0$, and therefore $a>2/n$ and thus $||f_n|| = n^2a^2e^{-na}$ which tends towards zero, and thus converges uniformly. My question : It seems that the problem is at $x=0$ because removing this point allows for uniform convergence. What I dont get is why $x=0$ causes a problem to begin with. If we analyze $f_n$ pointwise at $x=0$, dont we have that $f_n(0) = 0$ for all $n\in \mathbb{N}$?  Im conceptually lost as to where the problem arises by retaining the $x=0$ value. Thanks for your help!","The problem: Show that if $a>0$, then the sequence $f_n(x) = (n^2x^2e^{-nx})$ converges uniformly on the interval $[a,\infty)$ but not on $[0,\infty)$ My Solution: On the interval of $[0,\infty)$ we notice that $f_n(x)$ attains a maximum at $2/n$ by taking the derivative and setting it equal to zero. Since $f_n(2/n) = 4e^{-2}$, we know that our function cannot converge uniformly on $[0,\infty)$ since $||f_n||\not\rightarrow 0$. On the interval of $[a,\infty)$ $f_n$ does converge uniformly, because for large $n$, $2/n \rightarrow 0$, and therefore $a>2/n$ and thus $||f_n|| = n^2a^2e^{-na}$ which tends towards zero, and thus converges uniformly. My question : It seems that the problem is at $x=0$ because removing this point allows for uniform convergence. What I dont get is why $x=0$ causes a problem to begin with. If we analyze $f_n$ pointwise at $x=0$, dont we have that $f_n(0) = 0$ for all $n\in \mathbb{N}$?  Im conceptually lost as to where the problem arises by retaining the $x=0$ value. Thanks for your help!",,"['real-analysis', 'convergence-divergence', 'uniform-convergence']"
99,How to show that the sum of $L^p$ spaces is Banach.,How to show that the sum of  spaces is Banach.,L^p,"Let $p<q$ be positive integers (with the allowance that $q$ may be $\infty$). How can we show that the sum of $L^p$ and $L^q$ is a Banach space under the norm $\|f\|=\inf\{\|g\|_p+\|h\|_q: g+h=f\}$? Let $\{f_n\}$ be a sequence in $L^p+L^q$, such that $$\sum_{n=1}^\infty \|f_n\|<\infty.$$ We would like to conclude that this implies that $\sum_{n=1}^\infty f_n$ converges to something in $L^p+L^q$, at which point it follows that $L^p+L^q$ is complete by a basic theorem. We could easily do this if for each $f_i$ it were possible to express $f_i=g_i+h_i$, where $g_i\in L^p, h_i\in L^q$ and $\|f_i\|=\|g_i\|_p+\|h_i\|_q$. Though we know Cauchy sequences in $L^p$ converge, it is not clear that for a given $f$, all sequences (or some sequence) $\{g_n+h_n\}$ such that the $(\|g_n\|_p+\|h_n\|_q)\to \|f_n\|$ have the property that $\{g_n\},\{h_n\}$ converge in their respective spaces. It seems possible to imagine the sum converging without the summands converging in their respective spaces. Any help would be much appreciated.","Let $p<q$ be positive integers (with the allowance that $q$ may be $\infty$). How can we show that the sum of $L^p$ and $L^q$ is a Banach space under the norm $\|f\|=\inf\{\|g\|_p+\|h\|_q: g+h=f\}$? Let $\{f_n\}$ be a sequence in $L^p+L^q$, such that $$\sum_{n=1}^\infty \|f_n\|<\infty.$$ We would like to conclude that this implies that $\sum_{n=1}^\infty f_n$ converges to something in $L^p+L^q$, at which point it follows that $L^p+L^q$ is complete by a basic theorem. We could easily do this if for each $f_i$ it were possible to express $f_i=g_i+h_i$, where $g_i\in L^p, h_i\in L^q$ and $\|f_i\|=\|g_i\|_p+\|h_i\|_q$. Though we know Cauchy sequences in $L^p$ converge, it is not clear that for a given $f$, all sequences (or some sequence) $\{g_n+h_n\}$ such that the $(\|g_n\|_p+\|h_n\|_q)\to \|f_n\|$ have the property that $\{g_n\},\{h_n\}$ converge in their respective spaces. It seems possible to imagine the sum converging without the summands converging in their respective spaces. Any help would be much appreciated.",,['real-analysis']
