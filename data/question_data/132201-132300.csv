,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Solving system of ODE with initial value problem (IVP),Solving system of ODE with initial value problem (IVP),,"I have a question about a system of ODE.  If we have: $\frac{dx}{dt}=x+2y$ $\frac{dy}{dt}=3x+2y$ with $x(0)=6$ and $y(0)=4$ , how come the solution to the IVP is: $x(t)=4e^{4t}+2e^{-t}$ $y(t)=6e^{4t}-2e^{-t}$ I tried doing integral by separating the variable but I didn't get that solution. That example & solution are from my numerical method book, please see the attached image example & solution","I have a question about a system of ODE.  If we have: with and , how come the solution to the IVP is: I tried doing integral by separating the variable but I didn't get that solution. That example & solution are from my numerical method book, please see the attached image example & solution",\frac{dx}{dt}=x+2y \frac{dy}{dt}=3x+2y x(0)=6 y(0)=4 x(t)=4e^{4t}+2e^{-t} y(t)=6e^{4t}-2e^{-t},"['ordinary-differential-equations', 'differential']"
1,Solving a symmetric pair of differential equations,Solving a symmetric pair of differential equations,,"Looking at graphical solutions of \begin{align} \dot{x} &= - x + cy \\ \dot{y} &= - cy + x, \end{align} which for example describe how a two-level toy system approaches thermal equilibrium: plot created with Octave (1) one might be tempted to think that \begin{align} x(t) &= x_\infty (1-e^{-t/\tau}) + x_0e^{-t/\tau} \\ y(t) &= y_\infty (1-e^{-t/\tau}) + y_0e^{-t/\tau}, \end{align} would be canonical solutions – provided that $x_\infty + y_\infty = x_0 + y_0$. Then one might want to find out how $\tau$ depends on $c$ . But unfortunately, it seems not to be true: $$\dot{x} = -\frac{x_\infty-x_0}{\tau}e^{-t/\tau} \neq - x + cy$$ My questions are: Did I make a mistake, and it is a solution? If not so: What are the canonical solutions to the pair of equations above? Is there a closed form? How do I derive it? (1) Octave code: function dx = f(x,t,c)    dx(1) = - x(1) + c * x(2);    dx(2) = -c * x(2) + x(1); endfunction g = @(x,t) f(x, t, 5); xs = lsode(g,[1,2],0:0.01:1); plot(xs);","Looking at graphical solutions of \begin{align} \dot{x} &= - x + cy \\ \dot{y} &= - cy + x, \end{align} which for example describe how a two-level toy system approaches thermal equilibrium: plot created with Octave (1) one might be tempted to think that \begin{align} x(t) &= x_\infty (1-e^{-t/\tau}) + x_0e^{-t/\tau} \\ y(t) &= y_\infty (1-e^{-t/\tau}) + y_0e^{-t/\tau}, \end{align} would be canonical solutions – provided that $x_\infty + y_\infty = x_0 + y_0$. Then one might want to find out how $\tau$ depends on $c$ . But unfortunately, it seems not to be true: $$\dot{x} = -\frac{x_\infty-x_0}{\tau}e^{-t/\tau} \neq - x + cy$$ My questions are: Did I make a mistake, and it is a solution? If not so: What are the canonical solutions to the pair of equations above? Is there a closed form? How do I derive it? (1) Octave code: function dx = f(x,t,c)    dx(1) = - x(1) + c * x(2);    dx(2) = -c * x(2) + x(1); endfunction g = @(x,t) f(x, t, 5); xs = lsode(g,[1,2],0:0.01:1); plot(xs);",,"['ordinary-differential-equations', 'statistical-mechanics']"
2,Curvature of a curve on a surface,Curvature of a curve on a surface,,"I dont know how to solve this problem in Do Carmo's Differential geometry of Curves and Surfaces. Can anyone help me? Let $C$ be a regular curve on a surface $S$ with Gaussian curvature $K>0$ and principle curvatures $k_1$ and $k_2$. Show that the curvature $k$ of $C$ at $p$ satisfies: $k\ge min\{|k_1|,|k_2|\}$. Do I have to use the normal curvature to solve the problem?","I dont know how to solve this problem in Do Carmo's Differential geometry of Curves and Surfaces. Can anyone help me? Let $C$ be a regular curve on a surface $S$ with Gaussian curvature $K>0$ and principle curvatures $k_1$ and $k_2$. Show that the curvature $k$ of $C$ at $p$ satisfies: $k\ge min\{|k_1|,|k_2|\}$. Do I have to use the normal curvature to solve the problem?",,"['ordinary-differential-equations', 'differential-geometry', 'curvature']"
3,System differential equations,System differential equations,,Solve the system differential equation $$\frac{dx}{\cos y}=\frac{dy}{\cos x}=\frac{dz}{\cos x \cos y}$$ I think: $$\frac{dx}{\cos y}=\frac{dy}{\cos x}$$ $$\cos x~dx = \cos y~dy$$ $$\sin x = \sin y + C_1$$ $$C_1 = \sin x - \sin y$$ And then I do not know how to solve it. Maybe: $$\frac{dxdy}{\cos y \cos x}=\frac{dz}{\cos x \cos y}$$ $$dx dy=dz$$ But what next?,Solve the system differential equation I think: And then I do not know how to solve it. Maybe: But what next?,\frac{dx}{\cos y}=\frac{dy}{\cos x}=\frac{dz}{\cos x \cos y} \frac{dx}{\cos y}=\frac{dy}{\cos x} \cos x~dx = \cos y~dy \sin x = \sin y + C_1 C_1 = \sin x - \sin y \frac{dxdy}{\cos y \cos x}=\frac{dz}{\cos x \cos y} dx dy=dz,['ordinary-differential-equations']
4,An interesting differential equations problem,An interesting differential equations problem,,"Suppose we have four ants, initially at rest, at the four corners of a square centered at the origin. They start walking clockwise, each ant walking directly toward the one in front of him. Suppose also that each ant walks with unit velocity, derive a differential equation that describes the trajectories. Thought: Here is the situation of the problem After some time $t$ , the ants are now at points $E,F,G,H$ . If we denote by $\mathbf{r(t)}$ the position of ant at $A$ , we know $\mathbf{r(0)} = (1,1)$ and after some $t$ , at point $E$ , we have $\mathbf{r(t)} = (x(t),y(t)) = E $ . We are given that $$ \frac{ y - 1 }{x - 1 } = 1 $$ Also, using the arclength formula, we know the path of ant $A$ is $$ \int\limits_0^t \sqrt{ (x')^2 + (y')^2 } dt $$ Am I on the right track?","Suppose we have four ants, initially at rest, at the four corners of a square centered at the origin. They start walking clockwise, each ant walking directly toward the one in front of him. Suppose also that each ant walks with unit velocity, derive a differential equation that describes the trajectories. Thought: Here is the situation of the problem After some time , the ants are now at points . If we denote by the position of ant at , we know and after some , at point , we have . We are given that Also, using the arclength formula, we know the path of ant is Am I on the right track?","t E,F,G,H \mathbf{r(t)} A \mathbf{r(0)} = (1,1) t E \mathbf{r(t)} = (x(t),y(t)) = E   \frac{ y - 1 }{x - 1 } = 1  A  \int\limits_0^t \sqrt{ (x')^2 + (y')^2 } dt ",['ordinary-differential-equations']
5,solution of differential equation $\left(\frac{dy}{dx}\right)^2-x\frac{dy}{dx}+y=0$,solution of differential equation,\left(\frac{dy}{dx}\right)^2-x\frac{dy}{dx}+y=0,"The solution of differential equation $\displaystyle \left(\frac{dy}{dx}\right)^2-x\frac{dy}{dx}+y=0$ $\bf{My\; Try::}$ Let $\displaystyle \frac{dy}{dx} = t\;,$ Then Diferential equation convert into $t^2-xt+y=0$ So Its solution is given by $\displaystyle t=\frac{x\pm \sqrt{x^2-4y}}{2}$ So we get $$\frac{dy}{dx} = \frac{x\pm \sqrt{x^2-4y}}{2}$$ Now How can I solve after that, Help me Thanks","The solution of differential equation $\displaystyle \left(\frac{dy}{dx}\right)^2-x\frac{dy}{dx}+y=0$ $\bf{My\; Try::}$ Let $\displaystyle \frac{dy}{dx} = t\;,$ Then Diferential equation convert into $t^2-xt+y=0$ So Its solution is given by $\displaystyle t=\frac{x\pm \sqrt{x^2-4y}}{2}$ So we get $$\frac{dy}{dx} = \frac{x\pm \sqrt{x^2-4y}}{2}$$ Now How can I solve after that, Help me Thanks",,['ordinary-differential-equations']
6,shape of membrane on circular frame with pressure difference,shape of membrane on circular frame with pressure difference,,"This is a problem I thought about recently, but I have no idea how to go about it: Consider a membrane evenly stretched across a round frame. What shape does this membrane take, when you have different air pressures on each side of the membrane? Basically we have a constant force in normal direction for every small piece of the same area.","This is a problem I thought about recently, but I have no idea how to go about it: Consider a membrane evenly stretched across a round frame. What shape does this membrane take, when you have different air pressures on each side of the membrane? Basically we have a constant force in normal direction for every small piece of the same area.",,"['ordinary-differential-equations', 'differential-geometry']"
7,Fourier transform and Laplace transform to solve differential equation,Fourier transform and Laplace transform to solve differential equation,,"Generally we know that both Fourier transform and Laplace transform are used to solve differential equation, first of all let us recall both forms, first Fourier transform: Sometimes instead of $-2*\pi*f$ , it is written $\omega$ , and we know also Laplace transform: In some cases the lower bound starts at $-\infty$ , and we know that: $s=\sigma+\omega*t$ , which means that Fourier transform is a special case of Laplace when $\sigma=0$ . But both methods is used to solve differential equations, my question us following  how could I guess which method should I used in given concrete example? Laplace transform is used to transform a differential equation into an algebraic equation which simplifies things, but what about the Fourier transform? What is its goal in solving differential equations?","Generally we know that both Fourier transform and Laplace transform are used to solve differential equation, first of all let us recall both forms, first Fourier transform: Sometimes instead of , it is written , and we know also Laplace transform: In some cases the lower bound starts at , and we know that: , which means that Fourier transform is a special case of Laplace when . But both methods is used to solve differential equations, my question us following  how could I guess which method should I used in given concrete example? Laplace transform is used to transform a differential equation into an algebraic equation which simplifies things, but what about the Fourier transform? What is its goal in solving differential equations?",-2*\pi*f \omega -\infty s=\sigma+\omega*t \sigma=0,"['ordinary-differential-equations', 'fourier-analysis', 'laplace-transform']"
8,"Finding the general solution of the differential equation $\,\,y''+y=f(x)$",Finding the general solution of the differential equation,"\,\,y''+y=f(x)","I am stuck with the following problem: I have to show that the general solution of the differential equation $$y''+y=f(x)\,\, ,x \in (-\infty,\infty)$$, where $f$ is continuous real valued function on  $(-\infty,\infty)$ is $$y(x)=A \cos x+B \sin x + \displaystyle \int_{0}^{x} f(t) \sin (x-t) dt\,\, $$ where $A,B$ are constants. C.F. part of the reduced differential equation $y''+y=0$ is : $A \cos x+B \sin x$. But I am having trouble to get the P.I.(particular integral) which can be obtained by solving $$\frac {1}{D^2+1} f(x)$$,where $D \equiv \frac {d}{dx}$. This is where I am stuck. Am I going in the right direction? Can someone help? Thanks and regards to all.","I am stuck with the following problem: I have to show that the general solution of the differential equation $$y''+y=f(x)\,\, ,x \in (-\infty,\infty)$$, where $f$ is continuous real valued function on  $(-\infty,\infty)$ is $$y(x)=A \cos x+B \sin x + \displaystyle \int_{0}^{x} f(t) \sin (x-t) dt\,\, $$ where $A,B$ are constants. C.F. part of the reduced differential equation $y''+y=0$ is : $A \cos x+B \sin x$. But I am having trouble to get the P.I.(particular integral) which can be obtained by solving $$\frac {1}{D^2+1} f(x)$$,where $D \equiv \frac {d}{dx}$. This is where I am stuck. Am I going in the right direction? Can someone help? Thanks and regards to all.",,['ordinary-differential-equations']
9,"With Euler's method for differential equations, is it possible to take the limit as $h \to 0$ and get an exact approximation?","With Euler's method for differential equations, is it possible to take the limit as  and get an exact approximation?",h \to 0,"I was recently watching a tutorial on Euler's method for approximating differential equations, and the whole time I was thinking ""why can't you just take the limit of the step size $h$ as it goes to $0$, and get an exact or near-exact approximation of the differential equation?"" So that is my question: is it possible to do that?","I was recently watching a tutorial on Euler's method for approximating differential equations, and the whole time I was thinking ""why can't you just take the limit of the step size $h$ as it goes to $0$, and get an exact or near-exact approximation of the differential equation?"" So that is my question: is it possible to do that?",,"['calculus', 'ordinary-differential-equations', 'limits', 'numerical-methods', 'approximation']"
10,integral transforms: why do roots in frequency domain correspond to eigenvalues in time domain (and how does it help solve differential equations)?,integral transforms: why do roots in frequency domain correspond to eigenvalues in time domain (and how does it help solve differential equations)?,,"In Wikipedia you can read about integral transforms , esp. the Laplace transform which maps a differential equation in the time domain into a polynomial equation in the complex frequency domain: roots of the polynomial equations in the complex frequency domain correspond to eigenvalues in the time domain Can anybody give a simple example and foremost give an intuition why this is so - and why does this help solving the differential equation? I don't see the big picture.","In Wikipedia you can read about integral transforms , esp. the Laplace transform which maps a differential equation in the time domain into a polynomial equation in the complex frequency domain: roots of the polynomial equations in the complex frequency domain correspond to eigenvalues in the time domain Can anybody give a simple example and foremost give an intuition why this is so - and why does this help solving the differential equation? I don't see the big picture.",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'integral-transforms', 'big-picture', 'laplace-transform']"
11,Explicit solutions to this nonlinear system of two differential equations,Explicit solutions to this nonlinear system of two differential equations,,"I am interested in a system of differential equations that is non-linear, but it doesn't seem to be too crazy. I'm not very good at non-linear stuff, so I thought I'd throw it out there. The actual equations I'm looking at have several parameters that'd I'd like to tweak eventually. q' = k - m / r r' = i - n r - j q i, j, k, m and n are all real-valued constants. I'm guessing that this system would be cyclical in nature, but I'm not sure if it has any explicit solution, so I have produced a version of it with the constants removed to see if that can be solved: q' = 1 - 1 / r r' = 1 - r - q Anyone know if either of these are solvable and what kind of techniques would be needed to solve them if so? The first equation is based on a polar coordinate system where Q (or theta) is the angle and r is radius, and I've made a number of simplifications to make it somewhat tractable.","I am interested in a system of differential equations that is non-linear, but it doesn't seem to be too crazy. I'm not very good at non-linear stuff, so I thought I'd throw it out there. The actual equations I'm looking at have several parameters that'd I'd like to tweak eventually. q' = k - m / r r' = i - n r - j q i, j, k, m and n are all real-valued constants. I'm guessing that this system would be cyclical in nature, but I'm not sure if it has any explicit solution, so I have produced a version of it with the constants removed to see if that can be solved: q' = 1 - 1 / r r' = 1 - r - q Anyone know if either of these are solvable and what kind of techniques would be needed to solve them if so? The first equation is based on a polar coordinate system where Q (or theta) is the angle and r is radius, and I've made a number of simplifications to make it somewhat tractable.",,['ordinary-differential-equations']
12,Analytical solution to a nonlinear system of coupled ODEs,Analytical solution to a nonlinear system of coupled ODEs,,"I think I'm being rather silly but I'm trying to see if the system of nonlinear coupled ODEs given by $$x'=-xy$$ $$y'=-xy$$ has any non-trivial solutions. After some googling I have found that a solution to this problem is given by $$ y(t)=c_{1}-\frac{c_{1}e^{c_{1}c_{2}}}{e^{c_{1}c_{2}}-e^{c_{1}t}}$$ $$ x(t)=-\frac{c_{1}e^{c_{1}c_{2}}}{e^{c_{1}c_{2}}-e^{c_{1}t}}$$ but I have no idea how this solution was obtained? Any insight into the problem or this solution would be a great help. This all started as I was trying to see if the system given by $$x'=-k_{1}xy-k_{2}xz$$ $$y'=-k_{1}xy$$ $$z'=-k_{2}xz$$ where $k_{1}$ and $k_{2}$ are constants, has an analytical solution. I simplified the problem and obtained what I presented at the start of this post. Anyway, any insight into any of this is appreciated.","I think I'm being rather silly but I'm trying to see if the system of nonlinear coupled ODEs given by has any non-trivial solutions. After some googling I have found that a solution to this problem is given by but I have no idea how this solution was obtained? Any insight into the problem or this solution would be a great help. This all started as I was trying to see if the system given by where and are constants, has an analytical solution. I simplified the problem and obtained what I presented at the start of this post. Anyway, any insight into any of this is appreciated.",x'=-xy y'=-xy  y(t)=c_{1}-\frac{c_{1}e^{c_{1}c_{2}}}{e^{c_{1}c_{2}}-e^{c_{1}t}}  x(t)=-\frac{c_{1}e^{c_{1}c_{2}}}{e^{c_{1}c_{2}}-e^{c_{1}t}} x'=-k_{1}xy-k_{2}xz y'=-k_{1}xy z'=-k_{2}xz k_{1} k_{2},"['ordinary-differential-equations', 'dynamical-systems', 'nonlinear-system']"
13,"Exactly one solution of $y'=-xy^{1/3}, y(0)=0$",Exactly one solution of,"y'=-xy^{1/3}, y(0)=0","I want to show that the IVP $$y'=-xy^{1/3}, y(0)=0$$ has an unique solution given by $y\equiv 0$ . One cannot apply the Picard-Lindelöf theorem because $f(x,y)=-xy^{1/3}$ is not Lipschitz in any neighbourhood of (0,0). Futhermore, the sufficent condition that $$\int_0^{0+\alpha} s^{-1/3} ds$$ is not convergent is not useful. So, how does one prove that there exists a unique solution for the IVP? Thanks for your answers!","I want to show that the IVP has an unique solution given by . One cannot apply the Picard-Lindelöf theorem because is not Lipschitz in any neighbourhood of (0,0). Futhermore, the sufficent condition that is not convergent is not useful. So, how does one prove that there exists a unique solution for the IVP? Thanks for your answers!","y'=-xy^{1/3}, y(0)=0 y\equiv 0 f(x,y)=-xy^{1/3} \int_0^{0+\alpha} s^{-1/3} ds","['ordinary-differential-equations', 'lipschitz-functions', 'initial-value-problems']"
14,Using Maclaurin series to solve $y'=xe^x$.,Using Maclaurin series to solve .,y'=xe^x,"I am trying to solve a differential equation using the Maclaurin series. The differential equation is $$y'=xe^x$$ My solution 1: $xe^x$ expressed in sigma notation is: $\sum_{k=0}^\infty\frac{x^{k+1}}{k!}$ and $y'=\sum_{k=0}^\infty(k+1)a_{k+1}x^k$ . Equating them will lead to $$\sum_{k=0}^\infty(k+1)a_{k+1}x^k=\sum_{k=0}^\infty\frac{x^{k+1}}{k!}$$ I was trying to comparing the coefficients, so as to obtain a recurrence relation. But I couldn't do it because the $x$ on both sides of the equation has different exponents ( $k$ and $k+1$ ) respectively. My solution 2: I integrate the differential equation to obtain $y=xe^x-e^x+c$ which in sigma notation corresponds to $$y=c+\sum_{k=0}^\infty\frac{x^{k+1}}{k!}-\sum_{k=0}^\infty\frac{x^k}{k!}$$ which can be samplified as $$y=c+\sum_{k=0}^\infty\frac{x^k(x-1)}{k!}$$ Question: Am I going in the right direction? In both solutions, I am stuck and am unable to arrive at the textbook solution of $$y=c+\sum_{k=0}^\infty\frac{x^{k+2}}{(k+2)k!}$$ Thank you in advance.","I am trying to solve a differential equation using the Maclaurin series. The differential equation is My solution 1: expressed in sigma notation is: and . Equating them will lead to I was trying to comparing the coefficients, so as to obtain a recurrence relation. But I couldn't do it because the on both sides of the equation has different exponents ( and ) respectively. My solution 2: I integrate the differential equation to obtain which in sigma notation corresponds to which can be samplified as Question: Am I going in the right direction? In both solutions, I am stuck and am unable to arrive at the textbook solution of Thank you in advance.",y'=xe^x xe^x \sum_{k=0}^\infty\frac{x^{k+1}}{k!} y'=\sum_{k=0}^\infty(k+1)a_{k+1}x^k \sum_{k=0}^\infty(k+1)a_{k+1}x^k=\sum_{k=0}^\infty\frac{x^{k+1}}{k!} x k k+1 y=xe^x-e^x+c y=c+\sum_{k=0}^\infty\frac{x^{k+1}}{k!}-\sum_{k=0}^\infty\frac{x^k}{k!} y=c+\sum_{k=0}^\infty\frac{x^k(x-1)}{k!} y=c+\sum_{k=0}^\infty\frac{x^{k+2}}{(k+2)k!},"['ordinary-differential-equations', 'power-series']"
15,"Solving a coupled system of linear ODEs (one second order, the other first order)","Solving a coupled system of linear ODEs (one second order, the other first order)",,"I have two coupled ODEs for $T(x)$ and $t(x)$ : $$\frac{d^2 T(x)}{d x^2}-\beta (T(x)-t(x))+K=0 \tag 1$$ $$\frac{d t(x)}{dx}-\alpha(T(x)-t(x))=0 \tag 2$$ $\alpha, \beta$ and $K$ are constants $>0$ . Also, it is known that $t(x=0)=t_i$ . Additionally, for $(1)$ we know: $$\frac{d T(x=0)}{d x}=\frac{d T(x=L)}{d x} = 0$$ I need to determine $T(x)$ and $t(x)$ .  Can anyone suggest a way towards moving ahead with this problem ? Probably this system of coupled equations can be solved using the matrix method, but I am not aware of it. I normally solve a single equation using either the method of integrating factor or using a characteristic equation and finding the roots.","I have two coupled ODEs for and : and are constants . Also, it is known that . Additionally, for we know: I need to determine and .  Can anyone suggest a way towards moving ahead with this problem ? Probably this system of coupled equations can be solved using the matrix method, but I am not aware of it. I normally solve a single equation using either the method of integrating factor or using a characteristic equation and finding the roots.","T(x) t(x) \frac{d^2 T(x)}{d x^2}-\beta (T(x)-t(x))+K=0 \tag 1 \frac{d t(x)}{dx}-\alpha(T(x)-t(x))=0 \tag 2 \alpha, \beta K >0 t(x=0)=t_i (1) \frac{d T(x=0)}{d x}=\frac{d T(x=L)}{d x} = 0 T(x) t(x)",['ordinary-differential-equations']
16,"Power Series Solution of $y''+y=0$, and Summation Indices","Power Series Solution of , and Summation Indices",y''+y=0,"I have a general question about this following problem \begin{equation} y''+y=0\end{equation} The required method to solve this problem is based on generating the power series solution, using the power series method. I began my problem by setting up the summations. \begin{align}\sum_{n=2}^\infty(n)(n-1)c_nx^{n-2}+\sum_{n=0}^\infty x^nc_n&=0\end{align} Then I did two substitutions, and then got to the following equation: \begin{equation}\sum_{k=0}^\infty[(k+2)(k+1)c_{k+2}+c_k]x^k=0\end{equation} Then I set the part that I know could zero out which was the inner portion of the sum: \begin{equation}(k+2)(k+1)c_{k+2}+c_k=0 \end{equation} Then I get the following equation: \begin{equation} c_{k+2}=-\frac{c_k}{(k+2)(k+1)}\end{equation} After that I decided to do the following and break it apart into a table: \begin{array}{|c|c|}k=0&k=1\\c_2=-\frac{c_0}{2\cdot1}&c_3=-\frac{c_1}{3\cdot2\cdot1}\\ \hline k=2 & k=3 \\ c_4 = \frac{c_o}{4\cdot 3\cdot 2 \cdot 1} & c_5 = \frac{c_1}{5\cdot 4\cdot 3\cdot 2\cdot 1}\end{array} Based off the pattern I that sgn changes, and that there is factorial in the denominator is my attempt right thus far, and how to collaborate them into a power series solution?","I have a general question about this following problem The required method to solve this problem is based on generating the power series solution, using the power series method. I began my problem by setting up the summations. Then I did two substitutions, and then got to the following equation: Then I set the part that I know could zero out which was the inner portion of the sum: Then I get the following equation: After that I decided to do the following and break it apart into a table: Based off the pattern I that sgn changes, and that there is factorial in the denominator is my attempt right thus far, and how to collaborate them into a power series solution?",\begin{equation} y''+y=0\end{equation} \begin{align}\sum_{n=2}^\infty(n)(n-1)c_nx^{n-2}+\sum_{n=0}^\infty x^nc_n&=0\end{align} \begin{equation}\sum_{k=0}^\infty[(k+2)(k+1)c_{k+2}+c_k]x^k=0\end{equation} \begin{equation}(k+2)(k+1)c_{k+2}+c_k=0 \end{equation} \begin{equation} c_{k+2}=-\frac{c_k}{(k+2)(k+1)}\end{equation} \begin{array}{|c|c|}k=0&k=1\\c_2=-\frac{c_0}{2\cdot1}&c_3=-\frac{c_1}{3\cdot2\cdot1}\\ \hline k=2 & k=3 \\ c_4 = \frac{c_o}{4\cdot 3\cdot 2 \cdot 1} & c_5 = \frac{c_1}{5\cdot 4\cdot 3\cdot 2\cdot 1}\end{array},"['ordinary-differential-equations', 'power-series', 'solution-verification']"
17,Why most of the differential equation and theorem were formalized at no more than second derivative?,Why most of the differential equation and theorem were formalized at no more than second derivative?,,"I'm reading book where I realized that in many math books, i.e. Calculus, ODE, DG etc., many theorems and propositions were formalized in terms of first or second derivatives, or just any arbitrary derivatives. I mean, if it stopped at first derivative, it's somewhat understandable. But if a proposition proceeded to second derivative, why doesn't it just go to third, fourth, fifth... derivatives? I'm wondering that why people tend to stop at second derivative? Is there any particular theorems that second derivative was somewhat sufficient for some special/nontrivial conditions?","I'm reading book where I realized that in many math books, i.e. Calculus, ODE, DG etc., many theorems and propositions were formalized in terms of first or second derivatives, or just any arbitrary derivatives. I mean, if it stopped at first derivative, it's somewhat understandable. But if a proposition proceeded to second derivative, why doesn't it just go to third, fourth, fifth... derivatives? I'm wondering that why people tend to stop at second derivative? Is there any particular theorems that second derivative was somewhat sufficient for some special/nontrivial conditions?",,"['calculus', 'ordinary-differential-equations', 'analysis', 'differential-geometry']"
18,Optimal control problem with minimization of cost,Optimal control problem with minimization of cost,,"Consider the scalar system $\dot{x}=x+u$ , given the constraint $u(t+T)=u(t)$ . Find the control that drive $x(0)=1$ to $x(2T)=0$ and minimizes $$\int_0^{2T}u^2(t)\,\mathrm{d}t$$ Attempt: substituting $u(t)$ we get $$J=2\int_0^T (\dot{x}^2-2x\dot{x}+x^2 )dt$$ Now i can use Euler Lagrange's eq to find a differential equation of the form $$\ddot{x}=x \implies x(t)=c_1e^{t}+c_2e^{-t}$$ Now i can find $c_1$ and $c_2$ using Initial conditions, then find $x(t)$ and after that i can get the optimal control $u^{*}=\dot{x}-x$ . Is my attemp correct ? i'm unsure about using the constrain given, in the first step of the integral i took out the factor $2$ with incorporates the constrain, is this a valid method ?","Consider the scalar system , given the constraint . Find the control that drive to and minimizes Attempt: substituting we get Now i can use Euler Lagrange's eq to find a differential equation of the form Now i can find and using Initial conditions, then find and after that i can get the optimal control . Is my attemp correct ? i'm unsure about using the constrain given, in the first step of the integral i took out the factor with incorporates the constrain, is this a valid method ?","\dot{x}=x+u u(t+T)=u(t) x(0)=1 x(2T)=0 \int_0^{2T}u^2(t)\,\mathrm{d}t u(t) J=2\int_0^T (\dot{x}^2-2x\dot{x}+x^2 )dt \ddot{x}=x \implies x(t)=c_1e^{t}+c_2e^{-t} c_1 c_2 x(t) u^{*}=\dot{x}-x 2","['ordinary-differential-equations', 'optimization', 'control-theory', 'optimal-control']"
19,Quadratic Formula With Independent and Dependent Variables,Quadratic Formula With Independent and Dependent Variables,,"Given the differential equation $dy/dt = (y + t)^2$ , we can apply the u-substitution $u = y + t$ to arrive at the separable differential equation $du/dt = u^2 + 1$ .  This separates to $1/(u^2 + 1)\ du = dt$ which integrates to (EDIT:  As LutzL has pointed out, I integrated incorrectly.  However, correcting it would eclipse potentially interesting part of the question, so I'll leave the mistake) $u^2 + 1 - Ce^t = 0$ .  Reverting the substitution yields $y^2 + 2ty + t^2 + 1 - Ce^t = 0$ .  Note that $y$ is a dependent variable, $t$ is the independent variable, and $C$ is an arbitrary constant. Is it legal to proceed via the quadratic formula, using the appropriate expressions in terms of $t$ as coefficients?  This would look like $y = (-(2t) ± \sqrt{(2t)^2 - 4(1)(t^2 + 1 - Ce^t)})\ /\ 2(1)$ , which works out to $y = -t ± \sqrt{Ce^t - 1}$ .  However, this practice feels a bit suspect, since in other instances of applying the quadratic formula, there is no dependency between the variable and its coefficients, whereas here there is.  Is this a legal and correct approach to the problem? Secondly, suppose a similar problem yielded $y^2 + 2ty + t^2 + 1 − Ce^y = 0$ , where $y$ is still a dependent variable, $t$ is still the independent variable, and $C$ is still an arbitrary constant.  Would it be legal to solve for $t$ using the quadratic formula using the appropriate expressions in terms of $y$ as coefficients?","Given the differential equation , we can apply the u-substitution to arrive at the separable differential equation .  This separates to which integrates to (EDIT:  As LutzL has pointed out, I integrated incorrectly.  However, correcting it would eclipse potentially interesting part of the question, so I'll leave the mistake) .  Reverting the substitution yields .  Note that is a dependent variable, is the independent variable, and is an arbitrary constant. Is it legal to proceed via the quadratic formula, using the appropriate expressions in terms of as coefficients?  This would look like , which works out to .  However, this practice feels a bit suspect, since in other instances of applying the quadratic formula, there is no dependency between the variable and its coefficients, whereas here there is.  Is this a legal and correct approach to the problem? Secondly, suppose a similar problem yielded , where is still a dependent variable, is still the independent variable, and is still an arbitrary constant.  Would it be legal to solve for using the quadratic formula using the appropriate expressions in terms of as coefficients?",dy/dt = (y + t)^2 u = y + t du/dt = u^2 + 1 1/(u^2 + 1)\ du = dt u^2 + 1 - Ce^t = 0 y^2 + 2ty + t^2 + 1 - Ce^t = 0 y t C t y = (-(2t) ± \sqrt{(2t)^2 - 4(1)(t^2 + 1 - Ce^t)})\ /\ 2(1) y = -t ± \sqrt{Ce^t - 1} y^2 + 2ty + t^2 + 1 − Ce^y = 0 y t C t y,"['ordinary-differential-equations', 'independence', 'constants']"
20,What is the intuition behind the method of undetermined coefficients?,What is the intuition behind the method of undetermined coefficients?,,"Our teacher has recently begun teaching second-order differential equations and the methods used for solving them. The method which we are taught to solve linear differential equations is currently the method of undetermined coefficients. I would like to know if there is a reason as to why the method works. My chief question is about why we're able to just ""step-up"" our guesses by an x term (i.e. if $$e^{ax}$$ does not work we're simply able to amend our guess to $$xe^{ax}$$ Secondly, why is it that when doing so the terms in between seem to nicely cancel out? An example is the differential equation in one of our tutorials $$\frac{d^2y}{dx^2}-6\frac{dy}{dx}+9y=e^{3x}$$ As the solution to the characteristic polynomial has repeated roots 3, I understand why guesses of the form $$Ae^3x$$ $$Axe^3x$$ fail to work (because they get ""absorbed"" into the general solution of the complementary solution) and as a result why my ""guess"" has to be $$Ax^2e^{3x}$$ but I do not understand why when plugging in the solved integrals that somehow I get this mess of an equation $$A(9x^2e^{3x}+12xe^{3x}+2e^{3x}-18x^2e^{3x}-12xe^{3x}+9x^2e^{3x})=e^{3x}$$ that somehow resolves nicely to $$A(2e^{3x})=e^{3x}$$ Thank you for taking the time to read this.","Our teacher has recently begun teaching second-order differential equations and the methods used for solving them. The method which we are taught to solve linear differential equations is currently the method of undetermined coefficients. I would like to know if there is a reason as to why the method works. My chief question is about why we're able to just ""step-up"" our guesses by an x term (i.e. if does not work we're simply able to amend our guess to Secondly, why is it that when doing so the terms in between seem to nicely cancel out? An example is the differential equation in one of our tutorials As the solution to the characteristic polynomial has repeated roots 3, I understand why guesses of the form fail to work (because they get ""absorbed"" into the general solution of the complementary solution) and as a result why my ""guess"" has to be but I do not understand why when plugging in the solved integrals that somehow I get this mess of an equation that somehow resolves nicely to Thank you for taking the time to read this.",e^{ax} xe^{ax} \frac{d^2y}{dx^2}-6\frac{dy}{dx}+9y=e^{3x} Ae^3x Axe^3x Ax^2e^{3x} A(9x^2e^{3x}+12xe^{3x}+2e^{3x}-18x^2e^{3x}-12xe^{3x}+9x^2e^{3x})=e^{3x} A(2e^{3x})=e^{3x},['ordinary-differential-equations']
21,Difference between direction field and vector field,Difference between direction field and vector field,,"This is from Arnold's ODE: $\mathbf{Problem.}$ Can every smooth direction field in a domain of the plane be extended to a smooth vector field? $\mathbf{Answer.}$ No, if the domain is not simply connected. I cannot see why simply connected is required. Here is the definition of vector field given in the book: $\mathbf{Definition}$ . A smooth vector field $\mathbf{v}$ is defined in a domain $M$ if to each point $x$ there is assigned a vector $\mathbf{v}(x)\in T_{x}M$ (tangent space) attached at that point and depending smoothly on the point From this definition, I have no clue why we cannot just assign direction field a smooth scalar function(the length of vector) then we obtain a smooth vector field. Please help. Thank you!","This is from Arnold's ODE: Can every smooth direction field in a domain of the plane be extended to a smooth vector field? No, if the domain is not simply connected. I cannot see why simply connected is required. Here is the definition of vector field given in the book: . A smooth vector field is defined in a domain if to each point there is assigned a vector (tangent space) attached at that point and depending smoothly on the point From this definition, I have no clue why we cannot just assign direction field a smooth scalar function(the length of vector) then we obtain a smooth vector field. Please help. Thank you!",\mathbf{Problem.} \mathbf{Answer.} \mathbf{Definition} \mathbf{v} M x \mathbf{v}(x)\in T_{x}M,"['ordinary-differential-equations', 'differential-geometry']"
22,What is the physical interpretation of homogeneous?,What is the physical interpretation of homogeneous?,,"In ode's and pde's we pay great attention as to whether the equations are homogeneous or nonhomogeneous. I remember learning in my first ODE class that for the general linear ode $$a_n(x)\frac{d^ny}{dx^n}+a_{n-1}(x)\frac{d^{n-1}y}{dx^{n-1}}+\cdots+a_1(x)\frac{dy}{dx}+a_0(x)y=g(x),$$ that $g(x)$ takes on some very important physical meanings in engineering problems but I can't remember what they are. And in general, could someone provide an interpretation of the physical meaning of g(x) in odes and in pdes? If your examples are only from famous and specific equations then that's welcome too.","In ode's and pde's we pay great attention as to whether the equations are homogeneous or nonhomogeneous. I remember learning in my first ODE class that for the general linear ode $$a_n(x)\frac{d^ny}{dx^n}+a_{n-1}(x)\frac{d^{n-1}y}{dx^{n-1}}+\cdots+a_1(x)\frac{dy}{dx}+a_0(x)y=g(x),$$ that $g(x)$ takes on some very important physical meanings in engineering problems but I can't remember what they are. And in general, could someone provide an interpretation of the physical meaning of g(x) in odes and in pdes? If your examples are only from famous and specific equations then that's welcome too.",,"['ordinary-differential-equations', 'partial-differential-equations', 'soft-question', 'homogeneous-equation']"
23,Distinction between stable and asymptotically stable equilibria,Distinction between stable and asymptotically stable equilibria,,"I read the book A Linear Systems Primer by Antsaklis, Panos J., and Anthony N. Michel (Vol. 1. Boston: Birkhäuser, 2007) and I am confused by the definition of stable . It is defined as: \begin{equation}  \dot{x}=f(x), f(0)=0\tag{4.8}  \end{equation} Definition 4.6 . The equilibrium $x = 0$ of (4.8) is said to be stable if for every $\epsilon > 0$, there exists a $\delta(\epsilon)>0$, such that   \begin{equation}  ||φ(t, x_0 )|| < \epsilon  ~for ~all~ t ≥ 0  \end{equation}   whenever $||x_0||<\delta(\epsilon) $ In this definition, does it mean that $ \lim_{t\to\infty} φ(t, x_0 ) = 0$ because $\epsilon$ could be any arbitrarily small positive number? If this is true, then the definition of stable equals to that of asymptotically stable which should NOT be correct. So what is the wrong part in my thought?","I read the book A Linear Systems Primer by Antsaklis, Panos J., and Anthony N. Michel (Vol. 1. Boston: Birkhäuser, 2007) and I am confused by the definition of stable . It is defined as: \begin{equation}  \dot{x}=f(x), f(0)=0\tag{4.8}  \end{equation} Definition 4.6 . The equilibrium $x = 0$ of (4.8) is said to be stable if for every $\epsilon > 0$, there exists a $\delta(\epsilon)>0$, such that   \begin{equation}  ||φ(t, x_0 )|| < \epsilon  ~for ~all~ t ≥ 0  \end{equation}   whenever $||x_0||<\delta(\epsilon) $ In this definition, does it mean that $ \lim_{t\to\infty} φ(t, x_0 ) = 0$ because $\epsilon$ could be any arbitrarily small positive number? If this is true, then the definition of stable equals to that of asymptotically stable which should NOT be correct. So what is the wrong part in my thought?",,"['ordinary-differential-equations', 'stability-theory']"
24,Does it make sense to talk about eigenvalues in a 1x1 matrix?,Does it make sense to talk about eigenvalues in a 1x1 matrix?,,"I am an Economics student and I am trying to apply the determinacy conditions as described in Blanchard, Kahn (1980) to check for the existence of a unique solution in a differential equation. These conditions state that I need to have the number of eigenvalues of a matrix inside the unit circle equal to the number of jumpy variables in the system. Putting aside the economic meaning of that, I have a case in which I am dealing with a unique differential equation so that I do not have a matrix but a scalar, i.e. \begin{gather} \pi_t=\frac{1}{1+\phi}\pi_{t+1}+\epsilon_t \end{gather} Thus I am wondering if the eigenvalue in this context could be identified with the unique element of the 1x1 matrix made of $\frac{1}{1+\phi}$.","I am an Economics student and I am trying to apply the determinacy conditions as described in Blanchard, Kahn (1980) to check for the existence of a unique solution in a differential equation. These conditions state that I need to have the number of eigenvalues of a matrix inside the unit circle equal to the number of jumpy variables in the system. Putting aside the economic meaning of that, I have a case in which I am dealing with a unique differential equation so that I do not have a matrix but a scalar, i.e. \begin{gather} \pi_t=\frac{1}{1+\phi}\pi_{t+1}+\epsilon_t \end{gather} Thus I am wondering if the eigenvalue in this context could be identified with the unique element of the 1x1 matrix made of $\frac{1}{1+\phi}$.",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors']"
25,a linear differential equation with periodic coefficients,a linear differential equation with periodic coefficients,,"Let  $$y' = a(x) y + b(x)$$ be a linear differential equation with continuous, periodic coefficients $a, b: \mathbb{R} \to \mathbb{R}$ that both have a period of $T > 0$. Also, we assume that $\int_0^T a(t) dt ≠ 0$. Based on this, I now want to show that the given differential equation has exactly one $T$-periodic solution. Thanks in advance. My thought so far was that we might somehow transfer the periodicity onto $y$ by maybe considering a function like $z(x) = y(x + T)$, and then find one solution that fits both functions, and show that it's the only one. For the existence of a solution, we probably have to use the given integral, but I'm not entirely sure how.","Let  $$y' = a(x) y + b(x)$$ be a linear differential equation with continuous, periodic coefficients $a, b: \mathbb{R} \to \mathbb{R}$ that both have a period of $T > 0$. Also, we assume that $\int_0^T a(t) dt ≠ 0$. Based on this, I now want to show that the given differential equation has exactly one $T$-periodic solution. Thanks in advance. My thought so far was that we might somehow transfer the periodicity onto $y$ by maybe considering a function like $z(x) = y(x + T)$, and then find one solution that fits both functions, and show that it's the only one. For the existence of a solution, we probably have to use the given integral, but I'm not entirely sure how.",,"['real-analysis', 'analysis', 'ordinary-differential-equations']"
26,Inverse Laplace Transform of $s^n$,Inverse Laplace Transform of,s^n,"I want to calculate Inverse Laplace Transform of $s^n$. I have an idea, but I do not know if it works? We have a formula to compute inverse laplace transforms of functions as below, $$\mathcal{L}^{-1} [ F(s) ] = -\frac{\mathcal{L}^{-1} [ F^{\prime}(s) ]}{t}.$$ So from the given formula, we can obtain $$\mathcal{L}^{-1} [ s ]= -\frac{\mathcal{L}^{-1} [ 1 ]}{t}= -\frac{\delta (t)}{t}.$$ and as a result, $$\mathcal{L}^{-1} [ s^n ] = (-1)^n\frac{n!\delta (t)}{t^n}$$  Is it right? In fact, I want to know the necessary conditions to use the given formula.","I want to calculate Inverse Laplace Transform of $s^n$. I have an idea, but I do not know if it works? We have a formula to compute inverse laplace transforms of functions as below, $$\mathcal{L}^{-1} [ F(s) ] = -\frac{\mathcal{L}^{-1} [ F^{\prime}(s) ]}{t}.$$ So from the given formula, we can obtain $$\mathcal{L}^{-1} [ s ]= -\frac{\mathcal{L}^{-1} [ 1 ]}{t}= -\frac{\delta (t)}{t}.$$ and as a result, $$\mathcal{L}^{-1} [ s^n ] = (-1)^n\frac{n!\delta (t)}{t^n}$$  Is it right? In fact, I want to know the necessary conditions to use the given formula.",,"['ordinary-differential-equations', 'laplace-transform']"
27,Asymptote of solution of a differential equation without solving it,Asymptote of solution of a differential equation without solving it,,Consider the following differential equation (domain $\mathbb{R}$): $$ u(x) = 1 - u'(x) $$ and suppose $u(0) = 0$. How can one prove that $u(x) \to 1$ for $x \to \infty$ without solving the equation explicitly?,Consider the following differential equation (domain $\mathbb{R}$): $$ u(x) = 1 - u'(x) $$ and suppose $u(0) = 0$. How can one prove that $u(x) \to 1$ for $x \to \infty$ without solving the equation explicitly?,,"['real-analysis', 'ordinary-differential-equations']"
28,How long would it take to clean up the Great Lakes?,How long would it take to clean up the Great Lakes?,,"Problem Details The idea of the problem is to find out how long it would take to flush the Great Lakes of pollution. They're set up as a series of five tanks and you are given inflow rates of clean water, inflow rates from the other tanks, and outflow rates. For our model, we make the following assumptions: The volume of each lake remains constant. The flow rates are constant throughout the year. When a liquid enters the lake, perfect mixing occurs and the pollutants are uniformly distributed. Pollutants are dissolved in the water and enter or leave by inflow or outflow of solution. Before using this model to obtain estimates on the cleanup times for the lakes, we consider some simpler models: (a) Use the outflow rates given in the figure to determine the time   it would take to “drain” each lake. This gives a lower bound on how   long it would take to remove all the pollutants. (b) A better estimate is obtained by assuming that each lake is a   separate tank with only clean water flowing in. Use this approach to   determine how long it would take the pollution level in each lake to   be reduced to 50% of its original level. How long would it take to   reduce the pollution to 5% of its original level? (c) Finally, to take into account the fact that pollution from one   lake flows into the next lake in the chain, use the entire multiple   compartment model given in the figure  to determine when the   pollution level in each lake has been reduced to 50% of its original   level, assuming pollution has ceased (that is, inflows not from a   lake are clean water). Assume that all the lakes initially have the same pollution concentration $p$. How long would it take for the pollution to be reduced to 5% of its original level? Solution so far: $\frac{dA}{dt}$ = rate in-rate out where $A$ is the amount of pollution at time $t$. First I wrote equations for each lake. Rather than using $A$ as my variable, I used the first letter of each lake (with $n$ for Ontario) to stand for the amount of pollution in the given lake at time $t$. This gives... $$\frac{ds}{dt} = \frac{-15s}{2900} $$ $$\frac{dm}{dt} = \frac{-38m}{1180} $$ $$\frac{dh}{dt} = \frac{15s}{2900} + \frac{38m}{1180} - \frac{68h}{850} $$ $$\frac{de}{dt} = \frac{68h}{850} - \frac{85e}{116} $$ $$\frac{dn}{dt} = \frac{85e}{116} - \frac{99n}{393} $$ Rearranging and pulling out the differential operator leads to the following system: $$\left(D + \frac{38}{1180}\right)[m] = 0 $$ $$\left(D - \frac{68}{850}\right)[h] + \frac{38m}{1180} - \frac{15s}{2900}=0 $$ $$\left(D + \frac{15}{2900}\right)[s] = 0 $$ $$\left(D + \frac{85}{116}\right)[e] - \frac{68h}{850} = 0 $$ $$\left(D + \frac{99}{393}\right)[n] - \frac{85e}{116} = 0 $$ This is where I'm stuck. I have this system of five equations with five variables. It seems like it should be fairly straightforward to solve from here, but I can't figure out what to do next.","Problem Details The idea of the problem is to find out how long it would take to flush the Great Lakes of pollution. They're set up as a series of five tanks and you are given inflow rates of clean water, inflow rates from the other tanks, and outflow rates. For our model, we make the following assumptions: The volume of each lake remains constant. The flow rates are constant throughout the year. When a liquid enters the lake, perfect mixing occurs and the pollutants are uniformly distributed. Pollutants are dissolved in the water and enter or leave by inflow or outflow of solution. Before using this model to obtain estimates on the cleanup times for the lakes, we consider some simpler models: (a) Use the outflow rates given in the figure to determine the time   it would take to “drain” each lake. This gives a lower bound on how   long it would take to remove all the pollutants. (b) A better estimate is obtained by assuming that each lake is a   separate tank with only clean water flowing in. Use this approach to   determine how long it would take the pollution level in each lake to   be reduced to 50% of its original level. How long would it take to   reduce the pollution to 5% of its original level? (c) Finally, to take into account the fact that pollution from one   lake flows into the next lake in the chain, use the entire multiple   compartment model given in the figure  to determine when the   pollution level in each lake has been reduced to 50% of its original   level, assuming pollution has ceased (that is, inflows not from a   lake are clean water). Assume that all the lakes initially have the same pollution concentration $p$. How long would it take for the pollution to be reduced to 5% of its original level? Solution so far: $\frac{dA}{dt}$ = rate in-rate out where $A$ is the amount of pollution at time $t$. First I wrote equations for each lake. Rather than using $A$ as my variable, I used the first letter of each lake (with $n$ for Ontario) to stand for the amount of pollution in the given lake at time $t$. This gives... $$\frac{ds}{dt} = \frac{-15s}{2900} $$ $$\frac{dm}{dt} = \frac{-38m}{1180} $$ $$\frac{dh}{dt} = \frac{15s}{2900} + \frac{38m}{1180} - \frac{68h}{850} $$ $$\frac{de}{dt} = \frac{68h}{850} - \frac{85e}{116} $$ $$\frac{dn}{dt} = \frac{85e}{116} - \frac{99n}{393} $$ Rearranging and pulling out the differential operator leads to the following system: $$\left(D + \frac{38}{1180}\right)[m] = 0 $$ $$\left(D - \frac{68}{850}\right)[h] + \frac{38m}{1180} - \frac{15s}{2900}=0 $$ $$\left(D + \frac{15}{2900}\right)[s] = 0 $$ $$\left(D + \frac{85}{116}\right)[e] - \frac{68h}{850} = 0 $$ $$\left(D + \frac{99}{393}\right)[n] - \frac{85e}{116} = 0 $$ This is where I'm stuck. I have this system of five equations with five variables. It seems like it should be fairly straightforward to solve from here, but I can't figure out what to do next.",,"['ordinary-differential-equations', 'mathematical-modeling']"
29,$u''(t)+u(t) = |t|$,,u''(t)+u(t) = |t|,"Solve the Cauchy problem, $\forall t \in \mathbb{R}$, $$ \begin{cases} u''(t) + u(t) = |t|\\ u(0)=1, \quad u'(0) = -1 \end{cases} $$ The solution to the homogeneous equation is $A\cos(t) + B \sin(t)$. Empirically, $|t|$ is ""more or less"" a particular solution, however it is not differentiable in $0$... What is the fastest way to find a particular solution two times differentiable?","Solve the Cauchy problem, $\forall t \in \mathbb{R}$, $$ \begin{cases} u''(t) + u(t) = |t|\\ u(0)=1, \quad u'(0) = -1 \end{cases} $$ The solution to the homogeneous equation is $A\cos(t) + B \sin(t)$. Empirically, $|t|$ is ""more or less"" a particular solution, however it is not differentiable in $0$... What is the fastest way to find a particular solution two times differentiable?",,"['calculus', 'real-analysis', 'ordinary-differential-equations']"
30,Use Euler's method to approximate $\int^2_0 e^{-u^2}du$,Use Euler's method to approximate,\int^2_0 e^{-u^2}du,"We learned Euler's method today there is one hw problem totally stunned my hat off. It says: Use Euler's method to approximate $\int^2_0 e^{-u^2}du$. I know Euler's method is $y_{n+1} = y_n + hf(t_n,y_n)$, but this is for $y'$ right? somehow I can compute an integral? This is supposed to be done using computer, I know how to use Euler's method to solve equation like $y' = 2y - 3e^{-t}$. Thanks for viewing!","We learned Euler's method today there is one hw problem totally stunned my hat off. It says: Use Euler's method to approximate $\int^2_0 e^{-u^2}du$. I know Euler's method is $y_{n+1} = y_n + hf(t_n,y_n)$, but this is for $y'$ right? somehow I can compute an integral? This is supposed to be done using computer, I know how to use Euler's method to solve equation like $y' = 2y - 3e^{-t}$. Thanks for viewing!",,['ordinary-differential-equations']
31,Finding fundamental set of solution of higher order differential equation,Finding fundamental set of solution of higher order differential equation,,"The fundamental set of solution $$y^{(4)} - 16y = 0$$ I worked this problem out but I was under the impression that I can apply the general method of characteristic equation to solve : $$ r^4 - 16 = 0$$ $r = \pm 2 $ so my solutions are $e^{2t} $ and $e^{-2t}$ and then I can use the Wronskian to check if it is fundamental or not. If $$W(e^{2t}, e^{-2t}) \neq 0 $$ then it is fundamental (and also linearly dependent). This is true. However, the book's solution to this problem is $$e^{2t}, e^{-2t}, cos(2t),sin(2t)$$ But I don't understand why there could be sinusoidal functions in the set of fundamental solutions since the gen. solution to the problem has no imaginary part.","The fundamental set of solution $$y^{(4)} - 16y = 0$$ I worked this problem out but I was under the impression that I can apply the general method of characteristic equation to solve : $$ r^4 - 16 = 0$$ $r = \pm 2 $ so my solutions are $e^{2t} $ and $e^{-2t}$ and then I can use the Wronskian to check if it is fundamental or not. If $$W(e^{2t}, e^{-2t}) \neq 0 $$ then it is fundamental (and also linearly dependent). This is true. However, the book's solution to this problem is $$e^{2t}, e^{-2t}, cos(2t),sin(2t)$$ But I don't understand why there could be sinusoidal functions in the set of fundamental solutions since the gen. solution to the problem has no imaginary part.",,['ordinary-differential-equations']
32,How can the Laplace transform be used to solve piecewise functions?,How can the Laplace transform be used to solve piecewise functions?,,"For example, suppose we have the following two problems that we'd like to find the Laplace transform of: $f(t) = \begin{cases} 1, & t \lt 2 \\ 0, & t \geq 2 \end{cases}$ $f(t) = \begin{cases} 1-0.5t, & t \lt 2 \\ 5, & t \geq 2 \end{cases}$ Is there a general method to solve these sorts of problems?  How can I approach them and make solving them more intuitive? Thanks for the help!","For example, suppose we have the following two problems that we'd like to find the Laplace transform of: $f(t) = \begin{cases} 1, & t \lt 2 \\ 0, & t \geq 2 \end{cases}$ $f(t) = \begin{cases} 1-0.5t, & t \lt 2 \\ 5, & t \geq 2 \end{cases}$ Is there a general method to solve these sorts of problems?  How can I approach them and make solving them more intuitive? Thanks for the help!",,['ordinary-differential-equations']
33,Physical meaning behind Frequency domain?,Physical meaning behind Frequency domain?,,I understand its usage and why is it important because It transforms differential equations to algebraic ones..  But I can't get the physical meaning of the new form of the equation and the meaning of this transformation.. and also what does it mean to change the domain of the function ?,I understand its usage and why is it important because It transforms differential equations to algebraic ones..  But I can't get the physical meaning of the new form of the equation and the meaning of this transformation.. and also what does it mean to change the domain of the function ?,,"['ordinary-differential-equations', 'functions', 'laplace-transform']"
34,Theory of the Mathieu Operator,Theory of the Mathieu Operator,,How important is the theory of the Mathieu operator in mathematics/applied mathematics? What are the major mathematical concepts required to study it? The Mathieu operator is an ordinary periodic differential operator of the form $$\frac{d^2y}{dx^2} + (a-2q \cos{2x})y =0.$$,How important is the theory of the Mathieu operator in mathematics/applied mathematics? What are the major mathematical concepts required to study it? The Mathieu operator is an ordinary periodic differential operator of the form $$\frac{d^2y}{dx^2} + (a-2q \cos{2x})y =0.$$,,"['analysis', 'reference-request', 'ordinary-differential-equations', 'special-functions', 'fluid-dynamics']"
35,particular solution of $y''+y'=xe^{-x}$,particular solution of,y''+y'=xe^{-x},"I'm using the method of undetermined coefficients to find a particular solution of: $$y''+y'=xe^{-x}$$ Ostensibly, it seems that $y_p$ should take the form of $(Ax + B)e^{-x}$ At least that's the form that I think I've been taught. Problem is that it just doesn't work out for me. I get a value for A, but not for B... Am I choosing an incorrect yp form?","I'm using the method of undetermined coefficients to find a particular solution of: $$y''+y'=xe^{-x}$$ Ostensibly, it seems that $y_p$ should take the form of $(Ax + B)e^{-x}$ At least that's the form that I think I've been taught. Problem is that it just doesn't work out for me. I get a value for A, but not for B... Am I choosing an incorrect yp form?",,['ordinary-differential-equations']
36,Find solution for $\frac{dx}{dt}=|x|$,Find solution for,\frac{dx}{dt}=|x|,My solution: $x>0$ $$\begin{align}\frac{\mathrm dx}{\mathrm dt}=x &\Rightarrow \frac{\mathrm dx}{x}=\mathrm dt \\ &\Rightarrow \int \frac{\mathrm dx}{x} = \int\mathrm dt \\ &\Rightarrow \ln(x)= t + C_{0} \\ &\Rightarrow e^{\ln(x)}=e^{t+C_0} \\ &\Rightarrow x=C_1e^t.\end{align}$$ $x<0$ $$\begin{align} \frac{\mathrm dx}{\mathrm dt}=-x &\Rightarrow -\frac{\mathrm dx}{x}=\mathrm dt \\ &\Rightarrow -\int \frac{\mathrm dx}{x} = \int\mathrm dt \\ &\Rightarrow \ln(x)= -t - C_{2} \\ &\Rightarrow e^{\ln x} =e^{-t-C_2} \\ &\Rightarrow x=C_3e^{-t}. \end{align}$$ $x=0$ $$\begin{align}\frac{\mathrm dx}{\mathrm dt}=0 &\Rightarrow \int \frac{\mathrm dx}{\mathrm dt} = \int 0 \\ &\Rightarrow \int\mathrm dx= \int 0\mathrm dt \\ &\Rightarrow x=C_4.\end{align}$$ I think that 1. and 2. are ok but I'm not sure 3. is correct.,My solution: I think that 1. and 2. are ok but I'm not sure 3. is correct.,x>0 \begin{align}\frac{\mathrm dx}{\mathrm dt}=x &\Rightarrow \frac{\mathrm dx}{x}=\mathrm dt \\ &\Rightarrow \int \frac{\mathrm dx}{x} = \int\mathrm dt \\ &\Rightarrow \ln(x)= t + C_{0} \\ &\Rightarrow e^{\ln(x)}=e^{t+C_0} \\ &\Rightarrow x=C_1e^t.\end{align} x<0 \begin{align} \frac{\mathrm dx}{\mathrm dt}=-x &\Rightarrow -\frac{\mathrm dx}{x}=\mathrm dt \\ &\Rightarrow -\int \frac{\mathrm dx}{x} = \int\mathrm dt \\ &\Rightarrow \ln(x)= -t - C_{2} \\ &\Rightarrow e^{\ln x} =e^{-t-C_2} \\ &\Rightarrow x=C_3e^{-t}. \end{align} x=0 \begin{align}\frac{\mathrm dx}{\mathrm dt}=0 &\Rightarrow \int \frac{\mathrm dx}{\mathrm dt} = \int 0 \\ &\Rightarrow \int\mathrm dx= \int 0\mathrm dt \\ &\Rightarrow x=C_4.\end{align},['ordinary-differential-equations']
37,What is the solution to this non-linear second order differential equation?,What is the solution to this non-linear second order differential equation?,,"I'm trying to solve the following non-linear second order differential equation: $$\tag{1} \frac{d\, }{dx} \Bigl( \frac{1}{y^2} \, \frac{dy}{dx} \Bigr) = -\, \frac{2}{y^3}, $$ where $y(x)$ is an unknown function on the real axis.  I already know the ""trivial"" solution $y(x) = y_0 \pm x$ .  The solution I'm looking may involve trigonometric functions, but I'm not sure.  Take note that we may pose $$\tag{2} u = \frac{1}{y}, $$ so that (1) takes another form: $$\tag{3} \frac{d^2 u}{dx^2} - 2 \, u^3 = 0. $$ So what is the non-linear solution $y(x)$ ?  Or $u(x)$ ?","I'm trying to solve the following non-linear second order differential equation: where is an unknown function on the real axis.  I already know the ""trivial"" solution .  The solution I'm looking may involve trigonometric functions, but I'm not sure.  Take note that we may pose so that (1) takes another form: So what is the non-linear solution ?  Or ?","\tag{1}
\frac{d\, }{dx} \Bigl( \frac{1}{y^2} \, \frac{dy}{dx} \Bigr) = -\, \frac{2}{y^3},
 y(x) y(x) = y_0 \pm x \tag{2}
u = \frac{1}{y},
 \tag{3}
\frac{d^2 u}{dx^2} - 2 \, u^3 = 0.
 y(x) u(x)","['calculus', 'ordinary-differential-equations', 'functions', 'fundamental-solution']"
38,What is it to solve an equation forward?,What is it to solve an equation forward?,,"I'm reading a book in Monetary Economics and I don't understand a step. I have this expression: $$ \dfrac{\lambda_{t}}{P_{t}} = \beta \left( \dfrac{\lambda_{t+1} + \mu_{t+1}}{P_{t+1}} \right) $$ And then it says ""solving this equation forward implies that"": $$ \dfrac{\lambda_{t}}{P_{t}} = \sum_{i=1}^{\infty} \beta^{i} \left( \dfrac{\mu_{t+i}}{P_{t+i}} \right) $$ I dont't know what they're doing. What is it to solve an equation forward?","I'm reading a book in Monetary Economics and I don't understand a step. I have this expression: And then it says ""solving this equation forward implies that"": I dont't know what they're doing. What is it to solve an equation forward?", \dfrac{\lambda_{t}}{P_{t}} = \beta \left( \dfrac{\lambda_{t+1} + \mu_{t+1}}{P_{t+1}} \right)   \dfrac{\lambda_{t}}{P_{t}} = \sum_{i=1}^{\infty} \beta^{i} \left( \dfrac{\mu_{t+i}}{P_{t+i}} \right) ,"['ordinary-differential-equations', 'recurrence-relations', 'problem-solving', 'finance', 'economics']"
39,Solving linear differential equation with variable coefficients.,Solving linear differential equation with variable coefficients.,,"I have faced this task, when preparing to my university's ""mathematical methods"" exam, and I do not really understand how to solve it. The task content is as follows: Given : $$(1-x^2)\cdot y''_{xx} -x\cdot y'_x +y = \frac{\sqrt{1-x^2}}{x}$$ Solve the equation by substituion of the independent variable: $x(t) = \cos(t), x\in(0,1).$ My attempt : I have performed the suggested substitution: $(1-\cos^{2}(t))\cdot \large\frac{\partial^2y}{\partial x(t)^2}\normalsize-\cos(t)\cdot\large\frac{\partial y}{\partial x(t)} \normalsize+ y = \large\frac{\sqrt{1-\cos^2(t)}}{cos(t)}.$ $\sin^2(t)\cdot \large\frac{\partial^2y}{\partial x(t)^2}\normalsize-\cos(t)\cdot\large\frac{\partial y / \partial t}{\partial x / \partial t} \normalsize+ y = \tan(t).$ $\sin^2(t)\cdot\frac{\large\partial\left(\frac{\dot{y}}{-\sin(t)}\right) / \partial t}{\large\partial x / \partial t} + \large\frac{\cos(t)}{\sin(t)}\normalsize\dot{y} + y = \tan(t).$ $-\ddot{y} + 2\dot{y}\cot(t) +y = \tan(t).$ That is now I have to solve: $-\ddot{y} + 2\dot{y}\cot(t) +y = \tan(t).$ And that is the point, where I am stuck, I have no idea how to proceed from here, and I did not manage to find the way to solve such a linear D.E. in my course book. I would appreciate any help, hints or book references, thank you in advance! Post Scriptum : As correctly mentioned by @Aryadeva, I have made a mistake, when calculating $\large\frac{\partial^2 y}{\partial x^2}$ , I simply lost ""minus"". That is, after recalculating I had to solve: $$\ddot{y} + y = \tan(t), \text{ s.t. } x\in(0,1) \text{ and } x = \cos(t) \Leftrightarrow \forall x\in \mathbb{D}_x: \text{ }t = \arccos{x}.\tag{1}$$ My solution has involved appying vatiation of parameters method: $\underline{\text{Step I}}$ : find characteristic polynomial over $y(t)$ to get a general solution $(\large y_{\small G})$ for $(1)$ : $\lambda^2 + 1 = 0. \Rightarrow \lambda = \pm i.$ $\begin{bmatrix} \lambda_1\\ \lambda_2\\ \end{bmatrix} = \begin{bmatrix} i\\ -i\\ \end{bmatrix} \Rightarrow \large y_{\small G} \normalsize = C_1 e^{it}+C_2 e^{-it} = \dot{\tilde {C}}_1\cos(t) + \dot{\tilde{C}}_2 \sin(t).$ $\underline{\text{Step II}}$ : Finding particular solution $(\large y_{\small P})$ for $(1)$ , using Wronskian matrices: I would omit my calculations here, would just note that for the given D.E.: $\mathbb{W} = \begin{pmatrix}\cos(t) & \sin(t) \\ -\sin(t) & \cos(t) \end{pmatrix}, \mathbb{W}_{\small \Delta} = \begin{vmatrix}\cos(t) & \sin(t) \\ -\sin(t) & \cos(t) \end{vmatrix} = 1.$ $\mathbb{W}_{\small \Delta,\normalsize 1} = \begin{vmatrix}0 & \sin(t) \\ 1 & \cos(t) \end{vmatrix} = -\sin(t), \mathbb{W}_{\small \Delta,\normalsize 2} = \begin{vmatrix}\cos(t) & 0 \\ -\sin(t) & 1 \end{vmatrix} = \cos(t).$ Thus: $$\dot{\tilde{C}}_1(t) = \int\frac{\tan(t)\cdot \mathbb{W}_{\small \Delta,\normalsize 1}}{\mathbb{W}_{\small \Delta}} \partial t = \sin(t) +\frac{1}{2}\ln\left(\frac{\sin(t)-1}{\sin(t)+1}\right) +\bar{C}_1,$$ $$\dot{\tilde{C}}_2(t) = \int\frac{\tan(t)\cdot \mathbb{W}_{\small \Delta,\normalsize 2}}{\mathbb{W}_{\small \Delta}} \partial t = -\cos(t) + \bar{C}_2.$$ Thereof: $$\large y_{\small P} \normalsize = \left(\sin(t)+\frac{1}{2}\ln\left(\frac{\sin(t)-1}{\sin(t)+1}\right) +\bar{C}_1\right)\cos(t) +\left(-\cos(t) +\bar{C}_2\right)\sin(t).$$ $\underline{\text{Step III}}$ : Now it just suffices to write down the whole solution and transfer from $t$ to $x$ : $\large y \normalsize (t) = \large y_{\small G} + y_{\small P} \normalsize = \left(\sin(t)+\frac{1}{2}\ln\left(\frac{\sin(t)-1}{\sin(t)+1}\right) +\tilde{C}_1\right)\cos(t) +\left(-\cos(t) +\tilde{C}_2\right)\sin(t).$ Now, as $x \in (0, 1): \cos(\arccos{x}) = x, \text{ }\sin(\arccos{x}) = \sqrt{1-x^2}:$ $$\text{Answer: } \begin{array}{|c|} \hline y(x) = \left(\sqrt{1-x^2} + \frac{1}{2}\ln\left(\frac{\sqrt{1-x^2} - 1}{\sqrt{1-x^2} +1}\right)+\tilde{C}_1 \right)x +\left(-x+\tilde{C}_2\right)\sqrt{1-x^2}.\\ \hline \end{array}$$","I have faced this task, when preparing to my university's ""mathematical methods"" exam, and I do not really understand how to solve it. The task content is as follows: Given : Solve the equation by substituion of the independent variable: My attempt : I have performed the suggested substitution: That is now I have to solve: And that is the point, where I am stuck, I have no idea how to proceed from here, and I did not manage to find the way to solve such a linear D.E. in my course book. I would appreciate any help, hints or book references, thank you in advance! Post Scriptum : As correctly mentioned by @Aryadeva, I have made a mistake, when calculating , I simply lost ""minus"". That is, after recalculating I had to solve: My solution has involved appying vatiation of parameters method: : find characteristic polynomial over to get a general solution for : : Finding particular solution for , using Wronskian matrices: I would omit my calculations here, would just note that for the given D.E.: Thus: Thereof: : Now it just suffices to write down the whole solution and transfer from to : Now, as","(1-x^2)\cdot y''_{xx} -x\cdot y'_x +y = \frac{\sqrt{1-x^2}}{x} x(t) = \cos(t), x\in(0,1). (1-\cos^{2}(t))\cdot \large\frac{\partial^2y}{\partial x(t)^2}\normalsize-\cos(t)\cdot\large\frac{\partial y}{\partial x(t)} \normalsize+ y = \large\frac{\sqrt{1-\cos^2(t)}}{cos(t)}. \sin^2(t)\cdot \large\frac{\partial^2y}{\partial x(t)^2}\normalsize-\cos(t)\cdot\large\frac{\partial y / \partial t}{\partial x / \partial t} \normalsize+ y = \tan(t). \sin^2(t)\cdot\frac{\large\partial\left(\frac{\dot{y}}{-\sin(t)}\right) / \partial t}{\large\partial x / \partial t} + \large\frac{\cos(t)}{\sin(t)}\normalsize\dot{y} + y = \tan(t). -\ddot{y} + 2\dot{y}\cot(t) +y = \tan(t). -\ddot{y} + 2\dot{y}\cot(t) +y = \tan(t). \large\frac{\partial^2 y}{\partial x^2} \ddot{y} + y = \tan(t), \text{ s.t. } x\in(0,1) \text{ and } x = \cos(t) \Leftrightarrow \forall x\in \mathbb{D}_x: \text{ }t = \arccos{x}.\tag{1} \underline{\text{Step I}} y(t) (\large y_{\small G}) (1) \lambda^2 + 1 = 0. \Rightarrow \lambda = \pm i. \begin{bmatrix}
\lambda_1\\
\lambda_2\\
\end{bmatrix} = \begin{bmatrix}
i\\
-i\\
\end{bmatrix} \Rightarrow \large y_{\small G} \normalsize = C_1 e^{it}+C_2 e^{-it} = \dot{\tilde {C}}_1\cos(t) + \dot{\tilde{C}}_2 \sin(t). \underline{\text{Step II}} (\large y_{\small P}) (1) \mathbb{W} = \begin{pmatrix}\cos(t) & \sin(t) \\ -\sin(t) & \cos(t) \end{pmatrix}, \mathbb{W}_{\small \Delta} = \begin{vmatrix}\cos(t) & \sin(t) \\ -\sin(t) & \cos(t) \end{vmatrix} = 1. \mathbb{W}_{\small \Delta,\normalsize 1} = \begin{vmatrix}0 & \sin(t) \\ 1 & \cos(t) \end{vmatrix} = -\sin(t), \mathbb{W}_{\small \Delta,\normalsize 2} = \begin{vmatrix}\cos(t) & 0 \\ -\sin(t) & 1 \end{vmatrix} = \cos(t). \dot{\tilde{C}}_1(t) = \int\frac{\tan(t)\cdot \mathbb{W}_{\small \Delta,\normalsize 1}}{\mathbb{W}_{\small \Delta}} \partial t = \sin(t) +\frac{1}{2}\ln\left(\frac{\sin(t)-1}{\sin(t)+1}\right) +\bar{C}_1, \dot{\tilde{C}}_2(t) = \int\frac{\tan(t)\cdot \mathbb{W}_{\small \Delta,\normalsize 2}}{\mathbb{W}_{\small \Delta}} \partial t = -\cos(t) + \bar{C}_2. \large y_{\small P} \normalsize = \left(\sin(t)+\frac{1}{2}\ln\left(\frac{\sin(t)-1}{\sin(t)+1}\right) +\bar{C}_1\right)\cos(t) +\left(-\cos(t) +\bar{C}_2\right)\sin(t). \underline{\text{Step III}} t x \large y \normalsize (t) = \large y_{\small G} + y_{\small P} \normalsize = \left(\sin(t)+\frac{1}{2}\ln\left(\frac{\sin(t)-1}{\sin(t)+1}\right) +\tilde{C}_1\right)\cos(t) +\left(-\cos(t) +\tilde{C}_2\right)\sin(t). x \in (0, 1): \cos(\arccos{x}) = x, \text{ }\sin(\arccos{x}) = \sqrt{1-x^2}: \text{Answer: } \begin{array}{|c|}
\hline
y(x) = \left(\sqrt{1-x^2} + \frac{1}{2}\ln\left(\frac{\sqrt{1-x^2} - 1}{\sqrt{1-x^2} +1}\right)+\tilde{C}_1 \right)x +\left(-x+\tilde{C}_2\right)\sqrt{1-x^2}.\\
\hline
\end{array}","['ordinary-differential-equations', 'book-recommendation', 'substitution']"
40,How did we discover the method of solving the linear 1st order ODE?,How did we discover the method of solving the linear 1st order ODE?,,"I have the first order linear ode in the standard form which I'd like to solve: $ \frac{d y}{d x}+P(x) y=f(x) $ The solution in the book I'm reading (A First Course in Differential Equations with Modeling Applications by Dennis Zill 10th edition) is as follow: The method for solving the differential equation above hinges on a   remarkable fact that the left hand side of the equation can be recast   into the form of the exact derivative of a product by multiplying both   sides of the equation by a special function $\mu(x)$ . It is relatively   easy to find the function $\mu(x)$ because we want: $ \frac{d}{d x}[\mu(x) y]=\mu \frac{d y}{d x}+\frac{d \mu}{d x} y=\mu\frac{d y}{d x}+\mu P y $ The equality is true provided that: $\frac{d \mu}{d x}=\mu P$ The last equation can be solved by separation of variables.   Integrating $\frac{d \mu}{\mu}=P d x \quad \text { and solving } \quad \ln|\mu(x)|=\int P(x) d x+c_{1}$ gives $\mu(x)=c_{2} e^{\int P(x) d x}$ . Even though there are an   infinite choices of $\mu(x)$ (all constant multiples of $e^{\int P(x) d x}$ ), all produce the same desired result. Hence we can simplify   life and choose $c_{2}=1$ . The function $\mu(x)=e^{\int P(x) d x}$ is called an integrating   factor for the differential equation. Here is what we have so far: We multiplied both sides of the DE by the   integrating factor and, by construction, the left-hand side is the   derivative of a product of the integrating factor and y: $\begin{aligned} e^{\int P(x) d x} \frac{d y}{d x}+P(x) e^{\int P(x) d x} y &=e^{\int P(x) d x} f(x) \\ \frac{d}{d x}\left[e^{\int P(x) d x} y\right] &=e^{\int P(x) d x} f(x) \end{aligned}$ Finally, we discover why $\mu(x)$ is called an integrating factor. We   can integrate both sides of the last equation, $e^{\int P(x) d x} y=\int e^{\int P(x) d x} f(x)+c$ and solve for y. The result is a one-parameter family of solutions of   the DE: $y=e^{-\int P(x) d x} \int e^{\int P(x) d x} f(x) d x+c e^{-\int P(x) d x}$ I have a few points that I couldn't understand in this method: ""The method for solving the differential equation above hinges on a remarkable fact that the left hand side of the equation can be recast into the form of the exact derivative of a product by multiplying both sides of the equation by a special function $\mu(x)$ "". Where did this fact come from and why? Was this method done by construction? It wasn't clear to me how could this method be discovered for the first time, like is it intuitive? What should you have in mind in order to do the construction done in this proof? i.e. what was the motivation to multiply the DE by $\mu(x)$ ? In summary, I did not understand the first few steps of this proof and why we chose to do it that way. In practice, while solving linear DE using this method we tend to multiply the whole equation by $\mu(x)$ and then set the left hand side of the DE equal to $ \frac{d}{d x}[\mu(x) y]$ ,  and then find the solution. So I want to know why we do that procedure in the first place. P.S: It would be nice if you can give me a little bit of history about who developed this method and when did he do it.","I have the first order linear ode in the standard form which I'd like to solve: The solution in the book I'm reading (A First Course in Differential Equations with Modeling Applications by Dennis Zill 10th edition) is as follow: The method for solving the differential equation above hinges on a   remarkable fact that the left hand side of the equation can be recast   into the form of the exact derivative of a product by multiplying both   sides of the equation by a special function . It is relatively   easy to find the function because we want: The equality is true provided that: The last equation can be solved by separation of variables.   Integrating gives . Even though there are an   infinite choices of (all constant multiples of ), all produce the same desired result. Hence we can simplify   life and choose . The function is called an integrating   factor for the differential equation. Here is what we have so far: We multiplied both sides of the DE by the   integrating factor and, by construction, the left-hand side is the   derivative of a product of the integrating factor and y: Finally, we discover why is called an integrating factor. We   can integrate both sides of the last equation, and solve for y. The result is a one-parameter family of solutions of   the DE: I have a few points that I couldn't understand in this method: ""The method for solving the differential equation above hinges on a remarkable fact that the left hand side of the equation can be recast into the form of the exact derivative of a product by multiplying both sides of the equation by a special function "". Where did this fact come from and why? Was this method done by construction? It wasn't clear to me how could this method be discovered for the first time, like is it intuitive? What should you have in mind in order to do the construction done in this proof? i.e. what was the motivation to multiply the DE by ? In summary, I did not understand the first few steps of this proof and why we chose to do it that way. In practice, while solving linear DE using this method we tend to multiply the whole equation by and then set the left hand side of the DE equal to ,  and then find the solution. So I want to know why we do that procedure in the first place. P.S: It would be nice if you can give me a little bit of history about who developed this method and when did he do it.", \frac{d y}{d x}+P(x) y=f(x)  \mu(x) \mu(x)  \frac{d}{d x}[\mu(x) y]=\mu \frac{d y}{d x}+\frac{d \mu}{d x} y=\mu\frac{d y}{d x}+\mu P y  \frac{d \mu}{d x}=\mu P \frac{d \mu}{\mu}=P d x \quad \text { and solving } \quad \ln|\mu(x)|=\int P(x) d x+c_{1} \mu(x)=c_{2} e^{\int P(x) d x} \mu(x) e^{\int P(x) d x} c_{2}=1 \mu(x)=e^{\int P(x) d x} \begin{aligned} e^{\int P(x) d x} \frac{d y}{d x}+P(x) e^{\int P(x) d x} y &=e^{\int P(x) d x} f(x) \\ \frac{d}{d x}\left[e^{\int P(x) d x} y\right] &=e^{\int P(x) d x} f(x) \end{aligned} \mu(x) e^{\int P(x) d x} y=\int e^{\int P(x) d x} f(x)+c y=e^{-\int P(x) d x} \int e^{\int P(x) d x} f(x) d x+c e^{-\int P(x) d x} \mu(x) \mu(x) \mu(x)  \frac{d}{d x}[\mu(x) y],"['ordinary-differential-equations', 'proof-verification']"
41,What are equations such as $dz = dx + dy$ called?,What are equations such as  called?,dz = dx + dy,"These look like differential equations: $$dz = dx + dy,$$ $$dU = TdS - PdV,$$ and many other equations in physics, but they aren't ordinary differential equations or partial differential equations . What are they then?","These look like differential equations: and many other equations in physics, but they aren't ordinary differential equations or partial differential equations . What are they then?","dz = dx + dy, dU = TdS - PdV,",['ordinary-differential-equations']
42,Particular solution of second order differential equation,Particular solution of second order differential equation,,"Given the ode: $$ y''-2y'+y=e^t, $$ how can I find the form of the particular solution? At first, I tried the form $y=Ae^t$ but $$ \begin{split} &\frac{d^2y}{dt^2}Ae^t=\frac{dy}{dt}Ae^t=Ae^t\\ \\ &Ae^t-2Ae^t+Ae^t=e^t\\ \\ &0=e^t\\ \end{split}. $$ So this doesn't work. I also tried the form $y=Ate^t$ , but again $$ \begin{split} &\frac{d^2y}{dt^2}=A(2e^t+te^t)\\ \\ &\frac{dy}{dt}=A(e^t+te^t)\\ \\ &A(2e^t+te^t)-2A(e^t+te^t)+Ate^t=e^t\\ \\ &2A+At-2A-2At+At=1\\ \end{split}. $$ and again this doesn't work Generally, what is the best way to guess the form of the solution?","Given the ode: how can I find the form of the particular solution? At first, I tried the form but So this doesn't work. I also tried the form , but again and again this doesn't work Generally, what is the best way to guess the form of the solution?","
y''-2y'+y=e^t,
 y=Ae^t 
\begin{split}
&\frac{d^2y}{dt^2}Ae^t=\frac{dy}{dt}Ae^t=Ae^t\\
\\
&Ae^t-2Ae^t+Ae^t=e^t\\
\\
&0=e^t\\
\end{split}.
 y=Ate^t 
\begin{split}
&\frac{d^2y}{dt^2}=A(2e^t+te^t)\\
\\
&\frac{dy}{dt}=A(e^t+te^t)\\
\\
&A(2e^t+te^t)-2A(e^t+te^t)+Ate^t=e^t\\
\\
&2A+At-2A-2At+At=1\\
\end{split}.
",['ordinary-differential-equations']
43,Finding the Beltrami-Klein Geodesics,Finding the Beltrami-Klein Geodesics,,"I've been experimenting with the Beltrami-Klein model of hyperbolic geometry. According to Wikipedia, in this model the geodesics are given by Euclidean line segments, and I want to prove this. Let $x^{\alpha}$ denote a geodesic with parameter $t$, then the geodesic is given by parallel transporting the tangent along itself, i.e.; $$\frac{d^2x^{\alpha}}{dt^2}+\Gamma^{\alpha}_{\ \beta \gamma}\frac{dx^{\beta}}{dt}\frac{dx^{\gamma}}{dt}=0$$ Where $\Gamma^{\alpha}_{\ \beta \gamma}$ are the Christoffel symbols. For this model we have that: \begin{align*} &\Gamma^{1}_{\ 11}=\frac{2x}{1-x^2-y^2}\\ &\Gamma^{1}_{\ 12}=\frac{y}{1-x^2-y^2}\\ &\Gamma^{2}_{\ 12}=\frac{x}{1-x^2-y^2}\\ &\Gamma^{2}_{\ 22}=\frac{2y}{1-x^2-y^2}\\ \end{align*} With $\Gamma^{\alpha}_{\ \beta \gamma}$ symmetric on the lower indices and all other symbols zero. This yields 2 second order ODES for the components of the geodesic: $$\frac{d^2x}{dt^2}+\frac{2}{1-x^2-y^2}\left(x\left(\frac{dx}{dt}\right)^2+y\frac{dx}{dt}\frac{dy}{dt}\right)=0$$ $$\frac{d^2y}{dt^2}+\frac{2}{1-x^2-y^2}\left(y\left(\frac{dy}{dt}\right)^2+x\frac{dy}{dt}\frac{dx}{dt}\right)=0$$ I have no idea how to solve these equations and any help yould be much appreciated. I tried a simple case however: Consider the geodesic joining $(x,y)=(0,0)$ to $(x,y)=(\frac{1}{2},0)$. If this is a Euclidean line as stated on wikipedia, it should be given by $x=t$, $y=0$, for $0\leq t\leq \frac{1}{2}$. Plugging this into the system of ODES yields: $$\frac{2t}{1-t^2}=0$$ $$0=0$$ So clearly this is not a solution! I thought that if Euclidean line segments were geodesics then they would satisfy the geodesic equation. I would really appreciate any help in solving the general ODES for the geodesic joining two points, as well as in understanding why the Euclidean line given above doesn't appear to be a geodesic.","I've been experimenting with the Beltrami-Klein model of hyperbolic geometry. According to Wikipedia, in this model the geodesics are given by Euclidean line segments, and I want to prove this. Let $x^{\alpha}$ denote a geodesic with parameter $t$, then the geodesic is given by parallel transporting the tangent along itself, i.e.; $$\frac{d^2x^{\alpha}}{dt^2}+\Gamma^{\alpha}_{\ \beta \gamma}\frac{dx^{\beta}}{dt}\frac{dx^{\gamma}}{dt}=0$$ Where $\Gamma^{\alpha}_{\ \beta \gamma}$ are the Christoffel symbols. For this model we have that: \begin{align*} &\Gamma^{1}_{\ 11}=\frac{2x}{1-x^2-y^2}\\ &\Gamma^{1}_{\ 12}=\frac{y}{1-x^2-y^2}\\ &\Gamma^{2}_{\ 12}=\frac{x}{1-x^2-y^2}\\ &\Gamma^{2}_{\ 22}=\frac{2y}{1-x^2-y^2}\\ \end{align*} With $\Gamma^{\alpha}_{\ \beta \gamma}$ symmetric on the lower indices and all other symbols zero. This yields 2 second order ODES for the components of the geodesic: $$\frac{d^2x}{dt^2}+\frac{2}{1-x^2-y^2}\left(x\left(\frac{dx}{dt}\right)^2+y\frac{dx}{dt}\frac{dy}{dt}\right)=0$$ $$\frac{d^2y}{dt^2}+\frac{2}{1-x^2-y^2}\left(y\left(\frac{dy}{dt}\right)^2+x\frac{dy}{dt}\frac{dx}{dt}\right)=0$$ I have no idea how to solve these equations and any help yould be much appreciated. I tried a simple case however: Consider the geodesic joining $(x,y)=(0,0)$ to $(x,y)=(\frac{1}{2},0)$. If this is a Euclidean line as stated on wikipedia, it should be given by $x=t$, $y=0$, for $0\leq t\leq \frac{1}{2}$. Plugging this into the system of ODES yields: $$\frac{2t}{1-t^2}=0$$ $$0=0$$ So clearly this is not a solution! I thought that if Euclidean line segments were geodesics then they would satisfy the geodesic equation. I would really appreciate any help in solving the general ODES for the geodesic joining two points, as well as in understanding why the Euclidean line given above doesn't appear to be a geodesic.",,"['ordinary-differential-equations', 'differential-geometry', 'hyperbolic-geometry', 'geodesic']"
44,Solving PDE with Laplace transform,Solving PDE with Laplace transform,,"Please help me to solve the following PDE with Laplace transform. In class we got less than 10 mins to study the topic (the semester ended and we were behind the schedule) and I am totally confused about how to handle/approach the problem. My texts do not show any relevant stuff for the problem. If fact I have only one sample and it confuses me a lot and explains nothing. I can solve ODEs and compute Laplace/inverse Laplace transforms well, so do not bother with it. Just please show me how to reduce the PDE problem to solving ODE and/or computations of (inverse)Laplace transform and I will be able to handle the rest. The problem: Find a (just one) solution of the equation:  $$u_{xx}=u_{tt}, t>0, -\infty <x<+ \infty, u|_{t=0}=3x, u_t|_{t=0}=\sin 2x.$$ Thanks a lot for your help!","Please help me to solve the following PDE with Laplace transform. In class we got less than 10 mins to study the topic (the semester ended and we were behind the schedule) and I am totally confused about how to handle/approach the problem. My texts do not show any relevant stuff for the problem. If fact I have only one sample and it confuses me a lot and explains nothing. I can solve ODEs and compute Laplace/inverse Laplace transforms well, so do not bother with it. Just please show me how to reduce the PDE problem to solving ODE and/or computations of (inverse)Laplace transform and I will be able to handle the rest. The problem: Find a (just one) solution of the equation:  $$u_{xx}=u_{tt}, t>0, -\infty <x<+ \infty, u|_{t=0}=3x, u_t|_{t=0}=\sin 2x.$$ Thanks a lot for your help!",,"['calculus', 'ordinary-differential-equations', 'analysis', 'partial-differential-equations', 'laplace-transform']"
45,"Periods of periodic solutions of the (Hamiltonian) system $\dot{x}=y$, $\dot{y}=-x-x^2$","Periods of periodic solutions of the (Hamiltonian) system ,",\dot{x}=y \dot{y}=-x-x^2,"I'm preparing for a scholarship examination (no solutions available) and in older tests I'm coming across problems like the following. Consider the (Hamiltonian) system   $$\begin{cases}\dot{x}=y \\ \dot{y}=-x-x^2 \end{cases} $$   (a) Find the initial conditions under which the motion is periodic and estimate the set of possible periods; (b) Prove that there exist periodic orbits for which the average value of the position is not $0$ (if $x(t)$ is a periodic orbit of period $T$, the average value is defined as $T^{-1}\int_0^Tx(t)dt$). The approach I've been taught in my undergraduate ODE course is to draw a phase portrait given by the level sets of the Hamiltonian, which gives me full information on the support of the solutions. For instance, I can find the region of the plane which encloses all periodic solutions. However, I've never been taught how to study the graphs of solutions, or how to obtain quantitative results on the periods of periodic solutions. Can anyone give me an idea on how this could be accomplished? EDIT: Maybe an idea would be to study the linearized system about the critical point $(0,0)$, which is $$\begin{cases}\dot{x}=y \\ \dot{y}=-x\end{cases} $$ in this case, the solutions can be explicitly calculated from $x(t)=a\cos t+b\sin t$, $a,b\in \mathbb{R}$, so their period is $2\pi$. Intuitively, I would say that the nonlinear system would also have periods $2\pi$ near the origin, but this still doesn't give me any estimate on the periods on the whole region.","I'm preparing for a scholarship examination (no solutions available) and in older tests I'm coming across problems like the following. Consider the (Hamiltonian) system   $$\begin{cases}\dot{x}=y \\ \dot{y}=-x-x^2 \end{cases} $$   (a) Find the initial conditions under which the motion is periodic and estimate the set of possible periods; (b) Prove that there exist periodic orbits for which the average value of the position is not $0$ (if $x(t)$ is a periodic orbit of period $T$, the average value is defined as $T^{-1}\int_0^Tx(t)dt$). The approach I've been taught in my undergraduate ODE course is to draw a phase portrait given by the level sets of the Hamiltonian, which gives me full information on the support of the solutions. For instance, I can find the region of the plane which encloses all periodic solutions. However, I've never been taught how to study the graphs of solutions, or how to obtain quantitative results on the periods of periodic solutions. Can anyone give me an idea on how this could be accomplished? EDIT: Maybe an idea would be to study the linearized system about the critical point $(0,0)$, which is $$\begin{cases}\dot{x}=y \\ \dot{y}=-x\end{cases} $$ in this case, the solutions can be explicitly calculated from $x(t)=a\cos t+b\sin t$, $a,b\in \mathbb{R}$, so their period is $2\pi$. Intuitively, I would say that the nonlinear system would also have periods $2\pi$ near the origin, but this still doesn't give me any estimate on the periods on the whole region.",,"['ordinary-differential-equations', 'dynamical-systems']"
46,Emden‐Fowler differential equation,Emden‐Fowler differential equation,,"Good day. I am trying to solve the following equation: $$\ddot{y}(x)-\frac{A}{x}\dot{y}(x)+\frac{Bx^2}{2}y(x)=0.$$ WolframAlpha says it is an Emden‐Fowler equation, but I have no idea how to solve this. Can you give me some tips? In case of $$A=B/2=1$$ WolframALpha gives an analytical solution $$y(x)=c_1 sin\frac{x^2}{2}+c_1 cos\frac{x^2}{2}.$$ If there is no analytical way to solve, can I do it numerical? Thank you.","Good day. I am trying to solve the following equation: $$\ddot{y}(x)-\frac{A}{x}\dot{y}(x)+\frac{Bx^2}{2}y(x)=0.$$ WolframAlpha says it is an Emden‐Fowler equation, but I have no idea how to solve this. Can you give me some tips? In case of $$A=B/2=1$$ WolframALpha gives an analytical solution $$y(x)=c_1 sin\frac{x^2}{2}+c_1 cos\frac{x^2}{2}.$$ If there is no analytical way to solve, can I do it numerical? Thank you.",,['ordinary-differential-equations']
47,Asymptotic behaviour / non-linear ODE,Asymptotic behaviour / non-linear ODE,,"I'm trying to find the behaviour at large $t$ of solutions to the non-linear differential equation: $$\dfrac{d^2y}{dt^2}-\left(\dfrac{dy}{dt}\right)^2+(2-a)t\dfrac{dy}{dt}+2ay=0$$ I tried to replicate the approach detailed on Wikipedia , making the assumption that $y(t)\sim e^{S(t)}$ as $t\to\infty$ for some function $S(t)$, but the $\left(\dfrac{dy}{dt}\right)^2$ term in the DE leaves me with an extra factor of $e^{S(t)}$ that I don't know how to deal with. How do I go about finding the asymptotic series for $y(t)$?","I'm trying to find the behaviour at large $t$ of solutions to the non-linear differential equation: $$\dfrac{d^2y}{dt^2}-\left(\dfrac{dy}{dt}\right)^2+(2-a)t\dfrac{dy}{dt}+2ay=0$$ I tried to replicate the approach detailed on Wikipedia , making the assumption that $y(t)\sim e^{S(t)}$ as $t\to\infty$ for some function $S(t)$, but the $\left(\dfrac{dy}{dt}\right)^2$ term in the DE leaves me with an extra factor of $e^{S(t)}$ that I don't know how to deal with. How do I go about finding the asymptotic series for $y(t)$?",,"['ordinary-differential-equations', 'asymptotics']"
48,"Prove that the family of curves $\frac{x^2}{a^2+\lambda}+\frac{y^2}{b^2+\lambda}=1$,where $\lambda$ is a parameter,is self orthogonal.","Prove that the family of curves ,where  is a parameter,is self orthogonal.",\frac{x^2}{a^2+\lambda}+\frac{y^2}{b^2+\lambda}=1 \lambda,"Prove that the family of curves $\frac{x^2}{a^2+\lambda}+\frac{y^2}{b^2+\lambda}=1$,where $\lambda$ is a parameter,is self orthogonal. I tried to find the differential equation of first order(because there is one parametr $\lambda$) corresponding to $\frac{x^2}{a^2+\lambda}+\frac{y^2}{b^2+\lambda}=1$ and then replace $\frac{dy}{dx}$ by $\frac{-dx}{dy}$ and then re-integrating it but the calculations has gone messy.","Prove that the family of curves $\frac{x^2}{a^2+\lambda}+\frac{y^2}{b^2+\lambda}=1$,where $\lambda$ is a parameter,is self orthogonal. I tried to find the differential equation of first order(because there is one parametr $\lambda$) corresponding to $\frac{x^2}{a^2+\lambda}+\frac{y^2}{b^2+\lambda}=1$ and then replace $\frac{dy}{dx}$ by $\frac{-dx}{dy}$ and then re-integrating it but the calculations has gone messy.",,"['calculus', 'ordinary-differential-equations']"
49,How to turn a system of first order into a second order,How to turn a system of first order into a second order,,"So I have two equations $X' = aX + bY$ $Y' = cX + dY$ I want to convert it back to a second order equation with the form $X'' + \alpha X' + \beta X$ with $\alpha,\beta$ in terms of a,b,c,d. I have been racking my brain for hours trying to go backwards from a reduction of order, but just can't seem to figure it out. Any help would be much appreciated!","So I have two equations $X' = aX + bY$ $Y' = cX + dY$ I want to convert it back to a second order equation with the form $X'' + \alpha X' + \beta X$ with $\alpha,\beta$ in terms of a,b,c,d. I have been racking my brain for hours trying to go backwards from a reduction of order, but just can't seem to figure it out. Any help would be much appreciated!",,"['ordinary-differential-equations', 'derivatives']"
50,Uniqueness of IVP solution with a condition weaker than Lipschitz?,Uniqueness of IVP solution with a condition weaker than Lipschitz?,,"We know that Lipschitz condition with respect to $x$ in $$x' = f(t,x) , x(t_0)=x_0 $$ implies uniqueness of IVP problem above. Can we have uniqueness with condition less than Lipschitz?","We know that Lipschitz condition with respect to $x$ in $$x' = f(t,x) , x(t_0)=x_0 $$ implies uniqueness of IVP problem above. Can we have uniqueness with condition less than Lipschitz?",,"['ordinary-differential-equations', 'lipschitz-functions']"
51,Inhomogeneous modified Bessel differential equation,Inhomogeneous modified Bessel differential equation,,"I'm trying to solve the following inhomogeneous modified bessel equation.  $$y^{\prime\prime}+\frac{1}{x}y^{}\prime-\frac{x^2+4}{x^2}y=x^4$$ I know the homogeneous solution for this differential equation is $y_h=c_1I_2(x)+c_2K_2(x)$ Where $I_2$ and $K_2$ are the modified Bessel function of the first and second kind respectively both of order 2. For a articular solution i'm trying to get an answer using variation of parameters and full knowing that $W[K_\nu,I_\nu]=1/x$. Next, i know the particular solution has the form: $$y_p=v_1(x)y_1+v_2(x)y_2$$ where $y_1$ and $y_2$ are the solutions of the homogeneous differential equation respectively. $v_1(x)=-\int\frac{fy_2}{W}$ and $v_2(x)=\int\frac{fy_1}{W}$ where $f=x^4$ The answer to the problem is give and $y_p=-x^2(x^2+12)$ I don't know how the two integrals can be solved and give something so simple in the end, there's something i'm missing.","I'm trying to solve the following inhomogeneous modified bessel equation.  $$y^{\prime\prime}+\frac{1}{x}y^{}\prime-\frac{x^2+4}{x^2}y=x^4$$ I know the homogeneous solution for this differential equation is $y_h=c_1I_2(x)+c_2K_2(x)$ Where $I_2$ and $K_2$ are the modified Bessel function of the first and second kind respectively both of order 2. For a articular solution i'm trying to get an answer using variation of parameters and full knowing that $W[K_\nu,I_\nu]=1/x$. Next, i know the particular solution has the form: $$y_p=v_1(x)y_1+v_2(x)y_2$$ where $y_1$ and $y_2$ are the solutions of the homogeneous differential equation respectively. $v_1(x)=-\int\frac{fy_2}{W}$ and $v_2(x)=\int\frac{fy_1}{W}$ where $f=x^4$ The answer to the problem is give and $y_p=-x^2(x^2+12)$ I don't know how the two integrals can be solved and give something so simple in the end, there's something i'm missing.",,"['ordinary-differential-equations', 'bessel-functions']"
52,Solving the differential equation $\frac{dy}{dt}=e^{t-y}$,Solving the differential equation,\frac{dy}{dt}=e^{t-y},"I am working on the equation  $$\frac{dy}{dt}=e^{t-y},\qquad y(0) = 1$$ This is what I have tried to get it to its exact solution: $$\frac{dy}{dt}=e^{t}e^{-y}$$ $$\frac{1}{e^{-y}}dy=e^{t}dt$$ $$e^{y}dy=e^{t}dt$$ $$\int e^{y}dy=\int e^{t}dt$$ $$e^{y}=e^{t}+C $$ $$e^{1}-e^{0}=C <y=1, t=0>$$ $$e^{1}-1=C$$ going back: $$e^{y}=e^{t}+\ln(e^{1}-1) $$ $$\ln(e^{y})=\ln(e^{t}+e-1) $$ $$y=\ln(e^{t}+e-1)$$ Latex made me see my problem, Thank you for helping if you answered!","I am working on the equation  $$\frac{dy}{dt}=e^{t-y},\qquad y(0) = 1$$ This is what I have tried to get it to its exact solution: $$\frac{dy}{dt}=e^{t}e^{-y}$$ $$\frac{1}{e^{-y}}dy=e^{t}dt$$ $$e^{y}dy=e^{t}dt$$ $$\int e^{y}dy=\int e^{t}dt$$ $$e^{y}=e^{t}+C $$ $$e^{1}-e^{0}=C <y=1, t=0>$$ $$e^{1}-1=C$$ going back: $$e^{y}=e^{t}+\ln(e^{1}-1) $$ $$\ln(e^{y})=\ln(e^{t}+e-1) $$ $$y=\ln(e^{t}+e-1)$$ Latex made me see my problem, Thank you for helping if you answered!",,['ordinary-differential-equations']
53,Why this equation is correct?,Why this equation is correct?,,"I am going through the University of Pennsylvania's numerical analysis lectures, which can be found here . I am a bit confused with equation 1.1.9 at page 6-7. The equation is $y'(x) + a(x)y(x) = b(x)$. The notes say that this can be rewritten as $$a^{-A(x)}d(a^{A(x)}y(x)) = b(x)$$  where $A(x)$ is an antiderivative of $a(x)$. So, we have that  $$y'(x) + a(x)y(x) = a^{-A(x)}d(a^{A(x)}y(x)).$$ Why is that? I can not go from the first equation to the other.","I am going through the University of Pennsylvania's numerical analysis lectures, which can be found here . I am a bit confused with equation 1.1.9 at page 6-7. The equation is $y'(x) + a(x)y(x) = b(x)$. The notes say that this can be rewritten as $$a^{-A(x)}d(a^{A(x)}y(x)) = b(x)$$  where $A(x)$ is an antiderivative of $a(x)$. So, we have that  $$y'(x) + a(x)y(x) = a^{-A(x)}d(a^{A(x)}y(x)).$$ Why is that? I can not go from the first equation to the other.",,['ordinary-differential-equations']
54,Solving a challenging differential equation,Solving a challenging differential equation,,How would one go about finding a closed form analytic solution to the following differential equation? $$\frac{d^2y}{dx^2} +(x^4 +x^2+x+c)y(x) =0 $$ where $c\in\mathbb{R}$,How would one go about finding a closed form analytic solution to the following differential equation? $$\frac{d^2y}{dx^2} +(x^4 +x^2+x+c)y(x) =0 $$ where $c\in\mathbb{R}$,,"['analysis', 'ordinary-differential-equations']"
55,Laplace Transform or Characteristic Equation?,Laplace Transform or Characteristic Equation?,,The proponents of the use of Laplace Transform in differential equations claim that is easier and faster but is that always the case? Often I have found out that solving an ODE through the characteristic equation and the use of undetermined coefficients is significantly easier and less time-consuming. What is your opinion? Thank you.,The proponents of the use of Laplace Transform in differential equations claim that is easier and faster but is that always the case? Often I have found out that solving an ODE through the characteristic equation and the use of undetermined coefficients is significantly easier and less time-consuming. What is your opinion? Thank you.,,['ordinary-differential-equations']
56,Polynomial differential equation,Polynomial differential equation,,"I came across this problem in an old olympiad paper (Putnam?) Find all polynomials $p(x)$ with real coefficients satisfying the differential equation $7\dfrac{d }{dx } [xp(x)]=3p(x)+4p(x+1)$   $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ -\infty<x<\infty$ I didn't find any ""official"" solution on internet and it would be interesting to see your approaches on this one. My approach was the following , i don't know if i'm right.  Consider the leading coefficient $a_n$ on both sides. Then we have $7na_n=7a_n$. Suppose $a_n\neq 0$ then we get $n=1$. I plugged in the general solution $p(x)=a_0+a_1x$ in the differential equation and solved for $a_1$ wich turned out to be $0$ , contradicition.  The other option is that $a_n=0$ but then $a_{n-1 }$ would be the leading coefficient of the polynomial wich would be again $0$ (by the same reasoning). Hence by induction the only non zero leading coefficient must be $a_0$. Could you tell me if this is ok? Thanks in advance.","I came across this problem in an old olympiad paper (Putnam?) Find all polynomials $p(x)$ with real coefficients satisfying the differential equation $7\dfrac{d }{dx } [xp(x)]=3p(x)+4p(x+1)$   $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ -\infty<x<\infty$ I didn't find any ""official"" solution on internet and it would be interesting to see your approaches on this one. My approach was the following , i don't know if i'm right.  Consider the leading coefficient $a_n$ on both sides. Then we have $7na_n=7a_n$. Suppose $a_n\neq 0$ then we get $n=1$. I plugged in the general solution $p(x)=a_0+a_1x$ in the differential equation and solved for $a_1$ wich turned out to be $0$ , contradicition.  The other option is that $a_n=0$ but then $a_{n-1 }$ would be the leading coefficient of the polynomial wich would be again $0$ (by the same reasoning). Hence by induction the only non zero leading coefficient must be $a_0$. Could you tell me if this is ok? Thanks in advance.",,"['ordinary-differential-equations', 'polynomials', 'contest-math']"
57,What is a general solution to a differential equation?,What is a general solution to a differential equation?,,"I understand how to find the general solution of a differential equation, for complex roots, simple roots, nonhomogeneous equations etc... but I still don't really know what the general solution is? As in, what is the purpose of it? What is it used for? thanks...","I understand how to find the general solution of a differential equation, for complex roots, simple roots, nonhomogeneous equations etc... but I still don't really know what the general solution is? As in, what is the purpose of it? What is it used for? thanks...",,['ordinary-differential-equations']
58,How to solve exact equations by integrating factors?,How to solve exact equations by integrating factors?,,"I know how to solve an exact equation like $$M(x,y) + N(x,y)y=0 $$ We just check $$\frac{\partial M}{\partial y} =\frac{\partial N}{\partial x} $$ If so, then it's just a little bit of algebra, taking anti-derivative and solve, such that $f = f(x,y) = c$, where we can obtain $f$ by just taking integal w.r.t. $x$ etc. But now I'm stuck with integrating factors. I can do it if the integrating factor $\mu=\mu(x)$ depends only on one variable. But now I want to do another exercise, which uses a function of the form $\mu(x+y)$. How can I solve $$(7x^3+3x^2y+4y) +(4x^3+x+5y)y'=0 $$ I'm given a hint that I should use a function of the form $\mu(x+y)$. I guess I should just multiply by $(x+y)^m$ for an arbitrary $m$, but I don't know how to do the algebra after I multiplied... I guess something like this  : $$\frac{\partial (M \mu)}{\partial y}= m(x+y)^{m-1}(7x^3+3x^2y+4y) + (x+y)^m(3x^2+4) =$$ $$\frac{\partial (N\mu)}{\partial x} = m(x+y)^{m-1}(4x^3+x+5y) + (x+y)^m(12x^2+1) $$ How solve for $\mu$ ?","I know how to solve an exact equation like $$M(x,y) + N(x,y)y=0 $$ We just check $$\frac{\partial M}{\partial y} =\frac{\partial N}{\partial x} $$ If so, then it's just a little bit of algebra, taking anti-derivative and solve, such that $f = f(x,y) = c$, where we can obtain $f$ by just taking integal w.r.t. $x$ etc. But now I'm stuck with integrating factors. I can do it if the integrating factor $\mu=\mu(x)$ depends only on one variable. But now I want to do another exercise, which uses a function of the form $\mu(x+y)$. How can I solve $$(7x^3+3x^2y+4y) +(4x^3+x+5y)y'=0 $$ I'm given a hint that I should use a function of the form $\mu(x+y)$. I guess I should just multiply by $(x+y)^m$ for an arbitrary $m$, but I don't know how to do the algebra after I multiplied... I guess something like this  : $$\frac{\partial (M \mu)}{\partial y}= m(x+y)^{m-1}(7x^3+3x^2y+4y) + (x+y)^m(3x^2+4) =$$ $$\frac{\partial (N\mu)}{\partial x} = m(x+y)^{m-1}(4x^3+x+5y) + (x+y)^m(12x^2+1) $$ How solve for $\mu$ ?",,['ordinary-differential-equations']
59,Periodic solution of differential equation,Periodic solution of differential equation,,"let be the ODE $ -y''(x)+f(x)y(x)=0 $ if the function $ f(x+T)=f(x) $ is PERIODIC does it mean that the ODE has only periodic solutions ? if all the solutions are periodic , then can all be determined by Fourier series ??","let be the ODE $ -y''(x)+f(x)y(x)=0 $ if the function $ f(x+T)=f(x) $ is PERIODIC does it mean that the ODE has only periodic solutions ? if all the solutions are periodic , then can all be determined by Fourier series ??",,"['ordinary-differential-equations', 'periodic-functions']"
60,Error When Using Mathematica To Solve Differential Equation,Error When Using Mathematica To Solve Differential Equation,,"I have a concern of why mathematica does not like/except the independent variable x . When it is used it gives the following error: In[1]:= DSolve[{2 x''[t] + $6$ x'[t] + 5 x[t] == 0, x[0] == 10, x'[0] == 0}, x[t], t] Gives DSolve::deqn : $\hspace{0.2cm}$ Equation or list of equations expected instead of True in the first argument $\hspace{0.9cm}$ {$11$x[t]+$2$x''[t] == $0,~$ x[$0$]==$10,~$True}. Out[1]= DSolve[{$11$x[t]+$2$x''[t]==$0,~$ x[$0$]==$10,~$ True}, x[t],$\:$ t] But when the variable is switched out for another, you obtain the solution as expected like so: In[2]:= DSolve[{2 s''[t] + 6 s[t] + 5 s[t] == 0, s[0] == 10, s'[0] == 0}, s[t], t] Out[2]= $\Bigl\{\Bigl\{s\Bigl[t\Bigr] \rightarrow \text{10 Cos}\left[\sqrt{\dfrac{11}{2}}t \right]\Bigr\}\Bigr\}$ What could be the cause of this? Note: I also cleared the variable from memory just to be on the safe side.","I have a concern of why mathematica does not like/except the independent variable x . When it is used it gives the following error: In[1]:= DSolve[{2 x''[t] + $6$ x'[t] + 5 x[t] == 0, x[0] == 10, x'[0] == 0}, x[t], t] Gives DSolve::deqn : $\hspace{0.2cm}$ Equation or list of equations expected instead of True in the first argument $\hspace{0.9cm}$ {$11$x[t]+$2$x''[t] == $0,~$ x[$0$]==$10,~$True}. Out[1]= DSolve[{$11$x[t]+$2$x''[t]==$0,~$ x[$0$]==$10,~$ True}, x[t],$\:$ t] But when the variable is switched out for another, you obtain the solution as expected like so: In[2]:= DSolve[{2 s''[t] + 6 s[t] + 5 s[t] == 0, s[0] == 10, s'[0] == 0}, s[t], t] Out[2]= $\Bigl\{\Bigl\{s\Bigl[t\Bigr] \rightarrow \text{10 Cos}\left[\sqrt{\dfrac{11}{2}}t \right]\Bigr\}\Bigr\}$ What could be the cause of this? Note: I also cleared the variable from memory just to be on the safe side.",,"['ordinary-differential-equations', 'mathematica']"
61,solving a differential equation involving $\frac{y-x^2}{\sin y-x}$,solving a differential equation involving,\frac{y-x^2}{\sin y-x},I'm trying to find the general solution to $$\frac{\text{d}y}{\text{d}x} = \frac{y-x^2}{\sin y-x}$$ Any ideas would be greatly appreciated. Thanks!,I'm trying to find the general solution to $$\frac{\text{d}y}{\text{d}x} = \frac{y-x^2}{\sin y-x}$$ Any ideas would be greatly appreciated. Thanks!,,['ordinary-differential-equations']
62,Lee Smooth Manifolds: Fundamental Theorem of Flows (9.12) Details,Lee Smooth Manifolds: Fundamental Theorem of Flows (9.12) Details,,"Theorem 9.12 For the second paragraph of the proof, I do not see how it is obvious that $\theta^{(p)}$ is the unique, maximal extension of all such integral curves. Without appealing to a Zorn's Lemma argument. Edit March 28: The Zorn's Lemma argument is unnecessary because we can view the integral curves as a subset of the product space $\mathbb{R}\times M$ . Taking the union over all the graphs gives us a maximal extension. Zorn's Lemma Fix $p\in M$ , define $$ \mathcal{I}_p = \Biggl\{\gamma:J\to M,\quad \substack{J\text{ is an open interval containing 0, and}\\ \gamma\text{ is an integral curve of } V \text{ starting at } p}\Biggr\} $$ If $\gamma_1$ , and $\gamma_2$ are in $\mathcal{I}_p$ , we define $$ \gamma_1\lesssim\gamma_2\iff \operatorname{domain}\gamma_1\subseteq\operatorname{domain}\gamma_2 $$ Through a 'union of connected open sets with a common point is again connected and open' argument, we obtain a maximal curve denoted by $\theta^{(p)}$ with domain $\mathcal{D}^{(p)}$ , similar to the proof for Tychonoff's Theorem. Smoothness of $\theta$ Once we have these unique maximal integral curves, on the 6th paragraph of the proof (where he introduces some $t_1>t_0-\varepsilon$ ), I fail to see why $$ t_1<t_0\implies (t,p_0)\in W $$ without the added restriction $t_1>0$ . Later in the paragraph, we define $\widetilde{\theta}: [0, t_1+\varepsilon)\times U_1\to M$ , with our original flow $\theta$ being smooth on $(t_1-\delta,\: t_1+\delta)\times U_1$ . Shouldn't $\widetilde{\theta}$ be defined on some $(-\delta, t_1+\varepsilon)$ (by shrinking $\delta$ if necessary) instead? By showing $\widetilde{\theta}(\cdot,p)$ is again an integral curve starting at $p$ for any $p\in U_1$ . $$\operatorname{domain}\widetilde{\theta}(\cdot,p)=(-\delta,\: t_1+\varepsilon)\implies \widetilde{\theta}(\cdot,p)\lesssim \theta^{(p)}$$ It has to be pointwise equal to $\theta$ . $$\widetilde{\theta}=\theta\vert_{(-\delta,\:t_1+\varepsilon)\times U_1}$$ Since $\widetilde{\theta}$ is smooth on $(-\delta, t_1+\varepsilon)\times U_1$ , this means $\theta$ is smooth on the same neighbourhood as well, breaking the supremum. Excerpt from book Errata can be found here .","Theorem 9.12 For the second paragraph of the proof, I do not see how it is obvious that is the unique, maximal extension of all such integral curves. Without appealing to a Zorn's Lemma argument. Edit March 28: The Zorn's Lemma argument is unnecessary because we can view the integral curves as a subset of the product space . Taking the union over all the graphs gives us a maximal extension. Zorn's Lemma Fix , define If , and are in , we define Through a 'union of connected open sets with a common point is again connected and open' argument, we obtain a maximal curve denoted by with domain , similar to the proof for Tychonoff's Theorem. Smoothness of Once we have these unique maximal integral curves, on the 6th paragraph of the proof (where he introduces some ), I fail to see why without the added restriction . Later in the paragraph, we define , with our original flow being smooth on . Shouldn't be defined on some (by shrinking if necessary) instead? By showing is again an integral curve starting at for any . It has to be pointwise equal to . Since is smooth on , this means is smooth on the same neighbourhood as well, breaking the supremum. Excerpt from book Errata can be found here .","\theta^{(p)} \mathbb{R}\times M p\in M 
\mathcal{I}_p = \Biggl\{\gamma:J\to M,\quad \substack{J\text{ is an open interval containing 0, and}\\ \gamma\text{ is an integral curve of } V \text{ starting at } p}\Biggr\}
 \gamma_1 \gamma_2 \mathcal{I}_p 
\gamma_1\lesssim\gamma_2\iff \operatorname{domain}\gamma_1\subseteq\operatorname{domain}\gamma_2
 \theta^{(p)} \mathcal{D}^{(p)} \theta t_1>t_0-\varepsilon 
t_1<t_0\implies (t,p_0)\in W
 t_1>0 \widetilde{\theta}: [0, t_1+\varepsilon)\times U_1\to M \theta (t_1-\delta,\: t_1+\delta)\times U_1 \widetilde{\theta} (-\delta, t_1+\varepsilon) \delta \widetilde{\theta}(\cdot,p) p p\in U_1 \operatorname{domain}\widetilde{\theta}(\cdot,p)=(-\delta,\: t_1+\varepsilon)\implies \widetilde{\theta}(\cdot,p)\lesssim \theta^{(p)} \theta \widetilde{\theta}=\theta\vert_{(-\delta,\:t_1+\varepsilon)\times U_1} \widetilde{\theta} (-\delta, t_1+\varepsilon)\times U_1 \theta","['ordinary-differential-equations', 'differential-geometry', 'smooth-manifolds']"
63,"Solving $\frac{dx}{dt}=\frac{xt}{x^2+t^2},\ x(0)=1$",Solving,"\frac{dx}{dt}=\frac{xt}{x^2+t^2},\ x(0)=1","I have started self-studying differential equations and I have come across the following initial value problem $$\frac{dx}{dt}=\frac{xt}{x^2+t^2}, \quad x(0)=1$$ Now, since $f(t,x)=\frac{xt}{x^2+t^2}$ is such that $f(rt,rx)=f(t,x)$ for every $r\in\mathbb{R}\setminus\{0\}$ , we can use the change of variables $y=\frac{x}{t}$ to rewrite it in the form \begin{align} y+t\frac{dy}{dt} &=\frac{t^2 y}{t^2(1+y^2)} \\ &=\frac{y}{1+y^2} \\ \implies t\frac{dy}{dt} &=\frac{y}{1+y^2}-y \\ &=-\frac{y^3}{1+y^2} \\ \implies \frac{dy}{dt} &= \left(-\frac{y^3}{1+y^2}\right)\cdot\frac{1}{t} \end{align} which is separable, and becomes: \begin{align} \left(\frac{1+y^2}{y^3}\right)dy &= -\frac{dt}{t} \\ \implies \int_{y_1}^{y_2} \left(\frac{1+y^2}{y^3}\right) dy &= -\int_{t_1}^{t_2} \frac{dt}{t} \\ \implies -\frac{1}{2y_2^2}+\ln \left\lvert \frac{y_2}{y_1} \right\rvert + \frac{1}{2y_1^2} &= -\ln \left\lvert \frac{t_2}{t_1} \right\rvert \end{align} but now I don't see how to go forward and find $y(t)$ . Also, I integrated from a generic time $t_1$ to a generic time $t_2$ because the right hand side wouldn't have converged otherwise. So, I would appreciate any hint about how to go forward in solving this IVP. Thanks","I have started self-studying differential equations and I have come across the following initial value problem Now, since is such that for every , we can use the change of variables to rewrite it in the form which is separable, and becomes: but now I don't see how to go forward and find . Also, I integrated from a generic time to a generic time because the right hand side wouldn't have converged otherwise. So, I would appreciate any hint about how to go forward in solving this IVP. Thanks","\frac{dx}{dt}=\frac{xt}{x^2+t^2}, \quad x(0)=1 f(t,x)=\frac{xt}{x^2+t^2} f(rt,rx)=f(t,x) r\in\mathbb{R}\setminus\{0\} y=\frac{x}{t} \begin{align}
y+t\frac{dy}{dt} &=\frac{t^2 y}{t^2(1+y^2)} \\
&=\frac{y}{1+y^2} \\
\implies t\frac{dy}{dt} &=\frac{y}{1+y^2}-y \\
&=-\frac{y^3}{1+y^2} \\
\implies \frac{dy}{dt} &= \left(-\frac{y^3}{1+y^2}\right)\cdot\frac{1}{t}
\end{align} \begin{align}
\left(\frac{1+y^2}{y^3}\right)dy &= -\frac{dt}{t} \\
\implies \int_{y_1}^{y_2} \left(\frac{1+y^2}{y^3}\right) dy &= -\int_{t_1}^{t_2} \frac{dt}{t} \\
\implies -\frac{1}{2y_2^2}+\ln \left\lvert \frac{y_2}{y_1} \right\rvert + \frac{1}{2y_1^2} &= -\ln \left\lvert \frac{t_2}{t_1} \right\rvert
\end{align} y(t) t_1 t_2","['ordinary-differential-equations', 'initial-value-problems']"
64,Why doesn't this differential technique work?,Why doesn't this differential technique work?,,"Suppose we try to solve the ODE $y' = y,$ we can rearrange this into $y'/y = 1,$ integrate and obtain $\ln(y) = x.$ However, let's try this on $y'' = xy$ , then we rearrange this into $y''/y = x,$ but the solution is Ai $(x)$ and Bi $(x)$ , the airy functions. Can this technique of rearranging then integrating be salvaged to explain this result or does it fail in some way? Is there a known way to integrate to derive a function $f$ in terms to $y'$ and $y$ ?","Suppose we try to solve the ODE we can rearrange this into integrate and obtain However, let's try this on , then we rearrange this into but the solution is Ai and Bi , the airy functions. Can this technique of rearranging then integrating be salvaged to explain this result or does it fail in some way? Is there a known way to integrate to derive a function in terms to and ?","y' = y, y'/y = 1, \ln(y) = x. y'' = xy y''/y = x, (x) (x) f y' y",['ordinary-differential-equations']
65,Torus Killing vectors,Torus Killing vectors,,"I'm trying to calculate the Killing vectors associated to a 2D torus, parametrised as $ds^2=a^2d\theta^2+(b+a\sin\theta)^2d\phi^2$ . From here I got the metric and found out that the only connection coefficients that don't vanish are: $$ \Gamma^\theta{}_{\phi\phi}=-\frac{(b+a\sin\theta)\cos\theta}{a} \hspace{1cm}\Gamma^\phi{}_{\theta\phi}=\Gamma^\phi{}_{\phi\theta}=\frac{a\cos\theta}{b+a\sin{\theta}}$$ From here, and given the Killing condition: $\nabla_\alpha \xi_\beta + \nabla_\beta \xi_\alpha =\partial_\alpha \xi_\beta + \partial_\beta \xi_\alpha-2\Gamma^{\lambda}_{\beta\alpha}\xi_\lambda=0$ , one can get the three following equations for the killing vectors: $$\partial_\phi \xi_\phi = \Gamma^\theta{}_{\phi\phi}\xi_\theta=-\frac{(b+a\sin\theta)\cos\theta}{a}\xi_\theta \hspace{2cm} (1)$$ $$\partial_\phi \xi_\theta + \partial_\theta \xi_\phi = \frac{2a\cos\theta}{b+a\sin{\theta}}\xi_\phi \hspace{2cm} (2)$$ $$ 2\partial_\theta \xi_\theta=0 \hspace{2cm} (3)$$ From (3) it's easy to see that $\xi_\theta=f(\phi)$ , so that (1) can be solved as: $$\xi_\phi = -\frac{(b+a\sin\theta)\cos\theta}{a}F(\phi)+g(\theta),$$ with $F'(\phi)=f(\phi)$ . This way, one can substitute in (2) and get the following differential equation: $$f'(\phi)+[-\cos(2\theta) + \frac{b}{a}\sin\theta]F(\phi)=- \frac{2a\cos\theta}{b+a\sin\theta}g(\theta)-g'(\theta)$$ But I can't see how to solve this (could be that my differential equation solving skills aren't pretty much on point though... If only I could separate variables), and wouldn't like to spend too much time on it since it could also be that I made some mistake in the previous calculations, so if anyone sees something wrong (or that I'm on the good way), any feedback will be much appreciated!","I'm trying to calculate the Killing vectors associated to a 2D torus, parametrised as . From here I got the metric and found out that the only connection coefficients that don't vanish are: From here, and given the Killing condition: , one can get the three following equations for the killing vectors: From (3) it's easy to see that , so that (1) can be solved as: with . This way, one can substitute in (2) and get the following differential equation: But I can't see how to solve this (could be that my differential equation solving skills aren't pretty much on point though... If only I could separate variables), and wouldn't like to spend too much time on it since it could also be that I made some mistake in the previous calculations, so if anyone sees something wrong (or that I'm on the good way), any feedback will be much appreciated!","ds^2=a^2d\theta^2+(b+a\sin\theta)^2d\phi^2  \Gamma^\theta{}_{\phi\phi}=-\frac{(b+a\sin\theta)\cos\theta}{a} \hspace{1cm}\Gamma^\phi{}_{\theta\phi}=\Gamma^\phi{}_{\phi\theta}=\frac{a\cos\theta}{b+a\sin{\theta}} \nabla_\alpha \xi_\beta + \nabla_\beta \xi_\alpha =\partial_\alpha \xi_\beta + \partial_\beta \xi_\alpha-2\Gamma^{\lambda}_{\beta\alpha}\xi_\lambda=0 \partial_\phi \xi_\phi = \Gamma^\theta{}_{\phi\phi}\xi_\theta=-\frac{(b+a\sin\theta)\cos\theta}{a}\xi_\theta \hspace{2cm} (1) \partial_\phi \xi_\theta + \partial_\theta \xi_\phi = \frac{2a\cos\theta}{b+a\sin{\theta}}\xi_\phi \hspace{2cm} (2)  2\partial_\theta \xi_\theta=0 \hspace{2cm} (3) \xi_\theta=f(\phi) \xi_\phi = -\frac{(b+a\sin\theta)\cos\theta}{a}F(\phi)+g(\theta), F'(\phi)=f(\phi) f'(\phi)+[-\cos(2\theta) + \frac{b}{a}\sin\theta]F(\phi)=- \frac{2a\cos\theta}{b+a\sin\theta}g(\theta)-g'(\theta)","['ordinary-differential-equations', 'differential-geometry', 'metric-spaces', 'differential-topology']"
66,Why does not noise accumulate in a stochastic differential equation as it would in a random walk?,Why does not noise accumulate in a stochastic differential equation as it would in a random walk?,,"I had a question about stochastic differential equations. I come from statistics, so I am more familiar with say random walks, which are of course, discrete. So if I had a process like where $x(t)$ traces the random walk evolution of a variable over time, then that might look like this recursively: $$ x(t+1) = x(t) + \epsilon \sim Binomial(p,n) $$ where $p,n$ are parameters for the chance of success and number of trials, respectively. If I ran this simulation multiple and plotted time $t$ versus $x(t)$ , I would get a fan or cone shaped plot with all trajectories starting at the same point, but then progressively spreading out with each time step. My question is, when I look at plots of stochastic differential equations, I don't see this same kind of fanning behavior. When talking about an SDE of the form: $$ du = f(u, p, t)dt + g(u, p, t)dW $$ In this case, $f$ is the deterministic part of the process and $g$ is the noise process, usually Brownian motion. This is of course a continuous time equation. Here is a plot of a stochastic version of the lorenz equation that conforms to the SDE form above--taken from the Julia Differential Equations documentation . Again, my question is, why don't we see the trajectory fan out over time. It seems like the trajectory does wiggle, but it does not diffuse as in a random walk. I was just trying to understand what are the forces at play that prevent the noise process to overwhelm the deterministic part of the process over time?","I had a question about stochastic differential equations. I come from statistics, so I am more familiar with say random walks, which are of course, discrete. So if I had a process like where traces the random walk evolution of a variable over time, then that might look like this recursively: where are parameters for the chance of success and number of trials, respectively. If I ran this simulation multiple and plotted time versus , I would get a fan or cone shaped plot with all trajectories starting at the same point, but then progressively spreading out with each time step. My question is, when I look at plots of stochastic differential equations, I don't see this same kind of fanning behavior. When talking about an SDE of the form: In this case, is the deterministic part of the process and is the noise process, usually Brownian motion. This is of course a continuous time equation. Here is a plot of a stochastic version of the lorenz equation that conforms to the SDE form above--taken from the Julia Differential Equations documentation . Again, my question is, why don't we see the trajectory fan out over time. It seems like the trajectory does wiggle, but it does not diffuse as in a random walk. I was just trying to understand what are the forces at play that prevent the noise process to overwhelm the deterministic part of the process over time?","x(t) 
x(t+1) = x(t) + \epsilon \sim Binomial(p,n)
 p,n t x(t) 
du = f(u, p, t)dt + g(u, p, t)dW
 f g","['ordinary-differential-equations', 'stochastic-processes', 'stochastic-differential-equations']"
67,What is the application/use to know about the order and degree of a differential equation?,What is the application/use to know about the order and degree of a differential equation?,,"A degree of a polynomial tells us the maximum number of possible roots of the equation. But what is the use to know about the order and degree of the differential equation. Like, what does it really tell us about the equation we are working with? For Example: If I am working with a polynomial of degree 3, and I tell that to you, you will be expecting, at most 3 roots of that polynomial. Degree of a polynomial only tells us about the number of roots, and so, x 3 and x 3 + 3x 2 + 5 both are equation of degree 3 and tells it will have a maximum of 3 roots. But what does telling you that I am working with a differential equation of order 3 and degree 2 really tell you about the equation because you can not predict what the equation is, but then what can you predict with the data I give you? I am new in the Journey of Differential Equations and wanted to know about the things (and reasons) that I am learning. Thanks in advance for your answer. :)","A degree of a polynomial tells us the maximum number of possible roots of the equation. But what is the use to know about the order and degree of the differential equation. Like, what does it really tell us about the equation we are working with? For Example: If I am working with a polynomial of degree 3, and I tell that to you, you will be expecting, at most 3 roots of that polynomial. Degree of a polynomial only tells us about the number of roots, and so, x 3 and x 3 + 3x 2 + 5 both are equation of degree 3 and tells it will have a maximum of 3 roots. But what does telling you that I am working with a differential equation of order 3 and degree 2 really tell you about the equation because you can not predict what the equation is, but then what can you predict with the data I give you? I am new in the Journey of Differential Equations and wanted to know about the things (and reasons) that I am learning. Thanks in advance for your answer. :)",,['ordinary-differential-equations']
68,How can this system not be asymptotically stable?,How can this system not be asymptotically stable?,,"I am currently studying stability of nonautonomous systems using the book Applied Nonlinear Control by Slotine & Li. On page 125, there is example 4.13: $$ \begin{align} \dot{e} &= -e + \theta \, w(t) \\ \dot{\theta} &= -e \, w(t) \end{align} \tag{1} $$ with $w(t)$ a bounded, continuous but otherwise arbitrary time-varying function. They consider the Lyapunov function $$V(e, \theta) = e^2 + \theta^2$$ with derivative $$\dot{V}(e, \theta) = -2 e^2 \leq 0 \tag{2}$$ so $e$ and $\theta$ are bounded. Then, they use Barbalat's lemma to show that $\dot{V}(e, \theta) \rightarrow 0$ as $t \rightarrow \infty$ , so also $e \rightarrow 0$ . Then they say: Note that, although $e$ converges to zero, the system is not asymptotically stable, because $\theta$ is only guaranteed to be bounded. However, isn't it like this: If $e \rightarrow 0$ then this implies that $\dot{e} \rightarrow 0$ as well. So, for $t \rightarrow \infty$ , system $(1)$ reduces to $$ \begin{align} 0 &= \theta \, w(t) \\ \dot{\theta} &= 0 \end{align} \tag{3} $$ Because $w(t)$ can be arbitrary for all time, the first equation of $(3)$ is only true if $\theta \rightarrow 0$ as well. The second equation of $(3)$ also confirms that $\theta$ doesn't change anymore for $t \rightarrow \infty$ . So, my conclusion would be: Since $e \rightarrow 0$ and $\theta \rightarrow 0$ , the system is actually asymptotically stable. However, this is in contradiction to the citation above. Question : Basically two questions: Where is the mistake in my argument? Or is it actually correct and the book is wrong? Is system $(1)$ now asymptotically stable or not? Note : I also tried some functions like $w(t) = \sin(t)$ with different initial conditions in simulation, and at least for those examples, the system always seemed to converge to $(0,0)$ .","I am currently studying stability of nonautonomous systems using the book Applied Nonlinear Control by Slotine & Li. On page 125, there is example 4.13: with a bounded, continuous but otherwise arbitrary time-varying function. They consider the Lyapunov function with derivative so and are bounded. Then, they use Barbalat's lemma to show that as , so also . Then they say: Note that, although converges to zero, the system is not asymptotically stable, because is only guaranteed to be bounded. However, isn't it like this: If then this implies that as well. So, for , system reduces to Because can be arbitrary for all time, the first equation of is only true if as well. The second equation of also confirms that doesn't change anymore for . So, my conclusion would be: Since and , the system is actually asymptotically stable. However, this is in contradiction to the citation above. Question : Basically two questions: Where is the mistake in my argument? Or is it actually correct and the book is wrong? Is system now asymptotically stable or not? Note : I also tried some functions like with different initial conditions in simulation, and at least for those examples, the system always seemed to converge to .","
\begin{align}
\dot{e} &= -e + \theta \, w(t) \\
\dot{\theta} &= -e \, w(t)
\end{align} \tag{1}
 w(t) V(e, \theta) = e^2 + \theta^2 \dot{V}(e, \theta) = -2 e^2 \leq 0 \tag{2} e \theta \dot{V}(e, \theta) \rightarrow 0 t \rightarrow \infty e \rightarrow 0 e \theta e \rightarrow 0 \dot{e} \rightarrow 0 t \rightarrow \infty (1) 
\begin{align}
0 &= \theta \, w(t) \\
\dot{\theta} &= 0
\end{align} \tag{3}
 w(t) (3) \theta \rightarrow 0 (3) \theta t \rightarrow \infty e \rightarrow 0 \theta \rightarrow 0 (1) w(t) = \sin(t) (0,0)","['ordinary-differential-equations', 'dynamical-systems', 'control-theory', 'stability-in-odes', 'lyapunov-functions']"
69,Do I have something wrong when solving $y'+2y=6$?,Do I have something wrong when solving ?,y'+2y=6,"Solve $$y'+2y=6.$$ When I do $$y'=2(3-y)\implies\int\frac{\mathrm dy}{3-y}=2\int\mathrm dx\implies-\ln{|3-y|}=2x+c\implies3-y=ke^{-2x}\therefore y=\boxed{3-ke^{-2x}},\quad c,k\in\Bbb R.$$ It satisfies the ODE because $$2ke^{-2x}+6-2ke^{-2x}=6=6.$$ However, when I try another solution, namely first solve the homogeneous equation: $$y'+2y=0\implies\int\frac{\mathrm dy}y=-2\int\mathrm dx\implies\ln{|y|}=-2x+c\implies y=ke^{-2x},\quad c,k\in\Bbb R$$ then $y_P=k(x)e^{-2x}$ , so then $$y'_P=k'(x)e^{-2x}-2k(x)e^{-2x}\implies k'(x)e^{-2x}-2k(x)e^{-2x}+2k(x)e^{-2x}=6\implies k'(x)=6e^{2x}\implies k(x)=3e^{2x}\implies y_P=3e^{2x}e^{-2x}=3\therefore y=y_H+y_P=\boxed{3+ke^{-2x}},$$ where this solution also satisfies $y'+2y=6$ , because $$-2ke^{-2x}+6+2ke^{-2x}=6=6.$$ My question is, how can we express both solutions with the same expression of $y$ ? I would like both solutions to be identical, but how? Thanks!","Solve When I do It satisfies the ODE because However, when I try another solution, namely first solve the homogeneous equation: then , so then where this solution also satisfies , because My question is, how can we express both solutions with the same expression of ? I would like both solutions to be identical, but how? Thanks!","y'+2y=6. y'=2(3-y)\implies\int\frac{\mathrm dy}{3-y}=2\int\mathrm dx\implies-\ln{|3-y|}=2x+c\implies3-y=ke^{-2x}\therefore y=\boxed{3-ke^{-2x}},\quad c,k\in\Bbb R. 2ke^{-2x}+6-2ke^{-2x}=6=6. y'+2y=0\implies\int\frac{\mathrm dy}y=-2\int\mathrm dx\implies\ln{|y|}=-2x+c\implies y=ke^{-2x},\quad c,k\in\Bbb R y_P=k(x)e^{-2x} y'_P=k'(x)e^{-2x}-2k(x)e^{-2x}\implies k'(x)e^{-2x}-2k(x)e^{-2x}+2k(x)e^{-2x}=6\implies k'(x)=6e^{2x}\implies k(x)=3e^{2x}\implies y_P=3e^{2x}e^{-2x}=3\therefore y=y_H+y_P=\boxed{3+ke^{-2x}}, y'+2y=6 -2ke^{-2x}+6+2ke^{-2x}=6=6. y",['ordinary-differential-equations']
70,Stability of a Degenerate Equilibrium Point in a Planar ODE,Stability of a Degenerate Equilibrium Point in a Planar ODE,,"Consider the planar ODE $\dot x_1 = x_2$ $\dot x_2 = - x_1^2 - 2 x_1 - 1$ Obliviously, $(x_1,x_2)=(-1,0)$ is an equilibrium point. The Jacobian matrix at this point is $$J = \begin{bmatrix}  0  & 1 \\ 0  & 0  \end{bmatrix}$$ Thus, linearizarion fails in determining the stability. How can we determine the stability of this equilibrium point?","Consider the planar ODE Obliviously, is an equilibrium point. The Jacobian matrix at this point is Thus, linearizarion fails in determining the stability. How can we determine the stability of this equilibrium point?","\dot x_1 = x_2 \dot x_2 = - x_1^2 - 2 x_1 - 1 (x_1,x_2)=(-1,0) J = \begin{bmatrix} 
0  & 1 \\
0  & 0 
\end{bmatrix}","['ordinary-differential-equations', 'dynamical-systems', 'control-theory']"
71,Solving $f''(t) = g(f(t))$,Solving,f''(t) = g(f(t)),"A recent question asked for a solution to $f'(t)= k f^2(t)$ .  I could do this.  In fact, it seems that a large class of similar differential equations $f'(t) = g(f(t))$ are not hard to solve. This led me to consider $f''(t) = g(f(t))$ . I started with a specific example: $f''(t)= k f^2(t)$ .  I could solve this but I did so by just guessing that there would be a solution of the form $a x^b$ .  I found $f(x) = \frac{6}{k x^2}$ as a specific solution. Obviously, for this case and others of this class, translations of a solution as also solutions. However, as $g$ becomes more complex, guesswork will probably fail me.  Is there any good way to approach equations of this form?","A recent question asked for a solution to .  I could do this.  In fact, it seems that a large class of similar differential equations are not hard to solve. This led me to consider . I started with a specific example: .  I could solve this but I did so by just guessing that there would be a solution of the form .  I found as a specific solution. Obviously, for this case and others of this class, translations of a solution as also solutions. However, as becomes more complex, guesswork will probably fail me.  Is there any good way to approach equations of this form?",f'(t)= k f^2(t) f'(t) = g(f(t)) f''(t) = g(f(t)) f''(t)= k f^2(t) a x^b f(x) = \frac{6}{k x^2} g,['ordinary-differential-equations']
72,How to solve this particular difference equations?,How to solve this particular difference equations?,,"Considere a real polynomial function $P_n\left(x\right)$ of two variable, when $n$ is a discrete variable and $x$ a continuous variable where $P_0\left(x\right)=1$ and satisfies the following recurrence relations: $$P_{n+1}\left(x\right)=\left(x+1\right)\cdot P_n\left(x\right)+x\cdot\frac{d}{dx}\left(P_n\left(x\right)\right).$$ How could I find the explicit solution $P_n\left(x\right)$ for this expression? And what name do these types of equations receive?","Considere a real polynomial function $P_n\left(x\right)$ of two variable, when $n$ is a discrete variable and $x$ a continuous variable where $P_0\left(x\right)=1$ and satisfies the following recurrence relations: $$P_{n+1}\left(x\right)=\left(x+1\right)\cdot P_n\left(x\right)+x\cdot\frac{d}{dx}\left(P_n\left(x\right)\right).$$ How could I find the explicit solution $P_n\left(x\right)$ for this expression? And what name do these types of equations receive?",,"['ordinary-differential-equations', 'recurrence-relations', 'delay-differential-equations']"
73,"References for ""Multidimensional Sturm-Liouville Theory""","References for ""Multidimensional Sturm-Liouville Theory""",,"I am interested in what I'll call (perhaps erroneously) multivariate Sturm–Liouville theory , i.e., solutions to equations of the form $$\nabla\cdot(P(x)\nabla Y)+Q(x)Y=-\lambda W(x)Y\tag{1}$$ for $\lambda\in\mathbb R$, $Y:\mathbb R^d\to\mathbb R$, and $P,Q,W:\mathbb R^d\to\mathbb R$. If $d=1$, then, as shown on the Wikipedia page, there is a very well developed theory for solutions of $(1)$ on an interval $[a,b]$ with fixed boundary conditions. This is called Sturm-Liouville theory. I suspect that such a theory has been generalized to higher dimensions. However, after googling for a while using keywords such as ""multivariate Sturm-Liouville theory"", I'm starting to suspect that the study of $(1)$ in higher dimensions has a different name, since I've completely failed to find good comprehensive resources on such problems. Question: Are there textbooks that treat multivariate problems such as $(1)$ in detail? I'm especially interested in fixed point-type arguments for existence and uniqueness, as well as continuity results for the solution $(Y,\lambda)$ with respect to the ""data"" $P,Q,W$.","I am interested in what I'll call (perhaps erroneously) multivariate Sturm–Liouville theory , i.e., solutions to equations of the form $$\nabla\cdot(P(x)\nabla Y)+Q(x)Y=-\lambda W(x)Y\tag{1}$$ for $\lambda\in\mathbb R$, $Y:\mathbb R^d\to\mathbb R$, and $P,Q,W:\mathbb R^d\to\mathbb R$. If $d=1$, then, as shown on the Wikipedia page, there is a very well developed theory for solutions of $(1)$ on an interval $[a,b]$ with fixed boundary conditions. This is called Sturm-Liouville theory. I suspect that such a theory has been generalized to higher dimensions. However, after googling for a while using keywords such as ""multivariate Sturm-Liouville theory"", I'm starting to suspect that the study of $(1)$ in higher dimensions has a different name, since I've completely failed to find good comprehensive resources on such problems. Question: Are there textbooks that treat multivariate problems such as $(1)$ in detail? I'm especially interested in fixed point-type arguments for existence and uniqueness, as well as continuity results for the solution $(Y,\lambda)$ with respect to the ""data"" $P,Q,W$.",,"['ordinary-differential-equations', 'analysis', 'reference-request', 'partial-differential-equations']"
74,Under what conditions a rational function has bounded derivative?,Under what conditions a rational function has bounded derivative?,,"Under what conditions a rational function has bounded derivative? This question arise to me when considering the following theorem: If $f \in C^1(I,\mathbb{R})$ where $I$ is an interval then: $f$ is globally lipschitz $\iff \exists L \ge 0.\forall  t \in I.|f'(t)| \le L $ So taking rational function $f(x) = \frac{p(x)}{q(x)}$ we have $f'(x) = \frac{p'(x)q(x)-p(x)q'(x)}{q(x)^2}$. My view I think I should assume that $f:\mathbb{R} \to \mathbb{R}$ so that $\forall x \in \mathbb{R}.q'(x) \neq 0$ (however this doesn't seem to be necesary). And then perhaps a condition on the degree guarantees boundedness...","Under what conditions a rational function has bounded derivative? This question arise to me when considering the following theorem: If $f \in C^1(I,\mathbb{R})$ where $I$ is an interval then: $f$ is globally lipschitz $\iff \exists L \ge 0.\forall  t \in I.|f'(t)| \le L $ So taking rational function $f(x) = \frac{p(x)}{q(x)}$ we have $f'(x) = \frac{p'(x)q(x)-p(x)q'(x)}{q(x)^2}$. My view I think I should assume that $f:\mathbb{R} \to \mathbb{R}$ so that $\forall x \in \mathbb{R}.q'(x) \neq 0$ (however this doesn't seem to be necesary). And then perhaps a condition on the degree guarantees boundedness...",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'lipschitz-functions', 'rational-functions']"
75,Obtain an explicit function from an implicit expression,Obtain an explicit function from an implicit expression,,"Let $y=y(x)$ be a function implicitly defined as $$xy+\ln(xy)=1$$ near a point $P(1,1)$. I have to find the explicit expression $y(x)$ as well as the values $y'(1)$ and $dy(1)$. I've tried applying the exponential to both sides but I could not find a solution: $$  e^{xy}xy=e  $$  doesn't seem to be very helpful. Also, using implicit differentiation didn't help me that much with obtaining $y'(1)$. If I am not mistaking, by implicit differentiation one obtains  $$y+xy'+\frac{1}{x}+\frac{y'}{y}=0$$ But I still can't see how I can obtain the value $y'(1)$ from this. As for $dy(1)$: I have no idea how I could obtain that! Any help/suggestions would be appreciated.","Let $y=y(x)$ be a function implicitly defined as $$xy+\ln(xy)=1$$ near a point $P(1,1)$. I have to find the explicit expression $y(x)$ as well as the values $y'(1)$ and $dy(1)$. I've tried applying the exponential to both sides but I could not find a solution: $$  e^{xy}xy=e  $$  doesn't seem to be very helpful. Also, using implicit differentiation didn't help me that much with obtaining $y'(1)$. If I am not mistaking, by implicit differentiation one obtains  $$y+xy'+\frac{1}{x}+\frac{y'}{y}=0$$ But I still can't see how I can obtain the value $y'(1)$ from this. As for $dy(1)$: I have no idea how I could obtain that! Any help/suggestions would be appreciated.",,"['calculus', 'real-analysis', 'ordinary-differential-equations']"
76,Lipschitz continuous ODE solution intersecting a hyperplane infinitely often in finite time,Lipschitz continuous ODE solution intersecting a hyperplane infinitely often in finite time,,"I'm trying to prove that a solution of globally Lipschitz continuous system of ODEs cannot intersect any hyperplane infinitely many times in a finite amount of time. So for example, something like a spiral which converges to it's centre in finite time is not possible, because the centre is an equilibrium and therefore the solution for that point is not unique (either a constant trajectory which stays in the equilibrium, or the spiral itself). Also, the solution can't blow up to infinity because the system is globally Lipschitz. My intuition so far is that even in more complex cases, this follows from the fact that Lipschitz continuous equations have unique solutions, which would break if the trajectory converges ""too fast"" to a specific point - like in the case of the spiral above. But I can't quite grasp how to show this for trajectories which do not converge to an equilibrium. For example, take the 2-dimensional above mentioned spiral, but set it into a 3-dimensional system such that $z' = 1$. The spiral still converges to it's centre in finite time, but the centre is not an equilibrium any more. How do I show that the solution is still not unique? Or is it also just ""obvious"" consequence of Picard–Lindelöf? :)","I'm trying to prove that a solution of globally Lipschitz continuous system of ODEs cannot intersect any hyperplane infinitely many times in a finite amount of time. So for example, something like a spiral which converges to it's centre in finite time is not possible, because the centre is an equilibrium and therefore the solution for that point is not unique (either a constant trajectory which stays in the equilibrium, or the spiral itself). Also, the solution can't blow up to infinity because the system is globally Lipschitz. My intuition so far is that even in more complex cases, this follows from the fact that Lipschitz continuous equations have unique solutions, which would break if the trajectory converges ""too fast"" to a specific point - like in the case of the spiral above. But I can't quite grasp how to show this for trajectories which do not converge to an equilibrium. For example, take the 2-dimensional above mentioned spiral, but set it into a 3-dimensional system such that $z' = 1$. The spiral still converges to it's centre in finite time, but the centre is not an equilibrium any more. How do I show that the solution is still not unique? Or is it also just ""obvious"" consequence of Picard–Lindelöf? :)",,"['ordinary-differential-equations', 'limits', 'dynamical-systems', 'lipschitz-functions']"
77,Domain of a solution of differential equation,Domain of a solution of differential equation,,"Given the following system of differential equations \begin{equation} \begin{cases} \frac{dx}{dt}=-x+xy \\\frac{dy}{dt}=-2y-x^2 \end{cases} \end{equation} I want to prove that the maximal solutions of the system are defined on all $\mathbb{R}$ and that $$\lim_{t\rightarrow +\infty } {(x(t),y(t))}=0$$ As shown here I tried proceeding same way but I don't have any information about $t=0$. $$x′x+y′y=-x^2-2y^2\leq 0$$ $$x′x+y′y= x'+y'\cdot x+y =\frac{1}{2}\frac{d}{dt}|| x+y ||^2$$ $$\frac{1}{2}\frac{d}{dt}|| x+y ||^2\leq 0$$ Hence I have information about the monotony, but I don't know how to show that, for example, the solution is limited on a subset of $\mathbb{R}$ (I could use this hypothesis to prolong the solution on all $\mathbb{R}$).","Given the following system of differential equations \begin{equation} \begin{cases} \frac{dx}{dt}=-x+xy \\\frac{dy}{dt}=-2y-x^2 \end{cases} \end{equation} I want to prove that the maximal solutions of the system are defined on all $\mathbb{R}$ and that $$\lim_{t\rightarrow +\infty } {(x(t),y(t))}=0$$ As shown here I tried proceeding same way but I don't have any information about $t=0$. $$x′x+y′y=-x^2-2y^2\leq 0$$ $$x′x+y′y= x'+y'\cdot x+y =\frac{1}{2}\frac{d}{dt}|| x+y ||^2$$ $$\frac{1}{2}\frac{d}{dt}|| x+y ||^2\leq 0$$ Hence I have information about the monotony, but I don't know how to show that, for example, the solution is limited on a subset of $\mathbb{R}$ (I could use this hypothesis to prolong the solution on all $\mathbb{R}$).",,"['real-analysis', 'ordinary-differential-equations', 'limits', 'dynamical-systems']"
78,Is it valid to integrate on both sides in order to simplify a differential equation?,Is it valid to integrate on both sides in order to simplify a differential equation?,,"So I am attempting to solve this equation $$y^{\prime\prime\prime} + y^{\prime\prime} +y^{\prime} = x+1. $$ Is it vaild to take the indefinite integral on both sides in order to simplify the expression and make it easier to solve? \begin{align*} \int (y^{\prime\prime\prime} + y^{\prime\prime} +y^{\prime})\, dx&= \int (x+1)\, dx\\ y^{\prime\prime} +y^{\prime} + y &= \frac{x^2}{2} +x + c \end{align*} Thanks!","So I am attempting to solve this equation $$y^{\prime\prime\prime} + y^{\prime\prime} +y^{\prime} = x+1. $$ Is it vaild to take the indefinite integral on both sides in order to simplify the expression and make it easier to solve? \begin{align*} \int (y^{\prime\prime\prime} + y^{\prime\prime} +y^{\prime})\, dx&= \int (x+1)\, dx\\ y^{\prime\prime} +y^{\prime} + y &= \frac{x^2}{2} +x + c \end{align*} Thanks!",,"['calculus', 'ordinary-differential-equations']"
79,Solving the differential equation $q \vec{v} \times \vec{B} + q \vec{E} = m \frac{d\vec{v}}{dt}$,Solving the differential equation,q \vec{v} \times \vec{B} + q \vec{E} = m \frac{d\vec{v}}{dt},"This is more of a physics-y question, but it's not the physics I care about here -- just the mathematics. In my physics class we learned a particle with constant velocity in a uniform magnetic field perpendicular to it causes it to rotated in a circle of constant radius. The professor told me how to ""prove this"", in essence solving the following: $$q \vec{v} \times \vec{B} = m \frac{d\vec{v}}{dt}$$ with $\vec{v}=\langle v_x, v_y, v_z \rangle$ and $\vec{B}=\langle 0,0,B \rangle$ with $B\in\mathbb R_{>0}$, and it worked out great! Got the equation for a circle. He then told me that if there was an electric field perpendicular to both the moving particle and field, I should get a cycloid from solving this (with $\vec{E}= \langle 0,E,0 \rangle$): $$q \vec{v} \times \vec{B} + q \vec{E} = m \frac{d\vec{v}}{dt}$$ And, well, I didn't get a cycloid as an answer, but another circle. Here's my attempt at this differential equation, does anyone see anything wrong? To clarify to those who haven't taken Electromagnetism, $q \in \mathbb R$ is charge, $m,B,E \in \mathbb R_{>0}$, and $v_x,v_y,v_z: \mathbb R \to \mathbb R$. So let $\vec{v}=\langle v_x, v_y, v_z \rangle$, $\vec{B}=\langle 0,0,B \rangle$, and $\vec{E}=\langle 0,E,0 \rangle$. Then $\begin{align*} &\ q \vec{v} \times \vec{B} + q \vec{E} = m \frac{d\vec{v}}{dt} \\ &\implies \langle qv_yB, -qv_xB+qE \rangle = \langle m \frac{dv_x}{dt}, m \frac{dv_y}{dt} \rangle \\ &\implies \begin{cases} \frac{dv_x}{dt} = \frac{q}{m} v_y B \\ \frac{dv_y}{dt}=-\frac{q}{m} v_xB+\frac{q}{m}E \end{cases} \\ &\implies \frac{d^2v_x}{dt^2} = \frac{qB}{m} \frac{dv_y}{dt} \\ &\implies \frac{dv_y}{dt} = \frac{m}{qB}\frac{d^2v_x}{dt^2} \\ &\implies \frac{m}{qB} \frac{d^2v_x}{dt^2}=-\frac{qB}{m}v_x+\frac{qE}{m} \\ &\implies \frac{d^2 v_x}{dt^2}=-\frac{q^2B^2}{m^2}v_x+\frac{q^2BE}{m^2} \\ &\implies \frac{d^2 v_x}{dt^2} + \frac{q^2B^2}{m^2}v_x = \frac{q^2BE}{m^2} \end{align*}$ Now the auxiliary equation of the $LHS$ is  $$g^2+\frac{q^2B^2}{m^2}=0\implies g=\pm \frac{qB}{m} i$$ so we get $(v_x)_c=c_1 \cos\left(\frac{qB}{m}t\right)+c_2\sin\left(\frac{qB}{m}t\right)$. And $$RHS=\frac{q^2BE}{m^2} \implies \begin{cases} (v_x)_p = A \\ (v_x)_p' = 0 \\ (v_x)_p''=0\end{cases} \implies (0)+\frac{q^2B^2}{m^2}(A) = \frac{q^2BE}{m^2} \implies A = \frac{E}{B}$$ Thus, $$v_x = (v_x)_c+(v_x)_p = c_1 \cos\left(\frac{qB}{m}t\right)+c_2\sin\left(\frac{qB}{m}t\right) + \frac{E}{B}.$$ Now back from the top, we knew $$\frac{dv_x}{dt} = \frac{qB}{m} v_y \implies v_y = \frac{m}{qB} \frac{dv_x}{dt}$$ and $$\frac{dv_x}{dt} = \frac{c_2qB}{m} \cos \left(\frac{qB}{m}t\right)-\frac{c_1qB}{m} \sin \left(\frac{qB}{m}t\right)$$ so we get $$v_y =  c_2 \cos \left(\frac{qB}{m}t\right)- c_1 \sin \left(\frac{qB}{m}t\right)$$ and in totality $$\vec{v} = \left[c_1 \cos\left(\frac{qB}{m}t\right)+c_2\sin\left(\frac{qB}{m}t\right) + \frac{E}{B} \right] \hat{i} + \left[ c_2 \cos \left(\frac{qB}{m}t\right)- c_1 \sin \left(\frac{qB}{m}t\right) \right] \hat{j}.$$ This doesn't even look remotely close to an equation of a cycloid . Does someone know where my mistake may be? Edit: Upon integrating, I arrived at $$\vec{x}= \left[ \frac{c_1m}{qB} \sin \left( \frac{qB}{m}t \right) - \frac{c_2m}{qB} \cos \left( \frac{qB}{m}t \right) + \frac{E}{B}t \right] \hat{i} + \left[ \frac{c_2 m}{qB} \sin \left( \frac{qB}{m}t \right) + \frac{c_1 m}{qB} \cos \left( \frac{qB}{m}t \right) \right] \hat{j}$$ which sadly still doesn't seem to be a cycloid on Wolfram after attempting multiple different values for the constants. Am I missing something?","This is more of a physics-y question, but it's not the physics I care about here -- just the mathematics. In my physics class we learned a particle with constant velocity in a uniform magnetic field perpendicular to it causes it to rotated in a circle of constant radius. The professor told me how to ""prove this"", in essence solving the following: $$q \vec{v} \times \vec{B} = m \frac{d\vec{v}}{dt}$$ with $\vec{v}=\langle v_x, v_y, v_z \rangle$ and $\vec{B}=\langle 0,0,B \rangle$ with $B\in\mathbb R_{>0}$, and it worked out great! Got the equation for a circle. He then told me that if there was an electric field perpendicular to both the moving particle and field, I should get a cycloid from solving this (with $\vec{E}= \langle 0,E,0 \rangle$): $$q \vec{v} \times \vec{B} + q \vec{E} = m \frac{d\vec{v}}{dt}$$ And, well, I didn't get a cycloid as an answer, but another circle. Here's my attempt at this differential equation, does anyone see anything wrong? To clarify to those who haven't taken Electromagnetism, $q \in \mathbb R$ is charge, $m,B,E \in \mathbb R_{>0}$, and $v_x,v_y,v_z: \mathbb R \to \mathbb R$. So let $\vec{v}=\langle v_x, v_y, v_z \rangle$, $\vec{B}=\langle 0,0,B \rangle$, and $\vec{E}=\langle 0,E,0 \rangle$. Then $\begin{align*} &\ q \vec{v} \times \vec{B} + q \vec{E} = m \frac{d\vec{v}}{dt} \\ &\implies \langle qv_yB, -qv_xB+qE \rangle = \langle m \frac{dv_x}{dt}, m \frac{dv_y}{dt} \rangle \\ &\implies \begin{cases} \frac{dv_x}{dt} = \frac{q}{m} v_y B \\ \frac{dv_y}{dt}=-\frac{q}{m} v_xB+\frac{q}{m}E \end{cases} \\ &\implies \frac{d^2v_x}{dt^2} = \frac{qB}{m} \frac{dv_y}{dt} \\ &\implies \frac{dv_y}{dt} = \frac{m}{qB}\frac{d^2v_x}{dt^2} \\ &\implies \frac{m}{qB} \frac{d^2v_x}{dt^2}=-\frac{qB}{m}v_x+\frac{qE}{m} \\ &\implies \frac{d^2 v_x}{dt^2}=-\frac{q^2B^2}{m^2}v_x+\frac{q^2BE}{m^2} \\ &\implies \frac{d^2 v_x}{dt^2} + \frac{q^2B^2}{m^2}v_x = \frac{q^2BE}{m^2} \end{align*}$ Now the auxiliary equation of the $LHS$ is  $$g^2+\frac{q^2B^2}{m^2}=0\implies g=\pm \frac{qB}{m} i$$ so we get $(v_x)_c=c_1 \cos\left(\frac{qB}{m}t\right)+c_2\sin\left(\frac{qB}{m}t\right)$. And $$RHS=\frac{q^2BE}{m^2} \implies \begin{cases} (v_x)_p = A \\ (v_x)_p' = 0 \\ (v_x)_p''=0\end{cases} \implies (0)+\frac{q^2B^2}{m^2}(A) = \frac{q^2BE}{m^2} \implies A = \frac{E}{B}$$ Thus, $$v_x = (v_x)_c+(v_x)_p = c_1 \cos\left(\frac{qB}{m}t\right)+c_2\sin\left(\frac{qB}{m}t\right) + \frac{E}{B}.$$ Now back from the top, we knew $$\frac{dv_x}{dt} = \frac{qB}{m} v_y \implies v_y = \frac{m}{qB} \frac{dv_x}{dt}$$ and $$\frac{dv_x}{dt} = \frac{c_2qB}{m} \cos \left(\frac{qB}{m}t\right)-\frac{c_1qB}{m} \sin \left(\frac{qB}{m}t\right)$$ so we get $$v_y =  c_2 \cos \left(\frac{qB}{m}t\right)- c_1 \sin \left(\frac{qB}{m}t\right)$$ and in totality $$\vec{v} = \left[c_1 \cos\left(\frac{qB}{m}t\right)+c_2\sin\left(\frac{qB}{m}t\right) + \frac{E}{B} \right] \hat{i} + \left[ c_2 \cos \left(\frac{qB}{m}t\right)- c_1 \sin \left(\frac{qB}{m}t\right) \right] \hat{j}.$$ This doesn't even look remotely close to an equation of a cycloid . Does someone know where my mistake may be? Edit: Upon integrating, I arrived at $$\vec{x}= \left[ \frac{c_1m}{qB} \sin \left( \frac{qB}{m}t \right) - \frac{c_2m}{qB} \cos \left( \frac{qB}{m}t \right) + \frac{E}{B}t \right] \hat{i} + \left[ \frac{c_2 m}{qB} \sin \left( \frac{qB}{m}t \right) + \frac{c_1 m}{qB} \cos \left( \frac{qB}{m}t \right) \right] \hat{j}$$ which sadly still doesn't seem to be a cycloid on Wolfram after attempting multiple different values for the constants. Am I missing something?",,"['ordinary-differential-equations', 'physics']"
80,Von Bertalanffy model of tumor growth and Gompertz model - equilibria and stability,Von Bertalanffy model of tumor growth and Gompertz model - equilibria and stability,,"I'm having trouble reviewing for a calc test and I'm stumbling on this particular question. If I could get a step by step or solution to study then that would be more than helpful! Studying a growth rate for a cucumber using the following the equation: a) Von Bertalanffy model of tumor growth b) $y'=\alpha (1-\frac{ln(y)}{ln(C)})y$,$y(0)=y_0$ - this is a Gompertz model 1) Find the equilibria of the equation 2) Find whether each equilibrium is stable or unstable. And my question also is how to write this two model with parameters.","I'm having trouble reviewing for a calc test and I'm stumbling on this particular question. If I could get a step by step or solution to study then that would be more than helpful! Studying a growth rate for a cucumber using the following the equation: a) Von Bertalanffy model of tumor growth b) $y'=\alpha (1-\frac{ln(y)}{ln(C)})y$,$y(0)=y_0$ - this is a Gompertz model 1) Find the equilibria of the equation 2) Find whether each equilibrium is stable or unstable. And my question also is how to write this two model with parameters.",,"['calculus', 'ordinary-differential-equations', 'mathematical-modeling', 'biology']"
81,General closed form solution to $f'(x) = P(f(x))/P(x)$,General closed form solution to,f'(x) = P(f(x))/P(x),"Does there exist a general closed form solution (in terms of elementary or special functions) to the differential equation: $$ \frac{df(x)}{dx} = \frac{P(f(x))}{P(x)} $$ when $P(x)$ is a polynomial of degree higher than 3? (excluding the trivial case $f(x)=x$). Context: I'm trying to find the action of a certain class of composition operators $$C_f(x,\frac{d}{dx}) = e^{P(x) \frac{d}{dx}} $$ where $P(x)$ is a polynomial in $\mathbb{C}$ of degree $n \geq 3$, such that for a complex function $g$ $$C_f(g) = g \circ f$$ After some manipulations, one arrives at the Abel equation $$ f(x) = \alpha^{-1}(\alpha(x) + 1) $$ where $$ \alpha(x) = \int^x \frac{dt}{P(t)} $$ Differentiating this last expression, one obtains a differential equation that all the family of iterations of $f$ (even fractional ones) must satisfy: $$ \frac{df(x)}{dx} = \frac{P(f(x))}{P(x)} $$ I already know the basic properties of this function, and I know how to calculate it numerically. What I'm trying to find is whether there exists a general closed form expression for $f$ when $\deg P \geq 3$ (in the case $n \leq 2$, $f(x)$ is a Möbius transformation). There are some special cases I've checked manually, such as the case $P(x) = ax^n$, whose solution is a combination of a rational function and radicals, but I don't know if this holds in general, or how to prove it.","Does there exist a general closed form solution (in terms of elementary or special functions) to the differential equation: $$ \frac{df(x)}{dx} = \frac{P(f(x))}{P(x)} $$ when $P(x)$ is a polynomial of degree higher than 3? (excluding the trivial case $f(x)=x$). Context: I'm trying to find the action of a certain class of composition operators $$C_f(x,\frac{d}{dx}) = e^{P(x) \frac{d}{dx}} $$ where $P(x)$ is a polynomial in $\mathbb{C}$ of degree $n \geq 3$, such that for a complex function $g$ $$C_f(g) = g \circ f$$ After some manipulations, one arrives at the Abel equation $$ f(x) = \alpha^{-1}(\alpha(x) + 1) $$ where $$ \alpha(x) = \int^x \frac{dt}{P(t)} $$ Differentiating this last expression, one obtains a differential equation that all the family of iterations of $f$ (even fractional ones) must satisfy: $$ \frac{df(x)}{dx} = \frac{P(f(x))}{P(x)} $$ I already know the basic properties of this function, and I know how to calculate it numerically. What I'm trying to find is whether there exists a general closed form expression for $f$ when $\deg P \geq 3$ (in the case $n \leq 2$, $f(x)$ is a Möbius transformation). There are some special cases I've checked manually, such as the case $P(x) = ax^n$, whose solution is a combination of a rational function and radicals, but I don't know if this holds in general, or how to prove it.",,"['ordinary-differential-equations', 'function-and-relation-composition']"
82,Solve integral (convolution) equation,Solve integral (convolution) equation,,"Given a function: $u(t) = \exp\left( -\frac{At^2}{1+t}\right),$ $A>0, t>0,$ and an equation: $\frac{d u(t)}{dt} = \int^{t}_0 \phi(t-\tau) u(\tau) d \tau .$ How to find a closed expression for $\phi(t)$?","Given a function: $u(t) = \exp\left( -\frac{At^2}{1+t}\right),$ $A>0, t>0,$ and an equation: $\frac{d u(t)}{dt} = \int^{t}_0 \phi(t-\tau) u(\tau) d \tau .$ How to find a closed expression for $\phi(t)$?",,"['ordinary-differential-equations', 'partial-differential-equations', 'laplace-transform', 'convolution', 'integral-equations']"
83,What is the antiderivative of $\left(\frac{dy}{dx}\right)^3$,What is the antiderivative of,\left(\frac{dy}{dx}\right)^3,I have the following differential equation: $$\frac{dy}{dx} + \left(\frac{dy}{dx}\right)^3 = C$$ How would one solve this? I tried integrating both sides and I got: $$y + \int\left(\frac{dy}{dx}\right)^3\;dx = Cx + D$$ What is $$\int\left(\frac{dy}{dx}\right)^3\;dx ?$$ Thank you for the help =),I have the following differential equation: $$\frac{dy}{dx} + \left(\frac{dy}{dx}\right)^3 = C$$ How would one solve this? I tried integrating both sides and I got: $$y + \int\left(\frac{dy}{dx}\right)^3\;dx = Cx + D$$ What is $$\int\left(\frac{dy}{dx}\right)^3\;dx ?$$ Thank you for the help =),,['ordinary-differential-equations']
84,Need explanation for simple differential equation,Need explanation for simple differential equation,,"I can't figure out this really simple linear equation: $$x'=x$$ I know that the result should be an exponential function with $t$ in the exponent, but I can't really say why. I tried integrating both sides but it doesn't seem to work. I know this is shameful noob question, but I would be grateful for any hints.","I can't figure out this really simple linear equation: $$x'=x$$ I know that the result should be an exponential function with $t$ in the exponent, but I can't really say why. I tried integrating both sides but it doesn't seem to work. I know this is shameful noob question, but I would be grateful for any hints.",,"['ordinary-differential-equations', 'exponential-function']"
85,Solution to ODE Abel Equation,Solution to ODE Abel Equation,,"I aim to find the exact form solution to the this ODE: $$\frac{dS}{dw}S = \frac{a}{w}S^2 + \frac{b}{w}S - c$$ where S is a continuous differentiable function of w, real positive and a, b, c are positive, non zero, real values. I follow the procedure in: Panayotounakos, D. E. and Zarmpoutis, T. I. (2011). Construction of Exact Parametric or Closed Form Solutions of Some Unsolvable Classes of Nonlinear ODEs (Abel's Nonlinear ODEs of the First Kind and Relative Degenerate Equations). International Journal of Mathematics and Mathematical Sciences, 2011. In particular I move from eq. 4.3 in the paper. I obtain a particular solution of the form: $$S(w) = K w^{-1/2} $$ where K is a combination of the parameters a, b, c. But this form is not a solution to the ODE I started from. Could you please help me to find the solution? Where am I wrong? Thanks.","I aim to find the exact form solution to the this ODE: $$\frac{dS}{dw}S = \frac{a}{w}S^2 + \frac{b}{w}S - c$$ where S is a continuous differentiable function of w, real positive and a, b, c are positive, non zero, real values. I follow the procedure in: Panayotounakos, D. E. and Zarmpoutis, T. I. (2011). Construction of Exact Parametric or Closed Form Solutions of Some Unsolvable Classes of Nonlinear ODEs (Abel's Nonlinear ODEs of the First Kind and Relative Degenerate Equations). International Journal of Mathematics and Mathematical Sciences, 2011. In particular I move from eq. 4.3 in the paper. I obtain a particular solution of the form: $$S(w) = K w^{-1/2} $$ where K is a combination of the parameters a, b, c. But this form is not a solution to the ODE I started from. Could you please help me to find the solution? Where am I wrong? Thanks.",,['ordinary-differential-equations']
86,Initial Value Problem with Repeated Eigenvalues,Initial Value Problem with Repeated Eigenvalues,,"Given the matrix $$ A=\left(\begin{array}{ccc} 1 & 0 & 1 \\ 1 & 1 & 0 \\ 0 & 0 & 1\end{array}\right) $$ For $X'= AX.\quad$ $X\left(0\right)=\left(\begin{array}{r}1 \\ 0 \\ -2\end{array}\right)\,.\quad$ What is the solution ?.","Given the matrix $$ A=\left(\begin{array}{ccc} 1 & 0 & 1 \\ 1 & 1 & 0 \\ 0 & 0 & 1\end{array}\right) $$ For $X'= AX.\quad$ $X\left(0\right)=\left(\begin{array}{r}1 \\ 0 \\ -2\end{array}\right)\,.\quad$ What is the solution ?.",,['ordinary-differential-equations']
87,Proof: Gradient of a Hamiltonian System,Proof: Gradient of a Hamiltonian System,,I am trying to prove the following: Given that $f \in C^1(E) $ where E is a open simply connected subsets of the plane.  Show that the system $\dot x=f(x)$ is a hamiltonian if and only if $\nabla \cdot f=0$ for all $x \in E.$ So $\nabla \cdot f= \partial f/ \partial xe_x +\partial f/ \partial y e_y =0$.  I am confused as to where to go from here in order to prove it is a hamiltonian...,I am trying to prove the following: Given that $f \in C^1(E) $ where E is a open simply connected subsets of the plane.  Show that the system $\dot x=f(x)$ is a hamiltonian if and only if $\nabla \cdot f=0$ for all $x \in E.$ So $\nabla \cdot f= \partial f/ \partial xe_x +\partial f/ \partial y e_y =0$.  I am confused as to where to go from here in order to prove it is a hamiltonian...,,"['ordinary-differential-equations', 'physics', 'hamiltonian-path']"
88,Lyapunov Stability,Lyapunov Stability,,"Let $\dot{x}=v(x)$ with $v(x)=Ax+O(\left \| x \right \|^2)$, $v\in C^k(U)$, $U\subset \mathbb{R}^n$, $  n\geq 2$, is true or not that if the origin is a singular point Lyapunov stable for $\dot{x}=Ax$? then is a point Lyapunov stable for $\dot{x}=v(x)$ I think this is false, let ${x}'=-y+x^3 \\ {y}'=x+y^3$ so we have the origin is a singular point Lyapunov stable for $\dot{x}=Ax$ because if I take $g(x,y)=\frac{x^2}{2}+\frac{y^2}{2}+1>0$ then I have $\bigtriangledown g(x,y)(-y,x)=0$ then the origin is Lyapunov stable, then I would find another $g´$ such that $g´(x,y)({x}',{y}')>0$  bye Lyapunov´s Stability theorem I will have that the origin is not stable, well that´s my idea, I just want to know if I'm on the right way Thanks for any comments!","Let $\dot{x}=v(x)$ with $v(x)=Ax+O(\left \| x \right \|^2)$, $v\in C^k(U)$, $U\subset \mathbb{R}^n$, $  n\geq 2$, is true or not that if the origin is a singular point Lyapunov stable for $\dot{x}=Ax$? then is a point Lyapunov stable for $\dot{x}=v(x)$ I think this is false, let ${x}'=-y+x^3 \\ {y}'=x+y^3$ so we have the origin is a singular point Lyapunov stable for $\dot{x}=Ax$ because if I take $g(x,y)=\frac{x^2}{2}+\frac{y^2}{2}+1>0$ then I have $\bigtriangledown g(x,y)(-y,x)=0$ then the origin is Lyapunov stable, then I would find another $g´$ such that $g´(x,y)({x}',{y}')>0$  bye Lyapunov´s Stability theorem I will have that the origin is not stable, well that´s my idea, I just want to know if I'm on the right way Thanks for any comments!",,"['ordinary-differential-equations', 'partial-differential-equations']"
89,Solve differential equation $y'' = -a y +\frac by$,Solve differential equation,y'' = -a y +\frac by,"I am trying to solve for y(t): $$y'' =-ay + \frac by$$ I have tried a lot, but haven't succeeded so far. Actually I am not sure there is a 'nice' solution. Do any of you have ideas of how to solve this?","I am trying to solve for y(t): $$y'' =-ay + \frac by$$ I have tried a lot, but haven't succeeded so far. Actually I am not sure there is a 'nice' solution. Do any of you have ideas of how to solve this?",,['ordinary-differential-equations']
90,Sturm-Liouville Questions,Sturm-Liouville Questions,,"In thinking about Sturm-Liouville theory a bit I see I have no actual idea what is going on. The first issue I have is that my book began with the statement that given $$L[y] = a(x)y'' + b(x)y' + c(x)y = f(x)$$ the problem $L[y] \ = \ f$ can be re-cast in the form $L[y] \ = \ \lambda y$. Now it could be a typo on their part but I see no justification for the way you can just do that! More importantly though is the motivation for Sturm-Liouville theory in the first place. The story as I know it is as follows: Given a linear second order ode $$F(x,y,y',y'') = L[y] = a(x)y'' + b(x)y' + c(x)y = f(x)$$ it is an exact equation if it is derivable from a differential equation of one order lower, i.e. $$F(x,y,y',y'') = \frac{d}{dx}g(x,y,y').$$ The equation is exact iff $$a''(x) - b'(x) + c(x) = 0. $$ If $F$ is not exact it can be made exact on multiplication by a suitable integrating factor $\alpha(x)$. This equation is exact iff $$(\alpha(x)a(x))'' - (\alpha(x)b(x))' + \alpha(x)c(x) = 0 $$ If you expand this out you get the Adjoint operator $$L^*[\alpha(x)] \ = \ (\alpha(x)a(x))'' \ - \ (\alpha(x)b(x))' \ + \ \alpha(x)c(x) \ = 0 $$ If you expand $L^*$ you see that we can satisfy $L \ = \ L^*$ if $a'(x) \ = \ b(x)$ & $a''(x) \ = \ b'(x)$ which then turns $L[y]$ into something of the form $$L[y] \ = \ \frac{d}{dx}[a(x)y'] \ + \ c(x)y \ = \ f(x).$$ Thus we seek an integratiing factor $\alpha(x)$ so that we can satisfy this & the condition this will hold is that $\alpha(x) \ = \ \frac{1}{a(x)}e^{\int\frac{b(x)}{a(x)}dx}$ Then we're dealing with: $$\frac{d}{dx}[\alpha(x)a(x)y'] \ + \ \alpha(x)c(x)y \ = \ \alpha(x)f(x)$$ But again, by what my book said they magically re-cast this problem as $$\frac{d}{dx}[\alpha(x)a(x)y'] \ + \ \alpha(x)c(x)y \ = \ \lambda \alpha(x) y(x)$$ Then calling $$\frac{d}{dx}[\alpha(x)a(x)y'] \ + \ ( \alpha(x)c(x)y \ - \ \lambda \alpha(x) )y(x) \ = \ 0$$ a Sturm-Liouville problem. My question is, how can I make sense of everything I wrote above? How can I clean it up & interpret it, like at one stage I thought we were turning our 2nd order ode into something so that it reduces to the derivative of a first order ode so we can easily find first integrals then the next moment we're pulling out eigenvalues & finding full solutions - what's going on? I want to be able to look at $a(x)y'' \ + \ b(x)y' \ + \ c(x)y \ = \ f(x)$ & know how & why we're turning this into a Sturm-Liouville problem in a way that makes sense of exactness & integrating factors, thanks for reading!","In thinking about Sturm-Liouville theory a bit I see I have no actual idea what is going on. The first issue I have is that my book began with the statement that given $$L[y] = a(x)y'' + b(x)y' + c(x)y = f(x)$$ the problem $L[y] \ = \ f$ can be re-cast in the form $L[y] \ = \ \lambda y$. Now it could be a typo on their part but I see no justification for the way you can just do that! More importantly though is the motivation for Sturm-Liouville theory in the first place. The story as I know it is as follows: Given a linear second order ode $$F(x,y,y',y'') = L[y] = a(x)y'' + b(x)y' + c(x)y = f(x)$$ it is an exact equation if it is derivable from a differential equation of one order lower, i.e. $$F(x,y,y',y'') = \frac{d}{dx}g(x,y,y').$$ The equation is exact iff $$a''(x) - b'(x) + c(x) = 0. $$ If $F$ is not exact it can be made exact on multiplication by a suitable integrating factor $\alpha(x)$. This equation is exact iff $$(\alpha(x)a(x))'' - (\alpha(x)b(x))' + \alpha(x)c(x) = 0 $$ If you expand this out you get the Adjoint operator $$L^*[\alpha(x)] \ = \ (\alpha(x)a(x))'' \ - \ (\alpha(x)b(x))' \ + \ \alpha(x)c(x) \ = 0 $$ If you expand $L^*$ you see that we can satisfy $L \ = \ L^*$ if $a'(x) \ = \ b(x)$ & $a''(x) \ = \ b'(x)$ which then turns $L[y]$ into something of the form $$L[y] \ = \ \frac{d}{dx}[a(x)y'] \ + \ c(x)y \ = \ f(x).$$ Thus we seek an integratiing factor $\alpha(x)$ so that we can satisfy this & the condition this will hold is that $\alpha(x) \ = \ \frac{1}{a(x)}e^{\int\frac{b(x)}{a(x)}dx}$ Then we're dealing with: $$\frac{d}{dx}[\alpha(x)a(x)y'] \ + \ \alpha(x)c(x)y \ = \ \alpha(x)f(x)$$ But again, by what my book said they magically re-cast this problem as $$\frac{d}{dx}[\alpha(x)a(x)y'] \ + \ \alpha(x)c(x)y \ = \ \lambda \alpha(x) y(x)$$ Then calling $$\frac{d}{dx}[\alpha(x)a(x)y'] \ + \ ( \alpha(x)c(x)y \ - \ \lambda \alpha(x) )y(x) \ = \ 0$$ a Sturm-Liouville problem. My question is, how can I make sense of everything I wrote above? How can I clean it up & interpret it, like at one stage I thought we were turning our 2nd order ode into something so that it reduces to the derivative of a first order ode so we can easily find first integrals then the next moment we're pulling out eigenvalues & finding full solutions - what's going on? I want to be able to look at $a(x)y'' \ + \ b(x)y' \ + \ c(x)y \ = \ f(x)$ & know how & why we're turning this into a Sturm-Liouville problem in a way that makes sense of exactness & integrating factors, thanks for reading!",,['ordinary-differential-equations']
91,Solving a 5 dimensional function in a neighbourhood,Solving a 5 dimensional function in a neighbourhood,,"Consider a function $f:\mathbb{R}^5 \to \mathbb{R}^2$ defined by $$f(u,v,w,x,y)=(uy+vx+w+x^2,uvw+x+y+1)$$ such that $f(2,1,0,-1,0)=(0,0)$ (i) Show that we can solve $f(u,v,w,x,y) = (0,0)$ for $(x,y)$ in terms of (u,v,w) in a neighbourhood of $(2,1,0)$. (ii)If $(x,y) = \phi(u,v,w)$ is the solution for (i) then show that derivative of $\phi$ at $(2,1,0)$ is $$D\phi(2,1,0)=\frac13 \begin{bmatrix}         0 & -1 & -3 \\         0 & 1 & -3 \\         \end{bmatrix}$$ Here's how I tried: Let $F=uy+vx+w+x^2=0$ & $G=uvw+x+y+1=0$ $$\frac{\partial(F,G)}{\partial(x,y)}_{(2,1,0,-1,0)} = \begin{bmatrix}         v+2x & u \\         1 & 1 \\         \end{bmatrix}$$ $$\qquad \qquad \qquad= \begin{bmatrix}         -1 & 2 \\         1 & 1 \\         \end{bmatrix}$$ which is non singular, so solution exists. Part (ii): we can write $$x=X(u,v,w)$$ $$y=Y(u,v,w)$$ defined in the neighbourhood of $(2,1,0)$ such that: $$X(2,1,0)=-1$$ $$Y(2,1,0)=0$$ what to do next? How to find $x=X(u,v,w)$ & $y=Y(u,v,w)$?","Consider a function $f:\mathbb{R}^5 \to \mathbb{R}^2$ defined by $$f(u,v,w,x,y)=(uy+vx+w+x^2,uvw+x+y+1)$$ such that $f(2,1,0,-1,0)=(0,0)$ (i) Show that we can solve $f(u,v,w,x,y) = (0,0)$ for $(x,y)$ in terms of (u,v,w) in a neighbourhood of $(2,1,0)$. (ii)If $(x,y) = \phi(u,v,w)$ is the solution for (i) then show that derivative of $\phi$ at $(2,1,0)$ is $$D\phi(2,1,0)=\frac13 \begin{bmatrix}         0 & -1 & -3 \\         0 & 1 & -3 \\         \end{bmatrix}$$ Here's how I tried: Let $F=uy+vx+w+x^2=0$ & $G=uvw+x+y+1=0$ $$\frac{\partial(F,G)}{\partial(x,y)}_{(2,1,0,-1,0)} = \begin{bmatrix}         v+2x & u \\         1 & 1 \\         \end{bmatrix}$$ $$\qquad \qquad \qquad= \begin{bmatrix}         -1 & 2 \\         1 & 1 \\         \end{bmatrix}$$ which is non singular, so solution exists. Part (ii): we can write $$x=X(u,v,w)$$ $$y=Y(u,v,w)$$ defined in the neighbourhood of $(2,1,0)$ such that: $$X(2,1,0)=-1$$ $$Y(2,1,0)=0$$ what to do next? How to find $x=X(u,v,w)$ & $y=Y(u,v,w)$?",,"['real-analysis', 'ordinary-differential-equations', 'multivariable-calculus']"
92,Looking for help with a proof that n-th derivative of $e^\frac{-1}{x^2} = 0$ for $x=0$. [duplicate],Looking for help with a proof that n-th derivative of  for . [duplicate],e^\frac{-1}{x^2} = 0 x=0,This question already has answers here : Real analysis question $e^{-1/x^2}$ (3 answers) Closed 2 years ago . Given the function $$ f(x) = \left\{\begin{array}{cc} e^{- \frac{1}{x^2}} & x \neq 0 \\ 0 & x = 0 \end{array}\right. $$ show that $\forall_{n\in \Bbb N} f^{(n)}(0) = 0$. So I have to show that nth derivative is always equal to zero $0$. Now I guess that it is about finding some dependencies between the previous and next differential but I have yet to notice one. Could you be so kind to help me with that? Thanks in advance!,This question already has answers here : Real analysis question $e^{-1/x^2}$ (3 answers) Closed 2 years ago . Given the function $$ f(x) = \left\{\begin{array}{cc} e^{- \frac{1}{x^2}} & x \neq 0 \\ 0 & x = 0 \end{array}\right. $$ show that $\forall_{n\in \Bbb N} f^{(n)}(0) = 0$. So I have to show that nth derivative is always equal to zero $0$. Now I guess that it is about finding some dependencies between the previous and next differential but I have yet to notice one. Could you be so kind to help me with that? Thanks in advance!,,"['analysis', 'ordinary-differential-equations', 'derivatives']"
93,Solving $f'(x) +f(x)=cf(x-1)$,Solving,f'(x) +f(x)=cf(x-1),"To show that $f(x) =Ae^{nx}$ for constant $n$ and $A$ starting with this thing: $$f'(x) +f(x)=cf(x-1)$$ Where $c$ is constant and $c\not= 0$. If it wasn't for the $f(x-1)$ bit, I would just use the integrating factor where $I=e^x$ and plug it into the equation. But the $f(x)$ throws me off, so I would have to put it into a form like $\frac{dy}{dx}+y=?$, in order to feel ok about it. EDIT: Oh as somebody rightly pointed out this is only for the condition when $n$ satistfies $$n+1 = ce^{-n}$$","To show that $f(x) =Ae^{nx}$ for constant $n$ and $A$ starting with this thing: $$f'(x) +f(x)=cf(x-1)$$ Where $c$ is constant and $c\not= 0$. If it wasn't for the $f(x-1)$ bit, I would just use the integrating factor where $I=e^x$ and plug it into the equation. But the $f(x)$ throws me off, so I would have to put it into a form like $\frac{dy}{dx}+y=?$, in order to feel ok about it. EDIT: Oh as somebody rightly pointed out this is only for the condition when $n$ satistfies $$n+1 = ce^{-n}$$",,"['calculus', 'ordinary-differential-equations']"
94,Why some differential equation can be solved while similar difference equations cannot?,Why some differential equation can be solved while similar difference equations cannot?,,Take an equation $$w'+w-w^2-1=0$$ Its solution is $$w(x)=\frac{\sqrt{3}}{2} \tan \left( \frac{\sqrt{3}}2 C+\frac{\sqrt{3}}2 x\right)+\frac12$$ I wonder why a similar difference equation $$\Delta w+w-w^2-1=0$$ cannot be solved?,Take an equation $$w'+w-w^2-1=0$$ Its solution is $$w(x)=\frac{\sqrt{3}}{2} \tan \left( \frac{\sqrt{3}}2 C+\frac{\sqrt{3}}2 x\right)+\frac12$$ I wonder why a similar difference equation $$\Delta w+w-w^2-1=0$$ cannot be solved?,,"['ordinary-differential-equations', 'recurrence-relations', 'finite-differences']"
95,Solving $\frac{dy}{dx} = xy^2$,Solving,\frac{dy}{dx} = xy^2,"This problem appears to be pretty simple to me but my book gets a different answer. $$\frac{dy}{dx} = xy^2$$ For when y is not 0 $$\frac{dy}{y^2} = x \, dx$$ $$\int \frac{dy}{y^2} = \int x \, dx$$ $$\frac{-1}{y^1} = \frac{x^2}{2}$$ $$\frac{-2}{x^2} = y$$ Is there anything wrong with this solution? It is not what my book gets but it is similar to how they do it in the example.","This problem appears to be pretty simple to me but my book gets a different answer. $$\frac{dy}{dx} = xy^2$$ For when y is not 0 $$\frac{dy}{y^2} = x \, dx$$ $$\int \frac{dy}{y^2} = \int x \, dx$$ $$\frac{-1}{y^1} = \frac{x^2}{2}$$ $$\frac{-2}{x^2} = y$$ Is there anything wrong with this solution? It is not what my book gets but it is similar to how they do it in the example.",,['calculus']
96,A simple explanation of differential calculus and its link to geometry?,A simple explanation of differential calculus and its link to geometry?,,"The wikipedia articles on differential calculus and differential geometry are quite long and not so straightforward for a layman like me. Is there a master of math vulgarization out there that could summarize in couple strong sentences what differential calculus is and how it is linked to geometry? I have read this nice post and this one too which give good examples and references but does not really explain what ""differential"" means, this post is good too but focuses on equations.","The wikipedia articles on differential calculus and differential geometry are quite long and not so straightforward for a layman like me. Is there a master of math vulgarization out there that could summarize in couple strong sentences what differential calculus is and how it is linked to geometry? I have read this nice post and this one too which give good examples and references but does not really explain what ""differential"" means, this post is good too but focuses on equations.",,"['ordinary-differential-equations', 'differential-geometry', 'definition', 'differential-topology']"
97,Does this ODE question have closed form solution?,Does this ODE question have closed form solution?,,"These days, I am struggling with following ODE problem when I build up my research model: $1/2f''(x)+a(b - x) f'(x) -(c+ e^{A+Bx})f(x)=0$ where f(x) is a smooth function, and $a,b,c, A,B$ are all constants. How to get the closed form of f(x)? I tried the Laplace transform to work on it, say $F(s) = L(f(x)) $, but because of $e^{A+Bx}$, there will be a term $F(s-B)$ in the transformed equation. How to deal with this term? I also tried the power series method, but got some very complicate coefficients, which stops me going further. I think the term $e^{A+Bx}$ is the difficult part. Could anyone here tell me how to deal with this kind of problem? Does the solution exit? I tried several ODE books but cannot find similar examples. Or could any one can suggest some relevant books? Thank you very much.","These days, I am struggling with following ODE problem when I build up my research model: $1/2f''(x)+a(b - x) f'(x) -(c+ e^{A+Bx})f(x)=0$ where f(x) is a smooth function, and $a,b,c, A,B$ are all constants. How to get the closed form of f(x)? I tried the Laplace transform to work on it, say $F(s) = L(f(x)) $, but because of $e^{A+Bx}$, there will be a term $F(s-B)$ in the transformed equation. How to deal with this term? I also tried the power series method, but got some very complicate coefficients, which stops me going further. I think the term $e^{A+Bx}$ is the difficult part. Could anyone here tell me how to deal with this kind of problem? Does the solution exit? I tried several ODE books but cannot find similar examples. Or could any one can suggest some relevant books? Thank you very much.",,['ordinary-differential-equations']
98,Numerically solving ODEs — how to estimate the solution between the nodes?,Numerically solving ODEs — how to estimate the solution between the nodes?,,"I have heard about a lot of fancy numerical methods for solving ODEs. I know that there are methods that (assuming sufficient smoothness) asymptotically give a low error, like the Runge-Kutta methods.  These estimate the solution in a set of points $t_0$ , $t_1$ , etc. But what if I want to have a function that is close to the correct solution everywhere, not just in a discrete set of points? I can extend the numerical solution to a piecewise linear function. This will be a continuous function and it will converge to the correct solution if the step-size goes to zero. But the error estimate will be poor in most places unless I use a very small step-size, which rather defeats the purpose of using a high-order method. So how does one go about estimating the solution in practice between the $t_i$ ?","I have heard about a lot of fancy numerical methods for solving ODEs. I know that there are methods that (assuming sufficient smoothness) asymptotically give a low error, like the Runge-Kutta methods.  These estimate the solution in a set of points , , etc. But what if I want to have a function that is close to the correct solution everywhere, not just in a discrete set of points? I can extend the numerical solution to a piecewise linear function. This will be a continuous function and it will converge to the correct solution if the step-size goes to zero. But the error estimate will be poor in most places unless I use a very small step-size, which rather defeats the purpose of using a high-order method. So how does one go about estimating the solution in practice between the ?",t_0 t_1 t_i,"['ordinary-differential-equations', 'numerical-methods']"
99,Solving a differential equation related to $\log (1+t)$,Solving a differential equation related to,\log (1+t),"How does one find the solution of $$\dfrac{dy}{dx}\left( 1-\left( 1-t\right) x-x^{2}\right) -\left( 1+h\left( 1+t\right) +x\right) y=0\quad ?$$ where $h$ is an integer constant and $t$ is a real constant between $0$ and $1$. $($ In Roger Apéry, Interpolations de Fractions Continues et Irrationalité de certaines Constantes , Bull. section des sciences du C.T.H.S., n.º3, p.37-53, the solution is $$y=(1-x)^{-1-h}(1+tx)^{h}.)$$ Note: The sequence $(v_{h,n})$ in $y=f_{h}(x)=\displaystyle\sum_{n\ge 0}v_{h,n}x^n$  satisfies a recurrence related to $\log (1+t)$. Added : Copy of the original with the equation and solution Addendum 2: I transcribe the comment in the 1st answer: ""the corrected differential equation above agrees with the recurrence in your excerpt so there is clearly a typo in the printed differential equation.""","How does one find the solution of $$\dfrac{dy}{dx}\left( 1-\left( 1-t\right) x-x^{2}\right) -\left( 1+h\left( 1+t\right) +x\right) y=0\quad ?$$ where $h$ is an integer constant and $t$ is a real constant between $0$ and $1$. $($ In Roger Apéry, Interpolations de Fractions Continues et Irrationalité de certaines Constantes , Bull. section des sciences du C.T.H.S., n.º3, p.37-53, the solution is $$y=(1-x)^{-1-h}(1+tx)^{h}.)$$ Note: The sequence $(v_{h,n})$ in $y=f_{h}(x)=\displaystyle\sum_{n\ge 0}v_{h,n}x^n$  satisfies a recurrence related to $\log (1+t)$. Added : Copy of the original with the equation and solution Addendum 2: I transcribe the comment in the 1st answer: ""the corrected differential equation above agrees with the recurrence in your excerpt so there is clearly a typo in the printed differential equation.""",,"['calculus', 'ordinary-differential-equations']"
