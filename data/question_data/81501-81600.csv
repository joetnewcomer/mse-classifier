,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Prove 1-Norm is a Norm,Prove 1-Norm is a Norm,,I am just curious how you would simply prove that a 1-norm is a norm. Step-by-step would be very helpful. Proofs are not my strong point. Thank you!,I am just curious how you would simply prove that a 1-norm is a norm. Step-by-step would be very helpful. Proofs are not my strong point. Thank you!,,"['linear-algebra', 'matrices', 'vectors', 'normed-spaces', 'numerical-linear-algebra']"
1,Differentiate wrt Cholesky decomposition,Differentiate wrt Cholesky decomposition,,"I wish to find the most likely estimator of the precision matrix (inverse covariance matrix). One option is to maximise the following: $$ f(\Theta) = \frac{N}{2}\log|\Theta|-\sum_i \mathbf{x}_i^T\Theta\mathbf{x}_i $$ (assume that the mean is zero without loss of generality). $\mathbf{x}_i$'s are constant. I know how to differentiate the above by exploiting the fact that $\mathbf{x}_i^T\Theta\mathbf{x}_i=Tr(\Theta\mathbf{x}_i\mathbf{x}_i^T)$. However, if I pose the question as instead to optimise the cholesky decomposition $L$ where $LL^T=\Theta$, $$ f(L) = {N}\log|L|-\sum_i \mathbf{x}_i^TLL^T\mathbf{x}_i $$ what is $\frac{\partial f(L)}{\partial L}$? It's really the second term that I am struggling with.","I wish to find the most likely estimator of the precision matrix (inverse covariance matrix). One option is to maximise the following: $$ f(\Theta) = \frac{N}{2}\log|\Theta|-\sum_i \mathbf{x}_i^T\Theta\mathbf{x}_i $$ (assume that the mean is zero without loss of generality). $\mathbf{x}_i$'s are constant. I know how to differentiate the above by exploiting the fact that $\mathbf{x}_i^T\Theta\mathbf{x}_i=Tr(\Theta\mathbf{x}_i\mathbf{x}_i^T)$. However, if I pose the question as instead to optimise the cholesky decomposition $L$ where $LL^T=\Theta$, $$ f(L) = {N}\log|L|-\sum_i \mathbf{x}_i^TLL^T\mathbf{x}_i $$ what is $\frac{\partial f(L)}{\partial L}$? It's really the second term that I am struggling with.",,"['matrices', 'matrix-calculus', 'matrix-decomposition']"
2,How many cube roots does an $n\times n$ identity matrix have over $\mathbb C$?,How many cube roots does an  identity matrix have over ?,n\times n \mathbb C,"I thought there are infinite solutions, because if $A$ is a solution, then $Q^TAQ$  is also a solution, where $Q$ is an orthogonal matrix. But I used MATLAB to symbolically solve for the cube root of a $2\times 2$ identity matrix, I got 16 solutions. Every entry of the solutions is either $0$ or a cube root of $1$. MATLAB is unable to solve for higher $n$. So how many cube roots does an $n\times n$ identity matrix have? $n^4$?","I thought there are infinite solutions, because if $A$ is a solution, then $Q^TAQ$  is also a solution, where $Q$ is an orthogonal matrix. But I used MATLAB to symbolically solve for the cube root of a $2\times 2$ identity matrix, I got 16 solutions. Every entry of the solutions is either $0$ or a cube root of $1$. MATLAB is unable to solve for higher $n$. So how many cube roots does an $n\times n$ identity matrix have? $n^4$?",,"['linear-algebra', 'matrices']"
3,"Inner product space, prove $\det(A) \geq 0$ given a Gram matrix $A$","Inner product space, prove  given a Gram matrix",\det(A) \geq 0 A,"Let $K=\mathbb R$ and let $V$ be a $\mathbb K-$ finite dimensional  inner product space, $\dim(V)=n$ . Consider $v_1,\ldots,v_m \in V$ with $1 \leq m \leq n$ . Let $A \in\Bbb K^{m\times m}$ defined as $A=(a_{ij})$ with $a_{ij}=\langle v_i,v_j \rangle$ . Prove the following: 1) $\det(A) \geq 0$ 2) $\det(A)>0$ if and only if $v_1,\ldots,v_m$ are linearly independent What I thought of was to consider $S=\langle v_1,\ldots,v_m \rangle$ and $\mathcal B_S=\{s_1,\ldots,s_k\}$ ( $k \leq m$ ) an orthonormal basis of the subspace generated by these vectors. Each $v_i$ can be written as $$v_i=\sum_{t=1}^kb_{ti}s_t$$ The determinant of $A$ is defined as $$\det(A)=\sum_{\sigma \in S_n} \operatorname{sg}(\sigma)a_{\sigma(1)1}\cdots a_{\sigma(n)n}$$ $$=\sum_{\sigma \in S_n} \operatorname{sg}(\sigma)\langle v_{\sigma(1)},v_1 \rangle...\langle v_{\sigma(n)},v_n \rangle$$ $$=\sum_{\sigma \in S_n} \operatorname{sg}(\sigma)\left (\sum_{l_1=1}^kb_{{l_1}\sigma(1)}b_{{l_1}1}\right)\cdots\left (\sum_{l_n=1}^kb_{{l_n}\sigma(n)}b_{{l_n}n} \right ).$$ I don't understand why this last expression should be greater than or equal to $0$ , I would appreciate suggestions on how to continue the exercise.","Let and let be a finite dimensional  inner product space, . Consider with . Let defined as with . Prove the following: 1) 2) if and only if are linearly independent What I thought of was to consider and ( ) an orthonormal basis of the subspace generated by these vectors. Each can be written as The determinant of is defined as I don't understand why this last expression should be greater than or equal to , I would appreciate suggestions on how to continue the exercise.","K=\mathbb R V \mathbb K- \dim(V)=n v_1,\ldots,v_m \in V 1 \leq m \leq n A \in\Bbb K^{m\times m} A=(a_{ij}) a_{ij}=\langle v_i,v_j \rangle \det(A) \geq 0 \det(A)>0 v_1,\ldots,v_m S=\langle v_1,\ldots,v_m \rangle \mathcal B_S=\{s_1,\ldots,s_k\} k \leq m v_i v_i=\sum_{t=1}^kb_{ti}s_t A \det(A)=\sum_{\sigma \in S_n} \operatorname{sg}(\sigma)a_{\sigma(1)1}\cdots a_{\sigma(n)n} =\sum_{\sigma \in S_n} \operatorname{sg}(\sigma)\langle v_{\sigma(1)},v_1 \rangle...\langle v_{\sigma(n)},v_n \rangle =\sum_{\sigma \in S_n} \operatorname{sg}(\sigma)\left (\sum_{l_1=1}^kb_{{l_1}\sigma(1)}b_{{l_1}1}\right)\cdots\left (\sum_{l_n=1}^kb_{{l_n}\sigma(n)}b_{{l_n}n} \right ). 0","['linear-algebra', 'matrices', 'determinant', 'inner-products']"
4,How do you show the following is diagonalizable?,How do you show the following is diagonalizable?,,"So if $A$ is diagonalizable, how would you show the following is diagonalizable? a)$p(A),p(x)$ (any polynomial) b)$kI+A$ for any scalar $k$ c)$U^{-1}AU$ for any invertible matrix $U$. So for the first one, I have no idea how polynomials associate with diagonalization. And neither can think of anything for the third. For the second one, I assume that $P^{-1}(kI+A)P=P^{-1}kIP+P^{-1}AP=kI+D$, since $kI$ and$D$ each is diagonalizable, $kI + D$ is diagonalizable. But I felt I have a wrong idea about this, someone help?","So if $A$ is diagonalizable, how would you show the following is diagonalizable? a)$p(A),p(x)$ (any polynomial) b)$kI+A$ for any scalar $k$ c)$U^{-1}AU$ for any invertible matrix $U$. So for the first one, I have no idea how polynomials associate with diagonalization. And neither can think of anything for the third. For the second one, I assume that $P^{-1}(kI+A)P=P^{-1}kIP+P^{-1}AP=kI+D$, since $kI$ and$D$ each is diagonalizable, $kI + D$ is diagonalizable. But I felt I have a wrong idea about this, someone help?",,"['linear-algebra', 'matrices', 'diagonalization']"
5,Finding an analytical expression for the eigen values,Finding an analytical expression for the eigen values,,"I would like to know if there is any way to find an analytical expression for the eigen values of the following matrix. $$ A^h = \frac{1}{h^4}  \begin{pmatrix}   5&-4&1&&&&&&\\   -4&6&-4&1&&&&\bigcirc&\\   1&-4&6&-4&1&&&&\\   &\ddots  & \ddots  & \ddots & \ddots&&&  \\   &&\ddots  & \ddots  & \ddots & \ddots&&  \\   &&&&1&-4&6&-4&1\\   &\bigcirc&&&&1&-4&6&-4\\   &&&&&&1&-4&5\\  \end{pmatrix} $$ I have heard that it is possible through DFT, but I am not sure how to proceed with that. The size of the matrix is $N \times N$ and $h = \frac{1}{N}$. Thanks!","I would like to know if there is any way to find an analytical expression for the eigen values of the following matrix. $$ A^h = \frac{1}{h^4}  \begin{pmatrix}   5&-4&1&&&&&&\\   -4&6&-4&1&&&&\bigcirc&\\   1&-4&6&-4&1&&&&\\   &\ddots  & \ddots  & \ddots & \ddots&&&  \\   &&\ddots  & \ddots  & \ddots & \ddots&&  \\   &&&&1&-4&6&-4&1\\   &\bigcirc&&&&1&-4&6&-4\\   &&&&&&1&-4&5\\  \end{pmatrix} $$ I have heard that it is possible through DFT, but I am not sure how to proceed with that. The size of the matrix is $N \times N$ and $h = \frac{1}{N}$. Thanks!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
6,"Let $x,y \in \mathbb C^n$ , where $n>1$ ; then does there exist symmetric $A \in M_n(\mathbb C) $ such that $Ax=y$?","Let  , where  ; then does there exist symmetric  such that ?","x,y \in \mathbb C^n n>1 A \in M_n(\mathbb C)  Ax=y","Let $x,y \in \mathbb C^n$ , where $n>1$ ; then does there exist $A \in M_n(\mathbb C) $ such that $Ax=y$ ? Can we find such a symmetric matrix $A$ ?","Let $x,y \in \mathbb C^n$ , where $n>1$ ; then does there exist $A \in M_n(\mathbb C) $ such that $Ax=y$ ? Can we find such a symmetric matrix $A$ ?",,['linear-algebra']
7,Find $p_{ij}^{(n)}$ for the transition matrix,Find  for the transition matrix,p_{ij}^{(n)},"Let   $$P=\begin{bmatrix}\frac{1}{3}&0&\frac{2}{3}\\\frac{1}{3}&\frac{2}{3}&0\\\frac{1}{3}&\frac{1}{3}&\frac{1}{3}\end{bmatrix}$$   find $p_{11}^{(n)},p_{12}^{(n)},p_{13}^{(n)}$ Since the characteristic function is $$-\mu^3+\frac{4}{3}\mu^2-\frac{1}{3}\mu=0$$ the eigenvalues are $$\mu_0=1,\mu_1=\frac{1}{3},\mu_2=0$$ Here the problem starts, from what I understood from the book, we need to found three constants such that $$p_{ij}^{(n)}=\mu_0^nA+\mu_1^n+\mu_2^nC=A+\left(\frac{1}{3}\right)^nB$$ But how can I find the values of A and B?","Let   $$P=\begin{bmatrix}\frac{1}{3}&0&\frac{2}{3}\\\frac{1}{3}&\frac{2}{3}&0\\\frac{1}{3}&\frac{1}{3}&\frac{1}{3}\end{bmatrix}$$   find $p_{11}^{(n)},p_{12}^{(n)},p_{13}^{(n)}$ Since the characteristic function is $$-\mu^3+\frac{4}{3}\mu^2-\frac{1}{3}\mu=0$$ the eigenvalues are $$\mu_0=1,\mu_1=\frac{1}{3},\mu_2=0$$ Here the problem starts, from what I understood from the book, we need to found three constants such that $$p_{ij}^{(n)}=\mu_0^nA+\mu_1^n+\mu_2^nC=A+\left(\frac{1}{3}\right)^nB$$ But how can I find the values of A and B?",,"['probability', 'matrices', 'stochastic-processes', 'self-learning']"
8,How do the rows of a change of basis matrix form a basis for expressing columns?,How do the rows of a change of basis matrix form a basis for expressing columns?,,"I am reading this article on Principal Component Analysis (PCA) and in section III-B (page 3) it has strange definition I don't understand. In the toy example $\mathbf{X}$ is an $m \times n$ matrix.... Let $\mathbf{Y}$ be another $m \times n$ matrix related by a linear transformation $\mathbf{P}$ . $\mathbf{X}$ is the original recorded data set and $\mathbf{Y}$ is a re-representation of that data set. $$\mathbf{P} \mathbf{X} = \mathbf{Y} \tag{1}$$ Also let us define the following quantities. $\mathbf{p}_i$ are the rows of $\mathbf{P}$ . $\mathbf{x}_i$ are the columns of $\mathbf{X}$ (or individual $\vec{X}$ ). $\mathbf{y}_i$ are the columns of $\mathbf{Y}$ . Equation 1 represents a change of basis and thus can have many interpretations. $\mathbf{P}$ is a matrix that transforms $\mathbf{X}$ into $\mathbf{Y}$ . Geometrically $\mathbf{P}$ is a rotation and a stretch whcih again transforms $\mathbf{X}$ into $\mathbf{Y}$ . The rows of $\mathbf{P}$ , $\{ \mathbf{p}_1, \ldots , \mathbf{p}_m \}$ , are a set of new basis vectors for expressing the columns of $\mathbf{X}$ . I do not understand this last part, how the rows $\mathbf{p}_i$ of $\mathbf{P}$ are a set of new basis vectors for expressing the columns of $\mathbf{X}$ . The reason I don't understand latter part is that change of basis matrix usually has basis in its columns , not rows . Then multiplying by column vector on the right we get combination of matrix's columns, which is exactly representation in new basis. So I would expect new basis to be in columns of $\mathbf{P}$ , not rows of $\mathbf{P}$ . What am I missing here?","I am reading this article on Principal Component Analysis (PCA) and in section III-B (page 3) it has strange definition I don't understand. In the toy example is an matrix.... Let be another matrix related by a linear transformation . is the original recorded data set and is a re-representation of that data set. Also let us define the following quantities. are the rows of . are the columns of (or individual ). are the columns of . Equation 1 represents a change of basis and thus can have many interpretations. is a matrix that transforms into . Geometrically is a rotation and a stretch whcih again transforms into . The rows of , , are a set of new basis vectors for expressing the columns of . I do not understand this last part, how the rows of are a set of new basis vectors for expressing the columns of . The reason I don't understand latter part is that change of basis matrix usually has basis in its columns , not rows . Then multiplying by column vector on the right we get combination of matrix's columns, which is exactly representation in new basis. So I would expect new basis to be in columns of , not rows of . What am I missing here?","\mathbf{X} m \times n \mathbf{Y} m \times n \mathbf{P} \mathbf{X} \mathbf{Y} \mathbf{P} \mathbf{X} = \mathbf{Y} \tag{1} \mathbf{p}_i \mathbf{P} \mathbf{x}_i \mathbf{X} \vec{X} \mathbf{y}_i \mathbf{Y} \mathbf{P} \mathbf{X} \mathbf{Y} \mathbf{P} \mathbf{X} \mathbf{Y} \mathbf{P} \{ \mathbf{p}_1, \ldots , \mathbf{p}_m \} \mathbf{X} \mathbf{p}_i \mathbf{P} \mathbf{X} \mathbf{P} \mathbf{P}","['linear-algebra', 'matrices', 'linear-transformations', 'machine-learning', 'svd']"
9,Is it possible to express $\left(\begin{array}{cc}a & -a \\ a-1 & 1-a \\ \end{array} \right)$ as a certain product of two matrices?,Is it possible to express  as a certain product of two matrices?,\left(\begin{array}{cc}a & -a \\ a-1 & 1-a \\ \end{array} \right),"Is it possible to express $$\left(           \begin{array}{cc}             a & -a \\             a-1 & 1-a \\           \end{array}         \right), \ \ \ \ a\in\mathbb R$$ as a certain product of two matrices? Namely, $$\left(           \begin{array}{cc}             a & -a \\             a-1 & 1-a \\           \end{array}         \right)=PQ$$ where $P$, $Q$ are two matrices with entries not all $1$. I tried to do some product but without success. Any suggestions please?","Is it possible to express $$\left(           \begin{array}{cc}             a & -a \\             a-1 & 1-a \\           \end{array}         \right), \ \ \ \ a\in\mathbb R$$ as a certain product of two matrices? Namely, $$\left(           \begin{array}{cc}             a & -a \\             a-1 & 1-a \\           \end{array}         \right)=PQ$$ where $P$, $Q$ are two matrices with entries not all $1$. I tried to do some product but without success. Any suggestions please?",,"['linear-algebra', 'matrices']"
10,"The matrix $A-I$ is invertible, suppose $A^2=A$ and show that $A=0$","The matrix  is invertible, suppose  and show that",A-I A^2=A A=0,"Let $A_{n\times n}$ over $F$ such that the matrix $A-I$ is invertible. Suppose $A^2=A$, prove that $A=0$. It's easy to see that $A(A-I)=0=(A-I)A$ I also know that there exist a matrix $B$ such that $B(A-I)=(A-I)B=I$ but I can't find a way to use that. Everything I try leads to $A-A=0$ which doesn't help.","Let $A_{n\times n}$ over $F$ such that the matrix $A-I$ is invertible. Suppose $A^2=A$, prove that $A=0$. It's easy to see that $A(A-I)=0=(A-I)A$ I also know that there exist a matrix $B$ such that $B(A-I)=(A-I)B=I$ but I can't find a way to use that. Everything I try leads to $A-A=0$ which doesn't help.",,"['linear-algebra', 'matrices']"
11,Inverse of a matrix and its transpose,Inverse of a matrix and its transpose,,"I'm trying to figure out why the calculation below works. I do know that $(A^T)^{-1} = (A^{-1})^T$. The matrix A = $\begin{pmatrix} 1 & -1 & 0 \\ 1 & 1 & -1\\ 1 & 2 & -1    \end{pmatrix} $ has the inverse $\begin{pmatrix} 1 & -1 & 1 \\ 0 & -1 & 1\\ 1 & -3 & 2    \end{pmatrix} $ Another matrix B = $\begin{pmatrix} 1 & 1 & 1 & 0 \\ -1 & 1 & 2 & 0 \\ 0 & -1 & -1 & 0 \\  0 & 0 & 0 & 4  \end{pmatrix} $ which has the transpose of matrix A embedded within it has the inverse  $\begin{pmatrix} 1 & 0 & 1 & 0 \\ -1 & -1 & -3 & 0 \\ 1 & 1 & 2 & 0 \\  0 & 0 & 0 & 1/4  \end{pmatrix} $. Clearly, the inverse of B can be worked out quicker by using the result of the inverse of A and then changing 4 to its inverse (1/4). Does anyone know the actual rules that have been applied here? Is this a special case because the newly added row 4 and column 4 all have zeroes apart from 4? I just want to know why this works and in what context can I apply this trick? Thanks","I'm trying to figure out why the calculation below works. I do know that $(A^T)^{-1} = (A^{-1})^T$. The matrix A = $\begin{pmatrix} 1 & -1 & 0 \\ 1 & 1 & -1\\ 1 & 2 & -1    \end{pmatrix} $ has the inverse $\begin{pmatrix} 1 & -1 & 1 \\ 0 & -1 & 1\\ 1 & -3 & 2    \end{pmatrix} $ Another matrix B = $\begin{pmatrix} 1 & 1 & 1 & 0 \\ -1 & 1 & 2 & 0 \\ 0 & -1 & -1 & 0 \\  0 & 0 & 0 & 4  \end{pmatrix} $ which has the transpose of matrix A embedded within it has the inverse  $\begin{pmatrix} 1 & 0 & 1 & 0 \\ -1 & -1 & -3 & 0 \\ 1 & 1 & 2 & 0 \\  0 & 0 & 0 & 1/4  \end{pmatrix} $. Clearly, the inverse of B can be worked out quicker by using the result of the inverse of A and then changing 4 to its inverse (1/4). Does anyone know the actual rules that have been applied here? Is this a special case because the newly added row 4 and column 4 all have zeroes apart from 4? I just want to know why this works and in what context can I apply this trick? Thanks",,"['linear-algebra', 'matrices', 'inverse']"
12,Find all complex matrices $A$ such that $n\operatorname{Tr}(AB) = \operatorname{Tr}(A)\operatorname{Tr}(B)$ for all $B$. [duplicate],Find all complex matrices  such that  for all . [duplicate],A n\operatorname{Tr}(AB) = \operatorname{Tr}(A)\operatorname{Tr}(B) B,"This question already has an answer here : Finding the dimension of the orthogonal complement (1 answer) Closed 9 years ago . Consider a bilinear form $f(A,B) = n\operatorname{Tr}(AB) - \operatorname{Tr}(A)\operatorname{Tr}(B)$ defined on $M_n(\mathbb{C})$. I need to find the set $U^\perp$ of all matrices $A$ such that $f(A,B) = 0$ for every $B \in M_n(\mathbb{C})$, or, more specifically, find $\dim(U^\perp)$. Other than the zero matrix I can't find any other general matrix. So $\dim (U^\perp) = 0$ I'd assume?","This question already has an answer here : Finding the dimension of the orthogonal complement (1 answer) Closed 9 years ago . Consider a bilinear form $f(A,B) = n\operatorname{Tr}(AB) - \operatorname{Tr}(A)\operatorname{Tr}(B)$ defined on $M_n(\mathbb{C})$. I need to find the set $U^\perp$ of all matrices $A$ such that $f(A,B) = 0$ for every $B \in M_n(\mathbb{C})$, or, more specifically, find $\dim(U^\perp)$. Other than the zero matrix I can't find any other general matrix. So $\dim (U^\perp) = 0$ I'd assume?",,"['linear-algebra', 'matrices', 'orthogonality', 'bilinear-form']"
13,Can you solve a quadratic equation using matrices?,Can you solve a quadratic equation using matrices?,,"I was wondering whether there are any alternatives or more efficient methods to finding a solution to a quadratic equation other than simply trial and error or by using the quadratic formula. I was once told that it could be very easily done using matrices. How would this work? Additionally, are there any other ""better"" alternatives? I would really appreciate if you were to give me examples and explain how to solve them with the alternate method. Thank you :)","I was wondering whether there are any alternatives or more efficient methods to finding a solution to a quadratic equation other than simply trial and error or by using the quadratic formula. I was once told that it could be very easily done using matrices. How would this work? Additionally, are there any other ""better"" alternatives? I would really appreciate if you were to give me examples and explain how to solve them with the alternate method. Thank you :)",,"['matrices', 'quadratics']"
14,Matrix solving equation.,Matrix solving equation.,,The number of real solutions of equation $$\begin{vmatrix}x^2-12&-18&-5\\10&x^2+2&1\\-2&12&x^2\end{vmatrix}=0$$ is? Well I wanted to do something like this: $$\begin{vmatrix}-12&-18&-5\\10&2&1\\-2&12&0\end{vmatrix}+|x^2{\rm I}|=0$$ And then I got: $$x^2=704>0$$ Does this prove that it has two real roots?,The number of real solutions of equation $$\begin{vmatrix}x^2-12&-18&-5\\10&x^2+2&1\\-2&12&x^2\end{vmatrix}=0$$ is? Well I wanted to do something like this: $$\begin{vmatrix}-12&-18&-5\\10&2&1\\-2&12&0\end{vmatrix}+|x^2{\rm I}|=0$$ And then I got: $$x^2=704>0$$ Does this prove that it has two real roots?,,"['matrices', 'proof-verification', 'determinant']"
15,Prove that matrix is symmetric and positive definite given the fact that $A+iB$ is.,Prove that matrix is symmetric and positive definite given the fact that  is.,A+iB,"I have some questions regarding the following problem Let $ A + iB $ - hermitian and positive definite, where $A, B \in \mathbb R^{n\ \times\ n} $ show that the real matrix $$C =\begin{pmatrix} A & -B \\ B & A \end{pmatrix} $$ is symmetric and positive definite. How can the following system of linear equations be solved using Cholesky decomposition of C: $ (A+iB)(x+iy)=B+iC$ I've tried to transform C and get to $A + iB$ , I've tried to compute the determinants to get to $A + iB$ but I had no result. Moreover, I have no idea how to approach the second part of the problem. Thank you","I have some questions regarding the following problem Let - hermitian and positive definite, where show that the real matrix is symmetric and positive definite. How can the following system of linear equations be solved using Cholesky decomposition of C: I've tried to transform C and get to , I've tried to compute the determinants to get to but I had no result. Moreover, I have no idea how to approach the second part of the problem. Thank you"," A + iB  A, B \in \mathbb R^{n\ \times\ n}  C =\begin{pmatrix} A & -B \\ B & A \end{pmatrix}   (A+iB)(x+iy)=B+iC A + iB A + iB","['linear-algebra', 'matrices', 'matrix-equations', 'matrix-decomposition']"
16,Row space and Column space and product of matrices question,Row space and Column space and product of matrices question,,"The theorems state that the $\operatorname{ColumnSpace}(A \cdot B) \subseteq \operatorname{ColumnSpace}(A)$ and the $\operatorname{RowSpace}(A \cdot B) \subseteq \operatorname{RowSpace}(B)$. When doing $A \cdot B$, it's the dot product of the rows of $A$ by the columns of $B$. If its the rows of $A$ by the columns of $B$, why is it that $\operatorname{RowSpace}(A \cdot B) \subseteq \operatorname{RowSpace}(B)$ and not $\operatorname{RowSpace}(A)$? Also, If its the rows of $A$ by the columns of $B$, why is it that $\operatorname{ColumnSpace}(A \cdot B) \subseteq \operatorname{ColumnSpace}(A)$ and not $\operatorname{ColumnSpace}(B)$?","The theorems state that the $\operatorname{ColumnSpace}(A \cdot B) \subseteq \operatorname{ColumnSpace}(A)$ and the $\operatorname{RowSpace}(A \cdot B) \subseteq \operatorname{RowSpace}(B)$. When doing $A \cdot B$, it's the dot product of the rows of $A$ by the columns of $B$. If its the rows of $A$ by the columns of $B$, why is it that $\operatorname{RowSpace}(A \cdot B) \subseteq \operatorname{RowSpace}(B)$ and not $\operatorname{RowSpace}(A)$? Also, If its the rows of $A$ by the columns of $B$, why is it that $\operatorname{ColumnSpace}(A \cdot B) \subseteq \operatorname{ColumnSpace}(A)$ and not $\operatorname{ColumnSpace}(B)$?",,"['linear-algebra', 'matrices']"
17,"Is it true that $\sum_{i,j=1}^n A^{i,j}x^iy^j \leq \sqrt{\left(\sum_{i,j=1}^n A^{i,j}x^ix^j\right)\left(\sum_{i,j=1}^n A^{i,j}y^iy^j\right)}?$",Is it true that,"\sum_{i,j=1}^n A^{i,j}x^iy^j \leq \sqrt{\left(\sum_{i,j=1}^n A^{i,j}x^ix^j\right)\left(\sum_{i,j=1}^n A^{i,j}y^iy^j\right)}?","If $A$ is a symmetric and positive semidefinite matrix is it true that $$\sum_{i,j=1}^n A^{i,j}x^iy^j \leq \sqrt{\left(\sum_{i,j=1}^n A^{i,j}x^ix^j\right)\left(\sum_{i,j=1}^n A^{i,j}y^iy^j\right)},$$ where $x,y \in \mathbb{R^d}$? I thought Holder's inequality could be applied but I don't think it works for signed measure spaces. Thanks.","If $A$ is a symmetric and positive semidefinite matrix is it true that $$\sum_{i,j=1}^n A^{i,j}x^iy^j \leq \sqrt{\left(\sum_{i,j=1}^n A^{i,j}x^ix^j\right)\left(\sum_{i,j=1}^n A^{i,j}y^iy^j\right)},$$ where $x,y \in \mathbb{R^d}$? I thought Holder's inequality could be applied but I don't think it works for signed measure spaces. Thanks.",,"['linear-algebra', 'algebra-precalculus', 'matrices', 'inequality']"
18,If $f_A(x) \ne m_A(x)$ and $A^3=I$ then $A=I$?,If  and  then ?,f_A(x) \ne m_A(x) A^3=I A=I,"Suppose $A \in M_{3\times3}(\mathbb R)$ and $f_A(x) \ne m_A(x)$ where $f_A(x)$ is the characteristic polynomial of $A$ and $m_A(x)$ is the minimal polynomial of $A$ . If we were to assume that $A^3=I$ , would we get that $A=I$ ? Would the result be different in case $A$ was $2\times2$ matirx? If it possible to find such $A$ which isn't $I$ , please explain how to do it. Otherwise, please explain why it isn't possible. My current work: I started with the case where $A$ is $3 \times 3 $ matrix. From the assumption that $A^3=I$ we know that the polynomial $g(x)=x^3-1$ annihilates $A$ . From there I tried to construct $A \in M_{3\times3}(\mathbb R)$ such that $A^3=I$ while keeping in mind that any eigenvalue $c$ of $A$ must satisfies $g(c)=c^3-1=0$ . The only matrices I were able to look for were the rotating matrices and the projection matrices which didn't satisfy $A^3=I$ . Is there any other way to be able to construct such matrix? Where $A$ is a $2 \times 2 $ matrix I had the same problem to construct the matrix I want, but I was able to try different combinations. None of them matched the $A$ I was looking for, therefore I'm almost sure it is impossible to find such a matrix when $A$ is $2 \times 2$ matrix. Thank you","Suppose and where is the characteristic polynomial of and is the minimal polynomial of . If we were to assume that , would we get that ? Would the result be different in case was matirx? If it possible to find such which isn't , please explain how to do it. Otherwise, please explain why it isn't possible. My current work: I started with the case where is matrix. From the assumption that we know that the polynomial annihilates . From there I tried to construct such that while keeping in mind that any eigenvalue of must satisfies . The only matrices I were able to look for were the rotating matrices and the projection matrices which didn't satisfy . Is there any other way to be able to construct such matrix? Where is a matrix I had the same problem to construct the matrix I want, but I was able to try different combinations. None of them matched the I was looking for, therefore I'm almost sure it is impossible to find such a matrix when is matrix. Thank you",A \in M_{3\times3}(\mathbb R) f_A(x) \ne m_A(x) f_A(x) A m_A(x) A A^3=I A=I A 2\times2 A I A 3 \times 3  A^3=I g(x)=x^3-1 A A \in M_{3\times3}(\mathbb R) A^3=I c A g(c)=c^3-1=0 A^3=I A 2 \times 2  A A 2 \times 2,"['linear-algebra', 'matrices', 'diagonalization', 'minimal-polynomials']"
19,Inverse of non-square matrix,Inverse of non-square matrix,,"Is it true that a non-square matrix cannot have both the left and right inverse? Consider an $m \times n$ matrix $A$. If $n \times m$ matrix $B$ is the right inverse of $A$, viz., $AB = I_m$. Now if given that $A$ also has a left inverse $C$, which is an $n \times m$ matrix such that $CA=I_n$. Then it can be shown that $B=C$ because $$C=CI_m=C(AB)=(CA)B=I_nB=B$$. So $B=C$ is the inverse of $A$. Is it true that for all non-square $A$, such an inverse does not exist?","Is it true that a non-square matrix cannot have both the left and right inverse? Consider an $m \times n$ matrix $A$. If $n \times m$ matrix $B$ is the right inverse of $A$, viz., $AB = I_m$. Now if given that $A$ also has a left inverse $C$, which is an $n \times m$ matrix such that $CA=I_n$. Then it can be shown that $B=C$ because $$C=CI_m=C(AB)=(CA)B=I_nB=B$$. So $B=C$ is the inverse of $A$. Is it true that for all non-square $A$, such an inverse does not exist?",,"['matrices', 'inverse']"
20,To show a matrix $A=I$ if all eigen values are $1$ and the set $\{A^n:n\in\mathbb{N}\}$ be bounded,To show a matrix  if all eigen values are  and the set  be bounded,A=I 1 \{A^n:n\in\mathbb{N}\},"$A\in M_n(\mathbb{C})$ with all eigenvalues equal to $1$. Suppose the set $\{A^k:k\in\mathbb{N}\}$ be bounded, then show that $A\equiv I$. I tried from spectral radius formuale $\displaystyle\lim_{n\to 0}A^n=0\iff \rho(A)<1$. Here $\rho(A)=1$ but then I am lost, please give me hint. Thanks","$A\in M_n(\mathbb{C})$ with all eigenvalues equal to $1$. Suppose the set $\{A^k:k\in\mathbb{N}\}$ be bounded, then show that $A\equiv I$. I tried from spectral radius formuale $\displaystyle\lim_{n\to 0}A^n=0\iff \rho(A)<1$. Here $\rho(A)=1$ but then I am lost, please give me hint. Thanks",,"['linear-algebra', 'matrices']"
21,Diagonalization and find matrix that corresponds to the given condition,Diagonalization and find matrix that corresponds to the given condition,,"Diagonalize the matrix $$ A= \begin{pmatrix} 1 & 2\\ 0 & 3 \end{pmatrix} $$ and find $B^3=A$. I derived $A \sim \text{diag}(1,3)$ but I have problem finding any $B$. I tried to solve it by writing $B= \begin{pmatrix} 1 & x\\ 0 & 3\end{pmatrix}$, but is it okay to solve the problem in this way?","Diagonalize the matrix $$ A= \begin{pmatrix} 1 & 2\\ 0 & 3 \end{pmatrix} $$ and find $B^3=A$. I derived $A \sim \text{diag}(1,3)$ but I have problem finding any $B$. I tried to solve it by writing $B= \begin{pmatrix} 1 & x\\ 0 & 3\end{pmatrix}$, but is it okay to solve the problem in this way?",,"['linear-algebra', 'matrices', 'diagonalization']"
22,Maximizing $\log(|A|)-\text{Tr}(AB)$ for pd and symmetric $A$ and $B$,Maximizing  for pd and symmetric  and,\log(|A|)-\text{Tr}(AB) A B,"Let $A$ and $B$ be two symmetric and positive definite matrices of the   same size. Then the function $$ f(A)\equiv\log(\det(A))-\text{Tr}(AB)  $$ is maximized uniquely by $A=B^{-1}$. This is mentioned in passing in Hayashi (2000) . Does anyone know a proof, please?","Let $A$ and $B$ be two symmetric and positive definite matrices of the   same size. Then the function $$ f(A)\equiv\log(\det(A))-\text{Tr}(AB)  $$ is maximized uniquely by $A=B^{-1}$. This is mentioned in passing in Hayashi (2000) . Does anyone know a proof, please?",,"['linear-algebra', 'matrices', 'inequality', 'optimization']"
23,"What is ""cyclic shift unitary"" on $M_{n}(\mathbb{C})$?","What is ""cyclic shift unitary"" on ?",M_{n}(\mathbb{C}),"From page 264 of Brown & Ozawa's $C^*$ -algebras and finite-dimensional approximations : Let $M_{n}(\mathbb{C})$ be the $n\times n$ complex matrices, and what is the "" cyclic shift unitary of order $n$ "" on $M_{n}(\mathbb{C})$ ? Maybe it is a very basic concept in functional analysis or matrix theory?","From page 264 of Brown & Ozawa's -algebras and finite-dimensional approximations : Let be the complex matrices, and what is the "" cyclic shift unitary of order "" on ? Maybe it is a very basic concept in functional analysis or matrix theory?",C^* M_{n}(\mathbb{C}) n\times n n M_{n}(\mathbb{C}),"['matrices', 'orthogonal-matrices', 'permutation-matrices', 'circulant-matrices']"
24,"Prove that $\mathrm{span}\{ I,A,A^2... \} = \mathrm{span} \{ I,A,A^2,..., A^{k-1}\}$",Prove that,"\mathrm{span}\{ I,A,A^2... \} = \mathrm{span} \{ I,A,A^2,..., A^{k-1}\}","Let $A\in M_n(F)$ and $k=\deg(m_A)$ where $m_A$ is the minimal polynomial of $A$. Prove that $\mathrm{span}\{ I,A,A^2... \} = \mathrm{span} \{ I,A,A^2,..., A^{k-1}\}$ So we have that $m_A = a_0 + a_1x + ... +x^{k}$ (Notice that the coefficient of $x^{k}$ is $1$) Since, $m_A$ is the minimal polynomial: $$m_A(A) = 0 \\a_0I+a_1A + ... + A^{k} = 0$$ What should I do next? I am kinda stuck","Let $A\in M_n(F)$ and $k=\deg(m_A)$ where $m_A$ is the minimal polynomial of $A$. Prove that $\mathrm{span}\{ I,A,A^2... \} = \mathrm{span} \{ I,A,A^2,..., A^{k-1}\}$ So we have that $m_A = a_0 + a_1x + ... +x^{k}$ (Notice that the coefficient of $x^{k}$ is $1$) Since, $m_A$ is the minimal polynomial: $$m_A(A) = 0 \\a_0I+a_1A + ... + A^{k} = 0$$ What should I do next? I am kinda stuck",,"['linear-algebra', 'matrices', 'vector-spaces', 'eigenvalues-eigenvectors']"
25,If two real symmetric square matrices commute then does they have a common eigenvector ?,If two real symmetric square matrices commute then does they have a common eigenvector ?,,"Let $A,B$ be real symmetric $n \times n$ matrices such that $AB=BA$ , then is it true that $A,B$ have a common eigenvector in $\mathbb R^n$ ?","Let $A,B$ be real symmetric $n \times n$ matrices such that $AB=BA$ , then is it true that $A,B$ have a common eigenvector in $\mathbb R^n$ ?",,"['matrices', 'eigenvalues-eigenvectors']"
26,"Number of positive, negative eigenvalues and the number of sign changes in the determinants of the upper left submatrices of a symmetric matrix.","Number of positive, negative eigenvalues and the number of sign changes in the determinants of the upper left submatrices of a symmetric matrix.",,"How do we prove that the number of sign changes in the sequence of the determinants of the upper-left matrices of a symmetric matrix $A$ corresponds to the number of positive and negative eigenvalues of $A$ ? Progress I know that it is true for symmetric matrices that if all the upper left determinants are positive, then all the eigenvalues are positive. This is usually stated in texts when positive definite matrices are introduced.","How do we prove that the number of sign changes in the sequence of the determinants of the upper-left matrices of a symmetric matrix corresponds to the number of positive and negative eigenvalues of ? Progress I know that it is true for symmetric matrices that if all the upper left determinants are positive, then all the eigenvalues are positive. This is usually stated in texts when positive definite matrices are introduced.",A A,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
27,Is the set of invertible upper triangular matrices open in $GL_n(\mathbb R)$? Is it open in the set of all upper triangular matrices?,Is the set of invertible upper triangular matrices open in ? Is it open in the set of all upper triangular matrices?,GL_n(\mathbb R),"I think the answer to the second question is yes, but can't quite prove this. I've no idea about the first part. I've done a few exercises of this kind but all have used the continuity of the determinant function and the same idea doesn't seem to work here. How do I do these?","I think the answer to the second question is yes, but can't quite prove this. I've no idea about the first part. I've done a few exercises of this kind but all have used the continuity of the determinant function and the same idea doesn't seem to work here. How do I do these?",,"['linear-algebra', 'general-topology', 'matrices']"
28,Showing that a matrix is invertible and finding its inverse,Showing that a matrix is invertible and finding its inverse,,"I'm incredibly rusty at linear algebra, and in preparation for my course I've been doing some review questions. I've been staring at this one for a half hour and still don't know how to approach it: ""Let A be a square matrix such that $A^3 = 0$. Show that the matrix $I + A + 2A^2$ is invertible and find its inverse."" I'm pretty sure I need to find a relationship between $A^3$ and $I + A + 2A^2$, but I'm not sure how. A matrix is invertible if the determinant is nonzero, and I know how to find the inverse of a matrix, but since this is a more theoretical question I'm not entirely certain how to approach it. Any hints would be much-appreciated :)","I'm incredibly rusty at linear algebra, and in preparation for my course I've been doing some review questions. I've been staring at this one for a half hour and still don't know how to approach it: ""Let A be a square matrix such that $A^3 = 0$. Show that the matrix $I + A + 2A^2$ is invertible and find its inverse."" I'm pretty sure I need to find a relationship between $A^3$ and $I + A + 2A^2$, but I'm not sure how. A matrix is invertible if the determinant is nonzero, and I know how to find the inverse of a matrix, but since this is a more theoretical question I'm not entirely certain how to approach it. Any hints would be much-appreciated :)",,"['linear-algebra', 'matrices', 'inverse']"
29,What are the conditions should be added so that submatrix has full rank,What are the conditions should be added so that submatrix has full rank,,"Suppose a $6 \times 4$ matrix satisfies the following where $\alpha, \beta, \gamma, \theta, \sigma, \mu$ are non-zero. What are the conditions should be added so that any $4 \times 4$ submatrix  has full rank? I think the conditions are $\alpha_i \neq \beta_j \neq \gamma_k$ and $\theta_i \neq \sigma_j \neq \mu_k$ for $i,j,k=1,2,3$, in other words, all entries of both rank $3$ matrices are distinct. But I don't know whether the conditions are sufficient to conclude the statement. Can anyone help me? EDIT: Okay, so the conditions stated above are not sufficient. Can I use an $6 \times 4$ Cauchy matrix instead? Because any square-submatrix of a cauchy matrix has full rank. But in this case, we have a few zeros in the matrix. So I don't know whether these zeros will affect the rank of submatrix or not. Also, what if I change to finite field?","Suppose a $6 \times 4$ matrix satisfies the following where $\alpha, \beta, \gamma, \theta, \sigma, \mu$ are non-zero. What are the conditions should be added so that any $4 \times 4$ submatrix  has full rank? I think the conditions are $\alpha_i \neq \beta_j \neq \gamma_k$ and $\theta_i \neq \sigma_j \neq \mu_k$ for $i,j,k=1,2,3$, in other words, all entries of both rank $3$ matrices are distinct. But I don't know whether the conditions are sufficient to conclude the statement. Can anyone help me? EDIT: Okay, so the conditions stated above are not sufficient. Can I use an $6 \times 4$ Cauchy matrix instead? Because any square-submatrix of a cauchy matrix has full rank. But in this case, we have a few zeros in the matrix. So I don't know whether these zeros will affect the rank of submatrix or not. Also, what if I change to finite field?",,"['linear-algebra', 'matrices', 'matrix-rank']"
30,Derivative of matrix product: is it true that $\frac{d}{dt}(A^TA) = 2A^T \frac{dA}{dt}$?,Derivative of matrix product: is it true that ?,\frac{d}{dt}(A^TA) = 2A^T \frac{dA}{dt},"$A$ is a square matrix.  All elements of $A$ depend on a parameter $t$, that is, $a_{ij}=a_{ij}(t)$.  Let $S(A):=A^TA$, and take the derivative of $S$ w.r.t. $t$: $\displaystyle \frac{dS}{dt}$ Now, pretty clearly $\displaystyle \frac{dS}{dt} = \frac{dA^T}{dt}A + A^T\frac{dA}{dt}$ But now, can this also be written $\displaystyle \frac{dS}{dt} = 2 A^T \frac{dA}{dt}$  ? A math text I am working though right now -- if I am reading it right -- implies that this is the case, but I haven't been able to prove it myself. Thanks.","$A$ is a square matrix.  All elements of $A$ depend on a parameter $t$, that is, $a_{ij}=a_{ij}(t)$.  Let $S(A):=A^TA$, and take the derivative of $S$ w.r.t. $t$: $\displaystyle \frac{dS}{dt}$ Now, pretty clearly $\displaystyle \frac{dS}{dt} = \frac{dA^T}{dt}A + A^T\frac{dA}{dt}$ But now, can this also be written $\displaystyle \frac{dS}{dt} = 2 A^T \frac{dA}{dt}$  ? A math text I am working though right now -- if I am reading it right -- implies that this is the case, but I haven't been able to prove it myself. Thanks.",,"['matrices', 'derivatives', 'matrix-calculus']"
31,"Suppose A has eigenvalues 1,2, 4.","Suppose A has eigenvalues 1,2, 4.",,"a) What is the trace of  $A^2$ b) What is the determinant of $(A^{-1})^T$ I need someone to check my answers and correct me, am especially not sure about part a), help me me out; for a), I did--- Trace $A = 1+2+4 = 7$. So trace $A^2 = 14$ for b) $det(A^{-1})^T = 1/(1\times2\times4) = 1/8$","a) What is the trace of  $A^2$ b) What is the determinant of $(A^{-1})^T$ I need someone to check my answers and correct me, am especially not sure about part a), help me me out; for a), I did--- Trace $A = 1+2+4 = 7$. So trace $A^2 = 14$ for b) $det(A^{-1})^T = 1/(1\times2\times4) = 1/8$",,"['linear-algebra', 'matrices', 'vector-spaces', 'eigenvalues-eigenvectors']"
32,Solving inhomogenous ODE,Solving inhomogenous ODE,,"I have an inhomogenous  ODE. The main issue here is variables are matrices. It is bit of matrix calculus.    A solution would be highly appreciated interms of x . I guess we can use same methods for solving ODEs but have to be careful because these are matrices $R'(x)-(C_1 +C_2 x) R(x) = R_1-C_1 R_0\, x $ where  except x rest are $3\times 3$ matrices means $C_1,C_2,R'(x),R(x)$  all are matrices. x is a scalar variable. $ C_1,C_2,R_0 $ are constant $3\times 3$  matrices . .$C_1$ and $C_2$ are skew symmetric matrices","I have an inhomogenous  ODE. The main issue here is variables are matrices. It is bit of matrix calculus.    A solution would be highly appreciated interms of x . I guess we can use same methods for solving ODEs but have to be careful because these are matrices $R'(x)-(C_1 +C_2 x) R(x) = R_1-C_1 R_0\, x $ where  except x rest are $3\times 3$ matrices means $C_1,C_2,R'(x),R(x)$  all are matrices. x is a scalar variable. $ C_1,C_2,R_0 $ are constant $3\times 3$  matrices . .$C_1$ and $C_2$ are skew symmetric matrices",,"['matrices', 'ordinary-differential-equations', 'algorithms', 'matrix-equations', 'matrix-calculus']"
33,"Prove that if product of matrices is singular, one of the matrices is singular.","Prove that if product of matrices is singular, one of the matrices is singular.",,"I'm having trouble with this proof, it would be much easier to work out the other way it seems. Let $A$ and $B$ be square matrices of equal size. Prove that if $\det(AB) = 0 =C$ then either $A$ or $B$ must be singular. I claimed that because $AB$, denoted $C$, is zero then by rule of matrix product that one has to be zero. This is obviously not a real proof","I'm having trouble with this proof, it would be much easier to work out the other way it seems. Let $A$ and $B$ be square matrices of equal size. Prove that if $\det(AB) = 0 =C$ then either $A$ or $B$ must be singular. I claimed that because $AB$, denoted $C$, is zero then by rule of matrix product that one has to be zero. This is obviously not a real proof",,"['linear-algebra', 'matrices', 'proof-writing']"
34,Prove that if $A$ is invertible then $AA^\top$ is positive definite [duplicate],Prove that if  is invertible then  is positive definite [duplicate],A AA^\top,"This question already has answers here : $A^{T}A$ positive definite then A is invertible? (3 answers) Closed 10 years ago . I need to prove that if $A$ is a square invertible matrix then $AA^\top$ ($A$ multiply $A$ transpose) is positive definite. I tried to prove that all the eigenvalues are positive. I know that $AA^\top$ is symmetric, and its determinant is positive , and trace is positive . Therefore: product of eigenvalues is positive , and sum of eigenvalues is positive . In addition $0$ is not an eigenvalue. That's what I know so far but I still cannot conclude that every eigenvalue for itself is positive. What have I missed? Thanks!","This question already has answers here : $A^{T}A$ positive definite then A is invertible? (3 answers) Closed 10 years ago . I need to prove that if $A$ is a square invertible matrix then $AA^\top$ ($A$ multiply $A$ transpose) is positive definite. I tried to prove that all the eigenvalues are positive. I know that $AA^\top$ is symmetric, and its determinant is positive , and trace is positive . Therefore: product of eigenvalues is positive , and sum of eigenvalues is positive . In addition $0$ is not an eigenvalue. That's what I know so far but I still cannot conclude that every eigenvalue for itself is positive. What have I missed? Thanks!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
35,What is the derivative of a skew symmetric matrix?,What is the derivative of a skew symmetric matrix?,,"I'm trying to work out some Jacobians and I ran across a problem. If I have a function of a vector making it a skew symmetric matrix, like below, what is the derivative $f'$? $$ f(\boldsymbol{\omega}) = \lfloor \boldsymbol{\omega} \, \times \rfloor = \left( \begin{array}{ccc} 0 & -\omega_3 & \omega_2 \\ \omega_3 & 0 & -\omega_1 \\ -\omega_2 & \omega_1 & 0 \end{array} \right) $$","I'm trying to work out some Jacobians and I ran across a problem. If I have a function of a vector making it a skew symmetric matrix, like below, what is the derivative $f'$? $$ f(\boldsymbol{\omega}) = \lfloor \boldsymbol{\omega} \, \times \rfloor = \left( \begin{array}{ccc} 0 & -\omega_3 & \omega_2 \\ \omega_3 & 0 & -\omega_1 \\ -\omega_2 & \omega_1 & 0 \end{array} \right) $$",,"['matrices', 'derivatives', 'matrix-calculus', 'skew-symmetric-matrices']"
36,Find a matrix such that $Ax=0$,Find a matrix such that,Ax=0,"Let $$W = span\left\{ {\left( {\matrix{    1  \cr     0  \cr     0  \cr     1  \cr   } } \right),\left( {\matrix{    0  \cr     2  \cr     1  \cr     { - 1}  \cr   } } \right)} \right\}$$ I was asked to find a matrix, $A$ which satisify the following: $$W = \left\{ {x \in {\mathbb{R}^4}|Ax = 0} \right\}$$ $A$ must be $n\times 4$. $W$ is actually $null(A)$, so every linear combination of the vectors is a solution for $Ax = 0$. Other then those observations, I am not sure how to find $A$. Looking for guidance. Thanks.","Let $$W = span\left\{ {\left( {\matrix{    1  \cr     0  \cr     0  \cr     1  \cr   } } \right),\left( {\matrix{    0  \cr     2  \cr     1  \cr     { - 1}  \cr   } } \right)} \right\}$$ I was asked to find a matrix, $A$ which satisify the following: $$W = \left\{ {x \in {\mathbb{R}^4}|Ax = 0} \right\}$$ $A$ must be $n\times 4$. $W$ is actually $null(A)$, so every linear combination of the vectors is a solution for $Ax = 0$. Other then those observations, I am not sure how to find $A$. Looking for guidance. Thanks.",,"['linear-algebra', 'matrices', 'vector-spaces']"
37,Regular matrix and regular stochastic matrix,Regular matrix and regular stochastic matrix,,"We know that : A matrix is regular if its determinant is non zero. A stochastic matrix is regular if at a certain power all elements are positive. Question is how can I make the link between the two definitions, if there are any ? thanks in advance.","We know that : A matrix is regular if its determinant is non zero. A stochastic matrix is regular if at a certain power all elements are positive. Question is how can I make the link between the two definitions, if there are any ? thanks in advance.",,"['linear-algebra', 'matrices', 'stochastic-processes', 'stochastic-calculus']"
38,Inverse matrix for a matrix with sinus and cosinus functions,Inverse matrix for a matrix with sinus and cosinus functions,,"I have this matrix A: $$\left(\begin{array}{cc} \cos x & -\sin x \\ \sin x & \cos x \end{array}\right)$$ and I need to create an inverse matrix for this matrix A. The sinus and cosinus functions in there makes me confused, I don't know how to start and proceed. To count a determinant from this matrix is kinda easy, but how to count an inverse matrix to this? Thank you","I have this matrix A: $$\left(\begin{array}{cc} \cos x & -\sin x \\ \sin x & \cos x \end{array}\right)$$ and I need to create an inverse matrix for this matrix A. The sinus and cosinus functions in there makes me confused, I don't know how to start and proceed. To count a determinant from this matrix is kinda easy, but how to count an inverse matrix to this? Thank you",,"['matrices', 'matrix-equations', 'matrix-calculus']"
39,Example: Sum of non-commuting matrices is not normal,Example: Sum of non-commuting matrices is not normal,,"I'm trying to find an example of two non-commuting normal matrices, such that their sum is not normal. I know that unitary, orthogonal, hermitian and symmetric matrices are all normal. I figure it can't be the sum of symmetric or hermitian matrices, because they would be symmetric (hermitian) again. I also know that 2x2 orthogonal matrices are also symmetric so their sum wouldn't be an example either. Any help on which characterstics I could play with to find such an example would be greatly appreciated!","I'm trying to find an example of two non-commuting normal matrices, such that their sum is not normal. I know that unitary, orthogonal, hermitian and symmetric matrices are all normal. I figure it can't be the sum of symmetric or hermitian matrices, because they would be symmetric (hermitian) again. I also know that 2x2 orthogonal matrices are also symmetric so their sum wouldn't be an example either. Any help on which characterstics I could play with to find such an example would be greatly appreciated!",,"['linear-algebra', 'matrices']"
40,"Prove that $AB=BA$ if $A, B$ are diagonal matrices",Prove that  if  are diagonal matrices,"AB=BA A, B","Could you confirm my proof? A fixed Proof (Confirm please): Let $A, B$ be two diagonal matrices of order $n$. Then, both $AB,BA$ are defined and are of the same order $n$ (i.e. sizes match). Also, $A_{ij},B_{ij}=0$ whenever $i\ne j$. Consider the case $i\ne j$: $$\eqalign{   & {\left( {AB} \right)_{ij}} = \sum\limits_{k = 1}^n {{A_{ik}}{B_{kj}}}  = \sum\limits_{i \ne k = j} {{A_{ik}}{B_{kj}}}  + \sum\limits_{i = k \ne j} {{A_{ik}}{B_{kj}}}  + \sum\limits_{i \ne k \ne j} {{A_{ik}}{B_{kj}}}  = \sum\limits_{i \ne k = j} {0 \cdot {B_{kj}}}  + \sum\limits_{i = k \ne j} {{A_{ik}} \cdot 0}  + \sum\limits_{i \ne k \ne j} 0   \cr    &  = 0 + 0 + 0 = 0 = \sum\limits_{i = k \ne j} {{B_{ik}} \cdot 0}  + \sum\limits_{i \ne k = j} {0 \cdot {A_{kj}}}  + \sum\limits_{i \ne k \ne j} 0  = \sum\limits_{i = k \ne j} {{B_{ik}} \cdot {A_{kj}}}  + \sum\limits_{i \ne k = j} {{B_{ik}} \cdot {A_{kj}}}  + \sum\limits_{i \ne k \ne j} {{A_{ik}}{B_{kj}}}   \cr    &  = \sum\limits_{k = 1}^n {{B_{ik}}{A_{kj}}}  = {\left( {BA} \right)_{ij}} \cr} $$ Consider the case  $i=j$ $$\eqalign{   & {\left( {AB} \right)_{ij}} = {\left( {AB} \right)_{ii}} = \sum\limits_{k = 1}^n {{A_{ik}}{B_{ki}}}  = \sum\limits_{k \ne i} {{A_{ik}}{B_{ki}}}  + \sum\limits_{k = i} {{A_{ik}}{B_{ki}}}  = \sum\limits_{k \ne i} {0 \cdot 0}  + {A_{ii}}{B_{ii}} = 0 + {A_{ii}}{B_{ii}}  \cr    &  = 0 + {B_{ii}}{A_{ii}} = \sum\limits_{k \ne i} {0 \cdot 0}  + {B_{ii}}{A_{ii}} = \sum\limits_{k \ne i} {{B_{ik}}{A_{ki}}}  + \sum\limits_{k = i} {{B_{ik}}{A_{ki}}}  = \sum\limits_{k = 1}^n {{B_{ik}}{A_{ki}}}  = {\left( {BA} \right)_{ii}} = {\left( {BA} \right)_{ij}} \cr} $$ Hence, corresponding entries are equal. Thus, $AB=BA$. Quod Erat Demonstrandum. Thanks in advance","Could you confirm my proof? A fixed Proof (Confirm please): Let $A, B$ be two diagonal matrices of order $n$. Then, both $AB,BA$ are defined and are of the same order $n$ (i.e. sizes match). Also, $A_{ij},B_{ij}=0$ whenever $i\ne j$. Consider the case $i\ne j$: $$\eqalign{   & {\left( {AB} \right)_{ij}} = \sum\limits_{k = 1}^n {{A_{ik}}{B_{kj}}}  = \sum\limits_{i \ne k = j} {{A_{ik}}{B_{kj}}}  + \sum\limits_{i = k \ne j} {{A_{ik}}{B_{kj}}}  + \sum\limits_{i \ne k \ne j} {{A_{ik}}{B_{kj}}}  = \sum\limits_{i \ne k = j} {0 \cdot {B_{kj}}}  + \sum\limits_{i = k \ne j} {{A_{ik}} \cdot 0}  + \sum\limits_{i \ne k \ne j} 0   \cr    &  = 0 + 0 + 0 = 0 = \sum\limits_{i = k \ne j} {{B_{ik}} \cdot 0}  + \sum\limits_{i \ne k = j} {0 \cdot {A_{kj}}}  + \sum\limits_{i \ne k \ne j} 0  = \sum\limits_{i = k \ne j} {{B_{ik}} \cdot {A_{kj}}}  + \sum\limits_{i \ne k = j} {{B_{ik}} \cdot {A_{kj}}}  + \sum\limits_{i \ne k \ne j} {{A_{ik}}{B_{kj}}}   \cr    &  = \sum\limits_{k = 1}^n {{B_{ik}}{A_{kj}}}  = {\left( {BA} \right)_{ij}} \cr} $$ Consider the case  $i=j$ $$\eqalign{   & {\left( {AB} \right)_{ij}} = {\left( {AB} \right)_{ii}} = \sum\limits_{k = 1}^n {{A_{ik}}{B_{ki}}}  = \sum\limits_{k \ne i} {{A_{ik}}{B_{ki}}}  + \sum\limits_{k = i} {{A_{ik}}{B_{ki}}}  = \sum\limits_{k \ne i} {0 \cdot 0}  + {A_{ii}}{B_{ii}} = 0 + {A_{ii}}{B_{ii}}  \cr    &  = 0 + {B_{ii}}{A_{ii}} = \sum\limits_{k \ne i} {0 \cdot 0}  + {B_{ii}}{A_{ii}} = \sum\limits_{k \ne i} {{B_{ik}}{A_{ki}}}  + \sum\limits_{k = i} {{B_{ik}}{A_{ki}}}  = \sum\limits_{k = 1}^n {{B_{ik}}{A_{ki}}}  = {\left( {BA} \right)_{ii}} = {\left( {BA} \right)_{ij}} \cr} $$ Hence, corresponding entries are equal. Thus, $AB=BA$. Quod Erat Demonstrandum. Thanks in advance",,"['linear-algebra', 'matrices']"
41,"If $A$ is normal such that $AB=BA$, then $A^*B=BA^*$","If  is normal such that , then",A AB=BA A^*B=BA^*,"Please help me. Let $A,B\in M_n(\mathbb{C})$. Show that if $A$ is normal such that $AB=BA$, then $A^*B=BA^*$.","Please help me. Let $A,B\in M_n(\mathbb{C})$. Show that if $A$ is normal such that $AB=BA$, then $A^*B=BA^*$.",,"['linear-algebra', 'matrices', 'matrix-equations']"
42,Norm of a matrix and lower bound for its determinant,Norm of a matrix and lower bound for its determinant,,"Assume that $M$ is a positive constant, $A=[a_{ij}]$ is a matrix, and $\vert a_{ij}\vert \geq M $ for all $1\leq i,j \leq n$. Also, assume that $\det(A) \neq 0$ .Can we conclude that there exists a constant $C > 0$ such that $\det(A) \geq CM$?","Assume that $M$ is a positive constant, $A=[a_{ij}]$ is a matrix, and $\vert a_{ij}\vert \geq M $ for all $1\leq i,j \leq n$. Also, assume that $\det(A) \neq 0$ .Can we conclude that there exists a constant $C > 0$ such that $\det(A) \geq CM$?",,"['linear-algebra', 'matrices', 'inequality', 'determinant', 'upper-lower-bounds']"
43,Moment Matrix Positive Semidefinite,Moment Matrix Positive Semidefinite,,"Let $\phi(x)$ be a probability distribution on$[0,1]$, and consider the moment matrix $M$ where the $(i,j)^{th}$ entry is $$ M_{ij} := \int_0^1 x^{i+j}\phi(x)dx, $$ or in other words, the expectation $\mathbb E(x^{i+j})$. Is there an easy way to see that this matrix is positive semidefinite? The matrix is definitely symmetric, and $$z^T Mz = \begin{bmatrix} \sum_{i=1}^n M_{i1}z_i & \cdots & \sum_{i=1}^nM_{in}z_i \end{bmatrix} \begin{bmatrix} z_1\\\vdots\\z_n \end{bmatrix} $$ $$= \left( \sum_{i=1}^nM_{i1}z_i \right) z_1 + \cdots + \left( \sum_{i=1}^nM_{in}z_i \right) z_n $$ $$= \left( \sum_{i=1}^n\mathbb E(x^{i+1})z_i \right) z_1 + \cdots + \left( \sum_{i=1}^n\mathbb E(x^{i+n})z_i \right) z_n$$ I got stuck here aiming to prove it via the definition. Or perhaps there is an eigenvalue approach?","Let $\phi(x)$ be a probability distribution on$[0,1]$, and consider the moment matrix $M$ where the $(i,j)^{th}$ entry is $$ M_{ij} := \int_0^1 x^{i+j}\phi(x)dx, $$ or in other words, the expectation $\mathbb E(x^{i+j})$. Is there an easy way to see that this matrix is positive semidefinite? The matrix is definitely symmetric, and $$z^T Mz = \begin{bmatrix} \sum_{i=1}^n M_{i1}z_i & \cdots & \sum_{i=1}^nM_{in}z_i \end{bmatrix} \begin{bmatrix} z_1\\\vdots\\z_n \end{bmatrix} $$ $$= \left( \sum_{i=1}^nM_{i1}z_i \right) z_1 + \cdots + \left( \sum_{i=1}^nM_{in}z_i \right) z_n $$ $$= \left( \sum_{i=1}^n\mathbb E(x^{i+1})z_i \right) z_1 + \cdots + \left( \sum_{i=1}^n\mathbb E(x^{i+n})z_i \right) z_n$$ I got stuck here aiming to prove it via the definition. Or perhaps there is an eigenvalue approach?",,"['linear-algebra', 'probability', 'matrices']"
44,How do you find angular velocity given a pair of 3x3 rotation matrices?,How do you find angular velocity given a pair of 3x3 rotation matrices?,,"Let's say I have two 3x3 rotation matrices R1 and R2, each signifying rotation from the global frame to the local frame. I am also given the time difference t between these two matrices. How would I find the angular velocity vector [wx,wy,wz] from this information? The relationship between rotation matrices and angular velocity is described here: http://www.physicsforums.com/showthread.php?t=522190 where it seems like the angular velocity matrix is: W(t) = dR(t)/dt * R(t)^-1 However, I am having trouble figuring out what dR/dt is, is it just: dR(t)/dt = R(t) - R(t-1) / t Using the above ""relation"" with actual data does not yield the correct angular velocity tensor, where the diagonal elements are 0's. Alternatively, it may be easier to convert the rotation matrices to quarternions first. However, I am not clear on the relationship between quarternions, time, and angular velocity.","Let's say I have two 3x3 rotation matrices R1 and R2, each signifying rotation from the global frame to the local frame. I am also given the time difference t between these two matrices. How would I find the angular velocity vector [wx,wy,wz] from this information? The relationship between rotation matrices and angular velocity is described here: http://www.physicsforums.com/showthread.php?t=522190 where it seems like the angular velocity matrix is: W(t) = dR(t)/dt * R(t)^-1 However, I am having trouble figuring out what dR/dt is, is it just: dR(t)/dt = R(t) - R(t-1) / t Using the above ""relation"" with actual data does not yield the correct angular velocity tensor, where the diagonal elements are 0's. Alternatively, it may be easier to convert the rotation matrices to quarternions first. However, I am not clear on the relationship between quarternions, time, and angular velocity.",,"['matrices', 'rotations', 'matrix-calculus']"
45,Why can matrix exponentiation be done by squaring?,Why can matrix exponentiation be done by squaring?,,"Matrix multiplication is not communative: A*B != B*A Then why can matrix exponentiation be done by squaring? I have tried searching for special cases where this rule did not apply, but from what I've understood, none of it seemed to apply specifically to powers of two equal matrices. For example a matrix M is raised to a 12, then which of the following is true? M 4 * M 8 = M 12 OR M 8 * M 4 = M 12 ? Do we need to follow a particular order while calculating matrix exponentiation by squaring ? If yes then how is the order determined and why does it lead to the correct exponentiation. If not then why doesn't the fact that matrix multiplication is not communative does not affect it? Can this be explained this to me in plain english? Thanks in advance.","Matrix multiplication is not communative: A*B != B*A Then why can matrix exponentiation be done by squaring? I have tried searching for special cases where this rule did not apply, but from what I've understood, none of it seemed to apply specifically to powers of two equal matrices. For example a matrix M is raised to a 12, then which of the following is true? M 4 * M 8 = M 12 OR M 8 * M 4 = M 12 ? Do we need to follow a particular order while calculating matrix exponentiation by squaring ? If yes then how is the order determined and why does it lead to the correct exponentiation. If not then why doesn't the fact that matrix multiplication is not communative does not affect it? Can this be explained this to me in plain english? Thanks in advance.",,"['matrices', 'exponentiation']"
46,Inverse of a matrix,Inverse of a matrix,,"I'm trying to show that  \begin{equation} P^H ( I_M + PBP^H) ^{-1} P = \big( (P^H P)^{-1} + B \big)^{-1}, \end{equation} where $P$ is an $M$-by-$N$ matrix, $I_M$ is the $M$-by-$M$ identity matrix, $B$ is an $N$-by-$N$ matrix, and $(P^H P)$ is invertible. I've used various versions of matrix inversion lemmas, but I'm stuck. How can the above equality be shown?","I'm trying to show that  \begin{equation} P^H ( I_M + PBP^H) ^{-1} P = \big( (P^H P)^{-1} + B \big)^{-1}, \end{equation} where $P$ is an $M$-by-$N$ matrix, $I_M$ is the $M$-by-$M$ identity matrix, $B$ is an $N$-by-$N$ matrix, and $(P^H P)$ is invertible. I've used various versions of matrix inversion lemmas, but I'm stuck. How can the above equality be shown?",,"['linear-algebra', 'matrices']"
47,"How sum work, vectors and matrices","How sum work, vectors and matrices",,"I am having troubles understanding what is going on here. Would anyone be able to do this step by step with values so that I will be able to understand the SUM and how it works. xi is in this case colum 1, and Matrix X is the entire thing. Matrix X Also note that looking at the large general matrix you it should have a diagonal with sum of squares and off-diagonal is the cross product. Also to clarify X`X(where 'is the transpose) is equal to the one above. This I am not able to understand. To Clarify the question, It is stated in the book that X'X should be equal to the sumof xixi'. --> as shown in the picture. however if I take Matrix(as given above) X'X i get {{4,14},{14,54}}. When I try this with the sum function I am not able to get the same result, could someone please be so kind to take the effort writing this example out step by step.","I am having troubles understanding what is going on here. Would anyone be able to do this step by step with values so that I will be able to understand the SUM and how it works. xi is in this case colum 1, and Matrix X is the entire thing. Matrix X Also note that looking at the large general matrix you it should have a diagonal with sum of squares and off-diagonal is the cross product. Also to clarify X`X(where 'is the transpose) is equal to the one above. This I am not able to understand. To Clarify the question, It is stated in the book that X'X should be equal to the sumof xixi'. --> as shown in the picture. however if I take Matrix(as given above) X'X i get {{4,14},{14,54}}. When I try this with the sum function I am not able to get the same result, could someone please be so kind to take the effort writing this example out step by step.",,"['matrices', 'random-matrices']"
48,Finding determinant using properties of determinant without expanding [duplicate],Finding determinant using properties of determinant without expanding [duplicate],,This question already has an answer here : Finding determinant of matrix without expanding (1 answer) Closed 10 years ago . show that determinant $$\left|\matrix{ x^2+L  &  xy   &   xz \\ xy   &    y^2+L &   yz \\ xz   &    yz   &   z^2+L \\ }\right| =  L^2(x^2+y^2+z^2+L)$$ without expanding by using the appropriate properties of determinant . All i can do is LHS $$x^2y^2z^2\left|\matrix{ 1+L/x^2  & 1  &   1 \\ 1   &   1+L/y^2 &   1 \\ 1  &    1   &   1+L/z^2 \\ }\right|$$ Is it a must to relate to eigenvalue problem?,This question already has an answer here : Finding determinant of matrix without expanding (1 answer) Closed 10 years ago . show that determinant $$\left|\matrix{ x^2+L  &  xy   &   xz \\ xy   &    y^2+L &   yz \\ xz   &    yz   &   z^2+L \\ }\right| =  L^2(x^2+y^2+z^2+L)$$ without expanding by using the appropriate properties of determinant . All i can do is LHS $$x^2y^2z^2\left|\matrix{ 1+L/x^2  & 1  &   1 \\ 1   &   1+L/y^2 &   1 \\ 1  &    1   &   1+L/z^2 \\ }\right|$$ Is it a must to relate to eigenvalue problem?,,"['matrices', 'determinant']"
49,Show that $\det(A-\lambda B)$ is a nonconstant polynomial if $B$ is invertible,Show that  is a nonconstant polynomial if  is invertible,\det(A-\lambda B) B,"Let $A$ and $B$ be arbitrary complex square matrices. If $B$ is invertible, show that $$p(\lambda)=\det(A-\lambda B)$$ is a nonconstant polynomial in $\lambda$.","Let $A$ and $B$ be arbitrary complex square matrices. If $B$ is invertible, show that $$p(\lambda)=\det(A-\lambda B)$$ is a nonconstant polynomial in $\lambda$.",,"['linear-algebra', 'matrices']"
50,Prove Solving a Lower Triangular Matrix By Forward Substitution is Backwards Stable,Prove Solving a Lower Triangular Matrix By Forward Substitution is Backwards Stable,,"I'm taking a class in scientific computing and we are working on proving stability of certain algorithms. Unfortunately, at this stage, everything is proof-based, and I have little to no experience in proofs. Any suggestions are greatly appreciated. Let L be a nonsingular lower triangular matrix. How can I prove solving Lx = b by forward substitution is backwards stable? I know forward substitution is defined by the algo: $$ \ y_i = {l_{ii}}^{-1}* ( b_i-\sum^{i-1}_{j=1} l_{ij}*y_{i}) $$ and that backwards stability implies $$ \frac{||x - \tilde{x} ||}{||x||} $$ where $$ \tilde{x} = $$  the computer approximation of some real x But I'm lost as to how to even get this started. Please advise! P.S. this is my first interaction with LaTxT-- and I'm in love","I'm taking a class in scientific computing and we are working on proving stability of certain algorithms. Unfortunately, at this stage, everything is proof-based, and I have little to no experience in proofs. Any suggestions are greatly appreciated. Let L be a nonsingular lower triangular matrix. How can I prove solving Lx = b by forward substitution is backwards stable? I know forward substitution is defined by the algo: $$ \ y_i = {l_{ii}}^{-1}* ( b_i-\sum^{i-1}_{j=1} l_{ij}*y_{i}) $$ and that backwards stability implies $$ \frac{||x - \tilde{x} ||}{||x||} $$ where $$ \tilde{x} = $$  the computer approximation of some real x But I'm lost as to how to even get this started. Please advise! P.S. this is my first interaction with LaTxT-- and I'm in love",,"['linear-algebra', 'matrices', 'algorithms']"
51,Maximize the determinant,Maximize the determinant,,"Over the class $S$ of symmetric $n$ by $n$ matrices such that the diagonal entries are +1 and off diagonals are between $-1$ and $+1$ (inclusive/exclusive), is $$\max_{A \in S} \det A = \det(I_n)$$ ?","Over the class $S$ of symmetric $n$ by $n$ matrices such that the diagonal entries are +1 and off diagonals are between $-1$ and $+1$ (inclusive/exclusive), is $$\max_{A \in S} \det A = \det(I_n)$$ ?",,"['matrices', 'analysis', 'optimization']"
52,"Show that the diagonal entries of symmetric & idempotent matrix must be in [$0,1$]",Show that the diagonal entries of symmetric & idempotent matrix must be in [],"0,1","Show that the diagonal entries of symmetric & idempotent  matrix must be in [$0,1$]. Let $A$ be a symmetric and idempotent $n \times n$ matrix. By the definition of eigenvectors and since $A$ is an idempotent, $Ax=\lambda x \implies A^2x=\lambda Ax \implies Ax=\lambda Ax=\lambda^2 x.$ So $\lambda^2=\lambda$ and hence $\lambda \in \{0,1\}$. To show the part about the ""diagonal matrix"" I use the fact that every symmetric matrix is diagonalizable. Is this a complete proof?","Show that the diagonal entries of symmetric & idempotent  matrix must be in [$0,1$]. Let $A$ be a symmetric and idempotent $n \times n$ matrix. By the definition of eigenvectors and since $A$ is an idempotent, $Ax=\lambda x \implies A^2x=\lambda Ax \implies Ax=\lambda Ax=\lambda^2 x.$ So $\lambda^2=\lambda$ and hence $\lambda \in \{0,1\}$. To show the part about the ""diagonal matrix"" I use the fact that every symmetric matrix is diagonalizable. Is this a complete proof?",,"['linear-algebra', 'matrices', 'proof-verification']"
53,Applications/Motivations of matrix decomposition techniques,Applications/Motivations of matrix decomposition techniques,,"Matrix decomposition is one area of matrices that has always intrigued me. Every time I open a matrix book, I can interestingly follow it till Eigen values and Eigen vectors because they are well motivated. However, I am not able to understand the motivation of matrix decomposition techniques. I am just revising on Matrices before I start learning linear and non linear optimization methods for machine learning. It would be great if people here could motivate the need and benefits of matrix decomposition techniques like LU, QR, Cholesky, SVD etc. Also, any interpretations of matrix decomposition techniques (if any) would be great. Thanks!","Matrix decomposition is one area of matrices that has always intrigued me. Every time I open a matrix book, I can interestingly follow it till Eigen values and Eigen vectors because they are well motivated. However, I am not able to understand the motivation of matrix decomposition techniques. I am just revising on Matrices before I start learning linear and non linear optimization methods for machine learning. It would be great if people here could motivate the need and benefits of matrix decomposition techniques like LU, QR, Cholesky, SVD etc. Also, any interpretations of matrix decomposition techniques (if any) would be great. Thanks!",,"['matrices', 'block-matrices', 'svd']"
54,"How to prove that a matrix $U$ is unitary, if and only if the columns form an orthonormal basis?","How to prove that a matrix  is unitary, if and only if the columns form an orthonormal basis?",U,"And also, is it true that a matrix is unitary if and only if $T^{-1}=T^{*}$ ? Thanks.","And also, is it true that a matrix is unitary if and only if $T^{-1}=T^{*}$ ? Thanks.",,"['linear-algebra', 'matrices']"
55,"Find all the linear involutions $f: E \to E$, where $E$ is a finite-dimensional real vector space","Find all the linear involutions , where  is a finite-dimensional real vector space",f: E \to E E,"Can someone help me? I've been thinking about this question for a while and got stuck. At first I only found the Identity transformation ($I$) and the anti-Identity transformation ($-I$). But then I realized that every reflection is also an involution. The only relevant information I got about the transformation's matrix is that $A^{-1} = A$ and, of course, $(A - I)\cdot(A + I) = 0$.","Can someone help me? I've been thinking about this question for a while and got stuck. At first I only found the Identity transformation ($I$) and the anti-Identity transformation ($-I$). But then I realized that every reflection is also an involution. The only relevant information I got about the transformation's matrix is that $A^{-1} = A$ and, of course, $(A - I)\cdot(A + I) = 0$.",,"['linear-algebra', 'matrices', 'matrix-equations']"
56,Prove the following properties of matrices,Prove the following properties of matrices,,"Let $A$ be a $M \times N$ matrix and $B$ a $N \times P$ matrix. Prove that $(AB)^T = B^TA^T$. Use the result in Problem 1 and the associative property of matrix multiplication to show that $(ABC)^T$ = $C^TB^TA^T$ I already have a drafted anwswer in Problem 1. I let $C = AB$ and use the definition of matrix multiplication that the $(i,j)^{th}$ entry of $C^T$ must be equal to the  $(i,j)^{th}$ entry of $B^TA^T$. Then I was having a problem with Problem 2. Could you help me out?","Let $A$ be a $M \times N$ matrix and $B$ a $N \times P$ matrix. Prove that $(AB)^T = B^TA^T$. Use the result in Problem 1 and the associative property of matrix multiplication to show that $(ABC)^T$ = $C^TB^TA^T$ I already have a drafted anwswer in Problem 1. I let $C = AB$ and use the definition of matrix multiplication that the $(i,j)^{th}$ entry of $C^T$ must be equal to the  $(i,j)^{th}$ entry of $B^TA^T$. Then I was having a problem with Problem 2. Could you help me out?",,['matrices']
57,Why is matrix multiplication defined a certain way? [duplicate],Why is matrix multiplication defined a certain way? [duplicate],,"This question already has answers here : Intuition behind Matrix Multiplication (14 answers) Closed 10 years ago . Why is it that when multiplying a (1x3) by (3x1) matrix, you get a (1x1) matrix, but when multiplying a (3x1) matrix by a (1x3) matrix, you get a (3x3) matrix? Why is matrix multiplication defined this way? Why can't a (1x3) by (3x1) yield a (3x3), or a (3x1) by (1x3) yield a (1x1)? I really would like to get to the root of this problem or 'axiomatization'. Thanks.","This question already has answers here : Intuition behind Matrix Multiplication (14 answers) Closed 10 years ago . Why is it that when multiplying a (1x3) by (3x1) matrix, you get a (1x1) matrix, but when multiplying a (3x1) matrix by a (1x3) matrix, you get a (3x3) matrix? Why is matrix multiplication defined this way? Why can't a (1x3) by (3x1) yield a (3x3), or a (3x1) by (1x3) yield a (1x1)? I really would like to get to the root of this problem or 'axiomatization'. Thanks.",,['matrices']
58,Property of the identity matrix,Property of the identity matrix,,Is the identity the only matrix $A \in \mathbb R^{n \times n}$ with real positive eigenvalues that is equal to its inverse? Thanks.,Is the identity the only matrix $A \in \mathbb R^{n \times n}$ with real positive eigenvalues that is equal to its inverse? Thanks.,,['matrices']
59,"If we know the eigenvalues of a matrix $A$, and the minimal polynom $m_t(a)$, how do we find the Jordan form of $A$?","If we know the eigenvalues of a matrix , and the minimal polynom , how do we find the Jordan form of ?",A m_t(a) A,"We have just learned the Jordan Form of a matrix, and I have to admit that I did not understand the algorithm. Given $A = \begin{pmatrix} 1 & 1 & 1 & -1 \\ 0 & 2 & 1 & -1 \\ 1 & -1 & 2 & -1 \\ 1 & -1 & 0 & 1 \end{pmatrix} $ , find the Jordan Form $J(A)$ of the matrix. So what I did so far: (I) Calculate the polynomial: $P_A(\lambda) = (\lambda - 1)^2(\lambda -2)^2$ . (II) Calculate the minimum polynomial: $m_A(\lambda) = P_A(\lambda) =(\lambda - 1)^2(\lambda -2)^2 $ But I am stuck now, how do we exactly calculate the Jordan Form of $A$ ? And an extra question that has been confusing me. In this case, does $A$ have $4$ eigenvalues or $2$ eigenvalues?","We have just learned the Jordan Form of a matrix, and I have to admit that I did not understand the algorithm. Given , find the Jordan Form of the matrix. So what I did so far: (I) Calculate the polynomial: . (II) Calculate the minimum polynomial: But I am stuck now, how do we exactly calculate the Jordan Form of ? And an extra question that has been confusing me. In this case, does have eigenvalues or eigenvalues?",A = \begin{pmatrix} 1 & 1 & 1 & -1 \\ 0 & 2 & 1 & -1 \\ 1 & -1 & 2 & -1 \\ 1 & -1 & 0 & 1 \end{pmatrix}  J(A) P_A(\lambda) = (\lambda - 1)^2(\lambda -2)^2 m_A(\lambda) = P_A(\lambda) =(\lambda - 1)^2(\lambda -2)^2  A A 4 2,"['linear-algebra', 'matrices', 'jordan-normal-form']"
60,"Column entries of a matrix sum to zero, so what are the properties?","Column entries of a matrix sum to zero, so what are the properties?",,What kind of properties does a matrix whose column entries sum to zero have? $$ \begin{pmatrix} a_{11} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{m1} & \cdots & a_{mn} \end{pmatrix}$$ Where $a_{11}+\cdots+a_{m1}=0$ and so on.,What kind of properties does a matrix whose column entries sum to zero have? $$ \begin{pmatrix} a_{11} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{m1} & \cdots & a_{mn} \end{pmatrix}$$ Where $a_{11}+\cdots+a_{m1}=0$ and so on.,,"['linear-algebra', 'abstract-algebra', 'matrices', 'determinant']"
61,"Gaussian Elimination, Question Check.","Gaussian Elimination, Question Check.",,"I'm going through my practice problems, and just want to know if I am doing this right: $$ 2x_1 - 3x_2 = -2$$ $$ 2x_1 + x_2 = 1$$ $$ 3x_1 + 2x_2 = 1$$ And this is my solution: $$ \begin{align} \begin{bmatrix} 2 & 3 & -2\\ 2 & 1 & 1\\ 3 & 2 & 1 \end{bmatrix} \end{align} $$ $$ \begin{align} \begin{bmatrix} 1 & -3/2 & -1\\ 2 & 1 & 1\\ 3 & 2 & 1 \end{bmatrix} \end{align} $$ $$ \begin{align} \begin{bmatrix} 1 & -3/2 & -1\\ 0 & -1 & -1\\ 3 & 2 & 1 \end{bmatrix} \end{align} $$ $$ \begin{align} \begin{bmatrix} 1 & -3/2 & -1\\ 0 & -1 & -1\\ 0 & -1 & -2 \end{bmatrix} \end{align} $$ $$ \begin{align} \begin{bmatrix} 1 & -3/2 & -1\\ 0 & 1 & 1\\ 0 & -1 & -2 \end{bmatrix} \end{align} $$ $$ \begin{align} \begin{bmatrix} 1 & -3/2 & -1\\ 0 & 1 & 1\\ 0 & 0 & -1 \end{bmatrix} \end{align} $$ Can anyone tell me if I did this right, or if I did a mistake where? Thanks.","I'm going through my practice problems, and just want to know if I am doing this right: $$ 2x_1 - 3x_2 = -2$$ $$ 2x_1 + x_2 = 1$$ $$ 3x_1 + 2x_2 = 1$$ And this is my solution: $$ \begin{align} \begin{bmatrix} 2 & 3 & -2\\ 2 & 1 & 1\\ 3 & 2 & 1 \end{bmatrix} \end{align} $$ $$ \begin{align} \begin{bmatrix} 1 & -3/2 & -1\\ 2 & 1 & 1\\ 3 & 2 & 1 \end{bmatrix} \end{align} $$ $$ \begin{align} \begin{bmatrix} 1 & -3/2 & -1\\ 0 & -1 & -1\\ 3 & 2 & 1 \end{bmatrix} \end{align} $$ $$ \begin{align} \begin{bmatrix} 1 & -3/2 & -1\\ 0 & -1 & -1\\ 0 & -1 & -2 \end{bmatrix} \end{align} $$ $$ \begin{align} \begin{bmatrix} 1 & -3/2 & -1\\ 0 & 1 & 1\\ 0 & -1 & -2 \end{bmatrix} \end{align} $$ $$ \begin{align} \begin{bmatrix} 1 & -3/2 & -1\\ 0 & 1 & 1\\ 0 & 0 & -1 \end{bmatrix} \end{align} $$ Can anyone tell me if I did this right, or if I did a mistake where? Thanks.",,"['linear-algebra', 'matrices', 'gaussian-elimination']"
62,How to construct a matrix $A$,How to construct a matrix,A,Construct a matrix $A$ such that $A^2\ne 0$ but $A^3=0$. I need your help to find $A$. Please help. Thanks in advance.,Construct a matrix $A$ such that $A^2\ne 0$ but $A^3=0$. I need your help to find $A$. Please help. Thanks in advance.,,"['linear-algebra', 'matrices']"
63,Express differential equations as system of first order equations,Express differential equations as system of first order equations,,Express the differential equation $$y'''-6y''-y'+6y=0$$ as a system of first order equations i.e. a matrix equation of the form $$A(\vec x)'=0$$ where $$\vec x\text{ is the vector }\left[ \begin{array}{rrr}  x_1  \\\ x_2\\\ x_3 \end{array} \right].$$,Express the differential equation $$y'''-6y''-y'+6y=0$$ as a system of first order equations i.e. a matrix equation of the form $$A(\vec x)'=0$$ where $$\vec x\text{ is the vector }\left[ \begin{array}{rrr}  x_1  \\\ x_2\\\ x_3 \end{array} \right].$$,,"['linear-algebra', 'matrices', 'ordinary-differential-equations']"
64,Get code words from generator matrix,Get code words from generator matrix,,"I have some issue regarding the generator matrix. Please can some body can explain me ""How to get Codebook from Generator matrix?"" Following is my issue Generator matrix has 3 code words. Then codebook should has 8 code words. So for the codebook I already have 4 code words. (All zero vector and 3 vectors in G) I can get another 3 by adding (1+2),(1+3), (2+3) So now I have all together 7 code words. How to get the Last???","I have some issue regarding the generator matrix. Please can some body can explain me ""How to get Codebook from Generator matrix?"" Following is my issue Generator matrix has 3 code words. Then codebook should has 8 code words. So for the codebook I already have 4 code words. (All zero vector and 3 vectors in G) I can get another 3 by adding (1+2),(1+3), (2+3) So now I have all together 7 code words. How to get the Last???",,"['matrices', 'coding-theory']"
65,What does it mean for two $2 \times 2$ matrices to be orthogonal to each other? [closed],What does it mean for two  matrices to be orthogonal to each other? [closed],2 \times 2,"It's difficult to tell what is being asked here. This question is ambiguous, vague, incomplete, overly broad, or rhetorical and cannot be reasonably answered in its current form. For help clarifying this question so that it can be reopened, visit the help center . Closed 11 years ago . This is just a quick question about definitions.  What exactly does it mean for two matrices to be orthogonal to each other?  Thank you.","It's difficult to tell what is being asked here. This question is ambiguous, vague, incomplete, overly broad, or rhetorical and cannot be reasonably answered in its current form. For help clarifying this question so that it can be reopened, visit the help center . Closed 11 years ago . This is just a quick question about definitions.  What exactly does it mean for two matrices to be orthogonal to each other?  Thank you.",,"['linear-algebra', 'matrices', 'inner-products']"
66,Prove that $Au\cdot v = u\cdot A^Tv$,Prove that,Au\cdot v = u\cdot A^Tv,"Let A be an $n$ x $n$ matrix and let $u$,$v$ $\in$ $\mathbb{R^n}$. Prove that $$Au\cdot v = u\cdot A^Tv$$ I tried using the fact that $A^Tu=A\cdot u$. However, I cannot seem to get to this result. Could anyone please help me out?","Let A be an $n$ x $n$ matrix and let $u$,$v$ $\in$ $\mathbb{R^n}$. Prove that $$Au\cdot v = u\cdot A^Tv$$ I tried using the fact that $A^Tu=A\cdot u$. However, I cannot seem to get to this result. Could anyone please help me out?",,"['linear-algebra', 'matrices']"
67,Graphs with zero spectrum / nilpotent symmetric matrices,Graphs with zero spectrum / nilpotent symmetric matrices,,"Is there a graph theoretic characterization of those graphs with zero spectrum? Alternatively, can one at least characterize all symmetric nilpotent (complex) matrices, so that one could recognize those graphs by their adjacency matrices? Thank you in advance!","Is there a graph theoretic characterization of those graphs with zero spectrum? Alternatively, can one at least characterize all symmetric nilpotent (complex) matrices, so that one could recognize those graphs by their adjacency matrices? Thank you in advance!",,"['matrices', 'graph-theory', 'algebraic-graph-theory', 'spectral-graph-theory']"
68,How do I write this matrix in Jordan-Normal Form,How do I write this matrix in Jordan-Normal Form,,"I have the matrix $A=\begin{pmatrix}2&2&1\\-1&0&1\\4&1&-1\end{pmatrix}$, I want to write it in  Jordan-Normal Form. I have $x_1=3,x_2=x_3=-1$ and calculated eigenvectors $v_1=\begin{pmatrix}1\\0\\1\end{pmatrix},v_2=\begin{pmatrix}1\\-4\\5\end{pmatrix},v_3=\begin{pmatrix}0\\0\\0\end{pmatrix}$. But, the matrix $Z=\begin{pmatrix}1&1&0\\0&-4&0\\1&5&0\end{pmatrix}$ is not invertible since $\text{det}(Z)=0$. Does this mean the matrix cannot be written in JNF or do I need to find different eigenvectors? I have tried to find different eigenvectors, but keep arriving at the same problem, any suggestions? Thanks","I have the matrix $A=\begin{pmatrix}2&2&1\\-1&0&1\\4&1&-1\end{pmatrix}$, I want to write it in  Jordan-Normal Form. I have $x_1=3,x_2=x_3=-1$ and calculated eigenvectors $v_1=\begin{pmatrix}1\\0\\1\end{pmatrix},v_2=\begin{pmatrix}1\\-4\\5\end{pmatrix},v_3=\begin{pmatrix}0\\0\\0\end{pmatrix}$. But, the matrix $Z=\begin{pmatrix}1&1&0\\0&-4&0\\1&5&0\end{pmatrix}$ is not invertible since $\text{det}(Z)=0$. Does this mean the matrix cannot be written in JNF or do I need to find different eigenvectors? I have tried to find different eigenvectors, but keep arriving at the same problem, any suggestions? Thanks",,"['matrices', 'eigenvalues-eigenvectors', 'jordan-normal-form']"
69,"Given the product of a unitary matrix and an orthogonal matrix, can it be easily inverted _without_ knowing these factors?","Given the product of a unitary matrix and an orthogonal matrix, can it be easily inverted _without_ knowing these factors?",,"Given the product $M$ of a unitary matrix $U$ (i.e. $U^\dagger U=1$) and an orthogonal matrix $O$ (i.e. $O^TO=1$), can it be easily inverted without knowing $U$ and $O$? Sure enough, if $M=UO$, then $M^{-1}=O^TU^\dagger$. But assuming you only know that $M$ is composed in such a way, but not how $U$ and $O$ actually look, does there still exist a simple formula for $M^{-1}$?","Given the product $M$ of a unitary matrix $U$ (i.e. $U^\dagger U=1$) and an orthogonal matrix $O$ (i.e. $O^TO=1$), can it be easily inverted without knowing $U$ and $O$? Sure enough, if $M=UO$, then $M^{-1}=O^TU^\dagger$. But assuming you only know that $M$ is composed in such a way, but not how $U$ and $O$ actually look, does there still exist a simple formula for $M^{-1}$?",,"['linear-algebra', 'matrices', 'inverse']"
70,"How to ""flip"" and change the sign of one particular row of this matrix?","How to ""flip"" and change the sign of one particular row of this matrix?",,I would like to transform the following matrix :  $\mathbf A$ =$\ \begin{bmatrix} a&b\\ c&d\\ e&f\\ g&h \end{bmatrix}\ $ into this one : $\mathbf B$ = $\ \begin{bmatrix} g&-h\\ e&-f\\ c&-d\\ a&-b \end{bmatrix}\ $. I can easily transform each column separately by doing these operations : $ \left\{ \begin{array}{rcl} \begin{bmatrix} 0&0&0&1\\ 0&0&1&0\\ 0&1&0&0\\ 1&0&0&0 \end{bmatrix} \ \begin{bmatrix} a\\ c\\ e\\ g \end{bmatrix}  = \begin{bmatrix} g\\ e\\ c\\ a \end{bmatrix}\\ \begin{bmatrix} 0&0&0&-1\\ 0&0&-1&0\\ 0&-1&0&0\\ -1&0&0&0 \end{bmatrix} \ \begin{bmatrix} b\\ d\\ f\\ h \end{bmatrix}  = \begin{bmatrix} -h\\ -f\\ -d\\ -b \end{bmatrix} \end{array} \right. $ But doing this wont provide with the matrix $\mathbf B$. Sure I can concatenate the two matrices after doing two separate operations. But I would like to do this in only one operation... Does anyone know how to do this? Thanks in advance.,I would like to transform the following matrix :  $\mathbf A$ =$\ \begin{bmatrix} a&b\\ c&d\\ e&f\\ g&h \end{bmatrix}\ $ into this one : $\mathbf B$ = $\ \begin{bmatrix} g&-h\\ e&-f\\ c&-d\\ a&-b \end{bmatrix}\ $. I can easily transform each column separately by doing these operations : $ \left\{ \begin{array}{rcl} \begin{bmatrix} 0&0&0&1\\ 0&0&1&0\\ 0&1&0&0\\ 1&0&0&0 \end{bmatrix} \ \begin{bmatrix} a\\ c\\ e\\ g \end{bmatrix}  = \begin{bmatrix} g\\ e\\ c\\ a \end{bmatrix}\\ \begin{bmatrix} 0&0&0&-1\\ 0&0&-1&0\\ 0&-1&0&0\\ -1&0&0&0 \end{bmatrix} \ \begin{bmatrix} b\\ d\\ f\\ h \end{bmatrix}  = \begin{bmatrix} -h\\ -f\\ -d\\ -b \end{bmatrix} \end{array} \right. $ But doing this wont provide with the matrix $\mathbf B$. Sure I can concatenate the two matrices after doing two separate operations. But I would like to do this in only one operation... Does anyone know how to do this? Thanks in advance.,,"['matrices', 'transformation', 'block-matrices']"
71,Explicit conjugacy on 2 linear systems involving flow.,Explicit conjugacy on 2 linear systems involving flow.,,"We need to find an explicit conjugacy between the flows of these 2 systems 1st system $X'$ = $AX$ and second system $Y'$ = $BY$ A = $$\begin{bmatrix} -1 & 1 \\ 0 &2\end{bmatrix}$$ B= $$\begin{bmatrix} 1 & 0 \\ 1 &-2 \end{bmatrix}$$ I have tried by finding the 2 eigen values of both and solving both systems. then applying a Mapping H such that $X(0)$ = $X_{0}$= $(x_{0},y_{0})$ so that $HAX$ = $ BY(h_{1},h_{2})$ we want $Y(0)$ = $Y_{0}$= $(h_{1} x_{0}, h_{2}y_{0})$ my text book has no examples. their should be a capital Theta showing the flow of theta A and Flow of theta B such that their must some mapping $H$ that acts differently on y then x to that they are the same. Any anything welcome especially considering i don't expect you to understand what i am talking about cause i don't. for $X'$ eigenvalues  $a_{1}$=-1 and $a_{2}$=2 and vectors $v_{1}$= <1,0> $v_{2}$= <1,3> for $Y'$ eigenvalues  $b_{1}$=-2 and $b_{2}$=1 and vectors $w_{1}$= <0,1> $w_{2}$= <3,1> we have $C_{a1}$ = $x_{0}$ - $y_{0}/3$ and $C_{a2}$ = $y_{0}/3$ $C_{b1}$ = $y_{0}$ - $x_{0}/3$ and $C_{b2}$ = $x_{0}/3$ $h_{1}$ $C_{a1}$ + $h_{1}$ $C_{a2}$ + $h_{2}$ 3$C_{a2}$ needs to equal $h_{1}$ $C_{b1}$ + $h_{2}$ [3 $C_{b1}$ + $C_{b1}$] and we need to make the 2 equal by guessing  h1 and h2","We need to find an explicit conjugacy between the flows of these 2 systems 1st system $X'$ = $AX$ and second system $Y'$ = $BY$ A = $$\begin{bmatrix} -1 & 1 \\ 0 &2\end{bmatrix}$$ B= $$\begin{bmatrix} 1 & 0 \\ 1 &-2 \end{bmatrix}$$ I have tried by finding the 2 eigen values of both and solving both systems. then applying a Mapping H such that $X(0)$ = $X_{0}$= $(x_{0},y_{0})$ so that $HAX$ = $ BY(h_{1},h_{2})$ we want $Y(0)$ = $Y_{0}$= $(h_{1} x_{0}, h_{2}y_{0})$ my text book has no examples. their should be a capital Theta showing the flow of theta A and Flow of theta B such that their must some mapping $H$ that acts differently on y then x to that they are the same. Any anything welcome especially considering i don't expect you to understand what i am talking about cause i don't. for $X'$ eigenvalues  $a_{1}$=-1 and $a_{2}$=2 and vectors $v_{1}$= <1,0> $v_{2}$= <1,3> for $Y'$ eigenvalues  $b_{1}$=-2 and $b_{2}$=1 and vectors $w_{1}$= <0,1> $w_{2}$= <3,1> we have $C_{a1}$ = $x_{0}$ - $y_{0}/3$ and $C_{a2}$ = $y_{0}/3$ $C_{b1}$ = $y_{0}$ - $x_{0}/3$ and $C_{b2}$ = $x_{0}/3$ $h_{1}$ $C_{a1}$ + $h_{1}$ $C_{a2}$ + $h_{2}$ 3$C_{a2}$ needs to equal $h_{1}$ $C_{b1}$ + $h_{2}$ [3 $C_{b1}$ + $C_{b1}$] and we need to make the 2 equal by guessing  h1 and h2",,"['matrices', 'ordinary-differential-equations']"
72,Need help with a linear equation with a free variable?,Need help with a linear equation with a free variable?,,"Is $a_1, a_2, a_3$ a linear combination of $b$? $a_1 = (1, -2, 0), a_2 = (0, 1, 2), a_3 = (5, -6, 8), b = (2, -1, 6)$ I used Gaussian elimination to get to. $$ \left[ \begin{array}{@{}ccc|c@{}} 1&0&5 & 2 \\ 0&1&4 & 3 \\ 0&0&0 & 0 \\ \end{array} \right] $$ so now $x_3$, is a free variable and I can't tell if it is a linear has a linear combination? Can someone tell me if it does and why?","Is $a_1, a_2, a_3$ a linear combination of $b$? $a_1 = (1, -2, 0), a_2 = (0, 1, 2), a_3 = (5, -6, 8), b = (2, -1, 6)$ I used Gaussian elimination to get to. $$ \left[ \begin{array}{@{}ccc|c@{}} 1&0&5 & 2 \\ 0&1&4 & 3 \\ 0&0&0 & 0 \\ \end{array} \right] $$ so now $x_3$, is a free variable and I can't tell if it is a linear has a linear combination? Can someone tell me if it does and why?",,"['linear-algebra', 'matrices']"
73,Matrix is singular to working precision,Matrix is singular to working precision,,"I have a problem while evaluating inverse using inv in MATLAB. My matrix looks like this: term1 =         29929       29756       29929           0       29756       29756        29756       29584       29756           0       29584       29584        29929       29756       29929           0       29756       29756            0           0           0           0           0           0        29756       29584       29756           0       29584       29584        29756       29584       29756           0       29584       29584 when i try to calculate inverse, MATLAB throws a warning Matrix is singular to working precision and the result is: ans =     Inf   Inf   Inf   Inf   Inf   Inf    Inf   Inf   Inf   Inf   Inf   Inf    Inf   Inf   Inf   Inf   Inf   Inf    Inf   Inf   Inf   Inf   Inf   Inf    Inf   Inf   Inf   Inf   Inf   Inf    Inf   Inf   Inf   Inf   Inf   Inf Can anyone tell me why this is happening and any ways to resolve it and get the correct result?","I have a problem while evaluating inverse using inv in MATLAB. My matrix looks like this: term1 =         29929       29756       29929           0       29756       29756        29756       29584       29756           0       29584       29584        29929       29756       29929           0       29756       29756            0           0           0           0           0           0        29756       29584       29756           0       29584       29584        29756       29584       29756           0       29584       29584 when i try to calculate inverse, MATLAB throws a warning Matrix is singular to working precision and the result is: ans =     Inf   Inf   Inf   Inf   Inf   Inf    Inf   Inf   Inf   Inf   Inf   Inf    Inf   Inf   Inf   Inf   Inf   Inf    Inf   Inf   Inf   Inf   Inf   Inf    Inf   Inf   Inf   Inf   Inf   Inf    Inf   Inf   Inf   Inf   Inf   Inf Can anyone tell me why this is happening and any ways to resolve it and get the correct result?",,"['matrices', 'matlab', 'inverse']"
74,How to show $AB^{-1}A=A$,How to show,AB^{-1}A=A,"Let $$A^{n \times n}=\begin{pmatrix} a & b &b & \dots & b \\ b & a &b & \dots & b \\ b & b & a & \dots & b \\ \vdots & \vdots & \vdots & & \vdots \\ b & b & b & \dots &a\end{pmatrix}$$   where $a \neq b$ and $a + (n - 1)b = 0$.   Suppose $B=A+\frac{11'}{n}$ , where $1=(1,1,\dots,1)'$ is an $n \times 1$ vector. Show that $AB^{-1}A=A$ Trial: Here $B^{-1}=\frac{1}{\alpha-\beta}I_n-\frac{\beta~11'}{(\alpha-\beta)(\alpha+(n-1)\beta)}$ Where $(\alpha-\beta)=(a+\frac{1}{n}-b-\frac{1}{n})=(a-b)$ and $\alpha+(n-1)\beta=1$ So, $B^{-1}=\frac{1}{a-b}[I_n-(b+\frac{1}{n})11']$. Then pre and post multipling $A$ I can't reach to the desire result . Please help.","Let $$A^{n \times n}=\begin{pmatrix} a & b &b & \dots & b \\ b & a &b & \dots & b \\ b & b & a & \dots & b \\ \vdots & \vdots & \vdots & & \vdots \\ b & b & b & \dots &a\end{pmatrix}$$   where $a \neq b$ and $a + (n - 1)b = 0$.   Suppose $B=A+\frac{11'}{n}$ , where $1=(1,1,\dots,1)'$ is an $n \times 1$ vector. Show that $AB^{-1}A=A$ Trial: Here $B^{-1}=\frac{1}{\alpha-\beta}I_n-\frac{\beta~11'}{(\alpha-\beta)(\alpha+(n-1)\beta)}$ Where $(\alpha-\beta)=(a+\frac{1}{n}-b-\frac{1}{n})=(a-b)$ and $\alpha+(n-1)\beta=1$ So, $B^{-1}=\frac{1}{a-b}[I_n-(b+\frac{1}{n})11']$. Then pre and post multipling $A$ I can't reach to the desire result . Please help.",,"['matrices', 'inverse']"
75,Special matrices for which the cost of matrix-vector multiplication is less than $O(n^2)$,Special matrices for which the cost of matrix-vector multiplication is less than,O(n^2),"I am looking for some special type of matrices for which the cost of matrix-vector multiplication is less than $O(n^2)$ . Examples are Hankel and Toeplitz matrices, which have few degrees of freedom (i.e., less than the number of free variables). Is there some other kind of matrix that has more degrees of freedom?","I am looking for some special type of matrices for which the cost of matrix-vector multiplication is less than . Examples are Hankel and Toeplitz matrices, which have few degrees of freedom (i.e., less than the number of free variables). Is there some other kind of matrix that has more degrees of freedom?",O(n^2),"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
76,Linear Algebra munkres analysis on manifolds question.,Linear Algebra munkres analysis on manifolds question.,,"If $A$ is an $n$ by $m$ matrix and $B$ is an $m$ by $p$ matrix, then $$ |AB| \leq m|A||B|$$ where $|A| = \max\{|a_{ij}| : i = 1,\ldots,n \text{ and} j = 1,\ldots,m\}$ Attempt: $ |AB| = \max\{| \sum_{j=1}^{m}a_{ij}b_{jk} |: i = 1,\ldots,n \text{ and } k = 1,\ldots,p\} \leq \max\{ \sum_{j=1}^{m}|a_{ij}b_{jk}| : i = 1,\ldots,n \text{ and } k = 1,\ldots,p\}  \leq m\max\{|a_ij|\}\max\{|b_{jk}|\} = m|A||B| $ Is this correct?","If $A$ is an $n$ by $m$ matrix and $B$ is an $m$ by $p$ matrix, then $$ |AB| \leq m|A||B|$$ where $|A| = \max\{|a_{ij}| : i = 1,\ldots,n \text{ and} j = 1,\ldots,m\}$ Attempt: $ |AB| = \max\{| \sum_{j=1}^{m}a_{ij}b_{jk} |: i = 1,\ldots,n \text{ and } k = 1,\ldots,p\} \leq \max\{ \sum_{j=1}^{m}|a_{ij}b_{jk}| : i = 1,\ldots,n \text{ and } k = 1,\ldots,p\}  \leq m\max\{|a_ij|\}\max\{|b_{jk}|\} = m|A||B| $ Is this correct?",,"['linear-algebra', 'matrices', 'normed-spaces']"
77,Ax = 0 easier than Ax = b?,Ax = 0 easier than Ax = b?,,Is it easier (computationally speaking) to solve the matrix equation of the form $A\vec{x}=\vec{0}$ (with $\vec{x} \neq \vec{0}$) than for the general case $A\vec{x}=\vec{b}$ ?,Is it easier (computationally speaking) to solve the matrix equation of the form $A\vec{x}=\vec{0}$ (with $\vec{x} \neq \vec{0}$) than for the general case $A\vec{x}=\vec{b}$ ?,,"['matrices', 'numerical-linear-algebra']"
78,Vector * Matrix * Vector properties,Vector * Matrix * Vector properties,,"If $A$ is a matrix and $v$ is a vector, what do we know about the magnitude of the vector $vAv$? Can I write something like $|vAv| \leq |v||Av| \leq |v| \max |A_{ij}| |v|$?","If $A$ is a matrix and $v$ is a vector, what do we know about the magnitude of the vector $vAv$? Can I write something like $|vAv| \leq |v||Av| \leq |v| \max |A_{ij}| |v|$?",,"['calculus', 'matrices']"
79,Jordan Form of a matrix,Jordan Form of a matrix,,"I'm trying to find a matrix $P$ such that $J=P^{-1}AP$, where $J$ is the Jordan Form of the matrix: $$A=\begin{pmatrix} -1&2&2\\ -3&4&3\\ 1&-1&0 \end{pmatrix} $$ The characteristic polynomial is: $p(\lambda)=(\lambda-1)^3$, and a eigenvector for $A-I$ is $\begin{pmatrix} 0 \\ 1 \\-1 \end{pmatrix}$. Now, how can I find other $2$ vectors? Thanks for your help.","I'm trying to find a matrix $P$ such that $J=P^{-1}AP$, where $J$ is the Jordan Form of the matrix: $$A=\begin{pmatrix} -1&2&2\\ -3&4&3\\ 1&-1&0 \end{pmatrix} $$ The characteristic polynomial is: $p(\lambda)=(\lambda-1)^3$, and a eigenvector for $A-I$ is $\begin{pmatrix} 0 \\ 1 \\-1 \end{pmatrix}$. Now, how can I find other $2$ vectors? Thanks for your help.",,"['linear-algebra', 'matrices']"
80,"When a directed graph is represented in matrix form, what is the interpretation of the inverse of this matrix?","When a directed graph is represented in matrix form, what is the interpretation of the inverse of this matrix?",,"Let $G$ be a directed (unweighted) graph with $n$ nodes.  We can represent $G$ with an $n \times n$ binary matrix $A$, with $A_{ij} = 1$ if there is an edge $i \to j$ and $A_{ij} = 0$ otherwise. Assuming $A$ is nonsingular, does $A^{-1}$ have any nice interpretation in the world of graphs?","Let $G$ be a directed (unweighted) graph with $n$ nodes.  We can represent $G$ with an $n \times n$ binary matrix $A$, with $A_{ij} = 1$ if there is an edge $i \to j$ and $A_{ij} = 0$ otherwise. Assuming $A$ is nonsingular, does $A^{-1}$ have any nice interpretation in the world of graphs?",,"['matrices', 'graph-theory']"
81,Inverse of a matrix with $a+1$ on the diagonal and $a$ in other places,Inverse of a matrix with  on the diagonal and  in other places,a+1 a,Let $a>0$. Let $A$ be the $n\times n$ matrix with $a+1$ on the diagonal and $a$ in all other entries. How can one compute $A^{-1}$ as a function of $n$?,Let $a>0$. Let $A$ be the $n\times n$ matrix with $a+1$ on the diagonal and $a$ in all other entries. How can one compute $A^{-1}$ as a function of $n$?,,"['linear-algebra', 'matrices', 'inverse']"
82,Symbol for Euclidean norm (Euclidean distance),Symbol for Euclidean norm (Euclidean distance),,Which symbol is more commonly used to denote the Euclidean norm: $ \left \| \textbf a \right \|  $ or $ \left | \textbf b \right |$?,Which symbol is more commonly used to denote the Euclidean norm: $ \left \| \textbf a \right \|  $ or $ \left | \textbf b \right |$?,,"['linear-algebra', 'matrices', 'notation']"
83,"Is there an explicit way to determine $\mathrm{Mat}_n(R[X_1,\dots,X_m])\simeq\mathrm{Mat}_n(R)[X_1,\dots,X_m]$?",Is there an explicit way to determine ?,"\mathrm{Mat}_n(R[X_1,\dots,X_m])\simeq\mathrm{Mat}_n(R)[X_1,\dots,X_m]","For a commutative ring $R$, let $\mathrm{Mat}_n(R[X_1,\dots,X_m])$ denotes the matrix ring with entries from $R[X_1,\dots,X_m]$, and let $\mathrm{Mat}_n(R)[X_1,\dots,X_m]$ denotes the polynomial ring with coefficients in $\mathrm{Mat}_n(R)$. Is there an easy way to see that both structures are isomorphic as rings? Even experimenting with just one indeterminate at small cases of $n$, I'm having difficulty finding a suitable map to verify. What is the natural ring isomorphism here? Thanks.","For a commutative ring $R$, let $\mathrm{Mat}_n(R[X_1,\dots,X_m])$ denotes the matrix ring with entries from $R[X_1,\dots,X_m]$, and let $\mathrm{Mat}_n(R)[X_1,\dots,X_m]$ denotes the polynomial ring with coefficients in $\mathrm{Mat}_n(R)$. Is there an easy way to see that both structures are isomorphic as rings? Even experimenting with just one indeterminate at small cases of $n$, I'm having difficulty finding a suitable map to verify. What is the natural ring isomorphism here? Thanks.",,"['matrices', 'polynomials', 'ring-theory']"
84,calculate generally the determinant of $A = a_{ij} = \begin{cases}a & i \neq j \\ 1 & i=j \end{cases}$,calculate generally the determinant of,A = a_{ij} = \begin{cases}a & i \neq j \\ 1 & i=j \end{cases},calculate generally the determinant of $A = a_{ij} = \begin{cases}a & i \neq j \\ 1 & i=j \end{cases} = \begin{pmatrix} 1 & a & a & · & a \\ · & · & · & · \\ a & a & a & · & 1 \\ \end{pmatrix}$ Any hints?,calculate generally the determinant of $A = a_{ij} = \begin{cases}a & i \neq j \\ 1 & i=j \end{cases} = \begin{pmatrix} 1 & a & a & · & a \\ · & · & · & · \\ a & a & a & · & 1 \\ \end{pmatrix}$ Any hints?,,"['matrices', 'determinant']"
85,Characteristic polynomials of powers and sums of matrices,Characteristic polynomials of powers and sums of matrices,,"If I know the characteristic polynomial of a matrix $A$, what can I know about the charpoly of $A^2$? And if I have the charpolys of $A$ and $B$, what can I know about the charpoly of $A+B$? I'm trying to solve the following problem: The eigenvalues of $A$ are $1,-3,0$. Show that the eigenvalues of $A^2+A-2I$ are $0,2,-4$. Thank you! Edit: I now know that the eigenvalues of $A^2$ are the squares of the eigenvalues of $A$. I still need help solving the problem. Thanks!","If I know the characteristic polynomial of a matrix $A$, what can I know about the charpoly of $A^2$? And if I have the charpolys of $A$ and $B$, what can I know about the charpoly of $A+B$? I'm trying to solve the following problem: The eigenvalues of $A$ are $1,-3,0$. Show that the eigenvalues of $A^2+A-2I$ are $0,2,-4$. Thank you! Edit: I now know that the eigenvalues of $A^2$ are the squares of the eigenvalues of $A$. I still need help solving the problem. Thanks!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
86,Creating Unique Values based off Two Sets of Sequential Integers,Creating Unique Values based off Two Sets of Sequential Integers,,"First off, I apologize if this is the wrong board. I'm a heavy StackOverflow user, and this is technically a programming question (or at least serves programming use), but I find it to be based moreso in math. I have two sets of sequential integers that relate to each other, for the sake of example, defined as such: Set A: [1..50] Set B: [1..150] I need to generate a third set from a combination of these two sets, with unique values. All integers from Set A must be programmatically paired with all integers from Set B to create Set C , which will contain unique integers. Jumping right into it, I thought ""Well, just add the two to together!"" I found quickly that such an idea wasn't even close to feasible. Set A   Set B   Set C -----   -----   ----- 1       1       2 1       2       3 * 1       3       4 * 2       1       3 * 2       2       4 * 2       3       5  *Non-unique Values Multiplication didn't go so well either... Set A   Set B   Set C -----   -----   ----- 1       1       1 1       2       2 * 1       3       3  2       1       2 * 2       2       4  2       3       6  *Non-unique Values So I'm looking for an operator or small formula to put between Set A and Set B to generate a unique, integral Set C . EDIT Both sets will have values added to them over time, so I need a solution that could handle an infinitely large Set A and Set B","First off, I apologize if this is the wrong board. I'm a heavy StackOverflow user, and this is technically a programming question (or at least serves programming use), but I find it to be based moreso in math. I have two sets of sequential integers that relate to each other, for the sake of example, defined as such: Set A: [1..50] Set B: [1..150] I need to generate a third set from a combination of these two sets, with unique values. All integers from Set A must be programmatically paired with all integers from Set B to create Set C , which will contain unique integers. Jumping right into it, I thought ""Well, just add the two to together!"" I found quickly that such an idea wasn't even close to feasible. Set A   Set B   Set C -----   -----   ----- 1       1       2 1       2       3 * 1       3       4 * 2       1       3 * 2       2       4 * 2       3       5  *Non-unique Values Multiplication didn't go so well either... Set A   Set B   Set C -----   -----   ----- 1       1       1 1       2       2 * 1       3       3  2       1       2 * 2       2       4  2       3       6  *Non-unique Values So I'm looking for an operator or small formula to put between Set A and Set B to generate a unique, integral Set C . EDIT Both sets will have values added to them over time, so I need a solution that could handle an infinitely large Set A and Set B",,"['sequences-and-series', 'matrices']"
87,"$R(AB)=R(A)$ iff rank$(AB)$=rank$(A)$, $N(AB)=N(B)$ iff rank$(AB)$=rank$(B)$","iff rank=rank,  iff rank=rank",R(AB)=R(A) (AB) (A) N(AB)=N(B) (AB) (B),"$A$ and $B$ are two square matrices then show that $R(AB) = R(A)$ iff $\mathrm{rank} (AB) = \mathrm{rank} (A)$, and $N(AB) = N(B)$ iff $\mathrm{rank} (AB) = \mathrm{rank} (B)$. Here is my attempt: Clearly $R(A)\supset R(AB)$, now can we say that these two subspaces are identical iff they have the same dimension? also i know $N(AB)\supset N(B)$ but i am not finding a way to prove converse part. Thanks for giving me time.","$A$ and $B$ are two square matrices then show that $R(AB) = R(A)$ iff $\mathrm{rank} (AB) = \mathrm{rank} (A)$, and $N(AB) = N(B)$ iff $\mathrm{rank} (AB) = \mathrm{rank} (B)$. Here is my attempt: Clearly $R(A)\supset R(AB)$, now can we say that these two subspaces are identical iff they have the same dimension? also i know $N(AB)\supset N(B)$ but i am not finding a way to prove converse part. Thanks for giving me time.",,"['linear-algebra', 'matrices']"
88,Matrix exponential and rank,Matrix exponential and rank,,"Let $A$ be a square matrix. Invertibility of $\exp(A)$ follows easily from properties of the matrix exponential. Is $\int_0^t \exp(A u)du$ also invertible? I believe it should be, and that the inverse should be $I - At/2 + A^2t^2/12 + ...$ This comes from expanding the real-valued function $x/(e^x - 1)$ in a power series about $x=0$. How should I approach a proof of this (or could I find it in a book somewhere?) What about the more general case when $\Phi(t)$ is defined by $\frac{dX}{dt} = A(t)X,\ \  X_0 = x_0 $ and $X(t) = \Phi(t)x_0$? How might one show that $\int_0^t \Phi(u)du$ is invertible?","Let $A$ be a square matrix. Invertibility of $\exp(A)$ follows easily from properties of the matrix exponential. Is $\int_0^t \exp(A u)du$ also invertible? I believe it should be, and that the inverse should be $I - At/2 + A^2t^2/12 + ...$ This comes from expanding the real-valued function $x/(e^x - 1)$ in a power series about $x=0$. How should I approach a proof of this (or could I find it in a book somewhere?) What about the more general case when $\Phi(t)$ is defined by $\frac{dX}{dt} = A(t)X,\ \  X_0 = x_0 $ and $X(t) = \Phi(t)x_0$? How might one show that $\int_0^t \Phi(u)du$ is invertible?",,"['calculus', 'linear-algebra', 'matrices']"
89,Inequality involving norm of matrix integral,Inequality involving norm of matrix integral,,"This question seems basic but I could not find an answer. I have seen the inequality $$\left\|\int_a^b x(t) dt \right\| \leq \int_a^b \left\| x(t) \right\| dt $$ where $x(t) \in \mathbb{R}^n$ is a vector function and $\|\cdot\|$ is a vector norm, and $a < b$. I wonder if this also holds for matrices with induced norm, that is $$\left\|\int_a^b X(t) dt \right\| \leq \int_a^b \left\| X(t) \right\| dt $$ where $X(t)$ is a matrix function and $\|\cdot\|$ is an induced matrix norm, and $a < b$.  If it is true, is there any reliable citation source?","This question seems basic but I could not find an answer. I have seen the inequality $$\left\|\int_a^b x(t) dt \right\| \leq \int_a^b \left\| x(t) \right\| dt $$ where $x(t) \in \mathbb{R}^n$ is a vector function and $\|\cdot\|$ is a vector norm, and $a < b$. I wonder if this also holds for matrices with induced norm, that is $$\left\|\int_a^b X(t) dt \right\| \leq \int_a^b \left\| X(t) \right\| dt $$ where $X(t)$ is a matrix function and $\|\cdot\|$ is an induced matrix norm, and $a < b$.  If it is true, is there any reliable citation source?",,"['matrices', 'integration', 'inequality', 'normed-spaces']"
90,limit law and product of matrices,limit law and product of matrices,,Are there two $ n\times n $ matrices $A$ and $B$ such that $ \lim_{m\to\infty} A^m$  and $ \lim_{m\to\infty} B^m  $ both exists but $ \lim_{m\to\infty} (A \cdot B)^m  $ doesn't ?,Are there two $ n\times n $ matrices $A$ and $B$ such that $ \lim_{m\to\infty} A^m$  and $ \lim_{m\to\infty} B^m  $ both exists but $ \lim_{m\to\infty} (A \cdot B)^m  $ doesn't ?,,"['linear-algebra', 'matrices']"
91,"No non-trivial subspace is invariant under $T$, then $P(T)$ is inveritble","No non-trivial subspace is invariant under , then  is inveritble",T P(T),"Linear transformation $T\colon V \to V$ has the property that there is no non-trivial subspace $W$ for which $T(W) \subseteq W$ . Prove that for every polynomial $P$ , $P(T)$ is either invertible or zero.","Linear transformation $T\colon V \to V$ has the property that there is no non-trivial subspace $W$ for which $T(W) \subseteq W$ . Prove that for every polynomial $P$ , $P(T)$ is either invertible or zero.",,"['linear-algebra', 'matrices']"
92,"Matrix multiplication, equivalent to numeric multiplication, or just shares the name?","Matrix multiplication, equivalent to numeric multiplication, or just shares the name?",,"Is matrix multiplication equivalent to numeric multiplication, or do they just share the same name? While there are similarities between how they work, and one can be thought of being derived from the other, I ask because they have different properties such as not being commutative, a × b ≠ b × a, and sometimes multiplication between matrices is referred to by the alternative name ap ply instead of multi ply. For example applying a transformation matrix, where this is the same as multiplying by it. Additionally sometimes in programming operations can be defined between new types of things, allowing the language to expand with new concepts, however the link between the name and rules such as commutative are supposed to continue to hold true.","Is matrix multiplication equivalent to numeric multiplication, or do they just share the same name? While there are similarities between how they work, and one can be thought of being derived from the other, I ask because they have different properties such as not being commutative, a × b ≠ b × a, and sometimes multiplication between matrices is referred to by the alternative name ap ply instead of multi ply. For example applying a transformation matrix, where this is the same as multiplying by it. Additionally sometimes in programming operations can be defined between new types of things, allowing the language to expand with new concepts, however the link between the name and rules such as commutative are supposed to continue to hold true.",,"['linear-algebra', 'abstract-algebra', 'matrices']"
93,Determinants of symmetric tridiagonal matrix and of Toeplitz matrix,Determinants of symmetric tridiagonal matrix and of Toeplitz matrix,,Is there any fast way to compute the determinant of this matrix: $$ \begin{vmatrix} a & b & 0 &0 &0 \\ b & a & b &0 &0 \\ 0 & b & a &b &0 \\ 0 & 0 & b &a &b \\ 0 & 0 & 0 &b &a \end{vmatrix} $$ And can you say anything about  $$ \frac{det(M_{k+1})}{det(M_{k})} $$ where $M_{k}$ is a toeplitz matrix?,Is there any fast way to compute the determinant of this matrix: $$ \begin{vmatrix} a & b & 0 &0 &0 \\ b & a & b &0 &0 \\ 0 & b & a &b &0 \\ 0 & 0 & b &a &b \\ 0 & 0 & 0 &b &a \end{vmatrix} $$ And can you say anything about  $$ \frac{det(M_{k+1})}{det(M_{k})} $$ where $M_{k}$ is a toeplitz matrix?,,"['matrices', 'determinant']"
94,Finding inverse of a $3\times 4$ or $4\times 3$ matrix,Finding inverse of a  or  matrix,3\times 4 4\times 3,"Now I have no problem getting an inverse of a square  matrix where you just calculate the matrix of minors, then apply matrix of co-factors and then transpose that and what you get you multiply by the determinant of the original matrix. Now on the last test we didn't get a square  matrix; we had to find a inverse and determinant of a $3\times 4$ matrix and I was lost. $$\begin{pmatrix} 2 & 3 & 4 & 5\\ 2 & 3 & 5 & 6\\ 1 & 2 & 0 & 8 \end{pmatrix}$$ How would you calculate inverse of such a matrix. The determinant is obviously 0 since you need a square matrix to calculate a determinant.","Now I have no problem getting an inverse of a square  matrix where you just calculate the matrix of minors, then apply matrix of co-factors and then transpose that and what you get you multiply by the determinant of the original matrix. Now on the last test we didn't get a square  matrix; we had to find a inverse and determinant of a $3\times 4$ matrix and I was lost. $$\begin{pmatrix} 2 & 3 & 4 & 5\\ 2 & 3 & 5 & 6\\ 1 & 2 & 0 & 8 \end{pmatrix}$$ How would you calculate inverse of such a matrix. The determinant is obviously 0 since you need a square matrix to calculate a determinant.",,"['matrices', 'determinant', 'inverse']"
95,What is known about totally positive matrices?,What is known about totally positive matrices?,,"A totally positive matrix is one whose minors are all positive. This is a simple elementary concept but most of the development on the subject is far from elementary. I am having a hard time understanding most papers on the subject because of the complicated language. I would like to know, in simple terms, what is known about real totally positive matrices. What are necessary and sufficient conditions for a matrix to be totally positive? What is the simplest known algorithm to verify total positivity? Thanks in advance.","A totally positive matrix is one whose minors are all positive. This is a simple elementary concept but most of the development on the subject is far from elementary. I am having a hard time understanding most papers on the subject because of the complicated language. I would like to know, in simple terms, what is known about real totally positive matrices. What are necessary and sufficient conditions for a matrix to be totally positive? What is the simplest known algorithm to verify total positivity? Thanks in advance.",,['matrices']
96,Exponential bound on norm of matrix exponential (of linear ODE),Exponential bound on norm of matrix exponential (of linear ODE),,"Consider a linear ODE: $\dot{x} = A x$ where $A$ is Hurwitz, i.e. all its eigenvalues have negative real parts. Thus the system is exponentially stable. We know that there exists positive numbers $\beta$ and $\alpha$ such that $\| e^{A t} \| \leq \beta e^{-\alpha t}$ for all $t$. I see this result being used in many analysis. My question is how to (practically) compute these values? In particular, if I pick $\alpha$ so that $ -\alpha > \max_i \Re(\lambda_i)$, where $\lambda_i$ are the eigenvalues of $A$, then how to compute a tight value for $\beta$?","Consider a linear ODE: $\dot{x} = A x$ where $A$ is Hurwitz, i.e. all its eigenvalues have negative real parts. Thus the system is exponentially stable. We know that there exists positive numbers $\beta$ and $\alpha$ such that $\| e^{A t} \| \leq \beta e^{-\alpha t}$ for all $t$. I see this result being used in many analysis. My question is how to (practically) compute these values? In particular, if I pick $\alpha$ so that $ -\alpha > \max_i \Re(\lambda_i)$, where $\lambda_i$ are the eigenvalues of $A$, then how to compute a tight value for $\beta$?",,"['linear-algebra', 'matrices']"
97,Decomposition of a unitary matrix via Householder matrices,Decomposition of a unitary matrix via Householder matrices,,"If $U$ is unitary, how can I show that there exist $w_{1},w_{2},...,w_{k}\in \mathbb{C}^{n}$, $k\leq n$, and $\theta_{1},\theta_{2},...,\theta_{n}\in \mathbb{R}$ such that $U=U_{w_{1}}U_{w_{2}}\cdots U_{w_{k}}  \begin{pmatrix}   e^{i\theta_{1}} & 0 & \cdots & 0 \\   0 & e^{i\theta_{2}} & \cdots & 0 \\   \vdots  & \vdots  & \ddots & \vdots  \\   0 & 0 & \cdots & e^{i\theta_{n}}  \end{pmatrix}  $ where $U_{w_{i}}=I-\frac{2w_{i}w_{i}^{*}}{w_{i}^{*}w_{i}}$ are householder matrices.","If $U$ is unitary, how can I show that there exist $w_{1},w_{2},...,w_{k}\in \mathbb{C}^{n}$, $k\leq n$, and $\theta_{1},\theta_{2},...,\theta_{n}\in \mathbb{R}$ such that $U=U_{w_{1}}U_{w_{2}}\cdots U_{w_{k}}  \begin{pmatrix}   e^{i\theta_{1}} & 0 & \cdots & 0 \\   0 & e^{i\theta_{2}} & \cdots & 0 \\   \vdots  & \vdots  & \ddots & \vdots  \\   0 & 0 & \cdots & e^{i\theta_{n}}  \end{pmatrix}  $ where $U_{w_{i}}=I-\frac{2w_{i}w_{i}^{*}}{w_{i}^{*}w_{i}}$ are householder matrices.",,"['linear-algebra', 'matrices', 'transformation', 'numerical-linear-algebra', 'control-theory']"
98,Case of Matrix Inverse not existing,Case of Matrix Inverse not existing,,"Here's a question: $A$ is a $3\times 2$ matrix, and $B$ is a $2\times 3$ matrix, so $AB$ is $3\times 3$ matrix. The problem given to me was to show that the inverse of $AB$ does not exist. I was able to verify it using actual values, but could not find a way to show it in general. Can anyone help? Thanks.","Here's a question: $A$ is a $3\times 2$ matrix, and $B$ is a $2\times 3$ matrix, so $AB$ is $3\times 3$ matrix. The problem given to me was to show that the inverse of $AB$ does not exist. I was able to verify it using actual values, but could not find a way to show it in general. Can anyone help? Thanks.",,['matrices']
99,Limit of a Markov transition matrix,Limit of a Markov transition matrix,,"Here, $$T_n=\begin{pmatrix} 1&&&&&&\\ \frac{n-1}{n}&0&\frac{1}{n}&&&&\\ &\frac{n-2}{n}&0&\frac{2}{n}&&&\\ &&\ddots&\ddots&\ddots&&\\ &&&\frac{2}{n}&0&\frac{n-2}{n}&\\ &&&&\frac{1}{n}&0&\frac{n-1}{n}\\ &&&&&&1 \end{pmatrix}_{(n+1)\times(n+1)}$$ Now what is $\lim_{k\to\infty}T_n^k$? Note that this chain doesn't have a stationary distribution because of the absorption states which make other states transient (and thus the eigenvalue 1 has multiple corresponding eigenvectors). Experiment shows that the limit has the form of $$T_n^{\infty}=(\mathbf{v,0,\cdots,0,e-v}),$$ where $\mathbf{v}$ is an eigenvector of eigenvalue 1 and can be calculated via Mathematica clause TransitionMatrix[n_] := Module[{band = Table[k/n, {k, 0, n - 1}]},     SparseArray[{Band[{1, 2}] -> band, Band[{1, 1}] -> {1},                  Band[{n + 1, n + 1}] -> {1},                  Band[{2, 1}] -> Reverse[band]}]] NonstationaryDistribution[n_] :=     #/#[[1]]&[Eigenvectors[TransitionMatrix[n]][[2]]] and $\mathbf{e}$ is a vector of all 1's with a proper length. Can anyone give some hints? Thank you~","Here, $$T_n=\begin{pmatrix} 1&&&&&&\\ \frac{n-1}{n}&0&\frac{1}{n}&&&&\\ &\frac{n-2}{n}&0&\frac{2}{n}&&&\\ &&\ddots&\ddots&\ddots&&\\ &&&\frac{2}{n}&0&\frac{n-2}{n}&\\ &&&&\frac{1}{n}&0&\frac{n-1}{n}\\ &&&&&&1 \end{pmatrix}_{(n+1)\times(n+1)}$$ Now what is $\lim_{k\to\infty}T_n^k$? Note that this chain doesn't have a stationary distribution because of the absorption states which make other states transient (and thus the eigenvalue 1 has multiple corresponding eigenvectors). Experiment shows that the limit has the form of $$T_n^{\infty}=(\mathbf{v,0,\cdots,0,e-v}),$$ where $\mathbf{v}$ is an eigenvector of eigenvalue 1 and can be calculated via Mathematica clause TransitionMatrix[n_] := Module[{band = Table[k/n, {k, 0, n - 1}]},     SparseArray[{Band[{1, 2}] -> band, Band[{1, 1}] -> {1},                  Band[{n + 1, n + 1}] -> {1},                  Band[{2, 1}] -> Reverse[band]}]] NonstationaryDistribution[n_] :=     #/#[[1]]&[Eigenvectors[TransitionMatrix[n]][[2]]] and $\mathbf{e}$ is a vector of all 1's with a proper length. Can anyone give some hints? Thank you~",,"['linear-algebra', 'matrices', 'stochastic-processes']"
