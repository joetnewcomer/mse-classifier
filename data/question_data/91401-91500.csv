,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Why the real and imaginary parts of a complex analytic function are not independent?,Why the real and imaginary parts of a complex analytic function are not independent?,,"I have trouble understanding a whole array of things in complex analysis, which I have basically tracked to the statement ""real and imaginary parts of a complex analytic function are not independent."" Because of that, I don't really understand the Cauchy-Riemann equations, the fact that for an analytic function, if its real part is constant, then the whole function is constant, and other fundamental things, such as Cauchy's Integral formula, Maximum modulus principle, etc. (the last two just make zero sense to me.) The thing is, I pretty much understand the proofs, starting from the beginning, when we define differentiability of a complex function. I don't have any problems with the introduction of complex numbers as well, and different identities. But I just don't have any intuition for why things are like that, and it's very frustrating, because I always feel like I don't understand complex numbers at all, and just do some standard exercises in class, relying on proven facts that I just assume to be true as a starting point. But as soon as I go and try to understand the meaning of things we in the class work with, I just immediately stop understanding anything. Can anyone help me understand why the real and imaginary parts of a complex function are not independent?","I have trouble understanding a whole array of things in complex analysis, which I have basically tracked to the statement ""real and imaginary parts of a complex analytic function are not independent."" Because of that, I don't really understand the Cauchy-Riemann equations, the fact that for an analytic function, if its real part is constant, then the whole function is constant, and other fundamental things, such as Cauchy's Integral formula, Maximum modulus principle, etc. (the last two just make zero sense to me.) The thing is, I pretty much understand the proofs, starting from the beginning, when we define differentiability of a complex function. I don't have any problems with the introduction of complex numbers as well, and different identities. But I just don't have any intuition for why things are like that, and it's very frustrating, because I always feel like I don't understand complex numbers at all, and just do some standard exercises in class, relying on proven facts that I just assume to be true as a starting point. But as soon as I go and try to understand the meaning of things we in the class work with, I just immediately stop understanding anything. Can anyone help me understand why the real and imaginary parts of a complex function are not independent?",,['complex-analysis']
1,Is L'Hopitals rule applicable to complex functions?,Is L'Hopitals rule applicable to complex functions?,,"I have a question about something I'm wondering about. I've read somewhere that L'Hopitals rule can also be applied to complex functions, when they are analytic. So if have for instance: $$ \lim_{z \rightarrow 0} \frac{\log(1+z)}{z} \stackrel{?}{=} \lim_{z \rightarrow 0} \frac{1}{(1+z)} = 1 $$ Now i'm wondering if this is correct? Also if we take $|z|<1$, is it then correct? Thanks,","I have a question about something I'm wondering about. I've read somewhere that L'Hopitals rule can also be applied to complex functions, when they are analytic. So if have for instance: $$ \lim_{z \rightarrow 0} \frac{\log(1+z)}{z} \stackrel{?}{=} \lim_{z \rightarrow 0} \frac{1}{(1+z)} = 1 $$ Now i'm wondering if this is correct? Also if we take $|z|<1$, is it then correct? Thanks,",,"['complex-analysis', 'complex-numbers']"
2,Proof that 1-1 analytic functions have nonzero derivative,Proof that 1-1 analytic functions have nonzero derivative,,"I recently saw a lecturer prove the following theorem (assuming the result that every analytic function is locally 1-1 whenever its derivative is nonzero): Let $\Omega \subset \mathbb{C}$ be open, and let $f : \Omega \to \mathbb{C}$ be 1-1 and analytic on $\Omega$.  Then $f'(z_0) \not = 0$ for every $z_0 \in \Omega$. I got the basic idea behind the proof: we assume for contradiction that $f'(z_0) = 0$, and, assuming without loss of generality that $z_0 = f(z_0) =0$, we have (from the power-series expansion) that $f(z) = z^kg(z)$ for some analytic $g$ in some disk at the origin (i.e., $z_0$) and some $k \ge 2$. Since $z^k$ is not 1-1 in any such disk (because there are multiple roots of unity), then $f$ isn't either. However, the proof he gave was rather awkward and technical- it involved defining three different axillary functions, even though the idea was simple, and I've since forgotten how it exactly worked.  In any case, I'm convinced there's a better way. The problem is that I'm having trouble turning the idea into a real proof- I know that it obviously follows if $g$ is 1-1, but I'm also pretty sure that that is too strong an assumption.  Am I missing something, or does the argument just have to be more complicated?","I recently saw a lecturer prove the following theorem (assuming the result that every analytic function is locally 1-1 whenever its derivative is nonzero): Let $\Omega \subset \mathbb{C}$ be open, and let $f : \Omega \to \mathbb{C}$ be 1-1 and analytic on $\Omega$.  Then $f'(z_0) \not = 0$ for every $z_0 \in \Omega$. I got the basic idea behind the proof: we assume for contradiction that $f'(z_0) = 0$, and, assuming without loss of generality that $z_0 = f(z_0) =0$, we have (from the power-series expansion) that $f(z) = z^kg(z)$ for some analytic $g$ in some disk at the origin (i.e., $z_0$) and some $k \ge 2$. Since $z^k$ is not 1-1 in any such disk (because there are multiple roots of unity), then $f$ isn't either. However, the proof he gave was rather awkward and technical- it involved defining three different axillary functions, even though the idea was simple, and I've since forgotten how it exactly worked.  In any case, I'm convinced there's a better way. The problem is that I'm having trouble turning the idea into a real proof- I know that it obviously follows if $g$ is 1-1, but I'm also pretty sure that that is too strong an assumption.  Am I missing something, or does the argument just have to be more complicated?",,['complex-analysis']
3,What does infinity in complex analysis even mean?,What does infinity in complex analysis even mean?,,"Ive always thought that infinity isn't really a number. It is just an idea - a name we attach to something that grows without bound. So in real analysis, when the terms of a sequence or partial sums of a sequence (series) keep increasing without an upper bound, we say the sequence or the series goes to infinity. Negative infinity is the same idea, but with a minus sign, that is negative terms, which keep decreasing without any lower bound go to $-\infty$. And this ""infinity"" object is bigger than any number you could possibly think of (because it isn't a number in itself). All fine. But attached with this idea of bigger than everything else, is the notion of big or small, i.e. order. However for complex numbers, there is no total order. We just can't compare any 2 given complex numbers and say which is ""bigger"". How, then is infinity thought of in complex analysis? It can't be an element that is bigger than all other elements, because ""bigger"" doesn't make any sense. And since there is only one infinity in the complex plane, unlike $+\infty$ and $-\infty$ in $\mathbb{R}$, does that mean that the complex infinity is more of a scalar (with only a modulus and no direction or argument) than a vector (like we can associate a vector with all other complex numbers)? I know I sound very confused. I am. Please shed some light. Thanks.","Ive always thought that infinity isn't really a number. It is just an idea - a name we attach to something that grows without bound. So in real analysis, when the terms of a sequence or partial sums of a sequence (series) keep increasing without an upper bound, we say the sequence or the series goes to infinity. Negative infinity is the same idea, but with a minus sign, that is negative terms, which keep decreasing without any lower bound go to $-\infty$. And this ""infinity"" object is bigger than any number you could possibly think of (because it isn't a number in itself). All fine. But attached with this idea of bigger than everything else, is the notion of big or small, i.e. order. However for complex numbers, there is no total order. We just can't compare any 2 given complex numbers and say which is ""bigger"". How, then is infinity thought of in complex analysis? It can't be an element that is bigger than all other elements, because ""bigger"" doesn't make any sense. And since there is only one infinity in the complex plane, unlike $+\infty$ and $-\infty$ in $\mathbb{R}$, does that mean that the complex infinity is more of a scalar (with only a modulus and no direction or argument) than a vector (like we can associate a vector with all other complex numbers)? I know I sound very confused. I am. Please shed some light. Thanks.",,"['complex-analysis', 'complex-numbers', 'infinity']"
4,Entire function bounded by a polynomial is a polynomial,Entire function bounded by a polynomial is a polynomial,,"Suppose that an entire function $f(z)$ satisfies $\left|f(z)\right|\leq k\left|z\right|^n$ for sufficiently large $\left|z\right|$, where $n\in\mathbb{Z^+}$ and $k>0$ is constant. Show that $f$ is a polynomial of degree at most $n$.","Suppose that an entire function $f(z)$ satisfies $\left|f(z)\right|\leq k\left|z\right|^n$ for sufficiently large $\left|z\right|$, where $n\in\mathbb{Z^+}$ and $k>0$ is constant. Show that $f$ is a polynomial of degree at most $n$.",,['complex-analysis']
5,What is wrong with this fake proof $e^i = 1$?,What is wrong with this fake proof ?,e^i = 1,"$$e^{i} = e^{i2\pi/2\pi} = (e^{2\pi i})^{1/(2\pi)} = 1^{1/(2\pi )} = 1$$ Obviously, one of my algebraic manipulations is not valid.","$$e^{i} = e^{i2\pi/2\pi} = (e^{2\pi i})^{1/(2\pi)} = 1^{1/(2\pi )} = 1$$ Obviously, one of my algebraic manipulations is not valid.",,"['complex-analysis', 'complex-numbers', 'fake-proofs']"
6,Picard's Little Theorem Proofs,Picard's Little Theorem Proofs,,"Picard's little theorem says that If there exist two complex numbers $a,b$ such that $f: \Bbb{C} \to \Bbb{C}\setminus \{a,b\}$ is holomorphic then $f$ is constant. I am interested in proofs for this theorem. Until now I found at least two, one in W. Rudin's Real and Complex Analysis and another one in S. Krantz, Geometric Function Theory . Both of them need some preparation before someone not very advanced in Complex Analysis could understand them, especially the one in S. Krantz's book. My questions are How many proofs are there for Picard's little theorem? (references if possible) Is there a ""simple"" proof for Picard's little theorem? Simple means that it could be presented to an audience which had a one semester course in complex analysis. Thank you.","Picard's little theorem says that If there exist two complex numbers such that is holomorphic then is constant. I am interested in proofs for this theorem. Until now I found at least two, one in W. Rudin's Real and Complex Analysis and another one in S. Krantz, Geometric Function Theory . Both of them need some preparation before someone not very advanced in Complex Analysis could understand them, especially the one in S. Krantz's book. My questions are How many proofs are there for Picard's little theorem? (references if possible) Is there a ""simple"" proof for Picard's little theorem? Simple means that it could be presented to an audience which had a one semester course in complex analysis. Thank you.","a,b f: \Bbb{C} \to \Bbb{C}\setminus \{a,b\} f","['reference-request', 'complex-analysis']"
7,Demystifying modular forms,Demystifying modular forms,,"I am really struggling to understand what modular forms are and how I should think of them. Unfortunately I often see others being in the same shoes as me when it comes to modular forms, I imagine because the amount of background knowledge needed to fully appreciate and grasp the constructions and methods is rather large, so hopefully with this post some clarity can be offered, also for future readers.. The usual definitions one comes across are often of the form: (here taken from wikipedia ) A modular form is a (complex) analytic function on the upper   half-plane satisfying a certain kind of functional equation with   respect to the group action of the modular group, and also satisfying   a growth condition. A modular form of weight $k$ for the modular group $$  \text{SL}(2,\mathbb{Z})=\left\{\begin{pmatrix} a & b \\ c & d  \end{pmatrix}| a,b,c,d \in \mathbb{Z} , ad-bc = 1 \right\} $$ is a   complex-valued function  $f$  on the upper half-plane $\mathbf{H}=\{z  \in \mathbb{C},\text{Im}(z)>0 \}$, satisfying the following three   conditions: $f$ is a holomorphic function on $\mathbf{H}.$ For any $z \in \mathbf{H}$ and any matrix in $\text{SL}(2,\mathbb{Z})$ as above, we have: $$  f\left(\frac{az+b}{cz+d}\right)=(cz+d)^k f(z) $$ $f$ is required to be holomorphic as $z\to i\infty.$ Questions : (a): I guess what I'm having least familiarity with is the modular group part. My interpretation of $\text{SL}(2,\mathbb{Z}):$ The set of all $2$ by $2$ matrices, with integer components, having their determinant equal to $1.$ But where does the name come from, as in why do we call this set a group and what modular entails? (b): If I understand correctly, the group operation here is function composition, of type: $\begin{pmatrix}a & b \\ c & d\end{pmatrix}z = \frac{az+b}{cz+d}$ which is also called a linear fractional transformation. How should one interpret the condition $2.$ that $f$ has to satisfy? My observation is that, as a result of the group operation of $\text{SL}$ on a given integer $z,$ the corresponding image is multiplied by a polynomial of order $k$ (which is the weight of the modular form). (c) The condition $3.$ I interpret as: $f$ should not exhibit any poles in the upper half plane, not even at infinity. About right? (d) A more general question: Given the definition above, it is tempting to see modular forms as particular classes of functions, much like the Schwartz class of functions, or $L^p$ functions and so on. Is this an acceptable assessment of modular forms? (e) Last question: It is often said that modular forms have interesting Fourier transforms, as in their Fourier coefficients are often interesting (or known) sequences. Is there an intuitive way of seeing, from the definition of modular forms, the above expectation of their Fourier transforms?","I am really struggling to understand what modular forms are and how I should think of them. Unfortunately I often see others being in the same shoes as me when it comes to modular forms, I imagine because the amount of background knowledge needed to fully appreciate and grasp the constructions and methods is rather large, so hopefully with this post some clarity can be offered, also for future readers.. The usual definitions one comes across are often of the form: (here taken from wikipedia ) A modular form is a (complex) analytic function on the upper   half-plane satisfying a certain kind of functional equation with   respect to the group action of the modular group, and also satisfying   a growth condition. A modular form of weight $k$ for the modular group $$  \text{SL}(2,\mathbb{Z})=\left\{\begin{pmatrix} a & b \\ c & d  \end{pmatrix}| a,b,c,d \in \mathbb{Z} , ad-bc = 1 \right\} $$ is a   complex-valued function  $f$  on the upper half-plane $\mathbf{H}=\{z  \in \mathbb{C},\text{Im}(z)>0 \}$, satisfying the following three   conditions: $f$ is a holomorphic function on $\mathbf{H}.$ For any $z \in \mathbf{H}$ and any matrix in $\text{SL}(2,\mathbb{Z})$ as above, we have: $$  f\left(\frac{az+b}{cz+d}\right)=(cz+d)^k f(z) $$ $f$ is required to be holomorphic as $z\to i\infty.$ Questions : (a): I guess what I'm having least familiarity with is the modular group part. My interpretation of $\text{SL}(2,\mathbb{Z}):$ The set of all $2$ by $2$ matrices, with integer components, having their determinant equal to $1.$ But where does the name come from, as in why do we call this set a group and what modular entails? (b): If I understand correctly, the group operation here is function composition, of type: $\begin{pmatrix}a & b \\ c & d\end{pmatrix}z = \frac{az+b}{cz+d}$ which is also called a linear fractional transformation. How should one interpret the condition $2.$ that $f$ has to satisfy? My observation is that, as a result of the group operation of $\text{SL}$ on a given integer $z,$ the corresponding image is multiplied by a polynomial of order $k$ (which is the weight of the modular form). (c) The condition $3.$ I interpret as: $f$ should not exhibit any poles in the upper half plane, not even at infinity. About right? (d) A more general question: Given the definition above, it is tempting to see modular forms as particular classes of functions, much like the Schwartz class of functions, or $L^p$ functions and so on. Is this an acceptable assessment of modular forms? (e) Last question: It is often said that modular forms have interesting Fourier transforms, as in their Fourier coefficients are often interesting (or known) sequences. Is there an intuitive way of seeing, from the definition of modular forms, the above expectation of their Fourier transforms?",,"['complex-analysis', 'number-theory', 'modular-forms']"
8,Why do we negate the imaginary part when conjugating?,Why do we negate the imaginary part when conjugating?,,"For $z=x+iy \in \mathbb C$ we all know the definition for the ""conjugate"" of $z$, $\bar{z}=x-iy$. Geometrically this is the reflection of $z$ across the $y$ axis. My question is: couldn't we have defined $\underline{z}=-x+iy$ instead? (this is the reflection across the $x$ axis) Is there anything wrong with it? I agree that the formula $|z|^2=z \bar{z}$ looks better than $|z|^2=-z \underline{z}$, but are there any more serious problems?","For $z=x+iy \in \mathbb C$ we all know the definition for the ""conjugate"" of $z$, $\bar{z}=x-iy$. Geometrically this is the reflection of $z$ across the $y$ axis. My question is: couldn't we have defined $\underline{z}=-x+iy$ instead? (this is the reflection across the $x$ axis) Is there anything wrong with it? I agree that the formula $|z|^2=z \bar{z}$ looks better than $|z|^2=-z \underline{z}$, but are there any more serious problems?",,"['complex-analysis', 'analysis', 'soft-question', 'complex-numbers']"
9,Characterizing non-constant entire functions with modulus $1$ on the unit circle,Characterizing non-constant entire functions with modulus  on the unit circle,1,"Is there a characterization of the nonconstant entire functions $f$ that satisfy $|f(z)|=1$ for all $|z|=1$ ? Clearly, $f(z)=z^n$ works for all $n$ . Also, it's not difficult to show that if $f$ is such an entire function, then $f$ must vanish somewhere inside the unit disk. What else can be said about those functions? Thank you.","Is there a characterization of the nonconstant entire functions that satisfy for all ? Clearly, works for all . Also, it's not difficult to show that if is such an entire function, then must vanish somewhere inside the unit disk. What else can be said about those functions? Thank you.",f |f(z)|=1 |z|=1 f(z)=z^n n f f,['complex-analysis']
10,Complex Analysis Book [duplicate],Complex Analysis Book [duplicate],,"This question already has answers here : What is a good complex analysis textbook, barring Ahlfors's? (28 answers) Closed 8 years ago . I want a really good book on Complex Analysis, for a good understanding of theory. There are many complex variable books that are only a list of identities and integrals and I hate it. For example, I found Munkres to be a very good book for learning topology, and ""Curso de Análise vol I"" by Elon Lages Lima is the best Real Analysis book (and the best math book) that I have read with many examples, good theory and challenging exercises. An intuitive and introductory approach is not very important if the book has good explanations and has correct proofs. Added: If it is possible, tell me your experience with your recommended books and if you got a really good understanding of complex analysis with a deep reading.","This question already has answers here : What is a good complex analysis textbook, barring Ahlfors's? (28 answers) Closed 8 years ago . I want a really good book on Complex Analysis, for a good understanding of theory. There are many complex variable books that are only a list of identities and integrals and I hate it. For example, I found Munkres to be a very good book for learning topology, and ""Curso de Análise vol I"" by Elon Lages Lima is the best Real Analysis book (and the best math book) that I have read with many examples, good theory and challenging exercises. An intuitive and introductory approach is not very important if the book has good explanations and has correct proofs. Added: If it is possible, tell me your experience with your recommended books and if you got a really good understanding of complex analysis with a deep reading.",,"['complex-analysis', 'reference-request', 'book-recommendation']"
11,How is $\mathbb{C}$ different than $\mathbb{R}^2$?,How is  different than ?,\mathbb{C} \mathbb{R}^2,"I'm taking a course in Complex Analysis, and the teacher mentioned that if we do not restrict our attention to analytic functions, we would just be looking at functions from $\mathbb{R}^2$ to $\mathbb{R}^2$. What I don't understand is why this is not true when we do restrict our attention to analytic functions. I understand that complex analytic functions have different properties than real functions on $\mathbb{R}^2$, but I don't understand why this is so.  If I look at a complex number $z$ as a vector in $\mathbb{R}^2$, then isn't differentiability of $w=f(z)$ in $\mathbb{C}$ defined the same way as differentiability of $(u,v)=F(x,y)$ in $\mathbb{R}^2$?","I'm taking a course in Complex Analysis, and the teacher mentioned that if we do not restrict our attention to analytic functions, we would just be looking at functions from $\mathbb{R}^2$ to $\mathbb{R}^2$. What I don't understand is why this is not true when we do restrict our attention to analytic functions. I understand that complex analytic functions have different properties than real functions on $\mathbb{R}^2$, but I don't understand why this is so.  If I look at a complex number $z$ as a vector in $\mathbb{R}^2$, then isn't differentiability of $w=f(z)$ in $\mathbb{C}$ defined the same way as differentiability of $(u,v)=F(x,y)$ in $\mathbb{R}^2$?",,['complex-analysis']
12,Is there any connection between Green's Theorem and the Cauchy-Riemann equations?,Is there any connection between Green's Theorem and the Cauchy-Riemann equations?,,"Green's Theorem has the form:  $$\oint P(x,y)dx = - \iint \frac{\partial P}{\partial x}dxdy , \oint Q(x,y)dy = \iint \frac{\partial Q}{\partial y}dxdy $$ The Cauchy-Riemann equations have the following form:(Assuming $z = P(x,y) + iQ(x,y)$) $$\frac{\partial P}{\partial x} = \frac{\partial Q}{\partial y}, \frac{\partial P}{\partial y} = - \frac{\partial Q}{\partial x}$$ Is there any connection between this two equations?","Green's Theorem has the form:  $$\oint P(x,y)dx = - \iint \frac{\partial P}{\partial x}dxdy , \oint Q(x,y)dy = \iint \frac{\partial Q}{\partial y}dxdy $$ The Cauchy-Riemann equations have the following form:(Assuming $z = P(x,y) + iQ(x,y)$) $$\frac{\partial P}{\partial x} = \frac{\partial Q}{\partial y}, \frac{\partial P}{\partial y} = - \frac{\partial Q}{\partial x}$$ Is there any connection between this two equations?",,"['complex-analysis', 'multivariable-calculus', 'greens-theorem', 'cauchy-riemann-equations']"
13,Easy explanation of analytic continuation,Easy explanation of analytic continuation,,"Today, as I was flipping through my copy of Higher Algebra by Barnard and Child, I came across a theorem which said, The series $$ 1+\frac{1}{2^p} +\frac{1}{3^p}+...$$ diverges for $p\leq 1$ and converges for $p>1$. But later I found out that the zeta function is defined for all complex values other than 1. Now I know that Riemann analytically continued this function to fit all complex values, but how do I explain, to a layman, that $\zeta(0)=1+1+1+...=-\frac{1}{2}$? The Wiki articles on these topics go way over my head. I'd appreciate it if someone can explain it to me what analytic continuation actually is, and which functions can be analytically continued? Edit If the function diverges for $p\leq1$, how is WolframAlpha able to compute $\zeta(1/5)$? Shouldn't it give out infinity as the answer?","Today, as I was flipping through my copy of Higher Algebra by Barnard and Child, I came across a theorem which said, The series $$ 1+\frac{1}{2^p} +\frac{1}{3^p}+...$$ diverges for $p\leq 1$ and converges for $p>1$. But later I found out that the zeta function is defined for all complex values other than 1. Now I know that Riemann analytically continued this function to fit all complex values, but how do I explain, to a layman, that $\zeta(0)=1+1+1+...=-\frac{1}{2}$? The Wiki articles on these topics go way over my head. I'd appreciate it if someone can explain it to me what analytic continuation actually is, and which functions can be analytically continued? Edit If the function diverges for $p\leq1$, how is WolframAlpha able to compute $\zeta(1/5)$? Shouldn't it give out infinity as the answer?",,"['complex-analysis', 'number-theory', 'special-functions', 'riemann-zeta', 'analytic-continuation']"
14,Why does the graph of $e^{1/z}$ look like a dipole?,Why does the graph of  look like a dipole?,e^{1/z},"I was looking at the color wheel graph of $e^{1/z}$, and my girlfriend commented that it looked just like a dipole. Does anyone have an explanation for that, why the geometry would be so similar? I guess as we follow e.g. the red color from the left side as it goes up, and then back down to the right side, we are tracing a ray outward from the origin in $\mathbb{C}$.","I was looking at the color wheel graph of $e^{1/z}$, and my girlfriend commented that it looked just like a dipole. Does anyone have an explanation for that, why the geometry would be so similar? I guess as we follow e.g. the red color from the left side as it goes up, and then back down to the right side, we are tracing a ray outward from the origin in $\mathbb{C}$.",,"['complex-analysis', 'graphing-functions']"
15,Why does this distribution of polynomial roots resemble a collection of affine IFS fractals?,Why does this distribution of polynomial roots resemble a collection of affine IFS fractals?,,"Consider the following spectacular image, created by Sam Derbyshire and described in John Baez's article "" The Beauty of Roots "": In this image are plotted all the complex roots of all polynomials of degree $\le 24$ with coefficients drawn from the set $\{-1, 1\}$. There is some amazing structure in there, about which many questions can be asked! The symmetries about the $x$-axis, the $y$-axis, and the unit circle are easily explained, though: it is not hard to show that if $z$ is a root of a polynomial with coefficients in $\{-1,1\}$, then so are $\bar z$, $-z$, and $z^{-1}$. Please do look at John Baez's aforementioned article for more zoomed images showing the beautiful detail in this distribution of roots. In particular, towards the interior of the unit circle, the distribution does not fall off smoothly, but seems to form fractal patterns, such as near $z = 4/5$ , near $z = 4i/5$ , and near $z = e^{i/5}/2$ . In fact, these patterns are extremely reminiscent of the images produced by iterated function systems of affine transformations. With a little experimentation using David Eck's Chaos Game applet , I found that to produce fractals similar to these patterns, one only needs to use two affine maps, both of which are the composition of a scaling by about 70% and a rotation by $\arg z$, and differ by a translation. To illustrate: I'm tempted to say that the whole distribution looks like a catalogue of such affine IFSs, the way the Mandelbrot set is like a catalogue of Julia sets . My question is simple: Why is this so? What is the reason that the roots of $\{-1,1\}$ polynomials form such a fractal distribution, that behaves locally like an affine IFS?","Consider the following spectacular image, created by Sam Derbyshire and described in John Baez's article "" The Beauty of Roots "": In this image are plotted all the complex roots of all polynomials of degree $\le 24$ with coefficients drawn from the set $\{-1, 1\}$. There is some amazing structure in there, about which many questions can be asked! The symmetries about the $x$-axis, the $y$-axis, and the unit circle are easily explained, though: it is not hard to show that if $z$ is a root of a polynomial with coefficients in $\{-1,1\}$, then so are $\bar z$, $-z$, and $z^{-1}$. Please do look at John Baez's aforementioned article for more zoomed images showing the beautiful detail in this distribution of roots. In particular, towards the interior of the unit circle, the distribution does not fall off smoothly, but seems to form fractal patterns, such as near $z = 4/5$ , near $z = 4i/5$ , and near $z = e^{i/5}/2$ . In fact, these patterns are extremely reminiscent of the images produced by iterated function systems of affine transformations. With a little experimentation using David Eck's Chaos Game applet , I found that to produce fractals similar to these patterns, one only needs to use two affine maps, both of which are the composition of a scaling by about 70% and a rotation by $\arg z$, and differ by a translation. To illustrate: I'm tempted to say that the whole distribution looks like a catalogue of such affine IFSs, the way the Mandelbrot set is like a catalogue of Julia sets . My question is simple: Why is this so? What is the reason that the roots of $\{-1,1\}$ polynomials form such a fractal distribution, that behaves locally like an affine IFS?",,"['complex-analysis', 'polynomials']"
16,Showing that the roots of a polynomial with descending positive coefficients lie in the unit disc.,Showing that the roots of a polynomial with descending positive coefficients lie in the unit disc.,,"Let $P(z)=a_0+a_1z+\cdots+a_nz^n$ be a polynomial whose coefficients satisfy $$0<a_0<a_1<\cdots<a_n.$$ I want to show that the roots of $P$ live in unit disc. The obvious idea is to use Rouche's theorem, but that doesn't quite work here, at least with the choice $f(z)=a_nz^n, g(z)=$ (the rest). Any ideas?","Let $P(z)=a_0+a_1z+\cdots+a_nz^n$ be a polynomial whose coefficients satisfy $$0<a_0<a_1<\cdots<a_n.$$ I want to show that the roots of $P$ live in unit disc. The obvious idea is to use Rouche's theorem, but that doesn't quite work here, at least with the choice $f(z)=a_nz^n, g(z)=$ (the rest). Any ideas?",,"['complex-analysis', 'polynomials', 'roots']"
17,Perspectives on Riemann Surfaces,Perspectives on Riemann Surfaces,,"So, I have come to a somewhat impasse concerning my class selection for next term, and I have exhausted all the 'biased' sources. So, I was wondering if anyone in this fantastic mathematical community has any input on the matter. Next term I would like to take a course on Riemann surfaces. Initially I had intended to do an independent study with a fantastic teacher and use Otto Forsters book Lectures on Riemann Surfaces . But, recently I have come to learn that there is going to be a graduate course on Riemann surfaces taught as well. This class is taught by an expert in moduli spaces and will use Riemann Surfaces by Way of Complex Analytic Geometry by Dror Varolin (freely available on the author's website: here ). These books differ greatly in style--in the way that they approach the subject. So, of course it's important to decide which course I am going to take, and so it made sense to ask around about these different styles in the books. Here is what I have gathered: Forster's book is much more classical. It does things fairly sheaf-theoretically and involves quite a bit of algebra. Moreover, it seems to focus much more on topological algebraic considerations than anything else. Varolin's book is much, much more analytic and PDEish. Most of the proofs seem to be calculations of sorts. So, the issue is this. I am, at least historically, of a very algebraic persuasion. I eventually think I want to do something in algebraic geometry or algebraic number theory. This automatically makes me want to go more for Forster since I have a fair amount of experience with sheaves and cohomology. Moreover, I also have (only half-seriously) a dislike of very computational analysis [even though I know it's useful]. That said, I have been told by multiple people that Forster's approach to the subject is ""dead""--no one does things like that any more. They tell me that Varolin's approach is much more focused on modern techniques, that it's closer to ""the source"". So, the two things I was hoping someone could clear up for me is 1) Is it true that the analysis/PDE approach is much closer to what is actually important to learn about Riemann surfaces? Is that where the powerful theorems and techniques lie? 2) I am chiefly interested in Riemann surfaces so that I have a good geometric background for algebraic geometry. I do not want to be one of those people that can understand all of the algebra yet is clueless as to what is geometrically going on. Is Forster or Varolin's approach better suited to this goal? Of course, any input about anything even slightly related to this that I did not ask, but you think would be helpful to know will be greatly appreciated. Thanks again everyone! NB: If you're answer is going to be ""you should do both"" (which I would imagine is both likely and correct) please, instead, indicate which you think would be preferable to do first. I know I will do both in tandem regardless, but I shall end up (inevitably) focusing on one approach over the other. EDIT: I would like to make clear that one of the main reasons for this question is to basically figure out if people doing work/studying intensely in algebraic complex geometry or algebraic number theory (the more algebraic geometry side--like arithmetic geometry) feel that there is a reason for me to do the analysis part. Will it provide a prospective on things that will be elucidating, or helpful. EDIT(2): I have decided to also post this on mathoverflow. While I know there is a considerable intersection between the participants here and there, I feel as though this question may be better suited for that website.","So, I have come to a somewhat impasse concerning my class selection for next term, and I have exhausted all the 'biased' sources. So, I was wondering if anyone in this fantastic mathematical community has any input on the matter. Next term I would like to take a course on Riemann surfaces. Initially I had intended to do an independent study with a fantastic teacher and use Otto Forsters book Lectures on Riemann Surfaces . But, recently I have come to learn that there is going to be a graduate course on Riemann surfaces taught as well. This class is taught by an expert in moduli spaces and will use Riemann Surfaces by Way of Complex Analytic Geometry by Dror Varolin (freely available on the author's website: here ). These books differ greatly in style--in the way that they approach the subject. So, of course it's important to decide which course I am going to take, and so it made sense to ask around about these different styles in the books. Here is what I have gathered: Forster's book is much more classical. It does things fairly sheaf-theoretically and involves quite a bit of algebra. Moreover, it seems to focus much more on topological algebraic considerations than anything else. Varolin's book is much, much more analytic and PDEish. Most of the proofs seem to be calculations of sorts. So, the issue is this. I am, at least historically, of a very algebraic persuasion. I eventually think I want to do something in algebraic geometry or algebraic number theory. This automatically makes me want to go more for Forster since I have a fair amount of experience with sheaves and cohomology. Moreover, I also have (only half-seriously) a dislike of very computational analysis [even though I know it's useful]. That said, I have been told by multiple people that Forster's approach to the subject is ""dead""--no one does things like that any more. They tell me that Varolin's approach is much more focused on modern techniques, that it's closer to ""the source"". So, the two things I was hoping someone could clear up for me is 1) Is it true that the analysis/PDE approach is much closer to what is actually important to learn about Riemann surfaces? Is that where the powerful theorems and techniques lie? 2) I am chiefly interested in Riemann surfaces so that I have a good geometric background for algebraic geometry. I do not want to be one of those people that can understand all of the algebra yet is clueless as to what is geometrically going on. Is Forster or Varolin's approach better suited to this goal? Of course, any input about anything even slightly related to this that I did not ask, but you think would be helpful to know will be greatly appreciated. Thanks again everyone! NB: If you're answer is going to be ""you should do both"" (which I would imagine is both likely and correct) please, instead, indicate which you think would be preferable to do first. I know I will do both in tandem regardless, but I shall end up (inevitably) focusing on one approach over the other. EDIT: I would like to make clear that one of the main reasons for this question is to basically figure out if people doing work/studying intensely in algebraic complex geometry or algebraic number theory (the more algebraic geometry side--like arithmetic geometry) feel that there is a reason for me to do the analysis part. Will it provide a prospective on things that will be elucidating, or helpful. EDIT(2): I have decided to also post this on mathoverflow. While I know there is a considerable intersection between the participants here and there, I feel as though this question may be better suited for that website.",,"['complex-analysis', 'intuition', 'riemann-surfaces', 'learning', 'advice']"
18,How do I rigorously show $f(z)$ is analytic if and only if $\overline{f(\bar{z})}$ is?,How do I rigorously show  is analytic if and only if  is?,f(z) \overline{f(\bar{z})},"I'm doing a bit of self study, but I'm uncomfortable with a certain idea. I want to show that $f(z)$ is analytic if and only if $\overline{f(\bar{z})}$ is analytic, and by analytic I mean differentiable at each point. Here $f$ is a complex valued function. What I do is write $f(z)=u(x,y)+iv(x,y)$, where $u$ and $v$ are real functions of two variables. Then $\overline{f(\bar{z})}=u(x,-y)-iv(x,-y)$. These two forms look very similar, in the sense the one function being differentiable should immediately imply the other is differentiable, since the only thing really changing might be a $-$ sign popping out due to the chain rule. How can I more rigorously express that $f(z)$ is analytic iff $\overline{f(\bar{z})}$ using this? Many thanks.","I'm doing a bit of self study, but I'm uncomfortable with a certain idea. I want to show that $f(z)$ is analytic if and only if $\overline{f(\bar{z})}$ is analytic, and by analytic I mean differentiable at each point. Here $f$ is a complex valued function. What I do is write $f(z)=u(x,y)+iv(x,y)$, where $u$ and $v$ are real functions of two variables. Then $\overline{f(\bar{z})}=u(x,-y)-iv(x,-y)$. These two forms look very similar, in the sense the one function being differentiable should immediately imply the other is differentiable, since the only thing really changing might be a $-$ sign popping out due to the chain rule. How can I more rigorously express that $f(z)$ is analytic iff $\overline{f(\bar{z})}$ using this? Many thanks.",,['complex-analysis']
19,Proving a known zero of the Riemann Zeta has real part exactly 1/2,Proving a known zero of the Riemann Zeta has real part exactly 1/2,,"Much effort has been expended on a famous unsolved problem about the Riemann Zeta function $\zeta(s)$ .  Not surprisingly, it's called the Riemann hypothesis, which asserts: $$ \zeta(s) = 0 \Rightarrow \operatorname{Re} s = \frac1 2 \text{ or } \operatorname{Im} s = 0 .$$ Now there are numerical methods for approximating $\zeta(s)$, but as I understand, no one knows any exact values except at even integers, which include the trivial zeroes for which $\operatorname{Im} s = 0 $.  So I've always wondered: For the rest, how does one prove $\operatorname{Re} s = 1/2$ exactly ? (All I know that seems vaguely useful is the argument principle, which I'm not sure helps here, but I'd be happy to learn about other techniques that aren't too far advanced.) Edit :  Looking over this again, I found out I missed the closed-form values at odd negative integers .  This doesn't affect the question but seemed worth correcting.  Thanks to the people who contributed.","Much effort has been expended on a famous unsolved problem about the Riemann Zeta function $\zeta(s)$ .  Not surprisingly, it's called the Riemann hypothesis, which asserts: $$ \zeta(s) = 0 \Rightarrow \operatorname{Re} s = \frac1 2 \text{ or } \operatorname{Im} s = 0 .$$ Now there are numerical methods for approximating $\zeta(s)$, but as I understand, no one knows any exact values except at even integers, which include the trivial zeroes for which $\operatorname{Im} s = 0 $.  So I've always wondered: For the rest, how does one prove $\operatorname{Re} s = 1/2$ exactly ? (All I know that seems vaguely useful is the argument principle, which I'm not sure helps here, but I'd be happy to learn about other techniques that aren't too far advanced.) Edit :  Looking over this again, I found out I missed the closed-form values at odd negative integers .  This doesn't affect the question but seemed worth correcting.  Thanks to the people who contributed.",,"['complex-analysis', 'special-functions', 'riemann-zeta', 'conjectures', 'riemann-hypothesis']"
20,"Finding poles, indicating their order and then computing their residues","Finding poles, indicating their order and then computing their residues",,"I really don't understand the concept behind finding poles in Complex Analysis and I can't find anything on the internet or in books that helps me grasp the concept... The following are past exam questions that I'm looking at but don't know where to go with with them in order to find their poles, indicate their order to then compute their residues.. Any help would be greatly appreciated... $(i) f(z)=\dfrac{\sin z }{(z-1)\sinh z}$ $(ii)g(z)= \dfrac{\sin z}{z(z^2+4)}$","I really don't understand the concept behind finding poles in Complex Analysis and I can't find anything on the internet or in books that helps me grasp the concept... The following are past exam questions that I'm looking at but don't know where to go with with them in order to find their poles, indicate their order to then compute their residues.. Any help would be greatly appreciated... $(i) f(z)=\dfrac{\sin z }{(z-1)\sinh z}$ $(ii)g(z)= \dfrac{\sin z}{z(z^2+4)}$",,"['complex-analysis', 'singularity']"
21,"Show that $\int_0^ \infty \frac{1}{1+x^n} dx= \frac{ \pi /n}{\sin(\pi /n)}$ , where $n$ is a positive integer.","Show that  , where  is a positive integer.",\int_0^ \infty \frac{1}{1+x^n} dx= \frac{ \pi /n}{\sin(\pi /n)} n,"Using residues, try the contour below with $R \rightarrow  \infty$ and  $$\lim_{R \rightarrow  \infty }  \int_0^R  \frac{1}{1+r^n}  dr \rightarrow \int_0^\infty  \frac{1}{1+x^n}  dx$$ I've attempted the residue summation, but my sum did not converge.","Using residues, try the contour below with $R \rightarrow  \infty$ and  $$\lim_{R \rightarrow  \infty }  \int_0^R  \frac{1}{1+r^n}  dr \rightarrow \int_0^\infty  \frac{1}{1+x^n}  dx$$ I've attempted the residue summation, but my sum did not converge.",,"['complex-analysis', 'residue-calculus']"
22,"Singularities, essential singularities, poles, simple poles","Singularities, essential singularities, poles, simple poles",,"Could someone possible explain the differences between each of these; Singularities, essential singularities, poles, simple poles. I understand the concept and how to use them in order to work out the residue at each point, however, done fully understand what the difference is for each of these As far as i understand a simple pole is a singularity of order $1$? then we have poles of order $n$ which aren't simple? not too sure about essential singularity","Could someone possible explain the differences between each of these; Singularities, essential singularities, poles, simple poles. I understand the concept and how to use them in order to work out the residue at each point, however, done fully understand what the difference is for each of these As far as i understand a simple pole is a singularity of order $1$? then we have poles of order $n$ which aren't simple? not too sure about essential singularity",,"['complex-analysis', 'singularity']"
23,Which sets are removable for holomorphic functions?,Which sets are removable for holomorphic functions?,,"Let $\Omega$ be a domain in $\mathbb C$, and let $\mathscr X$ be some class of functions from $\Omega$ to $\mathbb C$. A set $E\subset \Omega$ is called removable for holomorphic functions of class $\mathscr X$ if the following holds: every function $f\in\mathscr X$ that is holomorphic on $\Omega\setminus E$ is actually holomorphic on $\Omega$, possibly, after being redefined on $E$. (An example of the above: $E$ is a line interval, $\mathscr X$ consists of continuous functions. In this case $E$ is removable, which is shown in the answer.) It is clear that the larger $\mathscr X$ is, the smaller is the class of removable sets. In the extreme case, if $\mathscr X$ contains all functions $\Omega\to\mathbb C$, there are no nonempty removable sets. Indeed, if $a\in E$, then $f(z)=\frac{1}{z-a}$ (arbitrarily defined at $z=a$) is holomorphic on $\Omega\setminus E$ but has no holomorphic extension to $\Omega$. The problem of describing removable sets is nontrivial in many classes $\mathscr X$ such as $L^{\infty}(\Omega)$, bounded functions $C(\Omega)$, continuous functions $C^{\alpha}(\Omega)$, Hölder continuous functions $\mathrm{Lip}(\Omega)$, Lipschitz functions Which sets are removable for holomorphic functions in these classes?","Let $\Omega$ be a domain in $\mathbb C$, and let $\mathscr X$ be some class of functions from $\Omega$ to $\mathbb C$. A set $E\subset \Omega$ is called removable for holomorphic functions of class $\mathscr X$ if the following holds: every function $f\in\mathscr X$ that is holomorphic on $\Omega\setminus E$ is actually holomorphic on $\Omega$, possibly, after being redefined on $E$. (An example of the above: $E$ is a line interval, $\mathscr X$ consists of continuous functions. In this case $E$ is removable, which is shown in the answer.) It is clear that the larger $\mathscr X$ is, the smaller is the class of removable sets. In the extreme case, if $\mathscr X$ contains all functions $\Omega\to\mathbb C$, there are no nonempty removable sets. Indeed, if $a\in E$, then $f(z)=\frac{1}{z-a}$ (arbitrarily defined at $z=a$) is holomorphic on $\Omega\setminus E$ but has no holomorphic extension to $\Omega$. The problem of describing removable sets is nontrivial in many classes $\mathscr X$ such as $L^{\infty}(\Omega)$, bounded functions $C(\Omega)$, continuous functions $C^{\alpha}(\Omega)$, Hölder continuous functions $\mathrm{Lip}(\Omega)$, Lipschitz functions Which sets are removable for holomorphic functions in these classes?",,['complex-analysis']
24,Why do complex functions have derivatives?,Why do complex functions have derivatives?,,"I'm sorry if I sound too ignorant, I don't have a high level of knowledge in math. The function $f(z)=z^2$ (where $z$ is a complex number) has a derivative equal to $2z$. I'm really confused about this. If we define the derivative of $f(z)$ as the limit as $h$ approaches $0$ (being $h$ a complex number) of $(f(z+h)-f(z))/h$, then clearly the derivative is $2z$, but what does this derivative represent?? Also, shouldn't we be able to represent a complex function in 4-dimensional space, since our input and output have 2 variables each ($z=x+iy$) and then we could take directional derivatives...right? But if we define the derivative as above, it would be the same if we approach it from all directions. That's what's bothering me so much. I would really appreciate any explanation. Thanks!","I'm sorry if I sound too ignorant, I don't have a high level of knowledge in math. The function $f(z)=z^2$ (where $z$ is a complex number) has a derivative equal to $2z$. I'm really confused about this. If we define the derivative of $f(z)$ as the limit as $h$ approaches $0$ (being $h$ a complex number) of $(f(z+h)-f(z))/h$, then clearly the derivative is $2z$, but what does this derivative represent?? Also, shouldn't we be able to represent a complex function in 4-dimensional space, since our input and output have 2 variables each ($z=x+iy$) and then we could take directional derivatives...right? But if we define the derivative as above, it would be the same if we approach it from all directions. That's what's bothering me so much. I would really appreciate any explanation. Thanks!",,"['complex-analysis', 'derivatives', 'complex-numbers', 'partial-derivative', 'intuition']"
25,Mean value theorem for holomorphic functions,Mean value theorem for holomorphic functions,,"The mean value theorem for holomorphic functions states that if $f$ is analytic in $D$ and $a \in D$, then $f(a)$ equals the integral around any circle centered at $a$ divided by $2\pi$.  But if $f$ is analytic, then the line integral around any closed curve is 0, so $f(a) = 0$.  Why does the MVT not result in all holomorphic functions being identically zero?  There must be something I'm missing here.","The mean value theorem for holomorphic functions states that if $f$ is analytic in $D$ and $a \in D$, then $f(a)$ equals the integral around any circle centered at $a$ divided by $2\pi$.  But if $f$ is analytic, then the line integral around any closed curve is 0, so $f(a) = 0$.  Why does the MVT not result in all holomorphic functions being identically zero?  There must be something I'm missing here.",,['complex-analysis']
26,When is a function satisfying the Cauchy-Riemann equations holomorphic?,When is a function satisfying the Cauchy-Riemann equations holomorphic?,,"It is, of course, one of the first results in basic complex analysis that a holomorphic function satisfies the Cauchy-Riemann equations when considered as a differentiable two-variable real function. I have always seen the converse as: if $f$ is continuously differentiable as a function from $U \subset \mathbb{R}^2$ to $\mathbb{R}^2$ and satisfies the Cauchy-Riemann equations, then it is holomorphic (see e.g. Stein and Shakarchi, or Wikipedia ). Why is the $C^1$ condition necessary? I don't see where this comes in to the proof below. Assume that $u(x,y)$ and $v(x,y)$ are continuously differentiable and satisfy the Cauchy-Riemann equations. Let $h=h_1 + h_2i$. Then \begin{equation*} u(x+h_1, y+h_2) - u(x,y) = \frac{\partial u}{\partial x} h_1 + \frac{\partial u}{\partial y}h_2 + o(|h|) \end{equation*} and  \begin{equation*} v(x+h_1, y+h_2) - v(x,y) = \frac{\partial v}{\partial x} h_1 + \frac{\partial v}{\partial y} h_2 + o(|h|). \end{equation*} Multiplying the second equation by $i$ and adding the two together gives  \begin{align*} (u+iv)(z+h)-(u+iv)(z) &= \frac{\partial u}{\partial x} h_1 + i \frac{\partial v}{\partial x} h_1 + \frac{\partial u}{\partial y} h_2 + i \frac{\partial v}{\partial y} h_2 + o(|h|)\\\  &= \left( \frac{\partial u}{\partial x} + i \frac{\partial v}{\partial x} \right) (h_1+i h_2) + o(|h|). \end{align*}  Now dividing by $h$ gives us the desired result. Does there exist a differentiable but not $C^1$ function $f: U \rightarrow \mathbb{R}^2$ which satisfies the Cauchy-Riemann equations and does NOT correspond to a complex-differentiable function?","It is, of course, one of the first results in basic complex analysis that a holomorphic function satisfies the Cauchy-Riemann equations when considered as a differentiable two-variable real function. I have always seen the converse as: if $f$ is continuously differentiable as a function from $U \subset \mathbb{R}^2$ to $\mathbb{R}^2$ and satisfies the Cauchy-Riemann equations, then it is holomorphic (see e.g. Stein and Shakarchi, or Wikipedia ). Why is the $C^1$ condition necessary? I don't see where this comes in to the proof below. Assume that $u(x,y)$ and $v(x,y)$ are continuously differentiable and satisfy the Cauchy-Riemann equations. Let $h=h_1 + h_2i$. Then \begin{equation*} u(x+h_1, y+h_2) - u(x,y) = \frac{\partial u}{\partial x} h_1 + \frac{\partial u}{\partial y}h_2 + o(|h|) \end{equation*} and  \begin{equation*} v(x+h_1, y+h_2) - v(x,y) = \frac{\partial v}{\partial x} h_1 + \frac{\partial v}{\partial y} h_2 + o(|h|). \end{equation*} Multiplying the second equation by $i$ and adding the two together gives  \begin{align*} (u+iv)(z+h)-(u+iv)(z) &= \frac{\partial u}{\partial x} h_1 + i \frac{\partial v}{\partial x} h_1 + \frac{\partial u}{\partial y} h_2 + i \frac{\partial v}{\partial y} h_2 + o(|h|)\\\  &= \left( \frac{\partial u}{\partial x} + i \frac{\partial v}{\partial x} \right) (h_1+i h_2) + o(|h|). \end{align*}  Now dividing by $h$ gives us the desired result. Does there exist a differentiable but not $C^1$ function $f: U \rightarrow \mathbb{R}^2$ which satisfies the Cauchy-Riemann equations and does NOT correspond to a complex-differentiable function?",,['complex-analysis']
27,An entire function whose real part is bounded above must be constant.,An entire function whose real part is bounded above must be constant.,,"Greets This is exercise 15.d chapter 3 of Stein & Shakarchi's ""Complex Analysis"", they hint: ""Use the maximum modulus principle"", but I didn't see how to do the exercise with this hint rightaway, instead I knew how to do it with the Casorati-Weiestrass Theorem, here is my answer: Define $g(z)=f(1/z)$ for $z\neq{0}$,then by the hypothesis we must have that for any $\epsilon>0$ $g(D_{\epsilon>0}(0)-\{0\})$ is not dense in $\mathbb{C}$, then the singularity at $0$ of $g$ is not essential, this implies $f$ must be a polynomial, but if $f$ is a non-constant polynomial, it is easy to see that its real part must be unbounded, so $f$ must be constant. I would like to know an answer with the the maximum modulus principle. Thanks","Greets This is exercise 15.d chapter 3 of Stein & Shakarchi's ""Complex Analysis"", they hint: ""Use the maximum modulus principle"", but I didn't see how to do the exercise with this hint rightaway, instead I knew how to do it with the Casorati-Weiestrass Theorem, here is my answer: Define $g(z)=f(1/z)$ for $z\neq{0}$,then by the hypothesis we must have that for any $\epsilon>0$ $g(D_{\epsilon>0}(0)-\{0\})$ is not dense in $\mathbb{C}$, then the singularity at $0$ of $g$ is not essential, this implies $f$ must be a polynomial, but if $f$ is a non-constant polynomial, it is easy to see that its real part must be unbounded, so $f$ must be constant. I would like to know an answer with the the maximum modulus principle. Thanks",,['complex-analysis']
28,"How can it be shown that $\mathrm{Aut}(\mathbb{C})=\{f\,|\,f(z)=az+b,a\neq 0\},$ is defined as bijective ...",How can it be shown that  is defined as bijective ...,"\mathrm{Aut}(\mathbb{C})=\{f\,|\,f(z)=az+b,a\neq 0\},","How can it be shown that $$\mathrm{Aut}(\mathbb{C})=\{f\,|\,f(z)=az+b,a\neq 0\},$$ where an automorphism of $\mathbb{C}$ is defined as a bijective entire function with entire inverse? If $f$ is of the form $f(z)=az+b$ , with $a\neq 0$ , then obviously $f$ is in $\mathrm{Aut}(\mathbb{C})$ . How can I prove the converse?","How can it be shown that where an automorphism of is defined as a bijective entire function with entire inverse? If is of the form , with , then obviously is in . How can I prove the converse?","\mathrm{Aut}(\mathbb{C})=\{f\,|\,f(z)=az+b,a\neq 0\}, \mathbb{C} f f(z)=az+b a\neq 0 f \mathrm{Aut}(\mathbb{C})",['complex-analysis']
29,Application of Liouville's Theorem,Application of Liouville's Theorem,,"Let $f(z)$ be an entire function such that $$|f(z)|<\frac{1}{|\text{Im}(z)|},\qquad z\in\Bbb C-\Bbb R.$$ The question asked me to prove that $f(z)=0$. At least looking at it, it really seems to have an application of Liouville's theorem lurking around somewhere, but I haven't found it. My thoughts first led me to think about doing this by contradiction and using Picard's little theorem. So, I've considered a strip containing the real axis (say of width $2$ for simplicity). This will imply the complement maps into the unit disk, which implies that this strip has to map almost everywhere else on the complex plane. Now, on the imaginary axis, I know that $f(z)$ will vanish as $z\to\infty$, and this seems like it might be useful. I'm sort of under the impression $f(z)$ might have an essential singularity at $\infty$, which would mean $f(1/z)$ has an essential singularity at $0$. Either way, it of course can't have a pole at $\infty$ because of $f(z)$ vanishing on the imaginary axis, and if it is a removable singularity, it must be $0$, which still gives the solution. Therefore, I think it must have something to do with $f(z)$ having an essential singularity at $\infty$. I was hoping to combine this with the above inequality and deduce a contradiction, but I haven't thought of one yet. Any hints or suggestions? I'm studying for a prelim. exam, so this isn't homework. Update: So, in the spirit of searching methods using the essential singularity at $z=0$, we have $$\left|f\left(\frac{1}{z}\right)\right|\leq\frac{1}{\left|\text{Im}\left(\frac{1}{z}\right)\right|}=\frac{|z|^2}{|y|}.$$ Thus, for $|y|>1$, we have $$\left|f\left(\frac{1}{z}\right)\right|<|y|\left|f\left(\frac{1}{z}\right)\right|<|z|^2.$$ Thus, if I can show that $|f(1/z)|$ is a polynomial, we'll know it can't have an essential singularity at the origin, concluding my proof.","Let $f(z)$ be an entire function such that $$|f(z)|<\frac{1}{|\text{Im}(z)|},\qquad z\in\Bbb C-\Bbb R.$$ The question asked me to prove that $f(z)=0$. At least looking at it, it really seems to have an application of Liouville's theorem lurking around somewhere, but I haven't found it. My thoughts first led me to think about doing this by contradiction and using Picard's little theorem. So, I've considered a strip containing the real axis (say of width $2$ for simplicity). This will imply the complement maps into the unit disk, which implies that this strip has to map almost everywhere else on the complex plane. Now, on the imaginary axis, I know that $f(z)$ will vanish as $z\to\infty$, and this seems like it might be useful. I'm sort of under the impression $f(z)$ might have an essential singularity at $\infty$, which would mean $f(1/z)$ has an essential singularity at $0$. Either way, it of course can't have a pole at $\infty$ because of $f(z)$ vanishing on the imaginary axis, and if it is a removable singularity, it must be $0$, which still gives the solution. Therefore, I think it must have something to do with $f(z)$ having an essential singularity at $\infty$. I was hoping to combine this with the above inequality and deduce a contradiction, but I haven't thought of one yet. Any hints or suggestions? I'm studying for a prelim. exam, so this isn't homework. Update: So, in the spirit of searching methods using the essential singularity at $z=0$, we have $$\left|f\left(\frac{1}{z}\right)\right|\leq\frac{1}{\left|\text{Im}\left(\frac{1}{z}\right)\right|}=\frac{|z|^2}{|y|}.$$ Thus, for $|y|>1$, we have $$\left|f\left(\frac{1}{z}\right)\right|<|y|\left|f\left(\frac{1}{z}\right)\right|<|z|^2.$$ Thus, if I can show that $|f(1/z)|$ is a polynomial, we'll know it can't have an essential singularity at the origin, concluding my proof.",,['complex-analysis']
30,Can a Power Series tell when to stop?,Can a Power Series tell when to stop?,,"The naive description of the radius of convergence of a complex power series is as the largest radius so that the ball avoids poles and branch cuts. This makes sense in a world where analytic functions are at worst meromorphic on $\mathbb{C}$ or involve the complex logarithm, but is patently false when you consider things like $$f(z) = \sum_{n=0}^\infty \frac{z^{n!+1}}{n!+1}$$ which is continuous on the boundary $|z|=1$ but has $\mathbb{D}$ as its full domain. A better way of talking about power series is to say that a power series terminates when it encounters a singular point: a point that the function cannot be analytically continued through. Question: Is there a sufficient way of looking at properties of a function within its domain to tell if you are ""getting close"" (in some sense or another) to a singular point? Can this be done while avoiding statements about points on the boundary explicitly, perhaps by talking exclusively about the behavior of the function or its derivatives? This is to say, can a power series tell when it's time to stop? Consider these examples. When $f$ has a pole at an isolated point in its domain, then clearly the power series cannot extend beyond such a point. This follows from a simple argument about the modulus of $|z|$ in the power series as we near a point where $|f(z)| \to \infty$. Specifically, we have a sequence of values $z_k$ in the domain approaching the singular point with $\lim_k |f(z_k)| = \infty$. On the other hand, $|f(z)|$ might be bounded on a domain and still cause the power series to fail. One example of a bounded function on $\mathbb{D}$ with natural boundary $\partial \mathbb{D}$ is a Blaschke product whose zeros accumulate at every point on the boundary. Here we can understand the failure of the power series for $f$ as arising from the data of $f$ itself, since $f$ cannot have its zeros accumulating. This is to say, there is a sequence $z_k$ in the domain approaching the singular point(s) with $|f(z_k)|=0$. We can consider lacunary series like the one mentioned above. One way of explaining the failure of the power series of $f$ in this case is to think of it as being constructed as the Fourier decomposition of $f(Re^{i\theta})$ for some $\{|z|=R\}$ contained in the domain of $f$. In this case we have $$f(z) = \int_\gamma \frac{f(w)}{w-z}dw = \sum_{n=0}^\infty\left(\frac1{2\pi}\int_{-\pi}^\pi f(Re^{i\theta}) e^{-in\theta} d\theta\right) \frac{z^n}{R^n}$$ Take, for example, $\sum_{n=0}^\infty \frac{z^{2^n}}{n^2}$. When $|z| =r < 1$ then the geomtric decay of $z^n$ smooths out the Fourier series for $f(re^{i\theta})$, but as we approach the boundary $\partial \mathbb{D}$ we begin to see the lacunary Fourier series $\sum_{n=0}^\infty \frac1{n^2}e^{i2^n \theta}$. If fast decaying Fourier coefficients represent smooth functions, these incredibly slow decaying Fourier coefficients emphasize the fact that the graph of this Fourier series is fractal, and so it can't possibly be used to continue the function. In this situation, radial limits of $f$ exist ($f$ is in $H^2(\mathbb{D})$), but points approaching the boundary don't depend on each other smoothly enough. One last perspective. Constructing functions that are continuous on $\overline{\mathbb{D}}$ but have $\partial\mathbb{D}$ as their natural boundary usually involves taking lacunary series and adding in decaying coefficients. If the coefficients are in $\ell^2(\mathbb{N})$ then the function is in $H^2(\mathbb{D})$ and has radial boundary values almost everywhere on $\partial\mathbb{D}$. On the other hand, if the gaps are big enough then the coefficients of the derivative(s) will be wild (this is essentially the statement of Ostrowski–Hadamard). If the power series ends because of a point that $f$ extends to continuously along a radial line, we can think of trying to analytically continue the function along that line. In this case we get something like the following picture of $$f(z) = \sum_{n=0}^\infty \frac{z^{n!}}{n^2+1}$$ with real part plotted over the line going from 0 to $i$. $\hskip1in$ Final Comments: I have decided against removing any of these points in case there are students who find them interesting. I suppose a reformulation of my question might be: Is there a more geometric way of explaining when a power series decides to stop, in contrast to simply looking at successive derivative operations applied to the coefficients (which is more or less how we actually compute the radius of convergence)?","The naive description of the radius of convergence of a complex power series is as the largest radius so that the ball avoids poles and branch cuts. This makes sense in a world where analytic functions are at worst meromorphic on $\mathbb{C}$ or involve the complex logarithm, but is patently false when you consider things like $$f(z) = \sum_{n=0}^\infty \frac{z^{n!+1}}{n!+1}$$ which is continuous on the boundary $|z|=1$ but has $\mathbb{D}$ as its full domain. A better way of talking about power series is to say that a power series terminates when it encounters a singular point: a point that the function cannot be analytically continued through. Question: Is there a sufficient way of looking at properties of a function within its domain to tell if you are ""getting close"" (in some sense or another) to a singular point? Can this be done while avoiding statements about points on the boundary explicitly, perhaps by talking exclusively about the behavior of the function or its derivatives? This is to say, can a power series tell when it's time to stop? Consider these examples. When $f$ has a pole at an isolated point in its domain, then clearly the power series cannot extend beyond such a point. This follows from a simple argument about the modulus of $|z|$ in the power series as we near a point where $|f(z)| \to \infty$. Specifically, we have a sequence of values $z_k$ in the domain approaching the singular point with $\lim_k |f(z_k)| = \infty$. On the other hand, $|f(z)|$ might be bounded on a domain and still cause the power series to fail. One example of a bounded function on $\mathbb{D}$ with natural boundary $\partial \mathbb{D}$ is a Blaschke product whose zeros accumulate at every point on the boundary. Here we can understand the failure of the power series for $f$ as arising from the data of $f$ itself, since $f$ cannot have its zeros accumulating. This is to say, there is a sequence $z_k$ in the domain approaching the singular point(s) with $|f(z_k)|=0$. We can consider lacunary series like the one mentioned above. One way of explaining the failure of the power series of $f$ in this case is to think of it as being constructed as the Fourier decomposition of $f(Re^{i\theta})$ for some $\{|z|=R\}$ contained in the domain of $f$. In this case we have $$f(z) = \int_\gamma \frac{f(w)}{w-z}dw = \sum_{n=0}^\infty\left(\frac1{2\pi}\int_{-\pi}^\pi f(Re^{i\theta}) e^{-in\theta} d\theta\right) \frac{z^n}{R^n}$$ Take, for example, $\sum_{n=0}^\infty \frac{z^{2^n}}{n^2}$. When $|z| =r < 1$ then the geomtric decay of $z^n$ smooths out the Fourier series for $f(re^{i\theta})$, but as we approach the boundary $\partial \mathbb{D}$ we begin to see the lacunary Fourier series $\sum_{n=0}^\infty \frac1{n^2}e^{i2^n \theta}$. If fast decaying Fourier coefficients represent smooth functions, these incredibly slow decaying Fourier coefficients emphasize the fact that the graph of this Fourier series is fractal, and so it can't possibly be used to continue the function. In this situation, radial limits of $f$ exist ($f$ is in $H^2(\mathbb{D})$), but points approaching the boundary don't depend on each other smoothly enough. One last perspective. Constructing functions that are continuous on $\overline{\mathbb{D}}$ but have $\partial\mathbb{D}$ as their natural boundary usually involves taking lacunary series and adding in decaying coefficients. If the coefficients are in $\ell^2(\mathbb{N})$ then the function is in $H^2(\mathbb{D})$ and has radial boundary values almost everywhere on $\partial\mathbb{D}$. On the other hand, if the gaps are big enough then the coefficients of the derivative(s) will be wild (this is essentially the statement of Ostrowski–Hadamard). If the power series ends because of a point that $f$ extends to continuously along a radial line, we can think of trying to analytically continue the function along that line. In this case we get something like the following picture of $$f(z) = \sum_{n=0}^\infty \frac{z^{n!}}{n^2+1}$$ with real part plotted over the line going from 0 to $i$. $\hskip1in$ Final Comments: I have decided against removing any of these points in case there are students who find them interesting. I suppose a reformulation of my question might be: Is there a more geometric way of explaining when a power series decides to stop, in contrast to simply looking at successive derivative operations applied to the coefficients (which is more or less how we actually compute the radius of convergence)?",,"['complex-analysis', 'soft-question', 'power-series']"
31,How to find area of a polygon built on the roots of a given polynomial?,How to find area of a polygon built on the roots of a given polynomial?,,"How to find the area of a (maximum area convex) polygon, built on the roots of a given polynomial in the complex plane? For example, consider the equation: $$2x^5+3x^3-x+1=0$$ It has one real and four complex roots and makes a nice convex pentagon in the complex plane (thanks, Wolfram Alpha): Using the formula for the area of a convex polygon: $$A=\frac{1}{2} \left( \begin{array}| x_1 & x_2 \\ y_1 & y_2 \end{array} + \begin{array}| x_2 & x_3 \\ y_2 & y_3 \end{array} + \dots +  \begin{array}| x_n & x_1 \\ y_n & y_1 \end{array} \right)$$ I obtained for this case (using numerical values of the roots): $$A=1.460144\dots$$ Another simple case - roots of unity. They just make regular polygons and the general formula for the area is well known. However, I would like to know if it's possible to find out this area without computing the roots, using only the coefficients of the polynomial? (The coefficients are meant to be rational). I know that polynomials with only real roots will all have $A=0$ , and for the polynomials with several real roots some of them will be inside our maximum area polygon. There is a useful theorem (see Rouche's theorem ), according to which: For a monic polynomial $$z^n+a_{n-1} z^{n-1}+\dots+a_1 z+a_0$$ All its roots will be located inside the circle $|z|=1+\max |a_k|$ . But this theorem gives relatively large area, and can't be used to approximate the area of the polygon.","How to find the area of a (maximum area convex) polygon, built on the roots of a given polynomial in the complex plane? For example, consider the equation: It has one real and four complex roots and makes a nice convex pentagon in the complex plane (thanks, Wolfram Alpha): Using the formula for the area of a convex polygon: I obtained for this case (using numerical values of the roots): Another simple case - roots of unity. They just make regular polygons and the general formula for the area is well known. However, I would like to know if it's possible to find out this area without computing the roots, using only the coefficients of the polynomial? (The coefficients are meant to be rational). I know that polynomials with only real roots will all have , and for the polynomials with several real roots some of them will be inside our maximum area polygon. There is a useful theorem (see Rouche's theorem ), according to which: For a monic polynomial All its roots will be located inside the circle . But this theorem gives relatively large area, and can't be used to approximate the area of the polygon.",2x^5+3x^3-x+1=0 A=\frac{1}{2} \left( \begin{array}| x_1 & x_2 \\ y_1 & y_2 \end{array} + \begin{array}| x_2 & x_3 \\ y_2 & y_3 \end{array} + \dots +  \begin{array}| x_n & x_1 \\ y_n & y_1 \end{array} \right) A=1.460144\dots A=0 z^n+a_{n-1} z^{n-1}+\dots+a_1 z+a_0 |z|=1+\max |a_k|,"['complex-analysis', 'polynomials']"
32,When can we find holomorphic bijections between annuli?,When can we find holomorphic bijections between annuli?,,"I'm self-studying some complex analysis, and apparently holomorphic bijections between two annuli exist precisely when the ratios of the radii are the same. More exactly, if $A_{\sigma,\rho}=\{z\in\mathbb{C}:\sigma<|z|<\rho\}$, then there is a holomorphic bijection between $A_{\sigma,\rho}$ and $A_{\sigma',\rho'}$ iff $\rho/\sigma=\rho'/\sigma'$. Is there a reference where this fact is proven? Or can a proof be included here if it's not overly involved? Thanks.","I'm self-studying some complex analysis, and apparently holomorphic bijections between two annuli exist precisely when the ratios of the radii are the same. More exactly, if $A_{\sigma,\rho}=\{z\in\mathbb{C}:\sigma<|z|<\rho\}$, then there is a holomorphic bijection between $A_{\sigma,\rho}$ and $A_{\sigma',\rho'}$ iff $\rho/\sigma=\rho'/\sigma'$. Is there a reference where this fact is proven? Or can a proof be included here if it's not overly involved? Thanks.",,"['complex-analysis', 'reference-request', 'teichmueller-theory']"
33,Cauchy-Riemann equations in polar form. [duplicate],Cauchy-Riemann equations in polar form. [duplicate],,"This question already has answers here : Proof of Cauchy Riemann Equations in Polar Coordinates (6 answers) Closed 2 years ago . Show that in polar coordinates, the Cauchy-Riemann equations take the form   $\dfrac{\partial u}{\partial r} = \dfrac{1}r \dfrac{\partial v}{\partial \theta}$ and $\dfrac{1}r \dfrac{\partial u}{\partial \theta} = −\dfrac{\partial v}{\partial r}$. Use these equations to show that the logarithm function defined by   $\log z = \log r + i\theta$ where $z=re^{i\theta}$ with $-\pi<\theta<\pi$ is holomorphic in the region $r > 0$ and $-\pi<\theta<\pi$. What I have so far: Cauchy-Riemann Equations: Let $f(z)$ = $u(x, y)$ +$iv(x, y)$ be a function on an open domain with continuous partial derivatives in the underlying real variables. Then f is differentiable at $z = x+iy$ if and only if $\frac{∂u}{∂ x}(x, y)$ = $\frac{∂ v}{∂ y}(x, y)$ and $\frac{∂u}{∂ y}(x, y)$ = −$\frac{∂ v}{∂ x}(x, y)$. So we have $f'(z)= \frac{∂u}{∂ x}(z) +i \frac{∂ v}{∂ x}(z)$. Let $f(z)$ = $f(re^{iθ})$= $u(r,θ)$ +$iv(r,θ)$ be a function on an open domain that does not contain zero and with continuous partial derivatives in the underlying real variables. Then f is differentiable at $z$ = $re^{iθ}$ if and only if $r \frac{∂u}{∂r}=\frac{∂ v}{∂θ}$ and $\frac{∂u}{∂θ}$ = $−r \frac{∂v}{∂ r}$. Sorry, if this is not very good. I just decided to start learning complex analysis today...","This question already has answers here : Proof of Cauchy Riemann Equations in Polar Coordinates (6 answers) Closed 2 years ago . Show that in polar coordinates, the Cauchy-Riemann equations take the form   $\dfrac{\partial u}{\partial r} = \dfrac{1}r \dfrac{\partial v}{\partial \theta}$ and $\dfrac{1}r \dfrac{\partial u}{\partial \theta} = −\dfrac{\partial v}{\partial r}$. Use these equations to show that the logarithm function defined by   $\log z = \log r + i\theta$ where $z=re^{i\theta}$ with $-\pi<\theta<\pi$ is holomorphic in the region $r > 0$ and $-\pi<\theta<\pi$. What I have so far: Cauchy-Riemann Equations: Let $f(z)$ = $u(x, y)$ +$iv(x, y)$ be a function on an open domain with continuous partial derivatives in the underlying real variables. Then f is differentiable at $z = x+iy$ if and only if $\frac{∂u}{∂ x}(x, y)$ = $\frac{∂ v}{∂ y}(x, y)$ and $\frac{∂u}{∂ y}(x, y)$ = −$\frac{∂ v}{∂ x}(x, y)$. So we have $f'(z)= \frac{∂u}{∂ x}(z) +i \frac{∂ v}{∂ x}(z)$. Let $f(z)$ = $f(re^{iθ})$= $u(r,θ)$ +$iv(r,θ)$ be a function on an open domain that does not contain zero and with continuous partial derivatives in the underlying real variables. Then f is differentiable at $z$ = $re^{iθ}$ if and only if $r \frac{∂u}{∂r}=\frac{∂ v}{∂θ}$ and $\frac{∂u}{∂θ}$ = $−r \frac{∂v}{∂ r}$. Sorry, if this is not very good. I just decided to start learning complex analysis today...",,"['complex-analysis', 'polar-coordinates']"
34,Conjecture: Every analytic function on the closed disk is conformally a polynomial.,Conjecture: Every analytic function on the closed disk is conformally a polynomial.,,"Here is my conjecture, any proof, counter-example, or intuitions? If $f$ is analytic on $\text{cl}(\mathbb{D})$ (that is, analytic on some open set containing $\text{cl}(\mathbb{D})$), then there is some injective analytic function $\phi:\mathbb{D}\to\mathbb{C}$ and polynomial $p$ such that $f=p\circ\phi$ on $\mathbb{D}$. PS: I would then say that $f$ and $p$ are conformally equivalent.  Is there a better term for this relationship? Note: $\text{cl}(\mathbb{D})$ could be replaced by any simply connected compact set.  If we wanted to replace ""simply connected"" with ""finitely connected"", then we would have to replace ""polynomial"" with ""rational function"".","Here is my conjecture, any proof, counter-example, or intuitions? If $f$ is analytic on $\text{cl}(\mathbb{D})$ (that is, analytic on some open set containing $\text{cl}(\mathbb{D})$), then there is some injective analytic function $\phi:\mathbb{D}\to\mathbb{C}$ and polynomial $p$ such that $f=p\circ\phi$ on $\mathbb{D}$. PS: I would then say that $f$ and $p$ are conformally equivalent.  Is there a better term for this relationship? Note: $\text{cl}(\mathbb{D})$ could be replaced by any simply connected compact set.  If we wanted to replace ""simply connected"" with ""finitely connected"", then we would have to replace ""polynomial"" with ""rational function"".",,"['complex-analysis', 'polynomials', 'conformal-geometry']"
35,Explicitly reconstructing a function from its moments,Explicitly reconstructing a function from its moments,,"Let $f$ be an integrable real valued function defined on $[0,\infty)$.  Let $$m_n=\int_0^\infty f(x)x^n \mathrm dx$$ be the $n^{th}$ moment, and suppose that all of these integrals converge absolutely.  Are there conditions that can we impose on $f$ that would allow us to write $f$ explicitly in terms of its moments and certain simple functions. This idea is similar to the fact that we can reconstruct sufficiently nice functions on $[0,1]$ from a sum of their Fourier coefficients and $e^{inx}$.  Also, on $(-\infty,\infty)$ we can reconstruct $f$ from an integral over the real line. The analogy for $[0,\infty)$ and $x^s$ is of course the Mellin transform, which has an inversion formula as a line integral in the complex plane. My question is then: Can we impose nice enough (non-trivial) conditions on a class of functions so that we can invert the Mellin transform on the real line based only on its values at the positive integers? Thanks! Note: I am not asking about the moment problem and the requirements for uniqueness.  (Such as Carleman's Condition etc..)","Let $f$ be an integrable real valued function defined on $[0,\infty)$.  Let $$m_n=\int_0^\infty f(x)x^n \mathrm dx$$ be the $n^{th}$ moment, and suppose that all of these integrals converge absolutely.  Are there conditions that can we impose on $f$ that would allow us to write $f$ explicitly in terms of its moments and certain simple functions. This idea is similar to the fact that we can reconstruct sufficiently nice functions on $[0,1]$ from a sum of their Fourier coefficients and $e^{inx}$.  Also, on $(-\infty,\infty)$ we can reconstruct $f$ from an integral over the real line. The analogy for $[0,\infty)$ and $x^s$ is of course the Mellin transform, which has an inversion formula as a line integral in the complex plane. My question is then: Can we impose nice enough (non-trivial) conditions on a class of functions so that we can invert the Mellin transform on the real line based only on its values at the positive integers? Thanks! Note: I am not asking about the moment problem and the requirements for uniqueness.  (Such as Carleman's Condition etc..)",,"['complex-analysis', 'fourier-analysis']"
36,Why: A holomorphic function with constant magnitude must be constant.,Why: A holomorphic function with constant magnitude must be constant.,,"How can I prove the following assertion? Let $f$ be a holomorphic function such that $|f|$ is a constant.  Then $f$ is constant. Edit: The more elementary the proof, the better. I'm working my way through a complex analysis workbook, and by this exercise, the only substantial thing covered has been the Cauchy-Riemann equations.","How can I prove the following assertion? Let $f$ be a holomorphic function such that $|f|$ is a constant.  Then $f$ is constant. Edit: The more elementary the proof, the better. I'm working my way through a complex analysis workbook, and by this exercise, the only substantial thing covered has been the Cauchy-Riemann equations.",,['complex-analysis']
37,Showing that $\int_0^1 \log(\sin \pi x)dx=-\log2$,Showing that,\int_0^1 \log(\sin \pi x)dx=-\log2,"I need help with a textbook exercise (Stein's Complex Analysis, Chapter 3, Exercises 9). This exercise requires me to show that $$\int_0^1 \log(\sin \pi x)dx=-\log2$$ A hint is given as ""Use the contour shown in Figure 9."" Since this is an exercise from Chapter 3, I think I should use the residue formula or something like that. But the function $f(x)=\log(\sin \pi x)$ becomes singular on $x=0$ and $x=1$, which makes the contour illegal for the residue theorem. Can anyone give me a further hint on this problem? Many thanks in advance! P.S. This is my first time on Math Stack Exchange. If you find my post ambiguous, let me know.","I need help with a textbook exercise (Stein's Complex Analysis, Chapter 3, Exercises 9). This exercise requires me to show that $$\int_0^1 \log(\sin \pi x)dx=-\log2$$ A hint is given as ""Use the contour shown in Figure 9."" Since this is an exercise from Chapter 3, I think I should use the residue formula or something like that. But the function $f(x)=\log(\sin \pi x)$ becomes singular on $x=0$ and $x=1$, which makes the contour illegal for the residue theorem. Can anyone give me a further hint on this problem? Many thanks in advance! P.S. This is my first time on Math Stack Exchange. If you find my post ambiguous, let me know.",,"['complex-analysis', 'contour-integration']"
38,Simpler way to evaluate the Fourier transform of $\exp\left(i e^x\right)$?,Simpler way to evaluate the Fourier transform of ?,\exp\left(i e^x\right),"I have the task to evaluate $|a(k)|^2$ with $$ a(k) = \int_{-\infty}^\infty \!dx\,\exp\left(i k x + i e^{x}\right).\tag{1}$$ The integral in (1) can be evaluated explicitly via the substitution $y=e^{x}$ with the result $$ a(k) = e^{-\pi k/2} \Gamma(ik)$$ where $\Gamma(x)$ is the Gamma-function. Using the result $$\Gamma(ix)\Gamma(-ix) = \frac{\pi}{x \sinh(\pi x)} \tag{2}$$ we find $$|a(k)|^2 = \frac{2\pi}{k(e^{2\pi k} -1)}.$$ Does anybody know a direct way to evaluate $|a(k)|^2$ possibly using some methods of complex analysis such that one does not need to know the special property (2) of the Gamma-function (and even the Gamma-function itself)? Edit1: (see also edit 2 about the question of convergence) There was a question about possible problems with the convergence of the integral. Let me indicate in what sense one can give a meaning to the formulas above. Eq. (1) converges in the half-plane $\text{Im}(k) < 0$. Of course, we want in the end $k \in \mathbb{R}$. We should understand this as a limes $$a(k)=\lim_{\eta\downarrow 0}\int_{-\infty}^\infty \!dx\,\exp\left(i k x - \eta x + i e^{x}\right)$$ and we ask for the value of $$  |a(k)|^2= \lim_{\eta\downarrow 0}\int_{-\infty}^\infty \!dx\,dy\,\exp\left(i k x - \eta x + i e^{x}  -i k y - \eta y - i e^{y}\right) $$ To show that (1) is indeed convergent for $ \text{Im}(k) < 0$, we employ the substitution $y= e^x$ and obtain $$a(k)= \int_0^\infty\!dy\,e^{i y} y^{i k -1}.$$ The magnitude of the integrand is $\sim \exp[-\text{Im}(y)]$  for $|y|\to \infty$ and the integrand  has no singularities. So we can deform the integration contour along the positive imaginary axis with $y=iz$ and $$a(k) = i\int_0^\infty\!dz\, e^{-z} (iz)^{ik-1} $$ which is convergent for $\text{Re}(ik -1)= -\text{Im}(k) -1 >-1$. Edit2: (about the convergence) As the question about the convergence of the integral as defined arises again and again, here is some explanation without using complex analysis (which might be simpler to understand for some). Note that as before, we understand $a(k)$ as $\lim_{\eta\downarrow 0}a(k-i \eta)$. Starting with Eq. (1) we employ the change of variables $y=e^{x}$. The limits of the resulting integral are $y\in[0,\infty)$ which we divide in two integrals which we will analyze individually, $$a(k) = a_1(k) + a_2(k) = \int_0^1\!dy\, y^{ik-1} e^{iy} + \int_1^\infty\!dy\, y^{ik-1} e^{iy}.$$ It is only for the part $a_1(k)$ where we need to care about the limit $\eta\downarrow 0$. In particular, we can perform an integration by parts (integrating $y^{ik-1}$) and obtain $$a_1(k) = -i k^{-1} e^{i} + k^{-1}\lim_{y\to 0} (i e^{iy} y^{ik}) - k^{-1}\int_0^1\!dy\, y^{ik} e^{iy} =-i k^{-1} e^{i}- k^{-1}\int_0^1\!dy\, y^{ik} e^{iy} .$$ For the vanishing of the limit, we have used the replacement $k\mapsto k-i\eta$ with $\eta \downarrow 0$. The rest is finite as $| y^{ik} e^{iy}| =1$. The second part $a_2(k)$ is finite even for real $k$ (understanding the integral as an improper integral). To see that explicitly, we introduce $z(y)= y + k \log y$. For $y> y^* =\text{max}(1,-k)$ the variable change $z(y)$ is monotonously increasing. So we can write $$a_2(k) = \int_1^{y^*}\!dy\, y^{ik-1} e^{iy} + \underbrace{\int_{z^*}^\infty\!dz\, \frac{e^{iz}}{y(1+k/y)}}_{a_3(k)}.$$ The first term is trivially finite. The second term, we split in real and imaginary part. We show that the imaginary part is finite, the real part is analogous. We split the integral in parts with $z \in [ m \pi , (m+1) \pi]$ with $b_m= \int_{m\pi}^{(m+1)\pi}\!dz\,\sin(z)/y(1+k/y)$. We set $m^* = \lceil z^*/\pi\rceil$. We have $$\text{Im}\,a_3(k) = \int_{z^*}^{m^* \pi} \!dz\,\frac{\sin z}{y(1+k/y)} + \sum_{m=m^*}^\infty b_m.\tag{3}$$ We have that $(-1)^m b_m \geq 0$ thus $b_m$ is alternating in sign. Moreover, $$|b_m| \leq \frac{1}{m(1+k/m\pi)} \leq \frac1m \to 0 \quad (m\to\infty),$$ so the series in (3) converges due to the Leibniz criterion.","I have the task to evaluate $|a(k)|^2$ with $$ a(k) = \int_{-\infty}^\infty \!dx\,\exp\left(i k x + i e^{x}\right).\tag{1}$$ The integral in (1) can be evaluated explicitly via the substitution $y=e^{x}$ with the result $$ a(k) = e^{-\pi k/2} \Gamma(ik)$$ where $\Gamma(x)$ is the Gamma-function. Using the result $$\Gamma(ix)\Gamma(-ix) = \frac{\pi}{x \sinh(\pi x)} \tag{2}$$ we find $$|a(k)|^2 = \frac{2\pi}{k(e^{2\pi k} -1)}.$$ Does anybody know a direct way to evaluate $|a(k)|^2$ possibly using some methods of complex analysis such that one does not need to know the special property (2) of the Gamma-function (and even the Gamma-function itself)? Edit1: (see also edit 2 about the question of convergence) There was a question about possible problems with the convergence of the integral. Let me indicate in what sense one can give a meaning to the formulas above. Eq. (1) converges in the half-plane $\text{Im}(k) < 0$. Of course, we want in the end $k \in \mathbb{R}$. We should understand this as a limes $$a(k)=\lim_{\eta\downarrow 0}\int_{-\infty}^\infty \!dx\,\exp\left(i k x - \eta x + i e^{x}\right)$$ and we ask for the value of $$  |a(k)|^2= \lim_{\eta\downarrow 0}\int_{-\infty}^\infty \!dx\,dy\,\exp\left(i k x - \eta x + i e^{x}  -i k y - \eta y - i e^{y}\right) $$ To show that (1) is indeed convergent for $ \text{Im}(k) < 0$, we employ the substitution $y= e^x$ and obtain $$a(k)= \int_0^\infty\!dy\,e^{i y} y^{i k -1}.$$ The magnitude of the integrand is $\sim \exp[-\text{Im}(y)]$  for $|y|\to \infty$ and the integrand  has no singularities. So we can deform the integration contour along the positive imaginary axis with $y=iz$ and $$a(k) = i\int_0^\infty\!dz\, e^{-z} (iz)^{ik-1} $$ which is convergent for $\text{Re}(ik -1)= -\text{Im}(k) -1 >-1$. Edit2: (about the convergence) As the question about the convergence of the integral as defined arises again and again, here is some explanation without using complex analysis (which might be simpler to understand for some). Note that as before, we understand $a(k)$ as $\lim_{\eta\downarrow 0}a(k-i \eta)$. Starting with Eq. (1) we employ the change of variables $y=e^{x}$. The limits of the resulting integral are $y\in[0,\infty)$ which we divide in two integrals which we will analyze individually, $$a(k) = a_1(k) + a_2(k) = \int_0^1\!dy\, y^{ik-1} e^{iy} + \int_1^\infty\!dy\, y^{ik-1} e^{iy}.$$ It is only for the part $a_1(k)$ where we need to care about the limit $\eta\downarrow 0$. In particular, we can perform an integration by parts (integrating $y^{ik-1}$) and obtain $$a_1(k) = -i k^{-1} e^{i} + k^{-1}\lim_{y\to 0} (i e^{iy} y^{ik}) - k^{-1}\int_0^1\!dy\, y^{ik} e^{iy} =-i k^{-1} e^{i}- k^{-1}\int_0^1\!dy\, y^{ik} e^{iy} .$$ For the vanishing of the limit, we have used the replacement $k\mapsto k-i\eta$ with $\eta \downarrow 0$. The rest is finite as $| y^{ik} e^{iy}| =1$. The second part $a_2(k)$ is finite even for real $k$ (understanding the integral as an improper integral). To see that explicitly, we introduce $z(y)= y + k \log y$. For $y> y^* =\text{max}(1,-k)$ the variable change $z(y)$ is monotonously increasing. So we can write $$a_2(k) = \int_1^{y^*}\!dy\, y^{ik-1} e^{iy} + \underbrace{\int_{z^*}^\infty\!dz\, \frac{e^{iz}}{y(1+k/y)}}_{a_3(k)}.$$ The first term is trivially finite. The second term, we split in real and imaginary part. We show that the imaginary part is finite, the real part is analogous. We split the integral in parts with $z \in [ m \pi , (m+1) \pi]$ with $b_m= \int_{m\pi}^{(m+1)\pi}\!dz\,\sin(z)/y(1+k/y)$. We set $m^* = \lceil z^*/\pi\rceil$. We have $$\text{Im}\,a_3(k) = \int_{z^*}^{m^* \pi} \!dz\,\frac{\sin z}{y(1+k/y)} + \sum_{m=m^*}^\infty b_m.\tag{3}$$ We have that $(-1)^m b_m \geq 0$ thus $b_m$ is alternating in sign. Moreover, $$|b_m| \leq \frac{1}{m(1+k/m\pi)} \leq \frac1m \to 0 \quad (m\to\infty),$$ so the series in (3) converges due to the Leibniz criterion.",,"['complex-analysis', 'definite-integrals']"
39,"A holomorphic function sending integers (and only integers) to $\{0,1,2,3\}$",A holomorphic function sending integers (and only integers) to,"\{0,1,2,3\}","Does there exist a function $f$ , holomorphic on the whole complex plane $\mathbb{C}$ , such that $f\left(\mathbb{Z}\right)=\{0,1,2,3\}$ and $\forall z\in\mathbb{C}\ (f(z)\in\{0,1,2,3\}\Rightarrow z\in\mathbb{Z})$ ? If yes, is it possible to have an explicit construction? Note that, for example, $h(z)=\frac{1}{6} \left(9-8 \cos \left(\frac{\pi z}{3}\right)-\cos (\pi z)\right)$ is not a valid solution, since, in particular, certain roots of the equation $h(z)=0$ are not integers, but complex numbers. Also note that this question is answered in positive regarding the function $g$ such that $g\left(\mathbb{Z}\right)=\{0,1,2\}$ and $\forall z\in\mathbb{C}\ (g(z)\in\{0,1,2\}\Rightarrow z\in\mathbb{Z})$ . In this case, an example of such function is $g(z)=1-\cos \left(\frac{\pi z}{2}\right)$ .","Does there exist a function , holomorphic on the whole complex plane , such that and ? If yes, is it possible to have an explicit construction? Note that, for example, is not a valid solution, since, in particular, certain roots of the equation are not integers, but complex numbers. Also note that this question is answered in positive regarding the function such that and . In this case, an example of such function is .","f \mathbb{C} f\left(\mathbb{Z}\right)=\{0,1,2,3\} \forall z\in\mathbb{C}\ (f(z)\in\{0,1,2,3\}\Rightarrow z\in\mathbb{Z}) h(z)=\frac{1}{6} \left(9-8 \cos \left(\frac{\pi z}{3}\right)-\cos (\pi z)\right) h(z)=0 g g\left(\mathbb{Z}\right)=\{0,1,2\} \forall z\in\mathbb{C}\ (g(z)\in\{0,1,2\}\Rightarrow z\in\mathbb{Z}) g(z)=1-\cos \left(\frac{\pi z}{2}\right)","['complex-analysis', 'galois-theory', 'riemann-surfaces', 'elliptic-functions']"
40,Why do complex numbers lend themselves to rotation?,Why do complex numbers lend themselves to rotation?,,"In the introductory complex analysis course I am taking, nearly every theorem relates to rotation and argument. Why do complex numbers love doing this so much? I can understand why these theorems work; however, aside from basic knowledge of polar coordinates, I do not intuitively understand what property of complex numbers make rotation and angle such a common/convenient idea.","In the introductory complex analysis course I am taking, nearly every theorem relates to rotation and argument. Why do complex numbers love doing this so much? I can understand why these theorems work; however, aside from basic knowledge of polar coordinates, I do not intuitively understand what property of complex numbers make rotation and angle such a common/convenient idea.",,"['complex-analysis', 'complex-numbers', 'rotations', 'angle']"
41,Sheaves and complex analysis,Sheaves and complex analysis,,"A complex analysis professor once told me that ""sheaves are all over the place"" in complex analysis. Of course one can define the sheaf of holomorphic functions: if $U\subset \mathbf{C}$ (or $\mathbf{C}^n$) is a nonempty open set, let $\mathcal{O}(U)$ denote the $\mathbf{C}$-vector space of holomorphic functions $f:U\to\mathbf{C}$, and we let $\mathcal{O}(\varnothing)=\{0\}$. The restriction maps are given by restriction holomorphic functions to open subsets.  This defines a sheaf on $\mathbf{C}$ with respect to its usual topology. Here are my questions: Are there interesting re-interpretations of well-known results in basic complex analysis in the language of sheaf theory (just to get one thinking about how things might translate)? Are there interesting new geometric insights that one gains by introducing this structure? (Feel free to reformulate the context of the question if 2 doesn't make sense). I guess I find it counter-intuitive that sheaves should say anything interesting about complex analysis, while it seems natural that they should say things about the geometry of the space on which they're defined.","A complex analysis professor once told me that ""sheaves are all over the place"" in complex analysis. Of course one can define the sheaf of holomorphic functions: if $U\subset \mathbf{C}$ (or $\mathbf{C}^n$) is a nonempty open set, let $\mathcal{O}(U)$ denote the $\mathbf{C}$-vector space of holomorphic functions $f:U\to\mathbf{C}$, and we let $\mathcal{O}(\varnothing)=\{0\}$. The restriction maps are given by restriction holomorphic functions to open subsets.  This defines a sheaf on $\mathbf{C}$ with respect to its usual topology. Here are my questions: Are there interesting re-interpretations of well-known results in basic complex analysis in the language of sheaf theory (just to get one thinking about how things might translate)? Are there interesting new geometric insights that one gains by introducing this structure? (Feel free to reformulate the context of the question if 2 doesn't make sense). I guess I find it counter-intuitive that sheaves should say anything interesting about complex analysis, while it seems natural that they should say things about the geometry of the space on which they're defined.",,"['geometry', 'complex-analysis', 'sheaf-theory']"
42,Complex Analysis Question from Stein,Complex Analysis Question from Stein,,"The question is #$14$ from Chapter $2$ in Stein and Shakarchi's text Complex Analysis : Suppose that $f$ is holomorphic in an open set containing the closed unit disc, except for a pole at $z_0$ on the unit circle. Show that if $$\sum_{n=0}^\infty a_nz^n$$ denotes the power series expansion $f$ in the open unit disc, then $$\lim_{n\to\infty}\frac{a_n}{a_{n+1}}=z_0.$$ I've shown that we can take $z_0=1$ without a loss of generality, but I'm having trouble showing the proof otherwise. One of the problems I'm having is because we aren't told the definition of a pole except that it is a place where the function isn't holomorphic. Disregarding this fact, the other problem I'm running into is that I don't know the order of the pole. Making some additional assumptions, including that the pole is simple so we can write $F(z)=(z-1)f(z)$ as a holomorphic function, we see that $$F(z)=-a_0+z(a_0-a_1)+z^2(a_1-a_2)+\cdots$$ This almost gets me to the end with these added assumptions, but I don't think it's quite enough (why do we know some of the $a_i$'s aren't $0$, for example). On another note, if we know $\lim_{n\to\infty}\frac{a_n}{a_{n+1}}$ exists, then it is easy to see that $\lim_{n\to\infty}\frac{|a_n|}{|a_{n+1}|}=1=|z_0|$; I, however, do not see why the limit must exist. Are there any hints that someone can provide? Even a solution would be nice, especially if one can avoid making any assumptions about what a pole is or is not. EDIT : So there isn't any confusion, I know the definition of a pole and I'm inclined to believe that the problem, as stated, necessarily has a pole at $z_0$. The problem is that the exercise is in Chapter $2$, and poles are introduced in Chapter $3$.","The question is #$14$ from Chapter $2$ in Stein and Shakarchi's text Complex Analysis : Suppose that $f$ is holomorphic in an open set containing the closed unit disc, except for a pole at $z_0$ on the unit circle. Show that if $$\sum_{n=0}^\infty a_nz^n$$ denotes the power series expansion $f$ in the open unit disc, then $$\lim_{n\to\infty}\frac{a_n}{a_{n+1}}=z_0.$$ I've shown that we can take $z_0=1$ without a loss of generality, but I'm having trouble showing the proof otherwise. One of the problems I'm having is because we aren't told the definition of a pole except that it is a place where the function isn't holomorphic. Disregarding this fact, the other problem I'm running into is that I don't know the order of the pole. Making some additional assumptions, including that the pole is simple so we can write $F(z)=(z-1)f(z)$ as a holomorphic function, we see that $$F(z)=-a_0+z(a_0-a_1)+z^2(a_1-a_2)+\cdots$$ This almost gets me to the end with these added assumptions, but I don't think it's quite enough (why do we know some of the $a_i$'s aren't $0$, for example). On another note, if we know $\lim_{n\to\infty}\frac{a_n}{a_{n+1}}$ exists, then it is easy to see that $\lim_{n\to\infty}\frac{|a_n|}{|a_{n+1}|}=1=|z_0|$; I, however, do not see why the limit must exist. Are there any hints that someone can provide? Even a solution would be nice, especially if one can avoid making any assumptions about what a pole is or is not. EDIT : So there isn't any confusion, I know the definition of a pole and I'm inclined to believe that the problem, as stated, necessarily has a pole at $z_0$. The problem is that the exercise is in Chapter $2$, and poles are introduced in Chapter $3$.",,['complex-analysis']
43,Where does the theory of Banach space-valued holomorphic functions differ from the classical treatment?,Where does the theory of Banach space-valued holomorphic functions differ from the classical treatment?,,"For a Banach space $V$ over $\mathbb{C}$ and $U \subset \mathbb{C}$ open, one can easily check that the notions of holomorphy hold for maps $f: U \rightarrow V$ just as in the classical sense. Indeed, I believe one can even use the Lebesgue integral for contour integration since Lebesgue integration on Banach-space-valued functions is also well-developed. It seems that almost all the major results of classical complex analysis for holomorphic functions $f: U \rightarrow \mathbb{C}$ still hold in an analogous manner. Where are some crucial points where the theory differs when $f$ takes values in a Banach space?","For a Banach space $V$ over $\mathbb{C}$ and $U \subset \mathbb{C}$ open, one can easily check that the notions of holomorphy hold for maps $f: U \rightarrow V$ just as in the classical sense. Indeed, I believe one can even use the Lebesgue integral for contour integration since Lebesgue integration on Banach-space-valued functions is also well-developed. It seems that almost all the major results of classical complex analysis for holomorphic functions $f: U \rightarrow \mathbb{C}$ still hold in an analogous manner. Where are some crucial points where the theory differs when $f$ takes values in a Banach space?",,"['complex-analysis', 'banach-spaces']"
44,What is the analytic continuation of the Riemann Zeta Function,What is the analytic continuation of the Riemann Zeta Function,,I am told that when computing the zeroes one does not use the normal definition of the rieman zeta function but an altogether different one that obeys the same functional relation. What is this other function that they use explicitly given? Also if I were to take one of these non trivial zeroes and plug it into the original definition would my answer tend towards zero as I evaluate the series?,I am told that when computing the zeroes one does not use the normal definition of the rieman zeta function but an altogether different one that obeys the same functional relation. What is this other function that they use explicitly given? Also if I were to take one of these non trivial zeroes and plug it into the original definition would my answer tend towards zero as I evaluate the series?,,"['complex-analysis', 'analytic-number-theory', 'riemann-zeta', 'riemann-hypothesis']"
45,Line integration in complex analysis,Line integration in complex analysis,,"In normal line integration, from what I understand, you are measuring the area underneath $f(x,y)$ along a curve in the $x\text{-}y$ plane from point $a$ to point $b$. But what is being measured with complex line integration, when you go from a point $z_1$ to a point $z_2$ in the complex plane? With regular line integration I can see $f(x,y)$ maps $(x,y)$ to a point on the $z$ axis directly above above/below $(x,y)$. But in the complex case, when you map from the domain $Z$ to the image $W$, you are mapping from $\mathbb{R^2}$ to $\mathbb{R^2}$ ...it is not mapping a point to 'directly above/below'...so I don't have any intuition of what is happening with complex line integration.","In normal line integration, from what I understand, you are measuring the area underneath $f(x,y)$ along a curve in the $x\text{-}y$ plane from point $a$ to point $b$. But what is being measured with complex line integration, when you go from a point $z_1$ to a point $z_2$ in the complex plane? With regular line integration I can see $f(x,y)$ maps $(x,y)$ to a point on the $z$ axis directly above above/below $(x,y)$. But in the complex case, when you map from the domain $Z$ to the image $W$, you are mapping from $\mathbb{R^2}$ to $\mathbb{R^2}$ ...it is not mapping a point to 'directly above/below'...so I don't have any intuition of what is happening with complex line integration.",,"['complex-analysis', 'intuition']"
46,Geometric interpretation of complex path integral,Geometric interpretation of complex path integral,,"Let's say that we want to make sense of integrating a function $f: \mathbb{C}\rightarrow\mathbb{C}$ over some path $\gamma$.  I can imagine two reasonable ways of doing it.  First, there's the way that we all know and love: $$\int_\gamma f := \int_a^b f(\gamma(t))\gamma'(t)\,dt$$ where $\gamma(t)$ parametrizes $\gamma$. Alternatively, we could say, ""We already have a nice formulation of line integrals for paths in $\mathbb{R}^n$, and $\mathbb{C}$ is essentially just $\mathbb{R}^2$.""  So we decide that we'll define the path integral like this: $$\int_\gamma f := \int_a^b f(\gamma(t))|\gamma'(t)| \,dt$$ like we might do for functions $f: \mathbb{R}^2 \rightarrow \mathbb{R}$.  This second definition has the nice property that $\int_\gamma f = \int_\gamma \text{Re}f + i \int_\gamma \text{Im}f.$ But of course we don't use the second formulation.  We stick with the first, and there are some nice utilitarian reasons for doing so (Cauchy's integral formula, etc.).  But are there more natural (geometric, preferably) reasons for using the first definition above? EXAMPLE For a simple example, let's say that $f(z) = i$ and I want to integrate $f$ along $\gamma$, the line from $0$ to $i$.  I'll choose the parametrization $\gamma(t) = it$, $t \in [0,1]$.  Then $\gamma'(t) = i$, and, using the first definition for our path integral, we get:  $$\int_\gamma f = \int_0^1 i \gamma'(t) \,dt = \int_0^1 i^2\, dt = -1.$$ Meanwhile, in the wayward world where we choose the second formulation for our path integrals, we have: $$\int_\gamma f = \int_0^1 i |\gamma'(t)| \,dt = i.$$  In essence, this second computation didn't care about the complex structure on $f$'s domain.  As far as this method was concerned, we integrated a constant function over a path of measure 1, so naturally we get that constant as our answer.  On the other hand, in the first computation, multiplying by $\gamma'$ carries some additional complex structure.  It knows the difference between integrating along lines from $0$ to $i$ or from $0$ to $1$, for example. Heuristically, it seems like the first definition allows the complex structures on $f$'s domain and range to interact (through the $\gamma'$ term), whereas the second definition neglects any complex structure on the domain of $f$ and is only concerned with stretching/reorientation caused by our choice of parametrization.  But why, a priori, are we interested in allowing $f$'s complex domain and range to interact in this way (if that's even the right way of framing it)?  Does this ""interaction"" have a geometric interpretation?  And is there a nice way of interpreting the line integral that makes the first definition obviously the more natural choice, or do we just go with it because it's useful? Thanks for reading!","Let's say that we want to make sense of integrating a function $f: \mathbb{C}\rightarrow\mathbb{C}$ over some path $\gamma$.  I can imagine two reasonable ways of doing it.  First, there's the way that we all know and love: $$\int_\gamma f := \int_a^b f(\gamma(t))\gamma'(t)\,dt$$ where $\gamma(t)$ parametrizes $\gamma$. Alternatively, we could say, ""We already have a nice formulation of line integrals for paths in $\mathbb{R}^n$, and $\mathbb{C}$ is essentially just $\mathbb{R}^2$.""  So we decide that we'll define the path integral like this: $$\int_\gamma f := \int_a^b f(\gamma(t))|\gamma'(t)| \,dt$$ like we might do for functions $f: \mathbb{R}^2 \rightarrow \mathbb{R}$.  This second definition has the nice property that $\int_\gamma f = \int_\gamma \text{Re}f + i \int_\gamma \text{Im}f.$ But of course we don't use the second formulation.  We stick with the first, and there are some nice utilitarian reasons for doing so (Cauchy's integral formula, etc.).  But are there more natural (geometric, preferably) reasons for using the first definition above? EXAMPLE For a simple example, let's say that $f(z) = i$ and I want to integrate $f$ along $\gamma$, the line from $0$ to $i$.  I'll choose the parametrization $\gamma(t) = it$, $t \in [0,1]$.  Then $\gamma'(t) = i$, and, using the first definition for our path integral, we get:  $$\int_\gamma f = \int_0^1 i \gamma'(t) \,dt = \int_0^1 i^2\, dt = -1.$$ Meanwhile, in the wayward world where we choose the second formulation for our path integrals, we have: $$\int_\gamma f = \int_0^1 i |\gamma'(t)| \,dt = i.$$  In essence, this second computation didn't care about the complex structure on $f$'s domain.  As far as this method was concerned, we integrated a constant function over a path of measure 1, so naturally we get that constant as our answer.  On the other hand, in the first computation, multiplying by $\gamma'$ carries some additional complex structure.  It knows the difference between integrating along lines from $0$ to $i$ or from $0$ to $1$, for example. Heuristically, it seems like the first definition allows the complex structures on $f$'s domain and range to interact (through the $\gamma'$ term), whereas the second definition neglects any complex structure on the domain of $f$ and is only concerned with stretching/reorientation caused by our choice of parametrization.  But why, a priori, are we interested in allowing $f$'s complex domain and range to interact in this way (if that's even the right way of framing it)?  Does this ""interaction"" have a geometric interpretation?  And is there a nice way of interpreting the line integral that makes the first definition obviously the more natural choice, or do we just go with it because it's useful? Thanks for reading!",,"['complex-analysis', 'soft-question']"
47,"$f(ax)=f(x)^2-1$, what is $f$?",", what is ?",f(ax)=f(x)^2-1 f,"Suppose $f(ax)=(f(x))^2-1$ and suppose that $f$ is analytic in some neighborhood of $x=0$ . Expanding in power series, we get $a=1+\sqrt{5}$ or $1-\sqrt{5}$ . We take positive $a$ . If $f\neq{\rm const}$ then $f'(0)\neq0$ - it can be any non-zero number. After that, we can uniquely define coefficients in power series $f^{(n)}(0)/n!$ step by step using differentiation of the functional equation $$  f(0)=f(0)^2-1\ \Rightarrow\ f(0)=\frac{1\pm\sqrt{5}}2;\ 2f'(0)f(0)=af'(0)\ \Rightarrow\ f(0)=a/2;\ \ 2f''(0)f(0)+2(f'(0))^2=a^2f''(0)\ \Rightarrow\ f''(0)=\frac{2(f'(0))^2}{a^2-a};\ \ .... $$ Can $f$ be expressed in terms of some known functions? It seems that $f$ is entire function of order $1$ . Indeed, due to the Leibnitz formula, we have $$  a^nf^{(n)}(0)=\sum_{k=0}^n\binom{n}{k}f^{(n-k)}(0)f^{(k)}(0), $$ which leads to \begin{equation}\label{l1}  f^{(n)}(0)=\frac{\sum_{k=1}^{n-1}\binom{n}{k}f^{(n-k)}(0)f^{(k)}(0)}{a^n-a}. \end{equation} It is true that $|f''(0)|\leq|f'(0)|^2=:C^2$ , see above. Suppose that we already proved $|f^{(k)}(0)|\leq C^k$ , $1\leq k\leq n-1$ . Then $$  |f^{(n)}(0)|\leq \frac{C^n2^n}{(\sqrt{5}+1)^{n-1}\sqrt{5}}\leq C^n. $$ Hence, the power series converges everywhere and $|f(x)|\leq|f(0)|+e^{C|x|}-1$ , $x\in\mathbb{C}$ . (The functional equation is somewhat similar to $\cos 2x=2\cos^2x-1$ . If we denote $g(x)=f(x)/f(0)$ then $$  g'(x)=g'(0)g(a^{-1}x)g(a^{-2}x)g(a^{-3}x).... $$ Also, the polynomial $$  P_n(x)=f(a^nf^{-1}(x))=P\circ...\circ P(x),\ \ \ P(x)=x^2-1 $$ is some analog of Chebyshev polynomial $T_{2^n}(x)$ . The polynomials $P_n(x)$ are related to the representations of some class of infinite Lie algebras, but I forgot the story behind that...) Remark. It is seen that $f(\lambda x)$ satisfies also the functional equation, for any $\lambda$ . So, we can choose $f'(0)$ freely. Instead of this, let us choose the closest to $0$ zero as $x_0:=1$ (if zeroes exist), i.e. $f(1)=0$ . Then $f(a^{-1})=1$ (not $-1$ , since $x_0$ is the first zero and $f(0)>0$ ). Applying again the functional equation, we get $$  f(1)=0,\ f(a^{-1})=1,\ f(a^{-2})=\sqrt{2},\ ...,\  f(a^{-n})=\sqrt{1+f(a^{-n+1})},\ .... $$ Theoretically, we can recover $f(x)$ from infinite number of values $f(a^{-n})$ covergent to $f(0)=a/2$ . The next statement is wrong because, perhaps, $f$ can have complex zeros along with real zeros. I leave it just for possible improvements. Due to $f'(ax)=2a^{-1}f(x)f'(x)=(2a^{-1})^2f(x)f(a^{-1}x)f'(a^{-1}x)=...$ (see above infinite product for $g$ ), we can conclude that $f'(x)\neq0$ for $x\in[0,a)$ and, hence $f$ is monotonic on this interval. $f$ is also monotonic for $x\in(a,a^2)$ , since $f'(ax)=2a^{-1}f(x)f'(x)$ . Due to $f(a^2x)=f(x)^2(f(x)^2-2)$ , we see that $x_1=a^2$ is a double zero, $x_2=a^4$ is a zero of fourth order, etc. Following the same arguments as above, there are no other zeroes. Then $$  f(x)=e^{h(x)}\prod_{n=0}^{\infty}\left(1-\frac{x}{a^{2n}}\right)^{2^n}. $$ Is $h(x)=\ln a-\ln 2$ ? Perhaps, something wrong can be here... but if this is true then $$  \ln f(x)=\ln a-\ln 2-\sum_{k=1}^{\infty}\frac{a^{2k}x^k}{k(a^{2k}-2)} $$ or something like that. While the previous section was wrong, the true Hadamard expansion perhaps exist. This is still rough. Let $\{x_r\}$ be the smallest primitive zeros of $f$ , such that all are other zeroes are $a^{2n}x_r$ . Dentote $H(x)=\prod_r(1-x/x_r)$ - I do not know about the convergence of the product, I am trying to explain some ideas. We can follow the arguments from the wrong section above. Then, perhaps, it is true that $$  f(x)=\frac{a e^{dx}}{2}\prod_{n=0}^{\infty}H\left(\frac x{a^{2n}}\right)^{2^n} $$ with some constant $d$ (I hope it is $0$ ), since the order of $f$ does not exceed $1$ . Substituting this identity into $f(a^2x)=f(x)^2(f(x)^2-2)$ we obtain also $$  f(x)=\sqrt{2+e^{d(a^2-2)x}(2/a)H(a^2x)}. $$ Using $f'(ax)=f'(0)\prod_{n=1}^{\infty}(2f(x/a^n)/a)$ (see above $g$ ), we obtain $$  f'(x)=f'(0)e^{\frac{dx}{a-1}}\prod_{n=1}^{\infty}H\left(\frac{x}{a^{2n}}\right)^{2^n-1}\prod_{n=1}^{\infty}H\left(\frac{x}{a^{2n-1}}\right)^{2^n-1}. $$ It is possible to obtain other formulas. There is also another motivation to study $f$ : $$  f(a^nx)=P\circ..\circ P(f(x)),\ \ P(x)=x^2-1. $$ Thus, we can try to analyze the stability of values $f(z)$ under the action of the group of polynomials. This question is related to dynamical systems, fractals... Maybe the dynamical systems community already studied such analytic functions? There is an exact relation to fractals and holomorphic dynamics. Using $f(z)=P\circ...\circ P(f(a^{-n}z))$ , $P(z)=z^2-1$ , we obtain that if $z_0$ (located in the strip $S_n:=\{z:\ a^n\leq|z_0|<a^{n+1}\}$ ) is a primitive zero of $f$ , i.e. $f(a^{-k}z)\neq0$ , $k\geq1$ , then $f(a^{-n}z_0)$ is a non-trivial root of the polynomial $P_n=P^{\circ n}$ . All such non-trivia roots have the form $$  q= s_0\sqrt{1+s_1\sqrt{1+...+s_{n-1}\sqrt{1+s_n\sqrt{2}}}}, $$ where $s_j\in\{-1,1\}$ . Without loss of generality, consider the case $f'(0)=1$ . Then, for sufficiently large $n_0$ , we have $$  \frac{a}2+a^{-n-n_0}z_0\approx f(a^{-n-n_0}z_0)= s_0\sqrt{1+s_1\sqrt{1+...+s_{n-1}\sqrt{1+s_n\sqrt{2}}}}. $$ Hence $$  a^{-n-n_0}z_0\approx s_0\sqrt{1+s_1\sqrt{1+...+s_{n-1}\sqrt{1+s_n\sqrt{2}}}}-\frac{a}2(=q) $$ and all such $q$ lying in $a^{-n_0}\leq|q|<a^{-n_0+1}$ correspond to primitive roots of $f$ lying in $S_n$ . I have computed them for $n=20$ (and $n_0=4$ ): It looks like a Julia set. So, zeros of $f$ (even primitive zeros) are very complex. I hope I did not make large mistakes somewhere... It is also possible to introduce more general functions satisfying $$  f(ax)=bf(x)^2+cf(x)+d\ \ \ {\rm or\ even}\ \ \ f(ax)=Q(f(x))  $$ with some polynomial or rational $Q$ . Such class of functions contain $\sin,\cos,\exp,...$ One of the main questions is still open: are there any relations between $f$ and some known functions? Of course, any information about asymptotics, expansions, numerical results, differential equations, etc. is very welcome. A relation with the approximation of the golden ratio. Finally, I found one relation with more or less known functions. Consider $$  g(z)=\lim_{n\to\infty}a^n\biggl(\underbrace{\sqrt{1+\sqrt{1+...\sqrt{1+z}}}}_n-\frac{a}2\biggr). $$ The function $g$ is analytic (not entire), it is considered in Paris, R. B. ""An Asymptotic Approximation Connected with the Golden Number."" Amer. Math. Monthly 94, 272-278, 1987. It satisfies the functional equation $$  g(z)=ag(\sqrt{1+z}). $$ Hence, $f$ is exactly the inverse function to $g$ . It is useful to note that $g'$ admits an explicit representation $$  g'(z)=\frac{a}{2\sqrt{1+z}}\cdot\frac{a}{2\sqrt{1+\sqrt{1+z}}}.... $$ Edit - 04 June 2019. There are few remarks: due to the product expansion for $g'(x)$ , all primitive zeros are simple and each simple zero of $f$ is primitive. Due to the comments and answers below (many thanks to the authors), the order of $f$ is $\rho=\ln 2/\ln a<1$ . This value can be obtained by substituting $e^{A|z|^{\rho}}$ into $f(az)=f(z)^2-1$ , which gives $a^{\rho}=2$ . Because the order $\rho<1$ , we have $d=0$ in the Weierstrass-Hadamard expansion. Something like this... For me, it was a bit strange to see the more or less explicit entire function $f$ whose zeros form fractals. Edit - 05 June 2019 As mentioned above, the function $f$ has a lot of multiple roots. There is a linear transform which leaves simple roots of only. Without loss of generality, consider the case $f'(0)=1$ . Recall that, see above, if $(x_r)$ are simple roots of $f$ then $f$ can be expressed in terms of $H(x)=\prod(1-x/x_r)$ as $$  f(x)=\sqrt{2+(2/a)H(a^2x)} $$ or $$  f(x)^2-1=1+(2/a)H(a^2x), $$ which leads to $$  f(x)=1+(2/a)H(ax). $$ The function $H$ has simple zeros only forming a fractal structure. Edit - 06 June 2019 People from IMRN said that $f(az)=P(f(z))$ is a Poincare equation. They provided also somereference: [main] P. Fatou, ""Memoire sur les equations fonctionnelles"", Bull. Soc. Math. Fr., 47, 161-271; 48, 33-94, 208-314 (1919). [2] A. Eremenko and G. Levin,  ""Periodic points of polynomials"",  Ukrain. Mat. Zh., 41 (1989), 1467--1471 [3] A. Eremenko, M. Sodin,  ""Iterations of rational functions and the distribution of the values of  Poincare functions"",  Teor. Funktsii Funktsional. Anal. i Prilozhen., No. 53 (1990), 18--25;  translation in J. Soviet Math. 58 (1992), no. 6, 504–509 At the moment, I did not find a detailed analysis of Hadamard expansion for Poincare functions (especially for the case $P(z)=z^2-1$ ), but there is something in, e.g., [4] G. Derfel, P. Grabner, F. Vogl, ""Complex asymptotics of Poincare functions and properties of Julia sets"", Math. Proc. Cambridge Philos. Soc., 145 (2008), 699-718 Edit - 10 June 2019 There is a closed-form expression for $f(z)$ based on the explicit formula for inverse $g(w)=f^{-1}(w)$ , see above, $$  g(w)=(w-a/2)\frac{2a}{a+2\sqrt{1+w}}\cdot\frac{2a}{a+2\sqrt{1+\sqrt{1+w}}}\dot.... $$ We can obtain explicit Vi`ete-type formulas, involving nested radicals, for all zeros of $f$ . Then, Weierstrass-Hadamard factorization gives us $$  f(z)=\frac{a}{2}\prod_{\sigma\in\Sigma}\left(1+\frac{2z}{a}\prod_{n=1}^{\infty}\frac{a+2\sigma_n\sqrt{1+\sigma_{n-1}\sqrt{1+...+\sigma_1\sqrt{1}}}}{2a}\right), $$ where $$  \Sigma=\{\sigma:\mathbb{N}\to\{\pm1\},\ \ \lim_{n\to\infty}\sigma_n=1\}. $$ This factorization is one of those I was looking for.","Suppose and suppose that is analytic in some neighborhood of . Expanding in power series, we get or . We take positive . If then - it can be any non-zero number. After that, we can uniquely define coefficients in power series step by step using differentiation of the functional equation Can be expressed in terms of some known functions? It seems that is entire function of order . Indeed, due to the Leibnitz formula, we have which leads to It is true that , see above. Suppose that we already proved , . Then Hence, the power series converges everywhere and , . (The functional equation is somewhat similar to . If we denote then Also, the polynomial is some analog of Chebyshev polynomial . The polynomials are related to the representations of some class of infinite Lie algebras, but I forgot the story behind that...) Remark. It is seen that satisfies also the functional equation, for any . So, we can choose freely. Instead of this, let us choose the closest to zero as (if zeroes exist), i.e. . Then (not , since is the first zero and ). Applying again the functional equation, we get Theoretically, we can recover from infinite number of values covergent to . The next statement is wrong because, perhaps, can have complex zeros along with real zeros. I leave it just for possible improvements. Due to (see above infinite product for ), we can conclude that for and, hence is monotonic on this interval. is also monotonic for , since . Due to , we see that is a double zero, is a zero of fourth order, etc. Following the same arguments as above, there are no other zeroes. Then Is ? Perhaps, something wrong can be here... but if this is true then or something like that. While the previous section was wrong, the true Hadamard expansion perhaps exist. This is still rough. Let be the smallest primitive zeros of , such that all are other zeroes are . Dentote - I do not know about the convergence of the product, I am trying to explain some ideas. We can follow the arguments from the wrong section above. Then, perhaps, it is true that with some constant (I hope it is ), since the order of does not exceed . Substituting this identity into we obtain also Using (see above ), we obtain It is possible to obtain other formulas. There is also another motivation to study : Thus, we can try to analyze the stability of values under the action of the group of polynomials. This question is related to dynamical systems, fractals... Maybe the dynamical systems community already studied such analytic functions? There is an exact relation to fractals and holomorphic dynamics. Using , , we obtain that if (located in the strip ) is a primitive zero of , i.e. , , then is a non-trivial root of the polynomial . All such non-trivia roots have the form where . Without loss of generality, consider the case . Then, for sufficiently large , we have Hence and all such lying in correspond to primitive roots of lying in . I have computed them for (and ): It looks like a Julia set. So, zeros of (even primitive zeros) are very complex. I hope I did not make large mistakes somewhere... It is also possible to introduce more general functions satisfying with some polynomial or rational . Such class of functions contain One of the main questions is still open: are there any relations between and some known functions? Of course, any information about asymptotics, expansions, numerical results, differential equations, etc. is very welcome. A relation with the approximation of the golden ratio. Finally, I found one relation with more or less known functions. Consider The function is analytic (not entire), it is considered in Paris, R. B. ""An Asymptotic Approximation Connected with the Golden Number."" Amer. Math. Monthly 94, 272-278, 1987. It satisfies the functional equation Hence, is exactly the inverse function to . It is useful to note that admits an explicit representation Edit - 04 June 2019. There are few remarks: due to the product expansion for , all primitive zeros are simple and each simple zero of is primitive. Due to the comments and answers below (many thanks to the authors), the order of is . This value can be obtained by substituting into , which gives . Because the order , we have in the Weierstrass-Hadamard expansion. Something like this... For me, it was a bit strange to see the more or less explicit entire function whose zeros form fractals. Edit - 05 June 2019 As mentioned above, the function has a lot of multiple roots. There is a linear transform which leaves simple roots of only. Without loss of generality, consider the case . Recall that, see above, if are simple roots of then can be expressed in terms of as or which leads to The function has simple zeros only forming a fractal structure. Edit - 06 June 2019 People from IMRN said that is a Poincare equation. They provided also somereference: [main] P. Fatou, ""Memoire sur les equations fonctionnelles"", Bull. Soc. Math. Fr., 47, 161-271; 48, 33-94, 208-314 (1919). [2] A. Eremenko and G. Levin,  ""Periodic points of polynomials"",  Ukrain. Mat. Zh., 41 (1989), 1467--1471 [3] A. Eremenko, M. Sodin,  ""Iterations of rational functions and the distribution of the values of  Poincare functions"",  Teor. Funktsii Funktsional. Anal. i Prilozhen., No. 53 (1990), 18--25;  translation in J. Soviet Math. 58 (1992), no. 6, 504–509 At the moment, I did not find a detailed analysis of Hadamard expansion for Poincare functions (especially for the case ), but there is something in, e.g., [4] G. Derfel, P. Grabner, F. Vogl, ""Complex asymptotics of Poincare functions and properties of Julia sets"", Math. Proc. Cambridge Philos. Soc., 145 (2008), 699-718 Edit - 10 June 2019 There is a closed-form expression for based on the explicit formula for inverse , see above, We can obtain explicit Vi`ete-type formulas, involving nested radicals, for all zeros of . Then, Weierstrass-Hadamard factorization gives us where This factorization is one of those I was looking for.","f(ax)=(f(x))^2-1 f x=0 a=1+\sqrt{5} 1-\sqrt{5} a f\neq{\rm const} f'(0)\neq0 f^{(n)}(0)/n! 
 f(0)=f(0)^2-1\ \Rightarrow\ f(0)=\frac{1\pm\sqrt{5}}2;\ 2f'(0)f(0)=af'(0)\ \Rightarrow\ f(0)=a/2;\ \ 2f''(0)f(0)+2(f'(0))^2=a^2f''(0)\ \Rightarrow\ f''(0)=\frac{2(f'(0))^2}{a^2-a};\ \ ....
 f f 1 
 a^nf^{(n)}(0)=\sum_{k=0}^n\binom{n}{k}f^{(n-k)}(0)f^{(k)}(0),
 \begin{equation}\label{l1}
 f^{(n)}(0)=\frac{\sum_{k=1}^{n-1}\binom{n}{k}f^{(n-k)}(0)f^{(k)}(0)}{a^n-a}.
\end{equation} |f''(0)|\leq|f'(0)|^2=:C^2 |f^{(k)}(0)|\leq C^k 1\leq k\leq n-1 
 |f^{(n)}(0)|\leq \frac{C^n2^n}{(\sqrt{5}+1)^{n-1}\sqrt{5}}\leq C^n.
 |f(x)|\leq|f(0)|+e^{C|x|}-1 x\in\mathbb{C} \cos 2x=2\cos^2x-1 g(x)=f(x)/f(0) 
 g'(x)=g'(0)g(a^{-1}x)g(a^{-2}x)g(a^{-3}x)....
 
 P_n(x)=f(a^nf^{-1}(x))=P\circ...\circ P(x),\ \ \ P(x)=x^2-1
 T_{2^n}(x) P_n(x) f(\lambda x) \lambda f'(0) 0 x_0:=1 f(1)=0 f(a^{-1})=1 -1 x_0 f(0)>0 
 f(1)=0,\ f(a^{-1})=1,\ f(a^{-2})=\sqrt{2},\ ...,\  f(a^{-n})=\sqrt{1+f(a^{-n+1})},\ ....
 f(x) f(a^{-n}) f(0)=a/2 f f'(ax)=2a^{-1}f(x)f'(x)=(2a^{-1})^2f(x)f(a^{-1}x)f'(a^{-1}x)=... g f'(x)\neq0 x\in[0,a) f f x\in(a,a^2) f'(ax)=2a^{-1}f(x)f'(x) f(a^2x)=f(x)^2(f(x)^2-2) x_1=a^2 x_2=a^4 
 f(x)=e^{h(x)}\prod_{n=0}^{\infty}\left(1-\frac{x}{a^{2n}}\right)^{2^n}.
 h(x)=\ln a-\ln 2 
 \ln f(x)=\ln a-\ln 2-\sum_{k=1}^{\infty}\frac{a^{2k}x^k}{k(a^{2k}-2)}
 \{x_r\} f a^{2n}x_r H(x)=\prod_r(1-x/x_r) 
 f(x)=\frac{a e^{dx}}{2}\prod_{n=0}^{\infty}H\left(\frac x{a^{2n}}\right)^{2^n}
 d 0 f 1 f(a^2x)=f(x)^2(f(x)^2-2) 
 f(x)=\sqrt{2+e^{d(a^2-2)x}(2/a)H(a^2x)}.
 f'(ax)=f'(0)\prod_{n=1}^{\infty}(2f(x/a^n)/a) g 
 f'(x)=f'(0)e^{\frac{dx}{a-1}}\prod_{n=1}^{\infty}H\left(\frac{x}{a^{2n}}\right)^{2^n-1}\prod_{n=1}^{\infty}H\left(\frac{x}{a^{2n-1}}\right)^{2^n-1}.
 f 
 f(a^nx)=P\circ..\circ P(f(x)),\ \ P(x)=x^2-1.
 f(z) f(z)=P\circ...\circ P(f(a^{-n}z)) P(z)=z^2-1 z_0 S_n:=\{z:\ a^n\leq|z_0|<a^{n+1}\} f f(a^{-k}z)\neq0 k\geq1 f(a^{-n}z_0) P_n=P^{\circ n} 
 q= s_0\sqrt{1+s_1\sqrt{1+...+s_{n-1}\sqrt{1+s_n\sqrt{2}}}},
 s_j\in\{-1,1\} f'(0)=1 n_0 
 \frac{a}2+a^{-n-n_0}z_0\approx f(a^{-n-n_0}z_0)= s_0\sqrt{1+s_1\sqrt{1+...+s_{n-1}\sqrt{1+s_n\sqrt{2}}}}.
 
 a^{-n-n_0}z_0\approx s_0\sqrt{1+s_1\sqrt{1+...+s_{n-1}\sqrt{1+s_n\sqrt{2}}}}-\frac{a}2(=q)
 q a^{-n_0}\leq|q|<a^{-n_0+1} f S_n n=20 n_0=4 f 
 f(ax)=bf(x)^2+cf(x)+d\ \ \ {\rm or\ even}\ \ \ f(ax)=Q(f(x)) 
 Q \sin,\cos,\exp,... f 
 g(z)=\lim_{n\to\infty}a^n\biggl(\underbrace{\sqrt{1+\sqrt{1+...\sqrt{1+z}}}}_n-\frac{a}2\biggr).
 g 
 g(z)=ag(\sqrt{1+z}).
 f g g' 
 g'(z)=\frac{a}{2\sqrt{1+z}}\cdot\frac{a}{2\sqrt{1+\sqrt{1+z}}}....
 g'(x) f f \rho=\ln 2/\ln a<1 e^{A|z|^{\rho}} f(az)=f(z)^2-1 a^{\rho}=2 \rho<1 d=0 f f f'(0)=1 (x_r) f f H(x)=\prod(1-x/x_r) 
 f(x)=\sqrt{2+(2/a)H(a^2x)}
 
 f(x)^2-1=1+(2/a)H(a^2x),
 
 f(x)=1+(2/a)H(ax).
 H f(az)=P(f(z)) P(z)=z^2-1 f(z) g(w)=f^{-1}(w) 
 g(w)=(w-a/2)\frac{2a}{a+2\sqrt{1+w}}\cdot\frac{2a}{a+2\sqrt{1+\sqrt{1+w}}}\dot....
 f 
 f(z)=\frac{a}{2}\prod_{\sigma\in\Sigma}\left(1+\frac{2z}{a}\prod_{n=1}^{\infty}\frac{a+2\sigma_n\sqrt{1+\sigma_{n-1}\sqrt{1+...+\sigma_1\sqrt{1}}}}{2a}\right),
 
 \Sigma=\{\sigma:\mathbb{N}\to\{\pm1\},\ \ \lim_{n\to\infty}\sigma_n=1\}.
","['complex-analysis', 'dynamical-systems', 'functional-equations', 'entire-functions']"
48,How does a branch cut define a branch?,How does a branch cut define a branch?,,"I am studying complex analysis and I have problem understanding the concept of branch cut. The lecturer draw this as some curve that starts from a point and goes on and on in some direction (for example, something like $y=x$ for $x\geq0$ , but it doesn't have to be straight). The definition given in the lecture is A branch cut is a curve that is being presented in order to define a   branch and then he added a note Points on the branch cut are singular. Can someone please explain how does such a curve define a branch of a function ? As I understand it, if $f(z)=u(r,\theta)+iv(r,\theta)$ then we want $\alpha<\theta\leq\alpha+2\pi$ for some real $\alpha$ . How does a curve define this $\alpha$ ? Why are the points on a branch cut singular? Are we also assuming something about the function that we are trying to define a branch of it?","I am studying complex analysis and I have problem understanding the concept of branch cut. The lecturer draw this as some curve that starts from a point and goes on and on in some direction (for example, something like for , but it doesn't have to be straight). The definition given in the lecture is A branch cut is a curve that is being presented in order to define a   branch and then he added a note Points on the branch cut are singular. Can someone please explain how does such a curve define a branch of a function ? As I understand it, if then we want for some real . How does a curve define this ? Why are the points on a branch cut singular? Are we also assuming something about the function that we are trying to define a branch of it?","y=x x\geq0 f(z)=u(r,\theta)+iv(r,\theta) \alpha<\theta\leq\alpha+2\pi \alpha \alpha",['complex-analysis']
49,Contour integral for $x^3/(e^x-1)$?,Contour integral for ?,x^3/(e^x-1),What contour and integrand do we use to evaluate $$ \int_0^\infty \frac{x^3}{e^x-1} dx $$ Or is this going to need some other technique?,What contour and integrand do we use to evaluate $$ \int_0^\infty \frac{x^3}{e^x-1} dx $$ Or is this going to need some other technique?,,"['complex-analysis', 'definite-integrals']"
50,$f^3 + g^3=1$ for two meromorphic functions,for two meromorphic functions,f^3 + g^3=1,"Can you find two non-constant meromorphic functions $f,g$ such that $f^3 +g^3=1$?","Can you find two non-constant meromorphic functions $f,g$ such that $f^3 +g^3=1$?",,['complex-analysis']
51,Why is the ring of holomorphic functions not a UFD?,Why is the ring of holomorphic functions not a UFD?,,"Am I correct or not? I think that a ring of holomorphic functions in one variable is not a UFD , because there are holomorphic functions with an infinite number of $0$'s, and hence it will have an infinite number of irreducible factors! But I am unable to get a concrete example. Please give some example.","Am I correct or not? I think that a ring of holomorphic functions in one variable is not a UFD , because there are holomorphic functions with an infinite number of $0$'s, and hence it will have an infinite number of irreducible factors! But I am unable to get a concrete example. Please give some example.",,"['complex-analysis', 'commutative-algebra', 'unique-factorization-domains']"
52,on the boundary of analytic functions,on the boundary of analytic functions,,"Suppose I have a function $f$ that is analytic on the unit disk $D = \{ z \in \mathbb{C} : |z| < 1 \}$ that is also continuous up to $\bar{D}$. If $f$ is identically zero on some segment of of the boundary (e.g. $\{ e^{it}, 0 \leq t \leq \pi/2 \}$ ), is it then true that $f$ is identically zero on the entire boundary? I know that analytic functions that are zero at an accumulation point inside the domain of analyticity are identically zero throughout the entire domain, but I don't know what (if anything) can be said if something similar occurs on the boundary of the domain.","Suppose I have a function $f$ that is analytic on the unit disk $D = \{ z \in \mathbb{C} : |z| < 1 \}$ that is also continuous up to $\bar{D}$. If $f$ is identically zero on some segment of of the boundary (e.g. $\{ e^{it}, 0 \leq t \leq \pi/2 \}$ ), is it then true that $f$ is identically zero on the entire boundary? I know that analytic functions that are zero at an accumulation point inside the domain of analyticity are identically zero throughout the entire domain, but I don't know what (if anything) can be said if something similar occurs on the boundary of the domain.",,['complex-analysis']
53,"if $f$ is entire and $|f(z)| \leq 1+|z|^{1/2}$, why must $f$ be constant?","if  is entire and , why must  be constant?",f |f(z)| \leq 1+|z|^{1/2} f,"How can we prove that if $f:\mathbb{C}\rightarrow\mathbb{C}$ is holomorphic (analytic) and $|f(z)| \leq 1+|z|^{1/2} \forall z$, then $f$ is constant? Liouville's theorem springs to mind, but I can't see how to use it since $1+|z|^{1/2}$ is not holomorphic. The maximum modulus principle doesn't seem easily usable either. And the principle of isolated zeroes can't really be applied since all we know is an inequality, not an equation. Many thanks for any help with this!","How can we prove that if $f:\mathbb{C}\rightarrow\mathbb{C}$ is holomorphic (analytic) and $|f(z)| \leq 1+|z|^{1/2} \forall z$, then $f$ is constant? Liouville's theorem springs to mind, but I can't see how to use it since $1+|z|^{1/2}$ is not holomorphic. The maximum modulus principle doesn't seem easily usable either. And the principle of isolated zeroes can't really be applied since all we know is an inequality, not an equation. Many thanks for any help with this!",,['complex-analysis']
54,"How to rigorously justify ""picking up half a residue""?","How to rigorously justify ""picking up half a residue""?",,"Often in contour integrals, we integrate around a singularity by putting a small semicircular indent $\theta \rightarrow z_0 + re^{i\theta}$, $0 \leq \theta \leq \pi$ around the singularity at $z_0$. Then one claims that the integral ""picks up half a residue"" as $r \rightarrow 0$, so we compute the residue, divide by two, and multiply by $2\pi i$ to get the limiting value of the integral over the small semicircle. I don't see how to justify this rigorously. I tried adapted the proof of the residue theorem, which involves expanding Taylor series, but this crucially relies on the fact that the path is closed. I also tried using seeing if the values of the function on the semicircle all tend to the same value as the semicircle shrinks to zero. But I'm not sure if this is even true, since we don't really have nice behavior of the function at $z_0$. So my question is this. Under what circumstances can we claim that the integral over a small semicircle centered at a pole $z_0$ is $\pi i$ times the residue at $z_0$, and how do we prove this?","Often in contour integrals, we integrate around a singularity by putting a small semicircular indent $\theta \rightarrow z_0 + re^{i\theta}$, $0 \leq \theta \leq \pi$ around the singularity at $z_0$. Then one claims that the integral ""picks up half a residue"" as $r \rightarrow 0$, so we compute the residue, divide by two, and multiply by $2\pi i$ to get the limiting value of the integral over the small semicircle. I don't see how to justify this rigorously. I tried adapted the proof of the residue theorem, which involves expanding Taylor series, but this crucially relies on the fact that the path is closed. I also tried using seeing if the values of the function on the semicircle all tend to the same value as the semicircle shrinks to zero. But I'm not sure if this is even true, since we don't really have nice behavior of the function at $z_0$. So my question is this. Under what circumstances can we claim that the integral over a small semicircle centered at a pole $z_0$ is $\pi i$ times the residue at $z_0$, and how do we prove this?",,"['complex-analysis', 'residue-calculus']"
55,What is the image near the essential singularity of z sin(1/z)?,What is the image near the essential singularity of z sin(1/z)?,,"This was part of a homework problem from J.B. Conway's complex analysis text which I was assigned long ago but didn't get.  A few years later I was a TA for a course where the problem was assigned.  I still didn't know how to solve it, nor did any of my students.  If you can give me a simple answer, my gratefulness will far outweigh my embarrassment. Define $f(z)=z\sin(\frac{1}{z})$ on the punctured plane.  What is the image under $f$ of a (small) punctured disk at the origin? Remarks: The corresponding problem for $\sin(\frac{1}{z})$ is straightforward to solve using the definition of $\sin$ in terms of exponentials, the fact that $\exp$ is onto the punctured plane, and periodicity.  The answer in this case is the whole plane. The corresponding problem for $z\cos(\frac{1}{z})$ is straightforward to solve using Picard's theorem, because the oddness of the function implies that 0 is the only possible excluded point, and it is easy to see that 0 is in the image by taking reciprocals of large zeros of $\cos$.  Thus the answer in this case is also the whole plane. (The same argument could be applied to the previous remark, but there no big theorems are needed.  Perhaps Picard's theorem isn't needed here either.) Another part of the problem I couldn't solve generalizes this, taking $z^n\sin(\frac{1}{z})$ when $n$ is a positive integer.  When $n$ is even, there is no problem, because the argument from the previous remark applies.  But I don't know how to solve it when $n$ is odd. By Picard's theorem, there is at most one point missing. I would be surprised if there is a point missing, but I have no argument to back up this expectation. Although an elementary argument would be nice, I do not care whether the argument is ""appropriate"" for what is covered up to that point in the text.  This is pure curiosity.","This was part of a homework problem from J.B. Conway's complex analysis text which I was assigned long ago but didn't get.  A few years later I was a TA for a course where the problem was assigned.  I still didn't know how to solve it, nor did any of my students.  If you can give me a simple answer, my gratefulness will far outweigh my embarrassment. Define $f(z)=z\sin(\frac{1}{z})$ on the punctured plane.  What is the image under $f$ of a (small) punctured disk at the origin? Remarks: The corresponding problem for $\sin(\frac{1}{z})$ is straightforward to solve using the definition of $\sin$ in terms of exponentials, the fact that $\exp$ is onto the punctured plane, and periodicity.  The answer in this case is the whole plane. The corresponding problem for $z\cos(\frac{1}{z})$ is straightforward to solve using Picard's theorem, because the oddness of the function implies that 0 is the only possible excluded point, and it is easy to see that 0 is in the image by taking reciprocals of large zeros of $\cos$.  Thus the answer in this case is also the whole plane. (The same argument could be applied to the previous remark, but there no big theorems are needed.  Perhaps Picard's theorem isn't needed here either.) Another part of the problem I couldn't solve generalizes this, taking $z^n\sin(\frac{1}{z})$ when $n$ is a positive integer.  When $n$ is even, there is no problem, because the argument from the previous remark applies.  But I don't know how to solve it when $n$ is odd. By Picard's theorem, there is at most one point missing. I would be surprised if there is a point missing, but I have no argument to back up this expectation. Although an elementary argument would be nice, I do not care whether the argument is ""appropriate"" for what is covered up to that point in the text.  This is pure curiosity.",,['complex-analysis']
56,"Is there an exposition of complex analysis firmly separating the algebra, analysis, and topology?","Is there an exposition of complex analysis firmly separating the algebra, analysis, and topology?",,"Complex analysis seems to work because of the interplay between algebraic geometry over $\mathbb{C}$, and analysis and topology exploiting the fact that $\mathbb{C}/\mathbb{R}$ happens to be a quadratic extension.  Is there an exposition of elementary complex analysis (maybe at the level of Ahlfors's Complex Analysis ) that very carefully separates these things out, perhaps pointing out at each step what would be the same or different when working over other fields or field extensions?","Complex analysis seems to work because of the interplay between algebraic geometry over $\mathbb{C}$, and analysis and topology exploiting the fact that $\mathbb{C}/\mathbb{R}$ happens to be a quadratic extension.  Is there an exposition of elementary complex analysis (maybe at the level of Ahlfors's Complex Analysis ) that very carefully separates these things out, perhaps pointing out at each step what would be the same or different when working over other fields or field extensions?",,"['complex-analysis', 'reference-request', 'algebraic-geometry']"
57,The inverse of a bijective holomorphic function is also holomorphic,The inverse of a bijective holomorphic function is also holomorphic,,"I'm confused about the following proposition Proposition . Let $U,V$ are open sets in $\mathbf{C}$. If $f:U\to V$ is holomorphic and bijective, then  the inverse  $f^{-1}:V\to U$ is also holomorphic. The proof of the proposition think that the  continuity of $f^{-1}$ is obvious, but I find it is really difficult to prove  using $\epsilon-\delta$ definition. Can anyone give some hints?","I'm confused about the following proposition Proposition . Let $U,V$ are open sets in $\mathbf{C}$. If $f:U\to V$ is holomorphic and bijective, then  the inverse  $f^{-1}:V\to U$ is also holomorphic. The proof of the proposition think that the  continuity of $f^{-1}$ is obvious, but I find it is really difficult to prove  using $\epsilon-\delta$ definition. Can anyone give some hints?",,"['complex-analysis', 'inverse']"
58,Why can the complex conjugate of a variable be treated as a constant when differentiating with respect to that variable?,Why can the complex conjugate of a variable be treated as a constant when differentiating with respect to that variable?,,"I'm trying to understand the derivation of Wiener deconvolution given on its Wikipedia page .  In the last couple steps under the derivation section, they take the derivative with respect to $G(f)$ of an equation that has both $G(f)$ and $G^\ast(f)$ in it.  They simply state that $G^\ast (f)$ acts as a constant in the differentiation.  However, it seems to me that if you don't treat $G(f)$ as a constant, then you shouldn't be able to treat $G^\ast (f)$ as a constant because they are directly related. I searched around some looking for an explanation.  I found this page , which seems to agree that the complex conjugate can be treated as a constant.  I also found some stuff about the Cauchy-Riemann equations , which seem to be related.  However, I haven't had any classes on complex analysis and don't understand the intuition behind why this can be done. Why can the complex conjugate of a variable be treated as a constant when differentiating with respect to that variable?","I'm trying to understand the derivation of Wiener deconvolution given on its Wikipedia page .  In the last couple steps under the derivation section, they take the derivative with respect to $G(f)$ of an equation that has both $G(f)$ and $G^\ast(f)$ in it.  They simply state that $G^\ast (f)$ acts as a constant in the differentiation.  However, it seems to me that if you don't treat $G(f)$ as a constant, then you shouldn't be able to treat $G^\ast (f)$ as a constant because they are directly related. I searched around some looking for an explanation.  I found this page , which seems to agree that the complex conjugate can be treated as a constant.  I also found some stuff about the Cauchy-Riemann equations , which seem to be related.  However, I haven't had any classes on complex analysis and don't understand the intuition behind why this can be done. Why can the complex conjugate of a variable be treated as a constant when differentiating with respect to that variable?",,['complex-analysis']
59,Can we characterize the Möbius transformations that maps the unit disk into itself?,Can we characterize the Möbius transformations that maps the unit disk into itself?,,The Möbius transformations are the maps of the form $$ f(z)= \frac{az+b}{cz+d}.$$ Can we characterize the Möbius transformations that map the unit disk $$\{z\in \mathbb C: |z| <1\}$$ into itself?,The Möbius transformations are the maps of the form $$ f(z)= \frac{az+b}{cz+d}.$$ Can we characterize the Möbius transformations that map the unit disk $$\{z\in \mathbb C: |z| <1\}$$ into itself?,,['complex-analysis']
60,Inequality with complex numbers,Inequality with complex numbers,,"Consider the following problem. Fix $n \in \mathbb N$ . Prove that for every set of complex numbers $\{z_i\}_{1\le i \le n}$ , there is a subset $J\subset \{1,\dots , n\}$ such that $$\left|\sum_{j\in J} z_j\right|\ge \frac{1}{4\sqrt 2} \sum_{k=1}^n |z_k|.$$ I believe I proven have a stronger statement. Is this proof correct, and if so, what is the optimal constant? My proof. Consider all the $z_i$ with positive real part. Call the real part of the sum of these numbers $X^+$ . In a similar way, form $X^-$ , $Y^+$ , and $Y^-$ . Without loss of generality, let $X^+$ have the greatest magnitude of these. Note that because $|\operatorname{Re}(z)|+|\operatorname{Im}(z)|\ge |z|$ , we have $$ \left(\sum_{k=1}^n  |\operatorname{Re}(z_k)|+|\operatorname{Im}(z_k)| \right) \ge \sum_{k=1}^n |z_k|.$$ But note that $\sum \limits_{k=1}^n  |\operatorname{Re}(z_k)|+|\operatorname{Im}(z_k)| = X^+ + |X^-|+ Y^+ +|Y^-|$ , so we have $$ 4X^+ \ge \sum_{k=1}^n |z_k|.$$ By choosing $J$ to be the set of complex number with positive real part, this proves a stronger statement, because the factor of $1/\sqrt 2$ isn't needed.","Consider the following problem. Fix . Prove that for every set of complex numbers , there is a subset such that I believe I proven have a stronger statement. Is this proof correct, and if so, what is the optimal constant? My proof. Consider all the with positive real part. Call the real part of the sum of these numbers . In a similar way, form , , and . Without loss of generality, let have the greatest magnitude of these. Note that because , we have But note that , so we have By choosing to be the set of complex number with positive real part, this proves a stronger statement, because the factor of isn't needed.","n \in \mathbb N \{z_i\}_{1\le i \le n} J\subset \{1,\dots , n\} \left|\sum_{j\in J} z_j\right|\ge \frac{1}{4\sqrt 2} \sum_{k=1}^n |z_k|. z_i X^+ X^- Y^+ Y^- X^+ |\operatorname{Re}(z)|+|\operatorname{Im}(z)|\ge |z|  \left(\sum_{k=1}^n  |\operatorname{Re}(z_k)|+|\operatorname{Im}(z_k)| \right) \ge \sum_{k=1}^n |z_k|. \sum \limits_{k=1}^n  |\operatorname{Re}(z_k)|+|\operatorname{Im}(z_k)| = X^+ + |X^-|+ Y^+ +|Y^-|  4X^+ \ge \sum_{k=1}^n |z_k|. J 1/\sqrt 2","['complex-analysis', 'inequality', 'complex-numbers']"
61,Continuity of analytic function implies convergence of power series?,Continuity of analytic function implies convergence of power series?,,"Suppose that $f(z)$ is analytic in the unit disk $\Delta:\,|z|<1$. Then $f(z)$ has a Taylor series $\sum=\sum a_nz^n$ in the unit disk. One may assume that $\sum$ has $R=1$ as its radius of convergence for convenience. Question 1 (local version):  If $f(z)$   is continous at some point, say $z=1$, on the unit circle $S^1$, then its Taylor series $\sum$  is  covergent at $z=1$? Question 2 (global version):  Suppose that  $f(z)$   is continous on the closed unit disk $\overline\Delta$.  Is the  Taylor series $\sum$ covergent at every poin of $S^1$? Of course, if the answer to the local version is ""yes"", then so is the answer to the global version. EDIT. It seems that  Question 1 obviously has a negative answer. So I modify it into a new question. Question 3 (New local version):  Suppose that  $f(z)$   is continous at some point, say $z=1$, on the unit circle $S^1$ and the coefficients of $\sum$ satisfies $a_n\to 0$.  Is the Taylor series $\sum$  covergent at $z=1$? Note. If $f$ is analytic at $z=1$, then the answer to Question 3 is yes. This is Fatou's Theorem.","Suppose that $f(z)$ is analytic in the unit disk $\Delta:\,|z|<1$. Then $f(z)$ has a Taylor series $\sum=\sum a_nz^n$ in the unit disk. One may assume that $\sum$ has $R=1$ as its radius of convergence for convenience. Question 1 (local version):  If $f(z)$   is continous at some point, say $z=1$, on the unit circle $S^1$, then its Taylor series $\sum$  is  covergent at $z=1$? Question 2 (global version):  Suppose that  $f(z)$   is continous on the closed unit disk $\overline\Delta$.  Is the  Taylor series $\sum$ covergent at every poin of $S^1$? Of course, if the answer to the local version is ""yes"", then so is the answer to the global version. EDIT. It seems that  Question 1 obviously has a negative answer. So I modify it into a new question. Question 3 (New local version):  Suppose that  $f(z)$   is continous at some point, say $z=1$, on the unit circle $S^1$ and the coefficients of $\sum$ satisfies $a_n\to 0$.  Is the Taylor series $\sum$  covergent at $z=1$? Note. If $f$ is analytic at $z=1$, then the answer to Question 3 is yes. This is Fatou's Theorem.",,['complex-analysis']
62,"Newman's ""Natural proof""(Analytic) of Prime Number Theorem (1980)","Newman's ""Natural proof""(Analytic) of Prime Number Theorem (1980)",,"I am trying to understand this short proof by newmann. I faced some problems while grasping this very proof. Please help me out. 1 . I am not clear, why in step (1) 's proof he says that from unique factorization and the absolute convergence of zeta we have... [here factorization makes sense but where did he use absolute convergence to prove (1).] 2 . Step 2 is clear but where will he use it (may be I will out later as i am still working on next steps. 3 . My major problem is to understand step 3rd's last part that i put it in the red box. please elaborate it. I didn't understand anything in this box. I am still working on the next steps of the proofs. If i get any problem in them i will ask you. Thank you so much in advance.","I am trying to understand this short proof by newmann. I faced some problems while grasping this very proof. Please help me out. 1 . I am not clear, why in step (1) 's proof he says that from unique factorization and the absolute convergence of zeta we have... [here factorization makes sense but where did he use absolute convergence to prove (1).] 2 . Step 2 is clear but where will he use it (may be I will out later as i am still working on next steps. 3 . My major problem is to understand step 3rd's last part that i put it in the red box. please elaborate it. I didn't understand anything in this box. I am still working on the next steps of the proofs. If i get any problem in them i will ask you. Thank you so much in advance.",,"['complex-analysis', 'analysis', 'number-theory', 'prime-numbers', 'analytic-number-theory']"
63,Why are $L$-functions a big deal?,Why are -functions a big deal?,L,"I've been studying modular forms this semester and we did a lot of calculations of $L$-functions, e.g. $L$-functions of Dirichlet-characters and $L$-functions of cusp-forms. But I somehow don't see, why they are considered a big deal. To me it sounds more like a fun fact to say: ""You know, the Riemann-$\zeta$ has analytic continuation."", and I don't even know what to say about $L$-functions of cusp-forms. So why are $L$-functions such a big thing in automorphic forms and analytic number theory? Thanks!","I've been studying modular forms this semester and we did a lot of calculations of $L$-functions, e.g. $L$-functions of Dirichlet-characters and $L$-functions of cusp-forms. But I somehow don't see, why they are considered a big deal. To me it sounds more like a fun fact to say: ""You know, the Riemann-$\zeta$ has analytic continuation."", and I don't even know what to say about $L$-functions of cusp-forms. So why are $L$-functions such a big thing in automorphic forms and analytic number theory? Thanks!",,"['complex-analysis', 'number-theory', 'analytic-number-theory', 'modular-forms', 'automorphic-forms']"
64,Closed form of $\frac{e^{-\frac{\pi}{5}}}{1+\frac{e^{-\pi}}{1+\frac{e^{-2\pi}}{1+\frac{e^{-3\pi}}{1+\ddots}}}}$,Closed form of,\frac{e^{-\frac{\pi}{5}}}{1+\frac{e^{-\pi}}{1+\frac{e^{-2\pi}}{1+\frac{e^{-3\pi}}{1+\ddots}}}},"It is well known that $$\operatorname{R}(-e^{-\pi})=-\cfrac{e^{-\frac{\pi}{5}}}{1-\cfrac{e^{-\pi}}{1+\cfrac{e^{-2\pi}}{1-\cfrac{e^{-3\pi}}{1+\ddots}}}}=\frac{\sqrt{5}-1}{2}-\sqrt{\frac{5-\sqrt{5}}{2}}$$ where $\operatorname{R}$ is the Rogers-Ramanujan continued fraction: $$\operatorname{R}(q)=\cfrac{q^{\frac{1}{5}}}{1+\cfrac{q}{1+\cfrac{q^2}{1+\cfrac{q^3}{1+\ddots}}}},\, q=e^{\pi i\tau}.$$ But I'm interested in $\operatorname{R}(e^{-\pi})$ . Numerically, I checked that it agrees with the root of the following octic equation near $x=\frac{1}{2}$ to at least $16$ decimal places: $$x^8+14x^7+22x^6+22x^5+30x^4-22 x^3+22 x^2-14x+1=0;$$ the root turns out to be equal to $$\frac{\sqrt{5}-1}{2}\frac{\sqrt[4]{5}+\sqrt{2+\sqrt{5}}}{\sqrt{5}+\sqrt{2+\sqrt{5}}}.$$ So is it true that $$\frac{e^{-\frac{\pi}{5}}}{1+\cfrac{e^{-\pi}}{1+\cfrac{e^{-2\pi}}{1+\cfrac{e^{-3\pi}}{1+\ddots}}}}=\frac{\sqrt{5}-1}{2}\frac{\sqrt[4]{5}+\sqrt{2+\sqrt{5}}}{\sqrt{5}+\sqrt{2+\sqrt{5}}}?$$ The $2$ 's and $5$ 's under the square roots seem very suggestive of the nature of $\operatorname{R}$ . Also, how could it be proved in that case? Using $$\frac{1}{\operatorname{R}(q)}-\operatorname{R}(q)=\frac{\left(q^{\frac{1}{5}};q^{\frac{1}{5}}\right)_{\infty}}{q^{\frac{1}{5}}(q^5;q^5)_{\infty}}+1$$ and $$\frac{\eta (e^{-\pi\sqrt{n}})}{\eta \left(e^{-\frac{\pi}{\sqrt{n}}}\right)}=n^{-\frac{1}{4}},\, n\gt 0$$ (where $\eta (q)=q^{\frac{1}{12}}\prod_{n\ge 1}(1-q^{2n})$ is the Dedekind eta function), I've been able to evaluate $\operatorname{R}(-e^{-\pi})$ and $\operatorname{R}(e^{-2\pi})$ , but I don't know how could it be used to evaluate $\operatorname{R}(e^{-\pi})$ . Perhaps something else is necessary. I was inspired by Ramanujan's first letter to Hardy, where Ramanujan's $7\text{th}$ theorem states that $$\cfrac{1}{1+\cfrac{e^{-\pi\sqrt{n}}}{1+\cfrac{e^{-2\pi\sqrt{n}}}{1+\cfrac{e^{-3\pi\sqrt{n}}}{1+\ddots}}}}$$ can be exactly found for any $n\in\mathbb{Q}^{+}$ .","It is well known that where is the Rogers-Ramanujan continued fraction: But I'm interested in . Numerically, I checked that it agrees with the root of the following octic equation near to at least decimal places: the root turns out to be equal to So is it true that The 's and 's under the square roots seem very suggestive of the nature of . Also, how could it be proved in that case? Using and (where is the Dedekind eta function), I've been able to evaluate and , but I don't know how could it be used to evaluate . Perhaps something else is necessary. I was inspired by Ramanujan's first letter to Hardy, where Ramanujan's theorem states that can be exactly found for any .","\operatorname{R}(-e^{-\pi})=-\cfrac{e^{-\frac{\pi}{5}}}{1-\cfrac{e^{-\pi}}{1+\cfrac{e^{-2\pi}}{1-\cfrac{e^{-3\pi}}{1+\ddots}}}}=\frac{\sqrt{5}-1}{2}-\sqrt{\frac{5-\sqrt{5}}{2}} \operatorname{R} \operatorname{R}(q)=\cfrac{q^{\frac{1}{5}}}{1+\cfrac{q}{1+\cfrac{q^2}{1+\cfrac{q^3}{1+\ddots}}}},\, q=e^{\pi i\tau}. \operatorname{R}(e^{-\pi}) x=\frac{1}{2} 16 x^8+14x^7+22x^6+22x^5+30x^4-22 x^3+22 x^2-14x+1=0; \frac{\sqrt{5}-1}{2}\frac{\sqrt[4]{5}+\sqrt{2+\sqrt{5}}}{\sqrt{5}+\sqrt{2+\sqrt{5}}}. \frac{e^{-\frac{\pi}{5}}}{1+\cfrac{e^{-\pi}}{1+\cfrac{e^{-2\pi}}{1+\cfrac{e^{-3\pi}}{1+\ddots}}}}=\frac{\sqrt{5}-1}{2}\frac{\sqrt[4]{5}+\sqrt{2+\sqrt{5}}}{\sqrt{5}+\sqrt{2+\sqrt{5}}}? 2 5 \operatorname{R} \frac{1}{\operatorname{R}(q)}-\operatorname{R}(q)=\frac{\left(q^{\frac{1}{5}};q^{\frac{1}{5}}\right)_{\infty}}{q^{\frac{1}{5}}(q^5;q^5)_{\infty}}+1 \frac{\eta (e^{-\pi\sqrt{n}})}{\eta \left(e^{-\frac{\pi}{\sqrt{n}}}\right)}=n^{-\frac{1}{4}},\, n\gt 0 \eta (q)=q^{\frac{1}{12}}\prod_{n\ge 1}(1-q^{2n}) \operatorname{R}(-e^{-\pi}) \operatorname{R}(e^{-2\pi}) \operatorname{R}(e^{-\pi}) 7\text{th} \cfrac{1}{1+\cfrac{e^{-\pi\sqrt{n}}}{1+\cfrac{e^{-2\pi\sqrt{n}}}{1+\cfrac{e^{-3\pi\sqrt{n}}}{1+\ddots}}}} n\in\mathbb{Q}^{+}","['complex-analysis', 'closed-form', 'continued-fractions', 'modular-function']"
65,Work of Ted Kaczynski,Work of Ted Kaczynski,,"I hope this question is not too crazy sounding, but I was wondering if anyone is familiar with the work of Ted Kaczynski (or even has cited/used it before). After reading in Lars Ahlfors' Complex Analysis and Serge Lang's book by the same name, I became interested in some of the historic results in complex analysis. I know that Kaczynski did work in the field of complex analysis and specifically geometric function theory. From what I have gathered, he was actually rather brilliant as a research mathematician. What in specific did he research and how are his results used today? (If at all). Again, I do not want to know about his political views or his history as the Unabomber. I am only interested in the utility of his mathematics.","I hope this question is not too crazy sounding, but I was wondering if anyone is familiar with the work of Ted Kaczynski (or even has cited/used it before). After reading in Lars Ahlfors' Complex Analysis and Serge Lang's book by the same name, I became interested in some of the historic results in complex analysis. I know that Kaczynski did work in the field of complex analysis and specifically geometric function theory. From what I have gathered, he was actually rather brilliant as a research mathematician. What in specific did he research and how are his results used today? (If at all). Again, I do not want to know about his political views or his history as the Unabomber. I am only interested in the utility of his mathematics.",,"['complex-analysis', 'soft-question', 'math-history']"
66,Why do injective holomorphic functions have nonzero derivative,Why do injective holomorphic functions have nonzero derivative,,"For some open sets $U$, $V$ in the complex plane, let $f:U\rightarrow V$ be an injective holomorphic function. Then $f'(z) \ne 0$ for $z \in U$. Now I don't understand the proof, but here it is from my text. My comments are in italics.  Suppose $f(z_0) = 0$ for some $z_0 \in U$. $f(z) - f(z_0) = a(z - z_0)^k + G(z)$ for all $z$ near $z_0$, with $a \ne 0, k \ge 2.$ Also, $G$ vanishes to order $k+1$ at $z_0$. I'm not clear on what this ""vanishing"" thing means. Maybe it means that $G$ can be expressed as a power series of order $k+1$ around $z_0$. For sufficiently small $w$ we can write $f(z) - f(z_0) - w = F(z) + G(z)$, where $F(z) = a(z - z_0)^k - w$. I'm not sure why we need to have $w$ small. This equation will work for any $w$. Since $|G(z)| \lt |F(z)|$ on a small circle centered at $z_0$, and $F$ has at least two zeros inside that circle, Rouche's theorem implies that $f(z) - f(z_0) - w$ has a least two zeros there. Now I think that $|G(z)| \lt |F(z)|$ can follow simply from the fact that $F$ is a polynomial of degree $k$ while $G$ has degree $k+1$. And the remark about the two zeros can follow from the fact that $F$ must have $k$ zeros in the complex plane. But the first part requires that we consider $z$ only on a small circle. The second part requires that our circle be big enough to capture two zeros. How do we know that we can satisfy both? Since $f'(z) \ne 0$ for for all $z \ne z_0$ sufficiently close to $z_0$, the roots of $f(z) - f(z_0) - w$ are distinct, so $f$ is not injective - a contradiction. I think that the derivative is never zero for values of $z$ other than $z_0$ because otherwise we would have a sequence of zeros limiting towards $z_0$ which would cause our function to be constant which is a contradiction. But again we have the same problem - we can only consider a small circle. The roots of $f$ may lie outside this circle.","For some open sets $U$, $V$ in the complex plane, let $f:U\rightarrow V$ be an injective holomorphic function. Then $f'(z) \ne 0$ for $z \in U$. Now I don't understand the proof, but here it is from my text. My comments are in italics.  Suppose $f(z_0) = 0$ for some $z_0 \in U$. $f(z) - f(z_0) = a(z - z_0)^k + G(z)$ for all $z$ near $z_0$, with $a \ne 0, k \ge 2.$ Also, $G$ vanishes to order $k+1$ at $z_0$. I'm not clear on what this ""vanishing"" thing means. Maybe it means that $G$ can be expressed as a power series of order $k+1$ around $z_0$. For sufficiently small $w$ we can write $f(z) - f(z_0) - w = F(z) + G(z)$, where $F(z) = a(z - z_0)^k - w$. I'm not sure why we need to have $w$ small. This equation will work for any $w$. Since $|G(z)| \lt |F(z)|$ on a small circle centered at $z_0$, and $F$ has at least two zeros inside that circle, Rouche's theorem implies that $f(z) - f(z_0) - w$ has a least two zeros there. Now I think that $|G(z)| \lt |F(z)|$ can follow simply from the fact that $F$ is a polynomial of degree $k$ while $G$ has degree $k+1$. And the remark about the two zeros can follow from the fact that $F$ must have $k$ zeros in the complex plane. But the first part requires that we consider $z$ only on a small circle. The second part requires that our circle be big enough to capture two zeros. How do we know that we can satisfy both? Since $f'(z) \ne 0$ for for all $z \ne z_0$ sufficiently close to $z_0$, the roots of $f(z) - f(z_0) - w$ are distinct, so $f$ is not injective - a contradiction. I think that the derivative is never zero for values of $z$ other than $z_0$ because otherwise we would have a sequence of zeros limiting towards $z_0$ which would cause our function to be constant which is a contradiction. But again we have the same problem - we can only consider a small circle. The roots of $f$ may lie outside this circle.",,['complex-analysis']
67,"If there are entire $G_k$s such that $f=\exp\circ\exp\circ\cdots \circ\exp\circ G_k$ ($k$ times), must $f$ be constant?","If there are entire s such that  ( times), must  be constant?",G_k f=\exp\circ\exp\circ\cdots \circ\exp\circ G_k k f,"I am a French guest and I hope that my English isn't too bad... So here is my issue: I consider an entire function $f$ which satisfies the following property for each complex number $z\in \mathbb{C}$: $\forall ~ k \in \mathbb{N}^*$, there exists an entire function $G_k$ that satisfies $$f(z)=\exp_k(G_k(z))$$ where $\exp_k$ denotes $\exp \circ \exp \circ ...\circ \exp$, $k$-times. In other words, I can take (as many times as I wish) the $\log$ of my function $f$, and it will always give an entire function that doesn't vanish on $\mathbb{C}$. Is $f$ a constant? (surely different from $0$...) Thanks everyone! (PS: This forum is so cool!)","I am a French guest and I hope that my English isn't too bad... So here is my issue: I consider an entire function $f$ which satisfies the following property for each complex number $z\in \mathbb{C}$: $\forall ~ k \in \mathbb{N}^*$, there exists an entire function $G_k$ that satisfies $$f(z)=\exp_k(G_k(z))$$ where $\exp_k$ denotes $\exp \circ \exp \circ ...\circ \exp$, $k$-times. In other words, I can take (as many times as I wish) the $\log$ of my function $f$, and it will always give an entire function that doesn't vanish on $\mathbb{C}$. Is $f$ a constant? (surely different from $0$...) Thanks everyone! (PS: This forum is so cool!)",,"['complex-analysis', 'logarithms', 'exponential-function']"
68,Really: why is the Kelvin transform harmonic?,Really: why is the Kelvin transform harmonic?,,"So, it is a famous fact that if $u:\mathbb{R}^n \to \mathbb{R}$ is an harmonic function, then its Kelvin transform $$ (Ku)(x) := \frac{1}{|x|^{n-2}} u\left(\frac{x}{|x|^2} \right) $$ is harmonic too. Apparently, all the proofs of this fact that I have seen are some variation of ""do the computations and all the terms simplify"" . Moreover, I already did the computation once and so I am not interested in seeing the computations done in some other equivalent form . My point is the following: why, really, is this function harmonic? I don't buy the fact that the first person that discovered this just randomly did the computations with the correct power of the norm in front. For $n=2$ this can be proved very easily by properties of holomorphic functions and indeed there's a geometric motivation, in this case $n=2$ , on why it sends harmonic functions to harmonic functions. But what for general $n$ ? Does anyone have at least some geometric/physical intuition on why $Ku$ is harmonic if $u$ is? I tried many characterizations of harmonic functions like the mean value property or integrating against test functions, and all seem not to bring the result in a clean way. Indeed, at some point, there's always a big computation (being that some tangential Jacobian or Laplacian of composition) that I find basically equivalent to doing the computation from the beginning. I am searching for a proof or at least some motivation on why would someone think that this transformation is the natural candidate to send harmonic functions to harmonic functions.","So, it is a famous fact that if is an harmonic function, then its Kelvin transform is harmonic too. Apparently, all the proofs of this fact that I have seen are some variation of ""do the computations and all the terms simplify"" . Moreover, I already did the computation once and so I am not interested in seeing the computations done in some other equivalent form . My point is the following: why, really, is this function harmonic? I don't buy the fact that the first person that discovered this just randomly did the computations with the correct power of the norm in front. For this can be proved very easily by properties of holomorphic functions and indeed there's a geometric motivation, in this case , on why it sends harmonic functions to harmonic functions. But what for general ? Does anyone have at least some geometric/physical intuition on why is harmonic if is? I tried many characterizations of harmonic functions like the mean value property or integrating against test functions, and all seem not to bring the result in a clean way. Indeed, at some point, there's always a big computation (being that some tangential Jacobian or Laplacian of composition) that I find basically equivalent to doing the computation from the beginning. I am searching for a proof or at least some motivation on why would someone think that this transformation is the natural candidate to send harmonic functions to harmonic functions.","u:\mathbb{R}^n \to \mathbb{R} 
(Ku)(x) := \frac{1}{|x|^{n-2}} u\left(\frac{x}{|x|^2} \right)
 n=2 n=2 n Ku u","['complex-analysis', 'differential-geometry', 'math-history', 'harmonic-analysis', 'harmonic-functions']"
69,how to show image of a non constant entire function is dense in $\mathbb{C}$?,how to show image of a non constant entire function is dense in ?,\mathbb{C},how to show image of a non constant entire function is dense in $\mathbb{C}$? is there any smallest proof? I have seen this as a theorem in some books but I want some elementary proof.,how to show image of a non constant entire function is dense in $\mathbb{C}$? is there any smallest proof? I have seen this as a theorem in some books but I want some elementary proof.,,['complex-analysis']
70,"the zeros of $\sin(z)$, where $z$ is a complex number","the zeros of , where  is a complex number",\sin(z) z,"How do I find the zeros of $\sin(z)$, where $z$ is a complex number? I know that along the real line we have zeros along $k\pi$, where $k$ is an integer. But what about the rest of the plane? The taylor series: $$ \sum_{n=0}^{\infty}(-1)^n \dfrac{z^{2n+1}}{(2n+1)!}, $$ doesn't really tell me that much. How do I find the other zeros?","How do I find the zeros of $\sin(z)$, where $z$ is a complex number? I know that along the real line we have zeros along $k\pi$, where $k$ is an integer. But what about the rest of the plane? The taylor series: $$ \sum_{n=0}^{\infty}(-1)^n \dfrac{z^{2n+1}}{(2n+1)!}, $$ doesn't really tell me that much. How do I find the other zeros?",,['complex-analysis']
71,"Mini Mandelbrots, are they exact copies?","Mini Mandelbrots, are they exact copies?",,"( This one was found by magnifying 280,000,000 times. ) In popular ""zoom movies"" of the Mandelbrot set the last image is often what appears to be an exact copy of the original set. This is always very moving as it indicates that after a long an fantastic journey or transformation we are in many ways still in the same place. Some writers have compared this self similarity to the way that history can repeat itself. So, you see, the concept is very compelling even to non-mathematicians. I would like to know if such ""Mini Mandelbrots"" as they have been called by a few mathematics bloggers are ever perfectly isomorphic to the whole. The quote from the 2nd webpage indicates that at least some of them are not , but not all ""Mini Mandelbrots"" are found in the location described. Why are there mini sets in the Mandelbrot Set? ANATOMY OF THE MANDELBROT SET At the top and bottom of the main body are smaller bulbs with curved lines including a mini Mandelbrot set. More small bulbs of various descending sizes line the outer edge of the main body, curving inward toward the center and getting ever smaller. Between each bulb are smaller ones, and so on. On each bulb are more smaller bulbs, with more in between, to infinity. That means no matter how much you zoom in and magnify a bulb, you will always find more, smaller ones, each slightly different than the others. The same applies to all of the mini Mandelbrot sets. If we can't find a perfect copy are some copies more perfect than others? How would one measure such a thing?","( This one was found by magnifying 280,000,000 times. ) In popular ""zoom movies"" of the Mandelbrot set the last image is often what appears to be an exact copy of the original set. This is always very moving as it indicates that after a long an fantastic journey or transformation we are in many ways still in the same place. Some writers have compared this self similarity to the way that history can repeat itself. So, you see, the concept is very compelling even to non-mathematicians. I would like to know if such ""Mini Mandelbrots"" as they have been called by a few mathematics bloggers are ever perfectly isomorphic to the whole. The quote from the 2nd webpage indicates that at least some of them are not , but not all ""Mini Mandelbrots"" are found in the location described. Why are there mini sets in the Mandelbrot Set? ANATOMY OF THE MANDELBROT SET At the top and bottom of the main body are smaller bulbs with curved lines including a mini Mandelbrot set. More small bulbs of various descending sizes line the outer edge of the main body, curving inward toward the center and getting ever smaller. Between each bulb are smaller ones, and so on. On each bulb are more smaller bulbs, with more in between, to infinity. That means no matter how much you zoom in and magnify a bulb, you will always find more, smaller ones, each slightly different than the others. The same applies to all of the mini Mandelbrot sets. If we can't find a perfect copy are some copies more perfect than others? How would one measure such a thing?",,"['complex-analysis', 'fractals']"
72,Type of singularity of $\log z$ at $z=0$,Type of singularity of  at,\log z z=0,"What type of singularity is $z=0$ for $\log z$ (any branch)? What is the Laurent series for $\log z$ centered at 0, if exist? If the Laurent series has the form $\sum_{k=-\infty}^{\infty} a_kx^k$, then certainly among $a_{-1},a_{-2},...,a_{-j},...$, at least one is nonzero (or otherwise $\log z$ would be analytic at $0$). Since $\lim_{z\to 0}z\log z=0$, we must have $a_{-j}=0$ for all $j>0$, a contradiction. Hence the Laurent series centered at $0$ cannot exist. Is the singularity at $0$ a pole? If so, what is its order? Thanks.","What type of singularity is $z=0$ for $\log z$ (any branch)? What is the Laurent series for $\log z$ centered at 0, if exist? If the Laurent series has the form $\sum_{k=-\infty}^{\infty} a_kx^k$, then certainly among $a_{-1},a_{-2},...,a_{-j},...$, at least one is nonzero (or otherwise $\log z$ would be analytic at $0$). Since $\lim_{z\to 0}z\log z=0$, we must have $a_{-j}=0$ for all $j>0$, a contradiction. Hence the Laurent series centered at $0$ cannot exist. Is the singularity at $0$ a pole? If so, what is its order? Thanks.",,"['complex-analysis', 'analysis', 'complex-numbers', 'laurent-series', 'singularity-theory']"
73,roots of $f(z)=z^4+8z^3+3z^2+8z+3=0$ in the right half plane,roots of  in the right half plane,f(z)=z^4+8z^3+3z^2+8z+3=0,"This is a question in Ahlfors in the section on the argument principle: How many roots of the equation $f(z)=z^4+8z^3+3z^2+8z+3=0$ lie in the right half plane? He gives a hint that we should ""sketch the image of the imaginary axis and apply the argument principle to a large half disk."" Since $f$ is an entire function, I think I understand that the argument principle tells us that for any closed curve $\gamma$ in $\mathbb{C}$, the winding number of $f(\gamma)$ around 0 is equal to the number of zeros of $f$ contained inside $\gamma$. How would you go about actually applying the hint though? I am having trouble figuring out what the image of a large half disk under $f$ would look like.","This is a question in Ahlfors in the section on the argument principle: How many roots of the equation $f(z)=z^4+8z^3+3z^2+8z+3=0$ lie in the right half plane? He gives a hint that we should ""sketch the image of the imaginary axis and apply the argument principle to a large half disk."" Since $f$ is an entire function, I think I understand that the argument principle tells us that for any closed curve $\gamma$ in $\mathbb{C}$, the winding number of $f(\gamma)$ around 0 is equal to the number of zeros of $f$ contained inside $\gamma$. How would you go about actually applying the hint though? I am having trouble figuring out what the image of a large half disk under $f$ would look like.",,['complex-analysis']
74,When can't a real definite integral be evaluated using contour integration?,When can't a real definite integral be evaluated using contour integration?,,"Some older complex analysis textbooks state that  $ \displaystyle \int_{0}^{\infty}e^{-x^{2}} \ dx$ can't be evaluated using contour integration. But that's now known not to be true, which makes me  wonder if you can ever definitively state that a particular real definite integral can't be evaluated using contour integration. Edit: (t.b.) a famous instance of the above claim is in Watson, Complex Integration and Cauchy's theorem (1914), page 79:","Some older complex analysis textbooks state that  $ \displaystyle \int_{0}^{\infty}e^{-x^{2}} \ dx$ can't be evaluated using contour integration. But that's now known not to be true, which makes me  wonder if you can ever definitively state that a particular real definite integral can't be evaluated using contour integration. Edit: (t.b.) a famous instance of the above claim is in Watson, Complex Integration and Cauchy's theorem (1914), page 79:",,"['complex-analysis', 'contour-integration']"
75,Demonstration that 0 = 1 [duplicate],Demonstration that 0 = 1 [duplicate],,"This question already has answers here : Why this proof $0=1$ is wrong?(breakfast joke) (3 answers) Closed 9 years ago . I have been proposed this enigma, but can't solve it. So here it is: $$\begin{align} e^{2 \pi i n} &= 1 \quad \forall n \in \mathbb{N} && (\times e) \tag{0} \\ e^{2 \pi i n + 1} &= e &&(^{1 + 2 \pi i n})\ \text{(raising both sides to the $2\pi in+1$ power)} \tag{1} \\ e^{(2 \pi i n + 1)(2 \pi i n + 1)} &= e^{(2 \pi i n + 1)} = e &&(\text{because of (1)}) \tag{2} \\ e^{1 + 4 \pi i n - 4 \pi^2 n^2} &= e && (\div e) \tag{3} \\ e^{4 \pi i n - 4 \pi^2 n^2} &= 1 &&(n \rightarrow +\infty) \tag{4} \\ 0 &= 1 &&(?) \tag{5} \end{align}$$ So the question is: where is the error?","This question already has answers here : Why this proof $0=1$ is wrong?(breakfast joke) (3 answers) Closed 9 years ago . I have been proposed this enigma, but can't solve it. So here it is: $$\begin{align} e^{2 \pi i n} &= 1 \quad \forall n \in \mathbb{N} && (\times e) \tag{0} \\ e^{2 \pi i n + 1} &= e &&(^{1 + 2 \pi i n})\ \text{(raising both sides to the $2\pi in+1$ power)} \tag{1} \\ e^{(2 \pi i n + 1)(2 \pi i n + 1)} &= e^{(2 \pi i n + 1)} = e &&(\text{because of (1)}) \tag{2} \\ e^{1 + 4 \pi i n - 4 \pi^2 n^2} &= e && (\div e) \tag{3} \\ e^{4 \pi i n - 4 \pi^2 n^2} &= 1 &&(n \rightarrow +\infty) \tag{4} \\ 0 &= 1 &&(?) \tag{5} \end{align}$$ So the question is: where is the error?",,"['complex-analysis', 'limits', 'fake-proofs', 'natural-numbers', 'paradoxes']"
76,Prove that $\sum\limits_{k=0}^{n-1}\dfrac{1}{\cos^2\frac{\pi k}{n}}=n^2$ for odd $n$,Prove that  for odd,\sum\limits_{k=0}^{n-1}\dfrac{1}{\cos^2\frac{\pi k}{n}}=n^2 n,"In old popular science magazine for school students I've seen problem Prove that $\quad $   $\dfrac{1}{\cos^2 20^\circ} +  \dfrac{1}{\cos^2 40^\circ} +  \dfrac{1}{\cos^2 60^\circ} +  \dfrac{1}{\cos^2 80^\circ} = 40. $ How to prove more general identity: $$ \begin{array}{|c|} \hline \\ \sum\limits_{k=0}^{n-1}\dfrac{1}{\cos^2\frac{\pi k}{n}}=n^2 \\ \hline \end{array} ,  \qquad \mbox{ where } \ n \ \mbox{ is odd.}$$","In old popular science magazine for school students I've seen problem Prove that $\quad $   $\dfrac{1}{\cos^2 20^\circ} +  \dfrac{1}{\cos^2 40^\circ} +  \dfrac{1}{\cos^2 60^\circ} +  \dfrac{1}{\cos^2 80^\circ} = 40. $ How to prove more general identity: $$ \begin{array}{|c|} \hline \\ \sum\limits_{k=0}^{n-1}\dfrac{1}{\cos^2\frac{\pi k}{n}}=n^2 \\ \hline \end{array} ,  \qquad \mbox{ where } \ n \ \mbox{ is odd.}$$",,"['complex-analysis', 'trigonometry', 'complex-numbers', 'summation']"
77,"Suppose $f$ and $g$ are entire functions, and $|f(z)| \leq |g(z)|$ for all $z \in \mathbb{C}$, Prove that $f(z)=cg(z)$.","Suppose  and  are entire functions, and  for all , Prove that .",f g |f(z)| \leq |g(z)| z \in \mathbb{C} f(z)=cg(z),"Suppose $f$ and $g$ are entire functions, and $|f(z)| \leq |g(z)|$ for all $z \in \mathbb{C}$, Prove that $f(z)=cg(z)$. My try : I consider $h(z)=\frac{f(z)}{g(z)}$. If I prove that $h(z)$ is entire, using the fact that $|h(z)|\leq 1$, the result follows immediately from Liouville's theorem. To prove that $h(z)$ is entire, I have to prove that $h(z)$ has removable singularities at possible zeros of $g(z)$. Suppose $g(z_0)=0 \Rightarrow |g(z_0)|=0 \Rightarrow |f(z_0)|=0$, BUT what if $z_0$ be a root of of order $k$ of $g(z)$ (i.e. $f^{(n)}(z_0) = 0 $ for all $ 0 \leq n \leq k-1$) and be a zero of order $m$ of $f(z)$ and $k>m$ ? And if that's not the case, how to rigorously show that $z_0$ is a removable singularity ? Thank you all in advance.","Suppose $f$ and $g$ are entire functions, and $|f(z)| \leq |g(z)|$ for all $z \in \mathbb{C}$, Prove that $f(z)=cg(z)$. My try : I consider $h(z)=\frac{f(z)}{g(z)}$. If I prove that $h(z)$ is entire, using the fact that $|h(z)|\leq 1$, the result follows immediately from Liouville's theorem. To prove that $h(z)$ is entire, I have to prove that $h(z)$ has removable singularities at possible zeros of $g(z)$. Suppose $g(z_0)=0 \Rightarrow |g(z_0)|=0 \Rightarrow |f(z_0)|=0$, BUT what if $z_0$ be a root of of order $k$ of $g(z)$ (i.e. $f^{(n)}(z_0) = 0 $ for all $ 0 \leq n \leq k-1$) and be a zero of order $m$ of $f(z)$ and $k>m$ ? And if that's not the case, how to rigorously show that $z_0$ is a removable singularity ? Thank you all in advance.",,['complex-analysis']
78,$\lambda-z-e^{-z}=0$ has one solution in the right half plane,has one solution in the right half plane,\lambda-z-e^{-z}=0,"Let $\lambda > 1$ , want to show that the equation $$\lambda-z-e^{-z}=0$$ has exactly one solution in the right half plane $\{z:Re(z)>0\}$. Moreover, the solution must be real. I tried to use Rouche's theorem on $g(z)=\lambda - z$ and $f(z)=e^{-z}$ to get that the number of zeros of $f+g$ and the number of zeros of $g(z)$ is the same, and since $g(z)$ has only one solution then the equation about must also have one solution the problem is I don't know how to choose the correct curve $\gamma$ such that this will work. for the second part I used the IVT to show that $\lambda -x-e^{-x}$ has a zero in  $(0,\lambda)$ to conclude that the solution is real. is this acceptable? Thank you for your help.","Let $\lambda > 1$ , want to show that the equation $$\lambda-z-e^{-z}=0$$ has exactly one solution in the right half plane $\{z:Re(z)>0\}$. Moreover, the solution must be real. I tried to use Rouche's theorem on $g(z)=\lambda - z$ and $f(z)=e^{-z}$ to get that the number of zeros of $f+g$ and the number of zeros of $g(z)$ is the same, and since $g(z)$ has only one solution then the equation about must also have one solution the problem is I don't know how to choose the correct curve $\gamma$ such that this will work. for the second part I used the IVT to show that $\lambda -x-e^{-x}$ has a zero in  $(0,\lambda)$ to conclude that the solution is real. is this acceptable? Thank you for your help.",,"['complex-analysis', 'roots']"
79,Find all entire $f$ such that $f(f(z))=z$.,Find all entire  such that .,f f(f(z))=z,"Suppose $f:\mathbb{C}\to \mathbb{C}$ is entire. If $f(f(z))=z$, find all such $f$. Can we find $f$ such that $f(f(z))=z^2$? How about $f(f(z))=e^z$? Ideas: For #1, we can show that $f$ must be a bijection, since $f$ failing to be either injective or surjective quickly leads to a contradiction. My intuition tells me that $z\mapsto -z$ should be the only such (non-identity) function. We know $f'(z)\neq 0$ for all $z$ (or else there would be an area where $f$ were not injective). Therefore $1/f'(z)$ is an entire function. Taking derivatives of both sides, we get $f'(f(z))f'(z) = 1$, or $f'(f(z))=1/f'(z)$. $1/f(z)$ should be analytic everywhere, except for a simple pole at $z=f(0)$. Let $\max_{|z|=1}|f(z)-f(0)|=R_1$, then Schwarz's lemma implies that $f'(0)\leq R_1$. Let $\max_{|z|=1}|f(z+f(0))|=R_2$, then Schwarz's lemma implies $f'(f(0))\leq R_2$. But $f'(f(0)) = 1/f'(0)$, so $$\frac1R_2\leq f'(0)\leq R_1.$$ Trying to put all this together... Update: I've got #1. Consider the singularity of $f$ at infinity. If it is removable, then $f$ is a constant by Liouville's theorem. If it is a pole, then $f$ is a polynomial. If it is an essential singularity, then by the Big Picard theorem, $f$ cannot be injective since $f$ will take every value, except perhaps one, in a neighborhood of infinity. Therefore $f$ is a polynomial, and since $f'\neq 0$, we have $f=az+b$ for $a\neq 0$. Now we have $a(az+b)+b = z$, giving that the involutions are $z\mapsto z$ and $z\mapsto -z+c$ for $c\in \mathbb{C}$. Update 2: I think I've got #2 as follows: Suppose $f$ entire such that $f(f(z))=z^2$. Since $z\mapsto z^2$ is surjective, we must have $f$ surjective. Now $2z=f'(f(z))f'(z)$. So we know that $f'(f(z))f'(z)$ is zero only at $z=0$. Either $f'(f(0))=0$ or $f'(0)=0$. Suppose the former holds and not the latter. That is impossible, since then $f(f(z))$ would have zero derivative at two points, zero and $f(0)$. So we know $f'(0)=0$. Since $f$ is surjective, some point must map to zero. It cannot be a nonzero point $a$, because then $f(f(z))$ would have zero derivative at $a$. This implies that zero is the unique point mapping under $f$ to zero. Now $$(z^2)'' = 2 = f''(f(z))f'(z) + f'(f(z))f''(z)$$ and the right hand side is zero at $z=0$, a contradiction. Update 3: Here's what I've got for #3 so far: Note $e^z$ is nowhere zero. So if $f:w\mapsto 0$ then $w\not\in \text{Rg}(f)$. But only $0$ is not in $\text{Rg}(f)$. So only zero could possibly map to zero, but it does not, because $f(f(0))\neq 0$. So $f$ is nonzero. $f'$ is also nonzero since $$(e^z)'=e^z=f'(f(z))f'(z).$$ $f$ should have an essential singularity at infinity.","Suppose $f:\mathbb{C}\to \mathbb{C}$ is entire. If $f(f(z))=z$, find all such $f$. Can we find $f$ such that $f(f(z))=z^2$? How about $f(f(z))=e^z$? Ideas: For #1, we can show that $f$ must be a bijection, since $f$ failing to be either injective or surjective quickly leads to a contradiction. My intuition tells me that $z\mapsto -z$ should be the only such (non-identity) function. We know $f'(z)\neq 0$ for all $z$ (or else there would be an area where $f$ were not injective). Therefore $1/f'(z)$ is an entire function. Taking derivatives of both sides, we get $f'(f(z))f'(z) = 1$, or $f'(f(z))=1/f'(z)$. $1/f(z)$ should be analytic everywhere, except for a simple pole at $z=f(0)$. Let $\max_{|z|=1}|f(z)-f(0)|=R_1$, then Schwarz's lemma implies that $f'(0)\leq R_1$. Let $\max_{|z|=1}|f(z+f(0))|=R_2$, then Schwarz's lemma implies $f'(f(0))\leq R_2$. But $f'(f(0)) = 1/f'(0)$, so $$\frac1R_2\leq f'(0)\leq R_1.$$ Trying to put all this together... Update: I've got #1. Consider the singularity of $f$ at infinity. If it is removable, then $f$ is a constant by Liouville's theorem. If it is a pole, then $f$ is a polynomial. If it is an essential singularity, then by the Big Picard theorem, $f$ cannot be injective since $f$ will take every value, except perhaps one, in a neighborhood of infinity. Therefore $f$ is a polynomial, and since $f'\neq 0$, we have $f=az+b$ for $a\neq 0$. Now we have $a(az+b)+b = z$, giving that the involutions are $z\mapsto z$ and $z\mapsto -z+c$ for $c\in \mathbb{C}$. Update 2: I think I've got #2 as follows: Suppose $f$ entire such that $f(f(z))=z^2$. Since $z\mapsto z^2$ is surjective, we must have $f$ surjective. Now $2z=f'(f(z))f'(z)$. So we know that $f'(f(z))f'(z)$ is zero only at $z=0$. Either $f'(f(0))=0$ or $f'(0)=0$. Suppose the former holds and not the latter. That is impossible, since then $f(f(z))$ would have zero derivative at two points, zero and $f(0)$. So we know $f'(0)=0$. Since $f$ is surjective, some point must map to zero. It cannot be a nonzero point $a$, because then $f(f(z))$ would have zero derivative at $a$. This implies that zero is the unique point mapping under $f$ to zero. Now $$(z^2)'' = 2 = f''(f(z))f'(z) + f'(f(z))f''(z)$$ and the right hand side is zero at $z=0$, a contradiction. Update 3: Here's what I've got for #3 so far: Note $e^z$ is nowhere zero. So if $f:w\mapsto 0$ then $w\not\in \text{Rg}(f)$. But only $0$ is not in $\text{Rg}(f)$. So only zero could possibly map to zero, but it does not, because $f(f(0))\neq 0$. So $f$ is nonzero. $f'$ is also nonzero since $$(e^z)'=e^z=f'(f(z))f'(z).$$ $f$ should have an essential singularity at infinity.",,['complex-analysis']
80,"""Orientation"" of $\zeta$ zeroes on the critical line.","""Orientation"" of  zeroes on the critical line.",\zeta,"I am pretty ignorant about complex analysis so please forgive my lack of terminology. I saw a pretty picture (posted below) of the behavior of the Riemann zeta function along the critical line. What I noticed is that the function takes on negative imaginary values ""just before"" the zeros and positive imaginary values ""just after"" them. I would describe these as having the same ""positive orientation"" in that sense. (EDIT: Actually not in that sense; if the zeros were on the other side of the loops they would go from positive to negative but still have the same ""orientation"" inutitively. So I'm not as sure as I thought I was that I could formalize this) A couple of obvious questions spring to mind: 1) Does this keep happening? The behavior going on at the beginning shows one way in which it might deviate from a spiral shape, where it seems to twist itself into the opposite orientation. 2) Is this interesting? Obviously this can only tell us anything about zeros on the critical line so it's not saying anything interesting for the big problem :) But a ""yes"" answer to this question might be something like: there is a more useful notion of ""orientation of a zero"" which looks at a whole neighborhood of the zero. Or: there are function where a similar restriction and analysis tells us something interesting about some structure the function possesses.","I am pretty ignorant about complex analysis so please forgive my lack of terminology. I saw a pretty picture (posted below) of the behavior of the Riemann zeta function along the critical line. What I noticed is that the function takes on negative imaginary values ""just before"" the zeros and positive imaginary values ""just after"" them. I would describe these as having the same ""positive orientation"" in that sense. (EDIT: Actually not in that sense; if the zeros were on the other side of the loops they would go from positive to negative but still have the same ""orientation"" inutitively. So I'm not as sure as I thought I was that I could formalize this) A couple of obvious questions spring to mind: 1) Does this keep happening? The behavior going on at the beginning shows one way in which it might deviate from a spiral shape, where it seems to twist itself into the opposite orientation. 2) Is this interesting? Obviously this can only tell us anything about zeros on the critical line so it's not saying anything interesting for the big problem :) But a ""yes"" answer to this question might be something like: there is a more useful notion of ""orientation of a zero"" which looks at a whole neighborhood of the zero. Or: there are function where a similar restriction and analysis tells us something interesting about some structure the function possesses.",,"['complex-analysis', 'riemann-zeta']"
81,Existence of an holomorphic function,Existence of an holomorphic function,,"Is there a simple way to prove this fact : For all holomorphic functions $f : \mathbb C \to \mathbb C$, there is an holomorphic function $\psi : \mathbb C \to \mathbb C$ such that $$\psi(z+1) = \psi(z) + f(z) $$ The solution I know use Galois covering spaces and summand of automorphy. Thanks in advance.","Is there a simple way to prove this fact : For all holomorphic functions $f : \mathbb C \to \mathbb C$, there is an holomorphic function $\psi : \mathbb C \to \mathbb C$ such that $$\psi(z+1) = \psi(z) + f(z) $$ The solution I know use Galois covering spaces and summand of automorphy. Thanks in advance.",,[]
82,A conformal mapping onto a region bounded by convex contours (Ahlfors),A conformal mapping onto a region bounded by convex contours (Ahlfors),,"I want to solve the following exercise (from Ahlfors' text, page 261) *3. Using Ex. 2, show that $p + q$ maps $\Omega$ in a one-to-one manner onto a region bounded by convex contours. Comments: (i) A closed curve is said to be convex if it intersects every straight line at most twice. (ii) To prove that the image of $C_k$ under $p+q$ is convex we need only show that for every $\alpha$ the function $\Re (p + q)e^{i \alpha}$ takes no value more than twice on $C_k$. But $\Re (p +q)e^{i\alpha}$ differs from $\Re (q \cos \alpha +ip \sin \alpha)$ only by a constant, and the desired conclusion follows by the properties of the mapping function in Ex. 2. (iii) Finally, the argument principle can be used to show that the images of the contours $C_k$ have winding number $0$ with respect to all values of $p+q$. This implies, in particular, that the convex curves lie outside of each other. As a reference, exercise 2 was ${}$2. Show that the function $e^{-i \alpha}(q \cos \alpha +ip \sin \alpha)$ maps $\Omega$ onto a region bounded by inclined slits. Here, $\Omega$ is a a region of finite connectivity, bounded by analytic curves $C_1,C_2, \dots,C_n$. $p: \Omega \to \hat{\mathbb C}$ is a complex meromorphic function, determined by Its only pole is a simple one at $z_0$, with residue 1. Its real part is constant on each of the contours $C_k$. $q: \Omega \to \hat{\mathbb C}$ is similarly determined except its imaginary part is constant on the contours. I've managed to solve Ex. 2 completely, and regarding Ex. 3 I'm stuck at step (iii). Given that the contours $C_k \subset \partial \Omega$ are mapped onto convex contours, how can I prove that all of their images have winding number zero with respect to all values of $p+q$? I guess that the next step is observing that for any $w_0$ taken by $p+q$ we have $$\frac{1}{2 \pi i} \sum_{k=1}^N \oint_{C_k} \frac{p'(z)+q'(z)}{p(z)+q(z)-w_0} \mathrm{d} z=N-P=\text{Ind} \left[(p+q)(\sum_{k=1}^N C_k),w_0 \right] $$ Where $N,P$ are the number of zeros,poles (respectively) of the denominator of the integrand in $\Omega$. However, I can't see how to make progress from here. Any help is greatly appreciated! Thanks! EDIT: Since my goal is determining the winding number using the argument principle, I believe I should try to evaluate $N-P$: Well, $P=1$ since $p+q-w_0$ has a single simple pole at $z_0 \in \Omega$. And if $w_0$ is taken by $p+q$ then $N \geq 1$. I still cannot see why $N$ can't be strictly greater than $1$ - and even if it isn't, this is only the winding number of the image of the entire boundary, and not of its individual components as asked.","I want to solve the following exercise (from Ahlfors' text, page 261) *3. Using Ex. 2, show that $p + q$ maps $\Omega$ in a one-to-one manner onto a region bounded by convex contours. Comments: (i) A closed curve is said to be convex if it intersects every straight line at most twice. (ii) To prove that the image of $C_k$ under $p+q$ is convex we need only show that for every $\alpha$ the function $\Re (p + q)e^{i \alpha}$ takes no value more than twice on $C_k$. But $\Re (p +q)e^{i\alpha}$ differs from $\Re (q \cos \alpha +ip \sin \alpha)$ only by a constant, and the desired conclusion follows by the properties of the mapping function in Ex. 2. (iii) Finally, the argument principle can be used to show that the images of the contours $C_k$ have winding number $0$ with respect to all values of $p+q$. This implies, in particular, that the convex curves lie outside of each other. As a reference, exercise 2 was ${}$2. Show that the function $e^{-i \alpha}(q \cos \alpha +ip \sin \alpha)$ maps $\Omega$ onto a region bounded by inclined slits. Here, $\Omega$ is a a region of finite connectivity, bounded by analytic curves $C_1,C_2, \dots,C_n$. $p: \Omega \to \hat{\mathbb C}$ is a complex meromorphic function, determined by Its only pole is a simple one at $z_0$, with residue 1. Its real part is constant on each of the contours $C_k$. $q: \Omega \to \hat{\mathbb C}$ is similarly determined except its imaginary part is constant on the contours. I've managed to solve Ex. 2 completely, and regarding Ex. 3 I'm stuck at step (iii). Given that the contours $C_k \subset \partial \Omega$ are mapped onto convex contours, how can I prove that all of their images have winding number zero with respect to all values of $p+q$? I guess that the next step is observing that for any $w_0$ taken by $p+q$ we have $$\frac{1}{2 \pi i} \sum_{k=1}^N \oint_{C_k} \frac{p'(z)+q'(z)}{p(z)+q(z)-w_0} \mathrm{d} z=N-P=\text{Ind} \left[(p+q)(\sum_{k=1}^N C_k),w_0 \right] $$ Where $N,P$ are the number of zeros,poles (respectively) of the denominator of the integrand in $\Omega$. However, I can't see how to make progress from here. Any help is greatly appreciated! Thanks! EDIT: Since my goal is determining the winding number using the argument principle, I believe I should try to evaluate $N-P$: Well, $P=1$ since $p+q-w_0$ has a single simple pole at $z_0 \in \Omega$. And if $w_0$ is taken by $p+q$ then $N \geq 1$. I still cannot see why $N$ can't be strictly greater than $1$ - and even if it isn't, this is only the winding number of the image of the entire boundary, and not of its individual components as asked.",,"['complex-analysis', 'analysis', 'conformal-geometry']"
83,How large must $f(f(z))-z^2$ be in the unit disk?,How large must  be in the unit disk?,f(f(z))-z^2,"This is a follow-up question to $|f(f(z))-z^2|$ must be large somewhere in the disc $\mathbb{D}$? , where the following was proven: Let $f$ be holomorphic in the unit disk $\Bbb D$ with $f(0) = 0$ and $|f(z)| < 1$ for all $z \in \Bbb D$ . Then $$ \tag{*}\sup_{z \in \Bbb D} |f(f(z))-z^2| \geq \frac 14 \, .$$ The given proofs work by assuming the contrary, and applying Parseval's identity or the Schwarz-Pick theorem to $F(z) = 4(f(f(z))-z^2)$ . This leads to inequalities for the first Taylor coefficients of $f$ which can not be satisfied. Inspection of those proofs shows that the used inequalities are not sharp, which leads to the conjecture that the number $1/4$ in $(*)$ can be replaced by a larger number. Therefore I am curious what the best possible lower bound in $(*)$ would be. Let $$  {\cal F} = \{ f: \Bbb D \to \Bbb D \text{ holomorphic}, \, f(0) = 0 \} $$ be the family of functions to which the Schwarz lemma can be applied. For $f \in \cal F$ define $$  S(f) = \sup_{z \in \Bbb D} |f(f(z))-z^2| \, , $$ and finally set $$  C = \inf_{f \in \cal F} S(f) \, . $$ We know that $C \ge 1/4$ , and the question is if better estimates or the exact value can be computed. What I can show is that $$ \tag{**}  \boxed{\frac 12 \le C \le 1 \, .} $$ The upper bound $C \le 1$ comes from choosing $f(z) = 0$ . For the lower bound one proceeds as in the linked question above, but with more careful estimates. Assume that $f \in \cal F$ , $f(z) = az + bz^2 + \cdots$ , and $$ \sup_{z \in \Bbb D} |f(f(z))-z^2| < 1/2 \, .$$ Then $|a|^2 + b \le 1$ and \begin{align} 1 &\ge 4 |a|^4 + 2 |a(a+1)b - 1| \\&\ge 4 |a|^4 + 2\left(1 - |a|(|a|+1)(1-|a|^2)\right) \\&= 6|a|^4+2|a|^3-2|a|^2-2|a|+2.\end{align} On the other hand, plotting the right-hand side as a function of $|a| \in [0, 1]$ shows that it is always strictly larger than one: It is also confirmed by WolframAlpha that the minimum of the right-hand side is $\approx 1.1185 > 1$ . This is a contradiction and completes the proof that the supremum is at least $1/2$ . My question: Are there better bounds for $C$ , or can the exact value be computed?","This is a follow-up question to $|f(f(z))-z^2|$ must be large somewhere in the disc $\mathbb{D}$? , where the following was proven: Let be holomorphic in the unit disk with and for all . Then The given proofs work by assuming the contrary, and applying Parseval's identity or the Schwarz-Pick theorem to . This leads to inequalities for the first Taylor coefficients of which can not be satisfied. Inspection of those proofs shows that the used inequalities are not sharp, which leads to the conjecture that the number in can be replaced by a larger number. Therefore I am curious what the best possible lower bound in would be. Let be the family of functions to which the Schwarz lemma can be applied. For define and finally set We know that , and the question is if better estimates or the exact value can be computed. What I can show is that The upper bound comes from choosing . For the lower bound one proceeds as in the linked question above, but with more careful estimates. Assume that , , and Then and On the other hand, plotting the right-hand side as a function of shows that it is always strictly larger than one: It is also confirmed by WolframAlpha that the minimum of the right-hand side is . This is a contradiction and completes the proof that the supremum is at least . My question: Are there better bounds for , or can the exact value be computed?","f \Bbb D f(0) = 0 |f(z)| < 1 z \in \Bbb D  \tag{*}\sup_{z \in \Bbb D} |f(f(z))-z^2| \geq \frac 14 \, . F(z) = 4(f(f(z))-z^2) f 1/4 (*) (*) 
 {\cal F} = \{ f: \Bbb D \to \Bbb D \text{ holomorphic}, \, f(0) = 0 \}
 f \in \cal F 
 S(f) = \sup_{z \in \Bbb D} |f(f(z))-z^2| \, ,
 
 C = \inf_{f \in \cal F} S(f) \, .
 C \ge 1/4  \tag{**}
 \boxed{\frac 12 \le C \le 1 \, .}
 C \le 1 f(z) = 0 f \in \cal F f(z) = az + bz^2 + \cdots  \sup_{z \in \Bbb D} |f(f(z))-z^2| < 1/2 \, . |a|^2 + b \le 1 \begin{align}
1 &\ge 4 |a|^4 + 2 |a(a+1)b - 1| \\&\ge 4 |a|^4 + 2\left(1 - |a|(|a|+1)(1-|a|^2)\right) \\&= 6|a|^4+2|a|^3-2|a|^2-2|a|+2.\end{align} |a| \in [0, 1] \approx 1.1185 > 1 1/2 C","['complex-analysis', 'inequality']"
84,Refining my knowledge of the imaginary number,Refining my knowledge of the imaginary number,,"So I am about halfway through complex analysis (using Churchill amd Brown's book) right now.  I began thinking some more about the nature and behavior of $i$ and ran into some confusion.  I have seen the definition of $i$ in two different forms; $i = \sqrt{-1} $ and $i^2 = -1$.  Now I know that these two statements are not equivalent, so I am confused as to which is the 'correct' definition.  I see quite frequently that the first form is a common mistake, but then again Wolfram Math World says otherwise.  So my questions are: What is the 'correct' definition of $i$ and why?  Or are both definitions correct and you can view the first one as a principal branch? It seems that if we are treating $i$ as the number with the property $i^2 = -1$, it is implied that we are treating $i$ as a concept and not necessarily as a ""quantity""? If we are indeed treating $i$ as a concept rather than a ""quantity"", how would things such as $i^i$ and other equations/expressions involving $i$ be viewed?  How would such an equation have value if we treat $i$ like a concept? I've checked around on the various imaginary number posts on this site, so please don't mark this as a duplicate.  My questions are different than those that have already been asked.","So I am about halfway through complex analysis (using Churchill amd Brown's book) right now.  I began thinking some more about the nature and behavior of $i$ and ran into some confusion.  I have seen the definition of $i$ in two different forms; $i = \sqrt{-1} $ and $i^2 = -1$.  Now I know that these two statements are not equivalent, so I am confused as to which is the 'correct' definition.  I see quite frequently that the first form is a common mistake, but then again Wolfram Math World says otherwise.  So my questions are: What is the 'correct' definition of $i$ and why?  Or are both definitions correct and you can view the first one as a principal branch? It seems that if we are treating $i$ as the number with the property $i^2 = -1$, it is implied that we are treating $i$ as a concept and not necessarily as a ""quantity""? If we are indeed treating $i$ as a concept rather than a ""quantity"", how would things such as $i^i$ and other equations/expressions involving $i$ be viewed?  How would such an equation have value if we treat $i$ like a concept? I've checked around on the various imaginary number posts on this site, so please don't mark this as a duplicate.  My questions are different than those that have already been asked.",,"['complex-analysis', 'complex-numbers']"
85,Proving that a doubly-periodic entire function $f$ is constant.,Proving that a doubly-periodic entire function  is constant.,f,"Let $f: \Bbb C \to \Bbb C$ be an entire (analytic on the whole plane) function such that exists $\omega_1,\omega_2 \in \mathbb{S}^1$, linearly independent over $\Bbb R$ such that: $$f(z+\omega_1)=f(z)=f(z+\omega_2), \quad \forall\,z\in \Bbb C.$$Prove that $f$ is constant. The intuition seems clear to me, we have the three vertices of a triangle given by $0$, $\omega_1$ and $\omega_2$. All points in the plane are one of the vertices of that triangle under a suitable parallel translation. The constant value will be $f(0)$, fine. Throwing values for $z$ there, I have found that $$f(n\omega_1) = f(\omega_1) = f(0)=f(\omega_2) = f(n\omega_2), \quad \forall\, n \in \Bbb Z.$$ I don't know how to improve the above for, say, rationals (at least). Some another ideas would be: Checking that $f' \equiv 0$. I don't have a clue of how to do that. Write $w = a\omega_1+b\omega_2$, with $a,b \in \Bbb R$, do stuff and conclude that $f(w) = f(0)$. This approach doesn't seem good, because I only have a weak result with integers above. Finding that $f$ coincides with $f(0)$ on a set with an accumulation point. This seems also bad: the set on with $f$ coincides with $f(0)$ by which I found above is discrete. Nothing works and this is getting annoying... And I don't see how analyticity comes in there. I'll be very thankful if someone can give me an idea. (On a side note.. I know that this title is not informative at all. Feel free to edit if you come up with something better.)","Let $f: \Bbb C \to \Bbb C$ be an entire (analytic on the whole plane) function such that exists $\omega_1,\omega_2 \in \mathbb{S}^1$, linearly independent over $\Bbb R$ such that: $$f(z+\omega_1)=f(z)=f(z+\omega_2), \quad \forall\,z\in \Bbb C.$$Prove that $f$ is constant. The intuition seems clear to me, we have the three vertices of a triangle given by $0$, $\omega_1$ and $\omega_2$. All points in the plane are one of the vertices of that triangle under a suitable parallel translation. The constant value will be $f(0)$, fine. Throwing values for $z$ there, I have found that $$f(n\omega_1) = f(\omega_1) = f(0)=f(\omega_2) = f(n\omega_2), \quad \forall\, n \in \Bbb Z.$$ I don't know how to improve the above for, say, rationals (at least). Some another ideas would be: Checking that $f' \equiv 0$. I don't have a clue of how to do that. Write $w = a\omega_1+b\omega_2$, with $a,b \in \Bbb R$, do stuff and conclude that $f(w) = f(0)$. This approach doesn't seem good, because I only have a weak result with integers above. Finding that $f$ coincides with $f(0)$ on a set with an accumulation point. This seems also bad: the set on with $f$ coincides with $f(0)$ by which I found above is discrete. Nothing works and this is getting annoying... And I don't see how analyticity comes in there. I'll be very thankful if someone can give me an idea. (On a side note.. I know that this title is not informative at all. Feel free to edit if you come up with something better.)",,"['complex-analysis', 'analyticity', 'holomorphic-functions', 'entire-functions']"
86,Composition of a harmonic function with a holomorphic function is still harmonic,Composition of a harmonic function with a holomorphic function is still harmonic,,"If $f$ is a harmonic function in a domain $D \subset \mathbb{C}$ , and $g$ is a conformal mapping of a domain $D_0$ onto $D$ , is $f \circ g$ harmonic in $D_0$ ? I noticed this question while reading several pdf of lecture notes, and I'm having trouble figuring it out. Can anyone help? Thank you so much!","If is a harmonic function in a domain , and is a conformal mapping of a domain onto , is harmonic in ? I noticed this question while reading several pdf of lecture notes, and I'm having trouble figuring it out. Can anyone help? Thank you so much!",f D \subset \mathbb{C} g D_0 D f \circ g D_0,"['complex-analysis', 'harmonic-functions']"
87,Mapping half-plane to unit disk?,Mapping half-plane to unit disk?,,Say you have the half-plane $\{z\in\mathbb{C}:\Re(z)>0\}$. Is there a rigorous explanation why the transformation $w=\dfrac{z-1}{z+1}$ maps the half plane onto $|w|<1$?,Say you have the half-plane $\{z\in\mathbb{C}:\Re(z)>0\}$. Is there a rigorous explanation why the transformation $w=\dfrac{z-1}{z+1}$ maps the half plane onto $|w|<1$?,,['complex-analysis']
88,"Closed form for $_2F_1\left(\frac12,\frac23;\,\frac32;\,\frac{8\,\sqrt{11}\,i-5}{27}\right)$",Closed form for,"_2F_1\left(\frac12,\frac23;\,\frac32;\,\frac{8\,\sqrt{11}\,i-5}{27}\right)","I'm trying to find a closed form (in terms of simpler functions) for the following hypergeometric function with a complex argument: $$\mathcal{Q}=\,_2F_1\left(\frac12,\frac23;\,\frac32;\,\frac{8\,\sqrt{11}\,i-5}{27}\right).\tag1$$ I have a guess (supported by thousands of digits from numerical calculations) about its argument (phase), but no ideas about its absolute value yet: $$\arg\mathcal{Q}\approx0.168669236010871306727578153...\stackrel?=\arccos\left(\frac{12+\sqrt{33}}{18}\right)\tag2$$ $$|\mathcal{Q}|\approx0.915170225773196416688677425...\tag3$$ Can you suggest any ideas how to prove the conjecture $(2)$? Is there a closed form for the absolute value? As suggested by gammatester in a comment, the conjecture $(2)$ is equivalent to $$\arg\,B\left(\frac{8\,\sqrt{11}\,i-5}{27};\,\frac12,\frac13\right)\stackrel?=\frac\pi3,\tag4$$ where $B$ denotes the incomplete beta function . Also, it can be shown that another equivalent form of the conjecture $(2)$ is $$B\left(\frac19;\,\frac16,\frac13\right)\stackrel?=\frac{\Gamma\left(\frac16\right)\,\Gamma\left(\frac13\right)}{2\,\sqrt\pi}.\tag5$$","I'm trying to find a closed form (in terms of simpler functions) for the following hypergeometric function with a complex argument: $$\mathcal{Q}=\,_2F_1\left(\frac12,\frac23;\,\frac32;\,\frac{8\,\sqrt{11}\,i-5}{27}\right).\tag1$$ I have a guess (supported by thousands of digits from numerical calculations) about its argument (phase), but no ideas about its absolute value yet: $$\arg\mathcal{Q}\approx0.168669236010871306727578153...\stackrel?=\arccos\left(\frac{12+\sqrt{33}}{18}\right)\tag2$$ $$|\mathcal{Q}|\approx0.915170225773196416688677425...\tag3$$ Can you suggest any ideas how to prove the conjecture $(2)$? Is there a closed form for the absolute value? As suggested by gammatester in a comment, the conjecture $(2)$ is equivalent to $$\arg\,B\left(\frac{8\,\sqrt{11}\,i-5}{27};\,\frac12,\frac13\right)\stackrel?=\frac\pi3,\tag4$$ where $B$ denotes the incomplete beta function . Also, it can be shown that another equivalent form of the conjecture $(2)$ is $$B\left(\frac19;\,\frac16,\frac13\right)\stackrel?=\frac{\Gamma\left(\frac16\right)\,\Gamma\left(\frac13\right)}{2\,\sqrt\pi}.\tag5$$",,"['complex-analysis', 'special-functions', 'closed-form', 'hypergeometric-function']"
89,Multivariate Residue Theorem?,Multivariate Residue Theorem?,,"Is there an extension of the residue theorem to multivariate complex functions? Say you have a function of $n$ complex variables $s_{n}$ and you wish to integrate it over some region in $\mathbb{C}^{n}$. Can you exploit the singularities of the function as you would in the single variable case to evaluate the integral? For example  \begin{equation}G=\int_{\Omega} d^{n}s \frac{1}{\sum_{i} s_{i}^{2}}\end{equation} This is singular for the $n$ roots of the equation $\sum_{i}s_{i}^{2}=0$ but I can't think of a way to extend the residue theorem to such a case. I guess you could go to 'polar' coordinates and it may simplify this one, but what about more complicated functions where this isn't possible? I'm sorry if the question is ill-defined.","Is there an extension of the residue theorem to multivariate complex functions? Say you have a function of $n$ complex variables $s_{n}$ and you wish to integrate it over some region in $\mathbb{C}^{n}$. Can you exploit the singularities of the function as you would in the single variable case to evaluate the integral? For example  \begin{equation}G=\int_{\Omega} d^{n}s \frac{1}{\sum_{i} s_{i}^{2}}\end{equation} This is singular for the $n$ roots of the equation $\sum_{i}s_{i}^{2}=0$ but I can't think of a way to extend the residue theorem to such a case. I guess you could go to 'polar' coordinates and it may simplify this one, but what about more complicated functions where this isn't possible? I'm sorry if the question is ill-defined.",,"['complex-analysis', 'reference-request', 'several-complex-variables']"
90,How not to prove the Riemann hypothesis,How not to prove the Riemann hypothesis,,"I remember reading somewhere that there is a (probably a family of) quick false proof of the Riemann hypothesis that starts by using complex logarithms in a bad way, then does some elementary calculations and out pops the result. A perusal of the General Mathematics section of the arXiv also shows an abundance of presumably false proofs of the same. These tend to be somewhat... unclear and I'd prefer avoiding to look at them in any detail. Question: Can anyone describe a quick false proof of the Riemann hypothesis? Preferably I'd like one that has a very clear false step that misuses complex logarithms or $n$-th roots or some such gadget in an identifiable way. This is both because of my curiosity, and because I figure I could use it as an exercise problem for undergrads (if I ever end up teaching a course on complex analysis) to explain why they should be careful about taking logarithms.","I remember reading somewhere that there is a (probably a family of) quick false proof of the Riemann hypothesis that starts by using complex logarithms in a bad way, then does some elementary calculations and out pops the result. A perusal of the General Mathematics section of the arXiv also shows an abundance of presumably false proofs of the same. These tend to be somewhat... unclear and I'd prefer avoiding to look at them in any detail. Question: Can anyone describe a quick false proof of the Riemann hypothesis? Preferably I'd like one that has a very clear false step that misuses complex logarithms or $n$-th roots or some such gadget in an identifiable way. This is both because of my curiosity, and because I figure I could use it as an exercise problem for undergrads (if I ever end up teaching a course on complex analysis) to explain why they should be careful about taking logarithms.",,"['complex-analysis', 'fake-proofs', 'riemann-hypothesis']"
91,Intuition for Little Picard's theorem,Intuition for Little Picard's theorem,,"Little Picard's theorem is the following:  Suppose $f:\mathbb{C}\rightarrow \mathbb{C}$ is entire.  Then either 1)  $f$ is constant 2)  $f$ is surjective or 3)  $f$ is onto $\mathbb{C}-\{p\}$ for some point $p\in \mathbb{C}$. Said another way, an entire function which misses 2 points in the codomain is actually constant. The exponential function $f(z) = e^z$, which is never $0$, shows that in general all 3 cases can arise. As someone who knows a bit of differential geometry and algebraic topology, the proof I like (indeed, the only one I remember) goes as follows:  Consider $X= \mathbb{C}-\{p,q\}$ where $p$ and $q$ are distinct complex numbers.  Then an entire map $f:\mathbb{C}\rightarrow X$ lifts to the universal cover $\mathbb{D}(0,1)$ (the unit disc around $0$ in $\mathbb{C}$) of $X$.  The lift of $f$ is an entire bounded function, and hence, by Liouville's theorem, is constant.  This easily implies $f$ itself is constant.  $\square$ The part that's hazy, to me, is the justification that the universal cover of $X$ is (biholomorphic to) the unit disc.  Of course, by uniformization, the universal cover is biholomorphic to either $\mathbb{C}$, $\mathbb{D}(0,1)$, or $S^2$.  Since $S^2$ is compact and $X$ is not, it cannot be $S^2$.  Interestingly, if we remove only a single point from $\mathbb{C}$ (which we assume wlog is $0$), then $e^z$ is a covering map from $\mathbb{C}$ to $\mathbb{C}-\{0\}$.  In other words, the universal cover a once punctured plane is $\mathbb{C}$ but the universal cover of a twice or more punctured plane is the unit disc. Is there good intuition as to why 2 is the crucial number of punctures for which the universal cover is no longer biholomorphic to $\mathbb{C}$? Relatedly (and nicer since it would allow me to avoid using uniformization, which I feel is way overkill for this), Is the universal covering map $\pi:\mathbb{D}(0,1)\rightarrow X$ easy, or even possible, to explicitly write down? By translating, rotating, and scaling, one can obviously assume that $\{p,q\} = \{0,1\}$ A quick google search shows the answer to the second question should be ""yes"", and that $\pi$ can be expressed in terms of modular functions.  Sadly, I don't know anything at all about modular functions, but I couldn't find anyting that tells how to express $\pi$ with modular functions.","Little Picard's theorem is the following:  Suppose $f:\mathbb{C}\rightarrow \mathbb{C}$ is entire.  Then either 1)  $f$ is constant 2)  $f$ is surjective or 3)  $f$ is onto $\mathbb{C}-\{p\}$ for some point $p\in \mathbb{C}$. Said another way, an entire function which misses 2 points in the codomain is actually constant. The exponential function $f(z) = e^z$, which is never $0$, shows that in general all 3 cases can arise. As someone who knows a bit of differential geometry and algebraic topology, the proof I like (indeed, the only one I remember) goes as follows:  Consider $X= \mathbb{C}-\{p,q\}$ where $p$ and $q$ are distinct complex numbers.  Then an entire map $f:\mathbb{C}\rightarrow X$ lifts to the universal cover $\mathbb{D}(0,1)$ (the unit disc around $0$ in $\mathbb{C}$) of $X$.  The lift of $f$ is an entire bounded function, and hence, by Liouville's theorem, is constant.  This easily implies $f$ itself is constant.  $\square$ The part that's hazy, to me, is the justification that the universal cover of $X$ is (biholomorphic to) the unit disc.  Of course, by uniformization, the universal cover is biholomorphic to either $\mathbb{C}$, $\mathbb{D}(0,1)$, or $S^2$.  Since $S^2$ is compact and $X$ is not, it cannot be $S^2$.  Interestingly, if we remove only a single point from $\mathbb{C}$ (which we assume wlog is $0$), then $e^z$ is a covering map from $\mathbb{C}$ to $\mathbb{C}-\{0\}$.  In other words, the universal cover a once punctured plane is $\mathbb{C}$ but the universal cover of a twice or more punctured plane is the unit disc. Is there good intuition as to why 2 is the crucial number of punctures for which the universal cover is no longer biholomorphic to $\mathbb{C}$? Relatedly (and nicer since it would allow me to avoid using uniformization, which I feel is way overkill for this), Is the universal covering map $\pi:\mathbb{D}(0,1)\rightarrow X$ easy, or even possible, to explicitly write down? By translating, rotating, and scaling, one can obviously assume that $\{p,q\} = \{0,1\}$ A quick google search shows the answer to the second question should be ""yes"", and that $\pi$ can be expressed in terms of modular functions.  Sadly, I don't know anything at all about modular functions, but I couldn't find anyting that tells how to express $\pi$ with modular functions.",,['complex-analysis']
92,What can be said about the level set of the real part of an analytic function?,What can be said about the level set of the real part of an analytic function?,,"I am working with a function $F(z;a)$, for $z\in \mathbb{C}$ and $a$ being a set of parameters, from which I need to analyze the level set $\text{Re}(F(z))=0$ (for a fixed set of parameters $a$, which I will drop the notation now). The function $F$ is composed of elliptic functions and algebraic functions. To achieve my goal, I need to be able to say some concrete things about this level set. Here is what I know about the level sets right now: The real line is always part of this level set. The following symmetry exists: if $\text{Re}(F(z))=0$ then $\text{Re}(F(z^*))=0$ where $z^*$ is the complex conjugate of $z$. There are 4 points in $\mathbb{C}\setminus\mathbb{R}$ which are part of this level set (2 are conjugates of the other two). I can prove that the 2 conjugate points are endpoints of a ""band"" (arc) of the level set connecting the points. The curve parameterized by $\text{Re}(F(z))=0$ has four saddle points at which $\textrm{d}(\text{Re}(F(z)))/\textrm{d}z = 0$. For some values of $a$, two of these saddle points are on the real axis and I can show that the bands mentioned in the previous item intersect this point. I need to know more about this level set, in particular I would like to answer the following questions: Can I prove that there are $n$ connected components of this level set? ($n$ might depend on $a$). Is there any (nonreal) component of this level set outside of, for instance, the unit ball? Is there reason to believe that any additional components of the set (not listed above) should be connected to those listed above ($\textit{i.e.}$ the real line or one of the 4 points)? Or, is there reason to believe that there should only be bands of the set branching off of the real line at saddle points? Right now I am using software to get my hands on the geometry of this set. I have been looking at contour plots of $\text{Re}(F(z_r + i z_i))=0$ for different choices of $a$. The above questions are conjectures made from looking at different plots, but I know that the software misses things (for instance, it rarely shows the real line as being part of the desired set). Help in the right direction would be appreciated, my knowledge of complex analysis theorems is woefully inadequate to accomplish my goal here. Any other ideas of what can be said (that I may have missed) would also be appreciated. Edit: The expression was omitted originally because it is different in different contexts.  By request, the expression for one of these cases has been added. It is: $$F(z;a) = -2iz\omega_1 + 2\Gamma(\omega_1\zeta(\alpha;\Lambda) - \zeta(\omega_1;\Lambda)\alpha),$$ where $\Gamma = \pm 1$, $\zeta(\cdot;\Lambda)$ is the Weierstrass-zeta function defined on a rectangular lattice ($\omega_1\in \mathbb{R}, \omega_3\in i\mathbb{R}$) and $$ \alpha = \wp^{-1}(f(z;a); \Lambda)$$ where $f(z;a)$ has 4 branch points (corresponding to the 4 points mentioned in (3) above), has poles at $z=\infty$ and may also have poles at $z=0$. In general, $f(z;a)$ has points $\hat z$ and $\hat a$ such that $f(\hat z; \hat a) = 0$.","I am working with a function $F(z;a)$, for $z\in \mathbb{C}$ and $a$ being a set of parameters, from which I need to analyze the level set $\text{Re}(F(z))=0$ (for a fixed set of parameters $a$, which I will drop the notation now). The function $F$ is composed of elliptic functions and algebraic functions. To achieve my goal, I need to be able to say some concrete things about this level set. Here is what I know about the level sets right now: The real line is always part of this level set. The following symmetry exists: if $\text{Re}(F(z))=0$ then $\text{Re}(F(z^*))=0$ where $z^*$ is the complex conjugate of $z$. There are 4 points in $\mathbb{C}\setminus\mathbb{R}$ which are part of this level set (2 are conjugates of the other two). I can prove that the 2 conjugate points are endpoints of a ""band"" (arc) of the level set connecting the points. The curve parameterized by $\text{Re}(F(z))=0$ has four saddle points at which $\textrm{d}(\text{Re}(F(z)))/\textrm{d}z = 0$. For some values of $a$, two of these saddle points are on the real axis and I can show that the bands mentioned in the previous item intersect this point. I need to know more about this level set, in particular I would like to answer the following questions: Can I prove that there are $n$ connected components of this level set? ($n$ might depend on $a$). Is there any (nonreal) component of this level set outside of, for instance, the unit ball? Is there reason to believe that any additional components of the set (not listed above) should be connected to those listed above ($\textit{i.e.}$ the real line or one of the 4 points)? Or, is there reason to believe that there should only be bands of the set branching off of the real line at saddle points? Right now I am using software to get my hands on the geometry of this set. I have been looking at contour plots of $\text{Re}(F(z_r + i z_i))=0$ for different choices of $a$. The above questions are conjectures made from looking at different plots, but I know that the software misses things (for instance, it rarely shows the real line as being part of the desired set). Help in the right direction would be appreciated, my knowledge of complex analysis theorems is woefully inadequate to accomplish my goal here. Any other ideas of what can be said (that I may have missed) would also be appreciated. Edit: The expression was omitted originally because it is different in different contexts.  By request, the expression for one of these cases has been added. It is: $$F(z;a) = -2iz\omega_1 + 2\Gamma(\omega_1\zeta(\alpha;\Lambda) - \zeta(\omega_1;\Lambda)\alpha),$$ where $\Gamma = \pm 1$, $\zeta(\cdot;\Lambda)$ is the Weierstrass-zeta function defined on a rectangular lattice ($\omega_1\in \mathbb{R}, \omega_3\in i\mathbb{R}$) and $$ \alpha = \wp^{-1}(f(z;a); \Lambda)$$ where $f(z;a)$ has 4 branch points (corresponding to the 4 points mentioned in (3) above), has poles at $z=\infty$ and may also have poles at $z=0$. In general, $f(z;a)$ has points $\hat z$ and $\hat a$ such that $f(\hat z; \hat a) = 0$.",,"['complex-analysis', 'analysis', 'analytic-functions']"
93,What even *are* elliptic functions?,What even *are* elliptic functions?,,"I am just beginning to learn about elliptic functions. Wikipedia defines an elliptic function as a function which is meromorphic on $\Bbb C$ , and for which there exist two non-zero complex numbers $\omega_1$ and $\omega_2$ , with $\frac{\omega_1}{\omega_2}\not\in \Bbb R$ , which satisfy $$f(z)=f(z+\omega_1)=f(z+\omega_2).$$ That's all fine and dandy, but what does this have to do with an ellipse? I sort of know (but not really) about the Jacobi elliptic functions. I am told by the internet that the Jacobi elliptic functions can be defined as inverses of elliptic integrals, which relate to the arc lengths of ellipses. But other than that, I have no idea how elliptic functions relate to ellipses. I have looked at several sources, like this , this , and this . From what I can understand, any elliptic function can be expressed in terms of the Jacobi elliptic functions and Weierstrass elliptic functions, but I have yet to understand why that is true. Perhaps it has something to do with what ODE's elliptic functions satisfy? I do not know. I would really appreciate some help and/or a good source on the introduction to the study of elliptic functions in the context of elliptic integrals, because I do work best with integrals. Thanks!","I am just beginning to learn about elliptic functions. Wikipedia defines an elliptic function as a function which is meromorphic on , and for which there exist two non-zero complex numbers and , with , which satisfy That's all fine and dandy, but what does this have to do with an ellipse? I sort of know (but not really) about the Jacobi elliptic functions. I am told by the internet that the Jacobi elliptic functions can be defined as inverses of elliptic integrals, which relate to the arc lengths of ellipses. But other than that, I have no idea how elliptic functions relate to ellipses. I have looked at several sources, like this , this , and this . From what I can understand, any elliptic function can be expressed in terms of the Jacobi elliptic functions and Weierstrass elliptic functions, but I have yet to understand why that is true. Perhaps it has something to do with what ODE's elliptic functions satisfy? I do not know. I would really appreciate some help and/or a good source on the introduction to the study of elliptic functions in the context of elliptic integrals, because I do work best with integrals. Thanks!",\Bbb C \omega_1 \omega_2 \frac{\omega_1}{\omega_2}\not\in \Bbb R f(z)=f(z+\omega_1)=f(z+\omega_2).,"['complex-analysis', 'special-functions', 'analytic-number-theory', 'elliptic-integrals', 'elliptic-functions']"
94,Why are these two definitions of the Mandelbrot set equivalent?,Why are these two definitions of the Mandelbrot set equivalent?,,"The definition of the Mandelbrot set that most enthusiasts first encounter is that of the set of all complex numbers $c$ for which the sequence $z_{n+1} = z_n^2 + c$ starting from $z_0 = 0$ does not diverge. For convenience, let us name this familiar quadratic map $P_c(z) = z^2 + c$. I have read that an equivalent definition of the Mandelbrot set is as the ""connectedness locus"" of the Julia sets of $P_c$. That is, $c$ is in the Mandelbrot set if and only if its corresponding filled Julia set $K_c = \{z : P_c^n(z) \not\rightarrow \infty\}$ is connected. Why are these definitions equivalent? I understand that the equivalence boils down to the statement that $K_c$ is connected if and only it contains $0$, but I don't know how this is proved.","The definition of the Mandelbrot set that most enthusiasts first encounter is that of the set of all complex numbers $c$ for which the sequence $z_{n+1} = z_n^2 + c$ starting from $z_0 = 0$ does not diverge. For convenience, let us name this familiar quadratic map $P_c(z) = z^2 + c$. I have read that an equivalent definition of the Mandelbrot set is as the ""connectedness locus"" of the Julia sets of $P_c$. That is, $c$ is in the Mandelbrot set if and only if its corresponding filled Julia set $K_c = \{z : P_c^n(z) \not\rightarrow \infty\}$ is connected. Why are these definitions equivalent? I understand that the equivalence boils down to the statement that $K_c$ is connected if and only it contains $0$, but I don't know how this is proved.",,['complex-analysis']
95,Upper bound on differences of consecutive zeta zeros,Upper bound on differences of consecutive zeta zeros,,"The average gap $\delta_n=|\gamma_{n+1}-\gamma_n|$ between consecutive zeros $(\beta_n+\gamma_n i,\beta_{n+1}+\gamma_{n+1}i)$ of Riemann's zeta function is $\frac{2\pi}{\log\gamma_n}.$ There are many papers giving lower bounds to $$ \limsup_n\ \delta_n\frac{\log\gamma_n}{2\pi} $$ unconditionally or on RH or GRH. (The true value is believed to be $+\infty.$) I'm interested in an upper bound on the smaller quantity $\delta_n$. I asked the question on MathOverflow but have not yet found an effective bound. Both unconditional results and those relying on the RH are interesting.","The average gap $\delta_n=|\gamma_{n+1}-\gamma_n|$ between consecutive zeros $(\beta_n+\gamma_n i,\beta_{n+1}+\gamma_{n+1}i)$ of Riemann's zeta function is $\frac{2\pi}{\log\gamma_n}.$ There are many papers giving lower bounds to $$ \limsup_n\ \delta_n\frac{\log\gamma_n}{2\pi} $$ unconditionally or on RH or GRH. (The true value is believed to be $+\infty.$) I'm interested in an upper bound on the smaller quantity $\delta_n$. I asked the question on MathOverflow but have not yet found an effective bound. Both unconditional results and those relying on the RH are interesting.",,"['number-theory', 'complex-analysis', 'special-functions', 'analytic-number-theory', 'riemann-zeta']"
96,"For which complex $a,\,b,\,c$ does $(a^b)^c=a^{bc}$ hold?",For which complex  does  hold?,"a,\,b,\,c (a^b)^c=a^{bc}","Wolfram Mathematica simplifies $(a^b)^c$ to $a^{bc}$ only for positive real $a, b$ and $c$. See W|A output . I've previously been struggling to understand why does $\dfrac{\log(a^b)}{\log(a)}=b$ and $\log(a^b)=b\log(a)$ not always hold (while I was always thinking of logarithms of positive reals as $\log_a(a^b)=\dfrac{\log(a^b)}{\log(a)}=\dfrac{b\log(a)}{\log(a)}=b$ before I've started to self-study complexes), but then learned about branch cutting of multifunctions, so that natural logarithm can be defined to be a function by first finding a set of $w$ for each $z$, so that $z=e^w$ (inverse natural exponential), and then somehow (no matter how) selecting a unique solution $w$ from that set, so that $\forall{z}\exists!{w}$, yielding a function $z\mapsto{w}$. This results in some sort of discontinuity where solutions are being dropped: This is done because functions are generally more easy to deal with than multifunctions. Wolfram's convention is to define $\log(z)$ as the inverse of $e^z$ such that $\log(1)=0$ and such that the branch cut discontinuity is at ${(-\infty;0]}$. I know this is somewhat unpopular to think of natural logarithm as a single-valued function, but I am going to follow this convention (I personally think it is well-justified, at least that's what happens with $\operatorname{arcsin}$, $\operatorname{sqrt}$, etc, so in fact I am comfortable with it). Then it is clear for me why is $\dfrac{\log\left((-2)^{-3}\right)}{\log(-2)}\approx{0.814319+0.841574i}\neq{-3}$, while $-3$ perfectly satisfies the equation $(-2)^x=(-2)^{-3}$. Given that $\log(a^b)=b\log(a)$ holds for positive real $a, b$ only , I thought how would I then solve equations of form $a^x=b$, where $a,b$ are complexes and not necessarily positive reals, since my first step was always to rewrite everything to base $e$: $$e^{\log(a^x)}=e^{\log(b)}$$ and then carry the exponent out of the logarithm: $$e^{x\log(a)}=e^{\log(b)}$$ (adding $2\pi i k,\,k\in\mathbb{Z}$ to any exponent and then eliminating the exponentials yields the result). So I've asked on ##math , where I've been pointed out that another way to rewrite $a^b$ to base $e$ is using the definition of the logarithm: $a^b=(a)^b=(e^{\log(a)})^b=e^{b\log(a)}$. I was happy with that (and even solved a $(-2)^x=-3$ for $x$ just for fun using this approach, see here ) but some time later I have realized that this neither does not actually work for arbitrary complex $a$ and $b$! $$1=1^{1/2}=((-1)^2)^{1/2}=(-1)^{2\cdot\frac{1}{2}}=(-1)^1=-1$$ The rule $(a^b)^c$, as I was told by W|A, requires $a, b, c$ to be positive reals. When I pointed that out on ##math , I was told that $a^b=e^{b\log(a)}$ is by definition of complex exponentiation. I've checked, and Wolfram Mathematica agreed with this identity. Too good! But then I realized from these something must not be true: $(a^b)^c=a^{bc}$ holds only for positive real $a,b,c$ $\forall{a,b\in\mathbb{C}}:a^b=e^{b\log(a)}$ $\forall{a\in\mathbb{C}}:a=e^{log(a)}$ The latter is the definition of the logarithm, so should be true. The second also must be true, otherwise I do not know how to solve equations. Hence: $$a^c=e^{c\log(a)}$$ is by the definition of complex exponentiation, just as I was told. Then, rewriting $a$ in the LHS using the definition of logarithm: $$a=e^{log(a)}$$ $$\left(e^{log(a)}\right)^c=e^{c\log(a)}$$ Now, since that for every complex $b$ there exists a $z$ such that $b=\log(z)$, we can rewrite $\log(a)=b$: $$(e^b)^c=e^{bc}$$ for all complex $b,c$! So, what is that? I've made a mistake? Or is base $a=e$ that special? Or is in fact (I suspect) one just needs to require $a$ to be positive real, and $b,c$ are in fact irrelevant? $$\forall{a}\in\mathbb{R}\forall{b,c}\in\mathbb{C}:a>0\implies{(a^b)^c=a^{bc}}$$ Have I found a bug in Mathematica and W|A or made a huge stupid mistake leading myself to drastic misunderstanding? P. S. This is my first post at MSE, I am not a math major, just a hobbyist, so sorry if I am struggling at basics here. Also sorry for my English: it is not my native language. Edit: thank you @Andrew for your answer. $\forall{a,b,c}:-\pi<\Im(b\log(a))\leq\pi\implies(a^b)^c=a^{bc}$ Very clear and straightforward, works flawlessly. But it appears that though the implication is obviously true, there are more cases (read ""values of $a,\,b,\,c$"") from which $(a^b)^c=a^{bc}$ does follow, i.e. I found that it is true for $c\in\mathbb{Z}$ and arbitrary complex $a,\,b$, for example: $$\big((-2)^{-3}\big)^{-2}=(-2)^{(-3)\times(-2)};$$ $$\left(\frac{1}{(-2)^3}\right)^{-2}=(-2)^{6};$$ $$\left(-\frac{1}{8}\right)^{-2}=64;$$ $$\left(-8\right)^2=64;$$ $$64=64.$$ For this case, $b\log(a)=-3\log(-2)=-3(\log(2)+i\pi)=-3\log(2)-3i\pi$, and hence $\Im(-3\log(2)-3i\pi)=-3\pi\notin{({-\pi;\pi}]}$, therefore @Andrew's implication does not cover all cases. So, is there any more solutions of $(a^b)^c=a^{bc}$?","Wolfram Mathematica simplifies $(a^b)^c$ to $a^{bc}$ only for positive real $a, b$ and $c$. See W|A output . I've previously been struggling to understand why does $\dfrac{\log(a^b)}{\log(a)}=b$ and $\log(a^b)=b\log(a)$ not always hold (while I was always thinking of logarithms of positive reals as $\log_a(a^b)=\dfrac{\log(a^b)}{\log(a)}=\dfrac{b\log(a)}{\log(a)}=b$ before I've started to self-study complexes), but then learned about branch cutting of multifunctions, so that natural logarithm can be defined to be a function by first finding a set of $w$ for each $z$, so that $z=e^w$ (inverse natural exponential), and then somehow (no matter how) selecting a unique solution $w$ from that set, so that $\forall{z}\exists!{w}$, yielding a function $z\mapsto{w}$. This results in some sort of discontinuity where solutions are being dropped: This is done because functions are generally more easy to deal with than multifunctions. Wolfram's convention is to define $\log(z)$ as the inverse of $e^z$ such that $\log(1)=0$ and such that the branch cut discontinuity is at ${(-\infty;0]}$. I know this is somewhat unpopular to think of natural logarithm as a single-valued function, but I am going to follow this convention (I personally think it is well-justified, at least that's what happens with $\operatorname{arcsin}$, $\operatorname{sqrt}$, etc, so in fact I am comfortable with it). Then it is clear for me why is $\dfrac{\log\left((-2)^{-3}\right)}{\log(-2)}\approx{0.814319+0.841574i}\neq{-3}$, while $-3$ perfectly satisfies the equation $(-2)^x=(-2)^{-3}$. Given that $\log(a^b)=b\log(a)$ holds for positive real $a, b$ only , I thought how would I then solve equations of form $a^x=b$, where $a,b$ are complexes and not necessarily positive reals, since my first step was always to rewrite everything to base $e$: $$e^{\log(a^x)}=e^{\log(b)}$$ and then carry the exponent out of the logarithm: $$e^{x\log(a)}=e^{\log(b)}$$ (adding $2\pi i k,\,k\in\mathbb{Z}$ to any exponent and then eliminating the exponentials yields the result). So I've asked on ##math , where I've been pointed out that another way to rewrite $a^b$ to base $e$ is using the definition of the logarithm: $a^b=(a)^b=(e^{\log(a)})^b=e^{b\log(a)}$. I was happy with that (and even solved a $(-2)^x=-3$ for $x$ just for fun using this approach, see here ) but some time later I have realized that this neither does not actually work for arbitrary complex $a$ and $b$! $$1=1^{1/2}=((-1)^2)^{1/2}=(-1)^{2\cdot\frac{1}{2}}=(-1)^1=-1$$ The rule $(a^b)^c$, as I was told by W|A, requires $a, b, c$ to be positive reals. When I pointed that out on ##math , I was told that $a^b=e^{b\log(a)}$ is by definition of complex exponentiation. I've checked, and Wolfram Mathematica agreed with this identity. Too good! But then I realized from these something must not be true: $(a^b)^c=a^{bc}$ holds only for positive real $a,b,c$ $\forall{a,b\in\mathbb{C}}:a^b=e^{b\log(a)}$ $\forall{a\in\mathbb{C}}:a=e^{log(a)}$ The latter is the definition of the logarithm, so should be true. The second also must be true, otherwise I do not know how to solve equations. Hence: $$a^c=e^{c\log(a)}$$ is by the definition of complex exponentiation, just as I was told. Then, rewriting $a$ in the LHS using the definition of logarithm: $$a=e^{log(a)}$$ $$\left(e^{log(a)}\right)^c=e^{c\log(a)}$$ Now, since that for every complex $b$ there exists a $z$ such that $b=\log(z)$, we can rewrite $\log(a)=b$: $$(e^b)^c=e^{bc}$$ for all complex $b,c$! So, what is that? I've made a mistake? Or is base $a=e$ that special? Or is in fact (I suspect) one just needs to require $a$ to be positive real, and $b,c$ are in fact irrelevant? $$\forall{a}\in\mathbb{R}\forall{b,c}\in\mathbb{C}:a>0\implies{(a^b)^c=a^{bc}}$$ Have I found a bug in Mathematica and W|A or made a huge stupid mistake leading myself to drastic misunderstanding? P. S. This is my first post at MSE, I am not a math major, just a hobbyist, so sorry if I am struggling at basics here. Also sorry for my English: it is not my native language. Edit: thank you @Andrew for your answer. $\forall{a,b,c}:-\pi<\Im(b\log(a))\leq\pi\implies(a^b)^c=a^{bc}$ Very clear and straightforward, works flawlessly. But it appears that though the implication is obviously true, there are more cases (read ""values of $a,\,b,\,c$"") from which $(a^b)^c=a^{bc}$ does follow, i.e. I found that it is true for $c\in\mathbb{Z}$ and arbitrary complex $a,\,b$, for example: $$\big((-2)^{-3}\big)^{-2}=(-2)^{(-3)\times(-2)};$$ $$\left(\frac{1}{(-2)^3}\right)^{-2}=(-2)^{6};$$ $$\left(-\frac{1}{8}\right)^{-2}=64;$$ $$\left(-8\right)^2=64;$$ $$64=64.$$ For this case, $b\log(a)=-3\log(-2)=-3(\log(2)+i\pi)=-3\log(2)-3i\pi$, and hence $\Im(-3\log(2)-3i\pi)=-3\pi\notin{({-\pi;\pi}]}$, therefore @Andrew's implication does not cover all cases. So, is there any more solutions of $(a^b)^c=a^{bc}$?",,"['complex-analysis', 'complex-numbers', 'logarithms', 'exponential-function', 'exponentiation']"
97,When does a modular form satisfy a differential equation with rational coefficients?,When does a modular form satisfy a differential equation with rational coefficients?,,"Given a modular form $f$ of weight $k$ for a congruence subgroup $\Gamma$, and a modular function $t$ for $\Gamma$ with $t(i\infty)=0$, we can form a function $F$ such that $F(t(z))=f(z)$ (at least locally), and we know that this $F$ must now satisfy a linear ordinary differential equation $$P_{k+1}(T)F^{(k+1)} + P_{k}(T)F^{(k)} + ... + P_{0}(T)F = 0$$ Where $F^{(i)}$ is the i-th derivative, and the $P_i$ are algebraic functions of $T$, and are rational functions of $T$ if $t$ is a Hauptmodul for $X(\Gamma)$. My question is the following: given a modular form $f$, what are necessary and sufficient conditions for the existence of a modular function $t$ such that the $P_i(T)$ are rational functions? For example, the easiest sufficient condition is that $X(\Gamma)$ has genus 0, by letting $t$ be a Hauptmodul. But, this is not necessary, as the next condition will show. Another sufficient condition is that $f$ is a rational weight 2 eigenform. I can show this using Shimura's construction* of an associated elliptic curve, and a computation of a logarithm for the formal group in some coordinates (*any choice in the isogeny class will work). Trying to generalise, I have thought of the following: if $f$ is associated to a motive $h^i(V)$ of a variety $V$, with Artin-Mazur formal group $\Phi^i(V)$ of dimension 1, then we can construct formal group law a-la Stienstra style, and get a logarithm using the coefficients of powers of certain polynomials. Since the dimension is 1, there will actually be a single polynomial that we take powers of, making the coefficients have a rather simple recurrence relation, forcing our $P_i$ to be rational. Now, some people, without naming names, believe that rational eigenforms should correspond to the middle cohomology of certain rational Calabi-Yai varieties. I'm not entirely certain that such people exist. Probably. If this is true, then this should answer my question for rational eigenforms. Putting non-eigenforms aside, since I'm not interested as much in them, we are left with non-rational eigenforms. We can try to perform the same Stienstra construction, but this time we get that the galois orbit of $f$ is associated to a ""formal group law"" of a motive with dimension greater than one. This will make for an interesting recurrence for the vector of the galois orbit, but not necessarily for each form individually, as the isomorphism of formal groups laws (between Stienstra's and those with the modular forms as logarithm) will scramble them together. I realise this last paragraph might be difficult to understand, for the wording is clumsy, and the mathematical notions are even worse. If you're really interested in this, I'd be happy to elaborate.","Given a modular form $f$ of weight $k$ for a congruence subgroup $\Gamma$, and a modular function $t$ for $\Gamma$ with $t(i\infty)=0$, we can form a function $F$ such that $F(t(z))=f(z)$ (at least locally), and we know that this $F$ must now satisfy a linear ordinary differential equation $$P_{k+1}(T)F^{(k+1)} + P_{k}(T)F^{(k)} + ... + P_{0}(T)F = 0$$ Where $F^{(i)}$ is the i-th derivative, and the $P_i$ are algebraic functions of $T$, and are rational functions of $T$ if $t$ is a Hauptmodul for $X(\Gamma)$. My question is the following: given a modular form $f$, what are necessary and sufficient conditions for the existence of a modular function $t$ such that the $P_i(T)$ are rational functions? For example, the easiest sufficient condition is that $X(\Gamma)$ has genus 0, by letting $t$ be a Hauptmodul. But, this is not necessary, as the next condition will show. Another sufficient condition is that $f$ is a rational weight 2 eigenform. I can show this using Shimura's construction* of an associated elliptic curve, and a computation of a logarithm for the formal group in some coordinates (*any choice in the isogeny class will work). Trying to generalise, I have thought of the following: if $f$ is associated to a motive $h^i(V)$ of a variety $V$, with Artin-Mazur formal group $\Phi^i(V)$ of dimension 1, then we can construct formal group law a-la Stienstra style, and get a logarithm using the coefficients of powers of certain polynomials. Since the dimension is 1, there will actually be a single polynomial that we take powers of, making the coefficients have a rather simple recurrence relation, forcing our $P_i$ to be rational. Now, some people, without naming names, believe that rational eigenforms should correspond to the middle cohomology of certain rational Calabi-Yai varieties. I'm not entirely certain that such people exist. Probably. If this is true, then this should answer my question for rational eigenforms. Putting non-eigenforms aside, since I'm not interested as much in them, we are left with non-rational eigenforms. We can try to perform the same Stienstra construction, but this time we get that the galois orbit of $f$ is associated to a ""formal group law"" of a motive with dimension greater than one. This will make for an interesting recurrence for the vector of the galois orbit, but not necessarily for each form individually, as the isomorphism of formal groups laws (between Stienstra's and those with the modular forms as logarithm) will scramble them together. I realise this last paragraph might be difficult to understand, for the wording is clumsy, and the mathematical notions are even worse. If you're really interested in this, I'd be happy to elaborate.",,"['complex-analysis', 'number-theory', 'algebraic-geometry', 'modular-forms']"
98,Convexity of $\theta(q)$,Convexity of,\theta(q),"Define Jacobi's (fourth) theta function with argument zero and nome $q$: $$\theta(q) = 1+2\sum_{n=1}^\infty (-1)^n q^{n^2}$$ plot of the function via Wolfram|Alpha plot of the function via Sage I am looking for a simple/standard/illuminating proof of the fact that $\theta(q)$ is convex for $q\in[0,1]$. The proof I found goes like this: We have $$\theta'(q) = 2\sum_{n=1}^\infty (-1)^n n^2 q^{n^2-1}$$ and one can show that for some $q_0\in(0,1)$, $n^2q^{n^2-1} - (n+1)^2q^{(n+1)^2-1}$ is increasing in $[0,q_0]$ for any $n\ge 2$. This gives convexity of $\theta(q)$ in $[0,q_0]$. For the remaining values of $q$, one uses the representation of $\theta$ as a sum over Gaussian kernels: $$\theta(e^{-\pi^2t/2}) = 2 \sqrt{\frac{2}{\pi t}}\sum_{n=1}^\infty \exp\left(-\frac{(2n-1)^2}{2t}\right)$$ With this representation, one can show that the second derivative (wrt $q$) of each summand is positive for $q \ge q_1$, with $q_1 < q_0$. This yields convexity of theta. I don't like this proof, because it requires calculating $q_1$ and $q_0$ explicitly and it is not very illuminating. I tried playing around with the representation of $\theta(q)$ as the infinite product $$\theta(q) = \prod_{n=1}^\infty (1-q^{2n-1})^2(1-q^{2n}),$$ but didn't manage to find anything, except that the partial products $$\prod_{n=1}^N (1-q^{2n-1})^2(1-q^{2n})$$ all seem to be convex in $[0,1]$, which would prove the statement. All suggestions are very welcome!","Define Jacobi's (fourth) theta function with argument zero and nome $q$: $$\theta(q) = 1+2\sum_{n=1}^\infty (-1)^n q^{n^2}$$ plot of the function via Wolfram|Alpha plot of the function via Sage I am looking for a simple/standard/illuminating proof of the fact that $\theta(q)$ is convex for $q\in[0,1]$. The proof I found goes like this: We have $$\theta'(q) = 2\sum_{n=1}^\infty (-1)^n n^2 q^{n^2-1}$$ and one can show that for some $q_0\in(0,1)$, $n^2q^{n^2-1} - (n+1)^2q^{(n+1)^2-1}$ is increasing in $[0,q_0]$ for any $n\ge 2$. This gives convexity of $\theta(q)$ in $[0,q_0]$. For the remaining values of $q$, one uses the representation of $\theta$ as a sum over Gaussian kernels: $$\theta(e^{-\pi^2t/2}) = 2 \sqrt{\frac{2}{\pi t}}\sum_{n=1}^\infty \exp\left(-\frac{(2n-1)^2}{2t}\right)$$ With this representation, one can show that the second derivative (wrt $q$) of each summand is positive for $q \ge q_1$, with $q_1 < q_0$. This yields convexity of theta. I don't like this proof, because it requires calculating $q_1$ and $q_0$ explicitly and it is not very illuminating. I tried playing around with the representation of $\theta(q)$ as the infinite product $$\theta(q) = \prod_{n=1}^\infty (1-q^{2n-1})^2(1-q^{2n}),$$ but didn't manage to find anything, except that the partial products $$\prod_{n=1}^N (1-q^{2n-1})^2(1-q^{2n})$$ all seem to be convex in $[0,1]$, which would prove the statement. All suggestions are very welcome!",,"['complex-analysis', 'analysis', 'special-functions', 'elliptic-functions', 'theta-functions']"
99,inequality involving complex exponential,inequality involving complex exponential,,"Is it true that $$|e^{ix}-e^{iy}|\leq |x-y|$$ for $x,y\in\mathbb{R}$? I can't figure it out. I tried looking at the series for exponential but it did not help. Could someone offer a hint?","Is it true that $$|e^{ix}-e^{iy}|\leq |x-y|$$ for $x,y\in\mathbb{R}$? I can't figure it out. I tried looking at the series for exponential but it did not help. Could someone offer a hint?",,['complex-analysis']
