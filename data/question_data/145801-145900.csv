,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Computing an infinite limit involving a double integral,Computing an infinite limit involving a double integral,,"$$\lim_{T\to \infty}\frac{\int_0^T\cos^2(s) \exp(-s)\int_0^s \cos(\cos a)\exp(a)\,\mathrm{d}a\,\mathrm{d}s}{T}$$ I tried computing this limit in maple but got the answer: 'undefined'. How to compute this limit, I accept special functions.","$$\lim_{T\to \infty}\frac{\int_0^T\cos^2(s) \exp(-s)\int_0^s \cos(\cos a)\exp(a)\,\mathrm{d}a\,\mathrm{d}s}{T}$$ I tried computing this limit in maple but got the answer: 'undefined'. How to compute this limit, I accept special functions.",,"['limits', 'special-functions']"
1,Ball-dimension of space,Ball-dimension of space,,"I have to find ball-dimension of space $\{0\}\cup\{\frac{1}{n}:n\in\mathbb{N}\}$, which is $\limsup_{\epsilon\to 0}\frac{\log\beta(\epsilon)}{|\log\epsilon|}$. Here, $\beta(\epsilon)$ is minimum cardinality of a covering of metric space by $\epsilon$-balls. Diameter of $\epsilon$ ball is $2\epsilon$, therefore $\beta(\epsilon)>\frac{1}{2\epsilon}$ and $\limsup_{\epsilon\to 0}\frac{\log\beta(\epsilon)}{|\log\epsilon|}\geq 1$. How to conclude what is ball-dimension of this space? I'm also looking for textbook with similar problems or notes where I can read about ball dimension. By joriki's answer dimension of space is $\frac{1}{2}$, is there any way to see it intuitively? Any help is welcome. Thanks in advance.","I have to find ball-dimension of space $\{0\}\cup\{\frac{1}{n}:n\in\mathbb{N}\}$, which is $\limsup_{\epsilon\to 0}\frac{\log\beta(\epsilon)}{|\log\epsilon|}$. Here, $\beta(\epsilon)$ is minimum cardinality of a covering of metric space by $\epsilon$-balls. Diameter of $\epsilon$ ball is $2\epsilon$, therefore $\beta(\epsilon)>\frac{1}{2\epsilon}$ and $\limsup_{\epsilon\to 0}\frac{\log\beta(\epsilon)}{|\log\epsilon|}\geq 1$. How to conclude what is ball-dimension of this space? I'm also looking for textbook with similar problems or notes where I can read about ball dimension. By joriki's answer dimension of space is $\frac{1}{2}$, is there any way to see it intuitively? Any help is welcome. Thanks in advance.",,"['limits', 'reference-request', 'metric-spaces', 'dynamical-systems']"
2,Show that $\lim_{x\to 1}\frac{1}{x+1}=\frac{1}{2}$.,Show that .,\lim_{x\to 1}\frac{1}{x+1}=\frac{1}{2},"Could someone please explain a step in the following proof? Show that $\lim_{x\to 1}\frac{1}{x+1}=\frac{1}{2}$. The above limit exists if for every $\varepsilon > 0$, there exists a real number $\delta > 0$ such that if $0<|x-1|<\delta$, then $\left |\frac{1}{x+1}-\frac{1}{2}  \right |=\left | \frac{2-x-1}{2(x+1)} \right |=\left | \frac{1-x}{2(x+1)} \right |=\frac{|1-x|}{2|x+1|}=\frac{|x-1|}{2|x+1|}<\varepsilon$. I understand that $|x-1|$ can be made small, but for $|x+1|$, why does the solution say the condition ""if $|x-1| < 1$, $x+1\in (1,3)$""? Why and how is the condition $|x-1| <1$ made?","Could someone please explain a step in the following proof? Show that $\lim_{x\to 1}\frac{1}{x+1}=\frac{1}{2}$. The above limit exists if for every $\varepsilon > 0$, there exists a real number $\delta > 0$ such that if $0<|x-1|<\delta$, then $\left |\frac{1}{x+1}-\frac{1}{2}  \right |=\left | \frac{2-x-1}{2(x+1)} \right |=\left | \frac{1-x}{2(x+1)} \right |=\frac{|1-x|}{2|x+1|}=\frac{|x-1|}{2|x+1|}<\varepsilon$. I understand that $|x-1|$ can be made small, but for $|x+1|$, why does the solution say the condition ""if $|x-1| < 1$, $x+1\in (1,3)$""? Why and how is the condition $|x-1| <1$ made?",,['limits']
3,Show that $\lim_{x\to\infty}x^\frac{m}{m+1}(x^\frac{1}{m+1} - (x-1)^\frac{1}{m+1}) = \frac{1}{m+1}$ using the Mean Value Theorem,Show that  using the Mean Value Theorem,\lim_{x\to\infty}x^\frac{m}{m+1}(x^\frac{1}{m+1} - (x-1)^\frac{1}{m+1}) = \frac{1}{m+1},"Show that $\lim_{x\to\infty}x^\frac{m}{m+1}(x^\frac{1}{m+1} - (x-1)^\frac{1}{m+1}) = \frac{1}{m+1}$ using the Mean Value Theorem with m and x being whole numbers, for x greater or equal to 1 and m greater or equal to 0. My work so far: We define $f(x) := x^\frac{1}{m+1}$. From the MVT it follows that there exists an $x_0$ between a and b (with b > a) such that: $$\frac{f(b)-f(a)}{b-a} =  f'(x_0)$$ From choosing $b = x$ and $a = x - 1$ it follows that: $$x^\frac{1}{m+1} - (x-1)^\frac{1}{m+1} = \frac{1}{(m+1)x_0^\frac{m}{m+1}}$$ Multiplication with $x^\frac{m}{m+1}$ results in: $$x^\frac{m}{m+1}(x^\frac{1}{m+1} - (x-1)^\frac{1}{m+1}) = \frac{x^\frac{m}{m+1}}{(m+1)x_0^\frac{m}{m+1}}$$ I intuitively want to take the limit to infinity of the left-hand side, but I can't justify why the $x$ and $x_0$ terms will cancel-out. I'd also to be happy to hear of any methods that don't use the MVT.","Show that $\lim_{x\to\infty}x^\frac{m}{m+1}(x^\frac{1}{m+1} - (x-1)^\frac{1}{m+1}) = \frac{1}{m+1}$ using the Mean Value Theorem with m and x being whole numbers, for x greater or equal to 1 and m greater or equal to 0. My work so far: We define $f(x) := x^\frac{1}{m+1}$. From the MVT it follows that there exists an $x_0$ between a and b (with b > a) such that: $$\frac{f(b)-f(a)}{b-a} =  f'(x_0)$$ From choosing $b = x$ and $a = x - 1$ it follows that: $$x^\frac{1}{m+1} - (x-1)^\frac{1}{m+1} = \frac{1}{(m+1)x_0^\frac{m}{m+1}}$$ Multiplication with $x^\frac{m}{m+1}$ results in: $$x^\frac{m}{m+1}(x^\frac{1}{m+1} - (x-1)^\frac{1}{m+1}) = \frac{x^\frac{m}{m+1}}{(m+1)x_0^\frac{m}{m+1}}$$ I intuitively want to take the limit to infinity of the left-hand side, but I can't justify why the $x$ and $x_0$ terms will cancel-out. I'd also to be happy to hear of any methods that don't use the MVT.",,"['real-analysis', 'limits', 'derivatives']"
4,How to prove that $(1 + \frac{1}{n})^{\sqrt{n(n+1)}} < e$? [closed],How to prove that ? [closed],(1 + \frac{1}{n})^{\sqrt{n(n+1)}} < e,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . The community reviewed whether to reopen this question 8 months ago and left it closed: Original close reason(s) were not resolved Improve this question I've been trying to solve the following problem: Show that $\ln{(k+1)} - \ln{k} = \ln{(1 + \frac{1}{k})} \leq \frac{1}{\sqrt{k(k+1)}}$ EDIT: the title was inaccurate, my bad.  So what we have to prove is that the upper limit of $(1 + \frac{1}{n})^{\sqrt{n(n+1)}}$ equals $e$.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . The community reviewed whether to reopen this question 8 months ago and left it closed: Original close reason(s) were not resolved Improve this question I've been trying to solve the following problem: Show that $\ln{(k+1)} - \ln{k} = \ln{(1 + \frac{1}{k})} \leq \frac{1}{\sqrt{k(k+1)}}$ EDIT: the title was inaccurate, my bad.  So what we have to prove is that the upper limit of $(1 + \frac{1}{n})^{\sqrt{n(n+1)}}$ equals $e$.",,"['limits', 'inequality', 'exponential-function']"
5,Proving $x^{1/x}$ approaches $1$ as $x$ approaches infinity [duplicate],Proving  approaches  as  approaches infinity [duplicate],x^{1/x} 1 x,"This question already has answers here : How to show that $\lim_{n \to +\infty} n^{\frac{1}{n}} = 1$? (13 answers) Closed 6 years ago . I was trying to solve a question that was uploaded here ( Computing $\lim_{n\rightarrow \infty} \frac{1+\sqrt{2}+\sqrt[3]{3}+\cdots+\sqrt[n]{n}}{n}$. ): $$\lim_{x \to \infty} \frac  {1 + 2^{1/2} + 3^{1/3} +...+ x^{1/x}}{x}$$ So i used the identity $$\lim_{x \to \infty} x^{1/x} = 1$$ and tried to prove it myself. Since $$(\sqrt[x]{x})^x = x\implies f(x) = \sqrt[x]{x} =  \frac x{(\sqrt[x]{x})^{x-1}}$$ I tried to prove that there is a horizontal asymptote at $y=1$ when $\lim\limits_{x \to \infty} x$ using the latter function without using L'Hôpital's rule. I want to emphasize that I am not currently trying to solve the first limit, but trying to prove the asymptote of the function $f(x)$. Any suggestions?","This question already has answers here : How to show that $\lim_{n \to +\infty} n^{\frac{1}{n}} = 1$? (13 answers) Closed 6 years ago . I was trying to solve a question that was uploaded here ( Computing $\lim_{n\rightarrow \infty} \frac{1+\sqrt{2}+\sqrt[3]{3}+\cdots+\sqrt[n]{n}}{n}$. ): $$\lim_{x \to \infty} \frac  {1 + 2^{1/2} + 3^{1/3} +...+ x^{1/x}}{x}$$ So i used the identity $$\lim_{x \to \infty} x^{1/x} = 1$$ and tried to prove it myself. Since $$(\sqrt[x]{x})^x = x\implies f(x) = \sqrt[x]{x} =  \frac x{(\sqrt[x]{x})^{x-1}}$$ I tried to prove that there is a horizontal asymptote at $y=1$ when $\lim\limits_{x \to \infty} x$ using the latter function without using L'Hôpital's rule. I want to emphasize that I am not currently trying to solve the first limit, but trying to prove the asymptote of the function $f(x)$. Any suggestions?",,"['limits', 'limits-without-lhopital']"
6,Find $\lim_{n \to \infty}(n - \frac{1}{2^n} \sum_{k=1}^{2^n}\log_{2}(k+2^{n-1}))$ using Riemann sums,Find  using Riemann sums,\lim_{n \to \infty}(n - \frac{1}{2^n} \sum_{k=1}^{2^n}\log_{2}(k+2^{n-1})),"I need to examine whether the following limit exists or not. $$\lim_{n \to +\infty}(n - \frac{1}{2^n} \sum_{k=1}^{2^n}\log_{2}(k+2^{n-1}))$$ If it does, I need to calculate it's value. I think ""Riemann sums"" may be a good start. So, I'm trying to convert the expression $$=\lim_{n \to +\infty} (n-\frac{1}{2^n} \sum_{k=1}^{2^n} (\log_2(\frac{k}{2^n}+\frac{1}{2})+\log_22^n))\\  = \lim_{n \to +\infty} (n(1-\frac{1}{2^n})-\frac{1}{2^n} \sum_{k=1}^{2^n} \log_2(\frac{k}{2^n}+\frac{1}{2}))\\  = - \lim_{n \to +\infty} (\frac{1}{2^n} \sum_{k=1}^{2^n} (\log_2({\frac{k}{2^n}+\frac{1}{2}}) - n(1-\frac{1}{2^n})))$$ but I don't know what to do next and how to finish it. Can anybody help me?","I need to examine whether the following limit exists or not. $$\lim_{n \to +\infty}(n - \frac{1}{2^n} \sum_{k=1}^{2^n}\log_{2}(k+2^{n-1}))$$ If it does, I need to calculate it's value. I think ""Riemann sums"" may be a good start. So, I'm trying to convert the expression $$=\lim_{n \to +\infty} (n-\frac{1}{2^n} \sum_{k=1}^{2^n} (\log_2(\frac{k}{2^n}+\frac{1}{2})+\log_22^n))\\  = \lim_{n \to +\infty} (n(1-\frac{1}{2^n})-\frac{1}{2^n} \sum_{k=1}^{2^n} \log_2(\frac{k}{2^n}+\frac{1}{2}))\\  = - \lim_{n \to +\infty} (\frac{1}{2^n} \sum_{k=1}^{2^n} (\log_2({\frac{k}{2^n}+\frac{1}{2}}) - n(1-\frac{1}{2^n})))$$ but I don't know what to do next and how to finish it. Can anybody help me?",,"['calculus', 'integration', 'limits', 'summation', 'riemann-sum']"
7,Does the following limit exists?,Does the following limit exists?,,"$$ \lim_{a\to 0}\left\{a\int_{-R}^{R}\int_{-R}^{R} \frac{\mathrm{d}x\,\mathrm{d}y} {\,\sqrt{\,\left[\left(x + a\right)^{2} + y^{2}\right] \left[\left(x - a\right)^{2} + y^{2}\right]\,}\,}\right\} \quad\mbox{where}\ R\ \mbox{is a}\ positive\ \mbox{number.} $$  The integral exists, since you can take small disks around singularities and make a change of variables. It seems that the limit should be $0$, as $\lim_{a \to 0}\left[a\log\left(a\right)\right] = 0$.","$$ \lim_{a\to 0}\left\{a\int_{-R}^{R}\int_{-R}^{R} \frac{\mathrm{d}x\,\mathrm{d}y} {\,\sqrt{\,\left[\left(x + a\right)^{2} + y^{2}\right] \left[\left(x - a\right)^{2} + y^{2}\right]\,}\,}\right\} \quad\mbox{where}\ R\ \mbox{is a}\ positive\ \mbox{number.} $$  The integral exists, since you can take small disks around singularities and make a change of variables. It seems that the limit should be $0$, as $\lim_{a \to 0}\left[a\log\left(a\right)\right] = 0$.",,"['calculus', 'integration', 'analysis', 'limits']"
8,$a_{n+1}=\frac2{a_n+a_{n−1}}$ . Prove that this sequence has a limit. [duplicate],. Prove that this sequence has a limit. [duplicate],a_{n+1}=\frac2{a_n+a_{n−1}},"This question already has answers here : sequence of positive numbers satisfying $a_{n+1}=\frac{2}{a_n+a_{n-1}}$, prove it converges (2 answers) Closed 6 years ago . The sequense $\{a_n\}$ such that $a_1>0, a_2>0$ and $$a_{n+1}=\frac2{a_n+a_{n−1}}$$ for $n≥2$. Prove that this sequence has a limit. I know I need to prove  1) the sequence is monotone; 2) the sequence is bounded. But sequense is not monotone. Any hints?","This question already has answers here : sequence of positive numbers satisfying $a_{n+1}=\frac{2}{a_n+a_{n-1}}$, prove it converges (2 answers) Closed 6 years ago . The sequense $\{a_n\}$ such that $a_1>0, a_2>0$ and $$a_{n+1}=\frac2{a_n+a_{n−1}}$$ for $n≥2$. Prove that this sequence has a limit. I know I need to prove  1) the sequence is monotone; 2) the sequence is bounded. But sequense is not monotone. Any hints?",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
9,"How can we show that $ (a_n), (b_n), (c_n) $ are convergent and have the same limit?",How can we show that  are convergent and have the same limit?," (a_n), (b_n), (c_n) ","We have the following for $a \le b \le c >0$: $A(a,b,c)=\frac{a+b+c}{3}, B(a,b,c)= (abc)^{1/3}, C(a,b,c)=\frac{3}{\frac{1}{a}+\frac{1}{b}+\frac{1}{c}}  $. Then we define the sequences $(a_n),(b_n), (c_n)$ by $a_1=a, b_1=b, c_1=c,$ $a_{n+1}=A(a_n,b_n,c_n), b_{n+1}=B(a_n,b_n,c_n), c_{n+1}=C(a_n,b_n,c_n)$. How can we show that $(a_n),(b_n), (c_n)$ are convergent and have the same limit? I understand that $A(a,b,c)\ge B(a,b,c)$ and $B(a,b,c)\ge C(a,b,c)$","We have the following for $a \le b \le c >0$: $A(a,b,c)=\frac{a+b+c}{3}, B(a,b,c)= (abc)^{1/3}, C(a,b,c)=\frac{3}{\frac{1}{a}+\frac{1}{b}+\frac{1}{c}}  $. Then we define the sequences $(a_n),(b_n), (c_n)$ by $a_1=a, b_1=b, c_1=c,$ $a_{n+1}=A(a_n,b_n,c_n), b_{n+1}=B(a_n,b_n,c_n), c_{n+1}=C(a_n,b_n,c_n)$. How can we show that $(a_n),(b_n), (c_n)$ are convergent and have the same limit? I understand that $A(a,b,c)\ge B(a,b,c)$ and $B(a,b,c)\ge C(a,b,c)$",,"['sequences-and-series', 'limits', 'inequality', 'convergence-divergence']"
10,How to solve the limit of $\ln(x \sin x)$ as $x$ approaches $0$?,How to solve the limit of  as  approaches ?,\ln(x \sin x) x 0,"I have a limit: $$k = \lim_{x\to0+} \ln(x \sin x)$$ How do I find this? Since $\ln(x)$ is continuous I tried: $$k = \ln( \lim_{x\to0+}  (x \sin x))$$ $$k = \ln(0)$$ Which to my understanding is undefined, but the answer is $-\infty$ somehow. Where did I go wrong?","I have a limit: $$k = \lim_{x\to0+} \ln(x \sin x)$$ How do I find this? Since $\ln(x)$ is continuous I tried: $$k = \ln( \lim_{x\to0+}  (x \sin x))$$ $$k = \ln(0)$$ Which to my understanding is undefined, but the answer is $-\infty$ somehow. Where did I go wrong?",,"['calculus', 'limits', 'logarithms']"
11,"$\forall\epsilon>0, \exists N\in\Bbb N$ such that $\forall n>m\ge N, a_n-a_m<\epsilon$. Prove that $a_n$ converges to a real limit or to $-\infty$",such that . Prove that  converges to a real limit or to,"\forall\epsilon>0, \exists N\in\Bbb N \forall n>m\ge N, a_n-a_m<\epsilon a_n -\infty","Assume $(a_n)_{n=1}^\infty$ has the following property: For all $\epsilon>0$ exists some $N\in\Bbb N$ such that for every $n>m\ge N$, $a_n-a_m<\epsilon$. We want to prove that $a_n$ converges to a real limit or to $-\infty$. This condition implies that $a_n$ is bounded above. So, I tried to show what happens if it's bounded below or unbounded below. When bounded below, by Bolzano-Weierstrass $a_n$ has some convergent subsequence, $a_{n_k} \rightarrow L$, so for all $k\ge K$ for some $K\in \Bbb N$, $|a_{n_k}-L|<\epsilon \iff -\epsilon<a_{n_k}-L<\epsilon$ . I tried to use this to prove the convergence of the entire sequence but it didn't work -  I tried to take some $n>n_k>max(N, n_K)$, $a_n-L=a_n-a_{n_k}+a_{n_k}-L <2\epsilon$, but now I can't get the left inequality right. This makes me question my entire process. Maybe I should've gone about this a different way?","Assume $(a_n)_{n=1}^\infty$ has the following property: For all $\epsilon>0$ exists some $N\in\Bbb N$ such that for every $n>m\ge N$, $a_n-a_m<\epsilon$. We want to prove that $a_n$ converges to a real limit or to $-\infty$. This condition implies that $a_n$ is bounded above. So, I tried to show what happens if it's bounded below or unbounded below. When bounded below, by Bolzano-Weierstrass $a_n$ has some convergent subsequence, $a_{n_k} \rightarrow L$, so for all $k\ge K$ for some $K\in \Bbb N$, $|a_{n_k}-L|<\epsilon \iff -\epsilon<a_{n_k}-L<\epsilon$ . I tried to use this to prove the convergence of the entire sequence but it didn't work -  I tried to take some $n>n_k>max(N, n_K)$, $a_n-L=a_n-a_{n_k}+a_{n_k}-L <2\epsilon$, but now I can't get the left inequality right. This makes me question my entire process. Maybe I should've gone about this a different way?",,"['calculus', 'sequences-and-series', 'limits']"
12,What is $\lim\limits_{n \to 0} \frac{d}{dx} \frac{1}{n} x^n$?,What is ?,\lim\limits_{n \to 0} \frac{d}{dx} \frac{1}{n} x^n,"A limit that I find rather intriguing is $\lim\limits_{n \to 0} \frac{d}{dx} \frac{1}{n} x^n$. Following the usual rules for differentiation of polynomials, this would be  $\lim\limits_{n \to 0} \frac {nx^{n-1}}{n} = x^{-1}$. It seems unlikely that this is actually the limit because the derivative of $ln(x)$ is $1/x$. Is there a way to prove what this actually is?","A limit that I find rather intriguing is $\lim\limits_{n \to 0} \frac{d}{dx} \frac{1}{n} x^n$. Following the usual rules for differentiation of polynomials, this would be  $\lim\limits_{n \to 0} \frac {nx^{n-1}}{n} = x^{-1}$. It seems unlikely that this is actually the limit because the derivative of $ln(x)$ is $1/x$. Is there a way to prove what this actually is?",,['calculus']
13,Evaluate $\lim\limits_{x\to a} \left(f(x)/f(a)\right)^{1/(\log x-\log a)}$,Evaluate,\lim\limits_{x\to a} \left(f(x)/f(a)\right)^{1/(\log x-\log a)},Let $f : \mathbb R \to \mathbb R$ be differentiable at $x = a$ and let $f(a) > 0$.  Evaluate: $$\lim\limits_{x\to a} \left(\frac{f(x)}{f(a)}\right)^{\dfrac{1}{\log x-\log a}}$$ My attempts: Let $$y=\lim\limits_{x\to a} \left(\frac{f(x)}{f(a)}\right)^{\dfrac{1}{\log x-\log a}}$$ Now I take log on both sides \begin{align*} \log y &= \log\left(\left(\frac{f(x)}{f(a)}\right)^{\dfrac{1}{\log x-\log a}}\right)\\ &= \frac{\log f(x) - \log f(a)}{x-a} \cdot \frac{x-a}{\log x-\log a}\\ &=  \frac{f'(a)}{f(a)} \end{align*} Therefore $$y= e^{\dfrac{f'(a)}{f(a)}}$$ Is my answer is correct or not? Please verify.,Let $f : \mathbb R \to \mathbb R$ be differentiable at $x = a$ and let $f(a) > 0$.  Evaluate: $$\lim\limits_{x\to a} \left(\frac{f(x)}{f(a)}\right)^{\dfrac{1}{\log x-\log a}}$$ My attempts: Let $$y=\lim\limits_{x\to a} \left(\frac{f(x)}{f(a)}\right)^{\dfrac{1}{\log x-\log a}}$$ Now I take log on both sides \begin{align*} \log y &= \log\left(\left(\frac{f(x)}{f(a)}\right)^{\dfrac{1}{\log x-\log a}}\right)\\ &= \frac{\log f(x) - \log f(a)}{x-a} \cdot \frac{x-a}{\log x-\log a}\\ &=  \frac{f'(a)}{f(a)} \end{align*} Therefore $$y= e^{\dfrac{f'(a)}{f(a)}}$$ Is my answer is correct or not? Please verify.,,"['real-analysis', 'limits']"
14,Prove that $\lim \frac{f^{(n)}(0)}{n!}=\frac{2}{k+1}$,Prove that,\lim \frac{f^{(n)}(0)}{n!}=\frac{2}{k+1},"Suppose $k$ is a fixed integer. $\{a_n\}_{n \in \mathbb{Z}}$ is a sequence determined by the following recursion: $\forall n <0, a_n=0. \\a_0=1$ $\forall n+k>0, a_{n+k}=\frac{1}{k} \sum_{i=0}^{k-1} a_{n+i}$. Then the generating function $$f(z)=\frac{k}{k-(z+z^2+\dots+z^k)}$$ Prove that $\lim a_n =\lim \frac{f^{(n)}(0)}{n!}=\frac{2}{k+1}$","Suppose $k$ is a fixed integer. $\{a_n\}_{n \in \mathbb{Z}}$ is a sequence determined by the following recursion: $\forall n <0, a_n=0. \\a_0=1$ $\forall n+k>0, a_{n+k}=\frac{1}{k} \sum_{i=0}^{k-1} a_{n+i}$. Then the generating function $$f(z)=\frac{k}{k-(z+z^2+\dots+z^k)}$$ Prove that $\lim a_n =\lim \frac{f^{(n)}(0)}{n!}=\frac{2}{k+1}$",,"['combinatorics', 'limits']"
15,Does $f$ behave like $x^2$?,Does  behave like ?,f x^2,"Let $f \in C^1([0, \infty))$ be a nondecreasing function such that $\displaystyle \lim_{x \to \infty} \frac{f(x)}{x^{2+\epsilon}}=0 \ \forall \epsilon>0$ and $\displaystyle \lim_{x \to \infty} \frac{f(x)}{x^{2-\epsilon}}= \infty \ \forall \epsilon>0$. Then does the following hold? $\displaystyle \lim_{x \to \infty} \frac{f^{\prime}(x)}{x^{1+\epsilon}}=0 \ \forall \epsilon>0$ and $\displaystyle \lim_{x \to \infty} \frac{f^{\prime}(x)}{x^{1-\epsilon}}= \infty \ \forall \epsilon>0$. Any advise would be appreciated.","Let $f \in C^1([0, \infty))$ be a nondecreasing function such that $\displaystyle \lim_{x \to \infty} \frac{f(x)}{x^{2+\epsilon}}=0 \ \forall \epsilon>0$ and $\displaystyle \lim_{x \to \infty} \frac{f(x)}{x^{2-\epsilon}}= \infty \ \forall \epsilon>0$. Then does the following hold? $\displaystyle \lim_{x \to \infty} \frac{f^{\prime}(x)}{x^{1+\epsilon}}=0 \ \forall \epsilon>0$ and $\displaystyle \lim_{x \to \infty} \frac{f^{\prime}(x)}{x^{1-\epsilon}}= \infty \ \forall \epsilon>0$. Any advise would be appreciated.",,"['limits', 'functions']"
16,Proving limit without using dominated convergence theorem,Proving limit without using dominated convergence theorem,,"Since $|x^{1/n} \sin (x)| \leq \pi$ for $x \in [0,\pi]$ it is an easy result of the Dominated Convergence Theorem to say: $$\lim_{n \to \infty} \int_0^\pi x^{1/n} \sin(x) dx = \int_0^\pi \lim_{n \to \infty} x^{1/n} \sin(x) dx = \int_0^\pi \sin(x)dx = 2. $$ I would like to see how to get  the limit directly and rigorousl y using an $\epsilon - N$ type of argument if possible.  The convergence $x^{1/n} \to 1$ is not uniform on $[0,\pi]$ which rules out one approach.","Since $|x^{1/n} \sin (x)| \leq \pi$ for $x \in [0,\pi]$ it is an easy result of the Dominated Convergence Theorem to say: $$\lim_{n \to \infty} \int_0^\pi x^{1/n} \sin(x) dx = \int_0^\pi \lim_{n \to \infty} x^{1/n} \sin(x) dx = \int_0^\pi \sin(x)dx = 2. $$ I would like to see how to get  the limit directly and rigorousl y using an $\epsilon - N$ type of argument if possible.  The convergence $x^{1/n} \to 1$ is not uniform on $[0,\pi]$ which rules out one approach.",,"['real-analysis', 'limits']"
17,Proving $\lim_{x\to a}f(x)$ and $\lim_{h\to 0}f(a+h)$ are equivalent using $\epsilon$-$\delta$ argument,Proving  and  are equivalent using - argument,\lim_{x\to a}f(x) \lim_{h\to 0}f(a+h) \epsilon \delta,"In Spivak's Calculus , he asks for a proof that $\lim_{x\to a}f(x)=\lim_{h\to 0}f(a+h)$ . He first shows that the existence of $\lim_{x\to a}f(x)$ implies the existence and equivalence of $\lim_{h\to0}f(a+h)$ , and then he says the argument for the other direction is ""similar,"" but I am having a hard time replicating it (I may be getting unnecessarily bogged down in notational issues). His proof of the first direction is essentially as follows: (Spivak forward direction): Let $\ell=\lim_{x\to a}f(x)$ and define $g(h)=f(a+h)$ . Then for every $\epsilon>0$ there is a $\delta>0$ such that, for all $x$ , if $0<|x-a|<\delta$ , then $|f(x)-\ell|<\epsilon$ . Now, if $0<|h|<\delta$ , then $0<|(a+h)-a|<\delta$ , so $|f(a+h)-\ell|<\epsilon$ , which we can write as $|g(h)-\ell|<\epsilon$ . Thus, $\lim_{h\to0}g(h)=\ell$ , which can also be written $\lim_{h\to 0}f(a+h)=\ell$ . The same sort of argument shows that if $\lim_{h\to 0}f(a+h)=m$ , then $\lim_{x\to a}f(x)=m$ . So either limit exists if the other does, and in this case they are equal. My attempt at other direction: Let $m=\lim_{h\to 0}f(a+h)$ . Then for every $\epsilon>0$ there is a $\delta>0$ for all $h$ such that if $0<|h|<\delta$ then $|f(a+h)-m|<\epsilon$ . Now, if $0<|x-a|<\delta$ , then $|f(a+(x-a))-m|=|f(x)-m|<\epsilon$ . Thus, $\lim_{x\to a}f(x)=m$ . What am I missing here? Is my proof okay? Why does Spivak use the function $g$ in the previous direction? Is it really necessary? What would such a $g$ be in the other direction?","In Spivak's Calculus , he asks for a proof that . He first shows that the existence of implies the existence and equivalence of , and then he says the argument for the other direction is ""similar,"" but I am having a hard time replicating it (I may be getting unnecessarily bogged down in notational issues). His proof of the first direction is essentially as follows: (Spivak forward direction): Let and define . Then for every there is a such that, for all , if , then . Now, if , then , so , which we can write as . Thus, , which can also be written . The same sort of argument shows that if , then . So either limit exists if the other does, and in this case they are equal. My attempt at other direction: Let . Then for every there is a for all such that if then . Now, if , then . Thus, . What am I missing here? Is my proof okay? Why does Spivak use the function in the previous direction? Is it really necessary? What would such a be in the other direction?",\lim_{x\to a}f(x)=\lim_{h\to 0}f(a+h) \lim_{x\to a}f(x) \lim_{h\to0}f(a+h) \ell=\lim_{x\to a}f(x) g(h)=f(a+h) \epsilon>0 \delta>0 x 0<|x-a|<\delta |f(x)-\ell|<\epsilon 0<|h|<\delta 0<|(a+h)-a|<\delta |f(a+h)-\ell|<\epsilon |g(h)-\ell|<\epsilon \lim_{h\to0}g(h)=\ell \lim_{h\to 0}f(a+h)=\ell \lim_{h\to 0}f(a+h)=m \lim_{x\to a}f(x)=m m=\lim_{h\to 0}f(a+h) \epsilon>0 \delta>0 h 0<|h|<\delta |f(a+h)-m|<\epsilon 0<|x-a|<\delta |f(a+(x-a))-m|=|f(x)-m|<\epsilon \lim_{x\to a}f(x)=m g g,"['calculus', 'real-analysis', 'limits', 'epsilon-delta']"
18,Limits that take on a range rather than a unique value,Limits that take on a range rather than a unique value,,I've come across two limits which are reported to take on a range of values rather than a unique one. They are: $$\lim \limits_{x \to \infty} \space\frac{1+\cos x}{1-\sin x} = 0 \space to\space \infty$$ $$\lim \limits_{x \to \infty} \space \frac{2+2x+\sin(2x)}{(2x+\sin(2x))e^{\sin x}} =  \frac{1}{e} \space to \space e$$ These answers seem to contradict what I know and understand as the definition of a limit. I'm self-studying maths so am clearly missing something here. My questions: How are these limits solved? Why do they exist? Why is a range of values allowed here? What kind of books should I read to learn more about this? If you could provide references that would be perfect. ETA results from Wolfram: The First The Second,I've come across two limits which are reported to take on a range of values rather than a unique one. They are: $$\lim \limits_{x \to \infty} \space\frac{1+\cos x}{1-\sin x} = 0 \space to\space \infty$$ $$\lim \limits_{x \to \infty} \space \frac{2+2x+\sin(2x)}{(2x+\sin(2x))e^{\sin x}} =  \frac{1}{e} \space to \space e$$ These answers seem to contradict what I know and understand as the definition of a limit. I'm self-studying maths so am clearly missing something here. My questions: How are these limits solved? Why do they exist? Why is a range of values allowed here? What kind of books should I read to learn more about this? If you could provide references that would be perfect. ETA results from Wolfram: The First The Second,,"['real-analysis', 'limits', 'limits-without-lhopital']"
19,$\lim _{n\to \infty }\left[\sum _{k=1}^{n-1}\left(1+\frac{k}{n}\right)\sin \left(\frac{k\pi }{n^2}\right)\right]$,,\lim _{n\to \infty }\left[\sum _{k=1}^{n-1}\left(1+\frac{k}{n}\right)\sin \left(\frac{k\pi }{n^2}\right)\right],"I have to calculate this limit using Riemann sums: $$\lim _{n\to \infty }\left[\left(1+\frac{1}{n}\right)\sin \left(\frac{\pi }{n^2}\right)+\left(1+\frac{2}{n}\right)\sin \:\left(\frac{2\pi \:}{n^2}\right)+...+\left(1+\frac{n-1}{n}\right)\sin \:\left(\frac{\left(n-1\right)\pi \:}{n^2}\right)\right]$$ and $$\lim _{n\to \infty }\left[\sum _{k=1}^{n-1}\left(1+\frac{k}{n}\right)\sin \left(\frac{k\pi }{n^2}\right)\right]$$ I've done quite a few of this type of limits, but I haven't yet figured out what function $f$ to use with the Riemann sum for this particular one. Could I have some hints on how to find that function? Thank you.","I have to calculate this limit using Riemann sums: $$\lim _{n\to \infty }\left[\left(1+\frac{1}{n}\right)\sin \left(\frac{\pi }{n^2}\right)+\left(1+\frac{2}{n}\right)\sin \:\left(\frac{2\pi \:}{n^2}\right)+...+\left(1+\frac{n-1}{n}\right)\sin \:\left(\frac{\left(n-1\right)\pi \:}{n^2}\right)\right]$$ and $$\lim _{n\to \infty }\left[\sum _{k=1}^{n-1}\left(1+\frac{k}{n}\right)\sin \left(\frac{k\pi }{n^2}\right)\right]$$ I've done quite a few of this type of limits, but I haven't yet figured out what function $f$ to use with the Riemann sum for this particular one. Could I have some hints on how to find that function? Thank you.",,"['calculus', 'limits', 'definite-integrals']"
20,Finding one sided limit using L'Hospital's rule,Finding one sided limit using L'Hospital's rule,,$\lim_{x\to 0^+}\left(x^\sqrt{x}\right)=?$ How do I turn it into a fraction? Is L'Hospital's rule even applicable?,$\lim_{x\to 0^+}\left(x^\sqrt{x}\right)=?$ How do I turn it into a fraction? Is L'Hospital's rule even applicable?,,"['calculus', 'limits']"
21,Finding the value of a tricky limit,Finding the value of a tricky limit,,"I have some difficulties with my homework in mathematical analysis and I don't really have any good ideas, how to get off the mark. If you, guys, could  give me any ideas, tips or solutions for the following task, I would be really thankful.  The task is as follows: Let $a_{n}$ be a sequence with positive members so that $$ \lim_{n} \frac{a_{n}}{n} = 0 ,$$ $$\limsup\limits_{n}\frac{a_1 + a_2 + ... + a_n }{n} \in \mathbb{R}.$$ Find the value of: $$ \lim_{n} \frac{a_1^2 + a_2^2 + ... + a_n^2}{n^2}  $$","I have some difficulties with my homework in mathematical analysis and I don't really have any good ideas, how to get off the mark. If you, guys, could  give me any ideas, tips or solutions for the following task, I would be really thankful.  The task is as follows: Let $a_{n}$ be a sequence with positive members so that $$ \lim_{n} \frac{a_{n}}{n} = 0 ,$$ $$\limsup\limits_{n}\frac{a_1 + a_2 + ... + a_n }{n} \in \mathbb{R}.$$ Find the value of: $$ \lim_{n} \frac{a_1^2 + a_2^2 + ... + a_n^2}{n^2}  $$",,"['sequences-and-series', 'limits', 'limsup-and-liminf']"
22,$\lim_{x \to 1} {P(x^n)\over P(x^m)}$,,\lim_{x \to 1} {P(x^n)\over P(x^m)},"if $P$ is a polynomial and $n,m\in\Bbb N^*$ and $P(1)=0$ find: $$\lim_{x \to 1} {P(x^n)\over P(x^m)}$$ Let $P(x)=a_νx^ν+a_{ν-1}x^{ν-1}+...+a_1x+a_0$ and I used L'Hôpital's rule and I found that $$\lim_{x \to 1} {P(x^n)\over P(x^m)}={n\over m}$$ which I'm not sure if it's the correct answer. I'd like a help/hint (maybe without L'Hôpital's rule).","if $P$ is a polynomial and $n,m\in\Bbb N^*$ and $P(1)=0$ find: $$\lim_{x \to 1} {P(x^n)\over P(x^m)}$$ Let $P(x)=a_νx^ν+a_{ν-1}x^{ν-1}+...+a_1x+a_0$ and I used L'Hôpital's rule and I found that $$\lim_{x \to 1} {P(x^n)\over P(x^m)}={n\over m}$$ which I'm not sure if it's the correct answer. I'd like a help/hint (maybe without L'Hôpital's rule).",,"['calculus', 'limits']"
23,Epsilon-Delta Proof of Limits Being Equal,Epsilon-Delta Proof of Limits Being Equal,,"How would I go about proving that two limits are equal to each other using the Epsilon-Delta definition? Moreover how can I prove that: $$\lim_{x\to0}f(x) = \lim_{x\to a}f(x-a)$$ using the Epsilon-Delta definition? The intuition for this seem clear. However, I have do not know how a formal proof can be developed.","How would I go about proving that two limits are equal to each other using the Epsilon-Delta definition? Moreover how can I prove that: $$\lim_{x\to0}f(x) = \lim_{x\to a}f(x-a)$$ using the Epsilon-Delta definition? The intuition for this seem clear. However, I have do not know how a formal proof can be developed.",,"['calculus', 'real-analysis', 'limits']"
24,"On the recurrence $a_{k+1}=\frac{a_k}{p_{k+1}}+\frac{1}{a_k}$, with $a_1=1$ and being $p_k$ the $kth$ prime number","On the recurrence , with  and being  the  prime number",a_{k+1}=\frac{a_k}{p_{k+1}}+\frac{1}{a_k} a_1=1 p_k kth,"We denote for integers $n\geq 1$ the $nth$ prime number as $p_n$. And for integers $k\geq 1$, I consider this recurrence relation $$a_{k+1}=\frac{a_k}{p_{k+1}}+\frac{1}{a_k},\tag{1}$$ with $a_1$ defined to be equal to $1$. I've calculated some (few) terms of this sequence $a_k$. See here the first  examples. Examples of computations of some terms of our sequence $ \left\{ a_n\right\}_{n=1} ^\infty$: 1) Since $a_1=1$ then $a_2=\frac{a_1}{p_2}+(a_1)^{-1}=\frac{1}{3}+\frac{1}{1}=\frac{4}{3}$. 2) Since the third prime number is $p_3=5$ one has  $$a_3=\frac{a_2}{p_3}+\frac{1}{a_2}=\frac{4/3}{5}+\frac{3}{4}=\frac{61}{60}.$$  3) Similarly $a_4=\frac{61}{7\cdot 60}+(60/61)^{-1}\approx 1.1288$. $\square$ Thus our sequence starts as $a_1=1,a_2\approx 1.3333, a_3\approx 1.0117, a_4\approx 1.1288$ and similarly we can calculate $a_5\approx 0.9664,a_6\approx 1.1091$ or $a_7\approx 0.9669$. To create this problem I was inspired in a recurrence that $\sqrt{2}$ solves as you can see from this WIkipedia , and now I am curious about how to check that the sequence in $(1)$ is convergent. Question. Please, can you prove that $\left\{ a_n\right\}_{n=1} ^\infty$ defined from $(1)$ is convergent? Many thanks. I know a a main tool in the theory of prime numbers is the Prime Number Theorem : $p_n\sim n\log n$ as $n\to\infty$.","We denote for integers $n\geq 1$ the $nth$ prime number as $p_n$. And for integers $k\geq 1$, I consider this recurrence relation $$a_{k+1}=\frac{a_k}{p_{k+1}}+\frac{1}{a_k},\tag{1}$$ with $a_1$ defined to be equal to $1$. I've calculated some (few) terms of this sequence $a_k$. See here the first  examples. Examples of computations of some terms of our sequence $ \left\{ a_n\right\}_{n=1} ^\infty$: 1) Since $a_1=1$ then $a_2=\frac{a_1}{p_2}+(a_1)^{-1}=\frac{1}{3}+\frac{1}{1}=\frac{4}{3}$. 2) Since the third prime number is $p_3=5$ one has  $$a_3=\frac{a_2}{p_3}+\frac{1}{a_2}=\frac{4/3}{5}+\frac{3}{4}=\frac{61}{60}.$$  3) Similarly $a_4=\frac{61}{7\cdot 60}+(60/61)^{-1}\approx 1.1288$. $\square$ Thus our sequence starts as $a_1=1,a_2\approx 1.3333, a_3\approx 1.0117, a_4\approx 1.1288$ and similarly we can calculate $a_5\approx 0.9664,a_6\approx 1.1091$ or $a_7\approx 0.9669$. To create this problem I was inspired in a recurrence that $\sqrt{2}$ solves as you can see from this WIkipedia , and now I am curious about how to check that the sequence in $(1)$ is convergent. Question. Please, can you prove that $\left\{ a_n\right\}_{n=1} ^\infty$ defined from $(1)$ is convergent? Many thanks. I know a a main tool in the theory of prime numbers is the Prime Number Theorem : $p_n\sim n\log n$ as $n\to\infty$.",,"['analysis', 'limits']"
25,Areas where the limit doesn't exist,Areas where the limit doesn't exist,,"I am currently marking for a first year calculus class, and I think my prof made a mistake on the solution sheet for the most recent recitation. The prof provided a cartesian coordinate system with a function $f(x)$, with vertical asymptotes at $x=-3, x=0$, and a horizantal asymptote at $y=1$. At the asymptote at $x=0$, the function approaches $+ \infty$ from both sides (ie $\lim_{x \to 0^+} = \lim_{x \to 0^-} = +\infty$). Now, one of the questions asks to list all numbers $a$ which $\lim_{x \to a} f(x)$ does not exist. And my prof listed $x=0$ as a point where the limit doesn't exist. But wouldn't the $\lim_{x \to 0} = +\infty$, and thus shouldn't be not listed as a point where the limit doesn't exist? Thanks for any help","I am currently marking for a first year calculus class, and I think my prof made a mistake on the solution sheet for the most recent recitation. The prof provided a cartesian coordinate system with a function $f(x)$, with vertical asymptotes at $x=-3, x=0$, and a horizantal asymptote at $y=1$. At the asymptote at $x=0$, the function approaches $+ \infty$ from both sides (ie $\lim_{x \to 0^+} = \lim_{x \to 0^-} = +\infty$). Now, one of the questions asks to list all numbers $a$ which $\lim_{x \to a} f(x)$ does not exist. And my prof listed $x=0$ as a point where the limit doesn't exist. But wouldn't the $\lim_{x \to 0} = +\infty$, and thus shouldn't be not listed as a point where the limit doesn't exist? Thanks for any help",,['limits']
26,Understanding L'Hopital Second Rule statement,Understanding L'Hopital Second Rule statement,,"I'm reading the second L'Hopital rule theorem and I'm having hard time understanding the beginning of the proof. Let the functions $f$ and $g$ are defined, they have continuous derivatives in the open interval $(a,b)$ and $g'(x) \ne 0$ everywhere in the interval  $(a,b)$. Let  $$\lim_{x\to a}f(x)=\lim_{x\to a}g(x)=+\infty $$ and let $\lim_{x\to a}\frac{f'(a)}{g'(a)}$ exists in a generalised sense. Then $$ \lim_{x\to a}\frac{f(a)}{g(a)}=\lim_{x\to a}\frac{f'(a)}{g'(a)}. $$ The proof begin by saying that the assumptions for the theorem imply that $g'(x) < 0$. I really don't get that. How is it that the function's limit is $+\infty$, but it is actually decreasing? My guess is that one of the functions limit is viewed from the opposite direction but I don't get how is that given in the statement information. Thanks in advance!","I'm reading the second L'Hopital rule theorem and I'm having hard time understanding the beginning of the proof. Let the functions $f$ and $g$ are defined, they have continuous derivatives in the open interval $(a,b)$ and $g'(x) \ne 0$ everywhere in the interval  $(a,b)$. Let  $$\lim_{x\to a}f(x)=\lim_{x\to a}g(x)=+\infty $$ and let $\lim_{x\to a}\frac{f'(a)}{g'(a)}$ exists in a generalised sense. Then $$ \lim_{x\to a}\frac{f(a)}{g(a)}=\lim_{x\to a}\frac{f'(a)}{g'(a)}. $$ The proof begin by saying that the assumptions for the theorem imply that $g'(x) < 0$. I really don't get that. How is it that the function's limit is $+\infty$, but it is actually decreasing? My guess is that one of the functions limit is viewed from the opposite direction but I don't get how is that given in the statement information. Thanks in advance!",,"['real-analysis', 'limits', 'convergence-divergence']"
27,"Proof that $lim_{x->0,y->0}{\frac {\sqrt {a+x^2y^2} -1} {x^2+y^2}} (a>0)$ doesn't exist while a $\ne 1$.",Proof that  doesn't exist while a .,"lim_{x->0,y->0}{\frac {\sqrt {a+x^2y^2} -1} {x^2+y^2}} (a>0) \ne 1","How to prove that $\lim_{x\to 0,y\to 0}{\frac {\sqrt {a+x^2y^2} -1} {x^2+y^2}} (a>0)$ doesn't exist while a $\ne 1$? I already calculated that when a = 1 by multiplying $\sqrt {a+x^2y^2} + 1$ on both denominator and numerator and using the fact $x^2y^2<(x^2+y^2)^2/4$. Any help will be appreciated.","How to prove that $\lim_{x\to 0,y\to 0}{\frac {\sqrt {a+x^2y^2} -1} {x^2+y^2}} (a>0)$ doesn't exist while a $\ne 1$? I already calculated that when a = 1 by multiplying $\sqrt {a+x^2y^2} + 1$ on both denominator and numerator and using the fact $x^2y^2<(x^2+y^2)^2/4$. Any help will be appreciated.",,"['calculus', 'limits', 'multivariable-calculus']"
28,Approximation error of $e - (1+1/x)^x$.,Approximation error of .,e - (1+1/x)^x,"It is well known that $e = \lim_{x \rightarrow +\infty} (1+1/x)^x$, which means $\forall \epsilon > 0$, $\exists M>0$ such as $x > M \Rightarrow |(1+1/x)^x - e| < \epsilon$. I would like to know what would be $M$ in function of $\epsilon$. I've made some attempts and it seems that $M = e/\epsilon$ works fine, but I could not prove it. In other words, I'm interested in, given the maximum error tolerance $\epsilon$, after which point $M$, I can use $e$ instead of $(1+x)^x$.","It is well known that $e = \lim_{x \rightarrow +\infty} (1+1/x)^x$, which means $\forall \epsilon > 0$, $\exists M>0$ such as $x > M \Rightarrow |(1+1/x)^x - e| < \epsilon$. I would like to know what would be $M$ in function of $\epsilon$. I've made some attempts and it seems that $M = e/\epsilon$ works fine, but I could not prove it. In other words, I'm interested in, given the maximum error tolerance $\epsilon$, after which point $M$, I can use $e$ instead of $(1+x)^x$.",,"['real-analysis', 'limits', 'exponential-function', 'approximation', 'epsilon-delta']"
29,Pointwise a.e. convergence implies $\int_E f d\mu \leq \lim (\int f_n d\mu)$.,Pointwise a.e. convergence implies .,\int_E f d\mu \leq \lim (\int f_n d\mu),"I'm trying to prove / disprove the following: If $f_n \geq 0$ is a sequence of integrable functions and $f_n \to f$ a.e., then $\lim \int_E f_n d\mu$ exists and $\int_E fd\mu \leq \lim \int_E f_n d\mu$. All limits here are as $n \to \infty$. What I have so far: As $f_n \to f$, we have that $\liminf f_n = f$. If $(\int_Ef_nd\mu)$ converges, it is equal to $\liminf (\int_E f_n d\mu)$, and so by Fatou's Lemma, $$\int_E fd\mu = \int_E\left(\liminf f_n\right)d\mu \leq \liminf \left(\int_E f_nd\mu\right) = \lim\left(\int_Efd\mu\right).$$ However, I'm stuck on showing that $\lim (\int_E f_n d\mu)$ exists, if at all.","I'm trying to prove / disprove the following: If $f_n \geq 0$ is a sequence of integrable functions and $f_n \to f$ a.e., then $\lim \int_E f_n d\mu$ exists and $\int_E fd\mu \leq \lim \int_E f_n d\mu$. All limits here are as $n \to \infty$. What I have so far: As $f_n \to f$, we have that $\liminf f_n = f$. If $(\int_Ef_nd\mu)$ converges, it is equal to $\liminf (\int_E f_n d\mu)$, and so by Fatou's Lemma, $$\int_E fd\mu = \int_E\left(\liminf f_n\right)d\mu \leq \liminf \left(\int_E f_nd\mu\right) = \lim\left(\int_Efd\mu\right).$$ However, I'm stuck on showing that $\lim (\int_E f_n d\mu)$ exists, if at all.",,"['real-analysis', 'limits']"
30,Convergence of a recursively defined sequence,Convergence of a recursively defined sequence,,"Let a sequence $(a_n)_{n=0}^\infty$ be defined recursively $a_{n+1} = (1-a_n)^{\frac1p}$, where $p>1$, $0<a_0<(1-a_0)^{\frac1p}$. Let $a$ be the unique real root of $a=(1-a)^{\frac1p}$, $0<a<1$. It is clear $0<a_0<(1-a_0)^{\frac1p}\Leftrightarrow 0<a_0<a$. Prove 1) $a_{2k-2}<a_{2k}<a<a_{2k+1}<a_{2k-1}$ and $a_{2k+1}-a<a-a_{2k}$. 2) $\lim\limits_{n\to\infty}a_n=a$. Define $f(x):=(1-x)^{\frac1p}$. Consider $f^2$. When $p=2$, $a_{n+2}=f^2(a_n)=\big(1-(1-a_n)^{\frac12}\big)^{\frac12}$. $a_{n+2}>a_n\Leftrightarrow (1-a_n)(1+a_n)^2>1\Leftrightarrow a_n<(1-a_n)^{\frac12}$, and the conclusion is proved. But I am having difficulty generalizing this method to arbitrary $p>1$. I also suspect that there is a general method to solve this kind of problem.","Let a sequence $(a_n)_{n=0}^\infty$ be defined recursively $a_{n+1} = (1-a_n)^{\frac1p}$, where $p>1$, $0<a_0<(1-a_0)^{\frac1p}$. Let $a$ be the unique real root of $a=(1-a)^{\frac1p}$, $0<a<1$. It is clear $0<a_0<(1-a_0)^{\frac1p}\Leftrightarrow 0<a_0<a$. Prove 1) $a_{2k-2}<a_{2k}<a<a_{2k+1}<a_{2k-1}$ and $a_{2k+1}-a<a-a_{2k}$. 2) $\lim\limits_{n\to\infty}a_n=a$. Define $f(x):=(1-x)^{\frac1p}$. Consider $f^2$. When $p=2$, $a_{n+2}=f^2(a_n)=\big(1-(1-a_n)^{\frac12}\big)^{\frac12}$. $a_{n+2}>a_n\Leftrightarrow (1-a_n)(1+a_n)^2>1\Leftrightarrow a_n<(1-a_n)^{\frac12}$, and the conclusion is proved. But I am having difficulty generalizing this method to arbitrary $p>1$. I also suspect that there is a general method to solve this kind of problem.",,"['real-analysis', 'sequences-and-series', 'limits', 'inequality', 'fixed-point-theorems']"
31,"Show that if $f$ is uniformly continuous and is Lebesgue integrable on $\mathbb{R}$, then $\lim_{|x| \to \infty}|f(x)| = 0$","Show that if  is uniformly continuous and is Lebesgue integrable on , then",f \mathbb{R} \lim_{|x| \to \infty}|f(x)| = 0,"Show that if $f: \mathbb{R} \to \mathbb{R}$ is uniformly continuous such that $$\int_{-\infty}^{\infty}|f(x)| < \infty \Rightarrow \lim_{|x| \to \infty}|f(x)| = 0$$ I already try this but I'm not sure it works: Consider $$A_n = \{x \in \mathbb{R}: |f(x)| \geq n \}$$ Then $$m(A_n)n \leq \int_{A_n}|f(x)| \leq \int_{\mathbb{R}}|f(x)| < \infty$$ Hence, $$m(A_n) \leq \frac{1}{n}\int_{\mathbb{R}}|f(x)| < \infty$$ Thus, $$\lim_{n\to \infty}m(A_n) = 0 = m(\cap_{n = 1}^{\infty}A_n)$$ Therefore, $m(\{x \in \mathbb{R}: |f(x)| = \infty\}) = 0$ Did this implies that $f$ goes to 0 as $|x| \to \infty$? I don't think so. I was wondering if I can use another approach using the fact that it is lebesgue integrable and uniformly continuous. Then the integral of $f$ is the near the integral of a simple function that is bounded and vanish outside a set of finite measure, but I do not know how to attack this problem using this approach.","Show that if $f: \mathbb{R} \to \mathbb{R}$ is uniformly continuous such that $$\int_{-\infty}^{\infty}|f(x)| < \infty \Rightarrow \lim_{|x| \to \infty}|f(x)| = 0$$ I already try this but I'm not sure it works: Consider $$A_n = \{x \in \mathbb{R}: |f(x)| \geq n \}$$ Then $$m(A_n)n \leq \int_{A_n}|f(x)| \leq \int_{\mathbb{R}}|f(x)| < \infty$$ Hence, $$m(A_n) \leq \frac{1}{n}\int_{\mathbb{R}}|f(x)| < \infty$$ Thus, $$\lim_{n\to \infty}m(A_n) = 0 = m(\cap_{n = 1}^{\infty}A_n)$$ Therefore, $m(\{x \in \mathbb{R}: |f(x)| = \infty\}) = 0$ Did this implies that $f$ goes to 0 as $|x| \to \infty$? I don't think so. I was wondering if I can use another approach using the fact that it is lebesgue integrable and uniformly continuous. Then the integral of $f$ is the near the integral of a simple function that is bounded and vanish outside a set of finite measure, but I do not know how to attack this problem using this approach.",,"['real-analysis', 'limits', 'lebesgue-integral', 'uniform-continuity']"
32,solve limit with exponential function using maclaurin series,solve limit with exponential function using maclaurin series,,"Its a bit long so I am sorry, I tried to solve the following limit: $$\lim_{x \rightarrow 1} \left(\frac{x^{x+1}(\ln x+1)-x}{1-x} + x^{1/(x-1)} \right)$$ 1. $$\left[t=1-x ; x=1-t\right]$$ 2. $$\lim_{t \rightarrow 0}(\frac{(1+(-t))^{2-t}(\ln(1-t)+1)-1+t}{t}+(1+(-t))^{\frac{1}{-t}})$$ 3. $$  = \lim_{t \rightarrow 0}\frac{(1+(-t))^{2-t}(\ln(1-t)+1)-1+t}{t}+\lim_{t \rightarrow 0}((1+(-t))^{\frac{1}{-t}})$$ 4. $$ = \lim_{t \rightarrow 0}\frac{(1+(-t))^{2-t}(\ln(1-t)+1)-1+t}{t}+e$$ 5. Using maclaurin series for ln: $$ =\lim_{t \rightarrow 0}\frac{(1+(-t))^{2-t}(-t+O(t)+1)-1+t}{t}+e$$ 6. $$ =\lim_{t \rightarrow 0}\frac{1(-t+O(t)+1)-1+t}{t}+e$$ 7. $$ =e$$ At this point I was pretty happy! But to my deepest disappointment I realized that I made a mistake and the real answer is $e-2$. I had went over this many times and didn't find any arithmetical mistake so I guess the problem is with my logic(which is disturbing).  I'd like to get some advice what is wrong with this path (I know that I can solve this via l'hopital's rule but I want to practice using maclaurin) And another question can some one explain to me how to use the O(x) sign I mean I know that this is a reminder but what are the ground rules? when I can ignore her and what can I do if I have two maclaurin series- can I add the reminders together or subtract them?","Its a bit long so I am sorry, I tried to solve the following limit: $$\lim_{x \rightarrow 1} \left(\frac{x^{x+1}(\ln x+1)-x}{1-x} + x^{1/(x-1)} \right)$$ 1. $$\left[t=1-x ; x=1-t\right]$$ 2. $$\lim_{t \rightarrow 0}(\frac{(1+(-t))^{2-t}(\ln(1-t)+1)-1+t}{t}+(1+(-t))^{\frac{1}{-t}})$$ 3. $$  = \lim_{t \rightarrow 0}\frac{(1+(-t))^{2-t}(\ln(1-t)+1)-1+t}{t}+\lim_{t \rightarrow 0}((1+(-t))^{\frac{1}{-t}})$$ 4. $$ = \lim_{t \rightarrow 0}\frac{(1+(-t))^{2-t}(\ln(1-t)+1)-1+t}{t}+e$$ 5. Using maclaurin series for ln: $$ =\lim_{t \rightarrow 0}\frac{(1+(-t))^{2-t}(-t+O(t)+1)-1+t}{t}+e$$ 6. $$ =\lim_{t \rightarrow 0}\frac{1(-t+O(t)+1)-1+t}{t}+e$$ 7. $$ =e$$ At this point I was pretty happy! But to my deepest disappointment I realized that I made a mistake and the real answer is $e-2$. I had went over this many times and didn't find any arithmetical mistake so I guess the problem is with my logic(which is disturbing).  I'd like to get some advice what is wrong with this path (I know that I can solve this via l'hopital's rule but I want to practice using maclaurin) And another question can some one explain to me how to use the O(x) sign I mean I know that this is a reminder but what are the ground rules? when I can ignore her and what can I do if I have two maclaurin series- can I add the reminders together or subtract them?",,"['limits', 'taylor-expansion', 'exponential-function']"
33,Is $\displaystyle\sum_{k=1}^n \frac{k \sin^2k}{n^2+k \sin^2k}$ convergent?,Is  convergent?,\displaystyle\sum_{k=1}^n \frac{k \sin^2k}{n^2+k \sin^2k},Let $\displaystyle x_n=\sum_{k=1}^n\frac{k \sin^2k}{n^2+k \sin^2k}$ for all $n>0$. How can we prove that $(x_n)$ is convergent?,Let $\displaystyle x_n=\sum_{k=1}^n\frac{k \sin^2k}{n^2+k \sin^2k}$ for all $n>0$. How can we prove that $(x_n)$ is convergent?,,['limits']
34,"determine for which values of $x$ the function $f(x)=\begin{cases} \frac{x^2-4}{x-2}+10, &\text{if }x < 2\\ 2x^3-x, &\text{if } x ≥ 2\\ \end{cases}$",determine for which values of  the function,"x f(x)=\begin{cases} \frac{x^2-4}{x-2}+10, &\text{if }x < 2\\ 2x^3-x, &\text{if } x ≥ 2\\ \end{cases}","is continuous... $$f(x)=\begin{cases} \frac{x^2-4}{x-2}+10, &\text{if }x < 2\\ 2x^3-x, &\text{if } x ≥ 2\\ \end{cases}$$ These kind of problems are very unclear to me, I only know these kind of functions have several graphs, in this example There are 2, But I can't figure out what these intervals means and even worse I don't even know where to start from.","is continuous... $$f(x)=\begin{cases} \frac{x^2-4}{x-2}+10, &\text{if }x < 2\\ 2x^3-x, &\text{if } x ≥ 2\\ \end{cases}$$ These kind of problems are very unclear to me, I only know these kind of functions have several graphs, in this example There are 2, But I can't figure out what these intervals means and even worse I don't even know where to start from.",,"['calculus', 'limits']"
35,A limit involving square roots,A limit involving square roots,,"Transcribed from photo $$\require{cancel} \lim_{n\to\infty}\sqrt{n+\tfrac12}\left(\sqrt{2n+1}-\sqrt{2n+3}\right)\\[12pt] $$ $$ \lim_{n\to\infty}\sqrt{n+\tfrac12}\lim_{n\to\infty}\left(\sqrt{2n+1}-\sqrt{2n+3}\right)\frac{\sqrt{2n+1}+\sqrt{2n+3}}{\sqrt{2n+1}+\sqrt{2n+3}}\\[12pt] $$ $$ \lim_{n\to\infty}\underbrace{\sqrt{\frac{\cancel{n}1}{\cancel{n}}+\cancelto{0}{\frac1{2n}}}}_1\lim_{n\to\infty}\frac{(2n+1)-(2n+3)}{\sqrt{2n+1}+\sqrt{2n+3}}\\[24pt] $$ $$ \lim_{n\to\infty}\frac{(\cancel{2n}+1)-(\cancel{2n}+3)}{\sqrt{2n+1}+\sqrt{2n+3}}=\lim_{n\to\infty}\frac{-2}{\sqrt{2n+1}+\sqrt{2n+3}}\cdot\frac1{\sqrt{n}}\\[12pt] $$ $$ \lim_{n\to\infty}\frac{\cancelto{0}{\frac2{\sqrt{n}}}}{\sqrt{2+\cancelto{0}{\frac1n}}+\sqrt{2+\cancelto{0}{\frac3n}}}=\frac0{2\sqrt2}=0\\[24pt] $$ $$ 0\cdot1=0 $$ I solved it, but i dont know if it is right. I got $0$","Transcribed from photo $$\require{cancel} \lim_{n\to\infty}\sqrt{n+\tfrac12}\left(\sqrt{2n+1}-\sqrt{2n+3}\right)\\[12pt] $$ $$ \lim_{n\to\infty}\sqrt{n+\tfrac12}\lim_{n\to\infty}\left(\sqrt{2n+1}-\sqrt{2n+3}\right)\frac{\sqrt{2n+1}+\sqrt{2n+3}}{\sqrt{2n+1}+\sqrt{2n+3}}\\[12pt] $$ $$ \lim_{n\to\infty}\underbrace{\sqrt{\frac{\cancel{n}1}{\cancel{n}}+\cancelto{0}{\frac1{2n}}}}_1\lim_{n\to\infty}\frac{(2n+1)-(2n+3)}{\sqrt{2n+1}+\sqrt{2n+3}}\\[24pt] $$ $$ \lim_{n\to\infty}\frac{(\cancel{2n}+1)-(\cancel{2n}+3)}{\sqrt{2n+1}+\sqrt{2n+3}}=\lim_{n\to\infty}\frac{-2}{\sqrt{2n+1}+\sqrt{2n+3}}\cdot\frac1{\sqrt{n}}\\[12pt] $$ $$ \lim_{n\to\infty}\frac{\cancelto{0}{\frac2{\sqrt{n}}}}{\sqrt{2+\cancelto{0}{\frac1n}}+\sqrt{2+\cancelto{0}{\frac3n}}}=\frac0{2\sqrt2}=0\\[24pt] $$ $$ 0\cdot1=0 $$ I solved it, but i dont know if it is right. I got $0$",,"['limits', 'radicals']"
36,Evaluating Nested Limits,Evaluating Nested Limits,,"In my attempt to prove $\frac d{dx}\left(e^x\right)=e^x$, I arrived at the following step: $$\frac d{dx}\left(e^x\right)=e^x\lim_{h\to0}\frac{e^h-1}{h}$$ At this point, I substituted in  $$e=\lim_{n\to\infty}{\left(1+\frac 1n\right)^n}$$ to arrive at $$\frac d{dx}\left(e^x\right)=e^x\lim_{h\to0}\frac{\lim_{n\to\infty}{\left(1+\frac 1n\right)^{nh}}-1}{h}$$ Then, I let $h=\frac 1n$ to get $$\frac d{dx}\left(e^x\right)=e^x\lim_{h\to0}\frac{(1+h)^{h\left(\frac 1h\right)}-1}{h}$$ at which point a lot of things simplify and the limit is clearly equal to one. My question concerns the $h=\frac 1n$ step. While I see why this might be true, I don't really have a good conceptual or mathematical grasp of why this works, or whether this is even valid at all. Can the nested limit just be removed that way and $n$ replaced, or is there something else at work here?","In my attempt to prove $\frac d{dx}\left(e^x\right)=e^x$, I arrived at the following step: $$\frac d{dx}\left(e^x\right)=e^x\lim_{h\to0}\frac{e^h-1}{h}$$ At this point, I substituted in  $$e=\lim_{n\to\infty}{\left(1+\frac 1n\right)^n}$$ to arrive at $$\frac d{dx}\left(e^x\right)=e^x\lim_{h\to0}\frac{\lim_{n\to\infty}{\left(1+\frac 1n\right)^{nh}}-1}{h}$$ Then, I let $h=\frac 1n$ to get $$\frac d{dx}\left(e^x\right)=e^x\lim_{h\to0}\frac{(1+h)^{h\left(\frac 1h\right)}-1}{h}$$ at which point a lot of things simplify and the limit is clearly equal to one. My question concerns the $h=\frac 1n$ step. While I see why this might be true, I don't really have a good conceptual or mathematical grasp of why this works, or whether this is even valid at all. Can the nested limit just be removed that way and $n$ replaced, or is there something else at work here?",,"['calculus', 'limits', 'derivatives', 'exponential-function']"
37,Find the limit of $\lim_{x\to\infty}\left(\frac{2^{\frac{1}{x}}+3^{\frac{1}{x}}}{2}\right)^{3x}$,Find the limit of,\lim_{x\to\infty}\left(\frac{2^{\frac{1}{x}}+3^{\frac{1}{x}}}{2}\right)^{3x},"$$\lim_{x\to\infty}\left(\frac{2^{\frac{1}{x}}+3^{\frac{1}{x}}}{2}\right)^{3x}$$ I only managed to show that: $$8=\left(\frac{2^{\frac{1}{x}}+2^{\frac{1}{x}}}{2}\right)^{3x}\leq\left(\frac{2^{\frac{1}{x}}+3^{\frac{1}{x}}}{2}\right)^{3x}\leq\left(\frac{3^{\frac{1}{x}}+3^{\frac{1}{x}}}{2}\right)^{3x}=27$$ I don't know how to bound it better, or maybe there is so simpler way?","$$\lim_{x\to\infty}\left(\frac{2^{\frac{1}{x}}+3^{\frac{1}{x}}}{2}\right)^{3x}$$ I only managed to show that: $$8=\left(\frac{2^{\frac{1}{x}}+2^{\frac{1}{x}}}{2}\right)^{3x}\leq\left(\frac{2^{\frac{1}{x}}+3^{\frac{1}{x}}}{2}\right)^{3x}\leq\left(\frac{3^{\frac{1}{x}}+3^{\frac{1}{x}}}{2}\right)^{3x}=27$$ I don't know how to bound it better, or maybe there is so simpler way?",,['limits']
38,Continuity of a function with two variables,Continuity of a function with two variables,,"Is the function         $f(x)=\frac{x^2y}{x^2+y^2}, f(0,0)=0$ continuous? Is this function differentiable at (0,0)?","Is the function         $f(x)=\frac{x^2y}{x^2+y^2}, f(0,0)=0$ continuous? Is this function differentiable at (0,0)?",,"['limits', 'multivariable-calculus', 'derivatives', 'continuity']"
39,Asymptotics of a double integral,Asymptotics of a double integral,,"I want to  calculate the asymptotic form as $x\to 0$ of the following integral. \begin{alignat}{2} I_2(x) = \int_0^{\infty}du\int_0^{\infty}dv\, \frac{1}{(u+v)^{\frac{3}{2}}}\exp\left(-\frac{x}{u+v}\right)   \end{alignat} How can we solve? This question is related with this post ( Asymptotics of a double integral: $ \int_0^{\infty}du\int_0^{\infty}dv\, \frac{1}{(u+v)^2}\exp\left(-\frac{x}{u+v}\right)$ ),  or the solutions in three-dimensional space for this post ( https://physics.stackexchange.com/questions/61498/a-four-dimensional-integral-in-peskin-schroeder# ) Thank you so much","I want to  calculate the asymptotic form as $x\to 0$ of the following integral. \begin{alignat}{2} I_2(x) = \int_0^{\infty}du\int_0^{\infty}dv\, \frac{1}{(u+v)^{\frac{3}{2}}}\exp\left(-\frac{x}{u+v}\right)   \end{alignat} How can we solve? This question is related with this post ( Asymptotics of a double integral: $ \int_0^{\infty}du\int_0^{\infty}dv\, \frac{1}{(u+v)^2}\exp\left(-\frac{x}{u+v}\right)$ ),  or the solutions in three-dimensional space for this post ( https://physics.stackexchange.com/questions/61498/a-four-dimensional-integral-in-peskin-schroeder# ) Thank you so much",,"['integration', 'limits', 'definite-integrals', 'asymptotics', 'multiple-integral']"
40,Trigonometric Limit tends to $\infty$,Trigonometric Limit tends to,\infty,$$\lim_{x \to 0}\lim_{n \to \infty} \frac{n}{\left(1^\left(\cot^2x \right)+2^\left(\cot^2x \right)+...+n^\left(\cot^2x \right)\right)^\left(\tan^2x \right)}$$ How to approach? Need hints.,$$\lim_{x \to 0}\lim_{n \to \infty} \frac{n}{\left(1^\left(\cot^2x \right)+2^\left(\cot^2x \right)+...+n^\left(\cot^2x \right)\right)^\left(\tan^2x \right)}$$ How to approach? Need hints.,,"['limits', 'limits-without-lhopital']"
41,"""Multi-valued limit"" $\lim_{(s,t)\to (\frac{1}{2}, \frac{1}{2})} \frac{t-s}{1-2s}$","""Multi-valued limit""","\lim_{(s,t)\to (\frac{1}{2}, \frac{1}{2})} \frac{t-s}{1-2s}","I'm working on a larger problem and it would be very helpful if the following were true: For $0\leq s < \frac{1}{2}$ and $s \leq t \leq 1-s$, for every $\varepsilon > 0$ there exists a $\delta > 0$ such that if $\sqrt{(t-\frac{1}{2})^{2}+(s-\frac{1}{2})^{2}} < \delta$ then either   $$\left|\frac{t-s}{1-2s} \right| < \varepsilon \text{$\quad$ or $\quad$} \left|\frac{t-s}{1-2s} - 1 \right| < \varepsilon$$ That is, as $s$ and $t$ both approach $\frac{1}{2}$, the function is either very close to $1$ or very close to $0$. It's not obviously true to me, but it would wrap the problem up nicely and I haven't been able to think of a counter example path.","I'm working on a larger problem and it would be very helpful if the following were true: For $0\leq s < \frac{1}{2}$ and $s \leq t \leq 1-s$, for every $\varepsilon > 0$ there exists a $\delta > 0$ such that if $\sqrt{(t-\frac{1}{2})^{2}+(s-\frac{1}{2})^{2}} < \delta$ then either   $$\left|\frac{t-s}{1-2s} \right| < \varepsilon \text{$\quad$ or $\quad$} \left|\frac{t-s}{1-2s} - 1 \right| < \varepsilon$$ That is, as $s$ and $t$ both approach $\frac{1}{2}$, the function is either very close to $1$ or very close to $0$. It's not obviously true to me, but it would wrap the problem up nicely and I haven't been able to think of a counter example path.",,['real-analysis']
42,Examing a limit of sum of square roots,Examing a limit of sum of square roots,,"Calculate the following limit:   $$\lim\limits_{n \to\infty} \left(\frac{1}{\sqrt{n^2+1}} + \frac{1}{\sqrt{n^2+2}} + ... + \frac{1}{\sqrt{n^2+n}} \right)$$ I think this limit equals $1$. I am not sure. Tried using the squeeze theorem: $$1 \leq  \left(\frac{1}{\sqrt{n^2+1}} + \frac{1}{\sqrt{n^2+2}} + ... + \frac{1}{\sqrt{n^2+n}} \right) \leq n\cdot \frac{1}{\sqrt{n^2}}$$. It's quite clear why the right hand side is bigger than the middle term, but is $1$ really smaller of equals to the middle term? Please note I can't use integrals here nor taylor series. Thanks!","Calculate the following limit:   $$\lim\limits_{n \to\infty} \left(\frac{1}{\sqrt{n^2+1}} + \frac{1}{\sqrt{n^2+2}} + ... + \frac{1}{\sqrt{n^2+n}} \right)$$ I think this limit equals $1$. I am not sure. Tried using the squeeze theorem: $$1 \leq  \left(\frac{1}{\sqrt{n^2+1}} + \frac{1}{\sqrt{n^2+2}} + ... + \frac{1}{\sqrt{n^2+n}} \right) \leq n\cdot \frac{1}{\sqrt{n^2}}$$. It's quite clear why the right hand side is bigger than the middle term, but is $1$ really smaller of equals to the middle term? Please note I can't use integrals here nor taylor series. Thanks!",,"['real-analysis', 'limits']"
43,Prove integral limit exists $\lim\limits_{t\to 0^+}\int\limits_0^1\frac{dx}{(x^4+t^4)^{1/4}}+\ln t$,Prove integral limit exists,\lim\limits_{t\to 0^+}\int\limits_0^1\frac{dx}{(x^4+t^4)^{1/4}}+\ln t,Prove integral limit exists $$\lim\limits_{t\to 0^+}\left(\int\limits_0^1\frac{dx}{(x^4+t^4)^{1/4}}+\ln t\right)$$ I try to change variable $u=1/x$ then $\displaystyle\int\limits_0^1\frac{dx}{(x^4+t^4)^{1/4}}=\frac{1}{4}\int\limits_1^\infty\frac{4u^3du}{u^4(1+u^4x^4)^{1/4}}$. But I have no idea to continue.,Prove integral limit exists $$\lim\limits_{t\to 0^+}\left(\int\limits_0^1\frac{dx}{(x^4+t^4)^{1/4}}+\ln t\right)$$ I try to change variable $u=1/x$ then $\displaystyle\int\limits_0^1\frac{dx}{(x^4+t^4)^{1/4}}=\frac{1}{4}\int\limits_1^\infty\frac{4u^3du}{u^4(1+u^4x^4)^{1/4}}$. But I have no idea to continue.,,"['limits', 'definite-integrals', 'improper-integrals']"
44,Find the limit of a function as $ x$ approaches $0$,Find the limit of a function as  approaches, x 0,"How can I find the limit of $\dfrac{\cos(3x) - 1 }{x^2}$ as $x\to 0$? If someone could please break down the steps, for clear understanding. I'm studying for the GRE. Thanks in advance !!","How can I find the limit of $\dfrac{\cos(3x) - 1 }{x^2}$ as $x\to 0$? If someone could please break down the steps, for clear understanding. I'm studying for the GRE. Thanks in advance !!",,"['calculus', 'limits', 'limits-without-lhopital']"
45,Finding limit using inequalities: $\liminf \frac{a_{n+1}}{a_n} \le \liminf (a_n)^ {1/n}\le\limsup (a_n)^ {1/n}\le \limsup \frac{a_{n+1}}{a_n}$ [duplicate],Finding limit using inequalities:  [duplicate],\liminf \frac{a_{n+1}}{a_n} \le \liminf (a_n)^ {1/n}\le\limsup (a_n)^ {1/n}\le \limsup \frac{a_{n+1}}{a_n},"This question already has an answer here : Inequality involving $\limsup$ and $\liminf$: $ \liminf(a_{n+1}/a_n) \le \liminf a_n^{1/n} \le \limsup a_n^{1/n} \le \limsup(a_{n+1}/a_n)$ (1 answer) Closed 7 years ago . The purpose of this exercise is to prove that $\lim \frac{n}{(n!)^{1/n}}=e$ when $n$ goes to infinity. In order to find the limit, the following inequality is used when $n$ goes to infinity with ${a_n}$ a sequence of positive terms: $$ \liminf \frac{a_{n+1}}{a_n} \le \liminf (a_n)^ {1/n}\le\limsup (a_n)^ {1/n}\le \limsup \frac{a_{n+1}}{a_n}$$ What is the proof of this inequality?","This question already has an answer here : Inequality involving $\limsup$ and $\liminf$: $ \liminf(a_{n+1}/a_n) \le \liminf a_n^{1/n} \le \limsup a_n^{1/n} \le \limsup(a_{n+1}/a_n)$ (1 answer) Closed 7 years ago . The purpose of this exercise is to prove that $\lim \frac{n}{(n!)^{1/n}}=e$ when $n$ goes to infinity. In order to find the limit, the following inequality is used when $n$ goes to infinity with ${a_n}$ a sequence of positive terms: $$ \liminf \frac{a_{n+1}}{a_n} \le \liminf (a_n)^ {1/n}\le\limsup (a_n)^ {1/n}\le \limsup \frac{a_{n+1}}{a_n}$$ What is the proof of this inequality?",,"['real-analysis', 'limits', 'inequality', 'limsup-and-liminf']"
46,Limit of differences of products,Limit of differences of products,,"Is the following true: If $\displaystyle\lim_{k\to\infty}a_k=A$ and $\displaystyle\lim_{k\to\infty}c_k=C$ then $$\lim_{k\to\infty}\Big(a_kb_k-c_kd_k\Big)=\lim_{k\to\infty}\Big(Ab_k-Cd_k\Big).$$ This equality could be either finite or infinite, and nothing is assumed about either $b_k$ or $d_k$. I suspect it's not true when the convergence of $a_k$ and $c_k$ is too slow but I can't put together a proof.","Is the following true: If $\displaystyle\lim_{k\to\infty}a_k=A$ and $\displaystyle\lim_{k\to\infty}c_k=C$ then $$\lim_{k\to\infty}\Big(a_kb_k-c_kd_k\Big)=\lim_{k\to\infty}\Big(Ab_k-Cd_k\Big).$$ This equality could be either finite or infinite, and nothing is assumed about either $b_k$ or $d_k$. I suspect it's not true when the convergence of $a_k$ and $c_k$ is too slow but I can't put together a proof.",,"['calculus', 'real-analysis', 'analysis', 'limits']"
47,Prove that the limit is irrational,Prove that the limit is irrational,,"Let be $(a_{n})_{n\geq 1}$ a sequence of natural numbers, so $a_{n-1}$ divides $a_{n},n\geq 2$ and $\frac{a_{n}}{a_{n-1}}\geq n-\frac{1}{n}, n\geq 2$. Prove that $x_{n}=\frac{1}{a_{1}}+\frac{1}{a_{2}}+..+\frac{1}{a_{n}}$ is convergent and its limit is an irrational number.","Let be $(a_{n})_{n\geq 1}$ a sequence of natural numbers, so $a_{n-1}$ divides $a_{n},n\geq 2$ and $\frac{a_{n}}{a_{n-1}}\geq n-\frac{1}{n}, n\geq 2$. Prove that $x_{n}=\frac{1}{a_{1}}+\frac{1}{a_{2}}+..+\frac{1}{a_{n}}$ is convergent and its limit is an irrational number.",,"['real-analysis', 'limits']"
48,General rule for the $n^{th}$ order derivative of a function to exist,General rule for the  order derivative of a function to exist,n^{th},"I've just got this general rule for the $n^{th}$ order derivative of a function to exist: A function $f(x)$ is differentiable $n$ times only if: $$\lim_{h\rightarrow 0^+}\frac{\sum_{r=0}^{n}(-1)^r\cdot \binom{n}{r}\cdot f(x-(n-r)h)}{(-h)^n}=\lim_{h\rightarrow 0^+}\frac{\sum_{r=0}^n(-1)^r\cdot \binom{n}{r}\cdot f(x+(n-r)h)}{h^n}$$ Please not that $h$ itself is assumed to be positive in both the limits. That's why I've written that $h\rightarrow 0^+$ in both. I call the LHS the Left-hand $n^{th}$ derivative and the RHS, the right hand $n^{th}$ order derivative. Have I got this result correct or Can counter-examples be given against it (examples in which the $n^{th}$ order derivative of a function does not exist but still these two limits are equal or examples in which the $n^{th}$ derivative of a function exists but these two limits are unequal)? EDIT: Here, $x$ can be replaced by $c$ to check the differentiability at a particular point $x=c$. UPDATE: After the answer by mjqxxxx, I guess that the requirement is that the first $n-1$ derivatives should exist, only then the formula can be used.","I've just got this general rule for the $n^{th}$ order derivative of a function to exist: A function $f(x)$ is differentiable $n$ times only if: $$\lim_{h\rightarrow 0^+}\frac{\sum_{r=0}^{n}(-1)^r\cdot \binom{n}{r}\cdot f(x-(n-r)h)}{(-h)^n}=\lim_{h\rightarrow 0^+}\frac{\sum_{r=0}^n(-1)^r\cdot \binom{n}{r}\cdot f(x+(n-r)h)}{h^n}$$ Please not that $h$ itself is assumed to be positive in both the limits. That's why I've written that $h\rightarrow 0^+$ in both. I call the LHS the Left-hand $n^{th}$ derivative and the RHS, the right hand $n^{th}$ order derivative. Have I got this result correct or Can counter-examples be given against it (examples in which the $n^{th}$ order derivative of a function does not exist but still these two limits are equal or examples in which the $n^{th}$ derivative of a function exists but these two limits are unequal)? EDIT: Here, $x$ can be replaced by $c$ to check the differentiability at a particular point $x=c$. UPDATE: After the answer by mjqxxxx, I guess that the requirement is that the first $n-1$ derivatives should exist, only then the formula can be used.",,"['calculus', 'limits']"
49,Evaluate $\lim_{t\to0}\frac{-(t-2)(\sin t +1)-(t+2)\cos t}{(\sin t + \cos t - 1)t^2}$ without L'hopital,Evaluate  without L'hopital,\lim_{t\to0}\frac{-(t-2)(\sin t +1)-(t+2)\cos t}{(\sin t + \cos t - 1)t^2},Question: Let   \begin{align} &S(t):=\int_{\pi/4}^t (\sin t-\sin\left(\frac\pi4\right))dt\\ &T(t):=\frac{\left(\sin t-\sin\left(\frac\pi4\right)\right)\left(t-\frac\pi4\right)}2\\ \end{align}   Using $$\lim_{t\to0}\frac{\tan t - t}{t^3}=\frac13\tag1$$   Evaluate the following (without L'hopital)   \begin{align} &\quad\lim_{t\to\frac\pi4}\frac{S(t)-T(t)}{T(t)\left(t-\frac\pi4\right)} \end{align} What I've done so far is: $$\lim_{t\to\frac\pi4}\frac{S(t)-T(t)}{T(t)\left(t-\frac\pi4\right)}=\lim_{t\to0}\frac{-\cos\left(t+\frac\pi4\right)+\frac{\sqrt2}2-\left(\sin\left(t+\frac\pi4\right)+\frac{\sqrt2}2\right)\frac t2}{\left(\sin\left(t+\frac\pi4\right)-\frac{\sqrt2}2\right)\frac {t^2}2}$$ $$=\lim_{t\to0}\frac{-(t-2)(\sin t +1)-(t+2)\cos t}{(\sin t + \cos t - 1)t^2}$$ But I don't know how to go from here to the Eq.$(1)$. Thanks.,Question: Let   \begin{align} &S(t):=\int_{\pi/4}^t (\sin t-\sin\left(\frac\pi4\right))dt\\ &T(t):=\frac{\left(\sin t-\sin\left(\frac\pi4\right)\right)\left(t-\frac\pi4\right)}2\\ \end{align}   Using $$\lim_{t\to0}\frac{\tan t - t}{t^3}=\frac13\tag1$$   Evaluate the following (without L'hopital)   \begin{align} &\quad\lim_{t\to\frac\pi4}\frac{S(t)-T(t)}{T(t)\left(t-\frac\pi4\right)} \end{align} What I've done so far is: $$\lim_{t\to\frac\pi4}\frac{S(t)-T(t)}{T(t)\left(t-\frac\pi4\right)}=\lim_{t\to0}\frac{-\cos\left(t+\frac\pi4\right)+\frac{\sqrt2}2-\left(\sin\left(t+\frac\pi4\right)+\frac{\sqrt2}2\right)\frac t2}{\left(\sin\left(t+\frac\pi4\right)-\frac{\sqrt2}2\right)\frac {t^2}2}$$ $$=\lim_{t\to0}\frac{-(t-2)(\sin t +1)-(t+2)\cos t}{(\sin t + \cos t - 1)t^2}$$ But I don't know how to go from here to the Eq.$(1)$. Thanks.,,"['calculus', 'limits', 'limits-without-lhopital']"
50,"Is this limit $\lim\limits_{x \to\, -8}\frac{\sqrt{1-x}-3}{2+\sqrt[3]{x}} = 0$?",Is this limit ?,"\lim\limits_{x \to\, -8}\frac{\sqrt{1-x}-3}{2+\sqrt[3]{x}} = 0","Is this limit $\lim\limits_{x \to\, -8}\frac{\sqrt{1-x}-3}{2+\sqrt[3]{x}} = 0$? It's in indeterminant form, 0/0 when $x$ approaches $-8$. So I used LHopital's rule and got $$-\frac{3x^{\frac{2}{3}}}{2\sqrt{1-x}}$$ plug in $-8$ it is $-2(-1)^{2/3}$ which is imaginary. I used wolframalpha, the answer is $0$. So, which is correct?","Is this limit $\lim\limits_{x \to\, -8}\frac{\sqrt{1-x}-3}{2+\sqrt[3]{x}} = 0$? It's in indeterminant form, 0/0 when $x$ approaches $-8$. So I used LHopital's rule and got $$-\frac{3x^{\frac{2}{3}}}{2\sqrt{1-x}}$$ plug in $-8$ it is $-2(-1)^{2/3}$ which is imaginary. I used wolframalpha, the answer is $0$. So, which is correct?",,"['limits', 'limits-without-lhopital', 'indeterminate-forms']"
51,Regarding $e$ in $\lim\limits_{x \to a}{[\phi(x)]^{\psi(x)}} = e^{\lim\limits_{x \to a}{[\phi(x) - 1]\psi(x)}}$,Regarding  in,e \lim\limits_{x \to a}{[\phi(x)]^{\psi(x)}} = e^{\lim\limits_{x \to a}{[\phi(x) - 1]\psi(x)}},"I'm currently studying limits with $x$ in the exponent. The following formula simplifies the work to solve limits. If $\lim\limits_{x \to a}{\phi(x)} = 1$ and $\lim\limits_{x \to a}{\psi(x)} = \infty$ , then $\lim\limits_{x \to a}{[\phi(x)]^{\psi(x)}} =$ $\lim\limits_{x \to a}{\{[1+\alpha(x)]^{\frac{1}{\alpha(x)}}\}^{\alpha(x)\psi(x)}} =$ $e^{\lim\limits_{x \to a}{[\phi(x) - 1]\psi(x)}}$ For the most part, I understand how the formula is derived. However, there's one part I don't understand. Why does $\lim\limits_{x \to a}{\{[1+\alpha(x)]^{\frac{1}{\alpha(x)}}\}} = e$ ?","I'm currently studying limits with in the exponent. The following formula simplifies the work to solve limits. If and , then For the most part, I understand how the formula is derived. However, there's one part I don't understand. Why does ?",x \lim\limits_{x \to a}{\phi(x)} = 1 \lim\limits_{x \to a}{\psi(x)} = \infty \lim\limits_{x \to a}{[\phi(x)]^{\psi(x)}} = \lim\limits_{x \to a}{\{[1+\alpha(x)]^{\frac{1}{\alpha(x)}}\}^{\alpha(x)\psi(x)}} = e^{\lim\limits_{x \to a}{[\phi(x) - 1]\psi(x)}} \lim\limits_{x \to a}{\{[1+\alpha(x)]^{\frac{1}{\alpha(x)}}\}} = e,"['calculus', 'real-analysis', 'limits', 'exponential-function', 'proof-explanation']"
52,How can I find $\lim \limits_{n\to \infty} \frac{\ln(q_{n+1})}{\ln(q_n)}$?,How can I find ?,\lim \limits_{n\to \infty} \frac{\ln(q_{n+1})}{\ln(q_n)},"The irrationality measure of a positive real number $x$ can be calculated, if the following limit can be calculated, where $q_n$ are the convergents of the simple continued fraction of $x$. $$\lim \limits_{n\to \infty} \frac{\ln(q_{n+1})}{\ln(q_n)}$$ Suppose, I have a formula for the entries of the simple continued fraction. Lets say, $a_n=2^n$, so the continued fraction would be $[2,4,8,16,32,\cdots]$. How can I calculate the above limit ? And how can I find out whether it exists ? Is it possible that the values oscillate and that neither the limit exists nor the sequence diverges to $\infty$ ? I know the reccurence relation $$q_1=1\ , \,q_2=a_2\ ,\ q_n=a_nq_{n-1}+q_{n-2}\ for\ n>2$$ but in general, it will be difficult to find a closed form for $q_n$. The goal is to find a method to prove special numbers with known continued fraction to be transcendental, for which is enough to show that the above limit is greater than $1$.","The irrationality measure of a positive real number $x$ can be calculated, if the following limit can be calculated, where $q_n$ are the convergents of the simple continued fraction of $x$. $$\lim \limits_{n\to \infty} \frac{\ln(q_{n+1})}{\ln(q_n)}$$ Suppose, I have a formula for the entries of the simple continued fraction. Lets say, $a_n=2^n$, so the continued fraction would be $[2,4,8,16,32,\cdots]$. How can I calculate the above limit ? And how can I find out whether it exists ? Is it possible that the values oscillate and that neither the limit exists nor the sequence diverges to $\infty$ ? I know the reccurence relation $$q_1=1\ , \,q_2=a_2\ ,\ q_n=a_nq_{n-1}+q_{n-2}\ for\ n>2$$ but in general, it will be difficult to find a closed form for $q_n$. The goal is to find a method to prove special numbers with known continued fraction to be transcendental, for which is enough to show that the above limit is greater than $1$.",,"['sequences-and-series', 'limits', 'continued-fractions']"
53,Evaluate $\lim_{x\to0} \big((1+x)^x-1\big)^x$,Evaluate,\lim_{x\to0} \big((1+x)^x-1\big)^x,"How can one evaluate this limit? $$\lim_{x\to0} \big((1+x)^x-1\big)^x$$ I've already tried writing that using $f(x) = e^{\ln f(x)}$, but I don't dare going any further with this approach.","How can one evaluate this limit? $$\lim_{x\to0} \big((1+x)^x-1\big)^x$$ I've already tried writing that using $f(x) = e^{\ln f(x)}$, but I don't dare going any further with this approach.",,"['limits', 'exponential-function']"
54,Calculate: $\lim_n\frac{\ln(2^{\frac{1}{n}})-\ln(n^2)}{1+\frac{1}{2}+...+\frac{1}{n}}$,Calculate:,\lim_n\frac{\ln(2^{\frac{1}{n}})-\ln(n^2)}{1+\frac{1}{2}+...+\frac{1}{n}},Find the following limit: $$\lim_n\frac{\ln(2^{\frac{1}{n}})-\ln(n^2)}{1+\frac{1}{2}+...+\frac{1}{n}}$$ I tried this: $$\lim_n \frac{\ln(2^{\frac{1}{n}})-\ln(n^2)}{1+\frac{1}{2}+...+\frac{1}{n}}=\frac{\lim_n \ln \frac{2^{\frac{1}{n}}}{n^2}}{\lim_n 1+\frac{1}{2}+...+\frac{1}{n}}=\frac{\ln \lim_n \frac{2^{\frac{1}{n}}}{n^2}}{\lim_n 1+\frac{1}{2}+...+\frac{1}{n}}$$ But then I get $\ln 0$ in the numerator which is undefined. Also I have no idea what to do with the denominator. Any help is appreciated.,Find the following limit: $$\lim_n\frac{\ln(2^{\frac{1}{n}})-\ln(n^2)}{1+\frac{1}{2}+...+\frac{1}{n}}$$ I tried this: $$\lim_n \frac{\ln(2^{\frac{1}{n}})-\ln(n^2)}{1+\frac{1}{2}+...+\frac{1}{n}}=\frac{\lim_n \ln \frac{2^{\frac{1}{n}}}{n^2}}{\lim_n 1+\frac{1}{2}+...+\frac{1}{n}}=\frac{\ln \lim_n \frac{2^{\frac{1}{n}}}{n^2}}{\lim_n 1+\frac{1}{2}+...+\frac{1}{n}}$$ But then I get $\ln 0$ in the numerator which is undefined. Also I have no idea what to do with the denominator. Any help is appreciated.,,"['calculus', 'limits', 'limits-without-lhopital']"
55,Check Convergence of the series: $\sum_{n=1}^\infty {\frac{ \sqrt{n+1} - \sqrt{n}}{\sqrt{n}}}$,Check Convergence of the series:,\sum_{n=1}^\infty {\frac{ \sqrt{n+1} - \sqrt{n}}{\sqrt{n}}},I have to check the convergence of this series: $$\sum_{n=1}^\infty {\frac{ \sqrt{n+1} - \sqrt{n}}{\sqrt{n}}}$$ Which is equal to $$\sum_{n=1}^\infty \frac{\sqrt{n+1}}{\sqrt{n}} - 1$$ What can I do here to check whether this series convergences or not? Thank you very much.,I have to check the convergence of this series: $$\sum_{n=1}^\infty {\frac{ \sqrt{n+1} - \sqrt{n}}{\sqrt{n}}}$$ Which is equal to $$\sum_{n=1}^\infty \frac{\sqrt{n+1}}{\sqrt{n}} - 1$$ What can I do here to check whether this series convergences or not? Thank you very much.,,"['calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
56,Alternative definition of derivative,Alternative definition of derivative,,"Consider the following alternative definition of the derivative of a function $f:\mathbb R\to\mathbb R$ at a limit point $x$ of the domain of $f$: $$f'(x)=\lim_{x_1,x_2\to x}\frac{f(x_2)-f(x_1)}{x_2-x_1},$$ where $\lim_{x_1,x_2\to x}\frac{f(x_2)-f(x_1)}{x_2-x_1}$ is the $a\in\mathbb R$ such that for every $\epsilon>0$ there is a $\delta>0$ such that $|\frac{f(x_2)-f(x_1)}{x_2-x_1}-a|<\epsilon$ whenever $x_1$ and $x_2$ are in the domain of $f$, $x_1\neq x_2$, and $\max\{|x-x_1|,|x-x_2|\}<\delta$. What would happen to calculus if we replaced the usual definition with this one? Could the theory be developed in more or less the same way? Would all the major theorems still hold? Let me be clear that I am not asking whether this definition is strictly equivalent to the normal one. (In fact, I'm pretty sure it's not: I think that when under the normal definition $f'(x)$ is defined but $f'$ is discontinuous at $x$, $f'(x)$ is not defined under the alternative definition.) I'm asking something a little more vague: could we do pretty much the same thing with this definition?","Consider the following alternative definition of the derivative of a function $f:\mathbb R\to\mathbb R$ at a limit point $x$ of the domain of $f$: $$f'(x)=\lim_{x_1,x_2\to x}\frac{f(x_2)-f(x_1)}{x_2-x_1},$$ where $\lim_{x_1,x_2\to x}\frac{f(x_2)-f(x_1)}{x_2-x_1}$ is the $a\in\mathbb R$ such that for every $\epsilon>0$ there is a $\delta>0$ such that $|\frac{f(x_2)-f(x_1)}{x_2-x_1}-a|<\epsilon$ whenever $x_1$ and $x_2$ are in the domain of $f$, $x_1\neq x_2$, and $\max\{|x-x_1|,|x-x_2|\}<\delta$. What would happen to calculus if we replaced the usual definition with this one? Could the theory be developed in more or less the same way? Would all the major theorems still hold? Let me be clear that I am not asking whether this definition is strictly equivalent to the normal one. (In fact, I'm pretty sure it's not: I think that when under the normal definition $f'(x)$ is defined but $f'$ is discontinuous at $x$, $f'(x)$ is not defined under the alternative definition.) I'm asking something a little more vague: could we do pretty much the same thing with this definition?",,"['real-analysis', 'limits', 'derivatives']"
57,Prove that $\lim_{x\rightarrow 0}\frac{1}{x^{4}}=\infty $,Prove that,\lim_{x\rightarrow 0}\frac{1}{x^{4}}=\infty ,"Please check my proof :) We suppose that we are given $M\gt0$ we must find $\delta $ that $0<|x-0|<\delta \rightarrow \frac{1}{x^{4}}>M$ $$\frac{1}{x^{4}}>M$$ $$\frac{1}{M}>x^{4}$$ $$\frac{1}{\sqrt[4]{M}}>x$$ Choose $\delta =\frac{1}{\sqrt[4]{M}}$ Then $\frac{1}{\sqrt[4]{M}}>x\leftrightarrow \frac{1}{x}>\sqrt[4]{M}$ Then, limit is $\infty $","Please check my proof :) We suppose that we are given $M\gt0$ we must find $\delta $ that $0<|x-0|<\delta \rightarrow \frac{1}{x^{4}}>M$ $$\frac{1}{x^{4}}>M$$ $$\frac{1}{M}>x^{4}$$ $$\frac{1}{\sqrt[4]{M}}>x$$ Choose $\delta =\frac{1}{\sqrt[4]{M}}$ Then $\frac{1}{\sqrt[4]{M}}>x\leftrightarrow \frac{1}{x}>\sqrt[4]{M}$ Then, limit is $\infty $",,"['real-analysis', 'limits', 'proof-verification']"
58,Why do I first need to bring $-4x$ into the numerator in $\lim_{x\to \infty} 4x^2/(x-2) - 4x$,Why do I first need to bring  into the numerator in,-4x \lim_{x\to \infty} 4x^2/(x-2) - 4x,"I tried solving the question in the title as follows: $$\lim_{x\to \infty} \frac{4x^2}{x-2} - 4x \to 4x - 4x \to 0$$ However, apparently that first step ($\to 4x - 4x$) was wrong, and I should first have brought the second $4x$ into the numerator. My question is not how I need to solve the question, as I know that now. My question is why what I did was wrong, as I lack any intuition for it, and it seems a mystery to me.","I tried solving the question in the title as follows: $$\lim_{x\to \infty} \frac{4x^2}{x-2} - 4x \to 4x - 4x \to 0$$ However, apparently that first step ($\to 4x - 4x$) was wrong, and I should first have brought the second $4x$ into the numerator. My question is not how I need to solve the question, as I know that now. My question is why what I did was wrong, as I lack any intuition for it, and it seems a mystery to me.",,['limits']
59,Prove $a_{n}=\frac{- (\ln n)^2}{n + \ln n} \rightarrow 0$,Prove,a_{n}=\frac{- (\ln n)^2}{n + \ln n} \rightarrow 0,"Prove $$a_{n}=\frac{- (\ln n)^2}{n + \ln n} \rightarrow 0$$ So I know the fact that $\lim_{n \rightarrow+\infty}\frac{log_{a}n}{n^{\epsilon}}=0$ (($\epsilon>1$, $a>0, a\neq0$)). Is it useful?","Prove $$a_{n}=\frac{- (\ln n)^2}{n + \ln n} \rightarrow 0$$ So I know the fact that $\lim_{n \rightarrow+\infty}\frac{log_{a}n}{n^{\epsilon}}=0$ (($\epsilon>1$, $a>0, a\neq0$)). Is it useful?",,"['calculus', 'sequences-and-series', 'limits']"
60,A more concise way of showing the limit of $a_n := (n^{3n})/(n3^n)$,A more concise way of showing the limit of,a_n := (n^{3n})/(n3^n),"I am trying to show the limit of the sequence $a_n:=\frac{n^{3n}}{n3^n}$ I have done it using the ratio test for limits as I will illustrate below, but I feel like there is an easier method but cannot find one; it would sate my curiosity to see it done in less lines: Let $a_{n+1}:=\frac{(n+1)^{3n+3}}{(n+1)3^{n+1}}$ $$\frac{a_{n+1}}{a_n}=\frac{(n+1)^{3n+3}}{(n+1)3^{n+1}}\times\frac{n3^n}{n^{3n}}=\frac{n}{3}(n+1)^2(1+\frac{1}{n})^{3n}\ge \frac{n}{3}(n+1)$$ We are utilising the comparison test to observe that $\frac{a_{n+1}}{a_n}$ is greater than a sequence which clearly tends to infinity, and thus by the ratio test $lim\frac{a_{n+1}}{a_n}>1$ as n tends to infinity, so the limit of $a_n=+\infty$ as $n \rightarrow+\infty$ Could anyone provide a shorter method which doesn't use the ratio test maybe? Thanks!","I am trying to show the limit of the sequence $a_n:=\frac{n^{3n}}{n3^n}$ I have done it using the ratio test for limits as I will illustrate below, but I feel like there is an easier method but cannot find one; it would sate my curiosity to see it done in less lines: Let $a_{n+1}:=\frac{(n+1)^{3n+3}}{(n+1)3^{n+1}}$ $$\frac{a_{n+1}}{a_n}=\frac{(n+1)^{3n+3}}{(n+1)3^{n+1}}\times\frac{n3^n}{n^{3n}}=\frac{n}{3}(n+1)^2(1+\frac{1}{n})^{3n}\ge \frac{n}{3}(n+1)$$ We are utilising the comparison test to observe that $\frac{a_{n+1}}{a_n}$ is greater than a sequence which clearly tends to infinity, and thus by the ratio test $lim\frac{a_{n+1}}{a_n}>1$ as n tends to infinity, so the limit of $a_n=+\infty$ as $n \rightarrow+\infty$ Could anyone provide a shorter method which doesn't use the ratio test maybe? Thanks!",,"['real-analysis', 'limits']"
61,Compute $ \lim_{n\rightarrow \infty }\sum_{k=6}^{n}\frac{k^{3}-12k^{2}+47k-60}{k^{5}-5k^{3}+4k} $,Compute, \lim_{n\rightarrow \infty }\sum_{k=6}^{n}\frac{k^{3}-12k^{2}+47k-60}{k^{5}-5k^{3}+4k} ,Calculate $ \lim_{n\rightarrow \infty }\sum_{k=6}^{n}\frac{k^{3}-12k^{2}+47k-60}{k^{5}-5k^{3}+4k} $. So far I found that $ \frac{k^{3}-12k^{2}+47k-60}{k^{5}-5k^{3}+4k}=\frac{(k-5)(k-4)(k-3)}{(k-2)(k-1)k(k+1)(k+2)}=\frac{((k-3)!)^{2}}{(k-6)!\cdot (k+2)!}. $,Calculate $ \lim_{n\rightarrow \infty }\sum_{k=6}^{n}\frac{k^{3}-12k^{2}+47k-60}{k^{5}-5k^{3}+4k} $. So far I found that $ \frac{k^{3}-12k^{2}+47k-60}{k^{5}-5k^{3}+4k}=\frac{(k-5)(k-4)(k-3)}{(k-2)(k-1)k(k+1)(k+2)}=\frac{((k-3)!)^{2}}{(k-6)!\cdot (k+2)!}. $,,"['real-analysis', 'sequences-and-series', 'limits']"
62,find limit when x is infinity [duplicate],find limit when x is infinity [duplicate],,This question already has answers here : Prove that $\lim_{x\to \infty}\left(x-\ln\cosh x\right)=\ln 2$ (6 answers) Closed 7 years ago . How can we find the following limit. $$\lim_{x\to\infty}(x-\ln\cosh x)$$ where $$\cosh t=\frac{e^t+e^{-t}}{2}.$$ I thought about it alot but didn't get any start,This question already has answers here : Prove that $\lim_{x\to \infty}\left(x-\ln\cosh x\right)=\ln 2$ (6 answers) Closed 7 years ago . How can we find the following limit. $$\lim_{x\to\infty}(x-\ln\cosh x)$$ where $$\cosh t=\frac{e^t+e^{-t}}{2}.$$ I thought about it alot but didn't get any start,,"['calculus', 'limits']"
63,"Asymptotics of a double integral: $ \int_0^{\infty}du\int_0^{\infty}dv\, \frac{1}{(u+v)^2}\exp\left(-\frac{x}{u+v}\right)$",Asymptotics of a double integral:," \int_0^{\infty}du\int_0^{\infty}dv\, \frac{1}{(u+v)^2}\exp\left(-\frac{x}{u+v}\right)","I want to calculate the asymptotic form as $x \to 0$ of the following integral. \begin{alignat}{2} I_2(x) &=&& \int_0^{\infty}du\int_0^{\infty}dv\, \frac{1}{(u+v)^2}\exp\left(-\frac{x}{u+v}\right) \\ &=&& \frac{\partial^2}{\partial x^2} \int_0^{\infty}du\int_0^{\infty}dv\, \exp\left(-\frac{x}{u+v}\right) \end{alignat} How can we solve? This question is related with this post . Thanks.","I want to calculate the asymptotic form as $x \to 0$ of the following integral. \begin{alignat}{2} I_2(x) &=&& \int_0^{\infty}du\int_0^{\infty}dv\, \frac{1}{(u+v)^2}\exp\left(-\frac{x}{u+v}\right) \\ &=&& \frac{\partial^2}{\partial x^2} \int_0^{\infty}du\int_0^{\infty}dv\, \exp\left(-\frac{x}{u+v}\right) \end{alignat} How can we solve? This question is related with this post . Thanks.",,"['integration', 'limits', 'definite-integrals', 'asymptotics', 'multiple-integral']"
64,"Prove the limit of $(x+y)/(x^2+y^2)$ as $(x,y)$ approaches $(0,0)$ does not exist",Prove the limit of  as  approaches  does not exist,"(x+y)/(x^2+y^2) (x,y) (0,0)","How to prove that the limit of $$ \frac{x+y}{x^2+y^2} $$ as $(x,y)$ approaches $(0,0)$ does not exist? I tried with $y = x$ , $x = 0$ , $y = 0$, which gives the limit of negative infinity and positive infinity, Somehow, I feel It's not correct.  Any better ways?","How to prove that the limit of $$ \frac{x+y}{x^2+y^2} $$ as $(x,y)$ approaches $(0,0)$ does not exist? I tried with $y = x$ , $x = 0$ , $y = 0$, which gives the limit of negative infinity and positive infinity, Somehow, I feel It's not correct.  Any better ways?",,"['limits', 'multivariable-calculus']"
65,Evaluate a given limit,Evaluate a given limit,,Evaluate the following limit: $$\lim_{x \rightarrow 4} \left( \frac{1}{13 - 3x} \right) ^ {\tan \frac{\pi x}{8}}$$ I haven't managed to get anything meaningful yet. Thank you in advance!,Evaluate the following limit: $$\lim_{x \rightarrow 4} \left( \frac{1}{13 - 3x} \right) ^ {\tan \frac{\pi x}{8}}$$ I haven't managed to get anything meaningful yet. Thank you in advance!,,['limits']
66,Evaluate limit $\lim_\limits{x \to 0} \frac {\ln(\tan 2x )}{\ln(\tan 3x )} $,Evaluate limit,\lim_\limits{x \to 0} \frac {\ln(\tan 2x )}{\ln(\tan 3x )} ,"I'm supposed to evaluate this limit using L'Hopital's rule. $$ \lim_{x \to 0} \frac {\ln(\tan 2x )}{\ln(\tan 3x )} $$ I find the indeterminate form of $\frac{0}{0}$. The latter tells me that L'Hopital's is an option, after applying the rule once I end up with: $$ \lim_{x \to 0} \frac {\frac {1}{\tan 2x}*\sec^2(2x)*2}{\frac {1}{\tan 3x}*\sec^2(3x)*3} $$ After this step however I seem to get lost in my own translation time and time again. Can someone point me in the right direction?","I'm supposed to evaluate this limit using L'Hopital's rule. $$ \lim_{x \to 0} \frac {\ln(\tan 2x )}{\ln(\tan 3x )} $$ I find the indeterminate form of $\frac{0}{0}$. The latter tells me that L'Hopital's is an option, after applying the rule once I end up with: $$ \lim_{x \to 0} \frac {\frac {1}{\tan 2x}*\sec^2(2x)*2}{\frac {1}{\tan 3x}*\sec^2(3x)*3} $$ After this step however I seem to get lost in my own translation time and time again. Can someone point me in the right direction?",,"['limits', 'trigonometry']"
67,Do we need the axiom of choice to prove L'Hopital's rule?,Do we need the axiom of choice to prove L'Hopital's rule?,,"I was looking over an honors-calculus proof of L'Hopital's rule today, and I couldn't help but feel a sense of unease. The proof states that $f(x)$ and $g(x)$ are continuous on $[a,b]$; differentiable on $(a,b)$; $\lim_{x\to a^+} \tfrac{f'(x)}{g'(x)} = L$ exists; and that $\lim_{x\to a^+} f(x) = \lim_{x\to a^+} g(x) = 0$. WLOG, we may assume that $f(a)=g(a)=0$. It then proceeds by saying that for any given $x\in(a,b)$, $\;f$ and $g$ are continuous on $[a,x]$ and differentiable on $(a,x)$. Therefore by the Cauchy mean value theorem there must exist a number $\alpha_x\in(a,x)$ for which $$  \frac{f'(\alpha_x)}{g'(\alpha_x)} = \frac{f(x)-f(a)}{g(x)-g(a)} = \frac{f(x)}{g(x)}. $$ From here we can define a function $\alpha:(a,b)\to(a,b)$ given by $x\mapsto\alpha_x$. Hence, $a<\alpha(x)<x$ for all $x\in(a,b)$, and $\lim_{x\to a^+}\alpha(x) = a$. From here L'Hopital's follows since $$ \lim_{x\to a^+} \frac{f(x)}{g(x)} = \lim_{x\to a^+}\frac{f'(\alpha(x))}{g'(\alpha(x))} = L. $$ The only issue I have with this proof is the idea of choice involved with this function $\alpha(x)$. The Cauchy mean value theorem (which comes from Rolle's theorem, which in turn comes from the extreme value theorem) is not a constructive proof. It only implies that such an $\alpha_x\in(a,x)$ exists; and says nothing to the effect of how one would find such a value, considering there might be more than one such number in $(a,x)$ that satisfies the condition. So in this way, are we forced to appeal to the (full?) Axiom of Choice , to properly define our function, and thus to prove L'Hopital's rule? Is there any proof that gets around this issue?","I was looking over an honors-calculus proof of L'Hopital's rule today, and I couldn't help but feel a sense of unease. The proof states that $f(x)$ and $g(x)$ are continuous on $[a,b]$; differentiable on $(a,b)$; $\lim_{x\to a^+} \tfrac{f'(x)}{g'(x)} = L$ exists; and that $\lim_{x\to a^+} f(x) = \lim_{x\to a^+} g(x) = 0$. WLOG, we may assume that $f(a)=g(a)=0$. It then proceeds by saying that for any given $x\in(a,b)$, $\;f$ and $g$ are continuous on $[a,x]$ and differentiable on $(a,x)$. Therefore by the Cauchy mean value theorem there must exist a number $\alpha_x\in(a,x)$ for which $$  \frac{f'(\alpha_x)}{g'(\alpha_x)} = \frac{f(x)-f(a)}{g(x)-g(a)} = \frac{f(x)}{g(x)}. $$ From here we can define a function $\alpha:(a,b)\to(a,b)$ given by $x\mapsto\alpha_x$. Hence, $a<\alpha(x)<x$ for all $x\in(a,b)$, and $\lim_{x\to a^+}\alpha(x) = a$. From here L'Hopital's follows since $$ \lim_{x\to a^+} \frac{f(x)}{g(x)} = \lim_{x\to a^+}\frac{f'(\alpha(x))}{g'(\alpha(x))} = L. $$ The only issue I have with this proof is the idea of choice involved with this function $\alpha(x)$. The Cauchy mean value theorem (which comes from Rolle's theorem, which in turn comes from the extreme value theorem) is not a constructive proof. It only implies that such an $\alpha_x\in(a,x)$ exists; and says nothing to the effect of how one would find such a value, considering there might be more than one such number in $(a,x)$ that satisfies the condition. So in this way, are we forced to appeal to the (full?) Axiom of Choice , to properly define our function, and thus to prove L'Hopital's rule? Is there any proof that gets around this issue?",,"['calculus', 'limits', 'alternative-proof', 'axiom-of-choice']"
68,Calculate the upper boundary of the given sum,Calculate the upper boundary of the given sum,,"How to find upper boundary of $\sum^{\infty}_{k = 1}\frac{1}{k^{3/2}}$? I know that it can be solved using integrals, but can someone explain how this sum is connected with integral of $f(x) = \frac{1}{k^{3/2}}$?","How to find upper boundary of $\sum^{\infty}_{k = 1}\frac{1}{k^{3/2}}$? I know that it can be solved using integrals, but can someone explain how this sum is connected with integral of $f(x) = \frac{1}{k^{3/2}}$?",,"['integration', 'limits', 'functions', 'summation', 'supremum-and-infimum']"
69,Why does a polynomial's degree reduce when the leading coefficient approaches $0$ (but not equal to $0$)?,Why does a polynomial's degree reduce when the leading coefficient approaches  (but not equal to )?,0 0,"Here is the definition of a polynomial taken from here A polynomial is a mathematical expression involving a sum of powers in one or more variables multiplied by coefficients. A polynomial in one variable (i.e., a univariate polynomial) with constant coefficients is given by   $$a_0 + a_1x + a_2x^2 + \cdots + a_nx^n = \sum_{i = 0}^{n} a_ix^i$$ A polynomial is said to be an $nth$ degree polynomial if the highest power of the variable is $n$ and the leading coefficient is not equal to $0$. Now, take the following case:  $$f(x) = \lim_{a_n \to 0} \sum_{i = 0}^{n} a_ix^i = \sum_{i = 0}^{n - 1} a_ix^i$$ Now, this shows that the degree of the polynomial $$\lim_{a_n \to 0} \sum_{i = 0}^{n} a_ix^i$$ is $n - 1$. Now, here is another thing from Wikipedia To say that    $$\lim_{x \to p} f(x) = L,$$   means that $f(x)$ can be made as close as desired to $L$ by making $x$ close enough, but not equal , to $p$. It means in the expression  $$\lim_{a_n \to 0} \sum_{i = 0}^{n} a_ix^i$$ the value of $a_n$ is infinitesimally close to $0$ but $a_n \ne 0$. This means that the degree of the polynomial is $n$. So, my question is that is it just because $a_n$ is extremely close to zero that we consider it to be equal to zero? If so, then is the degree of the expression $n - 1$?","Here is the definition of a polynomial taken from here A polynomial is a mathematical expression involving a sum of powers in one or more variables multiplied by coefficients. A polynomial in one variable (i.e., a univariate polynomial) with constant coefficients is given by   $$a_0 + a_1x + a_2x^2 + \cdots + a_nx^n = \sum_{i = 0}^{n} a_ix^i$$ A polynomial is said to be an $nth$ degree polynomial if the highest power of the variable is $n$ and the leading coefficient is not equal to $0$. Now, take the following case:  $$f(x) = \lim_{a_n \to 0} \sum_{i = 0}^{n} a_ix^i = \sum_{i = 0}^{n - 1} a_ix^i$$ Now, this shows that the degree of the polynomial $$\lim_{a_n \to 0} \sum_{i = 0}^{n} a_ix^i$$ is $n - 1$. Now, here is another thing from Wikipedia To say that    $$\lim_{x \to p} f(x) = L,$$   means that $f(x)$ can be made as close as desired to $L$ by making $x$ close enough, but not equal , to $p$. It means in the expression  $$\lim_{a_n \to 0} \sum_{i = 0}^{n} a_ix^i$$ the value of $a_n$ is infinitesimally close to $0$ but $a_n \ne 0$. This means that the degree of the polynomial is $n$. So, my question is that is it just because $a_n$ is extremely close to zero that we consider it to be equal to zero? If so, then is the degree of the expression $n - 1$?",,"['limits', 'polynomials']"
70,Finding $\lim_{x \to \infty} \left(\frac{a_{1}^{1/x}+a_{2}^{1/x}+\cdot\cdot\cdot{a_{n}}^{1/x}}{n}\right)^{nx}$,Finding,\lim_{x \to \infty} \left(\frac{a_{1}^{1/x}+a_{2}^{1/x}+\cdot\cdot\cdot{a_{n}}^{1/x}}{n}\right)^{nx},"For non zero positive reals $a_{1},a_{2}\cdot\cdot\cdot a_{n}$ how to find  $$ \lim_{x \to \infty}    \left(\frac{a_1^{1/x} +a_2^{1/x}+\ldots +a_n^{1/x}}{n}\right)^{nx}? $$ It becomes indeterminant form $1^{\infty}.$ But difficult to solve by  L'Hospital's Rule. By using A.M.-G.M. inequality it comes that limit is $\geq a_{1}a_{2}\cdot\cdot\cdot a_{n}.$ I also tried by using Squeeze. Please help.Thanks.","For non zero positive reals $a_{1},a_{2}\cdot\cdot\cdot a_{n}$ how to find  $$ \lim_{x \to \infty}    \left(\frac{a_1^{1/x} +a_2^{1/x}+\ldots +a_n^{1/x}}{n}\right)^{nx}? $$ It becomes indeterminant form $1^{\infty}.$ But difficult to solve by  L'Hospital's Rule. By using A.M.-G.M. inequality it comes that limit is $\geq a_{1}a_{2}\cdot\cdot\cdot a_{n}.$ I also tried by using Squeeze. Please help.Thanks.",,"['real-analysis', 'limits']"
71,Finding $\lim\limits_{n \to \infty} \frac{\ln(n+1)}{\ln(n)} \frac{\ln(n!)}{\ln((n+1)!)}$,Finding,\lim\limits_{n \to \infty} \frac{\ln(n+1)}{\ln(n)} \frac{\ln(n!)}{\ln((n+1)!)},I am trying to find the limit as $n \to \infty$ of the function below: $$f(n) = \frac{\ln(n+1)}{\ln(n)} \frac{\ln(n!)}{\ln((n+1)!)}$$ The textbook only gives me an answer but I don't know how it got to it. I got confused with the factorials within logs. Edit: I understand that this could be expanded to: $$f(n) = \frac{\ln(n+1)}{\ln(n)} \frac{\ln(n) + \ln(n-1) + ...}{\ln(n+1) + \ln(n) + \ln(n-1) + ...}$$ I'm confused which terms get reduced to zero as $n \to \infty$ or how to group them.,I am trying to find the limit as $n \to \infty$ of the function below: $$f(n) = \frac{\ln(n+1)}{\ln(n)} \frac{\ln(n!)}{\ln((n+1)!)}$$ The textbook only gives me an answer but I don't know how it got to it. I got confused with the factorials within logs. Edit: I understand that this could be expanded to: $$f(n) = \frac{\ln(n+1)}{\ln(n)} \frac{\ln(n) + \ln(n-1) + ...}{\ln(n+1) + \ln(n) + \ln(n-1) + ...}$$ I'm confused which terms get reduced to zero as $n \to \infty$ or how to group them.,,"['limits', 'logarithms']"
72,Does Quasi-increasing and Quasi-decreasing imply continuity?,Does Quasi-increasing and Quasi-decreasing imply continuity?,,Suppose $f$ is a function $f$: $\mathbb{R} \to \mathbb{R}$. Then Do the following equations imply the continuity of $f$ at $s$? $ \lim_{x\uparrow s} \sup f(x) \leq f(s) \leq \lim_{x\downarrow s} \inf f(x)$ $ \lim_{x\uparrow s} \inf f(x) \geq f(s) \geq \lim_{x\downarrow s} \sup f(x)$,Suppose $f$ is a function $f$: $\mathbb{R} \to \mathbb{R}$. Then Do the following equations imply the continuity of $f$ at $s$? $ \lim_{x\uparrow s} \sup f(x) \leq f(s) \leq \lim_{x\downarrow s} \inf f(x)$ $ \lim_{x\uparrow s} \inf f(x) \geq f(s) \geq \lim_{x\downarrow s} \sup f(x)$,,"['analysis', 'limits', 'continuity']"
73,Indeterminate Limit using Second Fundamental Theorem of Calculus,Indeterminate Limit using Second Fundamental Theorem of Calculus,,"I am trying to find $$\lim_{x\to 0}\frac{\int_0^x(x-t)\sin(t^2) \, dt}{\ln(1+x^4)}$$It seems that I am meant to use the 2nd Fundamental Theorem of Calculus to solve this, but I have never used it on an integral that has $x$ in it. Do I approach it any differently? Or am I on the wrong track entirely?","I am trying to find $$\lim_{x\to 0}\frac{\int_0^x(x-t)\sin(t^2) \, dt}{\ln(1+x^4)}$$It seems that I am meant to use the 2nd Fundamental Theorem of Calculus to solve this, but I have never used it on an integral that has $x$ in it. Do I approach it any differently? Or am I on the wrong track entirely?",,"['limits', 'indeterminate-forms']"
74,Proof of standard limit $\lim_{x \to 0}(1 - \cos x)/x^{2}$ [duplicate],Proof of standard limit  [duplicate],\lim_{x \to 0}(1 - \cos x)/x^{2},This question already has answers here : Finding the limit of $\frac{1-\cos x}{x^2}$ (3 answers) Closed 5 years ago . $$\lim_{x\to 0} \frac{1-\cos(x)}{x^2}=\frac{1}{2}$$ Proof: $$\lim_{x\to 0} \frac{1-\cos (x)}{x^2} \times \frac{1+\cos (x)}{1+\cos(x)}$$ $$\lim_{x\to 0} \frac{1-\cos^2(x)}{x^2(1+\cos (x))}$$ $$\lim_{x\to 0} \frac{\sin^2(x)}{x^2(1+\cos (x))}$$ $$=\frac{1}{2}$$ Hence we prove it. But I try to use $\varepsilon$  & $\delta$ method but I was not able to complete the proof. Some one help me to prove it using $\varepsilon$ and $\delta$ method. Thanks in advance.,This question already has answers here : Finding the limit of $\frac{1-\cos x}{x^2}$ (3 answers) Closed 5 years ago . $$\lim_{x\to 0} \frac{1-\cos(x)}{x^2}=\frac{1}{2}$$ Proof: $$\lim_{x\to 0} \frac{1-\cos (x)}{x^2} \times \frac{1+\cos (x)}{1+\cos(x)}$$ $$\lim_{x\to 0} \frac{1-\cos^2(x)}{x^2(1+\cos (x))}$$ $$\lim_{x\to 0} \frac{\sin^2(x)}{x^2(1+\cos (x))}$$ $$=\frac{1}{2}$$ Hence we prove it. But I try to use $\varepsilon$  & $\delta$ method but I was not able to complete the proof. Some one help me to prove it using $\varepsilon$ and $\delta$ method. Thanks in advance.,,"['calculus', 'limits', 'alternative-proof', 'epsilon-delta', 'limits-without-lhopital']"
75,Calculate the limit: $\lim_{x\rightarrow \infty}\frac{\ln x}{x^{a}}$,Calculate the limit:,\lim_{x\rightarrow \infty}\frac{\ln x}{x^{a}},"Calculate the limit: $$\lim_{x\rightarrow \infty}\frac{\ln x}{x^{a}}$$ When try calculate limit, we get $\frac{\infty}{\infty}$, so use L'Hôpital again. $$(\ln x)' = \frac{1}{x}$$ $$x^{a} = e^{\ln x \cdot a} \Rightarrow (e^{\ln x \cdot a})'= e^{\ln x \cdot a} \cdot \frac{1}{x} \cdot a$$ $$\Rightarrow$$ $$\lim_{x\rightarrow\infty}\frac{\frac{1}{x}}{e^{\ln x \cdot a} \cdot \frac{1}{x} \cdot a}= \lim_{x\rightarrow\infty}\frac{x}{e^{\ln x \cdot a} \cdot a \cdot x} = \lim_{x\rightarrow\infty} \frac{1}{e^{\ln x \cdot a} \cdot a} = \frac{1}{\infty} = 0$$ Is correct result and limit?","Calculate the limit: $$\lim_{x\rightarrow \infty}\frac{\ln x}{x^{a}}$$ When try calculate limit, we get $\frac{\infty}{\infty}$, so use L'Hôpital again. $$(\ln x)' = \frac{1}{x}$$ $$x^{a} = e^{\ln x \cdot a} \Rightarrow (e^{\ln x \cdot a})'= e^{\ln x \cdot a} \cdot \frac{1}{x} \cdot a$$ $$\Rightarrow$$ $$\lim_{x\rightarrow\infty}\frac{\frac{1}{x}}{e^{\ln x \cdot a} \cdot \frac{1}{x} \cdot a}= \lim_{x\rightarrow\infty}\frac{x}{e^{\ln x \cdot a} \cdot a \cdot x} = \lim_{x\rightarrow\infty} \frac{1}{e^{\ln x \cdot a} \cdot a} = \frac{1}{\infty} = 0$$ Is correct result and limit?",,"['calculus', 'analysis', 'limits', 'functions', 'convergence-divergence']"
76,"Find the limit of $\lim\limits_{(x,y)\to (0,0)} \frac{x^2y^2}{x^2y^2+(x-y)^2}$",Find the limit of,"\lim\limits_{(x,y)\to (0,0)} \frac{x^2y^2}{x^2y^2+(x-y)^2}","Find the limit of $$\lim\limits_{(x,y)\to (0,0)} \frac{x^2y^2}{x^2y^2+(x-y)^2}$$ So, I know that $$\lim\limits_{x \to x_0} f(x)=c \Leftrightarrow \forall (x_n)\subseteq D\setminus\{x_0\}, x_n\to x_0: f(x_n)\to c \, (n\to \infty)$$ Let $x_n=\left(\frac{1}{n},\frac{1}{n}\right), y_n=\left(\frac{1}{n},0\right):$ $$f(x_n)=\frac{\frac{1}{n}^4}{\frac{1}{n}^4}=1\\ f(y_n)=\frac{0}{...}=0\\ \Rightarrow f(x_n)\neq f(y_n)\, (n\to \infty)$$ So the limit doesn't exist. Correct?","Find the limit of $$\lim\limits_{(x,y)\to (0,0)} \frac{x^2y^2}{x^2y^2+(x-y)^2}$$ So, I know that $$\lim\limits_{x \to x_0} f(x)=c \Leftrightarrow \forall (x_n)\subseteq D\setminus\{x_0\}, x_n\to x_0: f(x_n)\to c \, (n\to \infty)$$ Let $x_n=\left(\frac{1}{n},\frac{1}{n}\right), y_n=\left(\frac{1}{n},0\right):$ $$f(x_n)=\frac{\frac{1}{n}^4}{\frac{1}{n}^4}=1\\ f(y_n)=\frac{0}{...}=0\\ \Rightarrow f(x_n)\neq f(y_n)\, (n\to \infty)$$ So the limit doesn't exist. Correct?",,"['calculus', 'real-analysis', 'limits', 'multivariable-calculus']"
77,Limits - Solving a finite & non-zero limit with unknown power,Limits - Solving a finite & non-zero limit with unknown power,,"Okay, so I found this question in a text, For a certain value of 'c', the given limit is finite & non-zero, and equal to 'l'. Then find 'l' & 'c'. $$ \lim_{x \to \infty} [ (x^5 + 7x^4 +2)^c - x ] $$ To solve this problem, I thought that for the given limit to be finite, c must be equal to $\frac{1}{5}$ , because if it's anything else, than the answer will tend to negative or positive infinity (because we have $x^5$ in the polynomial, and it can't have a power larger than 1). Now I understand that this isn't exactly the best of ways to solve this question, so I would like to know how you would approach this question, and what would you generally do in cases like this? PS: The answers are $ c = \frac{1}{5} $ and $ l = \frac{7}{5} $ .","Okay, so I found this question in a text, For a certain value of 'c', the given limit is finite & non-zero, and equal to 'l'. Then find 'l' & 'c'. To solve this problem, I thought that for the given limit to be finite, c must be equal to , because if it's anything else, than the answer will tend to negative or positive infinity (because we have in the polynomial, and it can't have a power larger than 1). Now I understand that this isn't exactly the best of ways to solve this question, so I would like to know how you would approach this question, and what would you generally do in cases like this? PS: The answers are and .", \lim_{x \to \infty} [ (x^5 + 7x^4 +2)^c - x ]  \frac{1}{5} x^5  c = \frac{1}{5}   l = \frac{7}{5} ,['limits']
78,How to compute the limit of $a_n$ from the following expression?,How to compute the limit of  from the following expression?,a_n,"Let $\{a_n\}$ be a sequence such that  $$\lim_{n\to \infty}\left|a_n+3\left(\frac{n-2}{n}\right)^n\right|^\frac{1}{n}=\frac{3}{5}$$ Then calculate $\lim_{n\to \infty}a_n$. First I tried to take logarithm and got $\lim_{n\to \infty}\frac{1}{n}\ln\left|a_n+3\left(\frac{n-2}{n}\right)^n\right|=\ln\frac{3}{5}$, then I thought about L Hospital but that did not work. I am unable to dig it further. Can somebody give me a hint or push towards the solution? Thanks.","Let $\{a_n\}$ be a sequence such that  $$\lim_{n\to \infty}\left|a_n+3\left(\frac{n-2}{n}\right)^n\right|^\frac{1}{n}=\frac{3}{5}$$ Then calculate $\lim_{n\to \infty}a_n$. First I tried to take logarithm and got $\lim_{n\to \infty}\frac{1}{n}\ln\left|a_n+3\left(\frac{n-2}{n}\right)^n\right|=\ln\frac{3}{5}$, then I thought about L Hospital but that did not work. I am unable to dig it further. Can somebody give me a hint or push towards the solution? Thanks.",,"['sequences-and-series', 'limits']"
79,"Calculate $\lim_{x\to0}{F(x)\over g(x)}$, where $ g(x)=x$ and $F(x)=\int_0^x {e^{2t}-2e^t+1\over 2\cos3t-2\cos2t+\cos t} \, dt$.","Calculate , where  and .","\lim_{x\to0}{F(x)\over g(x)}  g(x)=x F(x)=\int_0^x {e^{2t}-2e^t+1\over 2\cos3t-2\cos2t+\cos t} \, dt","Calculate $\displaystyle\lim_{x\to0}{F(x)\over g(x)}$, where $ g(x)=x$ and  $\displaystyle F(x)=\int_0^x {e^{2t}-2e^t+1\over 2\cos3t-2\cos2t+\cos t} \, dt$. i'd love for someone to explain not only the technical procedure here but also what are the theorems that allow it to take place. To my understanding, the Fundamental theorem of calculus is key here and Newton-Leibniz theorem also contributes. Even though I do feel I understand them as well as the subtle connection they made between antiderivatives and definite integrals I can't seem to apply any of that when it comes to limits and proofs.","Calculate $\displaystyle\lim_{x\to0}{F(x)\over g(x)}$, where $ g(x)=x$ and  $\displaystyle F(x)=\int_0^x {e^{2t}-2e^t+1\over 2\cos3t-2\cos2t+\cos t} \, dt$. i'd love for someone to explain not only the technical procedure here but also what are the theorems that allow it to take place. To my understanding, the Fundamental theorem of calculus is key here and Newton-Leibniz theorem also contributes. Even though I do feel I understand them as well as the subtle connection they made between antiderivatives and definite integrals I can't seem to apply any of that when it comes to limits and proofs.",,"['limits', 'definite-integrals']"
80,Evalute the value of limit,Evalute the value of limit,,"$$ \lim_{x\to 0}\frac{(1+\sin x)^{\operatorname{cosec}x} - e + \left(\dfrac{\sin x}{2}\right)e}{\sin^2x} $$ I am stucked here , please tell me how to proceed further and Is there any way to solve this problem","$$ \lim_{x\to 0}\frac{(1+\sin x)^{\operatorname{cosec}x} - e + \left(\dfrac{\sin x}{2}\right)e}{\sin^2x} $$ I am stucked here , please tell me how to proceed further and Is there any way to solve this problem",,['limits']
81,Necessary and sufficient condition for uniform convergence.,Necessary and sufficient condition for uniform convergence.,,"Let $a_n$ and $b_n$ be real sequences and let $f_n(x) = a_nx + b_nx^2$ be a sequence of polynomials. What should be the necessary and sufficient conditions on the sequences $a_n$ and $b_n$ so that the $f_n$ converges uniformly to $f(x) = 0$ on $\mathbb{R}$ ? I have some conclusions: Since the convergence is unform, $\lim_{n\rightarrow \infty}f_n(1) = \lim (a_n + b_n) = 0$. Similarly, for $x = -1$, we have $\lim(-a_n + b_n) = 0$. Adding, we get $\lim{b_n} = 0$, and so we also have $\lim a_n = 0$. But these conditions are not sufficient.","Let $a_n$ and $b_n$ be real sequences and let $f_n(x) = a_nx + b_nx^2$ be a sequence of polynomials. What should be the necessary and sufficient conditions on the sequences $a_n$ and $b_n$ so that the $f_n$ converges uniformly to $f(x) = 0$ on $\mathbb{R}$ ? I have some conclusions: Since the convergence is unform, $\lim_{n\rightarrow \infty}f_n(1) = \lim (a_n + b_n) = 0$. Similarly, for $x = -1$, we have $\lim(-a_n + b_n) = 0$. Adding, we get $\lim{b_n} = 0$, and so we also have $\lim a_n = 0$. But these conditions are not sufficient.",,"['real-analysis', 'sequences-and-series', 'limits', 'uniform-convergence']"
82,$\lim_{x \to 0} \frac{\sin(a + b)x + \sin(a - b)x + sin(2ax)}{\cos^2 bx - cos^2 ax}$,,\lim_{x \to 0} \frac{\sin(a + b)x + \sin(a - b)x + sin(2ax)}{\cos^2 bx - cos^2 ax},"Question : - $$\lim_{x \to 0} \frac{\sin(a + b)x + \sin(a - b)x + \sin(2ax)}{\cos^2 bx - \cos^2 ax}$$ My attempt :- $$\lim_{x \to 0} \frac{2* \sin ax * \cos bx + 2*\sin ax * \cos (ax)}{(\cos bx - \cos ax)*(\cos bx +\cos ax)}$$   $$\lim_{x \to 0} \frac{2* \sin ax * (\cos bx + \cos (ax))}{(\cos bx - \cos ax)*(\cos bx +\cos ax)}$$   $$\lim_{x \to 0} \frac{2* \sin ax}{(\cos bx - \cos ax)}$$   $$\lim_{x \to 0} \frac{2* \sin ax}{(-2 * \sin {bx-ax\over2} * \sin {bx+ax\over2})}$$   $$-\lim_{x \to 0} \frac{{\sin ax \over ax} * {ax }}{\frac{\sin {bx-ax\over2} * \sin {bx+ax\over2}}{ {bx+ax\over2} * {bx-ax\over2}} * {bx+ax\over2} * {bx-ax\over2}}$$   $$-\lim_{x \to 0} \frac{ax}{{bx+ax\over2} * {bx-ax\over2}}$$   $$-\lim_{x \to 0} \frac{ax}{{(bx)^2 - (ax)^2\over 4}}$$   $$\lim_{x \to 0} \frac{4 * ax}{(ax)^2 -(bx)^2}$$   $$\lim_{x \to 0} \frac{4 * a}{x (a^2 - b^2)}$$ There i hit the dead end, i can't do anything now. Can anyone please tell me what i have done incorrect here ? Sorry but i can't confirm the legitimacy of the question, i just found it on net and there is no answer given.","Question : - $$\lim_{x \to 0} \frac{\sin(a + b)x + \sin(a - b)x + \sin(2ax)}{\cos^2 bx - \cos^2 ax}$$ My attempt :- $$\lim_{x \to 0} \frac{2* \sin ax * \cos bx + 2*\sin ax * \cos (ax)}{(\cos bx - \cos ax)*(\cos bx +\cos ax)}$$   $$\lim_{x \to 0} \frac{2* \sin ax * (\cos bx + \cos (ax))}{(\cos bx - \cos ax)*(\cos bx +\cos ax)}$$   $$\lim_{x \to 0} \frac{2* \sin ax}{(\cos bx - \cos ax)}$$   $$\lim_{x \to 0} \frac{2* \sin ax}{(-2 * \sin {bx-ax\over2} * \sin {bx+ax\over2})}$$   $$-\lim_{x \to 0} \frac{{\sin ax \over ax} * {ax }}{\frac{\sin {bx-ax\over2} * \sin {bx+ax\over2}}{ {bx+ax\over2} * {bx-ax\over2}} * {bx+ax\over2} * {bx-ax\over2}}$$   $$-\lim_{x \to 0} \frac{ax}{{bx+ax\over2} * {bx-ax\over2}}$$   $$-\lim_{x \to 0} \frac{ax}{{(bx)^2 - (ax)^2\over 4}}$$   $$\lim_{x \to 0} \frac{4 * ax}{(ax)^2 -(bx)^2}$$   $$\lim_{x \to 0} \frac{4 * a}{x (a^2 - b^2)}$$ There i hit the dead end, i can't do anything now. Can anyone please tell me what i have done incorrect here ? Sorry but i can't confirm the legitimacy of the question, i just found it on net and there is no answer given.",,['calculus']
83,How to prove the limit formula of the second order partial derivative?,How to prove the limit formula of the second order partial derivative?,,"Consider following limit formula of the second order partial derivative of a function $f(x,y,z)$: $$\frac{\partial^2f(x,y,z)}{\partial x^2}=\lim_{\Delta x\rightarrow 0}\frac{f(x+\Delta x,y,z)-2f(x,y,z)+f(x-\Delta x,y,z)}{(\Delta x)^2}.$$ In order to prove this formula, I start with the first order derivative: $$\frac{\partial f(x,y,z)}{\partial x}=\lim_{\Delta x\rightarrow 0}\frac{f(x+\Delta x,y,z)-f(x,y,z)}{\Delta x}.$$ How do I proceed now? The second derivative is the limit of the difference quotient of the first derivatives. But then I start messing up different limit operators with different variables that go to 0.","Consider following limit formula of the second order partial derivative of a function $f(x,y,z)$: $$\frac{\partial^2f(x,y,z)}{\partial x^2}=\lim_{\Delta x\rightarrow 0}\frac{f(x+\Delta x,y,z)-2f(x,y,z)+f(x-\Delta x,y,z)}{(\Delta x)^2}.$$ In order to prove this formula, I start with the first order derivative: $$\frac{\partial f(x,y,z)}{\partial x}=\lim_{\Delta x\rightarrow 0}\frac{f(x+\Delta x,y,z)-f(x,y,z)}{\Delta x}.$$ How do I proceed now? The second derivative is the limit of the difference quotient of the first derivatives. But then I start messing up different limit operators with different variables that go to 0.",,"['limits', 'derivatives', 'partial-derivative']"
84,A little help with $ x_n = \frac{1}{n^2} \sum_{k=1}^{n} \left[ k \alpha \right] $,A little help with, x_n = \frac{1}{n^2} \sum_{k=1}^{n} \left[ k \alpha \right] ,"So I have this little problem I want to solve that says the following: For every number $\alpha \in \mathbb{R} $, analyze the following sequence $$ \{ x_n \} = \frac{1}{n^2} \sum_{k=1}^{n} \left[ k \alpha \right] ,$$ where $ \left[ x \right] $ is the floor function. Calculate for which values of $ \alpha $ does $ x_n $ converges, or if it diverges. If it converges, calculate the limit of the sequence. My first thought was to prove that this sequence converges, or diverges, for every possible value of $ \alpha $, that is, when $ \alpha \in \mathbb{Z} ,$ $ \alpha \in \mathbb{Q} ,$ and when $ \alpha \in \mathbb{I} ,$ in other words, $ \alpha $ is irrational. So when $ \alpha \in \mathbb{Z} $ I got the following: If $ \alpha \in \mathbb{Z} $ then $ \left[ \alpha \right] = \alpha ,$ therefore $$ \{ x_n \} = \frac{1}{n^2} \sum_{k=1}^{n} k \alpha = \frac{\alpha}{n^2} \sum_{k=1}^{n} k = \frac{\alpha}{n^2} \frac{n(n+1)}{2} = \frac{\alpha (n+1)}{2n} ,$$ thus, $ \{ x_n \} $ converges to $$ \{ x_n \} = \frac{\alpha (n+1)}{2n} ;$$ since it converges, then we can calculate the limit as follows $$ \lim_{n \to \infty} \{ x_n \} = \lim_{n \to \infty} \frac{\alpha (n+1)}{2n} = \frac{\alpha}{2} \lim_{n \to \infty} \frac{n+1}{n} = \frac{\alpha}{2} \lim_{n \to \infty} \frac{1 + \frac{1}{n}}{1} = \frac{\alpha}{2} \left[ \lim_{n \to \infty} 1 + \lim_{n \to \infty} \frac{1}{n} \right] = \frac{\alpha}{2} .$$ So, first of all, is this proof correct? If so, how can I proceed to prove the sequence when $ \alpha \in \mathbb{Q} $ or when it's irrational? Should I prove the floor function for rational and irrational numbers and then apply this to the sequence?","So I have this little problem I want to solve that says the following: For every number $\alpha \in \mathbb{R} $, analyze the following sequence $$ \{ x_n \} = \frac{1}{n^2} \sum_{k=1}^{n} \left[ k \alpha \right] ,$$ where $ \left[ x \right] $ is the floor function. Calculate for which values of $ \alpha $ does $ x_n $ converges, or if it diverges. If it converges, calculate the limit of the sequence. My first thought was to prove that this sequence converges, or diverges, for every possible value of $ \alpha $, that is, when $ \alpha \in \mathbb{Z} ,$ $ \alpha \in \mathbb{Q} ,$ and when $ \alpha \in \mathbb{I} ,$ in other words, $ \alpha $ is irrational. So when $ \alpha \in \mathbb{Z} $ I got the following: If $ \alpha \in \mathbb{Z} $ then $ \left[ \alpha \right] = \alpha ,$ therefore $$ \{ x_n \} = \frac{1}{n^2} \sum_{k=1}^{n} k \alpha = \frac{\alpha}{n^2} \sum_{k=1}^{n} k = \frac{\alpha}{n^2} \frac{n(n+1)}{2} = \frac{\alpha (n+1)}{2n} ,$$ thus, $ \{ x_n \} $ converges to $$ \{ x_n \} = \frac{\alpha (n+1)}{2n} ;$$ since it converges, then we can calculate the limit as follows $$ \lim_{n \to \infty} \{ x_n \} = \lim_{n \to \infty} \frac{\alpha (n+1)}{2n} = \frac{\alpha}{2} \lim_{n \to \infty} \frac{n+1}{n} = \frac{\alpha}{2} \lim_{n \to \infty} \frac{1 + \frac{1}{n}}{1} = \frac{\alpha}{2} \left[ \lim_{n \to \infty} 1 + \lim_{n \to \infty} \frac{1}{n} \right] = \frac{\alpha}{2} .$$ So, first of all, is this proof correct? If so, how can I proceed to prove the sequence when $ \alpha \in \mathbb{Q} $ or when it's irrational? Should I prove the floor function for rational and irrational numbers and then apply this to the sequence?",,"['sequences-and-series', 'limits', 'ceiling-and-floor-functions']"
85,Proof of L'Hopital's rule,Proof of L'Hopital's rule,,"I'm attempting to prove L'Hopital's rule. My solution so far is the following: Let $f,g:(a,b)\to\mathbb{R}$ be differentiable and continuous on $[a, b]$ with $f(a)=g(a)=0$ and $g(x)\neq0$ for $x\in(a, b)$. Suppose that $\lim_{x\to a^+} \frac{f'(x)}{g'(x)}$ exists. Let $x_n\in(a,b)$ be a sequence such that $\lim_{n\to\infty}x_n=a.$ Now by Cauchy's mean value theorem there exists $c_n\in(a,x_n)$ such that $$\frac{f'(c_n)}{g'(c_n)}=\frac{f(x_n)-f(a)}{g(x_n)-g(a)}=\frac{f(x_n)}{g(x_n)}$$ By the sandwich rule we also have that $\lim_{n\to\infty}c_n=a.$ I think I'm very nearly there but I can't see how to conclude it.","I'm attempting to prove L'Hopital's rule. My solution so far is the following: Let $f,g:(a,b)\to\mathbb{R}$ be differentiable and continuous on $[a, b]$ with $f(a)=g(a)=0$ and $g(x)\neq0$ for $x\in(a, b)$. Suppose that $\lim_{x\to a^+} \frac{f'(x)}{g'(x)}$ exists. Let $x_n\in(a,b)$ be a sequence such that $\lim_{n\to\infty}x_n=a.$ Now by Cauchy's mean value theorem there exists $c_n\in(a,x_n)$ such that $$\frac{f'(c_n)}{g'(c_n)}=\frac{f(x_n)-f(a)}{g(x_n)-g(a)}=\frac{f(x_n)}{g(x_n)}$$ By the sandwich rule we also have that $\lim_{n\to\infty}c_n=a.$ I think I'm very nearly there but I can't see how to conclude it.",,"['calculus', 'real-analysis', 'limits']"
86,Finding Limit Points?,Finding Limit Points?,,"I have two sets: $ A = ({{ (-1)^n + 2/n : n = 1, 2, 3, ...}}) $ and $ B = ( x \in \mathbb{Q} : 0 < x < 1 ) $ How does one go about finding the limit points for these sets? Would I just do  $ \lim_{n\to\infty} $ for each set? Could someone clarify what exactly limit points are, and how they relate to closed sets? Also, how do I know if a set given like this is open or closed? How do I find the isolated points? What about the closure? I don't necessarily want the answers, but rather the method to finding these. I can try them and post what answers I get later so they can be checked by the community!","I have two sets: $ A = ({{ (-1)^n + 2/n : n = 1, 2, 3, ...}}) $ and $ B = ( x \in \mathbb{Q} : 0 < x < 1 ) $ How does one go about finding the limit points for these sets? Would I just do  $ \lim_{n\to\infty} $ for each set? Could someone clarify what exactly limit points are, and how they relate to closed sets? Also, how do I know if a set given like this is open or closed? How do I find the isolated points? What about the closure? I don't necessarily want the answers, but rather the method to finding these. I can try them and post what answers I get later so they can be checked by the community!",,"['real-analysis', 'limits', 'real-numbers']"
87,"Given $\lim_{x \to 0} \frac{f(x)}{x} = 1$, find $f(0)$","Given , find",\lim_{x \to 0} \frac{f(x)}{x} = 1 f(0),"This is a question taken out of one of MIT's Single Variable Calculus Exams in 2010 (Obtained via OCW) Problem : Find $f(0)$, given that $ f $ is continuous at $ x=0$ and $$\lim_{x \to 0} \frac{f(x)}{x} = 1$$ Solution: The official solution to this problem is quoted verbatim below : Since $\lim_{x \to 0} \frac{f(x)}{x} = 1$, $f(x) \to 0$ as $x \to 0$, so $f(0)=0$ However what this solution has done (at least to me it seems this way), is simply, directly substitute $x=0$, into $\frac{f(x)}{x}$, and then solve for $f(0)$, but the way in which it is done, doesn't seem very rigorous (at least to me it doesn't). This is essentially what the solution has done (at least it seems this way to me) \begin{aligned}        & \lim_{x \to 0}\ \frac{f(x)}{x} = 1 \\        & \implies \frac{f(0)}{0} = 1 & \text{(By direct substitution)} \\ & \implies f(0) = 0 \cdot 1 & \text{(Can we even do this?)}\\        & \implies f(0) = 0\\       \end{aligned} But is what is being done in the second and third steps even correct? I ask this because we have a denominator of $0$ on one side of the equation, and technically having $0$ as the denominator in one of the terms of an equation, makes the equation undefined, am I correct in saying this? Essentially my question boils down to the fact that division by $0$ is undefined, and I am asking whether algebraically manipulating an equation that has one term, with a denominator of $0$, is mathematically valid or not, and whether this is seen as a rigorous solution to the posed problem.","This is a question taken out of one of MIT's Single Variable Calculus Exams in 2010 (Obtained via OCW) Problem : Find $f(0)$, given that $ f $ is continuous at $ x=0$ and $$\lim_{x \to 0} \frac{f(x)}{x} = 1$$ Solution: The official solution to this problem is quoted verbatim below : Since $\lim_{x \to 0} \frac{f(x)}{x} = 1$, $f(x) \to 0$ as $x \to 0$, so $f(0)=0$ However what this solution has done (at least to me it seems this way), is simply, directly substitute $x=0$, into $\frac{f(x)}{x}$, and then solve for $f(0)$, but the way in which it is done, doesn't seem very rigorous (at least to me it doesn't). This is essentially what the solution has done (at least it seems this way to me) \begin{aligned}        & \lim_{x \to 0}\ \frac{f(x)}{x} = 1 \\        & \implies \frac{f(0)}{0} = 1 & \text{(By direct substitution)} \\ & \implies f(0) = 0 \cdot 1 & \text{(Can we even do this?)}\\        & \implies f(0) = 0\\       \end{aligned} But is what is being done in the second and third steps even correct? I ask this because we have a denominator of $0$ on one side of the equation, and technically having $0$ as the denominator in one of the terms of an equation, makes the equation undefined, am I correct in saying this? Essentially my question boils down to the fact that division by $0$ is undefined, and I am asking whether algebraically manipulating an equation that has one term, with a denominator of $0$, is mathematically valid or not, and whether this is seen as a rigorous solution to the posed problem.",,"['calculus', 'real-analysis', 'limits', 'proof-verification']"
88,$\lim_{x \to 4} \sqrt{x^2-16}$,,\lim_{x \to 4} \sqrt{x^2-16},"Not clear to me why the limit as $x$ goes to $4$ of $\sqrt{x^2-16}$ is $0$, since the limits on both sides of $4$ are not the same. From the right it is zero, but from the left ($x= 3.99999$) is undefined.","Not clear to me why the limit as $x$ goes to $4$ of $\sqrt{x^2-16}$ is $0$, since the limits on both sides of $4$ are not the same. From the right it is zero, but from the left ($x= 3.99999$) is undefined.",,['limits']
89,Rigorous Definition of One-Sided Limits,Rigorous Definition of One-Sided Limits,,"In a typical first-year Calculus course professors typically tend to put a lot of emphasis on making visual connections when working with ""one-sided"" limits or derivatives. This is something I find particularly distressing as I prefer to think as abstractly as possible, and most professors' emphasis on making visual connections comes at a price. Professors will typically in their wording, refer to taking limits or derivatives approaching from the left or the right of some point. But left and right has no meaning in Mathematics. While it may make a visual connection, it is fundamentally wrong, a fallacy. When talking about proving the existence of a limit, using two one-sided limits typical professor would say : A limit as $x$ approaches some number $a$ exists if and only if, the limit as $x$ approaches $a$ from the left is equivalent to the limit as $x$ approaches $a$ from the right Stated Mathematically :$$\left(\lim_{x \ \to\  a} f(x) = L\right) \Leftrightarrow \left(\lim_{x \ \to \ a^+} f(x) = L\right) \land \left(\lim_{x \ \to \ a^-} f(x) = L\right)$$ Defined (Fairly) Rigorously Using $\epsilon-\delta$ : $$\left(\forall \epsilon > 0\  (\exists\ \delta > 0 : 0 <|x-a|<\delta \implies |f(x)-L|<\epsilon)\right) \Leftrightarrow \left[(\forall \epsilon > 0\  (\exists\ \delta > 0 : a-\delta <x<a \implies |f(x)-L|<\epsilon) \land (\forall \epsilon > 0\  (\exists\ \delta > 0 : a <x<a +\delta \implies |f(x)-L|<\epsilon)\right]$$ Question : Is this a more rigorous definition of what professors are talking about when they say ""approaching from the left/right"" . I've attempted to answer this below. Possible Answer : Approaching from the left : Taking a sequence of elements as they increase and converge towards an arbitrary element $x$ which is the element acting as the limiting point in the domain $D$ (an ordered set) of $f$. Approaching from the right : Taking a sequence of elements as they decrease and converge towards an arbitrary element $x$ which is the element acting as the limiting point in the domain $D$ (an ordered set) of $f$. Is what I've written above correct? If not, then there has to be a deeper more rigorous definition of approaching some number from the left/right . I know that the answer to this question lies in Real Analysis, and while as a first-year undergrad I have some basic knowledge of Real Analysis based on what I've read on my own, I don't have enough to answer this question. I would assume that it would have something to do with convergence of sequences. If there exists a more formal answer to this, using formal mathematical notation and concepts from Real Analysis, I would really like to see it, as what I've written is very, very loose and non-rigorous.","In a typical first-year Calculus course professors typically tend to put a lot of emphasis on making visual connections when working with ""one-sided"" limits or derivatives. This is something I find particularly distressing as I prefer to think as abstractly as possible, and most professors' emphasis on making visual connections comes at a price. Professors will typically in their wording, refer to taking limits or derivatives approaching from the left or the right of some point. But left and right has no meaning in Mathematics. While it may make a visual connection, it is fundamentally wrong, a fallacy. When talking about proving the existence of a limit, using two one-sided limits typical professor would say : A limit as $x$ approaches some number $a$ exists if and only if, the limit as $x$ approaches $a$ from the left is equivalent to the limit as $x$ approaches $a$ from the right Stated Mathematically :$$\left(\lim_{x \ \to\  a} f(x) = L\right) \Leftrightarrow \left(\lim_{x \ \to \ a^+} f(x) = L\right) \land \left(\lim_{x \ \to \ a^-} f(x) = L\right)$$ Defined (Fairly) Rigorously Using $\epsilon-\delta$ : $$\left(\forall \epsilon > 0\  (\exists\ \delta > 0 : 0 <|x-a|<\delta \implies |f(x)-L|<\epsilon)\right) \Leftrightarrow \left[(\forall \epsilon > 0\  (\exists\ \delta > 0 : a-\delta <x<a \implies |f(x)-L|<\epsilon) \land (\forall \epsilon > 0\  (\exists\ \delta > 0 : a <x<a +\delta \implies |f(x)-L|<\epsilon)\right]$$ Question : Is this a more rigorous definition of what professors are talking about when they say ""approaching from the left/right"" . I've attempted to answer this below. Possible Answer : Approaching from the left : Taking a sequence of elements as they increase and converge towards an arbitrary element $x$ which is the element acting as the limiting point in the domain $D$ (an ordered set) of $f$. Approaching from the right : Taking a sequence of elements as they decrease and converge towards an arbitrary element $x$ which is the element acting as the limiting point in the domain $D$ (an ordered set) of $f$. Is what I've written above correct? If not, then there has to be a deeper more rigorous definition of approaching some number from the left/right . I know that the answer to this question lies in Real Analysis, and while as a first-year undergrad I have some basic knowledge of Real Analysis based on what I've read on my own, I don't have enough to answer this question. I would assume that it would have something to do with convergence of sequences. If there exists a more formal answer to this, using formal mathematical notation and concepts from Real Analysis, I would really like to see it, as what I've written is very, very loose and non-rigorous.",,"['real-analysis', 'analysis', 'limits', 'convergence-divergence', 'continuity']"
90,$(a_n)$ is a monotone increasing sequence of integers. Prove that: $\lim_{n\to\infty}(1+\frac{1}{a_n})^{a_n}=e$,is a monotone increasing sequence of integers. Prove that:,(a_n) \lim_{n\to\infty}(1+\frac{1}{a_n})^{a_n}=e,"I'm asked to prove the question above. I need to show that if $(a_n)$ is an increasing sequence of integers then: $\lim_{n\to\infty}\left(1+\frac{1}{a_n}\right)^{a_n}=e$ I was thinking of showing that $\lim_{n\to\infty}(a_n)=\infty$ and then, by definition, I can say that there exists a natural number $N$, such that for every $n>N, a_n>0$ and so $(a_n)^\infty_{n=N}$ is a sub-sequence of $(1+\frac{1}{n})^n$ and therefore shares the same limit- $e$ Any ideas?","I'm asked to prove the question above. I need to show that if $(a_n)$ is an increasing sequence of integers then: $\lim_{n\to\infty}\left(1+\frac{1}{a_n}\right)^{a_n}=e$ I was thinking of showing that $\lim_{n\to\infty}(a_n)=\infty$ and then, by definition, I can say that there exists a natural number $N$, such that for every $n>N, a_n>0$ and so $(a_n)^\infty_{n=N}$ is a sub-sequence of $(1+\frac{1}{n})^n$ and therefore shares the same limit- $e$ Any ideas?",,"['sequences-and-series', 'limits']"
91,Convergence of a compound sequence,Convergence of a compound sequence,,let $a_n=\frac{1}{2}\sqrt{n}+\sum_{k=1}^n(\sqrt{k}-\sqrt{k+\frac{1}{2}})$ be a sequence. Is this sequence convergent?,let $a_n=\frac{1}{2}\sqrt{n}+\sum_{k=1}^n(\sqrt{k}-\sqrt{k+\frac{1}{2}})$ be a sequence. Is this sequence convergent?,,"['calculus', 'sequences-and-series', 'limits']"
92,showing that a Limit exists,showing that a Limit exists,,"This question is stuffing me over. Let $f$ be defined by $$f(x) = \begin{cases} x & \text{if } x \in \mathbb Q \\ -x & \text{if } x \notin \mathbb Q  \end{cases}$$ Use the definition of a limit to show that $\lim_{x\to 0} f(x)$ exists. I get how for a limit to exist we need to show that for every $\epsilon$ that is greater than $0$ there exists a $\delta$ that greater than $0$, such that if $0<|x-a|<\delta$ then $|f(x)-L|<\epsilon$, but I don't know how to actually do it.","This question is stuffing me over. Let $f$ be defined by $$f(x) = \begin{cases} x & \text{if } x \in \mathbb Q \\ -x & \text{if } x \notin \mathbb Q  \end{cases}$$ Use the definition of a limit to show that $\lim_{x\to 0} f(x)$ exists. I get how for a limit to exist we need to show that for every $\epsilon$ that is greater than $0$ there exists a $\delta$ that greater than $0$, such that if $0<|x-a|<\delta$ then $|f(x)-L|<\epsilon$, but I don't know how to actually do it.",,['limits']
93,What value of c makes this true?,What value of c makes this true?,,Since $\lim_{x \rightarrow \infty}\frac{(x)!}{x^{x}} = 0$ and $\lim_{x \rightarrow \infty}\frac{(2x)!}{x^{x}} = \infty$ Is there a value c (or range of values) where $\lim_{x \rightarrow \infty}\frac{(cx)!}{x^{x}} = 1$ ?,Since $\lim_{x \rightarrow \infty}\frac{(x)!}{x^{x}} = 0$ and $\lim_{x \rightarrow \infty}\frac{(2x)!}{x^{x}} = \infty$ Is there a value c (or range of values) where $\lim_{x \rightarrow \infty}\frac{(cx)!}{x^{x}} = 1$ ?,,"['limits', 'factorial']"
94,Switching limits: $n \rightarrow \infty$ for $n\rightarrow 0$,Switching limits:  for,n \rightarrow \infty n\rightarrow 0,"I feel like this question may have already been asked, but despite my searches, I could not find it. I am looking to prove that $\lim\limits_{\epsilon \rightarrow 0} \int_{[b, b+\epsilon]} g=0$  for an integrable function $g$.  To do so, I would like to use the Dominated Convergence Theorem, as $g$ is my integrable dominating function.  However, I realize that the DCT is defined for the limit as $n$ goes to infinity.  I am looking to somehow switch this $\epsilon$ for $\frac{1}{k}$ and then take $k$ to infinity and use DCT.  However, I feel that it cannot be so simple.  How do I go about applying DCT to limits which are not going to infinity? Any hints would be appreciated.","I feel like this question may have already been asked, but despite my searches, I could not find it. I am looking to prove that $\lim\limits_{\epsilon \rightarrow 0} \int_{[b, b+\epsilon]} g=0$  for an integrable function $g$.  To do so, I would like to use the Dominated Convergence Theorem, as $g$ is my integrable dominating function.  However, I realize that the DCT is defined for the limit as $n$ goes to infinity.  I am looking to somehow switch this $\epsilon$ for $\frac{1}{k}$ and then take $k$ to infinity and use DCT.  However, I feel that it cannot be so simple.  How do I go about applying DCT to limits which are not going to infinity? Any hints would be appreciated.",,"['real-analysis', 'limits']"
95,Finding the limit of $\sqrt{9x^2+x} -3x$ at infinity [duplicate],Finding the limit of  at infinity [duplicate],\sqrt{9x^2+x} -3x,"This question already has answers here : Finding $\lim\limits_{x \to \infty} (\sqrt{9x^2+x} - 3x)$ (3 answers) Limit of $\sqrt{4x^2 + 3x} - 2x$ as $x \to \infty$ (4 answers) Closed 8 years ago . To find the limit $$\lim_{x\to\infty} (\sqrt{9x^2+x} -3x)$$ Basically I simplified this down to $$\lim_{x\to\infty} \frac{1}{\sqrt{9+1/x}+3x}$$ And I am unaware of what to do next. I tried to just sub in infinity and I get an answer of $0$ , since $1 / \infty = 0$. However, on symbolab, when I enter the problem it gives me an answer of $1/6$. Can anyone please explain to me what I need to do from this point? I haven't learned l'Hopitals rule yet so please don't suggest that. Thanks","This question already has answers here : Finding $\lim\limits_{x \to \infty} (\sqrt{9x^2+x} - 3x)$ (3 answers) Limit of $\sqrt{4x^2 + 3x} - 2x$ as $x \to \infty$ (4 answers) Closed 8 years ago . To find the limit $$\lim_{x\to\infty} (\sqrt{9x^2+x} -3x)$$ Basically I simplified this down to $$\lim_{x\to\infty} \frac{1}{\sqrt{9+1/x}+3x}$$ And I am unaware of what to do next. I tried to just sub in infinity and I get an answer of $0$ , since $1 / \infty = 0$. However, on symbolab, when I enter the problem it gives me an answer of $1/6$. Can anyone please explain to me what I need to do from this point? I haven't learned l'Hopitals rule yet so please don't suggest that. Thanks",,"['calculus', 'limits', 'radicals', 'limits-without-lhopital']"
96,Show that $f(x):=\frac{2x^3+x^2+x\sin(x)}{(\exp(x)-1)^2}$ is continuously extendable to $x_0=0$.,Show that  is continuously extendable to .,f(x):=\frac{2x^3+x^2+x\sin(x)}{(\exp(x)-1)^2} x_0=0,"What I know If $\lim\limits_{x \to x_0}f(x) := r$ exists, we can create a new function $\tilde f(x) = \begin{cases} f(x) &\text{if }x\in\mathbb{D}\setminus x_0 \\  r & \text{if }x = x_0 \end{cases}$ which is then the continously extended version of $f$. What my problem is I am struggling with $\lim\limits_{x\to 0}\frac{2x^3+x^2+x\sin(x)} {(\exp(x)-1)^2}:=r$. I tried using L'Hôpital's rule, because I noticed that  both denominator and numerator would equal to $0$ if I plug in $0$. This unfortunately didn't help at all, because you can derive those expressions as often as you want, without making your life easier. I deliberately phrased this question in regards of solving continuity problems like this, because I think that calculating the limit in this subtask of an actual first term exam is too hard. There has to be another way of solving this continuity issue, without having to calculate the limit. If there's no way around finding $r$, then there has to be an obvious trick that I am unaware of. Help is greatly appreciated!","What I know If $\lim\limits_{x \to x_0}f(x) := r$ exists, we can create a new function $\tilde f(x) = \begin{cases} f(x) &\text{if }x\in\mathbb{D}\setminus x_0 \\  r & \text{if }x = x_0 \end{cases}$ which is then the continously extended version of $f$. What my problem is I am struggling with $\lim\limits_{x\to 0}\frac{2x^3+x^2+x\sin(x)} {(\exp(x)-1)^2}:=r$. I tried using L'Hôpital's rule, because I noticed that  both denominator and numerator would equal to $0$ if I plug in $0$. This unfortunately didn't help at all, because you can derive those expressions as often as you want, without making your life easier. I deliberately phrased this question in regards of solving continuity problems like this, because I think that calculating the limit in this subtask of an actual first term exam is too hard. There has to be another way of solving this continuity issue, without having to calculate the limit. If there's no way around finding $r$, then there has to be an obvious trick that I am unaware of. Help is greatly appreciated!",,"['calculus', 'limits', 'continuity']"
97,Is $2^{\infty}$ an Indeterminate form,Is  an Indeterminate form,2^{\infty},We know that when $$\lim_{x \to a}f(x) \to 1$$ and $$\lim_{x \to a}g(x) \to \infty$$ then $$\lim_{x\to a}f(x)^{g(x)}$$ is an indeterminate form since in the neighbourhood of $a$ we cannot predict the exact value to where the limit approaches. Is it indeterminate if the limit is in $2^{\infty}$ form?,We know that when $$\lim_{x \to a}f(x) \to 1$$ and $$\lim_{x \to a}g(x) \to \infty$$ then $$\lim_{x\to a}f(x)^{g(x)}$$ is an indeterminate form since in the neighbourhood of $a$ we cannot predict the exact value to where the limit approaches. Is it indeterminate if the limit is in $2^{\infty}$ form?,,"['calculus', 'limits', 'indeterminate-forms']"
98,Solve limit with Lagrange theorem,Solve limit with Lagrange theorem,,"I tried to solve this limit: $$ \lim_{x \to +\infty} x^2\left(e^{\frac{1}{x+1}}-e^{\frac{1}{x}}\right) $$ Instead of solving it with Taylor series (using $u = 1/x$), I noticed that the difference within the parenthesis is the $\Delta f$ of the function $e^{\frac{1}{x}}$. $\lim\limits_{x \to +\infty} x^2 * \Delta\left(e^{1/x}\right) = $ The Lagrange theorem says that $\Delta f = D\left[f\right] * \Delta x$, so $=\lim\limits_{x \to +\infty} x^2 * \left( D\left[e^{1/x}\right]_{x_0} * \Delta x\right), x_0 \in \left(x, x+1\right)$ $$ x_0=x+r(x)$$    $$ 0<r(x)<1 $$ $=\lim\limits_{x \to +\infty} x^2 * \left( e^{\frac{1}{x+r(x)}} * \left(-\frac{1}{\left(x+r(x)\right)^2}\right)\right) $ $=\lim\limits_{x \to +\infty} -\frac{1}{\left(x+r(x)\right)^2} * x^2 * e^{\frac{1}{x+r(x)}}$ $\approx \lim\limits_{x \to +\infty} -\frac{1}{x^2} * x^2 * e^{\frac{1}{x}} $ $=\lim\limits_{x \to +\infty} -1 * e^{\frac{1}{x}} = -1$ Is it correct to proceed in this way? EDIT: Using approximations is not a very elegant solution. I might be more precise using the Squeeze theorem (I'll think the solution, then I'll post it) Instead of using the approximation, we consider these three functions: $f(x) = x^2 * e^{\frac{1}{x+1}} * \frac{1}{(x+1)^2}$ $h(x) = x^2 * e^{\frac{1}{x+r(x)}} * \frac{1}{(x+r(x))^2}$ $g(x) = x^2 * e^{\frac{1}{x}} * \frac{1}{x^2}$ Obviously, $f(x) \le h(x) \le g(x)$ for $x > 0$ and $r(x) \in (0, 1)$. $\lim\limits_{x \to +\infty} f(x) = \lim\limits_{x \to +\infty} x^2 * e^{\frac{1}{x+1}} * \frac{1}{(x+1)^2} = 1$ (It is 1 and not -1 just because I left the minus sign out of the limit) $\lim\limits_{x \to +\infty} g(x) = \lim\limits_{x \to +\infty} x^2 * e^{\frac{1}{x}} * \frac{1}{x^2} = 1$ So $\lim\limits_{x \to +\infty} h(x) = 1 \rightarrow \lim\limits_{x \to +\infty} -h(x) = \lim\limits_{x \to +\infty} x^2\left(e^{\frac{1}{x+1}}-e^{\frac{1}{x}}\right) = -1$ Here's how the f (the red function) and g (the blue function) look like:","I tried to solve this limit: $$ \lim_{x \to +\infty} x^2\left(e^{\frac{1}{x+1}}-e^{\frac{1}{x}}\right) $$ Instead of solving it with Taylor series (using $u = 1/x$), I noticed that the difference within the parenthesis is the $\Delta f$ of the function $e^{\frac{1}{x}}$. $\lim\limits_{x \to +\infty} x^2 * \Delta\left(e^{1/x}\right) = $ The Lagrange theorem says that $\Delta f = D\left[f\right] * \Delta x$, so $=\lim\limits_{x \to +\infty} x^2 * \left( D\left[e^{1/x}\right]_{x_0} * \Delta x\right), x_0 \in \left(x, x+1\right)$ $$ x_0=x+r(x)$$    $$ 0<r(x)<1 $$ $=\lim\limits_{x \to +\infty} x^2 * \left( e^{\frac{1}{x+r(x)}} * \left(-\frac{1}{\left(x+r(x)\right)^2}\right)\right) $ $=\lim\limits_{x \to +\infty} -\frac{1}{\left(x+r(x)\right)^2} * x^2 * e^{\frac{1}{x+r(x)}}$ $\approx \lim\limits_{x \to +\infty} -\frac{1}{x^2} * x^2 * e^{\frac{1}{x}} $ $=\lim\limits_{x \to +\infty} -1 * e^{\frac{1}{x}} = -1$ Is it correct to proceed in this way? EDIT: Using approximations is not a very elegant solution. I might be more precise using the Squeeze theorem (I'll think the solution, then I'll post it) Instead of using the approximation, we consider these three functions: $f(x) = x^2 * e^{\frac{1}{x+1}} * \frac{1}{(x+1)^2}$ $h(x) = x^2 * e^{\frac{1}{x+r(x)}} * \frac{1}{(x+r(x))^2}$ $g(x) = x^2 * e^{\frac{1}{x}} * \frac{1}{x^2}$ Obviously, $f(x) \le h(x) \le g(x)$ for $x > 0$ and $r(x) \in (0, 1)$. $\lim\limits_{x \to +\infty} f(x) = \lim\limits_{x \to +\infty} x^2 * e^{\frac{1}{x+1}} * \frac{1}{(x+1)^2} = 1$ (It is 1 and not -1 just because I left the minus sign out of the limit) $\lim\limits_{x \to +\infty} g(x) = \lim\limits_{x \to +\infty} x^2 * e^{\frac{1}{x}} * \frac{1}{x^2} = 1$ So $\lim\limits_{x \to +\infty} h(x) = 1 \rightarrow \lim\limits_{x \to +\infty} -h(x) = \lim\limits_{x \to +\infty} x^2\left(e^{\frac{1}{x+1}}-e^{\frac{1}{x}}\right) = -1$ Here's how the f (the red function) and g (the blue function) look like:",,['limits']
99,An approximation of $\log \frac{n+1}{n}$ inside of a limit,An approximation of  inside of a limit,\log \frac{n+1}{n},"This question arises from a question posed here : $$\lim_{n \to \infty} n - n^2 \log \frac{n+1}{n} = \frac{1}{2}$$ I figured we could approximate $\log \frac{n+1}{n} = \int_{n}^{n+1} \frac{1}{x} \, dx \approx \frac{1}{2} ( \frac{1}{n} + \frac{1}{n+1} )$ and we get the right answer if we plug that in and then take the limit. This method is worrying, however, since we seem to have applied limits separately. Indeed, if we take any different approximation of the $\log$ term of the form $\frac{\alpha}{n} + \frac{(1 - \alpha)}{n+1}$ with $\alpha \in (0,1)$ we get the limit to be $1 - \alpha$. I would actually think some $\alpha < 1/2$ would be ideal since $\log$ is concave, but I suppose as $n \to \infty$ the behavior of log on any interval $[n, n+1]$ becomes more and more linear, hence why we take $\alpha = \frac{1}{2}$. This is a lot of text; I suppose the question is: is it rigorous to approximate $\log$ like this and then apply the limit separately to the rest of the term, or are we just lucky that it works out? If it is not rigorous, what arguments can we make to better understand why and convince ourselves that it works out?","This question arises from a question posed here : $$\lim_{n \to \infty} n - n^2 \log \frac{n+1}{n} = \frac{1}{2}$$ I figured we could approximate $\log \frac{n+1}{n} = \int_{n}^{n+1} \frac{1}{x} \, dx \approx \frac{1}{2} ( \frac{1}{n} + \frac{1}{n+1} )$ and we get the right answer if we plug that in and then take the limit. This method is worrying, however, since we seem to have applied limits separately. Indeed, if we take any different approximation of the $\log$ term of the form $\frac{\alpha}{n} + \frac{(1 - \alpha)}{n+1}$ with $\alpha \in (0,1)$ we get the limit to be $1 - \alpha$. I would actually think some $\alpha < 1/2$ would be ideal since $\log$ is concave, but I suppose as $n \to \infty$ the behavior of log on any interval $[n, n+1]$ becomes more and more linear, hence why we take $\alpha = \frac{1}{2}$. This is a lot of text; I suppose the question is: is it rigorous to approximate $\log$ like this and then apply the limit separately to the rest of the term, or are we just lucky that it works out? If it is not rigorous, what arguments can we make to better understand why and convince ourselves that it works out?",,"['real-analysis', 'limits']"
