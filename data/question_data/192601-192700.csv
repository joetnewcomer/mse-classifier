,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Can someone please explain the joke in this picture?,Can someone please explain the joke in this picture?,,"I came across the following picture: I tried to do some research online to figure out why this picture was funny - but I couldn't really figure it out. Am I missing something, can someone please tell me what the intended joke in this picture is supposed to be? Thanks!","I came across the following picture: I tried to do some research online to figure out why this picture was funny - but I couldn't really figure it out. Am I missing something, can someone please tell me what the intended joke in this picture is supposed to be? Thanks!",,"['calculus', 'derivatives', 'soft-question']"
1,Does composition with itself convex implies function convex?,Does composition with itself convex implies function convex?,,"Let $f:[0,1] \to [0,1]$ be a twice differentiable increasing function and suppose that $g:=f \circ f$ is a convex function. Is it true that $f$ itself is convex? I am aware that the converse implication is true (namely that if $f$ is increasing and convex then $g$ is convex) but this seems to be more difficult.",Let be a twice differentiable increasing function and suppose that is a convex function. Is it true that itself is convex? I am aware that the converse implication is true (namely that if is increasing and convex then is convex) but this seems to be more difficult.,"f:[0,1] \to [0,1] g:=f \circ f f f g","['real-analysis', 'derivatives', 'convex-analysis']"
2,"$\forall n,\lim_{x\to 0}\frac{f(x)}{x^n}=0\Leftrightarrow\forall n,\lim_{x\to 0}\frac{f'(x)}{x^n}=0$?",?,"\forall n,\lim_{x\to 0}\frac{f(x)}{x^n}=0\Leftrightarrow\forall n,\lim_{x\to 0}\frac{f'(x)}{x^n}=0","Suppose $f$ is continuous function satisfying $f(0)=0$ and $f$ is differentiable at anywhere except $0$ . $(\Rightarrow)$ Assume that $$ \lim_{x\to 0}\frac{f'(x)}{x^n}=0 $$ for all $n\in\mathbb{N}$ . Then by L'Hospital's rule, $$ \lim_{x\to 0}\frac{f(x)}{x^n}=\lim_{x\to 0}\frac{f'(x)}{nx^{n-1}}=0 $$ for all $n\in\mathbb{N}$ because $\lim_{x\to 0}\frac{f(x)}{x}=\lim_{x\to0}f'(x)=0$ too. $(\Leftarrow)$ I'm tried to show this, but I struck by showing existence of $\lim_{x\to 0}\frac{f'(x)}{x^n}$ . If $\lim_{x\to 0}\frac{f'(x)}{x^n}$ exists, then obviously $\lim_{x\to 0}\frac{f'(x)}{x^n}=0$ . Can we show this? Any ideas will be appreciated.","Suppose is continuous function satisfying and is differentiable at anywhere except . Assume that for all . Then by L'Hospital's rule, for all because too. I'm tried to show this, but I struck by showing existence of . If exists, then obviously . Can we show this? Any ideas will be appreciated.","f f(0)=0 f 0 (\Rightarrow) 
\lim_{x\to 0}\frac{f'(x)}{x^n}=0
 n\in\mathbb{N} 
\lim_{x\to 0}\frac{f(x)}{x^n}=\lim_{x\to 0}\frac{f'(x)}{nx^{n-1}}=0
 n\in\mathbb{N} \lim_{x\to 0}\frac{f(x)}{x}=\lim_{x\to0}f'(x)=0 (\Leftarrow) \lim_{x\to 0}\frac{f'(x)}{x^n} \lim_{x\to 0}\frac{f'(x)}{x^n} \lim_{x\to 0}\frac{f'(x)}{x^n}=0","['real-analysis', 'calculus', 'limits', 'derivatives']"
3,On one example involving the mean value theorem,On one example involving the mean value theorem,,"Let me first remind you of what the theorem states (mainly for you to understand the notation): If $f(x)$ is continuous in the closed interval $[x_1;x_2]$ and differentiable at every point of the open interval $(x_1;x_2)$ , then there is at least one point $\xi \in (x_1;x_2)$ such that $$ \frac{f(x_2)-f(x_1)}{x_2-x_1} = f’(\xi)$$ I have been asked to determine the value of $\xi = \xi(x_1,x_2)$ , given that $$ f(x) = \frac{1}{x^2 + 1}.$$ The problem is that when I obtained the derivative $f’(x)$ and, given the definition of $f(x)$ , rewrote the expression from the theorem and simplified it, I arrived at the following equation: $$\xi^4 + 2\xi^2 - 2p\xi + 1 = 0,$$ where $$ p \equiv \frac{(x_1^2 + 1)(x_2^2 + 1)}{x_1 + x_2}.$$ The form of the equation, which is a quartic (though a depressed one, but that does not make the matters easier), suggests that there exist several roots of rather complicated form, but when I look at the graph of $f(x)$ , I see that almost always there is a singular value of $\xi$ (checking by means of the geometric interpretation of the mean value theorem) which satisfies the theorem. The questions is whether it is possible to avoid the cumbersome encounter with the quartic, or it is something I shall inevitably face in order to solve the problem? I also desire to ask you to propose hints only . I want to solve the problem myself, for otherwise I will not be able to learn.","Let me first remind you of what the theorem states (mainly for you to understand the notation): If is continuous in the closed interval and differentiable at every point of the open interval , then there is at least one point such that I have been asked to determine the value of , given that The problem is that when I obtained the derivative and, given the definition of , rewrote the expression from the theorem and simplified it, I arrived at the following equation: where The form of the equation, which is a quartic (though a depressed one, but that does not make the matters easier), suggests that there exist several roots of rather complicated form, but when I look at the graph of , I see that almost always there is a singular value of (checking by means of the geometric interpretation of the mean value theorem) which satisfies the theorem. The questions is whether it is possible to avoid the cumbersome encounter with the quartic, or it is something I shall inevitably face in order to solve the problem? I also desire to ask you to propose hints only . I want to solve the problem myself, for otherwise I will not be able to learn.","f(x) [x_1;x_2] (x_1;x_2) \xi \in (x_1;x_2)  \frac{f(x_2)-f(x_1)}{x_2-x_1} = f’(\xi) \xi = \xi(x_1,x_2)  f(x) = \frac{1}{x^2 + 1}. f’(x) f(x) \xi^4 + 2\xi^2 - 2p\xi + 1 = 0,  p \equiv \frac{(x_1^2 + 1)(x_2^2 + 1)}{x_1 + x_2}. f(x) \xi","['real-analysis', 'derivatives']"
4,Proof of Faà di Bruno formula,Proof of Faà di Bruno formula,,"By induction, I want to prove the Faà di Bruno formula for $n$ -times differentiable $f,g$ \begin{align*}         D^n(f\circ g) = \sum_{(k_1,\ldots,k_n)\in T_n} \frac{n!}{k_1!\cdots k_n!} \Big(\big(D^{k_1+\ldots+k_n} f\big) \circ g\Big) \prod_{\substack{m=1\\k_m\geq 1}}^n \bigg( \frac{D^m g}{m!}\bigg)^{k_m},     \end{align*} where $T_n := \big\{k\in \mathbb{N}_0^n : \sum_{j=1}^n j k_j = n\big\}$ . Induction step: \begin{align*}             D^n (f\circ g) &= D D^{n-1} (f\circ g) \\             &= D \Bigg( \sum_{(k_1,\ldots,k_{n-1})\in T_{n-1}} \frac{(n-1)!}{k_1!\cdots k_{n-1}!} \Big(\big(D^{k_1+\ldots+k_{n-1}} f\big) \circ g\Big) \prod_{\substack{m=1\\k_m\geq 1}}^{n-1} \bigg( \frac{D^m g}{m!}\bigg)^{k_m}\Bigg)\\             &= \sum_{(k_1,\ldots,k_{n-1})\in T_{n-1}} \frac{(n-1)!}{k_1!\cdots k_{n-1}!} D\Bigg( \Big(\big(D^{k_1+\ldots+k_{n-1}} f\big) \circ g\Big) \prod_{\substack{m=1\\k_m\geq 1}}^{n-1} \bigg( \frac{D^m g}{m!}\bigg)^{k_m}\Bigg)\\             &= \sum_{(k_1,\ldots,k_{n-1})\in T_{n-1}} \frac{(n-1)!}{k_1!\cdots k_{n-1}!} \Bigg( \Big(\big(D^{k_1+\ldots+k_{n-1}+1} f\big) \circ g\Big) Dg \prod_{\substack{m=1\\k_m\geq 1}}^{n-1} \bigg( \frac{D^m g}{m!}\bigg)^{k_m} \\             &\hspace{0.5cm}+ \Big(\big(D^{k_1+\ldots+k_{n-1}} f\big) \circ g\Big) \sum_{\substack{j=1\\k_j\geq 1}}^{n-1} k_j \bigg(\frac{D^j g}{j!}\bigg)^{k_j-1} \frac{D^{j+1} g}{j!} \prod_{\substack{m=1\\m\neq j\\k_m\geq 1}}^{n-1} \bigg( \frac{D^m g}{m!}\bigg)^{k_m}\Bigg)\\             &= \Bigg(\sum_{(k_1,\ldots,k_{n-1})\in T_{n-1}} \frac{(n-1)!}{k_1!\cdots k_{n-1}!} \Bigg( \Big(\big(D^{k_1+\ldots+k_{n-1}+1} f\big) \circ g\Big) Dg \prod_{\substack{m=1\\k_m\geq 1}}^{n-1} \bigg( \frac{D^m g}{m!}\bigg)^{k_m}\Bigg)\Bigg) \\             &\hspace{0.5cm}+ \Bigg(\sum_{(k_1,\ldots,k_{n-1})\in T_{n-1}} \frac{(n-1)!}{k_1!\cdots k_{n-1}!} \Bigg(\Big(\big(D^{k_1+\ldots+k_{n-1}} f\big) \circ g\Big) \sum_{\substack{j=1\\k_j\geq 1}}^{n-1} k_j \bigg(\frac{D^j g}{j!}\bigg)^{k_j-1} \frac{D^{j+1} g}{j!} \prod_{\substack{m=1\\m\neq j\\k_m\geq 1}}^{n-1} \bigg( \frac{D^m g}{m!}\bigg)^{k_m}\Bigg)\Bigg)\\             &= \Bigg(\sum_{(k_1,\ldots,k_{n})\in (T_{n-1},0)} \frac{(n-1)!}{k_1!\cdots k_{n-1}!} \Bigg( \Big(\big(D^{k_1+1+\ldots+k_{n-1}} f\big) \circ g\Big) \bigg(\frac{D^1g}{1!}\bigg)^{k_1+1} \prod_{\substack{m=2\\k_m\geq 1}}^{n} \bigg( \frac{D^m g}{m!}\bigg)^{k_m}\Bigg)\Bigg) \\             &\hspace{0.5cm}+ \Bigg(\sum_{(k_1,\ldots,k_{n-1})\in T_{n-1}} \frac{(n-1)!}{k_1!\cdots k_{n-1}!} \Bigg(\Big(\big(D^{k_1+\ldots+k_{n-1}} f\big) \circ g\Big) \sum_{\substack{j=1\\k_j\geq 1}}^{n-1} k_j \bigg(\frac{D^j g}{j!}\bigg)^{k_j-1} \frac{D^{j+1} g}{j!} \prod_{\substack{m=1\\m\neq j\\k_m\geq 1}}^{n-1} \bigg( \frac{D^m g}{m!}\bigg)^{k_m}\Bigg)\Bigg)\\         \end{align*} I don't know how to split $T_n$ so that I arrive at the correct sum, has anybody an idea?","By induction, I want to prove the Faà di Bruno formula for -times differentiable where . Induction step: I don't know how to split so that I arrive at the correct sum, has anybody an idea?","n f,g \begin{align*}
        D^n(f\circ g) = \sum_{(k_1,\ldots,k_n)\in T_n} \frac{n!}{k_1!\cdots k_n!} \Big(\big(D^{k_1+\ldots+k_n} f\big) \circ g\Big) \prod_{\substack{m=1\\k_m\geq 1}}^n \bigg( \frac{D^m g}{m!}\bigg)^{k_m},
    \end{align*} T_n := \big\{k\in \mathbb{N}_0^n : \sum_{j=1}^n j k_j = n\big\} \begin{align*}
            D^n (f\circ g) &= D D^{n-1} (f\circ g) \\
            &= D \Bigg( \sum_{(k_1,\ldots,k_{n-1})\in T_{n-1}} \frac{(n-1)!}{k_1!\cdots k_{n-1}!} \Big(\big(D^{k_1+\ldots+k_{n-1}} f\big) \circ g\Big) \prod_{\substack{m=1\\k_m\geq 1}}^{n-1} \bigg( \frac{D^m g}{m!}\bigg)^{k_m}\Bigg)\\
            &= \sum_{(k_1,\ldots,k_{n-1})\in T_{n-1}} \frac{(n-1)!}{k_1!\cdots k_{n-1}!} D\Bigg( \Big(\big(D^{k_1+\ldots+k_{n-1}} f\big) \circ g\Big) \prod_{\substack{m=1\\k_m\geq 1}}^{n-1} \bigg( \frac{D^m g}{m!}\bigg)^{k_m}\Bigg)\\
            &= \sum_{(k_1,\ldots,k_{n-1})\in T_{n-1}} \frac{(n-1)!}{k_1!\cdots k_{n-1}!} \Bigg( \Big(\big(D^{k_1+\ldots+k_{n-1}+1} f\big) \circ g\Big) Dg \prod_{\substack{m=1\\k_m\geq 1}}^{n-1} \bigg( \frac{D^m g}{m!}\bigg)^{k_m} \\
            &\hspace{0.5cm}+ \Big(\big(D^{k_1+\ldots+k_{n-1}} f\big) \circ g\Big) \sum_{\substack{j=1\\k_j\geq 1}}^{n-1} k_j \bigg(\frac{D^j g}{j!}\bigg)^{k_j-1} \frac{D^{j+1} g}{j!} \prod_{\substack{m=1\\m\neq j\\k_m\geq 1}}^{n-1} \bigg( \frac{D^m g}{m!}\bigg)^{k_m}\Bigg)\\
            &= \Bigg(\sum_{(k_1,\ldots,k_{n-1})\in T_{n-1}} \frac{(n-1)!}{k_1!\cdots k_{n-1}!} \Bigg( \Big(\big(D^{k_1+\ldots+k_{n-1}+1} f\big) \circ g\Big) Dg \prod_{\substack{m=1\\k_m\geq 1}}^{n-1} \bigg( \frac{D^m g}{m!}\bigg)^{k_m}\Bigg)\Bigg) \\
            &\hspace{0.5cm}+ \Bigg(\sum_{(k_1,\ldots,k_{n-1})\in T_{n-1}} \frac{(n-1)!}{k_1!\cdots k_{n-1}!} \Bigg(\Big(\big(D^{k_1+\ldots+k_{n-1}} f\big) \circ g\Big) \sum_{\substack{j=1\\k_j\geq 1}}^{n-1} k_j \bigg(\frac{D^j g}{j!}\bigg)^{k_j-1} \frac{D^{j+1} g}{j!} \prod_{\substack{m=1\\m\neq j\\k_m\geq 1}}^{n-1} \bigg( \frac{D^m g}{m!}\bigg)^{k_m}\Bigg)\Bigg)\\
            &= \Bigg(\sum_{(k_1,\ldots,k_{n})\in (T_{n-1},0)} \frac{(n-1)!}{k_1!\cdots k_{n-1}!} \Bigg( \Big(\big(D^{k_1+1+\ldots+k_{n-1}} f\big) \circ g\Big) \bigg(\frac{D^1g}{1!}\bigg)^{k_1+1} \prod_{\substack{m=2\\k_m\geq 1}}^{n} \bigg( \frac{D^m g}{m!}\bigg)^{k_m}\Bigg)\Bigg) \\
            &\hspace{0.5cm}+ \Bigg(\sum_{(k_1,\ldots,k_{n-1})\in T_{n-1}} \frac{(n-1)!}{k_1!\cdots k_{n-1}!} \Bigg(\Big(\big(D^{k_1+\ldots+k_{n-1}} f\big) \circ g\Big) \sum_{\substack{j=1\\k_j\geq 1}}^{n-1} k_j \bigg(\frac{D^j g}{j!}\bigg)^{k_j-1} \frac{D^{j+1} g}{j!} \prod_{\substack{m=1\\m\neq j\\k_m\geq 1}}^{n-1} \bigg( \frac{D^m g}{m!}\bigg)^{k_m}\Bigg)\Bigg)\\
        \end{align*} T_n","['combinatorics', 'derivatives', 'summation']"
5,A question about the derivatives of the Möbius inversion formula for $\zeta(s)$,A question about the derivatives of the Möbius inversion formula for,\zeta(s),"The following expression for the $\frac{1}{\zeta(s)}$ involving the Möbius function is well known: $$\frac{1}{\zeta(s)}=\sum _{n=1}^{\infty }\frac {\mu(n)}{n^s} \qquad s \in \mathbb{C},\Re(s) > 1$$ There also exists this (recursive) mobius inversion formula for $\zeta(s)$ : \begin{align} \zeta(s) &= 1+\sum _{k=1}^{\infty } {\frac {{\mu} \left( k \right)  }{k}\sum _{m=1}^{\infty }{\frac {\zeta \left( kms \right) -1}{m}}}\qquad s \in \mathbb{C},\Re(s) >0, s \ne \frac{1}{i}, i \in \mathbb{N}\\ \text{where the $d$-th derivative is:}\\ \zeta^{d}(s) &= \sum _{k=1}^{\infty } {{\mu} \left( k \right)  \sum _{m=1}^{\infty }{\zeta^{d}\left( kms \right)}} (km)^{d-1} \qquad d \in \mathbb{N}, d > 0\\ \text{which is simplest for $d=1$:}\\ \zeta^{1}(s) &= \sum _{k=1}^{\infty } {{\mu} \left( k \right)  \sum _{m=1}^{\infty }{\zeta^{1}\left( kms \right)}}\\ \end{align} Observe that in all equations, $k=m=1$ already yields the LHS, hence the remaining series must always sum to $0$ . Questions: Could any of these expressions be simplified further (I have tried swapping the sums, which worked fine, however didn't help simplifying things further)? Taking the anti-derivates for $d$ , always correctly recreates the $d-1$ derivative except for the step from $d=1$ to $d=0$ (i.e. when the constants $1$ and $-1$ have to re-appear). Is there a reason for why that final step to $d=0$ is more complicated?","The following expression for the involving the Möbius function is well known: There also exists this (recursive) mobius inversion formula for : Observe that in all equations, already yields the LHS, hence the remaining series must always sum to . Questions: Could any of these expressions be simplified further (I have tried swapping the sums, which worked fine, however didn't help simplifying things further)? Taking the anti-derivates for , always correctly recreates the derivative except for the step from to (i.e. when the constants and have to re-appear). Is there a reason for why that final step to is more complicated?","\frac{1}{\zeta(s)} \frac{1}{\zeta(s)}=\sum _{n=1}^{\infty }\frac {\mu(n)}{n^s} \qquad s \in \mathbb{C},\Re(s) > 1 \zeta(s) \begin{align}
\zeta(s) &= 1+\sum _{k=1}^{\infty } {\frac {{\mu} \left( k \right) 
}{k}\sum _{m=1}^{\infty }{\frac {\zeta \left( kms \right) -1}{m}}}\qquad s \in \mathbb{C},\Re(s) >0, s \ne \frac{1}{i}, i \in \mathbb{N}\\
\text{where the d-th derivative is:}\\
\zeta^{d}(s) &= \sum _{k=1}^{\infty } {{\mu} \left( k \right) 
\sum _{m=1}^{\infty }{\zeta^{d}\left( kms \right)}} (km)^{d-1} \qquad d \in \mathbb{N}, d > 0\\
\text{which is simplest for d=1:}\\
\zeta^{1}(s) &= \sum _{k=1}^{\infty } {{\mu} \left( k \right) 
\sum _{m=1}^{\infty }{\zeta^{1}\left( kms \right)}}\\
\end{align} k=m=1 0 d d-1 d=1 d=0 1 -1 d=0","['number-theory', 'derivatives', 'riemann-zeta', 'mobius-inversion']"
6,Differentiability on level sets of a locally Lipschitz function,Differentiability on level sets of a locally Lipschitz function,,"Proposition Let $f:\mathbb R^n \rightarrow \mathbb R$ be locally Lipschitz continuous, and $$Z := \{x \in \mathbb R^n \mid f(x) = 0 \}.$$ Then $Df(x) = 0$ for Leb $^n$ -a.e. $\,x \in Z.$ Where, $Df(x)$ is derivative of $f$ at $x$ . (ByLawrence C. Evans, Ronald F. Garzepy, ""Measure Theory and Fine Properties of Functions"", Chapter 3, Theorem 3.3(i)) At the beginning of the proof, it is stated that Choose $x \in Z$ so that $Df(x)$ exists, and $$\lim_{r \rightarrow 0} \frac{\textrm{Leb}^n(Z \cap B(x;r))}{\textrm{Leb}^n(B(x;r))} = 1;$$ Leb $^n$ a.e. point $x \in Z$ will do. for Leb $^n$ -a.e. $\,x \in Z.$ Why is this true? If $\textrm{Leb}^n(Z \setminus\textrm{Int}(Z)) =0$ , then this will be true in my opinion. But this is not always right. Could you give me some advice?","Proposition Let be locally Lipschitz continuous, and Then for Leb -a.e. Where, is derivative of at . (ByLawrence C. Evans, Ronald F. Garzepy, ""Measure Theory and Fine Properties of Functions"", Chapter 3, Theorem 3.3(i)) At the beginning of the proof, it is stated that Choose so that exists, and Leb a.e. point will do. for Leb -a.e. Why is this true? If , then this will be true in my opinion. But this is not always right. Could you give me some advice?","f:\mathbb R^n \rightarrow \mathbb R Z := \{x \in \mathbb R^n \mid f(x) = 0 \}. Df(x) = 0 ^n \,x \in Z. Df(x) f x x \in Z Df(x) \lim_{r \rightarrow 0} \frac{\textrm{Leb}^n(Z \cap B(x;r))}{\textrm{Leb}^n(B(x;r))} = 1; ^n x \in Z ^n \,x \in Z. \textrm{Leb}^n(Z \setminus\textrm{Int}(Z)) =0","['derivatives', 'lebesgue-measure', 'lipschitz-functions']"
7,When does $F'(c)$ exist? When does $F'(c) = f(c)$?,When does  exist? When does ?,F'(c) F'(c) = f(c),"If $f : [a,b] \to \mathbb{R} $ is Riemann integrable, we may define the function $F : [a,b] \to \mathbb{R}$ given by $$ F(x) = \int_a^x f(t) dt $$ The first fundamental theorem of calculus says that if $f$ is continuous at $c \in (a,b)$ , then $F'(c) = f(c)$ . If $f$ is not continuous at $c$ , then $F'(c)$ can fail to exist, and, if it does exist, it can happen that $F'(c) \neq f(c)$ . I'm wondering if there are easily stated necessary and sufficient conditions for when: $F'(c)$ exists. $F'(c) = f(c)$ . I've given this a little thought, but all I can see is that we have the following equivalences: $F'(c)$ exists $\iff$ $\lim_{h \to 0} \frac{\int_c^{c + h} f(t) dt}{h}$ exists $F' (c) = f(c) \iff \lim_{h \to 0} \frac{\int_c^{c + h} \left[f(t) - f(c)\right] dt}{h} = 0$ These of course are necessary and sufficient conditions, but, to my knowledge, they don't have a special name. Morally, 1 says that averages of $f$ don't ""blow up"" as intervals get smaller, and 2 says that $f$ is continuous ""in the mean."" Is there a named condition equivalent to 1 or 2?","If is Riemann integrable, we may define the function given by The first fundamental theorem of calculus says that if is continuous at , then . If is not continuous at , then can fail to exist, and, if it does exist, it can happen that . I'm wondering if there are easily stated necessary and sufficient conditions for when: exists. . I've given this a little thought, but all I can see is that we have the following equivalences: exists exists These of course are necessary and sufficient conditions, but, to my knowledge, they don't have a special name. Morally, 1 says that averages of don't ""blow up"" as intervals get smaller, and 2 says that is continuous ""in the mean."" Is there a named condition equivalent to 1 or 2?","f : [a,b] \to \mathbb{R}  F : [a,b] \to \mathbb{R} 
F(x) = \int_a^x f(t) dt
 f c \in (a,b) F'(c) = f(c) f c F'(c) F'(c) \neq f(c) F'(c) F'(c) = f(c) F'(c) \iff \lim_{h \to 0} \frac{\int_c^{c + h} f(t) dt}{h} F' (c) = f(c) \iff \lim_{h \to 0} \frac{\int_c^{c + h} \left[f(t) - f(c)\right] dt}{h} = 0 f f","['calculus', 'limits', 'derivatives']"
8,Infinitesimal generator in context of SDEs,Infinitesimal generator in context of SDEs,,"Consider the following SDE: $$X(t)= s + \int_{t_0}^t b(X(s)) dt + \int_{t_0}^t a(X(s)) dW(t) $$ where $X(t_0)=s$ and $a:\mathbb{R}^d \rightarrow \mathbb{R}^{d \times N} $ , $b:\mathbb{R}^d \rightarrow \mathbb{R}^d $ and $W$ denoting an $N$ dimensional Brownian Motion. The infinitesimal generator is defined then as $$L[u](x) = b(x) D_x u(x) + \frac{1}{2} tr[a(x)a(x)^T D^2_x u(x)]$$ I do not understand why in this formula the mixed second derivatives will not be considered, meaning something like $\partial_{x_i x_j}$ for $i \ne j $ An application of  Itos-formula on $du(X(t)))$ yields $$du(X(t))) = L[u](X(t))dt + D_x u(X(t)) a(x) dW(t)$$ Why do I not need the mixed second derivatives in this formula matching with the defintion of the infinitesimal generator?","Consider the following SDE: where and , and denoting an dimensional Brownian Motion. The infinitesimal generator is defined then as I do not understand why in this formula the mixed second derivatives will not be considered, meaning something like for An application of  Itos-formula on yields Why do I not need the mixed second derivatives in this formula matching with the defintion of the infinitesimal generator?",X(t)= s + \int_{t_0}^t b(X(s)) dt + \int_{t_0}^t a(X(s)) dW(t)  X(t_0)=s a:\mathbb{R}^d \rightarrow \mathbb{R}^{d \times N}  b:\mathbb{R}^d \rightarrow \mathbb{R}^d  W N L[u](x) = b(x) D_x u(x) + \frac{1}{2} tr[a(x)a(x)^T D^2_x u(x)] \partial_{x_i x_j} i \ne j  du(X(t))) du(X(t))) = L[u](X(t))dt + D_x u(X(t)) a(x) dW(t),"['derivatives', 'stochastic-calculus', 'stochastic-differential-equations']"
9,Calculus of variation - how to make a function stationary?,Calculus of variation - how to make a function stationary?,,"I just started learning about the calculus of variations and the following question arose to me: Given some functional $J[y]$ , there exists the notation of a directional derivative defined as $$ \frac{d}{dy}J[y] = \lim_{\epsilon \to 0}\frac{J[y+\epsilon h] - J[y]}{\epsilon} \quad\quad \quad(1) $$ which denotes the derivative of $J[y]$ in the direction of the function $h$ (for simplicity let's don't further examine the particularities of the function space at consideration). Now, if we wanted to find a function that makes  the functional $J[y]$ stationary one would similarly to the function-case take the derivative and set it to zero. However, this seems like a very complicated task in this case since there is no particular function with respect to which we could take the derivative which is why one considers a different approach where we assume $\phi$ to be the function which makes $J[y]$ stationary and then consider a small variation $J[\phi + \epsilon h] $ and now want compute $\left[\dfrac{d}{d\epsilon}J[\phi + \epsilon h]\right]_0$ which is a simpler task because one now has to deal with an ordinary derivative instead of a directional $(1)$ . Am I on the correct path of understanding this or did I misunderstand something? If the latter holds, I would appreciate it if someone could maybe clarify those points for me.","I just started learning about the calculus of variations and the following question arose to me: Given some functional , there exists the notation of a directional derivative defined as which denotes the derivative of in the direction of the function (for simplicity let's don't further examine the particularities of the function space at consideration). Now, if we wanted to find a function that makes  the functional stationary one would similarly to the function-case take the derivative and set it to zero. However, this seems like a very complicated task in this case since there is no particular function with respect to which we could take the derivative which is why one considers a different approach where we assume to be the function which makes stationary and then consider a small variation and now want compute which is a simpler task because one now has to deal with an ordinary derivative instead of a directional . Am I on the correct path of understanding this or did I misunderstand something? If the latter holds, I would appreciate it if someone could maybe clarify those points for me.","J[y] 
\frac{d}{dy}J[y] = \lim_{\epsilon \to 0}\frac{J[y+\epsilon h] - J[y]}{\epsilon}
\quad\quad \quad(1)
 J[y] h J[y] \phi J[y] J[\phi + \epsilon h]  \left[\dfrac{d}{d\epsilon}J[\phi + \epsilon h]\right]_0 (1)","['real-analysis', 'functional-analysis', 'derivatives']"
10,Is a function differentiable if its directional derivatives converge uniformly on the unit vectors?,Is a function differentiable if its directional derivatives converge uniformly on the unit vectors?,,"Given a function $f:\mathbb{R}^n \to \mathbb{R}$ , we say that its directional derivatives converge in $\mathbf{a}\in \mathbb{R}^n$ uniformly with respect to the unit vectors if $\forall \varepsilon >0$ there exists $\delta >0$ such that, for every $t \in B_\delta(0)$ , $$ \left|\frac{f(\mathbf{a}+t\mathbf{v})-f(\mathbf{a})}{t}-D_\mathbf{v}f(\mathbf{a})\right|<\varepsilon \quad \forall\, \mathbf{v}\,:\,\|\mathbf{v}\|=1 $$ where $D_\mathbf{v}f(\mathbf{a})$ denotes the direction derivative with respect to $\mathbf{v}$ . Now, I know that the existence of all the directional derivatives doesn't guarantee the differentiability of a function, but I was wondering, what can we say if its directional derivatives converge uniformly on the unit vector? Intuitively I'd say that the function is also differentiable in $\mathbf{a}$ , but I'm not exactly sure how to show it. Here's my attempt : I'd like to show that $\lim_{{x} \to {a}}  \frac{{f}({x})-{f}({a}) - {T}({x}-{a})}{\|{x}-{a}\|} = 0$ for some linear transformation $T$ . Ignoring for a moment the part about the linear transformation, I can rewrite the first inequality as: $$ \left|\frac{f(\mathbf{x})-f(\mathbf{a})-D_\mathbf{v}f(\mathbf{a})\|\mathbf{x}-\mathbf{a}\|}{\|\mathbf{x}-\mathbf{a}\|}\right|<\varepsilon \quad \forall\, \mathbf{x}\,:\,\|\mathbf{v}\|=1 $$ with $\mathbf{x}=\mathbf{a}+t\mathbf{v}$ and $\|t \mathbf{v}\|=t=\|\mathbf{x}-\mathbf{a}\|$ . At this point I'm stuck; maybe I'm missing something trivial. Did I do something wrong? Is the claim even true?","Given a function , we say that its directional derivatives converge in uniformly with respect to the unit vectors if there exists such that, for every , where denotes the direction derivative with respect to . Now, I know that the existence of all the directional derivatives doesn't guarantee the differentiability of a function, but I was wondering, what can we say if its directional derivatives converge uniformly on the unit vector? Intuitively I'd say that the function is also differentiable in , but I'm not exactly sure how to show it. Here's my attempt : I'd like to show that for some linear transformation . Ignoring for a moment the part about the linear transformation, I can rewrite the first inequality as: with and . At this point I'm stuck; maybe I'm missing something trivial. Did I do something wrong? Is the claim even true?","f:\mathbb{R}^n \to \mathbb{R} \mathbf{a}\in \mathbb{R}^n \forall \varepsilon >0 \delta >0 t \in B_\delta(0) 
\left|\frac{f(\mathbf{a}+t\mathbf{v})-f(\mathbf{a})}{t}-D_\mathbf{v}f(\mathbf{a})\right|<\varepsilon \quad \forall\, \mathbf{v}\,:\,\|\mathbf{v}\|=1
 D_\mathbf{v}f(\mathbf{a}) \mathbf{v} \mathbf{a} \lim_{{x} \to {a}} 
\frac{{f}({x})-{f}({a}) - {T}({x}-{a})}{\|{x}-{a}\|} = 0 T 
\left|\frac{f(\mathbf{x})-f(\mathbf{a})-D_\mathbf{v}f(\mathbf{a})\|\mathbf{x}-\mathbf{a}\|}{\|\mathbf{x}-\mathbf{a}\|}\right|<\varepsilon \quad \forall\, \mathbf{x}\,:\,\|\mathbf{v}\|=1
 \mathbf{x}=\mathbf{a}+t\mathbf{v} \|t \mathbf{v}\|=t=\|\mathbf{x}-\mathbf{a}\|","['real-analysis', 'calculus', 'derivatives']"
11,Definitions of f(x) when they are in the Dirac delta function argument ( δ[ f(x) ] ).,Definitions of f(x) when they are in the Dirac delta function argument ( δ[ f(x) ] ).,,"I edited the question to explore definitions other than this question . I`m trying to simplify $$δ((x^2-a^2)^{1/2})$$ using $$\delta\big(f(x)\big) = \sum_{i}\frac{\delta(x-a_{i})}{\left|{\frac{df}{dx}(a_{i})}\right|}$$ but the derivative in the denominator diverges in the points $a$ and $-a$ . From what I've been reading here on the mathstackexchange, the argument f(x) of the Dirac delta must be continuously differentiable. The derivative of $$(x^2-a^2)^{1/2}$$ is $$\frac{x}{\sqrt{x^2-a^2}}$$ . If the domain and image of f(x) are real, since, as far as I've learned, the Dirac delta argument cannot be complex, this derivative is discontinuous in the closed interval [-a;a]. Is this argument correct? From this, is it possible to say f(x) is not continuously differentiable and that the function $$δ((x^2-a^2)^{1/2})$$ is undefined?","I edited the question to explore definitions other than this question . I`m trying to simplify using but the derivative in the denominator diverges in the points and . From what I've been reading here on the mathstackexchange, the argument f(x) of the Dirac delta must be continuously differentiable. The derivative of is . If the domain and image of f(x) are real, since, as far as I've learned, the Dirac delta argument cannot be complex, this derivative is discontinuous in the closed interval [-a;a]. Is this argument correct? From this, is it possible to say f(x) is not continuously differentiable and that the function is undefined?",δ((x^2-a^2)^{1/2}) \delta\big(f(x)\big) = \sum_{i}\frac{\delta(x-a_{i})}{\left|{\frac{df}{dx}(a_{i})}\right|} a -a (x^2-a^2)^{1/2} \frac{x}{\sqrt{x^2-a^2}} δ((x^2-a^2)^{1/2}),"['real-analysis', 'derivatives', 'continuity', 'dirac-delta', 'function-and-relation-composition']"
12,Proof verification for simple derivative property,Proof verification for simple derivative property,,"Given a function $f:(a, b)\to \mathbb{R},\space f\in C^1$ , show that $f'(x_0) =  c > 0, \space x_0\in (a,b)$ implies that there exists some $x_1\in (x_0-\delta,x_0)$ s.t. $f(x_1) < f(x_0)$ and some $x_2\in (x_0, x_0 +  \delta)$ s.t. $f(x_2) > f(x_0)$ . Proof: We have that $\lim\limits_{x\to x_0}\dfrac{f(x)-f(x_0)}{x-x_0} = c$ meaning for $\epsilon= c$ there exists some $\delta > 0$ s.t. for all $x\in (x_0 -\delta, x_0 + \delta)$ we have: $-\epsilon (x-x_0)  < f(x)-f(x_0)-c(x-x_0) < \epsilon(x-x_0)$ $\Longleftrightarrow -\epsilon (x-x_0)  + f(x_0) + c(x-x_0) < f(x) < \epsilon (x-x_0) + f(x_0) + c(x-x_0) $ So when choosing $x_1\in (x_0-\delta, x_0):$ $f(x) < f(x_0)$ and for $x_2\in(x_0, x_0 + \delta): f(x_0) < f(x)$ because $c >0$ . Is this proof correct? What would I have to  adjust when dealing with $f'(x_0) = \infty$ ?","Given a function , show that implies that there exists some s.t. and some s.t. . Proof: We have that meaning for there exists some s.t. for all we have: So when choosing and for because . Is this proof correct? What would I have to  adjust when dealing with ?","f:(a, b)\to \mathbb{R},\space f\in C^1 f'(x_0) =  c > 0, \space x_0\in (a,b) x_1\in (x_0-\delta,x_0) f(x_1) < f(x_0) x_2\in (x_0, x_0 +  \delta) f(x_2) > f(x_0) \lim\limits_{x\to x_0}\dfrac{f(x)-f(x_0)}{x-x_0} = c \epsilon= c \delta > 0 x\in (x_0 -\delta, x_0 + \delta) -\epsilon (x-x_0)  < f(x)-f(x_0)-c(x-x_0) < \epsilon(x-x_0) \Longleftrightarrow -\epsilon (x-x_0)  + f(x_0) + c(x-x_0) < f(x) < \epsilon (x-x_0) + f(x_0) + c(x-x_0)  x_1\in (x_0-\delta, x_0): f(x) < f(x_0) x_2\in(x_0, x_0 + \delta): f(x_0) < f(x) c >0 f'(x_0) = \infty","['real-analysis', 'derivatives', 'solution-verification']"
13,Relation via derivative of two angle-based probabilities for $n$ points on a circle.,Relation via derivative of two angle-based probabilities for  points on a circle.,n,"Suppose we sample $n$ points uniformly (and independently) on the unit circle in the sense that the probability that a point lies within some circular arc is proportional to its length. For this question, we'll choose the convention to measure angles from 0 to 1 (rather than from 0 to $2\pi$ when we use radians) to make the formulae come out nicer. Then if we are given a fixed circular arc subtending an angle $0 \leq \alpha\leq \frac{1}{2}$ (the bounds chosen due to technicalities from reflex angles), we know that the probability that all $n$ points lie in this arc is $\alpha^n$ . It is also a known result/elementary exercise that the probability that $n$ such points lie within some arc which subtends an angle of $\alpha$ is $n\alpha^{n-1}$ . Is there some probabilistic reason that these formulae are related by differentiation with respect to $\alpha$ ?","Suppose we sample points uniformly (and independently) on the unit circle in the sense that the probability that a point lies within some circular arc is proportional to its length. For this question, we'll choose the convention to measure angles from 0 to 1 (rather than from 0 to when we use radians) to make the formulae come out nicer. Then if we are given a fixed circular arc subtending an angle (the bounds chosen due to technicalities from reflex angles), we know that the probability that all points lie in this arc is . It is also a known result/elementary exercise that the probability that such points lie within some arc which subtends an angle of is . Is there some probabilistic reason that these formulae are related by differentiation with respect to ?",n 2\pi 0 \leq \alpha\leq \frac{1}{2} n \alpha^n n \alpha n\alpha^{n-1} \alpha,"['probability', 'derivatives', 'geometric-probability']"
14,"Is $f'$ continuous in a small interval $[0,\epsilon)$?",Is  continuous in a small interval ?,"f' [0,\epsilon)","Let $f:\mathbb R\to\mathbb R$ be differentiable on $[0,\infty)$ . Suppose that $\lim_{x\to0^+}f'(x)$ exists and is finite, and that $f'$ is continuous at $0$ . Must there exist some $\epsilon>0$ such that $f'$ is continuous on $[0,\epsilon)$ ? My hunch is that this is false. My idea is that locally near $x=1/n$ , any positive integer $n$ , the function $f$ is a copy of $\frac{x^2}{n}\sin(1/x)$ . I guess strictly speaking, near $x=1/n$ , $f$ is locally given by $$\frac{(x-1/n)^2}{n}\sin\left(\frac{1}{x-1/n}\right).$$ Then $f'$ is discontinuous at $x=1/n$ , but we should still have $f'\to0$ , as the oscillations are decreasing in amplitude. Could anyone confirm that this works and/or find a simpler counterexample?","Let be differentiable on . Suppose that exists and is finite, and that is continuous at . Must there exist some such that is continuous on ? My hunch is that this is false. My idea is that locally near , any positive integer , the function is a copy of . I guess strictly speaking, near , is locally given by Then is discontinuous at , but we should still have , as the oscillations are decreasing in amplitude. Could anyone confirm that this works and/or find a simpler counterexample?","f:\mathbb R\to\mathbb R [0,\infty) \lim_{x\to0^+}f'(x) f' 0 \epsilon>0 f' [0,\epsilon) x=1/n n f \frac{x^2}{n}\sin(1/x) x=1/n f \frac{(x-1/n)^2}{n}\sin\left(\frac{1}{x-1/n}\right). f' x=1/n f'\to0","['real-analysis', 'derivatives', 'examples-counterexamples']"
15,"If $p^2=a^2(\cos x)^2+b^2(\sin x)^2$, prove that $p +\frac{d^2 p}{dx^2}=\frac{a^2 b^2}{p^3}$","If , prove that",p^2=a^2(\cos x)^2+b^2(\sin x)^2 p +\frac{d^2 p}{dx^2}=\frac{a^2 b^2}{p^3},How to prove this? It seems simple enough to start but the at the end I cannot prove the expression $p +\frac{d^2 p}{dx^2}=\frac{a^2 b^2}{p^3}$ . Is there a particular trick I am missing or is this just an ordinary sum with lots of manipulation. I did try modifying the question slightly by multiplying both the LHS and RHS by $p^3$ . That seems to lessen the manipulation somewhat but still the answer does not come.,How to prove this? It seems simple enough to start but the at the end I cannot prove the expression . Is there a particular trick I am missing or is this just an ordinary sum with lots of manipulation. I did try modifying the question slightly by multiplying both the LHS and RHS by . That seems to lessen the manipulation somewhat but still the answer does not come.,p +\frac{d^2 p}{dx^2}=\frac{a^2 b^2}{p^3} p^3,['derivatives']
16,"If a univariate function has directional derivatives on the whole domain, does this imply that it is differentiable except on a countable set?","If a univariate function has directional derivatives on the whole domain, does this imply that it is differentiable except on a countable set?",,"Suppose we have a univariate function $f:\mathbb{R} \rightarrow \mathbb{R}$ .  It is well-known that if both directional derivatives of the function exist at a point then the function is differentiable at that point.  Of course, even if both directional derivatives exist, it is possible that they are different values at a point, in which case the function is not differentiable at that point. My Question: If both directional derivatives exist over the whole domain (i.e., all real numbers), does this imply that $f$ is differentiable except on a countable set (i.e., that the directional derivatives match except on a countable set)?  Intuitively that seems right to me, but I'm not sure how to prove it.","Suppose we have a univariate function .  It is well-known that if both directional derivatives of the function exist at a point then the function is differentiable at that point.  Of course, even if both directional derivatives exist, it is possible that they are different values at a point, in which case the function is not differentiable at that point. My Question: If both directional derivatives exist over the whole domain (i.e., all real numbers), does this imply that is differentiable except on a countable set (i.e., that the directional derivatives match except on a countable set)?  Intuitively that seems right to me, but I'm not sure how to prove it.",f:\mathbb{R} \rightarrow \mathbb{R} f,"['real-analysis', 'derivatives']"
17,First variation and Gâteaux derivatives,First variation and Gâteaux derivatives,,"I have two questions about the relation between Gâteaux and functional derivatives. In calculus of variation textbooks, the first variation is usually defined as follows. Let $V$ be a 'function space' and $f: V \to \mathbb{R}$ a functional. Then, the first variation of $f$ is defined by: \begin{eqnarray} \delta f|_{y}(\eta) \equiv \frac{d}{dt}\bigg{|}_{t = 0}f(y+t\eta) := \lim_{t\to 0}\frac{f(y+t\eta)-f(y)}{t} \tag{1}\label{1} \end{eqnarray} The above definition (\ref{1}) is just the usual definition of a Gâteaux derivative of a function $f$ . As far as I know, Gâteaux derivatives are defined, in its most generic way, on locally convex spaces . Question 1: If the Gâteaux derivative is defined on locally convex spaces and if the first derivative is defined as the Gâteaux derivative of a functional $f$ , why do most books not assume $V$ to be a locally convex space and, instead, use an imprecise ""function space""? At least for most purposes, aren't locally convex spaces enough to cover all possible function spaces used in this subject? For the second question, the physicist's notion of a functional derivative is very close to the notion of a first variation defined above. In the calculus of variations, we are usually aiming to study functionals which have the form of an integral, so what physicists call functional derivative is the kernel of the integral obtained after evaluating the first derivative (in the physics literature, this kernel is defined as taken the derivative in the direction of a Dirac delta distribution). Question 2: Assume that the function $f$ has an integral form. Is it correct to define the functional derivative as the function (if it exists) $\delta f/\delta y$ satisfying: \begin{eqnarray} \frac{d}{dt}\bigg{|}_{t = 0}f(y+t\eta) = \int \frac{\delta f}{\delta y}(x)\eta(x) dx \tag{2}\label{2} \end{eqnarray} or is there a better definition?","I have two questions about the relation between Gâteaux and functional derivatives. In calculus of variation textbooks, the first variation is usually defined as follows. Let be a 'function space' and a functional. Then, the first variation of is defined by: The above definition (\ref{1}) is just the usual definition of a Gâteaux derivative of a function . As far as I know, Gâteaux derivatives are defined, in its most generic way, on locally convex spaces . Question 1: If the Gâteaux derivative is defined on locally convex spaces and if the first derivative is defined as the Gâteaux derivative of a functional , why do most books not assume to be a locally convex space and, instead, use an imprecise ""function space""? At least for most purposes, aren't locally convex spaces enough to cover all possible function spaces used in this subject? For the second question, the physicist's notion of a functional derivative is very close to the notion of a first variation defined above. In the calculus of variations, we are usually aiming to study functionals which have the form of an integral, so what physicists call functional derivative is the kernel of the integral obtained after evaluating the first derivative (in the physics literature, this kernel is defined as taken the derivative in the direction of a Dirac delta distribution). Question 2: Assume that the function has an integral form. Is it correct to define the functional derivative as the function (if it exists) satisfying: or is there a better definition?","V f: V \to \mathbb{R} f \begin{eqnarray}
\delta f|_{y}(\eta) \equiv \frac{d}{dt}\bigg{|}_{t = 0}f(y+t\eta) := \lim_{t\to 0}\frac{f(y+t\eta)-f(y)}{t} \tag{1}\label{1}
\end{eqnarray} f f V f \delta f/\delta y \begin{eqnarray}
\frac{d}{dt}\bigg{|}_{t = 0}f(y+t\eta) = \int \frac{\delta f}{\delta y}(x)\eta(x) dx \tag{2}\label{2}
\end{eqnarray}","['derivatives', 'mathematical-physics', 'calculus-of-variations', 'variational-analysis', 'gateaux-derivative']"
18,Find a third degree polynomial $p(x)$ such that the function $f$ is continuously differentiable,Find a third degree polynomial  such that the function  is continuously differentiable,p(x) f,"Function $f$ is defined piecewise as follows: $$ f = \left\{         \begin{array}{ll}             1 & \quad x<-1 \\             p(x) & \quad -1\le x\le1 \\             e^{-4(x-1)} & \quad x>1 \\         \end{array}     \right. $$ Find a third degree polynomial $p(x)$ such that the function $f$ is continuously differentiable. $$ p(x)=a_3x^3+a_2x^2+a_1x+a_0 $$ This is what I have tried this far. Lets call the functions: $$ h(x)=1 , x<-1$$ $$ g(x)=e^{-4(x-1)},x>1$$ I calculated the limits: $$\lim_{x \to -1^-} h(x)= 1$$ $$\lim_{x \to 1^+} g(x)=\lim_{x \to 1^+} e^{-4(x-1)}=1$$ Next I calculate $p(1)=1$ and $p(−1)=1$ $$p(1)=a_3+a_2+a_1+a_0 = 1$$ $$p(-1)=-a_3+a_2-a_1+a_0 = 1$$ I get: $$a_3+a_2x+a_1+a_0 + (-a_3+a_2-a_1+a_0) \Rightarrow 2a_2+2a_0=2 $$ Next I calculate the derivatives: $$ p'(x)=3a_3x^2+2a_2x+a_1 $$ $$ h'(x)=0 $$ $$ g'(x)=-4e^{-4(x-1)}$$ Next the I calculate $p'(-1) = h'(-1)$ and $p'(1) = g'(1)$ : $$3a_3-2a_2+a_1 = 0$$ $$3a_3+2a_2+a_1 = -4$$ I get: $$6a_3+2a_1=-4$$ Next I calculate the second order derivatives: $$ p''(x)=6a_3x+2a_2$$ $$ h''(x)=0 $$ $$ g''(x)=16e^{-4(x-1)}$$ Next the I calculate $p''(-1) = h''(-1)$ and $p''(1) = g''(1)$ : $$ -6a_3+2a_2=0$$ $$6a_3+2a_2=16$$ $$ \Rightarrow 4a_2=16\Rightarrow a_2=4$$ Next the I calculate $p'''(-1) = h'''(-1)$ and $p'''(1) = g'''(1)$ : $$ 6a_3=0$$ $$6a_3=-64$$ $$ \Rightarrow 12a_3=-64\Rightarrow a_3=-\frac{16}{3}$$ Now I can solve $a_0$ from the previous result $2a_2+2a_0=2$ : $$2(4)+2a_0=2\Rightarrow a_0=-3$$ I can solve $a_1$ from the previous result $6a_3+2a_1=-4$ : $$6(-\frac{16}{3})+2a_1=-4\Rightarrow a_1=14$$ Hence: $a_3 = -\frac{16}{3}\\a_2 =4\\a_1 =14\\a_0 =-3$ Edit: I found the answer. I will post it later today. In short, it was a matrix multiplication.","Function is defined piecewise as follows: Find a third degree polynomial such that the function is continuously differentiable. This is what I have tried this far. Lets call the functions: I calculated the limits: Next I calculate and I get: Next I calculate the derivatives: Next the I calculate and : I get: Next I calculate the second order derivatives: Next the I calculate and : Next the I calculate and : Now I can solve from the previous result : I can solve from the previous result : Hence: Edit: I found the answer. I will post it later today. In short, it was a matrix multiplication.","f 
f = \left\{
        \begin{array}{ll}
            1 & \quad x<-1 \\
            p(x) & \quad -1\le x\le1 \\
            e^{-4(x-1)} & \quad x>1 \\
        \end{array}
    \right.
 p(x) f  p(x)=a_3x^3+a_2x^2+a_1x+a_0   h(x)=1 , x<-1  g(x)=e^{-4(x-1)},x>1 \lim_{x \to -1^-} h(x)= 1 \lim_{x \to 1^+} g(x)=\lim_{x \to 1^+} e^{-4(x-1)}=1 p(1)=1 p(−1)=1 p(1)=a_3+a_2+a_1+a_0 = 1 p(-1)=-a_3+a_2-a_1+a_0 = 1 a_3+a_2x+a_1+a_0 + (-a_3+a_2-a_1+a_0) \Rightarrow 2a_2+2a_0=2   p'(x)=3a_3x^2+2a_2x+a_1   h'(x)=0   g'(x)=-4e^{-4(x-1)} p'(-1) = h'(-1) p'(1) = g'(1) 3a_3-2a_2+a_1 = 0 3a_3+2a_2+a_1 = -4 6a_3+2a_1=-4  p''(x)=6a_3x+2a_2  h''(x)=0   g''(x)=16e^{-4(x-1)} p''(-1) = h''(-1) p''(1) = g''(1)  -6a_3+2a_2=0 6a_3+2a_2=16  \Rightarrow 4a_2=16\Rightarrow a_2=4 p'''(-1) = h'''(-1) p'''(1) = g'''(1)  6a_3=0 6a_3=-64  \Rightarrow 12a_3=-64\Rightarrow a_3=-\frac{16}{3} a_0 2a_2+2a_0=2 2(4)+2a_0=2\Rightarrow a_0=-3 a_1 6a_3+2a_1=-4 6(-\frac{16}{3})+2a_1=-4\Rightarrow a_1=14 a_3 = -\frac{16}{3}\\a_2 =4\\a_1 =14\\a_0 =-3","['derivatives', 'polynomials', 'continuity']"
19,Derivative of $(A+BC^{-T}B^T)^{-1}BC^{-1}$,Derivative of,(A+BC^{-T}B^T)^{-1}BC^{-1},"Suppose $A:\mathbb{R}\rightarrow\mathbb{R}^{m\times m}$ , $B:\mathbb{R}\rightarrow\mathbb{R}^{m\times n}$ , and $C:\mathbb{R}\rightarrow\mathbb{R}^{n\times n}$ . I'm trying to find the derivative with respect to a scalar $x$ of \begin{equation}(A+BC^{-T}B^T)^{-1}BC^{-1}\end{equation} My matrix calculus is very rusty. I guess we can set \begin{equation}\phi=A+BC^{-T}B^T\end{equation} so that we have \begin{align} \frac{\partial(\phi^{-1} B C^{-1})}{\partial x}&=\frac{\partial\phi^{-1}}{\partial x}B C^{-1} + \phi^{-1}\frac{\partial B}{\partial x}C^{-1} + \phi^{-1}B\frac{\partial C^{-1}}{\partial x}\\ &=\frac{\partial\phi^{-1}}{\partial x}B C^{-1} + \phi^{-1}\frac{\partial B}{\partial x}C^{-1} - \phi^{-1}BC^{-1}\frac{\partial C}{\partial x}C^{-1}. \end{align} Now, we only need to find $\frac{\partial\phi^{-1}}{\partial x}$ . That is, \begin{equation}\frac{\partial}{\partial x}\left(A+BC^{-T}B^T\right)^{-1}\end{equation} Is the following correct? \begin{equation}\frac{\partial}{\partial x}\left(A+BC^{-T}B^T\right)^{-1}=-\left(A+BC^{-T}B^T\right)^{-1}\left[\frac{\partial}{\partial x}\left(A+BC^{-T}B^T\right)\right] \left(A+BC^{-T}B^T\right)^{-1}\end{equation} where \begin{align}\frac{\partial}{\partial x}\left(A+BC^{-T}B^T\right)&=\frac{\partial A}{\partial x}+\frac{\partial B}{\partial x}C^{-T}B^T+B\frac{\partial C^{-T}}{\partial x}B^T+BC^{-T}\frac{\partial B^T}{\partial x}\\ &=\frac{\partial A}{\partial x}+\frac{\partial B}{\partial x}C^{-T}B^T-BC^{-T}\left(\frac{\partial C}{\partial x}\right)^{T}C^{-T}B^T+BC^{-T}\left(\frac{\partial B}{\partial x}\right)^T\end{align}","Suppose , , and . I'm trying to find the derivative with respect to a scalar of My matrix calculus is very rusty. I guess we can set so that we have Now, we only need to find . That is, Is the following correct? where","A:\mathbb{R}\rightarrow\mathbb{R}^{m\times m} B:\mathbb{R}\rightarrow\mathbb{R}^{m\times n} C:\mathbb{R}\rightarrow\mathbb{R}^{n\times n} x \begin{equation}(A+BC^{-T}B^T)^{-1}BC^{-1}\end{equation} \begin{equation}\phi=A+BC^{-T}B^T\end{equation} \begin{align}
\frac{\partial(\phi^{-1} B C^{-1})}{\partial x}&=\frac{\partial\phi^{-1}}{\partial x}B C^{-1} + \phi^{-1}\frac{\partial B}{\partial x}C^{-1} + \phi^{-1}B\frac{\partial C^{-1}}{\partial x}\\
&=\frac{\partial\phi^{-1}}{\partial x}B C^{-1} + \phi^{-1}\frac{\partial B}{\partial x}C^{-1} - \phi^{-1}BC^{-1}\frac{\partial C}{\partial x}C^{-1}.
\end{align} \frac{\partial\phi^{-1}}{\partial x} \begin{equation}\frac{\partial}{\partial x}\left(A+BC^{-T}B^T\right)^{-1}\end{equation} \begin{equation}\frac{\partial}{\partial x}\left(A+BC^{-T}B^T\right)^{-1}=-\left(A+BC^{-T}B^T\right)^{-1}\left[\frac{\partial}{\partial x}\left(A+BC^{-T}B^T\right)\right] \left(A+BC^{-T}B^T\right)^{-1}\end{equation} \begin{align}\frac{\partial}{\partial x}\left(A+BC^{-T}B^T\right)&=\frac{\partial A}{\partial x}+\frac{\partial B}{\partial x}C^{-T}B^T+B\frac{\partial C^{-T}}{\partial x}B^T+BC^{-T}\frac{\partial B^T}{\partial x}\\
&=\frac{\partial A}{\partial x}+\frac{\partial B}{\partial x}C^{-T}B^T-BC^{-T}\left(\frac{\partial C}{\partial x}\right)^{T}C^{-T}B^T+BC^{-T}\left(\frac{\partial B}{\partial x}\right)^T\end{align}","['matrices', 'derivatives', 'matrix-calculus']"
20,Envelope of a real function consisting of a complex function and its conjugate,Envelope of a real function consisting of a complex function and its conjugate,,"For a real function $f(x)=A(x)e^{ix}+\overline{A}(x)e^{-ix}$ , where $\overline{A}$ represents complex conjugate of $A$ . Note that $A(x)$ itself is a complex function $A(x)=A_r(x)+i A_i(x)$ . It seems that the envelope of $f(x)$ should be $\pm 2\vert A \vert$ , where $\vert \cdot \vert$ repersents the modulus of a complex number. That is, the envelope of $f(x)$ is double amplitude of $A$ if $f$ can be written as $A(x)e^{ix}+c.c.$ Now, I am confusing with the envelope of the real function $g=ff_x$ since it cannot be written in a similar form. Substituting $f(x)$ into $g=ff_x$ , one obtains $g=A(A_x+iA)e^{2ix}+A\overline{A}_x+c.c.$ , where c.c. repersents the complex conjugate of the preceding terms. How can I express the amplitude of the envelope of $g$ with $A$ , $\overline{A}$ and their derivatives? For the 1st term and its conjugate, the envelope should be $\pm 2 \vert A(A_x+iA)\vert$ . But I do not know how to include the 2nd term and its c.c. Can anyone please give me some suggestions? Thank you in advance!","For a real function , where represents complex conjugate of . Note that itself is a complex function . It seems that the envelope of should be , where repersents the modulus of a complex number. That is, the envelope of is double amplitude of if can be written as Now, I am confusing with the envelope of the real function since it cannot be written in a similar form. Substituting into , one obtains , where c.c. repersents the complex conjugate of the preceding terms. How can I express the amplitude of the envelope of with , and their derivatives? For the 1st term and its conjugate, the envelope should be . But I do not know how to include the 2nd term and its c.c. Can anyone please give me some suggestions? Thank you in advance!",f(x)=A(x)e^{ix}+\overline{A}(x)e^{-ix} \overline{A} A A(x) A(x)=A_r(x)+i A_i(x) f(x) \pm 2\vert A \vert \vert \cdot \vert f(x) A f A(x)e^{ix}+c.c. g=ff_x f(x) g=ff_x g=A(A_x+iA)e^{2ix}+A\overline{A}_x+c.c. g A \overline{A} \pm 2 \vert A(A_x+iA)\vert,"['derivatives', 'complex-numbers', 'several-complex-variables', 'envelope']"
21,Any ideas on how to solve this integral $\sin ax / (x^2+b^2)^2$?,Any ideas on how to solve this integral ?,\sin ax / (x^2+b^2)^2,"I've been trying to find the value of the following integral using the residue theorem : $$\int_0^\infty \frac{\sin ax\ dx}{(x^2+b^2)^2}$$ So basically the idea that I had is to transform this integral and to make it in the numerator as $e^{iaz} = \cos az\ + i\sin az\ $ and thus $$\mathrm{Im}\, \int_0^\infty \frac{e^{iaz}dz}{(z^2+b^2)^2},  = \int_0^\infty \frac{\sin az\ dz}{(z^2+b^2)^2}$$ However in class we've only established the residue formula from $-\infty$ to $+\infty$ , and when I try to integrate it that way (as in from $-\infty$ to $+\infty$ ) I obtain $0$ obviously because the function is odd. I'd like to add that I worked on a similair integral $\int_0^\infty \frac{\cos ax\ dx}{(x^2+b^2)^2}$ , using the poles $+ib$ and $-ib$ , hence $(x^2+b^2)^2$ $=$ $(x+ib)^2(x-ib)^2$ and then calculating the $Res( \frac{\cos az\ dz}{(z^2+b^2)^2}, +ib)$ by using this : $$\mathrm{Re}\, \int_0^\infty \frac{e^{iaz}dz}{(z^2+b^2)^2},  = \int_0^\infty \frac{\cos az\ dz}{(z^2+b^2)^2}$$ I'm trying to do a similar thing with the sin function. calculating the cos integral was way easier due to the fact that the integrated is even. Any help is welcome to direct me in the right path. Also apologies since english isn't my first language!","I've been trying to find the value of the following integral using the residue theorem : So basically the idea that I had is to transform this integral and to make it in the numerator as and thus However in class we've only established the residue formula from to , and when I try to integrate it that way (as in from to ) I obtain obviously because the function is odd. I'd like to add that I worked on a similair integral , using the poles and , hence and then calculating the by using this : I'm trying to do a similar thing with the sin function. calculating the cos integral was way easier due to the fact that the integrated is even. Any help is welcome to direct me in the right path. Also apologies since english isn't my first language!","\int_0^\infty \frac{\sin ax\ dx}{(x^2+b^2)^2} e^{iaz} = \cos az\ + i\sin az\  \mathrm{Im}\, \int_0^\infty \frac{e^{iaz}dz}{(z^2+b^2)^2},
 = \int_0^\infty \frac{\sin az\ dz}{(z^2+b^2)^2} -\infty +\infty -\infty +\infty 0 \int_0^\infty \frac{\cos ax\ dx}{(x^2+b^2)^2} +ib -ib (x^2+b^2)^2 = (x+ib)^2(x-ib)^2 Res( \frac{\cos az\ dz}{(z^2+b^2)^2}, +ib) \mathrm{Re}\, \int_0^\infty \frac{e^{iaz}dz}{(z^2+b^2)^2},
 = \int_0^\infty \frac{\cos az\ dz}{(z^2+b^2)^2}","['integration', 'complex-analysis', 'derivatives', 'complex-integration', 'residue-calculus']"
22,Use integration by parts in cylindrical coordinates to turn these second derivatives into first derivatives,Use integration by parts in cylindrical coordinates to turn these second derivatives into first derivatives,,"I have a complex function $\psi(\rho,z)$ in cylindrical coordinates, which has the conditions $\psi=0$ as $z\rightarrow\pm\infty$ and $\rho\rightarrow +\infty$ . The quantity $E$ given by the double integral $$ E = 2\pi\int_{z=-\infty}^{z=+\infty}\int_{\rho=0}^{\rho=+\infty} \Bigg( \psi^*\frac{\partial^2\psi}{\partial\rho^2} + \psi^*\frac{1}{\rho}\frac{\partial\psi}{\partial\rho} + \psi^*\frac{\partial^2\psi}{\partial z^2} \Bigg) \rho \;\textrm{d}\rho\, \textrm{d}z \tag{1} $$ where $\psi^*$ denotes the complex conjugate of $\psi$ , and the $2\pi$ appears because I have integrated out the angular coordinate, since $\psi$ is cylindrically-symmetric. (The quantity $E$ expresses the kinetic energy of a quantum mechanical wavefunction). I would like to simplify this expression so that it contains first derivatives instead of second derivatives. I know that in one-dimension in a variable $x$ , I can use integration by parts to write the following $$ \int_{x=-\infty}^{x=+\infty} \psi^* \frac{\partial^2\psi}{\partial x^2} \,\textrm{d}x = \Bigg[ \psi^* \frac{\partial\psi}{\partial x} \Bigg]^{+\infty}_{-\infty} - \int_{x=-\infty}^{x=+\infty} \frac{\partial\psi^*}{\partial x} \frac{\partial\psi}{\partial x} \,\textrm{d}x $$ and if $\psi=0$ at $\pm\infty$ the first term vanishes to give $$ \begin{align} \int_{x=-\infty}^{x=+\infty} \psi^* \frac{\partial^2\psi}{\partial x^2} \,\textrm{d}x &= - \int_{x=-\infty}^{x=+\infty} \frac{\partial\psi^*}{\partial x} \frac{\partial\psi}{\partial x} \,\textrm{d}x\\ &= - \int_{x=-\infty}^{x=+\infty} \bigg(\frac{\partial\psi^*}{\partial x}\bigg)^* \bigg(\frac{\partial\psi}{\partial x}\bigg) \,\textrm{d}x \\ &= - \int_{x=-\infty}^{x=+\infty} \Bigg|\frac{\partial\psi}{\partial x}\Bigg|^2\,\textrm{d}x . \end{align} $$ Is it possible to use a similar trick to write Eq. (1) in terms of first derivatives? I am struggling with the order of integration and how to apply the integration by parts in 2D here, given that there is an extra factor of $\rho$ from the volume element in cylindrical coordinates. If someone could take me through it that would be great, thank you!","I have a complex function in cylindrical coordinates, which has the conditions as and . The quantity given by the double integral where denotes the complex conjugate of , and the appears because I have integrated out the angular coordinate, since is cylindrically-symmetric. (The quantity expresses the kinetic energy of a quantum mechanical wavefunction). I would like to simplify this expression so that it contains first derivatives instead of second derivatives. I know that in one-dimension in a variable , I can use integration by parts to write the following and if at the first term vanishes to give Is it possible to use a similar trick to write Eq. (1) in terms of first derivatives? I am struggling with the order of integration and how to apply the integration by parts in 2D here, given that there is an extra factor of from the volume element in cylindrical coordinates. If someone could take me through it that would be great, thank you!","\psi(\rho,z) \psi=0 z\rightarrow\pm\infty \rho\rightarrow +\infty E 
E = 2\pi\int_{z=-\infty}^{z=+\infty}\int_{\rho=0}^{\rho=+\infty} \Bigg( \psi^*\frac{\partial^2\psi}{\partial\rho^2} + \psi^*\frac{1}{\rho}\frac{\partial\psi}{\partial\rho} + \psi^*\frac{\partial^2\psi}{\partial z^2} \Bigg) \rho \;\textrm{d}\rho\, \textrm{d}z \tag{1}
 \psi^* \psi 2\pi \psi E x 
\int_{x=-\infty}^{x=+\infty} \psi^* \frac{\partial^2\psi}{\partial x^2} \,\textrm{d}x = \Bigg[ \psi^* \frac{\partial\psi}{\partial x} \Bigg]^{+\infty}_{-\infty} - \int_{x=-\infty}^{x=+\infty} \frac{\partial\psi^*}{\partial x} \frac{\partial\psi}{\partial x} \,\textrm{d}x
 \psi=0 \pm\infty 
\begin{align}
\int_{x=-\infty}^{x=+\infty} \psi^* \frac{\partial^2\psi}{\partial x^2} \,\textrm{d}x &= - \int_{x=-\infty}^{x=+\infty} \frac{\partial\psi^*}{\partial x} \frac{\partial\psi}{\partial x} \,\textrm{d}x\\
&= - \int_{x=-\infty}^{x=+\infty} \bigg(\frac{\partial\psi^*}{\partial x}\bigg)^* \bigg(\frac{\partial\psi}{\partial x}\bigg) \,\textrm{d}x \\
&= - \int_{x=-\infty}^{x=+\infty} \Bigg|\frac{\partial\psi}{\partial x}\Bigg|^2\,\textrm{d}x .
\end{align}
 \rho","['calculus', 'integration', 'derivatives', 'quantum-mechanics', 'cylindrical-coordinates']"
23,Derivation with respect to a measure,Derivation with respect to a measure,,"I thought about this recently: We first learn the ""usual"" integration, than we are taught that this is a special case, you are just integrating with respect to a particular measure: the Lebesgue measure, and that you can actually integrate with respect to other measures. So why don't we think about derivates the same way? You might differentiate with respect to a measure the following way: if $\mu$ is a measure that is non zero on any non empty invterval, $\partial_\mu f(x) := \lim_{y\to x} \frac{f(y)-f(x)}{\mu([x, x+y])}$ , when this exists. I think that given that definition I have proven an analog of the fundamental theorem of calculus: if f is a "" $\mu$ -differentiable"" function on $[a,b]$ , and $\partial_\mu f$ is bounded on $[a,b]$ , then $\int_{a}^{b} (\partial_\mu f) d\mu = f(b)-f(a)$ , where the integral is understood as the usual Lebesgue integral with respect to $\mu$ . I have a few questions: -Does this make any sense to you, or is it grossly wrong? -Does any of this exist, does it have any utility, or is it a special case of something I am not aware of? -Is the analog of the fundamental theorem wrong (which means I made a mistake) or could it be right? My proof is quite long but I could try to write it using Latex. Thanks","I thought about this recently: We first learn the ""usual"" integration, than we are taught that this is a special case, you are just integrating with respect to a particular measure: the Lebesgue measure, and that you can actually integrate with respect to other measures. So why don't we think about derivates the same way? You might differentiate with respect to a measure the following way: if is a measure that is non zero on any non empty invterval, , when this exists. I think that given that definition I have proven an analog of the fundamental theorem of calculus: if f is a "" -differentiable"" function on , and is bounded on , then , where the integral is understood as the usual Lebesgue integral with respect to . I have a few questions: -Does this make any sense to you, or is it grossly wrong? -Does any of this exist, does it have any utility, or is it a special case of something I am not aware of? -Is the analog of the fundamental theorem wrong (which means I made a mistake) or could it be right? My proof is quite long but I could try to write it using Latex. Thanks","\mu \partial_\mu f(x) := \lim_{y\to x} \frac{f(y)-f(x)}{\mu([x, x+y])} \mu [a,b] \partial_\mu f [a,b] \int_{a}^{b} (\partial_\mu f) d\mu = f(b)-f(a) \mu","['calculus', 'integration', 'derivatives']"
24,Canceling $r$ when finding the slope of a polar curve,Canceling  when finding the slope of a polar curve,r,Original problem: find points where the tangent line is vertical or horizontal for the curve $ r^2 = \sin(2\theta)$ . My work so far: $$ r^2 = \sin(2\theta) \\ 2r\frac{dr}{d\theta} = 2\cos(2\theta) \\ \frac{dr}{d\theta} = \frac{\cos(2\theta)}{r}$$ By the formula for the derivative of $y$ with respect to $x$ : $$ \frac{dy}{dx} = \frac{r\cos(\theta) + \frac{dr}{d\theta} \sin(\theta)}{-r\sin(\theta) +\frac{dr}{d\theta} \cos(\theta)}$$ $$ = \frac{ r\cos(\theta) + \frac 1 r \cos(2\theta)\sin(\theta)} {-r\sin(\theta) + \frac 1 r \cos(2\theta)\cos(\theta)}$$ $$ \frac {dy}{dx} = \frac {\dfrac{r^2 \cos(\theta) + \cos(2\theta)\sin(\theta)}{r}} {\dfrac{-r^2 \sin(\theta) + \cos(2\theta)\cos(\theta)} {r}}$$ Can I cancel the $r$ without losing any points with horizontal or vertical tangents? How do I know if this is an acceptable operation (meaning that the operation does not alter the equation)? Any help is appreciated. Thank you,Original problem: find points where the tangent line is vertical or horizontal for the curve . My work so far: By the formula for the derivative of with respect to : Can I cancel the without losing any points with horizontal or vertical tangents? How do I know if this is an acceptable operation (meaning that the operation does not alter the equation)? Any help is appreciated. Thank you, r^2 = \sin(2\theta)  r^2 = \sin(2\theta) \\ 2r\frac{dr}{d\theta} = 2\cos(2\theta) \\ \frac{dr}{d\theta} = \frac{\cos(2\theta)}{r} y x  \frac{dy}{dx} = \frac{r\cos(\theta) + \frac{dr}{d\theta} \sin(\theta)}{-r\sin(\theta) +\frac{dr}{d\theta} \cos(\theta)}  = \frac{ r\cos(\theta) + \frac 1 r \cos(2\theta)\sin(\theta)} {-r\sin(\theta) + \frac 1 r \cos(2\theta)\cos(\theta)}  \frac {dy}{dx} = \frac {\dfrac{r^2 \cos(\theta) + \cos(2\theta)\sin(\theta)}{r}} {\dfrac{-r^2 \sin(\theta) + \cos(2\theta)\cos(\theta)} {r}} r,"['derivatives', 'polar-coordinates']"
25,Rate of Change Calculus Application question,Rate of Change Calculus Application question,,"My teacher gave us this unit assignment and its today. I'm so confused. What we learned this unit was everything about Rate of Change, limits, and derivatives. I'm panicking. If anyone can help with any semblance of a solution, I will be so thankful questions: I need to catch my flight. I’m at the airport and I’m running late. I need to make it $1000m$ to get to my gate. I have two choices. I can either run or take the moving sidewalk. The moving sidewalk moves at a constant speed of $2 \frac{m}{s}$ . If I’m running, I start out quick, but because I’m old I quickly tire and slow down. Equations for these situations are: $$M(t)=2t$$ $$R(t)=\sqrt{2000x}$$ I cannot run on the moving sidewalk because it’s too crowded, but I can to go onto or off of the moving sidewalk whenever I want to. Assume that I never recover my energy, so I can only have one burst of running. How can I get to my gate in less than $500$ s? Explain the process and determine the minimum time it takes me to reach the gate. If the moving sidewalk moved at $2.5$ m/s, what is the best strategy and how quickly can I reach the gate? If the moving sidewalk is moving at $2$ m/s, but running speed is modeled by $R(t) = 153(x2)^{\frac{1}{3}}$ , what is the best strategy and how long will it take me to reach the gate? For 1. I know that $M(t)$ is the distance traveled after $t$ seconds on the moving sidewalk. $R(t)$ is the distance traveled after t seconds running. I'm going to spend some time running and some time on the sidewalk, so I need to find possible values of $t_{0}$ and $t_{1}$ such that $M(t_{0})+R(t_{1})=1000$ and $t_{0}+t_{1}<500$ . Then I am trying to minimize $t_{0}+t_{1}$ (the time it takes to get to your gate). if you just run the whole way, or just take the sidewalk the whole way, you get there in $500$ seconds, so the problem is asking you to do better than that. I'm just not sure how to translate this information into an equation. Once I can do #1 the #2, and#3 will be easy. if you just run the whole way, or just take the sidewalk the whole way, you get there in $500$ seconds, so the problem is asking you to do better than that.","My teacher gave us this unit assignment and its today. I'm so confused. What we learned this unit was everything about Rate of Change, limits, and derivatives. I'm panicking. If anyone can help with any semblance of a solution, I will be so thankful questions: I need to catch my flight. I’m at the airport and I’m running late. I need to make it to get to my gate. I have two choices. I can either run or take the moving sidewalk. The moving sidewalk moves at a constant speed of . If I’m running, I start out quick, but because I’m old I quickly tire and slow down. Equations for these situations are: I cannot run on the moving sidewalk because it’s too crowded, but I can to go onto or off of the moving sidewalk whenever I want to. Assume that I never recover my energy, so I can only have one burst of running. How can I get to my gate in less than s? Explain the process and determine the minimum time it takes me to reach the gate. If the moving sidewalk moved at m/s, what is the best strategy and how quickly can I reach the gate? If the moving sidewalk is moving at m/s, but running speed is modeled by , what is the best strategy and how long will it take me to reach the gate? For 1. I know that is the distance traveled after seconds on the moving sidewalk. is the distance traveled after t seconds running. I'm going to spend some time running and some time on the sidewalk, so I need to find possible values of and such that and . Then I am trying to minimize (the time it takes to get to your gate). if you just run the whole way, or just take the sidewalk the whole way, you get there in seconds, so the problem is asking you to do better than that. I'm just not sure how to translate this information into an equation. Once I can do #1 the #2, and#3 will be easy. if you just run the whole way, or just take the sidewalk the whole way, you get there in seconds, so the problem is asking you to do better than that.",1000m 2 \frac{m}{s} M(t)=2t R(t)=\sqrt{2000x} 500 2.5 2 R(t) = 153(x2)^{\frac{1}{3}} M(t) t R(t) t_{0} t_{1} M(t_{0})+R(t_{1})=1000 t_{0}+t_{1}<500 t_{0}+t_{1} 500 500,"['calculus', 'derivatives', 'contest-math']"
26,"According to the ""DI Method"" (Tabular Method) - isn't every integral a sum of the derivatives?","According to the ""DI Method"" (Tabular Method) - isn't every integral a sum of the derivatives?",,"Many years ago I encountered this really amazing solving technique called the ""DI Method for integration"" which is also called the tabular method (If I am not mistaken). It lets us choose two functions $g(x)$ and $h(x)$ and construct this table, for one which we take the derivative over and over (until we hit $0$ or until we hit the same integral we started with) and the second function we integrate (Which will be much easier as it is just one part of the whole function we want to compute the integral of) And then take the product of the diagonals, with an alternating sign. It lets us solve integrals such as $$ \int xe^x  ~ dx,  ~~\int \sin (x) e^x ~ dx$$ Pretty quickly (This is just a variation of the Integration By Parts condensed in a fine table) So I thought about this method, and wondered what would happend if we have this amorphous integral: $$ \int f(x) ~ dx$$ And choose $g(x) = f(x)$ and $h(x) = 1$ . This would give of this table: $$\begin{array}{|c|c|c|}\hline  \text{Sign}& D & I \\ \hline  \color{red}{+} & \color{red}{f(x)} & 1  \\ \hline  \color{green}{-} & \color{green}{f'(x)} & \color{red}{x} \\ \hline  \color{purple}{+} & \color{purple}{f''(x)} & \color{green}{\frac{x^2}{2}} \\ \hline  \color{blue}{-} & \color{blue}{f^{(3)}(x)} & \color{purple}{\frac{x^3}{6}} \\ \hline  \vdots & \vdots & \color{blue}{\vdots} \\ \hline  \end{array}$$ So we basically have a nice pattern, we can describe the integral as an infinite sum of derivatives as so: $$ \int f(x) ~ dx = \sum_{k=0}^{\infty} f^{(k)}(x) \cdot (-1)^k \cdot \frac{x^{k+1}}{(k+1)!}$$ Where $f^{(k)}(x)$ is the $k$ -th derivative ( $f^{(0)}(x) = f(x)$ ). This seems like a Taylor Series of a function, but comparing for example this specific infinite sum for $\frac{e^x}{x}$ we get a totally different sum then its Taylor expansion. I am not going to lie, the Taylor expansion seems to give a better approximation, but they are not the same as far as I checked: $$ \int \frac{e^x}{x} ~ dx = \text{Ei}(x) \approx  e^x \left ( \frac{1}{x} + \frac{1}{x^2} + \frac{2}{x^3} + \frac{6}{x^4} + \mathcal{O} \left ( \frac{1}{x^5} \right ) \right ) ~~~ \text{Taylor} $$ Our sum, is something else, starting like: $$ e^x \left ( 1 - \frac{x-1}{2} + \frac{x^2 -2x+2}{6} + \dots \right ) ~~~ \text{Our sum}$$ Which is much different. In addition, why can't we break our sum into two different sums, one for the derivatives and one for the rest, which would give us that every integral of a function is the infinite sum of the derivatives multiplied by this sum, which is evaluated to $1-e^{-x}$ : $$ \int f(x) ~ dx = \sum_{k=0}^{\infty} f^{(k)}(x) \cdot \sum_{k=0}^{\infty} (-1)^{k} \frac{x^{k+1}}{(k+1)!} = (1-e^{-x}) \cdot \sum_{k=0}^{\infty} f^{(k)}(x)$$ Why is it that different? Have I made a mistake somewhere in this progress? Or my way of thinking is completely off? I would like to hear what you think about it. Thank you!","Many years ago I encountered this really amazing solving technique called the ""DI Method for integration"" which is also called the tabular method (If I am not mistaken). It lets us choose two functions and and construct this table, for one which we take the derivative over and over (until we hit or until we hit the same integral we started with) and the second function we integrate (Which will be much easier as it is just one part of the whole function we want to compute the integral of) And then take the product of the diagonals, with an alternating sign. It lets us solve integrals such as Pretty quickly (This is just a variation of the Integration By Parts condensed in a fine table) So I thought about this method, and wondered what would happend if we have this amorphous integral: And choose and . This would give of this table: So we basically have a nice pattern, we can describe the integral as an infinite sum of derivatives as so: Where is the -th derivative ( ). This seems like a Taylor Series of a function, but comparing for example this specific infinite sum for we get a totally different sum then its Taylor expansion. I am not going to lie, the Taylor expansion seems to give a better approximation, but they are not the same as far as I checked: Our sum, is something else, starting like: Which is much different. In addition, why can't we break our sum into two different sums, one for the derivatives and one for the rest, which would give us that every integral of a function is the infinite sum of the derivatives multiplied by this sum, which is evaluated to : Why is it that different? Have I made a mistake somewhere in this progress? Or my way of thinking is completely off? I would like to hear what you think about it. Thank you!","g(x) h(x) 0  \int xe^x  ~ dx,  ~~\int \sin (x) e^x ~ dx  \int f(x) ~ dx g(x) = f(x) h(x) = 1 \begin{array}{|c|c|c|}\hline 
\text{Sign}& D & I \\ \hline 
\color{red}{+} & \color{red}{f(x)} & 1  \\ \hline 
\color{green}{-} & \color{green}{f'(x)} & \color{red}{x} \\ \hline 
\color{purple}{+} & \color{purple}{f''(x)} & \color{green}{\frac{x^2}{2}} \\ \hline 
\color{blue}{-} & \color{blue}{f^{(3)}(x)} & \color{purple}{\frac{x^3}{6}} \\ \hline 
\vdots & \vdots & \color{blue}{\vdots} \\ \hline 
\end{array}  \int f(x) ~ dx = \sum_{k=0}^{\infty} f^{(k)}(x) \cdot (-1)^k \cdot \frac{x^{k+1}}{(k+1)!} f^{(k)}(x) k f^{(0)}(x) = f(x) \frac{e^x}{x}  \int \frac{e^x}{x} ~ dx = \text{Ei}(x) \approx  e^x \left ( \frac{1}{x} + \frac{1}{x^2} + \frac{2}{x^3} + \frac{6}{x^4} + \mathcal{O} \left ( \frac{1}{x^5} \right ) \right ) ~~~ \text{Taylor}   e^x \left ( 1 - \frac{x-1}{2} + \frac{x^2 -2x+2}{6} + \dots \right ) ~~~ \text{Our sum} 1-e^{-x}  \int f(x) ~ dx = \sum_{k=0}^{\infty} f^{(k)}(x) \cdot \sum_{k=0}^{\infty} (-1)^{k} \frac{x^{k+1}}{(k+1)!} = (1-e^{-x}) \cdot \sum_{k=0}^{\infty} f^{(k)}(x)","['calculus', 'integration', 'derivatives', 'summation', 'taylor-expansion']"
27,"Differentiation under the integral sign with $h(x,t) = e^{xt}f(t)$",Differentiation under the integral sign with,"h(x,t) = e^{xt}f(t)","I'm working on the following problem and got stuck: Let $f:\mathbb{R}\rightarrow\mathbb{R}$ such that for every fixed $x\in(-1,1)$ the function $g_x(t):= e^{tx}f(t) \in L^1(\mathbb{R})$ . Let $\varphi:(-1,1)\rightarrow \mathbb{R}$ be defined as $$\varphi(x) = \int{e^{tx}f(t)}dt$$ Show that $\varphi$ is differentiable. My attempt: Let $h(x,t):(-1,1)\times\mathbb{R} \longrightarrow \mathbb{R}$ $$h(x,t) = e^{tx}f(t)$$ Then, in order to differentiate under the integral sign, we need: $\bullet\space h(.,t)=e^{tx}f(t)\in L^1(\mathbb{R})$ (which we know by definition) $\bullet\space h(x,.)\in C(\mathbb{R}) \space\space\space[\frac{d}{dx}\left(e^{xt}f(t)\right)=t\cdot e^{xt}f(t)]$ and $\bullet\space \lvert\frac{d}{dx} h(x,t)\rvert=\lvert t\cdot e^{xt}f(t)\rvert < s(t)\space$ for some $s(t)\in L^1(\mathbb{R})$ This last step is where I'm stuck. I can't find such $s(t)$ that dominated my function. I think that if I'm able to do so, then I'd be done. Or is my approach incorrect? Thanks.","I'm working on the following problem and got stuck: Let such that for every fixed the function . Let be defined as Show that is differentiable. My attempt: Let Then, in order to differentiate under the integral sign, we need: (which we know by definition) and for some This last step is where I'm stuck. I can't find such that dominated my function. I think that if I'm able to do so, then I'd be done. Or is my approach incorrect? Thanks.","f:\mathbb{R}\rightarrow\mathbb{R} x\in(-1,1) g_x(t):= e^{tx}f(t) \in L^1(\mathbb{R}) \varphi:(-1,1)\rightarrow \mathbb{R} \varphi(x) = \int{e^{tx}f(t)}dt \varphi h(x,t):(-1,1)\times\mathbb{R} \longrightarrow \mathbb{R} h(x,t) = e^{tx}f(t) \bullet\space h(.,t)=e^{tx}f(t)\in L^1(\mathbb{R}) \bullet\space h(x,.)\in C(\mathbb{R}) \space\space\space[\frac{d}{dx}\left(e^{xt}f(t)\right)=t\cdot e^{xt}f(t)] \bullet\space \lvert\frac{d}{dx} h(x,t)\rvert=\lvert t\cdot e^{xt}f(t)\rvert < s(t)\space s(t)\in L^1(\mathbb{R}) s(t)","['integration', 'derivatives', 'lebesgue-integral']"
28,If a function is continuous and differentiable everywhere is the derivative continuous?,If a function is continuous and differentiable everywhere is the derivative continuous?,,"Suppose $f$ is continuous on $[a,b]$ and differentiable on (a,b). Does it follow that $f'$ is continuous on $(a,b)$?","Suppose $f$ is continuous on $[a,b]$ and differentiable on (a,b). Does it follow that $f'$ is continuous on $(a,b)$?",,"['real-analysis', 'continuity']"
29,adjoint matrix of an operator,adjoint matrix of an operator,,"considering a $2\times 2$ matrix $\bf S$ , \begin{equation}    {\bf S} =  \begin{bmatrix} \frac{\partial}{\partial{t}} &  \kappa\nabla .\\ 1/\rho \nabla . &  \frac{\partial}{\partial{t}}     \end{bmatrix}    \end{equation} I want to get the adjoint of $\bf S$ , $$\bf S^{*}$$ for that I am using cofactor matrix and then I transpose this matrix (METHOD 1 to get adjoint matrix). \begin{equation}    {\bf S}_{\text{cof}} =  \begin{bmatrix} a_{11} &  a_{12} \\ a_{21} &  a_{22}     \end{bmatrix}    \end{equation} as $$a_{ij}=(-1)^{i+j}\det\bf S_{ij}$$ \begin{eqnarray} a_{11}&=&\frac{\partial}{\partial{t}}\\ a_{12}&=&-1/\rho \nabla .\\ a_{21}&=&-\kappa\nabla . \\ a_{22}&=&\frac{\partial}{\partial{t}} \end{eqnarray} In matrix notation we have, \begin{equation}    {\bf S}_{\text{cof}} =  \begin{bmatrix} \frac{\partial}{\partial{t}} &  -1/\rho \nabla \\ -\kappa\nabla . &  \frac{\partial}{\partial{t}}     \end{bmatrix}    \end{equation} Transposing the matrix $$\bf S_{\text{cof}}$$ we get: \begin{equation}  {\bf S}^{*}=  \begin{bmatrix} \frac{\partial}{\partial{t}} &  -\kappa\nabla .   \\ -1/\rho \nabla .&  \frac{\partial}{\partial{t}}     \end{bmatrix}    \end{equation} This is the way I am doing but I saw some other papers doing something different, the procedure other people are doing is transposing $\bf S$ and change the signal of every first derivative (METHOD 2 to get adjoint matrix), in this sense the adjoint of $\bf S$ is: \begin{equation}    {\bf S}^{*} =  \begin{bmatrix} - \frac{\partial}{\partial{t}} &  -1/\rho \nabla \\ -\kappa\nabla . & - \frac{\partial}{\partial{t}}     \end{bmatrix}    \end{equation} Which method is right? Am I doing something wrong? If you have any documentation please share it. Thank you, Rafael","considering a matrix , I want to get the adjoint of , for that I am using cofactor matrix and then I transpose this matrix (METHOD 1 to get adjoint matrix). as In matrix notation we have, Transposing the matrix we get: This is the way I am doing but I saw some other papers doing something different, the procedure other people are doing is transposing and change the signal of every first derivative (METHOD 2 to get adjoint matrix), in this sense the adjoint of is: Which method is right? Am I doing something wrong? If you have any documentation please share it. Thank you, Rafael","2\times 2 \bf S \begin{equation}
   {\bf S} =  \begin{bmatrix} \frac{\partial}{\partial{t}} &  \kappa\nabla .\\ 1/\rho \nabla . &  \frac{\partial}{\partial{t}}  
  \end{bmatrix} 
  \end{equation} \bf S \bf S^{*} \begin{equation}
   {\bf S}_{\text{cof}} =  \begin{bmatrix} a_{11} &  a_{12} \\ a_{21} &  a_{22}  
  \end{bmatrix} 
  \end{equation} a_{ij}=(-1)^{i+j}\det\bf S_{ij} \begin{eqnarray}
a_{11}&=&\frac{\partial}{\partial{t}}\\
a_{12}&=&-1/\rho \nabla .\\
a_{21}&=&-\kappa\nabla . \\
a_{22}&=&\frac{\partial}{\partial{t}}
\end{eqnarray} \begin{equation}
   {\bf S}_{\text{cof}} =  \begin{bmatrix} \frac{\partial}{\partial{t}} &  -1/\rho \nabla \\ -\kappa\nabla . &  \frac{\partial}{\partial{t}}  
  \end{bmatrix} 
  \end{equation} \bf S_{\text{cof}} \begin{equation}
 {\bf S}^{*}=  \begin{bmatrix} \frac{\partial}{\partial{t}} &  -\kappa\nabla .   \\ -1/\rho \nabla .&  \frac{\partial}{\partial{t}}  
  \end{bmatrix} 
  \end{equation} \bf S \bf S \begin{equation}
   {\bf S}^{*} =  \begin{bmatrix} - \frac{\partial}{\partial{t}} &  -1/\rho \nabla \\ -\kappa\nabla . & - \frac{\partial}{\partial{t}}  
  \end{bmatrix} 
  \end{equation}","['matrices', 'derivatives', 'adjoint-operators']"
30,Possible proof of Leibniz Theorem,Possible proof of Leibniz Theorem,,"As a matter of fact, I had been reading integration for a while, and I came with a possible proof of Leibniz rule, which I am little unsure about. Leibniz's Rule says :' If a $f(t)$ is a continuous function on intervals [a,b] and $α(x)$ and $β(x)$ are differnetiable functions of x whose values lie in [a,b] then; $$\frac{d}{dx}\int_{\beta(x)}^{\alpha(x)}f(t)dx=f(\alpha(x))\frac{d\alpha(x)}{dx}-f(β(x))\frac{dβ(x)}{dx}$$ Now proof: (Note: I am denoting $\frac{dg(x)}{dx}$ as $g'(t)$ ) Let indefinite integral of $f(t)$ be $g(t)+c$ $$\int f(t)dt=g(t)+c \rightarrow f(t)=g'(t)$$ Now putting $$t=\alpha(x) \rightarrow f(\alpha(x))=g'(\alpha(x))...(1)$$ $$t=\beta(x)\rightarrow f(\beta(x))=g'(\beta(x))...(2)$$ Now let's put limits on our indefinite integral $$\int_{\beta(x)}^{\alpha(x)}f(t)dt=[g(t)+c]_{\beta(x)}^{\alpha(x)}=g(\alpha(x))-g(\beta(x))$$ Differentiating both sides $w.r.t$ $x$ $$\frac{d}{dx}\int_{\beta(x)}^{\alpha(x)}f(t)dt=g'(\alpha(x))\frac{d(\alpha(x)}{dx}-=g'(\beta(x))\frac{d(\beta(x)}{dx}$$ Hence we get finally $$\frac{d}{dx}\int_{\beta(x)}^{\alpha(x)}f(t)dx=f(\alpha(x))\frac{d\alpha(x)}{dx}-f(β(x))\frac{dβ(x)}{dx}$$ {From equations $(1)$ and $(2)$ I replaced $g'(\alpha(x))$ with $f(\alpha(x))$ and similarly for 2nd term} Is this proof correct $?$ Please help and thanks in advance $!$ Any suggestions are appreciated.","As a matter of fact, I had been reading integration for a while, and I came with a possible proof of Leibniz rule, which I am little unsure about. Leibniz's Rule says :' If a is a continuous function on intervals [a,b] and and are differnetiable functions of x whose values lie in [a,b] then; Now proof: (Note: I am denoting as ) Let indefinite integral of be Now putting Now let's put limits on our indefinite integral Differentiating both sides Hence we get finally {From equations and I replaced with and similarly for 2nd term} Is this proof correct Please help and thanks in advance Any suggestions are appreciated.",f(t) α(x) β(x) \frac{d}{dx}\int_{\beta(x)}^{\alpha(x)}f(t)dx=f(\alpha(x))\frac{d\alpha(x)}{dx}-f(β(x))\frac{dβ(x)}{dx} \frac{dg(x)}{dx} g'(t) f(t) g(t)+c \int f(t)dt=g(t)+c \rightarrow f(t)=g'(t) t=\alpha(x) \rightarrow f(\alpha(x))=g'(\alpha(x))...(1) t=\beta(x)\rightarrow f(\beta(x))=g'(\beta(x))...(2) \int_{\beta(x)}^{\alpha(x)}f(t)dt=[g(t)+c]_{\beta(x)}^{\alpha(x)}=g(\alpha(x))-g(\beta(x)) w.r.t x \frac{d}{dx}\int_{\beta(x)}^{\alpha(x)}f(t)dt=g'(\alpha(x))\frac{d(\alpha(x)}{dx}-=g'(\beta(x))\frac{d(\beta(x)}{dx} \frac{d}{dx}\int_{\beta(x)}^{\alpha(x)}f(t)dx=f(\alpha(x))\frac{d\alpha(x)}{dx}-f(β(x))\frac{dβ(x)}{dx} (1) (2) g'(\alpha(x)) f(\alpha(x)) ? !,"['derivatives', 'definite-integrals']"
31,$f(x)\to A(x\to\infty)$ and $f''(x)+\lambda f'(x)\leq M.$How to prove $f'(x)\to 0(x\to\infty)$,and How to prove,f(x)\to A(x\to\infty) f''(x)+\lambda f'(x)\leq M. f'(x)\to 0(x\to\infty),"$f(x)\in C^{2}(0,\infty)$ , $f(x)\to A(x\to\infty)$ and $f''(x)+\lambda f'(x)\leq M.$ How to prove $f'(x)\to 0(x\to\infty)$ ? It's easy to see that without loss of generality we can assume $f(\infty)=0.$ So if we can prove $f'(x)$ is convergent at $\infty$ ,we can use $f(x+1)-f(x)=f'(t)\to 0$ to show $f'(\infty)=0.$ And I know another useful lemma.If $f(\infty)=A$ ,and $f''(x)$ is bounded then $f'(\infty)=0.$ But I found these lemmas don't work here.So how to solve this problem?Does it have any background?Any help will be thanked.",", and How to prove ? It's easy to see that without loss of generality we can assume So if we can prove is convergent at ,we can use to show And I know another useful lemma.If ,and is bounded then But I found these lemmas don't work here.So how to solve this problem?Does it have any background?Any help will be thanked.","f(x)\in C^{2}(0,\infty) f(x)\to A(x\to\infty) f''(x)+\lambda f'(x)\leq M. f'(x)\to 0(x\to\infty) f(\infty)=0. f'(x) \infty f(x+1)-f(x)=f'(t)\to 0 f'(\infty)=0. f(\infty)=A f''(x) f'(\infty)=0.","['real-analysis', 'derivatives']"
32,"An optimization problem related to parabolas, yields a hard to solve derivative","An optimization problem related to parabolas, yields a hard to solve derivative",,"Hello, I have came up with what I think is a unique optimization problem. We are given the positive real parameters $k,t$ . $t$ is the height of the rectangle, and $k$ is half of its width (see the picture). Let the parabola intersect the $x$ -axis at $x=p, -p$ and pass through the top vertices: $(k,t), (-k,t)$ of the rectangle. The equation of the parabola is: $$y=ax^2+c$$ $c > t > 0$ , $a < 0$ . We want to find $a,c$ such that the length of the parabolic curve from $-p$ to $p$ is minimized (note that $p$ is dependent on $a$ and $c$ ). My work on the problem so far: notice that $(k,t)$ is a point on the parabola. So: $$ak^2+c=t$$ $$c=t-ak^2$$ So know we really just need to find $a$ . let us solve for $p$ ( $x$ intersection): $$ap^2+c=0$$ $$p=\sqrt{\frac{-c}{a}}=\sqrt{\frac{ak^2-t}{a}}=\sqrt{\frac{t-ak^2}{-a}}=\frac{\sqrt{t-ak^2}}{\sqrt{-a}}$$ recall that the length of a curve from $-p$ to $p$ is given by: $$\int _{-p}^p\:\sqrt{1+\left(\frac{dy}{dx}\right)^2}dx$$ By symmetry, the length of the parabola from $-p$ to $p$ will be 2*length of the parabola from $0$ to $p$ . Subsituting: $$2\int _{x=0}^{x=p}\:\sqrt{1+\left(2ax\right)^2}dx$$ This integral is quite easy to solve with trig substitution. After solving I got that: $$f\left(a\right)=\frac{2ap\sqrt{1+4a^2p^2}+ln\left(2ap+\sqrt{1+4a^2p^2}\right)}{2a}$$ Note that $p$ is not a constant, because it is dependant on $a$ . It seems as though we have made progress, we have managed to build a function, dependant on only one variable, that returns the length of the curve. The function works and returns a correct result (I have verefied it with a couple of examples) but the derivative is very long and i doubt that it is solvable. I am quite stuck here with my current knowledge. But I am positive because when you graph the function on desmos there is indeed one local minimum for $a < 0$ . An easier variant is trying to minimize the area blocked by the parabola and the $x$ axis. It was much easier to solve...","Hello, I have came up with what I think is a unique optimization problem. We are given the positive real parameters . is the height of the rectangle, and is half of its width (see the picture). Let the parabola intersect the -axis at and pass through the top vertices: of the rectangle. The equation of the parabola is: , . We want to find such that the length of the parabolic curve from to is minimized (note that is dependent on and ). My work on the problem so far: notice that is a point on the parabola. So: So know we really just need to find . let us solve for ( intersection): recall that the length of a curve from to is given by: By symmetry, the length of the parabola from to will be 2*length of the parabola from to . Subsituting: This integral is quite easy to solve with trig substitution. After solving I got that: Note that is not a constant, because it is dependant on . It seems as though we have made progress, we have managed to build a function, dependant on only one variable, that returns the length of the curve. The function works and returns a correct result (I have verefied it with a couple of examples) but the derivative is very long and i doubt that it is solvable. I am quite stuck here with my current knowledge. But I am positive because when you graph the function on desmos there is indeed one local minimum for . An easier variant is trying to minimize the area blocked by the parabola and the axis. It was much easier to solve...","k,t t k x x=p, -p (k,t), (-k,t) y=ax^2+c c > t > 0 a < 0 a,c -p p p a c (k,t) ak^2+c=t c=t-ak^2 a p x ap^2+c=0 p=\sqrt{\frac{-c}{a}}=\sqrt{\frac{ak^2-t}{a}}=\sqrt{\frac{t-ak^2}{-a}}=\frac{\sqrt{t-ak^2}}{\sqrt{-a}} -p p \int _{-p}^p\:\sqrt{1+\left(\frac{dy}{dx}\right)^2}dx -p p 0 p 2\int _{x=0}^{x=p}\:\sqrt{1+\left(2ax\right)^2}dx f\left(a\right)=\frac{2ap\sqrt{1+4a^2p^2}+ln\left(2ap+\sqrt{1+4a^2p^2}\right)}{2a} p a a < 0 x","['calculus', 'derivatives', 'optimization', 'quadratics', 'conic-sections']"
33,Is there a function defined on an interval that is right-differentiable a.e. but not left-differentiable?,Is there a function defined on an interval that is right-differentiable a.e. but not left-differentiable?,,"Inspired by this post , I was thinking about how different the left and right sided derivatives could be at a point. For instance, if $f(x) = e^{-1/\log(x)}$ , $f(1)=0$ , the right derivative at $x=1$ is zero but the left-derivative doesn't exist. One could stitch together several of these functions, for instance $F_n(x) = \sum_{k=1}^n f(x-k)$ , but I'm wondering if this pathology can be extended to yield a function $F$ that is right-differentiable almost-everywhere but left-differentiable almost-nowhere. Happy to provide clarification or more details if needed.","Inspired by this post , I was thinking about how different the left and right sided derivatives could be at a point. For instance, if , , the right derivative at is zero but the left-derivative doesn't exist. One could stitch together several of these functions, for instance , but I'm wondering if this pathology can be extended to yield a function that is right-differentiable almost-everywhere but left-differentiable almost-nowhere. Happy to provide clarification or more details if needed.",f(x) = e^{-1/\log(x)} f(1)=0 x=1 F_n(x) = \sum_{k=1}^n f(x-k) F,"['real-analysis', 'derivatives']"
34,"Variational derivative of $F_w(f) = \int_{y_1}^{y_2} dx_1 \int_{y_1}^{y_2} dx_2 f(x_1)w(x_1,x_2)f(x_2)$",Variational derivative of,"F_w(f) = \int_{y_1}^{y_2} dx_1 \int_{y_1}^{y_2} dx_2 f(x_1)w(x_1,x_2)f(x_2)","I am starting to work with functional derivatives. In Density functional theory an advanced course by Engel and Dreizler they have the example of the functional in the title of this question. The variation of the functional can be calculated to be $$\delta F_w = \int_{y_1}^{y_2} dx_1 \int_{y_1}^{y_2} dx_2 w(x_1,x_2)[(f(x_1)\varepsilon \eta(x_2) + f(x_2) \varepsilon \eta(x_1) + \varepsilon \eta(x_1) \varepsilon \eta(x_2)] $$ The Taylor Expansion around $\varepsilon = 0$ is $$ \delta F_w = \frac{dF(f + \varepsilon \eta)}{d \varepsilon} \bigg|_{\varepsilon = 0} \varepsilon + \frac{d^2F(f + \varepsilon \eta)}{d \varepsilon^2} \bigg|_{\varepsilon = 0} \varepsilon^2= \\=   \int_{y_1}^{y_2} dx_1 \int_{y_1}^{y_2} dx_2 w(x_1,x_2)(f(x_1)\eta(x_2) + f(x_2) \eta(x_1)) \varepsilon + \int_{y_1}^{y_2} dx_1 \int_{y_1}^{y_2} dx_2 w(x_1,x_2)2 \eta(x_1) \eta(x_2) \varepsilon^2 $$ By the definition of functional derivative I get that in the first term I need to isolate $\eta(x_1)$ , this can be done by rewriting it in the following way $$\frac{dF(f + \varepsilon \eta)}{d \varepsilon} \bigg|_{\varepsilon = 0} =   \int_{y_1}^{y_2} dx_1 \int_{y_1}^{y_2} dx_2 (w(x_1,x_2) + w(x_2,x_1))f(x_2)\eta(x_1)$$ Which means that the first functional derivative is $$ \frac{\delta F}{\delta f(x_1)} = \int_{y_1}^{y_2} dx_2 (w(x_1,x_2) + w(x_2,x_1))f(x_2)$$ So far, this agrees with the textbook, however there is something I do not understand in the second derivative, and it might be just some simple algebra issue. We have that $$ \frac{d^2F(f + \varepsilon \eta)}{d \varepsilon^2} \bigg|_{\varepsilon = 0} = \int_{y_1}^{y_2} dx_1 \int_{y_1}^{y_2} dx_2 w(x_1,x_2)2 \eta(x_1) \eta(x_2)$$ From the definition of the second variational derivative I conclude from the above that $$ \frac{\delta^2 F_w}{\delta f(x_1) \delta f(x_2)} = 2 w(x_1,x_2)$$ However the textbook writes that $$\frac{\delta^2 F_w}{\delta f(x_1) \delta f(x_2)} = w(x_1,x_2) + w(x_2,x_1)$$ I get that (unless there is something I am not seeing) $$\int_{y_1}^{y_2} dx_1 \int_{y_1}^{y_2} dx_2 w(x_1,x_2)2 \eta(x_1) \eta(x_2) = \int_{y_1}^{y_2} dx1 \int_{y_1}^{y_2} dx_2 (w(x_1,x_2) + w(x_2,x_1)) \eta(x_1) \eta(x_2) $$ However, $2w(x_1,x_2) \neq w(x_1,x_2) + w(x_2,x_1)$ unless $w(.,.)$ is symmetric, so my solution to the second derivative differs from the one in the textbook. This might be just some simple algebra I am not seeing, what am I missing?","I am starting to work with functional derivatives. In Density functional theory an advanced course by Engel and Dreizler they have the example of the functional in the title of this question. The variation of the functional can be calculated to be The Taylor Expansion around is By the definition of functional derivative I get that in the first term I need to isolate , this can be done by rewriting it in the following way Which means that the first functional derivative is So far, this agrees with the textbook, however there is something I do not understand in the second derivative, and it might be just some simple algebra issue. We have that From the definition of the second variational derivative I conclude from the above that However the textbook writes that I get that (unless there is something I am not seeing) However, unless is symmetric, so my solution to the second derivative differs from the one in the textbook. This might be just some simple algebra I am not seeing, what am I missing?","\delta F_w = \int_{y_1}^{y_2} dx_1 \int_{y_1}^{y_2} dx_2 w(x_1,x_2)[(f(x_1)\varepsilon \eta(x_2) + f(x_2) \varepsilon \eta(x_1) + \varepsilon \eta(x_1) \varepsilon \eta(x_2)]  \varepsilon = 0  \delta F_w = \frac{dF(f + \varepsilon \eta)}{d \varepsilon} \bigg|_{\varepsilon = 0} \varepsilon + \frac{d^2F(f + \varepsilon \eta)}{d \varepsilon^2} \bigg|_{\varepsilon = 0} \varepsilon^2= \\= 
 \int_{y_1}^{y_2} dx_1 \int_{y_1}^{y_2} dx_2 w(x_1,x_2)(f(x_1)\eta(x_2) + f(x_2) \eta(x_1)) \varepsilon + \int_{y_1}^{y_2} dx_1 \int_{y_1}^{y_2} dx_2 w(x_1,x_2)2 \eta(x_1) \eta(x_2) \varepsilon^2  \eta(x_1) \frac{dF(f + \varepsilon \eta)}{d \varepsilon} \bigg|_{\varepsilon = 0} =   \int_{y_1}^{y_2} dx_1 \int_{y_1}^{y_2} dx_2 (w(x_1,x_2) + w(x_2,x_1))f(x_2)\eta(x_1)  \frac{\delta F}{\delta f(x_1)} = \int_{y_1}^{y_2} dx_2 (w(x_1,x_2) + w(x_2,x_1))f(x_2)  \frac{d^2F(f + \varepsilon \eta)}{d \varepsilon^2} \bigg|_{\varepsilon = 0} = \int_{y_1}^{y_2} dx_1 \int_{y_1}^{y_2} dx_2 w(x_1,x_2)2 \eta(x_1) \eta(x_2)  \frac{\delta^2 F_w}{\delta f(x_1) \delta f(x_2)} = 2 w(x_1,x_2) \frac{\delta^2 F_w}{\delta f(x_1) \delta f(x_2)} = w(x_1,x_2) + w(x_2,x_1) \int_{y_1}^{y_2} dx_1 \int_{y_1}^{y_2} dx_2 w(x_1,x_2)2 \eta(x_1) \eta(x_2) = \int_{y_1}^{y_2} dx1 \int_{y_1}^{y_2} dx_2 (w(x_1,x_2) + w(x_2,x_1)) \eta(x_1) \eta(x_2)  2w(x_1,x_2) \neq w(x_1,x_2) + w(x_2,x_1) w(.,.)","['derivatives', 'calculus-of-variations']"
35,"Cartan Differentiable calculus. Show $g(x,y)= \frac{f(x)-f(y)}{x-y}$ is differentiable at $(x_{0},x_{0})$",Cartan Differentiable calculus. Show  is differentiable at,"g(x,y)= \frac{f(x)-f(y)}{x-y} (x_{0},x_{0})","I'm doing problem 8 of Cartan Differentiable Calculus book. The problem says as follow: Let $f$ assume its values in a Banach space $E$ , an let it be of class $\mathcal{C}^1$ in an open interval $I$ . Put $ \begin{cases} g(x,y)&= \frac{f(x)-f(y)}{x-y} ~ \text{ if } x \neq y\\ g(x,x)&= f'(x) \end{cases} $ If $f''(x_{0})$ exists at $x_{0} \in I$ show that $g$ is differentiable in $(x_{0},x_{0})$ So, I think that we should have $Dg(x_{0},x_{0})[h_{1},h_{2}] = \frac{f''(x_{0})}{2}(h_{1}-h_{2})$ but I've manage nothing more. The version I have says the following hint: Apply the mean value theorem to the function $ f(x)=xf'(x_{0})- \frac{(x-x_{0})^2}{2}f''(x_{0}) $ I think there is a typo and it should be $h(x)=...$ Either way I would gladly appreciate any help. Edit: I already proved that g is continous in $I \times I$ and its $\mathcal{C}^1$ in $I \times I \backslash \cup_{x \in I} \{(x,x)\}$","I'm doing problem 8 of Cartan Differentiable Calculus book. The problem says as follow: Let assume its values in a Banach space , an let it be of class in an open interval . Put If exists at show that is differentiable in So, I think that we should have but I've manage nothing more. The version I have says the following hint: Apply the mean value theorem to the function I think there is a typo and it should be Either way I would gladly appreciate any help. Edit: I already proved that g is continous in and its in","f E \mathcal{C}^1 I 
\begin{cases}
g(x,y)&= \frac{f(x)-f(y)}{x-y} ~ \text{ if } x \neq y\\
g(x,x)&= f'(x)
\end{cases}
 f''(x_{0}) x_{0} \in I g (x_{0},x_{0}) Dg(x_{0},x_{0})[h_{1},h_{2}] = \frac{f''(x_{0})}{2}(h_{1}-h_{2}) 
f(x)=xf'(x_{0})- \frac{(x-x_{0})^2}{2}f''(x_{0})
 h(x)=... I \times I \mathcal{C}^1 I \times I \backslash \cup_{x \in I} \{(x,x)\}","['derivatives', 'banach-spaces', 'differential']"
36,Proof verification: Baby Rudin Chapter 5 Exercise 11,Proof verification: Baby Rudin Chapter 5 Exercise 11,,"Baby Rudin, Chapter 5, Exercise 11 Suppose $f$ is defined in a neighborhood of $x$ , and suppose $f^{\prime\prime}(x)$ exists. Show that \begin{equation}\tag{11.0}     \lim_{h \to 0} \frac{f(x+h)+ f(x-h)-2f(x)}{h^2} = f^{\prime\prime}(x) \end{equation} My attempt: Firstly, notice that we can obtain alternative and equivalent versions of the definition of the derivative by making some notational maneuvers in the standard definition. We state the new definitions as follows: Let $f$ be defined (and real valued) on $[a, b]$ . For any $x\in [a, b]$ , the rate of change of the function $f$ at the point $x$ , denoted by $f^{\prime}(x)$ , is defined as \begin{equation}\tag{11.1}    f^{\prime}(x) = \lim_{h \to 0}\; \frac{f(x)-f(x-h)}{x-(x-h)} = \lim_{h \to 0}\; \frac{f(x)-f(x-h)}{h} \end{equation} where $a< t< b$ and $t \ne x$ . Moreover, leaving everything else unchanged, we can rewrite $(11.1)$ as \begin{equation}\tag{11.2}    f^{\prime}(x) = \lim_{h \to 0}\; \frac{f(x+h)-f(x)}{x+h-(x)} = \lim_{h \to 0}\; \frac{f(x+h)-f(x)}{h} \end{equation} Next, if we assume that $f^{\prime}(x)$ exists in a neighborhood of $x$ and $f^{\prime}$ is differentiable at the point $x$ , then, using (11.2) we can define $f^{\prime\prime}(x)$ as: \begin{equation}\tag{11.3}    f^{\prime\prime}(x) = \lim_{h \to 0}\; \frac{f^{\prime}(x+h)-f^{\prime}(x)}{h} \end{equation} We only need to perform some routine algebra to show (11.0). Suppose $f$ is defined in a neighborhood of $x, [a, b]$ , and suppose $f^{\prime\prime}(x)$ exists. Then, we know that $f^\prime$ exists in a neighborhood of $x$ and is differentiable at $x$ . Thus, (11.3) holds and by (11.1), we have \begin{equation}\tag{11.4}     f^{\prime}(x+h) = \lim_{h \to 0}\; \frac{f(x+h)-f[(x+h)-h]}{(x+h)-[(x+h)-h]} = \lim_{h \to 0}\; \frac{f(x+h)-f(x)}{h} \end{equation} By substituting (11.4) and (11.1) in (11.3), we get \begin{align*}     f^{\prime\prime}(x) &= \frac{1}{h}\cdot \lim_{h \to 0}\; \left[\frac{f(x+h)-f(x)}{h} -  \left(\frac{f(x)-f(x-h)}{h}\right)\right] \\     &= \frac{1}{h^2} \lim_{h \to 0}\; \left[f(x+h)-f(x)-f(x)+f(x-h)\right] \end{align*} which proves (11.0). My question: Is my proof correct? If not, how can the mistakes in this proof be patched up? Especially, is the expression in (11.4) and the process for obtaining (11.4) correct? I've basically shown that the right hand sides of (11.4) and (11.2) are the same, despite the fact their left hand sided are not equal.","Baby Rudin, Chapter 5, Exercise 11 Suppose is defined in a neighborhood of , and suppose exists. Show that My attempt: Firstly, notice that we can obtain alternative and equivalent versions of the definition of the derivative by making some notational maneuvers in the standard definition. We state the new definitions as follows: Let be defined (and real valued) on . For any , the rate of change of the function at the point , denoted by , is defined as where and . Moreover, leaving everything else unchanged, we can rewrite as Next, if we assume that exists in a neighborhood of and is differentiable at the point , then, using (11.2) we can define as: We only need to perform some routine algebra to show (11.0). Suppose is defined in a neighborhood of , and suppose exists. Then, we know that exists in a neighborhood of and is differentiable at . Thus, (11.3) holds and by (11.1), we have By substituting (11.4) and (11.1) in (11.3), we get which proves (11.0). My question: Is my proof correct? If not, how can the mistakes in this proof be patched up? Especially, is the expression in (11.4) and the process for obtaining (11.4) correct? I've basically shown that the right hand sides of (11.4) and (11.2) are the same, despite the fact their left hand sided are not equal.","f x f^{\prime\prime}(x) \begin{equation}\tag{11.0}
    \lim_{h \to 0} \frac{f(x+h)+ f(x-h)-2f(x)}{h^2} = f^{\prime\prime}(x)
\end{equation} f [a, b] x\in [a, b] f x f^{\prime}(x) \begin{equation}\tag{11.1}
   f^{\prime}(x) = \lim_{h \to 0}\; \frac{f(x)-f(x-h)}{x-(x-h)} = \lim_{h \to 0}\; \frac{f(x)-f(x-h)}{h}
\end{equation} a< t< b t \ne x (11.1) \begin{equation}\tag{11.2}
   f^{\prime}(x) = \lim_{h \to 0}\; \frac{f(x+h)-f(x)}{x+h-(x)} = \lim_{h \to 0}\; \frac{f(x+h)-f(x)}{h}
\end{equation} f^{\prime}(x) x f^{\prime} x f^{\prime\prime}(x) \begin{equation}\tag{11.3}
   f^{\prime\prime}(x) = \lim_{h \to 0}\; \frac{f^{\prime}(x+h)-f^{\prime}(x)}{h}
\end{equation} f x, [a, b] f^{\prime\prime}(x) f^\prime x x \begin{equation}\tag{11.4}
    f^{\prime}(x+h) = \lim_{h \to 0}\; \frac{f(x+h)-f[(x+h)-h]}{(x+h)-[(x+h)-h]} = \lim_{h \to 0}\; \frac{f(x+h)-f(x)}{h}
\end{equation} \begin{align*}
    f^{\prime\prime}(x) &= \frac{1}{h}\cdot \lim_{h \to 0}\; \left[\frac{f(x+h)-f(x)}{h} -  \left(\frac{f(x)-f(x-h)}{h}\right)\right] \\
    &= \frac{1}{h^2} \lim_{h \to 0}\; \left[f(x+h)-f(x)-f(x)+f(x-h)\right]
\end{align*}","['real-analysis', 'derivatives', 'proof-explanation', 'solution-verification']"
37,Why is electric potential function in free space infinitely differentiable?,Why is electric potential function in free space infinitely differentiable?,,"Electric potential function in free space of a continuous charge distribution $\rho'$ distributed over volume $V' \subset \mathbb{R}^3$ is denoted by: $\psi (x,y,z): \mathbb{R}^3 \setminus{V'} \to \mathbb{R}$ and is defined as: $$\psi (x,y,z)=\int_{V'} \dfrac{\rho'}{R}\ dV'$$ where $R$ is distance between point $(x,y,z)$ to a point $ P \in V'$ I know electric potential function in free space is differentiable once but I don't see why it is infinitely differentiable . Please explain why it is so. EDIT Theorem: PD of $\dfrac{1}{R}$ of all orders are differentiable in domain $\mathbb{R^3} \setminus (R=0)$ Proof: Let $P_k$ denote polynomials of degree $k$ in $x,y,z,x',y',z'$ $$\dfrac{\partial\frac{1}{R}}{\partial x}=-\dfrac{x-x'}{R^3};\ \dfrac{\partial\frac{1}{R}}{\partial y}=-\dfrac{y-y'}{R^3};\ \dfrac{\partial\frac{1}{R}}{\partial z}=-\dfrac{z-z'}{R^3}$$ Therefore: PD of $\dfrac{1}{R}$ of $1^{st}$ order = $\dfrac{P_1}{R^{(2 \times 1) + 1}}$ \begin{align} \text{PD of $\dfrac{1}{R}$ of $k^{th}$ order} &= \dfrac{P_k}{R^{2k + 1}}\\ \implies \text{PD of $\dfrac{1}{R}$ of $(k+1)^{th}$ order} &= \dfrac{\partial}{\partial x} [\text{PD of $k^{th}$ order} ]\\ &= \dfrac{\partial \left[    \dfrac{P_k}{R^{2k + 1}}    \right]}{\partial x}\\ &= \dfrac{(P_k)'_x}{R^{2k+1}} - (2k+1) \dfrac{P_k}{R^{2k+2}} \dfrac{x-x'}{R}\\ &= \dfrac{(P_k)'_x\ [(x-x')^2+(y-y')^2+(z-z')^2]}{R^{2k+3}} - \dfrac{(2k+1)\ P_k\ (x-x')}{R^{2k+3}}\\ &=\dfrac{P_{k+1}}{R^{2(k+1)+1}}\\ \end{align} Thus by induction: PD of $\dfrac{1}{R}$ of $n^{th}$ order $=\dfrac{P_k}{R^{2n+1}}$ $P_k$ , being a polynomial function is continuous in $x,y,z$ in domain $\mathbb{R^3}$ $\dfrac{1}{R^{2n+1}}$ , being a radial function is continuous in $x,y,z$ in domain $\mathbb{R^3} \setminus (R=0)$ Thus: PD of $\dfrac{1}{R}$ of $n^{th}$ order is continuous in $x,y,z$ in domain $\mathbb{R^3} \setminus (R=0)$ PD of $\dfrac{1}{R}$ of all orders are continuous in $x,y,z$ in domain $\mathbb{R^3} \setminus (R=0)$ PD of $\dfrac{1}{R}$ of all orders are differentiable in $x,y,z$ in domain $\mathbb{R^3} \setminus (R=0)$ Now how shall we prove PD if $\psi$ of all orders are differentiable in $x,y,z$ in domain $\mathbb{R^3} \setminus {V'}$ ?","Electric potential function in free space of a continuous charge distribution distributed over volume is denoted by: and is defined as: where is distance between point to a point I know electric potential function in free space is differentiable once but I don't see why it is infinitely differentiable . Please explain why it is so. EDIT Theorem: PD of of all orders are differentiable in domain Proof: Let denote polynomials of degree in Therefore: PD of of order = Thus by induction: PD of of order , being a polynomial function is continuous in in domain , being a radial function is continuous in in domain Thus: PD of of order is continuous in in domain PD of of all orders are continuous in in domain PD of of all orders are differentiable in in domain Now how shall we prove PD if of all orders are differentiable in in domain ?","\rho' V' \subset \mathbb{R}^3 \psi (x,y,z): \mathbb{R}^3 \setminus{V'} \to \mathbb{R} \psi (x,y,z)=\int_{V'} \dfrac{\rho'}{R}\ dV' R (x,y,z)  P \in V' \dfrac{1}{R} \mathbb{R^3} \setminus (R=0) P_k k x,y,z,x',y',z' \dfrac{\partial\frac{1}{R}}{\partial x}=-\dfrac{x-x'}{R^3};\
\dfrac{\partial\frac{1}{R}}{\partial y}=-\dfrac{y-y'}{R^3};\
\dfrac{\partial\frac{1}{R}}{\partial z}=-\dfrac{z-z'}{R^3} \dfrac{1}{R} 1^{st} \dfrac{P_1}{R^{(2 \times 1) + 1}} \begin{align}
\text{PD of \dfrac{1}{R} of k^{th} order} &= \dfrac{P_k}{R^{2k + 1}}\\
\implies \text{PD of \dfrac{1}{R} of (k+1)^{th} order} &= \dfrac{\partial}{\partial x} [\text{PD of k^{th} order} ]\\
&= \dfrac{\partial \left[    \dfrac{P_k}{R^{2k + 1}}    \right]}{\partial x}\\
&= \dfrac{(P_k)'_x}{R^{2k+1}} - (2k+1) \dfrac{P_k}{R^{2k+2}} \dfrac{x-x'}{R}\\
&= \dfrac{(P_k)'_x\ [(x-x')^2+(y-y')^2+(z-z')^2]}{R^{2k+3}} - \dfrac{(2k+1)\ P_k\ (x-x')}{R^{2k+3}}\\
&=\dfrac{P_{k+1}}{R^{2(k+1)+1}}\\
\end{align} \dfrac{1}{R} n^{th} =\dfrac{P_k}{R^{2n+1}} P_k x,y,z \mathbb{R^3} \dfrac{1}{R^{2n+1}} x,y,z \mathbb{R^3} \setminus (R=0) \dfrac{1}{R} n^{th} x,y,z \mathbb{R^3} \setminus (R=0) \dfrac{1}{R} x,y,z \mathbb{R^3} \setminus (R=0) \dfrac{1}{R} x,y,z \mathbb{R^3} \setminus (R=0) \psi x,y,z \mathbb{R^3} \setminus {V'}","['derivatives', 'continuity', 'infinity', 'potential-theory', 'electromagnetism']"
38,Visualizing derivative of a matrix-valued function of a matrix variable,Visualizing derivative of a matrix-valued function of a matrix variable,,"Apologies if this is not at an appropriate level for this site or if it's too broad/scrambled of a question, but I was wondering how best to visualize a matrix-valued function of a matrix variable? For some context: in my single variable calculus class, we were taught that the derivative of $f(x)$ at a point $x = a$ is $f'(a)\in\mathbb{R}$ . This represents the slope of the tangent line to the graph of $f(x)\vert_{x = a}$ . The equation of this tangent line is $y = f'(a)(x-a) + f(a)$ , and this is the best linear approximation of $f(x)$ near $x = a$ . In my analysis class, we were taught that the derivative of a map $f:U\to F$ , where $U\subseteq E$ is open, $x\in U$ , and $E,F$ are complete normed linear spaces, is a continuous linear map $\lambda=f'(x):E\to F$ satisfying $$f(x+h)-f(x)=f'(x)(h)+|h|\psi(h) $$ with $\lim_{h\to 0}\psi(h)=0$ and $\psi(0)=0.$ There is no confusion here. This definition is in accordance with our definition from single variable calculus, it's a special case as expected, where the standard matrix of this linear transformation (in the one dimensional case) is the $1\times 1$ matrix $[f'(a)]$ . If we consider the special case of our scenario where $E$ and $F$ are Euclidean spaces rather than arbitrary complete normed linear spaces, i.e. for $f : \mathbb{R}^n \to \mathbb{R}^m$ at a point $a \in \mathbb{R}^n$ then the derivative $Df(a)$ is an $m\times n$ matrix $Df(a) = \left[\frac{\partial f_i}{\partial x_j}( a)\right]$ , or the Jacobian of $f$ at $a$ . On our exam, we were asked to compute the derivative of $f:M_{n\times n}(\mathbb{R})\to M_{n\times n}(\mathbb{R}) $ given by $f(x)=x^3$ . This was a fairly straightforward computational task, however I have been stuck about how to visualize/understand my answer. I know you could just say that $M_{n\times n}(\mathbb{R})$ and $\mathbb{R}^{n^2}$ are isomorphic as complete normed linear spaces, so think of it in terms of the framework that has already been established. My question is: I know how to visualize/conceptualize this exam question when $f$ is a real-valued function of a real variable, but in the case from the exam is there a way to understand it in terms of what a linear map actually does (some combination of rotation and scaling) or where the vectors that comprise the matrix live with respect to the function (in some tangent subspace would be my intuition, but I'm not sure since I don't have that level of background in such a subject)? Is there a meaningful way to answer/subject to address my question or is the whole matter moot since even for a $3\times 3$ matrix, that would correspond to a function of $9$ variables, which we cannot visualize?","Apologies if this is not at an appropriate level for this site or if it's too broad/scrambled of a question, but I was wondering how best to visualize a matrix-valued function of a matrix variable? For some context: in my single variable calculus class, we were taught that the derivative of at a point is . This represents the slope of the tangent line to the graph of . The equation of this tangent line is , and this is the best linear approximation of near . In my analysis class, we were taught that the derivative of a map , where is open, , and are complete normed linear spaces, is a continuous linear map satisfying with and There is no confusion here. This definition is in accordance with our definition from single variable calculus, it's a special case as expected, where the standard matrix of this linear transformation (in the one dimensional case) is the matrix . If we consider the special case of our scenario where and are Euclidean spaces rather than arbitrary complete normed linear spaces, i.e. for at a point then the derivative is an matrix , or the Jacobian of at . On our exam, we were asked to compute the derivative of given by . This was a fairly straightforward computational task, however I have been stuck about how to visualize/understand my answer. I know you could just say that and are isomorphic as complete normed linear spaces, so think of it in terms of the framework that has already been established. My question is: I know how to visualize/conceptualize this exam question when is a real-valued function of a real variable, but in the case from the exam is there a way to understand it in terms of what a linear map actually does (some combination of rotation and scaling) or where the vectors that comprise the matrix live with respect to the function (in some tangent subspace would be my intuition, but I'm not sure since I don't have that level of background in such a subject)? Is there a meaningful way to answer/subject to address my question or is the whole matter moot since even for a matrix, that would correspond to a function of variables, which we cannot visualize?","f(x) x = a f'(a)\in\mathbb{R} f(x)\vert_{x = a} y = f'(a)(x-a) + f(a) f(x) x = a f:U\to F U\subseteq E x\in U E,F \lambda=f'(x):E\to F f(x+h)-f(x)=f'(x)(h)+|h|\psi(h)  \lim_{h\to 0}\psi(h)=0 \psi(0)=0. 1\times 1 [f'(a)] E F f : \mathbb{R}^n \to \mathbb{R}^m a \in \mathbb{R}^n Df(a) m\times n Df(a) = \left[\frac{\partial f_i}{\partial x_j}( a)\right] f a f:M_{n\times n}(\mathbb{R})\to M_{n\times n}(\mathbb{R})  f(x)=x^3 M_{n\times n}(\mathbb{R}) \mathbb{R}^{n^2} f 3\times 3 9","['real-analysis', 'linear-algebra', 'matrices', 'derivatives', 'visualization']"
39,Is there any situation where the derivative ${ dy \over (dx)^2 }$ makes sense?,Is there any situation where the derivative  makes sense?,{ dy \over (dx)^2 },"Is there any situation where the following expression ever make any sense in calculus? ${ dy \over (dx)^2 }$ If we imagine a square with side length $x$ and increase by a small amount $dx$ , and $dy$ is the increase in area, then $$dy = 2xdx+(dx)^2$$ Normally, the purpose is to calculate ${dy \over dx}=2x+dx = 2x$ by saying the extra $dx$ term is infinitesimal and can be treated as zero. However, if we divide $(dx)^2$ instead we get $${dy \over (dx)^2}={2x \over dx}+1$$ Now in this example the ${2x \over dx}$ term becomes infinity and therefore the expression makes no sense, but I am wondering is there any case that doing a similar thing would make sense and there is a meaning for the term ${ dy \over (dx)^2 }$ . It seems impossible to me at a glance for any expression to have a non-infinity term on the right, but once things get complicated and infinite series are involved I am wondering if it is really impossible or actually there is some field of study that the expression actually makes sense.","Is there any situation where the following expression ever make any sense in calculus? If we imagine a square with side length and increase by a small amount , and is the increase in area, then Normally, the purpose is to calculate by saying the extra term is infinitesimal and can be treated as zero. However, if we divide instead we get Now in this example the term becomes infinity and therefore the expression makes no sense, but I am wondering is there any case that doing a similar thing would make sense and there is a meaning for the term . It seems impossible to me at a glance for any expression to have a non-infinity term on the right, but once things get complicated and infinite series are involved I am wondering if it is really impossible or actually there is some field of study that the expression actually makes sense.",{ dy \over (dx)^2 } x dx dy dy = 2xdx+(dx)^2 {dy \over dx}=2x+dx = 2x dx (dx)^2 {dy \over (dx)^2}={2x \over dx}+1 {2x \over dx} { dy \over (dx)^2 },"['calculus', 'derivatives']"
40,"If $f$ and $g$ are differentiable at $x_{0}$, and $g$ is non-zero on $X$, then $f/g$ is also differentiable at $x_{0}$ and . . .","If  and  are differentiable at , and  is non-zero on , then  is also differentiable at  and . . .",f g x_{0} g X f/g x_{0},"If $f$ and $g$ are differentiable at $x_{0}$ , and $g$ is non-zero on $X$ , then $f/g$ is also differentiable at $x_{0}$ and \begin{align*} \left(\frac{f}{g}\right)' = \frac{f'(x_{0})g(x_{0}) - f(x_{0})g'(x_{0})}{g(x_{0})^{2}} \end{align*} MY ATTEMPT According to the definition of derivative, we have that \begin{align*} \lim_{x\rightarrow x_{0}}\frac{(f/g)(x) - (f/g)(x_{0})}{x-x_{0}} = \lim_{x\rightarrow x_{0}}\frac{f(x)g(x_{0}) - f(x_{0})g(x)}{(x-x_{0})g(x)g(x_{0})} \end{align*} On the other hand, we have that \begin{align*} f(x)g(x_{0}) - f(x_{0})g(x) & = f(x)g(x_{0}) - f(x_{0})g(x_{0}) + f(x_{0})g(x_{0}) - f(x_{0})g(x)\\\\ & = [f(x) - f(x_{0})]g(x_{0}) - f(x_{0})[g(x) - g(x_{0})] \end{align*} Consequently, we have that \begin{align*} \lim_{x\rightarrow x_{0}}\frac{(f/g)(x) - (f/g)(x_{0})}{x-x_{0}} & = \lim_{x\rightarrow x_{0}}\frac{[f(x) - f(x_{0})]g(x_{0}) - f(x_{0})[g(x) - g(x_{0})] }{(x-x_{0})g(x)g(x_{0})}\\\\ & = \lim_{x\rightarrow x_{0}}\frac{[f(x) - f(x_{0})]g(x_{0})}{(x-x_{0})g(x)g(x_{0})} - \lim_{x\rightarrow x_{0}}\frac{f(x_{0})[g(x) - g(x_{0})]}{(x-x_{0})g(x)g(x_{0})}\\\\ & = \frac{f'(x_{0})g(x_{0}) - f(x_{0})g'(x_{0})}{g(x_{0})^{2}} \end{align*} and we are done. I would like to know if someone could provide an alternative solution to this problem. Any contribution is appreciated.","If and are differentiable at , and is non-zero on , then is also differentiable at and MY ATTEMPT According to the definition of derivative, we have that On the other hand, we have that Consequently, we have that and we are done. I would like to know if someone could provide an alternative solution to this problem. Any contribution is appreciated.","f g x_{0} g X f/g x_{0} \begin{align*}
\left(\frac{f}{g}\right)' = \frac{f'(x_{0})g(x_{0}) - f(x_{0})g'(x_{0})}{g(x_{0})^{2}}
\end{align*} \begin{align*}
\lim_{x\rightarrow x_{0}}\frac{(f/g)(x) - (f/g)(x_{0})}{x-x_{0}} = \lim_{x\rightarrow x_{0}}\frac{f(x)g(x_{0}) - f(x_{0})g(x)}{(x-x_{0})g(x)g(x_{0})}
\end{align*} \begin{align*}
f(x)g(x_{0}) - f(x_{0})g(x) & = f(x)g(x_{0}) - f(x_{0})g(x_{0}) + f(x_{0})g(x_{0}) - f(x_{0})g(x)\\\\
& = [f(x) - f(x_{0})]g(x_{0}) - f(x_{0})[g(x) - g(x_{0})]
\end{align*} \begin{align*}
\lim_{x\rightarrow x_{0}}\frac{(f/g)(x) - (f/g)(x_{0})}{x-x_{0}} & = \lim_{x\rightarrow x_{0}}\frac{[f(x) - f(x_{0})]g(x_{0}) - f(x_{0})[g(x) - g(x_{0})]
}{(x-x_{0})g(x)g(x_{0})}\\\\
& = \lim_{x\rightarrow x_{0}}\frac{[f(x) - f(x_{0})]g(x_{0})}{(x-x_{0})g(x)g(x_{0})} - \lim_{x\rightarrow x_{0}}\frac{f(x_{0})[g(x) - g(x_{0})]}{(x-x_{0})g(x)g(x_{0})}\\\\
& = \frac{f'(x_{0})g(x_{0}) - f(x_{0})g'(x_{0})}{g(x_{0})^{2}}
\end{align*}","['real-analysis', 'derivatives', 'solution-verification', 'alternative-proof']"
41,Does a concave increasing function that goes through the origin always have a diminishing elasticity?,Does a concave increasing function that goes through the origin always have a diminishing elasticity?,,"Consider a twice continuously differentiable function $f: \mathbb{R}_+^0 \rightarrow \mathbb{R}$ with $f(0)=0$ , $f'>0$ , $f''<0$ . Does that function feature non-increasing elasticity, i.e. $$\frac{ \partial \frac{f'(x) x}{f(x)}}{\partial x} \leq 0\quad \forall x?$$ I am unable to prove this simple point, but also unable to find a counter example. EDIT: I have made some progress, but it is not yet a proof. Take any $x>0$ and calculate the derivative on the LHS of the statement. The statement is true if (and only if) $$ f'(x) x - f(x) \geq \frac{f''(x) f(x) x}{f'(x)}$$ Using a second order Taylor expansion we obtain $$0=f(0)=f(x) - f'(x)x + 1/2 f''(x)x^2 - O(x^3).$$ Rearranging and plugging into the inequality we get $$ f''(x)x^2/2 - O(x^3) \geq \frac{f''(x) f(x) x}{f'(x)}$$ Which (at least in a neighborhood of 0 or if $-O(x^3)\geq 0$ ) has a sufficient condition in $$ f'(x) \leq 2\frac{f(x)}{x}.$$ Since $f(x)/x \geq f'(x)$ (because $f$ is concave, increasing and goes through the origin) that is true. Yet we don't know anything about what happens if $O(x^3) >0$ and we are far away from $x=0$ .","Consider a twice continuously differentiable function with , , . Does that function feature non-increasing elasticity, i.e. I am unable to prove this simple point, but also unable to find a counter example. EDIT: I have made some progress, but it is not yet a proof. Take any and calculate the derivative on the LHS of the statement. The statement is true if (and only if) Using a second order Taylor expansion we obtain Rearranging and plugging into the inequality we get Which (at least in a neighborhood of 0 or if ) has a sufficient condition in Since (because is concave, increasing and goes through the origin) that is true. Yet we don't know anything about what happens if and we are far away from .",f: \mathbb{R}_+^0 \rightarrow \mathbb{R} f(0)=0 f'>0 f''<0 \frac{ \partial \frac{f'(x) x}{f(x)}}{\partial x} \leq 0\quad \forall x? x>0  f'(x) x - f(x) \geq \frac{f''(x) f(x) x}{f'(x)} 0=f(0)=f(x) - f'(x)x + 1/2 f''(x)x^2 - O(x^3).  f''(x)x^2/2 - O(x^3) \geq \frac{f''(x) f(x) x}{f'(x)} -O(x^3)\geq 0  f'(x) \leq 2\frac{f(x)}{x}. f(x)/x \geq f'(x) f O(x^3) >0 x=0,"['real-analysis', 'derivatives', 'examples-counterexamples']"
42,Prove that the product of a continuous function with a differentiable function is also differentiable,Prove that the product of a continuous function with a differentiable function is also differentiable,,"Given a function $g : \mathbb{R}^2 \rightarrow \mathbb{R} $ differentiable such that $g(0,0)=0$ and a function $f : \mathbb{R}^2 \rightarrow \mathbb{R} $ continuous. Prove that $f\cdot g$ (the product of those functions) is differentiable in (0,0). I have already prove that $\frac{\partial f g}{\partial x} (0,0) = f(0,0) \frac{\partial g(0,0)}{\partial x}$ and $\frac{\partial f g}{\partial y} (0,0) =f(0,0) \frac{\partial g(0,0)}{\partial y}$ , so I guess I have to prove that $$ \lim_{(x,y) \to (0,0)} \frac{|fg(x,y)-f(0,0) \cdot \left(\frac{\partial g (0,0)}{\partial x} x + \frac{ \partial g(0,0)}{\partial y} y \right)|}{||x,y||} = 0$$ But I don't find any ""clean"" way to do that. What I can tell is that $$ \lim_{(x,y) \to (0,0)} \frac{|g(x,y)-\left(\frac{\partial g (0,0)}{\partial x} x + \frac{ \partial g(0,0)}{\partial y} y \right)|}{||x,y||} = 0$$ is true since g is differentiable by hypothesis. I also know that $\lim_{(x,y) \to (0,0)} f(x,y)=f(0,0)$ since f is continuous. So I would like to do something like $$\lim_{(x,y) \to (0,0)} \frac{fg(x,y)}{||x,y||} = \lim_{(x,y) \to (0,0)} \frac{f(0,0)\left(\frac{\partial g (0,0)}{\partial x} x + \frac{ \partial g(0,0)}{\partial y} y \right)}{||x,y||}$$ $$\lim_{(x,y) \to (0,0)}f(x,y) \lim_{(x,y) \to (0,0)} \frac{g(x,y)}{||x,y||} =f(0,0) \lim_{(x,y) \to (0,0)} \frac{\left(\frac{\partial g (0,0)}{\partial x} x + \frac{ \partial g(0,0)}{\partial y} y \right)}{||x,y||}$$ And since f is continuous $lim{(x,y) \to (0,0)} \frac{f(x,y)}{f(0,0)} = 1$ . $$\lim_{(x,y) \to (0,0)} \frac{g(x,y)}{||x,y||} = \lim_{(x,y) \to (0,0)} \frac{\left(\frac{\partial g (0,0)}{\partial x} x + \frac{ \partial g(0,0)}{\partial y} y \right)}{||x,y||}$$ And then $$ \lim_{(x,y) \to (0,0)} \frac{|g(x,y)-\left(\frac{\partial g (0,0)}{\partial x} x + \frac{ \partial g(0,0)}{\partial y} y \right)|}{||x,y||} = 0$$ is true since g is differentiable. But this doesn't seem right, I'm specially not sure about ""forgetting"" of the absolute value (it kinda makes sense since the limit is equal to zero but still not sure).","Given a function differentiable such that and a function continuous. Prove that (the product of those functions) is differentiable in (0,0). I have already prove that and , so I guess I have to prove that But I don't find any ""clean"" way to do that. What I can tell is that is true since g is differentiable by hypothesis. I also know that since f is continuous. So I would like to do something like And since f is continuous . And then is true since g is differentiable. But this doesn't seem right, I'm specially not sure about ""forgetting"" of the absolute value (it kinda makes sense since the limit is equal to zero but still not sure).","g : \mathbb{R}^2 \rightarrow \mathbb{R}  g(0,0)=0 f : \mathbb{R}^2 \rightarrow \mathbb{R}  f\cdot g \frac{\partial f g}{\partial x} (0,0) = f(0,0) \frac{\partial g(0,0)}{\partial x} \frac{\partial f g}{\partial y} (0,0) =f(0,0) \frac{\partial g(0,0)}{\partial y}  \lim_{(x,y) \to (0,0)} \frac{|fg(x,y)-f(0,0) \cdot \left(\frac{\partial g (0,0)}{\partial x} x + \frac{ \partial g(0,0)}{\partial y} y \right)|}{||x,y||} = 0  \lim_{(x,y) \to (0,0)} \frac{|g(x,y)-\left(\frac{\partial g (0,0)}{\partial x} x + \frac{ \partial g(0,0)}{\partial y} y \right)|}{||x,y||} = 0 \lim_{(x,y) \to (0,0)} f(x,y)=f(0,0) \lim_{(x,y) \to (0,0)} \frac{fg(x,y)}{||x,y||} = \lim_{(x,y) \to (0,0)} \frac{f(0,0)\left(\frac{\partial g (0,0)}{\partial x} x + \frac{ \partial g(0,0)}{\partial y} y \right)}{||x,y||} \lim_{(x,y) \to (0,0)}f(x,y) \lim_{(x,y) \to (0,0)} \frac{g(x,y)}{||x,y||} =f(0,0) \lim_{(x,y) \to (0,0)} \frac{\left(\frac{\partial g (0,0)}{\partial x} x + \frac{ \partial g(0,0)}{\partial y} y \right)}{||x,y||} lim{(x,y) \to (0,0)} \frac{f(x,y)}{f(0,0)} = 1 \lim_{(x,y) \to (0,0)} \frac{g(x,y)}{||x,y||} = \lim_{(x,y) \to (0,0)} \frac{\left(\frac{\partial g (0,0)}{\partial x} x + \frac{ \partial g(0,0)}{\partial y} y \right)}{||x,y||}  \lim_{(x,y) \to (0,0)} \frac{|g(x,y)-\left(\frac{\partial g (0,0)}{\partial x} x + \frac{ \partial g(0,0)}{\partial y} y \right)|}{||x,y||} = 0","['calculus', 'derivatives']"
43,"Find the point, if any, the graph of $f(x) = \sqrt{8x^2+x-3}$ has a horizontal tangent line","Find the point, if any, the graph of  has a horizontal tangent line",f(x) = \sqrt{8x^2+x-3},"Section 2.5 #14 Find the point, if any, the graph of $f(x) = \sqrt{8x^2+x-3}$ has a horizontal tangent line. Okay, so having a horizontal tangent line at a point on the graph means that the slope of that tangent line is zero. The derivative of a function is another function that tells us the slope of the tangent line at any given point on the graph of the original function. Thus, to find where the graph of $f(x) = \sqrt{8x^2+x-3}$ has a horizontal tangent line, we need to take the derivative, set it equal to zero, and solve for $x$ . This will give us the $x$ -coordinate of where the graph of $f(x)$ has a horizontal tangent line. To find the corresponding $y$ value, we plug the $x$ value that we found into the original equation. In this problem, when we plug the $x$ value we find into the original equation, we get an imaginary number, which means that no point on the graph of $f(x)$ has a horizontal tangent line, and thus our answer is DNE, does not exist. Let's go through the motions!!! $f(x) = \sqrt{8x^2+x-3}=(8x^2+x-3)^{1/2}$ $f'(x) = \frac{d}{dx}(8x^2+x-3)^{1/2}$ Time do the chain rule!!! $$\begin{align} f'(x) &= \frac{(8x^2+x-3)^{-1/2}}{2}\frac{d}{dx}(8x^2+x-3)\\ &= \frac{(8x^2+x-3)^{-1/2}}{2}(16x+1)\\ &= \frac{(16x+1)}{2(8x^2+x-3)^{1/2}} \end{align}$$ Alright, we have our derivative. We want to find horizontal tangent lines, so we set this equal to zero and solve for $x$ $$0 = \frac{(16x+1)}{2(8x^2+x-3)^{1/2}}$$ multiplying both sides of the equation by $2(8x^2+x-3)^{1/2}$ we get $0 = (16x+1)$ And thus $x = \frac{-1}{16}$ Now, we plug this value into the original equation to get the corresponding $y$ value, because remember, we are looking for a point on the graph where the horizontal line is tangent, so our answer will be in $(x,y)$ format, is it exists, (which in this case, it won't).. $f(\frac{-1}{16}) = \sqrt{8(\frac{-1}{16})^2+\frac{-1}{16}-3}$ But $8(\frac{-1}{16})^2+\frac{-1}{16}-3<0$ , so taking its square root will give us an imaginary number. Thus the answer is DNE","Section 2.5 #14 Find the point, if any, the graph of has a horizontal tangent line. Okay, so having a horizontal tangent line at a point on the graph means that the slope of that tangent line is zero. The derivative of a function is another function that tells us the slope of the tangent line at any given point on the graph of the original function. Thus, to find where the graph of has a horizontal tangent line, we need to take the derivative, set it equal to zero, and solve for . This will give us the -coordinate of where the graph of has a horizontal tangent line. To find the corresponding value, we plug the value that we found into the original equation. In this problem, when we plug the value we find into the original equation, we get an imaginary number, which means that no point on the graph of has a horizontal tangent line, and thus our answer is DNE, does not exist. Let's go through the motions!!! Time do the chain rule!!! Alright, we have our derivative. We want to find horizontal tangent lines, so we set this equal to zero and solve for multiplying both sides of the equation by we get And thus Now, we plug this value into the original equation to get the corresponding value, because remember, we are looking for a point on the graph where the horizontal line is tangent, so our answer will be in format, is it exists, (which in this case, it won't).. But , so taking its square root will give us an imaginary number. Thus the answer is DNE","f(x) = \sqrt{8x^2+x-3} f(x) = \sqrt{8x^2+x-3} x x f(x) y x x f(x) f(x) = \sqrt{8x^2+x-3}=(8x^2+x-3)^{1/2} f'(x) = \frac{d}{dx}(8x^2+x-3)^{1/2} \begin{align}
f'(x) &= \frac{(8x^2+x-3)^{-1/2}}{2}\frac{d}{dx}(8x^2+x-3)\\
&= \frac{(8x^2+x-3)^{-1/2}}{2}(16x+1)\\
&= \frac{(16x+1)}{2(8x^2+x-3)^{1/2}}
\end{align} x 0 = \frac{(16x+1)}{2(8x^2+x-3)^{1/2}} 2(8x^2+x-3)^{1/2} 0 = (16x+1) x = \frac{-1}{16} y (x,y) f(\frac{-1}{16}) = \sqrt{8(\frac{-1}{16})^2+\frac{-1}{16}-3} 8(\frac{-1}{16})^2+\frac{-1}{16}-3<0",['calculus']
44,Is the nth time derivative of position ever really zero?,Is the nth time derivative of position ever really zero?,,"Change in velocity comes from acceleration, but that acceleration was also a leap that started from nothing. It was zero, then positive, and then something else. So it's application must have been pushed by some other, more interior, and higher-derivative force. But the force that pushed this positive acceleration into existence, it also arose from zero. So it must have a higher-order parent that produced it. And so on. So is any nth time derivative of position ever really zero? Doesn’t any change in constant velocity rest finally on an infinite time derivative of position?","Change in velocity comes from acceleration, but that acceleration was also a leap that started from nothing. It was zero, then positive, and then something else. So it's application must have been pushed by some other, more interior, and higher-derivative force. But the force that pushed this positive acceleration into existence, it also arose from zero. So it must have a higher-order parent that produced it. And so on. So is any nth time derivative of position ever really zero? Doesn’t any change in constant velocity rest finally on an infinite time derivative of position?",,['derivatives']
45,Prove that Weierstrass-like function is not differentiable,Prove that Weierstrass-like function is not differentiable,,"Problem Define $$ f_1= \begin{cases}  x & \text{if } x\leq \frac12\\  1-x & \text{if }\frac12 \leq x \leq 1 \end{cases}$$ Then extend over $[0,\infty]$ by defining that for $x\geq 1$ , $f_1(x+1) = f_1(x)$ . Then define for $n\geq 2$ , $f_n(x)=\frac12 f_{n-1}(2x)$ . Let $S_m(x)=\sum_{n=1}^{m}f_n(x)$ . Then $S_m$ is continuous on $[0,\infty)$ and the sequence $(S_m)_{m\in\mathbb{N}}$ converges uniformly to a continuous function $S$ . Show that $S$ is not differentiable at any point in $(0,\infty)$ . My Attempt I am working through a textbook to practice real analysis in preparation for an upcoming analysis sequence. I have been stuck on this problem for hours and have no one to ask about it. My first idea was to switch the limits: $$ \lim_{h\rightarrow 0}\frac{S(x+h)-S(x)}{h}=\lim_{m\rightarrow\infty}\lim_{h\rightarrow 0}\frac{S_m(x+h)-S_m(x)}{h}$$ . Then I realized that $f_1$ isn't differentiable at multiples of $\frac12$ , $f_2$ isn't differentiable at multiples of $\frac14$ , and in general $f_n$ isn't differentiable at multiples of $\frac{1}{2^n}$ . Intuitively, as m approaches $\infty$ then $x$ gets closer and closer to some multiple of $\frac{1}{2^n}$ for some $n$ . Hence, $f_m(x)$ approximately isn't differentiable at some m (I mean this in the vaguest of terms). If $f_m(x)$ isn't differentiable, then $S_m(x)$ isn't differentiable. Hence, $S(x)$ isn't differentiable. However, this is where I stuck. $\textbf{Does anyone know what to do next? Am I even on the right track?}$ Thank you!","Problem Define Then extend over by defining that for , . Then define for , . Let . Then is continuous on and the sequence converges uniformly to a continuous function . Show that is not differentiable at any point in . My Attempt I am working through a textbook to practice real analysis in preparation for an upcoming analysis sequence. I have been stuck on this problem for hours and have no one to ask about it. My first idea was to switch the limits: . Then I realized that isn't differentiable at multiples of , isn't differentiable at multiples of , and in general isn't differentiable at multiples of . Intuitively, as m approaches then gets closer and closer to some multiple of for some . Hence, approximately isn't differentiable at some m (I mean this in the vaguest of terms). If isn't differentiable, then isn't differentiable. Hence, isn't differentiable. However, this is where I stuck. Thank you!","
f_1=
\begin{cases}
 x & \text{if } x\leq \frac12\\
 1-x & \text{if }\frac12 \leq x \leq 1
\end{cases} [0,\infty] x\geq 1 f_1(x+1) = f_1(x) n\geq 2 f_n(x)=\frac12 f_{n-1}(2x) S_m(x)=\sum_{n=1}^{m}f_n(x) S_m [0,\infty) (S_m)_{m\in\mathbb{N}} S S (0,\infty)  \lim_{h\rightarrow 0}\frac{S(x+h)-S(x)}{h}=\lim_{m\rightarrow\infty}\lim_{h\rightarrow 0}\frac{S_m(x+h)-S_m(x)}{h} f_1 \frac12 f_2 \frac14 f_n \frac{1}{2^n} \infty x \frac{1}{2^n} n f_m(x) f_m(x) S_m(x) S(x) \textbf{Does anyone know what to do next? Am I even on the right track?}","['real-analysis', 'derivatives', 'cauchy-sequences']"
46,"Proving that, $|f'(x)-f'(y)|\le k|x-y| \implies (f'(x))^2< 2 kf(x) $","Proving that,",|f'(x)-f'(y)|\le k|x-y| \implies (f'(x))^2< 2 kf(x) ,"Let $f:\Bbb R\to (0,\infty)$ be a differentiable function such that for some constant $k$ we have, $$|f'(x)-f'(y)|\le k|x-y|$$   for all $x,y \in\Bbb R.$ Then prove that, $$(f'(x))^2<2 kf(x).$$ Due to lack of arguments I could not proof this inequality. But rather I proved that is true for the particular function, $$f(x) =\cos^2(x)+1\implies f'(x) = -\sin(2x).$$ And we readily have,$$|\sin(2x)-\sin(2y)| = \left|\int_{2x}^{2y}\cos t dt\right|\le 2|x-y|$$ as well $$(f'(x))^2 = 4 \sin^2 x\cos^2x < 4(\cos^2x +1) = 4f(x).$$ It is also true for the function $x\mapsto \sin^2x +1.$ From these example I don't see how to prove the general case. Any hint will be welcome.","Let $f:\Bbb R\to (0,\infty)$ be a differentiable function such that for some constant $k$ we have, $$|f'(x)-f'(y)|\le k|x-y|$$   for all $x,y \in\Bbb R.$ Then prove that, $$(f'(x))^2<2 kf(x).$$ Due to lack of arguments I could not proof this inequality. But rather I proved that is true for the particular function, $$f(x) =\cos^2(x)+1\implies f'(x) = -\sin(2x).$$ And we readily have,$$|\sin(2x)-\sin(2y)| = \left|\int_{2x}^{2y}\cos t dt\right|\le 2|x-y|$$ as well $$(f'(x))^2 = 4 \sin^2 x\cos^2x < 4(\cos^2x +1) = 4f(x).$$ It is also true for the function $x\mapsto \sin^2x +1.$ From these example I don't see how to prove the general case. Any hint will be welcome.",,"['calculus', 'real-analysis', 'analysis', 'functions', 'lipschitz-functions']"
47,Why does $g$ have a global min at $x=0$ while $f$ does not and yet $g$ is defined in terms of $f$? [closed],Why does  have a global min at  while  does not and yet  is defined in terms of ? [closed],g x=0 f g f,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question $1.$ Let $f(x) = x^2\sin(1/x)$ for $x\neq 0$ and $f(0)=0.$ Show that $0$ is a critical point of $f$ that is not a local maximum nor a local minimum nor an inflection point. $2.$ Let $g(x) = 2x^2+f(x).$ Show that $g$ does have a global minimum at $0,$ but $g'(x)$ changes sign infinitely often on both $(0,\epsilon)$ and $(-\epsilon,0)$ for any $\epsilon >0.$ Here's what I've done. $1.$ By the product rule and chain rule, $f'(x) = 2x\sin(1/x)-\cos(1/x).$ We will show that $f'(0)=0$ is undefined at $x=0,$ which will show that $f$ has a critical point at $x=0.$ $f'(0)=\lim\limits_{h\to 0}\dfrac{f(h)}{h}=\lim\limits_{h\to 0}\dfrac{h^2\sin(1/h)}{h}\\ =\lim\limits_{h\to0} h\sin(1/h)=\lim\limits.$ Since $-h\leq h\sin(1/h)\leq h\;\forall h\geq 0$ and $h\leq h\sin(1/h)\leq -h\;\forall h\leq 0,$ by the Squeeze Theorem, $\lim\limits_{h\to0}h\sin(1/h)=0=\lim\limits_{h\to 0}h=0.$ (by the Squeeze Theorem). We first prove that $(0,0)$ is neither a local minimum nor a local maximum of $f.$ Notice that whenever $x$ is of the form $\dfrac{1}{-\frac\pi2+2n\pi},n\in\mathbb{Z},$ $\sin(1/x)=-1.$ We know this because $\sin(1/x)=-1$ whenever $1/x$ is of the form $-\frac\pi2+2n\pi,n\in\mathbb{Z}.$ As well, $\sin(1/x)=1$ whenever $x$ is of the form $\dfrac{1}{\frac\pi2+2n\pi},n\in\mathbb{Z}\Rightarrow f(x)=x^2>0$ (as $x\neq 0$ ). Let $x_n = \dfrac{1}{-\frac\pi2+2n\pi},n\in\mathbb{N}$ and $y_n = \dfrac{1}{\frac\pi2+2n\pi},n\in\mathbb{N}.$ Note that $\lim\limits_{n\to\infty}f(x_n)=\lim\limits_{n\to\infty} -x_n^2=\lim\limits_{n\to\infty} f(y_n)=\lim\limits_{n\to\infty} y_n^2=0.$ Then $\forall \epsilon>0,$ there exists $N\in\mathbb{N}$ such that $\forall n\geq N, |x_n|<\epsilon$ and $\exists N_2\in\mathbb{N}$ such that $\forall n\geq N_2, |y_n|<\epsilon.$ Thus, when $N_3=\max\{N,N_2\},$ we have that $-\epsilon < x_n,y_n<\epsilon.$ So there exist $x_n,y_n \in (-\epsilon,\epsilon)\;\forall \epsilon >0$ for some $n\in\mathbb{N}$ such that $f(x_n) = -x_n^2 < 0 <f(y_n) = y_n^2.$ A similar proof for $x<0$ can be shown by using the opposite values of $x_n$ and $y_n.$ Thus $f$ does not have a local maximum or minimum at $x=0.$ By the product rule and chain rule, $f''(x)=2\sin(1/x)-\dfrac{2}{x}\cos(1/x)-\dfrac{\sin(1/x)}{x^2}.$ When $x=\dfrac{1}{2n\pi},n\in\mathbb{Z}\backslash\{0\},f''(x)=-4n\pi.$ When $x=\dfrac{1}{-\frac\pi2+2n\pi},n\in\mathbb{Z},f''(x)=-2+(-\frac\pi2+2n\pi)^2>0.$ So, using a similar approach to above, with $x_n = \dfrac{1}{2n\pi}$ and $y_n=\dfrac{1}{-\frac\pi2+2n\pi},n\in\mathbb{N},$ we can show that $f$ does not have an inflection point when $x>0.$ The proof for $x<0$ also involves using negative values for $x_n$ and $y_n.$ $2.$ Since $f(0)=0, g(0)=2(0)^2+f(0)=0.$ We have that since $\sin(1/x)\geq -1\;\forall x\in\mathbb{R}\backslash\{0\}, g(x)=2x^2+x^2\sin(1/x)\geq x^2>0\;\forall x\in\mathbb{R}\backslash \{0\}.$ Hence $(0,0)$ is a global minimum for $g.$ Also, $g'(0)=0,$ so it is indeed a minimum. Here is the proof: By the derivative definition, $$g'(0) =\lim\limits_{h\to 0}\dfrac{g(h)-g(0)}{h}\\ =\lim\limits_{h\to 0}\dfrac{2h^2+h^2\sin(1/h)}{h}.$$ From above, $\lim\limits_{h\to 0}\dfrac{h^2\sin(1/h)}{h}=0,$ and so since $\lim\limits_{h\to 0}\dfrac{2h^2}{h}=\lim\limits_{h\to 0}2h=0,g'(0)=0.$ Now, $g'(x) = 4x+2x\sin(1/x)-\cos(1/x).$ We show that $g'(x)$ changes sign infinitely often on $(-\epsilon,0)$ and $(0,\epsilon)$ for any $\epsilon >0.$ We first show this result for $(0,\epsilon)$ . Whenever $x=\dfrac{1}{n\pi},n\in\mathbb{N},g'(x)=\dfrac{4}{n\pi}+1>0.$ Whenever $x=\dfrac{1}{2n\pi},n\in\mathbb{N},g'(x)=\dfrac{2}{n\pi}-1<0.$ Let $(y_n)_{n=1}^\infty$ be the sequence defined by $y_n=\dfrac{1}{2n\pi}$ and let $(x_n)_{n=1}^\infty$ be the sequence defined by $x_n=\dfrac{1}{n\pi}.$ We have that $\lim\limits_{n\to\infty} x_n = \lim\limits_{n\to\infty} y_n=0.$ So $\exists N_1\in \mathbb{N}$ such that $n \geq N_1 \Rightarrow |x_n| < \epsilon, \forall \epsilon >0$ and $\exists N_1\in\mathbb{N}$ such that $n \geq N_2 \Rightarrow |y_n| < \epsilon, \forall \epsilon >0.$ Pick $N_4=\max\{N_1,N_2\}.$ So for all $n \geq N_4, 0< |y_n| = y_n <\epsilon$ and $0<|x_n|=x_n<\epsilon.$ Since $g'(y_n)<0$ at every $y_n$ and $g'(x_n)>0$ at every $x_n,$ we have that $g'(x)$ changes sign infinitely many times on $(0,\epsilon)$ . The proof that $g'(x)$ changes sign inifinitely often on $(-\epsilon,0)$ is very similar. Notice that when $x=-\dfrac{1}{2n\pi},n\in\mathbb{N},g'(x)<0$ and when $x=-\dfrac{1}{n\pi},n\in\mathbb{N},n>1,g'(x)>0.$ We set $y_{n_2} =-\dfrac{1}{2n_2\pi}$ and $x_{n_2}=-\dfrac{1}{n_2\pi},n_2\in\mathbb{N}$ and obtain the same result as above. Edit: some notes: the work above can definitely be simplified. Also, I need to show that $f'(0)=0$ using the definition of a derivative and the Squeeze Theorem.","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question Let for and Show that is a critical point of that is not a local maximum nor a local minimum nor an inflection point. Let Show that does have a global minimum at but changes sign infinitely often on both and for any Here's what I've done. By the product rule and chain rule, We will show that is undefined at which will show that has a critical point at Since and by the Squeeze Theorem, (by the Squeeze Theorem). We first prove that is neither a local minimum nor a local maximum of Notice that whenever is of the form We know this because whenever is of the form As well, whenever is of the form (as ). Let and Note that Then there exists such that and such that Thus, when we have that So there exist for some such that A similar proof for can be shown by using the opposite values of and Thus does not have a local maximum or minimum at By the product rule and chain rule, When When So, using a similar approach to above, with and we can show that does not have an inflection point when The proof for also involves using negative values for and Since We have that since Hence is a global minimum for Also, so it is indeed a minimum. Here is the proof: By the derivative definition, From above, and so since Now, We show that changes sign infinitely often on and for any We first show this result for . Whenever Whenever Let be the sequence defined by and let be the sequence defined by We have that So such that and such that Pick So for all and Since at every and at every we have that changes sign infinitely many times on . The proof that changes sign inifinitely often on is very similar. Notice that when and when We set and and obtain the same result as above. Edit: some notes: the work above can definitely be simplified. Also, I need to show that using the definition of a derivative and the Squeeze Theorem.","1. f(x) = x^2\sin(1/x) x\neq 0 f(0)=0. 0 f 2. g(x) = 2x^2+f(x). g 0, g'(x) (0,\epsilon) (-\epsilon,0) \epsilon >0. 1. f'(x) = 2x\sin(1/x)-\cos(1/x). f'(0)=0 x=0, f x=0. f'(0)=\lim\limits_{h\to 0}\dfrac{f(h)}{h}=\lim\limits_{h\to 0}\dfrac{h^2\sin(1/h)}{h}\\
=\lim\limits_{h\to0} h\sin(1/h)=\lim\limits. -h\leq h\sin(1/h)\leq h\;\forall h\geq 0 h\leq h\sin(1/h)\leq -h\;\forall h\leq 0, \lim\limits_{h\to0}h\sin(1/h)=0=\lim\limits_{h\to 0}h=0. (0,0) f. x \dfrac{1}{-\frac\pi2+2n\pi},n\in\mathbb{Z}, \sin(1/x)=-1. \sin(1/x)=-1 1/x -\frac\pi2+2n\pi,n\in\mathbb{Z}. \sin(1/x)=1 x \dfrac{1}{\frac\pi2+2n\pi},n\in\mathbb{Z}\Rightarrow f(x)=x^2>0 x\neq 0 x_n = \dfrac{1}{-\frac\pi2+2n\pi},n\in\mathbb{N} y_n = \dfrac{1}{\frac\pi2+2n\pi},n\in\mathbb{N}. \lim\limits_{n\to\infty}f(x_n)=\lim\limits_{n\to\infty} -x_n^2=\lim\limits_{n\to\infty} f(y_n)=\lim\limits_{n\to\infty} y_n^2=0. \forall \epsilon>0, N\in\mathbb{N} \forall n\geq N, |x_n|<\epsilon \exists N_2\in\mathbb{N} \forall n\geq N_2, |y_n|<\epsilon. N_3=\max\{N,N_2\}, -\epsilon < x_n,y_n<\epsilon. x_n,y_n \in (-\epsilon,\epsilon)\;\forall \epsilon >0 n\in\mathbb{N} f(x_n) = -x_n^2 < 0 <f(y_n) = y_n^2. x<0 x_n y_n. f x=0. f''(x)=2\sin(1/x)-\dfrac{2}{x}\cos(1/x)-\dfrac{\sin(1/x)}{x^2}. x=\dfrac{1}{2n\pi},n\in\mathbb{Z}\backslash\{0\},f''(x)=-4n\pi. x=\dfrac{1}{-\frac\pi2+2n\pi},n\in\mathbb{Z},f''(x)=-2+(-\frac\pi2+2n\pi)^2>0. x_n = \dfrac{1}{2n\pi} y_n=\dfrac{1}{-\frac\pi2+2n\pi},n\in\mathbb{N}, f x>0. x<0 x_n y_n. 2. f(0)=0, g(0)=2(0)^2+f(0)=0. \sin(1/x)\geq -1\;\forall x\in\mathbb{R}\backslash\{0\}, g(x)=2x^2+x^2\sin(1/x)\geq x^2>0\;\forall x\in\mathbb{R}\backslash \{0\}. (0,0) g. g'(0)=0, g'(0) =\lim\limits_{h\to 0}\dfrac{g(h)-g(0)}{h}\\
=\lim\limits_{h\to 0}\dfrac{2h^2+h^2\sin(1/h)}{h}. \lim\limits_{h\to 0}\dfrac{h^2\sin(1/h)}{h}=0, \lim\limits_{h\to 0}\dfrac{2h^2}{h}=\lim\limits_{h\to 0}2h=0,g'(0)=0. g'(x) = 4x+2x\sin(1/x)-\cos(1/x). g'(x) (-\epsilon,0) (0,\epsilon) \epsilon >0. (0,\epsilon) x=\dfrac{1}{n\pi},n\in\mathbb{N},g'(x)=\dfrac{4}{n\pi}+1>0. x=\dfrac{1}{2n\pi},n\in\mathbb{N},g'(x)=\dfrac{2}{n\pi}-1<0. (y_n)_{n=1}^\infty y_n=\dfrac{1}{2n\pi} (x_n)_{n=1}^\infty x_n=\dfrac{1}{n\pi}. \lim\limits_{n\to\infty} x_n = \lim\limits_{n\to\infty} y_n=0. \exists N_1\in \mathbb{N} n \geq N_1 \Rightarrow |x_n| < \epsilon, \forall \epsilon >0 \exists N_1\in\mathbb{N} n \geq N_2 \Rightarrow |y_n| < \epsilon, \forall \epsilon >0. N_4=\max\{N_1,N_2\}. n \geq N_4, 0< |y_n| = y_n <\epsilon 0<|x_n|=x_n<\epsilon. g'(y_n)<0 y_n g'(x_n)>0 x_n, g'(x) (0,\epsilon) g'(x) (-\epsilon,0) x=-\dfrac{1}{2n\pi},n\in\mathbb{N},g'(x)<0 x=-\dfrac{1}{n\pi},n\in\mathbb{N},n>1,g'(x)>0. y_{n_2} =-\dfrac{1}{2n_2\pi} x_{n_2}=-\dfrac{1}{n_2\pi},n_2\in\mathbb{N} f'(0)=0","['real-analysis', 'calculus']"
48,"Given the second derivatives of y and x wrt. a third variable, what is the second derivative of y wrt. x?","Given the second derivatives of y and x wrt. a third variable, what is the second derivative of y wrt. x?",,"I understand that if ${dy\over dt}$ and ${dx\over dt}$ are known, then ${dy\over dx}={{dy\over dt}\over {dx\over dt}}$ based on this logic: y' wrt x in terms of y' wrt t and x' wrt t I'm wondering how this relates to second derivatives: If ${d^2y\over dt^2}$ and ${d^2x\over dt^2}$ are known, what is ${d^2y\over dx^2}$ ? I tried using the same method as the one above, but got stuck at this step: y'' wrt. x in terms of y'' wrt. t and x'' wrt. t My guess is that the above expression is the same as ${{d^2y\over dt^2}\over {d^2x\over dt^2}}$ . If this is the case, is the result for third derivatives ${d^3y\over dx^3}={{d^3y\over dt^3}\over {d^3x\over dt^3}}$ , and so on? I tried googling it but couldn't find an answer.","I understand that if and are known, then based on this logic: y' wrt x in terms of y' wrt t and x' wrt t I'm wondering how this relates to second derivatives: If and are known, what is ? I tried using the same method as the one above, but got stuck at this step: y'' wrt. x in terms of y'' wrt. t and x'' wrt. t My guess is that the above expression is the same as . If this is the case, is the result for third derivatives , and so on? I tried googling it but couldn't find an answer.",{dy\over dt} {dx\over dt} {dy\over dx}={{dy\over dt}\over {dx\over dt}} {d^2y\over dt^2} {d^2x\over dt^2} {d^2y\over dx^2} {{d^2y\over dt^2}\over {d^2x\over dt^2}} {d^3y\over dx^3}={{d^3y\over dt^3}\over {d^3x\over dt^3}},['calculus']
49,How to understand this derivation - derivative of double dot product,How to understand this derivation - derivative of double dot product,,"I am looking at a derivation from a continuum mechanics book and I am not sure I understand how the author goes from step 2 to step 3.  The author goes like this: $\frac{\partial tr[\bar{\textbf{C}}]}{\partial \textbf{C}}$ $\frac{\partial [I_3^{-1/3}\textbf{C}:\textbf{I}]}{\partial \textbf{C}}$ $I_3^{-1/3}\textbf{I}-\frac{1}{3}I_3^{-4/3}I_3\textbf{C}^{-1}(\textbf{C}:\textbf{I})$ I would think it would be easier to go from the second step to: 3'. $\frac{\partial [I_3^{-1/3}I_1]}{\partial \textbf{C}}$ Because $\textbf{C}:\textbf{I} = tr[\textbf{C}] = I_1$ Is this a valid move? If I do that I end up at the same final answer as the author, but I want to make sure it isn't blind luck.  If we follow the author's approach, it looks like one needs to apply the product rule to the three terms that are each a function of $\textbf{C}$ but I am not sure how this works out.  For example, applying the product rule to the second step gives: 2'. $\frac{\partial I_3^{-1/3}}{\partial \textbf{C}}\textbf{C}:\textbf{I}+I_3^{-1/3}\frac{\partial \textbf{C}}{\partial \textbf{C}}:\textbf{I}+I_3^{-1/3}\textbf{C}:\frac{\partial \textbf{I}}{\partial \textbf{C}}$ The third term will be zero because the identity matrix is independent of $\textbf{C}$ .  But does the author's step 3 imply that $\frac{\partial \textbf{C}}{\partial \textbf{C}}:\textbf{I} = \textbf{I}$ ?  I thought that $\frac{\partial \textbf{C}}{\partial \textbf{C}}$ was the 4th order identity? The final expression is: $I_3^{-1/3}(\textbf{I}-\frac{1}{3}I_1\textbf{C}^{-1})$ So in summary, my questions are: Is my method (3') valid?  Do I understand correctly what the author's work implies about the inner product of a 4th order identity with a 2nd order identity being a second order identity? Thanks","I am looking at a derivation from a continuum mechanics book and I am not sure I understand how the author goes from step 2 to step 3.  The author goes like this: I would think it would be easier to go from the second step to: 3'. Because Is this a valid move? If I do that I end up at the same final answer as the author, but I want to make sure it isn't blind luck.  If we follow the author's approach, it looks like one needs to apply the product rule to the three terms that are each a function of but I am not sure how this works out.  For example, applying the product rule to the second step gives: 2'. The third term will be zero because the identity matrix is independent of .  But does the author's step 3 imply that ?  I thought that was the 4th order identity? The final expression is: So in summary, my questions are: Is my method (3') valid?  Do I understand correctly what the author's work implies about the inner product of a 4th order identity with a 2nd order identity being a second order identity? Thanks",\frac{\partial tr[\bar{\textbf{C}}]}{\partial \textbf{C}} \frac{\partial [I_3^{-1/3}\textbf{C}:\textbf{I}]}{\partial \textbf{C}} I_3^{-1/3}\textbf{I}-\frac{1}{3}I_3^{-4/3}I_3\textbf{C}^{-1}(\textbf{C}:\textbf{I}) \frac{\partial [I_3^{-1/3}I_1]}{\partial \textbf{C}} \textbf{C}:\textbf{I} = tr[\textbf{C}] = I_1 \textbf{C} \frac{\partial I_3^{-1/3}}{\partial \textbf{C}}\textbf{C}:\textbf{I}+I_3^{-1/3}\frac{\partial \textbf{C}}{\partial \textbf{C}}:\textbf{I}+I_3^{-1/3}\textbf{C}:\frac{\partial \textbf{I}}{\partial \textbf{C}} \textbf{C} \frac{\partial \textbf{C}}{\partial \textbf{C}}:\textbf{I} = \textbf{I} \frac{\partial \textbf{C}}{\partial \textbf{C}} I_3^{-1/3}(\textbf{I}-\frac{1}{3}I_1\textbf{C}^{-1}),"['derivatives', 'tensors']"
50,Using the Mean Value Theorem to Prove an Inequality involving the second derivative [closed],Using the Mean Value Theorem to Prove an Inequality involving the second derivative [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Consider the twice-differentiable function $f$ such that $f(0) = 0$ , $f(1/2) = 1/2$ , and $f'(0) = 0$ . Prove that $f''(x)$ is greater than or equal to 4 for some $x$ in the domain $[0, 1/2]$ . NOTE: Please don't just use one example of a possible function for $f$ and solve it using that example as I want to be able to generalize it to all cases. EDIT: Sorry if it wasn't clear - the question is asking to prove that the second derivative of $f(x)$ ( $f''(x)$ ) is greater than or equal to 4. I've tried solving for $f'(x)$ from $f(0)$ and $f(1/2)$ , and then trying to disprove a counterexample to the inequality we are given (trying to prove that $f''(x)$ cannot be less than 4), but that's as far as I've gotten. EDIT2: I've just realized that the question did not ask for all x, but only some x. This was a huge mistake on my part, so I'm extremely sorry to all those who have helped thus far.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Consider the twice-differentiable function such that , , and . Prove that is greater than or equal to 4 for some in the domain . NOTE: Please don't just use one example of a possible function for and solve it using that example as I want to be able to generalize it to all cases. EDIT: Sorry if it wasn't clear - the question is asking to prove that the second derivative of ( ) is greater than or equal to 4. I've tried solving for from and , and then trying to disprove a counterexample to the inequality we are given (trying to prove that cannot be less than 4), but that's as far as I've gotten. EDIT2: I've just realized that the question did not ask for all x, but only some x. This was a huge mistake on my part, so I'm extremely sorry to all those who have helped thus far.","f f(0) = 0 f(1/2) = 1/2 f'(0) = 0 f''(x) x [0, 1/2] f f(x) f''(x) f'(x) f(0) f(1/2) f''(x)","['calculus', 'derivatives']"
51,Derivative of eigenvalue of matrix with respect to its elements,Derivative of eigenvalue of matrix with respect to its elements,,"Assuming that matrix $A$ is positive semidefinite and that $\lambda$ denotes the eigenvalue, I would like to compute the following gradient $$\nabla_A \lambda(A)$$ I wanted to set this problem up as follows: $$\frac{ \partial \lambda(A)}{\partial A} = \frac{ \partial \lambda(A)}{\partial tr(A)} \frac{ \partial tr(A)}{\partial (A)} = I$$ However, I found different solutions online, namely : $$\partial \lambda(A) / \partial A_{ij} = (\mu \cdot b_i) (\mu \cdot b_j)$$ where $\mu$ is the associated eigenvector. What am I doing wrong? Thanks in advance for your help!","Assuming that matrix is positive semidefinite and that denotes the eigenvalue, I would like to compute the following gradient I wanted to set this problem up as follows: However, I found different solutions online, namely : where is the associated eigenvector. What am I doing wrong? Thanks in advance for your help!",A \lambda \nabla_A \lambda(A) \frac{ \partial \lambda(A)}{\partial A} = \frac{ \partial \lambda(A)}{\partial tr(A)} \frac{ \partial tr(A)}{\partial (A)} = I \partial \lambda(A) / \partial A_{ij} = (\mu \cdot b_i) (\mu \cdot b_j) \mu,"['matrices', 'derivatives', 'eigenvalues-eigenvectors', 'matrix-calculus', 'positive-semidefinite']"
52,"The directional derivative of a given function $w = f (x, y)$ at point $Po (1, 2) $",The directional derivative of a given function  at point,"w = f (x, y) Po (1, 2) ","The directional derivative of a given function $w = f (x, y)$ at point $Po (1, 2) $ in the direction toward $P1 (2, 3)$ is $2\sqrt2$ and in the direction toward $P2 (1, 0)$ is $(-3)$ . What is the value of $\frac{dw}{ds}$ at Po in the direction toward the origin? I don't understand what the question wants and how to solve, but I know that the topic is ""Directional Derivative"" so it contains partial derivatives and vectors, please tell me some tips and hints, where should I start?","The directional derivative of a given function at point in the direction toward is and in the direction toward is . What is the value of at Po in the direction toward the origin? I don't understand what the question wants and how to solve, but I know that the topic is ""Directional Derivative"" so it contains partial derivatives and vectors, please tell me some tips and hints, where should I start?","w = f (x, y) Po (1, 2)  P1 (2, 3) 2\sqrt2 P2 (1, 0) (-3) \frac{dw}{ds}","['calculus', 'derivatives', 'partial-differential-equations', 'vector-spaces', 'partial-derivative']"
53,How can I solve this rates of change problem?,How can I solve this rates of change problem?,,"How can I find the change in the surface area of this problem ? I tried finding the different areas of the cylinder. For its sides, I get: $$A_\mathrm{side}=2\pi rh,$$ for the half sphere: $$A_\mathrm{sphere}=2\pi r^3,$$ for the circle in the bottom $$A_\mathrm{circle}=\pi r^2,$$ for total surface area: $$S=2\pi rh+2\pi r^3+\pi r^2,$$ since $$\frac{dS}{dt}=\frac{\partial{S}}{\partial{r}}\frac{dr}{dt}+\frac{\partial{S}}{\partial{h}}\frac{dh}{dt}.$$ So: $$\frac{dS}{dt}=2\pi\left[-\frac{t}{2}(h+3r^2+r)+6rt\right]\label{1}\tag{1}$$ Given the values $$t=4;\:r(4)=4;\:h(4)=50,$$ when I evaluate \eqref{1} I get: $$\frac{dS}{dt}=-216\pi$$ while the correct answer is $$ \frac{dS}{dt}=-56\pi. $$ Now, what am I doing wrong?","How can I find the change in the surface area of this problem ? I tried finding the different areas of the cylinder. For its sides, I get: for the half sphere: for the circle in the bottom for total surface area: since So: Given the values when I evaluate \eqref{1} I get: while the correct answer is Now, what am I doing wrong?","A_\mathrm{side}=2\pi rh, A_\mathrm{sphere}=2\pi r^3, A_\mathrm{circle}=\pi r^2, S=2\pi rh+2\pi r^3+\pi r^2, \frac{dS}{dt}=\frac{\partial{S}}{\partial{r}}\frac{dr}{dt}+\frac{\partial{S}}{\partial{h}}\frac{dh}{dt}. \frac{dS}{dt}=2\pi\left[-\frac{t}{2}(h+3r^2+r)+6rt\right]\label{1}\tag{1} t=4;\:r(4)=4;\:h(4)=50, \frac{dS}{dt}=-216\pi 
\frac{dS}{dt}=-56\pi.
","['calculus', 'integration', 'derivatives', 'partial-differential-equations', 'partial-derivative']"
54,Derivative of Tr$(\rho\log\rho)$,Derivative of Tr,(\rho\log\rho),"The von Neumann entropy is the analogue of the Shannon entropy and is defined for positive semidefinite matrices as $$S = \text{Tr}(\rho\log\rho)$$ It is computed in terms of the eigenvalues $\lambda_i$ as $S = \sum_i\lambda_i\log\lambda_i$ where we set $0\log(0) = 0$ . Is $S$ differentiable with respect to $\rho$ ? The answer to this seems to be yes (see for example the question here and links therein) and is given as $S'(\rho) = I + \log\rho$ . If the result above is true, what happens when $\rho$ has at least one eigenvalue of zero? The von Neumann entropy is defined for positive semidefinite matrices while the log is only defined for positive definite matrices.","The von Neumann entropy is the analogue of the Shannon entropy and is defined for positive semidefinite matrices as It is computed in terms of the eigenvalues as where we set . Is differentiable with respect to ? The answer to this seems to be yes (see for example the question here and links therein) and is given as . If the result above is true, what happens when has at least one eigenvalue of zero? The von Neumann entropy is defined for positive semidefinite matrices while the log is only defined for positive definite matrices.",S = \text{Tr}(\rho\log\rho) \lambda_i S = \sum_i\lambda_i\log\lambda_i 0\log(0) = 0 S \rho S'(\rho) = I + \log\rho \rho,"['linear-algebra', 'derivatives', 'singularity']"
55,Derivatives of matrix norms,Derivatives of matrix norms,,"I am reading The Matrix Cookbook , which has the following  for the Frobenius norm $$ \frac{\partial }{\partial X} \| X\|_{F}^{2} = \frac{\partial}{\partial X}\textrm{Tr}(XX^H) = 2 X$$ If, in general, the norm $\| \cdot \|_{p,q}$ is given by $$ \| X \|_{p,q} =  \Bigg( \sum_{j=1}^{n} \Bigg( \sum_{i=1}^{m} |x_{ij}|^{p} \Bigg)^{\frac{q}{p}}  \Bigg)^{\frac{1}{q}} $$ how would you go about finding $\frac{\partial}{\partial X}\| X\|_{p,q} $ ?","I am reading The Matrix Cookbook , which has the following  for the Frobenius norm If, in general, the norm is given by how would you go about finding ?"," \frac{\partial }{\partial X} \| X\|_{F}^{2} = \frac{\partial}{\partial X}\textrm{Tr}(XX^H) = 2 X \| \cdot \|_{p,q}  \| X \|_{p,q} =  \Bigg( \sum_{j=1}^{n} \Bigg( \sum_{i=1}^{m} |x_{ij}|^{p} \Bigg)^{\frac{q}{p}}  \Bigg)^{\frac{1}{q}}  \frac{\partial}{\partial X}\| X\|_{p,q} ","['matrices', 'derivatives']"
56,A proof of Faa di Bruno's Formula,A proof of Faa di Bruno's Formula,,"I am writing a proof of Calculus which uses Faa di Bruno's formula to show that the composition of two analytic functions is analytic. I tried to prove the result with induction. The base case was easy, but I got stuck in the inductive step. This is my work for the inductive step: Define $h(x)=f(g(x))$ . Then $$\begin{align} h^{(n+1)}(x)&=\left(h^{(n)}(x)\right)'\\\\ &=\left(\sum\limits_{1k_1+2k_2+\cdots+nk_n=n}\frac{n!}{k_1!\cdots k_n!}f^{(k_1+\cdots+k_n)}(g(x))\prod\limits_{j=1}^n\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)'\\\\ &=\sum\limits_{1k_1+2k_2+\cdots+nk_n=n}\frac{n!}{k_1!\cdots k_n!}\left(f^{(k_1+\cdots+k_n)}(g(x))\prod\limits_{j=1}^n\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)'\\\\         &=\sum\limits_{1k_1+2k_2+\cdots+nk_n=n}\frac{n!}{k_1!\cdots k_n!}\left[\left(f^{(k_1+\cdots+k_n)}(g(x))\right)'\left(\prod\limits_{j=1}^n\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)\right.\\ &\left.\qquad\qquad+\left(\prod\limits_{j=1}^n\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)'\left(f^{(k_1+\cdots+k_n)}(g(x))\right)\right]\\\\ &=\sum\limits_{1k_1+2k_2+\cdots+nk_n=n}\frac{n!}{k_1!\cdots k_n!}\left[\left(f^{(k_1+\cdots+k_n+1)}(g(x))g'(x)\right)\left(\prod\limits_{j=1}^n\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)\right.\\\\ &\left.\qquad\qquad+\left(\sum\limits_{k=1}^{n} \left(\left(\frac{g^{(k)}(x)}{k!}\right)^{k_k}\right)'\prod\limits_{\substack{j=1\\j\ne k}}^{n} \left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)\left(f^{(k_1+\cdots+k_n)}(g(x))\right)\right]\\\\  &=\sum\limits_{1k_1+2k_2+\cdots+nk_n=n}\frac{n!}{k_1!\cdots k_n!}\left[\left(f^{(k_1+\cdots+k_n+1)}(g(x))g'(x)\right)\left(\prod\limits_{j=1}^n\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)\right.\\\\ &\left.\qquad\qquad+\left(\sum\limits_{k=1}^{n} k_k\left(\left(\frac{g^{(k)}(x)}{k!}\right)^{k_k-1}\left(\frac{g^{(k+1)}(x)}{k!}\right)\right)\prod\limits_{\substack{j=1\\j\ne k}}^{n} \left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)\left(f^{(k_1+\cdots+k_n)}(g(x))\right)\right]\\ &=\sum\limits_{1k_1+2k_2+\cdots+nk_n=n}\frac{n!}{k_1!\cdots k_n!}\left[\left(f^{(k_1+\cdots+k_n+1)}(g(x))g'(x)\right)\left(\prod\limits_{j=1}^n\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)\right.\\ &\left.\qquad\qquad+\left(\sum\limits_{k=1}^{n} k_k\left(\frac{g^{(k)}(x)}{k!}\right)^{k_k-1}\left(\frac{g^{(k+1)}(x)}{k!}\right)\prod\limits_{\substack{j=1\\j\ne k}}^{n} \left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)\left(f^{(k_1+\cdots+k_n)}(g(x))\right)\right]\\ \end{align}$$ I eventually want to simplify this expression to $$\sum\limits_{1k_1+2k_2+\cdots+nk_n+(n+1)k_{n+1}=n+1}\frac{(n+1)!}{k_1!\cdots k_n!k_{n+1}!}f^{(k_1+\cdots+k_n+k_{n+1})}(g(x))\prod\limits_{j=1}^{n+1}\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}$$ I noticed that the first term of the right hand side of my last step is equivalent to that simplified sum under the restriction that $k_1\ge 1$ except it is missing a factor of $n+1$ . The $nth$ term of the sum from $j=1$ to $n$ on the right hand side exactly matches the term on the simplified version where $k_{n+1}=1$ and the other $k_j's$ are $0$ . The remaining terms of the sum (i.e. the terms for $j=1$ to $j=n-1$ ) correspond to a relative increase in $k_{j+1}$ by $1$ and relative decrease in $k_j$ by $1$ , with the exception of a missing factor of $\frac{n+1}{(k+1)k_k}$ . I am not sure how to address these missing factors and particuticularly how to rewrite the terms from $j=1$ to $n-1$ of the sum of the right hand side. I tested my work for $n=2$ and the two expressions were equivalent. This makes me think my work is correct so far I just don't know how to proceed. Thanks, Andrew Murdza","I am writing a proof of Calculus which uses Faa di Bruno's formula to show that the composition of two analytic functions is analytic. I tried to prove the result with induction. The base case was easy, but I got stuck in the inductive step. This is my work for the inductive step: Define . Then I eventually want to simplify this expression to I noticed that the first term of the right hand side of my last step is equivalent to that simplified sum under the restriction that except it is missing a factor of . The term of the sum from to on the right hand side exactly matches the term on the simplified version where and the other are . The remaining terms of the sum (i.e. the terms for to ) correspond to a relative increase in by and relative decrease in by , with the exception of a missing factor of . I am not sure how to address these missing factors and particuticularly how to rewrite the terms from to of the sum of the right hand side. I tested my work for and the two expressions were equivalent. This makes me think my work is correct so far I just don't know how to proceed. Thanks, Andrew Murdza","h(x)=f(g(x)) \begin{align} h^{(n+1)}(x)&=\left(h^{(n)}(x)\right)'\\\\
&=\left(\sum\limits_{1k_1+2k_2+\cdots+nk_n=n}\frac{n!}{k_1!\cdots k_n!}f^{(k_1+\cdots+k_n)}(g(x))\prod\limits_{j=1}^n\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)'\\\\
&=\sum\limits_{1k_1+2k_2+\cdots+nk_n=n}\frac{n!}{k_1!\cdots k_n!}\left(f^{(k_1+\cdots+k_n)}(g(x))\prod\limits_{j=1}^n\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)'\\\\
        &=\sum\limits_{1k_1+2k_2+\cdots+nk_n=n}\frac{n!}{k_1!\cdots k_n!}\left[\left(f^{(k_1+\cdots+k_n)}(g(x))\right)'\left(\prod\limits_{j=1}^n\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)\right.\\
&\left.\qquad\qquad+\left(\prod\limits_{j=1}^n\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)'\left(f^{(k_1+\cdots+k_n)}(g(x))\right)\right]\\\\
&=\sum\limits_{1k_1+2k_2+\cdots+nk_n=n}\frac{n!}{k_1!\cdots k_n!}\left[\left(f^{(k_1+\cdots+k_n+1)}(g(x))g'(x)\right)\left(\prod\limits_{j=1}^n\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)\right.\\\\
&\left.\qquad\qquad+\left(\sum\limits_{k=1}^{n} \left(\left(\frac{g^{(k)}(x)}{k!}\right)^{k_k}\right)'\prod\limits_{\substack{j=1\\j\ne k}}^{n} \left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)\left(f^{(k_1+\cdots+k_n)}(g(x))\right)\right]\\\\ 
&=\sum\limits_{1k_1+2k_2+\cdots+nk_n=n}\frac{n!}{k_1!\cdots k_n!}\left[\left(f^{(k_1+\cdots+k_n+1)}(g(x))g'(x)\right)\left(\prod\limits_{j=1}^n\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)\right.\\\\
&\left.\qquad\qquad+\left(\sum\limits_{k=1}^{n} k_k\left(\left(\frac{g^{(k)}(x)}{k!}\right)^{k_k-1}\left(\frac{g^{(k+1)}(x)}{k!}\right)\right)\prod\limits_{\substack{j=1\\j\ne k}}^{n} \left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)\left(f^{(k_1+\cdots+k_n)}(g(x))\right)\right]\\
&=\sum\limits_{1k_1+2k_2+\cdots+nk_n=n}\frac{n!}{k_1!\cdots k_n!}\left[\left(f^{(k_1+\cdots+k_n+1)}(g(x))g'(x)\right)\left(\prod\limits_{j=1}^n\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)\right.\\
&\left.\qquad\qquad+\left(\sum\limits_{k=1}^{n} k_k\left(\frac{g^{(k)}(x)}{k!}\right)^{k_k-1}\left(\frac{g^{(k+1)}(x)}{k!}\right)\prod\limits_{\substack{j=1\\j\ne k}}^{n} \left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)\left(f^{(k_1+\cdots+k_n)}(g(x))\right)\right]\\
\end{align} \sum\limits_{1k_1+2k_2+\cdots+nk_n+(n+1)k_{n+1}=n+1}\frac{(n+1)!}{k_1!\cdots k_n!k_{n+1}!}f^{(k_1+\cdots+k_n+k_{n+1})}(g(x))\prod\limits_{j=1}^{n+1}\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j} k_1\ge 1 n+1 nth j=1 n k_{n+1}=1 k_j's 0 j=1 j=n-1 k_{j+1} 1 k_j 1 \frac{n+1}{(k+1)k_k} j=1 n-1 n=2","['real-analysis', 'derivatives', 'chain-rule']"
57,derivative of quaternion product,derivative of quaternion product,,"I want to calculate the derivative of quaternion product. Say $p$ and $q$ are unit quaternions. And I want to calculate $\frac{\partial p\bigotimes q}{\partial q}$ . From one reference Quaternion kinematics for the error-state Kalman filter I found that I can calculate this by converting p to a left-quaternion-product matrix: $p\bigotimes q=[p]_{R}q$ Then, the partial derivative is just $[p]_{R}$ . My question is that what if I calculate this by the definition of partial derivative like the following $$\frac{\partial p\bigotimes q}{\partial q} =\frac{p\bigotimes q\bigotimes dq\ominus p\bigotimes q}{dq} =\frac{q^{*}\bigotimes p^{*}\bigotimes p\bigotimes q\bigotimes dq}{dq} =\frac{dq}{dq} =I.$$ What is the problem of this derivation? Also I would like to ask what is the common way to calculate quaternion derivative. Just treat them as normal 4 by 1 vector and calculate vector derivative? Thanks","I want to calculate the derivative of quaternion product. Say and are unit quaternions. And I want to calculate . From one reference Quaternion kinematics for the error-state Kalman filter I found that I can calculate this by converting p to a left-quaternion-product matrix: Then, the partial derivative is just . My question is that what if I calculate this by the definition of partial derivative like the following What is the problem of this derivation? Also I would like to ask what is the common way to calculate quaternion derivative. Just treat them as normal 4 by 1 vector and calculate vector derivative? Thanks","p q \frac{\partial p\bigotimes q}{\partial q} p\bigotimes q=[p]_{R}q [p]_{R} \frac{\partial p\bigotimes q}{\partial q}
=\frac{p\bigotimes q\bigotimes dq\ominus p\bigotimes q}{dq}
=\frac{q^{*}\bigotimes p^{*}\bigotimes p\bigotimes q\bigotimes dq}{dq}
=\frac{dq}{dq}
=I.","['derivatives', 'quaternions']"
58,Matrix geometric sum with a unit eigenvalue,Matrix geometric sum with a unit eigenvalue,,"Let $A$ be a complex, square matrix, and define the geometric sum $$S = I+A+\cdots + A^{N-1}. \tag{1} $$ Just like in the scalar case, one can expand and see that $$(A-I)S =A^N-I, \tag{2} $$ and hence, provided that $A-I$ is invertible, we get $$S=(A-I)^{-1} (A^N-I) . \tag{3}$$ In the scalar case, the corresponding formula $$s=\frac{a^n-1}{a-1} , \tag{4}$$ exhibits a removable singularity at the point $a=1$ . That is, one can use the formula with $a \neq 1$ , in order to get the right value of $s$ (which is $s=n$ ) when $a=1$ using limits (e.g. l'Hôpital's rule, or even simpler --- the definition of the derivative of $a \mapsto a^n$ at the point $a=1$ ). My question is about similar results in the matrix case: The singularities in Equation $(3)$ occur whenever $\det(A-I) =0$ (that is whenever $A$ has $\lambda =1$ as an eigenvalue). Can one use Equation $(3)$ with $A-I$ non-singular, in order to get the right interpretation of Equation $(3)$ with $A-I$ singular? Is there some appropriate matrix analogue of l'Hôpital's rule? Any other limiting process? Thank you!","Let be a complex, square matrix, and define the geometric sum Just like in the scalar case, one can expand and see that and hence, provided that is invertible, we get In the scalar case, the corresponding formula exhibits a removable singularity at the point . That is, one can use the formula with , in order to get the right value of (which is ) when using limits (e.g. l'Hôpital's rule, or even simpler --- the definition of the derivative of at the point ). My question is about similar results in the matrix case: The singularities in Equation occur whenever (that is whenever has as an eigenvalue). Can one use Equation with non-singular, in order to get the right interpretation of Equation with singular? Is there some appropriate matrix analogue of l'Hôpital's rule? Any other limiting process? Thank you!","A S = I+A+\cdots + A^{N-1}. \tag{1}  (A-I)S =A^N-I, \tag{2}  A-I S=(A-I)^{-1} (A^N-I) . \tag{3} s=\frac{a^n-1}{a-1} , \tag{4} a=1 a \neq 1 s s=n a=1 a \mapsto a^n a=1 (3) \det(A-I) =0 A \lambda =1 (3) A-I (3) A-I","['linear-algebra', 'matrices', 'derivatives', 'matrix-calculus', 'indeterminate-forms']"
59,Derivative of Dirac delta function,Derivative of Dirac delta function,,"Is the relation of the Dirac delta function correct? $$ \frac{\partial}{\partial x''}\delta(x''-x') = -\frac{\partial}{\partial x'}\delta(x'-x'').\tag{1} $$ If it is, how to derive the above relation?","Is the relation of the Dirac delta function correct? If it is, how to derive the above relation?","
\frac{\partial}{\partial x''}\delta(x''-x')
= -\frac{\partial}{\partial x'}\delta(x'-x'').\tag{1}
","['derivatives', 'distribution-theory', 'dirac-delta']"
60,What am I doing wrong with this derivative? (Product rule & derivative of arctan),What am I doing wrong with this derivative? (Product rule & derivative of arctan),,"This is my first time posting here, so sorry if I don't get the formatting right! For my Calc 2 pretest, this was one of the questions: Let $f(x)=\mathrm{x^3}\!\cdot\!\mathrm{tan^{-1} (2x)}$ Find $f'(x)$ Here is what I did to attempt to solve the problem. First, applying the product rule: $$f'(x)=\mathrm{x^3}\!\cdot\!\mathrm{\frac{d}{dx}[tan^{-1}(2x)]} + \mathrm{tan^{-1}(2x)}\!\cdot\!\mathrm{\frac{d}{dx}(x^3)}$$ \mathrm{u}!\cdot!\mathrm{v} Second, I am applying the chain rule and the derivative of $tan^{-1}(x)$ to simplify the first addend, and then applying the power rule to find the derivative of $x^3$ . $$f'(x)=\mathrm{x^3}\!\cdot\!\mathrm{\frac1{1+(2x)^2}}\!\cdot\!\mathrm{\frac{d}{dx}[2x]} + \mathrm{tan^{-1}(2x)}\!\cdot\!\mathrm{3x^2}$$ Next up, using the power rule to find the derivative of $2x$ and then the calculus part will be done: $$f'(x)=\mathrm{x^3}\!\cdot\!\mathrm{\frac1{1+(2x)^2}}\!\cdot\!\mathrm{2} + \mathrm{tan^{-1}(2x)}\!\cdot\!\mathrm{3x^2}$$ Now I simplify and get that $f'(x)=$ $$\frac{2x^3}{1+4x^2}+3x^2tan^{-1}(2x)$$ But the problem is this wasn't one of the answers! I did it again and again and kept getting this answer, but no matter what I tried I couldn't manipulate the above expression to match one of my multiple choice answers. I ended up guessing wrong and losing points on the pretest, but I am not able to see what the correct answer is. Hopefully someone can tell me where I went wrong!","This is my first time posting here, so sorry if I don't get the formatting right! For my Calc 2 pretest, this was one of the questions: Let Find Here is what I did to attempt to solve the problem. First, applying the product rule: \mathrm{u}!\cdot!\mathrm{v} Second, I am applying the chain rule and the derivative of to simplify the first addend, and then applying the power rule to find the derivative of . Next up, using the power rule to find the derivative of and then the calculus part will be done: Now I simplify and get that But the problem is this wasn't one of the answers! I did it again and again and kept getting this answer, but no matter what I tried I couldn't manipulate the above expression to match one of my multiple choice answers. I ended up guessing wrong and losing points on the pretest, but I am not able to see what the correct answer is. Hopefully someone can tell me where I went wrong!",f(x)=\mathrm{x^3}\!\cdot\!\mathrm{tan^{-1} (2x)} f'(x) f'(x)=\mathrm{x^3}\!\cdot\!\mathrm{\frac{d}{dx}[tan^{-1}(2x)]} + \mathrm{tan^{-1}(2x)}\!\cdot\!\mathrm{\frac{d}{dx}(x^3)} tan^{-1}(x) x^3 f'(x)=\mathrm{x^3}\!\cdot\!\mathrm{\frac1{1+(2x)^2}}\!\cdot\!\mathrm{\frac{d}{dx}[2x]} + \mathrm{tan^{-1}(2x)}\!\cdot\!\mathrm{3x^2} 2x f'(x)=\mathrm{x^3}\!\cdot\!\mathrm{\frac1{1+(2x)^2}}\!\cdot\!\mathrm{2} + \mathrm{tan^{-1}(2x)}\!\cdot\!\mathrm{3x^2} f'(x)= \frac{2x^3}{1+4x^2}+3x^2tan^{-1}(2x),"['calculus', 'derivatives']"
61,Shock and mass conservation law derivation (Rankine-Hugoniot),Shock and mass conservation law derivation (Rankine-Hugoniot),,"Suppose we are looking at the non-linear system $u_t+uu_x=0$ . Some of the waves from this system are drawn below where I included $a$ and $b$ as interval-bounds around where the most happens. There is a vertical green line drawn at some point of the shock wave such that the area in yellow and orange are equal, this line is called $\sigma(t)$ . The three points where $\sigma(t)$ intersects $u(t,x)$ are $u_+(t),u_0(t)$ and $u_-(t)$ from up to down respectively. We define $$M(t)=M_{a,b}(t)=\int_a^bu(t,x)dx,$$ the area enclosed by the shock wave, the $t$ -axis and the vertival lines $x=a$ and $x=b$ ; from this $M(t)$ we have already derived a law that $$\frac{dM}{dt}=\frac{1}{2}\left(u(t,a)^2-u(t,b)^2\right).$$ We also now define $u(0,x)=f(x)$ and $x=X(t,u)=g(u)+tu$ , so $X(0,u)=g(u)$ (and $u=u(t,x)$ ). The goal is to show that the following expression for $M(t)$ also satisfies the law for $\frac{dM}{dt}$ as written above: $$M(t)=\int_{u(t,b)}^{u(t,a)}\left(X(t,u)-a\right)du+(b-a)u(t,b).$$ I find this latter expression for $M$ very disturbing and do not know exactly how to differentiate this w.r.t. $t$ . The Leibniz integral rule would probably be the way to go, but I do not see how we could get $\frac{dM}{dt}=\frac{1}{2}\left(u(t,a)^2-u(t,b)^2\right)$ out of this. My problem is that there is a $du$ after the integral and the Leibniz rule is for $\frac{d}{dx}\int u(x,t) dt$ or something. Suggestions and hints for a proper approach to this is very much appreciated. Thanks for the time and help! I tried, but failed miserably: $$\frac{d}{dt}\int_{u(t,b)}^{u(t,a)}\left(X(t,u)-a\right)du+(b-a)u(t,b)=\int_{\frac{du(t,b)}{dt}}^{\frac{du(t,a)}{dt}}\frac{dX(t,u)}{dt}\frac{du}{dt}+(b-a)\frac{du(t,b)}{dt}?$$ Edit:(why did I not use chain rule anywhere? I have no idea how to properly differentiate this integral.)","Suppose we are looking at the non-linear system . Some of the waves from this system are drawn below where I included and as interval-bounds around where the most happens. There is a vertical green line drawn at some point of the shock wave such that the area in yellow and orange are equal, this line is called . The three points where intersects are and from up to down respectively. We define the area enclosed by the shock wave, the -axis and the vertival lines and ; from this we have already derived a law that We also now define and , so (and ). The goal is to show that the following expression for also satisfies the law for as written above: I find this latter expression for very disturbing and do not know exactly how to differentiate this w.r.t. . The Leibniz integral rule would probably be the way to go, but I do not see how we could get out of this. My problem is that there is a after the integral and the Leibniz rule is for or something. Suggestions and hints for a proper approach to this is very much appreciated. Thanks for the time and help! I tried, but failed miserably: Edit:(why did I not use chain rule anywhere? I have no idea how to properly differentiate this integral.)","u_t+uu_x=0 a b \sigma(t) \sigma(t) u(t,x) u_+(t),u_0(t) u_-(t) M(t)=M_{a,b}(t)=\int_a^bu(t,x)dx, t x=a x=b M(t) \frac{dM}{dt}=\frac{1}{2}\left(u(t,a)^2-u(t,b)^2\right). u(0,x)=f(x) x=X(t,u)=g(u)+tu X(0,u)=g(u) u=u(t,x) M(t) \frac{dM}{dt} M(t)=\int_{u(t,b)}^{u(t,a)}\left(X(t,u)-a\right)du+(b-a)u(t,b). M t \frac{dM}{dt}=\frac{1}{2}\left(u(t,a)^2-u(t,b)^2\right) du \frac{d}{dx}\int u(x,t) dt \frac{d}{dt}\int_{u(t,b)}^{u(t,a)}\left(X(t,u)-a\right)du+(b-a)u(t,b)=\int_{\frac{du(t,b)}{dt}}^{\frac{du(t,a)}{dt}}\frac{dX(t,u)}{dt}\frac{du}{dt}+(b-a)\frac{du(t,b)}{dt}?","['integration', 'derivatives', 'partial-differential-equations', 'wave-equation']"
62,A rigorous yet intuitive summary of inflection and critical points for beginning calculus?,A rigorous yet intuitive summary of inflection and critical points for beginning calculus?,,"I haven't done these in awhile. While my analysis covered continuity but not differentiability, I have so far not revisited these in learning geometry or algebra. I am trying to help a calculus student, so any notion of 'interior' is intuitive. First, please verify if these are correct. I think I may be missing phrases like 'open neighbourhood' or 'open interval' and might have confused my ifs and only ifs. Some collections like Wikipedia, are mixed for both higher maths and beginning calculus. Let $A, B \subseteq \mathbb R$ . Definition of critical point : A function $f: A \to B$ has a critical point at $x \in A$ (I represent the point $(x,f(x))$ by the real number $x$ ) if (a) $x$ is in the interior of $A$ and (b) $f'(x)$ is undefined or $f'(x)=0$ . 1.1. Example to illustrate the 'interior' part of Definition (1) : The function $f: A \to \mathbb R, f(t)=t^2$ does not have a critical point at $x=0$ if $A \in \{(-\infty,0], [-7,-3) \cup [-2,0], [0,4], [-8,-6) \cup \{0\} \cup (2,10),\{0\},\{0,1\} \}$ ( rigorously, all these $A$ 's are given the subspace topology...or the topology of $\mathbb R$ ...not sure...which is the right one? ). 1.2. An example for the 'undefined' part of Definition (1) $f:\mathbb R \to \mathbb R, f(t)=|t|$ Definition of inflection point : A function $f: A \to B$ has an inflection point at $(x,f(x))$ , where $x \in A$ may be at the boundary of $A$ or the interior of $A$ but must be in $A$ (I believe this is both intuitive and rigorous: A point in a set is either in the set's interior or the set's boundary), if $f$ has a tangent line at $(x,f(x))$ and the concavity of $f$ changes at $(x,f(x))$ . 2.1. Equivalent definition of inflection point : (This is my attempt to try to rigorously explain 'has a tangent line' while incorporating tangent lines with infinite slope like in this example with $f(t)=t^{1/3}$ ) A function $f: A \to B$ has an inflection point at $(x,f(x))$ if $f'$ exists in $C$ , the intersection of the whole of $A$ and not just $A$ 's interior with some open interval $(a,b)$ ( $x \in C = A \cap (a,b)$ ) where (a) $f'$ is increasing on $(\inf C,x)$ and not increasing (constant, decreasing or does not exist) on $(x,\sup C)$ or (b) $f'$ is decreasing on $(\inf C,x)$ and not decreasing (constant, increasing or does not exist) on $(x,\sup C)$ . Equivalent definition of an inflection point when $x$ is an interior point (for the previous example of $f(t)=t^{1/3}$ , I assume the concern is that $x=0$ is not an interior point): A function $f: A \to B$ has an inflection point at $x$ , where $x$ is an interior point of $A$ , if $f'$ exists in some open interval $(a,b)$ that both contains $x$ and is contained in $A$ ( $x \in (a,b) \subseteq A$ , and $(a,b)$ is contained in $A$ by the definition of $x$ as an interior point of $A$ ), where (a) $f'$ is increasing on $(a,x)$ and decreasing on $(x,b)$ or (b) $f'$ is decreasing on $(a,x)$ and increasing on $(x,b)$ . Consequence of the definition of inflection : A function $f: A \to B$ has an inflection point at $x \in A$ only if $f''(x)$ is undefined or $f''(x)=0$ . 4.1. An example of Consequence (4) for the 'undefined' part is based precisely on Example (1.2) $f: \mathbb R \to \mathbb R,   f(t) = \begin{cases}     -\frac{t^2}{2} &\text{if $t < 0$} \\     \frac{t^2}{2} &\text{if $t \geq 0$.}   \end{cases}, f'(t)=|t| $ . Observation : The 'undefined or zero part' of Consequence (4) is just like in Definition (1) except (4) is not a definition for inflection. 5.1. A counterexample to the converse of Consequence (4) for $f''(x)=0$ is $f: \mathbb R \to \mathbb R, f(t)=\frac{t^4}{12},f''(t)=t^2$ , where $x=0$ is an undulation point rather than an inflection point. 5.2. A counterexample to the converse of Consequence (4): When is $f''(x)$ undefined but $x$ not an inflection point of $f$? Proposition : A differentiable function $f: A \to B$ has an inflection point at an interior point $(x,f(x))$ only if $f'$ has a critical point at $(x,f(x))$ . Second , I use the above to rigorously (as rigorously as possible for calculus students) answer as follows. Please verify. If an answer or argument (such as if something in the first part above is wrong) is incorrect, then please give the corresponding correct answer or argument. : Find critical points and inflection points given the derivative (also check for consistency with Darboux's theorem)? I am splitting this up to not cover a lot in one post.","I haven't done these in awhile. While my analysis covered continuity but not differentiability, I have so far not revisited these in learning geometry or algebra. I am trying to help a calculus student, so any notion of 'interior' is intuitive. First, please verify if these are correct. I think I may be missing phrases like 'open neighbourhood' or 'open interval' and might have confused my ifs and only ifs. Some collections like Wikipedia, are mixed for both higher maths and beginning calculus. Let . Definition of critical point : A function has a critical point at (I represent the point by the real number ) if (a) is in the interior of and (b) is undefined or . 1.1. Example to illustrate the 'interior' part of Definition (1) : The function does not have a critical point at if ( rigorously, all these 's are given the subspace topology...or the topology of ...not sure...which is the right one? ). 1.2. An example for the 'undefined' part of Definition (1) Definition of inflection point : A function has an inflection point at , where may be at the boundary of or the interior of but must be in (I believe this is both intuitive and rigorous: A point in a set is either in the set's interior or the set's boundary), if has a tangent line at and the concavity of changes at . 2.1. Equivalent definition of inflection point : (This is my attempt to try to rigorously explain 'has a tangent line' while incorporating tangent lines with infinite slope like in this example with ) A function has an inflection point at if exists in , the intersection of the whole of and not just 's interior with some open interval ( ) where (a) is increasing on and not increasing (constant, decreasing or does not exist) on or (b) is decreasing on and not decreasing (constant, increasing or does not exist) on . Equivalent definition of an inflection point when is an interior point (for the previous example of , I assume the concern is that is not an interior point): A function has an inflection point at , where is an interior point of , if exists in some open interval that both contains and is contained in ( , and is contained in by the definition of as an interior point of ), where (a) is increasing on and decreasing on or (b) is decreasing on and increasing on . Consequence of the definition of inflection : A function has an inflection point at only if is undefined or . 4.1. An example of Consequence (4) for the 'undefined' part is based precisely on Example (1.2) . Observation : The 'undefined or zero part' of Consequence (4) is just like in Definition (1) except (4) is not a definition for inflection. 5.1. A counterexample to the converse of Consequence (4) for is , where is an undulation point rather than an inflection point. 5.2. A counterexample to the converse of Consequence (4): When is $f''(x)$ undefined but $x$ not an inflection point of $f$? Proposition : A differentiable function has an inflection point at an interior point only if has a critical point at . Second , I use the above to rigorously (as rigorously as possible for calculus students) answer as follows. Please verify. If an answer or argument (such as if something in the first part above is wrong) is incorrect, then please give the corresponding correct answer or argument. : Find critical points and inflection points given the derivative (also check for consistency with Darboux's theorem)? I am splitting this up to not cover a lot in one post.","A, B \subseteq \mathbb R f: A \to B x \in A (x,f(x)) x x A f'(x) f'(x)=0 f: A \to \mathbb R, f(t)=t^2 x=0 A \in \{(-\infty,0], [-7,-3) \cup [-2,0], [0,4], [-8,-6) \cup \{0\} \cup (2,10),\{0\},\{0,1\} \} A \mathbb R f:\mathbb R \to \mathbb R, f(t)=|t| f: A \to B (x,f(x)) x \in A A A A f (x,f(x)) f (x,f(x)) f(t)=t^{1/3} f: A \to B (x,f(x)) f' C A A (a,b) x \in C = A \cap (a,b) f' (\inf C,x) (x,\sup C) f' (\inf C,x) (x,\sup C) x f(t)=t^{1/3} x=0 f: A \to B x x A f' (a,b) x A x \in (a,b) \subseteq A (a,b) A x A f' (a,x) (x,b) f' (a,x) (x,b) f: A \to B x \in A f''(x) f''(x)=0 f: \mathbb R \to \mathbb R,
  f(t) = \begin{cases}
    -\frac{t^2}{2} &\text{if t < 0} \\
    \frac{t^2}{2} &\text{if t \geq 0.}
  \end{cases}, f'(t)=|t|
 f''(x)=0 f: \mathbb R \to \mathbb R, f(t)=\frac{t^4}{12},f''(t)=t^2 x=0 f: A \to B (x,f(x)) f' (x,f(x))","['real-analysis', 'calculus']"
63,What precisely does the notation $\frac {\partial h} {\partial z}$ mean in this context?,What precisely does the notation  mean in this context?,\frac {\partial h} {\partial z},"I am given the following problem. Let $Ω\subset \mathbb{C} $ and let $h \in C^1(Ω)$ be such that $\frac {\partial h} {\partial z}=0$ . Show that $h(z)=\overline {f(z)}$ for some $f$ analytic in $Ω$ . The $\frac {\partial h} {\partial z}$ notation is mysterious to me. But, after looking at this , I have a guess as to what it means. Write $h(x+iy)=u(x,y)+iv(x,y).$ Then $\frac {\partial h} {\partial z}=\frac {[u_x+iv_x]-i[u_y+iv_y]} 2=\frac {u_x+v_y+iv_x-iu_y} 2$ . Is this interpretation correct?","I am given the following problem. Let and let be such that . Show that for some analytic in . The notation is mysterious to me. But, after looking at this , I have a guess as to what it means. Write Then . Is this interpretation correct?","Ω\subset \mathbb{C}  h \in C^1(Ω) \frac {\partial h} {\partial z}=0 h(z)=\overline {f(z)} f Ω \frac {\partial h} {\partial z} h(x+iy)=u(x,y)+iv(x,y). \frac {\partial h} {\partial z}=\frac {[u_x+iv_x]-i[u_y+iv_y]} 2=\frac {u_x+v_y+iv_x-iu_y} 2","['complex-analysis', 'derivatives', 'notation', 'partial-derivative']"
64,Function has nth derivatives bounded by exponential,Function has nth derivatives bounded by exponential,,"Does there exist an infinitely differentiable function $f: (0,\infty) \to \mathbb{R}$ , other than a constant multiple of $e^{-x}$ , satisfying $|f^{(n)}(x)| \leq e^{-x}$ for all $n$ , $x$ ? Some counterexamples we have excluded are $e^{-kx}$ for $k \not = 1$ (if $k < 1$ then the inequality fails for large $x$ , if $k > 1$ then the inequality fails for large $n$ ) and the function $\frac{1}{1+e^{x}}$ (the higher derivatives blow up around $x=0$ ). We have tried using the Laplace transform to write $f(x) = \int_0^{\infty} g(t)e^{-tx}dt$ , which shows we should expect higher derivatives to blow up if $f$ is not $e^{-x}$ but since $g$ need not be nonnegative we can't get anything concrete this way.","Does there exist an infinitely differentiable function , other than a constant multiple of , satisfying for all , ? Some counterexamples we have excluded are for (if then the inequality fails for large , if then the inequality fails for large ) and the function (the higher derivatives blow up around ). We have tried using the Laplace transform to write , which shows we should expect higher derivatives to blow up if is not but since need not be nonnegative we can't get anything concrete this way.","f: (0,\infty) \to \mathbb{R} e^{-x} |f^{(n)}(x)| \leq e^{-x} n x e^{-kx} k \not = 1 k < 1 x k > 1 n \frac{1}{1+e^{x}} x=0 f(x) = \int_0^{\infty} g(t)e^{-tx}dt f e^{-x} g","['real-analysis', 'derivatives', 'exponential-function']"
65,Commutator of the Hamiltonian and Parity Operator - evaluation of derivatives,Commutator of the Hamiltonian and Parity Operator - evaluation of derivatives,,"I was studying the commutator of the Hamiltonian and parity operators in the $L^2$ space from Quantum mechanics and came upon the following: To show that the two operators commute, assuming we have a symmetric potential $V(x)=V(-x)$ , we had that the commutator of a function $\psi(x)$ was; \begin{align*}[\hat{H},\hat{P}]=&-\frac{\hbar^2}{2m}\frac{\partial^2}{\partial x^2}\psi(-x)+V(x)\psi(-x)+\frac{\hbar^2}{2m}\frac{\partial^2}{\partial (-x)^2}\psi(-x)-V(-x)\psi(-x)\\=&-\frac{\hbar^2}{2m}\frac{\partial^2}{\partial x^2}\psi(-x)+\frac{\hbar^2}{2m}\frac{\partial^2}{\partial (-x)^2}\psi(-x), \end{align*} assuming the symmetry of the potential $V(x)$ . Now I struggled to show that the two second derivative terms are the negative of each other. The only possible way I could deduce the were equal was by using the chain rule for the second partial derivative to deduce that: $$\frac{\partial \psi(-x)}{\partial (-x)}=\frac{\partial \psi(-x)}{\partial x}\frac{\partial x}{\partial (-x)}=-\frac{\partial \psi(-x)}{\partial x},$$ and hence the partial derivative terms both cancel out to show the operators commute. Now my main issue is struggling with the derivatives, could this be clarified? As an aside I also had thought about the Lagrange and Leibniz notation, and was wondering whether the following was incorrect or not? $$\frac{\partial \psi(-x)}{\partial x}=-\psi '(-x) \,\text{ and }\, \frac{\partial \psi(-x)}{\partial (-x)}=\psi'(-x).$$","I was studying the commutator of the Hamiltonian and parity operators in the space from Quantum mechanics and came upon the following: To show that the two operators commute, assuming we have a symmetric potential , we had that the commutator of a function was; assuming the symmetry of the potential . Now I struggled to show that the two second derivative terms are the negative of each other. The only possible way I could deduce the were equal was by using the chain rule for the second partial derivative to deduce that: and hence the partial derivative terms both cancel out to show the operators commute. Now my main issue is struggling with the derivatives, could this be clarified? As an aside I also had thought about the Lagrange and Leibniz notation, and was wondering whether the following was incorrect or not?","L^2 V(x)=V(-x) \psi(x) \begin{align*}[\hat{H},\hat{P}]=&-\frac{\hbar^2}{2m}\frac{\partial^2}{\partial x^2}\psi(-x)+V(x)\psi(-x)+\frac{\hbar^2}{2m}\frac{\partial^2}{\partial (-x)^2}\psi(-x)-V(-x)\psi(-x)\\=&-\frac{\hbar^2}{2m}\frac{\partial^2}{\partial x^2}\psi(-x)+\frac{\hbar^2}{2m}\frac{\partial^2}{\partial (-x)^2}\psi(-x),
\end{align*} V(x) \frac{\partial \psi(-x)}{\partial (-x)}=\frac{\partial \psi(-x)}{\partial x}\frac{\partial x}{\partial (-x)}=-\frac{\partial \psi(-x)}{\partial x}, \frac{\partial \psi(-x)}{\partial x}=-\psi '(-x) \,\text{ and }\, \frac{\partial \psi(-x)}{\partial (-x)}=\psi'(-x).","['derivatives', 'quantum-mechanics']"
66,Squeezing function defined over infinite range down to finite,Squeezing function defined over infinite range down to finite,,"Abstract What I'm generally trying to do is to squeeze the first derivative of the sigmoid function defined over infinite range into finite. I.e. $$f(x)=\frac{1}{1+e^{-x}}$$ This function is heavily used as an activation function for neural networks. It's integral has close relation to another relu activation function which is also heavily used with neural networks: $$\int{f(x)}{dx}=ln(1+e^{-x})-ln(e^{-x})$$ It's derivative is useful in modeling and is similar to normal distribution pdf function: $$\frac{d}{dx}f(x)=\frac{e^{-x}}{(1+e^{-x})^{2}}=df(x)$$ It is also simple to evaluate for computer, it's derivative is expressed in terms of function itself. But $df(x)$ is defined over infinite range which makes it unsuitable for some modeling tasks where the range should be finite. Taking into account the fact that the most of the values close to zero are located outside [-10..10] range, this is shown by the following equation: $$\int_{-\infty}^{\infty}{df(x)}{dx}-\int_{-10}^{10}{df(x)}{dx}=1-\frac{e^{10}-e^{-10}}{(1+e^{10})(1+e^{-10})}=0.00009079573740486882...$$ That means that all of the values outside [-10..10] would introduce minor effect to the shape of function when it's range is squeezed from infinite to finite. So how do we squeeze the range? It can be done through the use of $arctanh(x)$ : That goes as an argument to $df$ function. So when approaching +/-1 arctanh would asymptotically approach +/- $\infty$ thus when used as an argument it will map the argument range from infinite to finite: $$df(arctanh(x))=\frac{\sqrt{1-x^2}}{(1+\frac{\sqrt{1-x^2}}{x+1})^2(x+1)}$$ while the function doesn't even look similar to what was originally intended it is easy to get the shape close through a factor before arctanh(x): $$df(arctanh(x)*6)$$ yields to: thus getting the final equation equal to: $$df(n*arctanh(x))=\frac{(1-x^2)^{\frac n2}}{(1+\frac{(1-x^{2})^{\frac n2}}{(x+1)^n})^2(x+1)^n}=F(x,n).$$ It's interesting to see how $4*F(x,\pi)$ is similar to $\frac{sin(x\pi+\frac{\pi}{2})}{2}+\frac 12$ : Which could essentially serve as yet another way to approximate sine. Problem While playing with $F(x,n)$ I've noticed that it's hard to find values of $n$ that suffice some condition, for instance I want to find $n$ such that it best approximates $\frac{sin(x\pi+\frac{\pi}{2})}{2}+\frac 12$ . Let's define an error function of an approximation: $$err(x, n) = (4·df(n·arctanh(x))-\frac 12·sin(x\pi+\frac{\pi}{2})-\frac 12)^2$$ or $$err(x, n) = (\frac{(1-x^2)^{\frac n2}}{(1+\frac{(1-x^{2})^{\frac n2}}{(x+1)^n})^2(x+1)^n}-(\frac{sin(x\pi+\frac{\pi}{2})}{2}+\frac 12))^2.$$ Then we would need to find $n$ such that: $$\frac{d}{dn}\int_{-1}^{1}{err(x, n)}{dx}=0.$$ Please note that $n$ is real here This is where things start getting messy, an integral over $F(x, n)$ doesn't seem to have an analytical (closed) form nor it is clear how to approach differentiation of such integral due to different integration and differentiation variables. I don't understand how to differentiate such anintegral, so thought it would be better to ask here? Thank you in advance!","Abstract What I'm generally trying to do is to squeeze the first derivative of the sigmoid function defined over infinite range into finite. I.e. This function is heavily used as an activation function for neural networks. It's integral has close relation to another relu activation function which is also heavily used with neural networks: It's derivative is useful in modeling and is similar to normal distribution pdf function: It is also simple to evaluate for computer, it's derivative is expressed in terms of function itself. But is defined over infinite range which makes it unsuitable for some modeling tasks where the range should be finite. Taking into account the fact that the most of the values close to zero are located outside [-10..10] range, this is shown by the following equation: That means that all of the values outside [-10..10] would introduce minor effect to the shape of function when it's range is squeezed from infinite to finite. So how do we squeeze the range? It can be done through the use of : That goes as an argument to function. So when approaching +/-1 arctanh would asymptotically approach +/- thus when used as an argument it will map the argument range from infinite to finite: while the function doesn't even look similar to what was originally intended it is easy to get the shape close through a factor before arctanh(x): yields to: thus getting the final equation equal to: It's interesting to see how is similar to : Which could essentially serve as yet another way to approximate sine. Problem While playing with I've noticed that it's hard to find values of that suffice some condition, for instance I want to find such that it best approximates . Let's define an error function of an approximation: or Then we would need to find such that: Please note that is real here This is where things start getting messy, an integral over doesn't seem to have an analytical (closed) form nor it is clear how to approach differentiation of such integral due to different integration and differentiation variables. I don't understand how to differentiate such anintegral, so thought it would be better to ask here? Thank you in advance!","f(x)=\frac{1}{1+e^{-x}} \int{f(x)}{dx}=ln(1+e^{-x})-ln(e^{-x}) \frac{d}{dx}f(x)=\frac{e^{-x}}{(1+e^{-x})^{2}}=df(x) df(x) \int_{-\infty}^{\infty}{df(x)}{dx}-\int_{-10}^{10}{df(x)}{dx}=1-\frac{e^{10}-e^{-10}}{(1+e^{10})(1+e^{-10})}=0.00009079573740486882... arctanh(x) df \infty df(arctanh(x))=\frac{\sqrt{1-x^2}}{(1+\frac{\sqrt{1-x^2}}{x+1})^2(x+1)} df(arctanh(x)*6) df(n*arctanh(x))=\frac{(1-x^2)^{\frac n2}}{(1+\frac{(1-x^{2})^{\frac n2}}{(x+1)^n})^2(x+1)^n}=F(x,n). 4*F(x,\pi) \frac{sin(x\pi+\frac{\pi}{2})}{2}+\frac 12 F(x,n) n n \frac{sin(x\pi+\frac{\pi}{2})}{2}+\frac 12 err(x, n) = (4·df(n·arctanh(x))-\frac 12·sin(x\pi+\frac{\pi}{2})-\frac 12)^2 err(x, n) = (\frac{(1-x^2)^{\frac n2}}{(1+\frac{(1-x^{2})^{\frac n2}}{(x+1)^n})^2(x+1)^n}-(\frac{sin(x\pi+\frac{\pi}{2})}{2}+\frac 12))^2. n \frac{d}{dn}\int_{-1}^{1}{err(x, n)}{dx}=0. n F(x, n)","['integration', 'derivatives', 'optimization']"
67,Why do we use $Df$ rather than $f'$ for the derivative of a multivariable function?,Why do we use  rather than  for the derivative of a multivariable function?,Df f',Is there any reason we use $Df$ for derivatives of multivariable functions but $f'$ derivatives of single variable functions despite having a definition that works for both: $$Df(c) = f'(c) = L \iff \lim_{x \to c} \frac{f(x) - f(c) - L(x-c)}{||x-c||} = 0$$,Is there any reason we use for derivatives of multivariable functions but derivatives of single variable functions despite having a definition that works for both:,Df f' Df(c) = f'(c) = L \iff \lim_{x \to c} \frac{f(x) - f(c) - L(x-c)}{||x-c||} = 0,['derivatives']
68,Find $y'$ where $y=\ln(x+\sqrt{a^2+x^2}).$,Find  where,y' y=\ln(x+\sqrt{a^2+x^2}).,"My solution Of course, you may apply the derivative rule for the compound function. But I want to give another solution with little computation. Denote $$u=x+\sqrt{a^2+x^2},$$ $$v=x-\sqrt{a^2+x^2}.$$ Then $$u+v=2x,uv=-a^2.$$ Hence $$u'+v'=2,u'v+uv'=0.$$ We may obtain $$\frac{u'}{u}=\frac{2}{u-v}=\frac{1}{\sqrt{a^2+x^2}}.$$ As a result $$y'=\frac{u'}{u}=\frac{1}{\sqrt{a^2+x^2}}.$$ AM I RIGHT?","My solution Of course, you may apply the derivative rule for the compound function. But I want to give another solution with little computation. Denote Then Hence We may obtain As a result AM I RIGHT?","u=x+\sqrt{a^2+x^2}, v=x-\sqrt{a^2+x^2}. u+v=2x,uv=-a^2. u'+v'=2,u'v+uv'=0. \frac{u'}{u}=\frac{2}{u-v}=\frac{1}{\sqrt{a^2+x^2}}. y'=\frac{u'}{u}=\frac{1}{\sqrt{a^2+x^2}}.","['calculus', 'derivatives', 'proof-verification']"
69,Derivative of polynomial roots with respect to a parameter,Derivative of polynomial roots with respect to a parameter,,"Consider the polynomial $$p(s;\tau,a)=3(56-56a\tau^2+a^2\tau^4)-112(-6+a\tau^2)\tau s-14(-36+a\tau^2)\tau^2s^2+112\tau^3s^3+7\tau^4 s^4,$$ I want to examine that how the roots of $p$ change with respect to the parameter $\tau$ . I determined the derivative of a root with respect to $\tau$ using $$ \frac{\partial s}{\partial \tau}(s_i;\tau,a)=- \frac{ \frac{\partial p}{\partial \tau}(s_i;\tau,a)}{ \frac{\partial p}{\partial s}(s_i;\tau,a)},$$ where $s_i$ is one of the roots of $p$ . Is it correct to define the derivative $\frac{\partial s}{\partial \tau}$ ? And can we use the implicit function derivative formula?",Consider the polynomial I want to examine that how the roots of change with respect to the parameter . I determined the derivative of a root with respect to using where is one of the roots of . Is it correct to define the derivative ? And can we use the implicit function derivative formula?,"p(s;\tau,a)=3(56-56a\tau^2+a^2\tau^4)-112(-6+a\tau^2)\tau s-14(-36+a\tau^2)\tau^2s^2+112\tau^3s^3+7\tau^4 s^4, p \tau \tau  \frac{\partial s}{\partial \tau}(s_i;\tau,a)=- \frac{ \frac{\partial p}{\partial \tau}(s_i;\tau,a)}{ \frac{\partial p}{\partial s}(s_i;\tau,a)}, s_i p \frac{\partial s}{\partial \tau}","['derivatives', 'polynomials', 'parametric', 'implicit-differentiation']"
70,Why can't we treat $\text{d}y/\text{d}x$ as a fraction? [duplicate],Why can't we treat  as a fraction? [duplicate],\text{d}y/\text{d}x,"This question already has answers here : Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? (27 answers) When can we not treat differentials as fractions? And when is it perfectly OK? (9 answers) Closed 5 years ago . Disclaimer - I'm not a mathematician, I'm a dirty physicist. My work often involves performing calculus on various things without thinking about what I'm doing too much (I leave the proof of various identities etc for the pure mathematicians to worry about). However I've often noticed that mathematicians get a little upset when I do tricks such as treating the differential $\frac{\text{d}y}{\text{d}x}$ as if it were a fraction. The simplest example I can think of is how I think about the chain rule: $$\frac{\text{d}y}{\text{d}x} = \frac{\text{d}y}{\text{d}u}\frac{\text{d}u}{\text{d}x}$$ In my head, I imagine the $\text{d}u$ terms cancelling, which is why this works.  Indeed, this is how I'll explain to others how to use the chain rule when asked about it. My question is the following: Is is dangerous to think about differentials in this way? After all, one of the very first examples of calculus I've ever seen (back when I was a baby in high-school) was the derivative of $y=x^2$ calculated from first principles in the following way: \begin{align} y&=x^2\\ y+\delta y&=(x+\delta x)^2\\ y+\delta y&=x^2+2x\delta x+\delta x^2\\ \require{cancel}\cancel{x^2}+\delta y&=\cancel{x^2}+2x\delta x+\delta x^2\\ \delta y&=2x\delta x+\delta x^2\\ \frac{\delta y}{\delta x}&=2x+\delta x\\ \text{Now let }&\delta x\rightarrow0\text{ leaving}\\ \frac{\delta y}{\delta x}&=2x\\ \frac{\text{d}y}{\text{d}x}&=2x \end{align} And to me, this is just treating $\frac{\delta y}{\delta x}$ as a fraction. I know that technically you're doing $0/0$ if you think about it, but are there any examples where treating $\text{d}y/\text{d}x$ is really inappropriate?","This question already has answers here : Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? (27 answers) When can we not treat differentials as fractions? And when is it perfectly OK? (9 answers) Closed 5 years ago . Disclaimer - I'm not a mathematician, I'm a dirty physicist. My work often involves performing calculus on various things without thinking about what I'm doing too much (I leave the proof of various identities etc for the pure mathematicians to worry about). However I've often noticed that mathematicians get a little upset when I do tricks such as treating the differential as if it were a fraction. The simplest example I can think of is how I think about the chain rule: In my head, I imagine the terms cancelling, which is why this works.  Indeed, this is how I'll explain to others how to use the chain rule when asked about it. My question is the following: Is is dangerous to think about differentials in this way? After all, one of the very first examples of calculus I've ever seen (back when I was a baby in high-school) was the derivative of calculated from first principles in the following way: And to me, this is just treating as a fraction. I know that technically you're doing if you think about it, but are there any examples where treating is really inappropriate?","\frac{\text{d}y}{\text{d}x} \frac{\text{d}y}{\text{d}x} = \frac{\text{d}y}{\text{d}u}\frac{\text{d}u}{\text{d}x} \text{d}u y=x^2 \begin{align}
y&=x^2\\
y+\delta y&=(x+\delta x)^2\\
y+\delta y&=x^2+2x\delta x+\delta x^2\\
\require{cancel}\cancel{x^2}+\delta y&=\cancel{x^2}+2x\delta x+\delta x^2\\
\delta y&=2x\delta x+\delta x^2\\
\frac{\delta y}{\delta x}&=2x+\delta x\\
\text{Now let }&\delta x\rightarrow0\text{ leaving}\\
\frac{\delta y}{\delta x}&=2x\\
\frac{\text{d}y}{\text{d}x}&=2x
\end{align} \frac{\delta y}{\delta x} 0/0 \text{d}y/\text{d}x","['derivatives', 'fractions']"
71,If $V= x^nlogx$ then prove that $V_n = nV_{n-1} + (n-1)!$ where $V_n $ is the nth derivative of $V$,If  then prove that  where  is the nth derivative of,V= x^nlogx V_n = nV_{n-1} + (n-1)! V_n  V,"If $V= x^nlogx$ then prove that $V_n = nV_{n-1} + (n-1)!$ where $V_n $ is the nth derivative of $V$ My attempt : $V=x^nlogx$ $\therefore \, V_1 = nx^{n-1}logx + x^{n-1}$ $\therefore \, V_1= \dfrac{nV}{x}+x^{n-1}$ $\therefore \, xV_1=nV+x^n$ Differentiating $(n-1)$ times, $\therefore \, xV_n+(0)V_1=nV_{n-1}\,+ (n-1)!$ $\therefore \, xV_n=nV_{n-1}\,+ (n-1)!$ I don't know how an extra $x$ appears in the LHS in my answer.","If then prove that where is the nth derivative of My attempt : Differentiating times, I don't know how an extra appears in the LHS in my answer.","V= x^nlogx V_n = nV_{n-1} + (n-1)! V_n  V V=x^nlogx \therefore \, V_1 = nx^{n-1}logx + x^{n-1} \therefore \, V_1= \dfrac{nV}{x}+x^{n-1} \therefore \, xV_1=nV+x^n (n-1) \therefore \, xV_n+(0)V_1=nV_{n-1}\,+ (n-1)! \therefore \, xV_n=nV_{n-1}\,+ (n-1)! x","['calculus', 'derivatives']"
72,The derivative of a tangent vector having identified vectors as directional derivatives,The derivative of a tangent vector having identified vectors as directional derivatives,,"In introductory calculus, we learn to take derivatives of vectors. For instance, in Cartesian, we have the basis vectors $\vec{e}_x, \vec{e}_y$ and in polars we have $\vec{e}_\rho, \vec{e}_\theta$ given by $$ \vec{e}_\rho=\cos\theta \: \vec{e}_x + \sin\theta \: \vec{e}_y \\ \vec{e}_\theta=\cos\theta \: \vec{e}_y - \sin\theta \: \vec{e}_x  $$ Taking derivatives, we get $\partial_x \vec{e}_x = \partial_x \vec{e}_y = \partial_y \vec{e}_x = \partial_y \vec{e}_y = \vec{0}$ and we can differentiate polars as follows: \begin{align} \partial_\theta \vec{e}_\rho &=\partial_\theta \left( \cos\theta \:\vec{e}_x + \sin\theta \:\vec{e}_y \right) \\ &= -\sin\theta \:\vec{e}_x + \cos\theta \:\partial_\theta (\vec{e}_x)+ \cos\theta \:\vec{e}_y +\sin\theta \:\partial_\theta (\vec{e}_y) \\ &=\vec{e}_\theta \end{align} Fair enough. This all seems reasonable and the idea that the derivative of a basis vector with respect to the coordinates can be written as some linear combination of the basis vectors seems obvious. My question is, how can I arrive at the same conclusions having identified basis vectors with the set of partial derivatives . If $\vec{e}_x \to \partial_x$ , why should $\partial_x \vec{e}_x \to \partial_x \partial_x = \frac{\partial^2}{\partial x^2}=0$ ? I'm confused. More worrying still, using the polar example, I am getting \begin{align} \partial_\theta \vec{e}_\rho \to\partial_\theta \partial_\rho = \frac{\partial^2}{\partial \theta \partial \rho} \neq \partial_ \theta \end{align} I can't seem to reconcile any of this. It is also not at all obvious to me why the derivatives of basis vectors (partial derivatives) should be vectors - aren't they second derivatives? Grateful for any help...","In introductory calculus, we learn to take derivatives of vectors. For instance, in Cartesian, we have the basis vectors and in polars we have given by Taking derivatives, we get and we can differentiate polars as follows: Fair enough. This all seems reasonable and the idea that the derivative of a basis vector with respect to the coordinates can be written as some linear combination of the basis vectors seems obvious. My question is, how can I arrive at the same conclusions having identified basis vectors with the set of partial derivatives . If , why should ? I'm confused. More worrying still, using the polar example, I am getting I can't seem to reconcile any of this. It is also not at all obvious to me why the derivatives of basis vectors (partial derivatives) should be vectors - aren't they second derivatives? Grateful for any help...","\vec{e}_x, \vec{e}_y \vec{e}_\rho, \vec{e}_\theta 
\vec{e}_\rho=\cos\theta \: \vec{e}_x + \sin\theta \: \vec{e}_y \\
\vec{e}_\theta=\cos\theta \: \vec{e}_y - \sin\theta \: \vec{e}_x 
 \partial_x \vec{e}_x = \partial_x \vec{e}_y = \partial_y \vec{e}_x = \partial_y \vec{e}_y = \vec{0} \begin{align}
\partial_\theta \vec{e}_\rho
&=\partial_\theta \left( \cos\theta \:\vec{e}_x + \sin\theta \:\vec{e}_y \right) \\
&= -\sin\theta \:\vec{e}_x + \cos\theta \:\partial_\theta (\vec{e}_x)+ \cos\theta \:\vec{e}_y +\sin\theta \:\partial_\theta (\vec{e}_y) \\
&=\vec{e}_\theta
\end{align} \vec{e}_x \to \partial_x \partial_x \vec{e}_x \to \partial_x \partial_x = \frac{\partial^2}{\partial x^2}=0 \begin{align}
\partial_\theta \vec{e}_\rho
\to\partial_\theta \partial_\rho
= \frac{\partial^2}{\partial \theta \partial \rho} \neq \partial_ \theta
\end{align}","['calculus', 'derivatives', 'differential-geometry', 'vectors', 'partial-derivative']"
73,Meaning of the leibniz notation for the derivatives of parametric equations,Meaning of the leibniz notation for the derivatives of parametric equations,,"I'm trying to understand the meaning behind the leibniz notations when taking the derivative of a parametric equation. Say we have $x=f(t)$ and $y=g(t)$ Why does the derivative of this equal $\frac{\frac{dy}{dt}}{\frac{dx}{dt}}$ ? I guess what I'm trying to ask is, how does the ""derivative of $y$ with respect to $t$ "" over ""derivative of $x$ with respect to $t$ "" equal ""derivative of $y$ with respect to $x$ ""? And what happens when I want to take a second derivative of a parametric equation? How would that work?","I'm trying to understand the meaning behind the leibniz notations when taking the derivative of a parametric equation. Say we have and Why does the derivative of this equal ? I guess what I'm trying to ask is, how does the ""derivative of with respect to "" over ""derivative of with respect to "" equal ""derivative of with respect to ""? And what happens when I want to take a second derivative of a parametric equation? How would that work?",x=f(t) y=g(t) \frac{\frac{dy}{dt}}{\frac{dx}{dt}} y t x t y x,"['calculus', 'derivatives', 'notation']"
74,Second derivative finite difference of a convolution of two functions.,Second derivative finite difference of a convolution of two functions.,,I have a discrete function $I(V)$ which is defined as $$I(V) = R\big(V\big) * Q \big(V \big) = \int_a^b R\big(V\big) Q \big(V-V' \big) dV'$$ and I want to find $\frac{\partial^2R}{\partial V^2}$ but do not know $Q \big(V \big)$.  I am able to take the second derivative of $I(V)$ via finite difference: $$\frac{\partial^2I}{\partial V^2}  \approx  \frac{I(V+V_{0})+I(V-V_{0})-2I(V)}{V_{0}^2}$$ and I know that derivatives can be applied to either function in a convolution: $$\frac{\partial^2I}{\partial V^2} = \frac{\partial^2R}{\partial V^2} * Q \big(V \big) = R \big(V \big) * \frac{\partial^2Q}{\partial V^2} = \frac{\partial R}{\partial V} * \frac{\partial Q}{\partial V}$$ I also know that the process of taking the second derivative of a discrete function is the equivalent of convolving the function with the 3x3 Toeplitz matrix: $$\begin{bmatrix}-2 & 1 & 0\\1 & -2 & 1\\0 & 1 & -2\end{bmatrix}$$ Can I find either $\frac{\partial^2R}{\partial V^2}$ or $Q \big(V \big)$ if I know $I(V)$ (and therefore $\frac{\partial^2I}{\partial V^2}$)?,I have a discrete function $I(V)$ which is defined as $$I(V) = R\big(V\big) * Q \big(V \big) = \int_a^b R\big(V\big) Q \big(V-V' \big) dV'$$ and I want to find $\frac{\partial^2R}{\partial V^2}$ but do not know $Q \big(V \big)$.  I am able to take the second derivative of $I(V)$ via finite difference: $$\frac{\partial^2I}{\partial V^2}  \approx  \frac{I(V+V_{0})+I(V-V_{0})-2I(V)}{V_{0}^2}$$ and I know that derivatives can be applied to either function in a convolution: $$\frac{\partial^2I}{\partial V^2} = \frac{\partial^2R}{\partial V^2} * Q \big(V \big) = R \big(V \big) * \frac{\partial^2Q}{\partial V^2} = \frac{\partial R}{\partial V} * \frac{\partial Q}{\partial V}$$ I also know that the process of taking the second derivative of a discrete function is the equivalent of convolving the function with the 3x3 Toeplitz matrix: $$\begin{bmatrix}-2 & 1 & 0\\1 & -2 & 1\\0 & 1 & -2\end{bmatrix}$$ Can I find either $\frac{\partial^2R}{\partial V^2}$ or $Q \big(V \big)$ if I know $I(V)$ (and therefore $\frac{\partial^2I}{\partial V^2}$)?,,"['integration', 'derivatives', 'convolution', 'finite-differences']"
75,"Differentiation of $(x,y)\mapsto g(x+h(x,y))$",Differentiation of,"(x,y)\mapsto g(x+h(x,y))","Let $g:\mathbb{R}\to\mathbb{R}$ and $h:\mathbb{R}^2\to\mathbb{R}$ be two differentiable functions. I would like to compute the differential of $T:\mathbb{R}^2\to\mathbb{R}$ such that $T(x,y)=g(x+h(x,y))$. I get $D T(x,y) = Dg(x+h(x,y))\circ D(x_1(\cdot)+h(\cdot)) = g'(x+h(x,y))\times \big(1+\partial_1 h(x,y) \quad \partial_2 h(x,y)\big)$. Is it correct?","Let $g:\mathbb{R}\to\mathbb{R}$ and $h:\mathbb{R}^2\to\mathbb{R}$ be two differentiable functions. I would like to compute the differential of $T:\mathbb{R}^2\to\mathbb{R}$ such that $T(x,y)=g(x+h(x,y))$. I get $D T(x,y) = Dg(x+h(x,y))\circ D(x_1(\cdot)+h(\cdot)) = g'(x+h(x,y))\times \big(1+\partial_1 h(x,y) \quad \partial_2 h(x,y)\big)$. Is it correct?",,['derivatives']
76,Approximation of derivative using combinations,Approximation of derivative using combinations,,"I have sets $A = \left\{a_{1},a_{2},a_{3},\dots,a_{n} \right\}$ and $B = \left\{b_{1},b_{2},b_{3},\dots,b_{n} \right\}$. If they are time-series sorted by their indices, I can take the differences, $$ d = \frac{\Delta A}{\Delta B} = \left\{\frac{a_2-a_1}{b_2-b_1},\frac{a_3-a_2}{b_3-b_2}, \dots, \frac{a_n-a_{n-1}}{b_n-b_{n-1}}\right\} $$ and take the mean finite difference $\bar{d}$ to approximate the derivative $\frac{dA(t)}{dB(t)}$. If they are not sorted by time (i.e., they are just sequences but not time-series), can I take all combinations of differences in the numerator and denominator, take their ratio, and finally their mean to approximate $\bar{d}$? That is, can we say: $$ \bar{d} \approx \frac{1}{n}\sum_{i\neq j}\frac{a_i-a_j}{b_i-b_j} $$ It will be awesome if someone could suggest an approximation. This is needed in my biology research. Thanks.","I have sets $A = \left\{a_{1},a_{2},a_{3},\dots,a_{n} \right\}$ and $B = \left\{b_{1},b_{2},b_{3},\dots,b_{n} \right\}$. If they are time-series sorted by their indices, I can take the differences, $$ d = \frac{\Delta A}{\Delta B} = \left\{\frac{a_2-a_1}{b_2-b_1},\frac{a_3-a_2}{b_3-b_2}, \dots, \frac{a_n-a_{n-1}}{b_n-b_{n-1}}\right\} $$ and take the mean finite difference $\bar{d}$ to approximate the derivative $\frac{dA(t)}{dB(t)}$. If they are not sorted by time (i.e., they are just sequences but not time-series), can I take all combinations of differences in the numerator and denominator, take their ratio, and finally their mean to approximate $\bar{d}$? That is, can we say: $$ \bar{d} \approx \frac{1}{n}\sum_{i\neq j}\frac{a_i-a_j}{b_i-b_j} $$ It will be awesome if someone could suggest an approximation. This is needed in my biology research. Thanks.",,['derivatives']
77,Prove for the time derivative of a vector with constant magnitude,Prove for the time derivative of a vector with constant magnitude,,"First of all, sorry for my bad English, I have several doubts about the geometric demonstration made by Kleppner for the derivative of a vector that has a constant magnitude, page 25, which is attached in this photo: . Is this proof rigorously validated? I understand that $\sin (x) \approx x$ for small $x$, by dividing both sides by $\Delta t$, and take the limit for the equality. I understand that equality is correct, but is the argument correct? In the book, i understand the intuitive part, but for example what would have happened if I had taken an approximation of higher order, this would not have had any contribution in the limit? How are these terms depreciated rigorously, since I think that depends on how the time angle $θ (t)$ depends, for example, you can say that the vector varies with $Δθ (t)$, very slow or very fast so that the terms of higher order have a contribution in the limit, and therefore the equality raised is not correct, I apologize in case the question is a triviality.","First of all, sorry for my bad English, I have several doubts about the geometric demonstration made by Kleppner for the derivative of a vector that has a constant magnitude, page 25, which is attached in this photo: . Is this proof rigorously validated? I understand that $\sin (x) \approx x$ for small $x$, by dividing both sides by $\Delta t$, and take the limit for the equality. I understand that equality is correct, but is the argument correct? In the book, i understand the intuitive part, but for example what would have happened if I had taken an approximation of higher order, this would not have had any contribution in the limit? How are these terms depreciated rigorously, since I think that depends on how the time angle $θ (t)$ depends, for example, you can say that the vector varies with $Δθ (t)$, very slow or very fast so that the terms of higher order have a contribution in the limit, and therefore the equality raised is not correct, I apologize in case the question is a triviality.",,"['geometry', 'derivatives', 'vectors', 'physics', 'mathematical-physics']"
78,How to take derivative with respect to logarithmic function,How to take derivative with respect to logarithmic function,,"I'm having a little bit of trouble trying to get my head around the following problem. I have this main function $$S_n(\alpha, \beta) = \dfrac{dV}{V} \dfrac{\alpha}{d\alpha} = \dfrac{4 \alpha  \beta^n}{(1 + \alpha)^2 - (1-\alpha)^2\beta^{2 n}}$$ The author explains that ""For all values of $\alpha$, the logarithm of $S$ is, asymptotically for small $\beta$, linearly related to the logarithm of $\beta$. Thus, the slope can be determined by: $$\dfrac{d \ln S}{d ln \beta}$$ From my understanding, I approached this problem by substituting $ln(x)$ using $$\dfrac{d \ln(x)}{dx} = 1/ x \Rightarrow d \ln (x) = \dfrac{dx}{x} $$ Therefore, the slope would be obtained using: $$ \dfrac{dS}{d \beta} \dfrac{\beta}{S}$$ But I'm not sure if this approach is correct, as so far it hasn't got me close to the results of the author. Any suggestion is highly appreciated.","I'm having a little bit of trouble trying to get my head around the following problem. I have this main function $$S_n(\alpha, \beta) = \dfrac{dV}{V} \dfrac{\alpha}{d\alpha} = \dfrac{4 \alpha  \beta^n}{(1 + \alpha)^2 - (1-\alpha)^2\beta^{2 n}}$$ The author explains that ""For all values of $\alpha$, the logarithm of $S$ is, asymptotically for small $\beta$, linearly related to the logarithm of $\beta$. Thus, the slope can be determined by: $$\dfrac{d \ln S}{d ln \beta}$$ From my understanding, I approached this problem by substituting $ln(x)$ using $$\dfrac{d \ln(x)}{dx} = 1/ x \Rightarrow d \ln (x) = \dfrac{dx}{x} $$ Therefore, the slope would be obtained using: $$ \dfrac{dS}{d \beta} \dfrac{\beta}{S}$$ But I'm not sure if this approach is correct, as so far it hasn't got me close to the results of the author. Any suggestion is highly appreciated.",,"['linear-algebra', 'derivatives']"
79,Derivative of exponential of a sparse parametrized matrix with respect to the parameters,Derivative of exponential of a sparse parametrized matrix with respect to the parameters,,"I have a Hermitian matrix of the type $$H = H(c_1, c_2, \dots, c_n)$$ where $c_i$ 's are some complex parameters. I need to find the derivatives $$\frac{\partial}{\partial c_j}\text{exp}(H)$$ for each $c_j$ . I don't think it will be of the form $\text{exp}(H)\frac{dH}{dc_j}$ because $H$ and $\frac{dH}{dc_j}$ won't necessarily commute. Does anyone have an analytical/numerical suggestion for this problem? This looks promising: Derivative of matrix exponential - but would be very complicated to implement numerically. Are there any other methods for this? Edit: Will the fact that $H$ is a sparse matrix help?",I have a Hermitian matrix of the type where 's are some complex parameters. I need to find the derivatives for each . I don't think it will be of the form because and won't necessarily commute. Does anyone have an analytical/numerical suggestion for this problem? This looks promising: Derivative of matrix exponential - but would be very complicated to implement numerically. Are there any other methods for this? Edit: Will the fact that is a sparse matrix help?,"H = H(c_1, c_2, \dots, c_n) c_i \frac{\partial}{\partial c_j}\text{exp}(H) c_j \text{exp}(H)\frac{dH}{dc_j} H \frac{dH}{dc_j} H","['matrices', 'derivatives', 'numerical-methods', 'matrix-calculus', 'matrix-exponential']"
80,Proof for the pointwise limit of functions with bounded derivatives being Lipschitz,Proof for the pointwise limit of functions with bounded derivatives being Lipschitz,,"Let $f_n\colon \mathbb{R} \rightarrow \mathbb{R}$ be differentiable for  each $n \in \mathbb{N}$ with $\lvert f'_n(x)\rvert \le 1$ for all $n$ and $x$.  Let $\lim_{n \rightarrow \infty} f_n(x) = g(x)$ for all $x$. Prove  that $g\colon \mathbb{R} \rightarrow \mathbb{R}$ is Lipschitz-continuous. I know one can prove this using the triangle inequality, e.g. as suggested here . I wanted to check if the following, similar argument is also valid: Let $x$,$y \in \mathbb{R}$ and $n$ be arbitrary. By the mean value theorem, there exists $c \in (x,y)$ such that $$ f_n(x)-f_n(y)= f_n^{'}(c) \cdot (x-y).$$ Using this, we obtain a bound for the differential quotient of $f_n(x)$: $$\frac{\mid f_n(x)-f_n(y)\mid}{\mid x-y \mid} = \frac{\mid f_n^{'}(c)\mid \cdot\mid x-y \mid}{\mid x-y \mid} \leq 1.$$  Therefore, we obtain the following: $$\frac{\mid g(x)-g(y)\mid}{\mid x-y \mid} = \frac{\mid \lim f_n(x)- \lim f_n(y)\mid}{\mid x-y \mid} =\frac{\mid \lim (f_n(x)-f_n(y))\mid}{\mid x-y \mid} = \lim\frac{\mid f_n(x)-f_n(y)\mid}{\mid x-y \mid} \leq 1 ,$$ where the first equality follows by definition, the second by limit laws for addition, the third as $\mid {}\cdot{}\mid$ is continuous and the inequality as if a convergent sequence is bounded, then the limit is bounded in the same way.","Let $f_n\colon \mathbb{R} \rightarrow \mathbb{R}$ be differentiable for  each $n \in \mathbb{N}$ with $\lvert f'_n(x)\rvert \le 1$ for all $n$ and $x$.  Let $\lim_{n \rightarrow \infty} f_n(x) = g(x)$ for all $x$. Prove  that $g\colon \mathbb{R} \rightarrow \mathbb{R}$ is Lipschitz-continuous. I know one can prove this using the triangle inequality, e.g. as suggested here . I wanted to check if the following, similar argument is also valid: Let $x$,$y \in \mathbb{R}$ and $n$ be arbitrary. By the mean value theorem, there exists $c \in (x,y)$ such that $$ f_n(x)-f_n(y)= f_n^{'}(c) \cdot (x-y).$$ Using this, we obtain a bound for the differential quotient of $f_n(x)$: $$\frac{\mid f_n(x)-f_n(y)\mid}{\mid x-y \mid} = \frac{\mid f_n^{'}(c)\mid \cdot\mid x-y \mid}{\mid x-y \mid} \leq 1.$$  Therefore, we obtain the following: $$\frac{\mid g(x)-g(y)\mid}{\mid x-y \mid} = \frac{\mid \lim f_n(x)- \lim f_n(y)\mid}{\mid x-y \mid} =\frac{\mid \lim (f_n(x)-f_n(y))\mid}{\mid x-y \mid} = \lim\frac{\mid f_n(x)-f_n(y)\mid}{\mid x-y \mid} \leq 1 ,$$ where the first equality follows by definition, the second by limit laws for addition, the third as $\mid {}\cdot{}\mid$ is continuous and the inequality as if a convergent sequence is bounded, then the limit is bounded in the same way.",,"['real-analysis', 'derivatives', 'proof-verification', 'lipschitz-functions']"
81,"Characterizing derivatives and other functions as ""best local approximations""","Characterizing derivatives and other functions as ""best local approximations""",,"People often say that the derivative of a function at a point is its ""best"" linear (or affine) approximation around that point. This seems like a good intuition, but I've never seen it made precise. What I'd hope for is a sort of universal property, given a class of possible approximations, that specifies which approximation is the best one, if it exists. I'd want to get continuity from the class of constant functions, derivatives from the class of affine functions, and in general the kth-order Taylor polynomial from the class of polynomials of degree up to k. One possible definition of one approximation being better than another might be that its error is smaller at every point in some neighborhood. That is: Given some class $G$ of functions $: X \to Y$, $f : X \to Y$, and $x \in X$, The best $G$-approximation of $f$ at $x$ is a function $g_0 \in G$ such that for all $g \in G$, there's a neighborhood $A$ of $x$ such that for all $x' \in A$, $d(g_0(x'), f(x')) \le d(g(x'), f(x'))$. (In particular, if we say that a function is continuous if it can be approximated by a constant function, we get this somewhat unusual definition of continuity: $f : X \to Y$ is continuous at $x \in X$ if for all $y \in Y$, there's a neighborhood $A$ of $x$ such that for all $x' \in A$, $d(f(x'), f(x)) \le d(f(x'), y)$.) Does this definition do what I want? Can it be generalized to non-metric spaces? Or is there another definition of ""best approximation"" that works better?","People often say that the derivative of a function at a point is its ""best"" linear (or affine) approximation around that point. This seems like a good intuition, but I've never seen it made precise. What I'd hope for is a sort of universal property, given a class of possible approximations, that specifies which approximation is the best one, if it exists. I'd want to get continuity from the class of constant functions, derivatives from the class of affine functions, and in general the kth-order Taylor polynomial from the class of polynomials of degree up to k. One possible definition of one approximation being better than another might be that its error is smaller at every point in some neighborhood. That is: Given some class $G$ of functions $: X \to Y$, $f : X \to Y$, and $x \in X$, The best $G$-approximation of $f$ at $x$ is a function $g_0 \in G$ such that for all $g \in G$, there's a neighborhood $A$ of $x$ such that for all $x' \in A$, $d(g_0(x'), f(x')) \le d(g(x'), f(x'))$. (In particular, if we say that a function is continuous if it can be approximated by a constant function, we get this somewhat unusual definition of continuity: $f : X \to Y$ is continuous at $x \in X$ if for all $y \in Y$, there's a neighborhood $A$ of $x$ such that for all $x' \in A$, $d(f(x'), f(x)) \le d(f(x'), y)$.) Does this definition do what I want? Can it be generalized to non-metric spaces? Or is there another definition of ""best approximation"" that works better?",,"['general-topology', 'derivatives', 'metric-spaces', 'universal-property']"
82,What is the deep concept behind the Leibniz rule?,What is the deep concept behind the Leibniz rule?,,"There are quite a few occasions in mathematics, where some form of the Leibniz rule $(fg)'=f'g+fg'$ appears - often in connection with some kind of ""differential"". Some examples are: Of coure the standard Leibnitz rule for two functions $f,g:\mathbb{R}\rightarrow\mathbb{R}$, which we can easily generalize for higher dimensional or holomorphic functions and finaly to functions between manifolds. There is also the boundary-formula for the product of two manifolds with Boundary: $\partial(M\times N)=(\partial M \times N)\cup(M\times\partial N)$. There seems to be some connection to the first example via Stokes theorem, but I think it is already unclear wether this formula implies the formula of the first example or vice versa. In the case of an algebraic varietie $X$ over a field $k$ we define the tangent space at a point $x\in X$ is defined as the space $D$ of derivations i.e. the space of linear functions $\nu:\mathcal{O}_{X,x}\rightarrow k$ satisfying $\nu(fg)=\nu(f)g(p)+f(p)\nu(g)$. In the case $k=\mathbb{C}$ there is some intuition for this, but the Leibniz rule also seems to be the crutial part of the definition for any field and also tangent spaces seem to do what we want with this definition. Finally there is also the Serre-Spectral sequence $\lbrace E_r^{p,q}\rbrace$ in cohomology. It can be equipped with a product structure $E_r^{p,q}\times E_r^{s,t}\rightarrow E_r^{p+s,q+t}$ satisfying the (graded) Leibniz rule $d(xy)=d(x)y+(-1)^{p+q}xd(y)$. I'm quite new to the usage of spectral sequences, so I really don't know if there is any connection to anything connected to differentiation in analyis. So the Leibniz rule appears in quite a lot of occasions and moreover it seems to be sometimes the crutial part in a definition (note, that in the first case it follows from some other definition whilest in the third example it is a crutial part of the definition).  Now I wonder if there is some deeper principle or theory connecting all those cases, or maybe explaining why this formula is so important?","There are quite a few occasions in mathematics, where some form of the Leibniz rule $(fg)'=f'g+fg'$ appears - often in connection with some kind of ""differential"". Some examples are: Of coure the standard Leibnitz rule for two functions $f,g:\mathbb{R}\rightarrow\mathbb{R}$, which we can easily generalize for higher dimensional or holomorphic functions and finaly to functions between manifolds. There is also the boundary-formula for the product of two manifolds with Boundary: $\partial(M\times N)=(\partial M \times N)\cup(M\times\partial N)$. There seems to be some connection to the first example via Stokes theorem, but I think it is already unclear wether this formula implies the formula of the first example or vice versa. In the case of an algebraic varietie $X$ over a field $k$ we define the tangent space at a point $x\in X$ is defined as the space $D$ of derivations i.e. the space of linear functions $\nu:\mathcal{O}_{X,x}\rightarrow k$ satisfying $\nu(fg)=\nu(f)g(p)+f(p)\nu(g)$. In the case $k=\mathbb{C}$ there is some intuition for this, but the Leibniz rule also seems to be the crutial part of the definition for any field and also tangent spaces seem to do what we want with this definition. Finally there is also the Serre-Spectral sequence $\lbrace E_r^{p,q}\rbrace$ in cohomology. It can be equipped with a product structure $E_r^{p,q}\times E_r^{s,t}\rightarrow E_r^{p+s,q+t}$ satisfying the (graded) Leibniz rule $d(xy)=d(x)y+(-1)^{p+q}xd(y)$. I'm quite new to the usage of spectral sequences, so I really don't know if there is any connection to anything connected to differentiation in analyis. So the Leibniz rule appears in quite a lot of occasions and moreover it seems to be sometimes the crutial part in a definition (note, that in the first case it follows from some other definition whilest in the third example it is a crutial part of the definition).  Now I wonder if there is some deeper principle or theory connecting all those cases, or maybe explaining why this formula is so important?",,"['derivatives', 'soft-question']"
83,Derivative of Projection | Derivative of Matrix w.r.t matrix,Derivative of Projection | Derivative of Matrix w.r.t matrix,,"I am trying to take derivative of following function  w.r.t matrix $X$ where $X$ is not a square matrix hence singular. $$ f(X) = X(X^TX)^{-1}X^T $$ I used product rule for the function with $U = X, V = (X^TX)^{-1} and W = X^T$. I am stuck at how to take derivative of matrix w.r.t to a matrix. I used the vec concept given in http://www.iro.umontreal.ca/~pift6266/A06/refs/minka-matrix.pdf it solved $U$ but not sure about $V and W$. Is there a better way to solve the function ?","I am trying to take derivative of following function  w.r.t matrix $X$ where $X$ is not a square matrix hence singular. $$ f(X) = X(X^TX)^{-1}X^T $$ I used product rule for the function with $U = X, V = (X^TX)^{-1} and W = X^T$. I am stuck at how to take derivative of matrix w.r.t to a matrix. I used the vec concept given in http://www.iro.umontreal.ca/~pift6266/A06/refs/minka-matrix.pdf it solved $U$ but not sure about $V and W$. Is there a better way to solve the function ?",,"['matrices', 'derivatives', 'optimization', 'matrix-calculus', 'projection-matrices']"
84,Compute $\frac{d}{dx(t)}\int_0^Tx(\tau)^TAx(\tau)d\tau$,Compute,\frac{d}{dx(t)}\int_0^Tx(\tau)^TAx(\tau)d\tau,"I need to compute: $$ \frac{d}{dx(t)}\int_0^Tx(\tau)^TAx(\tau)d\tau, $$ where $t\in (0,T)$, $A\in\mathbb R^{n\times n}$ and $x\in\mathbb R^n$. Using Leibniz differentiation under an integral sign, I have: $$ \frac{d}{dx(t)}\int_0^Tx(\tau)^TAx(\tau)d\tau = \int_0^T\frac{dx(\tau)^TAx(\tau)}{dx(t)}d\tau, $$ My course notes say this equals $Ax(t)$, which is almost as if: $$ \int_0^T\frac{dx(\tau)^TAx(\tau)}{dx(t)}d\tau = \int_0^TAx(\tau)\delta(\tau-t)d\tau $$ But I am not sure whether this is true or if something else tells us that it is $=Ax(t)$. Thanks for helping!","I need to compute: $$ \frac{d}{dx(t)}\int_0^Tx(\tau)^TAx(\tau)d\tau, $$ where $t\in (0,T)$, $A\in\mathbb R^{n\times n}$ and $x\in\mathbb R^n$. Using Leibniz differentiation under an integral sign, I have: $$ \frac{d}{dx(t)}\int_0^Tx(\tau)^TAx(\tau)d\tau = \int_0^T\frac{dx(\tau)^TAx(\tau)}{dx(t)}d\tau, $$ My course notes say this equals $Ax(t)$, which is almost as if: $$ \int_0^T\frac{dx(\tau)^TAx(\tau)}{dx(t)}d\tau = \int_0^TAx(\tau)\delta(\tau-t)d\tau $$ But I am not sure whether this is true or if something else tells us that it is $=Ax(t)$. Thanks for helping!",,"['calculus', 'integration', 'derivatives', 'calculus-of-variations', 'functional-calculus']"
85,Quaternion holomorphic functions are affine?,Quaternion holomorphic functions are affine?,,"I believe the fallowing is a folklore: Suppose $f:\mathbb{H}^n \rightarrow \mathbb{H}^m$ (or actually between open subsets of them) is of class $\mathcal{C}^1$ (can we assume to be only Frechet differentiable?) in the sense that the Frechet derivative $d_qf \in \mathcal{M}_{n,m}(\mathbb{H})$ defined by $$ lim_{h \rightarrow 0}\frac{||f(q+h)-f(q)-d_qf(h)||}{||h||}=0$$ exist in every point and is continuous as a function $df:\mathbb{H}^n \rightarrow \mathcal{M}_{n,m}(\mathbb{H})$. Then $f(q)=Aq + q_0$ for some $A \in \mathcal{M}_{n,m}(\mathbb{H})$ and $q_0 \in \mathbb{H}$. The only reference I know for the case $n=1$ is by A. Sudbery . There a different definition of ""differentiability"" is used although from Frechet differentiability the former fallows (as well as its right version). The bigger problem is that reasoning involves quaternion differentials as well as passing to some formal complex derivatives. Does one know other references (possibly to the general situation) or maybe a simpler reasoning?","I believe the fallowing is a folklore: Suppose $f:\mathbb{H}^n \rightarrow \mathbb{H}^m$ (or actually between open subsets of them) is of class $\mathcal{C}^1$ (can we assume to be only Frechet differentiable?) in the sense that the Frechet derivative $d_qf \in \mathcal{M}_{n,m}(\mathbb{H})$ defined by $$ lim_{h \rightarrow 0}\frac{||f(q+h)-f(q)-d_qf(h)||}{||h||}=0$$ exist in every point and is continuous as a function $df:\mathbb{H}^n \rightarrow \mathcal{M}_{n,m}(\mathbb{H})$. Then $f(q)=Aq + q_0$ for some $A \in \mathcal{M}_{n,m}(\mathbb{H})$ and $q_0 \in \mathbb{H}$. The only reference I know for the case $n=1$ is by A. Sudbery . There a different definition of ""differentiability"" is used although from Frechet differentiability the former fallows (as well as its right version). The bigger problem is that reasoning involves quaternion differentials as well as passing to some formal complex derivatives. Does one know other references (possibly to the general situation) or maybe a simpler reasoning?",,"['derivatives', 'reference-request', 'quaternions']"
86,$\frac{d}{dX}[tr(-(CX(X^TCX)^{-1})(A+A^T)(X^TCX)^{-1})]=?$,,\frac{d}{dX}[tr(-(CX(X^TCX)^{-1})(A+A^T)(X^TCX)^{-1})]=?,"I want to obtain the derivative of the trace of the following statement with regard to $X$, where $A$, $C$, and $X$ are matrices and $C$ is symmetric. $$\frac{d}{dX}[tr(-(CX(X^TCX)^{-1})(A+A^T)(X^TCX)^{-1})]=?$$ where $\frac{d}{dX}(y)$is a matrix whose $(i,j)$ element is $\frac{dy}{dX}(i,j)$. I doubt that my calculation is correct or not, so I'm grateful for your help. Thanks a lot for any response.","I want to obtain the derivative of the trace of the following statement with regard to $X$, where $A$, $C$, and $X$ are matrices and $C$ is symmetric. $$\frac{d}{dX}[tr(-(CX(X^TCX)^{-1})(A+A^T)(X^TCX)^{-1})]=?$$ where $\frac{d}{dX}(y)$is a matrix whose $(i,j)$ element is $\frac{dy}{dX}(i,j)$. I doubt that my calculation is correct or not, so I'm grateful for your help. Thanks a lot for any response.",,"['linear-algebra', 'matrices', 'derivatives']"
87,Interrelation between continuity and differentiability?,Interrelation between continuity and differentiability?,,"I need a ""big picture"" of how those things relate to each other and probably a list of fundamental theorems that glue them up together. My current (quite limited) understanding: 1) The fact that function $f(x)$ has a derivative at point $P$ does mean that it behaves nearly like a straight line infinitely close to the $P$ and thus might be approximated with such a straight line with infinitely small error (which becomes exactly 0 on at least two points $P, P_{neighbor}$ since both are shared by given line and original $f(x)$). 2) As such, $f(x)$ has to be continuous (not necessarily uniformly continuous) at the $P$, otherwise as long as $P_{neighbor} \rightarrow P$, $f(x)$ kind of ""jumps"" rather then being smooth and thus can not be estimated with a straight line to an infinitely small tolerance (but still could be approximated with a line, which obviously won't be precise and practically valuable, but still theoretically possible). 3) As such the fact that $f(x)$ is indeed continuous at $P$ guarantees that it could be ""goodly"" approximated with a straight line? Am I right or wrong? UPDATE : I think the given answer beneath helped me to discover one mistake I wasn't able to see before. Let me try to formulate a new statement and you, guys, please correct me in case I am still mistaking things. There are 2 different ways to approximate any (or only continuous?) $f(x)$: 1) when you pick up any 2 points belong to $f(x)$ and graph what is called a ""secant line""; thus $[P_1, P_2]$ interval emerges where your linear estimation could be as precise as one wish (as long as $P_2$ approaches $P_1$). 2) But when things come down to the limit of such an constantly decreasing distance between both points, $P_2$ vanish and merges with $P_1$ such that only single point exits. Since this very moment, given line ceases to be a secant and becomes an ""tangent line"", which guaranteed to share at least single $P_1$ point with original $f(x)$ (and thus the error there must be precisely zero). Such an approach approximates $f(x)$ around $P_1$: to be more concrete, on some $[P_1 - \delta, P_1 + \delta]$. So whenever $$x \in [P_1 - \delta, P_1 + \delta]$$ there exists $\epsilon \gt 0$ such that: $$|L(x) - f(x)| \lt \epsilon$$ 4) Now my question: $[P_1 - \delta, P_1 + \delta]$ actually ships two points, which makes me come back to the initial point: there is a secant line, crossing two points belonging for original $f(x)$.","I need a ""big picture"" of how those things relate to each other and probably a list of fundamental theorems that glue them up together. My current (quite limited) understanding: 1) The fact that function $f(x)$ has a derivative at point $P$ does mean that it behaves nearly like a straight line infinitely close to the $P$ and thus might be approximated with such a straight line with infinitely small error (which becomes exactly 0 on at least two points $P, P_{neighbor}$ since both are shared by given line and original $f(x)$). 2) As such, $f(x)$ has to be continuous (not necessarily uniformly continuous) at the $P$, otherwise as long as $P_{neighbor} \rightarrow P$, $f(x)$ kind of ""jumps"" rather then being smooth and thus can not be estimated with a straight line to an infinitely small tolerance (but still could be approximated with a line, which obviously won't be precise and practically valuable, but still theoretically possible). 3) As such the fact that $f(x)$ is indeed continuous at $P$ guarantees that it could be ""goodly"" approximated with a straight line? Am I right or wrong? UPDATE : I think the given answer beneath helped me to discover one mistake I wasn't able to see before. Let me try to formulate a new statement and you, guys, please correct me in case I am still mistaking things. There are 2 different ways to approximate any (or only continuous?) $f(x)$: 1) when you pick up any 2 points belong to $f(x)$ and graph what is called a ""secant line""; thus $[P_1, P_2]$ interval emerges where your linear estimation could be as precise as one wish (as long as $P_2$ approaches $P_1$). 2) But when things come down to the limit of such an constantly decreasing distance between both points, $P_2$ vanish and merges with $P_1$ such that only single point exits. Since this very moment, given line ceases to be a secant and becomes an ""tangent line"", which guaranteed to share at least single $P_1$ point with original $f(x)$ (and thus the error there must be precisely zero). Such an approach approximates $f(x)$ around $P_1$: to be more concrete, on some $[P_1 - \delta, P_1 + \delta]$. So whenever $$x \in [P_1 - \delta, P_1 + \delta]$$ there exists $\epsilon \gt 0$ such that: $$|L(x) - f(x)| \lt \epsilon$$ 4) Now my question: $[P_1 - \delta, P_1 + \delta]$ actually ships two points, which makes me come back to the initial point: there is a secant line, crossing two points belonging for original $f(x)$.",,"['calculus', 'derivatives', 'continuity', 'uniform-continuity']"
88,Computing the gradient of scalar wrt a vector,Computing the gradient of scalar wrt a vector,,"Let  $$ \alpha = x^TAx \enspace x\in \mathbf{R}^{nx1}, A \in \mathbf{R}^{nxn} $$ How do I compute the derivative $\Large \frac{\partial\alpha}{\partial x}$ without using the coordinate way i.e, writing A in terms of $\large A = (a_{ij})$ My attempt is as follows (using the product rule): $$ \begin{align} \frac{\partial\alpha}{\partial x} = &\frac{\partial(x^TAx)}{\partial x}\\ =&\frac{\partial(x^TA)x}{\partial x} + x^TA\frac{\partial x}{\partial x}\\ =&\frac{\partial(A^Tx)^Tx}{\partial x} + x^TAI\\ =&\left(\frac{\partial A^Tx}{\partial x}\right)^Tx+ x^TA\\ =&Ax+x^TA \end{align} $$ This is different from the answer that I find in wikipedia which says is to be: $$ \frac{\partial\alpha}{\partial x} = x^TA+x^TA^T $$ Where am I going wrong, please present your answer in the product rule form.","Let  $$ \alpha = x^TAx \enspace x\in \mathbf{R}^{nx1}, A \in \mathbf{R}^{nxn} $$ How do I compute the derivative $\Large \frac{\partial\alpha}{\partial x}$ without using the coordinate way i.e, writing A in terms of $\large A = (a_{ij})$ My attempt is as follows (using the product rule): $$ \begin{align} \frac{\partial\alpha}{\partial x} = &\frac{\partial(x^TAx)}{\partial x}\\ =&\frac{\partial(x^TA)x}{\partial x} + x^TA\frac{\partial x}{\partial x}\\ =&\frac{\partial(A^Tx)^Tx}{\partial x} + x^TAI\\ =&\left(\frac{\partial A^Tx}{\partial x}\right)^Tx+ x^TA\\ =&Ax+x^TA \end{align} $$ This is different from the answer that I find in wikipedia which says is to be: $$ \frac{\partial\alpha}{\partial x} = x^TA+x^TA^T $$ Where am I going wrong, please present your answer in the product rule form.",,['matrices']
89,How to figure out if there is an actual horizontal tangent without a graph,How to figure out if there is an actual horizontal tangent without a graph,,"There is this practice problem that asks to determine the points at which the graph of $y^4=y^2-x^2$ has a horizontal tangent. So I did implicit differentiation to find that $$\displaystyle\frac{dy}{dx} = \frac{-x}{2y^3-y}$$ To find the horizontal tangent, I set $\frac{dy}{dx}=0$ and solved for $x$: $$\begin{align} \displaystyle\frac{dy}{dx} &= 0 \\ \frac{-x}{2y^3-y} &= 0 \\ -x &= 0 \\ x &= 0 \end{align}$$ Then I substituted $x=0$ into the equation of the curve: $$\begin{align} y^4&=y^2-(0)^2 \\ 0 &= y^4 - y^2\\ 0 &= y^2(y+1)(y-1) \\ y&=-1,\,0,\,1 \end{align}$$ I concluded that the points $(0,0)$, $(0,-1)$, and $(0,1)$ were the points with a horizontal tangent. However, when I graphed this using Desmos, it turns out that the point at $(0,0)$ did not look like it has horizontal tangent. Graph of y^4=y^2-x^2 How would I have been able to figure this out without graphing it?","There is this practice problem that asks to determine the points at which the graph of $y^4=y^2-x^2$ has a horizontal tangent. So I did implicit differentiation to find that $$\displaystyle\frac{dy}{dx} = \frac{-x}{2y^3-y}$$ To find the horizontal tangent, I set $\frac{dy}{dx}=0$ and solved for $x$: $$\begin{align} \displaystyle\frac{dy}{dx} &= 0 \\ \frac{-x}{2y^3-y} &= 0 \\ -x &= 0 \\ x &= 0 \end{align}$$ Then I substituted $x=0$ into the equation of the curve: $$\begin{align} y^4&=y^2-(0)^2 \\ 0 &= y^4 - y^2\\ 0 &= y^2(y+1)(y-1) \\ y&=-1,\,0,\,1 \end{align}$$ I concluded that the points $(0,0)$, $(0,-1)$, and $(0,1)$ were the points with a horizontal tangent. However, when I graphed this using Desmos, it turns out that the point at $(0,0)$ did not look like it has horizontal tangent. Graph of y^4=y^2-x^2 How would I have been able to figure this out without graphing it?",,['calculus']
90,Solving derivative of squared error where the predictor is a sigmoid function,Solving derivative of squared error where the predictor is a sigmoid function,,"$\newcommand{\sigmoid}{\operatorname{sigmoid}}$In the book ""Make your own neural network"" by Tariq Rashid, I have to take the derivative of my cost function which is: $$ \left(t-\sigmoid\left(\sum_j w_{jk}\times o_j\right)\right)^2 $$ where $t$ is the true value and thus is a constant. $o_j$ is the value of the previous node and $w_{jk}$ are the weights that connect $o_j$ to the error node. Trying to work out the derivative of the function myself I get the following result: $$ 2\left(t-\sigmoid\left(\sum_j w_{jk}\times o_j\right)\right)\times \left(\sigmoid\left(\sum_j w_{jk}\times o_j\right)\right)\left(1-\sigmoid\left(\sum_j w_{jk}\times o_j\right)\right) $$ The problem is that the derivative in the book is a bit different and I have no idea why or what I did wrong. The answer in the book has $-2$ and is multiplied by $o_j$ at the end. Where does the $-2$ and $o_j$ in the equation come from? what step of the chain rule did I miss? $$ -2\left(t-\sigmoid\left(\sum_j w_{jk}\times o_j\right)\right)\times \left(\sigmoid\left(\sum_j w_{jk}\times o_j\right)\right)\left(1-\sigmoid\left(\sum_j w_{jk}\times o_j\right)\right)\times o_j $$","$\newcommand{\sigmoid}{\operatorname{sigmoid}}$In the book ""Make your own neural network"" by Tariq Rashid, I have to take the derivative of my cost function which is: $$ \left(t-\sigmoid\left(\sum_j w_{jk}\times o_j\right)\right)^2 $$ where $t$ is the true value and thus is a constant. $o_j$ is the value of the previous node and $w_{jk}$ are the weights that connect $o_j$ to the error node. Trying to work out the derivative of the function myself I get the following result: $$ 2\left(t-\sigmoid\left(\sum_j w_{jk}\times o_j\right)\right)\times \left(\sigmoid\left(\sum_j w_{jk}\times o_j\right)\right)\left(1-\sigmoid\left(\sum_j w_{jk}\times o_j\right)\right) $$ The problem is that the derivative in the book is a bit different and I have no idea why or what I did wrong. The answer in the book has $-2$ and is multiplied by $o_j$ at the end. Where does the $-2$ and $o_j$ in the equation come from? what step of the chain rule did I miss? $$ -2\left(t-\sigmoid\left(\sum_j w_{jk}\times o_j\right)\right)\times \left(\sigmoid\left(\sum_j w_{jk}\times o_j\right)\right)\left(1-\sigmoid\left(\sum_j w_{jk}\times o_j\right)\right)\times o_j $$",,['derivatives']
91,Derivative of square-root Moore-Pseudo-Inverse,Derivative of square-root Moore-Pseudo-Inverse,,"I have a symmetric matrix $A(x)\in \mathbb{R}^{JxJ}$ indexed by $x\in \mathbb{R}^K$. I'am interested in $$\frac{\partial (A(x)^+)^{1/2}}{\partial x_k}$$ where $M^+$ is the Moore-Pseudo-Inverse of the matrix $M$ and $M^{-1/2}$ is the matrix square root. I'm able to compute the derivative of the usual square root inverse of $A$, i.e. $$\frac{\partial A(x)^{-1/2}}{\partial x_k}$$ via the eigenvalue decomposition and its derivatives. However, for numerical reasons I want to switch to the MPI. I also know how to compute the derivative  $$\frac{\partial A(x)^+}{\partial x_k}$$ given by $$-A^+ dA  A^+ + (A^+)^2dA(I-AA^+) + (I -AA^+)dA (A^+)^2.$$ But I don't really see how I can combine these parts. I would be very grateful for any hints.","I have a symmetric matrix $A(x)\in \mathbb{R}^{JxJ}$ indexed by $x\in \mathbb{R}^K$. I'am interested in $$\frac{\partial (A(x)^+)^{1/2}}{\partial x_k}$$ where $M^+$ is the Moore-Pseudo-Inverse of the matrix $M$ and $M^{-1/2}$ is the matrix square root. I'm able to compute the derivative of the usual square root inverse of $A$, i.e. $$\frac{\partial A(x)^{-1/2}}{\partial x_k}$$ via the eigenvalue decomposition and its derivatives. However, for numerical reasons I want to switch to the MPI. I also know how to compute the derivative  $$\frac{\partial A(x)^+}{\partial x_k}$$ given by $$-A^+ dA  A^+ + (A^+)^2dA(I-AA^+) + (I -AA^+)dA (A^+)^2.$$ But I don't really see how I can combine these parts. I would be very grateful for any hints.",,"['matrices', 'derivatives', 'pseudoinverse']"
92,Derivative of positive function is positive close to zero,Derivative of positive function is positive close to zero,,"Assume you have a function $f \mapsto \mathbb R \to \mathbb R$. Let $f\in C^1(\mathbb R)$, $f(0) = 0$, $f(x) > 0$ for $x > 0$. Additionally $f'(0) = 0$. I want to prove that exists a $x^* > 0$ such that $f(x)$ is increasing for $0 \le x < x^*$. I have been toying with the theorem of permanence of sign of the derivative (which is continuos) and the fact that the function must be positive, but I can't quite get there. Is it even true? If not, would be cool to have a counterexample :)","Assume you have a function $f \mapsto \mathbb R \to \mathbb R$. Let $f\in C^1(\mathbb R)$, $f(0) = 0$, $f(x) > 0$ for $x > 0$. Additionally $f'(0) = 0$. I want to prove that exists a $x^* > 0$ such that $f(x)$ is increasing for $0 \le x < x^*$. I have been toying with the theorem of permanence of sign of the derivative (which is continuos) and the fact that the function must be positive, but I can't quite get there. Is it even true? If not, would be cool to have a counterexample :)",,"['calculus', 'real-analysis', 'derivatives']"
93,The length of parametric curve defined by integrals,The length of parametric curve defined by integrals,,"A curve is defined by the parametric equations $$x=\int \limits_{1}^{t}\dfrac{\cos u}{u}du, \ y=\int \limits_{1}^{t}\dfrac{\sin u}{u}du.$$ Find the length of the arc of the curve from the origin to the nearest point where there is a vertical tangent line. My solution: In order to find point where there is a vertical tangent line we need to solve $\dfrac{dx}{dt}=0$ and the nearest point is $\dfrac{\pi}{2}$. Using FTC we have $\sqrt{(x')^2+(y')^2}=\dfrac{1}{t}$. Hence, $$L=\int\limits_{0}^{\pi/2}\dfrac{dt}{t}=\ln t |_{0}^{\pi/2}$$ But $\ln(0)=-\infty$. Can anyone explain why there is a problem here?","A curve is defined by the parametric equations $$x=\int \limits_{1}^{t}\dfrac{\cos u}{u}du, \ y=\int \limits_{1}^{t}\dfrac{\sin u}{u}du.$$ Find the length of the arc of the curve from the origin to the nearest point where there is a vertical tangent line. My solution: In order to find point where there is a vertical tangent line we need to solve $\dfrac{dx}{dt}=0$ and the nearest point is $\dfrac{\pi}{2}$. Using FTC we have $\sqrt{(x')^2+(y')^2}=\dfrac{1}{t}$. Hence, $$L=\int\limits_{0}^{\pi/2}\dfrac{dt}{t}=\ln t |_{0}^{\pi/2}$$ But $\ln(0)=-\infty$. Can anyone explain why there is a problem here?",,"['integration', 'derivatives', 'arc-length']"
94,What is the derivative of the associated Legendre Polynomials at the end points?,What is the derivative of the associated Legendre Polynomials at the end points?,,"I have been searching for different solutions for the derivatives of associated Legendre polynomials at the end points. The associated Legendre polynomial is defined as: $$P^m_l(x)=(-1)^m(1-x^2)^{m/2}\frac{d^m}{dx^m}P_n(x)$$ See matlab's link The derivative of the associated Legendre polynomials can be defined using a recurrence relationship where the derivative is defined by other polynomials within the associated Legendre polynomial family.  In this case I am trying to use the recurrence described on wikipedia's page : $$(x^2-1)\frac{d}{dx}P_l^m(x)=\sqrt{1-x^2}P_l^{m+1}(x)+mxP_l^m(x)$$ To solve for the derivative divide both sides by $(x^2-1)$ and you will see the function goes unbounded at $x= \pm 1$ which are the end points of the function also it can be undefined (i.e., $0/0$) for when $m = 0$ and $x=\pm 1$. How can I compute the derivative at these end points? L'Hopital's rule does not work here. I'm wondering If i can use something like a Laurent series or some kind of series to approximate it but with the end goal of implementing this into code eventually. As I continue my search for solutions has anyone had experience in computing the endpoints of the first derivative of the associated Legendre polynomials?","I have been searching for different solutions for the derivatives of associated Legendre polynomials at the end points. The associated Legendre polynomial is defined as: $$P^m_l(x)=(-1)^m(1-x^2)^{m/2}\frac{d^m}{dx^m}P_n(x)$$ See matlab's link The derivative of the associated Legendre polynomials can be defined using a recurrence relationship where the derivative is defined by other polynomials within the associated Legendre polynomial family.  In this case I am trying to use the recurrence described on wikipedia's page : $$(x^2-1)\frac{d}{dx}P_l^m(x)=\sqrt{1-x^2}P_l^{m+1}(x)+mxP_l^m(x)$$ To solve for the derivative divide both sides by $(x^2-1)$ and you will see the function goes unbounded at $x= \pm 1$ which are the end points of the function also it can be undefined (i.e., $0/0$) for when $m = 0$ and $x=\pm 1$. How can I compute the derivative at these end points? L'Hopital's rule does not work here. I'm wondering If i can use something like a Laurent series or some kind of series to approximate it but with the end goal of implementing this into code eventually. As I continue my search for solutions has anyone had experience in computing the endpoints of the first derivative of the associated Legendre polynomials?",,"['derivatives', 'legendre-polynomials']"
95,Differentiation with respect to matrix,Differentiation with respect to matrix,,I have matrices $W$ and $X$ of dimensions $h\times d$ and $d\times1$ respectively. I want to calculate the partial derivative of $WX$ with respect to $W$. Will that be $X$?,I have matrices $W$ and $X$ of dimensions $h\times d$ and $d\times1$ respectively. I want to calculate the partial derivative of $WX$ with respect to $W$. Will that be $X$?,,"['matrices', 'derivatives', 'vectors', 'partial-derivative', 'implicit-differentiation']"
96,"Bounded derivative has a fixed point at [0,1].","Bounded derivative has a fixed point at [0,1].",,"$f$ is differentiable at $[0,1]$, and $0 \le f'(x) \le 1$. Show that there exists $x\in[0,1]$ such that $f'(x) = x$. So far I've got that if $f'(0)=0$ || $f'(1)=1$ we are done. Otherwise, $f'(0) > 0$ and $f'(1) < 1$. The thing is that $f'$ is not necessarily continuous, therefore I cannot define a continuous $H(x) = f'(x) - x$ and use the Intermediate Value Theorem.","$f$ is differentiable at $[0,1]$, and $0 \le f'(x) \le 1$. Show that there exists $x\in[0,1]$ such that $f'(x) = x$. So far I've got that if $f'(0)=0$ || $f'(1)=1$ we are done. Otherwise, $f'(0) > 0$ and $f'(1) < 1$. The thing is that $f'$ is not necessarily continuous, therefore I cannot define a continuous $H(x) = f'(x) - x$ and use the Intermediate Value Theorem.",,"['calculus', 'real-analysis', 'derivatives']"
97,Please explain how the following derivative graphically makes sense.,Please explain how the following derivative graphically makes sense.,,"I have two vectors $\vec{A}$ and $\vec{B}$ as shown below: The point at the origin of vector $\vec{B}$ has coordinates $(x,y)$. The angle between the two vectors is $\theta$. Now in my physics book there is an expression $\dfrac{\partial(\cos\theta)}{\partial x}$. How does this expression makes sense? At point $(x,y)$, there is a vector $\vec{B}$. But at point $(x+dx,y)$, there should be no vector. By changing $dx$ (i.e. by moving the point from $(x,y)$ to $(x+dx,y)$) we only change the point, not the whole vector. If someone says that the whole vector needs to be moved, how can we prove it mathematically? Edit: Simplified version of a part of the treatise","I have two vectors $\vec{A}$ and $\vec{B}$ as shown below: The point at the origin of vector $\vec{B}$ has coordinates $(x,y)$. The angle between the two vectors is $\theta$. Now in my physics book there is an expression $\dfrac{\partial(\cos\theta)}{\partial x}$. How does this expression makes sense? At point $(x,y)$, there is a vector $\vec{B}$. But at point $(x+dx,y)$, there should be no vector. By changing $dx$ (i.e. by moving the point from $(x,y)$ to $(x+dx,y)$) we only change the point, not the whole vector. If someone says that the whole vector needs to be moved, how can we prove it mathematically? Edit: Simplified version of a part of the treatise",,"['calculus', 'derivatives', 'vectors', 'coordinate-systems', 'angle']"
98,derivative of multiplication of three matrices with a matrix,derivative of multiplication of three matrices with a matrix,,"In short, my problem is to compute $\frac{d(X^tAX)}{dX}$; where both $A$ and $X$ are matrices. I have to maximize a negative-log likelihood function $L$ $$L = \frac{1}{2}\ln(|\Sigma|)+\frac{1}{2}\varepsilon^t\Sigma^{-1}\varepsilon;$$ where $\Sigma$ is the covariance matrix, $\varepsilon$ is a column vector of residuals (in my case) and $t$ denotes the transponse. The probelm is that $\Sigma$ is a function of other matrices $$\Sigma = J^tCJ$$ where, both $J$ and $C$ are matrices. The matrix $J$ is again a function of a vector $\lambda$. I have to maximize the function $L$ w.r.t. vector $\lambda$. I tried chain rule to solve this problem as follows $$\frac{dL}{d\lambda}=0\implies\frac{dJ}{d\lambda}\frac{d\Sigma}{dJ}\frac{dL}{d\Sigma}=0.$$ In the above equation, $\frac{dJ}{d\lambda}$ and $\frac{d\Sigma}{dJ}$ becomes a tensor. So, I am no longer able to write these quantities on paper. Also, taking derivative of $L$ w.r.t. an individual element of $ \lambda$ does not solve the problem. There are few online resources which suggest using $vec$ operator to deal with tensors; but they heavily use Kronecker product etc. which I have not been able to understand very well, because most of the online material is very opaque. Can someone please point me to the solution? If someone can refer a good text dealing with a similar problem, that would be great.","In short, my problem is to compute $\frac{d(X^tAX)}{dX}$; where both $A$ and $X$ are matrices. I have to maximize a negative-log likelihood function $L$ $$L = \frac{1}{2}\ln(|\Sigma|)+\frac{1}{2}\varepsilon^t\Sigma^{-1}\varepsilon;$$ where $\Sigma$ is the covariance matrix, $\varepsilon$ is a column vector of residuals (in my case) and $t$ denotes the transponse. The probelm is that $\Sigma$ is a function of other matrices $$\Sigma = J^tCJ$$ where, both $J$ and $C$ are matrices. The matrix $J$ is again a function of a vector $\lambda$. I have to maximize the function $L$ w.r.t. vector $\lambda$. I tried chain rule to solve this problem as follows $$\frac{dL}{d\lambda}=0\implies\frac{dJ}{d\lambda}\frac{d\Sigma}{dJ}\frac{dL}{d\Sigma}=0.$$ In the above equation, $\frac{dJ}{d\lambda}$ and $\frac{d\Sigma}{dJ}$ becomes a tensor. So, I am no longer able to write these quantities on paper. Also, taking derivative of $L$ w.r.t. an individual element of $ \lambda$ does not solve the problem. There are few online resources which suggest using $vec$ operator to deal with tensors; but they heavily use Kronecker product etc. which I have not been able to understand very well, because most of the online material is very opaque. Can someone please point me to the solution? If someone can refer a good text dealing with a similar problem, that would be great.",,"['derivatives', 'matrix-calculus']"
99,Prove that $F(x) = \sum_{n=1}^\infty f\left(\frac{x}{n}\right)$ is smooth,Prove that  is smooth,F(x) = \sum_{n=1}^\infty f\left(\frac{x}{n}\right),"Let $f$ be a smooth function, for which $f(0)=0=f'(0)$. Prove that for $x\in\mathbb  R$ and $$F(x) = \sum_{n=1}^\infty f\left(\frac{x}{n}\right),$$ $F(x) \in \mathbb R$ and $F$ is smooth. I wanted to prove first that $F$ is differentiable. $F(0)$ is convergent. I don't really have a clue about the rest of the task.","Let $f$ be a smooth function, for which $f(0)=0=f'(0)$. Prove that for $x\in\mathbb  R$ and $$F(x) = \sum_{n=1}^\infty f\left(\frac{x}{n}\right),$$ $F(x) \in \mathbb R$ and $F$ is smooth. I wanted to prove first that $F$ is differentiable. $F(0)$ is convergent. I don't really have a clue about the rest of the task.",,"['sequences-and-series', 'derivatives', 'convergence-divergence']"
