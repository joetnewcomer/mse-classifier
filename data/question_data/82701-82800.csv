,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Singular Value Decomposition of Commuting Matrices,Singular Value Decomposition of Commuting Matrices,,"If two square matrices $M_1$ and $M_2$ commute, does it mean that the $U$ and $V^\dagger$ appearing in their singular value decompositions are the same? Specifically, does it imply that $$M_1 = U \Sigma_1 V^\dagger$$ $$M_2 = U \Sigma_2 V^\dagger$$ for two possibly different $\Sigma_{1,2}$? As $M_1$ and $M_2$ commute, I know that they can be simultaneously diagonalized, which means there's an orthogonal transformation $\mathcal{O}$ which can diagonalize both matrices. But does this have any bearing on $U$ and $V$ of the singular value decomposition?","If two square matrices $M_1$ and $M_2$ commute, does it mean that the $U$ and $V^\dagger$ appearing in their singular value decompositions are the same? Specifically, does it imply that $$M_1 = U \Sigma_1 V^\dagger$$ $$M_2 = U \Sigma_2 V^\dagger$$ for two possibly different $\Sigma_{1,2}$? As $M_1$ and $M_2$ commute, I know that they can be simultaneously diagonalized, which means there's an orthogonal transformation $\mathcal{O}$ which can diagonalize both matrices. But does this have any bearing on $U$ and $V$ of the singular value decomposition?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-decomposition']"
1,Does subtracting a positive semi-definite diagonal matrix from a Hurwitz matrix keep it Hurwitz?,Does subtracting a positive semi-definite diagonal matrix from a Hurwitz matrix keep it Hurwitz?,,"I am having a linear algebra problem here. I will be grateful if someone can help me. Let $A\in \mathbb{R}^{n\times n}$ be Hurwitz and diagonizable, and let $B$ be a diagonal matrix whose diagonal elements are non-negative. Is $A-B$ still Hurwitz? I know that if $B=cI$, where $c$ is a positive scalar, $A-B$ is a Hurwitz matrix. However, I am not sure whether $A-B$ is still a Hurwitz matrix when some diagonal elements in $B$ are zero and the others are positive. Are there any general results on the similar topic? Thanks in advance!","I am having a linear algebra problem here. I will be grateful if someone can help me. Let $A\in \mathbb{R}^{n\times n}$ be Hurwitz and diagonizable, and let $B$ be a diagonal matrix whose diagonal elements are non-negative. Is $A-B$ still Hurwitz? I know that if $B=cI$, where $c$ is a positive scalar, $A-B$ is a Hurwitz matrix. However, I am not sure whether $A-B$ is still a Hurwitz matrix when some diagonal elements in $B$ are zero and the others are positive. Are there any general results on the similar topic? Thanks in advance!",,"['linear-algebra', 'matrices', 'hurwitz-matrices']"
2,find all matrices that com-mute with the given matrix,find all matrices that com-mute with the given matrix,,"So I have this matrix and I need to find  all matrices that commute with the given matrix A.\begin{bmatrix}2 & 3\\-3 & 2\end{bmatrix} I know how to get to the point where $2a+3c = 2a-3b$ $2b+ 3d = 3a + 2b$ $-3a + 2c = 2c - 3d$ $-3b + 2d = 3c + 2d$ it's just the dot product What confuses me is the rest of the solution. According to the book, $c = -b$ $a = d$ $a = d$ $b = -c$ So the solution would be \begin{bmatrix}a & b\\-b & a\end{bmatrix} How do you get $c = -b,\ a = d,\ a = d,\ b = -c$ and the matrix from the equations above? I think is something simple I might be missing.","So I have this matrix and I need to find  all matrices that commute with the given matrix A.\begin{bmatrix}2 & 3\\-3 & 2\end{bmatrix} I know how to get to the point where $2a+3c = 2a-3b$ $2b+ 3d = 3a + 2b$ $-3a + 2c = 2c - 3d$ $-3b + 2d = 3c + 2d$ it's just the dot product What confuses me is the rest of the solution. According to the book, $c = -b$ $a = d$ $a = d$ $b = -c$ So the solution would be \begin{bmatrix}a & b\\-b & a\end{bmatrix} How do you get $c = -b,\ a = d,\ a = d,\ b = -c$ and the matrix from the equations above? I think is something simple I might be missing.",,"['linear-algebra', 'matrices']"
3,Is square-root of a real symmetric positive semi-definite matrix real as well?,Is square-root of a real symmetric positive semi-definite matrix real as well?,,"Given a real symmetric positive semi-definite matrix $A$, will there be a root $R$ which is real symmetric positive semi-definite as well? Can you comment on it's uniqueness? It will be nice if you can give proofs as well.","Given a real symmetric positive semi-definite matrix $A$, will there be a root $R$ which is real symmetric positive semi-definite as well? Can you comment on it's uniqueness? It will be nice if you can give proofs as well.",,"['linear-algebra', 'matrices']"
4,Two rows corresponding to adjacent vertices of a tree cannot both be linear combinations of the other rows,Two rows corresponding to adjacent vertices of a tree cannot both be linear combinations of the other rows,,"Let $T$ be a tree and $A$ its corresponding adjacency matrix. Let $u,v \in V(T)$ and $u$ is adjacent to $v$. If the row corresponding to $u$ is a linear combination of the other rows of $A$, then prove that the row corresponding to $v$ cannot be written   as a linear combination of the other rows of $A$. I can prove it when $u$ is pendant but couldn't prove it in general. Any help is appreciated. Example Take $T$ as path on $7$ vertices, $u=3$ and $v=2$ or $5$. Here $R_3=R_1+R_5-R_7$ but we can see that $R_2$ can't be written in any linear combination of rows in $A$. Same is the case with $R_4$.","Let $T$ be a tree and $A$ its corresponding adjacency matrix. Let $u,v \in V(T)$ and $u$ is adjacent to $v$. If the row corresponding to $u$ is a linear combination of the other rows of $A$, then prove that the row corresponding to $v$ cannot be written   as a linear combination of the other rows of $A$. I can prove it when $u$ is pendant but couldn't prove it in general. Any help is appreciated. Example Take $T$ as path on $7$ vertices, $u=3$ and $v=2$ or $5$. Here $R_3=R_1+R_5-R_7$ but we can see that $R_2$ can't be written in any linear combination of rows in $A$. Same is the case with $R_4$.",,"['linear-algebra', 'matrices', 'graph-theory']"
5,nullity of infinite matrix A equals nullity of $A^T$?,nullity of infinite matrix A equals nullity of ?,A^T,"Suppose you have an infinite matrix A with real entries. I know the dimension of the null space of A. Question 1) if the dimension of the null space is a finite number k, is the dimension of the null space of $A^T$ also k? Question 2) if the dimension of the null space of A is infinite, is the dimension of the null space of $A^T$ also infinite? Thanks!","Suppose you have an infinite matrix A with real entries. I know the dimension of the null space of A. Question 1) if the dimension of the null space is a finite number k, is the dimension of the null space of $A^T$ also k? Question 2) if the dimension of the null space of A is infinite, is the dimension of the null space of $A^T$ also infinite? Thanks!",,"['linear-algebra', 'matrices', 'functional-analysis']"
6,Minimize $\operatorname{tr}(X^TA^TAX(X^T(I-P)X)^{-1})$ by solving an eigenproblem?,Minimize  by solving an eigenproblem?,\operatorname{tr}(X^TA^TAX(X^T(I-P)X)^{-1}),"My optimization problem is $$\min_X\operatorname{tr}(X^TA^TAX(X^T(I-P)X)^{-1}),$$ where $P$ is a projection matrix. I was told this could be solved as an eigenproblem: columns of $X^*$ (the solution) are eigenvectors of $(A^TA)^{-1}(I-P)$, but I failed to see why. The form looks so familiar to me that I feel I am just lacking one last bit to reach the solution. Update -- Thanks to @AWashburn, I realize my projection matrix $P$ is symmetric, and equivalently, the projection is orthogonal .","My optimization problem is $$\min_X\operatorname{tr}(X^TA^TAX(X^T(I-P)X)^{-1}),$$ where $P$ is a projection matrix. I was told this could be solved as an eigenproblem: columns of $X^*$ (the solution) are eigenvectors of $(A^TA)^{-1}(I-P)$, but I failed to see why. The form looks so familiar to me that I feel I am just lacking one last bit to reach the solution. Update -- Thanks to @AWashburn, I realize my projection matrix $P$ is symmetric, and equivalently, the projection is orthogonal .",,"['matrices', 'optimization', 'eigenvalues-eigenvectors']"
7,Operator norm under shrinkage,Operator norm under shrinkage,,"If I have a $n$-dim matrix $A=\{a_{ij}\}$, and I multiply each elements by a factor $w_{ij}$ in $[0,1]$, and get a new matrix $A_w=\{a_{ij}w_{ij}\}$. Do I have $$||A||\ge \lVert A_w\rVert$$ where the norm is the operator norm, i.e. the largest eigenvalue of $A$? If not, what if we add some conditions, say, $A$ is elementwise positive $A$ is PSD $A$ is symmetric $w_{ij}$ is 1 on major diagonals, and decreases to 0 gradually to the northeast, and southwest corner? Thanks!","If I have a $n$-dim matrix $A=\{a_{ij}\}$, and I multiply each elements by a factor $w_{ij}$ in $[0,1]$, and get a new matrix $A_w=\{a_{ij}w_{ij}\}$. Do I have $$||A||\ge \lVert A_w\rVert$$ where the norm is the operator norm, i.e. the largest eigenvalue of $A$? If not, what if we add some conditions, say, $A$ is elementwise positive $A$ is PSD $A$ is symmetric $w_{ij}$ is 1 on major diagonals, and decreases to 0 gradually to the northeast, and southwest corner? Thanks!",,"['matrices', 'inequality', 'normed-spaces']"
8,"$p(x) \in \mathbb R[x]$ be non-constant polynomial , $n>1$ , the function $A \to p(A)$ is surjective on $M(n, \mathbb C)$?","be non-constant polynomial ,  , the function  is surjective on ?","p(x) \in \mathbb R[x] n>1 A \to p(A) M(n, \mathbb C)","Let $p(x) \in \mathbb R[x]$ be a non-constant polynomial and $n>1$  , then is it true that the function $f:M(n,\mathbb C) \to M(n, \mathbb C)$ defined as $f(A)=p(A) , \forall A \in M(n, \mathbb C)$ is surjective ? ( I only know that the function is not surjective on $M(n,\mathbb R)$ when degree of $p(x)$ is even , but I don't know when we allow complex matrices )","Let $p(x) \in \mathbb R[x]$ be a non-constant polynomial and $n>1$  , then is it true that the function $f:M(n,\mathbb C) \to M(n, \mathbb C)$ defined as $f(A)=p(A) , \forall A \in M(n, \mathbb C)$ is surjective ? ( I only know that the function is not surjective on $M(n,\mathbb R)$ when degree of $p(x)$ is even , but I don't know when we allow complex matrices )",,"['linear-algebra', 'matrices']"
9,Is there a name for matrices that are symmetric along the cross diagonal? [duplicate],Is there a name for matrices that are symmetric along the cross diagonal? [duplicate],,"This question already has an answer here : Name of a special matrix (1 answer) Closed 7 years ago . Something like $$ A= \begin{bmatrix}     a & b & c\\     b & d & e\\     c & e & f \end{bmatrix} $$ would be a symmetric matrix because the values are reflected along the diagonal, and $A=A^\intercal$ Is there a name for a matrix that's symmetric along the cross diagonal? Something like $$ B= \begin{bmatrix}     c & b & a\\     e & d & b\\     f & e & c \end{bmatrix} $$","This question already has an answer here : Name of a special matrix (1 answer) Closed 7 years ago . Something like $$ A= \begin{bmatrix}     a & b & c\\     b & d & e\\     c & e & f \end{bmatrix} $$ would be a symmetric matrix because the values are reflected along the diagonal, and $A=A^\intercal$ Is there a name for a matrix that's symmetric along the cross diagonal? Something like $$ B= \begin{bmatrix}     c & b & a\\     e & d & b\\     f & e & c \end{bmatrix} $$",,"['matrices', 'terminology', 'symmetric-matrices']"
10,$(AB=BA\wedge A^*Bx=0)\implies BA^*x=0$?,?,(AB=BA\wedge A^*Bx=0)\implies BA^*x=0,"Let $X^*$ mean the conjugate transpose of matrix $X.$ I am given two matrices $A,B$ and a vector $x$ such that $AB=BA$ and $A^*Bx=0.$ Does $BA^*x=0$ then? It may look out of context, but such a property would help me with proving ellipticity of some differential operator. Unfortunately I am not handy with matrix algebra. If the answer is no, would such additional assumptions like those below help? $$A^*B^*x=0\hspace{10pt}ABx=0\hspace{10pt}B^*A=0$$","Let $X^*$ mean the conjugate transpose of matrix $X.$ I am given two matrices $A,B$ and a vector $x$ such that $AB=BA$ and $A^*Bx=0.$ Does $BA^*x=0$ then? It may look out of context, but such a property would help me with proving ellipticity of some differential operator. Unfortunately I am not handy with matrix algebra. If the answer is no, would such additional assumptions like those below help? $$A^*B^*x=0\hspace{10pt}ABx=0\hspace{10pt}B^*A=0$$",,"['linear-algebra', 'matrices']"
11,The inner product matrix with zero determinant implies that all the vectors are linearly dependent.,The inner product matrix with zero determinant implies that all the vectors are linearly dependent.,,"Let $(V,\langle \cdot,\cdot\rangle)$ be an inner product space over the real field $\mathbb{R}$ and $v_1,\dots,v_n\in V$. Suppose that $A=(a_{ij})\in M_n(\mathbb{R)}$ with $a_{ij}=\langle v_i,v_j\rangle$ for all $1\leq i,j\leq n$. Show that if $\det A=0$, then $v_1,\dots,v_n$ are linearly dependent in $V$. Here I try to work out for $M_2(\mathbb{R})$. So $$A=\begin{pmatrix}\langle v_1,v_1\rangle &\langle v_1,v_2\rangle\\\langle v_1,v_2\rangle &\langle v_i,v_j\rangle\end{pmatrix}$$ Since its determinant is zero, we get $$||v_1||^2||v_2||^2-\langle v_1,v_2\rangle^2=0$$ So here I don't have idea how to show that $v_1=\alpha v_2$ for some $\alpha\in \mathbb{R}$.","Let $(V,\langle \cdot,\cdot\rangle)$ be an inner product space over the real field $\mathbb{R}$ and $v_1,\dots,v_n\in V$. Suppose that $A=(a_{ij})\in M_n(\mathbb{R)}$ with $a_{ij}=\langle v_i,v_j\rangle$ for all $1\leq i,j\leq n$. Show that if $\det A=0$, then $v_1,\dots,v_n$ are linearly dependent in $V$. Here I try to work out for $M_2(\mathbb{R})$. So $$A=\begin{pmatrix}\langle v_1,v_1\rangle &\langle v_1,v_2\rangle\\\langle v_1,v_2\rangle &\langle v_i,v_j\rangle\end{pmatrix}$$ Since its determinant is zero, we get $$||v_1||^2||v_2||^2-\langle v_1,v_2\rangle^2=0$$ So here I don't have idea how to show that $v_1=\alpha v_2$ for some $\alpha\in \mathbb{R}$.",,"['linear-algebra', 'matrices']"
12,Is $\ln \det F$ a rank one convex function of $F$,Is  a rank one convex function of,\ln \det F F,"Let $F=(F_{ij})$ be a matrix where $i,j \in \{1,2,...,d\}$ . $F_{ij} \in \mathbb{R}$ . And $F$ is taken to be invertible. Is $\ln \det F$ a convex function of $F$ ? Or is it a rank-one convex function of $F$ ? Since I thought it is rank-one convex, but I now find ${\partial \ln \det F \over \partial F_{i\alpha } \partial F_{j \beta} }\xi_i \xi_j \eta_\alpha \eta_\beta  = (\xi F^{-1} \eta)^2$ may not be greater than $c |\xi|^2 |\eta|^2$ . Is there some error in my calculation? And if we suppose $F$ to be positive-definite, could we say $\ln \det F$ is a convex function of $F$ ?","Let be a matrix where . . And is taken to be invertible. Is a convex function of ? Or is it a rank-one convex function of ? Since I thought it is rank-one convex, but I now find may not be greater than . Is there some error in my calculation? And if we suppose to be positive-definite, could we say is a convex function of ?","F=(F_{ij}) i,j \in \{1,2,...,d\} F_{ij} \in \mathbb{R} F \ln \det F F F {\partial \ln \det F \over \partial F_{i\alpha } \partial F_{j \beta} }\xi_i \xi_j \eta_\alpha \eta_\beta  = (\xi F^{-1} \eta)^2 c |\xi|^2 |\eta|^2 F \ln \det F F","['linear-algebra', 'matrices']"
13,How to show bound ${\rm Tr} ( {\bf A} ({\bf I}+ b {\bf A})^{-1}) \le \frac{n {\rm Tr}(A)}{n +b {\rm Tr}(A) }$,How to show bound,{\rm Tr} ( {\bf A} ({\bf I}+ b {\bf A})^{-1}) \le \frac{n {\rm Tr}(A)}{n +b {\rm Tr}(A) },Let ${\bf A} \in \mathbb{R}^{n \times n}$ be symmetric positive definite. Can one prove the following inequality for some positive constant $b$? \begin{align} {\rm Tr} ( {\bf A} ({\bf I}+ b {\bf A})^{-1}) \le \frac{n {\rm Tr}({\bf A})}{n +b {\rm Tr}({\bf A}) } \end{align} Edit: Based on one of the answers below the question boils down to showing the following inequality \begin{align} \sum_{i=1}^n \frac{d_i}{1+b d_i } \le \frac{ n \sum_{i=1}^n d_i}{\sum_{i=1}^n  (1+b d_i) } \end{align} where $d_i \ge 0$.,Let ${\bf A} \in \mathbb{R}^{n \times n}$ be symmetric positive definite. Can one prove the following inequality for some positive constant $b$? \begin{align} {\rm Tr} ( {\bf A} ({\bf I}+ b {\bf A})^{-1}) \le \frac{n {\rm Tr}({\bf A})}{n +b {\rm Tr}({\bf A}) } \end{align} Edit: Based on one of the answers below the question boils down to showing the following inequality \begin{align} \sum_{i=1}^n \frac{d_i}{1+b d_i } \le \frac{ n \sum_{i=1}^n d_i}{\sum_{i=1}^n  (1+b d_i) } \end{align} where $d_i \ge 0$.,,"['linear-algebra', 'matrices']"
14,Let $A$ and $B$ be matrices 2014 by 2014. Matrix $A$ is invertible. Is this equation possible: $AB-BA=A$?,Let  and  be matrices 2014 by 2014. Matrix  is invertible. Is this equation possible: ?,A B A AB-BA=A,"Problem: Let $A$ and $B$ be matrices 2014 by 2014. Matrix $A$ is invertible. Is this equation possible: $AB-BA=A$? Explain your answer. I was studying for exams and I found this problem. Since it says that matrix $A$ is invertible I tried multiplying it with $A^{-1}$ or assuming $A$ is orthogonal but, sadly, I see no connection. I appreciate any help.","Problem: Let $A$ and $B$ be matrices 2014 by 2014. Matrix $A$ is invertible. Is this equation possible: $AB-BA=A$? Explain your answer. I was studying for exams and I found this problem. Since it says that matrix $A$ is invertible I tried multiplying it with $A^{-1}$ or assuming $A$ is orthogonal but, sadly, I see no connection. I appreciate any help.",,"['linear-algebra', 'matrices']"
15,Eigenvalue of a matrix and a polynomial of that matrix,Eigenvalue of a matrix and a polynomial of that matrix,,"Let $A$ be a $n \times n$ matrix over $F$, and let $c_1, ... c_n$ be its eigenvalues. Show that for every polynomial $g(x) \in F[x]$, the eigenvalues of $g(A)$ are $g(c_1), ... , g(c_n)$. I think by making $A$ an upper triangular matrix the question might be solver easily. But I have no idea how to show how many times each eigenvalue has been repeated.","Let $A$ be a $n \times n$ matrix over $F$, and let $c_1, ... c_n$ be its eigenvalues. Show that for every polynomial $g(x) \in F[x]$, the eigenvalues of $g(A)$ are $g(c_1), ... , g(c_n)$. I think by making $A$ an upper triangular matrix the question might be solver easily. But I have no idea how to show how many times each eigenvalue has been repeated.",,"['linear-algebra', 'matrices', 'polynomials', 'eigenvalues-eigenvectors']"
16,Prove two complex matrices have null trace,Prove two complex matrices have null trace,,"Let $A,B \in \mathbb{C}^{2 \times 2} \setminus \{O_2\}$, where $AB=-BA$ and $\det(A+B)=0$. Prove that $\operatorname{tr}(A) = \operatorname{tr}(B) = 0$ (where $\operatorname{tr}$ is the trace). My attempt Let $C=A+B$. Then, using the Cayley–Hamilton for $C$: $C^2 -Tr(C)C + \det(C)I_2=0$ therefore $C^2=Tr(C)C$ and $(A+B)^2=(Tr(A) + Tr(B))(A+B)$ then $$A^2 + B^2=(Tr(A) + Tr(B))(A+B) \tag1$$ Using Cayley–Hamilton for $A$ and $B$ in (1) one gets:  $$ -\det(A)I_2 -\det(B)I_2=Tr(A)B + Tr(B)A \tag2$$ Applying trace to (2): $$-\det(A) -\det(B)=Tr(A)Tr(B) \tag3$$ So I have to prove $\det(A)=-\det(B)$ and here I got stuck. Update 1 Showing $Tr(A)=Tr(B)$ is enough to finish the proof. Update 2 Following an idea by @Dietrich Burde Suppose $A$ is invertible. Then $B=A^{-1}(-B)A$ and $Tr(B)=Tr(A^{-1}(-B)A)=Tr(A^{-1}(-BA))=Tr((-BA)A^{-1})=Tr(-B)=-Tr(B)$. Therefore $Tr(B)=0$. Now, from (3) one gets $\det(A)=-\det(B)$ then from (2) $Tr(A)=0$ Similar for $B$ invertible. If both matrixes are singular then, from (3) one of $Tr(A)$ or $Tr(B)$ is zero and from (2) the other one is also zero.","Let $A,B \in \mathbb{C}^{2 \times 2} \setminus \{O_2\}$, where $AB=-BA$ and $\det(A+B)=0$. Prove that $\operatorname{tr}(A) = \operatorname{tr}(B) = 0$ (where $\operatorname{tr}$ is the trace). My attempt Let $C=A+B$. Then, using the Cayley–Hamilton for $C$: $C^2 -Tr(C)C + \det(C)I_2=0$ therefore $C^2=Tr(C)C$ and $(A+B)^2=(Tr(A) + Tr(B))(A+B)$ then $$A^2 + B^2=(Tr(A) + Tr(B))(A+B) \tag1$$ Using Cayley–Hamilton for $A$ and $B$ in (1) one gets:  $$ -\det(A)I_2 -\det(B)I_2=Tr(A)B + Tr(B)A \tag2$$ Applying trace to (2): $$-\det(A) -\det(B)=Tr(A)Tr(B) \tag3$$ So I have to prove $\det(A)=-\det(B)$ and here I got stuck. Update 1 Showing $Tr(A)=Tr(B)$ is enough to finish the proof. Update 2 Following an idea by @Dietrich Burde Suppose $A$ is invertible. Then $B=A^{-1}(-B)A$ and $Tr(B)=Tr(A^{-1}(-B)A)=Tr(A^{-1}(-BA))=Tr((-BA)A^{-1})=Tr(-B)=-Tr(B)$. Therefore $Tr(B)=0$. Now, from (3) one gets $\det(A)=-\det(B)$ then from (2) $Tr(A)=0$ Similar for $B$ invertible. If both matrixes are singular then, from (3) one of $Tr(A)$ or $Tr(B)$ is zero and from (2) the other one is also zero.",,['matrices']
17,Showing a map is nilpotent,Showing a map is nilpotent,,"Let $\mathbf{A},\mathbf{B}\in\mathrm{M}_n(\mathbb{R})$ such that $\mathbf{A}$ invertible and diagonalisable, and $\mathbf{AB}=\lambda \mathbf{BA}$ for some $\lambda >1$. I want to show that $\mathbf{B}$ is nilpotent i.e. $\mathbf{B}^k=0$  for some $k$. What I have so far: The condition implies $\mathbf{AB}^m=\lambda^m \mathbf{B}^m\mathbf{A}$. Taking a basis in which $\mathbf{A}$ is diagonal, and writing $b_{ij}^{(m)}$ as the $ij$th entry in $\mathbf{B}^m$, we have: $$a_ib_{ij}^{(m)}= \lambda^m a_j b_{ij}^{(m)}\implies b_{ij}^{(m)}=\frac{1}{\lambda^m a_i\big/a_j-1}\overset{m\to\infty}\longrightarrow 0$$  where $a_j\neq 0$ since $\mathbf{A}$ invertible. So $\mathbf{B}^m\to 0$ but it's not clear to me why the sequence must actually hit $0$? Would appreciate some help.","Let $\mathbf{A},\mathbf{B}\in\mathrm{M}_n(\mathbb{R})$ such that $\mathbf{A}$ invertible and diagonalisable, and $\mathbf{AB}=\lambda \mathbf{BA}$ for some $\lambda >1$. I want to show that $\mathbf{B}$ is nilpotent i.e. $\mathbf{B}^k=0$  for some $k$. What I have so far: The condition implies $\mathbf{AB}^m=\lambda^m \mathbf{B}^m\mathbf{A}$. Taking a basis in which $\mathbf{A}$ is diagonal, and writing $b_{ij}^{(m)}$ as the $ij$th entry in $\mathbf{B}^m$, we have: $$a_ib_{ij}^{(m)}= \lambda^m a_j b_{ij}^{(m)}\implies b_{ij}^{(m)}=\frac{1}{\lambda^m a_i\big/a_j-1}\overset{m\to\infty}\longrightarrow 0$$  where $a_j\neq 0$ since $\mathbf{A}$ invertible. So $\mathbf{B}^m\to 0$ but it's not clear to me why the sequence must actually hit $0$? Would appreciate some help.",,"['linear-algebra', 'matrices', 'diagonalization', 'nilpotence']"
18,Determinant of determinant is determinant?,Determinant of determinant is determinant?,,"Looking at this question , I am thinking to consider the map $R\to M_n(R)$ where $R$ is a ring, sending $r\in R$ to $rI_n\in M_n(R).$ Then this induces a map. $$f:M_n(R)\rightarrow M_n(M_n(R))$$  Then we consider another map $g:M_n(M_n(R))\rightarrow M_{n^2}(R)$ sending, e.g. $$\begin{pmatrix} \begin{pmatrix}1&0\\0&1\end{pmatrix}&\begin{pmatrix}2&1\\3&0\end{pmatrix}\\ \begin{pmatrix} 0&0\\0&0 \end{pmatrix}&\begin{pmatrix} 2&3\\5&2\end{pmatrix} \end{pmatrix}$$ to $$\begin{pmatrix}1&0&2&1\\ 0&1&3&0\\ 0&0&2&3\\ 0&0&5&2\end{pmatrix}.$$ Is it true that $$\det_{M_n(R)}(\det_{M_n(M_n(R))}A)=\det_{M_{n^2}(R)}g(A)$$ for some properly-defined determinant on $M_n(M_n(R))?$ If this is true, then $\det_{M_n(R)}\operatorname{ch}_{A}(B)=\det_{M_n(R)}\circ\det_{M_n(M_n(R))}(f(A)-B\cdot I_{M_n(M_n(R))})=\det_{M_{n^2}}\circ g(f(A)-B\cdot I_{M_{n^2}(R)}),$ which is what OP of the linked question wants to prove. Any hint or reference is greatly appreciated, thanks in advance. P.S. @user1551 pointed out that determinant is defined on commutative rings only and $M_n(R)$ is in general a non-commutative ring. So I am thinking maybe we could use the Dieudonné determinant . In any case, I changed the question accordingly.","Looking at this question , I am thinking to consider the map $R\to M_n(R)$ where $R$ is a ring, sending $r\in R$ to $rI_n\in M_n(R).$ Then this induces a map. $$f:M_n(R)\rightarrow M_n(M_n(R))$$  Then we consider another map $g:M_n(M_n(R))\rightarrow M_{n^2}(R)$ sending, e.g. $$\begin{pmatrix} \begin{pmatrix}1&0\\0&1\end{pmatrix}&\begin{pmatrix}2&1\\3&0\end{pmatrix}\\ \begin{pmatrix} 0&0\\0&0 \end{pmatrix}&\begin{pmatrix} 2&3\\5&2\end{pmatrix} \end{pmatrix}$$ to $$\begin{pmatrix}1&0&2&1\\ 0&1&3&0\\ 0&0&2&3\\ 0&0&5&2\end{pmatrix}.$$ Is it true that $$\det_{M_n(R)}(\det_{M_n(M_n(R))}A)=\det_{M_{n^2}(R)}g(A)$$ for some properly-defined determinant on $M_n(M_n(R))?$ If this is true, then $\det_{M_n(R)}\operatorname{ch}_{A}(B)=\det_{M_n(R)}\circ\det_{M_n(M_n(R))}(f(A)-B\cdot I_{M_n(M_n(R))})=\det_{M_{n^2}}\circ g(f(A)-B\cdot I_{M_{n^2}(R)}),$ which is what OP of the linked question wants to prove. Any hint or reference is greatly appreciated, thanks in advance. P.S. @user1551 pointed out that determinant is defined on commutative rings only and $M_n(R)$ is in general a non-commutative ring. So I am thinking maybe we could use the Dieudonné determinant . In any case, I changed the question accordingly.",,"['linear-algebra', 'matrices', 'determinant']"
19,non abelian von Neumann algebras,non abelian von Neumann algebras,,"I'm not familiar with von Neumann algebras, but I need the following fact (if it's true) for an other proof. Let $H$ be a Hilbert space, $A\subseteq L(H)$ a non abelian von Neumann algebra. Must $A$ contain a von Neumann subalgebra which is isomorphic to $M_2(\mathbb{C})$? I think that $M_2(\mathbb{C})$ is the smallest non abelian von Neumann algebra. I'm not sure if it there is an embedding of $M_2(\mathbb{C})$ into $A$. Does anybody how to justify it? There must be a non abelian von Neumann subalgebra of $A$ of dimension 4 if the answer of my question is yes.","I'm not familiar with von Neumann algebras, but I need the following fact (if it's true) for an other proof. Let $H$ be a Hilbert space, $A\subseteq L(H)$ a non abelian von Neumann algebra. Must $A$ contain a von Neumann subalgebra which is isomorphic to $M_2(\mathbb{C})$? I think that $M_2(\mathbb{C})$ is the smallest non abelian von Neumann algebra. I'm not sure if it there is an embedding of $M_2(\mathbb{C})$ into $A$. Does anybody how to justify it? There must be a non abelian von Neumann subalgebra of $A$ of dimension 4 if the answer of my question is yes.",,"['matrices', 'operator-algebras', 'von-neumann-algebras']"
20,Find a power of matrix by Cayley-Hamilton theorem,Find a power of matrix by Cayley-Hamilton theorem,,Let $$A=         \begin{pmatrix}         0 & 1 & 0 \\         1 & 0 & 1 \\         1 & 0 & 0 \\         \end{pmatrix} $$ And I should calculate $A^2$ and $A^{12}$ by Cayley Hamilton theorem. I found that the characteristic polynomial is $f_A(x)=x^3-x-1$ and thus by Cayley Hamilton: $A^3-A=I_{3}$ . I tried to multiply by $A^9$ but it didn't lead to something simple to express $A^2$ and $A^{12}$ by. Any suggestions?,Let $$A=         \begin{pmatrix}         0 & 1 & 0 \\         1 & 0 & 1 \\         1 & 0 & 0 \\         \end{pmatrix} $$ And I should calculate $A^2$ and $A^{12}$ by Cayley Hamilton theorem. I found that the characteristic polynomial is $f_A(x)=x^3-x-1$ and thus by Cayley Hamilton: $A^3-A=I_{3}$ . I tried to multiply by $A^9$ but it didn't lead to something simple to express $A^2$ and $A^{12}$ by. Any suggestions?,,"['linear-algebra', 'matrices', 'cayley-hamilton']"
21,How do I find the Jordan normal form when I only have one eigenvalue?,How do I find the Jordan normal form when I only have one eigenvalue?,,I have matrix $A=\begin{pmatrix}3 &1 \\ -1 &1 \end{pmatrix}$. I have found that the eigenvalue is $2$ and the eigenvector is $\begin{pmatrix}1\\ -1\end{pmatrix}$. How do I find $T$ so that I can calculate $T^{-1}AT=J$? How to form $T$ when I have only one eigenvector?,I have matrix $A=\begin{pmatrix}3 &1 \\ -1 &1 \end{pmatrix}$. I have found that the eigenvalue is $2$ and the eigenvector is $\begin{pmatrix}1\\ -1\end{pmatrix}$. How do I find $T$ so that I can calculate $T^{-1}AT=J$? How to form $T$ when I have only one eigenvector?,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'jordan-normal-form']"
22,Eigenvalues of matrices of order $n$,Eigenvalues of matrices of order,n,"How to find eigenvalues of following matrices of order $n$? $$A=\begin{bmatrix}  1 & 1 & 1 & 1 & \cdots & 1 \\ 1 & 0 & 1 & 1 & \cdots & 1 \\ 1 & 1 & 0 & 1 & \cdots & 1 \\ 1 & 1 & 1 & 0 & \cdots & 1 \\ \vdots & \vdots & \vdots & \vdots &\ddots & \vdots \\ 1 & 1 & 1 & 1 & \cdots & 0 \\ \end{bmatrix}_n$$ $$B=\begin{bmatrix}  -1 & 1 & 1 & 1 & \cdots & 1 \\ 1 & 0 & 1 & 1 & \cdots & 1 \\ 1 & 1 & 0 & 1 & \cdots & 1 \\ 1 & 1 & 1 & 0 & \cdots & 1 \\ \vdots & \vdots & \vdots & \vdots &\ddots & \vdots \\ 1 & 1 & 1 & 1 & \cdots & 0 \\ \end{bmatrix}_n$$ I had apply matrix calculator to find eigenvalues and I found that exactly $n-2$ eigenvalues of both matrices are $-1$,but I was not able to find rest of the eigenvalues.","How to find eigenvalues of following matrices of order $n$? $$A=\begin{bmatrix}  1 & 1 & 1 & 1 & \cdots & 1 \\ 1 & 0 & 1 & 1 & \cdots & 1 \\ 1 & 1 & 0 & 1 & \cdots & 1 \\ 1 & 1 & 1 & 0 & \cdots & 1 \\ \vdots & \vdots & \vdots & \vdots &\ddots & \vdots \\ 1 & 1 & 1 & 1 & \cdots & 0 \\ \end{bmatrix}_n$$ $$B=\begin{bmatrix}  -1 & 1 & 1 & 1 & \cdots & 1 \\ 1 & 0 & 1 & 1 & \cdots & 1 \\ 1 & 1 & 0 & 1 & \cdots & 1 \\ 1 & 1 & 1 & 0 & \cdots & 1 \\ \vdots & \vdots & \vdots & \vdots &\ddots & \vdots \\ 1 & 1 & 1 & 1 & \cdots & 0 \\ \end{bmatrix}_n$$ I had apply matrix calculator to find eigenvalues and I found that exactly $n-2$ eigenvalues of both matrices are $-1$,but I was not able to find rest of the eigenvalues.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
23,"When diagonalizing a matrix, in what order should you arrange the the eigenvectors to form the invertible matrix $P$?","When diagonalizing a matrix, in what order should you arrange the the eigenvectors to form the invertible matrix ?",P,"I was following this example online to diagonalize a matrix. It lists the eigenvectors as $\lambda =3,2,4$ (note the order). It then arranges each eigenvalue's corresponding eigenvector (3 column vectors) in the same order and puts them together to form the invertible matrix $P$ to solve $A=PDP^{-1}$. My question is: does it matter what order the eigenvectors are put in to form $P$? Or does it not matter because $P$ will always be invertible? Or is it purely a trial and error thing?","I was following this example online to diagonalize a matrix. It lists the eigenvectors as $\lambda =3,2,4$ (note the order). It then arranges each eigenvalue's corresponding eigenvector (3 column vectors) in the same order and puts them together to form the invertible matrix $P$ to solve $A=PDP^{-1}$. My question is: does it matter what order the eigenvectors are put in to form $P$? Or does it not matter because $P$ will always be invertible? Or is it purely a trial and error thing?",,"['matrices', 'eigenvalues-eigenvectors', 'inverse', 'diagonalization']"
24,"Linear Map of an ellipsoid in $\mathbb{R}^N$ into another ellipsoid in $\mathbb{R}^n$, with $n<N$","Linear Map of an ellipsoid in  into another ellipsoid in , with",\mathbb{R}^N \mathbb{R}^n n<N,"Starting from the closed set describing an ellipsoid in $\mathbb{R}^N$: $$\Omega_x = \{  x \in \mathbb{R}^N : (x-x_0)^T\Sigma_x^{-1}(x-x_0) \leq \varepsilon^2 \}$$ where $\Sigma_x \in \mathbb{R}^{N\times N}$ is a symmetric positive definite square matrix and $x_0 \in \mathbb{R}^N$, I need to find a way a to proove that its image through the linear map $\varphi: \mathbb{R}^N \rightarrow \mathbb{R}^n,\; n<N$, defined as the product with a full rank rectangular matrix: $$y = \varphi(x) := Px, \; P\in\mathbb{R}^{n\times N},\;\rho(P)=n$$ is still an ellipsoid, in $\mathbb{R}^n$ of course, defined in this way: $$\Omega_y = \varphi(\Omega_x) = \{y \in \mathbb{R}^n: y = Px, \;x \in \Omega_x\} = \{y \in \mathbb{R}^n : (y-y_0)^T\Sigma_y^{-1}(y-y_0) \leq \varepsilon^2 \}$$ with: $$y_0 = Px_0 \in \mathbb{R}^n$$ $$\Sigma_y = P\Sigma_yP^T \in \mathbb{R}^{n\times n}$$","Starting from the closed set describing an ellipsoid in $\mathbb{R}^N$: $$\Omega_x = \{  x \in \mathbb{R}^N : (x-x_0)^T\Sigma_x^{-1}(x-x_0) \leq \varepsilon^2 \}$$ where $\Sigma_x \in \mathbb{R}^{N\times N}$ is a symmetric positive definite square matrix and $x_0 \in \mathbb{R}^N$, I need to find a way a to proove that its image through the linear map $\varphi: \mathbb{R}^N \rightarrow \mathbb{R}^n,\; n<N$, defined as the product with a full rank rectangular matrix: $$y = \varphi(x) := Px, \; P\in\mathbb{R}^{n\times N},\;\rho(P)=n$$ is still an ellipsoid, in $\mathbb{R}^n$ of course, defined in this way: $$\Omega_y = \varphi(\Omega_x) = \{y \in \mathbb{R}^n: y = Px, \;x \in \Omega_x\} = \{y \in \mathbb{R}^n : (y-y_0)^T\Sigma_y^{-1}(y-y_0) \leq \varepsilon^2 \}$$ with: $$y_0 = Px_0 \in \mathbb{R}^n$$ $$\Sigma_y = P\Sigma_yP^T \in \mathbb{R}^{n\times n}$$",,"['matrices', 'algebraic-geometry', 'linear-transformations', 'quadrics', 'system-identification']"
25,Matrix --> Scalar Valued Function: Differentiation,Matrix --> Scalar Valued Function: Differentiation,,"In class, we called a real-valued function from the space of matrices to the reals $f: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}$ differentiable at $\mathbf{X}$ if: $$\lim_{\mathbf{H} \to \mathbf{0_{m \times n}}} \frac{\lvert\lvert f(\mathbf{X} + \mathbf{H}) - f(\mathbf{X}) - tr([\nabla f(\mathbf{X})^T]\mathbf{H})\rvert \rvert}{\lvert\lvert \mathbf{H}\rvert\rvert} = 0$$ where the gradient is the transpose of the total derivative. In this definition of differentiability, I'm trying to understand the intuition behind using the $tr(\cdot)$. In this case, is it true that the total derivative $\mathscr{D}f$ is a map $\mathscr{D}f : \mathbb{R}^{m \times n} \rightarrow \mathscr{L}(\mathbb{R}^{m \times n},\mathbb{R})$? If the above is true, then $tr([\mathscr{D}f](\mathbf{H})) = [\mathscr{D}f](\mathbf{H})$ since $[\mathscr{D}f](\mathbf{H})$ is a real number. So do we take the trace just for convenience in manipulating the algebraic expressions of the matrices? Could we use $det(\cdot)$ instead? Thanks.","In class, we called a real-valued function from the space of matrices to the reals $f: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}$ differentiable at $\mathbf{X}$ if: $$\lim_{\mathbf{H} \to \mathbf{0_{m \times n}}} \frac{\lvert\lvert f(\mathbf{X} + \mathbf{H}) - f(\mathbf{X}) - tr([\nabla f(\mathbf{X})^T]\mathbf{H})\rvert \rvert}{\lvert\lvert \mathbf{H}\rvert\rvert} = 0$$ where the gradient is the transpose of the total derivative. In this definition of differentiability, I'm trying to understand the intuition behind using the $tr(\cdot)$. In this case, is it true that the total derivative $\mathscr{D}f$ is a map $\mathscr{D}f : \mathbb{R}^{m \times n} \rightarrow \mathscr{L}(\mathbb{R}^{m \times n},\mathbb{R})$? If the above is true, then $tr([\mathscr{D}f](\mathbf{H})) = [\mathscr{D}f](\mathbf{H})$ since $[\mathscr{D}f](\mathbf{H})$ is a real number. So do we take the trace just for convenience in manipulating the algebraic expressions of the matrices? Could we use $det(\cdot)$ instead? Thanks.",,"['matrices', 'derivatives', 'intuition', 'linear-transformations']"
26,For which complex parameters the following matrix is diagonalizable,For which complex parameters the following matrix is diagonalizable,,"For all possible complex values of the parameter $\lambda$ , determine if the matrix $A$ is diagonalizable and if so find an invertible matrix $C$ and a diagonal matrix $D$ so that $C^{-1}$$DC=A$ $A$ =  $\left( \begin{array}{ccc} 1 & i & 0 \\ 0 & \lambda & 1 \\ 0 & 0 & i \end{array} \right)$ To begin with, I am not even sure what are the parameters I should be working with for $\lambda$ and then how should I proceed with them?","For all possible complex values of the parameter $\lambda$ , determine if the matrix $A$ is diagonalizable and if so find an invertible matrix $C$ and a diagonal matrix $D$ so that $C^{-1}$$DC=A$ $A$ =  $\left( \begin{array}{ccc} 1 & i & 0 \\ 0 & \lambda & 1 \\ 0 & 0 & i \end{array} \right)$ To begin with, I am not even sure what are the parameters I should be working with for $\lambda$ and then how should I proceed with them?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
27,Finding the inverse of linear transformation using matrix,Finding the inverse of linear transformation using matrix,,"Assuming I have a linear transformation represented by a matrix with respect to some random bases, how could I find the inverse of the transformation using the matrix representation? I know I should find the inverse matrix but from there on, I have no clue what to do. Thanks in advance!","Assuming I have a linear transformation represented by a matrix with respect to some random bases, how could I find the inverse of the transformation using the matrix representation? I know I should find the inverse matrix but from there on, I have no clue what to do. Thanks in advance!",,"['linear-algebra', 'matrices', 'inverse', 'linear-transformations']"
28,The Lie algebra of the generalized unitary group $\{g \in GL_n(\mathbb{C}) : gS\bar{g}^t=S\}$ is $\{XS+S\overline{X}{}^t=0\}$,The Lie algebra of the generalized unitary group  is,\{g \in GL_n(\mathbb{C}) : gS\bar{g}^t=S\} \{XS+S\overline{X}{}^t=0\},"Let $ S \in M_n(\mathbb{C}) $ be a square matrix and let $  X$ be in the Lie algebra $\mathbb{\mu(S)}  $ of the generalized unitary group,   $$U(S):=\{g \in GL_n(\mathbb{C}); gS\bar{g}^t=S\} .$$   Show that $$XS+S\overline{X^t}=0 .$$ I need to find the relationship between elements of the Lie algebra and the Lie group I know that the Lie algebra of a linear Lie group can be characterized in terms of the exponential matrix as follows: $\mathbb{g} \rightarrow G$ through the map $X \rightarrow \exp(X)=\sum{\frac{X^k}{k!}}$ So in the above case $X \in \mathbb{\mu}$ and $\exp(X)$ Where could I go from here?","Let $ S \in M_n(\mathbb{C}) $ be a square matrix and let $  X$ be in the Lie algebra $\mathbb{\mu(S)}  $ of the generalized unitary group,   $$U(S):=\{g \in GL_n(\mathbb{C}); gS\bar{g}^t=S\} .$$   Show that $$XS+S\overline{X^t}=0 .$$ I need to find the relationship between elements of the Lie algebra and the Lie group I know that the Lie algebra of a linear Lie group can be characterized in terms of the exponential matrix as follows: $\mathbb{g} \rightarrow G$ through the map $X \rightarrow \exp(X)=\sum{\frac{X^k}{k!}}$ So in the above case $X \in \mathbb{\mu}$ and $\exp(X)$ Where could I go from here?",,"['abstract-algebra', 'matrices', 'representation-theory', 'lie-groups', 'lie-algebras']"
29,How to design a matrix with multiple chosen eigenvalues?,How to design a matrix with multiple chosen eigenvalues?,,"I want to ""design"" (build) a matrix that have multiple known eigenvalues. For these apriori chosen eigenvalues, I want to know the corresponding eigenvectors, too. The point is that I want to start from the eigenvalues, chose the eigenvectors appropriately and then compute the matrix. All values must be integers. I need this for a cryptographic application. Namely, I'm looking for something similar to Chinese remainder theorem , but done with eigenvalues and eigenvectors. To make an idea, in paper Fully Homomorphic SIMD Operations the authors propose an improvement upon previous schemes. They choose a specific cyclotomic polynomial which factorizes in n polynomials and so they can encrypt n message at a time. I want the same but for the approximate eigenvector problem .","I want to ""design"" (build) a matrix that have multiple known eigenvalues. For these apriori chosen eigenvalues, I want to know the corresponding eigenvectors, too. The point is that I want to start from the eigenvalues, chose the eigenvectors appropriately and then compute the matrix. All values must be integers. I need this for a cryptographic application. Namely, I'm looking for something similar to Chinese remainder theorem , but done with eigenvalues and eigenvectors. To make an idea, in paper Fully Homomorphic SIMD Operations the authors propose an improvement upon previous schemes. They choose a specific cyclotomic polynomial which factorizes in n polynomials and so they can encrypt n message at a time. I want the same but for the approximate eigenvector problem .",,"['matrices', 'eigenvalues-eigenvectors', 'cyclotomic-polynomials']"
30,Find a normal matrix with a given characteristic polynomial,Find a normal matrix with a given characteristic polynomial,,"Find a normal matrix with characteristic polynomial $t^2 + 4$ and eigenspace $E_{2i} = span  {(1 \ 3i)^t}$ Since any vector in different eignespaces are perpendicular to each other, so i compute $E_{-2i} = span  {(-3 \ i)^t}$. Then I conclude that the matrix is \begin{matrix}         1 & 3i  \\         -3 & i\\         \end{matrix} Then when I try to calculate back the char. polynomial, I didn't get back the same ans. What's wrong with my ans? Thank you!","Find a normal matrix with characteristic polynomial $t^2 + 4$ and eigenspace $E_{2i} = span  {(1 \ 3i)^t}$ Since any vector in different eignespaces are perpendicular to each other, so i compute $E_{-2i} = span  {(-3 \ i)^t}$. Then I conclude that the matrix is \begin{matrix}         1 & 3i  \\         -3 & i\\         \end{matrix} Then when I try to calculate back the char. polynomial, I didn't get back the same ans. What's wrong with my ans? Thank you!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
31,Is it faster to calculate inverses of symmetric matrices as opposed to asymmetric matrices? How? [closed],Is it faster to calculate inverses of symmetric matrices as opposed to asymmetric matrices? How? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 20 days ago . Improve this question I know there are several methods to inverse or decompose matrices. I am looking for a comparison of the computational cost of inverting an arbitrary real, symmetric matrix vs a real, asymmetric one. I am familiar with the choices for optimizing the decomposition of asymmetric matrices. Is there a faster one when you are certain you are handling symmetric ones?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 20 days ago . Improve this question I know there are several methods to inverse or decompose matrices. I am looking for a comparison of the computational cost of inverting an arbitrary real, symmetric matrix vs a real, asymmetric one. I am familiar with the choices for optimizing the decomposition of asymmetric matrices. Is there a faster one when you are certain you are handling symmetric ones?",,"['linear-algebra', 'matrices', 'optimization', 'matrix-decomposition']"
32,Can anyone explain why commuting matrices share common eigenvector? [duplicate],Can anyone explain why commuting matrices share common eigenvector? [duplicate],,"This question already has answers here : Matrices commute if and only if they share a common basis of eigenvectors? (5 answers) Do commuting matrices share the same eigenvectors? (8 answers) Closed 8 years ago . If two matrices $A$ and $B$ commute with each other, why would they share some eigenvector? Does that mean that an eigenvector for $A$ is also an eigenvector for $B$ and vice-versa?","This question already has answers here : Matrices commute if and only if they share a common basis of eigenvectors? (5 answers) Do commuting matrices share the same eigenvectors? (8 answers) Closed 8 years ago . If two matrices $A$ and $B$ commute with each other, why would they share some eigenvector? Does that mean that an eigenvector for $A$ is also an eigenvector for $B$ and vice-versa?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
33,"in a non-full rank matrix, how to find the dependent rows especifically in MATLAB?","in a non-full rank matrix, how to find the dependent rows especifically in MATLAB?",,"I have a very large matrix, which its determinant is zero, and hence it is not full rank. Now I wonder is there a way to find which rows are linearly dependent? specifically in MatLab is there any command which outputs the desired result?","I have a very large matrix, which its determinant is zero, and hence it is not full rank. Now I wonder is there a way to find which rows are linearly dependent? specifically in MatLab is there any command which outputs the desired result?",,"['linear-algebra', 'matrices', 'determinant', 'matlab', 'matrix-rank']"
34,A random invertible matrix,A random invertible matrix,,"I work on a project, for these project i need to generate a square random invertible matrix. I found out how to generate a square random matrix, still i want to be sure that this is an invertible one, without having to compute the determinant or to generate this matrix multiple times, can you please give me a tip ?","I work on a project, for these project i need to generate a square random invertible matrix. I found out how to generate a square random matrix, still i want to be sure that this is an invertible one, without having to compute the determinant or to generate this matrix multiple times, can you please give me a tip ?",,"['matrices', 'random']"
35,Sum of a row in a matrix,Sum of a row in a matrix,,I have two matrices $A \in M^{m \times n}(\mathbb{R}) $ and $ B \in M^{n \times p} (\mathbb{R}).$ For $A$ it says that sum of every row in $A$ is $a$ and sum of every row in $B$ is $b.$ I have to show that the product $AB$ has a constant sum of rows. I started with matrix  $$A= \begin{bmatrix} e_{11}& e_{12} & \cdots & \cdots & e_{1n} \\ e_{21}& \cdots & \cdots & \cdots & e_{2n} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ e_{m1} & \cdots & \cdots & \cdots & e_{mn} \end{bmatrix} $$ and  $$B =\begin{bmatrix} e_{11}& e_{12} & \cdots & \cdots & e_{1p} \\ e_{21}& \cdots & \cdots & \cdots & e_{2p} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ e_{n1} & \cdots & \cdots & \cdots & e_{np} \end{bmatrix} $$ When I calculate the product $AB$ I get that the sum of the first row is: $$[e_{11} e_{11}+e_{12}e_{21}+\cdots+e_{1n}e_{n1}]+[e_{11} e_{12}+e_{12}e_{22}+\cdots+e_{1n}e_{n2}]+\cdots+[e_{11} e_{1p}+e_{12}e_{2p}+\cdots+e_{1n}e_{np}]$$ I know that $e_{11}+e_{12}+\cdots+e_{1n}=a$ and $e_{11}+e_{12}+\cdots+e_{1p}=b $ and also for other rows but I don't know how to continue. Thanks for any help.,I have two matrices $A \in M^{m \times n}(\mathbb{R}) $ and $ B \in M^{n \times p} (\mathbb{R}).$ For $A$ it says that sum of every row in $A$ is $a$ and sum of every row in $B$ is $b.$ I have to show that the product $AB$ has a constant sum of rows. I started with matrix  $$A= \begin{bmatrix} e_{11}& e_{12} & \cdots & \cdots & e_{1n} \\ e_{21}& \cdots & \cdots & \cdots & e_{2n} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ e_{m1} & \cdots & \cdots & \cdots & e_{mn} \end{bmatrix} $$ and  $$B =\begin{bmatrix} e_{11}& e_{12} & \cdots & \cdots & e_{1p} \\ e_{21}& \cdots & \cdots & \cdots & e_{2p} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ e_{n1} & \cdots & \cdots & \cdots & e_{np} \end{bmatrix} $$ When I calculate the product $AB$ I get that the sum of the first row is: $$[e_{11} e_{11}+e_{12}e_{21}+\cdots+e_{1n}e_{n1}]+[e_{11} e_{12}+e_{12}e_{22}+\cdots+e_{1n}e_{n2}]+\cdots+[e_{11} e_{1p}+e_{12}e_{2p}+\cdots+e_{1n}e_{np}]$$ I know that $e_{11}+e_{12}+\cdots+e_{1n}=a$ and $e_{11}+e_{12}+\cdots+e_{1p}=b $ and also for other rows but I don't know how to continue. Thanks for any help.,,"['linear-algebra', 'matrices']"
36,"Finding the basis of $\mathfrak{so}(2,2)$ (Lie-Algebra of $SO(2,2)$)",Finding the basis of  (Lie-Algebra of ),"\mathfrak{so}(2,2) SO(2,2)","I want to calculate the Basis of the Lie-Algebra $\mathfrak{so}(2,2)$. My idea was, to use a similar Argument as in this Question . The $SO(2,2)$ is defined by: $$ SO(2,2) := \left\{ X \in Mat_4(\mathbb R): X^t\eta X = \eta,\; \det(X) = 1 \right\} $$ (With $\eta = diag(1,1,-1,-1)$) With the argument from the link, i get the following equation: $\forall X \in \mathfrak{so(2,2)}$: $$ X^t\eta + \eta X = 0. $$ My idea was to use the block decomposition: $$ X = \left(\begin{matrix} A & B\\ C & D \end{matrix} \right), \;  \eta = \left(\begin{matrix} \mathbb I & 0\\ 0 & -\mathbb I\\ \end{matrix}\right). $$ I get the following equation: $$ \left(\begin{matrix} A^t & -B^t\\ C^t & -D^t\\ \end{matrix}\right) + \left(\begin{matrix} A & B\\ C & D \end{matrix}\right) = 0. $$ Is this correct? I also don't really know, what to do with the $det(X) = 1$ condition.","I want to calculate the Basis of the Lie-Algebra $\mathfrak{so}(2,2)$. My idea was, to use a similar Argument as in this Question . The $SO(2,2)$ is defined by: $$ SO(2,2) := \left\{ X \in Mat_4(\mathbb R): X^t\eta X = \eta,\; \det(X) = 1 \right\} $$ (With $\eta = diag(1,1,-1,-1)$) With the argument from the link, i get the following equation: $\forall X \in \mathfrak{so(2,2)}$: $$ X^t\eta + \eta X = 0. $$ My idea was to use the block decomposition: $$ X = \left(\begin{matrix} A & B\\ C & D \end{matrix} \right), \;  \eta = \left(\begin{matrix} \mathbb I & 0\\ 0 & -\mathbb I\\ \end{matrix}\right). $$ I get the following equation: $$ \left(\begin{matrix} A^t & -B^t\\ C^t & -D^t\\ \end{matrix}\right) + \left(\begin{matrix} A & B\\ C & D \end{matrix}\right) = 0. $$ Is this correct? I also don't really know, what to do with the $det(X) = 1$ condition.",,"['matrices', 'lie-groups', 'lie-algebras', 'matrix-equations']"
37,Intuitive understanding of vector / matrix calculcation,Intuitive understanding of vector / matrix calculcation,,"I am currently dealing with calculations done on vectors and matrices. For some parts I have gained an intuitive understanding, for others I didn't. E.g., when we are adding two vectors, you can imagine that this means adding two arrows. The result is a single arrow that reflects the combined forces of the two individual source vectors. The resulting vector will probably have a new direction, which is influenced by the two original ones. When we multiply a vector by a scalar, you can imagine that this means putting the very same arrow multiple times behind itself, to make it longer. I.e., the new vector has more force, but the direction stay the same. Now… if I want to multiply a matrix by a vector, what is the analogy for that? What does this mean in terms of geometry?","I am currently dealing with calculations done on vectors and matrices. For some parts I have gained an intuitive understanding, for others I didn't. E.g., when we are adding two vectors, you can imagine that this means adding two arrows. The result is a single arrow that reflects the combined forces of the two individual source vectors. The resulting vector will probably have a new direction, which is influenced by the two original ones. When we multiply a vector by a scalar, you can imagine that this means putting the very same arrow multiple times behind itself, to make it longer. I.e., the new vector has more force, but the direction stay the same. Now… if I want to multiply a matrix by a vector, what is the analogy for that? What does this mean in terms of geometry?",,"['matrices', 'vectors']"
38,"If $x^{T}By = 1$, should $\operatorname{Tr}(Byx^{T}) = 1$?","If , should ?",x^{T}By = 1 \operatorname{Tr}(Byx^{T}) = 1,"would appreciate any hints with this question: Assume $x$, $y$ are both $n \times 1$ vectors, and that $B$ is $n\times n$. Given that $x^{T}By = 1$, should $\operatorname{Tr}(Byx^{T}) = 1$ ? Thank you very much.","would appreciate any hints with this question: Assume $x$, $y$ are both $n \times 1$ vectors, and that $B$ is $n\times n$. Given that $x^{T}By = 1$, should $\operatorname{Tr}(Byx^{T}) = 1$ ? Thank you very much.",,['matrices']
39,Is every linear operator which is $SO(n)$-invariant necessarily isotropic?,Is every linear operator which is -invariant necessarily isotropic?,SO(n),"Let $M_n$ be the vector space of $n \times n$ real matrices. We say a linear operator $\alpha:M_n \to M_n$ is hemitropic* if: $(*) \, \, \alpha(S^TXS)=S^T\alpha(X)S \, , \, \forall S \in SO(n)$ and isotropic , if the above formula holds $\forall S \in O(n)$ My question: Is every hemitropic operator necessarily isotropic? Does the answer change if we assume $\alpha$ is injective? In odd dimensions, the answer is yes, since for any $Q \in O(n) \setminus SO(n)$, $\det(-Q)=1$ so $-Q \in SO(n)$ and by (*): $\alpha(Q^TXQ)=\alpha\big((-Q)^TX(-Q)\big)=(-Q)^T\alpha(X)(-Q)=Q^T\alpha(X)Q$ I suspect the answer in even dimensions is negative, but so far I didn't find a way to construct a hemitropic non-isotropic operator. (Though I guess this can be done even in dimension $2$). Update: For dimension $2$ I have constructed (see answer below) a hemitropic non-isotropic operator. However, my construction relied on the fact that all $2$-dimensional rotation commute. This is not the case for higher dimensions. (See here ). This leaves open the question for even dimensions above two. The terminology ""hemitropic"" comes from Elasticity theory.","Let $M_n$ be the vector space of $n \times n$ real matrices. We say a linear operator $\alpha:M_n \to M_n$ is hemitropic* if: $(*) \, \, \alpha(S^TXS)=S^T\alpha(X)S \, , \, \forall S \in SO(n)$ and isotropic , if the above formula holds $\forall S \in O(n)$ My question: Is every hemitropic operator necessarily isotropic? Does the answer change if we assume $\alpha$ is injective? In odd dimensions, the answer is yes, since for any $Q \in O(n) \setminus SO(n)$, $\det(-Q)=1$ so $-Q \in SO(n)$ and by (*): $\alpha(Q^TXQ)=\alpha\big((-Q)^TX(-Q)\big)=(-Q)^T\alpha(X)(-Q)=Q^T\alpha(X)Q$ I suspect the answer in even dimensions is negative, but so far I didn't find a way to construct a hemitropic non-isotropic operator. (Though I guess this can be done even in dimension $2$). Update: For dimension $2$ I have constructed (see answer below) a hemitropic non-isotropic operator. However, my construction relied on the fact that all $2$-dimensional rotation commute. This is not the case for higher dimensions. (See here ). This leaves open the question for even dimensions above two. The terminology ""hemitropic"" comes from Elasticity theory.",,"['linear-algebra', 'matrices', 'linear-transformations', 'rotations', 'symmetry']"
40,Linear Least Squares with Linear Equality Constraints - Iterative Solver,Linear Least Squares with Linear Equality Constraints - Iterative Solver,,"I am looking for iterative procedures for solution of the linear least squares problems with linear equality constraints. The Problem: $$ \arg \min_{x} \frac{1}{2} \left\| A x - b \right\|_{2}^{2}, \quad \text{subject to} \quad B x = d $$ How can best the two systems can be combined so that iterative procedures can be applied on it?","I am looking for iterative procedures for solution of the linear least squares problems with linear equality constraints. The Problem: $$ \arg \min_{x} \frac{1}{2} \left\| A x - b \right\|_{2}^{2}, \quad \text{subject to} \quad B x = d $$ How can best the two systems can be combined so that iterative procedures can be applied on it?",,"['matrices', 'convex-optimization', 'numerical-linear-algebra', 'least-squares', 'numerical-optimization']"
41,"If $A$ be nonnegative and tridiagonal . Can we say that, all the eigenvalues of $A$ are real?","If  be nonnegative and tridiagonal . Can we say that, all the eigenvalues of  are real?",A A,"Suppose  $A\in M_n$ is nonnegative(all $a_{ij}\ge0$) and tridiagonal. Can we say  that, all the eigenvalues of $A$ are real?","Suppose  $A\in M_n$ is nonnegative(all $a_{ij}\ge0$) and tridiagonal. Can we say  that, all the eigenvalues of $A$ are real?",,"['linear-algebra', 'matrices']"
42,What is the numerical range of $A$?,What is the numerical range of ?,A,Let $A = \left( {\begin{array}{*{20}{c}}    0 & 0 & 0 & 1  \\    0 & 0 & 1 & 0  \\    0 & 1 & 0 & 0  \\    1 & 0 & 0 & 0  \\ \end{array}} \right)$ What is the  numerical range of $A$?,Let $A = \left( {\begin{array}{*{20}{c}}    0 & 0 & 0 & 1  \\    0 & 0 & 1 & 0  \\    0 & 1 & 0 & 0  \\    1 & 0 & 0 & 0  \\ \end{array}} \right)$ What is the  numerical range of $A$?,,"['linear-algebra', 'matrices']"
43,Multivariable chain rule: how to take this derivative with respect to a matrix?,Multivariable chain rule: how to take this derivative with respect to a matrix?,,"I have a simple model and I want to update the parameters using a gradient descent algorithm. Thus I must find derivative with respect to my parameters. Here is my model: $$s = Wx + b$$ $$a = max(0, s)$$ $$t = Ma + c$$ $$f = {1 \over 2}\Sigma(t_i - y_i)^2$$ Where: $x$ is vector of length $n$, $b$ is vector length $m$, $W$ is size $(m,n)$, $s$, $a$ are vectors of length $m$, $c$ is length $p$, $M$ is size $(p, m)$, $t$, $y$, $c$ are vectors of length $p$, $f$ is a real-valued (scalar output) function. I am minimizing $f$, this is my ""loss function"". I need to update the parameter $W$, so I need something like ""${df \over dW}$"", whatever that quantity is. I suppose it must be a matrix of the same size so I can do something like $W = W - \gamma{df \over dW}$ in my program. What is a systematic way of deriving this quantity? I know the chain rule will be involved, and I understand how to take gradients, but I have never taken a derivative of a scalar function with respect to a matrix. This example is illustrative of a larger model I'm working on. Thanks!","I have a simple model and I want to update the parameters using a gradient descent algorithm. Thus I must find derivative with respect to my parameters. Here is my model: $$s = Wx + b$$ $$a = max(0, s)$$ $$t = Ma + c$$ $$f = {1 \over 2}\Sigma(t_i - y_i)^2$$ Where: $x$ is vector of length $n$, $b$ is vector length $m$, $W$ is size $(m,n)$, $s$, $a$ are vectors of length $m$, $c$ is length $p$, $M$ is size $(p, m)$, $t$, $y$, $c$ are vectors of length $p$, $f$ is a real-valued (scalar output) function. I am minimizing $f$, this is my ""loss function"". I need to update the parameter $W$, so I need something like ""${df \over dW}$"", whatever that quantity is. I suppose it must be a matrix of the same size so I can do something like $W = W - \gamma{df \over dW}$ in my program. What is a systematic way of deriving this quantity? I know the chain rule will be involved, and I understand how to take gradients, but I have never taken a derivative of a scalar function with respect to a matrix. This example is illustrative of a larger model I'm working on. Thanks!",,"['matrices', 'derivatives', 'vectors', 'chain-rule']"
44,Finding the number of sub-grids of a matrix,Finding the number of sub-grids of a matrix,,"I am trying to find the number of $M\times M$ sub-grids given an $N\times N$ matrix, where $M \leq N$ . For any concrete example it is easy to find the correct answer: Eg: $$\left( \begin{array}{ccc} 4 & 3 & 8\\ 9 & 5 & 1\\  2 & 7 & 6\\ \end{array} \right)$$ This is a $3 \times 3$ matrix: The number of $1 \times 1$ sub-grids is 9; the number of $2 \times 2$ sub-grids is 4; and the number of $3 \times 3$ sub-grids is 1. I am trying to find a general formula to find the number of sub-grids. So far I have been looking at the dimensions of each: A $4 \times 4$ matrix will have: $$16 \quad (1 \times 1)$$ $$9 \quad (2 \times 2)$$ $$4 \quad (3 \times 3)$$ $$1 \quad (4 \times 4)$$ sub-grids. By trial I have concluded that the number of sub-grids can be calculated by: $$({N-M+1})^2$$ Could someone please confirm that this is a correct conclusion and maybe give some intuition to why is so? (e.g. the number of shifts it is possible to make with the sub-grids in the matrix)","I am trying to find the number of sub-grids given an matrix, where . For any concrete example it is easy to find the correct answer: Eg: This is a matrix: The number of sub-grids is 9; the number of sub-grids is 4; and the number of sub-grids is 1. I am trying to find a general formula to find the number of sub-grids. So far I have been looking at the dimensions of each: A matrix will have: sub-grids. By trial I have concluded that the number of sub-grids can be calculated by: Could someone please confirm that this is a correct conclusion and maybe give some intuition to why is so? (e.g. the number of shifts it is possible to make with the sub-grids in the matrix)","M\times M N\times N M \leq N \left( \begin{array}{ccc}
4 & 3 & 8\\
9 & 5 & 1\\ 
2 & 7 & 6\\
\end{array} \right) 3 \times 3 1 \times 1 2 \times 2 3 \times 3 4 \times 4 16 \quad (1 \times 1) 9 \quad (2 \times 2) 4 \quad (3 \times 3) 1 \quad (4 \times 4) ({N-M+1})^2",['matrices']
45,Cardinality of set of all orthogonal matrices over $\Bbb Z$ [duplicate],Cardinality of set of all orthogonal matrices over  [duplicate],\Bbb Z,"This question already has answers here : Find a real orthogonal matrix of order $3$ , other than +- I_3 ,having all integer elements. [closed] (3 answers) Closed 8 years ago . Let $S$ be the set of all $3\times 3$ matrices over $\Bbb Z$ such that the product $AA^t=I$. Then find $|S|$? Taking $A=$\begin{bmatrix} a & b &c \\d & e& f\\g & h & i\end{bmatrix} we get $3$ relations $a^2+b^2+c^2=1;d^2+e^2+f^2=1;g^2+h^2+i^2=1;$ How should I count from here?","This question already has answers here : Find a real orthogonal matrix of order $3$ , other than +- I_3 ,having all integer elements. [closed] (3 answers) Closed 8 years ago . Let $S$ be the set of all $3\times 3$ matrices over $\Bbb Z$ such that the product $AA^t=I$. Then find $|S|$? Taking $A=$\begin{bmatrix} a & b &c \\d & e& f\\g & h & i\end{bmatrix} we get $3$ relations $a^2+b^2+c^2=1;d^2+e^2+f^2=1;g^2+h^2+i^2=1;$ How should I count from here?",,"['matrices', 'orthogonality']"
46,system of equations when the matrix corresponding $\det(A)=\pm1$ has integers solution,system of equations when the matrix corresponding  has integers solution,\det(A)=\pm1,I am reading a book about continued fractions and one of the theorem's proof constructs a system of linear equations and states that  the matrix corresponding with the system of equations satisfies $\det(A)=\pm1$  and hence has integers solution why is that so?,I am reading a book about continued fractions and one of the theorem's proof constructs a system of linear equations and states that  the matrix corresponding with the system of equations satisfies $\det(A)=\pm1$  and hence has integers solution why is that so?,,"['linear-algebra', 'matrices', 'determinant', 'systems-of-equations']"
47,Efficiency of Matrix Multiplication; least number of operations,Efficiency of Matrix Multiplication; least number of operations,,"In which order should the following product of four matrices ABCD be evaluated so as to perform the least number of operations? A is a 1 by 5 matrix, B is a 5 by 100 matrix, C is a 100 by 10 matrix, D is a 10 by 5 matrix. I have what seems to be conflicting information on how to solve this problem. Research on the internet leads me to believe that I compute the efficiency one way, however my professor seems to have given me an entirely different and conflicting formula. I'm not sure if two different approaches are used two solve two slightly different wordings of similar questions, like solving for total number of operations versus solving for total number of multiplications. From what I have read on the internet, I believe that the answer to this problem would be (((AB)C)D) is the order with the least amount of operations, with a total number of operations calculated at 1550. To elaborate on my conflicting sources of information, here is an example. Given the above values, if we were to multiply (AB)C: The internet has shown me the # of operations is (1)(5)(10) + (1)(100)(10). Contrast this with my professor's in class example of: (1)(100)(2(5) - 1) + (1)(10)(2(100) - 1). These are clearly very different approaches to solving this problem, and I am wondering where my misunderstanding is. Are these two approaches solving a different problem entirely (such as number of multiplications vs number of operations)? Or is one approach completely wrong? Thanks in advance for any input.","In which order should the following product of four matrices ABCD be evaluated so as to perform the least number of operations? A is a 1 by 5 matrix, B is a 5 by 100 matrix, C is a 100 by 10 matrix, D is a 10 by 5 matrix. I have what seems to be conflicting information on how to solve this problem. Research on the internet leads me to believe that I compute the efficiency one way, however my professor seems to have given me an entirely different and conflicting formula. I'm not sure if two different approaches are used two solve two slightly different wordings of similar questions, like solving for total number of operations versus solving for total number of multiplications. From what I have read on the internet, I believe that the answer to this problem would be (((AB)C)D) is the order with the least amount of operations, with a total number of operations calculated at 1550. To elaborate on my conflicting sources of information, here is an example. Given the above values, if we were to multiply (AB)C: The internet has shown me the # of operations is (1)(5)(10) + (1)(100)(10). Contrast this with my professor's in class example of: (1)(100)(2(5) - 1) + (1)(10)(2(100) - 1). These are clearly very different approaches to solving this problem, and I am wondering where my misunderstanding is. Are these two approaches solving a different problem entirely (such as number of multiplications vs number of operations)? Or is one approach completely wrong? Thanks in advance for any input.",,"['matrices', 'matrix-equations']"
48,Show that $P$ is symmetric.,Show that  is symmetric.,P,"Let $P$ = $A(A^TA)^{-1}A^T$, where A is an m x n matrix with rank $n$. I feel like this is wrong, but here is my attempt: $A(A^TA)^{-1}A^T$ = $AA^{-1}(A^T)^{-1}A^T$ = $I$ And $I^T$ = $I$, so the matrix is symmetric.","Let $P$ = $A(A^TA)^{-1}A^T$, where A is an m x n matrix with rank $n$. I feel like this is wrong, but here is my attempt: $A(A^TA)^{-1}A^T$ = $AA^{-1}(A^T)^{-1}A^T$ = $I$ And $I^T$ = $I$, so the matrix is symmetric.",,"['linear-algebra', 'matrices', 'inverse', 'symmetry', 'transpose']"
49,"A matrix has only $0$'s in its diagonal, all the other entries are $1$'s. What are the eigenvalues and eigenspaces of the matrix?","A matrix has only 's in its diagonal, all the other entries are 's. What are the eigenvalues and eigenspaces of the matrix?",0 1,"I think that I should to rewrite the matrix in a appropriate form, but I can't find it. For a $2\times2$ matrix I get the characteristics polynomial $x^2-1$ and the eigenvalues $-1,1$. For $3\times3$ matrix I get $-x^3+3x+2$ and the eigenvalues are $2,-1,-1$.","I think that I should to rewrite the matrix in a appropriate form, but I can't find it. For a $2\times2$ matrix I get the characteristics polynomial $x^2-1$ and the eigenvalues $-1,1$. For $3\times3$ matrix I get $-x^3+3x+2$ and the eigenvalues are $2,-1,-1$.",,"['linear-algebra', 'matrices']"
50,First course in linear algebra and matrices over arbitrary fields,First course in linear algebra and matrices over arbitrary fields,,"I'm looking for an elementary introduction to linear algebra (and matrices) over an arbitrary field. A lot of recommended books on the subject work only over the field of real or complex numbers (and this is inappropriate for me, I would like that there won't be a discussion whether I'm right from a pedagogical perspective). I know of the book by Hoffman and Kunze. Are there any more books like that (maybe more modern and a little less dry, maybe not)?","I'm looking for an elementary introduction to linear algebra (and matrices) over an arbitrary field. A lot of recommended books on the subject work only over the field of real or complex numbers (and this is inappropriate for me, I would like that there won't be a discussion whether I'm right from a pedagogical perspective). I know of the book by Hoffman and Kunze. Are there any more books like that (maybe more modern and a little less dry, maybe not)?",,"['linear-algebra', 'matrices', 'reference-request', 'book-recommendation']"
51,Chevalley Decomposition of $x+y$ in terms of Chevalley decomposition of $x$ and $y$.,Chevalley Decomposition of  in terms of Chevalley decomposition of  and .,x+y x y,"Given $V$ finite dimensional vector space on an algebraically closed field $k$ write for $x\in \text{End}_k(V)$   \begin{gather} x=x_s+x_n \end{gather}   the Chevalley decomposition of $x$ with $x_s$ diagonalizable and $x_n$ nilpotent.   Let $x,y \in \text{End}_k(V)$ be such that $xy=yx$, then   \begin{gather} (x+y)_s=x_s+y_s \\ (x+y)_n=x_n+y_n \end{gather} How can I prove it? If I show that $x_sy_s=y_sx_s$ then $x_s$ and $y_s$ admit a simultaneous basis of eigenvectors, then $x_s+y_s$ is diagonalizable and $x+y-(x_s-y_s)=x_n+y_n$ is nilpotent since sum of nilpotent matrices, and we get the result.","Given $V$ finite dimensional vector space on an algebraically closed field $k$ write for $x\in \text{End}_k(V)$   \begin{gather} x=x_s+x_n \end{gather}   the Chevalley decomposition of $x$ with $x_s$ diagonalizable and $x_n$ nilpotent.   Let $x,y \in \text{End}_k(V)$ be such that $xy=yx$, then   \begin{gather} (x+y)_s=x_s+y_s \\ (x+y)_n=x_n+y_n \end{gather} How can I prove it? If I show that $x_sy_s=y_sx_s$ then $x_s$ and $y_s$ admit a simultaneous basis of eigenvectors, then $x_s+y_s$ is diagonalizable and $x+y-(x_s-y_s)=x_n+y_n$ is nilpotent since sum of nilpotent matrices, and we get the result.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'linear-transformations']"
52,Multiplying permutation matrix by itself to get identity,Multiplying permutation matrix by itself to get identity,,"What is rigorous proof that any permutation matrix $P$ raised to some power $k$ equals identity matrix i.e. $P^k= I_n$? Is there any way to find the smallest $k$ other than looking at the matrix, finding all its cycles and then calculating least common multiple? Thank You!","What is rigorous proof that any permutation matrix $P$ raised to some power $k$ equals identity matrix i.e. $P^k= I_n$? Is there any way to find the smallest $k$ other than looking at the matrix, finding all its cycles and then calculating least common multiple? Thank You!",,"['linear-algebra', 'matrices', 'permutations', 'permutation-matrices']"
53,if $A^2 = A$ then $|A|=0$ or $|A| =1$,if  then  or,A^2 = A |A|=0 |A| =1,"More a verification of work then anything else, I am trying to prove the above statement.  Intuitively I feel that if $A^2 = A$ then $\det(A) = \det(A^2)$. From here I know the property of $\det(AB) = \det(A)\det(B)$ can be used to prove the statement, but I am looking for either a different way to solve it or a confirmation of my intuitive jump from $A^2 = A$ to $\det(A) = \det(A^2)$.","More a verification of work then anything else, I am trying to prove the above statement.  Intuitively I feel that if $A^2 = A$ then $\det(A) = \det(A^2)$. From here I know the property of $\det(AB) = \det(A)\det(B)$ can be used to prove the statement, but I am looking for either a different way to solve it or a confirmation of my intuitive jump from $A^2 = A$ to $\det(A) = \det(A^2)$.",,"['linear-algebra', 'matrices', 'proof-verification', 'determinant', 'matrix-calculus']"
54,Is there a formula for differentiating a nonlinear function by a matrix?,Is there a formula for differentiating a nonlinear function by a matrix?,,"I'm struggling with matrix notation for representing the derivative of a nonlinear function by a matrix.  Specifically, I'm calculating a gradient.  I have: $\quad \frac{\partial}{\partial \mathbf{W}} \phi ( \mathbf{W} \vec{x} )^T \vec{\beta}$ Where, say, $\vec{x}$ is $n \times 1$, $\vec{\beta}$ is $m \times 1$, and $\mathbf{W}$ is $m \times n$.  To simplify the question, let's say $\vec{x}$ and $\vec{\beta}$ are constant vectors. What has me stuck is $\phi(u)$ - a nonlinear transformation of its argument vector.  (For my purpose it is the sigmoid function $\frac{1}{1 + e^{-u}}$).  I can calculate this gradient exhaustively, but is there a shortcut that has a clean representation in matrix notation? For example, if the problem were simply: $\quad \frac{\partial}{\partial \mathbf{W}} (\mathbf{W} \vec{x})^T \vec{\beta}$ Then I could do this very neatly: $\quad \frac{\partial}{\partial \mathbf{W}} (\mathbf{W} \vec{x})^T \vec{\beta} = \frac{\partial}{\partial \mathbf{W}} \vec{x}^T \mathbf{W}^T \vec{\beta} = \vec{\beta} \vec{x}^T$","I'm struggling with matrix notation for representing the derivative of a nonlinear function by a matrix.  Specifically, I'm calculating a gradient.  I have: $\quad \frac{\partial}{\partial \mathbf{W}} \phi ( \mathbf{W} \vec{x} )^T \vec{\beta}$ Where, say, $\vec{x}$ is $n \times 1$, $\vec{\beta}$ is $m \times 1$, and $\mathbf{W}$ is $m \times n$.  To simplify the question, let's say $\vec{x}$ and $\vec{\beta}$ are constant vectors. What has me stuck is $\phi(u)$ - a nonlinear transformation of its argument vector.  (For my purpose it is the sigmoid function $\frac{1}{1 + e^{-u}}$).  I can calculate this gradient exhaustively, but is there a shortcut that has a clean representation in matrix notation? For example, if the problem were simply: $\quad \frac{\partial}{\partial \mathbf{W}} (\mathbf{W} \vec{x})^T \vec{\beta}$ Then I could do this very neatly: $\quad \frac{\partial}{\partial \mathbf{W}} (\mathbf{W} \vec{x})^T \vec{\beta} = \frac{\partial}{\partial \mathbf{W}} \vec{x}^T \mathbf{W}^T \vec{\beta} = \vec{\beta} \vec{x}^T$",,"['matrices', 'multivariable-calculus', 'matrix-calculus']"
55,"If $C=A-A^T$ and $a_{13}=1,a_{23}=-5,a_{21}=15$,then find the value of det(adj $A$)+det(adj$C$)","If  and ,then find the value of det(adj )+det(adj)","C=A-A^T a_{13}=1,a_{23}=-5,a_{21}=15 A C","Let $A$ be a $3\times 3$ matrix given by $A=[a_{ij}]$ and $B$ be a column vector such that $B^TAB$ is a null matrix for every column vector $B.$If $C=A-A^T$ and $a_{13}=1,a_{23}=-5,a_{21}=15$,then find the value of det(adj $A$)+det(adj$C$). My Attempt: Since $C=A-A^T$,therefore $C$ is a skew symmetric matrix,and the determinant of a skew symmetric matrix is $0$.So,$\det C=0$ $\det(\operatorname{adj}C)=(\det C)^2=0$ by using the property $\det(\operatorname{adj}M)=(\det M)^{n-1}$ But i cannot find $\det(\operatorname{adj}A)$.Please help me.Thanks.","Let $A$ be a $3\times 3$ matrix given by $A=[a_{ij}]$ and $B$ be a column vector such that $B^TAB$ is a null matrix for every column vector $B.$If $C=A-A^T$ and $a_{13}=1,a_{23}=-5,a_{21}=15$,then find the value of det(adj $A$)+det(adj$C$). My Attempt: Since $C=A-A^T$,therefore $C$ is a skew symmetric matrix,and the determinant of a skew symmetric matrix is $0$.So,$\det C=0$ $\det(\operatorname{adj}C)=(\det C)^2=0$ by using the property $\det(\operatorname{adj}M)=(\det M)^{n-1}$ But i cannot find $\det(\operatorname{adj}A)$.Please help me.Thanks.",,"['linear-algebra', 'matrices', 'determinant']"
56,Partition into block matrix,Partition into block matrix,,"If $$A=\begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix}$$ is a partition of $A$ such that $A_{11}$ and $A_{22}$ are $r \times r$ and $(n − r) \times (n − r)$ matrices, respectively, then $$\det(A) \leq \det(A_{11}) \cdot \det(A_{22})$$ with equality holding if and only if $A$ is block diagonal, Hint: Use $\det(A+B)^{\frac{1}{n}} \geq \det(A)^{\frac{1}{n}} + \det(B)^{\frac{1}{n}}$ with $$B=\begin{bmatrix} A_{11} & -A_{12} \\ -A_{21} & A_{22} \\ \end{bmatrix}$$ Why is $B$ positive definite?","If is a partition of such that and are and matrices, respectively, then with equality holding if and only if is block diagonal, Hint: Use with Why is positive definite?","A=\begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix} A A_{11} A_{22} r \times r (n − r) \times (n − r) \det(A) \leq \det(A_{11}) \cdot \det(A_{22}) A \det(A+B)^{\frac{1}{n}} \geq \det(A)^{\frac{1}{n}} + \det(B)^{\frac{1}{n}} B=\begin{bmatrix} A_{11} & -A_{12} \\ -A_{21} & A_{22} \\
\end{bmatrix} B","['linear-algebra', 'matrices', 'positive-definite', 'block-matrices']"
57,$Ax = b$ and $Ax = 0$ solutions,and  solutions,Ax = b Ax = 0,"I'm having trouble classifying the solution set of the systems $Ax = b$ and $Ax = 0$. Let A be an ($m \times n$) matrix. Case I: For $Ax = 0$, the system has a unique solution (the trivial one) when A is invertible, and infinitely many solutions when A is not. We can scratch off the ""no solution"" case because there is always the zero matrix solution correct? Case II: For $Ax = b$, if A is invertible, then for all ($n \times 1$) vector b, the matrix equation has a unique solution given by $x = A^{-1}b$. Else, there are two remaining cases: infinitely many solutions or no solutions. How would I know which it is? Can we not tell until we have row reduced the augmented matrix? Finally is the following true or false? If y and z are solutions of the system Ax = b then any linear combination of y and z is also a solution. My thoughts are that it is correct but I am not too sure.","I'm having trouble classifying the solution set of the systems $Ax = b$ and $Ax = 0$. Let A be an ($m \times n$) matrix. Case I: For $Ax = 0$, the system has a unique solution (the trivial one) when A is invertible, and infinitely many solutions when A is not. We can scratch off the ""no solution"" case because there is always the zero matrix solution correct? Case II: For $Ax = b$, if A is invertible, then for all ($n \times 1$) vector b, the matrix equation has a unique solution given by $x = A^{-1}b$. Else, there are two remaining cases: infinitely many solutions or no solutions. How would I know which it is? Can we not tell until we have row reduced the augmented matrix? Finally is the following true or false? If y and z are solutions of the system Ax = b then any linear combination of y and z is also a solution. My thoughts are that it is correct but I am not too sure.",,"['linear-algebra', 'matrices']"
58,matrix analysis and rank of matrix,matrix analysis and rank of matrix,,"I'm trying to prove that if $V(A) \cap N(A)=0$ , where A is a $n \times n$ matrix and $V(A)$ denote the range (column space) and $N(A)$ is the kernel (null space) of $A$, then there exists a non-singular matrix $B$ such that $A^2=BA$?","I'm trying to prove that if $V(A) \cap N(A)=0$ , where A is a $n \times n$ matrix and $V(A)$ denote the range (column space) and $N(A)$ is the kernel (null space) of $A$, then there exists a non-singular matrix $B$ such that $A^2=BA$?",,"['matrices', 'matrix-decomposition', 'matrix-rank']"
59,Expressing an inequality constraint as a linear matrix inequality (LMI),Expressing an inequality constraint as a linear matrix inequality (LMI),,"I am trying to formulate an optimization problem as a semidefinite program (SDP). My optimization variable is  $\mathbf x = [x_1, x_2, \dots, x_N]'$, where $\mathbf x$ is an $N \times 1$ vector, and one of my constraints is $$\prod_{i=1}^{N} x_i \ge a$$ Can I express this inequality constraint as a linear matrix inequality (LMI)?","I am trying to formulate an optimization problem as a semidefinite program (SDP). My optimization variable is  $\mathbf x = [x_1, x_2, \dots, x_N]'$, where $\mathbf x$ is an $N \times 1$ vector, and one of my constraints is $$\prod_{i=1}^{N} x_i \ge a$$ Can I express this inequality constraint as a linear matrix inequality (LMI)?",,"['linear-algebra', 'matrices', 'optimization', 'semidefinite-programming', 'linear-matrix-inequality']"
60,Finding possible Characteristic and Minimal polynomials of a matrix,Finding possible Characteristic and Minimal polynomials of a matrix,,$A$ is a 3x3 matrix over $\mathbb{C}$ that's not diagonalizable with trace 3 and determinant 1. Instructions: Find all possible characteristic and minimal polynomials. Characteristic polynomials:  The trace and determinant correspond to the constant term and degree $n-1$ term so that $c_A(x)=x^3-3x^2+\square x-1$. I am stuck here. Does the diagonalizable condition give constraints on $\square$ and the choices of minimal polynomials?,$A$ is a 3x3 matrix over $\mathbb{C}$ that's not diagonalizable with trace 3 and determinant 1. Instructions: Find all possible characteristic and minimal polynomials. Characteristic polynomials:  The trace and determinant correspond to the constant term and degree $n-1$ term so that $c_A(x)=x^3-3x^2+\square x-1$. I am stuck here. Does the diagonalizable condition give constraints on $\square$ and the choices of minimal polynomials?,,"['linear-algebra', 'matrices', 'determinant', 'trace', 'minimal-polynomials']"
61,"Show that $Ax=0, Bx=0$ share the same solution space iff there is some invertible $P$ s.t. $B=PA$.",Show that  share the same solution space iff there is some invertible  s.t. .,"Ax=0, Bx=0 P B=PA","The question is said in the title, suppose $A,B\in M_{m\times n}(K)$, where $K$ is some infinite number field. If we regard $A,B$ as linear maps from $K^n$ to $K^m$, then they share the same solution space $\Leftrightarrow$ $Ker A=Ker B$. And since kernels are orthogonally complementary to row spaces, it is equivalent to  $R(A)=R(B)$ where $R$ denotes the row space of a matrix. Therefore the row vector groups  of $A,B$ are equivalent, which means every row  of $A$ can be expressed as a linear combination of those of $B$, and vice versa. But since $A,B$ may not have full row rank, I'm having trouble showing such a invertible $P$ exists. I believe I'm on the right track but may miss something. Can you help me? Best regards! EDIT: Thanks go to @DavidP who provided a very good argument using RREF, which is by far slicker than any other approach I could imagine. But I am still looking for a way to complete my proof, which I think is going to be an ok one. So the real question is, if two row vector groups $$A=\begin{bmatrix} \alpha_1\\ \vdots\\ \alpha_m\end{bmatrix},\quad B=\begin{bmatrix} \beta_1\\ \vdots\\ \beta_m\end{bmatrix}$$ are equivalent, is it true that there exists some invertible $P\in M_m(K)$ such that $$B=PA$$?","The question is said in the title, suppose $A,B\in M_{m\times n}(K)$, where $K$ is some infinite number field. If we regard $A,B$ as linear maps from $K^n$ to $K^m$, then they share the same solution space $\Leftrightarrow$ $Ker A=Ker B$. And since kernels are orthogonally complementary to row spaces, it is equivalent to  $R(A)=R(B)$ where $R$ denotes the row space of a matrix. Therefore the row vector groups  of $A,B$ are equivalent, which means every row  of $A$ can be expressed as a linear combination of those of $B$, and vice versa. But since $A,B$ may not have full row rank, I'm having trouble showing such a invertible $P$ exists. I believe I'm on the right track but may miss something. Can you help me? Best regards! EDIT: Thanks go to @DavidP who provided a very good argument using RREF, which is by far slicker than any other approach I could imagine. But I am still looking for a way to complete my proof, which I think is going to be an ok one. So the real question is, if two row vector groups $$A=\begin{bmatrix} \alpha_1\\ \vdots\\ \alpha_m\end{bmatrix},\quad B=\begin{bmatrix} \beta_1\\ \vdots\\ \beta_m\end{bmatrix}$$ are equivalent, is it true that there exists some invertible $P\in M_m(K)$ such that $$B=PA$$?",,"['linear-algebra', 'matrices']"
62,Is the spectral radius of a Hermitian matrix a non-decreasing function of the magnitude of its entries?,Is the spectral radius of a Hermitian matrix a non-decreasing function of the magnitude of its entries?,,"I strongly suspect the answer is yes. By the min-max theorem , the largest eigenvalue of a hermitian matrix $M$ is $$ \lambda_{max}=\text{max} \left( \frac{x^*Mx}{x^*x} \right) $$ This is also its spectral radius. It seems intuitively that $\lambda_{max}$ could not decrease if some entries of $M$ increased in magnitude (in such a way that $M$ remains hermitian of course), but I cannot prove it.","I strongly suspect the answer is yes. By the min-max theorem , the largest eigenvalue of a hermitian matrix $M$ is $$ \lambda_{max}=\text{max} \left( \frac{x^*Mx}{x^*x} \right) $$ This is also its spectral radius. It seems intuitively that $\lambda_{max}$ could not decrease if some entries of $M$ increased in magnitude (in such a way that $M$ remains hermitian of course), but I cannot prove it.",,['matrices']
63,Does multiplication by a positive definite matrix preserve eigenvalues?,Does multiplication by a positive definite matrix preserve eigenvalues?,,"Let $A$ be a positive definite matrix and let $B$ a matrix. Then, $AB$ is similar to $A^{\frac{1}{2}}BA^{-\frac{1}{2}}$, which is in turn similar to $B$, so I get that $AB$ and $B$ are similar. Hence, $AB$ and $B$ have the same eigenvalues. Is my reasoning correct?","Let $A$ be a positive definite matrix and let $B$ a matrix. Then, $AB$ is similar to $A^{\frac{1}{2}}BA^{-\frac{1}{2}}$, which is in turn similar to $B$, so I get that $AB$ and $B$ are similar. Hence, $AB$ and $B$ have the same eigenvalues. Is my reasoning correct?",,"['linear-algebra', 'matrices']"
64,Is the entrywise nonnegative part of a real positive semidefinite matrix still positive semidefinite?,Is the entrywise nonnegative part of a real positive semidefinite matrix still positive semidefinite?,,"Let M be a real positive semidefinte matrix and consider the entrywise nonnegative matrix M' obtained from from M by zeroing out all the negative entries of M .  Is it true that M' is always positive semidefinite? Addendum 1: More generally, consider the entrywise nonnegative matrix M'' obtained from M by zeroing out an arbitrary set of off-diagonal entries (symmetrically, of course).  Is it true that M'' is always positive semidefinite? Addendum 2: Thanks to @orangeskid and @user1551 for prompt answers.  The question of Addendum 1 has a counterexample even in 3 dimensions .","Let M be a real positive semidefinte matrix and consider the entrywise nonnegative matrix M' obtained from from M by zeroing out all the negative entries of M .  Is it true that M' is always positive semidefinite? Addendum 1: More generally, consider the entrywise nonnegative matrix M'' obtained from M by zeroing out an arbitrary set of off-diagonal entries (symmetrically, of course).  Is it true that M'' is always positive semidefinite? Addendum 2: Thanks to @orangeskid and @user1551 for prompt answers.  The question of Addendum 1 has a counterexample even in 3 dimensions .",,['matrices']
65,Product of a Finite Number of Matrices with a Cosine Entry,Product of a Finite Number of Matrices with a Cosine Entry,,"Does any one know how to prove the following identity? $$ \mathop{\mathrm{Tr}}\left(\prod_{j=0}^{n-1}\begin{pmatrix}   2\cos\frac{2j\pi}{n} & a \\ b & 0 \end{pmatrix}\right)=2 $$ when $n$ is odd. The product sign means usual matrix multiplication, and $a$ and $b$ are arbitrary real numbers. Since the product of $2\cos\frac{2j\pi}{n}$ is $2$, so we only need to prove that the trace is a constant polynomial in $a$ and $b$. Because of the cosine term, the approach of polynomial analysis used in my previous post does not seem to work here.","Does any one know how to prove the following identity? $$ \mathop{\mathrm{Tr}}\left(\prod_{j=0}^{n-1}\begin{pmatrix}   2\cos\frac{2j\pi}{n} & a \\ b & 0 \end{pmatrix}\right)=2 $$ when $n$ is odd. The product sign means usual matrix multiplication, and $a$ and $b$ are arbitrary real numbers. Since the product of $2\cos\frac{2j\pi}{n}$ is $2$, so we only need to prove that the trace is a constant polynomial in $a$ and $b$. Because of the cosine term, the approach of polynomial analysis used in my previous post does not seem to work here.",,"['matrices', 'number-theory', 'polynomials', 'recurrence-relations']"
66,"Conditional Probability, Markov Chain Matrices","Conditional Probability, Markov Chain Matrices",,"In Freedonia, every day is either cloudy or sunny (not both). If it's sunny on any given day, then the probability that the next day will be sunny is $\frac 34$. If it's cloudy on any given day, then the probability that the next day will be cloudy is $\frac 23$. a. In the long run, what fraction of days are sunny? b. Given that a consecutive Saturday and Sunday had the same weather in Freedonia, what is the probability that that weather was sunny? I tried using weighted coins, but that didn't work. Can I get two answers, one for each problem, solution not necessary, as I need to figure out which of my methods leads to the correct answer. Thanks. I found a congruent problem, but it didn't have answers I could comprehend.","In Freedonia, every day is either cloudy or sunny (not both). If it's sunny on any given day, then the probability that the next day will be sunny is $\frac 34$. If it's cloudy on any given day, then the probability that the next day will be cloudy is $\frac 23$. a. In the long run, what fraction of days are sunny? b. Given that a consecutive Saturday and Sunday had the same weather in Freedonia, what is the probability that that weather was sunny? I tried using weighted coins, but that didn't work. Can I get two answers, one for each problem, solution not necessary, as I need to figure out which of my methods leads to the correct answer. Thanks. I found a congruent problem, but it didn't have answers I could comprehend.",,"['probability', 'matrices', 'markov-chains']"
67,"Show that there are numbers c and d such that F(A) = cTr(A^2) + d(Tr(A))^2,","Show that there are numbers c and d such that F(A) = cTr(A^2) + d(Tr(A))^2,",,"Suppose F(A) is a quadratic function of a real symmetric matrix, A.  This means that there are numbers $f_{ijkl}$ so that F(A) = $\sum_{ijkl}f_{ijkl}a_{ij}a_{kl}$. Suppose that $F(A) = F(QAQ^t)$ for every orthogonal matrix, Q.  Show that there are numbers c and d so that $F(A) = cTr(A^2) + d(Tr(A))^2$.  Here, Tr(A) is the trace of A. Edited work: Since A is symmetric, then we know that A is orthogonally diagonalizable, so that there's an orthogonal matrix Q such that $QAQ^t$ = D, where D is a diagonal matrix with the eigenvalues of A on the main diagonal. Using the assumption that F is invariant under all orthogonal similarity transformations, including (of course) the transformations that diagonalize A, I have that: $$F(A) = F(QAQ^t)$$ $$= F(D)$$ $$=\sum_{i,j,k,l} f_{i,j,k,l}d_{ij}d_{kl}$$ $$=\sum_{i,k} f_{iikk}d_{ii}d_{kk}$$ $$= F(Q_1DQ_1^t)$$ $$= F(Q_2DQ_2^t)$$ $$= F(Q_3DQ_3^t)$$ $$....$$ $$=\sum_{i,k} f_{iikk}d_{ii}d_{kk},$$ where $Q_i$ is an orthogonal, permutation matrix, so that $Q_iDQ_i^t$ is swapping the eigenvalues on the diagonal, resulting in a permuted diagonal matrix, still with the eigenvalues of A on the main diagonal. Now, the problem reduces to proving the equation for diagonal matrices. As whacka stated in his answer below, considering only the permuted, diagonal matrices, then F defines a quadratic form in the variables $\lambda_i$, for $1\le i \le n$. So, we have $$ F(\lambda_1, ... \lambda_n) = \sum_{i,k} f_{iikk}\lambda_i \lambda_k $$ $$ = \sum_{i=k} \alpha_i (\lambda_i)^2 + \sum_{i,k} \beta_{ik}\lambda_i \lambda_k $$ $$ = \sum_{i=k} \alpha_i (\lambda_i)^2 + \sum_i \sum_k \beta_{ik}\lambda_i \lambda_k$$ $$ ?? = \sum_{i=k} \alpha_i (\lambda_i)^2 + \beta_{ik} (\sum_i\ \lambda_i\sum_k  \lambda_k) $$ $$ = \alpha_i Tr(A^2) + \beta_{i,k}(Tr(A))^2$$ $$=F(A)$$ And, since F is invariant under any permutation of the eigenvalues on the diagonal, then this quadratic polynomial is also invariant; hence, the coefficients $\alpha_i$ and $\beta_{i,k}$ exist, are well-defined, and unique. How is my proof?  I don't feel confident about the equality that I labeled (??).  But somehow, I have got to make that number, $\beta_{i,k}$ not dependent on the indices, in order to pull it outside of the summation, so that I can get my $(tr(A))^2$. Any hints or suggestions are welcome and greatly appreciated. Thanks,","Suppose F(A) is a quadratic function of a real symmetric matrix, A.  This means that there are numbers $f_{ijkl}$ so that F(A) = $\sum_{ijkl}f_{ijkl}a_{ij}a_{kl}$. Suppose that $F(A) = F(QAQ^t)$ for every orthogonal matrix, Q.  Show that there are numbers c and d so that $F(A) = cTr(A^2) + d(Tr(A))^2$.  Here, Tr(A) is the trace of A. Edited work: Since A is symmetric, then we know that A is orthogonally diagonalizable, so that there's an orthogonal matrix Q such that $QAQ^t$ = D, where D is a diagonal matrix with the eigenvalues of A on the main diagonal. Using the assumption that F is invariant under all orthogonal similarity transformations, including (of course) the transformations that diagonalize A, I have that: $$F(A) = F(QAQ^t)$$ $$= F(D)$$ $$=\sum_{i,j,k,l} f_{i,j,k,l}d_{ij}d_{kl}$$ $$=\sum_{i,k} f_{iikk}d_{ii}d_{kk}$$ $$= F(Q_1DQ_1^t)$$ $$= F(Q_2DQ_2^t)$$ $$= F(Q_3DQ_3^t)$$ $$....$$ $$=\sum_{i,k} f_{iikk}d_{ii}d_{kk},$$ where $Q_i$ is an orthogonal, permutation matrix, so that $Q_iDQ_i^t$ is swapping the eigenvalues on the diagonal, resulting in a permuted diagonal matrix, still with the eigenvalues of A on the main diagonal. Now, the problem reduces to proving the equation for diagonal matrices. As whacka stated in his answer below, considering only the permuted, diagonal matrices, then F defines a quadratic form in the variables $\lambda_i$, for $1\le i \le n$. So, we have $$ F(\lambda_1, ... \lambda_n) = \sum_{i,k} f_{iikk}\lambda_i \lambda_k $$ $$ = \sum_{i=k} \alpha_i (\lambda_i)^2 + \sum_{i,k} \beta_{ik}\lambda_i \lambda_k $$ $$ = \sum_{i=k} \alpha_i (\lambda_i)^2 + \sum_i \sum_k \beta_{ik}\lambda_i \lambda_k$$ $$ ?? = \sum_{i=k} \alpha_i (\lambda_i)^2 + \beta_{ik} (\sum_i\ \lambda_i\sum_k  \lambda_k) $$ $$ = \alpha_i Tr(A^2) + \beta_{i,k}(Tr(A))^2$$ $$=F(A)$$ And, since F is invariant under any permutation of the eigenvalues on the diagonal, then this quadratic polynomial is also invariant; hence, the coefficients $\alpha_i$ and $\beta_{i,k}$ exist, are well-defined, and unique. How is my proof?  I don't feel confident about the equality that I labeled (??).  But somehow, I have got to make that number, $\beta_{i,k}$ not dependent on the indices, in order to pull it outside of the summation, so that I can get my $(tr(A))^2$. Any hints or suggestions are welcome and greatly appreciated. Thanks,",,"['linear-algebra', 'matrices', 'quadratics', 'orthogonality', 'trace']"
68,Eigenvalues of matrix sums,Eigenvalues of matrix sums,,"Under what conditions are the eigenvalues of $A+B$ equal to the eigenvalues of $A$ plus the eigenvalues of $B$, where $A,B$ are square matrices? From searching, it seems that the condition is that $AB=BA$.  If that is indeed the case, why?","Under what conditions are the eigenvalues of $A+B$ equal to the eigenvalues of $A$ plus the eigenvalues of $B$, where $A,B$ are square matrices? From searching, it seems that the condition is that $AB=BA$.  If that is indeed the case, why?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
69,Inverting a $3\times 3$ block matrix,Inverting a  block matrix,3\times 3,"Suppose that $a$ and $b$ below are scalars, $F$ a square matrix, $v$ a column vector. I'm trying to invert the matrix $M$ of the form $$ M=\begin{pmatrix} a & v' & 0\\ v & F  & 0\\ 0 & 0  & b \end{pmatrix} $$ When $v$ and $F$ are actually scalars, we can use $M^{-1}=\frac{1}{\det M}\text{adj}(M)$ but how does one handle the general case above please?","Suppose that $a$ and $b$ below are scalars, $F$ a square matrix, $v$ a column vector. I'm trying to invert the matrix $M$ of the form $$ M=\begin{pmatrix} a & v' & 0\\ v & F  & 0\\ 0 & 0  & b \end{pmatrix} $$ When $v$ and $F$ are actually scalars, we can use $M^{-1}=\frac{1}{\det M}\text{adj}(M)$ but how does one handle the general case above please?",,"['linear-algebra', 'matrices', 'inverse']"
70,"matrix with fractional exponent, not getting expected output in Matlab/Octave","matrix with fractional exponent, not getting expected output in Matlab/Octave",,"I have a matrix exponential function that is called a number of times in an integration routine from the heat conduction model I'm trying to implement . It works, and my results match the samples in the paper, but want to speed things up by breaking apart the exponential function so that I can precalculate the constant part and just pass that to the calling function. Unfortunately, the separation only works for integer values of the exponent, and I'm not sure why the manipulation doesn't hold for fractional values. the matrix exponential is equation (20) in the paper defined as: $e^{[F(s)]x}=\pmatrix{ \mathrm{cosh}{(\sqrt{s/\alpha_j}x)} & \frac{1}{\lambda_j\sqrt{s/\alpha_j}}\mathrm{sinh}{(\sqrt{s/\alpha_j}x)} \\ \lambda_j\sqrt{s/\alpha_j}\mathrm{sinh}{(\sqrt{s/\alpha_j}x)} & \mathrm{cosh}{(\sqrt{s/\alpha_j}x})}$ this is most often called in the model as $e^{[F(-\beta^2)](x_j-x)}$ where $\beta$ is a real eigenvalue, and $x_j$ is a nodal location. Because $\beta$ and $x_j$ are known prior to the call for $e^{[F(-\beta^2)](x_j-x)}$, I wanted to do the manipulation below to separate out $x$: $$ e^{[F(-\beta^2)](x-x_j)} = e^{[F(-\beta^2)]x_j}*e^{[F(-\beta^2)](-x)} = e^{[F(-\beta^2)]x_j}*[e^{[F(-\beta^2)](-1)}]^x $$ In that form, I can precalculate both exponential functions, and just do a scalar power operation with x inside the integration and other calling loops. Implemented it in Octave (and tried it in Matlab), led to a great speedup, but non integer values of x produce ""wrong"" answers.  I realize a fractional matrix exponent involves solving an eigenvalue equation where $A^m=P(D^m)P^{-1}$ as per This Answer . It seems what I've done is consistent with rules for matrix exponentials . But here's an example to belabor the point ($\alpha_j=1, \lambda_j=2$): $e^{[F(-\beta^2)]x}, \beta=1,x=1 = \pmatrix{\mathrm{cos}{(1)}&\mathrm{sin}{(1)}/2\\-2\cdot \mathrm{sin}{(1)}&\mathrm{cos}{(1)}}=\pmatrix{0.54030&  0.42074\\-1.68294& 0.54030}$ $e^{[F(-\beta^2)]x}, \beta=1,x=3 = \pmatrix{\mathrm{cos}\left( 3\right)  & \frac{\mathrm{sin}\left( 3\right) }{2}\cr -2\cdot \mathrm{sin}\left( 3\right)  & \mathrm{cos}\left( 3\right) }=\pmatrix{0.45360 &  0.44560\\-1.78241 & 0.45360}$ $e^{[F(-\beta^2)]x}, \beta=1,x=1.1 =\pmatrix{-0.989992 &  0.070560\\-0.282240 & -0.989992}$ These work fine in Octave, Matlab, and the integer ones checked out in Maxima whether x is input as a multiplication in the matrix function, or as an exponent to the matrix function. Now, the problem seems to creep in when $\beta > \pi$. Integer values of x are ok: $e^{[F(-\beta^2)]x}, \beta=1.1\cdot\pi,x=3 = \pmatrix{-0.58779 & -0.11705\\   5.59152  &-0.58779}$ $[e^{[F(-\beta^2)](1)}]^{x}, \beta=1.1\cdot\pi,x=3 = \pmatrix{-0.58779 & -0.11705\\   5.59152  &-0.58779}$ But fractional values are not: $e^{[F(-\beta^2)]x}, \beta=1.1\cdot\pi,x=1.1 = \pmatrix{ -0.790155 & -0.088679\\   4.236109 & -0.790155}$ $[e^{[F(-\beta^2)](1)}]^{x}, \beta=1.1\cdot\pi,x=1.1 = \pmatrix{-0.9995066 & -0.0045447\\   0.2170956 & -0.9995066}$ As I'm summing over a large number of Beta's of increasing size, this is a problem, and I can't seem to identify the cause. I just realized the issue occurs at $\beta > \pi$ when typing this up, so maybe there's something obvious there that I'm missing. for reference, here's my implementation of the calling function: function answer=expfxs(x,s,alphaj,lamj)    answer = [cosh(sqrt(s/alphaj)*x),...          sinh(sqrt(s/alphaj)*x)/(lamj*sqrt(s/alphaj));...          sinh(sqrt(s/alphaj)*x)*lamj*sqrt(s/alphaj),...          cosh(sqrt(s/alphaj)*x)]; endfunction","I have a matrix exponential function that is called a number of times in an integration routine from the heat conduction model I'm trying to implement . It works, and my results match the samples in the paper, but want to speed things up by breaking apart the exponential function so that I can precalculate the constant part and just pass that to the calling function. Unfortunately, the separation only works for integer values of the exponent, and I'm not sure why the manipulation doesn't hold for fractional values. the matrix exponential is equation (20) in the paper defined as: $e^{[F(s)]x}=\pmatrix{ \mathrm{cosh}{(\sqrt{s/\alpha_j}x)} & \frac{1}{\lambda_j\sqrt{s/\alpha_j}}\mathrm{sinh}{(\sqrt{s/\alpha_j}x)} \\ \lambda_j\sqrt{s/\alpha_j}\mathrm{sinh}{(\sqrt{s/\alpha_j}x)} & \mathrm{cosh}{(\sqrt{s/\alpha_j}x})}$ this is most often called in the model as $e^{[F(-\beta^2)](x_j-x)}$ where $\beta$ is a real eigenvalue, and $x_j$ is a nodal location. Because $\beta$ and $x_j$ are known prior to the call for $e^{[F(-\beta^2)](x_j-x)}$, I wanted to do the manipulation below to separate out $x$: $$ e^{[F(-\beta^2)](x-x_j)} = e^{[F(-\beta^2)]x_j}*e^{[F(-\beta^2)](-x)} = e^{[F(-\beta^2)]x_j}*[e^{[F(-\beta^2)](-1)}]^x $$ In that form, I can precalculate both exponential functions, and just do a scalar power operation with x inside the integration and other calling loops. Implemented it in Octave (and tried it in Matlab), led to a great speedup, but non integer values of x produce ""wrong"" answers.  I realize a fractional matrix exponent involves solving an eigenvalue equation where $A^m=P(D^m)P^{-1}$ as per This Answer . It seems what I've done is consistent with rules for matrix exponentials . But here's an example to belabor the point ($\alpha_j=1, \lambda_j=2$): $e^{[F(-\beta^2)]x}, \beta=1,x=1 = \pmatrix{\mathrm{cos}{(1)}&\mathrm{sin}{(1)}/2\\-2\cdot \mathrm{sin}{(1)}&\mathrm{cos}{(1)}}=\pmatrix{0.54030&  0.42074\\-1.68294& 0.54030}$ $e^{[F(-\beta^2)]x}, \beta=1,x=3 = \pmatrix{\mathrm{cos}\left( 3\right)  & \frac{\mathrm{sin}\left( 3\right) }{2}\cr -2\cdot \mathrm{sin}\left( 3\right)  & \mathrm{cos}\left( 3\right) }=\pmatrix{0.45360 &  0.44560\\-1.78241 & 0.45360}$ $e^{[F(-\beta^2)]x}, \beta=1,x=1.1 =\pmatrix{-0.989992 &  0.070560\\-0.282240 & -0.989992}$ These work fine in Octave, Matlab, and the integer ones checked out in Maxima whether x is input as a multiplication in the matrix function, or as an exponent to the matrix function. Now, the problem seems to creep in when $\beta > \pi$. Integer values of x are ok: $e^{[F(-\beta^2)]x}, \beta=1.1\cdot\pi,x=3 = \pmatrix{-0.58779 & -0.11705\\   5.59152  &-0.58779}$ $[e^{[F(-\beta^2)](1)}]^{x}, \beta=1.1\cdot\pi,x=3 = \pmatrix{-0.58779 & -0.11705\\   5.59152  &-0.58779}$ But fractional values are not: $e^{[F(-\beta^2)]x}, \beta=1.1\cdot\pi,x=1.1 = \pmatrix{ -0.790155 & -0.088679\\   4.236109 & -0.790155}$ $[e^{[F(-\beta^2)](1)}]^{x}, \beta=1.1\cdot\pi,x=1.1 = \pmatrix{-0.9995066 & -0.0045447\\   0.2170956 & -0.9995066}$ As I'm summing over a large number of Beta's of increasing size, this is a problem, and I can't seem to identify the cause. I just realized the issue occurs at $\beta > \pi$ when typing this up, so maybe there's something obvious there that I'm missing. for reference, here's my implementation of the calling function: function answer=expfxs(x,s,alphaj,lamj)    answer = [cosh(sqrt(s/alphaj)*x),...          sinh(sqrt(s/alphaj)*x)/(lamj*sqrt(s/alphaj));...          sinh(sqrt(s/alphaj)*x)*lamj*sqrt(s/alphaj),...          cosh(sqrt(s/alphaj)*x)]; endfunction",,"['linear-algebra', 'matrices', 'matlab', 'octave']"
71,"For any $A, B \in SL(2, F)$, does knowing $\operatorname{tr}A$, $\operatorname{tr}B$, and $\operatorname{tr}AB$ specify $A$ and $B$?","For any , does knowing , , and  specify  and ?","A, B \in SL(2, F) \operatorname{tr}A \operatorname{tr}B \operatorname{tr}AB A B","In title, $F$ denotes a field. Does knowing the trace of two matrices and their product specify those two matrices? Up to some equivalence, perhaps?","In title, $F$ denotes a field. Does knowing the trace of two matrices and their product specify those two matrices? Up to some equivalence, perhaps?",,['matrices']
72,Matrix Exponential and Logarithm,Matrix Exponential and Logarithm,,"Consider the following matrix $A$: $A = \begin{bmatrix}     \cos^2(1) & -\sin(2) & \sin^2(1) \\     \cos(1)\sin(1) & \cos(2) & -\cos(1)\sin(1) \\     \sin^2(1) & \sin(2) & \cos^2(1)\\ \end{bmatrix}$ I want to find a matrix $B$ such that $\exp(B)=A$ (or essentially finding $\log(A))$. Is there a systematic way to approach these kinds of problems? I was thinking of using some properties involving diagonalization to get to the answer, which should be (obtained using Mathematica): $B = \begin{bmatrix}     0 & -2 & 0 \\     1 & 0 & -1 \\     0 & 2 & 0\\ \end{bmatrix}$ However, I'm not sure how to get to this result. Thank you for your help.","Consider the following matrix $A$: $A = \begin{bmatrix}     \cos^2(1) & -\sin(2) & \sin^2(1) \\     \cos(1)\sin(1) & \cos(2) & -\cos(1)\sin(1) \\     \sin^2(1) & \sin(2) & \cos^2(1)\\ \end{bmatrix}$ I want to find a matrix $B$ such that $\exp(B)=A$ (or essentially finding $\log(A))$. Is there a systematic way to approach these kinds of problems? I was thinking of using some properties involving diagonalization to get to the answer, which should be (obtained using Mathematica): $B = \begin{bmatrix}     0 & -2 & 0 \\     1 & 0 & -1 \\     0 & 2 & 0\\ \end{bmatrix}$ However, I'm not sure how to get to this result. Thank you for your help.",,"['linear-algebra', 'matrices', 'matrix-calculus']"
73,Realizing the oscillator algebra as a matrix Lie algebra,Realizing the oscillator algebra as a matrix Lie algebra,,"In Hilgert's & Neeb's Structure and Geometry of Lie Groups , they introduce a Lie algebra, which they call the ""oscillator algebra,"" as an extension of the Heisenberg algebra. They give a basis $\{p,q,z,h\}$ for this Lie algebra, related by $$[p,q]=z, \\ [h,p]=q, \\ [h,q]=-p,$$ with all other brackets being $0$. For some reason, I felt the need to try to find a finite-dimensional faithful representation for this algebra. Twenty-four hours later...still no real progress. A quick check (directly from the relations or from Cartan's solvability criterion) will show that this Lie algebra is solvable. The adjoint representation is, thus, not a viable candidate. I hoped playing with upper triangular matrices would bring something, but trying to write out general terms became tedious and unhelpful, and Lie's theorem only works over $\mathbb{C}$ (and some special cases in $\mathbb{R}$). I've tried finding useful information from the commutators themselves. I think we can assume that $pq=z$ and $qp=0$, but I have no actual proof of this. I could really use a forceful shove in the right direction. Is there an easier way to go about finding this representation than brute force? If so, what is it?","In Hilgert's & Neeb's Structure and Geometry of Lie Groups , they introduce a Lie algebra, which they call the ""oscillator algebra,"" as an extension of the Heisenberg algebra. They give a basis $\{p,q,z,h\}$ for this Lie algebra, related by $$[p,q]=z, \\ [h,p]=q, \\ [h,q]=-p,$$ with all other brackets being $0$. For some reason, I felt the need to try to find a finite-dimensional faithful representation for this algebra. Twenty-four hours later...still no real progress. A quick check (directly from the relations or from Cartan's solvability criterion) will show that this Lie algebra is solvable. The adjoint representation is, thus, not a viable candidate. I hoped playing with upper triangular matrices would bring something, but trying to write out general terms became tedious and unhelpful, and Lie's theorem only works over $\mathbb{C}$ (and some special cases in $\mathbb{R}$). I've tried finding useful information from the commutators themselves. I think we can assume that $pq=z$ and $qp=0$, but I have no actual proof of this. I could really use a forceful shove in the right direction. Is there an easier way to go about finding this representation than brute force? If so, what is it?",,"['linear-algebra', 'matrices', 'representation-theory', 'lie-algebras']"
74,Get amount of submatrixes from $a \times b $matrix,Get amount of submatrixes from matrix,a \times b ,"I was trying to do the following exercise Given a grid of size $a \times b$, write a formula able t calculate the total number of rectangles contained in this rectangle. All integer sizes and positions are counted. Examples: $3 \times 2$ matrix $\Rightarrow 18$ $4 \times 4$ matrix $\Rightarrow 100$ Here is how the $3\times2$ grid works: $1$ rectangle of size $3\times 2$: [][][] [][][] $2$ rectangles of size $3\times 1$: [][][] $4$ rectangles of size $2\times 1$: [][] $2$ rectangles of size $2\times 2$ [][] [][] $3$ rectangles of size $1\times 2$: [] [] $6$ rectangles of size $1\times 1$: [] As you can see $(1 + 2 + 4 + 2 + 3 + 6) = 18$, and is the solution for the $3\times 2$ grid. ... Alledgedly, the answer is $\frac{((ab)+a)((ab)+b))}{4}$ I can't understand why, can anybody explain this?","I was trying to do the following exercise Given a grid of size $a \times b$, write a formula able t calculate the total number of rectangles contained in this rectangle. All integer sizes and positions are counted. Examples: $3 \times 2$ matrix $\Rightarrow 18$ $4 \times 4$ matrix $\Rightarrow 100$ Here is how the $3\times2$ grid works: $1$ rectangle of size $3\times 2$: [][][] [][][] $2$ rectangles of size $3\times 1$: [][][] $4$ rectangles of size $2\times 1$: [][] $2$ rectangles of size $2\times 2$ [][] [][] $3$ rectangles of size $1\times 2$: [] [] $6$ rectangles of size $1\times 1$: [] As you can see $(1 + 2 + 4 + 2 + 3 + 6) = 18$, and is the solution for the $3\times 2$ grid. ... Alledgedly, the answer is $\frac{((ab)+a)((ab)+b))}{4}$ I can't understand why, can anybody explain this?",,"['combinatorics', 'matrices']"
75,Elements of SO(n) is block-diagonalizable,Elements of SO(n) is block-diagonalizable,,"I am not able to show that elements of SO(n) are conjugate to a block-diagonal matrix with 2x2 blocs that are rotation matrices, and a 1x1 bloc 1 if n is odd. Can someone help me please?","I am not able to show that elements of SO(n) are conjugate to a block-diagonal matrix with 2x2 blocs that are rotation matrices, and a 1x1 bloc 1 if n is odd. Can someone help me please?",,"['matrices', 'lie-groups', 'linear-groups']"
76,Linear Recurrence In Faster Time,Linear Recurrence In Faster Time,,"I am trying to solve this linear recurrence using matrix exponentiation:- $$f(n) = 2f(n-1) - f(n-2) + c,$$ where $c$ is a constant. What I have come up with is this - Let the matrix $M$ be $$ \begin{bmatrix} 2 & -1 & 1\\ 1 & 0 & 0\\ 0 & 0 & 1 \end{bmatrix}. $$ Now, we have $$M^{n-4}\cdot\begin{bmatrix}f(4)\\f(3)\\c\end{bmatrix} = \begin{bmatrix}f(n)\\f(n-1)\\c\end{bmatrix}. $$ I have used this to solve the recurrence. However, I was wondering: since there is a constant in the recurrence, is there any way to solve it using a $2\times 2$ matrix $M$ instead of the $3\times 3$ one which I have used? Any suggestions would be valued, thank you.","I am trying to solve this linear recurrence using matrix exponentiation:- $$f(n) = 2f(n-1) - f(n-2) + c,$$ where $c$ is a constant. What I have come up with is this - Let the matrix $M$ be $$ \begin{bmatrix} 2 & -1 & 1\\ 1 & 0 & 0\\ 0 & 0 & 1 \end{bmatrix}. $$ Now, we have $$M^{n-4}\cdot\begin{bmatrix}f(4)\\f(3)\\c\end{bmatrix} = \begin{bmatrix}f(n)\\f(n-1)\\c\end{bmatrix}. $$ I have used this to solve the recurrence. However, I was wondering: since there is a constant in the recurrence, is there any way to solve it using a $2\times 2$ matrix $M$ instead of the $3\times 3$ one which I have used? Any suggestions would be valued, thank you.",,"['matrices', 'recurrence-relations', 'contest-math', 'exponentiation']"
77,Prove that the given block matrix is positive semi-definite,Prove that the given block matrix is positive semi-definite,,"How do I show $M =  \begin{bmatrix} A & B \\ B^T & C \end{bmatrix} \succeq 0$ i.e. $M$ is positive semi-definite (PSD) given that $A$ is PSD and for some $\Lambda = \text{diag}(\lambda_1, \lambda_2, \cdots, \lambda_p), \lambda_i \in [0,1]$ $$ B = B_1 (I-\Lambda) + B_2 \Lambda\\ C = (I-\Lambda)C_1(I-\Lambda) + (I-\Lambda)C_2\Lambda + \Lambda C_2^T (I-\Lambda) + \Lambda C_3 \Lambda, $$ where $B_1, B_2, C_1, C_2, C_3$ are all PSD. I know about the Schur complement, but I'm not sure how to take the inverse of $C$. Is there any other way I can approach to solve this problem? Thanks for the help.","How do I show $M =  \begin{bmatrix} A & B \\ B^T & C \end{bmatrix} \succeq 0$ i.e. $M$ is positive semi-definite (PSD) given that $A$ is PSD and for some $\Lambda = \text{diag}(\lambda_1, \lambda_2, \cdots, \lambda_p), \lambda_i \in [0,1]$ $$ B = B_1 (I-\Lambda) + B_2 \Lambda\\ C = (I-\Lambda)C_1(I-\Lambda) + (I-\Lambda)C_2\Lambda + \Lambda C_2^T (I-\Lambda) + \Lambda C_3 \Lambda, $$ where $B_1, B_2, C_1, C_2, C_3$ are all PSD. I know about the Schur complement, but I'm not sure how to take the inverse of $C$. Is there any other way I can approach to solve this problem? Thanks for the help.",,"['matrices', 'matrix-equations', 'matrix-decomposition']"
78,Scaling a svg image while keeping the offset position.,Scaling a svg image while keeping the offset position.,,"I have an svg image of a map that i have to scale up to make it zoom in. Javascript has a function to scale up SVG images. However the svg scale function uses the upper left corner as center when zooming. So to counter this the usual trick is to shift the svg 50% to the left and 50% up so that the center moves to the upper left corner. Then scale the image and again move the image back. Edit: I think this article might say what i need to do but i do not know enought about matrices: http://www.cs.rit.edu/~icss571/clipTrans/2DTransBack.html#BACK3.0 Edit3: Here is some more information about the problem: http://commons.oreilly.com/wiki/index.php/SVG_Essentials/Transforming_the_Coordinate_System#svgess-CHP-5-FIG-9 The logic behind it looks like this. var bbox = mapGroup.getBBox(); // the element of the svg image i want to scale up/down. // finding center of element var cx = bbox.x + (bbox.width/2); // x is the offset of the element horizontally var cy = bbox.y + (bbox.height/2);  // y is the offset of the element vertically  // Shift the image so that the middle is in the upper left corner taking into account the amout of scaling; mapGroup.attr('transform', 'translate(-' +(cx-(scale*cx)) + ', -' + (cy-(scale*cy)) + ')'); // scale the image mapScaleGroup.attr('transform', 'scale('+ scale +')'); // Shift the image back again mapGroup.attr('transform', 'translate(' + (cx-(scale*cx)) + ', ' + (cy-(scale*cy)) + ')'); This Works perfectly when i only want to hit the center of the image, however i have added a drag functionality that causes the offset to change. This causes problems as you can see here: http://nho-municipality-map.divshot.io/ Zoom to see how it should be or drag the map and then zoom to see the mistake. So I think the key to the problem lies ether in this part: cx = bbox.x + (bbox.width/2) or this part (cx-(scale*cx)) How do i take the custom offset into account? Edit2: Here is all the information i get from the bbox.","I have an svg image of a map that i have to scale up to make it zoom in. Javascript has a function to scale up SVG images. However the svg scale function uses the upper left corner as center when zooming. So to counter this the usual trick is to shift the svg 50% to the left and 50% up so that the center moves to the upper left corner. Then scale the image and again move the image back. Edit: I think this article might say what i need to do but i do not know enought about matrices: http://www.cs.rit.edu/~icss571/clipTrans/2DTransBack.html#BACK3.0 Edit3: Here is some more information about the problem: http://commons.oreilly.com/wiki/index.php/SVG_Essentials/Transforming_the_Coordinate_System#svgess-CHP-5-FIG-9 The logic behind it looks like this. var bbox = mapGroup.getBBox(); // the element of the svg image i want to scale up/down. // finding center of element var cx = bbox.x + (bbox.width/2); // x is the offset of the element horizontally var cy = bbox.y + (bbox.height/2);  // y is the offset of the element vertically  // Shift the image so that the middle is in the upper left corner taking into account the amout of scaling; mapGroup.attr('transform', 'translate(-' +(cx-(scale*cx)) + ', -' + (cy-(scale*cy)) + ')'); // scale the image mapScaleGroup.attr('transform', 'scale('+ scale +')'); // Shift the image back again mapGroup.attr('transform', 'translate(' + (cx-(scale*cx)) + ', ' + (cy-(scale*cy)) + ')'); This Works perfectly when i only want to hit the center of the image, however i have added a drag functionality that causes the offset to change. This causes problems as you can see here: http://nho-municipality-map.divshot.io/ Zoom to see how it should be or drag the map and then zoom to see the mistake. So I think the key to the problem lies ether in this part: cx = bbox.x + (bbox.width/2) or this part (cx-(scale*cx)) How do i take the custom offset into account? Edit2: Here is all the information i get from the bbox.",,['matrices']
79,a matrix metric,a matrix metric,,"Let $U_1,...,U_n$ and $V_1,...,V_n$ be two sets of $n$ unitary matrices of the same size. We'll denote $E(U_i,V_i)= \max_v \, |(U_i -V_i)v|$ (max over all the quantum states ), $U=\prod_i U_i$ and $V=\prod_i V_i$. I'd like to show that $E(U,V) \leq \sum_i E(U_i,V_i) $. At first I thought proving this by induction on $n$, but then I got stuck even in the simple case of $n=2$. I also tried expanding the expression: $$E(U,V)=\max_v |(\prod U_i - \prod V_i)v | $$ But I got stuck on here too. Maybe there's an easier way I'm missing out? Edit: $U_i, V_i$ are unitary matrices","Let $U_1,...,U_n$ and $V_1,...,V_n$ be two sets of $n$ unitary matrices of the same size. We'll denote $E(U_i,V_i)= \max_v \, |(U_i -V_i)v|$ (max over all the quantum states ), $U=\prod_i U_i$ and $V=\prod_i V_i$. I'd like to show that $E(U,V) \leq \sum_i E(U_i,V_i) $. At first I thought proving this by induction on $n$, but then I got stuck even in the simple case of $n=2$. I also tried expanding the expression: $$E(U,V)=\max_v |(\prod U_i - \prod V_i)v | $$ But I got stuck on here too. Maybe there's an easier way I'm missing out? Edit: $U_i, V_i$ are unitary matrices",,"['linear-algebra', 'matrices', 'quantum-computation']"
80,Describe the space of solutions of the matrix equation $UV^T=VU^T$,Describe the space of solutions of the matrix equation,UV^T=VU^T,"I would like to describe the space of solutions to the following matrix equation. Here $U$ and $V$ are two unknown real matrices with $n$ lines and $p$ columns (i.e. they belong to $M_{n,p}(\mathbb{R})$) : $$ U V^T - V U^T = 0$$ Note : The result is a square matrix with $n$ lines and $n$ columns. What I found up to now : if $p=1$ then the equations are equivalent with the statement $U$ and $V$ are colinear. The system can be rewritten under the following form : Let $X$ be the following matrix with $2p$ lines and $n$ columns (the unknown) : $$ X = \left[ \begin{matrix}U^T\\V^T\end{matrix}\right]$$ Let $\Omega$ be the following square matrix with $2p$ lines and $2p$ columns :  $$ \Omega = \left[ \begin{matrix}0_{p \times p} & I_{p \times p}\\-I_{p \times p} & 0_{p \times p}\end{matrix}\right]$$ Then my equations can be re-written like this : $$ X^T \Omega X = 0$$ This formal similarity with isotropic spaces in symplectic geometry makes me think that there is some litterature on the subject (the difference with symplectic geometry is that we do not have a ""form"" since the result is not a scalar when $n \neq 1$). I however cannot find anything relevant yet. Any hint or partial answer is most welcome. Edit : Trying to generalize the special $p=1$ case to general $p$, I found the following sufficient condition :  If there exists a symmetric $p\times p$ matrix $P$ such that $U=VP$ then $(U,V)$ is a solution to the equation. Is this condition sufficient ? Can it be re-written so that it highlights the symmetric role of $U$ and $V$ ?","I would like to describe the space of solutions to the following matrix equation. Here $U$ and $V$ are two unknown real matrices with $n$ lines and $p$ columns (i.e. they belong to $M_{n,p}(\mathbb{R})$) : $$ U V^T - V U^T = 0$$ Note : The result is a square matrix with $n$ lines and $n$ columns. What I found up to now : if $p=1$ then the equations are equivalent with the statement $U$ and $V$ are colinear. The system can be rewritten under the following form : Let $X$ be the following matrix with $2p$ lines and $n$ columns (the unknown) : $$ X = \left[ \begin{matrix}U^T\\V^T\end{matrix}\right]$$ Let $\Omega$ be the following square matrix with $2p$ lines and $2p$ columns :  $$ \Omega = \left[ \begin{matrix}0_{p \times p} & I_{p \times p}\\-I_{p \times p} & 0_{p \times p}\end{matrix}\right]$$ Then my equations can be re-written like this : $$ X^T \Omega X = 0$$ This formal similarity with isotropic spaces in symplectic geometry makes me think that there is some litterature on the subject (the difference with symplectic geometry is that we do not have a ""form"" since the result is not a scalar when $n \neq 1$). I however cannot find anything relevant yet. Any hint or partial answer is most welcome. Edit : Trying to generalize the special $p=1$ case to general $p$, I found the following sufficient condition :  If there exists a symmetric $p\times p$ matrix $P$ such that $U=VP$ then $(U,V)$ is a solution to the equation. Is this condition sufficient ? Can it be re-written so that it highlights the symmetric role of $U$ and $V$ ?",,"['matrices', 'matrix-equations', 'bilinear-form', 'symplectic-geometry', 'symplectic-linear-algebra']"
81,"Computing the matrix that represents orthogonal projection,","Computing the matrix that represents orthogonal projection,",,"There is a theorem that says if $U$ is an orthogonal matrix, i.e., its columns (or rows) form an orthonormal basis, then the action of $UU^T$ represents orthogonal projection of the vector space onto the space spanned by the columns of $U$. But, why does this theorem also apply when my ""$U$"" matrix is a $3\times1$ column vector and hence not an orthogonal matrix (it's not even square)? I had to compute the matrix that represents orthogonal projection onto the line spanned by $(1, 2, -1)$.  So, following the Gram-Schmidt process for this simple case, just set $v_1= (1,2,-1)$.  Then I normalized this vector to make it of unit length.  Finally, I compute $VV^T$ to get a $3\times3$ matrix.  I applied the theorem stated above and concluded that this $3\times3$ matrix represents orthogonal projection onto the line spanned by the one column in $V$, which has the same span as the span of $(1, 2,-1)$, since the Gram-Schmidt process produces a set of orthogonal vectors with the same span as the original set of linearly independent vectors. Have I applied the theorem incorrectly?  I got the correct matrix. Thanks,","There is a theorem that says if $U$ is an orthogonal matrix, i.e., its columns (or rows) form an orthonormal basis, then the action of $UU^T$ represents orthogonal projection of the vector space onto the space spanned by the columns of $U$. But, why does this theorem also apply when my ""$U$"" matrix is a $3\times1$ column vector and hence not an orthogonal matrix (it's not even square)? I had to compute the matrix that represents orthogonal projection onto the line spanned by $(1, 2, -1)$.  So, following the Gram-Schmidt process for this simple case, just set $v_1= (1,2,-1)$.  Then I normalized this vector to make it of unit length.  Finally, I compute $VV^T$ to get a $3\times3$ matrix.  I applied the theorem stated above and concluded that this $3\times3$ matrix represents orthogonal projection onto the line spanned by the one column in $V$, which has the same span as the span of $(1, 2,-1)$, since the Gram-Schmidt process produces a set of orthogonal vectors with the same span as the original set of linearly independent vectors. Have I applied the theorem incorrectly?  I got the correct matrix. Thanks,",,"['linear-algebra', 'matrices', 'projective-geometry', 'orthogonality']"
82,Possible to solve $A + P^{-1}AP = B$?,Possible to solve ?,A + P^{-1}AP = B,"Is it possible to solve for a matrix $A$ in an equation involving a matrix similar to $A$, of the form $$A + P^{-1}AP = B$$? The solution I'd be looking for would be for $A$ in terms of $P$ and $B$, ideally. EDIT: Thanks for the existing answers! Just to add, in the particular case I'm trying to solve, P is diagonal.","Is it possible to solve for a matrix $A$ in an equation involving a matrix similar to $A$, of the form $$A + P^{-1}AP = B$$? The solution I'd be looking for would be for $A$ in terms of $P$ and $B$, ideally. EDIT: Thanks for the existing answers! Just to add, in the particular case I'm trying to solve, P is diagonal.",,['matrices']
83,Proof: $|||AB|||^ \frac{1}{2} \leq |||A ^ \frac{1}{2}B ^ \frac{1}{2}||| $?,Proof: ?,|||AB|||^ \frac{1}{2} \leq |||A ^ \frac{1}{2}B ^ \frac{1}{2}||| ,"I am looking for a proof of the following: \begin{equation*} |||AB|||^ \frac{1}{2} \leq |||A ^ \frac{1}{2}B ^ \frac{1}{2}|||  \end{equation*} Where A, B are positive, hermitian matrices, and $|||⋅|||$ is a unitarily invariant norm. Perhaps someone has a reference to a book, or paper that contains such a proof? I have search myself and cannot seem to find anything. Also tried writing matlab code to find counterexamples and prove the contrary but that does not return anything - which leads to me to believe a proof must exist. Thank you.","I am looking for a proof of the following: \begin{equation*} |||AB|||^ \frac{1}{2} \leq |||A ^ \frac{1}{2}B ^ \frac{1}{2}|||  \end{equation*} Where A, B are positive, hermitian matrices, and $|||⋅|||$ is a unitarily invariant norm. Perhaps someone has a reference to a book, or paper that contains such a proof? I have search myself and cannot seem to find anything. Also tried writing matlab code to find counterexamples and prove the contrary but that does not return anything - which leads to me to believe a proof must exist. Thank you.",,"['matrices', 'inequality', 'normed-spaces', 'matrix-equations']"
84,Solve linear system with variables?,Solve linear system with variables?,,"I have a system of equations like the below: $$x + 3y - z = a \\ x + y + 2z = b \\    2y - 3z = c$$ And have put it in an augmented matrix: $$\begin{bmatrix} 1 & 3 & -1 & a \\ 1 & 1 & 2 & b \\ 0 & 2 & -3 & c\end{bmatrix}$$ I need to find the conditions where the system is consistent (values of $a$, $b$, and $c$ whereby the system has a solution). I've attempted to reduce the matrix to row echelon form, but the last column is getting quite crazy. I have to wonder what I should do (if this is even the correct start) once it is reduced: I am left with things like: $$\begin{bmatrix} 1 & 0 & 0 & \text{a mess}\\ 0 & 1 & 0 & \text{a mess}\\ 0 & 0 & 1 & \text{a mess}\end{bmatrix}$$ If (or when) I get to the endpoint, and only if this is the right methodology, how do I determine the values of $a$, $b$, and $c$ in relation to $z_1$, $z_2$, and $z_3$?","I have a system of equations like the below: $$x + 3y - z = a \\ x + y + 2z = b \\    2y - 3z = c$$ And have put it in an augmented matrix: $$\begin{bmatrix} 1 & 3 & -1 & a \\ 1 & 1 & 2 & b \\ 0 & 2 & -3 & c\end{bmatrix}$$ I need to find the conditions where the system is consistent (values of $a$, $b$, and $c$ whereby the system has a solution). I've attempted to reduce the matrix to row echelon form, but the last column is getting quite crazy. I have to wonder what I should do (if this is even the correct start) once it is reduced: I am left with things like: $$\begin{bmatrix} 1 & 0 & 0 & \text{a mess}\\ 0 & 1 & 0 & \text{a mess}\\ 0 & 0 & 1 & \text{a mess}\end{bmatrix}$$ If (or when) I get to the endpoint, and only if this is the right methodology, how do I determine the values of $a$, $b$, and $c$ in relation to $z_1$, $z_2$, and $z_3$?",,"['linear-algebra', 'matrices']"
85,A question about unitary block matrix,A question about unitary block matrix,,"For $n,m \in \mathbb N$, let $M_{n,m}(\mathbb C)$ denote the set of complex $n \times m$ matrices and put $M_{n}(\mathbb C):=M_{n,n}(\mathbb C)$. For matrices $A \in M_{n}(\mathbb C), B \in M_{n,m}(\mathbb C), C \in M_{m,n}(\mathbb C)$ and $D \in M_{m}(\mathbb C)$, we define the matrix $P \in M_{m+n}(\mathbb C)$ as $$P : = \begin{pmatrix}A &B \\ C & D \end{pmatrix}.$$ Give a necessary and sufficient condition that $P$ is unitary. My attempt: We can find that $$P^* =  \begin{pmatrix}\overline{A^T} &\overline{C^T} \\ \overline{B^T} & \overline{D^T} \end{pmatrix}.$$ Therefore, $P$ is unitary iff $PP*=I_{m+n}$ ($I$ is the identity matrix) iff $$\begin{pmatrix}A\overline{A^T} +B\overline{B^T}&A\overline{C^T} +B\overline{D^T}\\ C\overline{A^T}+D\overline{B^T} &C\overline{C^T}+ D\overline{D^T} \end{pmatrix} = I_{m+n}.$$ Then we end up with  $$A\overline{A^T} +B\overline{B^T}= I_n, A\overline{C^T} +B\overline{D^T}=0_{n,m}, C\overline{A^T}+D\overline{B^T} =0_{m,n}, C\overline{C^T}+ D\overline{D^T}=I_m$$ is the necessary and sufficient condition that $P$ is unitary. Is that a final answer? Can we find another better or more explicit answer?","For $n,m \in \mathbb N$, let $M_{n,m}(\mathbb C)$ denote the set of complex $n \times m$ matrices and put $M_{n}(\mathbb C):=M_{n,n}(\mathbb C)$. For matrices $A \in M_{n}(\mathbb C), B \in M_{n,m}(\mathbb C), C \in M_{m,n}(\mathbb C)$ and $D \in M_{m}(\mathbb C)$, we define the matrix $P \in M_{m+n}(\mathbb C)$ as $$P : = \begin{pmatrix}A &B \\ C & D \end{pmatrix}.$$ Give a necessary and sufficient condition that $P$ is unitary. My attempt: We can find that $$P^* =  \begin{pmatrix}\overline{A^T} &\overline{C^T} \\ \overline{B^T} & \overline{D^T} \end{pmatrix}.$$ Therefore, $P$ is unitary iff $PP*=I_{m+n}$ ($I$ is the identity matrix) iff $$\begin{pmatrix}A\overline{A^T} +B\overline{B^T}&A\overline{C^T} +B\overline{D^T}\\ C\overline{A^T}+D\overline{B^T} &C\overline{C^T}+ D\overline{D^T} \end{pmatrix} = I_{m+n}.$$ Then we end up with  $$A\overline{A^T} +B\overline{B^T}= I_n, A\overline{C^T} +B\overline{D^T}=0_{n,m}, C\overline{A^T}+D\overline{B^T} =0_{m,n}, C\overline{C^T}+ D\overline{D^T}=I_m$$ is the necessary and sufficient condition that $P$ is unitary. Is that a final answer? Can we find another better or more explicit answer?",,"['linear-algebra', 'matrices']"
86,How can I project a matrix on the set of symmetric positive definite matrices with trace 1?,How can I project a matrix on the set of symmetric positive definite matrices with trace 1?,,"Given a square matrix $A \in \mathbb{R}^{n \times n}$ , I need to compute $$ \min_{X \in \Omega} \lVert A - X\rVert^2$$ where $\Omega = \{X \in \mathbb{R}^{n \times n} |\, {\rm tr}(X) = 1, X \text{ is symmetric, }X \geq 0 \}$ , namely I want the projection of a given matrix on the set $\Omega$ . But I also neec to compute it fast. I tried using cvx_solver but it's way too slow computing it directly. is there a better way to write this problem? Or is there a known closed formula or quick algorithm for finding such projection?","Given a square matrix , I need to compute where , namely I want the projection of a given matrix on the set . But I also neec to compute it fast. I tried using cvx_solver but it's way too slow computing it directly. is there a better way to write this problem? Or is there a known closed formula or quick algorithm for finding such projection?","A \in \mathbb{R}^{n \times n}  \min_{X \in \Omega} \lVert A - X\rVert^2 \Omega = \{X \in \mathbb{R}^{n \times n} |\, {\rm tr}(X) = 1, X \text{ is symmetric, }X \geq 0 \} \Omega","['matrices', 'optimization', 'numerical-methods']"
87,Inverse Square Root Of Matrix,Inverse Square Root Of Matrix,,So let's say a matrix is A. Then how do you find A^-1/2 ? It seems to be different from finding the inverse of A. Could someone provide a simple example as explanation? Thanks a lot!!,So let's say a matrix is A. Then how do you find A^-1/2 ? It seems to be different from finding the inverse of A. Could someone provide a simple example as explanation? Thanks a lot!!,,"['matrices', 'statistics']"
88,Inquiry about determinant of $ \left(\begin{matrix} A & B \\ B^T & C \end{matrix}\right)$,Inquiry about determinant of, \left(\begin{matrix} A & B \\ B^T & C \end{matrix}\right),"Based off of http://en.wikipedia.org/wiki/Determinant#Block_matrices , I'm trying to find the formula for $\det(M)$ when $M = \left(\begin{matrix} A & B \\ B^T & C \end{matrix}\right)$. It is known that $A$ and $C$ are diagonal matrices in my use case. I don't even know where to begin proving this. Any proofs or hints in the right direction would be appreciated.","Based off of http://en.wikipedia.org/wiki/Determinant#Block_matrices , I'm trying to find the formula for $\det(M)$ when $M = \left(\begin{matrix} A & B \\ B^T & C \end{matrix}\right)$. It is known that $A$ and $C$ are diagonal matrices in my use case. I don't even know where to begin proving this. Any proofs or hints in the right direction would be appreciated.",,"['matrices', 'determinant']"
89,How to quickly determine the inverse of elementary matrices?,How to quickly determine the inverse of elementary matrices?,,Say I have an elementary matrix associated with a row operation performed when doing Jordan Gaussian elimination so for example if I took the matrix that added 3 times the 1st row and added it to the 3rd row then the matrix would be the $3\times3$ identity matrix with a $3$ in the first column 3rd row instead of a zero. Is there a way to quickly determine it's inverse (as in just by looking at it pretty much and without calculating cofactor matrix and transposing it.) Thanks.,Say I have an elementary matrix associated with a row operation performed when doing Jordan Gaussian elimination so for example if I took the matrix that added 3 times the 1st row and added it to the 3rd row then the matrix would be the $3\times3$ identity matrix with a $3$ in the first column 3rd row instead of a zero. Is there a way to quickly determine it's inverse (as in just by looking at it pretty much and without calculating cofactor matrix and transposing it.) Thanks.,,"['matrices', 'inverse']"
90,"How do I define a block of a $(0,1)$-matrix as one that has no proper sub-blocks?",How do I define a block of a -matrix as one that has no proper sub-blocks?,"(0,1)","I'm struggling to come up with a definition of a ""block"" in a $(0,1)$-matrix $M$ such that we can decompose $M$ into blocks, but the blocks themselves don't further decompose.  This is what I've got so far: Given any $r \times s$ $(0,1)$-matrix $M$, we define a block of $M$ to be a submatrix $H$ in which: (a) every row and every column of $H$ contains a $1$, (b) in $M$, there are no $1$'s in the rows of $H$ outside of $H$, (c) in $M$, there are no $1$'s in the columns of $H$ outside of $H$, and (d) no proper submatrix of $H$ satisfies (a)-(c). I want to define blocks in such a way that there are no proper sub-blocks in blocks.  But I feel item (d) is difficult to parse.  I can't just say ""there's no proper sub-blocks"" because this is a circular definition. Q : Could the community suggest a better way of phrasing this definition? My conundrum reminds me of this comic:","I'm struggling to come up with a definition of a ""block"" in a $(0,1)$-matrix $M$ such that we can decompose $M$ into blocks, but the blocks themselves don't further decompose.  This is what I've got so far: Given any $r \times s$ $(0,1)$-matrix $M$, we define a block of $M$ to be a submatrix $H$ in which: (a) every row and every column of $H$ contains a $1$, (b) in $M$, there are no $1$'s in the rows of $H$ outside of $H$, (c) in $M$, there are no $1$'s in the columns of $H$ outside of $H$, and (d) no proper submatrix of $H$ satisfies (a)-(c). I want to define blocks in such a way that there are no proper sub-blocks in blocks.  But I feel item (d) is difficult to parse.  I can't just say ""there's no proper sub-blocks"" because this is a circular definition. Q : Could the community suggest a better way of phrasing this definition? My conundrum reminds me of this comic:",,"['matrices', 'definition']"
91,Name of the matrix transform $AA^*$ given A?,Name of the matrix transform  given A?,AA^*,"There are a number of places this matrix transform making its appearance: Every positive semi-definite matrix $B$ can have a decomposition $B=AA^*$ If the matrix $A$ is a lower triangular matrix then $B=AA^*$ is also called as Cholesky decomposition of $B$ Given a system of linear equations: $Ax=b$, $(A^*A)x = (A^*b)$ represents ""normal equations"" If X is a random vector, $\mathbb{E}[XX^*]$ is called its covariance matrix Poalr decomposition problem : A = UP ( U nitary and P ositive semi-definite) is solved by $P=\sqrt{A^*A}$  (Courtesy: Jonas Meyer ) ... and so on. I suspect there is a name for the tranformation $A^*A$ or $AA^*$! Is there a name really? What would you suggest if there is n't? A matrix of normal equations ?? (Doesn't look nice!)","There are a number of places this matrix transform making its appearance: Every positive semi-definite matrix $B$ can have a decomposition $B=AA^*$ If the matrix $A$ is a lower triangular matrix then $B=AA^*$ is also called as Cholesky decomposition of $B$ Given a system of linear equations: $Ax=b$, $(A^*A)x = (A^*b)$ represents ""normal equations"" If X is a random vector, $\mathbb{E}[XX^*]$ is called its covariance matrix Poalr decomposition problem : A = UP ( U nitary and P ositive semi-definite) is solved by $P=\sqrt{A^*A}$  (Courtesy: Jonas Meyer ) ... and so on. I suspect there is a name for the tranformation $A^*A$ or $AA^*$! Is there a name really? What would you suggest if there is n't? A matrix of normal equations ?? (Doesn't look nice!)",,"['linear-algebra', 'matrices', 'terminology', 'matrix-equations', 'matrix-decomposition']"
92,Using jordan form to find nilpotent $4\times 4$ matrices,Using jordan form to find nilpotent  matrices,4\times 4,"Related to this question. I am trying to understand how to show all nilpotent matrices of size $4\times 4$ using the Jordan normal form. From what I understand, we string together Jordan blocks of different sizes. I am trying to find how to find all forms of some matrix $A$ where $A$ is a $4\times 4$ matrix, for $A^2=0$ using these blocks. Since all the eigenvalues of a nilpotent matrix are $0$, the Jordan form should look like this I thought: $$\def\b{\begin{bmatrix}}\def\e{\end{bmatrix}} \b \lambda&1&0&0\\0&\lambda&1&0\\0&0&\lambda&1\\0&0&0&\lambda\e=\b 0&1&0&0\\0&0&1&0\\0&0&0&1\\0&0&0&0\e$$ As a single Jordan block(which seems right, since I have one eigenvalue $0$) This isn't nilpotent of degree $2$ Also Will Jagy suggested in my previous question that I use $2$ $2\times 2$ matrices, or a $2\times 2$ and two $1\times 1$ blocks. I can see that two $2\times 2$ blocks do give me $A^2=0$, but I can't get this to work with one $2\times 2$ block and two $1\times 1$ blocks.","Related to this question. I am trying to understand how to show all nilpotent matrices of size $4\times 4$ using the Jordan normal form. From what I understand, we string together Jordan blocks of different sizes. I am trying to find how to find all forms of some matrix $A$ where $A$ is a $4\times 4$ matrix, for $A^2=0$ using these blocks. Since all the eigenvalues of a nilpotent matrix are $0$, the Jordan form should look like this I thought: $$\def\b{\begin{bmatrix}}\def\e{\end{bmatrix}} \b \lambda&1&0&0\\0&\lambda&1&0\\0&0&\lambda&1\\0&0&0&\lambda\e=\b 0&1&0&0\\0&0&1&0\\0&0&0&1\\0&0&0&0\e$$ As a single Jordan block(which seems right, since I have one eigenvalue $0$) This isn't nilpotent of degree $2$ Also Will Jagy suggested in my previous question that I use $2$ $2\times 2$ matrices, or a $2\times 2$ and two $1\times 1$ blocks. I can see that two $2\times 2$ blocks do give me $A^2=0$, but I can't get this to work with one $2\times 2$ block and two $1\times 1$ blocks.",,['matrices']
93,"In terms of matrices: $\forall v\in V,\phi(\phi(v))=0$",In terms of matrices:,"\forall v\in V,\phi(\phi(v))=0","$\phi: V\to V$( a linear operator here) How to interpret $\forall v\in V,\phi(\phi(v))=0$ in terms of matrices? Can I have some hint? I suppose $\phi(V)= \begin{bmatrix} \phi(v_1)\\\phi(v_2)\\\dots\\\phi(v_n)\end{bmatrix}$ for $V=\dim n$","$\phi: V\to V$( a linear operator here) How to interpret $\forall v\in V,\phi(\phi(v))=0$ in terms of matrices? Can I have some hint? I suppose $\phi(V)= \begin{bmatrix} \phi(v_1)\\\phi(v_2)\\\dots\\\phi(v_n)\end{bmatrix}$ for $V=\dim n$",,['abstract-algebra']
94,Find the Least Integer $k$ such that $B^k=I$,Find the Least Integer  such that,k B^k=I,"9>If $A$ and $B$ are two non-singular matrices such that $B\ne I$ , $A^6=I$ and $$AB^2=BA,$$ then what is the  least positive integer $k$ such that $B^k=I$ ? My Try: Given $$AB^2=BA$$ which we can write as $$ABB=BA$$ Post Multiply with $A$ we get $$ABBA=BA^2$$ i.e., $$ABAB^2=BA^2$$ i.e., $$A^2B^4=BA^2$$ Continuing we get $$A^3B^8=BA^3$$ $$A^4B^{16}=BA^4$$ $$A^5B^{32}=BA^5$$ and finally $$A^6B^{64}=BA^6$$ and since $A^6=I$ we have $$B^{64}=B$$ i.e, $$B^{63}=I$$ so $k$ is 63. Can i have any better approach?","9>If and are two non-singular matrices such that , and then what is the  least positive integer such that ? My Try: Given which we can write as Post Multiply with we get i.e., i.e., Continuing we get and finally and since we have i.e, so is 63. Can i have any better approach?","A B B\ne I A^6=I AB^2=BA, k B^k=I AB^2=BA ABB=BA A ABBA=BA^2 ABAB^2=BA^2 A^2B^4=BA^2 A^3B^8=BA^3 A^4B^{16}=BA^4 A^5B^{32}=BA^5 A^6B^{64}=BA^6 A^6=I B^{64}=B B^{63}=I k","['linear-algebra', 'matrices']"
95,Dimesion of a subspace subject to linear constraints,Dimesion of a subspace subject to linear constraints,,"Suppose $X$ is $n\times K$ with full column rank $K$ and $G$ is $q\times K$ with full row rank $q$. If $q<K$, how do I see that $\mathcal{L}\equiv\{Xb,b\in\mathbb{R}^K,Gb=0\}$ has dimension    $K-q$? Because $X$ has full column rank, its column space can be spanned by $K$ independent column vectors in $\mathbb{R}^n$. The constraint $Gb=0$ imposes limits on the coefficients contained in $b$ when we form linear combinations of these $K$ vectors. But I don't know how to proceed formally. Thank you for your help.","Suppose $X$ is $n\times K$ with full column rank $K$ and $G$ is $q\times K$ with full row rank $q$. If $q<K$, how do I see that $\mathcal{L}\equiv\{Xb,b\in\mathbb{R}^K,Gb=0\}$ has dimension    $K-q$? Because $X$ has full column rank, its column space can be spanned by $K$ independent column vectors in $\mathbb{R}^n$. The constraint $Gb=0$ imposes limits on the coefficients contained in $b$ when we form linear combinations of these $K$ vectors. But I don't know how to proceed formally. Thank you for your help.",,"['linear-algebra', 'matrices', 'vector-spaces']"
96,Does $\log(f(X))$ concave implies $\log(f(X^{-1}))$ convex?,Does  concave implies  convex?,\log(f(X)) \log(f(X^{-1})),"One of my professor claims that $\log f(X)$ concave implies that $\log(f(X^{-1}))$ convex where $X$ is symmetric positive definite matrix. $\log(f(X))$ is a function defined on symmetric positive definite matrix. When I ask him why, he said if you do not believe it, give me a counterexample. So I wonder is it true or any counterexample to this claim. I only know how to check a composition function convex by computing its Hessian. But what would be the Hessian in this case? I do not know how to compute.","One of my professor claims that $\log f(X)$ concave implies that $\log(f(X^{-1}))$ convex where $X$ is symmetric positive definite matrix. $\log(f(X))$ is a function defined on symmetric positive definite matrix. When I ask him why, he said if you do not believe it, give me a counterexample. So I wonder is it true or any counterexample to this claim. I only know how to check a composition function convex by computing its Hessian. But what would be the Hessian in this case? I do not know how to compute.",,"['matrices', 'convex-analysis', 'convex-optimization']"
97,I have a problem with finding the trace to a matrix.,I have a problem with finding the trace to a matrix.,,"Let $T$ be a matrix of which I know its characteristic values, how can I find $\operatorname{Tr}(T-I)^{-1}$? I know that the sum of the characteristic values is the trace, but I'm having a problem as I can't understand how I can know the new characteristic values of the new matrix using the old characteristic values and the old matrix.","Let $T$ be a matrix of which I know its characteristic values, how can I find $\operatorname{Tr}(T-I)^{-1}$? I know that the sum of the characteristic values is the trace, but I'm having a problem as I can't understand how I can know the new characteristic values of the new matrix using the old characteristic values and the old matrix.",,"['linear-algebra', 'matrices', 'trace']"
98,How to solve $C = X^\top C X$?,How to solve ?,C = X^\top C X,All matrices are $n \times n$. $C$ is real symmetric positive definite. How to solve $C = X^\top C X$ for $X$? I am interested in characterizing both the set of real matrices satisfying the equation and the (possibly larger) set of complex solutions.,All matrices are $n \times n$. $C$ is real symmetric positive definite. How to solve $C = X^\top C X$ for $X$? I am interested in characterizing both the set of real matrices satisfying the equation and the (possibly larger) set of complex solutions.,,"['linear-algebra', 'matrices', 'matrix-equations', 'matrix-calculus']"
99,Number of self-inverse matrices over prime field,Number of self-inverse matrices over prime field,,"Regarding the cryptosystem known as the Hill Cipher, my textbook by Douglas R. Stinson has an exercise asking you to find the number of involutory keys for $m=2$ over $\mathbb Z_{26}$. This means that we are to find the number of $2\times 2$ matrices over $\mathbb Z_{26}$ that satisfies $K=K^{-1}$. This can be considered a basic mathematical problem regardless of the details of the Hill Cipher cryptosystem. I have found the number to be $736$ by applying the easily proven result $$ K=K^{-1}\implies\det(K)=\pm1\pmod{26} $$ stated in the previous exercise, and the general formula $$ \begin{pmatrix} a&b\\c&d \end{pmatrix}^{-1} =\det(K)^{-1} \begin{pmatrix} d&-b\\-c&a \end{pmatrix} $$ stated elsewhere in the textbook and hinted at in the exercise in question. But here is my problem: This approach seemed far from general, dividing into cases and subcases regarding whether $\det(K)=1$ or $\det(K)=-1$ and regarding the profiles of $a,b,c$ modulo $2,13$ respectively (Chineese Remainder Theorem [CRT]). This lead to conclusions equivalent to saying that for $\det(K)=-1$ the equation $K=K^{-1}$ has $4$ solutions over $\mathbb Z_2$ and $182$ solutions over $\mathbb Z_{13}$. By [CRT] this provides $4\cdot182=728$ solutions over $\mathbb Z_{26}$ for the subcase $\det(K)=-1$. The case $\det(K)=1$ provided $8$ matrices in my analysis. So my question is: Is there a general approach for finding the number of solutions to the matrix equation $K=K^{-1}$ over a prime field $\mathbb Z_p$ for matrices of arbitrary size $m\times m$?","Regarding the cryptosystem known as the Hill Cipher, my textbook by Douglas R. Stinson has an exercise asking you to find the number of involutory keys for $m=2$ over $\mathbb Z_{26}$. This means that we are to find the number of $2\times 2$ matrices over $\mathbb Z_{26}$ that satisfies $K=K^{-1}$. This can be considered a basic mathematical problem regardless of the details of the Hill Cipher cryptosystem. I have found the number to be $736$ by applying the easily proven result $$ K=K^{-1}\implies\det(K)=\pm1\pmod{26} $$ stated in the previous exercise, and the general formula $$ \begin{pmatrix} a&b\\c&d \end{pmatrix}^{-1} =\det(K)^{-1} \begin{pmatrix} d&-b\\-c&a \end{pmatrix} $$ stated elsewhere in the textbook and hinted at in the exercise in question. But here is my problem: This approach seemed far from general, dividing into cases and subcases regarding whether $\det(K)=1$ or $\det(K)=-1$ and regarding the profiles of $a,b,c$ modulo $2,13$ respectively (Chineese Remainder Theorem [CRT]). This lead to conclusions equivalent to saying that for $\det(K)=-1$ the equation $K=K^{-1}$ has $4$ solutions over $\mathbb Z_2$ and $182$ solutions over $\mathbb Z_{13}$. By [CRT] this provides $4\cdot182=728$ solutions over $\mathbb Z_{26}$ for the subcase $\det(K)=-1$. The case $\det(K)=1$ provided $8$ matrices in my analysis. So my question is: Is there a general approach for finding the number of solutions to the matrix equation $K=K^{-1}$ over a prime field $\mathbb Z_p$ for matrices of arbitrary size $m\times m$?",,"['linear-algebra', 'matrices', 'finite-fields', 'involutions']"
