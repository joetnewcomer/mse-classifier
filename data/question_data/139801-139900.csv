,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Trouble in using finite difference method to solve a boundary value problem (attempts and pictures included),Trouble in using finite difference method to solve a boundary value problem (attempts and pictures included),,"I want to numerically solve (using FDM) $$-y''(t)+2y'(t)=1, t\in (0,1)\\y(0)=1, y(1)=3$$ First, I check with symbolab that the analytic solution would be $1-\frac{3}{2\left(-1+e^2\right)}+\frac{3}{2\left(-1+e^2\right)}e^{2t}+\frac{t}{2}$ : $\frac{d}{dt}\left(1-\frac{3}{2\left(-1+e^2\right)}+\frac{3}{2\left(-1+e^2\right)}e^{2t}+\frac{t}{2}\right) = \frac{3}{e^2-1}e^{2t}+\frac{1}{2}$ , and $\frac{d^2}{dt^2}\left(1-\frac{3}{2\left(-1+e^2\right)}+\frac{3}{2\left(-1+e^2\right)}e^{2t}+\frac{t}{2}\right) = \frac{6}{e^2-1}e^{2t}$ , and we can easily check that the boundary conditions are satisfied. Now, I attempt this problem numerically: Using the centered difference approximations for the first and second derivative, I get two equations: $D^2y_j = (y_{j-1}-2y_j+y_{j+1})/h^2\\Dy_j = (y_{j-1}+y_{j+1})/2h$ . I use the second order equation of the problem to derive the relation $(\frac{-1}{h^2} + \frac{1}{h})y_{j-1}+\frac{2}{h^2}y_j+(\frac{-1}{h^2} + \frac{1}{h})y_{j+1}$ . For example, if we take a mesh size of $.25$ , we would have to solve 3 unknowns at $.25, .5, .75$ , and our matrix equation would look like $$\begin{pmatrix}32&-12&0\\ -12&32&-12\\ 0&-12&32\end{pmatrix}v = \begin{pmatrix}1+12\\ \:\:1\\ \:\:1+12\cdot 3\end{pmatrix} = \begin{pmatrix}13\\ 1\\ 37\end{pmatrix}\\v= \begin{pmatrix}y_{.25}\\ \:\:y_{.5}\\ y_{.75}\end{pmatrix}$$ But if we solve this $\begin{pmatrix}y_{.25}\\ \:\:y_{.5}\\ y_{.75}\end{pmatrix} = \begin{pmatrix}\frac{67}{92}\\ \frac{79}{92}\\ \frac{34}{23}\end{pmatrix}$ , and plot it, it looks off from the analytic solution. One would think that increasing the number of nodes between 0 and 1 would be better, but increasing the unknown to 500 only made it worse: Now, I may have made the program incorrectly, but the matrix vector equations seem to come out fine. I will upload the code if anyone is interested, but I think that my problem comes from a misunderstanding of the algorithm of the finite difference method. Any help would be lovely.","I want to numerically solve (using FDM) First, I check with symbolab that the analytic solution would be : , and , and we can easily check that the boundary conditions are satisfied. Now, I attempt this problem numerically: Using the centered difference approximations for the first and second derivative, I get two equations: . I use the second order equation of the problem to derive the relation . For example, if we take a mesh size of , we would have to solve 3 unknowns at , and our matrix equation would look like But if we solve this , and plot it, it looks off from the analytic solution. One would think that increasing the number of nodes between 0 and 1 would be better, but increasing the unknown to 500 only made it worse: Now, I may have made the program incorrectly, but the matrix vector equations seem to come out fine. I will upload the code if anyone is interested, but I think that my problem comes from a misunderstanding of the algorithm of the finite difference method. Any help would be lovely.","-y''(t)+2y'(t)=1, t\in (0,1)\\y(0)=1, y(1)=3 1-\frac{3}{2\left(-1+e^2\right)}+\frac{3}{2\left(-1+e^2\right)}e^{2t}+\frac{t}{2} \frac{d}{dt}\left(1-\frac{3}{2\left(-1+e^2\right)}+\frac{3}{2\left(-1+e^2\right)}e^{2t}+\frac{t}{2}\right) = \frac{3}{e^2-1}e^{2t}+\frac{1}{2} \frac{d^2}{dt^2}\left(1-\frac{3}{2\left(-1+e^2\right)}+\frac{3}{2\left(-1+e^2\right)}e^{2t}+\frac{t}{2}\right) = \frac{6}{e^2-1}e^{2t} D^2y_j = (y_{j-1}-2y_j+y_{j+1})/h^2\\Dy_j = (y_{j-1}+y_{j+1})/2h (\frac{-1}{h^2} + \frac{1}{h})y_{j-1}+\frac{2}{h^2}y_j+(\frac{-1}{h^2} + \frac{1}{h})y_{j+1} .25 .25, .5, .75 \begin{pmatrix}32&-12&0\\ -12&32&-12\\ 0&-12&32\end{pmatrix}v = \begin{pmatrix}1+12\\ \:\:1\\ \:\:1+12\cdot 3\end{pmatrix} = \begin{pmatrix}13\\ 1\\ 37\end{pmatrix}\\v= \begin{pmatrix}y_{.25}\\ \:\:y_{.5}\\ y_{.75}\end{pmatrix} \begin{pmatrix}y_{.25}\\ \:\:y_{.5}\\ y_{.75}\end{pmatrix} = \begin{pmatrix}\frac{67}{92}\\ \frac{79}{92}\\ \frac{34}{23}\end{pmatrix}","['matrices', 'ordinary-differential-equations', 'numerical-methods', 'finite-differences', 'finite-element-method']"
1,Can we define $\ln(x)$ as a solution of $\frac{d}{dx}(f(x))=\frac{1}{x}$?,Can we define  as a solution of ?,\ln(x) \frac{d}{dx}(f(x))=\frac{1}{x},Can we define $\ln(x)$ as the solution of the differential equation $$\frac{d}{dx}(f(x))=\frac{1}{x}$$ such that $\ln(1)=0$ ?,Can we define as the solution of the differential equation such that ?,\ln(x) \frac{d}{dx}(f(x))=\frac{1}{x} \ln(1)=0,"['ordinary-differential-equations', 'logarithms']"
2,Solutions to the under-damped Harmonic Oscillator equation ??,Solutions to the under-damped Harmonic Oscillator equation ??,,"For the damped harmonic oscillator equation $$\frac{d^2x}{dt^2}+\frac{c}{m}\frac{dx}{dt}+\frac{k}{m}x=0$$ we get that the general solution is $$x(t)=Ae^{-\gamma t}e^{i\omega_d t}+Be^{-\gamma t}e^{-i\omega_d t}$$ where $\gamma = \frac{c}{2m}$ and $ \omega_d=\sqrt{\omega^2-\gamma ^2}$ . Using Euler's equation, we can expand this as follows: $$Ae^{-\gamma t}(\cos(\omega _dt)+i\sin(\omega_d t))+Be^{-\gamma t}(\cos(\omega _dt)-i\sin(\omega_d t))$$ $$\Rightarrow e^{-\gamma t}(A+B)\cos(\omega_d t) +e^{-\gamma t}(Ai-Bi)\sin(\omega_d t)$$ But now we are dealing with a physical problem so we only examine the real part which is $e^{-\gamma t}(A+B)\cos(\omega_d t)$ . But this does not have any phase difference. Yet textbooks always make the claim that the real part of the solution is $$e^{-\gamma t}(C)\cos(\omega_d t+\phi)$$ where $\phi$ is some arbitrary initial phase. But where does that initial phase come from if the real part of the solution does not have a phase change in it? I understand that $A$ and $B$ themselves need not be real however I do not understand how this fact could ever lead to a non zero initial phase in the real part of the solution. This issue has bothered me for quite some time now so any help would be immensely appreciated!","For the damped harmonic oscillator equation we get that the general solution is where and . Using Euler's equation, we can expand this as follows: But now we are dealing with a physical problem so we only examine the real part which is . But this does not have any phase difference. Yet textbooks always make the claim that the real part of the solution is where is some arbitrary initial phase. But where does that initial phase come from if the real part of the solution does not have a phase change in it? I understand that and themselves need not be real however I do not understand how this fact could ever lead to a non zero initial phase in the real part of the solution. This issue has bothered me for quite some time now so any help would be immensely appreciated!",\frac{d^2x}{dt^2}+\frac{c}{m}\frac{dx}{dt}+\frac{k}{m}x=0 x(t)=Ae^{-\gamma t}e^{i\omega_d t}+Be^{-\gamma t}e^{-i\omega_d t} \gamma = \frac{c}{2m}  \omega_d=\sqrt{\omega^2-\gamma ^2} Ae^{-\gamma t}(\cos(\omega _dt)+i\sin(\omega_d t))+Be^{-\gamma t}(\cos(\omega _dt)-i\sin(\omega_d t)) \Rightarrow e^{-\gamma t}(A+B)\cos(\omega_d t) +e^{-\gamma t}(Ai-Bi)\sin(\omega_d t) e^{-\gamma t}(A+B)\cos(\omega_d t) e^{-\gamma t}(C)\cos(\omega_d t+\phi) \phi A B,"['complex-analysis', 'ordinary-differential-equations', 'complex-numbers', 'mathematical-physics']"
3,Weird looking ODE solution: How to verify this is indeed a solution?,Weird looking ODE solution: How to verify this is indeed a solution?,,"I will try to express my question using an example. Consider this homogeneous ODE: $y' = \frac{y-x}{x+y }$ Its solution is: $\boxed{\frac12 \log\left( \frac{y^2(x)}{x^2} +1\right) - \log(x) + \arctan\left( \frac{y(x)}{x}\right) = c} \quad  c\in \mathbb{R}$ As far as I am understanding solving a differential equation means finding a $y(x)$ that satisfies the differential equation. But this particular solution, cannot be expressed in terms of $y(x)$ (at least I can't) . If the previous argument is true how could one verify the solution, given the fact that there is no actual $y$ to plug into the ODE? If the previous argument is false, how could the solution be expressed in terms of $y$ ? P.S: A previous question I've asked is How do $\arctan$ and $\ln$ relate? . Given the fact that the solution only contains those two functions, I suspect that it may be helpful to note this down. Could there be some complex analysis involved?","I will try to express my question using an example. Consider this homogeneous ODE: Its solution is: As far as I am understanding solving a differential equation means finding a that satisfies the differential equation. But this particular solution, cannot be expressed in terms of (at least I can't) . If the previous argument is true how could one verify the solution, given the fact that there is no actual to plug into the ODE? If the previous argument is false, how could the solution be expressed in terms of ? P.S: A previous question I've asked is How do and relate? . Given the fact that the solution only contains those two functions, I suspect that it may be helpful to note this down. Could there be some complex analysis involved?",y' = \frac{y-x}{x+y } \boxed{\frac12 \log\left( \frac{y^2(x)}{x^2} +1\right) - \log(x) + \arctan\left( \frac{y(x)}{x}\right) = c} \quad  c\in \mathbb{R} y(x) y(x) y y \arctan \ln,"['ordinary-differential-equations', 'soft-question']"
4,Limit Behavior of Differential Equation,Limit Behavior of Differential Equation,,"I have a differential equation of the form $$r(t)f(t,x) + \frac{\partial}{\partial t}f(t,x) = g(t,x)$$ where $r(t),g(t,x)$ are given and I want to solve for $f(\cdot,\cdot)$ . I also know that there exist a constant $r$ and a function $g(x)$ such that $\lim_{t \to \infty} r(t) = r^{\ast}$ and $\lim_{t \to \infty} g(t,x) = g^{\ast}(x)$ uniformly over $x$ . In the limit as $t \to \infty$ , the differential equation would become $r^{\ast} f(x) = g^{\ast}(x)$ , so that $f(x) = g^{\ast}(x)/r^{\ast}$ . If $f(t,x)$ is a solution to the original differential equation, under what conditions do we have that $$\lim_{t \to \infty} f(t,x) = \frac{g^{\ast}(x)}{r^{\ast}}$$","I have a differential equation of the form where are given and I want to solve for . I also know that there exist a constant and a function such that and uniformly over . In the limit as , the differential equation would become , so that . If is a solution to the original differential equation, under what conditions do we have that","r(t)f(t,x) + \frac{\partial}{\partial t}f(t,x) = g(t,x) r(t),g(t,x) f(\cdot,\cdot) r g(x) \lim_{t \to \infty} r(t) = r^{\ast} \lim_{t \to \infty} g(t,x) = g^{\ast}(x) x t \to \infty r^{\ast} f(x) = g^{\ast}(x) f(x) = g^{\ast}(x)/r^{\ast} f(t,x) \lim_{t \to \infty} f(t,x) = \frac{g^{\ast}(x)}{r^{\ast}}","['functional-analysis', 'ordinary-differential-equations']"
5,Asymptotical behavior of the SIR epidemic model,Asymptotical behavior of the SIR epidemic model,,"The SIR epidemic model presents three differential equations for three time-dependent variables $s(t), i(t), r(t)$ : $$\begin{align} 	\frac{ds}{dt} & = - \beta i s 			 \\ 	\frac{di}{dt} & = \beta i s - \gamma i	 \\ 	\frac{dr}{dt} & = \gamma i  \end{align}$$ It is assumed that the variables are non-negative, $s(t) + i(t) + r(t) = 1$ , and the coefficients $\beta, \gamma$ are positive. In the literature it is claimed or taken as self-evident that $$ 	\lim_{t \to \infty} i(t) = 0 $$ How can this behavior be proven rigorously?","The SIR epidemic model presents three differential equations for three time-dependent variables : It is assumed that the variables are non-negative, , and the coefficients are positive. In the literature it is claimed or taken as self-evident that How can this behavior be proven rigorously?","s(t), i(t), r(t) \begin{align}
	\frac{ds}{dt} & = - \beta i s 			 \\
	\frac{di}{dt} & = \beta i s - \gamma i	 \\
	\frac{dr}{dt} & = \gamma i 
\end{align} s(t) + i(t) + r(t) = 1 \beta, \gamma 
	\lim_{t \to \infty} i(t) = 0
","['ordinary-differential-equations', 'limits', 'dynamical-systems', 'stability-in-odes']"
6,Find equilibrium points when given polar coordinates,Find equilibrium points when given polar coordinates,,"Consider the following system given in polar coordinates $\dot{r}=-r^3+r+r\sin{(2\theta)}/2$ and $\dot{\theta} = 1+\cos^2{\theta}$ . Find all equilibria and show that there are no invariant circles centered at the origin. I know that when a system is given in cartesian coordinates then we convert to polar coordinates. And to find equilibria, we just look at values of $x$ and $y$ such that $\dot{x}$ and $\dot{y}$ are $0$ . But here, we are given only polar coordinates. Can we convert it back to cartesian coordinates and then find equilibrium points? Otherwise, using polar coordinates, I can think of only origin as the equilibrium point since $r=0$ will yield $\dot{r}=0$ . And no other point will yield $\dot{\theta}=0$ . So would origin be the only equilibrium point? And how do we show there are no invariant circles centered at the origin?","Consider the following system given in polar coordinates and . Find all equilibria and show that there are no invariant circles centered at the origin. I know that when a system is given in cartesian coordinates then we convert to polar coordinates. And to find equilibria, we just look at values of and such that and are . But here, we are given only polar coordinates. Can we convert it back to cartesian coordinates and then find equilibrium points? Otherwise, using polar coordinates, I can think of only origin as the equilibrium point since will yield . And no other point will yield . So would origin be the only equilibrium point? And how do we show there are no invariant circles centered at the origin?",\dot{r}=-r^3+r+r\sin{(2\theta)}/2 \dot{\theta} = 1+\cos^2{\theta} x y \dot{x} \dot{y} 0 r=0 \dot{r}=0 \dot{\theta}=0,['ordinary-differential-equations']
7,Lagrange formula for solving linear differential equations,Lagrange formula for solving linear differential equations,,"I am studying control systems, and my textbook uses ""Lagrange's formula"" for solving time-continuous linear systems in ""state-space"". Below are the equations presented: $$\dot{x}(t) = Ax(t) + Bu(t)$$ $$y(t) = Cx(t) + Du(t)$$ where A,B,C and D are matrices of coefficients, but let's assume they are all singular to make things simpler. Assuming that $t > t_0$ and $x(t_0) = x_{t_0}$ ,the formula used to calculate $x(t)$ is the following: $$x(t) = e^{A(t-t_0)}x_{t_0} + \int_{t_0}^t e^{A(t-\tau)}Bu(\tau) d\tau $$ This formula is very similar to another formula I learned in calc 2 for an identical purpose, albeit this next formula is defined for all values of t and contains an indefinite integral in place of the definite integral in the formula above $$ x(t) = e^{-At} \int Bu(t)e^{At} dt $$ It is obvious to me that these formulas are strongly connected, but whereas I understand how the second one is derived, I cannot say the same about the first one. And what is up with using $\tau$ as the variable for intergration? I've been told it is a ""dummy variable"", but it was presented as a fact, rather than a proven result of calculus. I've also tried to google this so-called ""Lagrange formula"", but unfortunately I haven't had any success with it. Can anybody please help me understand this?","I am studying control systems, and my textbook uses ""Lagrange's formula"" for solving time-continuous linear systems in ""state-space"". Below are the equations presented: where A,B,C and D are matrices of coefficients, but let's assume they are all singular to make things simpler. Assuming that and ,the formula used to calculate is the following: This formula is very similar to another formula I learned in calc 2 for an identical purpose, albeit this next formula is defined for all values of t and contains an indefinite integral in place of the definite integral in the formula above It is obvious to me that these formulas are strongly connected, but whereas I understand how the second one is derived, I cannot say the same about the first one. And what is up with using as the variable for intergration? I've been told it is a ""dummy variable"", but it was presented as a fact, rather than a proven result of calculus. I've also tried to google this so-called ""Lagrange formula"", but unfortunately I haven't had any success with it. Can anybody please help me understand this?",\dot{x}(t) = Ax(t) + Bu(t) y(t) = Cx(t) + Du(t) t > t_0 x(t_0) = x_{t_0} x(t) x(t) = e^{A(t-t_0)}x_{t_0} + \int_{t_0}^t e^{A(t-\tau)}Bu(\tau) d\tau   x(t) = e^{-At} \int Bu(t)e^{At} dt  \tau,"['calculus', 'ordinary-differential-equations', 'control-theory', 'linear-control']"
8,A variant form of Gronwall's inequality,A variant form of Gronwall's inequality,,"Suppose $u(t)$ is a measurable function on $[0,\infty)$ and satisfies $$0\le u(t) \le A + Bt\int_0^t u(s) ds$$ for all $t\ge 0$ ; here $A, B$ are positive constants. Is it possible to derive an upper bound for $u(t)$ ? Thanks for your help.",Suppose is a measurable function on and satisfies for all ; here are positive constants. Is it possible to derive an upper bound for ? Thanks for your help.,"u(t) [0,\infty) 0\le u(t) \le A + Bt\int_0^t u(s) ds t\ge 0 A, B u(t)","['real-analysis', 'ordinary-differential-equations', 'reference-request']"
9,Are singular foliations spanned by collinear vector fields equal?,Are singular foliations spanned by collinear vector fields equal?,,"Let $M$ be a compact $n$ -manifold (let's say with boundary, but this isn't too important), $X$ a vector field on $M$ and $f:M \to \mathbb{R}$ a non-zero function on $M$ . My question: are the singular foliations spanned by $X$ and $fX$ equal? In other words, do the traces of trajectories of $X$ and $fX$ with the same starting point coincide? I tried a couple of simple examples and checked that they do coincide, and I did a heuristic argument which works in my favor: if $p \in M$ such that $X_p = 0$ , then obviously the trajectories starting at $p$ of $X$ and $fX$ coincide (it's just the point $p$ ). Otherwise, there exists a chart centered at $p$ where $X = \frac{\partial}{\partial x_1}$ , and so in this chart, $\phi_t^X(p) = (e^t,0,\dots,0)$ . On the other hand, the flow of $fX$ starting at $p$ is locally the solution of the equation $$x' = (f(x) \cdot x_1, 0, \dots, 0), \hspace{5pt} x(0) = 0,$$ and if we set $g(x_1) := f(x_1,0,\dots,0)$ , then $x_1' = g(x_1)x_1$ , $x_2 = \cdots = x_n = 0.$ If we define $$G(x_1) := \int \frac{dx_1}{g(x_1)x_1},$$ then, heuristically, $$x_1 = G^{-1}(t),$$ and both of the trajectories are contained in the $\{ x_2 = \cdots = x_n = 0\}$ -part of the chart. However, I don't think this is a difficult question and I'd like to see a formal argument. I suspect that the only reason that I'm unable to resolve this is because of my (very) rusty knowledge of the theory of ODE's.","Let be a compact -manifold (let's say with boundary, but this isn't too important), a vector field on and a non-zero function on . My question: are the singular foliations spanned by and equal? In other words, do the traces of trajectories of and with the same starting point coincide? I tried a couple of simple examples and checked that they do coincide, and I did a heuristic argument which works in my favor: if such that , then obviously the trajectories starting at of and coincide (it's just the point ). Otherwise, there exists a chart centered at where , and so in this chart, . On the other hand, the flow of starting at is locally the solution of the equation and if we set , then , If we define then, heuristically, and both of the trajectories are contained in the -part of the chart. However, I don't think this is a difficult question and I'd like to see a formal argument. I suspect that the only reason that I'm unable to resolve this is because of my (very) rusty knowledge of the theory of ODE's.","M n X M f:M \to \mathbb{R} M X fX X fX p \in M X_p = 0 p X fX p p X = \frac{\partial}{\partial x_1} \phi_t^X(p) = (e^t,0,\dots,0) fX p x' = (f(x) \cdot x_1, 0, \dots, 0), \hspace{5pt} x(0) = 0, g(x_1) := f(x_1,0,\dots,0) x_1' = g(x_1)x_1 x_2 = \cdots = x_n = 0. G(x_1) := \int \frac{dx_1}{g(x_1)x_1}, x_1 = G^{-1}(t), \{ x_2 = \cdots = x_n = 0\}","['ordinary-differential-equations', 'differential-geometry', 'vector-fields', 'foliations']"
10,Finding the general solution of a system of Differential Equations,Finding the general solution of a system of Differential Equations,,"I need helping find the general solution to the following systen $$ x'=2t^2+2-4x+6y $$ $$ y'=-2t^2-t+6-3x+5y $$ I know i need to turn the 2 equations into a matrix, but I can't figure out how to do it. So far I have the matrix for the $x$ and $y$ values and for $t$ , but I don't know what to do with the $+2$ and $+6$ in each equation.","I need helping find the general solution to the following systen I know i need to turn the 2 equations into a matrix, but I can't figure out how to do it. So far I have the matrix for the and values and for , but I don't know what to do with the and in each equation.", x'=2t^2+2-4x+6y   y'=-2t^2-t+6-3x+5y  x y t +2 +6,"['ordinary-differential-equations', 'systems-of-equations']"
11,Why is $f(x)=C_1e^{\lambda_1x}+C_2e^{\lambda_2x}$ the only solution family to $a\cdot f''(x)+b\cdot f'(x)+c\cdot f(x)=0$,Why is  the only solution family to,f(x)=C_1e^{\lambda_1x}+C_2e^{\lambda_2x} a\cdot f''(x)+b\cdot f'(x)+c\cdot f(x)=0,"I was taught that $f(x)=C_1e^{\lambda_1x}+C_2e^{\lambda_2x}$ is the only solution family to $a\cdot f''(x)+b\cdot f'(x)+c\cdot f(x)=0$ (provided that $b^2-4ac\neq0$ ). Here is my teacher's proof showing why: Assume that $f(x)=e^{\lambda x}$ for some $\lambda$ . Then the differential equation becomes: $$a\frac{d^2}{dx^2}e^{\lambda x}+b\frac{d}{dx}e^{\lambda x}+ce^{\lambda x}=0$$ $$a\lambda^2e^{\lambda x}+b\lambda e^{\lambda x}+ce^{\lambda x}=0$$ $$e^{\lambda x}(a\lambda^2+b\lambda+c)=0$$ $$a\lambda^2+b\lambda+c=0$$ Then if $\lambda_1, \lambda_2$ are the roots of the equation $a\lambda^2+b\lambda+c=0$ , $f(x)=e^{\lambda_1 x}$ and $f(x)=e^{\lambda_2 x}$ are solutions to the differential equation. Now suppose $g(x)$ is a solution to the differential equation, then we can show $C\cdot g(x)$ is a solution to the differential equation where $C$ is a real number: $$aC\cdot g''(x)+bC\cdot g'(x)+cC\cdot g(x)=0$$ $$C(a\cdot g''(x)+b\cdot g'(x)+c\cdot g(x)=0)$$ $$a\cdot g''(x)+b\cdot g'(x)+c\cdot g(x)=0$$ which is true by definition. Therefore we know $f(x)=C_1e^{\lambda_1 x}$ and $f(x)=C_2e^{\lambda_2 x}$ are solution sets for real $C_1, C_2$ . We can also show $f_1(x)+f_2(x)$ is a solution to the differential equation provided that $f_1(x)$ and $f_2(x)$ satisfy the equation: $$a(f_1(x)+f_2(x))''+b(f_1(x)+f_2(x))'+c(f_1(x)+f_2(x))=0$$ $$a(f_1''(x)+f_2''(x))+b(f_1'(x)+f_2'(x))+c(f_1(x)+f_2(x))=0$$ $$a\cdot f_1''(x)+b\cdot f_1'(x)+c\cdot f_1(x)+a\cdot f_2''(x)+b\cdot f_2''(x)+c\cdot f_2''(x)=0$$ $$a\cdot f_1''(x)+b\cdot f_1'(x)+c\cdot f_1(x)=0, a\cdot f_2''(x)+b\cdot f_2''(x)+c\cdot f_2''(x)=0$$ both of which are true by definition. Therefore $f(x)=C_1e^{\lambda_1 x}+C_2e^{\lambda_2x}$ is a solution family to the differential equation (the only exception being when $\lambda_1=\lambda_2$ ). However, there is an issue with this proof: it merely shows that $f(x)=C_1e^{\lambda_1x}+C_2e^{\lambda_2x}$ is a solution family, but it does not show that $f(x)=C_1e^{\lambda_1x}+C_2e^{\lambda_2x}$ is the only solution family. In other words, the proof does not show that there are solutions to the equation not of the form $f(x)=C_1e^{\lambda_1x}+C_2e^{\lambda_2x}$ . My question is: How can we show that $f(x)=C_1e^{\lambda_1x}+C_2e^{\lambda_2x}$ is the only solution family","I was taught that is the only solution family to (provided that ). Here is my teacher's proof showing why: Assume that for some . Then the differential equation becomes: Then if are the roots of the equation , and are solutions to the differential equation. Now suppose is a solution to the differential equation, then we can show is a solution to the differential equation where is a real number: which is true by definition. Therefore we know and are solution sets for real . We can also show is a solution to the differential equation provided that and satisfy the equation: both of which are true by definition. Therefore is a solution family to the differential equation (the only exception being when ). However, there is an issue with this proof: it merely shows that is a solution family, but it does not show that is the only solution family. In other words, the proof does not show that there are solutions to the equation not of the form . My question is: How can we show that is the only solution family","f(x)=C_1e^{\lambda_1x}+C_2e^{\lambda_2x} a\cdot f''(x)+b\cdot f'(x)+c\cdot f(x)=0 b^2-4ac\neq0 f(x)=e^{\lambda x} \lambda a\frac{d^2}{dx^2}e^{\lambda x}+b\frac{d}{dx}e^{\lambda x}+ce^{\lambda x}=0 a\lambda^2e^{\lambda x}+b\lambda e^{\lambda x}+ce^{\lambda x}=0 e^{\lambda x}(a\lambda^2+b\lambda+c)=0 a\lambda^2+b\lambda+c=0 \lambda_1, \lambda_2 a\lambda^2+b\lambda+c=0 f(x)=e^{\lambda_1 x} f(x)=e^{\lambda_2 x} g(x) C\cdot g(x) C aC\cdot g''(x)+bC\cdot g'(x)+cC\cdot g(x)=0 C(a\cdot g''(x)+b\cdot g'(x)+c\cdot g(x)=0) a\cdot g''(x)+b\cdot g'(x)+c\cdot g(x)=0 f(x)=C_1e^{\lambda_1 x} f(x)=C_2e^{\lambda_2 x} C_1, C_2 f_1(x)+f_2(x) f_1(x) f_2(x) a(f_1(x)+f_2(x))''+b(f_1(x)+f_2(x))'+c(f_1(x)+f_2(x))=0 a(f_1''(x)+f_2''(x))+b(f_1'(x)+f_2'(x))+c(f_1(x)+f_2(x))=0 a\cdot f_1''(x)+b\cdot f_1'(x)+c\cdot f_1(x)+a\cdot f_2''(x)+b\cdot f_2''(x)+c\cdot f_2''(x)=0 a\cdot f_1''(x)+b\cdot f_1'(x)+c\cdot f_1(x)=0, a\cdot f_2''(x)+b\cdot f_2''(x)+c\cdot f_2''(x)=0 f(x)=C_1e^{\lambda_1 x}+C_2e^{\lambda_2x} \lambda_1=\lambda_2 f(x)=C_1e^{\lambda_1x}+C_2e^{\lambda_2x} f(x)=C_1e^{\lambda_1x}+C_2e^{\lambda_2x} f(x)=C_1e^{\lambda_1x}+C_2e^{\lambda_2x} f(x)=C_1e^{\lambda_1x}+C_2e^{\lambda_2x}",['ordinary-differential-equations']
12,How to reduce this Riccati ODE to a $1^{st}$ order linear ODE: $y'=1+x-(1+2x)y+xy^2$?,How to reduce this Riccati ODE to a  order linear ODE: ?,1^{st} y'=1+x-(1+2x)y+xy^2,"I am trying to solve this differential equation: $$ y'=1+x-(1+2x)y+xy^2 \quad (E_y)$$ that has a given partial solution $y_1(x)=1$ This equation is apparently Riccati form. The theory states that the following substitution will reduce the ODE to a linear first order form: $$(y(x)\neq 1): u(x) = \frac{1}{y(x)-y_1(x)} = \frac{1}{y(x)-1} \iff $$ $$ \bbox[15px,#ffd,border:1px solid blue]{y(x)=1 + \frac{1}{u(x)} \quad(1)} \quad \bbox[15px,#ffd,border:1px solid blue]{\text{and}\quad  y'(x) =-\frac{u'(x)}{u^2(x)} \quad (2)}$$ Hence, plugging $(1),(2)$ in $(E_y)$ : $$ u'(x) = u^2(x)(2+x)+u(x)(2x+1)-x \quad (E_u)$$ Apparently, the substitution didn't change the form of the differential equation. We notice that $(E_u)$ is still Riccati form. Why did this happen and what is a proper way to solve this?","I am trying to solve this differential equation: that has a given partial solution This equation is apparently Riccati form. The theory states that the following substitution will reduce the ODE to a linear first order form: Hence, plugging in : Apparently, the substitution didn't change the form of the differential equation. We notice that is still Riccati form. Why did this happen and what is a proper way to solve this?"," y'=1+x-(1+2x)y+xy^2 \quad (E_y) y_1(x)=1 (y(x)\neq 1): u(x) = \frac{1}{y(x)-y_1(x)} = \frac{1}{y(x)-1} \iff   \bbox[15px,#ffd,border:1px solid blue]{y(x)=1 + \frac{1}{u(x)} \quad(1)} \quad \bbox[15px,#ffd,border:1px solid blue]{\text{and}\quad  y'(x) =-\frac{u'(x)}{u^2(x)} \quad (2)} (1),(2) (E_y)  u'(x) = u^2(x)(2+x)+u(x)(2x+1)-x \quad (E_u) (E_u)","['calculus', 'ordinary-differential-equations']"
13,Attractors of nonlinear dynamical systems on the sphere,Attractors of nonlinear dynamical systems on the sphere,,"This is related to my other recent question , which involved linear flows on the unit sphere. Here, we are going to consider nonlinear flows. Let $\mathbb S^{d-1}=\{x\in\mathbb R^d\ :\ x^Tx=1\}$ denote the unit sphere. Let moreover $A$ be a real $d\times d$ matrix. Following this MathOverflow question we consider the unique solution $x(t, x_0)$ to the nonlinear initial value problem $$ \dot{x}=(I-x x^T)Ax, \quad x(0)=x_0\in \mathbb S^{d-1}. $$ For $x\in \mathbb S^{d-1}$ , that is $\lvert x \rvert^2=1$ , we see that $$\tfrac{d}{dt}(x^T x)=x^TA^Tx-x^TA^Tx\lvert x\rvert^2+x^TAx - \lvert x \rvert^2 x^TAx=0,$$ so $x(t, x_0)$ remains on $\mathbb S^{d-1}$ for all $t>0$ . Now the linked MathOverflow question states, without proof, that if $A$ is negative semi-definite, then $x(t, x_0)$ converges to a stable equilibrium. Can you prove an appropriate version of this statement? Here's some of my thoughts. I can think of two versions of the statement to prove. But I cannot prove either of them. First of all, it is easy to see that the normalized eigenvectors of $A$ correspond to equilibria; precisely, if $Av=\lambda v$ and $v\in \mathbb S^{d-1}$ then $$ \left.\tfrac{d}{dt} x(t, v)\right|_{t=0}= (I-vv^T)\lambda v=0,$$ which implies that $x(t, v)=v$ for all $t\ge 0$ . This leads me to think that the ""stable equilibrium"" mentioned in the statement above is an eigenvector. The two conjectures follow. Conjecture 1 . For each $x_0\in \mathbb S^{d-1}$ there is an eigenvector $v\in\mathbb S^{d-1}$ of $A$ such that $x(t, x_0)\to v$ as $t\to \infty$ . Conjecture 2 . (stronger). Let $\lambda_j$ denote the eigenvalues of $A$ and suppose that $0>\lambda_1>\lambda_j$ for all $j>1$ , and that $\lambda_1$ is non-degenerate. Let $v\in \mathbb S^{d-1}$ be a $\lambda_1$ -eigenvector of $A$ . Then $$ x(t, x_0)\to v,\quad \text{or}\quad x(t, x_0)\to -v$$ as $t\to \infty$ , unless $v^Tx_0=0$ . (In the latter case the system never leaves the $(d-2)$ dimensional sphere $\{x\in\mathbb R^d\ :\ v^Tx=0,\ \lvert x\rvert^2=1\}$ ).","This is related to my other recent question , which involved linear flows on the unit sphere. Here, we are going to consider nonlinear flows. Let denote the unit sphere. Let moreover be a real matrix. Following this MathOverflow question we consider the unique solution to the nonlinear initial value problem For , that is , we see that so remains on for all . Now the linked MathOverflow question states, without proof, that if is negative semi-definite, then converges to a stable equilibrium. Can you prove an appropriate version of this statement? Here's some of my thoughts. I can think of two versions of the statement to prove. But I cannot prove either of them. First of all, it is easy to see that the normalized eigenvectors of correspond to equilibria; precisely, if and then which implies that for all . This leads me to think that the ""stable equilibrium"" mentioned in the statement above is an eigenvector. The two conjectures follow. Conjecture 1 . For each there is an eigenvector of such that as . Conjecture 2 . (stronger). Let denote the eigenvalues of and suppose that for all , and that is non-degenerate. Let be a -eigenvector of . Then as , unless . (In the latter case the system never leaves the dimensional sphere ).","\mathbb S^{d-1}=\{x\in\mathbb R^d\ :\ x^Tx=1\} A d\times d x(t, x_0) 
\dot{x}=(I-x x^T)Ax, \quad x(0)=x_0\in \mathbb S^{d-1}.  x\in \mathbb S^{d-1} \lvert x \rvert^2=1 \tfrac{d}{dt}(x^T x)=x^TA^Tx-x^TA^Tx\lvert x\rvert^2+x^TAx - \lvert x \rvert^2 x^TAx=0, x(t, x_0) \mathbb S^{d-1} t>0 A x(t, x_0) A Av=\lambda v v\in \mathbb S^{d-1} 
\left.\tfrac{d}{dt} x(t, v)\right|_{t=0}= (I-vv^T)\lambda v=0, x(t, v)=v t\ge 0 x_0\in \mathbb S^{d-1} v\in\mathbb S^{d-1} A x(t, x_0)\to v t\to \infty \lambda_j A 0>\lambda_1>\lambda_j j>1 \lambda_1 v\in \mathbb S^{d-1} \lambda_1 A 
x(t, x_0)\to v,\quad \text{or}\quad x(t, x_0)\to -v t\to \infty v^Tx_0=0 (d-2) \{x\in\mathbb R^d\ :\ v^Tx=0,\ \lvert x\rvert^2=1\}","['linear-algebra', 'ordinary-differential-equations', 'dynamical-systems', 'spheres', 'nonlinear-analysis']"
14,Bar problem and heat conduction equation,Bar problem and heat conduction equation,,"A thin bar, defined through $ x\in [0,l] $ has a temparature distribution $ \theta (x,t) $ and has at the point $ x=0 $ a temperature of $0$ . At the other end there is a heat emission to another medium of temperature $0$ . Here holds $$ \frac{ \partial \theta }{ \partial x} (l,t)+ \sigma \theta (l,t)= 0 $$ for all $ t \geq 0 $ At the timepoint $ t_o =0 $ the bar has a temperature destribution $ x \mapsto f(x) $ And it holds the heat conduction equation $$ \frac{ \partial \theta }{ \partial t} = a^2 \frac{ \partial^2 \theta }{ \partial x^2} $$ where $ \sigma \in \mathbb{ R}^+, a \in \mathbb{R} \backslash \{0\} $ are constants. Firstly..how can I show with the seperation method , so plugging in $ \theta (x,t)= u(x)v(t) $ I am confused, at which equation do I have to use the separation method? that it will become the Sturm-Liouville Eigenvalueproblem $$ u''+ \lambda u = 0  , u(0)=0$$ $$ \sigma u(l)+ u'(l)=0 $$ where $ \lambda \in \mathbb{R} $ is a constant. And how can I solve the problem and determine the Eigenvalues? many thanks in advance!","A thin bar, defined through has a temparature distribution and has at the point a temperature of . At the other end there is a heat emission to another medium of temperature . Here holds for all At the timepoint the bar has a temperature destribution And it holds the heat conduction equation where are constants. Firstly..how can I show with the seperation method , so plugging in I am confused, at which equation do I have to use the separation method? that it will become the Sturm-Liouville Eigenvalueproblem where is a constant. And how can I solve the problem and determine the Eigenvalues? many thanks in advance!"," x\in [0,l]   \theta (x,t)   x=0  0 0  \frac{ \partial \theta }{ \partial x} (l,t)+ \sigma \theta (l,t)= 0   t \geq 0   t_o =0   x \mapsto f(x)   \frac{ \partial \theta }{ \partial t} = a^2 \frac{ \partial^2 \theta }{ \partial x^2}   \sigma \in \mathbb{ R}^+, a \in \mathbb{R} \backslash \{0\}   \theta (x,t)= u(x)v(t)   u''+ \lambda u = 0  , u(0)=0  \sigma u(l)+ u'(l)=0   \lambda \in \mathbb{R} ","['ordinary-differential-equations', 'partial-differential-equations', 'heat-equation', 'sturm-liouville']"
15,Reducing ordinary differential equation into clairaut's form,Reducing ordinary differential equation into clairaut's form,,Given equation is $$x^{2}p^{2}+py\left ( 2x+y \right )+y^{2}=0$$ Where $p=\frac{\mathrm{d} y}{\mathrm{d} x}$ I know about clairaut's form and if we put $y=u$ and $xy=v$ then we will get clairaut's form. I want to learn how to guess that we need to put $y$ and $xy$ .,Given equation is Where I know about clairaut's form and if we put and then we will get clairaut's form. I want to learn how to guess that we need to put and .,x^{2}p^{2}+py\left ( 2x+y \right )+y^{2}=0 p=\frac{\mathrm{d} y}{\mathrm{d} x} y=u xy=v y xy,['ordinary-differential-equations']
16,Solution to the equation $ y'' = 2 \alpha y - 2 \sqrt{y} $,Solution to the equation, y'' = 2 \alpha y - 2 \sqrt{y} ,"I'm trying to find the solution to the equation $ (A^2)'' = 2 \alpha A^2 - 2 A  $ on the interval $(0,1)$ where $A'(0) = 0,\, A(1) = 0 $ for $\alpha >0$ . Firstly I make the substitution $y = A^2$ , this reduces the equation to $y'' = 2 \alpha y -2\sqrt{y}.$ Since the equation is lacking the independent variable $x$ , one may make the substitution $y'=v$ which reduces the equation to $ v \frac{dv}{dy} = 2\alpha y - 2\sqrt{y}.$ By seperating variables, and using $y'=v$ , one has: $$ y' = \sqrt{\frac{2}{3}}\cdot\sqrt{C+3\alpha y^2 - 4 y^{1.5}}.$$ for some integration constant $C$ . This is as far as I've got without rurnning in to problems. I am not able to get this down to an explicit form. Question 1: Does there exist a solution to the above problem, and if one does can it be written explicitly. Question 2: If an explicit solution can't be determined, can we learn anything about it by looking at the $(y,y')$ phase plane? Question 3: Are there any other methods that could be applied to solve this, or to at least find some information about the solution? I'm not sure if you could apply asymptotics as it's a BVP on a finite interval. EDIT 1: You can use $y'(0) = 0$ to get rid of the $C$ in the above equation. You can the substitute A^2 straight back in to the equation for $y'$ to give: $$ 2A\cdot A' = \sqrt{\frac{2}{3}}\cdot A \cdot \sqrt{3\alpha A^2 - 4A} \implies A' = \frac{1}{\sqrt{6}} \sqrt{3\alpha A^2 - 4A}$$ Which, according to matlab, has the solution: $$A(x) =  -\frac{1}{3\alpha}\cdot\bigg\{2\cos\bigg[\frac{\sqrt{3\alpha}}{6}\bigg(C_1+x\sqrt{6}\bigg)\cdot \int_0^x \frac{1}{ln(t)}dt\bigg]-2\bigg\}$$ If anyone would like to point out any floors I've made it would be much appreciative! Also - It would still be interesting to hear the answers to the questions I have previously asked.","I'm trying to find the solution to the equation on the interval where for . Firstly I make the substitution , this reduces the equation to Since the equation is lacking the independent variable , one may make the substitution which reduces the equation to By seperating variables, and using , one has: for some integration constant . This is as far as I've got without rurnning in to problems. I am not able to get this down to an explicit form. Question 1: Does there exist a solution to the above problem, and if one does can it be written explicitly. Question 2: If an explicit solution can't be determined, can we learn anything about it by looking at the phase plane? Question 3: Are there any other methods that could be applied to solve this, or to at least find some information about the solution? I'm not sure if you could apply asymptotics as it's a BVP on a finite interval. EDIT 1: You can use to get rid of the in the above equation. You can the substitute A^2 straight back in to the equation for to give: Which, according to matlab, has the solution: If anyone would like to point out any floors I've made it would be much appreciative! Also - It would still be interesting to hear the answers to the questions I have previously asked."," (A^2)'' = 2 \alpha A^2 - 2 A   (0,1) A'(0) = 0,\, A(1) = 0  \alpha >0 y = A^2 y'' = 2 \alpha y -2\sqrt{y}. x y'=v  v \frac{dv}{dy} = 2\alpha y - 2\sqrt{y}. y'=v  y' = \sqrt{\frac{2}{3}}\cdot\sqrt{C+3\alpha y^2 - 4 y^{1.5}}. C (y,y') y'(0) = 0 C y'  2A\cdot A' = \sqrt{\frac{2}{3}}\cdot A \cdot \sqrt{3\alpha A^2 - 4A} \implies A' = \frac{1}{\sqrt{6}} \sqrt{3\alpha A^2 - 4A} A(x) =  -\frac{1}{3\alpha}\cdot\bigg\{2\cos\bigg[\frac{\sqrt{3\alpha}}{6}\bigg(C_1+x\sqrt{6}\bigg)\cdot \int_0^x \frac{1}{ln(t)}dt\bigg]-2\bigg\}","['calculus', 'ordinary-differential-equations', 'dynamical-systems', 'boundary-value-problem']"
17,Energy-preserving numerical method for system of coupled 2nd order differential equations,Energy-preserving numerical method for system of coupled 2nd order differential equations,,"I'm a physicist  who, due to Covid-19, switched to programming simulations. I would like to know if there is a numerical method that preserves the energy of the system for a  second order coupled differential equation of the form: $$\begin{aligned}   \ddot x &= a x + b \dot y\\   \ddot y &= a y - b \dot x   \end{aligned}$$ where $a$ and $b$ are real constants. I'm searching for something equivalent to the leapfrog integration , which seems to work only for non-coupled systems where $\ddot x$ does not depend on $\dot x$ . The goal is to preserve the Hamiltonian of the system over time (i.e., constrain its error).","I'm a physicist  who, due to Covid-19, switched to programming simulations. I would like to know if there is a numerical method that preserves the energy of the system for a  second order coupled differential equation of the form: where and are real constants. I'm searching for something equivalent to the leapfrog integration , which seems to work only for non-coupled systems where does not depend on . The goal is to preserve the Hamiltonian of the system over time (i.e., constrain its error).","\begin{aligned}
  \ddot x &= a x + b \dot y\\
  \ddot y &= a y - b \dot x
  \end{aligned} a b \ddot x \dot x","['ordinary-differential-equations', 'numerical-methods']"
18,Solve for $y$ in $\frac{dy}{dx}-\frac{3y}{2x+1}=3x^2$,Solve for  in,y \frac{dy}{dx}-\frac{3y}{2x+1}=3x^2,"I saw a challenge problem on social media by a friend, solve for $y$ in $$\frac{dy}{dx}-\frac{3y}{2x+1}=3x^2$$ I think this is an integration factor ODE $$\frac{1}{{(2x+1)}^{\frac{3}{2}}} \cdot \frac{dy}{dx}-\frac{3y}{{(2x+1)}^{\frac{5}{2}}}=\frac{3x^2}{{(2x+1)}^{\frac{3}{2}}}$$ Is this correct? $$\left(\frac{y}{{(2x+1)}^{\frac{3}{2}}} \right)'=\frac{3x^2}{{(2x+1)}^{\frac{3}{2}}}$$ $$\left(\frac{y}{{(2x+1)}^{\frac{3}{2}}} \right)=\int \frac{3x^2}{{(2x+1)}^{\frac{3}{2}}} \mathop{dx}$$","I saw a challenge problem on social media by a friend, solve for in I think this is an integration factor ODE Is this correct?",y \frac{dy}{dx}-\frac{3y}{2x+1}=3x^2 \frac{1}{{(2x+1)}^{\frac{3}{2}}} \cdot \frac{dy}{dx}-\frac{3y}{{(2x+1)}^{\frac{5}{2}}}=\frac{3x^2}{{(2x+1)}^{\frac{3}{2}}} \left(\frac{y}{{(2x+1)}^{\frac{3}{2}}} \right)'=\frac{3x^2}{{(2x+1)}^{\frac{3}{2}}} \left(\frac{y}{{(2x+1)}^{\frac{3}{2}}} \right)=\int \frac{3x^2}{{(2x+1)}^{\frac{3}{2}}} \mathop{dx},['calculus']
19,"Trying to understand example, determine fx and fy","Trying to understand example, determine fx and fy",,"I´m trying to learn 2nd order Taylor but i cant understand the example i have. This is an example i have from a book, $$y´=\frac{y}{2}+x$$ $$-1\le x \le  1 $$ $$y(-1)=1$$ How does fx = 1 and fy = 1/2 ? $$f´(x,y)= fx+fyf = 1+\frac{1}{2}\left(\frac{y}{2}+x\right) = 1+\frac{y}{4}+\frac{x}{2}$$","I´m trying to learn 2nd order Taylor but i cant understand the example i have. This is an example i have from a book, How does fx = 1 and fy = 1/2 ?","y´=\frac{y}{2}+x -1\le x \le  1  y(-1)=1 f´(x,y)= fx+fyf = 1+\frac{1}{2}\left(\frac{y}{2}+x\right) = 1+\frac{y}{4}+\frac{x}{2}",['ordinary-differential-equations']
20,Solve this differential equation $x^2y''-5xy'+6y=0$,Solve this differential equation,x^2y''-5xy'+6y=0,"Solve this equation $$ \begin{cases} x^2y''-5xy'+6y=0 \\ y(-1)=3 \\ y'(-1)=2 \end{cases} $$ I got $$y=c_1x^{3+\sqrt3}+c_2x^{3-\sqrt3}$$ I have three little questions. Could I solve the problem by substituting $(-1)^{\sqrt3}=\cos((\sqrt 3) \pi)+i\sin((\sqrt 3)\pi)$ ? I need to substitute $t=-x$ ? If i have to use 2. , isn't the problem wrong because the problem itself contains $y(-1)=3$ ?","Solve this equation I got I have three little questions. Could I solve the problem by substituting ? I need to substitute ? If i have to use 2. , isn't the problem wrong because the problem itself contains ?","
\begin{cases}
x^2y''-5xy'+6y=0 \\
y(-1)=3 \\
y'(-1)=2
\end{cases}
 y=c_1x^{3+\sqrt3}+c_2x^{3-\sqrt3} (-1)^{\sqrt3}=\cos((\sqrt 3) \pi)+i\sin((\sqrt 3)\pi) t=-x y(-1)=3",['ordinary-differential-equations']
21,Separation of variables in ODE with complex conjugate function,Separation of variables in ODE with complex conjugate function,,"I am trying to solve the following system: $$\dot z = \bar z·e^{it}$$ $$z:\mathbb{R}\rightarrow\mathbb{C},\qquad z(t_0) = z_0$$ I cannot figure out how to properly separate the right- and lefthandside. I tried: $$\int\frac{z}{||z||^2}dz + C= -ie^{it}$$ but wasn't successful. I will be glad about any tips.",I am trying to solve the following system: I cannot figure out how to properly separate the right- and lefthandside. I tried: but wasn't successful. I will be glad about any tips.,"\dot z = \bar z·e^{it} z:\mathbb{R}\rightarrow\mathbb{C},\qquad z(t_0) = z_0 \int\frac{z}{||z||^2}dz + C= -ie^{it}","['complex-analysis', 'ordinary-differential-equations']"
22,Classification of Critical Points of Second Order Differential Equation,Classification of Critical Points of Second Order Differential Equation,,"I am given a single second order differential equation: $\ddot{x}-x^3 - 2x^2\dot{x} + 1 = 0$ .. and am asked to classify the critical points as stable, unstable or saddle points. Finding the critical points is an easy task for first order differential equation(s), both single equation and system of equations. However, I have never done so for second order, and higher, equations. I have an idea on how to solve it but not sure the approach is correct. Am I correct in saying I need to split the single differential equation into two differential equations and rename the relevant terms? Doing so on the above equation gives: $\dot{x_1} = x_2$ $\dot{x_2}-x_1^3-2x_1^2x_2+1 = 0$ Thereafter, I follow the same process as starting off with a set of two first order differential equations. That is, set $\dot{x}_1$ and $\dot{x}_2$ to $0$ , and solve for the intersection of $x_1$ and $x_2$ to find the fixed points. The nature of the fixed points would then be determined by calculating the Trace and Determinant of the Jacobian at the specific fixed points. Is my thinking on the right track?","I am given a single second order differential equation: .. and am asked to classify the critical points as stable, unstable or saddle points. Finding the critical points is an easy task for first order differential equation(s), both single equation and system of equations. However, I have never done so for second order, and higher, equations. I have an idea on how to solve it but not sure the approach is correct. Am I correct in saying I need to split the single differential equation into two differential equations and rename the relevant terms? Doing so on the above equation gives: Thereafter, I follow the same process as starting off with a set of two first order differential equations. That is, set and to , and solve for the intersection of and to find the fixed points. The nature of the fixed points would then be determined by calculating the Trace and Determinant of the Jacobian at the specific fixed points. Is my thinking on the right track?",\ddot{x}-x^3 - 2x^2\dot{x} + 1 = 0 \dot{x_1} = x_2 \dot{x_2}-x_1^3-2x_1^2x_2+1 = 0 \dot{x}_1 \dot{x}_2 0 x_1 x_2,"['ordinary-differential-equations', 'systems-of-equations']"
23,"Find best approximation of $\sin(\pi x)$ over $[0,1]$ with quadratic polynomial $a_0+a_1x+a_2x^2$",Find best approximation of  over  with quadratic polynomial,"\sin(\pi x) [0,1] a_0+a_1x+a_2x^2","Use the theory of orthogonal functions to find best in the mean approximation of the function $\sin(πx)$ on the interval $[0,1]$ by a second-order polynomial That is, find such coefficients $a_0, a_1$ and $a_2$ that, $$\int^1_0 (\sin(\pi x)-a_0-a_1x-a_2x^2)^2 \, dx, $$ takes a minimal possible value. I feel as though this has something to do with Fourier series but I really cant be sure because I am not very familiar with this area. Also, I'm not sure what ""best in mean approximation"" means, so any help with that would be great.","Use the theory of orthogonal functions to find best in the mean approximation of the function on the interval by a second-order polynomial That is, find such coefficients and that, takes a minimal possible value. I feel as though this has something to do with Fourier series but I really cant be sure because I am not very familiar with this area. Also, I'm not sure what ""best in mean approximation"" means, so any help with that would be great.","\sin(πx) [0,1] a_0, a_1 a_2 \int^1_0 (\sin(\pi x)-a_0-a_1x-a_2x^2)^2 \, dx, ","['linear-algebra', 'ordinary-differential-equations']"
24,"Solve $x^4y^{\prime\prime} = (y-xy^\prime)^3, y(1) = y^\prime(1) = 1$",Solve,"x^4y^{\prime\prime} = (y-xy^\prime)^3, y(1) = y^\prime(1) = 1","$\lambda^4x^4\lambda^{n-2}y^{\prime\prime} = (\lambda^ny-\lambda x\lambda^{n-1}y^\prime)^3 \Rightarrow \lambda^{n+2}x^4y^{\prime\prime} = (\lambda^n(y-xy^\prime))^3$ . $\lambda^{n+2} = \lambda^{3n} \Rightarrow n+2 = 3n \Rightarrow n = 1$ . Let $x = e^t, y = ue^{nt} = ue^t$ . Now, $\frac{dy}{dx} = \frac{\frac{dy}{dt}}{\frac{dx}{dt}} = \frac{u^\prime e^t + ue^t}{e^t} = u^\prime + u$ . And, $\frac{d^2y}{dx^2} = \frac{\frac{d}{dt}}{\frac{dx}{dt}}(\frac{dy}{dx}) = \frac{\frac{d}{dt}(u^\prime + u)}{e^t} = e^{-t}(\frac{d^2u}{dt^2} + \frac{du}{dt})$ . Thus, $e^{4t}(e^{-t}(\frac{du^2}{dt^2} + \frac{du}{dt})) = (ue^{t}-e^t(u+\frac{du}{dt}))^3 \Rightarrow e^{3t}(\frac{d^2u}{t^2} + \frac{du}{dt}) = e^{3t}(u-(u+\frac{du}{dt}))^3 \Rightarrow (\frac{d^2u}{dt^2} + \frac{du}{dt}) = (\frac{du}{dt})^3$ . Let $p = \frac{du}{dt}, p^\prime = \frac{d^2u}{dt^2} = \frac{dp}{du}\frac{du}{dt} = p\frac{dp}{du}$ . Thus, $(p + p\frac{dp}{du}) = p^3 \Rightarrow 1 + \frac{dp}{du} = p^2 \Rightarrow \frac{dp}{du} = p^2-1 \Rightarrow u + c_1 = \int\frac{dp}{p^2-1} = \int\frac{dp}{(p-1)(p+1)} = \int(\frac{1}{2(p-1)} - \frac{1}{2(p+1)})dp = \frac{1}{2}\ln(p-1)-\frac{1}{2}\ln(p+1) = \ln(\frac{\sqrt{p-1}}{\sqrt{p+1}}) \Rightarrow c_1e^u = \frac{\sqrt{p-1}}{\sqrt{p+1}} \Rightarrow c_1e^{2u} = \frac{p-1}{p+1} \Rightarrow p-1 = c_1e^{2u}p + c_1e^{2u} \Rightarrow p = \frac{c_1e^{2u}+1}{1-c_1e^{2u}} = \frac{du}{dt}$ . But then I think that I have to use the initial conditions to get $c_1$ , but I don know how to do that.",". . Let . Now, . And, . Thus, . Let . Thus, . But then I think that I have to use the initial conditions to get , but I don know how to do that.","\lambda^4x^4\lambda^{n-2}y^{\prime\prime} = (\lambda^ny-\lambda x\lambda^{n-1}y^\prime)^3 \Rightarrow \lambda^{n+2}x^4y^{\prime\prime} = (\lambda^n(y-xy^\prime))^3 \lambda^{n+2} = \lambda^{3n} \Rightarrow n+2 = 3n \Rightarrow n = 1 x = e^t, y = ue^{nt} = ue^t \frac{dy}{dx} = \frac{\frac{dy}{dt}}{\frac{dx}{dt}} = \frac{u^\prime e^t + ue^t}{e^t} = u^\prime + u \frac{d^2y}{dx^2} = \frac{\frac{d}{dt}}{\frac{dx}{dt}}(\frac{dy}{dx}) = \frac{\frac{d}{dt}(u^\prime + u)}{e^t} = e^{-t}(\frac{d^2u}{dt^2} + \frac{du}{dt}) e^{4t}(e^{-t}(\frac{du^2}{dt^2} + \frac{du}{dt})) = (ue^{t}-e^t(u+\frac{du}{dt}))^3 \Rightarrow e^{3t}(\frac{d^2u}{t^2} + \frac{du}{dt}) = e^{3t}(u-(u+\frac{du}{dt}))^3 \Rightarrow (\frac{d^2u}{dt^2} + \frac{du}{dt}) = (\frac{du}{dt})^3 p = \frac{du}{dt}, p^\prime = \frac{d^2u}{dt^2} = \frac{dp}{du}\frac{du}{dt} = p\frac{dp}{du} (p + p\frac{dp}{du}) = p^3 \Rightarrow 1 + \frac{dp}{du} = p^2 \Rightarrow \frac{dp}{du} = p^2-1 \Rightarrow u + c_1 = \int\frac{dp}{p^2-1} = \int\frac{dp}{(p-1)(p+1)} = \int(\frac{1}{2(p-1)} - \frac{1}{2(p+1)})dp = \frac{1}{2}\ln(p-1)-\frac{1}{2}\ln(p+1) = \ln(\frac{\sqrt{p-1}}{\sqrt{p+1}}) \Rightarrow c_1e^u = \frac{\sqrt{p-1}}{\sqrt{p+1}} \Rightarrow c_1e^{2u} = \frac{p-1}{p+1} \Rightarrow p-1 = c_1e^{2u}p + c_1e^{2u} \Rightarrow p = \frac{c_1e^{2u}+1}{1-c_1e^{2u}} = \frac{du}{dt} c_1",['ordinary-differential-equations']
25,linear independence of vector functions,linear independence of vector functions,,would the below vector functions be linearly independent? Even though the wronskian is equal to 0? $$\begin{bmatrix}0 & 0 & 0\\ 1 & t & t\\ 2t^2 & 2t^3 & 3t^2\\ \end{bmatrix}$$,would the below vector functions be linearly independent? Even though the wronskian is equal to 0?,\begin{bmatrix}0 & 0 & 0\\ 1 & t & t\\ 2t^2 & 2t^3 & 3t^2\\ \end{bmatrix},"['linear-algebra', 'ordinary-differential-equations']"
26,Find the interval of convergence of a power series with a single endpoint,Find the interval of convergence of a power series with a single endpoint,,"The given series is: $$\sum^\infty_{n=0}\left(x^{2n+1}+2x^{2n+2}\right)$$ I know that the interval of convergence can be found by the ratio test, so what I tried was: $$\lim_{n \to \infty}\frac{x^{2n+3}+2x^{2n+4}}{x^{2n+1}+2x^{2n+2}}=x^2$$ Now we know that $-1<|x^2|<1$ or just $x<1$ . Then solving for the single endpoint: $$\sum^\infty_{n=0}\left(1^{2n+1}+2^{2n+2}\right)$$ $$\lim_{n\to\infty}1^{2n+1}+2^{2n+2}=\infty$$ So the endpoint diverges, and the interval is $(-\infty,1)$","The given series is: I know that the interval of convergence can be found by the ratio test, so what I tried was: Now we know that or just . Then solving for the single endpoint: So the endpoint diverges, and the interval is","\sum^\infty_{n=0}\left(x^{2n+1}+2x^{2n+2}\right) \lim_{n \to \infty}\frac{x^{2n+3}+2x^{2n+4}}{x^{2n+1}+2x^{2n+2}}=x^2 -1<|x^2|<1 x<1 \sum^\infty_{n=0}\left(1^{2n+1}+2^{2n+2}\right) \lim_{n\to\infty}1^{2n+1}+2^{2n+2}=\infty (-\infty,1)","['calculus', 'ordinary-differential-equations', 'limits', 'power-series']"
27,Solution of the differential equation $\sin(x\frac{dy}{dx})\cos(y)=\frac{dy}{dx}+\sin(y)\cos(x\frac{dy} {dx})$,Solution of the differential equation,\sin(x\frac{dy}{dx})\cos(y)=\frac{dy}{dx}+\sin(y)\cos(x\frac{dy} {dx}),"Find the solution of the differential equation $\sin(x\frac{dy}{dx})\cos(y)=\frac{dy}{dx}+\sin(y)\cos(x\frac{dy} {dx})$ My approach is as follow $A=x\frac{dy}{dx}$ $\sin(A)\cos(y)-\cos(A)\sin(y)=\frac{dy}{dx}$ $\sin(A-y)=\frac{dy}{dx}$ $\sin(x\frac{dy}{dx}-y)=\frac{dy}{dx}$ After this step I am not able to solve it,","Find the solution of the differential equation My approach is as follow After this step I am not able to solve it,","\sin(x\frac{dy}{dx})\cos(y)=\frac{dy}{dx}+\sin(y)\cos(x\frac{dy}
{dx}) A=x\frac{dy}{dx} \sin(A)\cos(y)-\cos(A)\sin(y)=\frac{dy}{dx} \sin(A-y)=\frac{dy}{dx} \sin(x\frac{dy}{dx}-y)=\frac{dy}{dx}","['ordinary-differential-equations', 'derivatives']"
28,Is there an analytic way to tell if a system of ordinary differential equations is conservative?,Is there an analytic way to tell if a system of ordinary differential equations is conservative?,,"I don’t know the exact definition of conservative, but we can assume that conservative means the sum of Lyapunov exponents is zero. Is there an analytic method to show that a given system is conservative? I came to this question by reading Sprott’s Elegant Chaos. A system of interest could be e.g.: $$U''' + U' + \frac{1}{3}U^3 - Uc=0$$ where $c$ is constant, $U(t)$ is a function of time. It can be described as system of three equations: $$\begin{align} U' &= X \\ X' &= Y  \\ Y' &= -X - \frac{1}{3} U^3 +Uc \end{align}$$ Then I think (but cannot prove it) that the trace of the Jacobian $\operatorname{Tr}(J)$ shows the conservativeness. $$\operatorname{Tr}(J) = \frac{\partial U'}{\partial U} + \frac{\partial X'}{\partial X} + \frac{\partial Y'}{\partial Y} = 0 + 0 + 0 = 0$$ It also means that if $\operatorname{Tr}(J) < 0$ , the system is dissipative, and if $\operatorname{Tr}(J) > 0$ , the system diverges, i.e., it is not bounded. Can you say if I am right or wrong?","I don’t know the exact definition of conservative, but we can assume that conservative means the sum of Lyapunov exponents is zero. Is there an analytic method to show that a given system is conservative? I came to this question by reading Sprott’s Elegant Chaos. A system of interest could be e.g.: where is constant, is a function of time. It can be described as system of three equations: Then I think (but cannot prove it) that the trace of the Jacobian shows the conservativeness. It also means that if , the system is dissipative, and if , the system diverges, i.e., it is not bounded. Can you say if I am right or wrong?","U''' + U' + \frac{1}{3}U^3 - Uc=0 c U(t) \begin{align}
U' &= X \\
X' &= Y  \\
Y' &= -X - \frac{1}{3} U^3 +Uc
\end{align} \operatorname{Tr}(J) \operatorname{Tr}(J) = \frac{\partial U'}{\partial U} + \frac{\partial X'}{\partial X} + \frac{\partial Y'}{\partial Y} = 0 + 0 + 0 = 0 \operatorname{Tr}(J) < 0 \operatorname{Tr}(J) > 0","['ordinary-differential-equations', 'dynamical-systems', 'jacobian']"
29,Solution of $f(x) = \mu x f'(x) + \lambda x f''(x)$,Solution of,f(x) = \mu x f'(x) + \lambda x f''(x),"While there is a simple solution to the Euler equation $f(x) = \mu x f'(x) + \lambda x^2 f''(x)$ as pointed out in my previous question (see here ), the one discussed here seems much more challenging. I don't think there is a simple solution for the general case, but the case $\mu = 1$ leads to an interesting result. I was wondering if the result I am presenting here (see below) is correct. I proceeded as follows: Step 1 Let $f(x) = x\phi(x)$ . Step 2 This leads to $(x+2\lambda) \phi'(x) + \lambda x \phi''(x) = 0$ , that is $$\frac{d \log\phi'(x)}{dx} = -\frac{1}{\lambda}-\frac{2}{x}.$$ Step 3 Simple integration (twice) leads to $$f(x) = x\phi(x) = Ax + Bx\int \frac{1}{x^2}\cdot\exp\Big(-\frac{x}{\lambda}\Big) dx.$$ So we have the general solution if $\mu =1$ . It is trivial to verify that $f(x) = Ax$ is indeed a particular solution. Step 4 Using WolframAlpha or by other means, the solution can be expressed in terms of the exponential integral (Ei) function: $$f(x) = Ax - \frac{B}{\lambda} \Big\{ x \mbox{Ei}\Big(-\frac{x}{\lambda}\Big) + \lambda\exp\Big(-\frac{x}{\lambda}\Big)\Big\}.$$ Is this final result correct (if $\mu=1$ )? Is this a well known differential equation?","While there is a simple solution to the Euler equation as pointed out in my previous question (see here ), the one discussed here seems much more challenging. I don't think there is a simple solution for the general case, but the case leads to an interesting result. I was wondering if the result I am presenting here (see below) is correct. I proceeded as follows: Step 1 Let . Step 2 This leads to , that is Step 3 Simple integration (twice) leads to So we have the general solution if . It is trivial to verify that is indeed a particular solution. Step 4 Using WolframAlpha or by other means, the solution can be expressed in terms of the exponential integral (Ei) function: Is this final result correct (if )? Is this a well known differential equation?",f(x) = \mu x f'(x) + \lambda x^2 f''(x) \mu = 1 f(x) = x\phi(x) (x+2\lambda) \phi'(x) + \lambda x \phi''(x) = 0 \frac{d \log\phi'(x)}{dx} = -\frac{1}{\lambda}-\frac{2}{x}. f(x) = x\phi(x) = Ax + Bx\int \frac{1}{x^2}\cdot\exp\Big(-\frac{x}{\lambda}\Big) dx. \mu =1 f(x) = Ax f(x) = Ax - \frac{B}{\lambda} \Big\{ x \mbox{Ei}\Big(-\frac{x}{\lambda}\Big) + \lambda\exp\Big(-\frac{x}{\lambda}\Big)\Big\}. \mu=1,"['calculus', 'integration', 'ordinary-differential-equations', 'special-functions']"
30,Why is it false that for all $y\in\mathbb{R^n}$ the solution of the initial value problem $x(0) = y$ exists for all time $t$.,Why is it false that for all  the solution of the initial value problem  exists for all time .,y\in\mathbb{R^n} x(0) = y t,"Is the following statement false because our solutions for the initial value problem may not exist when $t=0$ ; depending on our function? Also, uniqueness does not exist if the system is nonlinear, such as quadratic. Hence, the statement below is false. Statement: Consider $x'=f(x)$ and suppose that for every initial condition $x(0)=x_0$ solutions exists and are unique for some time interval $[-a(x_0), a(x_0)]$ whose length depends on the initial value $x_0$ . Then for all $y\in\mathbb{R^n}$ the solution of the initial value problem $x(0) = y$ exists for all time $t$ .","Is the following statement false because our solutions for the initial value problem may not exist when ; depending on our function? Also, uniqueness does not exist if the system is nonlinear, such as quadratic. Hence, the statement below is false. Statement: Consider and suppose that for every initial condition solutions exists and are unique for some time interval whose length depends on the initial value . Then for all the solution of the initial value problem exists for all time .","t=0 x'=f(x) x(0)=x_0 [-a(x_0), a(x_0)] x_0 y\in\mathbb{R^n} x(0) = y t","['real-analysis', 'ordinary-differential-equations', 'dynamical-systems', 'initial-value-problems']"
31,How to find general solution of a DE given its particular solutions?,How to find general solution of a DE given its particular solutions?,,"Can you suggest how can I find the general solution of the equation $f(x)y'' + g(x) y'+y = 1$ if $x^2$ , $x$ , $1$ are solutions of it. My idea is to make somehow a linear combination of the solutions like that $C_1 + C_2x +C_3x^2$ but it does require that this indeed general solution of the equation. Then I thought of finding the roots of the characteristic equation but I have functions, not constant coefficients.","Can you suggest how can I find the general solution of the equation if , , are solutions of it. My idea is to make somehow a linear combination of the solutions like that but it does require that this indeed general solution of the equation. Then I thought of finding the roots of the characteristic equation but I have functions, not constant coefficients.",f(x)y'' + g(x) y'+y = 1 x^2 x 1 C_1 + C_2x +C_3x^2,"['calculus', 'ordinary-differential-equations']"
32,True/False: If the Wronskian of n functions vanishes at all points on the real line then these functions must be linearly dependent in R.,True/False: If the Wronskian of n functions vanishes at all points on the real line then these functions must be linearly dependent in R.,,"I know that if a set of functions are linearly dependent, then its Wronskian = 0 at all values of t in the interval. So can you conclude that if Wronskian = 0 for all values of t in the interval, then the functions must be dependent?","I know that if a set of functions are linearly dependent, then its Wronskian = 0 at all values of t in the interval. So can you conclude that if Wronskian = 0 for all values of t in the interval, then the functions must be dependent?",,"['linear-algebra', 'ordinary-differential-equations', 'wronskian']"
33,Quarentine modification to the SIR-model intepretation,Quarentine modification to the SIR-model intepretation,,"I'm trying to model the corona virus using the SIR model. I added in a new parameter to the model, that would simulate quarantine. My goal is to see what the effect of quarantine of the infectious would be, and thus not accounting for social distancing. The equations using the new c parameter become: $$S'(t)=-a \times S(t) \times I(t) $$ $$I'(t)=a \times S(t) \times I(t)-b \times I(t)-c \times a \times S(t) \times I(t) $$ $$R'(t)=b \times I(t)+c \times a \times S(t) \times I(t) $$ If t is measured in weeks, is it then true that this would put c percent of the newly infectious into R each week?","I'm trying to model the corona virus using the SIR model. I added in a new parameter to the model, that would simulate quarantine. My goal is to see what the effect of quarantine of the infectious would be, and thus not accounting for social distancing. The equations using the new c parameter become: If t is measured in weeks, is it then true that this would put c percent of the newly infectious into R each week?",S'(t)=-a \times S(t) \times I(t)  I'(t)=a \times S(t) \times I(t)-b \times I(t)-c \times a \times S(t) \times I(t)  R'(t)=b \times I(t)+c \times a \times S(t) \times I(t) ,['ordinary-differential-equations']
34,Solving a differential equation with one parameter defined by another differential equation,Solving a differential equation with one parameter defined by another differential equation,,I have a differential equation as below: $$\frac{dx}{dt}=(y-z)x$$ $z$ is constant and $y$ is given by another differential equation as below: $$\frac{dy}{dt}=ay$$ $a$ is a constant How can I solve the equation? I tried to find the value of $y(t)$ and I substituted that in the $\frac{dx}{dt}$ . But I am not sure whether is correct way of doing it. Kindly help.,I have a differential equation as below: is constant and is given by another differential equation as below: is a constant How can I solve the equation? I tried to find the value of and I substituted that in the . But I am not sure whether is correct way of doing it. Kindly help.,\frac{dx}{dt}=(y-z)x z y \frac{dy}{dt}=ay a y(t) \frac{dx}{dt},"['ordinary-differential-equations', 'derivatives']"
35,Computational issues with the Runge Katta method for ODE first order,Computational issues with the Runge Katta method for ODE first order,,"Since the corona virus there are no office hours hence I can only come here for help and the internet got hacked so school email is down. Plus its even harder to get feedback through an email. I am not sure why my answer is off. I am not sure how many times I have to do the method.This took an hour for me to type up because I caught a mistake in my first steps so I had to type this up while doing it on a calculator $\frac{dy}{dx}=y+1$ where $y(0)=1$ $h=.25$ on the interval $[0,.5]$ round to 5 decimal places The formulas are: $k_1=f(x_n,y_n)$ $k_2=f(x_n+\frac{1}{2}h,y_n+\frac{1}{2}hk_1)$ $k_3 = f(x_n+\frac{1}{2}h,y_n+\frac{1}{2}hk_2)$ $k_4 = f(x_{n+1},y_nhk_3)$ $y_{n+1} = y_n+\frac{h}{6}[k_1+k_2+2k_3+k_4]$ Okay here we go: $k_1 = f(x_n,y_n)=f(0,1)=1+1=2$ $k_2 = f(x_n+\frac{1}{2}h,y_n+\frac{1}{2}hk_1)=f(0+\frac{1}{4}\frac{1}{2},1+\frac{1}{2}\frac{1}{4}2)=f(0 + \frac{1}{8}, 1+\frac{1}{8}2)=f(\frac{1}{8},1+.25) = 1+.25=1.25$ $k_3 = f(x_n + \frac{1}{2}h, + y_n + \frac{1}{2}hk_2)=f(0+\frac{1}{2}\frac{1}{4},1+\frac{1}{2}\frac{1}{4}1.25)=f(0+\frac{1}{8},1+\frac{1}{8}1.25)=f(\frac{1}{8},1+.15625)=1+1.15625=2.15626$ $k_4=f(x_n+1,y_nhk_3)= f(.25,1+\frac{1}{4}2.15626)=f(.25,1+.53907)=1+1.53907=2.53907$ $y_{n+1}= y_n + \frac{h}{6}[k_1+2k_2+2k_3+k_4]=1 +\frac{.25}{6}[2+[2(1.25)]+[2(2.15626)]+2.93507]=1+.04167[2+2.5+4.31252+2.93507]=1+.41607[11.74759]=1+.48952=1.48952$ so we have $y_1=1.48952$ Now I assume I do the whole thing over with $y(.25)=1.48952$ $k_1= f(.25,1.48952)=1+1.48952=2.48952$ However I run into the problem that the answer is supposed to be $2.29740$ ...I am already over the answer on my first iteration.....I assume that I am supposed to do the two iterations stop at $y_2$ I don't understand what I am doing wrong...",Since the corona virus there are no office hours hence I can only come here for help and the internet got hacked so school email is down. Plus its even harder to get feedback through an email. I am not sure why my answer is off. I am not sure how many times I have to do the method.This took an hour for me to type up because I caught a mistake in my first steps so I had to type this up while doing it on a calculator where on the interval round to 5 decimal places The formulas are: Okay here we go: so we have Now I assume I do the whole thing over with However I run into the problem that the answer is supposed to be ...I am already over the answer on my first iteration.....I assume that I am supposed to do the two iterations stop at I don't understand what I am doing wrong...,"\frac{dy}{dx}=y+1 y(0)=1 h=.25 [0,.5] k_1=f(x_n,y_n) k_2=f(x_n+\frac{1}{2}h,y_n+\frac{1}{2}hk_1) k_3 = f(x_n+\frac{1}{2}h,y_n+\frac{1}{2}hk_2) k_4 = f(x_{n+1},y_nhk_3) y_{n+1} = y_n+\frac{h}{6}[k_1+k_2+2k_3+k_4] k_1 = f(x_n,y_n)=f(0,1)=1+1=2 k_2 = f(x_n+\frac{1}{2}h,y_n+\frac{1}{2}hk_1)=f(0+\frac{1}{4}\frac{1}{2},1+\frac{1}{2}\frac{1}{4}2)=f(0 + \frac{1}{8}, 1+\frac{1}{8}2)=f(\frac{1}{8},1+.25) = 1+.25=1.25 k_3 = f(x_n + \frac{1}{2}h, + y_n + \frac{1}{2}hk_2)=f(0+\frac{1}{2}\frac{1}{4},1+\frac{1}{2}\frac{1}{4}1.25)=f(0+\frac{1}{8},1+\frac{1}{8}1.25)=f(\frac{1}{8},1+.15625)=1+1.15625=2.15626 k_4=f(x_n+1,y_nhk_3)= f(.25,1+\frac{1}{4}2.15626)=f(.25,1+.53907)=1+1.53907=2.53907 y_{n+1}= y_n + \frac{h}{6}[k_1+2k_2+2k_3+k_4]=1 +\frac{.25}{6}[2+[2(1.25)]+[2(2.15626)]+2.93507]=1+.04167[2+2.5+4.31252+2.93507]=1+.41607[11.74759]=1+.48952=1.48952 y_1=1.48952 y(.25)=1.48952 k_1= f(.25,1.48952)=1+1.48952=2.48952 2.29740 y_2","['ordinary-differential-equations', 'solution-verification', 'runge-kutta-methods']"
36,How to interpret this population growth model?,How to interpret this population growth model?,,"I was working on this equation $$\frac{d P}{d t}=\frac{v P}{K+P}-d P$$ where $P$ is population density and $v,d,K$ is constant, the stedy-state of this equation is $P_1=0$ or $P_2=\dfrac{v}{d}-K$ . If we let $\frac{d P}{d t}=f(P)$ , and take derivative wrt $P$ , we get $\frac{df}{dP}=\frac{v}{P+K}-\frac{v P}{(P+K)^{2}}-d$ , plug the stedy-state, we have $\dfrac{df(P_1)}{dP}=\dfrac{v}{K}-d$ and $\dfrac{df(P_2)}{dP}=\dfrac{Kd^2}{v}-d$ , what is the biological meaning of the two steay-state if one of them is stable in terms of the three constant. And how to interpret what this equation saying, I mean, I don't know the meaning of the three constant and the derivation of this equation.","I was working on this equation where is population density and is constant, the stedy-state of this equation is or . If we let , and take derivative wrt , we get , plug the stedy-state, we have and , what is the biological meaning of the two steay-state if one of them is stable in terms of the three constant. And how to interpret what this equation saying, I mean, I don't know the meaning of the three constant and the derivation of this equation.","\frac{d P}{d t}=\frac{v P}{K+P}-d P P v,d,K P_1=0 P_2=\dfrac{v}{d}-K \frac{d P}{d t}=f(P) P \frac{df}{dP}=\frac{v}{P+K}-\frac{v P}{(P+K)^{2}}-d \dfrac{df(P_1)}{dP}=\dfrac{v}{K}-d \dfrac{df(P_2)}{dP}=\dfrac{Kd^2}{v}-d","['ordinary-differential-equations', 'dynamical-systems', 'mathematical-modeling', 'biology']"
37,Implicit Answer to this DE $e^xyy'=e^{-y}+e^{-2x-y}$,Implicit Answer to this DE,e^xyy'=e^{-y}+e^{-2x-y},"$$e^xyy'=e^{-y}+e^{-2x-y}$$ I have to find a solution, implict solution for the preceding differential equation. The method that I have to use is the separable method.  My attempt went as the following: \begin{align} e^xy\frac{dy}{dx}&=e^{-y}+e^{-2x-y} \\ e^xy\frac{dy}{dx}&=e^{-y}+e^{-2x}e^{-y} \\ ye^x\frac{dy}{dx}&=e^{-y}(1+e^{-2x}) \\ ye^ydy&=\frac{(1+e^{-2x})}{e^x}dx \\ ye^ydy&=(e^{-x}+e^{-3x})dx  \end{align} $$\bbox[5px,border:2px solid red]{ e^y(y-1)=-e^{-x}-\frac{1}{3}e^{-3x}}$$ Is this the implicit way to write the solution, using the separation of variables?","I have to find a solution, implict solution for the preceding differential equation. The method that I have to use is the separable method.  My attempt went as the following: Is this the implicit way to write the solution, using the separation of variables?","e^xyy'=e^{-y}+e^{-2x-y} \begin{align}
e^xy\frac{dy}{dx}&=e^{-y}+e^{-2x-y} \\
e^xy\frac{dy}{dx}&=e^{-y}+e^{-2x}e^{-y} \\
ye^x\frac{dy}{dx}&=e^{-y}(1+e^{-2x}) \\
ye^ydy&=\frac{(1+e^{-2x})}{e^x}dx \\
ye^ydy&=(e^{-x}+e^{-3x})dx 
\end{align} \bbox[5px,border:2px solid red]{ e^y(y-1)=-e^{-x}-\frac{1}{3}e^{-3x}}","['calculus', 'integration', 'ordinary-differential-equations']"
38,3-dimensional Runge-Kutta method,3-dimensional Runge-Kutta method,,"I have a second order non-linear differential equation in 3 dimensions which can't be solved analytically and have been looking at different numerical methods for solving it. I am particularly interested in RK4, but I don't know how it generalizes to 3 dimensions after checking Google for quite some time. The problem looks as such: $$ x'' = f(x, x', y', z') \\ y'' = g(y, x', y', z') \\  z'' = h(z, x', y', z') \\ x(0) = x_0, y(0)=y_0,z(0) = z_0 \\ x'(0) = x'_0, y'(0)=y'_0, z'(0)=z'_0 $$ From here, I've gotten it down to a system of first order differential equations and I get: $$ u' = f(x,u,v,w) \\  v' = g(y,u,v,w) \\ w' = h(z,u,v,w) \\  x' = u \\  y' = v \\ z' = w $$ My problem now is how Runge-Kutta works in 3 dimensions. The calculation for $K_1$ seems easy enough, but for the other ones I'm stuck. For example, $K_2 = \Delta x f(x_n + \frac{\Delta x}{2},y_n+\frac{K_1}{2}) $ but I don't know what to put for the other parameters.","I have a second order non-linear differential equation in 3 dimensions which can't be solved analytically and have been looking at different numerical methods for solving it. I am particularly interested in RK4, but I don't know how it generalizes to 3 dimensions after checking Google for quite some time. The problem looks as such: From here, I've gotten it down to a system of first order differential equations and I get: My problem now is how Runge-Kutta works in 3 dimensions. The calculation for seems easy enough, but for the other ones I'm stuck. For example, but I don't know what to put for the other parameters.","
x'' = f(x, x', y', z') \\
y'' = g(y, x', y', z') \\ 
z'' = h(z, x', y', z') \\
x(0) = x_0, y(0)=y_0,z(0) = z_0 \\
x'(0) = x'_0, y'(0)=y'_0, z'(0)=z'_0
 
u' = f(x,u,v,w) \\ 
v' = g(y,u,v,w) \\
w' = h(z,u,v,w) \\ 
x' = u \\ 
y' = v \\
z' = w
 K_1 K_2 = \Delta x f(x_n + \frac{\Delta x}{2},y_n+\frac{K_1}{2}) ","['ordinary-differential-equations', 'numerical-methods', 'runge-kutta-methods']"
39,Some help and clarification to more-rigorously derive Green's Function solution for ODE $\dot{x}(t)-Ax(t) = f(t)$ with $x(0) = x_0$,Some help and clarification to more-rigorously derive Green's Function solution for ODE  with,\dot{x}(t)-Ax(t) = f(t) x(0) = x_0,"I'm trying to derive this solution given to us in a systems / control-theory course without proof. The first-order vector linear differential equation in question is $$\dot{x}(t)-Ax(t) = f(t)$$ $$x(0) = x_0$$ Where $x$ is a $n$ -dimensional vector in $R^n$ (for instance, a two or three dimensional vector), and $A$ is an $n$ x $n$ constant matrix. The unexplained solution is $$x(t) = e^{At}x_0 + \int_0^t e^{A(t-\tau)}f(\tau)d\tau$$ Where the first term $e^{At}x_0$ is the homogenous solution to $$\dot{x}(t)-Ax(t) = 0$$ $$x(0) = x_0$$ and the second term is the ""particular"" solution, the one I'm interested in deriving. I think it comes from a Green's function approach where $f(t)$ is decomposed into an infinite sum of dirac delta functions $\delta$ : $$ f(t) = \int_0^\infty f(\tau)\delta(t-\tau)d\tau $$ So by linearity of the differential equation, we first find a solution (response, trajectory, aka Green's function, $G(\tau,t)$ ) for a single delta function ""system input"" at time $\tau$ and add them up invoking the linearity of the differential equation to obtain: $$ x(t) = \int_0^\infty G(t, \tau)f(\tau)d\tau $$ But I'm a bit confused how to set up the ODE to get the Green's function. Following wiki and other sources, I have: $$ \dot{G}(t,\tau) - A G(t, \tau) = \delta(t-\tau) $$ but this $\delta$ has to be vector-valued since the left side is. And I suppose this ""dirac vector"" should point parallel to $f(\tau)$ : let $\hat{f}(\tau)$ be this unit vector to get it pointing properly. $$ \dot{G}(t,\tau) - A G(t, \tau) = \hat{f}(\tau) \delta(t-\tau) $$ Then there is the question of what initial value to use with this ODE. I don't really know how to answer that and why. But I tried $G(t=0,\tau)=0$ for all $\tau$ . I figured if the initial condition wasn't zero it would mess up the homogenous solution obeying a non-zero initial condition(?). Then the ODE says the system does nothing until $t > \tau$ , after which it does its natural (homogenous) response after a sudden jump from the origin to the point $\hat{f}(\tau)$ : $$ G(t,\tau) = 0\space for \space 0<t<\tau $$ $$ G(t,\tau) = e^{A(t-\tau)} \hat{f}(\tau) \space for \space 0<\tau<t $$ Which can be captured in one line by using the step function $H(t-\tau)$ ( $1$ if $t>\tau$ , $0$ otherwise): $$ G(t,\tau) = H(t-\tau) e^{A(t-\tau)} $$ Then adding up all the responses (ODE is linear, more justification desired) gives $$ x(t) = \int_0^\infty H(t-\tau) e^{A(t-\tau)} f(\tau) d\tau $$ The integrand is zero if $\tau$ exceeds t, so it simplifies to $$ x(t) = \int_0^t e^{A(t-\tau)} f(\tau) d\tau $$ All of which tries to explain the particular solution. Does this argument seem ok? Any areas where you could better explain what is going on? Specifically, I'm not sure if my reasoning about the boundary condition / initial value on $G$ is correct. Also my justification or reasoning for using $\hat{f}(\tau)$ to give my dirac delta function a vector value seems a little hand-wavy. Thanks.","I'm trying to derive this solution given to us in a systems / control-theory course without proof. The first-order vector linear differential equation in question is Where is a -dimensional vector in (for instance, a two or three dimensional vector), and is an x constant matrix. The unexplained solution is Where the first term is the homogenous solution to and the second term is the ""particular"" solution, the one I'm interested in deriving. I think it comes from a Green's function approach where is decomposed into an infinite sum of dirac delta functions : So by linearity of the differential equation, we first find a solution (response, trajectory, aka Green's function, ) for a single delta function ""system input"" at time and add them up invoking the linearity of the differential equation to obtain: But I'm a bit confused how to set up the ODE to get the Green's function. Following wiki and other sources, I have: but this has to be vector-valued since the left side is. And I suppose this ""dirac vector"" should point parallel to : let be this unit vector to get it pointing properly. Then there is the question of what initial value to use with this ODE. I don't really know how to answer that and why. But I tried for all . I figured if the initial condition wasn't zero it would mess up the homogenous solution obeying a non-zero initial condition(?). Then the ODE says the system does nothing until , after which it does its natural (homogenous) response after a sudden jump from the origin to the point : Which can be captured in one line by using the step function ( if , otherwise): Then adding up all the responses (ODE is linear, more justification desired) gives The integrand is zero if exceeds t, so it simplifies to All of which tries to explain the particular solution. Does this argument seem ok? Any areas where you could better explain what is going on? Specifically, I'm not sure if my reasoning about the boundary condition / initial value on is correct. Also my justification or reasoning for using to give my dirac delta function a vector value seems a little hand-wavy. Thanks.","\dot{x}(t)-Ax(t) = f(t) x(0) = x_0 x n R^n A n n x(t) = e^{At}x_0 + \int_0^t e^{A(t-\tau)}f(\tau)d\tau e^{At}x_0 \dot{x}(t)-Ax(t) = 0 x(0) = x_0 f(t) \delta 
f(t) = \int_0^\infty f(\tau)\delta(t-\tau)d\tau
 G(\tau,t) \tau 
x(t) = \int_0^\infty G(t, \tau)f(\tau)d\tau
 
\dot{G}(t,\tau) - A G(t, \tau) = \delta(t-\tau)
 \delta f(\tau) \hat{f}(\tau) 
\dot{G}(t,\tau) - A G(t, \tau) = \hat{f}(\tau) \delta(t-\tau)
 G(t=0,\tau)=0 \tau t > \tau \hat{f}(\tau) 
G(t,\tau) = 0\space for \space 0<t<\tau
 
G(t,\tau) = e^{A(t-\tau)} \hat{f}(\tau) \space for \space 0<\tau<t
 H(t-\tau) 1 t>\tau 0 
G(t,\tau) = H(t-\tau) e^{A(t-\tau)}
  x(t) = \int_0^\infty H(t-\tau) e^{A(t-\tau)} f(\tau) d\tau  \tau  x(t) = \int_0^t e^{A(t-\tau)} f(\tau) d\tau  G \hat{f}(\tau)","['ordinary-differential-equations', 'dynamical-systems', 'greens-function', 'linear-control']"
40,Searching for a second order ODE whose solution is bell shape (Gaussian function),Searching for a second order ODE whose solution is bell shape (Gaussian function),,"I'm studying a nonequilibrium dynamics of a stochastic system. I found that in mean-field approximation the numerical solution resembles a bell shaped function (Gaussian function) with is zero at initial time, then reaches its maximum and finally decays to zero. I was wondering if there exist a second order ODE whose solutions are smooth and resemble Gaussian curves.  I know that one can get the ODE satisfies by a Gaussian function just deriving it twice. But this is not the point.  I'm interested in a general ODE which exhibits solutions which have similar behaviour of Gaussians. Thank you in advance.","I'm studying a nonequilibrium dynamics of a stochastic system. I found that in mean-field approximation the numerical solution resembles a bell shaped function (Gaussian function) with is zero at initial time, then reaches its maximum and finally decays to zero. I was wondering if there exist a second order ODE whose solutions are smooth and resemble Gaussian curves.  I know that one can get the ODE satisfies by a Gaussian function just deriving it twice. But this is not the point.  I'm interested in a general ODE which exhibits solutions which have similar behaviour of Gaussians. Thank you in advance.",,"['ordinary-differential-equations', 'stochastic-calculus', 'gaussian']"
41,First integral of ODE system,First integral of ODE system,,"I am trying to make sense out of the first integral of non-linear ODE systems. $\bullet$ Is the first integral only relevant to a certain type of ODE's e.g. autonomous, first order? $\bullet$ How is the first integral generally defined and/or calculated of say: $$ \begin{pmatrix} \dot x\\\dot y \end{pmatrix} = \begin{pmatrix} x-y\\ xy \end{pmatrix} $$ $\bullet$ What can I normally do knowing it (very generally speaking)? Thanks for your help. Edit: Concrete problem is given (Open to suggestions)","I am trying to make sense out of the first integral of non-linear ODE systems. Is the first integral only relevant to a certain type of ODE's e.g. autonomous, first order? How is the first integral generally defined and/or calculated of say: What can I normally do knowing it (very generally speaking)? Thanks for your help. Edit: Concrete problem is given (Open to suggestions)","\bullet \bullet 
\begin{pmatrix}
\dot x\\\dot y
\end{pmatrix}
=
\begin{pmatrix}
x-y\\ xy
\end{pmatrix}
 \bullet","['integration', 'ordinary-differential-equations']"
42,Problem separating homogeneous differential equation,Problem separating homogeneous differential equation,,"I need to solve the following homogeneous differential equation: $$ \frac{dy}{dx}=\frac{x+3y}{3x+y} $$ As I understand it, I have to use a substitution, either $y = ux, dy=u\,dx+x\,du$ or $x=uy, dx=u\,dy+y\,du$ . So I try to substitute either, and I get stuck because I'm not having a separated equation I can integrate. Here's what I have done so far: Using the first substitution: $$ \frac{u\,dx+x\,du}{dx}=\frac{x+3ux}{3x+ux} \\ (3x+ux)(u\,dx+x\,du)=(x+3ux)dx \\ 3x^2\,du+ux^2\,du=x\,dx-u^2x\,dx\\ 3\,du+u\,du=\frac{dx}{x}-\frac{u^2\,dx}{x} $$ But at this point, dividing by $u^2$ will make me have the following term: $\frac{dx}{u^2x}$ , and this is where I'm stuck, as I cannot separate. I checked my algebra to make sure I didn't blunder, and I think I'm clear (I hope). Using the second substitution yields a similar result: $$ \frac{dy}{u\,dy+uy\,du}=\frac{uy+3y}{3uy+y} \\ y\,dy-u^2y\,dy=3y^2\,du+uy^2\,du \\ \frac{dy}{y}-\frac{u^2\,dy}{y}=3\,du+u\,du $$ Any pointer in the right direction will be appreciated. Thanks! Edit: so for completeness sake, thanks to the answer and comment provided: $$ \frac{u\,dx+x\,du}{dx}=\frac{x+3ux}{3x+ux} \\ \frac{u\,dx+x\,du}{dx}=\frac{x(1+3u)}{x(3+u)} \\ \frac{u\,dx+x\,du}{dx}=\frac{1+3u}{3+u} \\ (u\,dx+x\,du)(3+u)=(1+3u)dx \\ 3u\,dx+u^2\,dx+3x\,du+ux\,du=dx+3u\,dx \\ u^2\,dx+2x\,du+ux\,du=dx \\ 3x\,du+u\,du=\frac{dx}{x}-\frac{u^2\,dx}{x} \\ (3+u)\,du=(1-u^2)\frac{dx}{x} \\ \frac{3+u}{1-u^2}\,du=\frac{dx}{x} \\ \frac{3\,du}{1-u^2}+\frac{u\,du}{1-u^2}=\frac{dx}{x} \\ \int{\frac{3\,du}{1-u^2}}+\int{\frac{u\,du}{1-u^2}}=\int{\frac{dx}{x}} $$ Which is now solvable. Thanks for the help!","I need to solve the following homogeneous differential equation: As I understand it, I have to use a substitution, either or . So I try to substitute either, and I get stuck because I'm not having a separated equation I can integrate. Here's what I have done so far: Using the first substitution: But at this point, dividing by will make me have the following term: , and this is where I'm stuck, as I cannot separate. I checked my algebra to make sure I didn't blunder, and I think I'm clear (I hope). Using the second substitution yields a similar result: Any pointer in the right direction will be appreciated. Thanks! Edit: so for completeness sake, thanks to the answer and comment provided: Which is now solvable. Thanks for the help!","
\frac{dy}{dx}=\frac{x+3y}{3x+y}
 y = ux, dy=u\,dx+x\,du x=uy, dx=u\,dy+y\,du 
\frac{u\,dx+x\,du}{dx}=\frac{x+3ux}{3x+ux} \\
(3x+ux)(u\,dx+x\,du)=(x+3ux)dx \\
3x^2\,du+ux^2\,du=x\,dx-u^2x\,dx\\
3\,du+u\,du=\frac{dx}{x}-\frac{u^2\,dx}{x}
 u^2 \frac{dx}{u^2x} 
\frac{dy}{u\,dy+uy\,du}=\frac{uy+3y}{3uy+y} \\
y\,dy-u^2y\,dy=3y^2\,du+uy^2\,du \\
\frac{dy}{y}-\frac{u^2\,dy}{y}=3\,du+u\,du
 
\frac{u\,dx+x\,du}{dx}=\frac{x+3ux}{3x+ux} \\
\frac{u\,dx+x\,du}{dx}=\frac{x(1+3u)}{x(3+u)} \\
\frac{u\,dx+x\,du}{dx}=\frac{1+3u}{3+u} \\
(u\,dx+x\,du)(3+u)=(1+3u)dx \\
3u\,dx+u^2\,dx+3x\,du+ux\,du=dx+3u\,dx \\
u^2\,dx+2x\,du+ux\,du=dx \\
3x\,du+u\,du=\frac{dx}{x}-\frac{u^2\,dx}{x} \\
(3+u)\,du=(1-u^2)\frac{dx}{x} \\
\frac{3+u}{1-u^2}\,du=\frac{dx}{x} \\
\frac{3\,du}{1-u^2}+\frac{u\,du}{1-u^2}=\frac{dx}{x} \\
\int{\frac{3\,du}{1-u^2}}+\int{\frac{u\,du}{1-u^2}}=\int{\frac{dx}{x}}
",['ordinary-differential-equations']
43,Linear ordinary differential equations and matrix exponentiation,Linear ordinary differential equations and matrix exponentiation,,"I feel the solution to this problem is simple - but I am not entirely clear on what the question actually wants. Given the following: $$A = \left[ \begin{array}{ccc}   -0.1005 & -0.266\\   -0.1498 & 0.2005  \end{array} \right] $$ $$\frac{dx}{dt} = Ax; $$ $$x(0) = x0 = [1; −2]$$ Show, in general, that the following form solves $\frac{dx}{dt} = Ax$ i) $$  x(t) = e^{At}x_0$$ ii) $$x(t) = \sum_{i}a_i e_i e^{\lambda_it}$$ Usually when I come across a problem I don'tt even understand, I try to solve simpler analogous problems. I can't quite find one comparable to this. Can someone point out the first couple of steps for me? Maybe for i), for example?","I feel the solution to this problem is simple - but I am not entirely clear on what the question actually wants. Given the following: Show, in general, that the following form solves i) ii) Usually when I come across a problem I don'tt even understand, I try to solve simpler analogous problems. I can't quite find one comparable to this. Can someone point out the first couple of steps for me? Maybe for i), for example?","A = \left[
\begin{array}{ccc}
  -0.1005 & -0.266\\
  -0.1498 & 0.2005 
\end{array}
\right]  \frac{dx}{dt} = Ax;  x(0) = x0 = [1; −2] \frac{dx}{dt} = Ax   x(t) = e^{At}x_0 x(t) = \sum_{i}a_i e_i e^{\lambda_it}","['matrices', 'ordinary-differential-equations', 'matrix-exponential']"
44,Does the Frobenius method work for all second order linear differential equations with only regular singular points?,Does the Frobenius method work for all second order linear differential equations with only regular singular points?,,"For $$x^2y''+x(x^2+1)y'+(x-4)y=0,\tag1$$ there is a regular singular point at $0$ , but when I tried to use the Frobenius method and substituted $$y=\sum_{n=0}^\infty a_n x^\left(n+r\right)\tag2$$ into the differential equation, I obtained \begin{multline} (r^2-4)a_0x^r+[(r^2+2r+5)a_1+a_0]x^\left(r+1\right) \\+\sum_{n=0}^\infty\left\{[(n+r+2)^2+2]a_\left(n+2\right)+a_\left(n+1\right)+(n+2)a_n\right\}=0\tag3 \end{multline} I understand that if all $x$ in an interval satisfy the differential equation, then all the coefficients must be zero. But then this means $r=\pm2$ (from the first term), and certainly the second term will not be satisfied by arbitrary $a_0$ and $a_1$ (the initial conditions). Therefore, my question is does the Frobenius method only work for certain second order linear differential equations with only regular singular points, like where $p(x)$ and $q(x)$ in $~x^2y''+p(x)y'+q(x)y=0~$ are first or second degree polynomials?","For there is a regular singular point at , but when I tried to use the Frobenius method and substituted into the differential equation, I obtained I understand that if all in an interval satisfy the differential equation, then all the coefficients must be zero. But then this means (from the first term), and certainly the second term will not be satisfied by arbitrary and (the initial conditions). Therefore, my question is does the Frobenius method only work for certain second order linear differential equations with only regular singular points, like where and in are first or second degree polynomials?","x^2y''+x(x^2+1)y'+(x-4)y=0,\tag1 0 y=\sum_{n=0}^\infty a_n x^\left(n+r\right)\tag2 \begin{multline}
(r^2-4)a_0x^r+[(r^2+2r+5)a_1+a_0]x^\left(r+1\right)
\\+\sum_{n=0}^\infty\left\{[(n+r+2)^2+2]a_\left(n+2\right)+a_\left(n+1\right)+(n+2)a_n\right\}=0\tag3
\end{multline} x r=\pm2 a_0 a_1 p(x) q(x) ~x^2y''+p(x)y'+q(x)y=0~","['ordinary-differential-equations', 'frobenius-method']"
45,"A nonlinear 4th order ODE, not sure how to solve","A nonlinear 4th order ODE, not sure how to solve",,"Here is a problem I am not sure how the solution is arrived. It would be great if you could show how to obtain the given solution as in the question. Given the ODE, and the boundaries conditions, obtain P(r) in terms of W(r). For those who are interested, the full questions is as such,","Here is a problem I am not sure how the solution is arrived. It would be great if you could show how to obtain the given solution as in the question. Given the ODE, and the boundaries conditions, obtain P(r) in terms of W(r). For those who are interested, the full questions is as such,",,"['ordinary-differential-equations', 'fluid-dynamics']"
46,Linearization of $C^r$ ODE system,Linearization of  ODE system,C^r,"Consider the following ODE system \begin{align*} \dot{x} &= x^5 + y^3 = f(x\,,y) \\ \dot{y} &= x^3 - y^5 = g(x\,,y) \end{align*} Clearly, $(x\,,y) = (0\,,0)$ is the only fixed point. Linearizing $$ J =  \begin{bmatrix} 5x^4 & 3y^2 \\ 3x^2 & -5y^4 \end{bmatrix} $$ evaluating at the fixed point will just give a zero matrix. Thus, need to use some other approach to analyze the stability, e.g., Lyapunov function. However, I can't find any Lyapunov function related to this problem. Since this ODE system is at least $C^3$ in both $x$ and $y$ . So I am wondering, for the linearization, is it possible to go all the way to 3rd-order derivative? Maybe something like $$ \begin{bmatrix} \frac{\partial^3f}{\partial x^3} & \frac{\partial^3f}{\partial y^3} \\ \frac{\partial^3g}{\partial x^3} & \frac{\partial^3g}{\partial y^3} \end{bmatrix} $$ This way the matrix will not be a zero matrix and can find corresponding eigenvalues and eigenvectors.","Consider the following ODE system Clearly, is the only fixed point. Linearizing evaluating at the fixed point will just give a zero matrix. Thus, need to use some other approach to analyze the stability, e.g., Lyapunov function. However, I can't find any Lyapunov function related to this problem. Since this ODE system is at least in both and . So I am wondering, for the linearization, is it possible to go all the way to 3rd-order derivative? Maybe something like This way the matrix will not be a zero matrix and can find corresponding eigenvalues and eigenvectors.","\begin{align*}
\dot{x} &= x^5 + y^3 = f(x\,,y) \\
\dot{y} &= x^3 - y^5 = g(x\,,y)
\end{align*} (x\,,y) = (0\,,0) 
J = 
\begin{bmatrix}
5x^4 & 3y^2 \\
3x^2 & -5y^4
\end{bmatrix}
 C^3 x y 
\begin{bmatrix}
\frac{\partial^3f}{\partial x^3} & \frac{\partial^3f}{\partial y^3} \\
\frac{\partial^3g}{\partial x^3} & \frac{\partial^3g}{\partial y^3}
\end{bmatrix}
","['ordinary-differential-equations', 'stability-theory', 'linearization']"
47,"Question about ODEs, while reading Milnor's Topology from the Differentiable Viewpoint","Question about ODEs, while reading Milnor's Topology from the Differentiable Viewpoint",,"I am reading Milnor's Topology from the Differential Viewpoint , p.23. The paragraph below is extracted from the book: Let $\varphi : \Bbb R^n \to \Bbb R$ be a smooth function. Given any fixed unit vector $c=(c_1,...,c_n)\in S^{n-1}$ and a point $x_0 \in \Bbb R^n$ , consider the differential equations $$ \frac{dx_i}{dt}(t)=c_i \cdot \varphi (x_1(t),...,x_n(t))~(i=1,...,n),~~ ~x(0)=x_0.$$ These equations have a unique solution $x=x(t)=(x_1(t),...,x_n(t))$ defined for all real numbers. I know that existence and uniqueness follow from a theorem in ODE, but how can we assure that the domain of $x$ is the whole $\Bbb R$ ?","I am reading Milnor's Topology from the Differential Viewpoint , p.23. The paragraph below is extracted from the book: Let be a smooth function. Given any fixed unit vector and a point , consider the differential equations These equations have a unique solution defined for all real numbers. I know that existence and uniqueness follow from a theorem in ODE, but how can we assure that the domain of is the whole ?","\varphi : \Bbb R^n \to \Bbb R c=(c_1,...,c_n)\in S^{n-1} x_0 \in \Bbb R^n  \frac{dx_i}{dt}(t)=c_i \cdot \varphi (x_1(t),...,x_n(t))~(i=1,...,n),~~ ~x(0)=x_0. x=x(t)=(x_1(t),...,x_n(t)) x \Bbb R","['ordinary-differential-equations', 'differential-topology']"
48,Is the SEIS Model more accurate and/or realistic than the SEIRS Model in modelling the 2019-nCov?,Is the SEIS Model more accurate and/or realistic than the SEIRS Model in modelling the 2019-nCov?,,"I am modelling the CoronaVirus for my IB maths IA (project). I have explained the principles and assumptions and derived the system of differential equations of the SEIRS Model. However, the R(recovered) compartment does not make much sense for me... and its presence adds difficulty in the calculation. I am considering the SEIS model which is pretty much the same as the SEIRS model apart from having a separate compartment. The current problem is that it seems to be worthless calculating the R compartment as it is added back to the S(susceptible) compartment anyways, assuming that recovered individuals do not develop immunity and can be infected again immediately. Also, the system of ordinary differential equations that I got without vital dynamics and in absence of vaccination and isolation is Can someone confirm this? I couldn't find one online. Many thanks!!","I am modelling the CoronaVirus for my IB maths IA (project). I have explained the principles and assumptions and derived the system of differential equations of the SEIRS Model. However, the R(recovered) compartment does not make much sense for me... and its presence adds difficulty in the calculation. I am considering the SEIS model which is pretty much the same as the SEIRS model apart from having a separate compartment. The current problem is that it seems to be worthless calculating the R compartment as it is added back to the S(susceptible) compartment anyways, assuming that recovered individuals do not develop immunity and can be infected again immediately. Also, the system of ordinary differential equations that I got without vital dynamics and in absence of vaccination and isolation is Can someone confirm this? I couldn't find one online. Many thanks!!",,"['ordinary-differential-equations', 'mathematical-modeling', 'biology']"
49,Gronwall-type inequality assumption implies function is identically zero,Gronwall-type inequality assumption implies function is identically zero,,"After stating and proving a version of Gronwall's lemma for continuous functions (as in this related question ), the author of the book I'm reading suggests trying to prove a related fact as an exercise. Suppose that $$ \phi(t) \le \int_{t_0}^t \psi(s)\phi(s) ds, $$ where $t_0 \le t \le t_0 + a$ , and the functions $\phi(t), \psi(t) \ge 0$ are continuous. The exercise is to show that this implies $\phi(t) = 0$ for all $t \in [t_0, t_0+a]$ . I can see why this might be true (by picturing what happens if $\phi(t)$ is not identically zero),  but I am struggling to prove this. The given proof of Gronwall's lemma does not work for this assumption. It looks as if an integral form of the MVT may help, but I was not able to make it work. Can someone point me in the right direction?","After stating and proving a version of Gronwall's lemma for continuous functions (as in this related question ), the author of the book I'm reading suggests trying to prove a related fact as an exercise. Suppose that where , and the functions are continuous. The exercise is to show that this implies for all . I can see why this might be true (by picturing what happens if is not identically zero),  but I am struggling to prove this. The given proof of Gronwall's lemma does not work for this assumption. It looks as if an integral form of the MVT may help, but I was not able to make it work. Can someone point me in the right direction?"," \phi(t) \le \int_{t_0}^t \psi(s)\phi(s) ds,  t_0 \le t \le t_0 + a \phi(t), \psi(t) \ge 0 \phi(t) = 0 t \in [t_0, t_0+a] \phi(t)","['real-analysis', 'ordinary-differential-equations']"
50,"ODE, and Questioning Method","ODE, and Questioning Method",,"I have a question about the method used to answer this question: IVP, and ODE problem $$\frac{dy}{dx}=ye^{-x^2} \ \ \ \ \ \ y(4)=1 $$ \begin{align} \frac{dy}y&=\int e^{-x^2}dx \\ \ln(y)&=\int(e^{-x^2})dx \\ \end{align} Then this is where I get confused the work suddenly jumps to the following: \begin{align} \frac{dy}y&=e^{-x^2}dx \\ \frac{1}{y}\frac{dy}{dt}dt&=e^{-x^2}dx \\ \int_4^x\frac{1}y\frac{dy}{dt}dt&=\int_4^xe^{-t^2}dt \\ \ln(\lvert(y(t)\rvert)\Bigg\vert_4^x&=\int_4^xe^{-t^2}dt \end{align} The final answer is the following: $$y=e^{\int_4^xe^{-t^2}dt}$$ My Question My question is could someone explain the use of the dummy variables, and how the method actually works? Because I am confused about the final answer to the ODE.","I have a question about the method used to answer this question: IVP, and ODE problem Then this is where I get confused the work suddenly jumps to the following: The final answer is the following: My Question My question is could someone explain the use of the dummy variables, and how the method actually works? Because I am confused about the final answer to the ODE.","\frac{dy}{dx}=ye^{-x^2} \ \ \ \ \ \ y(4)=1  \begin{align}
\frac{dy}y&=\int e^{-x^2}dx \\
\ln(y)&=\int(e^{-x^2})dx \\
\end{align} \begin{align}
\frac{dy}y&=e^{-x^2}dx \\
\frac{1}{y}\frac{dy}{dt}dt&=e^{-x^2}dx \\
\int_4^x\frac{1}y\frac{dy}{dt}dt&=\int_4^xe^{-t^2}dt \\
\ln(\lvert(y(t)\rvert)\Bigg\vert_4^x&=\int_4^xe^{-t^2}dt
\end{align} y=e^{\int_4^xe^{-t^2}dt}","['calculus', 'ordinary-differential-equations']"
51,Can a set of functions be orthogonal w.r.t. more than one weight function?,Can a set of functions be orthogonal w.r.t. more than one weight function?,,"A random thought that I had: suppose you have a set of real-valued one-dimensional functions $\{f_i(x)\}$ which are: Smooth over the domain of interest. Orthogonal with respect to integration against some weight function $w_1(x)$ (which is also smooth over the domain of interest): $$\int_a^b f_i(x) f_j(x) w(x)\,dx=\delta_{ij} $$ Complete for smooth functions defined over that interval, i.e. any smooth function can be written as a series expansion in $\{f_i(x)\}$ : $$g(x)=\sum_{n=0}^{\infty} c_n f_n(x)$$ Is it possible for the same set of functions $\{f_i(x)\}$ to be orthogonal with respect to a separate weight function $w_2(x)$ , which is independent from $w_1(x)$ ? (i.e. the Wronskian of the two functions is non-zero everywhere) If yes, can you provide an example?","A random thought that I had: suppose you have a set of real-valued one-dimensional functions which are: Smooth over the domain of interest. Orthogonal with respect to integration against some weight function (which is also smooth over the domain of interest): Complete for smooth functions defined over that interval, i.e. any smooth function can be written as a series expansion in : Is it possible for the same set of functions to be orthogonal with respect to a separate weight function , which is independent from ? (i.e. the Wronskian of the two functions is non-zero everywhere) If yes, can you provide an example?","\{f_i(x)\} w_1(x) \int_a^b f_i(x) f_j(x) w(x)\,dx=\delta_{ij}  \{f_i(x)\} g(x)=\sum_{n=0}^{\infty} c_n f_n(x) \{f_i(x)\} w_2(x) w_1(x)","['real-analysis', 'linear-algebra', 'ordinary-differential-equations']"
52,Differential equation- Solutions to initial condition problem,Differential equation- Solutions to initial condition problem,,"The equation given is $$x(t)=C_{1}e^{-t}+C_{2}e^{2t}$$ $$x^{''}-x^{'}-2x=0$$ The first and second derivatives are given respectfully $$-C_{1}e^{-t}+2C_{2}e^{2t}$$ $$C_{1}e^{-t}+4C_{2}e^{2t}$$ I have verified that the problem does in fact equal zero. The given conditions are: $$x(0)=10$$ $$x^{'}(0)=8$$ After substitution and evaluation of the original and first prime equations I got $C_2=\frac{8}{3}$ and $C_1=\frac{16}{3}$ and having no one around to check if I'm correct,I was hoping that someone could tell me if I'm right or if I made a small mistake.","The equation given is The first and second derivatives are given respectfully I have verified that the problem does in fact equal zero. The given conditions are: After substitution and evaluation of the original and first prime equations I got and and having no one around to check if I'm correct,I was hoping that someone could tell me if I'm right or if I made a small mistake.",x(t)=C_{1}e^{-t}+C_{2}e^{2t} x^{''}-x^{'}-2x=0 -C_{1}e^{-t}+2C_{2}e^{2t} C_{1}e^{-t}+4C_{2}e^{2t} x(0)=10 x^{'}(0)=8 C_2=\frac{8}{3} C_1=\frac{16}{3},"['ordinary-differential-equations', 'initial-value-problems']"
53,Wronskian of $x|x|$ and $x^2$.,Wronskian of  and .,x|x| x^2,"Wikipedia says wronskian of $x|x|$ and $x^2$ is identically zero. But it is not LD. I know why these two are LI and not LD. since x|x| is not differentiable function,how to find their wronskian????  And plz suggest ways to check LI and LD when functions are not differentiable. Thanks in advance. Plz help.","Wikipedia says wronskian of and is identically zero. But it is not LD. I know why these two are LI and not LD. since x|x| is not differentiable function,how to find their wronskian????  And plz suggest ways to check LI and LD when functions are not differentiable. Thanks in advance. Plz help.",x|x| x^2,"['calculus', 'ordinary-differential-equations', 'wronskian']"
54,Does Lyapunov stability imply that the Lyapunov function is monotonically decreasing?,Does Lyapunov stability imply that the Lyapunov function is monotonically decreasing?,,"Suppose we have a system $$\dot x = Ax$$ And a Lyapunov function $$V (x(t)) \geq 0, \forall t $$ My question is, does the fact $\dot V(x) = \nabla V(x)^\top \dot x < 0$ imply that $V(x(t))$ is monotonically decreasing along with time? Ok, so intuitively, integrating bothsides we have $V(x(t)) < V(x(0))$ , $\forall t$ . So it is decreasing relative to the initial time. But what about the time afterwards, can we guarantee $V(x(t_2)) < V(x(t_1)) $ where $t_2>t_1>0$ ?","Suppose we have a system And a Lyapunov function My question is, does the fact imply that is monotonically decreasing along with time? Ok, so intuitively, integrating bothsides we have , . So it is decreasing relative to the initial time. But what about the time afterwards, can we guarantee where ?","\dot x = Ax V (x(t)) \geq 0, \forall t  \dot V(x) = \nabla V(x)^\top \dot x < 0 V(x(t)) V(x(t)) < V(x(0)) \forall t V(x(t_2)) < V(x(t_1))  t_2>t_1>0","['ordinary-differential-equations', 'dynamical-systems', 'lyapunov-functions']"
55,Solution of the linear differential equation: $\frac {dy}{dx} + P(x) \cdot y=Q(x)$. What is the error in this approach?,Solution of the linear differential equation: . What is the error in this approach?,\frac {dy}{dx} + P(x) \cdot y=Q(x),"Derive the solution of the linear differential equation: $\frac {dy}{dx} + P(x) \cdot y=Q(x)$ Rewriting the given differential equation, we obtain: $(Py-Q) dx+1 \cdot dy=0$ . Let $M=Py-Q, N=1$ . Then : $\dfrac {\partial M }{\partial y}=M_y=P$ and $\dfrac {\partial N}{\partial x}=N_x=0$ . Thus $\dfrac{M_y-N_x}{N}=P(x)$ . Thus, the integrating factor is $I.F= e^{\int P dx}$ . Therefore $e^{\int P dx}(Py-Q) dx+e^{\int P dx} \cdot dy=0$ is an exact differential equation. The solution of this exact differential equation is $\int_{\text {treat y as constant} } M dx + \int \text{terms in N not containing x}~~ dy= $ constant $\implies \int e^{\int P dx}(Py-Q)~ dx + 0=c$ $\implies y \int P~ e^{\int P dx}~dx = \int e^{\int P dx} Q ~dx + c.~$ But the solution of the differential equation in almost every textbook is given as $\implies y e^{\int P dx}~dx = \int e^{\int P dx} Q ~dx + c$ What is the error in the above steps. Thanks a lot for your help.","Derive the solution of the linear differential equation: Rewriting the given differential equation, we obtain: . Let . Then : and . Thus . Thus, the integrating factor is . Therefore is an exact differential equation. The solution of this exact differential equation is constant But the solution of the differential equation in almost every textbook is given as What is the error in the above steps. Thanks a lot for your help.","\frac {dy}{dx} + P(x) \cdot y=Q(x) (Py-Q) dx+1 \cdot dy=0 M=Py-Q, N=1 \dfrac {\partial M }{\partial y}=M_y=P \dfrac {\partial N}{\partial x}=N_x=0 \dfrac{M_y-N_x}{N}=P(x) I.F= e^{\int P dx} e^{\int P dx}(Py-Q) dx+e^{\int P dx} \cdot dy=0 \int_{\text {treat y as constant} } M dx + \int \text{terms in N not containing x}~~ dy=  \implies \int e^{\int P dx}(Py-Q)~ dx + 0=c \implies y \int P~ e^{\int P dx}~dx = \int e^{\int P dx} Q ~dx + c.~ \implies y e^{\int P dx}~dx = \int e^{\int P dx} Q ~dx + c",['ordinary-differential-equations']
56,Exercise on life of maximal solutions of ODE,Exercise on life of maximal solutions of ODE,,"Let $V: \mathbb{R}^n \to \mathbb{R}^n$ be a $C^1$ vector field and suppose that $\int_{1}^{\infty}\frac{dr}{B(r)}=\infty$ where $$ B(r) = \sup(|V(x)|: |x|<r ). $$ Prove that the maximal solutions of the differential equation $\dot x = V(x)$ live on all $\mathbb{R}$ . Let $\sigma_p:(\alpha, \omega) \to \mathbb{R}^n$ be the maximal solution with $\sigma_p(0)=p$ , I want to prove that $\omega = \infty$ (I imagine that $\alpha=-\infty$ can be proved similarly). I've tried reasoning by contradiction so that $t_R = \inf(0\le t \lt \omega: |\sigma_p(t)|\ge R)$ is finite for every $R>0$ , but the best I could came up with is $B(R) \ge |V(\sigma_p(t_R))|$ and I don't know if $$ \int_{1}^{\infty} \frac{1}{|V(\sigma_p(t_R))|}dR< +\infty $$ i.e. if it converges. I've also some tried other things like $$ |\sigma_p(t)-p|\le \int_{0}^{t}|V(\sigma_p(s))|ds $$ and got $R\le t_R B(R)$ , but nothing again. Can you give me a hint?","Let be a vector field and suppose that where Prove that the maximal solutions of the differential equation live on all . Let be the maximal solution with , I want to prove that (I imagine that can be proved similarly). I've tried reasoning by contradiction so that is finite for every , but the best I could came up with is and I don't know if i.e. if it converges. I've also some tried other things like and got , but nothing again. Can you give me a hint?","V: \mathbb{R}^n \to \mathbb{R}^n C^1 \int_{1}^{\infty}\frac{dr}{B(r)}=\infty 
B(r) = \sup(|V(x)|: |x|<r ).
 \dot x = V(x) \mathbb{R} \sigma_p:(\alpha, \omega) \to \mathbb{R}^n \sigma_p(0)=p \omega = \infty \alpha=-\infty t_R = \inf(0\le t \lt \omega: |\sigma_p(t)|\ge R) R>0 B(R) \ge |V(\sigma_p(t_R))| 
\int_{1}^{\infty} \frac{1}{|V(\sigma_p(t_R))|}dR< +\infty
 
|\sigma_p(t)-p|\le \int_{0}^{t}|V(\sigma_p(s))|ds
 R\le t_R B(R)","['ordinary-differential-equations', 'analysis', 'dynamical-systems', 'mathematical-physics', 'vector-fields']"
57,"Linear differential equations, integrating factor","Linear differential equations, integrating factor",,Solve the following differential equation: $$ dr+(2r \cot\theta+\sin 2\theta)d\theta=0$$ I have tried like this: $$ \frac{dr}{d\theta}+2r\cot\theta=-\sin{2\theta}$$ \begin{align} I.F. &=e^{\int{2\cot\theta d\theta}}\\ & =e^{-2\log\sin\theta}\\ & =\frac 1{\sin^2\theta}\\ \end{align} $$\therefore\frac r{\sin^2\theta}=-\int\frac{\sin 2\theta d\theta}{\sin^2\theta}\\ \implies \frac r{\sin^2\theta}=-2\int{\cot\theta d\theta}\\ \implies \frac r{\sin^2\theta}=2\log\sin\theta+c $$ But in my book the answer is: $$2r{\sin^2\theta}+{\sin^4\theta}=c$$ Please check out which is correct..,Solve the following differential equation: I have tried like this: But in my book the answer is: Please check out which is correct..," dr+(2r \cot\theta+\sin 2\theta)d\theta=0  \frac{dr}{d\theta}+2r\cot\theta=-\sin{2\theta} \begin{align}
I.F. &=e^{\int{2\cot\theta d\theta}}\\
& =e^{-2\log\sin\theta}\\
& =\frac 1{\sin^2\theta}\\
\end{align} \therefore\frac r{\sin^2\theta}=-\int\frac{\sin 2\theta d\theta}{\sin^2\theta}\\
\implies \frac r{\sin^2\theta}=-2\int{\cot\theta d\theta}\\
\implies \frac r{\sin^2\theta}=2\log\sin\theta+c  2r{\sin^2\theta}+{\sin^4\theta}=c","['calculus', 'ordinary-differential-equations']"
58,Proving a solution doesn't exist of a DE,Proving a solution doesn't exist of a DE,,"Find all solutions of differential equation $xy'+(1-x)y=0$ and $xy'+(1-x)y=1$ . Also does there exist a solution such that $y(0)=0$ and $y(0)=1$ for both these D.Es ? My attempt : $xy'+ (1-x)y=0 \implies$ $\int \frac{dy}{y} = \int \frac{x-1}{x} dx$ for $x\ne 0$ and $y(x)\ne 0$ for any $x \in \mathbb{R}$ . Hence $y=\frac{k}{x}e^x$ for constant $k\in \mathbb{R}-\{0\}$ . But $y(x)=0 \,\forall\, x$ is also a valid solution of this D.E. Hence there exist a solution such that $y(0)=0$ . All possible solutions of this D.E are: $$y=\left \{ 	\begin{array}{ll} 		\frac{k}{x}e^x  & \mbox{if } x \ne 0 \\ 		0 & \mbox{if } x =0 	\end{array} \right. $$ where $k\in \mathbb{R}$ .  But $y(0)=1$ is not possible since $0\cdot \frac{dy}{dx}\mid_{x=0}+(1-0)y(0)=0$ yields $y(0)=0$ . I am not sure wether this proves the first part of the question. For the second part, we have the D.E $xy'+(1-x)y=1 \implies$ $\int d(xe^{-x}y) = \int e^{-x}dx$ . Hence $xy=-1+ce^{x}$ where $c \in \mathbb{R}$ . In this case, all possible solutions are: $$y=\left \{ 	\begin{array}{ll} 		\frac{-1}{x}+\frac{c}{x}e^x  & \mbox{if } x \ne 0 \\ 		1 & \mbox{if } x =0 	\end{array} \right. $$ $y(0)=0$ is not possible since $0\cdot \frac{dy}{dx}\mid_{x=0}+(1-0)y(0)=1$ yields $y(0)=1$ . From the definition of $y$ above, is that a valid solution such that $y(0)=1$ ? Is this proof correct ?","Find all solutions of differential equation and . Also does there exist a solution such that and for both these D.Es ? My attempt : for and for any . Hence for constant . But is also a valid solution of this D.E. Hence there exist a solution such that . All possible solutions of this D.E are: where .  But is not possible since yields . I am not sure wether this proves the first part of the question. For the second part, we have the D.E . Hence where . In this case, all possible solutions are: is not possible since yields . From the definition of above, is that a valid solution such that ? Is this proof correct ?","xy'+(1-x)y=0 xy'+(1-x)y=1 y(0)=0 y(0)=1 xy'+ (1-x)y=0 \implies \int \frac{dy}{y} = \int \frac{x-1}{x} dx x\ne 0 y(x)\ne 0 x \in \mathbb{R} y=\frac{k}{x}e^x k\in \mathbb{R}-\{0\} y(x)=0 \,\forall\, x y(0)=0 y=\left \{
	\begin{array}{ll}
		\frac{k}{x}e^x  & \mbox{if } x \ne 0 \\
		0 & \mbox{if } x =0
	\end{array}
\right.  k\in \mathbb{R} y(0)=1 0\cdot \frac{dy}{dx}\mid_{x=0}+(1-0)y(0)=0 y(0)=0 xy'+(1-x)y=1 \implies \int d(xe^{-x}y) = \int e^{-x}dx xy=-1+ce^{x} c \in \mathbb{R} y=\left \{
	\begin{array}{ll}
		\frac{-1}{x}+\frac{c}{x}e^x  & \mbox{if } x \ne 0 \\
		1 & \mbox{if } x =0
	\end{array}
\right.  y(0)=0 0\cdot \frac{dy}{dx}\mid_{x=0}+(1-0)y(0)=1 y(0)=1 y y(0)=1","['ordinary-differential-equations', 'solution-verification']"
59,"Solve the initial value problem $xy'=y(xy-1), y(e^{-1})=e$",Solve the initial value problem,"xy'=y(xy-1), y(e^{-1})=e","Solve the initial value problem $xy'=y(xy-1), y(e^{-1})=e$ I was given the hint to use the substitution, but don't know how to find the proper substitution. What's the general rule of thumb in finding a substitution?","Solve the initial value problem I was given the hint to use the substitution, but don't know how to find the proper substitution. What's the general rule of thumb in finding a substitution?","xy'=y(xy-1), y(e^{-1})=e","['ordinary-differential-equations', 'initial-value-problems']"
60,General solution for a second order linear ODE,General solution for a second order linear ODE,,"Given the equation \begin{equation} \frac{d^2y}{dx^2}=f(x)y\,, \end{equation} is it somehow possible to find an expression for $y$ in terms of $f(x)$ in the interval $x>0$ , assuming that we know that $\lim_{x \to \infty}f(x) \sim 1/x^2$ and $\lim_{x \to 0}f(x) \sim 1/x^2$ and that $f(x)$ and $y$ are everywhere smooth.","Given the equation is it somehow possible to find an expression for in terms of in the interval , assuming that we know that and and that and are everywhere smooth.","\begin{equation}
\frac{d^2y}{dx^2}=f(x)y\,,
\end{equation} y f(x) x>0 \lim_{x \to \infty}f(x) \sim 1/x^2 \lim_{x \to 0}f(x) \sim 1/x^2 f(x) y","['real-analysis', 'ordinary-differential-equations']"
61,$y^{IV} - y = 0$,,y^{IV} - y = 0,$y^{IV} - y = 0$ I solved this differential equation and got an answer : $$y = C_1e^x + C_2e^{-x} +C_3\cos x + C_4\sin x .$$ But the book gives different answer: $$y = C_1\cos2x + C_2\sin2x. $$ Could you please explain me how did they get this?,I solved this differential equation and got an answer : But the book gives different answer: Could you please explain me how did they get this?,y^{IV} - y = 0 y = C_1e^x + C_2e^{-x} +C_3\cos x + C_4\sin x . y = C_1\cos2x + C_2\sin2x. ,['ordinary-differential-equations']
62,Differential Equations µ substitution,Differential Equations µ substitution,,"My professor has removed the lecture notes which were our only resource (no textbook) for the class, so I do not have much to go on except for a class I took years ago. He never named it in class, but I remember calling the solution technique µ sub. I can't find any tutorials that I recognize as being this kind of problem or using the equation he provides. Solve: $v'=5-0.1v$ with $v(0)=44$ The hint provided is to use the equation: $y(x)=e^{-µ(x)}\int{e^{-µ(x)}q(x)dx}$","My professor has removed the lecture notes which were our only resource (no textbook) for the class, so I do not have much to go on except for a class I took years ago. He never named it in class, but I remember calling the solution technique µ sub. I can't find any tutorials that I recognize as being this kind of problem or using the equation he provides. Solve: with The hint provided is to use the equation:",v'=5-0.1v v(0)=44 y(x)=e^{-µ(x)}\int{e^{-µ(x)}q(x)dx},['ordinary-differential-equations']
63,Determine the steady state temperature distribution for the given problems,Determine the steady state temperature distribution for the given problems,,"I have the following problem, where I'm kinda lost what to do: $${y}''=-T_{0}, \quad {y}'(0)=0, \quad y(1)=0$$ how can I solve this equation that has the constant $T_{0}$ . Any kind of help will be appreciated. Thank you.","I have the following problem, where I'm kinda lost what to do: how can I solve this equation that has the constant . Any kind of help will be appreciated. Thank you.","{y}''=-T_{0}, \quad {y}'(0)=0, \quad y(1)=0 T_{0}",['ordinary-differential-equations']
64,How can I show the symmetry of an operator?,How can I show the symmetry of an operator?,,"Given the operator $L$ and $p, q, V$ smooth functions: \begin{equation} L=\frac{1}{p(x)}\left[\frac{d}{dx}\left(q(x)\frac{d}{dx}\right)+V(x)\right] \end{equation} I should show that, whenever p is not constant, the operator $L$ is not symmetric on $L^2(\mathbb{R})$ but on $L^2(\mathbb{R}; p(x)dx)$ . My problem is that I do not actually understand what does it mean to be symmetric on $L^2(\mathbb{R}; p(x)dx)$ : is $p(x)dx$ a measure? And if yes, what does it matter it with the calculation of hermiticity and consequently of symmetry of the operator? Thank you in advance for any help!","Given the operator and smooth functions: I should show that, whenever p is not constant, the operator is not symmetric on but on . My problem is that I do not actually understand what does it mean to be symmetric on : is a measure? And if yes, what does it matter it with the calculation of hermiticity and consequently of symmetry of the operator? Thank you in advance for any help!","L p, q, V \begin{equation}
L=\frac{1}{p(x)}\left[\frac{d}{dx}\left(q(x)\frac{d}{dx}\right)+V(x)\right]
\end{equation} L L^2(\mathbb{R}) L^2(\mathbb{R}; p(x)dx) L^2(\mathbb{R}; p(x)dx) p(x)dx","['functional-analysis', 'ordinary-differential-equations', 'operator-theory', 'differential-operators']"
65,Periodic solution existence using Poincaré-Bendixson,Periodic solution existence using Poincaré-Bendixson,,Given the following system: $\dot{x}=x-y-2x(x^2+y^2)$ $\dot{y}=x+y+xy-2y(x^2+y^2)$ Converting this to polar coordinates using $r\dot{r}=x\dot{x}+y\dot{y}$ and $\dot{\theta}=\frac{x\dot{y}-y\dot{x}}{r^2}$ I got: $\dot{r}=r-2r^3+r^2\cos(\theta)\sin^2(\theta)$ and $\dot{\theta}=1+r\cos^2(\theta)\sin(\theta)$ I'm trying to identify the regions where $r$ is expanding or shrinking by limiting the values of $\cos(\theta)\sin^2(\theta)$ I thought rewriting it to be $\cos(\theta)(1-\cos^2(\theta))=\cos(\theta)-\cos^3(\theta)$ would help seeing as how $-0.4\leq \cos(\theta)-\cos^3(\theta)\leq 0.4$ But I don't know how to move forward from here. Most examples lead to a much simpler reduction of $\dot{r}$ am I doing something wrong here?,Given the following system: Converting this to polar coordinates using and I got: and I'm trying to identify the regions where is expanding or shrinking by limiting the values of I thought rewriting it to be would help seeing as how But I don't know how to move forward from here. Most examples lead to a much simpler reduction of am I doing something wrong here?,\dot{x}=x-y-2x(x^2+y^2) \dot{y}=x+y+xy-2y(x^2+y^2) r\dot{r}=x\dot{x}+y\dot{y} \dot{\theta}=\frac{x\dot{y}-y\dot{x}}{r^2} \dot{r}=r-2r^3+r^2\cos(\theta)\sin^2(\theta) \dot{\theta}=1+r\cos^2(\theta)\sin(\theta) r \cos(\theta)\sin^2(\theta) \cos(\theta)(1-\cos^2(\theta))=\cos(\theta)-\cos^3(\theta) -0.4\leq \cos(\theta)-\cos^3(\theta)\leq 0.4 \dot{r},"['ordinary-differential-equations', 'dynamical-systems']"
66,Solving Cauchy problem using implicit function,Solving Cauchy problem using implicit function,,"I am trying to solve the following Cauchy problem: $$ y' = \sin(y), \quad y(0) = \frac{\pi}{3} $$ After some calculation, I get to $$ \int_{\frac{\pi}{3}}^y \frac{ds}{\sin{s}} = x $$ Actually, I just want to find the maximum interval where the solution is defined, so I study the integral function: $$ F(y) = \int_{\frac{\pi}{3}}^y \frac{ds}{\sin{s}} $$ Now I know I have to calculate some limits to find the range, but I am not sure how. Can someone give me some hints?","I am trying to solve the following Cauchy problem: After some calculation, I get to Actually, I just want to find the maximum interval where the solution is defined, so I study the integral function: Now I know I have to calculate some limits to find the range, but I am not sure how. Can someone give me some hints?"," y' = \sin(y), \quad y(0) = \frac{\pi}{3}   \int_{\frac{\pi}{3}}^y \frac{ds}{\sin{s}} = x   F(y) = \int_{\frac{\pi}{3}}^y \frac{ds}{\sin{s}} ","['integration', 'ordinary-differential-equations', 'cauchy-problem']"
67,Differential equation: $y=2xy'+\frac{1}{(y')^2}$,Differential equation:,y=2xy'+\frac{1}{(y')^2},"I've encountered the following differential equation: $y=2xy'+\frac{1}{(y')^2}$ I tried to differentiate in order to $x$ and then used $p=y'$ . After some calculation, I arrived at the formula $ x = \frac{2}{p^3} + \frac{C_{1}}{p^2}$ Does anyone has any suggestion to solve the first equation? Thanks for your attention.","I've encountered the following differential equation: I tried to differentiate in order to and then used . After some calculation, I arrived at the formula Does anyone has any suggestion to solve the first equation? Thanks for your attention.",y=2xy'+\frac{1}{(y')^2} x p=y'  x = \frac{2}{p^3} + \frac{C_{1}}{p^2},['ordinary-differential-equations']
68,Solving a first Order ordinary differential equation with only linear terms,Solving a first Order ordinary differential equation with only linear terms,,"The following problem is from the book ""Introduction to Ordinary Differential Equations"" By Sheply L. Ross. It is problem 8 in section 2.4. It can be found on page 67. I believe I have the wrong answer. Where did I go wrong? Problem: Solve the following differential equations. $$ ( 3x - y + 1 ) dx - ( 6x - 2y - 3) dy = 0 $$ Answer: I am going to use the substitution $z = 3x -y + 1$ . \begin{align*} \frac{dz}{dx} &= 3 -  \frac{dy}{dx} \\ dz &= 3 \, dx - dy \\ - dy &= dz - 3 dx \\ dy &= 3 dx - dz \\ z \, dx - ( 6x - 2y - 3) \left( 3 dx - dz \right)  = 0 \\ z \, dx - ( 6x - 2y + 2 - 5) \left( 3 dx - dz \right)  = 0 \\ z \, dx - ( 2z - 5) \left( 3 dx - dz \right)  = 0 \\ z \, dx + (-2z + 5)(3 \, dx - dz) &= 0 \\ (z - 6z + 15)\, dx + (2z-5) dz &= 0 \\ dx + \frac{2z-5 }{-5z + 15 } \, dz &= 0 \\ dx - \frac{2z-5 }{5z - 15 } \, dz &= 0  \end{align*} Now we need to perform the following integration: $$ I = \int \frac{2z-5 }{5z - 15 } \, dz $$ To perform this integration, I use the substitution $u = 5z - 15$ which gives me $du = 5 \, dz$ . \begin{align*} I &= \int \frac{ \left( 2z - 5 \right) \left( \frac{1}{5} \right) } {u} \, du \\ 5z &= u + 15 \\ z &= \frac{u+15}{5} \\ I &= \int \frac{ \left( 2 \left( \frac{u+15}{5} \right)  - 5 \right) \left( \frac{1}{5} \right) } {u} \, du \\ I &= \frac{1}{5} \int \frac{\frac{2u+15} {5} -  5} {u} \, du \\ I &= \frac{1}{5} \int \frac{ \frac{2u + 15 - 25} { 5} } { u } \, du \\ I &= \frac{1}{25} \int \frac{ 2u - 10 }{u} \, du \\ I &= \frac{2u}{25} - \frac{10}{25}  \ln{|u|} + C_1 \\ I &= \frac{1}{25} \left( 2( 5z - 15) \right) - \frac{2}{5} \ln{|5z - 15|} + C_1 \\ x &= \frac{1}{25} \left( 2( 5z - 15) \right) - \frac{2}{5} \ln{|5z - 15|} + C_1 \\ 5x &= \frac{1}{5} \left( 2( 5z - 15) \right) - 2 \ln{|5z - 15|} + C \text{ with } C = 5C_1 \\ 5x &= \frac{1}{5} \left( 2( 5(3x -y + 1) - 15) \right) - 2 \ln{|5(3x -y + 1) - 15|} + C \\ 5x &= \frac{1}{5} \left( 2( 15x -5y + 5 - 15) \right) - 2 \ln{|15x - 5y + 5 - 15|} + C \\ 5x &= \frac{1}{5} \left( 30x - 10y - 20 \right) - 2 \ln{|15x - 5y - 10|} + C \\ 5x &= \left( 6x - 2y - 4 \right) - 2 \ln{|15x - 5y - 10|} + C \end{align*} Now, I am going to check the answer by differentiating it. \begin{align*} 5 &= 6 - 2 \frac{dy}{dx} - 2 \frac{15 - 5 \frac{dy}{dx}}{15x - 5y - 10} \\ -1 &= - 2 \frac{dy}{dx} - 2 \frac{15 - 5 \frac{dy}{dx}}{15x - 5y - 10} \\ 1 &=  2 \frac{dy}{dx} + 2 \frac{15 - 5 \frac{dy}{dx}}{15x - 5y - 10} \\ 15x - 5y - 10 &=  2 (15x - 5y - 10) \frac{dy}{dx} + 2  (15 - 5 \frac{dy}{dx}) \\ 15x - 5y - 10 &=  2 (15x - 5y - 10) \frac{dy}{dx} +  30 - 10 \frac{dy}{dx} \\ 15x - 5y - 40 &=  2 (15x - 5y - 10) \frac{dy}{dx} - 10 \frac{dy}{dx} \\ 15x - 5y - 40 &= ( 30x - 10y - 20) \frac{dy}{dx} \\ \frac{dy}{dx} &= \frac{15x - 5y - 40}{30x - 10y - 20} \\ \frac{dy}{dx} &= \frac{ 3x - y + 8 } { 6x - 2y - 4 } \end{align*} Hence the answer does not check. Based upon the comment I got from N74 I have updated my answer. I feel I am closer to the correct solution but I am still off. I am going to use the substitution $z = 3x -y + 1$ . \begin{align*} \frac{dz}{dx} &= 3 -  \frac{dy}{dx} \\ dz &= 3 \, dx - dy \\ - dy &= dz - 3 dx \\ dy &= 3 dx - dz \\ z \, dx - ( 6x - 2y - 3) \left( 3 dx - dz \right)  = 0 \\ z \, dx - ( 6x - 2y + 2 - 5) \left( 3 dx - dz \right)  = 0 \\ z \, dx - ( 2z - 5) \left( 3 dx - dz \right)  = 0 \\ z \, dx + (-2z + 5)(3 \, dx - dz) &= 0 \\ (z - 6z + 15)\, dx + (2z-5) dz &= 0 \\ dx + \frac{2z-5 }{-5z + 15 } \, dz &= 0 \\ dx - \frac{2z-5 }{5z - 15 } \, dz &= 0  \end{align*} Now we need to perform the following integration: $$ I = \int \frac{2z-5 }{5z - 15 } \, dz $$ To perform this integration, I use the substitution $u = 5z - 15$ which gives me $du = 5 \, dz$ . \begin{align*} I &= \int \frac{ \left( 2z - 5 \right) \left( \frac{1}{5} \right) } {u} \, du \\ 5z &= u + 15 \\ z &= \frac{u+15}{5} \\ I &= \int \frac{ \left( 2 \left( \frac{u+15}{5} \right)  - 5 \right) \left( \frac{1}{5} \right) } {u} \, du \\ I &= \frac{1}{5} \int \frac{\frac{2u+30} {5} -  5} {u} \, du \\ I &= \frac{1}{5} \int \frac{ \frac{2u + 30 - 25} { 5} } { u } \, du \\ I &= \frac{1}{25} \int \frac{ 2u + 5 }{u} \, du \\ I &= \frac{2u}{25} + \frac{5}{25}  \ln{|u|} + C_1 \\ I &= \frac{1}{25} \left( 2( 5z - 15) \right) + \frac{1}{5} \ln{|5z - 15|} + C_1 \\ x &= \frac{1}{25} \left( 2( 5z - 15) \right) + \frac{1}{5} \ln{|5z - 15|} + C_1 \\ 5x &= \frac{1}{5} \left( 2( 5z - 15) \right) + \ln{|5z - 15|} + C \text{ with } C = 5C_1 \\ 5x &= \frac{1}{5} \left( 2( 5(3x -y + 1) - 15) \right) + \ln{|5(3x -y + 1) - 15|} + C \\ 5x &= \frac{1}{5} \left( 2( 15x -5y + 5 - 15) \right) + \ln{|15x - 5y + 5 - 15|} + C \\ 5x &= \frac{1}{5} \left( 30x - 10y - 20 \right) + \ln{|15x - 5y - 10|} + C \\ 5x &= \left( 6x - 2y - 4 \right) + \ln{|15x - 5y - 10|} + C \end{align*} Now, I am going to check the answer by differentiating it. \begin{align*} 5 &= 6 - 2 \frac{dy}{dx} + \frac{15 - 5 \frac{dy}{dx}}{15x - 5y - 10} \\ -1 &= - 2 \frac{dy}{dx} + \frac{15 - 5 \frac{dy}{dx}}{15x - 5y - 10} \\ % 1 &=  2 \frac{dy}{dx} - \frac{15 - 5 \frac{dy}{dx}}{15x - 5y - 10} \\ 15x - 5y - 10 &= 2(15x - 5y - 10) \frac{dy}{dx} - 15 - 5 \frac{dy}{dx} \\ 15x - 5y + 5 &= ( 30x - 10y - 20 - 5 ) \frac{dy}{dx} \\ \frac{dy}{dx} &= \frac{ 15x - 5y + 5 } { 30x - 10y - 25 } = \frac{ 3x - y + 1}{ 6x - 2y - 5} \\ (3x - y + 1) \, dx &= ( 6x - 2y - 5 ) \, dy \\ \end{align*} Hence the answer does not check. I now believe that my second answer is correct. The check I did above is not right. Here is my updated check of the answer I got. Now, I am going to check the answer by differentiating it. \begin{align*} 5 &= 6 - 2 \frac{dy}{dx} + \frac{15 - 5 \frac{dy}{dx}}{15x - 5y - 10} \\ -1 &= - 2 \frac{dy}{dx} + \frac{15 - 5 \frac{dy}{dx}}{15x - 5y - 10} \\ % 1 &=  2 \frac{dy}{dx} - \frac{15 - 5 \frac{dy}{dx}}{15x - 5y - 10} \\ % 15x - 5y - 10 &= 5y - 10 &= 2(15x - 5y - 10)\frac{dy}{dx} + 5 \frac{dy}{dx}- 15 \\ 15x - 5y + 5 &= (30x - 10y - 20)\frac{dy}{dx} + 5 + \frac{dy}{dx} \\ 3x - y + 1 &= (6x - 2y - 4)\frac{dy}{dx} + 5 \frac{dy}{dx} \\ 3x - y + 1 &= (6x - 2y +1 )\frac{dy}{dx} \\ (3x-y+1) dx &= (6x-2y+1) dy \end{align*} Hence the answer checks.","The following problem is from the book ""Introduction to Ordinary Differential Equations"" By Sheply L. Ross. It is problem 8 in section 2.4. It can be found on page 67. I believe I have the wrong answer. Where did I go wrong? Problem: Solve the following differential equations. Answer: I am going to use the substitution . Now we need to perform the following integration: To perform this integration, I use the substitution which gives me . Now, I am going to check the answer by differentiating it. Hence the answer does not check. Based upon the comment I got from N74 I have updated my answer. I feel I am closer to the correct solution but I am still off. I am going to use the substitution . Now we need to perform the following integration: To perform this integration, I use the substitution which gives me . Now, I am going to check the answer by differentiating it. Hence the answer does not check. I now believe that my second answer is correct. The check I did above is not right. Here is my updated check of the answer I got. Now, I am going to check the answer by differentiating it. Hence the answer checks."," ( 3x - y + 1 ) dx - ( 6x - 2y - 3) dy = 0  z = 3x -y + 1 \begin{align*}
\frac{dz}{dx} &= 3 -  \frac{dy}{dx} \\
dz &= 3 \, dx - dy \\
- dy &= dz - 3 dx \\
dy &= 3 dx - dz \\
z \, dx - ( 6x - 2y - 3) \left( 3 dx - dz \right)  = 0 \\
z \, dx - ( 6x - 2y + 2 - 5) \left( 3 dx - dz \right)  = 0 \\
z \, dx - ( 2z - 5) \left( 3 dx - dz \right)  = 0 \\
z \, dx + (-2z + 5)(3 \, dx - dz) &= 0 \\
(z - 6z + 15)\, dx + (2z-5) dz &= 0 \\
dx + \frac{2z-5 }{-5z + 15 } \, dz &= 0 \\
dx - \frac{2z-5 }{5z - 15 } \, dz &= 0 
\end{align*}  I = \int \frac{2z-5 }{5z - 15 } \, dz  u = 5z - 15 du = 5 \, dz \begin{align*}
I &= \int \frac{ \left( 2z - 5 \right) \left( \frac{1}{5} \right) } {u} \, du \\
5z &= u + 15 \\
z &= \frac{u+15}{5} \\
I &= \int \frac{ \left( 2 \left( \frac{u+15}{5} \right)  - 5 \right) \left( \frac{1}{5} \right) } {u} \, du \\
I &= \frac{1}{5} \int \frac{\frac{2u+15} {5} -  5} {u} \, du \\
I &= \frac{1}{5} \int \frac{ \frac{2u + 15 - 25} { 5} } { u } \, du \\
I &= \frac{1}{25} \int \frac{ 2u - 10 }{u} \, du \\
I &= \frac{2u}{25} - \frac{10}{25}  \ln{|u|} + C_1 \\
I &= \frac{1}{25} \left( 2( 5z - 15) \right) - \frac{2}{5} \ln{|5z - 15|} + C_1 \\
x &= \frac{1}{25} \left( 2( 5z - 15) \right) - \frac{2}{5} \ln{|5z - 15|} + C_1 \\
5x &= \frac{1}{5} \left( 2( 5z - 15) \right) - 2 \ln{|5z - 15|} + C \text{ with } C = 5C_1 \\
5x &= \frac{1}{5} \left( 2( 5(3x -y + 1) - 15) \right) - 2 \ln{|5(3x -y + 1) - 15|} + C \\
5x &= \frac{1}{5} \left( 2( 15x -5y + 5 - 15) \right) - 2 \ln{|15x - 5y + 5 - 15|} + C \\
5x &= \frac{1}{5} \left( 30x - 10y - 20 \right) - 2 \ln{|15x - 5y - 10|} + C \\
5x &= \left( 6x - 2y - 4 \right) - 2 \ln{|15x - 5y - 10|} + C
\end{align*} \begin{align*}
5 &= 6 - 2 \frac{dy}{dx} - 2 \frac{15 - 5 \frac{dy}{dx}}{15x - 5y - 10} \\
-1 &= - 2 \frac{dy}{dx} - 2 \frac{15 - 5 \frac{dy}{dx}}{15x - 5y - 10} \\
1 &=  2 \frac{dy}{dx} + 2 \frac{15 - 5 \frac{dy}{dx}}{15x - 5y - 10} \\
15x - 5y - 10 &=  2 (15x - 5y - 10) \frac{dy}{dx} + 2  (15 - 5 \frac{dy}{dx}) \\
15x - 5y - 10 &=  2 (15x - 5y - 10) \frac{dy}{dx} +  30 - 10 \frac{dy}{dx} \\
15x - 5y - 40 &=  2 (15x - 5y - 10) \frac{dy}{dx} - 10 \frac{dy}{dx} \\
15x - 5y - 40 &= ( 30x - 10y - 20) \frac{dy}{dx} \\
\frac{dy}{dx} &= \frac{15x - 5y - 40}{30x - 10y - 20} \\
\frac{dy}{dx} &= \frac{ 3x - y + 8 } { 6x - 2y - 4 }
\end{align*} z = 3x -y + 1 \begin{align*}
\frac{dz}{dx} &= 3 -  \frac{dy}{dx} \\
dz &= 3 \, dx - dy \\
- dy &= dz - 3 dx \\
dy &= 3 dx - dz \\
z \, dx - ( 6x - 2y - 3) \left( 3 dx - dz \right)  = 0 \\
z \, dx - ( 6x - 2y + 2 - 5) \left( 3 dx - dz \right)  = 0 \\
z \, dx - ( 2z - 5) \left( 3 dx - dz \right)  = 0 \\
z \, dx + (-2z + 5)(3 \, dx - dz) &= 0 \\
(z - 6z + 15)\, dx + (2z-5) dz &= 0 \\
dx + \frac{2z-5 }{-5z + 15 } \, dz &= 0 \\
dx - \frac{2z-5 }{5z - 15 } \, dz &= 0 
\end{align*}  I = \int \frac{2z-5 }{5z - 15 } \, dz  u = 5z - 15 du = 5 \, dz \begin{align*}
I &= \int \frac{ \left( 2z - 5 \right) \left( \frac{1}{5} \right) } {u} \, du \\
5z &= u + 15 \\
z &= \frac{u+15}{5} \\
I &= \int \frac{ \left( 2 \left( \frac{u+15}{5} \right)  - 5 \right) \left( \frac{1}{5} \right) } {u} \, du \\
I &= \frac{1}{5} \int \frac{\frac{2u+30} {5} -  5} {u} \, du \\
I &= \frac{1}{5} \int \frac{ \frac{2u + 30 - 25} { 5} } { u } \, du \\
I &= \frac{1}{25} \int \frac{ 2u + 5 }{u} \, du \\
I &= \frac{2u}{25} + \frac{5}{25}  \ln{|u|} + C_1 \\
I &= \frac{1}{25} \left( 2( 5z - 15) \right) + \frac{1}{5} \ln{|5z - 15|} + C_1 \\
x &= \frac{1}{25} \left( 2( 5z - 15) \right) + \frac{1}{5} \ln{|5z - 15|} + C_1 \\
5x &= \frac{1}{5} \left( 2( 5z - 15) \right) + \ln{|5z - 15|} + C \text{ with } C = 5C_1 \\
5x &= \frac{1}{5} \left( 2( 5(3x -y + 1) - 15) \right) + \ln{|5(3x -y + 1) - 15|} + C \\
5x &= \frac{1}{5} \left( 2( 15x -5y + 5 - 15) \right) + \ln{|15x - 5y + 5 - 15|} + C \\
5x &= \frac{1}{5} \left( 30x - 10y - 20 \right) + \ln{|15x - 5y - 10|} + C \\
5x &= \left( 6x - 2y - 4 \right) + \ln{|15x - 5y - 10|} + C
\end{align*} \begin{align*}
5 &= 6 - 2 \frac{dy}{dx} + \frac{15 - 5 \frac{dy}{dx}}{15x - 5y - 10} \\
-1 &= - 2 \frac{dy}{dx} + \frac{15 - 5 \frac{dy}{dx}}{15x - 5y - 10} \\
%
1 &=  2 \frac{dy}{dx} - \frac{15 - 5 \frac{dy}{dx}}{15x - 5y - 10} \\
15x - 5y - 10 &= 2(15x - 5y - 10) \frac{dy}{dx} - 15 - 5 \frac{dy}{dx} \\
15x - 5y + 5 &= ( 30x - 10y - 20 - 5 ) \frac{dy}{dx} \\
\frac{dy}{dx} &= \frac{ 15x - 5y + 5 } { 30x - 10y - 25 } = \frac{ 3x - y + 1}{ 6x - 2y - 5} \\
(3x - y + 1) \, dx &= ( 6x - 2y - 5 ) \, dy \\
\end{align*} \begin{align*}
5 &= 6 - 2 \frac{dy}{dx} + \frac{15 - 5 \frac{dy}{dx}}{15x - 5y - 10} \\
-1 &= - 2 \frac{dy}{dx} + \frac{15 - 5 \frac{dy}{dx}}{15x - 5y - 10} \\
%
1 &=  2 \frac{dy}{dx} - \frac{15 - 5 \frac{dy}{dx}}{15x - 5y - 10} \\
%
15x - 5y - 10 &= 5y - 10 &= 2(15x - 5y - 10)\frac{dy}{dx} + 5 \frac{dy}{dx}- 15 \\
15x - 5y + 5 &= (30x - 10y - 20)\frac{dy}{dx} + 5 + \frac{dy}{dx} \\
3x - y + 1 &= (6x - 2y - 4)\frac{dy}{dx} + 5 \frac{dy}{dx} \\
3x - y + 1 &= (6x - 2y +1 )\frac{dy}{dx} \\
(3x-y+1) dx &= (6x-2y+1) dy
\end{align*}",['ordinary-differential-equations']
69,Clarification about guessing a particular solution in the method of undetermined coefficients,Clarification about guessing a particular solution in the method of undetermined coefficients,,"Given the following Cauchy-Euler equation $$t^2y''-ty'-3y=4t^2+12,~t>0$$ one can first find the homogeneous roots through the characteristic equation $$r^2-2t-3=0$$ $$(r-3)(r+1)=0$$ $$r_1=3,~r_2=-1$$ to find the homogeneous solution of $$y_h(t)=c_1t^3+c_2t^{-1}$$ Next, one can find the particular solution by the method of undetermined coefficients . Since the right-hand side contains $4t^2+12$ , a perfectly reasonable guess for the particular solution is $$y_p(t)=At^2 + Bt+C \tag{1}$$ where upon differentiating twice and relating coefficients forms $$y_p(t)=-\frac{4}{3}t^2-4$$ However, one is also able to guess $$y_p(t)=At^2 + B \tag{2}$$ and arrive at the correct particular solution. One explanation for this is because the right-hand side of the equation has powers with exponents $2$ and $0$ , neither of which is a root of the characteristic polynomial. Since a multiple of $t$ is not present in the right-hand side of the equation, it isn't necessary to include a multiple of $t$ in the guess for the particular solution. Is this logic correct? Would the same logic apply to any similar second order Cauchy-Euler equation? For example, suppose the original equation was changed to $$t^2y''-ty'-3y=4t^4+4t^2+12,~t>0$$ then one could guess either $$y_p(t)=At^4 + Bt^3+Ct^2+Dt+E \tag{3}$$ or $$y_p(t)=At^4 + Bt^2+C \tag{4}$$ and still arrive at the correct particular solution of $$y_p(t)=\frac{4}{5}t^4-\frac{4}{3}t^2-4$$ Is it unnecessary to include a power of $t$ in the guess if it isn't included in the right-hand side of the original equation?","Given the following Cauchy-Euler equation one can first find the homogeneous roots through the characteristic equation to find the homogeneous solution of Next, one can find the particular solution by the method of undetermined coefficients . Since the right-hand side contains , a perfectly reasonable guess for the particular solution is where upon differentiating twice and relating coefficients forms However, one is also able to guess and arrive at the correct particular solution. One explanation for this is because the right-hand side of the equation has powers with exponents and , neither of which is a root of the characteristic polynomial. Since a multiple of is not present in the right-hand side of the equation, it isn't necessary to include a multiple of in the guess for the particular solution. Is this logic correct? Would the same logic apply to any similar second order Cauchy-Euler equation? For example, suppose the original equation was changed to then one could guess either or and still arrive at the correct particular solution of Is it unnecessary to include a power of in the guess if it isn't included in the right-hand side of the original equation?","t^2y''-ty'-3y=4t^2+12,~t>0 r^2-2t-3=0 (r-3)(r+1)=0 r_1=3,~r_2=-1 y_h(t)=c_1t^3+c_2t^{-1} 4t^2+12 y_p(t)=At^2 + Bt+C \tag{1} y_p(t)=-\frac{4}{3}t^2-4 y_p(t)=At^2 + B \tag{2} 2 0 t t t^2y''-ty'-3y=4t^4+4t^2+12,~t>0 y_p(t)=At^4 + Bt^3+Ct^2+Dt+E \tag{3} y_p(t)=At^4 + Bt^2+C \tag{4} y_p(t)=\frac{4}{5}t^4-\frac{4}{3}t^2-4 t",['ordinary-differential-equations']
70,Finding eigenfunction(s),Finding eigenfunction(s),,"I am stuck in a part of a proof perhaps somebody can help me. I've got the following eigenvalue problem: $$u'' + \lambda u = 0 \quad 0 <x < L$$ With boundary conditions: $$ -u'(0) + \sigma_1 u(0) = 0, \quad u'(L) + \sigma_2 u(L) = 0$$ Where both $ \sigma_1$ and $ \sigma_2$ are constants. By writing $\lambda = k^2$ , for real $k$ , the general solution is : $$u(x) = A \cos kx + B \sin kx$$ and imposing the boundary conditions: $u(0) = A$ , $u(L) =A \cos kL + B \sin kL$ , $u'(x)= -kA \sin kx +kB \cos kx$ Now substituting the given boundary values: $u'(0) = kB$ , and $u'(L) = -kA \sin  kL +kB  \cos kL$ Which gives: $$-kB + \sigma_1A = 0 $$ $$-kA \sin  kL +kB  \cos kL + \sigma_2(A \cos kL + B \sin kL)=0$$ The last equation can be re-arranged to: $$(-kA+\sigma_2B) \sin KL + (kB + \sigma_2A)\cos kL = 0 $$ Giving: $$\frac{(-kA+\sigma_2B)}{(kB + \sigma_2A)}\tan kL = -1$$ Now solving for $\tan kL$ gives: $$\tan kL = -\frac {(kB + \sigma_2A)}{(-kA+\sigma_2B)}$$ By using the first BC we obtain: $$\tan kL =\frac {(\sigma_1 + \sigma_2)k}{(k^2-\sigma_1 \sigma_2)}$$ But how do I get to the corresponding eigen function: $$u_n(x) = u(x;k_n) \quad u(x;k) = K(k) \sin(kx + θ(k))$$ where: $$\tan(θ(k))= \frac {k}{\sigma_1}$$ For some undetermined normalisation constant $K(k)$","I am stuck in a part of a proof perhaps somebody can help me. I've got the following eigenvalue problem: With boundary conditions: Where both and are constants. By writing , for real , the general solution is : and imposing the boundary conditions: , , Now substituting the given boundary values: , and Which gives: The last equation can be re-arranged to: Giving: Now solving for gives: By using the first BC we obtain: But how do I get to the corresponding eigen function: where: For some undetermined normalisation constant","u'' + \lambda u = 0 \quad 0 <x < L  -u'(0) + \sigma_1 u(0) = 0, \quad u'(L) + \sigma_2 u(L) = 0  \sigma_1  \sigma_2 \lambda = k^2 k u(x) = A \cos kx + B \sin kx u(0) = A u(L) =A \cos kL + B \sin kL u'(x)= -kA \sin kx +kB \cos kx u'(0) = kB u'(L) = -kA \sin  kL +kB  \cos kL -kB + \sigma_1A = 0  -kA \sin  kL +kB  \cos kL + \sigma_2(A \cos kL + B \sin kL)=0 (-kA+\sigma_2B) \sin KL + (kB + \sigma_2A)\cos kL = 0  \frac{(-kA+\sigma_2B)}{(kB + \sigma_2A)}\tan kL = -1 \tan kL \tan kL = -\frac {(kB + \sigma_2A)}{(-kA+\sigma_2B)} \tan kL =\frac {(\sigma_1 + \sigma_2)k}{(k^2-\sigma_1 \sigma_2)} u_n(x) = u(x;k_n) \quad u(x;k) = K(k) \sin(kx + θ(k)) \tan(θ(k))= \frac {k}{\sigma_1} K(k)","['ordinary-differential-equations', 'boundary-value-problem', 'eigenfunctions']"
71,solve $(D^2-2D+1)y=x\sin x$,solve,(D^2-2D+1)y=x\sin x,Find a particular solution to the equation $$(D^2-2D+1)y=x\sin x$$ \begin{align} \text{P.I.}&=\frac{1}{(D^2-2D+1)}x\sin x\\ &=\frac{1}{(D-1)^2}x\sin x\\ &=\text{I.P.}\left[\frac{1}{(D-1)^2}xe^{ix}\right]\\ &=\text{I.P.}\left[e^{ix}\frac{1}{(D+i-1)^2}x\right]\\ &=\text{I.P.}\left[e^{ix}\frac{1}{2}(1-\frac{iD^2}{2}+D+iD+\cdots)x\right]\\ &=\text{I.P.}\left[(\cos x+i\sin x)(\frac{1}{2}x+\frac{1}{2}+\frac{1}{2}i)\right]\\ &=\text{I.P.}\left[\frac{1}{2}x\cos x+\frac{1}{2}\cos x+\frac{1}{2}i\cos x+\frac{1}{2}xi\sin x+\frac{1}{2}i\sin x-\frac{1}{2}\sin x\right]\\ &=\frac{1}{2}\cos x+\frac{1}{2}x\sin x+\frac{1}{2}\sin x\\ \text{I.P.}&=\text{Imaginary Part} \end{align} But the solution provided by Mathematica is: Is my solution is wrong $?$ Or I am getting correct answer in different form $!$,Find a particular solution to the equation But the solution provided by Mathematica is: Is my solution is wrong Or I am getting correct answer in different form,"(D^2-2D+1)y=x\sin x \begin{align}
\text{P.I.}&=\frac{1}{(D^2-2D+1)}x\sin x\\
&=\frac{1}{(D-1)^2}x\sin x\\
&=\text{I.P.}\left[\frac{1}{(D-1)^2}xe^{ix}\right]\\
&=\text{I.P.}\left[e^{ix}\frac{1}{(D+i-1)^2}x\right]\\
&=\text{I.P.}\left[e^{ix}\frac{1}{2}(1-\frac{iD^2}{2}+D+iD+\cdots)x\right]\\
&=\text{I.P.}\left[(\cos x+i\sin x)(\frac{1}{2}x+\frac{1}{2}+\frac{1}{2}i)\right]\\
&=\text{I.P.}\left[\frac{1}{2}x\cos x+\frac{1}{2}\cos x+\frac{1}{2}i\cos x+\frac{1}{2}xi\sin x+\frac{1}{2}i\sin x-\frac{1}{2}\sin x\right]\\
&=\frac{1}{2}\cos x+\frac{1}{2}x\sin x+\frac{1}{2}\sin x\\
\text{I.P.}&=\text{Imaginary Part}
\end{align} ? !",['ordinary-differential-equations']
72,Solving the ODE $3 x y(x) \cos(x)+ 3 y(x) \sin(x) = \sqrt{9x^2 \sin(x)^2 + 1} y'(x)$,Solving the ODE,3 x y(x) \cos(x)+ 3 y(x) \sin(x) = \sqrt{9x^2 \sin(x)^2 + 1} y'(x),"Can someone please help me to solve the following differential equation: $$3 x y(x) \cos(x)+ 3 y(x) \sin(x) = \sqrt{9x^2 \sin(x)^2 + 1} y'(x)$$ I tried using Variation of Parameters, Laplace transform and also undetermined coefficients, but nothing works. Also I was not able to solve it with Mathematica.","Can someone please help me to solve the following differential equation: I tried using Variation of Parameters, Laplace transform and also undetermined coefficients, but nothing works. Also I was not able to solve it with Mathematica.",3 x y(x) \cos(x)+ 3 y(x) \sin(x) = \sqrt{9x^2 \sin(x)^2 + 1} y'(x),['ordinary-differential-equations']
73,Is it possible to transform a cubic ODE to a set of quadratic ODEs?,Is it possible to transform a cubic ODE to a set of quadratic ODEs?,,"I am wondering if it is possible (or why it's not possible) to transform a set of cubic ODEs into a larger set of quadratic ODEs. Take as a simple example the ODE $$\dot{x}_1 = x_1^3$$ then can one rewrite this ODE in the form \begin{align} \dot{x}_1 =& f_1(\mathbf{x})\\ \vdots\\ \dot{x}_n =& f_n(\mathbf{x})\\ \end{align} where each $f_i(\mathbf{x})$ is, at most, quadratic in the variables $\mathbf{x} = (x_1,...,x_k)$ ? A more general question is then: is it possible (and does there exist a general method) to transform a cubic ODE in $n$ dimensions to a quadratic ODE in $m>n$ dimensions?","I am wondering if it is possible (or why it's not possible) to transform a set of cubic ODEs into a larger set of quadratic ODEs. Take as a simple example the ODE then can one rewrite this ODE in the form where each is, at most, quadratic in the variables ? A more general question is then: is it possible (and does there exist a general method) to transform a cubic ODE in dimensions to a quadratic ODE in dimensions?","\dot{x}_1 = x_1^3 \begin{align}
\dot{x}_1 =& f_1(\mathbf{x})\\
\vdots\\
\dot{x}_n =& f_n(\mathbf{x})\\
\end{align} f_i(\mathbf{x}) \mathbf{x} = (x_1,...,x_k) n m>n","['ordinary-differential-equations', 'polynomials']"
74,Solution of differential equation as x tends to infinity,Solution of differential equation as x tends to infinity,,I have been asked to show that each solution of the following equation remains bounded as x tends to infinity. $d^2y/dx^2 + e^xy=0$ Any hints?,I have been asked to show that each solution of the following equation remains bounded as x tends to infinity. Any hints?,d^2y/dx^2 + e^xy=0,['ordinary-differential-equations']
75,Method to solve long differential equation,Method to solve long differential equation,,$$(D^3-2D^2-5D+6)y=2e^{x}+4e^{3x}+7e^{-2x}+8e^{2x}+15$$ The complementary solution is : $ae^{x}+be^{3x}+ce^{-2x}$ . For the particular solution I get the form : $d_1+d_2xe^{x}+d_3xe^{-2x}+d_4e^{2x}+d_5xe^{3x}$ Proceeding after this becomes difficult as too many terms are getting involved. Is there a shorter method to solve such questions? I don't have any knowledge except trial solution method.,The complementary solution is : . For the particular solution I get the form : Proceeding after this becomes difficult as too many terms are getting involved. Is there a shorter method to solve such questions? I don't have any knowledge except trial solution method.,(D^3-2D^2-5D+6)y=2e^{x}+4e^{3x}+7e^{-2x}+8e^{2x}+15 ae^{x}+be^{3x}+ce^{-2x} d_1+d_2xe^{x}+d_3xe^{-2x}+d_4e^{2x}+d_5xe^{3x},['ordinary-differential-equations']
76,System of three non-linear differential equations,System of three non-linear differential equations,,"I've been interested in a problem that involves a fairly simple looking set of differential equations: $$ \frac{dx}{dt}=yz\;\;\;\frac{dy}{dt}=xz\;\;\;\frac{dz}{dt}=xy $$ From this it follows that $\frac{dx}{dy}=\frac{y}{x}$ , $\frac{dy}{dz}=\frac{z}{y}$ , etc. Besides the trivial solutions of $x=0, y=0, z=0$ , and where one of $x, y$ or $z$ equals some constant $k$ and the rest equal 0, I'm not sure how to continue with this.","I've been interested in a problem that involves a fairly simple looking set of differential equations: From this it follows that , , etc. Besides the trivial solutions of , and where one of or equals some constant and the rest equal 0, I'm not sure how to continue with this.","
\frac{dx}{dt}=yz\;\;\;\frac{dy}{dt}=xz\;\;\;\frac{dz}{dt}=xy
 \frac{dx}{dy}=\frac{y}{x} \frac{dy}{dz}=\frac{z}{y} x=0, y=0, z=0 x, y z k",['ordinary-differential-equations']
77,How to solve this simple nonlinear ODE using the Galerkin's Method,How to solve this simple nonlinear ODE using the Galerkin's Method,,"I'm trying to solve a more complicated differential equation using the Galerkin's Method, but before that, I'm trying to understand how I would solve this simpler one: $$ \cfrac{d^2u}{dx^2} + u^2 = 1;\quad\text{where} \;u(-1) = u(1) = 0.$$ I have to use the basis functions $\;\phi_j(x) = \sin(j\pi x)\;$ . Therefore, an approximation to the solution has the form: $$ u(x) \approx \sum_{j=1}^{N} c_j \phi_j(x)$$ I notice that this choice of basis functions automatically satisfies the boundary conditions. If I plug this approximation into the equation, I get: $$ \sum_{j=1}^{N} c_j \phi_j''(x) + \left(\sum_{j=1}^{N} c_j \phi_j(x)\right)^{\!2} = 1.$$ So the residual can be defined as: $$ r(x) = \sum_{j=1}^{N} c_j \phi_j''(x) + \left(\sum_{j=1}^{N} c_j \phi_j(x)\right)^{\!2} - 1$$ and the Galerkin's Method imposes that ${\displaystyle \int_{-1}^{1}} r(x) \cdot \phi_i(x)\,dx \:=\:0,\;$ for $i = 1,2,\dots,N$ . When I substitute the residual in the integral above, however, I face the nonlinear term $$ \int_{-1}^{1} \left(\sum_{j=1}^{N} c_j \phi_j(x)\right)^{\!2} \cdot \phi_i(x)\,dx,$$ which evaluates to zero for any $i$ and $j$ . So this means that the nonlinear term $u^2$ at the original equation doesn't make any difference at all? I'm pretty sure to be missing something here...","I'm trying to solve a more complicated differential equation using the Galerkin's Method, but before that, I'm trying to understand how I would solve this simpler one: I have to use the basis functions . Therefore, an approximation to the solution has the form: I notice that this choice of basis functions automatically satisfies the boundary conditions. If I plug this approximation into the equation, I get: So the residual can be defined as: and the Galerkin's Method imposes that for . When I substitute the residual in the integral above, however, I face the nonlinear term which evaluates to zero for any and . So this means that the nonlinear term at the original equation doesn't make any difference at all? I'm pretty sure to be missing something here..."," \cfrac{d^2u}{dx^2} + u^2 = 1;\quad\text{where} \;u(-1) = u(1) = 0. \;\phi_j(x) = \sin(j\pi x)\;  u(x) \approx \sum_{j=1}^{N} c_j \phi_j(x)  \sum_{j=1}^{N} c_j \phi_j''(x) + \left(\sum_{j=1}^{N} c_j \phi_j(x)\right)^{\!2} = 1.  r(x) = \sum_{j=1}^{N} c_j \phi_j''(x) + \left(\sum_{j=1}^{N} c_j \phi_j(x)\right)^{\!2} - 1 {\displaystyle \int_{-1}^{1}} r(x) \cdot \phi_i(x)\,dx \:=\:0,\; i = 1,2,\dots,N  \int_{-1}^{1} \left(\sum_{j=1}^{N} c_j \phi_j(x)\right)^{\!2} \cdot \phi_i(x)\,dx, i j u^2","['ordinary-differential-equations', 'galerkin-methods']"
78,Are these two functions describing spring motion in the same way?,Are these two functions describing spring motion in the same way?,,"Suppose I have a differential equation in the form $m\ddot{x} + kx = 0$ then the solution is in the form $x(t)=C_1cos\left(\sqrt{\dfrac{k}{m}}t\right) + C_2sin\left(\sqrt{\dfrac{k}{m}}t\right)$ Prior to learning differential equations, when  I did physics I learned that the equation of motion of the spring was $x(t) = Acos\left(\omega t + \phi\right)$ . Both of these functions seem to be able to handle the same set of functions. The first equation seems to use the fact that it has an additional term to shift the function and avoid the need of $\phi$ . Can it be shown that they are equivalent solutions or do they differ in some way?","Suppose I have a differential equation in the form then the solution is in the form Prior to learning differential equations, when  I did physics I learned that the equation of motion of the spring was . Both of these functions seem to be able to handle the same set of functions. The first equation seems to use the fact that it has an additional term to shift the function and avoid the need of . Can it be shown that they are equivalent solutions or do they differ in some way?",m\ddot{x} + kx = 0 x(t)=C_1cos\left(\sqrt{\dfrac{k}{m}}t\right) + C_2sin\left(\sqrt{\dfrac{k}{m}}t\right) x(t) = Acos\left(\omega t + \phi\right) \phi,['ordinary-differential-equations']
79,Solving the differential equation $\frac{dy}{dt}=R-ry^2$,Solving the differential equation,\frac{dy}{dt}=R-ry^2,"I'm working on a problem for my optics class regarding electron-hole recombination in a semiconductor under strong injection. Assuming I haven't done anything wrong, I've arrived at the following equation: $$\frac{dy}{dt}=R-ry^2$$ where $R$ and $r$ are different variables and $y$ stands in for the change in carriers, a function of $t$ . The problem asks for an analytical solution to this equation, but I'm incredibly rusty with diff eqs. Does this have an analytical solution? I don't readily see how it could be separable or anything else. I've looked for similar forms on a few websites, but none of them seem applicable. For example, here ( http://web.uvic.ca/~kumara/econ501/schap22.pdf ), on page 15, it provides a solution for (nonautonomous, separable) equations of the form $$\frac{dy}{dt} = f(y,t),$$ but even though the right side is a function of $y$ and $t$ (constant with $t$ ), I can't seem to separate it. So, I'm thinking that isn't right. I think arriving at this equation might belong on a different forum (physics or somewhere), but I'm more interested in the math for this. Would anyone happen to know if this even has an analytical solution? If so, how does one arrive there? The problem is trying to prove a power-law behavior instead of exponential behavior, if that gives any hint. Thanks for the help","I'm working on a problem for my optics class regarding electron-hole recombination in a semiconductor under strong injection. Assuming I haven't done anything wrong, I've arrived at the following equation: where and are different variables and stands in for the change in carriers, a function of . The problem asks for an analytical solution to this equation, but I'm incredibly rusty with diff eqs. Does this have an analytical solution? I don't readily see how it could be separable or anything else. I've looked for similar forms on a few websites, but none of them seem applicable. For example, here ( http://web.uvic.ca/~kumara/econ501/schap22.pdf ), on page 15, it provides a solution for (nonautonomous, separable) equations of the form but even though the right side is a function of and (constant with ), I can't seem to separate it. So, I'm thinking that isn't right. I think arriving at this equation might belong on a different forum (physics or somewhere), but I'm more interested in the math for this. Would anyone happen to know if this even has an analytical solution? If so, how does one arrive there? The problem is trying to prove a power-law behavior instead of exponential behavior, if that gives any hint. Thanks for the help","\frac{dy}{dt}=R-ry^2 R r y t \frac{dy}{dt} = f(y,t), y t t",['ordinary-differential-equations']
80,Utility of Variation of Parameters Over Reduction of Order,Utility of Variation of Parameters Over Reduction of Order,,"Why is variation of parameters ever useful relative to reduction of order? Reduction of order can solve any linear ODE given a single, particular solution to the associated homogeneous ODE.  Variation of parameters can solve any linear ODE given the general solution to the associated homogeneous ODE.  It should always be at least as difficult, and usually more difficult, to find a general homogeneous solution than to find a particular homogeneous solution.  This sounds like a higher barrier to entry with no extra reward. Why use variation of parameters to solve second or higher order linear ODEs at all?  Can it solve any ODEs that reduction of order can't?","Why is variation of parameters ever useful relative to reduction of order? Reduction of order can solve any linear ODE given a single, particular solution to the associated homogeneous ODE.  Variation of parameters can solve any linear ODE given the general solution to the associated homogeneous ODE.  It should always be at least as difficult, and usually more difficult, to find a general homogeneous solution than to find a particular homogeneous solution.  This sounds like a higher barrier to entry with no extra reward. Why use variation of parameters to solve second or higher order linear ODEs at all?  Can it solve any ODEs that reduction of order can't?",,"['ordinary-differential-equations', 'algorithms', 'reduction-of-order-ode']"
81,Proof of conjecture concerning numerically satisfactory pairs of solutions to generalized Airy equation?,Proof of conjecture concerning numerically satisfactory pairs of solutions to generalized Airy equation?,,"Question: Given a generalized Airy equation of the form $$ \frac{d^2y}{dx^2}-x^ny=0 $$ I conjecture that the following functions form a pair of linearly independent, numerically satisfactory solutions for $n\in \{0,1,2,3,4,...\}$ . How is this proven? $$ A_n(x)=\frac{_0F_1(;\frac{n+1}{n+2};\frac{x^{n+2}}{(n+2)^2})}{(n+2)^{\frac{n+1}{n+2}} \ \ \Gamma(\frac{n+1}{n+2})}-x\frac{_0F_1(;\frac{n+3}{n+2};\frac{x^{n+2}}{(n+2)^2})}{(n+2)^{\frac{n+3}{n+2}-1} \ \ \Gamma(\frac{n+3}{n+2}-1)} \\  $$ $$ B_n(x)=\frac{_0F_1(;\frac{n+1}{n+2};\frac{x^{n+2}}{(n+2)^2})}{(n+2)^{\frac{n+1}{n+2}-\frac{1}{2}} \ \ \Gamma(\frac{n+1}{n+2})}+x\frac{_0F_1(;\frac{n+3}{n+2};\frac{x^{n+2}}{(n+2)^2})}{(n+2)^{\frac{n+3}{n+2}-1-\frac{1}{2}} \ \ \Gamma(\frac{n+3}{n+2}-1)} $$ Context: First, I already know that these functions are solutions because the confluent hypergeometric limit functions within them are simply the series solutions obtained through a straightforward solution attempt with a power series method . Second, I suspect that they are all linearly independent because these linear combinations appear to have a Wronskian given by $$ W\{A_n(x),B_n(x)\}=\frac{2\sin{(\frac{\pi}{n+2}})}{\pi\sqrt{n+2}} $$ I have verified this Wronskian up to $n=8$ in Mathematica, but I'm not sure how to prove it. Furthermore, just for clarity, it's worth noting that these solutions reduce for the $n=0$ and $n=1$ cases to $$ A_0(x)=\frac{1}{\sqrt{2\pi}}e^{-x} \\  B_0(x)=\frac{1}{\sqrt{\pi}}e^x \\ A_1(x)=\text{Ai}(x) \\ B_1(x)=\text{Bi}(x) \\ $$ I have tested all of the pairs of solutions possible up to the limits of the numerical precision of Mathematica and found that $A_n(x)$ always converges to $0$ as $x\rightarrow\infty$ while $B_n(x)$ always diverges in the same limit. Likewise, for odd $n$ both solutions appear to oscillate exactly $\frac{\pi}{2}$ out of phase with one another as $x\rightarrow-\infty$ . These are (basically) the conditions set forward by J. C. P. Miller in the linked article for numerical satisfaction. However, I discovered these specific linear combinations through inspection. I have no idea why these constants are the ones that work. If you alter the constants even slightly, the solutions diverge wildly and no longer satisfy any of Miller's criteria. How could I have deduced the form of these solutions more systematically? Is there a proof to demonstrate that these specific solutions are the preferred ones? I know that there is a connection to the modified Bessel functions , but I don't really see it. There is clearly a hidden structure here that I'd like to understand and (hopefully) generalize to other, similar ODE families.","Question: Given a generalized Airy equation of the form I conjecture that the following functions form a pair of linearly independent, numerically satisfactory solutions for . How is this proven? Context: First, I already know that these functions are solutions because the confluent hypergeometric limit functions within them are simply the series solutions obtained through a straightforward solution attempt with a power series method . Second, I suspect that they are all linearly independent because these linear combinations appear to have a Wronskian given by I have verified this Wronskian up to in Mathematica, but I'm not sure how to prove it. Furthermore, just for clarity, it's worth noting that these solutions reduce for the and cases to I have tested all of the pairs of solutions possible up to the limits of the numerical precision of Mathematica and found that always converges to as while always diverges in the same limit. Likewise, for odd both solutions appear to oscillate exactly out of phase with one another as . These are (basically) the conditions set forward by J. C. P. Miller in the linked article for numerical satisfaction. However, I discovered these specific linear combinations through inspection. I have no idea why these constants are the ones that work. If you alter the constants even slightly, the solutions diverge wildly and no longer satisfy any of Miller's criteria. How could I have deduced the form of these solutions more systematically? Is there a proof to demonstrate that these specific solutions are the preferred ones? I know that there is a connection to the modified Bessel functions , but I don't really see it. There is clearly a hidden structure here that I'd like to understand and (hopefully) generalize to other, similar ODE families.","
\frac{d^2y}{dx^2}-x^ny=0
 n\in \{0,1,2,3,4,...\} 
A_n(x)=\frac{_0F_1(;\frac{n+1}{n+2};\frac{x^{n+2}}{(n+2)^2})}{(n+2)^{\frac{n+1}{n+2}} \ \ \Gamma(\frac{n+1}{n+2})}-x\frac{_0F_1(;\frac{n+3}{n+2};\frac{x^{n+2}}{(n+2)^2})}{(n+2)^{\frac{n+3}{n+2}-1} \ \ \Gamma(\frac{n+3}{n+2}-1)} \\ 
 
B_n(x)=\frac{_0F_1(;\frac{n+1}{n+2};\frac{x^{n+2}}{(n+2)^2})}{(n+2)^{\frac{n+1}{n+2}-\frac{1}{2}} \ \ \Gamma(\frac{n+1}{n+2})}+x\frac{_0F_1(;\frac{n+3}{n+2};\frac{x^{n+2}}{(n+2)^2})}{(n+2)^{\frac{n+3}{n+2}-1-\frac{1}{2}} \ \ \Gamma(\frac{n+3}{n+2}-1)}
 
W\{A_n(x),B_n(x)\}=\frac{2\sin{(\frac{\pi}{n+2}})}{\pi\sqrt{n+2}}
 n=8 n=0 n=1 
A_0(x)=\frac{1}{\sqrt{2\pi}}e^{-x} \\ 
B_0(x)=\frac{1}{\sqrt{\pi}}e^x \\
A_1(x)=\text{Ai}(x) \\
B_1(x)=\text{Bi}(x) \\
 A_n(x) 0 x\rightarrow\infty B_n(x) n \frac{\pi}{2} x\rightarrow-\infty","['ordinary-differential-equations', 'proof-writing', 'numerical-methods', 'hypergeometric-function', 'airy-functions']"
82,"Lorenz system for $s=10$, $r=28$ and $b=8/3$ has unstable critical points, but they never go out of the butterfly shape","Lorenz system for ,  and  has unstable critical points, but they never go out of the butterfly shape",s=10 r=28 b=8/3,"I'm studying the Lorenz dynamical system, and I'm asking myself if the critical points are unstable critical points. Considering the theory they are unstable - one eigenvalue $\in \mathbb{R}$ which is negative and 2 complex eigenvalues with a negative real part. But when I look at the critical points the results seem to oscillate around the critical value and never go to $\infty$ . It appears to me it's not an unstable critical point then? Can someone clarify which type of critical points these are?","I'm studying the Lorenz dynamical system, and I'm asking myself if the critical points are unstable critical points. Considering the theory they are unstable - one eigenvalue which is negative and 2 complex eigenvalues with a negative real part. But when I look at the critical points the results seem to oscillate around the critical value and never go to . It appears to me it's not an unstable critical point then? Can someone clarify which type of critical points these are?",\in \mathbb{R} \infty,"['ordinary-differential-equations', 'systems-of-equations']"
83,Complete Butcher Array,Complete Butcher Array,,"I've been given a partially complete Butcher Array, but I seem to be missing some other condition to find out the rest of the values. This is the Butcher Array I've been given: Using the conditions about row sums and consistency I get: $ c_1 = a_{11} + a_{12} => a_{12} = -1/4$ ; $ c_2 = a_{21} + a_{22} => c_2 - a_{21} = 5/12$ ; $b_1 + b_2 = 1 => b_1 = 1/4$ So I have to be missing some condition for $c_2$ and $a_{21}$ .","I've been given a partially complete Butcher Array, but I seem to be missing some other condition to find out the rest of the values. This is the Butcher Array I've been given: Using the conditions about row sums and consistency I get: ; ; So I have to be missing some condition for and .", c_1 = a_{11} + a_{12} => a_{12} = -1/4  c_2 = a_{21} + a_{22} => c_2 - a_{21} = 5/12 b_1 + b_2 = 1 => b_1 = 1/4 c_2 a_{21},"['ordinary-differential-equations', 'numerical-methods', 'runge-kutta-methods']"
84,Solving a system of ordinary differential equations with complex roots,Solving a system of ordinary differential equations with complex roots,,"I need help solving the differential equation $$x' = \left(\begin{matrix}  0 & -1 \\ 1 & 0  \end{matrix}\right)x$$ with initial state $x(0) = \left(\begin{matrix} z \\ 0 \end{matrix}\right)$ . Here, $x' = \frac{dx}{ds}$ . The teacher got the following solution $$x = \left(\begin{matrix} \cos(s) && -\sin(s) \\ \sin(s) && \cos(s) \end{matrix}\right)\left(\begin{matrix} z \\ 0 \end{matrix}\right)$$ But I have no idea how he got this. Could somebody please explain it to me?","I need help solving the differential equation with initial state . Here, . The teacher got the following solution But I have no idea how he got this. Could somebody please explain it to me?","x' = \left(\begin{matrix} 
0 & -1 \\
1 & 0 
\end{matrix}\right)x x(0) = \left(\begin{matrix} z \\ 0 \end{matrix}\right) x' = \frac{dx}{ds} x = \left(\begin{matrix} \cos(s) && -\sin(s) \\ \sin(s) && \cos(s) \end{matrix}\right)\left(\begin{matrix} z \\ 0 \end{matrix}\right)","['linear-algebra', 'matrices', 'ordinary-differential-equations']"
85,Find optimal fishing plan,Find optimal fishing plan,,"A population of $N$ fish in a certain lake grow at rate $$N'(t) = aN(t) - bN^2(t)$$ if undisturbed by people. Fish can be withdrawn from the lake and consumed at   rate $c(t)$ , yielding utility $w(c(t))$ to the consuming community and reducing the   fish growth rate accordingly: $$N'(t) = aN(t) - bN^2(t) - c(t)$$ Assume future utilities to the community are discounted at constant rate $r$ .   Characterize a fishing (consumption) plan to maximize the present value of the   discounted stream of utilities. Assume that $N(0) = \frac{a}{b}$ , and that $u' > 0$ , $u'' < 0$ .","A population of fish in a certain lake grow at rate if undisturbed by people. Fish can be withdrawn from the lake and consumed at   rate , yielding utility to the consuming community and reducing the   fish growth rate accordingly: Assume future utilities to the community are discounted at constant rate .   Characterize a fishing (consumption) plan to maximize the present value of the   discounted stream of utilities. Assume that , and that , .",N N'(t) = aN(t) - bN^2(t) c(t) w(c(t)) N'(t) = aN(t) - bN^2(t) - c(t) r N(0) = \frac{a}{b} u' > 0 u'' < 0,"['ordinary-differential-equations', 'control-theory', 'optimal-control', 'operations-research']"
86,Proving the solution of a Cauchy problem is continuous,Proving the solution of a Cauchy problem is continuous,,"Determine for $I_{(t_0,x_0)}$ the maximal domain that defines the solution $\phi_{(t_0,x_0)}(.)$ for the Cauchy problem: $$ \begin{cases} \dot{x}=f(x)g(t)\\ x(t_0)=x_0 \end{cases} $$ where $f$ is continuous and non-zero in the interval $[a_1,a_2]$ and $g$ is contious in the interval $(t_1,t_2)$ . Show that the set $\mathscr{D}=\{(t,t_0,x_0):t_0,x_0\in(t_1,t_2)\times(a_1,a_2)\}$ and $t\in I(t_0,x_0)$ is open and the function $\varphi:D\to\mathbb{R}$ given by $\varphi_{(t,t_0,x_0)}=\varphi_{(t_0,x_0)}(t)$ is continuous on $D$ . I have no idea on how to solve this exercise. I thought of using the Picard theorem: admitting the problem has a solution then $f$ would be Lipschitz contious which would imply $\varphi$ to be continuous. But I am not sure I could use the reverse implicarion of the theorem is valid. Question : How should I solve this problem? Thanks in advance!",Determine for the maximal domain that defines the solution for the Cauchy problem: where is continuous and non-zero in the interval and is contious in the interval . Show that the set and is open and the function given by is continuous on . I have no idea on how to solve this exercise. I thought of using the Picard theorem: admitting the problem has a solution then would be Lipschitz contious which would imply to be continuous. But I am not sure I could use the reverse implicarion of the theorem is valid. Question : How should I solve this problem? Thanks in advance!,"I_{(t_0,x_0)} \phi_{(t_0,x_0)}(.) 
\begin{cases}
\dot{x}=f(x)g(t)\\
x(t_0)=x_0
\end{cases}
 f [a_1,a_2] g (t_1,t_2) \mathscr{D}=\{(t,t_0,x_0):t_0,x_0\in(t_1,t_2)\times(a_1,a_2)\} t\in I(t_0,x_0) \varphi:D\to\mathbb{R} \varphi_{(t,t_0,x_0)}=\varphi_{(t_0,x_0)}(t) D f \varphi","['ordinary-differential-equations', 'cauchy-problem']"
87,"Defining a ""good"" periodic function","Defining a ""good"" periodic function",,"Suppose I have a function $f$ . Suppose there is some $a,b \in \mathbb{R}$ so that $f(a)=f(b)$ . What are the conditions on $f$ so I know that I can ""loop"" $f$ to get a smooth periodic function? I guess I would have to look at the higher order taylor coefficents on one side, right? Like, if I looked at $\frac{df}{dt}$ as $t$ approached $b$ and $\frac{df}{dt}$ as $t$ approached $a$ and you did this for all $\frac{d^nf}{dt^n}$ then you would get a good function of this nature. Is this a good way to think about the problem?","Suppose I have a function . Suppose there is some so that . What are the conditions on so I know that I can ""loop"" to get a smooth periodic function? I guess I would have to look at the higher order taylor coefficents on one side, right? Like, if I looked at as approached and as approached and you did this for all then you would get a good function of this nature. Is this a good way to think about the problem?","f a,b \in \mathbb{R} f(a)=f(b) f f \frac{df}{dt} t b \frac{df}{dt} t a \frac{d^nf}{dt^n}","['ordinary-differential-equations', 'derivatives']"
88,Find the singular points of the differential equation $x^3(x - 1)y'' - 2(x - 1)y' + 3xy = 0$.,Find the singular points of the differential equation .,x^3(x - 1)y'' - 2(x - 1)y' + 3xy = 0,"Consider the second order linear homogeneous equation $$a_0(x)y'' + a_1(x)y'+ a_2(x)y = 0, x \in I \tag{1}$$ Suppose that $a_0$ , $a_1$ and $a_2$ are analytic at $x_0 \in I$ . If $a_0(x_0) = 0$ , then $x_0$ is a singular point for $(1)$ . Definition :  A point $x_0 \in I$ is a regular singular point for $(1)$ if $(1)$ can be written as $$b_0(x)(x − x_0)^2y''+ b_1(x)(x − x_0)y'+b_2(x)y = 0, \tag{2}$$ where $b_0(x_0) \neq 0$ and $b_0$ , $b_1$ , $b_2$ are analytic at $x_0$ . The question is: Find the singular points of the differential equation $x^3(x - 1)y'' - 2(x - 1)y' + 3xy = 0$ and state whether they are regular singular points or irregular singular points. I think, $x = 0$ , irregular singular point, $x = 1$ , regular singular point. But, How can I prove this? Please proper guide me.","Consider the second order linear homogeneous equation Suppose that , and are analytic at . If , then is a singular point for . Definition :  A point is a regular singular point for if can be written as where and , , are analytic at . The question is: Find the singular points of the differential equation and state whether they are regular singular points or irregular singular points. I think, , irregular singular point, , regular singular point. But, How can I prove this? Please proper guide me.","a_0(x)y'' + a_1(x)y'+ a_2(x)y = 0, x \in I \tag{1} a_0 a_1 a_2 x_0 \in I a_0(x_0) = 0 x_0 (1) x_0 \in I (1) (1) b_0(x)(x − x_0)^2y''+ b_1(x)(x − x_0)y'+b_2(x)y = 0, \tag{2} b_0(x_0) \neq 0 b_0 b_1 b_2 x_0 x^3(x - 1)y'' - 2(x - 1)y' + 3xy = 0 x = 0 x = 1",[]
89,Real eigensolutions to the diffusion equation.,Real eigensolutions to the diffusion equation.,,"How can I find the real eigensolutions to the diﬀusion equation $u_t$ = $\left(x^2 u_x\right)_x,$ modeling diffusion in an inhomogeneous medium on the half-line $x>0?$ And which solutions satisfy the Dirichlet boundary conditions $u(t,1)=u(t,2)=0?$",How can I find the real eigensolutions to the diﬀusion equation = modeling diffusion in an inhomogeneous medium on the half-line And which solutions satisfy the Dirichlet boundary conditions,"u_t \left(x^2 u_x\right)_x, x>0? u(t,1)=u(t,2)=0?","['ordinary-differential-equations', 'boundary-value-problem']"
90,"Exact solution to a non-linear differential equation needed, if possible [closed]","Exact solution to a non-linear differential equation needed, if possible [closed]",,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question I'd like to know how to solve the ODE $$\ddot{x} = \frac{x}{1 + ax^2}.$$ This equation represents the basic mechanism involved during the mass acceleration of an object by a Flywheel. Because the mass accelerates on the radial track, that crosses the Flywheel, the transfer of momentum from the Flywheel to the accelerating mass, produces a decrease in the angular velocity of the Flywheel. The solution to this equation would allow the tracking of the angular velocity change of the Flywheel over time. To simplify the presentation, the radial reference $r$ has been replaced by "" $x$ "" and "" $a$ "" represents a constant. The derivative of $x$ is in terms of time "" $t$ "".","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question I'd like to know how to solve the ODE This equation represents the basic mechanism involved during the mass acceleration of an object by a Flywheel. Because the mass accelerates on the radial track, that crosses the Flywheel, the transfer of momentum from the Flywheel to the accelerating mass, produces a decrease in the angular velocity of the Flywheel. The solution to this equation would allow the tracking of the angular velocity change of the Flywheel over time. To simplify the presentation, the radial reference has been replaced by "" "" and "" "" represents a constant. The derivative of is in terms of time "" "".",\ddot{x} = \frac{x}{1 + ax^2}. r x a x t,['ordinary-differential-equations']
91,Prove that the Bessel function $J_n(x)$ satisfied $\int x J_0^2(x) dx = \frac{x^2}{2}[J_0^2(x)+J_1^2(x)]$,Prove that the Bessel function  satisfied,J_n(x) \int x J_0^2(x) dx = \frac{x^2}{2}[J_0^2(x)+J_1^2(x)],Prove that the Bessel function $J_n(x)$ satisfied $\int x J_0^2(x) dx = \frac{x^2}{2}[J_0^2(x)+J_1^2(x)]$ I really don't know where to even begin. Any hints or ideas is greatly appreciated. Thank you!,Prove that the Bessel function satisfied I really don't know where to even begin. Any hints or ideas is greatly appreciated. Thank you!,J_n(x) \int x J_0^2(x) dx = \frac{x^2}{2}[J_0^2(x)+J_1^2(x)],"['ordinary-differential-equations', 'bessel-functions']"
92,Logistic function: where does it come from?,Logistic function: where does it come from?,,"I read the book titled ""Seventeen Equations that Changed the World"" where it explains how the equation [A] $x_{t+1}=k \ x_t \cdot (1-x_t)$ where $x_t$ is the population of a certain species at generation $t$ ; while $x_{t+1}$ is the population of a certain species at the next generation. invented by Robert May , was the basis for the chaos theory . This is better known as logistic function [1]. Now, as Wikipedia[1] and other sources state, logistic function is described with another equation, that is [B] $f(x) = {{L} \over {1+e^{-k(x-x_0)}}}$ a completely different form. The starting issue was: how equation [A] is the same to [B] ? Then, I googled and found an explaination on Quora[2] that is clear: [B] comes from a differential version of [A] , that is [C] $y'=k \ y \cdot (L-y)$ Long story short, now the actual question is: how derive [C] from [A] ? (or vice versa) under which conditions? [1] https://en.wikipedia.org/wiki/Logistic_function [2] https://www.quora.com/How-is-the-logistic-function-derived","I read the book titled ""Seventeen Equations that Changed the World"" where it explains how the equation [A] where is the population of a certain species at generation ; while is the population of a certain species at the next generation. invented by Robert May , was the basis for the chaos theory . This is better known as logistic function [1]. Now, as Wikipedia[1] and other sources state, logistic function is described with another equation, that is [B] a completely different form. The starting issue was: how equation [A] is the same to [B] ? Then, I googled and found an explaination on Quora[2] that is clear: [B] comes from a differential version of [A] , that is [C] Long story short, now the actual question is: how derive [C] from [A] ? (or vice versa) under which conditions? [1] https://en.wikipedia.org/wiki/Logistic_function [2] https://www.quora.com/How-is-the-logistic-function-derived",x_{t+1}=k \ x_t \cdot (1-x_t) x_t t x_{t+1} f(x) = {{L} \over {1+e^{-k(x-x_0)}}} y'=k \ y \cdot (L-y),"['ordinary-differential-equations', 'derivatives', 'chaos-theory']"
93,"Find the solution of $~\dfrac{dy}{dt} + \sqrt{1+ t^2}~e^{-t}~y=0$ , $~y(0)=1~.$","Find the solution of  ,",~\dfrac{dy}{dt} + \sqrt{1+ t^2}~e^{-t}~y=0 ~y(0)=1~.,"Find the solution of the initial value problem $$\dfrac{dy}{dt} + \sqrt{1+ t^2}~e^{-t}~y=0~ ,~~~ y(0)=1~.$$ It seems like 'separable equation' so I tried $$-\frac{1}{y} \frac{dy}{dt} = \sqrt{1+ t^2}~e^{-t} $$ However, righthand side is complicated to integrate, so I think there is another method but I don't know. How can I  solve this?","Find the solution of the initial value problem It seems like 'separable equation' so I tried However, righthand side is complicated to integrate, so I think there is another method but I don't know. How can I  solve this?","\dfrac{dy}{dt} + \sqrt{1+ t^2}~e^{-t}~y=0~ ,~~~ y(0)=1~. -\frac{1}{y} \frac{dy}{dt} = \sqrt{1+ t^2}~e^{-t} ","['calculus', 'ordinary-differential-equations']"
94,Proving a Simple Differential Equality by Induction,Proving a Simple Differential Equality by Induction,,"I would like to prove the following statement. For $\phi (r) \in C^{k+1}(\mathbb{R})$ , and $k \in \mathbb{N}$ : $(d_{r}^{2}) (\frac{1}{r} d_{r})^{k-1} (r^{2k - 1} \phi(r)) = (\frac{1}{r}d_{r})^{k} (r^{2k}d_{r} \phi(r))$ Equality was easy to show for the $k=1$ case. I would then like to assume true for $k = n-1$ case and show that the $k=n$ case follows. However, I have been unable to do this. Could someone please help me with the induction step? Here is my attempt: Assume true for $k = n-1, \ n \in \mathbb{N}$ . Consider the $k = n$ case. We can rewrite the left-hand side of the equality as follows: $(d_{r}^{2}) (\frac{1}{r} d_{r})^{n-1} (r^{2n - 1} \phi(r))$ $= (d_{r}^{2}) (\frac{1}{r} d_{r})^{k-2} (\frac{1}{r} d_{r} [r^{2k - 1} \phi(r)])$ $= (d_{r}^{2}) (\frac{1}{r} d_{r})^{k-2} (\frac{1}{r} [(2k - 1)r^{2k - 2} \phi(r) + r^{2k - 1} d_{r}\phi(r)])$ $= (d_{r}^{2}) (\frac{1}{r} d_{r})^{k-2} [(2k - 1)r^{2k - 3} \phi(r) + r^{2k - 2} d_{r}\phi(r)]$ Since the $k = n-1$ case is true, we can say: $= (2k - 1) (\frac{1}{r} d_{r})^{k-1} (r^{2(k-1)} d_{r}\phi) + (d_{r}^{2}) (\frac{1}{r} d_{r})^{k-2} (r^{2(k-1)} d_{r} \phi)$ We now try to rewrite the right-hand side: $(\frac{1}{r} d_{r})^{k} (r^{2k} d_{r} \phi) $ $= (\frac{1}{r} d_{r})^{k-1} (\frac{1}{r} [2kr^{2k-1} d_{r}\phi + r^{2k} d_{r}^{2} \phi ] )$ $= (\frac{1}{r} d_{r})^{k-1} (2kr^{2k-2} d_{r}\phi + r^{2k - 1} d_{r}^{2} \phi) $ Unfortunately, this is about as far as I can go! I have tried manipulating these in slightly different ways, but I just can't seem to get the desired result using the $k = n-1$ case. Could someone try to fix/point out a mistake I've made?","I would like to prove the following statement. For , and : Equality was easy to show for the case. I would then like to assume true for case and show that the case follows. However, I have been unable to do this. Could someone please help me with the induction step? Here is my attempt: Assume true for . Consider the case. We can rewrite the left-hand side of the equality as follows: Since the case is true, we can say: We now try to rewrite the right-hand side: Unfortunately, this is about as far as I can go! I have tried manipulating these in slightly different ways, but I just can't seem to get the desired result using the case. Could someone try to fix/point out a mistake I've made?","\phi (r) \in C^{k+1}(\mathbb{R}) k \in \mathbb{N} (d_{r}^{2}) (\frac{1}{r} d_{r})^{k-1} (r^{2k - 1} \phi(r)) = (\frac{1}{r}d_{r})^{k} (r^{2k}d_{r} \phi(r)) k=1 k = n-1 k=n k = n-1, \ n \in \mathbb{N} k = n (d_{r}^{2}) (\frac{1}{r} d_{r})^{n-1} (r^{2n - 1} \phi(r)) = (d_{r}^{2}) (\frac{1}{r} d_{r})^{k-2} (\frac{1}{r} d_{r} [r^{2k - 1} \phi(r)]) = (d_{r}^{2}) (\frac{1}{r} d_{r})^{k-2} (\frac{1}{r} [(2k - 1)r^{2k - 2} \phi(r) + r^{2k - 1} d_{r}\phi(r)]) = (d_{r}^{2}) (\frac{1}{r} d_{r})^{k-2} [(2k - 1)r^{2k - 3} \phi(r) + r^{2k - 2} d_{r}\phi(r)] k = n-1 = (2k - 1) (\frac{1}{r} d_{r})^{k-1} (r^{2(k-1)} d_{r}\phi) + (d_{r}^{2}) (\frac{1}{r} d_{r})^{k-2} (r^{2(k-1)} d_{r} \phi) (\frac{1}{r} d_{r})^{k} (r^{2k} d_{r} \phi)  = (\frac{1}{r} d_{r})^{k-1} (\frac{1}{r} [2kr^{2k-1} d_{r}\phi + r^{2k} d_{r}^{2} \phi ] ) = (\frac{1}{r} d_{r})^{k-1} (2kr^{2k-2} d_{r}\phi + r^{2k - 1} d_{r}^{2} \phi)  k = n-1","['real-analysis', 'ordinary-differential-equations', 'derivatives', 'induction']"
95,Uniqueness of solutions of a Cauchy problem,Uniqueness of solutions of a Cauchy problem,,"Consider the Cauchy problem $$\begin{cases}y'(x)=\sqrt{y(x)}\\ y(0)=0\end{cases}$$ Clearly $y'(x)=\sqrt{y(x)}$ is a differential equation that can be solved with the separation of the variables, in fact: $y'(x)=a(x)b(y(x))$ where $a(x)=1$ is continuous on $I=\mathbb{R}$ , while $b(y(x))=\sqrt{y(x)}$ is continuous on $J=[0,+\infty)$ , but it's not a lipschitzian function in a neighborhood of $0.$ .. I can't use Picard–Lindelöf theorem. The uniqueness is not guaranteed. Now the question: can I use the separation of variables, if I can't find a neighborhood of $0$ such that $b(y)\ne 0$ ?","Consider the Cauchy problem Clearly is a differential equation that can be solved with the separation of the variables, in fact: where is continuous on , while is continuous on , but it's not a lipschitzian function in a neighborhood of .. I can't use Picard–Lindelöf theorem. The uniqueness is not guaranteed. Now the question: can I use the separation of variables, if I can't find a neighborhood of such that ?","\begin{cases}y'(x)=\sqrt{y(x)}\\ y(0)=0\end{cases} y'(x)=\sqrt{y(x)} y'(x)=a(x)b(y(x)) a(x)=1 I=\mathbb{R} b(y(x))=\sqrt{y(x)} J=[0,+\infty) 0. 0 b(y)\ne 0","['real-analysis', 'calculus', 'ordinary-differential-equations']"
96,Find if an ordinary differential equation is linear,Find if an ordinary differential equation is linear,,"So my course shows me three differential equations: $$\dot x + x^2 = t$$ $$\dot x = (t^2+1)(x-1)$$ $$\dot x + x = t^2$$ The first one is not a linear ordinary differential equation (ODE) apparently, the other two are. Unfortunately, they don't show a clear way how to find out if an ODE is linear or not. So how we can find out if an ODE is linear? For the second one, I thought I bring it into standard form somehow: $$\dot x = (t^2+1)(x-1) = xt^2 + x - t^2 -1 = x(t^2+1)-t^2-1$$ If we say we let $p(t)=t^2+1$ and $q(t)=1+t^2$ , then we could say: $$\dot x = xp(t) - q(t) = ...$$ And so on, to simplify until we reach standard form of a linear ODE (or not). Is that the way to go? Or is there some other way to check if a ODE is linear?","So my course shows me three differential equations: The first one is not a linear ordinary differential equation (ODE) apparently, the other two are. Unfortunately, they don't show a clear way how to find out if an ODE is linear or not. So how we can find out if an ODE is linear? For the second one, I thought I bring it into standard form somehow: If we say we let and , then we could say: And so on, to simplify until we reach standard form of a linear ODE (or not). Is that the way to go? Or is there some other way to check if a ODE is linear?",\dot x + x^2 = t \dot x = (t^2+1)(x-1) \dot x + x = t^2 \dot x = (t^2+1)(x-1) = xt^2 + x - t^2 -1 = x(t^2+1)-t^2-1 p(t)=t^2+1 q(t)=1+t^2 \dot x = xp(t) - q(t) = ...,"['calculus', 'ordinary-differential-equations']"
97,Reference request: stability of the Mathieu equation,Reference request: stability of the Mathieu equation,,"I am interested in the Mathieu equation, $\frac{d^2x}{dt^2}+(\delta+\epsilon\cos t)x=0$ . I often see diagrams illustrating the stability of solutions, like this one (S=stable, U=unstable): Does anyone know of a good source which discusses how to generate the stability curves? I've looked at Advanced Methods of Scientists and Engineers by Bender and Orszag, but I feel like there's something that I'm missing, though I'm not sure what it is. I understand where the $\delta$ -intercepts of $0.25, 1, 2.25$ come from, but I do not see how to generate the shaded part of the diagram. I am hoping that another reference or two will fill in the missing gaps.","I am interested in the Mathieu equation, . I often see diagrams illustrating the stability of solutions, like this one (S=stable, U=unstable): Does anyone know of a good source which discusses how to generate the stability curves? I've looked at Advanced Methods of Scientists and Engineers by Bender and Orszag, but I feel like there's something that I'm missing, though I'm not sure what it is. I understand where the -intercepts of come from, but I do not see how to generate the shaded part of the diagram. I am hoping that another reference or two will fill in the missing gaps.","\frac{d^2x}{dt^2}+(\delta+\epsilon\cos t)x=0 \delta 0.25, 1, 2.25","['ordinary-differential-equations', 'reference-request', 'special-functions', 'stability-in-odes']"
98,Differential equation describing diffusion to a partially covered sphere,Differential equation describing diffusion to a partially covered sphere,,"I'm trying to work through a paper by Zwanzig (1990, Diffusion controlled ligand binding to spheres partially covered by receptors: An effective medium treatment). There is one section that I can't follow the math, here it is: The steady state diffusion equation: $$D~ \bigtriangledown ^{2}C = 0$$ [Where $~D~$ is the diffusion coefficient and $~C~$ is concentration.] ""This is to be solved with appropriate boundary conditions on the surface of the sphere. To an observer far from the sphere, the surface appears to be uniform but neither perfectly reflecting nor perfectly absorbing. This suggests the use of a ""radiation boundary condition,"" $$D~\frac{\partial C}{\partial r} = k~C$$ on $$r = R$$ If $~k = 0~$ , the surface is perfectly reflecting, and if $~k~$ goes to $~\infty~$ , the surface is perfectly absorbing. Then the appropriate solution of the diffusion equation is $$C = 1 - \frac{\alpha}{r}$$ and the boundary condition on R determines the coefficient $$\alpha = \frac{k}{\frac{k}{R} + \frac{D}{R^{2}}}$$ I'm stuck on how to get that solution, any help is appreciated!","I'm trying to work through a paper by Zwanzig (1990, Diffusion controlled ligand binding to spheres partially covered by receptors: An effective medium treatment). There is one section that I can't follow the math, here it is: The steady state diffusion equation: [Where is the diffusion coefficient and is concentration.] ""This is to be solved with appropriate boundary conditions on the surface of the sphere. To an observer far from the sphere, the surface appears to be uniform but neither perfectly reflecting nor perfectly absorbing. This suggests the use of a ""radiation boundary condition,"" on If , the surface is perfectly reflecting, and if goes to , the surface is perfectly absorbing. Then the appropriate solution of the diffusion equation is and the boundary condition on R determines the coefficient I'm stuck on how to get that solution, any help is appreciated!",D~ \bigtriangledown ^{2}C = 0 ~D~ ~C~ D~\frac{\partial C}{\partial r} = k~C r = R ~k = 0~ ~k~ ~\infty~ C = 1 - \frac{\alpha}{r} \alpha = \frac{k}{\frac{k}{R} + \frac{D}{R^{2}}},"['ordinary-differential-equations', 'derivatives', 'partial-differential-equations']"
99,Determining if $u''+(u^2+u'{^2}-1)u'+u=0$ has a limit cycle,Determining if  has a limit cycle,u''+(u^2+u'{^2}-1)u'+u=0,"I am trying to determine if the ODE $$u''+(u^2+u'{^2}-1)u'+u=0,$$ has a limit cycle. Converting this ODE into a system of first-order ODES, $$\underline{x}=\begin{pmatrix} x \\  y \end{pmatrix}, \ \  \underline{x}'=\begin{pmatrix} y \\ -y(x^2+y^2-1)-x \end{pmatrix}.$$ Using polar coordinates, we can show that $$r'=-r\sin^2(\theta)(r^2-1).$$ Clearly, $r'=0$ when $r=0,1$ and when $\theta=n\pi\ $ for $\ n\in\mathbb{Z}.$ I have been instructed that $r=0$ is a fixed point, my question is why ? From my understanding, a fixed point was such that $r'=0$ and $\theta'=0$ , but the equation for $\theta'$ is $$\theta'=-1-\sin(\theta)\cos(\theta)(r^2-1).$$ For $r=0$ , $\theta'\neq 0.$","I am trying to determine if the ODE has a limit cycle. Converting this ODE into a system of first-order ODES, Using polar coordinates, we can show that Clearly, when and when for I have been instructed that is a fixed point, my question is why ? From my understanding, a fixed point was such that and , but the equation for is For ,","u''+(u^2+u'{^2}-1)u'+u=0, \underline{x}=\begin{pmatrix}
x \\ 
y
\end{pmatrix}, \ \  \underline{x}'=\begin{pmatrix}
y \\
-y(x^2+y^2-1)-x \end{pmatrix}. r'=-r\sin^2(\theta)(r^2-1). r'=0 r=0,1 \theta=n\pi\  \ n\in\mathbb{Z}. r=0 r'=0 \theta'=0 \theta' \theta'=-1-\sin(\theta)\cos(\theta)(r^2-1). r=0 \theta'\neq 0.","['calculus', 'ordinary-differential-equations', 'dynamical-systems', 'stability-theory']"
