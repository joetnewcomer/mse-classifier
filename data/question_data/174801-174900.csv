,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Compute $\int_{0}^{1}\int_{0}^{z}\int_{0}^{\sqrt{z^2-x^2}} \dfrac{e^{z^2}}{\sqrt{x^2+y^2}}\, dy\, dx\, dz$ using this change of coorninates.",Compute  using this change of coorninates.,"\int_{0}^{1}\int_{0}^{z}\int_{0}^{\sqrt{z^2-x^2}} \dfrac{e^{z^2}}{\sqrt{x^2+y^2}}\, dy\, dx\, dz","Compute $\int_{0}^{1}\int_{0}^{z}\int_{0}^{\sqrt{z^2-x^2}} \dfrac{e^{z^2}}{\sqrt{x^2+y^2}}\, dy\, dx\, dz$ using this change of coorninates. I could say that: $0\leq 1, 0\leq x\leq z, 0\leq y \leq \sqrt{z^2-x^2}$ I believe that cylindrical and not spherical coordinates should be used. But the calculation has become difficult for me, I think there are more things to consider. If $x=r\cos\theta, y=r\sinθ, z=z$ , then $$\int_{0}^{1}\int_{0}^{z}\int_{0}^{\sqrt{z^2-r^2\cos \theta}} e^{z^2}\, dr\, d\theta\, dz$$ The limit of integration $\sqrt{z^2-r^2\cos \theta}$ stays like this, or I must do something else.","Compute using this change of coorninates. I could say that: I believe that cylindrical and not spherical coordinates should be used. But the calculation has become difficult for me, I think there are more things to consider. If , then The limit of integration stays like this, or I must do something else.","\int_{0}^{1}\int_{0}^{z}\int_{0}^{\sqrt{z^2-x^2}} \dfrac{e^{z^2}}{\sqrt{x^2+y^2}}\, dy\, dx\, dz 0\leq 1, 0\leq x\leq z, 0\leq y \leq \sqrt{z^2-x^2} x=r\cos\theta, y=r\sinθ, z=z \int_{0}^{1}\int_{0}^{z}\int_{0}^{\sqrt{z^2-r^2\cos \theta}} e^{z^2}\, dr\, d\theta\, dz \sqrt{z^2-r^2\cos \theta}","['multivariable-calculus', 'multiple-integral', 'iterated-integrals']"
1,Normal derivative of a partial derivative,Normal derivative of a partial derivative,,"I am reading lecture some lecture notes where the professor defines a function as such: $$\frac{d}{dc}\bigg[\frac{\partial W(c',c)}{\partial c'}\bigg|_{c'=c}\bigg]_{c=c^*} < 0$$ This I can make sense of as: the slope of the function $\frac{\partial W(c',c)}{\partial c'}\bigg|_{c'=c}$ being less than 0 at $c=c^*$ The professor goes on to say that by ""carefully differentiating"" we can rewrite this as: $$ \bigg[\frac{\partial^2W(c',c)}{\partial c'^2} + \frac{\partial^2 W(c',c)}{\partial c' \partial c}\bigg]\Bigg|_{c'=c=c^*} < 0 $$ This I do not understand. My initial thought would be that by taking the derivative of $\frac{\partial W(c',c)}{\partial c'}\bigg|_{c'=c}$ with respect to $c$ only should simply yield: $\frac{\partial^2 W(c',c)}{\partial c' \partial c}\bigg|_{c'=c=c^*}$ . Where does this extra term of $\frac{\partial^2W(c',c)}{\partial c'^2}$ come from? Can someone point out where I have misunderstood?","I am reading lecture some lecture notes where the professor defines a function as such: This I can make sense of as: the slope of the function being less than 0 at The professor goes on to say that by ""carefully differentiating"" we can rewrite this as: This I do not understand. My initial thought would be that by taking the derivative of with respect to only should simply yield: . Where does this extra term of come from? Can someone point out where I have misunderstood?","\frac{d}{dc}\bigg[\frac{\partial W(c',c)}{\partial c'}\bigg|_{c'=c}\bigg]_{c=c^*} < 0 \frac{\partial W(c',c)}{\partial c'}\bigg|_{c'=c} c=c^*  \bigg[\frac{\partial^2W(c',c)}{\partial c'^2} + \frac{\partial^2 W(c',c)}{\partial c' \partial c}\bigg]\Bigg|_{c'=c=c^*} < 0  \frac{\partial W(c',c)}{\partial c'}\bigg|_{c'=c} c \frac{\partial^2 W(c',c)}{\partial c' \partial c}\bigg|_{c'=c=c^*} \frac{\partial^2W(c',c)}{\partial c'^2}","['calculus', 'multivariable-calculus', 'partial-derivative']"
2,"I have a function $f(x,y,z)$. Can this be rewritten as $f(y,z)$ if $x$ is not present in the function?",I have a function . Can this be rewritten as  if  is not present in the function?,"f(x,y,z) f(y,z) x","Let's say that I have $f(x,y,z)$ which is derived from another function g. And this is true for all cases. For a certain case, $f(x,y,z) = yz$ Is this the same as writing/ defining the function as $f(y,z) = yz$ ? Or is it imperative that I write it as the $f(x,y,z)=yz$ as its definition?","Let's say that I have which is derived from another function g. And this is true for all cases. For a certain case, Is this the same as writing/ defining the function as ? Or is it imperative that I write it as the as its definition?","f(x,y,z) f(x,y,z) = yz f(y,z) = yz f(x,y,z)=yz","['multivariable-calculus', 'functions']"
3,Is the minimum a continuous function?,Is the minimum a continuous function?,,"Let's consider a continuous function $$ f:\mathbb{R}\times [a,b]\to\mathbb{R}$$ Such that $g(y)=\min_{x\in\mathbb{R}} f(x,y)$ is well defined for every $y\in[a,b]$ . Is it true that $g(y)$ is continuous on [a,b]? I believe this statement to be true, but I don't know how to prove it. Any hint would be appreciated.","Let's consider a continuous function Such that is well defined for every . Is it true that is continuous on [a,b]? I believe this statement to be true, but I don't know how to prove it. Any hint would be appreciated."," f:\mathbb{R}\times [a,b]\to\mathbb{R} g(y)=\min_{x\in\mathbb{R}} f(x,y) y\in[a,b] g(y)","['real-analysis', 'calculus', 'multivariable-calculus', 'continuity', 'maxima-minima']"
4,"Let me assume that the function $f(x,y,z,w)$ is continuous. Is the $\max_{w} f(x,y,z,w)$ continuous?",Let me assume that the function  is continuous. Is the  continuous?,"f(x,y,z,w) \max_{w} f(x,y,z,w)","Let me assume that the function $f(x,y,z,w)$ is continuous. Is the $\max_{w} f(x,y,z,w)$ continuous? Since $f(x,y,z,w)$ is continuous, it is seperately continuous for each $x,y,z,$ and $w$ .","Let me assume that the function is continuous. Is the continuous? Since is continuous, it is seperately continuous for each and .","f(x,y,z,w) \max_{w} f(x,y,z,w) f(x,y,z,w) x,y,z, w","['multivariable-calculus', 'continuity', 'maxima-minima']"
5,find minimum value of $n$ that satisfies this inequality: $4(x_1^2+x_2^2+...+x_n^2)<2(x_1+x_2+...+x_n)<x_1^3+x_2^3+...+x_n^3$,find minimum value of  that satisfies this inequality:,n 4(x_1^2+x_2^2+...+x_n^2)<2(x_1+x_2+...+x_n)<x_1^3+x_2^3+...+x_n^3,"I saw this question, it looks very hard: there are real positive numbers    $\{x_1,x_2,...x_n\}$ , given the inequality: $$4(x_1^2+x_2^2+...+x_n^2)<2(x_1+x_2+...+x_n)<x_1^3+x_2^3+...+x_n^3$$ What is the minimum value of $n$, for this to be possible? This is what I've tried so far: $$\sum_{k=1}^nx_k^3>\sum_{k=1}^n4x_k^2\\\sum_{k=1}^nx_k^3-\sum_{k=1}^n4x_k^2>0\\\sum_{k=1}^nx_k^2(x_k-4)>0$$So it must be at least one number that is bigger than $4$. So I go with trial and error .Example 1;$$x_1=10, n=3001, x_2=x_3=...=x_{3001}=0.1\\4(100+3000\cdot0.01)=520<2(10+3000\cdot0.1)=620<1000+3000\cdot0.001=1003$$ Example 2;$$x_1=6, n=1001, x_2=x_3=...=x_{1001}=0.1\\4(36+1000\cdot0.01)=184<2(6+1000\cdot0.1)=212<216+1000\cdot0.001=217$$ S0 $n$ is getting smaller, but I have no idea how to minimize $n$, or even how to approach this kind of question. Thanks in advance for any help.","I saw this question, it looks very hard: there are real positive numbers    $\{x_1,x_2,...x_n\}$ , given the inequality: $$4(x_1^2+x_2^2+...+x_n^2)<2(x_1+x_2+...+x_n)<x_1^3+x_2^3+...+x_n^3$$ What is the minimum value of $n$, for this to be possible? This is what I've tried so far: $$\sum_{k=1}^nx_k^3>\sum_{k=1}^n4x_k^2\\\sum_{k=1}^nx_k^3-\sum_{k=1}^n4x_k^2>0\\\sum_{k=1}^nx_k^2(x_k-4)>0$$So it must be at least one number that is bigger than $4$. So I go with trial and error .Example 1;$$x_1=10, n=3001, x_2=x_3=...=x_{3001}=0.1\\4(100+3000\cdot0.01)=520<2(10+3000\cdot0.1)=620<1000+3000\cdot0.001=1003$$ Example 2;$$x_1=6, n=1001, x_2=x_3=...=x_{1001}=0.1\\4(36+1000\cdot0.01)=184<2(6+1000\cdot0.1)=212<216+1000\cdot0.001=217$$ S0 $n$ is getting smaller, but I have no idea how to minimize $n$, or even how to approach this kind of question. Thanks in advance for any help.",,"['multivariable-calculus', 'inequality', 'optimization', 'contest-math', 'symmetric-polynomials']"
6,Chain Rule for Surface Differentials,Chain Rule for Surface Differentials,,"So given three surfaces $S_1,S_2,S_3$ in $\mathbb{R}^n$ (I am currently doing differential geometry of curves and surfaces, so I am not really familiar with most manifold jargon. I assume the proper terminology would be something to the effect of smooth manifolds embedded in $\mathbb{R}^n$) and a chain of smooth maps: $$ S_1\xrightarrow[]{\phi} S_2 \xrightarrow{\psi}S_3 $$ I would like to prove the chain rule for the differentials of $\phi,\psi$. That is, for a point $p$ in $S_1$, we have: $$ d(\psi \circ \phi )_p = d\psi_{\phi(p)} \circ d\phi_p $$ My book's definition of the differential for a function $ \phi: S_1 \to S_2$ is a map $d\phi_p:T_pS_1 \to T_\phi(p)S_2$ and is defined by: $$ d\phi_p = (\phi \circ \gamma )'(0) $$ where $\gamma$ is a smooth curve, $\gamma:(-\epsilon,\epsilon)\to S_1$ with the property that $\gamma(0)  = p$. Using this definition, computing the differential of the composition at $p$ comes down to computing: $$ (\psi \circ \phi \circ \gamma)'(0) $$ However, here is where I am a bit unsure. Let us define $\beta = \phi \circ \gamma$. Then, we have that $\beta$ is a smooth curve from $\mathbb{R}$ into $S_2$, with $\beta(0) = \phi(p)$. Then, we conclude: $$ d(\psi \circ \phi )_p  = (\psi \circ \phi \circ \gamma)'(0) = (\psi \circ \beta)'(0) =  d\psi_{\phi(p)} $$ But this is clearly incorrect. What am I doing wrong?","So given three surfaces $S_1,S_2,S_3$ in $\mathbb{R}^n$ (I am currently doing differential geometry of curves and surfaces, so I am not really familiar with most manifold jargon. I assume the proper terminology would be something to the effect of smooth manifolds embedded in $\mathbb{R}^n$) and a chain of smooth maps: $$ S_1\xrightarrow[]{\phi} S_2 \xrightarrow{\psi}S_3 $$ I would like to prove the chain rule for the differentials of $\phi,\psi$. That is, for a point $p$ in $S_1$, we have: $$ d(\psi \circ \phi )_p = d\psi_{\phi(p)} \circ d\phi_p $$ My book's definition of the differential for a function $ \phi: S_1 \to S_2$ is a map $d\phi_p:T_pS_1 \to T_\phi(p)S_2$ and is defined by: $$ d\phi_p = (\phi \circ \gamma )'(0) $$ where $\gamma$ is a smooth curve, $\gamma:(-\epsilon,\epsilon)\to S_1$ with the property that $\gamma(0)  = p$. Using this definition, computing the differential of the composition at $p$ comes down to computing: $$ (\psi \circ \phi \circ \gamma)'(0) $$ However, here is where I am a bit unsure. Let us define $\beta = \phi \circ \gamma$. Then, we have that $\beta$ is a smooth curve from $\mathbb{R}$ into $S_2$, with $\beta(0) = \phi(p)$. Then, we conclude: $$ d(\psi \circ \phi )_p  = (\psi \circ \phi \circ \gamma)'(0) = (\psi \circ \beta)'(0) =  d\psi_{\phi(p)} $$ But this is clearly incorrect. What am I doing wrong?",,"['multivariable-calculus', 'differential-geometry', 'manifolds']"
7,Meaningless partial derivative,Meaningless partial derivative,,"The function in question is $$f(x,y) = \begin{cases} 0 & (x,y)=(0,0)\\ \frac{xy}{|x|+|y|} & (x,y) \neq (0,0) \end{cases} $$ So far I've calculated the continuity of the function at $(0,0)$ using the squeeze theorem with the functions $xy$ and $\frac{xy}{\sqrt{x^2+y^2}}$. The former is always bigger than the second branch of $f$, and the latter always smaller; they all have limit $0$, therefore the function is continuous at $(0,0)$. Now I have to calculate the first partial derivative in order to $x$ of $f$ at $(0,0)$. Using the definition, we have $$\frac{\partial f}{\partial x}(0,0)=\lim_{t\to 0}\frac{f(t,0)-f(0,0)}{t}$$ Now, $f(0,0)$ is just $0$, and $f(t,0)$ is $0$ as well. So we have $$\lim_{t\to 0}\frac{f(t,0)-f(0,0)}{t}=\lim_{t\to 0}\frac{0}{t}$$ which is not really meaningful, at least to me, and there doesn't seem to be an obvious way of solving this limit without finding the full expression. So I started hunting for an expression for the partial derivative using the limit definition. That should be $$\frac{\partial f}{\partial x}(x_0,y_0)=\lim_{t\to 0}\frac{f(x_0+t,y_0)-f(x_0,y_0)}{t}$$ but that seems impressingly difficult to calculate. If any of this is correct, we have $$\lim_{t\to 0}\frac{\frac{(x+t)y}{|x+t|+|y|}-\frac{xy}{|x|+|y|}}{t}=\lim_{t\to 0}\frac{(x+t)y}{t(|x+t|+|y|)}-\frac{xy}{t(|x|+|y|)}$$ and after a few ""simplifications"" (which don't simplify anything at all), we get $$\lim_{t\to 0} \frac{xy|x|+ty|x|+xy|y|+ty|y|-xy|x+t|-xy|y|}{t(|x+t|+|y|)(|x|+|y|)}$$ and there's where it becomes a huge mess. I think separating $|x+t|$ would help a lot , but I have no idea on how to justify doing that. This analysis class is killing me, even if you can't help me with the problem just recommending a good textbook would help a ton. Here's the function plotted on WA: https://www.wolframalpha.com/input/?i=f(x,y)+%3D+(xy)%2F(%7Cx%7C%2B%7Cy%7C)","The function in question is $$f(x,y) = \begin{cases} 0 & (x,y)=(0,0)\\ \frac{xy}{|x|+|y|} & (x,y) \neq (0,0) \end{cases} $$ So far I've calculated the continuity of the function at $(0,0)$ using the squeeze theorem with the functions $xy$ and $\frac{xy}{\sqrt{x^2+y^2}}$. The former is always bigger than the second branch of $f$, and the latter always smaller; they all have limit $0$, therefore the function is continuous at $(0,0)$. Now I have to calculate the first partial derivative in order to $x$ of $f$ at $(0,0)$. Using the definition, we have $$\frac{\partial f}{\partial x}(0,0)=\lim_{t\to 0}\frac{f(t,0)-f(0,0)}{t}$$ Now, $f(0,0)$ is just $0$, and $f(t,0)$ is $0$ as well. So we have $$\lim_{t\to 0}\frac{f(t,0)-f(0,0)}{t}=\lim_{t\to 0}\frac{0}{t}$$ which is not really meaningful, at least to me, and there doesn't seem to be an obvious way of solving this limit without finding the full expression. So I started hunting for an expression for the partial derivative using the limit definition. That should be $$\frac{\partial f}{\partial x}(x_0,y_0)=\lim_{t\to 0}\frac{f(x_0+t,y_0)-f(x_0,y_0)}{t}$$ but that seems impressingly difficult to calculate. If any of this is correct, we have $$\lim_{t\to 0}\frac{\frac{(x+t)y}{|x+t|+|y|}-\frac{xy}{|x|+|y|}}{t}=\lim_{t\to 0}\frac{(x+t)y}{t(|x+t|+|y|)}-\frac{xy}{t(|x|+|y|)}$$ and after a few ""simplifications"" (which don't simplify anything at all), we get $$\lim_{t\to 0} \frac{xy|x|+ty|x|+xy|y|+ty|y|-xy|x+t|-xy|y|}{t(|x+t|+|y|)(|x|+|y|)}$$ and there's where it becomes a huge mess. I think separating $|x+t|$ would help a lot , but I have no idea on how to justify doing that. This analysis class is killing me, even if you can't help me with the problem just recommending a good textbook would help a ton. Here's the function plotted on WA: https://www.wolframalpha.com/input/?i=f(x,y)+%3D+(xy)%2F(%7Cx%7C%2B%7Cy%7C)",,"['real-analysis', 'multivariable-calculus']"
8,Schwarz's Theorem and Discontinuous Second Derivatives,Schwarz's Theorem and Discontinuous Second Derivatives,,Let $f:\mathbb{R}^2 \to \mathbb{R}$ be twice differentiable in $a$. Suppose that $\frac{\partial^2f}{\partial x \partial y}$ is continuous in $a$. Is it possible that $\frac{\partial^2 f}{\partial y \partial x}$ is discontinuous in $a$? The question is motivated by Schwarz' Theorem - as this observation would show that it would not help at merely computing the derivatives because mostly we would not know whether both second derivates are continuous. And that's needed to apply Schwarz (at least in the way I know the theorem).,Let $f:\mathbb{R}^2 \to \mathbb{R}$ be twice differentiable in $a$. Suppose that $\frac{\partial^2f}{\partial x \partial y}$ is continuous in $a$. Is it possible that $\frac{\partial^2 f}{\partial y \partial x}$ is discontinuous in $a$? The question is motivated by Schwarz' Theorem - as this observation would show that it would not help at merely computing the derivatives because mostly we would not know whether both second derivates are continuous. And that's needed to apply Schwarz (at least in the way I know the theorem).,,"['real-analysis', 'multivariable-calculus']"
9,Multivariable Calculus surface integral over a square,Multivariable Calculus surface integral over a square,,"Thanks for any help in advance. I'm currently working on a question which is as follows: Find the area of the part of the sphere of radius a at the origin which is above the square in the (x,y) plane bounded by: $$  x = \frac{a}{\sqrt{2}} ,  x = -\frac{a}{\sqrt{2}} , y = \frac{a}{\sqrt{2}} , y = -\frac{a}{\sqrt{2}} $$ Hint for evaluating the integral: change to polar coordinates and evaluate the $r$ integral first. I have found the surface element in terms of spherical polar coordinates, $a^2\sin\theta$, where $\theta$ is the angle from the $z$ axis, but i am having difficulty projecting it onto the square in the $(x,y)$ plane which does not agree with spherical coordinates.","Thanks for any help in advance. I'm currently working on a question which is as follows: Find the area of the part of the sphere of radius a at the origin which is above the square in the (x,y) plane bounded by: $$  x = \frac{a}{\sqrt{2}} ,  x = -\frac{a}{\sqrt{2}} , y = \frac{a}{\sqrt{2}} , y = -\frac{a}{\sqrt{2}} $$ Hint for evaluating the integral: change to polar coordinates and evaluate the $r$ integral first. I have found the surface element in terms of spherical polar coordinates, $a^2\sin\theta$, where $\theta$ is the angle from the $z$ axis, but i am having difficulty projecting it onto the square in the $(x,y)$ plane which does not agree with spherical coordinates.",,['multivariable-calculus']
10,Any deeper insights for why $\int \frac{1}{x}dx = \ln|x|+C$?,Any deeper insights for why ?,\int \frac{1}{x}dx = \ln|x|+C,"I have already learned that $$\int x^\alpha \ dx = \cases{\frac{1}{\alpha+1}x^{\alpha+1}+C & if $\alpha\neq-1$ \\ \ln|x|+C & if $\alpha=-1$} \ . $$ I am just curious if there is any deeper insight to explain the abrupt jump from powers of $x$ to the natural logarithm $\ln|x|$ at $\alpha=-1$ , rather than just saying because the coefficient $1/(\alpha+1)$ is undefined when $\alpha=-1$. I have tried plotting things out using Desmos , and the 'jump' at $k=-1$ still doesn't go away. Here is what I did: I plotted a function $$f_k(x) := x^k$$ with parameter $k$. As I adjust $k$, I can see all the $x^k$, but none of them are close to the plot of $\ln{x}$. My guess: I suspect we need function of two variables to visualize the 'jump'. I tried to construct $g(x, y) := x^y$ and observed what happen at $y=0$. It seems like there is some 'saddle line' parametrized by $(t,0,1)$ for all $t\in\mathbb{R}$. But I'm still not sure how $\ln{x}$ can come into play.","I have already learned that $$\int x^\alpha \ dx = \cases{\frac{1}{\alpha+1}x^{\alpha+1}+C & if $\alpha\neq-1$ \\ \ln|x|+C & if $\alpha=-1$} \ . $$ I am just curious if there is any deeper insight to explain the abrupt jump from powers of $x$ to the natural logarithm $\ln|x|$ at $\alpha=-1$ , rather than just saying because the coefficient $1/(\alpha+1)$ is undefined when $\alpha=-1$. I have tried plotting things out using Desmos , and the 'jump' at $k=-1$ still doesn't go away. Here is what I did: I plotted a function $$f_k(x) := x^k$$ with parameter $k$. As I adjust $k$, I can see all the $x^k$, but none of them are close to the plot of $\ln{x}$. My guess: I suspect we need function of two variables to visualize the 'jump'. I tried to construct $g(x, y) := x^y$ and observed what happen at $y=0$. It seems like there is some 'saddle line' parametrized by $(t,0,1)$ for all $t\in\mathbb{R}$. But I'm still not sure how $\ln{x}$ can come into play.",,"['multivariable-calculus', 'derivatives', 'logarithms']"
11,Fubini's theorem for multiple Riemann integrals,Fubini's theorem for multiple Riemann integrals,,"I'm working through Analysis on Manifolds by Munkres on my own. He proves a version of Fubini's theorem for multivariable Riemann integrals that goes like: If $Q= A \times B \subset \mathbb{R}^n \times \mathbb{R}^m$ is a closed bounded rectangle and the Riemann integral $\int_Q f$ exists then $$\int_Q f = \int_A \underline{\int}_B f(x,y) dy dx = \int_A \overline{\int}_B f(x,y) dy dx $$ The lower and upper integrals are needed here when, fixing $x$, $f(x,\cdot):B \to \mathbb{R}$ is bounded but not Riemann integrable over $B$.   So if $f(x, \cdot)$ is integrable then we get Fubini's theorem where $$\int_Q f = \int_A \int_B f(x,y) dy dx = \int_B \int_A f(x,y) dy dx $$ I think this could happen if $f$ is not absolutely integrable, otherwise Fubini's theorem for Lebesgue integrals applies. My question is what are some examples where $\int_Q f$ exists but $\int_Bf(x,y)dy$ does not?  More than one is welcome. EDIT To clarify, I mean I am looking for examples where $\int_Bf(x,y) dy$ is not Riemann integrable almost everywhere over $A$, but still $f$ is Riemann integrable over $Q$, so that $$\int_Q f \neq \int_A \int_B f(x,y) dy dx$$","I'm working through Analysis on Manifolds by Munkres on my own. He proves a version of Fubini's theorem for multivariable Riemann integrals that goes like: If $Q= A \times B \subset \mathbb{R}^n \times \mathbb{R}^m$ is a closed bounded rectangle and the Riemann integral $\int_Q f$ exists then $$\int_Q f = \int_A \underline{\int}_B f(x,y) dy dx = \int_A \overline{\int}_B f(x,y) dy dx $$ The lower and upper integrals are needed here when, fixing $x$, $f(x,\cdot):B \to \mathbb{R}$ is bounded but not Riemann integrable over $B$.   So if $f(x, \cdot)$ is integrable then we get Fubini's theorem where $$\int_Q f = \int_A \int_B f(x,y) dy dx = \int_B \int_A f(x,y) dy dx $$ I think this could happen if $f$ is not absolutely integrable, otherwise Fubini's theorem for Lebesgue integrals applies. My question is what are some examples where $\int_Q f$ exists but $\int_Bf(x,y)dy$ does not?  More than one is welcome. EDIT To clarify, I mean I am looking for examples where $\int_Bf(x,y) dy$ is not Riemann integrable almost everywhere over $A$, but still $f$ is Riemann integrable over $Q$, so that $$\int_Q f \neq \int_A \int_B f(x,y) dy dx$$",,"['real-analysis', 'multivariable-calculus', 'riemann-integration', 'multiple-integral']"
12,"Differentiability of $f(x)=\frac{x|y|}{\sqrt{x^2+y^2}}$ at $(0,0)$",Differentiability of  at,"f(x)=\frac{x|y|}{\sqrt{x^2+y^2}} (0,0)","Prove that $$\begin{array}\\&f(x)&=\begin{cases}\frac{x|y|}{\sqrt{x^2+y^2}}&\text{ if }(x,y) \ne (0,0)\\0&\text{ otherwise }\end{cases}\end{array}$$ is differentiable in $(0,0)$ . Proof: We know that $f$ is differentiable in $(0,0)$ , if all partial derivatives exist and are continuous in $(0,0)$ . We have $$\frac{\partial f}{\partial x}=\lim_{h\to\ 0}\frac{f(h,0)}{h}=0$$ $$\frac{\partial f}{\partial y}=\lim_{h\to\ 0}\frac{f(0,h)}{h}=0$$ So the partial derivatives exist. I am now stuck at proving the continuity of the partial derivatives in $(0,0)$ . Can anyone help me with that?","Prove that is differentiable in . Proof: We know that is differentiable in , if all partial derivatives exist and are continuous in . We have So the partial derivatives exist. I am now stuck at proving the continuity of the partial derivatives in . Can anyone help me with that?","\begin{array}\\&f(x)&=\begin{cases}\frac{x|y|}{\sqrt{x^2+y^2}}&\text{ if }(x,y) \ne (0,0)\\0&\text{ otherwise }\end{cases}\end{array} (0,0) f (0,0) (0,0) \frac{\partial f}{\partial x}=\lim_{h\to\ 0}\frac{f(h,0)}{h}=0 \frac{\partial f}{\partial y}=\lim_{h\to\ 0}\frac{f(0,h)}{h}=0 (0,0)","['multivariable-calculus', 'partial-derivative']"
13,"Proof that $\lim_{(x,y)\rightarrow(0,0)} f(x,y) = 2x + 2y = 0$ exists",Proof that  exists,"\lim_{(x,y)\rightarrow(0,0)} f(x,y) = 2x + 2y = 0","I'm trying to prove via $\delta$-$\epsilon$ argument that: $$\lim_{(x,y)\rightarrow(0,0)} f(x,y) = 2x + 2y = 0$$ What i do is, given that: $\lvert x \rvert \leq \sqrt{x^2 + y^2} < \delta$ and $\lvert y \rvert \leq \sqrt{x^2 + y^2} < \delta$ I sum the two inequalities, giving: $$\lvert x\rvert + \lvert y \rvert < 2\delta$$ Multiplying both sides by 2, and using the triangular inequality: $$2\lvert x + y\rvert \leq 2\lvert x \rvert + 2\lvert y \rvert < 4\delta$$ So using $\delta = \frac{\epsilon}{4}$ the proof is complete. Did i made any mistakes?","I'm trying to prove via $\delta$-$\epsilon$ argument that: $$\lim_{(x,y)\rightarrow(0,0)} f(x,y) = 2x + 2y = 0$$ What i do is, given that: $\lvert x \rvert \leq \sqrt{x^2 + y^2} < \delta$ and $\lvert y \rvert \leq \sqrt{x^2 + y^2} < \delta$ I sum the two inequalities, giving: $$\lvert x\rvert + \lvert y \rvert < 2\delta$$ Multiplying both sides by 2, and using the triangular inequality: $$2\lvert x + y\rvert \leq 2\lvert x \rvert + 2\lvert y \rvert < 4\delta$$ So using $\delta = \frac{\epsilon}{4}$ the proof is complete. Did i made any mistakes?",,"['multivariable-calculus', 'proof-verification']"
14,Is there a non-affine harmonic map with constant determinant?,Is there a non-affine harmonic map with constant determinant?,,Does there exist a smooth harmonic map $f:\mathbb{R}^2 \to \mathbb{R}^2$ such that: $\det(df)$ is constant. $f$ is not affine. Is there such a map with $\det(df) \neq 0$? ($f$ is harmonic if each of its two components is a harmonic function).,Does there exist a smooth harmonic map $f:\mathbb{R}^2 \to \mathbb{R}^2$ such that: $\det(df)$ is constant. $f$ is not affine. Is there such a map with $\det(df) \neq 0$? ($f$ is harmonic if each of its two components is a harmonic function).,,"['multivariable-calculus', 'harmonic-functions']"
15,Is there always a direction in which the directional derivative of a function is zero?,Is there always a direction in which the directional derivative of a function is zero?,,"Simple question like the title asked, Is there always a direction in which the directional derivative of a function is zero? At origin, wouldn't directional derivative always be 0?","Simple question like the title asked, Is there always a direction in which the directional derivative of a function is zero? At origin, wouldn't directional derivative always be 0?",,"['calculus', 'multivariable-calculus']"
16,How to read the Jacobian (determinant) shorthand notation?,How to read the Jacobian (determinant) shorthand notation?,,"Lets say we have a function $f : \mathbb{R}^3\rightarrow \mathbb{R}^3$ , as defined below, with its value being denoted as $(a, b, c)$ for convenient reference. $$f(x,y,z) = (x^2, y^2, z^2) = (a, b, c)$$ The Jacobian matrix of $f$ and subsequently the Jacobian determinant would then be written: $$ \begin{bmatrix} a'\\  b'\\  c' \end{bmatrix} = \begin{bmatrix} \frac {\partial a}{\partial x} & \frac {\partial a}{\partial y} & \frac {\partial a}{\partial z}\\  \frac {\partial b}{\partial x} &\frac {\partial b}{\partial y} & \frac {\partial b}{\partial z}\\  \frac {\partial c}{\partial x} &\frac {\partial c}{\partial y} & \frac {\partial c}{\partial z} \end{bmatrix} = \begin{bmatrix} 2x & 0 &0 \\  0 & 2y &0 \\  0 & 0 &2z  \end{bmatrix} $$ $$ \begin{vmatrix} a'\\  b'\\  c' \end{vmatrix} = \begin{vmatrix} \frac {\partial a}{\partial x} & \frac {\partial a}{\partial y} & \frac {\partial a}{\partial z}\\  \frac {\partial b}{\partial x} &\frac {\partial b}{\partial y} & \frac {\partial b}{\partial z}\\  \frac {\partial c}{\partial x} &\frac {\partial c}{\partial y} & \frac {\partial c}{\partial z} \end{vmatrix} = \begin{vmatrix} 2x & 0 &0 \\  0 & 2y &0 \\  0 & 0 &2z  \end{vmatrix} = 2x2y2z $$ Ok sure, this makes sense. It's kind of just like normal calculus but expanding everything out into a matrix. Now I look at the shorthand notation for the Jacobian determinant: $$ \frac {\partial(a,b,c)}{\partial(x,y,z)} = 2x2y2z $$ Where did this even come from? Why are there partials there. How does it convey the same amount of information? How do I even read this ""shorthand notation"". It just seems to have appeared out of nowhere. How do I read it, is there a common rule of thumb to follow? How did this arguably rather cryptic notation come about?","Lets say we have a function , as defined below, with its value being denoted as for convenient reference. The Jacobian matrix of and subsequently the Jacobian determinant would then be written: Ok sure, this makes sense. It's kind of just like normal calculus but expanding everything out into a matrix. Now I look at the shorthand notation for the Jacobian determinant: Where did this even come from? Why are there partials there. How does it convey the same amount of information? How do I even read this ""shorthand notation"". It just seems to have appeared out of nowhere. How do I read it, is there a common rule of thumb to follow? How did this arguably rather cryptic notation come about?","f : \mathbb{R}^3\rightarrow \mathbb{R}^3 (a, b, c) f(x,y,z) = (x^2, y^2, z^2) = (a, b, c) f 
\begin{bmatrix}
a'\\ 
b'\\ 
c'
\end{bmatrix}
=
\begin{bmatrix}
\frac {\partial a}{\partial x} & \frac {\partial a}{\partial y} & \frac {\partial a}{\partial z}\\ 
\frac {\partial b}{\partial x} &\frac {\partial b}{\partial y} & \frac {\partial b}{\partial z}\\ 
\frac {\partial c}{\partial x} &\frac {\partial c}{\partial y} & \frac {\partial c}{\partial z}
\end{bmatrix}
=
\begin{bmatrix}
2x & 0 &0 \\ 
0 & 2y &0 \\ 
0 & 0 &2z 
\end{bmatrix}
 
\begin{vmatrix}
a'\\ 
b'\\ 
c'
\end{vmatrix}
=
\begin{vmatrix}
\frac {\partial a}{\partial x} & \frac {\partial a}{\partial y} & \frac {\partial a}{\partial z}\\ 
\frac {\partial b}{\partial x} &\frac {\partial b}{\partial y} & \frac {\partial b}{\partial z}\\ 
\frac {\partial c}{\partial x} &\frac {\partial c}{\partial y} & \frac {\partial c}{\partial z}
\end{vmatrix}
=
\begin{vmatrix}
2x & 0 &0 \\ 
0 & 2y &0 \\ 
0 & 0 &2z 
\end{vmatrix}
=
2x2y2z
 
\frac {\partial(a,b,c)}{\partial(x,y,z)} = 2x2y2z
","['multivariable-calculus', 'notation', 'terminology', 'jacobian']"
17,"If the limit of a multivariable function is identical along a certain class of paths, can we claim the existence of the limit?","If the limit of a multivariable function is identical along a certain class of paths, can we claim the existence of the limit?",,"I'm a TA in multivariable calculus course, and I met the following problem. If when $(x,y)$ goes to origin along paths $y=mx^k$ and $x=0$ for all $m,k$, $f(x,y)$ along all these paths converges to a same number, can we conclude $\lim_{(x,y) \rightarrow (0,0)} f(x,y)$ exist? If not, what class of paths where the condition is satisfied, is enough to guarantee the existence of a limit of a function? The purpose of this question is to find some sort of the general rule for students to see limit exists without using $\epsilon-\delta$ language or polar coordinate system.  In fact, it's clear that if  $$\lim_{r \rightarrow 0}f(rcost,rsint)$$converges to the same number uniformly for all $t$, then by the negation of the definition of limit, $$\lim_{(x,y) \rightarrow (0,0)}f(x,y)$$ exists. But this is not what I want, because students cannot understand. I really appreciate if anyone can give me any comments or ideas of this question. Thank you very much in advance!","I'm a TA in multivariable calculus course, and I met the following problem. If when $(x,y)$ goes to origin along paths $y=mx^k$ and $x=0$ for all $m,k$, $f(x,y)$ along all these paths converges to a same number, can we conclude $\lim_{(x,y) \rightarrow (0,0)} f(x,y)$ exist? If not, what class of paths where the condition is satisfied, is enough to guarantee the existence of a limit of a function? The purpose of this question is to find some sort of the general rule for students to see limit exists without using $\epsilon-\delta$ language or polar coordinate system.  In fact, it's clear that if  $$\lim_{r \rightarrow 0}f(rcost,rsint)$$converges to the same number uniformly for all $t$, then by the negation of the definition of limit, $$\lim_{(x,y) \rightarrow (0,0)}f(x,y)$$ exists. But this is not what I want, because students cannot understand. I really appreciate if anyone can give me any comments or ideas of this question. Thank you very much in advance!",,"['real-analysis', 'multivariable-calculus']"
18,Parameterization of a torus,Parameterization of a torus,,"Given that the parameterization of a torus is given by: $x(\theta,\phi) = (R + r\cos(\theta))\cos(\phi)$ $y(\theta,\phi) = (R + r\cos(\theta))\sin(\phi)$ $z(\theta,\phi) = r\sin(\theta)$ and the equation of a torus in Cartesian coordinates is given by: $(R - \sqrt{x^2 + y^2})^2 + z^2 = r^2$ Where $R$ represents the major radius and $r$ the minor radius. How does one show that the parameterization equations satisfy the Cartesian coordinates? I've tried plugging in and using trig identities, but keep getting stuck. Also, how would I calculate the volume of a general torus using a triple integral in Cartesian coordinates?","Given that the parameterization of a torus is given by: $x(\theta,\phi) = (R + r\cos(\theta))\cos(\phi)$ $y(\theta,\phi) = (R + r\cos(\theta))\sin(\phi)$ $z(\theta,\phi) = r\sin(\theta)$ and the equation of a torus in Cartesian coordinates is given by: $(R - \sqrt{x^2 + y^2})^2 + z^2 = r^2$ Where $R$ represents the major radius and $r$ the minor radius. How does one show that the parameterization equations satisfy the Cartesian coordinates? I've tried plugging in and using trig identities, but keep getting stuck. Also, how would I calculate the volume of a general torus using a triple integral in Cartesian coordinates?",,"['multivariable-calculus', 'trigonometry', 'analytic-geometry', 'parametric']"
19,Why isn't the fundamental theorem of line integrals applicable here?,Why isn't the fundamental theorem of line integrals applicable here?,,"The question is as follows: Given vector field $$V = \left(\frac{1-y}{x^2 + (y-1)^2}, \frac{x}{x^2+(y-1)^2}\right)$$ Evaluate $$\int_{l_1}V \bullet dr\text{, }\int_{l_2}V \bullet dr$$ Where $l_1$ and $l_2$ are given as $$l_1: x^2+(y-1)^2=1\text{, } l_2: x^2+(y-4)^2=1$$ According to the answer key given by my TA, the first integral can't be evaluated using the fundamental theorem of line integrals. He mentions the fact that $V$ is conservative, except at (0,1) where it is not defined, but the first curve doesn't pass through this point. Using a verbose method which isn't really the focus here, he finds that the integral evaluates to 2$\pi$, not zero, which is what conservativity would imply. However, he evaluates the second integral using the fundamental theorem and gets zero. I believe both of these answers are correct, as I didn't find any errors in his evaluation of the first integral. My question is, why can the fundamental theorem be applied to the second integral but not the first one?","The question is as follows: Given vector field $$V = \left(\frac{1-y}{x^2 + (y-1)^2}, \frac{x}{x^2+(y-1)^2}\right)$$ Evaluate $$\int_{l_1}V \bullet dr\text{, }\int_{l_2}V \bullet dr$$ Where $l_1$ and $l_2$ are given as $$l_1: x^2+(y-1)^2=1\text{, } l_2: x^2+(y-4)^2=1$$ According to the answer key given by my TA, the first integral can't be evaluated using the fundamental theorem of line integrals. He mentions the fact that $V$ is conservative, except at (0,1) where it is not defined, but the first curve doesn't pass through this point. Using a verbose method which isn't really the focus here, he finds that the integral evaluates to 2$\pi$, not zero, which is what conservativity would imply. However, he evaluates the second integral using the fundamental theorem and gets zero. I believe both of these answers are correct, as I didn't find any errors in his evaluation of the first integral. My question is, why can the fundamental theorem be applied to the second integral but not the first one?",,"['multivariable-calculus', 'vector-fields', 'line-integrals']"
20,Graphs of functions and level sets,Graphs of functions and level sets,,"While going through the first few chapters of my multivariable calculus book, I came across the following: The graph of a function of two variables is a surface in $\mathbb{R}^3$ and is a level set of a function of three variables. However, not all level sets of functions of three variables are graphs of functions of two variables. I am finding trouble grasping the notion of this intuitively. Is it actually impossible to find an arbitrary graph that corresponds to a given level set (for the cases given above)? Could I possibly ask for a concrete example that demonstrates the statement?","While going through the first few chapters of my multivariable calculus book, I came across the following: The graph of a function of two variables is a surface in $\mathbb{R}^3$ and is a level set of a function of three variables. However, not all level sets of functions of three variables are graphs of functions of two variables. I am finding trouble grasping the notion of this intuitively. Is it actually impossible to find an arbitrary graph that corresponds to a given level set (for the cases given above)? Could I possibly ask for a concrete example that demonstrates the statement?",,[]
21,What is the set $\mathbb{R}^{\nvDash}$ defined as?,What is the set  defined as?,\mathbb{R}^{\nvDash},"The set $\mathbb{R}^{\nvDash}$ is used in my multivariable-calculus assignment, but I do not know what the superscript '$\nvDash$'  means on the set of real numbers. The context in which it used is that a curve lies in set domain and I must evaluate its line integral. Thanks! -Bryan","The set $\mathbb{R}^{\nvDash}$ is used in my multivariable-calculus assignment, but I do not know what the superscript '$\nvDash$'  means on the set of real numbers. The context in which it used is that a curve lies in set domain and I must evaluate its line integral. Thanks! -Bryan",,"['multivariable-calculus', 'notation']"
22,Choosing the definition of $\frac{\partial^2}{\partial x\partial y}$,Choosing the definition of,\frac{\partial^2}{\partial x\partial y},"Today, I answered this question and discovered that the definition of $\dfrac{\partial^2}{\partial x\partial y}$ is a matter of convention. For example this .edu link and this other .edu link use the convention $$\frac{\partial^2}{\partial x\partial y}:=\frac{\partial}{\partial y}\left(\frac{\partial}{\partial x}\right) \qquad \qquad (1)$$ However, this wikipedia article , this .edu link and this other .edu link use the convention $$\frac{\partial^2}{\partial x\partial y}:=\frac{\partial}{\partial x}\left(\frac{\partial}{\partial y}\right) \qquad \qquad (2)$$ Since apparently the definition has not been fixed yet, I can imagine that both definitions has advantages/disadvantages depending on the context they are used. However, I can't find any of these situation. Question: What are the pros and cons of each definition? EDIT: At the time I write this edit, it seems that even in the wikipedia article about partial derivatives, there is a ""contradiction"", here definition $(1)$ is used but here they use definition $(2)$...","Today, I answered this question and discovered that the definition of $\dfrac{\partial^2}{\partial x\partial y}$ is a matter of convention. For example this .edu link and this other .edu link use the convention $$\frac{\partial^2}{\partial x\partial y}:=\frac{\partial}{\partial y}\left(\frac{\partial}{\partial x}\right) \qquad \qquad (1)$$ However, this wikipedia article , this .edu link and this other .edu link use the convention $$\frac{\partial^2}{\partial x\partial y}:=\frac{\partial}{\partial x}\left(\frac{\partial}{\partial y}\right) \qquad \qquad (2)$$ Since apparently the definition has not been fixed yet, I can imagine that both definitions has advantages/disadvantages depending on the context they are used. However, I can't find any of these situation. Question: What are the pros and cons of each definition? EDIT: At the time I write this edit, it seems that even in the wikipedia article about partial derivatives, there is a ""contradiction"", here definition $(1)$ is used but here they use definition $(2)$...",,"['multivariable-calculus', 'derivatives', 'notation', 'definition', 'convention']"
23,Is there a relationship between curl and area?,Is there a relationship between curl and area?,,"The cross product of two vectors is a new vector which lies on a new direction perpendicular to the plane of the multiplicand vectors. Its magnitude is the area of the parallelogram formed between these multiplicand vectors. On the other hand, the definitions of curl operation are all related with rotations. Since this operation also involves a cross product $(\nabla\times\vec A)$, I wonder if there is a relationship between curl operation and some kind of (may be infinitesimal) area?","The cross product of two vectors is a new vector which lies on a new direction perpendicular to the plane of the multiplicand vectors. Its magnitude is the area of the parallelogram formed between these multiplicand vectors. On the other hand, the definitions of curl operation are all related with rotations. Since this operation also involves a cross product $(\nabla\times\vec A)$, I wonder if there is a relationship between curl operation and some kind of (may be infinitesimal) area?",,[]
24,Smoothness at the origin of a radial function obtained by rotating an even function,Smoothness at the origin of a radial function obtained by rotating an even function,,"Let $f:\mathbb{R}\to\mathbb{R}$ be a smooth even function. Define $g:\mathbb{R}^n\to\mathbb{R}$ by $g(x)=f(|x|)$ . How to show that $g$ is smooth at the origin? We can calculate $$\frac{\partial g}{\partial x^i}=\begin{cases} f'(|x|)\frac{x^i}{|x|} & \text{if $x\neq 0$} \\ 0 & \text{if $x=0$.}\end{cases}$$ Then since $f'(0)=0$ , we see that $\partial g/\partial x^i$ is continuous everywhere, so that $g$ is $C^1$ . We can continue and calculate $\partial^2 g/\partial x^j\partial x^i$ and check that it is everywhere continuous, but it's annoying, and then we've only shown that $g$ is $C^2$ , and I don't see how to induct. Is there an easier way?","Let be a smooth even function. Define by . How to show that is smooth at the origin? We can calculate Then since , we see that is continuous everywhere, so that is . We can continue and calculate and check that it is everywhere continuous, but it's annoying, and then we've only shown that is , and I don't see how to induct. Is there an easier way?",f:\mathbb{R}\to\mathbb{R} g:\mathbb{R}^n\to\mathbb{R} g(x)=f(|x|) g \frac{\partial g}{\partial x^i}=\begin{cases} f'(|x|)\frac{x^i}{|x|} & \text{if x\neq 0} \\ 0 & \text{if x=0.}\end{cases} f'(0)=0 \partial g/\partial x^i g C^1 \partial^2 g/\partial x^j\partial x^i g C^2,"['real-analysis', 'calculus', 'multivariable-calculus', 'derivatives', 'symmetry']"
25,Show that constant curvature $\kappa = 1/r$ is necessary and sufficient that the curve is a circular arc of radius $r$,Show that constant curvature  is necessary and sufficient that the curve is a circular arc of radius,\kappa = 1/r r,"We have to prove that a curve has constant curvature $\kappa = 1/r$ if  and only if it is in a circular arc of radius $r$. I am confused because doesn't a helix also have a constant curvature given by $\frac{a}{a^2 + b^2}$ where $a$ is the radius of the circle and $b$ is the rate of ascension? I feel like an additional assumption here is needed (such as that the curve is planar, thus torsion $\tau = 0$). Indeed, using the assumption $\tau = 0$ and Frenet-Serret I found a differential equation involving the Normal vector $N$ with a trigonometric solution. I wasn't sure what to do from here, however. Edit: The question definitely asks for curves (doesn't specify plane curve) so I'll ask the TA tomorrow. From now assume that it wants only planar curves, so $\tau = 0$. Can somebody help me with that solution?","We have to prove that a curve has constant curvature $\kappa = 1/r$ if  and only if it is in a circular arc of radius $r$. I am confused because doesn't a helix also have a constant curvature given by $\frac{a}{a^2 + b^2}$ where $a$ is the radius of the circle and $b$ is the rate of ascension? I feel like an additional assumption here is needed (such as that the curve is planar, thus torsion $\tau = 0$). Indeed, using the assumption $\tau = 0$ and Frenet-Serret I found a differential equation involving the Normal vector $N$ with a trigonometric solution. I wasn't sure what to do from here, however. Edit: The question definitely asks for curves (doesn't specify plane curve) so I'll ask the TA tomorrow. From now assume that it wants only planar curves, so $\tau = 0$. Can somebody help me with that solution?",,['multivariable-calculus']
26,Without Stokes's Theorem - Calculate $\iint_S \operatorname{curl} \mathbf{F} \cdot\; d\mathbf{S}$ for $\mathbf{F} = yz^2\mathbf{i}$ - 2013 10C,Without Stokes's Theorem - Calculate  for  - 2013 10C,\iint_S \operatorname{curl} \mathbf{F} \cdot\; d\mathbf{S} \mathbf{F} = yz^2\mathbf{i},"2013 10C. Consider the bounded surface S that is the union of $x^2 + y^2 = 4$ for $−2 \le z \le 2$   and $(4 − z)^2 = x^2 + y^2 $ for $2 \le z \le 4.$ Sketch the surface.   Use suitable parametrisations for the two parts of S to verify Stokes’s Theorem for    for $\mathbf{F} = (yz^2,0,0)$. picture Herein, I enquire only about directly computing $ \iint_S (\nabla × F )· d\mathbf{S}$. Denote the $2 \le z \le 4$ cone P, and the $-2 \le z \le 2$ cylinder C. I use only the first paragraph of this . Then $\mathbf{\nabla × F} = (0, 2yz, -z^2)$,   $ \iint_P (\nabla × F ) · d\mathbf{S} = \iint_{x^2 + y^2 \le z^2, z = 2} (\nabla × F ) \cdot \color{darkred}{\mathbf{n}} \, dA \\ = \iint_{x^2 + y^2 \le z^2, z = 2} (♦, ♦, \underbrace{-z^2}_{=-4} ) \cdot \color{darkred}{(0, 0,-1)} \, dA = \iint_{x^2 + y^2 \le z^2, z = 2} 4 dA = 4\pi(2)^2 $. ♦ denote objects that don't need to be computed because they're dot-producted with 0. $\large{2.}$ To ellya especially, is it necessary to parameterise the P piece? Comparing my work to yours, I see that we differ only by a negative sign? Yet my work has far fewer steps. Does it not function?  How would one determine that the correct normal vector is $\color{green}{\mathbf{n} = (0, 0, 1)}$?","2013 10C. Consider the bounded surface S that is the union of $x^2 + y^2 = 4$ for $−2 \le z \le 2$   and $(4 − z)^2 = x^2 + y^2 $ for $2 \le z \le 4.$ Sketch the surface.   Use suitable parametrisations for the two parts of S to verify Stokes’s Theorem for    for $\mathbf{F} = (yz^2,0,0)$. picture Herein, I enquire only about directly computing $ \iint_S (\nabla × F )· d\mathbf{S}$. Denote the $2 \le z \le 4$ cone P, and the $-2 \le z \le 2$ cylinder C. I use only the first paragraph of this . Then $\mathbf{\nabla × F} = (0, 2yz, -z^2)$,   $ \iint_P (\nabla × F ) · d\mathbf{S} = \iint_{x^2 + y^2 \le z^2, z = 2} (\nabla × F ) \cdot \color{darkred}{\mathbf{n}} \, dA \\ = \iint_{x^2 + y^2 \le z^2, z = 2} (♦, ♦, \underbrace{-z^2}_{=-4} ) \cdot \color{darkred}{(0, 0,-1)} \, dA = \iint_{x^2 + y^2 \le z^2, z = 2} 4 dA = 4\pi(2)^2 $. ♦ denote objects that don't need to be computed because they're dot-producted with 0. $\large{2.}$ To ellya especially, is it necessary to parameterise the P piece? Comparing my work to yours, I see that we differ only by a negative sign? Yet my work has far fewer steps. Does it not function?  How would one determine that the correct normal vector is $\color{green}{\mathbf{n} = (0, 0, 1)}$?",,[]
27,continuity of a functions involving trig functions,continuity of a functions involving trig functions,,"LEt $f$ be given as $$ f(x,y) = \begin{cases} \frac{ \sin x - \sin y }{x-y}, & \text{if }\text{ $x \neq y $} \\ \cos x, & \text{if } x \text{ $=y$} \end{cases} $$ Notice when $y=x$, along the diagonal, $f = \cos x$ which is continuous everywhere. if $y \neq x$, then $f(x,y) = \frac{ \sin x - \sin y}{x-y}$, which I claim is continuous also everywhere. To show this, let $\epsilon > 0$ be given. and take $(x,y), (u,v) \in \mathbb{R}^2 \setminus \{ (t,t) \} $. We want to estimate $$ \left| \frac{ \sin x - \sin y }{x-y} - \frac{ \sin u - \sin v }{u-v} \right|$$ given that $||(x,y) - (u,v)|| = \sqrt{ (x-u)^2 + (y-v)^2} < \delta $ for some $\delta > 0 $. How can I estimate so that we can choose an appropiate $\delta$ ?","LEt $f$ be given as $$ f(x,y) = \begin{cases} \frac{ \sin x - \sin y }{x-y}, & \text{if }\text{ $x \neq y $} \\ \cos x, & \text{if } x \text{ $=y$} \end{cases} $$ Notice when $y=x$, along the diagonal, $f = \cos x$ which is continuous everywhere. if $y \neq x$, then $f(x,y) = \frac{ \sin x - \sin y}{x-y}$, which I claim is continuous also everywhere. To show this, let $\epsilon > 0$ be given. and take $(x,y), (u,v) \in \mathbb{R}^2 \setminus \{ (t,t) \} $. We want to estimate $$ \left| \frac{ \sin x - \sin y }{x-y} - \frac{ \sin u - \sin v }{u-v} \right|$$ given that $||(x,y) - (u,v)|| = \sqrt{ (x-u)^2 + (y-v)^2} < \delta $ for some $\delta > 0 $. How can I estimate so that we can choose an appropiate $\delta$ ?",,['calculus']
28,"Prove that a mapping $f:[-1,1]^2\to\mathbb R^2$ with certain properties has the value $(0,0)$.",Prove that a mapping  with certain properties has the value .,"f:[-1,1]^2\to\mathbb R^2 (0,0)","The mapping $f:[-1,1]^2\to\mathbb R^2$ is known to be continuous. Also the image of the upper edge of the rectangle is contained in the upper half-plane, the left edge's image is contained in the left half-plane, and so on for the bottom and right edge. Formally speaking this means: If $f_1,f_2:[-1,1]^2\to\mathbb R$ are defined to be the components of $f$, i. e. $$f(x,y)=(f_1(x,y),f_2(x,y))\,,$$ then the following conditions hold for each $x\in[-1,1]$ it holds $f_2(x,1)>0$, for each $y\in[-1,1]$ it holds $f_1(-1,y)<0$, for each $x\in[-1,1]$ it holds $f_2(x,-1)<0$ and for each $y\in[-1,1]$ it holds $f_1(1,y)>0$. Under these assumptions prove that there exist $x_0,y_0\in[-1,1]$ such that $f(x_0,y_0)=(0,0)$. This appears to be obvious intuitively, but I'm looking for a formal and rigorous proof.","The mapping $f:[-1,1]^2\to\mathbb R^2$ is known to be continuous. Also the image of the upper edge of the rectangle is contained in the upper half-plane, the left edge's image is contained in the left half-plane, and so on for the bottom and right edge. Formally speaking this means: If $f_1,f_2:[-1,1]^2\to\mathbb R$ are defined to be the components of $f$, i. e. $$f(x,y)=(f_1(x,y),f_2(x,y))\,,$$ then the following conditions hold for each $x\in[-1,1]$ it holds $f_2(x,1)>0$, for each $y\in[-1,1]$ it holds $f_1(-1,y)<0$, for each $x\in[-1,1]$ it holds $f_2(x,-1)<0$ and for each $y\in[-1,1]$ it holds $f_1(1,y)>0$. Under these assumptions prove that there exist $x_0,y_0\in[-1,1]$ such that $f(x_0,y_0)=(0,0)$. This appears to be obvious intuitively, but I'm looking for a formal and rigorous proof.",,"['real-analysis', 'multivariable-calculus', 'algebraic-topology', 'continuity', 'roots']"
29,Derivatives of multivariable functions,Derivatives of multivariable functions,,"I would like to make a few statements about a simple object - the derivative of a univariate function - and apply and relate its features and my understaning of them to multivariate functions. Univariate functions. A derivative of a real function $f: {\mathrm R} \to {\mathrm R}$ at the point $a \in {\mathrm R}$ is the slope of the function at this point; that is how much the function value changes with respect to the change in variable, or $$f'(a) = \lim_{h\to0} \frac{f(a + h) - f(a)}{h}.$$ The derivate of this a real function $f: {\mathrm R} \to {\mathrm R}$ is the function $$f': a \mapsto f'(a)$$ that maps a point to the slope of the funtion $f$ at that point. The derivative at a point is not itself the tangent to the function graph at that point, but it is closely related to it. The tangent at the point $a$ can be expressed as $$t(x) = f(a) + f'(a)(x-a),$$ which happens to be the best linear approximation of the function $f$ around $a$, or the first-degree Taylor polynomial $T^{f,a}_1$. The function $f'(a)(x-a)$ is linear in $x$. Multivariate functions. Let $f: {\mathrm R}^n \to {\mathrm R}^m$, where $m,\ n \in {\mathrm N}$. We can consider partial derivatives of $f$ at $a \in {\mathrm R}^n$, defined for example as $${\partial f\over\partial x_i} = \lim_{h \to 0} \frac{f(a + h{\bf e^i}) - f(a)}{h},$$ that is the derivative of the function at $a$ with respect to $x_i$ and other variables held constant, where ${\bf e^i} = (0, \dots, 0, 1, 0, \dots, 0)$ ($1$ is $i$-th from the left). These are derivatives of single-variable partial functions and therefore the same applies to them what I have written in the first section. The gradient of the function at a point is the vector of partial derivatives at that point, i.e. $$\nabla f(a) = \Big({\partial f\over x_1}, \dots, {\partial f\over x_n}\Big).$$ Its geometrical meaning is that it points in the direction of the steepest growth while its value is the growth in that direction. The equivalent of the derivative at a point seems to be what is called the total differential at that point. If $L$ is total differential at $a \in {\mathrm R}^n$, then $$\lim_{\bf h \to 0} \frac{||f(a + h) - f(a) - L(h)||}{||h||} = 0,$$ where $||\cdot||$ is the Euclidian norm, which means that $L$ has the ""approximative property"" - it appoximates the difference $f(a + h) - f(a)$ locally. If the total differential exists, it can be expressed as $L(h) = \nabla{f}(a) \cdot h$, where $\cdot$ is the dot product. (I think: the total differential does not approximate the function itself - this resembles ""derivative of a function at a point"" for univariate functions.) I come to understand that the derivative at a point of a multivariate function can be defined exactly the same way as the total differential at a point. (For some reason we have only defined the total differential for functions ${\mathrm R}^n \to {\mathrm R}$. Is this related to math, or is it a problem of terminology?) When I try to look at the derivative $f'$ of the function f , I should see that: $f': {\mathrm R}^n \to {\mathscr L}(\mathrm{R}^n, \mathrm{R}^m)$ - and I do, this simply states that the first derivative at a point is a local linear approximation of the original function f, but also that $f'': {\mathrm R}^n \to {\mathscr L}\Big({\mathrm R}^n, {\mathscr L}({\mathrm R}^n, {\mathrm R}^m)\Big)$, which is driving me crazy. I would like to ask: Why is the total differential not called simply the derivative ? Why is it true that $f'': {\mathrm R}^n \to {\mathscr L}\Big({\mathrm R}^n, {\mathscr L}({\mathrm R}^n, {\mathrm R}^m)\Big)$? I need an intuitive way of understanding what a ${\mathscr L}\Big({\mathrm R}^n, {\mathscr L}({\mathrm R}^n, {\mathrm R}^m)\Big)$ is. Thanks!","I would like to make a few statements about a simple object - the derivative of a univariate function - and apply and relate its features and my understaning of them to multivariate functions. Univariate functions. A derivative of a real function $f: {\mathrm R} \to {\mathrm R}$ at the point $a \in {\mathrm R}$ is the slope of the function at this point; that is how much the function value changes with respect to the change in variable, or $$f'(a) = \lim_{h\to0} \frac{f(a + h) - f(a)}{h}.$$ The derivate of this a real function $f: {\mathrm R} \to {\mathrm R}$ is the function $$f': a \mapsto f'(a)$$ that maps a point to the slope of the funtion $f$ at that point. The derivative at a point is not itself the tangent to the function graph at that point, but it is closely related to it. The tangent at the point $a$ can be expressed as $$t(x) = f(a) + f'(a)(x-a),$$ which happens to be the best linear approximation of the function $f$ around $a$, or the first-degree Taylor polynomial $T^{f,a}_1$. The function $f'(a)(x-a)$ is linear in $x$. Multivariate functions. Let $f: {\mathrm R}^n \to {\mathrm R}^m$, where $m,\ n \in {\mathrm N}$. We can consider partial derivatives of $f$ at $a \in {\mathrm R}^n$, defined for example as $${\partial f\over\partial x_i} = \lim_{h \to 0} \frac{f(a + h{\bf e^i}) - f(a)}{h},$$ that is the derivative of the function at $a$ with respect to $x_i$ and other variables held constant, where ${\bf e^i} = (0, \dots, 0, 1, 0, \dots, 0)$ ($1$ is $i$-th from the left). These are derivatives of single-variable partial functions and therefore the same applies to them what I have written in the first section. The gradient of the function at a point is the vector of partial derivatives at that point, i.e. $$\nabla f(a) = \Big({\partial f\over x_1}, \dots, {\partial f\over x_n}\Big).$$ Its geometrical meaning is that it points in the direction of the steepest growth while its value is the growth in that direction. The equivalent of the derivative at a point seems to be what is called the total differential at that point. If $L$ is total differential at $a \in {\mathrm R}^n$, then $$\lim_{\bf h \to 0} \frac{||f(a + h) - f(a) - L(h)||}{||h||} = 0,$$ where $||\cdot||$ is the Euclidian norm, which means that $L$ has the ""approximative property"" - it appoximates the difference $f(a + h) - f(a)$ locally. If the total differential exists, it can be expressed as $L(h) = \nabla{f}(a) \cdot h$, where $\cdot$ is the dot product. (I think: the total differential does not approximate the function itself - this resembles ""derivative of a function at a point"" for univariate functions.) I come to understand that the derivative at a point of a multivariate function can be defined exactly the same way as the total differential at a point. (For some reason we have only defined the total differential for functions ${\mathrm R}^n \to {\mathrm R}$. Is this related to math, or is it a problem of terminology?) When I try to look at the derivative $f'$ of the function f , I should see that: $f': {\mathrm R}^n \to {\mathscr L}(\mathrm{R}^n, \mathrm{R}^m)$ - and I do, this simply states that the first derivative at a point is a local linear approximation of the original function f, but also that $f'': {\mathrm R}^n \to {\mathscr L}\Big({\mathrm R}^n, {\mathscr L}({\mathrm R}^n, {\mathrm R}^m)\Big)$, which is driving me crazy. I would like to ask: Why is the total differential not called simply the derivative ? Why is it true that $f'': {\mathrm R}^n \to {\mathscr L}\Big({\mathrm R}^n, {\mathscr L}({\mathrm R}^n, {\mathrm R}^m)\Big)$? I need an intuitive way of understanding what a ${\mathscr L}\Big({\mathrm R}^n, {\mathscr L}({\mathrm R}^n, {\mathrm R}^m)\Big)$ is. Thanks!",,"['real-analysis', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
30,Continuity of a function defined by an integral,Continuity of a function defined by an integral,,"Ok, Here's my question: Let $f(x,y)$ be defined and continuous on a $\le x \le b, c \le y\le d$, and $F(x)$ be defined >by the integral $$\int_c^d f(x,y)dy.$$ Prove that $F(x)$ is continuous on $[a,b]$. I think I want to show that since $f(x,y)$ is continuous on $[a,b]$, I can use proof by contradiction to get $F'(x)$ continuous on $[a,b]$, which would then imply that $F(x)$ is continuous. But How do I go about setting this up? Any hints would be great. Thank you in advance. Also, this is my first attempt to format everything properly, So I'm sorry if this didn't post properly.","Ok, Here's my question: Let $f(x,y)$ be defined and continuous on a $\le x \le b, c \le y\le d$, and $F(x)$ be defined >by the integral $$\int_c^d f(x,y)dy.$$ Prove that $F(x)$ is continuous on $[a,b]$. I think I want to show that since $f(x,y)$ is continuous on $[a,b]$, I can use proof by contradiction to get $F'(x)$ continuous on $[a,b]$, which would then imply that $F(x)$ is continuous. But How do I go about setting this up? Any hints would be great. Thank you in advance. Also, this is my first attempt to format everything properly, So I'm sorry if this didn't post properly.",,"['multivariable-calculus', 'improper-integrals']"
31,Tangent of implicit function (of two variables),Tangent of implicit function (of two variables),,"Wikipedia has the following : equation of the tangent line at a point $(a,b)$ such that $f(a,b) = 0$ (the implicit function) is given by: ${\partial f \over \partial x} (x-a) + {\partial f \over \partial y}(y-b) = 0$ I guess it's related to the implicit function theorem, which I know (that the said theorem exists, not that I can prove it myself or even be acquitted with it). Explanation or non-rigorous outline of proof would suffice.","Wikipedia has the following : equation of the tangent line at a point $(a,b)$ such that $f(a,b) = 0$ (the implicit function) is given by: ${\partial f \over \partial x} (x-a) + {\partial f \over \partial y}(y-b) = 0$ I guess it's related to the implicit function theorem, which I know (that the said theorem exists, not that I can prove it myself or even be acquitted with it). Explanation or non-rigorous outline of proof would suffice.",,"['calculus', 'real-analysis', 'multivariable-calculus', 'partial-derivative']"
32,"How to change variables in the function $\phi(r)=\int_{\partial B(x,r)}u(y)\;dS(y)$ to take out $r$ of the domain of integration?",How to change variables in the function  to take out  of the domain of integration?,"\phi(r)=\int_{\partial B(x,r)}u(y)\;dS(y) r","Let $\alpha(n)$ be the volume of the unit ball in $\mathbb{R}^n$, $U\subset\mathbb{R}^n$ an open set, $u\in C^2(U)$ a harmonic function, $x\in U$ and, for $r$ small enough, $$\phi(r)=\frac{1}{n\alpha(n)r^{n-1}}\int_{\partial B(x,r)}u(y)\;dS(y).$$ In order to calculate $\phi'(r)$ we need to change variables. According to the PDE Evans book (p. 26) , if we take $y=x+rz$ then we get $$\frac{1}{n\alpha(n)r^{n-1}}\int_{\partial B(x,r)}u(y)\;dS(y)=\frac{1}{n\alpha(n)}\int_{\partial B(0,1)}u(x+rz)\;dS(z).$$ Therefore, the determinant $|J|$ of the Jacobian matrix $J$ is $|J|=r^{n-1}$. My question is: since $u$ is a function of $n$ variables, shouldn't $J$ be a $n\text{-by-}n$ matrix? Thus, shouldn't we get $|J|=r^n$? I think it's a very elementary question, but I hope you help me. Thanks.","Let $\alpha(n)$ be the volume of the unit ball in $\mathbb{R}^n$, $U\subset\mathbb{R}^n$ an open set, $u\in C^2(U)$ a harmonic function, $x\in U$ and, for $r$ small enough, $$\phi(r)=\frac{1}{n\alpha(n)r^{n-1}}\int_{\partial B(x,r)}u(y)\;dS(y).$$ In order to calculate $\phi'(r)$ we need to change variables. According to the PDE Evans book (p. 26) , if we take $y=x+rz$ then we get $$\frac{1}{n\alpha(n)r^{n-1}}\int_{\partial B(x,r)}u(y)\;dS(y)=\frac{1}{n\alpha(n)}\int_{\partial B(0,1)}u(x+rz)\;dS(z).$$ Therefore, the determinant $|J|$ of the Jacobian matrix $J$ is $|J|=r^{n-1}$. My question is: since $u$ is a function of $n$ variables, shouldn't $J$ be a $n\text{-by-}n$ matrix? Thus, shouldn't we get $|J|=r^n$? I think it's a very elementary question, but I hope you help me. Thanks.",,"['multivariable-calculus', 'partial-differential-equations']"
33,"How does the gradient of a function show greatest slope for a function $f(x,y,z)$?",How does the gradient of a function show greatest slope for a function ?,"f(x,y,z)","$$\nabla f(x,y,z)=\frac{\partial f}{\partial x}\mathbf{i}+\frac{\partial f}{\partial y}\mathbf{j}+\frac{\partial f}{\partial z}\mathbf{k}$$ $\nabla$ is the gradient operator. $(\nabla f).\mathbf{r}$ is the rate of change of $f$ in the $\mathbf{r}$ direction. I have to prove that a scalar multiple of $\frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}+\frac{\partial f}{\partial z}$ is the fastest rate of change of $f$ at any point. Let $r$ be the unit vector $(a,b,c)$. $$a^2+b^2+c^2=1$$ $(\nabla f).\mathbf{r}=a.\frac{\partial f}{\partial x}+b\frac{\partial f}{\partial y}+c\frac{\partial f}{\partial z}$ How do we know this is maximized when $a=b=c$?","$$\nabla f(x,y,z)=\frac{\partial f}{\partial x}\mathbf{i}+\frac{\partial f}{\partial y}\mathbf{j}+\frac{\partial f}{\partial z}\mathbf{k}$$ $\nabla$ is the gradient operator. $(\nabla f).\mathbf{r}$ is the rate of change of $f$ in the $\mathbf{r}$ direction. I have to prove that a scalar multiple of $\frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}+\frac{\partial f}{\partial z}$ is the fastest rate of change of $f$ at any point. Let $r$ be the unit vector $(a,b,c)$. $$a^2+b^2+c^2=1$$ $(\nabla f).\mathbf{r}=a.\frac{\partial f}{\partial x}+b\frac{\partial f}{\partial y}+c\frac{\partial f}{\partial z}$ How do we know this is maximized when $a=b=c$?",,[]
34,"Find maximum and minimum of $f(x, y) = xy$ on $D = \left\{ (x,y) \in \mathbb{R}^2: x^2+2y^2 \leq 1 \right\}$",Find maximum and minimum of  on,"f(x, y) = xy D = \left\{ (x,y) \in \mathbb{R}^2: x^2+2y^2 \leq 1 \right\}","I'm kinda stuck on this one : Find the minimum and maximum of the given function $f$ on $D$, where $$f(x, y) = xy$$ and $$D = \left\{(x,y) \in \mathbb{R}^2 : x^2+2y^2 \leq 1 \right\}$$ I don't know what to do with this domain $D$. I counted the first derivatives and got only point $(0,0)$ as a possible maximum/minimum inside $D$ but what about the boundary of $D$? Should I parametrize this ellipse or how should I approach this? Thanks for your tips!","I'm kinda stuck on this one : Find the minimum and maximum of the given function $f$ on $D$, where $$f(x, y) = xy$$ and $$D = \left\{(x,y) \in \mathbb{R}^2 : x^2+2y^2 \leq 1 \right\}$$ I don't know what to do with this domain $D$. I counted the first derivatives and got only point $(0,0)$ as a possible maximum/minimum inside $D$ but what about the boundary of $D$? Should I parametrize this ellipse or how should I approach this? Thanks for your tips!",,"['multivariable-calculus', 'optimization']"
35,Line Integral of Every Positively Oriented Simple Closed Path - Green's Theorem,Line Integral of Every Positively Oriented Simple Closed Path - Green's Theorem,,"This question is from Example #5, Section 16.4 on P1059 of Calculus , 6th Ed, by James Stewart. Given Question: If $\mathbf{F}(x,y) = \left(\dfrac{-y}{x^2 + y^2}, \dfrac{x}{x^2 + y^2}\right)$, show that $\int_C \mathbf{F} \cdot d\mathbf{r} = 2\pi $ for every positively oriented simple closed path that encloses the origin. Part of the Given Solution: Since $C$ is an ARBITRARY closed path that encloses the origin, it's difficult to compute the given integral directly. So let's consider a counterclockwise circle $A$ with center the origin and radius $a$, where $a$ is chosen to be small enough that $A$ lies inside $C$, as indicated by the picture below. Let $D$ be the region bounded by $C$ and $A$. Then positively oriented $\partial D = C \cup (-A)$. So the version of Green Theorem's applied to regions with holes gives: $\int_C \mathbf{F} \cdot d\mathbf{r} + \int_{-A} \mathbf{F} \cdot d\mathbf{r} = \iint_D \underbrace{(\partial_x Q - \partial_y P)}_{\Large{= 0}} \, dA $   $\Longrightarrow \int_C \mathbf{F} \cdot d\mathbf{r} = \int_{A} \mathbf{F} \cdot d\mathbf{r}$. (Rest of solution omitted) $\Large{\text{Q1.}}$ I can't perceive how one would divine to construct $A$ to solve this problem. So I feel that my following solution is more intuitive. Is it really? However, I don't seem to need Green's Thoerem. How and where would it be applied? My solution: It's hard to calculate $\int_C \mathbf{F} \cdot d\mathbf{r} $. The idea then is to find one $A$ so that $(\dagger) \int_{A} \mathbf{F} \cdot d\mathbf{r}$ is easier to compute and $(\ddagger)\int_C \mathbf{F} \cdot d\mathbf{r} = \int_{A} \mathbf{F} \cdot d\mathbf{r}$. Now, $(\ddagger) \iff \int_C \mathbf{F} \cdot d\mathbf{r} \huge{\color{red}{-}} \normalsize\int_A \mathbf{F} \cdot d\mathbf{r} = 0 \iff \int_C \mathbf{F} \cdot d\mathbf{r}  +\int_{\huge{\color{red}{-}\normalsize{A}}} \mathbf{F} \cdot d\mathbf{r} = 0 $ $\iff \int_{\Large{\partial D} \normalsize{ \, = \, (C} \, \cup \huge{\color{red}{-}\normalsize{A})}} \mathbf{F} \cdot d\mathbf{r} = 0$. For $\partial D$ to be positively oriented, $C$ and $\color{red}{-}A$ must both be positively oriented $\Longrightarrow A$ must be NEGATIVELY oriented, so $D$ must be to the right of $A$. One convenient choice of $A$ would just be a counterclockwise circle with center the origin and the radius $a$. $\Large{\text{Q2.}}$ Why don't I get the requested answer if I pick $A$ as a counterclockwise ellipse enclosing the origin? In other words, $A$ is parameterised via $r(t) = (a\cos t, b\sin t), 0 \leq t \leq 2\pi $. Then $\int_{A} \mathbf{F} \cdot d\mathbf{r} = \int_0^{2 \pi} \left(\dfrac{-b\sin t}{a^2 \cos^2 t + b^2 \sin^2 t},\dfrac{a\cos t}{a^2 \cos^2 t + b^2 \sin^2 t} \right) \cdot (-a\sin t, b \cos t) \, dt$. $ = ab \int_0^{2 \pi} \dfrac{1}{a^2 \cos^2 t + b^2 \sin^2 t} \, dt $. If $a = b$, then the requested answer follows by inspection. But what if $a \neq b$? $\Large{\text{Supplementaries to Muphrid's Answer: }}$ $\Large{\text{Q1.1.}}$ To rephrase your answer, are you saying that my solution has to justify why there exists an $A \ni \int_C \mathbf{F} \cdot d\mathbf{r} = \int_{A} \mathbf{F} \cdot d\mathbf{r}. ?$  This is true because $\int_C \mathbf{F} \cdot d\mathbf{r} = \int_{A} \mathbf{F} \cdot d\mathbf{r} \iff \int_{\Large{\partial D} \normalsize{ \, = \, (C} \, \cup \huge{\color{red}{-}\normalsize{A})}} \mathbf{F} \cdot d\mathbf{r} = 0, $  where the last equation is true by Green's Theorem: $\int_{\Large{\partial D} \normalsize{ \, = \, (C} \, \cup \huge{\color{red}{-}\normalsize{A})}} \mathbf{F} \cdot d\mathbf{r} = \iint_D \underbrace{(\partial_x Q - \partial_y P)}_{\Large{= 0}} \, dA. $ $\Large{\text{Q1.2.}}$ I wrote that $\partial D \normalsize{ \, = \, (C} \, \cup \huge{\color{red}{-}\normalsize{A})} $. Are you saying that this is wrong and that $\partial D \normalsize{ \, \subsetneqq \, (C} \, \cup \huge{\color{red}{-}\normalsize{A})} $? $\Large{\text{Q1.3.}}$ I understand that $\mathbf{F}(0,0)$ is undefined. However, how does this imply that integrals enclosing the origin pick up $2\pi$? $\Large{\text{Q1.4.}}$ How is $(\nabla \times \mathbf{F}) \cdot d\mathbf{A} = \partial_x Q - \partial_y P $? How is $d\mathbf{A} = z \, dx \, dy ?$ $\Large{\text{Q2.1.}}$ I understand that using a circle works fine and is easier. I just want to derive the same answer with an ellipse though. What went wrong? $\Large{\text{Supplementaries to Muphrid's 2nd Comment: }}$ $\Large{\text{Q1.3.1.}}$ You write: ""When you choose a unit circular path, the resulting integral looks like an arclength integral."" Are you saying that choice of a unit circular path $\Longrightarrow \int_{A} \mathbf{F} \cdot d\mathbf{r} = ... = \int_{0}^{2\pi} 1 \, dt = 2\pi? $ How are you so confident that $...$ always yields this answer? $\Large{\text{Q1.4.1.}}$ Why can you define $d\mathbf{A} = z \, dx \, dy ?$ We're not given $d\mathbf{A}$?","This question is from Example #5, Section 16.4 on P1059 of Calculus , 6th Ed, by James Stewart. Given Question: If $\mathbf{F}(x,y) = \left(\dfrac{-y}{x^2 + y^2}, \dfrac{x}{x^2 + y^2}\right)$, show that $\int_C \mathbf{F} \cdot d\mathbf{r} = 2\pi $ for every positively oriented simple closed path that encloses the origin. Part of the Given Solution: Since $C$ is an ARBITRARY closed path that encloses the origin, it's difficult to compute the given integral directly. So let's consider a counterclockwise circle $A$ with center the origin and radius $a$, where $a$ is chosen to be small enough that $A$ lies inside $C$, as indicated by the picture below. Let $D$ be the region bounded by $C$ and $A$. Then positively oriented $\partial D = C \cup (-A)$. So the version of Green Theorem's applied to regions with holes gives: $\int_C \mathbf{F} \cdot d\mathbf{r} + \int_{-A} \mathbf{F} \cdot d\mathbf{r} = \iint_D \underbrace{(\partial_x Q - \partial_y P)}_{\Large{= 0}} \, dA $   $\Longrightarrow \int_C \mathbf{F} \cdot d\mathbf{r} = \int_{A} \mathbf{F} \cdot d\mathbf{r}$. (Rest of solution omitted) $\Large{\text{Q1.}}$ I can't perceive how one would divine to construct $A$ to solve this problem. So I feel that my following solution is more intuitive. Is it really? However, I don't seem to need Green's Thoerem. How and where would it be applied? My solution: It's hard to calculate $\int_C \mathbf{F} \cdot d\mathbf{r} $. The idea then is to find one $A$ so that $(\dagger) \int_{A} \mathbf{F} \cdot d\mathbf{r}$ is easier to compute and $(\ddagger)\int_C \mathbf{F} \cdot d\mathbf{r} = \int_{A} \mathbf{F} \cdot d\mathbf{r}$. Now, $(\ddagger) \iff \int_C \mathbf{F} \cdot d\mathbf{r} \huge{\color{red}{-}} \normalsize\int_A \mathbf{F} \cdot d\mathbf{r} = 0 \iff \int_C \mathbf{F} \cdot d\mathbf{r}  +\int_{\huge{\color{red}{-}\normalsize{A}}} \mathbf{F} \cdot d\mathbf{r} = 0 $ $\iff \int_{\Large{\partial D} \normalsize{ \, = \, (C} \, \cup \huge{\color{red}{-}\normalsize{A})}} \mathbf{F} \cdot d\mathbf{r} = 0$. For $\partial D$ to be positively oriented, $C$ and $\color{red}{-}A$ must both be positively oriented $\Longrightarrow A$ must be NEGATIVELY oriented, so $D$ must be to the right of $A$. One convenient choice of $A$ would just be a counterclockwise circle with center the origin and the radius $a$. $\Large{\text{Q2.}}$ Why don't I get the requested answer if I pick $A$ as a counterclockwise ellipse enclosing the origin? In other words, $A$ is parameterised via $r(t) = (a\cos t, b\sin t), 0 \leq t \leq 2\pi $. Then $\int_{A} \mathbf{F} \cdot d\mathbf{r} = \int_0^{2 \pi} \left(\dfrac{-b\sin t}{a^2 \cos^2 t + b^2 \sin^2 t},\dfrac{a\cos t}{a^2 \cos^2 t + b^2 \sin^2 t} \right) \cdot (-a\sin t, b \cos t) \, dt$. $ = ab \int_0^{2 \pi} \dfrac{1}{a^2 \cos^2 t + b^2 \sin^2 t} \, dt $. If $a = b$, then the requested answer follows by inspection. But what if $a \neq b$? $\Large{\text{Supplementaries to Muphrid's Answer: }}$ $\Large{\text{Q1.1.}}$ To rephrase your answer, are you saying that my solution has to justify why there exists an $A \ni \int_C \mathbf{F} \cdot d\mathbf{r} = \int_{A} \mathbf{F} \cdot d\mathbf{r}. ?$  This is true because $\int_C \mathbf{F} \cdot d\mathbf{r} = \int_{A} \mathbf{F} \cdot d\mathbf{r} \iff \int_{\Large{\partial D} \normalsize{ \, = \, (C} \, \cup \huge{\color{red}{-}\normalsize{A})}} \mathbf{F} \cdot d\mathbf{r} = 0, $  where the last equation is true by Green's Theorem: $\int_{\Large{\partial D} \normalsize{ \, = \, (C} \, \cup \huge{\color{red}{-}\normalsize{A})}} \mathbf{F} \cdot d\mathbf{r} = \iint_D \underbrace{(\partial_x Q - \partial_y P)}_{\Large{= 0}} \, dA. $ $\Large{\text{Q1.2.}}$ I wrote that $\partial D \normalsize{ \, = \, (C} \, \cup \huge{\color{red}{-}\normalsize{A})} $. Are you saying that this is wrong and that $\partial D \normalsize{ \, \subsetneqq \, (C} \, \cup \huge{\color{red}{-}\normalsize{A})} $? $\Large{\text{Q1.3.}}$ I understand that $\mathbf{F}(0,0)$ is undefined. However, how does this imply that integrals enclosing the origin pick up $2\pi$? $\Large{\text{Q1.4.}}$ How is $(\nabla \times \mathbf{F}) \cdot d\mathbf{A} = \partial_x Q - \partial_y P $? How is $d\mathbf{A} = z \, dx \, dy ?$ $\Large{\text{Q2.1.}}$ I understand that using a circle works fine and is easier. I just want to derive the same answer with an ellipse though. What went wrong? $\Large{\text{Supplementaries to Muphrid's 2nd Comment: }}$ $\Large{\text{Q1.3.1.}}$ You write: ""When you choose a unit circular path, the resulting integral looks like an arclength integral."" Are you saying that choice of a unit circular path $\Longrightarrow \int_{A} \mathbf{F} \cdot d\mathbf{r} = ... = \int_{0}^{2\pi} 1 \, dt = 2\pi? $ How are you so confident that $...$ always yields this answer? $\Large{\text{Q1.4.1.}}$ Why can you define $d\mathbf{A} = z \, dx \, dy ?$ We're not given $d\mathbf{A}$?",,[]
36,Change of variables in improper double integral - how to find the limits?,Change of variables in improper double integral - how to find the limits?,,"Consider the following integral (originating from the product of the Laplace transforms of $f$ and $g$): $$\int_0^\infty \int_0^\infty f(u)\ g(v) e^{-s(u+v)}\ du\ dv.$$ For this integral, the variables are to be changed from $(u, v)$ to $(u, t)$ so that $v = t - u$. According to this video (~at 25:00), the result is $$\int_0^\infty \int_0^t f(u)\ g(t - u) e^{-st}\ du\ dt.$$ The integrand is found easily, but what about the limits? In the video Prof. Mattuck gives a geometric argument, but I wonder how to the limits can be found in a more systematic way, like in the standard substitution rule $$\int_{g(a)}^{g(b)} f(x)\ dx = \int_a^b f(g(t)) g'(t)\ dt.$$ In particular, where does the $t$ in the upper limit of the inner integral come from?","Consider the following integral (originating from the product of the Laplace transforms of $f$ and $g$): $$\int_0^\infty \int_0^\infty f(u)\ g(v) e^{-s(u+v)}\ du\ dv.$$ For this integral, the variables are to be changed from $(u, v)$ to $(u, t)$ so that $v = t - u$. According to this video (~at 25:00), the result is $$\int_0^\infty \int_0^t f(u)\ g(t - u) e^{-st}\ du\ dt.$$ The integrand is found easily, but what about the limits? In the video Prof. Mattuck gives a geometric argument, but I wonder how to the limits can be found in a more systematic way, like in the standard substitution rule $$\int_{g(a)}^{g(b)} f(x)\ dx = \int_a^b f(g(t)) g'(t)\ dt.$$ In particular, where does the $t$ in the upper limit of the inner integral come from?",,"['multivariable-calculus', 'improper-integrals']"
37,Find Max/Min volume of rectangular box using Lagrange Multipliers,Find Max/Min volume of rectangular box using Lagrange Multipliers,,"Can anyone help me solve the problem below?  This is question number 14.8.42 in the seventh edition of Stewart Calculus. Here is the problem definition: ""Find the maximum and minimum volumes of a rectangular box whose surface area is $1500 cm^3$ and whose total edge length is 200 cm."" Here is my work so far(With edits based on Phyra's suggestions below): $V=f(x,y,z)=xyz$ Subject to two constraints: Surface area: $g(x,y,z)=2xy+2xz+2yz=1500$ Edge length: $h(x,y,z)=4x+4y+4z=200$ Simplify constraints to: $g(x,y,z)=xy+xz+yz=750$ $h(x,y,z)=x+y+z=50$ Solve system of equations: $f_x=yz=\lambda (y+z)+\mu$ $f_y=xz=\lambda (x+z)+\mu$ $f_z=xy=\lambda (x+y)+\mu$ This leads to: $\lambda y+\lambda z+\mu=yz$ $\lambda x+\lambda z+\mu=xz$ $\lambda x+\lambda y+\mu=xy$ Thus, $\lambda x=xy-\lambda y-\mu=xz-\lambda z-\mu$, so that $xy-\lambda y=xz-\lambda z$, which simplifies to: $x(y-z)=\lambda (y-z)$, so that $\lambda=x$ when $(y-z)\ne 0$ Similarly, $\lambda y =yz-\lambda z -\mu=xy-\lambda x -\mu$, so that $y(z-x)=\lambda (z-x)$, and $\lambda = y$ when $(z-x)\ne 0$ Also, $\lambda z = yz-\lambda y -\mu=xz-\lambda x - \mu$, so that $z(y-x)=\lambda (y-x)$, and $\lambda = z$ when $(y-x)\ne 0$ The above can be summarized as: $\lambda=x=y=z  $ Substitute x=y into constraints: $x^2 +2xz =750$ and $2x+z=50$ Substitute $z=50-2x$ into first constraint: $3x^2-100x+750=0$ gives $x=y=\frac{5}{3}(\frac{+}{}\sqrt{10}+10)$ Also, $z=50-2x=\frac{50\frac{+}{}10\sqrt{10}}{3}$ Thus, $V=xyz\approx 2948$ or $\approx 3534$ My remaining questions are as follows: a) What do you mean in your comments below when you refer to checking the border?  This concept is not in my textbook. b) Why choose x=y?  Are you saying that choosing y=z would have given the same result? c) Why use $z=50-2x$ instead of $x=y=z$? d) When you mention 8 cases, to what are you referring?  I do not see any cases. e) What do you mean when you use the word symmetric in this situation?","Can anyone help me solve the problem below?  This is question number 14.8.42 in the seventh edition of Stewart Calculus. Here is the problem definition: ""Find the maximum and minimum volumes of a rectangular box whose surface area is $1500 cm^3$ and whose total edge length is 200 cm."" Here is my work so far(With edits based on Phyra's suggestions below): $V=f(x,y,z)=xyz$ Subject to two constraints: Surface area: $g(x,y,z)=2xy+2xz+2yz=1500$ Edge length: $h(x,y,z)=4x+4y+4z=200$ Simplify constraints to: $g(x,y,z)=xy+xz+yz=750$ $h(x,y,z)=x+y+z=50$ Solve system of equations: $f_x=yz=\lambda (y+z)+\mu$ $f_y=xz=\lambda (x+z)+\mu$ $f_z=xy=\lambda (x+y)+\mu$ This leads to: $\lambda y+\lambda z+\mu=yz$ $\lambda x+\lambda z+\mu=xz$ $\lambda x+\lambda y+\mu=xy$ Thus, $\lambda x=xy-\lambda y-\mu=xz-\lambda z-\mu$, so that $xy-\lambda y=xz-\lambda z$, which simplifies to: $x(y-z)=\lambda (y-z)$, so that $\lambda=x$ when $(y-z)\ne 0$ Similarly, $\lambda y =yz-\lambda z -\mu=xy-\lambda x -\mu$, so that $y(z-x)=\lambda (z-x)$, and $\lambda = y$ when $(z-x)\ne 0$ Also, $\lambda z = yz-\lambda y -\mu=xz-\lambda x - \mu$, so that $z(y-x)=\lambda (y-x)$, and $\lambda = z$ when $(y-x)\ne 0$ The above can be summarized as: $\lambda=x=y=z  $ Substitute x=y into constraints: $x^2 +2xz =750$ and $2x+z=50$ Substitute $z=50-2x$ into first constraint: $3x^2-100x+750=0$ gives $x=y=\frac{5}{3}(\frac{+}{}\sqrt{10}+10)$ Also, $z=50-2x=\frac{50\frac{+}{}10\sqrt{10}}{3}$ Thus, $V=xyz\approx 2948$ or $\approx 3534$ My remaining questions are as follows: a) What do you mean in your comments below when you refer to checking the border?  This concept is not in my textbook. b) Why choose x=y?  Are you saying that choosing y=z would have given the same result? c) Why use $z=50-2x$ instead of $x=y=z$? d) When you mention 8 cases, to what are you referring?  I do not see any cases. e) What do you mean when you use the word symmetric in this situation?",,"['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
38,"Writing ""$\nabla f$"" or ""$\operatorname{grad} f$""","Writing """" or """"",\nabla f \operatorname{grad} f,"When hand-writing the gradient of $f$ as ""$\nabla f$"" or ""grad $f$"", is it necessary to indicate that it is a vector using the usual vector markings (cap, arrow, wavy line, etc.)?","When hand-writing the gradient of $f$ as ""$\nabla f$"" or ""grad $f$"", is it necessary to indicate that it is a vector using the usual vector markings (cap, arrow, wavy line, etc.)?",,"['multivariable-calculus', 'notation']"
39,How to find a vector potential (inverse curl)?,How to find a vector potential (inverse curl)?,,"If you are given a vector field, how do you find a vector potential for it? In my particular case, I need to find a vector field $\vec{A}$ such that $$ \vec{\nabla} \times \vec{A}(\,\vec{r}) = \begin{cases} B_0\hat{z} && \text{if $\vec{r} \in$ some cylinder along } \hat{z} \newline \vec{0} && \text{ otherwise} \end{cases} $$ but I'm hoping for a better answer than ""guess-and-check"" (or at least, a more generic way of guess-and-checking) that would help me in other cases as well. Note: The vector field (and hence, the potential) does not necessarily go to zero as we approach infinity.","If you are given a vector field, how do you find a vector potential for it? In my particular case, I need to find a vector field $\vec{A}$ such that $$ \vec{\nabla} \times \vec{A}(\,\vec{r}) = \begin{cases} B_0\hat{z} && \text{if $\vec{r} \in$ some cylinder along } \hat{z} \newline \vec{0} && \text{ otherwise} \end{cases} $$ but I'm hoping for a better answer than ""guess-and-check"" (or at least, a more generic way of guess-and-checking) that would help me in other cases as well. Note: The vector field (and hence, the potential) does not necessarily go to zero as we approach infinity.",,['multivariable-calculus']
40,$|f(x)-f(y)|\leq \|x-y\|^2$ implies $f$ is constant,implies  is constant,|f(x)-f(y)|\leq \|x-y\|^2 f,"Let $f:\Bbb{R}^2\to \Bbb{R}$ be such that for all $(x,y) \in (\Bbb{R}^2)^2$ we have that $|f(x)-f(y)|\leq \|x-y\|^2$ . I need to show that $f$ is constant. First, by hypothesis we have that $\lim_{x\to y}\frac{|f(x)-f(y)|}{\|x-y\|}=0$ which implies that $f$ is differentiable (Taylor expension of order 1) and that $Df(y)=0 \ \forall y \in \Bbb{R}^2$ . Now by mean value theorem considering $[x,y]$ we have that there is $c\in ]x,y[$ such that $f(x)-f(y)=Df(c)(x-y)=0\implies f(x)=f(y)$ . As $x$ and $y$ are arbitrary we conclude that $f$ is constant. Is it correct? Thank you in advance.","Let be such that for all we have that . I need to show that is constant. First, by hypothesis we have that which implies that is differentiable (Taylor expension of order 1) and that . Now by mean value theorem considering we have that there is such that . As and are arbitrary we conclude that is constant. Is it correct? Thank you in advance.","f:\Bbb{R}^2\to \Bbb{R} (x,y) \in (\Bbb{R}^2)^2 |f(x)-f(y)|\leq \|x-y\|^2 f \lim_{x\to y}\frac{|f(x)-f(y)|}{\|x-y\|}=0 f Df(y)=0 \ \forall y \in \Bbb{R}^2 [x,y] c\in ]x,y[ f(x)-f(y)=Df(c)(x-y)=0\implies f(x)=f(y) x y f","['multivariable-calculus', 'derivatives']"
41,Oscillation of the function at each point,Oscillation of the function at each point,,"I would like to find the oscillation of the function $f:\Bbb R^2\to\Bbb R,$ $$f(x,y)=\begin{cases}\sin\left(\frac1x\right)+\sin\left(\frac1y\right),&x,y\ne 0\\0, x,=0\text{ or } y=0.\end{cases}$$ This is our definition: Let $A\subset\Bbb R^2$ and $f:A\to\Bbb R$ any function. Oscillation $O(f,c)$ of the function $f$ at the point $c\in A$ is defined as $$O(f,c)=\inf_{U\ni c\\ U\text{ open }}\sup_{x_1,x_2\in U\cap A}|f(x_1)-f(x_2)|.$$ And a lemma: Function $f:A\to\Bbb R$ is continuous at $c\in A$ if and only if $O(f,c)=0.$ My attempt: According to the lemma, since the given function is continuous on $S:=\{(x,y)\in\Bbb R^2\mid xy\ne 0\},O(f,c)=0,\forall c\in S_1.$ Now, suppose $x_0\ne 0$ and $y_0=0.$ For any $\varepsilon>0,$ there is $n\in\Bbb N$ s. t. $\left(x_1,\frac{2\pi}{4n+1}\right),\left(x_2,\frac{2\pi}{4n+3}\right)\in B((x_0,0);\varepsilon)$ for $x_1,x_2$ close enough to $x_0$ since $x\mapsto\sin\left(\frac1x\right)$ is uniformly continuous on segments not containing $0$ . Then $$f\left(x_1,\frac{2\pi}{4n+1}\right)-f\left(x_2,\frac{2n}{4n+3}\right)\to 2,$$ and I think this is the oscillation at such point, but I'm not sure if this is right. If $x=y=0,$ then we can consider expressions $$f\left(\frac{2\pi}{4n+1}, \frac{2\pi}{4n+1}\right)-f\left(\frac{2\pi}{4n+3}, \frac{2\pi}{4n+3}\right),$$ but $|f(x_1,y_1)-f(x_2,y_2)|\le 4,\forall (x_1,y_1),(x_2,y_2)\in\Bbb R^2,$ so I think this works. Can somebody help me with the case of the axes?","I would like to find the oscillation of the function This is our definition: Let and any function. Oscillation of the function at the point is defined as And a lemma: Function is continuous at if and only if My attempt: According to the lemma, since the given function is continuous on Now, suppose and For any there is s. t. for close enough to since is uniformly continuous on segments not containing . Then and I think this is the oscillation at such point, but I'm not sure if this is right. If then we can consider expressions but so I think this works. Can somebody help me with the case of the axes?","f:\Bbb R^2\to\Bbb R, f(x,y)=\begin{cases}\sin\left(\frac1x\right)+\sin\left(\frac1y\right),&x,y\ne 0\\0, x,=0\text{ or } y=0.\end{cases} A\subset\Bbb R^2 f:A\to\Bbb R O(f,c) f c\in A O(f,c)=\inf_{U\ni c\\ U\text{ open }}\sup_{x_1,x_2\in U\cap A}|f(x_1)-f(x_2)|. f:A\to\Bbb R c\in A O(f,c)=0. S:=\{(x,y)\in\Bbb R^2\mid xy\ne 0\},O(f,c)=0,\forall c\in S_1. x_0\ne 0 y_0=0. \varepsilon>0, n\in\Bbb N \left(x_1,\frac{2\pi}{4n+1}\right),\left(x_2,\frac{2\pi}{4n+3}\right)\in B((x_0,0);\varepsilon) x_1,x_2 x_0 x\mapsto\sin\left(\frac1x\right) 0 f\left(x_1,\frac{2\pi}{4n+1}\right)-f\left(x_2,\frac{2n}{4n+3}\right)\to 2, x=y=0, f\left(\frac{2\pi}{4n+1}, \frac{2\pi}{4n+1}\right)-f\left(\frac{2\pi}{4n+3}, \frac{2\pi}{4n+3}\right), |f(x_1,y_1)-f(x_2,y_2)|\le 4,\forall (x_1,y_1),(x_2,y_2)\in\Bbb R^2,","['multivariable-calculus', 'supremum-and-infimum']"
42,Question about showing the equation defines an implicit function and give the Taylor expansion,Question about showing the equation defines an implicit function and give the Taylor expansion,,"I have a question while doing the exercise, I want to be sure if my step is correct, the exercise is: Show that the equation $$x^3 - y^3 -3xy + 1 = 0$$ defines an implicit function $\phi:x \to y$ in a neighbourhood of $0$ such that $\phi (0) =1$ . Give the third order Taylor expansion of $\phi$ in this neighbourhood of $0$ . Here is my attempt: I have first calculated the derivative of $\frac{dF}{dx}$ and $\frac{dF}{dy}$ , and then find $dF=(3x^2-3y  \ \ \ -3y^2-3x)$ , then we have $det(\frac{dF}{dy})(0)=(-3y^2-3x)(0)=0$ but $\phi(0)=1 \ne 0$ now we have $$\frac{d\phi}{dy} = -[\frac{dF}{dy}(x,\phi(x)]^{-1}[\frac{dF}{dx}(x,\phi(x)]=\frac{3x^2-3y}{3y^2-3y}$$ Then Taylor expansion for $\phi(x)$ at $0$ is: $$\phi(0) + \phi'(0)(x) +\frac{\phi''(0)(x^2)}{2}+\frac{\phi'''(0)(x^3)}{6}+ ...$$ which gives me: $$1+0+0+0+...=1$$ I doubt my step is not correct, can somebody helps me to check, thank you very much in advance.","I have a question while doing the exercise, I want to be sure if my step is correct, the exercise is: Show that the equation defines an implicit function in a neighbourhood of such that . Give the third order Taylor expansion of in this neighbourhood of . Here is my attempt: I have first calculated the derivative of and , and then find , then we have but now we have Then Taylor expansion for at is: which gives me: I doubt my step is not correct, can somebody helps me to check, thank you very much in advance.","x^3 - y^3 -3xy + 1 = 0 \phi:x \to y 0 \phi (0) =1 \phi 0 \frac{dF}{dx} \frac{dF}{dy} dF=(3x^2-3y  \ \ \ -3y^2-3x) det(\frac{dF}{dy})(0)=(-3y^2-3x)(0)=0 \phi(0)=1 \ne 0 \frac{d\phi}{dy} = -[\frac{dF}{dy}(x,\phi(x)]^{-1}[\frac{dF}{dx}(x,\phi(x)]=\frac{3x^2-3y}{3y^2-3y} \phi(x) 0 \phi(0) + \phi'(0)(x) +\frac{\phi''(0)(x^2)}{2}+\frac{\phi'''(0)(x^3)}{6}+ ... 1+0+0+0+...=1",['multivariable-calculus']
43,Fairly rigorous multivariable calculus books,Fairly rigorous multivariable calculus books,,"I'm looking for recommendations for a multivariable calculus book at a somewhat sophisticated level; somewhere between Stewart's Calculus and Munkres' Analysis on Manifolds . I'll have a background in single variable calculus and the typical material from a basic ""proofs"" class (set theory, logic, proof techniques, some topics in discrete math). This will be my first formal exposure to multivariable calculus beyond some reading I've done for fun. Note that, although I'll have some mathematical maturity and some background in proof-writing, I'll have learned single variable calculus from Stewart, obviously not a very rigorous book. Let me know if you think it's really necessary that I read a more sophisticated calculus text (like Spivak's Calculus ) before moving on to multivariable calculus at the level that I'm describing. I think a book like Spivak's Calculus on Manifolds or Apostol's Calculus, Vol. 2 would be what I'm looking for. Of these two, I think I'd slightly prefer using Spivak since I'm interested in differential geometry and I like the idea of introducing manifolds in multivariable calculus. I hope this gives some idea of the kind of book I'm looking for. I don't want something crazy rigorous, but definitely something more sophisticated than a typical computational calculus book.","I'm looking for recommendations for a multivariable calculus book at a somewhat sophisticated level; somewhere between Stewart's Calculus and Munkres' Analysis on Manifolds . I'll have a background in single variable calculus and the typical material from a basic ""proofs"" class (set theory, logic, proof techniques, some topics in discrete math). This will be my first formal exposure to multivariable calculus beyond some reading I've done for fun. Note that, although I'll have some mathematical maturity and some background in proof-writing, I'll have learned single variable calculus from Stewart, obviously not a very rigorous book. Let me know if you think it's really necessary that I read a more sophisticated calculus text (like Spivak's Calculus ) before moving on to multivariable calculus at the level that I'm describing. I think a book like Spivak's Calculus on Manifolds or Apostol's Calculus, Vol. 2 would be what I'm looking for. Of these two, I think I'd slightly prefer using Spivak since I'm interested in differential geometry and I like the idea of introducing manifolds in multivariable calculus. I hope this gives some idea of the kind of book I'm looking for. I don't want something crazy rigorous, but definitely something more sophisticated than a typical computational calculus book.",,"['multivariable-calculus', 'book-recommendation']"
44,"Evaluating the recurrence $f_k(x)= f'_{k-1}(x)+f_{k-1}(x) f_1(x)$ with $f_0(x)=1$, $f_1(x)=e^x$","Evaluating the recurrence  with ,",f_k(x)= f'_{k-1}(x)+f_{k-1}(x) f_1(x) f_0(x)=1 f_1(x)=e^x,"Is it possible to recover $f_n(x)$ for any $n$ based off this question ? The relation is: $ \frac{d^n}{dx^n} f_1(x)$ from $\frac{d}{dx} f_{k-1}(x)=f_k(x)-f_{k-1}(x) f_1(x)$ $f_0(x)=1$ and I add $f_1(x)=e^x$ Using this you can find $f_n(x)$ for all $n$ when $n$ is a whole number. $f_0(x)=1,f_1(x)=e^x$ $f_2(x)=e^x+e^{2x}$ $f_3(x)=e^x+3e^{2x}+e^{3x}$ $f_4(x)=e^x+7e^{2x}+6e^{3x}+e^{4x}$ , etc. My question is can you generalize $f_n(x)$ get $n$ to be any number. If so how would you solve $f_{1/2} (x)$ or $f_{i}(x)$","Is it possible to recover for any based off this question ? The relation is: from and I add Using this you can find for all when is a whole number. , etc. My question is can you generalize get to be any number. If so how would you solve or","f_n(x) n  \frac{d^n}{dx^n} f_1(x) \frac{d}{dx} f_{k-1}(x)=f_k(x)-f_{k-1}(x) f_1(x) f_0(x)=1 f_1(x)=e^x f_n(x) n n f_0(x)=1,f_1(x)=e^x f_2(x)=e^x+e^{2x} f_3(x)=e^x+3e^{2x}+e^{3x} f_4(x)=e^x+7e^{2x}+6e^{3x}+e^{4x} f_n(x) n f_{1/2} (x) f_{i}(x)","['multivariable-calculus', 'functions']"
45,"Is $g(\eta)$, as defined in the question, a Dirac delta function?","Is , as defined in the question, a Dirac delta function?",g(\eta),"Section 1: Maths Question (TL;DR version) In the course of trying to solve a physics problem (ref. Section 2), I encountered a mathematical question. To make my post brief, I'll write only the maths question here that needs to be addressed: \begin{align} &f(r,\eta)= -\frac{r-R\eta}{(r^2+R^2-2rR\eta)^{3/2}} &\text{where, }0\leq r \leq \infty \text{ & }-1 \leq \eta \leq 1\end{align} When one plots $f$ as a function of $r$ for various values of $\eta$ , one observes that $f$ is continuous at $r=R$ for all values of $\eta$ except $\eta=1$ . In the case of $\eta=1$ , $f$ diverges to $+\infty$ and $-\infty$ on the left and right sides of $r=R$ respectively $\left(\because f(r,1)=-\frac{r-R}{|r-R|^3}\right)$ . This implies the following, \begin{align}g(\eta) \equiv \lim_{r \to R+}f(r,\eta)-\lim_{r \to R-}f(r,\eta) \; &\text{is zero for }\eta \neq 1 \\ & \text{ blows up for }\eta=1 \end{align} This is similar to how a Dirac delta function behaves (blows up at one point and zero everywhere else). A stronger motivation for why I believe it might be a Dirac delta function is given in the next section. Question : Is $g(\eta)$ as defined above, a Dirac delta function in $\eta$ (up to some scale factor)? Section 2: Physics Problem The physics problem setup is a general spherical surface charge distribution $\sigma(\theta,\phi)$ of radius $R$ . It is known that the component of the electric field, $\mathbf{E}=-\nabla\Phi$ , that is normal to the spherical surface is discontinuous. i.e., $$\lim_{r \to R+}\partial_r \Phi(r,\theta,\phi)-\lim_{r \to R-}\partial_r\Phi(r,\theta,\phi)=-\frac{\sigma(\theta,\phi)}{\epsilon_0} \tag{1; eq. 2.31 in [1]}$$ The above result is commonly proved by applying Gauss' law to an infinitesimal Gaussian ""pill-box"" covering the region of interest. However, I wish to prove the above result (eq. 1) by only using the following Green's function solution for the electric potential (eq. 2). \begin{align}&\Phi(\mathbf{r}) =\frac{1}{4\pi \epsilon_0}\int \frac{\rho(\mathbf{r}')}{|\mathbf{r}-\mathbf{r}'|}d^3\mathbf{r}' &\rho(\mathbf{r})=\sigma(\theta,\phi)\delta(r-R) \tag{2}\\  \Rightarrow \;&\Phi(\mathbf{r})=\frac{1}{4\pi \epsilon_0}\int\frac{\sigma(\theta',\phi')}{|r \hat{r}-R\hat{r}'|}R^2\sin\theta' d\theta' d\phi' &\text{where, }\hat{r}=\hat{r}(\theta,\phi)  \text{ & }\hat{r}'=\hat{r}(\theta\,',\phi') \end{align} Using $|r \hat{r}-R\hat{r}'|=\sqrt{r^2+R^2-2rR\hat{r}\cdot\hat{r}'}$ , we have, \begin{align}\partial_r \Phi=-\frac{1}{4\pi \epsilon_0}\int\sigma(\theta',\phi')\frac{r-R\hat{r}\cdot\hat{r}'}{|r \hat{r}-R\hat{r}'|^3}R^2\sin\theta' d\theta' d\phi'  \end{align} I encountered a question in the course of my attempt to prove eq. 1. I'll describe it below. \begin{align}&\partial_r \Phi=-\frac{1}{4\pi \epsilon_0}\int\sigma(\theta',\phi')\frac{r-R\eta}{(r^2+R^2-2rR\eta)^{3/2}}R^2\sin\theta' d\theta' d\phi'  &\text{where, }\eta \equiv \hat{r}\cdot\hat{r}' \end{align} $$\lim_{r \to R+}\partial_r \Phi-\lim_{r \to R-}\partial_r\Phi =\frac{1}{4\pi \epsilon_0}\int\sigma(\theta',\phi')(\lim_{r \to R+}f-\lim_{r \to R-}f)R^2\sin\theta' d\theta' d\phi'\tag{3}$$ $$\text{where, }f(r,\eta)\equiv -\frac{r-R\eta}{(r^2+R^2-2rR\eta)^{3/2}} $$ When one plots this function $f$ online as a function of $r$ for various values of $\eta$ , one observes that $f$ is continuous at $r=R$ for all values of $\eta$ ( $\eta \in [-1,1]$ ) except $\eta=1$ . For $\eta=1$ , the function $f$ diverges to $+ \infty$ and $- \infty$ on the left and right  sides of $r=R$ respectively $\left(\because f(r,1)=-\frac{r-R}{|r-R|^3}\right)$ . This implies the following, \begin{align}g(\eta) \equiv \lim_{r \to R+}f(r,\eta)-\lim_{r \to R-}f(r,\eta) \; &\text{is zero for }\eta \neq 1 \tag{4}\\ & \text{ blows up for }\eta=1 \text{ ($\eta=1$ $\Leftrightarrow$ $\theta'=\theta$ and $\phi'=\phi$)}\end{align} This looks promising because the above behavior is similar to a Dirac delta function (blows up at one point and zero everywhere else). The discontinuity in the electric field at $(\theta,\phi)$ is only ""aware"" of the value of the surface charge density $\sigma$ at $(\theta,\phi)$ (ref. eq. 1) and hence, I believe I need a Dirac delta function in the integral in eq. 3 to get the $\sigma$ out of the integral. Question: Is $g(\eta)$ as defined in eq. 4, a Dirac delta function (up to some scale factor $\#$ )? That is, $$\text{Is }g= (\#)\; \delta(\theta'-\theta)\delta(\phi'-\phi)?$$ I'd really appreciate any insight that addresses my problem. References $[1]$ Griffiths, Introduction to Electrodynamics (3rd ed.)","Section 1: Maths Question (TL;DR version) In the course of trying to solve a physics problem (ref. Section 2), I encountered a mathematical question. To make my post brief, I'll write only the maths question here that needs to be addressed: When one plots as a function of for various values of , one observes that is continuous at for all values of except . In the case of , diverges to and on the left and right sides of respectively . This implies the following, This is similar to how a Dirac delta function behaves (blows up at one point and zero everywhere else). A stronger motivation for why I believe it might be a Dirac delta function is given in the next section. Question : Is as defined above, a Dirac delta function in (up to some scale factor)? Section 2: Physics Problem The physics problem setup is a general spherical surface charge distribution of radius . It is known that the component of the electric field, , that is normal to the spherical surface is discontinuous. i.e., The above result is commonly proved by applying Gauss' law to an infinitesimal Gaussian ""pill-box"" covering the region of interest. However, I wish to prove the above result (eq. 1) by only using the following Green's function solution for the electric potential (eq. 2). Using , we have, I encountered a question in the course of my attempt to prove eq. 1. I'll describe it below. When one plots this function online as a function of for various values of , one observes that is continuous at for all values of ( ) except . For , the function diverges to and on the left and right  sides of respectively . This implies the following, This looks promising because the above behavior is similar to a Dirac delta function (blows up at one point and zero everywhere else). The discontinuity in the electric field at is only ""aware"" of the value of the surface charge density at (ref. eq. 1) and hence, I believe I need a Dirac delta function in the integral in eq. 3 to get the out of the integral. Question: Is as defined in eq. 4, a Dirac delta function (up to some scale factor )? That is, I'd really appreciate any insight that addresses my problem. References Griffiths, Introduction to Electrodynamics (3rd ed.)","\begin{align} &f(r,\eta)= -\frac{r-R\eta}{(r^2+R^2-2rR\eta)^{3/2}} &\text{where, }0\leq r \leq \infty \text{ & }-1 \leq \eta \leq 1\end{align} f r \eta f r=R \eta \eta=1 \eta=1 f +\infty -\infty r=R \left(\because f(r,1)=-\frac{r-R}{|r-R|^3}\right) \begin{align}g(\eta) \equiv \lim_{r \to R+}f(r,\eta)-\lim_{r \to R-}f(r,\eta) \; &\text{is zero for }\eta \neq 1 \\ & \text{ blows up for }\eta=1 \end{align} g(\eta) \eta \sigma(\theta,\phi) R \mathbf{E}=-\nabla\Phi \lim_{r \to R+}\partial_r \Phi(r,\theta,\phi)-\lim_{r \to R-}\partial_r\Phi(r,\theta,\phi)=-\frac{\sigma(\theta,\phi)}{\epsilon_0} \tag{1; eq. 2.31 in [1]} \begin{align}&\Phi(\mathbf{r}) =\frac{1}{4\pi \epsilon_0}\int \frac{\rho(\mathbf{r}')}{|\mathbf{r}-\mathbf{r}'|}d^3\mathbf{r}' &\rho(\mathbf{r})=\sigma(\theta,\phi)\delta(r-R) \tag{2}\\ 
\Rightarrow \;&\Phi(\mathbf{r})=\frac{1}{4\pi \epsilon_0}\int\frac{\sigma(\theta',\phi')}{|r \hat{r}-R\hat{r}'|}R^2\sin\theta' d\theta' d\phi' &\text{where, }\hat{r}=\hat{r}(\theta,\phi)  \text{ & }\hat{r}'=\hat{r}(\theta\,',\phi') \end{align} |r \hat{r}-R\hat{r}'|=\sqrt{r^2+R^2-2rR\hat{r}\cdot\hat{r}'} \begin{align}\partial_r \Phi=-\frac{1}{4\pi \epsilon_0}\int\sigma(\theta',\phi')\frac{r-R\hat{r}\cdot\hat{r}'}{|r \hat{r}-R\hat{r}'|^3}R^2\sin\theta' d\theta' d\phi'  \end{align} \begin{align}&\partial_r \Phi=-\frac{1}{4\pi \epsilon_0}\int\sigma(\theta',\phi')\frac{r-R\eta}{(r^2+R^2-2rR\eta)^{3/2}}R^2\sin\theta' d\theta' d\phi'  &\text{where, }\eta \equiv \hat{r}\cdot\hat{r}' \end{align} \lim_{r \to R+}\partial_r \Phi-\lim_{r \to R-}\partial_r\Phi =\frac{1}{4\pi \epsilon_0}\int\sigma(\theta',\phi')(\lim_{r \to R+}f-\lim_{r \to R-}f)R^2\sin\theta' d\theta' d\phi'\tag{3} \text{where, }f(r,\eta)\equiv -\frac{r-R\eta}{(r^2+R^2-2rR\eta)^{3/2}}  f r \eta f r=R \eta \eta \in [-1,1] \eta=1 \eta=1 f + \infty - \infty r=R \left(\because f(r,1)=-\frac{r-R}{|r-R|^3}\right) \begin{align}g(\eta) \equiv \lim_{r \to R+}f(r,\eta)-\lim_{r \to R-}f(r,\eta) \; &\text{is zero for }\eta \neq 1 \tag{4}\\ & \text{ blows up for }\eta=1 \text{ (\eta=1 \Leftrightarrow \theta'=\theta and \phi'=\phi)}\end{align} (\theta,\phi) \sigma (\theta,\phi) \sigma g(\eta) \# \text{Is }g= (\#)\; \delta(\theta'-\theta)\delta(\phi'-\phi)? [1]","['multivariable-calculus', 'physics', 'dirac-delta']"
46,Inverse function derivative of multivariable functions,Inverse function derivative of multivariable functions,,"In one dimension, if the inverse of function $x(\zeta)$ exists, $\frac{d\zeta}{dx}=(\frac{dx}{d\zeta})^{-1}$ , and $\frac{d^2\zeta}{dx^2}=(-\frac{d^2x}{d\zeta^2}(\frac{dx}{d\zeta})^{-3})$ . So I can calculate these derivatives with only knowing the $x(\zeta)$ function. This is all nice in one dimension, but I would like to do something like this in 2 dimensions. The problem is given: $x(\zeta,\eta)$ and $y(\zeta,\eta)$ are known and I need to calculate $\frac{\partial^2 \zeta}{\partial x^2}$ and $\frac{\partial^2 \zeta}{\partial y^2}$ , and calculate $\frac{\partial^2\eta}{\partial x^2}$ and $\frac{\partial^2\eta}{\partial y^2}$ , without knowing $\zeta(x,y)$ and $\eta(x,y)$ . Can you help me in this? The aim of the whole thing is to calculate $\Delta_{xy} N(\zeta(x,y), \eta(x,y))$ . So N is defined in the $(\zeta,\eta)$ coordinate system, and a transformation from $(\zeta,\eta)$ to $(x,y)$ is given by $x(\zeta,\eta)$ and $y(\zeta,\eta)$ Any hint is appreciated. Thank you!","In one dimension, if the inverse of function exists, , and . So I can calculate these derivatives with only knowing the function. This is all nice in one dimension, but I would like to do something like this in 2 dimensions. The problem is given: and are known and I need to calculate and , and calculate and , without knowing and . Can you help me in this? The aim of the whole thing is to calculate . So N is defined in the coordinate system, and a transformation from to is given by and Any hint is appreciated. Thank you!","x(\zeta) \frac{d\zeta}{dx}=(\frac{dx}{d\zeta})^{-1} \frac{d^2\zeta}{dx^2}=(-\frac{d^2x}{d\zeta^2}(\frac{dx}{d\zeta})^{-3}) x(\zeta) x(\zeta,\eta) y(\zeta,\eta) \frac{\partial^2 \zeta}{\partial x^2} \frac{\partial^2 \zeta}{\partial y^2} \frac{\partial^2\eta}{\partial x^2} \frac{\partial^2\eta}{\partial y^2} \zeta(x,y) \eta(x,y) \Delta_{xy} N(\zeta(x,y), \eta(x,y)) (\zeta,\eta) (\zeta,\eta) (x,y) x(\zeta,\eta) y(\zeta,\eta)","['real-analysis', 'calculus', 'multivariable-calculus', 'partial-derivative', 'inverse']"
47,Realising I don't truly understand differentiation when not stated explicitly.,Realising I don't truly understand differentiation when not stated explicitly.,,"Probably much too late in my maths 'career' to be asking this but, I don't fully understand differentiation in multiple dimensions when functions aren't stated explicitly. I have got by up to now, but I think it's time to learn properly. Some examples: In my PDE's course we are studying the wave equation and for one question we define $$u(x,t) = v(\xi,\eta),\,\, \xi = x+ct,\,\, \eta = x-ct$$ where $c\in\mathbb{R}$ . And the following line is $\partial_{x}u = \partial_{\xi}v+\partial_{\eta}v$ . I understand where this comes looking at the formula for differentiation, but get confused with the dependancies, as it is almost never explicitly written. Also considering functions from multiple variables to one. Say for example you have a differentiable function $f$ and you try to calculate $$\frac{d}{dx}f(ax+bt)\quad \text{and}\quad \frac{d}{dt}f(ax+bt)$$ where $a,b \in\mathbb{R}$ and $x,t$ are variables. I believe the answers respectively are $$af'(ax+bt)\quad \text{and}\quad bf'(ax+bt).$$ But, I have no idea where $f'$ comes from in this context. Does it mean $$f' = \frac{d}{d(ax+bt)}?$$ If so why? How can I find some intuition with calculating non-explicit forms as such?","Probably much too late in my maths 'career' to be asking this but, I don't fully understand differentiation in multiple dimensions when functions aren't stated explicitly. I have got by up to now, but I think it's time to learn properly. Some examples: In my PDE's course we are studying the wave equation and for one question we define where . And the following line is . I understand where this comes looking at the formula for differentiation, but get confused with the dependancies, as it is almost never explicitly written. Also considering functions from multiple variables to one. Say for example you have a differentiable function and you try to calculate where and are variables. I believe the answers respectively are But, I have no idea where comes from in this context. Does it mean If so why? How can I find some intuition with calculating non-explicit forms as such?","u(x,t) = v(\xi,\eta),\,\, \xi = x+ct,\,\, \eta = x-ct c\in\mathbb{R} \partial_{x}u = \partial_{\xi}v+\partial_{\eta}v f \frac{d}{dx}f(ax+bt)\quad \text{and}\quad \frac{d}{dt}f(ax+bt) a,b \in\mathbb{R} x,t af'(ax+bt)\quad \text{and}\quad bf'(ax+bt). f' f' = \frac{d}{d(ax+bt)}?","['multivariable-calculus', 'soft-question', 'partial-derivative']"
48,"Explicitly calculate shape operator for graph of $f(x,y)=xy$",Explicitly calculate shape operator for graph of,"f(x,y)=xy","This seems trivial but I am stuck. To gain intuition for a bigger problem, I am trying to compute the shape operator of the graph of $f(x,y)=xy$ at the point $p=(0,0,0)$ , call this surface $\Sigma$ . In class we defined the shape operator at $p\in \Sigma$ , in terms of the covariant derivative. $$S(\vec v) = -\nabla_{\vec v}N=-\frac{d}{dt}\bigg|_{t=0}N(p+t\vec v).$$ where $\vec v\in T_p\Sigma$ and $N$ is normal to $\Sigma$ . Let $F(x,y,z)=xy-z$ . I know $N = \frac{\nabla F}{|\nabla F|}=\frac{1}{\sqrt{1+x^2+y^2}}(y,x,-1)$ . Say $\vec e_1,\vec e_2$ are basis vectors for $\Sigma$ . I know I need to find $S(\vec e_1)$ and $S(\vec e_2)$ , from there I can find the matrix of $S$ . This is where I am stuck, if someone can show how to do the calculation, that would be helpful.","This seems trivial but I am stuck. To gain intuition for a bigger problem, I am trying to compute the shape operator of the graph of at the point , call this surface . In class we defined the shape operator at , in terms of the covariant derivative. where and is normal to . Let . I know . Say are basis vectors for . I know I need to find and , from there I can find the matrix of . This is where I am stuck, if someone can show how to do the calculation, that would be helpful.","f(x,y)=xy p=(0,0,0) \Sigma p\in \Sigma S(\vec v) = -\nabla_{\vec v}N=-\frac{d}{dt}\bigg|_{t=0}N(p+t\vec v). \vec v\in T_p\Sigma N \Sigma F(x,y,z)=xy-z N = \frac{\nabla F}{|\nabla F|}=\frac{1}{\sqrt{1+x^2+y^2}}(y,x,-1) \vec e_1,\vec e_2 \Sigma S(\vec e_1) S(\vec e_2) S","['multivariable-calculus', 'differential-geometry']"
49,Evaluate $\int_{S^2} \frac{dx \wedge dy}{z} e^{1 - (x^2 + y^2)}$,Evaluate,\int_{S^2} \frac{dx \wedge dy}{z} e^{1 - (x^2 + y^2)},"Any ideas how to evaluate this integral over the sphere? $$ \int_{S^2} \frac{dx \wedge dy}{z} e^{1 - (x^2 + y^2)} =  \underbrace{\cdots }_{ (0,0,1)} + \underbrace{\cdots }_{ (0,0,-1)}$$ I considered using Green's Theorem or Stokes' Theorem .  The sphere could be read as the boundary of the ball $B = \{ x^2 + y^2 + z^2 < 1\}$ but what could the differential form be?  Perhaps we could find the divergence $\nabla \cdot F$. Another idea could be to exploit the rotation invariance: \begin{eqnarray*} x&=& \;\,x \cos \theta + y \sin \theta\\ y &=& -x \sin \theta + y \cos \theta \end{eqnarray*} and the integrate over the remaining interval $[-1,1]$. Let's check that our 2-form is defined at the poles of the hemispheres.  If $z \approx 0$ let's say $z = \varepsilon \ll 1$ then $(x,y) = \sqrt{1 - \varepsilon^2} (\cos \theta, \sin \theta)$.  Then using the polar coordinaes form of area $dx \wedge dy = r \, dr \wedge d\theta$: \begin{align} \frac{dx \wedge dy}{z} & = \frac{dx \wedge dy}{\sqrt{1 - (x^2 + y^2)}} \\[10pt] & \approx  \frac{d \big( (1 - \frac{1}{2}\varepsilon^2)(\cos \theta) \big)\wedge d \big( (1 - \frac{1}{2}\varepsilon^2)(\sin \theta)\big)}{\varepsilon} = \frac{\varepsilon (-2\varepsilon) \,d\varepsilon \wedge d\theta}{\varepsilon}  \end{align} And it looks like the epsilon's cancel out.  A more careful check should use the Mean value theorem.","Any ideas how to evaluate this integral over the sphere? $$ \int_{S^2} \frac{dx \wedge dy}{z} e^{1 - (x^2 + y^2)} =  \underbrace{\cdots }_{ (0,0,1)} + \underbrace{\cdots }_{ (0,0,-1)}$$ I considered using Green's Theorem or Stokes' Theorem .  The sphere could be read as the boundary of the ball $B = \{ x^2 + y^2 + z^2 < 1\}$ but what could the differential form be?  Perhaps we could find the divergence $\nabla \cdot F$. Another idea could be to exploit the rotation invariance: \begin{eqnarray*} x&=& \;\,x \cos \theta + y \sin \theta\\ y &=& -x \sin \theta + y \cos \theta \end{eqnarray*} and the integrate over the remaining interval $[-1,1]$. Let's check that our 2-form is defined at the poles of the hemispheres.  If $z \approx 0$ let's say $z = \varepsilon \ll 1$ then $(x,y) = \sqrt{1 - \varepsilon^2} (\cos \theta, \sin \theta)$.  Then using the polar coordinaes form of area $dx \wedge dy = r \, dr \wedge d\theta$: \begin{align} \frac{dx \wedge dy}{z} & = \frac{dx \wedge dy}{\sqrt{1 - (x^2 + y^2)}} \\[10pt] & \approx  \frac{d \big( (1 - \frac{1}{2}\varepsilon^2)(\cos \theta) \big)\wedge d \big( (1 - \frac{1}{2}\varepsilon^2)(\sin \theta)\big)}{\varepsilon} = \frac{\varepsilon (-2\varepsilon) \,d\varepsilon \wedge d\theta}{\varepsilon}  \end{align} And it looks like the epsilon's cancel out.  A more careful check should use the Mean value theorem.",,"['multivariable-calculus', 'differential-forms']"
50,Computing Neural Network Gradients,Computing Neural Network Gradients,,"The following note ""Computing Neural Network Gradients"" explains how we can take derivate with respect to matrix and vector. I have some questions: Figure below from the above note shows when we take derivative with respect to column vector we have:  $\frac{\partial(WX)}{\partial x} = W$ . Suppose W is $n \times m$ matrix and $x$ is m-dimensional vector. ============================================================= ============================================================= While in the picture below from Wikipedia , we have $\frac{\partial(AX)}{\partial x} = A^T$(since we take derivative with respect to column vector, I assume the denominator layout is the equivalent format. ) ================================================================ ================================================================ Q1. I don't understand why in the note it shows $W$ instead of $W^T$. Can anyone help explaining this please? Next, In the last part of note , the author added : We see that the dimensions of all the terms in the gradient match up (i.e., the     number of columns in a term equals the number of rows in the next term). This will always be the case if we computed our gradients correctly.     Now we can use the error terms to compute our gradients. Note that we transpose out answers when computing the gradients for column vectors terms to follow the shape convention. You can see the picture below for further details. ============================================================= ============================================================= Q2- If we always take the derivative with respect to column vector, why do we need to transpose the result? Rather than matching dimension as a way of verification how can I understand from the beginning when I need to transpose the result when calculating the derivative using the chain rule? I would appreciate any help to understand this. Thank you.","The following note ""Computing Neural Network Gradients"" explains how we can take derivate with respect to matrix and vector. I have some questions: Figure below from the above note shows when we take derivative with respect to column vector we have:  $\frac{\partial(WX)}{\partial x} = W$ . Suppose W is $n \times m$ matrix and $x$ is m-dimensional vector. ============================================================= ============================================================= While in the picture below from Wikipedia , we have $\frac{\partial(AX)}{\partial x} = A^T$(since we take derivative with respect to column vector, I assume the denominator layout is the equivalent format. ) ================================================================ ================================================================ Q1. I don't understand why in the note it shows $W$ instead of $W^T$. Can anyone help explaining this please? Next, In the last part of note , the author added : We see that the dimensions of all the terms in the gradient match up (i.e., the     number of columns in a term equals the number of rows in the next term). This will always be the case if we computed our gradients correctly.     Now we can use the error terms to compute our gradients. Note that we transpose out answers when computing the gradients for column vectors terms to follow the shape convention. You can see the picture below for further details. ============================================================= ============================================================= Q2- If we always take the derivative with respect to column vector, why do we need to transpose the result? Rather than matching dimension as a way of verification how can I understand from the beginning when I need to transpose the result when calculating the derivative using the chain rule? I would appreciate any help to understand this. Thank you.",,"['multivariable-calculus', 'vectors', 'partial-derivative', 'matrix-calculus', 'neural-networks']"
51,Logarithmic Differentiation for Multivariable functions?,Logarithmic Differentiation for Multivariable functions?,,"I was wondering if, and how exactly logarithmic differentiation may be applied to a multivariate function. For example, I have been working with the function $$f(x,y)=\frac{x-y}{x+y}$$ Does the following process hold?  Specifically, does the chain rule step with del f hold? $$\ln(f(x,y))=\ln(x-y)-\ln(x+y)$$ $$\Rightarrow \frac{1}{f(x,y)} \nabla f = \langle \frac{2y}{x^2-y^2} , \frac{-2x}{x^2-y^2} \rangle$$ Solving for $\nabla f$, does give me the correct answer.  I'm wondering is my notation would be correct, and if this will ever not work. Thanks for any help.","I was wondering if, and how exactly logarithmic differentiation may be applied to a multivariate function. For example, I have been working with the function $$f(x,y)=\frac{x-y}{x+y}$$ Does the following process hold?  Specifically, does the chain rule step with del f hold? $$\ln(f(x,y))=\ln(x-y)-\ln(x+y)$$ $$\Rightarrow \frac{1}{f(x,y)} \nabla f = \langle \frac{2y}{x^2-y^2} , \frac{-2x}{x^2-y^2} \rangle$$ Solving for $\nabla f$, does give me the correct answer.  I'm wondering is my notation would be correct, and if this will ever not work. Thanks for any help.",,"['calculus', 'multivariable-calculus', 'logarithms', 'vector-analysis']"
52,"Minimizing the multivariable function $f(x,y) = \frac{1}{xy} + x^2y$",Minimizing the multivariable function,"f(x,y) = \frac{1}{xy} + x^2y","I need to locate the optimal point(s) of the function $$f(x,y) = \frac{1}{xy} + x^2y$$ subject to the conditions $x \gt 0, y \gt 0$. I have tried the following approach $$\frac{\partial f}{\partial x} = \frac{-1}{x^2y} + 2xy = 0 \rightarrow 1$$ $$\frac{\partial f}{\partial y} = \frac{-1}{xy^2} + x^2 = 0 \rightarrow 2$$ Now, $x \ne 0, y \ne 0,$ hence, we solve the two equations to get the optimal point, $(x^*, y^*)$. But the above system of equations has no solution. Physically, when $x \rightarrow 0$ or $y \rightarrow 0$, the function blows up. Also, when either $x \rightarrow \infty$ or $y \rightarrow \infty$, the function also blows up. Since it takes finite values in between and is continuous and differentiable, it should have some minimum value(s), and hence an optimal point. Where am I going wrong ? SOLVED Thanks to Arthur, I have realised the error in the above statements. I have assumed that whenever x or y becomes large, the function will blow up. But if I were to go along the path $y = \sqrt{\frac{1}{x^3}}$, and take $x$ smaller and smaller, $f = 2\sqrt x$ will go to 0. Hence, the infimum is $0$","I need to locate the optimal point(s) of the function $$f(x,y) = \frac{1}{xy} + x^2y$$ subject to the conditions $x \gt 0, y \gt 0$. I have tried the following approach $$\frac{\partial f}{\partial x} = \frac{-1}{x^2y} + 2xy = 0 \rightarrow 1$$ $$\frac{\partial f}{\partial y} = \frac{-1}{xy^2} + x^2 = 0 \rightarrow 2$$ Now, $x \ne 0, y \ne 0,$ hence, we solve the two equations to get the optimal point, $(x^*, y^*)$. But the above system of equations has no solution. Physically, when $x \rightarrow 0$ or $y \rightarrow 0$, the function blows up. Also, when either $x \rightarrow \infty$ or $y \rightarrow \infty$, the function also blows up. Since it takes finite values in between and is continuous and differentiable, it should have some minimum value(s), and hence an optimal point. Where am I going wrong ? SOLVED Thanks to Arthur, I have realised the error in the above statements. I have assumed that whenever x or y becomes large, the function will blow up. But if I were to go along the path $y = \sqrt{\frac{1}{x^3}}$, and take $x$ smaller and smaller, $f = 2\sqrt x$ will go to 0. Hence, the infimum is $0$",,"['calculus', 'multivariable-calculus', 'optimization', 'a.m.-g.m.-inequality']"
53,Is there a constructive proof for Heine Borel Theorem?,Is there a constructive proof for Heine Borel Theorem?,,"Is there a constructive proof for Heine Borel Theorem, where the Heine Borel Theorem refers to every open covering of a closed bounded interval has a finite sub-cover? The proof I know use proof by contradiction. However, is it good to have a constructive proof (i.e., really give a way to construct the finite sub-cover from arbitrary open covering)? And are there such proofs?","Is there a constructive proof for Heine Borel Theorem, where the Heine Borel Theorem refers to every open covering of a closed bounded interval has a finite sub-cover? The proof I know use proof by contradiction. However, is it good to have a constructive proof (i.e., really give a way to construct the finite sub-cover from arbitrary open covering)? And are there such proofs?",,"['real-analysis', 'multivariable-calculus', 'metric-spaces', 'real-numbers']"
54,Prove that $f$ is Lipschitz iff its derivative is bounded,Prove that  is Lipschitz iff its derivative is bounded,f,"Let $f$ be a vector-valued, continuously differentiable function on an open, convex set $U$ in $\mathbb R^n$ with values in $\mathbb R^m$. Show that $\|Df(c)(y)\| \le 2\|y\|$ for all $c\in U, y\in \mathbb R^n$ iff $\|f(x)-f(z)\|\le 2\|x-z\|$ for all $x, z\in U$. My attempt: $(\Rightarrow)$ $\|Df(c)(y)\| \le 2\|y\|$ means that the operator norm of $Df$ is bounded by $2$, and $U$ is convex, so the conclusion follows. I had no difficulty figuring out this. $(\Leftarrow)$ What I am stuck is to prove the converse. I attempted as follows: Let $\gamma(t)=tx+(1-t)z$ for $0\le t\le 1$ and $g(t)=f(\gamma(t))$. Then $$\|f(x)-f(z)\|=\|g(1)-g(0)\|\le \|g'(\tilde{t})\|=\|Df(\gamma(\tilde{t}))(x-z)\|,$$ and I'm stuck here. Does anyone have ideas? Thank you for your help!","Let $f$ be a vector-valued, continuously differentiable function on an open, convex set $U$ in $\mathbb R^n$ with values in $\mathbb R^m$. Show that $\|Df(c)(y)\| \le 2\|y\|$ for all $c\in U, y\in \mathbb R^n$ iff $\|f(x)-f(z)\|\le 2\|x-z\|$ for all $x, z\in U$. My attempt: $(\Rightarrow)$ $\|Df(c)(y)\| \le 2\|y\|$ means that the operator norm of $Df$ is bounded by $2$, and $U$ is convex, so the conclusion follows. I had no difficulty figuring out this. $(\Leftarrow)$ What I am stuck is to prove the converse. I attempted as follows: Let $\gamma(t)=tx+(1-t)z$ for $0\le t\le 1$ and $g(t)=f(\gamma(t))$. Then $$\|f(x)-f(z)\|=\|g(1)-g(0)\|\le \|g'(\tilde{t})\|=\|Df(\gamma(\tilde{t}))(x-z)\|,$$ and I'm stuck here. Does anyone have ideas? Thank you for your help!",,"['real-analysis', 'multivariable-calculus', 'continuity', 'lipschitz-functions']"
55,"Proof of property of normals to a star shaped region (""elementary"" vector calculus lemma in Evans' PDE)","Proof of property of normals to a star shaped region (""elementary"" vector calculus lemma in Evans' PDE)",,"I cannot follow the proof of the following Lemma (Evans's PDE , p. 515). Lemma. If $U\subset\mathbb{R}^n$ is an open star-shaped region with $\partial U\in C^1$, then $x\cdot\nu(x)\ge0$ $\forall x\in \partial U$, where $\nu$ denotes the outward normal. Proof. We assume $U$ is star-shaped with respect to the origin $0$. Since $\partial U$ is $C^1$, if $x\in\partial U$ then for each $\varepsilon >0$ there exists $\delta >0$ such that $|y-x|<\delta$ and $y\in\bar{U}$ imply  $$\nu(x)\cdot\frac{y-x}{|y-x|}\le\varepsilon.$$ Why is that? In particular $$\limsup_{y \to x, y \in \overline{U}} \nu(x) \cdot \frac{y-x}{|y-x|} \le 0.$$ Let $y = \lambda x$ for $0 < \lambda < 1$. Then $y \in \overline{U}$ since $U$ is star-shaped  with respect to the origin $0$. Therefore $$\nu(x)\cdot\frac{x}{|x|} = - \lim_{\lambda \to 1^-} \nu(x) \cdot\frac{\lambda x-x}{|\lambda x-x|}.$$ Why is that? But we have shown that the last term is $\ge 0$, hence we are done.","I cannot follow the proof of the following Lemma (Evans's PDE , p. 515). Lemma. If $U\subset\mathbb{R}^n$ is an open star-shaped region with $\partial U\in C^1$, then $x\cdot\nu(x)\ge0$ $\forall x\in \partial U$, where $\nu$ denotes the outward normal. Proof. We assume $U$ is star-shaped with respect to the origin $0$. Since $\partial U$ is $C^1$, if $x\in\partial U$ then for each $\varepsilon >0$ there exists $\delta >0$ such that $|y-x|<\delta$ and $y\in\bar{U}$ imply  $$\nu(x)\cdot\frac{y-x}{|y-x|}\le\varepsilon.$$ Why is that? In particular $$\limsup_{y \to x, y \in \overline{U}} \nu(x) \cdot \frac{y-x}{|y-x|} \le 0.$$ Let $y = \lambda x$ for $0 < \lambda < 1$. Then $y \in \overline{U}$ since $U$ is star-shaped  with respect to the origin $0$. Therefore $$\nu(x)\cdot\frac{x}{|x|} = - \lim_{\lambda \to 1^-} \nu(x) \cdot\frac{\lambda x-x}{|\lambda x-x|}.$$ Why is that? But we have shown that the last term is $\ge 0$, hence we are done.",,"['calculus', 'real-analysis']"
56,Divergence chain rule,Divergence chain rule,,"Suppose I have two vector fields $f,g : \mathbb{R}^{2} \rightarrow \mathbb{R}^{2}$. How do I calculate $\vec{\nabla} \cdot f(g(\vec{x}))$? Thank you very much!","Suppose I have two vector fields $f,g : \mathbb{R}^{2} \rightarrow \mathbb{R}^{2}$. How do I calculate $\vec{\nabla} \cdot f(g(\vec{x}))$? Thank you very much!",,['calculus']
57,"Derivatives of the Lagrangian $L=L(x, \dot x , t)$",Derivatives of the Lagrangian,"L=L(x, \dot x , t)","Let me say I consider this a mathematics post, not a physics one -- correct me if I'm wrong. In Lagrangian Mechanics we consider the Lagrangian function, which is a function of the position, the velocity and time $L(x, \dot x,t)$. In arbitrary coordinates it has the form $L= L (q, \dot q,t) $ (where $q$ is a vector if we have a problem in $2$ or $3$ dimensions). Suppose we change the coordinate system to $q'=f(q,t)$ so that $\dot {q'}={\partial f \over \partial q} \dot q + {\partial f \over \partial t}$. Then we get a new Lagrangian by $$L'\left(q', \dot q',t\right)=  L'\left(f(q,t),{\partial f \over \partial q} \dot q + {\partial f \over \partial t},t\right) = L(q,\dot q,t).$$ We know these two Lagrangians describe the same system, when the Euler-Lagrange equations are implemented on them (a bit a grammar issue here maybe- sorry). These two functions are different, but for a given time $t$, their values coincide. I want to show that $$ {d \over dt} \left({ \partial L \over \partial{\dot q}}\right) - {\partial L \over \partial q} = {\partial f \over \partial q} \left[ {d \over dt} \left( { \partial L' \over \partial{ \dot q'}}\right) -{ \partial L' \over \partial q'}\right] \space \space (1)$$ So my question is how do I approach this? The functions $L$, $L'$ are not equal (as functions), I only have a relation between the values of them in time $t$. More specifically how can I use the chain rule for a function like $L'$, what are the partial derivatives and what's its time derivative? I'm not so much interested in showing $(1)$ as I'm interesting in knowing how to find these derivatives.","Let me say I consider this a mathematics post, not a physics one -- correct me if I'm wrong. In Lagrangian Mechanics we consider the Lagrangian function, which is a function of the position, the velocity and time $L(x, \dot x,t)$. In arbitrary coordinates it has the form $L= L (q, \dot q,t) $ (where $q$ is a vector if we have a problem in $2$ or $3$ dimensions). Suppose we change the coordinate system to $q'=f(q,t)$ so that $\dot {q'}={\partial f \over \partial q} \dot q + {\partial f \over \partial t}$. Then we get a new Lagrangian by $$L'\left(q', \dot q',t\right)=  L'\left(f(q,t),{\partial f \over \partial q} \dot q + {\partial f \over \partial t},t\right) = L(q,\dot q,t).$$ We know these two Lagrangians describe the same system, when the Euler-Lagrange equations are implemented on them (a bit a grammar issue here maybe- sorry). These two functions are different, but for a given time $t$, their values coincide. I want to show that $$ {d \over dt} \left({ \partial L \over \partial{\dot q}}\right) - {\partial L \over \partial q} = {\partial f \over \partial q} \left[ {d \over dt} \left( { \partial L' \over \partial{ \dot q'}}\right) -{ \partial L' \over \partial q'}\right] \space \space (1)$$ So my question is how do I approach this? The functions $L$, $L'$ are not equal (as functions), I only have a relation between the values of them in time $t$. More specifically how can I use the chain rule for a function like $L'$, what are the partial derivatives and what's its time derivative? I'm not so much interested in showing $(1)$ as I'm interesting in knowing how to find these derivatives.",,"['multivariable-calculus', 'derivatives', 'partial-derivative', 'euler-lagrange-equation']"
58,Applying Stokes' theorem - what surface?,Applying Stokes' theorem - what surface?,,"$\def\d{\mathrm{d}}$Determine the integral $$\oint_L \mathbf{A} \cdot \,\d\mathbf{r},$$   where $$\mathbf{A} = \mathbf{e}_x(x^2-a(y+z))+\mathbf{e}_y(y^2-az)+\mathbf{e}_z(z^2-a(x+y)),$$   and $L$ is the curve given by the intersection between the cylinder $$\begin{cases}(x-a)^2+y^2=a^2 \\z\geq0\end{cases}$$   and the sphere $$x^2+y^2+z^2=R^2, \quad (R^2>4a^2)$$   The orientation is such that at $x=0$ the tangent to the curve is parallel with $-\mathbf{e}_y$. Attempted solution: Let's apply Stokes' theorem. First, let me introduce a graphical representation of the problem. The path $L$ will then, as seen from above, be the following: A simple calculation shows $\nabla \times \textbf{A} = (0,0,a)$. Here comes my problem... Question What surface am I looking to take a surface integral over? Is it the whole cylinder or just the ""top""? How can I determine?","$\def\d{\mathrm{d}}$Determine the integral $$\oint_L \mathbf{A} \cdot \,\d\mathbf{r},$$   where $$\mathbf{A} = \mathbf{e}_x(x^2-a(y+z))+\mathbf{e}_y(y^2-az)+\mathbf{e}_z(z^2-a(x+y)),$$   and $L$ is the curve given by the intersection between the cylinder $$\begin{cases}(x-a)^2+y^2=a^2 \\z\geq0\end{cases}$$   and the sphere $$x^2+y^2+z^2=R^2, \quad (R^2>4a^2)$$   The orientation is such that at $x=0$ the tangent to the curve is parallel with $-\mathbf{e}_y$. Attempted solution: Let's apply Stokes' theorem. First, let me introduce a graphical representation of the problem. The path $L$ will then, as seen from above, be the following: A simple calculation shows $\nabla \times \textbf{A} = (0,0,a)$. Here comes my problem... Question What surface am I looking to take a surface integral over? Is it the whole cylinder or just the ""top""? How can I determine?",,"['multivariable-calculus', 'vector-analysis', 'surface-integrals', 'stokes-theorem']"
59,Is there ever a case where a two-dimensional limit exists but not the one dimensional limits?,Is there ever a case where a two-dimensional limit exists but not the one dimensional limits?,,"I'm just learning some proof-based multivariable calculus, and in Apostol Volume 2 chapter 8.5 exercise number 2 there is the following statement. If  $$ \lim_{(x,y)\to (a,b)}f(x,y)=L $$ and $\lim_{x\to a}f(x,y)$ and $\lim_{y\to b}f(x,y)$ both exist, prove that $$ \lim_{x \to a}[\lim_{y \to b}f(x,y)] = \lim_{y \to b}[\lim_{x \to a}f(x,y)]=L. $$ This is easy enough to believe. My question is whether it is redundant to state that the single variable limits exist, given that the two variable limit exists. Is there ever a case where the two dimensional limit exists but the individual one dimensional limits don't? Thanks.","I'm just learning some proof-based multivariable calculus, and in Apostol Volume 2 chapter 8.5 exercise number 2 there is the following statement. If  $$ \lim_{(x,y)\to (a,b)}f(x,y)=L $$ and $\lim_{x\to a}f(x,y)$ and $\lim_{y\to b}f(x,y)$ both exist, prove that $$ \lim_{x \to a}[\lim_{y \to b}f(x,y)] = \lim_{y \to b}[\lim_{x \to a}f(x,y)]=L. $$ This is easy enough to believe. My question is whether it is redundant to state that the single variable limits exist, given that the two variable limit exists. Is there ever a case where the two dimensional limit exists but the individual one dimensional limits don't? Thanks.",,"['real-analysis', 'multivariable-calculus', 'proof-writing']"
60,Frechet derivative chain rule,Frechet derivative chain rule,,"I need to prove that, Fix $U\subset \mathbb{R}^m, V\subset \mathbb{R}^n$ open, $f =  (f_1,\cdots,f_n):U\to\mathbb{R}^n$ such that $f(U)\subset V$ and each   coordinate function $f_k:U\to\mathbb{R}$ is differentiable at the   point $a\in U$ . Let $g:V\to \mathbb{R}$ be a differentiable function   at the point $b=f(a)$ . Then, the composite $g\circ f:U\to\mathbb{R}$ is differentiable in $a$ and its partial derivatives are given by: $$\frac{\partial(g\circ f)}{\partial x_i}(a) =  \sum_{k=1}^n\frac{\partial g}{\partial y_k}(b)\cdot \frac{\partial  f_k}{\partial x_i}(a)$$ Here's the demonstration in my book: Let $U_0$ be the set of vectors $v = (\alpha_1,\cdots,\alpha_n)\in\mathbb{R}^m$ such that $a+v\in U$ . For $v\in U_0$ and $k=1,\cdots,n$ we have: $$f_k(a+v) = f_k(a)+\sum_{i=1}\frac{\partial f_k}{\partial x_i}\cdot \alpha_i + p_k\cdot |v|$$ where each $p_k = p_k(v)$ is a function defined in $U_0$ , continuous in point $0$ , which annihilates when $v=0$ (We'll consider $\frac{\partial f_k}{\partial x_i}$ and $\frac{\partial g}{\partial y_k}$ to be in the points $a$ and $b$ respectively). Consider the application $w = (\beta_1,\cdots,\beta_n):U_0\to\mathbb{R}^n$ , continuous in $0$ such that the coordinate functions are defined by: $$\beta_k(v) = \sum_{i=1}^m\frac{\partial f_k}{\partial x_i}\cdot \alpha_i + p_k\cdot |v|$$ Adopting, for example, the norm of the sum, we have $|\alpha_i|/|v|\le 1$ if $v\neq 0$ , therefore each $|\beta_k|/|v|$ , and therefore the function $|w|/|v|$ , is bounded in a neighborhood of $v=0$ . Writing $gf$ instead of $g\circ f$ , we can say that by the formula for $f_k(a+v)$ given before, that, from differentiability of $g$ in $b=f(a)$ and that for al v\in U_0$, we have: $$gf(a+v) = g(b+w) = g(b) + \sum_{i=1}^{n}\frac{\partial g}{\partial y_k}\cdot \beta_k + \sigma\cdot |w|$$ where $\sigma = \sigma(v)$ is a continuous real function in $0$ , which anihilates in $v=0$ . Then it just concludes that this is the proof What is that 'writing $gf$ instead of $g\circ f$ ' thing?","I need to prove that, Fix open, such that and each   coordinate function is differentiable at the   point . Let be a differentiable function   at the point . Then, the composite is differentiable in and its partial derivatives are given by: Here's the demonstration in my book: Let be the set of vectors such that . For and we have: where each is a function defined in , continuous in point , which annihilates when (We'll consider and to be in the points and respectively). Consider the application , continuous in such that the coordinate functions are defined by: Adopting, for example, the norm of the sum, we have if , therefore each , and therefore the function , is bounded in a neighborhood of . Writing instead of , we can say that by the formula for given before, that, from differentiability of in and that for al v\in U_0$, we have: where is a continuous real function in , which anihilates in . Then it just concludes that this is the proof What is that 'writing instead of ' thing?","U\subset \mathbb{R}^m, V\subset \mathbb{R}^n f =
 (f_1,\cdots,f_n):U\to\mathbb{R}^n f(U)\subset V f_k:U\to\mathbb{R} a\in U g:V\to \mathbb{R} b=f(a) g\circ f:U\to\mathbb{R} a \frac{\partial(g\circ f)}{\partial x_i}(a) =
 \sum_{k=1}^n\frac{\partial g}{\partial y_k}(b)\cdot \frac{\partial
 f_k}{\partial x_i}(a) U_0 v = (\alpha_1,\cdots,\alpha_n)\in\mathbb{R}^m a+v\in U v\in U_0 k=1,\cdots,n f_k(a+v) = f_k(a)+\sum_{i=1}\frac{\partial f_k}{\partial x_i}\cdot \alpha_i + p_k\cdot |v| p_k = p_k(v) U_0 0 v=0 \frac{\partial f_k}{\partial x_i} \frac{\partial g}{\partial y_k} a b w = (\beta_1,\cdots,\beta_n):U_0\to\mathbb{R}^n 0 \beta_k(v) = \sum_{i=1}^m\frac{\partial f_k}{\partial x_i}\cdot \alpha_i + p_k\cdot |v| |\alpha_i|/|v|\le 1 v\neq 0 |\beta_k|/|v| |w|/|v| v=0 gf g\circ f f_k(a+v) g b=f(a) gf(a+v) = g(b+w) = g(b) + \sum_{i=1}^{n}\frac{\partial g}{\partial y_k}\cdot \beta_k + \sigma\cdot |w| \sigma = \sigma(v) 0 v=0 gf g\circ f","['calculus', 'real-analysis', 'multivariable-calculus', 'derivatives']"
61,2 variables calculus word problem,2 variables calculus word problem,,"A certain factory using $x$ do-dads and $y$ widgets per hour will produce: $T(x,y)=\frac{90x^2}{y}$ thingamabobs per day. Describe the relationship between inputs  $x$ and $y$ such that $3600$ thingamabobs are produced per day. (That is, find the level curve of $T$ corresponding to $T = 3600.$) My answer is wrong and I'm not sure why. This is what I did. $$3600=90x^2y$$ $$\frac{3600}{90x^2}=y$$ $$\frac{40}{x^2}= y$$ Where did I go wrong in answering this question? What should the right solution be?","A certain factory using $x$ do-dads and $y$ widgets per hour will produce: $T(x,y)=\frac{90x^2}{y}$ thingamabobs per day. Describe the relationship between inputs  $x$ and $y$ such that $3600$ thingamabobs are produced per day. (That is, find the level curve of $T$ corresponding to $T = 3600.$) My answer is wrong and I'm not sure why. This is what I did. $$3600=90x^2y$$ $$\frac{3600}{90x^2}=y$$ $$\frac{40}{x^2}= y$$ Where did I go wrong in answering this question? What should the right solution be?",,"['calculus', 'multivariable-calculus']"
62,Can someone please help me understand Implicit Differentiation?,Can someone please help me understand Implicit Differentiation?,,"I have seen how people implicitly differentiate the equation $x^2 + y^2 = c$. $$d/dx(x^2) + d/dx(y^2) = d/dx(c)$$ treating ""$y$"" as ""$f(x)$"" and using the chainrule we get $$2x + 2y(y') = 0$$ and solving for $y'$ $$y'= -2x/2y$$ The problem is that I just don´t understand implicit differentiation, I do know the rules but they don´t make any sense to me. The fact that it is valid to differentiate both ""$x$"" and ""$y$"" on the same side of the equation is what´s bothering me and even if I see ""$y$"" as a function of ""$x$"" I just end up imagining $$x^2 + (-x^2 + c) = c$$ which doesn´t help me. I also don´t know very much about partial derivatives but I´m willing to learn about them if that helps me understand implicit differentiation. I really appreciate any thoughts or ideas. Thank you!","I have seen how people implicitly differentiate the equation $x^2 + y^2 = c$. $$d/dx(x^2) + d/dx(y^2) = d/dx(c)$$ treating ""$y$"" as ""$f(x)$"" and using the chainrule we get $$2x + 2y(y') = 0$$ and solving for $y'$ $$y'= -2x/2y$$ The problem is that I just don´t understand implicit differentiation, I do know the rules but they don´t make any sense to me. The fact that it is valid to differentiate both ""$x$"" and ""$y$"" on the same side of the equation is what´s bothering me and even if I see ""$y$"" as a function of ""$x$"" I just end up imagining $$x^2 + (-x^2 + c) = c$$ which doesn´t help me. I also don´t know very much about partial derivatives but I´m willing to learn about them if that helps me understand implicit differentiation. I really appreciate any thoughts or ideas. Thank you!",,"['multivariable-calculus', 'implicit-differentiation', 'implicit-function-theorem']"
63,When does one consider the laplacian as a dirac delta function?,When does one consider the laplacian as a dirac delta function?,,"If we consider that $$\nabla^2\left(\frac{1}{r}\right) = -4\pi\delta(\vec{r})$$ we can explain the dirac-delta function here via the results of Gauss' law $$\int_V \nabla^2\left(\frac{1}{r}\right) = \int_S\nabla\left(\frac{1}{r}\right)\cdot d\vec{a} = \int_S \frac{-\hat{r}}{r^2}\cdot d\vec{a} = -4\pi\frac{R^2}{r^2} $$ where the surface integral has been taken over a sphere of radius $R$. Now, if we say $r>0$ and $R\to 0$, we get  $$\int_V \nabla^2\left(\frac{1}{r}\right) = 0. $$ Furthermore, if we set $r=R$ and send $R\to0$ we get  $$\int_V \nabla^2\left(\frac{1}{r}\right) = -4\pi $$ Thus we have demonstrated the validity of the first equation. However, if we were to directly calculate the Laplacian of $\frac 1r$ we obtain $$\frac{1}{r^2}\frac{\partial }{\partial r}\left(r^2\frac{\partial}{\partial r}\left(\frac{1}{r}\right)\right) = 0$$ So it seems like the -$4\pi\delta(\vec{r})$ has somehow been neglected by the direct calculation. That being said, what about the case where we have, for some $\epsilon\in\mathbb{R} > 0$ $$\nabla^2\left(\frac{1}{r^\epsilon}\right) = \frac{1}{r^2}\frac{\partial }{\partial r}\left(r^2\frac{\partial}{\partial r}\left(\frac{1}{r^\epsilon}\right)\right) = \frac{(\epsilon-1)\epsilon}{r^{\epsilon+2}}$$ Has there also been a dirac delta function neglected here? In general, when does one need to consider contributions due to the dirac-delta function?","If we consider that $$\nabla^2\left(\frac{1}{r}\right) = -4\pi\delta(\vec{r})$$ we can explain the dirac-delta function here via the results of Gauss' law $$\int_V \nabla^2\left(\frac{1}{r}\right) = \int_S\nabla\left(\frac{1}{r}\right)\cdot d\vec{a} = \int_S \frac{-\hat{r}}{r^2}\cdot d\vec{a} = -4\pi\frac{R^2}{r^2} $$ where the surface integral has been taken over a sphere of radius $R$. Now, if we say $r>0$ and $R\to 0$, we get  $$\int_V \nabla^2\left(\frac{1}{r}\right) = 0. $$ Furthermore, if we set $r=R$ and send $R\to0$ we get  $$\int_V \nabla^2\left(\frac{1}{r}\right) = -4\pi $$ Thus we have demonstrated the validity of the first equation. However, if we were to directly calculate the Laplacian of $\frac 1r$ we obtain $$\frac{1}{r^2}\frac{\partial }{\partial r}\left(r^2\frac{\partial}{\partial r}\left(\frac{1}{r}\right)\right) = 0$$ So it seems like the -$4\pi\delta(\vec{r})$ has somehow been neglected by the direct calculation. That being said, what about the case where we have, for some $\epsilon\in\mathbb{R} > 0$ $$\nabla^2\left(\frac{1}{r^\epsilon}\right) = \frac{1}{r^2}\frac{\partial }{\partial r}\left(r^2\frac{\partial}{\partial r}\left(\frac{1}{r^\epsilon}\right)\right) = \frac{(\epsilon-1)\epsilon}{r^{\epsilon+2}}$$ Has there also been a dirac delta function neglected here? In general, when does one need to consider contributions due to the dirac-delta function?",,"['multivariable-calculus', 'vectors', 'dirac-delta']"
64,"Show that $\lim\limits_{r\to\infty} \textrm{\{} \|x-ru\|-\|y-ru\| \textrm{}\} = \left<y-x,u\right>$",Show that,"\lim\limits_{r\to\infty} \textrm{\{} \|x-ru\|-\|y-ru\| \textrm{}\} = \left<y-x,u\right>","Let $x,y,u \in \mathbb{R}^2, r\in\mathbb{R}$ and $\|\cdot\|$ be the norm. Show that $$\lim\limits_{r\to\infty} \textrm{\{} \|x-ru\|-\|y-ru\| \textrm{}\} = \left<y-x,u\right>$$ I have to tried to use the polarization identity but am still stuck. I am not able to get rid of the variable $r$ or remove the limit. Any help or insight is appreciated. Maybe the RHS of the above is not in the correct/accurate form, as in my analysis $||u||=1$, but more or less RHS should be achieved in a similar form.","Let $x,y,u \in \mathbb{R}^2, r\in\mathbb{R}$ and $\|\cdot\|$ be the norm. Show that $$\lim\limits_{r\to\infty} \textrm{\{} \|x-ru\|-\|y-ru\| \textrm{}\} = \left<y-x,u\right>$$ I have to tried to use the polarization identity but am still stuck. I am not able to get rid of the variable $r$ or remove the limit. Any help or insight is appreciated. Maybe the RHS of the above is not in the correct/accurate form, as in my analysis $||u||=1$, but more or less RHS should be achieved in a similar form.",,"['calculus', 'multivariable-calculus', 'vectors']"
65,What is the geometrical meaning of the total differential?,What is the geometrical meaning of the total differential?,,"Could anybody give me a geometrical explanation of the total differential, if there is such? For me(a non-mathematician) it just looks like the generalization of the derivative to more dimensions but in those higher dimensions it hasn't got a geometrical meaning. Am i right? Thank you.","Could anybody give me a geometrical explanation of the total differential, if there is such? For me(a non-mathematician) it just looks like the generalization of the derivative to more dimensions but in those higher dimensions it hasn't got a geometrical meaning. Am i right? Thank you.",,"['multivariable-calculus', 'derivatives', 'vector-analysis']"
66,Matrix calculus - computing the Hessian of a vector-Matrix equation.,Matrix calculus - computing the Hessian of a vector-Matrix equation.,,"Let $\vec{b}=\langle b_1,\dots,b_n\rangle ^T$ be an n-dimensional vector of coefficients. Let $\vec{x}_1,\dots,\vec{x}_n$ be $n$ $p$-dimensional vectors. Let $G(\vec{b})=\log\det\left( \sum_{i=1}^n b_i \vec{x}_i\vec{x}_i^T\right)$. Let $A=\sum_{i=1}^n b_i \vec{x}_i\vec{x}_i^T$. If one wants to compute the $i$-th component of the gradient, we get \begin{eqnarray} \nabla_i G(\vec{b}) &=& \text{Tr}\left(\partial_i A \right) \\ &=& \text{Tr}\left( A^{-1} \vec{x}_i\vec{x}_i^T \right) \\ &=& \text{Tr}\left(\vec{x}_i^T A^{-1} \vec{x}_i  \right) \\ &=& x_i^T A^{-1} x_i \end{eqnarray} I am filling in the details so far of this paper (page 19, before equation (33)). So far I agree with their calculation. However, I do not understand their calculation of the line (33) and (34) in which they calculate the Hessian. They claim that  $$ \nabla^2_{ij} (G(\vec{b})) = -(\vec{x}_i^T A^{-1}\vec{x}_j)^2.  \tag1 $$ I get something different. Using the Matrix Cookbook (equation (61)), I see that  \begin{eqnarray} \partial_j(\vec{x}_i^T A^{-1} \vec{x}_i) &=& -A^{-1}\vec{x}_i\vec{x}_i^T A^{-1}\cdot\partial_i(A) \tag2\\ &=& -A^{-1}\vec{x}_i\vec{x}_i^T A^{-1} \vec{x}_j\vec{x}_j^T, \end{eqnarray} which is a matrix and not a scalar! I know I must be making a mistake somewhere. I am still not quite comfortable with matrix calculus. Can someone help me figure out where I'm going wrong?","Let $\vec{b}=\langle b_1,\dots,b_n\rangle ^T$ be an n-dimensional vector of coefficients. Let $\vec{x}_1,\dots,\vec{x}_n$ be $n$ $p$-dimensional vectors. Let $G(\vec{b})=\log\det\left( \sum_{i=1}^n b_i \vec{x}_i\vec{x}_i^T\right)$. Let $A=\sum_{i=1}^n b_i \vec{x}_i\vec{x}_i^T$. If one wants to compute the $i$-th component of the gradient, we get \begin{eqnarray} \nabla_i G(\vec{b}) &=& \text{Tr}\left(\partial_i A \right) \\ &=& \text{Tr}\left( A^{-1} \vec{x}_i\vec{x}_i^T \right) \\ &=& \text{Tr}\left(\vec{x}_i^T A^{-1} \vec{x}_i  \right) \\ &=& x_i^T A^{-1} x_i \end{eqnarray} I am filling in the details so far of this paper (page 19, before equation (33)). So far I agree with their calculation. However, I do not understand their calculation of the line (33) and (34) in which they calculate the Hessian. They claim that  $$ \nabla^2_{ij} (G(\vec{b})) = -(\vec{x}_i^T A^{-1}\vec{x}_j)^2.  \tag1 $$ I get something different. Using the Matrix Cookbook (equation (61)), I see that  \begin{eqnarray} \partial_j(\vec{x}_i^T A^{-1} \vec{x}_i) &=& -A^{-1}\vec{x}_i\vec{x}_i^T A^{-1}\cdot\partial_i(A) \tag2\\ &=& -A^{-1}\vec{x}_i\vec{x}_i^T A^{-1} \vec{x}_j\vec{x}_j^T, \end{eqnarray} which is a matrix and not a scalar! I know I must be making a mistake somewhere. I am still not quite comfortable with matrix calculus. Can someone help me figure out where I'm going wrong?",,"['multivariable-calculus', 'derivatives', 'matrix-calculus']"
67,How do I compute the gradient vector of pixels in an image?,How do I compute the gradient vector of pixels in an image?,,"I'm trying to find the curvature of the features in an image and I was advised to calculate the gradient vector of pixels. So if the matrix below are the values from a grayscale image, how would I go about calculating the gradient vector for the pixel with the value '99'? 21 20 22 24 18 11 23 21 20 22 24 18 11 23 21 20 22 24 18 11 23 21 20 22 99 18 11 23 21 20 22 24 18 11 23 21 20 22 24 18 11 23 21 20 22 24 18 11 23 Apologies for asking such an open ended question, I've never done much maths and am not sure how to start tackling this.","I'm trying to find the curvature of the features in an image and I was advised to calculate the gradient vector of pixels. So if the matrix below are the values from a grayscale image, how would I go about calculating the gradient vector for the pixel with the value '99'? 21 20 22 24 18 11 23 21 20 22 24 18 11 23 21 20 22 24 18 11 23 21 20 22 99 18 11 23 21 20 22 24 18 11 23 21 20 22 24 18 11 23 21 20 22 24 18 11 23 Apologies for asking such an open ended question, I've never done much maths and am not sure how to start tackling this.",,"['multivariable-calculus', 'image-processing', 'gradient-flows']"
68,Showing that the gradient is orthogonal to level surface,Showing that the gradient is orthogonal to level surface,,"It is well known that the gradient of a function (which is sufficiently well behaving) $g(x)$ is orthogonal to its level surface, for example $g(x)=0$. I have seen the following derivation of this fact in a book I am following: We first note that at any point on the constraint surface the gradient $\nabla g(x)$ of   the constraint function will be orthogonal to the surface. To see this, consider a point   $x$ that lies on the constraint surface, and consider a nearby point $x + \epsilon$ that also lies   on the surface. If we make a Taylor expansion around $x$, we have $g(x + \epsilon) \approx g(x) + \epsilon^T\nabla g(x)$. Because both $x$ and $x+ \epsilon$ lie on the constraint surface, we have $g(x) = g(x+ \epsilon)$ and   hence $\epsilon^T \nabla g(x) \approx 0$. In the limit $||\epsilon|| \to 0$ we have $\epsilon^T \nabla g(x)=0$, and because $\epsilon$ is then parallel to the constraint surface $g(x) = 0$, we see that the vector $\nabla g$ is normal   to the surface. I am trying to build a complete proof out of this description. By the definition of differentiability it should be: $$ \lim_{\epsilon \to 0}\dfrac{|g(x + \epsilon) - g(x) - \epsilon^{T}\nabla g(x)|}{||\epsilon||}=0 \tag{1}$$ Since $x + \epsilon$ is assumed to be on the level surface, we have $ \lim_{\epsilon \to 0}\dfrac{|- \epsilon^{T}\nabla g(x)|}{||\epsilon||}=0$. But the problematic part is we only consider $\epsilon$ which causes $x+\epsilon$ to be on the level surface. The classic delta-epsilon limit evaluation covers all possible $\epsilon$, so I can't imagine a way to modify this to work with only ""feasible"" $\epsilon$, as stated in the description. Moreover, in the limit $||\epsilon|| \to 0$  $\epsilon$ disappears and I can't see how we can say that it is parallel to the surface there; it just does not make sense. So, how can we build a proof which follows the given description above? I think the definition of the derivative at $(1)$ is the starting point but I can't see how it should go on.","It is well known that the gradient of a function (which is sufficiently well behaving) $g(x)$ is orthogonal to its level surface, for example $g(x)=0$. I have seen the following derivation of this fact in a book I am following: We first note that at any point on the constraint surface the gradient $\nabla g(x)$ of   the constraint function will be orthogonal to the surface. To see this, consider a point   $x$ that lies on the constraint surface, and consider a nearby point $x + \epsilon$ that also lies   on the surface. If we make a Taylor expansion around $x$, we have $g(x + \epsilon) \approx g(x) + \epsilon^T\nabla g(x)$. Because both $x$ and $x+ \epsilon$ lie on the constraint surface, we have $g(x) = g(x+ \epsilon)$ and   hence $\epsilon^T \nabla g(x) \approx 0$. In the limit $||\epsilon|| \to 0$ we have $\epsilon^T \nabla g(x)=0$, and because $\epsilon$ is then parallel to the constraint surface $g(x) = 0$, we see that the vector $\nabla g$ is normal   to the surface. I am trying to build a complete proof out of this description. By the definition of differentiability it should be: $$ \lim_{\epsilon \to 0}\dfrac{|g(x + \epsilon) - g(x) - \epsilon^{T}\nabla g(x)|}{||\epsilon||}=0 \tag{1}$$ Since $x + \epsilon$ is assumed to be on the level surface, we have $ \lim_{\epsilon \to 0}\dfrac{|- \epsilon^{T}\nabla g(x)|}{||\epsilon||}=0$. But the problematic part is we only consider $\epsilon$ which causes $x+\epsilon$ to be on the level surface. The classic delta-epsilon limit evaluation covers all possible $\epsilon$, so I can't imagine a way to modify this to work with only ""feasible"" $\epsilon$, as stated in the description. Moreover, in the limit $||\epsilon|| \to 0$  $\epsilon$ disappears and I can't see how we can say that it is parallel to the surface there; it just does not make sense. So, how can we build a proof which follows the given description above? I think the definition of the derivative at $(1)$ is the starting point but I can't see how it should go on.",,"['calculus', 'multivariable-calculus', 'derivatives']"
69,Laplace-Beltrami on a Curve,Laplace-Beltrami on a Curve,,"Is there a way to write out Laplace-Beltrami operator explicitly for a  sufficiently smooth plane curve given by implicit equation $\,s(x,y)=0\,$? If I knew knew the parametrization of the curve I could have computed metric tensor $\,g_{ij}\,$ and use well-known expression $$ \Delta_{LB} u = \left\lvert g\right\rvert^{-1/2}\partial_i\big(\left\lvert g\right\rvert \,g^{ij}\,\partial_j u \big), $$ where $g^{ij}$ is the dual metric, and $\,\left\lvert g\right\rvert\,$ is the determinant of  $\,g_{ij}$. However, I am looking for an explicit formula for $\,\Delta_{LB}u\,$ which could be useful without knowing parametrization of the curve.","Is there a way to write out Laplace-Beltrami operator explicitly for a  sufficiently smooth plane curve given by implicit equation $\,s(x,y)=0\,$? If I knew knew the parametrization of the curve I could have computed metric tensor $\,g_{ij}\,$ and use well-known expression $$ \Delta_{LB} u = \left\lvert g\right\rvert^{-1/2}\partial_i\big(\left\lvert g\right\rvert \,g^{ij}\,\partial_j u \big), $$ where $g^{ij}$ is the dual metric, and $\,\left\lvert g\right\rvert\,$ is the determinant of  $\,g_{ij}$. However, I am looking for an explicit formula for $\,\Delta_{LB}u\,$ which could be useful without knowing parametrization of the curve.",,"['multivariable-calculus', 'differential-geometry', 'plane-curves', 'laplacian']"
70,"Find $J: S^2\rightarrow \mathbb{R}$, if given $I:S^2\rightarrow \mathbb{R}$, s.t. $I(\vec{a})=\int_{S^2}\vec{n}\cdot\vec{a} J(\vec{n}) ds$.","Find , if given , s.t. .",J: S^2\rightarrow \mathbb{R} I:S^2\rightarrow \mathbb{R} I(\vec{a})=\int_{S^2}\vec{n}\cdot\vec{a} J(\vec{n}) ds,"I thought of the following problem, when we were discussing radiation intensity in an astrophysics lecture. Suppose $\mathbb{R}^3$ is filled with uniform radiation, i.e. there is a function $J:S^2\rightarrow \mathbb{R}$, so that at any point in $\mathbb{R}$ the amount of radiation in direction $\vec{n}$ is $J(\vec{n})$. If we put a unit area with normal vector $\vec{a}$ into space, we can calculate the flux $I$ through it by $I(\vec{a})=\int_{S^2}\vec{n}\cdot\vec{a} J(\vec{n}) ds = \int_{S^2}\cos(\sphericalangle(\vec{n},\vec{a})) J(\vec{n}) ds $. Under what conditions and how is it possible to do the inverse, i.e. calculate $J$ given $I$? Of course there will be several $J$'s for an $I$, since many $J$'s lead to the same $I$. This leads to the next question: 'How big' are the classes of $J$'s that correspond to an $I$? For example, when calculating the flux, the symmetric part of $J$ does not contribute anything. I think, this problem is analogous to calculating the deconvolution of a function convoluted around the cosine, just on a sphere instead of the real line, and the distance between two points on the sphere is the angle of their position vector.","I thought of the following problem, when we were discussing radiation intensity in an astrophysics lecture. Suppose $\mathbb{R}^3$ is filled with uniform radiation, i.e. there is a function $J:S^2\rightarrow \mathbb{R}$, so that at any point in $\mathbb{R}$ the amount of radiation in direction $\vec{n}$ is $J(\vec{n})$. If we put a unit area with normal vector $\vec{a}$ into space, we can calculate the flux $I$ through it by $I(\vec{a})=\int_{S^2}\vec{n}\cdot\vec{a} J(\vec{n}) ds = \int_{S^2}\cos(\sphericalangle(\vec{n},\vec{a})) J(\vec{n}) ds $. Under what conditions and how is it possible to do the inverse, i.e. calculate $J$ given $I$? Of course there will be several $J$'s for an $I$, since many $J$'s lead to the same $I$. This leads to the next question: 'How big' are the classes of $J$'s that correspond to an $I$? For example, when calculating the flux, the symmetric part of $J$ does not contribute anything. I think, this problem is analogous to calculating the deconvolution of a function convoluted around the cosine, just on a sphere instead of the real line, and the distance between two points on the sphere is the angle of their position vector.",,"['multivariable-calculus', 'physics', 'inverse-problems']"
71,"How to obtain a diffeomorphism between $\mathrm{SL}(2,\mathbb{R})$ and $ (\mathbb{C}\!\smallsetminus\!\{0\})\times\mathbb R$?",How to obtain a diffeomorphism between  and ?,"\mathrm{SL}(2,\mathbb{R})  (\mathbb{C}\!\smallsetminus\!\{0\})\times\mathbb R","Could someone please give me a tip on how to show that the map $\mathrm{SL}(2,\mathbb{R}) \to (\mathbb{C}\setminus\{0\})\times\mathbb{R}$ \begin{equation}\begin{pmatrix} a & b\\ c & d \end{pmatrix}\to(a+i c,ab+cd)\end{equation}  is a diffeomorphism? Thanks.","Could someone please give me a tip on how to show that the map $\mathrm{SL}(2,\mathbb{R}) \to (\mathbb{C}\setminus\{0\})\times\mathbb{R}$ \begin{equation}\begin{pmatrix} a & b\\ c & d \end{pmatrix}\to(a+i c,ab+cd)\end{equation}  is a diffeomorphism? Thanks.",,"['real-analysis', 'multivariable-calculus']"
72,Show that the given function is a diffeomorphism,Show that the given function is a diffeomorphism,,"Let $U=\{x\in\mathbb{R}^n: ||x||<1\}$. If we define $f:U\rightarrow\mathbb{R}^n$ by $f(x) = \displaystyle\frac{x}{\sqrt{1-||x||^2}}$, show that $f$ is a diffeomorphism and $f^{-1}:\mathbb{R}^n\rightarrow U$ is given by $f(y) = \displaystyle\frac{y}{\sqrt{1-||y||^2}}$. (1) Proving bijectiviy (1.1) Injectivity $f(x) = f(y) \iff \displaystyle\frac{x_i}{\sqrt{1-||x||^2}}=\displaystyle\frac{y_i}{\sqrt{1-||y||^2}} \iff x_i^2[1-(y_1^2+\dots+y_n^2)] =y_i[1-(x_1^2+\dots+x_n^2)]$. Now adding all of the following identities $\begin{cases} x_1^2 -x_1^2y_1^2+\dots+x_1^2y_n^2 = y_1^2-x_1^2y_1^2+\dots+x_n^2y_1^2 & (1)\\ \dots\\ x_n^2 -x_n^2y_1^2+\dots+x_n^2y_n^2 = y_n^2-x_1^2y_n^2+\dots+x_n^2y_n^2 & (n)\end{cases}$ I have $x_1^2+\dots+x_n^2 = y_1^2+\dots+y_n^2 \implies ||x||=||y||$. Then $\displaystyle\frac{x_i}{\sqrt{1-||x||^2}}=\displaystyle\frac{y_i}{\sqrt{1-||y||^2}} \iff \displaystyle\frac{x_i}{\sqrt{1-\alpha}}=\displaystyle\frac{y_i}{\sqrt{1-\alpha}}\iff x_i=y_i\implies x=y$. (1.2) Surjectiviy I'm not sure how should I do this, see (3.3) (2) Proving that $f$ is differentiable applying the quotient rule to the coordinate functions If $i\neq j$ then $\displaystyle\frac{\partial f_i}{\partial x_j}= \displaystyle\frac{\partial}{\partial x_j}\left( \displaystyle\frac{x_i}{1-||x||^2}\right) = \displaystyle\frac{\frac{2x_i}{2\sqrt{1-||x||}}}{{1-||x||}} = \displaystyle\frac{x_i}{(1-||x||^2)^{3/2}}$ which is continous for every $x\in U$. And $\displaystyle\frac{\partial f_i}{\partial x_i} = \displaystyle\frac{\partial}{\partial x_i}\left( \displaystyle\frac{x_i}{1-||x||^2}\right)\displaystyle\frac{\sqrt{1-||x||^2}-2x_i^2}{1-||x||^2} = \displaystyle\frac{1}{\sqrt{1-||x||^2}} - \displaystyle\frac{2x_i^2}{1-||x||^2}$ which is also continuous if $x\in U$, right?. By the continuity of the partial derivatives for $f_i$ I conclude that $f_i$ is differentiable and since it applies for every $i=1,\dots,n$ I conclude $f$ that is differentiable. (3) Some questions. I'm not sure how to proceed from here, I have a few guesses and questions: (3.1) The differential for $f$ will be a matrix with values computed above, right?. I can use the inverse function theorem to find the inverse of $f$ by $(f^{-1})'(y)=1/(f'(y))^{-1}$, but how does it works? -I mean, am I supposed to divide by the matrix?-. (3.2) Even if I get $f^{-1})'$, how can I get $f^{-1}$. Should I integrate the coordinate functions? (3.3) I'm not sure how to prove the surjectivity either, could I use that $\operatorname{dim}(U)=\operatorname{dim}(\mathbb{R}^n)$ the injectivity implies surjectivity?. But wouldn't I need a basis for $U$?, can I use the canonical basis with the restriction that $\sum_i x_i^2 < 1$?.","Let $U=\{x\in\mathbb{R}^n: ||x||<1\}$. If we define $f:U\rightarrow\mathbb{R}^n$ by $f(x) = \displaystyle\frac{x}{\sqrt{1-||x||^2}}$, show that $f$ is a diffeomorphism and $f^{-1}:\mathbb{R}^n\rightarrow U$ is given by $f(y) = \displaystyle\frac{y}{\sqrt{1-||y||^2}}$. (1) Proving bijectiviy (1.1) Injectivity $f(x) = f(y) \iff \displaystyle\frac{x_i}{\sqrt{1-||x||^2}}=\displaystyle\frac{y_i}{\sqrt{1-||y||^2}} \iff x_i^2[1-(y_1^2+\dots+y_n^2)] =y_i[1-(x_1^2+\dots+x_n^2)]$. Now adding all of the following identities $\begin{cases} x_1^2 -x_1^2y_1^2+\dots+x_1^2y_n^2 = y_1^2-x_1^2y_1^2+\dots+x_n^2y_1^2 & (1)\\ \dots\\ x_n^2 -x_n^2y_1^2+\dots+x_n^2y_n^2 = y_n^2-x_1^2y_n^2+\dots+x_n^2y_n^2 & (n)\end{cases}$ I have $x_1^2+\dots+x_n^2 = y_1^2+\dots+y_n^2 \implies ||x||=||y||$. Then $\displaystyle\frac{x_i}{\sqrt{1-||x||^2}}=\displaystyle\frac{y_i}{\sqrt{1-||y||^2}} \iff \displaystyle\frac{x_i}{\sqrt{1-\alpha}}=\displaystyle\frac{y_i}{\sqrt{1-\alpha}}\iff x_i=y_i\implies x=y$. (1.2) Surjectiviy I'm not sure how should I do this, see (3.3) (2) Proving that $f$ is differentiable applying the quotient rule to the coordinate functions If $i\neq j$ then $\displaystyle\frac{\partial f_i}{\partial x_j}= \displaystyle\frac{\partial}{\partial x_j}\left( \displaystyle\frac{x_i}{1-||x||^2}\right) = \displaystyle\frac{\frac{2x_i}{2\sqrt{1-||x||}}}{{1-||x||}} = \displaystyle\frac{x_i}{(1-||x||^2)^{3/2}}$ which is continous for every $x\in U$. And $\displaystyle\frac{\partial f_i}{\partial x_i} = \displaystyle\frac{\partial}{\partial x_i}\left( \displaystyle\frac{x_i}{1-||x||^2}\right)\displaystyle\frac{\sqrt{1-||x||^2}-2x_i^2}{1-||x||^2} = \displaystyle\frac{1}{\sqrt{1-||x||^2}} - \displaystyle\frac{2x_i^2}{1-||x||^2}$ which is also continuous if $x\in U$, right?. By the continuity of the partial derivatives for $f_i$ I conclude that $f_i$ is differentiable and since it applies for every $i=1,\dots,n$ I conclude $f$ that is differentiable. (3) Some questions. I'm not sure how to proceed from here, I have a few guesses and questions: (3.1) The differential for $f$ will be a matrix with values computed above, right?. I can use the inverse function theorem to find the inverse of $f$ by $(f^{-1})'(y)=1/(f'(y))^{-1}$, but how does it works? -I mean, am I supposed to divide by the matrix?-. (3.2) Even if I get $f^{-1})'$, how can I get $f^{-1}$. Should I integrate the coordinate functions? (3.3) I'm not sure how to prove the surjectivity either, could I use that $\operatorname{dim}(U)=\operatorname{dim}(\mathbb{R}^n)$ the injectivity implies surjectivity?. But wouldn't I need a basis for $U$?, can I use the canonical basis with the restriction that $\sum_i x_i^2 < 1$?.",,['multivariable-calculus']
73,Does integration by parts work for partial derivatives?,Does integration by parts work for partial derivatives?,,"Does integration by parts works for partial derivatives? Can we write $$\int_a^b \frac{\partial f(x,y)}{\partial x}g(x,y) dx = f(x,y)g(x,y)|_a^b - \int f(x,y)\frac{\partial g(x,y)}{\partial x}dx$$","Does integration by parts works for partial derivatives? Can we write $$\int_a^b \frac{\partial f(x,y)}{\partial x}g(x,y) dx = f(x,y)g(x,y)|_a^b - \int f(x,y)\frac{\partial g(x,y)}{\partial x}dx$$",,['multivariable-calculus']
74,Volume of the first octant under a surface,Volume of the first octant under a surface,,"Find the volume of the first octant region under the surface $\sqrt{x}+\sqrt{y}+\sqrt{z}=1$ I think that the integral should be: $$\int_{0}^1\int_{0}^{\left(1-\sqrt x\right)^2}\int_{0}^{\left(1-\sqrt x -\sqrt y\right)^2}\,dz\,dy\,dx$$ Could someone tell me if this is correct?","Find the volume of the first octant region under the surface $\sqrt{x}+\sqrt{y}+\sqrt{z}=1$ I think that the integral should be: $$\int_{0}^1\int_{0}^{\left(1-\sqrt x\right)^2}\int_{0}^{\left(1-\sqrt x -\sqrt y\right)^2}\,dz\,dy\,dx$$ Could someone tell me if this is correct?",,['multivariable-calculus']
75,"How to use Lagrange Multipliers, when the constraint surface has a boundary?","How to use Lagrange Multipliers, when the constraint surface has a boundary?",,"The method called Lagrange Multipliers is used to find critical points of $f(x_1,x_2,\ldots,x_n)$, when $f$ is constrained to the level set $S = \{ x\in \mathbb{R}^n \, | \, g(x_1,x_2,\ldots,x_n)=0 \}$. These critical points may be used to locate local minima and maxima. When $S$ is a compact set, we know that a global max and min of $f$ exist on $S$. Do we also have to check what happens on the boundary of $S$, in case a boundary exists ?","The method called Lagrange Multipliers is used to find critical points of $f(x_1,x_2,\ldots,x_n)$, when $f$ is constrained to the level set $S = \{ x\in \mathbb{R}^n \, | \, g(x_1,x_2,\ldots,x_n)=0 \}$. These critical points may be used to locate local minima and maxima. When $S$ is a compact set, we know that a global max and min of $f$ exist on $S$. Do we also have to check what happens on the boundary of $S$, in case a boundary exists ?",,"['multivariable-calculus', 'lagrange-multiplier']"
76,Find volume using double integrals?,Find volume using double integrals?,,"Question: Use double integral to find the volume of the solid enclosed by the spheres $x^2+y^2+z^2=1$ and $x^2+y^2+(z-1)^2=1$ Alright so I tried to doing this by myself and I'm not sure if this is right. Could someone check over my work? Curve of intersection: \begin{align*} x^2 + y^2 + z^2 &= x^2 + y^2 + (z - 1)^2\\ \implies z^2 &= z^2 - 2z + 1\\ \implies z &= 1/2. \end{align*} So, the curve of intersection is $x^2 + y^2 = 3/4$ with $z = 1/2$. By symmetry, it suffices to double the volume between $z = 1/2$ and $z = 1$. $x^2 + y^2 + (z-1)^2 = 1$ is above $x^2 + y^2 + z^2 = 1$ when $z > 1/2$. Solving for (positive) $z$ yields $z = \sqrt{1 - x^2 - y^2}$ and $z = 1 + \sqrt{1 - x^2 - y^2}$. Hence, the volume equals \begin{align*} V &= 2  \iint \left[(1 + \sqrt{1 - x^2 - y^2}) - \sqrt{1 - x^2 - y^2}\right]\, dA\\ \\ &= 2 \int_0^{2\pi}\!\int_0^{\sqrt{3/4}}\!r\,dr \,d\theta, \qquad\textrm{via polar coordinates}\\ \\ &= 2 \int_0^{2\pi} \!\left[(1/2)r^2\right]_{r = 0}^{\sqrt{3/4}}\, d\theta\\ \\ &= \int_0^{2\pi}\!3/4\,d\theta\\ \\ &= 3\pi/2. \end{align*}","Question: Use double integral to find the volume of the solid enclosed by the spheres $x^2+y^2+z^2=1$ and $x^2+y^2+(z-1)^2=1$ Alright so I tried to doing this by myself and I'm not sure if this is right. Could someone check over my work? Curve of intersection: \begin{align*} x^2 + y^2 + z^2 &= x^2 + y^2 + (z - 1)^2\\ \implies z^2 &= z^2 - 2z + 1\\ \implies z &= 1/2. \end{align*} So, the curve of intersection is $x^2 + y^2 = 3/4$ with $z = 1/2$. By symmetry, it suffices to double the volume between $z = 1/2$ and $z = 1$. $x^2 + y^2 + (z-1)^2 = 1$ is above $x^2 + y^2 + z^2 = 1$ when $z > 1/2$. Solving for (positive) $z$ yields $z = \sqrt{1 - x^2 - y^2}$ and $z = 1 + \sqrt{1 - x^2 - y^2}$. Hence, the volume equals \begin{align*} V &= 2  \iint \left[(1 + \sqrt{1 - x^2 - y^2}) - \sqrt{1 - x^2 - y^2}\right]\, dA\\ \\ &= 2 \int_0^{2\pi}\!\int_0^{\sqrt{3/4}}\!r\,dr \,d\theta, \qquad\textrm{via polar coordinates}\\ \\ &= 2 \int_0^{2\pi} \!\left[(1/2)r^2\right]_{r = 0}^{\sqrt{3/4}}\, d\theta\\ \\ &= \int_0^{2\pi}\!3/4\,d\theta\\ \\ &= 3\pi/2. \end{align*}",,['multivariable-calculus']
77,Proof that differential of differential form $=0$ i.e $d(df) = 0$.,Proof that differential of differential form  i.e .,=0 d(df) = 0,"Let $f$ be a differentiable function on an open space $U \subset \mathbb{R}^n$. Proove that $d(df) = 0$. So my proof is: Let $$f = \sum c_{i_1, \cdots i_k}(x_*)dx_{i_1} \wedge \cdots dx_{i_k}$$ where $c_{i_1, \cdots i_k}$ are constants and $x_* = (x_1, \cdots x_n)$. We then get $$df = \sum d[c_{i_1, \cdots i_k}(x_*)] \wedge dx_{i_1} \wedge \cdots dx_{i_k}$$ $$\implies d(df) = \sum d\{ d[c_{i_1, \cdots i_k}(x_*)] \wedge dx_{i_1} \wedge \cdots dx_{i_k}\} = 0.$$ How does that last line $=0$ though?","Let $f$ be a differentiable function on an open space $U \subset \mathbb{R}^n$. Proove that $d(df) = 0$. So my proof is: Let $$f = \sum c_{i_1, \cdots i_k}(x_*)dx_{i_1} \wedge \cdots dx_{i_k}$$ where $c_{i_1, \cdots i_k}$ are constants and $x_* = (x_1, \cdots x_n)$. We then get $$df = \sum d[c_{i_1, \cdots i_k}(x_*)] \wedge dx_{i_1} \wedge \cdots dx_{i_k}$$ $$\implies d(df) = \sum d\{ d[c_{i_1, \cdots i_k}(x_*)] \wedge dx_{i_1} \wedge \cdots dx_{i_k}\} = 0.$$ How does that last line $=0$ though?",,"['multivariable-calculus', 'differential-forms']"
78,Prove that $\frac{\partial x}{\partial y} \frac{\partial y}{\partial z} \frac{\partial z}{\partial x} = -1$ and verify ideal gas law,Prove that  and verify ideal gas law,\frac{\partial x}{\partial y} \frac{\partial y}{\partial z} \frac{\partial z}{\partial x} = -1,"Ok guys, continuing my passage through edwards... here is the question... thanks for hints/solutions in advance: Suppose $f(x,y,z)=0$ can be solved for each of the three variables $x,y,z$ as a differentiable function of the other two. Then prove that $\displaystyle \frac{\partial x}{\partial y} \frac{\partial y}{\partial z} \frac{\partial z}{\partial x} = -1$ Verify this is the case for the ideal gas equation $pv =RT$ where (where $p,v,T$ are the three variables and $R$ is the constant).","Ok guys, continuing my passage through edwards... here is the question... thanks for hints/solutions in advance: Suppose $f(x,y,z)=0$ can be solved for each of the three variables $x,y,z$ as a differentiable function of the other two. Then prove that $\displaystyle \frac{\partial x}{\partial y} \frac{\partial y}{\partial z} \frac{\partial z}{\partial x} = -1$ Verify this is the case for the ideal gas equation $pv =RT$ where (where $p,v,T$ are the three variables and $R$ is the constant).",,"['calculus', 'real-analysis', 'multivariable-calculus']"
79,Specific path in a gradient field of a saddle,Specific path in a gradient field of a saddle,,"I think this is a classic question but I could not find a useful reference to understand it so I post it. Take the classic saddle $z=x^2-y^2$. When you sit over it in $(0,0)$ you see in front of you that the function rises and your legs are over a parabola which is the less wide one. Now take a more complicated saddle-like function such as $z=(3x+3y)^2-(4x+y)^2$. In this way the saddle isn't symmetric and I want to determine the curve which has the same role of the ""less wide parabola"" of the first case (so where to put my legs comfortably in a certain sense) I thought that a possible solution could be to calculate the gradient field (which is immediate since only derivations are needed) and then determine the path which goes through the ""steepest descent"" through the gradients. So this path is the curve I'm looking for (the equivalent of the ""less wide parabola""). Is the approach to my problem correct? How do I calculate analytically the steepest descent? Indeed I need a procedure that can be applied to more complicated functions than the two mentioned. Thank you!","I think this is a classic question but I could not find a useful reference to understand it so I post it. Take the classic saddle $z=x^2-y^2$. When you sit over it in $(0,0)$ you see in front of you that the function rises and your legs are over a parabola which is the less wide one. Now take a more complicated saddle-like function such as $z=(3x+3y)^2-(4x+y)^2$. In this way the saddle isn't symmetric and I want to determine the curve which has the same role of the ""less wide parabola"" of the first case (so where to put my legs comfortably in a certain sense) I thought that a possible solution could be to calculate the gradient field (which is immediate since only derivations are needed) and then determine the path which goes through the ""steepest descent"" through the gradients. So this path is the curve I'm looking for (the equivalent of the ""less wide parabola""). Is the approach to my problem correct? How do I calculate analytically the steepest descent? Indeed I need a procedure that can be applied to more complicated functions than the two mentioned. Thank you!",,"['calculus', 'multivariable-calculus']"
80,A lemma about extension of function,A lemma about extension of function,,"Definition Suppose that $f(M)$ is a $\mathcal C^n$-function whose domain is $\mathcal X$. If $f^*(M)$ is a $\mathcal C^n$-function whose domain is $\mathcal X^*$, and $f(M)=f^*(M)$ whenever $M\in\mathcal X\cap\mathcal X^*$, we call that $f^*$ is an ($\mathcal C^n$-)extension of $f$, and $f$ is ($\mathcal C^n$-)extended into $\mathcal X^*$. Lemma Given that $f(x,y)$ is $\mathcal C^n$ ($n\ge1$) function on some bounded open set $\mathcal M\subset\Bbb R^2$, whose boundary is $\mathcal L$, and for each point on $\mathcal L$, there's a neighborhood into which $f$ could be $\mathcal C^n$-extended. We conclude that $f$ could be extended into $\Bbb R^2$. Source Григорий Михайлович Фихтенгольц I found that the proof on the book was so complicated for me to understand, so I'm looking for some explanation, as intuitive as possible. Can anyone help me? Thanks a lot!","Definition Suppose that $f(M)$ is a $\mathcal C^n$-function whose domain is $\mathcal X$. If $f^*(M)$ is a $\mathcal C^n$-function whose domain is $\mathcal X^*$, and $f(M)=f^*(M)$ whenever $M\in\mathcal X\cap\mathcal X^*$, we call that $f^*$ is an ($\mathcal C^n$-)extension of $f$, and $f$ is ($\mathcal C^n$-)extended into $\mathcal X^*$. Lemma Given that $f(x,y)$ is $\mathcal C^n$ ($n\ge1$) function on some bounded open set $\mathcal M\subset\Bbb R^2$, whose boundary is $\mathcal L$, and for each point on $\mathcal L$, there's a neighborhood into which $f$ could be $\mathcal C^n$-extended. We conclude that $f$ could be extended into $\Bbb R^2$. Source Григорий Михайлович Фихтенгольц I found that the proof on the book was so complicated for me to understand, so I'm looking for some explanation, as intuitive as possible. Can anyone help me? Thanks a lot!",,"['calculus', 'real-analysis', 'multivariable-calculus']"
81,surface integral of vector along the curved surface of cylinder,surface integral of vector along the curved surface of cylinder,,"Evaluate  $$ \iint_s (4x \hat i  - 2y^2 \hat j  + z^2 \hat k)\cdot \hat n ds $$ over the curved surface of $x^2 + y^2 = 4$ and $z = 0 \text{ to }z = 3$.  Using method  $$ \iint_s f(x,y(x,z),z)\cdot \frac{\nabla u(x, y)}{|\nabla u(x, y)|} \sqrt{ 1 + \left ( \partial y \over \partial x\right )^2 + \left ( \partial y \over \partial z\right )^2} dxdz$$I got $48 \pi - 128.$ EDIT:: added method for above $$ \int_0^3 \int_{-2}^2  (4x \hat i  - 2y^2 \hat j  + z^2 \hat k)\cdot(\hat i x + \hat iy) \left ( \sqrt{ 1 + \frac{x^2}{4 - x^2 } }\right ) dx dz$$ $$ \implies  12 \int_{-2}^{2} \frac{2x^2}{\sqrt{4 - x^2}} - (4 - x^2)dx = 48 \pi - 128 $$ EDIT:: I got wrong answer below because of  formula. I got right answer from this! The correct formula should have been $$ \iint_s F(\theta, z) \cdot (u_\theta \times u_z) d\theta dz$$ Parametrizing $x = 2 \cos \theta, y = 2\sin\theta , z=z $ $$ \int_0^3 \int_0^{2\pi} F(\theta, z)\cdot \frac{u_{\theta}\times u_z}{|u_{\theta}\times u_z|}d\theta dz$$ I got $24 \pi$. The answer sheet says $48\pi$.Please help!! Thank you!!","Evaluate  $$ \iint_s (4x \hat i  - 2y^2 \hat j  + z^2 \hat k)\cdot \hat n ds $$ over the curved surface of $x^2 + y^2 = 4$ and $z = 0 \text{ to }z = 3$.  Using method  $$ \iint_s f(x,y(x,z),z)\cdot \frac{\nabla u(x, y)}{|\nabla u(x, y)|} \sqrt{ 1 + \left ( \partial y \over \partial x\right )^2 + \left ( \partial y \over \partial z\right )^2} dxdz$$I got $48 \pi - 128.$ EDIT:: added method for above $$ \int_0^3 \int_{-2}^2  (4x \hat i  - 2y^2 \hat j  + z^2 \hat k)\cdot(\hat i x + \hat iy) \left ( \sqrt{ 1 + \frac{x^2}{4 - x^2 } }\right ) dx dz$$ $$ \implies  12 \int_{-2}^{2} \frac{2x^2}{\sqrt{4 - x^2}} - (4 - x^2)dx = 48 \pi - 128 $$ EDIT:: I got wrong answer below because of  formula. I got right answer from this! The correct formula should have been $$ \iint_s F(\theta, z) \cdot (u_\theta \times u_z) d\theta dz$$ Parametrizing $x = 2 \cos \theta, y = 2\sin\theta , z=z $ $$ \int_0^3 \int_0^{2\pi} F(\theta, z)\cdot \frac{u_{\theta}\times u_z}{|u_{\theta}\times u_z|}d\theta dz$$ I got $24 \pi$. The answer sheet says $48\pi$.Please help!! Thank you!!",,['multivariable-calculus']
82,"Rigorous but elementary exposition of ""div, grad, curl, and all that""","Rigorous but elementary exposition of ""div, grad, curl, and all that""",,"What book gives a rigorous but elementary exposition of ""div, grad, curl, and all that""?  Conventional second-year calculus books are as far from rigorous as anything ever gets.","What book gives a rigorous but elementary exposition of ""div, grad, curl, and all that""?  Conventional second-year calculus books are as far from rigorous as anything ever gets.",,"['reference-request', 'multivariable-calculus']"
83,Derivation of Basic Level Set Equations,Derivation of Basic Level Set Equations,,"For the level set method, $\phi(\vec{x},t)$ is the level set function in 3D and the level set $\phi(\vec{x},t) = 0$ forms the interface. For evolving $\phi$ the derivation says to imagine a particle $\textbf{x}(t)$ on the surface, then we differentiate with respect to t $\frac{d}{dt}(\phi(\vec{x}, t)=0)$ Using chain rule we get $\frac{\partial \phi}{\partial t} + \triangledown \phi \cdot \frac{d\vec{x}}{dt} = 0$ Then this equation is turned into $\frac{\partial \phi}{\partial t} + \triangledown \phi \cdot \vec{V} = 0$ I guess $\vec{V}$ is the speed, and its elements are to replace $dx$, $dy$, .. that are elements of $d{\vec{x}}$. Hence we can substitute our own displacement and receive a result. What I do not understand is they continue like this: Seperate $\vec{V}$ into normal and tangential components $\frac{\partial \phi}{\partial t} + \triangledown \phi \cdot (\vec{V_N}\vec{N} + \vec{V_T}\vec{T}) = 0$ Then since $\vec{N} = \frac{\triangledown \phi}{|\triangledown \phi |}$ We get $\frac{\partial \phi}{\partial t} + V_N \cdot |\triangledown \phi| = 0$ What happened to $V_T$? Also how can the dot product of $\triangledown \phi$ with  $\frac{\triangledown \phi}{|\triangledown \phi |}$ result in $|\triangledown \phi|$ ? Link: http://www.cs.au.dk/~bang/smokeandwater2006/Lecture9_IntroToWaterAndLS.ppt","For the level set method, $\phi(\vec{x},t)$ is the level set function in 3D and the level set $\phi(\vec{x},t) = 0$ forms the interface. For evolving $\phi$ the derivation says to imagine a particle $\textbf{x}(t)$ on the surface, then we differentiate with respect to t $\frac{d}{dt}(\phi(\vec{x}, t)=0)$ Using chain rule we get $\frac{\partial \phi}{\partial t} + \triangledown \phi \cdot \frac{d\vec{x}}{dt} = 0$ Then this equation is turned into $\frac{\partial \phi}{\partial t} + \triangledown \phi \cdot \vec{V} = 0$ I guess $\vec{V}$ is the speed, and its elements are to replace $dx$, $dy$, .. that are elements of $d{\vec{x}}$. Hence we can substitute our own displacement and receive a result. What I do not understand is they continue like this: Seperate $\vec{V}$ into normal and tangential components $\frac{\partial \phi}{\partial t} + \triangledown \phi \cdot (\vec{V_N}\vec{N} + \vec{V_T}\vec{T}) = 0$ Then since $\vec{N} = \frac{\triangledown \phi}{|\triangledown \phi |}$ We get $\frac{\partial \phi}{\partial t} + V_N \cdot |\triangledown \phi| = 0$ What happened to $V_T$? Also how can the dot product of $\triangledown \phi$ with  $\frac{\triangledown \phi}{|\triangledown \phi |}$ result in $|\triangledown \phi|$ ? Link: http://www.cs.au.dk/~bang/smokeandwater2006/Lecture9_IntroToWaterAndLS.ppt",,"['calculus', 'differential-geometry', 'multivariable-calculus']"
84,Prove $\frac{a}{\sqrt{4a+3bc}}+\frac{b}{\sqrt{4b+3ca}}+\frac{c}{\sqrt{4c+3ab}}\le \frac{\sqrt{a+b+c+2}}{2}.$,Prove,\frac{a}{\sqrt{4a+3bc}}+\frac{b}{\sqrt{4b+3ca}}+\frac{c}{\sqrt{4c+3ab}}\le \frac{\sqrt{a+b+c+2}}{2}.,"Problem. Let $a,b,c\ge 0: ab+bc+ca=1.$ Prove that $$\frac{a}{\sqrt{4a+3bc}}+\frac{b}{\sqrt{4b+3ca}}+\frac{c}{\sqrt{4c+3ab}}\le \frac{\sqrt{a+b+c+2}}{2}.$$ Equality holds at $abc=0.$ I've tried to use Cauchy-Schwarz inequality $$\sum_{cyc}\sqrt{\frac{a}{4a+3bc}}\cdot \sqrt{a}\le \sqrt{(a+b+c)\cdot \left(\frac{a}{4a+3bc}+\frac{b}{4b+3ca}+\frac{c}{4c+3ab}\right)},$$ hence it's enough to prove $$\frac{a}{4a+3bc}+\frac{b}{4b+3ca}+\frac{c}{4c+3ab}\le \frac{a+b+c+2}{4(a+b+c)}.$$ The last inequality is wrong when $a=0$ . I'd like to ask a better approach. All idea and comment are welcome.",Problem. Let Prove that Equality holds at I've tried to use Cauchy-Schwarz inequality hence it's enough to prove The last inequality is wrong when . I'd like to ask a better approach. All idea and comment are welcome.,"a,b,c\ge 0: ab+bc+ca=1. \frac{a}{\sqrt{4a+3bc}}+\frac{b}{\sqrt{4b+3ca}}+\frac{c}{\sqrt{4c+3ab}}\le \frac{\sqrt{a+b+c+2}}{2}. abc=0. \sum_{cyc}\sqrt{\frac{a}{4a+3bc}}\cdot \sqrt{a}\le \sqrt{(a+b+c)\cdot \left(\frac{a}{4a+3bc}+\frac{b}{4b+3ca}+\frac{c}{4c+3ab}\right)}, \frac{a}{4a+3bc}+\frac{b}{4b+3ca}+\frac{c}{4c+3ab}\le \frac{a+b+c+2}{4(a+b+c)}. a=0","['multivariable-calculus', 'inequality', 'cauchy-schwarz-inequality', 'symmetric-polynomials', 'uvw']"
85,Why can't I use the power rule on this partial derivative?,Why can't I use the power rule on this partial derivative?,,"I came across this question on Khan Academy: let $f(x, y, z) = \sqrt{xyz}$ , find $\frac{\partial f}{\partial x}\bigg\rvert_{(-1, -1, 4)}$ My first intuition was to use the power rule: $$ \frac{\partial f}{\partial x}=\sqrt{yz}\cdot\frac{\partial}{\partial x}\big(\sqrt{x}\big)=\frac{\sqrt{yz}}{2\sqrt{x}} $$ $$ \frac{\partial f}{\partial x}\bigg\rvert_{(-1, -1, 4)}=\frac{\sqrt{yz}}{2\sqrt{x}}\bigg\rvert_{(-1, -1, 4)}=\frac{\sqrt{(-1)(4)}}{2\sqrt{-1}}=\frac{i\sqrt{4}}{2i}=\frac{2i}{2i}=1 $$ However, the official solution used the chain rule and arrived at a different result $$ \frac{\partial f}{\partial x}=\frac{\partial f}{\partial (xyz)}\cdot\frac{\partial (xyz)}{\partial x}=\frac{yz}{2\sqrt{xyz}} $$ $$ \frac{\partial f}{\partial x}\bigg\rvert_{(-1, -1, 4)}=\frac{yz}{2\sqrt{xyz}}\bigg\rvert_{(-1, -1, 4)}=\frac{(-1)(4)}{2\sqrt{(-1)(-1)(4)}}=\frac{-4}{2\sqrt{4}}=\frac{-4}{4}=-1 $$ Where was my mistake? If it was with the differentiation, what are some situations where I'm not allowed to take the power rule? If it was with complex numbers, what would be a general rule to prevent myself from making this mistake again? (I know $\sqrt{(-1)(-1)} \neq\sqrt{-1}\cdot\sqrt{-1}$ but that's about it)","I came across this question on Khan Academy: let , find My first intuition was to use the power rule: However, the official solution used the chain rule and arrived at a different result Where was my mistake? If it was with the differentiation, what are some situations where I'm not allowed to take the power rule? If it was with complex numbers, what would be a general rule to prevent myself from making this mistake again? (I know but that's about it)","f(x, y, z) = \sqrt{xyz} \frac{\partial f}{\partial x}\bigg\rvert_{(-1, -1, 4)} 
\frac{\partial f}{\partial x}=\sqrt{yz}\cdot\frac{\partial}{\partial x}\big(\sqrt{x}\big)=\frac{\sqrt{yz}}{2\sqrt{x}}
 
\frac{\partial f}{\partial x}\bigg\rvert_{(-1, -1, 4)}=\frac{\sqrt{yz}}{2\sqrt{x}}\bigg\rvert_{(-1, -1, 4)}=\frac{\sqrt{(-1)(4)}}{2\sqrt{-1}}=\frac{i\sqrt{4}}{2i}=\frac{2i}{2i}=1
 
\frac{\partial f}{\partial x}=\frac{\partial f}{\partial (xyz)}\cdot\frac{\partial (xyz)}{\partial x}=\frac{yz}{2\sqrt{xyz}}
 
\frac{\partial f}{\partial x}\bigg\rvert_{(-1, -1, 4)}=\frac{yz}{2\sqrt{xyz}}\bigg\rvert_{(-1, -1, 4)}=\frac{(-1)(4)}{2\sqrt{(-1)(-1)(4)}}=\frac{-4}{2\sqrt{4}}=\frac{-4}{4}=-1
 \sqrt{(-1)(-1)} \neq\sqrt{-1}\cdot\sqrt{-1}","['multivariable-calculus', 'complex-numbers', 'partial-derivative']"
86,Intuition about existence of directional derivatives doesn't imply differentiability [duplicate],Intuition about existence of directional derivatives doesn't imply differentiability [duplicate],,"This question already has answers here : Why does existence of directional derivatives not imply differentiability? (4 answers) Closed last year . I am aware of several examples showing that existence of directional derivative in all directions does not imply differentiability at a point, but why is the existence of directional derivatives not enough to assure differentiability? For example, I understand the idea and relation between smoothness and differentiability, but how come smoothness cannot be guaranteed when we just know directional derivatives exist? Hope I am clear about my concern. Thanks!","This question already has answers here : Why does existence of directional derivatives not imply differentiability? (4 answers) Closed last year . I am aware of several examples showing that existence of directional derivative in all directions does not imply differentiability at a point, but why is the existence of directional derivatives not enough to assure differentiability? For example, I understand the idea and relation between smoothness and differentiability, but how come smoothness cannot be guaranteed when we just know directional derivatives exist? Hope I am clear about my concern. Thanks!",,"['multivariable-calculus', 'intuition']"
87,Perturbation of identity still surjective?,Perturbation of identity still surjective?,,"Let $f : \mathbb{R^n}\mapsto \mathbb{R^n}$ a smooth function. Can we give a condition for the application $F=I+f$ , where $I$ is the identity, to be surjective ? I know that if the operator norm of the differential of $f$ is small, $\vert Df\vert <= 1/2$ , say, then $F$ is a submersion, but it does not tell me that it is surjective. On the other hand, if we look at the problem for $n=1$ , then we have $F'>1/2$ , which of course implies that $F$ is surjective. In other words we avoid counter-examples like $arctan$ , which are not surjective submersions. Can such a result be extended to higher dimensions ? Edit : Another way to formulate this, is that $f$ is $1/2$ -Lipschitz. Any additionnal hypothesis on $f$ is welcomed Edit 2 :  More precisely, wlog suppose that $f(0)=0$ , then the function $F$ is a quasi isometry in the sens that $\frac{1}{2}\vert\vert F(x) - F(y)\vert\vert \leq \vert\vert x - y\vert\vert \leq 2\vert\vert F(x) - F(y)\vert\vert$ . It is known that an isometry of $\mathbb{R^n}$ must be surjective, see for instance Isometries of $\mathbb{R}^n$","Let a smooth function. Can we give a condition for the application , where is the identity, to be surjective ? I know that if the operator norm of the differential of is small, , say, then is a submersion, but it does not tell me that it is surjective. On the other hand, if we look at the problem for , then we have , which of course implies that is surjective. In other words we avoid counter-examples like , which are not surjective submersions. Can such a result be extended to higher dimensions ? Edit : Another way to formulate this, is that is -Lipschitz. Any additionnal hypothesis on is welcomed Edit 2 :  More precisely, wlog suppose that , then the function is a quasi isometry in the sens that . It is known that an isometry of must be surjective, see for instance Isometries of $\mathbb{R}^n$",f : \mathbb{R^n}\mapsto \mathbb{R^n} F=I+f I f \vert Df\vert <= 1/2 F n=1 F'>1/2 F arctan f 1/2 f f(0)=0 F \frac{1}{2}\vert\vert F(x) - F(y)\vert\vert \leq \vert\vert x - y\vert\vert \leq 2\vert\vert F(x) - F(y)\vert\vert \mathbb{R^n},"['calculus', 'multivariable-calculus', 'differential-geometry']"
88,Volume integral of $(x+y+z)^{2n}$ over unit sphere,Volume integral of  over unit sphere,(x+y+z)^{2n},"$\iiint_{V}(x+y+z)^{2n}\,dV$ where $V$ is the unit sphere I tried converting to spherical polars but ended up needing to solve $$\int_0^\pi\int_0^{2\pi}(\sin\theta\cos\phi+\sin\theta\sin\phi + \cos\theta)^{2n}\sin\theta\,d\phi\,d\theta$$ which I cannot see a way of doing, anyone have any ideas?","where is the unit sphere I tried converting to spherical polars but ended up needing to solve which I cannot see a way of doing, anyone have any ideas?","\iiint_{V}(x+y+z)^{2n}\,dV V \int_0^\pi\int_0^{2\pi}(\sin\theta\cos\phi+\sin\theta\sin\phi + \cos\theta)^{2n}\sin\theta\,d\phi\,d\theta",['multivariable-calculus']
89,Partial Derivative Chain Rule When Variables Are Not Independent,Partial Derivative Chain Rule When Variables Are Not Independent,,"Let's say, $x$ is a function of $t$ ( $x = x(t)$ ) and $y$ is a function of $t$ ( $y = y(t)$ ). And, $f$ is a function of $x$ and $y$ ( $f = f(x, y)$ ). Then by the chain rule $$\frac{df}{dt} = \frac{\partial f}{\partial x}\frac{dx}{dt} + \frac{\partial f}{\partial y}\frac{dy}{dt}$$ . However, when you take the partial derivative of a function wrt a variable you keep all other variables constant. So, I am not sure how $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ above can be calculated, since you cannot keep $y$ constant if $x$ is allowed to be varied and vice-versa. Can someone please explain what I am understanding wrong?","Let's say, is a function of ( ) and is a function of ( ). And, is a function of and ( ). Then by the chain rule . However, when you take the partial derivative of a function wrt a variable you keep all other variables constant. So, I am not sure how and above can be calculated, since you cannot keep constant if is allowed to be varied and vice-versa. Can someone please explain what I am understanding wrong?","x t x = x(t) y t y = y(t) f x y f = f(x, y) \frac{df}{dt} = \frac{\partial f}{\partial x}\frac{dx}{dt} + \frac{\partial f}{\partial y}\frac{dy}{dt} \frac{\partial f}{\partial x} \frac{\partial f}{\partial y} y x","['multivariable-calculus', 'partial-derivative', 'chain-rule']"
90,Why does the method of Lagrange Multipliers fail when $\nabla g =0$?,Why does the method of Lagrange Multipliers fail when ?,\nabla g =0,"I know that one of the preconditions to find the extrema of $f(x,y)$ subject to $g(x,y)=0$ using the method of Lagrange Multipliers is that $\nabla g \neq 0$ . I do understand that if $\nabla g$ does equal zero, we will have to equate ( $(0,0)$ with some other finite ordered pair representing the gradient of $f$ and we will end up missing an extremum. I am looking for a more intuitive reason for why this is a condition. For instance, why does it graphically mean that we will end up skipping an extremum? Why can't we say that, well, since we cannot equate $(0,0)$ with some $(a,b): a,b\neq0$ , such an extremum doesn't exist at all?","I know that one of the preconditions to find the extrema of subject to using the method of Lagrange Multipliers is that . I do understand that if does equal zero, we will have to equate ( with some other finite ordered pair representing the gradient of and we will end up missing an extremum. I am looking for a more intuitive reason for why this is a condition. For instance, why does it graphically mean that we will end up skipping an extremum? Why can't we say that, well, since we cannot equate with some , such an extremum doesn't exist at all?","f(x,y) g(x,y)=0 \nabla g \neq 0 \nabla g (0,0) f (0,0) (a,b): a,b\neq0","['multivariable-calculus', 'optimization', 'maxima-minima', 'lagrange-multiplier']"
91,"Find the Jacobian of $f(x,y,z)=x^y$",Find the Jacobian of,"f(x,y,z)=x^y","Let $f(x,y,z)=x^y$ . I am trying to find the Jacobian $f’(a,b,c)$ without using partial derivatives, i.e. using the Chain Rule and several other elementary results on component functions etc.. Since $x^y=\exp(y\cdot\ln(x))$ , using the identity function $\pi$ we can write $$f(x,y,z)=\exp\left(\pi^2 \cdot \ln(\pi^1)\right)(x,y,z).$$ The proof then seems to require the Chain Rule, with $g(x)=\exp(x)$ and $h(x,y,z) =\pi^2 \ln(\pi^1)(x,y,z)$ , so that $f=g\circ h $ and $f’(a,b,c)= g’\left(h(a,b,c)\right)\cdot h’(a,b,c)$ . From here, I can calculate the solution $$\left[ba^{b-a}\; \; a^b \ln(a)\; \; 0\right].$$ However,in order to apply the Chain Rule, we require $h:\mathbb{R}^n\to \mathbb{R}^p$ and $g: \mathbb{R}^p \to  \mathbb{R}^m $ so that $f: \mathbb{R}^n \to \mathbb{R}^m$ is defined by $f= g\circ h$ . To get the right solution, however, I had to use $g’(h(a,b,c))=[a^b]$ . Question: How can we use the function $g:\mathbb{R}\to\mathbb{R}$ in the above proof for $f’(x)$ ?","Let . I am trying to find the Jacobian without using partial derivatives, i.e. using the Chain Rule and several other elementary results on component functions etc.. Since , using the identity function we can write The proof then seems to require the Chain Rule, with and , so that and . From here, I can calculate the solution However,in order to apply the Chain Rule, we require and so that is defined by . To get the right solution, however, I had to use . Question: How can we use the function in the above proof for ?","f(x,y,z)=x^y f’(a,b,c) x^y=\exp(y\cdot\ln(x)) \pi f(x,y,z)=\exp\left(\pi^2 \cdot \ln(\pi^1)\right)(x,y,z). g(x)=\exp(x) h(x,y,z) =\pi^2 \ln(\pi^1)(x,y,z) f=g\circ h  f’(a,b,c)= g’\left(h(a,b,c)\right)\cdot h’(a,b,c) \left[ba^{b-a}\; \; a^b \ln(a)\; \; 0\right]. h:\mathbb{R}^n\to \mathbb{R}^p g: \mathbb{R}^p \to  \mathbb{R}^m  f: \mathbb{R}^n \to \mathbb{R}^m f= g\circ h g’(h(a,b,c))=[a^b] g:\mathbb{R}\to\mathbb{R} f’(x)","['real-analysis', 'multivariable-calculus', 'derivatives']"
92,"If $f(x) \leq f(a)$ for all $x$ in an open ball $B(a)$ , then prove that $\nabla f(a) = 0$","If  for all  in an open ball  , then prove that",f(x) \leq f(a) x B(a) \nabla f(a) = 0,"Question statement : Assume $f$ is a scalar field function differentiable at each point of an n-ball $B(a)$ . If $f(x) \leq f(a) \ \ \forall \ x $ in an open ball $B(a)$ , then prove that $\nabla f(a) = 0$ My try : $f(a+he_i)-f(a)\leq0$ where $e_i$ is a unit coordinate vector. Now I divide both sides by $h>0$ and take limit $\lim_{h\to 0} \frac{f(a+he_i)-f(a)}{h}\leq0$ which gives $\frac{\partial f}{\partial x_i}\leq0$ at $a$ . Doing the same thing, but now dividing both sides by $h<0$ and take limit $\lim_{h\to 0} \frac{f(a+he_i)-f(a)}{h}\geq0$ which gives $\frac{\partial f}{\partial x_i}\geq0$ at $a$ . Thus $\frac{\partial f}{\partial x_i}=0$ at $a$ for all $x_i, \ i \in {1,2,...n}$ . Thus $\nabla f(a) = 0$ . I have no clue as to whether this ""weird"" method of mine is correct or not. Someone please guide.","Question statement : Assume is a scalar field function differentiable at each point of an n-ball . If in an open ball , then prove that My try : where is a unit coordinate vector. Now I divide both sides by and take limit which gives at . Doing the same thing, but now dividing both sides by and take limit which gives at . Thus at for all . Thus . I have no clue as to whether this ""weird"" method of mine is correct or not. Someone please guide.","f B(a) f(x) \leq f(a) \ \ \forall \ x  B(a) \nabla f(a) = 0 f(a+he_i)-f(a)\leq0 e_i h>0 \lim_{h\to 0} \frac{f(a+he_i)-f(a)}{h}\leq0 \frac{\partial f}{\partial x_i}\leq0 a h<0 \lim_{h\to 0} \frac{f(a+he_i)-f(a)}{h}\geq0 \frac{\partial f}{\partial x_i}\geq0 a \frac{\partial f}{\partial x_i}=0 a x_i, \ i \in {1,2,...n} \nabla f(a) = 0","['real-analysis', 'multivariable-calculus', 'derivatives', 'scalar-fields']"
93,Clean/simple way of computing $\nabla f(U U^\mathsf{T})$ with respect to $U$,Clean/simple way of computing  with respect to,\nabla f(U U^\mathsf{T}) U,"Let $f(X)$ be a matrix function which takes in an $n \times n$ matrix $X$ and spits out something in $\mathbb{R}$ . Let $U \in \mathbb{R}^{n \times r}$ for some $r << n$ . I was reading a paper which stated without explanation that the gradient of $f$ with respect to $U$ is $$ \left(\nabla f (U U^\mathsf{T}) + \nabla f(U U^\mathsf{T})^\mathsf{T}\right) \cdot U. $$ The gradient of $f$ with respect to $X$ is naturally defined as the matrix $\nabla f(X) \in \mathbb{R}^{n \times n}$ such that $$ [\nabla f(X)]_{ij} = \frac{\partial f(X)}{\partial x_{ij}}. $$ I spent a significant amount of time trying to derive this using the chain rule for multivariate functions. I.e., I thought of $f : \mathbb{R^{n^2}} \to \mathbb{R}$ as being precomposed by $g : \mathbb{R}^{n \times r} \to \mathbb{R}^{n^2}$ defined by $g(U) = U U^\mathsf{T}$ . But I don't see how the fact given follows easily from this. It could be that I'm just far too used to thinking about gradients as vectors, and I can't translate to these matrix functions. Ultimately, I gave up on that approach and verified the identity by working with individual partial derivatives. But my question is, is there a neat way to derive this fact without resorting to working with partial derivatives? If there is, I really want to know it, because I find that I struggle to compute derivatives of matrix functions. How would someone experienced in this area approach this problem? Thank you! EDIT: Thanks to @greg's answer, I have decided to study (matrix) differentials. Here are some resources (in order from favorite to least favorite; but all three are useful) I found which might help others who stumble upon this question: https://tminka.github.io/papers/matrix/ http://artem.sobolev.name/posts/2017-01-29-matrix-and-vector-calculus-via-differentials.html http://thousandfold.net/cz/2013/11/12/a-useful-trick-for-computing-gradients-w-r-t-matrix-arguments-with-some-examples/","Let be a matrix function which takes in an matrix and spits out something in . Let for some . I was reading a paper which stated without explanation that the gradient of with respect to is The gradient of with respect to is naturally defined as the matrix such that I spent a significant amount of time trying to derive this using the chain rule for multivariate functions. I.e., I thought of as being precomposed by defined by . But I don't see how the fact given follows easily from this. It could be that I'm just far too used to thinking about gradients as vectors, and I can't translate to these matrix functions. Ultimately, I gave up on that approach and verified the identity by working with individual partial derivatives. But my question is, is there a neat way to derive this fact without resorting to working with partial derivatives? If there is, I really want to know it, because I find that I struggle to compute derivatives of matrix functions. How would someone experienced in this area approach this problem? Thank you! EDIT: Thanks to @greg's answer, I have decided to study (matrix) differentials. Here are some resources (in order from favorite to least favorite; but all three are useful) I found which might help others who stumble upon this question: https://tminka.github.io/papers/matrix/ http://artem.sobolev.name/posts/2017-01-29-matrix-and-vector-calculus-via-differentials.html http://thousandfold.net/cz/2013/11/12/a-useful-trick-for-computing-gradients-w-r-t-matrix-arguments-with-some-examples/","f(X) n \times n X \mathbb{R} U \in \mathbb{R}^{n \times r} r << n f U 
\left(\nabla f (U U^\mathsf{T}) + \nabla f(U U^\mathsf{T})^\mathsf{T}\right) \cdot U.
 f X \nabla f(X) \in \mathbb{R}^{n \times n} 
[\nabla f(X)]_{ij} = \frac{\partial f(X)}{\partial x_{ij}}.
 f : \mathbb{R^{n^2}} \to \mathbb{R} g : \mathbb{R}^{n \times r} \to \mathbb{R}^{n^2} g(U) = U U^\mathsf{T}","['multivariable-calculus', 'matrix-calculus']"
94,Surface integral over a cylinder problem,Surface integral over a cylinder problem,,"I have a generic cylinder $x^2+y^2=a^2, 0≤z≤h$ labeled as $G$ . It has an outward-pointing unit normal vector $\vec{n}$ . As seen in the picture below: Now I am asked to solve this integral on $G$ : $$\iint\limits_{G}\vec{n}⋅\vec{n}ds$$ Note: I know that the most convenient way to solve that problem, is just to do the calculation of the dot product which equals to 1. From there I can easily proceed to the calculation of the surface area of $G$ which equals to $2\pi ah$ . And here is my problem. I did not pay attention that $\vec{n}⋅\vec{n}=1$ in our case. I went straight forward and noticed that $\vec{n}=∇f(x,y,z)=(2x,2y,0)$ . Then I used the known equation of surface integral: $$\iint\limits_{S}f(x,y,z)ds=\iint\limits_{D}f(x,y,g(x,y))\sqrt{(z_x)^2+(z_y)^2+1} dA$$ And tried to solve the following integral: $$\iint\limits_{S}(2x,2y,0)⋅(2x,2y,0)ds=4\iint\limits_{S}x^2+y^2ds=4\iint\limits_{D}x^2+y^2dA=4\int_{θ=0}^{2\pi}\int_{r=0}^{a}r^3drdθ$$ Here I got a wrong answer. The problem is that there is no reference to value of $z$ which limits the height of that cylinder. I think that I have missed something here, because my way of solving seems logical to me in general, but I am sure that there is a right way to deal with the height constraint here that I can't understand well.","I have a generic cylinder labeled as . It has an outward-pointing unit normal vector . As seen in the picture below: Now I am asked to solve this integral on : Note: I know that the most convenient way to solve that problem, is just to do the calculation of the dot product which equals to 1. From there I can easily proceed to the calculation of the surface area of which equals to . And here is my problem. I did not pay attention that in our case. I went straight forward and noticed that . Then I used the known equation of surface integral: And tried to solve the following integral: Here I got a wrong answer. The problem is that there is no reference to value of which limits the height of that cylinder. I think that I have missed something here, because my way of solving seems logical to me in general, but I am sure that there is a right way to deal with the height constraint here that I can't understand well.","x^2+y^2=a^2, 0≤z≤h G \vec{n} G \iint\limits_{G}\vec{n}⋅\vec{n}ds G 2\pi ah \vec{n}⋅\vec{n}=1 \vec{n}=∇f(x,y,z)=(2x,2y,0) \iint\limits_{S}f(x,y,z)ds=\iint\limits_{D}f(x,y,g(x,y))\sqrt{(z_x)^2+(z_y)^2+1} dA \iint\limits_{S}(2x,2y,0)⋅(2x,2y,0)ds=4\iint\limits_{S}x^2+y^2ds=4\iint\limits_{D}x^2+y^2dA=4\int_{θ=0}^{2\pi}\int_{r=0}^{a}r^3drdθ z","['multivariable-calculus', 'multiple-integral']"
95,Second order derivative of a chain rule (regarding reduction to canonical form),Second order derivative of a chain rule (regarding reduction to canonical form),,"I've been stuck on this for a couple of days. So this is from this book (""Partial Differential Equations in Mechanics 1"", page 125). Section 4.2 Reduction to canonical forms, which leads to the development of the Laplace equation. In this section, I don't understand how they expand the second-order partial derivative: Where, Here is what I got so far. When I do it, I only get to have 4 terms, and not 5 like what's in the book. Here I apply product rule first and then the chain rule (Note, I'm using square brackets to indicate that I am taking the partial derivative of whatever is in them. Just to keep it organized). $$\begin{align} \frac{\partial}{\partial x}\frac{\partial u}{\partial x} &= \\ &= \frac{\partial}{\partial x} \biggl( \frac{\partial u}{\partial \xi} \frac{\partial \xi}{\partial x} + \frac{\partial u}{\partial \eta} \frac{\partial \eta}{\partial x} \biggr) \\ &=\frac{\partial}{\partial x} \biggl( \frac{\partial u}{\partial \xi} \frac{\partial \xi}{\partial x}\biggr) + \frac{\partial}{\partial x} \biggl(\frac{\partial u}{\partial \eta} \frac{\partial \eta}{\partial x} \biggr) \\ &= \frac{\partial}{\partial x} \biggl[ \frac{\partial u}{\partial \xi} \biggr] \frac{\partial \xi}{\partial x} + \frac{\partial u}{\partial \xi} \frac{\partial}{\partial x} \biggl[ \frac{\partial \xi}{\partial x} \biggr] +  \frac{\partial}{\partial x} \biggl[ \frac{\partial u}{\partial \eta} \biggr] \frac{\partial \eta}{\partial x} + \frac{\partial u}{\partial \eta} \frac{\partial}{\partial x} \biggl[ \frac{\partial \eta}{\partial x} \biggr] \\ \text{Now the chain rule:}\\ &= \frac{\partial}{\partial \xi}\biggl[\frac{\partial u}{\partial \xi}\biggr] \frac{\partial \xi}{\partial x} \frac{\partial \xi}{\partial x}  + \frac{\partial u}{\partial \xi} \frac{\partial^2 \xi}{\partial x^2}  + \frac{\partial}{\partial \eta}\biggl[\frac{\partial u}{\partial \eta}\biggr] \frac{\partial \eta}{\partial x} \frac{\partial \eta}{\partial x}  + \frac{\partial u}{\partial \eta} \frac{\partial^2 \eta}{\partial x^2} \\ &=\frac{\partial^2 u}{\partial \xi^2} \biggl(\frac{\partial \xi}{\partial x} \biggr)^2  + \frac{\partial u}{\partial \xi} \frac{\partial^2 \xi}{\partial x^2}  + \frac{\partial^2 u}{\partial \eta^2} \biggl(\frac{\partial \eta}{\partial x} \biggr)^2  + \frac{\partial u}{\partial \eta} \frac{\partial^2 \eta}{\partial x^2}  \end{align} $$ My tree of the chain rule looks like this (is it correct?) In addition, if someone could explain why this chain rule is valid? Granted, this may be a whole topic on its own, so if you could just point to some resource or what this particular operation is called, that would do. $$ \frac{\partial}{\partial x}\biggl[ \frac{\partial u}{\partial \xi} \biggr] = \frac{\partial}{\partial \xi} \biggl[\frac{\partial u}{\partial \xi}\biggr]\frac{\partial \xi}{\partial x} $$ Thank you in advance. UPDATE: (as per answer by @peek-a-boo) P.S. Corrections or edits are welcomed.","I've been stuck on this for a couple of days. So this is from this book (""Partial Differential Equations in Mechanics 1"", page 125). Section 4.2 Reduction to canonical forms, which leads to the development of the Laplace equation. In this section, I don't understand how they expand the second-order partial derivative: Where, Here is what I got so far. When I do it, I only get to have 4 terms, and not 5 like what's in the book. Here I apply product rule first and then the chain rule (Note, I'm using square brackets to indicate that I am taking the partial derivative of whatever is in them. Just to keep it organized). My tree of the chain rule looks like this (is it correct?) In addition, if someone could explain why this chain rule is valid? Granted, this may be a whole topic on its own, so if you could just point to some resource or what this particular operation is called, that would do. Thank you in advance. UPDATE: (as per answer by @peek-a-boo) P.S. Corrections or edits are welcomed.","\begin{align}
\frac{\partial}{\partial x}\frac{\partial u}{\partial x} &= \\
&= \frac{\partial}{\partial x} \biggl( \frac{\partial u}{\partial \xi} \frac{\partial \xi}{\partial x} + \frac{\partial u}{\partial \eta} \frac{\partial \eta}{\partial x} \biggr) \\
&=\frac{\partial}{\partial x} \biggl( \frac{\partial u}{\partial \xi} \frac{\partial \xi}{\partial x}\biggr) + \frac{\partial}{\partial x} \biggl(\frac{\partial u}{\partial \eta} \frac{\partial \eta}{\partial x} \biggr) \\
&= \frac{\partial}{\partial x} \biggl[ \frac{\partial u}{\partial \xi} \biggr] \frac{\partial \xi}{\partial x} + \frac{\partial u}{\partial \xi} \frac{\partial}{\partial x} \biggl[ \frac{\partial \xi}{\partial x} \biggr] + 
\frac{\partial}{\partial x} \biggl[ \frac{\partial u}{\partial \eta} \biggr] \frac{\partial \eta}{\partial x} + \frac{\partial u}{\partial \eta} \frac{\partial}{\partial x} \biggl[ \frac{\partial \eta}{\partial x} \biggr] \\
\text{Now the chain rule:}\\
&= \frac{\partial}{\partial \xi}\biggl[\frac{\partial u}{\partial \xi}\biggr] \frac{\partial \xi}{\partial x} \frac{\partial \xi}{\partial x} 
+ \frac{\partial u}{\partial \xi} \frac{\partial^2 \xi}{\partial x^2} 
+ \frac{\partial}{\partial \eta}\biggl[\frac{\partial u}{\partial \eta}\biggr] \frac{\partial \eta}{\partial x} \frac{\partial \eta}{\partial x} 
+ \frac{\partial u}{\partial \eta} \frac{\partial^2 \eta}{\partial x^2} \\
&=\frac{\partial^2 u}{\partial \xi^2} \biggl(\frac{\partial \xi}{\partial x} \biggr)^2 
+ \frac{\partial u}{\partial \xi} \frac{\partial^2 \xi}{\partial x^2} 
+ \frac{\partial^2 u}{\partial \eta^2} \biggl(\frac{\partial \eta}{\partial x} \biggr)^2 
+ \frac{\partial u}{\partial \eta} \frac{\partial^2 \eta}{\partial x^2} 
\end{align}
 
\frac{\partial}{\partial x}\biggl[ \frac{\partial u}{\partial \xi} \biggr] = \frac{\partial}{\partial \xi} \biggl[\frac{\partial u}{\partial \xi}\biggr]\frac{\partial \xi}{\partial x}
","['multivariable-calculus', 'partial-differential-equations']"
96,Change along some direction is positive,Change along some direction is positive,,"Let: $$f(x_1,\cdots,x_n) = \prod_{i}x_i(1-x_i) \prod_{i<j}|x_i-x_j|$$ Suppose all $x_i \in (0,1)$ are fixed and $\sum_{i}x_i < \frac{n}{2}$ . Show that there is some $i$ and a sufficiently small $\epsilon$ so that $x_i \mapsto x_i +\epsilon$ doesn't decrease the value of $f$ . That is to say, at least one of the partial derivatives of $f$ is non-negative. After taking the derivative in each $x_i$ one gets a system of inequalities. I was able to prove the statement for $n=2,3$ this way through basically brute force. This doesn't generalize well though.","Let: Suppose all are fixed and . Show that there is some and a sufficiently small so that doesn't decrease the value of . That is to say, at least one of the partial derivatives of is non-negative. After taking the derivative in each one gets a system of inequalities. I was able to prove the statement for this way through basically brute force. This doesn't generalize well though.","f(x_1,\cdots,x_n) = \prod_{i}x_i(1-x_i) \prod_{i<j}|x_i-x_j| x_i \in (0,1) \sum_{i}x_i < \frac{n}{2} i \epsilon x_i \mapsto x_i +\epsilon f f x_i n=2,3","['multivariable-calculus', 'inequality']"
97,"Acceleration, velocity and position vectors","Acceleration, velocity and position vectors",,"Given a vector parametrization that describes the position of a particle $\vec{r}(t)$ , is it valid to say that $\vec{r} (t)$ is perpendicular to $\vec{r'} (t)$ through the following: $$ \frac{\mathrm{d}(\vec{r}(t) \cdot \vec{r}(t))}{\mathrm{d}t} = 0 $$ so through the product rule $$ \frac{\mathrm{d}\vec{r}(t)}{\mathrm{d}t} \cdot \vec{r}(t) + \vec{r}(t) \cdot \frac{\mathrm{d}\vec{r}(t)}{\mathrm{d}t} = 0 $$ which means $$ 2\frac{\mathrm{d}\vec{r}(t)}{\mathrm{d}t} \cdot \vec{r}(t) = 0 $$ so $\vec{r}(t) \cdot \vec{r'}(t) = 0$ , meaning that they are perpendicular to each other? Whenever I draw a curve, if I draw a position vector to a minima in that curve and then the velocity vector, they are not perpendicular to each other. I feel like I'm making a very simple mistake.","Given a vector parametrization that describes the position of a particle , is it valid to say that is perpendicular to through the following: so through the product rule which means so , meaning that they are perpendicular to each other? Whenever I draw a curve, if I draw a position vector to a minima in that curve and then the velocity vector, they are not perpendicular to each other. I feel like I'm making a very simple mistake.","\vec{r}(t) \vec{r} (t) \vec{r'} (t) 
\frac{\mathrm{d}(\vec{r}(t) \cdot \vec{r}(t))}{\mathrm{d}t} = 0
 
\frac{\mathrm{d}\vec{r}(t)}{\mathrm{d}t} \cdot \vec{r}(t) + \vec{r}(t) \cdot \frac{\mathrm{d}\vec{r}(t)}{\mathrm{d}t} = 0
 
2\frac{\mathrm{d}\vec{r}(t)}{\mathrm{d}t} \cdot \vec{r}(t) = 0
 \vec{r}(t) \cdot \vec{r'}(t) = 0",['multivariable-calculus']
98,Definition of divergence,Definition of divergence,,"We see in certain places that $$\operatorname{div} \mathbf{F}|_p = \lim_{V \to \{p\}} \iint_{\partial V} \frac{\mathbf{F} \cdot \mathbf{\hat n}}{|V|} \: dS.$$ But what is $V$? Neighborhoods of $p$? Balls around $p$? How can you take a limit that is not as one number approaches another? What is the actual rigorous definition of this limit, and furthermore, divergence? I am already familiar with the $\nabla \cdot F$ definition.","We see in certain places that $$\operatorname{div} \mathbf{F}|_p = \lim_{V \to \{p\}} \iint_{\partial V} \frac{\mathbf{F} \cdot \mathbf{\hat n}}{|V|} \: dS.$$ But what is $V$? Neighborhoods of $p$? Balls around $p$? How can you take a limit that is not as one number approaches another? What is the actual rigorous definition of this limit, and furthermore, divergence? I am already familiar with the $\nabla \cdot F$ definition.",,['multivariable-calculus']
99,The convenient approach to a calculus problem in two variables,The convenient approach to a calculus problem in two variables,,"A student came to me with the following problem. Construct a function $g \colon {\Bbb R}\to {\Bbb R}$ such that the function $f \colon {\Bbb R}^2 \to {\Bbb R}$ defined by $$ f(x,y)= \begin{cases} \frac{e^{x^2}-e^{y^2}}{x-y} &\text{if $x \neq y$} \\ g(x) &\text{if $x=y$} \end{cases} $$ is continuous. Is the function $f$ differentiable at the origin? It is immediate to find the unique $g$ that could solve the problem. However, the student was in trouble since he was unable to check that this function $g$ was an actual solution. The computation of $$\lim_{(x,y) \to (x_0,x_0)} \frac{e^{x^2}-e^{y^2}}{x-y}$$ is intuitive, but the student could not prove rigorously his conjecture. The same for the limit $$\lim_{(x,y) \to (0,0)} \frac{f(x,y)-f(0,0)-\nabla f(0,0)\cdot (x,y)}{\sqrt{x^2+y^2}}.$$ I suggested the use of some Taylor expansion of the exponential around zero, but I wonder if there is some straightforward approach. Actually, the definition of $f$ is given as a ""incremental ratio"" of a smooth function, and I guess there is a general result about the extension of the incremental ratio to the diagonal of $\mathbb{R}^2$. Any suggestion is welcome.","A student came to me with the following problem. Construct a function $g \colon {\Bbb R}\to {\Bbb R}$ such that the function $f \colon {\Bbb R}^2 \to {\Bbb R}$ defined by $$ f(x,y)= \begin{cases} \frac{e^{x^2}-e^{y^2}}{x-y} &\text{if $x \neq y$} \\ g(x) &\text{if $x=y$} \end{cases} $$ is continuous. Is the function $f$ differentiable at the origin? It is immediate to find the unique $g$ that could solve the problem. However, the student was in trouble since he was unable to check that this function $g$ was an actual solution. The computation of $$\lim_{(x,y) \to (x_0,x_0)} \frac{e^{x^2}-e^{y^2}}{x-y}$$ is intuitive, but the student could not prove rigorously his conjecture. The same for the limit $$\lim_{(x,y) \to (0,0)} \frac{f(x,y)-f(0,0)-\nabla f(0,0)\cdot (x,y)}{\sqrt{x^2+y^2}}.$$ I suggested the use of some Taylor expansion of the exponential around zero, but I wonder if there is some straightforward approach. Actually, the definition of $f$ is given as a ""incremental ratio"" of a smooth function, and I guess there is a general result about the extension of the incremental ratio to the diagonal of $\mathbb{R}^2$. Any suggestion is welcome.",,"['multivariable-calculus', 'derivatives', 'education']"
