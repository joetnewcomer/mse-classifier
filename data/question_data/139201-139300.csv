,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Differential Equations: Finding the increasing/decreasing values of autonomous diff eq.,Differential Equations: Finding the increasing/decreasing values of autonomous diff eq.,,"The question gives the differential equation: $${dP\over dt} = 0.4P\left(1-{P\over 230}\right)$$ The textbook question is, "" For what values of $P$ is the population (b) increasing and (c) decreasing. "" I understand that we must first find the values of $P$ the population is in equilibrium which are $P = 0$ and $P = 230$, but I'm generally confused on how the bounds for the values of $P$ when they are increasing, as the solution states that ""if the population is increasing if $\frac{dP}{dt} > 0$ and decreasing if $\frac{dP}{dt} < 0$. Where do the values of equilibrium play into finding this increase/decrease thing and how do I solve that question?","The question gives the differential equation: $${dP\over dt} = 0.4P\left(1-{P\over 230}\right)$$ The textbook question is, "" For what values of $P$ is the population (b) increasing and (c) decreasing. "" I understand that we must first find the values of $P$ the population is in equilibrium which are $P = 0$ and $P = 230$, but I'm generally confused on how the bounds for the values of $P$ when they are increasing, as the solution states that ""if the population is increasing if $\frac{dP}{dt} > 0$ and decreasing if $\frac{dP}{dt} < 0$. Where do the values of equilibrium play into finding this increase/decrease thing and how do I solve that question?",,['ordinary-differential-equations']
1,Will the solution of the boundary value problem be unique?,Will the solution of the boundary value problem be unique?,,"Suppose that we have the following boundary value problem: $$a^2 u_{xx}=u_t, 0<x<L, t>0 \\ u(x,0)=f(x) , 0 \leq x \leq L\\ u(0,t)=0, u(L,t)=0, t>0$$ By supposing that $u(x,t)=X(x) T(t)$ we find that the solution is of the form $u(x,t)=\sum_{n=1}^{\infty} c_n e^{-\frac{n^2 \pi^2 a^2 t}{L^2}} \sin{\frac{n \pi x}{L}}$ where $c_n=\frac{2}{L} \int_0^L f(x) \sin{\frac{n \pi x}{L}}$. But do we know that this solution is unique? Or could there also be an other solution that will not be of the form $X(x) T(t)$ ?","Suppose that we have the following boundary value problem: $$a^2 u_{xx}=u_t, 0<x<L, t>0 \\ u(x,0)=f(x) , 0 \leq x \leq L\\ u(0,t)=0, u(L,t)=0, t>0$$ By supposing that $u(x,t)=X(x) T(t)$ we find that the solution is of the form $u(x,t)=\sum_{n=1}^{\infty} c_n e^{-\frac{n^2 \pi^2 a^2 t}{L^2}} \sin{\frac{n \pi x}{L}}$ where $c_n=\frac{2}{L} \int_0^L f(x) \sin{\frac{n \pi x}{L}}$. But do we know that this solution is unique? Or could there also be an other solution that will not be of the form $X(x) T(t)$ ?",,['ordinary-differential-equations']
2,Solving vector valued differential equation?,Solving vector valued differential equation?,,"I want to solve the following differential equation $$y'(t) = f(y(t),t)$$ where the right hand side is a function that has no closed form expression. I have a vector of numerical values representing $f(y)$. It is time independent (each value corresponds to a grid point in y). How can I solve the differential equation? I tried solving this as a loop as follows: $$y(t+\Delta t) = y(t) + \Delta t f(y(t),t)$$ I use interpolation on the the second term. The problem is that this is very slow and I wonder if this can be solved more efficiently. I would like to calculate how long it takes for $y(t_0)$ to reach a given value $y(T)$. For instance, in the graphs shown below, I want to know how long it takes to reach $\underline{y}$ for every point in the grid of $y$. How could I do this?","I want to solve the following differential equation $$y'(t) = f(y(t),t)$$ where the right hand side is a function that has no closed form expression. I have a vector of numerical values representing $f(y)$. It is time independent (each value corresponds to a grid point in y). How can I solve the differential equation? I tried solving this as a loop as follows: $$y(t+\Delta t) = y(t) + \Delta t f(y(t),t)$$ I use interpolation on the the second term. The problem is that this is very slow and I wonder if this can be solved more efficiently. I would like to calculate how long it takes for $y(t_0)$ to reach a given value $y(T)$. For instance, in the graphs shown below, I want to know how long it takes to reach $\underline{y}$ for every point in the grid of $y$. How could I do this?",,"['ordinary-differential-equations', 'numerical-methods', 'runge-kutta-methods']"
3,Compact general solution to the DE of the form $u'' +u =0$,Compact general solution to the DE of the form,u'' +u =0,"I want to pass from a solution of the form $u(\phi)=c_1 \sin (\phi) + c_2 \cos (\phi)$ to a one that look like this $u(\phi)=A \epsilon \cos (\phi-\phi_0)$ How i can get it analytically? And what is the value of the constant $A$, is it totally arbitrary? Thanks. EDIT: Ok, i know that the boundary values determine a unique solution. But i.e binet's formula has the form $u'' +u = -k$ and the given answer answer (in the paper) is $u = -k + k \epsilon \cos (\phi - \phi _0 )$ where $\epsilon$ and $\phi _0 $ are the boundary values. so $A$ is equaled to $k$  arbitrarily.","I want to pass from a solution of the form $u(\phi)=c_1 \sin (\phi) + c_2 \cos (\phi)$ to a one that look like this $u(\phi)=A \epsilon \cos (\phi-\phi_0)$ How i can get it analytically? And what is the value of the constant $A$, is it totally arbitrary? Thanks. EDIT: Ok, i know that the boundary values determine a unique solution. But i.e binet's formula has the form $u'' +u = -k$ and the given answer answer (in the paper) is $u = -k + k \epsilon \cos (\phi - \phi _0 )$ where $\epsilon$ and $\phi _0 $ are the boundary values. so $A$ is equaled to $k$  arbitrarily.",,"['ordinary-differential-equations', 'trigonometry']"
4,derive $\delta _{tt} y=y$ without guessing,derive  without guessing,\delta _{tt} y=y,"Say we have a second order differential equation $\delta _{tt} y=y$ We can on the basis of our intuition say that the functions $e^t$ and $e^{-t}$ satisfy this second order ODE. We can then reason that the superposition must also satisfy the ODE. But is there a way to derive directly from the ODE, without make the abovementioned educated guess, the general solution of that ODE? (I don't have much experience with second order ODE's).","Say we have a second order differential equation $\delta _{tt} y=y$ We can on the basis of our intuition say that the functions $e^t$ and $e^{-t}$ satisfy this second order ODE. We can then reason that the superposition must also satisfy the ODE. But is there a way to derive directly from the ODE, without make the abovementioned educated guess, the general solution of that ODE? (I don't have much experience with second order ODE's).",,['ordinary-differential-equations']
5,"Showing a function is a function of only a variable $\eta(x,t)$",Showing a function is a function of only a variable,"\eta(x,t)","Suppose I have a differential equation $$\frac{\partial\theta}{\partial t}=\frac{\partial^2\theta}{\partial x^2}$$ satisfied by $\theta(x,t)$ and I then had that $\theta(x,t)=k F(x,t)$, and I know $F(x,t)$ is dimensionless. What is a general method in this situation to answer the question ""Show  $F$ is a function of only the similarity variable $\eta=\eta(x,t)$""? I am not really sure how to approach this as I don't recall ever encountering a problem like this. Any help apprecieted.","Suppose I have a differential equation $$\frac{\partial\theta}{\partial t}=\frac{\partial^2\theta}{\partial x^2}$$ satisfied by $\theta(x,t)$ and I then had that $\theta(x,t)=k F(x,t)$, and I know $F(x,t)$ is dimensionless. What is a general method in this situation to answer the question ""Show  $F$ is a function of only the similarity variable $\eta=\eta(x,t)$""? I am not really sure how to approach this as I don't recall ever encountering a problem like this. Any help apprecieted.",,"['ordinary-differential-equations', 'heat-equation', 'dimensional-analysis']"
6,ODE when the nonhomogeneity is complex,ODE when the nonhomogeneity is complex,,"How should I use the method of undetermined coefficients to solve the following differential equation? I know how to do it when the nonhomogeneity is real, such as polynomial(t) * Sin(t), but I am not sure what to do when the right-hand side is complex. $${\frac {d^{2}y}{dt^{2}}}+2{\frac {dy}{dt}}+4y= \sqrt{3} e^{-t+\sqrt{3}it}$$","How should I use the method of undetermined coefficients to solve the following differential equation? I know how to do it when the nonhomogeneity is real, such as polynomial(t) * Sin(t), but I am not sure what to do when the right-hand side is complex. $${\frac {d^{2}y}{dt^{2}}}+2{\frac {dy}{dt}}+4y= \sqrt{3} e^{-t+\sqrt{3}it}$$",,"['ordinary-differential-equations', 'complex-numbers']"
7,System of differential equations (3x3),System of differential equations (3x3),,"I am stuck in solving the following system: $$y_1'=-2y_1+y_2-2y_3$$ $$y_2'=y_1-2y_2+2y_3$$ $$y_3'=3y_1-3y_2+5y_3$$ EDIT: My book gives the following solutions: $$y_1(x)=-C_1e^{3x}+(C_2-2C_3)e^{-x}$$ $$y_2(x)=C_1e^{3x}+C_2e^{-x}$$ $$y_3(x)=3C_1e^{3x}+C_3e^{-x}$$ Here is my attempt: We differentiate the first equation: $$y_1''=-2y_1'+y_2'-2y_3'$$$$=-y_1+2y_2-4y_3$$ Now we have a $2x2$ system for $y_2,y_3$, using the first equation from the system and the last equation for $y_1''$: $$y_1'=-2y_1+y_2-2y_3$$ $$y_1''=-y_1+2y_2-4y_3$$ Multiplying the first equation by $-2$ and adding these two equations gives: $$y_1''-2y_1'-3y_1=0$$ This gives: $$\lambda_1=3,\lambda_2=-1\Rightarrow y_1(x)=C_1e^{3x}+C_2e^{-x}$$ This doesn't match with the above solution. Is there a mistake? I will show my full attempt with this (maybe wrong) first solution. Now, for finding $y_2,y_3$ we have a new $2x2$ system such that $y_1$ is known: $$y_2'=y_1-2y_2+2y_3$$ $$y_3'=3y_1-3y_2+5y_3$$ Again, the same procedure. We differentiate the first equation: $$y_2''=y_1'-2y_2'+2y_3'$$ $$=y_1'-2y_1+4y_2-4y_3$$ Now, for finding $y_3$ we have a new $2x2$ system such that $y_1,y_2$ are known: $$y_2'=y_1-2y_2+2y_3$$ $$y_2''=y_1'-2y_1+4y_2-4y_3$$ By adding these two, we get: $$y_3=-\frac{1}{2}y_2''-\frac{1}{2}y_2'+\frac{1}{2}y_1'-\frac{1}{2}y_1+y_2$$ Now we differentiate the equation $$y_2''=y_1'-2y_1+4y_2-4y_3$$ to obtain $$y_2'''=y_1''-2y_1'+4y_2'-4y_3'$$ By plugin previously evaluated $y_3$ we get: $$y_2'''+4y_2''+4y_2'=y_1''+2y_1'$$ We already have $y_1$ (note that this may be wrong), so: $$y_1'=3C_1e^{3x}-C_2e^{-x}$$ $$y_1''=9C_1e^{3x}+C_2e^{-x}$$ We have: $$y_2'''+4y_2''+4y_2'=15C_1e^{3x}-C_2e^{-x}$$ Now we will use variation of parameters: $$\lambda^3+4\lambda^2+4\lambda=0$$ $$\lambda_1=0,\lambda_2=\lambda_3=-2$$ This gives multiple roots so it might again be a mistake. I will stop here. Could someone check this? Where are mistakes so far? EDIT: Could someone show the approach with linear algebra, eigenvalues/eigenvectors?","I am stuck in solving the following system: $$y_1'=-2y_1+y_2-2y_3$$ $$y_2'=y_1-2y_2+2y_3$$ $$y_3'=3y_1-3y_2+5y_3$$ EDIT: My book gives the following solutions: $$y_1(x)=-C_1e^{3x}+(C_2-2C_3)e^{-x}$$ $$y_2(x)=C_1e^{3x}+C_2e^{-x}$$ $$y_3(x)=3C_1e^{3x}+C_3e^{-x}$$ Here is my attempt: We differentiate the first equation: $$y_1''=-2y_1'+y_2'-2y_3'$$$$=-y_1+2y_2-4y_3$$ Now we have a $2x2$ system for $y_2,y_3$, using the first equation from the system and the last equation for $y_1''$: $$y_1'=-2y_1+y_2-2y_3$$ $$y_1''=-y_1+2y_2-4y_3$$ Multiplying the first equation by $-2$ and adding these two equations gives: $$y_1''-2y_1'-3y_1=0$$ This gives: $$\lambda_1=3,\lambda_2=-1\Rightarrow y_1(x)=C_1e^{3x}+C_2e^{-x}$$ This doesn't match with the above solution. Is there a mistake? I will show my full attempt with this (maybe wrong) first solution. Now, for finding $y_2,y_3$ we have a new $2x2$ system such that $y_1$ is known: $$y_2'=y_1-2y_2+2y_3$$ $$y_3'=3y_1-3y_2+5y_3$$ Again, the same procedure. We differentiate the first equation: $$y_2''=y_1'-2y_2'+2y_3'$$ $$=y_1'-2y_1+4y_2-4y_3$$ Now, for finding $y_3$ we have a new $2x2$ system such that $y_1,y_2$ are known: $$y_2'=y_1-2y_2+2y_3$$ $$y_2''=y_1'-2y_1+4y_2-4y_3$$ By adding these two, we get: $$y_3=-\frac{1}{2}y_2''-\frac{1}{2}y_2'+\frac{1}{2}y_1'-\frac{1}{2}y_1+y_2$$ Now we differentiate the equation $$y_2''=y_1'-2y_1+4y_2-4y_3$$ to obtain $$y_2'''=y_1''-2y_1'+4y_2'-4y_3'$$ By plugin previously evaluated $y_3$ we get: $$y_2'''+4y_2''+4y_2'=y_1''+2y_1'$$ We already have $y_1$ (note that this may be wrong), so: $$y_1'=3C_1e^{3x}-C_2e^{-x}$$ $$y_1''=9C_1e^{3x}+C_2e^{-x}$$ We have: $$y_2'''+4y_2''+4y_2'=15C_1e^{3x}-C_2e^{-x}$$ Now we will use variation of parameters: $$\lambda^3+4\lambda^2+4\lambda=0$$ $$\lambda_1=0,\lambda_2=\lambda_3=-2$$ This gives multiple roots so it might again be a mistake. I will stop here. Could someone check this? Where are mistakes so far? EDIT: Could someone show the approach with linear algebra, eigenvalues/eigenvectors?",,"['ordinary-differential-equations', 'systems-of-equations']"
8,Finding periodic orbits,Finding periodic orbits,,"Given the following equations \begin{cases}      \dot x = - x - y + \cos t,\\     \dot y = x - y + \sin t. \end{cases} I am looking for periodic orbits. I managed to find the general solution for some initial conditions $x_0,y_0$ \begin{equation} \begin{pmatrix}  x \\ y  \end{pmatrix} = e^{-t} \begin{pmatrix}  \cos t & -\sin t \\ \sin t & \cos t  \end{pmatrix} \begin{pmatrix}  x_0-1 \\ y_0 \end{pmatrix} + \begin{pmatrix}  \cos t \\ \sin t  \end{pmatrix} \end{equation} However, I am not sure what to do and how to proceed to finding the periodic orbits. One way of doing that seems to be choosing the initial conditions so that the solution is a periodic function, so $x_0 = 1, y_0=0$ but it seems odd... Can someone suggest any hints/steps? Thank you.","Given the following equations \begin{cases}      \dot x = - x - y + \cos t,\\     \dot y = x - y + \sin t. \end{cases} I am looking for periodic orbits. I managed to find the general solution for some initial conditions $x_0,y_0$ \begin{equation} \begin{pmatrix}  x \\ y  \end{pmatrix} = e^{-t} \begin{pmatrix}  \cos t & -\sin t \\ \sin t & \cos t  \end{pmatrix} \begin{pmatrix}  x_0-1 \\ y_0 \end{pmatrix} + \begin{pmatrix}  \cos t \\ \sin t  \end{pmatrix} \end{equation} However, I am not sure what to do and how to proceed to finding the periodic orbits. One way of doing that seems to be choosing the initial conditions so that the solution is a periodic function, so $x_0 = 1, y_0=0$ but it seems odd... Can someone suggest any hints/steps? Thank you.",,"['ordinary-differential-equations', 'dynamical-systems']"
9,Solving the differential equation $y'= 2 xy + 4 x$ using power series.,Solving the differential equation  using power series.,y'= 2 xy + 4 x,This differential equation can be solved using a Power Series Method: $$f'(x) = 2 x f(x) + 4 x$$ I found $f'(x)$ and substituted it back into the equation but i do not understand where to go from there. How would i be able to find the power series which satisfies this differential equation?,This differential equation can be solved using a Power Series Method: $$f'(x) = 2 x f(x) + 4 x$$ I found $f'(x)$ and substituted it back into the equation but i do not understand where to go from there. How would i be able to find the power series which satisfies this differential equation?,,"['sequences-and-series', 'ordinary-differential-equations', 'power-series']"
10,Plotting phase plane in Matlab for SIR model,Plotting phase plane in Matlab for SIR model,,"I want to plot the phase plane of the SIR model, I used the method describe in here This is the code that I wrote, function SIREquilibrium() S0=0.8; I0=0.2; R=1-S0-I0; beta=1; gamma=1/10; mu=5e-4;  f=@(t,y)[mu-beta*y(1)*y(2)-mu*y(1);beta*y(1)*y(2)-gamma*y(2)-mu*y(2);gamma*y(2)-mu*y(3)];  y1=linspace(0,1,20); y2=linspace(0,1,20); [x,y]=meshgrid(y1,y2); u=zeros(size(x)); v=zeros(size(y)); t=0; for i=1:numel(x)     Yprime=f(t,[x(i);y(i)]);     u(i)=Yprime(1);     v(i)=Yprime(2); end quiver(x,y,u,v,'r') But I get an error as index exceeds matrix dimensions.  Error in SIREquilibrium>@(t,y)[mu-beta*y(1)*y(2)-mu*y(1);beta*y(1)*y(2)-gamma*y(2)-mu*y(2);gamma*y(2)-mu*y(3)] (line 16) f=@(t,y)[mu-beta*y(1)*y(2)-mu*y(1);beta*y(1)*y(2)-gamma*y(2)-mu*y(2);gamma*y(2)-mu*y(3)];  Error in SIREquilibrium (line 25)     Yprime=f(t,[x(i);y(i)]); I infact do not understand what Yprime=f(t,[x(i);y(i)]); does in this Can someone please tell me how I can plot the phase plane of this SIR model After making the changes to the code as f=@(t,y)[mu-beta*y(1)*y(2)-mu*y(1);beta*y(1)*y(2)-gamma*y(2)-mu*y(2)]; the phase plane I get is But, I think it should be Can you suggest me what I am doing wrong to get a phase plane like that. I changed the linspace to see why the spiraling in behavior couldn't be seen. And this is the phase plot that I get. function SIREquilibrium() if nargin==0    S0=0.8;    I0=0.2;    beta=0.3;    gamma=1/10;    mu=5e-5; end   f=@(t,y)[mu-beta*y(1)*y(2)-mu*y(1);beta*y(1)*y(2)-gamma*y(2)-mu*y(2)];  y1=linspace(0,1,20); y2=linspace(0,1,20); [x,y]=meshgrid(y1,y2); u=zeros(size(x)); v=zeros(size(y)); t=0; for i=1:numel(x)      Yprime=f(t,[x(i);y(i)]);     Yprime=Yprime/norm(Yprime);     u(i)=Yprime(1);     v(i)=Yprime(2);  end quiver(x,y,u,v,'r') axis tight equal hold on  for y10=[0:0.2:1]     for y20=[0:0.2:1]     options=odeset('MaxStep',0.1);     [ts,ys]=ode45(f,[0,4000],[y10,y20],options);     plot(ys(:,1),ys(:,2))     end end hold off","I want to plot the phase plane of the SIR model, I used the method describe in here This is the code that I wrote, function SIREquilibrium() S0=0.8; I0=0.2; R=1-S0-I0; beta=1; gamma=1/10; mu=5e-4;  f=@(t,y)[mu-beta*y(1)*y(2)-mu*y(1);beta*y(1)*y(2)-gamma*y(2)-mu*y(2);gamma*y(2)-mu*y(3)];  y1=linspace(0,1,20); y2=linspace(0,1,20); [x,y]=meshgrid(y1,y2); u=zeros(size(x)); v=zeros(size(y)); t=0; for i=1:numel(x)     Yprime=f(t,[x(i);y(i)]);     u(i)=Yprime(1);     v(i)=Yprime(2); end quiver(x,y,u,v,'r') But I get an error as index exceeds matrix dimensions.  Error in SIREquilibrium>@(t,y)[mu-beta*y(1)*y(2)-mu*y(1);beta*y(1)*y(2)-gamma*y(2)-mu*y(2);gamma*y(2)-mu*y(3)] (line 16) f=@(t,y)[mu-beta*y(1)*y(2)-mu*y(1);beta*y(1)*y(2)-gamma*y(2)-mu*y(2);gamma*y(2)-mu*y(3)];  Error in SIREquilibrium (line 25)     Yprime=f(t,[x(i);y(i)]); I infact do not understand what Yprime=f(t,[x(i);y(i)]); does in this Can someone please tell me how I can plot the phase plane of this SIR model After making the changes to the code as f=@(t,y)[mu-beta*y(1)*y(2)-mu*y(1);beta*y(1)*y(2)-gamma*y(2)-mu*y(2)]; the phase plane I get is But, I think it should be Can you suggest me what I am doing wrong to get a phase plane like that. I changed the linspace to see why the spiraling in behavior couldn't be seen. And this is the phase plot that I get. function SIREquilibrium() if nargin==0    S0=0.8;    I0=0.2;    beta=0.3;    gamma=1/10;    mu=5e-5; end   f=@(t,y)[mu-beta*y(1)*y(2)-mu*y(1);beta*y(1)*y(2)-gamma*y(2)-mu*y(2)];  y1=linspace(0,1,20); y2=linspace(0,1,20); [x,y]=meshgrid(y1,y2); u=zeros(size(x)); v=zeros(size(y)); t=0; for i=1:numel(x)      Yprime=f(t,[x(i);y(i)]);     Yprime=Yprime/norm(Yprime);     u(i)=Yprime(1);     v(i)=Yprime(2);  end quiver(x,y,u,v,'r') axis tight equal hold on  for y10=[0:0.2:1]     for y20=[0:0.2:1]     options=odeset('MaxStep',0.1);     [ts,ys]=ode45(f,[0,4000],[y10,y20],options);     plot(ys(:,1),ys(:,2))     end end hold off",,"['ordinary-differential-equations', 'dynamical-systems', 'matlab']"
11,Solve the differential equation $\frac{dy}{dx} - xy=1$,Solve the differential equation,\frac{dy}{dx} - xy=1,Solve the differential equation $\frac{dy}{dx} - xy=1$ given $y(0)=1$ The given differential equation is a first order linear differential equation of the form $\frac{dy}{dx} + Py=Q$ The integrating factor is $$e^{\int Pdx}=e^{-\int xdx}=e^{-\frac{x^2}{2}}$$ Solution is $$ye^{-\frac{x^2}{2}}=\int e^{-\frac{x^2}{2}}dx + c$$ But how do I integrate the second term?,Solve the differential equation $\frac{dy}{dx} - xy=1$ given $y(0)=1$ The given differential equation is a first order linear differential equation of the form $\frac{dy}{dx} + Py=Q$ The integrating factor is $$e^{\int Pdx}=e^{-\int xdx}=e^{-\frac{x^2}{2}}$$ Solution is $$ye^{-\frac{x^2}{2}}=\int e^{-\frac{x^2}{2}}dx + c$$ But how do I integrate the second term?,,"['calculus', 'ordinary-differential-equations']"
12,Solving ordinary differential equation,Solving ordinary differential equation,,"I am trying to find analytical solution to the following ODE: $\dot{x}=c_1x+\frac{c_2}{x}+c_3$, where $c_1,c_2,c_3$ are known constants. If $c_2=0$, then it's a standard linear ODE which I know how to solve. But, I have no idea how to proceed when all the above constants are non-zero. I really appreciate any hints as to how proceed. Update: I am looking to get a solution like x=f(t) for the above problem. Thanks in advance. $\textbf{P.S.}$: If this information helps, I basically started from the following ODE (below) and applied change of variables to obtain the above form : $\dot{y}=c_1y+c_3 \sqrt{y}+c_2$ and used $x=\sqrt{y}$ variable transformation to get the above ODE. I want to obtain the analytical solution to this ODE, in fact.","I am trying to find analytical solution to the following ODE: $\dot{x}=c_1x+\frac{c_2}{x}+c_3$, where $c_1,c_2,c_3$ are known constants. If $c_2=0$, then it's a standard linear ODE which I know how to solve. But, I have no idea how to proceed when all the above constants are non-zero. I really appreciate any hints as to how proceed. Update: I am looking to get a solution like x=f(t) for the above problem. Thanks in advance. $\textbf{P.S.}$: If this information helps, I basically started from the following ODE (below) and applied change of variables to obtain the above form : $\dot{y}=c_1y+c_3 \sqrt{y}+c_2$ and used $x=\sqrt{y}$ variable transformation to get the above ODE. I want to obtain the analytical solution to this ODE, in fact.",,['ordinary-differential-equations']
13,Solve differential equation: $ \frac{dy}{dx}= e^{y+x} $ with the initial condition that y(0) = -ln(4),Solve differential equation:  with the initial condition that y(0) = -ln(4), \frac{dy}{dx}= e^{y+x} ,"How to solve: $ \frac{dy}{dx}= e^{y+x} $ with the initial condition that y(0) = -ln(4)? First, I separated to $ \frac{dy}{(e^y)}= dx(e^x)$. Then I integrated both sides, which gave me: $ - e^{-y} = e^x + K$. Here I am stuck?","How to solve: $ \frac{dy}{dx}= e^{y+x} $ with the initial condition that y(0) = -ln(4)? First, I separated to $ \frac{dy}{(e^y)}= dx(e^x)$. Then I integrated both sides, which gave me: $ - e^{-y} = e^x + K$. Here I am stuck?",,['ordinary-differential-equations']
14,How to find a differential equation given its solution?,How to find a differential equation given its solution?,,"I was given the solution of a differential equation and I need to find its differential equation, specifically a second order differential equation with constant coefficients. $e^x + e^{-x} + e^{2x}$ Someone please help me with this. I am very confused since the solution has three terms. Doesn't the solution should contain only two terms since it is the solution of a second order differential equation?","I was given the solution of a differential equation and I need to find its differential equation, specifically a second order differential equation with constant coefficients. $e^x + e^{-x} + e^{2x}$ Someone please help me with this. I am very confused since the solution has three terms. Doesn't the solution should contain only two terms since it is the solution of a second order differential equation?",,"['ordinary-differential-equations', 'derivatives']"
15,How do we find the inverse Laplace transform of $\frac{1}{(s^2+a^2)^2}$?,How do we find the inverse Laplace transform of ?,\frac{1}{(s^2+a^2)^2},How do we find the inverse Laplace transform of $\frac{1}{(s^2+a^2)^2}$? Do I need to use the convolution theory? It doesn't match any of the known laplace inverse transforms. It matches with the Laplace transform of $\sin(at)$ but I don't know if that helps or not. Also there seems to be a formula with limits and imaginary numbers. How do I just know to apply that here?,How do we find the inverse Laplace transform of $\frac{1}{(s^2+a^2)^2}$? Do I need to use the convolution theory? It doesn't match any of the known laplace inverse transforms. It matches with the Laplace transform of $\sin(at)$ but I don't know if that helps or not. Also there seems to be a formula with limits and imaginary numbers. How do I just know to apply that here?,,"['ordinary-differential-equations', 'laplace-transform', 'convolution']"
16,Solving an ODE by integration,Solving an ODE by integration,,"Suppose $f$ is differentiable on $[0,1]$, and that $f'(x) = g(x)$ on $(0,1]$. Can I conclude that for $x \in [0,1]$, $$f(x) - f(0) = \int_0^x g(x) dx?$$ My issue is that we don't know that $f'(0) = g(0)$, but that shouldn't matter since the integral of two functions that differ on a set of measure zero are the same, right? Also, what if all we knew was that $f$ was differentiable on $(0,1]$?","Suppose $f$ is differentiable on $[0,1]$, and that $f'(x) = g(x)$ on $(0,1]$. Can I conclude that for $x \in [0,1]$, $$f(x) - f(0) = \int_0^x g(x) dx?$$ My issue is that we don't know that $f'(0) = g(0)$, but that shouldn't matter since the integral of two functions that differ on a set of measure zero are the same, right? Also, what if all we knew was that $f$ was differentiable on $(0,1]$?",,['ordinary-differential-equations']
17,How to solve $\frac{\text{d}y}{\text{d}\tau} = -\varepsilon y/\left(1+\frac{k\Delta}{(ky+1)^2}\right)$,How to solve,\frac{\text{d}y}{\text{d}\tau} = -\varepsilon y/\left(1+\frac{k\Delta}{(ky+1)^2}\right),"If we take $$\frac{\text{d}y}{\text{d}\tau} = -\varepsilon y/\left(1+\frac{k\Delta}{(ky+1)^2}\right) \qquad\qquad (1)$$ with $y(0)=y_0$ then we can solve it as such: \begin{align} -\varepsilon\tau + C_0 &= \int \frac {1+\frac{k\Delta}{\left(ky+1\right)^2}} {y} \ \text{d}y \\ %% &= \int \frac {1} {y} % + % \frac {k\Delta} {y\left(ky+1\right)^2} \ \text{d}y\\ %% &= \log{y} % + % \frac{\Delta}{k}\log{\left(\frac{y}{k+y}\right)} % + % \frac{\Delta}{k+y} \end{align} with \begin{align} C_0 &= \log{y_0} % + % \frac{\Delta}{k}\log{\left(\frac{y_0}{k+y_0}\right)} % + % \frac{\Delta}{k+y_0} \end{align} Therefore we can write: \begin{align} \tau= - \frac{1}{\varepsilon} \left( \log{y} % + % \frac{\Delta}{k}\log{\left(\frac{y}{k+y}\right)} % + % \frac{\Delta}{k+y} -C_0\right) \qquad\qquad (2) \end{align} I have solved $(1)$ numerically, using an ODE solver in MATLAB, with the following parameters: $$\Delta=0.4351$$  $$\varepsilon=4.4261\times 10^{-5}$$ $$k=22.9806$$  $$y_0=0.5945$$ $$\tau\in [0,200000]$$ This is plotted by the black line in the following figure. I then solved $(2)$ for $\tau$ for $y\in[y_\text{min},y_0]$ where $y_\text{min}$ was the smallest value for y output from the numerical simulation. I then plotted this on the same figure, in dashed magenta. They don't match! I have checked this over and over, I do not know what I have done wrong here. Any help would be really appreciated.","If we take $$\frac{\text{d}y}{\text{d}\tau} = -\varepsilon y/\left(1+\frac{k\Delta}{(ky+1)^2}\right) \qquad\qquad (1)$$ with $y(0)=y_0$ then we can solve it as such: \begin{align} -\varepsilon\tau + C_0 &= \int \frac {1+\frac{k\Delta}{\left(ky+1\right)^2}} {y} \ \text{d}y \\ %% &= \int \frac {1} {y} % + % \frac {k\Delta} {y\left(ky+1\right)^2} \ \text{d}y\\ %% &= \log{y} % + % \frac{\Delta}{k}\log{\left(\frac{y}{k+y}\right)} % + % \frac{\Delta}{k+y} \end{align} with \begin{align} C_0 &= \log{y_0} % + % \frac{\Delta}{k}\log{\left(\frac{y_0}{k+y_0}\right)} % + % \frac{\Delta}{k+y_0} \end{align} Therefore we can write: \begin{align} \tau= - \frac{1}{\varepsilon} \left( \log{y} % + % \frac{\Delta}{k}\log{\left(\frac{y}{k+y}\right)} % + % \frac{\Delta}{k+y} -C_0\right) \qquad\qquad (2) \end{align} I have solved $(1)$ numerically, using an ODE solver in MATLAB, with the following parameters: $$\Delta=0.4351$$  $$\varepsilon=4.4261\times 10^{-5}$$ $$k=22.9806$$  $$y_0=0.5945$$ $$\tau\in [0,200000]$$ This is plotted by the black line in the following figure. I then solved $(2)$ for $\tau$ for $y\in[y_\text{min},y_0]$ where $y_\text{min}$ was the smallest value for y output from the numerical simulation. I then plotted this on the same figure, in dashed magenta. They don't match! I have checked this over and over, I do not know what I have done wrong here. Any help would be really appreciated.",,['ordinary-differential-equations']
18,Can you convert a system of coupled first order DEs into a higher order DE?,Can you convert a system of coupled first order DEs into a higher order DE?,,"I know the reverse is possible (simply define every higher order derivative as a new variable) but is it always possible to take $n$ equations of the form $\dot{x_1} = f_1(x_1, x_2, ... , x_n, t)$ $\dot{x_2} = f_2(x_1, x_2, ... , x_n, t)$ . . $\dot{x_n} = f_n(x_1, x_2, ... , x_n, t)$ Can we write a single $n^{th}$ order DE from this? I seem to remember that this is generally not possible but if so, can someone show a counter-example and a bit of intuiton for why not?","I know the reverse is possible (simply define every higher order derivative as a new variable) but is it always possible to take $n$ equations of the form $\dot{x_1} = f_1(x_1, x_2, ... , x_n, t)$ $\dot{x_2} = f_2(x_1, x_2, ... , x_n, t)$ . . $\dot{x_n} = f_n(x_1, x_2, ... , x_n, t)$ Can we write a single $n^{th}$ order DE from this? I seem to remember that this is generally not possible but if so, can someone show a counter-example and a bit of intuiton for why not?",,['ordinary-differential-equations']
19,How many initial conditions are required?,How many initial conditions are required?,,"Consider the following system of ODE: $$\begin{array}{ll}\ddot y + y + \ddot x + x = 0 \\ y+\dot x - x = 0 \end{array}$$ Question : How many initial conditions are required to determine a unique solution? A naive reasoning leads to four: $y(0),\dot y(0), x(0)$ and $\dot x(0)$. However, if we write the system in a first-order form: $$\begin{bmatrix} 1  & 0 & 0 & 0 \\ 1 & 0 & 1 & 0 \\ 0 & 0 & 1 & 0 \\ 1 & 0 & 0 & 0 \end{bmatrix} \begin{bmatrix} \dot x \\ \ddot x \\ \dot y \\ \ddot y\end{bmatrix} =  \begin{bmatrix} 0 & 1 & 0 & 0 \\ -1 & 0 & -1 & 0 \\ 0 & 0  & 0 & 1\\ 1 & 0 & -1 & 0 \end{bmatrix}\begin{bmatrix} x  \\ \dot x \\ y \\ \dot y\end{bmatrix}$$ the left matrix is not of full rank, which means the equations are not all independent.  Indeed, by differentiating the second equation: $\dot y + \ddot x -\dot x=0$ which leads to $\ddot y + y + \dot x - \dot y + x =0$, or, in the first-order form: $$ \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 1 & 0 & 1 \end{bmatrix} \begin{bmatrix} \dot x \\ \dot y \\ \ddot y \end{bmatrix} = \begin{bmatrix} 1 & -1 & 0 \\ 0 & 0 & 1 \\ -1 & -1 & 1 \end{bmatrix} \begin{bmatrix} x \\ y \\ \dot y \end{bmatrix}$$ With the above form, it shows only three initial conditions are required: $x(0),y(0),\dot y(0)$. And, what if I had: $$\begin{array}{ll}\ddot y + y + x^{(n)} + x = 0 \\ \dot x - x = 0 \end{array}$$ for some $n$. Then, I could solve the second equation for some $x(0)$, then differentiate $x$ $n$ times and inject in the first equation, so only $x(0), y(0), \dot y(0)$ are needed. Can this be seen directly, in a robust manner?","Consider the following system of ODE: $$\begin{array}{ll}\ddot y + y + \ddot x + x = 0 \\ y+\dot x - x = 0 \end{array}$$ Question : How many initial conditions are required to determine a unique solution? A naive reasoning leads to four: $y(0),\dot y(0), x(0)$ and $\dot x(0)$. However, if we write the system in a first-order form: $$\begin{bmatrix} 1  & 0 & 0 & 0 \\ 1 & 0 & 1 & 0 \\ 0 & 0 & 1 & 0 \\ 1 & 0 & 0 & 0 \end{bmatrix} \begin{bmatrix} \dot x \\ \ddot x \\ \dot y \\ \ddot y\end{bmatrix} =  \begin{bmatrix} 0 & 1 & 0 & 0 \\ -1 & 0 & -1 & 0 \\ 0 & 0  & 0 & 1\\ 1 & 0 & -1 & 0 \end{bmatrix}\begin{bmatrix} x  \\ \dot x \\ y \\ \dot y\end{bmatrix}$$ the left matrix is not of full rank, which means the equations are not all independent.  Indeed, by differentiating the second equation: $\dot y + \ddot x -\dot x=0$ which leads to $\ddot y + y + \dot x - \dot y + x =0$, or, in the first-order form: $$ \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 1 & 0 & 1 \end{bmatrix} \begin{bmatrix} \dot x \\ \dot y \\ \ddot y \end{bmatrix} = \begin{bmatrix} 1 & -1 & 0 \\ 0 & 0 & 1 \\ -1 & -1 & 1 \end{bmatrix} \begin{bmatrix} x \\ y \\ \dot y \end{bmatrix}$$ With the above form, it shows only three initial conditions are required: $x(0),y(0),\dot y(0)$. And, what if I had: $$\begin{array}{ll}\ddot y + y + x^{(n)} + x = 0 \\ \dot x - x = 0 \end{array}$$ for some $n$. Then, I could solve the second equation for some $x(0)$, then differentiate $x$ $n$ times and inject in the first equation, so only $x(0), y(0), \dot y(0)$ are needed. Can this be seen directly, in a robust manner?",,"['ordinary-differential-equations', 'initial-value-problems']"
20,Can I absorb the sign into the constant of a solution to the homogeneous equation in a second order ode?,Can I absorb the sign into the constant of a solution to the homogeneous equation in a second order ode?,,"I want to solve $$y''+\frac{x}{1-x}y'-\frac{1}{1-x}y=(x-1)e^{x}$$ using variation of parameters. Now, I could guess a solution to the inhomogeneous, i.e. $y_1 = x$. Then I used the formula of reduction of order $$y_2 = x\int\frac{e^{-\int \frac{x}{1-x}dx}}{x^2}dx =x\int \frac{e^{x+\ln(1-x)}}{x^2} = x\int\frac{(1-x)e^x}{x^2} = x\left[\int \frac{e^{x}}{x^2}-\int\frac{e^{x}}{x}\right] = x\left[-x^{-1}e^x+\int\frac{e^{x}}{x}-\int\frac{e^{x}}{x}  \right] = -e^{x}$$ to find $y_2  =-e^x$. However Mathematica says e solution is $y_2 = e^x$ so my question is: Can I absorb the sign into the constant and take $y_2  =e^x$ ?","I want to solve $$y''+\frac{x}{1-x}y'-\frac{1}{1-x}y=(x-1)e^{x}$$ using variation of parameters. Now, I could guess a solution to the inhomogeneous, i.e. $y_1 = x$. Then I used the formula of reduction of order $$y_2 = x\int\frac{e^{-\int \frac{x}{1-x}dx}}{x^2}dx =x\int \frac{e^{x+\ln(1-x)}}{x^2} = x\int\frac{(1-x)e^x}{x^2} = x\left[\int \frac{e^{x}}{x^2}-\int\frac{e^{x}}{x}\right] = x\left[-x^{-1}e^x+\int\frac{e^{x}}{x}-\int\frac{e^{x}}{x}  \right] = -e^{x}$$ to find $y_2  =-e^x$. However Mathematica says e solution is $y_2 = e^x$ so my question is: Can I absorb the sign into the constant and take $y_2  =e^x$ ?",,"['integration', 'ordinary-differential-equations', 'reduction-of-order-ode']"
21,Second-order linear differential equation of the form $x^2 y'' + (ax-b)y' - ay =0$,Second-order linear differential equation of the form,x^2 y'' + (ax-b)y' - ay =0,"I need to solve the following differential equation \begin{equation} x^2 y'' + (ax-b)y' - ay =0 \end{equation} with $a,b>0$, $x\geq 0$ and $y(0)=0$. The power series method will fail since there is a singularity at $x=0$, while the form of the equation does not conform with the Frobenius method. What other methods can I try in order to solve this?","I need to solve the following differential equation \begin{equation} x^2 y'' + (ax-b)y' - ay =0 \end{equation} with $a,b>0$, $x\geq 0$ and $y(0)=0$. The power series method will fail since there is a singularity at $x=0$, while the form of the equation does not conform with the Frobenius method. What other methods can I try in order to solve this?",,"['ordinary-differential-equations', 'power-series', 'frobenius-method']"
22,Picard–Lindelöf_theorem with parameter : what regularity?,Picard–Lindelöf_theorem with parameter : what regularity?,,"I'm looking on the ordinary differential equation $$ \frac{\partial y}{\partial t}(t,\lambda)=f(t,y(t,\lambda),\lambda) $$ $$ y(t_0,\lambda)=y_0 $$ where $f$ is Lipschitz in $y$ and $C^p$ with respect to every variable. $\lambda$ is a parameter in some compact set in $R^n$. For fixed $t$, let $y_t(\lambda)=y(t,\lambda)$. I want to know if $y_t$ is differentiable (with respect to $\lambda$). Starting from $$ y(t,\lambda)=y(t_0,\lambda)+\int_{t_0}^{t}\frac{\partial y}{\partial t}(s,\lambda)\,ds =y_0+\int_{t_0}^{t} f(s,y(s,\lambda),\lambda)ds $$ then differentiating with respect to $\lambda$ and taking the derivative with respect to $t$ I get that $(dy_t)_{\lambda}$ satisfies an equation of the form $$ \frac{\partial A}{\partial t}(t,\lambda)=F_{\lambda}(t,A) $$ $$ A(t_0)=id $$ where $F$ is some function made from the derivatives of $f$. This proves that the differential if exists , satisfies that equation. From the usual Picard–Lindelöf theorem, I know that this equation has unique continuous solution. If I can prove that this solution is the differential of $y_t$, I'm done. QUESTION : how to check that A(t) is the differential of $y_t$ ? QUESTION : do you know an online document providing a precise statement and a proof of the fact that $(t,\lambda)\mapsto y(t,\lambda)$ is $C^p$ ? This question is related to this one , but I'm asking more : first I want to know why the derivative with respecto to the parameter exists, and second, I want to have a $C^p$-regularity with respect to the parameters. EDIT John's answer is correct. Here are two other documents in which it is explaned with more details: https://www.math.uni-bielefeld.de/%7Egrigor/odelec2009.pdf http://www.math.pitt.edu/%7Ebard/bardware/classes/2920/Grant_4july2007.pdf","I'm looking on the ordinary differential equation $$ \frac{\partial y}{\partial t}(t,\lambda)=f(t,y(t,\lambda),\lambda) $$ $$ y(t_0,\lambda)=y_0 $$ where $f$ is Lipschitz in $y$ and $C^p$ with respect to every variable. $\lambda$ is a parameter in some compact set in $R^n$. For fixed $t$, let $y_t(\lambda)=y(t,\lambda)$. I want to know if $y_t$ is differentiable (with respect to $\lambda$). Starting from $$ y(t,\lambda)=y(t_0,\lambda)+\int_{t_0}^{t}\frac{\partial y}{\partial t}(s,\lambda)\,ds =y_0+\int_{t_0}^{t} f(s,y(s,\lambda),\lambda)ds $$ then differentiating with respect to $\lambda$ and taking the derivative with respect to $t$ I get that $(dy_t)_{\lambda}$ satisfies an equation of the form $$ \frac{\partial A}{\partial t}(t,\lambda)=F_{\lambda}(t,A) $$ $$ A(t_0)=id $$ where $F$ is some function made from the derivatives of $f$. This proves that the differential if exists , satisfies that equation. From the usual Picard–Lindelöf theorem, I know that this equation has unique continuous solution. If I can prove that this solution is the differential of $y_t$, I'm done. QUESTION : how to check that A(t) is the differential of $y_t$ ? QUESTION : do you know an online document providing a precise statement and a proof of the fact that $(t,\lambda)\mapsto y(t,\lambda)$ is $C^p$ ? This question is related to this one , but I'm asking more : first I want to know why the derivative with respecto to the parameter exists, and second, I want to have a $C^p$-regularity with respect to the parameters. EDIT John's answer is correct. Here are two other documents in which it is explaned with more details: https://www.math.uni-bielefeld.de/%7Egrigor/odelec2009.pdf http://www.math.pitt.edu/%7Ebard/bardware/classes/2920/Grant_4july2007.pdf",,"['ordinary-differential-equations', 'differential']"
23,Separable but not exact equation,Separable but not exact equation,,"In class, my professor stated that all separable equations are exact, and we even proved it for homework, but I think I found an equation that is separable but not exact: $$(x\ln y+xy)+(y\ln x+xy)y′ =0$$ My Work: \begin{align*}   M &= x\ln y + xy \\   M_y &= \frac{x}{y} +x \\   N &= y\ln x + xy \\   N_x &= \frac{y}{x} +y \\   M_y &\neq N_x \end{align*} But \begin{align*}   x(\ln y + y) + y \times y'\times (\ln x +x) &= 0 \\   x(\ln y + y) &= - y \times y'\times (\ln x +x) \\   \frac{x}{\ln x +x} &= \frac{- y \times y'}{\ln y + y} \end{align*} Separated So whats up? Am I doing something wrong?","In class, my professor stated that all separable equations are exact, and we even proved it for homework, but I think I found an equation that is separable but not exact: $$(x\ln y+xy)+(y\ln x+xy)y′ =0$$ My Work: \begin{align*}   M &= x\ln y + xy \\   M_y &= \frac{x}{y} +x \\   N &= y\ln x + xy \\   N_x &= \frac{y}{x} +y \\   M_y &\neq N_x \end{align*} But \begin{align*}   x(\ln y + y) + y \times y'\times (\ln x +x) &= 0 \\   x(\ln y + y) &= - y \times y'\times (\ln x +x) \\   \frac{x}{\ln x +x} &= \frac{- y \times y'}{\ln y + y} \end{align*} Separated So whats up? Am I doing something wrong?",,['ordinary-differential-equations']
24,Solve $y=xy'+\frac{1}{2}y'^2$ (need approach),Solve  (need approach),y=xy'+\frac{1}{2}y'^2,"$y=xy'+\frac{1}{2}y'^2$ Can someone explain me how to solve these type of equations it's my first time ""seeing"" an equation that has $y'$ to some degree, no need for a complete solution just the general method. EDIT: I read some articles and came up with this: $y=xp+\frac{1}{2}p^2$ now if we differentiate in respect to $x$ we get: $p=xp'+p+pp'$ $pp'+xp'=0$ $p'(p+x)=0$ $p'=0$ then $p=C$ and if $p+x=0$ then $p=-x$ so we have $y_1 = Cx+K$ and $y_2=\frac{-x^2}{2}+K$, where C and K are const.","$y=xy'+\frac{1}{2}y'^2$ Can someone explain me how to solve these type of equations it's my first time ""seeing"" an equation that has $y'$ to some degree, no need for a complete solution just the general method. EDIT: I read some articles and came up with this: $y=xp+\frac{1}{2}p^2$ now if we differentiate in respect to $x$ we get: $p=xp'+p+pp'$ $pp'+xp'=0$ $p'(p+x)=0$ $p'=0$ then $p=C$ and if $p+x=0$ then $p=-x$ so we have $y_1 = Cx+K$ and $y_2=\frac{-x^2}{2}+K$, where C and K are const.",,[]
25,Check if a distribution is involutive,Check if a distribution is involutive,,"I am studying distributions and when they are involutive and the Froebenius's Theorem. I cam across an example but I don't understand how to solve it. I have a distribution $\Delta$ defined as: $\Delta=span\{v,w\}$ where $v= \left( \begin{matrix} -y \\ x\\0 \end{matrix} \right)$ $w= \left( \begin{matrix} 2zx \\ 2yz\\ z^2+1-x^2-y^2 \end{matrix} \right)$ In the slides I found on the internet it says: $\left[ v,w\right]\equiv0$ But I tried to compute it (with $s=[x\;y\;z]^T$): $\left[ v,w\right] \frac{\partial w}{\partial s}v-\frac{\partial v}{\partial s}w = -[2z\;2z\;2z][-y\;x\;0]^T=2zy-2zx \neq0$ What I do wrong? Thanks for the help","I am studying distributions and when they are involutive and the Froebenius's Theorem. I cam across an example but I don't understand how to solve it. I have a distribution $\Delta$ defined as: $\Delta=span\{v,w\}$ where $v= \left( \begin{matrix} -y \\ x\\0 \end{matrix} \right)$ $w= \left( \begin{matrix} 2zx \\ 2yz\\ z^2+1-x^2-y^2 \end{matrix} \right)$ In the slides I found on the internet it says: $\left[ v,w\right]\equiv0$ But I tried to compute it (with $s=[x\;y\;z]^T$): $\left[ v,w\right] \frac{\partial w}{\partial s}v-\frac{\partial v}{\partial s}w = -[2z\;2z\;2z][-y\;x\;0]^T=2zy-2zx \neq0$ What I do wrong? Thanks for the help",,"['ordinary-differential-equations', 'differential-geometry', 'distribution-theory', 'topological-vector-spaces', 'lie-derivative']"
26,How to solve this differential equation of second order (non-linear)?,How to solve this differential equation of second order (non-linear)?,,How to solve the system using the Runge-Kutta Felberg method (differential equation of second order (non-linear))?,How to solve the system using the Runge-Kutta Felberg method (differential equation of second order (non-linear))?,,"['ordinary-differential-equations', 'nonlinear-system', 'differential']"
27,How was this equation derived?,How was this equation derived?,,Does anyone know how the equation in red was derived?,Does anyone know how the equation in red was derived?,,"['ordinary-differential-equations', 'trigonometry']"
28,$(2x^2+3y^2-7)xdx-(3x^2+2y^2-8)ydy=0$,,(2x^2+3y^2-7)xdx-(3x^2+2y^2-8)ydy=0,The question is basically to find out the solution of the differential equation $$(2x^2+3y^2-7)xdx-(3x^2+2y^2-8)ydy=0$$. Since the given differential equation is non homogeneous I tried to find out the point of intersection of $2x^2+3y^2-7=0$ and $3x^2+2y^2-8=0$.I got the point of intersection and then I tried to shift the coordinate system to that point.this removed the constant terms.However I was not able to proceed further.Please help me in this regard.thanks.,The question is basically to find out the solution of the differential equation $$(2x^2+3y^2-7)xdx-(3x^2+2y^2-8)ydy=0$$. Since the given differential equation is non homogeneous I tried to find out the point of intersection of $2x^2+3y^2-7=0$ and $3x^2+2y^2-8=0$.I got the point of intersection and then I tried to shift the coordinate system to that point.this removed the constant terms.However I was not able to proceed further.Please help me in this regard.thanks.,,['ordinary-differential-equations']
29,What is difference between an ordinary equation and differential equation,What is difference between an ordinary equation and differential equation,,What is the difference between a normal equation such as $f(t) = t^2$ and a differential equation such as: $d/dt f(t) = t*f(t)$. I mean what is physical intuition of the difference between the two? thanks,What is the difference between a normal equation such as $f(t) = t^2$ and a differential equation such as: $d/dt f(t) = t*f(t)$. I mean what is physical intuition of the difference between the two? thanks,,"['ordinary-differential-equations', 'functions', 'intuition']"
30,Differentiate to get error on weighted mean (summation fraction),Differentiate to get error on weighted mean (summation fraction),,"I need help differentiating a fraction containing two summations. Given the weighted mean $ \bar{x} = \frac{\sum\limits_{i=1}^{\rm N} x_{i}/\sigma_{i}^{2}}{\sum\limits_{i=1}^{\rm N} 1/\sigma_{i}^{2}}$ where $x_{i}$ and $\sigma_{i}$ are uncorrelated. The error propagation formula $\sigma_{f}^2 = \sigma_{f}^2 \left(\frac{\delta f }{\delta x}\right)^2 +  \sigma_{f}^2 \left(\frac{\delta f }{\delta y}\right)^2 + ...$ yields the error on $\bar{x}$ as: $\sigma_{\bar{x}}^2 = \sum\limits_{i=1}^{\rm N} \left(\frac{\delta \bar{x} }{\delta x_{i}}\right)^2  \sigma_{i}^2$ The literature* solves the derivative as: $\frac{\delta \bar{x} }{\delta x_{i}} = \frac{\delta }{\delta x_{i}} \frac{\sum\limits_{i=1}^{\rm N} x_{i}/\sigma_{i}^2}{\sum\limits_{i=1}^{\rm N} 1/\sigma_{i}^2} 	= \frac{1/\sigma_{i}^2}{\sum\limits_{i=1}^{\rm N} 1/\sigma_{i}^2}$ I cannot figure out how this result is achieved. By my working, if $\sigma_{i}$ can be treated as a constant, then $\frac{\delta \bar{x} }{\delta x_{i}}=1$. Am I missing something? I have tried the Quotient rule but I don't see how it applies in this case. Eqn. 4.19, Bevington, Data Reduction and Error Analysis for the Physical Sciences http://astro.cornell.edu/academics/courses/astro3310/Books/Bevington_opt.pdf","I need help differentiating a fraction containing two summations. Given the weighted mean $ \bar{x} = \frac{\sum\limits_{i=1}^{\rm N} x_{i}/\sigma_{i}^{2}}{\sum\limits_{i=1}^{\rm N} 1/\sigma_{i}^{2}}$ where $x_{i}$ and $\sigma_{i}$ are uncorrelated. The error propagation formula $\sigma_{f}^2 = \sigma_{f}^2 \left(\frac{\delta f }{\delta x}\right)^2 +  \sigma_{f}^2 \left(\frac{\delta f }{\delta y}\right)^2 + ...$ yields the error on $\bar{x}$ as: $\sigma_{\bar{x}}^2 = \sum\limits_{i=1}^{\rm N} \left(\frac{\delta \bar{x} }{\delta x_{i}}\right)^2  \sigma_{i}^2$ The literature* solves the derivative as: $\frac{\delta \bar{x} }{\delta x_{i}} = \frac{\delta }{\delta x_{i}} \frac{\sum\limits_{i=1}^{\rm N} x_{i}/\sigma_{i}^2}{\sum\limits_{i=1}^{\rm N} 1/\sigma_{i}^2} 	= \frac{1/\sigma_{i}^2}{\sum\limits_{i=1}^{\rm N} 1/\sigma_{i}^2}$ I cannot figure out how this result is achieved. By my working, if $\sigma_{i}$ can be treated as a constant, then $\frac{\delta \bar{x} }{\delta x_{i}}=1$. Am I missing something? I have tried the Quotient rule but I don't see how it applies in this case. Eqn. 4.19, Bevington, Data Reduction and Error Analysis for the Physical Sciences http://astro.cornell.edu/academics/courses/astro3310/Books/Bevington_opt.pdf",,"['ordinary-differential-equations', 'summation', 'average', 'means']"
31,Existence of unique global solution,Existence of unique global solution,,"Let the function $f:[0,T] \times \mathbb R^d \to \mathbb R^d$ satisfy the Caratheodory conditions. Furthermore, let there be $l \in L^1(0,T)$ such that for all $t\in [0,T]$ and $v,w \in \mathbb R^d$ the inequalities $$\Vert f(t,0)\Vert \leq l(t)$$ and $$\vert f(t,v)-f(t,w) \Vert \leq l(t) \Vert v-w \Vert$$ hold. How can I show that for all $(t_0,u_0) \in [0,T] \times \mathbb R^d$ there exists a unique global solution $u$ on $[0,T]$ of the initial value problem $$\begin{cases}u'(t)=f(t,u(t))\\u(t_0)=u_0\end{cases}$$?","Let the function $f:[0,T] \times \mathbb R^d \to \mathbb R^d$ satisfy the Caratheodory conditions. Furthermore, let there be $l \in L^1(0,T)$ such that for all $t\in [0,T]$ and $v,w \in \mathbb R^d$ the inequalities $$\Vert f(t,0)\Vert \leq l(t)$$ and $$\vert f(t,v)-f(t,w) \Vert \leq l(t) \Vert v-w \Vert$$ hold. How can I show that for all $(t_0,u_0) \in [0,T] \times \mathbb R^d$ there exists a unique global solution $u$ on $[0,T]$ of the initial value problem $$\begin{cases}u'(t)=f(t,u(t))\\u(t_0)=u_0\end{cases}$$?",,"['real-analysis', 'ordinary-differential-equations', 'initial-value-problems']"
32,First Order Differential Equantion (Ordinary) question,First Order Differential Equantion (Ordinary) question,,I'm in second year of college (Programming and Games Development) and I have a calculus exam tomorrow. I have a maths problem I can't solve. (c) Solve the following differential equation using an Integrating Factor: $$x\frac{dy}{dx}-y=x^2$$ I can't solve it and I've looked at MANY videos online but none of them were any help.,I'm in second year of college (Programming and Games Development) and I have a calculus exam tomorrow. I have a maths problem I can't solve. (c) Solve the following differential equation using an Integrating Factor: I can't solve it and I've looked at MANY videos online but none of them were any help.,x\frac{dy}{dx}-y=x^2,['ordinary-differential-equations']
33,Greens function with non-zero boundary condition,Greens function with non-zero boundary condition,,"When solving a differential equation using a Greens function, is it possible to solve a problem with non-zero boundarys directly using a Greens function? For example, when solving a problem with non-zero boundarys I've broken the problem into multiple pieces, using the Greens function to solve the inhomogeneous part and then solving the homogeneous part using some other method, taking advantage of linearity. But is it possible to solve without breaking the problem up?","When solving a differential equation using a Greens function, is it possible to solve a problem with non-zero boundarys directly using a Greens function? For example, when solving a problem with non-zero boundarys I've broken the problem into multiple pieces, using the Greens function to solve the inhomogeneous part and then solving the homogeneous part using some other method, taking advantage of linearity. But is it possible to solve without breaking the problem up?",,"['ordinary-differential-equations', 'greens-function']"
34,Material Derivative's advective non linear term,Material Derivative's advective non linear term,,"I have read this question and the accompanying answers - Linear Vs Non Linear Differential Equation as well as the useful comments and my question is related. I am in fluid dynamics and I deal with the material derivative on a daily basis and I recently read that the advective term Here is the relevant text from the above link ""Note that the advection of momentum $(V · \nabla)V$ is non-linear and this is the term that leads to much of the interesting behaviour in fluid mechanics."" i.e $$ \mathbf u.\nabla A $$ where u is the flow velocity and A is any vector field is non linear and as a result the whole material derivative is non linear due to the presence of a single non linear term. What exactly is the reason for the non linearity in the advective term of the material derivative ? Is it because of the product of a tensor ( $ \nabla A$) and a vector ?","I have read this question and the accompanying answers - Linear Vs Non Linear Differential Equation as well as the useful comments and my question is related. I am in fluid dynamics and I deal with the material derivative on a daily basis and I recently read that the advective term Here is the relevant text from the above link ""Note that the advection of momentum $(V · \nabla)V$ is non-linear and this is the term that leads to much of the interesting behaviour in fluid mechanics."" i.e $$ \mathbf u.\nabla A $$ where u is the flow velocity and A is any vector field is non linear and as a result the whole material derivative is non linear due to the presence of a single non linear term. What exactly is the reason for the non linearity in the advective term of the material derivative ? Is it because of the product of a tensor ( $ \nabla A$) and a vector ?",,"['ordinary-differential-equations', 'derivatives']"
35,A question regarding the uniqueness of exponential distribution,A question regarding the uniqueness of exponential distribution,,"""It turns out that not only is the exponential distribution memoryless, but it is also the unique distribution possessing this property."" - Sheldon Ross, author of ""A First Course in Probability (eighth edition)"". The proof presented in the book isn't particularly rigorous (nor is it ment to be) so with some help from the the response of Michael Hardy in Uniqueness of memoryless property I believe I managed to arrive to the correct conclusion but I am uncertain and thus I seek some clarification regarding my result. Ultimately the problem boils down to solving the equation $f(s+t)=f(s)f(t)$ where $f$ is non-negative (since it's a cumulative distribution function) and is evaluated at real non-negative values. I believe I managed to show that only decaying exponential functions of the type $\alpha^{-\lambda x}$, $\lambda>0$ could serve as solutions to the problem but Sheldon Ross claims that $e^{-\lambda x}$ is the only unique solution. This leads to me believe that perhaps I have done something wrong. Is he correct or incorrect? I apologize for not providing you with the proof I came up with, I am bit too lazy to write it down. Edit: Book pdf , page $211$ if anyone is interested, perhaps I did not mention something important that might be in there.","""It turns out that not only is the exponential distribution memoryless, but it is also the unique distribution possessing this property."" - Sheldon Ross, author of ""A First Course in Probability (eighth edition)"". The proof presented in the book isn't particularly rigorous (nor is it ment to be) so with some help from the the response of Michael Hardy in Uniqueness of memoryless property I believe I managed to arrive to the correct conclusion but I am uncertain and thus I seek some clarification regarding my result. Ultimately the problem boils down to solving the equation $f(s+t)=f(s)f(t)$ where $f$ is non-negative (since it's a cumulative distribution function) and is evaluated at real non-negative values. I believe I managed to show that only decaying exponential functions of the type $\alpha^{-\lambda x}$, $\lambda>0$ could serve as solutions to the problem but Sheldon Ross claims that $e^{-\lambda x}$ is the only unique solution. This leads to me believe that perhaps I have done something wrong. Is he correct or incorrect? I apologize for not providing you with the proof I came up with, I am bit too lazy to write it down. Edit: Book pdf , page $211$ if anyone is interested, perhaps I did not mention something important that might be in there.",,"['probability', 'ordinary-differential-equations', 'probability-theory', 'probability-distributions', 'exponential-distribution']"
36,Stability of an equilibrium point,Stability of an equilibrium point,,"I would like to determine the stability of equilibrium point $x=0$ of the differential equation $$\frac{dx}{dt}=x-\sqrt{x}, \\ x(t) > 0$$ By the standard testing procedure, I calculated $f'(x)$ as $1-\frac{1}{2}x^{\frac{1}{2}}$. Plugging $x=0$ into the $f'(x)$ gives $f'(0)=1>0$. Then we have $x=0$ is unstable. However, when I plot the slope field of the DE. I found that the trajectories are actually approaching $x=0$. Why?","I would like to determine the stability of equilibrium point $x=0$ of the differential equation $$\frac{dx}{dt}=x-\sqrt{x}, \\ x(t) > 0$$ By the standard testing procedure, I calculated $f'(x)$ as $1-\frac{1}{2}x^{\frac{1}{2}}$. Plugging $x=0$ into the $f'(x)$ gives $f'(0)=1>0$. Then we have $x=0$ is unstable. However, when I plot the slope field of the DE. I found that the trajectories are actually approaching $x=0$. Why?",,"['ordinary-differential-equations', 'stability-in-odes']"
37,ODE how to check if the kernel is empty,ODE how to check if the kernel is empty,,"In class we are working on a type of problems called green function $G$, and we use it to solve second order ODE. What I have an issue is that for problems where the kernel of L is empty we use one approach to get to $G$, and if the kernel is non-empty we have a more complicated way. My problem in particular is that I do not know how to inspect when the kernel is empty. Example. $y''+y=1$ with $y(0)=0$, $y(\frac{\pi}{2})=0$ has an empty kernel Example $y''+y=f(x)$ with $y(0)=0$ $y(\pi)=0$ has a kernel spanned by $\sin(x)$. From what I understand what we do is we take the homogeneous equation and solve with the boundary conditions. Can someone explain to me how we see when the kernel is empty and how we do it in general. Thank you for the time guys","In class we are working on a type of problems called green function $G$, and we use it to solve second order ODE. What I have an issue is that for problems where the kernel of L is empty we use one approach to get to $G$, and if the kernel is non-empty we have a more complicated way. My problem in particular is that I do not know how to inspect when the kernel is empty. Example. $y''+y=1$ with $y(0)=0$, $y(\frac{\pi}{2})=0$ has an empty kernel Example $y''+y=f(x)$ with $y(0)=0$ $y(\pi)=0$ has a kernel spanned by $\sin(x)$. From what I understand what we do is we take the homogeneous equation and solve with the boundary conditions. Can someone explain to me how we see when the kernel is empty and how we do it in general. Thank you for the time guys",,"['linear-algebra', 'ordinary-differential-equations']"
38,Definition of ' blow up time' in the context of PDE,Definition of ' blow up time' in the context of PDE,,"$$ \partial_t u - x \partial_x u = -u^2$$ $$ u(x,0) = u_0 (x)$$ (i) Find the blow up time $t_*$ for the cauchy data $u_0(x) = cosx $ and determine the position $x(t_*)$ which the solution develops poles. Found a solution for this PDE using method of characteristics, but never heard the term ' blow up time ' ever for the context of PDE. Seems like it has something to do with solution going infinity but not sure. Any help how to solve this problem??","(i) Find the blow up time for the cauchy data and determine the position which the solution develops poles. Found a solution for this PDE using method of characteristics, but never heard the term ' blow up time ' ever for the context of PDE. Seems like it has something to do with solution going infinity but not sure. Any help how to solve this problem??"," \partial_t u - x \partial_x u = -u^2  u(x,0) = u_0 (x) t_* u_0(x) = cosx  x(t_*)","['ordinary-differential-equations', 'partial-differential-equations']"
39,To solve the non linear IVP $y'(t)=1-y^2(t)$,To solve the non linear IVP,y'(t)=1-y^2(t),"Let $y: \mathbb{R} \rightarrow \mathbb{R}$ satisfy the initial value problem $$y'(t)=1-y^2(t), \ t \in \mathbb{R},$$ $$y(0)=0$$   Then which of the following is true $(A)$ $y(t_1)=1$ for some $t_1 \in \mathbb{R}$ $(B)$ $y(t)>-1$ for all $t \in \mathbb{R}$ $(C)$ $y$ is strictly increasing in $\mathbb{R}$ $(D)$ $y$ is increasing in $(0,1)$ and decreasing in $(1,\infty)$ I am a biology researcher learning differential equations for related work. I did  an undergraduate course in differential equations but have forgotten most of it. Any help would be appreciated.","Let $y: \mathbb{R} \rightarrow \mathbb{R}$ satisfy the initial value problem $$y'(t)=1-y^2(t), \ t \in \mathbb{R},$$ $$y(0)=0$$   Then which of the following is true $(A)$ $y(t_1)=1$ for some $t_1 \in \mathbb{R}$ $(B)$ $y(t)>-1$ for all $t \in \mathbb{R}$ $(C)$ $y$ is strictly increasing in $\mathbb{R}$ $(D)$ $y$ is increasing in $(0,1)$ and decreasing in $(1,\infty)$ I am a biology researcher learning differential equations for related work. I did  an undergraduate course in differential equations but have forgotten most of it. Any help would be appreciated.",,['ordinary-differential-equations']
40,Rigorous Understanding of Behavior around Equilibrium Points of this Dynamical System,Rigorous Understanding of Behavior around Equilibrium Points of this Dynamical System,,"I'm starting Strogatz's Dynamical Systems and Chaos . At the beginning he wants to develop intuition towards the concepts in dynamical systems before jumping into rigorous theory. As such, he emphases graphical analysis as opposed to analysis. One of the first examples he gives is the simple one dimensional system $$\dot{x} = \sin x$$ The following question is asked: For an arbitrary initial condition $x_0$ what is the behavior of $x(t)$ as $t \to \infty$ ? First, lets turn to the graph of $\dot{x}$ . Strogatz's Answer Strogatz answers that the particle will asymptotically approach the nearest stable point on the graph; here's an example trajectory where $x_0=\frac{\pi}{2}$ While I intuitively understand why this would be so, being newly introduced to rigorous mathematics, I'm intellectually skeptical of myself if I think I understand an answer without first seeing some rigorous reasoning as to why it is so. I'm hoping someone can confirm my more rigorous solution or suggest what reasoning allows him to make his argument so confidently. My answer From the graph, we see that a particle dropped at any point $x_0$ will continue moving in a direction determined by $\text{sgn}(\dot{x}(x_0))$ . By the graph, the magnitude of the particle's velocity decreases as it nears a stable point of $\dot{x}(x)$ . Consider that after some time, the particle is sufficiently close to a point $\tilde{x}$ s.t. $\dot{x}(\tilde{x})=0$ and we can make a small angle approximation for velocity, $\dot{x}=\sin x\approx x'$ , where $x'$ shifts the function such that the origin is at $\tilde{x}$ . Now the derivative of velocity linearized at $\tilde{x}$ is $\dot{x}'=sgn{x'}x$ and solving this yields $x(t)=x'e^{-t}$ . It can trivially be shown that $x(t)$ converges to zero as $t \to \infty$ . $\square$ This answer is very heuristic – it only describes the magnitude of distance, not direction or sign and helps understand the qualitative nature of Fig 2.1.2 , but not rigorously. I'm looking for guidance as to make my argument more precise. Assume my formal background ends at Calc I and that I won't appreciate any advanced real analysis concepts. EDIT: I had originally said that the particle approaches the nearest zero of the graph, but I edited that to say stable point which is the correct limit of the system.","I'm starting Strogatz's Dynamical Systems and Chaos . At the beginning he wants to develop intuition towards the concepts in dynamical systems before jumping into rigorous theory. As such, he emphases graphical analysis as opposed to analysis. One of the first examples he gives is the simple one dimensional system The following question is asked: For an arbitrary initial condition what is the behavior of as ? First, lets turn to the graph of . Strogatz's Answer Strogatz answers that the particle will asymptotically approach the nearest stable point on the graph; here's an example trajectory where While I intuitively understand why this would be so, being newly introduced to rigorous mathematics, I'm intellectually skeptical of myself if I think I understand an answer without first seeing some rigorous reasoning as to why it is so. I'm hoping someone can confirm my more rigorous solution or suggest what reasoning allows him to make his argument so confidently. My answer From the graph, we see that a particle dropped at any point will continue moving in a direction determined by . By the graph, the magnitude of the particle's velocity decreases as it nears a stable point of . Consider that after some time, the particle is sufficiently close to a point s.t. and we can make a small angle approximation for velocity, , where shifts the function such that the origin is at . Now the derivative of velocity linearized at is and solving this yields . It can trivially be shown that converges to zero as . This answer is very heuristic – it only describes the magnitude of distance, not direction or sign and helps understand the qualitative nature of Fig 2.1.2 , but not rigorously. I'm looking for guidance as to make my argument more precise. Assume my formal background ends at Calc I and that I won't appreciate any advanced real analysis concepts. EDIT: I had originally said that the particle approaches the nearest zero of the graph, but I edited that to say stable point which is the correct limit of the system.",\dot{x} = \sin x x_0 x(t) t \to \infty \dot{x} x_0=\frac{\pi}{2} x_0 \text{sgn}(\dot{x}(x_0)) \dot{x}(x) \tilde{x} \dot{x}(\tilde{x})=0 \dot{x}=\sin x\approx x' x' \tilde{x} \tilde{x} \dot{x}'=sgn{x'}x x(t)=x'e^{-t} x(t) t \to \infty \square,"['calculus', 'ordinary-differential-equations', 'dynamical-systems']"
41,Complex exponential and the Cauchy problem $\begin{cases} f'=f\\ f(0)=1 \end{cases}$,Complex exponential and the Cauchy problem,\begin{cases} f'=f\\ f(0)=1 \end{cases},How do you prove that the exponential function is the unique solution to this Cauchy problem in $\mathbb{C}$? $$\begin{cases} f'=f\\ f(0)=1 \end{cases}$$,How do you prove that the exponential function is the unique solution to this Cauchy problem in $\mathbb{C}$? $$\begin{cases} f'=f\\ f(0)=1 \end{cases}$$,,['complex-analysis']
42,Laplace transform of a differential equation: $y'+y=e^{-3t}\cos(2t)$,Laplace transform of a differential equation:,y'+y=e^{-3t}\cos(2t),"Given $y(0)=0$, solve $$y'+y=e^{3t}cos(2t)  $$ Steps: $$sY(s)-y(0)+Y(s)=\frac{s+3}{(s+3)^2+2^2}$$ $$Y(s)(s+1)=\frac{s+3}{(s+3)^2+2^2}$$ $$Y(s)=\frac{s+1}{(s+1)((s+3)^2+2^2)}+\frac{2}{(s+1)((s+3)^2+2^2)}$$ $$Y(s)=\frac{1}{(s+3)^2+2^2}+\frac{\textbf{2}}{\textbf{(s+1)((s+3)^2+2^2)}}$$ Now I can reverse transform the first fraction, may I know how to deal with bolded fraction?","Given $y(0)=0$, solve $$y'+y=e^{3t}cos(2t)  $$ Steps: $$sY(s)-y(0)+Y(s)=\frac{s+3}{(s+3)^2+2^2}$$ $$Y(s)(s+1)=\frac{s+3}{(s+3)^2+2^2}$$ $$Y(s)=\frac{s+1}{(s+1)((s+3)^2+2^2)}+\frac{2}{(s+1)((s+3)^2+2^2)}$$ $$Y(s)=\frac{1}{(s+3)^2+2^2}+\frac{\textbf{2}}{\textbf{(s+1)((s+3)^2+2^2)}}$$ Now I can reverse transform the first fraction, may I know how to deal with bolded fraction?",,"['calculus', 'integration', 'ordinary-differential-equations', 'laplace-transform']"
43,Solving linear differential equation with Fourier transform,Solving linear differential equation with Fourier transform,,I want to solve $$y''(x)+2y'(x)+y(x)=f(x)$$ for $y(x)$ with the Fourier transform. $f$ is from Schwartz space. EDIT: I removed my old incorrect derivation and gave a correct answer below. - This equation can be solved easier using integrating factors. - Thanks to Physicist137 for the help!,I want to solve $$y''(x)+2y'(x)+y(x)=f(x)$$ for $y(x)$ with the Fourier transform. $f$ is from Schwartz space. EDIT: I removed my old incorrect derivation and gave a correct answer below. - This equation can be solved easier using integrating factors. - Thanks to Physicist137 for the help!,,"['integration', 'ordinary-differential-equations', 'fourier-analysis', 'residue-calculus']"
44,Why is the solution to a nonhomogeneous differential equation the complementary solution + a particular solution? [closed],Why is the solution to a nonhomogeneous differential equation the complementary solution + a particular solution? [closed],,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question Why does only one particular solution allow enough degrees of freedom for the general solution?,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question Why does only one particular solution allow enough degrees of freedom for the general solution?,,['ordinary-differential-equations']
45,Time for the tank to be empty,Time for the tank to be empty,,"A hemispherical tank of radius 2 metres is initially full of water and has an outlet of 12 cm$^2$ cross-sectional are at the bottom. The outlet is opened at some instant. The flow through the outlet is according to the law  $v(t) = 0.6 \sqrt{2gh(t)}$, where $v(t)$ and $h(t)$ are respectively the velocity of the flow through the outlet and the height of water level above the outlet at time $t$, and $g$ is the acceleration due to gravity. Find the time it takes to empty the tank. I know to write a differential equation by relating the decrease of water level to the outflow in order to solve. But don't know how and as we are not not given the initial height how can we solve it.","A hemispherical tank of radius 2 metres is initially full of water and has an outlet of 12 cm$^2$ cross-sectional are at the bottom. The outlet is opened at some instant. The flow through the outlet is according to the law  $v(t) = 0.6 \sqrt{2gh(t)}$, where $v(t)$ and $h(t)$ are respectively the velocity of the flow through the outlet and the height of water level above the outlet at time $t$, and $g$ is the acceleration due to gravity. Find the time it takes to empty the tank. I know to write a differential equation by relating the decrease of water level to the outflow in order to solve. But don't know how and as we are not not given the initial height how can we solve it.",,"['calculus', 'integration', 'ordinary-differential-equations', 'physics']"
46,Solving a weird differential equation with y in the exponent,Solving a weird differential equation with y in the exponent,,"I have this equation $$ xy'=y-4xe^{2y/x} $$ I can't clasify it to any of the differential equation types. I am new in differential equations, how can I solve this ?","I have this equation $$ xy'=y-4xe^{2y/x} $$ I can't clasify it to any of the differential equation types. I am new in differential equations, how can I solve this ?",,['ordinary-differential-equations']
47,Particular solution of $y''+4y=x\cos x$,Particular solution of,y''+4y=x\cos x,"Find both the general solution $y_h$ and a particular solution $y_p$ for   $$y''+4y=x\cos x$$ So far I've got $y_h$ from factoring the characteristic polynomial: $$y_h=C_1\sin2x+C_2\cos2x$$ But the $y_p$ part troubles me, any help?","Find both the general solution $y_h$ and a particular solution $y_p$ for   $$y''+4y=x\cos x$$ So far I've got $y_h$ from factoring the characteristic polynomial: $$y_h=C_1\sin2x+C_2\cos2x$$ But the $y_p$ part troubles me, any help?",,"['calculus', 'ordinary-differential-equations']"
48,System of ODE with variable coefficients,System of ODE with variable coefficients,,"How Can I solve this System of ODE ?    $$ \frac{dy}{dx}=a(x)y+b(x)u $$   $$ \frac{du}{dx}=c(x)y+d(x)u $$ I can solve this system with constant coefficient but, I wonder if there is any relation between the method of solving system of  ODE with constant coefficient and with variable coefficient ?  and Is the method of solving for system with constant coefficient special case of the method of solving for system with variable coefficient or not ? Thank you ...","How Can I solve this System of ODE ?    $$ \frac{dy}{dx}=a(x)y+b(x)u $$   $$ \frac{du}{dx}=c(x)y+d(x)u $$ I can solve this system with constant coefficient but, I wonder if there is any relation between the method of solving system of  ODE with constant coefficient and with variable coefficient ?  and Is the method of solving for system with constant coefficient special case of the method of solving for system with variable coefficient or not ? Thank you ...",,"['linear-algebra', 'ordinary-differential-equations', 'systems-of-equations']"
49,Existence and uniqueness theorem - ODE solutions,Existence and uniqueness theorem - ODE solutions,,"So we have an ODE namely $$\frac{dy}{dx} =\frac{-x+\sqrt{x^2+4y}}{2}, \ y(2)=-1 $$.  Ok so we have two solutions $y_1=1-x$ which is valid for $x\geq 2 $ and $y_2=-x^2/4$ which seems to be valid for $x\in \mathbb{R} $.  Now a question asks how the existence of two solutions to the initial value problem does not contradict the existence and uniqueness theorem.  Anyone have a simple explanation?","So we have an ODE namely $$\frac{dy}{dx} =\frac{-x+\sqrt{x^2+4y}}{2}, \ y(2)=-1 $$.  Ok so we have two solutions $y_1=1-x$ which is valid for $x\geq 2 $ and $y_2=-x^2/4$ which seems to be valid for $x\in \mathbb{R} $.  Now a question asks how the existence of two solutions to the initial value problem does not contradict the existence and uniqueness theorem.  Anyone have a simple explanation?",,"['integration', 'ordinary-differential-equations', 'derivatives', 'differential']"
50,How do I solve differential equation $\frac{dx}{dt}=x^2+5x$ [closed],How do I solve differential equation  [closed],\frac{dx}{dt}=x^2+5x,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question How do I solve differential equation $\frac{dx}{dt}=x^2+5x$ I don't understand what type of differential equation is this. What to do with 't', we don't have it on right side at all?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question How do I solve differential equation $\frac{dx}{dt}=x^2+5x$ I don't understand what type of differential equation is this. What to do with 't', we don't have it on right side at all?",,['ordinary-differential-equations']
51,Calculating the time it takes for a particle to move using a gamma function,Calculating the time it takes for a particle to move using a gamma function,,"Calculate the time it takes a point particle of mass $m$ to roll down a log    potential, $ V(x) = \frac{V_0}{2}ln\frac{x}{x_0}$ using incomplete gamma function. The particle starts at position $x_0$ at time $t=0$ with energy $E=0$, and falls all the way into the singularity at $x=0$. ok so I tried solving using conservation of energy: $E = T + V$ $E(x) = \frac{1}{2}m\ddot r$  + $\frac{V_0}{2}ln\frac{x}{x_0}$ with initial conditions: $r(0) = x_0$;  $E = 0$ Plug in: $0 = \frac{1}{2}m x_0$ + $\frac{V_0}{2}ln\frac{x}{x_0}$ $-mx_0$ = $v_0 ln\frac{x}{x_0}$ From here how do you proceed using a gamma function? to be honest I have no idea how to solve this problem with using a gamma function.. any help will be appreciated.","Calculate the time it takes a point particle of mass $m$ to roll down a log    potential, $ V(x) = \frac{V_0}{2}ln\frac{x}{x_0}$ using incomplete gamma function. The particle starts at position $x_0$ at time $t=0$ with energy $E=0$, and falls all the way into the singularity at $x=0$. ok so I tried solving using conservation of energy: $E = T + V$ $E(x) = \frac{1}{2}m\ddot r$  + $\frac{V_0}{2}ln\frac{x}{x_0}$ with initial conditions: $r(0) = x_0$;  $E = 0$ Plug in: $0 = \frac{1}{2}m x_0$ + $\frac{V_0}{2}ln\frac{x}{x_0}$ $-mx_0$ = $v_0 ln\frac{x}{x_0}$ From here how do you proceed using a gamma function? to be honest I have no idea how to solve this problem with using a gamma function.. any help will be appreciated.",,"['ordinary-differential-equations', 'gamma-function', 'classical-mechanics']"
52,How can I guess particular solution of a Riccati differential equation?,How can I guess particular solution of a Riccati differential equation?,,I am given the two following differential equations. $$y'=6+5y+y^2$$ $$y'=9+6y+y^2$$ I have learned how to solve one with a given particular solution but cannot seem to get my head around this. I need help with guessing the particular solution of both of these.,I am given the two following differential equations. $$y'=6+5y+y^2$$ $$y'=9+6y+y^2$$ I have learned how to solve one with a given particular solution but cannot seem to get my head around this. I need help with guessing the particular solution of both of these.,,['ordinary-differential-equations']
53,"Linear, first order ODE, given general solution","Linear, first order ODE, given general solution",,"Construct a linear first order ODE of $$xy'+a_0(x)y=g(x)$$ such that the general solution is $$y=x^3+\frac{c}{x^3}$$ If I substitute the general solution into the question,  $$xy'+a_0(x)(x^3+\frac{c}{x^3})=g(x)$$ this form looks unusual to solve and a(x), g(x) are not given in any expression.  how can I to approach this?","Construct a linear first order ODE of $$xy'+a_0(x)y=g(x)$$ such that the general solution is $$y=x^3+\frac{c}{x^3}$$ If I substitute the general solution into the question,  $$xy'+a_0(x)(x^3+\frac{c}{x^3})=g(x)$$ this form looks unusual to solve and a(x), g(x) are not given in any expression.  how can I to approach this?",,['ordinary-differential-equations']
54,"solution of differential equation of the form f(x,y)/g(x,y)","solution of differential equation of the form f(x,y)/g(x,y)",,"If I'm given a differential equation of the form, $\frac{dy}{dx} = \frac{a_1x+b_1y+c_1}{a_2x+b_2y+c_2}$ that apparently can't be homogenized, will the differential equation always be solvable? For example, if I'm given the equation, $\frac{dy}{dx} = \frac{x+2y-3}{2x+y-3}$ How am I supposed to solve it? N.B. This isn't a homework question. I'm just curious whether this type of equations are solvable or not.","If I'm given a differential equation of the form, $\frac{dy}{dx} = \frac{a_1x+b_1y+c_1}{a_2x+b_2y+c_2}$ that apparently can't be homogenized, will the differential equation always be solvable? For example, if I'm given the equation, $\frac{dy}{dx} = \frac{x+2y-3}{2x+y-3}$ How am I supposed to solve it? N.B. This isn't a homework question. I'm just curious whether this type of equations are solvable or not.",,"['calculus', 'ordinary-differential-equations']"
55,Why the rectification theorem fails on a global scale?,Why the rectification theorem fails on a global scale?,,"The rectification theorem states that if $y$ in the domain with the vector field $v(y)\neq 0$, then there is a change of coordinates for a small neighborhood of $y$ such that the vector field becomes a constant vector field. Now suppose that a vector field $V$ has no singular points in a domain $U$. Can we rectify the field $V$ in the whole domain $U$? Wikipedia explained that there is a ""global constraint"", where the trajectory starts out in a patch, and after visiting a series of other patches, comes back to the original one. If the next time the orbit loops around phase space in a different way, then it is impossible to rectify the vector field in the whole series of patches. I don't understand Wikipedia's explanation. Could someone please shed some light on this ? Much appreciated.","The rectification theorem states that if $y$ in the domain with the vector field $v(y)\neq 0$, then there is a change of coordinates for a small neighborhood of $y$ such that the vector field becomes a constant vector field. Now suppose that a vector field $V$ has no singular points in a domain $U$. Can we rectify the field $V$ in the whole domain $U$? Wikipedia explained that there is a ""global constraint"", where the trajectory starts out in a patch, and after visiting a series of other patches, comes back to the original one. If the next time the orbit loops around phase space in a different way, then it is impossible to rectify the vector field in the whole series of patches. I don't understand Wikipedia's explanation. Could someone please shed some light on this ? Much appreciated.",,"['ordinary-differential-equations', 'dynamical-systems']"
56,why two solutions to DE are contradictory?,why two solutions to DE are contradictory?,,"Solution to the given differential equation $$\frac{dx}{dt}=4.9-0.196x$$   is given by a) $x=25+ke^{-0.196t}\qquad$ b) $x=50+ke^{-0.196t}\qquad$ c) $x=50-ke^{-0.196t}\qquad$ d) $x=25-ke^{-0.196t}\qquad$ where k is some constant my try: method(1) $$\frac{dx}{4.9-0.196x}=dt$$ $$\int \frac{dx}{4.9-0.196x}=t+c$$ substituting $4.9-0.196 x=z$, $dx=-\frac{dz}{0.196}$, i get $$\int \frac{dz}{z}=-0.196(t+c)$$ $$\ln z=\ln(4.9-0.196x)=-0.196(t+c) \tag{*}$$ $$0.196x=4.9-e^{-0.196(t+c)}=4.9-e^{-0.196c}\cdot e^{-0.196t}$$ $$x=25-ke^{-0.196t}\tag 1$$ method(2) $$\frac{dx}{dt}=4.9-0.196x$$ $$\frac{dx}{dt}+0.196x=4.9$$ now, use integration factor $I.F.=e^{\int 0.196\ dt}=e^{0.196 t}$ so the solution is  $$x\cdot (I.F.)=\int (I.F.)\cdot 4.9\ dt+c$$ $$x\cdot e^{0.196 t}=\int e^{0.196 t}\cdot 4.9\ dt+c$$ $$x\cdot e^{0.196 t}=\frac{4.9}{0.196}e^{0.196 t}+c=25e^{0.196 t}+c$$ $$x=25+ke^{-0.196 t}\tag 2$$ now, you see i am getting two different solutions (1) & (2) to the same D.E., but i don't know which one is correct & why. please explain me where i am wrong or which is the correct option & why?","Solution to the given differential equation $$\frac{dx}{dt}=4.9-0.196x$$   is given by a) $x=25+ke^{-0.196t}\qquad$ b) $x=50+ke^{-0.196t}\qquad$ c) $x=50-ke^{-0.196t}\qquad$ d) $x=25-ke^{-0.196t}\qquad$ where k is some constant my try: method(1) $$\frac{dx}{4.9-0.196x}=dt$$ $$\int \frac{dx}{4.9-0.196x}=t+c$$ substituting $4.9-0.196 x=z$, $dx=-\frac{dz}{0.196}$, i get $$\int \frac{dz}{z}=-0.196(t+c)$$ $$\ln z=\ln(4.9-0.196x)=-0.196(t+c) \tag{*}$$ $$0.196x=4.9-e^{-0.196(t+c)}=4.9-e^{-0.196c}\cdot e^{-0.196t}$$ $$x=25-ke^{-0.196t}\tag 1$$ method(2) $$\frac{dx}{dt}=4.9-0.196x$$ $$\frac{dx}{dt}+0.196x=4.9$$ now, use integration factor $I.F.=e^{\int 0.196\ dt}=e^{0.196 t}$ so the solution is  $$x\cdot (I.F.)=\int (I.F.)\cdot 4.9\ dt+c$$ $$x\cdot e^{0.196 t}=\int e^{0.196 t}\cdot 4.9\ dt+c$$ $$x\cdot e^{0.196 t}=\frac{4.9}{0.196}e^{0.196 t}+c=25e^{0.196 t}+c$$ $$x=25+ke^{-0.196 t}\tag 2$$ now, you see i am getting two different solutions (1) & (2) to the same D.E., but i don't know which one is correct & why. please explain me where i am wrong or which is the correct option & why?",,"['calculus', 'ordinary-differential-equations']"
57,Difference between CF particular solution general solution,Difference between CF particular solution general solution,,I have am solving a 2nd order differential equation and have been asked to give i) Complementary function ii) a particular solution iii) General solution I just wanted to know whats the difference between particular solution and general solution,I have am solving a 2nd order differential equation and have been asked to give i) Complementary function ii) a particular solution iii) General solution I just wanted to know whats the difference between particular solution and general solution,,['ordinary-differential-equations']
58,Solving a Coupled Second-Order Differential Equation,Solving a Coupled Second-Order Differential Equation,,"I'm trying to solve the real-valued differential equation $$xy\ddot{y}+2y\dot{x}\dot{y}+\alpha\ddot{x}+x\dot{y}^2=0$$ where $\alpha\in\mathbb{R}$ and $\alpha>0$ for both $x(t)$ and $y(t)$.  How would I even approach this problem? EDIT: Sorry, I forgot a conservation law: $$\alpha^2\dot{x}^2+x^4\dot{y}^2 = P^2$$ where $P$ is a conserved quantity. EDIT 2: The original Lagrangian this comes from is $$\mathcal{L}=\frac{1}{2}\left(-\alpha\dot{x}^2+x^2\dot{y}^2+x^2y^2\dot{\theta}^2+x^2y^2\sin^2\theta\dot{\phi}^2\right)$$","I'm trying to solve the real-valued differential equation $$xy\ddot{y}+2y\dot{x}\dot{y}+\alpha\ddot{x}+x\dot{y}^2=0$$ where $\alpha\in\mathbb{R}$ and $\alpha>0$ for both $x(t)$ and $y(t)$.  How would I even approach this problem? EDIT: Sorry, I forgot a conservation law: $$\alpha^2\dot{x}^2+x^4\dot{y}^2 = P^2$$ where $P$ is a conserved quantity. EDIT 2: The original Lagrangian this comes from is $$\mathcal{L}=\frac{1}{2}\left(-\alpha\dot{x}^2+x^2\dot{y}^2+x^2y^2\dot{\theta}^2+x^2y^2\sin^2\theta\dot{\phi}^2\right)$$",,"['ordinary-differential-equations', 'nonlinear-system']"
59,Proving statement about the ODE $y'=y+y^4$,Proving statement about the ODE,y'=y+y^4,"Is there a solution to the problem $$\left\{\begin{matrix} y'=y+y^4\\  y(x_0)=y_0 \end{matrix}\right.$$   which is defined on $\mathbb{R}$? ($x_0,y_0$ might be any real numbers) It's easy to prove that for all $(x,y)\in\mathbb{R}^2$ there exists an open interval $I$ (with $x_0\in I$) where the problem has a unique solution. However is the maximal interval always $(-\infty,\infty)$? I know that the answer is no, but that's just because I found a solution for particular values of $x_0$ and $y_0$ and checked its domain. But is there a way to prove that $I$ need not be $I=(-\infty,\infty)$ without actually solving the problem for a certain initial condition? In other words, how can I prove that the unique solution need not be defined on $\mathbb{R}$?","Is there a solution to the problem $$\left\{\begin{matrix} y'=y+y^4\\  y(x_0)=y_0 \end{matrix}\right.$$   which is defined on $\mathbb{R}$? ($x_0,y_0$ might be any real numbers) It's easy to prove that for all $(x,y)\in\mathbb{R}^2$ there exists an open interval $I$ (with $x_0\in I$) where the problem has a unique solution. However is the maximal interval always $(-\infty,\infty)$? I know that the answer is no, but that's just because I found a solution for particular values of $x_0$ and $y_0$ and checked its domain. But is there a way to prove that $I$ need not be $I=(-\infty,\infty)$ without actually solving the problem for a certain initial condition? In other words, how can I prove that the unique solution need not be defined on $\mathbb{R}$?",,['ordinary-differential-equations']
60,Are there periodic functions satisfying a quadratic differential equation?,Are there periodic functions satisfying a quadratic differential equation?,,"Question: Are there periodic functions satisfying a quadratic differential equation, as opposed to just linear or cubic? Bonus question: Are there periodic functions satisfying differential equations which are polynomials of any degree $n$ ? Background: I know that on the real line, any periodic function can be decomposed into a (possibly infinite) sum of sines and cosines via Fourier series, and that this technique is somewhat extensible to the complex plane (I think). Likewise, cosine and sine can be defined as the solutions to a linear systems of ordinary differential equation (at least on the real line, I am not sure about the complex plane), see e.g here . A natural generalization/extension of the cosine and sine would be to either consider (1) functions defining the trigonometry of more general conic sections than the circle or (2) functions which have more ""advanced"" periodicity properties, like double periodicity. Functions which seem to satisfy both of these criteria would be the elliptic functions , because they are (1) the inverses of elliptic integrals, and (2) doubly periodic. However, what is surprising to me is that the (complex) differential equation which they satisfy is cubic in the function and its first derivative, rather than quadratic. This seems to suggest a substantial jump in complexity from the trigonometric functions. This question is motivated by Algebraic Geometry: A Problem Solving Approach , which mentions the elliptic functions in the context of cubic curves, but does not mention any special functions related to conic sections -- is this because there aren't any, or are they just somehow less important? Attempt: Maybe the hyperbolic tangent function? I don't think that it's periodic, but it is related to the exponential, which is periodic. Also it seems to satisfy a quadratic differential equation . Note: I tagged this (complex-analysis) because it seems to be about complex differential equations.","Question: Are there periodic functions satisfying a quadratic differential equation, as opposed to just linear or cubic? Bonus question: Are there periodic functions satisfying differential equations which are polynomials of any degree ? Background: I know that on the real line, any periodic function can be decomposed into a (possibly infinite) sum of sines and cosines via Fourier series, and that this technique is somewhat extensible to the complex plane (I think). Likewise, cosine and sine can be defined as the solutions to a linear systems of ordinary differential equation (at least on the real line, I am not sure about the complex plane), see e.g here . A natural generalization/extension of the cosine and sine would be to either consider (1) functions defining the trigonometry of more general conic sections than the circle or (2) functions which have more ""advanced"" periodicity properties, like double periodicity. Functions which seem to satisfy both of these criteria would be the elliptic functions , because they are (1) the inverses of elliptic integrals, and (2) doubly periodic. However, what is surprising to me is that the (complex) differential equation which they satisfy is cubic in the function and its first derivative, rather than quadratic. This seems to suggest a substantial jump in complexity from the trigonometric functions. This question is motivated by Algebraic Geometry: A Problem Solving Approach , which mentions the elliptic functions in the context of cubic curves, but does not mention any special functions related to conic sections -- is this because there aren't any, or are they just somehow less important? Attempt: Maybe the hyperbolic tangent function? I don't think that it's periodic, but it is related to the exponential, which is periodic. Also it seems to satisfy a quadratic differential equation . Note: I tagged this (complex-analysis) because it seems to be about complex differential equations.",n,"['complex-analysis', 'ordinary-differential-equations']"
61,ODEs uniqueness of nonlinear equation,ODEs uniqueness of nonlinear equation,,"I am to show that solutions to the ODE: $x'(t) = -\frac{1}{2} \left(x(t)^2 - 3 c \right)$, are not unique. I don't know why this is problematic, since, the ODE clearly has a solution that seems to work for all $t$: $x(t) = \sqrt{3} \sqrt{c} \tanh \left(\frac{1}{2} \left(\sqrt{3} \sqrt{c} t+2 \sqrt{3} \sqrt{c} c_1\right)\right)$, where $c_1$ is a constant. But, my professor insists that this is not a unique solution!","I am to show that solutions to the ODE: $x'(t) = -\frac{1}{2} \left(x(t)^2 - 3 c \right)$, are not unique. I don't know why this is problematic, since, the ODE clearly has a solution that seems to work for all $t$: $x(t) = \sqrt{3} \sqrt{c} \tanh \left(\frac{1}{2} \left(\sqrt{3} \sqrt{c} t+2 \sqrt{3} \sqrt{c} c_1\right)\right)$, where $c_1$ is a constant. But, my professor insists that this is not a unique solution!",,"['real-analysis', 'ordinary-differential-equations', 'dynamical-systems']"
62,Is it possible to have a $f(\vec{r})$ satisfy this relation?,Is it possible to have a  satisfy this relation?,f(\vec{r}),"It is known that: $$ (\nabla^2+k^2)(-\frac{e^{ikr}}{4\pi r})=\delta(\vec{r}) $$ where $k>0$ and $\delta(\vec{r})$ is the three dimensional Dirac delta function . My question is, is it possible to find a function $f(\vec{r})$ that satisfies the following relation: $$ \left[\frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2}-(\frac{\partial}{\partial z}+i\alpha)^2-k^2\right] f(\vec{r})=\delta(\vec{r}) $$ Where $\alpha>0,k>0$.","It is known that: $$ (\nabla^2+k^2)(-\frac{e^{ikr}}{4\pi r})=\delta(\vec{r}) $$ where $k>0$ and $\delta(\vec{r})$ is the three dimensional Dirac delta function . My question is, is it possible to find a function $f(\vec{r})$ that satisfies the following relation: $$ \left[\frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2}-(\frac{\partial}{\partial z}+i\alpha)^2-k^2\right] f(\vec{r})=\delta(\vec{r}) $$ Where $\alpha>0,k>0$.",,"['ordinary-differential-equations', 'partial-differential-equations', 'partial-fractions', 'dirac-delta', 'greens-function']"
63,What should I study first to understand Lojasiewicz gradient inequality?,What should I study first to understand Lojasiewicz gradient inequality?,,"I have read the paper ""Some application of the Lojasiewicz gradient inequality"" of Alain Haraux in the last days, and I found it really difficult for me from some first lines. For example, what is the definition for Lyapunov function that matches things referred in the article, since I've found several definitions for it on the internet. Here is the first page of the paper: I want to ask for some foundation knowledge to study first in order to prepare for reading this article. Thank you very much.","I have read the paper ""Some application of the Lojasiewicz gradient inequality"" of Alain Haraux in the last days, and I found it really difficult for me from some first lines. For example, what is the definition for Lyapunov function that matches things referred in the article, since I've found several definitions for it on the internet. Here is the first page of the paper: I want to ask for some foundation knowledge to study first in order to prepare for reading this article. Thank you very much.",,"['ordinary-differential-equations', 'dynamical-systems', 'vector-analysis']"
64,Using the Peano existence theorem (ODE's) to imply the implicit function theorem,Using the Peano existence theorem (ODE's) to imply the implicit function theorem,,"The question, found in Hale's Ordinary Differential Equations, is to State an implicit function theorem whose validity will be implied by the Peano existence theorem. I am confused on how this should be done, because the Peano existence theorem in the book used the implicit function theorem within the proof.  I realize there are other ways to prove Peano existence, such as Arzela-Ascoli, so perhaps this isn't a problem.  Nonetheless, I am out of ideas of how this statement should be posed. Any conceptual intuition or plain answers would be greatly appreciated.","The question, found in Hale's Ordinary Differential Equations, is to State an implicit function theorem whose validity will be implied by the Peano existence theorem. I am confused on how this should be done, because the Peano existence theorem in the book used the implicit function theorem within the proof.  I realize there are other ways to prove Peano existence, such as Arzela-Ascoli, so perhaps this isn't a problem.  Nonetheless, I am out of ideas of how this statement should be posed. Any conceptual intuition or plain answers would be greatly appreciated.",,"['ordinary-differential-equations', 'implicit-function-theorem']"
65,"Converting $\frac{d\, y}{d\, x} = x^2 + y^{\frac{2}{3}}$ to variable separable form",Converting  to variable separable form,"\frac{d\, y}{d\, x} = x^2 + y^{\frac{2}{3}}","The question is as follows: Use method of separation of variables to solve    $$\frac{d\, y}{d\, x} = x^2 + y^{\frac{2}{3}}$$ I am aware of following two methods (as given here ) for converting differential equations to variable separable form: If $\frac{d\, y}{d\, x} = F\left(\frac{y}{x}\right)$ If $\frac{d\, y}{d\, x} = F(ax + by + c)$ But I don't know how to use this knowledge to convert given differential equation to variable separable form.","The question is as follows: Use method of separation of variables to solve    $$\frac{d\, y}{d\, x} = x^2 + y^{\frac{2}{3}}$$ I am aware of following two methods (as given here ) for converting differential equations to variable separable form: If $\frac{d\, y}{d\, x} = F\left(\frac{y}{x}\right)$ If $\frac{d\, y}{d\, x} = F(ax + by + c)$ But I don't know how to use this knowledge to convert given differential equation to variable separable form.",,['ordinary-differential-equations']
66,"If y=y(x) and $\frac{2+sin(x)}{y+1}\frac{dy}{dx}=-cosx,y(0)=1$, then find $y(\frac{\pi}{2})$.","If y=y(x) and , then find .","\frac{2+sin(x)}{y+1}\frac{dy}{dx}=-cosx,y(0)=1 y(\frac{\pi}{2})","Question: If y=y(x) and $\frac{2+sin(x)}{y+1}\frac{dy}{dx}=-cosx,y(0)=1$, then find $y(\frac{\pi}{2})$. My attempt:- $$\frac{2+sin(x)}{y+1}\frac{dy}{dx}=-cosx$$ $$\frac{dy}{1+y}=-\frac{cos(x)}{2+sin(x)}dx$$ Integrating both sides $$\int\frac{dy}{1+y}=-\int\frac{cos(x)}{2+sin(x)}dx$$ $$log(1+y)=-log(2+sin(x))+C$$ $$log(1+y)=log(\frac{1}{2+sin(x)})+C$$ Antilog on both sides $$1+y=\frac{1}{2+sin(x)}+C$$ $$y=\frac{-1-sin(x)}{2+sin(x)}+C$$ Now it is given that $y(0)=1$ So $$y(0)=1=\frac{-1-sin(0)}{2+sin(0)}+C$$ $$1=\frac{-1}{2}+C$$ $$C=\frac{3}{2}$$ $$y(\frac{\pi}{2})=\frac{-1-sin(\frac{\pi}{2})}{2+sin(\frac{\pi}{2})}+C$$ $$y(\frac{\pi}{2})=\frac{-2}{3}+\frac{3}{2}$$ $$y(\frac{\pi}{2})=\frac{5}{6}$$ But the answer happens to be $\frac{1}{3}$.","Question: If y=y(x) and $\frac{2+sin(x)}{y+1}\frac{dy}{dx}=-cosx,y(0)=1$, then find $y(\frac{\pi}{2})$. My attempt:- $$\frac{2+sin(x)}{y+1}\frac{dy}{dx}=-cosx$$ $$\frac{dy}{1+y}=-\frac{cos(x)}{2+sin(x)}dx$$ Integrating both sides $$\int\frac{dy}{1+y}=-\int\frac{cos(x)}{2+sin(x)}dx$$ $$log(1+y)=-log(2+sin(x))+C$$ $$log(1+y)=log(\frac{1}{2+sin(x)})+C$$ Antilog on both sides $$1+y=\frac{1}{2+sin(x)}+C$$ $$y=\frac{-1-sin(x)}{2+sin(x)}+C$$ Now it is given that $y(0)=1$ So $$y(0)=1=\frac{-1-sin(0)}{2+sin(0)}+C$$ $$1=\frac{-1}{2}+C$$ $$C=\frac{3}{2}$$ $$y(\frac{\pi}{2})=\frac{-1-sin(\frac{\pi}{2})}{2+sin(\frac{\pi}{2})}+C$$ $$y(\frac{\pi}{2})=\frac{-2}{3}+\frac{3}{2}$$ $$y(\frac{\pi}{2})=\frac{5}{6}$$ But the answer happens to be $\frac{1}{3}$.",,['ordinary-differential-equations']
67,Solution of $ydx-xdy+3x^2y^2e^{x^2}dx=0$,Solution of,ydx-xdy+3x^2y^2e^{x^2}dx=0,Find the solution of given differential equation: $$ydx-xdy+3x^2y^2e^{x^2}dx=0$$ I am not able to solve this because of $e^{x^2}$. Could someone help me with this one?,Find the solution of given differential equation: $$ydx-xdy+3x^2y^2e^{x^2}dx=0$$ I am not able to solve this because of $e^{x^2}$. Could someone help me with this one?,,"['calculus', 'ordinary-differential-equations']"
68,Use of differential equations for modeling population which is a discrete variable,Use of differential equations for modeling population which is a discrete variable,,"Population dynamics is often modeled using ODE. For example one common model is logistic growth model: $$\frac {dx}{dt} = kx\left(1-\frac{x}{C}\right)$$ where $x$ is population size, $k$ is rate constant for growth, $C$ is carrying capacity. But population is a discrete variable. It is not continuous. It always takes whole numbers. You can have a population of 3000 fishes, but not 3001.2 fishes. Then how can one use population as a dependent variable in a differential equation? Integration of the ODE, given above, will give me a function to calculate size of population, $x_t$, at time $t$, when size of the population at $t = 0$, was $x_0$. We can specify a whole number for $x_0$. But, $x_t$ can be a real number with fraction. But a population size is always a whole number. How does one tackle this anomaly?","Population dynamics is often modeled using ODE. For example one common model is logistic growth model: $$\frac {dx}{dt} = kx\left(1-\frac{x}{C}\right)$$ where $x$ is population size, $k$ is rate constant for growth, $C$ is carrying capacity. But population is a discrete variable. It is not continuous. It always takes whole numbers. You can have a population of 3000 fishes, but not 3001.2 fishes. Then how can one use population as a dependent variable in a differential equation? Integration of the ODE, given above, will give me a function to calculate size of population, $x_t$, at time $t$, when size of the population at $t = 0$, was $x_0$. We can specify a whole number for $x_0$. But, $x_t$ can be a real number with fraction. But a population size is always a whole number. How does one tackle this anomaly?",,"['ordinary-differential-equations', 'mathematical-modeling', 'biology']"
69,Which ODE systems can be integrated by quadrature?,Which ODE systems can be integrated by quadrature?,,"I am interested if it can be proven the non integrability by quadrature of a particular system of ordinary differential equations. And I read that the differential Galois theory could be the answer, but I want to be sure before I start studying it and realize after several weeks that I have wasted my time. Suppose you have a nonlinear system of first order of ordinary differential equations: \begin{eqnarray} \dfrac{d v}{d r}(r)&=&\dfrac{R\sinh(r)}{r\sinh(R)}\left[1-\zeta_1\sqrt{\sigma^2(r)+2(\sigma(r)-\beta(r))^2}\right]-\epsilon\left[1+\zeta_2\sqrt{\sigma^2(r)+2(\sigma(r)-\beta(r))^2}\right]\\&-&2\dfrac{v(r)}{r},\\ \dfrac{d\sigma}{d r}(r)&=&-\dfrac{2\beta(r)}{r},\\ v(r)\dfrac{d\beta}{d r}(r)&=&2\left(\dfrac{1}{e^{\beta(r)}+2}\dfrac{d v}{d r}(r)+\left(\dfrac{2}{e^{\beta(r)}+2}-1\right)\dfrac{v(r)}{r}\right), \end{eqnarray} with $r\in(0,R)$. It is possible with the differential theory of Galois prove that this system can or can't be integrated by quadratures? If it isn't possible, there is another way to prove this? Thanks in advance.","I am interested if it can be proven the non integrability by quadrature of a particular system of ordinary differential equations. And I read that the differential Galois theory could be the answer, but I want to be sure before I start studying it and realize after several weeks that I have wasted my time. Suppose you have a nonlinear system of first order of ordinary differential equations: \begin{eqnarray} \dfrac{d v}{d r}(r)&=&\dfrac{R\sinh(r)}{r\sinh(R)}\left[1-\zeta_1\sqrt{\sigma^2(r)+2(\sigma(r)-\beta(r))^2}\right]-\epsilon\left[1+\zeta_2\sqrt{\sigma^2(r)+2(\sigma(r)-\beta(r))^2}\right]\\&-&2\dfrac{v(r)}{r},\\ \dfrac{d\sigma}{d r}(r)&=&-\dfrac{2\beta(r)}{r},\\ v(r)\dfrac{d\beta}{d r}(r)&=&2\left(\dfrac{1}{e^{\beta(r)}+2}\dfrac{d v}{d r}(r)+\left(\dfrac{2}{e^{\beta(r)}+2}-1\right)\dfrac{v(r)}{r}\right), \end{eqnarray} with $r\in(0,R)$. It is possible with the differential theory of Galois prove that this system can or can't be integrated by quadratures? If it isn't possible, there is another way to prove this? Thanks in advance.",,"['ordinary-differential-equations', 'galois-theory', 'nonlinear-system']"
70,Finding a differential equation orthogonal to a family of curves,Finding a differential equation orthogonal to a family of curves,,"The question is: Consider the family $F$ of circles in the $xy$ plane, $(x-c)^2+y^2=c^2$ tangent to the $y$ axis at the origin. Find a differential equation that is satisfied by the family of curves orthogonal to $F$. My thinking: Since the implicit equation represents the level sets of the function $$ f(x,y)=c^2=(x-c)^2+y^2 $$ The gradient of the function $f$ will be perpendicular to its level sets, and therefore orthogonal to the family of curves $F$. This yields $$ \nabla f(x,y)=(0,0)=(2x-2c,2y)\Rightarrow \left(x-\frac{x^2+y^2}{2x},y\right)=(0,0)\\ \Rightarrow \left(\frac{x^2-y^2}{2x},y\right)=(0,0) $$ So we have in differential form $$ \frac{x^2-y^2}{2x}dx+ydy=0\Rightarrow \frac{y^2-x^2}{2xy}=\frac{dy}{dx} $$ But the answer is the negative reciprocal, or perpendicular vector to this one. Why? I assume my reasoning was flawed in the first step, when i took the gradient of $f$ to be perpindicular to the family $F$, but I don't see why.","The question is: Consider the family $F$ of circles in the $xy$ plane, $(x-c)^2+y^2=c^2$ tangent to the $y$ axis at the origin. Find a differential equation that is satisfied by the family of curves orthogonal to $F$. My thinking: Since the implicit equation represents the level sets of the function $$ f(x,y)=c^2=(x-c)^2+y^2 $$ The gradient of the function $f$ will be perpendicular to its level sets, and therefore orthogonal to the family of curves $F$. This yields $$ \nabla f(x,y)=(0,0)=(2x-2c,2y)\Rightarrow \left(x-\frac{x^2+y^2}{2x},y\right)=(0,0)\\ \Rightarrow \left(\frac{x^2-y^2}{2x},y\right)=(0,0) $$ So we have in differential form $$ \frac{x^2-y^2}{2x}dx+ydy=0\Rightarrow \frac{y^2-x^2}{2xy}=\frac{dy}{dx} $$ But the answer is the negative reciprocal, or perpendicular vector to this one. Why? I assume my reasoning was flawed in the first step, when i took the gradient of $f$ to be perpindicular to the family $F$, but I don't see why.",,"['ordinary-differential-equations', 'multivariable-calculus', 'vector-analysis']"
71,How to solve $y'' + y = -2\sin(x)$?,How to solve ?,y'' + y = -2\sin(x),I don't know how to find the particular solution of  $$y'' +  y = -2\sin(x)$$ I started with $$y'' +  y = 0$$ to find the homogeneous form $$A\cos(x)+B\sin(x)$$ But now i am stuck.,I don't know how to find the particular solution of  $$y'' +  y = -2\sin(x)$$ I started with $$y'' +  y = 0$$ to find the homogeneous form $$A\cos(x)+B\sin(x)$$ But now i am stuck.,,['ordinary-differential-equations']
72,Solve $\frac{\mathrm{d}y}{\mathrm{d}x} = (x-y)/(x+y)$,Solve,\frac{\mathrm{d}y}{\mathrm{d}x} = (x-y)/(x+y),"Solve $$\frac { { d }y }{ { d }x } =\frac { x-y }{ x+y } $$ It is homogeneous, thus let $y = vx$. From this, $\frac{\mathrm{d}y}{\mathrm{d}x} = x\frac{\mathrm{d}v}{\mathrm{d}x} + v$ Thus, $v'x + v = (1-v)/(1+v)$ thus, $\frac{2}{1-v} + \ln(v - 1) = \ln(x) + C$. Which by substituion, $\frac{2x}{x-y} + \ln(y - x) = 2\ln(x) + C$ But I cant get it any further","Solve $$\frac { { d }y }{ { d }x } =\frac { x-y }{ x+y } $$ It is homogeneous, thus let $y = vx$. From this, $\frac{\mathrm{d}y}{\mathrm{d}x} = x\frac{\mathrm{d}v}{\mathrm{d}x} + v$ Thus, $v'x + v = (1-v)/(1+v)$ thus, $\frac{2}{1-v} + \ln(v - 1) = \ln(x) + C$. Which by substituion, $\frac{2x}{x-y} + \ln(y - x) = 2\ln(x) + C$ But I cant get it any further",,['ordinary-differential-equations']
73,Bessel Function of the first kind,Bessel Function of the first kind,,"Could you please help me understand how to prove $$J_{(1/2)} (x) =  \sqrt{\frac2{\pi x}}\cdot \sin⁡ x$$ using, $$J_p (x)  = \sum_{(n=0)}^\infty \frac{(-1)^n}{(n! \Gamma(n+p+1) )} \left( \frac x 2 \right)^{2n+p}$$ Thank you","Could you please help me understand how to prove $$J_{(1/2)} (x) =  \sqrt{\frac2{\pi x}}\cdot \sin⁡ x$$ using, $$J_p (x)  = \sum_{(n=0)}^\infty \frac{(-1)^n}{(n! \Gamma(n+p+1) )} \left( \frac x 2 \right)^{2n+p}$$ Thank you",,"['ordinary-differential-equations', 'special-functions', 'bessel-functions']"
74,How to solve the differential equation $x \dfrac {dy}{dx} = \dfrac {y^{2}}{1 - y\log x}$?,How to solve the differential equation ?,x \dfrac {dy}{dx} = \dfrac {y^{2}}{1 - y\log x},How to solve the differential equation $x \dfrac {dy}{dx} = \dfrac {y^{2}}{1 - y\log x}$ ? Does not seem like an easy one or reducible to LDE. Any suggestions?,How to solve the differential equation $x \dfrac {dy}{dx} = \dfrac {y^{2}}{1 - y\log x}$ ? Does not seem like an easy one or reducible to LDE. Any suggestions?,,['calculus']
75,Solve the ODE $yy''=y'$,Solve the ODE,yy''=y',Solve the ODE $yy''=y'$ Can anyone check my solution? And what is the answer? Thanks. Attempt:  \begin{align*} y''=\frac{y'}{y} &\implies \frac{dy}{dx}=\ln|y|+c_1 \qquad\text{by integrating both side with respect to $x$}  \\ &\implies \frac{dy}{\ln|y|+c_1}=dx   \\ &\implies \int \frac{dy}{\ln|y|+c_1}=\int dx \qquad\text{(*)}  \\ &\implies y(\ln|y|+c_2)=x+c_3 \qquad \text{by using integration by parts}  \end{align*} Here is the steps for the integral $I:=\int \frac{dy}{\ln|y|+c_1}$ : Let $u=\ln|y|+c_1$ and $dv=dy$. Then $du=dy/y$ and $v=y$. So $$I=uv-\int v du=y(\ln|y|+c_1)-(y+c_4)=y(\ln|y|+\underbrace{c_1-1}_\text{$c_2$})-c_4$$ So the equation (*) becomes $$y(\ln|y|+c_2)=x+c \qquad \text{where $c=c_3+c_4$ }$$ I couldn't see what the wrong is.,Solve the ODE $yy''=y'$ Can anyone check my solution? And what is the answer? Thanks. Attempt:  \begin{align*} y''=\frac{y'}{y} &\implies \frac{dy}{dx}=\ln|y|+c_1 \qquad\text{by integrating both side with respect to $x$}  \\ &\implies \frac{dy}{\ln|y|+c_1}=dx   \\ &\implies \int \frac{dy}{\ln|y|+c_1}=\int dx \qquad\text{(*)}  \\ &\implies y(\ln|y|+c_2)=x+c_3 \qquad \text{by using integration by parts}  \end{align*} Here is the steps for the integral $I:=\int \frac{dy}{\ln|y|+c_1}$ : Let $u=\ln|y|+c_1$ and $dv=dy$. Then $du=dy/y$ and $v=y$. So $$I=uv-\int v du=y(\ln|y|+c_1)-(y+c_4)=y(\ln|y|+\underbrace{c_1-1}_\text{$c_2$})-c_4$$ So the equation (*) becomes $$y(\ln|y|+c_2)=x+c \qquad \text{where $c=c_3+c_4$ }$$ I couldn't see what the wrong is.,,['ordinary-differential-equations']
76,Solutions to the differential equation $x(x+1)yy' = xy + 1$,Solutions to the differential equation,x(x+1)yy' = xy + 1,"I am having trouble solving the equation $x(x+1)yy' - xy - 1 = 0$ I will list the steps I followed: (I'm sure I have made some huge mistake.) Divide by $x(x+1)$ $yy' - y/(x+1) - 1/x(x+1) = 0$ Integrate w.r.t dx. $∫(yy')dx - ∫(y/(x+1))dx - ∫(1/x(x+1))dx = 0$ Integrating by parts $∫f(x)g'(x)dx = f(x)g(x) - ∫f'(x)g(x)dx$ $f(x) = y$ $g(x) = y$ $∫yy'dx = y.y - ∫y'.y.dx$ $∫yy'dx = y²/2$ Integrating by parts $f(x) = y$ $g(x) = \ln(x+1)$ $∫(y/(x+1))dx = y.\ln(x+1) - ∫(dy/dx).\ln(x+1).dx$ $∫(y/(x+1))dx = y.\ln(x+1) - ∫\ln(x+1).dy$ $∫(y/(x+1))dx = y.\ln(x+1) - y.\ln(x+1)$ $∫(y/(x+1))dx = 0$ Integrating last term $∫(dx/x(x+1)) = ∫(dx/x) - ∫dx/(x+1)$ $∫(dx/x(x+1)) = \ln(x) - \ln(x+1)$ $∫(dx/x(x+1)) = \ln(x/(x+1))$ $y = √2(\ln(x/x+1))$ $\ln(x/x+1) < 1$ when $0<x<∞$ $y$ becomes imaginary. Judging by that I am sure I have made some huge mistakes, could anyone help me with this.","I am having trouble solving the equation $x(x+1)yy' - xy - 1 = 0$ I will list the steps I followed: (I'm sure I have made some huge mistake.) Divide by $x(x+1)$ $yy' - y/(x+1) - 1/x(x+1) = 0$ Integrate w.r.t dx. $∫(yy')dx - ∫(y/(x+1))dx - ∫(1/x(x+1))dx = 0$ Integrating by parts $∫f(x)g'(x)dx = f(x)g(x) - ∫f'(x)g(x)dx$ $f(x) = y$ $g(x) = y$ $∫yy'dx = y.y - ∫y'.y.dx$ $∫yy'dx = y²/2$ Integrating by parts $f(x) = y$ $g(x) = \ln(x+1)$ $∫(y/(x+1))dx = y.\ln(x+1) - ∫(dy/dx).\ln(x+1).dx$ $∫(y/(x+1))dx = y.\ln(x+1) - ∫\ln(x+1).dy$ $∫(y/(x+1))dx = y.\ln(x+1) - y.\ln(x+1)$ $∫(y/(x+1))dx = 0$ Integrating last term $∫(dx/x(x+1)) = ∫(dx/x) - ∫dx/(x+1)$ $∫(dx/x(x+1)) = \ln(x) - \ln(x+1)$ $∫(dx/x(x+1)) = \ln(x/(x+1))$ $y = √2(\ln(x/x+1))$ $\ln(x/x+1) < 1$ when $0<x<∞$ $y$ becomes imaginary. Judging by that I am sure I have made some huge mistakes, could anyone help me with this.",,['ordinary-differential-equations']
77,"Find an equation of the curve that passes through the point $(0, 6)$ and whose slope at $(x, y)$ is $\frac{x}{y}$. Book wasn't helpful.",Find an equation of the curve that passes through the point  and whose slope at  is . Book wasn't helpful.,"(0, 6) (x, y) \frac{x}{y}","I am using James Stewarts Early Transcendentals Calculus, and Section 9.3 (which is where this problem comes from) doesn't seem to have anything remotely similar to the problem I am facing. No examples, nothing I hate to just dump a homework problem off, but I really don't know where to begin. I thought ""find the tangent line"" at first, but it doesn't mention tangent anywhere, and I only thought of that because of previous math classes where I had to find the equation of a tangent line. Reprinted problem below: Find an equation of the curve that passes through the point $(0, 6)$ and whose slope at $(x, y)$ is $\frac{x}{y}$.","I am using James Stewarts Early Transcendentals Calculus, and Section 9.3 (which is where this problem comes from) doesn't seem to have anything remotely similar to the problem I am facing. No examples, nothing I hate to just dump a homework problem off, but I really don't know where to begin. I thought ""find the tangent line"" at first, but it doesn't mention tangent anywhere, and I only thought of that because of previous math classes where I had to find the equation of a tangent line. Reprinted problem below: Find an equation of the curve that passes through the point $(0, 6)$ and whose slope at $(x, y)$ is $\frac{x}{y}$.",,"['calculus', 'ordinary-differential-equations']"
78,How to solve this system of PDEs?,How to solve this system of PDEs?,,"The 1-D Euler’s equation for constant pressure can be written in terms of the two equations $$u_t + uu_x = 0, x\in\mathbb{R}, t> 0, u(x,0)=f(x)$$ $$\rho_t+\rho u_x+u\rho_x=0, x\in\mathbb{R}, t>0, \rho(x,0)=g(x)$$ Solving the first equation we get $u(x,t)=f(x-ut)$. We substitute this for the second equation and get $$\rho_t + \rho f'(x-ut) + u\rho_x=0$$ We choose a parametrization with respect to $s$ and get $$\frac{dt}{ds}=1, \frac{dx}{ds}=f(x-ut), \frac{d\rho}{ds}=f'(x-ut)$$ Thus $s=t$, but... How do we now integrate $f(x-ut)$ or $f'(x-ut)$ with respect to $t$ to finally solve for $\rho$?","The 1-D Euler’s equation for constant pressure can be written in terms of the two equations $$u_t + uu_x = 0, x\in\mathbb{R}, t> 0, u(x,0)=f(x)$$ $$\rho_t+\rho u_x+u\rho_x=0, x\in\mathbb{R}, t>0, \rho(x,0)=g(x)$$ Solving the first equation we get $u(x,t)=f(x-ut)$. We substitute this for the second equation and get $$\rho_t + \rho f'(x-ut) + u\rho_x=0$$ We choose a parametrization with respect to $s$ and get $$\frac{dt}{ds}=1, \frac{dx}{ds}=f(x-ut), \frac{d\rho}{ds}=f'(x-ut)$$ Thus $s=t$, but... How do we now integrate $f(x-ut)$ or $f'(x-ut)$ with respect to $t$ to finally solve for $\rho$?",,"['ordinary-differential-equations', 'partial-differential-equations', 'initial-value-problems']"
79,Inconsistency between analytical solution and numerical solution,Inconsistency between analytical solution and numerical solution,,"I have a differential equation as below: $$u^{'}=y$$ $$y^{'}=-k_1u-k_3u^3$$ Analytical solution of this equation would be: $$(u_0,y_0)=(\pm\chi_esech\sqrt{\kappa}\tau,\mp\chi_e\sqrt{\kappa}sech\sqrt{\kappa}\tau  tanh\sqrt{\kappa}\tau)$$ $$\kappa=-k_1\qquad\chi_e=\sqrt{-2k_1\over k_3}$$ Obviously, this solution is not periodic; however, when I solve this differential equation with ode45 in Matlab, the solution is periodic for a specific time interval. I don't know why the numerical solution and analytical one are different from each other. I would greatly appreciate it if you kindly give me an answer for this problem. you can find simulation results from 1 .","I have a differential equation as below: $$u^{'}=y$$ $$y^{'}=-k_1u-k_3u^3$$ Analytical solution of this equation would be: $$(u_0,y_0)=(\pm\chi_esech\sqrt{\kappa}\tau,\mp\chi_e\sqrt{\kappa}sech\sqrt{\kappa}\tau  tanh\sqrt{\kappa}\tau)$$ $$\kappa=-k_1\qquad\chi_e=\sqrt{-2k_1\over k_3}$$ Obviously, this solution is not periodic; however, when I solve this differential equation with ode45 in Matlab, the solution is periodic for a specific time interval. I don't know why the numerical solution and analytical one are different from each other. I would greatly appreciate it if you kindly give me an answer for this problem. you can find simulation results from 1 .",,['ordinary-differential-equations']
80,how to solve this non linear ode,how to solve this non linear ode,,"$$ y y'(x) +y(x)^2(\sqrt {x^3}+{7\over4}\sqrt {x^5}+{1\over2}\sqrt {x^7})-{1\over2x}=0 $$ How to solve this equation?? I searched text book , and I only found bessel, legandre. But they are not same shape with my equation. What should I do for solving this equation?","$$ y y'(x) +y(x)^2(\sqrt {x^3}+{7\over4}\sqrt {x^5}+{1\over2}\sqrt {x^7})-{1\over2x}=0 $$ How to solve this equation?? I searched text book , and I only found bessel, legandre. But they are not same shape with my equation. What should I do for solving this equation?",,"['ordinary-differential-equations', 'nonlinear-optimization', 'nonlinear-system']"
81,Finishing off this Sturm-Liouville BVP,Finishing off this Sturm-Liouville BVP,,"I'm looking at the Sturm-Liouville BVP $$\begin{cases} y'' + \lambda y = 0\\ y(0) + y'(0) = 0, y(1) = (0) \end{cases}.$$ I can do the problem but I can't finish it off at the very end (it's probably some easy algebra or trigonometric manipulation that I just can't see). For the case $\lambda > 0$ my working is as follows: Set $k^2 = \lambda$. After some working we find that our solution is $y = C_1 \cos kx + C_2 \sin kx$. Using $y(1) = 0$ we find that $C_1 \cos k + C_2 \sin k = 0$. Using $y(0) + y'(0) = 0$ we find that $0 = C_1 \cos k + C_2 \sin k$. Both of these give $0 = C_2(\sin k - k \cos k)$. So we have a non-trivial solution if $\sin k = k\cos k$, i.e. $\tan k = k$. My lecture notes are happy to take $k_n \approx (2n-1)\frac{\pi}{2}$, i.e. $\lambda_n \approx \frac{(2n-1)^2\pi^2}{4}$. I'm happy up to here, but I don't understand how we end up with eigenfunctions $y_n = \sin(k_n(1-x)$. Also, it seems that we only take $n$ to be non-negative, why is this the case?","I'm looking at the Sturm-Liouville BVP $$\begin{cases} y'' + \lambda y = 0\\ y(0) + y'(0) = 0, y(1) = (0) \end{cases}.$$ I can do the problem but I can't finish it off at the very end (it's probably some easy algebra or trigonometric manipulation that I just can't see). For the case $\lambda > 0$ my working is as follows: Set $k^2 = \lambda$. After some working we find that our solution is $y = C_1 \cos kx + C_2 \sin kx$. Using $y(1) = 0$ we find that $C_1 \cos k + C_2 \sin k = 0$. Using $y(0) + y'(0) = 0$ we find that $0 = C_1 \cos k + C_2 \sin k$. Both of these give $0 = C_2(\sin k - k \cos k)$. So we have a non-trivial solution if $\sin k = k\cos k$, i.e. $\tan k = k$. My lecture notes are happy to take $k_n \approx (2n-1)\frac{\pi}{2}$, i.e. $\lambda_n \approx \frac{(2n-1)^2\pi^2}{4}$. I'm happy up to here, but I don't understand how we end up with eigenfunctions $y_n = \sin(k_n(1-x)$. Also, it seems that we only take $n$ to be non-negative, why is this the case?",,"['ordinary-differential-equations', 'boundary-value-problem', 'eigenfunctions', 'sturm-liouville']"
82,Decouple a system of two second order differential equations,Decouple a system of two second order differential equations,,"I have a system of second-order differential equations that I want to decouple. they are, $\ddot{x} = \frac{\omega_1^2}{2} x + \omega_2 \dot{y}$ and $\ddot{y} = \frac{\omega_1^2}{2} y - \omega_2 \dot{x}$ I am thinking that I should use some transformation, but it just isn't clear in my head yet! Thanks!","I have a system of second-order differential equations that I want to decouple. they are, $\ddot{x} = \frac{\omega_1^2}{2} x + \omega_2 \dot{y}$ and $\ddot{y} = \frac{\omega_1^2}{2} y - \omega_2 \dot{x}$ I am thinking that I should use some transformation, but it just isn't clear in my head yet! Thanks!",,"['ordinary-differential-equations', 'systems-of-equations']"
83,Simplest solution for differential equation,Simplest solution for differential equation,,"Find the simplest solution: $y' + 2y = z' + 2z$ I think proper notation is not sure, y' means first derivate of y. ($\frac{dy}{dt}+ 2y = \frac{dz}{dt} + 2z$) $y(0)=1$ I got kind of confused, is $y=z=1$ a proper solution here? Or is disqualified because a constant is not reliant on time and something like $e^t$ is the simplest solution? You can choose z and y however you like.","Find the simplest solution: $y' + 2y = z' + 2z$ I think proper notation is not sure, y' means first derivate of y. ($\frac{dy}{dt}+ 2y = \frac{dz}{dt} + 2z$) $y(0)=1$ I got kind of confused, is $y=z=1$ a proper solution here? Or is disqualified because a constant is not reliant on time and something like $e^t$ is the simplest solution? You can choose z and y however you like.",,['ordinary-differential-equations']
84,Can a Submanifold Become Tangent to a Nowhere Tangent Vector Field,Can a Submanifold Become Tangent to a Nowhere Tangent Vector Field,,"$\newcommand{\R}{\mathbf R}$ Let $M=\R^2$ and $S=\{(0, t):-1<t < -1\}$ be a submanifold of $M$. Let $V$ be a vector field on $M$ which is nowhere tangent to $S$. Let $\theta$ be the flow of this vector field defined on the flow domain $D\subseteq \R\times M$. Let $t_0\in \R$ be such that the integral curve starting at the origin is defined on $[0, t_0]$. It is known that there is a neighborhood $O$ of the origin such that $\theta_{t_0}:O\to M$ maps $O$ diffeomorphically onto $\theta_{t_0}(O)$, the latter being open in $M$. Write $\mathbf p=\theta_{t_0}(\mathbf 0)$ and note that $S_{t_0}:=\theta_{t_0}(O\cap S)$ is a $1$-submanifold of $M$ passing through $\mathbf p$. Question. Is it possible that $V_{\mathbf p}$ is tangent to $S_{t_0}$?","$\newcommand{\R}{\mathbf R}$ Let $M=\R^2$ and $S=\{(0, t):-1<t < -1\}$ be a submanifold of $M$. Let $V$ be a vector field on $M$ which is nowhere tangent to $S$. Let $\theta$ be the flow of this vector field defined on the flow domain $D\subseteq \R\times M$. Let $t_0\in \R$ be such that the integral curve starting at the origin is defined on $[0, t_0]$. It is known that there is a neighborhood $O$ of the origin such that $\theta_{t_0}:O\to M$ maps $O$ diffeomorphically onto $\theta_{t_0}(O)$, the latter being open in $M$. Write $\mathbf p=\theta_{t_0}(\mathbf 0)$ and note that $S_{t_0}:=\theta_{t_0}(O\cap S)$ is a $1$-submanifold of $M$ passing through $\mathbf p$. Question. Is it possible that $V_{\mathbf p}$ is tangent to $S_{t_0}$?",,"['ordinary-differential-equations', 'differential-geometry', 'smooth-manifolds', 'vector-fields']"
85,Book reference for theory of differential equations (not Coddington's book),Book reference for theory of differential equations (not Coddington's book),,"I'm looking for references to study theory of ordinary differential equations. I'm looking for a similar book to Coddington's book, theory of ordinary differential equations but not this one, because this is a little old. I've already taken a course of (applied) differential equations but now I want to delve into the theory. I love Coddington's book but it is quite old. Also I like have more than one reference. Thank you in advance for your time.","I'm looking for references to study theory of ordinary differential equations. I'm looking for a similar book to Coddington's book, theory of ordinary differential equations but not this one, because this is a little old. I've already taken a course of (applied) differential equations but now I want to delve into the theory. I love Coddington's book but it is quite old. Also I like have more than one reference. Thank you in advance for your time.",,"['ordinary-differential-equations', 'reference-request', 'book-recommendation']"
86,Second order non linear differential equation: Central force question,Second order non linear differential equation: Central force question,,"The problem is as below: I have derived that the particle satisfies the motion equation $$ \frac{d^2u}{d \theta ^2 } + u = \frac{F(1/u)}{mh^2u^2} $$  by Newton's Law, $u= 1/r$ and $h = r^2 \frac{d \theta }{dt}$ is constant. Hence, by substitution, we obtain from initial conditions that  $h=av$, and that from the motion equation, $$ \frac{d^2 u }{ d \theta ^2 } = 6au^2$$  And this as far I could get. I could not really solve this equation - I tried multiplying by $u'$ both sides but nothing useful turns out. Also I could not find a place to use the information on radial velocity. (Probably used for solving the differential equation). Any ideas? Thank you!","The problem is as below: I have derived that the particle satisfies the motion equation $$ \frac{d^2u}{d \theta ^2 } + u = \frac{F(1/u)}{mh^2u^2} $$  by Newton's Law, $u= 1/r$ and $h = r^2 \frac{d \theta }{dt}$ is constant. Hence, by substitution, we obtain from initial conditions that  $h=av$, and that from the motion equation, $$ \frac{d^2 u }{ d \theta ^2 } = 6au^2$$  And this as far I could get. I could not really solve this equation - I tried multiplying by $u'$ both sides but nothing useful turns out. Also I could not find a place to use the information on radial velocity. (Probably used for solving the differential equation). Any ideas? Thank you!",,"['ordinary-differential-equations', 'physics', 'classical-mechanics']"
87,General Methods to Solve First-Order PDE,General Methods to Solve First-Order PDE,,"Question is as simple as: What are the different methods for solving a first-order PDE? I'm aware of nearly all forms of Method of Characteristics - Lagrange Method, Charpit's Method. I'm learning PDE, and was curious if there are any other methods since most of the links suggest (only) method of characteristics. If there are any, it'll be great if you can give an outline of it or a link where I can read about it, to learn! I read through this unanswered question , I'm not considering higher dimensions, I'm talking about simpler cases.","Question is as simple as: What are the different methods for solving a first-order PDE? I'm aware of nearly all forms of Method of Characteristics - Lagrange Method, Charpit's Method. I'm learning PDE, and was curious if there are any other methods since most of the links suggest (only) method of characteristics. If there are any, it'll be great if you can give an outline of it or a link where I can read about it, to learn! I read through this unanswered question , I'm not considering higher dimensions, I'm talking about simpler cases.",,"['ordinary-differential-equations', 'partial-differential-equations', 'characteristics']"
88,Solve $2x^{3}ydy+(1-y^{2})((xy)^{2}+y^{2}-1)dx=0$,Solve,2x^{3}ydy+(1-y^{2})((xy)^{2}+y^{2}-1)dx=0,The question is to solve this $$2x^{3}ydy+(1-y^{2})(x^2y^{2}+y^{2}-1)dx=0$$ What I tried was to bring this equation in linear differential equation form but failed. I have also tried out rewriting the given in such a way so that each term can be integrated easily but to no avail it isnt exact either since $\frac{dm}{dy}$ is $4y-4y^{3}$ and  $\frac{dn}{dx}$ is $6x^2y$ Please help me to solve this. Thanks in advance.,The question is to solve this $$2x^{3}ydy+(1-y^{2})(x^2y^{2}+y^{2}-1)dx=0$$ What I tried was to bring this equation in linear differential equation form but failed. I have also tried out rewriting the given in such a way so that each term can be integrated easily but to no avail it isnt exact either since $\frac{dm}{dy}$ is $4y-4y^{3}$ and  $\frac{dn}{dx}$ is $6x^2y$ Please help me to solve this. Thanks in advance.,,"['calculus', 'ordinary-differential-equations']"
89,What is the indicial equation of this differential equation?,What is the indicial equation of this differential equation?,,"The differential equation is $$x^2 y''+xy'+\left(x^2-\frac{1}{9}\right)y=0.$$ Using Forbenius' Theorem I am getting two indicial equations, which are: ((-1/9)r^(2))a0 =0, and ((-1/9)+2r+r^(2)+1)a1 =0. The question I'm looking at seems to imply that there is only one indicial equation though. Should I be getting two of them for this differential equation, or just one?","The differential equation is $$x^2 y''+xy'+\left(x^2-\frac{1}{9}\right)y=0.$$ Using Forbenius' Theorem I am getting two indicial equations, which are: ((-1/9)r^(2))a0 =0, and ((-1/9)+2r+r^(2)+1)a1 =0. The question I'm looking at seems to imply that there is only one indicial equation though. Should I be getting two of them for this differential equation, or just one?",,['ordinary-differential-equations']
90,What's the value of $\alpha$ satisfying $||f'||^2\ge \alpha||f||^2$? [duplicate],What's the value of  satisfying ? [duplicate],\alpha ||f'||^2\ge \alpha||f||^2,"This question already has answers here : Known proofs of Wirtinger's Inequality? (2 answers) Closed 8 years ago . I am reading a paper about numerical analysis of a certain method for solving operator equation. Let our Hilbert space be $L^2[0,1]$, we define the subspace $D\in L^2[0,1]$ by $$ D:=\{f\in C^2(0,1)\cap C^1[0,1] \ | f(0)=f(1)=0 \}. $$ There is a line saying ""It is well known that there exists an $\alpha>0$ such that the inequality $$ \|f'\|^2\ge \alpha\|f\|^2 $$ holds uniformly for every $f\in D$."" without giving a reference. Since I am quite new to the field, I assumed that it is a common knowledge that such $\alpha$ exists. Could anyone please provide me with a reference for this fact? My current knowledge includes basic functional analysis, mainly from Kreyszig's book, but very little of Sobolev space since I have just begun studying it. Any help would be very appreciated.","This question already has answers here : Known proofs of Wirtinger's Inequality? (2 answers) Closed 8 years ago . I am reading a paper about numerical analysis of a certain method for solving operator equation. Let our Hilbert space be $L^2[0,1]$, we define the subspace $D\in L^2[0,1]$ by $$ D:=\{f\in C^2(0,1)\cap C^1[0,1] \ | f(0)=f(1)=0 \}. $$ There is a line saying ""It is well known that there exists an $\alpha>0$ such that the inequality $$ \|f'\|^2\ge \alpha\|f\|^2 $$ holds uniformly for every $f\in D$."" without giving a reference. Since I am quite new to the field, I assumed that it is a common knowledge that such $\alpha$ exists. Could anyone please provide me with a reference for this fact? My current knowledge includes basic functional analysis, mainly from Kreyszig's book, but very little of Sobolev space since I have just begun studying it. Any help would be very appreciated.",,"['linear-algebra', 'functional-analysis', 'ordinary-differential-equations', 'hilbert-spaces', 'sobolev-spaces']"
91,"simple way to solve differential equation $y''=3\sqrt y, y|_{x=0}=1, y'|_{x=0}=2$",simple way to solve differential equation,"y''=3\sqrt y, y|_{x=0}=1, y'|_{x=0}=2","$$y''=3\sqrt y, y|_{x=0}=1, y'|_{x=0}=2$$ On finding the differential equation solution, let $y'=p$, so $ p'=3 \sqrt y $ $p'= \frac{dp}{dx}=\frac{dp}{dy} \frac{dy}{dx} =p \frac{dp}{dy}$, then $ p dp = 3 \sqrt y dy$. Solve $$\int p dp = \int 3 \sqrt y dy$$ I have $$\frac12p^2=2y^{\frac32} +\frac12c,\ p= \sqrt{4y^{\frac32}+c}$$ $$\int (4y^{\frac32}+c)^{-\frac12} dy = \int dx$$ It's impossible for me to do this integration. I guess there is a simpler approach to use the initial values, but apparently, it can't be used till now.","$$y''=3\sqrt y, y|_{x=0}=1, y'|_{x=0}=2$$ On finding the differential equation solution, let $y'=p$, so $ p'=3 \sqrt y $ $p'= \frac{dp}{dx}=\frac{dp}{dy} \frac{dy}{dx} =p \frac{dp}{dy}$, then $ p dp = 3 \sqrt y dy$. Solve $$\int p dp = \int 3 \sqrt y dy$$ I have $$\frac12p^2=2y^{\frac32} +\frac12c,\ p= \sqrt{4y^{\frac32}+c}$$ $$\int (4y^{\frac32}+c)^{-\frac12} dy = \int dx$$ It's impossible for me to do this integration. I guess there is a simpler approach to use the initial values, but apparently, it can't be used till now.",,"['integration', 'ordinary-differential-equations', 'indefinite-integrals']"
92,"$ y' + \tan(x)\,y = \cos^2(x)$, $y(0) = C$",","," y' + \tan(x)\,y = \cos^2(x) y(0) = C","Consider the initial value problem $$ y' + \tan(x)\,y = \cos^2(x),\quad y(0) = C$$ For what values of C does the solution remain bounded for all values of x? I tried solve this problem by considering this equation as a linear equation, and solve its homogeneous solution. But I don't know how to do it. Could you please help me to do this problem? Thank you.","Consider the initial value problem $$ y' + \tan(x)\,y = \cos^2(x),\quad y(0) = C$$ For what values of C does the solution remain bounded for all values of x? I tried solve this problem by considering this equation as a linear equation, and solve its homogeneous solution. But I don't know how to do it. Could you please help me to do this problem? Thank you.",,['ordinary-differential-equations']
93,Show that $\bf{x^*}$ is an equilibrium point.,Show that  is an equilibrium point.,\bf{x^*},Let $\dot{\bf{x}}=f(\bf{x})$ be a dynamical system with $f: \mathbb{R}^n\rightarrow \mathbb{R}^n$ being continuous and locally Lipschitz. Suppose that a particular solution trajectory satisfies $\lim_{t\rightarrow \infty}\bf{x}$ $(t)=\bf{x^*}$. Show that $\bf{x^*}$ is an equilibrium point. Solution so far Observe  $\lim_{t \rightarrow \infty} \bf{x}$ $(t)= \lim_{t \rightarrow \infty}f(\bf{x}$$(t-1))$. But $f$ is a contraction map and continuous. That is $ \bf{x^*} $$= f(\lim_{t \rightarrow \infty}\bf{x}$$(t-1))=f$$(\bf{x^*}$),Let $\dot{\bf{x}}=f(\bf{x})$ be a dynamical system with $f: \mathbb{R}^n\rightarrow \mathbb{R}^n$ being continuous and locally Lipschitz. Suppose that a particular solution trajectory satisfies $\lim_{t\rightarrow \infty}\bf{x}$ $(t)=\bf{x^*}$. Show that $\bf{x^*}$ is an equilibrium point. Solution so far Observe  $\lim_{t \rightarrow \infty} \bf{x}$ $(t)= \lim_{t \rightarrow \infty}f(\bf{x}$$(t-1))$. But $f$ is a contraction map and continuous. That is $ \bf{x^*} $$= f(\lim_{t \rightarrow \infty}\bf{x}$$(t-1))=f$$(\bf{x^*}$),,"['ordinary-differential-equations', 'dynamical-systems', 'lipschitz-functions']"
94,Minimum Radius of convergence of a power series in Differential equations,Minimum Radius of convergence of a power series in Differential equations,,"If $x_0$ is an ordinary point of the differential equation $(1)$, we can always ﬁnd two linearly independent solutions in the form of a power series centered at $x_0$. A power series solution converges at least on some interval deﬁned by $|x-x_0|< R$ , where $R$ is the distance from $x_0$ to the closest singular point. A solution of the form is said to be a solution about the ordinary point $x_0$. The distance $R$ in Theorem 6.2.1 is the minimum value or lower bound for the radius of convergence. This passage is taken from my Differential equation book and I just want to focus on the last small but confusing adjective MINIMUM VALUE used in the last quoted sentence , now it is true that if we want to center a given power series solution of a DE at a point $x_0$ we want $x_0$ to be an ordinary points , fine till here . And we dont want any ordinary points in our radius of convergence which explains $x-x_0 < R$ , BUT ! shouldn't $R$ be called the maximum radius of convergence ? How is it considered minimum ? These definitions just keep me confused","If $x_0$ is an ordinary point of the differential equation $(1)$, we can always ﬁnd two linearly independent solutions in the form of a power series centered at $x_0$. A power series solution converges at least on some interval deﬁned by $|x-x_0|< R$ , where $R$ is the distance from $x_0$ to the closest singular point. A solution of the form is said to be a solution about the ordinary point $x_0$. The distance $R$ in Theorem 6.2.1 is the minimum value or lower bound for the radius of convergence. This passage is taken from my Differential equation book and I just want to focus on the last small but confusing adjective MINIMUM VALUE used in the last quoted sentence , now it is true that if we want to center a given power series solution of a DE at a point $x_0$ we want $x_0$ to be an ordinary points , fine till here . And we dont want any ordinary points in our radius of convergence which explains $x-x_0 < R$ , BUT ! shouldn't $R$ be called the maximum radius of convergence ? How is it considered minimum ? These definitions just keep me confused",,"['calculus', 'ordinary-differential-equations']"
95,Laplace Question $f(t) = e^{-t} \sin(t)$,Laplace Question,f(t) = e^{-t} \sin(t),"I need help with this Laplace question. $$f(t) = e^{-t} \sin(t) $$ Answer should be $\dfrac{1}{s^2 + 2s + 2}$ What I'm currently doing is as follows: $u = \sin(t)\qquad$ $dv = e^{-(s+1)t}dt$ $du = \cos(t)dt\qquad$ $v = \dfrac{e^{-(s+1)t}}{-(s+1)}$ $\dfrac{-\sin(t) e^{-(s+1)t}}{-(s+1)}  - \int\dfrac{ e^{-(s+1)t}\cos(t)}{ -(s+1)} dt$ But even if I solved the integral, I wouldn't get this (which is what I should, see picture).","I need help with this Laplace question. Answer should be What I'm currently doing is as follows: But even if I solved the integral, I wouldn't get this (which is what I should, see picture).",f(t) = e^{-t} \sin(t)  \dfrac{1}{s^2 + 2s + 2} u = \sin(t)\qquad dv = e^{-(s+1)t}dt du = \cos(t)dt\qquad v = \dfrac{e^{-(s+1)t}}{-(s+1)} \dfrac{-\sin(t) e^{-(s+1)t}}{-(s+1)}  - \int\dfrac{ e^{-(s+1)t}\cos(t)}{ -(s+1)} dt,"['ordinary-differential-equations', 'laplace-transform', 'laplace-method', 'laplace-expansion']"
96,Does a monodromy matrix depend on the initial condition?,Does a monodromy matrix depend on the initial condition?,,"My Floquet theory is little bit rusty so I have a trivial question. Does a monodromy matrix depend on the initial conditions? On the surface it should since it is given as $B=X^{-1}(0)X(T)$ where $T$ is periodicity of my system  $\dot{x}=A(t)x$, $A(t+T)=A(t)\mbox{, } T>0$. However $B$ actually doesn't depend on the choice of the fundamental matrix do I am not sure that it depends on the initial condition?","My Floquet theory is little bit rusty so I have a trivial question. Does a monodromy matrix depend on the initial conditions? On the surface it should since it is given as $B=X^{-1}(0)X(T)$ where $T$ is periodicity of my system  $\dot{x}=A(t)x$, $A(t+T)=A(t)\mbox{, } T>0$. However $B$ actually doesn't depend on the choice of the fundamental matrix do I am not sure that it depends on the initial condition?",,['ordinary-differential-equations']
97,Are block diagrams valuable for non-LTI systems?,Are block diagrams valuable for non-LTI systems?,,"Do block diagrams provide any value in analyzing non-LTI systems? For LTI systems, block diagrams permit a powerful algebra for manipulation/reduction of a system. Are there classes of non-LTI systems (e.g. restricted classes of hybrid systems, restricted classes of non-linear systems, zero-order hold systems) where the structure of a block diagram also facilitates analysis? Edit: I don't think I was precise enough with my original question. What I really want to understand is whether block diagrams are useful for proving properties (e.g. stability) of non-LTI systems.","Do block diagrams provide any value in analyzing non-LTI systems? For LTI systems, block diagrams permit a powerful algebra for manipulation/reduction of a system. Are there classes of non-LTI systems (e.g. restricted classes of hybrid systems, restricted classes of non-linear systems, zero-order hold systems) where the structure of a block diagram also facilitates analysis? Edit: I don't think I was precise enough with my original question. What I really want to understand is whether block diagrams are useful for proving properties (e.g. stability) of non-LTI systems.",,"['ordinary-differential-equations', 'control-theory']"
98,Solving Ordinary Differential Equation $y(x^4-y^2)dx+x(x^4+y^2)dy=0$,Solving Ordinary Differential Equation,y(x^4-y^2)dx+x(x^4+y^2)dy=0,"The Ordinary Differential Equation $y(x^4-y^2)dx+x(x^4+y^2)dy=0$ can be solve using Integrating Factor by Inspection. Here is the solution using the inspection method: By expanding the equation, we have,$$x^4ydx-y^3dx+x^5dy+xy^2dy=0$$ $$x^4(ydx+xdy)+y^2(xdy-ydx)=0$$ Multiplying $\frac{1}{x^4}$,$$(ydx+xdy)+\frac{y^2}{x^2}\frac{xdy-ydx}{x^2}=0$$ $$d(xy)+\left(\frac{y}{x}\right)^2d\left(\frac{y}{x}\right)=0$$ $$xy+\frac{y^3}{3x^3}=C ,\ where\ C\ is\ a\ constant\ no.$$ $$3x^4y+y^3=Cx^3$$ I wanna know if it is also possible to use other method to solve this equation, because I tried it using the ""General Method for finding the Integrating Factor"" but my answer is not complete, and if it is, what are those possible method? If possible with your solution attached.","The Ordinary Differential Equation $y(x^4-y^2)dx+x(x^4+y^2)dy=0$ can be solve using Integrating Factor by Inspection. Here is the solution using the inspection method: By expanding the equation, we have,$$x^4ydx-y^3dx+x^5dy+xy^2dy=0$$ $$x^4(ydx+xdy)+y^2(xdy-ydx)=0$$ Multiplying $\frac{1}{x^4}$,$$(ydx+xdy)+\frac{y^2}{x^2}\frac{xdy-ydx}{x^2}=0$$ $$d(xy)+\left(\frac{y}{x}\right)^2d\left(\frac{y}{x}\right)=0$$ $$xy+\frac{y^3}{3x^3}=C ,\ where\ C\ is\ a\ constant\ no.$$ $$3x^4y+y^3=Cx^3$$ I wanna know if it is also possible to use other method to solve this equation, because I tried it using the ""General Method for finding the Integrating Factor"" but my answer is not complete, and if it is, what are those possible method? If possible with your solution attached.",,['ordinary-differential-equations']
99,Having trouble classifying and solving the ODE $2yy' = y^2 + t - 1$,Having trouble classifying and solving the ODE,2yy' = y^2 + t - 1,"I seek to classify the ODE $$2yy' = y^2 + t - 1$$ but I am having trouble rearranging the equation such that it can be written as a separable, linear, homogeneous, Bernoulli, or exact ODE. My first intuition was that it was exact because it can be written like $$-y^2 - t + 1 + 2yy' = 0$$ so $M = -y^2 - t + 1$ and $N = 2y$. However, $\frac{\partial M}{\partial y} \neq \frac{\partial N}{\partial t}$ in this case, so the criterion for exactness was not met. I can't seem to fit it into any other of the aforementioned categories, so I'm unsure where to go. Any help or intuition would be greatly appreciated.","I seek to classify the ODE $$2yy' = y^2 + t - 1$$ but I am having trouble rearranging the equation such that it can be written as a separable, linear, homogeneous, Bernoulli, or exact ODE. My first intuition was that it was exact because it can be written like $$-y^2 - t + 1 + 2yy' = 0$$ so $M = -y^2 - t + 1$ and $N = 2y$. However, $\frac{\partial M}{\partial y} \neq \frac{\partial N}{\partial t}$ in this case, so the criterion for exactness was not met. I can't seem to fit it into any other of the aforementioned categories, so I'm unsure where to go. Any help or intuition would be greatly appreciated.",,['ordinary-differential-equations']
