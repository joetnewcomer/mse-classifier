,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Why does $A^TAx = A^Tb$ have infinitely many solution algebraically when $A$ has dependent columns?,Why does  have infinitely many solution algebraically when  has dependent columns?,A^TAx = A^Tb A,"This is a problem from least square approximation, where we solve the equation $A^TAx = A^Tb$ when $Ax = b$ is unsolvable. The case I am dealing with is when A has dependent columns, i.e. A is an m by n matrix where the rank r is smaller than n. In this case $A^TA$ is a singular n by n matrix with dependent columns, the rank of which is also r (Rank($A^TA$)=Rank($A$)). Now in the book Introduction to Linear Algebra of the legendary Gilbert Strang, he says and I quote, when A is singular, $A^TA$ is also singular, and the equation $A^TAx = A^Tb$ had infinitely many solutions, the pseudoinverse gives us a way to choose a ""best solution"" $x^+=A^+b$. I understand why the equation has infinitely many solutions geometrically : Because what the equation asserts geometrically is to find the projection of b, denoted by p, in the column space of A, then solve the new equation $A\hat x$ = p. Because we can always project b onto the column space of A, whether it's singular or not, we know there must be a solution to the equation $A\hat x$=p and if there is a solution, there are infinitely many because A is singular. My question is how do we know that the equation have infinitely many solution algebraically , to make it clearer, I don't understand why the equation has at least one solution. I do understand that once it has at least one solution, it has infinitely many. Algebraically, I understands that $A^Tb$ will take us to $C(A^T)$, and it will take away the part of b that lies in $N(A)$. But what does it has to do with   $C(A^TA)$ ? My hypothesis is there is some formula regarding $C(A^TA)$ and $C(A^T)$ that I am not aware of. For example, if $C(A^TA) = C(A^T)$, then my problem is solved. Also, I found this How come least square can have many solutions? , I know what  $\hat x^TA\hat x$ in sums will looks like, but I don't know where it came from, but assuming that this is actually correct, I understand the arguments made in that thread. Any instruction will be appreciated.","This is a problem from least square approximation, where we solve the equation $A^TAx = A^Tb$ when $Ax = b$ is unsolvable. The case I am dealing with is when A has dependent columns, i.e. A is an m by n matrix where the rank r is smaller than n. In this case $A^TA$ is a singular n by n matrix with dependent columns, the rank of which is also r (Rank($A^TA$)=Rank($A$)). Now in the book Introduction to Linear Algebra of the legendary Gilbert Strang, he says and I quote, when A is singular, $A^TA$ is also singular, and the equation $A^TAx = A^Tb$ had infinitely many solutions, the pseudoinverse gives us a way to choose a ""best solution"" $x^+=A^+b$. I understand why the equation has infinitely many solutions geometrically : Because what the equation asserts geometrically is to find the projection of b, denoted by p, in the column space of A, then solve the new equation $A\hat x$ = p. Because we can always project b onto the column space of A, whether it's singular or not, we know there must be a solution to the equation $A\hat x$=p and if there is a solution, there are infinitely many because A is singular. My question is how do we know that the equation have infinitely many solution algebraically , to make it clearer, I don't understand why the equation has at least one solution. I do understand that once it has at least one solution, it has infinitely many. Algebraically, I understands that $A^Tb$ will take us to $C(A^T)$, and it will take away the part of b that lies in $N(A)$. But what does it has to do with   $C(A^TA)$ ? My hypothesis is there is some formula regarding $C(A^TA)$ and $C(A^T)$ that I am not aware of. For example, if $C(A^TA) = C(A^T)$, then my problem is solved. Also, I found this How come least square can have many solutions? , I know what  $\hat x^TA\hat x$ in sums will looks like, but I don't know where it came from, but assuming that this is actually correct, I understand the arguments made in that thread. Any instruction will be appreciated.",,"['linear-algebra', 'matrices', 'least-squares', 'linear-regression']"
1,Why define three elementary row operations? When one of them can be performed by the others?,Why define three elementary row operations? When one of them can be performed by the others?,,"I'm learning about linear algebra and in the course we've defined three ""elementary row operations"" $$(1) \text{ Switching any two rows}$$ $$(2) \text{ Non-Zero scaling of any row}$$ $$(3) \text{ Adding a multiple of one row to a different row}$$ However it seems all these operations can be performed by the two simpler operations: $$(1) \text{ Non-Zero scaling of any row}$$ $$(2) \text{ Adding any two rows}$$ I mean why even have switching rows as an operation in the first place when it can be performed by the addition and scaling of rows? It just seems redundant, why add an extra operation that can already be performed by the other operations? Edit: Because someone in the comments asked how the swapping of rows could be performed with just non-zero scaling and adding rows. If you wanted to swap, say row $p$ with row $q$ you would: $$(1) \text{ Add row } q \text{ to row } p$$ $$(2) \text{ Multiply row } p \text{ by } -1$$ $$(3) \text{ Add row } p \text{ to row } q$$ $$(4) \text{ Multiply row } q \text{ by } -1$$ $$(5) \text{ Add row } q \text{ to row } p$$ $$(6) \text{ Multiply row } p \text{ by } -1$$","I'm learning about linear algebra and in the course we've defined three ""elementary row operations"" $$(1) \text{ Switching any two rows}$$ $$(2) \text{ Non-Zero scaling of any row}$$ $$(3) \text{ Adding a multiple of one row to a different row}$$ However it seems all these operations can be performed by the two simpler operations: $$(1) \text{ Non-Zero scaling of any row}$$ $$(2) \text{ Adding any two rows}$$ I mean why even have switching rows as an operation in the first place when it can be performed by the addition and scaling of rows? It just seems redundant, why add an extra operation that can already be performed by the other operations? Edit: Because someone in the comments asked how the swapping of rows could be performed with just non-zero scaling and adding rows. If you wanted to swap, say row $p$ with row $q$ you would: $$(1) \text{ Add row } q \text{ to row } p$$ $$(2) \text{ Multiply row } p \text{ by } -1$$ $$(3) \text{ Add row } p \text{ to row } q$$ $$(4) \text{ Multiply row } q \text{ by } -1$$ $$(5) \text{ Add row } q \text{ to row } p$$ $$(6) \text{ Multiply row } p \text{ by } -1$$",,['linear-algebra']
2,Do two matrices close in spectral norm have close traces?,Do two matrices close in spectral norm have close traces?,,"Let $A_p$ and $B_p$ be a sequence of symmetric positive-definite $p\times p$ matrices such that $$\|A_p - B_p\|_2 \le 1/p.$$ Suppose further that the spectra of $A_p$ and $B_p$, for all $p$, is contained in a compact interval in $(0,\infty)$. And finally suppose that for all $p$, $\operatorname{tr}(B_p) = p.$ Intuitively, it seems that it should follow from this that $$\|pA_p/\operatorname{tr}(A_p) - B_p\|_2 \le const./p.$$ Does this in fact hold?","Let $A_p$ and $B_p$ be a sequence of symmetric positive-definite $p\times p$ matrices such that $$\|A_p - B_p\|_2 \le 1/p.$$ Suppose further that the spectra of $A_p$ and $B_p$, for all $p$, is contained in a compact interval in $(0,\infty)$. And finally suppose that for all $p$, $\operatorname{tr}(B_p) = p.$ Intuitively, it seems that it should follow from this that $$\|pA_p/\operatorname{tr}(A_p) - B_p\|_2 \le const./p.$$ Does this in fact hold?",,"['linear-algebra', 'spectral-theory']"
3,"Prove that $A,B$ have a common eigenvector",Prove that  have a common eigenvector,"A,B","Let $A,B$ be $2\times2$ real matrices satisfying $\det(A)=\det(B)=1$ and $$\text{tr}(A)>2 , \text{tr}(B)>2, \text{tr}(ABA^{-1}B^{-1})=2$$ Prove that A,B have a common eigenvector.","Let $A,B$ be $2\times2$ real matrices satisfying $\det(A)=\det(B)=1$ and $$\text{tr}(A)>2 , \text{tr}(B)>2, \text{tr}(ABA^{-1}B^{-1})=2$$ Prove that A,B have a common eigenvector.",,"['linear-algebra', 'matrices', 'soft-question', 'education']"
4,Isogonal operator is the product of an orthogonal map with a homothety,Isogonal operator is the product of an orthogonal map with a homothety,,"Exercise $31$ in chapter $8$ of Shilov's Linear Algebra book states that if $A\colon \Bbb R^n \to \Bbb R^n$ is an isogonal operator (that is, $\langle x,y\rangle = 0 \implies \langle Ax,Ay\rangle = 0$, usual inner product), then $A$ is the product of an orthogonal map with a homothety. He gives the following hint in the end: $A$ transforms the standard orthonormal basis $\{e_1,\cdots,e_n\}$ into an orthogonal basis $\{f_1'=\alpha_1f_1,\cdots,f_n'=\alpha_nf_n\}$, where each $f_i$ is an unit vector. Let $Q$ take $f_i$ into $e_i$ - so that $Q$ is orthogonal. Thus $QA$ is diagonal and isogonal. If $\alpha_i \neq \alpha_j$, construct a pair of orthogonal vectors which are carried into non-orthogonal vectors via $QA$. I have a few problems with that sketch. He says that a homothety is of the form $Tx = \lambda x$ for all $x$ but does not exclude the possibility of $\lambda = 0$. If $A = 0$ the problem is trivial, but I could not prove that $A \neq 0$ being isogonal implies $A$ injective. Meaning I don't really know that the $\{f_i\}$ are independent. In other words, I don't know why each $\alpha_i$ is non-zero. When he says that $\alpha_i \neq \alpha_j$ enables us to construct a ""bad"" pair, the obvious choice is to take the diagonals: clearly $\langle e_i+e_j,e_i-e_j\rangle=0$, but $\langle QA(e_i+e_j),QA(e_i-e_j)\rangle = \alpha_i^2-\alpha_j^2$ still could be zero if $\alpha_i =- \alpha_j$. Lastly, I was curious to see if this result is valid for a pseudo-euclidean product in $\Bbb R^n_\nu$, say. I think lightlike vectors would screw everything, but perhaps this discussion is better suited for another question. Can someone help me dot the i's and cross the t's here? Thanks.","Exercise $31$ in chapter $8$ of Shilov's Linear Algebra book states that if $A\colon \Bbb R^n \to \Bbb R^n$ is an isogonal operator (that is, $\langle x,y\rangle = 0 \implies \langle Ax,Ay\rangle = 0$, usual inner product), then $A$ is the product of an orthogonal map with a homothety. He gives the following hint in the end: $A$ transforms the standard orthonormal basis $\{e_1,\cdots,e_n\}$ into an orthogonal basis $\{f_1'=\alpha_1f_1,\cdots,f_n'=\alpha_nf_n\}$, where each $f_i$ is an unit vector. Let $Q$ take $f_i$ into $e_i$ - so that $Q$ is orthogonal. Thus $QA$ is diagonal and isogonal. If $\alpha_i \neq \alpha_j$, construct a pair of orthogonal vectors which are carried into non-orthogonal vectors via $QA$. I have a few problems with that sketch. He says that a homothety is of the form $Tx = \lambda x$ for all $x$ but does not exclude the possibility of $\lambda = 0$. If $A = 0$ the problem is trivial, but I could not prove that $A \neq 0$ being isogonal implies $A$ injective. Meaning I don't really know that the $\{f_i\}$ are independent. In other words, I don't know why each $\alpha_i$ is non-zero. When he says that $\alpha_i \neq \alpha_j$ enables us to construct a ""bad"" pair, the obvious choice is to take the diagonals: clearly $\langle e_i+e_j,e_i-e_j\rangle=0$, but $\langle QA(e_i+e_j),QA(e_i-e_j)\rangle = \alpha_i^2-\alpha_j^2$ still could be zero if $\alpha_i =- \alpha_j$. Lastly, I was curious to see if this result is valid for a pseudo-euclidean product in $\Bbb R^n_\nu$, say. I think lightlike vectors would screw everything, but perhaps this discussion is better suited for another question. Can someone help me dot the i's and cross the t's here? Thanks.",,"['linear-algebra', 'linear-transformations', 'inner-products', 'orthogonality']"
5,Sum of outer product of vectors in a basis,Sum of outer product of vectors in a basis,,"If $\{|u_1\rangle, ..., |u_n\rangle \}$ are an orthonormal basis for $\mathbb{C}_n$, then $$ \sum_{j=1}^{n} |u_j\rangle\langle u_j| = I_n$$ I can see that this is true in the standard computational basis, but I'm having trouble seeing it intuitively when generalized to any basis, nor can I prove it. Can anyone help?","If $\{|u_1\rangle, ..., |u_n\rangle \}$ are an orthonormal basis for $\mathbb{C}_n$, then $$ \sum_{j=1}^{n} |u_j\rangle\langle u_j| = I_n$$ I can see that this is true in the standard computational basis, but I'm having trouble seeing it intuitively when generalized to any basis, nor can I prove it. Can anyone help?",,"['linear-algebra', 'quantum-computation']"
6,Does a surjective linear map commute with the interior of a convex body?,Does a surjective linear map commute with the interior of a convex body?,,"Let $T:\Bbb R^n \to \Bbb R^m $ be a surjective linear map. Suppose $A\subset\Bbb R^n$ is convex, compact, $0\in \operatorname{int}(A)$, and centrally symmetric. Is it true that $T(\operatorname{int}(A))=\operatorname{int}(T(A))$?. Notes: $\operatorname{int}(S)$ means interior of $S$ in the topology of $\mathbb{R}^n$ or $\mathbb{R}^m$. Convexity is important: e.g., consider the image of sphere $S^2\subset \mathbb{R}^3$ under projection $A$ having nonempty interior is also important: consider the image of a disk $D^2\subset \mathbb{R}^3$ under the same projection.","Let $T:\Bbb R^n \to \Bbb R^m $ be a surjective linear map. Suppose $A\subset\Bbb R^n$ is convex, compact, $0\in \operatorname{int}(A)$, and centrally symmetric. Is it true that $T(\operatorname{int}(A))=\operatorname{int}(T(A))$?. Notes: $\operatorname{int}(S)$ means interior of $S$ in the topology of $\mathbb{R}^n$ or $\mathbb{R}^m$. Convexity is important: e.g., consider the image of sphere $S^2\subset \mathbb{R}^3$ under projection $A$ having nonempty interior is also important: consider the image of a disk $D^2\subset \mathbb{R}^3$ under the same projection.",,"['linear-algebra', 'functional-analysis', 'convex-analysis', 'normed-spaces']"
7,"$\operatorname{Hom}_k(k,V)$ is a vector space?",is a vector space?,"\operatorname{Hom}_k(k,V)","Is it true that a vector space is just the set of maps from the underlying field to the space itself.  I.e. if $V$ is a vector space of the field $k$ then $$ V\cong \operatorname{Hom}_k(k,V) $$ if so then this would make an intuitive understanding of the dual space $V^*$ somewhat trivial since $$ V^{**}\cong V\cong \operatorname{Hom}_k(k,V)\implies V^*=\operatorname{Hom}_k(V,k)  $$ If true, an explanation of why $V\cong \operatorname{Hom}_k(k,V)$ with a simple example or two would provide a lot of clarity for me since I could easily grasp the dual vector space idea from that point. Edit:  I actually had to read two proposed answers a couple times for the idea to sink in but I could only pick one answer.","Is it true that a vector space is just the set of maps from the underlying field to the space itself.  I.e. if $V$ is a vector space of the field $k$ then $$ V\cong \operatorname{Hom}_k(k,V) $$ if so then this would make an intuitive understanding of the dual space $V^*$ somewhat trivial since $$ V^{**}\cong V\cong \operatorname{Hom}_k(k,V)\implies V^*=\operatorname{Hom}_k(V,k)  $$ If true, an explanation of why $V\cong \operatorname{Hom}_k(k,V)$ with a simple example or two would provide a lot of clarity for me since I could easily grasp the dual vector space idea from that point. Edit:  I actually had to read two proposed answers a couple times for the idea to sink in but I could only pick one answer.",,"['linear-algebra', 'abstract-algebra', 'functional-analysis', 'vector-spaces', 'modules']"
8,How to Derive Softmax Function,How to Derive Softmax Function,,"Can someone explain step by step how to to find the derivative of this softmax loss function/equation. \begin{equation} L_i=-log(\frac{e^{f_{y_{i}}}}{\sum_j e^{f_j}}) = -f_{y_i} + log(\sum_j e^{f_j}) \end{equation} where: \begin{equation} f = w_j*x_i \end{equation} let: \begin{equation} p = \frac{e^{f_{y_{i}}}}{\sum_j e^{f_j}} \end{equation} The code shows that the derivative of $L_i$ when $j = y_i$ is: \begin{equation} (p-1) * x_i \end{equation} and when $j \neq y_i$ the derivative is: \begin{equation} p * x_i \end{equation} It seems related to this this post , where the OP says the derivative of: \begin{equation} p_j = \frac{e^{o_j}}{\sum_k e^{o_k}} \end{equation} is: \begin{equation} \frac{\partial p_j}{\partial o_i} = p_i(1 - p_i),\quad i = j \end{equation} But I couldn't figure it out. I'm used to doing derivatives wrt to variables, but not familiar with doing derivatives wrt to indxes.","Can someone explain step by step how to to find the derivative of this softmax loss function/equation. \begin{equation} L_i=-log(\frac{e^{f_{y_{i}}}}{\sum_j e^{f_j}}) = -f_{y_i} + log(\sum_j e^{f_j}) \end{equation} where: \begin{equation} f = w_j*x_i \end{equation} let: \begin{equation} p = \frac{e^{f_{y_{i}}}}{\sum_j e^{f_j}} \end{equation} The code shows that the derivative of $L_i$ when $j = y_i$ is: \begin{equation} (p-1) * x_i \end{equation} and when $j \neq y_i$ the derivative is: \begin{equation} p * x_i \end{equation} It seems related to this this post , where the OP says the derivative of: \begin{equation} p_j = \frac{e^{o_j}}{\sum_k e^{o_k}} \end{equation} is: \begin{equation} \frac{\partial p_j}{\partial o_i} = p_i(1 - p_i),\quad i = j \end{equation} But I couldn't figure it out. I'm used to doing derivatives wrt to variables, but not familiar with doing derivatives wrt to indxes.",,"['linear-algebra', 'derivatives', 'machine-learning']"
9,Finding a basis for the set of polynomials where f(1)=f(-1)=0,Finding a basis for the set of polynomials where f(1)=f(-1)=0,,"I have the vector space $V$ above that belongs to $\mathbb{F}$, and $V$ is the group of all polynomials that are of degree $3$. $W= \{ p \in V | p(1)=p(-1)=0\}$ 1.) Prove that W is a subspace of $V$. A.) Ok let's prove that the space isn't empty. Let's take $p(x)= 0$, and this satisfies the condition of $p(1)=p(-1)=0$ B.) Now that the sum of two vectors in $W$ belong in $V$ Let's take two polynomials that satisfy $W$ and show that their sum belongs in $W$. This is pretty trivial since you can't go increase your degree from addition. Let's take $p_1(x)=x_1^3-x_1^2-x_1+1$ and $p_2(x)=x_2^3-x_2^2-x_2+1$. Their sum belongs in $V$. Let's check multiplication by a scalar $\lambda \in \mathbb{R} $(also pretty trivial): $p(\lambda x)=\lambda x^3-\lambda x^2-\lambda x+1 = \lambda p(x)$ Now onto the basis: My thought process is that I might need to find a basis that contains at least 4 components (since $\mathbb{R}_3[x]$ contains 4 components), which I don't think is correct. So far I have two components: B= $\{ (x^2-1),(x(x^2-1))\} $ This is where I'm stuck and can't find anymore polynomials that satisfy the requirement. TL;DR 1.) Is my thought process for proving that $W$ is a subspace of $V$ correct? 2.) How can I find a basis for $W$? Is there a simpler way using a system of equations rather than brute force trial and error?","I have the vector space $V$ above that belongs to $\mathbb{F}$, and $V$ is the group of all polynomials that are of degree $3$. $W= \{ p \in V | p(1)=p(-1)=0\}$ 1.) Prove that W is a subspace of $V$. A.) Ok let's prove that the space isn't empty. Let's take $p(x)= 0$, and this satisfies the condition of $p(1)=p(-1)=0$ B.) Now that the sum of two vectors in $W$ belong in $V$ Let's take two polynomials that satisfy $W$ and show that their sum belongs in $W$. This is pretty trivial since you can't go increase your degree from addition. Let's take $p_1(x)=x_1^3-x_1^2-x_1+1$ and $p_2(x)=x_2^3-x_2^2-x_2+1$. Their sum belongs in $V$. Let's check multiplication by a scalar $\lambda \in \mathbb{R} $(also pretty trivial): $p(\lambda x)=\lambda x^3-\lambda x^2-\lambda x+1 = \lambda p(x)$ Now onto the basis: My thought process is that I might need to find a basis that contains at least 4 components (since $\mathbb{R}_3[x]$ contains 4 components), which I don't think is correct. So far I have two components: B= $\{ (x^2-1),(x(x^2-1))\} $ This is where I'm stuck and can't find anymore polynomials that satisfy the requirement. TL;DR 1.) Is my thought process for proving that $W$ is a subspace of $V$ correct? 2.) How can I find a basis for $W$? Is there a simpler way using a system of equations rather than brute force trial and error?",,['linear-algebra']
10,possible eigenvalues of $A$,possible eigenvalues of,A,"Let $A$ be an $n\times n$ matrix such that $A^2=A^t$. Then prove that possible real eigenvalues of $A$ are $0,1$. Let $\lambda$ be an eigenvalue of $A$ then $\lambda^2$ is eigenvalue of $A^2$. As $A^2=A^t$, $\lambda^2$ is eigenvalue of $A^t$.. Eigenvalue of $A$ are same as eigenvalue of $A^t$.. So, real eigenvalues of $A^t$  are $\{\lambda_1,\cdots,\lambda_r,\lambda_1^2,\cdots,\lambda_r^2\}$. As number of real eigenvalues are fixed we must have $\lambda_i^2=\lambda_i$ or $\lambda_j$ or $\lambda_j^2$.. $\lambda_i^2=\lambda_j^2$ and $\lambda_i\neq \lambda_j$ implies $\lambda_i=-\lambda_j$ i do not see any contradiction here.. $\lambda_i^2=\lambda_j$ i do not know what to conclude... $\lambda_i^2=\lambda_i$ then $\lambda_i=0$ or $1$.. which is what i want.. Help me to clear this...","Let $A$ be an $n\times n$ matrix such that $A^2=A^t$. Then prove that possible real eigenvalues of $A$ are $0,1$. Let $\lambda$ be an eigenvalue of $A$ then $\lambda^2$ is eigenvalue of $A^2$. As $A^2=A^t$, $\lambda^2$ is eigenvalue of $A^t$.. Eigenvalue of $A$ are same as eigenvalue of $A^t$.. So, real eigenvalues of $A^t$  are $\{\lambda_1,\cdots,\lambda_r,\lambda_1^2,\cdots,\lambda_r^2\}$. As number of real eigenvalues are fixed we must have $\lambda_i^2=\lambda_i$ or $\lambda_j$ or $\lambda_j^2$.. $\lambda_i^2=\lambda_j^2$ and $\lambda_i\neq \lambda_j$ implies $\lambda_i=-\lambda_j$ i do not see any contradiction here.. $\lambda_i^2=\lambda_j$ i do not know what to conclude... $\lambda_i^2=\lambda_i$ then $\lambda_i=0$ or $1$.. which is what i want.. Help me to clear this...",,['linear-algebra']
11,Best Fit Line with 3d Points,Best Fit Line with 3d Points,,"Okay, I need to develop an alorithm to take a collection of 3d points with x,y,and z components and find a line of best fit.  I found a commonly referenced item from Geometric Tools but there doesn't seem to be a lot of information to get someone not already familiar with the method going.  Does anyone know of an alorithm to accomplish this?  Or can someone help me work the Geometric Tools approach out through an example so that I can figure it out from there?  Any help would be greatly appreciated.","Okay, I need to develop an alorithm to take a collection of 3d points with x,y,and z components and find a line of best fit.  I found a commonly referenced item from Geometric Tools but there doesn't seem to be a lot of information to get someone not already familiar with the method going.  Does anyone know of an alorithm to accomplish this?  Or can someone help me work the Geometric Tools approach out through an example so that I can figure it out from there?  Any help would be greatly appreciated.",,"['linear-algebra', 'geometry', 'differential-geometry', 'linear-regression']"
12,Prove that p divides to algebraic multiplicity of the eigenvalue,Prove that p divides to algebraic multiplicity of the eigenvalue,,"I need help in the following exercise of a qualifying exam: Let $A$ be a matrix of size $m$ by $m$ over the finite field $\mathbb{F}_p$ such that $\operatorname{trace}\left(A^n\right)=0$ for all $n$. If $\lambda$ is a nonzero eigenvalue of $A$, prove that the algebraic multiplicity of $\lambda$ is divisible by $p$. Thank you by some hints.","I need help in the following exercise of a qualifying exam: Let $A$ be a matrix of size $m$ by $m$ over the finite field $\mathbb{F}_p$ such that $\operatorname{trace}\left(A^n\right)=0$ for all $n$. If $\lambda$ is a nonzero eigenvalue of $A$, prove that the algebraic multiplicity of $\lambda$ is divisible by $p$. Thank you by some hints.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'determinant', 'positive-characteristic']"
13,A is a product of two self-adjoint matrices if and only if A is similar to adjoint of A?,A is a product of two self-adjoint matrices if and only if A is similar to adjoint of A?,,"How can we prove that $A$ is a product of two self-adjoint matrices $X, Y$ if and only if $A$ is similar to $A^\ast$? I'm thinking about proving it but no useful techniques come to my mind. Thanks!","How can we prove that $A$ is a product of two self-adjoint matrices $X, Y$ if and only if $A$ is similar to $A^\ast$? I'm thinking about proving it but no useful techniques come to my mind. Thanks!",,"['linear-algebra', 'matrices']"
14,Is it acceptable to use reduced row echelon to show basis?,Is it acceptable to use reduced row echelon to show basis?,,"I am asked to show that { a , b , c } forms a basis for $\Bbb R^3$. I'm just wondering if it is acceptable to use reduced row echelon to show it since it is not shown that way in the marking scheme? $$a= \begin{pmatrix}         2\\         -1\\         1 \\         \end{pmatrix}b= \begin{pmatrix}         1\\         1\\         1 \\         \end{pmatrix}  c= \begin{pmatrix}         0\\         1\\         -1 \\         \end{pmatrix}$$ Here's how I did it: For the vectors to form a basis, they must be linearly independent such that $$a_1\begin{pmatrix}         2\\         -1\\         1 \\         \end{pmatrix}+a_2\begin{pmatrix}         1\\         1\\         1 \\         \end{pmatrix}+a_3\begin{pmatrix}         0\\         1\\         -1 \\         \end{pmatrix}=\begin{pmatrix}         0\\         0\\         0\\         \end{pmatrix}$$ So, I did reduced row echelon and I got.. $$\begin{bmatrix} 2 & 1 & 0 \\-1 & 1 & 1\\  1 & 1 & -1 \\      \end{bmatrix} \rightarrow \begin{bmatrix}         1 & 0 & 0 \\         0 & 1 & 0\\         0 & 0 & 1 \\         \end{bmatrix}$$ After that, I solve for $a_1, a_2$ and $a_3$ $$\begin{bmatrix}         1 & 0 & 0 \\         0 & 1 & 0\\         0 & 0 & 1 \\         \end{bmatrix}\cdot \begin{Bmatrix}         a_1\\         a_2\\         a_3 \\         \end{Bmatrix}=0$$ and I got $a_1=a_2=a_3=0$. Then, I conclude that { a , b , c } forms a basis.","I am asked to show that { a , b , c } forms a basis for $\Bbb R^3$. I'm just wondering if it is acceptable to use reduced row echelon to show it since it is not shown that way in the marking scheme? $$a= \begin{pmatrix}         2\\         -1\\         1 \\         \end{pmatrix}b= \begin{pmatrix}         1\\         1\\         1 \\         \end{pmatrix}  c= \begin{pmatrix}         0\\         1\\         -1 \\         \end{pmatrix}$$ Here's how I did it: For the vectors to form a basis, they must be linearly independent such that $$a_1\begin{pmatrix}         2\\         -1\\         1 \\         \end{pmatrix}+a_2\begin{pmatrix}         1\\         1\\         1 \\         \end{pmatrix}+a_3\begin{pmatrix}         0\\         1\\         -1 \\         \end{pmatrix}=\begin{pmatrix}         0\\         0\\         0\\         \end{pmatrix}$$ So, I did reduced row echelon and I got.. $$\begin{bmatrix} 2 & 1 & 0 \\-1 & 1 & 1\\  1 & 1 & -1 \\      \end{bmatrix} \rightarrow \begin{bmatrix}         1 & 0 & 0 \\         0 & 1 & 0\\         0 & 0 & 1 \\         \end{bmatrix}$$ After that, I solve for $a_1, a_2$ and $a_3$ $$\begin{bmatrix}         1 & 0 & 0 \\         0 & 1 & 0\\         0 & 0 & 1 \\         \end{bmatrix}\cdot \begin{Bmatrix}         a_1\\         a_2\\         a_3 \\         \end{Bmatrix}=0$$ and I got $a_1=a_2=a_3=0$. Then, I conclude that { a , b , c } forms a basis.",,"['linear-algebra', 'self-learning']"
15,Derivations of important algebras?,Derivations of important algebras?,,"After knowing the importance of studying derivations of an algebras( Why we wonder to know all derivations of an algebra? ), this problem naturally raised ""what is the space of all derivations of important algebras like algebra of matrices, cliford algebra, exterior algebra and C* algebras?"" has anybody studied on this problem? In graded cases we have the notion of super and graded derivations. does this kinds of derivations for famous algebras obtained? If so, I want to be familiar with references with geometrical looking. any hint is appreciated.","After knowing the importance of studying derivations of an algebras( Why we wonder to know all derivations of an algebra? ), this problem naturally raised ""what is the space of all derivations of important algebras like algebra of matrices, cliford algebra, exterior algebra and C* algebras?"" has anybody studied on this problem? In graded cases we have the notion of super and graded derivations. does this kinds of derivations for famous algebras obtained? If so, I want to be familiar with references with geometrical looking. any hint is appreciated.",,"['linear-algebra', 'differential-geometry']"
16,Adjoint and Adjugate are same or different?,Adjoint and Adjugate are same or different?,,"The notions of adjoint and adjugate , which I saw, are as follows: (1) Let $T:V\rightarrow W$ be a linear map. Then there is a corresponding linear map between the duals of these spaces: $T^*:W^*\rightarrow V^*$, defined as follows: for every linear map $f$ from $W$ to $k$ (the field), there is a linear $f^*$ from $V$ to $k$ given by $f\circ T$, hence we have a map $f\mapsto f^*$ from $W^*$ to $V^*$. The map $T^*$ is called the adjoint of $T$. (2) The notion of adjugate of a matrix is defined as follows: given a square matrix $A$, the transpose of the matrix of cofactors of $A$ is called the adjugate of $A$. Question 1: Are these notions of adjoint and adjugate related? Question 2: In the formula, $A$.adj$(A)=det(A).I$, the term $adj(A)$ should be understood as adjoint or adjugate ?","The notions of adjoint and adjugate , which I saw, are as follows: (1) Let $T:V\rightarrow W$ be a linear map. Then there is a corresponding linear map between the duals of these spaces: $T^*:W^*\rightarrow V^*$, defined as follows: for every linear map $f$ from $W$ to $k$ (the field), there is a linear $f^*$ from $V$ to $k$ given by $f\circ T$, hence we have a map $f\mapsto f^*$ from $W^*$ to $V^*$. The map $T^*$ is called the adjoint of $T$. (2) The notion of adjugate of a matrix is defined as follows: given a square matrix $A$, the transpose of the matrix of cofactors of $A$ is called the adjugate of $A$. Question 1: Are these notions of adjoint and adjugate related? Question 2: In the formula, $A$.adj$(A)=det(A).I$, the term $adj(A)$ should be understood as adjoint or adjugate ?",,['linear-algebra']
17,Isolated Eigenvalue,Isolated Eigenvalue,,"What does it mean that an eigenvalue is ""isolated""? My intuitive understanding says it is when one can find an open ball around it such that there is no other eigenvalue in that open ball. However, I am reading a book (""Perturbation of Spectra in Hilbert Space"" by Friedrichs) that says: ...we assume that the undisturbed operator $H_0$ is Hermitian, possesses a spectral resolution, and has a single eigenvalue $\omega_0$ with an eigenvector $X_0$ [such that $X_0\neq 0$]. We assume this eigenvalue to be isolated, so that the equation $$ (H_0-\omega_0)X=\Psi $$ has a solution $X$ whenever the given right member $\Psi$ is orthogonal to $X_0$. My question is: how does it follow from $\omega_0$ being isolated that the equation has a solution only if $<X_0,\Psi>=0$? No matter if $\omega_0$ is isolated or not, we have that $$ <X_0, (H_0-\omega_0)X> = <(H_0-\omega_0)X_0, X> = <0,X> = 0$$so that if the equation $$ (H_0-\omega_0)X=\Psi $$ is satisfied by $X$ then $<X_0, \Psi>=0$. What does the isolatedness of $\omega_0$ have to do with it?","What does it mean that an eigenvalue is ""isolated""? My intuitive understanding says it is when one can find an open ball around it such that there is no other eigenvalue in that open ball. However, I am reading a book (""Perturbation of Spectra in Hilbert Space"" by Friedrichs) that says: ...we assume that the undisturbed operator $H_0$ is Hermitian, possesses a spectral resolution, and has a single eigenvalue $\omega_0$ with an eigenvector $X_0$ [such that $X_0\neq 0$]. We assume this eigenvalue to be isolated, so that the equation $$ (H_0-\omega_0)X=\Psi $$ has a solution $X$ whenever the given right member $\Psi$ is orthogonal to $X_0$. My question is: how does it follow from $\omega_0$ being isolated that the equation has a solution only if $<X_0,\Psi>=0$? No matter if $\omega_0$ is isolated or not, we have that $$ <X_0, (H_0-\omega_0)X> = <(H_0-\omega_0)X_0, X> = <0,X> = 0$$so that if the equation $$ (H_0-\omega_0)X=\Psi $$ is satisfied by $X$ then $<X_0, \Psi>=0$. What does the isolatedness of $\omega_0$ have to do with it?",,"['linear-algebra', 'functional-analysis', 'hilbert-spaces']"
18,Recognize or interpret this involution : $\frac{\prod_{x\neq i}(1-a_xa_j)}{\prod_{y\neq j}(a_j-a_y)}$,Recognize or interpret this involution :,\frac{\prod_{x\neq i}(1-a_xa_j)}{\prod_{y\neq j}(a_j-a_y)},"Let $a_1,a_2,\ldots ,a_n$ be distinct numbers. For indices $i,j$ between $1$ and $n$, put $d_i=\prod_{j\neq i}(a_i-a_j)$ and $b_{ij}=\frac{\prod_{x\neq i}(1-a_xa_j)}{d_j}$. Let $B$ be the $n\times n$ matrix $B=(b_{ij})$. Does anyone know how to show that $B$ is an involution ? A proof adding some interpretation to the computations would be better, of course. Perhpas this matrix has a name already ?","Let $a_1,a_2,\ldots ,a_n$ be distinct numbers. For indices $i,j$ between $1$ and $n$, put $d_i=\prod_{j\neq i}(a_i-a_j)$ and $b_{ij}=\frac{\prod_{x\neq i}(1-a_xa_j)}{d_j}$. Let $B$ be the $n\times n$ matrix $B=(b_{ij})$. Does anyone know how to show that $B$ is an involution ? A proof adding some interpretation to the computations would be better, of course. Perhpas this matrix has a name already ?",,"['linear-algebra', 'matrices']"
19,Question on proof of Lefschetz Fixed Point Theorem (from Hatcher Theorem 2C.3),Question on proof of Lefschetz Fixed Point Theorem (from Hatcher Theorem 2C.3),,"In Hatcher's statement of the Lefschetz Fixed Point Theorem ( 2C.3 ), he has a hypothesis that the space $X$ in question must be a retract of a finite simplicial complex. The first part of the proof qualifies this, and ultimately states: Suppose $r:K \rightarrow X$ is a retraction of the finite simplicial complex K onto   X. For a map $f:X \rightarrow X$, the composition $fr:K \rightarrow X \subset K$ then has exactly the same fixed points as $f$. Since $r_*:H_n(K) \rightarrow H_n(X)$ is projection onto a direct summand, we clearly have $\text{tr}(f_* r_*) = \text{tr} (f_*)$, so the Lefschetz numbers are equal, i.e. $\tau(f_*r_*) = \tau(f_*)$. Quesions: How is $r_*$ a projection onto a direct summand? How does it ""clearly"" follow from this that the trace of the composition is the same as the trace of $f_*$ itself? Approach: I think I see that $r_*$ is a projection onto a direct summand by looking at the exact sequence $$0 \rightarrow H_n(X) \xrightarrow{i_*} H_n(K) \rightarrow H_n(K, X) \rightarrow 0.$$ Then since $r_* \circ i_*$ is the identity on $H_n(X)$, the exact sequence splits, and so the splitting lemma yields $H_n(K) = H_n(X) \oplus H_n(K, X)$, and the function $r_*$ acts like projection onto the summand $H_n(X)$. From here, I have no ideas on how to see that $\text{tr}(f_* r_*) = \text{tr} (f_*)$. What does this follow from? It is not clear to me.","In Hatcher's statement of the Lefschetz Fixed Point Theorem ( 2C.3 ), he has a hypothesis that the space $X$ in question must be a retract of a finite simplicial complex. The first part of the proof qualifies this, and ultimately states: Suppose $r:K \rightarrow X$ is a retraction of the finite simplicial complex K onto   X. For a map $f:X \rightarrow X$, the composition $fr:K \rightarrow X \subset K$ then has exactly the same fixed points as $f$. Since $r_*:H_n(K) \rightarrow H_n(X)$ is projection onto a direct summand, we clearly have $\text{tr}(f_* r_*) = \text{tr} (f_*)$, so the Lefschetz numbers are equal, i.e. $\tau(f_*r_*) = \tau(f_*)$. Quesions: How is $r_*$ a projection onto a direct summand? How does it ""clearly"" follow from this that the trace of the composition is the same as the trace of $f_*$ itself? Approach: I think I see that $r_*$ is a projection onto a direct summand by looking at the exact sequence $$0 \rightarrow H_n(X) \xrightarrow{i_*} H_n(K) \rightarrow H_n(K, X) \rightarrow 0.$$ Then since $r_* \circ i_*$ is the identity on $H_n(X)$, the exact sequence splits, and so the splitting lemma yields $H_n(K) = H_n(X) \oplus H_n(K, X)$, and the function $r_*$ acts like projection onto the summand $H_n(X)$. From here, I have no ideas on how to see that $\text{tr}(f_* r_*) = \text{tr} (f_*)$. What does this follow from? It is not clear to me.",,"['linear-algebra', 'algebraic-topology', 'homology-cohomology', 'exact-sequence']"
20,How can I visualize the nuclear norm ball?,How can I visualize the nuclear norm ball?,,"I want to see what the unit nuclear norm ball looks like. So I think of matrices whose singular values add up to $1$. For simplicity, let's talk about symmetric, $2\times 2$ matrices (so that I can limit myself to $3$ dimensions). These matrices can be thought of as points in a $3$-dimensional space, and the coordinate values tell us about the entries in the matrix. But what shape will the matrices that have nuclear norm $1$ form? I have seen figures showing it to be like a solid cylinder, but I can't see why.","I want to see what the unit nuclear norm ball looks like. So I think of matrices whose singular values add up to $1$. For simplicity, let's talk about symmetric, $2\times 2$ matrices (so that I can limit myself to $3$ dimensions). These matrices can be thought of as points in a $3$-dimensional space, and the coordinate values tell us about the entries in the matrix. But what shape will the matrices that have nuclear norm $1$ form? I have seen figures showing it to be like a solid cylinder, but I can't see why.",,"['linear-algebra', 'matrices', 'functional-analysis', 'symmetric-matrices', 'nuclear-norm']"
21,$GL_n(\mathbb{F})$ contains a copy of $\mathbb{F}^{n-1}$,contains a copy of,GL_n(\mathbb{F}) \mathbb{F}^{n-1},"It is a fact of matrix multiplication that $$\left( \begin{matrix} 1 & a & b \\&1&\\&&1 \end{matrix} \right) \left( \begin{matrix} 1 & a' & b'\\&1&\\&&1 \end{matrix} \right) = \left( \begin{matrix} 1 & a +a'&b+b' \\&1&\\&&1 \end{matrix} \right). $$ Therefore $GL_n(\mathbb{F})$ contains a copy of the abelian group $\mathbb{F}^{n-1}$. Can we show that it does or does not contain a copy of $\mathbb{F}^n$, or any higher power? Edit: Derek points out that, for even $n$, $\frac {n^2}{4}$ and for odd $n$, $\frac{n^2-1}{4}$ are possible. Can we show this the is highest value which is possible for all fields (while of course for specific fields higher may be possible)?","It is a fact of matrix multiplication that $$\left( \begin{matrix} 1 & a & b \\&1&\\&&1 \end{matrix} \right) \left( \begin{matrix} 1 & a' & b'\\&1&\\&&1 \end{matrix} \right) = \left( \begin{matrix} 1 & a +a'&b+b' \\&1&\\&&1 \end{matrix} \right). $$ Therefore $GL_n(\mathbb{F})$ contains a copy of the abelian group $\mathbb{F}^{n-1}$. Can we show that it does or does not contain a copy of $\mathbb{F}^n$, or any higher power? Edit: Derek points out that, for even $n$, $\frac {n^2}{4}$ and for odd $n$, $\frac{n^2-1}{4}$ are possible. Can we show this the is highest value which is possible for all fields (while of course for specific fields higher may be possible)?",,"['linear-algebra', 'group-theory', 'matrices']"
22,"Linear algebra and geometric insight: a rigorous approach to vector spaces, matrices, and linear applications","Linear algebra and geometric insight: a rigorous approach to vector spaces, matrices, and linear applications",,"I'm searching for some material (books or lecture notes) that extensively uses a geometric approach to explain the meaning of the concepts related to vector spaces, matrices, and linear applications presented in an undergraduate course in linear algebra (for instance, the basis of a vector space, the orientation of a vector space, the determinant of a matrix, and so on). Nice pictures and graphics is a plus.","I'm searching for some material (books or lecture notes) that extensively uses a geometric approach to explain the meaning of the concepts related to vector spaces, matrices, and linear applications presented in an undergraduate course in linear algebra (for instance, the basis of a vector space, the orientation of a vector space, the determinant of a matrix, and so on). Nice pictures and graphics is a plus.",,"['linear-algebra', 'matrices', 'geometry', 'reference-request', 'soft-question']"
23,Determinant of a $n\times n $ matrix,Determinant of a  matrix,n\times n ,"Let $n$ be a positive odd integer and let $A$ be  a symmetric $n\times n$ matrix of integer entries such that $a_{ii}=0,i=1,2.....n$. Show that the determinant of $A$ is even. I tried using definition of determinant. Also I can't use induction since the problem is for odd integers. Please give some thoughts on how to solve it.","Let $n$ be a positive odd integer and let $A$ be  a symmetric $n\times n$ matrix of integer entries such that $a_{ii}=0,i=1,2.....n$. Show that the determinant of $A$ is even. I tried using definition of determinant. Also I can't use induction since the problem is for odd integers. Please give some thoughts on how to solve it.",,"['linear-algebra', 'matrices', 'determinant']"
24,A real matrix whose eigenvalues have all negative real parts,A real matrix whose eigenvalues have all negative real parts,,"While taking a look in some lecture notes of an ODE course, I found the following claim, which appeared in the text as an exercise: Let $A$ be a real $n\times n$ matrix whose eigenvalues have all negative real parts. Then there is some $\beta>0$ such that $$\forall x\in\mathbb{R}^n\quad\langle Ax,x\rangle\leq-\beta\|x\|^2.$$I think the claim is false , as can be shown by taking for example $$A=\left(\begin{array}{cc}0&-1\\1&-1\end{array}\right).$$The characteristic polynomial is $$f_A(x)=\left|\begin{array}{rr}x&1\\-1&x+1\end{array}\right|=x^2+x+1,$$and its roots are $$x=\frac{-1\pm\sqrt{-3}}{2},$$so the real parts are negative. But taking $x=(1,0),$ $$\langle Ax,x\rangle=\langle(0,1),(1,0)\rangle=0,$$which shows that the above claim does not hold. My problem: This claim is used in the text to prove a theorem which seems to be well-known and important. I guess the proof is not totally wrong, therefore I suspect that the above claim can be modified and turn into a true one. This is the point where I'll be happy to hear any suggestions. Some ODE intuition: Consider the linear equation $$\frac{dy}{dt}=Ay.$$ Any solution is of the form $$y(t)=e^{tA}y_0,$$and if $A$'s eigenvalues all have negative real parts, it turns out that all the solutions get closer to $0$ as $t\to\infty$. The above (false) claim says that $\|y\|$ is decreasing at any time in this case. It could be nice to state a condition that can actually guarantee that. Any thoughts?","While taking a look in some lecture notes of an ODE course, I found the following claim, which appeared in the text as an exercise: Let $A$ be a real $n\times n$ matrix whose eigenvalues have all negative real parts. Then there is some $\beta>0$ such that $$\forall x\in\mathbb{R}^n\quad\langle Ax,x\rangle\leq-\beta\|x\|^2.$$I think the claim is false , as can be shown by taking for example $$A=\left(\begin{array}{cc}0&-1\\1&-1\end{array}\right).$$The characteristic polynomial is $$f_A(x)=\left|\begin{array}{rr}x&1\\-1&x+1\end{array}\right|=x^2+x+1,$$and its roots are $$x=\frac{-1\pm\sqrt{-3}}{2},$$so the real parts are negative. But taking $x=(1,0),$ $$\langle Ax,x\rangle=\langle(0,1),(1,0)\rangle=0,$$which shows that the above claim does not hold. My problem: This claim is used in the text to prove a theorem which seems to be well-known and important. I guess the proof is not totally wrong, therefore I suspect that the above claim can be modified and turn into a true one. This is the point where I'll be happy to hear any suggestions. Some ODE intuition: Consider the linear equation $$\frac{dy}{dt}=Ay.$$ Any solution is of the form $$y(t)=e^{tA}y_0,$$and if $A$'s eigenvalues all have negative real parts, it turns out that all the solutions get closer to $0$ as $t\to\infty$. The above (false) claim says that $\|y\|$ is decreasing at any time in this case. It could be nice to state a condition that can actually guarantee that. Any thoughts?",,"['linear-algebra', 'ordinary-differential-equations']"
25,Existence of non-trivial linear functional on any vector space,Existence of non-trivial linear functional on any vector space,,"For every vector space $V$ does there exist a linear functional $f$ ( a linear map from $V$ to $F$ the underlying field ) such that for some $ \vec v \in V$ , $f(\vec v) \ne 0$ ? If it does exist , can we prove the existence without the ""axiom of choice "" ? Is the existence equivalent to axiom of choice ?","For every vector space $V$ does there exist a linear functional $f$ ( a linear map from $V$ to $F$ the underlying field ) such that for some $ \vec v \in V$ , $f(\vec v) \ne 0$ ? If it does exist , can we prove the existence without the ""axiom of choice "" ? Is the existence equivalent to axiom of choice ?",,"['linear-algebra', 'vector-spaces', 'axiom-of-choice', 'linear-transformations']"
26,How prove or disprove is positive define matrix?,How prove or disprove is positive define matrix?,,"prove or disprove following matrix is   positive definite matrix ? $$\begin{bmatrix} \dfrac{\sin(a_1-a_1)}{a_1-a_1}&\cdots&\dfrac{\sin(a_1-a_n)}{a_1-a_n}\\ \vdots& &\vdots\\ \dfrac{\sin(a_n-a_1)}{a_n-a_1}&\cdots&\dfrac{\sin(a_n-a_n)}{a_n-a_n} \end{bmatrix}_{n\times n}$$ and  where we defind $$\dfrac{\sin 0}{0}=1$$ Maybe this problem can use Integral to solve it. since $$\frac{1}{x} = \int_0^\infty e^{-tx} t \, dt$$? Thank you","prove or disprove following matrix is   positive definite matrix ? $$\begin{bmatrix} \dfrac{\sin(a_1-a_1)}{a_1-a_1}&\cdots&\dfrac{\sin(a_1-a_n)}{a_1-a_n}\\ \vdots& &\vdots\\ \dfrac{\sin(a_n-a_1)}{a_n-a_1}&\cdots&\dfrac{\sin(a_n-a_n)}{a_n-a_n} \end{bmatrix}_{n\times n}$$ and  where we defind $$\dfrac{\sin 0}{0}=1$$ Maybe this problem can use Integral to solve it. since $$\frac{1}{x} = \int_0^\infty e^{-tx} t \, dt$$? Thank you",,"['linear-algebra', 'matrices']"
27,Can Moore–Penrose pseudoinverse solve for underdetermined linear system?,Can Moore–Penrose pseudoinverse solve for underdetermined linear system?,,"Thanks for reading my thread. I am thinking, many of us know that Moore–Penrose pseudoinverse can solve for overdetermined system  $Ax=b$, where $x=(A^TA)^{-1}A^Tb$; for exmplae the linear regression application , or curve fitting applications. However, I am wondering for underdetermined system, can we use Moore–Penrose pseudoinverse solver? If yes, why we need many iterative reconstruction algorithm? Since we can know the derivative of the objective anyway, then why don't we just set the derivative to 0, then solve it using some skills like Moore–Penrose pseudoinverse? Some explanation in theory is highly appreciated. Does not have to be rigorous prove, but something that makes sense. Thanks a lot!","Thanks for reading my thread. I am thinking, many of us know that Moore–Penrose pseudoinverse can solve for overdetermined system  $Ax=b$, where $x=(A^TA)^{-1}A^Tb$; for exmplae the linear regression application , or curve fitting applications. However, I am wondering for underdetermined system, can we use Moore–Penrose pseudoinverse solver? If yes, why we need many iterative reconstruction algorithm? Since we can know the derivative of the objective anyway, then why don't we just set the derivative to 0, then solve it using some skills like Moore–Penrose pseudoinverse? Some explanation in theory is highly appreciated. Does not have to be rigorous prove, but something that makes sense. Thanks a lot!",,"['linear-algebra', 'derivatives', 'optimization']"
28,"Prove that if $W_1\subseteq V$ finite-dimensional, then there is $W_2\subseteq V$ such that $V=W_1\oplus W_2$","Prove that if  finite-dimensional, then there is  such that",W_1\subseteq V W_2\subseteq V V=W_1\oplus W_2,"Prove that if $W_1$ is any subspace of a finite-dimensional vector space $V$, then there exists a subspace $W_2$ of $V$ such that $V = W_1 \oplus W_2$ What I have done so far is to note that since $V$ is finite and $W_1$ is a subspace of $V$, we have $\dim(W_1) \leq \dim(V)$. If we have equality, then let $W_2 = \{0\}$, so that we have $V= W_1 \oplus W_2$. So now I need to look at the case when $\dim(W_1) \lt \dim(V)$. What I have tried for this case is to let $$\beta = \{v_1,v_2,..,v_n\}$$ be a basis for $V$ and $$\gamma=\{u_1,u_2,..,u_m\}$$ a basis for $W_1$. My idea was to extend $\gamma$ to a basis for $V$, so let $$\alpha=\{u_1,u_2,..,u_m,w_1,w_2,..,w_{n-m}\}$$ be the extension of $\gamma$ to $V$, where $w_1,w_2,..,w_{n-m}$ are basis vectors for $W_2$. If I'm doing this right then I would just have to show that $W_1 \cap W_2 = \{0\}$ and $W_1 + W_2= V$ Am I heading in the right direction? Any hints would be greatly appreciated.","Prove that if $W_1$ is any subspace of a finite-dimensional vector space $V$, then there exists a subspace $W_2$ of $V$ such that $V = W_1 \oplus W_2$ What I have done so far is to note that since $V$ is finite and $W_1$ is a subspace of $V$, we have $\dim(W_1) \leq \dim(V)$. If we have equality, then let $W_2 = \{0\}$, so that we have $V= W_1 \oplus W_2$. So now I need to look at the case when $\dim(W_1) \lt \dim(V)$. What I have tried for this case is to let $$\beta = \{v_1,v_2,..,v_n\}$$ be a basis for $V$ and $$\gamma=\{u_1,u_2,..,u_m\}$$ a basis for $W_1$. My idea was to extend $\gamma$ to a basis for $V$, so let $$\alpha=\{u_1,u_2,..,u_m,w_1,w_2,..,w_{n-m}\}$$ be the extension of $\gamma$ to $V$, where $w_1,w_2,..,w_{n-m}$ are basis vectors for $W_2$. If I'm doing this right then I would just have to show that $W_1 \cap W_2 = \{0\}$ and $W_1 + W_2= V$ Am I heading in the right direction? Any hints would be greatly appreciated.",,"['linear-algebra', 'direct-sum']"
29,Find linear transformation given kernel,Find linear transformation given kernel,,"Find linear transformation $F$ in canonical bases given $ F: \Bbb R^4 \to \Bbb R^3 $ $ \ker F=\operatorname{span}\left\{\begin{bmatrix}1\\2\\3\\4\end{bmatrix}, \begin{bmatrix}0\\1\\1\\1\end{bmatrix} \right\} $ I tried expanding these two vectors to a basis of $\Bbb R^4$, I tried adding two linear dependent vectors, but nothing seems to work. I would be glad for a little hint.","Find linear transformation $F$ in canonical bases given $ F: \Bbb R^4 \to \Bbb R^3 $ $ \ker F=\operatorname{span}\left\{\begin{bmatrix}1\\2\\3\\4\end{bmatrix}, \begin{bmatrix}0\\1\\1\\1\end{bmatrix} \right\} $ I tried expanding these two vectors to a basis of $\Bbb R^4$, I tried adding two linear dependent vectors, but nothing seems to work. I would be glad for a little hint.",,"['linear-algebra', 'transformation']"
30,Vector spaces and intersections,Vector spaces and intersections,,"I was thinking of the following problem lately: Suppose $V_1,V_2,V_3,V_4$ are vector subspaces of $\Bbb{R}^4$ of dimension $2$ such that $V_i\cap V_j=\{0\}$ for $i \neq j$. Is it true that we can find a two dimensional vector subspace of $\Bbb{R}^4$ such that $\dim V_i\cap W =1$ for $i=1,2,3,4$? The same problem with all dimensions doubled was given to a Miklos Schweitzer competition in 2012. Using a linear automorphism of $\Bbb{R}^4$ we can assume that $V_1=span\{e_1,e_2\},V_2=span\{e_3,e_4\}$ where $(e_1,e_2,e_3,e_4)$ is the canonical base. It is rather easy to construct $W$ which satisfies $\dim W\cap V_i = 1$ for $i=1,2,3$ but I didn't manage connecting it to the fourth space.","I was thinking of the following problem lately: Suppose $V_1,V_2,V_3,V_4$ are vector subspaces of $\Bbb{R}^4$ of dimension $2$ such that $V_i\cap V_j=\{0\}$ for $i \neq j$. Is it true that we can find a two dimensional vector subspace of $\Bbb{R}^4$ such that $\dim V_i\cap W =1$ for $i=1,2,3,4$? The same problem with all dimensions doubled was given to a Miklos Schweitzer competition in 2012. Using a linear automorphism of $\Bbb{R}^4$ we can assume that $V_1=span\{e_1,e_2\},V_2=span\{e_3,e_4\}$ where $(e_1,e_2,e_3,e_4)$ is the canonical base. It is rather easy to construct $W$ which satisfies $\dim W\cap V_i = 1$ for $i=1,2,3$ but I didn't manage connecting it to the fourth space.",,"['linear-algebra', 'vector-spaces']"
31,A generalization of IMO 1977 problem 2,A generalization of IMO 1977 problem 2,,"Here is the IMO 1977 problem 2: In a finite sequence of real numbers the sum of any seven successive terms is   negative, and the sum of any eleven successive terms is positive. Determine   the maximum number of terms in the sequence. I would like to generalize this problem : seven -> $p$; eleven -> $q$. I have read that the result is $p+q-1-\gcd(p,q)$, I wonder how to prove it. Firstly, we shall assume that $\gcd(p,q)=1$. I proved that the number of terms is less than $p+q-2$, but I still have to prove that it can be equal to $p+q-2$. Can you exhibit an example? Remark. It is sufficient to prove that the following square matrix is invertible: $\newcommand{\block}[1]{   \underbrace{1 \cdots 1}_{#1} }$ $$\underbrace{\begin{pmatrix}   \smash[b]{\block{p}} \\ &\ddots \\ & & \block{p}\\ \smash[b]{\block{q}} \\ &\ddots \\ & & \block{q}   \end{pmatrix} }_{p+q-2}$$","Here is the IMO 1977 problem 2: In a finite sequence of real numbers the sum of any seven successive terms is   negative, and the sum of any eleven successive terms is positive. Determine   the maximum number of terms in the sequence. I would like to generalize this problem : seven -> $p$; eleven -> $q$. I have read that the result is $p+q-1-\gcd(p,q)$, I wonder how to prove it. Firstly, we shall assume that $\gcd(p,q)=1$. I proved that the number of terms is less than $p+q-2$, but I still have to prove that it can be equal to $p+q-2$. Can you exhibit an example? Remark. It is sufficient to prove that the following square matrix is invertible: $\newcommand{\block}[1]{   \underbrace{1 \cdots 1}_{#1} }$ $$\underbrace{\begin{pmatrix}   \smash[b]{\block{p}} \\ &\ddots \\ & & \block{p}\\ \smash[b]{\block{q}} \\ &\ddots \\ & & \block{q}   \end{pmatrix} }_{p+q-2}$$",,"['linear-algebra', 'elementary-number-theory', 'contest-math']"
32,"Product of reflections is a rotation, by elementary vector methods","Product of reflections is a rotation, by elementary vector methods",,"Let $\mathbf{u}$ and $\mathbf{v}$ be two 3D unit vectors. The transform that performs reflection in the plane normal to $\mathbf{u}$ is given by $$ T_{\mathbf{u}}(\mathbf{x}) = \mathbf{x} - 2(\mathbf{x} \cdot \mathbf{u})\mathbf{u} $$ and similarly, reflection in the plane normal to $\mathbf{v}$ is performed by $$ T_{\mathbf{v}}(\mathbf{x}) = \mathbf{x} - 2(\mathbf{x} \cdot \mathbf{v})\mathbf{v} $$ Let $\theta$ be the angle between $\mathbf{u}$ and $\mathbf{v}$, and let $\mathbf{n}$ be the unit vector in the direction of $\mathbf{u} \times \mathbf{v}$. So, then we know that $\cos\theta = \mathbf{u} \cdot \mathbf{v}$, and $\mathbf{u} \times \mathbf{v} = (\sin\theta)\mathbf{n}$. The composition of these two reflections is a rotation around $\mathbf{n}$ by an angle of $2\theta$ (I believe), and that rotation is given by Rodrigues' formula: $$ R(\mathbf{x}) = (\cos2\theta)\mathbf{x} +                     (1 - \cos2\theta)(\mathbf{x} \cdot \mathbf{n})\mathbf{n} +                     (\sin 2\theta)(\mathbf{x} \times \mathbf{n}) $$ It seems to me that we ought to be able to prove from first principles that  $$ T_{\mathbf{u}}\big(  T_{\mathbf{v}}(\mathbf{x})  \big) = R(\mathbf{x}) $$ I've slogged through pages of vector algebra for a few hours, but to no avail. It's depressing -- I used to be good at this stuff, but apparently not any more. I'd like a proof that uses nothing but elementary vector arithmetic, and I'd like it to be coordinate-free, please. Edit As a couple of people have mentioned, it seems sensible to work in the $\mathbf{u}\text{-}\mathbf{v}\text{-}\mathbf{n}$ coordinate system. This doesn't violate my ""coordinate free"" requirement as long as we don't start writing out explicit coordinates for $\mathbf{u}$, $\mathbf{v}$ and $\mathbf{n}$. The vector $R(\mathbf{x}) - \mathbf{x}$ should be entirely in the  $\mathbf{u}\text{-}\mathbf{v}$ plane, so all of its $\mathbf{n}$ terms must vanish, and we should be left with an expression that involves only $\mathbf{u}$ and $\mathbf{v}$, which (I hope) will give us the link to the reflections. The algebraic grunt-work involved is what's giving me trouble.","Let $\mathbf{u}$ and $\mathbf{v}$ be two 3D unit vectors. The transform that performs reflection in the plane normal to $\mathbf{u}$ is given by $$ T_{\mathbf{u}}(\mathbf{x}) = \mathbf{x} - 2(\mathbf{x} \cdot \mathbf{u})\mathbf{u} $$ and similarly, reflection in the plane normal to $\mathbf{v}$ is performed by $$ T_{\mathbf{v}}(\mathbf{x}) = \mathbf{x} - 2(\mathbf{x} \cdot \mathbf{v})\mathbf{v} $$ Let $\theta$ be the angle between $\mathbf{u}$ and $\mathbf{v}$, and let $\mathbf{n}$ be the unit vector in the direction of $\mathbf{u} \times \mathbf{v}$. So, then we know that $\cos\theta = \mathbf{u} \cdot \mathbf{v}$, and $\mathbf{u} \times \mathbf{v} = (\sin\theta)\mathbf{n}$. The composition of these two reflections is a rotation around $\mathbf{n}$ by an angle of $2\theta$ (I believe), and that rotation is given by Rodrigues' formula: $$ R(\mathbf{x}) = (\cos2\theta)\mathbf{x} +                     (1 - \cos2\theta)(\mathbf{x} \cdot \mathbf{n})\mathbf{n} +                     (\sin 2\theta)(\mathbf{x} \times \mathbf{n}) $$ It seems to me that we ought to be able to prove from first principles that  $$ T_{\mathbf{u}}\big(  T_{\mathbf{v}}(\mathbf{x})  \big) = R(\mathbf{x}) $$ I've slogged through pages of vector algebra for a few hours, but to no avail. It's depressing -- I used to be good at this stuff, but apparently not any more. I'd like a proof that uses nothing but elementary vector arithmetic, and I'd like it to be coordinate-free, please. Edit As a couple of people have mentioned, it seems sensible to work in the $\mathbf{u}\text{-}\mathbf{v}\text{-}\mathbf{n}$ coordinate system. This doesn't violate my ""coordinate free"" requirement as long as we don't start writing out explicit coordinates for $\mathbf{u}$, $\mathbf{v}$ and $\mathbf{n}$. The vector $R(\mathbf{x}) - \mathbf{x}$ should be entirely in the  $\mathbf{u}\text{-}\mathbf{v}$ plane, so all of its $\mathbf{n}$ terms must vanish, and we should be left with an expression that involves only $\mathbf{u}$ and $\mathbf{v}$, which (I hope) will give us the link to the reflections. The algebraic grunt-work involved is what's giving me trouble.",,"['linear-algebra', 'geometry', 'rotations', 'vectors']"
33,Linearly independent random variables and independent random variables,Linearly independent random variables and independent random variables,,"Does one of these two assertions imply the other ? (1) $X_1, X_2, ..., X_n$  are linearly independent random variables (i.e. $\lambda_1 X_1 + \lambda_2 X_2 + ... + \lambda_n X_n = 0$ => $\lambda_1 =\lambda_2=...=\lambda_n=0$) and (2) $X_1, X_2, ..., X_n$  are independent random variables (stochastically independent) If not, is there some special cases for which one implication (1=>2 or 2=>1) is true (Gaussian law? etc.)","Does one of these two assertions imply the other ? (1) $X_1, X_2, ..., X_n$  are linearly independent random variables (i.e. $\lambda_1 X_1 + \lambda_2 X_2 + ... + \lambda_n X_n = 0$ => $\lambda_1 =\lambda_2=...=\lambda_n=0$) and (2) $X_1, X_2, ..., X_n$  are independent random variables (stochastically independent) If not, is there some special cases for which one implication (1=>2 or 2=>1) is true (Gaussian law? etc.)",,"['linear-algebra', 'probability', 'random-variables', 'independence']"
34,Rotation matrix convention; successive rotations in intermediate coordinate systems or not,Rotation matrix convention; successive rotations in intermediate coordinate systems or not,,"I am getting very confused about the different conventions used for rotation matrices. Thing is I want to accomplish (3) successive rotations each time in the newly defined coordinate system. I use this convention for a rotation matrix: $$R_{21} = \begin{bmatrix} \hat{u} & \hat{v} & \hat{w} \end{bmatrix} = \begin{bmatrix} u_x &v_x & w_x \\ u_y & v_y & w_y \\ u_z & v_z & w_z \\ \end{bmatrix}$$ Meaning that the x, y and z vector of the new $\underline{e}_2$ coordinate system are given column-wise . If I then define three successive rotations in the order $zyx$ / yaw pitch roll / $\psi \theta \varphi$ with: $$R(\psi) = \begin{bmatrix} \cos(\psi) & -\sin(\psi) & 0 \\ \sin(\psi) & \cos(\psi) & 0 \\ 0 & 0 & 1 \\  \end{bmatrix}$$ $$R(\theta) = \begin{bmatrix} \cos(\theta) & 0 & \sin(\theta) \\ 0 & 1 & 0 \\ -\sin(\theta) & 0 & \cos(\theta) \\ \end{bmatrix}$$ $$R(\varphi) = \begin{bmatrix} 1 & 0 & 0 \\ 0 & \cos(\varphi) & -\sin(\varphi)\\ 0 & \sin(\varphi) & \cos(\varphi) \\  \end{bmatrix}$$ Then I can calculate the final coordinate system $\underline{e}_4$ using: $$R_{41} = R(\varphi) R(\theta) R(\psi)$$ Unfortunately this results in each rotation around the initial coordinate system $\underline{e}_1$, which is what I do not want. I require $R(\psi)$ to rotate around the z axis of $\underline{e}_1$, $R(\theta)$ around the y axis of $\underline{e}_2$ and $R(\varphi)$ around the x axis of $\underline{e}_3$, together resulting in $\underline{e}_4$. I guess this has something to do with active/passive rotations (e.g. vector or coordinate system rotations). I achieve what I want by doing: $$R_{41} = \left( R(\varphi)^T R(\theta)^T R(\psi)^T \right) ^T$$ But this is a rather ugly way, nor do I fully understand why this is correct. Who can explain me what happens? And who can recommend me a better approach? Second thing is pre-multiplication versus post-multiplication when dealing with column vectors and row-vectors (seen in the context of the above). How do I transform a column vector to $\underline{e}_4$ taken into account that each successive rotation has to be around the newly defined coordinate system? Similar for a row-vector. Is this correct (for a row vector): $$\vec{w}_2 = \vec{w}_1 R_{41}^T$$ And for column vectors: $$\vec{v}_2 = R_{41} \vec{v}_1$$ Thanks for your help in this!","I am getting very confused about the different conventions used for rotation matrices. Thing is I want to accomplish (3) successive rotations each time in the newly defined coordinate system. I use this convention for a rotation matrix: $$R_{21} = \begin{bmatrix} \hat{u} & \hat{v} & \hat{w} \end{bmatrix} = \begin{bmatrix} u_x &v_x & w_x \\ u_y & v_y & w_y \\ u_z & v_z & w_z \\ \end{bmatrix}$$ Meaning that the x, y and z vector of the new $\underline{e}_2$ coordinate system are given column-wise . If I then define three successive rotations in the order $zyx$ / yaw pitch roll / $\psi \theta \varphi$ with: $$R(\psi) = \begin{bmatrix} \cos(\psi) & -\sin(\psi) & 0 \\ \sin(\psi) & \cos(\psi) & 0 \\ 0 & 0 & 1 \\  \end{bmatrix}$$ $$R(\theta) = \begin{bmatrix} \cos(\theta) & 0 & \sin(\theta) \\ 0 & 1 & 0 \\ -\sin(\theta) & 0 & \cos(\theta) \\ \end{bmatrix}$$ $$R(\varphi) = \begin{bmatrix} 1 & 0 & 0 \\ 0 & \cos(\varphi) & -\sin(\varphi)\\ 0 & \sin(\varphi) & \cos(\varphi) \\  \end{bmatrix}$$ Then I can calculate the final coordinate system $\underline{e}_4$ using: $$R_{41} = R(\varphi) R(\theta) R(\psi)$$ Unfortunately this results in each rotation around the initial coordinate system $\underline{e}_1$, which is what I do not want. I require $R(\psi)$ to rotate around the z axis of $\underline{e}_1$, $R(\theta)$ around the y axis of $\underline{e}_2$ and $R(\varphi)$ around the x axis of $\underline{e}_3$, together resulting in $\underline{e}_4$. I guess this has something to do with active/passive rotations (e.g. vector or coordinate system rotations). I achieve what I want by doing: $$R_{41} = \left( R(\varphi)^T R(\theta)^T R(\psi)^T \right) ^T$$ But this is a rather ugly way, nor do I fully understand why this is correct. Who can explain me what happens? And who can recommend me a better approach? Second thing is pre-multiplication versus post-multiplication when dealing with column vectors and row-vectors (seen in the context of the above). How do I transform a column vector to $\underline{e}_4$ taken into account that each successive rotation has to be around the newly defined coordinate system? Similar for a row-vector. Is this correct (for a row vector): $$\vec{w}_2 = \vec{w}_1 R_{41}^T$$ And for column vectors: $$\vec{v}_2 = R_{41} \vec{v}_1$$ Thanks for your help in this!",,"['linear-algebra', 'geometry', 'algebraic-geometry', 'rotations']"
35,Eigenvalues of Multiplication in algebraic number field,Eigenvalues of Multiplication in algebraic number field,,"Finally I have a new question which is puzzling me. I am currently reading a manuscript so there is no reference or link I can provide. I will try to put all the information needed in here. This implies that I hear several definitions for the first time while reading, which on the other hand is assumed and unless I provide references the idea is that everything needed should be obtainable from what is mentioned within the post. Question 1: (too general) Let $\mathbb{K}$ be an algebraic number field, i.e. $\mathbb{Q}\hookrightarrow\mathbb{K}$ with $[\mathbb{K}:\mathbb{Q}]=:d<\infty$. Let $b\in\mathbb{K}$ and define $\phi(b)(x):=bx$ for all $x\in\mathbb{K}$. Then $\phi(b)\in\operatorname{End}(\mathbb{Q}^{[\mathbb{K}:\mathbb{Q}]})$. What are the Eigenvalues of $\phi(b)$ (i.e. the roots of the characteristic polynomial)? What have I got: As $\mathbb{K}$ is an algebraic number field, we know that $\mathbb{K}\cong\mathbb{Q}(\alpha)$ for some number algebraic over $\mathbb{Q}$ and that for $m\in\mathbb{Q}[z]$ the minimal polynomial of $\alpha$. Letting $\zeta_{1},\ldots,\zeta_{r}$ the real roots of $m$ and $\zeta_{r+1},\overline{\zeta_{r+1}},\ldots,\zeta_{r+s},\overline{\zeta_{r+s}}$ the complex roots of $m$, we know that $\alpha$ is an annihilator of the characteristic polynomial of $\phi(\alpha)$ and hence the minimal polynomial divides the characteristic polynomial and hence $m$ being separable over $\mathbb{Q}$ is a $\mathbb{Q}$-multiple of the charcteristic polynomial for reasons of dimension. This argument holds for all roots of $m$. What is the issue: In the manuscript at hand we start off with an element $\zeta\in\mathbb{Q}(\alpha)$ (or $\mathbb{K}$ - as you prefer) satisfying some more restraints: There is a subring $\mathcal{O}\subseteq \mathbb{Q}(\alpha)$ such that as an additive group $\mathcal{O}\cong\mathbb{Z}^{d}$ (as $d=[\mathbb{Q}(\alpha):\mathbb{Q}]$ and the base field is $\mathbb{Q}$, this implies that $\mathcal{O}$ is a lattice in the $\mathbb{Q}$-vector space $\mathbb{Q}(\alpha)$). $\mathcal{O}$ is called an order - I am not sure whether this agrees with the standard definition of an order (one sometimes sees the requirement that $1\in\mathcal{O}$ which doesn't follow automatically from this definition). Now the manuscript claims that for $b\in\mathcal{O}$ the eigenvalues of $\phi(b)$ are also given by the roots of $m$. I am not sure but this seems odd to me. If we let $b=0$ then the eigenvalues are all zero. If $1\in\mathcal{O}$, then for $b=1$ the eigenvalues are all 1. Can we say something about the group of units, i.e. if $b\in\mathcal{O}^{\times}=\{o\in\mathcal{O};\exists p\in\mathcal{O}:op=1\}$? Question 2: Is there a way to identify the elements $b\in\mathcal{O}$ such that $\phi(b)$ has the roots of $m$ as eigenvalues?","Finally I have a new question which is puzzling me. I am currently reading a manuscript so there is no reference or link I can provide. I will try to put all the information needed in here. This implies that I hear several definitions for the first time while reading, which on the other hand is assumed and unless I provide references the idea is that everything needed should be obtainable from what is mentioned within the post. Question 1: (too general) Let $\mathbb{K}$ be an algebraic number field, i.e. $\mathbb{Q}\hookrightarrow\mathbb{K}$ with $[\mathbb{K}:\mathbb{Q}]=:d<\infty$. Let $b\in\mathbb{K}$ and define $\phi(b)(x):=bx$ for all $x\in\mathbb{K}$. Then $\phi(b)\in\operatorname{End}(\mathbb{Q}^{[\mathbb{K}:\mathbb{Q}]})$. What are the Eigenvalues of $\phi(b)$ (i.e. the roots of the characteristic polynomial)? What have I got: As $\mathbb{K}$ is an algebraic number field, we know that $\mathbb{K}\cong\mathbb{Q}(\alpha)$ for some number algebraic over $\mathbb{Q}$ and that for $m\in\mathbb{Q}[z]$ the minimal polynomial of $\alpha$. Letting $\zeta_{1},\ldots,\zeta_{r}$ the real roots of $m$ and $\zeta_{r+1},\overline{\zeta_{r+1}},\ldots,\zeta_{r+s},\overline{\zeta_{r+s}}$ the complex roots of $m$, we know that $\alpha$ is an annihilator of the characteristic polynomial of $\phi(\alpha)$ and hence the minimal polynomial divides the characteristic polynomial and hence $m$ being separable over $\mathbb{Q}$ is a $\mathbb{Q}$-multiple of the charcteristic polynomial for reasons of dimension. This argument holds for all roots of $m$. What is the issue: In the manuscript at hand we start off with an element $\zeta\in\mathbb{Q}(\alpha)$ (or $\mathbb{K}$ - as you prefer) satisfying some more restraints: There is a subring $\mathcal{O}\subseteq \mathbb{Q}(\alpha)$ such that as an additive group $\mathcal{O}\cong\mathbb{Z}^{d}$ (as $d=[\mathbb{Q}(\alpha):\mathbb{Q}]$ and the base field is $\mathbb{Q}$, this implies that $\mathcal{O}$ is a lattice in the $\mathbb{Q}$-vector space $\mathbb{Q}(\alpha)$). $\mathcal{O}$ is called an order - I am not sure whether this agrees with the standard definition of an order (one sometimes sees the requirement that $1\in\mathcal{O}$ which doesn't follow automatically from this definition). Now the manuscript claims that for $b\in\mathcal{O}$ the eigenvalues of $\phi(b)$ are also given by the roots of $m$. I am not sure but this seems odd to me. If we let $b=0$ then the eigenvalues are all zero. If $1\in\mathcal{O}$, then for $b=1$ the eigenvalues are all 1. Can we say something about the group of units, i.e. if $b\in\mathcal{O}^{\times}=\{o\in\mathcal{O};\exists p\in\mathcal{O}:op=1\}$? Question 2: Is there a way to identify the elements $b\in\mathcal{O}$ such that $\phi(b)$ has the roots of $m$ as eigenvalues?",,"['linear-algebra', 'abstract-algebra', 'field-theory', 'algebraic-number-theory']"
36,Is there a difference between abstract vector spaces and vector spaces?,Is there a difference between abstract vector spaces and vector spaces?,,"I am following my Oxford syllabus and my next step is abstract vector spaces , in my linear algebra book I've found vector spaces . I've searched a little and made a superficial comparison between both and found that they are the same thing. Is that correct or there's something I'm missing? Also, if it's correct, why two names to the same thing?","I am following my Oxford syllabus and my next step is abstract vector spaces , in my linear algebra book I've found vector spaces . I've searched a little and made a superficial comparison between both and found that they are the same thing. Is that correct or there's something I'm missing? Also, if it's correct, why two names to the same thing?",,"['linear-algebra', 'abstract-algebra', 'vector-spaces', 'terminology']"
37,Representing linear operators on infinite sequences as infinite matrices,Representing linear operators on infinite sequences as infinite matrices,,"So this question arose while I was working on a homework assignment to find the adjoint operator of a continuous operator on $l^2$. Anyway, it seemed like I could find it if I thought about the respective operators as infinite matrices. However, this is something that hasn't been discussed in class and I haven't found anything about on the internet. Is this valid? I've never really thought about infinite matrices before so I am just a little hesitant about what may be different from the finite cases I am use to. Can a linear operator on a sequence always be viewed as an infinite sequence? Can anybody point me in the direction of a website or suggest a book that would give a good introduction to this? Thank you!","So this question arose while I was working on a homework assignment to find the adjoint operator of a continuous operator on $l^2$. Anyway, it seemed like I could find it if I thought about the respective operators as infinite matrices. However, this is something that hasn't been discussed in class and I haven't found anything about on the internet. Is this valid? I've never really thought about infinite matrices before so I am just a little hesitant about what may be different from the finite cases I am use to. Can a linear operator on a sequence always be viewed as an infinite sequence? Can anybody point me in the direction of a website or suggest a book that would give a good introduction to this? Thank you!",,"['linear-algebra', 'matrices', 'analysis', 'infinite-matrices']"
38,What are the real life applications of quadratic forms?,What are the real life applications of quadratic forms?,,What are the real life applications of quadratic forms? I have used them to sketch conics but are there any other applications?,What are the real life applications of quadratic forms? I have used them to sketch conics but are there any other applications?,,"['linear-algebra', 'numerical-linear-algebra']"
39,"If the matrix of a linear map is independent from the basis, then the map is a multiple of the identity map.","If the matrix of a linear map is independent from the basis, then the map is a multiple of the identity map.",,"Let $V$ be a finite dimensional vector space over $F$, and let $$T:V\to V$$ be a linear map. Suppose that given any two bases $B$ and $C$ for $V$, we have that the matrix of $T$ with respect $B$ is equal to that with respect to $C$. How can we show that this implies that there exists some $\lambda\in F$ such that $T(v)=\lambda v$, $\forall v\in V$?","Let $V$ be a finite dimensional vector space over $F$, and let $$T:V\to V$$ be a linear map. Suppose that given any two bases $B$ and $C$ for $V$, we have that the matrix of $T$ with respect $B$ is equal to that with respect to $C$. How can we show that this implies that there exists some $\lambda\in F$ such that $T(v)=\lambda v$, $\forall v\in V$?",,['linear-algebra']
40,"Probability that a $3\times 3$ matrix with entries in $\{0,1,2,3\}$ is invertible.",Probability that a  matrix with entries in  is invertible.,"3\times 3 \{0,1,2,3\}","Let $A$ be a $3\times 3$ matrix, and each of its entries takes value from $\{0, 1, 2, 3\}$ with probability $1/4$ for each value. What is the probability that A is invertible? I have tried to list all possible combinations, but that is too complicated and I cannot proceed. Any help is appreciated. The matrix operation is in R, not restricted modulo 4. For example, 2*2 = 4, not 0, so a matrix with determinant 4 is regarded as invertible.","Let $A$ be a $3\times 3$ matrix, and each of its entries takes value from $\{0, 1, 2, 3\}$ with probability $1/4$ for each value. What is the probability that A is invertible? I have tried to list all possible combinations, but that is too complicated and I cannot proceed. Any help is appreciated. The matrix operation is in R, not restricted modulo 4. For example, 2*2 = 4, not 0, so a matrix with determinant 4 is regarded as invertible.",,"['linear-algebra', 'probability', 'combinatorics', 'group-theory', 'matrices']"
41,How to calculate basis of kernel?,How to calculate basis of kernel?,,I have a linear transformation. The transformation and what I tried is written on the attached work page. Is my way wrong? what is the basis of KerT? LinearAlgebra: S -> S is a joke with my friends. sorry for this.,I have a linear transformation. The transformation and what I tried is written on the attached work page. Is my way wrong? what is the basis of KerT? LinearAlgebra: S -> S is a joke with my friends. sorry for this.,,['linear-algebra']
42,Why would $(A^{\text T}A+\lambda I)^{-1}A^{\text T}$ be close to $A^{\dagger}$ when $A$ is with rank deficiency?,Why would  be close to  when  is with rank deficiency?,(A^{\text T}A+\lambda I)^{-1}A^{\text T} A^{\dagger} A,"In many applications that is not with high requirements, it is common to use $(A^{\text T}A+\lambda I)^{-1}A^{\text T}$ or $A^{\text T}(AA^{\text T}+\lambda I)^{-1}$ ($\lambda$ is small) to approximate the Moore-Penrose pseudoinverse $A^{\dagger}$. But from the properties of Moore-Penrose pseudoinverse, how do we know that $(A^{\text T}A+\lambda I)^{-1}A^{\text T}$ and $A^{\text T}(AA^{\text T}+\lambda I)^{-1}$ will be close to the exact solution? It seems that $$ \lim_{\lambda \rightarrow 0}(A^{\text T}A+\lambda I)^{-1}A^{\text T} = A^{\dagger} $$ I want to know why this happens (You can try in Matlab for verification). Besides, it is also possible for $A^{\text T}A + \lambda I$ to be singular. The whole thing is confusing me. And the question following this is, if we want to restrict the error of $(A^{\text T}A+\lambda I)^{-1}A^{\text T}$ approximating the $A^{\dagger}$ to a certain extent, can we know the upper bound for the $\lambda$ according to some criterions?","In many applications that is not with high requirements, it is common to use $(A^{\text T}A+\lambda I)^{-1}A^{\text T}$ or $A^{\text T}(AA^{\text T}+\lambda I)^{-1}$ ($\lambda$ is small) to approximate the Moore-Penrose pseudoinverse $A^{\dagger}$. But from the properties of Moore-Penrose pseudoinverse, how do we know that $(A^{\text T}A+\lambda I)^{-1}A^{\text T}$ and $A^{\text T}(AA^{\text T}+\lambda I)^{-1}$ will be close to the exact solution? It seems that $$ \lim_{\lambda \rightarrow 0}(A^{\text T}A+\lambda I)^{-1}A^{\text T} = A^{\dagger} $$ I want to know why this happens (You can try in Matlab for verification). Besides, it is also possible for $A^{\text T}A + \lambda I$ to be singular. The whole thing is confusing me. And the question following this is, if we want to restrict the error of $(A^{\text T}A+\lambda I)^{-1}A^{\text T}$ approximating the $A^{\dagger}$ to a certain extent, can we know the upper bound for the $\lambda$ according to some criterions?",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
43,"Simultaneous Iteration, Convergence to Eigenvectors","Simultaneous Iteration, Convergence to Eigenvectors",,"I have a question about the simultaneous iteration. I am currently working for an exam and I do not understand this step (taken from Numerical Linear Algebra from Trefethen/Bau): For the power iteration it holds, that for an arbitrary starting vector $v^{(0)}$ with $||v^{(0)}|| = 1$ that $A^kv \rightarrow q_j$ for $k \rightarrow \infty$, where $q_j$ is the eigenvector corresponding to the maximum eigenvalue (in absolute value). So far, so good. Now the book states the following: For a set of n linearly independent vectors $v_1^{(0)}, ..., v_n^{(0)}$, the space $span\{A^kv_1^{(0)}, ..., A^kv_n^{(0)}\}$ converges to the space $span\{q_1, ..., q_n\}$, i.e. the space spanned by the eigenvectors. And this is something I do not understand. Above it is stated, that for almost any arbitrary starting vector, $v$ would converge to the eigenvector corresponding to the largest eigenvalue. How can than the space spanned by several vectors span the same space as all eigenvectors? I would assume that the all will converge sooner or later to the same vector, thus only spanning a one-dimensional subspace. Where is my mistake?","I have a question about the simultaneous iteration. I am currently working for an exam and I do not understand this step (taken from Numerical Linear Algebra from Trefethen/Bau): For the power iteration it holds, that for an arbitrary starting vector $v^{(0)}$ with $||v^{(0)}|| = 1$ that $A^kv \rightarrow q_j$ for $k \rightarrow \infty$, where $q_j$ is the eigenvector corresponding to the maximum eigenvalue (in absolute value). So far, so good. Now the book states the following: For a set of n linearly independent vectors $v_1^{(0)}, ..., v_n^{(0)}$, the space $span\{A^kv_1^{(0)}, ..., A^kv_n^{(0)}\}$ converges to the space $span\{q_1, ..., q_n\}$, i.e. the space spanned by the eigenvectors. And this is something I do not understand. Above it is stated, that for almost any arbitrary starting vector, $v$ would converge to the eigenvector corresponding to the largest eigenvalue. How can than the space spanned by several vectors span the same space as all eigenvectors? I would assume that the all will converge sooner or later to the same vector, thus only spanning a one-dimensional subspace. Where is my mistake?",,"['linear-algebra', 'numerical-methods', 'numerical-linear-algebra']"
44,Showing that a one-to-one linear transformation maps a linearly independent set onto a linearly independent set,Showing that a one-to-one linear transformation maps a linearly independent set onto a linearly independent set,,"Problem 4.3.32 in Linear Algebra, Lay: Let $V$ and $W$ be vector spaces, let $T:V\to W$ be a linear transformation, and let $\{\mathbf{v}_1, \dots , \mathbf{v}_p \}$ be a subset of $V$ . Suppose that $T$ is a one-to-one transformation [...]. Show that if the set of images $\{T(\mathbf{v}_1), \dots ,T(\mathbf{v}_p)\}$ is linearly dependent, then $\{\mathbf{v}_1, \dots , \mathbf{v}_p \}$ is linearly dependent. This fact shows that a one-to-one linear transformation maps a linearly independent set onto a linearly independent set. All I can think of doing: If $\{T(\mathbf{v}_1), \dots ,T(\mathbf{v}_p)\}$ is linearly dependent, then there exist weights $c_1, \dots, c_p$ , not all zero, so that $$c_1 T(\mathbf{v}_1) + \cdots + c_p T(\mathbf{v}_p) = \mathbf{0}$$ Since $T$ is linear: $$c_1 T(\mathbf{v}_1) + \cdots + c_p T(\mathbf{v}_p) = T(c_1\mathbf{v}_1) + \cdots + T(c_p \mathbf{v}_p)=T(c_1 \mathbf{v}_1 +\cdots + c_p \mathbf{v}_p)=\mathbf{0}$$ So the set $\{\mathbf{v}_1, \dots , \mathbf{v}_p \}$ must be linearly dependent. Am I missing something? And why must $T$ be on-to-one for this to be true? An example showing that it is not true if $T$ is not one-to-one would be great!","Problem 4.3.32 in Linear Algebra, Lay: Let and be vector spaces, let be a linear transformation, and let be a subset of . Suppose that is a one-to-one transformation [...]. Show that if the set of images is linearly dependent, then is linearly dependent. This fact shows that a one-to-one linear transformation maps a linearly independent set onto a linearly independent set. All I can think of doing: If is linearly dependent, then there exist weights , not all zero, so that Since is linear: So the set must be linearly dependent. Am I missing something? And why must be on-to-one for this to be true? An example showing that it is not true if is not one-to-one would be great!","V W T:V\to W \{\mathbf{v}_1, \dots , \mathbf{v}_p \} V T \{T(\mathbf{v}_1), \dots ,T(\mathbf{v}_p)\} \{\mathbf{v}_1, \dots , \mathbf{v}_p \} \{T(\mathbf{v}_1), \dots ,T(\mathbf{v}_p)\} c_1, \dots, c_p c_1 T(\mathbf{v}_1) + \cdots + c_p T(\mathbf{v}_p) = \mathbf{0} T c_1 T(\mathbf{v}_1) + \cdots + c_p T(\mathbf{v}_p) = T(c_1\mathbf{v}_1) + \cdots + T(c_p \mathbf{v}_p)=T(c_1 \mathbf{v}_1 +\cdots + c_p \mathbf{v}_p)=\mathbf{0} \{\mathbf{v}_1, \dots , \mathbf{v}_p \} T T",['linear-algebra']
45,Contexts For Bessel's Inequality?,Contexts For Bessel's Inequality?,,"Bessel's inequality appears to be about orthonormal sequences. But (in the context of inner product spaces), I've thought of this inequality as being a demonstration that the hypotenuse of triangles is always at least as long as its cosine projection (and only equal when the angle is zero and the cosine is 1). Are the two uses related? Are they different versions of the same thing or two totally different things? And is there a ""third"" context that I've missed?","Bessel's inequality appears to be about orthonormal sequences. But (in the context of inner product spaces), I've thought of this inequality as being a demonstration that the hypotenuse of triangles is always at least as long as its cosine projection (and only equal when the angle is zero and the cosine is 1). Are the two uses related? Are they different versions of the same thing or two totally different things? And is there a ""third"" context that I've missed?",,"['linear-algebra', 'functional-analysis', 'inequality', 'hilbert-spaces']"
46,Do the maximal eigenvalues of $(X^TX)^{-1}$ matrix increase when number of columns of $X$ increases?,Do the maximal eigenvalues of  matrix increase when number of columns of  increases?,(X^TX)^{-1} X,"Suppose we have a square real $n\times n$ matrix $X=[x_1,...,x_n]$, where $x_i$ is $i$-th column of the matrix. Now define  $X_k=[x_1,..,x_k]$, i.e. matrix $X_k$ columns are the first $k$ columns of the matrix $X$. Define $\lambda_k$ as the maximal eigenvalue of $(X_k^TX_k)^{-1}$. Is it possible to prove that $\lambda_1\le \lambda_2\le ... \le \lambda_n$? If $X$ is orthogonal, then the answer is yes. Maybe this holds only for certain matrices? Any pointers would be greatly appreciated.","Suppose we have a square real $n\times n$ matrix $X=[x_1,...,x_n]$, where $x_i$ is $i$-th column of the matrix. Now define  $X_k=[x_1,..,x_k]$, i.e. matrix $X_k$ columns are the first $k$ columns of the matrix $X$. Define $\lambda_k$ as the maximal eigenvalue of $(X_k^TX_k)^{-1}$. Is it possible to prove that $\lambda_1\le \lambda_2\le ... \le \lambda_n$? If $X$ is orthogonal, then the answer is yes. Maybe this holds only for certain matrices? Any pointers would be greatly appreciated.",,"['linear-algebra', 'matrices']"
47,Are there variations on least-squares approximations?,Are there variations on least-squares approximations?,,"In least-squares approximations the normal equations act to project a vector existing in N-dimensional space onto a lower dimensional space, where our problem actually lies, thus providing the ""best"" solution we can hope for (the orthogonal projection of the N-vector onto our solution space).  The ""best"" solution is the one that minimizes the Euclidean distance (two-norm) between the N-dimensional vector and our lower dimensional space. There exist other norms and other spaces besides $\mathbb{R}^d$, what are the analogues of least-squares under a different norm, or in a different space?","In least-squares approximations the normal equations act to project a vector existing in N-dimensional space onto a lower dimensional space, where our problem actually lies, thus providing the ""best"" solution we can hope for (the orthogonal projection of the N-vector onto our solution space).  The ""best"" solution is the one that minimizes the Euclidean distance (two-norm) between the N-dimensional vector and our lower dimensional space. There exist other norms and other spaces besides $\mathbb{R}^d$, what are the analogues of least-squares under a different norm, or in a different space?",,"['linear-algebra', 'big-list', 'approximation', 'regression']"
48,Is there an efficient way to check if there exists a linear map with the following property?,Is there an efficient way to check if there exists a linear map with the following property?,,"Suppose I have two sets of non-zero vectors: $S_1 =\{v_1, \dots v_n\}$ and $S_2 = \{w_1, \dots w_n \}$ lying in $\mathbb{R}^m$ where $m < n$ . Further suppose that any $m$ of the vectors from $S_1$ or from $S_2$ is a basis for $\mathbb{R}^m$ . I want to know if there's an elegant way of knowing whether there exists an invertible linear map $T$ such that $T(S_1) = T(S_2)$ . A naive way which I could theoretically write a program for is as follows: Fix any choice of $m$ vectors in $S_1$ which I please. Then any choice of $m$ vectors in $S_2$ induces a linear map $T$ . I can represent this map as a matrix and check if $T(S_1) = S_2$ or not. This requires the construction of $\binom{n}{m}m!$ linear maps, which I would likely represent as matrices in a program. Thus $\binom{n}{m}m!$ matrices of dimension $m \times m$ . That's quite the calculation if $n$ is large. Is there a faster way to do this? Given the simplicity of the problem, it seems like there should be well-known faster way to do this if it exists.","Suppose I have two sets of non-zero vectors: and lying in where . Further suppose that any of the vectors from or from is a basis for . I want to know if there's an elegant way of knowing whether there exists an invertible linear map such that . A naive way which I could theoretically write a program for is as follows: Fix any choice of vectors in which I please. Then any choice of vectors in induces a linear map . I can represent this map as a matrix and check if or not. This requires the construction of linear maps, which I would likely represent as matrices in a program. Thus matrices of dimension . That's quite the calculation if is large. Is there a faster way to do this? Given the simplicity of the problem, it seems like there should be well-known faster way to do this if it exists.","S_1 =\{v_1, \dots v_n\} S_2 = \{w_1, \dots w_n \} \mathbb{R}^m m < n m S_1 S_2 \mathbb{R}^m T T(S_1) = T(S_2) m S_1 m S_2 T T(S_1) = S_2 \binom{n}{m}m! \binom{n}{m}m! m \times m n","['linear-algebra', 'matrices', 'linear-transformations']"
49,Is an NVS complete iff it is non meagre?,Is an NVS complete iff it is non meagre?,,"Let $V$ be an NVS. We know by Baires theorem that complete $\implies$ nonmeagre. What about the converse? Can we have a non complete space that is nonmeagre or not? For metric spaces the answer is yes we can, just disjoint union a complete space and a meagre space e.g. $\mathbb{Q}\underline{\cup} \mathbb{R}$ , then the $\mathbb{Q}$ part makes it not complete and the $\mathbb{R}$ part makes it nonmeagre. But no similar tactic works for an NVS so I think the answer will be no.","Let be an NVS. We know by Baires theorem that complete nonmeagre. What about the converse? Can we have a non complete space that is nonmeagre or not? For metric spaces the answer is yes we can, just disjoint union a complete space and a meagre space e.g. , then the part makes it not complete and the part makes it nonmeagre. But no similar tactic works for an NVS so I think the answer will be no.",V \implies \mathbb{Q}\underline{\cup} \mathbb{R} \mathbb{Q} \mathbb{R},"['linear-algebra', 'general-topology', 'functional-analysis', 'normed-spaces', 'baire-category']"
50,Calculate the determinant of the matrix under the condition $x \neq y$,Calculate the determinant of the matrix under the condition,x \neq y,"Calculate the determinant of the matrix: $$\left|\begin{array}{ccccc} a_{1} & x & x & \ldots & x \\ y & a_{2} & x & \ldots & x \\ y & y & a_{3} & \ldots & x \\ \ldots & \ldots & \ldots & \ldots & \ldots \\ y & y & y & \ldots & a_{n} \end{array}\right|.$$ Attempts and thoughts : One of my good ""colleagues"" noticed that if $D_n$ is the determinant of an $n×n$ matrix, then $$D_{n}=P_{n}-x y \sum_{i=0}^{n-2}(-1)^{i} P_{n-2-i} X_{i}.$$ Here $X_k$ is the sum of all products, $x^iy^j$ , where $i+j=k$ . $P_k$ is the sum of all possible products involving exactly $k$ distinct $a_i$ values. Manually counting the determinants, you can see a certain pattern: $$ \begin{aligned} &D_{0}=P_{0} \\ &D_{1}=P_{1} \\ &D_{2}=P_{2}-x y[P_{0} X_{0}] \\ &D_{3}=P_{3}-x y[P_{1} X_{0}-P_{0} X_{1}] \\ &D_{4}=P_{4}-x y[P_{2} X_{0}-P_{1} X_{1}+P_{0} X_{2}]. \end{aligned} $$ This is true up to $n=10$ . I did not check further. However, a better option appeared on the horizon, namely $\displaystyle\frac{x f(y)-y f(x)}{x-y}$ , where $f(z)=(a_{1}-z)(a_{2}-z) \ldots(a_{n}-z)$ . And verily, these two expressions agree with each other. Option $1$ is more complicated and requires additional calculations, while this formula is much simpler and fully satisfies the condition of the problem. Question : I can prove the second formula by the method of mathematical induction. It is not very difficult, much easier than deriving this formula. And here's the question. How do you derive this formula?","Calculate the determinant of the matrix: Attempts and thoughts : One of my good ""colleagues"" noticed that if is the determinant of an matrix, then Here is the sum of all products, , where . is the sum of all possible products involving exactly distinct values. Manually counting the determinants, you can see a certain pattern: This is true up to . I did not check further. However, a better option appeared on the horizon, namely , where . And verily, these two expressions agree with each other. Option is more complicated and requires additional calculations, while this formula is much simpler and fully satisfies the condition of the problem. Question : I can prove the second formula by the method of mathematical induction. It is not very difficult, much easier than deriving this formula. And here's the question. How do you derive this formula?",\left|\begin{array}{ccccc} a_{1} & x & x & \ldots & x \\ y & a_{2} & x & \ldots & x \\ y & y & a_{3} & \ldots & x \\ \ldots & \ldots & \ldots & \ldots & \ldots \\ y & y & y & \ldots & a_{n} \end{array}\right|. D_n n×n D_{n}=P_{n}-x y \sum_{i=0}^{n-2}(-1)^{i} P_{n-2-i} X_{i}. X_k x^iy^j i+j=k P_k k a_i  \begin{aligned} &D_{0}=P_{0} \\ &D_{1}=P_{1} \\ &D_{2}=P_{2}-x y[P_{0} X_{0}] \\ &D_{3}=P_{3}-x y[P_{1} X_{0}-P_{0} X_{1}] \\ &D_{4}=P_{4}-x y[P_{2} X_{0}-P_{1} X_{1}+P_{0} X_{2}]. \end{aligned}  n=10 \displaystyle\frac{x f(y)-y f(x)}{x-y} f(z)=(a_{1}-z)(a_{2}-z) \ldots(a_{n}-z) 1,"['linear-algebra', 'matrices', 'determinant']"
51,How can we state Sylvester's law of inertia without referring to a particular basis?,How can we state Sylvester's law of inertia without referring to a particular basis?,,"In elementary linear algebra, we talk about matrices, i.e. rectangular arrays of numbers. In advanced linear algebra, we prefer whenever possible to talk about abstract tensors, such as linear operators or bilinear forms, without going into any particular basis. This framing is considered more elegant, and also allows for easier generalization to vector spaces more abstract than the simple case of $\mathbb{R}^n$ for finite $n$ . In particular, I've found that almost all theorems about matrices generalize fairly easily to more abstract theorems about linear operators and/or bilinear forms. (One strong hint as to which one is that matrix similarity is the natural expression of ""equivalence"" for linear operators, and matrix congruence is the natural expression of ""equivalence"" for bilinear forms. For an inner product space, the distinction is often blurred, because there's a natural isomorphism between linear operators and bilinear forms.) But I'm not quite how this works for Sylvester's law of inertia , because it seems to combine elements of both notions without apparently requiring any inner product on the vector space. Part 1 of Sylvester's law of inertia says that every real symmetric square matrix is congruent to exactly one diagonal matrix with entries 1, -1, and 0 (up to permutation). This pretty clearly seems to generalize to the Wikipedia article's basis-independent statement about real quadratic forms: For any real quadratic form $Q$ , every maximal subspace on which the restriction of $Q$ is positive definite (respectively, negative definite or totally isotropic) has the same dimension. But I'm confused about part 2 of Sylvester's law of inertia, which says that two symmetric square matrices are congruent iff they have the same numbers of positive, negative, and zero eigenvalues (which equal the numbers of entries 1, -1, and 0 defined in part 1). I'm not sure how to state this in a basis-independent way, because matrix congruence is a equivalence relation on representations of bilinear or quadratic forms , but the notion of eigenvalues only makes sense for linear operators . Moreover, the statement of the theorem doesn't seem to require any inner product that would allow you to naturally convert between the two types of tensors. Is part 2 of Sylvester's law of inertia a statement about forms or about linear operators, and how can we generalize it to tensors in a basis-independent way? The only solution I can think of, which seems to me to be very much a hack, is to introduce an arbitrary inner product - something like the following: Let $Q$ be an arbitrary quadratic form on a real vector space $V$ , and $B$ be the naturally associated symmetric bilinear form. Define an arbitrary inner product $\langle \rangle$ on V. Consider the unique linear operator $L_{B,\langle \rangle}$ on $V$ associated with $B$ , which maps any vector $v$ to the vector $u$ such that $B(v, w) = \langle u, w \rangle$ for all $w \in V$ . Then regardless of the choice of inner product $\langle \rangle$ , the numbers of positive, negative, and zero eigenvalues of $L_{B, \langle \rangle}$ equal the indices of inertia of $Q$ defined in part 1. To me, this proposition seems ... rather convoluted. Is there a simpler version?","In elementary linear algebra, we talk about matrices, i.e. rectangular arrays of numbers. In advanced linear algebra, we prefer whenever possible to talk about abstract tensors, such as linear operators or bilinear forms, without going into any particular basis. This framing is considered more elegant, and also allows for easier generalization to vector spaces more abstract than the simple case of for finite . In particular, I've found that almost all theorems about matrices generalize fairly easily to more abstract theorems about linear operators and/or bilinear forms. (One strong hint as to which one is that matrix similarity is the natural expression of ""equivalence"" for linear operators, and matrix congruence is the natural expression of ""equivalence"" for bilinear forms. For an inner product space, the distinction is often blurred, because there's a natural isomorphism between linear operators and bilinear forms.) But I'm not quite how this works for Sylvester's law of inertia , because it seems to combine elements of both notions without apparently requiring any inner product on the vector space. Part 1 of Sylvester's law of inertia says that every real symmetric square matrix is congruent to exactly one diagonal matrix with entries 1, -1, and 0 (up to permutation). This pretty clearly seems to generalize to the Wikipedia article's basis-independent statement about real quadratic forms: For any real quadratic form , every maximal subspace on which the restriction of is positive definite (respectively, negative definite or totally isotropic) has the same dimension. But I'm confused about part 2 of Sylvester's law of inertia, which says that two symmetric square matrices are congruent iff they have the same numbers of positive, negative, and zero eigenvalues (which equal the numbers of entries 1, -1, and 0 defined in part 1). I'm not sure how to state this in a basis-independent way, because matrix congruence is a equivalence relation on representations of bilinear or quadratic forms , but the notion of eigenvalues only makes sense for linear operators . Moreover, the statement of the theorem doesn't seem to require any inner product that would allow you to naturally convert between the two types of tensors. Is part 2 of Sylvester's law of inertia a statement about forms or about linear operators, and how can we generalize it to tensors in a basis-independent way? The only solution I can think of, which seems to me to be very much a hack, is to introduce an arbitrary inner product - something like the following: Let be an arbitrary quadratic form on a real vector space , and be the naturally associated symmetric bilinear form. Define an arbitrary inner product on V. Consider the unique linear operator on associated with , which maps any vector to the vector such that for all . Then regardless of the choice of inner product , the numbers of positive, negative, and zero eigenvalues of equal the indices of inertia of defined in part 1. To me, this proposition seems ... rather convoluted. Is there a simpler version?","\mathbb{R}^n n Q Q Q V B \langle \rangle L_{B,\langle \rangle} V B v u B(v, w) = \langle u, w \rangle w \in V \langle \rangle L_{B, \langle \rangle} Q","['linear-algebra', 'linear-transformations', 'quadratic-forms', 'symmetric-matrices', 'change-of-basis']"
52,Can such a matrix be singular?,Can such a matrix be singular?,,"Let $A$ be an $n\times n$ matrix that satisfies i. All diagonal entries of $A$ are positive, even integers, ii. All non-diagonal entries of $A$ are positive, odd integers, iii. $A$ is symmetric: $A_{ij}=A_{ji}$ . iv. If $i\neq j$ , then $A_{ij}<A_{ii}$ and $A_{ij}<A_{jj}$ . Question: can such $A$ be singular (i.e. $|A|=0$ )? I can prove the answer is NO, when $n$ is even, and without using condition iv: reduce $A$ mod $2$ we get a matrix with zero diagonal and all other entries $1$ ; then we can compute directly that the determinant of such matrix is odd, in particular not zero. When $n$ is odd this argument says $|A|$ is even, I don't know if it can be zero, and it seems condition iv will be important here. I encountered it when I was trying to solve a question in this post .","Let be an matrix that satisfies i. All diagonal entries of are positive, even integers, ii. All non-diagonal entries of are positive, odd integers, iii. is symmetric: . iv. If , then and . Question: can such be singular (i.e. )? I can prove the answer is NO, when is even, and without using condition iv: reduce mod we get a matrix with zero diagonal and all other entries ; then we can compute directly that the determinant of such matrix is odd, in particular not zero. When is odd this argument says is even, I don't know if it can be zero, and it seems condition iv will be important here. I encountered it when I was trying to solve a question in this post .",A n\times n A A A A_{ij}=A_{ji} i\neq j A_{ij}<A_{ii} A_{ij}<A_{jj} A |A|=0 n A 2 1 n |A|,"['linear-algebra', 'combinatorics', 'number-theory']"
53,For a hyper cube of dimension N what number of its vertices can be covered by intersection with a hyperplane,For a hyper cube of dimension N what number of its vertices can be covered by intersection with a hyperplane,,"If you look at intersecting a binary cube (the set contained by [0,1]^n) with a plane in $\mathbb{R}^3$ . Then the plane can potentially intersect with the corners of the cube. Depending on the choice of plane you might intersect with 0,1,2,3 or 4 corners, but it’s impossible to intersect with any other number of corners. If we shift our attention to $\mathbb{R}^4$ it gets a little trickier, all the $\mathbb{R}^3$ numbers must be valid but we also know that hyperplanes such as $x_0+x_1+x_2+x_3=2$ let you intersect with 6 corners. This leads to our problem: Given a dimension $n$ , what does the list of possible intersection counts of a hyperplane with vertices of the Boolean cube look like? Even for $n=4$ at best we can show the list includes (but as of right now I haven’t exhausted) 0,1,2,3,4,6,8. Powers of 2 and Pascal’s triangle numbers are all present but there definitely are more exotic coverings out there especially for $n=5$ onwards","If you look at intersecting a binary cube (the set contained by [0,1]^n) with a plane in . Then the plane can potentially intersect with the corners of the cube. Depending on the choice of plane you might intersect with 0,1,2,3 or 4 corners, but it’s impossible to intersect with any other number of corners. If we shift our attention to it gets a little trickier, all the numbers must be valid but we also know that hyperplanes such as let you intersect with 6 corners. This leads to our problem: Given a dimension , what does the list of possible intersection counts of a hyperplane with vertices of the Boolean cube look like? Even for at best we can show the list includes (but as of right now I haven’t exhausted) 0,1,2,3,4,6,8. Powers of 2 and Pascal’s triangle numbers are all present but there definitely are more exotic coverings out there especially for onwards",\mathbb{R}^3 \mathbb{R}^4 \mathbb{R}^3 x_0+x_1+x_2+x_3=2 n n=4 n=5,"['linear-algebra', 'combinatorics', 'geometry', 'euclidean-geometry', 'combinatorial-geometry']"
54,"Show that $e^{A}$ makes sense, $A$ be an $n \times n$ real matrix","Show that  makes sense,  be an  real matrix",e^{A} A n \times n,I have to solve: Let $A$ be an $n \times n$ real matrix. The exponential of $A$ is defined as $$ e^{A}:=I+A+\frac{A^{2}}{2 !}+\frac{A^{3}}{3 !}+\cdots $$ provided each entry of the matrix converges. Show that $e^{A}$ makes sense and $P$ is an invertible matrix show that $P e^{A} P^{-1}$ is also an exponential matrix. I have solved Part (2) as : This follows by noting that $$ \begin{aligned} \left(T B T^{-1}\right)^{k} &=\left(T B T^{-1}\right)\left(T B T^{-1}\right) \cdots\left(T B T^{-1}\right) \\ &=T B\left(T^{-1} T\right) B\left(T^{-1} T\right) \cdots\left(T^{-1} T\right) B T^{-1} \\ &=T B^{k} T^{-1} \end{aligned} $$ then yields $$ \begin{aligned} e^{P^{-1} A P} &=I+P^{-1} A P+\frac{\left(P^{-1} A P\right)^{2}}{2 !}+\cdots \\ &=I+P^{-1} A P+P^{-1} \frac{A^{2}}{2 !} P+\cdots \\ &=P^{-1}\left(I+A+\frac{A^{2}}{2 !}+\cdots\right) P=P^{-1} e^{A} P \end{aligned} $$ I don't know how to solve part (1) i.e Show that $e^{A}$ makes sense ?,I have to solve: Let be an real matrix. The exponential of is defined as provided each entry of the matrix converges. Show that makes sense and is an invertible matrix show that is also an exponential matrix. I have solved Part (2) as : This follows by noting that then yields I don't know how to solve part (1) i.e Show that makes sense ?,"A n \times n A 
e^{A}:=I+A+\frac{A^{2}}{2 !}+\frac{A^{3}}{3 !}+\cdots
 e^{A} P P e^{A} P^{-1} 
\begin{aligned}
\left(T B T^{-1}\right)^{k} &=\left(T B T^{-1}\right)\left(T B T^{-1}\right) \cdots\left(T B T^{-1}\right) \\
&=T B\left(T^{-1} T\right) B\left(T^{-1} T\right) \cdots\left(T^{-1} T\right) B T^{-1} \\
&=T B^{k} T^{-1}
\end{aligned}
 
\begin{aligned}
e^{P^{-1} A P} &=I+P^{-1} A P+\frac{\left(P^{-1} A P\right)^{2}}{2 !}+\cdots \\
&=I+P^{-1} A P+P^{-1} \frac{A^{2}}{2 !} P+\cdots \\
&=P^{-1}\left(I+A+\frac{A^{2}}{2 !}+\cdots\right) P=P^{-1} e^{A} P
\end{aligned}
 e^{A}","['linear-algebra', 'matrices', 'exponential-function', 'matrix-equations']"
55,"Prove that the set ${\{} \frac{1}{x-c}{\}}_{\displaystyle\ c \in \mathbb{R}\setminus[0,1]}$ is linearly independent.",Prove that the set  is linearly independent.,"{\{} \frac{1}{x-c}{\}}_{\displaystyle\ c \in \mathbb{R}\setminus[0,1]}","Question: Let $V$ be the vector space of all real valued functions defined on the unit interval $[0,1]$ . Show that the set $\displaystyle\ \bigg{\{} \frac{1}{x-c}\bigg{\}}_{\ c \in \mathbb{R}\setminus[0,1]}$ is linearly independent. Attempt: Assume towards a contradiction that the set is linearly dependent. So, $\exists$ a finite subset $\displaystyle\bigg{\{}\frac{1}{x-c_i}\bigg{\}}_{i=1}^n$ which is linearly dependent. Therefore, for some $(d_1,d_2,...,d_n)\neq (0,0,..,0)$ we must have $\displaystyle f(x)=\sum_{i=1}^n\frac{d_i}{x-c_i}=\frac{d_1(x-c_2)...(x-c_n)+d_2(x-c_1)(x-c_3)...(x-c_n)+...+d_n(x-c_1)...(x-c_{n-1})}{(x-c_1)...(x-c_n)}=0$ for all $x\in[0,1]$ . Now, the denominator is $\neq 0$ $\forall x\in [0,1]$ . So, $f(x)=0$ only when $d_1(x-c_2)...(x-c_n)+d_2(x-c_1)(x-c_3)...(x-c_n)+...+d_n(x-c_1)...(x-c_{n-1})=g(x)=0$ . However, $g(x)$ is a polynomial of degree $\leq n-1$ and $g(x)=0$ for every $x \in[0,1]$ , which implies that number of zeros of $g(x)$ is $>deg(g(x))$ , hence $g(x)$ must be identically equal to $0 \implies (d_1,d_2,...,d_n)= (0,0,..,0) $ . Therefore, our assumption that the given set is linearly dependent is not tenable, i.e. the set is linearly independent. Is this correct?","Question: Let be the vector space of all real valued functions defined on the unit interval . Show that the set is linearly independent. Attempt: Assume towards a contradiction that the set is linearly dependent. So, a finite subset which is linearly dependent. Therefore, for some we must have for all . Now, the denominator is . So, only when . However, is a polynomial of degree and for every , which implies that number of zeros of is , hence must be identically equal to . Therefore, our assumption that the given set is linearly dependent is not tenable, i.e. the set is linearly independent. Is this correct?","V [0,1] \displaystyle\ \bigg{\{} \frac{1}{x-c}\bigg{\}}_{\ c \in \mathbb{R}\setminus[0,1]} \exists \displaystyle\bigg{\{}\frac{1}{x-c_i}\bigg{\}}_{i=1}^n (d_1,d_2,...,d_n)\neq (0,0,..,0) \displaystyle f(x)=\sum_{i=1}^n\frac{d_i}{x-c_i}=\frac{d_1(x-c_2)...(x-c_n)+d_2(x-c_1)(x-c_3)...(x-c_n)+...+d_n(x-c_1)...(x-c_{n-1})}{(x-c_1)...(x-c_n)}=0 x\in[0,1] \neq 0 \forall x\in [0,1] f(x)=0 d_1(x-c_2)...(x-c_n)+d_2(x-c_1)(x-c_3)...(x-c_n)+...+d_n(x-c_1)...(x-c_{n-1})=g(x)=0 g(x) \leq n-1 g(x)=0 x \in[0,1] g(x) >deg(g(x)) g(x) 0 \implies (d_1,d_2,...,d_n)= (0,0,..,0) ","['linear-algebra', 'polynomials', 'solution-verification']"
56,A problem on subspaces,A problem on subspaces,,I was studying for some quals and I remember running into this problem last year and I couldn't get anywhere with it. Even now I'm kind of stumped. I was wondering if you guys had any ideas. Here's the problem: Let $ V $ be a vector space and let $ 1\leq n< \operatorname{dim}(V) $ be an integer. Let $ \{V_i\} $ be a collection of $ n $ -dimensional subspaces of $ V $ with the property that $$ \operatorname{dim}(V_i\cap V_j) = n-1 $$ for every $ i\neq j $ . Show that at least one of the following holds: (i) All $ V_i $ share a common $ (n-1) $ -dimensional subspace. (ii) There is an $ (n+1) $ -dimensional subspace of $ V $ containing all $ V_i $ .,I was studying for some quals and I remember running into this problem last year and I couldn't get anywhere with it. Even now I'm kind of stumped. I was wondering if you guys had any ideas. Here's the problem: Let be a vector space and let be an integer. Let be a collection of -dimensional subspaces of with the property that for every . Show that at least one of the following holds: (i) All share a common -dimensional subspace. (ii) There is an -dimensional subspace of containing all ., V   1\leq n< \operatorname{dim}(V)   \{V_i\}   n   V   \operatorname{dim}(V_i\cap V_j) = n-1   i\neq j   V_i   (n-1)   (n+1)   V   V_i ,['linear-algebra']
57,Invertibility of infinite-dimensional matrix,Invertibility of infinite-dimensional matrix,,"I have a matrix $M \in \mathbb{R}^{n \times n}$ whose columns are linearly independent. Hence, $M$ is invertible. How to extend this conclusion to the case where $n$ is infinite? Specifically, given that $n\in\mathbb{N}$ , let $X$ and $Y$ be Banach spaces. $x\in X$ and $y \in Y$ satisfy that \begin{align} y = M x. \end{align} What conditions do I need to conclude that $M$ is a bounded invertible linear operator? p.s. If $n$ is finite, it seems that the conclusion hold when $M$ is of full rank and with bounded matrix norm. If $n$ is infinite, what arguments can I use?","I have a matrix whose columns are linearly independent. Hence, is invertible. How to extend this conclusion to the case where is infinite? Specifically, given that , let and be Banach spaces. and satisfy that What conditions do I need to conclude that is a bounded invertible linear operator? p.s. If is finite, it seems that the conclusion hold when is of full rank and with bounded matrix norm. If is infinite, what arguments can I use?","M \in \mathbb{R}^{n \times n} M n n\in\mathbb{N} X Y x\in X y \in Y \begin{align}
y = M x.
\end{align} M n M n","['linear-algebra', 'matrices', 'inverse', 'matrix-analysis', 'infinite-matrices']"
58,Computing challenging determinant,Computing challenging determinant,,"Seeking advice on how to tackle the below determinant $$ \begin{vmatrix} a_1b_1 & a_1b_2 & a_1b_3 & \ldots & a_1b_n \\ a_1b_2 & a_2b_2 & a_2b_3 & \ldots & a_2b_n \\ a_1b_3 & a_2b_3 & a_3b_3 & \ldots & a_3b_n \\ \ldots & \ldots & \ldots & \ddots & \vdots\\ a_1b_n & a_2b_n & a_3b_n & \ldots & a_nb_n\end{vmatrix} $$ I have tried countless techniques, including cofactor expansion as well as row/column swapping. I also tried to exploit the symmetricness of the matrix. I have looked elsewhere for guidance however this is apparently quite rare. Any help or advice would be appreciated.","Seeking advice on how to tackle the below determinant I have tried countless techniques, including cofactor expansion as well as row/column swapping. I also tried to exploit the symmetricness of the matrix. I have looked elsewhere for guidance however this is apparently quite rare. Any help or advice would be appreciated.","
\begin{vmatrix} a_1b_1 & a_1b_2 & a_1b_3 & \ldots & a_1b_n \\ a_1b_2 & a_2b_2 & a_2b_3 & \ldots & a_2b_n \\ a_1b_3 & a_2b_3 & a_3b_3 & \ldots & a_3b_n \\ \ldots & \ldots & \ldots & \ddots & \vdots\\ a_1b_n & a_2b_n & a_3b_n & \ldots & a_nb_n\end{vmatrix}
","['linear-algebra', 'determinant']"
59,A problem regarding the rank of a matrix,A problem regarding the rank of a matrix,,"$\mathbf {The \ Problem \ is}:$ If $A$ and $B$ be two $n\times n$ square matrix such that $A^2=A$ and $B^2=B,$ then show that $r(A-B)=r(A-AB)+r(AB-B)$ where $r(A)$ denotes rank of the square matrix $A.$ $\mathbf {My \ approach} :$ Actually I have tried that, from the two equations, $A(A-B)=(A-AB)$ and $(A-B)B=(AB-B)$ , we have $r(A-AB)+r(AB-B) \leq r(A)+r(B)$ and again $A(AB-B)=0$ and $(A-AB)B=0$ , then $r(A)+r(AB-B)\leq n$ and $r(B)+(A-AB)B\leq n$ , but I can't draw any further conclusion to show that $r(A-AB)+r(AB-B) \leq r(A-B)$ . And the other side is obvious by the rank-inequality $r(P+Q)\leq r(P)+r(Q).$ A small hint is warmly appreciated .","If and be two square matrix such that and then show that where denotes rank of the square matrix Actually I have tried that, from the two equations, and , we have and again and , then and , but I can't draw any further conclusion to show that . And the other side is obvious by the rank-inequality A small hint is warmly appreciated .","\mathbf {The \ Problem \ is}: A B n\times n A^2=A B^2=B, r(A-B)=r(A-AB)+r(AB-B) r(A) A. \mathbf {My \ approach} : A(A-B)=(A-AB) (A-B)B=(AB-B) r(A-AB)+r(AB-B) \leq r(A)+r(B) A(AB-B)=0 (A-AB)B=0 r(A)+r(AB-B)\leq n r(B)+(A-AB)B\leq n r(A-AB)+r(AB-B) \leq r(A-B) r(P+Q)\leq r(P)+r(Q).","['linear-algebra', 'matrices', 'matrix-rank']"
60,"If $H$ Hilbert, $A\colon H\to H$ bounded and $A(M)$ closed for all closed subspaces $M$, then $A(H)$ and $\ker(A)$ not both infinite dimensional?","If  Hilbert,  bounded and  closed for all closed subspaces , then  and  not both infinite dimensional?",H A\colon H\to H A(M) M A(H) \ker(A),"Let $H$ be a Hilbert space over $\mathbb{C}$ and $A\colon H\to H$ a bounded linear map. Assume that $A(M)$ is closed for all closed subspaces $M$ of $H$ . How do I prove that $A(H)$ and $\ker(A)$ cannot both be infinite dimensional? As the hint of the exercise suggests, I tried to construct an orthonormal sequence $u_{n}:=a_{n}x_{n}+b_{n}y_{n}$ for some suitable scalars $a_{n},b_{n}\in\mathbb{C}$ , and orthonormal sequences $x_{n}\in\ker(A)^{\perp}$ and $y_{n}\in\ker(A)$ . Then the closed linear span $M$ of $\{u_{n}\}_{n}$ should yield a contradiction. So if $\ker(A)$ is infinite dimensional, we can indeed find an orthonormal sequence $(y_{n})$ in $\ker(A)$ . I'm not sure why we can find one in $\ker(A)^{\perp}$ yet, but assume that it is possible and choose an orthonormal sequence in $(x_{n})$ in $\ker(A)^{\perp}$ . Then choose a real sequence $a\in\ell^{1}(\mathbb{N})$ with $a_{n}^{2}\leq1$ and define another sequence by $b_{n}:=(1-a_{n}^{2})^{1/2}$ (for example $a_{n}=1/n^{2}$ ). Then $u_{n}:=a_{n}x_{n}+b_{n}y_{n}$ is an orthonormal sequence. Then define $$v_{m}:=\sum_{n=1}^{m}u_{n}$$ and note that $$Av_{m}=\sum_{n=1}^{m}\alpha_{n}Ax_{n}\qquad\text{(since $Ay_{n}=0$)}.$$ The sequence $(v_{m})$ does not converge because it is a sum of pairwise orthonormal vectors. Now I was hoping to conclude that $(Av_{m})$ does converge and that the limit does not lie in $A(M)$ , where $M$ is closed span of $\{u_{n}\}_{n}$ . Is my reasoning right? And if it is, how do I finish the proof? Any suggestions are greatly appreciated.","Let be a Hilbert space over and a bounded linear map. Assume that is closed for all closed subspaces of . How do I prove that and cannot both be infinite dimensional? As the hint of the exercise suggests, I tried to construct an orthonormal sequence for some suitable scalars , and orthonormal sequences and . Then the closed linear span of should yield a contradiction. So if is infinite dimensional, we can indeed find an orthonormal sequence in . I'm not sure why we can find one in yet, but assume that it is possible and choose an orthonormal sequence in in . Then choose a real sequence with and define another sequence by (for example ). Then is an orthonormal sequence. Then define and note that The sequence does not converge because it is a sum of pairwise orthonormal vectors. Now I was hoping to conclude that does converge and that the limit does not lie in , where is closed span of . Is my reasoning right? And if it is, how do I finish the proof? Any suggestions are greatly appreciated.","H \mathbb{C} A\colon H\to H A(M) M H A(H) \ker(A) u_{n}:=a_{n}x_{n}+b_{n}y_{n} a_{n},b_{n}\in\mathbb{C} x_{n}\in\ker(A)^{\perp} y_{n}\in\ker(A) M \{u_{n}\}_{n} \ker(A) (y_{n}) \ker(A) \ker(A)^{\perp} (x_{n}) \ker(A)^{\perp} a\in\ell^{1}(\mathbb{N}) a_{n}^{2}\leq1 b_{n}:=(1-a_{n}^{2})^{1/2} a_{n}=1/n^{2} u_{n}:=a_{n}x_{n}+b_{n}y_{n} v_{m}:=\sum_{n=1}^{m}u_{n} Av_{m}=\sum_{n=1}^{m}\alpha_{n}Ax_{n}\qquad\text{(since Ay_{n}=0)}. (v_{m}) (Av_{m}) A(M) M \{u_{n}\}_{n}","['linear-algebra', 'sequences-and-series', 'functional-analysis', 'hilbert-spaces', 'orthonormal']"
61,How do you determine the characteristic polynomial of a permutation matrix based on the cycle type of the corresponding permutation??,How do you determine the characteristic polynomial of a permutation matrix based on the cycle type of the corresponding permutation??,,"I read in a paper that you could use the following equation to find the characteristic polynomial of any permutation matrix using the cycle type of the corresponding permutation, but did not understand what $n$ , $k$ , or $C_k$ stand for in the context of the equation: $$ p(\lambda) = \det(M \sigma − λI) = (−1)^n \prod_{k=1}^{n}(\lambda^k − 1)^{C_k} $$ Could someone please explain to me using a simple example such as (1 2 3) what numbers to plug in for $n$ , $k$ , and $C_k$ to arrive at the correct characteristic polynomial for the corresponding matrix (in this case [[0 0 1], [1 0 0], [0 1 0]])? Here is a link to the paper if this helps: https://www.math.arizona.edu/~ura-reports/003/blair-stahn/rpmevals.pdf Thank you so much for your help!","I read in a paper that you could use the following equation to find the characteristic polynomial of any permutation matrix using the cycle type of the corresponding permutation, but did not understand what , , or stand for in the context of the equation: Could someone please explain to me using a simple example such as (1 2 3) what numbers to plug in for , , and to arrive at the correct characteristic polynomial for the corresponding matrix (in this case [[0 0 1], [1 0 0], [0 1 0]])? Here is a link to the paper if this helps: https://www.math.arizona.edu/~ura-reports/003/blair-stahn/rpmevals.pdf Thank you so much for your help!",n k C_k  p(\lambda) = \det(M \sigma − λI) = (−1)^n \prod_{k=1}^{n}(\lambda^k − 1)^{C_k}  n k C_k,"['linear-algebra', 'abstract-algebra', 'matrices', 'permutation-cycles', 'permutation-matrices']"
62,determinant of a tricky matrix,determinant of a tricky matrix,,"I'm doing a research on matrix integrators and I ran into a problem in one particular case. To finish my proof the last thing remaining is to prove the nonsingularity of a specific matrix $$M_n: (m_{ij} = \frac{1}{a_i - a_j}, 1\leq i \leq n, 1\leq j \leq n,i\neq j;m_{ii} = \frac{c}{a_i - b} + \sum\limits_{k\neq i, 1\leq k \leq n}\frac{1}{a_i - a_k}),$$ where all $a_i, b$ are distinct. To be more clear I provide $$M_2 = \begin{pmatrix} \frac{c}{a_1 -b} + \frac{1}{a_1 - a_2} && \frac{1}{a_1 - a_2}\\ \frac{1}{a_2 - a_1} && \frac{c}{a_2 -b} + \frac{1}{a_2 - a_1} \end{pmatrix}$$ $$M_3 = \begin{pmatrix} \frac{c}{a_1 -b} + \frac{1}{a_1 - a_2} + \frac{1}{a_1 - a_3} && \frac{1}{a_1 - a_2} && \frac{1}{a_1 - a_3}\\ \frac{1}{a_2 - a_1} && \frac{c}{a_2 -b} + \frac{1}{a_2 - a_1} + \frac{1}{a_2 - a_3} && \frac{1}{a_2 - a_3}\\ \frac{1}{a_3 - a_1} && \frac{1}{a_3 - a_2} && \frac{c}{a_3 - b} + \frac{1}{a_3 - a_1} + \frac{1}{a_3 - a_2} \end{pmatrix}$$ For $n \leq 7$ I calculated the $det(M_n) = \frac{c(c+1)...(c + n -1)}{\prod\limits_{1\leq i\leq n}(a_i - b)}$ , but I have no idea how to prove this in general case. I my particular case $c\in \mathbb N$ , so this formula will prove the nonsingularity of $M_n$ . Any ideas and tips to prove the formula, or even to prove nonsingularity of $M_n$ in some other way - are very appreciated","I'm doing a research on matrix integrators and I ran into a problem in one particular case. To finish my proof the last thing remaining is to prove the nonsingularity of a specific matrix where all are distinct. To be more clear I provide For I calculated the , but I have no idea how to prove this in general case. I my particular case , so this formula will prove the nonsingularity of . Any ideas and tips to prove the formula, or even to prove nonsingularity of in some other way - are very appreciated","M_n: (m_{ij} = \frac{1}{a_i - a_j}, 1\leq i \leq n, 1\leq j \leq n,i\neq j;m_{ii} = \frac{c}{a_i - b} + \sum\limits_{k\neq i, 1\leq k \leq n}\frac{1}{a_i - a_k}), a_i, b M_2 = \begin{pmatrix}
\frac{c}{a_1 -b} + \frac{1}{a_1 - a_2} && \frac{1}{a_1 - a_2}\\
\frac{1}{a_2 - a_1} && \frac{c}{a_2 -b} + \frac{1}{a_2 - a_1}
\end{pmatrix} M_3 = \begin{pmatrix}
\frac{c}{a_1 -b} + \frac{1}{a_1 - a_2} + \frac{1}{a_1 - a_3} && \frac{1}{a_1 - a_2} && \frac{1}{a_1 - a_3}\\
\frac{1}{a_2 - a_1} && \frac{c}{a_2 -b} + \frac{1}{a_2 - a_1} + \frac{1}{a_2 - a_3} && \frac{1}{a_2 - a_3}\\
\frac{1}{a_3 - a_1} && \frac{1}{a_3 - a_2} && \frac{c}{a_3 - b} + \frac{1}{a_3 - a_1} + \frac{1}{a_3 - a_2}
\end{pmatrix} n \leq 7 det(M_n) = \frac{c(c+1)...(c + n -1)}{\prod\limits_{1\leq i\leq n}(a_i - b)} c\in \mathbb N M_n M_n","['linear-algebra', 'matrices', 'determinant']"
63,Inverse of a Gram matrix,Inverse of a Gram matrix,,"If $G$ is the Gram matrix of $n$ vectors of size $d$ that are linearly independent ( $n\leq d$ ), then $G$ is invertible and its inverse is a Gram matrix. Of what? To be more precise, if $G$ is the Gram matrix of $u_1,\ldots,u_n$ , where each $u_i\in\mathbb R^d$ and $u_1,\ldots,u_n$ are linearly independent, then there exists a collection of $n$ linearly independent vectors $v_1,\ldots,v_n$ whose $G^{-1}$ is the Gram matrix. Is there some relationship between the $u$ 's and the $v$ 's?","If is the Gram matrix of vectors of size that are linearly independent ( ), then is invertible and its inverse is a Gram matrix. Of what? To be more precise, if is the Gram matrix of , where each and are linearly independent, then there exists a collection of linearly independent vectors whose is the Gram matrix. Is there some relationship between the 's and the 's?","G n d n\leq d G G u_1,\ldots,u_n u_i\in\mathbb R^d u_1,\ldots,u_n n v_1,\ldots,v_n G^{-1} u v","['linear-algebra', 'matrix-calculus']"
64,Square matrix with rational coefficients having $k$-th root,Square matrix with rational coefficients having -th root,k,"Let $A\in M_{n}(\mathbb{Q})$, meaning that $A$ is $n\times n$ matrix with entries in the rational numbers $\mathbb{Q}$. Suppose that $A$ satisfies two conditions: $\det(A)\neq 0$ For every integer $k$, there exists a $B\in M_{n}(\mathbb{Q})$ such that $B^k = A$. Does it follow that $A = I_n$? Here, $I_n$ is the $n\times n$ identity matrix. Motivation. The analogous problem where $\mathbb{Q}$ everywhere is replaced by $\mathbb{Z}$ has an affirmative answer. More precisely, let's prove the following statement: Claim. Let $A\in M_{n}(\mathbb{Z})$, meaning that $A$ is $n\times n$ matrix with integer entries. Suppose that $\det(A)\neq 0$ and that for every integer $k$, there exists $B\in M_{n}(\mathbb{Z})$ such that $B^k = A$. Then $A = I_n$. Proof. Since $\det(A)\neq 0$, there are only finitely many primes dividing $\det(A)$. Let $p$ be any prime which does not divide $\det(A)$. After reducing mod $p$, we still get a non-singular matrix $\overline{A}$, so $\overline{A}\in\operatorname{GL}_{n}(\mathbb{Z}/p\mathbb{Z})$. Let $k=\# \operatorname{GL}_{n}(\mathbb{Z}/p\mathbb{Z})$ be the cardinality of the group of invertible matrices with entries in $\mathbb{Z}/p\mathbb{Z}$. By hypothesis, there exists $B\in M_{n}(\mathbb{Z})$ such that $B^k=A$. After reducing mod $p$, we get $(\overline{B})^k = \overline{A}$. By Lagrange's theorem, $(\overline{B})^k = \overline{I_{n}}$. Therefore, $\overline{A} = \overline{I_{n}}$, so that each entry of $A-I_{n}$ is divisible by $p$. Since we have infinitely many choices for the prime $p$, it follows that each entry of $A-I_{n}$ must in fact be zero, yielding $A=I_n$. Remark 1. The hypothesis $\det(A)\neq 0$ is necessary. Otherwise, we can just take any diagonal matrix $A= \operatorname{diag}(1, 1, .. 1, 0, 0 .., 0)$ with any number of $1$s and $0$s. Certainly $A^k = A$ for every $k$ but $A\neq I_{n}$ if there is at least one zero on the diagonal. Remark 2. It is tempting to modify the above argument for the case of $\mathbb{Q}$, but it doesn't seem to work immediately. Again using the fact that $\det(A)\neq 0$, there are only finitely many primes appearing in the numerator and denominator of $\det(A)$, so we can pick a prime $p$ which doesn't appear there. Reducing the original matrix $A$ mod $p$ still works fine. But the problem arises at the step when we use hypothesis to get a matrix $B$ such that $B^k = A$ (where $k$ clearly depends on p -- this is important). That matrix $B$ might have the prime $p$ show up in the denominator of some of its entries, so reduction mod $p$ doesn't make sense. Reference. I learnt the proof above from ""Mathematical Bridges"" by Andreescu, Mortici and Tetiva. The problem appears as Exercise 18 in Chapter 6, and the solution is presented few pages later. So I am also adding the (contest-math) tag, since that is the theme of the aforementioned book.","Let $A\in M_{n}(\mathbb{Q})$, meaning that $A$ is $n\times n$ matrix with entries in the rational numbers $\mathbb{Q}$. Suppose that $A$ satisfies two conditions: $\det(A)\neq 0$ For every integer $k$, there exists a $B\in M_{n}(\mathbb{Q})$ such that $B^k = A$. Does it follow that $A = I_n$? Here, $I_n$ is the $n\times n$ identity matrix. Motivation. The analogous problem where $\mathbb{Q}$ everywhere is replaced by $\mathbb{Z}$ has an affirmative answer. More precisely, let's prove the following statement: Claim. Let $A\in M_{n}(\mathbb{Z})$, meaning that $A$ is $n\times n$ matrix with integer entries. Suppose that $\det(A)\neq 0$ and that for every integer $k$, there exists $B\in M_{n}(\mathbb{Z})$ such that $B^k = A$. Then $A = I_n$. Proof. Since $\det(A)\neq 0$, there are only finitely many primes dividing $\det(A)$. Let $p$ be any prime which does not divide $\det(A)$. After reducing mod $p$, we still get a non-singular matrix $\overline{A}$, so $\overline{A}\in\operatorname{GL}_{n}(\mathbb{Z}/p\mathbb{Z})$. Let $k=\# \operatorname{GL}_{n}(\mathbb{Z}/p\mathbb{Z})$ be the cardinality of the group of invertible matrices with entries in $\mathbb{Z}/p\mathbb{Z}$. By hypothesis, there exists $B\in M_{n}(\mathbb{Z})$ such that $B^k=A$. After reducing mod $p$, we get $(\overline{B})^k = \overline{A}$. By Lagrange's theorem, $(\overline{B})^k = \overline{I_{n}}$. Therefore, $\overline{A} = \overline{I_{n}}$, so that each entry of $A-I_{n}$ is divisible by $p$. Since we have infinitely many choices for the prime $p$, it follows that each entry of $A-I_{n}$ must in fact be zero, yielding $A=I_n$. Remark 1. The hypothesis $\det(A)\neq 0$ is necessary. Otherwise, we can just take any diagonal matrix $A= \operatorname{diag}(1, 1, .. 1, 0, 0 .., 0)$ with any number of $1$s and $0$s. Certainly $A^k = A$ for every $k$ but $A\neq I_{n}$ if there is at least one zero on the diagonal. Remark 2. It is tempting to modify the above argument for the case of $\mathbb{Q}$, but it doesn't seem to work immediately. Again using the fact that $\det(A)\neq 0$, there are only finitely many primes appearing in the numerator and denominator of $\det(A)$, so we can pick a prime $p$ which doesn't appear there. Reducing the original matrix $A$ mod $p$ still works fine. But the problem arises at the step when we use hypothesis to get a matrix $B$ such that $B^k = A$ (where $k$ clearly depends on p -- this is important). That matrix $B$ might have the prime $p$ show up in the denominator of some of its entries, so reduction mod $p$ doesn't make sense. Reference. I learnt the proof above from ""Mathematical Bridges"" by Andreescu, Mortici and Tetiva. The problem appears as Exercise 18 in Chapter 6, and the solution is presented few pages later. So I am also adding the (contest-math) tag, since that is the theme of the aforementioned book.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'group-theory', 'contest-math']"
65,Actual example of tensor contraction,Actual example of tensor contraction,,"So I'm having trouble to compute tensor contractions with ""actual"" numbers from the matrix representations of the tensors. I have only seen abstract theoretical examples on the internet so I'm asking for a bit of help on how to find the contractions given the expressions of the tensors and the pair of indices where we will carry out the contraction. I'll show a simple example and I hope you can help me. Let's suppose we have two tensors of the type (1,1) (that means 1 contravariant, 1 covariant). They will be called Y and Z and knowing their coordinate forms, we can represent them through matrices in this way: Y = \begin{pmatrix}1&-1\\2&3\end{pmatrix} Z = \begin{pmatrix}-1&0\\1&2\end{pmatrix} Now, we could compute the Kronecker product Y  x  Z in order to get a type (2,2) tensor (2 contravariant, 2 covariant). The result would be: \begin{pmatrix}-1&0&1&0\\1&2&-1&-2\\-2&0&-3&0\\2&4&3&6\end{pmatrix} So, how could we get the contractions of this (2,2) tensor (with actual numbers instead of the parameters that you see in other examples) ?. Note that here we have 2 contravariant indices and 2 covariant indices so there are 4 possible contractions depending on the pair of indices that you choose, I guess. As far as I know, all possible contractions with the selection of pairs of indices are the following: - 1st covariant index and 1st contravariant index. - 1st covariant index and 2nd contravariant index. - 2nd covariant index and 1st contravariant index. - 2nd covariant index and 2nd contravariant index. So what matrices would be the result of those contractions. Just in case I didn't state it clearly before, these matrices are the representation of tensor of which we know the coefficients of their coordinate forms (explicit forms in a specific basis, and that info is known). So we want to perform contractions in the last 4x4 matrix that I showed (which represents a (2,2) tensor) and there a 4 different possibilities. My question has to do with the fact that I can't find a way to do this with actual numbers and I can't figure out the result for each possible contraction (I also don't know what the differences would be in the calculation of a contraction with a certain pair of indices or another). I would really appreciate that someone could find a specific result for the contractions that I proposed (I guess the calculation is easy but I just don't know how it could be done). Thank you really much for reading.","So I'm having trouble to compute tensor contractions with ""actual"" numbers from the matrix representations of the tensors. I have only seen abstract theoretical examples on the internet so I'm asking for a bit of help on how to find the contractions given the expressions of the tensors and the pair of indices where we will carry out the contraction. I'll show a simple example and I hope you can help me. Let's suppose we have two tensors of the type (1,1) (that means 1 contravariant, 1 covariant). They will be called Y and Z and knowing their coordinate forms, we can represent them through matrices in this way: Y = \begin{pmatrix}1&-1\\2&3\end{pmatrix} Z = \begin{pmatrix}-1&0\\1&2\end{pmatrix} Now, we could compute the Kronecker product Y  x  Z in order to get a type (2,2) tensor (2 contravariant, 2 covariant). The result would be: \begin{pmatrix}-1&0&1&0\\1&2&-1&-2\\-2&0&-3&0\\2&4&3&6\end{pmatrix} So, how could we get the contractions of this (2,2) tensor (with actual numbers instead of the parameters that you see in other examples) ?. Note that here we have 2 contravariant indices and 2 covariant indices so there are 4 possible contractions depending on the pair of indices that you choose, I guess. As far as I know, all possible contractions with the selection of pairs of indices are the following: - 1st covariant index and 1st contravariant index. - 1st covariant index and 2nd contravariant index. - 2nd covariant index and 1st contravariant index. - 2nd covariant index and 2nd contravariant index. So what matrices would be the result of those contractions. Just in case I didn't state it clearly before, these matrices are the representation of tensor of which we know the coefficients of their coordinate forms (explicit forms in a specific basis, and that info is known). So we want to perform contractions in the last 4x4 matrix that I showed (which represents a (2,2) tensor) and there a 4 different possibilities. My question has to do with the fact that I can't find a way to do this with actual numbers and I can't figure out the result for each possible contraction (I also don't know what the differences would be in the calculation of a contraction with a certain pair of indices or another). I would really appreciate that someone could find a specific result for the contractions that I proposed (I guess the calculation is easy but I just don't know how it could be done). Thank you really much for reading.",,"['linear-algebra', 'tensor-products', 'tensors', 'tensor-rank', 'tensor-decomposition']"
66,"Matrix with permutations of $\{1,\dots,n\}$ is invertible",Matrix with permutations of  is invertible,"\{1,\dots,n\}","Clearly the $1 \times 1$ and the $2 \times 2$ matrices $(1)$ and $\begin{pmatrix} 1 & 2\\ 2 & 1 \end{pmatrix}$ are invertible. I am wondering about the following: if I take any matrix that has in the first line $(1 \ 2 \ \dots \ n)$ and in each other line a different permutation of the numbers $\{1, \dots,n\}$, will it always be invertible? I think this is wrong for higher $n$; it might have to do with the fact that we get more permutations than we can fit in the rows and with the Leibniz formula. But still, it works with all (or most) $3 \times 3$ cases, so I am really getting curious. If it turns out that this does not work, I would still be interested in knowing if there is at least one way of doing it that works (for every $n$) and if it is relevant that we have chosen $\{1,\dots,n\}$ and not any other (real, different, positive) entries. Edit: As pointed out in the comments and in user1551's answer, this is not always the case, not even when in each column and in each row each element appears exactly once. I would still like to know why all the $3 \times 3$ matrices (of this kind) are always invertible and, above all, why the ""shift matrix"" mentioned in Hw Chu's comment is indeed always invertible (and whether there are other procedures that always work).","Clearly the $1 \times 1$ and the $2 \times 2$ matrices $(1)$ and $\begin{pmatrix} 1 & 2\\ 2 & 1 \end{pmatrix}$ are invertible. I am wondering about the following: if I take any matrix that has in the first line $(1 \ 2 \ \dots \ n)$ and in each other line a different permutation of the numbers $\{1, \dots,n\}$, will it always be invertible? I think this is wrong for higher $n$; it might have to do with the fact that we get more permutations than we can fit in the rows and with the Leibniz formula. But still, it works with all (or most) $3 \times 3$ cases, so I am really getting curious. If it turns out that this does not work, I would still be interested in knowing if there is at least one way of doing it that works (for every $n$) and if it is relevant that we have chosen $\{1,\dots,n\}$ and not any other (real, different, positive) entries. Edit: As pointed out in the comments and in user1551's answer, this is not always the case, not even when in each column and in each row each element appears exactly once. I would still like to know why all the $3 \times 3$ matrices (of this kind) are always invertible and, above all, why the ""shift matrix"" mentioned in Hw Chu's comment is indeed always invertible (and whether there are other procedures that always work).",,"['linear-algebra', 'permutations', 'determinant']"
67,Show determinant is non-negative,Show determinant is non-negative,,"Let $A,B \in M_{2}(\mathbb R)$ . Show that $\det((AB+BA)^4 + (AB-BA)^4)\geq 0$ My attempt: expression becomes $\det(2(M-N)^2+16MN)$ where $M=(AB)^2$ and $N=(BA)^2$. Not sure how to continue from here. Any hints appreciated.","Let $A,B \in M_{2}(\mathbb R)$ . Show that $\det((AB+BA)^4 + (AB-BA)^4)\geq 0$ My attempt: expression becomes $\det(2(M-N)^2+16MN)$ where $M=(AB)^2$ and $N=(BA)^2$. Not sure how to continue from here. Any hints appreciated.",,"['linear-algebra', 'determinant']"
68,How many coefficients do you have to change to lower the rank of a matrix?,How many coefficients do you have to change to lower the rank of a matrix?,,"Let $1\leq r \leq n$ be integers. I'd like to find the smallest integer $m$ such that every matrix $A \in M_n(\mathbb R)$ of rank $r$ can be transformed into a matrix $A'$ of rank $r-1$ by changing at most $m$ coefficients. The question is rather arbitrary, but here are some examples and remarks, partly to apologise: If $n =r$, you can transform any invertible matrix $A$ into a non-invertible one by modifying a single coefficient (just pick one whose corresponding cofactor doesn't vanish, and set it to the value that cancels the determinant), so $m = 1$. The matrix $(1)_{i,j}$ is of rank $1$, and you obviously have to change all of its coefficients to lower the rank, so $m=n^2$ when $r = 1$. In general, if $\mathop{\mathrm{rk}} A = r$, you can swap some rows and columns so that the $(r-1) \times (r-1)$ northeast submatrix of $A$ is invertible. Then, it's quite easy to show that you can modify the coefficients in the southwest block so that the rank becomes $r-1$. This proves that in general, $m \leq (n+1-r)^2$. These arguments make me believe (perhaps rather naïvely) that the answer is in fact $m= (n+1-r)^2$, but I haven't got any concrete proof strategy to show it (and the contemplation of small examples quickly gets somehow messy). Can you find the true value of $m$?","Let $1\leq r \leq n$ be integers. I'd like to find the smallest integer $m$ such that every matrix $A \in M_n(\mathbb R)$ of rank $r$ can be transformed into a matrix $A'$ of rank $r-1$ by changing at most $m$ coefficients. The question is rather arbitrary, but here are some examples and remarks, partly to apologise: If $n =r$, you can transform any invertible matrix $A$ into a non-invertible one by modifying a single coefficient (just pick one whose corresponding cofactor doesn't vanish, and set it to the value that cancels the determinant), so $m = 1$. The matrix $(1)_{i,j}$ is of rank $1$, and you obviously have to change all of its coefficients to lower the rank, so $m=n^2$ when $r = 1$. In general, if $\mathop{\mathrm{rk}} A = r$, you can swap some rows and columns so that the $(r-1) \times (r-1)$ northeast submatrix of $A$ is invertible. Then, it's quite easy to show that you can modify the coefficients in the southwest block so that the rank becomes $r-1$. This proves that in general, $m \leq (n+1-r)^2$. These arguments make me believe (perhaps rather naïvely) that the answer is in fact $m= (n+1-r)^2$, but I haven't got any concrete proof strategy to show it (and the contemplation of small examples quickly gets somehow messy). Can you find the true value of $m$?",,"['linear-algebra', 'matrices', 'matrix-rank']"
69,A linear operator $T:V\rightarrow V$ has a cyclic vector iff $f_T=m_T$ (minimal polynomial=characteristic polynomial) [duplicate],A linear operator  has a cyclic vector iff  (minimal polynomial=characteristic polynomial) [duplicate],T:V\rightarrow V f_T=m_T,"This question already has an answer here : Minimal polynomials and cyclic subspaces (1 answer) Closed 7 years ago . I want to prove the following statement: Let linear operator $T:V\rightarrow V$ ($V$ is $n$-dimentional). Then there exists a vector $v$ such that $\left\{ v, Tv, ..., T^{n-1}v \right\}$ form a basis of $V$, iff $f_T=m_T$. (Actually, the $\Rightarrow$ direction is quite easy. I need to prove the other direction.) I've seen couple of answers to this problem in this site, all using the ""Rational Canonial Form"", which is something I wasn't tought in the course (but I am familiar with the Jordan Form). Moreover, the only proof I've found on the internet is this , which seemed to be promising, until I got to this sentence, which unfortunately seems to be a mistake: See that $V_i$ is a subspace of $V$, for all $1 \leq i \leq m$, and   $V=\bigcup_{i=1}^m V_i$. Therefore $V=V_k$, for some $1 \leq k \leq m$. I don't know if that's true for infinite vector spaces, but for finite ones, such as $\mathbb{F}_p^n$ it's certainly isn't. Any step towards a proof would be appriciated (as well as an explanation for the suspicious claim stated above). Thank you!","This question already has an answer here : Minimal polynomials and cyclic subspaces (1 answer) Closed 7 years ago . I want to prove the following statement: Let linear operator $T:V\rightarrow V$ ($V$ is $n$-dimentional). Then there exists a vector $v$ such that $\left\{ v, Tv, ..., T^{n-1}v \right\}$ form a basis of $V$, iff $f_T=m_T$. (Actually, the $\Rightarrow$ direction is quite easy. I need to prove the other direction.) I've seen couple of answers to this problem in this site, all using the ""Rational Canonial Form"", which is something I wasn't tought in the course (but I am familiar with the Jordan Form). Moreover, the only proof I've found on the internet is this , which seemed to be promising, until I got to this sentence, which unfortunately seems to be a mistake: See that $V_i$ is a subspace of $V$, for all $1 \leq i \leq m$, and   $V=\bigcup_{i=1}^m V_i$. Therefore $V=V_k$, for some $1 \leq k \leq m$. I don't know if that's true for infinite vector spaces, but for finite ones, such as $\mathbb{F}_p^n$ it's certainly isn't. Any step towards a proof would be appriciated (as well as an explanation for the suspicious claim stated above). Thank you!",,"['linear-algebra', 'eigenvalues-eigenvectors', 'linear-transformations', 'minimal-polynomials', 'generalized-eigenvector']"
70,What theorem would be false if the span of empty set is not $\lbrace \mathbf{0}\rbrace$,What theorem would be false if the span of empty set is not,\lbrace \mathbf{0}\rbrace,"I know that many books define that the span of empty set is $\lbrace\mathbf{0}\rbrace$. However, I came across this definition of spanning set from many places (like Roman Steven's Advanced Linear Algebra or even Proofwiki: Let $M$ be a left $R$-module, let $E\subseteq M$, and let $I_n$ be the set $\lbrace x\in\mathbb{N}\mid 1\leq x\leq n\rbrace$ for each natural number $n$. The set span $E$    (or $\langle E\rangle$) is defined by \begin{equation}  \langle  E\rangle:=\left\{\mathbf{a}\in  M\Bigg|\exists(n\in\mathbb{Z}^+)\left[\exists(r:I_n\rightarrow  R)\exists(\mathbf{u}:I_n\rightarrow E)\left[\mathbf{a}=\sum_{i=1}^n  r(i)\mathbf{u}(i)\right]\right]\right\}. \end{equation} In other words, the set $\langle E\rangle$ is the set of all possible linear combinations of elements of $E$. My problem is, according to the definition, we must have this: A set $E$ is empty if and only if $\langle E\rangle$ is empty. Why? I will only prove the forward case: Suppose that $E$ is empty, and we assume for the sake of contradiction that $\langle E\rangle$ is not empty. Since $\langle E\rangle$ is not empty, there must exist $\mathbf{a}\in\langle E\rangle$. This implies that there must exist a positive integer $n$ such that $\mathbf{a}=\sum_{i=1}^n r(i)\mathbf{u}(i)$ for some function $r:I_n\rightarrow R$ and some function $\mathbf{u}:I_n\rightarrow E$. Since $I_n$ is not empty, it follows that $E$ must not be empty, which is a contradiction to the assumption that $E$ is empty. QED. So, according the definition, the span of empty set is EMPTY! It's like we have two conventions here. The first one is that the span of empty set is the zero vector space while the second one is that the span of empty set is empty. My problem is, which convention should I stick with (and no, not according to my prof, he doesn't care anyway, but I care). So, I'm considering the consequence of having $\langle\emptyset\rangle=\emptyset$. Well, it has the nice property that $E$ is empty iff $\langle E\rangle$ is empty. But if $\emptyset$ does not span the zero vector space, then the zero vector space would not have a basis, and that's pretty big drawback. So, it would not be true anymore that every vector space has a basis. What other theorems would be false from this basic definition? If there are many drawbacks, should I discard this definition and the book itself?","I know that many books define that the span of empty set is $\lbrace\mathbf{0}\rbrace$. However, I came across this definition of spanning set from many places (like Roman Steven's Advanced Linear Algebra or even Proofwiki: Let $M$ be a left $R$-module, let $E\subseteq M$, and let $I_n$ be the set $\lbrace x\in\mathbb{N}\mid 1\leq x\leq n\rbrace$ for each natural number $n$. The set span $E$    (or $\langle E\rangle$) is defined by \begin{equation}  \langle  E\rangle:=\left\{\mathbf{a}\in  M\Bigg|\exists(n\in\mathbb{Z}^+)\left[\exists(r:I_n\rightarrow  R)\exists(\mathbf{u}:I_n\rightarrow E)\left[\mathbf{a}=\sum_{i=1}^n  r(i)\mathbf{u}(i)\right]\right]\right\}. \end{equation} In other words, the set $\langle E\rangle$ is the set of all possible linear combinations of elements of $E$. My problem is, according to the definition, we must have this: A set $E$ is empty if and only if $\langle E\rangle$ is empty. Why? I will only prove the forward case: Suppose that $E$ is empty, and we assume for the sake of contradiction that $\langle E\rangle$ is not empty. Since $\langle E\rangle$ is not empty, there must exist $\mathbf{a}\in\langle E\rangle$. This implies that there must exist a positive integer $n$ such that $\mathbf{a}=\sum_{i=1}^n r(i)\mathbf{u}(i)$ for some function $r:I_n\rightarrow R$ and some function $\mathbf{u}:I_n\rightarrow E$. Since $I_n$ is not empty, it follows that $E$ must not be empty, which is a contradiction to the assumption that $E$ is empty. QED. So, according the definition, the span of empty set is EMPTY! It's like we have two conventions here. The first one is that the span of empty set is the zero vector space while the second one is that the span of empty set is empty. My problem is, which convention should I stick with (and no, not according to my prof, he doesn't care anyway, but I care). So, I'm considering the consequence of having $\langle\emptyset\rangle=\emptyset$. Well, it has the nice property that $E$ is empty iff $\langle E\rangle$ is empty. But if $\emptyset$ does not span the zero vector space, then the zero vector space would not have a basis, and that's pretty big drawback. So, it would not be true anymore that every vector space has a basis. What other theorems would be false from this basic definition? If there are many drawbacks, should I discard this definition and the book itself?",,"['linear-algebra', 'vector-spaces']"
71,$GL_n(F)$ acts on the flag variety,acts on the flag variety,GL_n(F),"I have the following 2-part question as a homework assignment... Let $F$ be a field and $n\in \mathbb{Z}_{\geq 1}$. A full flag in the vector space $F^n$ is a chain of subspaces    \begin{align} \{0\}\subset V_1\subset \cdots\subset V_{n-1}\subset V_n=F^n\end{align}   such that $\dim V_i=i$ for all $i\in \{1,2,...,n\}$. The flag variety of $GL_n(F)$ is the set $\mathcal{B}$ of all full flags in $F^n$. (a) Show that $GL_n(F)$ acts on the flag variety $\mathcal{B}$. (b) Let $e_i$ be the coordinate basis vectors of $F^n$. Let $e_{*}$ be the full flag given by $V_i=\mathrm{span}\{e_1,...,e_i\}$. Show that $\mathrm{Stab}_{GL_n(F)}(e_*)=B$, where $B$ is the subgroup of upper-triangular matrices. I feel like once i understand how to tackle part (a), I will understand part (b), but at the moment I really don't understand what this flag variety really is. $\quad$-What does an element of the flag variety look like? $\quad$-How would $GL_n(F)$ act on such an element? These are the things I need some hints on. Thank you!","I have the following 2-part question as a homework assignment... Let $F$ be a field and $n\in \mathbb{Z}_{\geq 1}$. A full flag in the vector space $F^n$ is a chain of subspaces    \begin{align} \{0\}\subset V_1\subset \cdots\subset V_{n-1}\subset V_n=F^n\end{align}   such that $\dim V_i=i$ for all $i\in \{1,2,...,n\}$. The flag variety of $GL_n(F)$ is the set $\mathcal{B}$ of all full flags in $F^n$. (a) Show that $GL_n(F)$ acts on the flag variety $\mathcal{B}$. (b) Let $e_i$ be the coordinate basis vectors of $F^n$. Let $e_{*}$ be the full flag given by $V_i=\mathrm{span}\{e_1,...,e_i\}$. Show that $\mathrm{Stab}_{GL_n(F)}(e_*)=B$, where $B$ is the subgroup of upper-triangular matrices. I feel like once i understand how to tackle part (a), I will understand part (b), but at the moment I really don't understand what this flag variety really is. $\quad$-What does an element of the flag variety look like? $\quad$-How would $GL_n(F)$ act on such an element? These are the things I need some hints on. Thank you!",,"['linear-algebra', 'abstract-algebra', 'schubert-calculus']"
72,Norm equivalence and the sequence limit of normalized vectors,Norm equivalence and the sequence limit of normalized vectors,,"Consider finite-dimensional vector spaces, on which two norms $\|\|_1$ and $\|\|_2$ are always equivalent. Then a sequence of vectors ${\bf v}_k \to {\bf v}$ w.r.t. norm 1 iff ${\bf v}_k \to {\bf v}$ w.r.t. norm 2. The question is about sequence $\frac{{{{\mathbf{v}}_k}}}{{{{\left\| {{{\mathbf{v}}_k}} \right\|}_1}}} \to \frac{{\mathbf{v}}}{{{{\left\| {\mathbf{v}} \right\|}_1}}}$ w.r.t. norm 1, will this limit imply $\frac{{{{\mathbf{v}}_k}}}{{{{\left\| {{{\mathbf{v}}_k}} \right\|}_2}}} \to \frac{{\mathbf{v}}}{{{{\left\| {\mathbf{v}} \right\|}_2}}}$ w.r.t. norm 2 (or maybe a weaker claim that one convergence implies the other but the limit might be different)? I have trouble showing this is true. If this is false, anyone can help provide a counterexample? Thanks! For example, $(k,\sqrt k),k=1,2,...$ satisfies $\frac{{(k,\sqrt k )}}{{{{\left\| {(k,\sqrt k )} \right\|}_p}}} = \frac{{(k,\sqrt k )}}{{{{({k^p} + {k^{\frac{p}{2}}})}^{\frac{1}{p}}}}} \to (1,0)$ for any $p$-norm.","Consider finite-dimensional vector spaces, on which two norms $\|\|_1$ and $\|\|_2$ are always equivalent. Then a sequence of vectors ${\bf v}_k \to {\bf v}$ w.r.t. norm 1 iff ${\bf v}_k \to {\bf v}$ w.r.t. norm 2. The question is about sequence $\frac{{{{\mathbf{v}}_k}}}{{{{\left\| {{{\mathbf{v}}_k}} \right\|}_1}}} \to \frac{{\mathbf{v}}}{{{{\left\| {\mathbf{v}} \right\|}_1}}}$ w.r.t. norm 1, will this limit imply $\frac{{{{\mathbf{v}}_k}}}{{{{\left\| {{{\mathbf{v}}_k}} \right\|}_2}}} \to \frac{{\mathbf{v}}}{{{{\left\| {\mathbf{v}} \right\|}_2}}}$ w.r.t. norm 2 (or maybe a weaker claim that one convergence implies the other but the limit might be different)? I have trouble showing this is true. If this is false, anyone can help provide a counterexample? Thanks! For example, $(k,\sqrt k),k=1,2,...$ satisfies $\frac{{(k,\sqrt k )}}{{{{\left\| {(k,\sqrt k )} \right\|}_p}}} = \frac{{(k,\sqrt k )}}{{{{({k^p} + {k^{\frac{p}{2}}})}^{\frac{1}{p}}}}} \to (1,0)$ for any $p$-norm.",,"['linear-algebra', 'functional-analysis']"
73,"Are Clifford and exterior algebras isomorphic as ""wedge product algebras""?","Are Clifford and exterior algebras isomorphic as ""wedge product algebras""?",,"$\newcommand{\Cl}{\mathscr{Cl}(V)}$$\newcommand{\ext}{\Lambda(V)}$Let $V$ be a finite dimensional vector space over a field with characteristic not equal to two. Assume we have made a choice of symmetric bilinear form for $V$. It is known that the Clifford algebra $\Cl$ and the exterior algebra $\ext$ are isomorphic as vector spaces (this is an exercise in Greub's Multilinear Algebra, I believe it is mentioned in Wikipedia, and also see this and this and this related question on Math.SE). Now obviously the Clifford algebra $\Cl$ with the Clifford product is not isomorphic as an algebra to the exterior algebra $\ext$ with the wedge product . However, the wedge product is also defined on $\Cl$ and different from the Clifford product. Question: Is the Clifford algebra $\Cl$ with the wedge product , i.e. not with the Clifford product, isomorphic as an algebra to the exterior algebra $\ext$ with the wedge product? Attempt: I feel like this should follow immediately from the following two facts: The Clifford algebra $\Cl$ and the exterior algebra $\ext$ are isomorphic as vector spaces. A list of vectors $(v_1, \dots, v_n)$ is linearly independent if and only if its wedge product is non-zero: $$v_1 \wedge \dots \wedge v_n \not=0\,.$$ (I believe this second fact is correct, it is the content of at least one exercise in Lee's Introduction to Smooth Manifolds if I remember correctly, although I haven't gotten around to doing it yet.) Note: This is effectively a follow-up to this question I asked on MathOverflow. Wedge product on the Clifford Algebra This might not be correct; if so, please explain why and/or give a pointer to a reference which explains why and I will also accept your answer. (Note: what I had previously was definitely wrong, so here's a second attempt, again based on this document .) We can decompose the Clifford product $vw$ as follows: $$vw = \frac{1}{2}(vw + wv) + \frac{1}{2}(vw -wv) \,. $$ Noting that the first term on the RHS is symmetric and bilinear (in $v$ and $w$), the author Chisholm states that it is plausible that this could be the inner product. (See the top of p.4 ) I think if one takes the formal definition of Clifford algebra, it follows from the quotient relation $v^2 = \langle v, v \rangle$ that this makes sense. So then the second term on the RHS, $\frac{1}{2}(vw -wv)$, is denoted $v \wedge w$, one has from the definition that it is skew-symmetric. In the Euclidean case for $\mathbb{R}^3$, it follows from $\frac{1}{2}(vw+wv)=\langle v,w\rangle=|u||v|\cos\theta$ that $(v \wedge w)^2 = -|v|^2|w|^2 \sin^2\theta = -|v \times w|^2$. Also, the wedge product of any element of the Clifford algebra $\Cl$ with a scalar is just scalar multiplication (I think). I think the definition of the wedge product above generalizes so that for any $v_1, \dots, v_n \in V$: $$v_1 \wedge \dots \wedge v_n = \frac{1}{n!}\sum_{\sigma \in S_n} (\operatorname{sgn} \sigma) v_{\sigma(1)} \dots v_{\sigma(n)} \,, $$ where the multiplication is the Clifford product (see eq. (30) p.13 here ) but I'm not sure. If that formula is correct, then the answer to my question might automatically be affirmative due to a bijection between the Clifford algebra (with the wedge product) and alternating/skew-symmetric tensors of rank $\le n$. In other words the proposed formula is very similar to that given for creating the exterior algebra from the tensor algebra. Following Qiaochu Yuan's suggestion in the comments, the wedge product of three vectors would for example then be: $$u \wedge v \wedge w = \frac{1}{6}uvw + \frac{1}{6}vwu + \frac{1}{6}wuv -\frac{1}{6}vuw -\frac{1}{6}uwv -\frac{1}{6}wvu \\ = \frac{1}{6}u(vw - wv)+\frac{1}{6}v(wu-uw)+\frac{1}{6}w(uv-vu) \\ = \frac{1}{3}u(v \wedge w) + \frac{1}{3}v(w \wedge u) + \frac{1}{3}w(u \wedge v) \,.$$ My hope is that this would be enough to define (via induction) what is meant by the wedge product of arbitrary elements of the Clifford algebra $\Cl$. Basically the motivation for my claim/belief that the Clifford algebra $\Cl$ also has a wedge product is how many texts on the Clifford algebra of $\mathbb{R}^n$ with the Euclidean inner product treat the Clifford product (for vectors) essentially as a combination of the inner and wedge products, i.e. an extension of the wedge product. This seems like it might be compatible with the formal definition of Clifford algebra, based on the fact that one should have $v^2 = vv = \langle v, v \rangle$, but I was hoping/assuming someone here might have already learned all of this and can confirm/deny that this is true before I try to verify it all in detail by myself only to find out that I have been trying to prove something false. Wikipedia says that: ...if one takes the Clifford algebra to be a filtered algebra , then the associated graded algebra is the exterior algebra. To be honest I don't know what that means, but it seems like it might be stronger than: ""the Clifford algebra is isomorphic to the exterior algebra as a vector space"".","$\newcommand{\Cl}{\mathscr{Cl}(V)}$$\newcommand{\ext}{\Lambda(V)}$Let $V$ be a finite dimensional vector space over a field with characteristic not equal to two. Assume we have made a choice of symmetric bilinear form for $V$. It is known that the Clifford algebra $\Cl$ and the exterior algebra $\ext$ are isomorphic as vector spaces (this is an exercise in Greub's Multilinear Algebra, I believe it is mentioned in Wikipedia, and also see this and this and this related question on Math.SE). Now obviously the Clifford algebra $\Cl$ with the Clifford product is not isomorphic as an algebra to the exterior algebra $\ext$ with the wedge product . However, the wedge product is also defined on $\Cl$ and different from the Clifford product. Question: Is the Clifford algebra $\Cl$ with the wedge product , i.e. not with the Clifford product, isomorphic as an algebra to the exterior algebra $\ext$ with the wedge product? Attempt: I feel like this should follow immediately from the following two facts: The Clifford algebra $\Cl$ and the exterior algebra $\ext$ are isomorphic as vector spaces. A list of vectors $(v_1, \dots, v_n)$ is linearly independent if and only if its wedge product is non-zero: $$v_1 \wedge \dots \wedge v_n \not=0\,.$$ (I believe this second fact is correct, it is the content of at least one exercise in Lee's Introduction to Smooth Manifolds if I remember correctly, although I haven't gotten around to doing it yet.) Note: This is effectively a follow-up to this question I asked on MathOverflow. Wedge product on the Clifford Algebra This might not be correct; if so, please explain why and/or give a pointer to a reference which explains why and I will also accept your answer. (Note: what I had previously was definitely wrong, so here's a second attempt, again based on this document .) We can decompose the Clifford product $vw$ as follows: $$vw = \frac{1}{2}(vw + wv) + \frac{1}{2}(vw -wv) \,. $$ Noting that the first term on the RHS is symmetric and bilinear (in $v$ and $w$), the author Chisholm states that it is plausible that this could be the inner product. (See the top of p.4 ) I think if one takes the formal definition of Clifford algebra, it follows from the quotient relation $v^2 = \langle v, v \rangle$ that this makes sense. So then the second term on the RHS, $\frac{1}{2}(vw -wv)$, is denoted $v \wedge w$, one has from the definition that it is skew-symmetric. In the Euclidean case for $\mathbb{R}^3$, it follows from $\frac{1}{2}(vw+wv)=\langle v,w\rangle=|u||v|\cos\theta$ that $(v \wedge w)^2 = -|v|^2|w|^2 \sin^2\theta = -|v \times w|^2$. Also, the wedge product of any element of the Clifford algebra $\Cl$ with a scalar is just scalar multiplication (I think). I think the definition of the wedge product above generalizes so that for any $v_1, \dots, v_n \in V$: $$v_1 \wedge \dots \wedge v_n = \frac{1}{n!}\sum_{\sigma \in S_n} (\operatorname{sgn} \sigma) v_{\sigma(1)} \dots v_{\sigma(n)} \,, $$ where the multiplication is the Clifford product (see eq. (30) p.13 here ) but I'm not sure. If that formula is correct, then the answer to my question might automatically be affirmative due to a bijection between the Clifford algebra (with the wedge product) and alternating/skew-symmetric tensors of rank $\le n$. In other words the proposed formula is very similar to that given for creating the exterior algebra from the tensor algebra. Following Qiaochu Yuan's suggestion in the comments, the wedge product of three vectors would for example then be: $$u \wedge v \wedge w = \frac{1}{6}uvw + \frac{1}{6}vwu + \frac{1}{6}wuv -\frac{1}{6}vuw -\frac{1}{6}uwv -\frac{1}{6}wvu \\ = \frac{1}{6}u(vw - wv)+\frac{1}{6}v(wu-uw)+\frac{1}{6}w(uv-vu) \\ = \frac{1}{3}u(v \wedge w) + \frac{1}{3}v(w \wedge u) + \frac{1}{3}w(u \wedge v) \,.$$ My hope is that this would be enough to define (via induction) what is meant by the wedge product of arbitrary elements of the Clifford algebra $\Cl$. Basically the motivation for my claim/belief that the Clifford algebra $\Cl$ also has a wedge product is how many texts on the Clifford algebra of $\mathbb{R}^n$ with the Euclidean inner product treat the Clifford product (for vectors) essentially as a combination of the inner and wedge products, i.e. an extension of the wedge product. This seems like it might be compatible with the formal definition of Clifford algebra, based on the fact that one should have $v^2 = vv = \langle v, v \rangle$, but I was hoping/assuming someone here might have already learned all of this and can confirm/deny that this is true before I try to verify it all in detail by myself only to find out that I have been trying to prove something false. Wikipedia says that: ...if one takes the Clifford algebra to be a filtered algebra , then the associated graded algebra is the exterior algebra. To be honest I don't know what that means, but it seems like it might be stronger than: ""the Clifford algebra is isomorphic to the exterior algebra as a vector space"".",,"['linear-algebra', 'multilinear-algebra', 'exterior-algebra', 'clifford-algebras', 'geometric-algebras']"
74,Relationship between Row Space and Reduced Row Echelon Form,Relationship between Row Space and Reduced Row Echelon Form,,"Consider two matrices A and B having the same dimension. It is said that: if A and B have different reduced row echelon forms (RREFs), then their row spaces are different. Any one know how to prove this statement? Or, is there any reference (book) on this? Thanks in advance.","Consider two matrices A and B having the same dimension. It is said that: if A and B have different reduced row echelon forms (RREFs), then their row spaces are different. Any one know how to prove this statement? Or, is there any reference (book) on this? Thanks in advance.",,"['linear-algebra', 'matrices', 'vector-spaces']"
75,Vector space as direct sum of kernel and image,Vector space as direct sum of kernel and image,,"Let $T: V\to V$ be a linear map on an infinite-dimensional vector space $V$ over a field $F$. Suppose that $T(V)$ is finite dimensional, and $T^2(V)=T(V)$. Show that $V=\ker T\oplus T(V)$. (Note that this is not a duplicate of Show that the direct sum of a kernel of a projection and its image create the originating vector space. , it seems to be more difficult since we only have $T^2(V)=T(V)$ rather than $T^2=T$.) What I tried: I can prove $V=\ker T+T(V)$, what is difficult seems to be showing the direct sum, i.e. the intersection is 0. Let $v\in V$. Since $T(v)\in T(V)=T^2(V)$, $T(v)=T^2(x)$ for some $x\in V$. Then $T(v-T(x))=0$, so that $v-T(x)=y\in\ker T$. Thus $v=y+T(x)\in\ker T+T(V)$. Hence $V\subseteq\ker T+T(V)$. $\ker T+T(V)\subseteq V$ is clear since $V$ is a vector space, so $V=\ker T+T(V)$. I also have a previous result Linear Transformation from Infinite dimensional to Finite dimensional Space which may or may not be useful. Thanks for any help!","Let $T: V\to V$ be a linear map on an infinite-dimensional vector space $V$ over a field $F$. Suppose that $T(V)$ is finite dimensional, and $T^2(V)=T(V)$. Show that $V=\ker T\oplus T(V)$. (Note that this is not a duplicate of Show that the direct sum of a kernel of a projection and its image create the originating vector space. , it seems to be more difficult since we only have $T^2(V)=T(V)$ rather than $T^2=T$.) What I tried: I can prove $V=\ker T+T(V)$, what is difficult seems to be showing the direct sum, i.e. the intersection is 0. Let $v\in V$. Since $T(v)\in T(V)=T^2(V)$, $T(v)=T^2(x)$ for some $x\in V$. Then $T(v-T(x))=0$, so that $v-T(x)=y\in\ker T$. Thus $v=y+T(x)\in\ker T+T(V)$. Hence $V\subseteq\ker T+T(V)$. $\ker T+T(V)\subseteq V$ is clear since $V$ is a vector space, so $V=\ker T+T(V)$. I also have a previous result Linear Transformation from Infinite dimensional to Finite dimensional Space which may or may not be useful. Thanks for any help!",,"['linear-algebra', 'abstract-algebra', 'linear-transformations']"
76,An identity about Vandermonde determinant.,An identity about Vandermonde determinant.,,"I want to prove the following identity $\sum_{i=1}^{k}x_i\Delta(x_1,\ldots,x_i+t,\ldots,x_k)=\left(x_1+x_2+\cdots+x_k+{k \choose 2}t\right)\cdot\Delta(x_1,x_2,\ldots,x_k),$ where we write  $\Delta(l_1,\ldots,l_k)$ for $\prod_{i<j}(l_i-l_j).$ I have checked that this identity is true for $k=2,3.$ I tried to calculate LHS using Vandermonde determinant, but it seems to work. I sense some properties of symmetric function could be useful…… Any help will be appreciated.:) The problem comes from Fulton's book ""Young Tableaux-With Applications to Representation Theory and Geometry"". It's a default fact in Exercise 10 in page55. However, I cannot prove this. He uses the following fact to prove Hook length formula. And set $x_i=l_i$ and $t=-1$ in Exercise 10.","I want to prove the following identity $\sum_{i=1}^{k}x_i\Delta(x_1,\ldots,x_i+t,\ldots,x_k)=\left(x_1+x_2+\cdots+x_k+{k \choose 2}t\right)\cdot\Delta(x_1,x_2,\ldots,x_k),$ where we write  $\Delta(l_1,\ldots,l_k)$ for $\prod_{i<j}(l_i-l_j).$ I have checked that this identity is true for $k=2,3.$ I tried to calculate LHS using Vandermonde determinant, but it seems to work. I sense some properties of symmetric function could be useful…… Any help will be appreciated.:) The problem comes from Fulton's book ""Young Tableaux-With Applications to Representation Theory and Geometry"". It's a default fact in Exercise 10 in page55. However, I cannot prove this. He uses the following fact to prove Hook length formula. And set $x_i=l_i$ and $t=-1$ in Exercise 10.",,"['linear-algebra', 'polynomials', 'determinant', 'symmetric-polynomials', 'symmetric-functions']"
77,Complex-Orthogonal Similarity,Complex-Orthogonal Similarity,,"First, a quick definition: I say that a matrix $M$ with complex entries is complex-orthogonal if $MM^T = I$.  Note that $T$ here refers to the transpose, not the conjugate transpose .  That is, we are taking the adjoint with respect to a bilinear (as opposed to sesquilinear) dot-product over $\Bbb C$.  A complex-orthogonal matrix need not be unitary, and vice versa (unless of its entries are in $\Bbb R$). Consider the following claim: Claim: For any $A \in \Bbb C^{n \times n}$, there exists a complex-orthogonal $U$ such that $UAU^T$ has equal diagonal entries. My Question: Does this claim hold? It may be helpful to take a look at this paper which proves that for any $A$, there is an $S$ such that $SAS^{-1}$ has equal diagonal entries.  It goes on to show that such an $S$ may be taken to be a unitary matrix .  Another proof of the same fact is presented in p. 77 of R.A. Horn and C.R. Johnson’s Matrix Analysis (1985/7, Cambridge Univ. Press). Notably, both of these proofs exploit facts for which an analog fails to exist in my case.  For Horn and Johnson's proof, we need the fact that the unitary matrices form a compact subset of $\Bbb C^{n \times n}$.  For the proof linked, we need the numerical range to have certain ""nice properties"", which I believe fail in my case. Any insight here is appreciated.","First, a quick definition: I say that a matrix $M$ with complex entries is complex-orthogonal if $MM^T = I$.  Note that $T$ here refers to the transpose, not the conjugate transpose .  That is, we are taking the adjoint with respect to a bilinear (as opposed to sesquilinear) dot-product over $\Bbb C$.  A complex-orthogonal matrix need not be unitary, and vice versa (unless of its entries are in $\Bbb R$). Consider the following claim: Claim: For any $A \in \Bbb C^{n \times n}$, there exists a complex-orthogonal $U$ such that $UAU^T$ has equal diagonal entries. My Question: Does this claim hold? It may be helpful to take a look at this paper which proves that for any $A$, there is an $S$ such that $SAS^{-1}$ has equal diagonal entries.  It goes on to show that such an $S$ may be taken to be a unitary matrix .  Another proof of the same fact is presented in p. 77 of R.A. Horn and C.R. Johnson’s Matrix Analysis (1985/7, Cambridge Univ. Press). Notably, both of these proofs exploit facts for which an analog fails to exist in my case.  For Horn and Johnson's proof, we need the fact that the unitary matrices form a compact subset of $\Bbb C^{n \times n}$.  For the proof linked, we need the numerical range to have certain ""nice properties"", which I believe fail in my case. Any insight here is appreciated.",,"['linear-algebra', 'matrices', 'complex-analysis']"
78,Eigenvalue of block matrix of order $2n$,Eigenvalue of block matrix of order,2n,"How to find eigenvalues of following block matrix? $$P=\begin{bmatrix} A & B \\ B & A \end{bmatrix}$$ Where, $A=\begin{bmatrix} 0 & 1 & 0 & 0 & 0 & 0 & \cdots & 1 \\ 1 & 0 & 1 & 0 & 0 & 0 & \cdots & 0 \\ 0 & 1 & 0 & 1 & 0 & 0 & \cdots & 0 \\ 0 & 0 & 1 & 0 & 1 & 0 & \cdots & 0 \\ 0 & 0 & 0 & 1 & 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \ddots & \vdots \\ 0 & 0 & 0 & 0 & 0 & 0 & \ddots & 1 \\ 1 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 \end{bmatrix}_n$ $B=\begin{bmatrix} 1 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 \\ \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \ddots & \vdots \\ 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 \end{bmatrix}_n$ I had read one result for block matrix which says that eigenvalues of matrix $P$ is union of eigenvalues of $A+B$ and $A-B$ Here, $A+B=\begin{bmatrix} 1 & 1 & 0 & 0 & 0 & 0 & \cdots & 1 \\ 1 & 0 & 1 & 0 & 0 & 0 & \cdots & 0 \\ 0 & 1 & 0 & 1 & 0 & 0 & \cdots & 0 \\ 0 & 0 & 1 & 0 & 1 & 0 & \cdots & 0 \\ 0 & 0 & 0 & 1 & 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \ddots & \vdots \\ 0 & 0 & 0 & 0 & 0 & 0 & \ddots & 1 \\ 1 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 \end{bmatrix}_n$ $A-B=\begin{bmatrix} -1 & 1 & 0 & 0 & 0 & 0 & \cdots & 1 \\ 1 & 0 & 1 & 0 & 0 & 0 & \cdots & 0 \\ 0 & 1 & 0 & 1 & 0 & 0 & \cdots & 0 \\ 0 & 0 & 1 & 0 & 1 & 0 & \cdots & 0 \\ 0 & 0 & 0 & 1 & 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \ddots & \vdots \\ 0 & 0 & 0 & 0 & 0 & 0 & \ddots & 1 \\ 1 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 \end{bmatrix}_n$ How to find eigenvalues of $A+B$ and $A-B$? I know that eigenvalues of $A$ are $2\cos\frac{2\pi j}{n},j=1,2,\cdots,n$","How to find eigenvalues of following block matrix? $$P=\begin{bmatrix} A & B \\ B & A \end{bmatrix}$$ Where, $A=\begin{bmatrix} 0 & 1 & 0 & 0 & 0 & 0 & \cdots & 1 \\ 1 & 0 & 1 & 0 & 0 & 0 & \cdots & 0 \\ 0 & 1 & 0 & 1 & 0 & 0 & \cdots & 0 \\ 0 & 0 & 1 & 0 & 1 & 0 & \cdots & 0 \\ 0 & 0 & 0 & 1 & 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \ddots & \vdots \\ 0 & 0 & 0 & 0 & 0 & 0 & \ddots & 1 \\ 1 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 \end{bmatrix}_n$ $B=\begin{bmatrix} 1 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 \\ \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \ddots & \vdots \\ 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 \end{bmatrix}_n$ I had read one result for block matrix which says that eigenvalues of matrix $P$ is union of eigenvalues of $A+B$ and $A-B$ Here, $A+B=\begin{bmatrix} 1 & 1 & 0 & 0 & 0 & 0 & \cdots & 1 \\ 1 & 0 & 1 & 0 & 0 & 0 & \cdots & 0 \\ 0 & 1 & 0 & 1 & 0 & 0 & \cdots & 0 \\ 0 & 0 & 1 & 0 & 1 & 0 & \cdots & 0 \\ 0 & 0 & 0 & 1 & 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \ddots & \vdots \\ 0 & 0 & 0 & 0 & 0 & 0 & \ddots & 1 \\ 1 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 \end{bmatrix}_n$ $A-B=\begin{bmatrix} -1 & 1 & 0 & 0 & 0 & 0 & \cdots & 1 \\ 1 & 0 & 1 & 0 & 0 & 0 & \cdots & 0 \\ 0 & 1 & 0 & 1 & 0 & 0 & \cdots & 0 \\ 0 & 0 & 1 & 0 & 1 & 0 & \cdots & 0 \\ 0 & 0 & 0 & 1 & 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \ddots & \vdots \\ 0 & 0 & 0 & 0 & 0 & 0 & \ddots & 1 \\ 1 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 \end{bmatrix}_n$ How to find eigenvalues of $A+B$ and $A-B$? I know that eigenvalues of $A$ are $2\cos\frac{2\pi j}{n},j=1,2,\cdots,n$",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
79,"Linear algebra for modern differential geometry( and other types of modern geometry, like analytic, complex and algebraic)","Linear algebra for modern differential geometry( and other types of modern geometry, like analytic, complex and algebraic)",,"I wish to study real and complex analysis(for example, Pugh ""Real Mathematical Analysis"" and Cartan ""Elementary theory of analytic functions of one and several complex variables"") and modern differential and riemannian geometry(for example, Jeffrey Lee ""Manifolds and differential geometry""). I would like to chose a book on theoretical linear algebra to cover prerequisites for these. That is, I don't need a linear algebra for applications in science. I don't need it for its own sake or history of mathematics. I want to study modern geometry and analysis - real and complex analysis, analysis on manifolds, differential geometry, riemannian geometry, complex algebraic and analytic geometry. I came across a few books on my search, but I'm not sure if they are the best option for my goals. Axler ""Linear Algebra Done Right"" - this book seems to have a different goal in mind. It avoid determinants as well as works only over $\mathbb{R}$ and $\mathbb{C}$. Do you need linear algebra over an arbitrary field $F$ for study of smooth manifolds? Roman ""Advanced Linear Algebra"". Isn't it an overkill for my goals? What do you think of this book? Any other recommendations will be considered, of course. I would prefer modern books over likes of Hoffman-Kunze or Halmos, but if you know any old gems, feel free to tell.","I wish to study real and complex analysis(for example, Pugh ""Real Mathematical Analysis"" and Cartan ""Elementary theory of analytic functions of one and several complex variables"") and modern differential and riemannian geometry(for example, Jeffrey Lee ""Manifolds and differential geometry""). I would like to chose a book on theoretical linear algebra to cover prerequisites for these. That is, I don't need a linear algebra for applications in science. I don't need it for its own sake or history of mathematics. I want to study modern geometry and analysis - real and complex analysis, analysis on manifolds, differential geometry, riemannian geometry, complex algebraic and analytic geometry. I came across a few books on my search, but I'm not sure if they are the best option for my goals. Axler ""Linear Algebra Done Right"" - this book seems to have a different goal in mind. It avoid determinants as well as works only over $\mathbb{R}$ and $\mathbb{C}$. Do you need linear algebra over an arbitrary field $F$ for study of smooth manifolds? Roman ""Advanced Linear Algebra"". Isn't it an overkill for my goals? What do you think of this book? Any other recommendations will be considered, of course. I would prefer modern books over likes of Hoffman-Kunze or Halmos, but if you know any old gems, feel free to tell.",,"['linear-algebra', 'differential-geometry', 'reference-request', 'book-recommendation', 'complex-geometry']"
80,What is the colon operator between matrices?,What is the colon operator between matrices?,,"While reading Robust Quasistatic Finite Elements and Flesh Simulation by Teran et al., I have seen in several equations a colon operator used between matrices. Here are some examples, using matrices $F$ , $P$ and $C$ : $\delta F : \delta P > 0$ $\delta F : (\partial P / \partial F) : \delta F > 0$ $i2 = C : C$ The only hint I have is that I believe that $C$ is a diagonal matrix with diagonal elements $[\sigma_1^2, \sigma_2^2, \sigma_3^2]$ , and the result of $C : C$ is $\sigma_1^4 + \sigma_2^4 + \sigma_3^4$ . Does anybody know what this operator represents?","While reading Robust Quasistatic Finite Elements and Flesh Simulation by Teran et al., I have seen in several equations a colon operator used between matrices. Here are some examples, using matrices , and : The only hint I have is that I believe that is a diagonal matrix with diagonal elements , and the result of is . Does anybody know what this operator represents?","F P C \delta F : \delta P > 0 \delta F : (\partial P / \partial F) : \delta F > 0 i2 = C : C C [\sigma_1^2, \sigma_2^2, \sigma_3^2] C : C \sigma_1^4 + \sigma_2^4 + \sigma_3^4","['linear-algebra', 'matrices', 'functional-analysis', 'notation', 'matrix-calculus']"
81,If $A$ is positive definite then any principal submatrix of $A$ is positive definite,If  is positive definite then any principal submatrix of  is positive definite,A A,If $A$ is positive definite then any principal submatrix of $A$ is  positive definite. Proof; In the proof i dont understand about $j_i$'s.. Can some one interpret the proof in a simpler way(may be the original one is simpler) if someboy is rewriting the proof it would give me a better understanding.(if possible),If $A$ is positive definite then any principal submatrix of $A$ is  positive definite. Proof; In the proof i dont understand about $j_i$'s.. Can some one interpret the proof in a simpler way(may be the original one is simpler) if someboy is rewriting the proof it would give me a better understanding.(if possible),,"['linear-algebra', 'positive-definite']"
82,Problem involving trace and determinant of symmetric matrices,Problem involving trace and determinant of symmetric matrices,,"I've stumbled upon this exercise on a linear algebra book that asks me to determine all the ordered pairs $(a,b)$ of real numbers to which there exists an unique symmetric matrix $A\in R^{2\times 2}$ so that $tr(A)=a$ and $Det(A)=b$. I don't even know how to tackle this problem, I've tried using Laplace expansion, and other properties of the trace and determinants. Any help will be appreciated.","I've stumbled upon this exercise on a linear algebra book that asks me to determine all the ordered pairs $(a,b)$ of real numbers to which there exists an unique symmetric matrix $A\in R^{2\times 2}$ so that $tr(A)=a$ and $Det(A)=b$. I don't even know how to tackle this problem, I've tried using Laplace expansion, and other properties of the trace and determinants. Any help will be appreciated.",,"['linear-algebra', 'determinant', 'trace']"
83,Finding minimum polynomial of $e^{i\pi/6 }$,Finding minimum polynomial of,e^{i\pi/6 },"Finding minimum polynomial of $e^{i\pi/6 }$: I know it satisfies $t^6 +1 = 0$. I factorized $t^6+1 = (t^2+1)(t^4-t^2+1)$ obviously it does not satisfy $t^2 +1$ so it must satisfy $t^4-t^2 +1$. How do I show that this is indeed the minimum? Is it enough to say: in modulo $2$ we have that $t^4 - t^2 + 1 \equiv t^4 +t^2 + 1$. In mod $2$ we only have 2 elements $0,1$. Obviously none of these satisfy $t^4+t^2+1 = 0$ hence it is irreducible in mod $2$ therefore in $\Bbb Q$?","Finding minimum polynomial of $e^{i\pi/6 }$: I know it satisfies $t^6 +1 = 0$. I factorized $t^6+1 = (t^2+1)(t^4-t^2+1)$ obviously it does not satisfy $t^2 +1$ so it must satisfy $t^4-t^2 +1$. How do I show that this is indeed the minimum? Is it enough to say: in modulo $2$ we have that $t^4 - t^2 + 1 \equiv t^4 +t^2 + 1$. In mod $2$ we only have 2 elements $0,1$. Obviously none of these satisfy $t^4+t^2+1 = 0$ hence it is irreducible in mod $2$ therefore in $\Bbb Q$?",,"['linear-algebra', 'abstract-algebra', 'polynomials', 'galois-theory', 'roots-of-unity']"
84,Change of basis and inner product in non-orthogonal basis,Change of basis and inner product in non-orthogonal basis,,"I have a vector, originally expressed in the standard coordinates system, and want to perform a change of basis and find coordinates in another basis, this basis being non-orthogonal. Let $B = \{e_1, e_2\}$ be the standard basis for $\Bbb R^2$ . Let $B' = \{e_1', e_2'\}$ be a non-orthogonal basis for $\Bbb R^2$ . Let $v$ be some vector in $\Bbb R^2$ . The standard inner product is $\langle a, b \rangle = \sum_{i=0}^n a_i b_i.$ I want to define an inner product in the non-orthogonal basis $B'$ so that $\langle e_1', e_2' \rangle_{B'} = 0$ since $\sum_{i=0}^n e_{1i}' e_{2i}' \neq 0$ . Basically, I want to use this new inner product to get the component/coordinates of the vector $v$ on the basis $B'$ .","I have a vector, originally expressed in the standard coordinates system, and want to perform a change of basis and find coordinates in another basis, this basis being non-orthogonal. Let be the standard basis for . Let be a non-orthogonal basis for . Let be some vector in . The standard inner product is I want to define an inner product in the non-orthogonal basis so that since . Basically, I want to use this new inner product to get the component/coordinates of the vector on the basis .","B = \{e_1, e_2\} \Bbb R^2 B' = \{e_1', e_2'\} \Bbb R^2 v \Bbb R^2 \langle a, b \rangle = \sum_{i=0}^n a_i b_i. B' \langle e_1', e_2' \rangle_{B'} = 0 \sum_{i=0}^n e_{1i}' e_{2i}' \neq 0 v B'",['linear-algebra']
85,Calculating the rank of a Boolean matrix and Boolean matrix factorization,Calculating the rank of a Boolean matrix and Boolean matrix factorization,,"I am interesting in some sort of algorithm for calculating the Boolean rank of small $M \times N$ Boolean matrices. Just to be clear, by Boolean matrices I mean matrices with entries $0$ or $1$ where $1+1=1$, and I am defining the Boolean rank as: Matrix $X$ has $rank(X) = 1$ iff $X = ab^T$ for column vectors $a$ and $b$ Matrix $X$ has $rank(X) ≤ k$ if it can be represented as a sum of $k$ rank-1 matrices. Smallest such $k$ is the rank of $X$ For example, $A = \pmatrix{1&1&0\\1&1&1\\0&1&1} = \pmatrix{1\\1\\0}\pmatrix{1&1&0} + \pmatrix{0\\1\\1}\pmatrix{0&1&1} = \pmatrix{1&1&0\\1&1&0\\0&0&0}+\pmatrix{0&0&0\\0&1&1\\0&1&1}$ so $rank(A)=2$ I am also interested in some way to factor an $M \times N$ Boolean matrix with rank $k$ into $M \times k$ and $k \times N$ Boolean matrices. For example, $A = \pmatrix{1&1&0\\1&1&1\\0&1&1} =\pmatrix{1&0\\1&1\\0&1}\pmatrix{1&1&0\\0&1&1}$","I am interesting in some sort of algorithm for calculating the Boolean rank of small $M \times N$ Boolean matrices. Just to be clear, by Boolean matrices I mean matrices with entries $0$ or $1$ where $1+1=1$, and I am defining the Boolean rank as: Matrix $X$ has $rank(X) = 1$ iff $X = ab^T$ for column vectors $a$ and $b$ Matrix $X$ has $rank(X) ≤ k$ if it can be represented as a sum of $k$ rank-1 matrices. Smallest such $k$ is the rank of $X$ For example, $A = \pmatrix{1&1&0\\1&1&1\\0&1&1} = \pmatrix{1\\1\\0}\pmatrix{1&1&0} + \pmatrix{0\\1\\1}\pmatrix{0&1&1} = \pmatrix{1&1&0\\1&1&0\\0&0&0}+\pmatrix{0&0&0\\0&1&1\\0&1&1}$ so $rank(A)=2$ I am also interested in some way to factor an $M \times N$ Boolean matrix with rank $k$ into $M \times k$ and $k \times N$ Boolean matrices. For example, $A = \pmatrix{1&1&0\\1&1&1\\0&1&1} =\pmatrix{1&0\\1&1\\0&1}\pmatrix{1&1&0\\0&1&1}$",,"['linear-algebra', 'matrices', 'matrix-rank']"
86,Optimal strategy for 2 players Lights Out game variation,Optimal strategy for 2 players Lights Out game variation,,"Consider a turn-based game for 2 players. They're both playing on the same board. The board is 8x8, randomly generated and each cell has 0 or 1 (with equal probabilities), for example: 01101100 01010101 10111100 00001111 10101010 00000001 11100011 00101000 In turn 1 player 1 moves, in turn 2 player 2 moves, in turn 3 player 1 moves and so on. Move consists of choosing coordinates (x, y) where 1 is. It flips elements on positions (x, y), (x+1, y) and (x, y+1), if they are in  bounds of the board. By flips I mean turns 1 to 0 or 0 to 1 . (0, 0) is top left corner. So, after playing in (1, 1) board looks like this: 01101100 00110101 11111100 00001111 10101010 00000001 11100011 00101000 And after playing in (7, 1) like this: 01101100 00110100 11111101 00001111 10101010 00000001 11100011 00101000 You win when after your move there are only 0 on board. My question is: what is the optimal strategy for this game? Research so far: Lights Out description on MathWorld is worth reading. Presented game is a variation of Lights Out, but instead of flipping 4 neighbor cells, we flip only 2 and we can move only in cells with 1 . It's also similar to Nim . In the same way like MathWorld describes we can find $S$ - a minimum set of moves to win. For 8x8 board there will be always exactly one solution. Since matrix addition is commutative, the order in which the moves are performed is irrelevant. If $|S|$ (minimal number of moves to win) is odd - it's a winning position. For example if it's 1 - we make only one move and we win. If it's 3 - we make a move from $S$ and opponent gets a board with 2 moves in $S$. Now, if he makes the move from $S$, he will lose, because we will have the final move. So, if he has an option, he will not make a move from $S$. By trying some examples I find out that opponent move not from $S$ increases our $S$ by 1 move - exactly the move he made. So, we get back the board with 3 (different) moves in $S$. But, he can't stop us like this forever - finally all his possible moves will be in $S$. So, my hypothesis is: When you don't make a move from $S$, $|S|$ will increase exactly by 1 and his ""destroying"" move will be added to the set (so later we can ""undo"" his ""destroying"" move). Which would mean that the opponent can only do -1 (by playing a move from $S$) or +1 (by not playing a move from  $S$), so he can't change the parity of our $|S|$! So, if all of above is true, it means that players have no influence on the result of the game. We could even play by random and if the initial $|S|$ was odd - we will win, if it was even - we will lose. It's hard for me to believe in that. What will be the sense of making AI challenge on HackerRank with such a game? Players in the leader-board have different scores and these scores are consistent each time after recalculation (same people in the top 10), suggesting that players actually have an influence on the game result. Maybe you can see some mistake I'm making?","Consider a turn-based game for 2 players. They're both playing on the same board. The board is 8x8, randomly generated and each cell has 0 or 1 (with equal probabilities), for example: 01101100 01010101 10111100 00001111 10101010 00000001 11100011 00101000 In turn 1 player 1 moves, in turn 2 player 2 moves, in turn 3 player 1 moves and so on. Move consists of choosing coordinates (x, y) where 1 is. It flips elements on positions (x, y), (x+1, y) and (x, y+1), if they are in  bounds of the board. By flips I mean turns 1 to 0 or 0 to 1 . (0, 0) is top left corner. So, after playing in (1, 1) board looks like this: 01101100 00110101 11111100 00001111 10101010 00000001 11100011 00101000 And after playing in (7, 1) like this: 01101100 00110100 11111101 00001111 10101010 00000001 11100011 00101000 You win when after your move there are only 0 on board. My question is: what is the optimal strategy for this game? Research so far: Lights Out description on MathWorld is worth reading. Presented game is a variation of Lights Out, but instead of flipping 4 neighbor cells, we flip only 2 and we can move only in cells with 1 . It's also similar to Nim . In the same way like MathWorld describes we can find $S$ - a minimum set of moves to win. For 8x8 board there will be always exactly one solution. Since matrix addition is commutative, the order in which the moves are performed is irrelevant. If $|S|$ (minimal number of moves to win) is odd - it's a winning position. For example if it's 1 - we make only one move and we win. If it's 3 - we make a move from $S$ and opponent gets a board with 2 moves in $S$. Now, if he makes the move from $S$, he will lose, because we will have the final move. So, if he has an option, he will not make a move from $S$. By trying some examples I find out that opponent move not from $S$ increases our $S$ by 1 move - exactly the move he made. So, we get back the board with 3 (different) moves in $S$. But, he can't stop us like this forever - finally all his possible moves will be in $S$. So, my hypothesis is: When you don't make a move from $S$, $|S|$ will increase exactly by 1 and his ""destroying"" move will be added to the set (so later we can ""undo"" his ""destroying"" move). Which would mean that the opponent can only do -1 (by playing a move from $S$) or +1 (by not playing a move from  $S$), so he can't change the parity of our $|S|$! So, if all of above is true, it means that players have no influence on the result of the game. We could even play by random and if the initial $|S|$ was odd - we will win, if it was even - we will lose. It's hard for me to believe in that. What will be the sense of making AI challenge on HackerRank with such a game? Players in the leader-board have different scores and these scores are consistent each time after recalculation (same people in the top 10), suggesting that players actually have an influence on the game result. Maybe you can see some mistake I'm making?",,"['linear-algebra', 'puzzle', 'game-theory', 'combinatorial-game-theory']"
87,"Given a vector space with two inner products, there is a linear transformation taking one to another","Given a vector space with two inner products, there is a linear transformation taking one to another",,"I am looking for some hint to the following question: Let $V$ be an $n$-dimensional real inner product space and let $\langle x,y\rangle$ and $[x,y] $ both be two different inner products on V. Prove that there exists a linear mapping $L : V → V $such that $$[L(x), L(y)] =  \langle x,y\rangle$$ for all $x,y \in V$. Thank you.","I am looking for some hint to the following question: Let $V$ be an $n$-dimensional real inner product space and let $\langle x,y\rangle$ and $[x,y] $ both be two different inner products on V. Prove that there exists a linear mapping $L : V → V $such that $$[L(x), L(y)] =  \langle x,y\rangle$$ for all $x,y \in V$. Thank you.",,"['linear-algebra', 'vector-spaces', 'inner-products', 'linear-transformations']"
88,$3 \times 3$ matrices are similar if and only if they have the same characteristic and minimal polynomial,matrices are similar if and only if they have the same characteristic and minimal polynomial,3 \times 3,"I want to prove: $B$ is similar to $A \Leftrightarrow m_A(x) = m_B(x)$ and $P_A(x) = P_B(x)$, where $m,P$ are the minimal and characteristic polynomial, respectively. ""$\Rightarrow$"" Let $A$ to be similar to $B$, then they have the same rational canonical form. On the other hand, the characteristic polynomial is the product of the invariant factors, but the invariant factors of an $n \times n$ matrix over a field $F$ are the invariant factors of its canonical form. Since $A,B$ have the same rational canonical form, then the characteristic polynomials are the same, hence: $P_A(x) = P_B(x)$, but the minimal polynomial is the largest invariant factor, so the minimal polynomials are the same and every other invariant factor divides the minimal polynomial, hence: $m_A(x) = m_B(x)$. (This comes from the fact that similar matrices have the same determinant) ""$\Leftarrow$"" Let $A, B \in M_3(F)$ with the property that they have the same characteristic and minimal polynomial, that is: $P_A(x) = P_B(x) = f(x),$ $m_A(x) = m_B(x) = g(x).$ we want to prove that $A$ and $B$ are similar. First, recall that since we have a $3 \times 3$ matrix, this implies that the characteristic polynomial will have degree $3$, $g | f$ and every irreducible factor of $f(x)$ appears in $g(x)$. Moreover the degree of $g(x)$ is at most $3$. To prove similarity, it is enough to prove that they have the same set of invariant factors, which implies that the companion matrices will be the same. $\deg(g(x)) = 3$ If the degree of $g(x)$ is $3$, basically we cannot do a lot, so, we have that: $f(x) = g(x)$, so in this case the minimal polynomial is just $f(x)$ which implies that they are similar. $\deg(g(x)) = 2$ Notice that $f(x) = (x-a)g(x)$ for some $a \in F$ and $(x-a)$ is irreducible. On the other hand, every irreducible factor of the characteristic polynomial must appear in $g(x)$, which implies that: $g(x) = (x-a)(x-b)$ for some $b \in F$ and $f(x) = (x-a)^2 (x-b)$. Again, in this case the set of invariant factors is given by $\{(x-a),(x-a)(x-b)\}$ Then, they share the same invariant factors, so the companion matrices are the same, hence there are similar. $\deg(g(x)) = 1$ clearly, we have that: $g(x) = (x-a)$ for some $a \in F$. Then, the only possibility is that the invariant factors have the following form: $$\{(x-a),(x-a),(x-a)\}$$ Then, the matrices, $A,B$ have the same set of invariant factors and the same companion matrix, which implies that they are similar","I want to prove: $B$ is similar to $A \Leftrightarrow m_A(x) = m_B(x)$ and $P_A(x) = P_B(x)$, where $m,P$ are the minimal and characteristic polynomial, respectively. ""$\Rightarrow$"" Let $A$ to be similar to $B$, then they have the same rational canonical form. On the other hand, the characteristic polynomial is the product of the invariant factors, but the invariant factors of an $n \times n$ matrix over a field $F$ are the invariant factors of its canonical form. Since $A,B$ have the same rational canonical form, then the characteristic polynomials are the same, hence: $P_A(x) = P_B(x)$, but the minimal polynomial is the largest invariant factor, so the minimal polynomials are the same and every other invariant factor divides the minimal polynomial, hence: $m_A(x) = m_B(x)$. (This comes from the fact that similar matrices have the same determinant) ""$\Leftarrow$"" Let $A, B \in M_3(F)$ with the property that they have the same characteristic and minimal polynomial, that is: $P_A(x) = P_B(x) = f(x),$ $m_A(x) = m_B(x) = g(x).$ we want to prove that $A$ and $B$ are similar. First, recall that since we have a $3 \times 3$ matrix, this implies that the characteristic polynomial will have degree $3$, $g | f$ and every irreducible factor of $f(x)$ appears in $g(x)$. Moreover the degree of $g(x)$ is at most $3$. To prove similarity, it is enough to prove that they have the same set of invariant factors, which implies that the companion matrices will be the same. $\deg(g(x)) = 3$ If the degree of $g(x)$ is $3$, basically we cannot do a lot, so, we have that: $f(x) = g(x)$, so in this case the minimal polynomial is just $f(x)$ which implies that they are similar. $\deg(g(x)) = 2$ Notice that $f(x) = (x-a)g(x)$ for some $a \in F$ and $(x-a)$ is irreducible. On the other hand, every irreducible factor of the characteristic polynomial must appear in $g(x)$, which implies that: $g(x) = (x-a)(x-b)$ for some $b \in F$ and $f(x) = (x-a)^2 (x-b)$. Again, in this case the set of invariant factors is given by $\{(x-a),(x-a)(x-b)\}$ Then, they share the same invariant factors, so the companion matrices are the same, hence there are similar. $\deg(g(x)) = 1$ clearly, we have that: $g(x) = (x-a)$ for some $a \in F$. Then, the only possibility is that the invariant factors have the following form: $$\{(x-a),(x-a),(x-a)\}$$ Then, the matrices, $A,B$ have the same set of invariant factors and the same companion matrix, which implies that they are similar",,"['linear-algebra', 'abstract-algebra', 'proof-verification']"
89,Conditioning of Triangular Matrices:,Conditioning of Triangular Matrices:,,"Let $U \in \mathbb{R}^{N\times N}$ be upper triangular. $U$ is well conditioned if  the magnitude of the diagonal elements is sufficiently large compared to that of the corresponding off-diagonal elements. I'm trying to show that if $|u_{ii}| -2\,\displaystyle\sum_{j\neq i}{|u_{ij}|}>0$, for all  $1\leq i\leq N,$ then $\kappa_{\infty}(U)\leq 2\>\dfrac{\|U\|_{\infty}}{\min_{1\leq j\leq N}{|u_{jj}|}}.$ I am struggling to show $\|U^{-1}\|_{\infty} \le \dfrac{2}{\min_{1\leq j\leq N}{|u_{jj}|}}$. Does anyone have any ideas?","Let $U \in \mathbb{R}^{N\times N}$ be upper triangular. $U$ is well conditioned if  the magnitude of the diagonal elements is sufficiently large compared to that of the corresponding off-diagonal elements. I'm trying to show that if $|u_{ii}| -2\,\displaystyle\sum_{j\neq i}{|u_{ij}|}>0$, for all  $1\leq i\leq N,$ then $\kappa_{\infty}(U)\leq 2\>\dfrac{\|U\|_{\infty}}{\min_{1\leq j\leq N}{|u_{jj}|}}.$ I am struggling to show $\|U^{-1}\|_{\infty} \le \dfrac{2}{\min_{1\leq j\leq N}{|u_{jj}|}}$. Does anyone have any ideas?",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
90,"Prove that for a set of self-adjoint projectors on a Hilbert space, $\sum_k P_k$ is a projection iff $P_i P_j=0$","Prove that for a set of self-adjoint projectors on a Hilbert space,  is a projection iff",\sum_k P_k P_i P_j=0,"It is known that a projection operator can be written explicitly as follows: $$\hat{P} = \sum_{k=1}^n \hat{P_k} = \sum_{k=1}^n | k \rangle\langle k|$$ where $\{|k\rangle$, $k= 1,\ldots,n\}$ are the orthonormal basis. So it is curious to ask if a projection should always be written as the sum of projection operators to the smaller subspaces, which are orthogonal to each other. This is proved as following. Given two projection operators $\hat{P_1}$ and $\hat{P_2}$, for the sum $\hat{P} = \hat{P_1} + \hat{P_2}$ to be also a projection operator: $$(\hat{P_1} + \hat{P_2})^2 = \hat{P_1} + \hat{P_2} \\  \Rightarrow \hat{P_1}^2 + \hat{P_2}^2 + \hat{P_1}\hat{P_2}  + \hat{P_2}\hat{P_1} \\ = \hat{P_1} + \hat{P_2} + \hat{P_1}\hat{P_2} + \hat{P_2}\hat{P_1} = \hat{P_1} + \hat{P_2}$$ Therefore we get $$\hat{P_1}\hat{P_2} + \hat{P_2}\hat{P_1}\tag 1 = 0$$ (1) left-multiplied by $\hat{P_1}$, we get: $$\hat{P_1}\hat{P_2} + \hat{P_1}\hat{P_2}\hat{P_1} = 0\tag 2$$ (1) right-multiplied by $\hat{P_1}$, we get: $$\hat{P_1}\hat{P_2}\hat{P_1} + \hat{P_2}\hat{P_1} = 0\tag3$$ (2) - (3) gives: $$\hat{P_1}\hat{P_2} - \hat{P_2}\hat{P_1} = 0\tag4$$ (1) + (4) eventually gives: $$\hat{P_1}\hat{P_2} = \hat{P_2}\hat{P_1} = 0\tag5$$ The above proved that equation (5) is the necessary condition for the sum $\hat{P} = \hat{P_1} + \hat{P_2}$ to be also a projection operator. It is straightforward to see that (5) is the sufficient condition, too. So we conclude that: $$(\hat{P_1} + \hat{P_2})^2 = \hat{P_1} + \hat{P_2} \\ \Longleftrightarrow \hat{P_1}\hat{P_2} = \hat{P_2}\hat{P_1} = 0$$ given two projection operators $\hat{P_1}$ and $\hat{P_2}$. My question is: what is the general theory/theorem that formally addressed the above question and stated the above result?","It is known that a projection operator can be written explicitly as follows: $$\hat{P} = \sum_{k=1}^n \hat{P_k} = \sum_{k=1}^n | k \rangle\langle k|$$ where $\{|k\rangle$, $k= 1,\ldots,n\}$ are the orthonormal basis. So it is curious to ask if a projection should always be written as the sum of projection operators to the smaller subspaces, which are orthogonal to each other. This is proved as following. Given two projection operators $\hat{P_1}$ and $\hat{P_2}$, for the sum $\hat{P} = \hat{P_1} + \hat{P_2}$ to be also a projection operator: $$(\hat{P_1} + \hat{P_2})^2 = \hat{P_1} + \hat{P_2} \\  \Rightarrow \hat{P_1}^2 + \hat{P_2}^2 + \hat{P_1}\hat{P_2}  + \hat{P_2}\hat{P_1} \\ = \hat{P_1} + \hat{P_2} + \hat{P_1}\hat{P_2} + \hat{P_2}\hat{P_1} = \hat{P_1} + \hat{P_2}$$ Therefore we get $$\hat{P_1}\hat{P_2} + \hat{P_2}\hat{P_1}\tag 1 = 0$$ (1) left-multiplied by $\hat{P_1}$, we get: $$\hat{P_1}\hat{P_2} + \hat{P_1}\hat{P_2}\hat{P_1} = 0\tag 2$$ (1) right-multiplied by $\hat{P_1}$, we get: $$\hat{P_1}\hat{P_2}\hat{P_1} + \hat{P_2}\hat{P_1} = 0\tag3$$ (2) - (3) gives: $$\hat{P_1}\hat{P_2} - \hat{P_2}\hat{P_1} = 0\tag4$$ (1) + (4) eventually gives: $$\hat{P_1}\hat{P_2} = \hat{P_2}\hat{P_1} = 0\tag5$$ The above proved that equation (5) is the necessary condition for the sum $\hat{P} = \hat{P_1} + \hat{P_2}$ to be also a projection operator. It is straightforward to see that (5) is the sufficient condition, too. So we conclude that: $$(\hat{P_1} + \hat{P_2})^2 = \hat{P_1} + \hat{P_2} \\ \Longleftrightarrow \hat{P_1}\hat{P_2} = \hat{P_2}\hat{P_1} = 0$$ given two projection operators $\hat{P_1}$ and $\hat{P_2}$. My question is: what is the general theory/theorem that formally addressed the above question and stated the above result?",,"['linear-algebra', 'operator-theory', 'projection']"
91,Trace inequality involving $A^TA-AA^T$,Trace inequality involving,A^TA-AA^T,"Let $A\in\mathbb{R}^{n\times n}$ and $B=(A^TA-AA^T)/2$. Furthermore, let $$ f(A)=\frac{1}{n}\text{Tr}(A),\qquad g(A)=\sqrt{f(A^TA)-(f(A))^2} $$ where $\text{Tr}(A)$ denotes the trace of the matrix $A$. After numerous experiments I think that the following bound holds: $$ g(B)\leq (g(A))^2 $$ How to prove/disprove this statements ? Some useful properties that I have found: $g(A)\geq 0$ for all $A$, follows directly from Cauchy-Schwarz inequality. $g(sI+A)=g(A)$, for all $s\in\mathbb{R}$. EDIT: I see now that I must have misread my notes, and in fact, the inequality I was after is $C=(A-A^T)/2$ and $$ g(C^2)\leq(g(A))^2 $$ Sorry for the inconviencience.","Let $A\in\mathbb{R}^{n\times n}$ and $B=(A^TA-AA^T)/2$. Furthermore, let $$ f(A)=\frac{1}{n}\text{Tr}(A),\qquad g(A)=\sqrt{f(A^TA)-(f(A))^2} $$ where $\text{Tr}(A)$ denotes the trace of the matrix $A$. After numerous experiments I think that the following bound holds: $$ g(B)\leq (g(A))^2 $$ How to prove/disprove this statements ? Some useful properties that I have found: $g(A)\geq 0$ for all $A$, follows directly from Cauchy-Schwarz inequality. $g(sI+A)=g(A)$, for all $s\in\mathbb{R}$. EDIT: I see now that I must have misread my notes, and in fact, the inequality I was after is $C=(A-A^T)/2$ and $$ g(C^2)\leq(g(A))^2 $$ Sorry for the inconviencience.",,"['linear-algebra', 'matrices', 'inequality', 'trace']"
92,Bilinear Map vs Inner Product,Bilinear Map vs Inner Product,,What is the difference between a Bilinear Map and a Inner Product?,What is the difference between a Bilinear Map and a Inner Product?,,"['linear-algebra', 'terminology', 'inner-products']"
93,Proof that a group representation matrix is diagonalizable?,Proof that a group representation matrix is diagonalizable?,,"Suppose we have a finite group $G$ and and an $n$-dimensional vector space $V\cong \Bbb C^n$ over the field $\Bbb C$ of complex number. My professor said the other day that for every group element $g$ in $G$ and for every representation $\rho:G\to\text{GL}_n(\Bbb C)$ of $G$ on $V$, the matrix $\rho(g)\in\text{GL}_n(\Bbb C)$ is diagonalizable. I'm having trouble convincing myself that this is true. The text below is from a website that agrees with what my professor said: $~~~~~$ I believe that all the eigenvalues of $M$ must be $n$-th roots of unity, and that each eigenvalue appears only once as root of $M$'s minimal polynomial. I am having trouble convincing myself, however, that this means $M$ is diagonalizable. How do we know that the eigenvectors of $M$ span the vector space $V$? Couldn't $M$ have a minimal polynomial of degree less than $n$? Any help would be much appreciated.","Suppose we have a finite group $G$ and and an $n$-dimensional vector space $V\cong \Bbb C^n$ over the field $\Bbb C$ of complex number. My professor said the other day that for every group element $g$ in $G$ and for every representation $\rho:G\to\text{GL}_n(\Bbb C)$ of $G$ on $V$, the matrix $\rho(g)\in\text{GL}_n(\Bbb C)$ is diagonalizable. I'm having trouble convincing myself that this is true. The text below is from a website that agrees with what my professor said: $~~~~~$ I believe that all the eigenvalues of $M$ must be $n$-th roots of unity, and that each eigenvalue appears only once as root of $M$'s minimal polynomial. I am having trouble convincing myself, however, that this means $M$ is diagonalizable. How do we know that the eigenvectors of $M$ span the vector space $V$? Couldn't $M$ have a minimal polynomial of degree less than $n$? Any help would be much appreciated.",,"['linear-algebra', 'representation-theory']"
94,Is the Frobenius norm minimizer the same as the $2$-norm minimizer?,Is the Frobenius norm minimizer the same as the -norm minimizer?,2,"Given matrices $A \in \mathbb{R}^{n \times m}$ and $B \in \mathbb{R}^{n \times k}$ , consider the least-squares minimizer $$\arg \min_{X \in \mathbb{R}^{m \times k}} \| AX - B \|_{\text{F}}$$ where $\| M \|_{\text{F}} := \left( \sum_i \sum_j M_{ij}^2 \right)^{1/2}$ denotes the Frobenius norm. I was wondering if this is identical to the minimizer $$\arg \min_{X \in \mathbb{R}^{m \times k}} \| AX - B \|_2$$ where $\| M \|_2 := \sigma_1 (M)$ denotes the matrix $2$ -norm. Since $$\sqrt{\operatorname{rank}(M)} \cdot \| M \|_2 \geq \| M \|_F \geq \| M \|_2$$ I feel there must be a simple argument to show these minimizers are identical. If it is not so, can you please describe what assumptions on matrices $A$ and $B$ are necessary for the minimizers to be identical?","Given matrices and , consider the least-squares minimizer where denotes the Frobenius norm. I was wondering if this is identical to the minimizer where denotes the matrix -norm. Since I feel there must be a simple argument to show these minimizers are identical. If it is not so, can you please describe what assumptions on matrices and are necessary for the minimizers to be identical?",A \in \mathbb{R}^{n \times m} B \in \mathbb{R}^{n \times k} \arg \min_{X \in \mathbb{R}^{m \times k}} \| AX - B \|_{\text{F}} \| M \|_{\text{F}} := \left( \sum_i \sum_j M_{ij}^2 \right)^{1/2} \arg \min_{X \in \mathbb{R}^{m \times k}} \| AX - B \|_2 \| M \|_2 := \sigma_1 (M) 2 \sqrt{\operatorname{rank}(M)} \cdot \| M \|_2 \geq \| M \|_F \geq \| M \|_2 A B,"['linear-algebra', 'matrices', 'optimization', 'least-squares', 'spectral-norm']"
95,Inner Product on a Vector Space over a field besides $\mathbb R$ or $\mathbb C$?,Inner Product on a Vector Space over a field besides  or ?,\mathbb R \mathbb C,"Are there any fields with vector spaces you can define an inner product over besides subfields of $\mathbb C$? I know that you'd want the field to contain an ordered subfield, so it must have characteristic $0$. Is there any inner product on, say, the ordered field of rational functions?","Are there any fields with vector spaces you can define an inner product over besides subfields of $\mathbb C$? I know that you'd want the field to contain an ordered subfield, so it must have characteristic $0$. Is there any inner product on, say, the ordered field of rational functions?",,"['linear-algebra', 'abstract-algebra', 'examples-counterexamples']"
96,using the Kronecker product and vec operators to write the following least squares problem in standard matrix form,using the Kronecker product and vec operators to write the following least squares problem in standard matrix form,,"I have a least squares problem with the following form: $$ \min_\mathbf{X} ~ \sum_{i=1}^n | \mathbf{u}_i^\top \mathbf{X} \mathbf{v}_i - b_i |^2 $$ where $\{\mathbf{u}_i\}_{i=1}^n$ and $\{\mathbf{v}_i\}_{i=1}^n$ are vectors, the $\{b_i\}_{i=1}^n$ are scalars, and $\mathbf{X}$ is a matrix. I would like to rewrite it in the more standard form which I can hand off to a solver in matlab or numpy: $$ \min_\mathbf{X} ~ \left\| \mathbf{A} \mathbf{x} - \mathbf{b} \right\|^2 $$ where $\mathbf{x} = \text{vec}(\mathbf{X})$ and $\mathbf{b}$ is a vector whose $i$'th component is the scalar $b_i$ from the first equation. I've figured out how to this for a single term of the sum. $$ \mathbf{u}^\top \mathbf{X} \mathbf{v} = \sum_{ij} \mathbf{X}_{ij} u_i v_j = \left(\text{vec}(\mathbf{u}\mathbf{v}^\top)\right)^\top \text{vec}(\mathbf{X}) = (\mathbf{v} \otimes \mathbf{u})^\top\, \text{vec}(\mathbf{X}).$$ But my actual problem doesn't just have a single $\mathbf{u}$ and $\mathbf{v}$, but instead, whole matrices $\mathbf{U}$ and $\mathbf{V}$ whose columns are the $\{\mathbf{u}_i\}$ and $\{\mathbf{v}_i\}$. I can't go from the single term case to the $n$-term case by letting $$A = \mathbf{V} \otimes \mathbf{U}$$ analagous to $\mathbf{v} \otimes \mathbf{u}$ because this produces a wrong matrix for $\mathbf{A}$.  What I need is a way to write $\mathbf{A}$ such that its $i$'th row contains $\text{vec}(\mathbf{u}_i \mathbf{v}_i^\top)$ which suggests that I need to write $\mathbf{A}$ as a flattened cube whose $i$'th level contains the matrix $\mathbf{u}_i \mathbf{v}_i^\top$. What is the way of writing this, both in standard mathematical notation and in the notation of numeric programming languages like Matlab or Numpy? Any help here is appreciated. Many thanks.","I have a least squares problem with the following form: $$ \min_\mathbf{X} ~ \sum_{i=1}^n | \mathbf{u}_i^\top \mathbf{X} \mathbf{v}_i - b_i |^2 $$ where $\{\mathbf{u}_i\}_{i=1}^n$ and $\{\mathbf{v}_i\}_{i=1}^n$ are vectors, the $\{b_i\}_{i=1}^n$ are scalars, and $\mathbf{X}$ is a matrix. I would like to rewrite it in the more standard form which I can hand off to a solver in matlab or numpy: $$ \min_\mathbf{X} ~ \left\| \mathbf{A} \mathbf{x} - \mathbf{b} \right\|^2 $$ where $\mathbf{x} = \text{vec}(\mathbf{X})$ and $\mathbf{b}$ is a vector whose $i$'th component is the scalar $b_i$ from the first equation. I've figured out how to this for a single term of the sum. $$ \mathbf{u}^\top \mathbf{X} \mathbf{v} = \sum_{ij} \mathbf{X}_{ij} u_i v_j = \left(\text{vec}(\mathbf{u}\mathbf{v}^\top)\right)^\top \text{vec}(\mathbf{X}) = (\mathbf{v} \otimes \mathbf{u})^\top\, \text{vec}(\mathbf{X}).$$ But my actual problem doesn't just have a single $\mathbf{u}$ and $\mathbf{v}$, but instead, whole matrices $\mathbf{U}$ and $\mathbf{V}$ whose columns are the $\{\mathbf{u}_i\}$ and $\{\mathbf{v}_i\}$. I can't go from the single term case to the $n$-term case by letting $$A = \mathbf{V} \otimes \mathbf{U}$$ analagous to $\mathbf{v} \otimes \mathbf{u}$ because this produces a wrong matrix for $\mathbf{A}$.  What I need is a way to write $\mathbf{A}$ such that its $i$'th row contains $\text{vec}(\mathbf{u}_i \mathbf{v}_i^\top)$ which suggests that I need to write $\mathbf{A}$ as a flattened cube whose $i$'th level contains the matrix $\mathbf{u}_i \mathbf{v}_i^\top$. What is the way of writing this, both in standard mathematical notation and in the notation of numeric programming languages like Matlab or Numpy? Any help here is appreciated. Many thanks.",,"['linear-algebra', 'optimization', 'kronecker-product']"
97,Motivation for Conjugate transpose of a matrix,Motivation for Conjugate transpose of a matrix,,I'am currently going through a self study of Linear algebra . I'am finding it difficult to grasp the intuition behind the concept of Conjugate transpose of a matrix .Why take the complex conjugate of each entry after a transpose for a complex matrix ?Can someone help me fill that gap ?,I'am currently going through a self study of Linear algebra . I'am finding it difficult to grasp the intuition behind the concept of Conjugate transpose of a matrix .Why take the complex conjugate of each entry after a transpose for a complex matrix ?Can someone help me fill that gap ?,,"['linear-algebra', 'matrices', 'intuition']"
98,When does this matrix have an integral square root?,When does this matrix have an integral square root?,,"Let $d_1$, $d_2$, ..., $d_n$ be positive integers. Let $B$ be the $n \times n$ matrix $$\begin{pmatrix} d_1 & 1 & 1 & \cdots & 1 \\ 1 & d_2 & 1 & \cdots & 1 \\ 1 & 1 & d_3 & \cdots & 1 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & 1 & 1 & \cdots & d_n \end{pmatrix}.$$ When does $B$ have a square root in $\mathrm{Mat}_n(\mathbb{Z})$? Motivation: The Friendship Theorem states that the only graph in which every pair of vertices is joined by a path of length $2$ is the ""Friendship Graph"", which you can see at the linked article. If $A$ is the adjacency matrix of such a graph, with degree sequence $(d_1, d_2, \ldots, d_n)$, then $A^2=B$. So this contributes the solution $(d_1, d_2, \ldots, d_n) = (2,2,2,\ldots,2,2m)$, with $n=2m+1$. I was preparing notes on the friendship theorem and got distracted by trying to figure out when this matrix has an integer square root at all. It seemed like it might make a nice challenge for here.","Let $d_1$, $d_2$, ..., $d_n$ be positive integers. Let $B$ be the $n \times n$ matrix $$\begin{pmatrix} d_1 & 1 & 1 & \cdots & 1 \\ 1 & d_2 & 1 & \cdots & 1 \\ 1 & 1 & d_3 & \cdots & 1 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & 1 & 1 & \cdots & d_n \end{pmatrix}.$$ When does $B$ have a square root in $\mathrm{Mat}_n(\mathbb{Z})$? Motivation: The Friendship Theorem states that the only graph in which every pair of vertices is joined by a path of length $2$ is the ""Friendship Graph"", which you can see at the linked article. If $A$ is the adjacency matrix of such a graph, with degree sequence $(d_1, d_2, \ldots, d_n)$, then $A^2=B$. So this contributes the solution $(d_1, d_2, \ldots, d_n) = (2,2,2,\ldots,2,2m)$, with $n=2m+1$. I was preparing notes on the friendship theorem and got distracted by trying to figure out when this matrix has an integer square root at all. It seemed like it might make a nice challenge for here.",,"['linear-algebra', 'number-theory', 'graph-theory']"
99,Complex numbers and their matrix form.,Complex numbers and their matrix form.,,"I have a line starting at the origin, and i extend it to a point $(a,b)$ in the plane.  This thing can be called a vector and be represented as $(a,b), [a\text{ }b]^T$ (column vector) or by $a\mathbf{i}+b\mathbf{j}$, where $(\mathbf{i},\mathbf{j})$ is the stardard basis in $\mathbb{R}^2$  Or it could be seen as a visual representation of a complex number where $(a,b)=a+bi,$ where $i=\sqrt{-1}$. So I want to rotate this vector $(a,b)$ $90$ degrees counter clockwise, so i know I can use my trusty matrix for rotations  $\begin{bmatrix} \cos(90) & -\sin(90) \\ \sin(90) & \cos(90)\\ \end{bmatrix}$=$\begin{bmatrix} 0 & -1 \\ 1 & 0\\ \end{bmatrix}$ and we find that $$\begin{bmatrix} 0 & -1 \\ 1 & 0\\ \end{bmatrix}\begin{bmatrix} a \\ b\\ \end{bmatrix}=\begin{bmatrix} -b \\ a\\ \end{bmatrix}$$ Or, I could choose the complex multiplication way and say,  $i(a+bi)=ai+bi^2=ai-b=-b+ai$ So we all know that, but what are some of the advantages and disadvantages to having two things that are completely identical operation in different systems?","I have a line starting at the origin, and i extend it to a point $(a,b)$ in the plane.  This thing can be called a vector and be represented as $(a,b), [a\text{ }b]^T$ (column vector) or by $a\mathbf{i}+b\mathbf{j}$, where $(\mathbf{i},\mathbf{j})$ is the stardard basis in $\mathbb{R}^2$  Or it could be seen as a visual representation of a complex number where $(a,b)=a+bi,$ where $i=\sqrt{-1}$. So I want to rotate this vector $(a,b)$ $90$ degrees counter clockwise, so i know I can use my trusty matrix for rotations  $\begin{bmatrix} \cos(90) & -\sin(90) \\ \sin(90) & \cos(90)\\ \end{bmatrix}$=$\begin{bmatrix} 0 & -1 \\ 1 & 0\\ \end{bmatrix}$ and we find that $$\begin{bmatrix} 0 & -1 \\ 1 & 0\\ \end{bmatrix}\begin{bmatrix} a \\ b\\ \end{bmatrix}=\begin{bmatrix} -b \\ a\\ \end{bmatrix}$$ Or, I could choose the complex multiplication way and say,  $i(a+bi)=ai+bi^2=ai-b=-b+ai$ So we all know that, but what are some of the advantages and disadvantages to having two things that are completely identical operation in different systems?",,"['linear-algebra', 'matrices', 'complex-numbers', 'rotations']"
