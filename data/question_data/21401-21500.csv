,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"""Non-linear"" algebra","""Non-linear"" algebra",,Linear algebra studies vector spaces and linear mappings between those spaces. What tools do we use for NON-linear mappings between vector spaces?,Linear algebra studies vector spaces and linear mappings between those spaces. What tools do we use for NON-linear mappings between vector spaces?,,"['linear-algebra', 'abstract-algebra', 'vector-spaces']"
1,Easy way to calculate inverse of an LU decomposition.,Easy way to calculate inverse of an LU decomposition.,,I have a matrix A and a lower triangular matrix L (with 1's along the diagonal) and an upper triangular matrix U. These are constructed such that $A=LU$. I know that $A^{-1} = L^{-1}U^{-1}$ and I know that the inverse of L is simply the non-diagonal entries with their signs flipped. Question: Is there an easy way to find the inverse of U? example: $$\begin{bmatrix}8 & 1 &6\\3 & 5 & 7\\4&9&2\end{bmatrix}^{-1} = \begin{bmatrix}1 & 0 &0\\-.5 & 1 & 0\\-.375 & -.544 & 1\end{bmatrix}\begin{bmatrix}8 & 1 &6\\0 & 8.5 & -1\\0&0&5.294\end{bmatrix}^{-1}$$ I need to find an algorithm for computing the inverse of the far right upper triangular matrix.,I have a matrix A and a lower triangular matrix L (with 1's along the diagonal) and an upper triangular matrix U. These are constructed such that $A=LU$. I know that $A^{-1} = L^{-1}U^{-1}$ and I know that the inverse of L is simply the non-diagonal entries with their signs flipped. Question: Is there an easy way to find the inverse of U? example: $$\begin{bmatrix}8 & 1 &6\\3 & 5 & 7\\4&9&2\end{bmatrix}^{-1} = \begin{bmatrix}1 & 0 &0\\-.5 & 1 & 0\\-.375 & -.544 & 1\end{bmatrix}\begin{bmatrix}8 & 1 &6\\0 & 8.5 & -1\\0&0&5.294\end{bmatrix}^{-1}$$ I need to find an algorithm for computing the inverse of the far right upper triangular matrix.,,"['linear-algebra', 'matrices', 'matrix-decomposition']"
2,Regarding a Basis for Infinite Dimensional Vector Spaces,Regarding a Basis for Infinite Dimensional Vector Spaces,,"In my linear algebra class, during the discussion of vector spaces, our instructor mentioned infinite dimensional spaces, including the polynomial space over Q and the space of all continuous functions over the interval [0,1]. Our teacher also warned that although Q[x] has an infinite basis, the actual elements of the vector space can be described as linear combinations of a finite subset of the basis. My question is this: is this always the case? Or are some vectors described only by a sort of ""infinite linear combination"" of their basis elements. Also, what do the basis elements of functional spaces, such as the one mentioned above, look like? And in what way are they defined?","In my linear algebra class, during the discussion of vector spaces, our instructor mentioned infinite dimensional spaces, including the polynomial space over Q and the space of all continuous functions over the interval [0,1]. Our teacher also warned that although Q[x] has an infinite basis, the actual elements of the vector space can be described as linear combinations of a finite subset of the basis. My question is this: is this always the case? Or are some vectors described only by a sort of ""infinite linear combination"" of their basis elements. Also, what do the basis elements of functional spaces, such as the one mentioned above, look like? And in what way are they defined?",,"['linear-algebra', 'functional-analysis', 'polynomials', 'vector-spaces']"
3,If $B$ is a maximal linearly independent set in $V$ then $B$ is a basis for $V$,If  is a maximal linearly independent set in  then  is a basis for,B V B V,"How can you show that if $B$ is a maximal linearly independent set of $V$, then this implies that $B$ is a basis of $V$?","How can you show that if $B$ is a maximal linearly independent set of $V$, then this implies that $B$ is a basis of $V$?",,['linear-algebra']
4,Proving $\det \left( \begin{smallmatrix} A & -B \\ B & A \end{smallmatrix} \right) =|\det(A+iB)|^2$,Proving,\det \left( \begin{smallmatrix} A & -B \\ B & A \end{smallmatrix} \right) =|\det(A+iB)|^2,"The complex general linear group is a subgroup of the group of real matrices of twice the dimension and with positive determinant. Let us decompose complex matrices $M$ as $M=A+iB$ , where $A,B$ are real matrices. Now consider the correspondence $$f(A+iB)=\begin{pmatrix} A & -B \\ B & A\end{pmatrix}.$$ If $\det f(M)=|\det M|^2$ for square matrices, then we would have $GL(n,\mathbb C)\subseteq GL_+(2n,\mathbb R)$ with the identification $M\to f(M)$ , which is an injective homomorphism. In other words, the complex general linear group would be a subgroup of the group of real matrices of twice the dimension and with positive determinant. How is $\det f(M)=|\det M|^2$ ?","The complex general linear group is a subgroup of the group of real matrices of twice the dimension and with positive determinant. Let us decompose complex matrices as , where are real matrices. Now consider the correspondence If for square matrices, then we would have with the identification , which is an injective homomorphism. In other words, the complex general linear group would be a subgroup of the group of real matrices of twice the dimension and with positive determinant. How is ?","M M=A+iB A,B f(A+iB)=\begin{pmatrix} A & -B \\ B & A\end{pmatrix}. \det f(M)=|\det M|^2 GL(n,\mathbb C)\subseteq GL_+(2n,\mathbb R) M\to f(M) \det f(M)=|\det M|^2","['linear-algebra', 'matrices', 'complex-numbers', 'determinant']"
5,Why invariance to change of basis is so important in linear algebra?,Why invariance to change of basis is so important in linear algebra?,,"I'm reading a book on linear algebra and I see that for every new presented concept (from simple vectors and linear functions and up to tensors) we immediately study how does it behave under a change of basis. Is it invariant or not, etc. This idea seems to be extremely important, if not central for LA. My question is - why? Why is it so important? My only vague thought so far is that talking/thinking about something without use of matrices and numeric coordinate to represent it, can be simpler than with them. But this is the point where my understanding ends.","I'm reading a book on linear algebra and I see that for every new presented concept (from simple vectors and linear functions and up to tensors) we immediately study how does it behave under a change of basis. Is it invariant or not, etc. This idea seems to be extremely important, if not central for LA. My question is - why? Why is it so important? My only vague thought so far is that talking/thinking about something without use of matrices and numeric coordinate to represent it, can be simpler than with them. But this is the point where my understanding ends.",,['linear-algebra']
6,Are there sometimes only finitely many square roots of a positive matrix?,Are there sometimes only finitely many square roots of a positive matrix?,,"Question: A positive (semi-) definite matrix has a unique positive (semi-) definite square root. What are its other square roots? In some cases there are infinitely many (such as for $aI$). Are there cases where there are finitely many? In [this][1] question it is shown that the identity matrix has infinitely many square roots (although of course it has a unique positive square root). But it doesn't seem to address the general question of the square roots of a positive operator. The behavior of the class of matrices $$\left( \begin{matrix} \sqrt{a} & t \\ 0 & -\sqrt{a} \end{matrix} \right)$$ as square roots of $aI$ seems to be due to the ""merging"" of the $-\sqrt{a}$ and $\sqrt{a}$ eigenspaces when you square. This might suggest that for a positive matrix with distinct eigenvalues, there are only $2^r$ square roots, with $r$ the rank of the matrix? Ideas: One thing that might narrow it down: if a matrix is normal, will all its square roots be normal, or not necessarily? If we know this, we can go straight to working with eigenvalues. [1]: In a finite dimensional vector space a positive operator can have infinitely many square roots [2]: https://math.stackexchange.com/questions/558072/how-many-square-roots-not-necessarily-positive-does-a-positive-matrix-have","Question: A positive (semi-) definite matrix has a unique positive (semi-) definite square root. What are its other square roots? In some cases there are infinitely many (such as for $aI$). Are there cases where there are finitely many? In [this][1] question it is shown that the identity matrix has infinitely many square roots (although of course it has a unique positive square root). But it doesn't seem to address the general question of the square roots of a positive operator. The behavior of the class of matrices $$\left( \begin{matrix} \sqrt{a} & t \\ 0 & -\sqrt{a} \end{matrix} \right)$$ as square roots of $aI$ seems to be due to the ""merging"" of the $-\sqrt{a}$ and $\sqrt{a}$ eigenspaces when you square. This might suggest that for a positive matrix with distinct eigenvalues, there are only $2^r$ square roots, with $r$ the rank of the matrix? Ideas: One thing that might narrow it down: if a matrix is normal, will all its square roots be normal, or not necessarily? If we know this, we can go straight to working with eigenvalues. [1]: In a finite dimensional vector space a positive operator can have infinitely many square roots [2]: https://math.stackexchange.com/questions/558072/how-many-square-roots-not-necessarily-positive-does-a-positive-matrix-have",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
7,Determinant of $2 \times 2$ block matrix with commuting blocks,Determinant of  block matrix with commuting blocks,2 \times 2,"Let $A,B,C$ and $D$ be $n \times n$ matrices such that $AC = CA$ . Prove that $$\det \begin{pmatrix} A & B\\ C & D\end{pmatrix} = \det(AD-CB)$$ The solution is to first assume that $A$ is invertible and then consider the product $$\begin{pmatrix} I & O\\ -CA^{-1} & I \end{pmatrix}\begin{pmatrix} A & B\\ C & D \end{pmatrix}=\begin{pmatrix} A & B\\ O & D-CA^{-1}B \end{pmatrix}$$ then it is not hard to prove that the claim is true if $A$ in invertible. Finally, we use the fact that the set $GL_n$ form a dense open subset of $M_n$ to get rid of the invertibility assumption. My question is: how to come up with such a weird matrix $\begin{pmatrix} I & O\\ -CA^{-1} & I \end{pmatrix}$ ? thank you so much. Is there any other problems that uses the technique of assuming invertibility? (one of which i know is to prove $\det (I+AB) = \det(I+BA)$ ), thanks in advance","Let and be matrices such that . Prove that The solution is to first assume that is invertible and then consider the product then it is not hard to prove that the claim is true if in invertible. Finally, we use the fact that the set form a dense open subset of to get rid of the invertibility assumption. My question is: how to come up with such a weird matrix ? thank you so much. Is there any other problems that uses the technique of assuming invertibility? (one of which i know is to prove ), thanks in advance","A,B,C D n \times n AC = CA \det \begin{pmatrix} A & B\\ C & D\end{pmatrix} = \det(AD-CB) A \begin{pmatrix}
I & O\\
-CA^{-1} & I
\end{pmatrix}\begin{pmatrix}
A & B\\
C & D
\end{pmatrix}=\begin{pmatrix}
A & B\\
O & D-CA^{-1}B
\end{pmatrix} A GL_n M_n \begin{pmatrix}
I & O\\
-CA^{-1} & I
\end{pmatrix} \det (I+AB) = \det(I+BA)","['linear-algebra', 'matrices', 'contest-math', 'determinant', 'block-matrices']"
8,Geometric interpretation of $\mathbb R^n$,Geometric interpretation of,\mathbb R^n,"Motivated by this question Basis of a basis I've been thinking when we say that a vector can have a geometrical interpretation, are we talking about the vectors themselves or the vectors coordinates in the usual basis $\{(1,0),(0,1)\}$ in for example $\mathbb R^2$?(see picture below to $\mathbb R^2$ case) I think vectors of $\mathbb R^n$ are just n-tuples of real numbers and then don't have a geometrical interpretation, am I right? I'm asking this silly question, because there are a lot of abuse of notation in this part of linear algebra and I want to know the correct concepts in order to not get lost in the future. Thanks in advance","Motivated by this question Basis of a basis I've been thinking when we say that a vector can have a geometrical interpretation, are we talking about the vectors themselves or the vectors coordinates in the usual basis $\{(1,0),(0,1)\}$ in for example $\mathbb R^2$?(see picture below to $\mathbb R^2$ case) I think vectors of $\mathbb R^n$ are just n-tuples of real numbers and then don't have a geometrical interpretation, am I right? I'm asking this silly question, because there are a lot of abuse of notation in this part of linear algebra and I want to know the correct concepts in order to not get lost in the future. Thanks in advance",,['linear-algebra']
9,"Proof: $\det\pmatrix{\langle v_i , v_j \rangle}\neq0$ $\iff \{v_1,\dots,v_n\}~\text{l.i.}$",Proof:,"\det\pmatrix{\langle v_i , v_j \rangle}\neq0 \iff \{v_1,\dots,v_n\}~\text{l.i.}","Let $V$ be a real inner product space and $S=\{v_1,v_2, \dots, v_n\}\subset V$. How am I to prove that $S$ is linearly independent if and only if the determinant of the matrix $$ a_{ij}=\pmatrix{\langle v_i , v_j \rangle}$$ is nonzero? Just to be clear, the matrix we're talking about is this one: $$\pmatrix{\langle v_1,v_1\rangle & \langle v_1,v_2\rangle &\langle v_1,v_3\rangle & \cdots & \langle v_1,v_{n-1}\rangle & \langle v_1, v_n\rangle \\\langle v_2,v_1\rangle & \langle v_2,v_2\rangle &\langle v_2,v_3\rangle & \cdots & \langle v_2,v_{n-1} \rangle & \langle v_2,v_n\rangle \\\langle v_3,v_1\rangle & \langle v_3,v_2\rangle &\langle v_3,v_3\rangle & \cdots & \langle v_3,v_{n-1}\rangle & \langle v_3,v_n \rangle \\ \cdots&\cdots&\cdots&\cdots&\cdots&\cdots\\\langle v_{n-1}, v_1\rangle & \langle v_{n-1},v_2\rangle &\langle v_{n-1},v_3\rangle & \cdots & \langle v_{n-1},v_{n-1}\rangle & \langle v_{n-1},v_n\rangle \\\langle v_n,v_1\rangle & \langle v_n,v_2\rangle &\langle v_n,v_3\rangle & \cdots & \langle v_n,v_{n-1}\rangle & \langle v_n,v_n\rangle \\ }$$ I highly doubt that anybody here has Roman's Advanced Linear Algebra , or maybe you do, but I think on page $261$ there is a small note on something which looks similar. Should anybody need the code (C++) in their research, here is a gadget which streams LaTeX code to a file named ""matrix.txt"" for an $n \times n$ matrix such as this with some value for $n$: ofstream fout; fout.open(""matrix.txt"");  int n; cout << ""Enter your desired n: ""; cin >> n;  fout << endl;  fout << ""$\\begin{pmatrix}"" << endl;  for( int j = 1 ; j <= n ; j++ ) {     for( int i = 1 ; i<= n ; i++ )     {         if( j == i && i == n )         {             fout << ""\\langle v_"" << j << "","" << ""v_"" << i << "" \\rangle"" << endl;         }         else         {             if( i == n )             {                 fout << ""\\langle v_"" << j << "","" << ""v_"" << i << "" \\rangle\\\\"" << endl;             }             else             {                 fout << ""\\langle v_"" << j << "","" << ""v_"" << i << "" \\rangle&"" << endl;             }         }     }     //fout << endl; }  fout << ""\\end{pmatrix}$"" << endl << endl; For n equal to 5 you get this result: $\begin{pmatrix} \langle v_1,v_1 \rangle& \langle v_1,v_2 \rangle& \langle v_1,v_3 \rangle& \langle v_1,v_4 \rangle& \langle v_1,v_5 \rangle\\ \langle v_2,v_1 \rangle& \langle v_2,v_2 \rangle& \langle v_2,v_3 \rangle& \langle v_2,v_4 \rangle& \langle v_2,v_5 \rangle\\ \langle v_3,v_1 \rangle& \langle v_3,v_2 \rangle& \langle v_3,v_3 \rangle& \langle v_3,v_4 \rangle& \langle v_3,v_5 \rangle\\ \langle v_4,v_1 \rangle& \langle v_4,v_2 \rangle& \langle v_4,v_3 \rangle& \langle v_4,v_4 \rangle& \langle v_4,v_5 \rangle\\ \langle v_5,v_1 \rangle& \langle v_5,v_2 \rangle& \langle v_5,v_3 \rangle& \langle v_5,v_4 \rangle& \langle v_5,v_5 \rangle \end{pmatrix}$ As is lengthily explained here .","Let $V$ be a real inner product space and $S=\{v_1,v_2, \dots, v_n\}\subset V$. How am I to prove that $S$ is linearly independent if and only if the determinant of the matrix $$ a_{ij}=\pmatrix{\langle v_i , v_j \rangle}$$ is nonzero? Just to be clear, the matrix we're talking about is this one: $$\pmatrix{\langle v_1,v_1\rangle & \langle v_1,v_2\rangle &\langle v_1,v_3\rangle & \cdots & \langle v_1,v_{n-1}\rangle & \langle v_1, v_n\rangle \\\langle v_2,v_1\rangle & \langle v_2,v_2\rangle &\langle v_2,v_3\rangle & \cdots & \langle v_2,v_{n-1} \rangle & \langle v_2,v_n\rangle \\\langle v_3,v_1\rangle & \langle v_3,v_2\rangle &\langle v_3,v_3\rangle & \cdots & \langle v_3,v_{n-1}\rangle & \langle v_3,v_n \rangle \\ \cdots&\cdots&\cdots&\cdots&\cdots&\cdots\\\langle v_{n-1}, v_1\rangle & \langle v_{n-1},v_2\rangle &\langle v_{n-1},v_3\rangle & \cdots & \langle v_{n-1},v_{n-1}\rangle & \langle v_{n-1},v_n\rangle \\\langle v_n,v_1\rangle & \langle v_n,v_2\rangle &\langle v_n,v_3\rangle & \cdots & \langle v_n,v_{n-1}\rangle & \langle v_n,v_n\rangle \\ }$$ I highly doubt that anybody here has Roman's Advanced Linear Algebra , or maybe you do, but I think on page $261$ there is a small note on something which looks similar. Should anybody need the code (C++) in their research, here is a gadget which streams LaTeX code to a file named ""matrix.txt"" for an $n \times n$ matrix such as this with some value for $n$: ofstream fout; fout.open(""matrix.txt"");  int n; cout << ""Enter your desired n: ""; cin >> n;  fout << endl;  fout << ""$\\begin{pmatrix}"" << endl;  for( int j = 1 ; j <= n ; j++ ) {     for( int i = 1 ; i<= n ; i++ )     {         if( j == i && i == n )         {             fout << ""\\langle v_"" << j << "","" << ""v_"" << i << "" \\rangle"" << endl;         }         else         {             if( i == n )             {                 fout << ""\\langle v_"" << j << "","" << ""v_"" << i << "" \\rangle\\\\"" << endl;             }             else             {                 fout << ""\\langle v_"" << j << "","" << ""v_"" << i << "" \\rangle&"" << endl;             }         }     }     //fout << endl; }  fout << ""\\end{pmatrix}$"" << endl << endl; For n equal to 5 you get this result: $\begin{pmatrix} \langle v_1,v_1 \rangle& \langle v_1,v_2 \rangle& \langle v_1,v_3 \rangle& \langle v_1,v_4 \rangle& \langle v_1,v_5 \rangle\\ \langle v_2,v_1 \rangle& \langle v_2,v_2 \rangle& \langle v_2,v_3 \rangle& \langle v_2,v_4 \rangle& \langle v_2,v_5 \rangle\\ \langle v_3,v_1 \rangle& \langle v_3,v_2 \rangle& \langle v_3,v_3 \rangle& \langle v_3,v_4 \rangle& \langle v_3,v_5 \rangle\\ \langle v_4,v_1 \rangle& \langle v_4,v_2 \rangle& \langle v_4,v_3 \rangle& \langle v_4,v_4 \rangle& \langle v_4,v_5 \rangle\\ \langle v_5,v_1 \rangle& \langle v_5,v_2 \rangle& \langle v_5,v_3 \rangle& \langle v_5,v_4 \rangle& \langle v_5,v_5 \rangle \end{pmatrix}$ As is lengthily explained here .",,"['linear-algebra', 'abstract-algebra', 'vector-spaces', 'determinant', 'inner-products']"
10,What does positive definite matrix mean?,What does positive definite matrix mean?,,What do we mean by a matrix is positive or negative definite? Does it have any analogy with a positive real number?,What do we mean by a matrix is positive or negative definite? Does it have any analogy with a positive real number?,,"['linear-algebra', 'matrices']"
11,If $A$ is a matrix and $p$ is a polynomial such that $p(A)=0$ then must the roots of $p$ necessarily be eigenvalues of $A$?,If  is a matrix and  is a polynomial such that  then must the roots of  necessarily be eigenvalues of ?,A p p(A)=0 p A,"This is just a small query: If $A$ is an $n\times n$ square matrix and $p(t)=(t-\lambda_1)(t-\lambda_2)\cdots(t-\lambda_m)$ be a polynomial (with $\lambda_i \in \mathbb{C}$ for all $i=1, \ldots, m$) such that $p(A)=0$, then is it necessary that $\lambda_1,\ldots,\lambda_m$ will be eigenvalues of $A?$ Now I know that if $p(A)=0$ then the minimal polynomial $m_A$ divides $p$; and as $m_A$ is a non-zero polynomial there should be at least one $\lambda_i$ which will be an eigenvector of $A$. But apart from that, I have no idea what to do for this question. The following question is something that I thought I might add with the first one in order to straighten up my understandings in this area. If not, what extra condition can be imposed on the polynomial and/or on the matrix to make sure that the $\lambda_i's$ are necessarily eigenvalues of $A$? Thanks and regards.","This is just a small query: If $A$ is an $n\times n$ square matrix and $p(t)=(t-\lambda_1)(t-\lambda_2)\cdots(t-\lambda_m)$ be a polynomial (with $\lambda_i \in \mathbb{C}$ for all $i=1, \ldots, m$) such that $p(A)=0$, then is it necessary that $\lambda_1,\ldots,\lambda_m$ will be eigenvalues of $A?$ Now I know that if $p(A)=0$ then the minimal polynomial $m_A$ divides $p$; and as $m_A$ is a non-zero polynomial there should be at least one $\lambda_i$ which will be an eigenvector of $A$. But apart from that, I have no idea what to do for this question. The following question is something that I thought I might add with the first one in order to straighten up my understandings in this area. If not, what extra condition can be imposed on the polynomial and/or on the matrix to make sure that the $\lambda_i's$ are necessarily eigenvalues of $A$? Thanks and regards.",,['linear-algebra']
12,Calculate $\begin{Vmatrix}1&2\\2&4\end{Vmatrix}$,Calculate,\begin{Vmatrix}1&2\\2&4\end{Vmatrix},"With $$\left\Vert A \right\Vert=\max_{\mathbf{x}\ne 0}\frac{\left\Vert A\mathbf{x}\right\Vert }{\left\Vert \mathbf{x}\right\Vert }$$ and $$A=\begin{bmatrix}1 & 2\\ 2 & 4 \end{bmatrix}  $$ Calculate $\Vert A \Vert$. Not exactly homework, but an exercise from some notes on applied linear algebra that I am trying to work though. I am given the following hint: Hint: note that $\begin{bmatrix}1 & 2\\ 2 & 4 \end{bmatrix}=\begin{bmatrix}1\\ 2 \end{bmatrix}\begin{bmatrix}1 & 2\end{bmatrix}  $ I've used the hint to obtain an upper bound on $\frac{\left\Vert A\mathbf{x}\right\Vert }{\left\Vert \mathbf{x}\right\Vert }$:$$\frac{\left\Vert A\mathbf{x}\right\Vert }{\left\Vert \mathbf{x}\right\Vert }	=	\frac{\left\Vert \begin{bmatrix}1 & 2\\ 2 & 4 \end{bmatrix}\begin{bmatrix}x_{1}\\ x_{2} \end{bmatrix}\right\Vert }{\left\Vert \begin{bmatrix}x_{1}\\ x_{2} \end{bmatrix}\right\Vert } 	=	\frac{\left\Vert \begin{bmatrix}1\\ 2 \end{bmatrix}\begin{bmatrix}1 & 2\end{bmatrix}\begin{bmatrix}x_{1}\\ x_{2} \end{bmatrix}\right\Vert }{\left\Vert \begin{bmatrix}x_{1}\\ x_{2} \end{bmatrix}\right\Vert } 	=	\frac{\left(\begin{bmatrix}1\\ 2 \end{bmatrix}\cdot\begin{bmatrix}x_{1}\\ x_{2} \end{bmatrix}\right)\left\Vert \begin{bmatrix}1\\ 2 \end{bmatrix}\right\Vert }{\left\Vert \begin{bmatrix}x_{1}\\ x_{2} \end{bmatrix}\right\Vert } 	\leq	\frac{\left\Vert \begin{bmatrix}1\\ 2 \end{bmatrix}\right\Vert \left\Vert \begin{bmatrix}x_{1}\\ x_{2} \end{bmatrix}\right\Vert \left\Vert \begin{bmatrix}1\\ 2 \end{bmatrix}\right\Vert }{\left\Vert \begin{bmatrix}x_{1}\\ x_{2} \end{bmatrix}\right\Vert } 	=	\left\Vert \begin{bmatrix}1\\ 2 \end{bmatrix}\right\Vert ^{2} 	=	5  $$ I believe that I am headed in the right direction here but I'm rather stuck with where to go next. I suppose I need to solve $$\frac{\left\Vert A\mathbf{x}\right\Vert }{\left\Vert \mathbf{x}\right\Vert }=5$$but after some struggle, I can't figure out how to solve this equation. I tried following the through component-wise to get $$|x_1 + 2x_2|^2 + |2x_1+4x_2|^2=25[|x_1|^2+|x_2|^2]$$ but that seems no closer to what I need. Does anyone see where I should go from here?","With $$\left\Vert A \right\Vert=\max_{\mathbf{x}\ne 0}\frac{\left\Vert A\mathbf{x}\right\Vert }{\left\Vert \mathbf{x}\right\Vert }$$ and $$A=\begin{bmatrix}1 & 2\\ 2 & 4 \end{bmatrix}  $$ Calculate $\Vert A \Vert$. Not exactly homework, but an exercise from some notes on applied linear algebra that I am trying to work though. I am given the following hint: Hint: note that $\begin{bmatrix}1 & 2\\ 2 & 4 \end{bmatrix}=\begin{bmatrix}1\\ 2 \end{bmatrix}\begin{bmatrix}1 & 2\end{bmatrix}  $ I've used the hint to obtain an upper bound on $\frac{\left\Vert A\mathbf{x}\right\Vert }{\left\Vert \mathbf{x}\right\Vert }$:$$\frac{\left\Vert A\mathbf{x}\right\Vert }{\left\Vert \mathbf{x}\right\Vert }	=	\frac{\left\Vert \begin{bmatrix}1 & 2\\ 2 & 4 \end{bmatrix}\begin{bmatrix}x_{1}\\ x_{2} \end{bmatrix}\right\Vert }{\left\Vert \begin{bmatrix}x_{1}\\ x_{2} \end{bmatrix}\right\Vert } 	=	\frac{\left\Vert \begin{bmatrix}1\\ 2 \end{bmatrix}\begin{bmatrix}1 & 2\end{bmatrix}\begin{bmatrix}x_{1}\\ x_{2} \end{bmatrix}\right\Vert }{\left\Vert \begin{bmatrix}x_{1}\\ x_{2} \end{bmatrix}\right\Vert } 	=	\frac{\left(\begin{bmatrix}1\\ 2 \end{bmatrix}\cdot\begin{bmatrix}x_{1}\\ x_{2} \end{bmatrix}\right)\left\Vert \begin{bmatrix}1\\ 2 \end{bmatrix}\right\Vert }{\left\Vert \begin{bmatrix}x_{1}\\ x_{2} \end{bmatrix}\right\Vert } 	\leq	\frac{\left\Vert \begin{bmatrix}1\\ 2 \end{bmatrix}\right\Vert \left\Vert \begin{bmatrix}x_{1}\\ x_{2} \end{bmatrix}\right\Vert \left\Vert \begin{bmatrix}1\\ 2 \end{bmatrix}\right\Vert }{\left\Vert \begin{bmatrix}x_{1}\\ x_{2} \end{bmatrix}\right\Vert } 	=	\left\Vert \begin{bmatrix}1\\ 2 \end{bmatrix}\right\Vert ^{2} 	=	5  $$ I believe that I am headed in the right direction here but I'm rather stuck with where to go next. I suppose I need to solve $$\frac{\left\Vert A\mathbf{x}\right\Vert }{\left\Vert \mathbf{x}\right\Vert }=5$$but after some struggle, I can't figure out how to solve this equation. I tried following the through component-wise to get $$|x_1 + 2x_2|^2 + |2x_1+4x_2|^2=25[|x_1|^2+|x_2|^2]$$ but that seems no closer to what I need. Does anyone see where I should go from here?",,"['linear-algebra', 'matrices', 'normed-spaces']"
13,QR factorization of complex matrix,QR factorization of complex matrix,,"If you have two complex numbers $a,b$ how can you find the QR factorization of $ M = \begin{bmatrix}        aI_n\\          bI_n \end{bmatrix} $, I can't seem to be able to do it. I tried an earlier trick, but I now know it's not necessary true for complex numbers. Is there anyway to do this? I think if I use a Givens algorithm I should be able to reduce it to a QR factorization.","If you have two complex numbers $a,b$ how can you find the QR factorization of $ M = \begin{bmatrix}        aI_n\\          bI_n \end{bmatrix} $, I can't seem to be able to do it. I tried an earlier trick, but I now know it's not necessary true for complex numbers. Is there anyway to do this? I think if I use a Givens algorithm I should be able to reduce it to a QR factorization.",,['linear-algebra']
14,The first column of the $n$th power for a triangular matrix,The first column of the th power for a triangular matrix,n,"I have found a interesting thing but I cannot prove it. Given $k_i$ are positive for any $i\geq1$, and we have $M+1$ by $M+1$ matrix $A$, which is $$ A=\left[\begin{array}{ccccc} 0\\ k_{1} & 0\\ k_{2} & \frac{1}{2}k_{1} & 0\\ \vdots &  &  & \ddots\\ k_{M} & \frac{M-1}{M}k_{M-1} & \cdots & \frac{1}{M}k_{1} & 0 \end{array}\right]  $$ I found that the first column of $n!A^{n}$ is always equal with the first column of $B^{n}$ for any $n\in\mathbb{N}$, where $$ B=\left[\begin{array}{ccccc} 0\\ k_{1} & 0\\ k_{2} & k_{1} & 0\\ \vdots &  &  & \ddots\\ k_{M} & k_{M-1} & \cdots & k_{1} & 0 \end{array}\right]  $$ Can you help me to prove it? Thanks in advance. The following is a reply to a comment from @GerryMyerson. (Updated on Aug. 27.) To @GerryMyerson. Here is the problem when I use the induction: Use induction method. First, it is obvious that when $n=0$  and $1$, the equality is hold for any $M\in\mathbb{N}$. Then, we assume when $n=N>1$, the equality is hold, and the matrix is $$ N!A_{M+1}^{N}=B_{M+1}^{N}=\left[\begin{array}{ccccccc} 0\\ \vdots\\ 0\\ t_{1} & 0\\ t_{2} & s & 0\\ \vdots & \vdots &  & \ddots\\ t_{M+1-N} & u & \cdots & v & 0 & \cdots & 0 \end{array}\right].  $$ Here, we only focus on the first column. When $n=N+1$, the left side is $$ \left(N+1\right)A_{M+1}\times\left(N!A_{M+1}^{N}\right)=\left(N+1\right)\left[\begin{array}{ccccccc} 0\\ k_{1} & 0\\ k_{2} & \frac{1}{2}k_{1} & 0\\ \vdots &  &  & \ddots\\ k_{i} & \frac{i-1}{i}k_{i-1} & \cdots & \frac{1}{i}k_{1} & 0\\ \vdots &  &  &  &  & \ddots\\ k_{M} &  &  &  &  & \frac{1}{M}k_{1} & 0 \end{array}\right]\left[\begin{array}{ccccccc} 0\\ \vdots\\ 0\\ t_{1} & 0\\ t_{2} & s & 0\\ \vdots & \vdots &  & \ddots\\ t_{M+1-N} & u & \cdots & v & 0 & \cdots & 0 \end{array}\right],  $$ then the $\left(i+1,1\right)$  entry is $\left(N+1\right)\left(\frac{i-N}{i}k_{i-n}t_{1}+\frac{i-N-1}{i}k_{i-N-1}t_{2}+\cdots+\frac{1}{i}k_{1}t_{i-N}\right)$. On the other side, the $\left(i+1,1\right)$ of $B$ entry is $k_{i-N}t_{1}+k_{i-N-1}t_{2}+\cdots k_{1}t_{i-N}$. Now, I cannot prove that these two are equal.","I have found a interesting thing but I cannot prove it. Given $k_i$ are positive for any $i\geq1$, and we have $M+1$ by $M+1$ matrix $A$, which is $$ A=\left[\begin{array}{ccccc} 0\\ k_{1} & 0\\ k_{2} & \frac{1}{2}k_{1} & 0\\ \vdots &  &  & \ddots\\ k_{M} & \frac{M-1}{M}k_{M-1} & \cdots & \frac{1}{M}k_{1} & 0 \end{array}\right]  $$ I found that the first column of $n!A^{n}$ is always equal with the first column of $B^{n}$ for any $n\in\mathbb{N}$, where $$ B=\left[\begin{array}{ccccc} 0\\ k_{1} & 0\\ k_{2} & k_{1} & 0\\ \vdots &  &  & \ddots\\ k_{M} & k_{M-1} & \cdots & k_{1} & 0 \end{array}\right]  $$ Can you help me to prove it? Thanks in advance. The following is a reply to a comment from @GerryMyerson. (Updated on Aug. 27.) To @GerryMyerson. Here is the problem when I use the induction: Use induction method. First, it is obvious that when $n=0$  and $1$, the equality is hold for any $M\in\mathbb{N}$. Then, we assume when $n=N>1$, the equality is hold, and the matrix is $$ N!A_{M+1}^{N}=B_{M+1}^{N}=\left[\begin{array}{ccccccc} 0\\ \vdots\\ 0\\ t_{1} & 0\\ t_{2} & s & 0\\ \vdots & \vdots &  & \ddots\\ t_{M+1-N} & u & \cdots & v & 0 & \cdots & 0 \end{array}\right].  $$ Here, we only focus on the first column. When $n=N+1$, the left side is $$ \left(N+1\right)A_{M+1}\times\left(N!A_{M+1}^{N}\right)=\left(N+1\right)\left[\begin{array}{ccccccc} 0\\ k_{1} & 0\\ k_{2} & \frac{1}{2}k_{1} & 0\\ \vdots &  &  & \ddots\\ k_{i} & \frac{i-1}{i}k_{i-1} & \cdots & \frac{1}{i}k_{1} & 0\\ \vdots &  &  &  &  & \ddots\\ k_{M} &  &  &  &  & \frac{1}{M}k_{1} & 0 \end{array}\right]\left[\begin{array}{ccccccc} 0\\ \vdots\\ 0\\ t_{1} & 0\\ t_{2} & s & 0\\ \vdots & \vdots &  & \ddots\\ t_{M+1-N} & u & \cdots & v & 0 & \cdots & 0 \end{array}\right],  $$ then the $\left(i+1,1\right)$  entry is $\left(N+1\right)\left(\frac{i-N}{i}k_{i-n}t_{1}+\frac{i-N-1}{i}k_{i-N-1}t_{2}+\cdots+\frac{1}{i}k_{1}t_{i-N}\right)$. On the other side, the $\left(i+1,1\right)$ of $B$ entry is $k_{i-N}t_{1}+k_{i-N-1}t_{2}+\cdots k_{1}t_{i-N}$. Now, I cannot prove that these two are equal.",,"['linear-algebra', 'combinatorics', 'matrices', 'factorial']"
15,Simultaneously Diagonalizing Bilinear Forms,Simultaneously Diagonalizing Bilinear Forms,,"Let $\theta$ and $\psi$ be symmetric bilinear forms on a   finite-dimensional real vector space $V$, and assume $\theta$ is positive   definite. Show that there exists a basis $\{v_1,\ldots,v_n\}$ for $V$   and $\lambda_1,\ldots,\lambda_n\in\mathbb{R}$ such that   $$\theta(v_i,v_j)=\delta_{i,j}\quad\text{and}\quad\psi(v_i,v_j)=\delta_{ij}\lambda_i$$   where $\delta_{ij}$ is the Kronecker delta function. I think it's enough to choose a basis $\{w_1,\ldots,w_n\}$ for which the matrix representations of $\theta$ and $\psi$ are both diagonal. Then $\left\{\frac{w_1}{\sqrt{\theta(w_1,w_1)}},\ldots,\frac{w_n}{\sqrt{\theta(w_n,w_n)}}\right\}$ is the required basis.","Let $\theta$ and $\psi$ be symmetric bilinear forms on a   finite-dimensional real vector space $V$, and assume $\theta$ is positive   definite. Show that there exists a basis $\{v_1,\ldots,v_n\}$ for $V$   and $\lambda_1,\ldots,\lambda_n\in\mathbb{R}$ such that   $$\theta(v_i,v_j)=\delta_{i,j}\quad\text{and}\quad\psi(v_i,v_j)=\delta_{ij}\lambda_i$$   where $\delta_{ij}$ is the Kronecker delta function. I think it's enough to choose a basis $\{w_1,\ldots,w_n\}$ for which the matrix representations of $\theta$ and $\psi$ are both diagonal. Then $\left\{\frac{w_1}{\sqrt{\theta(w_1,w_1)}},\ldots,\frac{w_n}{\sqrt{\theta(w_n,w_n)}}\right\}$ is the required basis.",,"['linear-algebra', 'bilinear-form']"
16,Proving that a right (or left) inverse of a square matrix is unique using only basic matrix operations,Proving that a right (or left) inverse of a square matrix is unique using only basic matrix operations,,"Proving that a right (or left) inverse of a square matrix is unique using only basic matrix operations -- i.e. without any reference to higher-order matters like rank, vector spaces or whatever ( :)). More precisely, armed with the knowledge only of: rules of matrix equality check, addition, multiplication, distributive law and friends Gauss-Jordan elimination and appropriate equation system solution cases for the reduced row-echelon form Thanks in advance.","Proving that a right (or left) inverse of a square matrix is unique using only basic matrix operations -- i.e. without any reference to higher-order matters like rank, vector spaces or whatever ( :)). More precisely, armed with the knowledge only of: rules of matrix equality check, addition, multiplication, distributive law and friends Gauss-Jordan elimination and appropriate equation system solution cases for the reduced row-echelon form Thanks in advance.",,"['linear-algebra', 'matrices', 'inverse']"
17,Does this set of vectors span the space of all polynomials of degree at most 3?,Does this set of vectors span the space of all polynomials of degree at most 3?,,"I'm having trouble understanding this homework problem. Suppose four polynomials are defined by the following: $ p_{1}(x) = x^3 - 2x^2 + x + 1 \\ p_{2}(x) = x^2 - x + 2 \\ p_{3}(x) = 2x^3 + 3x + 4 \\ p_{4}(x) = 3x^2 + 2x + 1 \\ $ Does the set $S = $ { ${p_{1}, p_{2}, p_{3}, p_{4}}$ } span $P_{3}$ (the space of all polynomials of degree at most 3)? So if I start with the polynomial $y = ax^3 + bx^2 + cx + d$ I understand(I think) that in order for $S$ to span $P_{3}$ then $y$ must be a linear combination of $S$ but I'm not sure where to go from there. EDIT: $$ A =  \begin{bmatrix} 1 & -2 & 1 & 1\\  0 & 1 & -1 & 2\\  2 & 0 & 3 & 4\\  0 & 3 & 2 & 1 \end{bmatrix}  \sim  \begin{bmatrix} 1 & 0 & 0 & 0\\  0 & 1 & 0 & 0\\  0 & 0 & 1 & 0\\  0 & 0 & 0 & 1 \end{bmatrix} $$ So $\operatorname{rank}(A) = 4$ which means the vectors in the set are linearly independent because there are only 4 column vectors in the matrix (AND there is only the trivial solution to the matrix) and therefore we span $P_{3}$?","I'm having trouble understanding this homework problem. Suppose four polynomials are defined by the following: $ p_{1}(x) = x^3 - 2x^2 + x + 1 \\ p_{2}(x) = x^2 - x + 2 \\ p_{3}(x) = 2x^3 + 3x + 4 \\ p_{4}(x) = 3x^2 + 2x + 1 \\ $ Does the set $S = $ { ${p_{1}, p_{2}, p_{3}, p_{4}}$ } span $P_{3}$ (the space of all polynomials of degree at most 3)? So if I start with the polynomial $y = ax^3 + bx^2 + cx + d$ I understand(I think) that in order for $S$ to span $P_{3}$ then $y$ must be a linear combination of $S$ but I'm not sure where to go from there. EDIT: $$ A =  \begin{bmatrix} 1 & -2 & 1 & 1\\  0 & 1 & -1 & 2\\  2 & 0 & 3 & 4\\  0 & 3 & 2 & 1 \end{bmatrix}  \sim  \begin{bmatrix} 1 & 0 & 0 & 0\\  0 & 1 & 0 & 0\\  0 & 0 & 1 & 0\\  0 & 0 & 0 & 1 \end{bmatrix} $$ So $\operatorname{rank}(A) = 4$ which means the vectors in the set are linearly independent because there are only 4 column vectors in the matrix (AND there is only the trivial solution to the matrix) and therefore we span $P_{3}$?",,"['linear-algebra', 'matrices']"
18,"How to prove the cofactor formula for determinants, using a different definition of the determinant?","How to prove the cofactor formula for determinants, using a different definition of the determinant?",,"So, I am very interested on this theorem (Laplace expansion), but I am still a high school student. I have four books about matrices, but only one of them have proo and that doesn't start with my definition of determinant. Also I don't understand the proof of Laplace expansion in wikipedia, because there is too much symbol and terms I didn't learn and that proof dosn't start with my definition. So may someone give me the outline of the proof start with my definition? Here's my definition of the $n \times n$ determinant: The value of the determinant of a matrix of order $n$ is defined as the sum of $n!$ terms of the form $(-1)^k a_{1 i_1} a_{2 i_2} \cdots a_{n i_n}$. Each term contains one and only one element from each row and one and only element from each column; i.e., the second subscripts $i_1, i_2 , \ldots, i_n$ are equal to $1,2, \ldots, n$ taken in some order. The exponent $k$ represents the number of interchanges of two elements necessary for the second subscripts to be placed in the order $1,2, \ldots, n$. For example, consider the term containing $a_{13} a_{21} a_{34} a_{42}$ in the evaluation of the determinant of a matrix of order four. The value of $k$ is $3$ since three interchanges of two elements are necessary for the second subscripts to be placed in the order $(1,2,3,4)$.","So, I am very interested on this theorem (Laplace expansion), but I am still a high school student. I have four books about matrices, but only one of them have proo and that doesn't start with my definition of determinant. Also I don't understand the proof of Laplace expansion in wikipedia, because there is too much symbol and terms I didn't learn and that proof dosn't start with my definition. So may someone give me the outline of the proof start with my definition? Here's my definition of the $n \times n$ determinant: The value of the determinant of a matrix of order $n$ is defined as the sum of $n!$ terms of the form $(-1)^k a_{1 i_1} a_{2 i_2} \cdots a_{n i_n}$. Each term contains one and only one element from each row and one and only element from each column; i.e., the second subscripts $i_1, i_2 , \ldots, i_n$ are equal to $1,2, \ldots, n$ taken in some order. The exponent $k$ represents the number of interchanges of two elements necessary for the second subscripts to be placed in the order $1,2, \ldots, n$. For example, consider the term containing $a_{13} a_{21} a_{34} a_{42}$ in the evaluation of the determinant of a matrix of order four. The value of $k$ is $3$ since three interchanges of two elements are necessary for the second subscripts to be placed in the order $(1,2,3,4)$.",,"['linear-algebra', 'matrices']"
19,What is the meaning of superscript $\perp$ for a vector space,What is the meaning of superscript  for a vector space,\perp,"If $A$ is a matrix, then the nullspace of $A$, i.e. $null(A)$, is a vector subspace. Then, what is the meaning of superscript inverted $T$, for example $$null(A)^\perp$$ on a vector subspace?","If $A$ is a matrix, then the nullspace of $A$, i.e. $null(A)$, is a vector subspace. Then, what is the meaning of superscript inverted $T$, for example $$null(A)^\perp$$ on a vector subspace?",,"['linear-algebra', 'notation', 'vector-spaces']"
20,Powers of the Laplacian matrix,Powers of the Laplacian matrix,,"Given a graph with an adjacency matrix $\bf A$, powers of this matrix give the number of walks from vertices. That is, $({\bf A}^k)_{ij}$ gives the number of walks from nodes $i$ to $j$ in $k$ steps. Is there any nice physical iterpertation for the powers of the Laplacian matrix?","Given a graph with an adjacency matrix $\bf A$, powers of this matrix give the number of walks from vertices. That is, $({\bf A}^k)_{ij}$ gives the number of walks from nodes $i$ to $j$ in $k$ steps. Is there any nice physical iterpertation for the powers of the Laplacian matrix?",,"['linear-algebra', 'graph-theory']"
21,Is there a dimensional multiplication operation? [closed],Is there a dimensional multiplication operation? [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 2 months ago . Improve this question When expressing numbers with any unit, we know this. We can multiply and divide numbers with different types of units, but we cannot add or compare them. From Terry Tao's 2012 blog post ""A mathematical formalisation of dimensional analysis"" (via terrytao.wordpress.com) (paragraph 6): There is however one important limitation to the ability to manipulate “dimensionful” quantities as if they were numbers: one is not supposed to add, subtract, or compare two physical quantities if they have different dimensions, although it is acceptable to multiply or divide two such quantities. For instance, if ${m}$ is a mass (having the units ${M}$ ) and ${v}$ is a speed (having the units ${LT^{-1}}$ ), then it is physically “legitimate” to form an expression such as ${\frac{1}{2} mv^2}$ , but not an expression such as ${m+v}$ or ${m-v}$ ; in a similar spirit, statements such as ${m=v}$ or ${m\geq v}$ are physically meaningless. While expressing numbers with objects, if we assign units to objects, we can analyze the following operations. As you can see above, we cannot collect 1 apple and 1 pear. Now I want to mention this: When performing operations on integers, considering them as vectors in one dimension and performing operations works perfectly. If we consider integers in one dimension, multiplying two numbers that do not have any units is actually repetitive addition. Or multiplying a number with a unit in a dimension by a number without a unit is also repetitive addition. However, while studying geometry, I noticed something. It is meaningless to multiply numbers with different units in one dimension. Also from Tao's post (paragraph 10): As mentioned before, it is then geometrically natural to multiply two lengths to form an area, by taking a rectangle whose line segments have the stated lengths, and using the area of that rectangle as a product. This geometric picture works well for units such as length and volume that have a spatial geometric interpretation, but it is less clear how to apply it for more general units. For instance, it does not seem geometrically natural (or, for that matter, conceptually helpful) to envision the equation ${E=mc^2}$ as the assertion that the energy ${E}$ is the volume of a rectangular box whose height is the mass ${m}$ and whose length and width is given by the speed of light ${c}$ . We can multiply or divide numbers that have a unit. However, multiplying $4m$ length by $4m$ length in one dimension is meaningless. But when calculating the area of ​​a square, we can multiply the length of $4m$ in two different dimensions $(4m\cdot 4m=16m^{2})$ . My question: We can approach numbers in many different ways. They can act as one-dimensional vectors in relation to objects (for example, apples, pears), or in the number direction. We can even manipulate units like numbers. So, does dimensional multiplication really exist ?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 2 months ago . Improve this question When expressing numbers with any unit, we know this. We can multiply and divide numbers with different types of units, but we cannot add or compare them. From Terry Tao's 2012 blog post ""A mathematical formalisation of dimensional analysis"" (via terrytao.wordpress.com) (paragraph 6): There is however one important limitation to the ability to manipulate “dimensionful” quantities as if they were numbers: one is not supposed to add, subtract, or compare two physical quantities if they have different dimensions, although it is acceptable to multiply or divide two such quantities. For instance, if is a mass (having the units ) and is a speed (having the units ), then it is physically “legitimate” to form an expression such as , but not an expression such as or ; in a similar spirit, statements such as or are physically meaningless. While expressing numbers with objects, if we assign units to objects, we can analyze the following operations. As you can see above, we cannot collect 1 apple and 1 pear. Now I want to mention this: When performing operations on integers, considering them as vectors in one dimension and performing operations works perfectly. If we consider integers in one dimension, multiplying two numbers that do not have any units is actually repetitive addition. Or multiplying a number with a unit in a dimension by a number without a unit is also repetitive addition. However, while studying geometry, I noticed something. It is meaningless to multiply numbers with different units in one dimension. Also from Tao's post (paragraph 10): As mentioned before, it is then geometrically natural to multiply two lengths to form an area, by taking a rectangle whose line segments have the stated lengths, and using the area of that rectangle as a product. This geometric picture works well for units such as length and volume that have a spatial geometric interpretation, but it is less clear how to apply it for more general units. For instance, it does not seem geometrically natural (or, for that matter, conceptually helpful) to envision the equation as the assertion that the energy is the volume of a rectangular box whose height is the mass and whose length and width is given by the speed of light . We can multiply or divide numbers that have a unit. However, multiplying length by length in one dimension is meaningless. But when calculating the area of ​​a square, we can multiply the length of in two different dimensions . My question: We can approach numbers in many different ways. They can act as one-dimensional vectors in relation to objects (for example, apples, pears), or in the number direction. We can even manipulate units like numbers. So, does dimensional multiplication really exist ?",{m} {M} {v} {LT^{-1}} {\frac{1}{2} mv^2} {m+v} {m-v} {m=v} {m\geq v} {E=mc^2} {E} {m} {c} 4m 4m 4m (4m\cdot 4m=16m^{2}),"['linear-algebra', 'geometry', 'real-numbers', 'integers']"
22,Find the $n$th power of a 3-by-3 circulant matrix,Find the th power of a 3-by-3 circulant matrix,n,"Consider the matrix given as $$A=\begin{bmatrix}a_0 & a_2 & a_1\\ a_1 & a_0 & a_2\\ a_2 & a_1 & a_0\end{bmatrix}$$ Write a formula for $A^n$ for $n\in\mathbb{N}$ . $$$$ My attempt: The first that comes to mind is to diagonalize it and hence find the formula for $A^n$ , but that is very messy, so I tried to do something else it goes as: bserve that $$A=\begin{bmatrix}a_0 & a_2 & a_1\\ a_1 & a_0 & a_2\\ a_2 & a_1 & a_0\end{bmatrix}=a_0\begin{bmatrix}1 & 0 & 0\\0 & 1 & 0\\0 & 0 & 1\end{bmatrix}+a_1\begin{bmatrix}0 & 0 & 1\\1 & 0 & 0\\0 & 1 & 0\end{bmatrix}+a_2\begin{bmatrix}0 & 1 & 0\\0 & 0 & 1\\1 & 0 & 0\end{bmatrix}$$ Let $U=\begin{bmatrix}0 & 0 & 1\\1 & 0 & 0\\0 & 1 & 0\end{bmatrix}$ then we will have that $$\begin{matrix}U^2=\begin{bmatrix}0 & 1 & 0\\0 & 0 & 1\\1 & 0 & 0\end{bmatrix} & \text{ and } & U^3=\begin{bmatrix}1 & 0 & 0\\0 & 1 & 0\\0 & 0 & 1\end{bmatrix}\end{matrix}$$ Hence we have $A=a_0I+a_1U+a_2U^2 = a_0U^3+a_1U+a_2U^2=(a_0U^2+a_1I+a_2U)U$ This got me thinking that there might be an easy way to solve the above problem, but I could not make any further progress. Please Help and thanks in advance.","Consider the matrix given as Write a formula for for . My attempt: The first that comes to mind is to diagonalize it and hence find the formula for , but that is very messy, so I tried to do something else it goes as: bserve that Let then we will have that Hence we have This got me thinking that there might be an easy way to solve the above problem, but I could not make any further progress. Please Help and thanks in advance.",A=\begin{bmatrix}a_0 & a_2 & a_1\\ a_1 & a_0 & a_2\\ a_2 & a_1 & a_0\end{bmatrix} A^n n\in\mathbb{N}  A^n A=\begin{bmatrix}a_0 & a_2 & a_1\\ a_1 & a_0 & a_2\\ a_2 & a_1 & a_0\end{bmatrix}=a_0\begin{bmatrix}1 & 0 & 0\\0 & 1 & 0\\0 & 0 & 1\end{bmatrix}+a_1\begin{bmatrix}0 & 0 & 1\\1 & 0 & 0\\0 & 1 & 0\end{bmatrix}+a_2\begin{bmatrix}0 & 1 & 0\\0 & 0 & 1\\1 & 0 & 0\end{bmatrix} U=\begin{bmatrix}0 & 0 & 1\\1 & 0 & 0\\0 & 1 & 0\end{bmatrix} \begin{matrix}U^2=\begin{bmatrix}0 & 1 & 0\\0 & 0 & 1\\1 & 0 & 0\end{bmatrix} & \text{ and } & U^3=\begin{bmatrix}1 & 0 & 0\\0 & 1 & 0\\0 & 0 & 1\end{bmatrix}\end{matrix} A=a_0I+a_1U+a_2U^2 = a_0U^3+a_1U+a_2U^2=(a_0U^2+a_1I+a_2U)U,"['linear-algebra', 'matrices', 'matrix-decomposition', 'diagonalization', 'circulant-matrices']"
23,Why is the trace of a matrix important?,Why is the trace of a matrix important?,,"Lower division linear algebra course at my university taught the simple computation steps to finding a trace of a matrix, but not the intuition nor the purpose for it. What information is gained from summing the diagonal entries? Why is the trace of a matrix important?","Lower division linear algebra course at my university taught the simple computation steps to finding a trace of a matrix, but not the intuition nor the purpose for it. What information is gained from summing the diagonal entries? Why is the trace of a matrix important?",,"['linear-algebra', 'matrices', 'trace']"
24,Geometric intuition for adjoint,Geometric intuition for adjoint,,"Let $V$ be a finite-dimensional inner product space, and let $T$ be a linear operator on $V$ . Then $T^*$ ( $T$ adjoint) is defined as the unique function such that $\langle T(x), y \rangle = \langle x, T^*(y) \rangle$ for all $x, y \in V$ . Furthermore, $T^*$ is linear. I know how to manipulate the adjoint algebraically, but I'm not sure how to interpret it geometrically. This has been asked before, but the previous questions did not suit my needs. Definition of adjoint operator (asking for intuition) I'm not asking about $T^*$ 's relation to $A$ 's conjugate transpose. Geometric intuition of adjoint I'm asking about intuition about $T^*$ , not $\text{Ker}(T^*)=(\text{Im}(T))^\perp$ . https://mathoverflow.net/q/6552 The answers to this question feel too complicated to me. https://mathoverflow.net/q/6573 I haven't yet learned about Hilbert spaces. Also, I don't know what $\langle \: |$ and $| \: \rangle$ are. https://mathoverflow.net/q/6567 biadjacency matrix?","Let be a finite-dimensional inner product space, and let be a linear operator on . Then ( adjoint) is defined as the unique function such that for all . Furthermore, is linear. I know how to manipulate the adjoint algebraically, but I'm not sure how to interpret it geometrically. This has been asked before, but the previous questions did not suit my needs. Definition of adjoint operator (asking for intuition) I'm not asking about 's relation to 's conjugate transpose. Geometric intuition of adjoint I'm asking about intuition about , not . https://mathoverflow.net/q/6552 The answers to this question feel too complicated to me. https://mathoverflow.net/q/6573 I haven't yet learned about Hilbert spaces. Also, I don't know what and are. https://mathoverflow.net/q/6567 biadjacency matrix?","V T V T^* T \langle T(x), y \rangle = \langle x, T^*(y) \rangle x, y \in V T^* T^* A T^* \text{Ker}(T^*)=(\text{Im}(T))^\perp \langle \: | | \: \rangle","['linear-algebra', 'adjoint-operators']"
25,Counting the number of similar matrices over finite fields,Counting the number of similar matrices over finite fields,,"Among $3 \times 3$ invertible matrices with entries from the field $ \mathbb{Z/3Z}$ , how many matrices are similar to the following matrix? \begin{pmatrix} 2 & 0 &0 \\   0&2  &0 \\   0&0  &1  \end{pmatrix} Things I'm familiar with:  I know similar matrices have the same determinant and trace. The number of invertible matrices over $ \mathbb{Z/3Z}$ is $(3^3-1)(3^3-3)(3^3-3^2)$ Please give me a hint to proceed from here.","Among invertible matrices with entries from the field , how many matrices are similar to the following matrix? Things I'm familiar with:  I know similar matrices have the same determinant and trace. The number of invertible matrices over is Please give me a hint to proceed from here.","3 \times 3  \mathbb{Z/3Z} \begin{pmatrix}
2 & 0 &0 \\ 
 0&2  &0 \\ 
 0&0  &1 
\end{pmatrix}  \mathbb{Z/3Z} (3^3-1)(3^3-3)(3^3-3^2)","['linear-algebra', 'matrices', 'finite-fields', 'similar-matrices']"
26,An extension of the determinant to non square matrices,An extension of the determinant to non square matrices,,"I'm an undergraduate student in mathematics and today I've been asked the following question by a friend of mine: let $v, w$ be vectors in $\mathbb{R}^3 $ and $ A=\left( \begin{array}{cc} v_1 \ v_2 \ v_3 \\ w_1 \ w_2 \ w_3 \end{array} \right)$ , she asked for a geometrical interpretation of the fact that $\det (AA^T)=$ area of the parallelogram enclosed between $v$ and $w$ squared. The algebra turns out to be correct (we proved it in this particular case), but I started wondering: is this true in general? Or, to put it in a better way: does it make sense to extend $\det$ to non square matrices as $\sqrt{\det(AA^T)}$ ? My reasoning is the following: we know that $\det(AA^T)=\det(A)^2 \ \forall A \in \mathbb{R}^{n\times n}$ . For now, let's just pretend that there exists a function $\operatorname{Area}: M(m, n, \mathbb{R}) \mapsto \mathbb{R}$ such that $Area(A) = \det(A)$ when $A$ is square and that behaves similarly to $\det$ (that is, is invariant under transposition, obeys Binet theorem ecc.). Thus, $\operatorname{Area}(M)^2=\operatorname{Area}(MM^T)=\det(MM^T) \  \forall M \in M(n, m, \mathbb{R})$ . So such a function must be identically equal to $\pm\sqrt{\det(M M^T)}$ (note that $\det(MM^T)$ is always positive, giving a hint that this extension is very likely to be sensible). Now comes the question: does any of this actually make sense? I.e.: Unfortunately the definition is not so straight forward, in the sense that $\operatorname{Area}(A^T)=\sqrt{\det(A^TA)}$ which is generally different from $\sqrt{\det(AA^T)}$ , so it looks like the definition might be flawed BUT I noticed (see edit) that one of the two is always zero, so $\operatorname{Area}$ could just be defined as the one that is not zero. Is it true that $\operatorname{Area}(A) \neq 0 \iff A$ has full rank? More specifically, does this actually still represent the area of the parallelogram (or hyperparallelogram in general) enclosed between the vectors $Ae_j$ where $\{e_j: 1\leq j \leq m\}$ is the standard basis of $\mathbb{R}^m$ ? If the previous point was true, how would one decide the sign of the result such that it still reflects the ""flipping"" somehow? Does ""flipping"" even make sense for a function $:\mathbb{R}^m \mapsto \mathbb{R}^n$ ? Any comment is appreciated! Edit: as was pointed out in the comments, if $m<n, A \in M(m, n, \mathbb{R})$ then $\det{A^TA}=0$ , because $A^TA \in M(n, n, \mathbb{R})$ and $rank(A^TA) \leq rank(A) \leq m < n$ which means that $rank(A) < n$ and therefore $A^TA$ is not invertible, so $\det(A^TA)=0$ .","I'm an undergraduate student in mathematics and today I've been asked the following question by a friend of mine: let be vectors in and , she asked for a geometrical interpretation of the fact that area of the parallelogram enclosed between and squared. The algebra turns out to be correct (we proved it in this particular case), but I started wondering: is this true in general? Or, to put it in a better way: does it make sense to extend to non square matrices as ? My reasoning is the following: we know that . For now, let's just pretend that there exists a function such that when is square and that behaves similarly to (that is, is invariant under transposition, obeys Binet theorem ecc.). Thus, . So such a function must be identically equal to (note that is always positive, giving a hint that this extension is very likely to be sensible). Now comes the question: does any of this actually make sense? I.e.: Unfortunately the definition is not so straight forward, in the sense that which is generally different from , so it looks like the definition might be flawed BUT I noticed (see edit) that one of the two is always zero, so could just be defined as the one that is not zero. Is it true that has full rank? More specifically, does this actually still represent the area of the parallelogram (or hyperparallelogram in general) enclosed between the vectors where is the standard basis of ? If the previous point was true, how would one decide the sign of the result such that it still reflects the ""flipping"" somehow? Does ""flipping"" even make sense for a function ? Any comment is appreciated! Edit: as was pointed out in the comments, if then , because and which means that and therefore is not invertible, so .","v, w \mathbb{R}^3   A=\left( \begin{array}{cc} v_1 \ v_2 \ v_3 \\ w_1 \ w_2 \ w_3 \end{array} \right) \det (AA^T)= v w \det \sqrt{\det(AA^T)} \det(AA^T)=\det(A)^2 \ \forall A \in \mathbb{R}^{n\times n} \operatorname{Area}: M(m, n, \mathbb{R}) \mapsto \mathbb{R} Area(A) = \det(A) A \det \operatorname{Area}(M)^2=\operatorname{Area}(MM^T)=\det(MM^T) \  \forall M \in M(n, m, \mathbb{R}) \pm\sqrt{\det(M M^T)} \det(MM^T) \operatorname{Area}(A^T)=\sqrt{\det(A^TA)} \sqrt{\det(AA^T)} \operatorname{Area} \operatorname{Area}(A) \neq 0 \iff A Ae_j \{e_j: 1\leq j \leq m\} \mathbb{R}^m :\mathbb{R}^m \mapsto \mathbb{R}^n m<n, A \in M(m, n, \mathbb{R}) \det{A^TA}=0 A^TA \in M(n, n, \mathbb{R}) rank(A^TA) \leq rank(A) \leq m < n rank(A) < n A^TA \det(A^TA)=0","['linear-algebra', 'matrices', 'euclidean-geometry', 'determinant', 'area']"
27,Classification of finite subgroups of $\textrm{GL}_2(\mathbb{C})$,Classification of finite subgroups of,\textrm{GL}_2(\mathbb{C}),"I was reading $\textit{A Report on Artin's Holomorphy Conjecture}$ by Dipendra Prasad and C. S. Yogananda. ( http://www.math.tifr.res.in/~dprasad/artin.pdf ) On p. 9, they state that the finite subgroups of $\textrm{GL}_2(\mathbb{C})$ can be classified according to their images in $\textrm{PGL}_2(\mathbb{C})$ , being one of the following: 1) Cyclic, 2) Dihedral, 3) Tetrahedral, 4) Octahedral, 5) Icosahedral. They attribute the classification of the finite subgroups of $\textrm{GL}_2(\mathbb{C})$ to Felix Klein, but they do not provide a reference. After extensive research, I have been unable to find a resource that treats the matter. This is of course what the progress made on the proof of Artin's conjecture in the case of two-dimensional representations hinges upon. If anyone could provide a reference, or briefly explain why the above list is an exhaustive classification of the finite subgroups of $\textrm{GL}_2(\mathbb{C})$ , I would be most grateful. $\textbf{Addendum}:$ I have still not solved the above and therefore add a bounty. I add that on p. 25 of $\textit{Base Change for}\ \textrm{GL}(2)$ by R.P. Langlands, he mentions in passing that $\textrm{PGL}(2,\mathbb{C}) \cong \textrm{SO}(3,\mathbb{C})$ , which is significant. But it still remains to show that the finite subgroups of $\textrm{SO}(3,\mathbb{C})$ fall into one of the five classes listed above, which is not obvious to me. The list is of course reminiscent of the five known platonic solids: The tetrahedron, the cube, the octahedron, the dodecahedron, and the icosahedron. But if there is a relationship, I fail to see it.","I was reading by Dipendra Prasad and C. S. Yogananda. ( http://www.math.tifr.res.in/~dprasad/artin.pdf ) On p. 9, they state that the finite subgroups of can be classified according to their images in , being one of the following: 1) Cyclic, 2) Dihedral, 3) Tetrahedral, 4) Octahedral, 5) Icosahedral. They attribute the classification of the finite subgroups of to Felix Klein, but they do not provide a reference. After extensive research, I have been unable to find a resource that treats the matter. This is of course what the progress made on the proof of Artin's conjecture in the case of two-dimensional representations hinges upon. If anyone could provide a reference, or briefly explain why the above list is an exhaustive classification of the finite subgroups of , I would be most grateful. I have still not solved the above and therefore add a bounty. I add that on p. 25 of by R.P. Langlands, he mentions in passing that , which is significant. But it still remains to show that the finite subgroups of fall into one of the five classes listed above, which is not obvious to me. The list is of course reminiscent of the five known platonic solids: The tetrahedron, the cube, the octahedron, the dodecahedron, and the icosahedron. But if there is a relationship, I fail to see it.","\textit{A Report on Artin's Holomorphy Conjecture} \textrm{GL}_2(\mathbb{C}) \textrm{PGL}_2(\mathbb{C}) \textrm{GL}_2(\mathbb{C}) \textrm{GL}_2(\mathbb{C}) \textbf{Addendum}: \textit{Base Change for}\ \textrm{GL}(2) \textrm{PGL}(2,\mathbb{C}) \cong \textrm{SO}(3,\mathbb{C}) \textrm{SO}(3,\mathbb{C})","['linear-algebra', 'group-theory', 'reference-request', 'representation-theory', 'general-linear-group']"
28,Help understanding the complex matrix representation of quaternions,Help understanding the complex matrix representation of quaternions,,"Using the basis $ B = \{1, j\}$ , one can show that quaternions can be represented by 2x2 complex matrices as follows: \begin{pmatrix} z & w\\ -\bar{w} & \bar{z}\\ \end{pmatrix} I would like some help to understand this. Lets say $z = a + bi, w = c + di$ Then we can represent the quaternion $h = a + bi + cj + dk$ as $z + wj$ . I would have thought that to find the matrix representation of complex numbers we would see what would happen if we multiply $z + wj$ by the basis elements. This would mean the first column of our matrix would be $(z + wj)(1) = z + wj =           \begin{bmatrix}            z \\            w \\          \end{bmatrix}$ And the second column would be $(z + wj)(j) = zj + wj^{2} = zj - w =          \begin{bmatrix}            -w \\            z \\          \end{bmatrix}$ So I would have thought the matrix representation would be \begin{pmatrix} z & -w\\ w & z\\ \end{pmatrix} I have a feeling I have some large misunderstanding about what I'm doing, I'm just following the same approach I did to find matrix representations of complex numbers with real 2x2 matrices and matrix representations of quaternions with real 4x4 matrices. e.g. with complex numbers, using a basis of $B = \{1, i\}$ , and a complex number $a + bi$ where $a$ and $b $ are : $(a + bi)(1) = a+ bi = \begin{bmatrix}            a \\            b \\          \end{bmatrix}$ $(a + bi)(i) = ai -b = \begin{bmatrix}            -b \\            a \\          \end{bmatrix}$ Which gives us the matrix \begin{pmatrix} a & -b\\ b & a\\ \end{pmatrix} Which is correct. The same approach worked for me for quaternions and real 4x4 matrices.","Using the basis , one can show that quaternions can be represented by 2x2 complex matrices as follows: I would like some help to understand this. Lets say Then we can represent the quaternion as . I would have thought that to find the matrix representation of complex numbers we would see what would happen if we multiply by the basis elements. This would mean the first column of our matrix would be And the second column would be So I would have thought the matrix representation would be I have a feeling I have some large misunderstanding about what I'm doing, I'm just following the same approach I did to find matrix representations of complex numbers with real 2x2 matrices and matrix representations of quaternions with real 4x4 matrices. e.g. with complex numbers, using a basis of , and a complex number where and are : Which gives us the matrix Which is correct. The same approach worked for me for quaternions and real 4x4 matrices."," B = \{1, j\} \begin{pmatrix}
z & w\\
-\bar{w} & \bar{z}\\
\end{pmatrix} z = a + bi, w = c + di h = a + bi + cj + dk z + wj z + wj (z + wj)(1) = z + wj = 
         \begin{bmatrix}
           z \\
           w \\
         \end{bmatrix} (z + wj)(j) = zj + wj^{2} = zj - w =          \begin{bmatrix}
           -w \\
           z \\
         \end{bmatrix} \begin{pmatrix}
z & -w\\
w & z\\
\end{pmatrix} B = \{1, i\} a + bi a b  (a + bi)(1) = a+ bi = \begin{bmatrix}
           a \\
           b \\
         \end{bmatrix} (a + bi)(i) = ai -b = \begin{bmatrix}
           -b \\
           a \\
         \end{bmatrix} \begin{pmatrix}
a & -b\\
b & a\\
\end{pmatrix}","['linear-algebra', 'matrices', 'complex-numbers', 'vectors', 'quaternions']"
29,Matrix Geometric Series,Matrix Geometric Series,,"For scalar geometric series, we know $$ \sum_{k=0}^{\infty} x^k  = \dfrac{1}{1-x} \text{ and } \sum_{k=0}^{\infty} kx^{k-1} = \dfrac{1}{(1-x)^2}\,.$$ Does the second one extend to square matrices? We know for $A$ being a $n \times n$ square matrix and $\|A\| < 1$, $\sum_{k=0}^{\infty} A^k = (I-A)^{-1}$. Does the following hold? $$\sum_{k=0}^{\infty} k A^{k-1} = (I-A)^{-2} $$","For scalar geometric series, we know $$ \sum_{k=0}^{\infty} x^k  = \dfrac{1}{1-x} \text{ and } \sum_{k=0}^{\infty} kx^{k-1} = \dfrac{1}{(1-x)^2}\,.$$ Does the second one extend to square matrices? We know for $A$ being a $n \times n$ square matrix and $\|A\| < 1$, $\sum_{k=0}^{\infty} A^k = (I-A)^{-1}$. Does the following hold? $$\sum_{k=0}^{\infty} k A^{k-1} = (I-A)^{-2} $$",,"['linear-algebra', 'matrices', 'power-series']"
30,About some details about the proof that real Lie algebra with positive Killing form is zero,About some details about the proof that real Lie algebra with positive Killing form is zero,,"In the post about proving that real Lie algebra with positive Killing form is zero: real Lie algebra with positive Killing form is zero : Let $L$ be a real Lie algebra with positive definite Killing form. Its Killing   form $\kappa$ defines an inner product on $L$. Hence $L$ is reductive. Thus the   quotient $L/Z(L)$ is semisimple. So, the Killing form is negative definite of    $L/Z(L)$. Therefore, this Killing form is both positive definite and negative definite, it follows that $L/Z(L) = {0}$. So we get $L = Z(L)=\ker(\kappa)$. But $\kappa$ is non-degenerate since it’s positive definite. It follows that $L= {0}$. I am confused with the following gaps: Why killing form on $L/Z(L)$ is negative definite? Why the induced Killing form on $L/Z(L)$ is positive definite? And is there any relation with the fact that $\mathfrak{g}$ is real?","In the post about proving that real Lie algebra with positive Killing form is zero: real Lie algebra with positive Killing form is zero : Let $L$ be a real Lie algebra with positive definite Killing form. Its Killing   form $\kappa$ defines an inner product on $L$. Hence $L$ is reductive. Thus the   quotient $L/Z(L)$ is semisimple. So, the Killing form is negative definite of    $L/Z(L)$. Therefore, this Killing form is both positive definite and negative definite, it follows that $L/Z(L) = {0}$. So we get $L = Z(L)=\ker(\kappa)$. But $\kappa$ is non-degenerate since it’s positive definite. It follows that $L= {0}$. I am confused with the following gaps: Why killing form on $L/Z(L)$ is negative definite? Why the induced Killing form on $L/Z(L)$ is positive definite? And is there any relation with the fact that $\mathfrak{g}$ is real?",,"['linear-algebra', 'lie-algebras']"
31,Why is the basis for the column space of a matrix $A$ merely the columns that which have pivots in $\operatorname{rref}(A)$?,Why is the basis for the column space of a matrix  merely the columns that which have pivots in ?,A \operatorname{rref}(A),"From my knowledge, the column space of a matrix is the vector space that is spanned by the column vectors of that matrix. From my lectures, I've been told to find a basis of the column space of a matrix (unless I've forgotten) by analog to finding a basis for the row space of a matrix, which would be to reduce the matrix $A^\sf T$ to row echelon form and note which rows have non-zero entries, just as similarly you would do for a basis of the row space—convert $A$ to row echelon form and take the rows that are non-zero. But apparently, and which I find easier, a column space basis can be found by merely noting which columns in $\operatorname{rref}(A)$ have a pivot, and then taking the original columns of $A$ as the basis for the column space. This doesn't make much intuitive sense to me. Why is this the case?","From my knowledge, the column space of a matrix is the vector space that is spanned by the column vectors of that matrix. From my lectures, I've been told to find a basis of the column space of a matrix (unless I've forgotten) by analog to finding a basis for the row space of a matrix, which would be to reduce the matrix to row echelon form and note which rows have non-zero entries, just as similarly you would do for a basis of the row space—convert to row echelon form and take the rows that are non-zero. But apparently, and which I find easier, a column space basis can be found by merely noting which columns in have a pivot, and then taking the original columns of as the basis for the column space. This doesn't make much intuitive sense to me. Why is this the case?",A^\sf T A \operatorname{rref}(A) A,"['linear-algebra', 'vector-spaces']"
32,Why do we define change of basis matrix to be the transpose of the transformation?,Why do we define change of basis matrix to be the transpose of the transformation?,,"Example. Let $V$ be a finite dim vector space with two different bases $S = \{ u_1,u_2 \} =   \{ (1,2),(3,5) \}$ and $S' = \{ v_1, v_2 \}  = \{ (1,-1), (1,-2) \} $ You can check that $v_1 = -8u_1 + 3u_2$ and $v_2 = -11u_1+4u_2$ and $P = \begin{bmatrix} -8 &-11 \\   3& 4 \end{bmatrix}$ is the change of basis where the columns are the coords. But why can't we define change of basis by their rows? Since it works nicely that $\begin{bmatrix}  v_1 \\ v_2  \end{bmatrix} = \begin{bmatrix} -8 &3 \\   -11& 4 \end{bmatrix}\begin{bmatrix} u_1 \\ u_2  \end{bmatrix}$ So to move from the old basis $S$, you apply the matrix $\begin{bmatrix} -8 &3 \\   -11& 4 \end{bmatrix}$ to get a new basis. Why do we have to transpose? If you transpose, how do you even use this change of basis? Why can't I use this definition of change of basis.","Example. Let $V$ be a finite dim vector space with two different bases $S = \{ u_1,u_2 \} =   \{ (1,2),(3,5) \}$ and $S' = \{ v_1, v_2 \}  = \{ (1,-1), (1,-2) \} $ You can check that $v_1 = -8u_1 + 3u_2$ and $v_2 = -11u_1+4u_2$ and $P = \begin{bmatrix} -8 &-11 \\   3& 4 \end{bmatrix}$ is the change of basis where the columns are the coords. But why can't we define change of basis by their rows? Since it works nicely that $\begin{bmatrix}  v_1 \\ v_2  \end{bmatrix} = \begin{bmatrix} -8 &3 \\   -11& 4 \end{bmatrix}\begin{bmatrix} u_1 \\ u_2  \end{bmatrix}$ So to move from the old basis $S$, you apply the matrix $\begin{bmatrix} -8 &3 \\   -11& 4 \end{bmatrix}$ to get a new basis. Why do we have to transpose? If you transpose, how do you even use this change of basis? Why can't I use this definition of change of basis.",,['linear-algebra']
33,Is $\mathbb{R^2} \subset \mathbb{C}$?,Is ?,\mathbb{R^2} \subset \mathbb{C},"Is $\mathbb{R^2} \subset \mathbb{C}$? I've heard that $\mathbb{R^2}$ is isomorphic to $\mathbb{C}$, but can we say that $\mathbb{R^2} \subset \mathbb{C}$? $\mathbb{R^2}$ is defined as $$\mathbb{R^2} = \{ (x,y) | x, y \in \mathbb{R}\}$$ and correct me if I'm wrong, but we can define $\mathbb{C}$ over the reals as follows: $$\mathbb{C} = \left\{(x, iy) | \ x, y \in \mathbb{R} \  \ \text{and} \  \ i = \sqrt{-1}  \right\}$$ But I don't see how we could show $a \in \mathbb{R^2} \implies a \in \mathbb{C}$. A counterexample could be that $(1,1) \in \mathbb{R^2}$, but $(1,1) \not\in \ \mathbb{C}$, however $(1, i) \in \mathbb{C}$. So is my conclusion that $\mathbb{R^2} \not\subset \mathbb{C}$ correct?","Is $\mathbb{R^2} \subset \mathbb{C}$? I've heard that $\mathbb{R^2}$ is isomorphic to $\mathbb{C}$, but can we say that $\mathbb{R^2} \subset \mathbb{C}$? $\mathbb{R^2}$ is defined as $$\mathbb{R^2} = \{ (x,y) | x, y \in \mathbb{R}\}$$ and correct me if I'm wrong, but we can define $\mathbb{C}$ over the reals as follows: $$\mathbb{C} = \left\{(x, iy) | \ x, y \in \mathbb{R} \  \ \text{and} \  \ i = \sqrt{-1}  \right\}$$ But I don't see how we could show $a \in \mathbb{R^2} \implies a \in \mathbb{C}$. A counterexample could be that $(1,1) \in \mathbb{R^2}$, but $(1,1) \not\in \ \mathbb{C}$, however $(1, i) \in \mathbb{C}$. So is my conclusion that $\mathbb{R^2} \not\subset \mathbb{C}$ correct?",,"['linear-algebra', 'complex-analysis', 'vector-spaces', 'complex-numbers']"
34,"Given finitely many points in a vector space $V$, is there a basis such that the first coordinate of each point is distinct?","Given finitely many points in a vector space , is there a basis such that the first coordinate of each point is distinct?",V,"Suppose I have some $n$-dimensional vector space $V$ and a finite collection of $m$ distinct points $v_1,\dotsc, v_m\in V$. Is there a basis of $V$ such that the first coordinate of each $v_i$ is distinct? This obviously fails when the base field is finite, but my intuition over $\mathbb{R}^n$ has convinced me it's true when the base field is infinite. I'm having a hard time proving it though. Any suggestions?","Suppose I have some $n$-dimensional vector space $V$ and a finite collection of $m$ distinct points $v_1,\dotsc, v_m\in V$. Is there a basis of $V$ such that the first coordinate of each $v_i$ is distinct? This obviously fails when the base field is finite, but my intuition over $\mathbb{R}^n$ has convinced me it's true when the base field is infinite. I'm having a hard time proving it though. Any suggestions?",,"['linear-algebra', 'abstract-algebra']"
35,Very general inner product determinant inequality,Very general inner product determinant inequality,,"Let $V$ be a vector space and $\langle \cdot, \cdot\rangle$ be an inner product on $V$. Prove that for any positive integer $n$ and any $x_1,\dots,x_n \in V$ \begin{equation}\det \left[ 	\begin{array}{cccc} 		\langle x_1,  x_1 \rangle & \langle x_2,  x_1 \rangle &\dots & \langle x_n,  x_1 \rangle\\ 		\langle x_1,  x_2 \rangle & \langle x_2,  x_2 \rangle & \dots & \langle x_n,  x_2 \rangle\\ 		\vdots & \vdots & \ddots & \vdots \\ 		\langle  x_1,  x_n \rangle & \langle x_2,  x_n \rangle & \dots & \langle  x_n,  x_n \rangle   \end{array} \right]\geq 0\,. \end{equation} The case of $n=1$ is trivial, it follows from the inner product's defining property. The case of $n=2$ is true due to the Cauchy-Schwarz inequality . A less general formula came up for $n=3$ during physics research, where there are physical reasons to expect that this inequality ought to hold. I cannot find a counterexample for any vector space dimension or matrix dimension. Is this fundamental inequality a well-known theorem?","Let $V$ be a vector space and $\langle \cdot, \cdot\rangle$ be an inner product on $V$. Prove that for any positive integer $n$ and any $x_1,\dots,x_n \in V$ \begin{equation}\det \left[ 	\begin{array}{cccc} 		\langle x_1,  x_1 \rangle & \langle x_2,  x_1 \rangle &\dots & \langle x_n,  x_1 \rangle\\ 		\langle x_1,  x_2 \rangle & \langle x_2,  x_2 \rangle & \dots & \langle x_n,  x_2 \rangle\\ 		\vdots & \vdots & \ddots & \vdots \\ 		\langle  x_1,  x_n \rangle & \langle x_2,  x_n \rangle & \dots & \langle  x_n,  x_n \rangle   \end{array} \right]\geq 0\,. \end{equation} The case of $n=1$ is trivial, it follows from the inner product's defining property. The case of $n=2$ is true due to the Cauchy-Schwarz inequality . A less general formula came up for $n=3$ during physics research, where there are physical reasons to expect that this inequality ought to hold. I cannot find a counterexample for any vector space dimension or matrix dimension. Is this fundamental inequality a well-known theorem?",,"['linear-algebra', 'hilbert-spaces', 'determinant', 'inner-products']"
36,Finding Euler decomposition of a symplectic matrix,Finding Euler decomposition of a symplectic matrix,,"A symplectic matrix is a $2n\times2n$ matrix $S$ with real entries that satisfies the condition $$ S^T \Omega S = \Omega $$ where $\Omega$ is the symplectic form, typically chosen to be $\Omega=\left(\begin{smallmatrix}0 & I_N \\ -I_N & 0\end{smallmatrix}\right)$. Sympletic matrices form the symplectic group $Sp(2n,\mathbb{R})$. Any symplectic matrix S can be decomposed as a product of three matrices as \begin{equation} S = O\begin{pmatrix}D & 0 \\ 0 & D^{-1}\end{pmatrix}O' \quad \quad \forall S \in Sp(2n,\mathbb{R}), \end{equation} where $O, O'$ are orthogonal and symplectic - $\operatorname{Sp}(2n,\mathbb{R})\cap \operatorname{O}(2n)$; $D$ is positive definite and diagonal. The form of a matrix that is both symplectic and orthogonal can be given in block form as $O=\left(\begin{smallmatrix}X & Y \\ -Y & X\end{smallmatrix}\right)$, where $XX^T+YY^T=I_N$ and $XY^T-YX^T=0$. The decomposition above is known as Euler decomposition or alternatively as Bloch-Messiah decomposition. How can I find the matrices in the decomposition for a given symplectic matrix? Apparently, the decomposition is closely related to the singular value decomposition and I think the elements of the matrices $D$ and $D^{-1}$ coincide with the singular values of $S$. Also, I have the impression that the case where it can be assumed that $S$ is also symmetric is easier. Any help, tips or pointers would be much appreciated!","A symplectic matrix is a $2n\times2n$ matrix $S$ with real entries that satisfies the condition $$ S^T \Omega S = \Omega $$ where $\Omega$ is the symplectic form, typically chosen to be $\Omega=\left(\begin{smallmatrix}0 & I_N \\ -I_N & 0\end{smallmatrix}\right)$. Sympletic matrices form the symplectic group $Sp(2n,\mathbb{R})$. Any symplectic matrix S can be decomposed as a product of three matrices as \begin{equation} S = O\begin{pmatrix}D & 0 \\ 0 & D^{-1}\end{pmatrix}O' \quad \quad \forall S \in Sp(2n,\mathbb{R}), \end{equation} where $O, O'$ are orthogonal and symplectic - $\operatorname{Sp}(2n,\mathbb{R})\cap \operatorname{O}(2n)$; $D$ is positive definite and diagonal. The form of a matrix that is both symplectic and orthogonal can be given in block form as $O=\left(\begin{smallmatrix}X & Y \\ -Y & X\end{smallmatrix}\right)$, where $XX^T+YY^T=I_N$ and $XY^T-YX^T=0$. The decomposition above is known as Euler decomposition or alternatively as Bloch-Messiah decomposition. How can I find the matrices in the decomposition for a given symplectic matrix? Apparently, the decomposition is closely related to the singular value decomposition and I think the elements of the matrices $D$ and $D^{-1}$ coincide with the singular values of $S$. Also, I have the impression that the case where it can be assumed that $S$ is also symmetric is easier. Any help, tips or pointers would be much appreciated!",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'svd', 'symplectic-linear-algebra']"
37,Trace norm of a triangular matrix with only ones above the diagonal,Trace norm of a triangular matrix with only ones above the diagonal,,"For $n\in\mathbb N^*$, we consider the triangular matrix $$ T_n =  \begin{pmatrix} 1 & \cdots & 1 \\   & \ddots & \vdots \\ 0 &  & 1 \end{pmatrix} \in M_{n,n}(\mathbb R) \,. $$ The trace norm of $T_n$, that is the sum of the singular values of $T_n$, is denoted by $\|T_n\|_{\text{Tr}}$. Is it true that $$ \sup_{n\in\mathbb N^*} \Big\{\frac{1}{n}\|T_n\|_{\text{Tr}}\Big\} < \infty \,? $$ EDIT Is it true that $$ \sup_{n\in\mathbb N^*} \Big\{\frac{1}{n\log(n)}\|T_n\|_{\text{Tr}}\Big\} < \infty \,? $$ An equivalent definition of the trace norm is $\|T_n\|_{\text{Tr}}:=\text{Tr}[\sqrt{T_n^T T_n}]$, where the square root $\sqrt{A}$ of a nonnegative matrix $A$ is the only nonnegative matrix such that $\sqrt{A}^2=A$. (And by $A$ nonnegative I mean $⟨u,Au⟩\geq0$ for all vector $u\in\mathbb R^n$). EDIT 2 One can explicitly compute the singular values of $T_n$. $$T_n^{-1} = \begin{pmatrix} 1 & -1 & & 0 \\   & \ddots & \ddots & \\  & & \ddots & -1 \\ 0 & & & 1 \end{pmatrix} \in M_{n,n}(\mathbb R) \,. $$ The singular values $\sigma_1,\dots,\sigma_n$ of $T_n$ are related to those, $\lambda_1,\dots,\lambda_n$, of $T_n^{-1}$ through $\sigma_j=\lambda_j^{-1}$. It is easier to compute the singular values of $T_n^{-1}$ because the eigenvalues $\mu_j=\lambda_j^2$ of $$A_n = (T_n^{-1})^* T_n^{-1} = \begin{pmatrix}   1    &    -1  &        &        &     0  \\  -1    &    2   &  -1    &        &        \\        &    -1  & \ddots & \ddots &        \\        &        & \ddots & \ddots &    -1  \\   0    &        &        &    -1  &     2 \end{pmatrix} \,,$$ can be computed explicitly (as for the discrete laplacian). Eigenvalues of $A_n$ First, $A_n$ is real symmetric, hence it can be diagonalized in an orthonormal basis, and its eigenvalues $\mu_j$ are real. Then using, say, Gershgorin's circle theorem , the eigenvalues are in the interval $[0,4]$. If $\psi=(\psi_1,\dots,\psi_n)^T$ is an eigenvector of $A_n$ associated with the eigenvalue $\mu$, then \begin{align} \psi_2 & = (1-\mu)\psi_1 & (1) \\ \psi_3 & = (2-\mu)\psi_2 - \psi_1 & (2)\\  & \vdots \\ \psi_{j+2} &=  (2-\mu)\psi_{j+1} - \psi_j & (j)\\  & \vdots \\ \psi_n & = (2-\mu)\psi_{n-1} - \psi_{n-2} & (n-1)\\ (2-\mu)\psi_n &= \psi_{n-1} & (n) \end{align} From Eq. $(2)$ to $(n-1)$, one can see that $\psi_j$ is linear, recursive sequence of order two. Since the roots of the polynomial $X^2+(\mu-2)X+1$ are $$\frac{2-\mu\pm i \sqrt{\mu(4-\mu)}}{2}=e^{\pm i\theta}$$ with $\theta\in [0,\pi]$ and $\cos(\theta)=1-\frac{\mu}{2}$, $\psi_j=\Re(a e^{i(j-1)\theta})$ with $a$ a complex number. Up to a (real) normalization factor $\psi_j=\Re(e^{i(\varphi+(j-1)\theta)})$ for some $\varphi\in\mathbb R$. Using $\mu = 2-e^{i\theta}-e^{-i\theta}$ and Eq.(1) and (n), yields \begin{align} e^{i(\varphi+\theta)}+e^{-i(\varphi+\theta)}&=(e^{i\theta}+e^{-i\theta}-1)(e^{i\varphi}+e^{-i\varphi}) \\ (e^{i\theta}+e^{-i\theta})(e^{i(\varphi+(n-1)\theta)}+e^{-i(\varphi+(n-1)\theta)})&=e^{i(\varphi+(n-2)\theta)}+e^{-i(\varphi+(n-2)\theta)} \end{align} i.e. \begin{align} \cos(\varphi-\theta)&=\cos(\varphi) & (1)'\\ \cos(\varphi+n\theta)&=0 & (n)' \end{align} From $(1)'$, either $\theta=0$ or $\varphi = \frac{\theta}{2}+k\pi$. $\theta=0$ would give $\mu=0$ which is excluded since $A_n$ is an invertible matrix. Hence using $\varphi = \frac{\theta}{2}+k\pi$ and $(n)'$: $$(n+\frac{1}{2})\theta=(k+\frac{1}{2})\pi$$ Using that $\theta\in[0,\pi]$, we get that $\theta\in \Big\{\frac{j-\frac{1}{2}}{n+\frac{1}{2}} \pi \mid j=1,\dots,n\Big\}$. And in fact each of these values yields an eigenvalue and an eigenvector. The corresponding eigenvalues are $\mu_j=4\sin^2\Big(\frac{j-\frac{1}{2}}{n+\frac{1}{2}} \frac{\pi}{2}\Big)$, $ j=1,\dots,n$. Trace Norm of $T_n$ The singular values of $T_n$ can now be deduced: $$\sigma_j=\frac{1}{2\sin\Big(\frac{j-\frac{1}{2}}{n+\frac{1}{2}} \frac{\pi}{2}\Big)}\,,\quad  j=1,\dots,n$$ and the trace norm is $$ \|T_n\|_{\text{Tr}}=\frac{1}{2}\sum_{j=1}^n \frac{1}{\sin\Big(\frac{j-\frac{1}{2}}{n+\frac{1}{2}} \frac{\pi}{2}\Big)} \,.$$ Using that $\sin x\geq \frac{2}{\pi}x$ on $[0,\frac{\pi}{2}]$ one gets the upper bound : $$ \|T_n\|_{\text{Tr}}\leq \frac{n+\frac{1}{2}}{2}\sum_{j=1}^n \frac{1}{j-1/2}\leq \frac{n+\frac{1}{2}}{2} \Big(\frac{1}{2}+\ln(2n+1)\Big)\,,$$ which implies that $$ \limsup_{n\in\mathbb N^*} \Big\{\frac{1}{n\log(n)}\|T_n\|_{\text{Tr}}\Big\} \leq \frac{1}{2} \,. $$ Actually one also has a lower bound $$ \frac{n+1/2}{\pi}\Big(\ln(\tan(\frac{\pi}{4}))-\ln(\tan(\frac{\pi}{4(n+1/2)}))\Big) \leq \frac{n+1/2}{\pi} \int_{\frac{\pi}{2n+1}}^{\frac{\pi}{2}} \frac{dx}{\sin(x)}  \leq \|T_n\|_{\text{Tr}} \,,$$ which implies that $$ \frac{1}{\pi} \leq \liminf_{n\in\mathbb N^*} \Big\{\frac{1}{n\log(n)}\|T_n\|_{\text{Tr}}\Big\} \,. $$","For $n\in\mathbb N^*$, we consider the triangular matrix $$ T_n =  \begin{pmatrix} 1 & \cdots & 1 \\   & \ddots & \vdots \\ 0 &  & 1 \end{pmatrix} \in M_{n,n}(\mathbb R) \,. $$ The trace norm of $T_n$, that is the sum of the singular values of $T_n$, is denoted by $\|T_n\|_{\text{Tr}}$. Is it true that $$ \sup_{n\in\mathbb N^*} \Big\{\frac{1}{n}\|T_n\|_{\text{Tr}}\Big\} < \infty \,? $$ EDIT Is it true that $$ \sup_{n\in\mathbb N^*} \Big\{\frac{1}{n\log(n)}\|T_n\|_{\text{Tr}}\Big\} < \infty \,? $$ An equivalent definition of the trace norm is $\|T_n\|_{\text{Tr}}:=\text{Tr}[\sqrt{T_n^T T_n}]$, where the square root $\sqrt{A}$ of a nonnegative matrix $A$ is the only nonnegative matrix such that $\sqrt{A}^2=A$. (And by $A$ nonnegative I mean $⟨u,Au⟩\geq0$ for all vector $u\in\mathbb R^n$). EDIT 2 One can explicitly compute the singular values of $T_n$. $$T_n^{-1} = \begin{pmatrix} 1 & -1 & & 0 \\   & \ddots & \ddots & \\  & & \ddots & -1 \\ 0 & & & 1 \end{pmatrix} \in M_{n,n}(\mathbb R) \,. $$ The singular values $\sigma_1,\dots,\sigma_n$ of $T_n$ are related to those, $\lambda_1,\dots,\lambda_n$, of $T_n^{-1}$ through $\sigma_j=\lambda_j^{-1}$. It is easier to compute the singular values of $T_n^{-1}$ because the eigenvalues $\mu_j=\lambda_j^2$ of $$A_n = (T_n^{-1})^* T_n^{-1} = \begin{pmatrix}   1    &    -1  &        &        &     0  \\  -1    &    2   &  -1    &        &        \\        &    -1  & \ddots & \ddots &        \\        &        & \ddots & \ddots &    -1  \\   0    &        &        &    -1  &     2 \end{pmatrix} \,,$$ can be computed explicitly (as for the discrete laplacian). Eigenvalues of $A_n$ First, $A_n$ is real symmetric, hence it can be diagonalized in an orthonormal basis, and its eigenvalues $\mu_j$ are real. Then using, say, Gershgorin's circle theorem , the eigenvalues are in the interval $[0,4]$. If $\psi=(\psi_1,\dots,\psi_n)^T$ is an eigenvector of $A_n$ associated with the eigenvalue $\mu$, then \begin{align} \psi_2 & = (1-\mu)\psi_1 & (1) \\ \psi_3 & = (2-\mu)\psi_2 - \psi_1 & (2)\\  & \vdots \\ \psi_{j+2} &=  (2-\mu)\psi_{j+1} - \psi_j & (j)\\  & \vdots \\ \psi_n & = (2-\mu)\psi_{n-1} - \psi_{n-2} & (n-1)\\ (2-\mu)\psi_n &= \psi_{n-1} & (n) \end{align} From Eq. $(2)$ to $(n-1)$, one can see that $\psi_j$ is linear, recursive sequence of order two. Since the roots of the polynomial $X^2+(\mu-2)X+1$ are $$\frac{2-\mu\pm i \sqrt{\mu(4-\mu)}}{2}=e^{\pm i\theta}$$ with $\theta\in [0,\pi]$ and $\cos(\theta)=1-\frac{\mu}{2}$, $\psi_j=\Re(a e^{i(j-1)\theta})$ with $a$ a complex number. Up to a (real) normalization factor $\psi_j=\Re(e^{i(\varphi+(j-1)\theta)})$ for some $\varphi\in\mathbb R$. Using $\mu = 2-e^{i\theta}-e^{-i\theta}$ and Eq.(1) and (n), yields \begin{align} e^{i(\varphi+\theta)}+e^{-i(\varphi+\theta)}&=(e^{i\theta}+e^{-i\theta}-1)(e^{i\varphi}+e^{-i\varphi}) \\ (e^{i\theta}+e^{-i\theta})(e^{i(\varphi+(n-1)\theta)}+e^{-i(\varphi+(n-1)\theta)})&=e^{i(\varphi+(n-2)\theta)}+e^{-i(\varphi+(n-2)\theta)} \end{align} i.e. \begin{align} \cos(\varphi-\theta)&=\cos(\varphi) & (1)'\\ \cos(\varphi+n\theta)&=0 & (n)' \end{align} From $(1)'$, either $\theta=0$ or $\varphi = \frac{\theta}{2}+k\pi$. $\theta=0$ would give $\mu=0$ which is excluded since $A_n$ is an invertible matrix. Hence using $\varphi = \frac{\theta}{2}+k\pi$ and $(n)'$: $$(n+\frac{1}{2})\theta=(k+\frac{1}{2})\pi$$ Using that $\theta\in[0,\pi]$, we get that $\theta\in \Big\{\frac{j-\frac{1}{2}}{n+\frac{1}{2}} \pi \mid j=1,\dots,n\Big\}$. And in fact each of these values yields an eigenvalue and an eigenvector. The corresponding eigenvalues are $\mu_j=4\sin^2\Big(\frac{j-\frac{1}{2}}{n+\frac{1}{2}} \frac{\pi}{2}\Big)$, $ j=1,\dots,n$. Trace Norm of $T_n$ The singular values of $T_n$ can now be deduced: $$\sigma_j=\frac{1}{2\sin\Big(\frac{j-\frac{1}{2}}{n+\frac{1}{2}} \frac{\pi}{2}\Big)}\,,\quad  j=1,\dots,n$$ and the trace norm is $$ \|T_n\|_{\text{Tr}}=\frac{1}{2}\sum_{j=1}^n \frac{1}{\sin\Big(\frac{j-\frac{1}{2}}{n+\frac{1}{2}} \frac{\pi}{2}\Big)} \,.$$ Using that $\sin x\geq \frac{2}{\pi}x$ on $[0,\frac{\pi}{2}]$ one gets the upper bound : $$ \|T_n\|_{\text{Tr}}\leq \frac{n+\frac{1}{2}}{2}\sum_{j=1}^n \frac{1}{j-1/2}\leq \frac{n+\frac{1}{2}}{2} \Big(\frac{1}{2}+\ln(2n+1)\Big)\,,$$ which implies that $$ \limsup_{n\in\mathbb N^*} \Big\{\frac{1}{n\log(n)}\|T_n\|_{\text{Tr}}\Big\} \leq \frac{1}{2} \,. $$ Actually one also has a lower bound $$ \frac{n+1/2}{\pi}\Big(\ln(\tan(\frac{\pi}{4}))-\ln(\tan(\frac{\pi}{4(n+1/2)}))\Big) \leq \frac{n+1/2}{\pi} \int_{\frac{\pi}{2n+1}}^{\frac{\pi}{2}} \frac{dx}{\sin(x)}  \leq \|T_n\|_{\text{Tr}} \,,$$ which implies that $$ \frac{1}{\pi} \leq \liminf_{n\in\mathbb N^*} \Big\{\frac{1}{n\log(n)}\|T_n\|_{\text{Tr}}\Big\} \,. $$",,"['linear-algebra', 'matrices', 'functional-analysis', 'asymptotics', 'operator-theory']"
38,What is the notation for the empty matrix?,What is the notation for the empty matrix?,,"My question: What is a notation for an empty 0x0 matrix (i.e. the matrix for the only linear map $f:\{0\}\to\{0\}$)? Is it written $()$? How can I distinguish the 0x0 matrix with for example the 0x3 matrix or the 3x0 matrix? What I have already found out: Concerning the section “Empty matrices” of the Wikipedia article “Matrix (mathematics)” there “is no common notation for empty matrices”. But unfortunately I haven't found any notation so far... Notes: I am looking for a notation which is used in a textbook. I am not interested in how empty matrices can be created in CAS like Matlab, Mathematica, etc. Reason for this question: In our course we had the task to draw all graphs with three vertices and to state the incidence matrix for each drawn graph. Thus, for the empty graph I have to state a 0x3 matrix, but I didn't know the right notation for it...","My question: What is a notation for an empty 0x0 matrix (i.e. the matrix for the only linear map $f:\{0\}\to\{0\}$)? Is it written $()$? How can I distinguish the 0x0 matrix with for example the 0x3 matrix or the 3x0 matrix? What I have already found out: Concerning the section “Empty matrices” of the Wikipedia article “Matrix (mathematics)” there “is no common notation for empty matrices”. But unfortunately I haven't found any notation so far... Notes: I am looking for a notation which is used in a textbook. I am not interested in how empty matrices can be created in CAS like Matlab, Mathematica, etc. Reason for this question: In our course we had the task to draw all graphs with three vertices and to state the incidence matrix for each drawn graph. Thus, for the empty graph I have to state a 0x3 matrix, but I didn't know the right notation for it...",,"['linear-algebra', 'matrices', 'notation']"
39,Show that every rotation in $\mathbb{R^3}$ can be written as the product of two rotations of order 2.,Show that every rotation in  can be written as the product of two rotations of order 2.,\mathbb{R^3},"Show that every rotation in  $\mathbb{R^3}$ can be written as the   product of two rotations of order 2. Here's my attempt at a solution: We know that any rotation in  $\mathbb{R^3}$ can be represented as the product of two reflections. So we write our rotation as $R_1R_2$ where the $R_j$ are reflections in  $\mathbb{R^3}$. We would like to show that $R_1R_2$=$(R_aR_b)(R_cR_d)$=$R_aR_bR_cR_d$ where $R_aR_b$ and $R_cR_d$ are rotations of order 2 in $\mathbb{R^3}$. I think I have shown that a rotation $R_1R_2$, which is the product of reflections in the planes $\Pi_1$ and $\Pi_2$ respectively, has order two if and only if $\Pi_1$ and $\Pi_2$ are perpendicular. Next I thought it sufficient to show that $R_1$=$R_aR_b$ (and similarly for $R_2$ with $R_c$ and $R_d$) and as $R_1$ and $R_2$ are just any reflections, I now attempt to show that any reflection can be written as the product of two reflections in perpendicular planes. The form of a reflection in the plane $x.n=d$ in $\mathbb{R^3}$ is given as $R(x)=x+2(d-x.n)n$. Suppose that we have $R_a$ and $R_b$ as reflections in the perpendicular planes (that is, the planes have perpendicular normals), $x.n_a=d_a$ and $x.n_b=d_b$ respectively. As $n_a.n_b=0$, we can show that $R_aR_b(x)= x+2(d_1+d_2)-(x.(n_1+n_2))(n_1+n_2)$ which is of the required form. So that for any reflection $R$ in $x.n=d$, we can set $n_1,n_2,d_1,d_2$ such that $d_1+d_2=d$ and $n_1+n_2=n$. First off, is this correct, have I shown what I was required to show? I was also wondering if there may be a nicer, perhaps geometric way of going about the question. Apologies if my attempt at a solution is difficult to follow, I have next to zero experience in writing formal solutions.","Show that every rotation in  $\mathbb{R^3}$ can be written as the   product of two rotations of order 2. Here's my attempt at a solution: We know that any rotation in  $\mathbb{R^3}$ can be represented as the product of two reflections. So we write our rotation as $R_1R_2$ where the $R_j$ are reflections in  $\mathbb{R^3}$. We would like to show that $R_1R_2$=$(R_aR_b)(R_cR_d)$=$R_aR_bR_cR_d$ where $R_aR_b$ and $R_cR_d$ are rotations of order 2 in $\mathbb{R^3}$. I think I have shown that a rotation $R_1R_2$, which is the product of reflections in the planes $\Pi_1$ and $\Pi_2$ respectively, has order two if and only if $\Pi_1$ and $\Pi_2$ are perpendicular. Next I thought it sufficient to show that $R_1$=$R_aR_b$ (and similarly for $R_2$ with $R_c$ and $R_d$) and as $R_1$ and $R_2$ are just any reflections, I now attempt to show that any reflection can be written as the product of two reflections in perpendicular planes. The form of a reflection in the plane $x.n=d$ in $\mathbb{R^3}$ is given as $R(x)=x+2(d-x.n)n$. Suppose that we have $R_a$ and $R_b$ as reflections in the perpendicular planes (that is, the planes have perpendicular normals), $x.n_a=d_a$ and $x.n_b=d_b$ respectively. As $n_a.n_b=0$, we can show that $R_aR_b(x)= x+2(d_1+d_2)-(x.(n_1+n_2))(n_1+n_2)$ which is of the required form. So that for any reflection $R$ in $x.n=d$, we can set $n_1,n_2,d_1,d_2$ such that $d_1+d_2=d$ and $n_1+n_2=n$. First off, is this correct, have I shown what I was required to show? I was also wondering if there may be a nicer, perhaps geometric way of going about the question. Apologies if my attempt at a solution is difficult to follow, I have next to zero experience in writing formal solutions.",,"['linear-algebra', 'geometry', '3d', 'rotations', 'affine-geometry']"
40,Scaling a matrix to make its eigenvalues fall within a certain interval,Scaling a matrix to make its eigenvalues fall within a certain interval,,"Suppose I have a diagonalizable matrix $M$ which has all its eigenvalues between $a$ and $b$. Is it possible to scale $M$ to $M_S$ such that all the eigenvalues of $M_s$ lie in the interval $[-1,1]$? One method I came across: Scale such that $$  M_s=\frac{M-(b+a)/2}{(b-a)/2}. $$ But, this is not working. Does anyone know anything better?","Suppose I have a diagonalizable matrix $M$ which has all its eigenvalues between $a$ and $b$. Is it possible to scale $M$ to $M_S$ such that all the eigenvalues of $M_s$ lie in the interval $[-1,1]$? One method I came across: Scale such that $$  M_s=\frac{M-(b+a)/2}{(b-a)/2}. $$ But, this is not working. Does anyone know anything better?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
41,Projection of a vector onto the null space of a matrix,Projection of a vector onto the null space of a matrix,,"I have the following optimization problem: $$ \text{minimize}_x \Vert z - x \Vert^2 \\ \text{subject to } Ax = 0, $$ where $x,z\in \mathbb{C}^N$, and $A\in\mathbb{C}^{M \times N}$. $A$ is a wide matrix, i.e. $M \le N$, with rank $M$. I found a closed-form solution to this problem in ""D. Bertsekas, Nonlinear Programming, 1999"", which is $$ x_\star = (I_N - A^H(AA^H)^{-1}A)z, $$ where $I_N$ is the $N \times N$ identity matrix. However, I'm having problems deriving this solution. I have tried to use the Lagrange multiplier method as follows. The dual optimization problem is $$ \text{minimize}_{\{x,\lambda\}} \Vert z - x \Vert^2 + \lambda \Vert Ax \Vert^2, $$ where $\lambda > 0$ is a Lagrange multiplier. Setting the derivative of the Lagrangian Dual with respect to $x$ to zero gives $$ x_\star = (I_N - A^HA)^{-1}z, $$ and,  applying the matrix inversion lemma, I get $$ x_\star = (I_N - A^H(\tfrac{1}{\lambda}I_M + A A^H)^{-1}A)z. $$ Thus, the solution in the book and the solution I'm getting are equal when $\lambda \rightarrow \infty$. What does this mean? Any ideas how can I get the $\lambda \rightarrow \infty$ condition? Thank you very much for your help.","I have the following optimization problem: $$ \text{minimize}_x \Vert z - x \Vert^2 \\ \text{subject to } Ax = 0, $$ where $x,z\in \mathbb{C}^N$, and $A\in\mathbb{C}^{M \times N}$. $A$ is a wide matrix, i.e. $M \le N$, with rank $M$. I found a closed-form solution to this problem in ""D. Bertsekas, Nonlinear Programming, 1999"", which is $$ x_\star = (I_N - A^H(AA^H)^{-1}A)z, $$ where $I_N$ is the $N \times N$ identity matrix. However, I'm having problems deriving this solution. I have tried to use the Lagrange multiplier method as follows. The dual optimization problem is $$ \text{minimize}_{\{x,\lambda\}} \Vert z - x \Vert^2 + \lambda \Vert Ax \Vert^2, $$ where $\lambda > 0$ is a Lagrange multiplier. Setting the derivative of the Lagrangian Dual with respect to $x$ to zero gives $$ x_\star = (I_N - A^HA)^{-1}z, $$ and,  applying the matrix inversion lemma, I get $$ x_\star = (I_N - A^H(\tfrac{1}{\lambda}I_M + A A^H)^{-1}A)z. $$ Thus, the solution in the book and the solution I'm getting are equal when $\lambda \rightarrow \infty$. What does this mean? Any ideas how can I get the $\lambda \rightarrow \infty$ condition? Thank you very much for your help.",,"['linear-algebra', 'optimization']"
42,Equivalence of $\|x\|_1\|x\|_{\infty}$ and $\|x\|_2^2$,Equivalence of  and,\|x\|_1\|x\|_{\infty} \|x\|_2^2,"Let $x$ be any complex $n$-vector and let $\|\cdot\|_p$ denote the usual $p$-norm . It is easy to show that $\|x\|_2^2\leq\|x\|_1\|x\|_{\infty}$ ( Hölder's inequality ). What I am rather interested in is the reversed inequality: finding a $c$ ($c>1$) such that $$\tag{1} \|x\|_1\|x\|_\infty\leq c\|x\|_2^2\quad\text{for all $x$.} $$ Obviously, $\|x\|_1\leq\sqrt{n}\|x\|_2$ and $\|x\|_\infty\leq\|x\|_2$, so an easy candidate is $c_\mathrm{naive}:=\sqrt{n}$. However, it seems that a better constant is about a half of the naive one: $c_\mathrm{better}:=(1+\sqrt{n})/2$. I was wondering about a proof for this better $c$, that is, how to prove that $$\tag{2}\|x\|_1\|x\|_\infty\leq\frac{1+\sqrt{n}}{2}\|x\|_2^2\quad\text{for all $x\in\mathbb{C}^n$.}$$ WLOG we can assume that $x:=[1,y^T]^T$, where $\|y\|_\infty\leq 1$, so the quest for the optimal $c$ in (1) is equivalent to finding (or bounding from above) $$\tag{3} c_\mathrm{optimal}:=\max_{\|y\|_\infty\leq 1}\frac{1+\|y\|_1}{1+\|y\|_2^2}\;. $$ By some experimentation, it seems that actually (2) is sharp, that is, $c_\mathrm{optimal}=c_\mathrm{better}$, and the bound in (2) is attained by $x=[1,\alpha,\ldots,\alpha]^T$, where $\alpha=1/(\sqrt{n}+1)$. Since this is a problem in an early chapter of Matrix Computations by Golub and Van Loan, I suppose its prove might be not overly complicated and, since a hint is missing, it should actually be quite easy. Any input will be highly appretiated; a hint if possible :-)","Let $x$ be any complex $n$-vector and let $\|\cdot\|_p$ denote the usual $p$-norm . It is easy to show that $\|x\|_2^2\leq\|x\|_1\|x\|_{\infty}$ ( Hölder's inequality ). What I am rather interested in is the reversed inequality: finding a $c$ ($c>1$) such that $$\tag{1} \|x\|_1\|x\|_\infty\leq c\|x\|_2^2\quad\text{for all $x$.} $$ Obviously, $\|x\|_1\leq\sqrt{n}\|x\|_2$ and $\|x\|_\infty\leq\|x\|_2$, so an easy candidate is $c_\mathrm{naive}:=\sqrt{n}$. However, it seems that a better constant is about a half of the naive one: $c_\mathrm{better}:=(1+\sqrt{n})/2$. I was wondering about a proof for this better $c$, that is, how to prove that $$\tag{2}\|x\|_1\|x\|_\infty\leq\frac{1+\sqrt{n}}{2}\|x\|_2^2\quad\text{for all $x\in\mathbb{C}^n$.}$$ WLOG we can assume that $x:=[1,y^T]^T$, where $\|y\|_\infty\leq 1$, so the quest for the optimal $c$ in (1) is equivalent to finding (or bounding from above) $$\tag{3} c_\mathrm{optimal}:=\max_{\|y\|_\infty\leq 1}\frac{1+\|y\|_1}{1+\|y\|_2^2}\;. $$ By some experimentation, it seems that actually (2) is sharp, that is, $c_\mathrm{optimal}=c_\mathrm{better}$, and the bound in (2) is attained by $x=[1,\alpha,\ldots,\alpha]^T$, where $\alpha=1/(\sqrt{n}+1)$. Since this is a problem in an early chapter of Matrix Computations by Golub and Van Loan, I suppose its prove might be not overly complicated and, since a hint is missing, it should actually be quite easy. Any input will be highly appretiated; a hint if possible :-)",,"['linear-algebra', 'normed-spaces']"
43,Linear programming algorithm that minimizes number of non-zero variables?,Linear programming algorithm that minimizes number of non-zero variables?,,"I have real world problems I'm trying to programmatically solve in the form of $$Z = c_1 x_1 + c_2 x_2 + \cdots + c_n x_n$$ Subject to \begin{align} & a_{11} x_1 + a_{21} x_2 + \cdots + a_{n1} = b_1 \\[6pt] & 0 \le a_{12} x_1 \le b_2 \\[6pt] & 0 \le a_{23} x_2 \le b_3 \\ & {}\qquad\vdots \\ & 0 \le a_{nm} x_n \le b_m \end{align} Right now I'm using the simplex method. Because of the first constraint there are many solutions and I don't really care to min/max to any particular coefficients. What I really care about is to have as few non-zero variables in the objective function as possible. A good way to look at it is, I have packages to deliver and $x$s are trucks. I don't care how long they take, I just want to use as few trucks as possible. One strategy I came up with was to minimize $Z$ and set the objective coefficients to something like: $$c_n = 10n$$ so that: $$Z = 10 x_1 + 20 x_2 + \cdots + 10 n x_n$$ This mostly works but is a little hacky and, depending on what $a_{nm}$ is, at times inconsistent. I'm not married to the simplex method, but it's fast and gives me a solution so I'm using it.","I have real world problems I'm trying to programmatically solve in the form of $$Z = c_1 x_1 + c_2 x_2 + \cdots + c_n x_n$$ Subject to \begin{align} & a_{11} x_1 + a_{21} x_2 + \cdots + a_{n1} = b_1 \\[6pt] & 0 \le a_{12} x_1 \le b_2 \\[6pt] & 0 \le a_{23} x_2 \le b_3 \\ & {}\qquad\vdots \\ & 0 \le a_{nm} x_n \le b_m \end{align} Right now I'm using the simplex method. Because of the first constraint there are many solutions and I don't really care to min/max to any particular coefficients. What I really care about is to have as few non-zero variables in the objective function as possible. A good way to look at it is, I have packages to deliver and $x$s are trucks. I don't care how long they take, I just want to use as few trucks as possible. One strategy I came up with was to minimize $Z$ and set the objective coefficients to something like: $$c_n = 10n$$ so that: $$Z = 10 x_1 + 20 x_2 + \cdots + 10 n x_n$$ This mostly works but is a little hacky and, depending on what $a_{nm}$ is, at times inconsistent. I'm not married to the simplex method, but it's fast and gives me a solution so I'm using it.",,"['linear-algebra', 'optimization', 'linear-programming', 'simplex']"
44,Diagonalizing $xyz$,Diagonalizing,xyz,"The quadratic form  $g(x,y) = xy$    can be diagonalized by the change of variables  $x = (u + v)$   and  $y = (u - v)$ . However, it seems unlikely that the cubic form  $f(x,y,z) = xyz$, can be diagonalized by a linear change of variables. Is there a short computational or theoretical proof of this? Thanks.","The quadratic form  $g(x,y) = xy$    can be diagonalized by the change of variables  $x = (u + v)$   and  $y = (u - v)$ . However, it seems unlikely that the cubic form  $f(x,y,z) = xyz$, can be diagonalized by a linear change of variables. Is there a short computational or theoretical proof of this? Thanks.",,"['linear-algebra', 'abstract-algebra']"
45,Density of Pythagorean triples,Density of Pythagorean triples,,"We define a Pythagorean triple as a triple $<a,b,c>$ such that $a,b,c\in \mathbb N$ and $a^2+b^2=c^2$. In order to avoid duplicates, we say that a triple $<a,b,c>$ is legit iff $b>a$. Let $\mathcal P$ be the set of all legit Pythagorean triples. We define $$L_{PT}^N=\{<a,b,c> | <a,b,c> \in  \mathcal P\wedge b\leq N\}$$ (If it's more convinient we can define it for $b^2\leq N$, $c\leq N$ or $c^2\leq N$). What is the density of $|L_{PT}^N|$ as a function of $N$? e.g. is $|L_{PT}^N|=\Theta(N^2)?\Theta(N)?$ We say that a triple $<a,b,c>$ is minimal if $gcd(a,b,c)=1$. Let $\mathcal P_M$ be the set of all legit, minimal triples. Let $$L_{MPT}^N=\{<a,b,c> | <a,b,c> \in  \mathcal P_M\wedge b\leq N\}$$ What is the density of $|L_{MPT}^N|$ as a function of $N$? e.g. is $|L_{MPT}^N|=\Theta(N)?$","We define a Pythagorean triple as a triple $<a,b,c>$ such that $a,b,c\in \mathbb N$ and $a^2+b^2=c^2$. In order to avoid duplicates, we say that a triple $<a,b,c>$ is legit iff $b>a$. Let $\mathcal P$ be the set of all legit Pythagorean triples. We define $$L_{PT}^N=\{<a,b,c> | <a,b,c> \in  \mathcal P\wedge b\leq N\}$$ (If it's more convinient we can define it for $b^2\leq N$, $c\leq N$ or $c^2\leq N$). What is the density of $|L_{PT}^N|$ as a function of $N$? e.g. is $|L_{PT}^N|=\Theta(N^2)?\Theta(N)?$ We say that a triple $<a,b,c>$ is minimal if $gcd(a,b,c)=1$. Let $\mathcal P_M$ be the set of all legit, minimal triples. Let $$L_{MPT}^N=\{<a,b,c> | <a,b,c> \in  \mathcal P_M\wedge b\leq N\}$$ What is the density of $|L_{MPT}^N|$ as a function of $N$? e.g. is $|L_{MPT}^N|=\Theta(N)?$",,"['linear-algebra', 'combinatorics', 'asymptotics', 'pythagorean-triples']"
46,"$ T $ is normal if and only if for every $ T $-invariant subspace, its orthogonal complement is also $ T $-invariant.","is normal if and only if for every -invariant subspace, its orthogonal complement is also -invariant.", T   T   T ,"Proposition: Suppose that $ V $ is a complex vector space and $ \dim(V) < \infty $. Then $ T \in \mathcal{L}(V) $ is normal if and only if the orthogonal complement of every $ T $-invariant subspace is $ T $-invariant. I hope that you can help me with a solution or a hint. Thanks. My idea: The forward implication: If $ T $ is normal, then $ T^{*} = p(T) $ for any polynomial $ p \in \Bbb{C}[X] $. Then given a $ T $-invariant subspace $ U $, we know that $ U $ is $ p(T) $-invariant. In other words, $ U $ is $ T^{*} $-invariant. As $ U $ is $ T^{*} $-invariant, it follows that $ W \stackrel{\text{df}}{=} U^{\perp} $ is $ (T^{*})^{*} $-invariant. Hence, $ W $ is $ T $-invariant. I was unable to work out the backward implication.","Proposition: Suppose that $ V $ is a complex vector space and $ \dim(V) < \infty $. Then $ T \in \mathcal{L}(V) $ is normal if and only if the orthogonal complement of every $ T $-invariant subspace is $ T $-invariant. I hope that you can help me with a solution or a hint. Thanks. My idea: The forward implication: If $ T $ is normal, then $ T^{*} = p(T) $ for any polynomial $ p \in \Bbb{C}[X] $. Then given a $ T $-invariant subspace $ U $, we know that $ U $ is $ p(T) $-invariant. In other words, $ U $ is $ T^{*} $-invariant. As $ U $ is $ T^{*} $-invariant, it follows that $ W \stackrel{\text{df}}{=} U^{\perp} $ is $ (T^{*})^{*} $-invariant. Hence, $ W $ is $ T $-invariant. I was unable to work out the backward implication.",,"['linear-algebra', 'vector-spaces', 'linear-transformations']"
47,Inequality for norm of linear combination of linearly independent vectors,Inequality for norm of linear combination of linearly independent vectors,,"I'm trying to find a proof for the following: Let {$u_{1},...,u_{n}$} be a linearly independent set of a normed space $X$. Then, there is a constant $c>0$ such that for every set of scalars $\{\alpha_{1},...,\alpha_{n}\}:$ $$\| \alpha_{1}u_{1}+...+\alpha_{n}u_{n} \| \ge c(|\alpha_{1}|+...|\alpha_{n}|)$$ I don't even know how to start.","I'm trying to find a proof for the following: Let {$u_{1},...,u_{n}$} be a linearly independent set of a normed space $X$. Then, there is a constant $c>0$ such that for every set of scalars $\{\alpha_{1},...,\alpha_{n}\}:$ $$\| \alpha_{1}u_{1}+...+\alpha_{n}u_{n} \| \ge c(|\alpha_{1}|+...|\alpha_{n}|)$$ I don't even know how to start.",,"['linear-algebra', 'inequality', 'normed-spaces']"
48,"Trace, determinant and eigenvalues for non-diagonalizable matrix","Trace, determinant and eigenvalues for non-diagonalizable matrix",,"Is the trace and determinant equal to the sum and product, respectively, of the eigenvalues even if a matrix is not diagonalizable? The proof I've seen for the trace equaling the sum of the eigenvalues and the determinant equaling the eigenvalues assumes a matrix can be written as $PDP^{-1}$. But if it is not diagonalizable, then we can't possibly write $PDP^{-1}$. So what happens then?","Is the trace and determinant equal to the sum and product, respectively, of the eigenvalues even if a matrix is not diagonalizable? The proof I've seen for the trace equaling the sum of the eigenvalues and the determinant equaling the eigenvalues assumes a matrix can be written as $PDP^{-1}$. But if it is not diagonalizable, then we can't possibly write $PDP^{-1}$. So what happens then?",,"['linear-algebra', 'matrices']"
49,Vector space bases without axiom of choice,Vector space bases without axiom of choice,,I want to find an example of a vector space with no base if we assume that axiom of choice is incorrect. This question might be duplicate so please alert me. Thanks.,I want to find an example of a vector space with no base if we assume that axiom of choice is incorrect. This question might be duplicate so please alert me. Thanks.,,"['linear-algebra', 'vector-spaces', 'axiom-of-choice', 'hamel-basis']"
50,To prove that the dimension of $V$ is $d_1^2 + \ldots + d_k^2$ [duplicate],To prove that the dimension of  is  [duplicate],V d_1^2 + \ldots + d_k^2,"This question already has an answer here : Diagonalizable Operators and characteristic polynomials (1 answer) Closed 3 years ago . Let $A$ be an $n \times n$ diagonal matrix with characteristic polynomial   $$(x - c_1)^{d_1} \cdots (x - c_k)^{d_k} , $$   where $c_1,\ldots,c_k$ are distinct. Let $V$ be the space of $n \times n$ matrices $B$ such that $AB = BA$. Prove that the dimension of $V$ is $d_1^2 + \cdots + d_k^2$. I am completely stuck on it. Can someone help me please? Thanks for your help.","This question already has an answer here : Diagonalizable Operators and characteristic polynomials (1 answer) Closed 3 years ago . Let $A$ be an $n \times n$ diagonal matrix with characteristic polynomial   $$(x - c_1)^{d_1} \cdots (x - c_k)^{d_k} , $$   where $c_1,\ldots,c_k$ are distinct. Let $V$ be the space of $n \times n$ matrices $B$ such that $AB = BA$. Prove that the dimension of $V$ is $d_1^2 + \cdots + d_k^2$. I am completely stuck on it. Can someone help me please? Thanks for your help.",,"['linear-algebra', 'matrices', 'vector-spaces', 'eigenvalues-eigenvectors']"
51,Similar matrices and minimal polynomial,Similar matrices and minimal polynomial,,"I guess that I'm missing here something... How to prove that, if two matrices are similar, then their minimal polynomials are the same one. Thanks!","I guess that I'm missing here something... How to prove that, if two matrices are similar, then their minimal polynomials are the same one. Thanks!",,"['linear-algebra', 'matrices', 'polynomials']"
52,Contradiction in the rules regarding determinants and row operations?,Contradiction in the rules regarding determinants and row operations?,,"In my textbook it says that if you multiply a row in a matrix $A$ by a nonzero constant $c$ to obtain $B$, then $\det{B}=c\det{A}$. Later on it says that if you obtain $B = cA$ by adding $c$ times the $k^{\text{th}}$ row of $A$ to the $j^{\text{th}}$ row, $\det{B}=\det{A}$. Isn't this a contradiction though? Is not adding $c$ times the $k^{\text{th}}$ row of $A$ to the $j^{\text{th}}$ row equivalent to multiplying the $k^{\text{th}}$ row by $c$, which increases the determinant by a factor of $c$, and then adding the row down? In other words, is (I) the same as the (II) with the $2$ steps combined? I. $cR_k + R_j \rightarrow R_j$. $1$ step in total. II. First, do $cR_k \rightarrow R_k$.   Second, do $cR_k + R_j \rightarrow R_j$. $2$ steps in total.","In my textbook it says that if you multiply a row in a matrix $A$ by a nonzero constant $c$ to obtain $B$, then $\det{B}=c\det{A}$. Later on it says that if you obtain $B = cA$ by adding $c$ times the $k^{\text{th}}$ row of $A$ to the $j^{\text{th}}$ row, $\det{B}=\det{A}$. Isn't this a contradiction though? Is not adding $c$ times the $k^{\text{th}}$ row of $A$ to the $j^{\text{th}}$ row equivalent to multiplying the $k^{\text{th}}$ row by $c$, which increases the determinant by a factor of $c$, and then adding the row down? In other words, is (I) the same as the (II) with the $2$ steps combined? I. $cR_k + R_j \rightarrow R_j$. $1$ step in total. II. First, do $cR_k \rightarrow R_k$.   Second, do $cR_k + R_j \rightarrow R_j$. $2$ steps in total.",,['linear-algebra']
53,Symmetric Square Root of Symmetric Invertible Matrix,Symmetric Square Root of Symmetric Invertible Matrix,,"I am trying to find out if for any symmetric (Not necessarily self-adjoint), invertible matrix $A$ over $\mathbb{C}$, there is a square root of the matrix that is also symmetric. I was able to figure out that for invertible matrices there always exists a square root since I can explicitly do it for a general Jordan block and go from there but I was hoping that it would necessarily be symmetric under this construction (which seems either not obvious or not true). Any thoughts?","I am trying to find out if for any symmetric (Not necessarily self-adjoint), invertible matrix $A$ over $\mathbb{C}$, there is a square root of the matrix that is also symmetric. I was able to figure out that for invertible matrices there always exists a square root since I can explicitly do it for a general Jordan block and go from there but I was hoping that it would necessarily be symmetric under this construction (which seems either not obvious or not true). Any thoughts?",,"['linear-algebra', 'matrices', 'operator-theory']"
54,A regular graph of degree k is connected if and only if the eigenvalue k has multiplicity one,A regular graph of degree k is connected if and only if the eigenvalue k has multiplicity one,,I saw this sentence in Wikipedia : A regular graph of degree k is connected if and only if the eigenvalue k has multiplicity one I couldn't find a proof to that statement - can someone address me to a proof?,I saw this sentence in Wikipedia : A regular graph of degree k is connected if and only if the eigenvalue k has multiplicity one I couldn't find a proof to that statement - can someone address me to a proof?,,"['linear-algebra', 'graph-theory']"
55,Find a nonnegative basis of a matrix nullspace / kernel,Find a nonnegative basis of a matrix nullspace / kernel,,"I have a matrix $S$ and need to find a set of basis vectors $\{\mathbf{x_i}\}$ such that $S\mathbf{x_i}=0$ and $\mathbf{x_i} \ge \mathbf{0}$ (component-wise, i.e. $x_i^k \ge 0$ ). This problem comes up in networks of chemical reactions where $S$ is the stoichiometric matrix and $\mathbf{x_i}$ are extreme vectors that define a proper, polyhedral cone at steady state (I don't understand entirely what all of this means - have a picture in my head though). More detail: Suppose you're looking at a concentration network involving $n$ chemical species , summarized in a vector $\textbf{u}$ , and $m$ reactions with rates $\textbf{w}(\textbf{u})$ . Then an ODE that describes the behaviour of this dynamical system is $$\frac{d \textbf{u}}{dt}=S\textbf{w}(\textbf{u}).$$ The stoichiometric matrix $S \in \mathbb{Z}^{n \times m}$ describes how much of the species involved in a reaction is consumed / produced relative to one other. At steady state, $d\textbf{u}/dt|_{\textbf{u}=\textbf{u}^*}=0$ , we now look for the kernel / null space of $S$ , i.e. reaction rates $\textbf{w}$ such that $S\mathbf{w}=0$ . Apparently (however I don't fully understand this), the intersection of $\text{ker}(S)$ and $\mathbb{R}_+^m$ forms a proper, polyhedral cone. This cone can be represented by a nonnegative combination of the finite set of extreme vectors that are unique up to scaling by a positive constant - the $\textbf{x}_i$ above are these extreme vectors. ( I quoted the latter bit mostly verbatim from .) Possible route of solving this: So there appears to be a way to define this as a linear programming (LP) problem, but I don't quite see this. This seems to be suggested here . Any elaboration on the LP approach or any other way of solving this is greatly appreciated.","I have a matrix and need to find a set of basis vectors such that and (component-wise, i.e. ). This problem comes up in networks of chemical reactions where is the stoichiometric matrix and are extreme vectors that define a proper, polyhedral cone at steady state (I don't understand entirely what all of this means - have a picture in my head though). More detail: Suppose you're looking at a concentration network involving chemical species , summarized in a vector , and reactions with rates . Then an ODE that describes the behaviour of this dynamical system is The stoichiometric matrix describes how much of the species involved in a reaction is consumed / produced relative to one other. At steady state, , we now look for the kernel / null space of , i.e. reaction rates such that . Apparently (however I don't fully understand this), the intersection of and forms a proper, polyhedral cone. This cone can be represented by a nonnegative combination of the finite set of extreme vectors that are unique up to scaling by a positive constant - the above are these extreme vectors. ( I quoted the latter bit mostly verbatim from .) Possible route of solving this: So there appears to be a way to define this as a linear programming (LP) problem, but I don't quite see this. This seems to be suggested here . Any elaboration on the LP approach or any other way of solving this is greatly appreciated.",S \{\mathbf{x_i}\} S\mathbf{x_i}=0 \mathbf{x_i} \ge \mathbf{0} x_i^k \ge 0 S \mathbf{x_i} n \textbf{u} m \textbf{w}(\textbf{u}) \frac{d \textbf{u}}{dt}=S\textbf{w}(\textbf{u}). S \in \mathbb{Z}^{n \times m} d\textbf{u}/dt|_{\textbf{u}=\textbf{u}^*}=0 S \textbf{w} S\mathbf{w}=0 \text{ker}(S) \mathbb{R}_+^m \textbf{x}_i,"['linear-algebra', 'linear-programming', 'convex-cone']"
56,"Computing the trace and determinant of $A+B$, given eigenvalues of $A$ and an expression for $B$","Computing the trace and determinant of , given eigenvalues of  and an expression for",A+B A B,"Let $A$ be $4\times 4$ matrix with real entries such that $-1$, $1$, $2$, and $-2$ are its eigenvalues. If $B = A^4 - 5A^2+5I$, where $I$ denotes $4\times 4$ identity matrix, then what would be determinant and trace of matrix $A+B$?","Let $A$ be $4\times 4$ matrix with real entries such that $-1$, $1$, $2$, and $-2$ are its eigenvalues. If $B = A^4 - 5A^2+5I$, where $I$ denotes $4\times 4$ identity matrix, then what would be determinant and trace of matrix $A+B$?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
57,Is there a name referring to this result?,Is there a name referring to this result?,,"For any real $m \times n$ matrix $A$, it seems that $$\det(I_n + A^{T}A) = \det(I_m + AA^{T}) $$ always holds, where $I_n$ is the identity matrix of size $n$. Though I have not tried to prove this yet, I'm sure it is a part of well-known results in linear algebra. So my question is, what is the name referring to this fact, and where can I find a reference to it?","For any real $m \times n$ matrix $A$, it seems that $$\det(I_n + A^{T}A) = \det(I_m + AA^{T}) $$ always holds, where $I_n$ is the identity matrix of size $n$. Though I have not tried to prove this yet, I'm sure it is a part of well-known results in linear algebra. So my question is, what is the name referring to this fact, and where can I find a reference to it?",,"['linear-algebra', 'matrices', 'determinant']"
58,Identities for other coefficients of the characteristic polynomial,Identities for other coefficients of the characteristic polynomial,,We have $\operatorname{tr}(A+B) = \operatorname{tr}(A) + \operatorname{tr}(B)$ and $\det(AB) = \det(A) \det(B)$.  Are there any analogous identities for the other coefficients of the characteristic polynomial?,We have $\operatorname{tr}(A+B) = \operatorname{tr}(A) + \operatorname{tr}(B)$ and $\det(AB) = \det(A) \det(B)$.  Are there any analogous identities for the other coefficients of the characteristic polynomial?,,"['linear-algebra', 'matrices']"
59,references for Jordan Canonical Forms,references for Jordan Canonical Forms,,I am trying to study Jordan forms from the book by Hoffman and Kunze (chapters 6 and 7) and find it a little too terse. Could someone please suggest an alternative reference or a supplement to this book. Thanks.,I am trying to study Jordan forms from the book by Hoffman and Kunze (chapters 6 and 7) and find it a little too terse. Could someone please suggest an alternative reference or a supplement to this book. Thanks.,,"['linear-algebra', 'reference-request']"
60,why is the following thing a projection operator?,why is the following thing a projection operator?,,"Let $T: E \rightarrow E$ be an endomorphism of a finite-dimensional vector space, and let $S$ be a circle in the complex plane that does not intersect any eigenvalues of $T$. Now let $Q = \frac{1}{2\pi i} \int_S (z-T)^{-1} \, dz$. Why is $Q$ a projection operator? The motivation behind this question is that the above situation occurs in a proof of Bott's periodicity theorem, but it's not clear to me that $Q$ is a projection...","Let $T: E \rightarrow E$ be an endomorphism of a finite-dimensional vector space, and let $S$ be a circle in the complex plane that does not intersect any eigenvalues of $T$. Now let $Q = \frac{1}{2\pi i} \int_S (z-T)^{-1} \, dz$. Why is $Q$ a projection operator? The motivation behind this question is that the above situation occurs in a proof of Bott's periodicity theorem, but it's not clear to me that $Q$ is a projection...",,['linear-algebra']
61,Intuition Behind Balanced Sets,Intuition Behind Balanced Sets,,"Suppose $B \subset X$ where $X$ is a vector space. $B$ is called balanced if $\alpha B \subset B$ for every $\alpha \in \Phi$ with $|\alpha| \leq 1$. Note that $\Phi = \textbf{R}$ or $\Phi = \textbf{C}$. What is the intuition behind balanced sets? Why do we define them? Is it ""good"" for a vector space to have more balanced sets? Why do we require $| \alpha| \leq 1$?","Suppose $B \subset X$ where $X$ is a vector space. $B$ is called balanced if $\alpha B \subset B$ for every $\alpha \in \Phi$ with $|\alpha| \leq 1$. Note that $\Phi = \textbf{R}$ or $\Phi = \textbf{C}$. What is the intuition behind balanced sets? Why do we define them? Is it ""good"" for a vector space to have more balanced sets? Why do we require $| \alpha| \leq 1$?",,"['linear-algebra', 'intuition', 'functional-analysis']"
62,Why do I mistakenly think there's an error in this proof by Hoffman and Kunze?,Why do I mistakenly think there's an error in this proof by Hoffman and Kunze?,,"Theorem 4 (Chapter 1) in Hoffman and Kunze's Linear Algebra , 2nd edition (HK) says Theorem 4: Every $m \times n$ matrix over the field $F$ is row-equivalent to a row-reduced matrix. Before supplying the proof, I'll note their definition of a row-reduced matrix: Definition. An $m \times n$ matrix $R$ is called row-reduced if: (a) the first non-zero entry in each non-zero row of R is equal to 1; (b) each column of $R$ which contains the leading non-zero entry of some row has all its other entries 0. Their proof of Theorem 4 is now given as below: Proof. Let A be an $m \times n$ matrix over $F$ . If every entry in the first row of $A$ is 0, then condition (a) is satisfied insofar as row 1 is concerned. If row 1 has a non-zero entry, let $k$ be the smallest positive integer $j$ for which $A_{1j} \neq 0$ . Multiply row 1 by $A_{1k}^{-1}$ , and then condition (a) is satisfied with regard to row 1. Now for each $i \geq 2$ , add ( $-A_{ik}$ ) times row 1 to row i. Now the leading non-zero entry of row 1 occurs in column $k$ , that entry is 1, and every other entry in column $k$ is 0. Now consider the matrix which has resulted from above. If every entry in row 2 is 0, we do nothing to row 2. If some entry in row 2 is different from 0, we multiply row 2 by a scalar so that the leading non-zero entry is 1. In the event that row 1 had a leading non-zero entry in column $k$ , this leading non-zero entry of row 2 cannot occur in column $k$ ; say it occurs in column $k' \neq k$ . By adding suitable multiples of row 2 to the various rows, we can arrange that all entries in column $k'$ are 0, except the 1 in row 2. The important thing to notice is this: In carrying out these last operations, we will not change the entries of row 1 in columns $1, . . . , k$ ; nor will we change any entry of column $k$ . Of course, if row 1 was identically 0, the operations with row 2 will not affect row 1. Working with one row at a time in the above manner, it is clear that in a finite number of steps we will arrive at a row-reduced matrix. My concern is with the bolded parts, and in particular the claim that ""these last operations, we will not change the entries of row 1 in columns $1, . . . , k$ "". Surely if $k>k'$ this need not be true in general, for then any entry $A_{1m}$ with $k' \leq m <k$ will be changed since it is permitted that $A_{2m} \neq 0$ for such $m$ ? Is it possible that HK make an error here and instead meant to say ""these last operations, we will not change the entries of row 1 in columns $1, . . . , k'$ ""? Or, perhaps, if we consult this proof from ProofWiki, we see that the first step is to consider the first non-zero column $j$ , and then take any row which contains a non-zero entry in said column. HK do no such thing, and just plug on ahead. If they did do this, I think their proof would be rescued since we would always have $k<k'$ , right? This did not appear as an erratum in this thread, so I am concerned that I am misunderstanding the proof.","Theorem 4 (Chapter 1) in Hoffman and Kunze's Linear Algebra , 2nd edition (HK) says Theorem 4: Every matrix over the field is row-equivalent to a row-reduced matrix. Before supplying the proof, I'll note their definition of a row-reduced matrix: Definition. An matrix is called row-reduced if: (a) the first non-zero entry in each non-zero row of R is equal to 1; (b) each column of which contains the leading non-zero entry of some row has all its other entries 0. Their proof of Theorem 4 is now given as below: Proof. Let A be an matrix over . If every entry in the first row of is 0, then condition (a) is satisfied insofar as row 1 is concerned. If row 1 has a non-zero entry, let be the smallest positive integer for which . Multiply row 1 by , and then condition (a) is satisfied with regard to row 1. Now for each , add ( ) times row 1 to row i. Now the leading non-zero entry of row 1 occurs in column , that entry is 1, and every other entry in column is 0. Now consider the matrix which has resulted from above. If every entry in row 2 is 0, we do nothing to row 2. If some entry in row 2 is different from 0, we multiply row 2 by a scalar so that the leading non-zero entry is 1. In the event that row 1 had a leading non-zero entry in column , this leading non-zero entry of row 2 cannot occur in column ; say it occurs in column . By adding suitable multiples of row 2 to the various rows, we can arrange that all entries in column are 0, except the 1 in row 2. The important thing to notice is this: In carrying out these last operations, we will not change the entries of row 1 in columns ; nor will we change any entry of column . Of course, if row 1 was identically 0, the operations with row 2 will not affect row 1. Working with one row at a time in the above manner, it is clear that in a finite number of steps we will arrive at a row-reduced matrix. My concern is with the bolded parts, and in particular the claim that ""these last operations, we will not change the entries of row 1 in columns "". Surely if this need not be true in general, for then any entry with will be changed since it is permitted that for such ? Is it possible that HK make an error here and instead meant to say ""these last operations, we will not change the entries of row 1 in columns ""? Or, perhaps, if we consult this proof from ProofWiki, we see that the first step is to consider the first non-zero column , and then take any row which contains a non-zero entry in said column. HK do no such thing, and just plug on ahead. If they did do this, I think their proof would be rescued since we would always have , right? This did not appear as an erratum in this thread, so I am concerned that I am misunderstanding the proof.","m \times n F m \times n R R m \times n F A k j A_{1j} \neq 0 A_{1k}^{-1} i \geq 2 -A_{ik} k k k k k' \neq k k' 1, . . . , k k 1, . . . , k k>k' A_{1m} k' \leq m <k A_{2m} \neq 0 m 1, . . . , k' j k<k'","['linear-algebra', 'proof-explanation']"
63,A determinantal inequality,A determinantal inequality,,"The following is an exercise from the 2023 international selection at École Normale Supérieure (ENS): Prove that for any complex numbers $a_1, \dots, a_n$ and positive semi-definite complex matrices $A_1, \dots ,A_n$ , the following inequality is satisfied: $$ \det \left( \left| a_1 \right| A_{1\ } + \left| a_2 \right| A_2 + \dots + \left| a_n \right| A_n \right) \geq | \det(a_1 A_1 + \dots + a_n A_n)| $$ So far, I have tried using the Hadamard inequality on determinants but it doesn't seem to lead anywhere. Same for expanding the determinant using the permutations formula. Since it's an exercise from ENS, it is supposed to be somehow hard and I don't know how to ""start"". The exercises from ENS are known to be very hard to approach since it's literally the best mathematics department of France. Any hints, please?","The following is an exercise from the 2023 international selection at École Normale Supérieure (ENS): Prove that for any complex numbers and positive semi-definite complex matrices , the following inequality is satisfied: So far, I have tried using the Hadamard inequality on determinants but it doesn't seem to lead anywhere. Same for expanding the determinant using the permutations formula. Since it's an exercise from ENS, it is supposed to be somehow hard and I don't know how to ""start"". The exercises from ENS are known to be very hard to approach since it's literally the best mathematics department of France. Any hints, please?","a_1, \dots, a_n A_1, \dots ,A_n  \det \left( \left| a_1 \right| A_{1\ } + \left| a_2 \right| A_2 + \dots + \left| a_n \right| A_n \right) \geq | \det(a_1 A_1 + \dots + a_n A_n)| ","['linear-algebra', 'permutations', 'determinant']"
64,"If every dense subspace of a normed space $E$ is of finite codimension, is $E$ finite-dimensional?","If every dense subspace of a normed space  is of finite codimension, is  finite-dimensional?",E E,"Let $E$ be a normed $\mathbb{K}$ -vector space ( $\mathbb{K} = \mathbb{R}$ or $\mathbb{C}$ ). I'm interested in the following question: If every dense subspace is of finite codimension in $E$ , then is $E$ finite-dimensional? As a reminder, the codimension of a subspace $D$ is the dimension of the quotient space $E/D$ . It can be shown that it's also the dimension of all algebraic complements of $D$ . This post provides an example of a space in which there exists a dense subspace of infinite codimension, and this post provides a way to construct dense subspaces of any given finite codimension, but I didn't find anything addressing my exact question, though there's probably something I missed here, or maybe it's on MathOverflow I don't know. If possible, if a non-Banach counterexample is found, it'd be nice to then reconsider the question for $E$ Banach, but let's not get ahead of ourselves. I also do not mind at all even just having partial answers, like ""true for $E$ separable Hilbert"", or ""true for $E$ of the form $\mathcal{C}(X,\mathbb{K})$ "", or whatever special cases may come up in your minds. I'm also pro-axiom of choice, in case it makes a difference for this question (like, if you need Hamel bases or that kind of things). Finally, I know that usually question-havers should show what they've tried, but, I'll be honest, I don't really know how to begin... However I thought it was somewhat interesting enough that I'd try asking here anyway, hopefully that is fine? This is not for any homework or project, just my own curiosity. (Feel free to re-tag or edit this post if needed)","Let be a normed -vector space ( or ). I'm interested in the following question: If every dense subspace is of finite codimension in , then is finite-dimensional? As a reminder, the codimension of a subspace is the dimension of the quotient space . It can be shown that it's also the dimension of all algebraic complements of . This post provides an example of a space in which there exists a dense subspace of infinite codimension, and this post provides a way to construct dense subspaces of any given finite codimension, but I didn't find anything addressing my exact question, though there's probably something I missed here, or maybe it's on MathOverflow I don't know. If possible, if a non-Banach counterexample is found, it'd be nice to then reconsider the question for Banach, but let's not get ahead of ourselves. I also do not mind at all even just having partial answers, like ""true for separable Hilbert"", or ""true for of the form "", or whatever special cases may come up in your minds. I'm also pro-axiom of choice, in case it makes a difference for this question (like, if you need Hamel bases or that kind of things). Finally, I know that usually question-havers should show what they've tried, but, I'll be honest, I don't really know how to begin... However I thought it was somewhat interesting enough that I'd try asking here anyway, hopefully that is fine? This is not for any homework or project, just my own curiosity. (Feel free to re-tag or edit this post if needed)","E \mathbb{K} \mathbb{K} = \mathbb{R} \mathbb{C} E E D E/D D E E E \mathcal{C}(X,\mathbb{K})","['linear-algebra', 'functional-analysis', 'normed-spaces']"
65,How to distinguish between vertical and horizontal stretch/shrink when ambiguous?,How to distinguish between vertical and horizontal stretch/shrink when ambiguous?,,"Please bear with me. I am trying to help my daughter with her Algebra 1 homework. We are asked to describe the transformation of function f to function g as follows: $$f(x) = x$$ $$g(x) = 2x+3$$ The provided answer states that $g(x)=2x+3$ can be re-written as $$g(x)=2f(x)+3$$ and is therefore a vertical stretch by a factor of 2 (plus a vertical translation up by 3 units). Well and good. However, $g(x)=2x+3$ can also be re-written as $$g(x)=f(2x)+3$$ and be described as a horizontal shrink by a factor of 1/2. But even though this horizontal shrink gives exactly the same graph as the vertical stretch, it is not mentioned as a possible correct answer. I understand that the order of transformations is important and can give completely different graphs if you mess up the order, but this is not the case here. There is at least one more question in the study material that likewise lists the vertical stretch, but not the identical horizontal shrink, as the correct answer. Is it because g is originally expressed as $g(x)=2x+3$ ? Does this necessitate that we think of the transformation only in the vertical axis? Something to do with $y=mx+b$ where $m=2$ ? Many thanks.","Please bear with me. I am trying to help my daughter with her Algebra 1 homework. We are asked to describe the transformation of function f to function g as follows: The provided answer states that can be re-written as and is therefore a vertical stretch by a factor of 2 (plus a vertical translation up by 3 units). Well and good. However, can also be re-written as and be described as a horizontal shrink by a factor of 1/2. But even though this horizontal shrink gives exactly the same graph as the vertical stretch, it is not mentioned as a possible correct answer. I understand that the order of transformations is important and can give completely different graphs if you mess up the order, but this is not the case here. There is at least one more question in the study material that likewise lists the vertical stretch, but not the identical horizontal shrink, as the correct answer. Is it because g is originally expressed as ? Does this necessitate that we think of the transformation only in the vertical axis? Something to do with where ? Many thanks.",f(x) = x g(x) = 2x+3 g(x)=2x+3 g(x)=2f(x)+3 g(x)=2x+3 g(x)=f(2x)+3 g(x)=2x+3 y=mx+b m=2,['linear-algebra']
66,"How to prove that the matrix P has eigenvalues $1,-\frac{1}{2},\cdots,(-1)^{n-1}\frac{1}{n}$?",How to prove that the matrix P has eigenvalues ?,"1,-\frac{1}{2},\cdots,(-1)^{n-1}\frac{1}{n}","I came up with this when trying to solve a problem of a markov chain with the transition matrix $$P=\begin{bmatrix} 0,0,0,\cdots,1\\ 0,0,\cdots,\frac{1}{2},\frac{1}{2}\\ 0,\cdots,\frac{1}{3},\frac{1}{3},\frac{1}{3}\\ \cdots\cdots\\ \frac{1}{n},\cdots,\frac{1}{n},\frac{1}{n} \end{bmatrix}$$ and it asked me to find $$\lim\limits_{k \rightarrow +\infty}{P^k}\alpha$$ where $\alpha=(0,1,\cdots,n-1)^\top$ . So, I tried to diagonalize $P$ and surprisedly found that it has eigenvalues $1,-\frac{1}{2},\cdots,(-1)^{n-1}\frac{1}{n}$ when $n\leq 7$ .So I wonder if this is true for all $n$ from $N^+$ and then how to calculate $$\lim\limits_{k \rightarrow +\infty}{P^k}\alpha$$ Thanks!","I came up with this when trying to solve a problem of a markov chain with the transition matrix and it asked me to find where . So, I tried to diagonalize and surprisedly found that it has eigenvalues when .So I wonder if this is true for all from and then how to calculate Thanks!","P=\begin{bmatrix} 0,0,0,\cdots,1\\ 0,0,\cdots,\frac{1}{2},\frac{1}{2}\\ 0,\cdots,\frac{1}{3},\frac{1}{3},\frac{1}{3}\\ \cdots\cdots\\ \frac{1}{n},\cdots,\frac{1}{n},\frac{1}{n} \end{bmatrix} \lim\limits_{k \rightarrow +\infty}{P^k}\alpha \alpha=(0,1,\cdots,n-1)^\top P 1,-\frac{1}{2},\cdots,(-1)^{n-1}\frac{1}{n} n\leq 7 n N^+ \lim\limits_{k \rightarrow +\infty}{P^k}\alpha","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'markov-chains']"
67,Clean proof of Baker-Campbell-Hausdorff Formula,Clean proof of Baker-Campbell-Hausdorff Formula,,"I am thinking of the cleanest way to prove the BCH formula and I have come up with this. First, work out $e^{\lambda A}Be^{-\lambda A}$ by expanding the exponentials (sums go from $0$ to $\infty$ ): $$\left(\sum_{n}\frac{\lambda^n}{n!}A^n \right)B\left(\sum_{k}\frac{(-\lambda)^k}{k!}A^k \right).$$ This can be written as $$\sum_{n,k}\frac{(-1)^k\lambda^{n+k}}{n!k!}A^nBA^k.$$ We define $m=n+k$ , and rewrite the previous expression as $$\sum_{m=0}^{\infty}\sum_{n=0}^m\frac{(-1)^{m-n}\lambda^{m}}{n!(m-n)!}A^nBA^{m-n}.$$ By dividing and multiplyling by $m!$ inside the sum we finally arrive to $$\sum_{m=0}^{\infty}\frac{\lambda^m}{m!} \sum_{n=0}^m(-1)^{m-n}\frac{m!}{n!(m-n)!}A^nBA^{m-n}.$$ The formula is  usually presented as $$e^{\lambda A}Be^{-\lambda A}=B+\lambda[A,B]+\frac{\lambda^2}{2!}[A,[A,B]]+...$$ By comparing with what I got, proving BCH is reduced to proving $$\underbrace{[A,[A,[...,[A,B]...]}_{m}=\sum_{n=0}^m(-1)^{m-n}\frac{m!}{n!(m-n)!}A^nBA^{m-n}.$$ At this point I thought this could be easily proven by induction, but now I'm not sure it is that simple. The equation is true for $m=1$ , and if we assume it is true for $m$ , then we get $$\underbrace{[A,[A,[...,[A,B]...]}_{m+1}=A\underbrace{[A,[A,[...,[A,B]...]}_{m}-\underbrace{[A,[A,[...,[A,B]...]}_{m}A$$ $$=A\left(\sum_{n=0}^m(-1)^{m-n}\frac{m!}{n!(m-n)!}A^nBA^{m-n}\right)-\left(\sum_{n=0}^m(-1)^{m-n}\frac{m!}{n!(m-n)!}A^nBA^{m-n}\right)A$$ $$=\sum_{n=0}^m(-1)^{m-n}\frac{m!}{n!(m-n)!}(A^{n+1}BA^{m-n}-A^nBA^{m+1-n})$$ $$=\sum_{n=0}^m(-1)^{m+1-n}\frac{m!}{n!(m-n)!}(A^nBA^{m+1-n}-A^{n+1}BA^{m-n}).$$ I would like to see this is equal to $$\sum_{n=0}^{m+1}(-1)^{m+1-n}\frac{(m+1)!}{n!(m+1-n)!}A^nBA^{m+1-n},$$ since that would complete the proof. I've tried to work it by inserting commutators here and there, but the algebra becomes too involved. Any help would be truly appreciated. Maybe the last expression is more transparent if read as $$\sum_{n=0}^{m+1}(-1)^{m+1-n}\begin{pmatrix}m+1\\n \end{pmatrix}A^nBA^{m+1-n}$$","I am thinking of the cleanest way to prove the BCH formula and I have come up with this. First, work out by expanding the exponentials (sums go from to ): This can be written as We define , and rewrite the previous expression as By dividing and multiplyling by inside the sum we finally arrive to The formula is  usually presented as By comparing with what I got, proving BCH is reduced to proving At this point I thought this could be easily proven by induction, but now I'm not sure it is that simple. The equation is true for , and if we assume it is true for , then we get I would like to see this is equal to since that would complete the proof. I've tried to work it by inserting commutators here and there, but the algebra becomes too involved. Any help would be truly appreciated. Maybe the last expression is more transparent if read as","e^{\lambda A}Be^{-\lambda A} 0 \infty \left(\sum_{n}\frac{\lambda^n}{n!}A^n \right)B\left(\sum_{k}\frac{(-\lambda)^k}{k!}A^k \right). \sum_{n,k}\frac{(-1)^k\lambda^{n+k}}{n!k!}A^nBA^k. m=n+k \sum_{m=0}^{\infty}\sum_{n=0}^m\frac{(-1)^{m-n}\lambda^{m}}{n!(m-n)!}A^nBA^{m-n}. m! \sum_{m=0}^{\infty}\frac{\lambda^m}{m!} \sum_{n=0}^m(-1)^{m-n}\frac{m!}{n!(m-n)!}A^nBA^{m-n}. e^{\lambda A}Be^{-\lambda A}=B+\lambda[A,B]+\frac{\lambda^2}{2!}[A,[A,B]]+... \underbrace{[A,[A,[...,[A,B]...]}_{m}=\sum_{n=0}^m(-1)^{m-n}\frac{m!}{n!(m-n)!}A^nBA^{m-n}. m=1 m \underbrace{[A,[A,[...,[A,B]...]}_{m+1}=A\underbrace{[A,[A,[...,[A,B]...]}_{m}-\underbrace{[A,[A,[...,[A,B]...]}_{m}A =A\left(\sum_{n=0}^m(-1)^{m-n}\frac{m!}{n!(m-n)!}A^nBA^{m-n}\right)-\left(\sum_{n=0}^m(-1)^{m-n}\frac{m!}{n!(m-n)!}A^nBA^{m-n}\right)A =\sum_{n=0}^m(-1)^{m-n}\frac{m!}{n!(m-n)!}(A^{n+1}BA^{m-n}-A^nBA^{m+1-n}) =\sum_{n=0}^m(-1)^{m+1-n}\frac{m!}{n!(m-n)!}(A^nBA^{m+1-n}-A^{n+1}BA^{m-n}). \sum_{n=0}^{m+1}(-1)^{m+1-n}\frac{(m+1)!}{n!(m+1-n)!}A^nBA^{m+1-n}, \sum_{n=0}^{m+1}(-1)^{m+1-n}\begin{pmatrix}m+1\\n \end{pmatrix}A^nBA^{m+1-n}","['linear-algebra', 'induction', 'lie-algebras', 'quantum-mechanics']"
68,Representation theory and generalizing the determinant and permanent.,Representation theory and generalizing the determinant and permanent.,,"Let $M$ be an arbitrary $n\times n$ complex matrix and $S_n$ be the symmetric group of order $n$ . Given $\sigma\in S_n$ , define $\sigma(M)$ to be the $n\times n$ matrix with elements $[\sigma(M)]_{ij} = M_{i\sigma(j)}$ . With this, there are two well-known matrix functions (functions that take matrices as inputs) that interplay nicely: \begin{align} \text{perm}(\sigma(M)) &= \text{perm}(M), \\ \det(\sigma(M)) &= \text{sgn}(\sigma)\det(M), \end{align} where perm and $\det$ are the permanent and determinant, respectively, and $\text{sgn}(\sigma)$ is the sign of the permutation $\sigma$ . Note that these two equations have the following form: \begin{align*} f(\sigma(M)) = \rho(\sigma) f(M) \end{align*} where $f$ is some function of matrices and $\rho$ is a representation of the symmetric group. In the above examples, $\rho$ is the trivial representation (in the case of $f=\text{perm}$ ) or the sign representation (in the case of $f=\det$ ). My question is this: given a representation $\rho$ , can one find a matrix function $f$ such that $f(\sigma(M)) = \rho(\sigma)f(M)$ ? Subsequent questions regarding uniqueness follow naturally. Note that $f:\mathbb{C}^{n\times n}\to\mathbb{C}^d$ with $d$ the dimension of the representation. As a concrete example, suppose $\rho$ is the two-dimensional representation of $S_3$ . Is there a known function $f:\mathbb{C}^{3\times 3}\to \mathbb{C}^2$ (takes in matrices, outputs vectors) such that the aforementioned relationship holds? Thanks in advance!","Let be an arbitrary complex matrix and be the symmetric group of order . Given , define to be the matrix with elements . With this, there are two well-known matrix functions (functions that take matrices as inputs) that interplay nicely: where perm and are the permanent and determinant, respectively, and is the sign of the permutation . Note that these two equations have the following form: where is some function of matrices and is a representation of the symmetric group. In the above examples, is the trivial representation (in the case of ) or the sign representation (in the case of ). My question is this: given a representation , can one find a matrix function such that ? Subsequent questions regarding uniqueness follow naturally. Note that with the dimension of the representation. As a concrete example, suppose is the two-dimensional representation of . Is there a known function (takes in matrices, outputs vectors) such that the aforementioned relationship holds? Thanks in advance!","M n\times n S_n n \sigma\in S_n \sigma(M) n\times n [\sigma(M)]_{ij} = M_{i\sigma(j)} \begin{align}
\text{perm}(\sigma(M)) &= \text{perm}(M),
\\
\det(\sigma(M)) &= \text{sgn}(\sigma)\det(M),
\end{align} \det \text{sgn}(\sigma) \sigma \begin{align*}
f(\sigma(M)) = \rho(\sigma) f(M)
\end{align*} f \rho \rho f=\text{perm} f=\det \rho f f(\sigma(M)) = \rho(\sigma)f(M) f:\mathbb{C}^{n\times n}\to\mathbb{C}^d d \rho S_3 f:\mathbb{C}^{3\times 3}\to \mathbb{C}^2","['linear-algebra', 'group-theory', 'representation-theory']"
69,Calculate determinant of a continuant matrix with variable elements?,Calculate determinant of a continuant matrix with variable elements?,,"One of the problems in Muir's Treatise on determinants in exercise set XXX asks to evaluate the determinant of a continuant matrix $$ f_n(x,y)=\left|\begin{array}{cccc} x & 1 & 0 & 0 & \ldots & 0 & 0\\ y-1 & x & 2 & 0 & \ldots & 0 & 0\\ 0 & y-2 & x & 3 & \ldots & 0 & 0\\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\ 0 & 0 & 0 & \ldots & y-n+2 & x & n-1\\ 0 & 0 & 0 & \ldots & 0 & y-n+1 & x \end{array} \right|_n. $$ No answer or any hints are given. I was able to transform this determinant to \begin{equation*} (-1)^{n-1} \left|  \begin{array}{cccccc}  \ldots & n-y & y-n & n-y & y-n & n+x-1 \\ \ldots & y-n & n-y & y-n & n+x-3 & n-1 \\ \ldots & n-y & y-n & n+x-5 & n-2 & 0 \\  \ldots & y-n & n+x-7 & n-3 & 0 & 0 \\ \ldots & n+x-9 & n-4 & 0 & 0 & 0 \\  & \vdots & \vdots & \vdots & \vdots & \vdots \\ \end{array} \right|_n. \end{equation*} by a series of lengthy calculations. Of course from this I can calculate the determinant when $y=n$ , in which case it becomes diagonal. But not sure if this is the right way to proceed if $y\neq n$ and have no clue what to do next. Does anybody have any ideas? I also established the following recurrence relation (which should not be confused with the standard recurrence for a determinant of a continuant): $$ f_n(x,y)=(x+n-1)f_{n-1}(x-1,y-1)+(1-n)(y-n)f_{n-2}(x-1,y-1). $$ Again, one can see that it can be easily solved when $y=n$ . But what to do when $y\neq n$ is not clear. EDIT: It turns out that when treated as a polynomial in $x$ , the the system of functions $f_n(x,y)$ form an orthogonal polynomial system. I found that then $f_n(x,y)$ is related to Meixner polynomials (from Chihara's book on orthogonal polynomials) with $f=d=0$ and $g$ and $h$ suitably chosen. But Muir's book was written in 19-th century, well before the Meixner polynomials were discovered by Meixner in 1934. So this problem in Muir's book is really confusing. Addendum. The problem exactly how it looks in the 1960 edition of the book: It is equivalent to the formulation I gave above after a simple eqiuivalence transformation and change of notation.","One of the problems in Muir's Treatise on determinants in exercise set XXX asks to evaluate the determinant of a continuant matrix No answer or any hints are given. I was able to transform this determinant to by a series of lengthy calculations. Of course from this I can calculate the determinant when , in which case it becomes diagonal. But not sure if this is the right way to proceed if and have no clue what to do next. Does anybody have any ideas? I also established the following recurrence relation (which should not be confused with the standard recurrence for a determinant of a continuant): Again, one can see that it can be easily solved when . But what to do when is not clear. EDIT: It turns out that when treated as a polynomial in , the the system of functions form an orthogonal polynomial system. I found that then is related to Meixner polynomials (from Chihara's book on orthogonal polynomials) with and and suitably chosen. But Muir's book was written in 19-th century, well before the Meixner polynomials were discovered by Meixner in 1934. So this problem in Muir's book is really confusing. Addendum. The problem exactly how it looks in the 1960 edition of the book: It is equivalent to the formulation I gave above after a simple eqiuivalence transformation and change of notation.","
f_n(x,y)=\left|\begin{array}{cccc}
x & 1 & 0 & 0 & \ldots & 0 & 0\\
y-1 & x & 2 & 0 & \ldots & 0 & 0\\
0 & y-2 & x & 3 & \ldots & 0 & 0\\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & 0 & \ldots & y-n+2 & x & n-1\\
0 & 0 & 0 & \ldots & 0 & y-n+1 & x
\end{array}
\right|_n.
 \begin{equation*}
(-1)^{n-1} \left| 
\begin{array}{cccccc}
 \ldots & n-y & y-n & n-y & y-n & n+x-1 \\
\ldots & y-n & n-y & y-n & n+x-3 & n-1 \\
\ldots & n-y & y-n & n+x-5 & n-2 & 0 \\
 \ldots & y-n & n+x-7 & n-3 & 0 & 0 \\
\ldots & n+x-9 & n-4 & 0 & 0 & 0 \\
 & \vdots & \vdots & \vdots & \vdots & \vdots \\
\end{array}
\right|_n.
\end{equation*} y=n y\neq n 
f_n(x,y)=(x+n-1)f_{n-1}(x-1,y-1)+(1-n)(y-n)f_{n-2}(x-1,y-1).
 y=n y\neq n x f_n(x,y) f_n(x,y) f=d=0 g h","['linear-algebra', 'matrices', 'determinant', 'tridiagonal-matrices']"
70,"Is a bijective, norm-preserving and rotation-invariant mapping linear?","Is a bijective, norm-preserving and rotation-invariant mapping linear?",,"Let $h: \mathbb{S}^N\to \mathbb{S}^N$ , where $\mathbb{S}^N$ is the $N$ -dimensional unit hypersphere in $\mathbb{R}^{N+1}$ , be a mapping with the following properties: $h$ is bijective and smooth. $\left\|h(x)\right\| = 1$ (given that we operate on a hypersphere). $h(x)^\top h(y) = h(Rx)^\top h(Ry)$ for any rotation $R$ , i.e. the inner product is invariant to rotations in the domain of $h$ . Is $h$ a linear or affine transformation (i.e. is $h(x) = Qx$ where $Q$ is a rotation)? It took me a few days, but I think I finally found a proof using that if $h(x)^\top h(y) = x^\top y$ for all $x, y$ then $h$ is linear. This can be shown by realising that if $h(x)^\top h(y) \ne x^\top y$ for any $x, y$ then $h$ cannot be bijective, see the proof below.","Let , where is the -dimensional unit hypersphere in , be a mapping with the following properties: is bijective and smooth. (given that we operate on a hypersphere). for any rotation , i.e. the inner product is invariant to rotations in the domain of . Is a linear or affine transformation (i.e. is where is a rotation)? It took me a few days, but I think I finally found a proof using that if for all then is linear. This can be shown by realising that if for any then cannot be bijective, see the proof below.","h: \mathbb{S}^N\to \mathbb{S}^N \mathbb{S}^N N \mathbb{R}^{N+1} h \left\|h(x)\right\| = 1 h(x)^\top h(y) = h(Rx)^\top h(Ry) R h h h(x) = Qx Q h(x)^\top h(y) = x^\top y x, y h h(x)^\top h(y) \ne x^\top y x, y h","['linear-algebra', 'functional-analysis']"
71,Proof verification: polynomials $\mathbb R[X]$ are a vector space that is not isomorphic to its dual,Proof verification: polynomials  are a vector space that is not isomorphic to its dual,\mathbb R[X],"I have never seen an elementary proof of the fact that $V$ need not be isomorphic to $V^*$ that does not require some set theoretic background. I came up with this [most likely incorrect] argument, which does not seem to depend so much on set theoretic arguments, except for basic ideas of cardinality. I wish to have this particular proof vetted. Consider the vector space of polynomials $V \equiv \mathbb R[X]$ as an $\mathbb R$ vector space. The set $B_V \equiv \{x^i : i \in \mathbb N \}$ is a basis for the vector space $V$ . Given any polynomial $p(x) \in \mathbb R[X]$ , since the polynomial $p$ only has finitely many non-zero coefficients. Thus $p(x)$ must be of the form $p(x) = \sum_{i \in \text{nonzero-powers}(p)} a_i x^i$ where the index set $\text{nonzero-powers}(p)$ has finite cardinality. Hence we can write any polynomial $p(x)$ as a finite linear combination of elements from the set $B_V$ . Next, consider the dual space $V^* \equiv \{ f : \mathbb R[X] \rightarrow \mathbb R \mid f \text{ is a linear function}  \}$ . We have the elements $eval_r$ which evaluate a polynomial at point $r \in \mathbb R$ as elements of $V^*$ . More formally, $eval_r(p) \equiv p(r); \forall r \in \mathbb R, eval_r \in V^*$ . All the $eval_r$ are linearly independent. Intuitively, this is because we cannot pin down the value of all polynomials by evaluating them at some finite number of points. More formally, suppose that we have that $\sum_{i \in I} a_i eval_{r_i} = 0$ for some finite index set $I$ . So this gives us a way to extrapolate $eval_{i_0}$ from the other $eval_i$ . However, this is absurd, since the value of a polynomial of degree $2|I|$ is not determined by its value at $|I|$ points.  Hence all the $eval_r$ are linearly independent. This means that we have a linearly independent set $L_{V^*} \equiv \{ eval_r : r \in \mathbb R \}$ whose cardinality is that of $|\mathbb R|$ . Wrapping up, we have that the basis of $V$ , $B_V$ has cardinality $|\mathbb N|$ . A linearly independent set of $V^*$ , whose cardinality is a lower bound on the cardinality of $V^*$ , has cardinality $|\mathbb R|$ . Hence the vector spaces cannot be isomorphic since the cardinality of their bases are different. Is this correct?","I have never seen an elementary proof of the fact that need not be isomorphic to that does not require some set theoretic background. I came up with this [most likely incorrect] argument, which does not seem to depend so much on set theoretic arguments, except for basic ideas of cardinality. I wish to have this particular proof vetted. Consider the vector space of polynomials as an vector space. The set is a basis for the vector space . Given any polynomial , since the polynomial only has finitely many non-zero coefficients. Thus must be of the form where the index set has finite cardinality. Hence we can write any polynomial as a finite linear combination of elements from the set . Next, consider the dual space . We have the elements which evaluate a polynomial at point as elements of . More formally, . All the are linearly independent. Intuitively, this is because we cannot pin down the value of all polynomials by evaluating them at some finite number of points. More formally, suppose that we have that for some finite index set . So this gives us a way to extrapolate from the other . However, this is absurd, since the value of a polynomial of degree is not determined by its value at points.  Hence all the are linearly independent. This means that we have a linearly independent set whose cardinality is that of . Wrapping up, we have that the basis of , has cardinality . A linearly independent set of , whose cardinality is a lower bound on the cardinality of , has cardinality . Hence the vector spaces cannot be isomorphic since the cardinality of their bases are different. Is this correct?","V V^* V \equiv \mathbb R[X] \mathbb R B_V \equiv \{x^i : i \in \mathbb N \} V p(x) \in \mathbb R[X] p p(x) p(x) = \sum_{i \in \text{nonzero-powers}(p)} a_i x^i \text{nonzero-powers}(p) p(x) B_V V^* \equiv \{ f : \mathbb R[X] \rightarrow \mathbb R \mid f \text{ is a linear function}  \} eval_r r \in \mathbb R V^* eval_r(p) \equiv p(r); \forall r \in \mathbb R, eval_r \in V^* eval_r \sum_{i \in I} a_i eval_{r_i} = 0 I eval_{i_0} eval_i 2|I| |I| eval_r L_{V^*} \equiv \{ eval_r : r \in \mathbb R \} |\mathbb R| V B_V |\mathbb N| V^* V^* |\mathbb R|","['linear-algebra', 'vector-spaces', 'solution-verification']"
72,Does there exist $X$ such that $A = X^2 + X^t$?,Does there exist  such that ?,X A = X^2 + X^t,"If $A \in M_n( \mathbb{R} )$ , then when does there exist $X \in M_n(\mathbb{R})$ such that $A = X^2 +  X^T$ ?","If , then when does there exist such that ?",A \in M_n( \mathbb{R} ) X \in M_n(\mathbb{R}) A = X^2 +  X^T,"['linear-algebra', 'matrices', 'matrix-equations']"
73,Inequality involving inner product and norm,Inequality involving inner product and norm,,"If $\|\cdot \|$ is the norm induced by the inner product $\langle,\rangle$ , how to prove the following interesting inequality? $$\langle x,y\rangle(\|x\|+\|y\|) \leq\|x+y\|\,\|x\|\,\|y\|$$ This is a exercise in my textbook which is available only in portuguese called ""Topologia e Análise no Espaço $\mathbb{R}^n$ "". The above inequality is obvious when $\langle x,y\rangle \leq 0$ , but I don't know how to proceed to prove the other case.","If is the norm induced by the inner product , how to prove the following interesting inequality? This is a exercise in my textbook which is available only in portuguese called ""Topologia e Análise no Espaço "". The above inequality is obvious when , but I don't know how to proceed to prove the other case.","\|\cdot \| \langle,\rangle \langle x,y\rangle(\|x\|+\|y\|) \leq\|x+y\|\,\|x\|\,\|y\| \mathbb{R}^n \langle x,y\rangle \leq 0","['linear-algebra', 'inner-products']"
74,$Ax=b$ has a solution over $\Bbb{F}_p$ for every prime $p~\implies$ existence of real solution??,has a solution over  for every prime  existence of real solution??,Ax=b \Bbb{F}_p p~\implies,"$A$ be an $m\times n$ matrix and $b$ a $m\times1$ vector, both with integer entries. If $Ax=b$ has a solution over $\Bbb{F}_p$ for every prime $p$, is a real solution guaranteed? I couldn't think of any easy way to begin this problem. Any hint please. Bonus question: A matrix with all diagonal elements odd integers and all non diagonal elements even integers, is the identity matrix in $\Bbb{F}_2$. Does that imply it is invertible as a real matrix?","$A$ be an $m\times n$ matrix and $b$ a $m\times1$ vector, both with integer entries. If $Ax=b$ has a solution over $\Bbb{F}_p$ for every prime $p$, is a real solution guaranteed? I couldn't think of any easy way to begin this problem. Any hint please. Bonus question: A matrix with all diagonal elements odd integers and all non diagonal elements even integers, is the identity matrix in $\Bbb{F}_2$. Does that imply it is invertible as a real matrix?",,"['linear-algebra', 'matrices']"
75,What does exterior algebra actually mean?,What does exterior algebra actually mean?,,"This question may be too basic and even silly, but I am new to exterior algebra and reading Wikipedia . Given $e_1, e_2,\cdots, e_n$ is a standard basis for a vector space $V$, what does $e_1\wedge e_2\cdots \wedge e_n$ actually look like? It is said to be a basis. So, it is a set of vectors? What is its dimension? Is it just a constant with dimension $1$?","This question may be too basic and even silly, but I am new to exterior algebra and reading Wikipedia . Given $e_1, e_2,\cdots, e_n$ is a standard basis for a vector space $V$, what does $e_1\wedge e_2\cdots \wedge e_n$ actually look like? It is said to be a basis. So, it is a set of vectors? What is its dimension? Is it just a constant with dimension $1$?",,['linear-algebra']
76,Finding subspaces with trivial intersection,Finding subspaces with trivial intersection,,Let $V$ be a $n$-dimensional real vector space and $P$ and $P'$ be subspaces with dimension $k$. I need to prove that it is possible to find a subspace $Q$ of dimension $n-k$ such that $Q$ has trivial intersection with both $P$ and $P'$. I tried to manipulate the bases but cannot find a way... Could anyone please help me?,Let $V$ be a $n$-dimensional real vector space and $P$ and $P'$ be subspaces with dimension $k$. I need to prove that it is possible to find a subspace $Q$ of dimension $n-k$ such that $Q$ has trivial intersection with both $P$ and $P'$. I tried to manipulate the bases but cannot find a way... Could anyone please help me?,,"['linear-algebra', 'vector-spaces', 'direct-sum']"
77,Product between a column vector and a row vector,Product between a column vector and a row vector,,"I know that matrices product is correct when the number of the columns of the first matrix is equal to the number of rows of the second matrix. Why I can't do the product between a column vector and a row vector? For example: $$\begin{bmatrix}1 \\ 2 \\ 3 \end{bmatrix} \, \begin{bmatrix}1 & 2 & 3\end{bmatrix}$$ Thank you so much.","I know that matrices product is correct when the number of the columns of the first matrix is equal to the number of rows of the second matrix. Why I can't do the product between a column vector and a row vector? For example: $$\begin{bmatrix}1 \\ 2 \\ 3 \end{bmatrix} \, \begin{bmatrix}1 & 2 & 3\end{bmatrix}$$ Thank you so much.",,"['linear-algebra', 'matrices']"
78,"Prove that the set of all real valued function on [a,b] is a vector space","Prove that the set of all real valued function on [a,b] is a vector space",,"Question - Show that the set of all real valued functions on [a,b] , $\mathrm F $[a,b] under usual addition and scalar multiplication is a vector space. What I did - let  $\mathrm L = \{ F:[a,b] \; | \; a,b \in \mathbb R \} \; $ & $\; \mathrm u, v \in L$ s.t $\mathrm u = f[a,b] \; \& \;  v= g[a,b] $ and after that I showed that the 10 axioms do satisfy under these conditions However , my instructor has marked it all wrong and she has highlighted that I have started the problem in the wrong way. She also have added the correct method , which is as follows ; $\mathrm L = \{ F:[a,b]\rightarrow \mathbb R^{2} \; | \; a,b \in \mathbb R \} \; $ let  $\; \mathrm x,y \in [a,b]$ Take any$ \,\mathrm F_1$ & $ \, \mathrm F_2$ in $\mathrm L \, $ s.t $\mathrm F_1(x,y) \in \mathbb R^{2} \; \& \;  F_2(x,y) \in \mathbb R^{2} $ Now Im really confused  because: 1.what is wrong with my approach ? , what is my mistake ? 2.why my instructor have used functions defined on (x,y) ? I think it should be [a,b] [please look into this photo to see if I have missed something else","Question - Show that the set of all real valued functions on [a,b] , $\mathrm F $[a,b] under usual addition and scalar multiplication is a vector space. What I did - let  $\mathrm L = \{ F:[a,b] \; | \; a,b \in \mathbb R \} \; $ & $\; \mathrm u, v \in L$ s.t $\mathrm u = f[a,b] \; \& \;  v= g[a,b] $ and after that I showed that the 10 axioms do satisfy under these conditions However , my instructor has marked it all wrong and she has highlighted that I have started the problem in the wrong way. She also have added the correct method , which is as follows ; $\mathrm L = \{ F:[a,b]\rightarrow \mathbb R^{2} \; | \; a,b \in \mathbb R \} \; $ let  $\; \mathrm x,y \in [a,b]$ Take any$ \,\mathrm F_1$ & $ \, \mathrm F_2$ in $\mathrm L \, $ s.t $\mathrm F_1(x,y) \in \mathbb R^{2} \; \& \;  F_2(x,y) \in \mathbb R^{2} $ Now Im really confused  because: 1.what is wrong with my approach ? , what is my mistake ? 2.why my instructor have used functions defined on (x,y) ? I think it should be [a,b] [please look into this photo to see if I have missed something else",,"['linear-algebra', 'vector-spaces']"
79,"Is self-adjointness really a property of an operator, or of an operator and an inner product?","Is self-adjointness really a property of an operator, or of an operator and an inner product?",,"On a Hilbert- (or otherwise inner-product-) space $\mathcal{H}$ with scalar product $S = \langle.|.\rangle_\mathcal{H}$, a self-adjoint operator is is readily defined as a linear mapping $A : \mathcal{H} \to \mathcal{H}$ with the property $$   S (A v, w) = S(v,A w) \quad \forall v,w\in \mathcal{H}. $$ Fair enough, but an inner product is a pretty specific structure on a space. Do we need it to define what self-adjoint means? It seems that an operator which is self-adjoint with respect to $S$ is also self-adjoint with respect to another scalar product $T$, but I really don't see how one could go about proving this. If so, would there be a definition of self-adjointness that makes no reference to any particular inner product? A useful candidate would be something like “a self-adjoint operator is one that has a system of eigenvectors $(\psi_i)_i$ which spans the entire space, such that $A(\psi_i) = \lambda_i\cdot \psi_i$ with real $\lambda$”. But is that actually equivalent to the usually given definition? The spectral theorem only goes one way, and makes itself reference to an inner product.","On a Hilbert- (or otherwise inner-product-) space $\mathcal{H}$ with scalar product $S = \langle.|.\rangle_\mathcal{H}$, a self-adjoint operator is is readily defined as a linear mapping $A : \mathcal{H} \to \mathcal{H}$ with the property $$   S (A v, w) = S(v,A w) \quad \forall v,w\in \mathcal{H}. $$ Fair enough, but an inner product is a pretty specific structure on a space. Do we need it to define what self-adjoint means? It seems that an operator which is self-adjoint with respect to $S$ is also self-adjoint with respect to another scalar product $T$, but I really don't see how one could go about proving this. If so, would there be a definition of self-adjointness that makes no reference to any particular inner product? A useful candidate would be something like “a self-adjoint operator is one that has a system of eigenvectors $(\psi_i)_i$ which spans the entire space, such that $A(\psi_i) = \lambda_i\cdot \psi_i$ with real $\lambda$”. But is that actually equivalent to the usually given definition? The spectral theorem only goes one way, and makes itself reference to an inner product.",,"['linear-algebra', 'functional-analysis', 'hilbert-spaces', 'linear-transformations']"
80,Can you get a basis for an infinite direct product of vector spaces from a basis for each factor?,Can you get a basis for an infinite direct product of vector spaces from a basis for each factor?,,"If $\{V_i\}_{i\in I}$ is a family of vector spaces over $F$ with basis $B_i$ for each $V_i$, then there is a vector space $\prod_i V_i$ over $F$, called the direct product of $V_i$'s; its definition involves a certain universal property in terms from projections from it onto $V_i$'s (see this wiki ) Since every vector space has a basis, $\prod_i V_i$ has so. Q. Can we obtain basis a of $\prod_i V_i$ from given basis $B_i$ of each $V_i$? I am not too familiar with Category theory; please explain in as elementary fashion as you can, so that this will be also accessible to undergraduates; I want to explain this in my Linear Algebra course to undergraduates, and my aim is to introduce maximum number of advanced concepts of other areas of mathematics from starting point in Linear Algebra.","If $\{V_i\}_{i\in I}$ is a family of vector spaces over $F$ with basis $B_i$ for each $V_i$, then there is a vector space $\prod_i V_i$ over $F$, called the direct product of $V_i$'s; its definition involves a certain universal property in terms from projections from it onto $V_i$'s (see this wiki ) Since every vector space has a basis, $\prod_i V_i$ has so. Q. Can we obtain basis a of $\prod_i V_i$ from given basis $B_i$ of each $V_i$? I am not too familiar with Category theory; please explain in as elementary fashion as you can, so that this will be also accessible to undergraduates; I want to explain this in my Linear Algebra course to undergraduates, and my aim is to introduce maximum number of advanced concepts of other areas of mathematics from starting point in Linear Algebra.",,"['linear-algebra', 'vector-spaces', 'direct-product']"
81,Prove that $T$ has a cyclic vector iff its minimal and characteristic polynomials are the same,Prove that  has a cyclic vector iff its minimal and characteristic polynomials are the same,T,"Let $k$ be an algebraically closed field and $V$ be a finite-dimensional $k$-vector space of dimension $n$. Let $T:V \rightarrow V$ be a $k$-linear endomorphism of $V$. A vector $v \in V$ is called a cyclic vector for $T$ if the set of vectors $\{T^nv: n \in \mathbb{Z}, n \geqslant 0\}$ span $V$. 1 Show that if $v \in V$ is a cyclic vector, then $\{v, Tv,\cdots, T^{n-1}v\}$ form a basis for $V$. 2 If $T$ admits a cyclic vector, and $A:V\rightarrow V$ is a linear map commuting with $T$, show that there exists a polynomial $P(x) \in k[x]$ such that $A=P(T)$. 3 Show that a cyclic vector for $T$ exists if and only if the minimal polynomial of $T$ is equal to the characteristic polynomial of $T$.","Let $k$ be an algebraically closed field and $V$ be a finite-dimensional $k$-vector space of dimension $n$. Let $T:V \rightarrow V$ be a $k$-linear endomorphism of $V$. A vector $v \in V$ is called a cyclic vector for $T$ if the set of vectors $\{T^nv: n \in \mathbb{Z}, n \geqslant 0\}$ span $V$. 1 Show that if $v \in V$ is a cyclic vector, then $\{v, Tv,\cdots, T^{n-1}v\}$ form a basis for $V$. 2 If $T$ admits a cyclic vector, and $A:V\rightarrow V$ is a linear map commuting with $T$, show that there exists a polynomial $P(x) \in k[x]$ such that $A=P(T)$. 3 Show that a cyclic vector for $T$ exists if and only if the minimal polynomial of $T$ is equal to the characteristic polynomial of $T$.",,"['linear-algebra', 'field-theory', 'finite-fields', 'vector-fields']"
82,"Rational numbers as vectors in infinite dimensional space with the basis $( \log 2,\log 3, \log 5, \log 7, \dots, \log p, \dots) $",Rational numbers as vectors in infinite dimensional space with the basis,"( \log 2,\log 3, \log 5, \log 7, \dots, \log p, \dots) ","Since every natural number can be represented as $a=2^{n_1}3^{n_2}5^{n_3}7^{n_4}\cdots p_k^{n_k}\cdots$ it makes sense to represent natural numbers by vectors, using the properties of logarithms: $$\log a=n_1 \log 2+n_2 \log3+n_3 \log5+\cdots$$ This space appears to be similar to the usual Euclidean space if we extend it to an infinite number of dimensions. If we allow negative coordinates, we can also put all rational numbers in this space. For example, here is part of the plane $(\log 2, \log 3)$: Does this space have any application in number theory? If it is studied, then how is it usually defined? Are the usual Cartesian vector dot product and the usual Euclidean norm used? Or does it make sense to use a different norm (for example, taxicab norm)?","Since every natural number can be represented as $a=2^{n_1}3^{n_2}5^{n_3}7^{n_4}\cdots p_k^{n_k}\cdots$ it makes sense to represent natural numbers by vectors, using the properties of logarithms: $$\log a=n_1 \log 2+n_2 \log3+n_3 \log5+\cdots$$ This space appears to be similar to the usual Euclidean space if we extend it to an infinite number of dimensions. If we allow negative coordinates, we can also put all rational numbers in this space. For example, here is part of the plane $(\log 2, \log 3)$: Does this space have any application in number theory? If it is studied, then how is it usually defined? Are the usual Cartesian vector dot product and the usual Euclidean norm used? Or does it make sense to use a different norm (for example, taxicab norm)?",,"['linear-algebra', 'number-theory', 'vector-spaces', 'rational-numbers']"
83,"Given a symmetric positive-definite matrix $M$, find all $A$ such that $A^\top M A=M$","Given a symmetric positive-definite matrix , find all  such that",M A A^\top M A=M,"Given $M$ a real symmetric positive-definite matrix, I would like to characterise all matrices $A$ such that $A^\top M A=M$. Note that the question of finding $A$ solutions to $A^\top M A=M$ for all positive-definite matrices $M$ has already been answered here . Necessarily $\det(A)^2=1$. Reducing the problem to diagonal p.q. matrices $M$ is symmetric, so diagonalisable in an orthonormal basis: $M=Q^\top D Q$ with $D$ diagonal and $QQ^\top=I$. Then, \begin{align} &A^\top Q^\top D Q A=Q^\top D Q \\ &(QA)^\top D(QA)=Q^\top D Q \\ &(QAQ^\top)^\top D (QAQ^\top)=D \end{align} so I think the problem can be simplified in $$\text{Find the matrices $B$ such that $B^\top D B=D$}$$   where $D=\operatorname{diag}(d_1,\dots,d_n)$, $d_i>0$. and then recover $A$ from $A=Q^\top B Q$ ($Q$ and $D$ are known since $M$ is given). The $d_i>0$ stems for $M$ positive-definite. Trying to solve this problem The $2^n$ matrices $B=\operatorname{diag}(\pm 1,\dots, \pm 1)$ are obvious solutions. What about the other ones? Writing $B^\top D B=D$ in components, the term $(i,j)$ is given by $$\sum_k B_{k,i}d_k B_{k,j}=d_i\delta_{ij}$$ In particular,  $$\sum_k B_{k,i}^2d_k =d_i.$$ However, I don't see any clear conclusions using these equalities. Question : How can to find/characterise the $B$ such that $B^\top D B=D$?","Given $M$ a real symmetric positive-definite matrix, I would like to characterise all matrices $A$ such that $A^\top M A=M$. Note that the question of finding $A$ solutions to $A^\top M A=M$ for all positive-definite matrices $M$ has already been answered here . Necessarily $\det(A)^2=1$. Reducing the problem to diagonal p.q. matrices $M$ is symmetric, so diagonalisable in an orthonormal basis: $M=Q^\top D Q$ with $D$ diagonal and $QQ^\top=I$. Then, \begin{align} &A^\top Q^\top D Q A=Q^\top D Q \\ &(QA)^\top D(QA)=Q^\top D Q \\ &(QAQ^\top)^\top D (QAQ^\top)=D \end{align} so I think the problem can be simplified in $$\text{Find the matrices $B$ such that $B^\top D B=D$}$$   where $D=\operatorname{diag}(d_1,\dots,d_n)$, $d_i>0$. and then recover $A$ from $A=Q^\top B Q$ ($Q$ and $D$ are known since $M$ is given). The $d_i>0$ stems for $M$ positive-definite. Trying to solve this problem The $2^n$ matrices $B=\operatorname{diag}(\pm 1,\dots, \pm 1)$ are obvious solutions. What about the other ones? Writing $B^\top D B=D$ in components, the term $(i,j)$ is given by $$\sum_k B_{k,i}d_k B_{k,j}=d_i\delta_{ij}$$ In particular,  $$\sum_k B_{k,i}^2d_k =d_i.$$ However, I don't see any clear conclusions using these equalities. Question : How can to find/characterise the $B$ such that $B^\top D B=D$?",,"['linear-algebra', 'matrices', 'matrix-equations']"
84,When exponential of a matrix is diagonal?,When exponential of a matrix is diagonal?,,"$\newcommand{\C}{\mathbb{C}}$ $\newcommand{\R}{\mathbb{R}}$ $\newcommand{\ga}{\gamma}$ $\newcommand{\al}{\alpha}$ Let $A$ be an $n \times n$ real matrix. Assume $e^A$ is a diagonal matrix. Does this imply $A$ is diagonal? If not, then for which matrices, their exponential is diagonal? Can we obtain some nice characterization? (The complex case might also be interesting) Update: Define $G=\C^* = \C \setminus \{0\}$ to be the group of nonzero comlplex numbers, with multiplication. Let $H=\{\begin{pmatrix}a&b\\-b&a\end{pmatrix}|a,b \in \R \, , \, a^2+b^2 \neq 0 \}$ be a group of $2 \times 2$ real matrices (with the operation of matrix multiplication). Look at the following group isomorphism: $\phi:G \to H, \phi(a+ib)=aI+bJ$, where $I=\begin{pmatrix}1&0\\0&1\end{pmatrix} \, , \, J = \begin{pmatrix}0&1\\-1&0\end{pmatrix}$ By the theory of Lie groups, we know: $$(1): \, \,  \phi(\exp^G(v))=\exp^H(\phi_*(v))$$ Since $H$ is a subgroup of $GL_2(\R)$ $\exp^H$ is just the usual matrix exponential. Note that $T_eG \cong \C$, We claim, $\exp^G(z)=e^z$ (where $e^z$  is the standard complex exponential). Proof: Let $v \in T_eG = \C$. Define $\ga:I \to \C^*$, $\ga(t)=e^{tv}$. Then $\ga$ satisfies $\ga(0)=1,\dot \ga (0) = v, \ga(t+s)=\ga(t)\cdot\ga(s)$, so $\ga$ is a one-parameter subgroup in $G=\C^*$ with initial velocity $v$. By definition, $\exp^G(v)=\ga(1)=e^v$, as required. Hence, equation $(1)$ becomes $(1'):$ $$(1'): \, \,  \phi(e^v)=\exp(\phi_*(v))$$ Let us calculate $\phi^*=(d\phi)_e:T_eG \to T_IH \subset M_2$. Let $v=x+iy \in T_eG=\C$. Define $\al(t)=1+tv=(1+tx)+i(ty),\dot \al(0)=v$, and $$\phi^*(v)=(d\phi)_e(v)=\frac{d}{dt}\big(\phi(\al(t))\big)|_{t=0}= \frac{d}{dt}\big( \begin{pmatrix}1+tx&ty\\-ty&1+tx\end{pmatrix}\big)|_{t=0}=\begin{pmatrix}x&y\\-y&x\end{pmatrix}$$ So, Hence, equation $(1')$ becomes $(1''):$ $$(1''): \, \,  \phi(e^{(x+iy)})=\exp(\begin{pmatrix}x&y\\-y&x\end{pmatrix})$$, Finally, since $\phi(e^{(x+iy)})=\phi(e^x\cos y+e^x\sin y)=\begin{pmatrix}e^x\cos y&e^x\sin y\\-e^x\sin y&e^x\cos y\end{pmatrix} $ we get the following formula: $$\exp(\begin{pmatrix}x&y\\-y&x\end{pmatrix}) = \begin{pmatrix}e^x\cos y&e^x\sin y\\-e^x\sin y&e^x\cos y\end{pmatrix} $$ In particualr, taking $x=0,y=t$ we get: $$\exp(\begin{pmatrix}0&t\\-t&0\end{pmatrix}) = \begin{pmatrix}\cos t&\sin t\\-\sin t&\cos t\end{pmatrix} $$","$\newcommand{\C}{\mathbb{C}}$ $\newcommand{\R}{\mathbb{R}}$ $\newcommand{\ga}{\gamma}$ $\newcommand{\al}{\alpha}$ Let $A$ be an $n \times n$ real matrix. Assume $e^A$ is a diagonal matrix. Does this imply $A$ is diagonal? If not, then for which matrices, their exponential is diagonal? Can we obtain some nice characterization? (The complex case might also be interesting) Update: Define $G=\C^* = \C \setminus \{0\}$ to be the group of nonzero comlplex numbers, with multiplication. Let $H=\{\begin{pmatrix}a&b\\-b&a\end{pmatrix}|a,b \in \R \, , \, a^2+b^2 \neq 0 \}$ be a group of $2 \times 2$ real matrices (with the operation of matrix multiplication). Look at the following group isomorphism: $\phi:G \to H, \phi(a+ib)=aI+bJ$, where $I=\begin{pmatrix}1&0\\0&1\end{pmatrix} \, , \, J = \begin{pmatrix}0&1\\-1&0\end{pmatrix}$ By the theory of Lie groups, we know: $$(1): \, \,  \phi(\exp^G(v))=\exp^H(\phi_*(v))$$ Since $H$ is a subgroup of $GL_2(\R)$ $\exp^H$ is just the usual matrix exponential. Note that $T_eG \cong \C$, We claim, $\exp^G(z)=e^z$ (where $e^z$  is the standard complex exponential). Proof: Let $v \in T_eG = \C$. Define $\ga:I \to \C^*$, $\ga(t)=e^{tv}$. Then $\ga$ satisfies $\ga(0)=1,\dot \ga (0) = v, \ga(t+s)=\ga(t)\cdot\ga(s)$, so $\ga$ is a one-parameter subgroup in $G=\C^*$ with initial velocity $v$. By definition, $\exp^G(v)=\ga(1)=e^v$, as required. Hence, equation $(1)$ becomes $(1'):$ $$(1'): \, \,  \phi(e^v)=\exp(\phi_*(v))$$ Let us calculate $\phi^*=(d\phi)_e:T_eG \to T_IH \subset M_2$. Let $v=x+iy \in T_eG=\C$. Define $\al(t)=1+tv=(1+tx)+i(ty),\dot \al(0)=v$, and $$\phi^*(v)=(d\phi)_e(v)=\frac{d}{dt}\big(\phi(\al(t))\big)|_{t=0}= \frac{d}{dt}\big( \begin{pmatrix}1+tx&ty\\-ty&1+tx\end{pmatrix}\big)|_{t=0}=\begin{pmatrix}x&y\\-y&x\end{pmatrix}$$ So, Hence, equation $(1')$ becomes $(1''):$ $$(1''): \, \,  \phi(e^{(x+iy)})=\exp(\begin{pmatrix}x&y\\-y&x\end{pmatrix})$$, Finally, since $\phi(e^{(x+iy)})=\phi(e^x\cos y+e^x\sin y)=\begin{pmatrix}e^x\cos y&e^x\sin y\\-e^x\sin y&e^x\cos y\end{pmatrix} $ we get the following formula: $$\exp(\begin{pmatrix}x&y\\-y&x\end{pmatrix}) = \begin{pmatrix}e^x\cos y&e^x\sin y\\-e^x\sin y&e^x\cos y\end{pmatrix} $$ In particualr, taking $x=0,y=t$ we get: $$\exp(\begin{pmatrix}0&t\\-t&0\end{pmatrix}) = \begin{pmatrix}\cos t&\sin t\\-\sin t&\cos t\end{pmatrix} $$",,"['linear-algebra', 'matrices']"
85,"Determining matrix $A$ and $B$, rectangular matrix","Determining matrix  and , rectangular matrix",A B,Let $A$ be a $3\times 2$ matrix and $B$ be a $2\times 3$ be matrices satisfying $$AB=\begin{pmatrix} 8 & 2 & -2\\ 2 & 5 & 4\\ -2 & 4 & 5\end{pmatrix}$$ Calculate $BA$. How would you go for this problem? Do we start by noticing the matrix is symmetric? Any hints/ideas? Thanks,Let $A$ be a $3\times 2$ matrix and $B$ be a $2\times 3$ be matrices satisfying $$AB=\begin{pmatrix} 8 & 2 & -2\\ 2 & 5 & 4\\ -2 & 4 & 5\end{pmatrix}$$ Calculate $BA$. How would you go for this problem? Do we start by noticing the matrix is symmetric? Any hints/ideas? Thanks,,"['linear-algebra', 'matrices']"
86,Is there a name for this type of vector norm?,Is there a name for this type of vector norm?,,"In the case of the $\mathcal{l}_2$ norm we have, $$||\mathbf{x}||_2^2=\mathbf{x}^T\mathbf{x}.$$ I was wondering if there was a type of norm that had a linear operation embedded in it, like this, $$||\mathbf{x}||^2_A=\mathbf{x}^T A \mathbf{x},$$ where A is a real matrix.","In the case of the $\mathcal{l}_2$ norm we have, $$||\mathbf{x}||_2^2=\mathbf{x}^T\mathbf{x}.$$ I was wondering if there was a type of norm that had a linear operation embedded in it, like this, $$||\mathbf{x}||^2_A=\mathbf{x}^T A \mathbf{x},$$ where A is a real matrix.",,"['linear-algebra', 'matrices', 'normed-spaces', 'inner-products']"
87,Prove that A(AB-BA) = (AB-BA)A implies AB-BA is nilpotent.,Prove that A(AB-BA) = (AB-BA)A implies AB-BA is nilpotent.,,"Let A and B be $n \times n$ complex matrices such that  $A(AB-BA) = (AB-BA)A$ a) Show that for every positive integer $k$, the matrix $(AB-BA)^k$ is of the form $AC-CA$, where $C$ is an $n \times n$ complex matrix. b) Prove that $AB-BA$ is nilpotent. I have tried the following. $A^{-1}A(AB-BA) = A^{-1}(AB-BA)A \implies AB-BA = A^{-1}(AB-BA)A$ $A(AB-BA)A^{-1} = (AB-BA)AA^{-1} \implies AB-BA = A(AB-BA)A^{-1}$ Then $(AB-BA)^{k} = (A^{-1}(AB-BA)A)^k = A^{-1}(AB-BA)^kA$ and $(AB-BA)^{k} = A(AB-BA)^{k}A^{-1}$. I don't know where do I go from here, thanks for your help.","Let A and B be $n \times n$ complex matrices such that  $A(AB-BA) = (AB-BA)A$ a) Show that for every positive integer $k$, the matrix $(AB-BA)^k$ is of the form $AC-CA$, where $C$ is an $n \times n$ complex matrix. b) Prove that $AB-BA$ is nilpotent. I have tried the following. $A^{-1}A(AB-BA) = A^{-1}(AB-BA)A \implies AB-BA = A^{-1}(AB-BA)A$ $A(AB-BA)A^{-1} = (AB-BA)AA^{-1} \implies AB-BA = A(AB-BA)A^{-1}$ Then $(AB-BA)^{k} = (A^{-1}(AB-BA)A)^k = A^{-1}(AB-BA)^kA$ and $(AB-BA)^{k} = A(AB-BA)^{k}A^{-1}$. I don't know where do I go from here, thanks for your help.",,['linear-algebra']
88,Uniqueness of Duals in a Monoidal Category,Uniqueness of Duals in a Monoidal Category,,"Given a monoidal category ${\cal C}$, and $X \in {\cal C}$, we define a left dual of $X$ to be an object $X^*$ such that there exist morphisms $\epsilon:X^* \otimes X \to I$, and $\eta:I \to X \otimes X^*$, for $I$ the identity of the category, satisfying certain axioms, see here for details. When is the left of a dual unique up to isomorphism, and when is this isomorphism unique.","Given a monoidal category ${\cal C}$, and $X \in {\cal C}$, we define a left dual of $X$ to be an object $X^*$ such that there exist morphisms $\epsilon:X^* \otimes X \to I$, and $\eta:I \to X \otimes X^*$, for $I$ the identity of the category, satisfying certain axioms, see here for details. When is the left of a dual unique up to isomorphism, and when is this isomorphism unique.",,"['linear-algebra', 'abstract-algebra', 'category-theory', 'monoidal-categories']"
89,Sparsest matrix with full inverse,Sparsest matrix with full inverse,,"What is the sparsest matrix in $\mathbb  R^{n,n}$ such that the inverse is full? I.e. I am looking for a matrix $A\in \mathbb R^{n,n}$ with as few non-zero entries as possible, such that $A^{-1}$ has no zeros. The best I could find was $$ A = \begin{pmatrix} 1 & 1 & 0 & \cdots & 0 & 1 \\ 0 & 1 & 1 & 0 & \cdots & 0 \\ \vdots &  & \ddots & \ddots & \ddots & \vdots \\ 0 & 0 & \cdots & 0 & 1 & 1 \end{pmatrix}\in \mathbb R^{n,n}, $$ which has $2n$ non-zero entries. Its inverse is $$ A^{-1} =\frac12 \begin{pmatrix} 1 & 1 & -1 & \cdots & -1 & 1 \\ -1 & 1 & 1 & -1 & \cdots & -1 \\ \vdots &  & \ddots & \ddots & \ddots & \vdots \\ \vdots &  & \ddots & \ddots & \ddots & \vdots \\   & \cdots & 1 & -1 & 1 & 1 \end{pmatrix}\in \mathbb R^{n,n}. $$ Both matrices are Toeplitz matrices, I am not that great on tex-ing large matrices. Is there a matrix of with these properties that has less than $2n$ non-zero entries. If not how could one prove that $2n$ is the best possible?","What is the sparsest matrix in $\mathbb  R^{n,n}$ such that the inverse is full? I.e. I am looking for a matrix $A\in \mathbb R^{n,n}$ with as few non-zero entries as possible, such that $A^{-1}$ has no zeros. The best I could find was $$ A = \begin{pmatrix} 1 & 1 & 0 & \cdots & 0 & 1 \\ 0 & 1 & 1 & 0 & \cdots & 0 \\ \vdots &  & \ddots & \ddots & \ddots & \vdots \\ 0 & 0 & \cdots & 0 & 1 & 1 \end{pmatrix}\in \mathbb R^{n,n}, $$ which has $2n$ non-zero entries. Its inverse is $$ A^{-1} =\frac12 \begin{pmatrix} 1 & 1 & -1 & \cdots & -1 & 1 \\ -1 & 1 & 1 & -1 & \cdots & -1 \\ \vdots &  & \ddots & \ddots & \ddots & \vdots \\ \vdots &  & \ddots & \ddots & \ddots & \vdots \\   & \cdots & 1 & -1 & 1 & 1 \end{pmatrix}\in \mathbb R^{n,n}. $$ Both matrices are Toeplitz matrices, I am not that great on tex-ing large matrices. Is there a matrix of with these properties that has less than $2n$ non-zero entries. If not how could one prove that $2n$ is the best possible?",,"['linear-algebra', 'matrices', 'recreational-mathematics']"
90,Isomorphisms between a vector space and its dual,Isomorphisms between a vector space and its dual,,"For finite dimensional vector spaces $V$ and $W$, let $i_V: V \rightarrow V^{**}$ and $i_W: W \rightarrow W^{**}$ be natural isomorphisms. Show that for any linear transformation $f : V \rightarrow W$, $i_W\circ f = f^{**}\circ i_V$. And in addition, show that there do not exist isomorphisms $i_V : V \rightarrow V^*$ and $i_W : W \rightarrow W^*$ such that for any linear transformation $f: V\rightarrow W$, such that $f^{*}\circ i_W\circ f = i_V$. Questions: For the first part of the question, are $f$ and $f^{**}$ related in any way? Similarly, for the second part of the question, are $f$ and $f^*$ related? In addition, can I assume a basis for $V$ and a basis for $W$ to solve these questions? Any hints? For the second part, where to start? Update: Part $1$: For an arbitrary element $v\in V$, the left hand side is $i_W[f(v)\in W]\in W^{**}$. That is, for an arbitrary $\psi\in W^*$, $i_W[f(v)\in W](\psi) =\psi[f(v)] \in\mathbb R$. Now consider the right hand side, for an arbitrary $\phi\in V^*$, $i_V(v)(\phi)=\phi(v)\in V^{**}$; then $f^{**}[i_V(v)]\in W^{**}.$ Hence, $f^{**}[i_V(v)](\psi)= i_V(v)[f^*(\psi)\in V^*]=f^*(\psi)(v)\in\mathbb R.$ I should now have $f^*(\psi)(v) = \psi[f(v)] $ simply by definition of $f^*$. Part $2$: Assume there exist the two isomorphism $i_V$ and $i_W$. Let $V=W=\mathbb R^2$ and set $f$ to be $\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}.$ Consider the left hand side, for an arbitrary $(a, b)\in \mathbb R^2$, $f(a, b) = (b, a)$. Then $i_W(b, a)\in W^*$ and $f^*[i_W(b, a)](a, b) = i_W(b, a)[f(a,b)] = i_W(b, a)(b, a)\in\mathbb R$. As for the right hand side, $i_V(a, b)(a,b)\in\mathbb R$. I should now have $i_V(a, b)(a,b)\neq i_W(b, a)(b, a)$, but how?","For finite dimensional vector spaces $V$ and $W$, let $i_V: V \rightarrow V^{**}$ and $i_W: W \rightarrow W^{**}$ be natural isomorphisms. Show that for any linear transformation $f : V \rightarrow W$, $i_W\circ f = f^{**}\circ i_V$. And in addition, show that there do not exist isomorphisms $i_V : V \rightarrow V^*$ and $i_W : W \rightarrow W^*$ such that for any linear transformation $f: V\rightarrow W$, such that $f^{*}\circ i_W\circ f = i_V$. Questions: For the first part of the question, are $f$ and $f^{**}$ related in any way? Similarly, for the second part of the question, are $f$ and $f^*$ related? In addition, can I assume a basis for $V$ and a basis for $W$ to solve these questions? Any hints? For the second part, where to start? Update: Part $1$: For an arbitrary element $v\in V$, the left hand side is $i_W[f(v)\in W]\in W^{**}$. That is, for an arbitrary $\psi\in W^*$, $i_W[f(v)\in W](\psi) =\psi[f(v)] \in\mathbb R$. Now consider the right hand side, for an arbitrary $\phi\in V^*$, $i_V(v)(\phi)=\phi(v)\in V^{**}$; then $f^{**}[i_V(v)]\in W^{**}.$ Hence, $f^{**}[i_V(v)](\psi)= i_V(v)[f^*(\psi)\in V^*]=f^*(\psi)(v)\in\mathbb R.$ I should now have $f^*(\psi)(v) = \psi[f(v)] $ simply by definition of $f^*$. Part $2$: Assume there exist the two isomorphism $i_V$ and $i_W$. Let $V=W=\mathbb R^2$ and set $f$ to be $\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}.$ Consider the left hand side, for an arbitrary $(a, b)\in \mathbb R^2$, $f(a, b) = (b, a)$. Then $i_W(b, a)\in W^*$ and $f^*[i_W(b, a)](a, b) = i_W(b, a)[f(a,b)] = i_W(b, a)(b, a)\in\mathbb R$. As for the right hand side, $i_V(a, b)(a,b)\in\mathbb R$. I should now have $i_V(a, b)(a,b)\neq i_W(b, a)(b, a)$, but how?",,"['linear-algebra', 'differential-geometry']"
91,Is the categorical product for projective spaces essentially the tensor product?,Is the categorical product for projective spaces essentially the tensor product?,,"I wonder whether the categorical product of two projective spaces is essentially given by the tensor product of the underlying vector spaces. Is this at least true for projective Hilbert spaces? One problem I have with verifying this gut feeling is that I don't even know which morphisms are allowed between two projective spaces. Every non-zero linear map between the underlying vector spaces gives rise to a projective morphism between the projective spaces. But is every projective morphism of this form, or are there other possibilities? What about inversions, for example? (For the Hilbert space case, are the projective morphisms induced by the continuous linear maps?)","I wonder whether the categorical product of two projective spaces is essentially given by the tensor product of the underlying vector spaces. Is this at least true for projective Hilbert spaces? One problem I have with verifying this gut feeling is that I don't even know which morphisms are allowed between two projective spaces. Every non-zero linear map between the underlying vector spaces gives rise to a projective morphism between the projective spaces. But is every projective morphism of this form, or are there other possibilities? What about inversions, for example? (For the Hilbert space case, are the projective morphisms induced by the continuous linear maps?)",,"['linear-algebra', 'category-theory', 'projective-geometry']"
92,Inverse of the sum of a symmetric and diagonal matrices,Inverse of the sum of a symmetric and diagonal matrices,,"I have two matrices $A$ and $B$ with quite a few notable properties. They are both square. They are both symmetric. They are the same size. $A$ has $1$'s along the diagonal and real numbers in $(0 - 1)$ on the off-diagonal. $B$ has real numbers along the diagonal and $0$'s on the off-diagonal. So, they look like this: $$ A= \left[\begin{matrix} 1 & b & ... & z\\ b & 1 & ... & y\\ \vdots & \vdots & \ddots & \vdots \\ z & y & ... & 1 \end{matrix}\right]\\ $$ and  $$ B =  \left[ \begin{matrix} \alpha & 0 & ... & 0\\ 0 & \beta & 0 & 0\\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & ... & \omega \end{matrix}\right] $$ I need to calculate $(A+\delta B)^{-1}$ many times, with a different value of $\delta$ each time.  This can be done directly, but it may be time consuming, depending on the number of $\delta$'s and the size of $A$ and $B$. If the values along the diagonal of $B$ were $1$, it would be the identity matrix, and it could straightforwardly be co-diagonalized with $A$ so that the inverse of the sum can be calculated by inverting the eigen value.  But, alas, that is not the case. My intuition is that no such matrix algebra shortcut can exist in the scenario under consideration, but I am hopeful that someone can prove me wrong. edit: I should have provided more information about that.  What I really want is a matrix, $M$, such that $MM^{T} = (A + \delta B)^{-1}$.  If I can eigen-decompose $A+\delta B$ quickly, then I need only invert the eigen-values ($n$ scalar divisions) and multiply by the eigen vectors ($n$ scalar-vector multiplications) to get $M$.","I have two matrices $A$ and $B$ with quite a few notable properties. They are both square. They are both symmetric. They are the same size. $A$ has $1$'s along the diagonal and real numbers in $(0 - 1)$ on the off-diagonal. $B$ has real numbers along the diagonal and $0$'s on the off-diagonal. So, they look like this: $$ A= \left[\begin{matrix} 1 & b & ... & z\\ b & 1 & ... & y\\ \vdots & \vdots & \ddots & \vdots \\ z & y & ... & 1 \end{matrix}\right]\\ $$ and  $$ B =  \left[ \begin{matrix} \alpha & 0 & ... & 0\\ 0 & \beta & 0 & 0\\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & ... & \omega \end{matrix}\right] $$ I need to calculate $(A+\delta B)^{-1}$ many times, with a different value of $\delta$ each time.  This can be done directly, but it may be time consuming, depending on the number of $\delta$'s and the size of $A$ and $B$. If the values along the diagonal of $B$ were $1$, it would be the identity matrix, and it could straightforwardly be co-diagonalized with $A$ so that the inverse of the sum can be calculated by inverting the eigen value.  But, alas, that is not the case. My intuition is that no such matrix algebra shortcut can exist in the scenario under consideration, but I am hopeful that someone can prove me wrong. edit: I should have provided more information about that.  What I really want is a matrix, $M$, such that $MM^{T} = (A + \delta B)^{-1}$.  If I can eigen-decompose $A+\delta B$ quickly, then I need only invert the eigen-values ($n$ scalar divisions) and multiply by the eigen vectors ($n$ scalar-vector multiplications) to get $M$.",,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'matrix-decomposition']"
93,Singular Value Decomposition of Rank 1 matrix,Singular Value Decomposition of Rank 1 matrix,,"I am trying to understand singular value decomposition.  I get the general definition and how to solve for the singular values of form the SVD of a given matrix however, I came across the following problem and realized that I did not fully understand how SVD works: Let $0\ne u\in \mathbb{R}^{m}$. Determine an SVD for the matrix $uu^{*}$. I understand that $uu^{*}$ has rank 1 and thus only has one singular value i.e. $$\Sigma =  \begin{pmatrix}    \sigma_1 & \ldots & 0\\   0& \ldots & 0  \end{pmatrix}  \in\mathbb{R}^{m\times m}$$ and I realize since $uu^{*}\in\mathbb{R}^{m\times m}$ then for $uu^{*}=U\sum V^{*}$ that $U,\Sigma,V\in\mathbb{R}^{m\times m}$.  Additionally, I realize that the columns of $U$ and $V$ are orthonormal. I guess my question is how do you determine U and V from $uu^{*}$?","I am trying to understand singular value decomposition.  I get the general definition and how to solve for the singular values of form the SVD of a given matrix however, I came across the following problem and realized that I did not fully understand how SVD works: Let $0\ne u\in \mathbb{R}^{m}$. Determine an SVD for the matrix $uu^{*}$. I understand that $uu^{*}$ has rank 1 and thus only has one singular value i.e. $$\Sigma =  \begin{pmatrix}    \sigma_1 & \ldots & 0\\   0& \ldots & 0  \end{pmatrix}  \in\mathbb{R}^{m\times m}$$ and I realize since $uu^{*}\in\mathbb{R}^{m\times m}$ then for $uu^{*}=U\sum V^{*}$ that $U,\Sigma,V\in\mathbb{R}^{m\times m}$.  Additionally, I realize that the columns of $U$ and $V$ are orthonormal. I guess my question is how do you determine U and V from $uu^{*}$?",,"['linear-algebra', 'matrices', 'svd']"
94,Describing a linear map geometrically,Describing a linear map geometrically,,"I have the following linear map $\mathbb{R}^2\to\mathbb{R}^2:$ $$\begin{pmatrix}x\\y\end{pmatrix}\mapsto \begin{pmatrix}9y-5x\\7y-4x\end{pmatrix}$$ I am asked to describe this geometrically. I can see what is happening: the upper left quadrant is being squeezed into the upper right quadrant, in between the lines $y=\frac{7}{9}x$ and $y=\frac{4}{5}x$. Likewise the lower right quadrant is scrambled in between the same lines in the lower left quadrant. The other quadrants are stretched in the obvious way. I can't find the proper terms to describe this well though. A rotation followed by a squeeze ? Thanks to anyone who responds. (I can upload an image if anyone is interested, but the lines are so close that it will not be very clear)","I have the following linear map $\mathbb{R}^2\to\mathbb{R}^2:$ $$\begin{pmatrix}x\\y\end{pmatrix}\mapsto \begin{pmatrix}9y-5x\\7y-4x\end{pmatrix}$$ I am asked to describe this geometrically. I can see what is happening: the upper left quadrant is being squeezed into the upper right quadrant, in between the lines $y=\frac{7}{9}x$ and $y=\frac{4}{5}x$. Likewise the lower right quadrant is scrambled in between the same lines in the lower left quadrant. The other quadrants are stretched in the obvious way. I can't find the proper terms to describe this well though. A rotation followed by a squeeze ? Thanks to anyone who responds. (I can upload an image if anyone is interested, but the lines are so close that it will not be very clear)",,['linear-algebra']
95,Existence theorem about the adjoint.,Existence theorem about the adjoint.,,"I'm trying to prove the existence of the linear map $f^*$ in the following theorem about the adjoint: Let $f: V \to W$ be a linear map, with $V$ finite-dimensional and $V,W$ inner product spaces over the same field. Then there is a unique linear map $f^*: W \to V$ such that $$\langle f(v),w \rangle = \langle v, f^*(w)\rangle$$ for all $v\in V, w \in W$. Here's how far I got so far. Try 1: Fix $w \in W$. Because $V$ is finite-dimensional there exists a unique $x \in V$ for all linear forms $\psi_1 = \langle f(\cdot),w \rangle \in V^*$ --- namely we have the isomorphism $V \to V^*$. Since the linear form $\langle f(\cdot),w \rangle$ is uniquely determined by $w \in W$, define the linear map $f^*$ by $x = f^*(w)$. Let $\psi_2 = \langle \cdot,f^*(w) \rangle \in V^*$  and $\theta = \psi_1 - \psi_2 \in V^*$. Now consider ker$(\theta)$. If ker$(\theta)$ = $V$ then our claim is proven --- namely $\psi_1 = \psi_2$. Mistake: Firstly $\psi_1 \in W^*$, and not in $V^*$. And because $W \to W^*$ need not be an isomorphism the argument doens't follow. EDIT: $\psi_1 \in V^*$ indeed. Namely $\psi_1: v \mapsto \langle f(v),w \rangle$ and $w \in W$ is fixed. However note that $\psi_1$ uses an inner product on $W$. EDIT: Note that rk$(\theta) = 1$, so we have dim$\big($ker$(\theta)\big) = $ dim$(V) - 1$. Which fails my attempted proof.  Namely take dim$(V) = 1$. So that, ker$(\theta)$ = $\{0\}$. And thus for all $0 \neq v \in V$ we have $\psi_1 \neq \psi_2$. Try 2: the same as Try 1 but with $v = x$. I'm assuming that the argument can be proven by a less cumbersome method. Also I think that my method is not correct, since I seem to get stuck. Suggestions or an alternative method are welcome. EDIT: Here's a summary of Fischer's proof. Define the linear map $\lambda: W \to V^*$ by $$\lambda(w) = \langle \cdot, w \rangle_W \circ f.$$ Next define the isomorphism $\sigma: V \to V^*$ by $$\sigma(v_0) = \langle \cdot, v_0 \rangle_V.$$ Now, consider the composition $$\varphi: \sigma^{-1} \circ \lambda: W \to V.$$ We then have $$\langle v, \varphi(w) \rangle_V = \langle f(v), w \rangle_W$$ for all $v \in V, w \in W$. We can now define $f^*:= \varphi$. This proves the theorem for the existence part. For uniqueness, assume that $\psi$ satisfies $\langle f(v),w \rangle_W = \langle v, \psi(w) \rangle_V$. Fixing $w_0 \in W$, we get $$\langle v,\psi(w_0) - f^*(w_0) \rangle_V = 0$$ for all $v \in V$. Choosing $v = \psi(w_0) - f^*(w_0)$ we get $\psi = f^*$, which proves the uniqueness part.","I'm trying to prove the existence of the linear map $f^*$ in the following theorem about the adjoint: Let $f: V \to W$ be a linear map, with $V$ finite-dimensional and $V,W$ inner product spaces over the same field. Then there is a unique linear map $f^*: W \to V$ such that $$\langle f(v),w \rangle = \langle v, f^*(w)\rangle$$ for all $v\in V, w \in W$. Here's how far I got so far. Try 1: Fix $w \in W$. Because $V$ is finite-dimensional there exists a unique $x \in V$ for all linear forms $\psi_1 = \langle f(\cdot),w \rangle \in V^*$ --- namely we have the isomorphism $V \to V^*$. Since the linear form $\langle f(\cdot),w \rangle$ is uniquely determined by $w \in W$, define the linear map $f^*$ by $x = f^*(w)$. Let $\psi_2 = \langle \cdot,f^*(w) \rangle \in V^*$  and $\theta = \psi_1 - \psi_2 \in V^*$. Now consider ker$(\theta)$. If ker$(\theta)$ = $V$ then our claim is proven --- namely $\psi_1 = \psi_2$. Mistake: Firstly $\psi_1 \in W^*$, and not in $V^*$. And because $W \to W^*$ need not be an isomorphism the argument doens't follow. EDIT: $\psi_1 \in V^*$ indeed. Namely $\psi_1: v \mapsto \langle f(v),w \rangle$ and $w \in W$ is fixed. However note that $\psi_1$ uses an inner product on $W$. EDIT: Note that rk$(\theta) = 1$, so we have dim$\big($ker$(\theta)\big) = $ dim$(V) - 1$. Which fails my attempted proof.  Namely take dim$(V) = 1$. So that, ker$(\theta)$ = $\{0\}$. And thus for all $0 \neq v \in V$ we have $\psi_1 \neq \psi_2$. Try 2: the same as Try 1 but with $v = x$. I'm assuming that the argument can be proven by a less cumbersome method. Also I think that my method is not correct, since I seem to get stuck. Suggestions or an alternative method are welcome. EDIT: Here's a summary of Fischer's proof. Define the linear map $\lambda: W \to V^*$ by $$\lambda(w) = \langle \cdot, w \rangle_W \circ f.$$ Next define the isomorphism $\sigma: V \to V^*$ by $$\sigma(v_0) = \langle \cdot, v_0 \rangle_V.$$ Now, consider the composition $$\varphi: \sigma^{-1} \circ \lambda: W \to V.$$ We then have $$\langle v, \varphi(w) \rangle_V = \langle f(v), w \rangle_W$$ for all $v \in V, w \in W$. We can now define $f^*:= \varphi$. This proves the theorem for the existence part. For uniqueness, assume that $\psi$ satisfies $\langle f(v),w \rangle_W = \langle v, \psi(w) \rangle_V$. Fixing $w_0 \in W$, we get $$\langle v,\psi(w_0) - f^*(w_0) \rangle_V = 0$$ for all $v \in V$. Choosing $v = \psi(w_0) - f^*(w_0)$ we get $\psi = f^*$, which proves the uniqueness part.",,['linear-algebra']
96,Subadditivity of positive index of inertia,Subadditivity of positive index of inertia,,"Denote the number of positive eigenvalues of a Hermitian matrix $H$ by $P_H$. If $A,B$ Hermitian, show that $$P_{A+B}\leq P_A+P_B.$$","Denote the number of positive eigenvalues of a Hermitian matrix $H$ by $P_H$. If $A,B$ Hermitian, show that $$P_{A+B}\leq P_A+P_B.$$",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
97,"Prove $(2, x)$ is not a free $R$-module.",Prove  is not a free -module.,"(2, x) R","Let $R = \mathbb Z[x]$ and let $M = (2, x)$ be the ideal generated by $2$ and $x$, considered as an $R$-module. Show that $\{2, x\}$ is not a basis for $M$. Show that any 2 elements of $M$ are linearly dependent. Show that $M$ is not a free $R$-module. My approach so far: Suppose by way of contradiction $\{2, x\}$ is in fact a basis for $M$, then it would follow that $2$ and $x$ are linearly independent, that is for $\alpha(x), \beta(x) \in \mathbb Z[x]$, if $2\alpha(x) + \beta(x)x = 0$ then it must follow that $\alpha(x) = \beta(x) = 0$, but observe that $\alpha(x) = x$ and $\beta(x) = -2$ satisfies this equation and they are both non-zero. Conclude that $\{2, x\}$ is a linearly dependent set, a contradiction to our hypothesis that $\{2, x\}$ is a basis of $M$. Conclude that $\{2, x\}$ is not a basis for $M$. Now I am stuck on the second part. I choose two arbitrary elements of $M = (2, x) = \{2f(x) + g(x)x : f, g \in \mathbb Z[x]\}$. Say $a, b \in M$ then by construction $a(x) = 2f_1(x) + g_1(x)x$ and $b(x) = 2f_2(x) + g_2(x)x$. If $\alpha, \beta \in \mathbb Z[x]$, that is, $a, b$ scalar and if $\alpha(x)a(x) + \beta(x)b(x) = 0$ we want to show that $\alpha(x)$, $\beta(x)$ are not both necessarily both zero. I started trying to choose values of $\alpha$ and $\beta$ nonzero such that the equality holds, but it never works out. Any help here would be appreciated. For the final part, I'm assuming the second part will help. I'm guessing if we show the dimension of $M$ over $\mathbb Z[x]$ must be 2, and then since we know any two elements in $M$ must be linearly dependent, no basis could exist. But my reasoning is a guess, I don't know why mathematically the dimension must be 2, or even if this is the best approach to this part of the problem. Finally I searched online before asking the question here, I found two proofs using the concept of 'rank', but I'd prefer not to use this in my proof since I am unfamiliar with the concept.","Let $R = \mathbb Z[x]$ and let $M = (2, x)$ be the ideal generated by $2$ and $x$, considered as an $R$-module. Show that $\{2, x\}$ is not a basis for $M$. Show that any 2 elements of $M$ are linearly dependent. Show that $M$ is not a free $R$-module. My approach so far: Suppose by way of contradiction $\{2, x\}$ is in fact a basis for $M$, then it would follow that $2$ and $x$ are linearly independent, that is for $\alpha(x), \beta(x) \in \mathbb Z[x]$, if $2\alpha(x) + \beta(x)x = 0$ then it must follow that $\alpha(x) = \beta(x) = 0$, but observe that $\alpha(x) = x$ and $\beta(x) = -2$ satisfies this equation and they are both non-zero. Conclude that $\{2, x\}$ is a linearly dependent set, a contradiction to our hypothesis that $\{2, x\}$ is a basis of $M$. Conclude that $\{2, x\}$ is not a basis for $M$. Now I am stuck on the second part. I choose two arbitrary elements of $M = (2, x) = \{2f(x) + g(x)x : f, g \in \mathbb Z[x]\}$. Say $a, b \in M$ then by construction $a(x) = 2f_1(x) + g_1(x)x$ and $b(x) = 2f_2(x) + g_2(x)x$. If $\alpha, \beta \in \mathbb Z[x]$, that is, $a, b$ scalar and if $\alpha(x)a(x) + \beta(x)b(x) = 0$ we want to show that $\alpha(x)$, $\beta(x)$ are not both necessarily both zero. I started trying to choose values of $\alpha$ and $\beta$ nonzero such that the equality holds, but it never works out. Any help here would be appreciated. For the final part, I'm assuming the second part will help. I'm guessing if we show the dimension of $M$ over $\mathbb Z[x]$ must be 2, and then since we know any two elements in $M$ must be linearly dependent, no basis could exist. But my reasoning is a guess, I don't know why mathematically the dimension must be 2, or even if this is the best approach to this part of the problem. Finally I searched online before asking the question here, I found two proofs using the concept of 'rank', but I'd prefer not to use this in my proof since I am unfamiliar with the concept.",,"['linear-algebra', 'modules']"
98,"For linear subspaces of equal dimension, there exists a common complement","For linear subspaces of equal dimension, there exists a common complement",,"I am attempting exercise #14 from Chapter 1 of Roman's Advanced Linear Algebra (p.57). Here is the statement of the problem: Suppose $V$ is a finite-dimensional vector space over an infinite field $\mathbb{F}$. Suppose $V_1$, $V_2, \dotsc, V_n\leq V$ are subspaces of $V$ such that $\dim{V_1} = \dotsb = \dim{V_n}$. Show that there exists a subspace $S$ such that $V = V_i \oplus S$ for all $i$. (In other words, there exists a common complement for $V_i$.) Now, if $\mathbb{F}$ is something familiar like $\mathbb{R}$ or $\mathbb{C}$ I can use measure-theoretic arguments to prove this. I might even be able to get away with it for any field of characteristic 0. But over something like $\mathbb{F}_2(t)$ or any other infinite field that is not obviously a measure space, that won't work. And that certainly is not the proof that Roman is looking for. I think we want something like the proof he offers for Theorem 1.2 on p. 39, where he shows that a nontrivial vector space over an infinite field $\mathbb{F}$ is not a finite union of proper subspaces. Here he supposes $V = V_1 \cup \dotsb \cup V_n$ and for $w\in V_1 \setminus (V_2 \cup \dotsb \cup V_n)$ considers a set $A=\{rw + v \mid  r \in \mathbb{F}\}$, and shows that each subspace must contain at most one vector from this set. Obviously this particular argument doesn't apply, but I think we want to make a similar argument. Any ideas? Thanks","I am attempting exercise #14 from Chapter 1 of Roman's Advanced Linear Algebra (p.57). Here is the statement of the problem: Suppose $V$ is a finite-dimensional vector space over an infinite field $\mathbb{F}$. Suppose $V_1$, $V_2, \dotsc, V_n\leq V$ are subspaces of $V$ such that $\dim{V_1} = \dotsb = \dim{V_n}$. Show that there exists a subspace $S$ such that $V = V_i \oplus S$ for all $i$. (In other words, there exists a common complement for $V_i$.) Now, if $\mathbb{F}$ is something familiar like $\mathbb{R}$ or $\mathbb{C}$ I can use measure-theoretic arguments to prove this. I might even be able to get away with it for any field of characteristic 0. But over something like $\mathbb{F}_2(t)$ or any other infinite field that is not obviously a measure space, that won't work. And that certainly is not the proof that Roman is looking for. I think we want something like the proof he offers for Theorem 1.2 on p. 39, where he shows that a nontrivial vector space over an infinite field $\mathbb{F}$ is not a finite union of proper subspaces. Here he supposes $V = V_1 \cup \dotsb \cup V_n$ and for $w\in V_1 \setminus (V_2 \cup \dotsb \cup V_n)$ considers a set $A=\{rw + v \mid  r \in \mathbb{F}\}$, and shows that each subspace must contain at most one vector from this set. Obviously this particular argument doesn't apply, but I think we want to make a similar argument. Any ideas? Thanks",,"['linear-algebra', 'vector-spaces']"
99,Inner product and norm of a function,Inner product and norm of a function,,"I have recently started a undergraduate linear algebra course in which these definitions came up: Let $V$ be the vector space $C[a, b]$ of all continuous functions on $[a, b]$ . Then the inner product and norm are defined as: \begin{align}     \langle f, g \rangle &= \int_a^b f(t) g(t) \,\mathrm{d}t     \\     \| f \| &= \sqrt{\langle f, f \rangle} = \sqrt{\int_a^b f^2(t) \,\mathrm{d}t}   \end{align} Concerns: What does it mean if $\int_a^b f^2(t) \,\mathrm{d}t < 0$ ? It is also, strangely, possible to calculate the angle between functions (non-linear), is this considered the average angle in $[a, b]$ or what is it’s geometrical representation?","I have recently started a undergraduate linear algebra course in which these definitions came up: Let be the vector space of all continuous functions on . Then the inner product and norm are defined as: Concerns: What does it mean if ? It is also, strangely, possible to calculate the angle between functions (non-linear), is this considered the average angle in or what is it’s geometrical representation?","V C[a, b] [a, b] \begin{align}
    \langle f, g \rangle &= \int_a^b f(t) g(t) \,\mathrm{d}t
    \\
    \| f \| &= \sqrt{\langle f, f \rangle} = \sqrt{\int_a^b f^2(t) \,\mathrm{d}t}
  \end{align} \int_a^b f^2(t) \,\mathrm{d}t < 0 [a, b]","['linear-algebra', 'functional-analysis']"
