,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Question regarding continuity ($\epsilon$-$\delta$),Question regarding continuity (-),\epsilon \delta,"In this post: Proof continuity of a function with epsilon-delta people explained how to proof that $$f(x) = \frac{x-1}{x^2+1}$$ is continuous in $x_0 = -1$. However, my solution is the following: $$|f(x) - f(x_0)| = \left\lvert\frac{x-1}{x^2+1} - (-1)\right\rvert = \left\lvert\frac{x}{x^2+1}\right\rvert|x+1| \leq |x-(-1)|=|x-x_0|.$$ Then choose $\delta := \epsilon$ and we're done. Now this solution seems by far easier than the once presented in the given post. Are both correct, and if yes, why would you choose those over mine?","In this post: Proof continuity of a function with epsilon-delta people explained how to proof that $$f(x) = \frac{x-1}{x^2+1}$$ is continuous in $x_0 = -1$. However, my solution is the following: $$|f(x) - f(x_0)| = \left\lvert\frac{x-1}{x^2+1} - (-1)\right\rvert = \left\lvert\frac{x}{x^2+1}\right\rvert|x+1| \leq |x-(-1)|=|x-x_0|.$$ Then choose $\delta := \epsilon$ and we're done. Now this solution seems by far easier than the once presented in the given post. Are both correct, and if yes, why would you choose those over mine?",,"['calculus', 'real-analysis', 'analysis']"
1,Imposing boundary conditions AND self-similarity on a PDE,Imposing boundary conditions AND self-similarity on a PDE,,"I have a PDE in the form $$u_t=F(u,u_x)$$ where the unknown is $u(x,t)$ on say $\mathbb R\times[0,\infty)$. $F$ is very nonlinear so I was told to assume self-similarity in the form $$u(x,t)=t\tilde u(x)$$ to reduce it to an ODE. I have convinced myself based on the dimensions of the variables involved that this is the appropriate scaling. My confusion is how then to deal with the boundary conditions at $t=0$. I'm interested in having some particular Dirichlet conditions there, but the scaling assumption seems to force $u(x,0)=0$ for all $x$. I guess this would be because the equation is only approximately self-similar, and then it somehow ``forgets'' about the boundary condition over time? Is there any way around this?","I have a PDE in the form $$u_t=F(u,u_x)$$ where the unknown is $u(x,t)$ on say $\mathbb R\times[0,\infty)$. $F$ is very nonlinear so I was told to assume self-similarity in the form $$u(x,t)=t\tilde u(x)$$ to reduce it to an ODE. I have convinced myself based on the dimensions of the variables involved that this is the appropriate scaling. My confusion is how then to deal with the boundary conditions at $t=0$. I'm interested in having some particular Dirichlet conditions there, but the scaling assumption seems to force $u(x,0)=0$ for all $x$. I guess this would be because the equation is only approximately self-similar, and then it somehow ``forgets'' about the boundary condition over time? Is there any way around this?",,"['analysis', 'partial-differential-equations', 'dimensional-analysis']"
2,On uniform convergence of bounded slowly varying functions,On uniform convergence of bounded slowly varying functions,,"A positive function $f: \mathbb{R} \to \mathbb{R}$ is said to be slowly varying if$$\lim \limits_{t \to \infty} \frac{f(tx)}{f(t)}=1$$ for all $x >0$. Assume that $f(x) \in [m,M]$ for all $x$. Is there a bound $$\left| \frac{f(tx)}{f(t)}-1 \right| \leq \mathcal{O}(t)$$ for all $x >0$, with $\mathcal{O}(t) \to 0$ as $t \to \infty$? Are there known regularity conditions for $f$ that would guarantee the existence of such bound? I have tried to have a look at some results of Karamata, but not much help from that so far. Locally uniform convergence of the above ratio is a well known result in this topic ( see Theorem A.1. ). I'm interested in existence of a global analogue of this result under some additional regularity conditions on $f$ (also whether such result can be expected to hold at all). EDIT: The original question also included the question whether $f(t)$ converges or not as $t \to \infty$. I have removed that part, since it has been previously answered (in negative) elsewhere .","A positive function $f: \mathbb{R} \to \mathbb{R}$ is said to be slowly varying if$$\lim \limits_{t \to \infty} \frac{f(tx)}{f(t)}=1$$ for all $x >0$. Assume that $f(x) \in [m,M]$ for all $x$. Is there a bound $$\left| \frac{f(tx)}{f(t)}-1 \right| \leq \mathcal{O}(t)$$ for all $x >0$, with $\mathcal{O}(t) \to 0$ as $t \to \infty$? Are there known regularity conditions for $f$ that would guarantee the existence of such bound? I have tried to have a look at some results of Karamata, but not much help from that so far. Locally uniform convergence of the above ratio is a well known result in this topic ( see Theorem A.1. ). I'm interested in existence of a global analogue of this result under some additional regularity conditions on $f$ (also whether such result can be expected to hold at all). EDIT: The original question also included the question whether $f(t)$ converges or not as $t \to \infty$. I have removed that part, since it has been previously answered (in negative) elsewhere .",,"['analysis', 'uniform-convergence', 'slowly-varying-functions']"
3,What is the Most General Setting in Which Limits Commute with Continuous Functions?,What is the Most General Setting in Which Limits Commute with Continuous Functions?,,"In metric spaces, we have that limits commute with continuous functions . In Hausdorff spaces, the limits of nets are always unique. Seemingly the second fact is necessary for the proof of the first. However, it is not clear to me that metric structure is necessary for the proof of the first either (for example, why would uniform structure not be sufficient). This might, however, just be a result of the fact that I don't understand the concept of limit very well in spaces more general than metric spaces. Question: What is the most general type of space for which a function is continuous if and only if it commutes with the limits of sequences? Somewhere strictly between Hausdorff and metrizable spaces? ( Note: I'm very surprised that this question seems not to be a duplicate -- I could not find an equivalent question or the answer when Googling. If you know where to find the answer, please just close the question without downvoting and comment with the link.) Background: Inspired by this question , in which an elegant proof using this property is given. Naturally I am curious to see the full generality to which this proof can be extended.","In metric spaces, we have that limits commute with continuous functions . In Hausdorff spaces, the limits of nets are always unique. Seemingly the second fact is necessary for the proof of the first. However, it is not clear to me that metric structure is necessary for the proof of the first either (for example, why would uniform structure not be sufficient). This might, however, just be a result of the fact that I don't understand the concept of limit very well in spaces more general than metric spaces. Question: What is the most general type of space for which a function is continuous if and only if it commutes with the limits of sequences? Somewhere strictly between Hausdorff and metrizable spaces? ( Note: I'm very surprised that this question seems not to be a duplicate -- I could not find an equivalent question or the answer when Googling. If you know where to find the answer, please just close the question without downvoting and comment with the link.) Background: Inspired by this question , in which an elegant proof using this property is given. Naturally I am curious to see the full generality to which this proof can be extended.",,"['general-topology', 'analysis', 'metric-spaces']"
4,Finding $\int^{1}_{0}{p^x(1-p)^{n-x}\exp\left\{-\frac{1}{2} \left(\ln \left(a\frac{p}{1-p}\right)\right)^2\right\}}dp$,Finding,\int^{1}_{0}{p^x(1-p)^{n-x}\exp\left\{-\frac{1}{2} \left(\ln \left(a\frac{p}{1-p}\right)\right)^2\right\}}dp,"I got the following integration and I could not figure it out.I wonder if it has a closed form, $$\int^{1}_{0}{p^x(1-p)^{n-x}\exp\left\{-\frac{1}{2} \left(\ln \left(a\frac{p}{1-p}\right)\right)^2\right\}}dp$$ where $n>x$ and $n,x,$ and $a$ are some positive constants. Thanks in advance!","I got the following integration and I could not figure it out.I wonder if it has a closed form, $$\int^{1}_{0}{p^x(1-p)^{n-x}\exp\left\{-\frac{1}{2} \left(\ln \left(a\frac{p}{1-p}\right)\right)^2\right\}}dp$$ where $n>x$ and $n,x,$ and $a$ are some positive constants. Thanks in advance!",,"['calculus', 'integration', 'analysis']"
5,Example 1.1 in Baby Rudin Generalised,Example 1.1 in Baby Rudin Generalised,,"In example 1.1 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition, the author shows that there is no rational number $p$ such that $p^2 = 2$; moreover, the set $A$ of all rational numbers $q$ such that $q^2 < 2$ has no largest element in the set of rationals and the set of all rational numbers $r$ such that $r^2 > 2$ has no smallest element. Now we  would like to state the following. Let $n$ be a positive integer greater than $1$. Then there is no rational number $p$ such that $p^n = 2$; moreover the set $A$ of all rationals $q$ such that $q^n < 2$ has no largest element, and the set $B$ of all rationals $r$ such that $r^n > 2$ has no smallest element. How to prove this statement, using the same idea as Rudin has used? How can we state and prove an even more generalised version of the above statement, with $2$ replaced by an arbitrary (positive) integer---or perhaps even by an arbitrary rational number?","In example 1.1 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition, the author shows that there is no rational number $p$ such that $p^2 = 2$; moreover, the set $A$ of all rational numbers $q$ such that $q^2 < 2$ has no largest element in the set of rationals and the set of all rational numbers $r$ such that $r^2 > 2$ has no smallest element. Now we  would like to state the following. Let $n$ be a positive integer greater than $1$. Then there is no rational number $p$ such that $p^n = 2$; moreover the set $A$ of all rationals $q$ such that $q^n < 2$ has no largest element, and the set $B$ of all rationals $r$ such that $r^n > 2$ has no smallest element. How to prove this statement, using the same idea as Rudin has used? How can we state and prove an even more generalised version of the above statement, with $2$ replaced by an arbitrary (positive) integer---or perhaps even by an arbitrary rational number?",,"['real-analysis', 'analysis']"
6,"Prob. 7, Chap. 1, in Baby Rudin: The Existence of Logarithms","Prob. 7, Chap. 1, in Baby Rudin: The Existence of Logarithms",,"Here's problem 7 in the exercises following Chap. 1 in Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Fix $b > 1$, $y > 0$, and prove that there is a unique real number $x$ such that $b^x = y$, by completing the following outline. (This $x$ is called the logarithm of $y$ to the base $b$ . ) (a) For any positive integer $n$, $\ $ $b^n-1 \geq n (b-1)$. (b) Hence $b-1 \geq n (b^{1/n} - 1)$. (c) If $t > 1$ and $n > (b-1)/(t-1)$, then $b^{1/n} < t$. (d) If $w$ is such that $b^w < y$, then $b^{w+1/n} < y$ for sufficiently large $n$; ... (e) If $b^w > y$, then $b^{w-1/n} > y$ for sufficiently large $n$. (f) Let $A$ be the set of all $w$ such that $b^w < y$, and show that $x = \sup A$ satisfies $b^x = y$. (g) Prove that this $x$ is unique. Now (a) can be proved using induction on $n$, and (b) can be obtained by putting $b^{1/n}$ for $b$ in (a) since $b > 1$ implies $b^{1/n} > 1$ also. (c) If $t > 1$ and $n > (b-1)/(t-1)$, then we have $$n (t-1) > b-1 \geq n(b^{1/n} -1) \ \ \ \mbox{ using (b)}. $$ So $$t-1 > b^{1/n} -1,$$ from which the desired result follows. (d) If $w$ is such that $b^w < y$, then $y b^{-w} > 1$ and we put $t \colon= y b^{-w}$ in (c) to obtain $b^{1/n} < y b^{-w}$ or $b^{w+1/n} < y$ for a sufficiently large $n$ (i.e., whenever $n > (b-1)/(yb^{-w}-1)$). (e) If $w$ is such that $b^w > y$, then $b^w y^{-1} > 1$, so as in (c), if we choose a positive integer $n$ such that  $n > (b-1)/(b^w y^{-1} -1)$, then we have $b^{1/n} < b^w y^{-1}$ and so $b^{w-1/n} >y$. Now let $$ S \colon= \{ \ b^n \ \colon \ n \in \mathbb{N} \ \}.$$ We show that the set $S$ is not bounded above in $\mathbb{R}$. Let's assume the contrary. Let $\alpha \colon = \sup S$. Then $\alpha \geq b^n$ for all $n \in \mathbb{N}$; so in particular, $\alpha > 1$. Now as $\alpha/b < \alpha$, so $\alpha/b$ is not an upper bound for $S$. Thus there exists a $k \in \mathbb{N}$ such that $\alpha/b < b^k$. But then $\alpha < b^{k+1}$ and $k+1 \in \mathbb{N}$ also. Hence a contradiction. Therefore, for any real number $\alpha$, there is a natural number $n$ such that $b^n > \alpha$. We can also show that, for any integers $m$ and $n$, $b^m < b^n$ if and only if $m< n$. (f) We first need to show that the set $A$ is non-empty and bounded above. If $y> 1$, then $0 \in A$, for $b^0 =1 < y$. For $y=1$, $-1\in A$, for $b^{-1} = 1/b < 1$. For $0< y< 1$, we have $1/y > 1$. So there exists a natural number $n$ such that $b^n > 1/y$. So for this $n$, we have $b^{-n} < y$ and so $n \in A$. Thus the set $A$ is non-empty. Now we show that the set $A$ is bounded from above. We can prove the following statement: For any rational numbers $p$, $q$, we have $b^p < b^q$ if and only if $p < q$. Since there is a positive integer $n$ such that $b^n \geq  y$, there is a least positive integer $n_0$ with this property. Now if $w$ is a real number such that $w > n_0$, then we can find a rational number $r$ such that $n_0 < r < w$, and for this $r$ we have $b^{n_0} < b^r$ since $n_0$ is also rational. Therefore, using the discussion in Prob. 6, Chap. 1 in Baby Rudin, we can write $$ \begin{align} b^w &= \sup B(w) = \sup \{ \ b^t \ \colon \ t \in \mathbb{Q}, \ t \leq w \ \} \\ &\geq \sup \{ \ b^t \ \colon \ t \in \mathbb{Q}, \ t \leq r \ \} = \sup B(r) \\ &= b^r \\ &> b^{n_0} \\ &\geq y.\end{align} $$ That is, $b^w > y$ if $w > n_0$; so $w \leq n_0$ for all $w \in A$, and the set $A$ is bounded above. Let $x \colon= \sup A$. We show that $b^x = y$. If $b^x < y$, then using part (d) above, we can find a positive integer $n$ such that $b^{x+1/n} < y$ and so $x+1/n \in A$ for this choice of $n$. But $x+1/n > x$. So we have a contradiction in view of our choice of $x$ as an upper bound for the set $A$. Hence $b^x \not< y$. If $b^x > y$, then using part (e) above, we can find a positive integer $n$ such that $b^{x-1/n} > y$ and so if $w > x-1/n$, then $b^w > y$, which implies that $w \leq x-1/n$ for all $w \in A$, showing that $x-1/n$ is also an upper bound for $A$. But $x-1/n < x$. So we have a contradiction in view of our choice of $x$ as the least upper bound for $A$. Hence $b^x \not>y$. Therefore, $b^x = y$. (g) Finally, we show that this $x$ is unique. Suppose that, for some $z > x$, we also have $b^z = y = b^x$. Since $z > x$, we can choos rational numbers $p$ and $q$ such that $x < p < q < z$, and therefore, we have $$ \begin{align} b^z &= \sup B(z) = \sup \{ \ b^t \ \colon \ t \in \mathbb{Q}, \ t \leq z \ \} \\ &\geq b^q \\ &> b^p \\ &\geq \sup \{ \ b^t \ \colon \ t \in \mathbb{Q}, \ t \leq x \ \} = \sup B(x) \\ &= b^x, \end{align} $$ and hence $b^z > b^x$, which contradicts our choice of $z$. So our real number $x$ such that $b^x = y$ is indeed unique. Is this proof correct? Or, are there any holes in it?","Here's problem 7 in the exercises following Chap. 1 in Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Fix $b > 1$, $y > 0$, and prove that there is a unique real number $x$ such that $b^x = y$, by completing the following outline. (This $x$ is called the logarithm of $y$ to the base $b$ . ) (a) For any positive integer $n$, $\ $ $b^n-1 \geq n (b-1)$. (b) Hence $b-1 \geq n (b^{1/n} - 1)$. (c) If $t > 1$ and $n > (b-1)/(t-1)$, then $b^{1/n} < t$. (d) If $w$ is such that $b^w < y$, then $b^{w+1/n} < y$ for sufficiently large $n$; ... (e) If $b^w > y$, then $b^{w-1/n} > y$ for sufficiently large $n$. (f) Let $A$ be the set of all $w$ such that $b^w < y$, and show that $x = \sup A$ satisfies $b^x = y$. (g) Prove that this $x$ is unique. Now (a) can be proved using induction on $n$, and (b) can be obtained by putting $b^{1/n}$ for $b$ in (a) since $b > 1$ implies $b^{1/n} > 1$ also. (c) If $t > 1$ and $n > (b-1)/(t-1)$, then we have $$n (t-1) > b-1 \geq n(b^{1/n} -1) \ \ \ \mbox{ using (b)}. $$ So $$t-1 > b^{1/n} -1,$$ from which the desired result follows. (d) If $w$ is such that $b^w < y$, then $y b^{-w} > 1$ and we put $t \colon= y b^{-w}$ in (c) to obtain $b^{1/n} < y b^{-w}$ or $b^{w+1/n} < y$ for a sufficiently large $n$ (i.e., whenever $n > (b-1)/(yb^{-w}-1)$). (e) If $w$ is such that $b^w > y$, then $b^w y^{-1} > 1$, so as in (c), if we choose a positive integer $n$ such that  $n > (b-1)/(b^w y^{-1} -1)$, then we have $b^{1/n} < b^w y^{-1}$ and so $b^{w-1/n} >y$. Now let $$ S \colon= \{ \ b^n \ \colon \ n \in \mathbb{N} \ \}.$$ We show that the set $S$ is not bounded above in $\mathbb{R}$. Let's assume the contrary. Let $\alpha \colon = \sup S$. Then $\alpha \geq b^n$ for all $n \in \mathbb{N}$; so in particular, $\alpha > 1$. Now as $\alpha/b < \alpha$, so $\alpha/b$ is not an upper bound for $S$. Thus there exists a $k \in \mathbb{N}$ such that $\alpha/b < b^k$. But then $\alpha < b^{k+1}$ and $k+1 \in \mathbb{N}$ also. Hence a contradiction. Therefore, for any real number $\alpha$, there is a natural number $n$ such that $b^n > \alpha$. We can also show that, for any integers $m$ and $n$, $b^m < b^n$ if and only if $m< n$. (f) We first need to show that the set $A$ is non-empty and bounded above. If $y> 1$, then $0 \in A$, for $b^0 =1 < y$. For $y=1$, $-1\in A$, for $b^{-1} = 1/b < 1$. For $0< y< 1$, we have $1/y > 1$. So there exists a natural number $n$ such that $b^n > 1/y$. So for this $n$, we have $b^{-n} < y$ and so $n \in A$. Thus the set $A$ is non-empty. Now we show that the set $A$ is bounded from above. We can prove the following statement: For any rational numbers $p$, $q$, we have $b^p < b^q$ if and only if $p < q$. Since there is a positive integer $n$ such that $b^n \geq  y$, there is a least positive integer $n_0$ with this property. Now if $w$ is a real number such that $w > n_0$, then we can find a rational number $r$ such that $n_0 < r < w$, and for this $r$ we have $b^{n_0} < b^r$ since $n_0$ is also rational. Therefore, using the discussion in Prob. 6, Chap. 1 in Baby Rudin, we can write $$ \begin{align} b^w &= \sup B(w) = \sup \{ \ b^t \ \colon \ t \in \mathbb{Q}, \ t \leq w \ \} \\ &\geq \sup \{ \ b^t \ \colon \ t \in \mathbb{Q}, \ t \leq r \ \} = \sup B(r) \\ &= b^r \\ &> b^{n_0} \\ &\geq y.\end{align} $$ That is, $b^w > y$ if $w > n_0$; so $w \leq n_0$ for all $w \in A$, and the set $A$ is bounded above. Let $x \colon= \sup A$. We show that $b^x = y$. If $b^x < y$, then using part (d) above, we can find a positive integer $n$ such that $b^{x+1/n} < y$ and so $x+1/n \in A$ for this choice of $n$. But $x+1/n > x$. So we have a contradiction in view of our choice of $x$ as an upper bound for the set $A$. Hence $b^x \not< y$. If $b^x > y$, then using part (e) above, we can find a positive integer $n$ such that $b^{x-1/n} > y$ and so if $w > x-1/n$, then $b^w > y$, which implies that $w \leq x-1/n$ for all $w \in A$, showing that $x-1/n$ is also an upper bound for $A$. But $x-1/n < x$. So we have a contradiction in view of our choice of $x$ as the least upper bound for $A$. Hence $b^x \not>y$. Therefore, $b^x = y$. (g) Finally, we show that this $x$ is unique. Suppose that, for some $z > x$, we also have $b^z = y = b^x$. Since $z > x$, we can choos rational numbers $p$ and $q$ such that $x < p < q < z$, and therefore, we have $$ \begin{align} b^z &= \sup B(z) = \sup \{ \ b^t \ \colon \ t \in \mathbb{Q}, \ t \leq z \ \} \\ &\geq b^q \\ &> b^p \\ &\geq \sup \{ \ b^t \ \colon \ t \in \mathbb{Q}, \ t \leq x \ \} = \sup B(x) \\ &= b^x, \end{align} $$ and hence $b^z > b^x$, which contradicts our choice of $z$. So our real number $x$ such that $b^x = y$ is indeed unique. Is this proof correct? Or, are there any holes in it?",,"['real-analysis', 'analysis', 'logarithms', 'foundations']"
7,"Weaker than ""weak"" convergence of measures","Weaker than ""weak"" convergence of measures",,"I am reading this page https://en.wikipedia.org/wiki/Convergence_of_measures and I'm wondering if there are some forms of convergence in measure that is weaker than ""weak convergence"" (aka weak* convergence) here. Or maybe just other forms of convergence of measure that don't imply this weak convergence. For example, I have heard of the Radon and Wasserstein metric and also the vague topology, but I believe (according to Wikipedia) each of these implies weak convergence.","I am reading this page https://en.wikipedia.org/wiki/Convergence_of_measures and I'm wondering if there are some forms of convergence in measure that is weaker than ""weak convergence"" (aka weak* convergence) here. Or maybe just other forms of convergence of measure that don't imply this weak convergence. For example, I have heard of the Radon and Wasserstein metric and also the vague topology, but I believe (according to Wikipedia) each of these implies weak convergence.",,"['analysis', 'measure-theory']"
8,On sufficient conditions for the Fréchet-differentiability of a function of several variables,On sufficient conditions for the Fréchet-differentiability of a function of several variables,,"Let $E_1,...,E_n$ and $F$ be normed vector spaces and let $\Omega \subset \prod E_k$ be open. Let $f: \Omega \to F$ and let $a \in \Omega$ Are the following conditions sufficient for $f$ to be differentiable at $a$ (Fréchet-wise)? The partial derivative maps of $f$ exist on $\Omega$ . The partial derivative maps of $f$ are continuous at $a$ . Why am I asking the question I read somewhere a theorem which stated that these two conditions plus the continuity of $f$ on $\Omega$ are sufficient for $f$ to be differentiable at $a$ . However, it appears to me that the continuity of $f$ is used nowhere in the proof. I apologize for not presenting the proof: the source is not available online, and the proof is too long to write it down. Thus, I'm just expecting an experienced person to confirm to me whether or not the continuity is needed; I am not looking for a proof, but just a hint for why the continuity of $f$ is needed (in case it is).","Let and be normed vector spaces and let be open. Let and let Are the following conditions sufficient for to be differentiable at (Fréchet-wise)? The partial derivative maps of exist on . The partial derivative maps of are continuous at . Why am I asking the question I read somewhere a theorem which stated that these two conditions plus the continuity of on are sufficient for to be differentiable at . However, it appears to me that the continuity of is used nowhere in the proof. I apologize for not presenting the proof: the source is not available online, and the proof is too long to write it down. Thus, I'm just expecting an experienced person to confirm to me whether or not the continuity is needed; I am not looking for a proof, but just a hint for why the continuity of is needed (in case it is).","E_1,...,E_n F \Omega \subset \prod E_k f: \Omega \to F a \in \Omega f a f \Omega f a f \Omega f a f f",['analysis']
9,if $A$ has Lebesgue outer measure $0$ then so does $B=\left\{x^2: x\in A \right\}$,if  has Lebesgue outer measure  then so does,A 0 B=\left\{x^2: x\in A \right\},"Let $A$ be a subset of $\mathbb{R}$ such that $\mu^*(A)=0$, where $\mu^*$ is the Lebesgue outer measure. Prove that if $B=\left\{x^2: x\in A \right\}$, then $\mu^*(B)=0$. Recall that the Lebesgue outer measure of a subset $A$ of $\mathbb{R}$ is defined as $$ \mu^*(A)=\inf \left\{\sum_{i \geq 1} (b_i-a_i): (a_i, b_i) \subseteq \mathbb{R}, A\subseteq \cup_{i\geq 1} (a_i, b_i)     \right\}.  $$ Since $B=\left\{x^2: x\in A \right\}$, then if $A\subseteq \cup_{i\geq 1} (a_i, b_i)$, then $B\subseteq \cup_{i\geq 1} (a_i^2, b_i^2)$. Then $$ \mu^*(B)=\inf \left\{\sum_{i \geq 1} (b_i^2-a_i^2): (a_i^2, b_i^2) \subseteq \mathbb{R}, A\subseteq \cup_{i\geq 1} (a_i, b_i)     \right\}=\inf \left\{\sum_{i \geq 1} (b_i+a_i)(b_i-a_i): (a_i^2, b_i^2) \subseteq \mathbb{R}, A\subseteq \cup_{i\geq 1} (a_i, b_i)     \right\}.    $$ From here I am not sure of what to do, although it seems like I am on the verge of getting the solution.","Let $A$ be a subset of $\mathbb{R}$ such that $\mu^*(A)=0$, where $\mu^*$ is the Lebesgue outer measure. Prove that if $B=\left\{x^2: x\in A \right\}$, then $\mu^*(B)=0$. Recall that the Lebesgue outer measure of a subset $A$ of $\mathbb{R}$ is defined as $$ \mu^*(A)=\inf \left\{\sum_{i \geq 1} (b_i-a_i): (a_i, b_i) \subseteq \mathbb{R}, A\subseteq \cup_{i\geq 1} (a_i, b_i)     \right\}.  $$ Since $B=\left\{x^2: x\in A \right\}$, then if $A\subseteq \cup_{i\geq 1} (a_i, b_i)$, then $B\subseteq \cup_{i\geq 1} (a_i^2, b_i^2)$. Then $$ \mu^*(B)=\inf \left\{\sum_{i \geq 1} (b_i^2-a_i^2): (a_i^2, b_i^2) \subseteq \mathbb{R}, A\subseteq \cup_{i\geq 1} (a_i, b_i)     \right\}=\inf \left\{\sum_{i \geq 1} (b_i+a_i)(b_i-a_i): (a_i^2, b_i^2) \subseteq \mathbb{R}, A\subseteq \cup_{i\geq 1} (a_i, b_i)     \right\}.    $$ From here I am not sure of what to do, although it seems like I am on the verge of getting the solution.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
10,Divergence operator of higher order and intrinsic point of view,Divergence operator of higher order and intrinsic point of view,,"Let $\underline{u}$ be a $1$ - order tensor (say a column vector) I want to prove that : $\underline{\operatorname{div}}  \left( (\underline{\underline{\operatorname{grad}}} \, \underline{u})^T\right)= \underline{\operatorname{grad}} \, (\operatorname{div} \underline{u})$ where $\underline{\operatorname{grad}}$ is the one order gradient (the usual one) and $\underline{\underline{\operatorname{grad}}}$ is the second order gradient (it is the jacobian matrix) I want a proof that those not involve any coordinates. Because it is easy to find a proof using for example cartesian coordinates. Here is what I've done so far : Since for any volume $V$ we have : $$\iiint _V \underline{\operatorname{div}}  \left( (\underline{\underline{\operatorname{grad}}} \, \underline{u})^T\right) \; \mathrm{d}V = \iint_{S} (\underline{\underline{\operatorname{grad}}} \, \underline{u})^T \cdot \underline{n}  \; \mathrm{d}S$$ (it it the definition of $\underline{\operatorname{div}}$)  where $\underline{n}$ is the normal vector to the surface $S$ at the limit of the volume $V$. And we can write : $$\iiint_V \underline{\operatorname{grad}} \, (\operatorname{div} \underline{u}) \; \mathrm{d}V = \iint_S (\operatorname{div} \underline{u} )\underline{n} \; \mathrm{d}S$$ Thus it is sufficient to prove that : $$\iint_{S} (\underline{\underline{\operatorname{grad}}} \, \underline{u})^T \cdot \underline{n}  \; \mathrm{d}S =\iint_S (\operatorname{div} \underline{u} )\underline{n} \; \mathrm{d} S$$ The problem is that we don't have $ (\underline{\underline{\operatorname{grad}}} \, \underline{u})^T \cdot \underline{n} = (\operatorname{div} \underline{u} )\underline{n} $ So how can I finish the proof ? If you need details, please tell me. Thank you.","Let $\underline{u}$ be a $1$ - order tensor (say a column vector) I want to prove that : $\underline{\operatorname{div}}  \left( (\underline{\underline{\operatorname{grad}}} \, \underline{u})^T\right)= \underline{\operatorname{grad}} \, (\operatorname{div} \underline{u})$ where $\underline{\operatorname{grad}}$ is the one order gradient (the usual one) and $\underline{\underline{\operatorname{grad}}}$ is the second order gradient (it is the jacobian matrix) I want a proof that those not involve any coordinates. Because it is easy to find a proof using for example cartesian coordinates. Here is what I've done so far : Since for any volume $V$ we have : $$\iiint _V \underline{\operatorname{div}}  \left( (\underline{\underline{\operatorname{grad}}} \, \underline{u})^T\right) \; \mathrm{d}V = \iint_{S} (\underline{\underline{\operatorname{grad}}} \, \underline{u})^T \cdot \underline{n}  \; \mathrm{d}S$$ (it it the definition of $\underline{\operatorname{div}}$)  where $\underline{n}$ is the normal vector to the surface $S$ at the limit of the volume $V$. And we can write : $$\iiint_V \underline{\operatorname{grad}} \, (\operatorname{div} \underline{u}) \; \mathrm{d}V = \iint_S (\operatorname{div} \underline{u} )\underline{n} \; \mathrm{d}S$$ Thus it is sufficient to prove that : $$\iint_{S} (\underline{\underline{\operatorname{grad}}} \, \underline{u})^T \cdot \underline{n}  \; \mathrm{d}S =\iint_S (\operatorname{div} \underline{u} )\underline{n} \; \mathrm{d} S$$ The problem is that we don't have $ (\underline{\underline{\operatorname{grad}}} \, \underline{u})^T \cdot \underline{n} = (\operatorname{div} \underline{u} )\underline{n} $ So how can I finish the proof ? If you need details, please tell me. Thank you.",,"['real-analysis', 'analysis', 'vector-analysis', 'tensors']"
11,Show $f$ is locally invertible if $f = L + g$ and $|g(x)| \le M|x|^2$,Show  is locally invertible if  and,f f = L + g |g(x)| \le M|x|^2,"Let $f, g, L: \mathbb{R}^n \to \mathbb{R}^n$ and $L$ is a linear isomorphism, and let $|g(x)| \le M|x|^2$ on $\mathbb{R}^n$ for some $M > 0$. Prove that $f$ is locally invertible at $0$, i.e. $f$ is invertible on some open neighborhood of $0$. I tried to use the inverse function theorem, but I failed because $f$ may be differentiable on no open neighborhood of $0$. How can I solve this?","Let $f, g, L: \mathbb{R}^n \to \mathbb{R}^n$ and $L$ is a linear isomorphism, and let $|g(x)| \le M|x|^2$ on $\mathbb{R}^n$ for some $M > 0$. Prove that $f$ is locally invertible at $0$, i.e. $f$ is invertible on some open neighborhood of $0$. I tried to use the inverse function theorem, but I failed because $f$ may be differentiable on no open neighborhood of $0$. How can I solve this?",,['analysis']
12,Given $g$ find an $f$ which is solution for $L f = g$. How do I do this?,Given  find an  which is solution for . How do I do this?,g f L f = g,"I am learning about Stochastic processes. To characterize uniqueness of solutions to a given Stochastic differential equation, I need to find for each continuous function $g :\Bbb{R}^2_+ \to \Bbb{R}$ a function $f:\Bbb{R}^2_+ \to \Bbb{R}$ such that $$ -\gamma f + (\partial_x f) (y-x) + (\partial_y f) (x-y) + \frac{1}{2} \bigg((\partial_{xx} f) x + (\partial_{yy} f) y\bigg) = g $$ This is I believe an inverse problem which has to do with spectral techniques such as proving that $\gamma  \in \rho(L)$ where $$L f = (\partial_x f) (y-x) + (\partial_y f) (x-y) + \frac{1}{2} \bigg((\partial_{xx} f) x + (\partial_{yy} f) y\bigg)$$ Maybe this can be solved more straightforwardly by finding a Green's function $G(x,y,\tilde{x}, \tilde{y})$ that satisfies  $$ -\gamma G  - L G = \delta_{x -\tilde{x}} \delta_{y - \tilde{y}}$$ or something similar and then defining $$f = G*g. $$ How can we find such an $f$? Does it exist? Is there any good reference on the literature for this subject? Note: Here $(\partial_x f) (y-x) $ means $(\partial_x f(x,y)) \cdot (y-x)$","I am learning about Stochastic processes. To characterize uniqueness of solutions to a given Stochastic differential equation, I need to find for each continuous function $g :\Bbb{R}^2_+ \to \Bbb{R}$ a function $f:\Bbb{R}^2_+ \to \Bbb{R}$ such that $$ -\gamma f + (\partial_x f) (y-x) + (\partial_y f) (x-y) + \frac{1}{2} \bigg((\partial_{xx} f) x + (\partial_{yy} f) y\bigg) = g $$ This is I believe an inverse problem which has to do with spectral techniques such as proving that $\gamma  \in \rho(L)$ where $$L f = (\partial_x f) (y-x) + (\partial_y f) (x-y) + \frac{1}{2} \bigg((\partial_{xx} f) x + (\partial_{yy} f) y\bigg)$$ Maybe this can be solved more straightforwardly by finding a Green's function $G(x,y,\tilde{x}, \tilde{y})$ that satisfies  $$ -\gamma G  - L G = \delta_{x -\tilde{x}} \delta_{y - \tilde{y}}$$ or something similar and then defining $$f = G*g. $$ How can we find such an $f$? Does it exist? Is there any good reference on the literature for this subject? Note: Here $(\partial_x f) (y-x) $ means $(\partial_x f(x,y)) \cdot (y-x)$",,"['analysis', 'reference-request', 'differential-operators', 'inverse-problems']"
13,Prove that $a_n$ $\rightarrow$ L $\implies$ |$a_n$| $\rightarrow$ |L|,Prove that   L  ||  |L|,a_n \rightarrow \implies a_n \rightarrow,The book I am using for my Advance Calculus course is Introduction to Analysis by Arthur Mattuck. Prove that $a_n$ $\rightarrow$ L $\implies$ |$a_n$| $\rightarrow$ |L|. We are needed to make a cases to use theorem 5.3B This is my rough proof to this question. I was wondering if anybody can look over it and see if I made a mistake or if there is a simpler way of doing this problem. I want to thank you ahead of time it is greatly appreciated.So lets begin: Proof:,The book I am using for my Advance Calculus course is Introduction to Analysis by Arthur Mattuck. Prove that $a_n$ $\rightarrow$ L $\implies$ |$a_n$| $\rightarrow$ |L|. We are needed to make a cases to use theorem 5.3B This is my rough proof to this question. I was wondering if anybody can look over it and see if I made a mistake or if there is a simpler way of doing this problem. I want to thank you ahead of time it is greatly appreciated.So lets begin: Proof:,,"['real-analysis', 'analysis']"
14,Can one solve $\int_{0}^{\infty} \frac{\sin(x)}{x} dx$ *from its Taylor series antiderivative directly*?,Can one solve  *from its Taylor series antiderivative directly*?,\int_{0}^{\infty} \frac{\sin(x)}{x} dx,"This question was inspired by this question: Evaluating the integral $\int_{0}^{\infty} \frac{\sin{x}}{x} \ dx = \frac{\pi}{2}$? Well, can anyone prove this without using Residue theory. I actually thought of doing this:   $$\int_{0}^{\infty} \frac{\sin x}{x} \, dx = \lim_{t \to \infty} \int_{0}^{t} \frac{1}{t} \left( t - \frac{t^3}{3!} + \frac{t^5}{5!} + \cdots \right) \, dt$$   but I don't see how $\pi$ comes here, since we need the answer to be equal to $\frac{\pi}{2}$. Answers were given to the stated question -- how to prove without using Residue theory. Yet the quote suggests an obvious follow-up question: can you prove the integral from the Taylor series expansion directly, somehow ?","This question was inspired by this question: Evaluating the integral $\int_{0}^{\infty} \frac{\sin{x}}{x} \ dx = \frac{\pi}{2}$? Well, can anyone prove this without using Residue theory. I actually thought of doing this:   $$\int_{0}^{\infty} \frac{\sin x}{x} \, dx = \lim_{t \to \infty} \int_{0}^{t} \frac{1}{t} \left( t - \frac{t^3}{3!} + \frac{t^5}{5!} + \cdots \right) \, dt$$   but I don't see how $\pi$ comes here, since we need the answer to be equal to $\frac{\pi}{2}$. Answers were given to the stated question -- how to prove without using Residue theory. Yet the quote suggests an obvious follow-up question: can you prove the integral from the Taylor series expansion directly, somehow ?",,"['calculus', 'analysis', 'definite-integrals', 'improper-integrals']"
15,"How do I prove that $\,\left\lvert\,\int f \,dg\, \right\rvert \leq \int\left\lvert f \right\rvert \,d\left\lvert g\right\rvert$?",How do I prove that ?,"\,\left\lvert\,\int f \,dg\, \right\rvert \leq \int\left\lvert f \right\rvert \,d\left\lvert g\right\rvert","Let $\;g:[a,b]\rightarrow \mathbb{C}\,$ be a function of bounded variation. Let $\;f:[a,b]\rightarrow \mathbb{C}\,$ be a function Riemann-integrable along $g$. Define $\,\alpha\left(x\right) = V_a^x \left(g\right)$, for all $\,x\in\left[a,b\right]$. How do I prove that $\displaystyle\,\left\lvert\,\int f \,dg\, \right\rvert \leq  \int \big\lvert\, f\, \big\rvert \,d\alpha$? I have proven it for the case $\,g\,$ is monotonic, but in general how do I prove it? Set $\,g=h+ik\,$ where $\,h,k\,$ are reals. Then, set $\,h_1\left(x\right)=1/2\big(V_a^x\left(h\right) +h\left(x\right)\big)\, $ and $\,h_2\left(x\right)=1/2\big(V_a^x\left(h\right) - h\left(x\right)\big)\,$ and $\,k_1\left(x\right) = 1/2\big(V_a^x \left(k\right) +k\left(x\right)\big)\,$ and $\,k_2\left(x\right) = 1/2\big(V_a^x \left(k\right) - k\left(x\right)\big)$. Then,  \begin{align} \left\lvert\int_a^b f \,dg \right\rvert  & =  \left\lvert\int_a^b f \,dh_1 - \int_a^b f\, dh_2 + i\left(\int_a^b f\, dk_1 -\int_a^b f \,dk_2\right) \right\rvert \\ & \leq  \left\lvert\int_a^b f \,dh_1 \right\rvert + \left\lvert\int_a^b f \,dh_2 \right\rvert + \left\lvert\int_a^b f\, dk_1 \right\rvert + \left\lvert\int_a^b f \,dk_2 \right\rvert  \\  & \leq  \int_a^b \big\lvert\, f\, \big\rvert \,dh_1 + \int_a^b \big\lvert\, f\, \big\rvert\,dh_2 + \int_a^b  \big\lvert\, f\, \big\rvert\,dk_1 + \int_a^b \big\lvert\, f\, \big\rvert \,dk_2  \\  & =  \int_a^b \big\lvert \,f\left(x\right) \big\rvert \,dV_a^x(h) + \int_a^b \big\lvert \,f\left(x\right)\big\rvert\,dV_a^x\left(k\right)  \\  & =  \int_a^b \big\lvert \,f\left(x\right)\big\rvert \, d\big( V_a^x\left(h\right) + V_a^x \left(k\right) \big). \end{align} However, $\,V_a^x\left(h\right)+ V_a^x\left(k\right)\geq V_a^x\left(g\right)\,$ for all $\,x$, we cannot relate the above inequalities to prove the inequality in question. How do I prove it? Thank you in advance.","Let $\;g:[a,b]\rightarrow \mathbb{C}\,$ be a function of bounded variation. Let $\;f:[a,b]\rightarrow \mathbb{C}\,$ be a function Riemann-integrable along $g$. Define $\,\alpha\left(x\right) = V_a^x \left(g\right)$, for all $\,x\in\left[a,b\right]$. How do I prove that $\displaystyle\,\left\lvert\,\int f \,dg\, \right\rvert \leq  \int \big\lvert\, f\, \big\rvert \,d\alpha$? I have proven it for the case $\,g\,$ is monotonic, but in general how do I prove it? Set $\,g=h+ik\,$ where $\,h,k\,$ are reals. Then, set $\,h_1\left(x\right)=1/2\big(V_a^x\left(h\right) +h\left(x\right)\big)\, $ and $\,h_2\left(x\right)=1/2\big(V_a^x\left(h\right) - h\left(x\right)\big)\,$ and $\,k_1\left(x\right) = 1/2\big(V_a^x \left(k\right) +k\left(x\right)\big)\,$ and $\,k_2\left(x\right) = 1/2\big(V_a^x \left(k\right) - k\left(x\right)\big)$. Then,  \begin{align} \left\lvert\int_a^b f \,dg \right\rvert  & =  \left\lvert\int_a^b f \,dh_1 - \int_a^b f\, dh_2 + i\left(\int_a^b f\, dk_1 -\int_a^b f \,dk_2\right) \right\rvert \\ & \leq  \left\lvert\int_a^b f \,dh_1 \right\rvert + \left\lvert\int_a^b f \,dh_2 \right\rvert + \left\lvert\int_a^b f\, dk_1 \right\rvert + \left\lvert\int_a^b f \,dk_2 \right\rvert  \\  & \leq  \int_a^b \big\lvert\, f\, \big\rvert \,dh_1 + \int_a^b \big\lvert\, f\, \big\rvert\,dh_2 + \int_a^b  \big\lvert\, f\, \big\rvert\,dk_1 + \int_a^b \big\lvert\, f\, \big\rvert \,dk_2  \\  & =  \int_a^b \big\lvert \,f\left(x\right) \big\rvert \,dV_a^x(h) + \int_a^b \big\lvert \,f\left(x\right)\big\rvert\,dV_a^x\left(k\right)  \\  & =  \int_a^b \big\lvert \,f\left(x\right)\big\rvert \, d\big( V_a^x\left(h\right) + V_a^x \left(k\right) \big). \end{align} However, $\,V_a^x\left(h\right)+ V_a^x\left(k\right)\geq V_a^x\left(g\right)\,$ for all $\,x$, we cannot relate the above inequalities to prove the inequality in question. How do I prove it? Thank you in advance.",,"['real-analysis', 'integration', 'analysis', 'riemann-integration']"
16,Ramanujan Infinity sum functional equations,Ramanujan Infinity sum functional equations,,i was reading about the mellin transform ans i found the following $$\sum _{k=1}^{\infty } \left(\frac{e^{-k x}}{e^{-2 k x}+1}-\frac{\pi  \text{sech}\left(\frac{\pi ^2 k}{x}\right)}{2 x}\right)=\frac{\pi }{4 x}-\frac{1}{4}$$ but i do not how to show? it seem interesting i try firt ti get the mellin transform it is equivalent to $$\sum _{k=1}^{\infty } \left(\frac{1}{2} \text{sech}(k x)-\frac{\pi  \text{sech}\left(\frac{\pi ^2 k}{x}\right)}{2 x}\right)=\frac{\pi }{4 x}-\frac{1}{4}$$ get by ramanujan I found a similar formula $$\sum _{k=1}^{\infty } \frac{\pi  (-1)^{k+1} \coth \left(\frac{\pi ^2 (2 k-1)}{2 x}\right)}{2 x}-\frac{1}{4}=\sum _{k=1}^{\infty } \frac{1}{2} \text{sech}(k x)$$ could you show it? Also i like to include $$\sum _{k=1}^{\infty } -\frac{16 x^2 \left(\pi  \coth \left(\frac{\pi ^2 (2 k-1)}{2 x}\right)-\pi \right)}{\pi ^5 (2 k-1)^5}-\frac{1}{180} \left(x \left(\pi ^2-6 x^2\right) \left(x^2+\pi ^2\right)-90 \zeta (5)\right)=\sum _{k=1}^{\infty } \frac{1}{k^5 \left(e^{2 k x}+1\right)}$$ and $$\sum _{k=1}^{\infty } \frac{4 x^2 \left(\pi  \coth \left(\frac{\pi ^2 (2 k-1)}{2 x}\right)-\pi \right)}{\pi ^3 (2 k-1)^3}-\frac{1}{12} \left(x^3+\pi ^2 x-6 \zeta (3)\right)=\sum _{k=1}^{\infty } \frac{1}{k^3 \left(e^{2 k x}+1\right)}$$,i was reading about the mellin transform ans i found the following $$\sum _{k=1}^{\infty } \left(\frac{e^{-k x}}{e^{-2 k x}+1}-\frac{\pi  \text{sech}\left(\frac{\pi ^2 k}{x}\right)}{2 x}\right)=\frac{\pi }{4 x}-\frac{1}{4}$$ but i do not how to show? it seem interesting i try firt ti get the mellin transform it is equivalent to $$\sum _{k=1}^{\infty } \left(\frac{1}{2} \text{sech}(k x)-\frac{\pi  \text{sech}\left(\frac{\pi ^2 k}{x}\right)}{2 x}\right)=\frac{\pi }{4 x}-\frac{1}{4}$$ get by ramanujan I found a similar formula $$\sum _{k=1}^{\infty } \frac{\pi  (-1)^{k+1} \coth \left(\frac{\pi ^2 (2 k-1)}{2 x}\right)}{2 x}-\frac{1}{4}=\sum _{k=1}^{\infty } \frac{1}{2} \text{sech}(k x)$$ could you show it? Also i like to include $$\sum _{k=1}^{\infty } -\frac{16 x^2 \left(\pi  \coth \left(\frac{\pi ^2 (2 k-1)}{2 x}\right)-\pi \right)}{\pi ^5 (2 k-1)^5}-\frac{1}{180} \left(x \left(\pi ^2-6 x^2\right) \left(x^2+\pi ^2\right)-90 \zeta (5)\right)=\sum _{k=1}^{\infty } \frac{1}{k^5 \left(e^{2 k x}+1\right)}$$ and $$\sum _{k=1}^{\infty } \frac{4 x^2 \left(\pi  \coth \left(\frac{\pi ^2 (2 k-1)}{2 x}\right)-\pi \right)}{\pi ^3 (2 k-1)^3}-\frac{1}{12} \left(x^3+\pi ^2 x-6 \zeta (3)\right)=\sum _{k=1}^{\infty } \frac{1}{k^3 \left(e^{2 k x}+1\right)}$$,,[]
17,Proof of Gruss inequality,Proof of Gruss inequality,,"I've been reading articles that use the Gruss inequality for some time now, but I can't seem to find a proof of it anywhere. The only source I could find that actually has the proof is the original paper by Gruss, but the article costs $40 and I don't want to pay that much. The statement is as follows: Let $f,g:[a,b]\to\mathbb{R}$ be integrable functions on $[a,b]$ such that $$ \phi \leq f(x) \leq \Phi, \enspace \gamma \leq g(x) \leq \Gamma, \enspace \forall x \in [a,b], \enspace \phi,\Phi,\gamma,\Gamma \in \mathbb{R}. $$ Then the following inequality holds: $$ \left| T(f,g) \right| \leq \frac{1}{4}(\Phi-\phi)(\Gamma-\gamma), $$ where $$ T(f,g) := \frac{1}{b-a} \int\limits_a^b \! f(x)g(x)\,\mathrm{d} x - \frac{1}{b-a} \int\limits_a^b \! f(x)\,\mathrm{d} x \cdot \frac{1}{b-a} \int\limits_a^b \! g(x)\,\mathrm{d} x. $$ I've tried following these steps of the proof but I'm stuck on the very first one: First show that $T(f,f)\geq 0$, $T(g,g)\geq 0$ and $T^2(f,g)\leq T(f,f)T(g,g)$. Then show that $T(f,f)\leq \frac{1}{4}(\Phi-\phi)^2$ and $T(g,g)\leq \frac{1}{4}(\Gamma-\gamma)^2$. Combining these steps then yields the desired inequality. I know that the policy of this site is that you should show your work first, but right now the only thing I could show is a failed attempt at using Cauchy's or Holder's inequality to prove the statement. $\textbf{EDIT:}$ I found a proof so I'll post it if anyone gets curious. Firstly, we use this identity proven by K.A.Andreief, which states that the Chebyshev functional can be written as $$ T(f,g) = \frac{1}{2(b-a)^2} \int\limits_a^b \! \int\limits_a^b \! (f(x)-f(y))(g(x)-g(y)) \mathrm{d} x \mathrm{d} y. $$ Now that it is written this way, by the Cauchy-Schwarz inequality for double integrals we have $$ T^2(f,g)\leq T(f,f)T(g,g). $$ By $A(f)$ we denote the arithmetical integral mean: $$ A(f) = \frac{1}{b-a} \int\limits_a^b \! f(x) \mathrm{d} x, $$ so by the inequality between the arithmetic and quadratic mean we obtain $$ T(f,f)=A(f^2)-A^2(f) \geq 0. $$ One can easily see that the following identity is also true $$ T(f,f) = (\Phi-A(f))(A(f)-\phi) - \frac{1}{b-a} \int\limits_a^b \! (\Phi-f(x))(f(x)-\phi) \mathrm{d} x, $$ which then implies $$ T(f,f) \leq (\Phi-A(f))(A(f)-\phi). $$ In the same way we can show that $$ T(g,g) \leq (\Gamma-A(g))(A(g)-\gamma) $$ is valid. Combining previous steps, we obtain $$ T^2(f,g) \leq (\Phi-A(f))(A(f)-\phi)(\Gamma-A(g))(A(g)-\gamma). $$ Finally, by the A-G inequality, it is obvious that $$ (\Phi-A(f))(A(f)-\phi)\leq \frac{1}{4}(\Phi-\phi)^2, \enspace (\Gamma-A(g))(A(g)-\gamma)\leq \frac{1}{4}(\Gamma-\gamma)^2 $$ so we have $$ T^2(f,g) \leq \frac{1}{16} (\Phi-\phi)^2(\Gamma-\gamma)^2, $$ that is $$ |T(f,g)|\leq \frac{1}{4}(\Phi-\phi)(\Gamma-\gamma). $$","I've been reading articles that use the Gruss inequality for some time now, but I can't seem to find a proof of it anywhere. The only source I could find that actually has the proof is the original paper by Gruss, but the article costs $40 and I don't want to pay that much. The statement is as follows: Let $f,g:[a,b]\to\mathbb{R}$ be integrable functions on $[a,b]$ such that $$ \phi \leq f(x) \leq \Phi, \enspace \gamma \leq g(x) \leq \Gamma, \enspace \forall x \in [a,b], \enspace \phi,\Phi,\gamma,\Gamma \in \mathbb{R}. $$ Then the following inequality holds: $$ \left| T(f,g) \right| \leq \frac{1}{4}(\Phi-\phi)(\Gamma-\gamma), $$ where $$ T(f,g) := \frac{1}{b-a} \int\limits_a^b \! f(x)g(x)\,\mathrm{d} x - \frac{1}{b-a} \int\limits_a^b \! f(x)\,\mathrm{d} x \cdot \frac{1}{b-a} \int\limits_a^b \! g(x)\,\mathrm{d} x. $$ I've tried following these steps of the proof but I'm stuck on the very first one: First show that $T(f,f)\geq 0$, $T(g,g)\geq 0$ and $T^2(f,g)\leq T(f,f)T(g,g)$. Then show that $T(f,f)\leq \frac{1}{4}(\Phi-\phi)^2$ and $T(g,g)\leq \frac{1}{4}(\Gamma-\gamma)^2$. Combining these steps then yields the desired inequality. I know that the policy of this site is that you should show your work first, but right now the only thing I could show is a failed attempt at using Cauchy's or Holder's inequality to prove the statement. $\textbf{EDIT:}$ I found a proof so I'll post it if anyone gets curious. Firstly, we use this identity proven by K.A.Andreief, which states that the Chebyshev functional can be written as $$ T(f,g) = \frac{1}{2(b-a)^2} \int\limits_a^b \! \int\limits_a^b \! (f(x)-f(y))(g(x)-g(y)) \mathrm{d} x \mathrm{d} y. $$ Now that it is written this way, by the Cauchy-Schwarz inequality for double integrals we have $$ T^2(f,g)\leq T(f,f)T(g,g). $$ By $A(f)$ we denote the arithmetical integral mean: $$ A(f) = \frac{1}{b-a} \int\limits_a^b \! f(x) \mathrm{d} x, $$ so by the inequality between the arithmetic and quadratic mean we obtain $$ T(f,f)=A(f^2)-A^2(f) \geq 0. $$ One can easily see that the following identity is also true $$ T(f,f) = (\Phi-A(f))(A(f)-\phi) - \frac{1}{b-a} \int\limits_a^b \! (\Phi-f(x))(f(x)-\phi) \mathrm{d} x, $$ which then implies $$ T(f,f) \leq (\Phi-A(f))(A(f)-\phi). $$ In the same way we can show that $$ T(g,g) \leq (\Gamma-A(g))(A(g)-\gamma) $$ is valid. Combining previous steps, we obtain $$ T^2(f,g) \leq (\Phi-A(f))(A(f)-\phi)(\Gamma-A(g))(A(g)-\gamma). $$ Finally, by the A-G inequality, it is obvious that $$ (\Phi-A(f))(A(f)-\phi)\leq \frac{1}{4}(\Phi-\phi)^2, \enspace (\Gamma-A(g))(A(g)-\gamma)\leq \frac{1}{4}(\Gamma-\gamma)^2 $$ so we have $$ T^2(f,g) \leq \frac{1}{16} (\Phi-\phi)^2(\Gamma-\gamma)^2, $$ that is $$ |T(f,g)|\leq \frac{1}{4}(\Phi-\phi)(\Gamma-\gamma). $$",,"['real-analysis', 'integration', 'analysis', 'inequality']"
18,"Find a formula for $f''$ in terms of $f$, where $f\gt 0$ and $(f')^2=f-\frac{1}{f^2}.$","Find a formula for  in terms of , where  and",f'' f f\gt 0 (f')^2=f-\frac{1}{f^2}.,"Problem: Suppose that a function $f \gt 0$ has the property    $$ (f')^2=f-\frac{1}{f^2} $$   Find a formula for $f''$ in terms of $f$. Hint: Use Theorem 7. Theorem 7: Suppose that $f$ is continuous at $a$, and that $f'(x)$ exists for all $x$ in some interval containing $a$, except perhaps for $x=a$. Suppose, moreover, that $\lim_{x\to a}f'(x)$ exists. Then $f'(a)$ also exists, and    $$ f'(a)=\lim_{x\to a}f'(x) $$ I think this problem assumes $f''$ exists everywhere. This is my work so far. By Chain Rule, $2f'f''=f'+\frac{2f'}{f^3}$. Dividing by $f'$, we get $f''=1/2+1/f^3$, at all points $x$ where $f'(x)\neq 0.$ Since $(f')^2=\frac{f^3-1}{f^2}$, we have $f'(x)=0$ only for $f(x)=1$. So I need to compute $f''(x)$ for such $x$. Using the hint, I first guessed that Theorem $7$(applied to $f'$) implies that the formula holds in this case also, with $f''(x)=\lim_{y\to x}1/2+1/f^3(y)=\frac{1}{2}+1=\frac{3}{2}$. However, upon close inspection, I realized that this may not be true, as for any neighborhood around such $x$, there may be another $x_0$ such that $f(x_0)=1$, and in such case I cannot compute the limit as above. How can I resolve this situation, or is the problem wrong? I would greatly appreciate any help.","Problem: Suppose that a function $f \gt 0$ has the property    $$ (f')^2=f-\frac{1}{f^2} $$   Find a formula for $f''$ in terms of $f$. Hint: Use Theorem 7. Theorem 7: Suppose that $f$ is continuous at $a$, and that $f'(x)$ exists for all $x$ in some interval containing $a$, except perhaps for $x=a$. Suppose, moreover, that $\lim_{x\to a}f'(x)$ exists. Then $f'(a)$ also exists, and    $$ f'(a)=\lim_{x\to a}f'(x) $$ I think this problem assumes $f''$ exists everywhere. This is my work so far. By Chain Rule, $2f'f''=f'+\frac{2f'}{f^3}$. Dividing by $f'$, we get $f''=1/2+1/f^3$, at all points $x$ where $f'(x)\neq 0.$ Since $(f')^2=\frac{f^3-1}{f^2}$, we have $f'(x)=0$ only for $f(x)=1$. So I need to compute $f''(x)$ for such $x$. Using the hint, I first guessed that Theorem $7$(applied to $f'$) implies that the formula holds in this case also, with $f''(x)=\lim_{y\to x}1/2+1/f^3(y)=\frac{1}{2}+1=\frac{3}{2}$. However, upon close inspection, I realized that this may not be true, as for any neighborhood around such $x$, there may be another $x_0$ such that $f(x_0)=1$, and in such case I cannot compute the limit as above. How can I resolve this situation, or is the problem wrong? I would greatly appreciate any help.",,"['calculus', 'real-analysis', 'analysis', 'derivatives']"
19,gateaux derivative and frechet derivative,gateaux derivative and frechet derivative,,"In calculus, we have the following equation $DF(x,y)=\partial F_xdx+\partial F_ydy$ if $F$ is differentiable. I think such equation still holds for frechet derivative, but not for gateaux derivative. So do we have a similar version of this equation when we are using gateaux derivative to find the derivative of a function like $F(x,y)$? Thanks!","In calculus, we have the following equation $DF(x,y)=\partial F_xdx+\partial F_ydy$ if $F$ is differentiable. I think such equation still holds for frechet derivative, but not for gateaux derivative. So do we have a similar version of this equation when we are using gateaux derivative to find the derivative of a function like $F(x,y)$? Thanks!",,"['calculus', 'analysis', 'multivariable-calculus', 'derivatives']"
20,Determine all functions satisfying $f(x + f(x + y)) + f(xy) = x + f(x + y) +yf(x)$,Determine all functions satisfying,f(x + f(x + y)) + f(xy) = x + f(x + y) +yf(x),Let $\Bbb R $ be the set of real numbers .Determine all functions $f:\Bbb R \rightarrow \Bbb R $ satisfying the equation $$f(x + f(x + y)) + f(xy) = x + f(x + y) +yf(x)$$ for all real numbers $x$ and $y$. My work  $$f(x + f(x + y)) + f(xy) = x + f(x + y) +yf(x)$$ Setting $y=0$ $$f(x + f(x)) + f(0) = x + f(x )$$ $$\implies f(x + f(x))  = x + f(x ) -f(0)$$ $$\implies f(t)  = t -f(0)$$ Setting $x=0$ $$f(f(y)) + f(0) = 0 + f(y) +yf(0)$$ $$f(f(y)) =  f(y) +f(0)(y-1)$$ Setting $y=1$ $$f(x + f(x + 1))  = x + f(x + 1)$$,Let $\Bbb R $ be the set of real numbers .Determine all functions $f:\Bbb R \rightarrow \Bbb R $ satisfying the equation $$f(x + f(x + y)) + f(xy) = x + f(x + y) +yf(x)$$ for all real numbers $x$ and $y$. My work  $$f(x + f(x + y)) + f(xy) = x + f(x + y) +yf(x)$$ Setting $y=0$ $$f(x + f(x)) + f(0) = x + f(x )$$ $$\implies f(x + f(x))  = x + f(x ) -f(0)$$ $$\implies f(t)  = t -f(0)$$ Setting $x=0$ $$f(f(y)) + f(0) = 0 + f(y) +yf(0)$$ $$f(f(y)) =  f(y) +f(0)(y-1)$$ Setting $y=1$ $$f(x + f(x + 1))  = x + f(x + 1)$$,,"['contest-math', 'functional-equations']"
21,Symmetry of the second derivative,Symmetry of the second derivative,,"For the purposes of this question, a function $f$ is differentiable at $x\in \mathbb{R}^d$ iff (i) the directional derivative $\mathrm{D}_vf(x)$ exists for all $v\in \mathrm{T}_x(\mathbb{R}^d)$ and (ii) the map $\mathrm{T}_x(\mathbb{R}^d)\ni v \mapsto \mathrm{D}_vf(x)\in \mathbb{R}$ is linear and continuous.  (You actually get continuity for free, but I include it because it belongs there if you replace $\mathbb{R}^d$ with something infinite dimensional.)  In this case, I write $v^a\nabla _af(x):=\mathrm{D}_vf(x)$.  A covariant tensor field $T_{a_1\ldots a_l}$ is differentiable at $x\in \mathbb{R}^d$ iff $[v_1]^{a_1}\cdots [v_l]^{a_l}T_{a_1\ldots a_l}$ is differentiable for all $[v_1]^a,\ldots ,[v_m]^a\in \mathbb{R}^d$, in which case the derivative is the tensor field of covariant rank $l+1$, $\nabla _aT_{a_1\ldots a_l}$ that sends $l+1$ vectors $v^a,[v_1]^a,\ldots ,[v_l]^a$ to $v^a\nabla _a\left( [v_1]^{a_1}\cdots [v_l]^{a_l}T_{a_1\ldots a_l}\right)$.  (Of course, you use essentially the same definition for arbitrary tensors, but math.stackexchange is not letting me stagger indices and I don't need to know how to differentiate contravariant tensors for the purposes of this question). The question is: If $f$ is twice differentiable at $x$, must we have that $\nabla _a\nabla _bf(x)=\nabla _b\nabla _af(x)$? The definition given above is strictly weaker than Fréchet differentiability , but strictly stronger than Gâteaux differentiability (if you want the examples that show strictness, ask in the comments).  The result is true for the Fréchet derivative (see, e.g., Pugh pg. 281), but false for the Gâteaux derivative (see, e.g., the standard example on Wikipedia).  This is not a counter-example for the definition above because the second derivative fails to be linear at the origin (and so it is not considered to be twice differentiable there).  Furthermore, it is true for all three definitions if we require continuity of $\nabla _a\nabla _bf(x)$ at $x$.  But can we remove this continuity hypothesis? For what it's worth, I think it is false.","For the purposes of this question, a function $f$ is differentiable at $x\in \mathbb{R}^d$ iff (i) the directional derivative $\mathrm{D}_vf(x)$ exists for all $v\in \mathrm{T}_x(\mathbb{R}^d)$ and (ii) the map $\mathrm{T}_x(\mathbb{R}^d)\ni v \mapsto \mathrm{D}_vf(x)\in \mathbb{R}$ is linear and continuous.  (You actually get continuity for free, but I include it because it belongs there if you replace $\mathbb{R}^d$ with something infinite dimensional.)  In this case, I write $v^a\nabla _af(x):=\mathrm{D}_vf(x)$.  A covariant tensor field $T_{a_1\ldots a_l}$ is differentiable at $x\in \mathbb{R}^d$ iff $[v_1]^{a_1}\cdots [v_l]^{a_l}T_{a_1\ldots a_l}$ is differentiable for all $[v_1]^a,\ldots ,[v_m]^a\in \mathbb{R}^d$, in which case the derivative is the tensor field of covariant rank $l+1$, $\nabla _aT_{a_1\ldots a_l}$ that sends $l+1$ vectors $v^a,[v_1]^a,\ldots ,[v_l]^a$ to $v^a\nabla _a\left( [v_1]^{a_1}\cdots [v_l]^{a_l}T_{a_1\ldots a_l}\right)$.  (Of course, you use essentially the same definition for arbitrary tensors, but math.stackexchange is not letting me stagger indices and I don't need to know how to differentiate contravariant tensors for the purposes of this question). The question is: If $f$ is twice differentiable at $x$, must we have that $\nabla _a\nabla _bf(x)=\nabla _b\nabla _af(x)$? The definition given above is strictly weaker than Fréchet differentiability , but strictly stronger than Gâteaux differentiability (if you want the examples that show strictness, ask in the comments).  The result is true for the Fréchet derivative (see, e.g., Pugh pg. 281), but false for the Gâteaux derivative (see, e.g., the standard example on Wikipedia).  This is not a counter-example for the definition above because the second derivative fails to be linear at the origin (and so it is not considered to be twice differentiable there).  Furthermore, it is true for all three definitions if we require continuity of $\nabla _a\nabla _bf(x)$ at $x$.  But can we remove this continuity hypothesis? For what it's worth, I think it is false.",,"['calculus', 'real-analysis', 'analysis', 'derivatives', 'tensors']"
22,On finding special kinds of power series,On finding special kinds of power series,,"Let $\sum a_n x^n$ be a real power series with finite positive radius of convergence $R$, then is it true that for every real number $s>0$ , we can find a real sequence $\{b_n\}$ (depending on $s$, of course) such  that $\sum b_n x^n$ has radius of convergence $s$ and $\sum a_nb_nx^n$ has radius of convergence $R$ ? Moreover, can we find a sequence $\{b_n\}$ such that $\sum b_nx^n$ converges everywhere and $\sum a_nb_n x^n$ has radius of convergence $R$ ?","Let $\sum a_n x^n$ be a real power series with finite positive radius of convergence $R$, then is it true that for every real number $s>0$ , we can find a real sequence $\{b_n\}$ (depending on $s$, of course) such  that $\sum b_n x^n$ has radius of convergence $s$ and $\sum a_nb_nx^n$ has radius of convergence $R$ ? Moreover, can we find a sequence $\{b_n\}$ such that $\sum b_nx^n$ converges everywhere and $\sum a_nb_n x^n$ has radius of convergence $R$ ?",,"['real-analysis', 'analysis']"
23,How to generate a function with multiple variables to fit experimental data?,How to generate a function with multiple variables to fit experimental data?,,"I am researching methods to increase the accuracy of an algorithm that is currently used to analyse radiation patterns as they hit our sensor.  (For the non-physicists, we will mainly see Alphas, Betas and Gammas which are the 3 most common radiation particles) The 3 types of radiation make very distinctly different patterns on our sensor (which can be viewed graphically) which we call clusters (clusters of activated pixels). I can retrieve lots of data about every cluster such as: Density, Roundness, Linearity, (and any others that you may think of?) But my question is: With a set of clusters from previous data sets that I can assign each as a Alpha, Beta or Gamma, is it possible to write an algorithm/equation which gives the most likely type particle for new clusters? For example here is some sample data (that ive made up on the spot, for the real algorithm I would try to use more like 500 pre-classified clusters each with around more like 6 variables each rather than the 3 shown here): Roundness = 0.9, Density = 0.8, Radius = 4, Cluster Type = Alpha Roundness = 0.5, Density = 0.2, Radius = 8, Cluster Type = Beta Roundness = 0.3, Density = 0.4, Radius = 7, Cluster Type = Beta Roundness = 1, Density = 1, Radius = 1, Cluster Type = Gamma With this data, could we go about forming a method for determining the type of cluster that has the variables: Roundness = 0.7, Density = 0.3, Radius = 6? If so, what or where should I research to learn more about such things? I believe this is relevant in fitting algorithms and scalar fields but I am not certain as im still doing pre-university maths. Any help in the right direction would be most appreciated, Thank You!","I am researching methods to increase the accuracy of an algorithm that is currently used to analyse radiation patterns as they hit our sensor.  (For the non-physicists, we will mainly see Alphas, Betas and Gammas which are the 3 most common radiation particles) The 3 types of radiation make very distinctly different patterns on our sensor (which can be viewed graphically) which we call clusters (clusters of activated pixels). I can retrieve lots of data about every cluster such as: Density, Roundness, Linearity, (and any others that you may think of?) But my question is: With a set of clusters from previous data sets that I can assign each as a Alpha, Beta or Gamma, is it possible to write an algorithm/equation which gives the most likely type particle for new clusters? For example here is some sample data (that ive made up on the spot, for the real algorithm I would try to use more like 500 pre-classified clusters each with around more like 6 variables each rather than the 3 shown here): Roundness = 0.9, Density = 0.8, Radius = 4, Cluster Type = Alpha Roundness = 0.5, Density = 0.2, Radius = 8, Cluster Type = Beta Roundness = 0.3, Density = 0.4, Radius = 7, Cluster Type = Beta Roundness = 1, Density = 1, Radius = 1, Cluster Type = Gamma With this data, could we go about forming a method for determining the type of cluster that has the variables: Roundness = 0.7, Density = 0.3, Radius = 6? If so, what or where should I research to learn more about such things? I believe this is relevant in fitting algorithms and scalar fields but I am not certain as im still doing pre-university maths. Any help in the right direction would be most appreciated, Thank You!",,"['analysis', 'algorithms', 'mathematical-modeling']"
24,A few questions on the later chapters in Principles of Mathematical Analysis by Walter Rudin (3rd Edition),A few questions on the later chapters in Principles of Mathematical Analysis by Walter Rudin (3rd Edition),,"I am currently reading Principles of Mathematical Analysis by Walter Rudin (3rd Edition). I am enjoying the book and it's terseness, which isn't an issue for me. What I do have a problem with is that I hear the later chapters (chapters 9-11 specifically) aren't very good. I have a couple of questions. 1)What precisely makes these chapters subpar? Is it still worth reading, but adding in some supplements? 2)I plan on reading Real and Complex Analysis by Walter Rudin after this book, does it fill in the supposed missing quality of the last few chapters? Like, I know Lebesgue Theory and the Lebesgue Integral are important and that apparently isn't treated nicely in Principles of Mathematical Analysis (it would be nice if you could tell me exactly why this is so) either, but is it developed better in Real and Complex Analysis ? Thanks in advance for any response.","I am currently reading Principles of Mathematical Analysis by Walter Rudin (3rd Edition). I am enjoying the book and it's terseness, which isn't an issue for me. What I do have a problem with is that I hear the later chapters (chapters 9-11 specifically) aren't very good. I have a couple of questions. 1)What precisely makes these chapters subpar? Is it still worth reading, but adding in some supplements? 2)I plan on reading Real and Complex Analysis by Walter Rudin after this book, does it fill in the supposed missing quality of the last few chapters? Like, I know Lebesgue Theory and the Lebesgue Integral are important and that apparently isn't treated nicely in Principles of Mathematical Analysis (it would be nice if you could tell me exactly why this is so) either, but is it developed better in Real and Complex Analysis ? Thanks in advance for any response.",,"['real-analysis', 'analysis']"
25,Homework problem about the smallest sigma algebra:,Homework problem about the smallest sigma algebra:,,"Let $\mathscr{E} \subset 2^X $ , then there is a unique smallest $\sigma$ -algebra containing $\mathscr{E} $ . Proof(Attempt): Since $\mathscr{E} \subset 2^X $ , $2^X$ is $\sigma$ -algebra containing $\mathscr{E} $ . Let $\{ \mathscr{F}_{\alpha} \}_{\alpha \in A} $ be a collection of $\sigma-$ algebras with $\mathscr{E} \subseteq \mathscr{F}_{\alpha} $ for all $\alpha \in A $ . We have that $$ \mathscr{M} = \bigcap_{\alpha \in A} \mathscr{F}_{\alpha} $$ is a $\sigma$ -algebra and that $\mathscr{E} \subseteq \mathscr{M} $ . Suppose there is a smaller $\sigma$ -algebra $\mathscr{G} $ with $\mathscr{E} \subseteq \mathscr{G} $ . It is evident that $\mathscr{G} \in \mathscr{F}_{\alpha} $ and so we must have $\mathscr{M} \subseteq \mathscr{G}$ showing that $\mathscr{M} $ is the smallest one. Suppose now there is another $\sigma$ -algebra $\mathscr{U}$ with the same characteristics as $\mathscr{M}$ . Since $\mathscr{M}$ is the smallest $\sigma$ -algebra containing $\mathscr{E}$ , then $\mathscr{U} \subseteq \mathscr{M}$ . Similarly, $\mathscr{M} \subseteq \mathscr{M} $ and so we have established uniqueness. Any feedback is much appreciated.","Let , then there is a unique smallest -algebra containing . Proof(Attempt): Since , is -algebra containing . Let be a collection of algebras with for all . We have that is a -algebra and that . Suppose there is a smaller -algebra with . It is evident that and so we must have showing that is the smallest one. Suppose now there is another -algebra with the same characteristics as . Since is the smallest -algebra containing , then . Similarly, and so we have established uniqueness. Any feedback is much appreciated.",\mathscr{E} \subset 2^X  \sigma \mathscr{E}  \mathscr{E} \subset 2^X  2^X \sigma \mathscr{E}  \{ \mathscr{F}_{\alpha} \}_{\alpha \in A}  \sigma- \mathscr{E} \subseteq \mathscr{F}_{\alpha}  \alpha \in A   \mathscr{M} = \bigcap_{\alpha \in A} \mathscr{F}_{\alpha}  \sigma \mathscr{E} \subseteq \mathscr{M}  \sigma \mathscr{G}  \mathscr{E} \subseteq \mathscr{G}  \mathscr{G} \in \mathscr{F}_{\alpha}  \mathscr{M} \subseteq \mathscr{G} \mathscr{M}  \sigma \mathscr{U} \mathscr{M} \mathscr{M} \sigma \mathscr{E} \mathscr{U} \subseteq \mathscr{M} \mathscr{M} \subseteq \mathscr{M} ,"['real-analysis', 'analysis']"
26,Moving circular disk between two parallel sinusoidal curves,Moving circular disk between two parallel sinusoidal curves,,"Find the largest radius of the circle that can be ""rolled"" between the curves $y = sin(x)$ and $y = sin(x)+1$. After two weeks of research, I finally give up.","Find the largest radius of the circle that can be ""rolled"" between the curves $y = sin(x)$ and $y = sin(x)+1$. After two weeks of research, I finally give up.",,"['analysis', 'metric-geometry', 'euclidean-geometry']"
27,Converge of a sequence in $L^p(\mathbb{R}^3)$,Converge of a sequence in,L^p(\mathbb{R}^3),"Let $f(x)\in L^p(\mathbb{R}^3)$ for every $p\in [1, \infty]$. Let $B(n)\subset \mathbb{R}^3$ be the ball of radius $n$ centered at the origin. I want to show that the sequence $$I_n(x)=\int_{\mathbb{R}^3}e^{-\xi^2/n}\int_{B(n)}f(y)e^{i(x-y)\xi}dyd\xi$$ converges in $L^p(\mathbb{R}^3)$ for every $p\in [1, \infty)$. I know that $L^p$ is complete, so I've tried to show that $I_n(x)$ is a Cauchy sequence. I've written  $$I_n(x)=\int\int \chi_{B(n)}(y)f(y)e^{i(x-y)\xi}e^{-\xi^2/n}dyd\xi$$ and tried to use some inequalities to estimate $I_n-I_m$, but nothing obtained. Any suggestions?","Let $f(x)\in L^p(\mathbb{R}^3)$ for every $p\in [1, \infty]$. Let $B(n)\subset \mathbb{R}^3$ be the ball of radius $n$ centered at the origin. I want to show that the sequence $$I_n(x)=\int_{\mathbb{R}^3}e^{-\xi^2/n}\int_{B(n)}f(y)e^{i(x-y)\xi}dyd\xi$$ converges in $L^p(\mathbb{R}^3)$ for every $p\in [1, \infty)$. I know that $L^p$ is complete, so I've tried to show that $I_n(x)$ is a Cauchy sequence. I've written  $$I_n(x)=\int\int \chi_{B(n)}(y)f(y)e^{i(x-y)\xi}e^{-\xi^2/n}dyd\xi$$ and tried to use some inequalities to estimate $I_n-I_m$, but nothing obtained. Any suggestions?",,"['analysis', 'convergence-divergence', 'lp-spaces']"
28,Proving using AM-GM inequality,Proving using AM-GM inequality,,"If $x,y\in \mathbb{R}$, and $x>y$, how to show $(x-y)^{((x-y)/(2x-y))}\times (x+y)^{((x)/(2x-y))}>x$? I know I have to use AM-GM inequality, but it is not clear how.","If $x,y\in \mathbb{R}$, and $x>y$, how to show $(x-y)^{((x-y)/(2x-y))}\times (x+y)^{((x)/(2x-y))}>x$? I know I have to use AM-GM inequality, but it is not clear how.",,"['real-analysis', 'analysis', 'inequality']"
29,Show that $\limsup$ of $\sin n$ is 1 [duplicate],Show that  of  is 1 [duplicate],\limsup \sin n,This question already has answers here : Showing $\sup \{ \sin n \mid n\in \mathbb N \} =1$ [closed] (3 answers) Closed 6 years ago . I want to prove that the $\limsup\limits_{n\rightarrow \infty} \sin n=1$. I know that $1$ is an upper bound for $\sin n$ but I cannot find a subsequence of $\sin n$ that converges to $1$. Can somebody help me construct such a subsequence?,This question already has answers here : Showing $\sup \{ \sin n \mid n\in \mathbb N \} =1$ [closed] (3 answers) Closed 6 years ago . I want to prove that the $\limsup\limits_{n\rightarrow \infty} \sin n=1$. I know that $1$ is an upper bound for $\sin n$ but I cannot find a subsequence of $\sin n$ that converges to $1$. Can somebody help me construct such a subsequence?,,"['real-analysis', 'analysis', 'numerical-methods']"
30,how to show $\lim \sup_{n \to \infty} \frac{\phi(n+1)}{\phi(n)}= \infty$ and $\lim \inf_{n \to \infty} \frac{\phi(n+1)}{\phi(n)}= 0$?,how to show  and ?,\lim \sup_{n \to \infty} \frac{\phi(n+1)}{\phi(n)}= \infty \lim \inf_{n \to \infty} \frac{\phi(n+1)}{\phi(n)}= 0,"Let $\phi (n)$ be the Euler's totient function . Thenhow to prove that the set $\{\dfrac{\phi(n+1)}{\phi(n)}: n \in \mathbb Z^+\}$ is unbounded above that is how to show $\lim \sup_{n \to \infty} \dfrac{\phi(n+1)}{\phi(n)}= \infty$ ? Moreover , how to show that  $\lim \inf_{n \to \infty} \dfrac{\phi(n+1)}{\phi(n)}= 0$ ?","Let $\phi (n)$ be the Euler's totient function . Thenhow to prove that the set $\{\dfrac{\phi(n+1)}{\phi(n)}: n \in \mathbb Z^+\}$ is unbounded above that is how to show $\lim \sup_{n \to \infty} \dfrac{\phi(n+1)}{\phi(n)}= \infty$ ? Moreover , how to show that  $\lim \inf_{n \to \infty} \dfrac{\phi(n+1)}{\phi(n)}= 0$ ?",,"['sequences-and-series', 'analysis']"
31,Proof verification.,Proof verification.,,"Let $x,y$ be real numbers, with $x < y$. Show that if $x$ and $y$ are rational, then there exists an irrational number $u$ such that $x < u < y$. Now, since $y - x > 0 \implies \frac{y-x}{\sqrt{2}} > 0$ if we take $u =  x + \frac{y-x}{\sqrt{2}}$, we are done. My question is, is this ok? I feel like i need to prove that $x < x + \frac{y-x}{\sqrt{2}} < y$, if that is the case, subtracting $x$ will give the desired result? i.e $0 <\frac{y-x}{\sqrt{2}} < y - x$. A follow up question would be, how would i begin to show that there exists a rational between the reals? This is my attempt, but it's a proof by contradiction; suppose that there is no rational number $q$ between $x$ and $y$, with $x < y$. take the set of real, irrational numbers $\frac{a}{b} + \sqrt{2}$, where $b$ is fixed and $a \in R$ suppose that $x = \frac{a}{b} + \sqrt{2}$ and $y =\frac{a + c}{b} + \sqrt{2}$ for some real $c>0$ then $y - x = c/b$, but if i divide this by 2 and add this to $x$, i get an irrational number. EDIT.  I am aware that there are proofs, such as; We must choose an $N > 1/(y-x) \implies 1/N < (y-x)$, then theres exists an integer multiple of $1/N$ such that $x < r < y$, where $r = M/N$. However,  I would like to know whether my approach holds any water.","Let $x,y$ be real numbers, with $x < y$. Show that if $x$ and $y$ are rational, then there exists an irrational number $u$ such that $x < u < y$. Now, since $y - x > 0 \implies \frac{y-x}{\sqrt{2}} > 0$ if we take $u =  x + \frac{y-x}{\sqrt{2}}$, we are done. My question is, is this ok? I feel like i need to prove that $x < x + \frac{y-x}{\sqrt{2}} < y$, if that is the case, subtracting $x$ will give the desired result? i.e $0 <\frac{y-x}{\sqrt{2}} < y - x$. A follow up question would be, how would i begin to show that there exists a rational between the reals? This is my attempt, but it's a proof by contradiction; suppose that there is no rational number $q$ between $x$ and $y$, with $x < y$. take the set of real, irrational numbers $\frac{a}{b} + \sqrt{2}$, where $b$ is fixed and $a \in R$ suppose that $x = \frac{a}{b} + \sqrt{2}$ and $y =\frac{a + c}{b} + \sqrt{2}$ for some real $c>0$ then $y - x = c/b$, but if i divide this by 2 and add this to $x$, i get an irrational number. EDIT.  I am aware that there are proofs, such as; We must choose an $N > 1/(y-x) \implies 1/N < (y-x)$, then theres exists an integer multiple of $1/N$ such that $x < r < y$, where $r = M/N$. However,  I would like to know whether my approach holds any water.",,['analysis']
32,Question 13 in Taylor's PDE vol III section 16.1.,Question 13 in Taylor's PDE vol III section 16.1.,,"My question comes from Taylor's PDE textbook, volume III. Consider a semilinear hyperbolic system, $u_t=Lu+g(u)$, $u(0)=f$, where $Lu=\sum_j A_j \partial_{x_j}u$, $g(0)=0, \ |g'(u)| \le C$, take $M=\mathbb{T}^n$. Let $u_\epsilon$ be a solution to an approximating equation: $$u_{\epsilon_t} = J_{\epsilon}LJ_\epsilon u_\epsilon +J_\epsilon g(J_\epsilon u_\epsilon), \ u_\epsilon(0)=f$$ Show that: $$d/dt\| u_\epsilon \|_{L^2}^2 \le C \| u_\epsilon \|_{L^2}^2 , \ d/dt \| \nabla u_\epsilon \|_{L^2}^2 \le C \| \nabla u_\epsilon \|_{L^2}^2$$ Deduce that there exists a solution to the approximating equation for any $\epsilon>0$ which is defined for all $t\in \mathbb{R}$ and for all compact $I \subset \mathbb{R}$: $u_\epsilon$ bounded in $L^{\infty}(I,H^1(M))\cap Lip(I,L^2(M))$. The above question I solved prior to question 13. 13) Deduce that passing that passing to a subsequence $u_{\epsilon_k}$ we have a limit point $u\in L^{\infty}_{loc}(\mathbb{R},H^1(M))\cap Lip_{loc}(\mathbb{R}, L^2(M))$. s.t $u_{\epsilon_k} \rightarrow u \in C(I,L^2(M))$ in norm for all compact $I\subset \mathbb{R}$; hence, $g(J_{\epsilon_k}u_{\epsilon_k})\rightarrow g(u)$ in $C(\mathbb{R}, L^2(M))$, and $u$ solves the original semilinear hyperbolic pde. Examine the issue of uniqueness. My problem is in exercise 13, I thought that the first part is a simple conclusion from problem 12 of Arzela-Ascoli theorem, but I am not sure it that's much easy. As for the uniqueness part, I believe we need to use here Gronwall's inequality on the norm, we get for if two solutions of the semilinear hyperbolic pde, $u1,u2$, then by denoting $w=u1-u2$, and $h(w)=g(u1)-g(u2): $$\| w(t) \|_{L^2}^2  = (w(0),w(t)) + \int_0^t (Lw(s)+h(w(s)), w(t))ds$$ I need somehow to estimate $(Lw(s),w(t))$ from above, which I don't see how. I see how to esitamte $(h(w(s)),w(t)$, by Cauchy-Schwarz we have $(h(w(s)),w(t))\le \|h(w(s))\|_{L^2} \| w(t)\|_{L^2}$, $\| h(w(s)) \|_{L^2} = \| w(s)\int_0^1 h'(rw)dr \|_{L^2} \le \| w(s)\|_{L^2} \| \int_0^1 h'(rw)dr\|_{L^{\infty}} \le 2C \| w(s) \|_{L^2} \sup_{v\in \mathbb{R}^n} |h'(v)| \le 2C\| w(s) \|_{L^2}$, since $|h'(v)|=|g'(u2+v)-g'(u2)| \le C+C =2C$. I appreciate your help here. Thanks!!! P.S I forgot to mention that $J_\epsilon u = j_\epsilon * u$, where $j_\epsilon$ is Friedrichs mollifier.","My question comes from Taylor's PDE textbook, volume III. Consider a semilinear hyperbolic system, $u_t=Lu+g(u)$, $u(0)=f$, where $Lu=\sum_j A_j \partial_{x_j}u$, $g(0)=0, \ |g'(u)| \le C$, take $M=\mathbb{T}^n$. Let $u_\epsilon$ be a solution to an approximating equation: $$u_{\epsilon_t} = J_{\epsilon}LJ_\epsilon u_\epsilon +J_\epsilon g(J_\epsilon u_\epsilon), \ u_\epsilon(0)=f$$ Show that: $$d/dt\| u_\epsilon \|_{L^2}^2 \le C \| u_\epsilon \|_{L^2}^2 , \ d/dt \| \nabla u_\epsilon \|_{L^2}^2 \le C \| \nabla u_\epsilon \|_{L^2}^2$$ Deduce that there exists a solution to the approximating equation for any $\epsilon>0$ which is defined for all $t\in \mathbb{R}$ and for all compact $I \subset \mathbb{R}$: $u_\epsilon$ bounded in $L^{\infty}(I,H^1(M))\cap Lip(I,L^2(M))$. The above question I solved prior to question 13. 13) Deduce that passing that passing to a subsequence $u_{\epsilon_k}$ we have a limit point $u\in L^{\infty}_{loc}(\mathbb{R},H^1(M))\cap Lip_{loc}(\mathbb{R}, L^2(M))$. s.t $u_{\epsilon_k} \rightarrow u \in C(I,L^2(M))$ in norm for all compact $I\subset \mathbb{R}$; hence, $g(J_{\epsilon_k}u_{\epsilon_k})\rightarrow g(u)$ in $C(\mathbb{R}, L^2(M))$, and $u$ solves the original semilinear hyperbolic pde. Examine the issue of uniqueness. My problem is in exercise 13, I thought that the first part is a simple conclusion from problem 12 of Arzela-Ascoli theorem, but I am not sure it that's much easy. As for the uniqueness part, I believe we need to use here Gronwall's inequality on the norm, we get for if two solutions of the semilinear hyperbolic pde, $u1,u2$, then by denoting $w=u1-u2$, and $h(w)=g(u1)-g(u2): $$\| w(t) \|_{L^2}^2  = (w(0),w(t)) + \int_0^t (Lw(s)+h(w(s)), w(t))ds$$ I need somehow to estimate $(Lw(s),w(t))$ from above, which I don't see how. I see how to esitamte $(h(w(s)),w(t)$, by Cauchy-Schwarz we have $(h(w(s)),w(t))\le \|h(w(s))\|_{L^2} \| w(t)\|_{L^2}$, $\| h(w(s)) \|_{L^2} = \| w(s)\int_0^1 h'(rw)dr \|_{L^2} \le \| w(s)\|_{L^2} \| \int_0^1 h'(rw)dr\|_{L^{\infty}} \le 2C \| w(s) \|_{L^2} \sup_{v\in \mathbb{R}^n} |h'(v)| \le 2C\| w(s) \|_{L^2}$, since $|h'(v)|=|g'(u2+v)-g'(u2)| \le C+C =2C$. I appreciate your help here. Thanks!!! P.S I forgot to mention that $J_\epsilon u = j_\epsilon * u$, where $j_\epsilon$ is Friedrichs mollifier.",,"['analysis', 'partial-differential-equations', 'sobolev-spaces']"
33,Bachmann's construction of the real numbers,Bachmann's construction of the real numbers,,"On page 44 of this book an approach to constructing the real numbers as equivalence classes of nested rational intervals is outlined and attributed to Bachmann. The outline in the book is very rudimentary, in fact avoiding presenting the definition of the arithmetic operations and order in favour of comparing the constructions with the other more familiar ones. So, can the reals be constructed as equivalence classes of nested rational intervals? if so is there a reference to the construction? and how painful are the details?","On page 44 of this book an approach to constructing the real numbers as equivalence classes of nested rational intervals is outlined and attributed to Bachmann. The outline in the book is very rudimentary, in fact avoiding presenting the definition of the arithmetic operations and order in favour of comparing the constructions with the other more familiar ones. So, can the reals be constructed as equivalence classes of nested rational intervals? if so is there a reference to the construction? and how painful are the details?",,"['analysis', 'foundations']"
34,Difficulty with Jensen's Equation. [duplicate],Difficulty with Jensen's Equation. [duplicate],,"This question already has an answer here : Function that is both midpoint convex and concave: $f\left(\frac{x+y}{2}\right) = \frac{f(x)+f(y)}{2}$ (1 answer) Closed 6 years ago . Its easy to find all continuous function $f: \Bbb R \to \Bbb R $ satisfing the Jensen equation  $$f \left( \frac{x+y}{2}\right )=\frac{f(x)+f(y)}{2}$$ But I am finding difficulty in finding all function  continuous on $(a,b),a,b \in \Bbb R$, satisfying the Jensen equation.","This question already has an answer here : Function that is both midpoint convex and concave: $f\left(\frac{x+y}{2}\right) = \frac{f(x)+f(y)}{2}$ (1 answer) Closed 6 years ago . Its easy to find all continuous function $f: \Bbb R \to \Bbb R $ satisfing the Jensen equation  $$f \left( \frac{x+y}{2}\right )=\frac{f(x)+f(y)}{2}$$ But I am finding difficulty in finding all function  continuous on $(a,b),a,b \in \Bbb R$, satisfying the Jensen equation.",,"['analysis', 'functions', 'continuity', 'functional-equations']"
35,What are Carnot groups?,What are Carnot groups?,,I'm trying to learn the Pansu differentiability theorem and I need to know what Carnot groups are. Can someone please explain what Carnot groups are? An introductory reference would be greatly appreciated as well. Thanks.,I'm trying to learn the Pansu differentiability theorem and I need to know what Carnot groups are. Can someone please explain what Carnot groups are? An introductory reference would be greatly appreciated as well. Thanks.,,"['analysis', 'derivatives', 'manifolds', 'lie-groups', 'riemannian-geometry']"
36,eventually $\epsilon$-close sequences are bounded.,eventually -close sequences are bounded.,\epsilon,"Let $ \epsilon $ >0. show that if $(a_n)_{n=1}$ and $(b_n)_{n=1}$ are eventually $\epsilon$-close sequences, then $(a_n)_{n=1}$ is bounded iff $(b_n)_{n=1}$ is bounded. proof; We can choose $M \geq 0$ such that $|a_i| \leq M$ for all $ i \geq 1$. and we can choose $\epsilon > 0$ and $ N \geq 0$ such that $|a_n - b_n|< \epsilon$ for all n $\geq$N. we have, $|b_i| = |b_i - a_i + a_i| \leq |b_i-a_i| + |a_i| \leq \epsilon + M $ Hence $b_i$ is bounded. for all i $\geq$ 1. Have i done good? or just terrible?","Let $ \epsilon $ >0. show that if $(a_n)_{n=1}$ and $(b_n)_{n=1}$ are eventually $\epsilon$-close sequences, then $(a_n)_{n=1}$ is bounded iff $(b_n)_{n=1}$ is bounded. proof; We can choose $M \geq 0$ such that $|a_i| \leq M$ for all $ i \geq 1$. and we can choose $\epsilon > 0$ and $ N \geq 0$ such that $|a_n - b_n|< \epsilon$ for all n $\geq$N. we have, $|b_i| = |b_i - a_i + a_i| \leq |b_i-a_i| + |a_i| \leq \epsilon + M $ Hence $b_i$ is bounded. for all i $\geq$ 1. Have i done good? or just terrible?",,[]
37,Proving that a function is Hölder-continuous,Proving that a function is Hölder-continuous,,"Let $\alpha\in(-1/2,0)$ and $x\in (0,1)$. Define the function $$f(x) = \int_x^1 z^{\alpha-\frac{1}{2}} (z-x)^{\alpha}dz.$$ I have the feeling that $|f(x)-f(y)| \leq C|x-y|^{1/2}$ or any other power between 0 and 1. The (improper) integral is convergent so one should expect that the resulting function is continuous of some Hölder degree. Does anyone see a quick way to prove it? Thank you for any hint!","Let $\alpha\in(-1/2,0)$ and $x\in (0,1)$. Define the function $$f(x) = \int_x^1 z^{\alpha-\frac{1}{2}} (z-x)^{\alpha}dz.$$ I have the feeling that $|f(x)-f(y)| \leq C|x-y|^{1/2}$ or any other power between 0 and 1. The (improper) integral is convergent so one should expect that the resulting function is continuous of some Hölder degree. Does anyone see a quick way to prove it? Thank you for any hint!",,"['calculus', 'real-analysis', 'integration', 'analysis', 'improper-integrals']"
38,Proving Integral Inequality,Proving Integral Inequality,,"I am working on proving the below inequality, but I am stuck. Let $g$ be a differentiable function such that $g(0)=0$ and $0<g'(x)\leq 1$ for all $x$ . For all $x\geq 0$ , prove that $$\int_{0}^{x}(g(t))^{3}dt\leq \left (\int_{0}^{x}g(t)dt  \right )^{2}$$","I am working on proving the below inequality, but I am stuck. Let be a differentiable function such that and for all . For all , prove that",g g(0)=0 0<g'(x)\leq 1 x x\geq 0 \int_{0}^{x}(g(t))^{3}dt\leq \left (\int_{0}^{x}g(t)dt  \right )^{2},"['calculus', 'inequality']"
39,local maxima of weierstrass function,local maxima of weierstrass function,,"Does the weierstrass function have uncountably many local maxima on (0,1)? I don't really know how to approach this problem at all, so any help is appreciated","Does the weierstrass function have uncountably many local maxima on (0,1)? I don't really know how to approach this problem at all, so any help is appreciated",,"['analysis', 'maxima-minima']"
40,Implicit Function Theorem exercise,Implicit Function Theorem exercise,,"I did an exercise in the book Vector Calculus [Marsden & Tromba] and I check my answer in the book Vector Calculus Study Guide and Solutions Manual [Karen Pao, Frederick Soon] but my answer is not the same and I don't understand why. Is it possible to solve the system of equations   \begin{align} xy^2+xzu+yv^2 & = 3 \\ u^3yz+2xv-u^2v^2 &= 2 \end{align}   for $u(x,y,z)$, $v(x,y,z)$ near $(x,y,z)=(1,1,1)$, $(u,v)=(1,1)$? Compute $\partial u/\partial y$ at $(x,y,z)=(1,1,1)$. This is my answer: \begin{align} f_1(x,y,z,u,v) &= xy^2+xzu+yv^2-3 \\ f_2(x,y,z,u,v) &= u^3yz+2xv-u^2v^2-2 \end{align} $$\Delta =  \frac{\partial(f_1,f_2)}{\partial (u,v)} =  \begin{vmatrix} xz & 2vy \\ -2uv^2+3u^2yz & -2u^2v+2x\end{vmatrix}$$ $$\left.\Delta\right|_{(x,y,z,u,v)=(1,1,1,1,1)} = \begin{vmatrix} 1 & 2 \\ 1 & 0\end{vmatrix} = -2 \neq 0$$ Thus, it is possible to solve in terms of $x$, $y$, $z$ at the given point. To compute $\partial v/\partial y$ I derive implicitly: $$ \begin{align} \frac{\partial f_1}{\partial y} =  2xy+xz\frac{\partial u}{\partial y}+v^2+2yv\frac{\partial v}{\partial y} &= 0 \\ \frac{\partial f_2}{\partial y} = 3u^2yz\frac{\partial u}{\partial y}+u^3z+2x\frac{\partial v}{\partial y}-2uv^2\frac{\partial u}{\partial y}-2u^2v\frac{\partial v}{\partial y} &= 0 \end{align} $$ For $(x,y,z)=(1,1,1)$ we have: $$ \begin{align} 2+\frac{\partial u}{\partial y} + v^2+2v\frac{\partial v}{\partial y} &= 0 \\ 3u^2\frac{\partial u}{\partial y}+u^3+2\frac{\partial v}{\partial y}-2uv^2\frac{\partial u}{\partial y}-2u^2v\frac{\partial v}{\partial y} &= 0 \end{align} \tag{1}\label{1} $$ And, after a boring time, I get: $$\frac{\partial v}{\partial y} = \frac{6u^2+3u^2v^2-u^3-4uv^2-2uv^4}{2+4uv^3-2u^2v-6u^2v}$$ But in the Solutions Manual , they substitute also $(u,v)=(1,1)$ in $\eqref{1}$(which is much faster) and their answer is $$ \frac{\partial v}{\partial y} = -1 $$ If that's correct, I don't understand it because in the exercise they ask us to compute $\partial v/\partial y$ at $(x,y,z)=(1,1,1)$","I did an exercise in the book Vector Calculus [Marsden & Tromba] and I check my answer in the book Vector Calculus Study Guide and Solutions Manual [Karen Pao, Frederick Soon] but my answer is not the same and I don't understand why. Is it possible to solve the system of equations   \begin{align} xy^2+xzu+yv^2 & = 3 \\ u^3yz+2xv-u^2v^2 &= 2 \end{align}   for $u(x,y,z)$, $v(x,y,z)$ near $(x,y,z)=(1,1,1)$, $(u,v)=(1,1)$? Compute $\partial u/\partial y$ at $(x,y,z)=(1,1,1)$. This is my answer: \begin{align} f_1(x,y,z,u,v) &= xy^2+xzu+yv^2-3 \\ f_2(x,y,z,u,v) &= u^3yz+2xv-u^2v^2-2 \end{align} $$\Delta =  \frac{\partial(f_1,f_2)}{\partial (u,v)} =  \begin{vmatrix} xz & 2vy \\ -2uv^2+3u^2yz & -2u^2v+2x\end{vmatrix}$$ $$\left.\Delta\right|_{(x,y,z,u,v)=(1,1,1,1,1)} = \begin{vmatrix} 1 & 2 \\ 1 & 0\end{vmatrix} = -2 \neq 0$$ Thus, it is possible to solve in terms of $x$, $y$, $z$ at the given point. To compute $\partial v/\partial y$ I derive implicitly: $$ \begin{align} \frac{\partial f_1}{\partial y} =  2xy+xz\frac{\partial u}{\partial y}+v^2+2yv\frac{\partial v}{\partial y} &= 0 \\ \frac{\partial f_2}{\partial y} = 3u^2yz\frac{\partial u}{\partial y}+u^3z+2x\frac{\partial v}{\partial y}-2uv^2\frac{\partial u}{\partial y}-2u^2v\frac{\partial v}{\partial y} &= 0 \end{align} $$ For $(x,y,z)=(1,1,1)$ we have: $$ \begin{align} 2+\frac{\partial u}{\partial y} + v^2+2v\frac{\partial v}{\partial y} &= 0 \\ 3u^2\frac{\partial u}{\partial y}+u^3+2\frac{\partial v}{\partial y}-2uv^2\frac{\partial u}{\partial y}-2u^2v\frac{\partial v}{\partial y} &= 0 \end{align} \tag{1}\label{1} $$ And, after a boring time, I get: $$\frac{\partial v}{\partial y} = \frac{6u^2+3u^2v^2-u^3-4uv^2-2uv^4}{2+4uv^3-2u^2v-6u^2v}$$ But in the Solutions Manual , they substitute also $(u,v)=(1,1)$ in $\eqref{1}$(which is much faster) and their answer is $$ \frac{\partial v}{\partial y} = -1 $$ If that's correct, I don't understand it because in the exercise they ask us to compute $\partial v/\partial y$ at $(x,y,z)=(1,1,1)$",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'implicit-function-theorem']"
41,Is every convex function differentiable amost every where?,Is every convex function differentiable amost every where?,,"If a function $f: D \subset R^n \rightarrow R$ is convex, then for every $x,y \in D$ and $\alpha \in [0,1]$,  $$ f(\alpha x + (1 - \alpha)y) \leq \alpha f(x) + (1 - \alpha)f(y). $$ I konw a convex function is a continuous function. I just wonder if every convex function is differentiable amost every where. This is important, because one of the most important way to solve a convex problem is gradient decent.","If a function $f: D \subset R^n \rightarrow R$ is convex, then for every $x,y \in D$ and $\alpha \in [0,1]$,  $$ f(\alpha x + (1 - \alpha)y) \leq \alpha f(x) + (1 - \alpha)f(y). $$ I konw a convex function is a continuous function. I just wonder if every convex function is differentiable amost every where. This is important, because one of the most important way to solve a convex problem is gradient decent.",,"['real-analysis', 'analysis', 'convex-analysis']"
42,Divisor summatory function for squares plus one,Divisor summatory function for squares plus one,,"As an exercise for my Analytic Number Theory course, I need to prove, using Dirichlet hyperbola method, that: $\sum_{n\leq x}\tau(n² + 1)= {3\over\pi}x\log(x) + O( x)$, where $\tau(n)=\sum_{d|n}1$  is the divisor function. We´ve already proved, using Dirichlet hyperbola method, that the leading behavior of the divisor summatory function is:  $\sum_{n \leq x} \tau (n)= x\log x + x(2\gamma-1) + O(\sqrt x)$. I have tried to rewrite the sum in this way: $\sum_{n \leq x}\tau(n²+1)= \sum_{k=1}^x(\sum_{n=1}^{k²+1} \tau(n) -\sum_{n=1}^{k²} \tau(n)) = \sum_{k=1}^x k²\log({k²+1\over k²})+\log(k²+1) +(2\gamma -1) +O(k)$ and evaluate its behavior but it seems it´s not the right way leading to the result we are looking for. I would appreciate suggestions about other solutions or remarks about what is wrong in my approach. I thought about the way I was approaching the problem and I think it is not the correct method that would lead me to the solution. At the moment my idea is to use the method underlying the proof of the leading behavior of the divisor summatory function, thus I need $\tau(n²+1)$ as the convolution $\tau(n²+1)= f*g(n)$  and then apply Dirichlet Hyperbola Method how it is well explained at page 74 of the following lecture notes: http://www.math.uiuc.edu/~hildebr/ant/main.pdf . In order to develop my idea, as I have written before, I need to represent $\tau(n² +1)$ as a convolution but I can't see yet how to do it. Again, any suggestion would be really helpful and appreciated.","As an exercise for my Analytic Number Theory course, I need to prove, using Dirichlet hyperbola method, that: $\sum_{n\leq x}\tau(n² + 1)= {3\over\pi}x\log(x) + O( x)$, where $\tau(n)=\sum_{d|n}1$  is the divisor function. We´ve already proved, using Dirichlet hyperbola method, that the leading behavior of the divisor summatory function is:  $\sum_{n \leq x} \tau (n)= x\log x + x(2\gamma-1) + O(\sqrt x)$. I have tried to rewrite the sum in this way: $\sum_{n \leq x}\tau(n²+1)= \sum_{k=1}^x(\sum_{n=1}^{k²+1} \tau(n) -\sum_{n=1}^{k²} \tau(n)) = \sum_{k=1}^x k²\log({k²+1\over k²})+\log(k²+1) +(2\gamma -1) +O(k)$ and evaluate its behavior but it seems it´s not the right way leading to the result we are looking for. I would appreciate suggestions about other solutions or remarks about what is wrong in my approach. I thought about the way I was approaching the problem and I think it is not the correct method that would lead me to the solution. At the moment my idea is to use the method underlying the proof of the leading behavior of the divisor summatory function, thus I need $\tau(n²+1)$ as the convolution $\tau(n²+1)= f*g(n)$  and then apply Dirichlet Hyperbola Method how it is well explained at page 74 of the following lecture notes: http://www.math.uiuc.edu/~hildebr/ant/main.pdf . In order to develop my idea, as I have written before, I need to represent $\tau(n² +1)$ as a convolution but I can't see yet how to do it. Again, any suggestion would be really helpful and appreciated.",,"['analysis', 'number-theory', 'analytic-number-theory', 'divisor-sum']"
43,spring representation of graphs,spring representation of graphs,,"Suppose we have a finite graph $G$ which we want to embed in ${\bf R}^d$; fix the positions of some nodes and connect all the nodes of the graphs with ideal springs of varying strength; (i.e. there is a force between every two nodes bringing them together where the magnitude is constant times the euclidean distnance between the two nodes), and consider the equilibrium position (i.e. all non-fixed nodes have net force 0). Now suppose that we take two adjacent nodes $a$ and $b$ whose positions were not fixed, and increase the strength of the spring that connects them, and consider the new equilibrium.  How do I show that the length of the edge between $a$ and $b$ necessrarily decreases? I know that the ""Energy"" (sum of (strength * square of length of edge)) is a convex function; the energy always increases when the edge weights increase, and that if $x$ and $y$ are the equilibria before and after increasing the edge length, the energy in $x$ is greater than the energy in $y$.","Suppose we have a finite graph $G$ which we want to embed in ${\bf R}^d$; fix the positions of some nodes and connect all the nodes of the graphs with ideal springs of varying strength; (i.e. there is a force between every two nodes bringing them together where the magnitude is constant times the euclidean distnance between the two nodes), and consider the equilibrium position (i.e. all non-fixed nodes have net force 0). Now suppose that we take two adjacent nodes $a$ and $b$ whose positions were not fixed, and increase the strength of the spring that connects them, and consider the new equilibrium.  How do I show that the length of the edge between $a$ and $b$ necessrarily decreases? I know that the ""Energy"" (sum of (strength * square of length of edge)) is a convex function; the energy always increases when the edge weights increase, and that if $x$ and $y$ are the equilibria before and after increasing the edge length, the energy in $x$ is greater than the energy in $y$.",,"['analysis', 'graph-theory']"
44,What is known about$\sum\limits_{p\text{ prime}} \frac{1}{p^2-1}$?,What is known about?,\sum\limits_{p\text{ prime}} \frac{1}{p^2-1},Are there some known results for $\sum\limits_{p\text{ prime}} \dfrac{1}{p^2-1}$? I wasn't able to find anything about this sum in the internet or in my books!,Are there some known results for $\sum\limits_{p\text{ prime}} \dfrac{1}{p^2-1}$? I wasn't able to find anything about this sum in the internet or in my books!,,"['analysis', 'number-theory', 'reference-request']"
45,Explain this step in lecture notes,Explain this step in lecture notes,,"The bounty offered is for the person that explains me how the author gets from equation 3.19 to equation 3.20 in these lecture see here . Normally I would agree that copying the relevant equation would be meaningful, but as this is a part of a longer proof, I think that you need to look at the context in order to understand the question.  am thinking about a step in the proof of the equivalence of ensembles in StatMech on page 25. step from 3.19 to 3.20 . It seems to be argued there that for $A$ and $K$ large enough, the term $$\sum_{N=0}^{\infty} \int_{-Bn}^{\infty} 1_{\text{{N>K or u>A}}}(u)e^{-\beta |\lambda_m|(\frac{u}{2}+n)}du $$ becomes arbitrarily small (We have $N = \frac{n}{|\lambda _m|}$ and $|\lambda_m| = \int_{\lambda_m}dx$), as the essential thing is to replace this whole term by $e^{\beta |\lambda_m|p(\beta, \mu)}$. $\beta, B$ are larger than zero. Despite, I do not really trust this step cause if we assume that only $N>K$, then it seems to be necessary for this whole expression to converge that $B$ is in $(0,2)$. Thus, there seems to be an additional requirement regarding $B$ in this step. Does anybody see if there is an additional trick used here or am I just missing something? In particular, I doubt that $$\sum_{N=0}^{\infty} \int_{-Bn}^{0} 1_{\text{{N>K or u>A}}}(u)e^{-\beta |\lambda_m|(\frac{u}{2}+n)}du $$ converges in general. The thing is that if we assume $N>K$, then we have $$\sum_{N=K+1}^{\infty} \int_{-B \frac{N}{|\lambda_m|}}^{0} e^{-\beta |\lambda_m|(\frac{u}{2}+n)}du $$ and this expression converges even for large $K$ only if $B<2$, wouldn't you agree?- So I just don't understand this step. If you have any questions,  please let me know.","The bounty offered is for the person that explains me how the author gets from equation 3.19 to equation 3.20 in these lecture see here . Normally I would agree that copying the relevant equation would be meaningful, but as this is a part of a longer proof, I think that you need to look at the context in order to understand the question.  am thinking about a step in the proof of the equivalence of ensembles in StatMech on page 25. step from 3.19 to 3.20 . It seems to be argued there that for $A$ and $K$ large enough, the term $$\sum_{N=0}^{\infty} \int_{-Bn}^{\infty} 1_{\text{{N>K or u>A}}}(u)e^{-\beta |\lambda_m|(\frac{u}{2}+n)}du $$ becomes arbitrarily small (We have $N = \frac{n}{|\lambda _m|}$ and $|\lambda_m| = \int_{\lambda_m}dx$), as the essential thing is to replace this whole term by $e^{\beta |\lambda_m|p(\beta, \mu)}$. $\beta, B$ are larger than zero. Despite, I do not really trust this step cause if we assume that only $N>K$, then it seems to be necessary for this whole expression to converge that $B$ is in $(0,2)$. Thus, there seems to be an additional requirement regarding $B$ in this step. Does anybody see if there is an additional trick used here or am I just missing something? In particular, I doubt that $$\sum_{N=0}^{\infty} \int_{-Bn}^{0} 1_{\text{{N>K or u>A}}}(u)e^{-\beta |\lambda_m|(\frac{u}{2}+n)}du $$ converges in general. The thing is that if we assume $N>K$, then we have $$\sum_{N=K+1}^{\infty} \int_{-B \frac{N}{|\lambda_m|}}^{0} e^{-\beta |\lambda_m|(\frac{u}{2}+n)}du $$ and this expression converges even for large $K$ only if $B<2$, wouldn't you agree?- So I just don't understand this step. If you have any questions,  please let me know.",,"['calculus', 'real-analysis']"
46,Real Analysis texts: Royden versus Stein & Shakarchi. Which is better? (and other suggestions welcome),Real Analysis texts: Royden versus Stein & Shakarchi. Which is better? (and other suggestions welcome),,"I am taking an introductory ""graduate"" analysis class and am comparing Analysis books that cover measure theory. I have had an ""advanced calculus"" class that covered the standard topics. I am having difficulty deciding what I want to buy. I am wondering how Royden's book compares to Stein and Shakarchi's. Which is better for a beginner? Which takes a more standard approach? Which is more motivated? Also other suggestions are welcome.","I am taking an introductory ""graduate"" analysis class and am comparing Analysis books that cover measure theory. I have had an ""advanced calculus"" class that covered the standard topics. I am having difficulty deciding what I want to buy. I am wondering how Royden's book compares to Stein and Shakarchi's. Which is better for a beginner? Which takes a more standard approach? Which is more motivated? Also other suggestions are welcome.",,"['analysis', 'measure-theory', 'reference-request']"
47,How to integrate these integrals? $\int{\frac{1}{(x^x-x^{-x})}} dx$ [closed],How to integrate these integrals?  [closed],\int{\frac{1}{(x^x-x^{-x})}} dx,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Problem : $$\int{\frac{1}{(x^x-x^{-x})}} dx$$ I need answer about the following problem. Please help . I will be grateful to you.  Thanks.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Problem : $$\int{\frac{1}{(x^x-x^{-x})}} dx$$ I need answer about the following problem. Please help . I will be grateful to you.  Thanks.",,"['integration', 'analysis']"
48,Differentiability-Related Condition that Implies Continuity,Differentiability-Related Condition that Implies Continuity,,"I previously asked a related question here that I did not phrase as I intended.  This is a revision of that question: It is a well-known fact that differentiability implies continuity.  And, for example, the existence and continuity of first partial derivatives implies continuity; but it also implies differentiability. My question is this: is there some differentiability-related condition for a function that is both weaker than differentiability and stronger than continuity? By ""differentiability-related"" I mean ""involving the partial derivatives of various orders of the given function"" (or any other conditions that you can think of that closely approximate whatever ""differentiability-related"" might mean, were it to be better defined.)","I previously asked a related question here that I did not phrase as I intended.  This is a revision of that question: It is a well-known fact that differentiability implies continuity.  And, for example, the existence and continuity of first partial derivatives implies continuity; but it also implies differentiability. My question is this: is there some differentiability-related condition for a function that is both weaker than differentiability and stronger than continuity? By ""differentiability-related"" I mean ""involving the partial derivatives of various orders of the given function"" (or any other conditions that you can think of that closely approximate whatever ""differentiability-related"" might mean, were it to be better defined.)",,"['analysis', 'continuity']"
49,Is Courant's Introduction to Calculus and Analysis still up-to-date?,Is Courant's Introduction to Calculus and Analysis still up-to-date?,,"I just found this marvelous book and I think that it's the best book in this category, but I'm worried that it is not up-to-date. I've heard that Hardy's A Course of Pure Mathematics has some switched terms (It seems that the ideas are the same, but in Hardy's time, people used to call it with different terms) - although it's a great book too. So, Is Courant's Introduction to Calculus and Analysis still up-to-date? The last printed edition was made by Springer in 1989 - I guess things didn't change much.","I just found this marvelous book and I think that it's the best book in this category, but I'm worried that it is not up-to-date. I've heard that Hardy's A Course of Pure Mathematics has some switched terms (It seems that the ideas are the same, but in Hardy's time, people used to call it with different terms) - although it's a great book too. So, Is Courant's Introduction to Calculus and Analysis still up-to-date? The last printed edition was made by Springer in 1989 - I guess things didn't change much.",,"['calculus', 'analysis', 'reference-request', 'soft-question']"
50,Integral and measure theory question,Integral and measure theory question,,"Let E_n be a sequence of Lebesgue measureable sets in [0,1].  Suppose that for $0 \leq k \leq 1$ we have that  $m(E_n \cap [0,r])= kr$ for any r, such that $0 \leq r \leq 1$. Prove that the  $$\lim_{ n \rightarrow \infty} \int_{E_n} f(x) dx= k \int_{[0,1]} f(x) dx,$$  where $f \in L^1([0,1])$. I have attempted the following.  $$k \int_{[0,1]} f(x) dx = \lim_{n \rightarrow \infty} \frac{m(E_n \cap [0,r])}{r} \int_{[0,1]} f(x)dx = \lim_{n \rightarrow \infty} \int_{[0,r]} \frac{\chi_{E_n} (t)}{r} dt \int_{[0,1]} f(x) dx= \int_{[0,1]} \int_{[0,r]}\frac{\chi_{E_n} (t) f(x)}{r} dt dx.$$ I want to somehow change the order of integration to change $\chi_{E_n}(t)$ to $\chi_{E_n}(x)$ (perhaps applying Fubini Tonelli), but I don't think its possible. *******Applying the comments suggestions******** Since step functions are dense in $L^1$ there exist $\phi_l \nearrow f$.  We note that we can apply DCT because $\phi_l \leq f$ and $f \in L^1$. $$\lim_{n \rightarrow \infty} \int_{[0,1]} \chi_{E_n}(x) \lim_{l \rightarrow \infty} \phi_l(x)dx= \lim_{n \rightarrow \infty} \lim_{l \rightarrow \infty} \int_{[0,1]} \chi_{E_n}(x) \phi_l(x)dx.$$ I want the change the order of the limits to say that  $$\lim_{n \rightarrow \infty} \lim_{l \rightarrow \infty} \int_{[0,1]} \chi_{E_n}(x) \phi_l(x)dx= \lim_{l \rightarrow \infty} \lim_{n \rightarrow \infty} \int_{[0,1]} \chi_{E_n}(x) \phi_l(x)dx= \lim_{l \rightarrow \infty} k \int_{[0,1]} \phi_l (x) dx =  k \int_{[0,1]} f(x)dx.$$ I don't know how to justify that I can indeed change the order of the limits.","Let E_n be a sequence of Lebesgue measureable sets in [0,1].  Suppose that for $0 \leq k \leq 1$ we have that  $m(E_n \cap [0,r])= kr$ for any r, such that $0 \leq r \leq 1$. Prove that the  $$\lim_{ n \rightarrow \infty} \int_{E_n} f(x) dx= k \int_{[0,1]} f(x) dx,$$  where $f \in L^1([0,1])$. I have attempted the following.  $$k \int_{[0,1]} f(x) dx = \lim_{n \rightarrow \infty} \frac{m(E_n \cap [0,r])}{r} \int_{[0,1]} f(x)dx = \lim_{n \rightarrow \infty} \int_{[0,r]} \frac{\chi_{E_n} (t)}{r} dt \int_{[0,1]} f(x) dx= \int_{[0,1]} \int_{[0,r]}\frac{\chi_{E_n} (t) f(x)}{r} dt dx.$$ I want to somehow change the order of integration to change $\chi_{E_n}(t)$ to $\chi_{E_n}(x)$ (perhaps applying Fubini Tonelli), but I don't think its possible. *******Applying the comments suggestions******** Since step functions are dense in $L^1$ there exist $\phi_l \nearrow f$.  We note that we can apply DCT because $\phi_l \leq f$ and $f \in L^1$. $$\lim_{n \rightarrow \infty} \int_{[0,1]} \chi_{E_n}(x) \lim_{l \rightarrow \infty} \phi_l(x)dx= \lim_{n \rightarrow \infty} \lim_{l \rightarrow \infty} \int_{[0,1]} \chi_{E_n}(x) \phi_l(x)dx.$$ I want the change the order of the limits to say that  $$\lim_{n \rightarrow \infty} \lim_{l \rightarrow \infty} \int_{[0,1]} \chi_{E_n}(x) \phi_l(x)dx= \lim_{l \rightarrow \infty} \lim_{n \rightarrow \infty} \int_{[0,1]} \chi_{E_n}(x) \phi_l(x)dx= \lim_{l \rightarrow \infty} k \int_{[0,1]} \phi_l (x) dx =  k \int_{[0,1]} f(x)dx.$$ I don't know how to justify that I can indeed change the order of the limits.",,"['real-analysis', 'analysis', 'lebesgue-measure']"
51,Q: An infinite subset $E$ of a compact set $K$ has a limit point in $K$,Q: An infinite subset  of a compact set  has a limit point in,E K K,"I'm having difficulty following this proof and was hoping someone could help give a clear picture of what Rudin is doing. pf If no point of $K$ were a limit point of $E$, then each $q \in K$ would have a neighborhood $V_{q}$ which contains at most one point of $E$ (namely, q, if $q \in E$).  It is clear that no finite subcollection of $\{V_{q}\}$ can cover $E$; and the same is true of $K$, since $E \subset K$.  This contradicts the compactness of $K$. I'm not exactly understanding the construction of $V_{q}$... if we have $q \in E$ then is $V_{q}$ by definition not a neighborhood by the construction of $V_{q}$?  Ie. if $q' \in E$ and every point around it is in $E$, then $V_{q'}$ is not a neighborhood, but just a point?","I'm having difficulty following this proof and was hoping someone could help give a clear picture of what Rudin is doing. pf If no point of $K$ were a limit point of $E$, then each $q \in K$ would have a neighborhood $V_{q}$ which contains at most one point of $E$ (namely, q, if $q \in E$).  It is clear that no finite subcollection of $\{V_{q}\}$ can cover $E$; and the same is true of $K$, since $E \subset K$.  This contradicts the compactness of $K$. I'm not exactly understanding the construction of $V_{q}$... if we have $q \in E$ then is $V_{q}$ by definition not a neighborhood by the construction of $V_{q}$?  Ie. if $q' \in E$ and every point around it is in $E$, then $V_{q'}$ is not a neighborhood, but just a point?",,['analysis']
52,Good family of kernels in $\mathbb{R}^n$,Good family of kernels in,\mathbb{R}^n,"I'm trying to prove that, given the heat equation $u_t = \Delta u$ with boundary values $u(x,0) = f(x)$, the solution given by $$u(x,t) = f \star H_t^{(d)}(x)$$ is continuous up to the boundary $t=0$, where $H_t^{(d)} = \frac{1}{(4 \pi^2 t)^{d/2}} e^{-\frac{|x|^2}{4t}}$ and $f \in \mathbb{S}(\mathbb{R})$ We know that $H_t^{(d)}$ is a family of good kernels/approx. to the identity as $t \to 0$ Question : Can we say $ f \star H_t^{(d)}(x)$ converges to $f(x)$ uniformly as $t \to 0$ as in the one dimensional case?  Stein and Shakarchi say that, for a good family of kernels $K_{\delta}$ in  $\mathbb{R}^n$, $f \star K_{\delta} \to f(0)$ for $f$ of Schwartz class, which confuses me.","I'm trying to prove that, given the heat equation $u_t = \Delta u$ with boundary values $u(x,0) = f(x)$, the solution given by $$u(x,t) = f \star H_t^{(d)}(x)$$ is continuous up to the boundary $t=0$, where $H_t^{(d)} = \frac{1}{(4 \pi^2 t)^{d/2}} e^{-\frac{|x|^2}{4t}}$ and $f \in \mathbb{S}(\mathbb{R})$ We know that $H_t^{(d)}$ is a family of good kernels/approx. to the identity as $t \to 0$ Question : Can we say $ f \star H_t^{(d)}(x)$ converges to $f(x)$ uniformly as $t \to 0$ as in the one dimensional case?  Stein and Shakarchi say that, for a good family of kernels $K_{\delta}$ in  $\mathbb{R}^n$, $f \star K_{\delta} \to f(0)$ for $f$ of Schwartz class, which confuses me.",,"['analysis', 'fourier-analysis', 'heat-equation']"
53,Missing step in Evan's PDE?,Missing step in Evan's PDE?,,"In the attached proof, I am having trouble justifying the presence of ""$x_n^{p-1}$"" in eh line immediately below ""Thus"" and ""Letting $m\to\infty$"". I understand he uses Minkowski's inequality, and upgrades the $x_n$ inequality to a full gradient, but I can't figure out how to get that term. I'd appreciate any help. Thank you!","In the attached proof, I am having trouble justifying the presence of ""$x_n^{p-1}$"" in eh line immediately below ""Thus"" and ""Letting $m\to\infty$"". I understand he uses Minkowski's inequality, and upgrades the $x_n$ inequality to a full gradient, but I can't figure out how to get that term. I'd appreciate any help. Thank you!",,"['analysis', 'partial-differential-equations', 'sobolev-spaces']"
54,Integral $=\int_0^\infty x^{\alpha -1}Li_n (-\sigma x) Li_m(-\omega x^r)dx$.,Integral .,=\int_0^\infty x^{\alpha -1}Li_n (-\sigma x) Li_m(-\omega x^r)dx,"I am trying to calculate an integral that can be expressed in terms of infinite hypergeometric series by using transforms and Residue method, the integral is  $$ I_{n,m}(\alpha,\sigma,\omega,r)=\int_0^\infty x^{\alpha -1}Li_n (-\sigma x) Li_m(-\omega x^r)dx $$ where n,m are positive integers.  The complex parameters are $\alpha,\sigma,\omega$ and real $r\neq 0$ are defined so that the integral exists.  This is an integral containing polyLogs so we define the Polylogarithm function for $|z|\leq 1$ and $n\geq 2$ by a power series expression of the form $$ Li_n(z)=\sum_{j=1}^\infty \frac{z^j}{j^n}, \ (|z| \leq 1). $$ For $|z| > 1$ we obtain $$ Li_n(z)=(-1)^{n+1}Li_n(z^{-1})-\frac{1}{n!}\ln^n(-z)-\sum_{j=0}^{n-2}\frac{1}{j!}(1+(-1)^{n-j})(1-2^{1-n+j})\zeta(n-j)\ln^j(-z) $$ where the Riemann Zeta function is given by $\zeta(n-j)$ and the logarithm is defined on the principle sheet. I was thinking of also using an integral representation of the polyLogs which are given by $$ Li_n(-z)=\frac{1}{2\pi i}\int_{\gamma -i\infty}^{\gamma+i\infty} \frac{\Gamma(s)\Gamma(-s)}{(-s)^{n-1}} z^{-s} ds, \ (-1<\gamma <0, |\arg z| < \pi, \ n=0,1,2,...) $$ and inserting into this $I_{n,m}$...Any ideas on what do, I need a complete solution.  Thanks.","I am trying to calculate an integral that can be expressed in terms of infinite hypergeometric series by using transforms and Residue method, the integral is  $$ I_{n,m}(\alpha,\sigma,\omega,r)=\int_0^\infty x^{\alpha -1}Li_n (-\sigma x) Li_m(-\omega x^r)dx $$ where n,m are positive integers.  The complex parameters are $\alpha,\sigma,\omega$ and real $r\neq 0$ are defined so that the integral exists.  This is an integral containing polyLogs so we define the Polylogarithm function for $|z|\leq 1$ and $n\geq 2$ by a power series expression of the form $$ Li_n(z)=\sum_{j=1}^\infty \frac{z^j}{j^n}, \ (|z| \leq 1). $$ For $|z| > 1$ we obtain $$ Li_n(z)=(-1)^{n+1}Li_n(z^{-1})-\frac{1}{n!}\ln^n(-z)-\sum_{j=0}^{n-2}\frac{1}{j!}(1+(-1)^{n-j})(1-2^{1-n+j})\zeta(n-j)\ln^j(-z) $$ where the Riemann Zeta function is given by $\zeta(n-j)$ and the logarithm is defined on the principle sheet. I was thinking of also using an integral representation of the polyLogs which are given by $$ Li_n(-z)=\frac{1}{2\pi i}\int_{\gamma -i\infty}^{\gamma+i\infty} \frac{\Gamma(s)\Gamma(-s)}{(-s)^{n-1}} z^{-s} ds, \ (-1<\gamma <0, |\arg z| < \pi, \ n=0,1,2,...) $$ and inserting into this $I_{n,m}$...Any ideas on what do, I need a complete solution.  Thanks.",,"['integration', 'analysis', 'definite-integrals', 'special-functions', 'contour-integration']"
55,Closure of Schwartz space in homogeneous Besov space,Closure of Schwartz space in homogeneous Besov space,,"Let $\dot{B}^s_{\infty,\infty}(\mathbb{R}^d)$ denote the homogeneous Besov space of order $s$ with second and third index $\infty$, i. e. the homogeneous Zygmund space. Let $\mathcal{S}(\mathbb{R}^d)$ denote the Schwartz space of rapidly decaying functions. It is known that $\mathcal{S}(\mathbb{R}^d)$ is not dense in $\dot{B}^0_{\infty,\infty}(\mathbb{R}^d)$. It is also known that $\dot{B}^0_{\infty,\infty}(\mathbb{R}^d)$ is stricly larger than the Lebesgue space $L_\infty(\mathbb{R}^d)$. My question is the following: Can we bound the $L_\infty$-norm of a Schwartz function $f$ by its $\dot{B}^0_{\infty,\infty}$-norm? In other words, is there a constant $C>0$ such that $$\|f\|_{L_\infty(\mathbb{R}^d)}\leq C\|f\|_{\dot{B}^0_{\infty,\infty}(\mathbb{R}^d)}$$ for every $f\in\mathcal{S}(\mathbb{R}^d)$? A somewhat weaker statement would be whether there is a constant $C>0$ and some arbitrarily large $s>0$ such that $$\|f\|_{L_\infty(\mathbb{R}^d)}\leq C\|f\|_{\dot{B}^0_{\infty,\infty}(\mathbb{R}^d)}+C\|f\|_{\dot{B}^s_{\infty,\infty}(\mathbb{R}^d)}$$ for every $f\in\mathcal{S}(\mathbb{R}^d)$. Interpolation theory tells us that if one replaces $\dot{B}^0_{\infty,\infty}(\mathbb{R}^d)$ in the above statement by $\dot{B}^{-\varepsilon}_{\infty,\infty}(\mathbb{R}^d)$ for some $\varepsilon>0$ then the latter inequality is true for every $f\in\dot{B}^{-\varepsilon}_{\infty,\infty}(\mathbb{R}^d)\cap \dot{B}^{s}_{\infty,\infty}(\mathbb{R}^d)$. However, $\mathcal{S}(\mathbb{R}^d)$ is not dense in $\dot{B}^{-\varepsilon}_{\infty,\infty}(\mathbb{R}^d)\cap \dot{B}^{s}_{\infty,\infty}(\mathbb{R}^d)$ either, so there might still be some room for improvement. Also, if one replaces the homogeneous Besov spaces above by their inhomogeneous counterparts then at least the latter statement holds by the Sobolev embedding Theorem.","Let $\dot{B}^s_{\infty,\infty}(\mathbb{R}^d)$ denote the homogeneous Besov space of order $s$ with second and third index $\infty$, i. e. the homogeneous Zygmund space. Let $\mathcal{S}(\mathbb{R}^d)$ denote the Schwartz space of rapidly decaying functions. It is known that $\mathcal{S}(\mathbb{R}^d)$ is not dense in $\dot{B}^0_{\infty,\infty}(\mathbb{R}^d)$. It is also known that $\dot{B}^0_{\infty,\infty}(\mathbb{R}^d)$ is stricly larger than the Lebesgue space $L_\infty(\mathbb{R}^d)$. My question is the following: Can we bound the $L_\infty$-norm of a Schwartz function $f$ by its $\dot{B}^0_{\infty,\infty}$-norm? In other words, is there a constant $C>0$ such that $$\|f\|_{L_\infty(\mathbb{R}^d)}\leq C\|f\|_{\dot{B}^0_{\infty,\infty}(\mathbb{R}^d)}$$ for every $f\in\mathcal{S}(\mathbb{R}^d)$? A somewhat weaker statement would be whether there is a constant $C>0$ and some arbitrarily large $s>0$ such that $$\|f\|_{L_\infty(\mathbb{R}^d)}\leq C\|f\|_{\dot{B}^0_{\infty,\infty}(\mathbb{R}^d)}+C\|f\|_{\dot{B}^s_{\infty,\infty}(\mathbb{R}^d)}$$ for every $f\in\mathcal{S}(\mathbb{R}^d)$. Interpolation theory tells us that if one replaces $\dot{B}^0_{\infty,\infty}(\mathbb{R}^d)$ in the above statement by $\dot{B}^{-\varepsilon}_{\infty,\infty}(\mathbb{R}^d)$ for some $\varepsilon>0$ then the latter inequality is true for every $f\in\dot{B}^{-\varepsilon}_{\infty,\infty}(\mathbb{R}^d)\cap \dot{B}^{s}_{\infty,\infty}(\mathbb{R}^d)$. However, $\mathcal{S}(\mathbb{R}^d)$ is not dense in $\dot{B}^{-\varepsilon}_{\infty,\infty}(\mathbb{R}^d)\cap \dot{B}^{s}_{\infty,\infty}(\mathbb{R}^d)$ either, so there might still be some room for improvement. Also, if one replaces the homogeneous Besov spaces above by their inhomogeneous counterparts then at least the latter statement holds by the Sobolev embedding Theorem.",,"['analysis', 'fourier-analysis', 'sobolev-spaces', 'harmonic-analysis', 'besov-space']"
56,Fixed point question with convergence,Fixed point question with convergence,,"Let $f:\mathbb{R}^n \to \mathbb{R}^n$ is $C^1$ and $1$ to $1$ and there exists a strict increasing sequence $t_{n} \in \mathbb{N}$ s.t $f^{t_{n}}(x) \to p$ for all $x$ as $n\to \infty$ (composition $t_{n}$ times) where $p$ is a fixed point, i.e $f(p)=p$. Can you prove or find a counterexample that $f^n(x) \to p$ as $n \to \infty$ for all $x$.","Let $f:\mathbb{R}^n \to \mathbb{R}^n$ is $C^1$ and $1$ to $1$ and there exists a strict increasing sequence $t_{n} \in \mathbb{N}$ s.t $f^{t_{n}}(x) \to p$ for all $x$ as $n\to \infty$ (composition $t_{n}$ times) where $p$ is a fixed point, i.e $f(p)=p$. Can you prove or find a counterexample that $f^n(x) \to p$ as $n \to \infty$ for all $x$.",,"['real-analysis', 'sequences-and-series']"
57,Nice convergent subsequence of $\cos(n)$.,Nice convergent subsequence of .,\cos(n),"This question is related to a few questions which have been posted on the website : Is there a limit of $\cos(n!)$ Converging subsequence on a circle The limit of $\sin(n!)$ Because of the irrationality of $\pi$, the sequence $(\cos(n))_{n\in\mathbb{N}}$ is dense in $[-1;1]$. For any value $a\in[-1;1]$, we can extract a subsequence $(\cos(n_k))_{k\in\mathbb{N}}$ convergent to $a$. My question is the following: Does someone know an example of a convergent subsequence of $\cos(n)$ with an explicit expression ? Some more comments: We could define a subsequence in the following way: $$n_0=1;\quad n_{k+1}>n_k \text{ such that } |\cos(n_k)-a|<1/k.$$ This subsequence is well defined (and unique if we add the condition that $n_{k+1}$ should be minimum) and converges. But I would say, that this not explicit. I don't have a definition of what should be an explicit expression, and any answer are welcome. Reading the nice answer of David Speyer in Is there a limit of $\cos(n!)$ , it seems that we still don't understand enough about $\pi$ to proof or disprove that $\cos(n!)$ diverges. Because of these comments, I would not be surprised if the answer to my question is no.","This question is related to a few questions which have been posted on the website : Is there a limit of $\cos(n!)$ Converging subsequence on a circle The limit of $\sin(n!)$ Because of the irrationality of $\pi$, the sequence $(\cos(n))_{n\in\mathbb{N}}$ is dense in $[-1;1]$. For any value $a\in[-1;1]$, we can extract a subsequence $(\cos(n_k))_{k\in\mathbb{N}}$ convergent to $a$. My question is the following: Does someone know an example of a convergent subsequence of $\cos(n)$ with an explicit expression ? Some more comments: We could define a subsequence in the following way: $$n_0=1;\quad n_{k+1}>n_k \text{ such that } |\cos(n_k)-a|<1/k.$$ This subsequence is well defined (and unique if we add the condition that $n_{k+1}$ should be minimum) and converges. But I would say, that this not explicit. I don't have a definition of what should be an explicit expression, and any answer are welcome. Reading the nice answer of David Speyer in Is there a limit of $\cos(n!)$ , it seems that we still don't understand enough about $\pi$ to proof or disprove that $\cos(n!)$ diverges. Because of these comments, I would not be surprised if the answer to my question is no.",,"['sequences-and-series', 'analysis', 'convergence-divergence', 'examples-counterexamples']"
58,Equivalent definitions of Fourier transform of a measure,Equivalent definitions of Fourier transform of a measure,,"For me the fourier transform of a measure $\mu\in\mathcal{S}'(\mathbb{R})$ is defined by $\hat{\mu}(\varphi)=\mu(\hat{\varphi})$ where $\varphi\in\mathcal{S}(\mathbb{R})$. My question is: if one has $\int_\mathbb{R}\operatorname{e}^{-\lambda s}d\mu(s)=f(\lambda)$ for all $\Re(\lambda)>0$, and if $f$ has a continuous prolonging on $\Re(z)=0$ with $t\mapsto f(t)\in \operatorname{L}^1_{loc}(\mathbb{R})$, can we conclude that the Fourier transform of $\mu$ is $t\mapsto f(2i\pi t)$? My question comes from arithmetic context: with $\mu=\sum_{p\in\mathbb{P}}\frac{1}{p}\delta_{\log{p}}$ we have easely that $\int_\mathbb{R}e^{-\lambda t}d\mu(t)=\zeta_\mathbb{P}(1+\lambda)$ and I'd like to conclude that $\hat{\mu}=\zeta_{\mathbb{P}}(1+2i\pi t)$. I know a specific proof of that but I'd like to know if it were a general thing.","For me the fourier transform of a measure $\mu\in\mathcal{S}'(\mathbb{R})$ is defined by $\hat{\mu}(\varphi)=\mu(\hat{\varphi})$ where $\varphi\in\mathcal{S}(\mathbb{R})$. My question is: if one has $\int_\mathbb{R}\operatorname{e}^{-\lambda s}d\mu(s)=f(\lambda)$ for all $\Re(\lambda)>0$, and if $f$ has a continuous prolonging on $\Re(z)=0$ with $t\mapsto f(t)\in \operatorname{L}^1_{loc}(\mathbb{R})$, can we conclude that the Fourier transform of $\mu$ is $t\mapsto f(2i\pi t)$? My question comes from arithmetic context: with $\mu=\sum_{p\in\mathbb{P}}\frac{1}{p}\delta_{\log{p}}$ we have easely that $\int_\mathbb{R}e^{-\lambda t}d\mu(t)=\zeta_\mathbb{P}(1+\lambda)$ and I'd like to conclude that $\hat{\mu}=\zeta_{\mathbb{P}}(1+2i\pi t)$. I know a specific proof of that but I'd like to know if it were a general thing.",,"['integration', 'analysis', 'measure-theory', 'fourier-analysis', 'arithmetic']"
59,"Given $c_{i} \rightarrow c$, prove if $c \ge 0$ then $\limsup c_{i} a_{i} = c \limsup a_{i}$","Given , prove if  then",c_{i} \rightarrow c c \ge 0 \limsup c_{i} a_{i} = c \limsup a_{i},"Let $a_{i}$ be a sequence of real numbers and suppose that $\limsup a_{i}$ is finite.  Let $c_{i}$ be another sequence and suppose $c_{i}$ converges to $c$.  Prove that if $c \ge 0$, then $\limsup c_{i} a_{i} = c \limsup a_{i}$ . I worked out the case where $c_{i}$ is non-negative.  Not sure what to do if both $a_{i}$ and $c_{i}$ are negative.  Thanks!","Let $a_{i}$ be a sequence of real numbers and suppose that $\limsup a_{i}$ is finite.  Let $c_{i}$ be another sequence and suppose $c_{i}$ converges to $c$.  Prove that if $c \ge 0$, then $\limsup c_{i} a_{i} = c \limsup a_{i}$ . I worked out the case where $c_{i}$ is non-negative.  Not sure what to do if both $a_{i}$ and $c_{i}$ are negative.  Thanks!",,['analysis']
60,Does there exist a bijective $f:\mathbb{N} \to \mathbb{N}$ such that $\sum f(n)/n^2$ converges?,Does there exist a bijective  such that  converges?,f:\mathbb{N} \to \mathbb{N} \sum f(n)/n^2,"We know that $\displaystyle\zeta(2)=\sum\limits_{n=1}^{\infty} \frac{1}{n^2} = \frac{\pi^2}{6}$ and it converges. Does there exists a bijective map $f:\mathbb{N} \to \mathbb{N}$ such that the sum $$\sum\limits_{n=1}^{\infty} \frac{f(n)}{n^2}$$ converges. If our $s=2$ was not fixed, then can we have a function such that $\displaystyle \zeta(s)=\sum\limits_{n=1}^{\infty} \frac{f(n)}{n^s}$ converges","We know that $\displaystyle\zeta(2)=\sum\limits_{n=1}^{\infty} \frac{1}{n^2} = \frac{\pi^2}{6}$ and it converges. Does there exists a bijective map $f:\mathbb{N} \to \mathbb{N}$ such that the sum $$\sum\limits_{n=1}^{\infty} \frac{f(n)}{n^2}$$ converges. If our $s=2$ was not fixed, then can we have a function such that $\displaystyle \zeta(s)=\sum\limits_{n=1}^{\infty} \frac{f(n)}{n^s}$ converges",,['number-theory']
61,convergence of series implies convergence of coefficients,convergence of series implies convergence of coefficients,,"Is it true that $$\sum_{i=0}^\infty a_{i_n} y^i \rightarrow \sum_{i=0}^\infty a_{i} y^i \quad \forall y \in [0,1]$$ implies $$a_{i_n} \to_{n \to \infty} a_{i} \quad \forall i$$ where $0 \leq a_i,a_{i_n} \leq 1$ for all $i,n \in \mathbb{N}$ and $\sum_{i=0}^\infty a_i = \sum_{i=0}^\infty a_{i_n} = 1 \quad \forall n \in \mathbb{N}$? Choosing $y = 0$ immediately gives $a_{0_n} \to_{n \to \infty} a_{0}$ My idea (please help for correctness and rigorousness): $$\operatorname{lim}_{n\to\infty} \sum_{i=0}^\infty a_{i_n} y^i = \sum_{i=0}^\infty a_{i} y^i \quad \forall y \in [0,1]$$ Since the series of coefficients converges and dominates this power series for all $y\in [0,1]$ we have $$\implies \sum_{i=0}^\infty \operatorname{lim}_{n\to\infty} a_{i_n} y^i = \sum_{i=0}^\infty a_{i} y^i \quad \forall y \in [0,1]$$ Two power series are equal iff all coefficients are equal, thus we have: $$\implies \operatorname{lim}_{n\to\infty} a_{i_n} = a_i$$ as desired. Is the above argument valid?","Is it true that $$\sum_{i=0}^\infty a_{i_n} y^i \rightarrow \sum_{i=0}^\infty a_{i} y^i \quad \forall y \in [0,1]$$ implies $$a_{i_n} \to_{n \to \infty} a_{i} \quad \forall i$$ where $0 \leq a_i,a_{i_n} \leq 1$ for all $i,n \in \mathbb{N}$ and $\sum_{i=0}^\infty a_i = \sum_{i=0}^\infty a_{i_n} = 1 \quad \forall n \in \mathbb{N}$? Choosing $y = 0$ immediately gives $a_{0_n} \to_{n \to \infty} a_{0}$ My idea (please help for correctness and rigorousness): $$\operatorname{lim}_{n\to\infty} \sum_{i=0}^\infty a_{i_n} y^i = \sum_{i=0}^\infty a_{i} y^i \quad \forall y \in [0,1]$$ Since the series of coefficients converges and dominates this power series for all $y\in [0,1]$ we have $$\implies \sum_{i=0}^\infty \operatorname{lim}_{n\to\infty} a_{i_n} y^i = \sum_{i=0}^\infty a_{i} y^i \quad \forall y \in [0,1]$$ Two power series are equal iff all coefficients are equal, thus we have: $$\implies \operatorname{lim}_{n\to\infty} a_{i_n} = a_i$$ as desired. Is the above argument valid?",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
62,On the sequence $(f_n)$ defined by $f_1(x)=x$ and $f_{n+1}(x)=x^{f_n(x)}$,On the sequence  defined by  and,(f_n) f_1(x)=x f_{n+1}(x)=x^{f_n(x)},"Consider the numbers $x^x$,$x^{(x^x)}$,$x^{(x^{(x^x)})}$, etc. Let $n$ be the number of times $x$ appears in the power tower and $f_n$ the corresponding function, for example $f_4(x)=x^{(x^{(x^x)})}$. a. It seems that the function $f_n$ has a (probably global) minimum inside the interval $[0,1]$ if and only if $n$ is even. Can this be proven? b. What can be said about the value of $$I_n=\int_0^1 f_n(x) dx?$$ It seems that the sequence $(I_{2n})$ decreases and the sequence $(I_{2n-1})$ increases. c. Finally, what about the limit of $I_n$ when $n\to\infty$? Does it exist, and can its value be given exactly or only by numerical calculation?","Consider the numbers $x^x$,$x^{(x^x)}$,$x^{(x^{(x^x)})}$, etc. Let $n$ be the number of times $x$ appears in the power tower and $f_n$ the corresponding function, for example $f_4(x)=x^{(x^{(x^x)})}$. a. It seems that the function $f_n$ has a (probably global) minimum inside the interval $[0,1]$ if and only if $n$ is even. Can this be proven? b. What can be said about the value of $$I_n=\int_0^1 f_n(x) dx?$$ It seems that the sequence $(I_{2n})$ decreases and the sequence $(I_{2n-1})$ increases. c. Finally, what about the limit of $I_n$ when $n\to\infty$? Does it exist, and can its value be given exactly or only by numerical calculation?",,"['analysis', 'power-towers']"
63,Surjective local diffeomorphism not injective,Surjective local diffeomorphism not injective,,"I would like to find a function $F:\mathbb{R}^N\to\mathbb{R}^N$ , for $N\geq 2$ , which is surjective and a local diffeomorphism, but which is not injective. I can solve the problem by using partition of unity, but I would like to find an explicit example. Any help is appreciated.","I would like to find a function , for , which is surjective and a local diffeomorphism, but which is not injective. I can solve the problem by using partition of unity, but I would like to find an explicit example. Any help is appreciated.",F:\mathbb{R}^N\to\mathbb{R}^N N\geq 2,['analysis']
64,Convex function problem and mean value theorem,Convex function problem and mean value theorem,,"I dont know how to solve this problem with not convex condition! Let $f \colon A \subseteq \mathbb R^n \to \mathbb R$ differentiable with $A$ convex and suppose $\|\mathop{\rm grad} f(x)\| \le M$ for $x \in A$. Prove $|f(x)-f(y)| \le M\|x-y\|$ for $x,y \in A$. Do you think this is true if $A$ is not convex?","I dont know how to solve this problem with not convex condition! Let $f \colon A \subseteq \mathbb R^n \to \mathbb R$ differentiable with $A$ convex and suppose $\|\mathop{\rm grad} f(x)\| \le M$ for $x \in A$. Prove $|f(x)-f(y)| \le M\|x-y\|$ for $x,y \in A$. Do you think this is true if $A$ is not convex?",,['analysis']
65,Question concerning mean value theorem,Question concerning mean value theorem,,"Here's the problem: Suppose the set $\{x\mid f(x)\not = 0,x\in[a,b]\}$ is not empty, and $f$ is differentiable on $[a,b]$ , with $f(a)=f(b)=0$ . Prove that $\exists c$ , such that $$|f'(c)|>\frac{4}{(b-a)^2}\int_{a}^{b}|f(x)|dx$$ My attempt is that first, to prove that exists $c$ s.t. $$|f'(c)|>\frac{1}{b-a}\int_{a}^{b}|f'(x)|dx$$ and then, use the positive variation to prove that $$\int_{a}^{b}|f'(x)|dx\ge\frac{2}{(b-a)}\int_{a}^{b}|f(x)|dx$$ since $|f(x)|\le P_F(b)$ , with $P_F(x)$ is positive variation function of $f$ . But the coefficient is only $\frac{2}{(b-a)^2}$ , not $\frac{4}{(b-a)^2}$ , which bother me  a lot. Both coefficient in my two inequality cannot be improved (as far as I know..) since the former one can be approximated by letting $f$ be a tent function and the latter one can be approximated by letting $f$ be a constant greater than zero.(Of course both example should be mollified so that $f$ can be differentiated.) Hope to find some valid method, thanks for your attention!","Here's the problem: Suppose the set is not empty, and is differentiable on , with . Prove that , such that My attempt is that first, to prove that exists s.t. and then, use the positive variation to prove that since , with is positive variation function of . But the coefficient is only , not , which bother me  a lot. Both coefficient in my two inequality cannot be improved (as far as I know..) since the former one can be approximated by letting be a tent function and the latter one can be approximated by letting be a constant greater than zero.(Of course both example should be mollified so that can be differentiated.) Hope to find some valid method, thanks for your attention!","\{x\mid f(x)\not = 0,x\in[a,b]\} f [a,b] f(a)=f(b)=0 \exists c |f'(c)|>\frac{4}{(b-a)^2}\int_{a}^{b}|f(x)|dx c |f'(c)|>\frac{1}{b-a}\int_{a}^{b}|f'(x)|dx \int_{a}^{b}|f'(x)|dx\ge\frac{2}{(b-a)}\int_{a}^{b}|f(x)|dx |f(x)|\le P_F(b) P_F(x) f \frac{2}{(b-a)^2} \frac{4}{(b-a)^2} f f f","['real-analysis', 'analysis']"
66,Integrability Question,Integrability Question,,"A question that has popped up while studying for qualifying exams is the following: Prove that $\int_0^1 \int_0^1 \frac{1}{x^p + y^q} dx dy$ is integrable iff $p^{-1} + q^{-1} > 1$ I can handle a few special cases (e.g. $p=q=1$) by changing variables, but the general case seems to be quite messy. Any ideas?","A question that has popped up while studying for qualifying exams is the following: Prove that $\int_0^1 \int_0^1 \frac{1}{x^p + y^q} dx dy$ is integrable iff $p^{-1} + q^{-1} > 1$ I can handle a few special cases (e.g. $p=q=1$) by changing variables, but the general case seems to be quite messy. Any ideas?",,"['analysis', 'integration', 'convergence-divergence']"
67,Property of convex functions.,Property of convex functions.,,"I have seen the following argument: If $F: \mathbb{R}^n \to \mathbb{R}$ is a convex function, then there exist a Borel function $\lambda\colon\mathbb{R}^n \to \mathbb{R}^n$ bounded on compact subsets, and such that   \begin{equation} F(w) \ge F(z) + \langle \lambda(z), w-z\rangle \end{equation}   for every $z,w \in \mathbb{R}^n.$ Why is this true? Can you give one answer or a reference? if you know something similar, please tell me.  Thank you.","I have seen the following argument: If $F: \mathbb{R}^n \to \mathbb{R}$ is a convex function, then there exist a Borel function $\lambda\colon\mathbb{R}^n \to \mathbb{R}^n$ bounded on compact subsets, and such that   \begin{equation} F(w) \ge F(z) + \langle \lambda(z), w-z\rangle \end{equation}   for every $z,w \in \mathbb{R}^n.$ Why is this true? Can you give one answer or a reference? if you know something similar, please tell me.  Thank you.",,['analysis']
68,Integration by Parts for PDEs,Integration by Parts for PDEs,,"I'm reading a paper on PDEs in preparation for some research. In it, integrals like this appear repreatedly:  $$ I(x,t) = \int_{|y|>1} e^{-i\alpha xy} \frac{e^{i\beta y^2}}{(1+y^2)^m} dy. $$ Here $y \in \mathbb{R}$, $m \in (1/4,1/2]$. The values of $\alpha = \frac{1}{2t}$ and $\beta = \frac{1}{4t}-1$ depend on $t$, but are nonzero. The paper says to integrate by parts to obtain $$ I(x,t) = \frac{C}{\beta} \int_{|y| > 1} \frac{1}{y} \left( \frac{d}{dy} \left(  \frac{e^{-i\alpha xy}}{(1+y^2)^m} \right) \right) e^{i\beta y^2} dy + \frac{1}{\beta} F(x), $$ where $F$ is a bounded continuous function. The function $F$ would be the boundary terms, though I'm not sure why the $t$ dependence disappeared (unless they're just suppressing it). I apologize if this should be obvious, but I can't see how to get this answer from integrating by parts. I get $$\frac{1}{\beta} \int_{|y| > 1} \frac{1}{y} \left( \frac{d}{dy} \left(  \frac{e^{-i\alpha xy}}{(1+y^2)^m} \right) \right) e^{i\beta y^2} dy $$ $$ = \int_{|y|>1} e^{-i\alpha xy} \frac{2ie^{i\beta y^2} - \frac{e^{i\beta y^2}}{\beta y^2}}{(1+y^2)^m} dy + \text{ boundary terms}.$$ It doesn't make sense to me to assume that the authors included the  $$\int_{|y|>1} e^{-i\alpha xy} \frac{\frac{e^{i\beta y^2}}{\beta y^2}}{(1+y^2)^m} dy $$ term in the function $F$, since they assert $F$ is continuous and the whole point of this discussion in the paper is to carefully show that such integrals are continuous... Any insight would be appreciated. I don't have much experience with PDEs, and this is proving a bit of a snag for me. Thanks. [This instance is on p. 798 of Bona and Saut's paper Dispersive Blow-Up II. Schrodinger-Type Equations, Optical and Oceanic Rogue Waves. ]","I'm reading a paper on PDEs in preparation for some research. In it, integrals like this appear repreatedly:  $$ I(x,t) = \int_{|y|>1} e^{-i\alpha xy} \frac{e^{i\beta y^2}}{(1+y^2)^m} dy. $$ Here $y \in \mathbb{R}$, $m \in (1/4,1/2]$. The values of $\alpha = \frac{1}{2t}$ and $\beta = \frac{1}{4t}-1$ depend on $t$, but are nonzero. The paper says to integrate by parts to obtain $$ I(x,t) = \frac{C}{\beta} \int_{|y| > 1} \frac{1}{y} \left( \frac{d}{dy} \left(  \frac{e^{-i\alpha xy}}{(1+y^2)^m} \right) \right) e^{i\beta y^2} dy + \frac{1}{\beta} F(x), $$ where $F$ is a bounded continuous function. The function $F$ would be the boundary terms, though I'm not sure why the $t$ dependence disappeared (unless they're just suppressing it). I apologize if this should be obvious, but I can't see how to get this answer from integrating by parts. I get $$\frac{1}{\beta} \int_{|y| > 1} \frac{1}{y} \left( \frac{d}{dy} \left(  \frac{e^{-i\alpha xy}}{(1+y^2)^m} \right) \right) e^{i\beta y^2} dy $$ $$ = \int_{|y|>1} e^{-i\alpha xy} \frac{2ie^{i\beta y^2} - \frac{e^{i\beta y^2}}{\beta y^2}}{(1+y^2)^m} dy + \text{ boundary terms}.$$ It doesn't make sense to me to assume that the authors included the  $$\int_{|y|>1} e^{-i\alpha xy} \frac{\frac{e^{i\beta y^2}}{\beta y^2}}{(1+y^2)^m} dy $$ term in the function $F$, since they assert $F$ is continuous and the whole point of this discussion in the paper is to carefully show that such integrals are continuous... Any insight would be appreciated. I don't have much experience with PDEs, and this is proving a bit of a snag for me. Thanks. [This instance is on p. 798 of Bona and Saut's paper Dispersive Blow-Up II. Schrodinger-Type Equations, Optical and Oceanic Rogue Waves. ]",,"['calculus', 'analysis', 'partial-differential-equations']"
69,An average estimate,An average estimate,,"In the paper here ,which the aim is to prove the Calderor-Zygmund theorem with a new approach   in the lemma 7, page 8 we have \begin{equation} \dfrac{1}{|B_4|}\int_{B_4} | D^2u|^2 \le 2^n \ \ \mbox{and}\ \ \dfrac{1}{|B_4|} \int_{B_4} |f|^2 \le 2^n \delta^2 \end{equation} Then  \begin{equation} \dfrac{1}{|B_4|}\int_{B_4}| \nabla u -(\overline{\nabla}u)_{B_4}|^2 \le C_1 \end{equation} where $\Delta u = f $ and $f_{B_r}=\dfrac{1}{|B_r|} \int_{B_r}f$ and $C_1$ is a universal constant. I can't understand the last estimate. I think that this can follows only on the first estimate above.","In the paper here ,which the aim is to prove the Calderor-Zygmund theorem with a new approach   in the lemma 7, page 8 we have \begin{equation} \dfrac{1}{|B_4|}\int_{B_4} | D^2u|^2 \le 2^n \ \ \mbox{and}\ \ \dfrac{1}{|B_4|} \int_{B_4} |f|^2 \le 2^n \delta^2 \end{equation} Then  \begin{equation} \dfrac{1}{|B_4|}\int_{B_4}| \nabla u -(\overline{\nabla}u)_{B_4}|^2 \le C_1 \end{equation} where $\Delta u = f $ and $f_{B_r}=\dfrac{1}{|B_r|} \int_{B_r}f$ and $C_1$ is a universal constant. I can't understand the last estimate. I think that this can follows only on the first estimate above.",,"['analysis', 'partial-differential-equations']"
70,The differentiability of distance function,The differentiability of distance function,,"Let $M$ be a submanifold of $\mathbb R^n$, then is there an open set $\Omega$ in $\mathbb R^n$ such that function $d(x,M)$ (distance function) is smooth on $\Omega$?","Let $M$ be a submanifold of $\mathbb R^n$, then is there an open set $\Omega$ in $\mathbb R^n$ such that function $d(x,M)$ (distance function) is smooth on $\Omega$?",,['differential-geometry']
71,Stokes' Theorem problem,Stokes' Theorem problem,,Let $M \subset \mathbf{R}^n$ be oriented compact smooth $k$-manifold and $\alpha$ be a $C^1$ diferential $(k-1)$-form defined in a neighborhood of M. Use Stokes' theorem to prove that \begin{align*}    \int_M d\alpha = 0 \end{align*} It looks like I have to apply Stokes' and then do something else but I don't what. I'm stuck with \begin{align*}    \int_M d\alpha = \int_{\partial{M}} \alpha \end{align*},Let $M \subset \mathbf{R}^n$ be oriented compact smooth $k$-manifold and $\alpha$ be a $C^1$ diferential $(k-1)$-form defined in a neighborhood of M. Use Stokes' theorem to prove that \begin{align*}    \int_M d\alpha = 0 \end{align*} It looks like I have to apply Stokes' and then do something else but I don't what. I'm stuck with \begin{align*}    \int_M d\alpha = \int_{\partial{M}} \alpha \end{align*},,"['real-analysis', 'analysis', 'integration', 'multivariable-calculus']"
72,asymptotic growth of entire functions with positive taylor coefficients,asymptotic growth of entire functions with positive taylor coefficients,,"I have an entire function $g(x)$ with taylor coefficients that go to zero at a ridiculously fast rate and I am trying to bound it with another entire function $f$ with similar properties but with the added claim that $f(x) \sim x^{1-\epsilon}$ so that I can say $g(x) = o(x)$. I need the coefficients of $f$ to be positive though.  For example I was thinking of a function like $$f(x) = \sum_{n=0}^{\infty} \frac{x^n}{n!^{n!}}$$ or something similar which must grow slowly. I don't know how to prove this though, and was wondering if there was some technique or if another function would work better.","I have an entire function $g(x)$ with taylor coefficients that go to zero at a ridiculously fast rate and I am trying to bound it with another entire function $f$ with similar properties but with the added claim that $f(x) \sim x^{1-\epsilon}$ so that I can say $g(x) = o(x)$. I need the coefficients of $f$ to be positive though.  For example I was thinking of a function like $$f(x) = \sum_{n=0}^{\infty} \frac{x^n}{n!^{n!}}$$ or something similar which must grow slowly. I don't know how to prove this though, and was wondering if there was some technique or if another function would work better.",,"['real-analysis', 'analysis']"
73,Differential calculus on Banach space,Differential calculus on Banach space,,"I'm revising for my upcoming test, and this problem dated back some years ago. I've been working on this problem for almost a day, but I don't even know how to start it correctly. Problem Given the open set $U$ in $\mathbb{R}^p$, $A$ closed and bounded in $U$; $V$ is an open set in $\mathbb{R}^k$; $B$ is closed, bounded, Jordan measurable set contained in $V$; and a continuous function $f:U \times V \times \mathbb{R}^n \times \mathbb{R}^{nk} \to \mathbb{R}$. Let: $E = C^1(B; \mathbb{R}^n)$, i.e the space of continuous functions from $B$ to $\mathbb{R}^n$ that have the first derivative continuous. We define the norm $||x||_E = \max\limits_{s \in B} \left\{ |x(s)|_2 + ||x'(s)|| \right\}$. $F = C(A; \color{red}{\mathbb{R}^n})$, i.e the space of continuous functions from $A$ to $\mathbb{R}^n$, with the norm $||y||_F = \max\limits_{t \in A} \{ |y(t)|_2 \}$ Where $|\bullet|_2$ is the normal Euclidean norm. Define $T: E \to F$ as follow: $$T(x)(t) = \int\limits_B f(t;s;x(s);x'(s)) ds$$ Question Prove that $T$ is compact. $\color{green}{\mathbf{(done) :)}}$ Assume that all partial derivative of $f$ wrt every variable is continuous. Prove that: For all $x \in E$, $T(x)$ has continuous partial derivatives. $\color{green}{\mathbf{(this \ is \ also \ done) :)}}$ For all $x \in E$, prove that $T$ is differentiable at $x$; and that, for every $h \in E$; $t \in A$, we'll have: $$T'(x)(h)(t) = \int\limits_B [ \left< f_x(t;s;x(s);x'(s);h(s)) \right> + \left< f_{x'}(t;s;x(s);x'(s);h'(s)) \right>] ds$$ Where $$\left< f_x(t;s;x(s);x'(s));h(s) \right> = \sum\limits_{i = 1}^{n} \frac{\partial f}{\partial x_i}(t;s;x(s);x'(s)) h_i(s)$$ My difficulties Should the $\color{red}{\mathbb{R}^n}$ at the beginning of the problem be changed to $\mathbb{R}$? Since I don't think that integral would return something in $\mathbb{R}^n$ at all. Or, am I missing something here? I don't really know where that monstrous $T'(x)(h)(t)$ comes from. I would be glad if someone can give me a small push. I know that we're supposed to use the uniform continuous property of $f$ on some compact set; and the mean value theorem, but I haven't come across any functions that have $x'(s)$ as its perimeter. Thanks guys a lot, And have a good day, P.S: I do have a small easy example on this kind of problem from our professor. So if you guys think that it's needed, please tell me, and I'll append it to the end of this post.","I'm revising for my upcoming test, and this problem dated back some years ago. I've been working on this problem for almost a day, but I don't even know how to start it correctly. Problem Given the open set $U$ in $\mathbb{R}^p$, $A$ closed and bounded in $U$; $V$ is an open set in $\mathbb{R}^k$; $B$ is closed, bounded, Jordan measurable set contained in $V$; and a continuous function $f:U \times V \times \mathbb{R}^n \times \mathbb{R}^{nk} \to \mathbb{R}$. Let: $E = C^1(B; \mathbb{R}^n)$, i.e the space of continuous functions from $B$ to $\mathbb{R}^n$ that have the first derivative continuous. We define the norm $||x||_E = \max\limits_{s \in B} \left\{ |x(s)|_2 + ||x'(s)|| \right\}$. $F = C(A; \color{red}{\mathbb{R}^n})$, i.e the space of continuous functions from $A$ to $\mathbb{R}^n$, with the norm $||y||_F = \max\limits_{t \in A} \{ |y(t)|_2 \}$ Where $|\bullet|_2$ is the normal Euclidean norm. Define $T: E \to F$ as follow: $$T(x)(t) = \int\limits_B f(t;s;x(s);x'(s)) ds$$ Question Prove that $T$ is compact. $\color{green}{\mathbf{(done) :)}}$ Assume that all partial derivative of $f$ wrt every variable is continuous. Prove that: For all $x \in E$, $T(x)$ has continuous partial derivatives. $\color{green}{\mathbf{(this \ is \ also \ done) :)}}$ For all $x \in E$, prove that $T$ is differentiable at $x$; and that, for every $h \in E$; $t \in A$, we'll have: $$T'(x)(h)(t) = \int\limits_B [ \left< f_x(t;s;x(s);x'(s);h(s)) \right> + \left< f_{x'}(t;s;x(s);x'(s);h'(s)) \right>] ds$$ Where $$\left< f_x(t;s;x(s);x'(s));h(s) \right> = \sum\limits_{i = 1}^{n} \frac{\partial f}{\partial x_i}(t;s;x(s);x'(s)) h_i(s)$$ My difficulties Should the $\color{red}{\mathbb{R}^n}$ at the beginning of the problem be changed to $\mathbb{R}$? Since I don't think that integral would return something in $\mathbb{R}^n$ at all. Or, am I missing something here? I don't really know where that monstrous $T'(x)(h)(t)$ comes from. I would be glad if someone can give me a small push. I know that we're supposed to use the uniform continuous property of $f$ on some compact set; and the mean value theorem, but I haven't come across any functions that have $x'(s)$ as its perimeter. Thanks guys a lot, And have a good day, P.S: I do have a small easy example on this kind of problem from our professor. So if you guys think that it's needed, please tell me, and I'll append it to the end of this post.",,"['calculus', 'analysis', 'banach-spaces']"
74,Green's function for third order boundary value problems,Green's function for third order boundary value problems,,"How to find the Green's function $G(t,x)$ for the BVP consisting of the equation : $$u'''(t)=0 , \quad  t\in (0,1)$$ and BC : $$u(0)=u'(p)=\int_q^1 w(s)u''(s) ds =0 $$ where $\frac12 < p<q<1$ are constants . they say that $$G(t,s)=-t(p-s)\chi_{[0,p]}(s)+\frac{(t-s)^2}{2}W(s)\chi_{[0,t]}(s)+\frac{t(2p-t)}{2}W(s)\chi_{[q,1]}(s)+\frac{t(2p-t)}{2}\chi_{[0,q]}(s)$$ Where $W(s)=(\int_q^1w(v)dv)^{-1}\int_s^1 w(v)dv , s\in [q,1]$ but how to prove it ? Please, Thank you","How to find the Green's function $G(t,x)$ for the BVP consisting of the equation : $$u'''(t)=0 , \quad  t\in (0,1)$$ and BC : $$u(0)=u'(p)=\int_q^1 w(s)u''(s) ds =0 $$ where $\frac12 < p<q<1$ are constants . they say that $$G(t,s)=-t(p-s)\chi_{[0,p]}(s)+\frac{(t-s)^2}{2}W(s)\chi_{[0,t]}(s)+\frac{t(2p-t)}{2}W(s)\chi_{[q,1]}(s)+\frac{t(2p-t)}{2}\chi_{[0,q]}(s)$$ Where $W(s)=(\int_q^1w(v)dv)^{-1}\int_s^1 w(v)dv , s\in [q,1]$ but how to prove it ? Please, Thank you",,"['analysis', 'derivatives', 'calculus-of-variations', 'solution-verification']"
75,Intermediate value and monotonic implies continuous?,Intermediate value and monotonic implies continuous?,,"$I$ is an interval, $I^0$ is the interior of $I$ . Let $f:I\to\Bbb R$ be a function with intermediate value property on $I$ , and $f$ is monotonic on $I^0$ . Does it follow that $f$ is continuous on $I$ ?","is an interval, is the interior of . Let be a function with intermediate value property on , and is monotonic on . Does it follow that is continuous on ?",I I^0 I f:I\to\Bbb R I f I^0 f I,"['real-analysis', 'analysis', 'continuity']"
76,Help me correct these properties of : $f_{n}(x)= nx(1-x)^{n}$? Is there maybe a typo in the sequence?,Help me correct these properties of : ? Is there maybe a typo in the sequence?,f_{n}(x)= nx(1-x)^{n},"Examine the sequence  of functions $(f_n)_{n\in \mathbb{N}}$ on $x\in[0,1]$: $$f_{n}(x)= nx(1-x)^{n}$$ Does $(f_n)_{n\in \mathbb{N}}$ converge pointwise or uniform? I will show that it does converge pointwise to 0 by taking the limit and applying l'hôpitals rule: $$\lim_{n\rightarrow \infty} (f_{n})_{n\in\mathbb{N}}= \lim_{n\rightarrow \infty}nx(1-x)^n = 0 $$ because for $x=0,1$  it follows directly that $f_n=0$ and for  $0<x<1$: $\lim_{n\rightarrow \infty} (1-x)^n \rightarrow 0$ from some n faster than $nx \rightarrow \infty$ If $(f_n)$ were uniform convergent, then : $\lim_{n\rightarrow \infty} \sup(|f_n-0|) = 0 $ but if we insert $x= \frac{1}{n}$ we see that $lim_{n\rightarrow \infty} \sup (|(1-\frac{1}{n})^n-0)|=\frac{1}{e}$ so $(f_n)$ can not be uniform convergent. *2 Why is it that: $\lim_{x\rightarrow 0} \lim_{n\rightarrow \infty} (f_n) =\lim_{n\rightarrow 0} \lim_{x\rightarrow 0}(f_n) $ ? Is it because both limits make the sequence go to 0 independently? *3. Is the limit function of $(f_n)$ continuous? This question makes me think that maybe there was a typo made when writing the function sequence, because who would ask to show continuity for $f(t)=0$ ? *4 $(f_n)_n$ is allowed to be integrate partwise  How can I show this fact ? Can you see what the typo could have possibly been? If there was no typo made, are my answers correct? What can I do better?","Examine the sequence  of functions $(f_n)_{n\in \mathbb{N}}$ on $x\in[0,1]$: $$f_{n}(x)= nx(1-x)^{n}$$ Does $(f_n)_{n\in \mathbb{N}}$ converge pointwise or uniform? I will show that it does converge pointwise to 0 by taking the limit and applying l'hôpitals rule: $$\lim_{n\rightarrow \infty} (f_{n})_{n\in\mathbb{N}}= \lim_{n\rightarrow \infty}nx(1-x)^n = 0 $$ because for $x=0,1$  it follows directly that $f_n=0$ and for  $0<x<1$: $\lim_{n\rightarrow \infty} (1-x)^n \rightarrow 0$ from some n faster than $nx \rightarrow \infty$ If $(f_n)$ were uniform convergent, then : $\lim_{n\rightarrow \infty} \sup(|f_n-0|) = 0 $ but if we insert $x= \frac{1}{n}$ we see that $lim_{n\rightarrow \infty} \sup (|(1-\frac{1}{n})^n-0)|=\frac{1}{e}$ so $(f_n)$ can not be uniform convergent. *2 Why is it that: $\lim_{x\rightarrow 0} \lim_{n\rightarrow \infty} (f_n) =\lim_{n\rightarrow 0} \lim_{x\rightarrow 0}(f_n) $ ? Is it because both limits make the sequence go to 0 independently? *3. Is the limit function of $(f_n)$ continuous? This question makes me think that maybe there was a typo made when writing the function sequence, because who would ask to show continuity for $f(t)=0$ ? *4 $(f_n)_n$ is allowed to be integrate partwise  How can I show this fact ? Can you see what the typo could have possibly been? If there was no typo made, are my answers correct? What can I do better?",,"['real-analysis', 'analysis', 'multivariable-calculus']"
77,Mean value inequality and Fixed-point theorem,Mean value inequality and Fixed-point theorem,,"I need help to understand this question, it's not that clear for me: Using the norm $|x|+|y|$ and the Mean value inequality, give a condition over the partial derivatives of $f(x,y)$ and $g(x,y)$ for the Fixed point theorem may be applied to the transformation: $$T(x,y)=(f(x,y),g(x,y))$$ I'm not sure what exactly I should do, how to start with this one. Thanks a lot!","I need help to understand this question, it's not that clear for me: Using the norm $|x|+|y|$ and the Mean value inequality, give a condition over the partial derivatives of $f(x,y)$ and $g(x,y)$ for the Fixed point theorem may be applied to the transformation: $$T(x,y)=(f(x,y),g(x,y))$$ I'm not sure what exactly I should do, how to start with this one. Thanks a lot!",,"['analysis', 'multivariable-calculus']"
78,Simplifying an integral arising in Physical Chemistry,Simplifying an integral arising in Physical Chemistry,,"I am struggling to understand the following transition (encountered in a paper on Physical Chemistry). Let $$D=\frac{\tau_0^{-1}\int_0^\infty G(t)dt}{1-\tau_0^{-1}\int_0^\infty G(t)\int w(R)\exp[-tw(R)] d^3R dt},\qquad (1)$$ where $$G(t)= \exp\left\{-\frac{t}{\tau_0}-n\int (1-e^{-tw(R)})dR-n_A\int (1-e^{-tu(R)})dR \right\},$$ $$w(R)=\frac{1}{\tau_0}\left(\frac{R_0}{R}\right)^6,\qquad u(R)=\frac{1}{\tau_0}\left(\frac{R_A}{R}\right)^6.$$ The authors claim that $$\qquad\qquad\quad\qquad D=\frac{1-f(y)}{1-\frac{c}{c+c_A}f(y)}.\qquad\qquad\qquad\qquad(2)$$ Here $$f(y)=\sqrt{\pi} y\exp(y^2)\left[1-\frac{2}{\sqrt{\pi}}\int_0^y\exp(-t^2)dt\right],$$ $$y=\frac{\sqrt{\pi}}{2}(c+c_A),$$ $$c=\frac{4\pi}{3}R_0^3n,\qquad c_A=\frac{4\pi}{3}R_A^3n_A.$$ Question. How does one pass from (1) to (2)?","I am struggling to understand the following transition (encountered in a paper on Physical Chemistry). Let $$D=\frac{\tau_0^{-1}\int_0^\infty G(t)dt}{1-\tau_0^{-1}\int_0^\infty G(t)\int w(R)\exp[-tw(R)] d^3R dt},\qquad (1)$$ where $$G(t)= \exp\left\{-\frac{t}{\tau_0}-n\int (1-e^{-tw(R)})dR-n_A\int (1-e^{-tu(R)})dR \right\},$$ $$w(R)=\frac{1}{\tau_0}\left(\frac{R_0}{R}\right)^6,\qquad u(R)=\frac{1}{\tau_0}\left(\frac{R_A}{R}\right)^6.$$ The authors claim that $$\qquad\qquad\quad\qquad D=\frac{1-f(y)}{1-\frac{c}{c+c_A}f(y)}.\qquad\qquad\qquad\qquad(2)$$ Here $$f(y)=\sqrt{\pi} y\exp(y^2)\left[1-\frac{2}{\sqrt{\pi}}\int_0^y\exp(-t^2)dt\right],$$ $$y=\frac{\sqrt{\pi}}{2}(c+c_A),$$ $$c=\frac{4\pi}{3}R_0^3n,\qquad c_A=\frac{4\pi}{3}R_A^3n_A.$$ Question. How does one pass from (1) to (2)?",,"['analysis', 'integration', 'multivariable-calculus', 'physics']"
79,Exercise on derivatives from Rudin's Principles of Mathematical Analysis,Exercise on derivatives from Rudin's Principles of Mathematical Analysis,,"Exercise 5.26 --Rudin [Principle of Mathematical Analysis] Ex.5.26 If $f'$ exists on $[a,b]$, $f(a)=0$ and $\exists A\in\mathbb{R}\;(|f'|\le A|f|\,\text{on }[a,b])$, then $f = 0$ on $[a,b]$. Hint: Fix $x_0\in [a,b]$, let $M_0 = \sup |f|([a,x_0])$, $M_1=\sup |f'|([a,x_0])$.  Then $$|f(x)|\le M_1(x_0 -a)\le A(x_0-a)M_0$$ (for $x\in [a,x_0]$). Hence $M_0= 0$ so $f = 0$ on $[a,x_0]$ if $A(x_0 - a) < 1$. Proceed. Now I've done the above but I'm asked to assume $f(a) = y_0 > 0$. Show that $f(t)\le y_0*e^{A(t-a)}$. How do I take care of the case where $f(t) = 0$ for some $t > a$? I'm then asked to examine $\ln (f(t))$? Proof for 5.26: Assume $A>0$ (otherwise nothing to prove) and let $\beta = \sup \{c\in [a,b]: f([a,c]) = \{0\}\}$.  Then $\beta \in [a,b],\; f([a,\beta]) = \{0\}$ since $f(a)=0$ and $f$ is continuous. we shall show $\beta = b$. If $\beta < b$, let $\gamma = \min(b,\beta+\frac{1}{A})$ and take $\beta_1\in (\beta, \gamma)$ then for $x\in [\beta,\beta_1]$ we have $|f(x)| = |f(x) - f(\beta)|\le M_1(x-\beta)\le A(\beta_1-\beta)M_0$ and $A(\beta_1-\beta) < A(\gamma-\beta)\le 1$ Where $M_0 = \sup |f|([\beta,\beta_1]),\; M_1 = \sup |f'|([\beta,\beta_1])$. Thus $M_0 \overset{(\star)}{=} 0$ but then we get $f([a,\beta_1]) = f([a,\beta]\cup [\beta,\beta_1]) = \{0\}$ and $\beta < \beta_1 \in [a,b]$, contradicts to the def. of $\beta.\;\square$ $$(0\le M_0 \le sM_0 \wedge s<1)\implies (0\le (1-s)M_0 \le 0)\implies (M_0=0)\tag{$\star$}$$","Exercise 5.26 --Rudin [Principle of Mathematical Analysis] Ex.5.26 If $f'$ exists on $[a,b]$, $f(a)=0$ and $\exists A\in\mathbb{R}\;(|f'|\le A|f|\,\text{on }[a,b])$, then $f = 0$ on $[a,b]$. Hint: Fix $x_0\in [a,b]$, let $M_0 = \sup |f|([a,x_0])$, $M_1=\sup |f'|([a,x_0])$.  Then $$|f(x)|\le M_1(x_0 -a)\le A(x_0-a)M_0$$ (for $x\in [a,x_0]$). Hence $M_0= 0$ so $f = 0$ on $[a,x_0]$ if $A(x_0 - a) < 1$. Proceed. Now I've done the above but I'm asked to assume $f(a) = y_0 > 0$. Show that $f(t)\le y_0*e^{A(t-a)}$. How do I take care of the case where $f(t) = 0$ for some $t > a$? I'm then asked to examine $\ln (f(t))$? Proof for 5.26: Assume $A>0$ (otherwise nothing to prove) and let $\beta = \sup \{c\in [a,b]: f([a,c]) = \{0\}\}$.  Then $\beta \in [a,b],\; f([a,\beta]) = \{0\}$ since $f(a)=0$ and $f$ is continuous. we shall show $\beta = b$. If $\beta < b$, let $\gamma = \min(b,\beta+\frac{1}{A})$ and take $\beta_1\in (\beta, \gamma)$ then for $x\in [\beta,\beta_1]$ we have $|f(x)| = |f(x) - f(\beta)|\le M_1(x-\beta)\le A(\beta_1-\beta)M_0$ and $A(\beta_1-\beta) < A(\gamma-\beta)\le 1$ Where $M_0 = \sup |f|([\beta,\beta_1]),\; M_1 = \sup |f'|([\beta,\beta_1])$. Thus $M_0 \overset{(\star)}{=} 0$ but then we get $f([a,\beta_1]) = f([a,\beta]\cup [\beta,\beta_1]) = \{0\}$ and $\beta < \beta_1 \in [a,b]$, contradicts to the def. of $\beta.\;\square$ $$(0\le M_0 \le sM_0 \wedge s<1)\implies (0\le (1-s)M_0 \le 0)\implies (M_0=0)\tag{$\star$}$$",,['analysis']
80,Diffeomorphism on the Sphere,Diffeomorphism on the Sphere,,Suppose $f:S^2\rightarrow S^2$ is a diffeomorphism. Is true that $f$ must have a fixed point or a point $x\in S^2$ such that $f(f(x))=x$?,Suppose $f:S^2\rightarrow S^2$ is a diffeomorphism. Is true that $f$ must have a fixed point or a point $x\in S^2$ such that $f(f(x))=x$?,,['analysis']
81,"If $f$ is quasi and left continuous on $[a,b]$ and $\alpha$ is increasing and right continuous then $f$ is Riemann-Stieltjes integrable over $[a,b]$",If  is quasi and left continuous on  and  is increasing and right continuous then  is Riemann-Stieltjes integrable over,"f [a,b] \alpha f [a,b]","Question: If $f$ is quasi and left continuous on $[a,b]$ and $\alpha$ is increasing and right continuous then $f$ is Riemann-Stieltjes integrable over $[a,b]$ with integrator $\alpha$. Quasi-continuous means $f(x_+)$ and $f(x_-)$ exist for all $x \in [a,b]$. Left {Right} continuous means $f(x)=f(x_-)$ {=$f(x_+)$} Text: Real Analysis by Carothers, also Rudin I have already shown that I can find a partition $P=\{a=t_0<t_1<...<t_n=b\}$ such that for $1 \le k \le n$ the oscillations $\omega(f,(t_{k-1}, t_k])<\epsilon$. I've tried using the Riemann criterion using upper and lower sums but I couldn't make any headway. I can rule out potential ways that it would not be integrable since $f$ and $\alpha$ do not share any common sided discontinuities.  Any suggestions or help would be much appreciated. Thanks.","Question: If $f$ is quasi and left continuous on $[a,b]$ and $\alpha$ is increasing and right continuous then $f$ is Riemann-Stieltjes integrable over $[a,b]$ with integrator $\alpha$. Quasi-continuous means $f(x_+)$ and $f(x_-)$ exist for all $x \in [a,b]$. Left {Right} continuous means $f(x)=f(x_-)$ {=$f(x_+)$} Text: Real Analysis by Carothers, also Rudin I have already shown that I can find a partition $P=\{a=t_0<t_1<...<t_n=b\}$ such that for $1 \le k \le n$ the oscillations $\omega(f,(t_{k-1}, t_k])<\epsilon$. I've tried using the Riemann criterion using upper and lower sums but I couldn't make any headway. I can rule out potential ways that it would not be integrable since $f$ and $\alpha$ do not share any common sided discontinuities.  Any suggestions or help would be much appreciated. Thanks.",,"['real-analysis', 'integration', 'analysis', 'partitions-for-integration']"
82,Why Characterize Structure Preserving Functions in Terms of Pre-images?,Why Characterize Structure Preserving Functions in Terms of Pre-images?,,"In Analysis, functions are often characterized as ""structure-preserving"" if structures from codomains are preserved into the domain under the preimage operation.  Specifically, if we let $f: (A, S_1) \rightarrow (B, S_2)$ be a function s.t. $S_1 \subseteq P(A)$ and $S_2 \subseteq P(B)$, then we say that $f$ is ""structure preserving"" if for any arbitrary $s_2 \in S_2$ we have that $f^{-1}(s_2) \in S_1$.  For example, we could let $S_1, S_2$ characterize the open subsets of $A,B$ and consider that the notion of ""continuity"" is that of ""any open subset of $B$ has associated with it an open subset of $A$ under the pre-image operation"".  Alternatively, we could let $S_1$ and $S_2$ characterize measurable subsets of $A$ and $B$ respectively and consider that the notion of ""measurable function"" is characterized as ""a function in which the preimage of a measurable subset of $B$ is itself measurable in $A$"". But why do we characterize the notion of ""structure preserving"" in terms of pre-images instead of images?  Why not, for example, say that our function $f$ is structure preserving if for some $s_1 \in S_1$ we have that $f(s_1) \in S_2$?  This to me seems more intuitive than the traditional approach.","In Analysis, functions are often characterized as ""structure-preserving"" if structures from codomains are preserved into the domain under the preimage operation.  Specifically, if we let $f: (A, S_1) \rightarrow (B, S_2)$ be a function s.t. $S_1 \subseteq P(A)$ and $S_2 \subseteq P(B)$, then we say that $f$ is ""structure preserving"" if for any arbitrary $s_2 \in S_2$ we have that $f^{-1}(s_2) \in S_1$.  For example, we could let $S_1, S_2$ characterize the open subsets of $A,B$ and consider that the notion of ""continuity"" is that of ""any open subset of $B$ has associated with it an open subset of $A$ under the pre-image operation"".  Alternatively, we could let $S_1$ and $S_2$ characterize measurable subsets of $A$ and $B$ respectively and consider that the notion of ""measurable function"" is characterized as ""a function in which the preimage of a measurable subset of $B$ is itself measurable in $A$"". But why do we characterize the notion of ""structure preserving"" in terms of pre-images instead of images?  Why not, for example, say that our function $f$ is structure preserving if for some $s_1 \in S_1$ we have that $f(s_1) \in S_2$?  This to me seems more intuitive than the traditional approach.",,['analysis']
83,Existence of a sequence which is good for mean convergence but not good for pointwise convergence,Existence of a sequence which is good for mean convergence but not good for pointwise convergence,,"The ergodic theorems talk about the limit behaviour of the ergodic averages. For example, if $(X,\chi,\mu,T)$ is a measure preserving system, where $\mu$ is a probability measure on $X$, then we have the mean ergodic theorem: $$\text{If  f}\in L^1(X),\text{ then we have that} \frac{1}{n}\sum_{i=1}^nf(T^ix) \text{ converges in } L^1.$$ and the pointwise ergodic theorem: $$\text{If  f}\in L^1(X),\text{ then we have that } \frac{1}{n}\sum_{i=1}^nf(T^ix) \text{ converges almost everywhere.}$$ Now we can consider a more general ergodic average of the form: $$\frac{1}{n}\sum_{i=1}^nf(T^{a(i)}x)$$  where $\{a(i)\}$ is a sequence of natural numbers with $a(1)<a(2)<\ldots$. If the averages converges in $L^1$, then we say that $\{a(i)\}$ is good for mean convergence, and similarly for pointwise convergence. In general not every sequence is good for the two kinds of convergence. Now my question is that: does there exist a sequence which is good for mean convergence but not good for pointwise convergence? I think such one exists but can not give an example.","The ergodic theorems talk about the limit behaviour of the ergodic averages. For example, if $(X,\chi,\mu,T)$ is a measure preserving system, where $\mu$ is a probability measure on $X$, then we have the mean ergodic theorem: $$\text{If  f}\in L^1(X),\text{ then we have that} \frac{1}{n}\sum_{i=1}^nf(T^ix) \text{ converges in } L^1.$$ and the pointwise ergodic theorem: $$\text{If  f}\in L^1(X),\text{ then we have that } \frac{1}{n}\sum_{i=1}^nf(T^ix) \text{ converges almost everywhere.}$$ Now we can consider a more general ergodic average of the form: $$\frac{1}{n}\sum_{i=1}^nf(T^{a(i)}x)$$  where $\{a(i)\}$ is a sequence of natural numbers with $a(1)<a(2)<\ldots$. If the averages converges in $L^1$, then we say that $\{a(i)\}$ is good for mean convergence, and similarly for pointwise convergence. In general not every sequence is good for the two kinds of convergence. Now my question is that: does there exist a sequence which is good for mean convergence but not good for pointwise convergence? I think such one exists but can not give an example.",,"['analysis', 'ergodic-theory']"
84,Derivative of a matrix norm,Derivative of a matrix norm,,Consider the function $V:\mathbb{R}\to\mathbb{R}$ given by $$ V(t)=\|I - e^{At}\|^2 $$ where $I$ is the identity matrix and $A$ is a square matrix. The norm is the Euclidean norm on $M_n(\mathbb{R})$: $$\|X\|=\sqrt{\lambda_{max}(X'X)};\ X\in M_n(\mathbb{R})$$ that is induced by the matrix norm $\|x\|^2=x'x$ on $\mathbb{R}^n$. I want to calculate the derivative $\frac{dV(t)}{dt}$. Is this possible in any way? Note: Maybe the use of the Frobenius norm would facilitate things a bit but I wouldn't prefer it as it is not an induced norm (by some matrix norm).,Consider the function $V:\mathbb{R}\to\mathbb{R}$ given by $$ V(t)=\|I - e^{At}\|^2 $$ where $I$ is the identity matrix and $A$ is a square matrix. The norm is the Euclidean norm on $M_n(\mathbb{R})$: $$\|X\|=\sqrt{\lambda_{max}(X'X)};\ X\in M_n(\mathbb{R})$$ that is induced by the matrix norm $\|x\|^2=x'x$ on $\mathbb{R}^n$. I want to calculate the derivative $\frac{dV(t)}{dt}$. Is this possible in any way? Note: Maybe the use of the Frobenius norm would facilitate things a bit but I wouldn't prefer it as it is not an induced norm (by some matrix norm).,,"['linear-algebra', 'analysis']"
85,A pseudo Fejér-Jackson inequality problem,A pseudo Fejér-Jackson inequality problem,,"$x\in (0,\pi)$ ,Prove that: \begin{align} \sum_{k=1}^{n}\frac{\sin{kx}}{k}>x\left(1-\frac{x}{\pi}\right)^3 \end{align} the inequality holds for all integer $n$ I tried Fourier, or Dirichlet kernel, but they don't work.Thanks for your attention!","$x\in (0,\pi)$ ,Prove that: \begin{align} \sum_{k=1}^{n}\frac{\sin{kx}}{k}>x\left(1-\frac{x}{\pi}\right)^3 \end{align} the inequality holds for all integer $n$ I tried Fourier, or Dirichlet kernel, but they don't work.Thanks for your attention!",,"['analysis', 'inequality', 'trigonometry']"
86,Series of Maximal Operator,Series of Maximal Operator,,"Let $p\in(1,\infty)$. Assume that we have a sequence of functions $\{f_i:i\in\mathbb{N}\}\subset L^p(\mathbb{R}^n)$ such that $$ \left(\sum\limits_{i=1}^\infty|f_i|^2\right)^{\frac{1}{2}}\in L^p(\mathbb{R}^n)\qquad \{\mu (f_i):i\in\mathbb{N}\}\subset L^p(\mathbb{R}^n) $$ I want to prove that  $$ \left\Vert\left(\sum\limits_{i=1}^\infty|\mu(f_i)|^2\right)^{\frac{1}{2}}\right\Vert_p \leq A_p\left\Vert\left(\sum\limits_{i=1}^\infty|f_i|^2\right)^{\frac{1}{2}}\right\Vert_p $$ Here we can assume that $\mu$ is the Hardy-Littlewood's maximal operator (centered or non-centered).","Let $p\in(1,\infty)$. Assume that we have a sequence of functions $\{f_i:i\in\mathbb{N}\}\subset L^p(\mathbb{R}^n)$ such that $$ \left(\sum\limits_{i=1}^\infty|f_i|^2\right)^{\frac{1}{2}}\in L^p(\mathbb{R}^n)\qquad \{\mu (f_i):i\in\mathbb{N}\}\subset L^p(\mathbb{R}^n) $$ I want to prove that  $$ \left\Vert\left(\sum\limits_{i=1}^\infty|\mu(f_i)|^2\right)^{\frac{1}{2}}\right\Vert_p \leq A_p\left\Vert\left(\sum\limits_{i=1}^\infty|f_i|^2\right)^{\frac{1}{2}}\right\Vert_p $$ Here we can assume that $\mu$ is the Hardy-Littlewood's maximal operator (centered or non-centered).",,"['real-analysis', 'analysis', 'harmonic-analysis']"
87,Question on Cauchy Criterion of Series,Question on Cauchy Criterion of Series,,"We know that the Cauchy Criterion of a series is as follow (proof is taken as excerpt from an analysis book): Theorem : A series $\sum_{j=1}^{\infty}a_j$ converges iff for all $\epsilon>0$ there is an $N\in \mathbb{N}$ so that for all $n\ge m \ge N$ we have $|\sum_{j=m}^{n} a_j|< \epsilon$ . Let $s_n$ denotes partial sum $\sum_{j=1}^n a_j$ . We know that the proof of $\Rightarrow$ direction makes use of the fact that convergent series implies the sequence of partial sum converges and thus is Cauchy in $\mathbb{R}$ . So here is part of the proof, since sequence of partial sum is Cauchy, therefore given $\epsilon>0$ , there exists $N \in \mathbb{N}$ such that for all $n \ge m \ge N$ , $|s_n - s_{m-1}|< \epsilon$ . I do not get the $s_{m-1}$ part, i.e. I do not get why you can pick $m-1$ , since there is possibility for $m=N$ , and thus having $m-1<N$ , which in this case you cannot guarantee the sequence of partial sum Cauchy. When I was thinking about this, I was rather confused, and my original thought was following: Starting from the sequence of partial sum being Cauchy, $|s_n-s_m|< \epsilon$ , for all $n,m\ge N$ (the definition of Cauchy sequence), and without loss of generality we can pick $n>m$ such that $|s_n- s_m|=|\sum_{j=m+1}^{n} a_j|< \epsilon$ , but this differs from the statement of the theorem somehow. I was also thinking of having $m \ne N$ and $m>N$ so that we have $m-1\ge N$ and that the original proof of the theorem that I showed right after the theorem is valid.","We know that the Cauchy Criterion of a series is as follow (proof is taken as excerpt from an analysis book): Theorem : A series converges iff for all there is an so that for all we have . Let denotes partial sum . We know that the proof of direction makes use of the fact that convergent series implies the sequence of partial sum converges and thus is Cauchy in . So here is part of the proof, since sequence of partial sum is Cauchy, therefore given , there exists such that for all , . I do not get the part, i.e. I do not get why you can pick , since there is possibility for , and thus having , which in this case you cannot guarantee the sequence of partial sum Cauchy. When I was thinking about this, I was rather confused, and my original thought was following: Starting from the sequence of partial sum being Cauchy, , for all (the definition of Cauchy sequence), and without loss of generality we can pick such that , but this differs from the statement of the theorem somehow. I was also thinking of having and so that we have and that the original proof of the theorem that I showed right after the theorem is valid.","\sum_{j=1}^{\infty}a_j \epsilon>0 N\in \mathbb{N} n\ge m \ge N |\sum_{j=m}^{n} a_j|< \epsilon s_n \sum_{j=1}^n a_j \Rightarrow \mathbb{R} \epsilon>0 N \in \mathbb{N} n \ge m \ge N |s_n - s_{m-1}|< \epsilon s_{m-1} m-1 m=N m-1<N |s_n-s_m|< \epsilon n,m\ge N n>m |s_n- s_m|=|\sum_{j=m+1}^{n} a_j|< \epsilon m \ne N m>N m-1\ge N","['sequences-and-series', 'analysis']"
88,Arzelà - Ascoli Theorem for an arbitrary family of functions,Arzelà - Ascoli Theorem for an arbitrary family of functions,,"I have just completed an exercise in Abott's Understanding analysis concerning the Arzelà - Ascoli theorem for functions on $\Bbb{R}$. The statement of the exercise is as follows: Let $\{f_n\}$ be an equicontinuous family of functions on $[0,1]$ that is uniformly bounded on the same interval, viz. there exists a constant $M$ such that $|f_n(x)| \leq M$ for all $n \in \Bbb{N}$ and for all $x \in [0,1]$. Then $\{f_n\}$ has a uniformly convergent subsequence. Now this exercise generalises nicely to the following statement for functions on an arbitrary compact metric space nicely, namely: Let $(X,d)$ be a compact metric space and $C(X;\Bbb{R}^n)$ the space of all continuous functions from $X$ to $\Bbb{R}^n$ with the sup metric derived from $d$. If $\{f_n\}$ is an equicontinuous and uniformly bounded family on $C(X;\Bbb{R}^n)$  then $\{f_n\}$ has a uniformly convergent subsequence. This generalises directly from the exercise in Abbott because a compact metric space $X$ is separable (has a countable dense subset $E$) and everything else follows through from the proof for functions on $[0,1]$. Now I am trying to apply the statement of the Arzelà - Ascoli theorem above for metric spaces to prove the following problem below: Let $(X,d)$ bee a compact metric space and let $C(X;\Bbb{R}^n)$ be the space of all continuous functions from $X$ to $\Bbb{R}^n$. Let $\mathcal{F}$ be any subfamily of $C(X;\Bbb{R}^n)$ which is closed, uniformly bounded and uniformly equicontinuous. Then $\mathcal{F}$ is compact in the sup metric on $C(X;\Bbb{R}^n)$ derived from $d$. It seems to me that as long as I can choose a countable subset $\{g_n\}$ of $\mathcal{F}$, the problem should be done by the Arzelà - Ascoli theorem. This is because we already know that $\{g_n\}$ is uniformly bounded and uniformly equicontinuous. Why do we need the hypothesis that $\mathcal{F}$ is closed? Thanks.","I have just completed an exercise in Abott's Understanding analysis concerning the Arzelà - Ascoli theorem for functions on $\Bbb{R}$. The statement of the exercise is as follows: Let $\{f_n\}$ be an equicontinuous family of functions on $[0,1]$ that is uniformly bounded on the same interval, viz. there exists a constant $M$ such that $|f_n(x)| \leq M$ for all $n \in \Bbb{N}$ and for all $x \in [0,1]$. Then $\{f_n\}$ has a uniformly convergent subsequence. Now this exercise generalises nicely to the following statement for functions on an arbitrary compact metric space nicely, namely: Let $(X,d)$ be a compact metric space and $C(X;\Bbb{R}^n)$ the space of all continuous functions from $X$ to $\Bbb{R}^n$ with the sup metric derived from $d$. If $\{f_n\}$ is an equicontinuous and uniformly bounded family on $C(X;\Bbb{R}^n)$  then $\{f_n\}$ has a uniformly convergent subsequence. This generalises directly from the exercise in Abbott because a compact metric space $X$ is separable (has a countable dense subset $E$) and everything else follows through from the proof for functions on $[0,1]$. Now I am trying to apply the statement of the Arzelà - Ascoli theorem above for metric spaces to prove the following problem below: Let $(X,d)$ bee a compact metric space and let $C(X;\Bbb{R}^n)$ be the space of all continuous functions from $X$ to $\Bbb{R}^n$. Let $\mathcal{F}$ be any subfamily of $C(X;\Bbb{R}^n)$ which is closed, uniformly bounded and uniformly equicontinuous. Then $\mathcal{F}$ is compact in the sup metric on $C(X;\Bbb{R}^n)$ derived from $d$. It seems to me that as long as I can choose a countable subset $\{g_n\}$ of $\mathcal{F}$, the problem should be done by the Arzelà - Ascoli theorem. This is because we already know that $\{g_n\}$ is uniformly bounded and uniformly equicontinuous. Why do we need the hypothesis that $\mathcal{F}$ is closed? Thanks.",,['real-analysis']
89,Orthogonal relation between divided functions.,Orthogonal relation between divided functions.,,"Let $m$ and $n$ are two integers such that , If $m \neq n$  then  $$\int _a^b {\dfrac{f_n(x)}{f_m(x)}   \ dx}=0$$ If $m=n$  then  $$\int _a^b {\dfrac{f_n(x)}{f_n(x)} \ dx}=\int _a^b 1 \ dx=b-a$$ I am looking for general properties of such functions ( such as how to find their differential equation, generating functions, polynomials, etc. ) For now I have found one example which is $$ f_n(x)= e^{2\pi inx}\ \rm{for} \ a=0, b=1 .$$ How can we find the general properties of such functions or other functions that satisfy the above property ? Thank you for answers. EDIT: We can find a function expression via using  $f_n(x)$. $$g(x)= \cdots +a_{-2}f_{-2}(x)+a_{-1}f_{-1}(x)+a_0f_0(x)+a_1f_1(x)+a_2f_2(x)+\cdots $$ $$ \int _a^b {\dfrac{g(x)}{f_m(x)} \ dx} =a_m (b-a)$$ or $$g(x)= \cdots + \frac{b_{-2}}{f_{-2}(x)}+ \frac{b_{-1}}{f_{-1}(x)}+ \frac{b_{0}}{f_{0}(x)}+\frac {b_{1}}{f_{1}(x)}+\frac {b_{2}}{f_{2}(x)}+\cdots$$ $$ \int _a^b g(x)f_m(x)  dx=b_m (b-a)$$ Thus $$\frac{a_m}{b_m}=\frac{\int _a^b {\dfrac{g(x)}{f_m(x)} \ dx} }{\int _a^b g(x)f_m(x)  dx}$$ Maybe it can be helpful further analysis to find such functions.","Let $m$ and $n$ are two integers such that , If $m \neq n$  then  $$\int _a^b {\dfrac{f_n(x)}{f_m(x)}   \ dx}=0$$ If $m=n$  then  $$\int _a^b {\dfrac{f_n(x)}{f_n(x)} \ dx}=\int _a^b 1 \ dx=b-a$$ I am looking for general properties of such functions ( such as how to find their differential equation, generating functions, polynomials, etc. ) For now I have found one example which is $$ f_n(x)= e^{2\pi inx}\ \rm{for} \ a=0, b=1 .$$ How can we find the general properties of such functions or other functions that satisfy the above property ? Thank you for answers. EDIT: We can find a function expression via using  $f_n(x)$. $$g(x)= \cdots +a_{-2}f_{-2}(x)+a_{-1}f_{-1}(x)+a_0f_0(x)+a_1f_1(x)+a_2f_2(x)+\cdots $$ $$ \int _a^b {\dfrac{g(x)}{f_m(x)} \ dx} =a_m (b-a)$$ or $$g(x)= \cdots + \frac{b_{-2}}{f_{-2}(x)}+ \frac{b_{-1}}{f_{-1}(x)}+ \frac{b_{0}}{f_{0}(x)}+\frac {b_{1}}{f_{1}(x)}+\frac {b_{2}}{f_{2}(x)}+\cdots$$ $$ \int _a^b g(x)f_m(x)  dx=b_m (b-a)$$ Thus $$\frac{a_m}{b_m}=\frac{\int _a^b {\dfrac{g(x)}{f_m(x)} \ dx} }{\int _a^b g(x)f_m(x)  dx}$$ Maybe it can be helpful further analysis to find such functions.",,"['analysis', 'integration']"
90,The Lebesgue differentiation Theorem holds for any measure?,The Lebesgue differentiation Theorem holds for any measure?,,"What are the assumptions to a measure to the Lebesgue Differentiation Theorem be True? What is(if there is) a practical way to find the decomposition of a measure $\lambda$ in relation to a measure $\mu$? OBS: I am only interested on non negative measure, but answers dealing with signed measures are welcomed as well.","What are the assumptions to a measure to the Lebesgue Differentiation Theorem be True? What is(if there is) a practical way to find the decomposition of a measure $\lambda$ in relation to a measure $\mu$? OBS: I am only interested on non negative measure, but answers dealing with signed measures are welcomed as well.",,"['analysis', 'measure-theory']"
91,"Transformation of a Taylor series: ""doubling"" the derivative order","Transformation of a Taylor series: ""doubling"" the derivative order",,"Suppose a function $f(z)$ has a convergent Taylor expansion: $$f(z)=\sum_{n=0}^{\infty} c_n \frac{z^n}{n!}$$ Are there general tools to compute $$g(z) = \sum_{n=0}^{\infty} c_{2n} \frac{z^n}{n!}=\sum_{n=0}^{\infty} \, f^{(2n)}(0)\frac{z^n}{n!} \text{ ?} $$ I came across this general problem in a more specific context explained in this question . One possible way is to do an integral transform  $$\dfrac{1}{s} \int_0^{\infty} e^{-z/s} f(z) dz$$  to remove the factorials, then replace $s \to \sqrt{s}$  and do the inverse transform. However, this easily leads to very complicated integrals, and does not seem to work for my particular problem.","Suppose a function $f(z)$ has a convergent Taylor expansion: $$f(z)=\sum_{n=0}^{\infty} c_n \frac{z^n}{n!}$$ Are there general tools to compute $$g(z) = \sum_{n=0}^{\infty} c_{2n} \frac{z^n}{n!}=\sum_{n=0}^{\infty} \, f^{(2n)}(0)\frac{z^n}{n!} \text{ ?} $$ I came across this general problem in a more specific context explained in this question . One possible way is to do an integral transform  $$\dfrac{1}{s} \int_0^{\infty} e^{-z/s} f(z) dz$$  to remove the factorials, then replace $s \to \sqrt{s}$  and do the inverse transform. However, this easily leads to very complicated integrals, and does not seem to work for my particular problem.",,"['analysis', 'power-series']"
92,Change of Variables Clarification,Change of Variables Clarification,,"How can we show that $v(L(C)) = |\det DL|v(C)$ for any open cube $C$ an element of $\mathbb{R}^n$ and any linear transformation $L: \mathbb{R}^n \rightarrow \mathbb{R}^n$, without direct applying the change of variables theorem? Thanks","How can we show that $v(L(C)) = |\det DL|v(C)$ for any open cube $C$ an element of $\mathbb{R}^n$ and any linear transformation $L: \mathbb{R}^n \rightarrow \mathbb{R}^n$, without direct applying the change of variables theorem? Thanks",,['analysis']
93,Help with removing singularities involving $ \int_{1}^{\infty} \exp\left( - \frac{x^2}{2y^{2r}} - \frac{y^2}{2}\right) \frac{dy}{y^r}$,Help with removing singularities involving, \int_{1}^{\infty} \exp\left( - \frac{x^2}{2y^{2r}} - \frac{y^2}{2}\right) \frac{dy}{y^r},"This post can be thought of as the prototype proof and the motivation for the question posted here Using a laplace type expansion to get bounds on an integral arising in the study of Brownian motion Let $ 0 < r < 1$, fix $x > 1$ and consider the integral $$ I_{1}(x) = \int_{1}^{\infty} \exp\left( - \frac{x^2}{2y^{2r}} - \frac{y^2}{2}\right) \frac{dy}{y^r}.$$ The result we have been trying to prove below can be thought of as an analysis of localizing around the maximum of the function $f(y) = \frac{x^2}{2y^{2r}} + \frac{y^2}{2}$ in the exponential of the integrand and then applying appropriate estimates along with integration by parts. Roughly speaking I am having trouble with the analysis on one side of the maximum. Let $I_1(x) = \int_{1}^{\infty}  \exp( -\frac{x^2}{2y^{2r}} - \frac{y^2}{2} ) \frac{dy}{y^r}$ and fix a constant $c^* = r^{\frac{1}{2r+2}} $ that is $c^* = argmin ( d(c) )$ where $d(c) = \frac{1}{2} (c^{-2r} + c^2)$ and let $x^* = x^{\frac{1}{1+r}}$.  Then we break up the integral as $$I_1(x): = I_{11}(x) + I_{12}(x) =  \int_{1}^{c^* x^*}  \exp( -\frac{x^2}{2y^{2r}} - \frac{y^2}{2} ) \frac{dy}{y^r} +  \int_{c^*x^*}^{\infty}  \exp( -\frac{x^2}{2y^{2r}} - \frac{y^2}{2} ) \frac{dy}{y^r} $$ The particular trouble I am having arises with estimating $I_{11}$. Using integration by parts on $I_{11}(x)$ we set $f(y) =  \frac{x^2}{2y^{2r}} + \frac{y^2}{2}$ and write  $$ I_{11}(x)  =  \int_{1}^{c^* x^*}  \exp( -\frac{x^2}{2y^{2r}} - \frac{y^2}{2} ) \frac{dy}{y^r}  = \int_{1}^{c^* x^*} \frac{y^{-r}}{f'(y)} f'(y) e^{-f(y)} dy$$ Let $v = \frac{1}{y^r f'(y)}, du = f'(y) e^{-f(y)}dy$ so that $u = - e^{-f(y)}$ and $dv = \frac{-r y^{-r-1}f'(y) -y^{r} f''(y) }{(f'(y))^2}$ and write $$dv = \frac{-r y^{-r-1}}{f'(y)} dy - \frac{y^{-r} f''(y)}{(f'(y))^2}dy := v_1(y)dy + v_2(y)dy$$ Applying integration by parts to $I_{11}(x)$ we get $$ \int_{1}^{c^* x^*} \frac{y^{-r}}{f'(y)} f'(y) e^{-f(y)} dy = \left[- \frac{y^{-r}}{f'(y)} exp(-f(y)) \right]_{1}^{c^* x^*} +  \int_{1}^{c^* x^*} e^{-f(y)} (v_1(y) +v_2(y)) dy $$ 1) Using this argument is it possible to show $I_{11}(x) \leq k(r,x)\exp(- x^{\frac{2}{1+r}} d(c^*))$ where $k(r,x)$ is a rational function in $x$? The issue seems to be for us finding a way to remove the singularities around $v_1$ and $v_2$ near $c^* x^*$.","This post can be thought of as the prototype proof and the motivation for the question posted here Using a laplace type expansion to get bounds on an integral arising in the study of Brownian motion Let $ 0 < r < 1$, fix $x > 1$ and consider the integral $$ I_{1}(x) = \int_{1}^{\infty} \exp\left( - \frac{x^2}{2y^{2r}} - \frac{y^2}{2}\right) \frac{dy}{y^r}.$$ The result we have been trying to prove below can be thought of as an analysis of localizing around the maximum of the function $f(y) = \frac{x^2}{2y^{2r}} + \frac{y^2}{2}$ in the exponential of the integrand and then applying appropriate estimates along with integration by parts. Roughly speaking I am having trouble with the analysis on one side of the maximum. Let $I_1(x) = \int_{1}^{\infty}  \exp( -\frac{x^2}{2y^{2r}} - \frac{y^2}{2} ) \frac{dy}{y^r}$ and fix a constant $c^* = r^{\frac{1}{2r+2}} $ that is $c^* = argmin ( d(c) )$ where $d(c) = \frac{1}{2} (c^{-2r} + c^2)$ and let $x^* = x^{\frac{1}{1+r}}$.  Then we break up the integral as $$I_1(x): = I_{11}(x) + I_{12}(x) =  \int_{1}^{c^* x^*}  \exp( -\frac{x^2}{2y^{2r}} - \frac{y^2}{2} ) \frac{dy}{y^r} +  \int_{c^*x^*}^{\infty}  \exp( -\frac{x^2}{2y^{2r}} - \frac{y^2}{2} ) \frac{dy}{y^r} $$ The particular trouble I am having arises with estimating $I_{11}$. Using integration by parts on $I_{11}(x)$ we set $f(y) =  \frac{x^2}{2y^{2r}} + \frac{y^2}{2}$ and write  $$ I_{11}(x)  =  \int_{1}^{c^* x^*}  \exp( -\frac{x^2}{2y^{2r}} - \frac{y^2}{2} ) \frac{dy}{y^r}  = \int_{1}^{c^* x^*} \frac{y^{-r}}{f'(y)} f'(y) e^{-f(y)} dy$$ Let $v = \frac{1}{y^r f'(y)}, du = f'(y) e^{-f(y)}dy$ so that $u = - e^{-f(y)}$ and $dv = \frac{-r y^{-r-1}f'(y) -y^{r} f''(y) }{(f'(y))^2}$ and write $$dv = \frac{-r y^{-r-1}}{f'(y)} dy - \frac{y^{-r} f''(y)}{(f'(y))^2}dy := v_1(y)dy + v_2(y)dy$$ Applying integration by parts to $I_{11}(x)$ we get $$ \int_{1}^{c^* x^*} \frac{y^{-r}}{f'(y)} f'(y) e^{-f(y)} dy = \left[- \frac{y^{-r}}{f'(y)} exp(-f(y)) \right]_{1}^{c^* x^*} +  \int_{1}^{c^* x^*} e^{-f(y)} (v_1(y) +v_2(y)) dy $$ 1) Using this argument is it possible to show $I_{11}(x) \leq k(r,x)\exp(- x^{\frac{2}{1+r}} d(c^*))$ where $k(r,x)$ is a rational function in $x$? The issue seems to be for us finding a way to remove the singularities around $v_1$ and $v_2$ near $c^* x^*$.",,"['calculus', 'real-analysis', 'analysis']"
94,Curvatures of contours of solutions of 3d Poisson's equation,Curvatures of contours of solutions of 3d Poisson's equation,,"Let $f(x,y,z)$ be a complex function in a 3d euclidian space that fulfill  the Poisson's equation $$\frac{\partial^2}{\partial x^2} f + \frac{\partial^2}{\partial y^2} f + \frac{\partial^2}{\partial z^2} f = c,$$ where c is a real number. Let $X$ be a surface being a contour surface of $|f|^2$ , i.e. $|f(x,y,z)|^2=k$ for some $k\geq0$ . Can anything be said on the curvature (mean or gaussian) of $X$ ? (I.e. if it is bounded from above or below? If, within one surface, its changes are bounded?) Motivation: I am interested in the penetration length of evanescent waves . Then the Poisson equation can be interpreted as the wave equation for a monochromatic wave, where $f$ is one component of electrical field. Surface of the same |f| are surfaces of the same intensity.","Let be a complex function in a 3d euclidian space that fulfill  the Poisson's equation where c is a real number. Let be a surface being a contour surface of , i.e. for some . Can anything be said on the curvature (mean or gaussian) of ? (I.e. if it is bounded from above or below? If, within one surface, its changes are bounded?) Motivation: I am interested in the penetration length of evanescent waves . Then the Poisson equation can be interpreted as the wave equation for a monochromatic wave, where is one component of electrical field. Surface of the same |f| are surfaces of the same intensity.","f(x,y,z) \frac{\partial^2}{\partial x^2} f + \frac{\partial^2}{\partial y^2} f + \frac{\partial^2}{\partial z^2} f = c, X |f|^2 |f(x,y,z)|^2=k k\geq0 X f","['analysis', 'partial-differential-equations', 'harmonic-analysis', 'curvature']"
95,"Iterating Arithmetic, Harmonic and Geometric Means","Iterating Arithmetic, Harmonic and Geometric Means",,"Starting with a data set $X_{0}$, compute its arithmetic, geometric and harmonic means, $A(X_{0}), G(X_{0})$ and $H(X_{0})$ respectively. Let $X_{1} = \{A(X_{0}),G(X_{0}),H(X_{0})\}$, and compute $A(X_{1}), G(X_{1})$ and $H(X_{1})$. Extending this, we recursively define $X_{n} = \{A(X_{n-1}),G(X_{n-1}),H(X_{n-1})\}$. Is there anything interesting about the behavior of the $\{X_{n}\}$ with regards to convergence, does a closed form representation exist, and how does it depend on the initial data set $X_{0}$? Any insight would be appreciated. Thanks!","Starting with a data set $X_{0}$, compute its arithmetic, geometric and harmonic means, $A(X_{0}), G(X_{0})$ and $H(X_{0})$ respectively. Let $X_{1} = \{A(X_{0}),G(X_{0}),H(X_{0})\}$, and compute $A(X_{1}), G(X_{1})$ and $H(X_{1})$. Extending this, we recursively define $X_{n} = \{A(X_{n-1}),G(X_{n-1}),H(X_{n-1})\}$. Is there anything interesting about the behavior of the $\{X_{n}\}$ with regards to convergence, does a closed form representation exist, and how does it depend on the initial data set $X_{0}$? Any insight would be appreciated. Thanks!",,['analysis']
96,Ways to define the directional derivative,Ways to define the directional derivative,,"I am reading a text where directional derivatives of functions $f:E\rightarrow F$, where $E,F$ are Banach spaces, at the point $x_0 \in E$ are defined as $d_v f(x_0)=\lim_{t\rightarrow 0+} \frac{f(x_0+tv)-f(x_0)}{t}$ for any $v\in E$. My question is: Why is the limit taken with respect to ""$t\rightarrow 0+$"" instead of just ""$t\rightarrow 0$"" ? What does it change if we have ""$t\rightarrow 0+$ instead of ""$t\rightarrow 0$"" ? Since in some textbooks, just the """"$t\rightarrow 0$"" version is used.","I am reading a text where directional derivatives of functions $f:E\rightarrow F$, where $E,F$ are Banach spaces, at the point $x_0 \in E$ are defined as $d_v f(x_0)=\lim_{t\rightarrow 0+} \frac{f(x_0+tv)-f(x_0)}{t}$ for any $v\in E$. My question is: Why is the limit taken with respect to ""$t\rightarrow 0+$"" instead of just ""$t\rightarrow 0$"" ? What does it change if we have ""$t\rightarrow 0+$ instead of ""$t\rightarrow 0$"" ? Since in some textbooks, just the """"$t\rightarrow 0$"" version is used.",,['analysis']
97,Can Egoroff's Theorem be Strengthened for A Sequence of Smooth Functions？,Can Egoroff's Theorem be Strengthened for A Sequence of Smooth Functions？,,"I have posted this question on MO, and I'll raise the question here in a more concrete way. To simplify the situaton, we assume the measure space we are dealing with right now is a closed interval $I$ of the real line. When we are using the Egoroff's theorem, little attention has been paid to the exceptional set, i.e. the set $E\subset I$ on which $(f_n)$ fails to converge uniformly. The only thing we know about $E$ is that $m(E)$ can be arbitrary small. Taking the topology on the real line into account, we can also assume that $E$ is closed. Recall that in the proof of this theorem, we constructed the exceptional set $E_{\varepsilon}$ with respect to a fixed $\varepsilon>0$ in the following way: \begin{equation}E_{\varepsilon}=\bigcup_{k=1}^{\infty}\bigcup_{i=n(k,\varepsilon)}^{\infty}\{x\in I:|f_{i}(x)-f(x)|\geq 1/k\},\end{equation} where $n(k,\varepsilon)$ is chosen so that $m(\bigcup_{i=n(k,\varepsilon)}^{\infty}\{x\in I:|f_{i}(x)-f(x)|\geq 1/k\})<\varepsilon/2^k$. Now assume $(f_n)$ is a sequence of smooth functions, and $f\in L^1(I)$ is the limit function. Since $f$ is only determined up to a null set, the set $E_{\varepsilon}$ can only be determined up to a null set. Thus it is very natural to require $f$ to be the ""best choice"" to make $E_{\varepsilon}$ as small as possible. A satisfatory and notable case is that the family $\{E_{\varepsilon}\}$ be a sequence of nested closed intervals, thus we have $\bigcap_{\varepsilon}E_{\varepsilon}=\{x_{0}\}$, where $x_{0}\in I$. Or more generally, we can ask in which case the set $\bigcap_{\varepsilon}E_{\varepsilon}$ is a union of isolated points? However, this is not true in general case, GTM 2 (p.38) [Graduate Texts in Mathematics] contains an example in which case the set $I-E$ is nowhere dense, of course this coincides with the well-known result that every subset of the line can be represented as a union of a null set and a set of first category.","I have posted this question on MO, and I'll raise the question here in a more concrete way. To simplify the situaton, we assume the measure space we are dealing with right now is a closed interval $I$ of the real line. When we are using the Egoroff's theorem, little attention has been paid to the exceptional set, i.e. the set $E\subset I$ on which $(f_n)$ fails to converge uniformly. The only thing we know about $E$ is that $m(E)$ can be arbitrary small. Taking the topology on the real line into account, we can also assume that $E$ is closed. Recall that in the proof of this theorem, we constructed the exceptional set $E_{\varepsilon}$ with respect to a fixed $\varepsilon>0$ in the following way: \begin{equation}E_{\varepsilon}=\bigcup_{k=1}^{\infty}\bigcup_{i=n(k,\varepsilon)}^{\infty}\{x\in I:|f_{i}(x)-f(x)|\geq 1/k\},\end{equation} where $n(k,\varepsilon)$ is chosen so that $m(\bigcup_{i=n(k,\varepsilon)}^{\infty}\{x\in I:|f_{i}(x)-f(x)|\geq 1/k\})<\varepsilon/2^k$. Now assume $(f_n)$ is a sequence of smooth functions, and $f\in L^1(I)$ is the limit function. Since $f$ is only determined up to a null set, the set $E_{\varepsilon}$ can only be determined up to a null set. Thus it is very natural to require $f$ to be the ""best choice"" to make $E_{\varepsilon}$ as small as possible. A satisfatory and notable case is that the family $\{E_{\varepsilon}\}$ be a sequence of nested closed intervals, thus we have $\bigcap_{\varepsilon}E_{\varepsilon}=\{x_{0}\}$, where $x_{0}\in I$. Or more generally, we can ask in which case the set $\bigcap_{\varepsilon}E_{\varepsilon}$ is a union of isolated points? However, this is not true in general case, GTM 2 (p.38) [Graduate Texts in Mathematics] contains an example in which case the set $I-E$ is nowhere dense, of course this coincides with the well-known result that every subset of the line can be represented as a union of a null set and a set of first category.",,"['analysis', 'measure-theory', 'set-theory']"
98,Stuck at the proof of the Riemann-Lebesgue lemma,Stuck at the proof of the Riemann-Lebesgue lemma,,"I'm currently trying to prove the Riemann-Lebesgue lemma using lower Darboux-sums and an approximation of any integrable function $f: [0,1] \to \mathbb{R}$ defined as $$t(x) := \begin{cases} m_i & \text{for } x_i \leq x < x_{i+1},\; i < n\\ m_n & \text{for } x= 1 \end{cases},$$ where $m_i := \inf\nolimits_{\xi \in [x_i, x_{i+1}]} f(\xi)$ and $P = \{x_i\}_{1 \leq i \leq n}$ is a partition of the interval $[0,1]$. Now I want to show that $$\int_0^1 t(x) \cos(\lambda x) \; \mathrm dx \leq \varepsilon$$ for some $\varepsilon > 0$. Our assistant professor told us that it was possible to actually integrate $t(x) \cos(\lambda x)$ but I just don't see how to. I guess I would have to make use of the product rule twice (as $\int t(x) \; \mathrm dx = s(f,P)$), but the problem is that I don't know the integral of the lower Darboux-sum (or whether it even exists) and also I don't know the derivative of $t(x)$ (or whether it exists). How am I supposed to proceed? Is there a different trick to integrate $t(x) \cos(\lambda x)$? Thanks for any answers in advance.","I'm currently trying to prove the Riemann-Lebesgue lemma using lower Darboux-sums and an approximation of any integrable function $f: [0,1] \to \mathbb{R}$ defined as $$t(x) := \begin{cases} m_i & \text{for } x_i \leq x < x_{i+1},\; i < n\\ m_n & \text{for } x= 1 \end{cases},$$ where $m_i := \inf\nolimits_{\xi \in [x_i, x_{i+1}]} f(\xi)$ and $P = \{x_i\}_{1 \leq i \leq n}$ is a partition of the interval $[0,1]$. Now I want to show that $$\int_0^1 t(x) \cos(\lambda x) \; \mathrm dx \leq \varepsilon$$ for some $\varepsilon > 0$. Our assistant professor told us that it was possible to actually integrate $t(x) \cos(\lambda x)$ but I just don't see how to. I guess I would have to make use of the product rule twice (as $\int t(x) \; \mathrm dx = s(f,P)$), but the problem is that I don't know the integral of the lower Darboux-sum (or whether it even exists) and also I don't know the derivative of $t(x)$ (or whether it exists). How am I supposed to proceed? Is there a different trick to integrate $t(x) \cos(\lambda x)$? Thanks for any answers in advance.",,"['analysis', 'integration']"
99,Analysing an optics model in discrete and continuous forms,Analysing an optics model in discrete and continuous forms,,"A discrete one-dimensional model of optical imaging looks like this: $$I(r) = \sum_i e_i P(r - r_i)$$ Here, the $e_i$ are point light sources at locations $r_i$ in the object and $P$ is a point spread function that blurs each point. We can assume that $P$ is even, non-negative and has a finite extent, ie $P(x) = 0$ for $|x| > p$. The $e_i$ are all positive. A more complex imaging process instead produces an image like this: $$I(r)^2 = \sum_{i,j} e_i e_j P(r - r_i) P(r - r_j) \cos (r_i - r_j)$$ (The $^2$ is a consequence of the reconstruction method. In practice we normally take the square root, but per my earlier question here , I think the salient structural features of the resulting image should be unchanged by this transformation.) Numerical simulations suggest that the latter method allows better resolution of the points when $p \sim \pi$, and is not materially different when $p \ll \pi$. (We ignore the case where $p \gg \pi$ as not physically tenable.) Intuitively, this is because the ""trough"" in the $\cos (r_i - r_j)$ term reduces the interaction between points at some intermediate separations. I'd like to be able to be more analytical about this. We can readily extend the above to a continuous model with an object function $O$ in place of the $e_i$. The ordinary image becomes a convolution integral $$I(r) = \int O(s) \: P(r - s) \; \mathrm{d}s$$ for which there are standard analytical approaches available. However, the more complex reconstruction looks something like this: $$I(r)^2 = \int \int  O(s) \; O(t) \; P(r-s) \; P(r-t) \; \cos (s - t) \;\mathrm{d}s\, \mathrm{d}t$$ This looks fairly intractable to me, and I'm not really sure where to begin with it. So, my first question is: is there any point in trying to look at the continuous model, or should I just concentrate on the discrete? On the one hand, every single practical use case for the technique will actually be using discrete measurements, so in that sense the discrete model is more realistic. On the other, it may be that there are things one could demonstrate using the continuous model that are not available otherwise. (But even if that's the case, I'm probably not capable of doing so without at least being nudged in the right direction!) And I guess my second question is: continuous or not, does anyone have any other suggestions of useful ways to analyse this model?","A discrete one-dimensional model of optical imaging looks like this: $$I(r) = \sum_i e_i P(r - r_i)$$ Here, the $e_i$ are point light sources at locations $r_i$ in the object and $P$ is a point spread function that blurs each point. We can assume that $P$ is even, non-negative and has a finite extent, ie $P(x) = 0$ for $|x| > p$. The $e_i$ are all positive. A more complex imaging process instead produces an image like this: $$I(r)^2 = \sum_{i,j} e_i e_j P(r - r_i) P(r - r_j) \cos (r_i - r_j)$$ (The $^2$ is a consequence of the reconstruction method. In practice we normally take the square root, but per my earlier question here , I think the salient structural features of the resulting image should be unchanged by this transformation.) Numerical simulations suggest that the latter method allows better resolution of the points when $p \sim \pi$, and is not materially different when $p \ll \pi$. (We ignore the case where $p \gg \pi$ as not physically tenable.) Intuitively, this is because the ""trough"" in the $\cos (r_i - r_j)$ term reduces the interaction between points at some intermediate separations. I'd like to be able to be more analytical about this. We can readily extend the above to a continuous model with an object function $O$ in place of the $e_i$. The ordinary image becomes a convolution integral $$I(r) = \int O(s) \: P(r - s) \; \mathrm{d}s$$ for which there are standard analytical approaches available. However, the more complex reconstruction looks something like this: $$I(r)^2 = \int \int  O(s) \; O(t) \; P(r-s) \; P(r-t) \; \cos (s - t) \;\mathrm{d}s\, \mathrm{d}t$$ This looks fairly intractable to me, and I'm not really sure where to begin with it. So, my first question is: is there any point in trying to look at the continuous model, or should I just concentrate on the discrete? On the one hand, every single practical use case for the technique will actually be using discrete measurements, so in that sense the discrete model is more realistic. On the other, it may be that there are things one could demonstrate using the continuous model that are not available otherwise. (But even if that's the case, I'm probably not capable of doing so without at least being nudged in the right direction!) And I guess my second question is: continuous or not, does anyone have any other suggestions of useful ways to analyse this model?",,"['analysis', 'signal-processing', 'physics']"
