,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Expected Angle of a Random 2D Vector,Expected Angle of a Random 2D Vector,,"I was playing around a bit until I noticed that the definition of sample correlation coefficient of two variables is literally the definition of the dot product of two realizations of the variable $$ r\equiv \frac{\mathbf{x}\cdot\mathbf{y}}{|x||y|} = \cos\phi$$ Given a single observation $\mathbf{x} = (x_1,x_2) \in \Omega_{X,Y}$, note $$\cos\phi = \frac{\mathbf{x}\cdot\hat{i}}{|x|}=\frac{x}{|x|}$$ In which case  $$\mathbb{E}[\cos\phi] = \int\limits_{\mathbf{x}\in\Omega}\cos(\phi|\mathbf{x})\mathbb{P}(x_1,x_2)$$ The continuous correlation coefficient is: $$\rho \equiv \frac{\mathbb{E}[XY]}{\sigma_X\sigma_Y}$$ I'm wondering if the correlation coefficient can be interpreted as the expected angle of a random 2D vector? If not, can you interpret the correlation coefficient of a 2D vector geometrically?","I was playing around a bit until I noticed that the definition of sample correlation coefficient of two variables is literally the definition of the dot product of two realizations of the variable $$ r\equiv \frac{\mathbf{x}\cdot\mathbf{y}}{|x||y|} = \cos\phi$$ Given a single observation $\mathbf{x} = (x_1,x_2) \in \Omega_{X,Y}$, note $$\cos\phi = \frac{\mathbf{x}\cdot\hat{i}}{|x|}=\frac{x}{|x|}$$ In which case  $$\mathbb{E}[\cos\phi] = \int\limits_{\mathbf{x}\in\Omega}\cos(\phi|\mathbf{x})\mathbb{P}(x_1,x_2)$$ The continuous correlation coefficient is: $$\rho \equiv \frac{\mathbb{E}[XY]}{\sigma_X\sigma_Y}$$ I'm wondering if the correlation coefficient can be interpreted as the expected angle of a random 2D vector? If not, can you interpret the correlation coefficient of a 2D vector geometrically?",,"['probability', 'statistics']"
1,Is weighting by relative variance a meaningful way to weight variables?,Is weighting by relative variance a meaningful way to weight variables?,,"For a school project, I am trying to determine similarity between participants based on different aspects of their athletic mechanics. I have the same number of observations per participant, and 8 variables per observation. I need to algorithmically determine weights for each variable. My current idea is to weight each variable by the variance of the variable across all observations. My reasoning is that variables that have greater variability will be better indicators of the similarity of participants, since variables with low variance will just clump all of the participants together. I've set the weight of the most varied variable to 1, and all others are weighted relative to this variable. Would this be considered a reasonable way to determine weights? Is there a peer-reviewed method of weighting similar types of variables? I could not find one that I clearly understood. Thanks in advance!","For a school project, I am trying to determine similarity between participants based on different aspects of their athletic mechanics. I have the same number of observations per participant, and 8 variables per observation. I need to algorithmically determine weights for each variable. My current idea is to weight each variable by the variance of the variable across all observations. My reasoning is that variables that have greater variability will be better indicators of the similarity of participants, since variables with low variance will just clump all of the participants together. I've set the weight of the most varied variable to 1, and all others are weighted relative to this variable. Would this be considered a reasonable way to determine weights? Is there a peer-reviewed method of weighting similar types of variables? I could not find one that I clearly understood. Thanks in advance!",,"['statistics', 'standard-deviation', 'variance']"
2,Differential entropy of Gaussian kernel density estimator?,Differential entropy of Gaussian kernel density estimator?,,"The kernel density estimator (when constructed using a Gaussian kernel) is $\hat{f}(x)=\frac{1}{n} \sum_{i=1}^{n} \frac{1}{\sqrt{2 \pi \sigma^{2}}}e^{\frac{-(x-x_{i})^{2}}{2 \sigma^{2}}}$ for a data set of size $n$. Let $\sigma$ be the bandwidth. I'm trying to find its differential entropy, as measured in nats (so that the base of the logarithm is e ): $- \int_{-\infty}^{\infty} \hat{f}(x) \ln(\hat{f}(x))dx$ I have no idea how to go about computing this. Can anyone tell me a closed form for the differential entropy of this kernel density estimator, for an arbitrary set of observations { $x_{1}, x_{2},... x_{n}$ }? (I'm being very liberal with the phrase ""closed form,"" I just need the integral to vanish.)","The kernel density estimator (when constructed using a Gaussian kernel) is $\hat{f}(x)=\frac{1}{n} \sum_{i=1}^{n} \frac{1}{\sqrt{2 \pi \sigma^{2}}}e^{\frac{-(x-x_{i})^{2}}{2 \sigma^{2}}}$ for a data set of size $n$. Let $\sigma$ be the bandwidth. I'm trying to find its differential entropy, as measured in nats (so that the base of the logarithm is e ): $- \int_{-\infty}^{\infty} \hat{f}(x) \ln(\hat{f}(x))dx$ I have no idea how to go about computing this. Can anyone tell me a closed form for the differential entropy of this kernel density estimator, for an arbitrary set of observations { $x_{1}, x_{2},... x_{n}$ }? (I'm being very liberal with the phrase ""closed form,"" I just need the integral to vanish.)",,"['statistics', 'improper-integrals', 'statistical-inference', 'entropy']"
3,How to interpret data given from statistical software question?,How to interpret data given from statistical software question?,,Hello I am trying to answer the following question. (Apologies for the image but it is perhaps clearer than me doing a table here). I am quite stuck on a few points and wanted some help. I think the distribution is a $t$ distribution with $6$ degrees of freedom. As we have $9$ countries recorded and $2$ things recorded so we get $9-(2+1)=6$. For b) I know the formula that $\text{Estimate} ~\div~\text{Standard Error}=\text{t-value}$ so for the second column we can figure out the standard error is $1.415$. Now for the top row I want to find the $t$ value corresponding to the $p$ value of $0.05$. So we want the number such that $95%$ of the distribution is less than the absolute value of that number. So I think this gives $P(>|t|)=0.05$ so we look in the tables to find the value of $t$ that gives $0.025$ in the upper tail (since by symmetry the lower tail will contribute the other $0.025$). This gives the value as $2.447$ and so the corresponding standard error is $6.0073$. This is the main thing I am not too sure about so if anyone could help me out and explain if I am wrong what I should be doing differently that would be great. Oh and for c) I said yes I would agree since $0.0003<0.01$.,Hello I am trying to answer the following question. (Apologies for the image but it is perhaps clearer than me doing a table here). I am quite stuck on a few points and wanted some help. I think the distribution is a $t$ distribution with $6$ degrees of freedom. As we have $9$ countries recorded and $2$ things recorded so we get $9-(2+1)=6$. For b) I know the formula that $\text{Estimate} ~\div~\text{Standard Error}=\text{t-value}$ so for the second column we can figure out the standard error is $1.415$. Now for the top row I want to find the $t$ value corresponding to the $p$ value of $0.05$. So we want the number such that $95%$ of the distribution is less than the absolute value of that number. So I think this gives $P(>|t|)=0.05$ so we look in the tables to find the value of $t$ that gives $0.025$ in the upper tail (since by symmetry the lower tail will contribute the other $0.025$). This gives the value as $2.447$ and so the corresponding standard error is $6.0073$. This is the main thing I am not too sure about so if anyone could help me out and explain if I am wrong what I should be doing differently that would be great. Oh and for c) I said yes I would agree since $0.0003<0.01$.,,"['probability', 'statistics']"
4,Normal Probability Plot: points oscillate around line of slope 1: polynomial?,Normal Probability Plot: points oscillate around line of slope 1: polynomial?,,"This is the Normal Probability Plot I've obtained for some data: As you can see the points seem to osciallate, as in a period, around the line of slope 1 and passing through the origin. I've obtained this plot by trying to fit a simple linear regression model. Can I deduce from this peculiar behaviour (just as an Anzat, or a guess, waiting for further tests) that this can indicate that there is a polynomial relationship? (for example the grade of the polynomial might be equal to the number of times the oscillation crosses the line) Or otherwise, what does this suggest? That there is a trigonometric relationship? Edit Basically Ive fitted a linear model data.lm <- lm(y~x, data=data) , then Ive created the standardized residuals data.lm$sr <- rstandard(data.lm) and finally plotted the qqplot qqnorm(data.lm$sr) and I've added abline(0,1)","This is the Normal Probability Plot I've obtained for some data: As you can see the points seem to osciallate, as in a period, around the line of slope 1 and passing through the origin. I've obtained this plot by trying to fit a simple linear regression model. Can I deduce from this peculiar behaviour (just as an Anzat, or a guess, waiting for further tests) that this can indicate that there is a polynomial relationship? (for example the grade of the polynomial might be equal to the number of times the oscillation crosses the line) Or otherwise, what does this suggest? That there is a trigonometric relationship? Edit Basically Ive fitted a linear model data.lm <- lm(y~x, data=data) , then Ive created the standardized residuals data.lm$sr <- rstandard(data.lm) and finally plotted the qqplot qqnorm(data.lm$sr) and I've added abline(0,1)",,"['statistics', 'regression']"
5,Sum of independent standard normal and chi square with one degree of freedom,Sum of independent standard normal and chi square with one degree of freedom,,"I need to calculate the distribution of the sum of two independent random variable: the first is a standard normal and the second is a chi square with one degree of freedom. I know that the sum has density given by the convolution between the respective densities, but i have trouble figuring out what this calculation leads to. Can anyone help me?","I need to calculate the distribution of the sum of two independent random variable: the first is a standard normal and the second is a chi square with one degree of freedom. I know that the sum has density given by the convolution between the respective densities, but i have trouble figuring out what this calculation leads to. Can anyone help me?",,"['probability', 'statistics', 'convolution']"
6,Solving an inverse problem with machine learning,Solving an inverse problem with machine learning,,"I am running up against a very tough inverse problem that I suspect might be solvable using machine learning. Here is the problem. Overview I am studying an object $X$ which, internally, is composed of two functions $f_1(x)$ and $f_2(x)$, $x \in [0,1]$. I want to know what $f_1$ and $f_2$ are, but I can't measure them directly. What I can measure is a list of numbers $\omega_1, \omega_2, \ldots, \omega_N$ which are determined by $f_1$ and $f_2$. Using these numbers $\vec \omega$, I want to infer $f_1$ and $f_2$ the best I can. The current approach When $N$ is very large and the measurement errors on each of the $\omega$'s is small, the following method works well. I have a model that I consider to be a good but not exact description of $X$. It differs in its internal $f_1$ and $f_2$'s, which causes differences in $\omega$. Additionally, there is an arbitrary polynomial difference in $\omega$ whose form is known but whose coefficients are unknown. Using the relative differences between the $\vec \omega$'s that come from the model and the $\vec \omega$'s that I observe in $X$, and also fitting for the polynomial difference in $\omega$, I can calculate the relative difference in $f_1$ and $f_2$ between the model and $X$ using the following system of equations: $$ \delta \omega_i = c_1(\delta\omega)^{-1} + c_2(\delta\omega)^3 + \int (K^{f_1}_i \cdot \delta f_1 + K^{f_2}_i \cdot \delta f_2) \; \text{d}x, \qquad i=1,\ldots,N$$ where $c_1$ and $c_2$ are constants that have to be determined (in the least squares sense) along with $\delta f_1$ and $\delta f_2$. The functions $K^{f_1}_i$ and $K^{f_2}_i$ are known functions called kernel functions that do not need to be estimated - they come for free from the model. Clearly this is an inverse problem because $\delta f_1$ and $\delta f_2$ are trapped in that integral. However, by representing $\delta f_1$ and $\delta f_2$ by basis functions, we can estimate the coefficients of the basis functions using normal techniques (e.g. regularized least squares) and then determine $f_1$ and $f_2$. A new approach? I am studying $X$'s for which $N$ is small and the measurement error on the $\omega$'s is large. The previous method fails to give reliable estimates, which I can test by using two models and pretending one of them is $X$. I am thinking that a machine learning approach might work here. In particular, I can calculate a large number of models and ask how their values of $\vec \omega$ correspond to $f_1$ and $f_2$'s. Then, I can put in the $\vec \omega$ that I observe and get out the real $f_1$ and $f_2$. That is, I will train a machine learning algorithm to predict $$f(\vec \omega) = [f_1(x_i), f_2(x_i)], \; x_i \in [0.01, 0.02, \ldots, 1.00]$$ based on a large number of simulations. The problem that I am coming up against is the arbitrary function of $\omega$ that I mentioned in the previous section. There is always a difference in practice between the real $\vec \omega$ and the model $\vec \omega$. We know the form of the difference is a polynomial with a cubic term and an inverse term, but we don't know the coefficients - they are determined simultaneously when determining the differences due to $f_1$ and $f_2$. Any advice?","I am running up against a very tough inverse problem that I suspect might be solvable using machine learning. Here is the problem. Overview I am studying an object $X$ which, internally, is composed of two functions $f_1(x)$ and $f_2(x)$, $x \in [0,1]$. I want to know what $f_1$ and $f_2$ are, but I can't measure them directly. What I can measure is a list of numbers $\omega_1, \omega_2, \ldots, \omega_N$ which are determined by $f_1$ and $f_2$. Using these numbers $\vec \omega$, I want to infer $f_1$ and $f_2$ the best I can. The current approach When $N$ is very large and the measurement errors on each of the $\omega$'s is small, the following method works well. I have a model that I consider to be a good but not exact description of $X$. It differs in its internal $f_1$ and $f_2$'s, which causes differences in $\omega$. Additionally, there is an arbitrary polynomial difference in $\omega$ whose form is known but whose coefficients are unknown. Using the relative differences between the $\vec \omega$'s that come from the model and the $\vec \omega$'s that I observe in $X$, and also fitting for the polynomial difference in $\omega$, I can calculate the relative difference in $f_1$ and $f_2$ between the model and $X$ using the following system of equations: $$ \delta \omega_i = c_1(\delta\omega)^{-1} + c_2(\delta\omega)^3 + \int (K^{f_1}_i \cdot \delta f_1 + K^{f_2}_i \cdot \delta f_2) \; \text{d}x, \qquad i=1,\ldots,N$$ where $c_1$ and $c_2$ are constants that have to be determined (in the least squares sense) along with $\delta f_1$ and $\delta f_2$. The functions $K^{f_1}_i$ and $K^{f_2}_i$ are known functions called kernel functions that do not need to be estimated - they come for free from the model. Clearly this is an inverse problem because $\delta f_1$ and $\delta f_2$ are trapped in that integral. However, by representing $\delta f_1$ and $\delta f_2$ by basis functions, we can estimate the coefficients of the basis functions using normal techniques (e.g. regularized least squares) and then determine $f_1$ and $f_2$. A new approach? I am studying $X$'s for which $N$ is small and the measurement error on the $\omega$'s is large. The previous method fails to give reliable estimates, which I can test by using two models and pretending one of them is $X$. I am thinking that a machine learning approach might work here. In particular, I can calculate a large number of models and ask how their values of $\vec \omega$ correspond to $f_1$ and $f_2$'s. Then, I can put in the $\vec \omega$ that I observe and get out the real $f_1$ and $f_2$. That is, I will train a machine learning algorithm to predict $$f(\vec \omega) = [f_1(x_i), f_2(x_i)], \; x_i \in [0.01, 0.02, \ldots, 1.00]$$ based on a large number of simulations. The problem that I am coming up against is the arbitrary function of $\omega$ that I mentioned in the previous section. There is always a difference in practice between the real $\vec \omega$ and the model $\vec \omega$. We know the form of the difference is a polynomial with a cubic term and an inverse term, but we don't know the coefficients - they are determined simultaneously when determining the differences due to $f_1$ and $f_2$. Any advice?",,"['statistics', 'inverse', 'machine-learning', 'estimation', 'inverse-problems']"
7,Statistics of a racketball/volleyball/badminton game,Statistics of a racketball/volleyball/badminton game,,"It's been a while since I've used my knowledge in statistics and I have no idea how to turn that problem into an equation. I wanted to challenge myself but I failed. I thought maybe you too would like this challenge. Starting with pre-defined score and who currently has the ball, I'd like to estimate the odds of winning of each player/team assuming they are precisely as likely to win any exchange. A player/team scores a point every time it wins an exchange when it has the ball. If it wins an exchange when the other player/team has the ball, the scores stays the same but it gets the ball and the chance to score some points. A racketball game, for example, ends when a player reaches 15 points but the game may continue as long as no one leads by 2 points (15-14 is not a valid final score, but 16-14 is). There is theoretically two possibilities that this game never ends : either you enter a loop where no-one scores any point or no-one can lead by two points, and this is what I find very tricky. Still I am pretty convinced that this can be solved, just not by a newbie like me. This problem has 4 variables : The current score of the player/team who has the ball The current score of the other player/team The score at which a game normally ends The minimum number of points by which the winning team/player has to lead This question looks like mine but starts with the final score. Maybe it could be useful anyway.","It's been a while since I've used my knowledge in statistics and I have no idea how to turn that problem into an equation. I wanted to challenge myself but I failed. I thought maybe you too would like this challenge. Starting with pre-defined score and who currently has the ball, I'd like to estimate the odds of winning of each player/team assuming they are precisely as likely to win any exchange. A player/team scores a point every time it wins an exchange when it has the ball. If it wins an exchange when the other player/team has the ball, the scores stays the same but it gets the ball and the chance to score some points. A racketball game, for example, ends when a player reaches 15 points but the game may continue as long as no one leads by 2 points (15-14 is not a valid final score, but 16-14 is). There is theoretically two possibilities that this game never ends : either you enter a loop where no-one scores any point or no-one can lead by two points, and this is what I find very tricky. Still I am pretty convinced that this can be solved, just not by a newbie like me. This problem has 4 variables : The current score of the player/team who has the ball The current score of the other player/team The score at which a game normally ends The minimum number of points by which the winning team/player has to lead This question looks like mine but starts with the final score. Maybe it could be useful anyway.",,['statistics']
8,MMSE estimation: Unable to verify property of MMSE,MMSE estimation: Unable to verify property of MMSE,,"$\newcommand{\Var}{\operatorname{Var}}$I am trying to verify,using code, property of MMSE estimator. If $X_M=E[X\mid Y]$ is unbiased estimator of $X$, property I want to verify is $$\Var(X) = \Var(X_M) + \Var(\tilde{X})$$ where $\tilde{X}$ is the error in estimation. I am quoting from THIS online resource. Please tell me where I am wrong. I am writing code in octave. For matlab, equivalent function to randsample is datasample . Say X, Y have joint distribution.  I computed PDF for X and PDF for E(X|Y) Joint PDF of X and Y X|Y  -1    0   1 2    1/6  2/6  0 3    2/6   0   0 4     0    0  1/6 PDF of X 1/2   2/6   1/6   X  2     3     4 PDF of Z=E(X|Y) 2/6   1/2   1/6 Z=E[X/Y]  2    8/3    4 CODE nSamples = 100000;  RX = [2 3 4]; PrX = [1/2 2/6 1/6]; X = randsample(RX,nSamples,true,PrX);  RZ = [2 8/3 4]; PrZ = [2/6 1/2 1/6]; Z = randsample(RZ,nSamples,true,PrZ);  Xtilde = X - Z; disp(var(X)) disp(var(Z)) disp(var(Xtilde))","$\newcommand{\Var}{\operatorname{Var}}$I am trying to verify,using code, property of MMSE estimator. If $X_M=E[X\mid Y]$ is unbiased estimator of $X$, property I want to verify is $$\Var(X) = \Var(X_M) + \Var(\tilde{X})$$ where $\tilde{X}$ is the error in estimation. I am quoting from THIS online resource. Please tell me where I am wrong. I am writing code in octave. For matlab, equivalent function to randsample is datasample . Say X, Y have joint distribution.  I computed PDF for X and PDF for E(X|Y) Joint PDF of X and Y X|Y  -1    0   1 2    1/6  2/6  0 3    2/6   0   0 4     0    0  1/6 PDF of X 1/2   2/6   1/6   X  2     3     4 PDF of Z=E(X|Y) 2/6   1/2   1/6 Z=E[X/Y]  2    8/3    4 CODE nSamples = 100000;  RX = [2 3 4]; PrX = [1/2 2/6 1/6]; X = randsample(RX,nSamples,true,PrX);  RZ = [2 8/3 4]; PrZ = [2/6 1/2 1/6]; Z = randsample(RZ,nSamples,true,PrZ);  Xtilde = X - Z; disp(var(X)) disp(var(Z)) disp(var(Xtilde))",,"['probability', 'statistics', 'statistical-inference']"
9,"is Cov(X, E(X|Y)) equal to VAR( E(X|Y) )??","is Cov(X, E(X|Y)) equal to VAR( E(X|Y) )??",,"I am writing a code which uses the value of $Cov(X,E(X|Y))$. I thought it will be equal to $Var( E(X|Y) )$. But my output seemed to be incorrect. Please let me know whether my understanding is correct. Let $Z = E(X|Y)$ which is basically $f(Y)$ and $$cov(X,Z) = E(XZ) - E(X)E(Z)$$ where $E(XZ) = E[ E(XZ|Y) ] = E [ E(f(Y)X|Y) ] = E[f(Y) E(X|Y) ] = E(Z^2)$ and $E(Z) = E(X)$ so then $Cov(X,Z) = E(Z^2) - E(Z) ^ 2$ which is $Var(Z)$ EDIT: with CODE I am writing code in octave. I will simplify my code. Say X, Y have joint distribution.  I computed PDF for X and PDF for E(X|Y) Joint PDF of X and Y X|Y  -1    0   1 2    1/6  2/6  0 3    2/6   0   0 4     0    0  1/6 PDF of X 1/2   2/6   1/6   X  2     3     4 PDF of E(X|Y) 2/6   1/2   1/6 Z=E[X/Y]  2    8/3    4 CODE nSamples = 100000;  RX = [2 3 4]; PrX = [1/2 2/6 1/6]; X = randsample(RX,nSamples,true,PrX);  RZ = [2 8/3 4]; PrZ = [2/6 1/2 1/6]; Z = randsample(RZ,nSamples,true,PrZ);  disp(cov(X,Z)) disp(var(Z))","I am writing a code which uses the value of $Cov(X,E(X|Y))$. I thought it will be equal to $Var( E(X|Y) )$. But my output seemed to be incorrect. Please let me know whether my understanding is correct. Let $Z = E(X|Y)$ which is basically $f(Y)$ and $$cov(X,Z) = E(XZ) - E(X)E(Z)$$ where $E(XZ) = E[ E(XZ|Y) ] = E [ E(f(Y)X|Y) ] = E[f(Y) E(X|Y) ] = E(Z^2)$ and $E(Z) = E(X)$ so then $Cov(X,Z) = E(Z^2) - E(Z) ^ 2$ which is $Var(Z)$ EDIT: with CODE I am writing code in octave. I will simplify my code. Say X, Y have joint distribution.  I computed PDF for X and PDF for E(X|Y) Joint PDF of X and Y X|Y  -1    0   1 2    1/6  2/6  0 3    2/6   0   0 4     0    0  1/6 PDF of X 1/2   2/6   1/6   X  2     3     4 PDF of E(X|Y) 2/6   1/2   1/6 Z=E[X/Y]  2    8/3    4 CODE nSamples = 100000;  RX = [2 3 4]; PrX = [1/2 2/6 1/6]; X = randsample(RX,nSamples,true,PrX);  RZ = [2 8/3 4]; PrZ = [2/6 1/2 1/6]; Z = randsample(RZ,nSamples,true,PrZ);  disp(cov(X,Z)) disp(var(Z))",,"['probability', 'statistics']"
10,Norm of covariance between scalar and vector,Norm of covariance between scalar and vector,,"I would like to show that if $Y\in\Re$ is a random real vector such that $\|Y\|≤ \bar Y$ and $X \in \mathcal H$ where $\mathcal H$ is a Hilbert space (whose field is $\Re$), then  $$ \|\operatorname{Cov} (Y,X)\| = \|E(YX^*)\| ≤ \bar Y \|X\|_∞. $$ The first idea is the following: if $X∈ \Re^p$, then I think that $$ \|\operatorname{Cov} (Y,X)\| = \|E(YX^T)\| ≤ \sqrt{\operatorname{Var} (Y)}\max_{i=1,\ldots,p}\sqrt{\operatorname{Var}  (X_i)} $$ which would yield the first equation based on the fact that if $|Z|≤\nu$, then $\operatorname{Var} (Z) ≤ ν^2$. The second idea (which yields the desired inequality in the real case when $X_i$ are independent one to each other) is to consider the fact that $X$ has limited total information upon $Y$.","I would like to show that if $Y\in\Re$ is a random real vector such that $\|Y\|≤ \bar Y$ and $X \in \mathcal H$ where $\mathcal H$ is a Hilbert space (whose field is $\Re$), then  $$ \|\operatorname{Cov} (Y,X)\| = \|E(YX^*)\| ≤ \bar Y \|X\|_∞. $$ The first idea is the following: if $X∈ \Re^p$, then I think that $$ \|\operatorname{Cov} (Y,X)\| = \|E(YX^T)\| ≤ \sqrt{\operatorname{Var} (Y)}\max_{i=1,\ldots,p}\sqrt{\operatorname{Var}  (X_i)} $$ which would yield the first equation based on the fact that if $|Z|≤\nu$, then $\operatorname{Var} (Z) ≤ ν^2$. The second idea (which yields the desired inequality in the real case when $X_i$ are independent one to each other) is to consider the fact that $X$ has limited total information upon $Y$.",,"['probability', 'functional-analysis', 'statistics', 'random-variables', 'covariance']"
11,Linear Regression: units of intecepts given $x$ and $y$ as log function?,Linear Regression: units of intecepts given  and  as log function?,x y,"Given a line $y = ax + b$, where $x = log(w)$ and $y = log(l)$. To me, the units of $b$ should be unitless as $y$ will be unitless. Is this right?","Given a line $y = ax + b$, where $x = log(w)$ and $y = log(l)$. To me, the units of $b$ should be unitless as $y$ will be unitless. Is this right?",,"['statistics', 'regression']"
12,Confusion - The probability of the sum of two dice is $\frac{b-a+1}{36}$,Confusion - The probability of the sum of two dice is,\frac{b-a+1}{36},"So the problem is: The solution writes: I understood all the steps except the step where I highlighted. Why is the probability of the $Y$, which is the sum of the two values $\frac{b-a+1}{36}$? I acknowledged that the $b-a+1$ is the total number of the discrete values from $b$ to $a$, but I thought if $x=k$ is from $a$ to $b$, then $Y$ should be $2(b-a+1)$ since it is the sum of two dice.","So the problem is: The solution writes: I understood all the steps except the step where I highlighted. Why is the probability of the $Y$, which is the sum of the two values $\frac{b-a+1}{36}$? I acknowledged that the $b-a+1$ is the total number of the discrete values from $b$ to $a$, but I thought if $x=k$ is from $a$ to $b$, then $Y$ should be $2(b-a+1)$ since it is the sum of two dice.",,"['probability', 'probability-theory', 'statistics', 'expectation', 'markov-chains']"
13,Curve fitting on dataset,Curve fitting on dataset,,"For my master's thesis I'm writing on a specific subject which requires curve fitting. In the first part I fixed everything with 12th degree polynomial fits. But when I derive the data from the place measures, to get the speeds, I get a curve which is hard to fit. The curve looks like a sinewave, but they go much more pointy on the minimas. Does anyone have any idea about what a good polynomial or other function would be for this kind of curves? My Curve I have tried this already with 12th degree polynomial and with some sorts of sine wave. But maybe it might be a good idea to combine a sine wave and a triangle wave? EDIT: as people are advising me to get into sine waves, the reason why I don't do this is because I need to fit a lot of datasets which are completely different to this dataset. I made another screenshot to show that sine waves are not a real option for me. This dataset would be way harder with a sine wave","For my master's thesis I'm writing on a specific subject which requires curve fitting. In the first part I fixed everything with 12th degree polynomial fits. But when I derive the data from the place measures, to get the speeds, I get a curve which is hard to fit. The curve looks like a sinewave, but they go much more pointy on the minimas. Does anyone have any idea about what a good polynomial or other function would be for this kind of curves? My Curve I have tried this already with 12th degree polynomial and with some sorts of sine wave. But maybe it might be a good idea to combine a sine wave and a triangle wave? EDIT: as people are advising me to get into sine waves, the reason why I don't do this is because I need to fit a lot of datasets which are completely different to this dataset. I made another screenshot to show that sine waves are not a real option for me. This dataset would be way harder with a sine wave",,"['statistics', 'algebraic-curves', 'curves', 'data-analysis']"
14,Bounding a sum of covariances under $\alpha$-mixing,Bounding a sum of covariances under -mixing,\alpha,"Assume that $Y=(Y_t, t \in \mathbb{N})$ is a strictly stationary $\alpha$-mixing process, and let $G_h(x)=G(x/h)=\int_{x/h}^{+\infty}K(u)du$, where $K$ is a Kernel function(symetric density in this case), and $h$ is the sequence of bandwidths, that satisfy certain properties, which can be found in the assumptions of Chen's work, that I will provide a link for, shortly. Moreover, assume that Y is a geometric $\alpha$-mixing process (i.e, there exist $C, \rho>0$ such that  $\alpha(k)\leq \rho^k$, for all $k\geq 1$), and let $\nu_p$ be the $(1-p)$-th order quantile of Y_t. I am trying to show that, for $i=0, 1$ and $j=0, 1$, \begin{align*} \Bigg|\sum_{k=1}^{n-1}\left(1-\frac{k}{n}\right)\Big[\text{Cov}\Big(Y_{1}^{i}G_{h}&(\nu_{p}-Y_{1}), Y_{k+1}^{j}G_{h}(\nu_{p}-Y_{k+1})\Big) \\ &-\text{Cov}\left(Y_{1}^{i}I\left(Y_{1}>\nu_{p}\right), Y_{k+1}^{j}I\left(Y_{k+1}>\nu_{p}\right)\right)\Big]\Bigg| = o\left(h\right). \end{align*} This lemma is in Chen's work, and he doesn't prove it. He refers to another article of his for the proof of i=j=0, and says the other cases are similar. You can find it here, under Lemma 4. http://www.public.iastate.edu/~songchen/shortfall.pdf The proof for i=j=0 is done in two steps. The first is to conclude that $$\Bigg|\text{Cov}\Big(G_{h}(\nu_{p}-Y_{1}), G_{h}(\nu_{p}-Y_{k+1})\Big)-\text{Cov}\left(I\left(Y_{1}>\nu_{p}\right), I\left(Y_{k+1}>\nu_{p}\right)\right)\Bigg| \leq Ch^2,$$ which can be found in Cai and Roussas http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.47.7779&rep=rep1&type=pdf under lemma 2.2 (i). The proof provides interesting insight. The second step is to notice that, under the assumption of an $\alpha$-mixing process, we get that $$\sup\limits_{(x,y) \in \mathbb{R}^2} \left|F_k(x,y)-F(x)F(y)\right| \leq \alpha(k)$$ Now, since  $$\text{Cov}\left(I\left(Y_{1}>\nu_{p}\right), I\left(Y_{k+1}>\nu_{p}\right)\right) = F_k(\nu_p,\nu_p)-F^2(\nu_p) \leq \sup\limits_{(x,y) \in \mathbb{R}^2} \left|F_k(x,y)-F(x)F(y)\right| \leq \alpha(k),$$ and using the same idea as is done in Cai and Roussas, from a change of variables and Hoeffding's identity we get \begin{align*} \text{Cov}&\Big(G_{h}(\nu_{p}-Y_{1}), G_{h}(\nu_{p}-Y_{k+1})\Big) \\ &= \int_{\mathbb{R}^2}\left[ F_k\left(\nu_p-hr, \nu_p-hs\right)-F\left(\nu_p-hr\right)F\left(\nu_p-hs\right) \right] K\left(r\right)K\left(s\right) dr ds \\ &\leq \sup\limits_{(x,y) \in \mathbb{R}^2} \left|F_k(x,y)-F(x)F(y)\right| \int_{\mathbb{R}^2} K\left(r\right)K\left(s\right) dr ds \\ &= \sup\limits_{(x,y) \in \mathbb{R}^2} \left|F_k(x,y)-F(x)F(y)\right| \leq \alpha(k) \end{align*} we get that  $$\Bigg|\text{Cov}\Big(G_{h}(\nu_{p}-Y_{1}), G_{h}(\nu_{p}-Y_{k+1})\Big)-\text{Cov}\left(I\left(Y_{1}>\nu_{p}\right), I\left(Y_{k+1}>\nu_{p}\right)\right)\Bigg| \leq 2\alpha(k).$$ Let us write $ \gamma_h^{i,j}(k) = \text{Cov}\left(Y_{1}^i G_{h}\left(\nu_{p}-Y_{1}\right), Y_{k+1}^j G_{h}\left(\nu_{p}-Y_{k+1}\right)\right)$ and $\gamma^{i,j}(k) = \text{Cov}\left(Y_{1}^i I\left(Y_{1}>\nu_{p}\right), Y_{k+1}^j I\left(Y_{k+1}>\nu_{p}\right)\right)$. These two bounds we provided for $\Bigg|\gamma_h^{0,0}(k)-\gamma^{0,0}(k)\Bigg|$ allow us write $$|\gamma_h^{0,0}(k)-\gamma^{0,0}(k)|= |\gamma_h^{0,0}(k)-\gamma^{0,0}(k)|^{2/3} |\gamma_h^{0,0}(k)-\gamma^{0,0}(k)|^{1/3}\leq Ch^{4/3}\alpha^{1/3}(k),$$ which allows us to conclude $$\left|\sum_{k=1}^{n-1}\left(1-\frac{k}{n}\right) \left[\gamma_h^{0,0}\left(k\right)-\gamma^{0,0}\left(k\right)\right]\right| \leq \sum_{k=1}^{n-1} \left|\left[ \gamma_h^{0,0}\left(k\right)-\gamma^{0,0}\left(k\right)\right]\right| \leq Ch^{4/3}\sum_{k=1}^\infty \alpha^{1/3}(k) = o\left(h\right),$$ which concludes the proof for the case i=j=0. In his article, Chen says that for the other cases of i and j, the proof is similar, but I can not conclude. I have already shown that, for i=j=1,  $$\Bigg|\gamma_h^{1,1}(k)-\gamma^{1,1}(k)\Bigg|\leq Ch^2.$$ However, I am not being able to show anything of the sort $\Bigg|\gamma_h^{1,1}(k)-\gamma^{1,1}(k)\Bigg| \leq C\alpha(k)$. This would allow me to conclude. Thank you very much for reading and for any help you can provide.","Assume that $Y=(Y_t, t \in \mathbb{N})$ is a strictly stationary $\alpha$-mixing process, and let $G_h(x)=G(x/h)=\int_{x/h}^{+\infty}K(u)du$, where $K$ is a Kernel function(symetric density in this case), and $h$ is the sequence of bandwidths, that satisfy certain properties, which can be found in the assumptions of Chen's work, that I will provide a link for, shortly. Moreover, assume that Y is a geometric $\alpha$-mixing process (i.e, there exist $C, \rho>0$ such that  $\alpha(k)\leq \rho^k$, for all $k\geq 1$), and let $\nu_p$ be the $(1-p)$-th order quantile of Y_t. I am trying to show that, for $i=0, 1$ and $j=0, 1$, \begin{align*} \Bigg|\sum_{k=1}^{n-1}\left(1-\frac{k}{n}\right)\Big[\text{Cov}\Big(Y_{1}^{i}G_{h}&(\nu_{p}-Y_{1}), Y_{k+1}^{j}G_{h}(\nu_{p}-Y_{k+1})\Big) \\ &-\text{Cov}\left(Y_{1}^{i}I\left(Y_{1}>\nu_{p}\right), Y_{k+1}^{j}I\left(Y_{k+1}>\nu_{p}\right)\right)\Big]\Bigg| = o\left(h\right). \end{align*} This lemma is in Chen's work, and he doesn't prove it. He refers to another article of his for the proof of i=j=0, and says the other cases are similar. You can find it here, under Lemma 4. http://www.public.iastate.edu/~songchen/shortfall.pdf The proof for i=j=0 is done in two steps. The first is to conclude that $$\Bigg|\text{Cov}\Big(G_{h}(\nu_{p}-Y_{1}), G_{h}(\nu_{p}-Y_{k+1})\Big)-\text{Cov}\left(I\left(Y_{1}>\nu_{p}\right), I\left(Y_{k+1}>\nu_{p}\right)\right)\Bigg| \leq Ch^2,$$ which can be found in Cai and Roussas http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.47.7779&rep=rep1&type=pdf under lemma 2.2 (i). The proof provides interesting insight. The second step is to notice that, under the assumption of an $\alpha$-mixing process, we get that $$\sup\limits_{(x,y) \in \mathbb{R}^2} \left|F_k(x,y)-F(x)F(y)\right| \leq \alpha(k)$$ Now, since  $$\text{Cov}\left(I\left(Y_{1}>\nu_{p}\right), I\left(Y_{k+1}>\nu_{p}\right)\right) = F_k(\nu_p,\nu_p)-F^2(\nu_p) \leq \sup\limits_{(x,y) \in \mathbb{R}^2} \left|F_k(x,y)-F(x)F(y)\right| \leq \alpha(k),$$ and using the same idea as is done in Cai and Roussas, from a change of variables and Hoeffding's identity we get \begin{align*} \text{Cov}&\Big(G_{h}(\nu_{p}-Y_{1}), G_{h}(\nu_{p}-Y_{k+1})\Big) \\ &= \int_{\mathbb{R}^2}\left[ F_k\left(\nu_p-hr, \nu_p-hs\right)-F\left(\nu_p-hr\right)F\left(\nu_p-hs\right) \right] K\left(r\right)K\left(s\right) dr ds \\ &\leq \sup\limits_{(x,y) \in \mathbb{R}^2} \left|F_k(x,y)-F(x)F(y)\right| \int_{\mathbb{R}^2} K\left(r\right)K\left(s\right) dr ds \\ &= \sup\limits_{(x,y) \in \mathbb{R}^2} \left|F_k(x,y)-F(x)F(y)\right| \leq \alpha(k) \end{align*} we get that  $$\Bigg|\text{Cov}\Big(G_{h}(\nu_{p}-Y_{1}), G_{h}(\nu_{p}-Y_{k+1})\Big)-\text{Cov}\left(I\left(Y_{1}>\nu_{p}\right), I\left(Y_{k+1}>\nu_{p}\right)\right)\Bigg| \leq 2\alpha(k).$$ Let us write $ \gamma_h^{i,j}(k) = \text{Cov}\left(Y_{1}^i G_{h}\left(\nu_{p}-Y_{1}\right), Y_{k+1}^j G_{h}\left(\nu_{p}-Y_{k+1}\right)\right)$ and $\gamma^{i,j}(k) = \text{Cov}\left(Y_{1}^i I\left(Y_{1}>\nu_{p}\right), Y_{k+1}^j I\left(Y_{k+1}>\nu_{p}\right)\right)$. These two bounds we provided for $\Bigg|\gamma_h^{0,0}(k)-\gamma^{0,0}(k)\Bigg|$ allow us write $$|\gamma_h^{0,0}(k)-\gamma^{0,0}(k)|= |\gamma_h^{0,0}(k)-\gamma^{0,0}(k)|^{2/3} |\gamma_h^{0,0}(k)-\gamma^{0,0}(k)|^{1/3}\leq Ch^{4/3}\alpha^{1/3}(k),$$ which allows us to conclude $$\left|\sum_{k=1}^{n-1}\left(1-\frac{k}{n}\right) \left[\gamma_h^{0,0}\left(k\right)-\gamma^{0,0}\left(k\right)\right]\right| \leq \sum_{k=1}^{n-1} \left|\left[ \gamma_h^{0,0}\left(k\right)-\gamma^{0,0}\left(k\right)\right]\right| \leq Ch^{4/3}\sum_{k=1}^\infty \alpha^{1/3}(k) = o\left(h\right),$$ which concludes the proof for the case i=j=0. In his article, Chen says that for the other cases of i and j, the proof is similar, but I can not conclude. I have already shown that, for i=j=1,  $$\Bigg|\gamma_h^{1,1}(k)-\gamma^{1,1}(k)\Bigg|\leq Ch^2.$$ However, I am not being able to show anything of the sort $\Bigg|\gamma_h^{1,1}(k)-\gamma^{1,1}(k)\Bigg| \leq C\alpha(k)$. This would allow me to conclude. Thank you very much for reading and for any help you can provide.",,"['real-analysis', 'probability', 'analysis', 'statistics', 'asymptotics']"
15,Finding the probability density function...,Finding the probability density function...,,"I've found the probability density function for other functions, but this one seems rather difficult. $$F(x)=P(X\le x)=1-e^{-x^2}$$ My teacher's solution manual is showing: $$ \begin{cases} 0, & x<0 \\ 2xe^{-x}, &  x>0 \end{cases} $$ Maybe I am just having a hard time finding this integral, but I don't seem to get the same results. If I use u-substitution I am setting $u = x^2$ where $du = 2x$ then, but how does that get into the answer, am I selecting a wrong $u$? Is it integration by parts maybe? Thanks.","I've found the probability density function for other functions, but this one seems rather difficult. $$F(x)=P(X\le x)=1-e^{-x^2}$$ My teacher's solution manual is showing: $$ \begin{cases} 0, & x<0 \\ 2xe^{-x}, &  x>0 \end{cases} $$ Maybe I am just having a hard time finding this integral, but I don't seem to get the same results. If I use u-substitution I am setting $u = x^2$ where $du = 2x$ then, but how does that get into the answer, am I selecting a wrong $u$? Is it integration by parts maybe? Thanks.",,"['probability', 'statistics']"
16,What kind of CLT is this? Can I prove it using Lindeberg-Levy CLT?,What kind of CLT is this? Can I prove it using Lindeberg-Levy CLT?,,"On deriving the asymptotic distribution of an estimator, I got stuck when proving something like follows: Let $\{X_i(\beta):i=1,2,\ldots,n\}$ be i.i.d. with $\text{E}[X_i(\beta)]=\mu(\beta)$ and $\text{Var}[X_i(\beta)]=\sigma^2(\beta)$, then by Lindeberg-Levy we have $$ \sqrt{n}[X_i(\beta)-\mu(\beta)]\overset{A}{\sim}\mathcal{N}[0,\sigma^2(\beta)]. $$ However, we don't know what $\beta$ is, and thus we must find some estimator for that. Now, assume that we have some estimator for $\beta$ that follows $\hat\beta\overset{p}{\to}\beta$. Then it sure follows that $\text{E}[X_i(\hat\beta)]=\mu(\hat\beta)\overset{p}{\to}\mu(\beta)$ and $\text{Var}[X_i(\hat\beta)]=\sigma^2(\hat\beta)\overset{p}{\to}\sigma(\beta)$. My question is whether it still follows $$ \sqrt{n}[X_i(\hat\beta)-\mu(\beta)]\overset{A}{\sim}\mathcal{N}[0,\sigma^2(\beta)] $$ or not? In other words, is this $X_i(\hat\beta)$ still an efficient estimator for $\mu(\beta)$? Can I use Lindeberg-Levy or Khinchine's WLLN to prove it? I've tried for some hours but with no idea. It's just an intuition. Any suggestion is appreciated, thanks! Also, if you'd like to look into the original problem directly, please go to this question .","On deriving the asymptotic distribution of an estimator, I got stuck when proving something like follows: Let $\{X_i(\beta):i=1,2,\ldots,n\}$ be i.i.d. with $\text{E}[X_i(\beta)]=\mu(\beta)$ and $\text{Var}[X_i(\beta)]=\sigma^2(\beta)$, then by Lindeberg-Levy we have $$ \sqrt{n}[X_i(\beta)-\mu(\beta)]\overset{A}{\sim}\mathcal{N}[0,\sigma^2(\beta)]. $$ However, we don't know what $\beta$ is, and thus we must find some estimator for that. Now, assume that we have some estimator for $\beta$ that follows $\hat\beta\overset{p}{\to}\beta$. Then it sure follows that $\text{E}[X_i(\hat\beta)]=\mu(\hat\beta)\overset{p}{\to}\mu(\beta)$ and $\text{Var}[X_i(\hat\beta)]=\sigma^2(\hat\beta)\overset{p}{\to}\sigma(\beta)$. My question is whether it still follows $$ \sqrt{n}[X_i(\hat\beta)-\mu(\beta)]\overset{A}{\sim}\mathcal{N}[0,\sigma^2(\beta)] $$ or not? In other words, is this $X_i(\hat\beta)$ still an efficient estimator for $\mu(\beta)$? Can I use Lindeberg-Levy or Khinchine's WLLN to prove it? I've tried for some hours but with no idea. It's just an intuition. Any suggestion is appreciated, thanks! Also, if you'd like to look into the original problem directly, please go to this question .",,"['statistics', 'asymptotics', 'regression', 'estimation', 'central-limit-theorem']"
17,Is the definition of z-score correct?,Is the definition of z-score correct?,,"I'm taking a course in Statistics, and I'm confused by the following wording which popped up on one of the lessons. Does the explanation sound correct? Are there any terms that are imprecise? Q: We can best describe the location of the sample mean in a sampling distribution in terms of: A: Standard error Explanation: We'll describe the location of the sample mean by calculating how many standard errors it is away from the center of the sampling distribution. That will give us a z-score for our sample mean.","I'm taking a course in Statistics, and I'm confused by the following wording which popped up on one of the lessons. Does the explanation sound correct? Are there any terms that are imprecise? Q: We can best describe the location of the sample mean in a sampling distribution in terms of: A: Standard error Explanation: We'll describe the location of the sample mean by calculating how many standard errors it is away from the center of the sampling distribution. That will give us a z-score for our sample mean.",,"['statistics', 'means']"
18,Randomly place N balls in K buckets. Expected min number of buckets to select G balls?,Randomly place N balls in K buckets. Expected min number of buckets to select G balls?,,"Looking for anything resembling a closed form solution of the following monte-carlo simulation. The basic idea is: Randomly place N balls in K buckets Sort the buckets from greatest to least number of balls Take buckets until you have taken at least G balls How many buckets did you have to take? from random import randint  n = 8000000 # balls k = 80000   # buckets g = 2000    # goal  # initialize buckets to 0 buckets = [0] * k    # populate buckets for _ in xrange(n):   buckets[randint(0,k-1)] += 1  # pick buckets from largest to smallest until g is reached picked = 0  count = 0  for b in reversed(sorted(buckets)):   if picked >= g:     break   picked += b   count += 1  print ""Picked %d balls in %d buckets"" % (picked, count) For the preset n,k,g, the average number of balls per bucket is 100, but if we pick only the largest buckets, most of those have ~140 balls, so it's possible to get the goal 2000 balls in around 14 or 15 buckets rather than the 20 one might expect based on the averages.","Looking for anything resembling a closed form solution of the following monte-carlo simulation. The basic idea is: Randomly place N balls in K buckets Sort the buckets from greatest to least number of balls Take buckets until you have taken at least G balls How many buckets did you have to take? from random import randint  n = 8000000 # balls k = 80000   # buckets g = 2000    # goal  # initialize buckets to 0 buckets = [0] * k    # populate buckets for _ in xrange(n):   buckets[randint(0,k-1)] += 1  # pick buckets from largest to smallest until g is reached picked = 0  count = 0  for b in reversed(sorted(buckets)):   if picked >= g:     break   picked += b   count += 1  print ""Picked %d balls in %d buckets"" % (picked, count) For the preset n,k,g, the average number of balls per bucket is 100, but if we pick only the largest buckets, most of those have ~140 balls, so it's possible to get the goal 2000 balls in around 14 or 15 buckets rather than the 20 one might expect based on the averages.",,"['probability', 'statistics', 'discrete-mathematics', 'monte-carlo', 'balls-in-bins']"
19,What's the right way of finding the median of the lower and upper quartiles of a box and whisker plot?,What's the right way of finding the median of the lower and upper quartiles of a box and whisker plot?,,"Let's say I have a set of test scores from 20 applicants $5, 15, 16, 20, 21, 25, 26, 27, 30, 30, 31, 32, 32, 34, 35, 38, 38, 41, 43, 66$ Obviously, the median of this set would be $30$ and $31$, so I would just get the average, though that's not what I'm looking for. What I'm really looking for is the median of the lower and upper quartiles. I have found 2 ways, both yielding different answers. 1st way : Finding the lower quartile, the range would be $5$ to $30$, and the median of the set would be $21$. This is based on a video that I watched. 2nd way : Same problem, but instead of just finding the median, I use a formula. $x_{kth}=(\frac{k}{4})n=\frac{1}{4}(20)=5$ This means the median for the lower quartile is the 5th spot, which is $21$. Sounds good, except that I have to do an extra step. Get the average of the values $x_k$ and $x_{k+1}$ positions. This means I have to average the 5th and 6th positions. $Q_1=\frac{5_{th}+6_{th}}{2}=\frac{21+25}{2}=23$ This is entirely different from $21$. This method came from a textbook. Which way is correct?","Let's say I have a set of test scores from 20 applicants $5, 15, 16, 20, 21, 25, 26, 27, 30, 30, 31, 32, 32, 34, 35, 38, 38, 41, 43, 66$ Obviously, the median of this set would be $30$ and $31$, so I would just get the average, though that's not what I'm looking for. What I'm really looking for is the median of the lower and upper quartiles. I have found 2 ways, both yielding different answers. 1st way : Finding the lower quartile, the range would be $5$ to $30$, and the median of the set would be $21$. This is based on a video that I watched. 2nd way : Same problem, but instead of just finding the median, I use a formula. $x_{kth}=(\frac{k}{4})n=\frac{1}{4}(20)=5$ This means the median for the lower quartile is the 5th spot, which is $21$. Sounds good, except that I have to do an extra step. Get the average of the values $x_k$ and $x_{k+1}$ positions. This means I have to average the 5th and 6th positions. $Q_1=\frac{5_{th}+6_{th}}{2}=\frac{21+25}{2}=23$ This is entirely different from $21$. This method came from a textbook. Which way is correct?",,"['statistics', 'order-statistics']"
20,Weightest Rankings Table for Office Pool League,Weightest Rankings Table for Office Pool League,,"My office recently got a pool table and many of us here are quite competitive, others are not but everyone is interested in a office pool ranking system. I have created a spreadsheet system for logging results but I cannot figure out a way of ranking fairly. I ideally we would like to have a system of weighted rankings, so if a low ranked player (say using data from the last month) beats the office pool champ it has a higher effect than than if the office champ beats Mr low-rank. I thought about a system of using only the scores against OTHER opponents to use for the adjusting of the players rankings but I can't figure out a system where this works. it seems like high ranking players might lose ranking by beating lower level opposition. All the matches are just W/L checks, no other variables. Does anyone know if a sensible matrix/formula/not sure that would be good for us to use here. This one has be a little stumped.","My office recently got a pool table and many of us here are quite competitive, others are not but everyone is interested in a office pool ranking system. I have created a spreadsheet system for logging results but I cannot figure out a way of ranking fairly. I ideally we would like to have a system of weighted rankings, so if a low ranked player (say using data from the last month) beats the office pool champ it has a higher effect than than if the office champ beats Mr low-rank. I thought about a system of using only the scores against OTHER opponents to use for the adjusting of the players rankings but I can't figure out a system where this works. it seems like high ranking players might lose ranking by beating lower level opposition. All the matches are just W/L checks, no other variables. Does anyone know if a sensible matrix/formula/not sure that would be good for us to use here. This one has be a little stumped.",,"['matrices', 'statistics']"
21,What's the intuition behind sub gaussian's moment?,What's the intuition behind sub gaussian's moment?,,"In Vershynin's book about high dimensional probability: http://www-personal.umich.edu/~romanv/teaching/2015-16/626/HDP-book.pdf , Proposition 2.5.2 states that $X$ is a sub gaussian r.v. iff, for some $K_1$, the tail of $X$ satisfies $\mathbb{P}\{|X|>t\}\leq2\exp(-t^2/K_1),~\forall t\geq0$ or equivalently, for some $K_2$, the moment of $X$ satisfies $||X||_p=(\mathbb{E}|X|^p)^{1/p}\leq K_2\sqrt{p}$ The first property is evident from the pdf of gaussian, but the second property is not really clear. Though it can be shown that the moment of the standard gaussian is $O(\sqrt{p})$, it does not really tell why it is $\sqrt{p}$.","In Vershynin's book about high dimensional probability: http://www-personal.umich.edu/~romanv/teaching/2015-16/626/HDP-book.pdf , Proposition 2.5.2 states that $X$ is a sub gaussian r.v. iff, for some $K_1$, the tail of $X$ satisfies $\mathbb{P}\{|X|>t\}\leq2\exp(-t^2/K_1),~\forall t\geq0$ or equivalently, for some $K_2$, the moment of $X$ satisfies $||X||_p=(\mathbb{E}|X|^p)^{1/p}\leq K_2\sqrt{p}$ The first property is evident from the pdf of gaussian, but the second property is not really clear. Though it can be shown that the moment of the standard gaussian is $O(\sqrt{p})$, it does not really tell why it is $\sqrt{p}$.",,"['probability', 'probability-theory', 'statistics', 'concentration-of-measure']"
22,"""Outlier"" vs ""Potential Outlier""","""Outlier"" vs ""Potential Outlier""",,"Is an outlier, defined by being further than 1.5x the range of the Interquartile Range outside of the IQR, an actual outlier or merely a potential outlier? Different sources seem to use different semantics.","Is an outlier, defined by being further than 1.5x the range of the Interquartile Range outside of the IQR, an actual outlier or merely a potential outlier? Different sources seem to use different semantics.",,['statistics']
23,Fitting an Akima Spline curve,Fitting an Akima Spline curve,,"I'm trying to fit an Akima Spline curve using the same method as this tool: https://www.mycurvefit.com/share/4ab90a5f-af5e-435e-9ce4-652c95c3d9a7 This curve gives me the exact shape I'm after for my case (the curve line peaking at X = 30M, the highest point from the sample data) But when I try to calculate this using an Akima library function from MathNet, it plots this: https://www.mycurvefit.com/share/faec5545-abf1-4768-b180-3e615dc60e3a Which isn't the same curve at all, I get a curve which peaks around X = 26M, and looks much more like a Natural Spline. By looking at the curve, any ideas as to what the library might be doing differently behind the scenes? Not a code issue, but here's the function I'm using for reference: var x = new List<double> { 0, 15000000, 30000000, 40000000, 60000000 }; var y = new List<double> { 0, 93279805, 108560423, 105689254, 90130257 };  var interpolation = MathNet.Numerics.Interpolation.CubicSpline.InterpolateAkima(x.ToArray(), y.ToArray());  for (int i=1; i<=52; i++) {     var cY = interpolation.Interpolate((60000000d/52d)*i); } What is the reason the Akima's look so different? (especially in terms of peak)","I'm trying to fit an Akima Spline curve using the same method as this tool: https://www.mycurvefit.com/share/4ab90a5f-af5e-435e-9ce4-652c95c3d9a7 This curve gives me the exact shape I'm after for my case (the curve line peaking at X = 30M, the highest point from the sample data) But when I try to calculate this using an Akima library function from MathNet, it plots this: https://www.mycurvefit.com/share/faec5545-abf1-4768-b180-3e615dc60e3a Which isn't the same curve at all, I get a curve which peaks around X = 26M, and looks much more like a Natural Spline. By looking at the curve, any ideas as to what the library might be doing differently behind the scenes? Not a code issue, but here's the function I'm using for reference: var x = new List<double> { 0, 15000000, 30000000, 40000000, 60000000 }; var y = new List<double> { 0, 93279805, 108560423, 105689254, 90130257 };  var interpolation = MathNet.Numerics.Interpolation.CubicSpline.InterpolateAkima(x.ToArray(), y.ToArray());  for (int i=1; i<=52; i++) {     var cY = interpolation.Interpolate((60000000d/52d)*i); } What is the reason the Akima's look so different? (especially in terms of peak)",,"['statistics', 'algebraic-curves', 'interpolation', 'curves', 'spline']"
24,Hypothesis testing for variance,Hypothesis testing for variance,,"From a random sample $X_1;...;X_8\sim \mathcal N(3,σ^2)$, we would like to test whether the variance $σ^2$ can be considered to be at least $5$, against the hypothesis that it is less than 5. We have recorded $\sum_{i=1}^8 (x_i-3)^2 = 37.5$. (i) Perform the relevant test at $1\%$ signicance level and conclude. (ii) In terms of the cumulative distribution function of a known distribution, express the power of the test when (a) $σ^2 = 4$, (b) $σ^2 = 3$ and (c) $σ^2 = 2$. (iii) Explain briefly what would change if we did not know the true mean in the population, but we had recorded the sample mean $x = 3$. I got the answer for part i,the observed t-statistic is $7.5$ so do not reject the null hypothesis. But I have problem with part ii. I try to use the power method but it seems not working as in the statistic T there is only one variance $σ^2$, can anyone helps me out?","From a random sample $X_1;...;X_8\sim \mathcal N(3,σ^2)$, we would like to test whether the variance $σ^2$ can be considered to be at least $5$, against the hypothesis that it is less than 5. We have recorded $\sum_{i=1}^8 (x_i-3)^2 = 37.5$. (i) Perform the relevant test at $1\%$ signicance level and conclude. (ii) In terms of the cumulative distribution function of a known distribution, express the power of the test when (a) $σ^2 = 4$, (b) $σ^2 = 3$ and (c) $σ^2 = 2$. (iii) Explain briefly what would change if we did not know the true mean in the population, but we had recorded the sample mean $x = 3$. I got the answer for part i,the observed t-statistic is $7.5$ so do not reject the null hypothesis. But I have problem with part ii. I try to use the power method but it seems not working as in the statistic T there is only one variance $σ^2$, can anyone helps me out?",,"['statistics', 'hypothesis-testing', 'variance']"
25,The Statistic Distribution of Image Gradient?,The Statistic Distribution of Image Gradient?,,"The gradient of an image $f$ is defined as: $\nabla f=\begin{bmatrix} \nabla f_{x} \\ \nabla f_{y} \end{bmatrix} = \begin{bmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \end{bmatrix} , $ Its discrete calculation can be as simple as finite difference. For example $\nabla f_{x} = \frac{f_n-f_{n-1}}{x_{n}-x_{n-1}} $  and  $\nabla f_{y} = \frac{f_n-f_{n-1}}{y_{n}-y_{n-1}}. $ I can simply define the total\whole image gradient is the norm of x and y gradient component: $||\nabla f|| = \sqrt{(\nabla f_{x})^2+(\nabla f_{y})^2}.  $ Nothing fancy so far. Now I am just wondering, what is the distribution of the image gradient in equation above? Here is an example: In above image, the histogram of the image gradient really looks exponential to me. This is just an example, but I have seen similar shape of the histogram in many cases. Can I claim the distribution of an image gradient follows exponential? If not, with what condition I can/cannot make this guess? Thanks a lot.","The gradient of an image $f$ is defined as: $\nabla f=\begin{bmatrix} \nabla f_{x} \\ \nabla f_{y} \end{bmatrix} = \begin{bmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \end{bmatrix} , $ Its discrete calculation can be as simple as finite difference. For example $\nabla f_{x} = \frac{f_n-f_{n-1}}{x_{n}-x_{n-1}} $  and  $\nabla f_{y} = \frac{f_n-f_{n-1}}{y_{n}-y_{n-1}}. $ I can simply define the total\whole image gradient is the norm of x and y gradient component: $||\nabla f|| = \sqrt{(\nabla f_{x})^2+(\nabla f_{y})^2}.  $ Nothing fancy so far. Now I am just wondering, what is the distribution of the image gradient in equation above? Here is an example: In above image, the histogram of the image gradient really looks exponential to me. This is just an example, but I have seen similar shape of the histogram in many cases. Can I claim the distribution of an image gradient follows exponential? If not, with what condition I can/cannot make this guess? Thanks a lot.",,"['statistics', 'probability-distributions', 'image-processing', 'exponential-distribution']"
26,Which statistic test should I use?,Which statistic test should I use?,,"For a task on data cleaning and analysis using SPSS I have two research questions for which I should pick an appropriate statistical test. It's been over a few years though since my last statistics course and I feel like some sources are contradicting each other. The first question is whether there's a relationship between two paired ordinal variables, more specifically, these variables are the Apgar-scores at 1 minute and 5 minutes, used in obstetrics. My first intention was to use Wilcoxon signed-ranks test, due to the description of the variables. But my statistics notes of back in the days mention a few times that when searching a relationship it is preferred to use correlation, rather than Wilcoxon, which would be more preferred when searching a difference.  If it is correct to search correlation, I assume this would be the Spearman's rank correlation coefficient, due to the ordinal character of the variables. As for the second question, is to find whether there's a difference in the need of extra care (nominal) when the duration of pregnancy is accounted for. The duration of pregnancy is divided into two categories, either less than 37 weeks or between 37 and 42 weeks. Due to the rather obvious greater than/lesser than character of the latter, I interpreted this as an ordinal variable. Because I'm working with two categorical variables here, I was rooting for a Chi-squared test, but, as I heard from my colleagues, a binary logistic regression would also be possible, I never learnt anything about the latter in the past, so I don't know if this would be a better fit. Thanks in advance.","For a task on data cleaning and analysis using SPSS I have two research questions for which I should pick an appropriate statistical test. It's been over a few years though since my last statistics course and I feel like some sources are contradicting each other. The first question is whether there's a relationship between two paired ordinal variables, more specifically, these variables are the Apgar-scores at 1 minute and 5 minutes, used in obstetrics. My first intention was to use Wilcoxon signed-ranks test, due to the description of the variables. But my statistics notes of back in the days mention a few times that when searching a relationship it is preferred to use correlation, rather than Wilcoxon, which would be more preferred when searching a difference.  If it is correct to search correlation, I assume this would be the Spearman's rank correlation coefficient, due to the ordinal character of the variables. As for the second question, is to find whether there's a difference in the need of extra care (nominal) when the duration of pregnancy is accounted for. The duration of pregnancy is divided into two categories, either less than 37 weeks or between 37 and 42 weeks. Due to the rather obvious greater than/lesser than character of the latter, I interpreted this as an ordinal variable. Because I'm working with two categorical variables here, I was rooting for a Chi-squared test, but, as I heard from my colleagues, a binary logistic regression would also be possible, I never learnt anything about the latter in the past, so I don't know if this would be a better fit. Thanks in advance.",,"['statistics', 'data-analysis', 'hypothesis-testing']"
27,how to apply 'margin of error' in opinion poll to small answer buckets?,how to apply 'margin of error' in opinion poll to small answer buckets?,,"Australia has preferential, aka alternate voting, at Federal elections. Opinion polls about voting intention ask two questions: which party do you intend to vote for, and if that is a 'minor' party, which of the two major parties do you intend to preference? Therefore, results of these polls indicate a ""two-party preferred vote"", and also results for the individual parties. If a properly-conducted, random-sample opinion poll of a population of 20m has a quoted 'margin of error' of 3%, and if respondents are asked to choose from one of ten political parties but also their preference for one of two major parties, the margin of error is usually applied to the two party vote. How do we apply the margin of error to a result that minor party A will attract 10% of the vote? Or 1% of the vote?","Australia has preferential, aka alternate voting, at Federal elections. Opinion polls about voting intention ask two questions: which party do you intend to vote for, and if that is a 'minor' party, which of the two major parties do you intend to preference? Therefore, results of these polls indicate a ""two-party preferred vote"", and also results for the individual parties. If a properly-conducted, random-sample opinion poll of a population of 20m has a quoted 'margin of error' of 3%, and if respondents are asked to choose from one of ten political parties but also their preference for one of two major parties, the margin of error is usually applied to the two party vote. How do we apply the margin of error to a result that minor party A will attract 10% of the vote? Or 1% of the vote?",,"['statistics', 'sampling']"
28,Tolerance Interval,Tolerance Interval,,Show that a tolerance interval is necessarily distribution-free. Do you know where can I find the proof or any helpful suggestion?,Show that a tolerance interval is necessarily distribution-free. Do you know where can I find the proof or any helpful suggestion?,,"['probability', 'statistics', 'order-statistics']"
29,Subsample of i.i.d. sequence $(X_i)_{i=1}^N$,Subsample of i.i.d. sequence,(X_i)_{i=1}^N,Let $(X_i)_{i=1}^N$ an i.i.d. sequence (in a finite set $A$) and let $(X_{(i)})_{i=1}^n$ be a subsample  of size $n \leq N$  (draw $n$ elements uniformly without replacement). I want to characterize in some sense the distribution of $(X_{(i)})_{i=1}^n$ in order to choose a $n$ such that the empirical distribution of $(X_{(i)})_{i=1}^n$  is close to the empirical distribution of $(X_i)_{i=1}^N$. If anyone has an idea how to do this it will be of much help Thanks!,Let $(X_i)_{i=1}^N$ an i.i.d. sequence (in a finite set $A$) and let $(X_{(i)})_{i=1}^n$ be a subsample  of size $n \leq N$  (draw $n$ elements uniformly without replacement). I want to characterize in some sense the distribution of $(X_{(i)})_{i=1}^n$ in order to choose a $n$ such that the empirical distribution of $(X_{(i)})_{i=1}^n$  is close to the empirical distribution of $(X_i)_{i=1}^N$. If anyone has an idea how to do this it will be of much help Thanks!,,"['probability', 'statistics', 'probability-distributions']"
30,Caculate the probability of sum of normal distribution,Caculate the probability of sum of normal distribution,,"$x_i \sim  \mathcal N(\mu,\sigma)$  $(i = 1,2,...n)$ are i.i.d normal distributed, $a$ is a constant, $a>0$, how to calculate the probability: $$P(x_1 < a, x_1 + x_2 < a, x_1 + x_2 + x_3 < a,..., x_1 + ... + x_{n-1} < a, x_1 + x_2 + ... + x_n >= a)$$ maybe we can translate to: $z_i \sim  \mathcal N(0,1)$  $(i = 1,2,...n)$ are i.i.d normal distributed, $a$ is a constant, we denote $$a_n = \frac{a - n  \mu}{\sigma\sqrt{n}}$$ the probability may transformed into: $P(z_1 < a_1, z_1 + z_2 < a_2, z_1 + z_2 + z_3 < a_3,..., z_1 + ... + z_{n-1} < a_{n-1}, z_1 + z_2 + ... + z_n >= a_n)$ now we denote: $P_1 = P(z_1 < a_1, z_1 + z_2 < a_2, z_1 + z_2 + z_3 < a_3,..., z_1 + ... + z_{n-1} < a_{n-1}, z_1 + z_2 + ... + z_n >= a_n)$ and $P_n = P(z_1 < a_1, z_1 + z_2 < a_2, z_1 + z_2 + z_3 < a_3,..., z_1 + ... + z_{n-1} < a_{n-1}, z_1 + z_2 + ... + z_n < a_n)$ that means $P_{n-1} = P_1 + P_n$ so the problem can be solved by get $P_n$, but how to calculate it?","$x_i \sim  \mathcal N(\mu,\sigma)$  $(i = 1,2,...n)$ are i.i.d normal distributed, $a$ is a constant, $a>0$, how to calculate the probability: $$P(x_1 < a, x_1 + x_2 < a, x_1 + x_2 + x_3 < a,..., x_1 + ... + x_{n-1} < a, x_1 + x_2 + ... + x_n >= a)$$ maybe we can translate to: $z_i \sim  \mathcal N(0,1)$  $(i = 1,2,...n)$ are i.i.d normal distributed, $a$ is a constant, we denote $$a_n = \frac{a - n  \mu}{\sigma\sqrt{n}}$$ the probability may transformed into: $P(z_1 < a_1, z_1 + z_2 < a_2, z_1 + z_2 + z_3 < a_3,..., z_1 + ... + z_{n-1} < a_{n-1}, z_1 + z_2 + ... + z_n >= a_n)$ now we denote: $P_1 = P(z_1 < a_1, z_1 + z_2 < a_2, z_1 + z_2 + z_3 < a_3,..., z_1 + ... + z_{n-1} < a_{n-1}, z_1 + z_2 + ... + z_n >= a_n)$ and $P_n = P(z_1 < a_1, z_1 + z_2 < a_2, z_1 + z_2 + z_3 < a_3,..., z_1 + ... + z_{n-1} < a_{n-1}, z_1 + z_2 + ... + z_n < a_n)$ that means $P_{n-1} = P_1 + P_n$ so the problem can be solved by get $P_n$, but how to calculate it?",,"['probability', 'statistics']"
31,Approximation of Exponentially and Normally Distributed Probabilities,Approximation of Exponentially and Normally Distributed Probabilities,,PROBLEM A company uses a portable high-intensity flashlight: Batteries and bulbs burn out quickly. The lifetime of batteries has Exponential Distribution with mean $10$ hours. The bulbs have lifetimes that are Normally Distributed with mean $32$ and standard deviation $5$ . Assume batteries and bulbs are randomly sampled. Find the probabilities for the following events: [Where appropriate you may approximate probabilities] A battery lasts over $11$ hours. A sample of $20$ batteries has a sample mean over $11$ hours. A sample of $200$ batteries has a sample mean over $11$ hours. I am not sure how to solve these questions because I've only learned approximating with the normal distribution to the binomial. Any suggestions?,PROBLEM A company uses a portable high-intensity flashlight: Batteries and bulbs burn out quickly. The lifetime of batteries has Exponential Distribution with mean hours. The bulbs have lifetimes that are Normally Distributed with mean and standard deviation . Assume batteries and bulbs are randomly sampled. Find the probabilities for the following events: [Where appropriate you may approximate probabilities] A battery lasts over hours. A sample of batteries has a sample mean over hours. A sample of batteries has a sample mean over hours. I am not sure how to solve these questions because I've only learned approximating with the normal distribution to the binomial. Any suggestions?,10 32 5 11 20 11 200 11,"['statistics', 'probability-distributions', 'normal-distribution', 'exponential-distribution', 'word-problem']"
32,Conditional expectation and Iterated expectation problem,Conditional expectation and Iterated expectation problem,,"I have $X_i|b_i \sim Poisson(\lambda_i)$, and $log(\lambda_i)=x_i^t\beta+\varepsilon_i$ further $\varepsilon\sim N(0,\sigma^2)$ then, how to find $E(X_i)$? I try using the iterated expectation $E(X_i)=E[E(X_i|b_i)]$, so $E(X_i)=E(\lambda_i)$ but how to use $log(\lambda)$ to find the solution? thanks.","I have $X_i|b_i \sim Poisson(\lambda_i)$, and $log(\lambda_i)=x_i^t\beta+\varepsilon_i$ further $\varepsilon\sim N(0,\sigma^2)$ then, how to find $E(X_i)$? I try using the iterated expectation $E(X_i)=E[E(X_i|b_i)]$, so $E(X_i)=E(\lambda_i)$ but how to use $log(\lambda)$ to find the solution? thanks.",,"['probability-theory', 'statistics', 'expectation']"
33,How to extract parameters of normal distribution from squared observations?,How to extract parameters of normal distribution from squared observations?,,"Suppose I know the random variables follow normal distribution, i.e., $X\sim\mathcal(\mu,\sigma^2)$, but I can only observe the square of each sample $X^2_i$. How could I extract the parameters of $\mu$ and $\sigma$. I am thinking of moment matching, but it is unclear to me how to proceed. Anybody can help?","Suppose I know the random variables follow normal distribution, i.e., $X\sim\mathcal(\mu,\sigma^2)$, but I can only observe the square of each sample $X^2_i$. How could I extract the parameters of $\mu$ and $\sigma$. I am thinking of moment matching, but it is unclear to me how to proceed. Anybody can help?",,"['statistics', 'random-variables', 'machine-learning', 'estimation']"
34,estimate gaussian background noise,estimate gaussian background noise,,"Let's say I have 3 Random Variables $X_1, X_2, X_b$ where ""$b$"" stands for ""background"". Each one of them is Gaussian with $N(\mu_i, \sigma^2_i)$ for $ i\in\{1,2,b\}$. I will assume $\mu_b=0$. Now I make $N$ experiments which measure the variables $X_1+X_b, X_2+X_b$ (where $X_b$ is measured at the same time for both of them) and I want to estimate all the $\mu_i$'s and $\sigma_i$'s. I know how the estimate the means by taking the average of the results (because $\mu_b = 0$). Also I can easily estimate $\sigma_j^2 + \sigma_b^2$ because if the fact that $X_j+X_b = N(\mu_j,\sigma_j^2 + \sigma_b^2)$... but I need another estimator so I can get all the values for $\sigma$'s! I thought about using the off-diagonal elements of the co-variance matrix (which are supposed to be $\sigma_b^2$) but I get huge problem when they are negative. Can someone help me find the missing estimator?","Let's say I have 3 Random Variables $X_1, X_2, X_b$ where ""$b$"" stands for ""background"". Each one of them is Gaussian with $N(\mu_i, \sigma^2_i)$ for $ i\in\{1,2,b\}$. I will assume $\mu_b=0$. Now I make $N$ experiments which measure the variables $X_1+X_b, X_2+X_b$ (where $X_b$ is measured at the same time for both of them) and I want to estimate all the $\mu_i$'s and $\sigma_i$'s. I know how the estimate the means by taking the average of the results (because $\mu_b = 0$). Also I can easily estimate $\sigma_j^2 + \sigma_b^2$ because if the fact that $X_j+X_b = N(\mu_j,\sigma_j^2 + \sigma_b^2)$... but I need another estimator so I can get all the values for $\sigma$'s! I thought about using the off-diagonal elements of the co-variance matrix (which are supposed to be $\sigma_b^2$) but I get huge problem when they are negative. Can someone help me find the missing estimator?",,"['statistics', 'estimation', 'parameter-estimation']"
35,"I want to change my corporate developer thinking mindset and want to learn Linear Algebra, Statistics and Calculus for improvement [closed]","I want to change my corporate developer thinking mindset and want to learn Linear Algebra, Statistics and Calculus for improvement [closed]",,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 5 years ago . Improve this question Hello Math Stackexchangers, I am a typical corporate developer who is bored by doing repetitive work, I need change and I am ready for it. I want to learn Linear Algebra, Statistics and Calculus, help me build my learning path or recommend me some good beginner level book. I can spend up to 8-10 hours per day for learning. Thank you","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 5 years ago . Improve this question Hello Math Stackexchangers, I am a typical corporate developer who is bored by doing repetitive work, I need change and I am ready for it. I want to learn Linear Algebra, Statistics and Calculus, help me build my learning path or recommend me some good beginner level book. I can spend up to 8-10 hours per day for learning. Thank you",,"['calculus', 'linear-algebra', 'statistics', 'career-development']"
36,Has the mean number decreased?,Has the mean number decreased?,,"This question is from a book. In 1970 the average number of children per family in a certain town was 3.8. In 1980 a random sample of 40 families had a total of 144 children. At the 5pc level of significance is there evidence to conclude that the mean number of children per family had decreased? I want to use this formula: $z=\frac{x-\mu}{\frac{\sigma}{\sqrt{n}}}=\frac{3.6-3.8}{\frac{\sigma}{\sqrt{40}}}$ The book says ""Yes"" and gives a z-value answer of -0.2108. I could not figure out the standard deviation from the problem. Is it there?","This question is from a book. In 1970 the average number of children per family in a certain town was 3.8. In 1980 a random sample of 40 families had a total of 144 children. At the 5pc level of significance is there evidence to conclude that the mean number of children per family had decreased? I want to use this formula: $z=\frac{x-\mu}{\frac{\sigma}{\sqrt{n}}}=\frac{3.6-3.8}{\frac{\sigma}{\sqrt{40}}}$ The book says ""Yes"" and gives a z-value answer of -0.2108. I could not figure out the standard deviation from the problem. Is it there?",,"['statistics', 'sampling', 'hypothesis-testing']"
37,Benford's Law Application,Benford's Law Application,,"Benford's law is a simple empirical tested first digit distribution in data series. I do my Macroeconomics Thesis and I am going to propose to evaluate various filtering processes. I have found an IMF article where they use the first digit law to evaluate data quality and it applies in EU12 and other developed countries. My hypothesis is the following: if benford's law applies on raw data, then it should apply if the filter just denoises and keep intact the information. If it does not apply something is going on and needs to be examined. However, I just realised that filtering (e.g wavelets) will detrend my signals, therefore it doesn't make sense to me to use benford's law after the process, because it examines the first digits' distribution with trend. Do you see any way to apply this and more importantly, is my hypothesis valid in first place?","Benford's law is a simple empirical tested first digit distribution in data series. I do my Macroeconomics Thesis and I am going to propose to evaluate various filtering processes. I have found an IMF article where they use the first digit law to evaluate data quality and it applies in EU12 and other developed countries. My hypothesis is the following: if benford's law applies on raw data, then it should apply if the filter just denoises and keep intact the information. If it does not apply something is going on and needs to be examined. However, I just realised that filtering (e.g wavelets) will detrend my signals, therefore it doesn't make sense to me to use benford's law after the process, because it examines the first digits' distribution with trend. Do you see any way to apply this and more importantly, is my hypothesis valid in first place?",,['statistics']
38,Bias of ratio mean estimator,Bias of ratio mean estimator,,"I'm having trouble understanding this proof from a textbook for the bias of the mean estimator using ratio estimation. How does $$\frac 1 {\bar{x_u}}[BV(\bar{x})-\operatorname{Cov}(\bar{x},\bar{y}) = \left(1 - \frac n N \right) \frac 1 {n\bar{x}_u}(BS_x^2-RS_xS_y) \text{ ?}$$ where: $S_x$ and $S_y$ are the population standard deviations, $B = \frac{\bar{y}_u}{\bar{x}_u}$ and $R$ is the population correlation coefficient","I'm having trouble understanding this proof from a textbook for the bias of the mean estimator using ratio estimation. How does $$\frac 1 {\bar{x_u}}[BV(\bar{x})-\operatorname{Cov}(\bar{x},\bar{y}) = \left(1 - \frac n N \right) \frac 1 {n\bar{x}_u}(BS_x^2-RS_xS_y) \text{ ?}$$ where: $S_x$ and $S_y$ are the population standard deviations, $B = \frac{\bar{y}_u}{\bar{x}_u}$ and $R$ is the population correlation coefficient",,"['statistics', 'sampling', 'sampling-theory']"
39,Continuous Random Variables,Continuous Random Variables,,"Given the function: $$ f(x) = \begin{cases} 0 & x<0, \\ a(b-x) & 0\le x\le b, \\ 0 & b<x, \end{cases} $$ where $a, b ∈ (0, ∞)$. We also know that $\operatorname{E}(X) = 1$. I need to find out $a,b$ and the distribution function. I made an equation system using $\operatorname{E}(X)=1$ and $F(X)=1$ from which I got $a = 2/9$ and $b = 3$, but I'm not really sure if my method is correct and I'm a bit lost on how to calculate the distribution function. Any help is greatly appreciated! :) $$\operatorname{E}(X)=\int_0^b y f(y) \,dy,$$ This gave me $ab^3=6$. $$F(X)=\int_0^b f(y) \, dy$$ This gave me $ab^2=2$.","Given the function: $$ f(x) = \begin{cases} 0 & x<0, \\ a(b-x) & 0\le x\le b, \\ 0 & b<x, \end{cases} $$ where $a, b ∈ (0, ∞)$. We also know that $\operatorname{E}(X) = 1$. I need to find out $a,b$ and the distribution function. I made an equation system using $\operatorname{E}(X)=1$ and $F(X)=1$ from which I got $a = 2/9$ and $b = 3$, but I'm not really sure if my method is correct and I'm a bit lost on how to calculate the distribution function. Any help is greatly appreciated! :) $$\operatorname{E}(X)=\int_0^b y f(y) \,dy,$$ This gave me $ab^3=6$. $$F(X)=\int_0^b f(y) \, dy$$ This gave me $ab^2=2$.",,"['probability', 'statistics']"
40,QQ plot and visual analysis based on sample distribution,QQ plot and visual analysis based on sample distribution,,"When would you use a Box Plot, a Histogram, or a QQPlot to graphically summarize a SAMPLE of numbers? Interpretation: a sample of numbers: like a dataset that only contains numbers but not discrete values such as categories. OR: It means whether the change of the number of samples in the dataset would have some effect on the box plot, QQplot and histogram.","When would you use a Box Plot, a Histogram, or a QQPlot to graphically summarize a SAMPLE of numbers? Interpretation: a sample of numbers: like a dataset that only contains numbers but not discrete values such as categories. OR: It means whether the change of the number of samples in the dataset would have some effect on the box plot, QQplot and histogram.",,"['probability', 'statistics', 'normal-distribution']"
41,How I find sufficient statistic for Student-t distribution? [closed],How I find sufficient statistic for Student-t distribution? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question how can I find sufficient statistics for student-t distribution (centred in 0)","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question how can I find sufficient statistics for student-t distribution (centred in 0)",,"['statistics', 'probability-distributions']"
42,Is $y(t)=\sin(2\pi t+\varphi)$ SSS?,Is  SSS?,y(t)=\sin(2\pi t+\varphi),"Let's say I define random variable $\varphi$ is uniformly distributed between $[0,2\pi).$ I then define Random process $y(t)=\sin(2\pi t+\varphi)$ Is this process SSS? For now, let's assume that the definition of SSS is that the PDF is not conditional on $t$ What about $$z(t)=\sin^2\left(2\pi t+\varphi\right) \text{ ?}$$","Let's say I define random variable $\varphi$ is uniformly distributed between $[0,2\pi).$ I then define Random process $y(t)=\sin(2\pi t+\varphi)$ Is this process SSS? For now, let's assume that the definition of SSS is that the PDF is not conditional on $t$ What about $$z(t)=\sin^2\left(2\pi t+\varphi\right) \text{ ?}$$",,"['probability', 'statistics', 'stationary-processes']"
43,Confusion Concerning Quadratic Forms (Statistics),Confusion Concerning Quadratic Forms (Statistics),,"There are two ways to represent the same random variable: $$\varepsilon_{1,t} \sim N(\vec{0},\Sigma\Sigma'), \text{ or } \Sigma\varepsilon_{2,t} \text{ where } \varepsilon_{2,t} \sim N(\vec{0},\mathbb{I}_n).$$ Consider the scalar term $\varepsilon_{1,t}'\delta'\delta\varepsilon_{1,t}$, and  its equivalent $\varepsilon_{2,t}'\Sigma'\delta'\delta\Sigma\varepsilon_{1,t}$ where $\delta$ is a $(1 \times n)$ non-stochastic vector, and $\Sigma$ is $(n\times n)$ and also non-stochastic. Also $\Sigma\Sigma'$ is PSD. According to the properties of the quadratic form , $\mathbb{E}[\varepsilon_t'\Lambda\varepsilon_t]= \operatorname{tr}(\Lambda \Sigma^*) + \mu'\Lambda\mu$] where $\varepsilon_t \sim N(\mu,\Sigma^*)$ (doesn't need to be normal). Assuming $\varepsilon_{1,t}=\Sigma\varepsilon_{2,t}$ does hold true, I get $\operatorname{tr}[\delta'\delta\Sigma\Sigma']=\operatorname{tr}[\Sigma'\delta'\delta\Sigma]$, since $\mu=\vec{0} \Rightarrow \mu'\Lambda\mu = 0$ Plugging arbitrary values into Matlab tells me this does not hold true. Where does my logic fail?","There are two ways to represent the same random variable: $$\varepsilon_{1,t} \sim N(\vec{0},\Sigma\Sigma'), \text{ or } \Sigma\varepsilon_{2,t} \text{ where } \varepsilon_{2,t} \sim N(\vec{0},\mathbb{I}_n).$$ Consider the scalar term $\varepsilon_{1,t}'\delta'\delta\varepsilon_{1,t}$, and  its equivalent $\varepsilon_{2,t}'\Sigma'\delta'\delta\Sigma\varepsilon_{1,t}$ where $\delta$ is a $(1 \times n)$ non-stochastic vector, and $\Sigma$ is $(n\times n)$ and also non-stochastic. Also $\Sigma\Sigma'$ is PSD. According to the properties of the quadratic form , $\mathbb{E}[\varepsilon_t'\Lambda\varepsilon_t]= \operatorname{tr}(\Lambda \Sigma^*) + \mu'\Lambda\mu$] where $\varepsilon_t \sim N(\mu,\Sigma^*)$ (doesn't need to be normal). Assuming $\varepsilon_{1,t}=\Sigma\varepsilon_{2,t}$ does hold true, I get $\operatorname{tr}[\delta'\delta\Sigma\Sigma']=\operatorname{tr}[\Sigma'\delta'\delta\Sigma]$, since $\mu=\vec{0} \Rightarrow \mu'\Lambda\mu = 0$ Plugging arbitrary values into Matlab tells me this does not hold true. Where does my logic fail?",,"['matrices', 'statistics', 'quadratic-forms']"
44,A covariance matrix of a normal distribution with strictly positive entries is positive definite,A covariance matrix of a normal distribution with strictly positive entries is positive definite,,"This is an intermediate step in a probability homework problem. I have all of it done except for this one step of justification which (I hope!) is true. Let $\Sigma$ be the covariance matrix of an $n$-dimensional Gaussian. Suppose $\forall i,j,\Sigma_{i,j}>0$. By properties of covariance, $\Sigma$ is symmetric. Show $\Sigma$ is positive definite. I'm unfortunately getting stuck on the algebra, which ironically probably should be the easiest part of the problem. It's easy enough to show that $\Sigma$ is positive semidefinite (I used the same method here ), but I'm having a hard time ruling out the possibility that there is some nonzero $x\in\mathbb R^n$ such that $x^\mathrm T\Sigma x=0$. Am I'm missing something obvious?","This is an intermediate step in a probability homework problem. I have all of it done except for this one step of justification which (I hope!) is true. Let $\Sigma$ be the covariance matrix of an $n$-dimensional Gaussian. Suppose $\forall i,j,\Sigma_{i,j}>0$. By properties of covariance, $\Sigma$ is symmetric. Show $\Sigma$ is positive definite. I'm unfortunately getting stuck on the algebra, which ironically probably should be the easiest part of the problem. It's easy enough to show that $\Sigma$ is positive semidefinite (I used the same method here ), but I'm having a hard time ruling out the possibility that there is some nonzero $x\in\mathbb R^n$ such that $x^\mathrm T\Sigma x=0$. Am I'm missing something obvious?",,"['linear-algebra', 'probability', 'matrices', 'statistics', 'normal-distribution']"
45,Distribution of blood types and alleles,Distribution of blood types and alleles,,"The blood type distribution in the US is as follows (according to this link ): O 45% A 40% B 11% AB 4% However the blood type is a result of the 2 alleles (in lower case) that a person gets from her parents. aa -> A oa -> A ao -> A oo -> O ob -> B bo -> B bb -> B ba -> AB ab -> AB How can I extract the distribution of the alleles {a,b,o} from the the known distribution of the blood types {O,A,B,AB} ?","The blood type distribution in the US is as follows (according to this link ): O 45% A 40% B 11% AB 4% However the blood type is a result of the 2 alleles (in lower case) that a person gets from her parents. aa -> A oa -> A ao -> A oo -> O ob -> B bo -> B bb -> B ba -> AB ab -> AB How can I extract the distribution of the alleles {a,b,o} from the the known distribution of the blood types {O,A,B,AB} ?",,"['probability', 'statistics', 'probability-distributions', 'multinomial-coefficients']"
46,Number of unique items in a big set from small sample,Number of unique items in a big set from small sample,,"We have a box with $m=1\,000\,000$ cards. Each card contains one word. The words are repeated so there is a relatively small number of $n$ unique words. $n$ is unknown. If we get a sample of $k=5000$ cards, we find that there is $42$ unique words in our sample. With this information, we know $P(n\geq42) = 1$. How can we know $P(n\geq43)$, $P(n\geq44)$..., and so on? Is this problem common, and does it have a ""common name""? PS: we have the information on the frequency of each of our $42$ words for the $5000$ card sample, they can be used for the solution if it is relevant. Lets call this frequencies $f_1, f_2, \dots,f_{42}$.","We have a box with $m=1\,000\,000$ cards. Each card contains one word. The words are repeated so there is a relatively small number of $n$ unique words. $n$ is unknown. If we get a sample of $k=5000$ cards, we find that there is $42$ unique words in our sample. With this information, we know $P(n\geq42) = 1$. How can we know $P(n\geq43)$, $P(n\geq44)$..., and so on? Is this problem common, and does it have a ""common name""? PS: we have the information on the frequency of each of our $42$ words for the $5000$ card sample, they can be used for the solution if it is relevant. Lets call this frequencies $f_1, f_2, \dots,f_{42}$.",,"['probability', 'statistics', 'combinatory-logic']"
47,Definition of intensity of a counting process,Definition of intensity of a counting process,,"Fix a filtered probability space $(\Omega,\mathcal{F},P,(\mathcal{F}_t)_{t\geq 0})$. Let $N=(N_t)_{t\geq 0}$ be a counting process. Then we have the following definitions for the intensity of a counting process: Brémaud gives the following definition in his book ""Point processes and queues"". Definition 1: A progressive process $\lambda=(\lambda_t)_{t\geq 0}$ is called the intensity of a counting process if   $$\int_0^t\lambda_s ds<\infty$$   for all $t\geq 0$ and   $$E\left[\int_0^\infty C_sdN_s\right]=E\left[\int_0^\infty C_s \lambda_s ds\right]$$ for all non-negative $(\mathcal{F}_t)_{t\geq 0}$ predictable processes $C=(C_t)_{t\geq 0}$. Then on wikipedia, we have the following definition: Definition 2: Let $N=(N_t)_{t\geq 0}$ be a counting process. Then $N$ is a submartingale and $$M=N-A$$   is a martingale where $A$ is a predictable increasing process. A is called the cummulative intensity of $N$ and if it is of the form $$A_t=\int_0^t\lambda_s ds$$, then $\lambda=(\lambda_t)_{t\geq 0}$ is the intensity of $N$. On wikipedia, we have also another definition: Definition 3: The intensity process $\lambda=(\lambda_t)_{t\geq 0}$ is defined by $$\lambda_t=\lim_{h\downarrow 0}\frac{1}{h}E[N_{t+h}-N_t\mid \mathcal{F}_t]$$ Now my question is if these definitions are all equivalent. For the last definition, we have $$\frac{1}{h}E[N_{t+h}-N_t\mid \mathcal{F}_t]=\frac{1}{h}E[M_{t+h}+A_{t+h}-M_t-A_t\mid \mathcal{F}_t]=\frac{1}{h}E[A_{t+h}-A_t\mid\mathcal{F}_t].$$  Is it true to say that $$\lim_{h\downarrow 0}\frac{1}{h}E[A_{t+h}-A_t\mid\mathcal{F}_t]=E[\lim_{h\downarrow 0}\frac{1}{h}(A_{t+h}-A_t)\mid\mathcal{F}_t].$$ When we have $A_t=\int_0^t\lambda_r ds$, then $$E[\lim_{h\downarrow 0}\frac{1}{h}(A_{t+h}-A_t)\mid\mathcal{F}_t]=E[\lambda_t\mid \mathcal{F}_t]=\lambda_t,$$ which would yield $(2)\implies (3)$. $(3)\implies(1)$ follows probably by  $$E[\int_0^\infty\lambda_sC_s ds]=E[\int_0^\infty\lim_{h\downarrow 0}E[N_{s+h}-N_s\mid\mathcal{F}_s]C_s ds]=E[\int_0^\infty\lim_{h\downarrow 0}(N_{s+h}-N_s) C_sds]=E[\int_0^\infty N_s'C_sds]=E[\int_0^\infty C_s dN_s]$$","Fix a filtered probability space $(\Omega,\mathcal{F},P,(\mathcal{F}_t)_{t\geq 0})$. Let $N=(N_t)_{t\geq 0}$ be a counting process. Then we have the following definitions for the intensity of a counting process: Brémaud gives the following definition in his book ""Point processes and queues"". Definition 1: A progressive process $\lambda=(\lambda_t)_{t\geq 0}$ is called the intensity of a counting process if   $$\int_0^t\lambda_s ds<\infty$$   for all $t\geq 0$ and   $$E\left[\int_0^\infty C_sdN_s\right]=E\left[\int_0^\infty C_s \lambda_s ds\right]$$ for all non-negative $(\mathcal{F}_t)_{t\geq 0}$ predictable processes $C=(C_t)_{t\geq 0}$. Then on wikipedia, we have the following definition: Definition 2: Let $N=(N_t)_{t\geq 0}$ be a counting process. Then $N$ is a submartingale and $$M=N-A$$   is a martingale where $A$ is a predictable increasing process. A is called the cummulative intensity of $N$ and if it is of the form $$A_t=\int_0^t\lambda_s ds$$, then $\lambda=(\lambda_t)_{t\geq 0}$ is the intensity of $N$. On wikipedia, we have also another definition: Definition 3: The intensity process $\lambda=(\lambda_t)_{t\geq 0}$ is defined by $$\lambda_t=\lim_{h\downarrow 0}\frac{1}{h}E[N_{t+h}-N_t\mid \mathcal{F}_t]$$ Now my question is if these definitions are all equivalent. For the last definition, we have $$\frac{1}{h}E[N_{t+h}-N_t\mid \mathcal{F}_t]=\frac{1}{h}E[M_{t+h}+A_{t+h}-M_t-A_t\mid \mathcal{F}_t]=\frac{1}{h}E[A_{t+h}-A_t\mid\mathcal{F}_t].$$  Is it true to say that $$\lim_{h\downarrow 0}\frac{1}{h}E[A_{t+h}-A_t\mid\mathcal{F}_t]=E[\lim_{h\downarrow 0}\frac{1}{h}(A_{t+h}-A_t)\mid\mathcal{F}_t].$$ When we have $A_t=\int_0^t\lambda_r ds$, then $$E[\lim_{h\downarrow 0}\frac{1}{h}(A_{t+h}-A_t)\mid\mathcal{F}_t]=E[\lambda_t\mid \mathcal{F}_t]=\lambda_t,$$ which would yield $(2)\implies (3)$. $(3)\implies(1)$ follows probably by  $$E[\int_0^\infty\lambda_sC_s ds]=E[\int_0^\infty\lim_{h\downarrow 0}E[N_{s+h}-N_s\mid\mathcal{F}_s]C_s ds]=E[\int_0^\infty\lim_{h\downarrow 0}(N_{s+h}-N_s) C_sds]=E[\int_0^\infty N_s'C_sds]=E[\int_0^\infty C_s dN_s]$$",,"['statistics', 'stochastic-processes', 'stochastic-calculus']"
48,How to derive pmf from a ramdom variable X to a random variable Y = (-1)^X?,How to derive pmf from a ramdom variable X to a random variable Y = (-1)^X?,,"Assume the random variable X has distribution X ∼ Bin(9, 0.5) and let  Y = (−1)^X.  Derive the probability mass function of Y . I know how to derive the mean and variance, but how to derive the pmf? Can anyone helps me out?","Assume the random variable X has distribution X ∼ Bin(9, 0.5) and let  Y = (−1)^X.  Derive the probability mass function of Y . I know how to derive the mean and variance, but how to derive the pmf? Can anyone helps me out?",,"['calculus', 'statistics']"
49,Determining sample size,Determining sample size,,"I have a large set of data and a copy of that data. The whole data set is $n$ bytes. I want to be 99.999% certain that the sets are identical. Assuming that copying errors occur randomly, how many bytes do I need to randomly select and compare against the reference to be 99.999% certain the two sets are completely identical? This problem, it appears to me, relates to that one here: Determining Sample Size for a Desired Margin of Error -- however I'm confused by the margin-of-error and confidence interval both occuring in the formula, but the sample size not being dependent at all on the input size (in that example, total number of students).","I have a large set of data and a copy of that data. The whole data set is $n$ bytes. I want to be 99.999% certain that the sets are identical. Assuming that copying errors occur randomly, how many bytes do I need to randomly select and compare against the reference to be 99.999% certain the two sets are completely identical? This problem, it appears to me, relates to that one here: Determining Sample Size for a Desired Margin of Error -- however I'm confused by the margin-of-error and confidence interval both occuring in the formula, but the sample size not being dependent at all on the input size (in that example, total number of students).",,['statistics']
50,Random numbers generator and collision probability,Random numbers generator and collision probability,,"[I don’t know how to properly write the notation, so any help is appreciated.] There is a device that generates random integer numbers. The number of possible values that it can generate is x . For ex., if a device can generate any number between 4221 and 5220, inclusive, we say that x =1000. If the device generates a number that it already generated in the past, we have a “collision”. Given x , what is the probability for a collision after generating n numbers?","[I don’t know how to properly write the notation, so any help is appreciated.] There is a device that generates random integer numbers. The number of possible values that it can generate is x . For ex., if a device can generate any number between 4221 and 5220, inclusive, we say that x =1000. If the device generates a number that it already generated in the past, we have a “collision”. Given x , what is the probability for a collision after generating n numbers?",,"['probability', 'statistics', 'random']"
51,Probability of Path on Randomly Edge-Coloured Graph,Probability of Path on Randomly Edge-Coloured Graph,,"I begin with a graph $G=(V,E)$. For each edge $e \in E$, I colour the edge with probability $p_e$. I'm looking for the probability that two vertices $v,w \in V$ are the endpoints of a coloured path. For an example of what I mean, consider the following graph: The path $\left \lbrace (0,5),\, (5,4) \right \rbrace$ has been coloured, and the vertices $\lbrace 0,\, 4 \rbrace$ are endpoints of a coloured path. Note that vertex $5$ is not the endpoint of a coloured path as I define it, since more than one of its incident edges is coloured. I've taken a look in some basic books about statistics/graph theory (Maurer/Ralston, Grimmett), but not found an easy answer to the problem, or a statement that it's truly difficult. I'm not too familiar with the literature, so reading suggestions would be appreciated.","I begin with a graph $G=(V,E)$. For each edge $e \in E$, I colour the edge with probability $p_e$. I'm looking for the probability that two vertices $v,w \in V$ are the endpoints of a coloured path. For an example of what I mean, consider the following graph: The path $\left \lbrace (0,5),\, (5,4) \right \rbrace$ has been coloured, and the vertices $\lbrace 0,\, 4 \rbrace$ are endpoints of a coloured path. Note that vertex $5$ is not the endpoint of a coloured path as I define it, since more than one of its incident edges is coloured. I've taken a look in some basic books about statistics/graph theory (Maurer/Ralston, Grimmett), but not found an easy answer to the problem, or a statement that it's truly difficult. I'm not too familiar with the literature, so reading suggestions would be appreciated.",,"['probability', 'statistics', 'graph-theory', 'percolation']"
52,Origin of diagrammatics for cumulant-moment relations?,Origin of diagrammatics for cumulant-moment relations?,,"The exponential-log transformation of exponential generating functions (see OEIS A036040 and A127671 ) relate the classical cumulants to their associated moments. Who were some of the first to introduce diagrammatic illustrations of the relationship between the classical cumulants and moments? (A excellent discussion of the connections can be found in "" Three lectures on free probability "" by Novak and LaCroix.) Agata Fronczak in the "" The microscopic meaning of the grand potential ..."" mentions Mayer, Riddel, and Uhlenbeck in relation to the cluster expansion theorem.","The exponential-log transformation of exponential generating functions (see OEIS A036040 and A127671 ) relate the classical cumulants to their associated moments. Who were some of the first to introduce diagrammatic illustrations of the relationship between the classical cumulants and moments? (A excellent discussion of the connections can be found in "" Three lectures on free probability "" by Novak and LaCroix.) Agata Fronczak in the "" The microscopic meaning of the grand potential ..."" mentions Mayer, Riddel, and Uhlenbeck in relation to the cluster expansion theorem.",,"['sequences-and-series', 'combinatorics', 'statistics', 'graph-theory', 'math-history']"
53,Graph-Third Quartle,Graph-Third Quartle,,"Find the third quartile class and frequency of the class from the given graph. How many students are there above the class? My attempt : Here, $N=120$. Third quartile lies in $\frac {3N}{4}$item. $$=90 item$$ So, $Q_3 class = 20-30$. but I couldn't do that second part of the question. Please help. Thanks","Find the third quartile class and frequency of the class from the given graph. How many students are there above the class? My attempt : Here, $N=120$. Third quartile lies in $\frac {3N}{4}$item. $$=90 item$$ So, $Q_3 class = 20-30$. but I couldn't do that second part of the question. Please help. Thanks",,"['statistics', 'graphing-functions']"
54,Independence of 3 random vectors,Independence of 3 random vectors,,"I have the following question and could not find an answer in any other thread: Let $A$, $B$ and $C$ be random vectors and let it be given, that: $A$ is independent of $B$ $(A, B)$ is independent of $C$ How can I show, that this implies $A$ independent of $C$ ? It seems intuitive, but I am looking for a formal proof. I know, that from (1.) and (2.) follows, that: $f_{ABC}(a,b,c) = f_{ABC}((a,b),c) = f_{AB}(a,b)\cdot f_{C}(c) = f_{A}(a)\cdot f_{B}(b)\cdot f_{C}(c)$ With $a, b, c$ being observations of $A, B, C$ respectively. Does this help at all? I guess, I want to get an equation like this: $f_{AC}(a,c) = f_A(a) \cdot f_C(c)$ Any help/resource reference is greatly appreciated!","I have the following question and could not find an answer in any other thread: Let $A$, $B$ and $C$ be random vectors and let it be given, that: $A$ is independent of $B$ $(A, B)$ is independent of $C$ How can I show, that this implies $A$ independent of $C$ ? It seems intuitive, but I am looking for a formal proof. I know, that from (1.) and (2.) follows, that: $f_{ABC}(a,b,c) = f_{ABC}((a,b),c) = f_{AB}(a,b)\cdot f_{C}(c) = f_{A}(a)\cdot f_{B}(b)\cdot f_{C}(c)$ With $a, b, c$ being observations of $A, B, C$ respectively. Does this help at all? I guess, I want to get an equation like this: $f_{AC}(a,c) = f_A(a) \cdot f_C(c)$ Any help/resource reference is greatly appreciated!",,"['probability', 'probability-theory', 'statistics', 'random-variables', 'independence']"
55,Which ranking system is best for time-based competitions (e.g. Rally car racing),Which ranking system is best for time-based competitions (e.g. Rally car racing),,"Let's suppose I have a database of 1000 competitors and 100 events.  Each event has anything from 150 to 300 entrants. The sport (e.g. rally car racing) is a simple against-the-clock event.  Participants set off at regular intervals, and the race is decided by finishing times. I assume there's already established algorithms for ranking competitors under these circumstances.  Where should I be looking? I've read up about Elo and TrueSkill, but it seems both are much more about 1-on-1 competition, or group-on-group, and not many-on-many.  While they could maybe be adapted I'm not quite sure how to go about it, and I'm also unsure if they're suitable at all in this case. I tried adapting Elo to change a competition of many people into hundreds of 1-on-1 matches, but you end up with the problem of a winner ""running away"" with a huge lead.  Maybe with some tweaking of various factors it could work, but I feel I'm probably barking up the wrong tree? Many thanks","Let's suppose I have a database of 1000 competitors and 100 events.  Each event has anything from 150 to 300 entrants. The sport (e.g. rally car racing) is a simple against-the-clock event.  Participants set off at regular intervals, and the race is decided by finishing times. I assume there's already established algorithms for ranking competitors under these circumstances.  Where should I be looking? I've read up about Elo and TrueSkill, but it seems both are much more about 1-on-1 competition, or group-on-group, and not many-on-many.  While they could maybe be adapted I'm not quite sure how to go about it, and I'm also unsure if they're suitable at all in this case. I tried adapting Elo to change a competition of many people into hundreds of 1-on-1 matches, but you end up with the problem of a winner ""running away"" with a huge lead.  Maybe with some tweaking of various factors it could work, but I feel I'm probably barking up the wrong tree? Many thanks",,"['statistics', 'algorithms']"
56,"How are these 2 results, derived from the Central Limit Theorem, related?","How are these 2 results, derived from the Central Limit Theorem, related?",,"Unfortunately, it's been a while since I last studied mathematics - so I'm currently fuzzy to say the least. I am now skim reading through lecture notes for a module on Markov Processes and have come across the below that I cannot quite understand. In my lecture notes, they have derived (from the CLT) that, for a simple random walk $$ \frac{\frac{X_{n}}{n}-(p-q)}{\sqrt{\frac{4pq}{n}}}   \rightarrow    N(0, 1) $$ as $n \rightarrow \infty$ (where $p$ is the probability of the process increasing and $q$ is the probability of it decreasing). I understand the above just fine. However, the notes then go on to say that this is equivalent to $$ X_{n} \simeq X_{n}' \sim N(n(p-q), 4npq) $$ for large $n$. This is the part I do not understand. Could anyone explain how these expressions are equal, and where the $X_{n}'$ term comes from?","Unfortunately, it's been a while since I last studied mathematics - so I'm currently fuzzy to say the least. I am now skim reading through lecture notes for a module on Markov Processes and have come across the below that I cannot quite understand. In my lecture notes, they have derived (from the CLT) that, for a simple random walk $$ \frac{\frac{X_{n}}{n}-(p-q)}{\sqrt{\frac{4pq}{n}}}   \rightarrow    N(0, 1) $$ as $n \rightarrow \infty$ (where $p$ is the probability of the process increasing and $q$ is the probability of it decreasing). I understand the above just fine. However, the notes then go on to say that this is equivalent to $$ X_{n} \simeq X_{n}' \sim N(n(p-q), 4npq) $$ for large $n$. This is the part I do not understand. Could anyone explain how these expressions are equal, and where the $X_{n}'$ term comes from?",,"['statistics', 'markov-process', 'random-walk']"
57,Solve expectation of a normal random variable times a log-normal random variable,Solve expectation of a normal random variable times a log-normal random variable,,"How would I go about solving: $$\mathbb{E} \left[\varepsilon_t \exp\left\{-\frac{1}{2} \Lambda' \Lambda -  \Lambda'\varepsilon_t\right\}\right]$$ where $\varepsilon_t \sim N(\vec{0},\mathbb{I}_n)$, and $\Lambda$ is an $(n \times 1)$ vector of constants. I know I will need to use Jensen's inequality, but am unsure how to group the premultiplied $\varepsilon_t$. Thanks.","How would I go about solving: $$\mathbb{E} \left[\varepsilon_t \exp\left\{-\frac{1}{2} \Lambda' \Lambda -  \Lambda'\varepsilon_t\right\}\right]$$ where $\varepsilon_t \sim N(\vec{0},\mathbb{I}_n)$, and $\Lambda$ is an $(n \times 1)$ vector of constants. I know I will need to use Jensen's inequality, but am unsure how to group the premultiplied $\varepsilon_t$. Thanks.",,"['probability', 'statistics', 'multivariable-calculus', 'expectation']"
58,What is the prob that the seq obtained is inc (ie. nondecreasing),What is the prob that the seq obtained is inc (ie. nondecreasing),,"Given n distinct numbers, k numbers are picked out, 1 by 1 w/ replacement to get a seq of numbers. What is the prob that this seq is increasing? I know that since non-decreasing seq are allowed, there are in addition n constant seq on top of the strictly increasing probability of C(n,k)/n^k. But I am stuck from here...","Given n distinct numbers, k numbers are picked out, 1 by 1 w/ replacement to get a seq of numbers. What is the prob that this seq is increasing? I know that since non-decreasing seq are allowed, there are in addition n constant seq on top of the strictly increasing probability of C(n,k)/n^k. But I am stuck from here...",,['statistics']
59,Likelihood Ratio test AR(1) model,Likelihood Ratio test AR(1) model,,"Question : Consider the normal random variables $X_1,\dots,X_n$, where    $X_{i} = \theta X_{i-1} + \epsilon_{i-1}$ for $i=1,2,\dots,n$ and $X_0 = 0 $ and $\epsilon_i \stackrel{iid}{\sim} N(0,\sigma^2)$. (a) Find the joint pdf of $\vec X$, (b) show that the LRT for $\theta = 0$ versus $\theta \neq 0$ is based on the test statistic $- \left( \sum_{i=2}^n  X_i X_{i-1} \right)^2 / \sum_{i=1}^{n-1} X_i^2$. This is an old exam problem I'm having lots of trouble with. Each $X_i$ has mean $0$, and $Cov(X_{t+k},X_t) = Cov(\theta X_{t+k-1} + \epsilon_{t+k-1}, X_t)= \cdots = \theta^{k} Cov(X_t,X_t)$. From here I just use: \begin{align*} X_t = \theta^{t-1} \epsilon_1 + \theta^{t-2} \epsilon_2 + \cdots + \epsilon_t \\ Cov(X_t,X_t) = E[X_t^2] = \sigma^2 \sum_{k=0}^{t-1} \theta^{2k} = \sigma^2 \frac{\theta^{2t}-1}{\theta-1} \end{align*} Then, $X_1,\dots,X_n$ are jointly normal with mean  $\mathbf 0$ and covariance matrix $\Sigma_{ij} = \theta^{|i-j|} \sigma^2 \dfrac{ \theta^{2 \min\{i,j\}}-1}{\theta-1}$. The joint pdf is then $\frac{1}{\sqrt{(2\pi)^n | \Sigma|}} e^{- \frac{1}{2} \mathbf x^T \Sigma^{-1} \mathbf x }$. Now for part (b). I need to find $$ \dfrac{L(\Sigma; \theta = 0)}{ \sup \limits_{\theta \neq 0}L(\Sigma)} = \dfrac{\frac{1}{\sqrt{(2\pi)^n}} e^{- \frac{1}{2} \mathbf{x}^T \mathbf{x}}}{\sup \limits_{\theta \neq 0} L(\Sigma)} $$ but I have no idea how to tackle the denominator.  I keep reminding myself that this is from an in-class exam, so the solution must be relatively uncomplicated . . . But no luck . . .","Question : Consider the normal random variables $X_1,\dots,X_n$, where    $X_{i} = \theta X_{i-1} + \epsilon_{i-1}$ for $i=1,2,\dots,n$ and $X_0 = 0 $ and $\epsilon_i \stackrel{iid}{\sim} N(0,\sigma^2)$. (a) Find the joint pdf of $\vec X$, (b) show that the LRT for $\theta = 0$ versus $\theta \neq 0$ is based on the test statistic $- \left( \sum_{i=2}^n  X_i X_{i-1} \right)^2 / \sum_{i=1}^{n-1} X_i^2$. This is an old exam problem I'm having lots of trouble with. Each $X_i$ has mean $0$, and $Cov(X_{t+k},X_t) = Cov(\theta X_{t+k-1} + \epsilon_{t+k-1}, X_t)= \cdots = \theta^{k} Cov(X_t,X_t)$. From here I just use: \begin{align*} X_t = \theta^{t-1} \epsilon_1 + \theta^{t-2} \epsilon_2 + \cdots + \epsilon_t \\ Cov(X_t,X_t) = E[X_t^2] = \sigma^2 \sum_{k=0}^{t-1} \theta^{2k} = \sigma^2 \frac{\theta^{2t}-1}{\theta-1} \end{align*} Then, $X_1,\dots,X_n$ are jointly normal with mean  $\mathbf 0$ and covariance matrix $\Sigma_{ij} = \theta^{|i-j|} \sigma^2 \dfrac{ \theta^{2 \min\{i,j\}}-1}{\theta-1}$. The joint pdf is then $\frac{1}{\sqrt{(2\pi)^n | \Sigma|}} e^{- \frac{1}{2} \mathbf x^T \Sigma^{-1} \mathbf x }$. Now for part (b). I need to find $$ \dfrac{L(\Sigma; \theta = 0)}{ \sup \limits_{\theta \neq 0}L(\Sigma)} = \dfrac{\frac{1}{\sqrt{(2\pi)^n}} e^{- \frac{1}{2} \mathbf{x}^T \mathbf{x}}}{\sup \limits_{\theta \neq 0} L(\Sigma)} $$ but I have no idea how to tackle the denominator.  I keep reminding myself that this is from an in-class exam, so the solution must be relatively uncomplicated . . . But no luck . . .",,"['statistics', 'probability-distributions', 'hypothesis-testing']"
60,Maximizing entropy of multinomial distribution,Maximizing entropy of multinomial distribution,,"Given a multinomial distribution: $Mul(\overline{x}|n, \overline{p}) = {n \choose x_{1...k}!}\prod_{i}^{k}p_i^{x_i} $ And the entropy of a probability distribution: $Entropy(x) = -\sum_i^n p_x(x)log(p_x(x))$ How do we maximize entropy when it comes to multinomial w.r.t $\overline{p}$? It's simple if we have a binomial example, which is p and 1-p, and just solving for p when the derivative of the entropy with respect to p is equal to 0(or if we can't find a maxima, then use lagrange multipliers with the bound shown next. However, in this case we have to satisfy a bound on maximizing entropy: $\sum_i^k p_i = 1$ Any hints would be appreciated!","Given a multinomial distribution: $Mul(\overline{x}|n, \overline{p}) = {n \choose x_{1...k}!}\prod_{i}^{k}p_i^{x_i} $ And the entropy of a probability distribution: $Entropy(x) = -\sum_i^n p_x(x)log(p_x(x))$ How do we maximize entropy when it comes to multinomial w.r.t $\overline{p}$? It's simple if we have a binomial example, which is p and 1-p, and just solving for p when the derivative of the entropy with respect to p is equal to 0(or if we can't find a maxima, then use lagrange multipliers with the bound shown next. However, in this case we have to satisfy a bound on maximizing entropy: $\sum_i^k p_i = 1$ Any hints would be appreciated!",,"['probability', 'statistics', 'entropy']"
61,"What is the pmf $f$ of a random variable $X$ if it satisfies$f(x)=\frac{\alpha+\beta x}{x}f(x-1)$ for $x=1,2...$?",What is the pmf  of a random variable  if it satisfies for ?,"f X f(x)=\frac{\alpha+\beta x}{x}f(x-1) x=1,2...","The actual textbook question: An integer valued random variable $X$ has p.m.f. $f$ satisfying $$f(x)=\frac{\alpha+\beta x}{x}f(x-1)\quad...(*),\quad\beta \ne1\quad\text{and}\quad x=1,2,...$$Find $\mu,\mathrm{Var}(X)$ and $\mathrm{MD}_{\mu}$ where $\mu=\mathrm{E}(X)$. While this problem was solvable by cross-multiplying the recursive equation of $f$ and summing up both sides for all possible $x$ to find the required moments, I wasn't able to find out the exact closed-form expression for $f$. I realise that we don't need the exact expression to find the required quantities, but is it possible to solve for $f$ from the given relation? The source adds that the binomial and Poisson random variables are of this type. Putting the values of $x$ in $(*)$ for a finite number of times from $x=1$ to $x=n$ and multiplying the $n$ equations I get, $$f(n)=\frac{f(0)}{n!}(\alpha+\beta)(a+2\beta)...(a+n\beta)$$ Then I could perhaps write $\displaystyle f(x)=\lim_{n\to\infty}f(n)=\frac{f(0)}{n!}\lim_{n\to\infty}p_n$, (say). Applying the AM-GM inequality on $p_n$, I could again possibly say that $$\lim_{n\to\infty}p_n=\lim_{n\to\infty}\left[\alpha+\frac{\beta (n+1)}{2}\right]^n,\quad\text{which diverges}.$$ I don't think using $\sum_{x=1}^{\infty}f(x)=1$ helps me out here.","The actual textbook question: An integer valued random variable $X$ has p.m.f. $f$ satisfying $$f(x)=\frac{\alpha+\beta x}{x}f(x-1)\quad...(*),\quad\beta \ne1\quad\text{and}\quad x=1,2,...$$Find $\mu,\mathrm{Var}(X)$ and $\mathrm{MD}_{\mu}$ where $\mu=\mathrm{E}(X)$. While this problem was solvable by cross-multiplying the recursive equation of $f$ and summing up both sides for all possible $x$ to find the required moments, I wasn't able to find out the exact closed-form expression for $f$. I realise that we don't need the exact expression to find the required quantities, but is it possible to solve for $f$ from the given relation? The source adds that the binomial and Poisson random variables are of this type. Putting the values of $x$ in $(*)$ for a finite number of times from $x=1$ to $x=n$ and multiplying the $n$ equations I get, $$f(n)=\frac{f(0)}{n!}(\alpha+\beta)(a+2\beta)...(a+n\beta)$$ Then I could perhaps write $\displaystyle f(x)=\lim_{n\to\infty}f(n)=\frac{f(0)}{n!}\lim_{n\to\infty}p_n$, (say). Applying the AM-GM inequality on $p_n$, I could again possibly say that $$\lim_{n\to\infty}p_n=\lim_{n\to\infty}\left[\alpha+\frac{\beta (n+1)}{2}\right]^n,\quad\text{which diverges}.$$ I don't think using $\sum_{x=1}^{\infty}f(x)=1$ helps me out here.",,"['probability', 'statistics', 'probability-distributions', 'random-variables']"
62,asymptotic distribution of LRT (bivariate normal),asymptotic distribution of LRT (bivariate normal),,"Let p(x;$\theta$) be the density of bivariate normal $\textbf{X}$ whose components have unknown mean $\theta=(\theta_1,\theta_2)$, known variances $\sigma_1^2,\sigma_2^2$ and known correlation coefficient $\rho\in(-1,1)$. For testing $H_0:\theta_1=0,\theta_2=0$ vs $H_1:\theta\in\mathbb{R}^2-\{0,0\}$, show that $\lambda$~ $\chi_1^2$ where $-2log\lambda_n\rightarrow\lambda$ in distribution as $n\rightarrow\infty$ and $\lambda_n$ is the log likelihood ratio test statistic. I firstly found the MLE of $\theta_1$ and $\theta_2$ as $\bar{X}$ and $\bar{Y}$. Then simplified $-2log(\lambda_n)$, and found \begin{equation} -2log(\lambda_n)=\frac{n}{1-\rho^2}\left[ \left(\frac{\bar{x}}{\sigma_1}\right)^2-\frac{2\rho}{\sigma_1\sigma_2}\bar{x}\bar{y}+\left(\frac{\bar{y}}{\sigma_2}\right)^2  \right] \end{equation} I got stuck at this point, and can't see how to proceed from here!","Let p(x;$\theta$) be the density of bivariate normal $\textbf{X}$ whose components have unknown mean $\theta=(\theta_1,\theta_2)$, known variances $\sigma_1^2,\sigma_2^2$ and known correlation coefficient $\rho\in(-1,1)$. For testing $H_0:\theta_1=0,\theta_2=0$ vs $H_1:\theta\in\mathbb{R}^2-\{0,0\}$, show that $\lambda$~ $\chi_1^2$ where $-2log\lambda_n\rightarrow\lambda$ in distribution as $n\rightarrow\infty$ and $\lambda_n$ is the log likelihood ratio test statistic. I firstly found the MLE of $\theta_1$ and $\theta_2$ as $\bar{X}$ and $\bar{Y}$. Then simplified $-2log(\lambda_n)$, and found \begin{equation} -2log(\lambda_n)=\frac{n}{1-\rho^2}\left[ \left(\frac{\bar{x}}{\sigma_1}\right)^2-\frac{2\rho}{\sigma_1\sigma_2}\bar{x}\bar{y}+\left(\frac{\bar{y}}{\sigma_2}\right)^2  \right] \end{equation} I got stuck at this point, and can't see how to proceed from here!",,"['statistics', 'asymptotics', 'bivariate-distributions', 'log-likelihood']"
63,Transform normal distribution to chi-square distribution,Transform normal distribution to chi-square distribution,,"Let $X_1$ and $X_2$ be independent random variables with distributions, with  $X_1 \sim N(-1; 2)$ and $X_2 \sim N(2; 3)$ Prove that random variable $Y_2=((2X_1+X_2)^2)/11$ is chi-square distributed. My approach: $2X_1+X_2 =Z,$ with $Z \sim N(-2+2;4*2+3)=N(0;11).$ $Y_2 \sim (N(0,11)^2)/11 =(N(0;1)^2)*(\sqrt{11}/11).$ Where am I making mistake?","Let $X_1$ and $X_2$ be independent random variables with distributions, with  $X_1 \sim N(-1; 2)$ and $X_2 \sim N(2; 3)$ Prove that random variable $Y_2=((2X_1+X_2)^2)/11$ is chi-square distributed. My approach: $2X_1+X_2 =Z,$ with $Z \sim N(-2+2;4*2+3)=N(0;11).$ $Y_2 \sim (N(0,11)^2)/11 =(N(0;1)^2)*(\sqrt{11}/11).$ Where am I making mistake?",,['statistics']
64,How to solve distribution function integral,How to solve distribution function integral,,"Given $f_{X,Y}(x,y)=\begin{cases}\frac{1}{2}x^2e^{-x}e^{-y} \ &x,y >0 \\ 0 & \text{O.T.} \end{cases}$ and $Z=X+Y$ Find $f_Z(z)$ My idea is to first find the distribution function $F_Z(z)=P(Z\leq z)=P(X+Y\leq z)=P((X,Y)\in A)$, where $A=\{(x,y)\mid x\leq z-y\}$ This leads to the double integral $$\int_0^\infty \int_0^{z-y} \frac 1 2 x^2e^{-x} e^{-y} \ \ dx \ dy$$ From here I´m kinda stuck as i can´t get a useful result out of the integral. Is the integral wrong?","Given $f_{X,Y}(x,y)=\begin{cases}\frac{1}{2}x^2e^{-x}e^{-y} \ &x,y >0 \\ 0 & \text{O.T.} \end{cases}$ and $Z=X+Y$ Find $f_Z(z)$ My idea is to first find the distribution function $F_Z(z)=P(Z\leq z)=P(X+Y\leq z)=P((X,Y)\in A)$, where $A=\{(x,y)\mid x\leq z-y\}$ This leads to the double integral $$\int_0^\infty \int_0^{z-y} \frac 1 2 x^2e^{-x} e^{-y} \ \ dx \ dy$$ From here I´m kinda stuck as i can´t get a useful result out of the integral. Is the integral wrong?",,"['statistics', 'probability-distributions']"
65,Prove formula for expected value for exponential families,Prove formula for expected value for exponential families,,"Let $\{P_\theta: \Theta \subset \mathbb{R^k}\}$ be a regular statistical family. Let $T=(T_1,\ldots,T_k)$ has density of form: $\displaystyle f=c(\theta)\cdot h(t) \cdot \exp \left\{\sum_{j=1}^{k}\theta_jt_j\right\} $. Let $\ell_1\ge0, \ldots , \ell_k \ge 0$ and $\ell_1+\cdots+\ell_k=\ell$. Prove that: $$E_\theta \left[ \prod_{i=1}^k T_i^{l_i} \right] = c(\theta)\frac{\partial^\ell}{\partial \theta_1^{\ell_1} \cdots \partial \theta_k^{\ell_k}}\frac{1}{c(\theta)}.$$ My attempt: $f=h(t)\cdot \exp \left\{\sum_{j=1}^ k\theta_jt_j  - \ln\frac 1 {c(\theta)} \right\}$ is in canonical form and we have $(T_1,\ldots,T_n)$ is sufficient statistic hence $$(ET_1,\ldots,ET_n)= \left( \frac{\partial}{\partial \theta_1}\ln\frac 1 {c(\theta)}, \ldots, \frac{\partial}{\partial \theta_k}\ln\frac 1 {c(\theta)}\right) = \left(c(\theta) \cdot\frac{\partial}{\partial \theta_1}\frac{1}{c(\theta)}, \ldots , c(\theta) \cdot\frac{\partial}{\partial \theta_n}\frac{1}{c(\theta)} \right)$$ and here I stuck.","Let $\{P_\theta: \Theta \subset \mathbb{R^k}\}$ be a regular statistical family. Let $T=(T_1,\ldots,T_k)$ has density of form: $\displaystyle f=c(\theta)\cdot h(t) \cdot \exp \left\{\sum_{j=1}^{k}\theta_jt_j\right\} $. Let $\ell_1\ge0, \ldots , \ell_k \ge 0$ and $\ell_1+\cdots+\ell_k=\ell$. Prove that: $$E_\theta \left[ \prod_{i=1}^k T_i^{l_i} \right] = c(\theta)\frac{\partial^\ell}{\partial \theta_1^{\ell_1} \cdots \partial \theta_k^{\ell_k}}\frac{1}{c(\theta)}.$$ My attempt: $f=h(t)\cdot \exp \left\{\sum_{j=1}^ k\theta_jt_j  - \ln\frac 1 {c(\theta)} \right\}$ is in canonical form and we have $(T_1,\ldots,T_n)$ is sufficient statistic hence $$(ET_1,\ldots,ET_n)= \left( \frac{\partial}{\partial \theta_1}\ln\frac 1 {c(\theta)}, \ldots, \frac{\partial}{\partial \theta_k}\ln\frac 1 {c(\theta)}\right) = \left(c(\theta) \cdot\frac{\partial}{\partial \theta_1}\frac{1}{c(\theta)}, \ldots , c(\theta) \cdot\frac{\partial}{\partial \theta_n}\frac{1}{c(\theta)} \right)$$ and here I stuck.",,['statistics']
66,How to calculate covariance and correlation?,How to calculate covariance and correlation?,,"Consider for $c\in \mathbb R $ the function $f:\mathbb R→\mathbb R$ is defined by $$f(t)=\begin{cases} c(t-1), & 1 \leq t <2\\ c, & 2 \leq t <5 \\ \frac{-c}{2}(t-7), & 5 \leq t <7 \\ 0, & \text{otherwise}  \end{cases} $$ By choosing the appropriate $c$, $f$ is a density function. Let $X$ a random variable, it's  density function is $f$. Consider the random variable $Y=X^2+2$. How do I calculate the covariance $\text{Cov}(X,Y)$ and correlation coefficients $ρ_{XY}$? Decide whether the random variables $X$ and $Y$ are uncorrelated or independent. I already calculated $c=\frac{2}{9}$ which was previous part of this example, but I dont know how to calculate covariance and correlation. $\text{Cov}(X,Y)=\text{Cov}(X,X^2+2)$ but what after this?","Consider for $c\in \mathbb R $ the function $f:\mathbb R→\mathbb R$ is defined by $$f(t)=\begin{cases} c(t-1), & 1 \leq t <2\\ c, & 2 \leq t <5 \\ \frac{-c}{2}(t-7), & 5 \leq t <7 \\ 0, & \text{otherwise}  \end{cases} $$ By choosing the appropriate $c$, $f$ is a density function. Let $X$ a random variable, it's  density function is $f$. Consider the random variable $Y=X^2+2$. How do I calculate the covariance $\text{Cov}(X,Y)$ and correlation coefficients $ρ_{XY}$? Decide whether the random variables $X$ and $Y$ are uncorrelated or independent. I already calculated $c=\frac{2}{9}$ which was previous part of this example, but I dont know how to calculate covariance and correlation. $\text{Cov}(X,Y)=\text{Cov}(X,X^2+2)$ but what after this?",,['statistics']
67,Probability of a certain event subject to a constraint (1D diffusion equation),Probability of a certain event subject to a constraint (1D diffusion equation),,"I am considering a one-dimensional process represented by a diffusion equation. $$\partial_tP=D \partial_x^2P$$ The probability distribution $P(x,t)$ obeys that $P(x,0)=\delta(x-l)$ for a given value of $l>0$. We consider this distribution in the interval $[0,d]$. I want to calculate the probability of getting to the point $x=d$ subject to the condition of not having been at 0 at any previous time. How can I calculate this probability? Without the condition I thought that I can express the condition of getting to $x=d$ as: $$P(d)=\int_0^\infty P(d,t) dt$$ but I don't know how to incorporate the supplementary condition about 0.","I am considering a one-dimensional process represented by a diffusion equation. $$\partial_tP=D \partial_x^2P$$ The probability distribution $P(x,t)$ obeys that $P(x,0)=\delta(x-l)$ for a given value of $l>0$. We consider this distribution in the interval $[0,d]$. I want to calculate the probability of getting to the point $x=d$ subject to the condition of not having been at 0 at any previous time. How can I calculate this probability? Without the condition I thought that I can express the condition of getting to $x=d$ as: $$P(d)=\int_0^\infty P(d,t) dt$$ but I don't know how to incorporate the supplementary condition about 0.",,"['statistics', 'partial-differential-equations', 'brownian-motion']"
68,$R^2$ of two regressed lines divided by each other. [closed],of two regressed lines divided by each other. [closed],R^2,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question In this situation I have two lines. For the sake of the question I will simplify the two lines to simple linear equations. $$f(x) = x + 1$$ $$g(x) = x + 2$$ $f(x)$  and $g(x)$ were linearly regressed using least sum of squares. If the R$^2$ value of $f(x)$ is $0.5$ and the R$^2$ value of $g(x)$ is $0.7$, what is the R$^2$ value of the result of $\frac{f(x)}{g(x)}$?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question In this situation I have two lines. For the sake of the question I will simplify the two lines to simple linear equations. $$f(x) = x + 1$$ $$g(x) = x + 2$$ $f(x)$  and $g(x)$ were linearly regressed using least sum of squares. If the R$^2$ value of $f(x)$ is $0.5$ and the R$^2$ value of $g(x)$ is $0.7$, what is the R$^2$ value of the result of $\frac{f(x)}{g(x)}$?",,"['statistics', 'regression']"
69,Proving that a MLE is an asymptotically unbiased estimator,Proving that a MLE is an asymptotically unbiased estimator,,"Let $X_1,\ldots,X_n$ be a sample in a space with PDF $f_X(x; \theta) = \frac{3}{\theta^3}x^2 I(0\le x \le \theta)$ then caclulate the MLE for $\theta$ and prove that it is an asymptotically unbiased estimator. So far, I managed to calculate $$ \theta_m (\mathrm{MLE}) = \max(X_i),$$ but proving that it is an asymptotically unbiased estimator isn't working out. I've tried integrating $\max(X_i)$, but the integral ends up not working out for me. Should I still use an integral for proving that $\operatorname{E}(\theta_m) = \theta$ or not? And if so, how should I integrate/calculate it? Thanks.","Let $X_1,\ldots,X_n$ be a sample in a space with PDF $f_X(x; \theta) = \frac{3}{\theta^3}x^2 I(0\le x \le \theta)$ then caclulate the MLE for $\theta$ and prove that it is an asymptotically unbiased estimator. So far, I managed to calculate $$ \theta_m (\mathrm{MLE}) = \max(X_i),$$ but proving that it is an asymptotically unbiased estimator isn't working out. I've tried integrating $\max(X_i)$, but the integral ends up not working out for me. Should I still use an integral for proving that $\operatorname{E}(\theta_m) = \theta$ or not? And if so, how should I integrate/calculate it? Thanks.",,[]
70,Covariance Matrix of Zero Mean Data,Covariance Matrix of Zero Mean Data,,Given a covariance matrix is there a way to tell if it came from zero mean data?,Given a covariance matrix is there a way to tell if it came from zero mean data?,,"['statistics', 'covariance']"
71,Kolmogorov–Smirnov test,Kolmogorov–Smirnov test,,"We measured time of reaction of 8 drivers right before and 15 minutes after drinking certain amount of alcohol. Times of reaction before were: 0.22, 0.18, 0.16, 0.19, 0.20, 0.23, 0.17, 0.25 and after: 0.28, 0.25, 0.20, 0.30, 0.19, 0.26, 0.28, 0.24. The problem is to test the hypothesis whether drinking alcohol prolongs time of reaction (with a significance level $\alpha=0.05$). My attempt to the problem: The classical approach to the problem like that would be to use Kolmogorov–Smirnov test to verify if the distribution of the random variable ""times before"" (X) is equal to distribution of the random variable ""times after"" (Y). The value of D=0.625 and p-value=0.08787. It would mean that with significance level $\alpha=0.05$ we operate only on one distribution. However the alternative hypothesis in such case would be in a from $F \neq G$ and it does denote hypothesis ""drinking alcohol prolongs time of reaction"". So my second step was to define alternative hypothesis as ""CDF of Y lies below CDF of X"". I found somewhere that in such situation I should focus only on $D^{-}$. Therefore $D^{-}=0$ and $p=1$. Is that a right solution?","We measured time of reaction of 8 drivers right before and 15 minutes after drinking certain amount of alcohol. Times of reaction before were: 0.22, 0.18, 0.16, 0.19, 0.20, 0.23, 0.17, 0.25 and after: 0.28, 0.25, 0.20, 0.30, 0.19, 0.26, 0.28, 0.24. The problem is to test the hypothesis whether drinking alcohol prolongs time of reaction (with a significance level $\alpha=0.05$). My attempt to the problem: The classical approach to the problem like that would be to use Kolmogorov–Smirnov test to verify if the distribution of the random variable ""times before"" (X) is equal to distribution of the random variable ""times after"" (Y). The value of D=0.625 and p-value=0.08787. It would mean that with significance level $\alpha=0.05$ we operate only on one distribution. However the alternative hypothesis in such case would be in a from $F \neq G$ and it does denote hypothesis ""drinking alcohol prolongs time of reaction"". So my second step was to define alternative hypothesis as ""CDF of Y lies below CDF of X"". I found somewhere that in such situation I should focus only on $D^{-}$. Therefore $D^{-}=0$ and $p=1$. Is that a right solution?",,['statistics']
72,Standard Error of weighted mean estimate,Standard Error of weighted mean estimate,,"$X$ is a random variable with unknown distribution. A number of experiments are conducted to estimate $X$. Each experiment has a different reliability measure in estimating $X$. These $n$ experiments resulted in following sample set $\{x_1, x_2, x_3, ... , x_n\}$ with corresponding non-zero weights being $\{w_1, w_2, w_3, ... , w_n\}$. The higher weight corresponds to higher reliability. Note ${\sum_{i=1}^n{w_i}}$ can be greater than $1$. The best unbiased estimator of true value of $X$ is the weighted mean of sample, $\hat{X} = \bar{x}_w$, where,   $\bar{x}_w = \frac{\sum_{i=1}^n{w_ix_i}}{\sum_{i=1}^n{w_i}}$ The estimator for variance of $X$ from its true mean is, $\hat{\sigma^2} = \bar{\sigma^2}_w$, where, $\bar{\sigma^2}_w = \frac{\sum_{i=1}^n {w_i(x_i-\bar{x}_w)^2}}{\sum_{i=1}^n{w_i}}$, What would be the best estimate of Standard Error of the sampling distribution of $\bar{x}_w$. Would it be $\frac{\bar{\sigma}_w}{\sqrt{n}}$. If yes, can someone help derive/explain it.","$X$ is a random variable with unknown distribution. A number of experiments are conducted to estimate $X$. Each experiment has a different reliability measure in estimating $X$. These $n$ experiments resulted in following sample set $\{x_1, x_2, x_3, ... , x_n\}$ with corresponding non-zero weights being $\{w_1, w_2, w_3, ... , w_n\}$. The higher weight corresponds to higher reliability. Note ${\sum_{i=1}^n{w_i}}$ can be greater than $1$. The best unbiased estimator of true value of $X$ is the weighted mean of sample, $\hat{X} = \bar{x}_w$, where,   $\bar{x}_w = \frac{\sum_{i=1}^n{w_ix_i}}{\sum_{i=1}^n{w_i}}$ The estimator for variance of $X$ from its true mean is, $\hat{\sigma^2} = \bar{\sigma^2}_w$, where, $\bar{\sigma^2}_w = \frac{\sum_{i=1}^n {w_i(x_i-\bar{x}_w)^2}}{\sum_{i=1}^n{w_i}}$, What would be the best estimate of Standard Error of the sampling distribution of $\bar{x}_w$. Would it be $\frac{\bar{\sigma}_w}{\sqrt{n}}$. If yes, can someone help derive/explain it.",,"['statistics', 'random-variables', 'standard-deviation', 'standard-error']"
73,How to prove that sample of size $O(\epsilon^{-2} log \delta^{-1})$ is enough to predict quantiles?,How to prove that sample of size  is enough to predict quantiles?,O(\epsilon^{-2} log \delta^{-1}),"There is a known problem: You are given a stream of numbers and you need to find it's $q$-th quantile ($0 \le q \le 1$). You may get wrong answer but you need to return answer between $q-\epsilon$-th and $q+\epsilon$-th quantiles with probability at least $1 - \delta$. In several articles I've found statement that if you get random sample of size $O(\epsilon^{-2} log \delta^{-1})$ and you'll find it's $q$-th quantile and return it as the answer to original problem, you'll get what is needed i.e answer between $q-\epsilon$-th and $q+\epsilon$-th quantiles with probability at least $1 - \delta$. I've seen this mentioned in several places, for example, Quantiles on Streams by Chiranjeeb Buragohain et al. Approximate Medians and other Quantiles in One Pass and with... by Gurmeet Singh Manku et al But I didn't manage to either prove it myself of trace the source. Any ideas how to prove that?","There is a known problem: You are given a stream of numbers and you need to find it's $q$-th quantile ($0 \le q \le 1$). You may get wrong answer but you need to return answer between $q-\epsilon$-th and $q+\epsilon$-th quantiles with probability at least $1 - \delta$. In several articles I've found statement that if you get random sample of size $O(\epsilon^{-2} log \delta^{-1})$ and you'll find it's $q$-th quantile and return it as the answer to original problem, you'll get what is needed i.e answer between $q-\epsilon$-th and $q+\epsilon$-th quantiles with probability at least $1 - \delta$. I've seen this mentioned in several places, for example, Quantiles on Streams by Chiranjeeb Buragohain et al. Approximate Medians and other Quantiles in One Pass and with... by Gurmeet Singh Manku et al But I didn't manage to either prove it myself of trace the source. Any ideas how to prove that?",,"['probability-theory', 'statistics', 'probability-distributions', 'asymptotics', 'quantile']"
74,How to integrate standard normal cdf to nth power,How to integrate standard normal cdf to nth power,,"I'm struggling to find a way to integrate $\Phi^n(a_nx + b)$ (I have values for $a_n$ and $b_n$) I have got $(\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{a_nx+b} \exp(-\frac{y^2}{2}) dy)^n$ but I am unsure on how to proceed. Eventually I would need to show that this integral tends to $exp(-exp(-x))$ as $n \to \infty$. Any help would be greatly appreciated! (Apologies for the poor format, I've not used MathJax before)","I'm struggling to find a way to integrate $\Phi^n(a_nx + b)$ (I have values for $a_n$ and $b_n$) I have got $(\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{a_nx+b} \exp(-\frac{y^2}{2}) dy)^n$ but I am unsure on how to proceed. Eventually I would need to show that this integral tends to $exp(-exp(-x))$ as $n \to \infty$. Any help would be greatly appreciated! (Apologies for the poor format, I've not used MathJax before)",,"['integration', 'statistics', 'normal-distribution']"
75,Computing expectation of MLE?,Computing expectation of MLE?,,"This is a part of lecture note. Reviewing it I am not clear with the process.  How can I expand $\operatorname{E}\left[\dfrac 1 {\left(\sum X_i\right)}\right]$ to make it $\displaystyle\int {(\lambda^n)(x^{n-1}) \exp(-\lambda x)\,dx \over x(n+1)!}$? It's usung MLE of Gamma distribution for $X_i$'s from exponential distribution.","This is a part of lecture note. Reviewing it I am not clear with the process.  How can I expand $\operatorname{E}\left[\dfrac 1 {\left(\sum X_i\right)}\right]$ to make it $\displaystyle\int {(\lambda^n)(x^{n-1}) \exp(-\lambda x)\,dx \over x(n+1)!}$? It's usung MLE of Gamma distribution for $X_i$'s from exponential distribution.",,"['calculus', 'probability', 'statistics', 'expectation']"
76,Finding Cramer Rao Lower bound for a bivariate parameter,Finding Cramer Rao Lower bound for a bivariate parameter,,"I am having a hard time figuring out the Cramer Rao lower bound for a random sample of size $n$ from a population with $\Gamma(p, \theta)$ with $p, \theta$ unknown. The problem doesn't say what formulation to use for the gamma function, but I have used $$ \frac{\theta^p}{\Gamma(p)} x^{p-1} e^{-\theta x} I_{[0,\infty)}(x). $$ So I know that I have to find the Fisher Information matrix and compute the inverse, however the entries I get are not ""nice"". The derivatives of the logarithm of the pdf is (considering only $x_1$) $$ \frac{ \partial }{ \partial p} \ln f(x_1|p,\theta) = \ln \theta - \frac{\partial}{\partial p} (\ln \Gamma (p)) + \ln x_1, \\ \frac{\partial}{\partial \theta} \ln f(x_1|p,\theta) = \frac{p}{\theta}-x_1. $$ Now, the entries in the Fisher Information matrix would be, with $\beta = [p, \theta]^\mathrm{T}$, $$ I_{ij} = n \mathrm{E} \left[ \left( \frac{ \partial}{\partial \beta_i} \ln f(X| p, \theta) \right) \left( \frac{ \partial}{\partial \beta_j} \ln f(X| p, \theta) \right)  \right]. $$ However, with the derivatives obtained above the entries become horrible... Apart from $I_{22} = np/\theta^2 $, that is. Am I on the right track here?","I am having a hard time figuring out the Cramer Rao lower bound for a random sample of size $n$ from a population with $\Gamma(p, \theta)$ with $p, \theta$ unknown. The problem doesn't say what formulation to use for the gamma function, but I have used $$ \frac{\theta^p}{\Gamma(p)} x^{p-1} e^{-\theta x} I_{[0,\infty)}(x). $$ So I know that I have to find the Fisher Information matrix and compute the inverse, however the entries I get are not ""nice"". The derivatives of the logarithm of the pdf is (considering only $x_1$) $$ \frac{ \partial }{ \partial p} \ln f(x_1|p,\theta) = \ln \theta - \frac{\partial}{\partial p} (\ln \Gamma (p)) + \ln x_1, \\ \frac{\partial}{\partial \theta} \ln f(x_1|p,\theta) = \frac{p}{\theta}-x_1. $$ Now, the entries in the Fisher Information matrix would be, with $\beta = [p, \theta]^\mathrm{T}$, $$ I_{ij} = n \mathrm{E} \left[ \left( \frac{ \partial}{\partial \beta_i} \ln f(X| p, \theta) \right) \left( \frac{ \partial}{\partial \beta_j} \ln f(X| p, \theta) \right)  \right]. $$ However, with the derivatives obtained above the entries become horrible... Apart from $I_{22} = np/\theta^2 $, that is. Am I on the right track here?",,"['statistics', 'statistical-inference', 'fisher-information']"
77,Mixed Poisson processes and independent increments,Mixed Poisson processes and independent increments,,"In reading Ross' Introduction to Probability Models , there's a part on mixed Poisson processes which states Suppose that $L$ is continuous with density function $g$. Because $P\{N(t+s) - N(s) = n \} = \int_0^{\infty} P\{N(t+s) - N(s) = n | L = \lambda \}\ g(\lambda)\ d\lambda = \int_0^{\infty} e^{-\lambda t}\frac{(\lambda t)^n}{n!}\ g(\lambda)\ d\lambda$ we see that a conditional Poisson process has stationary increments. However, because knowing how many events occur in an interval gives information about the possible value of $L$, which affects the distribution of the number of events in any other interval, it follows that a conditional Poisson process does not generally have independent increments. (...) I don't understand why ""it follows that a conditional Poisson process does not generally have independent increments"". Can someone give a concrete example? And in what instances does it have independent increments?","In reading Ross' Introduction to Probability Models , there's a part on mixed Poisson processes which states Suppose that $L$ is continuous with density function $g$. Because $P\{N(t+s) - N(s) = n \} = \int_0^{\infty} P\{N(t+s) - N(s) = n | L = \lambda \}\ g(\lambda)\ d\lambda = \int_0^{\infty} e^{-\lambda t}\frac{(\lambda t)^n}{n!}\ g(\lambda)\ d\lambda$ we see that a conditional Poisson process has stationary increments. However, because knowing how many events occur in an interval gives information about the possible value of $L$, which affects the distribution of the number of events in any other interval, it follows that a conditional Poisson process does not generally have independent increments. (...) I don't understand why ""it follows that a conditional Poisson process does not generally have independent increments"". Can someone give a concrete example? And in what instances does it have independent increments?",,"['probability', 'statistics', 'poisson-process']"
78,Geometric reason for PCA eigen vectors being transposed,Geometric reason for PCA eigen vectors being transposed,,"I was trying to think of the reason behind the PCA eigen vectors being transposed, that is, why $y= U_D^T x$. Geometrically, is it because since we are taking the projection of the point $x$ into a reduced dimension hyperplane, we are essentailly saying that the component of x which is already in the required hyperplane need not be changed in any way, but the ones which are lying outside the hyperplane at a certain angle will need to have their dot products taken as part of their being projected on to the plane? What would be the interpretation in terms of maximizing variance?","I was trying to think of the reason behind the PCA eigen vectors being transposed, that is, why $y= U_D^T x$. Geometrically, is it because since we are taking the projection of the point $x$ into a reduced dimension hyperplane, we are essentailly saying that the component of x which is already in the required hyperplane need not be changed in any way, but the ones which are lying outside the hyperplane at a certain angle will need to have their dot products taken as part of their being projected on to the plane? What would be the interpretation in terms of maximizing variance?",,"['linear-algebra', 'statistics', 'data-analysis']"
79,What is the point in taking Monte Carlo realizations of a data set?,What is the point in taking Monte Carlo realizations of a data set?,,"Let's say I have a dataset that is 100 elements long, $X = \{x_0...x_{100}\}$ and I do 1,000 Monte Carlo realizations of the data, $X_j, 0\leq j \leq1000$, sampling 10 points each time. If I then compute the variance (or any other estimator) on those ten points, and do that for every realization, I can then use that to tell me about the variance of the whole dataset. $Var(X) \approx \dfrac{\sum_{j=0}^{1000}Var(X_j)}{1000}$ So, why would I do this? If I take those 1,000 variances, and average them, shouldn't it just converge to the variance of the whole set, if I were to measure it once? Is the only reason to do it this way to be able to measure the error on the computed variance?","Let's say I have a dataset that is 100 elements long, $X = \{x_0...x_{100}\}$ and I do 1,000 Monte Carlo realizations of the data, $X_j, 0\leq j \leq1000$, sampling 10 points each time. If I then compute the variance (or any other estimator) on those ten points, and do that for every realization, I can then use that to tell me about the variance of the whole dataset. $Var(X) \approx \dfrac{\sum_{j=0}^{1000}Var(X_j)}{1000}$ So, why would I do this? If I take those 1,000 variances, and average them, shouldn't it just converge to the variance of the whole set, if I were to measure it once? Is the only reason to do it this way to be able to measure the error on the computed variance?",,"['statistics', 'random']"
80,What's the variance of an odds ratio?,What's the variance of an odds ratio?,,"I know that  $$\log(\hat{\psi}) \sim N\left(\log(\psi), \frac{1}{n_{11}} + \frac{1}{n_{12}} + \frac{1}{n_{21}} + \frac{1}{n_{22}}\right),$$ so what's the distribution of $\hat{\psi}$? Note that $\displaystyle\hat{\psi} = \frac{n_{11}n_{22}}{n_{12}n_{21}}$ I tried using the $\delta$-method, and found that  $$\hat{\psi} \sim N\left(\psi, \frac{1}{n_{11}} + \frac{1}{n_{12}} + \frac{1}{n_{21}} + \frac{1}{n_{22}}\right).$$ Is this correct?","I know that  $$\log(\hat{\psi}) \sim N\left(\log(\psi), \frac{1}{n_{11}} + \frac{1}{n_{12}} + \frac{1}{n_{21}} + \frac{1}{n_{22}}\right),$$ so what's the distribution of $\hat{\psi}$? Note that $\displaystyle\hat{\psi} = \frac{n_{11}n_{22}}{n_{12}n_{21}}$ I tried using the $\delta$-method, and found that  $$\hat{\psi} \sim N\left(\psi, \frac{1}{n_{11}} + \frac{1}{n_{12}} + \frac{1}{n_{21}} + \frac{1}{n_{22}}\right).$$ Is this correct?",,['statistics']
81,If best unbiased estimator exists then it's maximum likelihood estimator?,If best unbiased estimator exists then it's maximum likelihood estimator?,,"Our teacher proved in class that if the best unbiased estimator exists, then it is an MLE using a theorem that if $\hat{\theta}-\theta$ is proportional to the score of $\theta$ with probability $1$ , then $\hat{\theta}$ is the best unbiased estimator since it attains the CR bound. In general, I know that MLE attains the CR bound asymptotically. So I'm in doubt whether the statement in the title holds for a finite sample. Could anyone provide some insight about relationship between the  best unbiased estimator and MLE in finite sample case (and proof)?","Our teacher proved in class that if the best unbiased estimator exists, then it is an MLE using a theorem that if is proportional to the score of with probability , then is the best unbiased estimator since it attains the CR bound. In general, I know that MLE attains the CR bound asymptotically. So I'm in doubt whether the statement in the title holds for a finite sample. Could anyone provide some insight about relationship between the  best unbiased estimator and MLE in finite sample case (and proof)?",\hat{\theta}-\theta \theta 1 \hat{\theta},"['statistics', 'estimation', 'maximum-likelihood', 'parameter-estimation']"
82,"Entropy under linear transformation, a measure theoretic proof","Entropy under linear transformation, a measure theoretic proof",,"Prove that $H(Y)=H(X)+\log(|\det(M)|)$ where $Y=MX$ and  $$H(X)=\int_\mathbb{R^d} \log(f(X)) \, dP_X=\int_{-\infty}^\infty f(x) \log(f(x)) \, dx$$ $$H(X)=\int_\mathbb{R^d} log(f(x))f(x) \, dv_S=\sum_{x\in S} \log(f(x))f(x)$$  each is the entropy of a given random variable of a discrete/continuous but absolutely continuous random variable. prove this for general integral $\int \log(\frac{dP}{d\mu*}) \, dP$ where $\frac{dP}{d\mu*}$ is the Random-Nikodym Derivative I know the fact that $m(MX)=|\det(M)|m(X)$ for Lebesgue Measure $m$ (same of counting measure?) case 1: $$H(Y)=\int_\mathbb{R} \log(f(Y)) \, dP_Y=\int_{-\infty}^\infty \log(f_Y(y)) f_Y(y) \, dy=\text{???}$$ case 2: $$H(Y)=\int_\mathbb{R} \log(f(Y)) \, dP_Y=\sum_{x\in S} \log(f(x))f(x)=???$$ For general integral $$\int \log \left(\frac{dP}{d\mu*}\right) \, dP=\text{???}$$ How to proceed the proofs? Please give answer in detail","Prove that $H(Y)=H(X)+\log(|\det(M)|)$ where $Y=MX$ and  $$H(X)=\int_\mathbb{R^d} \log(f(X)) \, dP_X=\int_{-\infty}^\infty f(x) \log(f(x)) \, dx$$ $$H(X)=\int_\mathbb{R^d} log(f(x))f(x) \, dv_S=\sum_{x\in S} \log(f(x))f(x)$$  each is the entropy of a given random variable of a discrete/continuous but absolutely continuous random variable. prove this for general integral $\int \log(\frac{dP}{d\mu*}) \, dP$ where $\frac{dP}{d\mu*}$ is the Random-Nikodym Derivative I know the fact that $m(MX)=|\det(M)|m(X)$ for Lebesgue Measure $m$ (same of counting measure?) case 1: $$H(Y)=\int_\mathbb{R} \log(f(Y)) \, dP_Y=\int_{-\infty}^\infty \log(f_Y(y)) f_Y(y) \, dy=\text{???}$$ case 2: $$H(Y)=\int_\mathbb{R} \log(f(Y)) \, dP_Y=\sum_{x\in S} \log(f(x))f(x)=???$$ For general integral $$\int \log \left(\frac{dP}{d\mu*}\right) \, dP=\text{???}$$ How to proceed the proofs? Please give answer in detail",,"['real-analysis', 'probability', 'measure-theory', 'statistics']"
83,What is the Fisher information matrix of the Wishart distribution?,What is the Fisher information matrix of the Wishart distribution?,,"I have been struggling computing the Fisher's information of the Wishart distribution. I'll write what I have gone through. Let's $\Omega$ denote a $p\times p$ Wishart random variate denoted by $\mathcal{W}(k,V)$ where $k$ is the degrees of freedom and $V$ a positive definite scale matrix. If we write $\mathcal{W}(\Omega\,|\,k,V)$ for the density function, $$ \begin{align} \nabla_{\operatorname{vech}(V)}\log\mathcal{W}(\Omega\,|\,k,V) &= \dfrac{1}{2}D_{p}'\left(V^{-1}\otimes V^{-1}\right)D_{p}\operatorname{vech}(\Omega)-\dfrac{k}{2}D_{p}\operatorname{vech}\left(V^{-1}\right)\\ \nabla_{k}\log\mathcal{W}(\Omega\,|\,k,V) &= \dfrac{1}{2}\log|\Omega|-\dfrac{1}{2}\log|V|-\dfrac{p}{2}\log 2-\dfrac{1}{2}\sum_{i=1}^{p}\psi\left(\dfrac{k+1-i}{2}\right) \end{align} $$ where $D_{p}$ is the unique duplication matrix such that $D_{p}\operatorname{vech}(A)=\operatorname{vec}(A)$, $\otimes$ Kronecker product, and $\psi$ digamma function. What is the Fisher information matrix of the Wishart distribution?","I have been struggling computing the Fisher's information of the Wishart distribution. I'll write what I have gone through. Let's $\Omega$ denote a $p\times p$ Wishart random variate denoted by $\mathcal{W}(k,V)$ where $k$ is the degrees of freedom and $V$ a positive definite scale matrix. If we write $\mathcal{W}(\Omega\,|\,k,V)$ for the density function, $$ \begin{align} \nabla_{\operatorname{vech}(V)}\log\mathcal{W}(\Omega\,|\,k,V) &= \dfrac{1}{2}D_{p}'\left(V^{-1}\otimes V^{-1}\right)D_{p}\operatorname{vech}(\Omega)-\dfrac{k}{2}D_{p}\operatorname{vech}\left(V^{-1}\right)\\ \nabla_{k}\log\mathcal{W}(\Omega\,|\,k,V) &= \dfrac{1}{2}\log|\Omega|-\dfrac{1}{2}\log|V|-\dfrac{p}{2}\log 2-\dfrac{1}{2}\sum_{i=1}^{p}\psi\left(\dfrac{k+1-i}{2}\right) \end{align} $$ where $D_{p}$ is the unique duplication matrix such that $D_{p}\operatorname{vech}(A)=\operatorname{vec}(A)$, $\otimes$ Kronecker product, and $\psi$ digamma function. What is the Fisher information matrix of the Wishart distribution?",,"['statistics', 'random-variables', 'matrix-calculus', 'random-matrices']"
84,"A regression model for the discretization of $U \sim unif[0,1]$.",A regression model for the discretization of .,"U \sim unif[0,1]","Let $U \sim unif[0,1]$ and $U_n = \frac{\lfloor nU\rfloor}{n}$. a) Determine the distribution of the difference variable $W_n = U - U_n$. b) Using part a), evaluate the correlation coefficient $\rho(U,U_n)$. c) For $Y=U$ and $X=U_n$, obtain the unique $\alpha,\beta\in\mathbb{R}$ and error variable $Z$ such that $Y = \alpha + \beta X + Z$ with $E(Z)=0$ and $\rho(X,Z)=0$.","Let $U \sim unif[0,1]$ and $U_n = \frac{\lfloor nU\rfloor}{n}$. a) Determine the distribution of the difference variable $W_n = U - U_n$. b) Using part a), evaluate the correlation coefficient $\rho(U,U_n)$. c) For $Y=U$ and $X=U_n$, obtain the unique $\alpha,\beta\in\mathbb{R}$ and error variable $Z$ such that $Y = \alpha + \beta X + Z$ with $E(Z)=0$ and $\rho(X,Z)=0$.",,"['statistics', 'probability-distributions', 'uniform-distribution']"
85,How to interpret parameter estimates in factor prediction ( in R ),How to interpret parameter estimates in factor prediction ( in R ),,"So I have some data set in a .csv file and there are three factor levels, $1$ , $2$ , $3$,  (there are fifteen of each) and each has a corresponding score. Here are some details. so the data is contained in a simple csv file, the first column is labelled Team, and the second column is labelled Score. The first column consists of fifteen 1's, followed by fifteen 2's , followed by fifteen 3's. The R code I used was data.source<-""http.www..   "" ( the data set) SportScores<-read.csv(file=data.source) I set x such that x prints 1 1 1.... 1  2 2 2 ... 2 3 3 3 .... 3 Levels 1 2 3 names(sportScores) y<-SportScores$Scores So using lm I get parameter estimates in R as Intercept (35.800) x2 (0.066) x3 (12.40) the t value are very large for intercept and $x3$, but very small for $x2$, ie it indicated to me that we cannot reject the null in this case, but what is the null? $$\beta_{0}=35.8$$ , $$\beta_{1}^{c}=0.06667$$, $$\beta_{2}^{c}=12.40$$ But how do I interpret this? I want to see any differences in scores between the 3 levels, etc. I mean, what even is the test being conducted? For example  $$\beta_{1}^{c}=0.06667$$ has a small t value, so the null hypothesis is not rejected, but what even is the null hypothesis in this case?  Moreover, from the code output itself, how can I know the associated individual standard errors of the estimated means?","So I have some data set in a .csv file and there are three factor levels, $1$ , $2$ , $3$,  (there are fifteen of each) and each has a corresponding score. Here are some details. so the data is contained in a simple csv file, the first column is labelled Team, and the second column is labelled Score. The first column consists of fifteen 1's, followed by fifteen 2's , followed by fifteen 3's. The R code I used was data.source<-""http.www..   "" ( the data set) SportScores<-read.csv(file=data.source) I set x such that x prints 1 1 1.... 1  2 2 2 ... 2 3 3 3 .... 3 Levels 1 2 3 names(sportScores) y<-SportScores$Scores So using lm I get parameter estimates in R as Intercept (35.800) x2 (0.066) x3 (12.40) the t value are very large for intercept and $x3$, but very small for $x2$, ie it indicated to me that we cannot reject the null in this case, but what is the null? $$\beta_{0}=35.8$$ , $$\beta_{1}^{c}=0.06667$$, $$\beta_{2}^{c}=12.40$$ But how do I interpret this? I want to see any differences in scores between the 3 levels, etc. I mean, what even is the test being conducted? For example  $$\beta_{1}^{c}=0.06667$$ has a small t value, so the null hypothesis is not rejected, but what even is the null hypothesis in this case?  Moreover, from the code output itself, how can I know the associated individual standard errors of the estimated means?",,"['statistics', 'mathematical-modeling']"
86,Does Beta distribution act more generally than I reckon?,Does Beta distribution act more generally than I reckon?,,"In Bayes theory, Beta distribution is often introduced to make the assumption for parameter p which is from Binomial(N,p). My question is whether Beta distribution can model all probability or ratio in a random process or it should only be applied to a p under the context of a Binomial process? Thanks heaps","In Bayes theory, Beta distribution is often introduced to make the assumption for parameter p which is from Binomial(N,p). My question is whether Beta distribution can model all probability or ratio in a random process or it should only be applied to a p under the context of a Binomial process? Thanks heaps",,"['probability', 'statistics', 'mathematical-modeling', 'data-analysis']"
87,What is the PDF of the distances between points for an uniformly selected set of points in the unit square?,What is the PDF of the distances between points for an uniformly selected set of points in the unit square?,,"Specifically, if I have a random variable $(X,Y)$ uniformly distributed according to $D = \{(x,y) : 0 \leq x \leq 1, 0 \leq y \leq 1\}$. How do I compute the PDF of $Z = \left|(X,Y) - (X,Y)\right|$? I tried the following steps: Calculate the PDF of $Z_1 = X-X$ Calculate the PDF of $Z_2 = (Z_1)^2$ Calculate the PDF of $Z_3 = Z_2 + Z_2$ Calculate the PDF of $Z_4 = \sqrt{Z_3}$ And I believe my calculations are correct up to step 2., but after that the PDFs stop matching with my simulations. I also asked the same question on Quora, but the answer ( https://www.quora.com/Given-a-set-of-uniformly-selected-2D-points-in-the-unit-square-what-is-the-PDF-of-the-distances-between-them-1 ) also does not match with my simulations. For your consideration, here is my simulation: Distances histogram for 1000 uniformly selected points in the unit square .","Specifically, if I have a random variable $(X,Y)$ uniformly distributed according to $D = \{(x,y) : 0 \leq x \leq 1, 0 \leq y \leq 1\}$. How do I compute the PDF of $Z = \left|(X,Y) - (X,Y)\right|$? I tried the following steps: Calculate the PDF of $Z_1 = X-X$ Calculate the PDF of $Z_2 = (Z_1)^2$ Calculate the PDF of $Z_3 = Z_2 + Z_2$ Calculate the PDF of $Z_4 = \sqrt{Z_3}$ And I believe my calculations are correct up to step 2., but after that the PDFs stop matching with my simulations. I also asked the same question on Quora, but the answer ( https://www.quora.com/Given-a-set-of-uniformly-selected-2D-points-in-the-unit-square-what-is-the-PDF-of-the-distances-between-them-1 ) also does not match with my simulations. For your consideration, here is my simulation: Distances histogram for 1000 uniformly selected points in the unit square .",,"['probability', 'statistics', 'probability-distributions']"
88,Distribution-free statistics on compact Lie groups,Distribution-free statistics on compact Lie groups,,"Here is some background. Let $(X_i)_{i=1}^n$ be iid random variables with joint cdf $F$. Recall that the empirical distribution function is: $$ F_n(x) = \frac{1}{n} \sum_{i=1}^n \chi_{[-\infty,x]}(X_i) . $$ Note that $F_n$ is a function-valued random variable. It is known that the Kolmogorov–Smirnov statistic: $$ \mathrm{KS}_n = \|F-F_n\|_\infty , $$ converges almost surely to zero. Even better, the normalized statistic: $$ K_n = \sqrt{n}\cdot \mathrm{KS}_n  $$ converges to the Kolmogorov distribution (sup of the Brownian Bridge), which doesn't depend on the distribution of the $X_i$. Let $G$ be a compact Lie group (semisimple, if that helps). Is there a similar statistic measuring how close a sequence in $G$ is to a given probability distribution, such that the limiting statistic is distribution-free? Any ideas or pointers to a reference would be appreciated!","Here is some background. Let $(X_i)_{i=1}^n$ be iid random variables with joint cdf $F$. Recall that the empirical distribution function is: $$ F_n(x) = \frac{1}{n} \sum_{i=1}^n \chi_{[-\infty,x]}(X_i) . $$ Note that $F_n$ is a function-valued random variable. It is known that the Kolmogorov–Smirnov statistic: $$ \mathrm{KS}_n = \|F-F_n\|_\infty , $$ converges almost surely to zero. Even better, the normalized statistic: $$ K_n = \sqrt{n}\cdot \mathrm{KS}_n  $$ converges to the Kolmogorov distribution (sup of the Brownian Bridge), which doesn't depend on the distribution of the $X_i$. Let $G$ be a compact Lie group (semisimple, if that helps). Is there a similar statistic measuring how close a sequence in $G$ is to a given probability distribution, such that the limiting statistic is distribution-free? Any ideas or pointers to a reference would be appreciated!",,"['probability', 'statistics', 'lie-groups']"
89,how to prove that $\hat \sigma^2$ is a consistent for $\sigma^2$,how to prove that  is a consistent for,\hat \sigma^2 \sigma^2,"Consider a regression model $Y_n=X_n\beta +\varepsilon$, where $X_n$ is a $n \times p_n$ matrix, and $\varepsilon=(\varepsilon_1,...,\varepsilon_n)'$ consists of independent and identically distributed variables with $E(\varepsilon_1)=0$ and $Var(\varepsilon_1)=\sigma^2$. Suppose $\hat \sigma ^2$ is the estimator of $\sigma^2$. Let $$\hat \sigma ^2=\frac{\left\|Y_n-X_n\hat\beta\right\|^2}{n-p_n}.$$ How to prove that $\hat \sigma^2$ consistent for $\sigma^2$?","Consider a regression model $Y_n=X_n\beta +\varepsilon$, where $X_n$ is a $n \times p_n$ matrix, and $\varepsilon=(\varepsilon_1,...,\varepsilon_n)'$ consists of independent and identically distributed variables with $E(\varepsilon_1)=0$ and $Var(\varepsilon_1)=\sigma^2$. Suppose $\hat \sigma ^2$ is the estimator of $\sigma^2$. Let $$\hat \sigma ^2=\frac{\left\|Y_n-X_n\hat\beta\right\|^2}{n-p_n}.$$ How to prove that $\hat \sigma^2$ consistent for $\sigma^2$?",,"['statistics', 'regression']"
90,Showing that the zeros of a sequence of functions converge in probability as the functions converge in probability,Showing that the zeros of a sequence of functions converge in probability as the functions converge in probability,,"Let $S_n$ be a sequence of random real-valued continuous functions defined on $\Theta\subset \mathbb{R}$, such that as $n\rightarrow\infty$, $S_n(\theta)\overset{p}\rightarrow S(\theta)$ for every $\theta\in\Theta$, where $S$ is non-random. Suppose that for some $\theta_0$ in the interior of $\Theta$ and every $\epsilon>0$ small enough we have $$S(\theta_0 - \epsilon)<0<S(\theta_0 + \epsilon)$$ and that $S_n$ has exactly one zero $\hat{\theta}_n$ for every $n\in\mathbb{N}$. Deduce that $\hat{\theta}_n\overset{p}\rightarrow \theta_0$ as $n\rightarrow\infty$. I have literally no idea where to start with this - I know the definition of convergence in probability, but I don't know what trick to employ to convert knowledge knowledge about the range of $S$ to knowledge about its domain.","Let $S_n$ be a sequence of random real-valued continuous functions defined on $\Theta\subset \mathbb{R}$, such that as $n\rightarrow\infty$, $S_n(\theta)\overset{p}\rightarrow S(\theta)$ for every $\theta\in\Theta$, where $S$ is non-random. Suppose that for some $\theta_0$ in the interior of $\Theta$ and every $\epsilon>0$ small enough we have $$S(\theta_0 - \epsilon)<0<S(\theta_0 + \epsilon)$$ and that $S_n$ has exactly one zero $\hat{\theta}_n$ for every $n\in\mathbb{N}$. Deduce that $\hat{\theta}_n\overset{p}\rightarrow \theta_0$ as $n\rightarrow\infty$. I have literally no idea where to start with this - I know the definition of convergence in probability, but I don't know what trick to employ to convert knowledge knowledge about the range of $S$ to knowledge about its domain.",,"['real-analysis', 'statistics', 'convergence-divergence']"
91,Expected value of Poisson Distribution,Expected value of Poisson Distribution,,Question : Number of errors on a test area on a disk has a Poisson   distribution with $λ = 0.2$. What is the expected number of errors per   test area? I said number of errors is $0.2$ since expected value of Poisson Distribution just equals to $λ$. Isn't that the right answer?,Question : Number of errors on a test area on a disk has a Poisson   distribution with $λ = 0.2$. What is the expected number of errors per   test area? I said number of errors is $0.2$ since expected value of Poisson Distribution just equals to $λ$. Isn't that the right answer?,,"['probability', 'statistics', 'poisson-distribution']"
92,f(x+s)=f(x)f(s) imply that f is an exponential function when f(0) = 1.,f(x+s)=f(x)f(s) imply that f is an exponential function when f(0) = 1.,,"I was reading a proof for a theorem in Erhan Cinlars intro to stochastic processes textbook that any equation with the following $f(x+s) = f(x)f(s)$, with $f(0)=1$. Then $f(x)=e^{(-ax)}$ for some $a$. I have been trying to prove this on my own. I came up with the following, but I didn't quite like the proof, and was wondering if someone could offer a proof utilizing Laplace transforms, or something of that flavor. Here are the following ways I have tried to show it. 1) $f'(t) =\lim_{h\to0} \frac{f(t+h) - f(t-h)}{h} = f(t) f'(0) = f(t)$ a Therefore by differential equations we get our desired result. 2) using laplace transform on $f(t+a) = f(t)f(a)$. First notice that $f(t)$ cannot be zero anywhere (assume its zero at $x$, then $f(t) = f(t+x-x) = f(t-x)f(x) = 0)$. Since it cannot be zero, we know $F(s)$ is not zero anywhere (integral of a strictly positive function is strictly positive function is strictly positive). Therefore, we have $F(S)e^{Sa} = F(S)f(a)$ which is equivalent to $0 = F(S) (e^{Sa}-f(a))$. Which is only true if $e^{Sa} = f(a)$ for all $S$, and a fixed. However, this is only true if $a = 0\dots$ So I am not sure what I am doing wrong here... I wanted to try and prove it using Laplace transforms. I would appreciate any help with understanding this.","I was reading a proof for a theorem in Erhan Cinlars intro to stochastic processes textbook that any equation with the following $f(x+s) = f(x)f(s)$, with $f(0)=1$. Then $f(x)=e^{(-ax)}$ for some $a$. I have been trying to prove this on my own. I came up with the following, but I didn't quite like the proof, and was wondering if someone could offer a proof utilizing Laplace transforms, or something of that flavor. Here are the following ways I have tried to show it. 1) $f'(t) =\lim_{h\to0} \frac{f(t+h) - f(t-h)}{h} = f(t) f'(0) = f(t)$ a Therefore by differential equations we get our desired result. 2) using laplace transform on $f(t+a) = f(t)f(a)$. First notice that $f(t)$ cannot be zero anywhere (assume its zero at $x$, then $f(t) = f(t+x-x) = f(t-x)f(x) = 0)$. Since it cannot be zero, we know $F(s)$ is not zero anywhere (integral of a strictly positive function is strictly positive function is strictly positive). Therefore, we have $F(S)e^{Sa} = F(S)f(a)$ which is equivalent to $0 = F(S) (e^{Sa}-f(a))$. Which is only true if $e^{Sa} = f(a)$ for all $S$, and a fixed. However, this is only true if $a = 0\dots$ So I am not sure what I am doing wrong here... I wanted to try and prove it using Laplace transforms. I would appreciate any help with understanding this.",,"['algebra-precalculus', 'statistics', 'probability-distributions', 'applications']"
93,2D standardization of a matrix,2D standardization of a matrix,,"Given an n-by-n matrix $A$, and using only linear transformation on each of the rows and columns of $A$ (total of $2n$ transformations), is there a way to double-standardize a matrix, so that each row and each column have zero mean and unit variance? Define centering matrix as follow: $$ C_n = I_n-\frac{1}{n}O_n $$ where $I_n$ is the identity matrix of size $n$ and $O_n$ is an n-by-n matrix of all 1's. An n-by-n matrix $X$ is called doubly-centered if: $$ X = C_n X C_n $$ A matrix is doubly centered, iff the mean of each row and each column is $0$. It's a common practice in many fields to standardize data into zero mean and unit variance, given a set of numbers with mean $\mu$ and standard deviation $\sigma$, the following linear transformation will standardize that set: $$ g(x) = \frac{x-\mu}{\sigma} $$ It's easy to double-center a matrix, using only row/column linear transformation, by subtracting the mean of every row/column and adding back the grand mean of the whole matrix. It's also easy to standardize all rows (or columns) of a matrix using only $n$ linear transformations, one for each row (see $g(x)$ above). What I'm trying to do is to simultaneously standardize all rows and columns of a matrix in a similar way, is it possible?","Given an n-by-n matrix $A$, and using only linear transformation on each of the rows and columns of $A$ (total of $2n$ transformations), is there a way to double-standardize a matrix, so that each row and each column have zero mean and unit variance? Define centering matrix as follow: $$ C_n = I_n-\frac{1}{n}O_n $$ where $I_n$ is the identity matrix of size $n$ and $O_n$ is an n-by-n matrix of all 1's. An n-by-n matrix $X$ is called doubly-centered if: $$ X = C_n X C_n $$ A matrix is doubly centered, iff the mean of each row and each column is $0$. It's a common practice in many fields to standardize data into zero mean and unit variance, given a set of numbers with mean $\mu$ and standard deviation $\sigma$, the following linear transformation will standardize that set: $$ g(x) = \frac{x-\mu}{\sigma} $$ It's easy to double-center a matrix, using only row/column linear transformation, by subtracting the mean of every row/column and adding back the grand mean of the whole matrix. It's also easy to standardize all rows (or columns) of a matrix using only $n$ linear transformations, one for each row (see $g(x)$ above). What I'm trying to do is to simultaneously standardize all rows and columns of a matrix in a similar way, is it possible?",,"['linear-algebra', 'matrices', 'statistics', 'standard-deviation']"
94,How can I find the Margin of Error E,How can I find the Margin of Error E,,"the professor hasn't gone over this section but I want to understand it so that I won't confused in class. I want to learn so that I can do the other problems. So this is the problem, If anyone could teach me so I could take note I'd deeply appreciate it. Thank you for your time. Question: Assume that a random sample is used to estimate a population proportion p. Find the margin of error E that corresponds to the given statistics and confidence level. n = 550, x equals 330, 90 % confidence I know how to obtain the alpha/2 to find the confidence. I just went over the Binomial Distribution and Limit Theorem, which understand now this is next. If anyone would care to spent their time explaining I'd be forever grateful, thank you again for your time.","the professor hasn't gone over this section but I want to understand it so that I won't confused in class. I want to learn so that I can do the other problems. So this is the problem, If anyone could teach me so I could take note I'd deeply appreciate it. Thank you for your time. Question: Assume that a random sample is used to estimate a population proportion p. Find the margin of error E that corresponds to the given statistics and confidence level. n = 550, x equals 330, 90 % confidence I know how to obtain the alpha/2 to find the confidence. I just went over the Binomial Distribution and Limit Theorem, which understand now this is next. If anyone would care to spent their time explaining I'd be forever grateful, thank you again for your time.",,"['statistics', 'probability-limit-theorems', 'confidence-interval']"
95,Is the Sample Mean and the Sample Standard Deviation independent?,Is the Sample Mean and the Sample Standard Deviation independent?,,"I know that the Sample mean and the Sample Variance are independent. However, is there a follow up to say that the Sample Mean is independent to the Sample S.D.? Thanks","I know that the Sample mean and the Sample Variance are independent. However, is there a follow up to say that the Sample Mean is independent to the Sample S.D.? Thanks",,"['probability', 'statistics']"
96,Variance of probability distribution,Variance of probability distribution,,"I have continuous distribution, $X$,  with probability density function $$f_X(x) = \frac{\alpha{\beta}^\alpha}{(x+\beta)^{\alpha+1}}$$ where $\alpha > 1, \beta > 0, x \ge 0$. I have also found the expected value to be $$E(X) = \alpha\beta\left(\frac{1}{\alpha-1}-\frac{1}{\alpha}\right)$$ I am required to obtain the variance of the distribution and I am very stuck on doing this? I know that $$Var(X) = E(X^2)-E^2(X)$$ but got stuck funding $E(X^2)$.","I have continuous distribution, $X$,  with probability density function $$f_X(x) = \frac{\alpha{\beta}^\alpha}{(x+\beta)^{\alpha+1}}$$ where $\alpha > 1, \beta > 0, x \ge 0$. I have also found the expected value to be $$E(X) = \alpha\beta\left(\frac{1}{\alpha-1}-\frac{1}{\alpha}\right)$$ I am required to obtain the variance of the distribution and I am very stuck on doing this? I know that $$Var(X) = E(X^2)-E^2(X)$$ but got stuck funding $E(X^2)$.",,"['probability', 'statistics', 'probability-distributions']"
97,memoryless property of exponential distributions with random variables,memoryless property of exponential distributions with random variables,,"It is true that $P(X>t+s|X>t)=P(X>s)$ for certain values $t$ and $s$. However, how can I show that this still holds if: $T$ is a continuous random variable. That is $P(X>T+s|X>T)=P(X>s)$ Both $T$ and $S$ are continuous RVs: $P(X>T+S|X>T)=P(X>S)$ All the random variables are independent.","It is true that $P(X>t+s|X>t)=P(X>s)$ for certain values $t$ and $s$. However, how can I show that this still holds if: $T$ is a continuous random variable. That is $P(X>T+s|X>T)=P(X>s)$ Both $T$ and $S$ are continuous RVs: $P(X>T+S|X>T)=P(X>S)$ All the random variables are independent.",,"['probability', 'probability-theory', 'probability-distributions', 'random-variables', 'exponential-distribution']"
98,"How do I calculate the UMVUE of $P(X\leq c)$ where $X_i\sim \mathrm{N}(\mu,9)$?",How do I calculate the UMVUE of  where ?,"P(X\leq c) X_i\sim \mathrm{N}(\mu,9)","How do I calculate the UMVUE of $P(X\leq c)$ where $X_i\sim \mathrm{N}(\mu,9)$? I know that $\mathbf{1}_{\{X_1\leq c\}}$ is an unbiased estimator of $P(X_1\le c)$. Besides, $S=\bar{X}$ is a complete sufficient statistic for $\mu$, so applying Rao-Blackwell once by calculating $\mathrm{E}[\mathbf{1}_{\{X_1\leq c\}}|S=s]$ should give the UMVUE (by Lehmann-Scheffé). How do I proceed from here?","How do I calculate the UMVUE of $P(X\leq c)$ where $X_i\sim \mathrm{N}(\mu,9)$? I know that $\mathbf{1}_{\{X_1\leq c\}}$ is an unbiased estimator of $P(X_1\le c)$. Besides, $S=\bar{X}$ is a complete sufficient statistic for $\mu$, so applying Rao-Blackwell once by calculating $\mathrm{E}[\mathbf{1}_{\{X_1\leq c\}}|S=s]$ should give the UMVUE (by Lehmann-Scheffé). How do I proceed from here?",,"['probability-theory', 'statistics', 'normal-distribution']"
99,Regression with noise,Regression with noise,,"I have data acquired from simulation that is tainted by noise, which I know to a Wiener process (integration of white noise, with mean $\mu = 0$ and variance $\sigma^2$ known). I have good reasons to assume that the curve is a mixture of power laws, i.e., $y(x) \sim a_0 x^{-a}$ for $x< x_0$ and $y(x) \sim b_0 x^{-b}$ for $x>x_0$. The usual trick to this kind of function is to plot $\log(y(x))$ against $\log(x)$ and fit two linear curves. However, if the noise on $y(x)$ becomes significant ($y(x) \approx \sigma$), the noise on the log-log plot becomes asymmetrically distributed. Currently, I use a Tuley's biweight functions instead of the usual least-squares but I was wondering if there was an established estimator for this kind of problem. Here's an example of the problem :","I have data acquired from simulation that is tainted by noise, which I know to a Wiener process (integration of white noise, with mean $\mu = 0$ and variance $\sigma^2$ known). I have good reasons to assume that the curve is a mixture of power laws, i.e., $y(x) \sim a_0 x^{-a}$ for $x< x_0$ and $y(x) \sim b_0 x^{-b}$ for $x>x_0$. The usual trick to this kind of function is to plot $\log(y(x))$ against $\log(x)$ and fit two linear curves. However, if the noise on $y(x)$ becomes significant ($y(x) \approx \sigma$), the noise on the log-log plot becomes asymmetrically distributed. Currently, I use a Tuley's biweight functions instead of the usual least-squares but I was wondering if there was an established estimator for this kind of problem. Here's an example of the problem :",,"['statistics', 'numerical-methods', 'regression']"
