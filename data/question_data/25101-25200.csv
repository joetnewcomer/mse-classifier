,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What does the notation $\mathbb R[x]$ mean?,What does the notation  mean?,\mathbb R[x],"What does the notation $\mathbb R[x]$ mean? I thought it was just the set $\mathbb R^n$ but then I read somewhere that my lecturer wrote $\mathbb R[x] = ${$\alpha_0 + \alpha_1x + \alpha_2x^2 + ... + \alpha_nx^n : \alpha_0, ..., \alpha_n \in \mathbb R$} Edit: The reason why I asked this question was because I had a tutorial question that said: Check whether a system {$v_1,...,v_m$} of vectors in $\mathbb R^n$ (in $\mathbb R[x]$) is linearly independent. I just assumed it meant the same thing when they put it in brackets like that. Since it isn't the case, how must I interpret this question.","What does the notation $\mathbb R[x]$ mean? I thought it was just the set $\mathbb R^n$ but then I read somewhere that my lecturer wrote $\mathbb R[x] = ${$\alpha_0 + \alpha_1x + \alpha_2x^2 + ... + \alpha_nx^n : \alpha_0, ..., \alpha_n \in \mathbb R$} Edit: The reason why I asked this question was because I had a tutorial question that said: Check whether a system {$v_1,...,v_m$} of vectors in $\mathbb R^n$ (in $\mathbb R[x]$) is linearly independent. I just assumed it meant the same thing when they put it in brackets like that. Since it isn't the case, how must I interpret this question.",,"['linear-algebra', 'polynomials', 'vector-spaces', 'definition']"
1,"In a linear program, how to add a conditional bound to x?","In a linear program, how to add a conditional bound to x?",,"I am working with a standard linear program: $$\text{min}\:\:f'x$$ $$s.t.\:\:Ax = b$$ $$x ≥ 0$$ Goal: I want to enforce all nonzero solutions $x_i\in$ x to be greater than or equal to a certain threshold ""k"" if it's nonzero. In other words, I want to add a conditional bound to the LP: if any $x_i$ is > 0, enforce $x_i$ ≥ k. Main issue: Is there a way to set up this problem as an LP? Any alternate approaches? Any input would be appreciated and I'm happy to provide any additional info as needed! Thanks!","I am working with a standard linear program: $$\text{min}\:\:f'x$$ $$s.t.\:\:Ax = b$$ $$x ≥ 0$$ Goal: I want to enforce all nonzero solutions $x_i\in$ x to be greater than or equal to a certain threshold ""k"" if it's nonzero. In other words, I want to add a conditional bound to the LP: if any $x_i$ is > 0, enforce $x_i$ ≥ k. Main issue: Is there a way to set up this problem as an LP? Any alternate approaches? Any input would be appreciated and I'm happy to provide any additional info as needed! Thanks!",,"['linear-algebra', 'optimization', 'matlab', 'linear-programming']"
2,Matrices over a finite field with given Jordan normal form over the algebraic closure,Matrices over a finite field with given Jordan normal form over the algebraic closure,,"Can one describe the (conjugacy classes of) square matrices over a finite field such that over the algebraic closure of this finite field their Jordan normal form consists of one Jordan block? (Such matrices correspond to absolutely indecomposable representations of the quiver with one vertex and one loop, which are important for Kac's conjecture).","Can one describe the (conjugacy classes of) square matrices over a finite field such that over the algebraic closure of this finite field their Jordan normal form consists of one Jordan block? (Such matrices correspond to absolutely indecomposable representations of the quiver with one vertex and one loop, which are important for Kac's conjecture).",,"['linear-algebra', 'matrices', 'representation-theory', 'finite-fields', 'jordan-normal-form']"
3,Cayley transformation of a skew-symmetric matrix is orthogonal?,Cayley transformation of a skew-symmetric matrix is orthogonal?,,"If $S$ is skew-symmetric ($S^{T} = -S$), how do I show that $Q$ is orthogonal where $$Q = (I + S)(I - S)^{-1}$$ which is the Cayley transformation of $S$.","If $S$ is skew-symmetric ($S^{T} = -S$), how do I show that $Q$ is orthogonal where $$Q = (I + S)(I - S)^{-1}$$ which is the Cayley transformation of $S$.",,"['linear-algebra', 'matrices', 'transformation']"
4,Getting equations from a network graph,Getting equations from a network graph,,"I am learning about Network Analysis in Linear Algebra and I need help figuring out how to get the equations from this graph: The arrows represent how many particles are going in a given direction. The back of the book tells me they are: $$x_1 + x_2 = 20$$ $$x_3 + 20 = x_4$$ $$x_2 + x_3 = 20$$ $$x_1 + 10 = x_5$$ $$x_5 + 10 = x_4$$ From my understanding, if we look at first equation, there are twenty particles going to the $1$ and they can split in two directions. So if you add up $x_1$ and $x_2$, they should equal twenty as it is impossible to go higher than that. Even if I am correctly analyzing the first equation, I don't understand the rest of the equations that the book gives. For example, how does $x_3 + 20 = x_4$? What do $x_3$ and $20$ have to do with $x_4$? Any help would be appreciated. Thank you!","I am learning about Network Analysis in Linear Algebra and I need help figuring out how to get the equations from this graph: The arrows represent how many particles are going in a given direction. The back of the book tells me they are: $$x_1 + x_2 = 20$$ $$x_3 + 20 = x_4$$ $$x_2 + x_3 = 20$$ $$x_1 + 10 = x_5$$ $$x_5 + 10 = x_4$$ From my understanding, if we look at first equation, there are twenty particles going to the $1$ and they can split in two directions. So if you add up $x_1$ and $x_2$, they should equal twenty as it is impossible to go higher than that. Even if I am correctly analyzing the first equation, I don't understand the rest of the equations that the book gives. For example, how does $x_3 + 20 = x_4$? What do $x_3$ and $20$ have to do with $x_4$? Any help would be appreciated. Thank you!",,"['linear-algebra', 'graph-theory']"
5,Prove that the projection operator $\mathbb P_+\equiv|+z\rangle\!\langle +z|$ is Hermitian,Prove that the projection operator  is Hermitian,\mathbb P_+\equiv|+z\rangle\!\langle +z|,"Use Dirac notation (the properties of kets, bras and inner products) directly to establish that the projection operator $\mathbb{\hat P}_+$ is Hermitian. Use the fact that $\mathbb{\hat P}^2_+=\mathbb{\hat P}_+$ to establish that the eigenvalues of the projection operator are $1$ and $0$. I know how to prove this using mathematical notation, i.e. for any $x,y\in V$ we must show that $\langle x, \ \mathbb{\hat P}_+y\rangle = \langle \mathbb{\hat P}_+x, \ y\rangle$ but how can I prove the way the book suggested, i.e. using Dirac notation and the properties of kets and bras?","Use Dirac notation (the properties of kets, bras and inner products) directly to establish that the projection operator $\mathbb{\hat P}_+$ is Hermitian. Use the fact that $\mathbb{\hat P}^2_+=\mathbb{\hat P}_+$ to establish that the eigenvalues of the projection operator are $1$ and $0$. I know how to prove this using mathematical notation, i.e. for any $x,y\in V$ we must show that $\langle x, \ \mathbb{\hat P}_+y\rangle = \langle \mathbb{\hat P}_+x, \ y\rangle$ but how can I prove the way the book suggested, i.e. using Dirac notation and the properties of kets and bras?",,"['linear-algebra', 'physics', 'mathematical-physics', 'quantum-mechanics', 'projection-matrices']"
6,Inequality between operator norm and Hilbert-Schmidt norm,Inequality between operator norm and Hilbert-Schmidt norm,,"I have seen the following inequality here but I don't know where I can find a proof for it. Could somebody give me a hint to understand it or guide me to a reference please? $\|AB\|_{HS} \leq \|A\|_{\mathrm{op}} \|B\|_{HS} \ {\rm for}\  A, B ∈ \mathcal{M}_n$.","I have seen the following inequality here but I don't know where I can find a proof for it. Could somebody give me a hint to understand it or guide me to a reference please? $\|AB\|_{HS} \leq \|A\|_{\mathrm{op}} \|B\|_{HS} \ {\rm for}\  A, B ∈ \mathcal{M}_n$.",,"['linear-algebra', 'functional-analysis', 'inequality', 'operator-theory', 'normed-spaces']"
7,After removing any part the rest can be split evenly. Consequences?,After removing any part the rest can be split evenly. Consequences?,,"Let $S$ be a finite collection of real numbers (not necessarily distinct). If any element of $S$ is removed then the remaining real numbers can be divided into two collections with same size and same sum ; then is it true that all elements of $S$ are equal ? (I know the result is true if the ""reals"" are replaced by ""integers"".)","Let be a finite collection of real numbers (not necessarily distinct). If any element of is removed then the remaining real numbers can be divided into two collections with same size and same sum ; then is it true that all elements of are equal ? (I know the result is true if the ""reals"" are replaced by ""integers"".)",S S S,"['linear-algebra', 'combinatorics', 'induction', 'real-numbers']"
8,"If some vectors in $\mathbb Q^n$ are linearly independent over $\mathbb Q$ , then are they also linearly independent over $\mathbb C$?","If some vectors in  are linearly independent over  , then are they also linearly independent over ?",\mathbb Q^n \mathbb Q \mathbb C,"Let $\vec v_1 , ..., \vec v_k $ be vectors in $\mathbb Q^n$ linearly independent over $\mathbb Q$ , then is it true that $\sum_{i=1}^ka_i\vec v_i=0, a_i\in \mathbb C, \forall 1\leq i\leq k \implies a_i=0 ,\forall 1\leq i\leq k $ ?","Let $\vec v_1 , ..., \vec v_k $ be vectors in $\mathbb Q^n$ linearly independent over $\mathbb Q$ , then is it true that $\sum_{i=1}^ka_i\vec v_i=0, a_i\in \mathbb C, \forall 1\leq i\leq k \implies a_i=0 ,\forall 1\leq i\leq k $ ?",,"['linear-algebra', 'vector-spaces']"
9,Show A and B have a common eigenvalue,Show A and B have a common eigenvalue,,"Let A, B and C complex square matrices such that: $ C\neq 0 $ and $AC=CB $ prove that A and B has a common eigenvalue. It's worth mentioning that earlier in the assignment I have proved that $A^{n}C=CB^{n}$,  but I'm not sure how to use it. This is taken from a linear algebra 2 course.","Let A, B and C complex square matrices such that: $ C\neq 0 $ and $AC=CB $ prove that A and B has a common eigenvalue. It's worth mentioning that earlier in the assignment I have proved that $A^{n}C=CB^{n}$,  but I'm not sure how to use it. This is taken from a linear algebra 2 course.",,['linear-algebra']
10,proof of basic fact that torus actions are diagonalizable,proof of basic fact that torus actions are diagonalizable,,"Suppose a torus $T=(\mathbb{C}^\ast)^n$ acts on a finite dimensional vector space $W$, and define for $m \in M$ ($M$ is the character lattice of $T$) the eigenspace $W_m$ by  $$W_m = \{w \in W \mid t\cdot w = \chi^m(t)w \text{ for all }t \in T \}$$ i.e. for $w \in W_m$ is a simultaneous eigenvector for all $t \in T$, with eigenvalue $\chi^m(t)$ depending on $t \in T$. Then it is a famous fact $$W=\underset{m \in M} \bigoplus W_m$$ Can someone provide a somewhat self-contained proof of this result? I don't know much about the theory of algebraic groups.","Suppose a torus $T=(\mathbb{C}^\ast)^n$ acts on a finite dimensional vector space $W$, and define for $m \in M$ ($M$ is the character lattice of $T$) the eigenspace $W_m$ by  $$W_m = \{w \in W \mid t\cdot w = \chi^m(t)w \text{ for all }t \in T \}$$ i.e. for $w \in W_m$ is a simultaneous eigenvector for all $t \in T$, with eigenvalue $\chi^m(t)$ depending on $t \in T$. Then it is a famous fact $$W=\underset{m \in M} \bigoplus W_m$$ Can someone provide a somewhat self-contained proof of this result? I don't know much about the theory of algebraic groups.",,"['linear-algebra', 'representation-theory', 'algebraic-groups']"
11,Finding the inverse of a matrix using a series,Finding the inverse of a matrix using a series,,"I want to find the inverse of the matrix $A$ given by: $ \left( \begin{array}{cc} 1 & -\epsilon  \\ \epsilon & 1  \\   \end{array} \right) $ where $|\epsilon|$ $< 1$ (although I do not know how to use this yet) by finding the matrix $B$ such that $A = I-B$ or $B = I-A$. $B$ is $ \left( \begin{array}{cc} 0 & \epsilon  \\ -\epsilon & 0  \\   \end{array} \right) $ And I want to find the inverse by summing the series $I + B + B^2 + B^3 . . .$ this series doesn't converge (unless we use the fact about the absolute value given, but I don't know what to do with that here).","I want to find the inverse of the matrix $A$ given by: $ \left( \begin{array}{cc} 1 & -\epsilon  \\ \epsilon & 1  \\   \end{array} \right) $ where $|\epsilon|$ $< 1$ (although I do not know how to use this yet) by finding the matrix $B$ such that $A = I-B$ or $B = I-A$. $B$ is $ \left( \begin{array}{cc} 0 & \epsilon  \\ -\epsilon & 0  \\   \end{array} \right) $ And I want to find the inverse by summing the series $I + B + B^2 + B^3 . . .$ this series doesn't converge (unless we use the fact about the absolute value given, but I don't know what to do with that here).",,"['linear-algebra', 'matrices', 'vector-spaces']"
12,Proving $V$ is isomorphic to $W$ iff $\dim V=\dim W$,Proving  is isomorphic to  iff,V W \dim V=\dim W,"Let $V$ and $W$ be two finite vector spaces over $F$ . Prove that $V$ is isomorphic to $W$ iff $\dim V=\dim W$ I think I got the general approach but I don't think it's rigorous enough. $\Rightarrow$ Suppose $V$ is isomorphic to $W$ then there's a linear map $T$ that is a bijection. Let $\mathbb B=\{v_1,...,v_n\}$ be a basis for $V$ . We know that $V=\displaystyle\sum^{n}_{i=1}\alpha_iv_i$ . Since $T$ is a bijection there n elements in $W$ such that $Tv_i=w_i$ . So for each $w\in W$ there's a single representation of $\displaystyle\sum^{n}_{i=1}\alpha_iw_i$ . So we get that the set $\mathbb K=\{w_1,...,w_n\}$ is linearly independent thus it's basis so $\dim V=\dim W$ . $\Leftarrow$ I need to show that if there's a linear map between the two spaces then it's a bijection thus it's isomorphic but I'm not sure to word it right.",Let and be two finite vector spaces over . Prove that is isomorphic to iff I think I got the general approach but I don't think it's rigorous enough. Suppose is isomorphic to then there's a linear map that is a bijection. Let be a basis for . We know that . Since is a bijection there n elements in such that . So for each there's a single representation of . So we get that the set is linearly independent thus it's basis so . I need to show that if there's a linear map between the two spaces then it's a bijection thus it's isomorphic but I'm not sure to word it right.,"V W F V W \dim V=\dim W \Rightarrow V W T \mathbb B=\{v_1,...,v_n\} V V=\displaystyle\sum^{n}_{i=1}\alpha_iv_i T W Tv_i=w_i w\in W \displaystyle\sum^{n}_{i=1}\alpha_iw_i \mathbb K=\{w_1,...,w_n\} \dim V=\dim W \Leftarrow",['linear-algebra']
13,Time complexity of finding nullspace of a matrix,Time complexity of finding nullspace of a matrix,,The problem is finding the nullspace of a singular $n \times n$ square matrix $A$ (or alternatively computing the eigenvectors corresponding to the eigenvalue 0). What is the algorithm with the lowest time complexity of finding the nullspace. e.g. for SVD this is $O(n^3)$,The problem is finding the nullspace of a singular $n \times n$ square matrix $A$ (or alternatively computing the eigenvectors corresponding to the eigenvalue 0). What is the algorithm with the lowest time complexity of finding the nullspace. e.g. for SVD this is $O(n^3)$,,"['linear-algebra', 'eigenvalues-eigenvectors', 'svd']"
14,how to compute the determinant of the linear map $f(X)=AXC$,how to compute the determinant of the linear map,f(X)=AXC,"Let $V$ be the vector space of $m\times n$ matrices over a field $F$. Fix an $m\times m$ matrix $A$ and an $n\times n$ matrix $C$, and consider the map $\phi: V\longrightarrow V$ defined by $\phi(B)=ABC$ for $B\in V$. Compute the determinant of $\phi$ in terms of $A$ and $C$. How to compute? What is the result?","Let $V$ be the vector space of $m\times n$ matrices over a field $F$. Fix an $m\times m$ matrix $A$ and an $n\times n$ matrix $C$, and consider the map $\phi: V\longrightarrow V$ defined by $\phi(B)=ABC$ for $B\in V$. Compute the determinant of $\phi$ in terms of $A$ and $C$. How to compute? What is the result?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'ordinary-differential-equations', 'determinant']"
15,Quotient spaces in linear algebra,Quotient spaces in linear algebra,,"There's a statement in some notes I'm reading that goes like this: ""...$V/U$ is a 'simplified version' of $V$ where the elements of $U$ are ignored"" ($V$ and $U$ are vector spaces). I'm still trying to understand this idea: can someone shed some light on why we ignore $U$ in $V/U$?  I mean I understand that everything belonging to $U$ falls in the equivalence class $[0]$, but is that the sense in which we are ""ignoring"" $U$?  Is a quotient space a set of equivalence classes?  That is to say that, if we have equivalent affine subsets $v_1 + U = v_2 + U$, are $[v_1]$ and $[v_2]$ the same elements in $U/V$?  If this is the case, then aren't we ignoring certain elements of $V$ (like we would ignore $[v_2]$ in this case) because all equivalent affine subsets are redundancies and are therefore collapsed into one respective equivalence class?","There's a statement in some notes I'm reading that goes like this: ""...$V/U$ is a 'simplified version' of $V$ where the elements of $U$ are ignored"" ($V$ and $U$ are vector spaces). I'm still trying to understand this idea: can someone shed some light on why we ignore $U$ in $V/U$?  I mean I understand that everything belonging to $U$ falls in the equivalence class $[0]$, but is that the sense in which we are ""ignoring"" $U$?  Is a quotient space a set of equivalence classes?  That is to say that, if we have equivalent affine subsets $v_1 + U = v_2 + U$, are $[v_1]$ and $[v_2]$ the same elements in $U/V$?  If this is the case, then aren't we ignoring certain elements of $V$ (like we would ignore $[v_2]$ in this case) because all equivalent affine subsets are redundancies and are therefore collapsed into one respective equivalence class?",,"['linear-algebra', 'vector-spaces', 'definition', 'equivalence-relations']"
16,"If $A^2=-I$, Prove that $\det{A}=1$","If , Prove that",A^2=-I \det{A}=1,"If $A^2=-I$ , where $A$ is a square matrix of order $n$ and which contains real entries only and $I$ is identity matrix. Then how can we prove that $\det(A)=1$?. I could prove that $n$ should be an even integer. But could not proceed to prove that $\det(A)$ can take only $1$, finding out few matrices which satisfies such properies (of small order) also verifies the given statement that the determinant is only $1$ and not $-1$. Can anyone help with a hint ?","If $A^2=-I$ , where $A$ is a square matrix of order $n$ and which contains real entries only and $I$ is identity matrix. Then how can we prove that $\det(A)=1$?. I could prove that $n$ should be an even integer. But could not proceed to prove that $\det(A)$ can take only $1$, finding out few matrices which satisfies such properies (of small order) also verifies the given statement that the determinant is only $1$ and not $-1$. Can anyone help with a hint ?",,['linear-algebra']
17,Eigenvalues and eigenvectors computation (in infinite dimension),Eigenvalues and eigenvectors computation (in infinite dimension),,"Let $T$ be the backward shift operator: $Tv = T(v_1,v_2,....) = (v_2,v_3,....)$. I would like to determine all the eigenvectors and eigenvalues. So far I have the following: It is evident that $(\alpha, 0,0,....)$ is an eigenvector for the eigenvalue $0$. It is also easy to see that $(\alpha, \alpha, \alpha, ...)$ is an eigenvector for the eigenvalue $1$. Finally I observed that if $\lambda$ is any scalar in the underlying field then $(1,\lambda, \lambda^2, \lambda^3,...) , (\lambda, \lambda^2, \lambda^3,...)$ etc. are all eigenvectors for the eigenvalue $\lambda$. Now I need to either argue why these are all eigenvectors or find more. But although I think these are all I don't know how to prove it. How to proceed from here?","Let $T$ be the backward shift operator: $Tv = T(v_1,v_2,....) = (v_2,v_3,....)$. I would like to determine all the eigenvectors and eigenvalues. So far I have the following: It is evident that $(\alpha, 0,0,....)$ is an eigenvector for the eigenvalue $0$. It is also easy to see that $(\alpha, \alpha, \alpha, ...)$ is an eigenvector for the eigenvalue $1$. Finally I observed that if $\lambda$ is any scalar in the underlying field then $(1,\lambda, \lambda^2, \lambda^3,...) , (\lambda, \lambda^2, \lambda^3,...)$ etc. are all eigenvectors for the eigenvalue $\lambda$. Now I need to either argue why these are all eigenvectors or find more. But although I think these are all I don't know how to prove it. How to proceed from here?",,['linear-algebra']
18,What does it mean/imply that all my singular values are ones?,What does it mean/imply that all my singular values are ones?,,"Suppose I apply SVD (singular value decomposition) on some real-valued matrix $M$, that is, $M = USV^T$. Now, if $S$ is an identity matrix, what does it mean? Does $M$ have some special properties? Could SVD be then calculated in some peculiar (faster) way?","Suppose I apply SVD (singular value decomposition) on some real-valued matrix $M$, that is, $M = USV^T$. Now, if $S$ is an identity matrix, what does it mean? Does $M$ have some special properties? Could SVD be then calculated in some peculiar (faster) way?",,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'svd']"
19,Prove that a square matrix commutes with its inverse [duplicate],Prove that a square matrix commutes with its inverse [duplicate],,"This question already has answers here : If $AB = I$ then $BA = I$ (34 answers) Closed 10 years ago . The Question: This is a very fundamental and commonly used result in linear algebra, but I haven't been able to find a proof or prove it myself.  The statement is as follows: let $A$ be an $n\times n$ square matrix, and suppose that $B=\operatorname{LeftInv}(A)$ is a matrix such that $BA=I$.  Prove that $AB=I$.  That is, prove that a matrix commutes with its inverse, that the left-inverse is also the right-inverse My thoughts so far: This is particularly annoying to me because it seems like it should be easy. We have a similar statement for group multiplication, but the commutativity of inverses is often presented as part of the definition. Does this property necessarily follow from the associativity of multiplication?  I've noticed that from associativity, we have $$ \left(A\operatorname{LeftInv}(A)\right)A=A\left(\operatorname{LeftInv}(A)A\right) $$ But is that enough? It might help to talk about generalized inverses .","This question already has answers here : If $AB = I$ then $BA = I$ (34 answers) Closed 10 years ago . The Question: This is a very fundamental and commonly used result in linear algebra, but I haven't been able to find a proof or prove it myself.  The statement is as follows: let $A$ be an $n\times n$ square matrix, and suppose that $B=\operatorname{LeftInv}(A)$ is a matrix such that $BA=I$.  Prove that $AB=I$.  That is, prove that a matrix commutes with its inverse, that the left-inverse is also the right-inverse My thoughts so far: This is particularly annoying to me because it seems like it should be easy. We have a similar statement for group multiplication, but the commutativity of inverses is often presented as part of the definition. Does this property necessarily follow from the associativity of multiplication?  I've noticed that from associativity, we have $$ \left(A\operatorname{LeftInv}(A)\right)A=A\left(\operatorname{LeftInv}(A)A\right) $$ But is that enough? It might help to talk about generalized inverses .",,"['linear-algebra', 'group-theory']"
20,Help finding eigenvectors?,Help finding eigenvectors?,,"The given matrix is: $$ \begin{pmatrix}3 & 1 & 6 \\ 2 & 1 & 0 \\ -1 & 0 & -3\end{pmatrix}\qquad $$ I got the characteristic polynomial of $$x^3 - x^2 - 5x - 3 = 0$$ which factors down to $$(x+1)^2 * (x-3) = 0$$ I see that it  has eigenvalues of -1 and 3. I know I'm almost there, I plugged in the eigenvalues to $A-\lambda I$ but completely forgot how to find the eigenvectors after this. When $\lambda$ = 3, I got: $$\begin{pmatrix}0 & 1 & 6 \\ 2 & -2 & 0 \\ -1 & 0 & 0\end{pmatrix} \begin{pmatrix}x _1 \\ x_2 \\ x_3\end{pmatrix}=0\qquad$$ and when $\lambda$ = -1, I got: $$\begin{pmatrix}4 & 1 & 6 \\ 2 & 0 & 0 \\ -1 & 0 & 4\end{pmatrix}\begin{pmatrix}x _1 \\ x_2 \\ x_3\end{pmatrix}=0\qquad$$ Where do I go from here? Row-reduce the $3x3$ matrices to solve?","The given matrix is: $$ \begin{pmatrix}3 & 1 & 6 \\ 2 & 1 & 0 \\ -1 & 0 & -3\end{pmatrix}\qquad $$ I got the characteristic polynomial of $$x^3 - x^2 - 5x - 3 = 0$$ which factors down to $$(x+1)^2 * (x-3) = 0$$ I see that it  has eigenvalues of -1 and 3. I know I'm almost there, I plugged in the eigenvalues to $A-\lambda I$ but completely forgot how to find the eigenvectors after this. When $\lambda$ = 3, I got: $$\begin{pmatrix}0 & 1 & 6 \\ 2 & -2 & 0 \\ -1 & 0 & 0\end{pmatrix} \begin{pmatrix}x _1 \\ x_2 \\ x_3\end{pmatrix}=0\qquad$$ and when $\lambda$ = -1, I got: $$\begin{pmatrix}4 & 1 & 6 \\ 2 & 0 & 0 \\ -1 & 0 & 4\end{pmatrix}\begin{pmatrix}x _1 \\ x_2 \\ x_3\end{pmatrix}=0\qquad$$ Where do I go from here? Row-reduce the $3x3$ matrices to solve?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
21,"Symmetric matrix and inner product: $\langle Ah,x\rangle = \langle h,A^T x\rangle =\langle Ax,h\rangle$",Symmetric matrix and inner product:,"\langle Ah,x\rangle = \langle h,A^T x\rangle =\langle Ax,h\rangle","If A is real, symmetric, regular, positive definite matrix in $R^{n.n}$ and $x,h\in R^n$, why is it $\langle Ah,x\rangle = \langle h,A^T x\rangle =\langle Ax,h\rangle$? Is there some rule or theorem for this?","If A is real, symmetric, regular, positive definite matrix in $R^{n.n}$ and $x,h\in R^n$, why is it $\langle Ah,x\rangle = \langle h,A^T x\rangle =\langle Ax,h\rangle$? Is there some rule or theorem for this?",,"['linear-algebra', 'matrices', 'inner-products', 'symmetric-matrices']"
22,When does $A^p$ is a diagonal $2\times 2$ matrix imply that $A$ is a diagonal matrix,When does  is a diagonal  matrix imply that  is a diagonal matrix,A^p 2\times 2 A,"Let $A \in\mathrm{SL}(2,\mathbb{C})$ and $p > 1$ be a natural number. Under which conditions the following statement is true? $A^p$ is a diagonal matrix implies $A$ is a diagonal matrix For the case $p=2$ and $A^2 \neq \pm \mathrm{Id}$ it is a simple calculation to show that it is true.","Let $A \in\mathrm{SL}(2,\mathbb{C})$ and $p > 1$ be a natural number. Under which conditions the following statement is true? $A^p$ is a diagonal matrix implies $A$ is a diagonal matrix For the case $p=2$ and $A^2 \neq \pm \mathrm{Id}$ it is a simple calculation to show that it is true.",,"['linear-algebra', 'matrices', 'diagonalization']"
23,sum of the eigenvalues = trace($A$)?,sum of the eigenvalues = trace()?,A,"Is it true that for a square matrix $A$, all of whose eigenvalues exist in the base field, sum of the eigenvalues = trace($A$)? The result holds in all the matrices I've studied.","Is it true that for a square matrix $A$, all of whose eigenvalues exist in the base field, sum of the eigenvalues = trace($A$)? The result holds in all the matrices I've studied.",,['linear-algebra']
24,Linear algebra on Fibonacci number,Linear algebra on Fibonacci number,,"Consider the sequence $\{a_n\}_{n\ge 0}$ given by the recurrence relation $$a_0=1,\ a_1=-1,\ a_{n+1}=3a_n+10a_{n-1}\ \ \text{for } n\ge2$$ And I am asked to work out the closed form expression for an in the same fashion as the proof for Fibonacci numbers by using linear algebra way Thanks all for the help!","Consider the sequence $\{a_n\}_{n\ge 0}$ given by the recurrence relation $$a_0=1,\ a_1=-1,\ a_{n+1}=3a_n+10a_{n-1}\ \ \text{for } n\ge2$$ And I am asked to work out the closed form expression for an in the same fashion as the proof for Fibonacci numbers by using linear algebra way Thanks all for the help!",,['linear-algebra']
25,Order of calculation in all math equations,Order of calculation in all math equations,,"I already asked a question ( Order of operations in rotation matrix notation. ) about the order in which a particular equation is ""processed"" and now I need to generalise that and learn the rules of math notation. I can't find this sort of 'documentation' in any of my books or on the web. In the ""construction"" equation in J.M.'s answer to Matrix for rotation around a vector , in what order does the equation get 'processed'? Commentators on my first question seemed doubtful as to the ambiguity in mathematical notation that I seem to be experiencing and that parentheses are commonplace, yet I rarely see them used to denote processing priority, so its obviously implied (which is no good for the unwashed). The main problem I have is that the sine function has a power-of 2 right after it, which throws my assumption that the very next symbol is what should be fed into the function. Even though I've been programming since I was 8, I'm finding math almost as hard-going as anyone else and I feel that it is the compressive, symbolic syntax that's the problem. Anyway, can anyone recommend a primer on reading math, rather like a programming language specification? Luke","I already asked a question ( Order of operations in rotation matrix notation. ) about the order in which a particular equation is ""processed"" and now I need to generalise that and learn the rules of math notation. I can't find this sort of 'documentation' in any of my books or on the web. In the ""construction"" equation in J.M.'s answer to Matrix for rotation around a vector , in what order does the equation get 'processed'? Commentators on my first question seemed doubtful as to the ambiguity in mathematical notation that I seem to be experiencing and that parentheses are commonplace, yet I rarely see them used to denote processing priority, so its obviously implied (which is no good for the unwashed). The main problem I have is that the sine function has a power-of 2 right after it, which throws my assumption that the very next symbol is what should be fed into the function. Even though I've been programming since I was 8, I'm finding math almost as hard-going as anyone else and I feel that it is the compressive, symbolic syntax that's the problem. Anyway, can anyone recommend a primer on reading math, rather like a programming language specification? Luke",,"['linear-algebra', 'algebra-precalculus', 'matrices', 'notation']"
26,"Approximate a function over the interval $[0, 1]$ by a polynomial of degree $n$ (or less).",Approximate a function over the interval  by a polynomial of degree  (or less).,"[0, 1] n","To approximate a function $G$ over the interval $[0,1]$ by a polynomial $P$ of degree $n$ (or less), we minimize the function $f:R^{n+1} \to R$ given by $F(a) = \int_0^1  (G(x) - P_a(x))^2\,dx$, where $P_a(x) = a_nx^n+a_{n-1}x^{n-1}+\cdots+a_0$ and $a = (a_0,a_1,\ldots,a_n)$. Find the equation satisfied by the optimal coefficients $a_*$ (necessary condition). Show that this equation can by written as a linear equation of the form $Ma_* = B$ for some vector $B$ in $R^{n+1}$ and some symmetric matrix $M$ in $R^{(n+1)\times(n+1)}$. To be honest, I have never seen a question of this type and don't know where to start, any help in regards to the thought process would be appreciated. Thanks","To approximate a function $G$ over the interval $[0,1]$ by a polynomial $P$ of degree $n$ (or less), we minimize the function $f:R^{n+1} \to R$ given by $F(a) = \int_0^1  (G(x) - P_a(x))^2\,dx$, where $P_a(x) = a_nx^n+a_{n-1}x^{n-1}+\cdots+a_0$ and $a = (a_0,a_1,\ldots,a_n)$. Find the equation satisfied by the optimal coefficients $a_*$ (necessary condition). Show that this equation can by written as a linear equation of the form $Ma_* = B$ for some vector $B$ in $R^{n+1}$ and some symmetric matrix $M$ in $R^{(n+1)\times(n+1)}$. To be honest, I have never seen a question of this type and don't know where to start, any help in regards to the thought process would be appreciated. Thanks",,"['linear-algebra', 'multivariable-calculus', 'optimization', 'nonlinear-optimization']"
27,Optimal symmetric rank-1 approximation,Optimal symmetric rank-1 approximation,,"I want to find $\mathbf{x}$ that minimizes $\|A-\mathbf{x}\mathbf{x}'\|^2$ where $\|\cdot\|$ is Frobenius norm . Differentiating with respect to $\mathbf{x}$ and setting to $\mathbf{0}$, I get $$\mathbf{x}\mathbf{x}'\mathbf{x}=A \mathbf{x}$$ Any idea how to proceed further? (Per request, here's how I did the derivative) Let $J=\|A-\mathbf{x}\mathbf{x}'\|^2=\text{tr}(Y'Y)$ where $Y=A-\mathbf{x}\mathbf{x}'$ To compute derivative use technique of differentials as in Magnus,Neudecker and here $$dY(\mathbf{x})=-2\mathbf{x} \mathbf{dx}'$$ $$dJ(Y)=\text{tr}(2Y'dY)$$ substitute definition of $dY$ and $Y$ into above, simplify to get $$dJ(\mathbf{x})=-4 \text{tr}((A-\mathbf{x}\mathbf{x}')'\mathbf{x}\mathbf{dx}')=4\mathbf{x}'(\mathbf{x}\mathbf{x}'-A)'\mathbf{dx}$$ The last expression is a canonical form from which the derivative can be identified as the expression before $\mathbf{dx}$, doing transpose and setting to $\mathbf{0}$ gives the equation above.","I want to find $\mathbf{x}$ that minimizes $\|A-\mathbf{x}\mathbf{x}'\|^2$ where $\|\cdot\|$ is Frobenius norm . Differentiating with respect to $\mathbf{x}$ and setting to $\mathbf{0}$, I get $$\mathbf{x}\mathbf{x}'\mathbf{x}=A \mathbf{x}$$ Any idea how to proceed further? (Per request, here's how I did the derivative) Let $J=\|A-\mathbf{x}\mathbf{x}'\|^2=\text{tr}(Y'Y)$ where $Y=A-\mathbf{x}\mathbf{x}'$ To compute derivative use technique of differentials as in Magnus,Neudecker and here $$dY(\mathbf{x})=-2\mathbf{x} \mathbf{dx}'$$ $$dJ(Y)=\text{tr}(2Y'dY)$$ substitute definition of $dY$ and $Y$ into above, simplify to get $$dJ(\mathbf{x})=-4 \text{tr}((A-\mathbf{x}\mathbf{x}')'\mathbf{x}\mathbf{dx}')=4\mathbf{x}'(\mathbf{x}\mathbf{x}'-A)'\mathbf{dx}$$ The last expression is a canonical form from which the derivative can be identified as the expression before $\mathbf{dx}$, doing transpose and setting to $\mathbf{0}$ gives the equation above.",,"['linear-algebra', 'matrices', 'derivatives', 'optimization', 'rank-1-matrices']"
28,Largest eigenvalue of a $A^T A$ matrix?,Largest eigenvalue of a  matrix?,A^T A,"I have a large real matrix A of size $40K\times 400K$, is there an efficient way to calculate the largest eigenvalue of $A^T A$ (size $400K\times 400K$)? Thanks.","I have a large real matrix A of size $40K\times 400K$, is there an efficient way to calculate the largest eigenvalue of $A^T A$ (size $400K\times 400K$)? Thanks.",,"['linear-algebra', 'matrices']"
29,how can a matrix vector product reduce to a scalar?,how can a matrix vector product reduce to a scalar?,,"I have an Excel spreadsheet with the following formula (paraphrased): =MMULT(  MMULT(vector_as_n_column_matrix, n_by_n_square_matrix)           ,TRANSPOSE(vector_as_n_column_matrix)       ) the matrix and vector contain floating point values.  Excel evaluates this formula as a scalar.  I don't recall matrix math working that way.  However, it's been over 30 years since I studies linear algebra. Does it make sense that the result should be a scalar?  Can you explain why in terms of linear algebra?","I have an Excel spreadsheet with the following formula (paraphrased): =MMULT(  MMULT(vector_as_n_column_matrix, n_by_n_square_matrix)           ,TRANSPOSE(vector_as_n_column_matrix)       ) the matrix and vector contain floating point values.  Excel evaluates this formula as a scalar.  I don't recall matrix math working that way.  However, it's been over 30 years since I studies linear algebra. Does it make sense that the result should be a scalar?  Can you explain why in terms of linear algebra?",,"['linear-algebra', 'numerical-linear-algebra']"
30,Find a convex combination of scalars given a point within them.,Find a convex combination of scalars given a point within them.,,"I've been banging my head on this one all day! I'm going to do my best to explain the problem, but bear with me. Given a set of numbers $S = \{X_1, X_2, \dots, X_n\}$ and a scalar $T$, where it is guaranteed that there is at least one member of $S$ that is less than $T$ and at least one member that's greater than $T$, I'm looking for an algorithm to create a Convex Combination of these scalars that equals $T$. For example, for the set $\{2,4\}$ and the scalar $3$, the answer is: $$.5 \cdot 2 + .5 \cdot 4 = 3.$$ I believe in many cases there are infinite infinitely many solutions. I'm looking for a generalized algorithm/formula to find these coefficients. Additionally, I would like for the coefficient weights to be distributed as evenly as possible (of course while still adding up to 1.) For instance, for the set $\{1,2,4\}$ and the scalar $3$, a technically valid solution would be the same as the first example but with the coefficient for $1$ assigned a weight of 0 - but it would be prefferable to assign a non-zero weight. I may not be thinking through this last part very clearly :)","I've been banging my head on this one all day! I'm going to do my best to explain the problem, but bear with me. Given a set of numbers $S = \{X_1, X_2, \dots, X_n\}$ and a scalar $T$, where it is guaranteed that there is at least one member of $S$ that is less than $T$ and at least one member that's greater than $T$, I'm looking for an algorithm to create a Convex Combination of these scalars that equals $T$. For example, for the set $\{2,4\}$ and the scalar $3$, the answer is: $$.5 \cdot 2 + .5 \cdot 4 = 3.$$ I believe in many cases there are infinite infinitely many solutions. I'm looking for a generalized algorithm/formula to find these coefficients. Additionally, I would like for the coefficient weights to be distributed as evenly as possible (of course while still adding up to 1.) For instance, for the set $\{1,2,4\}$ and the scalar $3$, a technically valid solution would be the same as the first example but with the coefficient for $1$ assigned a weight of 0 - but it would be prefferable to assign a non-zero weight. I may not be thinking through this last part very clearly :)",,"['linear-algebra', 'algorithms', 'convex-analysis', 'linear-programming']"
31,"If $Q$ is an orthogonal, then is $Q+\frac{1}{2}I$ invertible?","If  is an orthogonal, then is  invertible?",Q Q+\frac{1}{2}I,"If $Q$ is an orthogonal matrix, then the matrix has orthonormal columns. I asked this question to my friend and he says: Let $Q= -\frac{1}{2}I$, then it is orthogonal, and $Q+\frac{1}{2}I$ is zero, so not invertible. But I think if we set $Q=-\frac{1}{2}I$, then it is not an orthogonal matrix since it doesn't have unit length, right? Does anyone know whether it is true or false? Let me know the reason or counter example.","If $Q$ is an orthogonal matrix, then the matrix has orthonormal columns. I asked this question to my friend and he says: Let $Q= -\frac{1}{2}I$, then it is orthogonal, and $Q+\frac{1}{2}I$ is zero, so not invertible. But I think if we set $Q=-\frac{1}{2}I$, then it is not an orthogonal matrix since it doesn't have unit length, right? Does anyone know whether it is true or false? Let me know the reason or counter example.",,['linear-algebra']
32,Why does the standard BFGS update rule preserve positive definiteness?,Why does the standard BFGS update rule preserve positive definiteness?,,"My class has recently learnt the BFGS method for unconstrained optimisation. In this procedure, we have a rank-1 update to a positive definite matrix at each step. This is specified as: $H_{k+1} = H_k + \frac{\eta\eta^T}{\delta^T\eta}-\frac{H_k\delta\delta^TH_k^T}{\delta^TH_k\delta}$ $\forall \eta, \delta \in \mathbb{R}^n$. Show that for any symmetric positive definite matrix $H_k,$ we have that $H_{k+1}$ is positive definite so long as $\delta^T\eta > 0$. Don't assume anything about $H_k$ other than the fact that it is symmetric p.d.","My class has recently learnt the BFGS method for unconstrained optimisation. In this procedure, we have a rank-1 update to a positive definite matrix at each step. This is specified as: $H_{k+1} = H_k + \frac{\eta\eta^T}{\delta^T\eta}-\frac{H_k\delta\delta^TH_k^T}{\delta^TH_k\delta}$ $\forall \eta, \delta \in \mathbb{R}^n$. Show that for any symmetric positive definite matrix $H_k,$ we have that $H_{k+1}$ is positive definite so long as $\delta^T\eta > 0$. Don't assume anything about $H_k$ other than the fact that it is symmetric p.d.",,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'numerical-optimization']"
33,Intersection and Span,Intersection and Span,,Assume $S_{1}$ and $S_{2}$ are subsets of a vector space V. It has already been proved that span $(S1 \cap S2)$ $\subseteq$ span $(S_{1}) \cap$ span $(S_{2})$  There seem to be many cases where span $(S1 \cap S2)$ $=$ span $(S_{1}) \cap$ span $(S_{2})$ but not many where span $(S1 \cap S2)$ $\not=$ span $(S_{1}) \cap$ span $(S_{2})$. Please help me find an example. Thanks.,Assume $S_{1}$ and $S_{2}$ are subsets of a vector space V. It has already been proved that span $(S1 \cap S2)$ $\subseteq$ span $(S_{1}) \cap$ span $(S_{2})$  There seem to be many cases where span $(S1 \cap S2)$ $=$ span $(S_{1}) \cap$ span $(S_{2})$ but not many where span $(S1 \cap S2)$ $\not=$ span $(S_{1}) \cap$ span $(S_{2})$. Please help me find an example. Thanks.,,['linear-algebra']
34,Inequality involving norm and inner product,Inequality involving norm and inner product,,"I am stuck proving this trivial inequality: on a real inner product space, $(||x||+||y||)\frac{\langle x,y\rangle}{||x|| \cdot ||y||}\leq||x+y||$ I have tried to square both sides and use the Cauchy Schwarz inequality to get to $||x||\cdot||y||\leq\langle x,y\rangle$, which is obviously incorrect. Any help is much appreciated.","I am stuck proving this trivial inequality: on a real inner product space, $(||x||+||y||)\frac{\langle x,y\rangle}{||x|| \cdot ||y||}\leq||x+y||$ I have tried to square both sides and use the Cauchy Schwarz inequality to get to $||x||\cdot||y||\leq\langle x,y\rangle$, which is obviously incorrect. Any help is much appreciated.",,"['linear-algebra', 'inequality', 'normed-spaces', 'inner-products']"
35,Can someone resolve my confusion about uniqueness of diagonalization?,Can someone resolve my confusion about uniqueness of diagonalization?,,"I am a bit confused about diagonalization. I have $A$ which I know is diagonalizable. I want to find $P$ such that $A = P \Sigma P^{-1}$ where $\Sigma$ is diagonal. Under what circumstances is $P$ unique, if ever? If it is not unique, is it at least unique up to some operation?","I am a bit confused about diagonalization. I have $A$ which I know is diagonalizable. I want to find $P$ such that $A = P \Sigma P^{-1}$ where $\Sigma$ is diagonal. Under what circumstances is $P$ unique, if ever? If it is not unique, is it at least unique up to some operation?",,['linear-algebra']
36,Relationship Between Basis For Vector Space And Basis For Dual Space,Relationship Between Basis For Vector Space And Basis For Dual Space,,"There exist the famous theorem about a basis for dual space Let $\mathbb V$ be finite dimensional vector space over $F$ and $\mathcal{B} = \{\alpha_1, \ldots ,\alpha_n\}$ is basis for vector space $\mathbb V$ then  $\mathcal{B^*} = \{f_1, \ldots ,f_n\}$ is basis for dual space $\mathbb V^*$ such that $f_i(\alpha_{j})=\delta_{ij}$ Is the converse of this Theorem true, to explain more: Let  $\mathcal{B^*} = \{f_1, \ldots ,f_n\}$ be basis for dual space $\mathbb V^*$. Does there exist $\mathcal{B} = \{\alpha_1, \ldots ,\alpha_n\}$ such that $\mathcal{B}$ would be basis for vector space and  $f_i(\alpha_{j})=\delta_{ij}$?","There exist the famous theorem about a basis for dual space Let $\mathbb V$ be finite dimensional vector space over $F$ and $\mathcal{B} = \{\alpha_1, \ldots ,\alpha_n\}$ is basis for vector space $\mathbb V$ then  $\mathcal{B^*} = \{f_1, \ldots ,f_n\}$ is basis for dual space $\mathbb V^*$ such that $f_i(\alpha_{j})=\delta_{ij}$ Is the converse of this Theorem true, to explain more: Let  $\mathcal{B^*} = \{f_1, \ldots ,f_n\}$ be basis for dual space $\mathbb V^*$. Does there exist $\mathcal{B} = \{\alpha_1, \ldots ,\alpha_n\}$ such that $\mathcal{B}$ would be basis for vector space and  $f_i(\alpha_{j})=\delta_{ij}$?",,"['linear-algebra', 'vector-spaces']"
37,Block-diagonalizing an antisymmetric matrix,Block-diagonalizing an antisymmetric matrix,,I was wondering how to block-diagonalize a $10 \times 10$ antisymmetric matrix into block matrices along the diagonal. Can I just diagonalize each non-diagonal block? Thanks!,I was wondering how to block-diagonalize a antisymmetric matrix into block matrices along the diagonal. Can I just diagonalize each non-diagonal block? Thanks!,10 \times 10,"['linear-algebra', 'matrices', 'diagonalization', 'matrix-decomposition', 'block-matrices']"
38,Degrees of freedom vs. cardinality of tuples,Degrees of freedom vs. cardinality of tuples,,"Sometimes it is said that the number of DoF of a system means how many real numbers have to be used at least to describe the system. But we know from set theory that the cardinality of any tuple of reals is the same as the reals, so all the information that is in a 3-tuple of reals, can be represented in just one real number. Of course this representation may not be very useful as the properties of the system will be non-continuous functions of this variable, but still it is enough to describe the state of the system. So how could this confusing thing be resolved?","Sometimes it is said that the number of DoF of a system means how many real numbers have to be used at least to describe the system. But we know from set theory that the cardinality of any tuple of reals is the same as the reals, so all the information that is in a 3-tuple of reals, can be represented in just one real number. Of course this representation may not be very useful as the properties of the system will be non-continuous functions of this variable, but still it is enough to describe the state of the system. So how could this confusing thing be resolved?",,['linear-algebra']
39,Finding a fourth vector that makes a set a basis,Finding a fourth vector that makes a set a basis,,"The following vectors are linearly independent - $v1 = (1, 2, 0, 2)$ $v2 = (1,1,1,0)$ $v3 = (2,0,1,3)$ Find a fourth vector v4 so that the set { v1, v2, v3, v4 } is a basis fpr $\mathbb{R}^4$? I asked this question before here - Show vectors are linearly independent and finding a basis - and someone suggested a way of doing it. However I am wondering if there is a simpler way. I put the vector $\begin{bmatrix} 0 \\ 0 \\ 0 \\ x \end{bmatrix}$ as the fourth column in a matrix of these vectors, then row reduce. $\begin{bmatrix} 1 & 1 & 2 & 0 & | & 0 \\ 2 & 1 & 0 & 0 & | & 0 \\ 0 & 1 & 1 & 0 & | & 0 \\ 2 & 0 & 3 & x & | & 0 \end{bmatrix}$ By doing this I will be left with the fourth column looking like, for example, $(0, 0, 0, x-2)$. Then as long as x is not equal to 2, there will be pivots in each column and the vectors will be linearly independent? In the matrix above the fourth column ends up as $(0, 0, 0, x)$. So as long as x is not equal to 0 the vectors will be linearly independent? Edit: David Mitra's answer here - Show vectors are linearly independent and finding a basis - is the best way to do this imo.","The following vectors are linearly independent - $v1 = (1, 2, 0, 2)$ $v2 = (1,1,1,0)$ $v3 = (2,0,1,3)$ Find a fourth vector v4 so that the set { v1, v2, v3, v4 } is a basis fpr $\mathbb{R}^4$? I asked this question before here - Show vectors are linearly independent and finding a basis - and someone suggested a way of doing it. However I am wondering if there is a simpler way. I put the vector $\begin{bmatrix} 0 \\ 0 \\ 0 \\ x \end{bmatrix}$ as the fourth column in a matrix of these vectors, then row reduce. $\begin{bmatrix} 1 & 1 & 2 & 0 & | & 0 \\ 2 & 1 & 0 & 0 & | & 0 \\ 0 & 1 & 1 & 0 & | & 0 \\ 2 & 0 & 3 & x & | & 0 \end{bmatrix}$ By doing this I will be left with the fourth column looking like, for example, $(0, 0, 0, x-2)$. Then as long as x is not equal to 2, there will be pivots in each column and the vectors will be linearly independent? In the matrix above the fourth column ends up as $(0, 0, 0, x)$. So as long as x is not equal to 0 the vectors will be linearly independent? Edit: David Mitra's answer here - Show vectors are linearly independent and finding a basis - is the best way to do this imo.",,['linear-algebra']
40,rotating a rectangle via a rotation matrix,rotating a rectangle via a rotation matrix,,"I want to rotate a 2D rectangle using a rotation matrix. After the rotation, I want the points (x, y) of the rectangle to be: (625.49939, 632.40015) top left (636.49939, 632.40015) top right (636.49939, 672.40015) bottom right (625.49939, 672.40015) bottom left Before the rectangle is rotated, it appears 40 units wide and 11 units tall. After the rotation, it will appear 11 units wide and 40 units tall. I know that I have to rotate it 90 degrees, but how can I find exactly where to place the rectangle before the rotation so that it ends up in my desired position? The rectangle starts in position ??? so that it appears 40 units wide and 11 units tall The rectangle will be rotated 90 degrees clockwise (at its center point). The rectangle ends with the coordinates listed above. How do I find the starting coordinates?","I want to rotate a 2D rectangle using a rotation matrix. After the rotation, I want the points (x, y) of the rectangle to be: (625.49939, 632.40015) top left (636.49939, 632.40015) top right (636.49939, 672.40015) bottom right (625.49939, 672.40015) bottom left Before the rectangle is rotated, it appears 40 units wide and 11 units tall. After the rotation, it will appear 11 units wide and 40 units tall. I know that I have to rotate it 90 degrees, but how can I find exactly where to place the rectangle before the rotation so that it ends up in my desired position? The rectangle starts in position ??? so that it appears 40 units wide and 11 units tall The rectangle will be rotated 90 degrees clockwise (at its center point). The rectangle ends with the coordinates listed above. How do I find the starting coordinates?",,"['linear-algebra', 'matrices', 'transformation']"
41,Why is the Kronecker delta a base for the dual vector space?,Why is the Kronecker delta a base for the dual vector space?,,"The Kronecker delta can be defined like this: $\delta_{ij} = \begin{cases} 1, & \text{if } i = j   \\ 0, & \text{if } i \ne j \end{cases}$ The dual space is a vector space $V^*$ that can be defined to any $\mathbb{K}-$vector space $V$. Its elements are linear transformations $\Phi: V \rightarrow \mathbb{K}$. According to my script and Wikipedia a base to the dual space can be found quite easily. If $X = \{x_i\}_{i = 1, 2, ..., n}$ is a base of V, then $X^* = \{x_i^*\}_{i = 1, 2, ...,n}$ with $x_i^*(x_j) = \delta_{ij}$. Now I chose the $\mathbb{R}^1$-vector space with $\mathbb{K} = \mathbb{R}$ to get a really simple example. Then $X = \{1\}$ and $X^* = \{x_1^*\}$ with $x_1^* (x) =  \begin{cases} 1, & \text{if } x = 1 \\ 0, & \text{if } x \neq 1 \end{cases}$. So this should be a base for some linear transformations. But all linear transformations $\Phi : V \rightarrow W$ have to fulfil two conditions: $\forall x, y \in V: \Phi(x+y) = \Phi(x) + \Phi(y)$ $\forall a \in \mathbb{K}: \forall x \in V: \Phi(a \cdot x) = a \cdot \Phi(x)$ But $x_1^*(1 + -1) = x_1^*(0) = 0 \neq 1 = 1 + 0 = x_1^*(1) + x_1^*(-1)$. So $x_i^*$ is no element of the dual space but its base?!? Can you please tell me where I got something wrong?","The Kronecker delta can be defined like this: $\delta_{ij} = \begin{cases} 1, & \text{if } i = j   \\ 0, & \text{if } i \ne j \end{cases}$ The dual space is a vector space $V^*$ that can be defined to any $\mathbb{K}-$vector space $V$. Its elements are linear transformations $\Phi: V \rightarrow \mathbb{K}$. According to my script and Wikipedia a base to the dual space can be found quite easily. If $X = \{x_i\}_{i = 1, 2, ..., n}$ is a base of V, then $X^* = \{x_i^*\}_{i = 1, 2, ...,n}$ with $x_i^*(x_j) = \delta_{ij}$. Now I chose the $\mathbb{R}^1$-vector space with $\mathbb{K} = \mathbb{R}$ to get a really simple example. Then $X = \{1\}$ and $X^* = \{x_1^*\}$ with $x_1^* (x) =  \begin{cases} 1, & \text{if } x = 1 \\ 0, & \text{if } x \neq 1 \end{cases}$. So this should be a base for some linear transformations. But all linear transformations $\Phi : V \rightarrow W$ have to fulfil two conditions: $\forall x, y \in V: \Phi(x+y) = \Phi(x) + \Phi(y)$ $\forall a \in \mathbb{K}: \forall x \in V: \Phi(a \cdot x) = a \cdot \Phi(x)$ But $x_1^*(1 + -1) = x_1^*(0) = 0 \neq 1 = 1 + 0 = x_1^*(1) + x_1^*(-1)$. So $x_i^*$ is no element of the dual space but its base?!? Can you please tell me where I got something wrong?",,"['linear-algebra', 'vector-spaces']"
42,"Given ${u, v, w}$ is a basis for $\mathbb{R}^3$, how can I show that $\{u + v + w, v + w, w\}$ is also a basis?","Given  is a basis for , how can I show that  is also a basis?","{u, v, w} \mathbb{R}^3 \{u + v + w, v + w, w\}","Given ${u, v, w}$ is a basis for $\mathbb{R}^3$, how can I show that $\{u + v + w, v + w, w\}$ is also a basis? I solved a similar problem in $\mathbb{R}^2$ (or at least think I did :p). Given $\{u, v\}$ is a basis for $\mathbb{R}^2$, show that $\{u + v, au\}$ is also a basis. I used the definition of a span to say $\implies cu + dv =\langle x, y\rangle $ (for $c, d$ in $\mathbb{R}$) Let $c = d + a^2$ (for $a$ in $\mathbb{R}$) $\implies (d + a^2)u + dv = \langle x, y\rangle $ $\implies d(u + v) + a(au) = \langle x, y\rangle $ $\implies \mathrm{span}(u + v, au) = \mathbb{R}^2$ From there I also showed that this set was linearly independent (by starting with putting $c$ and $d$ equal to zero) and concluded that, as it had both properties in $\mathbb{R}^2$, it must be a basis. So I've been wracking my brains trying to find ways to manipulate coefficients to achieve the new basis for $\mathbb{R}^3$, but I can't come up with anything. I'd like to know if the method I employed for the $\mathbb{R}^2$ question is acceptable? Is this the only way to do it? Is there another method that I should be using? P.S I apologise for my imperfect formatting. Still learning. Somehow I kept collapsing all the spaces between symbols.","Given ${u, v, w}$ is a basis for $\mathbb{R}^3$, how can I show that $\{u + v + w, v + w, w\}$ is also a basis? I solved a similar problem in $\mathbb{R}^2$ (or at least think I did :p). Given $\{u, v\}$ is a basis for $\mathbb{R}^2$, show that $\{u + v, au\}$ is also a basis. I used the definition of a span to say $\implies cu + dv =\langle x, y\rangle $ (for $c, d$ in $\mathbb{R}$) Let $c = d + a^2$ (for $a$ in $\mathbb{R}$) $\implies (d + a^2)u + dv = \langle x, y\rangle $ $\implies d(u + v) + a(au) = \langle x, y\rangle $ $\implies \mathrm{span}(u + v, au) = \mathbb{R}^2$ From there I also showed that this set was linearly independent (by starting with putting $c$ and $d$ equal to zero) and concluded that, as it had both properties in $\mathbb{R}^2$, it must be a basis. So I've been wracking my brains trying to find ways to manipulate coefficients to achieve the new basis for $\mathbb{R}^3$, but I can't come up with anything. I'd like to know if the method I employed for the $\mathbb{R}^2$ question is acceptable? Is this the only way to do it? Is there another method that I should be using? P.S I apologise for my imperfect formatting. Still learning. Somehow I kept collapsing all the spaces between symbols.",,"['linear-algebra', 'vector-spaces']"
43,Calculating the exponential of a $4 \times 4$ matrix,Calculating the exponential of a  matrix,4 \times 4,"Find $e^{At}$ , where $$A = \begin{bmatrix} 1 & -1 & 1 &  0\\ 1 &  1 & 0 &  1\\ 0 &  0 & 1 & -1\\ 0 &  0 & 1 &  1\\ \end{bmatrix}$$ So, let me just find $e^{A}$ for now and I can generalize later. I notice right away that I can write $$A = \begin{bmatrix} B & I_{2} \\ 0_{22} & B \end{bmatrix}$$ where $$B = \begin{bmatrix} 1 & -1\\ 1 &  1\\ \end{bmatrix}$$ I'm sort of making up a method here and I hope it works.  Can someone tell me if this is correct? I write: $$A = \mathrm{diag}(B,B) + \begin{bmatrix}0_{22} & I_{2}\\ 0_{22} & 0_{22}\end{bmatrix}$$ Call $S = \mathrm{diag}(B,B)$ , and $N = \begin{bmatrix}0_{22} & I_{2}\\ 0_{22} & 0_{22}\end{bmatrix}$ . I note that $N^2$ is $0_{44}$ , so $$e^{N} = \frac{N^{0}}{0!} + \frac{N}{1!} + \frac{N^2}{2!} + \cdots = I_{4} + N + 0_{44} + \cdots = I_{4} + N$$ and that $e^{S} = \mathrm{diag}(e^{B}, e^{B})$ and compute: $$e^{A} = e^{S + N} = e^{S}e^{N} = \mathrm{diag}(e^{B}, e^{B})\cdot[I_{4}  + N]$$ This reduces the problem to finding $e^B$ , which is much easier. Is my logic correct?  I just started writing everything as a block matrix and proceeded as if nothing about the process of finding the exponential of a matrix would change. But I don't really know the theory behind this I'm just guessing how it would work.","Find , where So, let me just find for now and I can generalize later. I notice right away that I can write where I'm sort of making up a method here and I hope it works.  Can someone tell me if this is correct? I write: Call , and . I note that is , so and that and compute: This reduces the problem to finding , which is much easier. Is my logic correct?  I just started writing everything as a block matrix and proceeded as if nothing about the process of finding the exponential of a matrix would change. But I don't really know the theory behind this I'm just guessing how it would work.","e^{At} A = \begin{bmatrix} 1 & -1 & 1 &  0\\ 1 &  1 & 0 &  1\\ 0 &  0 & 1 & -1\\ 0 &  0 & 1 &  1\\ \end{bmatrix} e^{A} A = \begin{bmatrix} B & I_{2} \\ 0_{22} & B \end{bmatrix} B = \begin{bmatrix} 1 & -1\\ 1 &  1\\ \end{bmatrix} A = \mathrm{diag}(B,B) + \begin{bmatrix}0_{22} & I_{2}\\ 0_{22} & 0_{22}\end{bmatrix} S = \mathrm{diag}(B,B) N = \begin{bmatrix}0_{22} & I_{2}\\ 0_{22} & 0_{22}\end{bmatrix} N^2 0_{44} e^{N} = \frac{N^{0}}{0!} + \frac{N}{1!} + \frac{N^2}{2!} + \cdots = I_{4} + N + 0_{44} + \cdots = I_{4} + N e^{S} = \mathrm{diag}(e^{B}, e^{B}) e^{A} = e^{S + N} = e^{S}e^{N} = \mathrm{diag}(e^{B}, e^{B})\cdot[I_{4}  + N] e^B","['linear-algebra', 'matrices', 'solution-verification', 'block-matrices', 'matrix-exponential']"
44,"If a matrix multiplied by its transpose equals the original matrix, is it symmetric?","If a matrix multiplied by its transpose equals the original matrix, is it symmetric?",,"Here's the question: Prove: If A T A = A, then A is symmetric and A = A 2 I tried to solve this by using inference. Assume A is symmetric, prove A = A 2 If A is symmetric, then by definition A T A = A 2 . Since A T A = A, then A = A 2 . Assume A = A 2 , prove A is symmetric ?? I wasn't sure what to put for the second part to prove 1. Is an idempotent matrix by definition symmetric? It would make sense, but I couldn't find anything definitive. Am I going about this correctly, or should I take a different approach?","Here's the question: Prove: If A T A = A, then A is symmetric and A = A 2 I tried to solve this by using inference. Assume A is symmetric, prove A = A 2 If A is symmetric, then by definition A T A = A 2 . Since A T A = A, then A = A 2 . Assume A = A 2 , prove A is symmetric ?? I wasn't sure what to put for the second part to prove 1. Is an idempotent matrix by definition symmetric? It would make sense, but I couldn't find anything definitive. Am I going about this correctly, or should I take a different approach?",,['linear-algebra']
45,An ill-conditioned matrix,An ill-conditioned matrix,,"If C is an ill-conditioned matrix and I want to get the inverse, one way is to take a pseudo-inverse of some sort. Instead, is the following, which uses the (normal) inverse, also a way to deal with this? Instead of $$ C^{-1} $$ use, $$(C^{T} C)^{-1} C^{T}  $$ Thanks,","If C is an ill-conditioned matrix and I want to get the inverse, one way is to take a pseudo-inverse of some sort. Instead, is the following, which uses the (normal) inverse, also a way to deal with this? Instead of $$ C^{-1} $$ use, $$(C^{T} C)^{-1} C^{T}  $$ Thanks,",,"['linear-algebra', 'matrices', 'inverse']"
46,Is Wolfram Alpha Wrong? (LU Decomposition),Is Wolfram Alpha Wrong? (LU Decomposition),,"Ok, I've been working on a matrix solver for a while (no, its not going well, thanks for asking), and just discovered something that worrys me alot; The problem at hand is LU Decomposition of a square matrix, for example $\begin{matrix} 1&2&3\\ 2&-1&1\\ 3&4&-1\\ \end{matrix}$ WolframAlpha's answer is completely different from the example source Wolfram $\begin{matrix} 1&2&3\\ 3&-2&-10\\ 2&2.5&20\\ \end{matrix}$ Source $\begin{matrix} 1&2&3\\ 2&-5&-5\\ 3&0.4&-8\\ \end{matrix}$ What's going on?","Ok, I've been working on a matrix solver for a while (no, its not going well, thanks for asking), and just discovered something that worrys me alot; The problem at hand is LU Decomposition of a square matrix, for example $\begin{matrix} 1&2&3\\ 2&-1&1\\ 3&4&-1\\ \end{matrix}$ WolframAlpha's answer is completely different from the example source Wolfram $\begin{matrix} 1&2&3\\ 3&-2&-10\\ 2&2.5&20\\ \end{matrix}$ Source $\begin{matrix} 1&2&3\\ 2&-5&-5\\ 3&0.4&-8\\ \end{matrix}$ What's going on?",,"['linear-algebra', 'matrices', 'wolfram-alpha']"
47,Showing that matrix is invertible using eigenvalues,Showing that matrix is invertible using eigenvalues,,Let $A$ be matrix from the  vector space of square $N \times N$ matrices. With the inital information: $A^2-4A=4I$. How does one show that $A+I$ is invertible? (I need please a solution that involves eigenvalues) Thank you,Let $A$ be matrix from the  vector space of square $N \times N$ matrices. With the inital information: $A^2-4A=4I$. How does one show that $A+I$ is invertible? (I need please a solution that involves eigenvalues) Thank you,,[]
48,Prove that the Ky Fan norm satisfies the triangle inequality,Prove that the Ky Fan norm satisfies the triangle inequality,,How can one simply see that Ky Fan $k$ -norm satisfies the triangle inequality? (The Ky Fan $k$ -norm of a matrix is the sum of the $k$ largest singular values of the matrix),How can one simply see that Ky Fan -norm satisfies the triangle inequality? (The Ky Fan -norm of a matrix is the sum of the largest singular values of the matrix),k k k,"['linear-algebra', 'matrices', 'functional-analysis', 'eigenvalues-eigenvectors', 'normed-spaces']"
49,Decomposition of vector with respect to direct sum of vector subspaces,Decomposition of vector with respect to direct sum of vector subspaces,,"$U$ and $W$ are two subspaces of vector space $V$. If $U \oplus W = V$, then $\forall v \in V$, there exist two unique vectors $u \in U$ and $w \in W$ such that $v = u + w$. Is the reverse true? That is, if any vector can has such unique decomposition, do we have $U \oplus W = V$? Can the above statements be generalized to finite number of subspaces $U_1,...,U_n$ instead of just two $U$ and $W$? Thanks for your help!","$U$ and $W$ are two subspaces of vector space $V$. If $U \oplus W = V$, then $\forall v \in V$, there exist two unique vectors $u \in U$ and $w \in W$ such that $v = u + w$. Is the reverse true? That is, if any vector can has such unique decomposition, do we have $U \oplus W = V$? Can the above statements be generalized to finite number of subspaces $U_1,...,U_n$ instead of just two $U$ and $W$? Thanks for your help!",,['linear-algebra']
50,"No 3 vectors independent over $\mathbb{Z}$ in $\mathbb{Z}^2$, without AoC","No 3 vectors independent over  in , without AoC",\mathbb{Z} \mathbb{Z}^2,"Q: Are there three $\mathbb{Z}^2$ vectors independent over $\mathbb{Z}$ ? Context: This problem arise naturally when I'm characterizing possible sub-""latice"" in $\mathbb{Z}^2$ . Formally let $S$ be a set of points (""lattice points"") with integer coordinates, and a set of vectors $V = \{ v_1, v_2, ...\} \subseteq \mathbb{Z}^2$ so that for all $p \in S$ and $v \in V$ , $$p \pm v \in S.$$ i.e., is also a lattice point. We can make a hexagon-like lattice by choosing $(v_1, v_2, v_3) = ((2, 1), (2, -1), (0, 2))$ . Notice, then, $v_1 - v_2 = v_3$ , so $v_3$ is redundant in $V$ - so we can remove $v_3$ while keeping the set $S$ . So we ask: is the third vector always redundant, in the sense that $$ cv_3 = av_1+bv_2, a,b,c \in \mathbb{Z} \implies a=b=c=0?$$ A quick search of ""L.I. over $\mathbb{Z}$ "" reveals that in $\mathbb{Z}^n$ , linear independence over $\mathbb{Z}$ is equivalent to $\mathbb{R}$ . This overkills the question: if there were 3 independent vectors over $\mathbb{Z}$ , then they are too in $\mathbb{R}$ , a contradiction. However, the prove involves using the basis of $\mathbb{R}$ over $\mathbb{Q}$ to deal with the real coefficients, which is a implication of Axiom of Choice. Since my question is simple enough, I would like to see if there is a proof without AoC. Q: Is the negation of ""there exists three $\mathbb{Z}^2$ vectors independent over $\mathbb{Z}$ "" provable without AoC?","Q: Are there three vectors independent over ? Context: This problem arise naturally when I'm characterizing possible sub-""latice"" in . Formally let be a set of points (""lattice points"") with integer coordinates, and a set of vectors so that for all and , i.e., is also a lattice point. We can make a hexagon-like lattice by choosing . Notice, then, , so is redundant in - so we can remove while keeping the set . So we ask: is the third vector always redundant, in the sense that A quick search of ""L.I. over "" reveals that in , linear independence over is equivalent to . This overkills the question: if there were 3 independent vectors over , then they are too in , a contradiction. However, the prove involves using the basis of over to deal with the real coefficients, which is a implication of Axiom of Choice. Since my question is simple enough, I would like to see if there is a proof without AoC. Q: Is the negation of ""there exists three vectors independent over "" provable without AoC?","\mathbb{Z}^2 \mathbb{Z} \mathbb{Z}^2 S V = \{ v_1, v_2, ...\} \subseteq \mathbb{Z}^2 p \in S v \in V p \pm v \in S. (v_1, v_2, v_3) = ((2, 1), (2, -1), (0, 2)) v_1 - v_2 = v_3 v_3 V v_3 S  cv_3 = av_1+bv_2, a,b,c \in \mathbb{Z} \implies a=b=c=0? \mathbb{Z} \mathbb{Z}^n \mathbb{Z} \mathbb{R} \mathbb{Z} \mathbb{R} \mathbb{R} \mathbb{Q} \mathbb{Z}^2 \mathbb{Z}","['linear-algebra', 'set-theory', 'axiom-of-choice', 'integer-lattices']"
51,Stuck on system of equations,Stuck on system of equations,,"What are the solutions of this system of equations, where $x,y \in \mathbb{R}$ ? $\begin{cases} \frac{1}{x} + \frac{1}{2y}  = (x^2+3y^2)(3x^2+y^2)\\ \frac{1}{x} - \frac{1}{2y}  = 2(y^4-x^4) \end{cases}$ First I rewrote the equations as $\begin{cases} \frac{1}{x} + \frac{1}{2y}  = 3x^4+3y^4+10x^2y^2\\ \frac{1}{x} - \frac{1}{2y}  = 2(y-x)(y+x)(y^2+x^2) \end{cases}$ then $\begin{cases} \frac{1}{x} + \frac{1}{2y}  = 3(x^2+y^2)^2+4x^2y^2\\ \frac{1}{x} - \frac{1}{2y}  = 2(y-x)(y+x)(y^2+x^2) \end{cases}$ I tried to add the two equations and I got: $\frac{2}{x}=2(y-x)(y+x)(y^2+x^2)+3(x^2+y^2)^2+4x^2y^2$ we can rewrite this as $\frac{2}{x}=(y^2+x^2)[2(y-x)(y+x)+3(x^2+y^2)]+4x^2y^2$ , $\frac{2}{x}=(y^2+x^2)[2y^2-2x^2+3x^2+3y^2]+4x^2y^2$ $\frac{2}{x}=(y^2+x^2)(5y^2-x^2)+4x^2y^2$ Im stuck from here, how should I proceed? Or is there any trick to solving this system?","What are the solutions of this system of equations, where ? First I rewrote the equations as then I tried to add the two equations and I got: we can rewrite this as , Im stuck from here, how should I proceed? Or is there any trick to solving this system?","x,y \in \mathbb{R} \begin{cases}
\frac{1}{x} + \frac{1}{2y}  = (x^2+3y^2)(3x^2+y^2)\\
\frac{1}{x} - \frac{1}{2y}  = 2(y^4-x^4)
\end{cases} \begin{cases}
\frac{1}{x} + \frac{1}{2y}  = 3x^4+3y^4+10x^2y^2\\
\frac{1}{x} - \frac{1}{2y}  = 2(y-x)(y+x)(y^2+x^2)
\end{cases} \begin{cases}
\frac{1}{x} + \frac{1}{2y}  = 3(x^2+y^2)^2+4x^2y^2\\
\frac{1}{x} - \frac{1}{2y}  = 2(y-x)(y+x)(y^2+x^2)
\end{cases} \frac{2}{x}=2(y-x)(y+x)(y^2+x^2)+3(x^2+y^2)^2+4x^2y^2 \frac{2}{x}=(y^2+x^2)[2(y-x)(y+x)+3(x^2+y^2)]+4x^2y^2 \frac{2}{x}=(y^2+x^2)[2y^2-2x^2+3x^2+3y^2]+4x^2y^2 \frac{2}{x}=(y^2+x^2)(5y^2-x^2)+4x^2y^2","['linear-algebra', 'algebra-precalculus', 'systems-of-equations']"
52,A geometrical puzzle involving calculus,A geometrical puzzle involving calculus,,"Some time ago I stumbled across a problem from the Putnam Mathematical Competition. I could not find it, but I remember the text quite well. There are two vectors: a =(10, $y$ ) and b =( $x$ ,10), where $0 ≤ x ≤ 10$ , and $0 ≤ y ≤ 10$ . We have to compute the probability of these two vectors forming a parallelogram with an area A ≥ 50. The hint in the text is that the probability can be expressed as $$\ln(\sqrt a) + \frac{b}{c}$$ (where $a, b, c \in \mathbb{N}$ ; $b$ and $c$ are coprime and $a$ is as smaller as possible). Here is a visual representation for the problem : First, I calculated the area of the parallelogram: $A = ||\vec{a} \times \vec{b}|| = xy - 100$ , which implies that $0 ≤ A ≤ 100$ . We can write the inequality $A ≥ 50$ as $xy ≤ 50$ . If we assume that $y$ is a given (we could repeat this reasoning for $x$ ), then $x ≤ \frac{50}{y}$ . If $y ≤ 5$ then all $x$ values $≤ 10$ are acceptable; therefore the probability of $A ≥ 50$ is at least $\frac{1}{2}$ . When $y ≥ 5, x ≤ 50$ ; so when $y$ increases the portion of acceptable $x$ values decreases. I calculated $$\int_{5}^{10} \frac{50}{y} \cdot \frac{1}{10} \ dy$$ to count these values. The idea is that for every $y$ value greater than 5 the probability of $x$ values being less than $\frac{50}{y}$ is $\frac{\frac{50}{y}}{10}$ , where 10 is the segment of all possible $x$ values. The integration yields $5 \ln (2)$ . Using the law of total probability I wrote: $$P(A \geq 50) = P(y < 5) \cdot P(A \geq 50 | y < 5) + P(y \geq 5) \cdot P(A \geq 50 | y > 5)$$ $$P(A \geq 50) = \frac{1}{2} \cdot 1 + \frac{1}{2} \cdot (5 \ln(2)) \approx 2.23.$$ But why is the probability greater than 1?","Some time ago I stumbled across a problem from the Putnam Mathematical Competition. I could not find it, but I remember the text quite well. There are two vectors: a =(10, ) and b =( ,10), where , and . We have to compute the probability of these two vectors forming a parallelogram with an area A ≥ 50. The hint in the text is that the probability can be expressed as (where ; and are coprime and is as smaller as possible). Here is a visual representation for the problem : First, I calculated the area of the parallelogram: , which implies that . We can write the inequality as . If we assume that is a given (we could repeat this reasoning for ), then . If then all values are acceptable; therefore the probability of is at least . When ; so when increases the portion of acceptable values decreases. I calculated to count these values. The idea is that for every value greater than 5 the probability of values being less than is , where 10 is the segment of all possible values. The integration yields . Using the law of total probability I wrote: But why is the probability greater than 1?","y x 0 ≤ x ≤ 10 0 ≤ y ≤ 10 \ln(\sqrt a) + \frac{b}{c} a, b, c \in \mathbb{N} b c a A = ||\vec{a} \times \vec{b}|| = xy - 100 0 ≤ A ≤ 100 A ≥ 50 xy ≤ 50 y x x ≤ \frac{50}{y} y ≤ 5 x ≤ 10 A ≥ 50 \frac{1}{2} y ≥ 5, x ≤ 50 y x \int_{5}^{10} \frac{50}{y} \cdot \frac{1}{10} \ dy y x \frac{50}{y} \frac{\frac{50}{y}}{10} x 5 \ln (2) P(A \geq 50) = P(y < 5) \cdot P(A \geq 50 | y < 5) + P(y \geq 5) \cdot P(A \geq 50 | y > 5) P(A \geq 50) = \frac{1}{2} \cdot 1 + \frac{1}{2} \cdot (5 \ln(2)) \approx 2.23.","['calculus', 'linear-algebra', 'geometry', 'geometric-probability']"
53,Cauchy-Schwarz inequality with non-standard dot product,Cauchy-Schwarz inequality with non-standard dot product,,"I have the following problem: Let's have the scalar product in space $ℝ^2$ given by the expression: $$ <x, y> = 2x_1 y_1 + x_2 y_2 + x_1 y_2 + x_2 y_1  $$ For a defined dot product, formulate the precise definition of the Cauchy-Schwarz inequality. For a given dot product, define the projection matrix onto the span{a} of the line a defined by the vector $ v = (1, 0)^T$ . For task 1, I was not really sure how to express the norm inducted by the dot product; therefore, I am struggling with the RHS of the inequality: $$ |<x, y>| ≤ ||x|| \ ||y|| $$ More precisely: $$ 2x_1 y_1 + x_2 y_2 + x_1 y_2 + x_2 y_1 ≤ \lVert x \rVert \ \lVert y \rVert $$ Should I express the euclidian norm in the way of $ \sqrt{<x, x>} $ , then get rid of the root and express it in some more suitable way? For task 2, I was able to determine the projection onto the line generated by span(a): $$ x_u = \frac{ax^T}{a^Ta} $$ from that, we get after substituting into the defined dot product the following equation: $$ \frac{<x, v>}{\lVert v \rVert} v = \frac{(2x_1 + x_2)}{\sqrt{2}} . (1, 0)^T $$ I believe this should be it, but now I need to express it using the projection matrix. I believe it by multiplying given vectors; I got the following matrix: $$ \begin{bmatrix} 2 & 0 \\ 1 & 0  \end{bmatrix} $$ But somehow, It does not seem right. Could anyone help me out? Thanks in advance.","I have the following problem: Let's have the scalar product in space given by the expression: For a defined dot product, formulate the precise definition of the Cauchy-Schwarz inequality. For a given dot product, define the projection matrix onto the span{a} of the line a defined by the vector . For task 1, I was not really sure how to express the norm inducted by the dot product; therefore, I am struggling with the RHS of the inequality: More precisely: Should I express the euclidian norm in the way of , then get rid of the root and express it in some more suitable way? For task 2, I was able to determine the projection onto the line generated by span(a): from that, we get after substituting into the defined dot product the following equation: I believe this should be it, but now I need to express it using the projection matrix. I believe it by multiplying given vectors; I got the following matrix: But somehow, It does not seem right. Could anyone help me out? Thanks in advance.","ℝ^2  <x, y> = 2x_1 y_1 + x_2 y_2 + x_1 y_2 + x_2 y_1    v = (1, 0)^T  |<x, y>| ≤ ||x|| \ ||y||   2x_1 y_1 + x_2 y_2 + x_1 y_2 + x_2 y_1 ≤ \lVert x \rVert \ \lVert y \rVert   \sqrt{<x, x>}   x_u = \frac{ax^T}{a^Ta}   \frac{<x, v>}{\lVert v \rVert} v = \frac{(2x_1 + x_2)}{\sqrt{2}} . (1, 0)^T   \begin{bmatrix}
2 & 0 \\
1 & 0 
\end{bmatrix} ","['linear-algebra', 'matrices', 'vector-spaces', 'inner-products', 'projection']"
54,How is Siegel's lemma applied in number theory?,How is Siegel's lemma applied in number theory?,,"In the Wikipedia page , Siegel's lemma is stated as follows: Consider the system $$ \begin{cases} \sum_{i=1}^Na_{1i}X_i=0\\ \vdots\\ \sum_{i=1}^Na_{Mi}X_i=0 \end{cases}, $$ where the coefficients $a_{11}, a_{12}, \dots, a_{MN}$ are integers between $-B$ and $B$ . The system has a nonzero integer solution $(X_1, X_2, \dots,  X_N)$ such that $\max_i \left|X_i\right| \le (NB)^{M/(N-M)}$ . In my understanding, Siegel's lemma is of great importance in Diophantine approximation. However, I haven't been able to find an example of a problem where this formulation of the lemma has been applied to problems in Diophantine approximation. How is this applied in number theory?","In the Wikipedia page , Siegel's lemma is stated as follows: Consider the system where the coefficients are integers between and . The system has a nonzero integer solution such that . In my understanding, Siegel's lemma is of great importance in Diophantine approximation. However, I haven't been able to find an example of a problem where this formulation of the lemma has been applied to problems in Diophantine approximation. How is this applied in number theory?"," \begin{cases} \sum_{i=1}^Na_{1i}X_i=0\\
\vdots\\ \sum_{i=1}^Na_{Mi}X_i=0 \end{cases},  a_{11}, a_{12}, \dots, a_{MN} -B B (X_1, X_2, \dots,
 X_N) \max_i \left|X_i\right| \le (NB)^{M/(N-M)}","['linear-algebra', 'number-theory', 'transcendental-numbers', 'diophantine-approximation']"
55,Rotation matrix from given axis,Rotation matrix from given axis,,"Let $T\colon\mathbb{R^3}\to\mathbb{R^3}$ denotes linear transformation which rotates by $\frac{\pi}{3}$ counter-clockwise along the vector $u=(1,1,1)$ . If $T(0,1,0)=(a,b,c)$ , Find $3a^2+b^2+c^2$ . My Attempt Consider a plane which contains a point $(0,1,0)$ and uses $u$ as normal vector. Then I got : $x+y+z=1$ . Since $T$ is rotation, $T(0,1,0)=(a,b,c)$ must be on the same plane $x+y+z=1$ . This means $a+b+c=1$ . Again, $T$ preserves norm of vector : $1=|(0,1,0)|=|T(a,b,c)|=a^2+b^2+c^2$ . So, I got two equations : $$a+b+c=1 \\ a^2+b^2+c^2=1$$ I want to Find $a, b, c$ without finding exact form of $T$ but I need one more equation about $a,b,c$ to solve this system. Is there any property of rotation which can give me one more equation about $a,b,c$ ?","Let denotes linear transformation which rotates by counter-clockwise along the vector . If , Find . My Attempt Consider a plane which contains a point and uses as normal vector. Then I got : . Since is rotation, must be on the same plane . This means . Again, preserves norm of vector : . So, I got two equations : I want to Find without finding exact form of but I need one more equation about to solve this system. Is there any property of rotation which can give me one more equation about ?","T\colon\mathbb{R^3}\to\mathbb{R^3} \frac{\pi}{3} u=(1,1,1) T(0,1,0)=(a,b,c) 3a^2+b^2+c^2 (0,1,0) u x+y+z=1 T T(0,1,0)=(a,b,c) x+y+z=1 a+b+c=1 T 1=|(0,1,0)|=|T(a,b,c)|=a^2+b^2+c^2 a+b+c=1 \\ a^2+b^2+c^2=1 a, b, c T a,b,c a,b,c","['linear-algebra', 'rotations']"
56,Is there always a general formula for the linear transformation that maps polynomials to one of their antiderivatives?,Is there always a general formula for the linear transformation that maps polynomials to one of their antiderivatives?,,"In Friedberg's Linear Algebra this example of a linear transformation comes up: $T:P_2(R) \rightarrow P_3(R)$ , such that $T(p)=q$ , with $q(x)=\int_0^x{p}$ , where $P_n(R)$ is the vector space of all polynomials over the reals of degree up to $n$ . At first my intuition suggested that, given any of a polynomial's antiderivatives, there should exist a linear transformation that maps that polynomial to that specific antiderivative, and that that would be accomplished by suitably varying the integral's lower bound in the preceding expression. However, consider the polynomial $f$ such that $f(x)=x$ , and take its integral for an arbitrary lower bound $C$ : $$g_C(x)=\int_C^x f=\frac{x^2}{2}-\frac{C^2}{2}$$ In other words, $g_C$ is of the form $g_C(x)=\frac{x^2}{2}-D$ , for $D\geq0$ . So no matter what lower bound we choose, we'll never obtain an antiderivative for which $g_c(0)>0$ , which seems quite surprising. Despite this, nothing stops us from defining a linear transformation $U$ such that $U(f)=h$ , with $f(x)=x$ and $h(x)=\frac{x^2}{2}+1$ , and analogously for the other basis vectors, whichever basis we might be using. I wasn't able to, however, find a general expression written in terms of basic operations and integrals that could express what this linear transformation does, as can be done for $T$ , which, again, seems to me quite surprising. Put informally, why should certain antiderivatives be ""privileged"" with respect to others in the realm of linear transformations and their expressions? Or am I just not seeing what this general expression should be? Is there one?","In Friedberg's Linear Algebra this example of a linear transformation comes up: , such that , with , where is the vector space of all polynomials over the reals of degree up to . At first my intuition suggested that, given any of a polynomial's antiderivatives, there should exist a linear transformation that maps that polynomial to that specific antiderivative, and that that would be accomplished by suitably varying the integral's lower bound in the preceding expression. However, consider the polynomial such that , and take its integral for an arbitrary lower bound : In other words, is of the form , for . So no matter what lower bound we choose, we'll never obtain an antiderivative for which , which seems quite surprising. Despite this, nothing stops us from defining a linear transformation such that , with and , and analogously for the other basis vectors, whichever basis we might be using. I wasn't able to, however, find a general expression written in terms of basic operations and integrals that could express what this linear transformation does, as can be done for , which, again, seems to me quite surprising. Put informally, why should certain antiderivatives be ""privileged"" with respect to others in the realm of linear transformations and their expressions? Or am I just not seeing what this general expression should be? Is there one?",T:P_2(R) \rightarrow P_3(R) T(p)=q q(x)=\int_0^x{p} P_n(R) n f f(x)=x C g_C(x)=\int_C^x f=\frac{x^2}{2}-\frac{C^2}{2} g_C g_C(x)=\frac{x^2}{2}-D D\geq0 g_c(0)>0 U U(f)=h f(x)=x h(x)=\frac{x^2}{2}+1 T,"['linear-algebra', 'integration', 'linear-transformations']"
57,Artin's Algebra 4.4.8 [duplicate],Artin's Algebra 4.4.8 [duplicate],,"This question already has answers here : Eigenvectors of a Homothety operator (2 answers) Closed 12 months ago . Q. Let $T$ be a linear operator on a finite-dimensional vector space for which every nonzero vector is an eigenvector. Prove that $T$ is multiplication by a scalar. I did not find this question in old posts. Approach:- First suppose $\operatorname{dim}(V)=1$ . Then there is a non-zero vector $v \in V$ such that $V=\{c v: c \in F\}$ . By hypothesis we have $\lambda \in F$ such that $T(v)=\lambda v$ , so that $T(c v)=c T(v)=c(\lambda v)=\lambda(c v)$ i.e. $T=\lambda I$ where $I: V \rightarrow V$ is the identity operator. So in this case we are done. Next suppose, $\operatorname{dim}(V) \geq 2$ . Then let $u, w$ be two linearly independent vectors of $V$ . Now we have $\alpha, \beta \in F$ such that $T(u)=\alpha u$ and $T(w)=\beta w$ . Now note that $u+w \neq 0$ as $\{u, w\}$ is a linearly independent set. Hence there is $\gamma \in F$ such that $T(u+w)=\gamma(u+w)$ . So that $\alpha u+\beta w=T(u)+T(w)=T(u+$ $w)=\gamma(u+w)$ . Hence $\alpha u+\beta w=\gamma u+\gamma w$ i.e. $(\alpha-\gamma) u=(\gamma-\beta) w$ . Since $\{u, w\}$ is a linearly independent set we have $\alpha-\gamma=0=\gamma-\beta$ i.e. $\alpha=\beta$ . What we observe is that for every vector $w$ which is linearly independent with $u$ we have $T(w)=\alpha u$ where $\alpha \in F$ is such that $T(u)=\alpha u$ . Now every linearly independent subset can be extended to a basis of $V$ . So let $\left\{v_{1}, \ldots, v_{n}\right\}$ be a basis of $V$ with $u=v_{1}$ , then for any $x \in V$ with representation $x=c_{1} v_{1}+c_{2} v_{2}+\ldots+c_{n} v_{n}$ with $c_{1}, \ldots, c_{n} \in F$ we have $T(x)=c_{1} T\left(v_{1}\right)+$ $c_{2} T\left(v_{2}\right)+\ldots+c_{n} T\left(v_{n}\right)=c_{1}\left(\alpha v_{1}\right)+c_{2}\left(\alpha v_{2}\right)+\ldots+\left(c_{n} \alpha v_{n}\right)=\alpha\left(c_{1} v_{1}+\ldots+c_{n} v_{n}\right)=\alpha x$ i.e. $T=\alpha I$ . The case when $\operatorname{dim}(V)=0$ is trivial as in this case $V=\{0\}$ so that $T=0=0 I$ , where $I: V \rightarrow V$ is the identity operator. Please cheak this.. Also you can give your approach. Thank you...","This question already has answers here : Eigenvectors of a Homothety operator (2 answers) Closed 12 months ago . Q. Let be a linear operator on a finite-dimensional vector space for which every nonzero vector is an eigenvector. Prove that is multiplication by a scalar. I did not find this question in old posts. Approach:- First suppose . Then there is a non-zero vector such that . By hypothesis we have such that , so that i.e. where is the identity operator. So in this case we are done. Next suppose, . Then let be two linearly independent vectors of . Now we have such that and . Now note that as is a linearly independent set. Hence there is such that . So that . Hence i.e. . Since is a linearly independent set we have i.e. . What we observe is that for every vector which is linearly independent with we have where is such that . Now every linearly independent subset can be extended to a basis of . So let be a basis of with , then for any with representation with we have i.e. . The case when is trivial as in this case so that , where is the identity operator. Please cheak this.. Also you can give your approach. Thank you...","T T \operatorname{dim}(V)=1 v \in V V=\{c v: c \in F\} \lambda \in F T(v)=\lambda v T(c v)=c T(v)=c(\lambda v)=\lambda(c v) T=\lambda I I: V \rightarrow V \operatorname{dim}(V) \geq 2 u, w V \alpha, \beta \in F T(u)=\alpha u T(w)=\beta w u+w \neq 0 \{u, w\} \gamma \in F T(u+w)=\gamma(u+w) \alpha u+\beta w=T(u)+T(w)=T(u+ w)=\gamma(u+w) \alpha u+\beta w=\gamma u+\gamma w (\alpha-\gamma) u=(\gamma-\beta) w \{u, w\} \alpha-\gamma=0=\gamma-\beta \alpha=\beta w u T(w)=\alpha u \alpha \in F T(u)=\alpha u V \left\{v_{1}, \ldots, v_{n}\right\} V u=v_{1} x \in V x=c_{1} v_{1}+c_{2} v_{2}+\ldots+c_{n} v_{n} c_{1}, \ldots, c_{n} \in F T(x)=c_{1} T\left(v_{1}\right)+ c_{2} T\left(v_{2}\right)+\ldots+c_{n} T\left(v_{n}\right)=c_{1}\left(\alpha v_{1}\right)+c_{2}\left(\alpha v_{2}\right)+\ldots+\left(c_{n} \alpha v_{n}\right)=\alpha\left(c_{1} v_{1}+\ldots+c_{n} v_{n}\right)=\alpha x T=\alpha I \operatorname{dim}(V)=0 V=\{0\} T=0=0 I I: V \rightarrow V",['linear-algebra']
58,In what sense $\bigwedge(V\oplus W) \simeq \bigwedge V \otimes \bigwedge W$?,In what sense ?,\bigwedge(V\oplus W) \simeq \bigwedge V \otimes \bigwedge W,"According to the Wikipedia article on the exterior algebra, there is a natural isomorphism $$\bigwedge(V\oplus W) \simeq \bigwedge V \otimes \bigwedge W,$$ where $V,W$ is a finite-dimensional $\Bbbk$ -vector space. My question is, in what sense they are isomorphic? Isomorphic as a vector space? (this is trivial.) Or isomorphic as a $\Bbbk$ -algebra? I hope that they are isomorphic as a $\Bbbk$ -algebra, but I think this is not true due to the anticommutative nature. In particular, for $v_1,v_2\in V$ and $w_1,w_2\in W$ , $$(v_1\otimes w_1)(v_2\otimes w_2) = (v_1\wedge v_2) \otimes (w_1\wedge w_2)$$ in RHS but it is not true in LHS that $$v_1\wedge w_1 \wedge v_2 \wedge w_2 = v_1\wedge v_2\wedge w_1\wedge w_2.$$ (However, for symmetric algebra, analogous relation is true and the isomorphism is in the $\Bbbk$ -algebra sense.) Is my reasoning correct?","According to the Wikipedia article on the exterior algebra, there is a natural isomorphism where is a finite-dimensional -vector space. My question is, in what sense they are isomorphic? Isomorphic as a vector space? (this is trivial.) Or isomorphic as a -algebra? I hope that they are isomorphic as a -algebra, but I think this is not true due to the anticommutative nature. In particular, for and , in RHS but it is not true in LHS that (However, for symmetric algebra, analogous relation is true and the isomorphism is in the -algebra sense.) Is my reasoning correct?","\bigwedge(V\oplus W) \simeq \bigwedge V \otimes \bigwedge W, V,W \Bbbk \Bbbk \Bbbk v_1,v_2\in V w_1,w_2\in W (v_1\otimes w_1)(v_2\otimes w_2) = (v_1\wedge v_2) \otimes (w_1\wedge w_2) v_1\wedge w_1 \wedge v_2 \wedge w_2 = v_1\wedge v_2\wedge w_1\wedge w_2. \Bbbk","['linear-algebra', 'exterior-algebra']"
59,Dimension of maximal totally isotropic space over finite field with standard bilinear form,Dimension of maximal totally isotropic space over finite field with standard bilinear form,,"Let $p$ be a prime and consider the vector space $\mathbb{F}_p^n$ , equipped with the standard bilinear form $\langle x, y \rangle = \sum_{i=1}^n x_iy_i$ . A subspace $U$ is said to be totally isotropic if $\langle u, v \rangle = 0$ for every two (not necessarily distinct) $u,v \in U$ . What can we say about the dimension of a maximal totally isotropic subspace? A very well known upper bound is $\dim U \leq \lfloor \frac{n}{2} \rfloor$ and it follows by $\dim U + \dim U^{\perp} = n$ and $U \subseteq U^{\perp}$ . But how really is this bound attainable for $p>2$ ? For $p=2$ there is an easy example - take the subspace with basis $(1,1,0,\ldots,0)$ , $(0,0,1,1,0,\ldots,0)$ , $(0,0,0,0,1,1,0,\ldots,0)$ , etc. If you mimic this for other $p$ (by taking chunks of $p$ consecutive $1$ -s) the result would be of dimension $\lfloor \frac{n}{p} \rfloor$ . So is the truth closer to $\lfloor \frac{n}{p} \rfloor$ or to $\lfloor \frac{n}{2} \rfloor$ ? If it is too hard to answer for all $p$ , perhaps focusing on $p=3$ only could give good insight. Any help appreciated!","Let be a prime and consider the vector space , equipped with the standard bilinear form . A subspace is said to be totally isotropic if for every two (not necessarily distinct) . What can we say about the dimension of a maximal totally isotropic subspace? A very well known upper bound is and it follows by and . But how really is this bound attainable for ? For there is an easy example - take the subspace with basis , , , etc. If you mimic this for other (by taking chunks of consecutive -s) the result would be of dimension . So is the truth closer to or to ? If it is too hard to answer for all , perhaps focusing on only could give good insight. Any help appreciated!","p \mathbb{F}_p^n \langle x, y \rangle = \sum_{i=1}^n x_iy_i U \langle u, v \rangle = 0 u,v \in U \dim U \leq \lfloor \frac{n}{2} \rfloor \dim U + \dim U^{\perp} = n U \subseteq U^{\perp} p>2 p=2 (1,1,0,\ldots,0) (0,0,1,1,0,\ldots,0) (0,0,0,0,1,1,0,\ldots,0) p p 1 \lfloor \frac{n}{p} \rfloor \lfloor \frac{n}{p} \rfloor \lfloor \frac{n}{2} \rfloor p p=3","['linear-algebra', 'finite-fields', 'orthogonality', 'bilinear-form']"
60,"Let $A_1,A_2\in L(V,V)$, $A_i(V)=V$ and $\dim\ker A_i=n_i\in\mathbf{N}$. Is $\dim\ker (A_2A_1)=n_1+n_2$ even if $\dim V=\infty$?","Let ,  and . Is  even if ?","A_1,A_2\in L(V,V) A_i(V)=V \dim\ker A_i=n_i\in\mathbf{N} \dim\ker (A_2A_1)=n_1+n_2 \dim V=\infty","Let $V$ be a vector space that is not necessarly finite dimensional. Consider two surjective linear operators $A_1,A_2\in L(V,V)$ with finite-dimensional kernel: $A_i(V)=V$ and $\dim\ker A_i=n_i\in\mathbf{N}$ . Can it be proven that $\dim\ker (A_2A_1)=n_1+n_2$ ? In words: Is the dimension of the kernel of the composition of surjective linear operators with finite dimensional kernels the sum of the dimensions of the kernels? Looking at this formula, one might think that the formula $\dim\ker A_2A_1=n_1+_2$ is valid, but I have looked at the answers and they are based on the rank nullity theorem, which I can't use, since $V$ is not finite dimensional. Motivation: I want to prove that the solution space of homogeneous linear differential equations of order $n$ is $n$ -dimensional by writing the $n$ -th order differential operator as composition of first-order differential order with one-dimensional kernel. Actually, the answer to this question completes my proof .","Let be a vector space that is not necessarly finite dimensional. Consider two surjective linear operators with finite-dimensional kernel: and . Can it be proven that ? In words: Is the dimension of the kernel of the composition of surjective linear operators with finite dimensional kernels the sum of the dimensions of the kernels? Looking at this formula, one might think that the formula is valid, but I have looked at the answers and they are based on the rank nullity theorem, which I can't use, since is not finite dimensional. Motivation: I want to prove that the solution space of homogeneous linear differential equations of order is -dimensional by writing the -th order differential operator as composition of first-order differential order with one-dimensional kernel. Actually, the answer to this question completes my proof .","V A_1,A_2\in L(V,V) A_i(V)=V \dim\ker A_i=n_i\in\mathbf{N} \dim\ker (A_2A_1)=n_1+n_2 \dim\ker A_2A_1=n_1+_2 V n n n","['linear-algebra', 'operator-theory']"
61,Determinant and inverse matrix calculation of a special matrix [duplicate],Determinant and inverse matrix calculation of a special matrix [duplicate],,This question already has answers here : Determinant of a Toeplitz matrix (2 answers) Closed 3 years ago . Is there any smart way of calculating the determinant of this kind matrix? \begin{pmatrix} 1 & 2 & 3 & \cdots &  n \\ 2 & 1 & 2 & \cdots & n-1 \\  3 & 2 & 1 & \cdots & n-2 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ n & n-1 & n-2 & \cdots & 1 \end{pmatrix} I encountered this in a problem for the case $n=4$ . I need to find the inverse matrix. I doubt the idea of this problem is to calculate all the cofactors and then the inverse matrix in the usual way. I see the pattern here and some recursive relations... but I am not sure if this helps for calculating the determinant of the existing matrix.,This question already has answers here : Determinant of a Toeplitz matrix (2 answers) Closed 3 years ago . Is there any smart way of calculating the determinant of this kind matrix? I encountered this in a problem for the case . I need to find the inverse matrix. I doubt the idea of this problem is to calculate all the cofactors and then the inverse matrix in the usual way. I see the pattern here and some recursive relations... but I am not sure if this helps for calculating the determinant of the existing matrix.,"\begin{pmatrix}
1 & 2 & 3 & \cdots &  n \\
2 & 1 & 2 & \cdots & n-1 \\ 
3 & 2 & 1 & \cdots & n-2 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
n & n-1 & n-2 & \cdots & 1 \end{pmatrix} n=4","['linear-algebra', 'matrices', 'determinant']"
62,Is $V= W\oplus W^{\perp}$ always?,Is  always?,V= W\oplus W^{\perp},"Is $V= W\oplus W^{\perp}$ always? Here, $W$ is a subspace of the vector space $V$ . I think this holds if $V$ is finite-dimensional, but what if it is not? Could someone help me prove $V= W\oplus W^{\perp}$ in the infinite-dimensional case, or produce a counterexample? I'm really not sure where to start. The thing is, I am not able to see how moving from finite dimensions to infinite dimensions changes anything. Shouldn't it be the case that a vector lies in $W$ or $W^\perp$ ? Where else can it lie? P.S. This isn't part of a homework exercise or something, it just came to my mind. I'm taking a class in Linear Algebra right now that deals mostly with finite-dimensional vector spaces only, so I don't know where to start in this case.","Is always? Here, is a subspace of the vector space . I think this holds if is finite-dimensional, but what if it is not? Could someone help me prove in the infinite-dimensional case, or produce a counterexample? I'm really not sure where to start. The thing is, I am not able to see how moving from finite dimensions to infinite dimensions changes anything. Shouldn't it be the case that a vector lies in or ? Where else can it lie? P.S. This isn't part of a homework exercise or something, it just came to my mind. I'm taking a class in Linear Algebra right now that deals mostly with finite-dimensional vector spaces only, so I don't know where to start in this case.",V= W\oplus W^{\perp} W V V V= W\oplus W^{\perp} W W^\perp,"['linear-algebra', 'vector-spaces']"
63,Rank preservation of Hankel matrix by adding constrained sample,Rank preservation of Hankel matrix by adding constrained sample,,"Let some $x_i \in \mathbb{R}$ for every $i$ such that the Hankel matrix $$H_0=\begin{bmatrix} x_0 & x_1 & x_2 & x_3\\ x_1 & x_2 & x_3 & x_4\\ x_2 & x_3 & x_4 & x_5 \end{bmatrix} $$ is full rank, with the rank equal to 3. Show that there exists some $y\in [-1,1]$ such that the following Hankel matrix is full rank too $$H_1=\begin{bmatrix}  x_1 & x_2 & x_3 & x_4\\  x_2 & x_3 & x_4 & x_5\\  x_3 & x_4 & x_5 & y \end{bmatrix} $$ The matrix $H_1$ is obtained by deleting the first column of $H_0$ and adding a new column as $[x_4 \ x_5 \ y]^\top$ . I tried to solve this by showing that there exists some $a_0,a_1,a_2,a_3\in \mathbb{R}$ with $a_0\neq 0$ and some $y_0\in [-1,1]$ such that \begin{equation} \begin{bmatrix} x_4\\ x_5\\ y_0 \end{bmatrix}= \begin{bmatrix} x_0 & x_1 & x_2 & x_3\\ x_1 & x_2 & x_3 & x_4\\ x_2 & x_3 & x_4 & x_5 \end{bmatrix} \begin{bmatrix} a_0\\ a_1 \\ a_2 \\ a_3 \end{bmatrix} \end{equation} But still couldn't come up with a formal proof for it. Do you have any suggestion?","Let some for every such that the Hankel matrix is full rank, with the rank equal to 3. Show that there exists some such that the following Hankel matrix is full rank too The matrix is obtained by deleting the first column of and adding a new column as . I tried to solve this by showing that there exists some with and some such that But still couldn't come up with a formal proof for it. Do you have any suggestion?","x_i \in \mathbb{R} i H_0=\begin{bmatrix} x_0 & x_1 & x_2 & x_3\\
x_1 & x_2 & x_3 & x_4\\
x_2 & x_3 & x_4 & x_5
\end{bmatrix}  y\in [-1,1] H_1=\begin{bmatrix}  x_1 & x_2 & x_3 & x_4\\
 x_2 & x_3 & x_4 & x_5\\
 x_3 & x_4 & x_5 & y
\end{bmatrix}  H_1 H_0 [x_4 \ x_5 \ y]^\top a_0,a_1,a_2,a_3\in \mathbb{R} a_0\neq 0 y_0\in [-1,1] \begin{equation}
\begin{bmatrix}
x_4\\ x_5\\ y_0
\end{bmatrix}= \begin{bmatrix} x_0 & x_1 & x_2 & x_3\\
x_1 & x_2 & x_3 & x_4\\
x_2 & x_3 & x_4 & x_5
\end{bmatrix} \begin{bmatrix}
a_0\\ a_1 \\ a_2 \\ a_3
\end{bmatrix}
\end{equation}","['linear-algebra', 'matrices', 'matrix-rank', 'hankel-matrices']"
64,Proof with invertible matrices,Proof with invertible matrices,,"Let $A,B,C$ be matrix of the same size, and suppose A is invertible, Prove that $(A-B)C=BA^{-1}$ then $C(A-B)=A^{-1}B$ I tried to prove it as following. $(A-B)C=BA^{-1}$ implies $AC-BC=BA^{-1}$ so $ACA-BCA=B$ taking $A^{-1}$ $CA-A^{-1}BCA=A^{-1}B$ Any hint will be appreciated","Let be matrix of the same size, and suppose A is invertible, Prove that then I tried to prove it as following. implies so taking Any hint will be appreciated","A,B,C (A-B)C=BA^{-1} C(A-B)=A^{-1}B (A-B)C=BA^{-1} AC-BC=BA^{-1} ACA-BCA=B A^{-1} CA-A^{-1}BCA=A^{-1}B","['linear-algebra', 'matrices']"
65,Identity relating coefficients of degrees $0$ and $1$ from characteristic polynomials,Identity relating coefficients of degrees  and  from characteristic polynomials,0 1,"Let $A$ be a square matrix each of whose columns has sum $1$ . Let $B$ be the matrix obtained by replacing the lowest row in $A-I$ with a row of ones. Can anybody show (or find a counterexample) that $-\det(B)$ is exactly the degree $1$ coefficient in the characteristic polynomial $\det(A-I-tI)$ ? With a computer I have checked this for $n=2,3,4,5$ . Even though the hypothesis does not ask that the entries of $A$ be non-negative, the problem has a stochastic flavour, so I added a [stochastic-matrices] tag.","Let be a square matrix each of whose columns has sum . Let be the matrix obtained by replacing the lowest row in with a row of ones. Can anybody show (or find a counterexample) that is exactly the degree coefficient in the characteristic polynomial ? With a computer I have checked this for . Even though the hypothesis does not ask that the entries of be non-negative, the problem has a stochastic flavour, so I added a [stochastic-matrices] tag.","A 1 B A-I -\det(B) 1 \det(A-I-tI) n=2,3,4,5 A","['linear-algebra', 'matrices', 'commutative-algebra', 'stochastic-matrices']"
66,"Given Gram matrix, find any set of vectors","Given Gram matrix, find any set of vectors",,"Given Gram matrix $G_{ij}=\left \langle e_i,e_j \right\rangle=\begin{bmatrix}41 &12 &13 \\ 12 &48  &4 \\  13& 4 & 11\end{bmatrix}$ . Find any set of vectors $\left\{ e_1,e_2,e_3\right\}$ corresponding to it. My attempt: Let $A=\begin{bmatrix} e_{11} & e_{21} &e_{31} \\  e_{12} &e_{22}  &e_{32} \\   e_{31}&e_{32}  &e_{33}  \end{bmatrix}$ -three columns of the  vectors. Then we have a $9$ eqution system with $9$ unknown coordinates - $A^T\cdot A=G$ , but it isn't linear. May I ask if there is another method for solving this problem? Thank you in advance!","Given Gram matrix . Find any set of vectors corresponding to it. My attempt: Let -three columns of the  vectors. Then we have a eqution system with unknown coordinates - , but it isn't linear. May I ask if there is another method for solving this problem? Thank you in advance!","G_{ij}=\left \langle e_i,e_j \right\rangle=\begin{bmatrix}41 &12 &13 \\ 12 &48  &4 \\  13& 4 & 11\end{bmatrix} \left\{ e_1,e_2,e_3\right\} A=\begin{bmatrix}
e_{11} & e_{21} &e_{31} \\ 
e_{12} &e_{22}  &e_{32} \\ 
 e_{31}&e_{32}  &e_{33} 
\end{bmatrix} 9 9 A^T\cdot A=G","['linear-algebra', 'matrices']"
67,"linear combination, span, independence and bases for infinite dimensional vector spaces.","linear combination, span, independence and bases for infinite dimensional vector spaces.",,"I've only recently started studying linear algebra using some lecture notes by Evan Dummit ( https://math.la.asu.edu/~dummit/docs/linalgprac_2_vector_spaces.pdf ). After defining vector spaces, the notions of linear combination, span, generating set and linear independence are introduced. All of this culminates in the definition of a basis for a vector space followed by the dimension. Def: A vector $w$ is a linear combination of a set of vectors $v_{1}, v_{2},...,v_{n}$ if $\exists$ scalars $a_{1}, a_{2},..., a_{n}$ s.t. $w=a_{1}v_{1}+a_{2}v_{2}+\cdots+a_{n}v_{n}$ . Even though it is not explicitly stated this is a finite set of vectors since otherwise the expression does not have any meaning. Def: The span of a set of vectors $S=\{v_{1}, v_{2},...,v_{n}\}$ is the set of all linear combinations of $S$ . Def: Given a vector space $V$ , we say that $S$ is a generating set for $V$ if $\operatorname{span}(S)=V$ . This means that every vector in $V$ can be written as a linear combination of the vectors in the set $S$ . Def: A finite set of vectors $v_{1}, v_{2},...,v_{n}$ is linearly independent if $a_{1}v_{1}+a_{2}v_{2}+\cdots+a_{n}v_{n}=0$ implies that $a_{i}=0$ $\forall i$ . An infinite set of vectors is linearly independent if every finite subset is linearly independent (this is again because a linear combination of infinitely many vectors does not make sense). Def: Given a vector space $V$ , we say that an independent set of vectors which spans $V$ is a basis. So far so good with the definitions, but there is one thing that I just couldn't understand so far. Given the basis we can talk about the dimension of the vector space (which is the number of basis elements) and there are also infinite-dimensional vector spaces. However, there is also a theorem that states that every vector space (finite- or infinite-dimensional) has a basis. So my question is how a basis can even exist for the infinite-dimensional case when the definition of a linear combination only makes sense for finitely many vectors and the basis in this case has an infinite number of elements by definition. Can someone please point me in the correct direction? What am I missing? Thanks very much!","I've only recently started studying linear algebra using some lecture notes by Evan Dummit ( https://math.la.asu.edu/~dummit/docs/linalgprac_2_vector_spaces.pdf ). After defining vector spaces, the notions of linear combination, span, generating set and linear independence are introduced. All of this culminates in the definition of a basis for a vector space followed by the dimension. Def: A vector is a linear combination of a set of vectors if scalars s.t. . Even though it is not explicitly stated this is a finite set of vectors since otherwise the expression does not have any meaning. Def: The span of a set of vectors is the set of all linear combinations of . Def: Given a vector space , we say that is a generating set for if . This means that every vector in can be written as a linear combination of the vectors in the set . Def: A finite set of vectors is linearly independent if implies that . An infinite set of vectors is linearly independent if every finite subset is linearly independent (this is again because a linear combination of infinitely many vectors does not make sense). Def: Given a vector space , we say that an independent set of vectors which spans is a basis. So far so good with the definitions, but there is one thing that I just couldn't understand so far. Given the basis we can talk about the dimension of the vector space (which is the number of basis elements) and there are also infinite-dimensional vector spaces. However, there is also a theorem that states that every vector space (finite- or infinite-dimensional) has a basis. So my question is how a basis can even exist for the infinite-dimensional case when the definition of a linear combination only makes sense for finitely many vectors and the basis in this case has an infinite number of elements by definition. Can someone please point me in the correct direction? What am I missing? Thanks very much!","w v_{1}, v_{2},...,v_{n} \exists a_{1}, a_{2},..., a_{n} w=a_{1}v_{1}+a_{2}v_{2}+\cdots+a_{n}v_{n} S=\{v_{1}, v_{2},...,v_{n}\} S V S V \operatorname{span}(S)=V V S v_{1}, v_{2},...,v_{n} a_{1}v_{1}+a_{2}v_{2}+\cdots+a_{n}v_{n}=0 a_{i}=0 \forall i V V","['linear-algebra', 'vector-spaces', 'hamel-basis']"
68,Proof for showing linear independence,Proof for showing linear independence,,"Very recently I started studying proof based linear algebra on my own and I am having a difficulty because there is no one to tell me whether if my proof is right or wrong. Please keep in mind that this is my first time dealing with proofs. The question is the following: Let a system of vectors $v_1, v_2 ... v_r$ be linearly independent but not generating. Show that it is possible to find a vector $v_{r+1}$ such that the system $v_1, v_2, \dots, v_r, v_{r+1}$ is linearly independent. Hint: Take for $v_{r+1}$ any vector that cannot be represented as a linear combination of $v_1,v_2,\dots, v_r$ and show that the system $v_1, v_2, \dots, v_r, v_{r+1}$ is linearly independent. My attempt: question says $v_1,\dots,v_r$ is linearly independent so it has trivial coordinates $a_k$ . $a_1v_1 + a_2v_2 + \dots + a_rv_r = 0; a_1=a_2=\dots=a_r=0$ I add $v_{r+1}$ to the set $\{v_1,v_2,\dots,v_r\}$ and see if they can form linear independence. $a_1v_1 + a_2v_2 + \dots + a_rv_r + a_{r+1} v_{r+1} = 0$ but $a_1=a_2=\dots=a_r=0$ so $a_{r+1} v_{r+1} = 0$ is true if $a_{r+1} = 0$ (which makes it linear independent) as long as $v{r+1}$ is not the $0$ vector. Therefore, there is a $v_{r+1}$ that makes a system $\{v_1,v_2,\dots,v_{r+1}\}$ linearly independent. My proof ends here. **Extra question: In the beginning, it says system of vectors given are linearly independent but not generating. Those this mean it has basis but not all of the basis? Like if the space was R^3, then only 2 or 1 out of 3 basis are in the system?","Very recently I started studying proof based linear algebra on my own and I am having a difficulty because there is no one to tell me whether if my proof is right or wrong. Please keep in mind that this is my first time dealing with proofs. The question is the following: Let a system of vectors be linearly independent but not generating. Show that it is possible to find a vector such that the system is linearly independent. Hint: Take for any vector that cannot be represented as a linear combination of and show that the system is linearly independent. My attempt: question says is linearly independent so it has trivial coordinates . I add to the set and see if they can form linear independence. but so is true if (which makes it linear independent) as long as is not the vector. Therefore, there is a that makes a system linearly independent. My proof ends here. **Extra question: In the beginning, it says system of vectors given are linearly independent but not generating. Those this mean it has basis but not all of the basis? Like if the space was R^3, then only 2 or 1 out of 3 basis are in the system?","v_1, v_2 ... v_r v_{r+1} v_1, v_2, \dots, v_r, v_{r+1} v_{r+1} v_1,v_2,\dots, v_r v_1, v_2, \dots, v_r, v_{r+1} v_1,\dots,v_r a_k a_1v_1 + a_2v_2 + \dots + a_rv_r = 0; a_1=a_2=\dots=a_r=0 v_{r+1} \{v_1,v_2,\dots,v_r\} a_1v_1 + a_2v_2 + \dots + a_rv_r + a_{r+1} v_{r+1} = 0 a_1=a_2=\dots=a_r=0 a_{r+1} v_{r+1} = 0 a_{r+1} = 0 v{r+1} 0 v_{r+1} \{v_1,v_2,\dots,v_{r+1}\}","['linear-algebra', 'solution-verification']"
69,How professional mathematicians work through problems [closed],How professional mathematicians work through problems [closed],,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 4 years ago . Improve this question I've seen other questions on this site asking for problem-solving tips and general problem-solving strategies but I haven't seen any questions asking for the thought process of professional mathematicians as they approach problems (although if there are some, feel free to link them). What are the first things that go through your mind when you read a problem? How do you understand what the problem wants from you, or what knowledge the solution requires? How do your methods and your process change as the solution becomes longer or more crafty? Anything else that goes through you mind as you work through a problem. As someone fairly new to rigorous, proof-based math, I am really interested in answers to these questions. Below, I've provided a few problems from Lang's Linear Algebra I was hoping could be used to illustrate a professional mathematician's thought process. Chapter 8, section 4, exercise 20: Chapter 7, section 3, exercise 14: Chapter 5, section 4, exercise 4: If it's of any relevance, I am nearing the end of Lang's Linear Algebra and I have previously studied most of Spivak's Calculus. I have largely felt comfortable with the material in both books. Some techniques I have found useful in problem solving include making sure I understand the relevant definitions and specialized terms used in difficult questions question, looking for similar examples in the text, and trying to see if I can recognize any previous methods of proof that may apply to the problem at hand.","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 4 years ago . Improve this question I've seen other questions on this site asking for problem-solving tips and general problem-solving strategies but I haven't seen any questions asking for the thought process of professional mathematicians as they approach problems (although if there are some, feel free to link them). What are the first things that go through your mind when you read a problem? How do you understand what the problem wants from you, or what knowledge the solution requires? How do your methods and your process change as the solution becomes longer or more crafty? Anything else that goes through you mind as you work through a problem. As someone fairly new to rigorous, proof-based math, I am really interested in answers to these questions. Below, I've provided a few problems from Lang's Linear Algebra I was hoping could be used to illustrate a professional mathematician's thought process. Chapter 8, section 4, exercise 20: Chapter 7, section 3, exercise 14: Chapter 5, section 4, exercise 4: If it's of any relevance, I am nearing the end of Lang's Linear Algebra and I have previously studied most of Spivak's Calculus. I have largely felt comfortable with the material in both books. Some techniques I have found useful in problem solving include making sure I understand the relevant definitions and specialized terms used in difficult questions question, looking for similar examples in the text, and trying to see if I can recognize any previous methods of proof that may apply to the problem at hand.",,"['linear-algebra', 'problem-solving', 'education']"
70,"In the context of mathematics, What is the difference between equation and formula? [duplicate]","In the context of mathematics, What is the difference between equation and formula? [duplicate]",,"This question already has answers here : What is the difference between equation and formula? (14 answers) Closed 4 years ago . Equation and formula, these 2 terms are used alternatively in ""Introduction to Linear Algebra 4th edition"" without giving a clear definition, so, what is the difference? Can any one provide some concrete examples like following, to distinguish these 2 terms, especially a case is a formula rather than an equation. Even a supercomputer doesn't want the inverse matrix: too slow.   Inverses give the simplest formula $x = A^{-1}$ b but not the top speed.   And everyone must know that determinants are even slower-there is no   way a linear algebra course should begin with formulas for the   determinant of an n by n matrix. Those formulas have a place, but not   first place. The equation Ax = b uses the language of linear combinations right   away. This question is different to this post , which gives a lot of non-math examples without giving a clear comparison between the 2 terms in the context of mathematics.","This question already has answers here : What is the difference between equation and formula? (14 answers) Closed 4 years ago . Equation and formula, these 2 terms are used alternatively in ""Introduction to Linear Algebra 4th edition"" without giving a clear definition, so, what is the difference? Can any one provide some concrete examples like following, to distinguish these 2 terms, especially a case is a formula rather than an equation. Even a supercomputer doesn't want the inverse matrix: too slow.   Inverses give the simplest formula b but not the top speed.   And everyone must know that determinants are even slower-there is no   way a linear algebra course should begin with formulas for the   determinant of an n by n matrix. Those formulas have a place, but not   first place. The equation Ax = b uses the language of linear combinations right   away. This question is different to this post , which gives a lot of non-math examples without giving a clear comparison between the 2 terms in the context of mathematics.",x = A^{-1},"['linear-algebra', 'terminology']"
71,"Prove that if $S$ and $T$ are subspaces of $V$, then so is $S\cap T$","Prove that if  and  are subspaces of , then so is",S T V S\cap T,"I am asked to prove that If $S$ and $T$ are subspaces of $V$ , then so is $S\cap T$ . I have been thinking about this for a while but remain perplexed. I tried this: Let $\{e_1, e_2, ..., e_k\}$ be a basis for $S$ and let $\{u_i, u_2, ..., u_k\}$ be a basis for $T$ . Let all $u_n=e_n$ be denoted $d_n$ . $\therefore$$\{d_1, d_2, ..., d_p\}$ is a basis for $S \cap T$ if $S\cap T$ is a linear space. Now all elements $x$ in $S$ have form $x=\sum^{k}_{i=1}c_ie_i$ for some $c_i \in \Bbb R$ and all elements $y$ in $T$ have form $y=\sum^k_{i=1}c_iu_i$ for some $c_i \in \Bbb R$ . Now $S\cap T$ contains all elements of $S$ and $T$ such that $x=y$ . $\therefore \sum^k_{i=1}c_ie_i=\sum^k_{i=1}c_iu_i$ . $\therefore e_i=u_i$ . $\therefore S\cap T$ is a subspace with basis $\{d_1, d_2, ..., d_p\}$ . I am not sure that this proof is valid, nor that it even really proves anything? Is there any merit to it? Otherwise, what would be a better approach to proving this?","I am asked to prove that If and are subspaces of , then so is . I have been thinking about this for a while but remain perplexed. I tried this: Let be a basis for and let be a basis for . Let all be denoted . is a basis for if is a linear space. Now all elements in have form for some and all elements in have form for some . Now contains all elements of and such that . . . is a subspace with basis . I am not sure that this proof is valid, nor that it even really proves anything? Is there any merit to it? Otherwise, what would be a better approach to proving this?","S T V S\cap T \{e_1, e_2, ..., e_k\} S \{u_i, u_2, ..., u_k\} T u_n=e_n d_n \therefore\{d_1, d_2, ..., d_p\} S \cap T S\cap T x S x=\sum^{k}_{i=1}c_ie_i c_i \in \Bbb R y T y=\sum^k_{i=1}c_iu_i c_i \in \Bbb R S\cap T S T x=y \therefore \sum^k_{i=1}c_ie_i=\sum^k_{i=1}c_iu_i \therefore e_i=u_i \therefore S\cap T \{d_1, d_2, ..., d_p\}","['linear-algebra', 'proof-verification', 'proof-writing']"
72,"Show that the system $v_1 - u, \dots, v_n - u$ is linearly dependent precisely when $\lambda_1 + \dots + \lambda_n=1$",Show that the system  is linearly dependent precisely when,"v_1 - u, \dots, v_n - u \lambda_1 + \dots + \lambda_n=1","Let $v_1, \dots v_n$ be a linearly independent system of the $\mathbb{K}$ -vectorspace $V$ and $u = \lambda_1v_1 + \dots + \lambda_n v_n$ with $\lambda_1, . . . \lambda_n \in \mathbb{K}$ . Show that the system $v_1 - u, \dots, v_n - u$ is linearly dependent precisely when $\lambda_1 + \dots + \lambda_n=1$ In other words, I have $u=\sum\limits^n_{i=1}\lambda_iv_i$ and I should show, that $$v_1 - \left(\sum\limits^n_{i=1}\lambda_iv_i\right), \dots, v_n - \left(\sum\limits^n_{i=1}\lambda_iv_i\right)$$ is linear dependend only for $\lambda_1 + \dots + \lambda_n=1$ So, all $v_i$ 's would terminate one $v_i$ : $$ -\sum\limits^{n}_{i=2}\lambda_iv_i, \dots, -\sum\limits^{n-1}_{i=1}\lambda_iv_i$$ I'm not sure how to go on exactly. I could suppose, that $\lambda_1 + \dots + \lambda_n=1$ and I would get $$ -\sum\limits^{n}_{i=2}v_i, \dots, -\sum\limits^{n-1}_{i=1}v_i=\left(-v_2-v_3-\dots-v_n\right),\dots,\left(-v_1-v_2-\dots-v_{n-2}-v_{n-1}\right)$$ But that wouldn't help me either, or am I missing somthing? EDIT: Using N. S. hint, I finally understood it: Because $\beta_i=\frac{\beta_i}{\beta_1+\dots+\beta_n} \quad i\in\{1,\dots,n\}$ one can say that $\beta_1+\beta_2+\dots+\beta_n=\frac{\beta_1}{\beta_1+\beta_2+\dots+\beta_n}+\dots+\frac{\beta_n}{\beta_1+\beta_2+\dots+\beta_n}=\frac{\beta_1+\beta_2+\dots+\beta_n}{\beta_1+\beta_2+\dots+\beta_n}=1$","Let be a linearly independent system of the -vectorspace and with . Show that the system is linearly dependent precisely when In other words, I have and I should show, that is linear dependend only for So, all 's would terminate one : I'm not sure how to go on exactly. I could suppose, that and I would get But that wouldn't help me either, or am I missing somthing? EDIT: Using N. S. hint, I finally understood it: Because one can say that","v_1, \dots v_n \mathbb{K} V u = \lambda_1v_1 + \dots + \lambda_n v_n \lambda_1, . . . \lambda_n \in \mathbb{K} v_1 - u, \dots, v_n - u \lambda_1 + \dots + \lambda_n=1 u=\sum\limits^n_{i=1}\lambda_iv_i v_1 - \left(\sum\limits^n_{i=1}\lambda_iv_i\right), \dots, v_n - \left(\sum\limits^n_{i=1}\lambda_iv_i\right) \lambda_1 + \dots + \lambda_n=1 v_i v_i  -\sum\limits^{n}_{i=2}\lambda_iv_i, \dots, -\sum\limits^{n-1}_{i=1}\lambda_iv_i \lambda_1 + \dots + \lambda_n=1  -\sum\limits^{n}_{i=2}v_i, \dots, -\sum\limits^{n-1}_{i=1}v_i=\left(-v_2-v_3-\dots-v_n\right),\dots,\left(-v_1-v_2-\dots-v_{n-2}-v_{n-1}\right) \beta_i=\frac{\beta_i}{\beta_1+\dots+\beta_n} \quad i\in\{1,\dots,n\} \beta_1+\beta_2+\dots+\beta_n=\frac{\beta_1}{\beta_1+\beta_2+\dots+\beta_n}+\dots+\frac{\beta_n}{\beta_1+\beta_2+\dots+\beta_n}=\frac{\beta_1+\beta_2+\dots+\beta_n}{\beta_1+\beta_2+\dots+\beta_n}=1","['linear-algebra', 'proof-verification', 'vector-spaces']"
73,Why is this method for solving linear equations systems using determinants works?,Why is this method for solving linear equations systems using determinants works?,,"I'm new here and english is not my native language but I'll try to explain my question the best I can. While studying methods for solving systems of linear equations I came across a method so called ""Gauss algorithm"" for which I doubt if it is it's real name or not, but I can't understand WHY it works. I would really appreciate a proper explanation or maybe a hint for what is happening inside the method process that accounts for the logic behind it. The process in question is the following and involves determinants: Suppose we have the following system of linear equations, for instance \begin{cases} \ x+2y-z=-5 \\2x-1y+2z=8 \\3x+3y+4z=5 \ \end{cases} Then the result of appliying the method continues as follow \begin{array}{ccc|c} x & y & z & i.t. \\ \hline 1 & 2 & -1 & -5 \\ 2 & -1 & 2 & 8 \\ 3 & 3 & 4 & 5 \\ \hline  & -5 & 4 & 18 \\  & -3 & 7 & 20 \\ \hline  &  & -23 & -46 \\ \end{array} Where for example the coefficients $-5$ and $4$ , and the independent term $18$ , of the first row of the reduced system has been computed with the follow determinants: \begin{equation}  \begin{vmatrix}     1 & 2  \\     2 & -1  \\     \end{vmatrix}=-5 \hspace{1cm} \begin{vmatrix}     1 & -1  \\     2 & 2  \\     \end{vmatrix}=4 \hspace{1cm} \begin{vmatrix}     1 & -5  \\     2 & 8  \\     \end{vmatrix}=18 \end{equation} Where those determinants were computed using the first and the second row of the system of equations matrix. Then the other coefficients -3 and 7 and the independant term 20 were found by computing the respective determinants as above but using the first and third row of the system of equations matrix. Appliying this method and if the system is compatible, then as above you can see that \begin{equation} -23z = -46\\ z=2\end{equation} and then by substitution in the subsequent former equations the other variables can be found. Well that's the method I was studying and will appreciate to know more about it, it's a pity I don't know the name of it but maybe you can help. Thanks in advance!","I'm new here and english is not my native language but I'll try to explain my question the best I can. While studying methods for solving systems of linear equations I came across a method so called ""Gauss algorithm"" for which I doubt if it is it's real name or not, but I can't understand WHY it works. I would really appreciate a proper explanation or maybe a hint for what is happening inside the method process that accounts for the logic behind it. The process in question is the following and involves determinants: Suppose we have the following system of linear equations, for instance Then the result of appliying the method continues as follow Where for example the coefficients and , and the independent term , of the first row of the reduced system has been computed with the follow determinants: Where those determinants were computed using the first and the second row of the system of equations matrix. Then the other coefficients -3 and 7 and the independant term 20 were found by computing the respective determinants as above but using the first and third row of the system of equations matrix. Appliying this method and if the system is compatible, then as above you can see that and then by substitution in the subsequent former equations the other variables can be found. Well that's the method I was studying and will appreciate to know more about it, it's a pity I don't know the name of it but maybe you can help. Thanks in advance!","\begin{cases}
\ x+2y-z=-5 \\2x-1y+2z=8 \\3x+3y+4z=5 \
\end{cases} \begin{array}{ccc|c}
x & y & z & i.t. \\
\hline
1 & 2 & -1 & -5 \\
2 & -1 & 2 & 8 \\
3 & 3 & 4 & 5 \\
\hline
 & -5 & 4 & 18 \\
 & -3 & 7 & 20 \\
\hline
 &  & -23 & -46 \\
\end{array} -5 4 18 \begin{equation}
 \begin{vmatrix}
    1 & 2  \\
    2 & -1  \\
    \end{vmatrix}=-5
\hspace{1cm}
\begin{vmatrix}
    1 & -1  \\
    2 & 2  \\
    \end{vmatrix}=4
\hspace{1cm}
\begin{vmatrix}
    1 & -5  \\
    2 & 8  \\
    \end{vmatrix}=18
\end{equation} \begin{equation} -23z = -46\\
z=2\end{equation}","['linear-algebra', 'systems-of-equations']"
74,Find $n \times n$ matrices $A$ such that $\det A = 0$ and $\text{rank}(AB) = \text{rank}(BA)$ for any $n \times n$ matrix $B$,Find  matrices  such that  and  for any  matrix,n \times n A \det A = 0 \text{rank}(AB) = \text{rank}(BA) n \times n B,"Find all complex-valued $n \times n$ matrices $A$ such that $\det A = 0$ and $\text{rank}(AB) = \text{rank}(BA)$ for any $n \times n$ complex-valued $B$ . I believe that $A = 0$ is the only answer. I have been able to prove that, if $A$ is of rank $r$ , then any $r$ lines and any $r$ columns are linearly independent. To see this, note that since $A$ is of rank $r$ , then A has $r$ linearly independent columns; say that the indexes of these columns are $i_1, i_2, ..., i_r$ . Then by making B equal to a matrix that has 1 in positions $(i_k,i_k)$ and 0 elsewhere, $AB$ basically ""selects"" $r$ independent columns from $A$ having all other columns equal to 0, so $\text{rank} AB = r$ . Now, $BA$ selects rows $i_1,i_2,..,i_r$ from $A$ . If $A$ were to have $r$ rows that were not linearly independent, then there would be an inversible matrix $M$ which would place these rows in positions $i_1,i_2,...,i_r$ . Then $$\text{rank} (BA) = \text{rank} (BAM) < r,$$ which would contradict our hypothesis. Thus any $r$ rows of $A$ are linearly independent. Running the same argument in reverse, we get that any $r$ lines of $A$ are linearly independent. Any ideas about how to proceed?","Find all complex-valued matrices such that and for any complex-valued . I believe that is the only answer. I have been able to prove that, if is of rank , then any lines and any columns are linearly independent. To see this, note that since is of rank , then A has linearly independent columns; say that the indexes of these columns are . Then by making B equal to a matrix that has 1 in positions and 0 elsewhere, basically ""selects"" independent columns from having all other columns equal to 0, so . Now, selects rows from . If were to have rows that were not linearly independent, then there would be an inversible matrix which would place these rows in positions . Then which would contradict our hypothesis. Thus any rows of are linearly independent. Running the same argument in reverse, we get that any lines of are linearly independent. Any ideas about how to proceed?","n \times n A \det A = 0 \text{rank}(AB) = \text{rank}(BA) n \times n B A = 0 A r r r A r r i_1, i_2, ..., i_r (i_k,i_k) AB r A \text{rank} AB = r BA i_1,i_2,..,i_r A A r M i_1,i_2,...,i_r \text{rank} (BA) = \text{rank} (BAM) < r, r A r A",['linear-algebra']
75,General form of elements of $SU(2)$,General form of elements of,SU(2),"$SU(2)$ is the set of $2\times 2$ complex matrices $A$ satisfying $AA^*=I$ and $\det(A)=1$ where $A^*$ denotes the conjugate transpose of $A$ and $I$ is the identity matrix. I've seen everywhere that the elements of $SU(2)$ can be represented as $\begin{pmatrix}\alpha&\beta\\-\bar\beta&\bar\alpha\end{pmatrix}$ where $\vert\alpha\vert^2+\vert\beta\vert^2=1$ , but I've been having a stupidly hard time working out the arithmetic for this claim. I was able to show that if $\begin{pmatrix}\alpha&\beta\\\gamma&\delta\end{pmatrix}$ is in $SU(2)$ (or even just $U(2)$ ), then $\vert\alpha\vert=\vert\delta\vert$ and $\vert\beta\vert=\vert\gamma\vert$ , but I'm stuck from there. I know I must use the equation $\alpha\delta-\beta\gamma=1$ to obtain the desired result, but I'm failing to do so.","is the set of complex matrices satisfying and where denotes the conjugate transpose of and is the identity matrix. I've seen everywhere that the elements of can be represented as where , but I've been having a stupidly hard time working out the arithmetic for this claim. I was able to show that if is in (or even just ), then and , but I'm stuck from there. I know I must use the equation to obtain the desired result, but I'm failing to do so.",SU(2) 2\times 2 A AA^*=I \det(A)=1 A^* A I SU(2) \begin{pmatrix}\alpha&\beta\\-\bar\beta&\bar\alpha\end{pmatrix} \vert\alpha\vert^2+\vert\beta\vert^2=1 \begin{pmatrix}\alpha&\beta\\\gamma&\delta\end{pmatrix} SU(2) U(2) \vert\alpha\vert=\vert\delta\vert \vert\beta\vert=\vert\gamma\vert \alpha\delta-\beta\gamma=1,"['linear-algebra', 'lie-groups']"
76,Prove $\operatorname{rank }(A) \le 4$ if $Ax_1=\cdots=Ax_7$ and $x_i$s are linearly independent,Prove  if  and s are linearly independent,\operatorname{rank }(A) \le 4 Ax_1=\cdots=Ax_7 x_i,"Let $A \in \mathbb R^{10\times10}$ , $x_{1},x_{2},...,x_{7}\in \mathbb R^{10}$ which are linearly independent vectors and $Ax_{1}=Ax_{2}=\cdots=Ax_{7}$ . Prove that $ \operatorname{rank} (A) \le 4$ . I know that it is truth because if I have seven linearly independent vectors and I multiply a matrix with these vectors I must have at least $6$ zero rows because I have $Ax_{1}=Ax_{2}=\cdots=Ax_{7}$ . For example if I have $5$ zero rows then this vectors are not linearly independent. However I don't know how to prove it in an elegant way.","Let , which are linearly independent vectors and . Prove that . I know that it is truth because if I have seven linearly independent vectors and I multiply a matrix with these vectors I must have at least zero rows because I have . For example if I have zero rows then this vectors are not linearly independent. However I don't know how to prove it in an elegant way.","A \in \mathbb R^{10\times10} x_{1},x_{2},...,x_{7}\in \mathbb R^{10} Ax_{1}=Ax_{2}=\cdots=Ax_{7}  \operatorname{rank} (A) \le 4 6 Ax_{1}=Ax_{2}=\cdots=Ax_{7} 5","['linear-algebra', 'matrix-rank']"
77,What inspires people to define linear maps?,What inspires people to define linear maps?,,"I am currently self studying Linear Algebra Done Right . I am doing OK and currently on Chapter 3 Linear Maps. My understanding now is that a map is like a function that maps something to another thing. Why people like to particularly define linear maps? Just curious..maybe there are non-linear maps? Another stupid question is that is linear maps really look like a straight line in some specific examples, like, $u, v $ is 1 dimensional vector space. And then $T(u+v) = T(u) + T(v)$ Or put it in another way why it is called linear map. And finally, why linear maps have the additivity and homogeneity properties?","I am currently self studying Linear Algebra Done Right . I am doing OK and currently on Chapter 3 Linear Maps. My understanding now is that a map is like a function that maps something to another thing. Why people like to particularly define linear maps? Just curious..maybe there are non-linear maps? Another stupid question is that is linear maps really look like a straight line in some specific examples, like, is 1 dimensional vector space. And then Or put it in another way why it is called linear map. And finally, why linear maps have the additivity and homogeneity properties?","u, v  T(u+v) = T(u) + T(v)",['linear-algebra']
78,Eigenvalues and Eigenvectors of Sum of Symmetric Matrix,Eigenvalues and Eigenvectors of Sum of Symmetric Matrix,,Question: Let A = \begin{bmatrix} 1 & 1 \\ 1 & 1 \\ \end{bmatrix} Find all eigenvalues and eigenvectors of the martrix: $$\sum_{n=1}^{100} A^n = A^{100} +A^{99} +...+A^2+A$$ I know that the eigenvectors of A are \begin{bmatrix} 1 \\ 1 \end{bmatrix} and \begin{bmatrix} 1 \\ -1 \end{bmatrix} But I do not see any sort of correlation with the sum term and A's eigenvectors.,Question: Let A = Find all eigenvalues and eigenvectors of the martrix: I know that the eigenvectors of A are and But I do not see any sort of correlation with the sum term and A's eigenvectors.,\begin{bmatrix} 1 & 1 \\ 1 & 1 \\ \end{bmatrix} \sum_{n=1}^{100} A^n = A^{100} +A^{99} +...+A^2+A \begin{bmatrix} 1 \\ 1 \end{bmatrix} \begin{bmatrix} 1 \\ -1 \end{bmatrix},"['linear-algebra', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
79,Proving v1+v2 is not an eigenvector of A,Proving v1+v2 is not an eigenvector of A,,"Let $\lambda_1$ and $\lambda_2$ be two distinct eigenvalues of an $n \times n$ matrix $A$ , $v_1$ and $v_2$ are the corresponding eigenvectors. Prove that $v_1 + v_2$ is not an eigenvector of $A$ . Is this how you set this up? Unsure where to begin. $A(v_1+v_2) = Av_1  + Av_2$ $A(v_1+v_2) = \lambda_1v_1 + \lambda_2v_2$ ...","Let and be two distinct eigenvalues of an matrix , and are the corresponding eigenvectors. Prove that is not an eigenvector of . Is this how you set this up? Unsure where to begin. ...",\lambda_1 \lambda_2 n \times n A v_1 v_2 v_1 + v_2 A A(v_1+v_2) = Av_1  + Av_2 A(v_1+v_2) = \lambda_1v_1 + \lambda_2v_2,"['linear-algebra', 'matrices']"
80,Eigenvalues of the Product of a Diagonal and a Symmetric Matrix,Eigenvalues of the Product of a Diagonal and a Symmetric Matrix,,Let $A \in \mathbb{R}^{n\times n}$ be a symmetric matrix and $D \in \mathbb{R}^{n\times n}$ be a diagonal matrix with positive entries. Prove that the matrix $P:=DA$ has real eigenvalues.,Let be a symmetric matrix and be a diagonal matrix with positive entries. Prove that the matrix has real eigenvalues.,A \in \mathbb{R}^{n\times n} D \in \mathbb{R}^{n\times n} P:=DA,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-calculus']"
81,Distinguishing between inner product and outer product in matrix notation,Distinguishing between inner product and outer product in matrix notation,,"As a recent field transferee from chemist to data scientist, I find myself wading through more matrix multiplication than I'm used to. I did some linear algebra way back, but I struggle with identifying 'which way' (i.e. inner or outer product) a given matrix multiplication is going. I feel like there's some sort of convention with vector multiplication where the  vector $\mathbf X$ is treated as a column vector, and $\mathbf X^T \mathbf X$ and $\mathbf X\mathbf X^T$ are the inner and outer product of  $\mathbf X$ respectively. Is this true in all cases? Are there any tricks or mnemonics to help me keep track of the product of a string of such multiplications? EDIT: There's a comment stating that it is common to assume that vectors are treated as column matrices. How common? Does it vary between disciplines? How likely (given that i'm looking through Wikipedia and stackexchange, not centuries-old manuscripts) am I to encounter the converse scenario?","As a recent field transferee from chemist to data scientist, I find myself wading through more matrix multiplication than I'm used to. I did some linear algebra way back, but I struggle with identifying 'which way' (i.e. inner or outer product) a given matrix multiplication is going. I feel like there's some sort of convention with vector multiplication where the  vector $\mathbf X$ is treated as a column vector, and $\mathbf X^T \mathbf X$ and $\mathbf X\mathbf X^T$ are the inner and outer product of  $\mathbf X$ respectively. Is this true in all cases? Are there any tricks or mnemonics to help me keep track of the product of a string of such multiplications? EDIT: There's a comment stating that it is common to assume that vectors are treated as column matrices. How common? Does it vary between disciplines? How likely (given that i'm looking through Wikipedia and stackexchange, not centuries-old manuscripts) am I to encounter the converse scenario?",,"['linear-algebra', 'inner-products', 'convention', 'outer-product']"
82,"Does least squares (approximate solution) minimize the orthogonal distance of $b$ to $Ax$, or does it minimize the error projected along the $b$ axis?","Does least squares (approximate solution) minimize the orthogonal distance of  to , or does it minimize the error projected along the  axis?",b Ax b,"I have always been confused about whether the approximate solution to $Ax=b$ is equivalent to minimizing the average distance of all of the $b$ vectors to $Ax$, or whether it is minimizing the distance projected along the $b$ axis? (where $A$ is full rank and skinny and the system is overdetermined). Consider these two pictures: from http://www.statisticshowto.com/least-squares-regression-line/ and another figure on the same page: also from from http://www.statisticshowto.com/least-squares-regression-line/ . Notice these are two figures on the same page!  I do not understand how these are both being minimized at the same time.  can someone please explain?  thanks. this is a related question Least squares solutions and orthogonal projection?","I have always been confused about whether the approximate solution to $Ax=b$ is equivalent to minimizing the average distance of all of the $b$ vectors to $Ax$, or whether it is minimizing the distance projected along the $b$ axis? (where $A$ is full rank and skinny and the system is overdetermined). Consider these two pictures: from http://www.statisticshowto.com/least-squares-regression-line/ and another figure on the same page: also from from http://www.statisticshowto.com/least-squares-regression-line/ . Notice these are two figures on the same page!  I do not understand how these are both being minimized at the same time.  can someone please explain?  thanks. this is a related question Least squares solutions and orthogonal projection?",,"['linear-algebra', 'linear-transformations', 'regression', 'linear-regression', 'regression-analysis']"
83,Proving the Jacobi method converges for diagonally-column dominant matrices,Proving the Jacobi method converges for diagonally-column dominant matrices,,"The Jacobi method is an iterative method for approaching the solution of the linear system $Ax=b$, with $A\in\mathbb{C}^{n\times n}$, where we write $A=K-L$, with $K=\mathrm{diag}(a_{11},\ldots,a_{nn})$, and where we use the fixed point iteration $$\alpha_{j+1}=K^{-1}L\alpha_j+K^{-1}b,$$ so that we have for a $j\in\mathbb{N}$: $$\alpha-\alpha_{j+1}=K^{-1}L(\alpha-\alpha_{j}).$$ Now my syllabus provides a proof for convergence for the case that $A$ is diagonally-row dominant, but I and our teacher both couldn't see a way to rewrite the proof to a proof for the diagonally-column dominant case. The proof for the diagonally-row dominant is given using the $\|\cdot\|_{\infty}$ norm, and I found on the internet that the diagonally-column dominant case can be proved using the $\|\cdot\|_{1}$ norm. The proof strategy would be to show that $$\|\alpha-\alpha_{j+1}\|\leq \rho^j\|\alpha-\alpha_1\|,$$ for some matrix norm $\|\cdot\|$ and a $0\leq\rho<1$, from which convergence would follow immediately. Does anyone have an idea how to continue? Using the $\|\cdot\|_{1}$ norm as suggested by the internet we want to have something like $$\|\alpha-\alpha_{j+1}\|_1\leq\|(K^{-1}L)^j\|_1\cdot\|\alpha-\alpha_1\|_1,$$ and somehow conclude $\|K^{-1}L\|_1$ should be bounded by $1$. I know about the characterization $$r:=\max_{i\in\{1,\ldots,n\}}\Bigg\{|a_{ii}|^{-1}\sum_{\substack{i&=1\\ i&\neq j}}|a_{ji}|\Bigg\}.$$ I'd like to proof this without Gershgorin's theorem, since this is not covered in my course. It is fine if the case follows from the diagonally-row dominant case.","The Jacobi method is an iterative method for approaching the solution of the linear system $Ax=b$, with $A\in\mathbb{C}^{n\times n}$, where we write $A=K-L$, with $K=\mathrm{diag}(a_{11},\ldots,a_{nn})$, and where we use the fixed point iteration $$\alpha_{j+1}=K^{-1}L\alpha_j+K^{-1}b,$$ so that we have for a $j\in\mathbb{N}$: $$\alpha-\alpha_{j+1}=K^{-1}L(\alpha-\alpha_{j}).$$ Now my syllabus provides a proof for convergence for the case that $A$ is diagonally-row dominant, but I and our teacher both couldn't see a way to rewrite the proof to a proof for the diagonally-column dominant case. The proof for the diagonally-row dominant is given using the $\|\cdot\|_{\infty}$ norm, and I found on the internet that the diagonally-column dominant case can be proved using the $\|\cdot\|_{1}$ norm. The proof strategy would be to show that $$\|\alpha-\alpha_{j+1}\|\leq \rho^j\|\alpha-\alpha_1\|,$$ for some matrix norm $\|\cdot\|$ and a $0\leq\rho<1$, from which convergence would follow immediately. Does anyone have an idea how to continue? Using the $\|\cdot\|_{1}$ norm as suggested by the internet we want to have something like $$\|\alpha-\alpha_{j+1}\|_1\leq\|(K^{-1}L)^j\|_1\cdot\|\alpha-\alpha_1\|_1,$$ and somehow conclude $\|K^{-1}L\|_1$ should be bounded by $1$. I know about the characterization $$r:=\max_{i\in\{1,\ldots,n\}}\Bigg\{|a_{ii}|^{-1}\sum_{\substack{i&=1\\ i&\neq j}}|a_{ji}|\Bigg\}.$$ I'd like to proof this without Gershgorin's theorem, since this is not covered in my course. It is fine if the case follows from the diagonally-row dominant case.",,"['linear-algebra', 'numerical-methods', 'numerical-linear-algebra']"
84,Relationship between rank of binary matrix and the NOT operator,Relationship between rank of binary matrix and the NOT operator,,"Let $A$ be a binary matrix. I'm looking for any information about the relationship between the rank of $A$ and the rank of NOT$(A)$, where NOT replaces all $0$s with $1$s, and vice-versa. What I know These ranks can sometimes be equal. For example, applying the NOT operator to the identity matrix returns another full rank matrix. They can sometimes not be equal. For example, the matrix  \begin{equation*} A= \begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix} \end{equation*} has rank $2$, but  \begin{equation*} \text{NOT}(A)= \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} \end{equation*} has rank $1$. My questions Are there known relationships between the two ranks?","Let $A$ be a binary matrix. I'm looking for any information about the relationship between the rank of $A$ and the rank of NOT$(A)$, where NOT replaces all $0$s with $1$s, and vice-versa. What I know These ranks can sometimes be equal. For example, applying the NOT operator to the identity matrix returns another full rank matrix. They can sometimes not be equal. For example, the matrix  \begin{equation*} A= \begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix} \end{equation*} has rank $2$, but  \begin{equation*} \text{NOT}(A)= \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} \end{equation*} has rank $1$. My questions Are there known relationships between the two ranks?",,"['linear-algebra', 'matrix-rank', 'binary']"
85,Mathematical notation for matrix slicing,Mathematical notation for matrix slicing,,"Suppose I have a matrix $M$. From that matrix, I create a matrix $N$ by only keeping some rows of $M$. The rows to keep are given in a vector or set called $r$. Then I do the same with columns in vector/set $c$ in order to obtain matrix $Q$. In a notation similar to Matlab or Numpy, this is realized by slicing: N = M[r, :] # keep only rows whose index is in r Q = N[:, c] # keep only columns whose index is in c How does the mathematical notation for such a construct look? I'm also happy with a solution that does the slicing in one step.","Suppose I have a matrix $M$. From that matrix, I create a matrix $N$ by only keeping some rows of $M$. The rows to keep are given in a vector or set called $r$. Then I do the same with columns in vector/set $c$ in order to obtain matrix $Q$. In a notation similar to Matlab or Numpy, this is realized by slicing: N = M[r, :] # keep only rows whose index is in r Q = N[:, c] # keep only columns whose index is in c How does the mathematical notation for such a construct look? I'm also happy with a solution that does the slicing in one step.",,"['linear-algebra', 'matrices', 'notation']"
86,Intuition for a Basis in Vector Spaces,Intuition for a Basis in Vector Spaces,,"Some confusion about the basis vectors. These questions came from the definition of a basis at the bottom. I'm wondering: What the relation is between basis and non-basis vectors. Or put another way, if every single element in the vector space is linearly dependent on the linearly independent basis vectors (they are independent among themselves, but dependent amongst the other vectors). This is based on my understanding of the definition of span and the idea that every vector in the space is a linear combination of the basis vectors (except the basis vectors). I'm visualizing the span as: the basis vectors are all linearly independent, but then they ""connect"" to every other vector outside of the basis via linear combination. In that sense they reach every other vector, and so span the vector space. Wondering if that is correct. If the number of basis vectors is the dimension. Just want to make sure I'm understanding. What the relation between a basis and a generating set is. A brief intuition on how to think of basis vectors. A set of elements in a vector space is called a basis if they are: Linearly independent . And every vector in the vector space is a linear combination of the set (the basis vectors?). In another perspective, a basis is a linearly independent spanning set . Formally, a basis $B$ of a vector space $V$ over a field $F$ is a linearly independent subset of $V$ that spans $V$. A span of a set of vectors in $V$ is the intersection of all subspaces containing that set. In the definition of free module they compare a basis to a generating set .","Some confusion about the basis vectors. These questions came from the definition of a basis at the bottom. I'm wondering: What the relation is between basis and non-basis vectors. Or put another way, if every single element in the vector space is linearly dependent on the linearly independent basis vectors (they are independent among themselves, but dependent amongst the other vectors). This is based on my understanding of the definition of span and the idea that every vector in the space is a linear combination of the basis vectors (except the basis vectors). I'm visualizing the span as: the basis vectors are all linearly independent, but then they ""connect"" to every other vector outside of the basis via linear combination. In that sense they reach every other vector, and so span the vector space. Wondering if that is correct. If the number of basis vectors is the dimension. Just want to make sure I'm understanding. What the relation between a basis and a generating set is. A brief intuition on how to think of basis vectors. A set of elements in a vector space is called a basis if they are: Linearly independent . And every vector in the vector space is a linear combination of the set (the basis vectors?). In another perspective, a basis is a linearly independent spanning set . Formally, a basis $B$ of a vector space $V$ over a field $F$ is a linearly independent subset of $V$ that spans $V$. A span of a set of vectors in $V$ is the intersection of all subspaces containing that set. In the definition of free module they compare a basis to a generating set .",,"['linear-algebra', 'vector-spaces']"
87,Prove symmetries of eigenvalues of symplectic matrices,Prove symmetries of eigenvalues of symplectic matrices,,"$M \in \mathrm{GL}(2m,\mathbb{R})$ is said to be symplectic if $$ M^T JM=J,$$ $$J =\begin{bmatrix} 0 & -I_m \\ I_m & 0\end{bmatrix}. $$ Suppose $M \in \mathrm{GL}(2m,\mathbb{R})$ is symplectic and has $\det(M)=1$, let $\chi_M(x)$ be the characteristic polynomial of M, prove that (a) If $\lambda \in \mathbb{C}, \lambda \neq 0$ is a root of $\chi_M$ with multiplicity $d$, then $\frac{1}{\lambda}$ is a root of $\chi_M$ with the same multiplicity $d$. (b) $1,-1$ have even multiplicity in $\chi_M$. It's difficult to argue part (b) rigorously, and I don't think that it is obvious from (a). The given hint is that $\chi_M = \chi_{M^{-1}}$, and therefore for $z \in \mathbb{C}, z \neq 0$ we have $$\chi_M(z) = z^{2m}\chi_M(\frac{1}{z}).$$","$M \in \mathrm{GL}(2m,\mathbb{R})$ is said to be symplectic if $$ M^T JM=J,$$ $$J =\begin{bmatrix} 0 & -I_m \\ I_m & 0\end{bmatrix}. $$ Suppose $M \in \mathrm{GL}(2m,\mathbb{R})$ is symplectic and has $\det(M)=1$, let $\chi_M(x)$ be the characteristic polynomial of M, prove that (a) If $\lambda \in \mathbb{C}, \lambda \neq 0$ is a root of $\chi_M$ with multiplicity $d$, then $\frac{1}{\lambda}$ is a root of $\chi_M$ with the same multiplicity $d$. (b) $1,-1$ have even multiplicity in $\chi_M$. It's difficult to argue part (b) rigorously, and I don't think that it is obvious from (a). The given hint is that $\chi_M = \chi_{M^{-1}}$, and therefore for $z \in \mathbb{C}, z \neq 0$ we have $$\chi_M(z) = z^{2m}\chi_M(\frac{1}{z}).$$",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symplectic-linear-algebra']"
88,Example of infinite dimensional linear spaces where the space is equal to its dual.,Example of infinite dimensional linear spaces where the space is equal to its dual.,,"My understanding is that in finite dimensions, every linear space $V$ is isomorphic to its dual $V^\ast$. In infinite dimensions, we have that any Hilbert space $\mathcal{H}$ is isomorphic (specifically, anti-isomorphic) to its dual $\mathcal{H}^\ast$ (Riesz Representation Theorem). Furthermore, every Hilbert space is also isomorphic to the square summable sequence space $\ell^2$. I am wondering if there are examples of infinite dimensional linear spaces where the dual is equal to itself, and the space is not isomorphic to $\ell^2$. Edit : We assume the underlying field to be $\mathbb{R}$ or $\mathbb{C}$.","My understanding is that in finite dimensions, every linear space $V$ is isomorphic to its dual $V^\ast$. In infinite dimensions, we have that any Hilbert space $\mathcal{H}$ is isomorphic (specifically, anti-isomorphic) to its dual $\mathcal{H}^\ast$ (Riesz Representation Theorem). Furthermore, every Hilbert space is also isomorphic to the square summable sequence space $\ell^2$. I am wondering if there are examples of infinite dimensional linear spaces where the dual is equal to itself, and the space is not isomorphic to $\ell^2$. Edit : We assume the underlying field to be $\mathbb{R}$ or $\mathbb{C}$.",,"['linear-algebra', 'functional-analysis', 'vector-spaces', 'hilbert-spaces', 'dual-spaces']"
89,A nilpotent matrix A implies AB is nilpotent?,A nilpotent matrix A implies AB is nilpotent?,,"Problem: Show that if A and B are n×n nilpotent matrices that commute, then AB is also nilpotent I ended up proving that if either A or B is nilpotent, then AB is nilpotent, so you don't need both matrices to be nilpotent. Is my proof correct? My proof seems kind of trivial, so I'm not sure if it's correct. Let m >= 1. If A is nilpotent then $A^m = 0$. Hence $A^m*B^m=0$ Hence $(AB)^m=0$ (Shown from previous question.)","Problem: Show that if A and B are n×n nilpotent matrices that commute, then AB is also nilpotent I ended up proving that if either A or B is nilpotent, then AB is nilpotent, so you don't need both matrices to be nilpotent. Is my proof correct? My proof seems kind of trivial, so I'm not sure if it's correct. Let m >= 1. If A is nilpotent then $A^m = 0$. Hence $A^m*B^m=0$ Hence $(AB)^m=0$ (Shown from previous question.)",,"['linear-algebra', 'matrices', 'proof-verification']"
90,explicit description of eigenvector of a rotation,explicit description of eigenvector of a rotation,,"One of the exercise in Artin's algebra gives an eigenvector of an element of $SO(3)$, in one possible case. Namely, it is asked to show that If $A=[a_{ij}]$ is a rotation in $SO(3)$, then the vector    $$v=\begin{bmatrix} (a_{23}+a_{32})^{-1}\\ (a_{13}+a_{31})^{-1} \\ (a_{12}+a_{21})^{-1}\end{bmatrix}$$   is an eigenvector of $A$, whenever these entries of the vector are well-defined in $\mathbb{R}$. I couldn't get any way to proceed to prove this. What I tried was, consider $A$ as sum of symmetric and skew-symmetric- $\frac{1}{2}(A+A^t)+\frac{1}{2}(A-A^t)$ and show that $v$ is an eigenvector of both with eigenvalues $1$ and $0$ respectively; but, this was not working beyond long algebraic/symbolic computations. Any hint for this?","One of the exercise in Artin's algebra gives an eigenvector of an element of $SO(3)$, in one possible case. Namely, it is asked to show that If $A=[a_{ij}]$ is a rotation in $SO(3)$, then the vector    $$v=\begin{bmatrix} (a_{23}+a_{32})^{-1}\\ (a_{13}+a_{31})^{-1} \\ (a_{12}+a_{21})^{-1}\end{bmatrix}$$   is an eigenvector of $A$, whenever these entries of the vector are well-defined in $\mathbb{R}$. I couldn't get any way to proceed to prove this. What I tried was, consider $A$ as sum of symmetric and skew-symmetric- $\frac{1}{2}(A+A^t)+\frac{1}{2}(A-A^t)$ and show that $v$ is an eigenvector of both with eigenvalues $1$ and $0$ respectively; but, this was not working beyond long algebraic/symbolic computations. Any hint for this?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'eigenvalues-eigenvectors', 'rotations']"
91,"Prove that for any linear $f$, there is an $m$ such that $\text{Ker}(f^m)\cap\text{Range}(f^m)=\{0\}$","Prove that for any linear , there is an  such that",f m \text{Ker}(f^m)\cap\text{Range}(f^m)=\{0\},"Let $V$ be a finite dimensional vector space. If $f$ is any linear transformation from $V$ to $V$. Prove that there is an integer $m$ such that the intersection of the image of $f^m$ and the kernel of $f^m$ is $\{0\}$. What I've tried: For any linear transformation $f$ we have $f(0)=0$. Thus $f\circ f\circ f\circ ....\circ f(0)=0$(m times) and hence $f^m(0)=0$. Thus $0 \in \ker(f^m) $. And since $f^m(0)=0$ , $0 \in Im(f^m) $. Then $0$ is in the intersection of $Im(f^m)$ and $\ker(f^m)$ Now for proving that their intersection is only $\{0\}$: I'm guessing that this implies that either $Im(f^m)=0$ or $\ker(f^m)=0$. For the first case, this would result in the zero map, but we can't get a zero map from multiple compositions of a non zero map (I think?). This would imply that $\ker(f^m)=0$ for some $m$, and the function would turn injective after $m$ compositions? Is my reasoning correct? Can anyone help me solve this or point me in the right direction? Thanks in advance!","Let $V$ be a finite dimensional vector space. If $f$ is any linear transformation from $V$ to $V$. Prove that there is an integer $m$ such that the intersection of the image of $f^m$ and the kernel of $f^m$ is $\{0\}$. What I've tried: For any linear transformation $f$ we have $f(0)=0$. Thus $f\circ f\circ f\circ ....\circ f(0)=0$(m times) and hence $f^m(0)=0$. Thus $0 \in \ker(f^m) $. And since $f^m(0)=0$ , $0 \in Im(f^m) $. Then $0$ is in the intersection of $Im(f^m)$ and $\ker(f^m)$ Now for proving that their intersection is only $\{0\}$: I'm guessing that this implies that either $Im(f^m)=0$ or $\ker(f^m)=0$. For the first case, this would result in the zero map, but we can't get a zero map from multiple compositions of a non zero map (I think?). This would imply that $\ker(f^m)=0$ for some $m$, and the function would turn injective after $m$ compositions? Is my reasoning correct? Can anyone help me solve this or point me in the right direction? Thanks in advance!",,"['linear-algebra', 'vector-spaces', 'linear-transformations']"
92,"Prove $\bigcap_{i=1}^k\ker(f_i)\subset \ker(f)\iff f\in {\rm span}(f_1,...,f_k) $",Prove,"\bigcap_{i=1}^k\ker(f_i)\subset \ker(f)\iff f\in {\rm span}(f_1,...,f_k) ","Let $V$ a $\mathbb{K}$ -vector space of finite dimension $n$ , with $\{f_1,...,f_n\}$ a set linearly independent of $V^*$ and $f\in V^*$ . Prove $\bigcap_{i=1}^k\ker(f_i)\subset \ker(f)\iff f\in {\rm span}(f_1,...,f_k).$ ( $\Leftarrow$ ) Let $f\in {\rm span}(f_1,...,fk)$ then exists $\alpha_1,...,\alpha_k$ such that $f=\alpha_1f_1+...+\alpha_kf_k$ . Let $f\in\bigcap_{i=1}^k\ker(f_i) $ then $f\in \ker(f_1),...,f\in \ker(f_k)$ . By hypothesis we have if $f\in {\rm span}(f_1,...,fk)$ then exists $\alpha_1,...,\alpha_k$ such that $f=\alpha_1f_1+...+\alpha_kf_k$ Here I'm a little stuck. Can someone help me? ( $\Rightarrow$ ) I do'nt know how to prove this part. Help me, if you can. I will be very grateful.","Let a -vector space of finite dimension , with a set linearly independent of and . Prove ( ) Let then exists such that . Let then . By hypothesis we have if then exists such that Here I'm a little stuck. Can someone help me? ( ) I do'nt know how to prove this part. Help me, if you can. I will be very grateful.","V \mathbb{K} n \{f_1,...,f_n\} V^* f\in V^* \bigcap_{i=1}^k\ker(f_i)\subset \ker(f)\iff f\in {\rm span}(f_1,...,f_k). \Leftarrow f\in {\rm span}(f_1,...,fk) \alpha_1,...,\alpha_k f=\alpha_1f_1+...+\alpha_kf_k f\in\bigcap_{i=1}^k\ker(f_i)  f\in \ker(f_1),...,f\in \ker(f_k) f\in {\rm span}(f_1,...,fk) \alpha_1,...,\alpha_k f=\alpha_1f_1+...+\alpha_kf_k \Rightarrow",['linear-algebra']
93,Solving the equation $15=|135-60t|$ on $0\le{t}\le{5}$; two possible solutions,Solving the equation  on ; two possible solutions,15=|135-60t| 0\le{t}\le{5},"Ann is driving along a highway that passes through Boston. Her distance $d$, in miles, from Boston is given by the equation $d=\vert{135-60t}$| where $t$ is the time in hours since the start of her trip and $0\le{t}\le{5}$. Determine when Ann will be exactly 15 miles from Boston. I have solved the equation $15=|135-60t|$ and I found two solutions by the mean of the absolute value by solving the equation $15=135-60t$ and after the calculation is found that $t=2$; or by solving the equation $-15=135-60t$ and after solving the equation is found that $t=2.5$. I am not clear in understanding the meaning of this exercise. What does the sign minus in the second equation express? When is Ann exactly 15 miles from Boston? I have only found that $t=2$ hours or $t=2.5$ hours. Would you help me please? Thank you in advance.","Ann is driving along a highway that passes through Boston. Her distance $d$, in miles, from Boston is given by the equation $d=\vert{135-60t}$| where $t$ is the time in hours since the start of her trip and $0\le{t}\le{5}$. Determine when Ann will be exactly 15 miles from Boston. I have solved the equation $15=|135-60t|$ and I found two solutions by the mean of the absolute value by solving the equation $15=135-60t$ and after the calculation is found that $t=2$; or by solving the equation $-15=135-60t$ and after solving the equation is found that $t=2.5$. I am not clear in understanding the meaning of this exercise. What does the sign minus in the second equation express? When is Ann exactly 15 miles from Boston? I have only found that $t=2$ hours or $t=2.5$ hours. Would you help me please? Thank you in advance.",,"['linear-algebra', 'absolute-value']"
94,Show that $V=\operatorname{null} T^{n-1}\oplus \operatorname{range} T^{n-1}$,Show that,V=\operatorname{null} T^{n-1}\oplus \operatorname{range} T^{n-1},"This question comes from A. Sheldon's Linear Algebra Done Right, 3rd Edition, Exercises 8.A. Suppose that $T\in\mathcal{L}(V)$ is not nilpotent. Let $n=\dim V$. Show that $V=\operatorname{null} T^{n-1}\oplus \operatorname{range} T^{n-1}$. After pondering over and over this question for a long time, I've tried expanding basis, constructing quotient space, and many others, but I cannot unravel this stuff. Unfortunately, my university hasn't adopted this excellent textbook for linear algebra course, and I have to learn it on my own. I've met many hard exercises in this book but this one seems to defy all my attempts. Any help or hint is welcome, best in the manner of ""Axler's way"", thank you so much.","This question comes from A. Sheldon's Linear Algebra Done Right, 3rd Edition, Exercises 8.A. Suppose that $T\in\mathcal{L}(V)$ is not nilpotent. Let $n=\dim V$. Show that $V=\operatorname{null} T^{n-1}\oplus \operatorname{range} T^{n-1}$. After pondering over and over this question for a long time, I've tried expanding basis, constructing quotient space, and many others, but I cannot unravel this stuff. Unfortunately, my university hasn't adopted this excellent textbook for linear algebra course, and I have to learn it on my own. I've met many hard exercises in this book but this one seems to defy all my attempts. Any help or hint is welcome, best in the manner of ""Axler's way"", thank you so much.",,"['linear-algebra', 'linear-transformations']"
95,Orthogonal projection of point onto line not through origin,Orthogonal projection of point onto line not through origin,,"What is the orthogonal projection of the point $(8,3)$ onto the line $y = -2x - 3$? Using basic analytical methods, I get the point $(-4/5,-7/5)$, but I want to obtain this point by seeing what happens to the standard basis vectors. But I always end up with the matrix $$ \begin{bmatrix}-1&-\frac{8}{5}\\-1&\frac{1}{5}\end{bmatrix}, $$ and $$ \begin{bmatrix}-1&-\frac{8}{5}\\-1&\frac{1}{5}\end{bmatrix}\begin{bmatrix}8\\3\end{bmatrix} =\begin{bmatrix}-\frac{64}{5}\\-\frac{37}{5}\end{bmatrix} \neq\begin{bmatrix}-\frac{4}{5}\\-\frac{7}{5}\end{bmatrix}. $$ Can anyone tell me where I'm going wrong?","What is the orthogonal projection of the point $(8,3)$ onto the line $y = -2x - 3$? Using basic analytical methods, I get the point $(-4/5,-7/5)$, but I want to obtain this point by seeing what happens to the standard basis vectors. But I always end up with the matrix $$ \begin{bmatrix}-1&-\frac{8}{5}\\-1&\frac{1}{5}\end{bmatrix}, $$ and $$ \begin{bmatrix}-1&-\frac{8}{5}\\-1&\frac{1}{5}\end{bmatrix}\begin{bmatrix}8\\3\end{bmatrix} =\begin{bmatrix}-\frac{64}{5}\\-\frac{37}{5}\end{bmatrix} \neq\begin{bmatrix}-\frac{4}{5}\\-\frac{7}{5}\end{bmatrix}. $$ Can anyone tell me where I'm going wrong?",,"['linear-algebra', 'projection']"
96,"Rank of a matrix with entries $1, \dots, n^2$",Rank of a matrix with entries,"1, \dots, n^2","Given $n\geq 2$ , let $M$ be a matrix with entries $1, \dots, n^2$ , e.g., $$\begin{alignat*}{1} A_3 = \begin{pmatrix}       1 & 2 & 3 \\       4 & 5 & 6 \\       7 & 8 & 9       \end{pmatrix} & \qquad A_4 = \begin{pmatrix}       1  &  2 &  3 & 4 \\       5  &  6 &  7 & 8 \\       9  & 10 & 11 & 12\\       13 & 14 & 15 & 16       \end{pmatrix} \end{alignat*}$$ How do I prove that $\operatorname{rank}(M)=2$ ? I am supposed to use only the elementary row operations. For me, the definition of $\operatorname{rank}$ is number of non-zero rows in a row-echelon form . For a specific $n$ , I can actually perform the row operations and obtain the row-echelon form . However, I don't know how to prove it in general. Should I prove it using induction? How do I proceed with the induction step? The matrices $A_n$ and $A_{n+1}$ are completely different! By the way, is there any specific name for this type of matrices?","Given , let be a matrix with entries , e.g., How do I prove that ? I am supposed to use only the elementary row operations. For me, the definition of is number of non-zero rows in a row-echelon form . For a specific , I can actually perform the row operations and obtain the row-echelon form . However, I don't know how to prove it in general. Should I prove it using induction? How do I proceed with the induction step? The matrices and are completely different! By the way, is there any specific name for this type of matrices?","n\geq 2 M 1, \dots, n^2 \begin{alignat*}{1}
A_3 = \begin{pmatrix}
      1 & 2 & 3 \\
      4 & 5 & 6 \\
      7 & 8 & 9
      \end{pmatrix} & \qquad
A_4 = \begin{pmatrix}
      1  &  2 &  3 & 4 \\
      5  &  6 &  7 & 8 \\
      9  & 10 & 11 & 12\\
      13 & 14 & 15 & 16
      \end{pmatrix}
\end{alignat*} \operatorname{rank}(M)=2 \operatorname{rank} n A_n A_{n+1}","['linear-algebra', 'matrices', 'terminology', 'matrix-rank']"
97,Prove that all the powers of these two matrices have the same trace,Prove that all the powers of these two matrices have the same trace,,"Let $A, B \in M_{2}(\mathbb{R})$ be two square matrices such that $$AB  \ne BA$$ and $$A^3 = B^3$$ Prove that $\mbox{Tr} (A^n) = \mbox{Tr} (B^n)$ for all $n \in \mathbb{N}$, where $\mbox{Tr} (\cdot)$ is the trace. First, it's clear that $\det A = \det B$ and $A \ne B$. Also, using Cayley-Hamilton it is enough to prove it for $n=1$. I have played around with Cayley-Hamilton for both matrices, with no result. Any idea is welcome.","Let $A, B \in M_{2}(\mathbb{R})$ be two square matrices such that $$AB  \ne BA$$ and $$A^3 = B^3$$ Prove that $\mbox{Tr} (A^n) = \mbox{Tr} (B^n)$ for all $n \in \mathbb{N}$, where $\mbox{Tr} (\cdot)$ is the trace. First, it's clear that $\det A = \det B$ and $A \ne B$. Also, using Cayley-Hamilton it is enough to prove it for $n=1$. I have played around with Cayley-Hamilton for both matrices, with no result. Any idea is welcome.",,"['linear-algebra', 'matrices']"
98,Show that $(C^1)^\perp =\{ 0\}$ in the inner product space of continuous functions,Show that  in the inner product space of continuous functions,(C^1)^\perp =\{ 0\},"In the book of linear algebra by Werner Greub, at page $195,$ it is asked that Let $C$ be the space of all continuous function in the interval $0\leq t \leq 1$ with the inner product defined as $$(f,g) = \int_0^1 f(t) g(t)dt.$$ If $C^1$ denotes the subspace of all continuous differentiable functions, show that $(C^1)^\perp =\{ 0\}$ . Since I'm dealing with differentiable functions, I thought using integration by parts might help, and I got $$f(1)G(1)  = f(0)G(0) + \int_0^1 f'(t)G(t) dt, $$ where $G' = g $ , but it didn't give me anything useful, as far as I see. Edit: This question is asked in abstract linear algebra book, and I have only taken freshman year Calculus courses about functional analysis, so I would appreciate if you give your answer according to this fact.","In the book of linear algebra by Werner Greub, at page it is asked that Let be the space of all continuous function in the interval with the inner product defined as If denotes the subspace of all continuous differentiable functions, show that . Since I'm dealing with differentiable functions, I thought using integration by parts might help, and I got where , but it didn't give me anything useful, as far as I see. Edit: This question is asked in abstract linear algebra book, and I have only taken freshman year Calculus courses about functional analysis, so I would appreciate if you give your answer according to this fact.","195, C 0\leq t \leq 1 (f,g) = \int_0^1 f(t) g(t)dt. C^1 (C^1)^\perp =\{ 0\} f(1)G(1)  = f(0)G(0) + \int_0^1 f'(t)G(t) dt,  G' = g ","['real-analysis', 'linear-algebra', 'functional-analysis', 'orthogonality']"
99,Find the maximum value of $a-b$ where $4a+5b=61$ and where $a$ and $b$ are positive whole numbers.,Find the maximum value of  where  and where  and  are positive whole numbers.,a-b 4a+5b=61 a b,"I am at a beginners level at maths. I have tried to work out the question myself, but I have seen a more simpler way of getting to the answer in which I haven't understood. Here's the simpler approach to this question: $4a+5b=61$ When $b=1$,  $a=14$ When $b=5$,  $a=9$ When $b=9$,  $a=4$ From the pattern we add $4$ to the value given to b each time and we subtract $5$ from the value given to a each time. That is the shortcut to get all the possible values. Why is this so? The solution goes on to say  when $a=14$ and $b=1$ ; $a-b=13$ which is the maximum value of $a-b$.","I am at a beginners level at maths. I have tried to work out the question myself, but I have seen a more simpler way of getting to the answer in which I haven't understood. Here's the simpler approach to this question: $4a+5b=61$ When $b=1$,  $a=14$ When $b=5$,  $a=9$ When $b=9$,  $a=4$ From the pattern we add $4$ to the value given to b each time and we subtract $5$ from the value given to a each time. That is the shortcut to get all the possible values. Why is this so? The solution goes on to say  when $a=14$ and $b=1$ ; $a-b=13$ which is the maximum value of $a-b$.",,"['linear-algebra', 'optimization', 'integer-programming', 'discrete-optimization', 'linear-diophantine-equations']"
