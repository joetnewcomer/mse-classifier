,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Is $C_0$ dense in $l^{\infty}$,Is  dense in,C_0 l^{\infty},"Is $C_0$ dense in $l^{p}$ with $1\leq p\leq \infty$ where $C_0=\{ (x_n): x_n\rightarrow 0, x_n\in R\}$ . Well I think that if $p<\infty$ is true because by definition if i take $y=(y_n)\in l^p$ then $\sum (y_n)^p <\infty$ so $(y_n)^p \rightarrow 0$ imply $y_n \rightarrow 0$ then I can choose the same $y_n \in C_0\cap l^\infty$ such that $y_n \rightarrow y_n$ and this is the definition of density, for all $z$ in the big set exist one succession $z_n$ in the small set such that $z_n\rightarrow z$ . But I don't know how to do with $p=\infty$ . Please somebody can you help me? Thank you","Is dense in with where . Well I think that if is true because by definition if i take then so imply then I can choose the same such that and this is the definition of density, for all in the big set exist one succession in the small set such that . But I don't know how to do with . Please somebody can you help me? Thank you","C_0 l^{p} 1\leq p\leq \infty C_0=\{ (x_n): x_n\rightarrow 0, x_n\in R\} p<\infty y=(y_n)\in l^p \sum (y_n)^p <\infty (y_n)^p \rightarrow 0 y_n \rightarrow 0 y_n \in C_0\cap l^\infty y_n \rightarrow y_n z z_n z_n\rightarrow z p=\infty","['real-analysis', 'calculus', 'functional-analysis', 'analysis', 'lp-spaces']"
1,Change of variables in proof of Bochner's theorem,Change of variables in proof of Bochner's theorem,,"I have $\phi: \mathbb{R} \to \mathbb{R}$ continuous, bounded, and integrable. I'm going through a book which makes the following calculation: $$\frac{1}{2\pi}\int\limits_{-\infty}^{\infty}e^{-itx}\phi(t)dt = \lim\limits_{T\to \infty} \frac{1}{2\pi}\int\limits_{-T}^{T}\left(1 - \frac{|t|}T \right)e^{-itx}\phi(t)dt =  \lim\limits_{T\to \infty} \frac{1}{2\pi T}\int\limits_{0}^{T} \int\limits_{0}^{T} e^{-i(t-s)x}\phi(t-s)dtds. $$ The first equality holds by the Dominated Convergence Theorem and the second is supposed to hold by a change of variables. Can someone please show me how this change of variables goes? Edit: If it helps, this is in the book of Varadhan in the proof of Bochner's theorem. This computation is being made to use the positive-definiteness of $\phi$ .","I have continuous, bounded, and integrable. I'm going through a book which makes the following calculation: The first equality holds by the Dominated Convergence Theorem and the second is supposed to hold by a change of variables. Can someone please show me how this change of variables goes? Edit: If it helps, this is in the book of Varadhan in the proof of Bochner's theorem. This computation is being made to use the positive-definiteness of .","\phi: \mathbb{R} \to \mathbb{R} \frac{1}{2\pi}\int\limits_{-\infty}^{\infty}e^{-itx}\phi(t)dt = \lim\limits_{T\to \infty}
\frac{1}{2\pi}\int\limits_{-T}^{T}\left(1 - \frac{|t|}T \right)e^{-itx}\phi(t)dt = 
\lim\limits_{T\to \infty}
\frac{1}{2\pi T}\int\limits_{0}^{T} \int\limits_{0}^{T} e^{-i(t-s)x}\phi(t-s)dtds.
 \phi","['real-analysis', 'probability-theory', 'measure-theory', 'probability-distributions', 'fourier-transform']"
2,In which case these integrals are equal?,In which case these integrals are equal?,,"Let $f\in C^1([0,\infty[$ such that $f(0)=0$ and $\forall x \in R^+_0$ , $0\le f'(x)\le1$ . Prove that $$\bigg(\int_0^x f(t) dt\bigg)^2\le\int_0^x f^3(t)dt$$ and find the cases that are equal. I already prove inequality but I can't find the cases for the equality.","Let such that and , . Prove that and find the cases that are equal. I already prove inequality but I can't find the cases for the equality.","f\in C^1([0,\infty[ f(0)=0 \forall x \in R^+_0 0\le f'(x)\le1 \bigg(\int_0^x f(t) dt\bigg)^2\le\int_0^x f^3(t)dt","['real-analysis', 'integration', 'integral-inequality']"
3,Integral of Infinite Sines,Integral of Infinite Sines,,"I constructed the following question: Let $S_n$ denote the sequence where \begin{align} S_1=&\sin{x}\\ S_2=&\sin{(\sin{x})}\\ S_3=&\sin(\sin(\sin{x}))\\ &\vdots \end{align} Evaluate $$I=\int_0^{\pi}\lim_{n\rightarrow\infty}S_n\,dx$$ Messing around in desmos, it would seem that $S_n$ approaches $0$ as $n\rightarrow\infty$ . However I cant seem to be able to prove this. Any ideas?","I constructed the following question: Let denote the sequence where Evaluate Messing around in desmos, it would seem that approaches as . However I cant seem to be able to prove this. Any ideas?","S_n \begin{align}
S_1=&\sin{x}\\
S_2=&\sin{(\sin{x})}\\
S_3=&\sin(\sin(\sin{x}))\\
&\vdots
\end{align} I=\int_0^{\pi}\lim_{n\rightarrow\infty}S_n\,dx S_n 0 n\rightarrow\infty","['real-analysis', 'calculus']"
4,$\int_0^\infty \frac{\sin^n x}{x^m}dx$ could be expressed via $\pi$ or $\log$,could be expressed via  or,\int_0^\infty \frac{\sin^n x}{x^m}dx \pi \log,"I want to show some results first (they were computed by MMA) $$ \int_0^\infty \frac{\sin^5 x}{x^3} dx =\frac{5}{32}{\color{Red}\pi} \quad \int_0^\infty \frac{\sin^5 x}{x^5} dx =\frac{115}{384} {\color{Red}\pi} \\ \int_0^\infty \frac{\sin^5 x}{x^2} dx =\frac{5}{16}\,{\color{Red}\log}\, \frac{27}{5} \quad \int_0^\infty \frac{\sin^5 x}{x^4} dx =-\frac{5}{96}(27\,{\color{Red }\log } \,3-25\,{\color{Red}\log }\,5) \\ $$ and $$ \int_0^\infty \frac{\sin^6 x}{x^4} dx =\frac{1}{8} {\color{Red}\pi} \quad \int_0^\infty \frac{\sin^6 x}{x^6} dx =\frac{11}{40} {\color{Red}\pi} \\ \int_0^\infty \frac{\sin^6 x}{x^3} dx =\frac{3}{16}\,{\color{Red}\log}\, \frac{256}{27} \quad \int_0^\infty \frac{\sin^6 x}{x^5} dx ={\color{Red}\log}\, \frac{3^\frac{27}{16}}{4} \\ $$ As we can see, in the integral $\displaystyle\int_0^\infty \frac{\sin^nx}{x^m}dx$ , if $n-m$ is even, the integral is expressed via $\pi$ , and if $n-m$ is odd, the integral is expressed via $\log$ . Amazing for me, it seems always true such as $$\int_0^\infty \frac{\sin^8 x}{x^2} dx =\frac{5\pi}{32}$$ and $$\int_0^\infty \frac{\sin^8 x}{x^3} dx =\frac{9}{8}\log\frac{4}{3}$$ Do we have a general method to compute $\displaystyle\int_0^\infty \frac{\sin^nx}{x^m}dx$ which implies these laws? My attempt This post tells us how to compute $\displaystyle\int_0^\infty \frac{\sin^n x}{x^n}dx \tag{*}$ We can compute some other $\displaystyle\int_0^\infty \frac{\sin^nx}{x^m}dx$ via $(*)$ , such as via the formulas $$\displaystyle\int_{0}^{\infty}\dfrac{\sin^3 x}{x}\,dx = \dfrac{3}{4}\int_{0}^{\infty}\dfrac{\sin x}{x}\,dx - \dfrac{1}{4}\int_{0}^{\infty}\dfrac{\sin 3x}{x}\,dx$$ and $$\int_0^\infty \frac{\sin^2 (2x)}{x^2}dx=\int_0^\infty \frac{4\sin^2 x-4\sin^4 x}{x^2}$$ But it's complicated to compute the general cases. Could you please  share some ideas of a possible method to show the law mentioned above?","I want to show some results first (they were computed by MMA) and As we can see, in the integral , if is even, the integral is expressed via , and if is odd, the integral is expressed via . Amazing for me, it seems always true such as and Do we have a general method to compute which implies these laws? My attempt This post tells us how to compute We can compute some other via , such as via the formulas and But it's complicated to compute the general cases. Could you please  share some ideas of a possible method to show the law mentioned above?","
\int_0^\infty \frac{\sin^5 x}{x^3} dx =\frac{5}{32}{\color{Red}\pi} \quad
\int_0^\infty \frac{\sin^5 x}{x^5} dx =\frac{115}{384} {\color{Red}\pi} \\
\int_0^\infty \frac{\sin^5 x}{x^2} dx =\frac{5}{16}\,{\color{Red}\log}\, \frac{27}{5} \quad
\int_0^\infty \frac{\sin^5 x}{x^4} dx =-\frac{5}{96}(27\,{\color{Red }\log } \,3-25\,{\color{Red}\log }\,5) \\
 
\int_0^\infty \frac{\sin^6 x}{x^4} dx =\frac{1}{8} {\color{Red}\pi} \quad
\int_0^\infty \frac{\sin^6 x}{x^6} dx =\frac{11}{40} {\color{Red}\pi} \\
\int_0^\infty \frac{\sin^6 x}{x^3} dx =\frac{3}{16}\,{\color{Red}\log}\, \frac{256}{27} \quad
\int_0^\infty \frac{\sin^6 x}{x^5} dx ={\color{Red}\log}\, \frac{3^\frac{27}{16}}{4} \\
 \displaystyle\int_0^\infty \frac{\sin^nx}{x^m}dx n-m \pi n-m \log \int_0^\infty \frac{\sin^8 x}{x^2} dx =\frac{5\pi}{32} \int_0^\infty \frac{\sin^8 x}{x^3} dx =\frac{9}{8}\log\frac{4}{3} \displaystyle\int_0^\infty \frac{\sin^nx}{x^m}dx \displaystyle\int_0^\infty \frac{\sin^n x}{x^n}dx \tag{*} \displaystyle\int_0^\infty \frac{\sin^nx}{x^m}dx (*) \displaystyle\int_{0}^{\infty}\dfrac{\sin^3 x}{x}\,dx = \dfrac{3}{4}\int_{0}^{\infty}\dfrac{\sin x}{x}\,dx - \dfrac{1}{4}\int_{0}^{\infty}\dfrac{\sin 3x}{x}\,dx \int_0^\infty \frac{\sin^2 (2x)}{x^2}dx=\int_0^\infty \frac{4\sin^2 x-4\sin^4 x}{x^2}","['real-analysis', 'calculus', 'complex-analysis', 'multivariable-calculus']"
5,Formalizing a proof for $ \sum_{n=0}^\infty \sum_{k=n}^{\infty} a_k = \sum_{n=0}^{\infty} (n+1)a_n$,Formalizing a proof for, \sum_{n=0}^\infty \sum_{k=n}^{\infty} a_k = \sum_{n=0}^{\infty} (n+1)a_n,"Let $(a_n)_n$ be a sequence of non-negative real numbers. I fail to see a formal proof of the following (conjectured) equality: $$ \sum_{n=0}^\infty \sum_{k=n}^{\infty} a_k = \sum_{n=0}^{\infty} (n+1)a_n.  $$ Roughly speaking the claim follows from expanding the LHS: $$ \sum_{n=0}^\infty \sum_{k=n}^{\infty} a_k = \sum_{k=0}^{\infty} a_k + \sum_{k=1}^{\infty} a_k + \ldots + \sum_{k=i}^{\infty} a_k + \ldots  $$ Now it is easy to see that each term $a_k$ appears in exactly $(k+1)$ terms in RHS (e.g. writing column-wise the different sums), so it is intuitively clear that it has to be $\sum_{n=0}^{\infty} (n+1)a_n$ . However I am not satisfied of this informal hand-waving proof. Any hints/helps to make it formal? Thanks.","Let be a sequence of non-negative real numbers. I fail to see a formal proof of the following (conjectured) equality: Roughly speaking the claim follows from expanding the LHS: Now it is easy to see that each term appears in exactly terms in RHS (e.g. writing column-wise the different sums), so it is intuitively clear that it has to be . However I am not satisfied of this informal hand-waving proof. Any hints/helps to make it formal? Thanks.","(a_n)_n 
\sum_{n=0}^\infty \sum_{k=n}^{\infty} a_k = \sum_{n=0}^{\infty} (n+1)a_n. 
 
\sum_{n=0}^\infty \sum_{k=n}^{\infty} a_k = \sum_{k=0}^{\infty} a_k + \sum_{k=1}^{\infty} a_k + \ldots + \sum_{k=i}^{\infty} a_k + \ldots 
 a_k (k+1) \sum_{n=0}^{\infty} (n+1)a_n","['real-analysis', 'calculus', 'sequences-and-series', 'summation']"
6,Simplifying the result of integration of $\int\frac{e^x + e^{3x}}{1-e^{2x}+e^{4x}}\mathop{dx}$,Simplifying the result of integration of,\int\frac{e^x + e^{3x}}{1-e^{2x}+e^{4x}}\mathop{dx},"Evaluate: $$ \int\frac{e^x + e^{3x}}{1-e^{2x}+e^{4x}}\mathop{dx} $$ I'm trying to simplify my answer so that it matches the keys section, no success so far. The integral itself is pretty simple. Factor $e^x$ in the denominator and then make an obvious substitution: $$ I = \int \frac{e^x(1 + e^{2x})}{1-e^{2x}+e^{4x}}\mathop{dx}\\ t = e^x\, , dt = e^x\mathop{dx}\\ $$ Thus: $$ \begin{align} I&=\int \frac{1+t^2}{1-t^2 + t^4}\mathop{dt} \\ &={1\over 2}\int\left(\frac{1}{t^2 + \sqrt3t + 1} + \frac{1}{t^2-\sqrt3t+1}\right)\mathop{dt} \\ &={1\over 2}\int\left(\frac{1}{\left(t+{\sqrt3\over 2}\right)^2+{1\over 4}} + \frac{1}{\left(t-{\sqrt3\over 2}\right)^2+{1\over 4}}\right)\mathop {dt} \end{align} $$ Which after some further substitutions yields: $$ \boxed{I = \arctan(2e^x+\sqrt3) + \arctan(2e^x-\sqrt3)}\tag1 $$ However, the answer section suggests that: $$ I = \arctan(2\sinh x)\tag2 $$ Which matches my answer up to a constant , $-{\pi \over 2}$ in this case. Even though the answer is correct, I would still like to see how I could arrive from $(1)$ to $(2)$ , I've given it several tries without any luck. I would appreciate it if someone could show me why: $$ \arctan(2\sinh x) = \arctan(2e^x+\sqrt3) + \arctan(2e^x-\sqrt3) - {\pi\over 2} $$ Thank you! As pointed out in the comments by @mickep , there is a way to directly arrive at the desired result. I would like to elaborate on it here. Instead of factoring $e^x$ one could factor $e^{2x}$ which would give: $$ \begin{align} I &= \int \frac{e^{2x}(e^{-x}+e^{x})}{e^{2x}(e^{-2x} - 1 + e^{2x})}\mathop{dx}\\ &= \int \frac{e^{-x}+e^{x}}{e^{-2x} - 1 + e^{2x}}\mathop{dx} \\ &= \int \frac{2(e^{-x}+e^{x})}{2(e^{-2x} - 1 + e^{2x})}\mathop{dx}\\ &= \int \frac{2\cosh x}{e^{-2x} - 1 + e^{2x}}\mathop{dx} \\ &= \int \frac{2\cosh x}{4{e^{-2x} - 2e^{x}e^{-x} + e^{2x}\over 4} + 1}\mathop{dx}\\ &= \int \frac{2\cosh x}{(2\sinh x)^2 + 1}\mathop{dx} \end{align} $$ Now using a substitution $t = 2\sinh x$ , one may obtain: $$ I = \int \frac{\mathop{dt}}{t^2 + 1} = \arctan(t) = \arctan(2\sinh x) + C $$ By this approach, we have arrived at the desired result.","Evaluate: I'm trying to simplify my answer so that it matches the keys section, no success so far. The integral itself is pretty simple. Factor in the denominator and then make an obvious substitution: Thus: Which after some further substitutions yields: However, the answer section suggests that: Which matches my answer up to a constant , in this case. Even though the answer is correct, I would still like to see how I could arrive from to , I've given it several tries without any luck. I would appreciate it if someone could show me why: Thank you! As pointed out in the comments by @mickep , there is a way to directly arrive at the desired result. I would like to elaborate on it here. Instead of factoring one could factor which would give: Now using a substitution , one may obtain: By this approach, we have arrived at the desired result.","
\int\frac{e^x + e^{3x}}{1-e^{2x}+e^{4x}}\mathop{dx}
 e^x 
I = \int \frac{e^x(1 + e^{2x})}{1-e^{2x}+e^{4x}}\mathop{dx}\\
t = e^x\, , dt = e^x\mathop{dx}\\
 
\begin{align}
I&=\int \frac{1+t^2}{1-t^2 + t^4}\mathop{dt} \\
&={1\over 2}\int\left(\frac{1}{t^2 + \sqrt3t + 1} + \frac{1}{t^2-\sqrt3t+1}\right)\mathop{dt} \\
&={1\over 2}\int\left(\frac{1}{\left(t+{\sqrt3\over 2}\right)^2+{1\over 4}} + \frac{1}{\left(t-{\sqrt3\over 2}\right)^2+{1\over 4}}\right)\mathop {dt}
\end{align}
 
\boxed{I = \arctan(2e^x+\sqrt3) + \arctan(2e^x-\sqrt3)}\tag1
 
I = \arctan(2\sinh x)\tag2
 -{\pi \over 2} (1) (2) 
\arctan(2\sinh x) = \arctan(2e^x+\sqrt3) + \arctan(2e^x-\sqrt3) - {\pi\over 2}
 e^x e^{2x} 
\begin{align}
I &= \int \frac{e^{2x}(e^{-x}+e^{x})}{e^{2x}(e^{-2x} - 1 + e^{2x})}\mathop{dx}\\
&= \int \frac{e^{-x}+e^{x}}{e^{-2x} - 1 + e^{2x}}\mathop{dx} \\
&= \int \frac{2(e^{-x}+e^{x})}{2(e^{-2x} - 1 + e^{2x})}\mathop{dx}\\
&= \int \frac{2\cosh x}{e^{-2x} - 1 + e^{2x}}\mathop{dx} \\
&= \int \frac{2\cosh x}{4{e^{-2x} - 2e^{x}e^{-x} + e^{2x}\over 4} + 1}\mathop{dx}\\
&= \int \frac{2\cosh x}{(2\sinh x)^2 + 1}\mathop{dx}
\end{align}
 t = 2\sinh x 
I = \int \frac{\mathop{dt}}{t^2 + 1} = \arctan(t) = \arctan(2\sinh x) + C
","['real-analysis', 'algebra-precalculus']"
7,How to generalize this version of Tarski’s Fixed Point Theorem?,How to generalize this version of Tarski’s Fixed Point Theorem?,,"I could prove the following result from my Real Analysis course: Let $f:[0,1] \rightarrow [0,1]$ be an increasing mapping. Then it has a fixed point. I understand that this is a very baby version of Tarski’s Fixed Point Theorem. Now, I wish to generalize this a little bit and get the following: Let $f:[0,1]^n \rightarrow [0,1]^n$ in which $f$ is increasing in the sense that if $y \geq x$ coordinate wise then $f(y) \geq f(x)$ coordinate wise. Then, f has a fixed point. From my point of view, we could just pick a point $x_0 \in [0,1]^n$ , fix all coordinates but one and apply the above lemma to that coordinate. Then, when the first coordinate of the fixed point is found, we do the same for the second and so on. However, I am not sure this route would be successful and even if it is, I can’t write the extension formally. Any ideas? Thanks a lot in advance!","I could prove the following result from my Real Analysis course: Let be an increasing mapping. Then it has a fixed point. I understand that this is a very baby version of Tarski’s Fixed Point Theorem. Now, I wish to generalize this a little bit and get the following: Let in which is increasing in the sense that if coordinate wise then coordinate wise. Then, f has a fixed point. From my point of view, we could just pick a point , fix all coordinates but one and apply the above lemma to that coordinate. Then, when the first coordinate of the fixed point is found, we do the same for the second and so on. However, I am not sure this route would be successful and even if it is, I can’t write the extension formally. Any ideas? Thanks a lot in advance!","f:[0,1] \rightarrow [0,1] f:[0,1]^n \rightarrow [0,1]^n f y \geq x f(y) \geq f(x) x_0 \in [0,1]^n","['real-analysis', 'general-topology', 'fixed-point-theorems', 'monotone-functions', 'fixed-points']"
8,"Help in proving, that $\int_{0}^{\infty} \frac{1}{\Gamma(x)} d x=e+\int_{0}^{\infty} \frac{e^{-x}}{\pi^{2}+(\ln x)^{2}} d x$ using real methods only","Help in proving, that  using real methods only",\int_{0}^{\infty} \frac{1}{\Gamma(x)} d x=e+\int_{0}^{\infty} \frac{e^{-x}}{\pi^{2}+(\ln x)^{2}} d x,"The above identity is the difference formula for the Fransén-Robinson Constant. Proving this statement gave me severe headaches those last days, since everytime I try to calculate the RHS I either miss the $+e$ term or nothing converges. In the picture you can find one of my attempts. As it seems as of now, the integral in the last line on the left does not converge. I'm completely at a loss. I would appreciate some help alot!!","The above identity is the difference formula for the Fransén-Robinson Constant. Proving this statement gave me severe headaches those last days, since everytime I try to calculate the RHS I either miss the term or nothing converges. In the picture you can find one of my attempts. As it seems as of now, the integral in the last line on the left does not converge. I'm completely at a loss. I would appreciate some help alot!!",+e,"['real-analysis', 'integration', 'improper-integrals', 'gamma-function']"
9,$\frac1n\sum _{k=1}^na_k\to0$ if and only if $\frac1n\sum _{k=1}^na^2_k\to0$ [duplicate],if and only if  [duplicate],\frac1n\sum _{k=1}^na_k\to0 \frac1n\sum _{k=1}^na^2_k\to0,"This question already has answers here : Sequence such that $\lim\limits_{n\to \infty} \frac{x_1^2+x_2^2+...+x_n^2}{n}=0$ (2 answers) Closed 5 years ago . If $(a_n)$ is a sequence in $(0,1)$ , show that $\frac1n\sum _{k=1}^na_k\to0$ if and only if $\frac1n\sum _{k=1}^na^2_k\to0$ My try: $\implies$ : Since $a_k\in (0,1)$ , we have $0\le\frac1n\sum _{k=1}^na^2_k\le \frac1n\sum _{k=1}^na_k$ , and if RHS goes to zero, then by squeeze theorem, $\frac1n\sum _{k=1}^na^2_k\to0$ How to show other direction?","This question already has answers here : Sequence such that $\lim\limits_{n\to \infty} \frac{x_1^2+x_2^2+...+x_n^2}{n}=0$ (2 answers) Closed 5 years ago . If is a sequence in , show that if and only if My try: : Since , we have , and if RHS goes to zero, then by squeeze theorem, How to show other direction?","(a_n) (0,1) \frac1n\sum _{k=1}^na_k\to0 \frac1n\sum _{k=1}^na^2_k\to0 \implies a_k\in (0,1) 0\le\frac1n\sum _{k=1}^na^2_k\le \frac1n\sum _{k=1}^na_k \frac1n\sum _{k=1}^na^2_k\to0","['real-analysis', 'sequences-and-series', 'limits']"
10,Does there exist a real number a given distance from each rational number?,Does there exist a real number a given distance from each rational number?,,"Let $r_n$ be an enumeration of the rational numbers and let $a_n$ be a sequence of positive real numbers that converges to zero. Does there exist $x\in \mathbb{R}$ such that $|x-r_n|>a_n$ for all $n$ ? This problem was inspired by an easier version of the problem where we assume the stronger condition that $\displaystyle\sum_{n=1}^\infty a_n$ converges. I have a simple solution in this particular case but I will spoiler it in case anyone would like to try this too. Let $\Omega$ denote the set of $x\in \mathbb{R}$ that do not satisfy the   given property; we claim that $\Omega \neq \mathbb{R}$ and so an $x\in \mathbb{R}$ with given property exists. Indeed, $\Omega=\{x\in\mathbb{R} \ | \ \exists \ n\in \mathbb{N} \ \mathrm{such \ that} \ |x-r_n| \leq a_n\}=\displaystyle\bigcup_{n=1}^\infty \ [r_n-a_n,r_n+a_n]$ whose Lebesgue measure $\lambda(\Omega)\leq \displaystyle\sum_{n=1}^\infty \lambda([r_n-a_n,r_n+a_n])=\displaystyle\sum_{n=1}^\infty 2a_n < \infty$ , so $\Omega \neq \mathbb{R}$ . Unfortunately it is very specific to this particular case so I doubt it helps with the general case, which I have no idea how to solve. I assume that, unless I'm missing something obvious, it uses some deeper theory (irrationality measure?) that I have not learned. Ideas for the general case or alternative (more elementary) solutions to the easier case would be appreciated.","Let be an enumeration of the rational numbers and let be a sequence of positive real numbers that converges to zero. Does there exist such that for all ? This problem was inspired by an easier version of the problem where we assume the stronger condition that converges. I have a simple solution in this particular case but I will spoiler it in case anyone would like to try this too. Let denote the set of that do not satisfy the   given property; we claim that and so an with given property exists. Indeed, whose Lebesgue measure , so . Unfortunately it is very specific to this particular case so I doubt it helps with the general case, which I have no idea how to solve. I assume that, unless I'm missing something obvious, it uses some deeper theory (irrationality measure?) that I have not learned. Ideas for the general case or alternative (more elementary) solutions to the easier case would be appreciated.","r_n a_n x\in \mathbb{R} |x-r_n|>a_n n \displaystyle\sum_{n=1}^\infty a_n \Omega x\in \mathbb{R} \Omega \neq \mathbb{R} x\in \mathbb{R} \Omega=\{x\in\mathbb{R} \ | \ \exists \ n\in \mathbb{N} \ \mathrm{such \ that} \ |x-r_n| \leq a_n\}=\displaystyle\bigcup_{n=1}^\infty \ [r_n-a_n,r_n+a_n] \lambda(\Omega)\leq \displaystyle\sum_{n=1}^\infty \lambda([r_n-a_n,r_n+a_n])=\displaystyle\sum_{n=1}^\infty 2a_n < \infty \Omega \neq \mathbb{R}","['real-analysis', 'sequences-and-series', 'alternative-proof']"
11,If $f$ is analytic and $f(z)^2$ = $\bar f(z)$ then $f$ is constant,If  is analytic and  =  then  is constant,f f(z)^2 \bar f(z) f,"I'm currently stuck on the following problem. Let f be an analytic function on a non-empty connected open set V. If $f(z)^2$ = $\bar f(z)$ $\forall z\in V$ then f is constant on V. I think I should be working with the Maximum Modulus theorem, but I am not sure how to use it.","I'm currently stuck on the following problem. Let f be an analytic function on a non-empty connected open set V. If = then f is constant on V. I think I should be working with the Maximum Modulus theorem, but I am not sure how to use it.",f(z)^2 \bar f(z) \forall z\in V,"['real-analysis', 'complex-analysis']"
12,Proving the continuity of the Cantor Function,Proving the continuity of the Cantor Function,,"Consider the Cantor Set $C=\{0,1\}^{\omega}$ , that is, the space of all sequences $(b_1,b_2,...)$ with each $b_i\in\{0,1\}$ . Define $g:C\rightarrow[0,1]$ by $$g(b_1,b_2,...)=\sum\limits_{i=1}^{\infty}\dfrac{b_i}{2^i}$$ In other words, $g(b_1,b_2,...)$ is the real number whose digits in base 2 are $0.b_1b_2...$ Prove that $g$ is continuous. Here is my attempt: Let sequences $(a_n)_{n\in\mathbb{N}}$ and $(b_n)_{n\in\mathbb{N}}$ be elements of the Cantor Set $C.$ For fixed $n\in\mathbb{N}$ let $A_n=(a_1,a_2,...,a_n)$ and $B_n=(b_1,b_2,...,b_n)$ be the first $n$ terms in each of those sequences. If $A_n\neq B_n$ , then $\exists m=min\{k\in\{1,2,...,n\}:a_k\neq b_k\}$ and the following holds: $$\begin{array}{c} \left|\sum\limits_{k=1}^n\dfrac{a_k}{2^k}-\sum\limits_{k=1}^n\dfrac{b_k}{2^k}\right|=\left|\sum\limits_{k=1}^n\dfrac{a_k}{2^k}-\dfrac{b_k}{2^k}\right|\geq\dfrac{|a_m-b_m|}{2^m}-\left|\sum\limits_{k=m+1}^n\dfrac{a_k}{2^k}-\dfrac{b_k}{2^k}\right|\geq\\ \geq\dfrac{|a_m-b_m|}{2^m}-\sum\limits_{k=m+1}^n\dfrac{|a_k-b_k|}{2^k}    \\ \text{where $|a_m-b_m|$=1 and}\\ \sum\limits_{k=m+1}^n\dfrac{|a_k-b_k|}{2^k}\leq \sum\limits_{k=m+1}^n\dfrac{1}{2^k}=\dfrac{1}{2^m}\\ \text{Now,}\\ \left|\sum\limits_{k=n+1}^{\infty}\dfrac{a_k}{2^k}-\sum\limits_{k=n+1}^{\infty}\dfrac{b_k}{2^k}\right|=\left|\sum\limits_{k=n+1}^{\infty}\dfrac{a_k-b_k}{2^k}\right|\leq\\ \leq\sum\limits_{k=n+1}^{\infty}\dfrac{|a_k-b_k|}{2^k}\leq\sum\limits_{k=n+1}^{\infty}\dfrac{1}{2^k}=\dfrac{1}{2^n}\\ \text{Therefore, if $A_n\neq B_n$, then}\\ \left|f(a)-f(b)\right|=\left|\sum\limits_{k=1}^{\infty}\dfrac{a_k-b_k}{2^k}\right|=\left|\sum\limits_{k=1}^{n}\dfrac{a_k-b_k}{2^k}+\sum\limits_{k=n+1}^{\infty}\dfrac{a_k-b_k}{2^k}\right|\geq\\ \geq\left|\sum\limits_{k=1}^{n}\dfrac{a_k-b_k}{2^k}\right|-\left|\sum\limits_{k=n+1}^{\infty}\dfrac{a_k-b_k}{2^k}\right|\geq\dfrac{1}{2^n}-\dfrac{1}{2^n}=0 \end{array}$$ I think I must have done something wrong because I wanted $|f(a)-f(b)|$ to be greater than or equal to something in terms of $m$ or $n$ so that I can say choose $n$ (or $m$ ) such that (something like) $\dfrac{1}{2^n}<\varepsilon$ , but I just got 0 instead. How can I improve my proof so that I can achieve this?","Consider the Cantor Set , that is, the space of all sequences with each . Define by In other words, is the real number whose digits in base 2 are Prove that is continuous. Here is my attempt: Let sequences and be elements of the Cantor Set For fixed let and be the first terms in each of those sequences. If , then and the following holds: I think I must have done something wrong because I wanted to be greater than or equal to something in terms of or so that I can say choose (or ) such that (something like) , but I just got 0 instead. How can I improve my proof so that I can achieve this?","C=\{0,1\}^{\omega} (b_1,b_2,...) b_i\in\{0,1\} g:C\rightarrow[0,1] g(b_1,b_2,...)=\sum\limits_{i=1}^{\infty}\dfrac{b_i}{2^i} g(b_1,b_2,...) 0.b_1b_2... g (a_n)_{n\in\mathbb{N}} (b_n)_{n\in\mathbb{N}} C. n\in\mathbb{N} A_n=(a_1,a_2,...,a_n) B_n=(b_1,b_2,...,b_n) n A_n\neq B_n \exists m=min\{k\in\{1,2,...,n\}:a_k\neq b_k\} \begin{array}{c}
\left|\sum\limits_{k=1}^n\dfrac{a_k}{2^k}-\sum\limits_{k=1}^n\dfrac{b_k}{2^k}\right|=\left|\sum\limits_{k=1}^n\dfrac{a_k}{2^k}-\dfrac{b_k}{2^k}\right|\geq\dfrac{|a_m-b_m|}{2^m}-\left|\sum\limits_{k=m+1}^n\dfrac{a_k}{2^k}-\dfrac{b_k}{2^k}\right|\geq\\ \geq\dfrac{|a_m-b_m|}{2^m}-\sum\limits_{k=m+1}^n\dfrac{|a_k-b_k|}{2^k}    \\
\text{where |a_m-b_m|=1 and}\\
\sum\limits_{k=m+1}^n\dfrac{|a_k-b_k|}{2^k}\leq \sum\limits_{k=m+1}^n\dfrac{1}{2^k}=\dfrac{1}{2^m}\\
\text{Now,}\\
\left|\sum\limits_{k=n+1}^{\infty}\dfrac{a_k}{2^k}-\sum\limits_{k=n+1}^{\infty}\dfrac{b_k}{2^k}\right|=\left|\sum\limits_{k=n+1}^{\infty}\dfrac{a_k-b_k}{2^k}\right|\leq\\
\leq\sum\limits_{k=n+1}^{\infty}\dfrac{|a_k-b_k|}{2^k}\leq\sum\limits_{k=n+1}^{\infty}\dfrac{1}{2^k}=\dfrac{1}{2^n}\\
\text{Therefore, if A_n\neq B_n, then}\\
\left|f(a)-f(b)\right|=\left|\sum\limits_{k=1}^{\infty}\dfrac{a_k-b_k}{2^k}\right|=\left|\sum\limits_{k=1}^{n}\dfrac{a_k-b_k}{2^k}+\sum\limits_{k=n+1}^{\infty}\dfrac{a_k-b_k}{2^k}\right|\geq\\
\geq\left|\sum\limits_{k=1}^{n}\dfrac{a_k-b_k}{2^k}\right|-\left|\sum\limits_{k=n+1}^{\infty}\dfrac{a_k-b_k}{2^k}\right|\geq\dfrac{1}{2^n}-\dfrac{1}{2^n}=0
\end{array} |f(a)-f(b)| m n n m \dfrac{1}{2^n}<\varepsilon","['real-analysis', 'general-topology', 'continuity', 'cantor-set']"
13,"Showing $\lim_{n \to \infty} L(f,P_n,[a,b]) = L(f,[a,b])$, where $P_n$ is partition of $[a,b]$ into $2^n$ subintervals","Showing , where  is partition of  into  subintervals","\lim_{n \to \infty} L(f,P_n,[a,b]) = L(f,[a,b]) P_n [a,b] 2^n","Suppose $f : [a,b] \to \mathbb{R}$ is bounded. With a partition $P$ of the form $a = x_0, \dots ,x_n = b$ of $[a,b]$ , the lower Riemann sum is $L(f,P,[a,b]) := \sum_{i=1}^{n} (x_i - x_{i-1}) \inf_{[x_{i-1},x_i]} f$ . Then the lower Riemann integral is $L(f,[a,b]) := \sup_{P} L(f,P,[a,b])$ ; that is, the lower Riemann integral is the supremum over all the lower Riemann sums. Define the sequence $L(f,P_n,[a,b])$ , where $P_n$ is the partition of $[a,b]$ obtained by splitting $[a,b]$ up into $2^n$ intervals of equal size. I want to prove that $\lim_{n \to \infty} L(f,P_n,[a,b]) = L(f,[a,b])$ . Here's my approach so far: Since the list defining partition $P_{n-1}$ is a sublist of the list defining the partition $P_n$ , we have $L(f, P_{n-1}, [a,b]) \leq L(f,P_n, [a,b])$ . That is, the sequence $L(f,P_n,[a,b])$ is monotone non-decreasing. Since it also has an upper bound of $L(f,[a,b])$ , it follows from the monotone convergence theorem that $L(f,P_n,[a,b])$ is a convergent sequence, and it converges to the least upper bound of its terms. I need to prove that this limit is equal to $L(f,[a,b])$ . This is where I am getting stuck. Let the limit of $L(f,P_n,[a,b])$ be $l$ . It is immediate that $l \leq L(f,[a,b])$ , because the supremum of a subset is at most the supremum of the original set. So I need just to show that $L(f,[a,b]) \leq l$ to complete the proof. It is equivalent to show that for all $\epsilon >0$ , we have $L(f,[a,b]) > l - \epsilon$ . To this end, let $\epsilon > 0$ . Since $L(f,[a,b])$ is the supremum of the lower Riemann sums, there exists a partition $P$ of $[a,b]$ such that $L(f,P,[a,b]) > L(f,[a,b]) - \frac{\epsilon}{2}$ . If I can show that there exists $N \in \mathbb{N}$ such that $L(f,P_N, [a,b]) \geq L(f,[a,b])$ and $|L(f,P_N,[a,b]) - l| < \frac{\epsilon}{2}$ , then I am done, since I will have as a consequence that $|l - L(f,[a,b])| < \epsilon$ , by the triangle inequality. Intuitively, I want to use the triangle inequality to show a connection between four 'things'. Firstly, I have the sequence $L(f,P_n,[a,b])$ , which is increasing (or at least non-decreasing) to $L(f,[a,b])$ as $n$ gets big. I know I can approximate $L(f,[a,b])$ , the second thing, with a margin $\epsilon$ of error, by $L(f,P,[a,b])$ , the third thing, for some partition $P$ . Then I just want to show that if I go far enough into the sequence $L(f,P_n,[a,b])$ , the terms eventually get at least as big as $L(f,P,[a,b])$ . Once they are at that threshold, the terms are within an $\epsilon$ margin of error to $L(f,P,[a,b])$ , and using the triangle inequality to get an upper bound on the distance between $l$ , the fourth thing, and $L(f,[a,b])$ , I would be finished. But how do I do this?","Suppose is bounded. With a partition of the form of , the lower Riemann sum is . Then the lower Riemann integral is ; that is, the lower Riemann integral is the supremum over all the lower Riemann sums. Define the sequence , where is the partition of obtained by splitting up into intervals of equal size. I want to prove that . Here's my approach so far: Since the list defining partition is a sublist of the list defining the partition , we have . That is, the sequence is monotone non-decreasing. Since it also has an upper bound of , it follows from the monotone convergence theorem that is a convergent sequence, and it converges to the least upper bound of its terms. I need to prove that this limit is equal to . This is where I am getting stuck. Let the limit of be . It is immediate that , because the supremum of a subset is at most the supremum of the original set. So I need just to show that to complete the proof. It is equivalent to show that for all , we have . To this end, let . Since is the supremum of the lower Riemann sums, there exists a partition of such that . If I can show that there exists such that and , then I am done, since I will have as a consequence that , by the triangle inequality. Intuitively, I want to use the triangle inequality to show a connection between four 'things'. Firstly, I have the sequence , which is increasing (or at least non-decreasing) to as gets big. I know I can approximate , the second thing, with a margin of error, by , the third thing, for some partition . Then I just want to show that if I go far enough into the sequence , the terms eventually get at least as big as . Once they are at that threshold, the terms are within an margin of error to , and using the triangle inequality to get an upper bound on the distance between , the fourth thing, and , I would be finished. But how do I do this?","f : [a,b] \to \mathbb{R} P a = x_0, \dots ,x_n = b [a,b] L(f,P,[a,b]) := \sum_{i=1}^{n} (x_i - x_{i-1}) \inf_{[x_{i-1},x_i]} f L(f,[a,b]) := \sup_{P} L(f,P,[a,b]) L(f,P_n,[a,b]) P_n [a,b] [a,b] 2^n \lim_{n \to \infty} L(f,P_n,[a,b]) = L(f,[a,b]) P_{n-1} P_n L(f, P_{n-1}, [a,b]) \leq L(f,P_n, [a,b]) L(f,P_n,[a,b]) L(f,[a,b]) L(f,P_n,[a,b]) L(f,[a,b]) L(f,P_n,[a,b]) l l \leq L(f,[a,b]) L(f,[a,b]) \leq l \epsilon >0 L(f,[a,b]) > l - \epsilon \epsilon > 0 L(f,[a,b]) P [a,b] L(f,P,[a,b]) > L(f,[a,b]) - \frac{\epsilon}{2} N \in \mathbb{N} L(f,P_N, [a,b]) \geq L(f,[a,b]) |L(f,P_N,[a,b]) - l| < \frac{\epsilon}{2} |l - L(f,[a,b])| < \epsilon L(f,P_n,[a,b]) L(f,[a,b]) n L(f,[a,b]) \epsilon L(f,P,[a,b]) P L(f,P_n,[a,b]) L(f,P,[a,b]) \epsilon L(f,P,[a,b]) l L(f,[a,b])","['real-analysis', 'riemann-integration']"
14,Prove $\frac{2n+\sin(n)}{n+2}$ converges to 2.,Prove  converges to 2.,\frac{2n+\sin(n)}{n+2},"Here's my attempt. Let $\epsilon >0.$ Then $N \geq \frac{5}{\epsilon}$ , with $n \geq N \implies$ $| \frac{2n+\sin(n)}{n+2} -2|=| \frac{2n+\sin(n)-2n-4}{n+2}|=| \frac{\sin(n)-4}{n+2}|= \frac{|\sin(n)-4|}{n+2} \leq \frac{5}{n+2} \leq \frac{5}{n} \leq \epsilon$","Here's my attempt. Let Then , with",\epsilon >0. N \geq \frac{5}{\epsilon} n \geq N \implies | \frac{2n+\sin(n)}{n+2} -2|=| \frac{2n+\sin(n)-2n-4}{n+2}|=| \frac{\sin(n)-4}{n+2}|= \frac{|\sin(n)-4|}{n+2} \leq \frac{5}{n+2} \leq \frac{5}{n} \leq \epsilon,"['real-analysis', 'convergence-divergence']"
15,Proving $\sum_{k=1}^n\vert\cos(k)\vert\ge \frac{n}{4}$ for all $n>0$,Proving  for all,\sum_{k=1}^n\vert\cos(k)\vert\ge \frac{n}{4} n>0,"Show that for all $n\in\mathbb{N}^* \quad \sum_{k=1}^n\vert\cos(k)\vert\ge \frac{n}{4}.$ I can do it using the fact that $\vert\cos(x)\vert\ge \cos(x)^2$ \begin{equation} \sum_{k=1}^n\vert\cos(k)\vert\ge \sum_{k=1}^n\cos(k)^2=\frac{n}{2}+\frac1{2}\Re\big(e^{2i}\frac{1-e^{2in}}{1-e^{2i}}\big)\\ =\frac{n}{2}+\frac{1}{2}\Re\big(e^{i(n+1)}\frac{\sin(n)}{\sin{1}}\big)=\frac{n}{2}+\frac{\cos(n+1)\sin(n)}{2\sin(1)}\ge\frac{n}{2}-\frac{1}{2\sin(1)} \end{equation} I can finish by using ""calculator"" or by hand but I would rather like a ""direct"" proof not using the value of $\sin(1).$ Other methods can be very interesting too.","Show that for all I can do it using the fact that I can finish by using ""calculator"" or by hand but I would rather like a ""direct"" proof not using the value of Other methods can be very interesting too.","n\in\mathbb{N}^* \quad \sum_{k=1}^n\vert\cos(k)\vert\ge \frac{n}{4}. \vert\cos(x)\vert\ge \cos(x)^2 \begin{equation}
\sum_{k=1}^n\vert\cos(k)\vert\ge \sum_{k=1}^n\cos(k)^2=\frac{n}{2}+\frac1{2}\Re\big(e^{2i}\frac{1-e^{2in}}{1-e^{2i}}\big)\\
=\frac{n}{2}+\frac{1}{2}\Re\big(e^{i(n+1)}\frac{\sin(n)}{\sin{1}}\big)=\frac{n}{2}+\frac{\cos(n+1)\sin(n)}{2\sin(1)}\ge\frac{n}{2}-\frac{1}{2\sin(1)}
\end{equation} \sin(1).",['real-analysis']
16,"Show that $ \max\big\{ (1-x)(1-y) , xy , x(1-y) + y(1-x)\big \} \geq \dfrac49$ for $x,y \in [0,1]$.",Show that  for .," \max\big\{ (1-x)(1-y) , xy , x(1-y) + y(1-x)\big \} \geq \dfrac49 x,y \in [0,1]","For $x$ and $y$ in the interval $[0,1]$, show that $$\max\big\{ (1-x)(1-y) , xy  , x(1-y) + y(1-x) \big\}\geq \frac49$$  I have been thinking on this for some time but couldn't get to prove it, any inputs are welcome.","For $x$ and $y$ in the interval $[0,1]$, show that $$\max\big\{ (1-x)(1-y) , xy  , x(1-y) + y(1-x) \big\}\geq \frac49$$  I have been thinking on this for some time but couldn't get to prove it, any inputs are welcome.",,"['real-analysis', 'algebra-precalculus', 'analysis', 'optimization']"
17,Function which satisfies $f(x-f(x))=f(x)-1$,Function which satisfies,f(x-f(x))=f(x)-1,"While trying to solve a particular problem, I got stuck trying to find an unknown function $f(x)$ , $f : \Bbb{R}\to\Bbb{R}$ whose only known property is that it satisfies the following equation: $$f(x-f(x))=f(x)-1$$ Is there a way to determine if the above equation has a unique solution? If $f(x)$ is indeed unique, but its explicit form cannot be determined, is there a way to find its value at some arbitrary point $f(a_0)$?","While trying to solve a particular problem, I got stuck trying to find an unknown function $f(x)$ , $f : \Bbb{R}\to\Bbb{R}$ whose only known property is that it satisfies the following equation: $$f(x-f(x))=f(x)-1$$ Is there a way to determine if the above equation has a unique solution? If $f(x)$ is indeed unique, but its explicit form cannot be determined, is there a way to find its value at some arbitrary point $f(a_0)$?",,"['real-analysis', 'functional-equations']"
18,Cubic root epsilon delta proof,Cubic root epsilon delta proof,,I'm reviewing this epsilon delta proof for the continuity of the cubic root: but I can't see why is so evident that $\sqrt[3]{x^2}+\sqrt[3]{xc}+\sqrt[3]{c^2}\ge \sqrt[3]{c^2}$. Any help please? Thanks in advance.,I'm reviewing this epsilon delta proof for the continuity of the cubic root: but I can't see why is so evident that $\sqrt[3]{x^2}+\sqrt[3]{xc}+\sqrt[3]{c^2}\ge \sqrt[3]{c^2}$. Any help please? Thanks in advance.,,"['real-analysis', 'continuity', 'radicals', 'epsilon-delta']"
19,"Show that the set of non-$\beta$-Hölder functions is dense in the space of $\alpha$-Hölder function, $0<\alpha<\beta<1$","Show that the set of non--Hölder functions is dense in the space of -Hölder function,",\beta \alpha 0<\alpha<\beta<1,"I want to show that $C^\alpha([0,1]) \setminus C^\beta([0,1])$ is dense in $C^\alpha([0,1])$ where $0<\alpha<\beta<1$. I know that I can show that $C^\beta$ is not dense here, using the function $x^\alpha$. To show that it's complement is dense, given some function $f \in C^\alpha$ and $\epsilon > 0$, I think I can use the function $f+\iota W$, where $\iota < \epsilon$ is chosen so that $f+\iota W \in C^\alpha$ but $f+\iota W \notin C^\beta$, where $W$ is Weierstrass' “sawtooth” function. I'm having a hard time making this idea rigorous however, and I was wondering if there are any easy ways to proceed.","I want to show that $C^\alpha([0,1]) \setminus C^\beta([0,1])$ is dense in $C^\alpha([0,1])$ where $0<\alpha<\beta<1$. I know that I can show that $C^\beta$ is not dense here, using the function $x^\alpha$. To show that it's complement is dense, given some function $f \in C^\alpha$ and $\epsilon > 0$, I think I can use the function $f+\iota W$, where $\iota < \epsilon$ is chosen so that $f+\iota W \in C^\alpha$ but $f+\iota W \notin C^\beta$, where $W$ is Weierstrass' “sawtooth” function. I'm having a hard time making this idea rigorous however, and I was wondering if there are any easy ways to proceed.",,"['real-analysis', 'functional-analysis', 'continuity']"
20,An alternative way to find the sum of this series? [duplicate],An alternative way to find the sum of this series? [duplicate],,"This question already has answers here : Calculating $1+\frac13+\frac{1\cdot3}{3\cdot6}+\frac{1\cdot3\cdot5}{3\cdot6\cdot9}+\frac{1\cdot3\cdot5\cdot7}{3\cdot6\cdot9\cdot12}+\dots? $ (7 answers) Closed 5 years ago . $\displaystyle \frac{4}{20}$+$\displaystyle \frac{4.7}{20.30}$+$\displaystyle \frac{4.7.10}{20.30.40}$+... Now  I  have  tried  to  solve  this  in  a  usual  way,  first  find  the  nth  term  $t_n$. $t_n$=  $\displaystyle \frac{1}{10}$($\displaystyle \frac{1+3}{2}$)  +  $\displaystyle \frac{1}{10^2}$($\displaystyle \frac{1+3}{2}$)($\displaystyle \frac{1+6}{3}$)  +  ...+  $\displaystyle \frac{1}{10^n}$($\displaystyle \frac{1+3}{2}$)($\displaystyle \frac{1+6}{3}$)...($\displaystyle \frac{1+3n}{n+1}$) =$\displaystyle \frac{1}{10^n}\prod$(1+$\displaystyle \frac{2r}{r+1}$)  ,  $r=1,2,..,n$ =$\displaystyle \prod$($\displaystyle \frac{3}{10}-\displaystyle \frac{1}{5(r+1)}$) thus,  $t_n=$ (x-$\displaystyle \frac{a}{2}$)(x-$\displaystyle \frac{a}{3}$)...(x-$\displaystyle \frac{a}{n+1}$),  x=$\displaystyle \frac{3}{10}$,  a=$\displaystyle \frac{1}{5}$ Now  to  calculate  $S_n$,  I  have  to  find  the  product  $t_n$,  and  then  take  sum  over  it.  But  this  seems  to  be  a  very  tedious  job.  Is  there  any  elegant  method(may  be  using  the  expansions  of  any  analytic  functions)  to  do  this?","This question already has answers here : Calculating $1+\frac13+\frac{1\cdot3}{3\cdot6}+\frac{1\cdot3\cdot5}{3\cdot6\cdot9}+\frac{1\cdot3\cdot5\cdot7}{3\cdot6\cdot9\cdot12}+\dots? $ (7 answers) Closed 5 years ago . $\displaystyle \frac{4}{20}$+$\displaystyle \frac{4.7}{20.30}$+$\displaystyle \frac{4.7.10}{20.30.40}$+... Now  I  have  tried  to  solve  this  in  a  usual  way,  first  find  the  nth  term  $t_n$. $t_n$=  $\displaystyle \frac{1}{10}$($\displaystyle \frac{1+3}{2}$)  +  $\displaystyle \frac{1}{10^2}$($\displaystyle \frac{1+3}{2}$)($\displaystyle \frac{1+6}{3}$)  +  ...+  $\displaystyle \frac{1}{10^n}$($\displaystyle \frac{1+3}{2}$)($\displaystyle \frac{1+6}{3}$)...($\displaystyle \frac{1+3n}{n+1}$) =$\displaystyle \frac{1}{10^n}\prod$(1+$\displaystyle \frac{2r}{r+1}$)  ,  $r=1,2,..,n$ =$\displaystyle \prod$($\displaystyle \frac{3}{10}-\displaystyle \frac{1}{5(r+1)}$) thus,  $t_n=$ (x-$\displaystyle \frac{a}{2}$)(x-$\displaystyle \frac{a}{3}$)...(x-$\displaystyle \frac{a}{n+1}$),  x=$\displaystyle \frac{3}{10}$,  a=$\displaystyle \frac{1}{5}$ Now  to  calculate  $S_n$,  I  have  to  find  the  product  $t_n$,  and  then  take  sum  over  it.  But  this  seems  to  be  a  very  tedious  job.  Is  there  any  elegant  method(may  be  using  the  expansions  of  any  analytic  functions)  to  do  this?",,"['real-analysis', 'sequences-and-series']"
21,"If $f(\mathbb{R}) \subseteq \mathbb{Q}$, prove that $f$ is constant.","If , prove that  is constant.",f(\mathbb{R}) \subseteq \mathbb{Q} f,"Let $f:\mathbb{R}\to \mathbb{R}$ is a function with $f(\mathbb{R})\subseteq \mathbb{Q}$ such that for every Cauchy sequence of rational numbers $(a_i)$, $\lim_{i\to \infty}f(a_i)$ exists. Prove that $f$ is constant. If I can prove that $f$ is continuous then I am able to do this problem, I am unable to prove the function is continuous. My try: Take $x\in \mathbb{R}$. Then there exists a sequence of rational nmber converging to that $x$, say $(x_n)$. After that can't think of what to do next.","Let $f:\mathbb{R}\to \mathbb{R}$ is a function with $f(\mathbb{R})\subseteq \mathbb{Q}$ such that for every Cauchy sequence of rational numbers $(a_i)$, $\lim_{i\to \infty}f(a_i)$ exists. Prove that $f$ is constant. If I can prove that $f$ is continuous then I am able to do this problem, I am unable to prove the function is continuous. My try: Take $x\in \mathbb{R}$. Then there exists a sequence of rational nmber converging to that $x$, say $(x_n)$. After that can't think of what to do next.",,['real-analysis']
22,Advantages of Lebesgue measurable sets over Borel ones in $L^p$ theory?,Advantages of Lebesgue measurable sets over Borel ones in  theory?,L^p,"Borel $\sigma$-algebra $\mathcal B$ is simply easier to work with (in my opinion) as it behaves well with respect to the topology of our space. Nevertheless, the $\sigma$-algebra $\mathcal M$ of all Lebesgue measurable sets is simply richer. I can conceive that there're probably some interesting stuff going on. Suppose $\Omega\subset\Bbb R^n$ be open domain. What are some major differences between $L^p(\Omega,\mathcal B,\mu)$ and $L^p(\Omega,\mathcal M,\mu)$? Does completeness of $\mathcal M$ allow us to prove some interesting theorems that the structure of $\mathcal B$ is not big enough to support? More generally, what do we gain by replacing $\mathcal B$ with its completion in a more general topological space $X$?","Borel $\sigma$-algebra $\mathcal B$ is simply easier to work with (in my opinion) as it behaves well with respect to the topology of our space. Nevertheless, the $\sigma$-algebra $\mathcal M$ of all Lebesgue measurable sets is simply richer. I can conceive that there're probably some interesting stuff going on. Suppose $\Omega\subset\Bbb R^n$ be open domain. What are some major differences between $L^p(\Omega,\mathcal B,\mu)$ and $L^p(\Omega,\mathcal M,\mu)$? Does completeness of $\mathcal M$ allow us to prove some interesting theorems that the structure of $\mathcal B$ is not big enough to support? More generally, what do we gain by replacing $\mathcal B$ with its completion in a more general topological space $X$?",,"['real-analysis', 'functional-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
23,Defining $N$ in the $\epsilon$-$N$ definiton of convergence,Defining  in the - definiton of convergence,N \epsilon N,"In my Real Analysis class we've been spending some time talking about the $\epsilon$-$N$ definition of convergence. The book we are using, Elementary Analysis by Ross, defines convergence as: A sequence of real numbers $(s_n)$ is said to converge to $s$ if For all $\epsilon>0$, there exists $N\in\mathbb{N}$ such that for all $n\geq N$, $|s_n-s|<\epsilon$. So he defines $N$ to be a natural number, yet in all the examples of convergence, for example proving that $\lim_{n\to\infty}1/n^2=0$, he says let $N=1/\sqrt{\epsilon}$. My professor told us that it does not technically matter, so why do we limit ourselves in the definition to the naturals? Wouldn't it just be easier to say there exists an $N$ in the reals? And if we define $N$ to be in the naturals, yet say let $N=1/\sqrt{\epsilon}$, which is clearly not a natural number, why even mention $N$ being a natural number at all?","In my Real Analysis class we've been spending some time talking about the $\epsilon$-$N$ definition of convergence. The book we are using, Elementary Analysis by Ross, defines convergence as: A sequence of real numbers $(s_n)$ is said to converge to $s$ if For all $\epsilon>0$, there exists $N\in\mathbb{N}$ such that for all $n\geq N$, $|s_n-s|<\epsilon$. So he defines $N$ to be a natural number, yet in all the examples of convergence, for example proving that $\lim_{n\to\infty}1/n^2=0$, he says let $N=1/\sqrt{\epsilon}$. My professor told us that it does not technically matter, so why do we limit ourselves in the definition to the naturals? Wouldn't it just be easier to say there exists an $N$ in the reals? And if we define $N$ to be in the naturals, yet say let $N=1/\sqrt{\epsilon}$, which is clearly not a natural number, why even mention $N$ being a natural number at all?",,"['real-analysis', 'analysis', 'convergence-divergence', 'real-numbers']"
24,Redundancy in the statement of the Monotone Convergence Theorem,Redundancy in the statement of the Monotone Convergence Theorem,,"Below is the common statement of the Monotone Convergence Theorem. Suppose $\{f_n\}$ is a sequence of non-negative measurable functions with $f_n(x) \leq f_{n+1}(x)$ a.e. and $\lim \limits_{n\rightarrow\infty} f_n(x) = f(x)$. Then $$\lim \limits_{n\rightarrow\infty} \int f_n = \int f$$ But proof requires only $f_n(x) \leq f(x)$ since it is sufficient to conclude that $\int f_n \leq \int f$ for all $n$ by monotonicity. So what is the value of the more restrictive statement than it could be (except that it leads to a nice name for the theorem). Edit: Proof. By Fatou's lemma $\int f \leq \lim \limits_{n\rightarrow\infty} \inf \int f_n$. Now, given $f_n(x) \leq f(x)$ it follows that $\int f_n \leq \int f$ for all $n$ by monotonicity. Hence $\lim \limits_{n\rightarrow\infty} \sup \int f_n \leq \int f$. Which combined with Fatou's lemma gives the desired result.","Below is the common statement of the Monotone Convergence Theorem. Suppose $\{f_n\}$ is a sequence of non-negative measurable functions with $f_n(x) \leq f_{n+1}(x)$ a.e. and $\lim \limits_{n\rightarrow\infty} f_n(x) = f(x)$. Then $$\lim \limits_{n\rightarrow\infty} \int f_n = \int f$$ But proof requires only $f_n(x) \leq f(x)$ since it is sufficient to conclude that $\int f_n \leq \int f$ for all $n$ by monotonicity. So what is the value of the more restrictive statement than it could be (except that it leads to a nice name for the theorem). Edit: Proof. By Fatou's lemma $\int f \leq \lim \limits_{n\rightarrow\infty} \inf \int f_n$. Now, given $f_n(x) \leq f(x)$ it follows that $\int f_n \leq \int f$ for all $n$ by monotonicity. Hence $\lim \limits_{n\rightarrow\infty} \sup \int f_n \leq \int f$. Which combined with Fatou's lemma gives the desired result.",,"['real-analysis', 'lebesgue-integral']"
25,"Show $\int_Af(x)\,dx=0$ for every measurable subset $A$ of $[0,1]$.",Show  for every measurable subset  of .,"\int_Af(x)\,dx=0 A [0,1]","Let $f$ be Lebesgue-integrable on $[0,1]$. Suppose $\int_a^bf(x)\,dx=0$ for all $0\leq a\leq b\leq 1$. Show $\int_Af(x)\,dx=0$ for every measurable subset $A$ of $[0,1]$. *Let $A$ be a measurable subset of $[0,1]$. Then $A$ can be written as the union of disjoint, countable? intervals. Since $\int_a^bf(x)\,dx=0$ for all $0\leq a\leq b\leq 1$, each integral of $f$ over each interval is $0$ so $\int_Af(x)\,dx=0$. I'm not sure if I did it right...","Let $f$ be Lebesgue-integrable on $[0,1]$. Suppose $\int_a^bf(x)\,dx=0$ for all $0\leq a\leq b\leq 1$. Show $\int_Af(x)\,dx=0$ for every measurable subset $A$ of $[0,1]$. *Let $A$ be a measurable subset of $[0,1]$. Then $A$ can be written as the union of disjoint, countable? intervals. Since $\int_a^bf(x)\,dx=0$ for all $0\leq a\leq b\leq 1$, each integral of $f$ over each interval is $0$ so $\int_Af(x)\,dx=0$. I'm not sure if I did it right...",,"['real-analysis', 'measure-theory', 'lebesgue-integral']"
26,Is every seminorm induced by a linear operator into a normed space?,Is every seminorm induced by a linear operator into a normed space?,,"I'm reading an analysis textbook chapter on convex topological vector spaces, and there is this statement that (one of) the most common way(s) to define a topology on a vector space $X$ is by requiring the continuity of certain linear maps, (i.e via the semi-norms induced by given linear maps $T\colon X\to Y$ where $Y$ is a normed space). This makes sense, but then there's this that bugs me: ""Can any semi-norm on $X$ be shown to be induced by some linear map T, to some normed linear space?"" I can't find a counter-example to this, or show that it is true. Any insight would be appreciated.","I'm reading an analysis textbook chapter on convex topological vector spaces, and there is this statement that (one of) the most common way(s) to define a topology on a vector space $X$ is by requiring the continuity of certain linear maps, (i.e via the semi-norms induced by given linear maps $T\colon X\to Y$ where $Y$ is a normed space). This makes sense, but then there's this that bugs me: ""Can any semi-norm on $X$ be shown to be induced by some linear map T, to some normed linear space?"" I can't find a counter-example to this, or show that it is true. Any insight would be appreciated.",,"['real-analysis', 'functional-analysis', 'normed-spaces', 'topological-vector-spaces', 'locally-convex-spaces']"
27,"Is there an example for $\|f_n-f\|_p\rightarrow 0$, but $\|f_n\|_p\nrightarrow \|f\|_p$, $0<p<1$","Is there an example for , but ,",\|f_n-f\|_p\rightarrow 0 \|f_n\|_p\nrightarrow \|f\|_p 0<p<1,"I know that if $p\geq1$, $\|f_n-f\|_p\rightarrow 0$ implies $\|f_n\|_p\rightarrow \|f\|_p$, since then Minkowski inequality holds for the $L^p$ norm ($p\geq1$).  Is there an example for $\|f_n-f\|_p\rightarrow 0$, but $\|f_n\|_p\nrightarrow \|f\|_p$, when $0<p<1$ and $\|f_n\|_p, \|f\|_p<\infty$? Here $\|f\|_p=(\int_\Omega|f(x)|^p \mathrm{d}x)^{1/p}$. Nathanael Skrepek's anwer reminded me that though the Minkowski inequality does not hold when $p<1$, the Cr inequality still implies the convergence.","I know that if $p\geq1$, $\|f_n-f\|_p\rightarrow 0$ implies $\|f_n\|_p\rightarrow \|f\|_p$, since then Minkowski inequality holds for the $L^p$ norm ($p\geq1$).  Is there an example for $\|f_n-f\|_p\rightarrow 0$, but $\|f_n\|_p\nrightarrow \|f\|_p$, when $0<p<1$ and $\|f_n\|_p, \|f\|_p<\infty$? Here $\|f\|_p=(\int_\Omega|f(x)|^p \mathrm{d}x)^{1/p}$. Nathanael Skrepek's anwer reminded me that though the Minkowski inequality does not hold when $p<1$, the Cr inequality still implies the convergence.",,"['real-analysis', 'lp-spaces']"
28,Evaluate $\lim\limits_{n \rightarrow \infty}\frac1{n^2}\sum\limits_{k=1}^n\sin\left (\frac{\pi k}n\right)\varphi(k)$,Evaluate,\lim\limits_{n \rightarrow \infty}\frac1{n^2}\sum\limits_{k=1}^n\sin\left (\frac{\pi k}n\right)\varphi(k),"Evaluate $$\lim_{n \rightarrow \infty}\frac{1}{n^2} \sum_{k=1}^{n}   \sin \left (\frac{\pi k}{n} \right)  \varphi(k)$$ where $\varphi$ denotes Euler's totient function. I first tried simplifying the sum $$\sum_{k=1}^{n}   \sin \left (\frac{\pi k}{n} \right)  \varphi(k)$$ by converting to exponentials: $\sin(x) = \dfrac{e^{ix}-e^{-ix}}{2i}$, so the sum is $$\sum_{k=1}^{n}   \sin \left (\frac{\pi k}{n} \right)  \varphi(k) = \sum_{k=1}^n \dfrac{e^{i \cdot \frac{\pi k}{n}}-e^{-i \cdot \frac{\pi k}{n}}}{2i} \varphi(k),$$ but I didn't see how to use this to simplify the sum. How can we calculate the limit?","Evaluate $$\lim_{n \rightarrow \infty}\frac{1}{n^2} \sum_{k=1}^{n}   \sin \left (\frac{\pi k}{n} \right)  \varphi(k)$$ where $\varphi$ denotes Euler's totient function. I first tried simplifying the sum $$\sum_{k=1}^{n}   \sin \left (\frac{\pi k}{n} \right)  \varphi(k)$$ by converting to exponentials: $\sin(x) = \dfrac{e^{ix}-e^{-ix}}{2i}$, so the sum is $$\sum_{k=1}^{n}   \sin \left (\frac{\pi k}{n} \right)  \varphi(k) = \sum_{k=1}^n \dfrac{e^{i \cdot \frac{\pi k}{n}}-e^{-i \cdot \frac{\pi k}{n}}}{2i} \varphi(k),$$ but I didn't see how to use this to simplify the sum. How can we calculate the limit?",,"['real-analysis', 'number-theory']"
29,Is the space of all bounded Hölder continuous functions a Banach space?,Is the space of all bounded Hölder continuous functions a Banach space?,,"Let $(\Omega,d)$ be a non complete metric space, and  consider the space $C^{\gamma}_b(\Omega, \mathbb{R})$, $0<\gamma<1$ of all bounded Hölder continuous functions. Is $C^{\gamma}_b(\Omega, \mathbb{R})$  endowed with the norm $$ \|f\|_{\gamma}=\|f\|_0+\sup_{x\neq y} \dfrac{|f(x)-f(y)|}{d(x,y)^{\gamma}} $$ a Banach Space? Could someone provide me some reference?","Let $(\Omega,d)$ be a non complete metric space, and  consider the space $C^{\gamma}_b(\Omega, \mathbb{R})$, $0<\gamma<1$ of all bounded Hölder continuous functions. Is $C^{\gamma}_b(\Omega, \mathbb{R})$  endowed with the norm $$ \|f\|_{\gamma}=\|f\|_0+\sup_{x\neq y} \dfrac{|f(x)-f(y)|}{d(x,y)^{\gamma}} $$ a Banach Space? Could someone provide me some reference?",,"['real-analysis', 'functional-analysis', 'reference-request', 'metric-spaces']"
30,"Show that $\lim_{n\to \infty}\sin{n\pi x} =0$ if $x\in \mathbb{Z},$ but the limit fails to exist if $x\notin \mathbb{Z}.$",Show that  if  but the limit fails to exist if,"\lim_{n\to \infty}\sin{n\pi x} =0 x\in \mathbb{Z}, x\notin \mathbb{Z}.","Show that $\lim_{n\to \infty}\sin{n\pi x} =0$ if $x\in \mathbb{Z},$ but the limit fails to exist if $x\notin \mathbb{Z}.$ 1st part If $x\in \mathbb{Z}$ then $\sin{n\pi x}=0$ for all $n,$ giving the first part. Edit: 2nd part If $x\notin \mathbb{Z},$ I want to show that the limit doesn't exist. How to do that?",Show that if but the limit fails to exist if 1st part If then for all giving the first part. Edit: 2nd part If I want to show that the limit doesn't exist. How to do that?,"\lim_{n\to \infty}\sin{n\pi x} =0 x\in \mathbb{Z}, x\notin \mathbb{Z}. x\in \mathbb{Z} \sin{n\pi x}=0 n, x\notin \mathbb{Z},","['calculus', 'real-analysis', 'limits']"
31,Please critique my proof that $\sqrt{12}$ is irrational,Please critique my proof that  is irrational,\sqrt{12},"I would like critiques on correctness, conciseness, and clarity. Thanks! Proposition: There is no rational number whose square is 12 Proof: Suppose there were such a number, $a = \in \mathbb{Q}$ s.t. $a^2 = 12$. This implies that $\exists$ $m, n \in \mathbb{Z}$ s.t. $\frac{m^2}{n^2} = 12.$ Assume without loss of generality that $m,~ n$ have no factors in common. $\Rightarrow m^2 = 12n^2$. This implies that $m^2$ is even, and therefore that $m$ is even; it can thus be written $2k = m$ for some $k \in \mathbb{Z}$. Thus $m^2 = 12n^2 $ $\Rightarrow 4k^2 = 12n^2 $ $\Rightarrow \frac{k^2}{3} = n^2$ Because $n^2$ is an integer, it is clear that $3$ divides $k^2$ which imples that $k$ has $3$ or $\frac{k}{n}$ has a factor (because $\frac{k^2}{n^2}= 3$) Suppose that the former is true, and $3$ is a factor of $k$. Then $k = 3j$ for some integer j, which implies that $(3j)^2 = 3n^2$ $\Rightarrow 9j^2 = 3n^2 $ $\Rightarrow n^2 = 3j^2 $ $\Rightarrow n^2 = \frac{k^2}{n^2}j^2$ $\Rightarrow k = \frac{n^2}{j}$ but this implies that $j$ divides $n^2$, but $j$ divides $m$, and by initial assumption $n$ and $m$ have no factors in common, so this is a contradiction. Suppose now that $\frac{k}{n}$ is a factor of k. Then $k = \frac{k}{n}j$ for some integer $j$. Then $(\frac{k}{n}j)^2 = 3n^2$ which implies that $3j^2 = 3n^2 \Rightarrow j^2 = n^2 \Rightarrow j = n$. But this means that $n$ divides $m$, which again is a contradiction. Thus any rational representation of the number whose square equals $12$ leads to a contradiction and this number must therefore have no rational representation.","I would like critiques on correctness, conciseness, and clarity. Thanks! Proposition: There is no rational number whose square is 12 Proof: Suppose there were such a number, $a = \in \mathbb{Q}$ s.t. $a^2 = 12$. This implies that $\exists$ $m, n \in \mathbb{Z}$ s.t. $\frac{m^2}{n^2} = 12.$ Assume without loss of generality that $m,~ n$ have no factors in common. $\Rightarrow m^2 = 12n^2$. This implies that $m^2$ is even, and therefore that $m$ is even; it can thus be written $2k = m$ for some $k \in \mathbb{Z}$. Thus $m^2 = 12n^2 $ $\Rightarrow 4k^2 = 12n^2 $ $\Rightarrow \frac{k^2}{3} = n^2$ Because $n^2$ is an integer, it is clear that $3$ divides $k^2$ which imples that $k$ has $3$ or $\frac{k}{n}$ has a factor (because $\frac{k^2}{n^2}= 3$) Suppose that the former is true, and $3$ is a factor of $k$. Then $k = 3j$ for some integer j, which implies that $(3j)^2 = 3n^2$ $\Rightarrow 9j^2 = 3n^2 $ $\Rightarrow n^2 = 3j^2 $ $\Rightarrow n^2 = \frac{k^2}{n^2}j^2$ $\Rightarrow k = \frac{n^2}{j}$ but this implies that $j$ divides $n^2$, but $j$ divides $m$, and by initial assumption $n$ and $m$ have no factors in common, so this is a contradiction. Suppose now that $\frac{k}{n}$ is a factor of k. Then $k = \frac{k}{n}j$ for some integer $j$. Then $(\frac{k}{n}j)^2 = 3n^2$ which implies that $3j^2 = 3n^2 \Rightarrow j^2 = n^2 \Rightarrow j = n$. But this means that $n$ divides $m$, which again is a contradiction. Thus any rational representation of the number whose square equals $12$ leads to a contradiction and this number must therefore have no rational representation.",,"['real-analysis', 'elementary-number-theory', 'solution-verification', 'proof-writing', 'rationality-testing']"
32,Book on Lipschitz continuity,Book on Lipschitz continuity,,"I have managed to end up in a fourth-year differential equations class without ever needing to take analysis (really think they messed the pre-reqs up). I know the notions of bounded, closed, compact, and convergent sequences, continuity and even uniform continuity. But give me a ball of radius $r$ at $x$ and all I know how to do with it is play soccer. I would like a good book that lays out different types of continuity and proves their relationship, preferably also with examples being applied to actual functions on the reals. (Turns out there is like 6 different types of continuity. And I really don’t want to drop this class if I can help it.)","I have managed to end up in a fourth-year differential equations class without ever needing to take analysis (really think they messed the pre-reqs up). I know the notions of bounded, closed, compact, and convergent sequences, continuity and even uniform continuity. But give me a ball of radius $r$ at $x$ and all I know how to do with it is play soccer. I would like a good book that lays out different types of continuity and proves their relationship, preferably also with examples being applied to actual functions on the reals. (Turns out there is like 6 different types of continuity. And I really don’t want to drop this class if I can help it.)",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'continuity', 'book-recommendation']"
33,What is the idea to show this property of a function based on its Fourier transform?,What is the idea to show this property of a function based on its Fourier transform?,,"Let $f\in \mathcal{M}(\mathbb{R})$ be a function of moderate decrease such that the Fourier transform $\mathfrak{F}(f)$ is continuous and satisfies: $$\mathfrak{F}(f)(\xi) = O\left(\dfrac{1}{|\xi|^{1+\alpha}}\right), \quad \text{as $|\xi|\to\infty$}$$ where $\alpha \in (0,1)$. I want to show that $f$ satisfies the Hölder condition of order $\alpha$: $$|f(x+h)-f(x)|\leq M|h|^\alpha, \quad \text{with $M> 0$ and for all $x,h\in \mathbb{R}$}.$$ Now the condition on $\mathfrak{F}(f)$ says that it is continuous and furthermore, there's $K\in \mathbb{R}$ such that if $|\xi|> K$ we have $$\mathfrak{F}(f)(\xi)\leq \dfrac{A}{|\xi|^{1+\alpha}}, \quad \text{for all $|\xi|> K$ and for some $A\in \mathbb{R}$}.$$ Now, I really have no idea on what to do here. The obvious initial step is to write: $$|f(x+h)-f(x)| = \left|\int_{-\infty}^{\infty} \mathfrak{F}(f)(\xi)e^{2\pi i (x+h)\xi}d\xi-\int_{-\infty}^{\infty}\mathfrak{F}(f)(\xi)e^{2\pi i x\xi}d\xi\right|$$ So that we have $$|f(x+h)-f(x)| \leq \int_{-\infty}^{\infty} |\mathfrak{F}(f)(\xi)| | e^{2\pi i h\xi}-1|d\xi.$$ Now one suggestion is to break this integral as follows: $$|f(x+h)-f(x)|\leq \int_{|\xi|\leq 1/|h|} |\mathfrak{F}(f)(\xi)||e^{2\pi ih\xi -1}|d\xi + \int_{|\xi|\geq 1/|h|} |\mathfrak{F}(f)(\xi)||e^{2\pi ih\xi -1}|d\xi. $$ And now I'm completely stuck. First of all, I really have no idea or intuition whatsoever on what has to be done . My only guess would be to use continuity on the compact $[-1/|h|,1/|h|]$ to bound $\mathfrak{F}(f)$ there, but this would not produce the desired $|h|^\alpha$. On the other integral I can't also use the other condition over $\mathfrak{F}(f)$ because the integral is for $|\xi| \geq 1/|h|$ not $|\xi| > K$. Anyway, I'm quite lost here. So how can I show this? But much more importantly than how to prove this , how can I actually arrive at the proof? How can I have the idea on what has to be done? What is the right way to think about this problem, in order to solve it? I'm much more interested here in how should I reason about this, and how can I have the idea of how to prove it.","Let $f\in \mathcal{M}(\mathbb{R})$ be a function of moderate decrease such that the Fourier transform $\mathfrak{F}(f)$ is continuous and satisfies: $$\mathfrak{F}(f)(\xi) = O\left(\dfrac{1}{|\xi|^{1+\alpha}}\right), \quad \text{as $|\xi|\to\infty$}$$ where $\alpha \in (0,1)$. I want to show that $f$ satisfies the Hölder condition of order $\alpha$: $$|f(x+h)-f(x)|\leq M|h|^\alpha, \quad \text{with $M> 0$ and for all $x,h\in \mathbb{R}$}.$$ Now the condition on $\mathfrak{F}(f)$ says that it is continuous and furthermore, there's $K\in \mathbb{R}$ such that if $|\xi|> K$ we have $$\mathfrak{F}(f)(\xi)\leq \dfrac{A}{|\xi|^{1+\alpha}}, \quad \text{for all $|\xi|> K$ and for some $A\in \mathbb{R}$}.$$ Now, I really have no idea on what to do here. The obvious initial step is to write: $$|f(x+h)-f(x)| = \left|\int_{-\infty}^{\infty} \mathfrak{F}(f)(\xi)e^{2\pi i (x+h)\xi}d\xi-\int_{-\infty}^{\infty}\mathfrak{F}(f)(\xi)e^{2\pi i x\xi}d\xi\right|$$ So that we have $$|f(x+h)-f(x)| \leq \int_{-\infty}^{\infty} |\mathfrak{F}(f)(\xi)| | e^{2\pi i h\xi}-1|d\xi.$$ Now one suggestion is to break this integral as follows: $$|f(x+h)-f(x)|\leq \int_{|\xi|\leq 1/|h|} |\mathfrak{F}(f)(\xi)||e^{2\pi ih\xi -1}|d\xi + \int_{|\xi|\geq 1/|h|} |\mathfrak{F}(f)(\xi)||e^{2\pi ih\xi -1}|d\xi. $$ And now I'm completely stuck. First of all, I really have no idea or intuition whatsoever on what has to be done . My only guess would be to use continuity on the compact $[-1/|h|,1/|h|]$ to bound $\mathfrak{F}(f)$ there, but this would not produce the desired $|h|^\alpha$. On the other integral I can't also use the other condition over $\mathfrak{F}(f)$ because the integral is for $|\xi| \geq 1/|h|$ not $|\xi| > K$. Anyway, I'm quite lost here. So how can I show this? But much more importantly than how to prove this , how can I actually arrive at the proof? How can I have the idea on what has to be done? What is the right way to think about this problem, in order to solve it? I'm much more interested here in how should I reason about this, and how can I have the idea of how to prove it.",,"['real-analysis', 'integration', 'functional-analysis', 'fourier-analysis', 'problem-solving']"
34,"Prove that if $a \ge c$ for all $c < b$, then $a \geq b$","Prove that if  for all , then",a \ge c c < b a \geq b,"Let $a$ and $b$ be elements in an ordered field, prove that if $a \ge c$ for every $c$ such that $c \lt b$, then $a\ge b$. My proof idea below: Let $S = \{x | x<b\}$. Then $a$ is an upper bound for $S$. If I can show that $b$ is the least upper bound for $S$, then it follows from the definition of least upper bound that $a\ge b$. However, I have a hard time proving the claim that $b$ is the least upper bound for $S$. Am I on the right direction? Can anyone help? Thank you.","Let $a$ and $b$ be elements in an ordered field, prove that if $a \ge c$ for every $c$ such that $c \lt b$, then $a\ge b$. My proof idea below: Let $S = \{x | x<b\}$. Then $a$ is an upper bound for $S$. If I can show that $b$ is the least upper bound for $S$, then it follows from the definition of least upper bound that $a\ge b$. However, I have a hard time proving the claim that $b$ is the least upper bound for $S$. Am I on the right direction? Can anyone help? Thank you.",,['real-analysis']
35,Which functions are continuous but nowhere Holder continuous for 0<a<1?,Which functions are continuous but nowhere Holder continuous for 0<a<1?,,"Can somebody provide an example of a function that is continuous on [0,1] but nowhere Holder continuous with degree $\alpha$ ? Why is the function continuous but nowhere Holder continuous? By nowhere Holder continuous, I mean that $\frac{|f(x + t_n) - f(x)|}{|t_n|^\alpha} \rightarrow +\infty$ where $t_n$ is a sequence which $\rightarrow 0 $ as $n\rightarrow +\infty$ .","Can somebody provide an example of a function that is continuous on [0,1] but nowhere Holder continuous with degree ? Why is the function continuous but nowhere Holder continuous? By nowhere Holder continuous, I mean that where is a sequence which as .",\alpha \frac{|f(x + t_n) - f(x)|}{|t_n|^\alpha} \rightarrow +\infty t_n \rightarrow 0  n\rightarrow +\infty,"['real-analysis', 'functions', 'continuity', 'examples-counterexamples', 'holder-spaces']"
36,Baby Rudin Chapter 1 Exercise 9: Complex Field ordering,Baby Rudin Chapter 1 Exercise 9: Complex Field ordering,,"Problem: Suppose $z=a+bi$, $w=c+di$. Define $z<w$ if $a<c$, and also if $a=c$ but $b<d$. Prove that this turns the set of all complex numbers into an ordered set. Does this ordered set has the least-upper-bound property? In my attempt, I've been using Rudin's definition of an ordered field as a proof structure: Definition 1.17: An ordered field is a field $F$ which is also an ordered set , such that: $(1)$ $x+y<x+z$ if $x, y, z \in F$ and $y<z$ $(2)$ $xy>0$ if $x\in F, y \in F, x> 0,$ and $y>0$. There are some propositions about ordered fields Rudin provides as well which I can prove, but I'm stuck on a part of proving $(2)$. My attempt on $(2)$: Let $x =a+bi$, $y=c+di$. Since $0$ = $(0,0)$ or $0 + 0i$, by the problem's defined ordering, $x>0$ if $a > 0$ OR $a = 0$ and $b>0$. And $y>0$ if the same properties hold for $c, d$. $xy = (ac-bd) + (ad+bc)i$. There are four cases to consider if both $x >0$ and $y>0$: (1) $a >0, c>0$. What if $ac-bd<0$? $ac > 0$ but if $bd > ac$, then $xy$ is not greater than $0 + 0i$. (2) $a > 0, c = 0$ but $d > 0$. $ac = 0, bc = 0$, so $xy = -bd + (ad)i$. If $-bd < 0$, then $xy$ is not greater than $0 + 0i$. And so forth. Would appreciate any clarification on this. For the least-upper-bound property, does it suffice to state than for any non-empty subset $E = \{(c, d)\ :\ c, d \in \mathbb{R}\}$ of the set of complex numbers, we observe that all complex numbers are an ordered pair of real numbers and the real field has the least-upper-bound property so we can always find a complex number $(a, b)$, $a, b \in \mathbb{R}$ such that $a$ and $b$ are supremums of the respective sets $\{c\}$ and $\{d\}$ as defined for subset $E$?","Problem: Suppose $z=a+bi$, $w=c+di$. Define $z<w$ if $a<c$, and also if $a=c$ but $b<d$. Prove that this turns the set of all complex numbers into an ordered set. Does this ordered set has the least-upper-bound property? In my attempt, I've been using Rudin's definition of an ordered field as a proof structure: Definition 1.17: An ordered field is a field $F$ which is also an ordered set , such that: $(1)$ $x+y<x+z$ if $x, y, z \in F$ and $y<z$ $(2)$ $xy>0$ if $x\in F, y \in F, x> 0,$ and $y>0$. There are some propositions about ordered fields Rudin provides as well which I can prove, but I'm stuck on a part of proving $(2)$. My attempt on $(2)$: Let $x =a+bi$, $y=c+di$. Since $0$ = $(0,0)$ or $0 + 0i$, by the problem's defined ordering, $x>0$ if $a > 0$ OR $a = 0$ and $b>0$. And $y>0$ if the same properties hold for $c, d$. $xy = (ac-bd) + (ad+bc)i$. There are four cases to consider if both $x >0$ and $y>0$: (1) $a >0, c>0$. What if $ac-bd<0$? $ac > 0$ but if $bd > ac$, then $xy$ is not greater than $0 + 0i$. (2) $a > 0, c = 0$ but $d > 0$. $ac = 0, bc = 0$, so $xy = -bd + (ad)i$. If $-bd < 0$, then $xy$ is not greater than $0 + 0i$. And so forth. Would appreciate any clarification on this. For the least-upper-bound property, does it suffice to state than for any non-empty subset $E = \{(c, d)\ :\ c, d \in \mathbb{R}\}$ of the set of complex numbers, we observe that all complex numbers are an ordered pair of real numbers and the real field has the least-upper-bound property so we can always find a complex number $(a, b)$, $a, b \in \mathbb{R}$ such that $a$ and $b$ are supremums of the respective sets $\{c\}$ and $\{d\}$ as defined for subset $E$?",,[]
37,Arzela-Ascoli Theorem: Is only pointwise boundedness required?,Arzela-Ascoli Theorem: Is only pointwise boundedness required?,,"In Royden's text the Arzela-Ascoli Theorem states: Let X be a compact metric space and $f_n$ a uniformly bounded , equicontinuous sequence of real valued functions on X. Then $f_n$ has a subsequence that converges uniformly on X, to a continuous function f on X. However I cannot seem to see where the hypothesis of uniform boundedness is used in the proof - it seems that only pointwise boundedness of the sequence is required. My question: is uniform boundedness actually required, or could we replace ""uniformly bounded"" with ""pointwise bounded"", in the statement of the theorem? And if we cannot: what is an example of a pointwise but not uniformly bounded sequence for which the theorem fails?","In Royden's text the Arzela-Ascoli Theorem states: Let X be a compact metric space and $f_n$ a uniformly bounded , equicontinuous sequence of real valued functions on X. Then $f_n$ has a subsequence that converges uniformly on X, to a continuous function f on X. However I cannot seem to see where the hypothesis of uniform boundedness is used in the proof - it seems that only pointwise boundedness of the sequence is required. My question: is uniform boundedness actually required, or could we replace ""uniformly bounded"" with ""pointwise bounded"", in the statement of the theorem? And if we cannot: what is an example of a pointwise but not uniformly bounded sequence for which the theorem fails?",,['real-analysis']
38,"Real Analysis, Folland Theorem 1.18 Borel measures on the real line","Real Analysis, Folland Theorem 1.18 Borel measures on the real line",,"Background information - We fix a complete Lebesgue-Stiltjes measure $\mu$ on $\mathbb{R}$ associated to the increasing right continuous function $F$ , and we denote by $M_{\mu}$ , the domain of $\mu$ . 1.17 Lemma - For any $E\in M_{\mu}$ , $$\mu(E) = \inf\{\sum_{1}^{\infty}\mu((a_j,b_j)):E\subset \bigcup_{1}^{\infty}(a_j,b_j)$$ Theorem 1.18 - If $E\in M_{\mu}$ , then \begin{align*} \mu(E) &= \inf\{\mu(U):E\subset U, U \ \text{open}\}\\ &=\sup\{\mu(K):E\subset K, K \ \text{compact}\}\end{align*} Proof - By lemma 1.17, for any $\epsilon > 0$ there exists intervals $(a_j,b_j)$ such that $E\subset \bigcup_{1}^{\infty}(a_j,b_j)$ and $\sum_{1}^{\infty}\mu((a_j,b_j))\leq \mu(E) + \epsilon$ . If $U = \bigcup_{1}^{\infty}(a_j,b_j)$ then $U$ is open, $E\subset U$ , and $\mu(U)\leq \sum_{1}^{\infty}\mu((a_j,b_j))\leq \mu(E) + \epsilon$ . Since $E\subset U$ then $\mu(E)\leq \mu(U)$ so the first part is done. I am not sure how to prove the second part, specifically following follands proof I am confused how when we choose $\overline{E}\setminus E\subset U$ such that $\mu(U)\leq \mu(\overline{E}\setminus E) + \epsilon$ . Then if we let $K = \overline{E}\setminus U$ this somehow makes $K$ compact?","Background information - We fix a complete Lebesgue-Stiltjes measure on associated to the increasing right continuous function , and we denote by , the domain of . 1.17 Lemma - For any , Theorem 1.18 - If , then Proof - By lemma 1.17, for any there exists intervals such that and . If then is open, , and . Since then so the first part is done. I am not sure how to prove the second part, specifically following follands proof I am confused how when we choose such that . Then if we let this somehow makes compact?","\mu \mathbb{R} F M_{\mu} \mu E\in M_{\mu} \mu(E) = \inf\{\sum_{1}^{\infty}\mu((a_j,b_j)):E\subset \bigcup_{1}^{\infty}(a_j,b_j) E\in M_{\mu} \begin{align*}
\mu(E) &= \inf\{\mu(U):E\subset U, U \ \text{open}\}\\
&=\sup\{\mu(K):E\subset K, K \ \text{compact}\}\end{align*} \epsilon > 0 (a_j,b_j) E\subset \bigcup_{1}^{\infty}(a_j,b_j) \sum_{1}^{\infty}\mu((a_j,b_j))\leq \mu(E) + \epsilon U = \bigcup_{1}^{\infty}(a_j,b_j) U E\subset U \mu(U)\leq \sum_{1}^{\infty}\mu((a_j,b_j))\leq \mu(E) + \epsilon E\subset U \mu(E)\leq \mu(U) \overline{E}\setminus E\subset U \mu(U)\leq \mu(\overline{E}\setminus E) + \epsilon K = \overline{E}\setminus U K","['real-analysis', 'measure-theory']"
39,Definition of positive measure,Definition of positive measure,,Why Rudin assumes that $\mu(A)<\infty$ for at least one $A\in \mathfrak{M}$? What about if $\mu(A)=\infty$ for any $A\in\mathfrak{M}$?,Why Rudin assumes that $\mu(A)<\infty$ for at least one $A\in \mathfrak{M}$? What about if $\mu(A)=\infty$ for any $A\in\mathfrak{M}$?,,"['real-analysis', 'measure-theory']"
40,"Real Analysis, Folland problem 1.4.19 Outer Measures","Real Analysis, Folland problem 1.4.19 Outer Measures",,"Background information: Exercise 18 -  Let $\mathcal{A}\subset P(X)$ be an algebra, $\mathcal{A}_\sigma$ the collection of countable unions of sets in $\mathcal{A}$, and $\mathcal{A}_{\sigma\delta}$ the collection of countable intersections of sets in $\mathcal{A}_\sigma$. Let $\mu_{0}$ be a premeasure on $\mathcal{A}$ and $\mu^*$ the induced outer measure. a.) For any $E\subset X$ and $\epsilon > 0$ there exists $A\in \mathcal{A}_\sigma$ with $E\subset A$ and $\mu^*(A) \leq \mu^*(E) + \epsilon$. b.) If $\mu^{*}(E) < \infty$, then $E$ is $\mu^{*}$-measurable if and only if there exists $B\in A_{\sigma\delta}$ with $E\subset B$ and $\mu^{*}(B\setminus E) = 0$ c.) If $\mu_0$ is $\sigma$-finite, the restriction $\mu^{*}(E) < \infty$ in (b) is superfluous a.) By the definition of outermeasure we know that $$\mu^{*}(E) = \inf\left\{ \sum_{j=1}^{\infty} \mu_{0} ( A_{j} ) : A_{j} \in \mathcal{A}, E \subset \bigcup_{j=1}^{\infty} A_{j} \right \}$$ Let $A = \bigcup_{j=1}^{\infty} A_{j}$ as above. Then $A \in \mathcal{A}_{\sigma}$ and $E \subset A$. For each $j$ we can construct a sequence $\{ B_{j}^{k} \} _{k=1}^{\infty}\subset A_\sigma$ such that $A_j\subset \bigcup_{j,k=1}^{\infty}B_{j}^{k}$. It follows that since $$\mu^{*}(A_j) = \inf\left\{ \sum_{j=1}^{\infty} \mu_{0} (B_{j}^{k} ) : B_{j}^{k} \in \mathcal{A}, A_j \subset \bigcup_{j,k=1}^{\infty} B_{j}^{k} \right \}$$ We have that, $$\mu^{*}(A_{j}) \leq \mu_{0}(A_{j}) + \epsilon 2^{-j}, \forall j \ \ \text{and} \ \ \epsilon>0$$ Thus: $$\mu^{*}(A) \leq  \sum_{j=1}^{\infty} \mu^{*} ( A_{j} ) \leq \sum_{j=1}^{\infty} ( \mu_{0}(A_{j}) + \epsilon 2^{-j}) = \mu^{*}(E) + \epsilon$$ Since $\epsilon$ is arbitrary we are done. b.) Suppose $E$ is $\mu^{*}$-measurable. From part (a), we know that $\forall n\in\mathbb{N}$ there exists $B_n\in A_\sigma$ with $E\subset B_n$ and $\mu^{*}(B_n) - \mu^{*}(E) \leq 1/n$. Let, $$B = \bigcap_{n = 1}^{\infty}B_n\in A_\sigma$$ since E is $\mu^{*}$-measurable, we have $\mu^{*}(B_n) = \mu^{*}(B_n\cap E) + \mu^{*}(B_n\cap E^{c})$ hence, $$\mu^{*}(B\cap E^{c})\leq \mu^{*}(B_n\cap E^{c})= \mu^{*}(B_n) - \mu^{*}(E)\leq 1/n$$ for every $n\in\mathbb{N}$. Hence we have $\mu^{*}(B\setminus E) = 0$\ To show the converse, let's suppose $B\in A_{\sigma\delta}$ with $E\subset B$ and $\mu^{*}(B\setminus E) = 0$. From part (a), we know that $\forall n\in\mathbb{N}$ there exists $A_n\in A_\sigma$ with $(B\setminus E)\subset A_n$ and $\mu^{*}(A_n) - \mu^{*}(B\setminus E)\leq 1/n$. But, since $\mu^{*}(B\setminus E) = 0$ then, $\mu^{*}(A_n)\leq 1/n$. Let, $$A = \bigcap_{n=1}^{\infty}A_n$$ then A is $\mu^{*}$-measurable (since $A\in A_{\sigma\delta}$ and the set of all $\mu^{*}$-measurable sets is a $\sigma$-algebra) such that $(B\setminus E)\subset A$ and $\mu^{*}(A) = 0$.\ By Carathedors's theorem we know that the restriction of $\mu^{*}$ to $\mu^{*}$-measurable sets is a complete measure. From this, we know that $(B\setminus E)$ is $\mu^{*}$-measurable. Also, since $B\in A_{\sigma\delta}$ then $B$ is also $\mu^{*}$-measurable and we can express $E$ as $$E = (B^{c}\cup (B\cap E^{c}))^{c}$$ Thus $E$ is $\mu^{*}$-measurable. c.) Let $\mu_0$ be $\sigma$-finite, then let $$X = \bigcup_{1}^{\infty}X_i$$ where $X_i\in M$ and $\mu(X_i) < \infty$ Now, suppose $E$ is $\mu^{*}$-measurable and $\mu^{*}(E) = \infty$, set $$E_n = (E\cap \bigcup_{1}^{n}X_i)$$ then $\mu^{*}(E_n) < \infty$ and $E = \bigcup_{1}^{\infty}E_n$. Let $\epsilon > 0$, from part (a) $\forall n\in\mathbb{N} \exists C_n\in A_\sigma$ such that $E_n\subset C_n$ and $$\mu^{*}(C_n\setminus E_n) = \mu^{*}(C_n) - \mu^{*}(E_n) \leq \epsilon/2^{-n}$$ Let $$C_{\epsilon} = \bigcup_{1}^{\infty}C_n$$ The $\epsilon$ in $C_\epsilon$ is there to stress the dependence upon $\epsilon$ of $C_n$'s and hence of $C$. Notice that, $C_\epsilon\in A_\delta$ being the countable union of unions of elements of $\mathcal{A}$ Intuitively, $C_\epsilon$ is a decent approximation pf $E$: $$\mu^{*}(C_\epsilon\cap E^{c}) = \mu^{*}\left(\bigcup_{1}^{\infty}(C_n\cap E^{c})\right) = \mu^{*}\left(\bigcup_{1}^{\infty}(C_n\cap E_n^{c})\right) \leq \sum_{1}^{\infty}\mu^{*}(C_n\setminus E_n) \leq \epsilon$$ Let's go back to our intuition: $C_{\epsilon}$ is an $\epsilon$-good approximation of $E$, therefore we would like to send $\epsilon$ to $0$ to get the best possible approximation. Since we want to end up in $\mathcal{A}_{\sigma \delta}$ what we need to do is to ""discretize"" the sets $C_{\epsilon}$ considering instead the sets $C_{\frac{1}{n}}$.Then clearly $$C := \bigcap_{n = 1}^{\infty}C_{\frac{1}{n}} \in \mathcal{A}_{\sigma \delta}$$ and satisfies $\mu^*(C \setminus E) = 0$, proving the claim. To show this last equality notice that $$\mu^*(C \setminus E) \le \mu^*(C_{\frac{1}{n}} \setminus E) \le \frac{1}{n} \to 0.$$ Exercise 19 - Let $\mu^*$ be an outer measure on $X$ induced from a finite premeasure $\mu_0$. If $E\subset X$, define the inner measure of $E$ to be $\mu_*(E) = \mu_0(X) - \mu^*(E^c)$. Then $E$ is $\mu^*$-measurable iff $\mu^*(E) = \mu_*(E)$ (Use Exercise 18). Since $E\supset X$ then $E$ is $\mu^*$-measurable says $$\mu^*(X) = \mu^*(E) + \mu^*(E\cap X^c)$$ then $$\mu^*(E) = \mu^*(X) - \mu^*(E\cap X^c)$$ As you can see we have that $\mu^*(E)$ which is the outer measure of $E$ is equal to the inner measure of $E$. But I am not sure if this what we need for the $\Rightarrow$ part of the proof since the inner measure of $E$ is defined in a different way.","Background information: Exercise 18 -  Let $\mathcal{A}\subset P(X)$ be an algebra, $\mathcal{A}_\sigma$ the collection of countable unions of sets in $\mathcal{A}$, and $\mathcal{A}_{\sigma\delta}$ the collection of countable intersections of sets in $\mathcal{A}_\sigma$. Let $\mu_{0}$ be a premeasure on $\mathcal{A}$ and $\mu^*$ the induced outer measure. a.) For any $E\subset X$ and $\epsilon > 0$ there exists $A\in \mathcal{A}_\sigma$ with $E\subset A$ and $\mu^*(A) \leq \mu^*(E) + \epsilon$. b.) If $\mu^{*}(E) < \infty$, then $E$ is $\mu^{*}$-measurable if and only if there exists $B\in A_{\sigma\delta}$ with $E\subset B$ and $\mu^{*}(B\setminus E) = 0$ c.) If $\mu_0$ is $\sigma$-finite, the restriction $\mu^{*}(E) < \infty$ in (b) is superfluous a.) By the definition of outermeasure we know that $$\mu^{*}(E) = \inf\left\{ \sum_{j=1}^{\infty} \mu_{0} ( A_{j} ) : A_{j} \in \mathcal{A}, E \subset \bigcup_{j=1}^{\infty} A_{j} \right \}$$ Let $A = \bigcup_{j=1}^{\infty} A_{j}$ as above. Then $A \in \mathcal{A}_{\sigma}$ and $E \subset A$. For each $j$ we can construct a sequence $\{ B_{j}^{k} \} _{k=1}^{\infty}\subset A_\sigma$ such that $A_j\subset \bigcup_{j,k=1}^{\infty}B_{j}^{k}$. It follows that since $$\mu^{*}(A_j) = \inf\left\{ \sum_{j=1}^{\infty} \mu_{0} (B_{j}^{k} ) : B_{j}^{k} \in \mathcal{A}, A_j \subset \bigcup_{j,k=1}^{\infty} B_{j}^{k} \right \}$$ We have that, $$\mu^{*}(A_{j}) \leq \mu_{0}(A_{j}) + \epsilon 2^{-j}, \forall j \ \ \text{and} \ \ \epsilon>0$$ Thus: $$\mu^{*}(A) \leq  \sum_{j=1}^{\infty} \mu^{*} ( A_{j} ) \leq \sum_{j=1}^{\infty} ( \mu_{0}(A_{j}) + \epsilon 2^{-j}) = \mu^{*}(E) + \epsilon$$ Since $\epsilon$ is arbitrary we are done. b.) Suppose $E$ is $\mu^{*}$-measurable. From part (a), we know that $\forall n\in\mathbb{N}$ there exists $B_n\in A_\sigma$ with $E\subset B_n$ and $\mu^{*}(B_n) - \mu^{*}(E) \leq 1/n$. Let, $$B = \bigcap_{n = 1}^{\infty}B_n\in A_\sigma$$ since E is $\mu^{*}$-measurable, we have $\mu^{*}(B_n) = \mu^{*}(B_n\cap E) + \mu^{*}(B_n\cap E^{c})$ hence, $$\mu^{*}(B\cap E^{c})\leq \mu^{*}(B_n\cap E^{c})= \mu^{*}(B_n) - \mu^{*}(E)\leq 1/n$$ for every $n\in\mathbb{N}$. Hence we have $\mu^{*}(B\setminus E) = 0$\ To show the converse, let's suppose $B\in A_{\sigma\delta}$ with $E\subset B$ and $\mu^{*}(B\setminus E) = 0$. From part (a), we know that $\forall n\in\mathbb{N}$ there exists $A_n\in A_\sigma$ with $(B\setminus E)\subset A_n$ and $\mu^{*}(A_n) - \mu^{*}(B\setminus E)\leq 1/n$. But, since $\mu^{*}(B\setminus E) = 0$ then, $\mu^{*}(A_n)\leq 1/n$. Let, $$A = \bigcap_{n=1}^{\infty}A_n$$ then A is $\mu^{*}$-measurable (since $A\in A_{\sigma\delta}$ and the set of all $\mu^{*}$-measurable sets is a $\sigma$-algebra) such that $(B\setminus E)\subset A$ and $\mu^{*}(A) = 0$.\ By Carathedors's theorem we know that the restriction of $\mu^{*}$ to $\mu^{*}$-measurable sets is a complete measure. From this, we know that $(B\setminus E)$ is $\mu^{*}$-measurable. Also, since $B\in A_{\sigma\delta}$ then $B$ is also $\mu^{*}$-measurable and we can express $E$ as $$E = (B^{c}\cup (B\cap E^{c}))^{c}$$ Thus $E$ is $\mu^{*}$-measurable. c.) Let $\mu_0$ be $\sigma$-finite, then let $$X = \bigcup_{1}^{\infty}X_i$$ where $X_i\in M$ and $\mu(X_i) < \infty$ Now, suppose $E$ is $\mu^{*}$-measurable and $\mu^{*}(E) = \infty$, set $$E_n = (E\cap \bigcup_{1}^{n}X_i)$$ then $\mu^{*}(E_n) < \infty$ and $E = \bigcup_{1}^{\infty}E_n$. Let $\epsilon > 0$, from part (a) $\forall n\in\mathbb{N} \exists C_n\in A_\sigma$ such that $E_n\subset C_n$ and $$\mu^{*}(C_n\setminus E_n) = \mu^{*}(C_n) - \mu^{*}(E_n) \leq \epsilon/2^{-n}$$ Let $$C_{\epsilon} = \bigcup_{1}^{\infty}C_n$$ The $\epsilon$ in $C_\epsilon$ is there to stress the dependence upon $\epsilon$ of $C_n$'s and hence of $C$. Notice that, $C_\epsilon\in A_\delta$ being the countable union of unions of elements of $\mathcal{A}$ Intuitively, $C_\epsilon$ is a decent approximation pf $E$: $$\mu^{*}(C_\epsilon\cap E^{c}) = \mu^{*}\left(\bigcup_{1}^{\infty}(C_n\cap E^{c})\right) = \mu^{*}\left(\bigcup_{1}^{\infty}(C_n\cap E_n^{c})\right) \leq \sum_{1}^{\infty}\mu^{*}(C_n\setminus E_n) \leq \epsilon$$ Let's go back to our intuition: $C_{\epsilon}$ is an $\epsilon$-good approximation of $E$, therefore we would like to send $\epsilon$ to $0$ to get the best possible approximation. Since we want to end up in $\mathcal{A}_{\sigma \delta}$ what we need to do is to ""discretize"" the sets $C_{\epsilon}$ considering instead the sets $C_{\frac{1}{n}}$.Then clearly $$C := \bigcap_{n = 1}^{\infty}C_{\frac{1}{n}} \in \mathcal{A}_{\sigma \delta}$$ and satisfies $\mu^*(C \setminus E) = 0$, proving the claim. To show this last equality notice that $$\mu^*(C \setminus E) \le \mu^*(C_{\frac{1}{n}} \setminus E) \le \frac{1}{n} \to 0.$$ Exercise 19 - Let $\mu^*$ be an outer measure on $X$ induced from a finite premeasure $\mu_0$. If $E\subset X$, define the inner measure of $E$ to be $\mu_*(E) = \mu_0(X) - \mu^*(E^c)$. Then $E$ is $\mu^*$-measurable iff $\mu^*(E) = \mu_*(E)$ (Use Exercise 18). Since $E\supset X$ then $E$ is $\mu^*$-measurable says $$\mu^*(X) = \mu^*(E) + \mu^*(E\cap X^c)$$ then $$\mu^*(E) = \mu^*(X) - \mu^*(E\cap X^c)$$ As you can see we have that $\mu^*(E)$ which is the outer measure of $E$ is equal to the inner measure of $E$. But I am not sure if this what we need for the $\Rightarrow$ part of the proof since the inner measure of $E$ is defined in a different way.",,"['real-analysis', 'measure-theory']"
41,Example of an open map on $\mathbb{R}^2$ that is not a submersion,Example of an open map on  that is not a submersion,\mathbb{R}^2,"I am searching for an example of a map $f : \mathbb{R}^2 \to \mathbb{R}$ that is open but is not a submersion... I know that any constant map is not a submersion, but it is indeed closed, I am wondering for an example where $f$ is an open map. I appreciate any help!","I am searching for an example of a map $f : \mathbb{R}^2 \to \mathbb{R}$ that is open but is not a submersion... I know that any constant map is not a submersion, but it is indeed closed, I am wondering for an example where $f$ is an open map. I appreciate any help!",,['real-analysis']
42,"Are these functions Riemann integrable on $[0,1]$ using this theorem?",Are these functions Riemann integrable on  using this theorem?,"[0,1]","The question asks to whether the theorem ""bounded function $f$ on $[a,b]$ that is continuous on $(a,b]$ is Riemann integrable"" to explain whether these functions are Riemann integrable: a) $\sin^2(\frac 1x)$ b) $\frac 1x\cdot\sin(\frac 1x)$ c) $\ln x$ I think the answer is Yes, No, No. For the first one, the function is clearly bounded and continuous on $(0,1)$. For the second one, I'm not entirely sure because the function is continuous on $(0,1]$ but is it bounded? The last one the function is unbounded. Could someone confirm and explain to me more clearly? Much appreciated!","The question asks to whether the theorem ""bounded function $f$ on $[a,b]$ that is continuous on $(a,b]$ is Riemann integrable"" to explain whether these functions are Riemann integrable: a) $\sin^2(\frac 1x)$ b) $\frac 1x\cdot\sin(\frac 1x)$ c) $\ln x$ I think the answer is Yes, No, No. For the first one, the function is clearly bounded and continuous on $(0,1)$. For the second one, I'm not entirely sure because the function is continuous on $(0,1]$ but is it bounded? The last one the function is unbounded. Could someone confirm and explain to me more clearly? Much appreciated!",,"['calculus', 'real-analysis', 'integration', 'analysis', 'riemann-integration']"
43,Limit L'Hopital's Rule $\lim_{x\to 0 } \frac {\tan^2x - \arctan^2x}{x^4}$,Limit L'Hopital's Rule,\lim_{x\to 0 } \frac {\tan^2x - \arctan^2x}{x^4},$$\lim_{x\to 0 } \frac {\tan^2x - \arctan^2x}{x^4}$$ I am having trouble calculating the above limit.  You could apply L'Hopital's rule 5 times battling with various terms but there must be a simpler way to express tanx / arctanx in another form. Anyone care to help?,$$\lim_{x\to 0 } \frac {\tan^2x - \arctan^2x}{x^4}$$ I am having trouble calculating the above limit.  You could apply L'Hopital's rule 5 times battling with various terms but there must be a simpler way to express tanx / arctanx in another form. Anyone care to help?,,"['calculus', 'real-analysis', 'limits', 'trigonometry', 'limits-without-lhopital']"
44,Differences between derivatives and strong derivatives,Differences between derivatives and strong derivatives,,"Definition: Let $f$ be a real valued function. We say $f$ is $\mathbf{strongly}$ $\mathbf{differentiable}$ at $x = a$ if the following limits exists and is finite: $$ \lim_{x \to a, y \to a, x \neq y} \frac{ f(x)-f(y)}{x-y} = f^*(a) $$ and we can $f^*(a)$ the strong derivative of $f$ at $a$. Why is this definition of derivative different than the usual one? What is the main crucial point to understand here that makes it different?","Definition: Let $f$ be a real valued function. We say $f$ is $\mathbf{strongly}$ $\mathbf{differentiable}$ at $x = a$ if the following limits exists and is finite: $$ \lim_{x \to a, y \to a, x \neq y} \frac{ f(x)-f(y)}{x-y} = f^*(a) $$ and we can $f^*(a)$ the strong derivative of $f$ at $a$. Why is this definition of derivative different than the usual one? What is the main crucial point to understand here that makes it different?",,['calculus']
45,"Why *all* $\epsilon > 0$, in the $\varepsilon-\delta$ limit definition?","Why *all* , in the  limit definition?",\epsilon > 0 \varepsilon-\delta,"Definition of $\lim_{x \to a} f(x) = L$ : $\forall \epsilon > 0, \exists \delta > 0 s.t. |f(x) - L| < \epsilon$ $ if \ 0 < |x-a| < \delta$ Question : Why can't we weaken the assumption to $\exists N > 0$ s.t. $\forall \epsilon \in (0, N), \exists \delta > 0 s.t. |f(x) - L| < \epsilon$ $ if \ 0 < |x-a| < \delta$ ? I think they are not equivalent. If they are, please explain how the latter proves the former and why we still need to have case 1 below. Consider proving $\forall \epsilon > 0, \exists \delta > 0 s.t. |x^2 - 25| < \epsilon$ $if \ 0 < |x-5| < \delta$ We first try to find some $\delta$ . $|x^2 - 25|$ $ = |x - 5| |x + 5| < \epsilon$ if we maybe choose $\delta$ s.t. ...: Let $M > 0$ (further restrictions may be needed). If $|x-5| < M$ , then we have $$- M < x-5 < M$$ $$\to 5 - M < x < 5 + M$$ $$\to 10 - M < x + 5 < 10 + M$$ $$\to (-10 - M) < 10 - M < x + 5 < 10 + M$$ $$|x + 5| < 10 + M$$ So we might choose $\delta = \min\{M, \frac{\epsilon}{10+M} \}$ for the two cases in the proof (it seems no further restrictions on M are needed). Proof: Let $\epsilon > 0$ . Case 1: $$\epsilon > M(10+M)$$ $$\delta = M$$ $$\to |x - 5| |x + 5| < M |x+5| < \frac{\epsilon}{10+M} (10+M) = \epsilon$$ Case 2: $$0 < \epsilon < M(10+M)$$ $$\delta = \frac{\epsilon}{10+M}$$ $$\to |x - 5| |x + 5| < \frac{\epsilon}{10+M} (10+M) = \epsilon$$ Case 3: $$\epsilon = M(10+M)$$ Pick either value of $\delta$ . QED Question in the case of this example : Cases 1 and 3 refer to tolerance levels $\ge M(10+M)$ . Why do we care about those? Why isn't enough that we have proved case 2? I'm thinking that we could just find $\delta$ 's that work for $\epsilon \in (0,N)$ for some $N > 0$ . Why do we care about all $\epsilon$ ie $\epsilon \ge N$ ?","Definition of : Question : Why can't we weaken the assumption to s.t. ? I think they are not equivalent. If they are, please explain how the latter proves the former and why we still need to have case 1 below. Consider proving We first try to find some . if we maybe choose s.t. ...: Let (further restrictions may be needed). If , then we have So we might choose for the two cases in the proof (it seems no further restrictions on M are needed). Proof: Let . Case 1: Case 2: Case 3: Pick either value of . QED Question in the case of this example : Cases 1 and 3 refer to tolerance levels . Why do we care about those? Why isn't enough that we have proved case 2? I'm thinking that we could just find 's that work for for some . Why do we care about all ie ?","\lim_{x \to a} f(x) = L \forall \epsilon > 0, \exists \delta > 0 s.t. |f(x) - L| < \epsilon  if \ 0 < |x-a| < \delta \exists N > 0 \forall \epsilon \in (0, N), \exists \delta > 0 s.t. |f(x) - L| < \epsilon  if \ 0 < |x-a| < \delta \forall \epsilon > 0, \exists \delta > 0 s.t. |x^2 - 25| < \epsilon if \ 0 < |x-5| < \delta \delta |x^2 - 25|  = |x - 5| |x + 5| < \epsilon \delta M > 0 |x-5| < M - M < x-5 < M \to 5 - M < x < 5 + M \to 10 - M < x + 5 < 10 + M \to (-10 - M) < 10 - M < x + 5 < 10 + M |x + 5| < 10 + M \delta = \min\{M, \frac{\epsilon}{10+M} \} \epsilon > 0 \epsilon > M(10+M) \delta = M \to |x - 5| |x + 5| < M |x+5| < \frac{\epsilon}{10+M} (10+M) = \epsilon 0 < \epsilon < M(10+M) \delta = \frac{\epsilon}{10+M} \to |x - 5| |x + 5| < \frac{\epsilon}{10+M} (10+M) = \epsilon \epsilon = M(10+M) \delta \ge M(10+M) \delta \epsilon \in (0,N) N > 0 \epsilon \epsilon \ge N","['calculus', 'real-analysis', 'limits', 'convergence-divergence', 'definition']"
46,properties of the integral (Rudin theorem 6.12c),properties of the integral (Rudin theorem 6.12c),,"if $ f\in\mathscr{R(\alpha})$ on $[a,b]$ and if $a<c<b$, then $ f\in\mathscr{R(\alpha})$ $[a,c]$ and $[c,b]$ and $$\int_{a}^{b}fd\alpha = \int_{a}^{c}fd\alpha + \int_{c}^{b}fd\alpha$$ does the implication work backwards too? as in if $ f\in\mathscr{R(\alpha})$ $[a,c]$ and $[c,b]$ and $a<c<b$ then $ f\in\mathscr{R(\alpha})$ on $[a,b]$ and $$\int_{a}^{b}fd\alpha = \int_{a}^{c}fd\alpha + \int_{c}^{b}fd\alpha$$ i think it does and i've written a proof using upper/lower sums that seems trivial to me. but Rudin didn't write this as an ""if and only if"" statement so I'm suspicious that there might be a counterexample, perhaps with some weird discontinuity at $c$ for either $f$ or $\alpha$ that makes the backward implication not generally true.","if $ f\in\mathscr{R(\alpha})$ on $[a,b]$ and if $a<c<b$, then $ f\in\mathscr{R(\alpha})$ $[a,c]$ and $[c,b]$ and $$\int_{a}^{b}fd\alpha = \int_{a}^{c}fd\alpha + \int_{c}^{b}fd\alpha$$ does the implication work backwards too? as in if $ f\in\mathscr{R(\alpha})$ $[a,c]$ and $[c,b]$ and $a<c<b$ then $ f\in\mathscr{R(\alpha})$ on $[a,b]$ and $$\int_{a}^{b}fd\alpha = \int_{a}^{c}fd\alpha + \int_{c}^{b}fd\alpha$$ i think it does and i've written a proof using upper/lower sums that seems trivial to me. but Rudin didn't write this as an ""if and only if"" statement so I'm suspicious that there might be a counterexample, perhaps with some weird discontinuity at $c$ for either $f$ or $\alpha$ that makes the backward implication not generally true.",,['real-analysis']
47,Study materials to help understand the generalized Stokes' theorem both intuitively and rigorously?,Study materials to help understand the generalized Stokes' theorem both intuitively and rigorously?,,Dear MSE: My goal is to understand the generalized Stokes' theorem both intuitively and rigorously. Could someone give advice or recommend study materials to help understand the generalized Stokes' theorem both intuitively and rigorously? Baby Rudin seems to contain a terse treatment of the generalized Stokes' theorem in Chapter 10: Integration of Differential Forms. Are there study materials suitable to accompany Baby Rudin's terse treatment?,Dear MSE: My goal is to understand the generalized Stokes' theorem both intuitively and rigorously. Could someone give advice or recommend study materials to help understand the generalized Stokes' theorem both intuitively and rigorously? Baby Rudin seems to contain a terse treatment of the generalized Stokes' theorem in Chapter 10: Integration of Differential Forms. Are there study materials suitable to accompany Baby Rudin's terse treatment?,,"['real-analysis', 'reference-request', 'self-learning', 'book-recommendation', 'differential-forms']"
48,"If $f,g$ integrable then $f(x-y)g(y)$ integrable for almost every $x$",If  integrable then  integrable for almost every,"f,g f(x-y)g(y) x","I am trying to prove that for two integrable functions $f,g: \mathbb{R}^n \rightarrow \mathbb{R}$ the function $y \mapsto f(x-y)g(y)$ is integrable for almost every $x$. By using the holder inequality I reduced this to showing that if a function is integrable then also its square is integrable but after browsing a bit I found this so I guess this leads nowhere. Any hints are welcomed.","I am trying to prove that for two integrable functions $f,g: \mathbb{R}^n \rightarrow \mathbb{R}$ the function $y \mapsto f(x-y)g(y)$ is integrable for almost every $x$. By using the holder inequality I reduced this to showing that if a function is integrable then also its square is integrable but after browsing a bit I found this so I guess this leads nowhere. Any hints are welcomed.",,"['real-analysis', 'integration', 'measure-theory']"
49,"What are the possible limits of the iteration $x_{n+1}=\sqrt{x_n+3}$, $x_0=0$?","What are the possible limits of the iteration , ?",x_{n+1}=\sqrt{x_n+3} x_0=0,"Let $f(x)=\sqrt{x+3}$ for $x\ge -3$. Consider the iteration $$x_{n+1}=f(x_n),x_0=0;n\ge 0$$  The possible limits of the iteration are -1 3 0 $\sqrt{3+\sqrt{3+\sqrt{3+\cdots}}}$ I think only option 4. is correct as it the only one satisfying $x^2-x-3=0$","Let $f(x)=\sqrt{x+3}$ for $x\ge -3$. Consider the iteration $$x_{n+1}=f(x_n),x_0=0;n\ge 0$$  The possible limits of the iteration are -1 3 0 $\sqrt{3+\sqrt{3+\sqrt{3+\cdots}}}$ I think only option 4. is correct as it the only one satisfying $x^2-x-3=0$",,"['real-analysis', 'sequences-and-series', 'limits', 'recurrence-relations']"
50,Limit of a sequence with some property,Limit of a sequence with some property,,"Given $$ a_n >0 \text{ and } \lim_{n \to + \infty} a_n \left( \sum_{k=1}^n a_k \right) =2$$ I need to show that $$\lim_{n \to + \infty} \sqrt{n}\ a_n=1$$ I tried first to compute $$\lim_{n \to + \infty} a_n,$$ but I don't know how, or how to handle these kind of questions so I appreciate any  help.","Given $$ a_n >0 \text{ and } \lim_{n \to + \infty} a_n \left( \sum_{k=1}^n a_k \right) =2$$ I need to show that $$\lim_{n \to + \infty} \sqrt{n}\ a_n=1$$ I tried first to compute $$\lim_{n \to + \infty} a_n,$$ but I don't know how, or how to handle these kind of questions so I appreciate any  help.",,"['real-analysis', 'sequences-and-series']"
51,Let $f$ be a continuous and open map from $\mathbb R $ to $\mathbb R$.Prove that $f$ is monotonic.,Let  be a continuous and open map from  to .Prove that  is monotonic.,f \mathbb R  \mathbb R f,"Let $f$ be  a continuous and open map from $\mathbb R $ to $\mathbb R$.Prove that $f$ is monotonic. Suppose that $f$ is not monotonic.Then $\exists a,b;a>b$ such that $f(a)<f(b)$ and $c,d;c<d$ such that $f(c)>f(d)$.Since $f$ is continuous then there exists no break in the graph of $f$. But how should I use the fact that $f$ is open.Any help on how should I do the proof?","Let $f$ be  a continuous and open map from $\mathbb R $ to $\mathbb R$.Prove that $f$ is monotonic. Suppose that $f$ is not monotonic.Then $\exists a,b;a>b$ such that $f(a)<f(b)$ and $c,d;c<d$ such that $f(c)>f(d)$.Since $f$ is continuous then there exists no break in the graph of $f$. But how should I use the fact that $f$ is open.Any help on how should I do the proof?",,"['real-analysis', 'continuity']"
52,"Prove that $\frac{f(x)}{x}$ is uniformly continuous in $[1, +∞)$ if $f$ is Lipschitz",Prove that  is uniformly continuous in  if  is Lipschitz,"\frac{f(x)}{x} [1, +∞) f","Let $f(x)$ be a Lipschitz function on $[1, +∞)$, i.e. there exists a   positive constant $C$ such that $$|f(x) − f(y)| ≤ C|x − y|, ∀x, y ∈ [1, +∞).$$ Prove that $\frac{f(x)}{x}$ is uniformly continuous in $[1,+\infty)$. I know that a  Lipschitz function is uniformly continuous. What I did so far is: let $g(x)  = \frac{f(x)}{x}$. Then I assumed $g(x)$ is Lipschitz. (Is the assumption wrong?)  Then  $|g(x)-g(y)| \le K|x-y|$ satisfies the Lipschitz condition. Therefore $|\frac{yf(x)-xf(y)}{xy}| \le K|x-y|$. How to continue from here?","Let $f(x)$ be a Lipschitz function on $[1, +∞)$, i.e. there exists a   positive constant $C$ such that $$|f(x) − f(y)| ≤ C|x − y|, ∀x, y ∈ [1, +∞).$$ Prove that $\frac{f(x)}{x}$ is uniformly continuous in $[1,+\infty)$. I know that a  Lipschitz function is uniformly continuous. What I did so far is: let $g(x)  = \frac{f(x)}{x}$. Then I assumed $g(x)$ is Lipschitz. (Is the assumption wrong?)  Then  $|g(x)-g(y)| \le K|x-y|$ satisfies the Lipschitz condition. Therefore $|\frac{yf(x)-xf(y)}{xy}| \le K|x-y|$. How to continue from here?",,"['real-analysis', 'uniform-continuity', 'lipschitz-functions']"
53,"Show that the sequence {$\sqrt{5},\sqrt{5+\sqrt{5}},\sqrt{5+\sqrt{5+\sqrt{5}}},....$} is convergent using monotone convergence theorem",Show that the sequence {} is convergent using monotone convergence theorem,"\sqrt{5},\sqrt{5+\sqrt{5}},\sqrt{5+\sqrt{5+\sqrt{5}}},....","QUESTION: Show that the sequence {$\sqrt{5},\sqrt{5+\sqrt{5}},\sqrt{5+\sqrt{5+\sqrt{5}}},\sqrt{5+\sqrt{5+\sqrt{5+\sqrt{5}}}},....$} is convergent and it converges to $\left(\frac{1+\sqrt{21}}{2}\right)$. MY ATTEMPT: The sequence takes the form of the recurrence $x_n=\sqrt{x_{n-1}+5}$. But neither can I show it to be monotonic increasing nor bounded. Once I have shown it to be convergent, I know how to find and show the limit. I have successfully done it too. But I cannot prove the convergence. ONLY HINTS required. P.S. Do not use Cauchy principle or any complicated test. I want the answer to be based on monotonicity and boundedness. For the problem belongs to that chapter only.","QUESTION: Show that the sequence {$\sqrt{5},\sqrt{5+\sqrt{5}},\sqrt{5+\sqrt{5+\sqrt{5}}},\sqrt{5+\sqrt{5+\sqrt{5+\sqrt{5}}}},....$} is convergent and it converges to $\left(\frac{1+\sqrt{21}}{2}\right)$. MY ATTEMPT: The sequence takes the form of the recurrence $x_n=\sqrt{x_{n-1}+5}$. But neither can I show it to be monotonic increasing nor bounded. Once I have shown it to be convergent, I know how to find and show the limit. I have successfully done it too. But I cannot prove the convergence. ONLY HINTS required. P.S. Do not use Cauchy principle or any complicated test. I want the answer to be based on monotonicity and boundedness. For the problem belongs to that chapter only.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
54,Characterization of group homomorphisms from $\mathbb R$ to $\mathbb R$,Characterization of group homomorphisms from  to,\mathbb R \mathbb R,"I am trying to characterize all the group homomorphisms from $\DeclareMathOperator{R}{\mathbb R}\R$ to $\R$ . I have characterized all the ""continuous"" group homomorphisms from $\R$ to $\R$ . They are of the form $f(x) = f(1) x$ . Now I claim that all the group homomorphisms from $\R$ to $\R$ are necessarily continuous. Is this claim correct? I provide my justifications below. Let $f$ be any group homomorphism from $\R$ to $\R$ . Since $f(x+y) = f(x) + f(y)$ would hold for all $x,y$ in $\mathbb Q$ , f(restricted to $\mathbb Q$ ) is a group homomorphism from $\mathbb Q$ to $\R$ . However, all the group homomorphisms from $\mathbb Q$ to $\R$ are continuous as they are of the form $f(x) = f(1)x$ . Not only that, they are uniformly continuous as $|f(x)-f(y)| = |f(1)(x-y)|< \epsilon$ if I choose $\delta$ as $\epsilon/|f(1)|$ . Since $f$ is uniformly continuous on a dense subspace of $\R$ , it can be extended to a unique continuous function in its closure. (Rud Exercise in Ch4. Continuity). Thus, every group homomorphism from $\R$ to $\R$ is continuous and hence are of the from $f(x) = f(1)x$ .","I am trying to characterize all the group homomorphisms from to . I have characterized all the ""continuous"" group homomorphisms from to . They are of the form . Now I claim that all the group homomorphisms from to are necessarily continuous. Is this claim correct? I provide my justifications below. Let be any group homomorphism from to . Since would hold for all in , f(restricted to ) is a group homomorphism from to . However, all the group homomorphisms from to are continuous as they are of the form . Not only that, they are uniformly continuous as if I choose as . Since is uniformly continuous on a dense subspace of , it can be extended to a unique continuous function in its closure. (Rud Exercise in Ch4. Continuity). Thus, every group homomorphism from to is continuous and hence are of the from .","\DeclareMathOperator{R}{\mathbb R}\R \R \R \R f(x) = f(1) x \R \R f \R \R f(x+y) = f(x) + f(y) x,y \mathbb Q \mathbb Q \mathbb Q \R \mathbb Q \R f(x) = f(1)x |f(x)-f(y)| = |f(1)(x-y)|< \epsilon \delta \epsilon/|f(1)| f \R \R \R f(x) = f(1)x","['real-analysis', 'general-topology', 'group-theory']"
55,"Show that the unit sphere is a complete metric space equipped with $d(x,y):= \arccos \langle x,y \rangle_{\mathbb{R}^n}$.",Show that the unit sphere is a complete metric space equipped with .,"d(x,y):= \arccos \langle x,y \rangle_{\mathbb{R}^n}","So we were shown this problem from our first functional analysis lecture and I was wondering if someone could help or give a hint: Show that the unit sphere $\mathbb{S}^{n-1} := \{ x \in \mathbb{R}^n: \| x \| = 1 \}$ is a complete metric space equipped with $d(x,y):= \arccos \langle x,y \rangle_{\mathbb{R}^n}$ where $\langle  x,y \rangle_{\mathbb{R}^n}$ denotes the standard dot product. We're having no trouble showing positivity and symmetry but could use help showing completeness and the triangle inequality. Kind regards Edit: So with MatiasHeikkilä's help, what's left to show is that $\theta_{x,y} \leq \theta_{x,z} + \theta_{y,z}$ if I'm not mistaken. Do I need to make a case-by-case proof to show this inequality or is there an easier way? I would argue something  along the lines of: ""$z$ lies between $x$ and $y$ implies the equality"", ""$z$ does not lie on the (smallest) path between $x$ and $y$ but $z$ lies on the half-sphere with $x$ and $y$ on it implies the inequality (since $\theta_{x,y} < \theta_{x,z}$) and lastly $z$ does neither lie on the (smallest) path between $x$ and $y$ nor does $z$ lie on the half-sphere implies the inequality (since $\theta_{x,z} + \theta_{y,z} = 2 \pi - \theta_{x,y} \geq \pi $)","So we were shown this problem from our first functional analysis lecture and I was wondering if someone could help or give a hint: Show that the unit sphere $\mathbb{S}^{n-1} := \{ x \in \mathbb{R}^n: \| x \| = 1 \}$ is a complete metric space equipped with $d(x,y):= \arccos \langle x,y \rangle_{\mathbb{R}^n}$ where $\langle  x,y \rangle_{\mathbb{R}^n}$ denotes the standard dot product. We're having no trouble showing positivity and symmetry but could use help showing completeness and the triangle inequality. Kind regards Edit: So with MatiasHeikkilä's help, what's left to show is that $\theta_{x,y} \leq \theta_{x,z} + \theta_{y,z}$ if I'm not mistaken. Do I need to make a case-by-case proof to show this inequality or is there an easier way? I would argue something  along the lines of: ""$z$ lies between $x$ and $y$ implies the equality"", ""$z$ does not lie on the (smallest) path between $x$ and $y$ but $z$ lies on the half-sphere with $x$ and $y$ on it implies the inequality (since $\theta_{x,y} < \theta_{x,z}$) and lastly $z$ does neither lie on the (smallest) path between $x$ and $y$ nor does $z$ lie on the half-sphere implies the inequality (since $\theta_{x,z} + \theta_{y,z} = 2 \pi - \theta_{x,y} \geq \pi $)",,"['real-analysis', 'functional-analysis', 'metric-spaces']"
56,Conditional Expectation and Interpretation of Projection of $Y$ Onto $X$ vs. $Y$ onto $L^{2}(\sigma(X))$,Conditional Expectation and Interpretation of Projection of  Onto  vs.  onto,Y X Y L^{2}(\sigma(X)),"Suppose $X,Y\in L^{2}(\Omega,\mathcal{F},\mathbb{P})$ are real-valued square-integrable random variables and that the joint density of $(X,Y)$ denoted by $f_{XY}(x,y)$ exists. Then as is well-known, the conditional expectation of $Y$ given $X$ can be computed explicitly by the following formula (where we have used $X(\omega)$ instead of $X=x$ to emphasize that the conditional expectation is a fully-fledged random variable that is merely $\sigma(X)$-measurable): $$\begin{align*} (1)\;\;\;\;\mathbb{E}[Y|X](\omega) &=\int_{-\infty}^{\infty}y\;f_{XY}(X(\omega),y)\;dy\;\Bigg/\;\int_{-\infty}^{\infty}f_{XY}(X(\omega),y)\;dy \\&=\frac{1}{f_{X}(X(\omega))}\int_{-\infty}^{\infty}y\;f_{XY}(X(\omega),y)\;dy. \end{align*}$$ It is straight-forward to prove that $$||Y-\mathbb{E}[Y|X]||^{2}_{L^{2}(\mathcal{F})}=\inf_{Z\in L^{2}(\sigma(X))}||Y-Z||^{2}_{L^{2}(\mathcal{F})},$$ which in turn proves (because conditional expectation is linear) that $$\mathbb{E}[Y|X]=\mathbb{proj}_{L^{2}(\sigma(X))}Y,$$ i.e. the conditional expectation operator is really a projection operator from $L^{2}(\mathcal{F})$ to the closed subspace $L^{2}(\sigma(X))$. When I first learned about this fact I was tempted to assert that given $X$ we have \begin{align*} (2)\;\;\;\;\mathbb{E}[Y|X](\omega) &\stackrel{?}{=}\frac{(X,Y)_{L^{2}}}{||X||_{L^{2}}^{2}}X(\omega) \\&=\frac{\mathbb{E}[XY]}{\mathbb{E}[X^{2}]}X(\omega) \\&=X(\omega)\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}xy\;f_{XY}(x,y)\;dxdy\;\Bigg/\;\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}x^{2}f_{XY}(x,y)\;dxdy \end{align*} because in a Hilbert space this can be thought of as the orthogonal projection of $Y$ in the direction of $X$.  However, the conditional expectation is an orthogonal projection into the entire subspace $L^{2}(\sigma(X))$, and so the formula above only holds for one random variable, namely $$X(\omega)=\mathbb{E}[Y|X](\omega),$$ which defeats the purpose of using this formulation as a computational device for $\mathbb{E}[Y|X](\omega)$.  Of particular note, if $\sigma(Z)=\sigma(X)$ (which implies $Z\in L^{2}(\sigma(X))$), then $$\mathbb{E}[Y|X](\omega)=\mathbb{E}[Y|Z](\omega).$$ Still I wonder, even if clearly wrong, whether (2) is known to be useful in any way.  So I have a couple of questions: Given $X$, can we easily find an orthonormal basis $\{X_{n}\}_{n>0}$ that spans $L^{2}(\sigma(X))$?  By easily, I mean in terms of the marginal and joint distributions of $X$ and $Y$, without explicit reference to whatever the underlying sample space looks like.  If so, then in principle (2) could be made valid by summing the formula over each $X_{n}$.  Of course, I think this would likely destroy the original motivation of finding an explicit way to represent the conditional expectation using the intuition of orthogonal projections. Can we relate (1) to such an orthonormal set $\{X_{n}\}_{n>0}$?  In particular, show that it arises from the sum of orthogonal projections. Are there any useful applications of orthogonal projection onto a particular random variable $X$ (as opposed to orthogonal projection onto an entire subspace $L^{2}(\sigma(X))$ which is represented by the conditional expectation).","Suppose $X,Y\in L^{2}(\Omega,\mathcal{F},\mathbb{P})$ are real-valued square-integrable random variables and that the joint density of $(X,Y)$ denoted by $f_{XY}(x,y)$ exists. Then as is well-known, the conditional expectation of $Y$ given $X$ can be computed explicitly by the following formula (where we have used $X(\omega)$ instead of $X=x$ to emphasize that the conditional expectation is a fully-fledged random variable that is merely $\sigma(X)$-measurable): $$\begin{align*} (1)\;\;\;\;\mathbb{E}[Y|X](\omega) &=\int_{-\infty}^{\infty}y\;f_{XY}(X(\omega),y)\;dy\;\Bigg/\;\int_{-\infty}^{\infty}f_{XY}(X(\omega),y)\;dy \\&=\frac{1}{f_{X}(X(\omega))}\int_{-\infty}^{\infty}y\;f_{XY}(X(\omega),y)\;dy. \end{align*}$$ It is straight-forward to prove that $$||Y-\mathbb{E}[Y|X]||^{2}_{L^{2}(\mathcal{F})}=\inf_{Z\in L^{2}(\sigma(X))}||Y-Z||^{2}_{L^{2}(\mathcal{F})},$$ which in turn proves (because conditional expectation is linear) that $$\mathbb{E}[Y|X]=\mathbb{proj}_{L^{2}(\sigma(X))}Y,$$ i.e. the conditional expectation operator is really a projection operator from $L^{2}(\mathcal{F})$ to the closed subspace $L^{2}(\sigma(X))$. When I first learned about this fact I was tempted to assert that given $X$ we have \begin{align*} (2)\;\;\;\;\mathbb{E}[Y|X](\omega) &\stackrel{?}{=}\frac{(X,Y)_{L^{2}}}{||X||_{L^{2}}^{2}}X(\omega) \\&=\frac{\mathbb{E}[XY]}{\mathbb{E}[X^{2}]}X(\omega) \\&=X(\omega)\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}xy\;f_{XY}(x,y)\;dxdy\;\Bigg/\;\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}x^{2}f_{XY}(x,y)\;dxdy \end{align*} because in a Hilbert space this can be thought of as the orthogonal projection of $Y$ in the direction of $X$.  However, the conditional expectation is an orthogonal projection into the entire subspace $L^{2}(\sigma(X))$, and so the formula above only holds for one random variable, namely $$X(\omega)=\mathbb{E}[Y|X](\omega),$$ which defeats the purpose of using this formulation as a computational device for $\mathbb{E}[Y|X](\omega)$.  Of particular note, if $\sigma(Z)=\sigma(X)$ (which implies $Z\in L^{2}(\sigma(X))$), then $$\mathbb{E}[Y|X](\omega)=\mathbb{E}[Y|Z](\omega).$$ Still I wonder, even if clearly wrong, whether (2) is known to be useful in any way.  So I have a couple of questions: Given $X$, can we easily find an orthonormal basis $\{X_{n}\}_{n>0}$ that spans $L^{2}(\sigma(X))$?  By easily, I mean in terms of the marginal and joint distributions of $X$ and $Y$, without explicit reference to whatever the underlying sample space looks like.  If so, then in principle (2) could be made valid by summing the formula over each $X_{n}$.  Of course, I think this would likely destroy the original motivation of finding an explicit way to represent the conditional expectation using the intuition of orthogonal projections. Can we relate (1) to such an orthonormal set $\{X_{n}\}_{n>0}$?  In particular, show that it arises from the sum of orthogonal projections. Are there any useful applications of orthogonal projection onto a particular random variable $X$ (as opposed to orthogonal projection onto an entire subspace $L^{2}(\sigma(X))$ which is represented by the conditional expectation).",,"['real-analysis', 'probability', 'probability-theory', 'hilbert-spaces']"
57,Boundedness in the functional monotone class theorem,Boundedness in the functional monotone class theorem,,"Let $\Omega$ be a set $\mathcal M\subseteq 2^\Omega$ be a $\pi$-system with $\Omega\in\mathcal M$ $\mathcal H$ be a set of functions $\Omega\to\mathbb R$ Suppose $\mathcal H$ satisfies the following conditions $A\in\mathcal M\Rightarrow 1_A\in\mathcal H$ $f,g\in\mathcal H\Rightarrow\alpha f+g\in\mathcal H$, for all $\alpha\in\mathbb R$ If $f:\Omega\to\mathbb R$ is bounded and there is a sequence $\left\{f_n\right\}\subseteq H$ with $f_n\ge 0$ and $f_n\uparrow f$, then $f\in\mathcal H$ Then, the montone class theorem states, that $\mathcal H$ contains all bounded $\sigma(\mathcal M)$-measurable functions $\Omega\to\mathbb R$. Why is boundedness crucial? Let's step through a possible proof: Let $$\mathcal D:=\left\{A\subseteq\Omega:1_A\in\mathcal H\right\}\;.$$ Claim : $\mathcal H$ is a $\lambda$-system Proof : $\Omega\in\mathcal M$ and (1.) $\Rightarrow$ $\Omega\in\mathcal D$ Let $A\in\mathcal D\Rightarrow 1_{A^c}=1_\Omega-1_A\in\mathcal H$ by (1.) and (2.)$\Rightarrow A^c\in\mathcal D$ Let $\left\{A_n\right\}\subseteq\mathcal D$ be pairwise disjoint, then $$1_{A^n}=\sum_{m\le n}1_{A_m}\in\mathcal H\;,$$ for $A^n:=\biguplus_{m\le n}A_m$, by (2.) and therefore $$1_A=\lim_{n\to\infty}1_{A^n}\in\mathcal H$$ by (3.) (since $1_A$ is bounded) Now, since $\mathcal M$ is a $\pi$-system, $$\sigma(\mathcal M)=\delta(\mathcal M)\subseteq\mathcal D\;,$$ by (1.) and the $\pi$-$\lambda$-theorem . So, by (2.), all simple $\sigma(\mathcal M)$-measurable functions are contained in $\mathcal H$. Until now, the whole argumentation has not made use of the boundedness restriction in (3.). Let $f:\Omega\to\mathbb R$ be $\sigma(\mathcal M)$-measurable. Then, it's positive and negative parts $f^+$ and $f^-$ can be written as the monotone limit of non-negative $\sigma(\mathcal M)$-measurable simple functions $f_n^+$ and $f_n^-$, respectively. Since $f_n^\pm\in\mathcal H$, we can conclude $$f=f^+-f^-=\lim_{n\to\infty}f_n^+-\lim_{n\to\infty}f_n^-\in\mathcal H\;,\tag{1}$$ even without the boundedness restriction in (3.) as long as the pointwise limits exist (in $\overline{\mathbb R}$). In the sense of existence in $\overline{\mathbb R}$, the limits exist as long there sum is well-defined, i.e. as long as there can't be a $\infty+(-\infty)$ term. So, it seems like we could loosen the boundedness in (3.) and only force $$\inf_\Omega f>-\infty\;.$$ Do I miss something?","Let $\Omega$ be a set $\mathcal M\subseteq 2^\Omega$ be a $\pi$-system with $\Omega\in\mathcal M$ $\mathcal H$ be a set of functions $\Omega\to\mathbb R$ Suppose $\mathcal H$ satisfies the following conditions $A\in\mathcal M\Rightarrow 1_A\in\mathcal H$ $f,g\in\mathcal H\Rightarrow\alpha f+g\in\mathcal H$, for all $\alpha\in\mathbb R$ If $f:\Omega\to\mathbb R$ is bounded and there is a sequence $\left\{f_n\right\}\subseteq H$ with $f_n\ge 0$ and $f_n\uparrow f$, then $f\in\mathcal H$ Then, the montone class theorem states, that $\mathcal H$ contains all bounded $\sigma(\mathcal M)$-measurable functions $\Omega\to\mathbb R$. Why is boundedness crucial? Let's step through a possible proof: Let $$\mathcal D:=\left\{A\subseteq\Omega:1_A\in\mathcal H\right\}\;.$$ Claim : $\mathcal H$ is a $\lambda$-system Proof : $\Omega\in\mathcal M$ and (1.) $\Rightarrow$ $\Omega\in\mathcal D$ Let $A\in\mathcal D\Rightarrow 1_{A^c}=1_\Omega-1_A\in\mathcal H$ by (1.) and (2.)$\Rightarrow A^c\in\mathcal D$ Let $\left\{A_n\right\}\subseteq\mathcal D$ be pairwise disjoint, then $$1_{A^n}=\sum_{m\le n}1_{A_m}\in\mathcal H\;,$$ for $A^n:=\biguplus_{m\le n}A_m$, by (2.) and therefore $$1_A=\lim_{n\to\infty}1_{A^n}\in\mathcal H$$ by (3.) (since $1_A$ is bounded) Now, since $\mathcal M$ is a $\pi$-system, $$\sigma(\mathcal M)=\delta(\mathcal M)\subseteq\mathcal D\;,$$ by (1.) and the $\pi$-$\lambda$-theorem . So, by (2.), all simple $\sigma(\mathcal M)$-measurable functions are contained in $\mathcal H$. Until now, the whole argumentation has not made use of the boundedness restriction in (3.). Let $f:\Omega\to\mathbb R$ be $\sigma(\mathcal M)$-measurable. Then, it's positive and negative parts $f^+$ and $f^-$ can be written as the monotone limit of non-negative $\sigma(\mathcal M)$-measurable simple functions $f_n^+$ and $f_n^-$, respectively. Since $f_n^\pm\in\mathcal H$, we can conclude $$f=f^+-f^-=\lim_{n\to\infty}f_n^+-\lim_{n\to\infty}f_n^-\in\mathcal H\;,\tag{1}$$ even without the boundedness restriction in (3.) as long as the pointwise limits exist (in $\overline{\mathbb R}$). In the sense of existence in $\overline{\mathbb R}$, the limits exist as long there sum is well-defined, i.e. as long as there can't be a $\infty+(-\infty)$ term. So, it seems like we could loosen the boundedness in (3.) and only force $$\inf_\Omega f>-\infty\;.$$ Do I miss something?",,"['real-analysis', 'probability-theory', 'measure-theory']"
58,Differentiable functions such that the derivative is nowhere continuous. [duplicate],Differentiable functions such that the derivative is nowhere continuous. [duplicate],,This question already has answers here : Discontinuous derivative. [duplicate] (2 answers) Closed 8 years ago . Is there a function $f:\mathbb R\to\mathbb R$ which is differentiable but such that the derivative is nowhere continuous?,This question already has answers here : Discontinuous derivative. [duplicate] (2 answers) Closed 8 years ago . Is there a function $f:\mathbb R\to\mathbb R$ which is differentiable but such that the derivative is nowhere continuous?,,['real-analysis']
59,Does $\lim_{n\to \infty}\sigma_n=0\implies \lim_{n\to \infty} a_n=0$?,Does ?,\lim_{n\to \infty}\sigma_n=0\implies \lim_{n\to \infty} a_n=0,"Let $\{a_n\}$ be a sequence of positive real numbers such that $\lim_{n\to \infty} \sigma_n=0$ where $\sigma_n=\left(\sum_{k=1}^n a_k\right)/n$. Does $\lim_{n} \sigma_n=0\implies \lim_n a_n=0$? I feel that this should be true but I cannot find a proof for this nor can I find a counterexample. I tried my hand at the proof in the following way: Since $a_n=n\sigma_n-(n-1)\sigma_{n-1},\ n\ge 2$, we have $\lim_n a_n/n=0$. Also, it is not difficult to prove that $\lim\inf_n a_n=0,\ L:=\lim\sup_n\ge 0$. But then I am stuck. I will really appreciate short hints, rather than a full answer. Thanks in advance.","Let $\{a_n\}$ be a sequence of positive real numbers such that $\lim_{n\to \infty} \sigma_n=0$ where $\sigma_n=\left(\sum_{k=1}^n a_k\right)/n$. Does $\lim_{n} \sigma_n=0\implies \lim_n a_n=0$? I feel that this should be true but I cannot find a proof for this nor can I find a counterexample. I tried my hand at the proof in the following way: Since $a_n=n\sigma_n-(n-1)\sigma_{n-1},\ n\ge 2$, we have $\lim_n a_n/n=0$. Also, it is not difficult to prove that $\lim\inf_n a_n=0,\ L:=\lim\sup_n\ge 0$. But then I am stuck. I will really appreciate short hints, rather than a full answer. Thanks in advance.",,"['real-analysis', 'sequences-and-series', 'limits', 'examples-counterexamples', 'limsup-and-liminf']"
60,Show that $(1+p/n)^n$ is a Cauchy sequence for arbitrary $p$,Show that  is a Cauchy sequence for arbitrary,(1+p/n)^n p,"It is a generalization of this question. I am looking for a similar derivation as in here . Can we prove that $(1+p/n)^n$ is a Cauchy sequence for any $p \in [a, b]$ by showing that $$ \Bigg| \left( 1 + \frac{p}{n}\right)^n - \left( 1 + \frac{p}{m}\right)^m \Bigg| \leq f(n)$$ where $f(n)$ is something that tends to zero as $n$ goes to infinity? Here is an attempt. Let $m=n+1$. Then, $$ \Bigg| \left( 1 + \frac{p}{n}\right)^n - \left( 1 + \frac{p}{n+1}\right)^{n+1} \Bigg| = \Bigg| \sum_{k=0}^{n} \binom{n}{k}\left( \frac{p}{n} \right)^{k} - \sum_{k=0}^{n+1} \binom{n+1}{k}\left( \frac{p}{n+1} \right)^{k} \Bigg| = \\ \Bigg| \sum_{k=0}^{n} \frac{p^k}{k!} \left[ \prod_{i=1}^{k-1} \left( 1 - \frac{i}{n+1} \right) - \prod_{i=1}^{k-1} \left( 1 - \frac{i}{n} \right) \right] + \frac{p^{n+1}}{(n+1)^{n+1}} \Bigg| \leq \sum_{k=0}^{n} \frac{|p|^k}{k!} \left[ \prod_{i=1}^{k-1} \left( 1 - \frac{i}{n+1} \right) - \prod_{i=1}^{k-1} \left( 1 - \frac{i}{n} \right) \right] + \Bigg| \frac{p^{n+1}}{(n+1)^{n+1}} \Bigg| $$ Now intuitively, $\sum_{k=0}^{n} \frac{|p|^k}{k!}$ tends to the constant $e^{|p|}$ while the expression in square brackets tends to zero as $n$ goes to infinity. I was trying to show that this expression is less than a constant divided by $n^2$ (because we will sum up the consecutive terms for $m >n$, and the sum should converge which $\frac{C}{n^2}$ would provide). Can we also find an estimate of the constant $C$? Thanks to all for good answers. Meanwhile, I'm still quite interested in working it out algebraically. My next idea was to use the triangle inequality. Let us ignore the last term in the inequality above, it obviously tends to zero as $n$ goes to infinity. We want the sum to go to zero fast enough. So, $$ \Bigg| \sum_{k=0}^{n} \frac{p^k}{k!} \prod_{i=1}^{k-1} \left( 1 - \frac{i}{n+1} \right) - \sum_{k=0}^{n} \frac{p^k}{k!} \prod_{i=1}^{k-1} \left( 1 - \frac{i}{n} \right) \Bigg| \leq \\ \Bigg| \sum_{k=0}^{n} \frac{p^k}{k!} \prod_{i=1}^{k-1} \left( 1 - \frac{i}{n} \right) - \sum_{k=0}^{n} \frac{p^k}{k!} \Bigg| + \\ \Bigg| \sum_{k=0}^{n} \frac{p^k}{k!} \prod_{i=1}^{k-1} \left( 1 - \frac{i}{n+1} \right) - \sum_{k=0}^{n} \frac{p^k}{k!} \Bigg| $$ Each product is evidently positive and smaller than one. So we end up having kind of a usual exponential series, but weighted. Otherwise, we could try to plug something instead of the products, what would, when subtracted from the products, give something which decreases fast enough. I feel this should be possible since we have all the freedom to plug in whatever we want.","It is a generalization of this question. I am looking for a similar derivation as in here . Can we prove that $(1+p/n)^n$ is a Cauchy sequence for any $p \in [a, b]$ by showing that $$ \Bigg| \left( 1 + \frac{p}{n}\right)^n - \left( 1 + \frac{p}{m}\right)^m \Bigg| \leq f(n)$$ where $f(n)$ is something that tends to zero as $n$ goes to infinity? Here is an attempt. Let $m=n+1$. Then, $$ \Bigg| \left( 1 + \frac{p}{n}\right)^n - \left( 1 + \frac{p}{n+1}\right)^{n+1} \Bigg| = \Bigg| \sum_{k=0}^{n} \binom{n}{k}\left( \frac{p}{n} \right)^{k} - \sum_{k=0}^{n+1} \binom{n+1}{k}\left( \frac{p}{n+1} \right)^{k} \Bigg| = \\ \Bigg| \sum_{k=0}^{n} \frac{p^k}{k!} \left[ \prod_{i=1}^{k-1} \left( 1 - \frac{i}{n+1} \right) - \prod_{i=1}^{k-1} \left( 1 - \frac{i}{n} \right) \right] + \frac{p^{n+1}}{(n+1)^{n+1}} \Bigg| \leq \sum_{k=0}^{n} \frac{|p|^k}{k!} \left[ \prod_{i=1}^{k-1} \left( 1 - \frac{i}{n+1} \right) - \prod_{i=1}^{k-1} \left( 1 - \frac{i}{n} \right) \right] + \Bigg| \frac{p^{n+1}}{(n+1)^{n+1}} \Bigg| $$ Now intuitively, $\sum_{k=0}^{n} \frac{|p|^k}{k!}$ tends to the constant $e^{|p|}$ while the expression in square brackets tends to zero as $n$ goes to infinity. I was trying to show that this expression is less than a constant divided by $n^2$ (because we will sum up the consecutive terms for $m >n$, and the sum should converge which $\frac{C}{n^2}$ would provide). Can we also find an estimate of the constant $C$? Thanks to all for good answers. Meanwhile, I'm still quite interested in working it out algebraically. My next idea was to use the triangle inequality. Let us ignore the last term in the inequality above, it obviously tends to zero as $n$ goes to infinity. We want the sum to go to zero fast enough. So, $$ \Bigg| \sum_{k=0}^{n} \frac{p^k}{k!} \prod_{i=1}^{k-1} \left( 1 - \frac{i}{n+1} \right) - \sum_{k=0}^{n} \frac{p^k}{k!} \prod_{i=1}^{k-1} \left( 1 - \frac{i}{n} \right) \Bigg| \leq \\ \Bigg| \sum_{k=0}^{n} \frac{p^k}{k!} \prod_{i=1}^{k-1} \left( 1 - \frac{i}{n} \right) - \sum_{k=0}^{n} \frac{p^k}{k!} \Bigg| + \\ \Bigg| \sum_{k=0}^{n} \frac{p^k}{k!} \prod_{i=1}^{k-1} \left( 1 - \frac{i}{n+1} \right) - \sum_{k=0}^{n} \frac{p^k}{k!} \Bigg| $$ Each product is evidently positive and smaller than one. So we end up having kind of a usual exponential series, but weighted. Otherwise, we could try to plug something instead of the products, what would, when subtracted from the products, give something which decreases fast enough. I feel this should be possible since we have all the freedom to plug in whatever we want.",,"['real-analysis', 'sequences-and-series', 'combinatorics', 'contest-math']"
61,"Is there continuous $f: [0, 1] \rightarrow [0, \infty)$ such that for all $x$ there is $y$ with $f(y) < f(x)$?",Is there continuous  such that for all  there is  with ?,"f: [0, 1] \rightarrow [0, \infty) x y f(y) < f(x)","I think there isn't. Here's a sketch of a proof. I'm just not sure whether it really works because I'm not confident with the transfinite versions of the standard theorems about limits and convergent sequences. Attempted proof: Suppose there were such a function. Then use transfinite recursion to generate the following transfinite sequence: BASE CASE: $s_0 = 1$. SUCCESSOR ORDINAL: Let $s_{\lambda + 1}$ be a number in $[0, 1]$ such that $f(s_{\lambda + 1}) < f(s_\lambda)$. LIMIT ORDINAL: Suppose $\lambda$ is a limit ordinal. Then the sequence $f(s_0)$, ..., $f(s_\alpha)$, ... with $\alpha < \lambda$ is bounded below by 0. So it has a limit $\lim_{\alpha < \lambda} f(s_\alpha$) (by a transfinite version of the Monotone Convergence Theorem). Also (by a transfinite version of the Bolzano-Weierstrass Theorem) there is a subsequence of $s_0$, ..., $s_\alpha$, ... that is unbounded in that sequence and it converges to a limit. Let's call that subsequence $s_{i_0}$, ..., $s_{i_\beta}, ...$ for $\beta < \gamma \leq \lambda$. Then let $s := \lim_{\beta < \gamma} s_{i_\beta}$ Then, since $f$ is continuous, $$ \lim_{\alpha < \lambda} f(s_\alpha) = \lim_{\beta < \gamma} f(s_{i_\beta}) = f(s) $$ So let $s_\lambda = s = \lim_{\beta < \gamma} s_{i_\beta}$. Then there must be $\alpha$ such that $s_\alpha = s_{\alpha + 1}$, since there are at most continuum-many distinct numbers in the list $f(s_0)$, ..., $f(s_\alpha)$, ... . This gives a contradiction.","I think there isn't. Here's a sketch of a proof. I'm just not sure whether it really works because I'm not confident with the transfinite versions of the standard theorems about limits and convergent sequences. Attempted proof: Suppose there were such a function. Then use transfinite recursion to generate the following transfinite sequence: BASE CASE: $s_0 = 1$. SUCCESSOR ORDINAL: Let $s_{\lambda + 1}$ be a number in $[0, 1]$ such that $f(s_{\lambda + 1}) < f(s_\lambda)$. LIMIT ORDINAL: Suppose $\lambda$ is a limit ordinal. Then the sequence $f(s_0)$, ..., $f(s_\alpha)$, ... with $\alpha < \lambda$ is bounded below by 0. So it has a limit $\lim_{\alpha < \lambda} f(s_\alpha$) (by a transfinite version of the Monotone Convergence Theorem). Also (by a transfinite version of the Bolzano-Weierstrass Theorem) there is a subsequence of $s_0$, ..., $s_\alpha$, ... that is unbounded in that sequence and it converges to a limit. Let's call that subsequence $s_{i_0}$, ..., $s_{i_\beta}, ...$ for $\beta < \gamma \leq \lambda$. Then let $s := \lim_{\beta < \gamma} s_{i_\beta}$ Then, since $f$ is continuous, $$ \lim_{\alpha < \lambda} f(s_\alpha) = \lim_{\beta < \gamma} f(s_{i_\beta}) = f(s) $$ So let $s_\lambda = s = \lim_{\beta < \gamma} s_{i_\beta}$. Then there must be $\alpha$ such that $s_\alpha = s_{\alpha + 1}$, since there are at most continuum-many distinct numbers in the list $f(s_0)$, ..., $f(s_\alpha)$, ... . This gives a contradiction.",,"['real-analysis', 'convergence-divergence', 'continuity', 'transfinite-recursion']"
62,Is this limit proof correct?,Is this limit proof correct?,,"I am currently studying the formal defenition of the limit. One of the examples given by my book is the following: Prove that: $$ \lim_{x \to 3} x^2 = 9 $$ So, using only the defenition of the limit, I have to prove that for every $\epsilon > 0$ there is a $\delta > 0$ for which the following is true: $$ 0 < |x - 3| < \delta \to |x^2 - 9| < \epsilon $$ So after some puzzeling I came up with: $\delta = \frac{\epsilon}{|x + 3|}$. And I though this was correct: under the assumption that the antecedent is true, we can make the following construct: $$ 0 < |x - 3| < \delta \Rightarrow 0 < |x - 3| < \frac{\epsilon}{|x + 3|} \Rightarrow 0 < |x + 3||x - 3| < \epsilon \Rightarrow |x^2 - 9| < \epsilon $$ Q.E.D, I thought. But the book came up with the following solution: $$ \delta = \min{(1,\frac{\epsilon}{7})} $$ Which is a correct solution. So, is mine wrong? Or did the book just provide a different proof?","I am currently studying the formal defenition of the limit. One of the examples given by my book is the following: Prove that: $$ \lim_{x \to 3} x^2 = 9 $$ So, using only the defenition of the limit, I have to prove that for every $\epsilon > 0$ there is a $\delta > 0$ for which the following is true: $$ 0 < |x - 3| < \delta \to |x^2 - 9| < \epsilon $$ So after some puzzeling I came up with: $\delta = \frac{\epsilon}{|x + 3|}$. And I though this was correct: under the assumption that the antecedent is true, we can make the following construct: $$ 0 < |x - 3| < \delta \Rightarrow 0 < |x - 3| < \frac{\epsilon}{|x + 3|} \Rightarrow 0 < |x + 3||x - 3| < \epsilon \Rightarrow |x^2 - 9| < \epsilon $$ Q.E.D, I thought. But the book came up with the following solution: $$ \delta = \min{(1,\frac{\epsilon}{7})} $$ Which is a correct solution. So, is mine wrong? Or did the book just provide a different proof?",,"['real-analysis', 'limits', 'proof-verification', 'epsilon-delta']"
63,Infinity norm of continuous function.,Infinity norm of continuous function.,,"Let $f$ be a continuous function on the measure space $\mathbb{R}^n,\mathcal{L},\lambda$ (Lebesgue measure). Prove that $\|f\|_\infty = \sup\{|f(x)|$ $|$ $x \in \mathbb{R}^n\}$ I saw same problem but I couldn't understand the answer. My idea is this: Let $\|f\|_\infty=K$ . Since $f$ is continuous, a small neighborhood of any $x\in  \mathbb{R}^n$ , say $p$ should satisfy $|f(p)|\leq M,$ for $M>K$ . So I can take $supremum$ over p. But I don't know how to proceed rigorously. Any help will be thankful.","Let be a continuous function on the measure space (Lebesgue measure). Prove that I saw same problem but I couldn't understand the answer. My idea is this: Let . Since is continuous, a small neighborhood of any , say should satisfy for . So I can take over p. But I don't know how to proceed rigorously. Any help will be thankful.","f \mathbb{R}^n,\mathcal{L},\lambda \|f\|_\infty = \sup\{|f(x)| | x \in \mathbb{R}^n\} \|f\|_\infty=K f x\in  \mathbb{R}^n p |f(p)|\leq M, M>K supremum","['real-analysis', 'lp-spaces']"
64,Baby Rudin Exercise 4.2,Baby Rudin Exercise 4.2,,"Can someone check my proof? If $f$ is a continuous mapping of a metric space $X$ into a metric space $Y$, prove that $$f(\overline{E}) \subset \overline{f(E)} $$   for every set $E\subset X$. ($\overline{E}$ denotes the closure of $E$.) Proof: Suppose $x\in \overline{E} = E\cup E'$, where $E'$ is the set of limit points of $E$. If $x\in E$, then $f(x)\in f(E)\subset \overline{f(E)}$ and we are done. Now suppose $x\in E'$ and let $\epsilon >0$ be given. Since $f$ is continuous, there exists a $\delta >0$ such that $d(x,y)<\delta$ implies $$d(f(x),f(y))<\epsilon. $$ Since $x$ is a limit point of $E$, there is a neighborhood $N_{\delta}(x)$ of radius $\delta$ about $x$ such that $(N_{\delta}(x)\setminus \{x\}) \cap E \neq \emptyset$. Then $p\in N_{\delta}(x)$ implies that $d(f(x),f(p))<\epsilon$. Hence, for any $\epsilon >0$, we can always find a $\delta$ such that $$\left( N_{\epsilon}(f(x)) \setminus \{f(x)\}\right) \cap f(E) \neq \emptyset.$$  Thus $f(x)\in f(E)'\subset \overline{f(E)}$. I'm not really sure about the last part, how can we make certain that if $p\in N_{\delta}(x) \setminus \{x\}$, then $f(p) \in N_{\epsilon}(f(x))\setminus \{f(x)\}$. Since we aren't given that $f$ is injective, we can have $x\neq p$ and $f(x) = f(p)$? Any input or alternative approaches would be greatly appreciated!","Can someone check my proof? If $f$ is a continuous mapping of a metric space $X$ into a metric space $Y$, prove that $$f(\overline{E}) \subset \overline{f(E)} $$   for every set $E\subset X$. ($\overline{E}$ denotes the closure of $E$.) Proof: Suppose $x\in \overline{E} = E\cup E'$, where $E'$ is the set of limit points of $E$. If $x\in E$, then $f(x)\in f(E)\subset \overline{f(E)}$ and we are done. Now suppose $x\in E'$ and let $\epsilon >0$ be given. Since $f$ is continuous, there exists a $\delta >0$ such that $d(x,y)<\delta$ implies $$d(f(x),f(y))<\epsilon. $$ Since $x$ is a limit point of $E$, there is a neighborhood $N_{\delta}(x)$ of radius $\delta$ about $x$ such that $(N_{\delta}(x)\setminus \{x\}) \cap E \neq \emptyset$. Then $p\in N_{\delta}(x)$ implies that $d(f(x),f(p))<\epsilon$. Hence, for any $\epsilon >0$, we can always find a $\delta$ such that $$\left( N_{\epsilon}(f(x)) \setminus \{f(x)\}\right) \cap f(E) \neq \emptyset.$$  Thus $f(x)\in f(E)'\subset \overline{f(E)}$. I'm not really sure about the last part, how can we make certain that if $p\in N_{\delta}(x) \setminus \{x\}$, then $f(p) \in N_{\epsilon}(f(x))\setminus \{f(x)\}$. Since we aren't given that $f$ is injective, we can have $x\neq p$ and $f(x) = f(p)$? Any input or alternative approaches would be greatly appreciated!",,"['real-analysis', 'proof-verification']"
65,Proving that $c$ is a Banach space.,Proving that  is a Banach space.,c,"I want to prove that $$c = \{  (x_n)_{n\geq 0} \mid x_n \in \Bbb C \text{ and the sequence converges} \}$$ with the norm $$ \left\|(x_n)_{n\geq 0}\right\|_{\infty} =\sup_{n\geq 0} |x_n|$$ is a Banach space. I have done some work, but I am having trouble concluding. So far: let $(\xi_n)_{n\geq 0} = \left( (x_k^{(n)})_{k\geq 0} \right)_{n\geq 0}$ be a $\|\cdot\|_{\infty}$-Cauchy sequence. Let $\epsilon > 0$, there exists $n_0 \in \Bbb N$ such that: $$\begin{align}  \| \xi_n - \xi_m \|_{\infty} &< \epsilon, \quad \forall\, n,m > n_0  \\ \sup_{k \geq 0} |x_k^{(n)}-x_k^{(m)}| &< \epsilon, \quad \forall\,n,m > n_0 \\  |x_k^{(n)} - x_k^{(m)}| &< \epsilon, \quad \forall\, k\geq 0, \quad \forall\,n,m > n_0 \end{align}$$ So fixed $k$, $(x_k^{(n)})_{n \geq 0}$ is a $|\cdot |$-Cauchy sequence, and since $\Bbb C$ is Banach, there exists a limit $\lim_{n\to \infty} x_k^{(n)} =: x_k$. Then define $\xi = (x_k)_{k \geq 0}$. Now I understand I have two things to do: prove that $\xi_n \stackrel{\|\cdot \|_{\infty}}{\longrightarrow} \xi $: we proceed as before. Let $\epsilon > 0$. Now, there is $n_0 \in \Bbb N$ such that: $$\begin{align}  \| \xi_n - \xi_m \|_{\infty} &< \epsilon, \quad \forall\, n,m > n_0  \\ \sup_{k \geq 0} |x_k^{(n)}-x_k^{(m)}| &< \epsilon, \quad \forall\,n,m > n_0 \\  |x_k^{(n)} - x_k^{(m)}| &< \epsilon, \quad \forall\, k\geq 0, \quad \forall\,n,m > n_0  \\   \lim_{m \to \infty} |x_k^{(n)} - x_k^{(m)}| &\leq \epsilon, \quad \forall\,k\geq 0, \quad \forall\,n >n_0 \\  |x_k^{(n)} - x_k| &\leq \epsilon,  \quad \forall\,k\geq 0 \quad \forall\, n>n_0 \\ \sup_{k\geq 0} |x_k^{(n)} - x_k| &\leq \epsilon,\quad \forall\, n > n_0 \\ \| \xi_n - \xi\|_{\infty} &\leq\epsilon, \quad \forall\,n>n_0, \end{align}$$ so ok. prove that $\xi \in c$. I'm not sure of how to do this. I think I must use that every $(x_k^{(n)})_{k \geq 0}$ converge, because I don't seem to have used this yet. How can I prove that $\xi \in c $? Thanks.","I want to prove that $$c = \{  (x_n)_{n\geq 0} \mid x_n \in \Bbb C \text{ and the sequence converges} \}$$ with the norm $$ \left\|(x_n)_{n\geq 0}\right\|_{\infty} =\sup_{n\geq 0} |x_n|$$ is a Banach space. I have done some work, but I am having trouble concluding. So far: let $(\xi_n)_{n\geq 0} = \left( (x_k^{(n)})_{k\geq 0} \right)_{n\geq 0}$ be a $\|\cdot\|_{\infty}$-Cauchy sequence. Let $\epsilon > 0$, there exists $n_0 \in \Bbb N$ such that: $$\begin{align}  \| \xi_n - \xi_m \|_{\infty} &< \epsilon, \quad \forall\, n,m > n_0  \\ \sup_{k \geq 0} |x_k^{(n)}-x_k^{(m)}| &< \epsilon, \quad \forall\,n,m > n_0 \\  |x_k^{(n)} - x_k^{(m)}| &< \epsilon, \quad \forall\, k\geq 0, \quad \forall\,n,m > n_0 \end{align}$$ So fixed $k$, $(x_k^{(n)})_{n \geq 0}$ is a $|\cdot |$-Cauchy sequence, and since $\Bbb C$ is Banach, there exists a limit $\lim_{n\to \infty} x_k^{(n)} =: x_k$. Then define $\xi = (x_k)_{k \geq 0}$. Now I understand I have two things to do: prove that $\xi_n \stackrel{\|\cdot \|_{\infty}}{\longrightarrow} \xi $: we proceed as before. Let $\epsilon > 0$. Now, there is $n_0 \in \Bbb N$ such that: $$\begin{align}  \| \xi_n - \xi_m \|_{\infty} &< \epsilon, \quad \forall\, n,m > n_0  \\ \sup_{k \geq 0} |x_k^{(n)}-x_k^{(m)}| &< \epsilon, \quad \forall\,n,m > n_0 \\  |x_k^{(n)} - x_k^{(m)}| &< \epsilon, \quad \forall\, k\geq 0, \quad \forall\,n,m > n_0  \\   \lim_{m \to \infty} |x_k^{(n)} - x_k^{(m)}| &\leq \epsilon, \quad \forall\,k\geq 0, \quad \forall\,n >n_0 \\  |x_k^{(n)} - x_k| &\leq \epsilon,  \quad \forall\,k\geq 0 \quad \forall\, n>n_0 \\ \sup_{k\geq 0} |x_k^{(n)} - x_k| &\leq \epsilon,\quad \forall\, n > n_0 \\ \| \xi_n - \xi\|_{\infty} &\leq\epsilon, \quad \forall\,n>n_0, \end{align}$$ so ok. prove that $\xi \in c$. I'm not sure of how to do this. I think I must use that every $(x_k^{(n)})_{k \geq 0}$ converge, because I don't seem to have used this yet. How can I prove that $\xi \in c $? Thanks.",,"['real-analysis', 'functional-analysis', 'banach-spaces']"
66,"True or False: If sets $A$ and $B$ have a maxima, and $A \cap B \neq \emptyset$, then $A \cap B$ has a maxima","True or False: If sets  and  have a maxima, and , then  has a maxima",A B A \cap B \neq \emptyset A \cap B,"I am almost certain that the statement in the title is True, but am not 100% sure how to prove it, or if my conclusion is valid. My reasoning is that since $A$ and $B$ both have a maxima, then they have an upperbound which belongs to their respective sets. As such, we can think of $A$ and $B$ as closed intervals, and their instersection should then also at least be bounded above, and closed from the right side. Is my line of thinking correct, is there anything I have missed?","I am almost certain that the statement in the title is True, but am not 100% sure how to prove it, or if my conclusion is valid. My reasoning is that since $A$ and $B$ both have a maxima, then they have an upperbound which belongs to their respective sets. As such, we can think of $A$ and $B$ as closed intervals, and their instersection should then also at least be bounded above, and closed from the right side. Is my line of thinking correct, is there anything I have missed?",,['real-analysis']
67,"Fundamental theorem of calculus, differentiable at the endpoints.","Fundamental theorem of calculus, differentiable at the endpoints.",,"One version states: Let f be a continuous real-valued function defined on a closed   interval $[a,b]$. Let f be the function defined for all x in $[a,b]$,   by $F(x)=\int_{a}^xf(t)dt$. Then, F is continuous on [a,b],   differentiable in the open interval (a,b), and $F'(x)=f(x)$. For all x   in $(a,b)$. http://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus#First_part My question is, can it be proved that the right hand derivative of F at a exists? Or are there examples where the right hand derivative of F at a, does not exist?","One version states: Let f be a continuous real-valued function defined on a closed   interval $[a,b]$. Let f be the function defined for all x in $[a,b]$,   by $F(x)=\int_{a}^xf(t)dt$. Then, F is continuous on [a,b],   differentiable in the open interval (a,b), and $F'(x)=f(x)$. For all x   in $(a,b)$. http://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus#First_part My question is, can it be proved that the right hand derivative of F at a exists? Or are there examples where the right hand derivative of F at a, does not exist?",,"['calculus', 'real-analysis', 'integration', 'derivatives']"
68,Show that $\int_{\mathbb{R}} F(x) F(dx)=\frac{1}{2}$ if $F$ is a continuous distribution function,Show that  if  is a continuous distribution function,\int_{\mathbb{R}} F(x) F(dx)=\frac{1}{2} F,"If $F$ is a continuous distribution function, prove that  \begin{align*} \int_{\mathbb{R}} F(x) F(dx)=\frac{1}{2} \end{align*} What I tried \begin{align*} \int_{\mathbb{R}} F(x) F(dx)&=\int_{\mathbb{R}} P(X \le x) F(dx)=\int_{\mathbb{R}} E[\mathsf{1}_{X \le x}]F(dx)=\int_{\mathbb{R}}  \int_\Omega \mathsf{1}_{X \le x} dP F(dx)\\ &=  \int_\Omega  \int_{\mathbb{R}} \mathsf{1}_{X \le x} F(dx) dP  \end{align*} where the last step is due to Fubini's theorem. If the above is correct what to do next? Thanks","If $F$ is a continuous distribution function, prove that  \begin{align*} \int_{\mathbb{R}} F(x) F(dx)=\frac{1}{2} \end{align*} What I tried \begin{align*} \int_{\mathbb{R}} F(x) F(dx)&=\int_{\mathbb{R}} P(X \le x) F(dx)=\int_{\mathbb{R}} E[\mathsf{1}_{X \le x}]F(dx)=\int_{\mathbb{R}}  \int_\Omega \mathsf{1}_{X \le x} dP F(dx)\\ &=  \int_\Omega  \int_{\mathbb{R}} \mathsf{1}_{X \le x} F(dx) dP  \end{align*} where the last step is due to Fubini's theorem. If the above is correct what to do next? Thanks",,"['real-analysis', 'probability', 'probability-theory']"
69,Continuous and preserves measurability $\implies$ preserves null sets.,Continuous and preserves measurability  preserves null sets.,\implies,"Let $X$ be a (Lebesgue-)measurable set of $\mathbb{R}^n$ and $f:X \to \mathbb{R}^n$ continuous function that preserves measurability ($A$ meausurable $\implies f(A)$ measurable). Prove: for all $A \subset X$,$\space$$\lambda(A)=0 \implies \lambda(f(A)) = 0$ . I'm totally stuck. Initially I made some progress but now I'm at a point where it feels like the statement shouldn't be true at all. Additionally i'd like to know if the statement works for general topological measure spaces (or even just metric measure spaces). That is: Let $X$ be a (Lebesgue-)measurable set of $Y$, Topological (or metric) measure space and $f:X \to Y$ continuous function that preserves measurability etc...","Let $X$ be a (Lebesgue-)measurable set of $\mathbb{R}^n$ and $f:X \to \mathbb{R}^n$ continuous function that preserves measurability ($A$ meausurable $\implies f(A)$ measurable). Prove: for all $A \subset X$,$\space$$\lambda(A)=0 \implies \lambda(f(A)) = 0$ . I'm totally stuck. Initially I made some progress but now I'm at a point where it feels like the statement shouldn't be true at all. Additionally i'd like to know if the statement works for general topological measure spaces (or even just metric measure spaces). That is: Let $X$ be a (Lebesgue-)measurable set of $Y$, Topological (or metric) measure space and $f:X \to Y$ continuous function that preserves measurability etc...",,['real-analysis']
70,"Why ""a function continuous at only one point"" is not an oxymoron?","Why ""a function continuous at only one point"" is not an oxymoron?",,"I understand that there are functions that, by definition of continuity, can be continuous at only one point, such as $$f(x)=\begin{cases} x,&\text{if }x\in\Bbb Q\\ 0,&\text{if }x\in\Bbb R\setminus\Bbb Q\;. \end{cases}$$ which is continuous only at $x=0$.  But it is continuous because it satisfies the formal definition of continuity. Still,  continuity at only-one-point sounds like an oxymoron to my mind. I understand that mathematical concepts are different than the standard meanings of the words in natural languages, so my question is this: Does the classical definition of continuity fail to capture the intended concept of continuity for this pathological case? Has anybody attempted to modify the definition of continuity to make this pathological cases fail? I call it pathological because I imagine that, historically, the original concept of continuity attempted to capture the idea of ""connected"" line. But I might be wrong.","I understand that there are functions that, by definition of continuity, can be continuous at only one point, such as $$f(x)=\begin{cases} x,&\text{if }x\in\Bbb Q\\ 0,&\text{if }x\in\Bbb R\setminus\Bbb Q\;. \end{cases}$$ which is continuous only at $x=0$.  But it is continuous because it satisfies the formal definition of continuity. Still,  continuity at only-one-point sounds like an oxymoron to my mind. I understand that mathematical concepts are different than the standard meanings of the words in natural languages, so my question is this: Does the classical definition of continuity fail to capture the intended concept of continuity for this pathological case? Has anybody attempted to modify the definition of continuity to make this pathological cases fail? I call it pathological because I imagine that, historically, the original concept of continuity attempted to capture the idea of ""connected"" line. But I might be wrong.",,"['real-analysis', 'continuity']"
71,"Can a power series converge uniformly on $(-1,1)$ but not on $[-1,1]?$",Can a power series converge uniformly on  but not on,"(-1,1) [-1,1]?","I am taking a course in analysis, and I am wondering whether it possible for a power series with radius of convergence $1$ to converge uniformly on $(-1,1)$ but not on $[-1,1]?$ I don't think this is possible, since the power series will define a continuous function over $[-1,1]$ (assuming it is defined at $-1$ and $1$) which drags in $-1,$ and $1$ into the game when considering uniform convergence on $(-1,1)$. I can't decide what happens if the series blows up at $-1$ or $1$. It looks like we cannot have uniform convergence, but I am not sure why.","I am taking a course in analysis, and I am wondering whether it possible for a power series with radius of convergence $1$ to converge uniformly on $(-1,1)$ but not on $[-1,1]?$ I don't think this is possible, since the power series will define a continuous function over $[-1,1]$ (assuming it is defined at $-1$ and $1$) which drags in $-1,$ and $1$ into the game when considering uniform convergence on $(-1,1)$. I can't decide what happens if the series blows up at $-1$ or $1$. It looks like we cannot have uniform convergence, but I am not sure why.",,"['real-analysis', 'sequences-and-series', 'power-series', 'uniform-convergence']"
72,$\int_{0}^{\infty} x \cdot \cos(x^3) dx$ convergence,convergence,\int_{0}^{\infty} x \cdot \cos(x^3) dx,"$$\int_{0}^{\infty} x \cdot \cos(x^3) dx$$ I only want to prove, that this integral converges, I don't need to calculate the exact value. I don't know what to do with the cosinus, I can't get rid of it. I know that the integral is equal to $$\frac{1}{3} \cdot \int_{0}^{\infty} \frac{\sin(x^3)}{x^2} dx$$ but here is also the problem, that I can't get rid of the sinus... Any hints?","$$\int_{0}^{\infty} x \cdot \cos(x^3) dx$$ I only want to prove, that this integral converges, I don't need to calculate the exact value. I don't know what to do with the cosinus, I can't get rid of it. I know that the integral is equal to $$\frac{1}{3} \cdot \int_{0}^{\infty} \frac{\sin(x^3)}{x^2} dx$$ but here is also the problem, that I can't get rid of the sinus... Any hints?",,['real-analysis']
73,"Show that if $A,B$ are measurable, $A\subset E\subset B$, and $m(A)=m(B)$, then $E$ is measurable.","Show that if  are measurable, , and , then  is measurable.","A,B A\subset E\subset B m(A)=m(B) E","Here's the full problem: Suppose $A\subset E\subset B$, where $A,B$ are measurable with finite measure. Show that if $m(A)=m(B)$, then $E$ is measurable. Here, we are dealing with measure space $(\mathbb{R},\mathcal{M},m)$. I come here just to see if this is a valid proof. Here we go: Proof We have that $m(A),m(B)<\infty$. Supposing $m(A)=m(B)$, we have $$m(A)\leq m(E) \leq m(B)=m(A)$$   $$\implies m(A)=m(E)$$ Since $A$ is measurable, it follows that $E$ must then be measruable. I know of another way to prove this, I was just wondering if this was a valid argument as well.","Here's the full problem: Suppose $A\subset E\subset B$, where $A,B$ are measurable with finite measure. Show that if $m(A)=m(B)$, then $E$ is measurable. Here, we are dealing with measure space $(\mathbb{R},\mathcal{M},m)$. I come here just to see if this is a valid proof. Here we go: Proof We have that $m(A),m(B)<\infty$. Supposing $m(A)=m(B)$, we have $$m(A)\leq m(E) \leq m(B)=m(A)$$   $$\implies m(A)=m(E)$$ Since $A$ is measurable, it follows that $E$ must then be measruable. I know of another way to prove this, I was just wondering if this was a valid argument as well.",,"['real-analysis', 'lebesgue-measure']"
74,Show that $f$ is a polynomial if it's the uniform limit of polynomais,Show that  is a polynomial if it's the uniform limit of polynomais,f,"Let $f:\Bbb R\to \Bbb R$ be a function which is the uniform limit of polynomials. I want to show that $f$ is a polynomial. I mean this seems a bit trivial...  If it's the uniform limit of the set of polynomials doesn't that guarantee it's a polynomial? If I define $f$ to be the set of all polynomial do I define a uniform limit $L$ s.t $d(f,L)<\epsilon$?  or approach via contradiction? any help would be great","Let $f:\Bbb R\to \Bbb R$ be a function which is the uniform limit of polynomials. I want to show that $f$ is a polynomial. I mean this seems a bit trivial...  If it's the uniform limit of the set of polynomials doesn't that guarantee it's a polynomial? If I define $f$ to be the set of all polynomial do I define a uniform limit $L$ s.t $d(f,L)<\epsilon$?  or approach via contradiction? any help would be great",,['real-analysis']
75,Integral limit of $\sin(x/n)f(x)$,Integral limit of,\sin(x/n)f(x),"For any $f\in L^1[0,\pi]$, evaluate $n\to \infty \int^\pi_0 n$sin$(x/n)f(x)dx$ My idea is, $n$sin$(x/n)f(x)\to xf(x)$ and it seems that it is increasing sequence. I am not able to show it is increasing. Next thing if it increasing how could we apply monotone convergence theorem unless f is positive. Next idea, i tried substitution, taking $x/n$ as $t$ but i get $f(tn)$ after substitution. Then stopped there. Do you have any specific idea for this..","For any $f\in L^1[0,\pi]$, evaluate $n\to \infty \int^\pi_0 n$sin$(x/n)f(x)dx$ My idea is, $n$sin$(x/n)f(x)\to xf(x)$ and it seems that it is increasing sequence. I am not able to show it is increasing. Next thing if it increasing how could we apply monotone convergence theorem unless f is positive. Next idea, i tried substitution, taking $x/n$ as $t$ but i get $f(tn)$ after substitution. Then stopped there. Do you have any specific idea for this..",,"['real-analysis', 'measure-theory', 'lebesgue-integral']"
76,"If limit of $f(x)$ exists and the limit of $f(x)g(x)$ exists, then does the limit of $g(x)$ exist?","If limit of  exists and the limit of  exists, then does the limit of  exist?",f(x) f(x)g(x) g(x),I was doing some exam preparation and I got stuck on this one. Whats the idea behind this question: If $\lim_{x\to a}f(x)$ and $\lim_{x\to a} [f(x)g(x)]$ exist then does $\lim_{x\to a}g(x)$ exist? Thanks..,I was doing some exam preparation and I got stuck on this one. Whats the idea behind this question: If $\lim_{x\to a}f(x)$ and $\lim_{x\to a} [f(x)g(x)]$ exist then does $\lim_{x\to a}g(x)$ exist? Thanks..,,"['real-analysis', 'analysis', 'limits']"
77,Does bounded and continuous implies Lipschitz?,Does bounded and continuous implies Lipschitz?,,"If a function $f : \mathbb{R} \rightarrow \mathbb{R}$ is integrable, bounded and continuous, is it also Lipschitz continuous?","If a function $f : \mathbb{R} \rightarrow \mathbb{R}$ is integrable, bounded and continuous, is it also Lipschitz continuous?",,"['real-analysis', 'functional-analysis', 'ordinary-differential-equations', 'functions', 'bounded-variation']"
78,"Showing limit of sequence $\left(\frac{3}{10}, \frac{33}{100}, \frac{333}{1000}, \dots\right)$",Showing limit of sequence,"\left(\frac{3}{10}, \frac{33}{100}, \frac{333}{1000}, \dots\right)","I'm trying to calculate the limit of the following sequence: $$ (s_n) = \left(\frac{3}{10}, \frac{33}{100}, \frac{333}{1000}, \dots\right). $$ Clearly, $(s_n) \to 1/3$, but I'm not sure how to show it rigorously. I need to be able to generate this to cases where $3$ is any integer $a \in \{ 1, 2, \dots, 9 \}$. I realize that the general limit would be $a/9$, but showing it is proving to be difficult. I'm looking for a way to express the numerator as a function of $n$, where $n$ is the ""length"" of the number. That is, if $a=4$ then $44$ corresponds to $n=2$ and $444$ corresponds to $n=3$. Any ideas?","I'm trying to calculate the limit of the following sequence: $$ (s_n) = \left(\frac{3}{10}, \frac{33}{100}, \frac{333}{1000}, \dots\right). $$ Clearly, $(s_n) \to 1/3$, but I'm not sure how to show it rigorously. I need to be able to generate this to cases where $3$ is any integer $a \in \{ 1, 2, \dots, 9 \}$. I realize that the general limit would be $a/9$, but showing it is proving to be difficult. I'm looking for a way to express the numerator as a function of $n$, where $n$ is the ""length"" of the number. That is, if $a=4$ then $44$ corresponds to $n=2$ and $444$ corresponds to $n=3$. Any ideas?",,"['real-analysis', 'sequences-and-series', 'limits']"
79,Every dense $G_\delta$ subset of $\Bbb R$ is uncountable,Every dense  subset of  is uncountable,G_\delta \Bbb R,Every dense $G_\delta$ subset of $\Bbb R$ is uncountable. I know that I have to use Baire's Theorem but I don't know how. Thanks!,Every dense $G_\delta$ subset of $\Bbb R$ is uncountable. I know that I have to use Baire's Theorem but I don't know how. Thanks!,,['real-analysis']
80,About linear bijection between Banach spaces!,About linear bijection between Banach spaces!,,"It is well-known that, by Banach theorem, every continuous, linear and bijective operator between Banach spaces is a isomorphism. There must be a linear bijective and discontinuous operator between Banach spaces! How can we show/construct such a map? Thanks for all helpings!","It is well-known that, by Banach theorem, every continuous, linear and bijective operator between Banach spaces is a isomorphism. There must be a linear bijective and discontinuous operator between Banach spaces! How can we show/construct such a map? Thanks for all helpings!",,"['real-analysis', 'examples-counterexamples']"
81,"Implicit function, not obvious version","Implicit function, not obvious version",,"I'm not sure if the title is meaningful. Here is the problem: Let $F: \mathbb{R}^2 \rightarrow \mathbb{R}$ be class $C^1$ and $F(0,0)=0$ and $\forall (x,y)\in \mathbb{R}^2: 0<|\frac{\partial F}{\partial x}(x,y)|<|\frac{\partial F}{\partial y}(x,y)|$ Prove that there exists $\varphi : \mathbb{R} \rightarrow \mathbb{R}$ class $C^1$ such that $\{ (x,y)\in \mathbb{R}^2 | F(x,y)=0\} = \{(x, \varphi(x) | x \in \mathbb{R})\}$ I have no idea how to approach this. Could you help me? Thank you!","I'm not sure if the title is meaningful. Here is the problem: Let $F: \mathbb{R}^2 \rightarrow \mathbb{R}$ be class $C^1$ and $F(0,0)=0$ and $\forall (x,y)\in \mathbb{R}^2: 0<|\frac{\partial F}{\partial x}(x,y)|<|\frac{\partial F}{\partial y}(x,y)|$ Prove that there exists $\varphi : \mathbb{R} \rightarrow \mathbb{R}$ class $C^1$ such that $\{ (x,y)\in \mathbb{R}^2 | F(x,y)=0\} = \{(x, \varphi(x) | x \in \mathbb{R})\}$ I have no idea how to approach this. Could you help me? Thank you!",,"['real-analysis', 'multivariable-calculus']"
82,Show that: $\inf(A+B) = \inf(A)+ \inf(B)$,Show that:,\inf(A+B) = \inf(A)+ \inf(B),"Let $A,B$ be non-empty, bounded subsets of $R$ and $A+B=\{a+b:a \in A,b \in B\}$. Show that: $$\inf(A+B)=\inf(A)+\inf(B)$$ This is what I have tried: Suppose that $x \in A+B \Rightarrow x=a+b, \ a \in A, \ b \in B$. We know that $a\geq \inf A \text{ and } b\geq \inf(B)$ . So $x \geq \inf(A)+\inf(B) \Rightarrow\inf(A+B) \geq \inf(A)+\inf(B)$ If $x\in A \Rightarrow x \in A+B, \text{ so we conclude that } \inf(A) \leq  \inf(A+B)$ and if $x \in B \Rightarrow x \in A+B,\text{ so we conclude that } \inf(B) \leq  \inf(A+B)$. Therefore, we have that $\displaystyle \frac{\inf(A)+\inf(B)}{2} \leq \inf(A)+\inf(B) \leq \inf(A+B)$. Could you tell me if it is right?","Let $A,B$ be non-empty, bounded subsets of $R$ and $A+B=\{a+b:a \in A,b \in B\}$. Show that: $$\inf(A+B)=\inf(A)+\inf(B)$$ This is what I have tried: Suppose that $x \in A+B \Rightarrow x=a+b, \ a \in A, \ b \in B$. We know that $a\geq \inf A \text{ and } b\geq \inf(B)$ . So $x \geq \inf(A)+\inf(B) \Rightarrow\inf(A+B) \geq \inf(A)+\inf(B)$ If $x\in A \Rightarrow x \in A+B, \text{ so we conclude that } \inf(A) \leq  \inf(A+B)$ and if $x \in B \Rightarrow x \in A+B,\text{ so we conclude that } \inf(B) \leq  \inf(A+B)$. Therefore, we have that $\displaystyle \frac{\inf(A)+\inf(B)}{2} \leq \inf(A)+\inf(B) \leq \inf(A+B)$. Could you tell me if it is right?",,"['real-analysis', 'solution-verification', 'supremum-and-infimum']"
83,Why the set of discontinuities can't be uncountable,Why the set of discontinuities can't be uncountable,,I am trying to understand why the set of discontinuities of an increasing function $f: \mathbb R \to \mathbb R$ must be finite or countable. I showed that such function can only have jump discontinuities. It is not clear to me why the discontinuities can't be uncountable and it is also not clear to me for given $f$ how to find a bijection between discontinuities of $f$ and a subset of $\mathbb Q$. Please can somebody explain me why it should be true?,I am trying to understand why the set of discontinuities of an increasing function $f: \mathbb R \to \mathbb R$ must be finite or countable. I showed that such function can only have jump discontinuities. It is not clear to me why the discontinuities can't be uncountable and it is also not clear to me for given $f$ how to find a bijection between discontinuities of $f$ and a subset of $\mathbb Q$. Please can somebody explain me why it should be true?,,['real-analysis']
84,"If $x\notin\mathbb{Q}$, then $\left|x-\frac{p}{q}\right|<\frac{1}{q^{2+\epsilon}}$ for infinitely many $\frac{p}{q}$?","If , then  for infinitely many ?",x\notin\mathbb{Q} \left|x-\frac{p}{q}\right|<\frac{1}{q^{2+\epsilon}} \frac{p}{q},"This appears on Problem 1 of Chapter 1 in Stein & Shakarchi's Real Analysis: Given an irrational $x$, one can show (using the pigeon-hole principle, for example) that there are infinitely many fractions $p/q$, with relatively prime integers $p$ and $q$ such that $$\left|x-\frac{p}{q}\right|\leq\frac{1}{q^2}.$$   However, prove that the set of all $x\in\mathbb R$ such that there exist infinitely many fractions $p/q$, with relatively prime integers $p$ and $q$ such that $$\left|x-\frac{p}{q}\right|\leq\frac{1}{q^{3}} \text{ (or} \leq 1/q^{\epsilon+2})$$ is a set of measure zero. By the hint, I am trying to prove this using the Borel-Cantelli lemma. As my family of countable sets, I'm considering $E_{q} = \{x : \exists p. |x - p/q| < 1/q^{2+\epsilon} \}$. I'm having trouble showing that the sum of the measures of $E_{q}$ converges. I can show that $E_q$ is the union of intervals of length $2/q^{2 + \epsilon}$, but I can't put a bound on the total number of intervals because the problem statement says $x \in \mathbb{R}$ so $p$ could be arbitrarily large. I found a similar problem statement in this question, but there the bound comes from the fact that $x \in [0,1]$. Am I considering the wrong family of sets for this problem, mis-interpreting the problem statement, or encountering some other issue?","This appears on Problem 1 of Chapter 1 in Stein & Shakarchi's Real Analysis: Given an irrational $x$, one can show (using the pigeon-hole principle, for example) that there are infinitely many fractions $p/q$, with relatively prime integers $p$ and $q$ such that $$\left|x-\frac{p}{q}\right|\leq\frac{1}{q^2}.$$   However, prove that the set of all $x\in\mathbb R$ such that there exist infinitely many fractions $p/q$, with relatively prime integers $p$ and $q$ such that $$\left|x-\frac{p}{q}\right|\leq\frac{1}{q^{3}} \text{ (or} \leq 1/q^{\epsilon+2})$$ is a set of measure zero. By the hint, I am trying to prove this using the Borel-Cantelli lemma. As my family of countable sets, I'm considering $E_{q} = \{x : \exists p. |x - p/q| < 1/q^{2+\epsilon} \}$. I'm having trouble showing that the sum of the measures of $E_{q}$ converges. I can show that $E_q$ is the union of intervals of length $2/q^{2 + \epsilon}$, but I can't put a bound on the total number of intervals because the problem statement says $x \in \mathbb{R}$ so $p$ could be arbitrarily large. I found a similar problem statement in this question, but there the bound comes from the fact that $x \in [0,1]$. Am I considering the wrong family of sets for this problem, mis-interpreting the problem statement, or encountering some other issue?",,['real-analysis']
85,Infinite Series $\sum\limits_{n=-\infty}^{\infty}\frac{1}{(x+n\pi)^m}$,Infinite Series,\sum\limits_{n=-\infty}^{\infty}\frac{1}{(x+n\pi)^m},How can we find a closed form for the following infinite series for any $m\in\mathbb N$? $$\sum_{n=-\infty}^{\infty}\frac{1}{(x+n\pi)^m}$$,How can we find a closed form for the following infinite series for any $m\in\mathbb N$? $$\sum_{n=-\infty}^{\infty}\frac{1}{(x+n\pi)^m}$$,,"['real-analysis', 'sequences-and-series', 'complex-analysis', 'closed-form', 'riemann-zeta']"
86,"Proposed proof for: if $\{s_{n}\}$ is bounded, then $\{\frac{s_{n}}{n}\}$ is convergent.","Proposed proof for: if  is bounded, then  is convergent.",\{s_{n}\} \{\frac{s_{n}}{n}\},"I have written a proposed proof for the proposition below, but I am not entirely certain if it is valid. Could someone take a look at it, and let me know if you see any errors or steps that could use more justification? Proposition 1: Let $\{s_{n}\}$ be a sequence of real numbers. If $\{s_{n}\}$ is bounded, prove that $\{s_n/n\}$ is convergent. Proof. Assume that $\{s_{n}\}$ is bounded. Then, by definition, there exists $M\in\mathbb{R},M>0$ such that $$|s_{n}|\leq M\qquad\forall n.$$ That is,  $$ -M\leq s_{n}\leq M\qquad\forall n. $$ It follows that $$ \frac{-M}{n}\leq\frac{s_{n}}{n}\leq\frac{M}{n}\qquad\forall n,$$ and so  $$ |\frac{s_{n}}{n}|\leq\frac{M}{n}\qquad\forall n.$$ Therefore, we have  $$ |\frac{s_{n}}{n}-0|\leq\frac{M}{n}\qquad\forall n.$$ Now, for every $\epsilon>0$ and every $M>0$, there exists an $n\in\mathbb{N}$ such that $\frac{M}{n}<\epsilon$. For, suppose not: then there exists $\epsilon>0$ and $M>0$ such that for every $n\in\mathbb{N}$, we have $\frac{M}{n}\geq\epsilon$. It follows that  $n\leq\frac{M}{\epsilon}\;\forall n$. This, however, contradicts the fact that $\mathbb{N}$ is unbounded above. Thus, our original claim must hold. Furthermore, if for some $\epsilon>0$, $M>0,$ and some $n\in\mathbb{N}$, we have  $\frac{M}{n}<\epsilon$, then $\frac{M}{p}<\epsilon$ for every $p\in\mathbb{N}$, where $p>n$ (should I include a proof by induction of this statement as a lemma? Or is it sufficiently obvious?). But if it is true that for every $\epsilon>0$ and every $M>0$, there exists an $n\in\mathbb{N}$ such that $\frac{M}{n}<\epsilon$, it implies that we can make the expression $|\frac{s_{n}}{n}-0|$ less than any given $\epsilon$. Therefore, for every $\epsilon>0$, there exists an $N\in\mathbb{N}$ such that $n>N$ implies  $|\frac{_{s_{n}}}{n}-0|\leq\frac{M}{n}<\epsilon$. It follows that $\{\frac{s_{n}}{n}\}$ converges, and, in particular, that $\{\frac{s_{n}}{n}\}\rightarrow0$ as $n\rightarrow\infty$. QED Thanks in advance!","I have written a proposed proof for the proposition below, but I am not entirely certain if it is valid. Could someone take a look at it, and let me know if you see any errors or steps that could use more justification? Proposition 1: Let $\{s_{n}\}$ be a sequence of real numbers. If $\{s_{n}\}$ is bounded, prove that $\{s_n/n\}$ is convergent. Proof. Assume that $\{s_{n}\}$ is bounded. Then, by definition, there exists $M\in\mathbb{R},M>0$ such that $$|s_{n}|\leq M\qquad\forall n.$$ That is,  $$ -M\leq s_{n}\leq M\qquad\forall n. $$ It follows that $$ \frac{-M}{n}\leq\frac{s_{n}}{n}\leq\frac{M}{n}\qquad\forall n,$$ and so  $$ |\frac{s_{n}}{n}|\leq\frac{M}{n}\qquad\forall n.$$ Therefore, we have  $$ |\frac{s_{n}}{n}-0|\leq\frac{M}{n}\qquad\forall n.$$ Now, for every $\epsilon>0$ and every $M>0$, there exists an $n\in\mathbb{N}$ such that $\frac{M}{n}<\epsilon$. For, suppose not: then there exists $\epsilon>0$ and $M>0$ such that for every $n\in\mathbb{N}$, we have $\frac{M}{n}\geq\epsilon$. It follows that  $n\leq\frac{M}{\epsilon}\;\forall n$. This, however, contradicts the fact that $\mathbb{N}$ is unbounded above. Thus, our original claim must hold. Furthermore, if for some $\epsilon>0$, $M>0,$ and some $n\in\mathbb{N}$, we have  $\frac{M}{n}<\epsilon$, then $\frac{M}{p}<\epsilon$ for every $p\in\mathbb{N}$, where $p>n$ (should I include a proof by induction of this statement as a lemma? Or is it sufficiently obvious?). But if it is true that for every $\epsilon>0$ and every $M>0$, there exists an $n\in\mathbb{N}$ such that $\frac{M}{n}<\epsilon$, it implies that we can make the expression $|\frac{s_{n}}{n}-0|$ less than any given $\epsilon$. Therefore, for every $\epsilon>0$, there exists an $N\in\mathbb{N}$ such that $n>N$ implies  $|\frac{_{s_{n}}}{n}-0|\leq\frac{M}{n}<\epsilon$. It follows that $\{\frac{s_{n}}{n}\}$ converges, and, in particular, that $\{\frac{s_{n}}{n}\}\rightarrow0$ as $n\rightarrow\infty$. QED Thanks in advance!",,['real-analysis']
87,Prove that f(x)=g(x),Prove that f(x)=g(x),,"Show that if $f,g:\mathbb{R}\to \mathbb{R}$ are continuous and periodic and $\lim_{x\to \infty}[f(x)-g(x)]=0$,   then $f=g$","Show that if $f,g:\mathbb{R}\to \mathbb{R}$ are continuous and periodic and $\lim_{x\to \infty}[f(x)-g(x)]=0$,   then $f=g$",,"['real-analysis', 'continuity', 'periodic-functions']"
88,Domain whose boundry has non zero volume.,Domain whose boundry has non zero volume.,,"Can There be a domain in $\mathbb{R^n}$, for any $n$ such that some domain has non zero boundry volume? I.E. volume of boundry is non zero? Motivation: In some theorems, it is specified that volume of boundary is non zero. But I cannot think of domains where volume of boundry is non zero. EDIT If domain by definition is expected to be open subset of $\mathbb{R^n}$, then I would be looking for such open subsets. Thank You.","Can There be a domain in $\mathbb{R^n}$, for any $n$ such that some domain has non zero boundry volume? I.E. volume of boundry is non zero? Motivation: In some theorems, it is specified that volume of boundary is non zero. But I cannot think of domains where volume of boundry is non zero. EDIT If domain by definition is expected to be open subset of $\mathbb{R^n}$, then I would be looking for such open subsets. Thank You.",,['real-analysis']
89,equivalency of weak and strong convergence,equivalency of weak and strong convergence,,"Why are weak convergence and strong convergence equivalent in finite dimensional spaces?  Please tell me, Where can I find a proof for this? Thanks.","Why are weak convergence and strong convergence equivalent in finite dimensional spaces?  Please tell me, Where can I find a proof for this? Thanks.",,"['real-analysis', 'functional-analysis']"
90,Find a sequence with an interesting property,Find a sequence with an interesting property,,"I was reading an Elon Lages Lima's book (""Curso de Análise"", IMPA) and there is an interesting question. Find a function $f$ from $\mathbb N$ onto $\mathbb N$ such that $\forall n\in \mathbb N$, the preimage set $f^{-1}(\{n\})$ is infinite. I don´t have any idea how to construct that.","I was reading an Elon Lages Lima's book (""Curso de Análise"", IMPA) and there is an interesting question. Find a function $f$ from $\mathbb N$ onto $\mathbb N$ such that $\forall n\in \mathbb N$, the preimage set $f^{-1}(\{n\})$ is infinite. I don´t have any idea how to construct that.",,"['real-analysis', 'sequences-and-series']"
91,Injectivity and Surjectivity of the Exponential Function,Injectivity and Surjectivity of the Exponential Function,,Why is the exponential function injective but not surjective?,Why is the exponential function injective but not surjective?,,['real-analysis']
92,Real Analysis Question from Terence Tao's Book,Real Analysis Question from Terence Tao's Book,,"I was having trouble with what I believe to be a rather easy problem from section six of Terence Tao's Introduction to measure theory.  Help will be appreciated. If $F$ is everywhere differentiable (and thus $F$ is continuous), show $F'$ is measurable.","I was having trouble with what I believe to be a rather easy problem from section six of Terence Tao's Introduction to measure theory.  Help will be appreciated. If $F$ is everywhere differentiable (and thus $F$ is continuous), show $F'$ is measurable.",,"['real-analysis', 'measure-theory']"
93,Convergence in measure of characteristic functions,Convergence in measure of characteristic functions,,"I was having trouble starting this problem.  I would appreciate some help.  Thanks in advance. Let $E_1, E_2, \ldots$ be measurable sets.  Suppose that the functions $f_j = 1_{E_j}$ converge in measure to a limit function $f$.  Show that $f$ is $a.e.$ equal to $1_E$ for some measurable set $E$.","I was having trouble starting this problem.  I would appreciate some help.  Thanks in advance. Let $E_1, E_2, \ldots$ be measurable sets.  Suppose that the functions $f_j = 1_{E_j}$ converge in measure to a limit function $f$.  Show that $f$ is $a.e.$ equal to $1_E$ for some measurable set $E$.",,"['real-analysis', 'measure-theory', 'convergence-divergence']"
94,Is $ \{x\in E | f(x) \le g(x)\}$ measurable if f and g are measurable?,Is  measurable if f and g are measurable?, \{x\in E | f(x) \le g(x)\},"I want to show that the set  $$X = \{x\in E | f(x) \le g(x)\}$$  is measurable. Here $f$ and $g$ are measurable functions over a measurable set $E$. My solution is,  $$X = \cup_{q \in Q} \{\ \{x |  f(x) \le q\} \cap \{x| g(x) \ge q\}\}$$  is measurable, because RHS is countable union of measurable sets which should be measurable. But aren't we ignoring the cases where $f(x)\le a$ and $g(x)\ge a$, where $a$ is irrational.","I want to show that the set  $$X = \{x\in E | f(x) \le g(x)\}$$  is measurable. Here $f$ and $g$ are measurable functions over a measurable set $E$. My solution is,  $$X = \cup_{q \in Q} \{\ \{x |  f(x) \le q\} \cap \{x| g(x) \ge q\}\}$$  is measurable, because RHS is countable union of measurable sets which should be measurable. But aren't we ignoring the cases where $f(x)\le a$ and $g(x)\ge a$, where $a$ is irrational.",,"['real-analysis', 'measure-theory']"
95,$f(\bigcap K_n)=\bigcap f(K_n)$? Where $K_n$ are compact,? Where  are compact,f(\bigcap K_n)=\bigcap f(K_n) K_n,"$f:X\rightarrow Y$ is continous map between metric spaces, $K_n$ are non empty nested sequence of compact subsets of $X$, then we need to  show the  title above. Please tell me which result I should apply here? regarding cont map and compact set I know that image of compact set is compact, attains bounds, uniformly continous etc. please help. well, we can start by taking $y\in \bigcap f(K_n)$ and then show that it is also in the left side?","$f:X\rightarrow Y$ is continous map between metric spaces, $K_n$ are non empty nested sequence of compact subsets of $X$, then we need to  show the  title above. Please tell me which result I should apply here? regarding cont map and compact set I know that image of compact set is compact, attains bounds, uniformly continous etc. please help. well, we can start by taking $y\in \bigcap f(K_n)$ and then show that it is also in the left side?",,"['real-analysis', 'general-topology', 'metric-spaces']"
96,$\epsilon$-$\delta$ proof that $f$ is continuous for $x\notin\mathbb Q$ but isn't for $x\in\mathbb Q$,- proof that  is continuous for  but isn't for,\epsilon \delta f x\notin\mathbb Q x\in\mathbb Q,"I'm trying to give an $\epsilon$-$\delta$ proof that the following function $f$ is continuous for $x\notin\mathbb Q$ but isn't for $x\in\mathbb Q$. Let $f:\mathbb{A\subset R\to R}, \mathbb{A=\{x\in R| x>0\}}$ be given by: $$   f(x) = \begin{cases} 1/n,&x=m/n\in\mathbb Q \\ 0,&x\notin\mathbb Q \end{cases} $$ where $m/n$ is in the lowest terms. Can anyone help me with this proof (I'd prefer an answer with an $\epsilon$-$\delta$ proof). Thank you very much!","I'm trying to give an $\epsilon$-$\delta$ proof that the following function $f$ is continuous for $x\notin\mathbb Q$ but isn't for $x\in\mathbb Q$. Let $f:\mathbb{A\subset R\to R}, \mathbb{A=\{x\in R| x>0\}}$ be given by: $$   f(x) = \begin{cases} 1/n,&x=m/n\in\mathbb Q \\ 0,&x\notin\mathbb Q \end{cases} $$ where $m/n$ is in the lowest terms. Can anyone help me with this proof (I'd prefer an answer with an $\epsilon$-$\delta$ proof). Thank you very much!",,"['calculus', 'real-analysis', 'functions', 'continuity']"
97,How to find limit of the sequence $\sum\limits_{k=1}^n\frac{1}{\sqrt {n^2 +kn}}$?,How to find limit of the sequence ?,\sum\limits_{k=1}^n\frac{1}{\sqrt {n^2 +kn}},How can I find the limit $$\lim_{n\to\infty}\displaystyle\sum_{k=1}^n\frac{1}{\sqrt {n^2 +kn}} \quad?$$ I have tried to solve it using squeeze theorem: $$\sum_{k=1}^n\frac{1}{\sqrt {n^2 +kn}} > \displaystyle\sum_{k=1}^n\frac{1}{\sqrt {n^2 +n^2}} = \displaystyle\sum_{k=1}^n\frac{1}{\sqrt {2n^2}}=\frac{1}{\sqrt {2}} $$ and $$\sum_{k=1}^n\frac{1}{\sqrt {n^2 +kn}} <\displaystyle\sum_{k=1}^n\frac{1}{\sqrt {n^2}} = 1.$$ But I could not find the sequences with the same limits. Please help - how to solve this?,How can I find the limit $$\lim_{n\to\infty}\displaystyle\sum_{k=1}^n\frac{1}{\sqrt {n^2 +kn}} \quad?$$ I have tried to solve it using squeeze theorem: $$\sum_{k=1}^n\frac{1}{\sqrt {n^2 +kn}} > \displaystyle\sum_{k=1}^n\frac{1}{\sqrt {n^2 +n^2}} = \displaystyle\sum_{k=1}^n\frac{1}{\sqrt {2n^2}}=\frac{1}{\sqrt {2}} $$ and $$\sum_{k=1}^n\frac{1}{\sqrt {n^2 +kn}} <\displaystyle\sum_{k=1}^n\frac{1}{\sqrt {n^2}} = 1.$$ But I could not find the sequences with the same limits. Please help - how to solve this?,,"['real-analysis', 'sequences-and-series']"
98,boundedness of an operator,boundedness of an operator,,"Define $T: L^2(\mathbb{R})\to L^2(\mathbb{R})$ by $(Tf)(x)=\int_{\mathbb{R}}\frac{f(y)}{1+|x|+|y|}dy$. Is this operator bounded? If it is, then is it also compact? I got stuck in simply applying Cauchy-Schwarz inequality to show it is bounded. Any hint on this problem? Thanks in advance! Remark: 1, I have tried to set $f_n(y)=\frac{1}{\sqrt{n}}1_{[0,n]}$, then $\{||Tf_n||_2\} $ are uniformly bounded, so I guess $T$ is a bounded operator. More details as following: As above, take $f_n(y)=\frac{1}{\sqrt{n}}1_{[0,n]}$, then $(Tf_n)(x)=\frac{1}{\sqrt{n}}\int_0^n\frac{1}{1+|x|+y}dy$, so $||Tf_n||_2^2=\int_{\mathbb{R}^1}\frac{1}{n}|\int_0^n\frac{1}{1+|x|+y}dy|^2dx=\int_{\mathbb{R}^1}\frac{1}{n}(ln(1+|x|+y)|_{y=0}^{y=n})^2dx=\int_{\mathbb{R}^1}\frac{1}{n}(ln(\frac{1+|x|+n}{1+|x|}))^2dx=\frac{2}{n}\int_0^{\infty}(ln(\frac{1+x+n}{1+x}))^2dx=\frac{2}{n}\int_1^{\infty}(ln(1+\frac{n}{y}))^2dy\overset{z=\frac{n}{y}}{=}2\int_{0}^n\frac{ln(1+z)^2}{z^2}dz\leq 2\int_{0}^{\infty}\frac{ln(1+z)^2}{z^2}dz$, while since $z\to 0, \frac{ln(1+z)^2}{z^2}\to 1; z\to\infty, \frac{ln(1+z)^2}{z^2}\leq \frac{1}{z^{1+\epsilon}}, \forall 0<\epsilon<1$, so value of this integral is finite. 2, Note that the kernel $K(x,y)=\frac{1}{1+|x|+|y|}$ appearing here does not belong to $L^2(\mathbb{R})$; otherwise, all the problems can be solved, for example, see Hilbert-Schmidt operator","Define $T: L^2(\mathbb{R})\to L^2(\mathbb{R})$ by $(Tf)(x)=\int_{\mathbb{R}}\frac{f(y)}{1+|x|+|y|}dy$. Is this operator bounded? If it is, then is it also compact? I got stuck in simply applying Cauchy-Schwarz inequality to show it is bounded. Any hint on this problem? Thanks in advance! Remark: 1, I have tried to set $f_n(y)=\frac{1}{\sqrt{n}}1_{[0,n]}$, then $\{||Tf_n||_2\} $ are uniformly bounded, so I guess $T$ is a bounded operator. More details as following: As above, take $f_n(y)=\frac{1}{\sqrt{n}}1_{[0,n]}$, then $(Tf_n)(x)=\frac{1}{\sqrt{n}}\int_0^n\frac{1}{1+|x|+y}dy$, so $||Tf_n||_2^2=\int_{\mathbb{R}^1}\frac{1}{n}|\int_0^n\frac{1}{1+|x|+y}dy|^2dx=\int_{\mathbb{R}^1}\frac{1}{n}(ln(1+|x|+y)|_{y=0}^{y=n})^2dx=\int_{\mathbb{R}^1}\frac{1}{n}(ln(\frac{1+|x|+n}{1+|x|}))^2dx=\frac{2}{n}\int_0^{\infty}(ln(\frac{1+x+n}{1+x}))^2dx=\frac{2}{n}\int_1^{\infty}(ln(1+\frac{n}{y}))^2dy\overset{z=\frac{n}{y}}{=}2\int_{0}^n\frac{ln(1+z)^2}{z^2}dz\leq 2\int_{0}^{\infty}\frac{ln(1+z)^2}{z^2}dz$, while since $z\to 0, \frac{ln(1+z)^2}{z^2}\to 1; z\to\infty, \frac{ln(1+z)^2}{z^2}\leq \frac{1}{z^{1+\epsilon}}, \forall 0<\epsilon<1$, so value of this integral is finite. 2, Note that the kernel $K(x,y)=\frac{1}{1+|x|+|y|}$ appearing here does not belong to $L^2(\mathbb{R})$; otherwise, all the problems can be solved, for example, see Hilbert-Schmidt operator",,"['real-analysis', 'functional-analysis', 'operator-theory']"
99,An application of the Mean Value Theorem,An application of the Mean Value Theorem,,"I'm recalling this question from memory, so I may be messing it up a bit. Let $a/3+b/2+c=0$.  Show that $ax^2+bx+c=0$ has at least one root in $[0,1]$ using the Mean Value Theorem. Let $f(x)=ax^2+bc+c$.  Then $f(0)=c$ and $f(1)=a+b+c$.  Also $f'(x)=2ax+b$.  So there exists $f(\xi)=[f(1)-f(0)]/1=a+b-c$.  Then $a+b-c=2a\xi+b \Rightarrow (a-c)/2a=\xi$. I'm not sure if this is right or where to go from here.","I'm recalling this question from memory, so I may be messing it up a bit. Let $a/3+b/2+c=0$.  Show that $ax^2+bx+c=0$ has at least one root in $[0,1]$ using the Mean Value Theorem. Let $f(x)=ax^2+bc+c$.  Then $f(0)=c$ and $f(1)=a+b+c$.  Also $f'(x)=2ax+b$.  So there exists $f(\xi)=[f(1)-f(0)]/1=a+b-c$.  Then $a+b-c=2a\xi+b \Rightarrow (a-c)/2a=\xi$. I'm not sure if this is right or where to go from here.",,['real-analysis']
